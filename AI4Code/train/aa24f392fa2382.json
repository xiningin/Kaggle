{"cell_type":{"aa81dc93":"code","6873727e":"code","026823b0":"code","2eed06d6":"code","0781c466":"code","0a9fcd2f":"code","ffcba785":"code","12ed1243":"code","588a0c30":"code","dce3e78f":"code","06419cae":"code","7de49728":"code","4b571ae5":"code","94585a76":"code","bb702f19":"code","cc2bbfba":"markdown","f7f615a5":"markdown","95f34222":"markdown","d62f44f3":"markdown","468da550":"markdown","53e93863":"markdown","f1870f2a":"markdown","fb56e45f":"markdown","7712dac1":"markdown","2e9e5903":"markdown","fe29ca56":"markdown"},"source":{"aa81dc93":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit, GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMRegressor\nimport optuna\nimport os\nimport sys\nimport time\nimport seaborn as sns\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport warnings\nimport seaborn as sns\n\nwarnings.filterwarnings(\"ignore\")\n\nINPUT_PATH = '..\/input\/tabular-playground-series-apr-2021\/'\nOUTPUT_PATH = '.\/'\nSEED = 2021","6873727e":"train_df = pd.read_csv(INPUT_PATH + 'train.csv')\ntest_df = pd.read_csv(INPUT_PATH + 'test.csv')\nsample_submission = pd.read_csv(INPUT_PATH + 'sample_submission.csv')\ntarget_df = train_df.pop('Survived')\n\ntrain_df","026823b0":"print('Length of the dataframe:', len(train_df))\nprint('Missing values:')\ntrain_df.isna().sum()","2eed06d6":"def substrings_in_string(big_string, substrings):\n    for substring in substrings:\n        if big_string.find(substring) != -1:\n            return substring\n    return np.nan\n\ndef replace_titles(x):\n    title=x['Title']\n    if title in ['Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col']:\n        return 'Mr'\n    elif title in ['Countess', 'Mme']:\n        return 'Mrs'\n    elif title in ['Mlle', 'Ms']:\n        return 'Miss'\n    elif title =='Dr':\n        if x['Sex']=='Male':\n            return 'Mr'\n        else:\n            return 'Mrs'\n    else:\n        return title\n\ntitle_list=['Mrs', 'Mr', 'Master', 'Miss', 'Major', 'Rev', 'Dr', 'Ms', 'Mlle','Col', 'Capt', 'Mme', 'Countess', 'Don', 'Jonkheer']\n\nfor df in list([train_df, test_df]):\n    df['Cabin_class'] = df['Cabin'].str.extract(r'([a-zA-Z])')\n    df['Cabin_number'] = df['Cabin'].str.extract(r'(\\d+)')\n    df['Ticket_prefix'] = df['Ticket'].str.extract(r'([a-zA-Z.]+)')\n    df['Ticket_number'] = df['Ticket'].str.extract(r'(\\d+)')\n    df['NameLen'] = df['Name'].str.len() - 1\n    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n    df['IsAlone'] = df['FamilySize'] == 1\n    df['FirstName'] = df['Name'].map(lambda x: x.split(', ')[1])\n    df['LastName'] = df['Name'].map(lambda x: x.split(', ')[0])\n    df['Title'] = df['Name'].map(lambda x: substrings_in_string(x, title_list))\n    df['Title'] = df.apply(replace_titles, axis=1)\n    df['Age*Class'] = df['Age'] * df['Pclass']\n    df['FarePerPerson'] = df['Fare'] \/ (df['FamilySize'] + 1)\n    \n    # It seems like those with no cabin class had also no cabin number and ticket prefix\n    df['Cabin_class'].fillna('NoCabin', inplace = True)\n    df['Cabin_number'].fillna(0, inplace = True)\n    df['Ticket_prefix'].fillna('', inplace = True)\n    \n    \n    # Those who had no title were just normal people\n    df['Title'].fillna('', inplace = True)\n\ntest_df","0781c466":"print(train_df['Cabin_class'].unique())\nsns.scatterplot(data = train_df, x = 'Cabin_class', y = 'FarePerPerson', hue = 'Pclass')","0a9fcd2f":"train_df = train_df.drop(['Name', 'Ticket', 'Cabin', 'PassengerId'], axis = 1)\npassenger_ids = test_df['PassengerId']\ntest_df = test_df.drop(['Name', 'Ticket', 'Cabin', 'PassengerId'], axis = 1)\ntrain_df","ffcba785":"from sklearn.preprocessing import StandardScaler\n\nnumeric_columns = ['Pclass', 'SibSp', 'Parch', 'Fare', 'Cabin_number', 'Ticket_number', 'NameLen', 'FamilySize', 'IsAlone',\n                   'Age*Class', 'FarePerPerson']\nordinal_columns = ['Ticket_prefix', 'Cabin_class', 'Embarked', 'FirstName', 'LastName', 'Title']\none_hot_columns = ['Sex']\n\ntotal_df = pd.concat([train_df, test_df])\n\nfor c in ordinal_columns:\n    most_frequent_value = train_df[c].value_counts().index[0]\n    train_df = train_df.fillna(value = {c: most_frequent_value})\n    test_df = test_df.fillna(value = {c: most_frequent_value})\n\nenc = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value = np.nan)\n\nenc.fit(train_df[ordinal_columns])\ntrain_df[ordinal_columns] = enc.transform(train_df[ordinal_columns])\ntest_df[ordinal_columns] = enc.transform(test_df[ordinal_columns])\n    \npreprocessing_pipeline = Pipeline([\n    ('preprocessor', ColumnTransformer(\n        transformers = [\n            ('num', SimpleImputer(strategy = 'most_frequent'), numeric_columns + ordinal_columns),\n            ('one_hot', OneHotEncoder(), one_hot_columns),\n            #('cat', SimpleImputer(strategy = 'most_frequent'), one_hot_columns),\n        ]\n    )),\n    ('std_scaler', StandardScaler())\n])\n\ntrain_df_preprocessed = preprocessing_pipeline.fit_transform(train_df)\ntest_df_preprocessed = preprocessing_pipeline.transform(test_df)\n\n\ncolumns_new = numeric_columns + ordinal_columns + list(preprocessing_pipeline['preprocessor'].named_transformers_['one_hot'].get_feature_names(one_hot_columns))\ncolumns_new","12ed1243":"train_df_preprocessed = pd.DataFrame(train_df_preprocessed, columns = columns_new)\ntest_df_preprocessed = pd.DataFrame(test_df_preprocessed, columns = columns_new)\ntrain_df_preprocessed","588a0c30":"X_train, X_valid, y_train, y_valid = train_test_split(train_df_preprocessed, target_df, train_size = 0.8, stratify = target_df, random_state = SEED)","dce3e78f":"from boruta import BorutaPy\n\nfeat_selector = BorutaPy(\n    XGBRegressor(tree_method = 'gpu_hist', sampling_method = 'gradient_based', n_estimators = 200, learning_rate = 0.05, n_jobs = 4),\n    n_estimators = 'auto',\n    verbose = 2,\n    random_state = 1\n)\n\nfeat_selector.fit(X_train.values, y_train.values)\n\nfeature_names = columns_new\nfeature_ranks = list(zip(feature_names, feat_selector.ranking_, feat_selector.support_))\nfeature_ranks","06419cae":"cols_to_remove = []\nfor feat in feature_ranks:\n    if feat[2] == False: #and feat[1] >= 3:\n        cols_to_remove.append(feat[0])\n        pass\n\nprint('Dropping the following features:', cols_to_remove)\n\nX_train_filtered = X_train.drop(cols_to_remove, axis = 1)\nX_valid_filtered = X_valid.drop(cols_to_remove, axis = 1)\ntest_df_filtered = test_df_preprocessed.drop(cols_to_remove, axis = 1)","7de49728":"from sklearn.metrics import accuracy_score, f1_score\n\nbest_params = None\n\n\"\"\"\nimport optuna\n\ndef objective(trial, X_train = X_train_filtered, X_valid = X_valid_filtered, y_train = y_train, y_valid = y_valid):\n    obj_params = {'random_state': SEED,\n                  #'eval_metric': 'error',\n                  'tree_method': 'gpu_hist',\n                  'verbosity': 0,\n                  'n_estimators': trial.suggest_int('n_estimators', 400, 1000),\n                  'alpha': trial.suggest_float('alpha', 0, 10),\n                  'gamma': trial.suggest_float('gamma', 0, 10),\n                  'lambda': trial.suggest_float('lambda', 1, 10),\n                  'min_child_weight': trial.suggest_float('min_child_weight', 0, 10),\n                  'max_delta_step': trial.suggest_float('max_delta_step', 0, 10),\n                  'max_depth': trial.suggest_int('max_depth', 2, 12),\n                  'subsample': trial.suggest_float('subsample', 0.01, 1),\n                  'colsample_by_tree': trial.suggest_float('colsample_by_tree', 0.01, 1),\n                  'colsample_by_level': trial.suggest_float('colsample_by_level', 0.01, 1),\n                  'colsample_by_node': trial.suggest_float('colsample_by_node', 0.01, 1),\n                  'sampling_method': trial.suggest_categorical('sampling_method', ['uniform', 'gradient_based']),\n                  'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.08),\n                  #'interaction_constraints': trial.suggest...,\n                  #'scale_pos_weight ': trial.suggest...,\n                  }\n    \n    fit_params = {\n        'verbose': False,\n        'early_stopping_rounds': 5,        \n        'eval_set': [(\n            X_valid,\n            y_valid,\n        )],\n    }\n    \n    \n    obj_model = XGBRegressor(**obj_params)\n    obj_model.fit(X_train, y_train, **fit_params)\n    obj_preds = list(np.round(np.array(obj_model.predict(X_valid_filtered)),0))\n    \n    obj_score = accuracy_score(y_valid, obj_preds)\n    return obj_score\n\nstart_time = time.time()\nstudy = optuna.create_study(study_name = f\"optimization\", direction = 'maximize')\nstudy.optimize(objective, 3000)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best score:', study.best_trial.value)\n\nbest_params = study.best_trial.params\n\"\"\"","4b571ae5":"start_time = time.time()\n\nif best_params is None:\n    best_params = {'n_estimators': 683, 'alpha': 8.422354464381405, 'gamma': 0.2564422701451709, 'lambda': 5.332526708685053, 'min_child_weight': 3.748293558015677, 'max_delta_step': 1.3052043989118083, 'max_depth': 7, 'subsample': 0.5079819093322351, 'colsample_by_tree': 0.37665321750268915, 'colsample_by_level': 0.29014453377484806, 'colsample_by_node': 0.2490993927786025, 'sampling_method': 'uniform', 'learning_rate': 0.028783844683866237}\n    \nmodel_pipeline = Pipeline([\n    ('model', XGBRegressor(\n        random_state = SEED,\n        n_jobs = 4,\n        verbosity = 0,\n        #eval_method = 'error',\n        tree_method = 'gpu_hist',\n        #objective = 'binary:logistic',\n        **best_params\n    ))\n])\n\nmodel_pipeline.fit(X_train_filtered, y_train, model__early_stopping_rounds = 5, model__eval_set = [(X_valid_filtered, y_valid)], model__verbose = False)\nscore = accuracy_score(y_valid, list(np.round(np.array(model_pipeline.predict(X_valid_filtered)),0)))\nprint('Elapsed time: {} seconds:'.format(time.time() - start_time))\nprint('Score:', score)\ny_test = model_pipeline.predict(test_df_filtered)\n\ny_test[y_test >= 0.5] = 1\ny_test[y_test < 0.5] = 0\ny_test = y_test.astype(int)","94585a76":"\"\"\"\nfrom sklearn.linear_model import LogisticRegression\n\nstart_time = time.time()\n# Instantiate our model\nlogReg = LogisticRegression()\n# Fit our model to the training data\nlogReg.fit(X_train_filtered, y_train)\n# Predict on the test data\nscore = roc_auc_score(y_valid, logReg.predict(X_valid_filtered))\nprint('Elapsed time: {} seconds:'.format(time.time() - start_time))\nprint('Score:', score)\ny_test = logReg.predict(test_df_filtered)\n\"\"\"","bb702f19":"submission = pd.DataFrame({'PassengerId': passenger_ids, 'Survived': y_test})\nsubmission.to_csv('submission.csv', index = False)\nsubmission","cc2bbfba":"Drop features which we thing that are not needed","f7f615a5":"We will see the relationship between CabinClass and FarePerPerson in order to think how can we replace the missing values of the cabin class","95f34222":"Stratified train test split seems to be giving better results","d62f44f3":"# Feature Engineering and data visualization\n\nIn this section we will create new features based on the current ones, fill the missing data and encode the parameters","468da550":"Some data preprocessing ","53e93863":"# Search for the best features with Optuna\n\nWe will get the best features for XGBoost with a hyperparameter optimizer","f1870f2a":"# Check features performance with Boruta\n\nWe will see which features contribute the most to our model and choose if we want to remove some of them based on their performance","fb56e45f":"# Train the model and submit","7712dac1":"We will remove the features that Boruta shows us that are not needed","2e9e5903":"Logistic regression seems to not perform as good as XGBoost","fe29ca56":"# Loading datasets"}}