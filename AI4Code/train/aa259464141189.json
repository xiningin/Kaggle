{"cell_type":{"536c92cf":"code","0b709159":"code","cf8c5fb1":"code","8457ec54":"code","4b3d2b4f":"code","a6f8d682":"code","54847ef0":"code","8f87406c":"code","c46fb2ed":"code","2ce82315":"code","3c6729e8":"code","1a967982":"code","46f3ea9a":"code","16b6f6de":"code","33868926":"code","edb3e7ea":"code","8001bcd4":"code","cc15f105":"code","b7e04df4":"code","19ed8378":"code","e2e94e10":"code","bce4d53f":"code","93ac7d53":"code","db44e131":"code","e639b6ac":"code","db6b7567":"code","c5403d9f":"code","15b90384":"code","9085ff9b":"code","7277c620":"code","6909100f":"code","b36e3dd2":"code","1ef8d0d4":"code","b99af9c5":"code","5262cda2":"code","f850b0b3":"code","db2406e0":"code","becdeaa8":"code","32300208":"code","42756331":"code","fe60fe3a":"code","f588a8a3":"code","3798b4fc":"code","6af0813a":"code","a4ac8898":"code","913306a3":"code","6370e47e":"code","81ffb020":"code","eb59fcb3":"code","bfa3e322":"code","fbcf2116":"code","97feb86a":"code","f4c1e059":"code","a522b32a":"code","82b01233":"code","42ef442c":"code","ea4a1ee8":"code","63eaa471":"code","c23973cd":"code","56485a06":"code","8df06a26":"code","0052303f":"code","a65bf30f":"code","1a2f06bd":"code","53e7df94":"code","c20037fd":"code","5a40ca55":"code","26457232":"code","d292be28":"code","18280285":"code","8e2f4e62":"code","fe7e4b24":"code","0b950366":"code","85bf3864":"code","b116ca27":"code","427af13d":"code","43a2e81d":"code","64ae7fcb":"code","c09bca06":"code","58a0e5b1":"code","d07d0830":"code","da65f592":"code","cf0d0205":"code","d401b280":"code","5f0455de":"code","03d16cb1":"code","f426f4eb":"code","d91fa67a":"code","da70dcc8":"code","f84a6751":"code","c7c2445e":"code","ba46fe3f":"code","fce8326c":"code","3ff2f405":"code","18919448":"code","e980fe48":"code","faed5db2":"code","5c212718":"code","2d5938b7":"code","05fa4377":"code","f7641223":"code","8979f5cc":"code","e9de204b":"code","dc239dbe":"code","5aa78391":"code","52ca7f31":"code","8a43ac3b":"code","33052127":"code","d8b15af2":"code","2b88ba34":"code","157cd462":"code","b7002bdf":"code","8c907e47":"code","38d5da54":"code","3304b188":"code","ac80f79f":"code","5e1ab3e0":"code","dfecca52":"code","e1bca8dd":"code","c607d855":"code","2ecd7af0":"code","74e90ddc":"code","322526ba":"code","2092a083":"code","3c133d9f":"code","93cff6a2":"code","a07b102e":"code","51b1dc8c":"code","ae1bf093":"code","8f0c3efd":"code","404517cb":"code","1074d568":"code","5e0e25af":"code","2fd2bdf6":"code","0e3b3f07":"code","5db5f339":"code","e68448cf":"code","5d035f53":"code","c1e371f2":"code","73aa16d5":"code","a0feac36":"code","0480c7f8":"code","fcf64220":"code","e15dc340":"code","c14a045a":"code","c84d692e":"code","521d415e":"code","cd7dadee":"code","7a6d5b74":"code","964789a8":"code","567d4eba":"code","78537bfc":"code","f52dd691":"code","0777aa0a":"code","7b7d31e1":"code","8ee676e6":"code","2fbebc85":"code","4dc67341":"code","d926f281":"code","a00bdc6d":"code","110fd007":"code","9fd7ed64":"code","f61a7a7b":"code","ae60f556":"code","5380151d":"code","bd8b1501":"code","6f8ef82c":"code","b12beb5e":"code","0e9f10f4":"code","2929c485":"code","1e2f0027":"code","9de58fb5":"code","2f5a37cb":"code","19009d03":"code","b0bc9fb3":"code","93a6e0f7":"code","60d0de23":"code","36c2071a":"code","fd5b238d":"code","db52703b":"code","258fc7ff":"code","4dcaeea5":"code","9eced9f9":"code","d41b89fd":"code","ab4ebbdf":"code","c60311a1":"code","ce97272b":"code","cf571cce":"code","736fbda9":"code","b36a3394":"code","42941ad4":"code","3f6fa948":"code","5ae00065":"code","15874bc6":"code","fed447b7":"code","0e0c49ec":"code","e7aa0786":"markdown","0e2e60c9":"markdown","19460a77":"markdown","8b71d01e":"markdown","6e2ab5aa":"markdown","16e7906e":"markdown","0a8ad271":"markdown","a16294d1":"markdown","fb9dc410":"markdown","ffa7fdc8":"markdown","1d04a826":"markdown","8df0fe59":"markdown","2f3c4ecc":"markdown","298843d9":"markdown","5e352264":"markdown","14316df0":"markdown","da4f9ed1":"markdown","7d612a1d":"markdown","87a7fa7f":"markdown","925a4d13":"markdown","fdc4b8e3":"markdown","25e82fd4":"markdown","27ec1c19":"markdown","f51bb15a":"markdown","d65a880c":"markdown","3323782b":"markdown","ffc493bc":"markdown","836a5836":"markdown","dcf21699":"markdown","84c13887":"markdown","a67fd47c":"markdown","31276585":"markdown","fb3858b3":"markdown","2f5c8fe9":"markdown","6fdfff81":"markdown","687cecd3":"markdown","c6a9aeef":"markdown","70803bdb":"markdown","75205fcd":"markdown","d17018c3":"markdown","190c9c51":"markdown","4e57f9cb":"markdown","47fc42d8":"markdown","8ccda0a3":"markdown","db8d798a":"markdown","584ad3f3":"markdown","81cb4fa8":"markdown","972cdf0d":"markdown","735a81c1":"markdown","081899b5":"markdown","74d45f16":"markdown","6924c54c":"markdown","2d848ac8":"markdown","f2bde23c":"markdown","a96485bf":"markdown","35957ded":"markdown","dd0a0123":"markdown","cd6a909e":"markdown","b644c512":"markdown","3555daa5":"markdown","27517630":"markdown","9506f59d":"markdown","ee2bb95f":"markdown","defc33f6":"markdown","e3429320":"markdown","0e2657b4":"markdown","c9ed639a":"markdown","0ac198cc":"markdown","05d544db":"markdown","25d3200e":"markdown","cb047aab":"markdown","6da73a8d":"markdown","bfa1ff90":"markdown","da985326":"markdown","93678af7":"markdown","0719bf3b":"markdown","121f821a":"markdown","0d756330":"markdown","8b0ea337":"markdown","03582046":"markdown","72155a1a":"markdown","c8f0d887":"markdown","29bb0aca":"markdown","fbcd652c":"markdown"},"source":{"536c92cf":"## First, let's import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# To visualize missing values\nimport missingno\n\n# Machine Learning\nimport catboost\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import model_selection, tree, preprocessing ,metrics\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\n\n\n%matplotlib inline\nmpl.rcParams[\"patch.force_edgecolor\"]=True\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0b709159":"# Import training data\ndata1 = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntrain = data1.copy()\ntrain.head()","cf8c5fb1":"# Import test data\ndata2 = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntest = data2.copy()\ntest.head()","8457ec54":"train.head()","4b3d2b4f":"train.describe()","a6f8d682":"train.info()","54847ef0":"# Plot graphic of missing values\nmissingno.matrix(train, figsize=(30,5))","8f87406c":"train.isnull().sum()","c46fb2ed":"# What procentage of missing values in column Age and Cabin\nage_column = (train[\"Age\"].isna().sum() \/ len(train)) *100\n\ncabin_column = (train[\"Cabin\"].isna().sum() \/ len(train)) * 100\n\nprint(f\"Procent of missing data in 'Age' column is: {round(age_column,2)}%\")\nprint(f\"Procent of missing data in 'Cabin' column is: {round(cabin_column, 2)}%\")","2ce82315":"# Drop Cabin column\ntrain.drop([\"Cabin\"], axis=1, inplace=True)\ntrain.head()","3c6729e8":"# train.PassengerId.unique()","1a967982":"train.drop([\"PassengerId\"], axis=1, inplace=True)\ntrain.head()","46f3ea9a":"train.Survived.value_counts()","16b6f6de":"survived_dict = {\"survived\":train.Survived.value_counts()[1],\n                 \"Not Survived\": train.Survived.value_counts()[0]}","33868926":"# Create a plot for 'Survived' column\nfig, ax = plt.subplots(figsize=(8,6))\n\n# Plot\nax.barh(list(survived_dict.keys()),list(survived_dict.values()), color=[\"red\", \"blue\"])\n\n# Description\nax.set(title=\"Survived vs Not Survived in Titanic\");","edb3e7ea":"np.sort(train[\"Pclass\"].unique())","8001bcd4":"train[\"Pclass\"].dtype","cc15f105":"pclass_count = train[\"Pclass\"].value_counts().sort_index()","b7e04df4":"class_list = [\"1st Class\", \"2nd Class\", \"3rd Class\"]\n\nclass_dict = {}\nfor i , count in enumerate(pclass_count):\n    class_dict[class_list[i]] = count\n    \n# Create a plot for Passenger Class\nfig, ax = plt.subplots(figsize=(8,5))\nax.bar(class_dict.keys(), class_dict.values(), color=[\"red\", \"blue\", \"green\"])\n\n# Describe plot\nax.set(title=\"Number of people per class\");","19ed8378":"# Create a plot\nfig, ax = plt.subplots(figsize=(8,6))\nax = sns.countplot(\"Pclass\", data=train, hue=\"Survived\", palette=\"coolwarm_r\")\n\nax.set(title=\"Survived per Passenger Class\")","e2e94e10":"# How many survived in the Pclass\nsurv_class = train.groupby(\"Pclass\").sum()[\"Survived\"]","bce4d53f":"surv_class","93ac7d53":"class_sum = train.Pclass.value_counts().sort_index()","db44e131":"class_sum","e639b6ac":"survived_class1 = (surv_class[1] \/ class_sum[1])* 100\nsurvived_class2 = (surv_class[2] \/ class_sum[2]) * 100\nsurvived_class3 = (surv_class[3] \/ class_sum[3]) * 100\n\nprint(f\"Survived procentage in 1st Class: {round(survived_class1, 2)}%\")\nprint(f\"Survived procentage in 2nd Class: {round(survived_class2, 2)}%\")\nprint(f\"Survived procentage in 3rd Class: {round(survived_class3, 2)}%\")","db6b7567":"train.Pclass.dtype","c5403d9f":"train.head()","15b90384":"# Check 5 rows of the dataframe\ntrain.head()","9085ff9b":"train.Name.dtype","7277c620":"# Display hole series or dataframe\nwith pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n    print(train.Name[:10])","6909100f":"# Extract title from string\nname_list = list(train.Name)\n\ntitle = []\n\nfor i in range(len(name_list)):\n    \n    person = name_list[i].split(\",\")[1].split(\".\")[0]\n    person = person.strip(\" \")\n    \n    title.append(person)   ","b36e3dd2":"title_series = pd.Series(title)","1ef8d0d4":"title_series.unique()","b99af9c5":"with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n    print(title_series[:10])","5262cda2":"title_series.value_counts() ","f850b0b3":"# Create column with title\ntrain[\"Title\"] = title","db2406e0":"# Create plot\nplt.figure(figsize=(8,6))\ntrain.Title.value_counts().plot(kind=\"bar\");","becdeaa8":"title_list_to_check = [\"Rev\", \"Mlle\", \"Mme\", \"Jonkheer\", \"Dr\",\"Master\"]","32300208":"element_dict = {}\n\ndef find_index(ser, ele_list):\n    \"\"\"\n    A function find element and their indexes in a series.\n    \"\"\"\n    for item in ele_list:\n        # indices = [i for i, x in enumerate(ser) if x ==item]\n        element_dict[item] = [i for i, x in enumerate(ser) if x ==item]\n    return element_dict","42756331":"find_err = find_index(title_series, title_list_to_check)","fe60fe3a":"my_list = find_err[\"Dr\"]\n\nfor index in my_list:\n    print(train.Name[index])","f588a8a3":"# check how to class 'Mme'\ntrain.Name[369]","3798b4fc":"train[train.Name == train.Name[369]]","6af0813a":"# Plot the age frequency for boy under 16\nmaster = train[train.Title ==\"Master\"]\nmaster = master.dropna()\n\nmiss = train[train.Title == \"Miss\"]\nmiss = miss.dropna()\nmiss_under16 = miss[miss[\"Age\"] < 16]\n\nfig, axes = plt.subplots(1,2, figsize=(14,6))\n\n# Plot for boys under 16\naxes[0].hist(master[\"Age\"])\n# Add labels and title\naxes[0].set(title=\"Number \")\n\n# Plot for girls under 16\naxes[1].hist(miss_under16[\"Age\"]);","a4ac8898":"miss_under16.Age.mean()","913306a3":"# How many boys and girls\nlen(miss_under16[\"Age\"]), len(master[\"Age\"])","6370e47e":"train = train.replace({\"Title\": {796: \"female\"}})","81ffb020":"train.head()","eb59fcb3":"train.Title.unique()","bfa3e322":"def replace_titles(df):\n    \"\"\"\n    A function which map title column\n    \"\"\"\n    title = df[\"Title\"]\n    \n    if title in [\"Mr\",\"Don\", \"Rev\",\"Major\",\"Sir\",\"Col\",\"Capt\",\"Jonkheer\"]:\n        return \"Mr\"\n    elif title in [\"Mrs\",\"Lady\",\"Mlle\",\"Mme\",\"the Countess\",\"Ms\"]:\n        return \"Mrs\"\n    elif title == \"Dr\":\n        if df[\"Sex\"] == \"male\":\n            return \"Mr\"\n        else:\n            return \"Mrs\"\n    else:\n        return title","fbcf2116":"train.Title = train.apply(replace_titles, axis=1)","97feb86a":"train.Title.value_counts()","f4c1e059":"# Check Title column again\nplt.figure(figsize=(8,6))\ntrain.Title.value_counts().plot(kind=\"bar\");","a522b32a":"train.Sex.value_counts()","82b01233":"train.Title.value_counts()","42ef442c":"182+131+1","ea4a1ee8":"uniq = train.Title.unique()","63eaa471":"plt.figure(figsize=(8,6))\nsns.countplot(\"Title\", data=train, hue=\"Survived\", palette=\"coolwarm\")","c23973cd":"train = train.drop(\"Name\", axis=1)\ntrain.head()","56485a06":"train.Sex.value_counts()","8df06a26":"from collections import Counter\n\nd = Counter(train.Sex)","0052303f":"plt.figure(figsize=(16,6))\n\nplt.subplot(1,2,1)\nsns.barplot(x=list(d.keys()), y=list(d.values()))\n\nplt.subplot(1,2,2)\nsns.countplot(\"Sex\", data=train, hue=\"Survived\")","a65bf30f":"child_df = train[train[\"Age\"] < 16]","1a2f06bd":"len(child_df)","53e7df94":"plt.figure(figsize=(14,6))\nplt.subplot(1,2,1)\nsns.countplot(\"Sex\", data=child_df, hue=\"Survived\", palette=\"Pastel1\")\n\nplt.subplot(1,2,2)\nsns.countplot(\"Sex\", data=child_df, hue=\"Pclass\", palette=\"Pastel1\");","c20037fd":"children_procent = len(child_df) \/ len(train) * 100\n\nprint(f\"Procent of children on the ship: {children_procent:.2f}%\")","5a40ca55":"train[\"Sex\"].head()","26457232":"fig, ax = plt.subplots(figsize=(14,6))\n\nax = sns.distplot(train[\"Age\"])\n\nax.set(title=\"Age Distribution\")","d292be28":"train.Age.isnull().sum()","18280285":"bool1 = train[\"Age\"].isna()\n\n# df1 = train[train.isna().any(axis=1)]","8e2f4e62":"# Null values in age column grouped by title with survived=1\ntrain[bool1].groupby(\"Title\").sum()","fe7e4b24":"# DataFrame contains all null values by Age\nall_age_null = train[bool1]\n\n\nall_age_null[\"Survived\"].value_counts()","0b950366":"# Let's visualize missing values in age column\nage_by_title = all_age_null.aggregate({\"Title\": [\"value_counts\"]})\nage_by_title = age_by_title[\"Title\"]\n# Data for barplot\nname_index = age_by_title.index\nvalues = age_by_title.values\nvalues = values.reshape(1,4)\n\n# Create figure\nfig, ax = plt.subplots(figsize=(8,6))\n\n# Draw a plot\nax = sns.barplot(x=name_index, y=values[0], palette=\"winter_r\")\n\n# Add title and y label\nax.set(title=\"Number of missing values in 'Age column' per 'Title column'\",\n       ylabel=\"Frequency\");","85bf3864":"age_by_title","b116ca27":"train.head()","427af13d":"# mean for men and women above 16\nadults_mean_df = train[train[\"Age\"] > 16]\nadults_mean = adults_mean_df[\"Age\"].mean()","43a2e81d":"master_mean = master[\"Age\"].mean()\nmiss_16_mean = miss_under16[\"Age\"].mean()\n\nprint(f\"The mean age for adults is: {adults_mean:.2f}\")\nprint(f\"The mean age for boys is: {master_mean:.2f}\")\nprint(f\"The mean age for girls is: {miss_16_mean:.2f}\")","64ae7fcb":"# Fill null value with their means\n# Could be a better method but I am still learning\ntitle_list = [\"Master\"]\ntrain.Age = np.where(train.Title.isin(title_list), train.Age.fillna(master_mean), train.Age)","c09bca06":"title_list2 = [\"Miss\"]\ntrain.Age = np.where(train.Title.isin(title_list2), train.Age.fillna(miss_16_mean), train.Age)","58a0e5b1":"title_list3 = [\"Mrs\", \"Mr\"]\ntrain.Age = np.where(train.Title.isin(title_list3), train.Age.fillna(adults_mean), train.Age)","d07d0830":"# Plot graphic of missing values\nmissingno.matrix(train, figsize=(30,5))","da65f592":"train.head()","cf0d0205":"def male_female_child(passenger):\n    \"\"\"\n    Function add another category 'child'\n    \"\"\"\n    age, sex = passenger\n    if age < 16:\n        return \"child\"\n    else:\n        return sex","d401b280":"train[\"Person\"] = train[[\"Age\",\"Sex\"]].apply(male_female_child, axis=1)\ntrain.head()","5f0455de":"# Let's drop Sex column\ntrain = train.drop([\"Sex\"], axis=1)\ntrain.head()","03d16cb1":"# Create plot for Person column\nfig, ax = plt.subplots(figsize=(8,6))\n\nax = sns.countplot(\"Person\", data=train, hue=\"Survived\", palette=\"winter_r\")","f426f4eb":"fig = sns.FacetGrid(train,hue='Person',aspect=4)\n\nfig.map(sns.kdeplot,'Age',shade=True)\n\noldest = train['Age'].max()\n\nfig.set(xlim=(0,oldest))\n\nfig.add_legend();","d91fa67a":"# Create plot for Sibsp and Parch\nplt.figure(figsize=(14,4))\n\nplt.subplot(1,2,1)\nsns.countplot(train[\"SibSp\"], palette=\"winter_r\")\n\nplt.subplot(1,2,2)\nsns.countplot(train[\"Parch\"], palette=\"winter_r\")","da70dcc8":"train.head()","f84a6751":"train[\"Ticket\"].value_counts()","c7c2445e":"# train[\"Ticket\"].unique()","ba46fe3f":"train = train.drop([\"Ticket\"], axis=1)\ntrain.head()","fce8326c":"train_df = train.copy()","3ff2f405":"plt.figure(figsize=(8,6))\nsns.distplot(train_df[\"Fare\"],kde_kws={\"color\": \"r\"})\nplt.title(\"Fare Price ($)\");","18919448":"train_df[\"Fare\"].describe()","e980fe48":"train_df.Fare.max()","faed5db2":"plt.figure(figsize=(8,5))\n\nsns.scatterplot(x=\"Age\", y=\"Fare\", data=train_df, hue=\"Survived\")\nplt.title(\"Ticket Price per Age\")","5c212718":"# List of people who paid 500+ for a ticket\ndata1[data1[\"Fare\"] > 300]","2d5938b7":"train_df.head()","05fa4377":"train_df[\"Embarked\"].isna().sum()","f7641223":"# Names of people where missing data in embarked column\ndata1[data1[\"Embarked\"].isna()]","8979f5cc":"train_df[\"Embarked\"] = train_df[\"Embarked\"].fillna(\"S\")\ntrain_df.head()","e9de204b":"# Make some visualizations\nembarked_count = train_df.Embarked.value_counts()\n\nembarked_dict = {\"Southampton\": embarked_count[0],\n                 \"Cherbourg\": embarked_count[1],\n                 \"Queenstown\": embarked_count[2]}\n\n# Ceate plot 1\nplt.figure(figsize=(16,5))\n\nplt.subplot(1,2,1)\nsns.barplot(x=list(embarked_dict.keys()),\n            y=list(embarked_dict.values()),\n            palette=\"winter_r\");\n# Add title for plot 1\nplt.title(\"Port of Embarkation\")\n\n# Create plot 2\nplt.subplot(1,2,2)\nsns.countplot(\"Embarked\", \n              data=train_df, \n              hue=\"Survived\", \n              palette=\"winter_r\")\nplt.title(\"People survived vs Embarked City\");","dc239dbe":"# Check for missing values before making dummies variables","5aa78391":"train_df.head()","52ca7f31":"train_df.isnull().sum()","8a43ac3b":"train_df.dtypes","33052127":"train_df_prep = train_df.copy()","d8b15af2":"# Export transformed dataset into the file\n#train_df_prep.to_csv(\"..\/input\/titanic\/train_no_missing_values.csv\", index=False)","2b88ba34":"train_df_prep.head()","157cd462":"from sklearn.preprocessing import OneHotEncoder, LabelEncoder, LabelBinarizer","b7002bdf":"train_df_con = train_df_prep.apply(LabelEncoder().fit_transform)\ntrain_df_con.head()","8c907e47":"# Train data\nX_train = train_df_con.drop([\"Survived\"], axis=1)\n# Labels\ny_train = train_df_con[\"Survived\"]","38d5da54":"X_train.shape, y_train.shape","3304b188":"# Let's create a function to deal with our models\ndef machine_algorithm(ml_model, X_train, y_train, cv):\n    \n    # Run model only one time\n    model = ml_model.fit(X_train, y_train)\n    acc = round(model.score(X_train, y_train) *100,2)\n    \n    # for cross-validation\n    train_pred = model_selection.cross_val_predict(ml_model,\n                                                   X_train,\n                                                   y_train,\n                                                   n_jobs=-1,\n                                                   cv=cv)\n    cv_acc = round(metrics.accuracy_score(y_train,\n                                          train_pred)*100, 2)\n    return train_pred, acc, cv_acc","ac80f79f":"%%time\ntrain_pred, lr_acc, lr_cv_acc = machine_algorithm(LogisticRegression(),\n                                                        X_train,\n                                                        y_train,\n                                                        10)\n\nprint(f\"Accuracy: {lr_acc}%\")\nprint(f\"Accuracy CV 10-fold: {lr_cv_acc}%\")\nprint(\"\\n\")","5e1ab3e0":"%%time\nknn_pred, knn_acc, knn_cv_acc = machine_algorithm(KNeighborsClassifier(),\n                                                          X_train,\n                                                          y_train,\n                                                          10)\n\nprint(f\"Accuracy: {knn_acc}%\")\nprint(f\"Accuracy CV 10-fold: {knn_cv_acc}%\")\nprint(\"\\n\")","dfecca52":"%%time\ngnb_pred, gnb_acc, gnb_cv_acc = machine_algorithm(GaussianNB(),\n                                                              X_train,\n                                                              y_train,\n                                                              10)\nprint(f\"Accuracy: {gnb_acc}%\")\nprint(f\"Accuracy CV 10-fold: {gnb_cv_acc}%\")\nprint(\"\\n\")","e1bca8dd":"%%time\nsvc_pred, svc_acc, svc_cv_acc = machine_algorithm(LinearSVC(),\n                                                  X_train,\n                                                  y_train,\n                                                  10)\n \nprint(f\"Accuracy: {svc_acc}%\")\nprint(f\"Accuracy CV 10-fold: {svc_cv_acc}%\")                                                           ","c607d855":"rfc_pred, rfc_acc, rfc_cv_acc = machine_algorithm(RandomForestClassifier(),\n                                                              X_train,\n                                                              y_train,\n                                                              10)\n\nprint(f\"Accuracy: {rfc_acc}%\")\nprint(f\"Accuracy CV 10-fold: {rfc_cv_acc}%\")","2ecd7af0":"%%time\ndtc_pred, dtc_acc, dtc_cv_acc = machine_algorithm(DecisionTreeClassifier(),\n                                                              X_train,\n                                                              y_train,\n                                                              10)\n\nprint(f\"Accuracy: {dtc_acc}%\")\nprint(f\"Accuracy CV 10-fold: {dtc_cv_acc}%\")","74e90ddc":"%%time\ngbc_pred, gbc_acc, gbc_cv_acc = machine_algorithm(GradientBoostingClassifier(),\n                                                  X_train,\n                                                  y_train,\n                                                  10)\nprint(f\"Accuracy: {gbc_acc}%\")\nprint(f\"Accuracy CV 10-fold: {gbc_cv_acc}%\")","322526ba":"%%time\ncat_feature = np.where(X_train.dtypes !=np.float)[0]\n\ntrain_pool = Pool(X_train, y_train, cat_feature)\n\n# Instatiate model\ncatboost_model = CatBoostClassifier(iterations=1000,\n                                    custom_loss=[\"Accuracy\"],\n                                    loss_function=\"Logloss\",\n                                    verbose=False)\n# Fit the model\ncatboost_model.fit(train_pool, plot=True)\n\n# Accuracy for catboost\ncat_acc = round(catboost_model.score(X_train, y_train)*100,2)\nprint(f\"Accuracy: {cat_acc}%\")","2092a083":"%%time\n# Set params for cross-validation same as for initial model\ncv_params = catboost_model.get_params()\n\n# Run Cross-validation for catboost\ncv_catboost = cv(train_pool, \n              cv_params,\n              fold_count=10,\n              plot=True,\n              verbose_eval=False)\n\n# Catboost result save later into dataframe \ncat_cv_acc = round(np.max(cv_catboost[\"test-Accuracy-mean\"]) *100 ,2)","3c133d9f":"# Accuracy dataframe for the models who run only ones\ndf_models1 =  pd.DataFrame({\n    \"Models\": [\"Logistic Regression\",\"KNN\",\"Naive Bayes\",\n               \"Linear SVC\", \"Random Forest\",\"Decision Tree\",\n               \"Gradient Boost\",\"CatBoost\"],\n    \"Score\": [lr_acc,\n               knn_acc,\n               gnb_acc,\n               svc_acc,\n               rfc_acc,\n               dtc_acc,\n               gbc_acc,\n               cat_acc\n             ]})\n\nprint(\"****Models Accuracy****\")\ndf_models1.sort_values(by=\"Score\", ascending=False, ignore_index=True)","93cff6a2":"# Accuracy dataframe for the models with cv=10\ndf_models_2 =  pd.DataFrame({\n      \"Models\": [\"Logistic Regression\",\"KNN\",\"Naive Bayes\",\n                 \"Linear SVC\", \"Random Forest\",\"Decision Tree\",\n                 \"Gradient Boost\",\"CatBoost\"],\n      \"Score\": [lr_cv_acc,\n                knn_cv_acc,\n                gnb_cv_acc,\n                svc_cv_acc,\n                rfc_cv_acc,\n                dtc_cv_acc,\n                gbc_cv_acc,\n                cat_cv_acc\n               ]})\n\nprint(\"***Cross Validation Models Accuracy***\")\ndf_models_2.sort_values(by=\"Score\", ascending=False, ignore_index=True)","a07b102e":"from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n\nnp.random.seed(45)\n\n# Parameters for Gradien Boost Classifier\ngbc_params_random = {\"n_estimators\": [100,200,300,400],\n                     \"min_samples_split\": np.arange(2,20,2),\n                     \"min_samples_leaf\": [1,2,4],\n                     \"max_features\": [\"auto\",\"sqrt\",\"log2\"],\n                     }\n\n\n#Itstantiate RandomizedSearchCV\ngbc_random_s = RandomizedSearchCV(GradientBoostingClassifier(),\n                                  param_distributions=gbc_params_random,\n                                  scoring='accuracy',\n                                  random_state=45,                      \n                                  n_iter=100,\n                                  verbose=1,\n                                  cv=3,\n                                  n_jobs=-1)\n# Fit random search model\ngbc_random_s.fit(X_train, y_train)","51b1dc8c":"gbc_best_rand = round((gbc_random_s.best_score_)*100,2)\n\ngbc_rand_diff = round(gbc_best_rand - gbc_cv_acc, 2)\nprint(f\"Accuracy for Gradient Boost with RandomizedSearch : {gbc_best_rand}%\")\nprint(f\"Accuracy for Gradient Boost with default parameters: {gbc_cv_acc}%\")\nprint(f\"Improvment of: {gbc_rand_diff}%\")","ae1bf093":"# Check the best parameters for GradientBoostClasifier\ngbc_random_s.best_params_","8f0c3efd":"%%time\n\nnp.random.seed(45)\n# Set of parameters for GridSearchCV\ngbc_params_gs = {\"n_estimators\":[200,300,400],\n                 \"min_samples_split\":[4,8,16],\n                 \"min_samples_leaf\":[1,2,4],\n                 \"max_features\":[\"sqrt\",\"auto\"],\n                 \"random_state\": [45]}\n\n# Instantiate GridSearchCV\ngbc_grid_s = GridSearchCV(GradientBoostingClassifier(),\n                          param_grid=gbc_params_gs,\n                          n_jobs=-1,\n                          cv=3)\n# Fit search model\ngbc_grid_s.fit(X_train,y_train)","404517cb":"gbc_grid_s.best_params_","1074d568":"best_grid_gbc_score = round(gbc_grid_s.best_score_ *100,2)\n\ngbc_gr_diff = best_grid_gbc_score - gbc_cv_acc\n\nprint(f\"Accuracy with best param and cv=10: {best_grid_gbc_score}%\")\nprint(f\"Accuracy with default param and cv=10: {round(gbc_cv_acc,2)}%\")\nprint(f\"Improvment of: {gbc_gr_diff:.2f}%\")","5e0e25af":"gbc_grid_s.best_params_","2fd2bdf6":"# Instantiate Gradient Boost with best parameters found by GridSearchCV \nbest_gbc_model = GradientBoostingClassifier(max_features=\"sqrt\",\n                                            min_samples_split=4,\n                                            min_samples_leaf=4,\n                                            n_estimators=300,\n                                            random_state=45)\n# Fit data\nbest_gbc_model.fit(X_train, y_train)\n\n# Cross-validation for the model\ngbc_best_pred = model_selection.cross_val_predict(best_gbc_model,\n                                                  X_train,\n                                                  y_train,\n                                                  n_jobs=-1,\n                                                  cv=10)\n# Accuracy score for the model with best parameters\nbest_gbc_cv_acc = round(metrics.accuracy_score(y_train, gbc_best_pred)*100,2)","0e3b3f07":"diff_cv = best_gbc_cv_acc - round(gbc_cv_acc,2)\nprint(f\"Accuracy for Gradient Boost with cv run 10-folds: {best_gbc_cv_acc}%\")\nprint(f\"Accuracy for Gradient Boost with default param and cv 10-folds : {round(gbc_cv_acc,2)}%\")\nprint(f\"Improvment of: {round(diff_cv,2)}%\")","5db5f339":"np.random.seed(45)\n\n# Parameters fo Random Forest Classifier\nrfc_params_random = {\"n_estimators\": [100,200,300,400],\n                     \"min_samples_split\": np.arange(2,20,2),\n                     \"min_samples_leaf\": [1,2,4],\n                     \"max_features\": [\"auto\",\"sqrt\",\"log2\"],\n                     }\n\n# Instantiate RandomizedSearchCV\nrfc_random_s = RandomizedSearchCV(RandomForestClassifier(),\n                                 param_distributions=rfc_params_random,\n                                 scoring=\"accuracy\",\n                                 random_state=45,\n                                 n_iter=100,\n                                 cv=3,\n                                 n_jobs=-1)\n# Fit the model\nrfc_random_s.fit(X_train, y_train)","e68448cf":"rfc_random_s.best_params_","5d035f53":"rfc_best_rand = round((rfc_random_s.best_score_)*100,2)\n\n# Difference between random search and cv fold 10 times\nrfc_diff = round((rfc_best_rand - rfc_cv_acc),2)\n\nprint(f\"Accuracy for Random Forest found with Random Search: {rfc_best_rand}%\")\nprint(f\"Accuracy for Random Forest with default and cv=10: {rfc_cv_acc}%\")\nprint(f\"Improvment of: {rfc_diff}%\")","c1e371f2":"rfc_random_s.best_params_","73aa16d5":"np.random.seed(45)\n# Set parameters for GridSearchCV\nrfc_params_gr = {\"n_estimators\": [100,200,300, 1000],\n                 \"min_samples_split\": [2,4,6],\n                 \"min_samples_leaf\": [2,4,5],\n                 \"max_features\": [\"sqrt\", \"log2\",\"auto\"],\n                 \"random_state\": [45]\n                 }\n\n# Instantiate GridSearchCV for Random Forest\nrfc_grid_s = GridSearchCV(RandomForestClassifier(),\n                          param_grid=rfc_params_gr,\n                          n_jobs=-1,\n                          cv=3)\n# Fit search model\nrfc_grid_s.fit(X_train, y_train)","a0feac36":"rfc_grid_s.best_params_","0480c7f8":"rfc_grid_s.best_score_","fcf64220":"# The best score, cv=3\nrfc_best_grid = round((rfc_grid_s.best_score_)*100,2)\n\n# Differnt between grid search and cv=10\nrfc_best_diff = round(rfc_best_grid - rfc_cv_acc,2)\n\nprint(f\"Accuracy for Random Forest found with Grid Search: {rfc_best_grid}%\")\nprint(f\"Accuracy for Random Forest with default param and cv=10: {rfc_cv_acc}%\")\nprint(f\"Improvment of: {rfc_best_diff}%\")","e15dc340":"rfc_grid_s.best_params_","c14a045a":"np.random.seed(45)\n# Instantiate Random Forest with the best parameters found by GridSearchCV\nbest_rfc_model = RandomForestClassifier(max_features=\"log2\",\n                                        min_samples_leaf=2,\n                                        min_samples_split=2,\n                                        n_estimators=100,\n                                        random_state=45)\n# fit model\nbest_rfc_model.fit(X_train, y_train)\n\n# Cross-validation for the model\nrfc_best_pred = model_selection.cross_val_predict(best_rfc_model,\n                                                  X_train,\n                                                  y_train,\n                                                  n_jobs=-1,\n                                                  verbose=1,\n                                                  cv=10)\n\n# Accuracy score for the model with best parameters\nbest_rfc_cv_acc = round(accuracy_score(y_train, rfc_best_pred))","c84d692e":"# Accuracy different beteen best and default parameters for Random Forest\n\ndiff_rfc = (best_rfc_cv_acc*100) - rfc_cv_acc\n\nprint(f\"Accuracy with best param and cv run 10-folds: {best_rfc_cv_acc*100}%\")\nprint(f\"Accuracy with default param and cv 10-folds : {rfc_cv_acc}%\")\nprint(f\"Improvment of: {diff_rfc:.2f}%\")","521d415e":"best_rfc_cv_acc = best_rfc_cv_acc*100","cd7dadee":"# Accuracy dataframe for the models with cv=10\ndf_models_3 =  pd.DataFrame({\n      \"Models\": [\"Logistic Regression\",\"KNN\",\"Naive Bayes\",\n                 \"Linear SVC\", \"Random Forest Best Param\",\"Decision Tree\",\n                 \"Gradient Boost Best Param\",\"CatBoost\"],\n      \"Score\": [lr_cv_acc,\n                knn_cv_acc,\n                gnb_cv_acc,\n                svc_cv_acc,\n                best_rfc_cv_acc,\n                dtc_cv_acc,\n                best_gbc_cv_acc,\n                cat_cv_acc\n               ]})\n\nprint(\"***Cross Validation Models Accuracy***\")\ndf_models_3.sort_values(by=\"Score\", ascending=False, ignore_index=True)","7a6d5b74":"test.head()","964789a8":"passengerId = test[\"PassengerId\"]","567d4eba":"def replace_titles(df):\n    \"\"\"\n    A function which map title column\n    \"\"\"\n    title = df[\"Title\"]\n    \n    if title in [\"Mr\",\"Don\", \"Rev\",\"Major\",\"Sir\",\"Col\",\"Capt\",\"Jonkheer\"]:\n        return \"Mr\"\n    elif title in [\"Ms\",\"Lady\",\"Mlle\",\"Mme\",\"the Countess\",\"Dona\"]:\n        return \"Mrs\"\n    elif title == \"Dr\":\n        if df[\"Sex\"] == \"male\":\n            return \"Mr\"\n        else:\n            return \"Mrs\"\n    else:\n        return title\n\ndef male_female_child(passenger):\n    \"\"\"\n    Function add another category 'child'\n    \"\"\"\n    age, sex = passenger\n    if age < 16:\n        return \"child\"\n    else:\n        return sex","78537bfc":"# Drop Cabin column and passanger column\ntest.drop([\"Cabin\"], axis=1, inplace=True)\n#test.drop([\"PassengerId\"], axis=1, inplace=True)\n\n# Extract title from string\nname_list = list(test.Name)\n\ntitle = []\n\nfor i in range(len(name_list)):\n    \n    person = name_list[i].split(\",\")[1].split(\".\")[0]\n    person = person.strip(\" \")\n    \n    title.append(person)   \n    \n# Create column with title\ntest[\"Title\"] = title\n\n# Map title column   \ntest.Title = test.apply(replace_titles, axis=1)\n\n# Drop Name column for test set\ntest = test.drop(\"Name\", axis=1)\n\n# mean for men and women above 16\nadults_mean_df = test[test[\"Age\"] > 16]\nadults_mean = adults_mean_df[\"Age\"].mean()\n\nmaster_mean = master[\"Age\"].mean()\nmiss_16_mean = miss_under16[\"Age\"].mean()\n\n# Fill null value with their means\ntitle_list = [\"Master\"]\ntest.Age = np.where(test.Title.isin(title_list), test.Age.fillna(master_mean), test.Age)\n\ntitle_list2 = [\"Miss\"]\ntest.Age = np.where(test.Title.isin(title_list2), test.Age.fillna(miss_16_mean), test.Age)\n\ntitle_list3 = [\"Mrs\", \"Mr\"]\ntest.Age = np.where(test.Title.isin(title_list3), test.Age.fillna(adults_mean), test.Age)\n\n# Fill one missing value in Fare column\ntest[\"Fare\"].fillna(test.Fare.mean(),inplace=True)\n\n# Create Person column with 3 categories (male, female, child)\ntest[\"Person\"] = test[[\"Age\",\"Sex\"]].apply(male_female_child, axis=1)\n\n# Let's drop Sex column\ntest = test.drop([\"Sex\"], axis=1)\n\n# Drop Ticket column\ntest = test.drop([\"Ticket\"], axis=1)","f52dd691":"test.head()","0777aa0a":"test_df_prep = test.copy()","7b7d31e1":"# Export transformed dataset into the file\n\n# test_df_prep.to_csv(\"..\/input\/titanic\/test_no_missing_values.csv\", index=False)","8ee676e6":"test_df_prep.head()","2fbebc85":"test_df_con = test_df_prep.apply(LabelEncoder().fit_transform)\ntest_df_con.head()","4dc67341":"test_df_con.head()","d926f281":"X_test = test_df_con.drop([\"PassengerId\"], axis=1).copy()","a00bdc6d":"X_train.shape, X_test.shape","110fd007":"# Instantiate Gradient Boost\nbest_gbc_model = GradientBoostingClassifier(max_features=\"sqrt\",\n                                            min_samples_split=8,\n                                            min_samples_leaf=1,\n                                            n_estimators=300,\n                                            random_state=45)\n# Fit data\nbest_gbc_model.fit(X_train, y_train)\n\n# Make predictions on test set\ngbc_predictions = best_gbc_model.predict(X_test)\n\n# Cross-validation for the model\ngbc_best_pred = model_selection.cross_val_predict(best_gbc_model,\n                                                  X_train,\n                                                  y_train,\n                                                  n_jobs=-1,\n                                                  cv=10)\n# Accuracy score for the model with best parameters\nbest_gbc_cv_acc = round(metrics.accuracy_score(y_train, gbc_best_pred)*100,2)","9fd7ed64":"# Import evaluating metrics\nfrom sklearn.metrics import confusion_matrix, classification_report","f61a7a7b":"# Check confusion metric\nprint(confusion_matrix(y_train, gbc_best_pred))\nprint(classification_report(y_train, gbc_best_pred))","ae60f556":"gbc_confusion = confusion_matrix(y_train, gbc_best_pred)\nsns.heatmap(gbc_confusion, annot=True, fmt=\"d\");","5380151d":"from sklearn.metrics import precision_score, recall_score, f1_score\n\nprint(\"Precision:\", round(precision_score(y_train, gbc_best_pred),2))\nprint(\"Recall:\", round(recall_score(y_train, gbc_best_pred),2))\nprint(\"F1-score: \", round(f1_score(y_train, gbc_best_pred),2))","bd8b1501":"from sklearn.metrics import precision_recall_curve\n\n# Get the probabilities of predictions\ngbc_y_scores = best_gbc_model.predict_proba(X_train)\ngbc_y_scores = gbc_y_scores[:,1]\n\ngbc_precision, gbc_recall, gbc_threshold = precision_recall_curve(y_train,\n                                                                  gbc_y_scores)\n\n# Create a plot\ndef plot_precision_recall(precision, recall, threshold):\n    # Plot for precision)\n    plt.plot(threshold, precision[:-1], \"r-\", label=\"precision\", linewidth=4)\n    # Plot for recall\n    plt.plot(threshold, recall[:-1], \"b\", label=\"recall\", linewidth=4)\n    # Description\n    plt.xlabel(\"threshold\", fontsize=16)\n    plt.legend(loc=\"upper right\", fontsize=16)\n    plt.ylim(0,1)\n\nplt.figure(figsize=(12,6))\nplot_precision_recall(gbc_precision, gbc_recall, gbc_threshold)\nplt.show()","6f8ef82c":"# Another way\ndef plot_precision_vs_recall(precision, recall):\n    plt.plot(precision, recall, \"g--\", linewidth=2.5)\n    plt.xlabel(\"precision\", fontsize=16)\n    plt.ylabel(\"recall\", fontsize=16)\n    plt.axis([0, 1.5, 0, 1.5])\n\nplt.figure(figsize=(14,6))\nplot_precision_vs_recall(gbc_precision, gbc_recall)\nplt.show()   ","b12beb5e":"from sklearn.metrics import roc_curve\n\n# Compute true positive rate and false positive rate\ngbc_false_positive_rate, gbc_true_positive_rate, thresholds = roc_curve(y_train,\n                                                                        gbc_y_scores)\n\n# Plot them against each other\ndef plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    # Create plot\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2,label=label)\n    # Plot diagonal line\n    plt.plot([0,1], [0,1], \"r\", linewidth=2)\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel(\"False Positive Rate (FPR)\", fontsize=14)\n    plt.ylabel(\"True Positive Rate (TPR)\", fontsize=14)\n    \nplt.figure(figsize=(14,6))\nplot_roc_curve(gbc_false_positive_rate, gbc_true_positive_rate)\nplt.show()","0e9f10f4":"from sklearn.metrics import roc_auc_score\n\ngbc_roc_score = roc_auc_score(y_train, gbc_y_scores)\nprint(\"ROC-AUC-Score: \", round(gbc_roc_score, 2),\"%\")","2929c485":"# Instantiate Random Forest with the best parameters found by GridSearchCV\nbest_rfc_model = RandomForestClassifier(max_features=\"log2\",\n                                        min_samples_leaf=2,\n                                        min_samples_split=2,\n                                        n_estimators=100,\n                                        random_state=45)\n# Fit model\nbest_rfc_model.fit(X_train, y_train)\n\n# Make predictions on test dataset\nrfc_predictions = best_rfc_model.predict(X_test)\n\n# Cross-validation for the model\nrfc_best_pred = model_selection.cross_val_predict(best_rfc_model,\n                                                  X_train,\n                                                  y_train,\n                                                  n_jobs=-1,\n                                                  verbose=0,\n                                                  cv=10)\n\n# Accuracy score for the model with best parameters\nbest_rfc_cv_acc = round(accuracy_score(y_train, rfc_best_pred)*100,2)","1e2f0027":"best_rfc_cv_acc","9de58fb5":"rfc_confusion = confusion_matrix(y_train, rfc_best_pred)\nsns.heatmap(rfc_confusion, annot=True, fmt=\"d\");","2f5a37cb":"print(confusion_matrix(y_train, rfc_best_pred))\nprint(classification_report(y_train, rfc_best_pred))","19009d03":"print(\"Precision: \", round(precision_score(y_train, rfc_best_pred),2))\nprint(\"Recall: \", round(recall_score(y_train, rfc_best_pred),2))\nprint(\"F1-score: \",round(f1_score(y_train, rfc_best_pred),2))","b0bc9fb3":"# Get the probabilities of predictions\nrfc_y_scores = best_rfc_model.predict_proba(X_train)\nrfc_y_scores = rfc_y_scores[:,1]\n\nrfc_precision, rfc_recall, rfc_thresholds = precision_recall_curve(y_train,\n                                                                   rfc_y_scores)\n# Create plot\nplt.figure(figsize=(12,6))\nplot_precision_recall(rfc_precision, rfc_recall, rfc_thresholds)\nplt.show()","93a6e0f7":"# Another way\nplt.figure(figsize=(14,6))\nplot_precision_vs_recall(rfc_precision, rfc_recall)\nplt.show()","60d0de23":"# Compute true positive rate and false positive rate\nrfc_false_positive_rate, rfc_true_positive_rate, thresholds = roc_curve(y_train,\n                                                                        rfc_y_scores)\n\n# Create plot\nplt.figure(figsize=(14,6))\nplot_roc_curve(rfc_false_positive_rate, rfc_true_positive_rate)\nplt.show()","36c2071a":"rfc_roc_score = roc_auc_score(y_train, rfc_y_scores)\nprint(\"ROC-AUC-Score: \", round(rfc_roc_score)*100,\"%\")","fd5b238d":"metrics = [\"Precision\", \"Recall\", \"F1\", \"AUC\"]\n\neval_metrics = catboost_model.eval_metrics(train_pool, \n                                           metrics=metrics,\n                                           plot=True,)\n\nfor metric in metrics:\n    print(str(metric)+ \": {}\".format(round(np.mean(eval_metrics[metric]),2)),\"%\")","db52703b":"# For Gradient Boost\nprint(\"Precision:\", round(precision_score(y_train, gbc_best_pred),2),\"%\")\nprint(\"Recall:\", round(recall_score(y_train, gbc_best_pred),2),\"%\")\nprint(\"F1-score: \", round(f1_score(y_train, gbc_best_pred),2),\"%\")\nprint(\"ROC-AUC-Score: \", round(gbc_roc_score, 2),\"%\")","258fc7ff":"# For Random Forest\nprint(\"Precision: \", round(precision_score(y_train, rfc_best_pred),2),\"%\")\nprint(\"Recall: \", round(recall_score(y_train, rfc_best_pred),2),\"%\")\nprint(\"F1-score: \",round(f1_score(y_train, rfc_best_pred),2),\"%\")\nprint(\"ROC-AUC-Score: \", round(rfc_roc_score)*100,\"%\")","4dcaeea5":"# Create a function to create plot\ndef feature_importance(model, df):\n    \"\"\"\n    Function show which features are most important in the model\n    \"\"\"\n    \n    feat_imp = pd.DataFrame({\"Importance\": model.feature_importances_,\n                             \"Column\":  df.columns})\n    feat_imp = feat_imp.sort_values([\"Importance\", \"Column\"], \n                                     ascending=[False, True],\n                                     ignore_index=True).iloc[-30:]\n    \n    plt.figure(figsize=(20,10))\n    sns.barplot(y=\"Column\", x=\"Importance\", data=feat_imp,\n                orient=\"horizontal\",palette=\"winter_r\")\n    \n    # plt.title(\"Gradient Boost Feature Importance for Titanic Project\")\n    # plt.savefig(\"Catboost_feature_importance.png\")\n    # plt.savefig(\"Gradient_Boost_imp_feature_importance.png\")\n    # plt.savefig(\"Random_Forest_feature_importance.png\")\n    return feat_imp ","9eced9f9":"feature_importance(catboost_model, X_train)","d41b89fd":"feature_importance(best_gbc_model, X_train)","ab4ebbdf":"feature_importance(best_rfc_model, X_train)","c60311a1":"X_train_col_d = X_train.drop([\"Embarked\", \"Parch\"], axis=1)\nX_test_col_d = X_test.drop([\"Embarked\", \"Parch\"], axis=1)","ce97272b":"#Instantiate Gradient Boost\ngbc_model = GradientBoostingClassifier(max_features=\"sqrt\",\n                                       min_samples_split=8,\n                                       min_samples_leaf=1,\n                                       n_estimators=300,\n                                       random_state=45)\n\n# Fit data\ngbc_model.fit(X_train_col_d, y_train)\n\n# Make predictions on test set\ngbc_pred = gbc_model.predict(X_test_col_d)\n\n# Cross-validation for the model\ngbc_pred_cv = model_selection.cross_val_predict(gbc_model,\n                                                X_train_col_d,\n                                                y_train,\n                                                n_jobs=-1,\n                                                cv=10)\n# Accuracy score for the model with best parameters\ngbc_cv_acc_d = round(accuracy_score(y_train, gbc_pred_cv)*100,2)\ngbc_cv_acc_d","cf571cce":"# Accuracy dataframe for the models with cv=10\ndf_models_4 =  pd.DataFrame({\n      \"Models\": [\"Logistic Regression\",\"KNN\",\"Naive Bayes\",\n                 \"Linear SVC\", \"Random Forest Best Param\",\"Decision Tree\",\n                 \"Gradient Boost Col Drop\",\"CatBoost\"],\n      \"Score\": [lr_cv_acc,\n                knn_cv_acc,\n                gnb_cv_acc,\n                svc_cv_acc,\n                best_rfc_cv_acc,\n                dtc_cv_acc,\n                gbc_cv_acc_d,\n                cat_cv_acc\n               ]})\n\nprint(\"***Cross Validation Models Accuracy***\")\ndf_models_4.sort_values(by=\"Score\", ascending=False, ignore_index=True)","736fbda9":"feature_importance(gbc_model, X_train_col_d)","b36a3394":"cat_feature = np.where(X_train_col_d.dtypes !=np.float)[0]\n\ntrain_pool = Pool(X_train_col_d, y_train, cat_feature)\n\n# Instatiate model\ncatboost_model = CatBoostClassifier(iterations=1000,\n                                    custom_loss=[\"Accuracy\"],\n                                    loss_function=\"Logloss\",\n                                    verbose=False)\n# Fit the model\ncatboost_model.fit(train_pool, plot=True)\n\n# Accuracy for catboost\ncat_acc = round(catboost_model.score(X_train_col_d, y_train)*100,2)\nprint(f\"Accuracy: {cat_acc}%\")","42941ad4":"cv_params = catboost_model.get_params()\n\n# Run Cross-validation for catboost\ncv_catboost = cv(train_pool, \n              cv_params,\n              fold_count=10,\n              plot=True,\n              verbose_eval=False)\n\n# Catboost result save later into dataframe \ncat_cv_acc_imp = round(np.max(cv_catboost[\"test-Accuracy-mean\"]) *100 ,2)","3f6fa948":"feature_importance(catboost_model, X_train_col_d)","5ae00065":"# Accuracy dataframe for the models with cv=10\ndf_models_5 =  pd.DataFrame({\n      \"Models\": [\"Logistic Regression\",\"KNN\",\"Naive Bayes\",\n                 \"Linear SVC\", \"Random Forest Best Param\",\"Decision Tree\",\n                 \"Gradient Boost Col Drop\",\"CatBoost Col Drop\"],\n      \"Score\": [lr_cv_acc,\n                knn_cv_acc,\n                gnb_cv_acc,\n                svc_cv_acc,\n                best_rfc_cv_acc,\n                dtc_cv_acc,\n                gbc_cv_acc_d,\n                cat_cv_acc_imp\n               ]})\n\nprint(\"***Cross Validation Models Accuracy***\")\ndf_models_5.sort_values(by=\"Score\", ascending=False, ignore_index=True)","15874bc6":"# Create submission for Kaggle\nsub = pd.DataFrame()\nsub[\"PassengerId\"] = passengerId\nsub[\"Survived\"] = gbc_pred\nsub[\"Survived\"] = sub[\"Survived\"].astype(int)","fed447b7":"sub","0e0c49ec":"# sub.to_csv(\"submission.csv\", index=False)","e7aa0786":"**Let's do it with GridSearchCV**","0e2e60c9":"**1. CatBoost**","19460a77":"## Let's split into X, y train","8b71d01e":"### Name column","6e2ab5aa":"# Finally I can evaluate my models with unseen data","16e7906e":"**2. Gradient Boost Classifier Feature Importance**","0a8ad271":"**1. Logistic Regression**","a16294d1":"* Let's replace missing values in Age column","fb9dc410":"**2. Check best parameters for Random Forest Classifier with RandomizedSearchCV**","ffa7fdc8":"### Age column\n\nLet's create some visualization","1d04a826":"# Prepare Test.csv in the same format as Train set","8df0fe59":">Fill missing values in Embarked column","2f3c4ecc":"## Let' do it for CatBoost","298843d9":"# 3. CatBoost metrics","5e352264":"**It is not obvious why Random Forest Classifier scored 100% with best parameters taken from GridSearchCV. Needs further investigation. But for now let's prepare test set and see how they perform with unknown data**","14316df0":"# Gradient Boost Classifier without \"Embarked\" and \"Parch\" column","da4f9ed1":"**5. ROC AUC Curve**","7d612a1d":" 1st = Upper 2nd = Middle 3rd = Lower","87a7fa7f":"## Check every column one by one in train dataframe","925a4d13":"#### Accuracy with best parameters for Gradient Boost found with GridSearchCV","fdc4b8e3":"### Pclass column","25e82fd4":"**4. Precision vs Recall**","27ec1c19":"**3. Random Forest Classifier Feature Importance**","f51bb15a":"**5. ROC AUC Curve**","d65a880c":"### Let's do it for cross-validation","3323782b":"# 2. Random Forest Classifier with Test dataset","ffc493bc":"* If Recall is low, it means there is higher amount of false negative (predict Did NOT Survived when actually survived)\n\n* When Precision is higher, it means less false positive( predict survived when actually not survived)\n\n* Fi-score is a harmonic mean of (Precision and Recall)\n\n* ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes.","836a5836":"#### Accuracy score for Random Forest found by GridSearch","dcf21699":"* Miss Amelle embarked from Southampton\n* Mrs. George Nelson embarked from Southampton","84c13887":"**4. Linear SVC**","a67fd47c":"**3. Precision Recall Curve**","31276585":"Now we've got this table we can create function to fill missing data in \"Age\" column with their median. \n\nAfter that we'll change \"Sex column\" into 3 categories:\n    * Female\n    * Male\n    * child","fb3858b3":"> Both Random Forest and Gradient Boost feature importance shows that column \"Embarked\" and \"Parch\" importance is low. We could ran the models again and see how it perform without this columns","2f5c8fe9":"**1. Find the best parameters for Gradient Boost Classifier with RandomizedSearchCV**","6fdfff81":"Both RandomizedSearchCV and GridSearchCV have the same improvements, though parameters have change. The randomized search and the grid search explore exactly the same space of parameters. The result in parameter settings is quite similar, while the run time for randomized search is drastically lower.\n","687cecd3":"   1. looks like we can class \"Rev\" title as male\n   2. Abbreviation \"Mlle\" in France means unmarried young women\n   3. Abbreviation \"Mme\" means madam(an elderly woman)\n   4. \"Jonkheer\"- means youn lord from Dutch so we class is as male\n   5. In \"Dr\" title there is one women - we need to change it\n   6. \"Master\" title means male boy between 0 and 12. There is 41 of them\n   7. In \"Miss\" column there is 36 girls under 16","c6a9aeef":"1. We'll train models with this column as y_train","70803bdb":"**2. Precision, Recall and F1-score**","75205fcd":"## Descriptive analysis on training set (EDA)","d17018c3":"### Sibsp  and Parch columns","190c9c51":"#### Age group less 16","4e57f9cb":"I'll also drop this column as it has no value for further analysis","47fc42d8":"## Checkpoint","8ccda0a3":"I'll drop this feature from further analysis","db8d798a":"## 1. Gradient Boost Evaluation with Test Dataset","584ad3f3":"* we can create another category in Sex column as child. There is 83 people under 16 but first we have to deal with missing values in Age column","81cb4fa8":"I'll create another column call \"Person\" from \"Sex\" column and make three categories:\n* male\n* femal\n* child","972cdf0d":"### Create dummies for test dataset","735a81c1":"**1. Confusion matrix and classification report for Random Forest**","081899b5":"## Even though Catboost is the best contender, let's play with it a little bit and optimize parameters for Random Forest Classifier and Gradient Boost Classifier\n\n* First I have to import RandomizedSearchCV, GridSearchCV to help me find best parameters for the models.\n* Second create dictionaries with parameters to choose for RandomizedSearchCV\n* Run RandomizedSearchCV with chosen models and parameters","74d45f16":"**Now, let's check how Random Forest will perform with best parameters found by GridSearchCV and cross-validation with 10-folds**","6924c54c":"**4. Precision vs Recall**","2d848ac8":"**1. Confusion matrix and Classification report**","f2bde23c":"# Catboost without \"Embarked\" and \"Parch\" columns","a96485bf":"### Fare column","35957ded":"# Predict whether passenger survived sinking of Titanic or not using machine learning models\n\n## 1. Definition\n\n> How well can we predict if a passenger survived the sinking of the Titanic or not.\n\n## 2. Data\n\nThe data is download from the Kaggle Titanic: Machine learning from disaster: https:\/\/www.kaggle.com\/c\/titanic\/overview\n\nThe data for this competition has been split into two groups:\n1. training set (train.csv) - the training set should be used to train the model\n2. test set (test.csv - should be used how well a model performes on unseen data.\n\n## 3. Evaluation\n> The evaluation metric for this competition is **accuracy**. The score is precentage of passangers the model correctly predict\n\n**Note:** The goal is to predict whether a passenger survived or not.\n\n## 4. Features\n\nKaggle provides dictionary explaining all of the features in the data. https:\/\/www.kaggle.com\/c\/titanic\/data\n\n#### Variable Notes\n\n**pclass:** A proxy for socio-economic status (SES)\n    1st = Upper\n    2nd = Middle\n    3rd = Lower\n\n**age:** Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n**sibsp:** The dataset defines family relations in this way...\n    Sibling = brother, sister, stepbrother, stepsister\n    Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n**parch:** The dataset defines family relations in this way...\n    Parent = mother, father\n    Child = daughter, son, stepdaughter, stepson\n    Some children travelled only with a nanny, therefore parch=0 for them.","dd0a0123":"### Sex column","cd6a909e":"**This imrovement is massive. I wonder if there is anything wrong. But for now I'll leave it unless someone who reads it give me some clues. Maybe when I test it with test data it'll give more explanation.**","b644c512":"**2. Precision, recall and F1-score**","3555daa5":"# Create a table which contains Accuracy scores for all machine learning models.","27517630":"### PassangerId column","9506f59d":"# Feature importance of the chosen models","ee2bb95f":"## Import models I am going to use\n1. Logistic Regression\n2. K-Nearest Neighbours\n3. Gaussian Naive Bayes\n4. Linear SVC\n5. Random Forests Classifier\n6. Decision Tree Classifier\n7. Gradient Boost Classifier\n8. CatBoost","defc33f6":"### Ticket column","e3429320":"**Check how the Gradient Boost Classifier will perform with best parameters and cross-validation with 10-folds**","0e2657b4":"## Compare all models where Random Forest Classifier and Gradient Boost are fitted with best parameters found by GridSearch","c9ed639a":"#### Accuracy score for Random Forest found by RandomizedSearchCV","0ac198cc":"## Create checkpoint","05d544db":"## Let's create some dummy variables for our models\n\nNow I have dataframe ready. We can encode the features so they're ready to be used for machine learning models.","25d3200e":"**5. Random Forest Classifier**","cb047aab":"**2. KNeighborsClassifier**","6da73a8d":"* Let's create new column \"Title\" with abbreviations and map it as Mr, Mrs, Miss","bfa1ff90":"### Survived column","da985326":"**Let's do it again with GridSearchCV**","93678af7":"### Checkpoint for Test dataset","0719bf3b":"**3. Precision Recall Curve**","121f821a":"As we finished with name column I'll remove it from our dataset","0d756330":"### Gender  \"Sex\" column","8b0ea337":"## Import relevant libraries and Titanic training and test datasets","03582046":"**7. Gradient Boost Classifier**","72155a1a":"> The procentage of missing data in column \"Cabin\" is 77.1%, therefore I will drop this column from further analysis.\n\n> Column \"Age\" I will leave and try to figure out how to fill missing values in this column","c8f0d887":"**3. Gaussian Naive Bayes**","29bb0aca":"### Embarked column","fbcd652c":"**6. Decision Tree Classifier**"}}