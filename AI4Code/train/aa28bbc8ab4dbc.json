{"cell_type":{"b15cf4d2":"code","c774e077":"code","434f19c8":"code","f59d82ba":"code","bce83097":"code","2a74a793":"code","4a2b663e":"code","f8dd6c88":"code","c8d78ff2":"code","90141671":"code","05164d3e":"code","86664488":"code","534f40c5":"code","f0636658":"code","ebb51ccf":"code","02d142a6":"code","96fe0f50":"code","e91fc982":"code","e30d491a":"code","dd652cd4":"code","29e79fd1":"code","1cd3474a":"code","51da80ae":"code","b66b213c":"code","2c9dcac4":"code","8fad60c0":"code","b4d54ce6":"code","03818b62":"code","98ff7548":"code","5efacfdb":"code","87c44fd4":"code","f6687093":"code","34843859":"code","23594062":"code","20e127c9":"code","3c848cdf":"code","8d7228f1":"code","52a3be91":"code","ca4683bf":"code","198dd4f9":"code","d928c4ef":"markdown","e08495dc":"markdown","caab746f":"markdown","f5e63dcb":"markdown","5164ebdc":"markdown","0c334299":"markdown","ea1e8113":"markdown","17e220e9":"markdown","f1953430":"markdown","1eeedb74":"markdown","adef1b8b":"markdown","2e1542be":"markdown","6db232c2":"markdown","fc2da1c0":"markdown","2d091a1b":"markdown","49623cd8":"markdown","f57f656e":"markdown","c8de7b2c":"markdown","f594763c":"markdown","41dcc498":"markdown"},"source":{"b15cf4d2":"#@title Import des librairies\nimport os\nimport glob\nimport shutil\n\n# Cr\u00e9ation d'un dossier\ndef create_repertory(rep):\n    \"\"\"\n    fonction de cr\u00e9ation de dossier\n    \"\"\"\n    try:\n        os.mkdir(rep)\n    except:\n        print('Le dossier est existant')\n\nKaggle = True\n\nif Kaggle == False:\n    from zipfile import ZipFile\n    from google.colab import drive\n    drive.mount('\/content\/drive')\n\n    # On liste les fichiers\n    path = \"\/content\/drive\/My Drive\/Colab Notebooks\/IML Projet 8\/\"\n    files = glob.glob(path + \"*.zip\")\n    # On supprime le dossier\n    shutil.rmtree('Data', True)\n    # On cr\u00e9\u00e9 les dossier de donn\u00e9es\n    create_repertory('Data')\n\n    # ouvrir les fichiers zip en mode lecture\n    for file in files:\n        with ZipFile(file, 'r') as zip: \n            # afficher tout le contenu du fichier zip\n            zip.printdir()\n            file = file.split(path)[1]\n            file = file.split('.zip')[0]\n            rep = 'Data\/' + file\n            # On supprime le dossier\n            shutil.rmtree(rep, True)\n            # On cr\u00e9\u00e9 les dossier de donn\u00e9es\n            create_repertory(rep)\n\n            # extraire tous les fichiers\n            print('extraction...') \n            zip.extractall(rep) \n            print('Termin\u00e9!')\n\n    files = glob.glob(path + \"*.csv\")\n    for file in files:\n        file = file.split(path)[1]\n        shutil.copy(path + file, 'Data\/' + file)\n    \n    PATH = 'Data\/'\n    os.listdir(PATH)\n\nelse:\n    PATH = '..\/input\/pku-autonomous-driving\/'\n    os.listdir(PATH)","c774e077":"!pip install efficientnet-pytorch","434f19c8":"import numpy as np\nimport pandas as pd\nimport cv2\nfrom tqdm import tqdm_notebook as tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom functools import reduce\nimport os\nfrom scipy.optimize import minimize\nimport plotly.express as px\nfrom math import sin, cos\nimport gc\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models\nfrom torchvision import transforms, utils\n\n\nfrom efficientnet_pytorch import EfficientNet\n\n","f59d82ba":"# Fonctions:\ndef imread(path, fast_mode=False):\n    img = cv2.imread(path)\n    if not fast_mode and img is not None and len(img.shape) == 3:\n        img = np.array(img[:, :, ::-1])\n    return img\n\ndef str2coords(s):\n    pred_string = s\n    items = pred_string.split(' ')\n    model_types, yaws, pitches, rolls, xs, ys, zs = [items[i::7] for i in range(7)]\n    liste = pd.DataFrame([ model_types, yaws, pitches, rolls, xs, ys, zs],\n                        index=['model_type', 'yaw', 'pitche', 'roll', 'x', 'y', 'z']).T\n    coords = []\n    for i in range(liste.shape[0]):\n        coords.append({'id': float(liste['model_type'][i]),\n                    'yaw': float(liste['yaw'][i]),\n                    'pitch': float(liste['pitche'][i]),\n                    'roll': float(liste['roll'][i]),\n                    'x': float(liste['x'][i]),\n                    'y': float(liste['y'][i]),\n                    'z': float(liste['z'][i])\n                    })\n    return coords\n\ndef rotate(x, angle):\n    x = x + angle\n    x = x - (x + np.pi) \/\/ (2 * np.pi) * 2 * np.pi\n    return x\n\ndef get_img_coords(s):\n    '''\n    Input is a PredictionString (e.g. from train dataframe)\n    Output is two arrays:\n        xs: x coordinates in the image\n        ys: y coordinates in the image\n    '''\n    coords = str2coords(s)\n    xs = [float(c['x']) for c in coords]\n    ys = [float(c['y']) for c in coords]\n    zs = [float(c['z']) for c in coords]\n    position = []\n    for i in range(len(xs)):\n        position.append([xs[i], ys[i], zs[i]])\n    P = np.array(position).T\n    img_p = np.dot(camera_matrix, P).T\n    img_p[:, 0] \/= img_p[:, 2]\n    img_p[:, 1] \/= img_p[:, 2]\n    img_xs = img_p[:, 0]\n    img_ys = img_p[:, 1]\n    img_zs = img_p[:, 2] # z = Distance from the camera\n    return img_xs, img_ys\n\n# convert euler angle to rotation matrix\ndef euler_to_Rot(yaw, pitch, roll):\n    Y = np.array([[cos(yaw), 0, sin(yaw)],\n                  [0, 1, 0],\n                  [-sin(yaw), 0, cos(yaw)]])\n    P = np.array([[1, 0, 0],\n                  [0, cos(pitch), -sin(pitch)],\n                  [0, sin(pitch), cos(pitch)]])\n    R = np.array([[cos(roll), -sin(roll), 0],\n                  [sin(roll), cos(roll), 0],\n                  [0, 0, 1]])\n    return np.dot(Y, np.dot(P, R))\n\ndef draw_line(image, points):\n    color = (255, 0, 0)\n    cv2.line(image, tuple(points[0][:2]), tuple(points[3][:2]), color, 16)\n    cv2.line(image, tuple(points[0][:2]), tuple(points[1][:2]), color, 16)\n    cv2.line(image, tuple(points[1][:2]), tuple(points[2][:2]), color, 16)\n    cv2.line(image, tuple(points[2][:2]), tuple(points[3][:2]), color, 16)\n    return image\n\ndef draw_points(image, points):\n    for (p_x, p_y, p_z) in points:\n        cv2.circle(image, (p_x, p_y), int(1000 \/ p_z), (0, 255, 0), -1)\n    return image\n\ndef visualize(img, coords):\n    x_l = 1.02\n    y_l = 0.80\n    z_l = 2.31\n    \n    img = img.copy()\n    for point in coords:\n        # Get values\n        x, y, z = point['x'], point['y'], point['z']\n        yaw, pitch, roll = -point['pitch'], -point['yaw'], -point['roll']\n        # Math\n        Rt = np.eye(4)\n        t = np.array([x, y, z])\n        Rt[:3, 3] = t\n        Rt[:3, :3] = euler_to_Rot(yaw, pitch, roll).T\n        Rt = Rt[:3, :]\n        P = np.array([[x_l, -y_l, -z_l, 1],\n                      [x_l, -y_l, z_l, 1],\n                      [-x_l, -y_l, z_l, 1],\n                      [-x_l, -y_l, -z_l, 1],\n                      [0, 0, 0, 1]]).T\n        img_cor_points = np.dot(camera_matrix, np.dot(Rt, P))\n        img_cor_points = img_cor_points.T\n        img_cor_points[:, 0] \/= img_cor_points[:, 2]\n        img_cor_points[:, 1] \/= img_cor_points[:, 2]\n        img_cor_points = img_cor_points.astype(int)\n        # Drawing\n        img = draw_line(img, img_cor_points)\n        img = draw_points(img, img_cor_points[-1:])\n    \n    return img\n\ndef _regr_preprocess(regr_dict, flip=False):\n    if flip:\n        for k in ['x', 'pitch', 'roll']:\n            regr_dict[k] = -regr_dict[k]\n    for name in ['x', 'y', 'z']:\n        regr_dict[name] = regr_dict[name] \/ 100\n    regr_dict['roll'] = rotate(regr_dict['roll'], np.pi)\n    regr_dict['pitch_sin'] = sin(regr_dict['pitch'])\n    regr_dict['pitch_cos'] = cos(regr_dict['pitch'])\n    regr_dict.pop('pitch')\n    regr_dict.pop('id')\n    return regr_dict\n\ndef _regr_back(regr_dict):\n    for name in ['x', 'y', 'z']:\n        regr_dict[name] = regr_dict[name] * 100\n    regr_dict['roll'] = rotate(regr_dict['roll'], -np.pi)\n    \n    pitch_sin = regr_dict['pitch_sin'] \/ np.sqrt(regr_dict['pitch_sin']**2 + regr_dict['pitch_cos']**2)\n    pitch_cos = regr_dict['pitch_cos'] \/ np.sqrt(regr_dict['pitch_sin']**2 + regr_dict['pitch_cos']**2)\n    regr_dict['pitch'] = np.arccos(pitch_cos) * np.sign(pitch_sin)\n    return regr_dict\n\ndef preprocess_image(img, flip=False):\n    img = img[img.shape[0] \/\/ 2:]\n    bg = np.ones_like(img) * img.mean(1, keepdims=True).astype(img.dtype)\n    bg = bg[:, :img.shape[1] \/\/ 6]\n    img = np.concatenate([bg, img, bg], 1)\n    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n    if flip:\n        img = img[:,::-1]\n    return (img \/ 255).astype('float32')\n\ndef get_mask_and_regr(img, labels, flip=False):\n    mask = np.zeros([IMG_HEIGHT \/\/ MODEL_SCALE, IMG_WIDTH \/\/ MODEL_SCALE], dtype='float32')\n    regr_names = ['x', 'y', 'z', 'yaw', 'pitch', 'roll']\n    regr = np.zeros([IMG_HEIGHT \/\/ MODEL_SCALE, IMG_WIDTH \/\/ MODEL_SCALE, 7], dtype='float32')\n    coords = str2coords(labels)\n    xs, ys = get_img_coords(labels)\n    position = []\n    for i in range(len(xs)):\n        position.append([xs[i], ys[i], coords[i]])\n    for x, y, regr_dict in position:\n        x, y = y, x\n        x = (x - img.shape[0] \/\/ 2) * IMG_HEIGHT \/ (img.shape[0] \/\/ 2) \/ MODEL_SCALE\n        x = np.round(x).astype('int')\n        y = (y + img.shape[1] \/\/ 6) * IMG_WIDTH \/ (img.shape[1] * 4\/3) \/ MODEL_SCALE\n        y = np.round(y).astype('int')\n        if x >= 0 and x < IMG_HEIGHT \/\/ MODEL_SCALE and y >= 0 and y < IMG_WIDTH \/\/ MODEL_SCALE:\n            mask[x, y] = 1\n            regr_dict = _regr_preprocess(regr_dict, flip)\n            regr[x, y] = [regr_dict[n] for n in sorted(regr_dict)]\n    if flip:\n        mask = np.array(mask[:,::-1])\n        regr = np.array(regr[:,::-1])\n    return mask, regr\n\ndef convert_3d_to_2d(x, y, z, fx = 2304.5479, fy = 2305.8757, cx = 1686.2379, cy = 1354.9849):\n    # stolen from https:\/\/www.kaggle.com\/theshockwaverider\/eda-visualization-baseline\n    return x * fx \/ z + cx, y * fy \/ z + cy\n\ndef optimize_xy(r, c, x0, y0, z0, flipped=False):\n    def distance_fn(xyz):\n        x, y, z = xyz\n        xx = -x if flipped else x\n        slope_err = (xzy_slope.predict([[xx,z]])[0] - y)**2\n        x, y = convert_3d_to_2d(x, y, z)\n        y, x = x, y\n        x = (x - IMG_SHAPE[0] \/\/ 2) * IMG_HEIGHT \/ (IMG_SHAPE[0] \/\/ 2) \/ MODEL_SCALE\n        y = (y + IMG_SHAPE[1] \/\/ 6) * IMG_WIDTH \/ (IMG_SHAPE[1] * 4 \/ 3) \/ MODEL_SCALE\n        return max(0.2, (x-r)**2 + (y-c)**2) + max(0.4, slope_err)\n    \n    res = minimize(distance_fn, [x0, y0, z0], method='Powell')\n    x_new, y_new, z_new = res.x\n    return x_new, y_new, z_new\n\ndef clear_duplicates(coords):\n    for c1 in coords:\n        xyz1 = np.array([c1['x'], c1['y'], c1['z']])\n        for c2 in coords:\n            xyz2 = np.array([c2['x'], c2['y'], c2['z']])\n            distance = np.sqrt(((xyz1 - xyz2)**2).sum())\n            if distance < DISTANCE_THRESH_CLEAR:\n                if c1['confidence'] < c2['confidence']:\n                    c1['confidence'] = -1\n    return [c for c in coords if c['confidence'] > 0]\n\ndef extract_coords(prediction, flipped=False):\n    logits = prediction[0]\n    regr_output = prediction[1:]\n    points = np.argwhere(logits > 0)\n    col_names = sorted(['x', 'y', 'z', 'yaw', 'pitch_sin', 'pitch_cos', 'roll'])\n    coords = []\n    for r, c in points:\n        position = []\n        for i in range(len(col_names)):\n            position.append([col_names[i], regr_output[:, r, c][i]])\n        regr_dict = dict(position)\n        coords.append(_regr_back(regr_dict))\n        coords[-1]['confidence'] = 1 \/ (1 + np.exp(-logits[r, c]))\n        coords[-1]['x'], coords[-1]['y'], coords[-1]['z'] = \\\n                optimize_xy(r, c,\n                            coords[-1]['x'],\n                            coords[-1]['y'],\n                            coords[-1]['z'], flipped)\n    coords = clear_duplicates(coords)\n    return coords\n\ndef coords2str(coords, names=['yaw', 'pitch', 'roll', 'x', 'y', 'z', 'confidence']):\n    s = []\n    for c in coords:\n        for n in names:\n            s.append(str(c.get(n, 0)))\n    return ' '.join(s)","bce83097":"train = pd.read_csv(PATH + 'train.csv')\ntest = pd.read_csv(PATH + 'sample_submission.csv')\ntrain.head()","2a74a793":"# From camera.zip\ncamera_matrix = np.array([[2304.5479, 0,  1686.2379],\n                          [0, 2305.8757, 1354.9849],\n                          [0, 0, 1]], dtype=np.float32)\ncamera_matrix_inv = np.linalg.inv(camera_matrix)","4a2b663e":"inp = train['PredictionString'][0]\nprint('Example input:\\n', inp)\nprint('Output:\\n', str2coords(inp))","f8dd6c88":"points_df = pd.DataFrame()\nfor col in ['x', 'y', 'z', 'yaw', 'pitch', 'roll']:\n    arr = []\n    for ps in train['PredictionString']:\n        coords = str2coords(ps)\n        arr += [c[col] for c in coords]\n    points_df[col] = arr\n\nprint('len(points_df)', len(points_df))\npoints_df.head()","c8d78ff2":"lens = [len(str2coords(s)) for s in train['PredictionString']]\n\nplt.figure(figsize=(15,5))\nsns.countplot(lens);\nplt.xlabel('Number of cars in image')\nplt.title('Number of cars per image')\nplt.show()","90141671":"plt.figure(figsize=(15,5))\nsns.distplot(points_df['x'], bins=200);\nplt.xlabel('x')\nplt.title('X distribution')\nplt.show()","05164d3e":"plt.figure(figsize=(15,5))\nsns.distplot(points_df['y'], bins=200);\nplt.xlabel('y')\nplt.title('Y distribution')\nplt.show()","86664488":"plt.figure(figsize=(15,5))\nsns.distplot(points_df['z'], bins=200);\nplt.xlabel('z')\nplt.title('Z distribution')\nplt.show()","534f40c5":"plt.figure(figsize=(15,5))\nsns.distplot(points_df['yaw'], bins=200);\nplt.xlabel('yaw')\nplt.title('Yaw distribution')\nplt.show()","f0636658":"plt.figure(figsize=(15,5))\nsns.distplot(points_df['pitch'], bins=200);\nplt.xlabel('pitch')\nplt.title('Pitch distribution')\nplt.show()","ebb51ccf":"plt.figure(figsize=(15,5))\nsns.distplot(points_df['roll'].map(lambda x: rotate(x, np.pi)), bins=200);\nplt.xlabel('roll rotated by pi')\nplt.title('Roll distribution')\nplt.show()","02d142a6":"# calculate the correlation matrix\ncorr = points_df.corr()\n\n# plot the heatmap\nplt.figure(figsize=(15,15))\nsns.heatmap(corr, annot=True,\n            xticklabels=corr.columns,\n            yticklabels=corr.columns)\nplt.title('Correlation matrix')\nplt.show()","96fe0f50":"# We define simple linear regression with z and y:\nzy_slope = LinearRegression()\nX = points_df[['z']]\ny = points_df['y']\nzy_slope.fit(X, y)\nprint('MAE without x:', mean_absolute_error(y, zy_slope.predict(X)))\n\n# with x, y and z\nxzy_slope = LinearRegression()\nX = points_df[['x', 'z']]\ny = points_df['y']\nxzy_slope.fit(X, y)\nprint('MAE with x:', mean_absolute_error(y, xzy_slope.predict(X)))\n\nprint('\\ndy\/dx = {:.3f}\\ndy\/dz = {:.3f}'.format(*xzy_slope.coef_))","e91fc982":"plt.figure(figsize=(25,5))\nplt.xlim(0,500)\nplt.ylim(0,100)\nplt.scatter(points_df['z'], points_df['y'], label='Real points')\nX_line = np.linspace(0,500, 10)\nplt.plot(X_line, zy_slope.predict(X_line.reshape(-1, 1)), color='orange', label='Regression')\nplt.legend()\nplt.xlabel('z coordinate')\nplt.ylabel('y coordinate')\nplt.title('Correlation z and y')\nplt.show()","e30d491a":"name = train['ImageId'][0]\nimg = imread(PATH + 'train_images\/' + name + '.jpg')\nIMG_SHAPE = img.shape\nplt.imshow(img)","dd652cd4":"plt.figure(figsize=(15, 15))\nplt.imshow(imread(PATH + 'train_images\/' + train['ImageId'][2217] + '.jpg'))\nplt.scatter(*get_img_coords(train['PredictionString'][2217]), color='red', s=100)\nplt.title('Sample center')","29e79fd1":"xs, ys = [], []\n\nfor ps in train['PredictionString']:\n    x, y = get_img_coords(ps)\n    xs += list(x)\n    ys += list(y)\n\nplt.figure(figsize=(15, 15))\nplt.imshow(imread(PATH + 'train_images\/' + train['ImageId'][2217] + '.jpg'), alpha=0.3)\nplt.scatter(xs, ys, color='red', s=10, alpha=0.2)\nplt.title('All centers distribution')\nplt.show()","1cd3474a":"# Road points\nroad_width = 3\nroad_xs = [-road_width, road_width, road_width, -road_width, -road_width]\nroad_ys = [0, 0, 500, 500, 0]\n\nplt.figure(figsize=(16,16))\nplt.axes().set_aspect(1)\nplt.xlim(-50, 50)\nplt.ylim(0, 100)\n\n# View road\nplt.fill(road_xs, road_ys, alpha=0.3, color='gray')\nplt.plot([road_width\/2,road_width\/2], [0,100], alpha=0.4, linewidth=4, color='white', ls='--')\nplt.plot([-road_width\/2,-road_width\/2], [0,100], alpha=0.4, linewidth=4, color='white', ls='--')\n# View cars\nplt.scatter(points_df['x'], np.sqrt(points_df['z']**2 + points_df['y']**2), color='red', s=10, alpha=0.1);","51da80ae":"n_rows = 10\n\nfor idx in range(n_rows):\n    fig, axes = plt.subplots(1, 2, figsize=(20,20))\n    img = imread(PATH + 'train_images\/' + train['ImageId'].iloc[idx] + '.jpg')\n    axes[0].imshow(img)\n    img_vis = visualize(img, str2coords(train['PredictionString'].iloc[idx]))\n    axes[1].imshow(img_vis)\n    plt.show()","b66b213c":"IMG_WIDTH = 512\nIMG_HEIGHT = IMG_WIDTH \/\/ 16 * 5\nMODEL_SCALE = 8","2c9dcac4":"img0 = imread(PATH + 'train_images\/' + train['ImageId'][0] + '.jpg')\nimg = preprocess_image(img0)\n\nmask, regr = get_mask_and_regr(img0, train['PredictionString'][0])\n\nprint('img.shape', img.shape, 'std:', np.std(img))\nprint('mask.shape', mask.shape, 'std:', np.std(mask))\nprint('regr.shape', regr.shape, 'std:', np.std(regr))\n\nplt.figure(figsize=(15, 15))\nplt.title('Processed image')\nplt.imshow(img)\nplt.show()\n\nplt.figure(figsize=(15, 15))\nplt.title('Detection Mask')\nplt.imshow(mask)\nplt.show()\n\nplt.figure(figsize=(15, 15))\nplt.title('id values')\nplt.imshow(regr[:,:,0])\nplt.show()\nt = 0\nfor i in range(len(regr[:,:,t])):\n    for j in range(len(regr[:,:,t][i])):\n        if regr[:,:,t][i][j] != 0:\n            print(regr[:,:,t][i][j])\n\nplt.figure(figsize=(15, 15))\nplt.title('pitch values')\nplt.imshow(regr[:,:,1])\nplt.show()\nt = 1\nfor i in range(len(regr[:,:,t])):\n    for j in range(len(regr[:,:,t][i])):\n        if regr[:,:,t][i][j] != 0:\n            print(regr[:,:,t][i][j])\n\nplt.figure(figsize=(15, 15))\nplt.title('roll values')\nplt.imshow(regr[:,:,2])\nplt.show()\nt = 2\nfor i in range(len(regr[:,:,t])):\n    for j in range(len(regr[:,:,t][i])):\n        if regr[:,:,t][i][j] != 0:\n            print(regr[:,:,t][i][j])\n\nplt.figure(figsize=(15, 15))\nplt.title('x values')\nplt.imshow(regr[:,:,3])\nplt.show()\nt = 3\nfor i in range(len(regr[:,:,t])):\n    for j in range(len(regr[:,:,t][i])):\n        if regr[:,:,t][i][j] != 0:\n            print(regr[:,:,t][i][j])\n\nplt.figure(figsize=(15, 15))\nplt.title('y values')\nplt.imshow(regr[:,:,4])\nplt.show()\nt = 4\nfor i in range(len(regr[:,:,t])):\n    for j in range(len(regr[:,:,t][i])):\n        if regr[:,:,t][i][j] != 0:\n            print(regr[:,:,t][i][j])\n\nplt.figure(figsize=(15, 15))\nplt.title('Yaw values')\nplt.imshow(regr[:,:,5])\nplt.show()\nt = 5\nfor i in range(len(regr[:,:,t])):\n    for j in range(len(regr[:,:,t][i])):\n        if regr[:,:,t][i][j] != 0:\n            print(regr[:,:,t][i][j])\n\nplt.figure(figsize=(15, 15))\nplt.title('z values')\nplt.imshow(regr[:,:,6])\nplt.show()\nt = 6\nfor i in range(len(regr[:,:,t])):\n    for j in range(len(regr[:,:,t][i])):\n        if regr[:,:,t][i][j] != 0:\n            print(regr[:,:,t][i][j])","8fad60c0":"class CarDataset(Dataset):\n    \"\"\"Car dataset.\"\"\"\n\n    def __init__(self, dataframe, root_dir, training=True, transform=None, ID=False):\n        self.df = dataframe\n        self.root_dir = root_dir\n        self.transform = transform\n        self.training = training\n        self.ID=ID\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        \n        # Get image name\n        idx, labels = self.df.values[idx]\n        img_name = self.root_dir.format(idx)\n        \n        # Augmentation\n        flip = False\n        if self.training:\n            flip = np.random.randint(10) == 1\n        \n        # Read image\n        img0 = imread(img_name, True)\n        img = preprocess_image(img0, flip=flip)\n        img = np.rollaxis(img, 2, 0)\n        \n        # Get mask and regression maps\n        mask, regr = get_mask_and_regr(img0, labels, flip=flip)\n        regr = np.rollaxis(regr, 2, 0)\n        if self.ID:\n            return [idx,img]\n        return [img, mask, regr]","b4d54ce6":"DISTANCE_THRESH_CLEAR = 2\nBATCH_SIZE = 2\n\ntrain_images_dir = PATH + 'train_images\/{}.jpg'\ntest_images_dir = PATH + 'test_images\/{}.jpg'\n\n# we select a part of \npart_1, part_2 = train_test_split(train, test_size=0.5, random_state=42)\ndf_train, df_test = train_test_split(part_1, test_size=0.2, random_state=42)\ndf_val = test\n\n# we create dataset objects\ntrain_dataset = CarDataset(df_train, train_images_dir, training=True)\ntest_dataset = CarDataset(df_test, train_images_dir, training=False)\ntest_dataset2 = CarDataset(df_test, train_images_dir, training=False, ID=True)\nval_dataset = CarDataset(df_val, test_images_dir, training=False)\n\n# we create data generators - they will produce batches\ntrain_loader = DataLoader(dataset=train_dataset,\n                          batch_size=BATCH_SIZE,\n                          shuffle=False,\n                          num_workers=2)\ntest_loader = DataLoader(dataset=test_dataset,\n                         batch_size=BATCH_SIZE,\n                         shuffle=False,\n                         num_workers=2)\ntest_loader2 = DataLoader(dataset=test_dataset2,\n                         batch_size=BATCH_SIZE,\n                         shuffle=False,\n                         num_workers=2)\nval_loader = DataLoader(dataset=val_dataset,\n                        batch_size=BATCH_SIZE,\n                        shuffle=False,\n                        num_workers=2)","03818b62":"img, mask, regr = train_dataset[0]\n\nplt.figure(figsize=(15, 15))\nplt.imshow(np.rollaxis(img, 0, 3))\nplt.show()\n\nplt.figure(figsize=(15, 15))\nplt.imshow(mask)\nplt.show()\n\nplt.figure(figsize=(15, 15))\nplt.title('id values')\nplt.imshow(regr[0])\nplt.show()\nt = 0\nfor i in range(len(regr[t])):\n    for j in range(len(regr[t][i])):\n        if regr[t][i][j] != 0:\n            print(regr[t][i][j])\n\nplt.figure(figsize=(15, 15))\nplt.title('pitch values')\nplt.imshow(regr[1])\nplt.show()\nt = 1\nfor i in range(len(regr[t])):\n    for j in range(len(regr[t][i])):\n        if regr[t][i][j] != 0:\n            print(regr[t][i][j])\n\nplt.figure(figsize=(15, 15))\nplt.title('roll values')\nplt.imshow(regr[2])\nplt.show()\nt = 2\nfor i in range(len(regr[t])):\n    for j in range(len(regr[t][i])):\n        if regr[t][i][j] != 0:\n            print(regr[t][i][j])\n\nplt.figure(figsize=(15, 15))\nplt.title('x values')\nplt.imshow(regr[3])\nplt.show()\nt = 3\nfor i in range(len(regr[t])):\n    for j in range(len(regr[t][i])):\n        if regr[t][i][j] != 0:\n            print(regr[t][i][j])\n\nplt.figure(figsize=(15, 15))\nplt.title('y values')\nplt.imshow(regr[4])\nplt.show()\nt = 4\nfor i in range(len(regr[t])):\n    for j in range(len(regr[t][i])):\n        if regr[t][i][j] != 0:\n            print(regr[t][i][j])\n\nplt.figure(figsize=(15, 15))\nplt.title('Yaw values')\nplt.imshow(regr[5])\nplt.show()\nt = 5\nfor i in range(len(regr[t])):\n    for j in range(len(regr[t][i])):\n        if regr[t][i][j] != 0:\n            print(regr[t][i][j])\n\nplt.figure(figsize=(15, 15))\nplt.title('z values')\nplt.imshow(regr[6])\nplt.show()\nt = 6\nfor i in range(len(regr[t])):\n    for j in range(len(regr[t][i])):\n        if regr[t][i][j] != 0:\n            print(regr[t][i][j])","98ff7548":"class double_conv(nn.Module):\n    '''(conv => BN => ReLU) * 2'''\n    def __init__(self, in_ch, out_ch):\n        super(double_conv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n            nn.BatchNorm2d(out_ch),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\nclass up(nn.Module):\n    def __init__(self, in_ch, out_ch, bilinear=True):\n        super(up, self).__init__()\n\n        #  would be a nice idea if the upsampling could be learned too,\n        #  but my machine do not have enough memory to handle all those weights\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n        else:\n            self.up = nn.ConvTranspose2d(in_ch\/\/2, in_ch\/\/2, 2, stride=2)\n\n        self.conv = double_conv(in_ch, out_ch)\n\n    def forward(self, x1, x2=None):\n        x1 = self.up(x1)\n        \n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, (diffX \/\/ 2, diffX - diffX\/\/2,\n                        diffY \/\/ 2, diffY - diffY\/\/2))\n        \n        # for padding issues, see \n        # https:\/\/github.com\/HaiyongJiang\/U-Net-Pytorch-Unstructured-Buggy\/commit\/0e854509c2cea854e247a9c615f175f76fbb2e3a\n        # https:\/\/github.com\/xiaopeng-liao\/Pytorch-UNet\/commit\/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n        \n        if x2 is not None:\n            x = torch.cat([x2, x1], dim=1)\n        else:\n            x = x1\n        x = self.conv(x)\n        return x\n\ndef get_mesh(batch_size, shape_x, shape_y):\n    mg_x, mg_y = np.meshgrid(np.linspace(0, 1, shape_y), np.linspace(0, 1, shape_x))\n    mg_x = np.tile(mg_x[None, None, :, :], [batch_size, 1, 1, 1]).astype('float32')\n    mg_y = np.tile(mg_y[None, None, :, :], [batch_size, 1, 1, 1]).astype('float32')\n    mesh = torch.cat([torch.tensor(mg_x).to(device), torch.tensor(mg_y).to(device)], 1)\n    return mesh","5efacfdb":"class MyUNet(nn.Module):\n    '''Mixture of previous classes'''\n    def __init__(self, n_classes):\n        super(MyUNet, self).__init__()\n        self.base_model = EfficientNet.from_pretrained('efficientnet-b0')\n        \n        self.conv0 = double_conv(5, 64)\n        self.conv1 = double_conv(64, 128)\n        self.conv2 = double_conv(128, 512)\n        self.conv3 = double_conv(512, 1024)\n        \n        self.mp = nn.MaxPool2d(2)\n        \n        self.up1 = up(1282 + 1024, 512)\n        self.up2 = up(512 + 512, 256)\n        self.outc = nn.Conv2d(256, n_classes, 1)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        mesh1 = get_mesh(batch_size, x.shape[2], x.shape[3])\n        x0 = torch.cat([x, mesh1], 1)\n        x1 = self.mp(self.conv0(x0))\n        x2 = self.mp(self.conv1(x1))\n        x3 = self.mp(self.conv2(x2))\n        x4 = self.mp(self.conv3(x3))\n        \n        x_center = x[:, :, :, IMG_WIDTH \/\/ 8: -IMG_WIDTH \/\/ 8]\n        feats = self.base_model.extract_features(x_center)\n        bg = torch.zeros([feats.shape[0], feats.shape[1], feats.shape[2], feats.shape[3] \/\/ 8]).to(device)\n        feats = torch.cat([bg, feats, bg], 3)\n        \n        # Add positional info\n        mesh2 = get_mesh(batch_size, feats.shape[2], feats.shape[3])\n        feats = torch.cat([feats, mesh2], 1)\n        \n        x = self.up1(feats, x4)\n        x = self.up2(x, x3)\n        x = self.outc(x)\n        return x","87c44fd4":"def criterion(prediction, mask, regr, size_average=True):\n    # Binary mask loss\n    pred_mask = torch.sigmoid(prediction[:, 0])\n    mask_loss = mask * torch.log(pred_mask + 1e-12) + (1 - mask) * torch.log(1 - pred_mask + 1e-12)\n    mask_loss = -mask_loss.mean(0).sum()\n    \n    # Regression L1 loss\n    pred_regr = prediction[:, 1:]\n    regr_loss = (torch.abs(pred_regr - regr).sum(1) * mask).sum(1).sum(1) \/ mask.sum(1).sum(1)\n    regr_loss = regr_loss.mean(0)\n    \n    # Sum\n    loss = mask_loss + regr_loss\n    if not size_average:\n        loss *= prediction.shape[0]\n    return loss","f6687093":"#copy from https:\/\/www.kaggle.com\/its7171\/metrics-evaluation-script\nfrom math import sqrt, acos, pi, sin, cos\nfrom scipy.spatial.transform import Rotation as R\nfrom sklearn.metrics import average_precision_score\ndef expand_df(df, PredictionStringCols):\n    df = df.dropna().copy()\n    df['NumCars'] = [int((x.count(' ')+1)\/7) for x in df['PredictionString']]\n    #fix nan bug\n    df['PredictionString'].fillna('0.15699866 0.029135812202187722 -3.0876481888168534 15.061413706416168 4.49235176722995 27.384573221206665 0.6447297909095615',inplace=True)\n    position = []\n    for i in range(len(df['ImageId'])):\n        position.append([list(df['ImageId'])[i], list(df['NumCars'])[i]])\n    image_id_expanded = [item for item, count in position for i in range(count)]\n    prediction_strings_expanded = df['PredictionString'].str.split(' ',expand = True).values.reshape(-1,7).astype(float)\n    prediction_strings_expanded = prediction_strings_expanded[~np.isnan(prediction_strings_expanded).all(axis=1)]\n    df = pd.DataFrame(\n        {\n            'ImageId': image_id_expanded,\n            PredictionStringCols[0]:prediction_strings_expanded[:,0],\n            PredictionStringCols[1]:prediction_strings_expanded[:,1],\n            PredictionStringCols[2]:prediction_strings_expanded[:,2],\n            PredictionStringCols[3]:prediction_strings_expanded[:,3],\n            PredictionStringCols[4]:prediction_strings_expanded[:,4],\n            PredictionStringCols[5]:prediction_strings_expanded[:,5],\n            PredictionStringCols[6]:prediction_strings_expanded[:,6]\n        })\n    return df\n\ndef str2coords2(s, names):\n    coords = []\n    for l in np.array(s.split()).reshape([-1, 7]):\n        position = []\n        for i in range(len(names)):\n            position.append([names[i], l.astype('float')[i]])\n        coords.append(dict(position))\n    return coords\n\ndef TranslationDistance(p,g, abs_dist = False):\n    dx = p['x'] - g['x']\n    dy = p['y'] - g['y']\n    dz = p['z'] - g['z']\n    diff0 = (g['x']**2 + g['y']**2 + g['z']**2)**0.5\n    diff1 = (dx**2 + dy**2 + dz**2)**0.5\n    if abs_dist:\n        diff = diff1\n    else:\n        diff = diff1\/diff0\n    return diff\n\ndef RotationDistance(p, g):\n    true=[ g['pitch'] ,g['yaw'] ,g['roll'] ]\n    pred=[ p['pitch'] ,p['yaw'] ,p['roll'] ]\n    q1 = R.from_euler('xyz', true)\n    q2 = R.from_euler('xyz', pred)\n    diff = R.inv(q2) * q1\n    W = np.clip(diff.as_quat()[-1], -1., 1.)\n    \n    # in the official metrics code:\n    # https:\/\/www.kaggle.com\/c\/pku-autonomous-driving\/overview\/evaluation\n    #   return Object3D.RadianToDegree( Math.Acos(diff.W) )\n    # this code treat \u03b8 and \u03b8+2\u03c0 differntly.\n    # So this should be fixed as follows.\n    W = (acos(W)*360)\/pi\n    if W > 180:\n        W = 180 - W\n    return W\n\ndef check_match(valid_df, train_df, thre_tr_dist, thre_ro_dist, keep_gt=False):\n    position = []\n    for i in range(len(train_df['ImageId'])):\n        position.append([list(train_df['ImageId'])[i], list(train_df['PredictionString'])[i]])\n    train_dict = {imgID:str2coords2(s, names=['carid_or_score', 'pitch', 'yaw', 'roll', 'x', 'y', 'z']) for imgID,s in position}\n    position = []\n    for i in range(len(valid_df['ImageId'])):\n        position.append([list(valid_df['ImageId'])[i], list(valid_df['PredictionString'])[i]])\n    valid_dict = {imgID:str2coords2(s, names=['pitch', 'yaw', 'roll', 'x', 'y', 'z', 'carid_or_score']) for imgID,s in position}\n    result_flg = [] # 1 for TP, 0 for FP\n    scores = []\n    MAX_VAL = 10**15\n    for img_id in valid_dict:\n        for pcar in sorted(valid_dict[img_id], key=lambda x: -x['carid_or_score']):\n            # find nearest GT\n            min_tr_dist = MAX_VAL\n            min_idx = -1\n            for idx, gcar in enumerate(train_dict[img_id]):\n                tr_dist = TranslationDistance(pcar,gcar)\n                if tr_dist < min_tr_dist:\n                    min_tr_dist = tr_dist\n                    min_ro_dist = RotationDistance(pcar,gcar)\n                    min_idx = idx\n       \n            # set the result\n            if min_tr_dist < thre_tr_dist and min_ro_dist < thre_ro_dist:\n                if not keep_gt:\n                    train_dict[img_id].pop(min_idx)\n                result_flg.append(1)\n            else:\n                result_flg.append(0)\n            scores.append(pcar['carid_or_score'])\n    \n    return result_flg, scores\n\n\n\ndef calc_map_df(valid_df, nrows=None):\n    expanded_valid_df = expand_df(valid_df, ['pitch','yaw','roll','x','y','z','Score'])\n\n\n    train_df = pd.read_csv(PATH + 'train.csv')\n    train_df = train_df[train_df.ImageId.isin(valid_df.ImageId.unique())]\n    expanded_train_df = expand_df(train_df, ['model_type','pitch','yaw','roll','x','y','z'])\n    n_gt = len(expanded_train_df)\n\n    thres_ro_list = [50, 45, 40, 35, 30, 25, 20, 15, 10, 5]\n    thres_tr_list = [0.1, 0.09, 0.08, 0.07, 0.06, 0.05, 0.04, 0.03, 0.02, 0.01]\n    ap_list = []\n    \n    position = []\n    for i in range(len(thres_ro_list)):\n        position.append([thres_ro_list[i], thres_tr_list[i]])\n    for thre_ro_dist, thre_tr_dist in tqdm(position):\n        abs_dist = False\n        result_flg, scores = check_match(valid_df, train_df, thre_tr_dist, thre_ro_dist)\n        n_tp = np.sum(result_flg)\n        recall = n_tp\/n_gt\n        ap = average_precision_score(result_flg, scores)*recall\n        ap_list.append(ap)\n    return np.mean(ap_list)","34843859":"#For validating,we fill some fixed codes in PredictionString field when it's nan\ndef fill_str(x):\n    if type(x)==float or len(x)<2:\n        return '0.15511163 0.025993261021686774 -3.1062442382150373 -15.10751805129461 12.073826862286817 70.47340792740864 0.5496648404696726'\n    return x","23594062":"IMG_WIDTH = 256\nIMG_HEIGHT = IMG_WIDTH \/\/ 16 * 5\nMODEL_SCALE = 8\nDISTANCE_THRESH_CLEAR = 2\nlearning_rate = 0.001\nn_epochs = 10\n\n# Gets the GPU if there is one, otherwise the cpu\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nmodel = MyUNet(8).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=n_epochs * len(train_loader) \/\/ 3, gamma=0.1)\n\n# We could see the construction of the model:\nmodel","20e127c9":"IMG_WIDTH = 512\nIMG_HEIGHT = IMG_WIDTH \/\/ 16 * 5\nMODEL_SCALE = 8\nDISTANCE_THRESH_CLEAR = 2\nlearning_rate = 0.001\nn_epochs = 10\nDISTANCE_THRESH_CLEAR = 2\nBATCH_SIZE = 2\n\ntrain_images_dir = PATH + 'train_images\/{}.jpg'\ntest_images_dir = PATH + 'test_images\/{}.jpg'\n\n# we select a part of\ndf_train, df_test = train_test_split(train, test_size=0.2, random_state=42)\ndf_val = test\n\n# we create dataset objects\ntrain_dataset = CarDataset(df_train, train_images_dir, training=True)\ntest_dataset = CarDataset(df_test, train_images_dir, training=False)\ntest_dataset2 = CarDataset(df_test, train_images_dir, training=False, ID=True)\nval_dataset = CarDataset(df_val, test_images_dir, training=False)\n\n# we create data generators - they will produce batches\ntrain_loader = DataLoader(dataset=train_dataset,\n                          batch_size=BATCH_SIZE,\n                          shuffle=False,\n                          num_workers=2)\ntest_loader = DataLoader(dataset=test_dataset,\n                         batch_size=BATCH_SIZE,\n                         shuffle=False,\n                         num_workers=2)\ntest_loader2 = DataLoader(dataset=test_dataset2,\n                         batch_size=BATCH_SIZE,\n                         shuffle=False,\n                         num_workers=2)\nval_loader = DataLoader(dataset=val_dataset,\n                        batch_size=BATCH_SIZE,\n                        shuffle=False,\n                        num_workers=2)\n\n\n\n# Gets the GPU if there is one, otherwise the cpu\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nmodel = MyUNet(8).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=n_epochs * len(train_loader) \/\/ 3, gamma=0.1)\n\nhistory = pd.DataFrame()\n\nfor epoch in range(n_epochs):\n    torch.cuda.empty_cache()\n    gc.collect()\n    # Training the model\n    model.train()\n    for batch_idx, (img_batch, mask_batch, regr_batch) in enumerate(tqdm(train_loader)):\n        img_batch = img_batch.to(device)\n        mask_batch = mask_batch.to(device)\n        regr_batch = regr_batch.to(device)\n        \n        optimizer.zero_grad()\n        output = model(img_batch)\n        loss = criterion(output, mask_batch, regr_batch)\n        if history is not None:\n            history.loc[epoch + batch_idx \/ len(train_loader), 'train_loss'] = loss.data.cpu().numpy()    \n        loss.backward()\n        optimizer.step()\n        exp_lr_scheduler.step()      \n    print('Train Epoch: {} \\tLR: {:.6f}\\tLoss: {:.6f}'.format(\n        epoch,\n        optimizer.state_dict()['param_groups'][0]['lr'],\n        loss.data))\n\n    # Evaluate the model\n    model.eval()\n    loss = 0\n    with torch.no_grad():\n        for img_batch, mask_batch, regr_batch in test_loader:\n            img_batch = img_batch.to(device)\n            mask_batch = mask_batch.to(device)\n            regr_batch = regr_batch.to(device)\n            output = model(img_batch)\n            loss += criterion(output,\n                            mask_batch,\n                            regr_batch,\n                            size_average=False).data\n    loss \/= len(test_loader.dataset)\n    \n    if history is not None:\n        history.loc[epoch, 'test_loss'] = loss.cpu().numpy()\n    \n    print('test loss: {:.4f}'.format(loss))\n    # evaluate with mAP\n    model.eval()\n    ids=[]\n    preds=[]\n    \n    with torch.no_grad():\n        for ids_batch,img_batch in tqdm(test_loader2):\n            img_batch = img_batch.to(device)\n            output = model(img_batch).cpu().numpy()\n            ids.extend(ids_batch)\n            for out in output:\n                predictions=[]\n                coords = extract_coords(out)\n                s = coords2str(coords)\n                predictions.append(s)\n                preds.append(' '.join(predictions))\n        \n\n    torch.cuda.empty_cache()  \n    validation_prediction='valid_preds.csv'\n    sub1=pd.DataFrame()\n    sub1['ImageId']=ids\n    sub1['PredictionString']=preds\n    #fix nan bug\n    sub1['PredictionString']=sub1['PredictionString'].apply(fill_str)\n    sub1.to_csv(validation_prediction,index=False)\n    map=calc_map_df(sub1,nrows=None)\n    if history is not None:\n        history.loc[epoch, 'test_map'] = map\n    print('test map: ', map) \n\nseries = history.dropna()['train_loss']\nplt.figure(figsize=(15, 5))\nplt.title('Train evaluation')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.plot(series.index, series)\nplt.show()\nbest_value = series.min()\nbest_epoch = series[series == series.min()].index\n\nseries = history.dropna()['test_loss']\nplt.figure(figsize=(15, 5))\nplt.title('Test evaluation')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.plot(series.index, series)\nplt.show()\nbest_value = series.min()\nprint('best value : ', best_value)\nbest_epoch = series[series == series.min()].index\nprint('for epoch : ', best_epoch)\n\nseries = history.dropna()['test_map']\nplt.figure(figsize=(15, 5))\nplt.title('Test mAP evaluation')\nplt.xlabel('Epoch')\nplt.ylabel('mAP')\nplt.plot(series.index, series)\nplt.show()\nbest_value = series.max()\nmap_evaluation.append(best_value)\nprint('best value : ', best_value)\nbest_epoch = series[series == series.min()].index\nprint('for epoch : ', best_epoch)","3c848cdf":"torch.save(model.state_dict(), PATH + 'model.pth')","8d7228f1":"img, mask, regr = test_dataset[0]\n\nplt.figure(figsize=(15,15))\nplt.title('Input image')\nplt.imshow(np.rollaxis(img, 0, 3))\nplt.show()\n\nplt.figure(figsize=(15,15))\nplt.title('Ground truth mask')\nplt.imshow(mask)\nplt.show()\n\noutput = model(torch.tensor(img[None]).to(device))\nlogits = output[0,0].data.cpu().numpy()\n\nplt.figure(figsize=(15,15))\nplt.title('Model predictions')\nplt.imshow(logits)\nplt.show()\n\nplt.figure(figsize=(15,15))\nplt.title('Model predictions thresholded')\nplt.imshow(logits > 0)\nplt.show()","52a3be91":"torch.cuda.empty_cache()\ngc.collect()\n\nfor idx in range(15):\n    img, mask, regr = test_dataset[idx]\n    \n    output = model(torch.tensor(img[None]).to(device)).data.cpu().numpy()\n    coords_pred = extract_coords(output[0])\n    coords_true = extract_coords(np.concatenate([mask[None], regr], 0))\n    \n    img = imread(train_images_dir.format(df_dev['ImageId'].iloc[idx]))\n    \n    fig, axes = plt.subplots(1, 2, figsize=(30, 30))\n    axes[0].set_title('Ground truth')\n    axes[0].imshow(visualize(img, coords_true))\n    axes[1].set_title('Prediction')\n    axes[1].imshow(visualize(img, coords_pred))\n    plt.show()","ca4683bf":"predictions = []\n\nval_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n\nmodel.eval()\n\nfor img, _, _ in tqdm(val_loader):\n    with torch.no_grad():\n        output = model(img.to(device))\n    output = output.data.cpu().numpy()\n    for out in output:\n        coords = extract_coords(out)\n        s = coords2str(coords)\n        predictions.append(s)","198dd4f9":"test = pd.read_csv(PATH + 'sample_submission.csv')\ntest['PredictionString'] = predictions\ntest.to_csv('predictions.csv', index=False)\ntest.head()","d928c4ef":"Show some generated examples","e08495dc":"# Image preprocessing","caab746f":"# Training","f5e63dcb":"I guess, pitch and yaw are mixed up in this dataset. Pitch cannot be that big. That would mean that cars are upside down.","5164ebdc":"We extract all values in the dataframe point_df","0c334299":"We could look this distribution \"from the sky\"","ea1e8113":"We define a fonction to evaluate with mask and regr","17e220e9":"# Exploratory Analysis","f1953430":"# Pictures","1eeedb74":"# Model construction","adef1b8b":"The best IMG_WIDTH is 512.\n\nWe have to learn on all train data.","2e1542be":"# Make submission","6db232c2":"**ImageId** column contains names of images and **PredictionString** column contains the targets\n\nFrom the data description:\n> The primary data is images of cars and related pose information. The pose information is formatted as strings, as follows:  \n>\n> `model type, yaw, pitch, roll, x, y, z`  \n>\n> A concrete example with two cars in the photo:  \n>\n> `5 0.5 0.5 0.5 0.0 0.0 0.0 32 0.25 0.25 0.25 0.5 0.4 0.7`  \n\nWe could extract the value like this:","fc2da1c0":"# Load data","2d091a1b":"# What is this competition about?\nWho do you think hates traffic more - humans or self-driving cars? The position of nearby automobiles is a key question for autonomous vehicles \u2015 and it's at the heart of our newest challenge.\n\nSelf-driving cars have come a long way in recent years, but they're still not flawless. Consumers and lawmakers remain wary of adoption, in part because of doubts about vehicles\u2019 ability to accurately perceive objects in traffic.\n\nBaidu's Robotics and Autonomous Driving Library (RAL), along with Peking University, hopes to close the gap once and for all with this challenge. They\u2019re providing Kagglers with more than 60,000 labeled 3D car instances from 5,277 real-world images, based on industry-grade CAD car models.\n\nYour challenge: develop an algorithm to estimate the absolute pose of vehicles (6 degrees of freedom) from a single image in a real-world traffic environment.\n\nSucceed and you'll help improve computer vision. That, in turn, will bring autonomous vehicles a big step closer to widespread adoption, so they can help reduce the environmental impact of our growing societies. \n\n# Evaluation\nSubmissions are evaluated on mean average precision between the predicted pose information and the correct position and rotation.\n\nWe use the following C# code to determine the translation and rotation distances:\n\n    public static double RotationDistance(Object3D o1, Object3D o2)\n    {\n        Quaternion q1 = Quaternion.CreateFromYawPitchRoll(o1.yaw, o1.pitch, \n             o1.roll);\n        Quaternion q2 = Quaternion.CreateFromYawPitchRoll(o2.yaw, o2.pitch, \n             o2.roll);\n        Quaternion diff = Quaternion.Normalize(q1) * \n             Quaternion.Inverse(Quaternion.Normalize(q2));\n\n        diff.W = Math.Clamp(diff.W, -1.0f, 1.0f);\n\n        return Object3D.RadianToDegree( Math.Acos(diff.W) );\n    }\n\n    public static double TranslationDistance(Object3D o1, Object3D o2)\n    {\n        var dx = o1.x - o2.x;\n        var dy = o1.y - o2.y;\n        var dz = o1.z - o2.z;\n\n        return Math.Sqrt(dx * dx + dy * dy + dz * dz);\n    }\nWe then take the resulting distances between all pairs of objects and determine which predicted objects are closest to solution objects, and apply thresholds for both translation and rotation. Confidence scores are used to sort submission objects. Units for rotation are radians; translation is meters.\n\nIf both of the distances between prediction and solution (as calculated above) are less than the threshold, then that prediction object is counted as a true positive for that threshold. If not the predicted object is counted as a false positive for that threshold.\n\nFinally, mAP is calculated using these TP\/FP determinations across all thresholds.\n\nThe thresholds are as follows:\n\nRotation: 50, 45, 40, 35, 30, 25, 20, 15, 10, 5\n\nTranslation: 0.1, 0.09, 0.08, 0.07, 0.06, 0.05, 0.04, 0.03, 0.02, 0.01\n\nKernel Submissions\nYou can make submissions directly from Kaggle Kernels. By adding your teammates as collaborators on a kernel, you can share and edit code privately with them.\n\nSubmission File\nFor each image ID in the test set, you must predict a pose (position and rotation) for all unmasked cars in the image. The file should contain a header and have the following format:\n\nImageId,PredictionString\n\nID_1d7bc9b31,0.5 0.25 0.5 0.0 0.5 0.0 1.0\n\nID_f9c21a4e3,0.5 0.5 0.5 0.0 0.0 0.0 0.9\n\nID_e83dd7c22,0.5 0.5 0.5 0.0 0.0 0.0 1.0\n\nID_1a050c9a4,0.5 0.5 0.5 0.0 0.0 0.0 0.25\n\nID_d943d1083,0.5 0.5 0.5 0.0 0.0 0.0 1.0 0.5 0.5 0.5 0.0 0.0 0.0 1.0\n\nID_3155084f7,0.5 0.5 0.5 0.0 0.0 0.0 1.0\n\nID_f74dcaa3d,0.5 0.5 0.5 0.0 0.0 0.0 1.0\n\nID_b183b55dd,0.5 0.5 0.5 0.0 0.0 0.0 1.0\n\nID_ff5ea7211,0.5 0.5 0.5 0.0 0.0 0.0 1.0\n\nEach 7-value element in PredictionString corresponds to pitch, yaw, roll, x, y, z and confidence for each car in the scene.","49623cd8":"We see some points are outside the picture","f57f656e":"We create mask and regr pictures","c8de7b2c":"We could compute a dataset with all of this pictures","f594763c":"We could look the distribution of all cars centers","41dcc498":"We could see predictions data:"}}