{"cell_type":{"d4eb07af":"code","c2458c69":"code","c1a96ef9":"code","6e5ef75a":"code","aaf4db1b":"code","b650d610":"code","cbab424e":"code","e3a76db8":"code","48a3fd0c":"code","702fb1ee":"code","5aebb7de":"code","629882de":"code","ceecaf52":"code","fd95a011":"code","d90228c8":"code","dec0143e":"code","eafa4d44":"code","114ead25":"code","fa66471f":"markdown","fb1094bf":"markdown","f08917e4":"markdown","40c86d49":"markdown","d3e463ea":"markdown","bdd066a7":"markdown","c0bef273":"markdown","039b3a04":"markdown","0574347f":"markdown","c8fa34bf":"markdown","bf79e5ec":"markdown","2c6ff451":"markdown","3d4a7ac6":"markdown"},"source":{"d4eb07af":"import numpy as np\nfrom scipy.spatial import distance as dis\n\n\ndef sum_distance(clusters, centers):\n    sum_all = 0\n    for i in range(len(centers)):\n        sum_cluster = 0\n        for j in range(len(clusters[i])):\n            sum_cluster += np.sum(np.power(np.array(centers[i]) - np.array(clusters[i][j]), 2))\n        sum_all += sum_cluster\n\n    return sum_all\n\n\nclass KMeans():\n    def __init__(self, n_clusters=2, max_iter=500):\n        self.k = n_clusters\n        self.max_iterations = max_iter\n        self.kmeans_centroids = []\n\n    # Initialize the centroids as random samples\n    def _init_random_centroids(self, data):\n        n_samples, n_features = np.shape(data)\n        centroids = np.zeros((self.k, n_features))\n        for i in range(self.k):\n            centroid = data[np.random.choice(range(n_samples))]\n            centroids[i] = centroid\n        return centroids\n\n    # Return the index of the closest centroid to the sample\n    def _closest_centroid(self, sample, centroids):\n        closest_i = None\n        closest_distance = float(\"inf\")\n        for i, centroid in enumerate(centroids):\n            distance = dis.euclidean(sample, centroid)\n            if distance < closest_distance:\n                closest_i = i\n                closest_distance = distance\n        return closest_i\n\n    # Assign the samples to the closest centroids to create clusters\n    def _create_clusters(self, centroids, data):\n        clusters = [[] for _ in range(self.k)]\n        for sample_i, sample in enumerate(data):\n            centroid_i = self._closest_centroid(sample, centroids)\n            clusters[centroid_i].append(sample_i)\n        return clusters\n\n    # Calculate new centroids as the means of the samples in each cluster\n    def _calculate_centroids(self, clusters, data):\n        n_samples, n_features = np.shape(data)\n        centroids = np.zeros((self.k, n_features))\n        for i, cluster in enumerate(clusters):\n            # Here we handle null clusters\n            if len(cluster) != 0:\n                centroid = np.mean(data[cluster], axis=0)\n            else:\n                centroid = data[np.random.choice(range(n_samples))]\n            centroids[i] = centroid\n        return centroids\n\n    # Classify samples as the index of their clusters\n    def _get_cluster_labels(self, clusters, data):\n        # One prediction for each sample\n        y_pred = np.zeros(np.shape(data)[0])\n        for cluster_i, cluster in enumerate(clusters):\n            for sample_i in cluster:\n                y_pred[sample_i] = cluster_i\n        return y_pred\n\n    # Do K-Means clustering and return the centroids of the clusters\n    def fit(self, data):\n        # Initialize centroids\n        centroids = self._init_random_centroids(data)\n        # Iterate until convergence or for max iterations\n        for _ in range(self.max_iterations):\n            # Assign samples to closest centroids (create clusters)\n            clusters = self._create_clusters(centroids, data)\n\n            prev_centroids = centroids\n            # Calculate new centroids from the clusters\n            centroids = self._calculate_centroids(clusters, data)\n\n            # If no centroids have changed => convergence\n            diff = centroids - prev_centroids\n            if not diff.any():\n                break\n        self.kmeans_centroids = centroids\n        return self\n\n    # Predict the class of each sample\n    def predict(self, data):\n\n        # First check if we have determined the K-Means centroids\n        if not self.kmeans_centroids.any():\n            raise Exception(\"K-Means centroids have not yet been determined.\\nRun the K-Means 'fit' function first.\")\n\n        clusters = self._create_clusters(self.kmeans_centroids, data)\n\n        predicted_labels = self._get_cluster_labels(clusters, data)\n\n        return predicted_labels","c2458c69":"from sklearn.cluster import DBSCAN\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import distance","c1a96ef9":"df = pd.read_csv('..\/input\/clustering-algorithms-applied-to-covid19-dataset\/Dataset1.csv')\nn_cluster = 4\nmax_iter = 20","6e5ef75a":"X = df.to_numpy()\nkmeans = KMeans(n_cluster, max_iter).fit(X)\npredict = kmeans.predict(X)\nclusters = [[] for i in range(n_cluster)]\nfor i in range(len(predict)):\n    clusters[int(predict[i])].append(X[i])","aaf4db1b":"centers = kmeans.kmeans_centroids","b650d610":"for i in range(n_cluster):\n    plt.plot(np.array(clusters[i])[:,0], np.array(clusters[i])[:,-1], 's',color=(1 - (i\/n_cluster), 1 - (i\/n_cluster), (i\/n_cluster)))\n    plt.plot(centers[i][0],centers[i][1], 'co')\nplt.show()","cbab424e":"errors = []\nfor i in range(n_cluster):\n    length = len(clusters[i])\n    error = 0\n    for dot in clusters[i]:\n        error += distance.euclidean(dot, centers[i])\n    errors.append(error\/length)\nprint(errors)","e3a76db8":"distorsions = []\nfor k in range(1, 15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(X)\n    predict = kmeans.predict(X)\n    clusters = [[] for i in range(k)]\n    centers = kmeans.kmeans_centroids\n    for i in range(len(predict)):\n        clusters[int(predict[i])].append(X[i])\n    distorsions.append(sum_distance(np.array(clusters), np.array(centers)))\n\nfig = plt.figure(figsize=(15, 5))\nplt.plot(range(1, 15), distorsions)\nplt.grid(True)\nplt.title('Elbow curve')","48a3fd0c":"df2 = pd.read_csv('..\/input\/clustering-algorithms-applied-to-covid19-dataset\/Dataset2.csv')\nplt.plot(df2.to_numpy(), 'bo')\nplt.show()","702fb1ee":"!pip3 install folium","5aebb7de":"import folium","629882de":"m = folium.Map(location=[32.427910, 53.688046], zoom_start=5)","ceecaf52":"COVID19_df = pd.read_csv('..\/input\/clustering-algorithms-applied-to-covid19-dataset\/covid-sample.csv')\nprint(COVID19_df.to_numpy()[:,-1].max(), COVID19_df.to_numpy()[:,-1].min())\nprint(COVID19_df.to_numpy()[:,0].max(), COVID19_df.to_numpy()[:,0].min())","fd95a011":"for item in COVID19_df.to_numpy():\n    folium.Circle(location = item, radius= 1, color=\"red\", fill=True).add_to(m)\nm","d90228c8":"eps = 0.1\nmin_samples = 10\ncluster_number = 4\nclustering = DBSCAN(eps=eps, min_samples=min_samples).fit(COVID19_df.to_numpy())\nclusters = np.array(clustering.labels_)\nsamples = []\nclusters_count = np.unique(clusters).shape[0] - 2\ndataset = COVID19_df.to_numpy()\nm = folium.Map(location=[32.427910, 53.688046], zoom_start=5)\nprint(\"Total number of classes: \", clusters_count)\nfor i in range(len(dataset)):\n    if(clusters[i] == cluster_number):\n        samples.append(dataset[i])\nfor item in samples:\n    folium.Circle(location = item, radius= 1, color=\"red\", fill=True).add_to(m)\nm","dec0143e":"colors = ['red', 'blue', 'green', 'purple', 'orange', 'darkred','lightred', 'beige', 'darkblue', 'darkgreen', 'cadetblue', 'darkpurple', 'white', 'pink', 'lightblue', 'lightgreen', 'gray', 'black', 'lightgray', 'blue', 'green', 'purple', 'orange', 'darkred','lightred', 'beige', 'darkblue', 'darkgreen', 'cadetblue', 'darkpurple', 'white', 'pink', 'lightblue', 'lightgreen', 'gray', 'black']\nm = folium.Map(location=[32.427910, 53.688046], zoom_start=5)\nfor i in range(-1, clusters_count):\n    for j in range(len(dataset)):\n        if i == clusters[j]:\n            folium.CircleMarker(location = dataset[j], radius= 1, color=colors[i], fill=True).add_to(m)\nm","eafa4d44":"from matplotlib import image\nimg = image.imread('..\/input\/clustering-algorithms-applied-to-covid19-dataset\/imageSmall.png')\nplt.imshow(img)\nplt.show()","114ead25":"rows = img.shape[0]\ncols = img.shape[1]\nimg = img.reshape(img.shape[0]*img.shape[1],3)\nkmeans = KMeans(40,200)\nkmeans.fit(img)\nlabels_ = kmeans.predict(img)\nclusters = np.asarray(kmeans.kmeans_centroids,dtype=np.uint8) \nlabels = np.asarray(labels_,dtype=np.uint8 )  \nlabels = labels.reshape(rows,cols);\nplt.imshow(labels)\nplt.show()","fa66471f":"<h1>Import libraries<\/h1>","fb1094bf":"<p>The graph indicates it doesn't seperate partitions so it seems partitioning is not possible<\/p>","f08917e4":"<h1>KMeans class<\/h1>\n<p>This class has implemented KMeans based on either Lloyd\u2019s or Elkan\u2019s algorithm.\nThe average complexity is given by O(k n T), were n is the number of samples and T is the number of iteration.\n\nThe worst case complexity is given by O(n^(k+2\/p)) with n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii, \u2018How slow is the k-means method?\u2019 SoCG2006)\n\nIn practice, the k-means algorithm is very fast (one of the fastest clustering algorithms available), but it falls in local minima. That\u2019s why it can be useful to restart it several times.<\/p>","40c86d49":"<h1>Q2.B load with different values<\/h1>\n<p>with different eps and min_samples, you can see clusters by cluster_number<\/p>","d3e463ea":"<h1>Q1.D,Error by n_cluster and elbow method<\/h1>\n<p>This part is not common with previous parts<\/p>","bdd066a7":"<h1>Q2.A plot in folium<\/h1>\n<p>Thanks for help and beautiful homework<\/p>","c0bef273":"<h1>Q1.B,C Error average by cluster<\/h1>","039b3a04":"<h1>KMeans<\/h1>\n<p>Here we do kmeans by parameters and all neccessary values saved<\/p>","0574347f":"<h1>Q1.F Dataset2<\/h1>\n<p>Running again is easy just with change df on top, here we analyze Dataset2<\/p>","c8fa34bf":"<h1>Q2.C,D Show clusters<\/h1>\n<p>Label -1 means out of range even direct or not, actually Tehran and Qom are centers<\/p>","bf79e5ec":"<h1>Q3. size reduction of image<\/h1>\n<p>It takes a longgggggggggggggg time to process and I tested compression with sklearn KMeans library and result was same as my own library<\/p>","2c6ff451":"<h1>Read data and set parameters<\/h1>","3d4a7ac6":"<h1>Q1.A Now we plot result<\/h1>\n<p>You just need to change parameters like n_cluster and it works<\/p>"}}