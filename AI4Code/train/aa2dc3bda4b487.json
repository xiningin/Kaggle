{"cell_type":{"5dbde192":"code","1f252630":"code","063f0ad2":"code","cbe4b9b6":"code","252e6082":"code","495ff435":"code","b6b000dd":"code","680ab8bb":"code","105d512b":"code","77c8aaa3":"code","3f7a5f10":"code","2502f48e":"code","2be80683":"code","ef721894":"code","5af541a4":"code","3c0aafb5":"code","be431f3f":"code","4f96cfd9":"code","47def8e1":"code","e386d725":"code","33cbc42e":"code","fd1090de":"code","a943b6ca":"code","b5944d7c":"code","f0de1f8e":"code","fcc70a52":"code","036e481d":"code","b0fd27f2":"code","e011e372":"code","1ed38e1c":"code","16cd1550":"code","5ff6d1c9":"code","ddc262e8":"code","1f2a8c42":"markdown","d89f1e5c":"markdown","6091978d":"markdown","8eabaabc":"markdown","de222558":"markdown","24386207":"markdown","67cba246":"markdown","a9486e83":"markdown","5c92662d":"markdown","0d34a77d":"markdown","5e38301b":"markdown","303c2772":"markdown","8b0b8236":"markdown","675a64c2":"markdown","dc0d42cf":"markdown"},"source":{"5dbde192":"import numpy as np\nimport pandas as pd\n\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_curve, roc_auc_score\nfrom pandas.plotting import scatter_matrix\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nsns.set_style(\"dark\")\nimport warnings\nwarnings.filterwarnings('ignore')","1f252630":"#import data\nbank_servey = pd.read_csv(\"..\/input\/bank-marketing-campaign-subscriptions\/Bank_Campaign.csv\", sep=';', header=[0])","063f0ad2":"bank_servey.columns","cbe4b9b6":"bank_servey.info()","252e6082":"#Data Cleaning\n# remove the duplicates values from dataframe\nbank_servey = bank_servey.drop_duplicates()\nbank_servey.shape","495ff435":"# There are many missing values in other forms like 'unknown'\nmissing_cols = bank_servey.isin(['unknown']).sum(axis=0)\nmissing_cols","b6b000dd":"#variables that are representing quantities\/numbers\nbank_servey.describe()","680ab8bb":"fig = plt.figure(figsize=(10,8)) \nplt.title('Number of Subscribed Clients', fontweight='bold')\nax=sns.countplot(x='subscribed', data=bank_servey)\nsize = float(bank_servey.shape[0])\nfor p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2., height + 4, '{:1.2f}%'.format(100 * height\/size), ha='center')\nplt.grid(True, alpha=0.5, color='black')\nplt.legend(loc='best')\nplt.show()","105d512b":"fig = plt.figure(figsize=(16,8)) \nplt.title('Number of Subscribed Clients per Job', fontweight='bold')\nax=sns.countplot(data=bank_servey, x='job', hue='subscribed')\nfor p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2., height + 4, '{:1.2f}%'.format(100 * height\/size), ha='center')\nplt.grid(True, alpha=0.5, color='black')\nplt.legend(loc='best')\nplt.show()","77c8aaa3":"fig = plt.figure(figsize=(16,8)) \nplt.title('Number of Subscribed Clients per Education', fontweight='bold')\nax=sns.countplot(data=bank_servey, x='education', hue='subscribed')\nfor p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2., height + 4, '{:1.2f}%'.format(100 * height\/size), ha='center')\nplt.grid(True, alpha=0.5, color='black')\nplt.legend(loc='best')\nplt.show()","3f7a5f10":"fig = plt.figure(figsize=(16,8)) \nplt.title('Number of Subscribed Clients per P-outcome', fontweight='bold')\nax=sns.countplot(data=bank_servey, x='poutcome', hue='subscribed')\nfor p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x()+p.get_width()\/2., height + 4, '{:1.2f}%'.format(100 * height\/size), ha='center')\nplt.grid(True, alpha=0.5, color='black')\nplt.legend(loc='best')\nplt.show()","2502f48e":"bank_servey.columns\nsns.pairplot(bank_servey, size=3)\nplt.suptitle('Relationship between continuous variables')","2be80683":"sns.pairplot(bank_servey, hue='subscribed', size=3);\nplt.suptitle('Visualizing Multidimensional Relationships')","ef721894":"#create the correlation matrix heat map\nplt.figure(figsize=(16,10))\nsns.heatmap(bank_servey.corr(),linewidths=.1,cmap=\"YlGnBu\", annot=True)\nplt.yticks(rotation=0);\nplt.suptitle('Correlation Matrix')","5af541a4":"# map the values from the target variable from 'yes' and 'no' to 1 and 0, as follows:\nbank_servey.drop(columns=['pdays'], inplace=True)\n# Transform the 'yes' and 'no' values (target variable) to 1 and 0 respectively\nbank_servey['subscribed'] = bank_servey['subscribed'].map({'yes': 1, 'no': 0})","3c0aafb5":"#the pre-processing part is to encode the categorical features of the dataset.\n#we will use the one-hot encoding method\ncategorical_cols = bank_servey.select_dtypes(include=['object']).columns\ncategorical_cols","be431f3f":"encoded_df = pd.concat([bank_servey, pd.get_dummies(bank_servey[categorical_cols])], axis=1)","4f96cfd9":"encoded_df = encoded_df.drop(categorical_cols, axis=1)\nencoded_df.head(10)","47def8e1":"# Scale\nscaler = MinMaxScaler(feature_range=(0, 1))\nencoded_scaled_df = pd.DataFrame(scaler.fit_transform(encoded_df), columns=encoded_df.columns)","e386d725":"# Split the data to train and test sets\nX = encoded_scaled_df.loc[:, encoded_scaled_df.columns != 'subscribed']\ny = encoded_scaled_df['subscribed']","33cbc42e":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","fd1090de":"#For model evaluation, let's define a function\ndef getModelEvaluationMetrics(classifier, model_name: str, x_test: pd.core.frame.DataFrame,\n                              y_test: pd.core.frame.DataFrame, y_predicted, plot_confusion_matrix=False,\n                              figsize=(10, 8)) -> np.ndarray:\n\n    conf_mat = confusion_matrix(y_true=y_test, y_pred=y_predicted)\n    print('Confusion matrix:\\n\\n {0}'.format(conf_mat))\n\n    if plot_confusion_matrix:\n        labels = ['no', 'yes']\n        fig = plt.figure(figsize=figsize)\n        ax = fig.add_subplot(111)\n        cax = ax.matshow(conf_mat, cmap=plt.cm.Reds)\n        fig.colorbar(cax)\n        ax.set_xticklabels([''] + labels)\n        ax.set_yticklabels([''] + labels)\n        plt.xlabel('Predicted')\n        plt.ylabel('Expected')\n        plt.title(f'Confusion Matrix for {model_name}', fontweight='bold')\n        plt.show()\n\n    # Calculating the precision (tp\/tp+fp)\n    precision = str(np.round((conf_mat[1][1] \/ (conf_mat[1][1] +\n                              conf_mat[0][1])) * 100, 2))\n    print('The precision is: {0} %'.format(precision))\n\n    # Calculating the recall (tp\/tp+fn)\n    recall = str(np.round((conf_mat[1][1] \/ (conf_mat[1][1] +\n                           conf_mat[1][0])) * 100, 2))\n    print('The recall is: {0} %'.format(recall))\n\n    return conf_mat","a943b6ca":"#To perform a full ROC analysis let's define a function\n#ROC Curve and calculating of the ROC AUC score\ndef createROCAnalysis(classifier, model_name: str, y_test: pd.core.series.Series, pred_probs: np.ndarray,\n                      plot_ROC_Curve=False, figsize=(10, 8)) -> int:\n   \n    if plot_ROC_Curve:\n        plt.figure(figsize=figsize)\n        plt.plot([0, 1], [0, 1], linestyle='--', label='No Skill Classifier')\n        fp_rate, tp_rate, _ = roc_curve(y_test, pred_probs[:, 1])\n        plt.plot(fp_rate, tp_rate, marker='.', label=model_name)\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title(f'ROC Curve for {model_name}', fontweight='bold')\n        plt.grid(True, alpha=0.1, color='black')\n        plt.legend(loc='lower right')\n        plt.show()\n\n    # Calculate Area Under Curve (AUC) for the Receiver Operating\n    # Characteristics Curve (ROC)\n    auc_score = np.round(roc_auc_score(y_test, pred_probs[:, 1]), 4)\n    print(f'{model_name} - ROC AUC score: {auc_score}')\n\n    return auc_score","b5944d7c":"# Instantiate the Random Forest model\n#Pre-tuned Hyperparameter\nrf_class = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features=10, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=20, min_samples_split=20,\n            min_weight_fraction_leaf=0.0, n_estimators=600, n_jobs=None,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)","f0de1f8e":"# Assign the above probabilities to the corresponding class ('no', 'yes')\nrf_class.fit(X_train, y_train)\nrf_y_pred = rf_class.predict(X_test)\n# Evaluate the model by using Recall\/Precission:\ngetModelEvaluationMetrics(classifier=rf_class, model_name='Random Forest',x_test=X_test, y_test=y_test,\n                              y_predicted=rf_y_pred, plot_confusion_matrix=True, figsize=(8,6))","fcc70a52":"# Evaluate the model by using ROC Curve:\nrf_pred_probs = rf_class.predict_proba(X_test)\ncreateROCAnalysis(classifier=rf_class, model_name='Random Forest', y_test=y_test, pred_probs=rf_pred_probs,\n                  plot_ROC_Curve=True, figsize=(8,6))","036e481d":"import xgboost as xgb\nxb = xgb.XGBClassifier(learning_rate = 0.01, max_depth = 10, min_child_weight = 2, subsample = 0.5, colsample_bytree = 0.7, n_estimators = 1000)\nxb.fit(X_train, y_train)\nxb_y_pred = xb.predict(X_test)\n# Evaluate the model by using Recall\/Precission:\ngetModelEvaluationMetrics(classifier=xb, model_name='Random Forest',x_test=X_test, y_test=y_test,\n                              y_predicted=xb_y_pred, plot_confusion_matrix=True, figsize=(8,6))","b0fd27f2":"xb_predict_probs = xb.predict_proba(X_test)\ncreateROCAnalysis(classifier=xb, model_name='Random Forest', y_test=y_test, pred_probs=xb_predict_probs,\n                  plot_ROC_Curve=True, figsize=(8,6))","e011e372":"# Find the feature importance based on Gini criterion\nfeature_importance = {}\nbest_estimator_fi = rf_class.feature_importances_\n\nfor feature, importance in zip(X_train.columns, best_estimator_fi):\n    feature_importance[feature] = importance\n\nimportances = pd.DataFrame.from_dict(feature_importance, orient='index').rename(columns={0: 'Gini Score'})\n\nimportances = importances.sort_values(by='Gini Score', ascending=False)","1ed38e1c":"# Plot for feature importance\nplt.figure(figsize=(14, 6))\nsns.barplot(x=importances.index[0:10],\n            y=importances['Gini Score'].iloc[0:10], palette='muted')\nplt.title(f'Importance for the Top 10 Features (Gini criterion) ',\n          fontweight='bold')\nplt.grid(True, alpha=0.1, color='black')\nplt.show()","16cd1550":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\npca = PCA(n_components = 10)\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)\nexplained_variance = pca.explained_variance_ratio_\nexplained_variance","5ff6d1c9":"xb = xgb.XGBClassifier(learning_rate = 0.01, max_depth = 10, min_child_weight = 2, subsample = 0.5, colsample_bytree = 0.7, n_estimators = 1000)\nxb.fit(X_train, y_train)\nxb_y_pred = xb.predict(X_test)\n# Evaluate the model by using Recall\/Precission:\ngetModelEvaluationMetrics(classifier=xb, model_name='Random Forest',x_test=X_test, y_test=y_test,\n                              y_predicted=xb_y_pred, plot_confusion_matrix=True, figsize=(8,6))","ddc262e8":"xb_predict_probs = xb.predict_proba(X_test)\ncreateROCAnalysis(classifier=xb, model_name='Random Forest', y_test=y_test, pred_probs=xb_predict_probs,\n                  plot_ROC_Curve=True, figsize=(8,6))","1f2a8c42":"Okay, my xgboost model perform very well compared to the random forest model in the case of the positive class, the xgboost classified correctly approximately 53% of the total clients who end up subscribing for the product. \n\n**Previous model [Random Forest Classifire]** = subscription(yes)- **39%**\n\n**Proposed model [XGboost Classifire]** = subscription(yes)- **53%**","d89f1e5c":"Following the encoding of the categorical features, we will continue with the normalization (scalling) of the numerical features. For this  we will use the MinMax scalling method.","6091978d":"# Data Preprocessing","8eabaabc":"**The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.**","de222558":"# Conclution\n\n**I have compare my new model (Xgboost) with the existing model (RF) in terms of classified correctly  the total clients who end up subscribing for the product. If you find this notebook useful, would like to hear from you about it.**\n\nRef.\n\n1. https:\/\/towardsdatascience.com\/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n2. https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\n3. https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\n4. https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score\n5. https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\n6. https:\/\/machinelearningmastery.com\/tour-of-evaluation-metrics-for-imbalanced-classification\/","24386207":"# Introduction\n**This is the extendede work of [Georgios Spyrou](https:\/\/github.com\/gpsyrou\/Binary_Classification_of_Bank_Marketing_Campaigns)**, In this project it is to analyze a dataset containing information about marketing campaigns that were conducted via phone calls from a Portuguese banking institution to their clients. Purpose of these campaigns is to prompt their clients to subscribe for a specific financial product of the bank (term deposit). After each call was conducted, the client had to inform the institution about their intention of either subscribing to the product (indicating a successful campaign) or not (unsucessful campaign).","67cba246":"# Apply PCA","a9486e83":"# Model Evaluation Metrics","5c92662d":"Well, the model shows very good at predicting clients who are not going to subscribe (negative class)\n**But did not perform very well in the case of the positive class**, as it classified correctly approximately 39% of the total clients who end up subscribing for the product. ","0d34a77d":"# Import libraries, datas3t and perform EDA","5e38301b":"Though the precision and recall score is pretty low but the ROC AUC score is not that bad!!","303c2772":"# Dataset Description\nThe dataset has 41188 rows (instances of calls to clients) and 21 columns (variables) which are describing certain aspects of the call. Please note that there are cases where the same client was contacted multiple times - something that practically doesn't affect the analysis as each call will be considered independent from another even if the client is the same.\n\nDataset description link: https:\/\/www.kaggle.com\/pankajbhowmik\/bank-marketing-campaign-subscriptions","8b0b8236":"# My Approach to XGboost\n\nNow, here i will try apply xgboost classifire tin this circumstance. Let see whether my model can perform better in the case of the positive class or not?? ","675a64c2":"Let's apply xgboost to pridict suscription using only 10 important fearures!","dc0d42cf":"# Apply Classification Models\n\nLet's first see the outcome found by **Georgios Spyrou in his project**"}}