{"cell_type":{"281ab30f":"code","6e17b452":"code","57681ed3":"code","33b7e899":"code","ae95f91d":"code","16cd0021":"code","caeeab47":"code","4ee081ff":"code","4031217d":"code","58d14625":"code","e9c5e7c4":"code","0e5078aa":"code","181710d8":"code","f84a72be":"code","4b72dd7c":"code","86d3f4cd":"code","a6d06f6e":"code","03f864b7":"code","6437b36f":"code","fceccb3e":"code","9500c509":"code","2f09bd1c":"code","b87f4665":"code","4f9395cd":"code","8406c7d6":"code","9eccedbf":"code","e9f18e82":"code","8c503740":"code","446daf90":"code","5468224c":"code","7f81e784":"code","8b6a12b4":"code","f2339338":"code","b15bff2e":"code","8020c49b":"code","7a9af47e":"code","51d6de7c":"code","42000640":"code","e78bc352":"code","24f83739":"code","80d37138":"code","d58efd76":"code","255d5288":"code","c10a7133":"code","5e45ca99":"code","5f9c85bd":"code","f670700b":"code","b50fe23a":"code","230c6621":"code","46026282":"code","4f83e6b0":"code","b429f68c":"code","3eda381e":"code","809a077d":"code","d4dd259d":"code","f996ca54":"code","f2c6ec87":"code","55a41aa5":"code","979759bb":"code","6a324e03":"code","3d74735b":"code","f12b49ef":"code","2b9353d1":"code","4fce5b5b":"code","15a25dd1":"code","eee34847":"code","593d8e20":"code","a363b973":"code","6c06368e":"code","9bd5ac98":"markdown","4b1266c7":"markdown","28df5537":"markdown","e8a26dc4":"markdown","1c5c8a7f":"markdown","1214c097":"markdown","8d14dddf":"markdown","37ee3a61":"markdown","56966e45":"markdown","344bbb3a":"markdown","ebc98a71":"markdown","c5b1f627":"markdown","a09088b8":"markdown","e80c8d2a":"markdown","0a4b43c6":"markdown","10fccb82":"markdown","87938fb5":"markdown","b788784d":"markdown","7f240f4b":"markdown","78e5a19f":"markdown","99e33732":"markdown","dbcd951b":"markdown","36b64d1d":"markdown","e2782642":"markdown"},"source":{"281ab30f":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom imblearn.over_sampling import SMOTE\n\nimport nltk\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.corpus import stopwords\nimport re\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom ast import literal_eval\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBClassifier\n\n\nfrom keras import models\nfrom keras import layers\nimport keras\nfrom keras import optimizers\nfrom keras.layers import Dense, Conv2D, Flatten\nfrom keras.layers import Dropout\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding","6e17b452":"hp = pd.read_csv('\/kaggle\/input\/harry-potter-and-the-philosophers-stone-script\/hp_script.csv',encoding='cp1252')","57681ed3":"hp.head()","33b7e899":"hp['character_name'].value_counts()","ae95f91d":"sns.set_style('whitegrid')\nplt.figure(figsize=(10,7))\nsns.countplot(y='character_name', data=hp, order=hp.character_name.value_counts().iloc[:20].index, palette=\"Reds_d\")\nplt.xlabel('Number of lines of dialogue', fontsize=15)\nplt.ylabel('Character', fontsize=15)\nplt.title('Character Importance by Number of Lines of Dialogue', fontsize=20)\nplt.show()","16cd0021":"# adding a new column to the dataframe, of number of words in each line\nhp['dialogue_wordcount'] = hp['dialogue'].map(lambda x:len(re.findall(r'\\w+', x)))","caeeab47":"hp","4ee081ff":"total_char_words = hp.groupby('character_name', as_index=False).dialogue_wordcount.sum()\ntotal_char_words = pd.DataFrame(total_char_words)\ntotal_char_words","4031217d":"sns.set_style('whitegrid')\nplt.figure(figsize=(10,7))\nsns.barplot(x='dialogue_wordcount',y='character_name', data=total_char_words, palette=\"Purples_d\", order=total_char_words.sort_values('dialogue_wordcount', ascending=False).character_name[0:20], orient='h')\nplt.xlabel('Number of words of dialogue', fontsize=15)\nplt.ylabel('Character', fontsize=15)\nplt.title('Character Importance by Number of Words of Dialogue', fontsize=20)","58d14625":"lemmatizer = WordNetLemmatizer()\nstemmer = PorterStemmer()\n\ndef preprocess(sentence):\n    sentence=str(sentence)\n    sentence = sentence.lower()\n    sentence = sentence.replace('{html}',\"\") \n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, '', sentence)\n    rem_url = re.sub(r'http\\S+', '',cleantext)\n    rem_num = re.sub('[0-9]+', '', rem_url)\n    tokenizer = RegexpTokenizer(r'\\w+')\n    tokens = tokenizer.tokenize(rem_num)  \n    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n    stem_words = [PorterStemmer().stem(w) for w in filtered_words]\n    lemma_words=[WordNetLemmatizer().lemmatize(w) for w in stem_words]\n    return \" \".join(filtered_words)","e9c5e7c4":"hp['cleanText']=hp['dialogue'].map(lambda x:preprocess(x))","0e5078aa":"common_words = Counter(\" \".join(hp[\"cleanText\"]).split()).most_common(10)\ncommon_words","181710d8":"text = \" \".join(line for line in hp[\"cleanText\"])\nwordcloud = WordCloud(width=1000, height=1000, background_color=\"white\", min_font_size=15).generate(text)\nplt.figure(figsize = (10,10))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","f84a72be":"harry = hp[hp['character_name']=='Harry Potter']","4b72dd7c":"common_harry = Counter(\" \".join(harry[\"cleanText\"]).split()).most_common(5)\ncommon_harry","86d3f4cd":"harry_text = \" \".join(line for line in harry[\"cleanText\"])\nwordcloud = WordCloud(width=1000, height=1000, background_color=\"white\", min_font_size=15).generate(harry_text)\nplt.figure(figsize = (10,10))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","a6d06f6e":"ron = hp[hp['character_name']=='Ron Weasley']","03f864b7":"common_ron = Counter(\" \".join(ron[\"cleanText\"]).split()).most_common(5)\ncommon_ron","6437b36f":"ron_text = \" \".join(line for line in ron[\"cleanText\"])\nwordcloud = WordCloud(width=1000, height=1000, background_color=\"white\", min_font_size=15).generate(ron_text)\nplt.figure(figsize = (10,10))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","fceccb3e":"hermione = hp[hp['character_name']=='Hermione Granger']","9500c509":"common_hermione = Counter(\" \".join(hermione[\"cleanText\"]).split()).most_common(5)\ncommon_hermione","2f09bd1c":"hermione_text = \" \".join(line for line in hermione[\"cleanText\"])\nwordcloud = WordCloud(width=1000, height=1000, background_color=\"white\", min_font_size=15).generate(hermione_text)\nplt.figure(figsize = (10,10))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","b87f4665":"# loading the data and adjusting to provide column names\n\ntwitter = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',encoding='cp1252', names = ['label', 'id', 'date', 'flag', 'user', 'text'])","4f9395cd":"twitter.head()","8406c7d6":"# dropping irrelevant columns\n\ntwitter = twitter.drop(['id', 'date', 'flag', 'user'], axis=1)","9eccedbf":"# Dataset contains 1,600,000 rows. This is a very large dataset, so I will take a sample 1\/4 the size of this.\n\ntwit_samp = twitter.sample(n=400000,replace=False)","e9f18e82":"# creating new column of clean text using previously defined function\n\ntwit_samp['cleanText']=twit_samp['text'].map(lambda x:preprocess(x))","8c503740":"# filtering data to only use tweets with more than two words after processing\n\ntwit_samp['clean_wordcount'] = twit_samp['cleanText'].map(lambda x:len(re.findall(r'\\w+', x)))\nfiltered_twit = twit_samp[twit_samp['clean_wordcount'] > 2]","446daf90":"x_train_samp = filtered_twit['cleanText']\ny_train_samp = filtered_twit['label']","5468224c":"y_train_samp = y_train_samp.replace(4,1)","7f81e784":"# creating training and validation sets - 90% training, 10% validation\n\nx_train_samp, x_valid_samp, y_train_samp, y_valid_samp = train_test_split(x_train_samp, y_train_samp, test_size=0.1)","8b6a12b4":"# this converts the words into vectors of numbers to allow use within models\n\ntokenizer = RegexpTokenizer(r'\\w+')\nvectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)\nfull_text = list(x_train_samp.values) + list(x_valid_samp.values)\nvectorizer.fit(full_text)\ntrain_vectorized_samp = vectorizer.transform(x_train_samp)\ntest_vectorized_samp = vectorizer.transform(x_valid_samp)","f2339338":"hp_vectorized = vectorizer.transform(hp['cleanText'])","b15bff2e":"logreg = LogisticRegression(max_iter=1000, multi_class='multinomial')","8020c49b":"logreg.fit(train_vectorized_samp, y_train_samp)","7a9af47e":"logreg.score(test_vectorized_samp, y_valid_samp)","51d6de7c":"linsvc = LinearSVC(max_iter=2000)\nlinsvc.fit(train_vectorized_samp, y_train_samp)\nlinsvc.score(test_vectorized_samp, y_valid_samp)","42000640":"multinb = MultinomialNB()\nmultinb.fit(train_vectorized_samp, y_train_samp)\nmultinb.score(test_vectorized_samp, y_valid_samp)","e78bc352":"bernb = BernoulliNB()\nbernb.fit(train_vectorized_samp, y_train_samp)\nbernb.score(test_vectorized_samp, y_valid_samp)","24f83739":"preds = logreg.predict(hp_vectorized)","80d37138":"hp['sentiment_preds'] = preds","d58efd76":"hp.head()","255d5288":"hp['clean_wordcount'] = hp['cleanText'].map(lambda x:len(re.findall(r'\\w+', x)))\nfiltered_hp = hp[hp['clean_wordcount'] > 2]\nfiltered_hp.head()","c10a7133":"# to filter the data further, I will only analyse the top 25 characters with the most lines\n\nchar_counts = filtered_hp['character_name'].value_counts()\nchar_counts = char_counts[0:25]\nchar_counts = pd.DataFrame(char_counts)\nchar_counts['count'] = char_counts['character_name']\nchar_counts['character_name'] = char_counts.index\nchar_counts = char_counts.reset_index()\nchar_counts.drop('index', axis=1)","5e45ca99":"filtered_hp = filtered_hp[filtered_hp.character_name.isin(char_counts['character_name'])]","5f9c85bd":"filtered_hp.head()","f670700b":"pos_neg_chars = filtered_hp.groupby('character_name', as_index=False).sentiment_preds.mean()\npos_neg_chars = pd.DataFrame(pos_neg_chars)\npos_neg_chars","b50fe23a":"plt.figure(figsize=(10,7))\nsns.barplot(x='sentiment_preds', y='character_name', data=pos_neg_chars, palette=\"Greens_d\", order=pos_neg_chars.sort_values('sentiment_preds', ascending=False).character_name[0:25], orient='h')\nplt.xlabel('Positivity', fontsize=15)\nplt.ylabel('Character', fontsize=15)\nplt.title('Mean Sentiment of Top 25 Characters', fontsize=20)\nplt.show()","230c6621":"scene_sent = pd.DataFrame(hp.groupby('scene', as_index=False).sentiment_preds.mean())\nplt.figure(figsize=(10,7))\nsns.lineplot(x=\"scene\", y=\"sentiment_preds\", data=scene_sent)\nplt.xlabel('Scene Number', fontsize=15)\nplt.ylabel('Mean Sentiment', fontsize=15)\nplt.title('Mean Sentiment Progression Throughout Movie', fontsize=20)\nplt.ylim(0,1)\nplt.xlim(1,34)\nplt.show()","46026282":"emotions = pd.read_csv('\/kaggle\/input\/emotion\/text_emotion.csv')\nemotions = emotions.drop(columns = ['tweet_id', 'author'])\nemotions.groupby('sentiment').count()","4f83e6b0":"# due to the very small number of anger and boredom tweets in comparison to the other emotions, I will remove these tweets\n\nemotions = emotions[emotions.sentiment.isin({'empty', 'enthusiasm', 'fun', 'happiness', 'hate', 'love', 'neutral', 'relief', 'sadness', 'surprise', 'worry'})]","b429f68c":"# converting each emotion to an integer\n\nemotions['sentiment'] = emotions['sentiment'].map({'empty':0, 'enthusiasm':1, 'fun':2, 'happiness':3, 'hate':4, 'love':5, 'neutral':6, 'relief':7, 'sadness':8, 'surprise':9, 'worry':10})\nemotions.head()","3eda381e":"emotions['cleanText']=emotions['content'].map(lambda x:preprocess(x))","809a077d":"x_train2 = emotions['cleanText']\ny_train2 = emotions['sentiment']","d4dd259d":"x_vectorized = vectorizer.transform(x_train2)","f996ca54":"smote = SMOTE()\nx_smote, y_smote = smote.fit_resample(x_vectorized, y_train2)","f2c6ec87":"train2_vectorized, test2_vectorized, y_train2, y_valid2 = train_test_split(x_smote, y_smote, test_size=0.1)","55a41aa5":"logreg2 = LogisticRegression(max_iter=500)\nlogreg2.fit(train2_vectorized, y_train2)\nlogreg2.score(test2_vectorized, y_valid2)","979759bb":"linsvc2 = LinearSVC(max_iter=800)\nlinsvc2.fit(train2_vectorized, y_train2)\nlinsvc2.score(test2_vectorized, y_valid2)","6a324e03":"multinb2 = MultinomialNB()\nmultinb2.fit(train2_vectorized, y_train2)\nmultinb2.score(test2_vectorized, y_valid2)","3d74735b":"preds2 = linsvc2.predict(hp_vectorized)","f12b49ef":"hp['emotion_preds'] = preds2","2b9353d1":"hp['emotion_preds'] = hp['emotion_preds'].map({0:'empty', 1:'enthusiasm', 2:'fun', 3:'happiness', 4:'hate', 5:'love', 6:'neutral', 7:'relief', 8:'sadness', 9:'surprise', 10:'worry'})","4fce5b5b":"total_emotions = pd.DataFrame(hp.groupby('emotion_preds', as_index=False).ID_number.count())\ntotal_emotions = total_emotions.sort_values('ID_number', ascending=False)\nplt.figure(figsize=(10,7))\nsns.barplot(y='emotion_preds', x='ID_number', data=total_emotions, palette=\"Oranges_d\", orient='h')\nplt.title('Most Common Emotions in Entire Movie', fontsize=20)\nplt.xlabel('Count', fontsize=15)\nplt.ylabel('Emotion', fontsize=15)","15a25dd1":"char_emotions = pd.DataFrame(hp.groupby('character_name').emotion_preds.value_counts())\nchar_emotions = char_emotions.rename(columns={'emotion_preds': 'counts'})\nchar_emotions = char_emotions.reset_index()\nplt.figure(figsize=(10,7))\nsns.barplot(y='emotion_preds', x='counts', data=char_emotions[char_emotions['character_name']=='Harry Potter'], palette='pink_d')\nplt.title('Harry Potter Most Common Emotions', fontsize=20)\nplt.xlabel('Count', fontsize=15)\nplt.ylabel('Emotion', fontsize=15)","eee34847":"char_emotions = pd.DataFrame(hp.groupby('character_name').emotion_preds.value_counts())\nchar_emotions = char_emotions.rename(columns={'emotion_preds': 'counts'})\nchar_emotions = char_emotions.reset_index()\nplt.figure(figsize=(10,7))\nsns.barplot(y='emotion_preds', x='counts', data=char_emotions[char_emotions['character_name']=='Ron Weasley'], palette='pink_d')\nplt.title('Ron Weasley Most Common Emotions', fontsize=20)\nplt.xlabel('Count', fontsize=15)\nplt.ylabel('Emotion', fontsize=15)","593d8e20":"char_emotions = pd.DataFrame(hp.groupby('character_name').emotion_preds.value_counts())\nchar_emotions = char_emotions.rename(columns={'emotion_preds': 'counts'})\nchar_emotions = char_emotions.reset_index()\nplt.figure(figsize=(10,7))\nsns.barplot(y='emotion_preds', x='counts', data=char_emotions[char_emotions['character_name']=='Hermione Granger'], palette='pink_d')\nplt.title('Hermione Granger Most Common Emotions', fontsize=20)\nplt.xlabel('Count', fontsize=15)\nplt.ylabel('Emotion', fontsize=15)","a363b973":"char_emotions = pd.DataFrame(hp.groupby('character_name').emotion_preds.value_counts())\nchar_emotions = char_emotions.rename(columns={'emotion_preds': 'counts'})\nchar_emotions = char_emotions.reset_index()\nplt.figure(figsize=(10,7))\nsns.barplot(y='emotion_preds', x='counts', data=char_emotions[char_emotions['character_name']=='Rubeus Hagrid'], palette='pink_d')\nplt.title('Rubeus Hagrid Most Common Emotions', fontsize=20)\nplt.xlabel('Count', fontsize=15)\nplt.ylabel('Emotion', fontsize=15)","6c06368e":"char_emotions = pd.DataFrame(hp.groupby('character_name').emotion_preds.value_counts())\nchar_emotions = char_emotions.rename(columns={'emotion_preds': 'counts'})\nchar_emotions = char_emotions.reset_index()\nplt.figure(figsize=(10,7))\nsns.barplot(y='emotion_preds', x='counts', data=char_emotions[char_emotions['character_name']=='Albus Dumbledore'], palette='pink_d')\nplt.title('Albus Dumbledore Most Common Emotions', fontsize=20)\nplt.xlabel('Count', fontsize=15)\nplt.ylabel('Emotion', fontsize=15)","9bd5ac98":"### Harry's Most Commonly Used Words","4b1266c7":"### Hermione's Most Commonly Used Words","28df5537":"## Emotion Analysis","e8a26dc4":"### Overall Most Commonly Used Words","1c5c8a7f":"### Ron's Most Commonly Used Words","1214c097":"In this section a new dataset was used, which also consists of tweets but instead of having sentiment labels 0 or 1, it has 13 emotions. Link: https:\/\/www.kaggle.com\/icw123\/emotion","8d14dddf":"So according to the validation accuracy, hopefully approx 78% of these lines of dialogue are corrently labelled. However, the data needs to be filtered before we go any further, as we can see above that 'boy' has been labelled as positive despite only being one word, and 'hagrid bringing' has also been labelled as positive despite being two words. It is hard to determine the correct sentiment from very few words, and therefore we can't trust these labels, so I will only consider processed text with more than two words in further analysis.","37ee3a61":"## Determining Character Importance by Number of Lines and Words of Dialogue","56966e45":"Linear SVC gives the best accuracy of these three models.","344bbb3a":"The following function is used to 'clean' text by removing any links, numbers, symbols and stopwords (such as 'the', 'a' etc). This is important when finding most commonly used words and when carrying out sentiment\/emotion analysis as stopwords in particular would appear most often in commonly used words and they also have no sentiment.","ebc98a71":"Thank you for taking the time to look at this notebook. I know it doesn't have much structure and really it's just me messing around and experimenting with the data and some new methods that I wasn't previously familiar with! I still see myself as a beginner so any suggestions on how to improve any of this would be very much appreciated! I plan to expand on this in the future, and add some other datasets in to explore character development and improve accuracy.","c5b1f627":"## Loading data","a09088b8":"This graph shows that the most negative point in the movie was scene 8. This scene involves Hagrid telling Harry the story of how his parents died and how he got the scar on his forehead, undoubtedly a very negative scene! However, it also shows that the most positive point was scene 29 - in this scene Ron sacrifices himself in the game of wizards' chess, which definitely isn't a positive scene. This shows that the model could benefit from some serious improvements.","e80c8d2a":"## Most Commonly Used Words Overall and by Character","0a4b43c6":"According to this graph, the most positive characters are Flitwick, Petunia and Lee Jordan with 100% positivity. At first I questioned Petunia's results, but looking at her dialogue, her vocabulary actually is mostly positive. On the other hand, the most negative characters are Dudley, Draco and Neville. 5 characters have roughly 50% positivity: Firenze, Harry, Filch, Hermione and Dumbledore. Firenze, Hermione and Dumbledore are all very wise characters with realistic points of view, so their results make sense as they are equally positive and negative. Some questionable results are those of Voldemort and Snape who both come across as negative characters in the movie, but all got highly positive results.\n\nCombining the script of this movie with scripts from other movies in the series could result in more accurate results for less important characters, provided those characters are in the other movies and that there are no major changes in the characters from one movie to the next. I might try this with the Chamber of Secrets script next!","10fccb82":"As the data is very imbalanced (759 enthusiasm tweets vs 8638 neutral) this can negatively affect the training of the model. Therefore I will use SMOTE to resample the data so that each emotion has an equal number of tweets. Regular resampling can result in overfitting, as the model will be trained on identical rows of data many times, whereas SMOTE involves the creation of new, synthetic data which is based on the existing data, so it will be similar but not identical. This helps to avoid overfitting.","87938fb5":"### Most common emotions of some of the main characters","b788784d":"Now I will calculate the mean sentiments of each of the top 25 characters, to determine which characters are the most positive, and which are the most negative.","7f240f4b":"Unsurprisingly, the most common emotion of each of these characters is in the top 2 overall most common emotions of the movie. However, Dumbledore's most common emotion, worry, leads by a huge amount, as does Hagrid's most common emotion, neutral.","78e5a19f":"Neutral and worry are the two most common emotions in the movie, whereas fun and enthusiasm are the least common. These were also two of the least common emotions in the training set so these results may not be accurate and may be a result of the dataset being imbalanced, despite having used SMOTE.","99e33732":"## Sentiment analysis","dbcd951b":"# Character Analysis in Harry Potter and the Philosopher's Stone\n\nIn this notebook, I will be using a dataset that I recently created to explore the characters in Harry Potter and the Philosopher's Stone. I'm using this as a fun way to learn some new methods including sentiment and emotion analysis.\n","36b64d1d":"Using an external dataset I will train a model to determine the sentiment of each line of dialogue from the movie. The dataset used is from \u039c\u03b1\u03c1\u03b9\u03bf\u03c2 \u039c\u03b9\u03c7\u03b1\u03b7\u03bb\u03b9\u03b4\u03b7\u03c2 KazAnova on Kaggle, link: https:\/\/www.kaggle.com\/kazanova\/sentiment140","e2782642":"Both the logistic regression and linear SVC models performed quite well, with accuracies of approx 78%. To achieve a higher accuracy, a larger sample size could be used to train the model, and a neural network could be attempted, however these options would both require a more powerful laptop than what I currently have (but not for long thankfully!!).\n\nI will use the logistic regression model to label each line of dialogue with either a 0 (negative) or 1 (positive)."}}