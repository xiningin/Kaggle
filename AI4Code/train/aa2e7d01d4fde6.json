{"cell_type":{"e8fa942a":"code","00338cd7":"code","c04f3706":"code","4a10f72c":"code","e799fc82":"code","d0407611":"code","e15f2e73":"code","0f95793c":"code","a7ac0995":"code","1af1b3f0":"code","5f25340c":"code","68a4a77e":"code","a54ecd32":"code","3980658b":"code","45d6afc1":"code","07428156":"code","4c5ff4ab":"code","37266122":"code","984cc0cd":"code","4d79ab36":"code","53904956":"code","1143dabf":"code","68d34bf8":"code","1b8d5aa7":"code","d1510a89":"code","cab0ba19":"code","c7a7163d":"code","1c08160c":"code","7891fb35":"code","c21f7b8f":"code","2b94fa40":"code","2fc817c0":"code","c87968ee":"code","37e70935":"code","6ccf6a6f":"code","d7b07cca":"code","e31c9b75":"code","8e06bc26":"code","3c796b56":"code","71715416":"code","618aacff":"markdown","6b32c7c7":"markdown","25a5cc19":"markdown","0f0f2761":"markdown","ee0e9779":"markdown","5ccea38a":"markdown","7042bf6d":"markdown","621b95b0":"markdown","24d3e1cd":"markdown","9bb9a6ab":"markdown","a3843435":"markdown","21c1dcde":"markdown"},"source":{"e8fa942a":"# data analysis libraries:\nimport numpy as np\nimport pandas as pd\n\n# data visualization libraries:\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\n# to ignore warnings:\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# to display all columns:\npd.set_option('display.max_columns', None)\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\n\n# Standard plotly imports\n\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks\nimport cufflinks as cf\nimport plotly.figure_factory as ff\nimport os\nprint(os.listdir(\"..\/input\"))\nplt.style.use('ggplot')\ncolor_pal = [x['color'] for x in plt.rcParams['axes.prop_cycle']]\n\n\n\n","00338cd7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy as sp\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Standard plotly imports\n#import plotly.plotly as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\n#import cufflinks\n#import cufflinks as cf\nimport plotly.figure_factory as ff\n\n# Using plotly + cufflinks in offline mode\ninit_notebook_mode(connected=True)\n#cufflinks.go_offline(connected=True)\n\n# Preprocessing, modelling and evaluating\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n\n## Hyperopt modules\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\nfrom functools import partial\n\nimport os\nimport gc\nprint(os.listdir(\"..\/input\"))","c04f3706":"# Read train and test data with pd.read_csv():\ntrain_id= pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv\")\ntest_id = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv\")\ntrain_tr = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv\")\ntest_tr = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv\")","4a10f72c":"## Function to reduce the DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","e799fc82":"## REducing memory\ntrain_tr = reduce_mem_usage(train_tr)\ntrain_id = reduce_mem_usage(train_id)","d0407611":"train_id.head()\n","e15f2e73":"test_id.head()","0f95793c":"train_tr.head()","a7ac0995":"test_tr.head()","1af1b3f0":"train_id.info()","5f25340c":"test_id.info()","68a4a77e":"train_tr.info()","a54ecd32":"test_tr.info()","3980658b":"print('train_transaction shape is {}'.format(train_tr.shape))\nprint('test_transaction shape is {}'.format(test_tr.shape))\nprint('train_identity shape is {}'.format(train_id.shape))\nprint('test_identity shape is {}'.format(test_id.shape))","45d6afc1":"missing_values_count = train_tr.isnull().sum()\nprint (missing_values_count[0:10])\ntotal_cells = np.product(train_tr.shape)\ntotal_missing = missing_values_count.sum()\nprint (\"% of missing data = \",(total_missing\/total_cells) * 100)","07428156":"missing_values_count = train_id.isnull().sum()\nprint (missing_values_count[0:10])\ntotal_cells = np.product(train_id.shape)\ntotal_missing = missing_values_count.sum()\nprint (\"% of missing data = \",(total_missing\/total_cells) * 100)","4c5ff4ab":"# Here we confirm that all of the transactions in `train_identity`\nprint(np.sum(train_tr['TransactionID'].isin(train_id['TransactionID'].unique())))\nprint(np.sum(test_tr['TransactionID'].isin(test_id['TransactionID'].unique())))","37266122":"print('  {:.4f}% of Transactions that are fraud in train '.format(train_tr['isFraud'].mean() * 100))","984cc0cd":"train_tr.groupby('isFraud') \\\n    .count()['TransactionID'] \\\n    .plot(kind='barh',\n          title='Is Fraud?',\n          figsize=(15, 3))\nplt.show()\n","4d79ab36":"train_tr['TransactionAmt'] \\\n    .apply(np.log) \\\n    .plot(kind='hist',\n          bins=100,\n          figsize=(15, 5),\n          title='Distribution of Log Transaction Amt')\nplt.show()","53904956":"print('Mean transaction amt for fraud is {:.4f}'.format(train_tr.loc[train_tr['isFraud'] == 1]['TransactionAmt'].mean()))\nprint('Mean transaction amt for non-fraud is {:.4f}'.format(train_tr.loc[train_tr['isFraud'] == 0]['TransactionAmt'].mean()))","1143dabf":"train_tr.groupby('ProductCD') \\\n    ['TransactionID'].count() \\\n    .sort_index() \\\n    .plot(kind='barh',\n          figsize=(20, 5),\n         title='Count of Observations by ProductCD')\nplt.show()","68d34bf8":"train_tr.groupby('ProductCD')['isFraud'] \\\n    .mean() \\\n    .sort_index() \\\n    .plot(kind='barh',\n          figsize=(15, 3),\n         title='Percentage of Fraud by ProductCD')\nplt.show()\n","1b8d5aa7":"card_cols = [c for c in train_tr.columns if 'card' in c]\ntrain_tr[card_cols].head()","d1510a89":"color_idx = 0\nfor c in card_cols:\n    if train_tr[c].dtype in ['float64','int64']:\n        train_tr[c].plot(kind='hist',\n                                      title=c,\n                                      bins=50,\n                                      figsize=(15, 2),\n                                      color=color_pal[color_idx])\n    color_idx += 1\n    plt.show()\n    ","cab0ba19":"train_tr.head()","c7a7163d":"train=pd.merge(train_tr, train_id, on = \"TransactionID\",how='left',left_index=True, right_index=True)\ntrain.head()","1c08160c":"test=pd.merge(test_tr, test_id, on = \"TransactionID\",how='left',left_index=True, right_index=True)\ntest.head()","7891fb35":"del train_id, train_tr, test_id, test_tr","c21f7b8f":"!pip install missingno\nimport missingno as msno","2b94fa40":"msno.matrix(train.iloc[:,:46]);","2fc817c0":"msno.matrix(train.iloc[:,40:80]);","c87968ee":"msno.matrix(train.iloc[:,80:120]);","37e70935":"msno.matrix(train.iloc[:,120:160]);","6ccf6a6f":"msno.matrix(train.iloc[:,160:200]);","d7b07cca":"msno.matrix(train.iloc[:,200:240]);","e31c9b75":"msno.matrix(train.iloc[:,240:280]);","8e06bc26":"msno.matrix(train.iloc[:,280:320]);","3c796b56":"msno.matrix(train.iloc[:,320:360]);","71715416":"msno.matrix(train.iloc[:,360:400]);","618aacff":"#Eksik Veri Yap\u0131s\u0131n\u0131n G\u00f6rselle\u015ftirilmesi\n!pip install missingno\nimport missingno as msno\nmsno.bar(df);\nmsno.matrix(df);\ndf\nimport seaborn as sns\ndf = sns.load_dataset('planets')\ndf.head()\ndf.isnull().sum()\nmsno.matrix(df);\nmsno.heatmap(df);\nmsno.heatmap(df);","6b32c7c7":"# ProductCD\nFor now we don't know exactly what these values represent.\nW has the most number of observations, C the least.\nProductCD C has the most fraud with >11%\nProductCD W has the least with ~2%","25a5cc19":"Missing Values in train_id DataFrame. % 35 percent data are missing","0f0f2761":"> Missing Values in train_tr DataFrame. % 41 percent data are missing","ee0e9779":"# card1 - card6\nWe are told these are all categorical, even though some appear numeric.","5ccea38a":"# train_identity","7042bf6d":"# train_transaction","621b95b0":"24.4% of TransactionIDs in train (144233 \/ 590540) have an associated train_identity.\n28.0% of TransactionIDs in test (144233 \/ 590540) have an associated train_identity.","24d3e1cd":"Distribution of Target in Training Set\n3.5% of transacations are fraud","9bb9a6ab":"1st problem: NaN\nRemember\n\nNot all transactions have corresponding identity information","a3843435":"# **TransactionAmt**\nThe ammount of transaction. I've taken a log transform in some of these plots to better show the distribution","21c1dcde":"Fraudulent charges appear to have a higher average transaction ammount\n\nMean transaction amt for fraud is **149.2444**\nMean transaction amt for non-fraud is **134.5119**"}}