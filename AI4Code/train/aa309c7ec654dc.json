{"cell_type":{"7d084a7c":"code","f39dc9a6":"code","71a8dc06":"code","b31039bb":"code","a4c6c042":"code","25386cc2":"code","67a011f4":"code","9776c2b5":"code","309f40c8":"code","568edb3f":"code","07063ae7":"code","3d53bfc5":"code","450d8985":"code","5e3092ce":"code","f441839e":"code","905f462c":"markdown","d9d5663c":"markdown","d8c5bb75":"markdown","ac8d6373":"markdown","3e6d465a":"markdown","99cab517":"markdown","fef338a2":"markdown","b0895b0d":"markdown","cea3f2a6":"markdown","23f25552":"markdown","bd9fb556":"markdown","16890c7a":"markdown"},"source":{"7d084a7c":"import numpy as np\nimport pandas as pd\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import OneHotEncoder","f39dc9a6":"df = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\")\nsample_submission = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/sample_submission.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')\n\nprint('IMPORTED DATA')\nprint('train data - ',df.head(),'sample submission - ',sample_submission.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\ndf['kfold'] = -1\nkfold = model_selection.KFold(n_splits = 5, shuffle = True)\nfor fold, ( train_indices, test_indices ) in enumerate(kfold.split(df)):\n    df.loc[test_indices,'kfold'] = fold\nprint('DATA AFTER FOLDING')\nprint(df.head(),'\\n\\n\\n\\n')\n\nobjects = [ col for col in df.columns if 'cat' in col ]\n\nencode = OrdinalEncoder()\ndf[objects] = encode.fit_transform(df[objects])\ndf_test[objects] = encode.transform(df_test[objects])\n\ndf_test.drop('id',axis = 1, inplace = True )\nprint('DATA AFTER ENCODING')\nprint('train data - ',df.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\nprint('MODEL TRAINING')\nnot_features = ['id','kfold','target']\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    \n    train_fold = df[df['kfold'] != fold ]\n    xtrain = train_fold.drop( not_features, axis = 1 )\n    ytrain = train_fold['target']\n    \n    valid_fold = df[df['kfold'] == fold ]\n    xval = valid_fold.drop( not_features, axis = 1 )\n    yval = valid_fold['target']\n    \n    model = XGBRegressor(n_jobs = -1, tree_method = 'gpu_hist', predictor = 'gpu_predictor', gpu_id = 0 )\n    model.fit(xtrain,ytrain)\n    prediction = model.predict(xval)\n    final_predictions.append(model.predict(df_test))\n    error = mean_squared_error(prediction, yval, squared = False )\n    scores.append(error)\n    print( f'Using fold { fold + 1 } as validation set we get {error} error' )\n\nprint('\\n\\n\\n\\nGENERAL MODEL SCORE')\nprint(f'Mean Scores: {np.mean(scores)}\\nStandard Deviation: {np.std(scores)}')","71a8dc06":"df = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\")\nsample_submission = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/sample_submission.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')\n\nprint('IMPORTED DATA')\nprint('train data - ',df.head(),'sample submission - ',sample_submission.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\ndf['kfold'] = -1\nkfold = model_selection.KFold(n_splits = 5, shuffle = True)\nfor fold, ( train_indices, test_indices ) in enumerate(kfold.split(df)):\n    df.loc[test_indices,'kfold'] = fold\nprint('DATA AFTER FOLDING')\nprint(df.head(),'\\n\\n\\n\\n')\n\nobjects = [ col for col in df.columns if 'cat' in col ]\n\nencode = OrdinalEncoder()\ndf[objects] = encode.fit_transform(df[objects])\ndf_test[objects] = encode.transform(df_test[objects])\n\ndf_test.drop('id',axis = 1, inplace = True )\nprint('DATA AFTER ENCODING')\nprint('train data - ',df.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\nnumerics = [ col for col in df.columns if 'cont' in col ]\n\nscaler = StandardScaler()\ndf[numerics] = scaler.fit_transform(df[numerics])\ndf_test[numerics] = scaler.transform(df_test[numerics])\nprint('DATA AFTER STANDARD SCALING')\nprint('train data - ',df.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\nprint('MODEL TRAINING')\nnot_features = ['id','kfold','target']\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    \n    train_fold = df[df['kfold'] != fold ]\n    xtrain = train_fold.drop( not_features, axis = 1 )\n    ytrain = train_fold['target']\n    \n    valid_fold = df[df['kfold'] == fold ]\n    xval = valid_fold.drop( not_features, axis = 1 )\n    yval = valid_fold['target']\n    \n    model = XGBRegressor(n_jobs = -1, tree_method = 'gpu_hist', predictor = 'gpu_predictor', gpu_id = 0 )\n    model.fit(xtrain,ytrain)\n    prediction = model.predict(xval)\n    final_predictions.append(model.predict(df_test))\n    error = mean_squared_error(prediction, yval, squared = False )\n    scores.append(error)\n    print( f'Using fold { fold + 1 } as validation set we get {error} error' )\n\nprint('\\n\\n\\n\\nGENERAL MODEL SCORE')\nprint(f'Mean Scores: {np.mean(scores)}\\nStandard Deviation: {np.std(scores)}')","b31039bb":"df = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\")\nsample_submission = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/sample_submission.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')\n\nprint('IMPORTED DATA')\nprint('train data - ',df.head(),'sample submission - ',sample_submission.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\ndf['kfold'] = -1\nkfold = model_selection.KFold(n_splits = 5, shuffle = True)\nfor fold, ( train_indices, test_indices ) in enumerate(kfold.split(df)):\n    df.loc[test_indices,'kfold'] = fold\nprint('DATA AFTER FOLDING')\nprint(df.head(),'\\n\\n\\n\\n')\n\nobjects = [ col for col in df.columns if 'cat' in col ]\n\nencode = OrdinalEncoder()\ndf[objects] = encode.fit_transform(df[objects])\ndf_test[objects] = encode.transform(df_test[objects])\n\ndf_test.drop('id',axis = 1, inplace = True )\nprint('DATA AFTER ENCODING')\nprint('train data - ',df.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\nnumerics = [ col for col in df.columns if 'cont' in col ]\n\nnorm = Normalizer()\ndf[numerics] = norm.fit_transform(df[numerics])\ndf_test[numerics] = norm.transform(df_test[numerics])\nprint('DATA AFTER NORMALIZING')\nprint('train data - ',df.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\nprint('MODEL TRAINING')\nnot_features = ['id','kfold','target']\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    \n    train_fold = df[df['kfold'] != fold ]\n    xtrain = train_fold.drop( not_features, axis = 1 )\n    ytrain = train_fold['target']\n    \n    valid_fold = df[df['kfold'] == fold ]\n    xval = valid_fold.drop( not_features, axis = 1 )\n    yval = valid_fold['target']\n    \n    model = XGBRegressor(n_jobs = -1, tree_method = 'gpu_hist', predictor = 'gpu_predictor', gpu_id = 0 )\n    model.fit(xtrain,ytrain)\n    prediction = model.predict(xval)\n    final_predictions.append(model.predict(df_test))\n    error = mean_squared_error(prediction, yval, squared = False )\n    scores.append(error)\n    print( f'Using fold { fold + 1 } as validation set we get {error} error' )\n\nprint('\\n\\n\\n\\nGENERAL MODEL SCORE')\nprint(f'Mean Scores: {np.mean(scores)}\\nStandard Deviation: {np.std(scores)}')","a4c6c042":"df = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\")\nsample_submission = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/sample_submission.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')\n\nprint('IMPORTED DATA')\nprint('train data - ',df.head(),'sample submission - ',sample_submission.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\ndf['kfold'] = -1\nkfold = model_selection.KFold(n_splits = 5, shuffle = True)\nfor fold, ( train_indices, test_indices ) in enumerate(kfold.split(df)):\n    df.loc[test_indices,'kfold'] = fold\nprint('DATA AFTER FOLDING')\nprint(df.head(),'\\n\\n\\n\\n')\n\nobjects = [ col for col in df.columns if 'cat' in col ]\n\nencode = OrdinalEncoder()\ndf[objects] = encode.fit_transform(df[objects])\ndf_test[objects] = encode.transform(df_test[objects])\n\ndf_test.drop('id',axis = 1, inplace = True )\nprint('DATA AFTER ENCODING')\nprint('train data - ',df.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\nnumerics = [ col for col in df.columns if 'cont' in col ]\n\nfor n in numerics:\n    df[n] = np.log1p(df[n])\n    df_test[n] = np.log1p(df_test[n])\n\nprint('DATA AFTER LOG TRANSFORMATION')\nprint('train data - ',df.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\nprint('MODEL TRAINING')\nnot_features = ['id','kfold','target']\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    \n    train_fold = df[df['kfold'] != fold ]\n    xtrain = train_fold.drop( not_features, axis = 1 )\n    ytrain = train_fold['target']\n    \n    valid_fold = df[df['kfold'] == fold ]\n    xval = valid_fold.drop( not_features, axis = 1 )\n    yval = valid_fold['target']\n    \n    model = XGBRegressor(n_jobs = -1, tree_method = 'gpu_hist', predictor = 'gpu_predictor', gpu_id = 0 )\n    model.fit(xtrain,ytrain)\n    prediction = model.predict(xval)\n    final_predictions.append(model.predict(df_test))\n    error = mean_squared_error(prediction, yval, squared = False )\n    scores.append(error)\n    print( f'Using fold { fold + 1 } as validation set we get {error} error' )\n\nprint('\\n\\n\\n\\nGENERAL MODEL SCORE')\nprint(f'Mean Scores: {np.mean(scores)}\\nStandard Deviation: {np.std(scores)}')","25386cc2":"df = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\")\nsample_submission = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/sample_submission.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')\n\nprint('IMPORTED DATA')\nprint('train data - ',df.head(),'sample submission - ',sample_submission.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\ndf['kfold'] = -1\nkfold = model_selection.KFold(n_splits = 5, shuffle = True)\nfor fold, ( train_indices, test_indices ) in enumerate(kfold.split(df)):\n    df.loc[test_indices,'kfold'] = fold\nprint('DATA AFTER FOLDING')\nprint(df.head(),'\\n\\n\\n\\n')\n\nobjects = [ col for col in df.columns if 'cat' in col ]\n\nencode = OrdinalEncoder()\ndf[objects] = encode.fit_transform(df[objects])\ndf_test[objects] = encode.transform(df_test[objects])\n\ndf_test.drop('id',axis = 1, inplace = True )\nprint('DATA AFTER ENCODING')\nprint('train data - ',df.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\nnumerics = [ col for col in df.columns if 'cont' in col ]\n\npoly = PolynomialFeatures(degree = 2, interaction_only = True, include_bias = False )\npol_train = poly.fit_transform(df[numerics])\ndf = pd.concat([df,pd.DataFrame(pol_train, columns = [ f'poly_{i}' for i in range(pol_train.shape[1])])], axis = 1)\npol_test = poly.fit_transform(df_test[numerics])\ndf_test = pd.concat([df_test,pd.DataFrame(pol_test,columns = [ f'poly_{i}' for i in range(pol_test.shape[1])])], axis = 1)\nprint('DATA TAKING POLYNOMIAL FEATURES')\nprint('train data - ',df.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\nprint('MODEL TRAINING')\nnot_features = ['id','kfold','target']\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    \n    train_fold = df[df['kfold'] != fold ]\n    xtrain = train_fold.drop( not_features, axis = 1 )\n    ytrain = train_fold['target']\n    \n    valid_fold = df[df['kfold'] == fold ]\n    xval = valid_fold.drop( not_features, axis = 1 )\n    yval = valid_fold['target']\n    \n    model = XGBRegressor(n_jobs = -1, tree_method = 'gpu_hist', predictor = 'gpu_predictor', gpu_id = 0 )\n    model.fit(xtrain,ytrain)\n    prediction = model.predict(xval)\n    final_predictions.append(model.predict(df_test))\n    error = mean_squared_error(prediction, yval, squared = False )\n    scores.append(error)\n    print( f'Using fold { fold + 1 } as validation set we get {error} error' )\n\nprint('\\n\\n\\n\\nGENERAL MODEL SCORE')\nprint(f'Mean Scores: {np.mean(scores)}\\nStandard Deviation: {np.std(scores)}')","67a011f4":"df = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\")\nsample_submission = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/sample_submission.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')\n\nprint('IMPORTED DATA')\nprint('train data - ',df.head(),'sample submission - ',sample_submission.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\ndf['kfold'] = -1\nkfold = model_selection.KFold(n_splits = 5, shuffle = True)\nfor fold, ( train_indices, test_indices ) in enumerate(kfold.split(df)):\n    df.loc[test_indices,'kfold'] = fold\nprint('DATA AFTER FOLDING')\nprint(df.head(),'\\n\\n\\n\\n')\n\nobjects = [ col for col in df.columns if 'cat' in col ]\n\nencode = OrdinalEncoder()\ndf[objects] = encode.fit_transform(df[objects])\ndf_test[objects] = encode.transform(df_test[objects])\n\ndf_test.drop('id',axis = 1, inplace = True )\nprint('DATA AFTER ENCODING')\nprint('train data - ',df.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\nnumerics = [ col for col in df.columns if 'cont' in col ]\n\npoly = PolynomialFeatures(degree = 3, interaction_only = True, include_bias = False )\npol_train = poly.fit_transform(df[numerics])\ndf = pd.concat([df,pd.DataFrame(pol_train, columns = [ f'poly_{i}' for i in range(pol_train.shape[1])])], axis = 1)\npol_test = poly.fit_transform(df_test[numerics])\ndf_test = pd.concat([df_test,pd.DataFrame(pol_test,columns = [ f'poly_{i}' for i in range(pol_test.shape[1])])], axis = 1)\nprint('DATA TAKING POLYNOMIAL FEATURES')\nprint('train data - ',df.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\nprint('MODEL TRAINING')\nnot_features = ['id','kfold','target']\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    \n    train_fold = df[df['kfold'] != fold ]\n    xtrain = train_fold.drop( not_features, axis = 1 )\n    ytrain = train_fold['target']\n    \n    valid_fold = df[df['kfold'] == fold ]\n    xval = valid_fold.drop( not_features, axis = 1 )\n    yval = valid_fold['target']\n    \n    model = XGBRegressor(n_jobs = -1, tree_method = 'gpu_hist', predictor = 'gpu_predictor', gpu_id = 0 )\n    model.fit(xtrain,ytrain)\n    prediction = model.predict(xval)\n    final_predictions.append(model.predict(df_test))\n    error = mean_squared_error(prediction, yval, squared = False )\n    scores.append(error)\n    print( f'Using fold { fold + 1 } as validation set we get {error} error' )\n\nprint('\\n\\n\\n\\nGENERAL MODEL SCORE')\nprint(f'Mean Scores: {np.mean(scores)}\\nStandard Deviation: {np.std(scores)}')","9776c2b5":"df = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\")\nsample_submission = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/sample_submission.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')\n\nprint('IMPORTED DATA')\nprint('train data - ',df.head(),'sample submission - ',sample_submission.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\ndf['kfold'] = -1\nkfold = model_selection.KFold(n_splits = 5, shuffle = True)\nfor fold, ( train_indices, test_indices ) in enumerate(kfold.split(df)):\n    df.loc[test_indices,'kfold'] = fold\nprint('DATA AFTER FOLDING')\nprint(df.head(),'\\n\\n\\n\\n')\n\nobjects = [ col for col in df.columns if 'cat' in col ]\n\nnumerics = [ col for col in df.columns if 'cont' in col ]\nfor n in numerics:\n    df[n],bins = pd.qcut(df[n],q=4,labels=[1,2,3,4],retbins = True)\n    bins = np.concatenate([[-np.Inf],bins[1:-1],[np.Inf]])\n    df_test[n] = pd.cut(df_test[n],bins,labels=[1,2,3,4])\n\nprint('AFTER BIN SUBSTITUTION')\nprint('train data - ',df.head(),'test data - ',df_test.head())\nprint('\\n\\n\\n\\n')\n\nencode = OrdinalEncoder()\ndf[objects + numerics ] = encode.fit_transform(df[objects + numerics ])\ndf_test[objects + numerics ] = encode.transform(df_test[objects + numerics ])\n\ndf_test.drop('id',axis = 1, inplace = True )\nprint('DATA AFTER ENCODING')\nprint('train data - ',df.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\nprint('MODEL TRAINING')\nnot_features = ['id','kfold','target']\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    \n    train_fold = df[df['kfold'] != fold ]\n    xtrain = train_fold.drop( not_features, axis = 1 )\n    ytrain = train_fold['target']\n    \n    valid_fold = df[df['kfold'] == fold ]\n    xval = valid_fold.drop( not_features, axis = 1 )\n    yval = valid_fold['target']\n    \n    model = XGBRegressor(n_jobs = -1, tree_method = 'gpu_hist', predictor = 'gpu_predictor', gpu_id = 0 )\n    model.fit(xtrain,ytrain)\n    prediction = model.predict(xval)\n    final_predictions.append(model.predict(df_test))\n    error = mean_squared_error(prediction, yval, squared = False )\n    scores.append(error)\n    print( f'Using fold { fold + 1 } as validation set we get {error} error' )\n\nprint('\\n\\n\\n\\nGENERAL MODEL SCORE')\nprint(f'Mean Scores: {np.mean(scores)}\\nStandard Deviation: {np.std(scores)}')","309f40c8":"df = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\")\nsample_submission = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/sample_submission.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')\n\nprint('IMPORTED DATA')\nprint('train data - ',df.head(),'sample submission - ',sample_submission.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\ndf['kfold'] = -1\nkfold = model_selection.KFold(n_splits = 5, shuffle = True)\nfor fold, ( train_indices, test_indices ) in enumerate(kfold.split(df)):\n    df.loc[test_indices,'kfold'] = fold\nprint('DATA AFTER FOLDING')\nprint(df.head(),'\\n\\n\\n\\n')\n\nobjects = [ col for col in df.columns if 'cat' in col ]\n\nencode = OneHotEncoder(sparse = False)\nen_train = encode.fit_transform(df[objects])\ndf = pd.concat([df.drop(objects,axis = 1),pd.DataFrame(en_train)],axis = 1)\nen_test = encode.transform(df_test[objects])\ndf_test = pd.concat([df_test.drop(objects,axis = 1),pd.DataFrame(en_test)],axis = 1)\n\ndf_test.drop('id',axis = 1, inplace = True )\nprint('DATA AFTER ENCODING')\nprint('train data - ',df.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\nprint('MODEL TRAINING')\nnot_features = ['id','kfold','target']\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    \n    train_fold = df[df['kfold'] != fold ]\n    xtrain = train_fold.drop( not_features, axis = 1 )\n    ytrain = train_fold['target']\n    \n    valid_fold = df[df['kfold'] == fold ]\n    xval = valid_fold.drop( not_features, axis = 1 )\n    yval = valid_fold['target']\n    \n    model = XGBRegressor(n_jobs = -1, tree_method = 'gpu_hist', predictor = 'gpu_predictor', gpu_id = 0 )\n    model.fit(xtrain,ytrain)\n    prediction = model.predict(xval)\n    final_predictions.append(model.predict(df_test))\n    error = mean_squared_error(prediction, yval, squared = False )\n    scores.append(error)\n    print( f'Using fold { fold + 1 } as validation set we get {error} error' )\n\nprint('\\n\\n\\n\\nGENERAL MODEL SCORE')\nprint(f'Mean Scores: {np.mean(scores)}\\nStandard Deviation: {np.std(scores)}')","568edb3f":"df = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\")\nsample_submission = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/sample_submission.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')\n\nprint('IMPORTED DATA')\nprint('train data - ',df.head(),'sample submission - ',sample_submission.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\ndf['kfold'] = -1\nkfold = model_selection.KFold(n_splits = 5, shuffle = True)\nfor fold, ( train_indices, test_indices ) in enumerate(kfold.split(df)):\n    df.loc[test_indices,'kfold'] = fold\nprint('DATA AFTER FOLDING')\nprint(df.head(),'\\n\\n\\n\\n')\n\nobjects = [ col for col in df.columns if 'cat' in col ]\n\nencode = OneHotEncoder(sparse = False)\nen_train = encode.fit_transform(df[objects])\ndf = pd.concat([df.drop(objects,axis = 1),pd.DataFrame(en_train)],axis = 1)\nen_test = encode.transform(df_test[objects])\ndf_test = pd.concat([df_test.drop(objects,axis = 1),pd.DataFrame(en_test)],axis = 1)\n\ndf_test.drop('id',axis = 1, inplace = True )\nprint('DATA AFTER ENCODING')\nprint('train data - ',df.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\nsel = [ col for col in df.columns if col not in ['kfold','target','id']]\nscaler = StandardScaler()\ndf[sel] = scaler.fit_transform(df[sel])\ndf_test[:] = scaler.transform(df_test[:])\n\nprint('DATA AFTER STANDARD SCALING')\nprint('train data - ',df.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\nprint('MODEL TRAINING')\nnot_features = ['id','kfold','target']\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    \n    train_fold = df[df['kfold'] != fold ]\n    xtrain = train_fold.drop( not_features, axis = 1 )\n    ytrain = train_fold['target']\n    \n    valid_fold = df[df['kfold'] == fold ]\n    xval = valid_fold.drop( not_features, axis = 1 )\n    yval = valid_fold['target']\n    \n    model = XGBRegressor(n_jobs = -1, tree_method = 'gpu_hist', predictor = 'gpu_predictor', gpu_id = 0 )\n    model.fit(xtrain,ytrain)\n    prediction = model.predict(xval)\n    final_predictions.append(model.predict(df_test))\n    error = mean_squared_error(prediction, yval, squared = False )\n    scores.append(error)\n    print( f'Using fold { fold + 1 } as validation set we get {error} error' )\n\nprint('\\n\\n\\n\\nGENERAL MODEL SCORE')\nprint(f'Mean Scores: {np.mean(scores)}\\nStandard Deviation: {np.std(scores)}')","07063ae7":"df = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\")\nsample_submission = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/sample_submission.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')\n\nprint('IMPORTED DATA')\nprint('train data - ',df.head(),'sample submission - ',sample_submission.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\ndf['kfold'] = -1\nkfold = model_selection.KFold(n_splits = 5, shuffle = True)\nfor fold, ( train_indices, test_indices ) in enumerate(kfold.split(df)):\n    df.loc[test_indices,'kfold'] = fold\nprint('DATA AFTER FOLDING')\nprint(df.head(),'\\n\\n\\n\\n')\n\nobjects = [ col for col in df.columns if 'cat' in col ]\n\nnumerics = [ col for col in df.columns if 'cont' in col ]\nfor n in numerics:\n    df[f'bin_{n}'],bins = pd.qcut(df[n],q=4,labels=[1,2,3,4],retbins = True)\n    bins = np.concatenate([[-np.Inf],bins[1:-1],[np.Inf]])\n    df_test[f'bin_{n}'] = pd.cut(df_test[n],bins,labels=[1,2,3,4])\n\nprint('AFTER BIN SUBSTITUTION')\nprint('train data - ',df.head(),'test data - ',df_test.head())\nprint('\\n\\n\\n\\n')\n\nencode = OneHotEncoder(sparse = False)\nen_train = encode.fit_transform(df[objects])\ndf = pd.concat([df.drop(objects,axis = 1),pd.DataFrame(en_train)],axis = 1)\nen_test = encode.transform(df_test[objects])\ndf_test = pd.concat([df_test.drop(objects,axis = 1),pd.DataFrame(en_test)],axis = 1)\nprint('DATA AFTER ENCODING')\nprint('train data - ',df.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\nscaler = StandardScaler()\ndf[numerics] = scaler.fit_transform(df[numerics])\ndf_test[numerics] = scaler.transform(df_test[numerics])\nprint('DATA AFTER STANDARD SCALING')\nprint('train data - ',df.head(),'test data - ',df_test.head(),sep = '\\n')\nprint('\\n\\n\\n\\n')\n\ndf = df.apply(pd.to_numeric)\ndf_test = df_test.apply(pd.to_numeric)\ndf_test.drop('id', axis = 1, inplace = True )\n\nprint('MODEL TRAINING')\nnot_features = ['id','kfold','target']\nfinal_predictions = []\nscores = []\nfor fold in range(5):\n    \n    train_fold = df[df['kfold'] != fold ]\n    xtrain = train_fold.drop( not_features, axis = 1 )\n    ytrain = train_fold['target']\n    \n    valid_fold = df[df['kfold'] == fold ]\n    xval = valid_fold.drop( not_features, axis = 1 )\n    yval = valid_fold['target']\n    \n    model = XGBRegressor(n_jobs = -1, tree_method = 'gpu_hist', predictor = 'gpu_predictor', gpu_id = 0 )\n    model.fit(xtrain,ytrain)\n    prediction = model.predict(xval)\n    final_predictions.append(model.predict(df_test))\n    error = mean_squared_error(prediction, yval, squared = False )\n    scores.append(error)\n    print( f'Using fold { fold + 1 } as validation set we get {error} error' )\n\nprint('\\n\\n\\n\\nGENERAL MODEL SCORE')\nprint(f'Mean Scores: {np.mean(scores)}\\nStandard Deviation: {np.std(scores)}')","3d53bfc5":"final_predictions = np.column_stack(final_predictions)\nfinal_predictions.shape","450d8985":"result = final_predictions.mean( axis = 1 )\nresult[:5]","5e3092ce":"sample_submission['target'] = result\nsample_submission.to_csv('submission.csv', index = False )","f441839e":"sample_submission.head()","905f462c":"Normalization: https:\/\/towardsdatascience.com\/data-normalization-in-machine-learning-395fdec69d02","d9d5663c":"# Ordinal Encoding, Polynomial Featuring (degree 3)","d8c5bb75":"# Ordinal Encoding, Standard Scaling","ac8d6373":"# One Hot Encoding and Standard Scaling the Whole Data Except Target","3e6d465a":"# Ordinal Encoding, Normalization","99cab517":"Standard Scaler: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html","fef338a2":"# Final Submission","b0895b0d":"# 4 Quartile binning the training data replacing the continuous values with categorical values","cea3f2a6":"Log(1+x) transformation:https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.log1p.html","23f25552":"# Ordinal Encoder, Log(1+x) Transformation","bd9fb556":"# One Hot Encoding","16890c7a":"# Ordinal Encoding, Polynomial Featuring (degree 2)"}}