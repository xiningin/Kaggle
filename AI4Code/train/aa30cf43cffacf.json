{"cell_type":{"7dd63d34":"code","e710b59f":"code","2bc55c68":"code","8ec069d9":"code","745c4c6b":"code","fa833258":"code","5f7847fa":"code","1242e5e4":"code","e7fa01f5":"code","b8a31a52":"code","da1c4804":"code","cf84cda7":"code","d6fd7d2d":"code","d4501a62":"code","2b269774":"code","eb2f5ffa":"code","b9e8e289":"code","d137781f":"code","00f0a1ee":"code","d18036a4":"code","bce5789e":"code","f17b722e":"code","1b49dea2":"code","dceaa498":"code","bc0d38db":"code","e0579263":"code","fc4f747e":"code","f2fa83fa":"code","df8b2142":"code","78d201aa":"code","5db46858":"code","9fef8d4c":"code","cdd64296":"code","c74247a0":"code","6d25b78b":"code","f5b074cc":"code","5d1a75c1":"code","9e75cc67":"code","444e7861":"code","c36219f2":"code","fc076810":"code","49f3a49e":"code","3437a0ed":"code","c486d855":"code","6f979e33":"code","8aa8c012":"code","122713d2":"code","54da6eeb":"code","f4e33511":"code","6f22f4b0":"code","08a9281e":"code","c2c6ddee":"code","d640d309":"code","f6450814":"code","1a0000fb":"markdown","67a16aba":"markdown","a87fa550":"markdown","1a7bea1c":"markdown","19e5f5e9":"markdown","f2af1ce5":"markdown","9c9af59a":"markdown","47b5a4a7":"markdown","49c6b550":"markdown","c9d80a1b":"markdown","e83a4448":"markdown","39e46e89":"markdown","b6d42477":"markdown","d03e77ac":"markdown","b8346ca2":"markdown","4f511ee0":"markdown","7d710c78":"markdown","1d51770a":"markdown","acb5bb4e":"markdown","0b46a8d1":"markdown","3c3495f7":"markdown","b628c348":"markdown","e6597428":"markdown","0bdc39c3":"markdown","52a2a938":"markdown","ccfa080e":"markdown","c76ea369":"markdown","bab97a45":"markdown","9e5e75c6":"markdown","ec4cefe2":"markdown","e79c2434":"markdown","6ce9663e":"markdown","92eb5d68":"markdown","7cf3aa18":"markdown"},"source":{"7dd63d34":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.cluster import KMeans\nfrom collections import Counter\nfrom numpy import linalg as la\n#from __future__ import print_function\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n%matplotlib inline","e710b59f":"MCR = pd.read_csv('..\/input\/multipleChoiceResponses.csv')\n#the answers of the participants\nMCR_A = MCR.iloc[1:,:]\n#the questions\nMCR_Q = MCR.iloc[0,:]","2bc55c68":"#cleaning the answers (the answers written by the interviewed are not taken into account)\npersonal_data = MCR_A.iloc[:,1:13].copy()\npersonal_data.drop(['Q1_OTHER_TEXT','Q6_OTHER_TEXT','Q7_OTHER_TEXT'], axis=1, inplace=True)\npersonal_data = personal_data.drop(list(personal_data[personal_data.Q9 == 'I do not wish to disclose my approximate yearly compensation'].index), axis=0)\npersonal_data = personal_data[~personal_data.Q9.isnull()].copy()\npersonal_data.head(3)","8ec069d9":"#It's created a column with an average of the possible intervals of incomes that were given to the interviewed \nsalary = personal_data.Q9.str.replace(',', '').str.replace('500000\\+', '500-500000').str.split('-')\npersonal_data['Q9_1'] = salary.apply(lambda x: (int(x[0])*1000 + int(x[1]))\/2)\n# Column with boolean values that says if the interviewed is in the top 20% of highest incomes.\npersonal_data['Q9_2'] = personal_data.Q9_1 > personal_data.Q9_1.quantile(0.8)","745c4c6b":"#series with all the personal data for those who recieve more than 90.000 dollars a year.\nTop_salary=personal_data.loc[list(personal_data.Q9_2[personal_data.Q9_2==True].index)]","fa833258":"#with this function it's found the frequency for every possible answers of a specific question.\ndef freq_salary(x_salary):\n    freq_x=np.array(x_salary.value_counts(dropna=False, sort=True))\n    freq_x=freq_x[1:]\n    freq_x=freq_x*100\/np.amax(freq_x) \n    ind_x=np.arange(len(freq_x))\n    return(freq_x,ind_x)","5f7847fa":"#Here are created series for men and women who have an income higher than 90.000 dollars a year. \nmale_top_salary=Top_salary.loc[list(personal_data[personal_data.Q1== 'Male'].index)]\nfemale_top_salary=Top_salary.loc[list(personal_data[personal_data.Q1== 'Female'].index)]\n#The frequencies for the different intervals of incomes are calculated, both for men and women \nfreq_male,ind_male=freq_salary(male_top_salary.Q9_1)\nfreq_female,ind_female=freq_salary(female_top_salary.Q9_1)\n#a list of string the intervals of incomes higher than 90.000 dollars is created\nL=('90-100.000','100-125.000','125-150.000','150-200.000','200-250.000',\n   ',250-300.000','300-400.000','400-500.000','500.000+')\n#the result are given in a bar plot.\nplt.figure(figsize=(15,10))\nw=0.3\nplt.xticks(ind_male,L,rotation='vertical')\nplt.bar(ind_male-w\/2,freq_male,width=w,label='Men')\nplt.bar(ind_female+w\/2,freq_female,width=w,label='Women')\nplt.xlabel('Income [Dollars]')\nplt.ylabel('Normalized Frequencies(%)')\nplt.title('Incomes for Men and Women(Top 20% of all incomes)')\nplt.legend()","1242e5e4":"L=('90-100.000','100-125.000','125-150.000','150-200.000','200-250.000',\n   ',250-300.000','300-400.000','400-500.000','500.000+')\nbusiness_top_salary=Top_salary.loc[list(personal_data[personal_data.Q5== 'A business discipline (accounting, economics, finance, etc.)'].index)]\nenviroment_top_salary=Top_salary.loc[list(personal_data[personal_data.Q5 == 'Environmental science or geology '].index)]\nphysics_top_salary=Top_salary.loc[list(personal_data[personal_data.Q5== 'Physics or astronomy'].index)]\nengineering_not_data_top_salary=Top_salary.loc[list(personal_data[personal_data.Q5 == 'Engineering (non-computer focused)'].index)]\nmath_top_salary=Top_salary.loc[list(personal_data[personal_data.Q5== 'Mathematics or statistics'].index)]\nmedical_top_salary=Top_salary.loc[list(personal_data[personal_data.Q5== 'Medical or life sciences (biology, chemistry, medicine, etc.)'].index)]\ninformation_tech_top_salary=Top_salary.loc[list(personal_data[personal_data.Q5== 'Information technology, networking, or system administration'].index)]\ncomputar_science_top_salary=Top_salary.loc[list(personal_data[personal_data.Q5== 'Computer science (software engineering, etc.)'].index)]\narts_top_salary=Top_salary.loc[list(personal_data[personal_data.Q5== 'Fine arts or performing arts'].index)]\nhumanities_top_salary=Top_salary.loc[list(personal_data[personal_data.Q5== 'Humanities (history, literature, philosophy, etc.)'].index)]\nother_top_salary=Top_salary.loc[list(personal_data[personal_data.Q5== 'Other'].index)]\nnot_declared_top_salary=Top_salary.loc[list(personal_data[personal_data.Q5== 'I never declared a major'].index)]\nsocial_sciences_top_salary=Top_salary.loc[list(personal_data[personal_data.Q5== 'Social sciences (anthropology, psychology, sociology, etc.)'].index)]\n\nfreq_business,ind_business=freq_salary(business_top_salary.Q9_1)\nfreq_physics, ind_physics=freq_salary(physics_top_salary.Q9_1)\nfreq_engineering_not_data,ind_engineering_not_data=freq_salary(engineering_not_data_top_salary.Q9_1)\nfreq_math,ind_math=freq_salary(math_top_salary.Q9_1)\nfreq_medical,ind_medical=freq_salary(medical_top_salary.Q9_1)\nfreq_information,ind_information=freq_salary(information_tech_top_salary.Q9_1)\nfreq_computar_science,ind_computar_science=freq_salary(computar_science_top_salary.Q9_1)\nfreq_arts,ind_arts=freq_salary(arts_top_salary.Q9_1)\nfreq_other,ind_other=freq_salary(other_top_salary.Q9_1)\nfreq_not_declared,ind_not_declared=freq_salary(not_declared_top_salary.Q9_1)\nfreq_social_sciences,ind_social_sciences=freq_salary(social_sciences_top_salary.Q9_1)","e7fa01f5":"w=0.135\nplt.figure(figsize=(15,10))\nplt.xticks(ind_computar_science,L,rotation='vertical')\nplt.bar(ind_engineering_not_data-w*5\/2,freq_engineering_not_data,width=w,label='Engineering (non-computer focused)')\nplt.bar(ind_math-w*4\/2,freq_math,width=w,label='Mathematics or statistics')\nplt.bar(ind_medical-w*3\/2,freq_medical,width=w,label='Medical or life sciences (biology, chemistry, medicine, etc.)')\nplt.bar(ind_information-w*2\/2,freq_information,width=w,label='Information technology, networking, or system administration')\nplt.bar(ind_computar_science-w*1\/2,freq_computar_science,width=w,label='Computer science (software engineering, etc.)')\nplt.bar(ind_arts+w*1\/2,freq_arts,width=w,label='Fine arts or performing arts')\nplt.bar(ind_business+w*2\/2,freq_business,width=w,label='A business discipline (accounting, economics, finance, etc.)')\nplt.bar(ind_physics+w*3\/2,freq_physics,width=w,label='Physics or astronomy')\nplt.bar(ind_other+w*4\/2,freq_other,width=w,label='Other')\nplt.bar(ind_not_declared+w*5\/2,freq_not_declared,width=w,label='I never declared a major')\nplt.bar(ind_social_sciences+w*6\/2,freq_social_sciences,width=w,label='Social sciences (anthropology, psychology, sociology, etc.)')\nplt.xlabel('Income [Dollars]')\nplt.ylabel('Normalized Frequencies(%)')\nplt.title('Income and Major (Top 20% of all incomes)')\nplt.legend()","b8a31a52":"L=('0-10,000', '10-20,000', '20-30,000', '30-40,000', '40-50,000','50-60,000', '60-70,000', '70-80,000','80-90,000',\n   '90-100,000','100-125,000', '125-150,000', '150-200,000', '200-250,000', '250-300,000','300-400,000', '400-500,000',\n   '500,000+')\na_salary=personal_data.loc[list(personal_data[personal_data.Q2 == '18-21'].index)]\nb_salary=personal_data.loc[list(personal_data[personal_data.Q2 == '22-24'].index)]\nc_salary=personal_data.loc[list(personal_data[personal_data.Q2 == '25-29'].index)]\nd_salary=personal_data.loc[list(personal_data[personal_data.Q2 == '30-34'].index)]\ne_salary=personal_data.loc[list(personal_data[personal_data.Q2 == '35-39'].index)]\nf_salary=personal_data.loc[list(personal_data[personal_data.Q2 == '40-44'].index)]\ng_salary=personal_data.loc[list(personal_data[personal_data.Q2 == '45-49'].index)]\nh_salary=personal_data.loc[list(personal_data[personal_data.Q2 == '50-54'].index)]\ni_salary=personal_data.loc[list(personal_data[personal_data.Q2 == '55-59'].index)]\nj_salary=personal_data.loc[list(personal_data[personal_data.Q2 == '60-69'].index)]\nk_salary=personal_data.loc[list(personal_data[personal_data.Q2 == '70-79'].index)]\nl_salary=personal_data.loc[list(personal_data[personal_data.Q2 == '80+'].index)]\n\nfreq_a,ind_a =freq_salary(a_salary.Q9_1)\nfreq_b,ind_b =freq_salary(b_salary.Q9_1)\nfreq_c,ind_c =freq_salary(c_salary.Q9_1)\nfreq_d,ind_d =freq_salary(d_salary.Q9_1)\nfreq_e,ind_e =freq_salary(e_salary.Q9_1)\nfreq_f,ind_f =freq_salary(f_salary.Q9_1)\nfreq_g,ind_g =freq_salary(g_salary.Q9_1)\nfreq_h,ind_h =freq_salary(h_salary.Q9_1)\nfreq_i,ind_i =freq_salary(i_salary.Q9_1)\nfreq_j,ind_j =freq_salary(j_salary.Q9_1)\nfreq_k,ind_k =freq_salary(k_salary.Q9_1)\nfreq_l,ind_l =freq_salary(l_salary.Q9_1)","da1c4804":"w=0.1\nplt.figure(figsize=(20,10))\nplt.xticks(ind_b,L,rotation='vertical')\nplt.bar(ind_a-w*6\/2,freq_a,width=w,label='18-21')\nplt.bar(ind_b-w*5\/2,freq_b,width=w,label='22-24')\nplt.bar(ind_c-w*4\/2,freq_c,width=w,label='25-29')\nplt.bar(ind_d-w*3\/2,freq_d,width=w,label='30-34')\nplt.bar(ind_e-w*2\/2,freq_e,width=w,label='35-39')\nplt.bar(ind_f-w*1\/2,freq_f,width=w,label='40-44')\nplt.bar(ind_g+w*1\/2,freq_g,width=w,label='45-49')\nplt.bar(ind_h+w*2\/2,freq_h,width=w,label='50-54')\nplt.bar(ind_i+w*3\/2,freq_i,width=w,label='55-59')\nplt.bar(ind_j+w*4\/2,freq_j,width=w,label='60-69')\nplt.bar(ind_k+w*5\/2,freq_k,width=w,label='70-79')\nplt.bar(ind_l+w*6\/2,freq_l,width=w,label='80+')\n\nplt.xlabel('Income [Dollars]')\nplt.ylabel('Normalized Frequencies(%)')\nplt.title('Income and Age')\nplt.legend()\n","cf84cda7":"energy_salary=Top_salary.loc[list(personal_data[personal_data.Q7== 'Energy\/Mining'].index)]\ngoverment_salary=Top_salary.loc[list(personal_data[personal_data.Q7 == 'Government\/Public Service'].index)]\nshipping_salary=Top_salary.loc[list(personal_data[personal_data.Q7== 'Shipping\/Transportation'].index)]\nother_salary=Top_salary.loc[list(personal_data[personal_data.Q7 == 'Other'].index)]\nacademics_salary=Top_salary.loc[list(personal_data[personal_data.Q7== 'Academics\/Education'].index)]\nonline_serv_salary=Top_salary.loc[list(personal_data[personal_data.Q7== 'Online Service\/Internet-based Services'].index)]\ncomputers_salary=Top_salary.loc[list(personal_data[personal_data.Q7== 'Computers\/Technology'].index)]\nsports_salary=Top_salary.loc[list(personal_data[personal_data.Q7== 'Hospitality\/Entertainment\/Sports'].index)]\nmarketing_salary=Top_salary.loc[list(personal_data[personal_data.Q7== 'Marketing\/CRM'].index)]\ninsurance_salary=Top_salary.loc[list(personal_data[personal_data.Q7== 'Insurance\/Risk Assessment'].index)]\nonline_buss_salary=Top_salary.loc[list(personal_data[personal_data.Q7== 'Online Business\/Internet-based Sales'].index)]\nbroadcasting_salary=Top_salary.loc[list(personal_data[personal_data.Q7== 'Broadcasting\/Communications'].index)]\nsales_salary=Top_salary.loc[list(personal_data[personal_data.Q7== 'Retail\/Sales'].index)]\naccounting_salary=Top_salary.loc[list(personal_data[personal_data.Q7== 'Accounting\/Finance'].index)]\nmilitary_salary=Top_salary.loc[list(personal_data[personal_data.Q7== 'Military\/Security\/Defense'].index)]\nservice_salary=Top_salary.loc[list(personal_data[personal_data.Q7== 'Non-profit\/Service'].index)]\nstudent_salary=Top_salary.loc[list(personal_data[personal_data.Q7== 'I am a student'].index)]\nfabrication_salary=Top_salary.loc[list(personal_data[personal_data.Q7== 'Manufacturing\/Fabrication'].index)]\nmedical_salary=Top_salary.loc[list(personal_data[personal_data.Q7== 'Medical\/Pharmaceutical'].index)]\n\nfreq_energy,ind_energy =freq_salary(energy_salary.Q9_1)\nfreq_goverment,ind_goverment =freq_salary(goverment_salary.Q9_1)\nfreq_shipping,ind_shipping =freq_salary(shipping_salary.Q9_1)\nfreq_other,ind_other=freq_salary(other_salary.Q9_1)\nfreq_academics,ind_academics =freq_salary(academics_salary.Q9_1)\nfreq_online_serv,ind_online_serv =freq_salary(online_serv_salary.Q9_1)\nfreq_computers,ind_computers =freq_salary(computers_salary.Q9_1)\nfreq_sports,ind_sports=freq_salary(sports_salary.Q9_1)\nfreq_marketing,ind_marketing =freq_salary(marketing_salary.Q9_1)\nfreq_insurance,ind_insurance =freq_salary(insurance_salary.Q9_1)\nfreq_online_buss,ind_online_buss =freq_salary(online_buss_salary.Q9_1)\nfreq_broadcasting,ind_broadcasting =freq_salary(broadcasting_salary.Q9_1)\nfreq_sales,ind_sales =freq_salary(sales_salary.Q9_1)\nfreq_accounting,ind_accounting =freq_salary(accounting_salary.Q9_1)\nfreq_military,ind_military=freq_salary(military_salary.Q9_1)\nfreq_service,ind_service =freq_salary(service_salary.Q9_1)\nfreq_student,ind_student =freq_salary(student_salary.Q9_1)\nfreq_fabrication,ind_fabrication =freq_salary(fabrication_salary.Q9_1)\nfreq_medical,ind_medical =freq_salary(medical_salary.Q9_1)\n","d6fd7d2d":"L=('90-100.000','100-125.000','125-150.000','150-200.000','200-250.000',\n   ',250-300.000','300-400.000','400-500.000','500.000+')\nw=0.15\nplt.figure(figsize=(20,10))\nplt.xticks(ind_computers,L,rotation='vertical')\nplt.bar(ind_goverment-w*5\/2,freq_goverment,width=w,label='Government\/Public Service')\nplt.bar(ind_shipping-w*4\/2,freq_shipping,width=w,label='Shipping\/Transportation')\nplt.bar(ind_online_serv-w*3\/2,freq_online_serv,width=w,label='Other')\nplt.bar(ind_other-w*2\/2,freq_other,width=w,label='Academics\/Education')\nplt.bar(ind_academics-w*1\/2,freq_academics,width=w,label='Online Service\/Internet-based Services')\nplt.bar(ind_computers+w*1\/2,freq_computers,width=w,label='Computers\/Technology')\nplt.bar(ind_sports+w*2\/2,freq_sports,width=w,label='Hospitality\/Entertainment\/Sports')\nplt.bar(ind_marketing+w*3\/2,freq_marketing,width=w,label='Marketing\/CRM')\nplt.bar(ind_insurance+w*4\/2,freq_insurance,width=w,label='Insurance\/Risk Assessment')\nplt.bar(ind_online_buss+w*5\/2,freq_online_buss,width=w,label='Online Business\/Internet-based Sales')\nplt.xlabel('Income [Dollars]')\nplt.ylabel('Normalized Frequencies(%)')\nplt.title('Income and Industries(Top 20% of all incomes)')\nplt.legend()","d4501a62":"L=('90-100.000','100-125.000','125-150.000','150-200.000','200-250.000',\n   ',250-300.000','300-400.000','400-500.000','500.000+')\nw=0.17\nplt.figure(figsize=(20,10))\nplt.xticks(ind_b,L,rotation='vertical')\nplt.bar(ind_sales-w*4\/2,freq_sales,width=w,label='Retail\/Sales')\nplt.bar(ind_accounting-w*3\/2,freq_accounting,width=w,label='Accounting\/Finance')\nplt.bar(ind_military-w*2\/2,freq_military,width=w,label='Military\/Security\/Defense')\nplt.bar(ind_service-w*1\/2,freq_service,width=w,label='Non-profit\/Service')\nplt.bar(ind_energy+w*1\/2,freq_energy,width=w,label='Energy\/Mining')\nplt.bar(ind_broadcasting+w*2\/2,freq_broadcasting,width=w,label='Broadcasting\/Communications')\nplt.bar(ind_student+w*3\/2,freq_student,width=w,label='I am a student')\nplt.bar(ind_fabrication+w*4\/2,freq_fabrication,width=w,label='Manufacturing\/Fabrication')\nplt.bar(ind_medical+w*5\/2,freq_medical,width=w,label='Medical\/Pharmaceutical')\nplt.xlabel('Income [Dollars]')\nplt.ylabel('Normalized Frequencies(%)')\nplt.title('Income and Industries(Top 20% of all incomes)')\nplt.legend()","2b269774":"prof_salary=personal_data.loc[list(personal_data[personal_data.Q4== 'Professional degree'].index)]\nmast_salary=personal_data.loc[list(personal_data[personal_data.Q4 == 'Master\u2019s degree'].index)]\nbach_salary=personal_data.loc[list(personal_data[personal_data.Q4== 'Bachelor\u2019s degree'].index)]\nnofor_salary=personal_data.loc[list(personal_data[personal_data.Q4 == 'No formal education past high school'].index)]\ndoc_salary=personal_data.loc[list(personal_data[personal_data.Q4== 'Doctoral degree'].index)]\nsomecol_salary=personal_data.loc[list(personal_data[personal_data.Q4== 'Some college\/university study without earning a bachelor\u2019s degree'].index)]\n\nfreq_prof,ind_prof =freq_salary(prof_salary.Q9_1)\nfreq_mast,ind_mast =freq_salary(mast_salary.Q9_1)\nfreq_bach,ind_bach =freq_salary(bach_salary.Q9_1)\nfreq_nofor,ind_nofor=freq_salary(nofor_salary.Q9_1)\nfreq_doc,ind_doc =freq_salary(doc_salary.Q9_1)\nfreq_somecol,ind_somecol =freq_salary(somecol_salary.Q9_1)\n\nL=('0-10,000', '10-20,000', '20-30,000', '30-40,000', '40-50,000','50-60,000', '60-70,000', '70-80,000','80-90,000',\n   '90-100,000','100-125,000', '125-150,000', '150-200,000', '200-250,000', '250-300,000','300-400,000', '400-500,000',\n   '500,000+')\nw=0.2\nplt.figure(figsize=(20,10))\nplt.xticks(ind_b,L,rotation='vertical')\n\nplt.bar(ind_doc-w*3\/2,freq_doc,width=w,label='Doctoral degree')\nplt.bar(ind_prof-w*2\/2,freq_prof,width=w,label='Professional degree')\nplt.bar(ind_mast-w*1\/2,freq_mast,width=w,label='Master\u2019s degree')\nplt.bar(ind_bach+w*1\/2,freq_bach,width=w,label='Bachelor\u2019s degree')\nplt.bar(ind_nofor+w*2\/2,freq_nofor,width=w,label='No formal education past high school')\nplt.bar(ind_somecol+w*3\/2,freq_somecol,width=w,label='Some college\/university study without earning a bachelor\u2019s degree')\n\n\nplt.xlabel('Income [Dollars]')\nplt.ylabel('Normalized Frequencies(%)')\nplt.title('Income and Level of Education')\nplt.legend()","eb2f5ffa":"L=('90-100,000','100-125,000', '125-150,000', '150-200,000', '200-250,000', '250-300,000','300-400,000', '400-500,000',\n   '500,000+')\na_salary=Top_salary.loc[list(personal_data[personal_data.Q8 == '15-20'].index)]\nb_salary=Top_salary.loc[list(personal_data[personal_data.Q8 == '0-1'].index)]\nc_salary=Top_salary.loc[list(personal_data[personal_data.Q8 == '5-10'].index)]\nd_salary=Top_salary.loc[list(personal_data[personal_data.Q8 == '10-15'].index)]\ne_salary=Top_salary.loc[list(personal_data[personal_data.Q8 == '30 +'].index)]\nf_salary=Top_salary.loc[list(personal_data[personal_data.Q8 == '2-3'].index)]\ng_salary=Top_salary.loc[list(personal_data[personal_data.Q8 == '20-25'].index)]\nh_salary=Top_salary.loc[list(personal_data[personal_data.Q8 == '3-4'].index)]\ni_salary=Top_salary.loc[list(personal_data[personal_data.Q8 == '1-2'].index)]\nj_salary=Top_salary.loc[list(personal_data[personal_data.Q8 == '4-5'].index)]\nk_salary=Top_salary.loc[list(personal_data[personal_data.Q8 == '25-30'].index)]\n\nfreq_a,ind_a =freq_salary(a_salary.Q9_1)\nfreq_b,ind_b =freq_salary(b_salary.Q9_1)\nfreq_c,ind_c =freq_salary(c_salary.Q9_1)\nfreq_d,ind_d =freq_salary(d_salary.Q9_1)\nfreq_e,ind_e =freq_salary(e_salary.Q9_1)\nfreq_f,ind_f =freq_salary(f_salary.Q9_1)\nfreq_g,ind_g =freq_salary(g_salary.Q9_1)\nfreq_h,ind_h =freq_salary(h_salary.Q9_1)\nfreq_i,ind_i =freq_salary(i_salary.Q9_1)\nfreq_j,ind_j =freq_salary(j_salary.Q9_1)\nfreq_k,ind_k =freq_salary(k_salary.Q9_1)","b9e8e289":"w=0.2\nplt.figure(figsize=(20,10))\nplt.xticks(ind_e,L,rotation='vertical')\n\nplt.bar(ind_b-w*3\/2,freq_b,width=w,label='0-1')\nplt.bar(ind_i-w*2\/2,freq_i,width=w,label='1-2')\nplt.bar(ind_f-w*1\/2,freq_f,width=w,label='2-3')\nplt.bar(ind_h+w*1\/2,freq_h,width=w,label='3-4')\nplt.bar(ind_j+w*2\/2,freq_j,width=w,label='4-5')\nplt.bar(ind_c+w*3\/2,freq_c,width=w,label='5-10')\n\n\nplt.xlabel('Income [Dollars]')\nplt.ylabel('Normalized Frequencies(%)')\nplt.title('Income and Years of Experience(Top 20% of all incomes)')\nplt.legend()","d137781f":"w=0.2\nplt.figure(figsize=(20,10))\nplt.xticks(ind_e,L,rotation='vertical')\nplt.bar(ind_d-w*3\/2,freq_d,width=w,label='10-15')\nplt.bar(ind_a-w*2\/2,freq_a,width=w,label='15-20')\nplt.bar(ind_g-w*1\/2,freq_g,width=w,label='20-25')\nplt.bar(ind_k+w*1\/2,freq_k,width=w,label='25-30')\nplt.bar(ind_e+w*2\/2,freq_e,width=w,label='30 +')\nplt.xlabel('Income [Dollars]')\nplt.ylabel('Normalized Frequencies(%)')\nplt.title('Income and Years of Experience(Top 20% of all incomes)')\nplt.legend()","00f0a1ee":"#Assigning dummies for the different answers of the personal data\ndummiesQ1 = pd.get_dummies(personal_data['Q1'], prefix='Q1')\ndummiesQ2 = pd.get_dummies(personal_data['Q2'], prefix='Q2')\ndummiesQ4 = pd.get_dummies(personal_data['Q4'], prefix='Q4')\ndummiesQ5 = pd.get_dummies(personal_data['Q5'], prefix='Q5')\ndummiesQ6 = pd.get_dummies(personal_data['Q6'], prefix='Q6')\ndummiesQ8 = pd.get_dummies(personal_data['Q8'], prefix='Q8')\ndummiesQ9 = pd.get_dummies(personal_data['Q9'], prefix='Q9')\n#All the dummies together\ndata = pd.concat([dummiesQ1,dummiesQ2,dummiesQ4,dummiesQ5,dummiesQ8,dummiesQ9], axis=1)\n#All the possible answers for all the personal data questions\nx_features = data.columns[:]","d18036a4":"#Function to calculate projection of vectors\ndef proy(a,b):\n    return np.dot(a,b)\/np.sqrt(np.dot(a,a))","bce5789e":"cov_matrix = np.array(data[x_features].cov())\nval, vec = la.eig(cov_matrix)\nvec = vec[:,val.argsort()[::-1]]\nval = val[val.argsort()[::-1]]\nvec1 = vec[:,0]\nvec2 = -vec[:,1]","f17b722e":"#New coordinates of the data\nnew_c = []\nlabels = []\nfor index, rows in data.iterrows():\n    fila = rows[x_features].values\n    new_c.append([proy(vec1,fila),proy(vec2,fila)])","1b49dea2":"new_c=np.array(new_c)\nplt.scatter(new_c[:,0],new_c[:,1])\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')\n","dceaa498":"#Kmeans for 11 clusters\ncluster11 = KMeans(n_clusters=11, random_state=10)\ncluster_labels = cluster11.fit_predict(data[x_features])\ndata['clusters'] = cluster_labels","bc0d38db":"data[data.clusters==8].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))\ndata[data.clusters==6].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))","e0579263":"from __future__ import print_function\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score","fc4f747e":"range_n_clusters = np.arange(94,99)\nX = data[x_features]\nscore = []\nfor n_clusters in range_n_clusters:\n    \n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    score.append(silhouette_avg)\n    print(\"For n_clusters =\", n_clusters,\n          \"The average silhouette_score is :\", silhouette_avg)\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)","f2fa83fa":"plt.plot(range_n_clusters,score)\nplt.xlabel('K')\nplt.ylabel('silhouette score')\nplt.grid()","df8b2142":"cluster97 = KMeans(n_clusters=97, random_state=10)\ncluster_labels = cluster97.fit_predict(data[x_features])\ndata['clusters'] = cluster_labels","78d201aa":"data[data.clusters==12].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))\ndata[data.clusters==33].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))\ndata[data.clusters==67].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))\ndata[data.clusters==88].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))","5db46858":"Q5=MCR_A.iloc[:,6].copy()\nQ6=MCR_A.iloc[:,7].copy()\nQ9=MCR_A.iloc[:,12].copy()\nQ9_1=personal_data.iloc[:,9].copy()\nQ9_2=personal_data.iloc[:,10].copy()\n#Selecting the answers for the computational knowledge questions\nQ10=MCR_A.iloc[:,13].copy()\nQ23=MCR_A.iloc[:,126].copy()\nQ24=MCR_A.iloc[:,127].copy()\nQ25=MCR_A.iloc[:,128].copy()\nQ26=MCR_A.iloc[:,129].copy()\nQ32=MCR_A.iloc[:,263].copy()\nQ37=MCR_A.iloc[:,305].copy()\nQ11=MCR_A.iloc[:,14:21].copy()\nQ36=MCR_A.iloc[:,291:304].copy()\nQ38=MCR_A.iloc[:,307:329].copy()\nQ44=MCR_A.iloc[:,343:349].copy()\nQ45=MCR_A.iloc[:,349:355].copy()\nQ47=MCR_A.iloc[:,356:372].copy()","9fef8d4c":"#Here a create dummies for all the possible answers for each of the questions a named before\ndummiesQ5 = pd.get_dummies(MCR_A['Q5'], prefix='Q5')\ndummiesQ6 = pd.get_dummies(MCR_A['Q6'], prefix='Q6')\ndummiesQ9 = pd.get_dummies(MCR_A['Q9'], prefix='Q9')\ndummiesQ9_1 = pd.get_dummies(personal_data['Q9_1'], prefix='Q9_1')\ndummiesQ9_2 = pd.get_dummies(personal_data['Q9_2'], prefix='Q9_2')\ndummiesQ10 = pd.get_dummies(MCR_A['Q10'], prefix='Q10')\ndummiesQ23 = pd.get_dummies(MCR_A['Q23'], prefix='Q23')\ndummiesQ24 = pd.get_dummies(MCR_A['Q24'], prefix='Q24')\ndummiesQ25 = pd.get_dummies(MCR_A['Q25'], prefix='Q25')\ndummiesQ26 = pd.get_dummies(MCR_A['Q26'], prefix='Q26')\ndummiesQ32 = pd.get_dummies(MCR_A['Q32'], prefix='Q32')\ndummiesQ37 = pd.get_dummies(MCR_A['Q37'], prefix='Q37')\ndummiesQ40 = pd.get_dummies(MCR_A['Q40'], prefix='Q40')\ndummiesQ11_1 = pd.get_dummies(MCR_A['Q11_Part_1'], prefix='Q11')\ndummiesQ11_2 = pd.get_dummies(MCR_A['Q11_Part_2'], prefix='Q11')\ndummiesQ11_3 = pd.get_dummies(MCR_A['Q11_Part_3'], prefix='Q11')\ndummiesQ11_4 = pd.get_dummies(MCR_A['Q11_Part_4'], prefix='Q11')\ndummiesQ11_5 = pd.get_dummies(MCR_A['Q11_Part_5'], prefix='Q11')\ndummiesQ11_6 = pd.get_dummies(MCR_A['Q11_Part_6'], prefix='Q11')\ndummiesQ11_7 = pd.get_dummies(MCR_A['Q11_Part_7'], prefix='Q11')\ndummiesQ36_1 = pd.get_dummies(MCR_A['Q36_Part_1'], prefix='Q36')\ndummiesQ36_2 = pd.get_dummies(MCR_A['Q36_Part_2'], prefix='Q36')\ndummiesQ36_3 = pd.get_dummies(MCR_A['Q36_Part_3'], prefix='Q36')\ndummiesQ36_4 = pd.get_dummies(MCR_A['Q36_Part_4'], prefix='Q36')\ndummiesQ36_5 = pd.get_dummies(MCR_A['Q36_Part_5'], prefix='Q36')\ndummiesQ36_6 = pd.get_dummies(MCR_A['Q36_Part_6'], prefix='Q36')\ndummiesQ36_7 = pd.get_dummies(MCR_A['Q36_Part_7'], prefix='Q36')\ndummiesQ36_8 = pd.get_dummies(MCR_A['Q36_Part_8'], prefix='Q36')\ndummiesQ36_9 = pd.get_dummies(MCR_A['Q36_Part_9'], prefix='Q36')\ndummiesQ36_10 = pd.get_dummies(MCR_A['Q36_Part_10'], prefix='Q36')\ndummiesQ36_11 = pd.get_dummies(MCR_A['Q36_Part_11'], prefix='Q36')\ndummiesQ36_12 = pd.get_dummies(MCR_A['Q36_Part_12'], prefix='Q36')\ndummiesQ36_13 = pd.get_dummies(MCR_A['Q36_Part_13'], prefix='Q36')\ndummiesQ44_1 = pd.get_dummies(MCR_A['Q44_Part_1'], prefix='Q44')\ndummiesQ44_2 = pd.get_dummies(MCR_A['Q44_Part_2'], prefix='Q44')\ndummiesQ44_3 = pd.get_dummies(MCR_A['Q44_Part_3'], prefix='Q44')\ndummiesQ44_4 = pd.get_dummies(MCR_A['Q44_Part_4'], prefix='Q44')\ndummiesQ44_5 = pd.get_dummies(MCR_A['Q44_Part_5'], prefix='Q44')\ndummiesQ44_6 = pd.get_dummies(MCR_A['Q44_Part_6'], prefix='Q44')\ndummiesQ45_1 = pd.get_dummies(MCR_A['Q45_Part_1'], prefix='Q45')\ndummiesQ45_2 = pd.get_dummies(MCR_A['Q45_Part_2'], prefix='Q45')\ndummiesQ45_3 = pd.get_dummies(MCR_A['Q45_Part_3'], prefix='Q45')\ndummiesQ45_4 = pd.get_dummies(MCR_A['Q45_Part_4'], prefix='Q45')\ndummiesQ45_5 = pd.get_dummies(MCR_A['Q45_Part_5'], prefix='Q45')\ndummiesQ45_6 = pd.get_dummies(MCR_A['Q45_Part_6'], prefix='Q45')\ndummiesQ47_1 = pd.get_dummies(MCR_A['Q47_Part_1'], prefix='Q47')\ndummiesQ47_2 = pd.get_dummies(MCR_A['Q47_Part_2'], prefix='Q47')\ndummiesQ47_3 = pd.get_dummies(MCR_A['Q47_Part_3'], prefix='Q47')\ndummiesQ47_4 = pd.get_dummies(MCR_A['Q47_Part_4'], prefix='Q47')\ndummiesQ47_5 = pd.get_dummies(MCR_A['Q47_Part_5'], prefix='Q47')\ndummiesQ47_6 = pd.get_dummies(MCR_A['Q47_Part_6'], prefix='Q47')\ndummiesQ47_7 = pd.get_dummies(MCR_A['Q47_Part_7'], prefix='Q47')\ndummiesQ47_8 = pd.get_dummies(MCR_A['Q47_Part_8'], prefix='Q47')\ndummiesQ47_9 = pd.get_dummies(MCR_A['Q47_Part_9'], prefix='Q47')\ndummiesQ47_10 = pd.get_dummies(MCR_A['Q47_Part_10'], prefix='Q47')\ndummiesQ47_11 = pd.get_dummies(MCR_A['Q47_Part_11'], prefix='Q47')\ndummiesQ47_12 = pd.get_dummies(MCR_A['Q47_Part_12'], prefix='Q47')\ndummiesQ47_13 = pd.get_dummies(MCR_A['Q47_Part_13'], prefix='Q47')\ndummiesQ47_14 = pd.get_dummies(MCR_A['Q47_Part_14'], prefix='Q47')\ndummiesQ47_15 = pd.get_dummies(MCR_A['Q47_Part_15'], prefix='Q47')\ndummiesQ47_16 = pd.get_dummies(MCR_A['Q47_Part_16'], prefix='Q47')\ndummiesQ47=pd.concat([dummiesQ47_1,dummiesQ47_2,dummiesQ47_3,dummiesQ47_4,dummiesQ47_5,dummiesQ47_6,dummiesQ47_7,dummiesQ47_8,dummiesQ47_9,dummiesQ47_10,dummiesQ47_11,dummiesQ47_12,dummiesQ47_13], axis=1)\ndummiesQ45=pd.concat([dummiesQ45_1,dummiesQ45_2,dummiesQ45_3,dummiesQ45_4,dummiesQ45_5,dummiesQ45_6], axis=1)\ndummiesQ44=pd.concat([dummiesQ44_1,dummiesQ44_2,dummiesQ44_3,dummiesQ44_4,dummiesQ44_5,dummiesQ44_6], axis=1)\ndummiesQ36=pd.concat([dummiesQ36_1,dummiesQ36_2,dummiesQ36_3,dummiesQ36_4,dummiesQ36_5,dummiesQ36_6,dummiesQ36_7,dummiesQ36_8,dummiesQ36_9,dummiesQ36_10,dummiesQ36_11,dummiesQ36_12,dummiesQ36_13], axis=1)\ndummiesQ11=pd.concat([dummiesQ11_1,dummiesQ11_2,dummiesQ11_3,dummiesQ11_4,dummiesQ11_5,dummiesQ11_6,dummiesQ11_7], axis=1)","cdd64296":"#Here I gruped the dummies in the subcategories\ndata1 = pd.concat([dummiesQ5,dummiesQ6,dummiesQ9_1,dummiesQ10,dummiesQ11], axis=1)\ndata2 = pd.concat([dummiesQ5,dummiesQ6,dummiesQ9_1,dummiesQ23,dummiesQ24,dummiesQ25,dummiesQ26], axis=1)\ndata3 = pd.concat([dummiesQ5,dummiesQ6,dummiesQ36,dummiesQ37], axis=1)\ndata4 = pd.concat([dummiesQ5,dummiesQ6,dummiesQ44,dummiesQ45,dummiesQ47], axis=1)\ndata1 = data1.dropna()\ndata2 = data2.dropna()\ndata3 = data3.dropna()\ndata4 = data4.dropna()\nx_features1 = data1.columns[:]\nx_features2 = data2.columns[:]\nx_features3 = data3.columns[:]\nx_features4 = data4.columns[:]","c74247a0":"cov_matrix = np.array(data1[x_features1].cov())\nval, vec = la.eig(cov_matrix)\nvec = vec[:,val.argsort()[::-1]]\nval = val[val.argsort()[::-1]]\nvec1 = vec[:,0]\nvec2 = -vec[:,1]\nnew_c = []\nlabels = []\nfor index, rows in data1.iterrows():\n    fila = rows[x_features1].values\n    new_c.append([proy(vec1,fila),proy(vec2,fila)])\nnew_c=np.array(new_c)\nplt.plot(new_c[:,0],new_c[:,1],'.',)\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')","6d25b78b":"range_n_clusters = np.arange(88,91)\nX = data1[x_features1]\nscore = []\nfor n_clusters in range_n_clusters:\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    score.append(silhouette_avg)\n    print(\"For n_clusters =\", n_clusters,\n          \"The average silhouette_score is :\", silhouette_avg)\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)","f5b074cc":"plt.plot(range_n_clusters,score)\nplt.xlabel('K')\nplt.ylabel('silhouette score')\nplt.xlim(88,90)\nplt.ylim(0.076,0.08)\nplt.grid()","5d1a75c1":"cluster89 = KMeans(n_clusters=89, random_state=10)\ncluster_labels = cluster89.fit_predict(data1[x_features1])\ndata1['clusters'] = cluster_labels","9e75cc67":"data1[data1.clusters==20].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))\ndata1[data1.clusters==33].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))\ndata1[data1.clusters==67].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))\ndata1[data1.clusters==88].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))","444e7861":"cov_matrix = np.array(data2[x_features2].cov())\nval, vec = la.eig(cov_matrix)\nvec = vec[:,val.argsort()[::-1]]\nval = val[val.argsort()[::-1]]\nvec1 = vec[:,0]\nvec2 = -vec[:,1]\nnew_c = []\nlabels = []\nfor index, rows in data2.iterrows():\n    fila = rows[x_features2].values\n    new_c.append([proy(vec1,fila),proy(vec2,fila)])\nnew_c=np.array(new_c)\nplt.plot(new_c[:,0],new_c[:,1],'.',)\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')","c36219f2":"range_n_clusters = np.arange(72,78)\nX = data2[x_features2]\nscore = []\nfor n_clusters in range_n_clusters:\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    score.append(silhouette_avg)\n    print(\"For n_clusters =\", n_clusters,\n          \"The average silhouette_score is :\", silhouette_avg)\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)","fc076810":"plt.plot(range_n_clusters,score)\nplt.xlabel('K')\nplt.ylabel('silhouette score')\nplt.grid()\nplt.xlim(72,77)\nplt.ylim(0.05,0.065)","49f3a49e":"cluster75 = KMeans(n_clusters=75, random_state=10)\ncluster_labels = cluster75.fit_predict(data2[x_features2])\ndata2['clusters'] = cluster_labels","3437a0ed":"data2[data2.clusters==20].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))\ndata2[data2.clusters==33].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))\ndata2[data2.clusters==67].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))\ndata2[data2.clusters==88].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))","c486d855":"cov_matrix = np.array(data3[x_features3].cov())\nval, vec = la.eig(cov_matrix)\nvec = vec[:,val.argsort()[::-1]]\nval = val[val.argsort()[::-1]]\nvec1 = vec[:,0]\nvec2 = -vec[:,1]\nnew_c = []\nlabels = []\nfor index, rows in data3.iterrows():\n    fila = rows[x_features3].values\n    new_c.append([proy(vec1,fila),proy(vec2,fila)])\nnew_c=np.array(new_c)\nplt.plot(new_c[:,0],new_c[:,1],'.',)\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')","6f979e33":"range_n_clusters = np.arange(95,100)\nX = data3[x_features3]\nscore = []\nfor n_clusters in range_n_clusters:\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    score.append(silhouette_avg)\n    print(\"For n_clusters =\", n_clusters,\n          \"The average silhouette_score is :\", silhouette_avg)\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)  ","8aa8c012":"plt.plot(range_n_clusters,score)\nplt.xlabel('K')\nplt.ylabel('silhouette score')\nplt.grid()\nplt.xlim(96,99)","122713d2":"cluster98 = KMeans(n_clusters=98, random_state=10)\ncluster_labels = cluster98.fit_predict(data3[x_features3])\ndata3['clusters'] = cluster_labels","54da6eeb":"data3[data3.clusters==20].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))\ndata3[data3.clusters==33].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))\ndata3[data3.clusters==67].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))\ndata3[data3.clusters==88].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))","f4e33511":"cov_matrix = np.array(data4[x_features4].cov())\nval, vec = la.eig(cov_matrix)\nvec = vec[:,val.argsort()[::-1]]\nval = val[val.argsort()[::-1]]\nvec1 = vec[:,0]\nvec2 = -vec[:,1]\nnew_c = []\nlabels = []\nfor index, rows in data4.iterrows():\n    fila = rows[x_features4].values\n    new_c.append([proy(vec1,fila),proy(vec2,fila)])\nnew_c=np.array(new_c)\nplt.plot(new_c[:,0],new_c[:,1],'.',)\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')","6f22f4b0":"range_n_clusters = np.arange(96,105)\nX = data4[x_features4]\nscore = []\nfor n_clusters in range_n_clusters:\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    score.append(silhouette_avg)\n    print(\"For n_clusters =\", n_clusters,\n          \"The average silhouette_score is :\", silhouette_avg)\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)","08a9281e":"plt.plot(range_n_clusters,score)\nplt.xlabel('K')\nplt.ylabel('silhouette score')\nplt.grid()","c2c6ddee":"cluster102 = KMeans(n_clusters=102, random_state=10)\ncluster_labels = cluster102.fit_predict(data4[x_features4])\ndata4['clusters'] = cluster_labels","d640d309":"data4[data4.clusters==20].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))\ndata4[data4.clusters==33].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))\ndata4[data4.clusters==67].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))\ndata4[data4.clusters==88].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))","f6450814":"data4[data4.clusters==25].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))\ndata4[data4.clusters==47].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))\ndata4[data4.clusters==72].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))\ndata4[data4.clusters==83].sum()[0:-2].plot.bar(fig = plt.figure(figsize = (20,5)))","1a0000fb":"Again, the same thing was done but with the different industries in which the interviewed have their current contracts. It was found that any of those who work with the goverment or as public servers or in the shippping, transportation,online bussines, internet based sales, military, non profit,retails and manufacturing industries, have an income equal or higher than 500.000 dollars a year.\nThose who work in the bussines and internet based sales or non profit industry do not reach the 300-400.000 dollars interval, and those who work in the shippping and transportation or the non profit industry do not reach the 250-300.000 dollars interval.\nAs there were many options for industries, there were given two plots, each one with half of the industries.","67a16aba":"Most of those who have majors in math, physics, software engineering and a few in information technology or have the role as data analyst, data scientist, data engineer or data base engineer, spent at least a fourth of their time coding. Of these, most of those who have used machine learning more than 2 years and consider themselves as data scientists, and those who have used machine learning less than 2 years maybe consider themselves a data scientists.\n","a87fa550":"### Major and Income:","1a7bea1c":"### Years of Experience and Income:","19e5f5e9":"### Age and Income:","f2af1ce5":"Again, a previous scanning was made for all clusters from 2 to 100, but was found that the optimal number was 89, so here just are shown from 88 to 91 to make the processing time shorter.","9c9af59a":"The data collected with the Kaggle survey can be separated in two main categories: one about the personal data of the interviewed, and the other about their computational knowledege. So to analyzed it, given that there are many parameters I will focus mainly in the link between the income and the other parameters given by the interviewed. First, the needed packages are imported. Then uploaded data it's cleaned, but to create a dataframe named data with all the personal information.","47b5a4a7":"From the plot is easy to deduce that 11 is a good number of clusters, so Kmeans is implemented for 11 clusters and the following is found.","49c6b550":"## Including the computational knowledge of the interviewed","c9d80a1b":"Most of the interviewed that have in mind that bias is an issue are students with majors in math, physics, engineering and information technology.","e83a4448":"o analyzed the questions that were not related to personal data, I chose some questions that seem relevant, but I did not include all of them to make it easier. I grouped then in some subcategories(all the categories include answers to questions 5, 6 and 9 which were about major, role and income): the first subcategory included questions 10 which asked if their current employer was incorporating machine learning methods into their business, and 11, which asked for activities that make up an important part of their role at work.\n\nThe second subcategory incuded questions 23, which asked for the percent of their time at work or school spent actively coding, question 24, which asked for the time that they have been writing code to analyze data, question 25, which asked for the time that they have used machine learning methods (at work or in school), and question 26, which asked if they considered themselves as data scientists.\n\nThe third subcategory included questions 36, which asked for online platforms in which they have begun or completed data science courses, and question 37, which asked for the online platform in which they have you spent the most amount of time.\n\nAnd finally the last subcategory included questions 44, which asked for what they find most difficult about ensuring that their algorithms are fair and unbiased, question 45, which asked for the circumstances in which they would explore model insights and interpret their model's predictions, and 47, which asked for methods they prefer for explaining and\/or interpreting decisions that are made by ML models.","39e46e89":"Given the 11 clusters there can be shown 11 of these plots, which show the frequency of every answer in each cluster. From these two examples, cluster 8 and 6 it is understood that men with ages between 18 and 21 have bachelor's degree, and men with ages between 25 and 29 have master's degree.\n\nNow, doing a scanning for different numbers of clusters, with the help of silhouette score an optimal number of clusters was found. In this case the scanning was from 2 to 100, but the best one was 97, so, to make the processing time shorter, it is shown the scanning from 90 to 100 clusters.\n","b6d42477":"### Gender and Income:","d03e77ac":"Choosing three random clusters (12, 33, 67 and 88) can be deduced that most of interviewed software engineers and some people who work in bussines disciplines, math, are using different machine learning methods at work.\n","b8346ca2":"To find links between gender, age, education level, major, role, years of experience and income, all at the same time the the PCA(principal component analysis) and Kmeans methods were used over the clustering of the data.\nFor this, it was necessary to change the possible answers to dummie variables because the packages of sklearn for PCA and Kmeans online work with numbers, but giving a certain number in a hierarchy would have given wrong values to every different answer.","4f511ee0":"The following algorithm was implemented by Natalia Cardona, who is in the same class as me, for plotting the principal components. First, the covariance matrix was calculated, then the eigenvectors of the matrix, then projection of these vectors and finally the new coordinates of the data.","7d710c78":"In the following plot it is possible to appreciate that for the highest incomes there are more men than women. It is important to make clear that the frequencies were normalized. There are no women who recieve more that 500.000 dollars a year. Even if the frequencies are normalized it can't be affirmed just with this data (even it is well known) that men tend to have higher incomes than women, because there were more interviewed men than women.","1d51770a":"## Frequencies","acb5bb4e":"## Subcategory 2: Time Coding","0b46a8d1":"# Kaggle Meta-Challenge","3c3495f7":"From these plots we can noticed tend to link to certain level of education and certain years of experience. If one looks closer to more clusters than just the ones that are shown here (12,33,67,88), one can seen that also, some majors and industries link to some incomes.","b628c348":"The same thing can be done for the undergraduate majors(um), and a similir plot is found. The percentege of people, no matter what um, dicreases as the income increases. But the important thing to say here is that for some majors their frequency at the highest incomes is zero. For example there are no people with medical or life science, Information technology, fine arts or performing arts (probably Brad Pitt was not interviewed) and social sciences majors that have an income equal or higher than 500.000 dollars a year. Social Scientists do not even reach the 400-500.000 dollars interval.","e6597428":"Comparing incomes and age of the participants in the same kind of plot, it was found that people older than 69 years do not reach the 400.000 dollars a year, and with 80 years old or more do not earn more than 90.000 dollars a year.","0bdc39c3":"So, for the first category I implemented the PCA and Kmeans methods and obtained the following:","52a2a938":"For the 98 clusters: Those who have majors like math, physics, bussines and engineering(non-computer focused) and have roles as data analyst, data scientist, data engineer or data base engineer have taken online courses. Software engineers tend to not take online courses.","ccfa080e":"## Subcategory 3: Data Science Courses","c76ea369":"### Level of Education and Income:","bab97a45":"Other personal data collected with the survey was the years of experience in their current jobs. The years of experience were compared with the incomes and as it can be appreciated in the plot, those who have less than 1 year of experience do not reach the 500.000 dollars a year and that those with less than 3 years of experience rarely earn more than 200.000 dollars a year. ","9e5e75c6":"## Subcategory 1: Machine Learning at the work place","ec4cefe2":"In this case it was analyzed the level of education versus the income and it was found that those with a professional degree, those who have university studies without earning a bacherlor's degree and those who do not have a formal education past high school do not reach the 300.000 dollars a year. And those who earn 500.000 dollars a year or more are those who have a Doctoral, a Master or a Bacherlor's degree.","e79c2434":"## Subcategory 4: Bias","6ce9663e":"### Doing some clustering with the personal data ","92eb5d68":"Now, for these data, with previous visualizations of it,there was found that the main differences between the different relations of the incomes and the other personal data, like Level of Education, Age, Major or Industry where seen in the top 20% of all the incomes, that means that in the scale from 0 to 500.000 dollars, relevant differences are appreciable for those who recieve an income higher than 90.000 dollars a year.\n","7cf3aa18":"### Industry of current contract and Income"}}