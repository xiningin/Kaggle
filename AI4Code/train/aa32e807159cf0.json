{"cell_type":{"baf6ef00":"code","c20cc8dd":"code","490378fc":"code","cf0100e2":"code","68cbef99":"code","cecd9c7f":"code","64387b1a":"code","99268a5b":"code","5477a5f0":"code","4763165b":"code","c385ece6":"code","47e8ed93":"code","c369379a":"code","8e313afb":"code","a6e16dd0":"code","8406ae61":"code","e4c4cca5":"markdown"},"source":{"baf6ef00":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport math\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\nwarnings.filterwarnings(\"ignore\") \n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c20cc8dd":"from keras.models import Sequential # model olu\u015fturuyoruz\nfrom keras.layers import Dense, LSTM # layer i\u00e7in , lstm ve dense output i\u00e7in\nfrom sklearn.preprocessing import MinMaxScaler # scale etmemize yar\u0131yor\nfrom sklearn.metrics import mean_squared_error # modeli kar\u015f\u0131la\u015ft\u0131rd\u0131\u011f\u0131m\u0131z y\u00f6ntem","490378fc":"data = pd.read_csv('\/kaggle\/input\/international-airline-passengers\/international-airline-passengers.csv',skipfooter=5)\ndata.head()","cf0100e2":"data.shape\ndata.iloc[:,1].values","68cbef99":"dataset = data.iloc[:,1].values\nplt.plot(dataset)\nplt.xlabel(\"time\")\nplt.ylabel(\"passenger\")\nplt.show()","cecd9c7f":"dataset.shape","64387b1a":"dataset = dataset.reshape(-1,1) # dataset'in shape'ini (142,1) olarak tan\u0131ml\u0131yoruz bazen sorun \u00e7\u0131kabilir\ndataset = dataset.astype(\"float32\")\ndataset.shape","99268a5b":"#scaling\nscaler = MinMaxScaler(feature_range = (0,1))\ndataset = scaler.fit_transform(dataset)\ndataset","5477a5f0":"train_size = int(len(dataset) * 0.50) # dataset'in boyututun yar\u0131s\u0131yla 71 71\ntest_size = len(dataset) - train_size \ntrain = dataset[0:train_size,:] # train'e dataset'imdeki verileri al\ntest = dataset[train_size:len(dataset),:] # test'e dataset'imin boyutundan sonras\u0131n\u0131 al\nprint(\"train size {}, teest size {}\".format(train_size, test_size))\nprint(\"train size {}, teest size {}\".format(len(train), len(test)))\n\n\n","4763165b":"timestep = 10\ndatax = []\ndatay = []\nfor i in range(len(train)-timestep-1): ## datalar\u0131 timestep = 10 step olarak 11. output yaparak train ettirmeye \u00e7al\u0131\u015f\u0131yoruz. 10 tanesini datax'e at 11. datay'e at gibi..\n    a = train[i:(i+timestep), 0]\n    datax.append(a)\n    datay.append(train[i + timestep, 0])\nx_train = np.array(datax)\ny_train = np.array(datay) ","c385ece6":"dataX = []\ndataY = []\nfor i in range(len(test)-timestep-1):\n    a = test[i:(i+timestep), 0]\n    dataX.append(a)\n    dataY.append(test[i + timestep, 0])\nx_test = np.array(dataX)\ny_test = np.array(dataY)  ","47e8ed93":"x_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\nx_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1])) # 3 boyut yap\u0131yoruz diyebiliriz...","c369379a":"print(x_train.shape)\nprint(x_test.shape) # 71 tane input vard\u0131 10 step ile y'e ald\u0131\u011f\u0131m\u0131z\u0131 d\u00fc\u015f\u00fcnelim  \nprint(y_train.shape)\n\n","8e313afb":"# model\nmodel = Sequential()\nmodel.add(LSTM(10, activation = 'relu', input_shape=(1, timestep))) # 10 lstm neuron(block) 1 layerda 10 tane lstm olsun\nmodel.summary()\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='Adam')\nmodel.fit(x_train, y_train, epochs=100, batch_size=1)","a6e16dd0":"trainPredict = model.predict(x_train)\ntestPredict = model.predict(x_test)\n# invert predictions\ntrainPredict = scaler.inverse_transform(trainPredict)\ny_train = scaler.inverse_transform([y_train])\ntestPredict = scaler.inverse_transform(testPredict)\ny_test = scaler.inverse_transform([y_test])\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(y_train[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(y_test[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","8406ae61":"# shifting train\ntrainPredictPlot = np.empty_like(dataset)\ntrainPredictPlot[:, :] = np.nan\ntrainPredictPlot[timestep:len(trainPredict)+timestep, :] = trainPredict\n# shifting test predictions for plotting\ntestPredictPlot = np.empty_like(dataset)\ntestPredictPlot[:, :] = np.nan\ntestPredictPlot[len(trainPredict)+(timestep*2)+1:len(dataset)-1, :] = testPredict\n# plot baseline and predictions\nplt.plot(scaler.inverse_transform(dataset))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.legend()\nplt.show()","e4c4cca5":"## Long Short Term Memory (LSTMs)\n* LSTM is variant of RNN.\n* LSTM de RNN'den farkl\u0131 olarak long term memory var. \n* LSTM architecture:\n    * x: scaling of information\n    * +: Adding information\n    * sigmoid layer. Sigmoid memory den bir \u015feyi hat\u0131rlamak i\u00e7in yada unutmak i\u00e7in kullan\u0131l\u0131r. 1 yada 0'd\u0131r.\n    * tanh: activation function tanh. Tanh vanishing gradient(yava\u015f \u00f6\u011frenme - \u00e7ok k\u00fc\u00e7\u00fck gradient) problemini \u00e7\u00f6zer. \u00c7\u00fcnk\u00fc parametreleri update ederken t\u00fcrev al\u0131yorduk. Tanh'\u0131n t\u00fcrevi hemen s\u0131f\u0131r'a ula\u015fmaz.\n    * h(t-1): output of LSTM unit\n    * c(t-1): memory from previous LSTM unit\n    * X(t): input\n    * c(t): new updated memory\n    * h(t): output\n    * From c(t-1) to c(t) is memory pipeline. or only memory.\n    * Oklar vekt\u00f6r.\n    * h(t-1) ile X(t) birle\u015fmiyor parallel iki yol olarak d\u00fc\u015f\u00fcnebilirsiniz.\n    \n    \n    \n    \n* 1) Forget gate: input olarak X(t) ve h(t-1) al\u0131r. Gelen bilginin unutulup unutulmayaca\u011f\u0131na karar verir. (X)\n* 2) Input gate: Hangi bilginin memory de depolan\u0131p depolanmayaca\u011f\u0131na karar verir. (X and +)\n* 3) Output gate: Hangi bilginin output olup olmayaca\u011f\u0131na karar verir. (X and tanh)\n* \u00d6rne\u011fin: \n    * ... \"Boys are watching TV\"\n    * \"On the other hand girls are playing baseball.\"\n    * Forget \"boys\". new input is \"girls\" and output is \"girls\""}}