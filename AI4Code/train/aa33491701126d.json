{"cell_type":{"c4e2afdb":"code","ed648913":"code","a5a20150":"code","f2f8cbbf":"code","7c7786e4":"code","9e28ad95":"code","cfac33f7":"code","2437edea":"code","dac14866":"code","e5c5b8fa":"code","d46f2c91":"code","c3d0cd2b":"code","59df7d0f":"code","8b5ba0f3":"code","433c8f22":"code","b80f7cf2":"code","f45ae1a9":"code","87b89213":"code","44184188":"code","12445111":"code","59680a7d":"code","46624a81":"code","0ebd8a12":"code","ace2f37b":"code","0dd5055a":"code","426c1bde":"code","45d28aae":"markdown","b4e262a2":"markdown","45d0a3c3":"markdown","63ee4fe2":"markdown","05065d03":"markdown","3b27df88":"markdown","42df3e01":"markdown","3537c434":"markdown","63b137e1":"markdown","a6e83b25":"markdown","4895d5ff":"markdown"},"source":{"c4e2afdb":"import pandas as pd\nimport numpy as np\nimport nltk\nimport re\nnltk.download('stopwords')\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords","ed648913":"df=pd.read_csv('..\/input\/nlp-dataset\/1.csv')","a5a20150":"df.head(5)","f2f8cbbf":"df.shape","7c7786e4":"df['reviewText'][0]","9e28ad95":"df['overall'].value_counts() #check balance data","cfac33f7":"df.isnull().any(axis=0)","2437edea":"df.isnull().sum()","dac14866":"df.dropna(inplace=True) #inplace true to change in original dataframe","e5c5b8fa":"df.shape","d46f2c91":"df['overall'].value_counts()","c3d0cd2b":"df.loc[df['overall']<3,'overall']=1\ndf.loc[df['overall']==3,'overall']=2\ndf.loc[df['overall']>3,'overall']=3","59df7d0f":"df['overall'].value_counts()","8b5ba0f3":"for i in range(0,len(df['overall'])):    \n    review=re.sub('[^a-zA-Z]',' ',df.iloc[i,1])\n    review=review.lower()\n    review=review.split()\n    review=[word for word in review if not word in stopwords.words('english')]\n    ps=PorterStemmer()\n    review=[ps.stem(word) for word in review]\n    review =\" \".join(review)\n    df.iloc[i,1]=review","433c8f22":"import cv2\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,10))\nbag_of_words=cv2.imread('..\/input\/bag-of-words-eg\/Bag_of_word_eg.PNG')\nplt.imshow(bag_of_words)","b80f7cf2":"from sklearn.model_selection import train_test_split","f45ae1a9":"features_train,features_test,labels_train,labels_test=train_test_split(df['reviewText'],df['overall'],random_state=40)","87b89213":"from sklearn.feature_extraction.text import CountVectorizer","44184188":"vect=CountVectorizer().fit(features_train)","12445111":"len(vect.get_feature_names())","59680a7d":"features_train_vectorized=vect.transform(features_train)","46624a81":"features_train_vectorized.shape","0ebd8a12":"from sklearn.feature_extraction.text import TfidfVectorizer","ace2f37b":"vect=TfidfVectorizer(min_df=5).fit(features_train) ","0dd5055a":"len(vect.get_feature_names())","426c1bde":"vect_train_tfidf_vect=vect.transform(features_train)","45d28aae":"# NLP  Count Vectorization(Bag of Words model\ud83d\udc5c)\n","b4e262a2":"    \n1.  This Notebook introduces the basics of feature engineering for text data. \n\n2. We start out with bag-of-words, which is the simplest representation based on word count statistics.\n\n3. A very much related transformation as count-vectorizer,tf-idf-vectorizer which are essentially a feature scaling tech\u2010nique.\n\n4. The Notebook talks about text extraction features, then dives into how to filter and clean those features.<\/h5>\n\n","45d0a3c3":"#### TF-IDF\nThe term tf\u2013idf stands for term frequency\u2013inverse document frequency, it is a mathematical statistic that is planned to reflect how significant a word is to a record in a collection or corpus. The tf\u2013idf esteem builds proportionally to the number of times a word shows up in the document. It is offset by the quantity of documents in the corpus that contain the word, which helps to adjust for the fact that a few words show up more often when all is said in done. tf\u2013idf is one of the most well-known term-weighting plans today. An overview led in 2015 demonstrated that 83% of text-based recommender frameworks in advanced libraries use tf\u2013idf. It would be difficult to understand tf\u2013idf together. So, let's understand each separately -\n\n#### Term Frequency (tf) - \nIt gives us the recurrence of the word in each report in the corpus. It is the proportion of the number of times the word shows up in a report contrasted with the all-out the number of words in that record. It increments as the quantity of events of that word inside the record increments.\n\n#### Inverse Data Frequency (idf) - \nIt is used to figure the heaviness of uncommon words over all reports in the corpus. The words that happen seldom in the corpus have a high IDF score.\n\nJoining these two, we think of the TF-IDF score (w) for a word in a record in the corpus.","63ee4fe2":"# Data Filtering for clean Features \ud83e\uddf9.","05065d03":"#  Tf-idf Vectorization","3b27df88":"# NLP Made Easy \ud83d\ude0e","42df3e01":"# Data Preprocessing \ud83d\udc77\u200d\u2642\ufe0f","3537c434":"### 1. Removing special characters using Regex.\n   Special character donot contain any such information about the review. \n    \n### 2. Removing Stopwords \n   Stopword lists are a way of weeding out common words that make for vacuous features.\n   such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d.\n        \n### 3. Stemming each word.\n   One problem with simple parsing is that different variations of the same word get\n    counted as separate words. \n    \n   The Porter stemmer is the most widely used free stemming tool for\n    the English language.\n    \n   For instance, \u201cflower\u201d and \u201cflowers\u201d are technically differ\u2010\n    ent tokens, and so are \u201cswimmer,\u201d \u201cswimming,\u201d and \u201cswim,\u201d even though they are\n    very close in meaning. It would be nice if all of these different variations got mapped\n    to the same word.\n    \n\n\n","63b137e1":"#### *This vect_train_tfidf_vect is our final Feature matrix we can use this to train any algorithim for Classification.*","a6e83b25":"1. For text data, we can start with a list of word count statistics called a bag-of-words\ud83d\udc5c.\n\n2. In bag-of-words (BoW) featurization, a text document\ud83d\udcc4 is converted into a vector of counts. (A vector is just a collection of n numbers.) The vector contains an entry for every possible word in the vocabulary.\n\n3. If a word in the vocabulary doesn\u2019t appear in the document, then it gets a count of 0.","4895d5ff":"## I took help of this notebook to webscrape the data\n\n### [Notebook](https:\/\/www.kaggle.com\/ishanishah8\/learning-webscraping)"}}