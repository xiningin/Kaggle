{"cell_type":{"20e06bd9":"code","5dc174ee":"code","a55b95ea":"code","c56429e6":"code","43538f76":"code","31d2abd7":"code","0585a5d0":"code","3b599c1c":"code","eb0687e4":"code","6361af3b":"code","5f4eecab":"code","9ce0b383":"code","3fc5d532":"code","0783f56b":"code","c07617c8":"code","20fead63":"code","dd122b56":"code","a0e0b9ee":"code","89ea2f9e":"code","d1e7a694":"code","152dcbb5":"code","68ba7893":"code","db9d877f":"code","dc063c25":"code","fca36084":"code","0eff4792":"code","fb970da0":"code","2222c233":"code","d0d727f1":"code","545d16bc":"code","a45f3b4f":"code","f1f523a2":"code","a4f8038e":"code","21fab2de":"code","5d310b2c":"code","c3358fe3":"code","205ce4ef":"code","ead9c4eb":"code","eb57cdd2":"code","afb229c9":"code","52added1":"code","88550b12":"code","36f22fd0":"code","b61b2491":"code","bfb27226":"code","34a5b282":"code","784ff02e":"code","97fa6cd6":"code","e8ed71af":"markdown","4488a57c":"markdown","248e7be9":"markdown","4f062d0f":"markdown","e0942dce":"markdown","e321ba74":"markdown","dde608cd":"markdown","d15ec5b0":"markdown","afa56649":"markdown","e065c5d4":"markdown","1d504c63":"markdown","e4c065b0":"markdown","6a823ad8":"markdown","69796889":"markdown","5f94325d":"markdown","e486fa0b":"markdown","85857495":"markdown","9a515284":"markdown","a261a46d":"markdown","efe4fc34":"markdown","0be92b94":"markdown","7e53e26a":"markdown","a4ac759b":"markdown","0d0eb50a":"markdown","3822beb1":"markdown","ca3bdccc":"markdown"},"source":{"20e06bd9":"#the basics\nimport pandas as pd, numpy as np\nimport math, re, gc, random, os, sys\nfrom matplotlib import pyplot as plt\n\n#for maximum aesthetics\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n#tensorflow deep learning basics\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\n\n#for model evaluation\nfrom sklearn.model_selection import train_test_split\n\n#no warnings\nimport warnings\nwarnings.filterwarnings('ignore')","5dc174ee":"SEED = 34\n\ndef seed_everything(seed):\n    os.environ['PYTHONHASHSEED']=str(seed)\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    \nseed_everything(SEED)","a55b95ea":"#load files into memory as Pandas DataFrames\ntrain_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_sub = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","c56429e6":"#sneak peak at training features\nprint(train_features.shape)\nif ~ train_features.isnull().values.any(): print('No missing values')\ntrain_features.head()","43538f76":"#sneak peak at train targets\nprint(train_targets_scored.shape)\nif ~ train_targets_scored.isnull().values.any(): print('No missing values')\ntrain_targets_scored.head()","31d2abd7":"#sneak peak at non scored train targets\nprint(train_targets_nonscored.shape)\nif ~ train_targets_nonscored.isnull().values.any(): print('No missing values')\ntrain_targets_nonscored.head()","0585a5d0":"#sneak peak at test features\nprint(test_features.shape)\nif ~ test_features.isnull().values.any(): print('No missing values')\ntest_features.head()","3b599c1c":"train = train_features.merge(train_targets_scored, on='sig_id', how='left')\ntrain = train.merge(train_targets_nonscored, on='sig_id', how='left')","eb0687e4":"fig = px.histogram(train, x='cp_type', histfunc='count',\n                  height=500, width=500)\nfig.show()","6361af3b":"control_ids = train.loc[train['cp_type'] == 'ctl_vehicle', 'sig_id']\ntrain_targets_scored.loc[train_targets_scored['sig_id'].isin(control_ids)].sum()[1:].sum()","5f4eecab":"cp_time_count = train['cp_time'].value_counts().reset_index()\ncp_time_count.columns = ['cp_time', 'count']\n\nfig = px.bar(cp_time_count, x='cp_time', y='count',\n             height=500, width=600)\nfig.show()","9ce0b383":"fig = px.histogram(train, x='cp_dose', height=500, width=600)\nfig.show()","3fc5d532":"fig = make_subplots(rows=15, cols=1)\n\nfor i in range(1,15):\n    fig.add_trace(\n    go.Histogram(x=train[f'g-{i}'], name=f'g-{i}'),\n    row=i, col=1)\n\n\nfig.update_layout(height=1200, width=800, title_text=\"Gene Expression Features\")\nfig.show()","0783f56b":"fig = make_subplots(rows=15, cols=1)\n\nfor i in range(1,15):\n    fig.add_trace(\n    go.Histogram(x=train[f'c-{i}'], name=f'c-{i}'),\n    row=i, col=1)\n\n\nfig.update_layout(height=1200, width=800, title_text=\"Cell Viability Features\")\nfig.show()","c07617c8":"fig = make_subplots(rows=1, cols=2)\n\nfig.add_trace(\n        go.Histogram(x=train[train_targets_scored[1:].columns.tolist()].sum(axis=1), name='Training Unique Scored Targets per Sample'),\n        row=1, col=1)\n\nfig.add_trace(\n        go.Histogram(x=train[train_targets_nonscored[1:].columns.tolist()].sum(axis=1), name='Training Unique Non-Scored Targets per Sample'),\n        row=1, col=2)\n\nfig.update_layout(height=400, width=1000, title_text=\"Unique Labels per Sample\")\nfig.show()","20fead63":"fig = px.bar(x=train[train_targets_scored.columns[1:].tolist()].sum(axis=0).sort_values(ascending=False).values,\n            y=train[train_targets_scored.columns[1:].tolist()].sum(axis=0).sort_values(ascending=False).index,\n            height=800, width=800, color=train[train_targets_scored.columns[1:].tolist()].sum(axis=0).sort_values(ascending=False).values)\n\nfig.update_layout(barmode='stack', yaxis={'categoryorder':'total ascending'}, title='Training Scored Target Classification Counts')\n\nfig.show()","dd122b56":"fig = px.bar(x=train[train_targets_nonscored.columns[1:].tolist()].sum(axis=0).sort_values(ascending=False).values,\n            y=train[train_targets_nonscored.columns[1:].tolist()].sum(axis=0).sort_values(ascending=False).index,\n            height=800, width=800, color=train[train_targets_nonscored.columns[1:].tolist()].sum(axis=0).sort_values(ascending=False).values)\n\nfig.update_layout(barmode='stack', yaxis={'categoryorder':'total ascending'}, title='Training NonScored Target Classification Counts')\n\nfig.show()","a0e0b9ee":"fig = px.imshow(train[train_features[1:].columns.tolist()].corr(method='pearson'), \n                title='Correlations Among Training Features',\n                height=800, width=800)\nfig.show()","89ea2f9e":"fig = px.imshow(train[[col for col in train_features.columns if 'c-' in col]].corr(method='pearson'), \n                title='Correlations Among Training Features',\n                height=800, width=800)\nfig.show()","d1e7a694":"c_cols = [col for col in train_features.columns if 'c-' in col]\ng_cols = [col for col in train_features.columns if 'g-' in col]\n\nc_corrs = train[[*c_cols,*train_targets_scored]].corr(method='pearson')","152dcbb5":"threshold_bad = .85\nbad_c_cols = []\n\nfor col in c_corrs.iloc[:len(c_cols), :len(c_cols)].columns:\n    for pair in c_corrs.iloc[:len(c_cols):, :len(c_cols)][col].iteritems():\n        if abs(pair[1]) > threshold_bad:\n            if pair[0] not in bad_c_cols and pair[0] is not col: \n                bad_c_cols.append(pair[0])\n            \nprint(f\"{len(bad_c_cols)} c- columns with correlation to other c- columns above {threshold_bad}\")\nprint('')\nprint(bad_c_cols)","68ba7893":"threshold_good = .65\ngood_c_cols = []\n\nfor col in c_corrs.iloc[:len(c_cols), len(c_cols):].columns:\n    for pair in c_corrs.iloc[:len(c_cols):, len(c_cols):][col].iteritems():\n        if abs(pair[1]) > threshold_good:\n            if pair[0] not in good_c_cols and pair[0] is not col: \n                good_c_cols.append(pair[0])\n            \nprint(f\"{len(good_c_cols)} c- columns with correlation to target above {threshold_good}\")\nprint('')\nprint(good_c_cols)","db9d877f":"c_cols_to_drop = [col for col in bad_c_cols if col not in good_c_cols]\nprint(len(c_cols_to_drop))\nprint(c_cols_to_drop)","dc063c25":"great_cols = [  0,   1,   2,   3,   5,   6,   8,   9,  10,  11,  12,  14,  15,\n        16,  18,  19,  20,  21,  23,  24,  25,  27,  28,  29,  30,  31,\n        32,  33,  34,  35,  36,  37,  39,  40,  41,  42,  44,  45,  46,\n        48,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,\n        63,  64,  65,  66,  68,  69,  70,  71,  72,  73,  74,  75,  76,\n        78,  79,  80,  81,  82,  83,  84,  86,  87,  88,  89,  90,  92,\n        93,  94,  95,  96,  97,  99, 100, 101, 103, 104, 105, 106, 107,\n       108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120,\n       121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 132, 133, 134,\n       135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147,\n       149, 150, 151, 152, 153, 154, 155, 157, 159, 160, 161, 163, 164,\n       165, 166, 167, 168, 169, 170, 172, 173, 175, 176, 177, 178, 180,\n       181, 182, 183, 184, 186, 187, 188, 189, 190, 191, 192, 193, 195,\n       197, 198, 199, 202, 203, 205, 206, 208, 209, 210, 211, 212, 213,\n       214, 215, 218, 219, 220, 221, 222, 224, 225, 227, 228, 229, 230,\n       231, 232, 233, 234, 236, 238, 239, 240, 241, 242, 243, 244, 245,\n       246, 248, 249, 250, 251, 253, 254, 255, 256, 257, 258, 259, 260,\n       261, 263, 265, 266, 268, 270, 271, 272, 273, 275, 276, 277, 279,\n       282, 283, 286, 287, 288, 289, 290, 294, 295, 296, 297, 299, 300,\n       301, 302, 303, 304, 305, 306, 308, 309, 310, 311, 312, 313, 315,\n       316, 317, 320, 321, 322, 324, 325, 326, 327, 328, 329, 330, 331,\n       332, 333, 334, 335, 338, 339, 340, 341, 343, 344, 345, 346, 347,\n       349, 350, 351, 352, 353, 355, 356, 357, 358, 359, 360, 361, 362,\n       363, 364, 365, 366, 368, 369, 370, 371, 372, 374, 375, 376, 377,\n       378, 379, 380, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n       392, 393, 394, 395, 397, 398, 399, 400, 401, 403, 405, 406, 407,\n       408, 410, 411, 412, 413, 414, 415, 417, 418, 419, 420, 421, 422,\n       423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435,\n       436, 437, 438, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450,\n       452, 453, 454, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n       466, 468, 469, 471, 472, 473, 474, 475, 476, 477, 478, 479, 482,\n       483, 485, 486, 487, 488, 489, 491, 492, 494, 495, 496, 500, 501,\n       502, 503, 505, 506, 507, 509, 510, 511, 512, 513, 514, 516, 517,\n       518, 519, 521, 523, 525, 526, 527, 528, 529, 530, 531, 532, 533,\n       534, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547,\n       549, 550, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563,\n       564, 565, 566, 567, 569, 570, 571, 572, 573, 574, 575, 577, 580,\n       581, 582, 583, 586, 587, 590, 591, 592, 593, 595, 596, 597, 598,\n       599, 600, 601, 602, 603, 605, 607, 608, 609, 611, 612, 613, 614,\n       615, 616, 617, 619, 622, 623, 625, 627, 630, 631, 632, 633, 634,\n       635, 637, 638, 639, 642, 643, 644, 645, 646, 647, 649, 650, 651,\n       652, 654, 655, 658, 659, 660, 661, 662, 663, 664, 666, 667, 668,\n       669, 670, 672, 674, 675, 676, 677, 678, 680, 681, 682, 684, 685,\n       686, 687, 688, 689, 691, 692, 694, 695, 696, 697, 699, 700, 701,\n       702, 703, 704, 705, 707, 708, 709, 711, 712, 713, 714, 715, 716,\n       717, 723, 725, 727, 728, 729, 730, 731, 732, 734, 736, 737, 738,\n       739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751,\n       752, 753, 754, 755, 756, 758, 759, 760, 761, 762, 763, 764, 765,\n       766, 767, 769, 770, 771, 772, 774, 775, 780, 781, 782, 783, 784,\n       785, 787, 788, 790, 793, 795, 797, 799, 800, 801, 805, 808, 809,\n       811, 812, 813, 816, 819, 820, 821, 822, 823, 825, 826, 827, 829,\n       831, 832, 833, 834, 835, 837, 838, 839, 840, 841, 842, 844, 845,\n       846, 847, 848, 850, 851, 852, 854, 855, 856, 858, 860, 861, 862,\n       864, 867, 868, 870, 871, 873, 874]","fca36084":"def preprocess(df):\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 2, 'D2': 3})\n    del df['sig_id']\n    return df","0eff4792":"train_features = preprocess(train_features)\ntest_features = preprocess(test_features)","fb970da0":"train_targets_scored = train_targets_scored.drop('sig_id', axis = 1)","2222c233":"train_targets_scored = train_targets_scored.loc[train_features['cp_type'] == 0].reset_index(drop=True)\ntrain_features = train_features.loc[train_features['cp_type'] == 0].reset_index(drop=True)","d0d727f1":"sample_sub.loc[:, train_targets_scored.columns] = 0","545d16bc":"USE_NN_ENSEMBLE = False\nUSE_PROCESSED = True","a45f3b4f":"#basic training configuration\nNUM_NETS = 1\nEPOCHS = 30\nBATCH_SIZE = 64\nVERBOSE = 0","f1f523a2":"def build_model(num_columns, num_nodes = 1024, use_swish = False, use_mish = False,\n                use_relu = False, use_selu = False, batch_norm = True, dropout = .4):\n    model = tf.keras.Sequential()\n    \n    if use_swish:\n        #first layer\n        if batch_norm:\n            model.add(tf.keras.layers.BatchNormalization(input_shape=(num_columns,)))\n            model.add(tf.keras.layers.Dropout(.2))\n        else: model.add(tf.keras.layers.Dropout(.2, input_shape=(num_columns,)))\n        model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(num_nodes, activation='swish')))\n        if batch_norm: model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Dropout(dropout))\n    \n        #second layer\n        model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(num_nodes, activation='swish')))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Dropout(dropout))\n                      \n    if use_mish:\n        #first layer\n        if batch_norm:\n            model.add(tf.keras.layers.BatchNormalization(input_shape=(num_columns,)))\n            model.add(tf.keras.layers.Dropout(.2))\n        else: model.add(tf.keras.layers.Dropout(.2, input_shape=(num_columns,)))\n        model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(num_nodes,\n                                                 activation = tfa.activations.mish)))\n        if batch_norm: model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Dropout(dropout))\n    \n        #second layer\n        model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(num_nodes,\n                                                 activation=tfa.activations.mish)))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Dropout(dropout))\n        \n    if use_relu:\n        #first layer\n        if batch_norm:\n            model.add(tf.keras.layers.BatchNormalization(input_shape=(num_columns,)))\n            model.add(tf.keras.layers.Dropout(.2))\n        else: model.add(tf.keras.layers.Dropout(.2, input_shape=(num_columns,)))\n        model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(num_nodes, activation='relu')))\n        if batch_norm: model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Dropout(dropout))\n    \n        #second layer\n        model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(num_nodes, activation='relu')))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Dropout(dropout))\n        \n    if use_selu:\n        #first layer\n        if batch_norm:\n            model.add(tf.keras.layers.BatchNormalization(input_shape=(num_columns,)))\n            model.add(tf.keras.layers.Dropout(.2))\n        else: model.add(tf.keras.layers.Dropout(.2, input_shape=(num_columns,)))\n        model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(num_nodes, activation='selu')))\n        if batch_norm: model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Dropout(dropout))\n    \n        #second layer\n        model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(num_nodes, activation='selu')))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Dropout(dropout))\n    \n    #output layer\n    model.add(tf.keras.layers.Dense(206, activation='sigmoid'))\n    \n    #compiler\n    model.compile(optimizer = tfa.optimizers.Lookahead(tf.optimizers.Adam(), sync_period = 10),\n                  loss = 'binary_crossentropy', metrics = ['AUC'])\n              \n    return model","a4f8038e":"preds = np.zeros((test_features.shape[0], 206)) \nhistories = []\n\nif USE_NN_ENSEMBLE:\n    for j in range(NUM_NETS):\n\n        #get datasets\n        train_ds = train_features.values\n        train_targets = train_targets_scored.values\n\n        #create a validation set to evaluate our model(s) performance\n        train_ds, val_ds, train_targets, val_targets = train_test_split(train_ds, train_targets, test_size = 0.1)\n\n        #some callbacks we can use\n        sv = tf.keras.callbacks.ModelCheckpoint(f'net-{j}.h5', monitor = 'val_loss', verbose = 0,\n                                                save_best_only = True, save_weights_only = True, mode = 'min')\n        reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, \n                                                              verbose=VERBOSE, epsilon=1e-4, mode='min')\n\n        #print fold info\n        model = build_model(train_features.shape[1], use_swish = True)\n        history = model.fit(train_ds, train_targets,\n                            validation_data = (val_ds, val_targets),\n                            epochs = EPOCHS, batch_size = BATCH_SIZE, \n                            callbacks = [reduce_lr_loss, sv], verbose = VERBOSE)\n        histories.append(history)\n\n        #report training results\n        print(f\"Neural Net {j + 1}: Epochs={EPOCHS}, Train AUC={round(max(history.history['auc']), 5)}, Train loss={round(min(history.history['loss']), 5)}, Validation AUC={round(max(history.history['val_auc']), 5)}, Validation loss={round(min(history.history['val_loss']), 5)}\")  \n        print('')\n\n        #predict out of fold\n        model.load_weights(f'net-{j}.h5')\n        pred = model.predict(test_features)\n        preds += pred \/ NUM_NETS","21fab2de":"preds_proc = np.zeros((test_features.shape[0], 206)) \nhistories_proc = []\n\nif USE_NN_ENSEMBLE & USE_PROCESSED:\n    for j in range(NUM_NETS):\n        \n        #train_dataset = train_features.drop(columns=c_cols_to_drop)\n        train_dataset = train_features.iloc[:, great_cols]\n        \n        #get datasets\n        train_ds = train_dataset.values\n        train_targets = train_targets_scored.values\n\n        #create a validation set to evaluate our model(s) performance\n        train_ds, val_ds, train_targets, val_targets = train_test_split(train_ds, train_targets, test_size = 0.1)\n\n        #some callbacks we can use\n        sv = tf.keras.callbacks.ModelCheckpoint(f'net-{j}.h5', monitor = 'val_loss', verbose = 0,\n                                                save_best_only = True, save_weights_only = True, mode = 'min')\n        reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, \n                                                              verbose=VERBOSE, epsilon=1e-4, mode='min')\n\n        #print fold info\n        model = build_model(train_dataset.shape[1], use_swish = True)\n        history = model.fit(train_ds, train_targets,\n                            validation_data = (val_ds, val_targets),\n                            epochs = EPOCHS, batch_size = BATCH_SIZE, \n                            callbacks = [reduce_lr_loss, sv], verbose = VERBOSE)\n        histories_proc.append(history)\n\n        #report training results\n        print(f\"Neural Net {j + 1}: Epochs={EPOCHS}, Train AUC={round(max(history.history['auc']), 5)}, Train loss={round(min(history.history['loss']), 5)}, Validation AUC={round(max(history.history['val_auc']), 5)}, Validation loss={round(min(history.history['val_loss']), 5)}\")  \n        print('')\n\n        #predict out of fold\n        model.load_weights(f'net-{j}.h5')\n        pred = model.predict(test_features)\n        preds += pred \/ NUM_NETS","5d310b2c":"if USE_NN_ENSEMBLE:\n    print(f\"Average validation loss: {np.average([min(histories[i].history['val_loss']) for i in range(len(histories))])}\")\n    print(f\"Average validation AUC: {np.average([max(histories[i].history['val_auc']) for i in range(len(histories))])}\")\n    \nif USE_NN_ENSEMBLE & USE_PROCESSED:\n    print(f\"Average validation loss: {np.average([min(histories[i].history['val_loss']) for i in range(len(histories))])}\")\n    print(f\"Average validation AUC: {np.average([max(histories[i].history['val_auc']) for i in range(len(histories))])}\")","c3358fe3":"#define function to visualize learning curves\ndef plot_learning_curves(histories, num): \n    fig, ax = plt.subplots(figsize = (20, 10))\n\n    #plot losses\n    for i in range(num):\n        plt.plot(histories[i].history['loss'], color = 'C0')\n        plt.plot(histories[i].history['val_loss'], color = 'C1')\n    \n    #set master title\n    fig.suptitle(\"Model Performance\", fontsize=14)\n\nif USE_NN_ENSEMBLE:\n    plot_learning_curves(histories, NUM_NETS)\n    plot_learning_curves(histories_proc, NUM_NETS)","205ce4ef":"USE_SKF_ENSEMBLE = True\nUSE_PROCESSED = True","ead9c4eb":"#basic training configuration\nFOLDS = 5\nREPEATS = 2\nBATCH_SIZE = 64\nVERBOSE = 0","eb57cdd2":"sys.path.append('..\/input\/iterativestrat\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import RepeatedMultilabelStratifiedKFold","afb229c9":"skf_preds = np.zeros((test_features.shape[0], 206)) \nskf_histories = []\nskf = RepeatedMultilabelStratifiedKFold(n_splits=FOLDS, n_repeats=REPEATS, random_state=SEED)\n\nif USE_SKF_ENSEMBLE:\n    for f, (train_index, val_index) in enumerate(skf.split(train_features.values, train_targets_scored.values)):\n\n        #get datasets\n        train_ds = train_features.values[train_index]\n        train_targets = train_targets_scored.values[train_index]\n        val_ds = train_features.values[val_index]\n        val_targets = train_targets_scored.values[val_index]\n\n        #some callbacks we can use\n        sv = tf.keras.callbacks.ModelCheckpoint(f'fold-{f}.h5', monitor = 'val_loss', verbose = 0,\n                                                save_best_only = True, save_weights_only = True, mode = 'min')\n        reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3,\n                                                              verbose=VERBOSE, epsilon=1e-4, mode='min')\n\n        #print fold info\n        model = build_model(train_features.shape[1], use_swish = True)\n        history = model.fit(train_ds, train_targets,\n                            validation_data = (val_ds, val_targets),\n                            epochs = EPOCHS, batch_size = BATCH_SIZE, \n                            callbacks = [reduce_lr_loss, sv], verbose = VERBOSE)\n        print('')\n        skf_histories.append(history)\n\n        #report training results\n        print(f\"Fold {f + 1}: Epochs={EPOCHS}, Train AUC={round(max(history.history['auc']), 5)}, Train loss={round(min(history.history['loss']), 5)}, Validation AUC={round(max(history.history['val_auc']), 5)}, Validation loss={round(min(history.history['val_loss']), 5)}\")  \n        print('')\n\n        #predict out of fold\n        model.load_weights(f'fold-{f}.h5')\n        pred = model.predict(test_features)\n        skf_preds += pred \/ FOLDS \/ REPEATS","52added1":"skf_preds_proc = np.zeros((test_features.shape[0], 206)) \nskf_histories_proc = []\nrmskf = RepeatedMultilabelStratifiedKFold(n_splits=FOLDS, n_repeats=REPEATS, random_state=SEED)\n\nif USE_SKF_ENSEMBLE & USE_PROCESSED:\n    for f, (train_index, val_index) in enumerate(rmskf.split(train_features.values, train_targets_scored.values)):\n\n        #train_dataset = train_features.drop(columns=c_cols_to_drop)\n        train_dataset = train_features.iloc[:, great_cols]\n        \n        #get datasets\n        train_ds = train_dataset.values[train_index]\n        train_targets = train_targets_scored.values[train_index]\n        val_ds = train_dataset.values[val_index]\n        val_targets = train_targets_scored.values[val_index]\n\n        #some callbacks we can use\n        sv = tf.keras.callbacks.ModelCheckpoint(f'fold-{f}.h5', monitor = 'val_loss', verbose = 0,\n                                                save_best_only = True, save_weights_only = True, mode = 'min')\n        reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3,\n                                                              verbose=VERBOSE, epsilon=1e-4, mode='min')\n\n        #print fold info\n        model = build_model(train_dataset.shape[1],\n                            use_swish = True)\n\n        history = model.fit(train_ds, train_targets, validation_data = (val_ds, val_targets),\n                            epochs = EPOCHS, batch_size = BATCH_SIZE,\n                            callbacks = [reduce_lr_loss, sv], verbose = VERBOSE)\n        print('')\n        skf_histories_proc.append(history)\n\n        #report training results\n        print(f\"Fold {f + 1}: Epochs={EPOCHS}, Train AUC={round(max(history.history['auc']), 5)}, Train loss={round(min(history.history['loss']), 5)}, Validation AUC={round(max(history.history['val_auc']), 5)}, Validation loss={round(min(history.history['val_loss']), 5)}\")  \n        print('')\n\n        #predict out of fold\n        model.load_weights(f'fold-{f}.h5')\n        pred = model.predict(test_features.iloc[:, great_cols])\n        skf_preds_proc += pred \/ FOLDS \/ REPEATS","88550b12":"if USE_SKF_ENSEMBLE:\n    print('#'*25)\n    print('SKF Ensemble Results')\n    print('#'*25); print('')\n    print(f\"Average validation loss: {np.average([min(skf_histories[i].history['val_loss']) for i in range(len(skf_histories))])}\")\n    print(f\"Average validation AUC: {np.average([max(skf_histories[i].history['val_auc']) for i in range(len(skf_histories))])}\")\n    print('')\n    \nif USE_SKF_ENSEMBLE & USE_PROCESSED:\n    print('#'*25)\n    print('SKF Ensemble Results - Processed')\n    print('#'*25); print('')\n    print(f\"Average validation loss: {np.average([min(skf_histories_proc[i].history['val_loss']) for i in range(len(skf_histories))])}\")\n    print(f\"Average validation AUC: {np.average([max(skf_histories_proc[i].history['val_auc']) for i in range(len(skf_histories))])}\")","36f22fd0":"print(sample_sub.shape)\nsample_sub.head()","b61b2491":"if USE_NN_ENSEMBLE:\n    #sample_sub.loc[:, train_targets_scored.columns] = preds\n    sample_sub.loc[:, train_targets_scored.columns] = preds_proc\n    \nif USE_SKF_ENSEMBLE:\n    #sample_sub.loc[:, train_targets_scored.columns] = skf_preds\n    sample_sub.loc[:, train_targets_scored.columns] = skf_preds_proc\n    \nsample_sub.head()","bfb27226":"#sanity check\nsample_sub.loc[test_features['cp_type'] == 1].head()","34a5b282":"sample_sub.loc[test_features['cp_type'] == 1, train_targets_scored.columns] = 0","784ff02e":"#last sanity check\nsample_sub.head()","97fa6cd6":"sample_sub.to_csv('submission.csv', index = False)\nprint('Submission saved')","e8ed71af":"**Let's see if the description we were given is accurate: do control group samples have any targets?**","4488a57c":"### 2. Random Ensemble Training\n\n**To get as diverse a set of NNs as possible, we place `train_test_split` within the training loop so that each model is trained\/evaluated on a different subset of the data. This approach might generalize better than stratified fold training if a large enough number of NNs are trainined.**","248e7be9":"# VI. Submission\n\n**We have already predicted during the training loops, so now we just need to assemble a submission DataFrame. We can just steal the sample submission given to us and with some minor tweaks, we are done:**","4f062d0f":"### 2. Train Features\n\n**We are told that features with `g-` signify gene expression data and `c-` signifies cell viability data. `cp_type` indicated samples that were treated with a compound (`cp_vehicle`) or with a control perturbation (`ctl_vehicle`). Control perturbations have no MoAs. Furthermore, `cp_time` and `cp_dose` indicate treatment duration (either 24, 48, or 72 hours) and dose (either high or low):**","e0942dce":"**Let's add `train_target` and `train_target_noscore` to `train` now for convenience:**","e321ba74":"**We see that most of the time, samples are either assigned to no label or one label, but there is a large difference in the 0 and 1 labels for the unscored targets.**","dde608cd":"### 1. StratifiedKFold Training","d15ec5b0":"**It would seem we have a lot of correlation going on in the bottom right corner of the above graph. This is the `c-` region.**","afa56649":"### 3. Evaluation","e065c5d4":"## 2. Evaluation","1d504c63":"### 3. Target Features\n\n**As mentioned above, the target features are categorized into two groups, scored and unscored. Both of the features in these two groups are binary. Now, this is a multi-classification problem but one sample can be classified as multiple or no targets. Let's see how often this happens:**","e4c065b0":"# II. EDA (Plotly)\n\n### 1. Basic","6a823ad8":"**From the above count plot, we conclude that the most frequently classified scored labels are `nfkb_inhibitor`, `proteasome_inhibitor`, `cyclooxygenase_inhibitor`, `dopamine_receptor_antagonist`, and `seratonin_receptor_antagonist`. Now let's do the same for the nonscored labels:**","69796889":"### 4. Correlations\n\n**Correlated features will not always worsen a model, but it does not always improve one either. In general, there are 3 reasons to remove correlated features:**\n\n1. Model trains faster\n2. Remove harmful bias\n3. Improve model interpretability\n\n**That being said, we only want to remove correlated features that are weakly correlated with our target. Suppose we have 3 features `col1`, `col2`, and `col3`. Also suppose that `col1` and `col2` are highly correlated to the target, but all 3 features are correlated to eachother. If we leave all the features and randomly select one of them, we have a `2\/3` chance of getting a feature correlated with our target (which is what we want). If we decide to remove one of these 3 correlated features, and we happen to remove, say, `col2`, then this probability of getting a 'good' feature drops to `1\/2`.**\n\n**So, we want to locate features that are highly correlated with eachother but only drop those that are weakly correlated with the target. Let's begin by creating some heatmaps:**","5f94325d":"# III. Processing\n\n**Indeed, we have a lot of correlation between `c-` variables. The least correlated pair seems to be no less than `.6`. Let's try to find the ones that aren't highly correlated with the target. We can do, rather sloppily I admit, like so:**","e486fa0b":"# V. RepeatedMultilabelStratifiedKFold\n\n**Now we can try stratified fold training. Since we have a multi-label target, we cannot use scikit-learn's `StratifiedKFold`, but we can use iterative stratification based on [this paper](https:\/\/link.springer.com\/chapter\/10.1007\/978-3-642-23808-6_10) from [this GitHub repository](https:\/\/github.com\/trent-b\/iterative-stratification). Of course, internet is not allowed for this competition, so we will use [this Kaggle dataset](https:\/\/www.kaggle.com\/mudittiwari255\/iterativestrat):**","85857495":"**Drop control group from training since all MoA's are 0:**","9a515284":"**Now we create a combined list of features: `c-` columns that are highly correlated with eachother, but that are also not too correlated with the target columns.**","a261a46d":"**Initialize sample submission with all 0s:**","efe4fc34":"### 1. Model\n\n**With lookahead optimizer inspired from [here](https:\/\/www.kaggle.com\/simakov\/keras-multilabel-neural-network-v1-2). For more on lookahead, I recommend this [video](https:\/\/www.youtube.com\/watch?v=ypqf7UUird4)**\n\n![Graph of different activation functions](https:\/\/raw.githubusercontent.com\/krutikabapat\/krutikabapat.github.io\/master\/assets\/activation.png)\n\n**I also wanted to freedom to experiment with different activation layers and batch normalization, so I included that in the `build_model` function. Here are some relevant papers:**\n\n**Batch Normalization**\n* **Batch normalization paper [here](https:\/\/arxiv.org\/abs\/1502.03167)**\n* **TensorFlow documentation [here](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/BatchNormalization)**\n\n**Swish Activation Function**\n* **Original paper [here](https:\/\/arxiv.org\/pdf\/1710.05941.pdf)**\n* **TensorFlow documentation [here](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/activations\/swish)**\n\n**Mish Activation Function**\n* **Original paper [here](https:\/\/arxiv.org\/abs\/1908.08681)**\n* **TensorFlow documentation [here](https:\/\/www.tensorflow.org\/addons\/api_docs\/python\/tfa\/activations\/mish)**\n\n**Relu Activation Function**\n* **Original paper [here](https:\/\/arxiv.org\/pdf\/1803.08375.pdf)**\n* **TensorFlow documentation [here](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/activations\/relu)**\n\n**Selu Activation Function**\n* **Original paper [here](https:\/\/papers.nips.cc\/paper\/6698-self-normalizing-neural-networks.pdf)**\n* **TensorFlow documentation [here](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/activations\/selu)**","0be92b94":"**Now we train with our processed data to compare performance:**","7e53e26a":"**Now we come up with a list of `c-` features that are highly correlated with eachother and a list of `c-` features that are highly correlated to the target columns:**","a4ac759b":"# IV. Neural Net Ensemble\n\n**Neural networks are random in nature because the weights of the nodes in each layer are randomly initalized at the beginning of training. From an experimental perspective, this means we need to run many experiments with the same parameters and average their results to compare different model architectures and processesing techniques. From a prediction perspective, this means we can train the same model many different times and use each of these trained models to predict, taking an average (weighted or not) for our final predictions:**","0d0eb50a":"# I. Intro\n\n**What is a Mechanism of Action? From Wikipedia:**\n\n> In pharmacology, the term mechanism of action (MOA) refers to the specific biochemical interaction through which a drug substance produces its pharmacological effect. A mechanism of action usually includes mention of the specific molecular targets to which the drug binds, such as an enzyme or receptor. Receptor sites have specific affinities for drugs based on the chemical structure of the drug, as well as the specific action that occurs there.\n\n**In this [competition](https:\/\/www.kaggle.com\/c\/lish-moa), we are asked to predict multiple target Mechanism of Action (MoA) responses of different samples(`sig_id`) given inputs like gene expression data and cell viability data. Note that the training data has an additional (optional) set of MoA labels that are *not* included in the test data are not used in scoring.**\n\n### Files\n\n**The following is taken from the data tab of the competition description page, found [here](https:\/\/www.kaggle.com\/c\/lish-moa\/data):**\n\n* `train_features.csv` - Features for the training set. Features g- signify gene expression data, and c- signify cell viability data. cp_type indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); control perturbations have no MoAs; cp_time and cp_dose indicate treatment duration (24, 48, 72 hours) and dose (high or low).\n* `train_targets_scored.csv` - The binary MoA targets that are scored.\n* `train_targets_nonscored.csv` - Additional (optional) binary MoA responses for the training data. These are not predicted nor scored.\n* `test_features.csv` - Features for the test data. You must predict the probability of each scored MoA for each row in the test data.\n* `sample_submission.csv` - A submission file in the correct format.","3822beb1":"**Great! That seemed to do the trick. So using the above procedure, we can drop only the highly correlated features that are uncorrelated to the target. We can also perform feature selection offline with permutation importanace, as is done in this notebook [here](https:\/\/www.kaggle.com\/stanleyjzheng\/multilabel-neural-network-improved)**","ca3bdccc":"**Recall that `cp_type` indicated samples that were treated with a compound (`cp_vehicle`) or with a control perturbation (`ctl_vehicle`) and that control perturbations have no MoAs. So, we should set the control samples in our test dataset to 0.**"}}