{"cell_type":{"9ff23593":"code","64ae73b2":"code","6403ff91":"code","ca91c090":"code","616c4660":"code","5a37fed8":"code","3b7bf6a2":"code","dbac4144":"code","7f3562ff":"code","038c1f5f":"code","6c17b130":"code","88dafd7f":"code","c632f791":"code","e58518d6":"code","daa3fa1b":"code","a4610ba9":"code","baf1948a":"code","de0342a9":"code","41b22c51":"code","4bb5fabe":"code","e3105db5":"code","7ca0f367":"code","d0d4de19":"code","a5d3556b":"code","76ec5092":"code","49a4e526":"code","2d416386":"code","f5d5a323":"code","a72860cb":"code","ef9ff680":"code","0ecd112c":"code","edcab03c":"code","381f05f1":"code","6718e6dc":"code","a703c31c":"code","29e8a9bf":"code","84977337":"code","f0f07f94":"code","87ebea1a":"code","d5b93b1e":"code","8b43ed18":"code","c7b6471b":"code","00a119ca":"code","663e42ab":"code","87b7c39b":"code","0cca1309":"code","5b87beb7":"code","73341c03":"code","e0e1a5f4":"code","a2084a69":"code","3fe32452":"code","5cb5a04e":"code","b4d562f9":"code","019b4ccd":"code","9d6156f4":"code","666a2cd8":"code","ef6f6eee":"code","64e5b5e4":"code","9dd088a0":"code","28f4e99c":"code","f258915e":"code","afb09de0":"code","4ea4f6c8":"markdown","8d1e60d0":"markdown","12b7083f":"markdown","85321f5b":"markdown","182f0124":"markdown","9e2c4c59":"markdown","65e24819":"markdown","77139287":"markdown","b91684f5":"markdown","022414ef":"markdown","7ebcd930":"markdown","5cdb1ca7":"markdown","3d24c9a2":"markdown","acb4e904":"markdown","2c6b0f6b":"markdown","f7b9caec":"markdown","8f8522e3":"markdown","eec019a4":"markdown","d4f44d36":"markdown","1e428dce":"markdown","48af56a1":"markdown","b9f4af8c":"markdown","8f152ac6":"markdown","6ee8439d":"markdown","d08c9bb3":"markdown","e3d85221":"markdown","73770a95":"markdown","415cbce2":"markdown","da85c239":"markdown","608b6c79":"markdown","b4ed68a4":"markdown","33119f93":"markdown","5745a9bb":"markdown","038cdf56":"markdown","89c935b2":"markdown","f2817be2":"markdown","d0cb17a9":"markdown","ff98449b":"markdown","7fade80c":"markdown","4332cf77":"markdown","bd46086a":"markdown"},"source":{"9ff23593":"from IPython.display import Image\nImage(filename=\"..\/input\/custchurn3\/CustChurn3.png\")","64ae73b2":"from IPython.display import Image\nImage(filename=\"..\/input\/custchurn\/CustChurn.png\")","6403ff91":"from IPython.display import Image\nImage(filename=\"..\/input\/custchurn2\/CustChurn2.png\")","ca91c090":"# Core libraries around dataframe and math handling tasks\nimport numpy as np\nimport pandas as pd\npd.set_option('precision', 3)\n\n# Data Visualisation Libraries\nimport matplotlib.pyplot as plt\n%config InlineBackend.figure_format = 'retina'\n#!pip install seaborn --upgrade\nimport seaborn as sns\nsns.set_style('darkgrid')\n\n# Statistics related libraries\nfrom scipy.stats import chi2_contingency\nfrom imblearn.over_sampling import SMOTE","616c4660":"# Machine Learning related libraries\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.model_selection import learning_curve\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier","5a37fed8":"from sklearn.metrics import accuracy_score, recall_score, precision_score, auc, roc_auc_score, roc_curve\nfrom sklearn.metrics import confusion_matrix\nimport scikitplot as skplt\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nprint('All required libraries are imported!!')","3b7bf6a2":"font_size = 20\nplt.rcParams['axes.labelsize'] = font_size\nplt.rcParams['axes.titlesize'] = font_size + 2\nplt.rcParams['xtick.labelsize'] = font_size - 2\nplt.rcParams['ytick.labelsize'] = font_size - 2\nplt.rcParams['legend.fontsize'] = font_size - 2\n\ncolors = ['#00A5E0', '#DD403A']\ncolors_cat = ['#E8907E', '#D5CABD', '#7A6F86', '#C34A36', '#B0A8B9', '#845EC2', '#8f9aaa', '#FFB86F', '#63BAAA', '#9D88B3', '#38c4e3']\ncolors_comp = ['steelblue', 'seagreen', 'black', 'darkorange', 'purple', 'firebrick', 'slategrey']\n\nrandom_state = 24\nscoring_metric = 'recall'\ncomparison_dict, comparison_test_dict = {}, {}\n\nprint('Default Parameters and Variables are set!!')","dbac4144":"df = pd.read_csv('..\/input\/predicting-churn-for-bank-customers\/Churn_Modelling.csv')\n\nprint('Dataset Imported Successfully!!\\n')\nprint('It contains {} rows and {} columns.'.format(df.shape[0], df.shape[1]))\ndf.head()","7f3562ff":"df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1, inplace=True)\ndf.columns","038c1f5f":"df.info()","6c17b130":"df.describe().T","88dafd7f":"train_df = df\ndef plot_continuous(feature):\n    '''Plot a histogram and boxplot for the churned and retained distributions for the specified feature.'''\n    df_func = train_df.copy()\n    df_func['Exited'] = df_func['Exited'].astype('category')\n\n    fig, (ax1, ax2) = plt.subplots(2,\n                                   figsize=(9, 7),\n                                   sharex=True,\n                                   gridspec_kw={'height_ratios': (.7, .3)})\n\n    for df, color, label in zip([df_retained, df_churned], colors, ['Retained', 'Churned']):\n        sns.histplot(data=df,\n                     x=feature,\n                     bins=15,\n                     color=color,\n                     alpha=0.66,\n                     edgecolor='firebrick',\n                     label=label,\n                     kde=False,\n                     ax=ax1)\n    ax1.legend()\n\n    sns.boxplot(x=feature, y='Exited', data=df_func, palette=colors, ax=ax2)\n    ax2.set_ylabel('')\n    ax2.set_yticklabels(['Retained', 'Churned'])\n\n    plt.tight_layout();\n\n\nprint('plot_continuous function defined!')","c632f791":"def plot_categorical(feature):\n    '''For a categorical feature, plot a seaborn.countplot for the total counts of each category next to a barplot for the churn rate.'''\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\n    sns.countplot(x=feature,\n                  hue='Exited',\n                  data=train_df,\n                  palette=colors,\n                  ax=ax1)\n    ax1.set_ylabel('Count')\n    ax1.legend(labels=['Retained', 'Churned'])\n\n    sns.barplot(x=feature,\n                y='Exited',\n                data=train_df,\n                palette=colors_cat,\n                ax=ax2)\n    ax2.set_ylabel('Churn rate')\n\n    if (feature == 'HasCrCard' or feature == 'IsActiveMember'):\n        ax1.set_xticklabels(['No', 'Yes'])\n        ax2.set_xticklabels(['No', 'Yes'])\n\n    plt.tight_layout();\n\n\nprint('plot_categorical function defined!')","e58518d6":"def plot_confusion_matrix(cm, ax):\n    '''Plot a confusion matrix in the specified axes object.'''\n    sns.heatmap(data=cm,\n                annot=True,\n                cmap='Blues',\n                annot_kws={'fontsize': 30},\n                ax=ax)\n\n    ax.set_xlabel('Predicted Label')\n    ax.set_xticks([0.5, 1.5])\n    ax.set_xticklabels(['Retained', 'Churned'])\n\n    ax.set_ylabel('True Label')\n    ax.set_yticks([0.25, 1.25])\n    ax.set_yticklabels(['Retained', 'Churned']);\n\n\nprint('plot_confusion_matrix function defined!')","daa3fa1b":"def plot_feature_importance(classifier, classifier_name, color, ax):\n    '''Plot the importance of features for a classifier as a barplot.'''\n    importances = pd.DataFrame({'Feature': X_train.columns,\n                                'Importance': np.round(classifier.best_estimator_.feature_importances_, 3)})\n\n    importances = importances.sort_values('Importance', ascending=True).set_index('Feature')\n\n    importances.plot.barh(color=color,\n                          edgecolor='firebrick',\n                          legend=False,\n                          ax=ax)\n    ax.set_title(classifier_name)\n    ax.set_xlabel('Importance');\n\nprint('plot_feature_importance function defined!')","a4610ba9":"def plot_learning_curve(estimator,\n                        X,\n                        y,\n                        ax,\n                        cv=None,\n                        train_sizes=np.linspace(0.1, 1.0, 5)):\n    '''Plot the learning curves for an estimator in the specified axes object.'''\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator,\n        X,\n        y,\n        cv=cv,\n        n_jobs=-1,\n        train_sizes=train_sizes,\n        scoring='accuracy')\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    ax.fill_between(train_sizes,\n                    train_scores_mean - train_scores_std,\n                    train_scores_mean + train_scores_std,\n                    alpha=0.1,\n                    color='dodgerblue')\n    ax.fill_between(train_sizes,\n                    test_scores_mean - test_scores_std,\n                    test_scores_mean + test_scores_std,\n                    alpha=0.1,\n                    color='darkorange')\n\n    ax.plot(train_sizes,\n            train_scores_mean,\n            color='dodgerblue',\n            marker='o',\n            linestyle='-',\n            label='Training Score')\n    ax.plot(train_sizes,\n            test_scores_mean,\n            color='darkorange',\n            marker='o',\n            linestyle='-',\n            label='Cross-validation Score')\n\n    ax.set_xlabel('Training Examples')\n    ax.set_ylabel('Score')\n    ax.legend(loc='best', fontsize=14);\n\n\nprint('plot_learning_curve function defined!')","baf1948a":"def classifier_performance(classifier, classifier_name, classifier_name_abv):\n    '''Display the overall performance of a classifier with this template.'''\n    print('\\n', classifier_name)\n    print('-------------------------------')\n    print('   Best Score ({}): '.format(scoring_metric) + str(np.round(classifier.best_score_, 3)))\n    print('   Best Parameters: ')\n    for key, value in classifier.best_params_.items():\n        print('      {}: {}'.format(key, value))\n\n    y_pred_pp = cross_val_predict(estimator=classifier.best_estimator_,\n                                  X=X_train,\n                                  y=y_train,\n                                  cv=5,\n                                  method='predict_proba')[:, 1]\n    y_pred = y_pred_pp.round()\n\n    cm = confusion_matrix(y_train, y_pred, normalize='true')\n\n    fpr, tpr, _ = roc_curve(y_train, y_pred_pp)\n    comparison_dict[classifier_name_abv] = [\n        accuracy_score(y_train, y_pred),\n        precision_score(y_train, y_pred),\n        recall_score(y_train, y_pred),\n        roc_auc_score(y_train, y_pred_pp), fpr, tpr\n    ]\n\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n    plot_confusion_matrix(cm, ax1)\n    plot_learning_curve(classifier.best_estimator_, X_train, y_train, ax2)\n\n    plt.tight_layout();\n\n\nprint('classifier_performance function defined!')","de0342a9":"def testset_performance(classifier, classifier_name, ax):\n    '''Assess the performance on the test set and plot the confusion matrix.'''\n    y_pred = classifier.predict(X_test)\n    cm = confusion_matrix(y_test, y_pred, normalize='true')\n\n    comparison_test_dict[classifier_name] = [accuracy_score(y_test, y_pred),\n                                             precision_score(y_test, y_pred),\n                                             recall_score(y_test, y_pred)]\n\n    sns.heatmap(cm,\n                annot=True,\n                annot_kws={'fontsize': 24},\n                cmap='Blues',\n                ax=ax)\n\n    ax.set_title(classifier_name)\n\n    ax.set_xlabel('Predicted Label')\n    ax.set_xticks([0.5, 1.5])\n    ax.set_xticklabels(['Retained', 'Churned'])\n\n    ax.set_ylabel('True Label')\n    ax.set_yticks([0.2, 1.4])\n    ax.set_yticklabels(['Retained', 'Churned']);\n\n\nprint('testset_performance function defined!')","41b22c51":"fig, ax = plt.subplots(figsize=(6, 6))\n\nsns.countplot(x='Exited', data=df, palette=colors, ax=ax)\n\nfor index, value in enumerate(df['Exited'].value_counts()):\n    label = '{}%'.format(round((value \/ df['Exited'].shape[0]) * 100, 2))\n    ax.annotate(label,\n                xy=(index, value + 250),\n                ha='center',\n                va='center',\n                color=colors[index],\n                fontweight='bold',\n                size=font_size + 4)\n\nax.set_xticklabels(['Retained', 'Churned'])\nax.set_xlabel('Status')\nax.set_ylabel('Count')\nax.set_ylim([0, 10000]);","4bb5fabe":"continuous = ['Age', 'CreditScore', 'Balance', 'EstimatedSalary']\ncategorical = ['Geography', 'Gender', 'Tenure', 'NumOfProducts', 'HasCrCard', 'IsActiveMember']\n\nprint('Continuous: ', ', '.join(continuous))\nprint('Categorical: ', ', '.join(categorical))","e3105db5":"df[continuous].hist(figsize=(12, 10),\n                          bins=20,\n                          layout=(2, 2),\n                          color='steelblue',\n                          edgecolor='firebrick',\n                          linewidth=1.5);","7ca0f367":"fig, ax = plt.subplots(figsize=(7, 6))\n\nsns.heatmap(df[continuous].corr(),\n            annot=True,\n            annot_kws={'fontsize': 16},\n            cmap='Blues',\n            ax=ax)\n\nax.tick_params(axis='x', rotation=45)\nax.tick_params(axis='y', rotation=360);","d0d4de19":"df_churned = df[df['Exited'] == 1]\ndf_retained = df[df['Exited'] == 0]\n\nplot_continuous('Age')","a5d3556b":"plot_continuous('CreditScore')","76ec5092":"plot_continuous('Balance')","49a4e526":"plot_continuous('EstimatedSalary')","2d416386":"df_cat = train_df[categorical]\n\nfig, ax = plt.subplots(2, 3, figsize=(12, 8))\n\nfor index, column in enumerate(df_cat.columns):\n\n    plt.subplot(2, 3, index + 1)\n    sns.countplot(x=column, data=train_df, palette=colors_cat)\n\n    plt.ylabel('Count')\n    if (column == 'HasCrCard' or column == 'IsActiveMember'):\n        plt.xticks([0, 1], ['No', 'Yes'])\n\nplt.tight_layout();","f5d5a323":"plot_categorical('Geography')","a72860cb":"plot_categorical('Gender')","ef9ff680":"plot_categorical('Tenure')","0ecd112c":"plot_categorical('NumOfProducts')","edcab03c":"plot_categorical('IsActiveMember')","381f05f1":"chi2_array, p_array = [], []\nfor column in categorical:\n    crosstab = pd.crosstab(df[column], df['Exited'])\n    chi2, p, dof, expected = chi2_contingency(crosstab)\n    chi2_array.append(chi2)\n    p_array.append(p)\n\ndf_chi = pd.DataFrame({\n    'Variable': categorical,\n    'Chi-square': chi2_array,\n    'p-value': p_array\n})\ndf_chi.sort_values(by='Chi-square', ascending=False)","6718e6dc":"features_drop = ['Tenure', 'HasCrCard', 'EstimatedSalary']\ndf = df.drop(features_drop, axis=1)\n\nprint('Additional features dropped which may not influence much on target!')","a703c31c":"df.head()","29e8a9bf":"df['Gender'] = LabelEncoder().fit_transform(df['Gender'])\n\ndf['Geography'] = df['Geography'].map({\n    'Germany': 1,\n    'Spain': 0,\n    'France': 0\n})\n\nprint('Gender and Geography - features have been encoded!')","84977337":"df.head()","f0f07f94":"scaler = StandardScaler()\n\nscl_columns = ['CreditScore', 'Age', 'Balance']\ndf[scl_columns] = scaler.fit_transform(df[scl_columns])\n\nprint('CreditScore,Age and Balance - features scaled!')","87ebea1a":"df.head()","d5b93b1e":"train_df, test_df = train_test_split(df, test_size=0.2, random_state=random_state)\n\ntrain_df.reset_index(drop=True, inplace=True)\ntest_df.reset_index(drop=True, inplace=True)\n\nprint('Train set: {} rows x {} columns'.format(train_df.shape[0],\n                                               train_df.shape[1]))\nprint(' Test set: {} rows x {} columns'.format(test_df.shape[0],\n                                               test_df.shape[1]))","8b43ed18":"y_train = train_df['Exited']\nX_train = train_df.drop('Exited', 1)\n\nprint('X_train and y_train Sets Created!')","c7b6471b":"y_train.value_counts()","00a119ca":"over = SMOTE(sampling_strategy='auto', random_state=random_state)\nX_train, y_train = over.fit_resample(X_train, y_train)\n\ny_train.value_counts()","663e42ab":"X_train.head()","87b7c39b":"y_train.head()","0cca1309":"from IPython.display import Image\nImage(filename=\"..\/input\/custchurn4\/grid_search_cross_validation.png\")","5b87beb7":"clf_list = [('Logistic Regression', LogisticRegression(random_state=random_state))]\n\ncv_base_mean, cv_std = [], []\nfor clf in clf_list:\n\n    cv = cross_val_score(estimator=clf[1],\n                         X=X_train,\n                         y=y_train,\n                         scoring=scoring_metric,\n                         cv=5,\n                         n_jobs=-1)\n\n    cv_base_mean.append(cv.mean())\n    cv_std.append(cv.std())\n\nprint('Baseline Models (Recall):')\n\nfor i in range(len(clf_list)):\n    print('   {}: {}'.format(clf_list[i][0], np.round(cv_base_mean[i], 2)))","73341c03":"from IPython.display import Image\nImage(filename=\"..\/input\/custchurn5\/CustChurn5.png\")","e0e1a5f4":"lr = LogisticRegression(random_state=random_state)\n\nparam_grid = {\n    'max_iter': [100],\n    'penalty': ['l1', 'l2'],\n    'C': [0.0001, 0.001, 0.01, 0.1, 1, 10],\n    'solver': ['lbfgs', 'liblinear']\n}\n\nlr_clf = GridSearchCV(estimator=lr,\n                      param_grid=param_grid,\n                      scoring=scoring_metric,\n                      cv=5,\n                      verbose=False,\n                      n_jobs=-1)\n\nbest_lr_clf = lr_clf.fit(X_train, y_train)\nclassifier_performance(best_lr_clf, 'Logistic Regression', 'LR')","a2084a69":"gbc = GradientBoostingClassifier(random_state=random_state)\nparam_grid = {\n    'n_estimators': [600],\n    'subsample': [0.66, 0.75],\n    'learning_rate': [0.001, 0.01],\n    'max_depth': [3],  # default=3\n    'min_samples_split': [5, 7],\n    'min_samples_leaf': [3, 5],\n    'max_features': ['auto', 'log2', None],\n    'n_iter_no_change': [20],\n    'validation_fraction': [0.2],\n    'tol': [0.01]\n}\n\ngbc_clf = GridSearchCV(estimator=gbc,\n                       param_grid=param_grid,\n                       scoring=scoring_metric,\n                       cv=5,\n                       verbose=False,\n                       n_jobs=-1)\n\nbest_gbc_clf = gbc_clf.fit(X_train, y_train)\nclassifier_performance(best_gbc_clf, 'Gradient Boosting Classifier', 'GBC')","3fe32452":"dt = DecisionTreeClassifier(criterion='entropy',max_depth=5)\nbest_dt_clf = dt.fit(X_train, y_train)\ny_pred_gini = best_dt_clf.predict(X_train)\nprint('Model accuracy score with entropy: {0:0.4f}'. format(accuracy_score(y_train, y_pred_gini)))","5cb5a04e":"dt = DecisionTreeClassifier(criterion='gini',max_depth=5)\nbest_dt_clf = dt.fit(X_train, y_train)\ny_pred_gini = best_dt_clf.predict(X_train)\nprint('Model accuracy score with gini: {0:0.4f}'. format(accuracy_score(y_train, y_pred_gini)))","b4d562f9":"rf = RandomForestClassifier(random_state=random_state)\nparam_grid = {\n    'n_estimators': [100],\n    'criterion': ['entropy', 'gini'],\n    'bootstrap': [True, False],\n    'max_depth': [6],\n    'max_features': ['auto', 'sqrt'],\n    'min_samples_leaf': [2, 3, 5],\n    'min_samples_split': [2, 3, 5]\n}\n\nrf_clf = GridSearchCV(estimator=rf,\n                      param_grid=param_grid,\n                      scoring=scoring_metric,\n                      cv=5,\n                      verbose=False,\n                      n_jobs=-1)\n\nbest_rf_clf = rf_clf.fit(X_train, y_train)\nclassifier_performance(best_rf_clf, 'Random Forest', 'RF')","019b4ccd":"colors_fi = ['steelblue', 'cadetblue']\n\nfig = plt.subplots(1, 2, figsize=(12, 6))\n\nfor i, (name, clf) in enumerate(zip(['RF', 'GBC'],\n                                    [best_rf_clf, best_gbc_clf])):\n\n    ax = plt.subplot(1, 2, i + 1)\n    plot_feature_importance(clf, name, colors_fi[i], ax)\n    plt.ylabel('')\n\nplt.tight_layout();","9d6156f4":"comparison_matrix = {}\nfor key, value in comparison_dict.items():\n    comparison_matrix[str(key)] = value[0:4]\n\ncomparison_df = pd.DataFrame(comparison_matrix,\n                             index=['Accuracy', 'Precision', 'Recall', 'AUC']).T\ncomparison_df.style.highlight_max(color='indianred', axis=0)","666a2cd8":"comparison_df.plot(kind='bar',\n                   figsize=(10, 5),\n                   fontsize=12,\n                   color=['#5081DE', '#A7AABD', '#D85870', '#424656'])\n\nplt.legend(loc='upper center',\n           fontsize=font_size - 6,\n           ncol=len(comparison_df.columns),\n           bbox_to_anchor=(0.5, 1.12))\nplt.xticks(rotation=0)\nplt.yticks([0, 0.4, 0.8])\n\nplt.axhline(y=0.70, color='red', linestyle='--')\nplt.text(x=-0.5, y=0.73, s='0.70', size=font_size + 2, color='red');","ef6f6eee":"test_df.head()","64e5b5e4":"y_test = test_df['Exited']\nX_test = test_df.drop('Exited', 1)\n\nprint('Test dataset processing complete!')","9dd088a0":"estimators = [('LR', best_lr_clf.best_estimator_),\n              ('GBC', best_gbc_clf.best_estimator_),\n              ('RF', best_rf_clf.best_estimator_)]\n\ntuned_voting_soft = VotingClassifier(estimators=estimators[1:],\n                                     voting='soft',\n                                     n_jobs=-1)\n\ntuned_voting_soft.fit(X_train, y_train)\n\nfig, ax = plt.subplots(3, 1, figsize=(5,16))\n\nfor i, (name, clf) in enumerate(zip(['LR', 'RF', 'GBC'], \n                                    [best_lr_clf.best_estimator_, best_rf_clf.best_estimator_, best_gbc_clf.best_estimator_, tuned_voting_soft])):\n    testset_performance(clf, name, ax=ax[i])\n\nplt.tight_layout();","28f4e99c":"comparison_test_df = pd.DataFrame(comparison_test_dict,\n                                  index=['Accuracy', 'Precision', 'Recall']).T\ncomparison_test_df.style.highlight_max(color='indianred', axis=0)","f258915e":"comparison_test_df.plot(kind='bar',\n                        figsize=(10, 5),\n                        fontsize=12,\n                        color=['#5081DE', '#A7AABD', '#D85870'])\n\nplt.legend(loc='upper center',\n           ncol=len(comparison_test_df.columns),\n           bbox_to_anchor=(0.5, 1.11))\nplt.xticks(rotation=0)\nplt.yticks([0, 0.4, 0.8])\n\nplt.axhline(y=0.70, color='red', linestyle='--')\nplt.text(x=-0.5, y=0.72, s='0.70', size=font_size + 2, color='red');","afb09de0":"from IPython.display import Image\nImage(filename=\"..\/input\/custchurn6\/CustChurn6.png\")","4ea4f6c8":"## Feature Importance","8d1e60d0":"## Further analysis on - Categorical feature: Active members","12b7083f":"# 5. Data Understanding\n\n## Import Required Libraries","85321f5b":"## Default Configurations","182f0124":"# 1. Customer Churn Prediction\n\n### Objective: Predicting Customer Churn using end to end Machine Learning\n\nCredit: From the notebook [here](https:\/\/www.kaggle.com\/korfanakis\/predicting-customer-churn-with-machine-learning). We wanted to leverage this classic use case and demonstrate the end to end data science structure in dealing with these type of problems. This is work in progress and we will continue to update it further.\n","9e2c4c59":"# 9 Deployment\n\nWe will go ahead and deploy the best model from test dataset.","65e24819":"### SMOTE (Synthetic Minority Oversampling Technique)\n- finds a record that is similar to the record being upsampled and creates a synthetic record that is a randomly weighted average of the original record and the neighbouring record, where the weight is generated separately for each predictor","77139287":"## Further analysis on - Categorical feature: No. of products","b91684f5":"## Analysis on \"Feature: Age\"","022414ef":"## Analysis on \"Feature: Credit Score\"","7ebcd930":"# 5.1 A quick glance at our dataset","5cdb1ca7":"## Further analysis on - Categorical feature: Geography","3d24c9a2":"## Analysis of Continuous variables","acb4e904":"# 7.3 Decision Tree","2c6b0f6b":"# 7.4 Random Forest","f7b9caec":"# 4. Business Understanding\n\n#### Business Goal is to understand and predict customers who are at a risk of closing their account with the bank.\n#### Identify two clasess -  Exits \/ No Exits\n#### Therefore, it is a Binary Classification problem\n#### We will use \"Recall\" to evaluate how well our model is identifying which customers will exit.","8f8522e3":"## Scaling","eec019a4":"## Generic Functions\n\nWe can define some generic functions which can be later used.","d4f44d36":"## Analysis on \"Feature: Balance\"","1e428dce":"# 2. Why Predict Churn?","48af56a1":"### Now, we will split the train set into \"X_train\" and \"y_train\" ","b9f4af8c":"# 7.2 Gradient Boosting Classifier","8f152ac6":"References: I am extremely thankful to Orfanakis Konstantinos and leveraged from the notebook and sincere credit to him for putting together a great application. Objective of this notebook is to give specific structure from end to end data science using a classic use case.","6ee8439d":"## Start with simple Baseline Models\n\nWe will start with a basic baseline performance using LR with default parameters and evaluate recall by doing a k-fold Cross Validation. ","d08c9bb3":"# 3. Let's predict using Data Science","e3d85221":"## Handling Class Imbalance","73770a95":"## Analysis on \"Feature: Estimated Salary\"","415cbce2":"## Target Variable: Exited - which tells whether Churn or No Churn\n","da85c239":"## Performance Comparision","608b6c79":"## Analysis of Categorical Variables","b4ed68a4":"# 5.2 Exploratory Data Analysis (EDA)","33119f93":"## Creating Train and Test Set\n\nWe will split our dataset into a train and test set using scikit-learn's train_test_split() function, which implements random sampling. Our dataset is large enough (especially relative to the number of features), so we do not risk introducing sampling bias.","5745a9bb":"# 6. Data Preparation\n\n## Feature Engineering Tasks\n\n- Feature Selection\n- Encoding some of the Categorical features\n- Scaling\n- Handling class imbalance (if needed)","038cdf56":"## Correlation Analysis","89c935b2":"## Encoding some Categorical features","f2817be2":"## Further analysis on - Categorical feature: Gender","d0cb17a9":"# 7.1 Logistic Regression","ff98449b":"# 8 Model Evaluation\n\n### Evaluate the Test dataset","7fade80c":"## Further analysis on - Categorical feature: Tenure","4332cf77":"## Feature Selection\n\n- We have already dropped columns 'RowNumber', 'CustomerId', and 'Surname' at the beginning of our analysis as they are not relevant\n- As per EDA, we can further drop 'Tenure', 'HasCrCard', 'EstimatedSalary' as well, and we can confirm that from more analysis as needed","bd46086a":"# 7. Model Development"}}