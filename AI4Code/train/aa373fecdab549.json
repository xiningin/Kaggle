{"cell_type":{"814f7193":"code","6333fb9a":"code","0ba5c155":"code","f2dadd67":"code","85692b13":"code","58e2f442":"code","1b1e0c1b":"code","fd3b4279":"code","1bc2f653":"code","28b1ad22":"code","4f14ac5e":"code","1e0442bb":"code","0cbe33f3":"code","012dc883":"code","212a8e4e":"code","09764b8b":"code","2ae6c6f8":"code","07067547":"code","69325861":"code","720d6120":"code","2170e281":"code","b7af8cd2":"code","88b77bb5":"markdown","a95de2bb":"markdown","d51b7adb":"markdown","5139aabc":"markdown","e5dd9b26":"markdown","ce99bf1c":"markdown","1055ae89":"markdown","6e5f80ed":"markdown","8510218d":"markdown","31c2d4b3":"markdown","9214759f":"markdown","801fe274":"markdown","75d2f474":"markdown","42018569":"markdown","2daf7a8a":"markdown","4ff4bddb":"markdown","deaf91a3":"markdown","11c045fa":"markdown","7e23ef8a":"markdown","f67d5f53":"markdown","eac472fa":"markdown"},"source":{"814f7193":"import pandas as pd\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn import model_selection\nfrom sklearn import tree\nimport graphviz","6333fb9a":"iris = datasets.load_iris()\nprint('Dataset structure= ', dir(iris))\n\ndf = pd.DataFrame(iris.data, columns = iris.feature_names)\ndf['target'] = iris.target\ndf['flower_species'] = df.target.apply(lambda x : iris.target_names[x]) # Each value from 'target' is used as index to get corresponding value from 'target_names' \n\nprint('Unique target values=',df['target'].unique())\n\ndf.sample(5)","0ba5c155":"# label = 0 (setosa)\ndf[df.target == 0].head(3)","f2dadd67":"# label = 1 (versicolor)\ndf[df.target == 1].head(3)","85692b13":"# label = 2 (verginica)\ndf[df.target == 2].head(3)","58e2f442":"#Lets create feature matrix X  and y labels\nX = df[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']]\ny = df[['target']]\n\nprint('X shape=', X.shape)\nprint('y shape=', y.shape)","1b1e0c1b":"X_train,X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size= 0.2, random_state= 1)\nprint('X_train dimension= ', X_train.shape)\nprint('X_test dimension= ', X_test.shape)\nprint('y_train dimension= ', y_train.shape)\nprint('y_train dimension= ', y_test.shape)","fd3b4279":"\"\"\"\nTo obtain a deterministic behaviour during fitting always set value for 'random_state' attribute\nAlso note that default value of criteria to split the data is 'gini'\n\"\"\"\ncls = tree.DecisionTreeClassifier(random_state= 1)\ncls.fit(X_train ,y_train)","1bc2f653":"print('Actual value of species for 10th training example=',iris.target_names[y_test.iloc[10]][0])\nprint('Predicted value of species for 10th training example=', iris.target_names[cls.predict([X_test.iloc[10]])][0])\n\nprint('\\nActual value of species for 20th training example=',iris.target_names[y_test.iloc[20]][0])\nprint('Predicted value of species for 20th training example=', iris.target_names[cls.predict([X_test.iloc[20]])][0])\n\nprint('\\nActual value of species for 30th training example=',iris.target_names[y_test.iloc[29]][0])\nprint('Predicted value of species for 30th training example=', iris.target_names[cls.predict([X_test.iloc[29]])][0])","28b1ad22":"cls.score(X_test, y_test)","4f14ac5e":"tree.plot_tree(cls) ","1e0442bb":"dot_data = tree.export_graphviz(cls, out_file=None) \ngraph = graphviz.Source(dot_data) \ngraph.render(\"iris_decision_tree\") ","0cbe33f3":"dot_data = tree.export_graphviz(cls, out_file=None, \n                      feature_names=iris.feature_names,  \n                      class_names=iris.target_names,  \n                      filled=True, rounded=True,  \n                      special_characters=True)  \ngraph = graphviz.Source(dot_data)  \ngraph ","012dc883":"boston = datasets.load_boston()\nprint('Dataset structure= ', dir(boston))\n\ndf = pd.DataFrame(boston.data, columns = boston.feature_names)\ndf['target'] = boston.target\n\ndf.sample(5)","212a8e4e":"#Lets create feature matrix X  and y labels\nX = df[['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']]\ny = df[['target']]\n\nprint('X shape=', X.shape)\nprint('y shape=', y.shape)","09764b8b":"X_train,X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size= 0.2, random_state= 1)\nprint('X_train dimension= ', X_train.shape)\nprint('X_test dimension= ', X_test.shape)\nprint('y_train dimension= ', y_train.shape)\nprint('y_train dimension= ', y_test.shape)","2ae6c6f8":"\"\"\"\nTo obtain a deterministic behaviour during fitting always set value for 'random_state' attribute\nTo keep the tree simple I am using max_depth = 3\nAlso note that default value of criteria to split the data is 'mse' (mean squared error)\nmse is equal to variance reduction as feature selection criterion and minimizes the L2 loss using the mean of each terminal node\n\"\"\"\ndtr = tree.DecisionTreeRegressor(max_depth= 3,random_state= 1)\ndtr.fit(X_train ,y_train)","07067547":"predicted_price= pd.DataFrame(dtr.predict(X_test), columns=['Predicted Price'])\nactual_price = pd.DataFrame(y_test, columns=['target'])\nactual_price = actual_price.reset_index(drop=True) # Drop the index so that we can concat it, to create new dataframe\ndf_actual_vs_predicted = pd.concat([actual_price,predicted_price],axis =1)\ndf_actual_vs_predicted.T","69325861":"dtr.score(X_test, y_test)","720d6120":"tree.plot_tree(dtr) ","2170e281":"dot_data = tree.export_graphviz(dtr, out_file=None) \ngraph = graphviz.Source(dot_data) \ngraph.render(\"boston_decision_tree\") ","b7af8cd2":"dot_data = tree.export_graphviz(dtr, out_file=None, \n                      feature_names=boston.feature_names,  \n                      filled=True, rounded=True,  \n                      special_characters=True)  \ngraph = graphviz.Source(dot_data)  \ngraph ","88b77bb5":"## Visualize The Decision Tree\nWe will use plot_tree() function from sklearn to plot the tree and then export the tree in Graphviz format using the export_graphviz exporter. Results will be saved in boston_decision_tree.pdf file","a95de2bb":"# Disadvantages Of Decision Tree\n* Decision-tree learners can create over-complex trees that do not generalize the data well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n* Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n* Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.","d51b7adb":"# Regression Problem Example\nFor regression exercise we are going to use sklearns Boston house prices dataset\nObjective is to predict house price based on available data\n\n## Understanding the Boston house dataset\n* boston.DESCR > Complete description of dataset\n* boston.data > Data to learn. There are 13 features, Median Value (attribute 14) is usually the target. Total 506 training sets\n    - CRIM     per capita crime rate by town\n    - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n    - INDUS    proportion of non-retail business acres per town\n    - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n    - NOX      nitric oxides concentration (parts per 10 million)\n    - RM       average number of rooms per dwelling\n    - AGE      proportion of owner-occupied units built prior to 1940\n    - DIS      weighted distances to five Boston employment centres\n    - RAD      index of accessibility to radial highways\n    - TAX      full-value property-tax rate per USD 10,000\n    - PTRATIO  pupil-teacher ratio by town\n    - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n    - LSTAT    % lower status of the population\n    - MEDV     Median value of owner-occupied homes in USD 1000's\n* boston.feature_names > Array of all 13 features ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n 'B' 'LSTAT']\n* boston.filename > CSV file name\n* boston.target > The price valueis in $1000\u2019s\n\nFrom above details its clear that X = 'boston.data' and y= 'boston.target'","5139aabc":"### Create Test And Train Dataset\n* We will split the dataset, so that we can use one set of data for training the model and one set of data for testing the model\n* We will keep 20% of data for testing and 80% of data for training the model\n* If you want to learn more about it, please refer [Train Test Split tutorial](https:\/\/satishgunjal.com\/train_test_split\/)","e5dd9b26":"# Classification Problem Example\nFor classification exercise we are going to use sklearns iris plant dataset.\nObjective is to classify iris flowers among three species (setosa, versicolor or virginica) from measurements of length and width of sepals and petals\n\n## Understanding the IRIS dataset\n* iris.DESCR > Complete description of dataset\n* iris.data > Data to learn. Each training set is 4 digit array of features. Total 150 training sets\n* iris.feature_names > Array of all 4 feature ['sepal length (cm)','sepal width cm)','petal length (cm)','petal width (cm)']\n* iris.filename > CSV file name\n* iris.target > The classification label. For every training set there is one classification label(0,1,2). Here 0 for setosa, 1 for versicolor and 2 for virginica\n* iris.target_names > the meaning of the features. It's an array >> ['setosa', 'versicolor', 'virginica']\n\nFrom above details its clear that X = 'iris.data' and y= 'iris.target'\n\n![Iris_setosa](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/iris_species.png)\n\n<sub><sup>Image from [Machine Learning in R for beginners](https:\/\/www.datacamp.com\/community\/tutorials\/machine-learning-in-r)<\/sup><\/sub>","ce99bf1c":"### Testing The Model\n* For testing we are going to use the test data only\n* Question: Predict the species of 10th, 20th and 29th test example from test data","1055ae89":"Note that, target value 0 = setosa, 1 = versicolor and 2 = virginica\n\nLet visualize the feature values for each type of flower","6e5f80ed":"Now lets train the model using Decision Tree","8510218d":"# Advantages Of Decision Tree\n* Simple to understand and to interpret. Trees can be visualized.\n* Requires little data preparation. Other techniques often require data normalization, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values.\n* Able to handle both numerical and categorical data.\n* Able to handle multi-output problems.\n* Uses a white box model. Results are easy to interpret.\n* Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model.","31c2d4b3":"## Visualize The Decision Tree\nWe will use plot_tree() function from sklearn to plot the tree and then export the tree in Graphviz format using the export_graphviz exporter. Results will be saved in iris_decision_tree.pdf file","9214759f":"# Inner Workings Of Decision Tree\n* At the root node decision tree selects feature to split the data in two major categories.\n* So at the end of root node we have two decision rules and two sub trees\n* Data will again be divided in two categories in each sub tree\n* This process will continue until every training example is grouped together.\n* So at the end of decision tree we end up with leaf node. Which represent the class or a continuous value that we are trying predict\n\n## Criteria To Split The Data\nThe objective of decision tree is to split the data in such a way that at the end we have different groups of data which has more similarity and less randomness\/impurity. In order to achieve this, every split in decision tree must reduce the randomness.\nDecision tree uses 'entropy' or 'gini' selection criteria to split the data.\nNote: We are going to use sklearn library to test classification and regression. 'entropy' or 'gini' are selection criteria for classifier whereas \u201cmse\u201d, \u201cfriedman_mse\u201d and \u201cmae\u201d are selection criteria for regressor.\n\n### Entropy\nIn order to find the best feature which will reduce the randomness after a split, we can compare the randomness before and after the split for every feature. In the end we choose the feature which will provide the highest reduction in randomness. Formally randomness in data is known as 'Entropy' and difference between the 'Entropy' before and after split is known as 'Information Gain'. Since in case of decision tree we may have multiple branches, information gain formula can be written as,\n\n```\n    Information Gain= Entropy(Parent Decision Node)\u2013(Average Entropy(Child Nodes))\n```\n\n'i' in below Entropy formula represent the target classes \n\n   ![entropy_formula](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/entropy_formula.png)\n\nSo in case of 'Entropy', decision tree will split the data using the feature that provides the highest information gain.\n\n### Gini\nIn case of gini impurity, we pick a random data point in our dataset. Then randomly classify it according to the class distribution in the dataset. So it becomes very important to know the accuracy of this random classification. Gini impurity gives us the probability of incorrect classification. We\u2019ll determine the quality of the split by weighting the impurity of each branch by how many elements it has. Resulting value is called as 'Gini Gain' or 'Gini Index'. This is what\u2019s used to pick the best split in a decision tree. Higher the Gini Gain, better the split\n\n'i' in below Gini formula represent the target classes \n\n   ![gini_formula](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/gini_formula.png)\n\nSo in case of 'gini', decision tree will split the data using the feature that provides the highest gini gain.\n\n### So Which Should We Use?\nGini impurity is computationally faster as it doesn\u2019t require calculating logarithmic functions, though in reality neither metric results in a more accurate tree than the other.","801fe274":"## Build Machine Learning Model","75d2f474":"## Import Libraries\n* pandas: Used for data manipulation and analysis\n* numpy : Numpy is the core library for scientific computing in Python. It is used for working with arrays and matrices.\n* datasets: Here we are going to use \u2018iris\u2019 and 'boston house prices' dataset\n* model_selection: Here we are going to use model_selection.train_test_split() for splitting the data\n* tree: Here we are going to decision tree classifier and regressor\n* graphviz: Is used to export the tree into Graphviz format using the export_graphviz exporter","42018569":"### Create Test And Train Dataset\n* We will split the dataset, so that we can use one set of data for training the model and one set of data for testing the model\n* We will keep 20% of data for testing and 80% of data for training the model\n* If you want to learn more about it, please refer [Train Test Split tutorial](https:\/\/satishgunjal.com\/train_test_split\/)","2daf7a8a":"## Load The Data","4ff4bddb":"![Decision_Tree_Header](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/Decision_Tree_Header.png)\n\nDecision tree algorithm belongs to the family of supervised learning algorithms. Unlike other supervised learning algorithms decision tree can be used to solve regression and classification problems. The goal of decision tree is to create training model that can predict class(single or multi) or value by learning simple decision rules from training data.\nDecision tree form a flow chart like structure that's why they are very easy to interpret and understand. It is one of the few ML algorithm where its very easy to visualize and analyze the internal working of algorithm.\n\nJust like flowchart, decision tree contains different types of nodes and branches. Every decision node represent the test on feature and based on the test result it will either form another branch or the leaf node. Every branch represents the decision rule and leaf node represent the final outcome.\n\n![decision tree](https:\/\/raw.githubusercontent.com\/satishgunjal\/images\/master\/Decision_Tree.png)\n\nTypes of decision tree\n* Classification decision trees \u2212 In this kind of decision trees, the decision variable is categorical. \n* Regression decision trees \u2212 In this kind of decision trees, the decision variable is continuous","deaf91a3":"Now lets train the model using Decision Tree","11c045fa":"### Testing The Model\n* For testing we are going to use the test data only\n* Question: predict the values for every test set in test data","7e23ef8a":"### Model Score\nCheck the model score using test data","f67d5f53":"### Model Score\nCheck the model score using test data","eac472fa":"## Build Machine Learning Model"}}