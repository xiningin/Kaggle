{"cell_type":{"33ea6de3":"code","4a8d1c10":"code","06b5b160":"code","4251f890":"code","3fa085a7":"code","a4a8c58c":"code","c8720adc":"code","9c9844c5":"code","da623d3e":"code","71680ebc":"code","c54a44ff":"code","625b1e23":"code","2c6a6d86":"code","f25f75ba":"code","0c9f7762":"code","9952f31f":"code","04b0dbdf":"code","e86779f5":"code","09c33fbf":"code","b8250aec":"code","a61c6476":"code","9c63557b":"code","aad764be":"code","d198030f":"code","01ca44a4":"code","42a05357":"code","c5d0621d":"code","10752ff3":"code","994f7166":"code","1ef2101c":"code","13b54d38":"code","1f1bb713":"code","8803eb79":"markdown","3226832a":"markdown","72d9fb29":"markdown","2bfd116a":"markdown","73d13a03":"markdown"},"source":{"33ea6de3":"import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, RandomForestRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nimport matplotlib.pyplot as plt\n%matplotlib inline","4a8d1c10":"X = pd.read_csv('..\/input\/train.csv')\ny = X.pop('Survived')","06b5b160":"X.describe()","4251f890":"X['Age'].fillna(X.Age.mean(), inplace=True)\n\nX.describe()","3fa085a7":"numeric_variables = list(X.dtypes[X.dtypes != 'object'].index)\nX[numeric_variables].head()","a4a8c58c":"model = RandomForestRegressor(n_estimators=100, oob_score=True, random_state=42)\n\nmodel.fit(X[numeric_variables], y)","c8720adc":"model.oob_score_","9c9844c5":"# Benchmark\ny_oob = model.oob_prediction_\nprint('c-stat: ', roc_auc_score(y, y_oob))","da623d3e":"def describe_categorical(X):\n    \"\"\"\n    Just like .describe(), but returns the results for \n    categorical variables only.\n    \"\"\"\n    from IPython.display import display, HTML\n    display(HTML(X[X.columns[X.dtypes == \"object\"]].describe().to_html()))","71680ebc":"describe_categorical(X)","c54a44ff":"X.drop(['Name', 'Ticket', 'PassengerId'], axis = 1, inplace = True)","625b1e23":"def clean_cabin(x):\n    try:\n        return x[0]\n    except TypeError:\n        return 'None'\n    \nX['Cabin'] = X.Cabin.apply(clean_cabin)","2c6a6d86":"categorical_variables = ['Sex', 'Cabin', 'Embarked']\n\nfor variable in categorical_variables:\n    X[variable].fillna('Missing', inplace=True)\n    dummies = pd.get_dummies(X[variable], prefix=variable)\n    X = pd.concat([X, dummies], axis=1)\n    X.drop([variable], axis=1, inplace=True)","f25f75ba":"X","0c9f7762":"def printall(X, max_rows=10):\n    from IPython.display import display, HTML\n    display(HTML(X.to_html(max_rows=max_rows)))\n    \nprintall(X)","9952f31f":"# setting n_jobs to -1 is actually very important for optimization, it tells SKLearn to use maximum\n# number of cores that you have, which is almost definitely greater than 1.\nmodel = RandomForestRegressor(100, oob_score=True, n_jobs=-1, random_state=42)\nmodel.fit(X, y)\nprint('C-stat:', roc_auc_score(y, model.oob_prediction_))","04b0dbdf":"model.feature_importances_","e86779f5":"feature_importances = pd.Series(model.feature_importances_, index = X.columns)\nfeature_importances.sort_values(inplace=True)\nfeature_importances.plot(kind='barh', figsize=(7,6));","09c33fbf":"# Complex version taht shows the summary view\n\ndef graph_feature_importances(model, feature_names, autoscale=True, headroom=0.05, width=10, summarized_columns=None):\n    \"\"\"\n    By Mike Bernico \n    \"\"\"\n    if autoscale:\n        x_scale = model.feature_importances_.max()+headroom\n    else:\n        x_scale = 1\n        \n    feature_dict = dict(zip(feature_names, model.feature_importances_))\n    \n    if summarized_columns:\n        # some dummy columns need to be summarized\n        for col_name in summarized_columns:\n            sum_value = sum(x for i, x in feature_dict.items() if col_name in i)\n            \n            keys_to_remove = [i for i in feature_dict.keys() if col_name in i]\n            for i in keys_to_remove:\n                feature_dict.pop(i)\n            feature_dict[col_name] = sum_value\n    \n    results = pd.Series(feature_dict.values(), index=feature_dict.keys())\n    results.sort_values(axis=0)\n    results.plot(kind=\"barh\", figsize=(width, len(results)\/4), xlim=(0,x_scale))","b8250aec":"# graph_feature_importances(model, X.columns, summarized_columns=categorical_variables)","a61c6476":"%%timeit\nmodel = RandomForestRegressor(1000, oob_score=True, n_jobs=1, random_state=42)\nmodel.fit(X,y)","9c63557b":"%%timeit\nmodel = RandomForestRegressor(1000, oob_score=True, n_jobs=-1, random_state=42)\nmodel.fit(X,y)","aad764be":"results = []\nn_estimator_options = [30, 50, 100, 200, 500, 1000, 2000]\n\nfor trees in n_estimator_options:\n    model = RandomForestRegressor(trees, oob_score=True, n_jobs=-1, random_state=42)\n    model.fit(X, y)\n    print(trees, 'trees')\n    roc = roc_auc_score(y, model.oob_prediction_)\n    print('C-stat: ', roc)\n    results.append(roc)\n    print(\"\")\n    \npd.Series(results, n_estimator_options).plot();","d198030f":"results = []\nmax_features_options = ['auto', None, 'sqrt', 'log2', 0.9, 0.2]\n\nfor max_features in max_features_options:\n    model = RandomForestRegressor(trees, oob_score=True, n_jobs=-1, random_state=42)\n    model.fit(X, y)\n    print(trees, 'trees')\n    roc = roc_auc_score(y, model.oob_prediction_)\n    print('C-stat: ', roc)\n    results.append(roc)\n    print(\"\")\n    \npd.Series(results, max_features_options).plot(kind=\"barh\", xlim=(.85,.88));","01ca44a4":"results = []\nmin_samples_leaf_options = [1,2,3,4,5,6,7,8,9,10]\n\nfor min_samples in min_samples_leaf_options:\n    model = RandomForestRegressor(n_estimators=1000,\n                                 oob_score=True,\n                                 n_jobs=-1,\n                                 random_state=42,\n                                 max_features=\"auto\",\n                                 min_samples_leaf=min_samples)\n    model.fit(X, y)\n    print(min_samples,\"min samples\")\n    roc = roc_auc_score(y, model.oob_prediction_)\n    print(\"C-stat: \",roc)\n    results.append(roc)\n    print(\"\")\n    \npd.Series(results, min_samples_leaf_options).plot()","42a05357":"X.drop(['Cabin_T'],inplace=True,axis=1)\n# X.drop(['Cabin_None'],inplace=True,axis=1)","c5d0621d":"model = RandomForestClassifier(n_estimators=1000,\n                             oob_score=True,\n                             n_jobs=-1,\n                             random_state=42,\n                             max_features=\"auto\",\n                             min_samples_leaf=5)\nmodel.fit(X[:-200], y[:-200]).score(X[-200:], y[-200:])","10752ff3":"model = AdaBoostClassifier()\nmodel.fit(X[:-200], y[:-200]).score(X[-200:], y[-200:])","994f7166":"model = SVC()\nmodel.fit(X[:-200], y[:-200]).score(X[-200:], y[-200:])","1ef2101c":"model = LogisticRegression()\nmodel.fit(X[:-200], y[:-200]).score(X[-200:], y[-200:])","13b54d38":"model = XGBClassifier()\nmodel.fit(X[:-200], y[:-200]).score(X[-200:], y[-200:])","1f1bb713":"X.columns.values","8803eb79":"### Final Model","3226832a":"### min_samples_leaf","72d9fb29":"### max features","2bfd116a":"### n-estimators","73d13a03":"I want to compare different learning models at classifying the Titanic data set. I would like gain some amount of understanding as to why one would choose one model as opposed to the other."}}