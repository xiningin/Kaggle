{"cell_type":{"9e3574f5":"code","84b21543":"code","d2c2dec2":"code","f4915fd8":"code","d40c03b7":"code","d71c520c":"code","55d72398":"code","e86163b1":"code","f6eff18f":"code","5f53471c":"code","ad0a3fdf":"code","cfa32533":"code","e1193e9a":"code","8b9236ec":"code","e60bec39":"code","522e7c69":"code","d3b16451":"code","86f0e4b2":"code","a09765cc":"code","8b97ff25":"code","19813f7a":"code","9df8365d":"code","228b8be3":"code","9edc1cf8":"code","e1a11128":"code","e2f62709":"code","19427473":"code","280f5f1b":"code","cab7600f":"code","2e7dcce3":"code","81dbbd2f":"code","20ddce5a":"code","f8f5b3e6":"code","59626666":"code","9e922896":"code","3e875e04":"code","40d96dba":"markdown","44bd5217":"markdown","c6759372":"markdown","e9c14d17":"markdown","958ef2b3":"markdown","b375fd06":"markdown","2a28753a":"markdown","67755072":"markdown","2ef1f68c":"markdown","3eb07edd":"markdown","689f4fed":"markdown","63be400e":"markdown","b00d8cad":"markdown","74eefbbd":"markdown","92005a60":"markdown","92186fee":"markdown","4ce4ef18":"markdown","5f484ae0":"markdown","2563d30e":"markdown","5dd3a152":"markdown","fffb0ab8":"markdown","7bded10d":"markdown","7daf123c":"markdown","56bf1cb5":"markdown","201d5c3b":"markdown","a6311c31":"markdown","7adf0d9f":"markdown","64b6c06d":"markdown","145ed5af":"markdown","9a05d22b":"markdown","bfd8254a":"markdown","e2f56a49":"markdown"},"source":{"9e3574f5":"##importing libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","84b21543":"data = pd.read_csv(os.path.join(dirname, filename))","d2c2dec2":"data.head()","f4915fd8":"data.shape","d40c03b7":"data.info()","d71c520c":"data.describe()","55d72398":"data.isnull().sum()","e86163b1":"data['diagnosis'].value_counts(normalize=True)","f6eff18f":"data.drop(['id','Unnamed: 32'],axis=1,inplace=True)","5f53471c":"data.head()","ad0a3fdf":"fields=data.drop(['diagnosis'],axis=1)\ny = (data['diagnosis'] == 'M').astype(int)\ncorrelations = fields.corrwith(y)\ncorrelations.plot(kind='bar')\n\n","cfa32533":"drop_list=correlations[correlations<0.2].index","e1193e9a":"from sklearn.preprocessing import LabelBinarizer\n\nlb = LabelBinarizer()\ndata['diagnosis'] = lb.fit_transform(data['diagnosis'])","8b9236ec":"data['diagnosis']","e60bec39":"data.drop(drop_list,axis=1,inplace=True)\ndata.head()","522e7c69":"predictors=data.columns[1:]","d3b16451":"plt.figure(figsize=(18,18))\nsns.heatmap(data[predictors].corr(),linewidths = 1, annot = True, fmt = \".2f\")\nplt.show()","86f0e4b2":"data[predictors].hist(figsize=(18,18))","a09765cc":"for col in predictors:\n    sns.scatterplot(x=data[col],y=data['diagnosis'])\n    plt.show()","8b97ff25":"from sklearn.model_selection import StratifiedShuffleSplit\nstrat_shuff_split = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)\n\n# Get the index values from the generator\ntrain_idx, test_idx = next(strat_shuff_split.split(data[predictors], data['diagnosis']))\n\n# Create the data sets\nX_train = data.loc[train_idx, predictors]\ny_train = data.loc[train_idx, 'diagnosis']\n\nX_test = data.loc[test_idx, predictors]\ny_test = data.loc[test_idx, 'diagnosis']","19813f7a":"y_train.value_counts(normalize=True)","9df8365d":"y_test.value_counts(normalize=True)","228b8be3":"from sklearn.preprocessing import StandardScaler\nss=StandardScaler()","9edc1cf8":"X_train=ss.fit_transform(X_train)\nX_test=ss.transform(X_test)","e1a11128":"from sklearn.linear_model import LogisticRegression\n\n# Standard logistic regression\nlr = LogisticRegression(solver='liblinear').fit(X_train, y_train)\ny_pred=lr.predict(X_test)","e2f62709":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","19427473":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nprint(classification_report(y_test, y_pred))","280f5f1b":"from sklearn.svm import SVC\nSVC_Gaussian = SVC(kernel='rbf', gamma=0.1,C=10)\nSVC_Gaussian.fit(X_train,y_train)\ny_pred = SVC_Gaussian.predict(X_test)\nprint(classification_report(y_test, y_pred))","cab7600f":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(random_state=42)\ndt = dt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\nprint(classification_report(y_test, y_pred))","2e7dcce3":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'max_depth':range(1, dt.tree_.max_depth+1, 2),\n              'max_features': range(1, len(dt.feature_importances_)+1)}\n\nGR = GridSearchCV(DecisionTreeClassifier(random_state=42),\n                  param_grid=param_grid,\n                  scoring='accuracy',\n                  n_jobs=-1)\n\nGR = GR.fit(X_train, y_train)","81dbbd2f":"GR.best_estimator_.tree_.node_count, GR.best_estimator_.tree_.max_depth","20ddce5a":"y_test_pred_gr = GR.predict(X_test)","f8f5b3e6":"print(classification_report(y_test, y_test_pred_gr))","59626666":"feature_imp = pd.Series(dt.feature_importances_,index=data.columns[1:]).sort_values(ascending=False)\n\nax = feature_imp.plot(kind='bar', figsize=(16, 6))\nax.set(ylabel='Relative Importance');\nax.set(xlabel='Feature');","9e922896":"from sklearn.ensemble import RandomForestClassifier\n\nparam_grid = {'n_estimators':[15, 20, 30, 40, 50, 100, 150, 200, 300, 400]}\n\nGR = GridSearchCV(RandomForestClassifier(random_state=42),\n                  param_grid=param_grid,\n                  scoring='accuracy',\n                  n_jobs=-1)\nGR = GR.fit(X_train, y_train)\ny_test_pred_gr = GR.predict(X_test)\nprint(GR.best_estimator_)\nprint(\"\")\nprint(classification_report(y_test, y_test_pred_gr))\nrf=RandomForestClassifier(n_estimators=400).fit(X_train,y_train)","3e875e04":"from sklearn.ensemble import GradientBoostingClassifier\nparam_grid = {'n_estimators': [15, 20, 30, 40, 50, 100, 150, 200, 300, 400,500],\n              'learning_rate': [0.1, 0.01, 0.001, 0.0001],\n              'subsample': [1.0, 0.5],\n              'max_features': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]}\n\n# The grid search object\nGV_GBC = GridSearchCV(GradientBoostingClassifier(random_state=42), \n                      param_grid=param_grid, \n                      scoring='accuracy',\n                      n_jobs=-1)\n\n# Do the grid search\nGV_GBC = GV_GBC.fit(X_train, y_train)\n\ny_test_pred_gr = GV_GBC.predict(X_test)\nprint(GV_GBC.best_estimator_)\nprint(\"\")\nprint(classification_report(y_test, y_test_pred_gr))","40d96dba":"## Decision Tree Classifier","44bd5217":"**Random Forest Classifier gives an accuracy of 97%**","c6759372":"## Exploratory Data Analysis","e9c14d17":"**Support vector machines give an accuracy of 98%**","958ef2b3":"# Breast Cancer Prediction ","b375fd06":"**Using GridSearchCV because we can't let our decision tree classfier be behind**","2a28753a":"Features like **smoothness_se , fractal_dimension_mean , texture_se , symmetry_se , fractal_dimension_se** will be dropped without question as they contribute so little to the outcome. ","67755072":"### Checking correlation of features with the target variable","2ef1f68c":"### Summary of initial exploration of data\nThe data has 569 examples and 33 features .\nIt has missing values in one column - **Unnamed: 32**\nBy just looking at the data we can clearly see that **id** and **Unnamed: 32** are useless columns which we will drop.\nThe second column i.e **diagnosis** is what we're trying to predict.We can also infer that every column in the dataframe which we are not trying to predict is a continuous numerical feature.We can see that 37.25 percent of the tumors are malignant and 62.74 percent of the tumors are benign. Therefore, we will be using a stratified shuffle split for the data. This kind of shuffling helps us maintain the proportion of each class in the train and test split.","3eb07edd":"## Suggestions\n\n1) My suggestion would be to try feature selection in the early stages and use only those features that have a high relative   importance and then train the model. Maybe that will increase the accuracy even more .\n\n2) After that stacking classifier can be used which eg. using a voting classifier with logistic regression and gradient boosting which should increase the accuracy even more.\n","689f4fed":"## Initial exploration of data","63be400e":"## Feature importances","b00d8cad":"## Random Trees Classifier","74eefbbd":"**Gradient Boosting also resulted in an accuracy of 98%**","92005a60":"### Checking correlation of features amongst themselves","92186fee":"This shows that only - \n\n**perimeter_worst,concave_points_worst,compactness_worst,texture_worst,texture_mean,area_se,\nradius_mean,fractal_dimension_worst,symmetry_worst,smoothness_worst,compactness_se,perimeter_mean**\n\ncould have been used as they are the only important features","4ce4ef18":"## Introduction ","5f484ae0":"## Support Vector Machine","2563d30e":"**These scatter plots show that-**\n\n***tumors with high radius, area, perimeter and concave points are usually malignant***","5dd3a152":"## KNN","fffb0ab8":"**KNN also gives an accuracy of 98%**","7bded10d":"### Description of dataset\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\nn the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n\nThis database is also available through the UW CS ftp server:\nftp ftp.cs.wisc.edu\ncd math-prog\/cpo-dataset\/machine-learn\/WDBC\/\n\nAlso can be found on UCI Machine Learning Repository: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+%28Diagnostic%29\n\nAttribute Information:\n\n1) ID number\n2) Diagnosis (M = malignant, B = benign)\n\n3-32)Ten real-valued features are computed for each cell nucleus:\n\na) radius (mean of distances from center to points on the perimeter)\nb) texture (standard deviation of gray-scale values)\nc) perimeter\nd) area\ne) smoothness (local variation in radius lengths)\nf) compactness (perimeter^2 \/ area - 1.0)\ng) concavity (severity of concave portions of the contour)\nh) concave points (number of concave portions of the contour)\ni) symmetry\nj) fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three\nlargest values) of these features were computed for each image,\nresulting in 30 features. For instance, field 3 is Mean Radius, field\n13 is Radius SE, field 23 is Worst Radius.\n\nAll feature values are recoded with four significant digits.\n\nMissing attribute values: none\n\nClass distribution: 357 benign, 212 malignant","7daf123c":"**We can see that most of the features are right skewed . Therefore, it would be better if we scaled the features.**","56bf1cb5":"## Preparing the dataset for fitting ","201d5c3b":"**Decision Tree gives an accuracy of just 89%**","a6311c31":"## Gradient Boosting","7adf0d9f":"### Objective\n**We will be using this dataset to predict whether a particular tumor is malignant or benign . Therefore, this is a prediction problem** ","64b6c06d":"**Okay We got it upto 91%**","145ed5af":"**It can be seen that many of the features are highly correlated with each other**\n1) radius_mean,perimeter_mean_area_mean,radius_worst,perimeter_worst,area_worst are highly correlated with each other.\n\n2) radius_se,perimeter_se and area_se are highly correlated with each other.\n\n3) texture_mean and texture_worst are highly correlated\n\n4) compactness,concavity,concave_points mean and worst values are also correlated with each other without any pattern which is w    quite confusing !","9a05d22b":"**Logistic Regression gives an accuracy of 98%**","bfd8254a":"## Conclusions\n1) Since this is a medical problem our main objective is to accuractly predict the malignant tumors i.e the label 1 . Therefore, the main aim should be to decrease the number of false negatives and have the highest possible recall as possible. This can be achieved by any of the 98% classifiers . However, I would reccomend using the gradient boosted algorithm as it has the highest recall for both the classes. \n\n2) Only some of the features are really important out of the 33 features with which we started . We can clearly see grid seach showing the maximum number of features used were 3 and the feature importances showed by the decision tree.\n\n","e2f56a49":"## Logistic Regression"}}