{"cell_type":{"b0a8b683":"code","924e5f1b":"code","3bdc8eb8":"code","1e8462b6":"code","863121d1":"code","8dbd94a4":"code","b1801205":"code","0b21c54f":"code","0e5dcaa8":"code","b08e1a09":"code","80c3830f":"code","244efadc":"code","51224902":"code","e22250cb":"code","d6d8fde3":"code","14c346aa":"code","e400da78":"code","79c8a8b9":"code","1657410e":"code","fe166a81":"code","54364626":"code","13ead15a":"code","c1c8ef3c":"code","c2f33c50":"code","71fe449a":"code","fcdc83e2":"code","a76bf1b0":"code","69e823a3":"code","ed49357c":"code","b6b7d810":"code","e96baa2a":"markdown","089337d9":"markdown","cb1cb03e":"markdown","7f2882da":"markdown","d236bd6c":"markdown","ba5c1760":"markdown","3b321f21":"markdown","26f0afd2":"markdown","20d3f175":"markdown","16697196":"markdown","5f91918e":"markdown","07f355fe":"markdown","a0122ce2":"markdown"},"source":{"b0a8b683":"# Ignore warnings :\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pandas import DataFrame\n\ndata = pd.read_csv(\"..\/input\/kc_house_data.csv\")\ndata_copy = data.copy()","924e5f1b":"data.sample(5)","3bdc8eb8":"#Let's get some basic info of data types\ndata.info()","1e8462b6":"# It seems there are no Null Values.\n# Let's Confirm\n\nprint(data.isnull().any().sum(),'\/',len(data.columns))\nprint(data.isnull().any(axis=1).sum(), '\/', len(data))","863121d1":"data.describe()","8dbd94a4":"sns.catplot(data=data , kind='box' , height=7, aspect=2.5)\nplt.show()","b1801205":"corr = data.corr()\nplt.figure(figsize=(20,16))\nsns.heatmap(data=corr, square=True , annot=True, cbar=True)\nplt.show()","0b21c54f":"from scipy.stats import pearsonr\n#It helps to measures the linear relationship between two datasets\n\nfeatures = data.iloc[:,3:].columns.tolist()\ntarget = data.iloc[:,2].name","0e5dcaa8":"correlations = {}\nfor f in features:\n    data_temp = data[[f,target]]\n    x1 = data_temp[f].values\n    x2 = data_temp[target].values\n    key = f + ' vs ' + target\n    correlations[key] = pearsonr(x1,x2)[0]","b08e1a09":"data_correlations = pd.DataFrame(correlations, index=['Value']).T\ndata_correlations.loc[data_correlations['Value'].abs().sort_values(ascending=False).index].head()","80c3830f":"columns = data[['sqft_living','grade','sqft_above','sqft_living15','bathrooms','price']]\n\nsns.pairplot(columns, kind=\"scatter\", palette=\"Set1\")\nplt.show()","244efadc":"plt.figure(figsize = (12, 6))\n\nplt.subplot(121)\nplt.title('Living Area Distribution')\nsns.distplot(data['sqft_living'])\n          \nplt.subplot(122)\nplt.title('Living Area in 2k15 Distribution')\nsns.distplot(data['sqft_living15'])\n\nplt.show()\n\nplt.figure(figsize = (12, 6))\n\nplt.title('Upper Area Distribution')\nsns.distplot(data['sqft_above'])\nplt.show()","51224902":"sns.catplot(x='grade', data=data , kind='count',aspect=2.5 )\nplt.show()\n\n\nsns.catplot(x='grade', y='price', data=data, kind='violin' ,aspect=2.5 )\nplt.show()","e22250cb":"data[\"bathrooms\"] = data['bathrooms'].round(0).astype(int)\n\nsns.catplot(x='bathrooms', data=data , kind='count',aspect=2.5 )\nplt.show()\n\nsns.catplot(x='bathrooms', y='price', data=data, kind='box' ,aspect=2.5 )\nplt.show()","d6d8fde3":"data[\"bathrooms\"] = data['bathrooms'].round(0).astype(int)\n\nlabels = data.bathrooms.unique().tolist()\nsizes = data.bathrooms.value_counts().tolist()\n\nprint(labels)\n\n\nexplode = (0.1,0.0,0.1,0.1,0.0,0.2,0.4,0.6,0.8)\nplt.figure(figsize=(20,20))\nplt.pie(sizes, explode=explode, labels=labels,autopct='%2.2f%%', startangle=0)\nplt.axis('equal')\nplt.title(\"Percentage of Clarity Categories\")\n\nplt.legend(labels,\n          title=\"No. of bathrooms\",\n          loc=\"center left\")\n          #bbox_to_anchor=(1, 0, 0.5, 1))\nplt.plot()\nplt.show()","14c346aa":"data2 = data.drop(['date','id'],axis=1)\n#I dont think that date and id help us in predicting prices","e400da78":"#Divide the Dataset into Train and Test, So that we can fit the Train for Modelling Algos and Predict on Test.\nfrom sklearn.model_selection import train_test_split","79c8a8b9":"x = data2.drop(['price'], axis=1)\ny = data2['price']\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=42)","1657410e":"x_train.shape,y_train.shape,x_test.shape,y_test.shape","fe166a81":"from sklearn.linear_model import LinearRegression,Ridge,ElasticNet\nfrom sklearn.model_selection import cross_val_score\n\nlr=LinearRegression()","54364626":"lr.fit(x_train,y_train)\nscore1= lr.score(x_test,y_test)\naccu1 = cross_val_score(lr,x_train,y_train,cv=5)\nprint(\"___Linear Regresion____\\n\")\nprint(score1)\nprint(accu1)","13ead15a":"from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,ExtraTreesRegressor,GradientBoostingRegressor,BaggingRegressor\nrf = RandomForestRegressor()","c1c8ef3c":"rf.fit(x_train,y_train)\n\nscore2 = rf.score(x_test,y_test)\naccu2 = cross_val_score(rf,x_train,y_train,cv=5)\nprint(\"____ Random Forest Regressor____\\n\")\nprint(score2)\nprint(accu2)","c2f33c50":"br = BaggingRegressor()\nbr.fit(x_train,y_train)\n\nscore3 = br.score(x_test,y_test)\naccu3= cross_val_score(br,x_train,y_train,cv=5)\nprint(\"____Bagging Regressor____\\n\")\nprint(score3)\nprint(accu3)","71fe449a":"from sklearn.neighbors import KNeighborsRegressor\nknr = KNeighborsRegressor()","fcdc83e2":"gb = GradientBoostingRegressor(n_estimators=1000)\ngb.fit(x_train,y_train)\n\nscore4 = gb.score(x_test,y_test)\naccu4 = cross_val_score(gb,x_train,y_train,cv=5)\nprint(\"____ Gradient Boosting Regressor____\\n\")\nprint(score4)\nprint(accu4)","a76bf1b0":"et = ExtraTreesRegressor(n_estimators=1000)\net.fit(x_train,y_train)\n\nscore5 = et.score(x_test,y_test)\naccu5 = cross_val_score(et,x_train,y_train,cv=5)\nprint(\"____ Extra Tree Regressor____\\n\")\nprint(score5)\nprint(accu5)","69e823a3":"Models = ['Linear Regression','RandomForest Regression','Bagging Regressor','Gradient Boosting Regression','ExtraTree Regression']\nScores = [score1,score2,score3,score4,score5]","ed49357c":"compare = pd.DataFrame({'Algorithms' : Models , 'Scores' : Scores})\ncompare.sort_values(by='Scores' ,ascending=False)","b6b7d810":"sns.factorplot(x='Algorithms', y='Scores' , data=compare, size=6 , aspect=2)\nplt.show()","e96baa2a":"Here my pie plot values are overlapping. Hope you understand it !! :p","089337d9":"## 1. Exploring Dataset","cb1cb03e":"Welcome to my kernel","7f2882da":"<p><ul>Thing can be done in future\n    <li>Applying some more regression models like elastic net, lasso, knearest, ada boost and can compare with these models.\n        <li>Feature Engineering part - May be it works better with total area = area of lot + area of living similarly with data of 2015. I don't try that part.\n            <li>Something to reduce multi-collinearnity. I left that part untouched\n                <li> You can try maxmin or standard scalar for feature scaling but i don't think that it much affect this our model.","d236bd6c":"Inferences drawn<br><li><b>Price and zipcode are inversely related<\/b><br><li><b>Price is highly corelated with sqft_living and grade. <\/b>\n<p> Ok, Nah... I don't get co-related data clearly","ba5c1760":"Yeah..... It seems there is no outliers in our dataset<br>\nFurther the Values are Distributed over a Small Scale.","3b321f21":"Now it seems more clear","26f0afd2":"## Some Data Visualization\nHere I visualize those features which are highly co-related with price","20d3f175":"### Visualising Scores of Algorithms","16697196":"## Modelling Algorithms\nOk so our data is ready to fit on models.","5f91918e":"## Correlation between features","07f355fe":"Do upvote if you like it :)","a0122ce2":"### Checking outliers\nOutliers are extreme values that deviate from other observations on data , they may indicate a variability in a measurement, experimental errors or a novelty."}}