{"cell_type":{"530cc318":"code","754057a5":"code","9d45f320":"code","4e8d020c":"code","a98a3c66":"code","87cf1768":"code","cd0d4482":"code","7d9b7120":"code","1bc83aa8":"code","e531cfc0":"code","f7b9cc73":"code","4261dd2f":"code","788525c2":"code","1611527a":"code","364df358":"code","0268e3f2":"code","30ad0209":"code","d7cb0bc4":"code","501d2b08":"code","76a26bfd":"code","a4071f89":"code","99d766ae":"code","ff1a94cd":"code","4bf0dbe9":"code","5fd8b405":"code","ed1753fc":"code","314ce1ad":"code","75c60b41":"code","8c676705":"code","f11979c5":"code","1def7103":"code","b4aedf4a":"code","2d940165":"code","a055ba5b":"code","e151c1ef":"code","cf4060c4":"code","80af2567":"code","676cbde9":"code","17ecb778":"code","5fa99862":"code","865858e0":"code","b94163a0":"code","637512e0":"code","40e2fc76":"code","aff928f5":"code","678bab0c":"code","789540e0":"code","2564128f":"code","92624791":"code","874b8ea1":"code","441657a0":"code","775b0580":"code","79d3c97b":"code","b3e0ddb6":"code","c374e87f":"code","ba2025a6":"code","982c82d8":"code","14cc0f11":"code","b2dd7a0e":"code","f6ecc259":"code","8633ce22":"code","118a0654":"markdown","dd3bb96d":"markdown","5a8b23c6":"markdown","f48855da":"markdown","6c2b5251":"markdown","4a13d9e5":"markdown","006d3b5c":"markdown","cee4c98d":"markdown","3fcf32f2":"markdown","f763aad0":"markdown","e8a69d8e":"markdown","17751db8":"markdown","7c234949":"markdown","cd61659a":"markdown"},"source":{"530cc318":"import numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.ticker as mtick \nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import roc_auc_score","754057a5":"df_train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf_test =pd.read_csv(\"..\/input\/titanic\/test.csv\") ","9d45f320":"df_train.head()","4e8d020c":"df_test.head()","a98a3c66":"df_train.shape","87cf1768":"df_train.info()","cd0d4482":"#Describing the columns with mean, min and max\ndf_train.describe(include='all')","7d9b7120":"df_test.shape","1bc83aa8":"df_test.info()","e531cfc0":"#Describing the columns with mean, min and max\ndf_test.describe(include='all')","f7b9cc73":"# make sure no of duplicate rows are zero\nsum(df_train.duplicated(subset = \"PassengerId\")) == 0","4261dd2f":"# check for NA values\nna_columns = (df_train.isnull().sum())\nna_columns = (df_train.isnull().sum()*100\/df_train.shape[0])\nna_columns = na_columns[na_columns>0]\nna_columns.sort_values(inplace=True, ascending=False)\nna_columns","788525c2":"#Selecting the  columns which has 70% missing value\ndrop_columns = na_columns[(na_columns > 70) == True].index\ndrop_columns","1611527a":"#Dropping the columns with 70% missing values\ndf_train = df_train.drop(drop_columns, axis = 1) \ndf_train.columns","364df358":"#Dropping 'PassengerId' 'Name' 'Fare' and 'Ticket' as they are not contributing directly to survival\ndf_train = df_train.drop(['PassengerId','Name','Fare','Ticket'], axis=1) \ndf_train.columns","0268e3f2":"#Replacing the null values in 'Age' colum using mean\ndf_train['Age'] = df_train['Age'].fillna(df_train['Age'].mean())","30ad0209":"#Checking the NA values and confirming the number is zero\ndf_train['Age'].isnull().sum()","d7cb0bc4":"#Replacing the null values of 'Embarked' column with most frequent  value\ndf_train['Embarked'] = df_train['Embarked'].fillna(df_train['Embarked'].value_counts().index[0])","501d2b08":"#Checking the NA values and confirming the number is zero\ndf_train['Embarked'].isnull().sum()","76a26bfd":"# make sure no of duplicate rows are zero\nsum(df_test.duplicated(subset = \"PassengerId\")) == 0","a4071f89":"# check for NA values in test data\nna_test = (df_test.isnull().sum())\nna_test = (df_test.isnull().sum()*100\/df_test.shape[0])\nna_test = na_test[na_test>0]\nna_test.sort_values(inplace=True, ascending=False)\nna_test","99d766ae":"#Selecting the  columns which has 70% missing value\ntest_drop_columns = na_test[(na_test > 70) == True].index\ntest_drop_columns","ff1a94cd":"#Dropping the columns with 70% missing values\ndf_test = df_test.drop(test_drop_columns, axis = 1) \ndf_test.columns","4bf0dbe9":"PassengerId = df_test['PassengerId']\nPassengerId.head(10)","5fd8b405":"#Dropping 'PassengerId' 'Name' 'Fare' and 'Ticket' from test data as they are not contributing directly to survival\ndf_test = df_test.drop(['PassengerId','Name','Fare','Ticket'], axis=1) \ndf_test.columns","ed1753fc":"#Replacing the null values in 'Age' column using mean\ndf_test['Age'] = df_test['Age'].fillna(df_test['Age'].mean())","314ce1ad":"#Checking the NA values and confirming the number is zero\ndf_test['Age'].isnull().sum()","75c60b41":"#Deriving new feature 'family_size' in train data by combining 'SibSp' and 'Parch'\ndf_train['family_size'] = df_train['SibSp'] + df_train['Parch']\ndf_train[['family_size', 'Survived']].groupby(['family_size'], as_index=False).mean().sort_values(by='Survived', ascending=False)","8c676705":"# Creating age bins in train data\ndf_train['age_bins'] = pd.cut(x=df_train['Age'], bins=[0,10,60,80], labels = [10, 60,80])\ndf_train['age_bins'].value_counts()","f11979c5":"#Deriving new feature 'family_size' in test data by combining 'SibSp' and 'Parch'\ndf_test['family_size'] = df_test['SibSp'] + df_test['Parch']","1def7103":"# Creating age bins in test data\ndf_test['age_bins'] = pd.cut(x=df_test['Age'], bins=[0,10,60,80], labels = [10, 60,80])\ndf_test['age_bins'].value_counts()","b4aedf4a":"sns.barplot(x='Sex', y='Survived', data = df_train)","2d940165":"sns.barplot(x='Pclass', y='family_size', data=df_train)","a055ba5b":"sns.barplot(x='Pclass', y='Survived', data=df_train)","e151c1ef":"sns.barplot(x=\"Survived\", y=\"Pclass\", hue=\"Sex\",data=df_train,orient='h')","cf4060c4":"Embarked_grid = sns.FacetGrid(df_train, row='Embarked', height=4.5, aspect=1.6)\nEmbarked_grid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )\nEmbarked_grid.add_legend()","80af2567":"grid = sns.FacetGrid(df_train,col=\"Pclass\", row=\"Survived\",hue=\"Sex\", aspect=1.6)\ngrid.map(plt.hist, \"Age\",bins=15)","676cbde9":"sns.barplot(x=\"family_size\", y=\"Survived\", hue=\"Sex\",data=df_train)","17ecb778":"with sns.axes_style(style='ticks'):\n    g = sns.catplot(\"Pclass\",\"family_size\", \"Survived\", data=df_train, kind=\"box\")\n    g.set_axis_labels( \"Pclass\", \"family_size\");","5fa99862":"sns.heatmap(df_train.loc[:, ['Survived','Pclass', 'Sex', 'family_size', 'Age', ]].corr(),annot=True)","865858e0":"# Converting train data objects and floats to integers- Sex,Embarked, Age\nlabel_encoder = preprocessing.LabelEncoder()\ndf_train['Sex'] = label_encoder.fit_transform(df_train['Sex'])\ndf_train['Sex'].head(10)","b94163a0":"label_encoder = preprocessing.LabelEncoder()\ndf_train['Embarked'] = label_encoder.fit_transform(df_train['Embarked'])\ndf_train['Embarked'].tail(10)","637512e0":"df_train['Age'] = df_train['Age'].astype(int)\ndf_train['age_bins'] = df_train['age_bins'].astype(int)\ndf_train.info()","40e2fc76":"# Converting test data objects and floats to integers- Age,Sex,Embarked\nlabel_encoder = preprocessing.LabelEncoder()\ndf_test['Sex'] = label_encoder.fit_transform(df_test['Sex'])\ndf_test['Sex'].head(10)","aff928f5":"label_encoder = preprocessing.LabelEncoder()\ndf_test['Embarked'] = label_encoder.fit_transform(df_test['Embarked'])\ndf_test['Embarked'].head(10)","678bab0c":"df_test['Age'] = df_test['Age'].astype(int)\ndf_test['age_bins'] = df_test['age_bins'].astype(int)\ndf_test.info()","789540e0":"X_train = df_train.drop(\"Survived\", axis=1)\nY_train = df_train[\"Survived\"]\nX_test  = df_test\nX_train.shape, Y_train.shape, X_test.shape","2564128f":"X_train.info()","92624791":"X_test.info()","874b8ea1":"# Logistic regression\nlogreg = LogisticRegression(max_iter=10000)\nlogreg.fit(X_train, Y_train)\n\nY_pred = logreg.predict(X_train)\n\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nprint(acc_log)","441657a0":"# random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)","775b0580":"# Decision Tree\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","79d3c97b":"# Perceptron\nperceptron = Perceptron(max_iter=100)\nperceptron.fit(X_train, Y_train)\n\nY_pred = perceptron.predict(X_test)\n\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)","b3e0ddb6":"# Gaussian Naive Bayes\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","c374e87f":"# Stochastic Gradient Descent\nsgd = SGDClassifier(max_iter=5, tol=None)\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\n\nsgd.score(X_train, Y_train)\n\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)","ba2025a6":"# Linear svc\nlinear_svc = LinearSVC(max_iter=10000)\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","982c82d8":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","14cc0f11":"coeff_df = pd.DataFrame(df_train.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","b2dd7a0e":"models = pd.DataFrame({\n    'models': [ 'Logistic Regression','Random Forest', 'Decision Tree',\n               'Perceptron','Gaussian Naive Bayes',  \n              'Stochastic Gradient Decent',  \n              'Linear SVC','Support Vector Machines'],\n    'Score': [acc_log, acc_random_forest, acc_decision_tree\n              , acc_perceptron, acc_gaussian,\n              acc_sgd, acc_linear_svc, acc_svc,]})\nmodels.sort_values(by='Score', ascending=False)\nmodels","f6ecc259":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\n","8633ce22":"submission = pd.DataFrame({\"PassengerId\": PassengerId,\"Survived\": Y_pred})\nsubmission.to_csv('submission.csv', index=False)","118a0654":"# Model Evaluation","dd3bb96d":"# Train Data","5a8b23c6":"From the above table we can see that 38% people survived.maximum age is 80.","f48855da":"**Importing all the required libraries**","6c2b5251":"# Visual Analysis","4a13d9e5":"# Data Preprocessing","006d3b5c":"The train data has 891 cases and 12 features including survived.Out of that 5 are objects, 5 are integers and and 2 are floats","cee4c98d":"# Deriving new features","3fcf32f2":"# Test Data","f763aad0":"# Exploring the data","e8a69d8e":"# Reading the data","17751db8":"# Training a model","7c234949":"The test data has 418 cases and 11 features.Out of that 5 are objects,4 are integers and 2 are floats","cd61659a":"# Model Building"}}