{"cell_type":{"953e4354":"code","1ec075b4":"code","c2d7e215":"code","6a5c7409":"code","8eb1b0ab":"code","5bb452ed":"code","d21710f7":"code","e36bb831":"code","b271f436":"code","2dc9e51b":"code","da3957d4":"code","6a03bcbf":"code","e1d409e5":"code","984b1b4e":"code","b802b1a1":"code","a2178d48":"markdown","7f773c58":"markdown","bd1ad414":"markdown","ce527896":"markdown","dce48491":"markdown","8e2faaf6":"markdown","da1f4f3c":"markdown","9e2e3b76":"markdown","41b8ec74":"markdown","bdaecdb5":"markdown","6b54a530":"markdown","21e7ada0":"markdown","18c59198":"markdown","960e8aa1":"markdown","60c03e11":"markdown","dc4def32":"markdown","ea745385":"markdown","3efdbd89":"markdown","a6efb216":"markdown","c261ffcd":"markdown","f6ad7ff9":"markdown","1edf87e1":"markdown","d8fb6555":"markdown","7e397dd8":"markdown","19bb4dce":"markdown","eacdb82f":"markdown","a93f9789":"markdown","936d21a6":"markdown","6298e5be":"markdown","e56bb6ff":"markdown"},"source":{"953e4354":"# Import order: data manipulating -> machine\/deep learning -> utilities\/helpers\/improvement -> configuration\nimport pandas as pd\nimport numpy as np\nimport scipy as sc\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None # Show all as it has\npd.options.display.max_rows = None","1ec075b4":"# Here, we use the intermediate result from ref [1] (after feature processing)\ntrain = pd.read_csv('..\/input\/optiverrealizedvolatilitypredictionpreprocess\/train_dataset.csv')\ntest = pd.read_csv('..\/input\/optiverrealizedvolatilitypredictionpreprocess\/test_dataset.csv')","c2d7e215":"train.head()","6a5c7409":"train.columns","8eb1b0ab":"remove_cols = [\"Unnamed: 0\"]\ntrain = train.drop(remove_cols, axis=1)","5bb452ed":"train.head()","d21710f7":"# Check how many null values for each col\ntrain.isnull().sum()","e36bb831":"# Get X out of train\nX = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\nX.iloc[:, :] = imp.fit_transform(X)","b271f436":"# Let's verify\nX.isnull().sum()","2dc9e51b":"train.describe()","da3957d4":"scaler = StandardScaler()\n# Don't forget to keep 'stock_id' out, as this number matters for its \"large range\"\nX_features = X.loc[:, X.columns != 'stock_id']\nX_features.iloc[:, :] = scaler.fit_transform(X_features)","6a03bcbf":"# You see, here we got mean around 0 and std around 1 for all cols\nX_features.describe()","e1d409e5":"X_features = X.loc[:, X.columns != 'stock_id']\nsteps = [\n    ('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n    ('scaler', StandardScaler())\n]\npipe = Pipeline(steps)\nX_features.iloc[:, :] = pipe.fit_transform(X_features)\nX.loc[:, X.columns != 'stock_id'] = X_features","984b1b4e":"X = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train['target']\n\nparam_grid = {\n    'boosting_type': ['gbdt', 'rf', 'dart', 'goss'],\n    'learning_rate': [0.1, 0.01, 0.001],\n    'num_leaves': [31, 100, 200, 500, 1000],\n    'feature_fraction': [0.5, 0.8, 1.0],\n    'bagging_fraction': [0.5, 0.8, 1.0]\n    \n}\n\nmodel = lgb.LGBMModel(objective='rmse')\ngs = GridSearchCV(model, param_grid=param_grid, scoring='neg_root_mean_squared_error', verbose=2, refit=True, cv=5, n_jobs=-1)\ngs.fit(X, y)\nprint('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))","b802b1a1":"from scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nfrom sklearn.model_selection import RandomizedSearchCV\n\nX = train.drop(['row_id', 'target', 'time_id'], axis = 1)\ny = train['target']\n\nparam_grid = {\n    'num_leaves': sp_randint(10, 1000), \n    'min_child_samples': sp_randint(10, 500), \n    'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n    'bagging_fraction': sp_uniform(loc=0.2, scale=0.8), \n    'feature_fraction': sp_uniform(loc=0.4, scale=0.5),\n    'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n    'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]\n}\n\nmodel = lgb.LGBMModel(objective='rmse')\ngs = RandomizedSearchCV(model, param_grid, n_iter=1000, scoring='neg_root_mean_squared_error', verbose=2, refit=True, cv=5, n_jobs=-1)\ngs.fit(X, y)\nprint('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))","a2178d48":"Several cols, like trade_log_return_realized_volatility_450, contain around 1700 null values. But compared with 428932 rows, it's not a large number.\n\nSo let's impute it rather than deleting.","7f773c58":"# Reference","bd1ad414":"**Version Notes**\n- V2.0 - Model Tuning | 3rd Sep 2021\n- V1.0 - 1st published & submitted version | 29th Aug 2021\n\n**Work Principles**\n- A clean & comfortable format to help our brains digest\n- Occam's Razor - Non sunt multiplicanda entia sine necessitate\n- Refactoring \n\n**General Notes**\n- This notebook will focus on ideas tuning our model based on ref [1], so more code snippets here which you could \"digest\" with [1] for a submission.","ce527896":"# Libs Import","dce48491":"**Normalization**","8e2faaf6":"# Overview\n- Libs Import\n- Clean Data\n    - Remove Coluns \n    - Imputing\n    - Normalization\n    - Pipeline\n- GridSearchCV & RandomSearchCV\n    - GridSearch\n    - RandoSearch\n    - Cloud Computing\n- Summary\n- Reference","da1f4f3c":"After applying all above (except StandardScaler as worsing the score), the learderboard score got immproved for a bit, around top 48% from 42%. Not an impressive one right.\n\nI tried several rounds more with GridSearch and RandomSearch, seems no better \"magic\" hyperparams I found for LGBM model.\n\nCheck back [the best-score ranked posts](https:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/code?competitionId=27233&sortBy=scoreAscending), I found most of them mentioned the NN\/Neural Network which is combined with using LGBM. I guess this result could be around the limit for the LGBM-only approach.\n\nThere should be some space for using the information from time series. A post I found here -- [Exploring time_id relationship](https:\/\/www.kaggle.com\/stassl\/exploring-time-id-relationships), leaving an open end for everyone to try.","9e2e3b76":"NaN or None values is a classical problem we need to do for our dataset, like washing your vegies \ud83e\udd6c before cutting for dinner. \n\nLet's how \"dirty\" they are now...","41b8ec74":"We are engineers! \ud83d\udc77\u200d No wait for building a pipeline if possible.","bdaecdb5":"**RandomSearchCV**","6b54a530":"**Pipeline it!**","21e7ada0":"**Remove NaN\/None Values**","18c59198":"After having our LGBM baseline as ref [1], the following \"headache\" will undoubtedly be tuning our model for better performance.  \n\nSo several ideas came just on time \ud83d\ude09 - are our data clean enough? do we need a normalization\/standardization? will get better results with certain hyperparams for LightGBM model? as time-related, so possible for time series analysis? ","960e8aa1":"# GridSearchCV & RandomSearchCV (with GCP)","60c03e11":"Here is the result:    \nBest score reached: -0.0010287521953893668 with params: {'bagging_fraction': 0.5, 'boosting_type': 'gbdt', 'feature_fraction': 0.8, 'learning_rate': 0.1, 'num_leaves': 500}","dc4def32":"**Imputing**","ea745385":"**Using Cloud Computing**","3efdbd89":"Check the stat for train dataset, we find the range for mean and std are a bit large for like wap1_sum, wap1_sum_150...\n\nIt is possible to make the model fitting slower and less accurate.","a6efb216":"[[1] [Reproduction & Explanation] LGBM Baseline](https:\/\/www.kaggle.com\/austinzhao\/reproduction-explanation-lgbm-baseline?scriptVersionId=73453864https:\/\/www.kaggle.com\/austinzhao\/reproduction-explanation-lgbm-baseline?scriptVersionId=73453864)","c261ffcd":"# Summary","f6ad7ff9":"Params like num_leaves are continous, quite hard for us to pick reasonable ones. So leave them for RandomSearch here","1edf87e1":"Oops. The 1st col seems weird, right... \"Unnamed\". Where does it come from? \n\nActually it is due to not setting index=False *for to_csv('submission.csv', index=False)*\n\nUse this as an example for snippets -- removing columns","d8fb6555":"Now, we kind of \"washing\" our vegies and pipeline the steps which we assume will perhaps improve our model.","7e397dd8":"Here is result:\nBest score reached: -0.001018671586219258 with params: {'bagging_fraction': 0.8202697666845151, 'feature_fraction': 0.6674647807946869, 'min_child_samples': 154, 'min_child_weight': 1e-05, 'num_leaves': 491, 'reg_alpha': 0, 'reg_lambda': 10}","19bb4dce":"To be honest, it took me a while to understand [all params listed for LGBM model](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html). I knew the basic idea is simple models are arranged into a \"powerful\" one and certain tree structure here, so having \"depth\", \"leaf\"...\n\nOk enough, let's don't waste the time. We could do Grid and Random Search in parallel while reading posts for these params right.","eacdb82f":"Let's try different boosting strategies, see if any differences. and also diff learning rates and 2 fractions which should be the \"promising\" ones.","a93f9789":"**GridSearchCV**","936d21a6":"# Clean Data","6298e5be":"**Remove Columns (as you reason\/want)**","e56bb6ff":"In order to shorten the time and have more trials, I decided to use GCP (a computing resource upgrade option you could find in \"File\", top-left). As we have Parallel code here, more CPUs mean faster. I chose the 96 vCPUs and got the 5000 around fitting compelted for around 1 hour.\n\nMore details for finding a balance for what we need and pricing, chekc [this](https:\/\/cloud.google.com\/compute\/all-pricing).\n\nI think cloud computing should be a valuable, even necessary, part for us to master -- no way seems for us to run such a heavy computing load on local in a reasonable time. "}}