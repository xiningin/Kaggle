{"cell_type":{"2d2af91f":"code","5a0358fa":"code","2f8e69f5":"code","7e75ea2e":"code","b0b17ce8":"code","24ce2fde":"code","4582ff31":"code","cb4c6b32":"code","09f6acb6":"markdown"},"source":{"2d2af91f":"# Imports\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow as tf\nfrom tqdm import tqdm\n\npd.set_option('display.max_rows', 20)\ntf.get_logger().setLevel('WARNING')","5a0358fa":"# Load the data\nrating_df = pd.read_csv('\/kaggle\/input\/movielens-100k-dataset\/ml-100k\/u.data', \n                        names=['userId', 'movieId', 'rating', 'timestamp'],\n                        sep='\\t', encoding='latin-1')\nuser_df = pd.read_csv('\/kaggle\/input\/movielens-100k-dataset\/ml-100k\/u.user', \n                      names=['userId', 'age', 'sex', 'occupation', 'zip_code'],\n                      sep='|', encoding='latin-1')\ngenre_df = pd.read_csv('\/kaggle\/input\/movielens-100k-dataset\/ml-100k\/u.genre', \n                       names=['genre', 'genreId'],\n                       sep='|', encoding='latin-1')\nmovie_df = pd.read_csv('\/kaggle\/input\/movielens-100k-dataset\/ml-100k\/u.item', \n                       names=['movieId', 'title', 'release_date', 'video_release_date', 'imdb_url'] + list(genre_df['genre'].values),\n                       sep='|', encoding='latin-1')","2f8e69f5":"# Map IDs to 0-based indexing\nuser_le = LabelEncoder()\nuser_le.fit(rating_df['userId'].unique())\nrating_df['userId'] = user_le.transform(rating_df['userId'])\nuser_df['userId'] = user_le.transform(user_df['userId'])\nmovie_le = LabelEncoder()\nmovie_le.fit(rating_df['movieId'].unique())\nrating_df['movieId'] = movie_le.transform(rating_df['movieId'])\nmovie_df['movieId'] = movie_le.transform(movie_df['movieId'])","7e75ea2e":"# Build factorization machine model\nembedding_dim = 64\nn_users = len(user_le.classes_)\nn_movies = len(movie_le.classes_)\n\ninputs = tf.keras.Input(shape=(2,))\nuser_emb = tf.keras.layers.Embedding(\n               input_dim=n_users, output_dim=embedding_dim, input_length=1,\n               embeddings_initializer=tf.keras.initializers.GlorotNormal()\n           )\nmovie_emb = tf.keras.layers.Embedding(\n               input_dim=n_movies, output_dim=embedding_dim, input_length=1,\n               embeddings_initializer=tf.keras.initializers.GlorotNormal()\n           )\noutput = tf.keras.layers.Dot(axes=1)([user_emb(inputs[:, 0]), movie_emb(inputs[:, 1])])\nfm_model = tf.keras.Model(inputs=inputs, outputs=output)\nfm_model.summary()","b0b17ce8":"# Set up inputs and target labels\nX = rating_df[['userId', 'movieId']].values\nY = rating_df['rating'].values\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.1, random_state=1, shuffle=True)","24ce2fde":"# Train FM model\nlearning_rate = 0.005\nregularization = 0.01\ngravity_coef = 0.1\nnum_epochs = 100\nbatch_size = 2**17\n\noptimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\nmse = tf.keras.losses.MeanSquaredError()\n\n@tf.function\ndef train_step(x, y):\n    with tf.GradientTape() as tape:\n        y_pred = fm_model(x) \n        U = user_emb.weights[0]\n        V = movie_emb.weights[0]\n        loss_mse = mse(y, y_pred)\n        loss_reg = tf.math.reduce_sum(U * U) \/ U.shape[0] + tf.math.reduce_sum(V * V) \/ V.shape[0]\n        loss_grv = tf.math.reduce_sum(tf.square(tf.matmul(U, V, transpose_b=True))) \/ (U.shape[0] * V.shape[0])\n        loss = loss_mse + regularization * loss_reg + gravity_coef * loss_grv\n    grads = tape.gradient(loss, fm_model.trainable_variables)\n    optimizer.apply_gradients(zip(grads, fm_model.trainable_variables))\n    return loss\n\nfor epoch in range(num_epochs):\n    loss = train_step(X_train, Y_train)\n    \n    U = user_emb.weights[0]\n    V = movie_emb.weights[0]\n    Y_val_pred = fm_model(X_val)\n    val_mse = mse(Y_val, Y_val_pred)\n    val_loss = val_mse + regularization * tf.math.reduce_sum(U * U) \/ U.shape[0] + tf.math.reduce_sum(V * V) \/ V.shape[0] + gravity_coef * tf.math.reduce_sum(tf.square(tf.matmul(U, V, transpose_b=True))) \/ (U.shape[0] * V.shape[0])\n    print('Epoch {}: train_loss - {:.4f}, val_loss - {:.4f}, val_mse - {:.4f}'.format(epoch + 1, loss, val_loss, val_mse))","4582ff31":"# Retrieve embedding table from the model\nuser_emb_table = user_emb.weights[0].numpy()\nmovie_emb_table = movie_emb.weights[0].numpy()","cb4c6b32":"# Sanity check: print movies that are similar in the latent space\n# We can use three measures to check similarity: Euclidean distance, cosine, and inner product\ndef similar_movies(movieId, measure='distance'):\n    emb = movie_emb_table[movieId]\n    if measure == 'cosine':\n        similarities = np.dot(movie_emb_table, emb)\n        norms = np.linalg.norm(movie_emb_table, axis=1)\n        similarities = similarities \/ (norms[movieId] * norms)\n    elif measure == 'dot':\n        similarities = np.dot(movie_emb_table, emb)\n    else:\n        similarities = -np.linalg.norm(movie_emb_table - emb, axis=1)\n    similar_movie_indices = np.argsort(similarities)[::-1]\n    similar_movies = pd.DataFrame({'movieId': similar_movie_indices, \n                                   'title': movie_df['title'].iloc[similar_movie_indices], \n                                   'similarity': similarities[similar_movie_indices]})\n    return similar_movies\n\nprint(similar_movies(94))\nprint(similar_movies(94, measure='cosine'))\nprint(similar_movies(94, measure='dot'))","09f6acb6":"# Movie Rating Prediction with Factorization Machine\nThis notebook utilizes TensorFlow Keras at its most basic level in order to predict a user's rating on movies."}}