{"cell_type":{"0335e3a2":"code","1785ecaf":"code","19cae5e3":"code","b90db1ef":"code","c1baa323":"code","8cce531e":"code","dca3caf8":"code","b727c63d":"code","6f8b1dfd":"code","349a587e":"code","e3c87874":"code","3897d74e":"code","d3be5563":"code","36d0c16a":"code","0f76c4c1":"code","e3d8959b":"code","de12e5ba":"code","58ba987b":"code","67feacc8":"code","2e9db76b":"code","d416b105":"code","b8301988":"code","fb196ed3":"code","bb37d099":"code","c05fbbae":"code","1c09d35d":"markdown","39a2c3a5":"markdown","7c8b931f":"markdown","b407defb":"markdown","4326ad07":"markdown","de64e2b9":"markdown"},"source":{"0335e3a2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","1785ecaf":"train_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntrain_df.head()","19cae5e3":"print(\"Shape of the data: \\n\", train_df.shape)\nprint(\"\\n \\n \", train_df.describe())\nprint(\"\\n \\n \", train_df.info)\n","b90db1ef":"# hyperparameters\n\nvocab_size = 10000\noov_token = \"<OOV>\"\nmax_length = 120\ntrunc_type = 'post'\npadding_type = 'post'\nembedding_dim = 16\ntraining_size = 5000","c1baa323":"# convert df to list\n\nsentences = train_df['text'].astype(str).str.lower().values.tolist()\nlabels = train_df['target'].values.tolist()","8cce531e":"\ntraining_sentences = sentences[0:training_size]\ntesting_sentences = sentences[training_size:]\ntraining_labels = labels[0:training_size]\ntesting_labels = labels[training_size:]","dca3caf8":"tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\ntokenizer.fit_on_texts(training_sentences)\n\nword_index = tokenizer.word_index\ntotal_words = len(word_index)\nprint(\"The total_words: \", total_words)\n\ntraining_sequences = tokenizer.texts_to_sequences(training_sentences)\ntraining_padded = pad_sequences(training_sequences, maxlen=max_length,\n                               padding=padding_type,\n                               truncating=trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences, maxlen=max_length,\n                               padding=padding_type,\n                               truncating=trunc_type)\n","b727c63d":"from tensorflow.keras import regularizers\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nfrom tensorflow.keras.optimizers import Adam\nadam = Adam(learning_rate=0.0001)\n\nmodel.compile(loss='binary_crossentropy',optimizer=adam ,metrics=['accuracy'])\nmodel.summary()","6f8b1dfd":"num_epochs = 60\n\ntraining_padded = np.array(training_padded)\ntraining_labels = np.array(training_labels)\ntesting_padded = np.array(testing_padded)\ntesting_labels = np.array(testing_labels)\n\nhistory = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=1)","349a587e":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n\nplot_graphs(history, 'accuracy')\nplot_graphs(history, 'loss')","e3c87874":"#model = tf.keras.Sequential([\n #   tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n #   tf.keras.layers.Conv1D(128, 5, activation='relu'),\n  #  tf.keras.layers.GlobalMaxPooling1D(),\n   # tf.keras.layers.Dense(24, activation='relu'),\n    #tf.keras.layers.Dense(1, activation='sigmoid')\n#])\n#model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n#model.summary()\n\n#num_epochs = 30\n\n#training_padded = np.array(training_padded)\n#training_labels = np.array(training_labels)\n#testing_padded = np.array(testing_padded)\n#testing_labels = np.array(testing_labels)\n\n#history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=1)","3897d74e":"#import matplotlib.pyplot as plt\n\n\n#def plot_graphs(history, string):\n # plt.plot(history.history[string])\n  #plt.plot(history.history['val_'+string])\n # plt.xlabel(\"Epochs\")\n # plt.ylabel(string)\n  #plt.legend([string, 'val_'+string])\n  #plt.show()\n\n#plot_graphs(history, 'accuracy')\n#plot_graphs(history, 'loss')","d3be5563":"# Getting weights from the embedding\ne = model.layers[0]\nweights = e.get_weights()[0]\nprint(weights.shape)\n\n# Reverse mapping function from token to word\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_review(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])","36d0c16a":"# Combining embedding and words into a DataFrame\nembedding_df = pd.DataFrame()\nfor word_num in range(1, vocab_size):\n    word = reverse_word_index[word_num]\n    embeddings = weights[word_num]\n    embedding_df = embedding_df.append(pd.Series({'word':word, 'em':embeddings}), ignore_index=True)\n    \nembedding_df = pd.concat([embedding_df['em'].apply(pd.Series), embedding_df['word']], axis=1)","0f76c4c1":"# Using PCA to map 16 embedding values to 3 to plot\nfrom sklearn.decomposition import PCA\n\np = PCA(n_components=3)\nprincipal_components = p.fit_transform(embedding_df.filter(regex=r'\\d'))\nembedding_df[['x', 'y', 'z']] = pd.DataFrame(principal_components)\n\nembedding_df.shape","e3d8959b":"import plotly.express as px\n\nfig = px.scatter_3d(embedding_df, x='x', y='y', z='z', hover_name='word', color='x')\nfig.show()","de12e5ba":"test_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntest_df.head()","58ba987b":"test_df.shape","67feacc8":"# convert df to list\n\ntest_sentences = test_df['text'].astype(str).str.lower().values.tolist()\n","2e9db76b":"\ntest_sequences = tokenizer.texts_to_sequences(test_sentences)\ntest_padded = pad_sequences(test_sequences, maxlen=max_length,\n                               padding=padding_type,\n                               truncating=trunc_type)\n","d416b105":"pred = model.predict(test_padded)\n\nbinary_predictions = []\n\nfor i in pred:\n    if i >= 0.5:\n        binary_predictions.append(1)\n    else:\n        binary_predictions.append(0) ","b8301988":"submission_df = pd.DataFrame()\nsubmission_df['id'] = test_df['id']\nsubmission_df['target'] = binary_predictions","fb196ed3":"submission_df","bb37d099":"submission_df['target'].value_counts()","c05fbbae":"submission = submission_df.to_csv('Result.csv',index = False)","1c09d35d":"### Toeknization","39a2c3a5":"### Prediction","7c8b931f":"There are 5 columns in this dataset. Text represents the content of the tweet. Target represents positive(1) or negative(0) comment. These 2 columns are useful for this analysis. Others, which we dont use, are id, keyword and location.\n\nThe goal of this analysis is to predict whether the given tweet is positive or negative.","b407defb":"### Data Overview","4326ad07":"### Import Libraries","de64e2b9":"### Visualization\n"}}