{"cell_type":{"b4aa95b0":"code","f3f202e3":"code","857a2879":"code","9cdbb7d9":"code","beefa6ed":"code","0c0bd1d9":"code","1d41fe46":"code","4588cc26":"code","14a9317f":"markdown"},"source":{"b4aa95b0":"import numpy as np\nimport pandas as pd\nimport scipy.stats\nimport random","f3f202e3":"# helper for \"generate_samples\" function\ndef probability_of_value(value, dist):\n    dist_mean = dist.mean()\n    if value > dist_mean:\n        prob = (1 - dist.cdf(value)) * 2\n    elif value <= dist_mean:\n        prob = dist.cdf(value) * 2\n    \n    return prob","857a2879":"# helper for \"sample_with_standard_error_noise\" function\ndef generate_samples(\n    target,\n    standard_error,\n    standard_error_range,\n    all_targets,\n    sample_num,\n    random_noise\n):\n\n    # this will hold the samples to return\n    valid_samples = []\n    \n    # edge case\n    if target == 0.0:\n        return [0.0 for i in range(sample_num)]\n    \n    # get the limits, and filter targets to that\n    bottom_limit = target - (standard_error * standard_error_range)\n    top_limit = target + (standard_error * standard_error_range)\n    targets_in_range = all_targets[(all_targets >= bottom_limit) & (all_targets <= top_limit)]\n    num_targets = len(targets_in_range) \/\/ 3\n    \n    # create distribution\n    dist = scipy.stats.norm(target, standard_error)\n    \n    # now we create them\n    while len(valid_samples) < sample_num:\n        # just any samples from the range\n        samples = list(targets_in_range.sample(num_targets))\n        for sample in samples:\n            # ok how probable is it?\n            sample_prob = probability_of_value(sample, dist)\n            # should we keep it?\n            if sample_prob > random.random():\n                random_noise_div = 1.0 \/ random_noise\n                valid_samples.append(sample+(random.random()\/random_noise_div-(random_noise\/2)))\n    \n    return valid_samples[:sample_num]","9cdbb7d9":"def sample_with_standard_error_noise(\n    df,\n    standard_error_range,\n    sample_num,\n    random_noise,\n    target_col=\"target\",\n    standard_error_col=\"standard_error\",\n    samples_col=\"samples\"\n):\n    all_targets = df[target_col].copy()\n    \n    working_df = df.copy()\n    working_df[samples_col] = working_df.apply(\n        lambda x: generate_samples(\n                      x[target_col],\n                      x[standard_error_col],\n                      standard_error_range,\n                      all_targets,\n                      sample_num,\n                      random_noise\n                  ), axis=1\n    )\n    \n    return working_df   ","beefa6ed":"# read training data\ntrain_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")","0c0bd1d9":"# adding sample y values\ntrain_df = sample_with_standard_error_noise(\n    df=train_df,\n    standard_error_range=0.5,\n    sample_num=100,\n    random_noise=0.1,\n    samples_col=\"samples\"\n)\ntrain_df = sample_with_standard_error_noise(\n    df=train_df,\n    standard_error_range=0.375,\n    sample_num=100,\n    random_noise=0.075,\n    samples_col=\"samples2\"\n)","1d41fe46":"train_df.head(1)","4588cc26":"train_df.to_csv(\"train_df_with_target_samples.csv\", index=False)","14a9317f":"# This notebook generates target samples as described in [this discussion](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/257470)\n\nThe notebook will output a csv file with two versions of target samples\n\nTo use them in your training notebook:  \n1. read in the output csv file instead of the commonlit train.csv  \n2. when you read in the csv file, use `pd.read_csv(<path_to_output_csv>, converters={'samples': eval, 'samples2': eval})`  \n3. in the \\_\\_get\\_\\_item() in your pytorch dataset, after `target = self.target[index]`, add `target = target[np.random.randint(1,100,1)[0]]`\n                "}}