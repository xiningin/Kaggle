{"cell_type":{"7eff5a3e":"code","65dc6500":"code","c1cc3d79":"code","4b2829b1":"code","3ea6c0d1":"code","ae6394b5":"code","b832dec8":"code","2558dbb6":"code","68b3e721":"code","200b383e":"code","fcf3b24d":"code","978f9a1e":"code","6e0655dd":"code","54170b86":"code","091ca79f":"code","0f63edf1":"code","380c0e0d":"code","e93d5cf3":"code","3ea6b8b2":"code","5abcba4d":"code","fd91d519":"code","4c879a9a":"code","363f7815":"code","78f42997":"code","8667aca4":"code","429732d4":"markdown","dbf24bdf":"markdown","4f0dfdd9":"markdown","cfaf813f":"markdown","adf8eda4":"markdown","bc35b5fe":"markdown","8b4c4572":"markdown","6fdd3ec7":"markdown","4a4066c6":"markdown","983d0f99":"markdown"},"source":{"7eff5a3e":"!pip install pmdarima","65dc6500":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nimport keras\nimport pmdarima as pm\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf","c1cc3d79":"df = pd.read_csv('..\/input\/energy-consumption-dublin-civic-offices-2009-2012\/Energy_consumption.csv')\nprint('Number of days in the data', len(df))\ndf.head()","4b2829b1":"df=df.drop('Date', axis=1)\ndf=df.dropna()\ndf_val=df.values\ndataset=np.sum(df_val, axis=1, dtype=float)\ndataset","3ea6c0d1":"print('The array contataining the',dataset.shape,'days of data')\nprint('Each data containing the aggregrated consumption as sum of total consumption')","ae6394b5":"plt.plot(dataset)\nplt.xlabel('Days')\nplt.ylabel('kWatts')\nplt.title('Energy consumed per day')","b832dec8":"train_df=dataset[:554]\nval_df=dataset[554:]\nplot_acf(train_df, lags=100, zero=False);\n","2558dbb6":"window_size = 7\n\nnumbers_series = pd.Series(train_df)\nwindows = numbers_series.rolling(window_size)\nmoving_averages = windows.mean()\n\nmoving_averages_list = moving_averages.tolist()\nts7 = moving_averages_list[window_size - 1:]\nts7=np.array(ts7)\n\nn1=math.nan\nn7=np.array([n1,n1,n1,n1,n1,n1,n1])\nts7=np.concatenate([n7,ts7])\nplt.plot(train_df)\nplt.plot(ts7)\nplt.xlabel('Days')\nplt.ylabel('Kilowatts')\nplt.title(\"7-day Simple Moving Average\")\nplt.show()","68b3e721":"Arima_model=pm.auto_arima(train_df, start_p=0, start_q=0, max_p=10, max_q=10, start_P=0, start_Q=0, max_P=10, max_Q=10, m=7, stepwise=True, seasonal=True, information_criterion='aic', trace=True, d=1, D=1, error_action='warn', suppress_warnings=True, random_state = 20, n_fits=30)","200b383e":"Arima_model.summary()","fcf3b24d":"prediction=pd.DataFrame(Arima_model.predict(n_periods=129), index=val_df)\nprediction=np.array(prediction)\nprediction=prediction.reshape(129,-1)\nplt.plot(val_df)\nplt.plot(prediction)\nplt.xlabel('Days')\nplt.ylabel('Kilowatts')\nplt.legend([\"Validation\", \"Prediction\"], loc =\"lower right\") \nplt.title(\"Validation vs. Prediction\")\nplt.show()","978f9a1e":"mse = mean_squared_error(val_df, prediction)\nrmse = math.sqrt(mse)\nprint('RMSE: %f' % rmse)","6e0655dd":"\ntrain_size = int(len(dataset) * 0.8)\nval_size = len(dataset) - train_size\ntrain, val = dataset[0:train_size], dataset[train_size:len(dataset)]","54170b86":"train = np.expand_dims(train,axis=1)\nval = np.expand_dims(val,axis=1)","091ca79f":"def create_dataset(df, previous=1):\n    dataX, dataY = [], []\n    for i in range(len(df)-previous-1):\n        a = df[i:(i+previous), 0]\n        dataX.append(a)\n        dataY.append(df[i + previous, 0])\n    return np.array(dataX), np.array(dataY)","0f63edf1":"scaler = MinMaxScaler(feature_range=(0, 1))\ntrain = scaler.fit_transform(train)","380c0e0d":"val = scaler.fit_transform(val)","e93d5cf3":"lookback = 5\nX_train, Y_train = create_dataset(train, lookback)\nX_val, Y_val = create_dataset(val, lookback)","3ea6b8b2":"print(X_train.shape, Y_train.shape, X_val.shape, Y_val.shape)","5abcba4d":"# reshape input to be [samples, time steps, features]\nX_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\nX_val = np.reshape(X_val, (X_val.shape[0], 1, X_val.shape[1]))\nprint(X_train.shape, Y_train.shape, X_val.shape, Y_val.shape)","fd91d519":"# Generate LSTM network\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.LSTM(4, input_shape=(1, lookback)))\nmodel.add(keras.layers.Dense(1))\nmodel.compile(loss='mse', optimizer='adam')\nhistory=model.fit(X_train, Y_train, validation_split=0.2, epochs=100, batch_size=1, verbose=2)\n\n# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","4c879a9a":"valpred = model.predict(X_val)\nvalpred = scaler.inverse_transform(valpred)\nY_val = scaler.inverse_transform([Y_val])\n\n","363f7815":"Y_val = Y_val.reshape(-1)\nvalpred = valpred.reshape(-1)","78f42997":"plt.plot(Y_val_plot)\nplt.plot(valpred_plot)\nplt.xlabel('Days')\nplt.ylabel('Kilowatts')\nplt.title(\"Validation vs. Prediction\")\nplt.show()","8667aca4":"mse = mean_squared_error(Y_val, valpred)\nrmse = math.sqrt(mse)\nprint('RMSE: %f' % rmse)","429732d4":"# LSTM","dbf24bdf":"We can see that the highest positive correlation (after the negative correlations) is identified at lag 7. This indicates that there is a strong correlation with the kilowatt reading 7 days after the original and so ARIMA model is identified as m=7.","4f0dfdd9":"The size of the RMSE accounts for 10% of the mean, which means the model shows superior performance to ARIMA in predicting electricity consumption trends.the LSTM model has been more effective at accounting for the volatility in the series.","cfaf813f":"### References:\n1. https:\/\/towardsdatascience.com\/arima-vs-lstm-forecasting-electricity-consumption-3215b086da77\n2. https:\/\/github.com\/MGCodesandStats\/energy-weather-modelling\n3. Machine Learning vs Statistical Methods for Time Series Forecasting:Size Matters","adf8eda4":"Let us see the ML Vs Stat models contributon in this data. To start with the analysis, we can initiate the modelling using ARIMA- Stat model and LSTM ML model for forcasting the consumption across the validation set ","bc35b5fe":"# ARIMA model\n\nThe ARMA (Auto-Regressive Moving Average) is one of\nthe most commonly used methods to model univariate time\nseries. ARMA(p,q) combines two components: AR(p), and\nMA(q).\n\nAccording to the AR(p) model, the value of a given time\nseries, yn, can be estimated using a linear combination of the\np past observations, together with an error term $\\epsilon_n $ and a constant term c.\n\nwhere,\n\np is the order of the AR term\n\nSimilarly, the MA(q) model uses past errors as explanatory variables\n\nwhere,\nq is the order of the MA term\n\n\nThe ARMA(p,q) is defined for stationary data. However,\nmany interesting phenomena in the real-world exhibit a nonstationary structure, e.g. time series with trend and seasonality. The ARIMA(p,d,q) model overcomes this limitation by\nincluding an integration parameter of order d. Essentially,\nARIMA works by applying d differencing transformations to\nthe time series (until it becomes stationary), before applying\nARMA(p,q).\n","8b4c4572":"### Train and Validation split \nThe first 80% of the time series (554 observations) are used as training data. Here is a plot of the autocorrelation function as generated on this training set.","6fdd3ec7":"ARIMA model has picked up the seasonal trend of the series quite well. That said, the peak consumption predictions in the first half of the graph appear to be quite a bit higher than the actual.\nMoreover, we see that the model fails to capture the drop in kilowatt consumption between roughly days 70\u201380 across the validation set. This anomaly may be contributing to the sizeable error we are seeing for the RMSE value, as RMSE is designed to penalise errors on outliers more heavily.\n","4a4066c6":"# Forecasting Electricity Consumption","983d0f99":"To forecast the total daily consumption and there is expected to be too much volatility if the time series is formed on a 15-minute basis. And so let us sort the data on daily basis."}}