{"cell_type":{"5252a250":"code","3436032b":"code","781b94e3":"code","2c7dc8f8":"code","59806cd6":"code","92f6f71c":"code","5d2596b5":"code","e4bfb32e":"code","513af09e":"code","071712bf":"code","2ee7e660":"code","d7a56dac":"code","c46747d0":"code","dc461ce4":"code","cd9010fd":"code","b8723082":"code","81775219":"code","6e5cc1c2":"code","6f75f010":"code","29651584":"code","4e6d55d0":"code","6afe7e63":"code","aa5adf74":"code","a8e52357":"code","409c4aea":"code","14bb8dea":"code","f5acc925":"code","4237188c":"markdown","218f2abd":"markdown","ca1c9999":"markdown","349da2df":"markdown","b57aad22":"markdown","b7197440":"markdown","1a751636":"markdown","bc4db782":"markdown","1c941764":"markdown","a9d398ef":"markdown","b7ebe6fe":"markdown","02ae1021":"markdown","d5e5ecba":"markdown","cac1d792":"markdown","6e748af5":"markdown","fed21d80":"markdown","cc6d28d7":"markdown","5c48fbff":"markdown","74a9f88c":"markdown"},"source":{"5252a250":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","3436032b":"from sklearn.datasets import load_files\n\ndef load_train_data():\n    ''' Load data from csv files, Train data \n        Splits data into X (feature matrix) and y (labels)\n        \n        returns: X_train, y_train\n        \n    '''\n    train_data = pd.read_csv('..\/input\/instagram-fake-spammer-genuine-accounts\/train.csv', header = 0)\n    \n    X_train = train_data.drop(columns='fake')\n    y_train = train_data['fake']\n    \n    return X_train, y_train","781b94e3":"from sklearn.datasets import load_files\n\ndef load_test_data():\n    ''' Load data from csv files, Train and Test data \n        Splits data into X (feature matrix) and y (labels)\n        \n        returns: X_test, y_test\n        \n    '''\n    test_data = pd.read_csv('..\/input\/instagram-fake-spammer-genuine-accounts\/test.csv', header = 0)\n    \n    X_test = test_data.drop(columns='fake')\n    y_test = test_data['fake']\n    \n    return X_test, y_test","2c7dc8f8":"from sklearn.model_selection import cross_validate\n\n\ndef get_classifier_cv_score(model, X, y, scoring='accuracy', cv=7):\n    '''Calculate train and validation score of classifier (model) using cross-validation\n        \n        \n        model (sklearn classifier): Classifier to train and evaluate\n        X (numpy.array or pandas.DataFrame): Feature matrix\n        y (numpy.array or pandas.Series): Target vector\n        scoring (str): a scoring string accepted by sklearn.metrics.cross_validate()\n        cv (int): number of cross-validation folds see sklearn.metrics.cross_validate()\n        \n        returns: mean training score, mean validation score\n    \n    '''\n    scores = cross_validate(model, X, y, cv=cv, scoring=scoring, return_train_score=True)\n    train_scores = scores['train_score']\n    val_scores = scores['test_score']\n    \n    train_mean = np.mean(train_scores)\n    val_mean = np.mean(val_scores)\n    \n    return train_mean, val_mean","59806cd6":"def print_grid_search_result(grid_search):\n    '''Prints best parameters and mean training and validation scores of a grid search object.\n    \n        grid_search (sklearn GridSearchCV): Fitted GridSearchCV object\n        \n        scores are printed with 3 decimal places.\n        \n    '''\n    \n     #TODO: implement function body\n    \n    print(grid_search.best_params_)\n    \n    best_train = grid_search.cv_results_[\"mean_train_score\"][grid_search.best_index_]\n    print(\"best mean_train_score: {:.3f}\".format(best_train))\n        \n    best_test = grid_search.cv_results_[\"mean_test_score\"][grid_search.best_index_]\n    print(\"best mean_test_score: {:.3f}\".format(best_test))","92f6f71c":"from sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(y_actual, y_pred, labels, title=''):\n    '''Creates a heatmap plot of the confusion matrix.\n    \n        y_actual (pandas.DataSeries or numpy.Array): Ground truth label vector\n        y_pred (pandas.DataSeries or numpy.Array): Predicted label vector\n        labels (list(str)): Class names used for plotting (ticklabels)\n        title (str): Plot title\n        \n        uses sklearn.metrics.confusion_matrix\n        \n    '''\n    data = confusion_matrix(y_actual, y_pred)\n    ax = sns.heatmap(data,\n                     annot=True,\n                     cbar=False,\n                     fmt='d',\n                     xticklabels = labels,\n                     yticklabels = labels)\n    ax.set_title(title)\n    ax.set_xlabel(\"predicted values\")\n    ax.set_ylabel(\"actual values\")","5d2596b5":"X_data, y_data = load_train_data()\nprint(X_data.info())","e4bfb32e":"X_data.head()","513af09e":"print(\"Size: \",X_data.shape, \", Type: \", type(X_data))\nprint(\"Size: \",y_data.shape, \", Type: \", type(y_data))","071712bf":"data_corr = X_data.corr(method='pearson')\nax = sns.heatmap(data_corr, vmin=-1, vmax=1, cmap='BrBG')\nax.set_title(\"Correlation Heatmap Between Features\")","2ee7e660":"print(X_data.isnull().sum())","d7a56dac":"unique, freq = np.unique(y_data, return_counts = True) \n\nfor i, j in zip(unique, freq):\n    print(\"Label: \", i, \", Frequency: \", j)","c46747d0":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=37)","dc461ce4":"print(X_train.shape)\nprint(y_train.shape)","cd9010fd":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\nmodel_list = [LogisticRegression(max_iter=600),\n              SVC(), \n              GaussianNB(),\n              RandomForestClassifier(random_state=55),\n              GradientBoostingClassifier(random_state=56)]\n\ntrain_scores = []\nval_scores = []\n\nfor model in model_list:\n    train, val = get_classifier_cv_score(model, X_train, y_train,'average_precision')\n    train_scores.append(train)\n    val_scores.append(val)\n    \nmodels_score = sorted(list(zip(val_scores, train_scores, model_list)), reverse=True)\n\nprint(\"-------------------------------------\")\nfor val, train, model in models_score:\n    print(\"Model: {} \".format(model.__class__.__name__))\n\n    print(\"train_score: {:.3f}\".format(train)) \n\n    print(\"validation_score: {:.3f}\".format(val)) \n\n    print(\"-------------------------------------\")","b8723082":"from sklearn.model_selection import GridSearchCV\nimport os\n#if run on own computer\n#num_cpu = int(os.environ['NUMBER_OF_PROCESSORS'])\n\nmodel = RandomForestClassifier(random_state=55)\n\nparameters = {'n_estimators': [300, 500, 700, 1000],\n              'max_depth': [7, 9, 11, 13]}\n\n#if run on own computer\n#grid1 = GridSearchCV(model, parameters, cv=7, scoring='average_precision', n_jobs=num_cpu, return_train_score=True)\ngrid1 = GridSearchCV(model, parameters, cv=7, scoring='average_precision',return_train_score=True)","81775219":"grid1.fit(X_train, y_train)","6e5cc1c2":"print_grid_search_result(grid1)","6f75f010":"model = GradientBoostingClassifier(max_depth=5, random_state=56)\n\nparameters = {'n_estimators': [50, 100, 200],\n              'learning_rate': [0.001, 0.01, 0.1, 1.0, 10.0]}\n\n#if run on own computer\n#grid2 = GridSearchCV(model, parameters, cv=7, scoring='average_precision', n_jobs=num_cpu, return_train_score=True)\ngrid2 = GridSearchCV(model, parameters, cv=7, scoring='average_precision', return_train_score=True)","29651584":"grid2.fit(X_train, y_train)","4e6d55d0":"print_grid_search_result(grid2)","6afe7e63":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\npipeline = Pipeline([('preprocessing', StandardScaler()), ('classifier', grid1.best_estimator_)])\npipeline.fit(X_train, y_train)","aa5adf74":"print(\"Test score: {:.3f}\".format(pipeline.score(X_test, y_test)))","a8e52357":"X_final, y_final = load_test_data()","409c4aea":"print(\"Test score: {:.3f}\".format(pipeline.score(X_final, y_final)))","14bb8dea":"from sklearn.metrics import classification_report\ny_pred = pipeline.predict(X_final)\nprint(classification_report(y_final, y_pred, target_names=[\"genuine\", \"fake\"]))","f5acc925":"labels = [\"genuine\", \"fake\"]\ntitle = \"Predicting Fake Instagram Account\"\nplot_confusion_matrix(y_final, y_pred, labels, title)","4237188c":"## 2. Inspect Data","218f2abd":"### 2.3 Checking if Imbalance in Labels\n- The labels is about 1:1 which means there is no imbalance in the labels. \n- If there was, the ratio would be more 2:1.","ca1c9999":"## 7. Final Evaluation","349da2df":"- Now we will use the best model from our grid search, which is Random Forest Classifier\n- We will apply a pipeline since we need to standardize our data","b57aad22":"# Predicting Whether an Instagram Account is Fake or Genuine\n\nAuthor: *Khoi Ngo*","b7197440":"### 2.2 Missing any Values\n- We can see that the features are all filled out and we do not need to modify the existing data","1a751636":"## 4. Compare Models Using Cross-Validation","bc4db782":"## 3. Create training and test sets","1c941764":"## Overview of Steps to Accomplish Notebook","a9d398ef":"- We can see our model predicted around 91.5% fake accounts and 90.2% genuine accounts correctly\n- The model only predicted 11 accounts wrong","b7ebe6fe":"- Finally we have a working model to do a final evaluation on a reserve data","02ae1021":"### 5.1 Grid Search for RandomForestClassifier","d5e5ecba":"## 1. Load Data","cac1d792":"### 5.2 Grid Search for Gradient Boosting Classifier","6e748af5":"## 5. Hyperparameter Tuning Using Grid Search","fed21d80":"## 0. Function Definitions","cc6d28d7":"- We will select the two best models to continue.\n- These models are Random Forest Classifier and Gradient Boosting Classifier.","5c48fbff":"## 6. Pipeline","74a9f88c":"### 2.1 Seeing Any Correlation in Features\n- We can see that there is no correlation among the features\n- They are roughly around 0 in each feature comparison."}}