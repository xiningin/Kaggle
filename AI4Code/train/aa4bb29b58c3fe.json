{"cell_type":{"1349e7f1":"code","f438a938":"code","41da5186":"code","0d1e1d63":"code","0d5152c0":"code","217393df":"code","214edd2b":"code","83827463":"code","a8780e1c":"code","93ba7fce":"code","db3b8f40":"code","fe09baaa":"code","a904b43f":"code","3bf4a871":"code","8f93858a":"code","8f1476b4":"code","2bb51826":"code","617fd903":"code","a0a38aac":"code","19ab559d":"code","9d9c4b09":"code","c7764cd2":"code","5168ce25":"code","16843903":"code","53e50bde":"code","c1cf09dc":"code","a31c691e":"code","8763e2ce":"code","4486a97a":"code","560c45c0":"code","eb5718c3":"code","b52ec6e9":"code","2f6b9912":"code","ea060a83":"code","0d6dc1cf":"markdown","f6f10238":"markdown","3716daf1":"markdown","cea1a4f2":"markdown","4820e0e6":"markdown","ca48eced":"markdown","2582f801":"markdown"},"source":{"1349e7f1":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime \nimport category_encoders as ce\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm import tqdm_notebook as tqdm\nimport random\nrandom.seed(20)\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\nfrom statistics import mean, median,variance,stdev\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier","f438a938":"#df = pd.read_csv('..\/input\/train.csv')\n#df = df.sample(n=10000, random_state=20)\n#df_train, df_test = train_test_split(df, test_size=0.2, random_state=20)","41da5186":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')","0d1e1d63":"df_gdp = pd.read_csv(\"..\/input\/US_GDP_by_State.csv\")\ndf_state = pd.read_csv(\"..\/input\/statelatlong.csv\")\ndf_spi = pd.read_csv(\"..\/input\/spi.csv\")","0d5152c0":"year = []\nfor y in list(df_train[\"issue_d\"]):\n    year.append(int(y[4:8]))\ndf_train[\"year\"] = year\n\nyear_test = []\nfor y in list(df_test[\"issue_d\"]):\n    year_test.append(int(y[4:8]))\ndf_test[\"year\"] = year_test","217393df":"years = [2007, 2008, 2009, 2010, 2011, 2012]\nadd_list = []\nfor y in years:\n    for state in set(df_gdp[\"State\"]):\n        add = []\n        add.append(state)\n        add.append(int(df_gdp[\"State & Local Spending\"][df_gdp.year == 2013][df_gdp.State == state]))\n        add.append(float(df_gdp[\"Gross State Product\"][df_gdp.year == 2013][df_gdp.State == state]))\n        add.append(float(df_gdp[\"Real State Growth %\"][df_gdp.year == 2013][df_gdp.State == state]))\n        add.append(float(df_gdp[\"Population (million)\"][df_gdp.year == 2013][df_gdp.State == state]))\n        add.append(y)\n        add_list.append(add)\n        df_gdp.append(pd.DataFrame(add_list, columns=[\"State\", \"State & Local Spending\", \"Gross State Product\", \"Real State Growth %\", \"Population (million)\", \"year\"]))\n\ndf_gdp = pd.merge(df_gdp, df_state, left_on = \"State\", right_on = \"City\", how='left')\n\ndate_spi = []\nfor d in list(df_spi['date']):\n    date_temp = datetime.strptime(d, '%d-%b-%y')\n    date_spi.append(date_temp.strftime('%b-%Y'))\ndf_spi[\"date\"] = date_spi\ndf_spi = df_spi.groupby(\"date\").mean()\n\ndf_train = pd.merge(df_train, df_gdp, left_on=['addr_state','year'], right_on = ['State_y','year'], how ='left')\ndf_test = pd.merge(df_test, df_gdp, left_on=['addr_state','year'], right_on = ['State_y','year'], how ='left')","214edd2b":"df_train = pd.merge(df_train, df_spi, left_on='issue_d', right_on ='date', how = 'left')\ndf_test = pd.merge(df_test, df_spi, left_on='issue_d', right_on ='date', how = 'left')","83827463":"#y_train = df_train.loan_condition\n#y_test = df_test.loan_condition\n#df_train = df_train.drop([\"loan_condition\", \"issue_d\"], axis=1)\n#df_test = df_test.drop([\"issue_d\"], axis=1)","a8780e1c":"columns_raw = df_train.columns\ncolumns_raw_test = df_test.columns","93ba7fce":"df_train['fold'] = df_train.loan_amnt.apply( lambda x: random.randint(0,5))","db3b8f40":"columns = df_train.columns\ncolumns_hasnull = []\nfor col in columns:\n    if df_train[col].isnull().sum() > 0:\n        columns_hasnull.append(col)","fe09baaa":"def MissingColumns(train, test, columns):\n    trainout = train.copy()\n    testout = test.copy()\n    for col in columns:\n        name = col + \"_Miss\"\n        trainout[name] = list(train[col].isnull())\n        trainout = trainout.drop(col,axis=1)\n        testout[name] = list(testout[col].isnull())\n        testout = testout.drop(col, axis=1)\n    return trainout, testout","a904b43f":"col_miss = [\"emp_length\", \"title\", \"emp_title\"]\ntrain_Miss, test_Miss = MissingColumns(df_train[col_miss], df_test[col_miss],col_miss)","3bf4a871":"for i in columns_hasnull:\n    if df_train[i].dtype == \"float64\":\n        df_train[i].fillna(df_train[i].median(), inplace=True)\n        df_test[i].fillna(df_train[i].median(), inplace=True)\n    if df_train[i].dtype == \"object\":\n        df_train[i].fillna(\"Kuhaku\", inplace=True)\n        df_test[i].fillna(\"Kuhaku\", inplace=True)","8f93858a":"df_train = df_train.join(train_Miss)\ndf_test = df_test.join(test_Miss)","8f1476b4":"df_train[\"loan_sal_ratio\"] = df_train[\"loan_amnt\"]\/df_train[\"annual_inc\"]\ndf_test[\"loan_sal_ratio\"] = df_test[\"loan_amnt\"]\/df_test[\"annual_inc\"]\ndf_train[\"loan_sal_ratio\"] = df_train[\"loan_sal_ratio\"].replace(np.inf, 1)\ndf_test[\"loan_sal_ratio\"] = df_test[\"loan_sal_ratio\"].replace(np.inf, 1)","2bb51826":"def OrdinalEncoder(traindf, testdf, category):\n    oe = ce.OrdinalEncoder(cols=category, return_df=True)\n    names = list(map(lambda x: x + \"_OrdinalEncoder\", category))\n    trainout = oe.fit_transform(traindf[category])\n    testout = oe.transform(testdf[category])\n    trainout.columns = names\n    testout.columns = names\n    for name in names:\n        trainout[name].fillna(trainout[name].median(), inplace=True)\n        testout[name].fillna(trainout[name].median(), inplace=True)\n    return trainout, testout\n\ndef OneHotEncoder(traindf, testdf, category):\n    ohe = ce.OneHotEncoder(cols=category, handle_unknown='impute')\n    trainout = ohe.fit_transform(traindf[category])\n    testout = ohe.transform(testdf[category])\n    return trainout, testout\n\ndef CountEncoder(traindf, testdf, category):\n    trainout = pd.DataFrame()\n    testout = pd.DataFrame()\n    for val in category:\n        newname = val + \"_CountEncoder\"\n        count = traindf.groupby(val)[val].count()\n        trainout[newname] = traindf.groupby(val)[val].transform('count')\n        testout[newname] = traindf.groupby(val)[val].transform('count')\n        trainout[newname].fillna(trainout[newname].median(), inplace=True)\n        testout[newname].fillna(trainout[newname].median(), inplace=True)\n    return trainout, testout\n\ndef TargetEncoder(traindf, testdf, category, target):\n    trainout = pd.DataFrame()\n    testout = pd.DataFrame()\n    for col in category:\n        trainout_temp = pd.DataFrame()\n        testout_temp = pd.DataFrame()\n        n= 0\n        name = col + \"_TargetEncoder\"\n        # training\n        for i in set(traindf[\"fold\"]):\n            label_mean = traindf[traindf[\"fold\"] != i].groupby(col)[target].mean()\n            if n == 0:\n                trainout_temp = traindf[traindf[\"fold\"] == i][col].map(label_mean)\n            else:\n                trainout_temp = trainout_temp.append(traindf[traindf[\"fold\"] == i][col].map(label_mean))\n            n = n + 1\n        trainout[name] = trainout_temp\n        trainout[name].fillna(trainout[name].median(), inplace=True)\n        \n        # test\n        label_mean = traindf.groupby(col)[target].mean()\n        testout[name] = testdf[col].map(label_mean)\n        testout[name].fillna(trainout[name].median(), inplace=True)\n    return trainout, testout","617fd903":"col_CE = [\n#\"earliest_cr_line\",\n]\ntrain_CE, test_CE = CountEncoder(df_train, df_test, col_CE)","a0a38aac":"col_OE = [\n#    \"initial_list_status\",\n#    \"zip_code\",\n#    \"earliest_cr_line\",\n]\ntrain_OE, test_OE = OrdinalEncoder(df_train, df_test, col_OE)","19ab559d":"col_TE = [\n        \"initial_list_status\",\n    \"zip_code\",\n    \"earliest_cr_line\",\n\"title\",\n\"grade\",\n\"sub_grade\",\n\"emp_length\",\n\"home_ownership\",\n\"purpose\",\n\"addr_state\"\n]\ntrain_TE, test_TE = TargetEncoder(df_train, df_test, col_TE, \"loan_condition\")","9d9c4b09":"for i in list(df_train.columns):\n    if df_train[i].dtype!=\"object\":\n        print(i)","c7764cd2":"def StandardScale(train, test, columns):\n    scaler = StandardScaler()\n    scaler.fit(train[columns])\n    trainout = train[columns].copy()\n    testout = test[columns].copy()\n    trainout[columns] = scaler.transform(train[columns])\n    testout[columns] = scaler.transform(test[columns])\n    names = list(map(lambda x: x + \"_StandardScale\", columns))\n    trainout.columns = names\n    testout.columns = names\n    for name in names:\n        trainout[name].fillna(trainout[name].median(), inplace=True)\n        testout[name].fillna(trainout[name].median(), inplace=True)\n    return trainout, testout\n\ndef MinMaxScale(train, test, columns):\n    scaler = MinMaxScaler()\n    scaler.fit(train[columns])\n    trainout = train[columns].copy()\n    testout = test[columns].copy()\n    trainout[columns] = scaler.transform(trainout[columns])\n    testout[columns] = scaler.transform(testout[columns])\n    names = list(map(lambda x: x + \"_MinMaxScale\", columns))\n    trainout.columns = names\n    testout.columns = names\n    for name in names:\n        trainout[name].fillna(trainout[name].median(), inplace=True)\n        testout[name].fillna(trainout[name].median(), inplace=True)\n    return trainout, testout\n\ndef LogTransform(train, test, columns):\n    trainout = train[columns].copy()\n    testout = test[columns].copy()\n    for col in columns:\n        logval = []\n        for i in list(train[col]):\n            tes = np.sign(i)*np.log(abs(i))\n            logval.append(tes)\n        trainout[col] = logval\n        \n        logval_test = []\n        for i in list(test[col]):\n            tes = np.sign(i)*np.log(abs(i))\n            logval_test.append(tes)\n        testout[col] = logval_test\n    names = list(map(lambda x: x + \"_Log\", columns))\n    trainout.columns = names\n    testout.columns = names\n    for name in names:\n        trainout[name].fillna(trainout[name].median(), inplace=True)\n        testout[name].fillna(trainout[name].median(), inplace=True)\n    return trainout, testout\n\ndef BoxCoxTransform(train, test, columns):\n    pt = PowerTransformer(method='yeo-johnson')\n    pt.fit(train[columns])\n    trainout = train[columns].copy()\n    testout = test[columns].copy()\n    trainout[columns] = pt.transform(train[columns])\n    testout[columns] = pt.transform(test[columns])\n    names = list(map(lambda x: x + \"_BoxCox\", columns))\n    trainout.columns = names\n    testout.columns = names\n    for name in names:\n        trainout[name].fillna(trainout[name].median(), inplace=True)\n        testout[name].fillna(trainout[name].median(), inplace=True)\n    return trainout, testout","5168ce25":"col_SS = [\n    \"loan_amnt\",\n    \"installment\",\n    \"dti\",\n    \"delinq_2yrs\",\n    \"inq_last_6mths\",\n    \"pub_rec\",\n    \"revol_util\",\n    \"State & Local Spending\",\n    \"Gross State Product\",\n    \"Population (million)\",\n    \"close\",\n    \"tot_coll_amt\",\n    \"loan_sal_ratio\",\n    \"mths_since_last_major_derog\",\n    \n]\ntrain_SS, test_SS = StandardScale(df_train, df_test, col_SS)","16843903":"col_log = [\n    \"annual_inc\",\n    \"open_acc\",\n    \"revol_bal\",\n    \"total_acc\",\n    \"tot_cur_bal\",\n    \"Real State Growth %\"\n]\ntrain_log, test_log = LogTransform(df_train, df_test, col_log)","53e50bde":"col_MinMax = [\n    \"mths_since_last_record\"\n]\ntrain_MinMax, test_MinMax = MinMaxScale(df_train, df_test, col_MinMax)","c1cf09dc":"col_BoxCox = [\n \"mths_since_last_delinq\"\n]\ntrain_BoxCox, test_BoxCox = BoxCoxTransform(df_train, df_test, col_BoxCox)","a31c691e":"col_text = [\n    \"emp_title\"\n]","8763e2ce":"def TFIDF(trainlist, testlist):\n    features = 30\n    vec = TfidfVectorizer(min_df = 1, max_features = features)\n    alltxt = trainlist + testlist\n    vec.fit(alltxt)\n    trainout = pd.DataFrame(vec.transform(trainlist).toarray())\n    testout = pd.DataFrame(vec.transform(testlist).toarray())\n    names = []\n    for f in range(features):\n        txt = \"txt_\" + str(f)\n        names.append(txt)\n    trainout.columns = names\n    testout.columns = names\n    return trainout, testout","4486a97a":"train_TXT, test_TXT = TFIDF(list(df_train[\"emp_title\"]), list(df_test[\"emp_title\"]))","560c45c0":"df_train.columns","eb5718c3":"df_train = df_train.join([\n    train_CE,\n    train_OE,\n    train_TE,\n    train_SS,\n    train_log,\n    train_MinMax,\n    train_BoxCox,\n    train_TXT\n])\ndf_test = df_test.join([\n    test_CE,\n    test_OE,\n    test_TE,\n    test_SS,\n    test_log,\n    test_MinMax,\n    test_BoxCox,\n    test_TXT\n])","b52ec6e9":"X_train = df_train.drop(columns_raw, axis = 1).drop(\"fold\", axis = 1)\ny_train = df_train.loan_condition\nX_test = df_test.drop(columns_raw_test, axis = 1)","2f6b9912":"clf = LGBMClassifier(boosting_type='gbdt', \n                         class_weight=None,\n                         colsample_bytree=0.71,\n                        importance_type=\"split\",\n                        learning_rate=0.05,\n                        max_depth=-1,\n                        min_child_samples=20,\n                        min_child_weight=0.001,\n                        min_split_gain=0.0,\n                        n_estimators=9999,\n                        n_jobs=-1,\n                        num_leaves=31,\n                        objective=None,\n                        random_state=71, \n                         reg_alpha=1.0,\n                        reg_lambda=1.0,\n                        silent=True,\n                        subsample=0.9, subsample_for_bin=200000,\n                        subsample_freq=0)\nX_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train, random_state=42)\nclf.fit(X_train_, y_train_, early_stopping_rounds=200,eval_metric='auc',eval_set=[(X_val,y_val)])","ea060a83":"y_pred = clf.predict_proba(X_test)[:,1]\nsubmission = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsubmission.loan_condition = y_pred\nsubmission.to_csv(\"submission.csv\", index=False)","0d6dc1cf":"# \u6570\u5024\u51e6\u7406","f6f10238":"# \u8ffd\u52a0\u7279\u5fb4\u91cf\u751f\u6210","3716daf1":"# \u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f","cea1a4f2":"# \u30c6\u30ad\u30b9\u30c8","4820e0e6":"# \u30ab\u30c6\u30b4\u30ea\u30fc\u51e6\u7406","ca48eced":"# NULL\u51e6\u7406","2582f801":"# \u30c7\u30fc\u30bf\u6e96\u5099"}}