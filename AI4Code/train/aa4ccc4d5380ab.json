{"cell_type":{"4aaf93be":"code","404a026d":"code","3c76a3f3":"code","239b283f":"code","c09820b7":"code","6941ac8e":"code","d1d95d90":"code","9dd54762":"code","f151dd9e":"code","df27f1e7":"code","bd692793":"code","60d5e967":"code","ecdb0af0":"code","f977a930":"code","5da23f58":"code","d9a3927c":"code","5020cee2":"code","e7a7887b":"code","41c58c71":"code","f48b3a67":"code","9530edd9":"code","fd64a39a":"code","f6a36fa1":"code","29619bfc":"code","071a2e90":"code","1a93cb8d":"code","74509d16":"markdown","0d450b60":"markdown","44b720f2":"markdown","1587891e":"markdown"},"source":{"4aaf93be":"from mlxtend.plotting import plot_decision_regions\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","404a026d":"#Loading the dataset\ndiabetes = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\n\n#Print the first 10 rows of the dataframe.\ndiabetes.head(5)","3c76a3f3":"# gives information about the data types,columns, null value counts, memory usage for all the features\ndiabetes.info(verbose=True)","239b283f":"diabetes.describe()","c09820b7":"diabetes.describe().T","6941ac8e":"diabetes_copy = diabetes.copy(deep = True)\ndiabetes_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = diabetes_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)\n# showing the count of Nans\nprint(diabetes_copy.isnull().sum())","d1d95d90":"a = diabetes.hist(figsize = (20,20))","9dd54762":"diabetes_copy['Glucose'].fillna(diabetes_copy['Glucose'].mean(), inplace = True)\ndiabetes_copy['BloodPressure'].fillna(diabetes_copy['BloodPressure'].mean(), inplace = True)\ndiabetes_copy['SkinThickness'].fillna(diabetes_copy['SkinThickness'].median(), inplace = True)\ndiabetes_copy['Insulin'].fillna(diabetes_copy['Insulin'].median(), inplace = True)\ndiabetes_copy['BMI'].fillna(diabetes_copy['BMI'].median(), inplace = True)","f151dd9e":"p = diabetes_copy.hist(figsize = (20,20))","df27f1e7":"## observing the shape of the data\ndiabetes.shape","bd692793":"diabetes.dtypes.value_counts()\nprint(diabetes.dtypes)","60d5e967":"## null count analysis\nimport missingno as msno\na=msno.bar(diabetes)","ecdb0af0":"## checking the balance of the data by plotting the count of outcomes by their value\ncolor_wheel = {1: \"#0392cf\", \n               2: \"#7bc043\"}\ncolors = diabetes[\"Outcome\"].map(lambda x: color_wheel.get(x + 1))\nprint(diabetes.Outcome.value_counts())\na=diabetes.Outcome.value_counts().plot(kind=\"bar\")","f977a930":"from pandas.plotting import scatter_matrix\np=scatter_matrix(diabetes,figsize=(25, 25))","5da23f58":"a=sns.pairplot(diabetes_copy, hue = 'Outcome')","d9a3927c":"plt.figure(figsize=(12,10))  # on this line I just set the size of figure to 12 by 10.\np=sns.heatmap(diabetes.corr(), annot=True,cmap ='RdYlGn')  # seaborn has very simple solution for heatmap","5020cee2":"plt.figure(figsize=(12,10))  # on this line I just set the size of figure to 12 by 10.\np=sns.heatmap(diabetes_copy.corr(), annot=True,cmap ='RdYlGn')  # seaborn has very simple solution for heatmap","e7a7887b":"from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX =  pd.DataFrame(sc_X.fit_transform(diabetes_copy.drop([\"Outcome\"],axis = 1),),\n        columns=['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age'])\nX.head(5)","41c58c71":"#X = diabetes.drop(\"Outcome\",axis = 1)\ny = diabetes_copy.Outcome\ny.head(5)","f48b3a67":"#importing train_test_split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=1\/3,random_state=42, stratify=y)","9530edd9":"from sklearn.neighbors import KNeighborsClassifier\n\ntest_scores = []\ntrain_scores = []\n\nfor i in range(1,15):\n\n    knn = KNeighborsClassifier(i)\n    knn.fit(X_train,y_train)\n    \n    train_scores.append(knn.score(X_train,y_train))\n    test_scores.append(knn.score(X_test,y_test))","fd64a39a":"## score that comes from testing on the same datapoints that were used for training\nmax_train_score = max(train_scores)\ntrain_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]\nprint('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind))))","f6a36fa1":"## score that comes from testing on the datapoints that were split in the beginning to be used for testing solely\nmax_test_score = max(test_scores)\ntest_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]\nprint('Max test score {} % and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))","29619bfc":"plt.figure(figsize=(12,5))\np = sns.lineplot(range(1,15),train_scores,marker='*',label='Train Score')\np = sns.lineplot(range(1,15),test_scores,marker='o',label='Test Score')","071a2e90":"#Setup a knn classifier with k neighbors\nknn = KNeighborsClassifier(11)\n\nknn.fit(X_train,y_train)\nknn.score(X_test,y_test)","1a93cb8d":"value = 20000\nwidth = 20000\nplot_decision_regions(X.values, y.values, clf=knn, legend=2, \n                      filler_feature_values={2: value, 3: value, 4: value, 5: value, 6: value, 7: value},\n                      filler_feature_ranges={2: width, 3: width, 4: width, 5: width, 6: width, 7: width},\n                      X_highlight=X_test.values)\n\nplt.title('KNN with Diabetes Data')\nplt.show()","74509d16":"**DataFrame.describe(**) method generates descriptive statistics that summarize the central tendency, dispersion and shape of a dataset\u2019s distribution, excluding NaN values. This method tells us a lot of things about a dataset. One important thing is that the describe() method deals only with numeric values. It doesn't work with any categorical values. So if there are any categorical values in a column the describe() method will ignore it and display summary for the other columns unless parameter include=\"all\" is passed.\n\nNow, let's understand the statistics that are generated by the describe() method:\n\nCount tells us the number of NoN-empty rows in a feature.   \nMean tells us the mean value of that feature.   \nStd tells us the Standard Deviation Value of that feature. \nMin tells us the minimum value of that feature. \n25%, 50%, and 75% are the percentile\/quartile of each features. This quartile information helps us to detect Outliers. \nMax tells us the maximum value of that feature. ","0d450b60":"# Basic Data Science and ML Pipeline","44b720f2":"# Import Libraries ","1587891e":"# EDA and statistical analysis"}}