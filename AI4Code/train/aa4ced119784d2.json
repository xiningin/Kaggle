{"cell_type":{"30d053af":"code","229ef78b":"code","d1f81b39":"code","4cc134e6":"code","ea36e833":"code","af63e5b0":"code","c49c8ab5":"code","896205ae":"code","c54dab20":"code","4823fe32":"code","f81b70e6":"code","cea681cb":"code","76c00353":"code","11ef44e7":"code","7fe4ce74":"code","826777ab":"code","3ecee22f":"code","f56d0e2b":"code","4e399c39":"code","b759e179":"code","f3ad87b9":"code","9c9b6c7c":"code","5fe8dbee":"code","350c2379":"code","794b2846":"code","17448e68":"code","6040d197":"code","ca52beed":"code","901c08c0":"code","f2a6ba0f":"code","9433e502":"code","704b2a42":"code","66613d24":"code","7773857e":"code","57d8b81d":"code","0dc0db9c":"code","7cfeeeba":"code","18c88645":"code","77cd81a6":"code","ea645dc7":"code","b9b3c8fb":"code","cb101e49":"code","11c6a511":"code","afc8b1c9":"code","502922af":"code","a5297392":"code","8ca78a2b":"code","197359e2":"code","b8d00fae":"code","8c485dfa":"code","08b34ab7":"code","beb804e8":"code","2b399d5c":"code","266b560e":"code","4ac2c2ad":"code","b8d45047":"code","a2faf4d0":"code","11f901e5":"code","f7164590":"code","2c165f42":"code","e160fabb":"code","7ad6f0fb":"code","8f01b7b0":"code","7243145d":"code","dd4b76d0":"code","5f2d2cc2":"code","c5ce8844":"code","f205406c":"code","2b954082":"code","ec583498":"code","97007c8d":"code","7d417fec":"code","127331e2":"code","c5593824":"code","26f8d155":"code","9d410e7f":"code","81e2977d":"code","27cdfbad":"code","b4229ebd":"code","7efc30ed":"code","8c4fb619":"code","53aefecb":"code","86e1946e":"code","a76c2cd7":"code","89fb8a83":"code","f6ffc3a9":"code","e1808088":"code","fcacbdc9":"code","3e9a630d":"code","566ffa17":"code","1deb91ad":"code","df75ad30":"code","6e1e5ab7":"code","e80265d9":"code","d1c4500e":"code","3ef637a0":"code","d7c19c9c":"code","f1c56fc3":"code","7e7419ab":"code","05f002f3":"code","a8195c31":"code","bfd52099":"code","4ba9c714":"code","dfd974ff":"code","1ad328a8":"code","75ef43d5":"code","045bc2f5":"code","df8d3b21":"code","da92f933":"code","81981e5e":"code","37a68472":"code","42715d97":"code","447a0fa6":"code","e5299c7a":"code","8817d2b9":"code","faa55f23":"code","1fb8f8f8":"code","ffe4a27b":"code","3219f326":"code","1ef15f51":"code","02309cf8":"code","32159c2a":"code","9ef4f78e":"code","a696e745":"code","2a9f4051":"code","63d6467f":"code","8678d728":"code","2b2abca8":"code","beef7ba9":"markdown","17d33393":"markdown","f48cecbb":"markdown","c61293f4":"markdown","fed3f426":"markdown","d980e2d6":"markdown","28487cde":"markdown","cabeeeb8":"markdown","51052016":"markdown","a4761287":"markdown","cb06e667":"markdown","a01775d7":"markdown","d01bb002":"markdown","f0bc1993":"markdown","d6bdc3cf":"markdown","c5964c93":"markdown","22862c63":"markdown","b2c2597a":"markdown","1df8e00b":"markdown","4264fc59":"markdown","46884a84":"markdown","25d2aa7d":"markdown","007df119":"markdown","d170488e":"markdown","c85a595d":"markdown","764db538":"markdown","88e9db85":"markdown","62867320":"markdown","039d9589":"markdown","82ec252a":"markdown","e6c3962f":"markdown","15fcce63":"markdown","79664e08":"markdown","1b39cf3b":"markdown","adb059f1":"markdown","5561c72f":"markdown","9b5f3623":"markdown","9f8228e0":"markdown","cf101a13":"markdown","0736ca26":"markdown","c4be9fac":"markdown","f32f1e10":"markdown","e731bae1":"markdown","a3bf57d4":"markdown","b5b722b4":"markdown","625c1ce7":"markdown","c9f2bbd7":"markdown","183b86cc":"markdown","5d023b27":"markdown","f26932dd":"markdown","e8299d13":"markdown","1d16e6a1":"markdown","ddd05fc3":"markdown","c4c00e45":"markdown","e5370ed1":"markdown","abc69f7a":"markdown","75f3faa7":"markdown","c480c16e":"markdown","8236b6b2":"markdown","eabc0a98":"markdown","440b07f8":"markdown","fe097b43":"markdown","d725e0aa":"markdown","370533eb":"markdown","200248de":"markdown","6a79bc67":"markdown","c936c7da":"markdown","de5016b9":"markdown","e1ee0ca1":"markdown","5ee8e06e":"markdown","35043646":"markdown","fd954567":"markdown","2f424483":"markdown","433c4a73":"markdown","441e0d0b":"markdown","591aa9d6":"markdown","078669cf":"markdown","845ad0a8":"markdown","4aa65e15":"markdown","a7aa1049":"markdown","f06cba5c":"markdown","bcd8ab55":"markdown","555c1680":"markdown","df244ec8":"markdown","166971e4":"markdown","043c8a01":"markdown","dba9f861":"markdown","8dca21ad":"markdown","a3288c7c":"markdown","e5d29deb":"markdown","c7460877":"markdown","fe23492c":"markdown","ea6a56f1":"markdown","33ef46d4":"markdown","23843655":"markdown"},"source":{"30d053af":"# Bruno Viera Ribeiro - 09\/2020","229ef78b":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d1f81b39":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px","4cc134e6":"pd.set_option('display.max_colwidth', None)\ndesc_df = pd.read_csv('..\/input\/covid19-healthy-diet-dataset\/Supply_Food_Data_Descriptions.csv', index_col = 'Categories')\ndesc_df","ea36e833":"kg_df_full = pd.read_csv('..\/input\/covid19-healthy-diet-dataset\/Food_Supply_Quantity_kg_Data.csv')\nkg_df_full.head()","af63e5b0":"kg_df_full.columns","c49c8ab5":"kg_df_full.columns.size","896205ae":"# Let's drop the last column as it is just a unit information\nkg_df = kg_df_full.drop('Unit (all except Population)', axis = 1)\nkg_df.head()","c54dab20":"kg_df.isnull().sum()","4823fe32":"kg_df.head()","f81b70e6":"kg_df = kg_df.dropna()","cea681cb":"kg_df.info()","76c00353":"kg_df['Undernourished'][:20]","11ef44e7":"kg_df['Undernourished'][0]","7fe4ce74":"kg_df.loc[kg_df['Undernourished'] == '<2.5', 'Undernourished'] = '2.0'","826777ab":"kg_df['Undernourished'][:20]","3ecee22f":"kg_df['Undernourished'] = pd.to_numeric(kg_df['Undernourished'])","f56d0e2b":"kg_df.info()","4e399c39":"fig = px.scatter(kg_df, x=\"Confirmed\", y = \"Deaths\",size = \"Active\", hover_name='Country', log_x=False,\n                 size_max=30, trendline = \"ols\", marginal_x = \"box\",marginal_y = \"violin\", template=\"simple_white\")\nfig.show()","b759e179":"kg_df.columns","f3ad87b9":"kg_df['Animal Products'] + kg_df['Vegetal Products']","9c9b6c7c":"(kg_df['Animal Products'] + kg_df['Vegetal Products']).mean()","5fe8dbee":"kg_df.iloc[:, 1:24].sum(axis=1)","350c2379":"kg_df.iloc[:,1:24] = kg_df.iloc[:, 1:24] * 2","794b2846":"(kg_df['Animal Products'] + kg_df['Vegetal Products']).round(1)","17448e68":"(kg_df['Animal Products'] + kg_df['Vegetal Products']).mean()","6040d197":"(kg_df['Confirmed'] - (kg_df['Deaths'] + kg_df['Recovered'] + kg_df['Active'])).round(2)","ca52beed":"kg_df['Mortality'] = kg_df['Deaths']\/kg_df['Confirmed']","901c08c0":"kg_df['Mortality']","f2a6ba0f":"# Distributions\nfig = px.bar(kg_df, x = \"Country\", y =\"Confirmed\").update_xaxes(categoryorder=\"total descending\")\nfig.show()","9433e502":"# Distributions\nfig = px.bar(kg_df, x = \"Country\", y =\"Deaths\").update_xaxes(categoryorder=\"total descending\")\nfig.show()","704b2a42":"# Distributions\nfig = px.bar(kg_df, x = \"Country\", y =\"Active\").update_xaxes(categoryorder=\"total descending\")\nfig.show()","66613d24":"# Distributions\nfig = px.bar(kg_df, x = \"Country\", y =\"Mortality\").update_xaxes(categoryorder=\"total descending\")\nfig.show()","7773857e":"kg_df[kg_df.Country == 'Yemen']['Deaths']","57d8b81d":"fig = px.scatter(kg_df[kg_df.Country != 'Yemen'], x=\"Mortality\", y = \"Obesity\", size = \"Active\", hover_name='Country', log_x=False,\n                 size_max=30, template=\"simple_white\")\n\nfig.add_shape(\n        # Line Horizontal\n            type=\"line\",\n            x0=0,\n            y0=kg_df[kg_df.Country != 'Yemen']['Obesity'].mean(),\n            x1=kg_df[kg_df.Country != 'Yemen']['Mortality'].max(),\n            y1=kg_df[kg_df.Country != 'Yemen']['Obesity'].mean(),\n            line=dict(\n                color=\"crimson\",\n                width=4\n            ),\n    )\n\n\nfig.show()","0dc0db9c":"fig = px.scatter(kg_df, x=\"Mortality\", y = \"Obesity\", size = \"Active\", hover_name='Country', log_x=False,\n                 size_max=30, template=\"simple_white\")\n\nfig.add_shape(\n        # Line Horizontal\n            type=\"line\",\n            x0=0,\n            y0=kg_df['Obesity'].mean(),\n            x1=kg_df['Mortality'].max(),\n            y1=kg_df['Obesity'].mean(),\n            line=dict(\n                color=\"crimson\",\n                width=4\n            ),\n    )\n\n\nfig.show()","7cfeeeba":"fig = px.scatter(kg_df, x=\"Deaths\", y = \"Obesity\", size = \"Mortality\",\n                 hover_name='Country', log_x=False, size_max=30, template=\"simple_white\")\n\nfig.add_shape(\n        # Line Horizontal\n            type=\"line\",\n            x0=0,\n            y0=kg_df['Obesity'].mean(),\n            x1=kg_df['Deaths'].max(),\n            y1=kg_df['Obesity'].mean(),\n            line=dict(\n                color=\"crimson\",\n                width=4\n            ),\n    )\n\nfig.show()","18c88645":"kg_df[kg_df.Obesity < kg_df['Obesity'].mean()].shape","77cd81a6":"kg_df[kg_df.Obesity > kg_df['Obesity'].mean()].shape","ea645dc7":"df_high_ob = kg_df[kg_df.Obesity > kg_df['Obesity'].mean()]\ndf_low_ob = kg_df[kg_df.Obesity <= kg_df['Obesity'].mean()]","b9b3c8fb":"kg_df['ObesityAboveAvg'] = (kg_df[\"Obesity\"] > kg_df['Obesity'].mean()).astype(int)","cb101e49":"fig = px.histogram(kg_df, x = \"Animal Products\", nbins=50, color = \"ObesityAboveAvg\", marginal=\"rug\")\n\nfig.add_shape(\n        # Mean value of Animal Products intake in low obesity countries\n            type=\"line\",\n            x0=df_low_ob['Animal Products'].median(),\n            y0=0,\n            x1=df_low_ob['Animal Products'].median(),\n            y1=12,\n            line=dict(\n                color=\"darkblue\",\n                width=4\n            ),\n    )\n\nfig.add_shape(\n        # Mean value of Animal Products intake in high obesity countries\n            type=\"line\",\n            x0=df_high_ob['Animal Products'].median(),\n            y0=0,\n            x1=df_high_ob['Animal Products'].median(),\n            y1=12,\n            line=dict(\n                color=\"crimson\",\n                width=4\n            ),\n    )\n\n\n\nfig.show()","11c6a511":"fig = px.histogram(kg_df, x = \"Vegetal Products\", nbins=50, color = \"ObesityAboveAvg\", marginal=\"rug\")\n\nfig.add_shape(\n        # Mean value of Vegetal Products intake in low obesity countries\n            type=\"line\",\n            x0=df_low_ob['Vegetal Products'].median(),\n            y0=0,\n            x1=df_low_ob['Vegetal Products'].median(),\n            y1=12,\n            line=dict(\n                color=\"darkblue\",\n                width=4\n            ),\n    )\n\nfig.add_shape(\n        # Mean value of Vegetal Products intake in high obesity countries\n            type=\"line\",\n            x0=df_high_ob['Vegetal Products'].median(),\n            y0=0,\n            x1=df_high_ob['Vegetal Products'].median(),\n            y1=12,\n            line=dict(\n                color=\"crimson\",\n                width=4\n            ),\n    )\n\nfig.show()","afc8b1c9":"fig = px.bar(kg_df, x = \"Country\", y =\"Deaths\", facet_col = \"ObesityAboveAvg\")\nfig.update_xaxes(matches=None,categoryorder=\"total descending\")\nfig.show()","502922af":"kg_df.columns","a5297392":"animal_features = ['Animal fats', 'Aquatic Products, Other', 'Eggs', 'Fish, Seafood', 'Meat',\n                   'Milk - Excluding Butter', 'Offals']\nvegetal_features = ['Alcoholic Beverages', 'Cereals - Excluding Beer', 'Fruits - Excluding Wine', 'Miscellaneous', 'Oilcrops', 'Pulses',\n                    'Spices', 'Starchy Roots', 'Stimulants', 'Sugar & Sweeteners', 'Sugar Crops', 'Treenuts',\n                    'Vegetable Oils', 'Vegetables']","8ca78a2b":"# Sanity check\nkg_df[animal_features + vegetal_features].sum(axis=1).round(2)","197359e2":"df_high_ob.mean()","b8d00fae":"fig = px.pie(values = df_high_ob[animal_features].mean().tolist(), names = animal_features,\n             title='Mean food intake by Animal products groups - High Obesity Countries')\nfig.show()","8c485dfa":"fig = px.pie(values = df_low_ob[animal_features].mean().tolist(), names = animal_features,\n             title='Mean food intake by Animal products groups - Low Obesity Countries')\nfig.show()","08b34ab7":"fig = px.pie(values = df_high_ob[vegetal_features].mean().tolist(), names = vegetal_features,\n             title='Mean food intake by Vegetal products groups - High Obesity Countries')\nfig.show()\n\nfig = px.pie(values = df_low_ob[vegetal_features].mean().tolist(), names = vegetal_features,\n             title='Mean food intake by Vegetal products groups - Low Obesity Countries')\nfig.show()","beb804e8":"fig = px.scatter(kg_df, x = 'Animal Products', y ='Vegetal Products',\n                 color='ObesityAboveAvg', hover_name = 'Country')\nfig.show()","2b399d5c":"df_ob = kg_df[animal_features+vegetal_features+['ObesityAboveAvg']]\ndf_ob.head()","266b560e":"df_ob.describe()","4ac2c2ad":"df_ob.corr()","b8d45047":"ob_features = df_ob.columns.drop('ObesityAboveAvg')\nob_target = 'ObesityAboveAvg'\n\nprint('Model features: ', ob_features)\nprint('Model target: ', ob_target)","a2faf4d0":"from sklearn.model_selection import train_test_split\n\ntrain_data, test_data = train_test_split(df_ob, test_size = 0.2, shuffle = True, random_state = 28)","11f901e5":"print('Training set shape:', train_data.shape)\n\nprint('Class 0 samples in the training set:', sum(train_data[ob_target] == 0))\nprint('Class 1 samples in the training set:', sum(train_data[ob_target] == 1))\n\nprint('Class 0 samples in the test set:', sum(test_data[ob_target] == 0))\nprint('Class 1 samples in the test set:', sum(test_data[ob_target] == 1))","f7164590":"from sklearn.utils import shuffle\n\nclass_0_no = train_data[train_data[ob_target] == 0]\nclass_1_no = train_data[train_data[ob_target] == 1]\n\nupsampled_class_0_no = class_0_no.sample(n=len(class_1_no), replace=True, random_state=42)\n\ntrain_data = pd.concat([class_1_no, upsampled_class_0_no])\ntrain_data = shuffle(train_data)","2c165f42":"print('Training set shape:', train_data.shape)\n\nprint('Class 1 samples in the training set:', sum(train_data[ob_target] == 1))\nprint('Class 0 samples in the training set:', sum(train_data[ob_target] == 0))","e160fabb":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\n\n## Defining the pipeline\n\nclassifier = Pipeline([\n    ('scaler', MinMaxScaler()),\n    ('estimator', KNeighborsClassifier(n_neighbors = 3))\n])\n\n# Visualize the pipeline\nfrom sklearn import set_config\nset_config(display='diagram')\nclassifier","7ad6f0fb":"# Get train data\nX_train = train_data[ob_features]\ny_train = train_data[ob_target]\n\n# Fit the classifier\nclassifier.fit(X_train, y_train)","8f01b7b0":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n\n# Using the fitted model to make predicitions on the training set\n\ntrain_preds = classifier.predict(X_train)\n\nprint('Model performance on the train set:')\nprint(confusion_matrix(y_train, train_preds))\nprint(classification_report(y_train, train_preds))\nprint(\"Train accuracy:\", accuracy_score(y_train, train_preds))","7243145d":"from sklearn.metrics import plot_confusion_matrix\n\ndisp = plot_confusion_matrix(classifier, X_train, y_train)\n\ndisp.ax_.set_title('Confusion matrix for train set');","dd4b76d0":"# Get data to test classifier\nX_test = test_data[ob_features]\ny_test = test_data[ob_target]\n\ntest_preds = classifier.predict(X_test)\n\nprint('Model performance on the test set:')\nprint(confusion_matrix(y_test, test_preds))\nprint(classification_report(y_test, test_preds))\nprint(\"Test accuracy:\", accuracy_score(y_test, test_preds))","5f2d2cc2":"disp = plot_confusion_matrix(classifier, X_test, y_test)\n\ndisp.ax_.set_title('Confusion matrix for test set');","c5ce8844":"# Setting k values to try on our validation performance\nk_values = list(range(1,11))\n\n# Creating a validation set within the train set\nsub_train_data, val_data = train_test_split(train_data, test_size = 0.2, shuffle = True, random_state = 28)\n\n# Upsampling to fix imbalance\nclass_0_no = sub_train_data[sub_train_data[ob_target] == 0]\nclass_1_no = sub_train_data[sub_train_data[ob_target] == 1]\n\nupsampled_class_0_no = class_0_no.sample(n=len(class_1_no), replace=True, random_state=42)\n\nsub_train_data = pd.concat([class_1_no, upsampled_class_0_no])\nsub_train_data = shuffle(sub_train_data, random_state = 28)\n\n# Creating training and validation sets\nX_sub_train = sub_train_data[ob_features]\ny_sub_train = sub_train_data[ob_target]\n\nX_val = val_data[ob_features]\ny_val = val_data[ob_target]","f205406c":"# Searching for best performing K value\nfor k in k_values:\n    classifier = Pipeline([\n    ('scaler', MinMaxScaler()),\n    ('estimator', KNeighborsClassifier(n_neighbors = k))\n    ])\n    \n    classifier.fit(X_sub_train, y_sub_train)\n    val_preds = classifier.predict(X_val)\n    print(f\"K = {k} -- Test accuracy: {accuracy_score(y_val, val_preds)}\")","2b954082":"# Build the classifier\nclassifier = Pipeline([\n    ('scaler', MinMaxScaler()),\n    ('estimator', KNeighborsClassifier(n_neighbors = 2))\n])\n\n# Fit the classifier\nclassifier.fit(X_train, y_train)\n\n# Making predictions on test set\ntest_preds = classifier.predict(X_test)\n\nprint('Model performance on the test set:')\nprint(confusion_matrix(y_test, test_preds))\nprint(classification_report(y_test, test_preds))\nprint(\"Test accuracy:\", accuracy_score(y_test, test_preds))\n\ndisp = plot_confusion_matrix(classifier, X_test, y_test)\n\ndisp.ax_.set_title('Confusion matrix for test set - k = 2');","ec583498":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nknn = KNeighborsClassifier()\n\n# Creating a dictionary of all values to test\nparam_grid = {'n_neighbors': np.arange(2,10)}\n\n# Use grid search to test all values\nknn_gscv = GridSearchCV(knn, param_grid, cv = 5)\n\n# Fit the model to data\nknn_gscv.fit(X_train, y_train)\n\n# Check for best parameter\nknn_gscv.best_params_","97007c8d":"# Accuracy when at best parameters\nknn_gscv.best_score_","7d417fec":"kg_df.columns","127331e2":"# df_mort = kg_df[animal_features+vegetal_features+['Obesity','Mortality']]\ndf_mort = kg_df[kg_df.Country != 'Yemen'][animal_features+vegetal_features+['Obesity','Mortality']]\n# df_mort = kg_df[['Animal Products','Vegetal Products','Obesity','Mortality']]\n\ndf_mort = shuffle(df_mort)\n\nmort_features = df_mort.columns.drop('Mortality')\nmort_target = 'Mortality'\n\nprint('Model features: ', mort_features)\nprint('Model target: ', mort_target)\n\nX = df_mort[mort_features]\ny = df_mort[mort_target]","c5593824":"train_data, test_data = train_test_split(df_mort, test_size = 0.2, shuffle = True, random_state = 28)","26f8d155":"df = df_mort[['Meat', 'Milk - Excluding Butter', 'Fish, Seafood',\n                         'Cereals - Excluding Beer', 'Obesity','Mortality']]\ng = sns.PairGrid(df)\ng.map(plt.scatter)","9d410e7f":"df_mort.corr().tail()","81e2977d":"df_mort.corr().loc['Mortality'].sort_values()","27cdfbad":"# Get train data\nX_train = train_data[mort_features]\ny_train = train_data[mort_target]","b4229ebd":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\n\n## Defining the pipeline\n\nregressor = Pipeline([\n    ('scaler', StandardScaler()),\n    ('estimator', Ridge(random_state=28))\n])\n\n# Visualize the pipeline\nfrom sklearn import set_config\nset_config(display='diagram')\nregressor","7efc30ed":"# Training\nregressor.fit(X_train, y_train)","8c4fb619":"# Scoring the training set\n\ntrain_preds = regressor.predict(X_train)\nregressor.score(X_train, y_train)","53aefecb":"# Cross validate\ncv_score = cross_val_score(regressor, X_train, y_train, cv = 10)\nprint(cv_score)\nprint(cv_score.mean())","86e1946e":"from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n# Create function to evaluate model on a few different scores\ndef show_scores(model, X_train, X_test, y_train, y_test):    \n    train_preds = model.predict(X_train)\n    test_preds = model.predict(X_test)\n    scores = {'Training MAE': mean_absolute_error(y_train, train_preds),\n              'Test MAE': mean_absolute_error(y_test, test_preds),\n              'Training MSE': mean_squared_error(y_train, train_preds),\n              'Test MSE': mean_squared_error(y_test, test_preds),\n              'Training R^2': r2_score(y_train, train_preds),\n              'Test R^2': r2_score(y_test, test_preds)}\n    return scores","a76c2cd7":"# Get data to test model\nX_test = test_data[mort_features]\ny_test = test_data[mort_target]\n\nshow_scores(regressor, X_train, X_test , y_train, y_test)","89fb8a83":"test_plot = X_test.copy()\ntest_plot['Mortality'] = y_test\ntest_plot['Mortality_pred'] = regressor.predict(X_test)\n\ntest_plot.head()","f6ffc3a9":"# fig = px.scatter(test_plot, x = 'Animal fats', y = ['Mortality','Mortality_pred'],\n#                  trendline = \"ols\")\n\n\n# fig.show()","e1808088":"fig, ax = plt.subplots(figsize=[10,8])\n\nsns.regplot(x = 'Animal fats', y = 'Mortality', data = test_plot, ax = ax, label='Mortality')\nsns.regplot(x = 'Animal fats', y = 'Mortality_pred', data = test_plot, ax = ax, label='Mortality_pred')\n\nplt.legend();","fcacbdc9":"from sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost.sklearn import XGBRegressor\n\n# First, we create a dict with our desired models\nmodels = {'Ridge':Ridge(random_state=28),\n          'SVR':SVR(),\n          'RandomForest':RandomForestRegressor(),\n          'XGBoost':XGBRegressor(n_estimators = 1000, learning_rate = 0.05)}\n\n# Now to build the function that tests each model\ndef model_build(model, X_train, y_train, X_test, y_test, scale=True):\n    \n    if scale:\n        regressor = Pipeline([\n            ('scaler', StandardScaler()),\n            ('estimator', model)\n        ])\n    \n    else:\n        regressor = Pipeline([\n            ('estimator', model)\n        ])\n\n    # Training\n    regressor.fit(X_train, y_train)\n\n    # Scoring the training set\n\n    train_preds = regressor.predict(X_train)\n    print(f\"R2 on single split: {regressor.score(X_train, y_train)}\")\n\n    # Cross validate\n    cv_score = cross_val_score(regressor, X_train, y_train, cv = 10)\n\n    print(f\"Cross validate R2 score: {cv_score.mean()}\")\n\n    # Scoring the test set\n    for k, v in show_scores(regressor, X_train, X_test , y_train, y_test).items():\n        print(\"     \", k, v)","3e9a630d":"for name, model in models.items():\n    print(f\"==== Scoring {name} model====\")\n    \n    if name == 'RandomForest' or name == 'XGBoost':\n        model_build(model, X_train, y_train, X_test, y_test, scale=False)\n    else:\n        model_build(model, X_train, y_train, X_test, y_test,)\n    print()\n    print(40*\"=\")\n        ","566ffa17":"xgb = XGBRegressor()\n\nparameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n              'objective':['reg:squarederror'],\n              'learning_rate': [.03, 0.05, .07], #so called `eta` value\n              'max_depth': [5, 6, 7],\n              'min_child_weight': [4],\n              'subsample': [0.7],\n              'colsample_bytree': [0.7],\n              'n_estimators': [500, 1000]}","1deb91ad":"# from sklearn.model_selection import GridSearchCV\n\n# xgb_grid = GridSearchCV(xgb, parameters, cv = 5, n_jobs = 4, verbose = True)\n\n# xgb_grid.fit(X_train, y_train)\n\n# print(xgb_grid.best_score_)\n# print(xgb_grid.best_params_)\n\n## RAN AND GOT THE PARAMETERS USED BELLOW","df75ad30":"xgb_best = XGBRegressor(colsample_bytree = 0.7,\n                        learning_rate = 0.05,\n                        max_depth = 6,\n                        min_child_weight = 4,\n                        n_estimators = 500,\n                        nthread = 4,\n                        objective = 'reg:squarederror',\n                        subsample = 0.7)","6e1e5ab7":"model_build(xgb_best, X_train, y_train, X_test, y_test, scale=False)","e80265d9":"df_mort2 = kg_df[kg_df.Country != 'Yemen'][['Animal Products','Vegetal Products','Obesity','Mortality']]\n\n\ndf_mort2 = shuffle(df_mort2)\n\nmort2_features = df_mort2.columns.drop('Mortality')\nmort2_target = 'Mortality'\n\nprint('Model features: ', mort2_features)\nprint('Model target: ', mort2_target)\n\nX = df_mort2[mort2_features]\ny = df_mort2[mort2_target]","d1c4500e":"df_mort2.head()","3ef637a0":"dummie = df_mort2.copy()\ndummie['Mortality'] = dummie['Mortality']*1000\n\n\nplt.figure(figsize=(10,10))\nsns.boxplot(data = dummie, palette = 'rainbow');","d7c19c9c":"train_data, test_data = train_test_split(df_mort2, test_size = 0.2, shuffle = True, random_state = 28)\n\n# Get train data\nX_train = train_data[mort2_features]\ny_train = train_data[mort2_target]\n\n# Get data to test model\nX_test = test_data[mort2_features]\ny_test = test_data[mort2_target]","f1c56fc3":"# First, we create a dict with our desired models\nmodels = {'Ridge':Ridge(random_state=28),\n          'SVR':SVR(),\n          'RandomForest':RandomForestRegressor(),\n          'XGBoost':XGBRegressor(n_estimators = 1000, learning_rate = 0.05)}","7e7419ab":"for name, model in models.items():\n    print(f\"==== Scoring {name} model====\")\n    \n    if name == 'RandomForest' or name == 'XGBoost':\n        model_build(model, X_train, y_train, X_test, y_test, scale=False)\n    else:\n        model_build(model, X_train, y_train, X_test, y_test,)\n    print()\n    print(40*\"=\")","05f002f3":"model = RandomForestRegressor()","a8195c31":"model.fit(X_train, y_train)","bfd52099":"test_preds = model.predict(X_test)\n\ntest_plot = X_test.copy()\ntest_plot['Mortality'] = y_test\ntest_plot['Mortality_pred'] = test_preds\n\ntest_plot.head()","4ba9c714":"def plotTest(col, target, data):\n    fig, ax = plt.subplots(figsize=[10,8])\n\n    sns.regplot(x = col, y = target, data = data, ax = ax, label=target)\n    sns.regplot(x = col, y = target+'_pred', data = data, ax = ax, label=target+'_pred')\n\n    plt.legend();","dfd974ff":"plotTest('Animal Products', 'Mortality', test_plot)","1ad328a8":"plotTest('Vegetal Products', 'Mortality', test_plot)","75ef43d5":"plotTest('Obesity', 'Mortality', test_plot)","045bc2f5":"df_obes = kg_df[animal_features+vegetal_features+['Obesity']]\n\ndf_obes = shuffle(df_obes)\n\nobes_features = df_obes.columns.drop('Obesity')\nobes_target = 'Obesity'\n\nprint('Model features: ', obes_features)\nprint('Model target: ', obes_target)\n\nX = df_obes[obes_features]\ny = df_obes[obes_target]","df8d3b21":"df_obes.corr().loc['Obesity'].sort_values()","da92f933":"train_data, test_data = train_test_split(df_obes, test_size = 0.2, shuffle = True, random_state = 28)\n\n# Get train data\nX_train = train_data[obes_features]\ny_train = train_data[obes_target]\n\n# Get data to test model\nX_test = test_data[obes_features]\ny_test = test_data[obes_target]","81981e5e":"# First, we create a dict with our desired models\nmodels = {'Ridge':Ridge(random_state=28),\n          'SVR':SVR(),\n          'RandomForest':RandomForestRegressor(),\n          'XGBoost':XGBRegressor(n_estimators = 1000, learning_rate = 0.05)}","37a68472":"for name, model in models.items():\n    print(f\"==== Scoring {name} model====\")\n    \n    if name == 'RandomForest' or name == 'XGBoost':\n        model_build(model, X_train, y_train, X_test, y_test, scale=False)\n    else:\n        model_build(model, X_train, y_train, X_test, y_test,)\n    print()\n    print(40*\"=\")","42715d97":"model = XGBRegressor(n_estimators = 1000, learning_rate = 0.05)","447a0fa6":"model.fit(X_train, y_train)","e5299c7a":"test_preds = model.predict(X_test)\n\ntest_plot = X_test.copy()\ntest_plot['Obesity'] = y_test\ntest_plot['Obesity_pred'] = test_preds\n\ntest_plot.head()","8817d2b9":"# def plotTest(col, target, data):\n#     fig, ax = plt.subplots(figsize=[10,8])\n\n#     sns.regplot(x = col, y = target, data = data, ax = ax, label=target)\n#     sns.regplot(x = col, y = target+'_pred', data = data, ax = ax, label=target+'_pred')\n\n#     plt.legend();","faa55f23":"plotTest('Cereals - Excluding Beer', 'Obesity', test_plot)","1fb8f8f8":"plotTest('Meat', 'Obesity', test_plot)","ffe4a27b":"X = kg_df[kg_df.Country != 'Yemen'][['Obesity', 'Mortality']]\n\nX.head()","3219f326":"scaler = StandardScaler()\n\n# Fit the scaler\nscaler.fit(X)","1ef15f51":"# Transform our data\nX_scaled = scaler.transform(X)\n\n# Sanity checks\nprint(X_scaled.mean(axis = 0))\n\nprint(X_scaled.std(axis=0))","02309cf8":"from sklearn.cluster import KMeans\n\n# Instantiate the model\nkmeans = KMeans(n_clusters = 3)\n\n# Fit the model\nkmeans.fit(X_scaled)\n\n# Make predictions\npreds = kmeans.predict(X_scaled)\n\nprint(preds)","32159c2a":"# Amount of countries in each cluster\n\nunique_countries, counts_countries = np.unique(preds, return_counts=True)\nprint(unique_countries)\nprint(counts_countries)","9ef4f78e":"df_vis = kg_df[kg_df.Country != 'Yemen'].copy()\ndf_vis['cluster'] = [str(i) for i in preds]\n\ndf_vis.head()","a696e745":"fig = px.scatter(df_vis, x = 'Mortality', y = 'Obesity', color = 'cluster', hover_name = 'Country')\nfig.show()","2a9f4051":"# Calculate inertia for a range of clusters number\ninertia = []\n\nfor i in np.arange(1,11):\n    km = KMeans(n_clusters = i)\n    km.fit(X_scaled)\n    inertia.append(km.inertia_)\n    \n# Plotting\nplt.plot(np.arange(1,11), inertia, marker = 'o')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia')\nplt.grid()\nplt.show();","63d6467f":"def cluster_preds(df, feat1, feat2, k):\n    X = df[[feat1, feat2]]\n\n    # Scaling\n    scaler = StandardScaler()\n\n    # Fit the scaler\n    scaler.fit(X)\n\n    # Transform our data\n    X_scaled = scaler.transform(X)\n\n    # Instantiate the model\n    kmeans = KMeans(n_clusters = k)\n\n    # Fit the model\n    kmeans.fit(X_scaled)\n\n    # Make predictions\n    preds = kmeans.predict(X_scaled)\n\n    # Visualizing\n    df_vis = df.copy()\n    df_vis['cluster'] = [str(i) for i in preds]\n\n    fig = px.scatter(df_vis, x = feat1, y = feat2, color = 'cluster', hover_name = 'Country')\n    fig.show()","8678d728":"cluster_preds(kg_df, 'Animal Products', 'Obesity', 3)","2b2abca8":"cluster_preds(kg_df, 'Confirmed', 'Deaths', 3)","beef7ba9":"## Data preprocessing pipeline\n\nFirst, we will do preprocessing on the training set. As there are no missing values, we will build a pipeline to scale features to have similar orders of magnitude by bringing all of them between 0 and 1 using MinMaxScaler and them apply a [KNN](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) classifier.","17d33393":"Let's take a look at the data before building our models.\n\nWe'll start with some visuals and build a scatter matrix for our dataframe. Because we have a large number of features, we will only plot those among the top intake in HOC (check the pie chart for HOC intake).","f48cecbb":"## 3 - Prediciting obesity\n\nAs obesity has a higher correlation with all \"food features\" let's try to buil our models to predict the actaul obesity rate. We expect these models to have better metrics that the ones build to predict mortality.","c61293f4":"## Clustering countries by obesity and mortality due to COVID-19\n\nNow that we have seen that there is a relation between `Obesity` rates and `Mortality` we can try to cluster countries together based on these features.\n\nThe first thing we have to do is to filter all other features:","fed3f426":"# Modeling - Classification\n\n## 1 - KNN for ObesityAboveAvg\n\nWe can do a couple of modeling exercises with this dataset. The first thing we'll try is to check if we can predict the `ObesityAboveAvg` column using food features. Let's start with some exploration:","d980e2d6":"### [Ridge Regression](https:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#ridge-regression)\n\nFirst, we'll split our training data.","28487cde":"#### Training the model","cabeeeb8":"It appears $k = 3$ already is a good value for our modelling.","51052016":"#### Training","a4761287":"With the correlation matrix we can sort the values of the `Mortality` row to get the info we need.","cb06e667":"# Food intake (in kg) by food group\n\n## Data cleaning","a01775d7":"<mark style=\"background-color: lightblue\">We excluded Yemen as it is an outlier in the `Mortality` distirbution.<\/mark>","d01bb002":"Now, to build our regressor with a standardization step in our pipeline (always scale your data for Ridge regression!).","f0bc1993":"### Training and testing multiple models\n\nNow that we have a general flow of testing our model, let's build a function to test different models.\n\nWe will use, besides our Ridge regressor, three other models:\n* [SVR](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVR.html#sklearn.svm.SVR)\n* [Random Forest Regressor](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html)\n*  [XGBoost Regressor](https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#module-xgboost.sklearn)\n\nAs tree base models don't require scaling as we have done for Ridge regressor, our function will have to account for scaling as a parameter. The main goal is to print out various metrics for each model.","d6bdc3cf":"We can wrap this whole clustering process in a function:","c5964c93":"Now, to turn data into numeric types:","22862c63":"#### Visualizing\n\nLet's make a simple visualization of our model's predictions using the firts feature entry (`Animal fats`).","b2c2597a":"As a further look into COVID-19 impact, we can cluster countries based on `Deaths` by `Confirmed` cases (referring to `Mortality`).","1df8e00b":"Now let's check performance on the test set:","4264fc59":"* Next, we'll look at some general distributions from the COVID-19 data:","46884a84":"This looks very bad... Let's see some other metrics for our model.\n\nWe'll use [mean squared error](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error), [mean absolute error](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.mean_absolute_error.html#sklearn.metrics.mean_absolute_error) and [R2](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score) to evaluate the model.\n\nFirst, let's build a simple helper function to return a dictionary with all of our scores for the chosen model.","25d2aa7d":"# Project Healthy Diet (fighting COVID-19)\n\nHow can eating habits help fight the current COVID-19 pandemic? A healthy diet is very important to prevent and recover from various infections. Keeping a healthy immune system is a **must** in our current situation, and what we eat (along with exercising and clearing our heads every now and then) is key.\n\nWhile it is clear that good nutrition alone will not cure nor prevent the spread of COVID-19, it helps us fight back in the case of infection and prevents several other health issues. A lot of tips can be found in [this](https:\/\/www.who.int\/campaigns\/connecting-the-world-to-combat-coronavirus\/healthyathome\/healthyathome---healthy-diet) very usefull and clear page kept by WHO (World Health Organization).\n\nIn this project, we will use data *from food intake by countries* along with data associated with the *spread of COVID-19 and other health issues* the help get new insights into the importance of nutrition and eating habits to combat spreading diseases.\n\nData for this project is taken from [this](https:\/\/www.kaggle.com\/mariaren\/covid19-healthy-diet-dataset) very interesting kaggle dataset. From the owner of the dataset:\n\n> In this dataset, I have combined data of different types of food, world population obesity and undernourished rate, and global COVID-19 cases count from around the world in order to learn more about how a healthy eating style could help combat the Corona Virus. And from the dataset, we can gather information regarding diet patterns from countries with lower COVID infection rate, and adjust our own diet accordingly\n\nThere are 5 files in the dataset:\n* Fat_Supply_Quantity_Data.csv: percentage of fat intake from different food groups for 170 different countries.\n* Food_Supply_Quantity_kg_Data.csv: percentage of food intake( in $kg$ ) from different food groups for 170 different countries.\n* Food_Supply_kcal_Data.csv: percentage of energy intake (in $kcal$) from different food groups for 170 different countries.\n* Protein_Supply_Quantity_Data.csv: percentage of protein intake from different food groups for 170 different countries.\n    * All of these files have, also, columns including obesity, undernourishment and COVID-19 cases as percentages of total population.\n* Supply_Food_Data_Descriptions.csv: This dataset is obtained from FAO.org, and is used to show the specific types of food that belongs to each category for the above datasets.\n","007df119":"The red line represents the avergae obesity rate among countries. In this analysis, we have excluded \"Yemen\", as it was far above the \"main cluster\" of other countries. To clarify, here is the same graph including \"Yemen\":","d170488e":"To visualize the resulting model, let's plot target (`Mortality`) dependecy with all features separately. In each plot, let's see both **real** data and predicted data.","c85a595d":"Let's make two plots:\n\n* First, the `Obesity` dependency on `Cereals - Excluding Beer`, as it has the most negative correlation with the target.\n* Second, the `Obesity` dependency on `Meat`, as it has the most positive correlation with the target.\n\nFor both graphs we'll plot the **real** values of `Obesity` and the ones predicted by our model.","764db538":"Now that we have a list with all categories within `Animal Products`, we can check the distribution of intake in our defined \"High Obesity\" and \"Low Obesity\" countries:","88e9db85":"Now we have no missing values and all data is numeric, except for country names.","62867320":"Let's check the correlation of features with target:","039d9589":"In this figure, the size of the points correspond to the country's COVID-19 mortality. Here we can see that Yemen indeed stands out as having a big mortality (the huge point just bellow the mean obesity red line).","82ec252a":"### K-means modeling","e6c3962f":"OK, so we have strings and some of them are of the form '<2.5'. Let's replace these values with '2.0', as a very crude way of dealing with these values. We need to remember, in the analysis, that all values '2.0' represent something below 2.5.","15fcce63":"We want to fix any imbalance only in the training set. The test set should keep the original distribution.","79664e08":"Something is not a number in the `Undernourished` columns. Let's inspect:","1b39cf3b":"We can use the same workflow above to test different models. To keep things clear, we'll define, again, the dictionary of all models we'll be using:","adb059f1":"## Investigate: does obesity rate affect impact of COVID-19?\n\nThere is a nice report from **Science** ([sciencemag](https:\/\/www.sciencemag.org\/)) linking obesity to COVID-19 mortalitiy:\n* [Why COVID-19 is more deadly in people with obesity\u2014even if they\u2019re young](https:\/\/www.sciencemag.org\/news\/2020\/09\/why-covid-19-more-deadly-people-obesity-even-if-theyre-young)\n\nFrom the authors:\n> Since the pandemic began, dozens of studies have reported that many of the sickest COVID-19 patients have been people with obesity. In recent weeks, that link has come into sharper focus as large new population studies have cemented the association and demonstrated that even people who are merely overweight are at higher risk.\n\nOur hypothesis is that we can find a pattern from this datset supporting this report. To do so, we'll start by simply plotting the Obesity rate against our newly defined Mortality.","5561c72f":"## Training and test datasets","9b5f3623":"We can find an optimal value for $k$ (number of clusters) using the \"elbow\" method.","9f8228e0":"Now we can dig into the files.","cf101a13":"So, the highest correlation with `Mortality` is `Milk - Excluding Butter` (note that this is the highest intake from both HOC and LOC).\n\nLet's actually build some models now.","0736ca26":"## Training\n\nFirst we train our classifier with the **.fit()** method.","c4be9fac":"### Building best performing model","f32f1e10":"### Using GridSearchCV","e731bae1":"Let's define, for ease of writting, the following: **HOC** - High Obesity Countries and **LOC** - Low Obesity Countries.\nHere, we have some major differences:\n* The intake of `Starchy Roots` in LOC is almost $20\\%$, double that of HOC.\n* The intake of `Alcoholic Beverages` is at $5.8\\%$ in LOC, as in HOC it reaches almost $10\\%$.\n* The intake of `Sugar & Sweeteners` is at $4.78\\%$ in LOC, as in HOC it reaches almost $9.5\\%$.\nSome others can be seen, but these caught my attention.","a3bf57d4":"So, we are interested in the last \"row\" of this matrix. Nothing seems particularly linear, but we'll see what we can tell from building linear models.\n\n**NOTE**: as the data seems very scattered, I am expecting bad values of $R^2$ score (for an interessting explanation on why this is so, I recommend reading this two well written articles: [Interpreting R-squared](https:\/\/statisticsbyjim.com\/regression\/interpret-r-squared-regression\/) and [Interpreting low R-squared in regression models](https:\/\/statisticsbyjim.com\/regression\/low-r-squared-regression\/)).","b5b722b4":"## 2.1 - A simpler model for Mortality\n\nLet's try to reduce the dimensionality by using only two features: **Animal Products** and **Vegetal Products**.","625c1ce7":"Beyond the columns described in the `Categories` from the data description, we have 7 other columns:\n* Obesity: obesity rate\n* Undernourished: undernourished rate\n* Confirmed: confirmed cases of COVID-19, by population\n* Deaths: confirmed deaths from COVID-19, by population\n* Recovered: recovered cases of COVID-19, by population\n* Active: active cases of COVID-19, by population\n* Population: country population","c9f2bbd7":"## Distribution of food intake (in kg) by product type\n\nOk, now we can dig into the separate food types (`Animal Products` and `Vegetal Products`) to see their distributions.\n\nFirst, let's define a list for the features in each food type:","183b86cc":"The color corresponds to the `ObesityAboveAvg`. Here, again, we see there is a relation between high consuption of Animal Products (comparing with Vegetal Products) and high obesity rates. Using the hover information you can find the country with highest Animal Products intake (Finland) and the one with highest Vegetal Products intake (Nigeria).","5d023b27":"We have very few data points and the data is very scattered (bad value for $R^2$). But, this simplified model seems to compare well with the more complex one.","f26932dd":"We have some missing data from these last columns. We'll start by simply dropping these data.","e8299d13":"Now we can do the search (note that it can take a long time).","1d16e6a1":"#### Training","ddd05fc3":"Well, for all countries this sum appears to be roughly $50 \\%$ of food intake in $kg$. That is strange, as this two are a sum of all other columns.\n\nTo understand the data better, let's sum all food related categories.","c4c00e45":"All centroid-based algorithms need a scaling step before modelling. And, as this is a case of unsupervised learning model, we don't need to split the data.\n\nLet's first instantiate our scaler.","e5370ed1":"It looks like our model is doing a good job predicting these feature's influence on the obesity rate.","abc69f7a":"### Train-test split","75f3faa7":"From this last figure, we can see that `Yemen` stands out as having a very alarming mortality (almost $30\\%$). However, `Yemen` also appears as one of the lowest death rate countries (death rate of 0.001955).","c480c16e":"### Hyperparameter tunning for XGBoost model\n\nWe can pick our best perfroming model and try some hyperparameter tunning with a simple [GridSearch](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html).\n\nLet's start by defining our parameters:","8236b6b2":"#### Cross validate our score","eabc0a98":"Now, to understand the dataset a bit more clearly, let's do some sanity checks.","440b07f8":"Great! Our understanding is correct.\n\nTo further investigate the impact of deaths by COVID-19, we will create a column `Mortality` which will be calculated as `Deaths` by `Confirmed`.","fe097b43":"**Train-test splitting**","d725e0aa":"Now to split our data.","370533eb":"Now, let's test the model in the test set.","200248de":"#### Visualizing","6a79bc67":"Now we can quickly cluster together countries based on `Animal Products` intake and `Obesity` rate (recalling we used this features in our \"simpler model to predict mortality\").","c936c7da":"# Conclusions\n\nThere are MANY factors that are important to fight against the current COVID-19 epidemic. Maintaining good eating habits helps keep our immune system healthy and ready to combat a possible disease.\n\nIn this notebook I tried to explore possible patterns found in data of COVID-19 and food intake in different countries. One major goal was to find the influence of obesity rates in the effect of the disease in each country. Splitting countries into HOC and LOC groups, it was possible to create a classifier, with good accuracy, predicting in which group would a country be based on its food intake data.\n\nHaving this, we created regression models to try to predict the `Mortality` of COVID-19 in countries based on ther eating habits and obesity rate. Two approaches were taken: one with all food related features taken as parameters and a simpler one. Both have issues (mainly of spread and non-linearity), but we could show use of different models and metrics.\n\nNext, we build a model to predict `Obesity` rates based on eating habits in each country. This model was far more succesfull and the overall tendecy of the data was captured and predicted.\n\nFinally, we build a quick helper function to do some clustering based on pairs of features.\n\nFor a more visual data exploration, I have built simple dashboards to do some EDA with Dash:\n- [App1](https:\/\/healthycovid19app1.herokuapp.com\/)\n- [App2](https:\/\/healthycovid19app2.herokuapp.com\/)\n- [App3](https:\/\/healthycovid19app3.herokuapp.com\/)\n\nThe app was split into 3 to avoid long processes erros in Heroku.\n\nPlease comment if you liked the notebook and critic if you found any inaccuracies. I am still very new to the field and this is my second ever notebook, so suggestions are very welcome!\n\nStay safe everyone!","de5016b9":"Let's do a simple box visual of our distributions. To better see the `Mortality` distribution, we'll multiply the column by $1000$ just for the boxplot.","e1ee0ca1":"### Target balancing","5ee8e06e":"#### Making predictions and visualizing","35043646":"Here, the size of points corresponds to the active cases of COVID-19. As expected, there is a tendency of having more deaths where more confirmed cases are present.","fd954567":"We can loop through these and use our `model_build` function once more:","2f424483":"Now that we have our helper function, we loop through our `models` dictionary and score each one of them.","433c4a73":"We have created a column `ObesityAboveAvg` that has value **1** if the country has obesity rate above the mean of all other countries, and **0** otherwise.","441e0d0b":"## Tunning the value of n_neighbors","591aa9d6":"In the figure above, we can see **clearly** that the \"high obesity rate\" countries have a worst impact from COVID-19.","078669cf":"That fixed the issue. Now, let's do some sanity checks with the COVID-19 categories. \n\nColumns related to this are: **'Confirmed', 'Deaths', 'Recovered', 'Active'**.\n\nIf my understanding is correct, the number of confirmed cases should be the sum of deaths, recoverd and active. Let's investigate.","845ad0a8":"To keep track of what is important later on, let's check what features correlate the most with our target.","4aa65e15":"Ok, so it looks like we are counting twice every entry inside `Animal Products` and `Vegetal Products`. From my understanding, `Animal Products` + `Vegetal Products` should sum to $100\\%$ of the food intake. This is easily fixed by multiplying all columns of food categories by 2.","a7aa1049":"Ok, the distributions are somewhat similar. The order of highest to lowest intake is the same (except for `Offals` and `Animal fats`). However, two things stand out:\n* The `Milk - Excluding Butter` intake int he first group is huge (almost $60\\%$!)\n* The difference between the `Fish, Seafood` intake in both groups (the first - around $7\\%$, the second - around $20\\%$).\n\nLet's see the vegetal intake:","f06cba5c":"We gained a slight improvement in accuracy with this quick tunning.","bcd8ab55":"# Modeling - Regression\n\n## 2 - Prediciting mortality\n\nWe'll try to build a model (regressor) to predict the mortality rate based on food inatke information and obesity. Let's start by choosing the right features.","555c1680":"#### Making predictions and visualizing","df244ec8":"**THIS HAS STOCHASTIC NATURE AND WILL CHANGE EACH RUN:** It looks that K = 2 has the best performance. Let's use K = 2 for our classifier, train it on the train set and test it on our test set.","166971e4":"What is the sum of `Animal Products` and `Vegetal Products`?","043c8a01":"## Testing","dba9f861":"Above we see a plot of `Mortality` as a function of the `Animal fats`. Our model fails to make a good prediction but it somehow captures the **tendency** of our target. Let's try to imporve by comparing other models.","8dca21ad":"This might be a naive first analysis, but countries with obesity rates above the mean of all countries have a higher consumption of `Animal Products` and lower consuption of `Vegetal Products`. The vertical lines in both figures represent the **median** value of intake for each group.","a3288c7c":"Our finding: **The \"high mortality\" and \"high death rate\" countries all seem to have an above average obesity rate.**","e5d29deb":"We can create a simple graph of `Vegetal Products` versus `Animal Products` to get a new visual on the distribtuion of **HOC** and **LOC**.","c7460877":"It looks like we might have some redundant categories. Reading `Animal Products` and `Vegetal Products`, it seems they are a summary of other categories. We should be carefull when using these categories for modeling.","fe23492c":"Let's start by looking into the descriptions","ea6a56f1":"### Building best performing model","33ef46d4":"## Distribution of food intake (in kg) - exploring high obesity cases\n\n<mark style=\"background-color: lightblue\">Let's inspect this further. What can we say about the food intake in countries grouped by obesity rate?<\/mark>","23843655":"##  General COVID-19 data: analysis and further cleaning\n\nBefore digging into the data from food intake, let's create a simple visualization of COVID-19 cases by country."}}