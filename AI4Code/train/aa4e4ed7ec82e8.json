{"cell_type":{"463599f6":"code","0908e36e":"code","f6444ace":"code","6753c26b":"code","081a85a3":"code","33813d11":"code","8431cf6e":"code","45ce856b":"code","c173db53":"code","71a8d0c9":"code","ced4e059":"code","2cac5ee5":"code","c6df0f24":"code","a2d95620":"code","8b77d812":"code","88707380":"code","612b7705":"code","578a242c":"code","2ccfcee0":"code","925cb805":"code","a56e80bc":"code","5bc05529":"code","9cfb16b8":"code","222e983c":"code","1d10cf23":"code","150541b5":"code","4b2c571e":"markdown","252aacdd":"markdown","75eef018":"markdown","665ff021":"markdown","c84dfb9a":"markdown","1d712107":"markdown","ed54f3c5":"markdown","c6b77223":"markdown","a1fb736c":"markdown"},"source":{"463599f6":"import numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport category_encoders as ce \nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nimport xgboost as xgb\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nfrom sklearn.feature_selection import SelectFromModel\nfrom collections import Counter\nimport pickle\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.impute import MissingIndicator\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.inspection import permutation_importance\nfrom category_encoders import TargetEncoder, LeaveOneOutEncoder\nimport random","0908e36e":"df_train =  pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n","f6444ace":"def seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\nseed_everything(0)","6753c26b":"y_train = np.log(df_train['SalePrice'])\nX_train = df_train.drop(['SalePrice','Id'],axis=1)\nX_test = df_test.drop('Id',axis=1)\nX_train_noise = X_train.copy()\nprint('Train data: ',X_train.shape)\nprint('Test data: ',X_test.shape)","081a85a3":"cat_nominal_features = pickle.load(open('..\/input\/features-housing\/cat_nominal_features.p', \"rb\" ))\ncat_ordinal_features = pickle.load(open('..\/input\/features-housing\/cat_ordinal_features.p', \"rb\" ))\nnum_features = pickle.load(open('..\/input\/features-housing\/num_features.p', \"rb\" ))\n\n# Besides these features, we are also going to add a random noise feature\ncat_features = cat_nominal_features + cat_ordinal_features\nnum_features_noise = num_features+['noise']\nprint('Number of numeric features: ',len(num_features))\nprint('Number of numeric features including noise: ',len(num_features_noise))\nprint('Number of ordinal features: ',len(cat_ordinal_features))\nprint('Number of nominal featuures: ',len(cat_nominal_features))","33813d11":"# Fill missing categorical entries with string \"Missing\"\n# Fill missing numeric entries with np.nan\n# We are going to let Xgboost to automatically handle missing features for us\nX_train_noise['noise'] = np.random.normal(size=(1460))\nX_train[cat_ordinal_features] = X_train[cat_ordinal_features].fillna(np.nan)\nX_train[cat_nominal_features] = X_train[cat_nominal_features].fillna('Missing')\nX_train_noise[cat_ordinal_features] = X_train_noise[cat_ordinal_features].fillna(np.nan)\nX_train_noise[cat_nominal_features] = X_train_noise[cat_nominal_features].fillna('Missing')","8431cf6e":"model = xgb.XGBRegressor(random_state=0)\nscoring = 'neg_root_mean_squared_error'","45ce856b":"# These features have order information embeded in them, that's why we use ordinal encoder.\nce_ord = ce.OrdinalEncoder(cols=cat_ordinal_features, mapping=[{'col': 'Street', 'mapping': {'Grvl': 1, 'Pave': 2}},\n                                            {'col': 'Alley', 'mapping': {np.nan:0,'Grvl': 1, 'Pave': 2}}, \n                                            {'col': 'Utilities', 'mapping': {'NoSeWa': 1, 'AllPub':2}},\n                                            {'col': 'ExterQual', 'mapping': {'Po':1,'Fa':2,'TA':3,\n                                                                            'Gd':4,'Ex':5}},\n                                            {'col': 'ExterCond', 'mapping': {'Po':1,'Fa':2,'TA':3,\n                                                                            'Gd':4,'Ex':5}},\n                                            {'col': 'BsmtCond', 'mapping': {np.nan:0,'Po':1,'Fa':2,'TA':3,\n                                                                            'Gd':4,'Ex':5}},\n                                            {'col': 'BsmtQual', 'mapping': {np.nan:0,'Po':1,'Fa':2,'TA':3,\n                                                                            'Gd':4,'Ex':5}},\n                                            {'col': 'BsmtExposure', 'mapping': {np.nan:0,'No':1,'Mn':2,'Av':3,\n                                                                            'Gd':4}},\n                                            {'col': 'BsmtFinType1', 'mapping': {np.nan:0,'Unf':1,'LwQ':2,'Rec':3,\n                                                                            'BLQ':4,'ALQ':5,'GLQ':6}},\n                                            {'col': 'BsmtFinType2', 'mapping': {np.nan:0,'Unf':1,'LwQ':2,'Rec':3,\n                                                                            'BLQ':4,'ALQ':5,'GLQ':6}},     \n                                            {'col': 'HeatingQC', 'mapping': {'Po':1,'Fa':2,'TA':3,\n                                                                            'Gd':4,'Ex':5}},\n                                            {'col': 'CentralAir', 'mapping': {'Y':1,'N':0}},\n                                            {'col': 'KitchenQual', 'mapping': {'Po':1,'Fa':2,'TA':3,\n                                                                            'Gd':4,'Ex':5}}, \n                                            {'col': 'Functional', 'mapping': {'Typ':8,'Min1':7,'Min2':6,\n                                                                            'Mod':5,'Maj1':4,'Maj2':3,\n                                                                             'Sev':2,\"Sal\":1}},                                    \n                                            {'col': 'FireplaceQu', 'mapping': {np.nan:0,'Po':1,'Fa':2,'TA':3,\n                                                                            'Gd':4,'Ex':5}},\n                                            {'col': 'GarageFinish', 'mapping': {np.nan:0,'Unf':1,'RFn':2,'Fin':3}},\n                                            {'col': 'GarageQual', 'mapping': {np.nan:0,'Po':1,'Fa':2,'TA':3,\n                                                                            'Gd':4,'Ex':5}},\n                                            {'col': 'GarageCond', 'mapping': {np.nan:0,'Po':1,'Fa':2,'TA':3,\n                                                                            'Gd':4,'Ex':5}},\n                                            {'col': 'PavedDrive', 'mapping': {'N':1,'P':2,'Y':3}},\n                                            {'col': 'PoolQC', 'mapping': {np.nan:0,'Fa':1,'TA':2,\n                                                                            'Gd':3,'Ex':4}},\n                                            {'col': 'Fence', 'mapping': {np.nan:0,'MnWw':1,'GdWo':2,'MnPrv':3,\n                                                                            'GdPrv':4}}],\n                                            handle_unknown='value',\n                                            handle_missing='value')\n\nce_nom = ce.OneHotEncoder(cols=cat_nominal_features,handle_unknown='value',handle_missing='value')","c173db53":"# This function is used to define the processing pipeline\ndef get_tree(seed):\n    numeric_transformer = Pipeline(steps=[\n            ('imputer', SimpleImputer(strategy='constant',fill_value=-1)),\n            ])\n\n    ct1 = ColumnTransformer(\n            transformers=[\n                ('nominal',ce_nom,cat_nominal_features),\n                ('ordinal',ce_ord,cat_ordinal_features),\n                ('num',numeric_transformer,num_features_noise)\n                ],remainder = 'passthrough')\n    clf_tree = Pipeline(steps=[('preprocessor', ct1),\n                          ('regressor', xgb.XGBRegressor(random_state=seed))])\n    return clf_tree","71a8d0c9":"# We are going to do a 5-fold cross-validation for MDI-based features\ndef MDI_imp(train_data,y_data):\n    all_feature_imp = []\n    kFold = KFold(n_splits=5, random_state=0, shuffle=True)\n    clf_tree = get_tree(0)\n    for fold, (trn_idx, val_idx) in enumerate(kFold.split(train_data)):\n        X_train_temp = train_data[trn_idx]\n        X_val_temp = train_data[val_idx]\n        y_train_temp = y_data.loc[trn_idx]\n        y_val_temp = y_data.loc[val_idx] \n        clf_tree.named_steps['regressor'].fit(X_train_temp,y_train_temp)\n        feature_imp_temp = np.abs(clf_tree.named_steps['regressor'].feature_importances_)\n        all_feature_imp.append(feature_imp_temp)\n    all_feature_imp = np.array(all_feature_imp)\n    feature_imp_mean = np.mean(all_feature_imp,axis=0)\n    feature_imp_std = np.std(all_feature_imp,axis=0)\n    return feature_imp_mean, feature_imp_std","ced4e059":"# Similarly, we are going to do 5-fold for permutation feature importance. \n# Both training and testing datasets will be evaluated. \n# Their results will be combined.\n# train_imp_mean shape - [n_features, n_repeats]\ndef perm_imp(train_data,y_data, num_repeats,seed):\n    all_perm_train_imp = np.zeros((train_data.shape[1],num_repeats))\n    all_perm_test_imp = np.zeros((train_data.shape[1],num_repeats))\n    kFold = KFold(n_splits=5, random_state=0, shuffle=True)\n    clf_tree = get_tree(0)\n    for fold, (trn_idx, val_idx) in enumerate(kFold.split(train_data.index)):\n        X_train_temp = train_data.loc[trn_idx]\n        X_test_temp = train_data.loc[val_idx]\n        y_train_temp = y_data.loc[trn_idx]\n        y_test_temp = y_data.loc[val_idx]\n        \n        clf_tree.fit(X_train_temp, y_train_temp)\n        feature_imp_temp = permutation_importance(clf_tree, X_train_temp, y_train_temp, n_repeats=num_repeats,\n                                         random_state=seed, n_jobs=-1)\n        feature_imp_temp = np.abs(feature_imp_temp['importances'])\n        all_perm_train_imp += feature_imp_temp\n        \n        feature_imp_temp = permutation_importance(clf_tree, X_test_temp, y_test_temp, n_repeats=num_repeats,\n                                         random_state=seed, n_jobs=-1)\n        feature_imp_temp = np.abs(feature_imp_temp['importances'])\n        all_perm_test_imp += feature_imp_temp\n        print(fold)\n        \n    all_perm_train_imp = all_perm_train_imp \/ 5\n    all_perm_test_imp = all_perm_test_imp \/ 5\n    train_imp_mean = np.mean(all_perm_train_imp,axis=1)\n    test_imp_mean = np.mean(all_perm_test_imp,axis=1)\n    train_imp_std = np.std(all_perm_train_imp,axis=1)\n    test_imp_std = np.std(all_perm_test_imp,axis=1)\n    return train_imp_mean, test_imp_mean, train_imp_std, test_imp_std","2cac5ee5":"# RFECV is going to do a kFold evaluation on all the features and rank them\n# Eliminate features at a step 0.05*n_featurees\ndef feature_RFE(train_data,y_data,seed):\n    support = []\n    n_features = []\n    scores = []\n    rfecv = RFECV(estimator=model, step=0.05, cv=KFold(5,random_state=seed,shuffle=True),\n                  scoring=scoring)\n    rfecv.fit(train_data, y_train)\n    return rfecv","c6df0f24":"# Helper function for geting unique feature \n# names as some features are one-hot encoded\ndef filter_features(features):\n    features_unique = []\n    for c in features:\n        features_unique.append(c.split(\"_\")[0])\n        feat_unique = list(Counter(features_unique).keys())\n        feat_counts = list(Counter(features_unique).values())\n    return feat_unique, feat_counts","a2d95620":"# Helper function for finding features ranked higher than noise feature\ndef find_useful_features(feat_imp,noise_loc,columns):\n    idx = feat_imp.argsort()\n    idx_noise = np.where(idx == noise_loc)[0][0]\n    idx = idx[idx_noise+1:]\n    features = np.array(columns)[idx]\n    return features","8b77d812":"# We need to get column names and check our noise feature\nclf_tree = get_tree(0)\ntrain_data = clf_tree.named_steps['preprocessor'].fit_transform(X_train_noise,y_train)\nnominal_columns =  list(ce_nom.fit_transform(X_train_noise[cat_nominal_features]).columns)\nordinal_columns = list(ce_ord.fit_transform(X_train_noise[cat_ordinal_features]).columns)\ncolumn_names = nominal_columns + ordinal_columns + num_features_noise\nprint('Training data shape: ',train_data.shape)\nprint('Number of columns: ',len(column_names))\n\n# We make a plot about this noise feature\nplt.hist(train_data[:,228])\nplt.title('Noise Feature')","88707380":"#Now apply MDI_imp, and find features ranked higher than noise features\nmdi_imp_mean, mdi_imp_std = MDI_imp(train_data,y_train)\nfeatures_mdi = find_useful_features(mdi_imp_mean,228,column_names)\nprint(\"Number of MDI features ranked above noisy feature: %d\" % len(features_mdi))","612b7705":"# We make a plot for visualization purpose for top 20 features\ntop_idx = mdi_imp_mean.argsort()[-20:]\ny_ticks = np.arange(0, 20)\nfig, ax = plt.subplots()\nax.barh(y_ticks, mdi_imp_mean[top_idx])\nax.set_yticklabels(np.array(column_names)[top_idx])\nax.set_yticks(y_ticks)\nax.set_title(\"Xgboost MDI\")\nfig.tight_layout()\nplt.show()","578a242c":"# Some of these features are one-hot-encoded as you can see based on the figure above \n# we can get their unique feature names by applying the filter function\nfeatures_mdi_filter, features_mdi_count = filter_features(features_mdi)\nprint('Num mdi features after filter: ',len(features_mdi_filter))\nprint(features_mdi_filter)","2ccfcee0":"# Now we do permutation feature importance\nperm_imp_mean_train1,perm_imp_mean_test1,perm_imp_std_train1,perm_imp_std_test1 = perm_imp(X_train_noise,y_train,5,1)","925cb805":"# The last feature in this list is noise feature, i.e. position 79\nperm_scores_train = dict(zip(list(X_train_noise.columns),list(perm_imp_mean_train1)))\nperm_scores_test = dict(zip(list(X_train_noise.columns),list(perm_imp_mean_test1)))\nprint(len(perm_scores_train))\nprint(len(perm_scores_test))","a56e80bc":"## We make some visualization\ntop_idx = perm_imp_mean_test1.argsort()[-40:]\ny_ticks = np.arange(0, 40)\nfig, ax = plt.subplots(figsize=(7,7))\nax.barh(y_ticks, perm_imp_mean_test1[top_idx])\nax.set_yticklabels(np.array(X_test.columns)[top_idx])\nax.set_yticks(y_ticks)\nax.set_title(\"Xgboost Perm\")\nfig.tight_layout()\nplt.show()","5bc05529":"# We are going to combine features found based on\n# both training and testing datasets\nperm_features_train = list(find_useful_features(perm_imp_mean_train1,79,X_train_noise.columns))\nperm_features_test = list(find_useful_features(perm_imp_mean_test1,79,X_train_noise.columns))\nperm_features = list(set(perm_features_train) | set(perm_features_test))\nperm_features_scores_train = dict((k, perm_scores_train[k]) for k in perm_features)\nperm_features_scores_test = dict((k, perm_scores_test[k]) for k in perm_features)\nperm_features_scores = {}\nfor key in perm_features_scores_train.keys():\n    perm_features_scores[key] = (perm_features_scores_train[key] + perm_features_scores_test[key]) \/ 2 \nperm_features = np.array(perm_features)[np.array(list(perm_features_scores.values())).argsort()]\nperm_scores = np.array(list(perm_features_scores.values()))[np.array(list(perm_features_scores.values())).argsort()]","9cfb16b8":"print(perm_features)\nprint(perm_scores)","222e983c":"# Now we run RFE\nrfecv = feature_RFE(train_data,y_train,0)","1d10cf23":"print(\"Optimal RFE number of features : %d\" % rfecv.n_features_)\nprint(\"Feature Ranking: \")\nprint(rfecv.ranking_)\nfeatures_rfe = np.array(column_names)[rfecv.support_]\n# Check whether noise feature is in optimal features or not \nprint('Is noise in optimal features: ','noise' in features_rfe)\nfeatures_rfe = [c for c in features_rfe if c not in ['noise']]\n# We filter these rfe features to get their unique names\nfeatures_rfe, features_rfe_count = filter_features(features_rfe)\nprint('Number of RFE features after filter: ',len(features_rfe))","150541b5":"# We make some plot for visiualization\nplt.figure()\nplt.xlabel(\"Steps\")\nplt.ylabel(\"CV score\")\nplt.scatter(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","4b2c571e":"## MDI vs Perm vs RFE ##\nIn this notebook, we are going to look at tree-based impurity reduction feature importance, permutation\nfeature importance and reccurent feature elimination.\n\nTree-based method tends to favor numeric or high cardinality features, which make feature selection less \naccurate. Feature importance based on tree methods are called mean decrease in impurity (MDI) methods.\n\nAs documented on Scikit-learn: \" The permutation feature importance is defined to be the decrease in a model score when a single feature value is randomly shuffled. This procedure breaks the relationship between the feature and the target, thus the drop in the model score is indicative of how much the model depends on the feature.\" \n(https:\/\/scikit-learn.org\/stable\/modules\/permutation_importance.html)\n\nRecurrent feature elimination: \"Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\" \n(https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.RFE.html)\n\n**To see whether a feature is really useful for your model, we can add an additional feature - random noise, and see the ranking position of this random noise feature. Features ranked below may not be useful comparing to features ranked above. When we do feature engineering, we can focus more on the features ranked above this noise feature. **\n\nThis notebook will go through all three techniques including how to use the random noise indicator.","252aacdd":"## Permutation Feature Importance ##","75eef018":"## MDI Feature Importance ##","665ff021":"Features have been grouped nominal, ordinal, and numeric features. ","c84dfb9a":"## Summary ##","1d712107":"In summary, in this notebook, we have three techniques to extract useful features, i.e. MDI\nfeature importance, permutation feature importance and recurrent feature elimination.\nOnce you have selected the most important features, then you can do feature engineering\non them to boost your model performance.\n\nIf you find this notebook helpful, please upvote.","ed54f3c5":"## Define categorical and ordinal encoder and helper functions ## ","c6b77223":"## Load Data ##","a1fb736c":"## Recurrent Feature Elimination ##"}}