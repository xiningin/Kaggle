{"cell_type":{"094608ae":"code","a1a615d9":"code","32ac9883":"code","e7dfdf1d":"code","7776a847":"code","6ac670cc":"code","0a5498f8":"code","06848f54":"code","ca218b5d":"code","10633aa7":"code","88646376":"code","a2720a27":"code","1c419757":"code","e81206b8":"code","f47abc63":"code","611841b1":"code","89f68770":"code","79d84b82":"code","175f1213":"code","417f55f2":"code","4f370c9d":"code","396879ac":"code","8e26edf8":"code","38d3d0e3":"code","a5639353":"markdown","b1e7e72d":"markdown","306122a0":"markdown","75e1e744":"markdown","fb6a0db9":"markdown","52b40c77":"markdown","ca57c056":"markdown","2ddf4aab":"markdown","52c7b032":"markdown","e0ac1f6a":"markdown","ca6a3832":"markdown","3bbca4f5":"markdown","7a3e5e03":"markdown","e9c500f6":"markdown","679c4da8":"markdown","d99a0712":"markdown","cf61d971":"markdown"},"source":{"094608ae":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn import preprocessing","a1a615d9":"titanic_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntitanic_df.head(5)","32ac9883":"print(titanic_df.info())","e7dfdf1d":"titanic_df['Age'].fillna(titanic_df['Age'].mean(), inplace = True)\ntitanic_df['Cabin'].fillna('N', inplace = True)\ntitanic_df['Embarked'].fillna('N', inplace = True)\nprint('dataset\uc758 Null \uac12 \uac2f\uc218:',titanic_df.isnull().sum().sum())","7776a847":"print(' Sex \uac12 \ubd84\ud3ec:\\n', titanic_df['Sex'].value_counts())\nprint('\\n Cabin \uac12 \ubd84\ud3ec:\\n', titanic_df['Cabin'].value_counts())\nprint('\\n Embarked \uac12 \ubd84\ud3ec:\\n', titanic_df['Embarked'].value_counts())","6ac670cc":"titanic_df['Cabin'] = titanic_df['Cabin'].str[:1]\nprint(titanic_df['Cabin'].head(5))","0a5498f8":"titanic_df.groupby(['Sex','Survived'])['Survived'].count()","06848f54":"sns.barplot(x='Sex', y='Survived', data =titanic_df)","ca218b5d":"sns.barplot(x='Pclass', y='Survived', hue='Sex', data=titanic_df)","10633aa7":"def get_category(age):\n    cat = ''\n    if age <= -1: cat = 'Unknown'\n    elif age <= 5: cat = 'Baby'\n    elif age <= 12: cat = 'Child'\n    elif age <= 18: cat = 'Teenager'\n    elif age <= 25: cat = 'Student'\n    elif age <= 35: cat = 'Young Adult'\n    elif age <= 60: cat = 'Adult'\n    else: cat = 'Elderly'\n        \n    return cat","88646376":"plt.figure(figsize=(10,6))\ngroup_names = ['Unknown', 'Baby', 'Child', 'Teenager', \n               'Student', 'Young Adult', 'Adult', 'Elderly']\n\ntitanic_df['Age_cat'] = titanic_df['Age'].apply(lambda x : get_category(x))\nsns.barplot(x='Age_cat', y='Survived', hue='Sex', data = titanic_df, order=group_names)","a2720a27":"def encode_features(dataDF):\n    features = ['Cabin', 'Sex', 'Embarked']\n    for feature in features:\n        le = preprocessing.LabelEncoder()\n        le = le.fit(dataDF[feature])\n        dataDF[feature] = le.transform(dataDF[feature])\n        \n    return dataDF\n\ntitanic_df = encode_features(titanic_df)\ntitanic_df.head()","1c419757":"def fillna(df):\n    df['Age'].fillna(df['Age'].mean(), inplace=True)\n    df['Cabin'].fillna('N', inplace = True)\n    df['Embarked'].fillna('N', inplace = True)\n    df['Fare'].fillna(0,inplace=True)\n    return df\n\ndef drop_features(df):\n    df.drop(['Name', 'Ticket'], axis = 1, inplace = True)\n    return df\n\ndef format_features(df):\n    df['Cabin'] = df['Cabin'].str[:1]\n    features = ['Cabin', 'Sex', 'Embarked']\n    for feature in features:\n        le = preprocessing.LabelEncoder()\n        le = le.fit(df[feature])\n        df[feature] = le.transform(df[feature])\n    return df\n\ndef transform_features(df):\n    df = fillna(df)\n    df = drop_features(df)\n    df = format_features(df)\n    return df","e81206b8":"titanic_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ny_titanic_df = titanic_df['Survived']\nX_titanic_df = titanic_df.drop('Survived', axis = 1)\n\nX_titanic_df = transform_features(X_titanic_df)","f47abc63":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X_titanic_df, y_titanic_df, test_size=0.2, random_state = 11)","611841b1":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\ndt_clf = DecisionTreeClassifier(random_state = 11)\nrf_clf = RandomForestClassifier(random_state = 11)\nlr_clf = LogisticRegression()\n\ndt_clf.fit(X_train, y_train)\ndt_pred = dt_clf.predict(X_test)\nprint('DecisionTreeClassifier \uc815\ud655\ub3c4: {0:.4f}'.format(accuracy_score(y_test, dt_pred)))\n\nrf_clf.fit(X_train, y_train)\nrf_pred = rf_clf.predict(X_test)\nprint('RandomForestClassifier \uc815\ud655\ub3c4: {0:.4f}'.format(accuracy_score(y_test, rf_pred)))\n\nlr_clf.fit(X_train, y_train)\nlr_pred = lr_clf.predict(X_test)\nprint('LogisticRegression \uc815\ud655\ub3c4: {0:.4f}'.format(accuracy_score(y_test, lr_pred)))","89f68770":"from sklearn.model_selection import KFold\n\ndef exec_kfold(clf, folds=5):\n    kfold = KFold(n_splits = folds)\n    scores = []\n    \n    for iter_count, (train_index, test_index) in enumerate(kfold.split(X_titanic_df)):\n        X_train, X_test = X_titanic_df.values[train_index], X_titanic_df.values[test_index]\n        y_train, y_test = y_titanic_df.values[train_index], y_titanic_df.values[test_index]\n        \n        clf.fit(X_train, y_train)\n        predictions = clf.predict(X_test)\n        accuracy = accuracy_score(y_test, predictions)\n        scores.append(accuracy)\n        print(\"KFold {0} \uc815\ud655\ub3c4: {1:.4f}\".format(iter_count, accuracy))\n        \n    mean_score = np.mean(scores)\n    print(\"\ud3c9\uade0 \uc815\ud655\ub3c4: {0:.4f}\".format(mean_score))\n    \nexec_kfold(dt_clf, folds=5)\n","79d84b82":"from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(dt_clf, X_titanic_df, y_titanic_df, cv=5)\nfor iter_count, accuracy in enumerate(scores):\n    print(\"\uad50\ucc28 \uac80\uc99d {0} \uc815\ud655\ub3c4: {1:.4f}\".format(iter_count, accuracy))\n    \nprint(\"\ud3c9\uade0 \uc815\ud655\ub3c4: {0:.4f}\".format(np.mean(scores)))","175f1213":"from sklearn.model_selection import GridSearchCV\n\nparameters = {'max_depth': [2,3,5,10],\n             'min_samples_split': [2,3,5], 'min_samples_leaf':[1,5,8]}\n\ngrid_dclf = GridSearchCV(dt_clf, param_grid=parameters, scoring='accuracy', cv=5)\ngrid_dclf.fit(X_train, y_train)\n\nprint('GridSearchCV \ucd5c\uc801 \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130 :', grid_dclf.best_params_)\nprint('GridSearchCV \ucd5c\uace0 \uc815\ud655\ub3c4: {0:.4f}'.format(grid_dclf.best_score_))\nbest_dclf = grid_dclf.best_estimator_\n\ndpredictions = best_dclf.predict(X_test)\naccuracy = accuracy_score(y_test, dpredictions)\nprint('\ud14c\uc2a4\ud2b8 \uc138\ud2b8\uc5d0\uc11c\uc758 DecisionTreeClassifier \uc815\ud655\ub3c4: {0:.4f}'.format(accuracy))","417f55f2":"new_titanic = pd.read_csv('..\/input\/titanic\/test.csv')\nnew_titanic.head(3)","4f370c9d":"new_titanic.info()","396879ac":"y_new_titanic = titanic_df['Survived']\nX_new_titanic = transform_features(new_titanic)","8e26edf8":"grid_dclf = GridSearchCV(dt_clf, param_grid=parameters, scoring='accuracy', cv=5)\ngrid_dclf.fit(X_train, y_train)\n\nprediction = best_dclf.predict(X_new_titanic)","38d3d0e3":"submission = pd.DataFrame({\n        \"PassengerId\":new_titanic['PassengerId'],\n        'Survived': prediction\n    })\n\nsubmission.to_csv('submission.csv', index=False)\n\nsubmission = pd.read_csv('submission.csv')\nsubmission.head()","a5639353":"I predicted it for the first time when I started learning machine learning, but I was a little sad because the accuracy was not high.\n\nIt was an experience that made me think that I should work harder while looking at other people's notebook.\n\n-I wrote the code after reading Kwon Chul Min's Python Machine Learning Perfect Guide book.","b1e7e72d":"## Changed the string feature to numeric data.","306122a0":"# Input train data","75e1e744":"### I further evaluated the decision tree model with cross-validation.","fb6a0db9":"## The null value was changed to mean or fixed value using the fillna function.","52b40c77":"### Used GridSearchCV to find hyperparameters and measure predictive performance.","ca57c056":"### Use DecisionTree, RandomForrest, and LogicRegression to predict survivors.\n\nRandomForrest is the highest.","2ddf4aab":"# Start\n\nThis is my first time trying to create a notebook.\n\nLibrary, data preprocessing, modeling, and predicting and evaluation will take place in order.\n\nEven if it's not enough, I hope you think it's cute!","52c7b032":"### Summarizes the data preprocessing process into a transform_features() function.\n\n","e0ac1f6a":"### The analysis so far shows that Sex, Age, and PClass features are important features for the Survived feature.","ca6a3832":"### looked to see if the survivors had anything to do with gender. \n\nThe number of passengers on board was 577 men and 314 women, more men, but 74.2 percent of women survived, and 18.8 percent of men survived.","3bbca4f5":"## Check the number of rows and columns and check the null value and think about how to handle it.","7a3e5e03":"### Extract separate test data sets using train_test_split().","e9c500f6":"### looked at the probability of survival according to the room class.\n\nIn the case of women, the probability of survival is relatively low when they are in the third-class room, and in the case of men, the probability of survival in the first-class room is far higher. As a result, the higher the rating, the higher the chance of survival.","679c4da8":"### categorized the survival probabilities according to age groups.\n\n","d99a0712":"# Library","cf61d971":"### I predicted the test file using the prediction model."}}