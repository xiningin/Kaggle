{"cell_type":{"3452d4a4":"code","4c8afff0":"code","083d96d4":"code","d9e2570b":"code","4c8f5e15":"code","3a0a099a":"code","b6bdbca4":"code","6405696e":"code","d05d893a":"code","73c9595a":"code","22c8eb14":"code","f611968e":"code","af9f06eb":"code","46c2ace0":"code","cb026e39":"code","970db8f3":"code","4106d9bf":"code","99d7757b":"code","ab34ea2b":"code","c5784674":"code","695370b2":"code","657f50f3":"code","1100be5e":"code","4fa3c475":"code","6fe6a934":"code","084fcc4f":"code","b7bc1ae1":"code","937a499e":"code","657feed9":"code","eef8dc5d":"code","55fd1972":"code","2f14ec08":"code","cd49a301":"code","964b94b9":"code","14e7699b":"code","c1cccf18":"code","288d621d":"code","3e105595":"code","913f1ea1":"code","845c98c0":"code","752ee423":"code","77290601":"markdown","05df8a57":"markdown","0c73a2ca":"markdown","746017d5":"markdown","a30b68f7":"markdown","f965d40e":"markdown","db050702":"markdown","dae2408c":"markdown","2c812b78":"markdown","9f537214":"markdown","355655c1":"markdown","1853bc6e":"markdown","54ee8ea1":"markdown","d9bd03bb":"markdown","74779bfb":"markdown","ec751635":"markdown","25ed2de0":"markdown","6cfe88aa":"markdown","a6faef54":"markdown","b6daf962":"markdown","9313dfd7":"markdown","2edf36ac":"markdown","a82c4612":"markdown","f821fb97":"markdown","18be3254":"markdown"},"source":{"3452d4a4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4c8afff0":"import numpy as np\nimport pandas as pd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set()\n\n# sklearn models & tools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport lightgbm as lgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import make_scorer\n# from sklearn.mixture import GaussianMixture\n# from sklearn.preprocessing import RobustScaler\n# from sklearn.decomposition import PCA\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport time","083d96d4":"%%time\nsubmission = pd.read_csv('\/kaggle\/input\/santander-customer-transaction-prediction\/sample_submission.csv')\ntrain = pd.read_csv('\/kaggle\/input\/santander-customer-transaction-prediction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/santander-customer-transaction-prediction\/test.csv')","d9e2570b":"# check data\ntrain.head()","4c8f5e15":"test.head()","3a0a099a":"train.shape","b6bdbca4":"test.shape","6405696e":"train.info()","d05d893a":"test.info()","73c9595a":"train.isnull().sum().sum()","22c8eb14":"test.isnull().sum().sum()","f611968e":"sns.countplot(train.target)","af9f06eb":"train.loc[train.target==1].shape[0] \/ train.loc[train.target==0].shape[0]","46c2ace0":"train[\"target\"].value_counts()","cb026e39":"train.describe()","970db8f3":"test.describe()","4106d9bf":"(train.drop([\"target\", \"ID_code\"], axis=1).corr()).mean().mean()","99d7757b":"train.drop([\"target\", \"ID_code\"], axis=1).corr()","ab34ea2b":"# \ucf54\ub4dc \uadf8\ub300\ub85c \uac00\uc838\uc634\ntrain_correlations = train.drop([\"target\"], axis=1).corr()\ntrain_correlations = train_correlations.values.flatten()\ntrain_correlations = train_correlations[train_correlations != 1]\n\ntest_correlations = test.corr()\ntest_correlations = test_correlations.values.flatten()\ntest_correlations = test_correlations[test_correlations != 1]\n\nplt.figure(figsize=(20,5))\nsns.distplot(train_correlations, color=\"Red\", label=\"train\")\nsns.distplot(test_correlations, color=\"Green\", label=\"test\")\nplt.xlabel(\"Correlation values found in train (except 1)\")\nplt.ylabel(\"Density\")\nplt.title(\"Are there correlations between features?\"); \nplt.legend()","c5784674":"%%time\nparameters = {'min_samples_leaf': [20, 25]}\nforest = RandomForestClassifier(max_depth=15, n_estimators=15)\ngrid = GridSearchCV(forest, parameters, cv=3, n_jobs=-1, verbose=2, scoring=make_scorer(roc_auc_score))","695370b2":"# \ubaa8\ub378 \ud559\uc2b5\ngrid.fit(train.drop([\"target\", \"ID_code\"], axis=1).values, train.target.values)","657f50f3":"grid.best_params_","1100be5e":"grid.best_score_","4fa3c475":"# \ubaa8\ub378\uc758 \ub0b4\uc7a5 \ud568\uc218\uc778 feature_importances_\ngrid.best_estimator_.feature_importances_ \n\n# \uc5b4\ub808\uc774 \ud615\ud0dc\ub85c \ubc18\ud658 # shape : 200","6fe6a934":"n_top = 5\n\n# \ubcc0\uc218 \uc911\uc694\ub3c4\nimportances = grid.best_estimator_.feature_importances_\n\n# \ubcc0\uc218 \uc911\uc694\ub3c4 \uc0c1\uc704 5\uac1c\uc758 \uc778\ub371\uc2a4\nidx = np.argsort(importances)[::-1][0:n_top] \n\n# \ubcc0\uc218 \uc774\ub984\nfeature_names = train.drop([\"target\", \"ID_code\"], axis=1).columns.values \n\n# \ubcc0\uc218 \uc911\uc694\ub3c4 \uae30\uc900 \uc0c1\uc704 5\uac1c \ubcc0\uc218\uc758 \uc911\uc694\ub3c4 \uc2dc\uac01\ud654\nplt.figure(figsize=(20,5))\nsns.barplot(x=feature_names[idx], y=importances[idx])\nplt.title(\"What are the top important features?\")","084fcc4f":"fig, ax = plt.subplots(n_top, 2, figsize=(20, 5*n_top))\n\nfor n in range(n_top):\n    sns.distplot(train.loc[train.target==0, feature_names[idx][n]], ax=ax[n,0], color=\"Orange\", norm_hist=True)\n    sns.distplot(train.loc[train.target==1, feature_names[idx][n]], ax=ax[n,0], color=\"Red\", norm_hist=True)\n    sns.distplot(test.loc[:, feature_names[idx][n]], ax=ax[n,1], color=\"Mediumseagreen\", norm_hist=True)\n    ax[n,0].set_title(\"Train {}\".format(feature_names[idx][n]))\n    ax[n,1].set_title(\"Test {}\".format(feature_names[idx][n]))\n    ax[n,0].set_xlabel(\"\")\n    ax[n,1].set_xlabel(\"\")","b7bc1ae1":"top = train.loc[:, feature_names[idx]]\ntop.describe()","937a499e":"# scatter plot\ntop = top.join(train.target)\nsns.pairplot(top, hue=\"target\")","657feed9":"top","eef8dc5d":"train.columns.values[2:202]","55fd1972":"plt.figure(figsize=(16,6))\nfeatures = train.columns.values[2:202]\nplt.title(\"Distribution of mean values per row in the train and test set\")\nsns.distplot(train[features].mean(axis=1), color=\"green\", kde=True, bins=120, label='train')\nsns.distplot(test[features].mean(axis=1), color=\"blue\", kde=True, bins=120, label='test')\nplt.legend()\nplt.show()","2f14ec08":"%%time\noriginal_features = train.drop([\"target\", \"ID_code\"], axis=1).columns.values\n\n# 5\uac1c\uc758 \uc911\uc694 \ubcc0\uc218 qcut\ud55c \ubcc0\uc218 \uc0dd\uc131\nencoder = LabelEncoder()\nfor your_feature in top.drop(\"target\", axis=1).columns.values:\n    train[your_feature + \"_qbinned\"] = pd.qcut(\n        train.loc[:, your_feature].values,\n        q=10,\n        labels=False\n    )\n    train[your_feature + \"_qbinned\"] = encoder.fit_transform(\n        train[your_feature + \"_qbinned\"].values.reshape(-1, 1)\n    )\n    \n# 5\uac1c\uc758 \uc911\uc694 \ubcc0\uc218 \ubc18\uc62c\ub9bc\ud55c \ubcc0\uc218 \uc0dd\uc131\nencoder = LabelEncoder()\nfor your_feature in top.drop(\"target\", axis=1).columns.values:\n    train[your_feature + \"_rounded\"] = np.round(train.loc[:, your_feature].values)\n    train[your_feature + \"_rounded_10\"] = np.round(10*train.loc[:, your_feature].values)\n    train[your_feature + \"_rounded_100\"] = np.round(100*train.loc[:, your_feature].values)\n\n    \n# test\uc5d0\ub3c4 \uac19\uc774 \uc801\uc6a9\n\nencoder = LabelEncoder()\nfor your_feature in top.drop(\"target\", axis=1).columns.values:\n    test[your_feature + \"_qbinned\"] = pd.qcut(\n        test.loc[:, your_feature].values,\n        q=10,\n        labels=False\n    )\n    test[your_feature + \"_qbinned\"] = encoder.fit_transform(\n        test[your_feature + \"_qbinned\"].values.reshape(-1, 1)\n    )\n\nencoder = LabelEncoder()\nfor your_feature in top.drop(\"target\", axis=1).columns.values:\n    test[your_feature + \"_rounded\"] = np.round(test.loc[:, your_feature].values)\n    test[your_feature + \"_rounded_10\"] = np.round(10*test.loc[:, your_feature].values)\n    test[your_feature + \"_rounded_100\"] = np.round(100*test.loc[:, your_feature].values)","cd49a301":"train.head()","964b94b9":"train.columns.values[2:202]","14e7699b":"%%time\nidx = features = train.columns.values[2:202] # \uc6d0\ub798 \ud53c\ucc98(var_0 ~ var_199)\nfor df in [test, train]:\n    df['sum'] = df[idx].sum(axis=1)  \n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['mean'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)\n    df['skew'] = df[idx].skew(axis=1)\n    df['kurt'] = df[idx].kurtosis(axis=1)\n    df['med'] = df[idx].median(axis=1)","c1cccf18":"# \ucd94\uac00\ud55c \ubcc0\uc218 \ud655\uc778\ntrain[train.columns[222:]].head()","288d621d":"test[test.columns[222:]].head()","3e105595":"train.shape","913f1ea1":"features = [c for c in train.columns if c not in ['ID_code', 'target']]\ntarget = train['target']","845c98c0":"# \ud30c\ub77c\ubbf8\ud130\nparams = {'objective' : \"binary\",  \n               'boost':\"gbdt\", # gbdt : Gradient Boosting Desicion Tree # \uc2e4\ud589\ud558\uace0\uc790 \ud558\ub294 \uc54c\uace0\ub9ac\uc998 \ud0c0\uc785 \uc815\uc758\n               'metric':\"auc\", # \uc131\ub2a5\ud3c9\uac00 \uc9c0\ud45c\n               'boost_from_average':\"false\",\n               'num_threads':8,\n               'learning_rate' : 0.01, # \ucd5c\uc885 \uacb0\uacfc\uc5d0 \ub300\ud55c \uac01\uac01\uc758 Tree\uc5d0 \uc601\ud5a5\uc744 \ubbf8\uce58\ub294 \ubcc0\uc218\n               'num_leaves' : 13, # 10, 13, 15 # \uc804\uccb4 Tree\uc758 leave \uc218. Tree \ubaa8\ub378\uc758 \ubcf5\uc7a1\uc131\uc744 \ucee8\ud2b8\ub864\ud558\ub294 \uc8fc\uc694 \ud30c\ub77c\ubbf8\ud130. # \ub514\ud3f4\ud2b8 31\n               'max_depth':-1,  # tree\uc758 \ucd5c\ub300 \uae4a\uc774 # 0\ubcf4\ub2e4 \uc791\uc740 \uac12\uc740 \uae4a\uc774\uc5d0 \uc81c\ud55c\uc774 \uc5c6\uc74c # \ub514\ud3f4\ud2b8 -1\n               'tree_learner' : \"serial\",\n               'feature_fraction' : 0.05, # \ubaa8\ub378\uc774 tree\ub97c \ub9cc\ub4e4 \ub54c \ub9e4\ubc88 \uac01\uac01\uc758 iteration\uc5d0\uc11c \ud30c\ub77c\ubbf8\ud130 \uc911 5%\ub97c \ub79c\ub364\ud558\uac8c \uc120\ud0dd\n               'bagging_freq' : 5,\n               'bagging_fraction' : 0.4, # \ub9e4\ubc88 iteration\uc744 \ub3cc \ub54c \uc0ac\uc6a9\ub418\ub294 \ub370\uc774\ud130\uc758 \uc77c\ubd80\ub97c \uc120\ud0dd\ud558\ub294\ub370 \ud2b8\ub808\uc774\ub2dd \uc18d\ub3c4\ub97c \ub192\uc774\uace0 \uacfc\uc801\ud569\uc744 \ubc29\uc9c0\ud560 \ub54c \uc8fc\ub85c \uc0ac\uc6a9\n               'min_data_in_leaf' : 80, # 70, 80, 90 # Leaf\uac00 \uac00\uc9c0\uace0 \uc788\ub294 \ucd5c\uc18c\ud55c\uc758 \ub808\ucf54\ub4dc \uc218, \uacfc\uc801\ud569 \ud574\uacb0\uc5d0 \uc0ac\uc6a9, \ub514\ud3f4\ud2b8 20(\ucd5c\uc801\uac12)\n               'min_sum_hessian_in_leaf' : 10.0,\n               'verbosity' : 1}\n\n# \uc77c\ubc18\uc801\uc73c\ub85c n_estimators\ub97c \ud06c\uac8c \ud558\uace0 learning_rate\ub97c \uc791\uac8c \ud574\uc11c \uc608\uce21 \uc131\ub2a5\uc744 \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\uc73c\ub098, \uacfc\uc801\ud569 \uc774\uc288\uc640 \ud559\uc2b5\uc2dc\uac04\uc774 \uae38\uc5b4\uc9c0\ub294 \ubd80\uc815\uc801\uc778 \uc601\ud5a5\ub3c4 \uace0\ub824\ud574\uc57c \ud568","752ee423":"%%time\n\nfolds = StratifiedKFold(n_splits=5, shuffle=False, random_state=44000)\noof = np.zeros(len(train))\npredictions = np.zeros(len(test))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train.values, target.values)):\n    print(\"Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx])\n\n    num_round = 1000000 # \ub2e4\ud3f4\ud2b8 : 100\n    clf = lgb.train(params, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 3000)\n    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    # early_stopping_rounds : The index of iteration that has the best performance will be saved in the best_iteration field if early stopping logic is enabled by setting early_stopping_rounds\n    # verbose_eval : If int, the eval metric on the valid set is printed at every verbose_eval boosting stage. \n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(test[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))","77290601":"### Check null data","05df8a57":"### 2)\n#### row\ubcc4 \uc9d1\uacc4\uac12 : sum, min, max \ub4f1 \n- \uae30\uc874 \ud53c\ucc98\uc5d0 \ub300\ud574 \uba87 \uac1c\uc758 \uc9d1\uacc4\uac12\uc744 \uacc4\uc0b0\ud574\uc11c \uc0c8\ub85c\uc6b4 \ud53c\ucc98\ub85c \ucd94\uac00\n- \uc65c?","0c73a2ca":"#### feature correlation","746017d5":"### target data ","a30b68f7":"## Modeling","f965d40e":"#### class imbalanced\n\n- We have to solve the problem about imbalanced class.\n- \uac70\ub798\ub97c \ud560 \uace0\uac1d\uc774 \uac70\ub798\ub97c \ud558\uc9c0 \uc54a\uc744 \uace0\uac1d\ubcf4\ub2e4 \ud6e8\uc529 \uc801\ub2e4.\n- There are far fewer customers who will transaction than those who will not.","db050702":"There is no missing data in both train and test datasets.","dae2408c":"\ub85c\uc6b0\ubcc4 \ud3c9\uade0\uac12\uc758 \ubd84\ud3ec \ud655\uc778(train, test)","2c812b78":"## Import Library","9f537214":"- train, test \ub370\uc774\ud130\uc758 \uc9d1\uacc4\uac12\uc774 \ube44\uc2b7\ud574 \ubcf4\uc778\ub2e4.\n- The aggregated values of train and test data look similar.\n\n- \ud53c\ucc98 \uac04 \uc9d1\uacc4\uac12\ub4e4\uc740 \uc870\uae08 \ucc28\uc774\uac00 \uc788\uc5b4 \ubcf4\uc778\ub2e4.\n- There seems to be a difference in the aggregated values between features.","355655c1":"#### Hyper Parameter Tunning\n- \uc131\ub2a5 \ub192\uc774\uace0 \uacfc\uc801\ud569\uc744 \uc904\uc774\ub294 \ubc29\ud5a5\uc73c\ub85c \uc8fc\uc694 \ud30c\ub77c\ubbf8\ud130\ub97c \ud29c\ub2dd\ud588\ub2e4.\n- learning_rate : 0.01\ub85c \ub0ae\uc744 \ub54c \uc131\ub2a5\uc774 \uc88b\uc558\ub2e4.\n- num_leaves : 10, 13, 15\n- min_data_in_leaf : 70, 80, 90\n\n#### kfold\n- class imbalanced \ubb38\uc81c\ub97c \uace0\ub824\ud574\uc11c StratifiedKFold \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud588\ub2e4.\n\n#### LightGBM\n- Dataset\uc774 \ud53c\ucc98\uc758 \uc218\uac00 \ub9ce\uace0, \ubaa8\ub450 \uc218\uce58\ud615 \ub370\uc774\ud130 \ud0c0\uc785\uc73c\ub85c \uc774\ub8e8\uc5b4\uc838 \uc788\ub2e4.\n- \ub370\uc774\ud130\uc758 \uc218\uac00 LightGBM\uc744 \uc0ac\uc6a9\ud558\uae30\uc5d0 \uc801\ud569\ud558\ub2e4.(\uacfc\uc801\ud569x)","1853bc6e":"### \ubaa8\ub378\ub9c1\uc758 \ubaa9\uc801 = \ubd84\uc11d \ubaa9\uc801\n- Purpose of modeling = Purpose of analysis\n\n- train dataset\uc758 \uc775\uba85\uc758 \uc218\uce58\ud615 \ub370\uc774\ud130\ub4e4\uc744 \ud1b5\ud574 test data\uc758 target\uc774 0\uc77c\uc9c0 1\uc77c\uc9c0 \uc608\uce21\ud558\ub294 \ubd84\ub958\ubb38\uc81c\n- Classification problem predicting whether the target of test dataset is 0 or 1 through anonymous numerical data of train dataset.\n \n \n- \uc911\uc694\ud55c \ud3ec\uc778\ud2b8\ub294 \ud53c\ucc98\uc758 \uc815\ubcf4\ub294 \uc624\uc9c1 \uc218\uce58\ub85c\ub9cc \ud655\uc778 \uac00\ub2a5\ud558\ub2e4\ub294 \uc810\uc774\ub2e4. \uc9c1\uad00\uc801\uc73c\ub85c \ud53c\ucc98 \uac04\uc758 \uad00\uacc4\ub97c \ud30c\uc545\ud558\uae30 \ud798\ub4e4\ub2e4.\n- The important point is that feature information can only be checked with numbers. It is difficult to intuitively grasp the relationship between features.\n\n\n- \uc774\ub7ec\ud55c \ub370\uc774\ud130\uc14b\uacfc \ubd84\uc11d \ubaa9\uc801\uc744 \uac00\uc9c8 \ub54c \uc5b4\ub5a4 \ubaa8\ub378\ub9c1\uc744 \ud574\uc57c \ud560\uae4c? \uadf8\ub9ac\uace0 \uc774 \ubaa8\ub378\ub9c1\uc758 \ubaa9\uc801\uc5d0 \ub9de\ub294 EDA\ub294 \uc5b4\ub5bb\uac8c \uc9c4\ud589\ud558\uba74 \uc88b\uc744\uae4c?\n- What modeling should we do when we have these datasets and analysis purposes? And how should we proceed with the EDA that fits the purpose of this modeling?\n\n\n- **\uc9c0\uc218\ub2d8\uc774 \uc124\uba85\ud574\uc8fc\uc2e0 lightgbm \ub3d9\uc791\uc6d0\ub9ac\uc640 \uad00\ub828\ud574\uc11c \uc124\uba85\uc744 \uc801\uc73c\uba74 \uc88b\uc744 \uac83 \uac19\uc2b5\ub2c8\ub2e4.**","54ee8ea1":"- train, test\uc758 \ubd84\ud3ec\ub294 \uc644\uc804 \ube44\uc2b7\ud568","d9bd03bb":"EDA\ub97c \ud1b5\ud574 \ud53c\ucc98\ub4e4\uc758 \ud788\uc2a4\ud1a0\uadf8\ub7a8\uc744 \uc0b4\ud3b4\ubcfc \ub54c 200\uac1c\uc758 \ubaa8\ub4e0 \ubcc0\uc218\ub97c \ud655\uc778\ud558\ub294 \uac83\ubcf4\ub2e4 \uba87 \uac1c\ub97c \ubf51\uc544\uc11c \ud655\uc778\ud558\ub294 \uac83\uc774 \ub354 \ud6a8\uc728\uc801\uc774\ub77c\uace0 \uc0dd\uac01.\n\n- \uadf8\ub0e5 \ubf51\ub294 \uac83\ubcf4\ub2e4 \uc911\uc694 \ubcc0\uc218 \uc120\ud0dd \ubc29\ubc95\uc744 \uc774\uc6a9\ud574\uc11c \ubf51\ub294 \uac83\uc774 \ub354 \ud6a8\uacfc\uc801\uc774\ub77c\uace0 \uc0dd\uac01.\n- \ubaa8\ub4e0 \ubcc0\uc218\ub4e4\uc774 \uc120\ud615 \uc0c1\uad00\uad00\uacc4\ub97c \uac00\uc9c0\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc5d0 nonlinear model\uc744 \uc0ac\uc6a9\ud574\uc11c \uc911\uc694 \ubcc0\uc218\ub97c \ubf51\uc544\ubcf4\uc790.\n- random forest\ub97c \uc0ac\uc6a9\ud558\uace0, \ubaa8\ub378\uc758 \ub0b4\uc7a5\ud568\uc218\uc778 feature_importances_\ub97c \uc0ac\uc6a9\ud574\uc11c \uc911\uc694 \ubcc0\uc218\ub97c \ubf51\uc744 \uac83\uc774\ub2e4.","74779bfb":"- target=1\uc778 \ub370\uc774\ud130\ub4e4\uc774 \uac11\uc790\uae30 \ub204\uc801\ub418\uc5b4\uc11c \uac70\uc758 \ub118\uc5b4\uac00\uc9c0 \uc54a\ub294 \ub0a0\uce74\ub85c\uc6b4 \uc9c0\uc810\uc774 \uc788\ub2e4.\n- \uc608\ub97c \ub4e4\uc5b4, 81\ubc88 \ubcc0\uc218\uc5d0\uc11c\ub294 10\uc5d0, 12\ubc88 \ubcc0\uc218\uc5d0\uc11c\ub294 13.5\uc5d0 \ub370\uc774\ud130\uac00 \ub204\uc801\ub418\uc5b4 \ub0a0\uce74\ub86d\uac8c \uc19f\uc740 \uc9c0\uc810\uc774 \uc788\ub2e4.(limit)","ec751635":"Q. \uc911\uc694 \ubcc0\uc218\ub97c \uba87\uac1c\ub97c \ubf51\uc544\uc11c \ud0d0\uc0c9\ud558\ub294 \uac83\uc774 \uc804\uccb4 EDA\uc5d0 \uc5b4\ub5a4 \ub3c4\uc6c0\uc774 \ub418\ub294\uc9c0?","25ed2de0":"#### train\n- ID_code : (object) string data type\n- target : int => target variable\n- var_0 ~ var_199 : (float) 200 numerical variables  \n\n#### test\n- ID_code : (object) string data type\n- var_0 ~ var_199 : (float) 200 numerical variables\n\n#### Except for ID_code, all are numerical variables.","6cfe88aa":"- describe() \ud568\uc218\uc5d0\uc11c\ub3c4 \ud655\uc778\ud588\ub4ef\uc774, train\uacfc test\uc758 \ubd84\ud3ec\uac00 \ube44\uc2b7\ud558\ub2e4\n- train\uc5d0\uc11c target=0, 1\uc758 pdf\uac00 \ub300\uccb4\ub85c \ube44\uc2b7\ud55c\ub370, \uc644\uc804 \ub2e4\ub974\uac8c target=1\uc5d0\uc11c \uac11\uc790\uae30 \uc19f\uc544\uc624\ub978 \uc9c0\uc810\uc774 \uc788\ub2e4.(\uac11\uc790\uae30 \ub204\uc801\ub41c \ubd80\ubd84)\n\n\n- significant different distribution for the two target values.\n- target=1\uacfc target=0\uc758 \ubd84\ud3ec \ubaa8\uc591\uc774 \ub2ec\ub77c\uc57c \ud655\uc2e4\ud558\uac8c \uad6c\ubd84 \uac00\ub2a5\ud558\uace0, \uc774\ub7ec\ud55c \ubcc0\uc218\ub97c \uc0ac\uc6a9\ud574\uc57c \uc608\uce21 \uc815\ud655\ub3c4\ub97c \ub192\uc77c \uc218 \uc788\ub2e4.\n- \uc120\ud0dd\ub41c \ubcc0\uc218\ub4e4\uc740 target\uc5d0 \ub530\ub77c \ub370\uc774\ud130\uc758 \ubd84\ud3ec\uac00 \ub2e4\ub978 \ubcc0\uc218\ub4e4\uc774\ub2e4.\n\n- train\uc758 target=1\uc778 \ub370\uc774\ud130\ub4e4\uc740 ","a6faef54":"#### \uc911\uc694 \ubcc0\uc218 \ubf51\uae30","b6daf962":"## feature engineering\n\n- \uae30\uc874 \ubcc0\uc218\ub97c \uae30\ubc18\uc73c\ub85c \uc0c8\ub85c\uc6b4 \ubcc0\uc218 \uc0dd\uc131\n- \uae30\uc874 \ubcc0\uc218\uc758 \ud2b9\uc131\uc744 \ubc18\uc601\ud558\ub294(\uac00\uc9c0\uace0 \uc788\ub294) \ubcc0\uc218 \uc0dd\uc131\n- target=1, 0\uc744 \uad6c\ubd84\ud558\ub294\ub370 \ub354\uc6b1 \ub3c4\uc6c0\uc774 \ub418\ub294 \ubcc0\uc218 \n","9313dfd7":"### 1)\n#### \ubc18\uc62c\ub9bc \ubc0f \ubd84\uc704\uc218 \uae30\ubc18 binning : label encoding\n\n1. \ubc18\uc62c\ub9bc : np.round\n    - \uadf8\ub0e5 \ubc18\uc62c\ub9bc\ud55c \uac83, \ubc18\uc62c\ub9bcx10, \ubc18\uc62c\ub9bcx100\n\n\n2. \ubd84\uc704\uc218 \uae30\ubc18 binning : pd.qcut\n    - \uc55e\uc11c \ubf51\uc740 5\uac1c\uc758 \uc911\uc694 \ubcc0\uc218\uc758 \ub370\uc774\ud130(\uc218\uce58\ud615)\uc758 \uc815\ud655\ud55c \uac12 \ub9d0\uace0 \ub370\uc774\ud130\ub4e4\uc758 \ubc94\uc704(\ubc94\uc8fc)\ub97c \uc54c\uae30 \uc704\ud574 (values range than actual values)\n    - qcut\ud568\uc218\ub97c \uc774\uc6a9\ud574\uc11c \ubd84\uc704\uc218 \uae30\ubc18\uc758 bin\uc744 \ub9cc\ub4e4\uace0, LabelEncoder\ub85c \ub77c\ubca8\ub9c1\ud558\uc5ec \uc0c8\ub85c\uc6b4 \ubcc0\uc218 \uc0dd\uc131\n    - We will use qcut to create 10 equally sized bins i.e quartiles\n    - qcut function can be used to generate equally sized quantiles bins for your data\n\n===============================================================================\n\n- Histograms are example of data binning that helps to visualize your data distribution in equal intervals\n\n- qcut : bin\uc744 \ub9cc\ub4e4\uae30 \uc704\ud55c \ubd84\uc704\uc218 \uae30\ubc18\uc758 \ud568\uc218\n- qcut is a quantile based function to create bins\n\n- Quantile is to divide the data into equal number of subgroups or probability distributions of equal probability into continuous interval\n- Quantile\uc740 \ub370\uc774\ud130\ub97c \ub3d9\uc77c\ud55c \uc218\uc758 subgroups\uc73c\ub85c \ub098\ub204\uac70\ub098 \uac19\uc740 \ud655\ub960\uc758 \ud655\ub960 \ubd84\ud3ec\ub97c \uc5f0\uc18d \uad6c\uac04\uc73c\ub85c \ub098\ub204\ub294 \uac83\uc785\ub2c8\ub2e4.","2edf36ac":"## EDA\n- \ub370\uc774\ud130 \ud30c\uc545\uc744 \uc704\ud574 \ub370\uc774\ud130\ub97c \uc0c5\uc0c5\uc774 \ud0d0\uc0c9\ud574\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.\n- We will explore the data to understand the data.\n\n- \ubaa8\ub378\ub9c1\uc758 \ubaa9\uc801\uc5d0 \ub9de\ub294 EDA\ub97c \uc9c4\ud589\ud574\uc11c \uc5b4\ub5a4 \uc2dd\uc73c\ub85c \ubaa8\ub378\ub9c1\uc744 \ud574\uc57c \ud560 \uc9c0 \uc815\ud574\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.","a82c4612":"## Load Data","f821fb97":"\ubaa8\ub4e0 \ubcc0\uc218\uac00 \uc120\ud615 \uc0c1\uad00\uad00\uacc4\ub97c \uac00\uc9c0\uc9c0 \uc54a\ub294 \uac83\uc73c\ub85c \ubcf4\uc778\ub2e4.","18be3254":"- \uc5ec\uae30\ub294 \uc9d1\uacc4\uac12 \ud655\uc778\uc744 \uc704\ud55c EDA\uc778\ub370, \ub123\uc744\uc9c0 \ub9d0\uc9c0 \uace0\ubbfc\ub428.."}}