{"cell_type":{"bdb85a73":"code","378e2488":"code","4339632d":"code","0a58eaf5":"code","2e92e8a1":"code","4553e145":"code","077185d6":"code","975de19a":"code","53f55909":"code","9e37b56e":"code","44d87709":"code","07cf49ae":"code","a2e1c65f":"code","bc97388e":"code","201ce584":"code","97cc1d47":"code","64474422":"code","d8e04665":"code","62a8ae33":"markdown"},"source":{"bdb85a73":"!wget -q https:\/\/storage.googleapis.com\/bert_models\/2018_10_18\/uncased_L-12_H-768_A-12.zip\n!wget -q https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/modeling.py \n!wget -q https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/optimization.py \n!wget -q https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/run_classifier.py \n!wget -q https:\/\/raw.githubusercontent.com\/google-research\/bert\/master\/tokenization.py ","378e2488":"import os\nimport numpy as np\nimport pandas as pd\nimport datetime\nimport sys\nimport zipfile\nimport modeling\nimport optimization\nimport run_classifier\nimport tokenization\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow as tf","4339632d":"folder = 'model_folder'\nwith zipfile.ZipFile(\"uncased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n    zip_ref.extractall(folder)","0a58eaf5":"BERT_MODEL = 'uncased_L-12_H-768_A-12'\nBERT_PRETRAINED_DIR = f'{folder}\/uncased_L-12_H-768_A-12'\nOUTPUT_DIR = f'{folder}\/outputs'\nprint(f'>> Model output directory: {OUTPUT_DIR}')\nprint(f'>>  BERT pretrained directory: {BERT_PRETRAINED_DIR}')","2e92e8a1":"train_df = pd.read_csv('..\/input\/train.csv', index_col='id').fillna(' ')\nval_df = pd.read_csv('..\/input\/valid.csv', index_col='id').fillna(' ')\ntest_df = pd.read_csv('..\/input\/test.csv', index_col='id').fillna(' ')","4553e145":"train_df['title_text'] = 'title: ' + train_df['title'] + ' text: '+ train_df['text']\nval_df['title_text'] = 'title: ' + val_df['title'] + ' text: '+ val_df['text']\ntest_df['title_text'] = 'title: ' + test_df['title'] + ' text: '+ test_df['text']","077185d6":"label_encoder = LabelEncoder().fit(pd.concat([train_df['label'], val_df['label']]))","975de19a":"X_train, X_test = pd.concat([train_df['title_text'], val_df['title_text']]).values, test_df['title_text'].values","53f55909":"y_train = label_encoder.fit_transform(pd.concat([train_df['label'], val_df['label']]))","9e37b56e":"X_train.shape, X_test.shape","44d87709":"def create_examples(lines, set_type, labels=None):\n    # Generate data for the BERT model\n    guid = f'{set_type}'\n    examples = []\n    if guid == 'train':\n        for line, label in zip(lines, labels):\n            text_a = line\n            label = str(label)\n            examples.append(\n              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    else:\n        for line in lines:\n            text_a = line\n            label = '0'\n            examples.append(\n              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    return examples","07cf49ae":"# Model Hyper Parameters\nTRAIN_BATCH_SIZE = 32\nEVAL_BATCH_SIZE = 8\nLEARNING_RATE = 1e-5\nNUM_TRAIN_EPOCHS = 3.0\nWARMUP_PROPORTION = 0.1\nMAX_SEQ_LENGTH = 128\nNUM_CLASSES = train_df['label'].nunique()\n# Model configs\nSAVE_CHECKPOINTS_STEPS = 100000 #if you wish to finetune a model on a larger dataset, use larger interval\n# each checpoint weights about 1,5gb\nITERATIONS_PER_LOOP = 100000\nNUM_TPU_CORES = 8\nVOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\nCONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\nINIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\nDO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n\nlabel_list = [str(num) for num in range(NUM_CLASSES)]\ntokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)\ntrain_examples = create_examples(X_train, 'train', labels=y_train)\n\ntpu_cluster_resolver = None #Since training will happen on GPU, we won't need a cluster resolver\n#TPUEstimator also supports training on CPU and GPU. You don't need to define a separate tf.estimator.Estimator.\nrun_config = tf.contrib.tpu.RunConfig(\n    cluster=tpu_cluster_resolver,\n    model_dir=OUTPUT_DIR,\n    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n    tpu_config=tf.contrib.tpu.TPUConfig(\n        iterations_per_loop=ITERATIONS_PER_LOOP,\n        num_shards=NUM_TPU_CORES,\n        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n\nnum_train_steps = int(\n    len(train_examples) \/ TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\nnum_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)","a2e1c65f":"model_fn = run_classifier.model_fn_builder(\n    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n    num_labels=len(label_list),\n    init_checkpoint=INIT_CHECKPOINT,\n    learning_rate=LEARNING_RATE,\n    num_train_steps=num_train_steps,\n    num_warmup_steps=num_warmup_steps,\n    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available  \n    use_one_hot_embeddings=True)\n\nestimator = tf.contrib.tpu.TPUEstimator(\n    use_tpu=False, #If False training will fall on CPU or GPU, depending on what is available \n    model_fn=model_fn,\n    config=run_config,\n    train_batch_size=TRAIN_BATCH_SIZE,\n    eval_batch_size=EVAL_BATCH_SIZE)","bc97388e":"print('Please wait...')\ntrain_features = run_classifier.convert_examples_to_features(\n    train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\nprint('>> Started training at {} '.format(datetime.datetime.now()))\nprint('  Num examples = {}'.format(len(train_examples)))\nprint('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\ntf.logging.info(\"  Num steps = %d\", num_train_steps)\ntrain_input_fn = run_classifier.input_fn_builder(\n    features=train_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=True,\n    drop_remainder=True)\nestimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\nprint('>> Finished training at {}'.format(datetime.datetime.now()))","201ce584":"def input_fn_builder(features, seq_length, is_training, drop_remainder):\n    \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n\n    all_input_ids = []\n    all_input_mask = []\n    all_segment_ids = []\n    all_label_ids = []\n\n    for feature in features:\n        all_input_ids.append(feature.input_ids)\n        all_input_mask.append(feature.input_mask)\n        all_segment_ids.append(feature.segment_ids)\n        all_label_ids.append(feature.label_id)\n\n    def input_fn(params):\n        \"\"\"The actual input function.\"\"\"\n        print(params)\n        batch_size = 500\n\n        num_examples = len(features)\n\n        d = tf.data.Dataset.from_tensor_slices({\n            \"input_ids\":\n                tf.constant(\n                    all_input_ids, shape=[num_examples, seq_length],\n                    dtype=tf.int32),\n            \"input_mask\":\n                tf.constant(\n                    all_input_mask,\n                    shape=[num_examples, seq_length],\n                    dtype=tf.int32),\n            \"segment_ids\":\n                tf.constant(\n                    all_segment_ids,\n                    shape=[num_examples, seq_length],\n                    dtype=tf.int32),\n            \"label_ids\":\n                tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n        })\n\n        if is_training:\n            d = d.repeat()\n            d = d.shuffle(buffer_size=100)\n\n        d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n        return d\n\n    return input_fn","97cc1d47":"predict_examples = create_examples(X_test, 'test')\n\npredict_features = run_classifier.convert_examples_to_features(\n    predict_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n\npredict_input_fn = input_fn_builder(\n    features=predict_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=False,\n    drop_remainder=False)\n\nresult = estimator.predict(input_fn=predict_input_fn)","64474422":"preds = [np.argmax(prediction['probabilities']) for prediction in result]","d8e04665":"pd.DataFrame(label_encoder.inverse_transform(preds), \n             columns=['label']).to_csv('bert_starter_submission.csv',\n                                       index_label='id')","62a8ae33":"Based on [this](https:\/\/www.kaggle.com\/thebrownviking20\/bert-multiclass-classification) Kernel, the code is taken almost as is. Validation is done in another kernel. "}}