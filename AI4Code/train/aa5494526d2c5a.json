{"cell_type":{"769c0b0b":"code","f9235c5a":"code","fb39ca73":"code","c8dabdc1":"code","384ca15b":"code","8fbb3460":"code","3d66ebb1":"code","1dbccd25":"code","bd4415f3":"code","66f8ba63":"code","68051ac9":"code","3d1d111f":"code","9a2bdeb6":"code","68f2dac6":"code","31340cc9":"code","b1a3a0cd":"code","a090e0a7":"code","02398be6":"code","fc6ed7b9":"code","5ebb2930":"code","8460d263":"code","d5af1f85":"code","9a57e575":"markdown","95f2bd6d":"markdown","e268bfb4":"markdown","dbf248d1":"markdown","99739517":"markdown","02a606fd":"markdown","a6f8350e":"markdown","537b9428":"markdown","30298e86":"markdown","77d8d4cb":"markdown","b43742b9":"markdown","48f787cf":"markdown","8e6bd271":"markdown","e7440bad":"markdown","6c9b5591":"markdown","b20ac91d":"markdown","82eb16c1":"markdown","1f91b3ae":"markdown"},"source":{"769c0b0b":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib as plt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\nimport imblearn\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score","f9235c5a":"# Importing the dataset and drop'id' and 'unnamed' columns as they are irrelevent.\ndf = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv').drop(columns=['id','Unnamed: 32'])\n\n# Subsets for predictors and target variable.\nX =  df.drop(columns=['diagnosis'])\ny = df['diagnosis']","fb39ca73":"# Summary Statistics\ndf.describe()","c8dabdc1":"# All independent variables are continous data type.\ndf.info()","384ca15b":"# Lets create a barchart of the response variable\nsns.set_style('ticks')\nsns.set_palette('Set1')\nsns.countplot(data=df, x=\"diagnosis\", order=[\"M\", \"B\"], palette='Set1')\nplt.xlabel('Diagnosis')\nplt.ylabel('Frequency')\nplt.ylim([0, 500])\nplt.show()\n\n\n# Proportion of Malignant Vs Benign \nx = df['diagnosis'].value_counts(normalize=True)\nprint(\"Percentage of Benign Observations: \", str(x['B'].round(3) * 100), '%')\nprint(\"Percentage of Malignant Observations:\", str(x['M'].round(3) * 100), '%')","8fbb3460":"# Kernal Density plots to see the distribution of independent variables.\nvariables = list(df.columns)\nsns.set(style=\"white\") \nplt.figure(figsize = (20 , 60))\nindependent = variables[1:]\nfor variable in range(30):\n    plt.subplot(15,3 , variable + 1 )\n    sns.kdeplot(df[independent[variable]], shade = True, color=\"olive\")\nplt.show()","3d66ebb1":"# Barcharts representing malignant and benign cancer cells against independent variables.\nsns.set(style=\"white\") \nplt.figure(figsize = (20 , 60))\nfor variable in range(30):\n    plt.subplot(15, 3 , variable + 1)\n    sns.barplot(x = df['diagnosis'], y =df[independent[variable]],  palette='Set1' )\nplt.show()","1dbccd25":"# Boxplots representing malignant and benign cancer cells against independent variables.\nsns.set(style=\"white\") \nplt.figure(figsize = (20 , 60))\nfor variable in range(30):\n    plt.subplot(15, 3 , variable + 1)\n    sns.boxplot(x = df['diagnosis'], y =df[independent[variable]],  palette='Set1' )\nplt.show()","bd4415f3":"# Let's produce a pearsons correlation correlogram to detect multicollinearity\ncorrelated_var = X.corr()\nplt.figure(figsize = (25 , 25))\ntriangle = np.triu(correlated_var)\ncolormap = sns.color_palette(\"Greens\")\nsns.heatmap(correlated_var, annot = True, center = 0, linecolor = 'black', mask = triangle,fmt='.2f', cmap = colormap)","66f8ba63":"# Removing highly correlated features\nvar = ['perimeter_mean',\n       'radius_mean',\n       'radius_worst',\n       'texture_mean',\n       'radius_se',\n       'area_se',\n       'concave points_mean']\n\nX = X.drop(var, axis = 1)\nX = np.array(X)","68051ac9":"# Encode 0 and 1 to represent benign and malignant cancer cells  \ndict_map =     {'M': 1, \n              'B': 0}\n\ny = y.map(dict_map)","3d1d111f":"# Splitting the dataset into the Training set and Test set at a ratio of 80% to 20%\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True, stratify = None)","9a2bdeb6":"# Performing Feature Scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","68f2dac6":"# Performing oversampling on training set using SMOTE\ncounter = Counter(y_train)\nprint(counter)\n\n# Synthetic Minority Oversampling Technique\nX_train, y_train = SMOTE().fit_resample(X_train, y_train)\nCounter(y_train).items()","31340cc9":"# Training the Logistic Regression model on the Training set\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, y_train)\n\n# Training\nclassifier.fit(X_train, y_train)\n# Extract predictions\ny_pred = classifier.predict(X_test)\n\n# k-Fold Cross Validation\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nlr = accuracies.mean()*100\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\nprint('\\n', classification_report(y_test, y_pred))","b1a3a0cd":"# Training the Decision Tree Classification model on the Training set\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)\n\n# Training\nclassifier.fit(X_train, y_train)\n# Extract predictions\ny_pred = classifier.predict(X_test)\n\n# k-Fold Cross Validation\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\ndt = accuracies.mean()*100\nprint(\"\\nCV Average Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\nprint('\\n', classification_report(y_test, y_pred))\n\n","a090e0a7":"# Training the K-Nearest Neighbour model on the Training set\nclassifier = KNeighborsClassifier(n_neighbors = 20, metric = 'minkowski', p = 2)\nclassifier.fit(X_train, y_train)\n\n# Training\nclassifier.fit(X_train, y_train)\n# Extract predictions\ny_pred = classifier.predict(X_test)\n\n# k-Fold Cross Validation\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nknn = accuracies.mean()*100\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\nprint('\\n', classification_report(y_test, y_pred))","02398be6":"# Training the Random Forest Classification model on the Training set\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n\n# Training\nclassifier.fit(X_train, y_train)\n\n# Extract predictions\ny_pred = classifier.predict(X_test)\n\n# k-Fold Cross Validation\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nrf = accuracies.mean()*100\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\nprint('\\n', classification_report(y_test, y_pred))","fc6ed7b9":"# Training the Naive Bayes model on the Training set\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\n# Training\nclassifier.fit(X_train, y_train)\n# Extract predictions\ny_pred = classifier.predict(X_test)\n\n# k-Fold Cross Validation\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nnb = accuracies.mean()*100\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\nprint('\\n', classification_report(y_test, y_pred))","5ebb2930":"# Training the SVM model on the Training set\nclassifier = SVC(kernel = 'rbf', random_state = 0)\nclassifier.fit(X_train, y_train)\n\n# Training\nclassifier.fit(X_train, y_train)\n# Extract predictions\ny_pred = classifier.predict(X_test)\n\n# k-Fold Cross Validation\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nsvc = accuracies.mean()*100\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\nprint('\\n', classification_report(y_test, y_pred))","8460d263":"# Training XGBoost on the Training set\nclassifier = XGBClassifier(use_label_encoder=False, eval_metric = 'logloss')\n\n# Training\nclassifier.fit(X_train, y_train)\n# Extract predictions\ny_pred = classifier.predict(X_test)\n\n# k-Fold Cross Validation\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nxg = accuracies.mean()*100\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\nprint('\\n', classification_report(y_test, y_pred))","d5af1f85":"# List out Model Accuracies in Descending Order.\nAlgorithm = pd.DataFrame({\n    \n    'Algorithm': ['Logistic Regression', 'Decision Tree', 'K-nearest Neighbour',  'Random Forest Classifier', \n                  'Naive Bayes', 'Support Vector Classifier',  'XgBoost'],\n    \n    'Accuracy': [lr, dt, knn, rf, nb, svc, xg] })\n\n\nAlgorithm.sort_values(by = 'Accuracy', ascending = False)","9a57e575":"**There is a stark difference in the properties of malignant and benign tissue samples as indicated in the visualizations above. Particularly all the malignant size related features have greater measures of central tendency compared with benign cells. Since majority of the variables relate to shapes and sizes of tissue samples, there is bound to be some multicollinearity amongst features. It would be worth conducting pearsons correlation tests to detect collinearity.**","95f2bd6d":"**This section will employ bivariate visualisations such as bar-charts and box-plots to detect any noticible patterns in malignant and benign tissue samples.**","e268bfb4":"#  2. Exploratory Data Analysis \n\n\n","dbf248d1":"# CONCLUSION\n\n**It is imperative for health clinics to apply novel strategies that could aid early classification \nand diagnoses of breast cancer. One of the core objectives of a practitioner is to accurately diagnose cancer patients and minimize instances of false positives and false negatives. This analysis clearly indicates that the integration of machine learning in the field of oncology has the potential to improve the decision-making ability of healthcare clinicians.**\n\n\n","99739517":"**The response variable is not balanced as the frequency of benign observations is greater than malignant ones. Resampling could be a potential solution.**","02a606fd":"# THANK YOU :D","a6f8350e":"# 2.1 Univariate Analysis","537b9428":"# 2.2 Bivariate Analysis","30298e86":"**Logistic Regression seems to be the winner in terms of predictive accuracy.** **:D**","77d8d4cb":"**In this section several preprocessing steps such as feature selection, label encoding, normalization and resampling are carried out.**","b43742b9":"**The heatmap indicates strong presense of multicollinearity amongst some independent variables. Predictors related to area, perimeter and radius have correlation coefficients upto 0.9. To conduct feature selection, I will remove several variables in the subsequent section.**","48f787cf":"**All independent variables are numeric. There are therefore 30 independent variables that will be used in the analysis. These variables can be divided into \ndivided into 3 sections. The first section indicates the mean values of each image, the \nsecond section indicates the standard error values of each image, while the third indicates worst values. Moreover, there are\nten real-valued independent features for each digitized cell nucleus image. These include radius, \ntexture, perimeter, area, smoothness, compactness, concavity, concave points, symmetry and \nfractal dimension. Lets take a deeper look into these variables through a univariate and bivariate analysis.** ","8e6bd271":"\n\n# Breast Cancer Wisconsin\n\n\n**Breast cancer is the second leading cause of mortality and the most common type of cancer\namong women. Approximately 2.1 million women are globally diagnosed with breast cancer every year. However, early and accurate diagnosis of this disease increases the effectiveness of \ncancer treatment, thereby increasing survival rates.**\n\n\n**Fine needle aspiration (FNA) is the most common method to diagnose breast cancer. A clinician examines a sample\nunder a microscope and classifies the sample as benign or malignant. However, \nthis method could result in false negatives and false positives through human error. Machine learning can facilitate effective diagnosis of breast cancer through supervised classification algorithms.**\n\n**This notebook conducts exploratory data analysis, data cleaning and predictive modelling using the Breast Cancer Wisconsin Dataset. The objective is to accurately classify benign and malignant samples.**\n\n\n**The visual characteristics of the digitized samples are described in terms of the size and shape of each cell \nwhich are the input variables enumerated in the list below.**\n\n\n1. radius (mean of distances from center to points on the perimeter)\n2. texture (standard deviation of gray-scale values)\n3. perimeter\n4. area\n5. smoothness (local variation in radius lengths)\n6. compactness (perimeter^2 \/ area - 1.0)\n7. concavity (severity of concave portions of the contour)\n8. concave points (number of concave portions of the contour)\n9. symmetry\n10. fractal dimension (\"coastline approximation\")","e7440bad":"# 3. Data Preprocessing","6c9b5591":"**This section employs univariate, bivariate and multivariate analysis using visualizations and summary statistics to detect any trends and patterns within the dataset.**","b20ac91d":"# 1.  Importing Libraries","82eb16c1":"# 4. Predictive Modelling","1f91b3ae":"**Some variables indicate positive skewness. It would be interesting to see if there are differences in these independent features with respect to malignant and benign cancer cells. This will be carried out using boxplots and barcharts in the subsequent bivariate analysis section.**"}}