{"cell_type":{"88935f7d":"code","c0b69c93":"code","2ca993cf":"code","1ca077de":"code","51653215":"code","52be456e":"code","97311d76":"code","88cee8ae":"code","f99e570a":"code","5ac7d6b8":"code","0605c7df":"code","2b02477c":"code","288a6ee4":"code","96c562e3":"code","b9bfbb86":"code","643b6221":"code","4c9b1d1f":"code","bf96c623":"code","e2ef6201":"code","4d56e85a":"code","d3e75c7f":"code","f6da3f33":"code","5fa28b02":"code","2bdc6401":"code","e92b7dcc":"code","9728868e":"code","8a9051f5":"code","ed3c9f08":"code","a98aa850":"code","f817bc52":"code","6ce632b2":"code","20315b48":"code","c4704ccf":"code","6a07db27":"code","5a1eb26c":"code","aec8ba28":"code","8cb38ae6":"markdown","84361870":"markdown","81de760e":"markdown","04890926":"markdown","f1ceb435":"markdown","5917c330":"markdown","3d5c15ae":"markdown","3772ef71":"markdown","f5a20e5a":"markdown","6d105831":"markdown","a04e6a53":"markdown","311c6861":"markdown","f4d30c4a":"markdown","8c508c16":"markdown"},"source":{"88935f7d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c0b69c93":"# Import modules.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","2ca993cf":"# Load orders dataset.\norders = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_order_items_dataset.csv')\n# Load products items dataset.pd.read_csv('\/kaggle\/input\/br\nproducts = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/olist_products_dataset.csv')\n\n# Load translations dataset.\ntranslations = pd.read_csv('\/kaggle\/input\/brazilian-ecommerce\/product_category_name_translation.csv')","1ca077de":"# Print orders header.\norders.head()","51653215":"# Print orders info.\norders.info()","52be456e":"# Print products header.\nproducts.head()","97311d76":"# Print products info.\nproducts.info()","88cee8ae":"# Print translations header.\ntranslations.head()","f99e570a":"# Print translations info.\ntranslations.info()","5ac7d6b8":"# Translate product names to English.\nproducts = products.merge(translations, on='product_category_name', how=\"left\")\n\n# Print English names.\nproducts['product_category_name_english']","0605c7df":"# Define product category name in orders DataFrame.\norders = orders.merge(products[['product_id','product_category_name_english']], on='product_id', how='left')\n\n# Print orders header.\norders.head()","2b02477c":"# Drop products without a defined category.\norders.dropna(inplace=True, subset=['product_category_name_english'])\n# Print number of unique items.\nlen(orders['product_id'].unique())","288a6ee4":"# Print number of unique categories.\nlen(orders['product_category_name_english'].unique())","96c562e3":"# Identify transactions associated with example order.\nexample1 = orders[orders['order_id'] == 'fe64170e936bc5f6a6a41def260984b9']['product_category_name_english']\n\n# Print example.\nexample1","b9bfbb86":"# Identify transactions associated with example order.\nexample2 = orders[orders['order_id'] == 'fffb9224b6fc7c43ebb0904318b10b5f']['product_category_name_english']\n\n# Print example.\nexample2","643b6221":"# Recover transaction itemsets from orders DataFrame.\ntransactions = orders.groupby(\"order_id\").product_category_name_english.unique()\n\n# Print transactions header.\ntransactions.head()","4c9b1d1f":"# Plot 50 largest categories of transactions.\ntransactions.value_counts()[:50].plot(kind='bar', figsize=(15,5))","bf96c623":"# Convert the pandas series to list of lists.\ntransactions = transactions.tolist()\n\n# Print length of transactions.\nlen(transactions)","e2ef6201":"# Count number of unique item categories for each transaction.\ncounts = [len(transaction) for transaction in transactions]\n# Print median number of items in a transaction.\nnp.median(counts)","4d56e85a":"# Print maximum number of items in a transaction.\nnp.max(counts)","d3e75c7f":"from mlxtend.preprocessing import TransactionEncoder\n\n# Instantiate an encoder.\nencoder = TransactionEncoder()\n\n# Fit encoder to list of lists.\nencoder.fit(transactions)\n\n# Transform lists into one-hot encoded array.\nonehot = encoder.transform(transactions)\n\n# Convert array to pandas DataFrame.\nonehot = pd.DataFrame(onehot, columns = encoder.columns_)\n# Print header.\nonehot.head()","f6da3f33":"# Print support metric over all rows for each column.\nonehot.mean(axis=0)","5fa28b02":"# Print distribution of item counts.\nonehot.sum(axis=1).value_counts()","2bdc6401":"# Add sports_leisure and health_beauty to DataFrame.\nonehot['sports_leisure_health_beauty'] = onehot['sports_leisure'] & onehot['health_beauty']\n\n# Print support value.\nonehot['sports_leisure_health_beauty'].mean(axis = 0)","e92b7dcc":"# Merge books_imported and books_technical.\nonehot['books'] = onehot['books_imported'] | onehot['books_technical']\n\n# Print support values for books, books_imported, and books_technical.\nonehot[['books','books_imported','books_technical']].mean(axis=0)","9728868e":"# Compute joint support for sports_leisure and health_beauty.\njoint_support = (onehot['sports_leisure'] & onehot['health_beauty']).mean()\n\n# Print confidence metric for sports_leisure -> health_beauty.\njoint_support \/ onehot['sports_leisure'].mean()","8a9051f5":"# Print confidence for health_beauty -> sports_leisure.\njoint_support \/ onehot['sports_leisure'].mean()","ed3c9f08":"from mlxtend.frequent_patterns import apriori\n\n# Apply apriori algorithm to data with min support threshold of 0.01.\nfrequent_itemsets = apriori(onehot, min_support = 0.01)\n\n# Print frequent itemsets.\nfrequent_itemsets","a98aa850":"# Apply apriori algorithm to data with min support threshold of 0.001.\nfrequent_itemsets = apriori(onehot, min_support = 0.001, use_colnames = True)\n\n# Print frequent itemsets.\nfrequent_itemsets","f817bc52":"# Apply apriori algorithm to data with min support threshold of 0.00005.\nfrequent_itemsets = apriori(onehot, min_support = 0.00005, use_colnames = True)\n\n# Print frequent itemsets.\nfrequent_itemsets","6ce632b2":"# Apply apriori algorithm to data with a two-item limit.\nfrequent_itemsets = apriori(onehot, min_support = 0.00005, max_len = 2, use_colnames = True)","20315b48":"from mlxtend.frequent_patterns import association_rules\n\n# Recover association rules using support and a minimum threshold of 0.0001.\nrules = association_rules(frequent_itemsets, metric = 'support', min_threshold = 0.0001)\n\n# Print rules header.\nrules.head()","c4704ccf":"# Recover association rules using confidence threshold of 0.01.\nrules = association_rules(frequent_itemsets, metric = 'confidence', min_threshold = 0.01)\n\n# Print rules.\nrules","6a07db27":"# Select rules with a consequent support above 0.095.\nrules = rules[rules['consequent support'] > 0.095]\n\n# Print rules.\nrules","5a1eb26c":"# Select rules with leverage higher than 0.0.\nrules = rules[rules['leverage'] > 0.0]\n\n# Print rules.\nrules","aec8ba28":"# Recover association rules with a minimum support greater than 0.000001.\nrules = association_rules(frequent_itemsets, metric = 'support', min_threshold = 0.000001)\n\n# Plot leverage against confidence.\nplt.figure(figsize=(15,5))\nsns.scatterplot(x=\"leverage\", y=\"confidence\", data=rules)","8cb38ae6":"# Visualizing patterns in metrics","84361870":"# Convert product IDs to product category names.**","81de760e":"# Map orders to transactions.\n\n","04890926":"# Construct transactions from order and product data**","f1ceb435":"# Compute the confidence metric\n","5917c330":"# Association Rules and Metrics","3d5c15ae":"# The Apriori Algorithm and Pruning","3772ef71":"# Pruning association rules","f5a20e5a":"# Compute the support metric\n","6d105831":"# Computing association rules from Apriori output**","a04e6a53":"# Compute the item count distribution over transactions","311c6861":"# Create a column for an itemset with multiple items\n","f4d30c4a":"# The leverage metric\n","8c508c16":"# **Aggregate the dataset further by combining product sub-categories**\nWe can use the inclusive OR operation to combine multiple categories.\n* True | True = True\n* True | False = True\n* False | True = True\n* False | False = False"}}