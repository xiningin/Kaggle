{"cell_type":{"ad1b2af7":"code","19f82d7b":"code","85a9190a":"code","c221984e":"code","2782a05d":"code","d6dae3ec":"markdown","10f2886b":"markdown","0014f2dd":"markdown","2bc3fa5a":"markdown","2a3f0727":"markdown","92c06382":"markdown"},"source":{"ad1b2af7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","19f82d7b":"df = pd.read_csv('..\/input\/iris-flower-dataset\/IRIS.csv')\ndf.head()","85a9190a":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set();\n%matplotlib inline\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 7))\nsns.scatterplot(df.sepal_length, df.sepal_width, hue=df.species, ax=ax1);\nsns.scatterplot(df.petal_length, df.petal_width, hue=df.species, ax=ax2);","c221984e":"from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n\nX = df.iloc[:, :-1].values\ny = df.iloc[:,-1].values\n\n# scale each feature between 0 and 1\nscaler = MinMaxScaler().fit(X)\nscaled_X = scaler.transform(X)\n\n# label target\nle = LabelEncoder()\nlabeled_y = le.fit_transform(y)\n\n\nprint('New X')\nprint(scaled_X[:5])\nprint('New y')\nprint(labeled_y[:5])","2782a05d":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\n# split the data\nX_train, X_test, y_train, y_test = train_test_split(scaled_X, labeled_y, stratify=labeled_y, random_state=0)\n\n# use default for all of them to compare\nclassifiers = {'Logistic': LogisticRegression(), 'Decision Tree': DecisionTreeClassifier(), \n               'SVM': SVC(), 'K Neighbors': KNeighborsClassifier()}\n\nscore_df = pd.DataFrame(index=['training set', 'test set'])\nfor clf in classifiers:\n    classifiers[clf].fit(X_train, y_train)\n    train_score = round(accuracy_score(y_train, classifiers[clf].predict(X_train)),4)\n    test_score = round(accuracy_score(y_test, classifiers[clf].predict(X_test)),4)\n    score_df[clf] = [train_score, test_score]\n    \nscore_df","d6dae3ec":"The goal of this project is to give me an opportunity to practice my ML skill. Plus, using simple dataset to see which classifiers perform well in default mode.","10f2886b":"# Prediction","0014f2dd":"When using default models on Iris dataset, decision tree's performance is the best, although it is overfitting slightly. K Neighbors is second. Then SVM is third. Logistic regression does not do well compared to other models.\n","2bc3fa5a":"# Visualization on data features","2a3f0727":"# Preprocess the features ","92c06382":"# Objective"}}