{"cell_type":{"27230e2d":"code","d14264f6":"code","ccfdfbf9":"code","3086c98a":"code","b4d19eb4":"code","620d5ff8":"code","746fd072":"code","5b003429":"code","09b1f693":"code","5bb15f69":"code","b97c23f9":"code","f76bb941":"code","d32b6dae":"code","7bc3487a":"code","3893170a":"code","da97875c":"code","60c88243":"code","7ab0cf1f":"code","f6726110":"code","43e9c24e":"code","b0009775":"code","9cb7151b":"code","f7b0f1e8":"code","f5465d9d":"code","2e66a347":"code","22ab112b":"code","7265d9eb":"code","cb782ae1":"code","c5a7368f":"code","3717f920":"code","ab66bd3a":"code","d24f2c20":"code","1222b2f2":"code","d653dcee":"code","1a6783f2":"code","17ad57c3":"code","35823675":"code","b6174467":"code","e885bfc7":"code","d0d44502":"code","9aa1adcb":"code","97a18db6":"code","a0863cb0":"code","a86e7cf3":"code","64721fbf":"code","ce3dddbf":"code","e1344088":"markdown","21d04988":"markdown","df8425ae":"markdown","157a9e70":"markdown","4bd8c2a6":"markdown","7c3368d4":"markdown","72240001":"markdown","a9be8da3":"markdown","55b51932":"markdown","07dce99f":"markdown","2964f20c":"markdown","36d50203":"markdown","094b471f":"markdown"},"source":{"27230e2d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d14264f6":"# main libraries\nimport pandas as pd\nimport numpy as np\nimport time# visual libraries\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom mpl_toolkits.mplot3d import Axes3D \nplt.style.use('ggplot')# sklearn libraries\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,f1_score,matthews_corrcoef,classification_report,roc_curve\n#from sklearn.externals import joblib\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n#check Path\nfrom pathlib import Path","ccfdfbf9":"\"\"\"\nCheck the current working directory and creating a path to the data folder\n\n\"\"\"\nprint(Path.cwd())\nparent_dir = os.chdir('\/kaggle\/')\nprint(os.listdir(\"..\/kaggle\/input\"))\ndata_dir = Path.cwd() \/'input\/cold-start'","3086c98a":"'''Read the data in the CSV file using pandas''' \ndf = pd.read_csv(data_dir\/'train.csv')\ndf.head()","b4d19eb4":"df = pd.read_csv(data_dir\/'train.csv',sep=';')\ndf.head()","620d5ff8":"'''size of the data set'''\ndf.shape","746fd072":"'''The null or NaN in the data set'''\ndf.isnull().any()","5b003429":"df.info()","09b1f693":"'''This will display a summary statistics of all observed features and labels.'''\ndf.describe()","5bb15f69":"#copy data frame(copying through variable will also change the previous data) using data frame copy instead\ndata = df.copy()","b97c23f9":"'''Eliminate or deleting Rows.'''\ndata.dropna(inplace = True)\n#df.isnull().any()\ndata.isnull().any().sum()","f76bb941":"df.shape,data.shape","d32b6dae":"#df[''].isnull().any().sum()\ndata = df.copy()\ndata['temperature'].mean()\n#data['temperature'].median()\n#data['temperature'].mode()","7bc3487a":"data['temperature'].replace(np.NaN, data['temperature'].mean(),inplace=True)\n#data.fillna(data.mean(), inplace=True)\n","3893170a":"data.head(),df.head()","da97875c":"\"\"\"\nCheck the current working directory and creating a path to the data folder\n\"\"\"\nprint(Path.cwd())\nparent_dir = os.chdir('\/kaggle\/')\nprint(os.listdir(\"..\/kaggle\/input\"))\ndata_dir = Path.cwd() \/'input\/hourly-energy-consumption'","60c88243":"'''Read the data in the CSV file''' \n\npjme = pd.read_csv('..\/kaggle\/input\/hourly-energy-consumption\/PJME_hourly.csv', index_col=[0], parse_dates=[0])","7ab0cf1f":"'''print out the data'''\nprint(pjme)","f6726110":"'''\nPlot the energy consumption data\n'''\ncolor_pal = [\"#F8766D\", \"#D39200\", \"#93AA00\", \"#00BA38\", \"#00C19F\", \"#00B9E3\", \"#619CFF\", \"#DB72FB\"]\n_ = pjme.plot(style='.', figsize=(15,5), color=color_pal[0], title='PJME')","43e9c24e":"'''\nPrepare the train and test data by splitting the data set to before 2015 and after 2015\n'''\nsplit_date = '01-Jan-2015'\npjme_train = pjme.loc[pjme.index <= split_date].copy()\npjme_test = pjme.loc[pjme.index > split_date].copy()","b0009775":"'''\nprint out the trainning set\n'''\nprint(pjme_train)","9cb7151b":"'''\nplot the trainning set (blue) and test set (red)\n'''\n_ = pjme_test \\\n    .rename(columns={'PJME_MW': 'TEST SET'}) \\\n    .join(pjme_train.rename(columns={'PJME_MW': 'TRAINING SET'}), how='outer') \\\n    .plot(figsize=(15,5), title='PJM East', style='.')","f7b0f1e8":"'''\nDefine the function to normalize the data\n'''\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef normalize(data):\n    scaler = MinMaxScaler()\n\n    data = data.dropna()\n    dataset = data.values\n    dataset = dataset.astype('float32')\n    data = scaler.fit_transform(dataset)\n    return data","f5465d9d":"'''\nApplied the normalization to train and test set\n'''\ntrain_data = normalize(pjme_train)\ntest_data = normalize(pjme_test)","2e66a347":"'''\nplot the data after normalization\n'''\nplt.plot(train_data)","22ab112b":"'''plt.figure(figsize=(15,6))\ndata_csv = pjme_train.dropna()\ndataset = pjme_train.values\ndataset = dataset.astype('float32')\nmax_value = np.max(dataset)\nmin_value = np.min(dataset)\nscalar = max_value - min_value\ndataset = list(map(lambda x: (x-min_value) \/ scalar, dataset))\nplt.plot(dataset)'''","7265d9eb":"#slower \n'''\ntrain_data = list(train_data)\ntest_data  = list(test_data)\n\ndef create_dataset(dataset, look_back):\n    dataX, dataY = [], []\n    for i in range(len(dataset) - look_back):\n        a = dataset[i:(i + look_back)]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back])\n    return np.array(dataX), np.array(dataY)\n'''","cb782ae1":"#dataset = dataframe.values\n#dataset = dataset.astype('float32')\n\n# convert an array of values into a dataset matrix\ndef create_dataset(dataset, look_back=1):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return np.array(dataX), np.array(dataY)\n","c5a7368f":"print(len(train_data))\nprint(len(test_data))","3717f920":"look_back = 5\nX_train, y_train = create_dataset(train_data, look_back)\nX_test, y_test   = create_dataset(test_data, look_back)","ab66bd3a":"print(X_train[0:5]), print(y_train[0:5])","d24f2c20":"X_train.shape, y_train.shape","1222b2f2":"'''\nplot train and test set.\n'''\nplt.figure(figsize=(15,6))\nx1=np.arange(0,113921)\nx2 = np.arange(113921,113921 + 31433)\nplt.plot(x1,y_train,'b',label='train')\nplt.plot(x2, y_test, 'r', label='test')\nplt.legend(loc='best')","d653dcee":"'''\nX_train = X_train.reshape(-1, 1, look_back)\ny_train = y_train.reshape(-1, 1, 1)\nX_test = X_test.reshape(-1, 1, look_back)\n'''","1a6783f2":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.layers import LSTM\nfrom keras.optimizers import RMSprop\nfrom keras.callbacks import LambdaCallback, ModelCheckpoint\nimport random\nimport sys\nimport io\nfrom tqdm import tqdm\nimport tensorflow as tf","17ad57c3":"# reshape input to be [samples, time steps, features]\nX_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\nX_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))","35823675":"X_train.shape, y_train.shape","b6174467":"model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.LSTM(24,return_sequences=True,input_shape=(1, look_back))) #input_shape=(1, look_back), input_shape=(X_train.shape[1],1)\nmodel.add(tf.keras.layers.LSTM(24,return_sequences=False))\nmodel.add(tf.keras.layers.Dense(1))\nmodel.compile(optimizer='sgd', loss='mse')\n#model.compile(optimizer='sgd', loss='mse')\n# This builds the model for the first time:\nmodel.fit(X_train, y_train, batch_size=32, epochs=5)","e885bfc7":"#predict the output using the trainned model.\nyhat = model.predict(X_test)","d0d44502":"predict = np.zeros(len(X_test))\nfor i in range(len(X_test)):\n# predict scaled value for next time step\n    temp = yhat[i][0]\n    predict[i] = temp\n    \n'''temp = np.zeros(len(X_test))\nfor i in range(len(X_test)):\n# predict scaled value for next time step\n    a = X_test[i][0]\n    temp[i] = a    \n'''","9aa1adcb":"plt.figure(figsize=(18,7))\nplt.plot(predict[0:500],'r')\nplt.plot(y_test[0:500],'b')\n#plt.plot(temp[0:500],'k')","97a18db6":"model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.LSTM(40,return_sequences=True,input_shape=(1, look_back))) #input_shape=(1, look_back), input_shape=(X_train.shape[1],1)\nmodel.add(tf.keras.layers.LSTM(40,return_sequences=False))\n#model.add(tf.keras.layers.ReLU())\nmodel.add(tf.keras.layers.Dense(1))\nmodel.compile(optimizer='adam', loss='mse')\n# This builds the model for the first time:\nmodel.fit(X_train, y_train, batch_size=32, epochs=5)","a0863cb0":"yhat = model.predict(X_test)","a86e7cf3":"predict = np.zeros(len(X_test))\nfor i in range(len(X_test)):\n# predict scaled value for next time step\n    temp = yhat[i][0]\n    predict[i] = temp\n    \n'''temp = np.zeros(len(X_test))\nfor i in range(len(X_test)):\n# predict scaled value for next time step\n    a = X_test[i][0]\n    temp[i] = a    \n'''","64721fbf":"plt.figure(figsize=(18,7))\nplt.plot(predict[0:500],'r')\nplt.plot(y_test[0:500],'b')\n#plt.plot(temp[0:500],'k')","ce3dddbf":"'''# instantiate a sequential model\n\nmodel = Sequential()\n\n# add LSTM layer - stateful MUST be true here in \n# order to learn the patterns within a series\nmodel.add(LSTM(units=num_neurons, \n              batch_input_shape=batch_input_shape, \n              stateful=True))\n\n# followed by a dense layer with a single output for regression\nmodel.add(Dense(1))\n\n'''\n#Adapt to run with Tensor flow 2.2\n\n# lag of 24 to simulate smallest cold start window. Our series\n# will be converted to a num_timesteps x lag size matrix\n\n''''# model parameters\nnum_neurons = 24\nbatch_size = 1  # this forces the lstm to step through each time-step one at a time\nbatch_input_shape=batch_size\n\n# instantiate a sequential model\n#model = tf.keras.Sequential()\nmodel = Sequential()\n\n\n# add LSTM layer - stateful MUST be true here in \n# order to learn the patterns within a series\nmodel.add(LSTM(units=num_neurons,return_sequences=True,input_shape=(X_train.shape[1],1)))\nmodel.add(LSTM(units=num_neurons,return_sequences=False))\n\n#model.add(tf.keras.layers.LSTM(units=num_neurons,batch_input_shape=batch_input_shape,stateful=True))\n# followed by a dense layer with a single output for regression\n#model. add(tf.keras.layers.Dense(1))\nmodel.add(Dense(1))\n# compile\nmodel.compile(loss='mean_absolute_error', optimizer='adam')\nmodel.fit(X_train, y_train, batch_size=32, epochs=10)\n'''","e1344088":"# Let try another example which using LSTM model to forecast energy consumption","21d04988":"> The data in the CSV file is the text file and separate by a semicolon. We need to separate these into different column. ","df8425ae":"LSTMs are a subclass of recurrent neural networks. Recurrent neural nets are by definition applied on sequential data, which without loss of generality means data samples that change over time. A full history of a data sample is then described by the sample values over a finite time window, i.e. if your data live in an N-dimensional space and evolve over t-time steps, your input representation must be of shape (num_samples, t, N).","157a9e70":"Replacing With Mean\/Median\/Mode\nThis strategy can be applied on a feature which has numeric data like the age of a person or the ticket fare. We can calculate the mean, median or mode of the feature and replace it with the missing values.","4bd8c2a6":"## References\n\nhttps:\/\/machinelearningmastery.com\/time-series-prediction-lstm-recurrent-neural-networks-python-keras\/\nhttps:\/\/towardsdatascience.com\/data-pre-processing-techniques-you-should-know-8954662716d6\n\n","7c3368d4":"### Missing values :\n\nMissing values is oftenly happen in your dataset and must be taken into consideration.\n\n* Eliminate rows with missing data :Simple and sometimes effective strategy. Fails if many objects have missing values. If a feature has mostly missing values, then that feature itself can also be eliminated.\n* Estimate missing values : If only a reasonable percentage of values are missing, then we can also run simple interpolation methods to fill in those values. However, most common method of dealing with missing values is by filling them in with the mean, median or mode value of the respective feature.\n","72240001":"Tune the model parameters such as changing an optimizer, reduce or increase the model layers...etc","a9be8da3":"Setup the model using keras and fit the data to the model.","55b51932":"> The LSTM network expects the input data (X) to be provided with a specific array structure in the form of: [samples, time steps, features].\n> \n> Currently, the data is in the form: [samples, features]. Using numpy.reshape() to transform the input data into the right structure [samples, time steps, features].","07dce99f":"Plot the predicted value and test value","2964f20c":"> **Missing values**","36d50203":"The X=t and Y=t+5","094b471f":"# Data Pre Processing\nData is often taken from multiple sources and simply unrealistic to expect that the can be used immidiately. Let go throug an example to learn more.\n\nData in this notebook has been used for Power Laws: Cold Start Energy Forecasting competition. The data can be downloaded in the link below.\n\nhttps:\/\/shop.exchange.se.com\/en-US\/apps\/39021\/forecasting-cold-start-building-energy-consumption"}}