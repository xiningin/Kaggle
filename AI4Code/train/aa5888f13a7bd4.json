{"cell_type":{"0f421704":"code","9d505f6c":"code","9693d251":"code","5845bcd9":"code","80e78196":"code","7ffa3fb1":"code","34c13601":"code","665e7bad":"code","c856260b":"code","255e2f8a":"code","696dbfb6":"code","0081d484":"code","5d7dc6a7":"code","a74a813e":"code","4427a43d":"code","a2c328d6":"code","05172bb9":"code","f3f97889":"code","94dd8adf":"code","4113901f":"code","bb4601cf":"code","9f304e66":"code","3a42f6f1":"code","847fc088":"code","946bf1ff":"code","3f072f74":"code","a6ed5e5c":"code","9e9e80ba":"code","a1ac09c4":"code","34700729":"code","15a752b1":"code","fecb01c2":"code","a4118a1e":"code","60218e80":"code","b0cf9db2":"code","ea9c222c":"code","e46461e7":"code","3173e1f5":"code","a1611ec0":"code","bcfb4434":"code","a3421fa1":"code","aed4ad00":"code","518e8827":"code","a1e02d44":"code","6cf0ebe4":"code","72d456f7":"code","6bebf528":"code","4d970ce0":"code","139007e8":"code","a87946c6":"code","1e6b7d15":"code","9fa1a7f3":"code","f9a1607f":"code","bc94bbfb":"code","0c52cccd":"code","db5b5712":"code","eb4a7242":"code","a29692ef":"code","8aff3119":"code","e6d93298":"code","1d9bc2d9":"code","e071a81b":"code","565f9e50":"code","2f1f059d":"code","8082882f":"code","66b86e61":"code","1cdf0bbf":"code","c19a25c0":"code","ed82dde2":"code","e0413a4e":"code","359c35dc":"code","c8163306":"code","6798b920":"code","928c1d30":"code","dee37e5e":"code","5e1f3de0":"code","58b54921":"code","988221d3":"code","e8ccd967":"code","a3c06b8d":"code","c7a03826":"code","765c0ffc":"code","9744b742":"code","606972fc":"code","06fa128d":"code","0433f123":"code","ff5485f6":"code","eb6ddde7":"code","e5240a92":"code","40d849f2":"code","de104740":"code","56d1bec6":"code","8d76c969":"code","5307f219":"code","5f52d9b6":"code","f1fdb816":"code","dbba4d5b":"code","00fe555b":"code","e9dddb06":"code","924f34a1":"code","e518819b":"code","9824b14b":"code","51834a46":"code","6c073d90":"code","be234830":"code","6884b1bb":"code","e84dc2b6":"code","87dd95ee":"code","89de1322":"code","375941ed":"code","5257340f":"code","7b9afd1e":"code","a47a417d":"code","5131c1ca":"code","65ace263":"code","c84e568b":"code","ea787dca":"code","b18b1ef1":"code","d493142a":"code","fd9b846d":"code","0cca6c54":"code","ba0632c8":"code","d8dd90ed":"code","7d50d5b6":"code","a8d83cc6":"code","b2256275":"code","a26b959c":"code","b1ca30af":"code","f63ad045":"code","351d4d03":"code","6b6123dc":"code","35bf604b":"code","04de818f":"code","7d4216e0":"code","6a2ef57d":"code","d77ea0b0":"code","3a3687b4":"code","ab73ef81":"code","de80e0cb":"code","4b5dea52":"code","998570a1":"code","bd0511aa":"code","4bb8d126":"code","86521124":"code","d405ef80":"code","d64798e0":"code","92ecb106":"code","f3f565b1":"code","44591b71":"code","28b65a85":"code","68d89f4e":"code","e11eeae6":"code","c60b4763":"code","3a8aba89":"code","f3f19d5c":"code","dae2dc84":"code","5f75ff50":"code","0c4c762b":"code","991e0410":"code","51eeb449":"code","4faa21b0":"code","7c17cc58":"code","74dff0fe":"code","30109b49":"code","3ce8c8cf":"code","fee5778d":"code","321361a1":"code","1a6280da":"code","60b4114c":"code","2f2707c5":"code","b143097b":"code","107625f7":"code","a7fc7dd2":"code","662a218d":"code","185dfeb6":"code","7c1e4b51":"code","fb08f75b":"code","23fc665d":"code","fdcf9617":"code","e479d391":"code","40a06066":"code","a830d71c":"code","d3837734":"code","465e2cdc":"code","21054ea6":"code","fba5556e":"code","50e03483":"code","be502a17":"code","5ba94c02":"code","2b11acbe":"code","fc9dfd9b":"code","e6b3c838":"code","ce465804":"code","0f079dfd":"code","ec5dc624":"code","9feea3b4":"code","f05bda32":"code","5990f3d4":"code","474e24dc":"code","b324138f":"code","ada0fe4d":"code","415afda9":"code","80c55127":"code","5e33c2cf":"code","becf80f2":"code","b88029bb":"code","b2cd070b":"code","c29883f9":"code","8f89f35a":"code","051ea139":"code","4bcd593f":"code","35532591":"code","5b8f10d2":"code","6599d697":"code","fa93527f":"code","5fa0ad56":"code","fb5e8eb9":"code","da1d6f32":"code","552dd10e":"code","cec68631":"code","ed79b3ff":"code","800dbf86":"code","7ec02ece":"code","4d231013":"code","ffde79c5":"code","cd2d8cd9":"code","307fd9ec":"markdown","ef26228e":"markdown","bb1ced7c":"markdown","aaf98ba4":"markdown","2293750e":"markdown","6b5fdf07":"markdown","96217d4c":"markdown","6516798d":"markdown","72a3c6bc":"markdown","9ec9550b":"markdown","548376ec":"markdown","3a26c58b":"markdown","e3d25d29":"markdown","02d6c486":"markdown","f6f66fa8":"markdown","a10c6e8e":"markdown","97f94e3a":"markdown","08faa05f":"markdown","10cf94d1":"markdown","3b2dc066":"markdown","775f5085":"markdown","8dfbc788":"markdown","207ff9ff":"markdown","56acc70a":"markdown","1bd6a81e":"markdown","83b2a9f8":"markdown","ddf66812":"markdown","92a29208":"markdown","ce8cc444":"markdown","90f280b1":"markdown","9a564760":"markdown","ad52a246":"markdown","3ab9724f":"markdown","3bbda90c":"markdown","dea8ff81":"markdown","d3171f75":"markdown","ce4aab5e":"markdown","cc8f048b":"markdown","acbd01b7":"markdown","334167d4":"markdown","e9a39f82":"markdown","48c36a7c":"markdown","b6c3d3c4":"markdown","4bba8ee8":"markdown","234b3839":"markdown","1d98fe79":"markdown","654fd8cb":"markdown","3a35e87b":"markdown","6d3e8692":"markdown","483ff980":"markdown","82c47b2a":"markdown","1d2fa9f9":"markdown","79466f3c":"markdown","d1c85920":"markdown","deebbd28":"markdown","a8f41e9f":"markdown","9f17f1e6":"markdown","43d92b16":"markdown","09b275ce":"markdown","60e3f999":"markdown","43b6e28f":"markdown","57ff8bda":"markdown","b7dbc7a6":"markdown","ee55afa4":"markdown","12419cff":"markdown","594e23f8":"markdown","bb343327":"markdown","302699ec":"markdown","42ccd33a":"markdown","d57df308":"markdown","3805743b":"markdown","24995d34":"markdown","5af64a6c":"markdown","404fb7ba":"markdown","1f63bfd9":"markdown","8be23153":"markdown","a081e957":"markdown","e74cb566":"markdown","6c295639":"markdown","4d3d2cbc":"markdown","a6d45eb9":"markdown","dc085e19":"markdown","8f35566d":"markdown","24389beb":"markdown","ce204a54":"markdown","6a877d13":"markdown","e036ce6a":"markdown","ac327f51":"markdown","90ce79d3":"markdown","c94b7f2c":"markdown","388a34d7":"markdown","6abf3a52":"markdown","449cd27f":"markdown","5451fdc2":"markdown","09ab7b70":"markdown","b43ac146":"markdown","54e962c0":"markdown","3cf6691a":"markdown","5464e503":"markdown","b44f747a":"markdown","e2e94623":"markdown","3ec5f0ff":"markdown","d9885210":"markdown","75a9a56c":"markdown","4b01e8a9":"markdown","5b709ccf":"markdown","889981a1":"markdown","4ad032e2":"markdown","878fea49":"markdown","35fed263":"markdown","ec4828ae":"markdown","d84820f5":"markdown","8d80101e":"markdown","4abe16c3":"markdown","3612ac72":"markdown","d58d6d86":"markdown","3cbb0007":"markdown","89e482a6":"markdown","77dae75a":"markdown","9f0ac9eb":"markdown","540c6cd5":"markdown","6c1f5b36":"markdown","39d6b27d":"markdown","068b2b30":"markdown","5364696b":"markdown","0a3271b5":"markdown","1c7131b2":"markdown","57461ff7":"markdown","faaa437b":"markdown","a8da666b":"markdown","4bfcc3b4":"markdown","3d2fc630":"markdown","80d275da":"markdown","2cc9c2e9":"markdown","52585d89":"markdown","d78f9de5":"markdown","4cde9f62":"markdown","c9b9aa66":"markdown","43b9f9e3":"markdown","b8bac2b9":"markdown","e3989a74":"markdown"},"source":{"0f421704":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9d505f6c":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport io\nimport warnings\nwarnings.filterwarnings(\"ignore\")","9693d251":"#Uploading dataset\ndf = pd.read_csv('\/kaggle\/input\/chronic-kidney-disease\/kidney_disease_train.csv')","5845bcd9":"\n#Getting a overview of DataFrame\ndf.head()","80e78196":"df.shape","7ffa3fb1":"df.columns","34c13601":"df.info()","665e7bad":"df.shape","c856260b":"# Percentage of missing values\n(df.isnull().sum()\/df.shape[0])*100","255e2f8a":"# checking if any row has all missing values\ndf[df.isnull().all(1)]","696dbfb6":"# Checking count of row with missing values for columns\ndf.isnull().sum(1).value_counts()","0081d484":"# Statistical Parameters for Columnn\ndf.describe()","5d7dc6a7":"# Object type describe\ndf.describe(include = 'O')","a74a813e":"# Dividing into numeric and categorical variables\ndf_cont = df.select_dtypes(exclude = 'object')\ndf_cat = df.select_dtypes(include = 'object')","4427a43d":"# EDA Numeric Variable\nfor col in df_cont.columns:\n  fig, ax = plt.subplots(1,3, figsize = (13,5))\n  df[col].plot(kind = 'kde', ax = ax[0])\n  ax[0].set_ylim(bottom = 0)\n  sns.boxplot(col, data = df_cont, ax = ax[1], orient = 'v')\n  sns.swarmplot(col, data = df_cont, ax = ax[2], orient = 'v')","a2c328d6":"# Seeing values of id column\ndf_cont['id'].sort_values(ascending = True)","05172bb9":"#Unique values of age\nnp.sort(df_cont['age'].unique())","f3f97889":"# Dataset with age less than 11\ndf[df['age'] < 11]","94dd8adf":"# Unique values of bp\ndf['bp'].value_counts()","4113901f":"# Unique values of sg\ndf['sg'].value_counts(dropna = False)","bb4601cf":"# Unique value of al - Aluminium\ndf['al'].value_counts(dropna = False)","9f304e66":"# Checking rows that have both al and sg missing\ndf[(df['al'].isnull()) & (df['sg'].isnull())].count()","3a42f6f1":"# Overview of dataset for missing value of sg and al \ndf_sg = df[(df['al'].isnull()) & (df['sg'].isnull())]\ndf_sg","847fc088":"# Trying to see the values of other variables for the missing values of column sg and al,\n# to understand if we can relate the variables somehow and this can help in missing value \n# imputation\nfor col in df_sg.columns:\n  if df_sg[col].dtypes != 'object':\n    fig, ax = plt.subplots(1,1, figsize =(6,3))\n    sns.swarmplot(col, data = df_sg)","946bf1ff":"# Checking the value count for Sugar(su)\ndf.su.value_counts(dropna = False)","3f072f74":"\nsns.countplot(x = 'su', hue = 'dm', data = df)\ng = sns.FacetGrid(data = df, hue = 'su', aspect = 2)\ng.map(sns.kdeplot, 'bgr')\nplt.legend()","a6ed5e5c":"# Checking the values of the columns Blood Glocuse random(bgr) and Blood Urea(bu)\nfig = plt.figure(figsize = (10,4))\nplt.subplot(1,2,1)\ndf.bgr.hist()\nplt.title('bgr')\nplt.subplot(1,2,2)\ndf.bu.hist()\nplt.title('bu')","9e9e80ba":"#Checking for high values of bgr (abnormality)\ndf[df['bgr']> 400]","a1ac09c4":"# Checking for abnormal high values of bu\ndf[df.bu > 200]","34700729":"# Generating insights using Serum Creatinine(sc)\ndf.sc.hist()","15a752b1":"# Understanding Potassimum(pot), Sodium(sod), Hemoglobin(hemo) data structure more closely\nfig, ax = plt.subplots(1,3, figsize = (10,4))\ndf.sod.hist(ax = ax[0])\nax[0].set_title('Sodium')\ndf.pot.hist(ax = ax[1])\nax[1].set_title('Potassium Distribution')\ndf.hemo.hist(ax = ax[2])\nax[2].set_title('Hemoglobin')","fecb01c2":"#Checking abnormality of sodium and instances of ckd\nckd = list(df.classification.unique())\nplt.figure(figsize = (10,6))\nfor c in ckd:\n  sns.distplot(df['sod'][df['classification'] == c], label = c)\n  plt.legend()","a4118a1e":"# Abnormality of Potassium and ckd\nplt.figure(figsize = (6,4))\nfor c in ckd:\n  sns.distplot(df['pot'][df['classification'] == c], label = c)\n  plt.legend()","60218e80":"# Checking the dataset for abnormal values of Sodium and Potassium\ndf[(df.sod < 50) | (df.pot > 10)]","b0cf9db2":"# Distribution of Pcked Cell Volume(Pcv)\ndf_cont.pcv.hist()\nplt.title('pcv')","ea9c222c":"df_cat.columns","e46461e7":"# Count plot of Categorical Varables\nsns.catplot(x = 'rbc', estimator = None, data = df_cat, kind = 'count')","3173e1f5":"# creating list of categories of rbc\nlabel = list(x for x in df['rbc'].unique())\nlabel.remove(np.nan)\nlabel","a1611ec0":"# Plotting different categories of rbc with hemoglobin\nfor z in label:\n  subset = df['hemo'][df['rbc'] == z]\n  sns.distplot(a = subset, label = label, rug = True)\n  plt.legend(['normal','abnormal'])","bcfb4434":"# Just the above same plot in a different manner to remember. Please ignore\ngrid = sns.FacetGrid(df, hue=\"rbc\", aspect = 2)\ngrid.map(sns.kdeplot, 'hemo')\ngrid.add_legend()","a3421fa1":"#Distirbution of Pus cell and Pus cells clump\nplt.figure(figsize = (10,5))\nplt.subplot(1,2,1)\nsns.countplot(x = 'pc', data = df_cat)\nplt.title('Pus Cell')\nplt.subplot(1,2,2)\nsns.countplot(x = 'pcc', data = df_cat)\nplt.title('Pus Cells Clump')","aed4ad00":"#Check overlap\nsns.countplot(x = 'pcc', hue = 'pc', data = df_cat, saturation = 1)","518e8827":"# Proportion of Bacteria present or not\ndf_cat.ba.value_counts(normalize = True).plot(kind = 'bar', colormap = 'cool')\nplt.axhline(y = 0.93, color = 'r', label = '0.93')","a1e02d44":"# Provide us the non integer values in the numeric column\ns = df.rc.apply(lambda x : str(x).replace('.','').isdigit())\nt = list(s[s == 0].index.values)\ndf.iloc[t,:].rc.unique()","6cf0ebe4":"#Missng counts\ndf.rc.isnull().sum()","72d456f7":"# Dropping the Nan and other non numeric values and seeing distribution\nplt.figure(figsize = (15,5))\nsns.distplot(df['rc'][(df['rc'] != '\\t?') & (~df['rc'].isnull())])\nplt.title('RC Distribution')","6bebf528":"df['rc'] = df['rc'].replace({'\\t?' : np.nan})\ndf['wc'] = df['wc'].replace({'\\t?' : np.nan , '\\t8400' : 8400})","4d970ce0":"df.rc.isnull().sum()\ndf.wc.isnull().sum()","139007e8":"label = list(x for x in df['rbc'].unique())\nlabel.remove(np.nan)\nlabel","a87946c6":"# Plotting different categories of rbc with rc - Count of Red blood cells\ndf_rc = df[(df['rc'] != '\\t?') & (~df['rc'].isnull())]\nfor z in label:\n  subset = df_rc['rc'][df['rbc'] == z]\n  sns.distplot(a = subset, label = label, rug = True)\n  plt.legend(('normal','abnormal'))","1e6b7d15":"# WC Analysis\n# Provide us the non integer values in the numeric column\ns = df.wc.apply(lambda x : str(x).isdigit())\nt = list(s[s == 0].index.values)\ndf.iloc[t,:].wc.unique()","9fa1a7f3":"sns.distplot(df['wc'][(df['wc'] != '\\t?') & (~df['wc'].isnull()) & (df['wc'] != '\\t8400')], \n             color = 'Orange')\nplt.title('WC Distribution')","f9a1607f":"# Distribution of HyperTension and DM and CAD\nplt.figure(figsize = (15,4))\nplt.subplot(1,3,1)\nsns.countplot(x = 'htn', data = df_cat)\nplt.title('HyperTension')\nplt.subplot(1,3,2)\nsns.countplot(x = 'dm', data = df_cat)\nplt.title('Diabetes Mellitus')\nplt.subplot(1,3,3)\nsns.countplot(x = 'cad', data = df_cat)\nplt.title('Coronary Artery Disease')\n\n","bc94bbfb":"df.dm.value_counts()","0c52cccd":"# Correcting the values of the variables dm and cad\ndf['dm'] = np.where(df.dm == '\\tno', 'no', df['dm'])\ndf['dm'] = np.where(df.dm == '\\tyes', 'yes', df['dm'])\ndf.dm.value_counts()","db5b5712":"df.cad.value_counts()","eb4a7242":"# Correcting the values of the variables dm and cad\ndf['cad'] = np.where(df.cad == '\\tno', 'no', df['cad'])\ndf.cad.value_counts()","a29692ef":"# See relation of HyperTension with age\nsns.swarmplot(x = 'htn', y = 'age', data = df)","8aff3119":"# Relation of dm with blood glucose variable can show some trend\ng = sns.FacetGrid(df, hue = 'dm', aspect = 2)\ng.map(sns.kdeplot, 'bgr')\ng.add_legend()","e6d93298":"# Distribution of dm with su(sugar)\n\nsns.countplot(x = 'su', hue = 'dm', data = df)","1d9bc2d9":"# Distribution of Appetite, Pedal Edema, Anemia\n\nplt.figure(figsize = (15,4))\nplt.subplot(1,3,1)\nsns.countplot(x = 'appet', data = df_cat)\nplt.title('Appetite')\nplt.subplot(1,3,2)\nsns.countplot(x = 'pe', data = df_cat)\nplt.title('Pedal Edema')\nplt.subplot(1,3,3)\nsns.countplot(x = 'ane', data = df_cat)\nplt.title('Anemia')","e071a81b":"#Anemia relation with red blood cell counts and hemoglobin level\nplt.figure(figsize = (10,4))\nplt.subplot(1,2,1)\nsns.countplot(x = 'ane', hue = 'rbc', data = df)\nplt.title('Anemia vs Red BC Counts')\nplt.subplot(1,2,2)\nsns.swarmplot(x = 'ane', y = 'hemo', data = df)\nplt.title('Anemia vs Hemo')","565f9e50":"#Target variable distribution\ndf.classification.value_counts().plot(kind = 'bar')\nplt.title('Classification')","2f1f059d":"##Rectifying some variables to make Bivariate analysis better\ndf['rc'] = df['rc'].replace({'\\t?' : np.nan})\ndf['wc'] = df['wc'].replace({'\\t?' : np.nan , '\\t8400' : 8400})\n\ndf['dm'] = np.where(df.dm == '\\tno', 'no', df['dm'])\ndf['dm'] = np.where(df.dm == '\\tyes', 'yes', df['dm'])\ndf.dm.value_counts()\n\n# Correcting the values of the variables dm and cad\ndf['cad'] = np.where(df.cad == '\\tno', 'no', df['cad'])\ndf.cad.value_counts()\n\ndf.rc = df.rc.astype(float)\ndf.wc = df.wc.astype(float)","8082882f":"plt.figure(figsize = (15,15))\nfor col in df_cat.drop(['rc','wc','classification'], axis = 1).columns:\n  plt.subplot(4,3,df_cat.columns.get_loc(col)+1)\n  sns.countplot(col, hue = 'classification', data = df)\n","66b86e61":"# Dropping discrete numeric variables and Dependent variable from Pairplot \nsns.pairplot(data = df.drop(['id','su','al','sg'], axis = 1), \n             hue = 'classification', \n             corner = True, height = 4)","1cdf0bbf":"# Relation of sc with Ckd can show some trend\ng = sns.FacetGrid(df, hue = 'classification', aspect = 2)\ng.map(sns.kdeplot, 'sc')\ng.add_legend()","c19a25c0":"# Plotting discrete variable with classification\nvar = ['su','al','sg']\nfor var in df[var].columns:\n  plt.subplots(1,1)\n  sns.countplot(var, hue = 'classification', data = df)","ed82dde2":"# Correlation Matrix\nplt.figure(figsize = (10,10))\nsns.heatmap(df.corr(), annot = True)","e0413a4e":"# Spearman Correlation \nplt.figure(figsize = (10,10))\nsns.heatmap(df.select_dtypes(exclude = 'object').corr(method = 'spearman'), annot = True)","359c35dc":"# Count of Missing Values\ndf.isnull().sum()","c8163306":"# Scatter of age with bp\nsns.scatterplot(x = 'age', y = 'bp', data = df)","6798b920":"df_m = df.copy()\ndf_m = df_m.drop(['id'], axis = 1)","928c1d30":"for col in df_m.columns:\n  if df_m[col].dtypes == 'object':\n    df_m[col] = df_m[col].fillna(2)\n\n\nfor col in df_m.columns:\n  if df_m[col].dtypes != 'object':\n    df_m[col] = df_m[col].fillna(999)","dee37e5e":"df_m.rbc = df_m.rbc.replace({'normal':0, 'abnormal':1})\ndf_m.pc = df_m.pc.replace({'normal':0, 'abnormal' : 1})\ndf_m.pcc = df_m.pcc.replace({'notpresent':0, 'present' : 1})\ndf_m.ba = df_m.ba.replace({'notpresent':0, 'present' : 1})\ndf_m.htn = df_m.htn.replace({'no':0,'yes' : 1})\ndf_m.dm = df_m.dm.replace({'no': 0,'yes' : 1})\ndf_m.cad = df_m.cad.replace({'no':0,'yes' : 1})\ndf_m.appet = df_m.appet.replace({'good':0,'poor' : 1})\ndf_m.pe = df_m.pe.replace({'no':0,'yes' : 1})\ndf_m.ane = df_m.ane.replace({'no':0,'yes' : 1})\ndf_m.classification = df_m.classification.replace({'notckd':0,'ckd' : 1})","5e1f3de0":"#Imptration for age\nx_train = df_m[df_m.age != 999].drop(['age'], axis = 1)\ny_train = df_m['age'][df_m.age != 999]\nx_test = df_m[df_m.age == 999].drop(['age'], axis = 1)\ny_test = df_m['age'][df_m.age == 999]","58b54921":"x_test","988221d3":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nrf = RandomForestRegressor(n_estimators = 100, max_features = 10, random_state = 1)\nmodel = rf.fit(x_train, y_train)\nnp.sqrt(mean_squared_error(y_train, model.predict(x_train)))\nr2_score(y_train, model.predict(x_train))\n","e8ccd967":"# imp = pd.Series(model.feature_importances_)\n# imp.index = x_train.columns\n# imp.sort_values(ascending = False)","a3c06b8d":"model.predict(x_test)","c7a03826":"from sklearn.neighbors import KNeighborsRegressor\nscore = []\nfor k in range(1,50,1):\n  rf = KNeighborsRegressor(n_neighbors= k)\n  model2 = rf.fit(x_train, y_train)\n  np.sqrt(mean_squared_error(y_train, model2.predict(x_train)))\n  score.append(r2_score(y_train, model2.predict(x_train)))\n\n\nplt.plot(range(1,50,1), score)","765c0ffc":"# Filling age value from Random Forest Regressor model values\ndf.age.fillna({81 : 57, 91: 36, 95: 53, 247: 57, 257: 56}, \n              axis = 0, inplace = True)","9744b742":"# Filling missing value of bp. We saw no particular variable had good correlation with bp.\n# Highest was pcv with a -0.32 value. So it would be better if we use Regressor as we did for age\n\nx_train = df_m[df_m.bp != 999].drop(['bp'], axis = 1)\ny_train = df_m['bp'][df_m.bp != 999]\nx_test = df_m[df_m.bp == 999].drop(['bp'], axis = 1)\ny_test = df_m['bp'][df_m.bp == 999]","606972fc":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nrf = RandomForestRegressor(n_estimators = 50, max_features = 10, random_state = 1)\nmodel = rf.fit(x_train, y_train)\nnp.sqrt(mean_squared_error(y_train, model.predict(x_train)))\nr2_score(y_train, model.predict(x_train))","06fa128d":"model.predict(x_test)","0433f123":"df['bp'] = df['bp'].fillna({38 : 71, 89: 71, 101: 73, 169: 67, 183 : 74, 209: 81, \n                            246: 74, 258: 76, 274: 66}\n                , axis = 0)\n","ff5485f6":"df[df.dm.isnull()]","eb6ddde7":"df['htn'].fillna(df.htn.mode()[0], inplace = True)\ndf['cad'].fillna(df.cad.mode()[0], inplace = True)\ndf['dm'].fillna(df.dm.mode()[0], inplace = True)","e5240a92":"df[df.dm.isnull()]","40d849f2":"# Concurrence of missing values\ndf[df.su.isnull()]","de104740":"x_train = df_m[['bgr','dm']][df_m.su != 999]\ny_train = df_m['su'][df_m.su != 999]\nx_test = df_m[['bgr','dm']][df_m.su == 999]\ny_test = df_m['su'][df_m.su == 999]","56d1bec6":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nrf = RandomForestRegressor(n_estimators = 50, random_state = 1)\nmodel = rf.fit(x_train, y_train)\nnp.sqrt(mean_squared_error(y_train, model.predict(x_train)))\nr2_score(y_train, model.predict(x_train))","8d76c969":"model.predict(x_test)","5307f219":"import pandas as pd\nseries = pd.Series(model.predict(x_test))\nseries.index = x_test.index\nseries = np.round(series)\ndf2 = series\ndf_joint = pd.concat([x_test, df2], axis = 1)\ndf_joint = df_joint.rename(columns = {df_joint.columns[2] : 'su'})","5f52d9b6":"df_joint.su.value_counts()","f1fdb816":"df.loc[df.su.isnull(), 'su'] = df_joint.loc[:,'su']","dbba4d5b":"# Values of Sugar Column after Imputation\ndf.su.value_counts()","00fe555b":"# We can see that for values greater than 140 , the bgr is taking 1 , 2, 3 as status. So\n\nplt.subplots(1,1)\n# sns.distplot(df['bgr'][df.su == 0], kde = False, label = 0, color = 'Blue')\nsns.distplot(df['bgr'][df.su == 1], kde = False, label = 1, color = 'Red')\nsns.distplot(df['bgr'][df.su == 2], kde = False, label = 2, color = 'Orange')\nsns.distplot(df['bgr'][df.su == 3], kde = False, label = 3, color = 'Yellow')\n# sns.distplot(df['bgr'][df.su == 4], kde = False)\nplt.legend()","e9dddb06":"df.al.isnull().sum()","924f34a1":"# Creating train and test split for al variable\nx_train = df_m[['rc','pcv','hemo','sc']][df_m.al != 999]\ny_train = df_m['al'][df_m.al != 999]\nx_test = df_m[['rc','pcv','hemo','sc']][df_m.al == 999]\ny_test = df_m['al'][df_m.al == 999]","e518819b":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nrf = RandomForestRegressor(n_estimators = 51, random_state = 1)\nmodel = rf.fit(x_train, y_train)\nnp.sqrt(mean_squared_error(y_train, model.predict(x_train)))\nr2_score(y_train, model.predict(x_train))","9824b14b":"series = pd.Series(model.predict(x_test))\nseries.index = x_test.index\nseries = np.round(series)\ndf2 = series\ndf_joint = pd.concat([x_test, df2], axis = 1)\ndf_joint = df_joint.rename(columns = {df_joint.columns[4] : 'al'})\ndf.loc[df.al.isnull(), 'al'] = df_joint.loc[:,'al']","51834a46":"# Imputation tried using KNN - FAILED\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nx_train_scaled = StandardScaler().fit_transform(x_train)\nscore = []\nfor k in range(1,10,1):\n  rf = KNeighborsClassifier(n_neighbors= k)\n  model2 = rf.fit(x_train_scaled, y_train)\n  score.append(accuracy_score(y_train, model2.predict(x_train_scaled)))\n\n\nplt.plot(range(1,10,1), score)","6c073d90":"# Imputation using KNN IMPUTER - Failed\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import KNNImputer\nx_train_scaled = StandardScaler().fit_transform(x_train)\nscore = []\nfor k in range(1,10,1):\n  model2 = KNNImputer(n_neighbors = k, weights = 'distance')\n  model2 = rf.fit(x_train_scaled, y_train)\n  score.append(accuracy_score(y_train, model2.predict(x_train_scaled)))\n\n\nplt.plot(range(1,10,1), score)","be234830":"df.sg.isnull().sum()","6884b1bb":"df.sg.value_counts()","e84dc2b6":"# PCV and Hemo, rc and sod have high corelation with this variable\n\n\n# Creating train and test split for al variable\nx_train = df_m[['rc','pcv','hemo','sc']][df_m.sg != 999]\ny_train = df_m['sg'][df_m.sg != 999]\nx_test = df_m[['rc','pcv','hemo','sc']][df_m.sg == 999]\ny_test = df_m['sg'][df_m.sg == 999]","87dd95ee":"# Running Random Forest Regressor Model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nrf = RandomForestRegressor(n_estimators = 51, random_state = 1)\nmodel = rf.fit(x_train, y_train)\nnp.sqrt(mean_squared_error(y_train, model.predict(x_train)))\nr2_score(y_train, model.predict(x_train))","89de1322":"a = list(np.round(model.predict(x_test),3))\na = [round(round(b\/0.005)*0.005,3) for b in a]","375941ed":"# Imputing the values\na = pd.Series(a)\na.index = x_test.index\ndf_joint = pd.concat([x_test, a], axis = 1)\ndf_joint = df_joint.rename(columns = {df_joint.columns[4] : 'sg'})\ndf.loc[df.sg.isnull(), 'sg'] = df_joint.loc[:,'sg']","5257340f":"df.sc.isnull().sum()","7b9afd1e":"# Replacing with mode\ndf['sc'].fillna(df['sc'].mode()[0], inplace = True)","a47a417d":"df.pc.isnull().sum()","5131c1ca":"observed = pd.crosstab(df['pcc'], df['pc'])","65ace263":"# Calculating CramerV\nfrom scipy.stats import chi2_contingency\nchi_stats = chi2_contingency(observed)[0]\nn = np.sum(observed).sum()\ndof = np.min([observed.shape[0],observed.shape[1]]) - 1 \ncramerv = np.sqrt(chi_stats\/(n * dof))\ncramerv","c84e568b":"# Removing the over approximation in CramerV Statistics\ndef cramers_v(x, y):\n    import scipy.stats as ss\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n    rcorr = r-((r-1)**2)\/(n-1)\n    kcorr = k-((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr\/min((kcorr-1),(rcorr-1)))\n  \n\ncramers_v(df['pcc'], df['pc'])","ea787dca":"# Imputing the missing values of pc as per pcc\ndf['pc'][((df.pcc == 'notpresent') & (df.pc.isnull()))] = 'normal'\ndf['pc'][((df.pcc == 'present') & (df.pc.isnull()))] = 'abnormal'","b18b1ef1":"# One imputation of pc is left, as pcc also has a null value there.\ndf[df.pc.isnull()]","d493142a":"df['pc'][df.pc.isnull()] = 'normal'","fd9b846d":"df[df.pcc.isnull()]","0cca6c54":"# Imputing with not present\ndf.loc[df.pcc.isnull(),'pcc'] = 'notpresent'","ba0632c8":"# Creating train and test split for al variable\nx_train = df[['sc','al','sg']][~df.pcv.isnull()]\ny_train = df['pcv'][~df.pcv.isnull()]\nx_test = df[['sc','al','sg']][df.pcv.isnull()]\ny_test = df['pcv'][df.pcv.isnull()]","d8dd90ed":"# Running Random Forest Regressor Model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nrf = RandomForestRegressor(n_estimators = 51, random_state = 1)\nmodel = rf.fit(x_train, y_train)\nnp.sqrt(mean_squared_error(y_train, model.predict(x_train)))\nr2_score(y_train, model.predict(x_train))","7d50d5b6":"model.predict(x_test)","a8d83cc6":"# Imputing the values\na = pd.Series(model.predict(x_test))\na.index = x_test.index\ndf_joint = pd.concat([x_test, a], axis = 1)\ndf_joint = df_joint.rename(columns = {df_joint.columns[3] : 'pcv'})\ndf.loc[df.pcv.isnull(), 'pcv'] = df_joint.loc[:,'pcv']","b2256275":"df.groupby(['sc','sg','al'])['pcv'].agg(['mean','median'])","a26b959c":"# Creating train and test split for Hemo variable\nx_train = df[['pcv']][~df.hemo.isnull()]\ny_train = df['hemo'][~df.hemo.isnull()]\nx_test = df[['pcv']][df.hemo.isnull()]\ny_test = df['hemo'][df.hemo.isnull()]","b1ca30af":"# Running Random Forest Regressor Model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nrf = LinearRegression()\nmodel = rf.fit(x_train, y_train)\nnp.sqrt(mean_squared_error(y_train, model.predict(x_train)))\nr2_score(y_train, model.predict(x_train))","f63ad045":"model.coef_, model.intercept_","351d4d03":"# Imputing the values\na = pd.Series(model.predict(x_test))\na.index = df[df.hemo.isnull()].index\ndf_joint = pd.DataFrame(a, columns= ['hemo'])\ndf.loc[df.hemo.isnull(), 'hemo'] = df_joint.loc[:,'hemo']","6b6123dc":"# Creating train and test split for Hemo variable\nx_train = df[['pcv','hemo']][~df.rc.isnull()]\ny_train = df['rc'][~df.rc.isnull()]\nx_test = df[['pcv','hemo']][df.rc.isnull()]\ny_test = df['rc'][df.rc.isnull()]","35bf604b":"# Running Linear Regressor Model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nrf = LinearRegression()\nmodel = rf.fit(x_train, y_train)\nnp.sqrt(mean_squared_error(y_train, model.predict(x_train)))\nr2_score(y_train, model.predict(x_train))","04de818f":"# Running Random Forest Regressor Model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nrf = RandomForestRegressor(n_estimators = 50, random_state = 1)\nmodel = rf.fit(x_train, y_train)\nnp.sqrt(mean_squared_error(y_train, model.predict(x_train)))\nr2_score(y_train, model.predict(x_train))","7d4216e0":"plt.subplots(1,1)\nsns.distplot(pd.Series(model.predict(x_test)), color = 'Yellow')\nsns.distplot(df.rc, color = 'Orange')","6a2ef57d":"# Imputing the values\na = pd.Series(model.predict(x_test))\na.index = df[df.rc.isnull()].index\ndf_joint = pd.DataFrame(a, columns= ['rc'])\ndf.loc[df.rc.isnull(), 'rc'] = df_joint.loc[:,'rc']","d77ea0b0":"df.rbc.value_counts(dropna = False)","3a3687b4":"sns.boxplot('rc', 'rbc', data = df)","ab73ef81":"# Calculating PointBiserial Corelation bwteen rbc anc rc\n\nfrom scipy.stats import pointbiserialr\ndf_pbr = df[~((df.rc.isnull()) | (df.rbc.isnull()))]\ndf_pbr.rbc = df_pbr.rbc.replace({'normal' : 0, 'abnormal' : 1})\nstats = pointbiserialr(df_pbr['rbc'], df_pbr['rc'])\nstats","de80e0cb":"fig, ax = plt.subplots(1,2, figsize = (10,3))\nsns.boxplot(df['hemo'], df['rbc'], ax = ax[0])\nsns.boxplot(df['pcv'], df['rbc'], ax = ax[1])","4b5dea52":"from scipy.stats import pointbiserialr\ndf_pbr = df[~((df.hemo.isnull()) | (df.rbc.isnull()))]\ndf_pbr.rbc = df_pbr.rbc.replace({'normal' : 0, 'abnormal' : 1})\nstats = pointbiserialr(df_pbr['hemo'], df_pbr['rbc'])\nstats","998570a1":"from scipy.stats import pointbiserialr\ndf_pbr = df[~((df.pcv.isnull()) | (df.rbc.isnull()))]\ndf_pbr.rbc = df_pbr.rbc.replace({'normal' : 0, 'abnormal' : 1})\nstats = pointbiserialr(df_pbr['rbc'], df_pbr['pcv'])\nstats","bd0511aa":"df[['rbc','hemo','pcv']][df.rbc.isnull()].isnull().all(axis = 1).sum()","4bb8d126":"df_new = df[['rbc','hemo','pcv']]\ndf_new.pcv_1 = np.where(df.pcv > 38, 0, 1)\ndf_new.hemo_1 = np.where(df.hemo > 12, 0, 1)\npd.crosstab(df_new.pcv_1, df_new.hemo_1, values = df_new.rbc, aggfunc = 'count')","86521124":"df['rbc'][df.rbc.isnull()] = np.where(df.hemo < 12 , 'abnormal', 'normal')","d405ef80":"# Spearman Correlation \nplt.figure(figsize = (10,10))\nsns.heatmap(df.select_dtypes(exclude = 'object').corr(method = 'spearman'), annot = True)","d64798e0":"sns.boxplot('su', 'bgr', data = df)","92ecb106":"# Creating train and test split for Bgr variable\nx_train = df[['su','al','sc']][~df.bgr.isnull()]\ny_train = df['bgr'][~df.bgr.isnull()]\nx_test = df[['su','al','sc']][df.bgr.isnull()]\ny_test = df['bgr'][df.bgr.isnull()]","f3f565b1":"# Running Random Forest Regressor Model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nrf = RandomForestRegressor(n_estimators = 50, random_state = 1)\nmodel = rf.fit(x_train, y_train)\nnp.sqrt(mean_squared_error(y_train, model.predict(x_train)))\nr2_score(y_train, model.predict(x_train))","44591b71":"plt.subplots(1,1)\nsns.distplot(pd.Series(model.predict(x_test)), color = 'Yellow')\nsns.distplot(df.bgr, color = 'Orange')","28b65a85":"# Imputing the values\na = pd.Series(model.predict(x_test))\na.index = df[df.bgr.isnull()].index\ndf_joint = pd.DataFrame(a, columns= ['bgr'])\ndf.loc[df.bgr.isnull(), 'bgr'] = df_joint.loc[:,'bgr']","68d89f4e":"df[df.ba.isnull()]","e11eeae6":"df['classification'][df.ba == 'present'].value_counts()","c60b4763":"df.loc[df.ba.isnull(),'ba'] = 'notpresent'","3a8aba89":"# Creating train and test split for Hemo variable\nx_train = df[['pcv','al','sc']][~df.bu.isnull()]\ny_train = df['bu'][~df.bu.isnull()]\nx_test = df[['pcv','al','sc']][df.bu.isnull()]\ny_test = df['bu'][df.bu.isnull()]","f3f19d5c":"# Running Random Forest Regressor Model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nrf = RandomForestRegressor(n_estimators = 50, random_state = 1)\nmodel = rf.fit(x_train, y_train)\nnp.sqrt(mean_squared_error(y_train, model.predict(x_train)))\nr2_score(y_train, model.predict(x_train))","dae2dc84":"# Imputing the values\na = pd.Series(model.predict(x_test))\na.index = df[df.bu.isnull()].index\ndf_joint = pd.DataFrame(a, columns= ['bu'])\ndf.loc[df.bu.isnull(), 'bu'] = df_joint.loc[:,'bu']","5f75ff50":"df.rbc = df.rbc.replace({'normal':0, 'abnormal':1})\ndf.pc = df.pc.replace({'normal':0, 'abnormal' : 1})\ndf.pcc = df.pcc.replace({'notpresent':0, 'present' : 1})\ndf.ba = df.ba.replace({'notpresent':0, 'present' : 1})\ndf.htn = df.htn.replace({'no':0,'yes' : 1})\ndf.dm = df.dm.replace({'no': 0,'yes' : 1})\ndf.cad = df.cad.replace({'no':0,'yes' : 1})\ndf.appet = df.appet.replace({'good':0,'poor' : 1})\ndf.pe = df.pe.replace({'no':0,'yes' : 1})\ndf.ane = df.ane.replace({'no':0,'yes' : 1})\ndf.classification = df.classification.replace({'notckd':0,'ckd' : 1})","0c4c762b":"# Creating train and test split for  variable\nx_train = df.drop(['sod','wc','pot'], axis = 1)[~df.pot.isnull()]\ny_train = df['pot'][~df.pot.isnull()]\nx_test = df.drop(['sod','wc','pot'], axis = 1)[df.pot.isnull()]\ny_test = df['pot'][df.pot.isnull()]","991e0410":"y_train.shape","51eeb449":"# Imputation using KNN Regressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import KNNImputer\nx_train_scaled = StandardScaler().fit_transform(x_train)\nx_test_scaled = StandardScaler().fit_transform(x_test)\nscore = []\nfor k in range(1,5,1):\n  model2 = KNeighborsRegressor(n_neighbors = k)\n  model2.fit(x_train_scaled, y_train)\n  np.sqrt(mean_squared_error(y_train, model2.predict(x_train_scaled)))\n  score.append(r2_score(y_train, model2.predict(x_train_scaled)))\n\n\nplt.plot(range(1,5,1), score)\n  ","4faa21b0":"model2 = KNeighborsRegressor(n_neighbors = 2)\nx_train_scaled = StandardScaler().fit_transform(x_train)\nx_test_scaled = StandardScaler().fit_transform(x_test)\nmodel2.fit(x_train_scaled, y_train)\na = pd.Series(model2.predict(x_test_scaled))\na.index = df[df.pot.isnull()].index\ndf_joint = pd.DataFrame(a, columns= ['pot'])\ndf.loc[df.pot.isnull(), 'pot'] = df_joint.loc[:,'pot']","7c17cc58":"# Creating train and test split for Bgr variable\nx_train = df.drop(['sod','wc'], axis = 1)[~df.wc.isnull()]\ny_train = df['wc'][~df.wc.isnull()]\nx_test = df.drop(['sod','wc'], axis = 1)[df.wc.isnull()]\ny_test = df['wc'][df.wc.isnull()]","74dff0fe":"# Imputation using KNN Regressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import KNNImputer\nx_train_scaled = StandardScaler().fit_transform(x_train)\nx_test_scaled = StandardScaler().fit_transform(x_test)\nscore = []\nfor k in range(1,5,1):\n  model2 = KNeighborsRegressor(n_neighbors = k)\n  model2.fit(x_train_scaled, y_train)\n  np.sqrt(mean_squared_error(y_train, model2.predict(x_train_scaled)))\n  score.append(r2_score(y_train, model2.predict(x_train_scaled)))\n\n\nplt.plot(range(1,5,1), score)\n  ","30109b49":"# Running Random Forest Regressor Model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nrf = RandomForestRegressor(n_estimators = 50, random_state = 1)\nmodel = rf.fit(x_train, y_train)\nnp.sqrt(mean_squared_error(y_train, model.predict(x_train)))\nr2_score(y_train, model.predict(x_train))","3ce8c8cf":"a = pd.Series(model.predict(x_test))\na.index = df[df.wc.isnull()].index\ndf_joint = pd.DataFrame(a, columns= ['wc'])\ndf.loc[df.wc.isnull(), 'wc'] = df_joint.loc[:,'wc']","fee5778d":"from sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors = 3)\ndf_filled = imputer.fit_transform(df)\ndf_filled = pd.DataFrame(df_filled, columns = df.columns)\ndf = df_filled.copy()","321361a1":"df.isnull().sum()","1a6280da":"# EDA Numeric Variable\n\nfor col in df.columns:\n  fig, ax = plt.subplots(1,1, figsize = (5,3))\n  sns.boxplot(col, data = df, orient = 'v')","60b4114c":"# from feature_engine.discretisers import DecisionTreeDiscretiser\n# from sklearn.model_selection import train_test_split\n# disc = DecisionTreeDiscretiser(cv = 5, scoring = 'accuracy',\n#                                       variables = ['bp'], param_grid = {'max_depth' : [1,2,3]}\n#                                       , regression = False, random_state = 1, )\n\n# X_train, X_test, y_train, y_test =  train_test_split(\n#             df.drop(['id','classification'], axis = 1),\n#             df['classification'], test_size=0.3, random_state=0)\n\n# disc.fit(X_train, y_train)\n# df.disc_bp = disc.transform(df['bp'])","2f2707c5":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test =  train_test_split(\n            df[['bp']], df['classification'], test_size=0.3, random_state=0)\n\ntree_model = DecisionTreeClassifier(max_depth=2)\ntree_model.fit(X_train, y_train)\nX_train['bp_tree']=tree_model.predict_proba(X_train)[:,1] \nX_train.head(10)","b143097b":"df['bp_o'] = tree_model.predict_proba(df[['bp']])[:,1]","107625f7":"df['bgr_o'] = np.log(df['bgr']) \nsns.boxplot(np.log(df['bgr']))","a7fc7dd2":"df['bu_o'] = np.log(df['bu'])\nsns.boxplot(np.log(df['bu']))","662a218d":"# df['sc'].idxmax()\n# df_sc = df['sc'].copy()\n# df_sc[df_sc.index == 253]\n# iqr = (1.5*(df_sc.quantile(0.75) - df_sc.quantile(0.25)) + df_sc.quantile(0.75))","185dfeb6":"sns.boxplot(df['sc'])","7c1e4b51":"from sklearn.preprocessing import KBinsDiscretizer\ndiscretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='kmeans')\ndiscretizer.fit(df[['sc']])\ndiscretizer.transform(df[['sc']])\npd.concat((pd.DataFrame(discretizer.transform(df[['sc']]), index = df.index, columns = ['sc_disc']), df['sc']), \n          axis = 1).groupby(['sc_disc'])['sc'].agg({'max','min'})\n# pd.concat(pd.Series(discretizer.transform(df[['sc']])), df['sc'])","fb08f75b":"df['sc_o'] = discretizer.transform(df[['sc']])","23fc665d":"disodretizer = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='kmeans')\ndisodretizer.fit(df[['sod']])\ndisodretizer.transform(df[['sod']])\npd.concat((pd.DataFrame(disodretizer.transform(df[['sod']]), index = df.index, columns = ['sod_disod']), df['sod']), \n          axis = 1).groupby(['sod_disod'])['sod'].agg({'max','min'})\ndf['sod_o'] = disodretizer.transform(df[['sod']])","fdcf9617":"ub_pot = (1.5*(df['pot'].quantile(0.75) - df['pot'].quantile(0.25)) + df['pot'].quantile(0.75))\nsns.boxplot(np.where(df['pot'] > ub_pot, ub_pot, df['pot']))\ndf['pot_o'] =  np.where(df['pot'] > ub_pot, ub_pot, df['pot'])","e479d391":"from scipy import stats\nsns.boxplot(stats.boxcox(df['wc'])[0])\ndf['wc_o'] = stats.boxcox(df['wc'])[0]","40a06066":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\ndf_pca = df[['rc','hemo','pcv']]\nscale = StandardScaler()\ndf_pca_scaled = scale.fit_transform(df_pca)\npca = PCA(n_components = 3)\narray_pca = pca.fit_transform(df_pca_scaled)\ndf_pca_done = pd.DataFrame(data = array_pca, columns = ['pc1','pc2','pc3'])\npca.explained_variance_ratio_","a830d71c":"plt.plot(range(3),pca.explained_variance_ratio_)","d3837734":"df.columns","465e2cdc":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 50, random_state = 1)\nrf.fit(df.drop(['id','classification'], axis = 1), df.classification)\nimp = pd.Series(rf.feature_importances_)\nimp.index = df.drop(['id','classification'], axis = 1).columns\nimp.sort_values(ascending = False)","21054ea6":"from sklearn.linear_model import Lasso, LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.preprocessing import StandardScaler\nX_train, X_test, y_train, y_test = train_test_split(\n    df.drop(labels=['classification', 'id'], axis=1),\n    df['classification'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape","fba5556e":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)","50e03483":"sel_ = SelectFromModel(LogisticRegression(C=1, penalty='l1', solver = 'liblinear'))\nsel_.fit(X_train, y_train)","be502a17":"selected_feat = df.drop(['id','classification'], axis = 1).columns[(sel_.get_support())]\nselected_feat","5ba94c02":"print('total features: {}'.format((X_train.shape[1])))\nprint('selected features: {}'.format(len(selected_feat)))\nprint('features with coefficients shrank to zero: {}'.format(\n      np.sum(sel_.estimator_.coef_ == 0)))","2b11acbe":"X_train, X_test, y_train, y_test = train_test_split(\n    df.drop(labels=['classification', 'id'], axis=1),\n    df['classification'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape","fc9dfd9b":"# Building a Basic Random Forest Model to record base ROC-AUC score\nfrom sklearn.metrics import roc_auc_score\nrf = RandomForestClassifier(\n    n_estimators=100, max_depth = 2, random_state=1)\n \nrf.fit(X_train, y_train)\n \n# print roc-auc in train and testing sets\nprint('train auc score: ',\n      roc_auc_score(y_train, (rf.predict_proba(X_train)[:, 1])))\nprint('test auc score: ',\n      roc_auc_score(y_test, (rf.predict_proba(X_test)[:, 1])))","e6b3c838":"from sklearn.metrics import confusion_matrix\nsns.heatmap(confusion_matrix(y_test, rf.predict(X_test)), annot = True)","ce465804":"# overall train roc-auc: using all the features\ntrain_auc = roc_auc_score(y_train, (rf.predict_proba(X_train)[:, 1]))\n \n# dictionary to capture the features and the drop in auc that they\n# cause when shuffled\nfeature_dict = {}\n \n# selection  logic\nfor feature in X_train.columns:\n    X_train_c = X_train.copy()\n    \n    # shuffle individual feature\n    X_train_c[feature] = X_train_c[feature].sample(frac=1, random_state = 1).reset_index(\n        drop=True)\n    \n    # make prediction with shuffled feature and calculate roc-auc\n    shuff_auc = roc_auc_score(y_train,\n                              (rf.predict_proba(X_train_c.fillna(0)))[:, 1])\n    \n    # save the drop in roc-auc\n    feature_dict[feature] = (train_auc - shuff_auc)","0f079dfd":"feature_importance = pd.Series(feature_dict).reset_index()\nfeature_importance.columns = ['feature', 'auc_drop']\nfeature_importance.auc_drop = np.round(feature_importance.auc_drop, 5)\nfeature_importance","ec5dc624":"# Dividing into Training and Test set \nX_train, X_test, y_train, y_test = train_test_split(\n    df.drop(['wc','bgr','bu','sod','sc','pot','bp','classification'], axis = 1),\n    df['classification'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape","9feea3b4":"from sklearn.metrics import classification_report\nLogR = LogisticRegression(random_state = 1, \n                            verbose = 1)\nLogR.fit(X_train, y_train)\ny_train_pred = LogR.predict(X_train)\ny_test_pred = LogR.predict(X_test)\nprint('\\n')\nprint('Classification Report for Train Set\\n')\nprint(classification_report(y_train, y_train_pred))\nprint('Classification Report for Test Set\\n')\nprint(classification_report(y_test, y_test_pred))","f05bda32":"from sklearn.metrics import confusion_matrix\nfig, ax = plt.subplots(1,2, figsize = (10,4))\nsns.heatmap(confusion_matrix(y_train, y_train_pred), annot = True, fmt = 'd' ,ax = ax[0])\nax[0].set_title('Train Data')\nsns.heatmap(confusion_matrix(y_test, y_test_pred), annot = True, fmt = 'd', ax = ax[1])\nax[1].set_title('Test Data')","5990f3d4":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef calc_vif(X):\n\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n    return(vif)","474e24dc":"X = df.drop(['classification','sc','bp','bgr','bu','pot','sod','wc','id'], axis = 1)\ncalc_vif(X)","b324138f":"X = df.drop(['classification','sc','bp','bgr','bu','pot','sod','wc','id','sg'], axis = 1)\ncalc_vif(X)","ada0fe4d":"X = df.drop(['classification','sc','bp','bgr','bu','pot','sod',\n             'wc','id','rc','sg','pcv','wc_o', 'bgr_o', 'pot_o','sod_o','bu_o','bp_o'], axis = 1)\ncalc_vif(X)","415afda9":"X_train, X_test, y_train, y_test = train_test_split(\n    df.drop(['classification','sc','bp','bgr','bu','pot','sod',\n             'wc','id','rc','sg','pcv','wc_o', 'bgr_o', 'pot_o','sod_o','bu_o','bp_o'], axis = 1),\n    df['classification'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape\n","80c55127":"scale = StandardScaler()\nX_train_scaled = scale.fit_transform(X_train)\nX_test_scaled = scale.fit_transform(X_test)","5e33c2cf":"from sklearn.metrics import classification_report\nLogR = LogisticRegression(random_state = 1, \n                            verbose = 1)\nLogR.fit(X_train_scaled, y_train)\ny_train_pred = LogR.predict(X_train_scaled)\ny_test_pred = LogR.predict(X_test_scaled)\nprint('\\n')\nprint('Classification Report for Train Set\\n')\nprint(classification_report(y_train, y_train_pred))\nprint('Classification Report for Test Set\\n')\nprint(classification_report(y_test, y_test_pred))","becf80f2":"from sklearn.metrics import confusion_matrix\nfig, ax = plt.subplots(1,2, figsize = (10,4))\nsns.heatmap(confusion_matrix(y_train, y_train_pred), annot = True, fmt = 'd' ,ax = ax[0])\nax[0].set_title('Train Data')\nsns.heatmap(confusion_matrix(y_test, y_test_pred), annot = True, fmt = 'd', ax = ax[1])\nax[1].set_title('Test Data')","b88029bb":"feature_coef = pd.Series(np.exp(LogR.coef_[0]))\nfeature_coef.index = X_train.columns\nfeature_coef.sort_values(ascending = False)","b2cd070b":"# # Dividing into Training and Test set \n# X_train, X_test, y_train, y_test = train_test_split(df.drop(['id','wc','bgr','bu','sod','sc','pot','bp','classification'], axis = 1),\n#     df['classification'],\n#     test_size=0.3,\n#     random_state=0)\n# X_train.shape, X_test.shape\n# # Scaling before Ridge Regression\n# scaler = StandardScaler()\n# X_train_scaled = scaler.fit_transform(X_train)\n# X_test_scaled = scaler.fit_transform(X_test)\n\n# from sklearn.metrics import classification_report\n# modelRR = LogisticRegression(penalty = 'l2', C = 1, random_state = 1, \n#                              solver = 'liblinear', verbose = 1)\n# modelRR.fit(X_train_scaled, y_train)\n# y_train_pred = modelRR.predict(X_train_scaled)\n# y_test_pred = modelRR.predict(X_test_scaled)\n# print('\\n')\n# print(classification_report(y_train, y_train_pred))\n# print(classification_report(y_test, y_test_pred))","c29883f9":"# Dividing into Training and Test set \nX_train, X_test, y_train, y_test = train_test_split(df.drop(['classification'], axis = 1),\n    df['classification'],\n    test_size=0.3,\n    random_state=0)\n\n# Scaling before Ridge Regression\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.fit_transform(X_test)","8f89f35a":"from sklearn.metrics import classification_report\nmodelLR = LogisticRegression(penalty = 'l1', C = 1, random_state = 1, \n                             solver = 'liblinear', verbose = 1)\nmodelLR.fit(X_train_scaled, y_train)\ny_train_pred = modelLR.predict(X_train_scaled)\ny_test_pred = modelLR.predict(X_test_scaled)\nprint('\\n')\nprint(classification_report(y_train, y_train_pred))\nprint(classification_report(y_test, y_test_pred))","051ea139":"from sklearn.metrics import confusion_matrix\nfig, ax = plt.subplots(1,2, figsize = (10,4))\nsns.heatmap(confusion_matrix(y_train, y_train_pred), annot = True, fmt = 'd' ,ax = ax[0])\nax[0].set_title('Train Data')\nsns.heatmap(confusion_matrix(y_test, y_test_pred), annot = True, fmt = 'd', ax = ax[1])\nax[1].set_title('Test Data')","4bcd593f":"feature_coef = pd.Series(modelLR.coef_[0])\nfeature_coef.index = X_train.columns\nfeature_coef.sort_values(ascending = False)","35532591":"feature_coef = pd.Series(modelLR.coef_[0])\nfeature_coef.index = X_train.columns\nnp.exp(feature_coef[feature_coef != 0]).sort_values(ascending = False)","5b8f10d2":"df = df.drop(['wc','bgr','bu','sod','sc_o','pot','bp','id'], axis = 1)","6599d697":"from sklearn.tree import DecisionTreeClassifier\nX_train, X_test, y_train, y_test = train_test_split(df.drop(['classification'], axis = 1),\n    df['classification'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape\n","fa93527f":"tree = DecisionTreeClassifier(random_state = 1)\ntree.fit(X_train, y_train)\ny_train_pred = tree.predict(X_train)\ny_test_pred = tree.predict(X_test)\nprint('\\n')\nprint('Classification Report for Train Set\\n')\nprint(classification_report(y_train, y_train_pred))\nprint('Classification Report for Test Set\\n')\nprint(classification_report(y_test, y_test_pred))","5fa0ad56":"imp = pd.Series(data = tree.feature_importances_, index = X_train.columns)\nimp.sort_values( ascending = False)","fb5e8eb9":"from sklearn.tree import DecisionTreeClassifier\nX_train, X_test, y_train, y_test = train_test_split(df.loc[:,['sg','su','hemo']],\n    df['classification'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape","da1d6f32":"tree = DecisionTreeClassifier(random_state = 1)\ntree.fit(X_train, y_train)\ny_train_pred = tree.predict(X_train)\ny_test_pred = tree.predict(X_test)\nprint('\\n')\nprint('Classification Report for Train Set\\n')\nprint(classification_report(y_train, y_train_pred))\nprint('Classification Report for Test Set\\n')\nprint(classification_report(y_test, y_test_pred))","552dd10e":"from sklearn.ensemble import RandomForestClassifier\nX_train, X_test, y_train, y_test = train_test_split(df.drop(['classification'], axis = 1),\n    df['classification'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape","cec68631":"rf = RandomForestClassifier(n_estimators = 100, random_state = 1)\nrf.fit(X_train, y_train)\ny_train_pred = rf.predict(X_train)\ny_test_pred = rf.predict(X_test)\nprint('\\n')\nprint('Classification Report for Train Set\\n')\nprint(classification_report(y_train, y_train_pred))\nprint('Classification Report for Test Set\\n')\nprint(classification_report(y_test, y_test_pred))","ed79b3ff":"from sklearn.naive_bayes import GaussianNB\nX_train, X_test, y_train, y_test = train_test_split(df.drop(['classification'], axis = 1),\n    df['classification'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape","800dbf86":"NB = GaussianNB()\nNB.fit(X_train, y_train)\ny_train_pred = NB.predict(X_train)\ny_test_pred = NB.predict(X_test)\nprint('\\n')\nprint('Classification Report for Train Set\\n')\nprint(classification_report(y_train, y_train_pred))\nprint('Classification Report for Test Set\\n')\nprint(classification_report(y_test, y_test_pred))","7ec02ece":"from sklearn.svm import SVC\n\nX_train, X_test, y_train, y_test = train_test_split(df.drop(['classification'], axis = 1),\n    df['classification'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape\n","4d231013":"classifier = SVC(kernel = 'linear', random_state = 1)\nclassifier.fit(X_train, y_train)\ny_train_pred = classifier.predict(X_train)\ny_test_pred = classifier.predict(X_test)\nprint('\\n')\nprint('Classification Report for Train Set\\n')\nprint(classification_report(y_train, y_train_pred))\nprint('Classification Report for Test Set\\n')\nprint(classification_report(y_test, y_test_pred))","ffde79c5":"classifier = SVC(kernel = 'poly',degree = 2, random_state = 1)\nclassifier.fit(X_train, y_train)\ny_train_pred = classifier.predict(X_train)\ny_test_pred = classifier.predict(X_test)\nprint('\\n')\nprint('Classification Report for Train Set\\n')\nprint(classification_report(y_train, y_train_pred))\nprint('Classification Report for Test Set\\n')\nprint(classification_report(y_test, y_test_pred))","cd2d8cd9":"classifier = SVC(kernel = 'rbf', random_state = 1)\nclassifier.fit(X_train, y_train)\ny_train_pred = classifier.predict(X_train)\ny_test_pred = classifier.predict(X_test)\nprint('\\n')\nprint('Classification Report for Train Set\\n')\nprint(classification_report(y_train, y_train_pred))\nprint('Classification Report for Test Set\\n')\nprint(classification_report(y_test, y_test_pred))","307fd9ec":"By seeing the dataset, **we could not confirm** if the missing value belong to particular age group, showing some particular characterstics in term of other variables. \n\nOne interesting observation is that for the missing value of the 5 columns , **pcc and ba take only non present values**. But the **non present value dominate for these variables as can be confirmed from above describe statistics**.","ef26228e":"The Original Logistics Regression gave us a accuracy of 1 and even after decreasing our multicollinarity , the accuracy has not been substantially hit. \nWe still are getting a accuracy of 0.95 with test set","bb1ced7c":"Treating Outliers of bgr and bu\n\nWe will use Transformation here - as these have a continuous stream of Outliers. So capping should not be opt as this will restrict the natural value of the variables.","aaf98ba4":"We can see that most of the no status of diabetes concur with 0 as sugar level. So we can conclude that above 0 , there are more chances of dm.","2293750e":"Before building the model, we need to complete some required checks - especially of Multicollinearity.","6b5fdf07":"Though many variables are showing high collinearity. But 5 variables - sg, bgr_o, wc_o, hemo, pcv. These have too high values of VIF","96217d4c":"Observations: **To check the unique and how many categories**\n 1. We have **13 Categorical variables**\n 2. rbc - many missing values. 2 unique values\n 3. pc - 2 unique values, with normal dominating\n 4. **pcc, ba - 2 unique values, with notpresent occuring 90% of times**. These two can be related also\n 5. **wc, rc - It is a discrete numerical variable.** It indicates count of white and red blood cell. It is wrongly characterised as object rather it should be a numeric (int) variables. It could also point to some unusal character occurence in the variable as they are read as object.\n 6. htn - 2 unique vales\n 7. dm, cad - **Have 4 and 3 unique values, rather as per dictionary it should be haveing only yes or no.**\n 8. appet, pe, ane - nothing unusual\n 9. classification - Target variable. It seems rather **balanced** with 2 unique values.\n","6516798d":"We can see that our columns have **missing values**. \n**Classification** is our target variable.\nWe have **integer, float** as our numeric columns - They can store **continuous numeric, discrete numeric** and also **categorical variables**.\nWe also have **object columns** that store string values. They are majorly used to store categorical variables or character values.\n\nWe have a column **id with no missing values**. It can be a auto increment or unique identifier column. We will confirm this based on your future findings.\n\nTwo columns - **rc and wc** in cloumn list were numeric, but above they are taking object type. We need to check more into it.","72a3c6bc":"I tried dropping variables iteratively and then see how much the VIF value decreased. By going in a iterative and dropping variable with highest VIF - Many variables have to be dropped.\n\nVariables dropped - 'rc','sg','pcv','wc_o', 'bgr_o', 'pot_o','sod_o','bu_o','bp_o'","9ec9550b":"We can see that we can bring 90% of variance in one Principal Component. However, we also need to see how significant these variables are. This will help us to decide whether we want to keep original variables or the reduced Principal Component","548376ec":"Imputation for SG","3a26c58b":"We can see that some values have very high values of Blood glucose range which is abnormal. As **bgr > 200 tends to indicate diabetis** and we can also see all these cases are of chronic kidney disease.\nSo it would not be advisable to consider them as errorneous values and drop them. **They are outliers but do provide valuable information**","e3d25d29":"Id column is some form of **numeric identifier**. It **does not provide any qualitative information**. So I have decided to drop it and no further analysis will be done related to this variable.","02d6c486":"Imputing PCV ","f6f66fa8":"Imputing Value of PCC - Using values of PC","a10c6e8e":"Calculating Spearman correlation also, as we have some ordinal variables. So to see their relation in terms of spearman would make more sense.","97f94e3a":"Filling Values of pc and pcc. These two are related, so using their values to impute","08faa05f":"Quite amazing. Only 3 variables are contributiing to division and thus we can understand why we are getting the perfect accuracy for train and test data.","10cf94d1":"Using KMeans Discretizer for sc - as this way most of Outliers can be grouped. Most abnormal will be grouped in one, the Ouliers in between in other group.","3b2dc066":"We can see the separation **between the two classes of rbc on the rc variables.**","775f5085":"Imputing Potassium","8dfbc788":"We can see how well has our Discretizer performed.","207ff9ff":"Importing the required Dataset","56acc70a":"Imputing values of sc","1bd6a81e":"Our shuffling method could not prove anything conclusive.","83b2a9f8":"**rc and wc are numerical** variable with String values in it. So we should **visualise it as a continuous variable.**","ddf66812":"- Level above 0 of sugar (su) is a symptom of ckd.\n- Level above 0 of AL indicate ckd\n- 1.005, 1.01, 1.015 indicate presence of ckd. With values of 1.02 and 1.025, less chances of belonging to ckd category.","92a29208":"Above we see that we have all the variables, with Outlier Treatment and also Without outlier Treatment. This will help us to see whether our treatment helped to increase importance of that variable in prediction.","ce8cc444":"# Naive Bayes ","90f280b1":"From above we can see that the importance of most of the variables after transformation(var_o) has increased, except for sc . We need to work on this as we do not want to reduce importance of any variable.","9a564760":"- Seeing the data, we can see that this is the row with htn, dm, cad missing value.\n\n- We can see that this person is not ckd , so there are more chances of him being not having hypertension, coronary disease, Diabetes (As we see in categorical bar plots from above)\n\n- Also the blood sugar random (bgr) = 70, which is a normal value. So dm should take 'no' as a value.","ad52a246":"We can see that rbf and polynimal SVM perform poor than Linear SVM.","3ab9724f":"Treating the Outliers of WC - Through Transformation.\nBox - Cox Transformation will help in making the distribution more normal and also decreasing the effects of Outliers.","3bbda90c":"We are **not able to find relation** with any other columns of the missing values of the 5 columns that are considered above - sg, al, rbc, su, pc. We will try to impute them suitably. But it would be better to also make **a column to indicate the missing values of these 5 columns** - as they overlap and such a **column can capture the missingness of these 5 variables** in one go.","dea8ff81":"Imputing sod value using KNN Imputer","d3171f75":"Most Important Feature :\n- Class 1 Prediction - al, dm, bgr_o\n- Class 0 Prediction - sg, hemo","ce4aab5e":"Seeing the row with missing value of dm","cc8f048b":"# Objective\nThe data was taken over a 2-month period in India with 25 features ( eg, red blood cell count, white blood cell count, etc). The target is the 'classification', which is either 'ckd' or 'notckd' - ckd=chronic kidney disease. Use machine learning techniques to predict if a patient is suffering from a chronic kidney disease or not.","acbd01b7":"The data gives us a indication of predictive power of the coefficients.\nAl, dm, htn are the most important predictors for class = 1 (Chronic kidney Disease). They incline the odds in favour of Class 1.\nIncrease in Hemo changes the prediction more in favour of class = 0 (No Chronic Disease)","334167d4":"Importing Libraries","e9a39f82":"Building Decision Tree Model keeping only the three variables - hemo, sg, su","48c36a7c":"Point Biserial is showing weak Correlation between the dichatamous and Continuous variable","b6c3d3c4":"26 Columns\n\nAttribute Information:\n\nWe use 24 + class = 25 ( 11 numeric ,14 nominal)\n\n    Age(numerical) - age in years\n    Blood Pressure(numerical) - bp in mm\/Hg\n    Specific Gravity(nominal) - sg - (1.005,1.010,1.015,1.020,1.025)\n    Albumin(nominal) - al - (0,1,2,3,4,5)\n    Sugar(nominal) - su - (0,1,2,3,4,5)\n    Red Blood Cells(nominal) - rbc - (normal,abnormal)\n    Pus Cell (nominal) - pc - (normal,abnormal)\n    Pus Cell clumps(nominal) - pcc - (present,notpresent)\n    Bacteria(nominal) - ba - (present,notpresent)\n    Blood Glucose Random(numerical) - bgr in mgs\/dl\n    Blood Urea(numerical) -bu in mgs\/dl\n    Serum Creatinine(numerical) - sc in mgs\/dl\n    Sodium(numerical) - sod in mEq\/L\n    Potassium(numerical) - pot in mEq\/L\n    Hemoglobin(numerical) - hemo in gms\n    Packed Cell Volume(numerical)\n    White Blood Cell Count(numerical) - wc in cells\/cumm\n    Red Blood Cell Count(numerical) - rc in millions\/cmm\n    Hypertension(nominal) - htn - (yes,no)\n    Diabetes Mellitus(nominal) - dm - (yes,no)\n    Coronary Artery Disease(nominal) - cad - (yes,no)\n    Appetite(nominal) - appet - (good,poor)\n    Pedal Edema(nominal) - pe - (yes,no)\n    Anemia(nominal) - ane - (yes,no)\n    Class (nominal)- class - (ckd,notckd)\n\n","4bba8ee8":"Seeing that pc(pus cell) presenece is an indication of chronic kidney disease. So in above observation, status is notckd, so it should be normal pc and no present pcc.","234b3839":"Correcting dm and cad values - as we are more or less sure on how to impute them","1d98fe79":"id columns is irrelevant.\n\nAge has outliers but it has no unnatural value. So we will not be treating its Outliers. However we can bin it, so that age Outliers effect are reduced. But still for time being we will leave it as it is, due to presence of no large Outliers.\n\nSg, Al, Su have discrete Vaues - so  they do not have any Ouliers. They have only count\/frequency which does not signify any number range.\n\n","654fd8cb":"Building Model using Lasso Regression, as it will help to understand Coefficient Significance and also help in dropping less significant variables","3a35e87b":"All these value are very abnormal and show us presence of ckd. We can treat these outliers to more probable values or we can drop the rows. \nWe also need to confirm if this point to a pattern or just a random chance.","6d3e8692":"Some observations -\n- Linear relation between hemo and pcv. Lower PCV values -10-40 tend to lead to ckd.\n- For **hemo also the lower value below 10 leads to ckd. (assumed by us earlier, normal range hemo - above 10)**\n- For pot and sod there is not clear separation of ckd.\n-  Not clear about sc, need more details on it\n- For blood urea the values above 50 indicate presence of ckd\n- bgr above 150 indicates ckd. \n- Both very **low and high bp can be causes of ckd**. However high bp is a more prevalent cause\n- **Age older people are more susceptible to ckd**, but there are high overlapping also across all age groups.","483ff980":"Potassium has 3 Outliers - we will be capping these values","82c47b2a":"A neat **Gaussian Distribution**. We need to **take special care in the imputation of this variable** - as it has a large number of missing values, so simple imputation can really bring biasness in its distribution\n\nWe can also see the distribution of the variable with rbc - normal or abnormal. This can help us in analysing this feature more closely.","1d2fa9f9":"Building model using the Variables we got after treating Missing Values and Outliers","79466f3c":"Imputing Value of Al","d1c85920":"We can see a separation in the concentrated part for the two categories of rbc.\n\nHowever there is a issue, that rc itself has a lot of null values. So we have to think of some other way.","deebbd28":"# Decision Tree Model","a8f41e9f":"The Lasso Regression shrinked the coefficients of 19 features to zero. That is a significant reduction. But we need to see how well these features perform in term of Prediction results also.","9f17f1e6":"All four cases are with good health stats like good appetite, normal potassium and sodium, no heart disease. So it is better to fill bacteria as not present as presence of bacteria is a symptoym of chronic kidney disease.","43d92b16":"**0** is the most common occurence. SO the **imputation** can be done using it.","09b275ce":"Linear SVM can very well separate the data.","60e3f999":"Pasting Column Meaning again just for Quick References\n\nAge(numerical) - age in years\nBlood Pressure(numerical) - bp in mm\/Hg\nSpecific Gravity(nominal) - sg - (1.005,1.010,1.015,1.020,1.025)\nAlbumin(nominal) - al - (0,1,2,3,4,5)\nSugar(nominal) - su - (0,1,2,3,4,5)\nRed Blood Cells(nominal) - rbc - (normal,abnormal)\nPus Cell (nominal) - pc - (normal,abnormal)\nPus Cell clumps(nominal) - pcc - (present,notpresent)\nBacteria(nominal) - ba - (present,notpresent)\nBlood Glucose Random(numerical) - bgr in mgs\/dl\nBlood Urea(numerical) -bu in mgs\/dl\nSerum Creatinine(numerical) - sc in mgs\/dl\nSodium(numerical) - sod in mEq\/L\nPotassium(numerical) - pot in mEq\/L\nHemoglobin(numerical) - hemo in gms\nPacked Cell Volume(numerical)\nWhite Blood Cell Count(numerical) - wc in cells\/cumm\nRed Blood Cell Count(numerical) - rc in millions\/cmm\nHypertension(nominal) - htn - (yes,no)\nDiabetes Mellitus(nominal) - dm - (yes,no)\nCoronary Artery Disease(nominal) - cad - (yes,no)\nAppetite(nominal) - appet - (good,poor)\nPedal Edema(nominal) - pe - (yes,no)\nAnemia(nominal) - ane - (yes,no)\nClass (nominal)- class - (ckd,notckd)\n  \n  \n\n","43b6e28f":"The distribution do over lap in the middle but have **quite good separation. This can help us in imputation.** We can consider 12.5 as the point of separation for the two rbc categories.\n\n(Source: Mayo Clinic)\nThe normal range for hemoglobin is:\n\n    For men, 13.5 to 17.5 grams per deciliter\n    For women, 12.0 to 15.5 grams per deciliter\n","57ff8bda":"Observations:\n 1. Some columns have **no missing values**\n 2. Columns l**ike - age, pcc, ba, bu, sc, htn, dm, cad, bp. They have less than 5% data as missing**. So we can subsitute them with **mean\/median\/mode** or it would be better if we find some **systematic mechaism** to relate to these missing values.\n 3. Other variables have **high missing values**. So here we need to be **sure why they are missing and what is best way to handle them**. Otherwise it can create biasness in our data if we miss the underlying logic of missing values.\n 4. Missing values are as high as - 38% and as low as 0.3%. Varing range of missing values. It needs to be paid more attaention.\n 5. We should be also **confirm the column significance** to understand its importance - to decide to drop or how to impute its missing values and Outliers.\n 6. **Distribution of the Feature**s will play a important factor in the way we will impute it.","b7dbc7a6":"Missing Values Treatment","ee55afa4":"More cases of chronic kidney diseases which is very **favourable** as it will help us in predicting ckd category more accurately.","12419cff":"Having gained knowledge of the variables. We should try to gain more insight of the variables to **reach some conclusion and remove some biases**. Therfore, the next logical step is **Exploratory Data Anlysis.**\n\nWe will start with Univariate Analysis.\nTO make the process more organised, I will be dividing the data set into numeric and categorical variables.","594e23f8":"Hemo and PCV Has single Outlier value and those also not that abnormal or deviant. So we will not be treating these variables - as the value is just above upper bound of Outliers.","bb343327":"Interesting to see that CramerV is showing only moderate association even though bar plots showed strong relation between two, also can be understood by definition.","302699ec":"Approach : We try to study the distribution of our variables using the kde plot, box plot and Swarmplot.\n**Kde plot** - can provide idea of **distribution** of variable. Concentration of data. Shape of distribution\n**Boxplot** - It helps to understand the **median, the Quantiles and Outliers**\n**SwarmPlot** - Gives an **actual idea of values taken by the variable. It help us to quantify distribution observations.**\n\nObservations:\n 1. Id -  Symmetric Distribution, No outliers and takes value across all of its range.\n 2. Age - It is slightly **left skewed with high concentration of data between 40-60**. There are few outliers on the lower spectrum of age\n 3. Bp - It has a long right tail that indicates presence of outliers as confirmed by Boxplot. **One interesting observation is that bp is taking discrete values only.**\n 4. sg, al - as observed earlier also they are discrete variables. Tend to concentrate values on the lower end.\n 5. **su - most of the values are 0**. It has outliers but we should not see them as outliers as they are actually true value. Due to a skewed distriibution, it is showing presence of outliers.\n 6. bgr - Right skewed, Outliers. Concentrated between 0-150.\n 7. bu - Right skewed, Outliers. Concentrated between 0-50.\n 8. **sc - Concentrated between 0-8. We need to see why some values are so high for this variables.**\n 9. sod - A left tail, Interesting. Has Outliers with very low value near 0 - which can be a error or a severe deficinecy of sodium.\n 10. Pot - Few outliers\n 11. Hemo, pcv - Both are little left skewed with a distribution close to normal.\n\n**Mostly all the variables have outliers ranging from few to many.**\n ","42ccd33a":"Now we will Logistic Regression to understand the coefficient Significance. We will also need to do some check before that.","d57df308":"Before building Decision Tree model we will dropped the variables that we had transformed and made more robust. \nFor sc we see in terms of Feature selection that sc was more important than sc_o , so we will keep the original variable here.","3805743b":"The normal range for creatinine in the blood may be 0.84 to 1.21 milligrams per deciliter(Mayo Clinic). \n**Higher than that it indicates kidney malfunction**. So it can be a big indicator for classification problem. We could confirm it in a better way by visualizing the relation between two latter.\n\nAlso the very far off value for SC certainly point to very **abnormal Outliers** which needs domain knowledge to be dealt properly.","24995d34":"Encoding df Categorical Variable","5af64a6c":"Filling htn, dm , cad missing values. They are categorical and have only one missing value. So we can use any simplified approach to fill them. Most probably fill with mode.","404fb7ba":"For sc , there were **many unusal values** as also noted earler. So we checked it separately.\nAll values above 4 tend to show ckd. So **outlier treatment** can be easily done for this variable by **capping **the values.\n","1f63bfd9":"Imputation of Hemo - Highlu Correlated with PCV.","8be23153":"For correlation variables, we can see that hemo is the most important, followed by pcv and rc. So we can drop pcv and rc to reduce collinerity.","a081e957":"### Feature Selection and Engineering","e74cb566":"Using su(sugar) for imputing bgr value","6c295639":"Seeing from above graphs and Correlation, it would be better if we use pcv and hemo values for filling of rbc values\n\nConditions , hemo > 12 and pcv > 38, rbc takes normal value. And for hemo < 12 and pcv < 38, it takes abnormal value.","4d3d2cbc":"Imputing bu (blood urea) values","a6d45eb9":"We will not be standardizing here, as we wont be able to make sense of the coefficients, due to presence of Multicollinearity.","dc085e19":" - From Amenia vs RBC - we can conclude that most of cases of **no anemia lies with normal rbc count** (as should be the case)\n - Anemia vs Hemo - **Below 10 -11 hemo level we have higher chances of being anemic**.(which should be the case)","8f35566d":"Treating bp variable Outliers. They are actually true values, so it is better to bin\/discreticize bp to show different levels.\n\nUsing DecisionTreeClassifier as Discritiser as it benefits - \n- Creating monotonic relation with target variable\n- Decreases Entropy within groups\n- Treats Outliers","24389beb":"A sassumed - Perfect score","ce204a54":"# Support Vector Machine","6a877d13":"Observations:\n\n- If rbc counts is normal then there are less chances of being ckd. But **rbc count abnormal can certainly lead to ckd**\n- Same is the case of **Pus cell and Pus cell clump and Bacteria** - If there is normal puss cell, it has almost equal chance of being ckd or non ckd. But **presence of pc, ba, pcc do certainly point of occurence of ckd**. We can assume that pc, ba, pcc presence will be good predictor fot presence of ckd.\n- **Presence of Hypertension(htn), diabetes(dm),  coronary artery disease, swollen feets(pe), amemia, poor appetitie leads to presence of ckd**. But absence of hypertension also can lead to ckd.\n- From above analysis, we can see **that presence of even one** - abnormal red cell count, bacteria, hypertension, pus call, diabetes, coronary disease, lack of appetite, amenic, swllen body parts - **increases chances of occurence of ckd to substantial level**.","e036ce6a":"We can see a **separation above 170 mark**. Most of the cases with dm as yes belong above 170. The cases of no dm are concentrated between **50 -170** , which can be taken as **normal bgr range**.","ac327f51":"Replacing with 'no' (which is the mode) for these three variables","90ce79d3":"We can see that **Pcc not present has high overlap with Normal PC** (which we assumed), and PCC present has high overlap with abnormal pcc.","c94b7f2c":"- Most of the people have a good appetite.\n- Pedel Edema - indicates swolleness of our feet. This can be a direct indicator of kidney issues. 'No' as being the major status here also.\n- Anemia - Indicates presence or absence of red blood cells\/hemoglobin. Most of the people have 'no' anemia condition.","388a34d7":"Imputation of RC","6abf3a52":"Conclusion: \n- Lots of missing value in the variable.\n- **Imbalanced** towards normal\n- If we **impute this variable with mode then it can cause loss of predictive power of the feature, as it will become highly imbalanced**.\n- It would be better if we can derive missing values accurately in some way or even think about creating a **new category - 'Other'** to understand the missing values importance.\n- **Red Blood count can have a relationship with hemoglobin** as Hemoglobin is carried by Red blood cells. So we can visual this relation to undertand if our data has such relation and this will also help us in Feature Engineering and Imputations.","449cd27f":"RBC has a direct relation with rc - that shows numerical value of red blood cell counts. So if the range of rc is in normal range, rbc will take normal as status or otherwise. This can be confirmed by boxplot.","5451fdc2":"Imputing WC (White Blood Cells) - Again no Significant Correlation with other variables","09ab7b70":"PCC value is available for all nan value of PC, except 1. ","b43ac146":"There is good separation also the two category of rbc for hemo and pcv.","54e962c0":"As thought most of the cases of Hypertension are in the age bracket above age > 40.","3cf6691a":"It shows slight right skewed and also precense of Outliers. Though shape is near to Gaussian Curve","5464e503":"Feature Importance","b44f747a":"The two have kind of **same distribution**. This can **point towards correlation**, which we will check for later using correlation matrix.\n\nBoth the variable also are **right skewed**. They also have long tails which point to **high Kurtosis** - pointing to existence of Outliers. ","e2e94623":"All three rc, pcv, hemo are highly correlated. So we need to impute one using some other variables and then using this variable, impute the other two\n\nWe should first impute the variable that has highest correlation with other variables.\n\nImpute pcv using - sc , al, sg.","3ec5f0ff":"Both sg and al have high correlation with same variable, \nthey their own correlation is not that high","d9885210":"This variables do not have high correlation with any other variable. So imputing them with KNN Regressor","75a9a56c":"By seeing the above table, we can conclude:\n 1. We have **13 numeric **variables\n 2. Same conclusion as above - all columns except id have missing values\n 3. Id column - We assumed it to be a unique identifier, but the number should be lying between 0 to 280 which is not the case as it max value is 399. So we need to go more deeper into it to understand what it is actually.\n 4. Age column - **Min age is 2 and max is 90. Min age of 2 shows a interesting case of chronic kidney disease** which usually should not have been a case.\n 5. bp - ranges from 50 to 180. So we can conclude that there are **cases of high and low blood pressure**\n 6. sg - It is a discrete numerical variable\n 7. al - It can take values from 0 to 5. However we see most of the people **75% have al value of 0,1 or 2, which is interesting**. It will be interesting to see how the low or high values of al relate to Kidney disease\n 8. su - Again, **75% people have sugar level of 0 which indicated that most of the people have low sugar** or it can be a data capturing error\n 9. bgr - has a high std dev of 70\n 10. bu - With a max value of 390, it will be having outiers\n 11. sc - **most of the values are lowas 3, but the max value of 76 needs to be investigated**\n 12. sod - seems to be **normally distributed**\n 13. pot - most of the values are low, but the max value of 47 needs to be investigated\n 14. hemo and pcv - looks rather symmetric\n \n We can coclude that features - **bgr, su, bp, bu, sc, sod, al have an asymmetric dstribution and are skewed. Their mean and median values do not overlap and we should be aware about existence of outliers and central tendency of such variables.**","4b01e8a9":"### Outliers Treatment","5b709ccf":"About **93 percent value**s belong to only one variable present. This indicates a very imbalanced sample which is not very strong in terms of predictive power.","889981a1":"Imputation of su\nAs we saw above by eda, that su have a relation with dm (diabetes) and bgr(blood glucose random).\nAlso su and bgr have a correlation of 0.65 which points towards a positive relation. So we woul be using these two variables for imputation.\n\n","4ad032e2":"We can see that people with age **less than 11 are also suffering from chronic Disease.** \nAlso Age does not have any value which can be an error other than Nan. So we **should not treat the Outliers for this variable **and fill the missing values with Median or other imputation technique, if we are able to relate the column with some other variable (which is not the case till now)","878fea49":"We have **4 column in Dm and 3 in Cad** , which should not be the case. But we can say that the error only are cases of yes and no as can be deduced from the suffixes of these categories.\n\nThe values are very less and will not much affect the distribution of the Variables.\n\nThere are more cases of 'no' Hypertension. We can see this distribution across age and some other factor to understand HyperTension cases.\nDM also has more cases of no. And same goes for cad - very few people do actually have a coronary disease.","35fed263":"We had earlier seen that the five columns (sg, al, su, rbc, pc) have missing values for the same observations(through rbc and pc have have missing values in larger number of observations). \n\nWe can also create a new column - with binary values 1 to mark these observations to create missing value significance and impute then with simple methods. However if we are able to impute with reasonable certainty then we need not create such a column","ec4828ae":"Checking Unique value counts of various discrete Variables to understand their distribution","d84820f5":"Dropping sg variable - as it is showing highest value. And running VIF test again.","8d80101e":"Imputing Bgr","4abe16c3":"bu has high correlation with sc","3612ac72":"Feature Selection by Shuffling method","d58d6d86":"We can also see no Row has all values as missing , so we cannot drop any row.","3cbb0007":"Same we will perform KMeansDiscretization with sodium as it as same distribution like sc","89e482a6":" We can see that our dataset has 280 observations and 26 columns","77dae75a":"### Data PreProcessing","9f0ac9eb":"Imputation of rbc","540c6cd5":"# Random Forest Model","6c1f5b36":"LASSO REGRESSION MODEL","39d6b27d":"Same as above the **high values indicate ckd** presence. We would need to **check** if this indicates a relation between bu and Chronic kidney Disease or just some one off observations.","068b2b30":"Both lower than 130 and higher than 160 values lead to ckd","5364696b":"- We can see **abnormal values of 0** in sodium values which should not be a case. \n- We can also see some **abnormal values of Potassium around 40.**\n- Hemoglobin has a slight left skewness but overall we can see a **normal distribution**.\n- Hemoglobin has normal values genrally in range of 11 to 17. So we can also **need to see of the lower values of hemoglobin point to something**.","0a3271b5":"  Feature Importance as per Random Forest Classifier","1c7131b2":"We Know that some varibles like - Hemo, PCV, rc have high correlation. So we can \ntry to perform PCA on these. These three also are continuous also. \nWe also have treated their Outliers. So we should try what the results and then see should \nwe proceed with PCA results or not.","57461ff7":"These values are not normal in the wc column. nan and \\t we have to deal. \\t8400 value could be 8400 simply as it falls in the scale of wc count.","faaa437b":"Observations - \n- High correlation between **Pcv and hemo(**as seen by their linear relation)\n- Not that high correlation among other variables.\n- **bgr and su** have a high corr of 0.65 as also assumed by us earlier\n- sodium and serum C has a negative correlation of 0.71\n- We made some earlier assumption about rc, rbc , hemo being correlated. Since corr() only takes\nnumeric variables **bold text** - so we cannot confirm those relations from here. \n- Also relation between bgr and dm could not be confirmed.\n","a8da666b":"**Abnormality in pot points to ckd.**","4bfcc3b4":"Still many variables have too high values of VIF. Dropping wc_o(as it has highest value for VIF), and also pcv and rc - we already seen they have high correlation with hemo.\n","3d2fc630":"Building Logistics Regression Model - with scaled variables to understand significance of variables\n","80d275da":"We can see that su, al, sg have around 35 missing values and there are **33 indexes where all three have missing values.** This is interesting. **Also Rbc and pc are also missing in such indexes. **","2cc9c2e9":"Checking feature importance by Lasso Regression","52585d89":" ## CATEGORICAL VARIABLES","d78f9de5":"We see that coefficients for a lot of variables changes to 0 and thus we can remove these variables if we want to decrease the complexity of the model, but it will increase the bias error.","4cde9f62":"### Logistics Regression Model","c9b9aa66":"sc, hemo, pcv, rc have strong correlation with al variables as we checked from above correlation table. So we can use this relation to impute the values.","43b9f9e3":"Impuying Values of ba","b8bac2b9":"# Bivariate Analysis","e3989a74":"- **No row with all missing values**.\n- We have **3 rows that have 10 variables missing. And 5 rows with 11 variables missing.**\n- **6 rows with 8 variables missing**. We need to decide how to handle them, understand if they can provide any important information of variable. \n- We can **drop them** if they do not in any way provide useful information and **do not improve computational efficiency and predictive power** of model.\n\n\n"}}