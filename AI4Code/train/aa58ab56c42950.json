{"cell_type":{"0ce615bc":"code","3bfa363b":"code","e9f22d78":"code","31457d97":"code","35a88073":"code","07d73149":"code","b55cbc68":"code","6f15945f":"code","1e90cd29":"code","604caa23":"code","ae7faa65":"code","c4baabc6":"code","d4bfe07e":"code","b5ab23ac":"code","013b3fd3":"code","fc037bda":"code","4aaa1875":"code","268e58c6":"code","010ad182":"code","e306135c":"code","78a58210":"code","f5857cc7":"code","f120ad79":"code","f145d83f":"code","b03a80c2":"code","c9863e71":"code","d0f85e3a":"code","7e9f51d4":"code","07e002f9":"code","eebdec6c":"code","2fa46319":"code","46b6d362":"code","8a9eaa6b":"code","f656ec0d":"code","77f19a16":"code","c3e9e978":"code","0f35eb9c":"code","45cad293":"code","e742d698":"code","2c7237f2":"code","c1e8b101":"code","ef65c28f":"code","9ab405d8":"markdown","8a302df5":"markdown","b20823c5":"markdown","a1bc460c":"markdown","a94d6cfd":"markdown","db5f0400":"markdown","2a510917":"markdown","3ddb305d":"markdown","2049b91d":"markdown","fb214d52":"markdown","dc7fbd9b":"markdown","52e34c26":"markdown","c4c4ccee":"markdown","1fc74e4a":"markdown","8ab8007e":"markdown","9dbbe2e5":"markdown","5d1749a8":"markdown","e0f648e0":"markdown","392f9f78":"markdown","53a03cb3":"markdown","19900f29":"markdown","cd1159dc":"markdown","dd891bd4":"markdown","67885ad5":"markdown"},"source":{"0ce615bc":"%load_ext autoreload\n%autoreload 2\n%pylab inline\n%cd '\/kaggle\/input\/face-detection-dataset\/face-detection'","3bfa363b":"from keras import backend as K","e9f22d78":"from matplotlib import pyplot as plt\nimport numpy as np\nfrom skimage import transform","31457d97":"from get_data import load_dataset, unpack","35a88073":"# First run will download 30 MB data from github\n\ntrain_images, train_bboxes, train_shapes = load_dataset(\"data\", \"train\")\nval_images, val_bboxes, val_shapes = load_dataset(\"data\", \"val\")","07d73149":"from graph import visualize_bboxes\nvisualize_bboxes(images=train_images,\n                 true_bboxes=train_bboxes\n                )","b55cbc68":"SAMPLE_SHAPE = (32, 32, 3)","6f15945f":"from scores import iou_score # https:\/\/en.wikipedia.org\/wiki\/Jaccard_index\n\ndef is_negative_bbox(new_bbox, true_bboxes, eps=1e-1):\n    \"\"\"Check if new bbox not in true bbox list.\n    \n    There bbox is 4 ints [min_row, min_col, max_row, max_col] without image index.\"\"\"\n    for bbox in true_bboxes:\n        if iou_score(new_bbox, bbox) >= eps:\n            return False\n    return True","1e90cd29":"# Write this function\ndef gen_negative_bbox(image_shape, bbox_size, true_bboxes):\n    \"\"\"Generate negative bbox for image.\"\"\"\n    tries = 1000\n    for i in range(tries):\n        corner_x = np.random.randint(max(image_shape[0] - bbox_size[0], 1))\n        corner_y = np.random.randint(max(image_shape[1] - bbox_size[1], 1))\n        new_bbox = [corner_x, corner_y, corner_x + bbox_size[0], corner_y + bbox_size[1]]\n        if is_negative_bbox(new_bbox,true_bboxes):\n            return new_bbox\n    return None\n\ndef get_positive_negative(images, true_bboxes, image_shapes, negative_bbox_count=None):\n    \"\"\"Retrieve positive and negative samples from image.\"\"\"\n    positive = []\n    negative = []\n    image_count = image_shapes.shape[0]\n    \n    if negative_bbox_count is None:\n        negative_bbox_count = len(true_bboxes)\n    \n    # Pay attention to the fact that most part of image may be black -\n    # extract negative samples only from part [0:image_shape[0], 0:image_shape[1]]\n    \n    # Write code here\n    # ...\n    w, h, c = SAMPLE_SHAPE\n    for true_bbox in true_bboxes:        \n        image_index = true_bbox[0]\n        pos_img = images[image_index][true_bbox[1]:true_bbox[1]+w, true_bbox[2]:true_bbox[2]+h, :]\n        positive.append(pos_img)\n    \n    print(negative_bbox_count)\n    for i in range(negative_bbox_count):\n        image_index = np.random.choice(len(images))\n        image_shape = image_shapes[image_index]\n        image_true_bboxes = true_bboxes[true_bboxes[:, 0] == image_index, 1:]\n        for j in range(100):\n            bbox_size = (w, h)\n            new_bbox = gen_negative_bbox(image_shape, bbox_size, image_true_bboxes)\n            if new_bbox is None: continue\n            neg_img = images[image_index][new_bbox[0]:new_bbox[2], new_bbox[1]:new_bbox[3]]\n            negative.append(neg_img)\n            break\n        print(i)\n    return positive, negative","604caa23":"def get_samples(images, true_bboxes, image_shapes):\n    \"\"\"Usefull samples for learning.\n    \n    X - positive and negative samples.\n    Y - one hot encoded list of zeros and ones. One is positive marker.\n    \"\"\"\n    positive, negative = get_positive_negative(images=images, true_bboxes=true_bboxes, \n                                               image_shapes=image_shapes)\n    X = positive\n    Y = [[0, 1]] * len(positive)\n    \n    X.extend(negative)\n    Y.extend([[1, 0]] * len(negative))\n    \n    return np.array(X), np.array(Y)","ae7faa65":"X_train, Y_train = get_samples(train_images, train_bboxes, train_shapes)\nX_val, Y_val = get_samples(val_images, val_bboxes, val_shapes)","c4baabc6":"out_file = '\/kaggle\/working\/{}.npy'\nnp.save(out_file.format('X_train'), X_train)\nnp.save(out_file.format('Y_train'), Y_train)\nnp.save(out_file.format('X_val'), X_val)\nnp.save(out_file.format('Y_val'), Y_val)","d4bfe07e":"out_file = '\/kaggle\/working\/{}.npy'\nX_train = np.load(out_file.format('X_train'))\nY_train = np.load(out_file.format('Y_train'))\nX_val = np.load(out_file.format('X_val'))\nY_val = np.load(out_file.format('Y_val'))","b5ab23ac":"# There we should see faces\nfrom graph import visualize_samples\nvisualize_samples(X_train[Y_train[:, 1] == 1])","013b3fd3":"# There we shouldn't see faces\nvisualize_samples(X_train[Y_train[:, 1] == 0])","fc037bda":"BATCH_SIZE = 64\nK.clear_session()","4aaa1875":"from keras.preprocessing.image import ImageDataGenerator # Usefull thing. Read the doc.\n\ndatagen = ImageDataGenerator(horizontal_flip=True,\n                             width_shift_range=0.2,\n                             height_shift_range=0.2,\n                             zoom_range=0.1,\n                            )\ndatagen.fit(X_train)","268e58c6":"import os.path\nfrom keras.optimizers import Adam\n# Very usefull, pay attention\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n\nfrom graph import plot_history\n\nmodel_path = '\/kaggle\/working\/FDmodel.hdf5'\nif os.path.isfile(model_path):\n    os.remove(model_path)\n    \ndef fit(model, datagen, X_train, Y_train, X_val, Y_val, class_weight=None, epochs=50, lr=0.001, verbose=False):\n    \"\"\"Fit model.\n    \n    You can edit this function anyhow.\n    \"\"\"\n    \n    if verbose:\n        model.summary()\n\n    model.compile(optimizer=Adam(lr=lr), # You can use another optimizer\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    \n    \n    history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=BATCH_SIZE),\n                                  validation_data=(datagen.standardize(X_val), Y_val),\n                                  epochs=epochs, steps_per_epoch=len(X_train) \/\/ BATCH_SIZE,\n                                  callbacks=[ModelCheckpoint(model_path, save_best_only=True)],\n                                  class_weight=class_weight,\n            \n                                 )  # starts training\n    \n    # summarize history for accuracy\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.show()","010ad182":"from keras.models import Model, Sequential\nfrom keras.layers import Flatten, Dense, Activation, Input, Dropout, Activation, BatchNormalization\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n\ndef generate_model(sample_shape):\n    # Classification model\n    # You can start from LeNet architecture\n    x = inputs = Input(shape=sample_shape)\n\n    # Write code here\n    x = Conv2D(32, (5, 5), activation='relu', padding='same')(x)\n    x = MaxPooling2D((2, 2))(x)\n    x = Conv2D(32, (5, 5), activation='relu', padding='same')(x)\n    x = MaxPooling2D((2, 2))(x)\n    x = Flatten()(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.25)(x)\n    x = Dense(64, activation='relu')(x)\n    x = Dropout(0.25)(x)\n\n    # This creates a model\n    predictions = Dense(2, activation='softmax')(x)\n    return Model(inputs=inputs, outputs=predictions)\n\nmodel = generate_model(SAMPLE_SHAPE)","e306135c":"model.summary()","78a58210":"# Attention: Windows implementation may cause an error here. In that case use model_name=None.\nfit(model=model, datagen=datagen, X_train=X_train.astype('float32'), X_val=X_val.astype('float32'), Y_train=Y_train, Y_val=Y_val)","f5857cc7":"def get_checkpoint():\n    return model_path\n\nmodel.load_weights(get_checkpoint())","f120ad79":"# FCNN\n\nIMAGE_SHAPE = (176, 176, 3)\n\ndef generate_fcnn_model(image_shape):\n    \"\"\"After model compilation input size cannot be changed.\n    \n    So, we need create a function to have ability to change size later.\n    \"\"\"\n    x = inputs = Input(image_shape)\n\n    # Write code here\n    x = Conv2D(32, (5, 5), activation='relu', padding='same')(x)\n    x = MaxPooling2D((2, 2))(x)\n    x = Conv2D(32, (5, 5), activation='relu', padding='same')(x)\n    x = MaxPooling2D((2, 2))(x)\n\n    x = Conv2D(128, (8, 8), activation='relu')(x)\n    x = Dropout(0.25)(x)\n    x = Conv2D(64, (1, 1), activation='relu')(x)\n    x = Dropout(0.25)(x)\n\n    # This creates a model\n    predictions = Conv2D(2, (1, 1), activation='linear')(x)\n    return Model(inputs=inputs, outputs=predictions)\n\nfcnn_model = generate_fcnn_model(IMAGE_SHAPE)","f145d83f":"fcnn_model.summary()","b03a80c2":"def copy_weights(base_model, fcnn_model):\n    \"\"\"Set FCNN weights from base model.\n    \"\"\"\n    \n    new_fcnn_weights = []\n    prev_fcnn_weights = fcnn_model.get_weights()\n    prev_base_weights = base_model.get_weights()\n    \n    # Write code here\n    for prev_fcnn_weight, prev_base_weight in zip(prev_fcnn_weights, prev_base_weights):\n        new_fcnn_weights.append(prev_base_weight.reshape(prev_fcnn_weight.shape))\n        \n    fcnn_model.set_weights(new_fcnn_weights)\n\ncopy_weights(base_model=model, fcnn_model=fcnn_model)","c9863e71":"from graph import visualize_heatmap","d0f85e3a":"predictions = fcnn_model.predict(np.array(val_images))\nvisualize_heatmap(val_images, predictions[:, :, :, 1])","7e9f51d4":"# Detection\nfrom skimage.feature import peak_local_max\n\ndef get_bboxes_and_decision_function(fcnn_model, images, image_shapes):      \n    cropped_images = np.array([transform.resize(image, IMAGE_SHAPE, mode=\"reflect\")  if image.shape != IMAGE_SHAPE else image for image in images])\n    pred_bboxes, decision_function = [], []\n   \n    # Predict\n    predictions = fcnn_model.predict(cropped_images)\n\n    # Write code here\n    for i in range(len(predictions)):\n        img_shape = image_shapes[i]\n        local_max_list = peak_local_max(predictions[i][:,:,1], num_peaks=5, min_distance=3, exclude_border=False)\n        for local_max_orig in local_max_list:\n            local_max = ((local_max_orig + 2)*176\/37).astype(int)\n            \n            if local_max[0] < img_shape[0] and local_max[1] < img_shape[1]:\n                bbox = [i] + [local_max[0]-16,local_max[1]-16,local_max[0]+16,local_max[1]+16]\n                \n                pred_bboxes.append(bbox)\n                decision_function.append(predictions[i, local_max_orig[0], local_max_orig[1], 1])\n        \n    return pred_bboxes, decision_function","07e002f9":"pred_bboxes, decision_function = get_bboxes_and_decision_function(fcnn_model=fcnn_model, images=val_images, image_shapes=val_shapes)\n\nvisualize_bboxes(images=val_images,\n                 pred_bboxes=pred_bboxes,\n                 true_bboxes=val_bboxes,\n                 decision_function=decision_function\n                )","eebdec6c":"from scores import best_match\nfrom graph import plot_precision_recall\n\ndef precision_recall_curve(pred_bboxes, true_bboxes, decision_function):\n    precision, recall, thresholds = [], [], []\n    \n    # Write code here\n    threshold = min(decision_function) - 1\n    max_th = max(decision_function) + 1\n    num_steps = 100\n    th_step = (max_th - threshold) \/ num_steps\n\n    sorted_boxes = [[x] + y for y, x in sorted(zip(pred_bboxes, decision_function), key=lambda pair : pair[1])]\n    \n    for step in range(num_steps):        \n        pred_bboxes_th = [x[1:] for x in sorted_boxes if x[0] > threshold]\n        if len(pred_bboxes_th) > 0:\n            matched, false_negative, false_positive = best_match(pred_bboxes_th, true_bboxes, decision_function)\n        else:\n            break\n        \n        prec = len(matched) \/ (len(matched) + len(false_positive))\n        rec = len(matched) \/ (len(matched) + len(false_negative))\n        \n        thresholds.append(threshold)\n        recall.append(rec)\n        precision.append(prec)\n        threshold += th_step\n        \n    return precision, recall, thresholds","2fa46319":"precision, recall, thresholds = precision_recall_curve(pred_bboxes=pred_bboxes, true_bboxes=val_bboxes, decision_function=decision_function)\nplot_precision_recall(precision=precision, recall=recall)","46b6d362":"def get_threshold(thresholds, recall):\n    return thresholds[np.argmax(np.asarray(recall) <= 0.85)] # Write this code\n\nTHRESHOLD = get_threshold(thresholds, recall)","8a9eaa6b":"def detect(fcnn_model, images, image_shapes, threshold, return_decision=True):\n    \"\"\"Get bboxes with decision_function not less then threshold.\"\"\"\n    pred_bboxes, decision_function = get_bboxes_and_decision_function(fcnn_model, images, image_shapes)   \n    result, result_decision = [], []\n    \n    # Write code here\n    for i in range(len(pred_bboxes)):\n        if decision_function[i] >= threshold:\n            result.append(pred_bboxes[i])\n            result_decision.append(decision_function[i])\n    \n    if return_decision:\n        return result, result_decision\n    else:\n        return result","f656ec0d":"pred_bboxes, decision_function = detect(fcnn_model=fcnn_model, images=val_images, image_shapes=val_shapes, threshold=THRESHOLD, return_decision=True)\n\nvisualize_bboxes(images=val_images,\n                 pred_bboxes=pred_bboxes,\n                 true_bboxes=val_bboxes,\n                 decision_function=decision_function\n                )\n\nprecision, recall, thresholds = precision_recall_curve(pred_bboxes=pred_bboxes, true_bboxes=val_bboxes, decision_function=decision_function)\nplot_precision_recall(precision=precision, recall=recall)","77f19a16":"test_images, test_bboxes, test_shapes = load_dataset(\"data\", \"test\")\n\n# We test get_bboxes_and_decision_function becouse we want pay attention to all recall values\npred_bboxes, decision_function = get_bboxes_and_decision_function(fcnn_model=fcnn_model, images=test_images, image_shapes=test_shapes)\n\nvisualize_bboxes(images=test_images,\n                 pred_bboxes=pred_bboxes,\n                 true_bboxes=test_bboxes,\n                 decision_function=decision_function\n                )\n\nprecision, recall, threshold = precision_recall_curve(pred_bboxes=pred_bboxes, true_bboxes=test_bboxes, decision_function=decision_function)\nplot_precision_recall(precision=precision, recall=recall)","c3e9e978":"# First run will download 523 MB data from github\n\noriginal_images, original_bboxes, original_shapes = load_dataset(\"data\", \"original\")","0f35eb9c":"# Write code here\n# ...\n","45cad293":"# Write this function\ndef hard_negative(train_images, image_shapes, train_bboxes, X_val, Y_val, base_model, fcnn_model):\n    pass","e742d698":"hard_negative(train_images=train_images, image_shapes=train_shapes, train_bboxes=train_bboxes, X_val=X_val, Y_val=Y_val, base_model=model, fcnn_model=fcnn_model)","2c7237f2":"model.load_weights(\"data\/checkpoints\/...\")","c1e8b101":"copy_weights(base_model=model, fcnn_model=fcnn_model)\n\npred_bboxes, decision_function = get_bboxes_and_decision_function(fcnn_model=fcnn_model, images=val_images, image_shapes=val_shapes)\n\nvisualize_bboxes(images=val_images,\n                 pred_bboxes=pred_bboxes,\n                 true_bboxes=val_bboxes,\n                 decision_function=decision_function\n                )\n\nprecision, recall, thresholds = precision_recall_curve(pred_bboxes=pred_bboxes, true_bboxes=val_bboxes, decision_function=decision_function)\nplot_precision_recall(precision=precision, recall=recall)","ef65c28f":"def multiscale_detector(fcnn_model, images, image_shapes):\n    return []","9ab405d8":"Do you have modern Nvidia [GPU](https:\/\/en.wikipedia.org\/wiki\/Graphics_processing_unit)? There is your video-card model in [list](https:\/\/developer.nvidia.com\/cuda-gpus) and CUDA capability >= 3.0?\n\n- Yes. You can use it for fast deep learning! In this work we recommend you use tensorflow backend with GPU. Read [installation notes](https:\/\/www.tensorflow.org\/install\/) with attention to gpu section, install all requirements and then install GPU version `tensorflow-gpu`.\n- No. CPU is enough for this task, but you have to use only simple model. Read [installation notes](https:\/\/www.tensorflow.org\/install\/) and install CPU version `tensorflow`.\n\nOf course, also you should install `keras`, `matplotlib`, `numpy` and `scikit-image`.","8a302df5":"## Classifier training (3 points)\n\nFirst of all, we should train face classifier that checks if face represented on sample.","b20823c5":"In this task we use processed [FDDB dataset](http:\/\/vis-www.cs.umass.edu\/fddb\/). Processing defined in file [.\/prepare_data.ipynb](prepare_data.ipynb) and consists of:\n\n1. Extract bboxes from dataset. In base dataset face defined by [ellipsis](http:\/\/vis-www.cs.umass.edu\/fddb\/samples\/) that not very useful for basic neural network learning.\n2. Remove images with big and small faces on one shoot.\n3. Re-size images to bounding boxes (bboxes) have same size 32 +\/- pixels.\n\nEach image in train, validation and test datasets have shape (176, 176, 3), but part of this image is black background. Interesting image aligned at top left corner.\n\nBounding boxes define face in image and consist of 5 integer numbers: [image_index, min_row, min_col, max_row, max_col]. Bounding box width and height are 32 +\/- 8 pixels wide.\n\n`train_bboxes` and `val_bboxes` is a list of bboxes.\n\n`train_shapes` and `val_shapes` is a list of interesting image shapes.","a1bc460c":"#### Detector visualization","a94d6cfd":"### Image augmentation\n\nImportant thing in deep learning is augmentation. Sometimes, if your model are complex and cool, you can increase quality by using good augmentation.\n\nKeras provide good [images preprocessing and augmentation](https:\/\/keras.io\/preprocessing\/image\/). This preprocessing executes online (on the fly) while learning.\n\nOf course, if you want using samplewise and featurewise center and std normalization you should run this transformation on predict stage. But you will use this classifier to fully convolution detector, in this case such transformation quite complicated, and we don't recommend use them in classifier.\n\nFor heavy augmentation you can use library [imgaug](https:\/\/github.com\/aleju\/imgaug). If you need, you can use this library in offline manner (simple way) and online manner (hard way). However, hard way is not so hard: you only have to write [python generator](https:\/\/wiki.python.org\/moin\/Generators), which returns image batches, and pass it to [fit_generator](https:\/\/keras.io\/models\/model\/#fit_generator)","db5f0400":"#### (first point out of three)\n\n![lenet architecture](lenet_architecture.png)\nLeCun, Y., Bottou, L., Bengio, Y. and Haffner, P., 1998. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), pp.2278-2324.\n\nOf course, you can use any another architecture, if want. Main thing is classification quality of your model.\n\nAcceptable validation accuracy for this task is 0.92.","2a510917":"## Detection\n\nIf you have prepared classification architecture with high validation score, you can use this architecture for detection.\n\nConvert classification architecture to fully convolution neural network (FCNN), that returns heatmap of activation.\n\n### Detector model or sliding window (1 point)\n\nNow you should replace fully-connected layers with $1 \\times 1$ convolution layers.\n\nEvery fully connected layer perform operation $f(Wx + b)$, where $f(\\cdot)$ is nonlinear activation function, $x$ is layer input, $W$ and $b$ is layer weights. This operation can be emulated with $1 \\times 1$ convolution with activation function $f(\\cdot)$, that perform exactly same operation $f(Wx + b)$.\n\nIf there is `Flatten` layer with $n \\times k$ input size before fully connected layers, convolution should have same $n \\times k$ input size.\nMultiple fully connected layers can be replaced with convolution layers sequence.\n\nAfter replace all fully connected layers with convolution layers, we get fully convolution network. If input shape is equal to input size of previous network, output will have size $1 \\times 1$. But if we increase input shape, output shape automatically will be increased. For example, if convolution step of previous network strides 4 pixels, increase input size with 100 pixels along all axis makes increase outputsize with 25 values along all axis. We got activation map of classifier without necessary extract samples from image and multiple calculate low-level features.\n\nIn total:\n1. $1 \\times 1$ convolution layer is equivalent of fully connected layer.\n2. $1 \\times 1$ convolution layers can be used to get activation map of classification network in \"sliding window\" manner.\n\nWe propose replace last fully connected layer with softmax actiovation to convolution layer with linear activation.It will be usefull to find good treshold. Of course, you can use softmax activation.\n\n#### Example of replace cnn head:\n\n##### Head before convert\n\n![before replace image](before_convert.png)\n\n##### Head after convert\n\n![before replace image](after_convert.png)\n\nOn this images displayed only head. `InputLayer` should be replaced with convolution part exit.\nBefore convert network head takes fifty $8 \\times 8$ feature maps and returns two values: probability of negative and positive classes. This output can be considered as activation map with size $1 \\times 1$.\n\nIf input have size $8 \\times 8$, output after convert would have $1 \\times 1$ size, but input size is $44 \\times 44$.\nAfter convert network head returns one $37 \\times 37$ activation map.","3ddb305d":"## Hard negative mining\n\nUpgrade the score with [hard negative mining](https:\/\/www.reddit.com\/r\/computervision\/comments\/2ggc5l\/what_is_hard_negative_mining_and_how_is_it\/).\n\nA hard negative is when you take that falsely detected patch, and explicitly create a negative example out of that patch, and add that negative to your training set. When you retrain your classifier, it should perform better with this extra knowledge, and not make as many false positives.","2049b91d":"### Model visualization","fb214d52":"# Face Detection\n\nHello! In this task you will create your own deep face detector.\n\nFirst of all, we need import some useful stuff.","dc7fbd9b":"#### Fit the model (second point out of three)\n\nIf you doesn't have fast video-card suitable for deep learning, you can first check neural network modifications with small value of parameter `epochs`, for example, 10, and then after selecting best model increase this parameter.\nFitting on CPU can be long, we suggest do it at bedtime.\n\nDon't forget change model name.","52e34c26":"### Fitting classifier\n\nFor fitting you can use one of Keras optimizer algorithms. [Good overview](http:\/\/ruder.io\/optimizing-gradient-descent\/)\n\nTo choose best learning rate strategy you should read about EarlyStopping and ReduceLROnPlateau or LearningRateScheduler on [callbacks](https:\/\/keras.io\/callbacks\/) page of keras documentation, it's very useful in deep learning.\n\nIf you repeat architecture from some paper, you can find information about good optimizer algorithm and learning rate strategy in this paper. For example, every [keras application](https:\/\/keras.io\/applications\/) has link to paper, that describes suitable learning procedure for this specific architecture.","c4c4ccee":"## Test dataset (1 point)\n\nLast detector preparation step is testing.\n\nAttention: to avoid over-fitting, after testing algorithm you should run [.\/prepare_data.ipynb](prepare_data.ipynb), and start all fitting from beginning.\n\nDetection score (in graph header) should be 0.85 or greater.","1fc74e4a":"### Detector (1 point)\n\nFirst detector part is getting bboxes and decision function.\nGreater decision function indicates better detector confidence.\n\nThis function should return pred_bboxes and decision_function:\n\n- `pred bboxes` is list of 5 int tuples like `true bboxes`: `[image_index, min_row, min_col, max_row, max_col]`.\n- `decision function` is confidence of detector for every pred bbox: list of float values, `len(decision function) == len(pred bboxes)` \n \nWe propose resize image to `IMAGE_SHAPE` size, find faces on resized image with `SAMPLE_SHAPE` size and then resize them back.","8ab8007e":"### Threshold (1 point)\n\nNext step in detector creating is select threshold for decision_function.\nEvery possible threshold presents point on recall-precision graph.\n\nSelect threshold for `recall=0.85`.","9dbbe2e5":"### Next  step\n\nNext steps in deep learning detection are R-CNN, Faster R-CNN and SSD architectures.\nThis architecture realization is quite complex.\nFor this reason the task doesn't cover them, but you can find the articles in the internet.","5d1749a8":"## Detector score (1 point)\n\nWrite [precision and recall](https:\/\/en.wikipedia.org\/wiki\/Precision_and_recall) graph.\n\nYou can use function `best_match` to extract matching between prediction and ground truth, false positive and false negative samples. Pseudo-code for calculation precision and recall graph:\n    \n    # Initialization for first step threshold := -inf\n    tn := 0 # We haven't any positive sample\n    fn := |false_negative| # But some faces wasn't been detected\n    tp := |true_bboxes| # All true bboxes have been caught\n    fp := |false_positive| # But also some false positive samples have been caught\n    \n    Sort decision_function and pred_bboxes with order defined by decision_function\n    y_true := List of answers for \"Is the bbox have matching in y_true?\" for every bbox in pred_bboxes\n    \n    for y_on_this_step in y_true:\n        # Now we increase threshold, so some predicted bboxes makes positive.\n        # If y_t is True then the bbox is true positive else bbox is false positive\n        # So we should\n        Update tp, tn, fp, fn with attention to y_on_this_step\n        \n        Add precision and recall point calculated by formula through tp, tn, fp, fn on this step\n        Threshold for this point is decision function on this step.","e0f648e0":"Now we can extract samples from images.","392f9f78":"#### (third point out of three)\n\nAfter learning model weights saves in folder `data\/checkpoints\/`.\nUse `model.load_weights(fname)` to load best weights.\n\nIf you use Windows and Model Checkpoint doesn't work on your configuration, you should implement [your own Callback](https:\/\/keras.io\/callbacks\/#create-a-callback) to save best weights in memory and then load it back.","53a03cb3":"### Multi scale detector\n\nWrite and test detector with [pyramid representation][pyramid].\n[pyramid]: https:\/\/en.wikipedia.org\/wiki\/Pyramid_(image_processing)\n\n1. Resize images to predefined scales.\n2. Run detector with different scales.\n3. Apply non-maximum supression to detection on different scales.\n\nReferences:\n1. [E. H. Adelson,C. H. Anderson, J. R. Bergen, P. J. Burt, J. M. Ogden: Pyramid methods in image processing](http:\/\/persci.mit.edu\/pub_pdfs\/RCA84.pdf)\n2. [PETER J. BURT, EDWARD H. ADELSON: The Laplacian Pyramid as a Compact Image Code](http:\/\/persci.mit.edu\/pub_pdfs\/pyramid83.pdf)","19900f29":"#### (1 point)\n\nThen you should write function that copy weights from classification model to fully convolution model.\nConvolution weights may be copied without modification, fully-connected layer weights should be reshaped before copy.\n\nPay attention to last layer.","cd1159dc":"Every image can represent multiple faces, so we should extract all faces from every images and crop them to `SAMPLE_SHAPE`. This set of extracted images are named `positive`.\n\nThen we chould extract `negative` set. This images should have `SAMPLE_SHAPE` size. Pseudocode for extracting:\n\n    negative_collection := []\n\n    for i in range(negative_bbox_count):\n        Select random image.\n        image_shape := image_shapes[image_index]\n        image_true_bboxes := true_bboxes[true_bboxes[:, 0] == image_index, 1:]\n        \n        for j in TRY_COUNT: # TRY_COUNT is a magic constant, for example, 100\n            Generate new_bbox within image_shape.\n            \n            if new_bbox is negative bbox for image_true_bboxes:\n                Extract from image, new_bbox and resize to SAMPLE_SIZE negative_sample.\n                Add negative sample to negative_collection.\n                Break # for j in TRY_COUNT","dd891bd4":"## Prepare data (1 point)\n\nFor learning we should extract positive and negative samples from image.\nPositive and negative samples counts should be similar.\nEvery samples should have same size.","67885ad5":"## Optional tasks\n\n### Real image dataset\n\nTest your algorithm on original (not scaled) data.\nVisualize bboxes and plot precision-recall curve."}}