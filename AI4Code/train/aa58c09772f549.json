{"cell_type":{"3f3e026b":"code","15f0ca2c":"code","03009baa":"code","612b3d88":"code","43ecf95b":"code","9bddf546":"code","1cd885ca":"code","1ab58808":"code","67ee29e1":"code","589492e8":"code","961edd9b":"code","54ef18f3":"code","72163cba":"code","d2b472d6":"code","863b551b":"code","32c7820a":"code","cb3e39bf":"code","64bd55e8":"code","da8c6185":"code","8dbe2d0c":"code","4416dbdb":"code","ed3088e1":"code","a8df30b2":"code","b2424c19":"code","6007065b":"code","cb3d75d5":"code","0a2ec71a":"code","9731cf37":"code","1dba1afe":"code","199583fe":"code","c1e2fdef":"code","7958ae87":"code","c1461ec9":"code","c0365fab":"code","74bc6d3b":"code","48174587":"code","42dc147f":"code","2dbd3db2":"code","96b91230":"code","631e7994":"code","ed5d938d":"code","f56c1701":"code","abd5f01a":"code","db0837a5":"code","65ecd05f":"code","beed92c3":"code","4bb6882d":"code","dd252232":"code","460f2aed":"code","f0a37ec9":"code","db18f7b0":"code","2e761d97":"code","e415bf8b":"markdown","770592a3":"markdown"},"source":{"3f3e026b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n#print(os.listdir(\"..\/input\/data-dictionary-xlsx\/Data Dictionary.xlsx\"))\n\n# Any results you write to the current directory are saved as output.\n# ['databikeloanprediction', 'data-dictionary-xlsx', 'bike-loan-defaulter-prediction']","15f0ca2c":"train_data = pd.read_csv('..\/input\/databikeloanprediction\/train.csv')\ntest_data = pd.read_csv('..\/input\/bike-loan-defaulter-prediction\/test_bqCt9Pv.csv')\nsample_submission = pd.read_csv('..\/input\/bike-loan-defaulter-prediction\/sample_submission_24jSKY6.csv')\ndata_dict = pd.read_excel('..\/input\/data-dictionary-xlsx\/Data Dictionary.xlsx')","03009baa":"sample_submission.head()","612b3d88":"train_data.head(10)","43ecf95b":"train_data.info()","9bddf546":"test_data.info()","1cd885ca":"# Renamed the column names as dots(.) available in column names\ntrain_data.columns = ['UniqueID', 'disbursed_amount', 'asset_cost', 'ltv', 'branch_id',\n'supplier_id', 'manufacturer_id', 'Current_pincode_ID', 'Date_of_Birth',\n'Employment_Type', 'DisbursalDate', 'State_ID', 'Employee_code_ID',\n'MobileNo_Avl_Flag', 'Aadhar_flag', 'PAN_flag', 'VoterID_flag',\n'Driving_flag', 'Passport_flag', 'PERFORM_CNS_SCORE',\n'PERFORM_CNS_SCORE_DESCRIPTION', 'PRI_NO_OF_ACCTS', 'PRI_ACTIVE_ACCTS',\n'PRI_OVERDUE_ACCTS', 'PRI_CURRENT_BALANCE', 'PRI_SANCTIONED_AMOUNT',\n'PRI_DISBURSED_AMOUNT', 'SEC_NO_OF_ACCTS', 'SEC_ACTIVE_ACCTS',\n'SEC_OVERDUE_ACCTS', 'SEC_CURRENT_BALANCE', 'SEC_SANCTIONED_AMOUNT',\n'SEC_DISBURSED_AMOUNT', 'PRIMARY_INSTAL_AMT', 'SEC_INSTAL_AMT',\n'NEW_ACCTS_IN_LAST_SIX_MONTHS', 'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS',\n'AVERAGE_ACCT_AGE', 'CREDIT_HISTORY_LENGTH', 'NO_OF_INQUIRIES',\n'loan_default']","1ab58808":"test_data.columns = ['UniqueID', 'disbursed_amount', 'asset_cost', 'ltv', 'branch_id',\n'supplier_id', 'manufacturer_id', 'Current_pincode_ID', 'Date_of_Birth',\n'Employment_Type', 'DisbursalDate', 'State_ID', 'Employee_code_ID',\n'MobileNo_Avl_Flag', 'Aadhar_flag', 'PAN_flag', 'VoterID_flag',\n'Driving_flag', 'Passport_flag', 'PERFORM_CNS_SCORE',\n'PERFORM_CNS_SCORE_DESCRIPTION', 'PRI_NO_OF_ACCTS', 'PRI_ACTIVE_ACCTS',\n'PRI_OVERDUE_ACCTS', 'PRI_CURRENT_BALANCE', 'PRI_SANCTIONED_AMOUNT',\n'PRI_DISBURSED_AMOUNT', 'SEC_NO_OF_ACCTS', 'SEC_ACTIVE_ACCTS',\n'SEC_OVERDUE_ACCTS', 'SEC_CURRENT_BALANCE', 'SEC_SANCTIONED_AMOUNT',\n'SEC_DISBURSED_AMOUNT', 'PRIMARY_INSTAL_AMT', 'SEC_INSTAL_AMT',\n'NEW_ACCTS_IN_LAST_SIX_MONTHS', 'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS',\n'AVERAGE_ACCT_AGE', 'CREDIT_HISTORY_LENGTH', 'NO_OF_INQUIRIES']","67ee29e1":"# Dropping some columns\ntrain_data = train_data.drop(['UniqueID'],axis=1)\n__train_data = train_data.drop(['loan_default'],axis=1)\n\ntest_data = test_data.drop(['UniqueID'],axis=1)","589492e8":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\n## Testing Pupose Feature Selection ##\ny = train_data['loan_default'] # Convert from string \"Yes\"\/\"No\" to binary\nfeature_names = [i for i in __train_data.columns if __train_data[i].dtype in [np.int64]]\nX = train_data[feature_names]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nmy_model = RandomForestClassifier(random_state=0).fit(train_X, train_y)\n\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\neli5.show_weights(perm, feature_names = val_X.columns.tolist())","961edd9b":"train_data.groupby('PERFORM_CNS_SCORE_DESCRIPTION').loan_default.sum()","54ef18f3":"import seaborn as sns\nsns.countplot(train_data['loan_default'])","72163cba":"#.groupby('Employment_Type').loan_default.sum\nsns.countplot(train_data['Employment_Type'])","d2b472d6":"#sns.catplot(x=\"Employment_Type\",hue=\"loan_default\", kind=\"box\", data=train_data)","863b551b":"sns.catplot(y=\"PERFORM_CNS_SCORE_DESCRIPTION\", x=\"loan_default\",kind=\"bar\", data=train_data)","32c7820a":"train_data.groupby('PERFORM_CNS_SCORE_DESCRIPTION').loan_default.count()","cb3e39bf":"predictor_with_categorical = train_data.select_dtypes(exclude=['int64','float64','<M8[ns]'])\npredictor_with_categorical['Employment_Type'] = predictor_with_categorical['Employment_Type'].fillna(predictor_with_categorical['Employment_Type'].mode()[0])\n#predictor_with_categorical.columns\n\ncolumns_names_categorical = train_data.loc[:,['Employment_Type','MobileNo_Avl_Flag', 'Aadhar_flag', 'PAN_flag', \n                                              'VoterID_flag','Driving_flag', 'Passport_flag']]\none_hot_encoded_training_predictors_with_categorical=pd.get_dummies(columns_names_categorical)\n\n","64bd55e8":"predictor_with_categorical_test = test_data.select_dtypes(exclude=['int64','float64','<M8[ns]'])\npredictor_with_categorical_test['Employment_Type'] = predictor_with_categorical_test['Employment_Type'].fillna(predictor_with_categorical_test['Employment_Type'].mode()[0])\n#predictor_with_categorical.columns\n\ncolumns_names_categorical_test = test_data.loc[:,['Employment_Type','PERFORM_CNS_SCORE_DESCRIPTION','MobileNo_Avl_Flag', 'Aadhar_flag', 'PAN_flag', \n                                              'VoterID_flag','Driving_flag', 'Passport_flag']]\none_hot_encoded_training_predictors_with_categorical_test=pd.get_dummies(columns_names_categorical_test)\n","da8c6185":"one_hot_encoded_training_predictors_with_categorical.head()","8dbe2d0c":"from datetime import date\n\n# --------Train data--------------#\npredictor_with_categorical['Date_of_Birth'] = pd.to_datetime(predictor_with_categorical['Date_of_Birth'],format=\"%d-%m-%y\")\npredictor_with_categorical['DisbursalDate'] = pd.to_datetime(predictor_with_categorical['DisbursalDate'],format=\"%d-%m-%y\")\n\n\n# --------Test data--------------#\npredictor_with_categorical_test['Date_of_Birth'] = pd.to_datetime(predictor_with_categorical_test['Date_of_Birth'],format=\"%d-%m-%y\")\npredictor_with_categorical_test['DisbursalDate'] = pd.to_datetime(predictor_with_categorical_test['DisbursalDate'],format=\"%d-%m-%y\")\nfrom dateutil.relativedelta import relativedelta\n\n##-----------Some of helper function-----------##\ndef f(end):\n    r = relativedelta(pd.to_datetime('now'), end) \n    return r.years\n\ndef fmonth(end):\n    r = relativedelta(pd.to_datetime('now'), end) \n    return r.years * 12 + r.months\n\ndef extract_month_str(_input):\n    '''This func extract month from a string'''\n    import re\n    list  = re.findall('\\d',_input)\n    return int(list[0])*12 + int(list[1])\n\n\n## ----------Applying function on Train Data--------- ##\npredictor_with_categorical['Age']= predictor_with_categorical['Date_of_Birth'].apply(f)\npredictor_with_categorical['DisbursalPeriod'] = predictor_with_categorical['DisbursalDate'].apply(fmonth)\npredictor_with_categorical['AVERAGE_ACCT_AGE_months'] = predictor_with_categorical['AVERAGE_ACCT_AGE'].apply(extract_month_str)\npredictor_with_categorical['CREDIT_HISTORY_LENGTH_months'] = predictor_with_categorical['CREDIT_HISTORY_LENGTH'].apply(extract_month_str)\n\n\n## ----------Applying function on Test Data--------- ##\npredictor_with_categorical_test['Age']= predictor_with_categorical_test['Date_of_Birth'].apply(f)\npredictor_with_categorical_test['DisbursalPeriod'] = predictor_with_categorical_test['DisbursalDate'].apply(fmonth)\npredictor_with_categorical_test['AVERAGE_ACCT_AGE_months'] = predictor_with_categorical_test['AVERAGE_ACCT_AGE'].apply(extract_month_str)\npredictor_with_categorical_test['CREDIT_HISTORY_LENGTH_months'] = predictor_with_categorical_test['CREDIT_HISTORY_LENGTH'].apply(extract_month_str)\n\n","4416dbdb":"predictor_with_categorical.head()","ed3088e1":"predictor_with_categorical.columns","a8df30b2":"predictor_with_categorical.Employment_Type.unique()","b2424c19":"predictor_with_categorical.columns[predictor_with_categorical.isnull().any()]\n\n","6007065b":"train_data.columns","cb3d75d5":"train_data.head()","0a2ec71a":"type(list(train_data.columns))","9731cf37":"# for i in list(train_data.columns):\n#     count = len(train_data[i].unique())\n#     print('The column {} is having {} unique values'.format(i,count))\n    ","1dba1afe":"_train_data = train_data.loc[:,['disbursed_amount', 'asset_cost', 'ltv', 'PERFORM_CNS_SCORE',\n        'PRI_NO_OF_ACCTS', 'PRI_ACTIVE_ACCTS',\n       'PRI_OVERDUE_ACCTS', 'PRI_CURRENT_BALANCE', 'PRI_SANCTIONED_AMOUNT',\n       'PRI_DISBURSED_AMOUNT', 'SEC_NO_OF_ACCTS', 'SEC_ACTIVE_ACCTS',\n       'SEC_OVERDUE_ACCTS', 'SEC_CURRENT_BALANCE', 'SEC_SANCTIONED_AMOUNT',\n       'SEC_DISBURSED_AMOUNT', 'PRIMARY_INSTAL_AMT', 'SEC_INSTAL_AMT',\n       'NEW_ACCTS_IN_LAST_SIX_MONTHS', 'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', 'NO_OF_INQUIRIES',\n       'State_ID', 'Employee_code_ID','branch_id','supplier_id','manufacturer_id','Current_pincode_ID']]\n\n# 'PERFORM_CNS_SCORE_DESCRIPTION','Aadhar_flag', 'PAN_flag', 'VoterID_flag',\n\n\n##------Test data selecting-------##\n\n_test_data = test_data.loc[:,['disbursed_amount', 'asset_cost', 'ltv', 'PERFORM_CNS_SCORE',\n        'PRI_NO_OF_ACCTS', 'PRI_ACTIVE_ACCTS',\n       'PRI_OVERDUE_ACCTS', 'PRI_CURRENT_BALANCE', 'PRI_SANCTIONED_AMOUNT',\n       'PRI_DISBURSED_AMOUNT', 'SEC_NO_OF_ACCTS', 'SEC_ACTIVE_ACCTS',\n       'SEC_OVERDUE_ACCTS', 'SEC_CURRENT_BALANCE', 'SEC_SANCTIONED_AMOUNT',\n       'SEC_DISBURSED_AMOUNT', 'PRIMARY_INSTAL_AMT', 'SEC_INSTAL_AMT',\n       'NEW_ACCTS_IN_LAST_SIX_MONTHS', 'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', 'NO_OF_INQUIRIES', \n        'State_ID', 'Employee_code_ID','branch_id','supplier_id','manufacturer_id','Current_pincode_ID']]","199583fe":"_train_data.columns","c1e2fdef":"predictor_with_categorical[predictor_with_categorical.Age < 0].head()","7958ae87":"test_predictor = predictor_with_categorical.loc[:,['Employment_Type','AVERAGE_ACCT_AGE','CREDIT_HISTORY_LENGTH','DisbursalPeriod'\n                                                                         ,'AVERAGE_ACCT_AGE_months', \n                                                   'CREDIT_HISTORY_LENGTH_months','branch_id','supplier_id','manufacturer_id','Current_pincode_ID'\n                                                                        ]]\n","c1461ec9":"# predictor_with_categorical[predictor_with_categorical.Date_of_Birth.dt.year > 2018]\n# import datetime\n# dd = datetime.datetime.strptime(d,'%y%m%d')\n# if dd.year > 2005:\n#    dd = dd.replace(year=dd.year-100)\n","c0365fab":"merged_df_predictors_train=pd.concat([_train_data,one_hot_encoded_training_predictors_with_categorical],axis=1)\nmerged_df_predictors_test=pd.concat([_test_data,one_hot_encoded_training_predictors_with_categorical_test],axis=1)\n#merged_df_predictors_test['PERFORM_CNS_SCORE_DESCRIPTION_Not Scored: More than 50 active Accounts found'] = 0\n","74bc6d3b":"merged_df_predictors_train.columns","48174587":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n","42dc147f":"feature_names = ['disbursed_amount', 'asset_cost', 'ltv', 'PERFORM_CNS_SCORE',\n       'PRI_NO_OF_ACCTS', 'PRI_ACTIVE_ACCTS', 'PRI_OVERDUE_ACCTS',\n       'PRI_CURRENT_BALANCE', 'PRI_SANCTIONED_AMOUNT', 'PRI_DISBURSED_AMOUNT',\n       'SEC_NO_OF_ACCTS', 'SEC_ACTIVE_ACCTS', 'SEC_OVERDUE_ACCTS',\n       'SEC_CURRENT_BALANCE', 'SEC_SANCTIONED_AMOUNT', 'SEC_DISBURSED_AMOUNT',\n       'PRIMARY_INSTAL_AMT', 'SEC_INSTAL_AMT', 'NEW_ACCTS_IN_LAST_SIX_MONTHS',\n       'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', 'NO_OF_INQUIRIES',\n       'MobileNo_Avl_Flag', 'Aadhar_flag', 'PAN_flag', 'VoterID_flag',\n       'Driving_flag', 'Passport_flag', 'Employment_Type_Salaried',\n       'Employment_Type_Self employed',\n        'State_ID', 'Employee_code_ID',\n                'branch_id','supplier_id','manufacturer_id','Current_pincode_ID']\n\n# 'PERFORM_CNS_SCORE_DESCRIPTION','Aadhar_flag', 'PAN_flag', 'VoterID_flag','Employment_Type',\n\ny = train_data['loan_default']\n    \nX = merged_df_predictors_train[feature_names]\nX_test = merged_df_predictors_test[feature_names]\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nmy_model = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini', max_depth=None,\n                                  max_features='auto',\n                                  max_leaf_nodes=None,\n                                  min_samples_leaf=1,\n                                  min_samples_split=2, min_weight_fraction_leaf=0.0,\n                                  n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n                                  verbose=0, warm_start=False).fit(train_X, train_y)\ny_pred = my_model.predict(val_X)","2dbd3db2":"merged_df_predictors_train.columns","96b91230":"from sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(val_y, y_pred)\nroc_auc = auc(false_positive_rate, true_positive_rate)","631e7994":"roc_auc","ed5d938d":"# import matplotlib.pyplot as plt\n# n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200]\n# train_results = []\n# test_results = []\n# for estimator in n_estimators:\n#    my_model = RandomForestClassifier(n_estimators=estimator, n_jobs=-1)\n#    my_model.fit(train_X, train_y)\n#    train_pred = my_model.predict(train_X)\n#    false_positive_rate, true_positive_rate, thresholds = roc_curve(train_y, train_pred)\n#    roc_auc = auc(false_positive_rate, true_positive_rate)\n#    train_results.append(roc_auc)\n#    y_pred = my_model.predict(val_X)\n#    false_positive_rate, true_positive_rate, thresholds = roc_curve(val_y, y_pred)\n#    roc_auc = auc(false_positive_rate, true_positive_rate)\n#    test_results.append(roc_auc)\n# from matplotlib.legend_handler import HandlerLine2D\n# line1, = plt.plot(n_estimators, train_results, 'b', label=\"Train AUC\")\n# line2, = plt.plot(n_estimators, test_results, 'r', label=\"Test AUC\")\n# plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n# plt.ylabel('AUC score')\n# plt.xlabel('n_estimators')\n# plt.show()\n","f56c1701":"from sklearn.metrics import confusion_matrix\nprint('The accuracy of the Random forest classifier is {:.2f} out of 1 on training data'.format(my_model.score(train_X, train_y)))\n","abd5f01a":"\nfrom sklearn.metrics import confusion_matrix\nprint('The accuracy of the Random forest classifier is {:.2f} out of 1 on test data'.format(my_model.score(val_X, val_y)))\n","db0837a5":"import xgboost as xgb\nXgb_model = xgb.XGBClassifier().fit(train_X, train_y)\ny_pred_xgb = my_model.predict(val_X)\nfrom sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(val_y, y_pred_xgb)\nroc_auc_xgb = auc(false_positive_rate, true_positive_rate)\n#roc_auc_xgb","65ecd05f":"# some testing\ntrain_data.groupby('Employment_Type').loan_default.count()","beed92c3":"train_data[['PERFORM_CNS_SCORE','PERFORM_CNS_SCORE_DESCRIPTION']]","4bb6882d":"# # Necessary imports: \n#  #cross_val_predict\n# from sklearn import metrics\n# from sklearn.model_selection import cross_val_score,cross_val_predict\n\n# # Perform 6-fold cross validation\n# scores = cross_val_score(my_model, train_X, train_y, cv=6)\n# print('Cross-validated scores:', scores)\n","dd252232":"# # Necessary imports: \n#  #cross_val_predict\n# from sklearn import metrics\n# from sklearn.model_selection import cross_val_score,cross_val_predict\n\n# # Perform 6-fold cross validation\n# scores = cross_val_score(Xgb_model, train_X, train_y, cv=6)\n# print('Cross-validated scores:', scores)\n","460f2aed":"prediction_xgb=Xgb_model.predict(X_test) ","f0a37ec9":"results_df_xgb=pd.DataFrame(prediction_xgb,columns=['predicted'])\nget_final_prediction_xgb=pd.concat([sample_submission,results_df_xgb], axis=1)\nget_final_prediction_xgb.drop(labels=['loan_default'], inplace=True, axis=1)\nget_final_prediction_xgb.rename(columns={'predicted':'loan_default'}, inplace=True)\nget_final_prediction_xgb.to_csv('Xgboost_Scaled.csv', index=False)","db18f7b0":"# #final_test_predictor = sc_X.transform(final_test_predictor)\n# prediction_xgb=xgb_clf.predict(final_test_predictor) #now we pass the testing data to the trained algorithm\n\n# from sklearn.metrics import confusion_matrix\n# results_df_xgb=pd.DataFrame(prediction_xgb,columns=['predicted'])\n# get_final_prediction_xgb=pd.concat([submission_df,results_df_xgb], axis=1)\n# get_final_prediction_xgb.drop(labels=['is_promoted'], inplace=True, axis=1)\n# get_final_prediction_xgb.rename(columns={'predicted':'is_promoted'}, inplace=True)\n# # you could use any filename. We choose submission here\n# get_final_prediction_xgb.to_csv('Xgboost_Scaled.csv', index=False)","2e761d97":"# import xgboost as xgb\n# from sklearn.model_selection import GridSearchCV\n\n# xgb_model = xgb.XGBClassifier()\n# optimization_dict = {'max_depth': [2,4,6],\n#                      'n_estimators': [50,100,200]}\n\n# model = GridSearchCV(xgb_model, optimization_dict, \n#                      scoring='accuracy', verbose=1)\n\n# model.fit(train_X, train_y)\n# print(model.best_score_)","e415bf8b":"XGB Classification****","770592a3":"train_data.groupby('Employment_Type').loan_default.sum()"}}