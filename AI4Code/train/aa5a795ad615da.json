{"cell_type":{"ac3a7b95":"code","e86ffd0f":"code","412828f4":"code","028198f2":"code","3bce969a":"code","901f8833":"code","b6c922f4":"code","d4fda518":"code","e4ce4226":"code","f6af9ba7":"code","173eddd4":"code","5398b47c":"code","1b7fe9fc":"code","d5bdb65b":"code","38c17383":"code","090417ef":"code","1458a609":"code","4869d65c":"code","e0d67325":"code","a4a64b47":"markdown","f863d3ac":"markdown","fe38db00":"markdown","3c69220f":"markdown","d339aca7":"markdown","9dc1c676":"markdown","2139a913":"markdown","fe394607":"markdown","1cb29638":"markdown"},"source":{"ac3a7b95":"#Credit Card Fraud Detection Project\n#Date April 21, 2021","e86ffd0f":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport sys\nimport scipy\nimport sklearn\n\nimport warnings\nwarnings.filterwarnings('ignore') # To supress warnings\nsns.set(style=\"whitegrid\") # set the background for the graphs\n","412828f4":"#importing data from kaggle\ndata = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\ndata.head(5)","028198f2":"print(data.columns)","3bce969a":"data.info()","901f8833":"data.isnull().sum()","b6c922f4":"data.describe()","d4fda518":"data.shape","e4ce4226":"# random_state helps assure that you always get the same output when you split the data\n# this helps create reproducible results and it does not actually matter what the number is\n# frac is percentage of the data that will be returned\ndata = data.sample(frac = 0.2, random_state = 1)\nprint(data.shape)","f6af9ba7":"# Visualize the count of survivors\nsns.countplot('Class', data=data)","173eddd4":"print(\"Fraud to NonFraud Ratio of {:.3f}%\".format(492\/284315*100))\n","5398b47c":"sns.kdeplot(data.Amount[data.Class == 0], label = 'Fraud', shade=True)\nsns.kdeplot(data.Amount[data.Class == 1], label = 'NonFraud', shade=True)\nplt.xlabel('Amount');","1b7fe9fc":"sns.kdeplot(data.Time[data.Class == 0], label = 'Fraud', shade=True)\nsns.kdeplot(data.Time[data.Class == 1], label = 'NonFraud', shade=True)\nplt.xlabel('Time')","d5bdb65b":"plt.figure(figsize=(15,15))\nsns.heatmap(data.corr()) # Displaying the Heatmap\n\nplt.title('Heatmap correlation')\nplt.show()","38c17383":"# get the columns from the dataframe\ncolumns = data.columns.tolist()\n\n# filter the columns to remove the data we do not want\ncolumns = [c for c in columns if c not in ['Class']]\n\n# store the variable we will be predicting on which is class\ntarget = 'Class'\n\n# X includes everything except our class column\nX = data[columns]\n# Y includes all the class labels for each sample\n# this is also one-dimensional\nY = data[target]\n\n# print the shapes of X and Y\nprint(X.shape)\nprint(Y.shape)","090417ef":"from sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor","1458a609":"# determine the number of fraud cases\nfraud = data[data['Class'] == 1]\nvalid = data[data['Class'] == 0]\n\noutlier_fraction = len(fraud) \/ float(len(valid))\nprint(outlier_fraction)\n\nprint('Fraud Cases: {}'.format(len(fraud)))\nprint('Valid Cases: {}'.format(len(valid)))","4869d65c":"state = 1\n\n# define the outlier detection methods\nclassifiers = {\n    # contamination is the number of outliers we think there are\n    'Isolation Forest': IsolationForest(max_samples = len(X),\n                                       contamination = outlier_fraction,\n                                       random_state = state),\n    # number of neighbors to consider, the higher the percentage of outliers the higher you want to make this number\n    'Local Outlier Factor': LocalOutlierFactor(\n    n_neighbors = 20,\n    contamination = outlier_fraction)\n}","e0d67325":"n_outliers = len(fraud)\n\nfor i, (clf_name, clf) in enumerate(classifiers.items()):\n    \n    # fit the data and tag outliers\n    if clf_name == 'Local Outlier Factor':\n        y_pred = clf.fit_predict(X)\n        scores_pred = clf.negative_outlier_factor_\n    else:\n        clf.fit(X)\n        scores_pred = clf.decision_function(X)\n        y_pred = clf.predict(X)\n        \n        \n        \n# reshape the prediction values to 0 for valid and 1 for fraud\n    y_pred[y_pred == 1] = 0\n    y_pred[y_pred == -1] = 1\n\n    # calculate the number of errors\n    n_errors = (y_pred != Y).sum()\n     \n    # classification matrix\n    print('{}: {}'.format(clf_name, n_errors))\n    print(accuracy_score(Y, y_pred))\n    print(classification_report(Y, y_pred))","a4a64b47":"Wow! The count of fraudulent transactions as compared to the non fraudulent one's is almost null. It makes it so difficult for us to classify the test data.\n\nRemember, Rule 1 of the dataset is that the predicted value should be somewhat equally divided between the two classes!\n\nAnyway, lets see how well we are able to perform!","f863d3ac":"As we can notice, most of the features are not correlated with each other.\n\nWhat can generally be done on a massive dataset is a dimension reduction. By picking the most important dimensions, there is a possiblity of explaining most of the problem, thus gaining a considerable amount of time while preventing the accuracy to drop too much.","fe38db00":"**1.1 Introduction**\n\nI have recently started using Kaggle after completing the course \"Python for Machine Learning and Data Science Bootcamp\" on Udemy. Right now, I'm trying to do as many projects as I can before starting my Masters in Data Science for Fall 2021.\n\nSo, without any further adeiu, lets get started!\n\n**1.2 About the Dataset**** \n* The datasets contains transactions made by credit cards in September 2013 by european cardholders.\n\n* This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions.\n\n* The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\n* It contains only numerical input variables which are the result of a PCA transformation.\n\n* Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data.\n\n* Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'.\n\n* Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset.\n\n* The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning.\n\n* Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\n**1.3 Objective**\n* The dataset contains a very minute percentage transacions, which are fraudulent. We need to find out those transactions which belong to the Fraud Class\n\n* Based on the data we have to generate a set of insights and recommendations that will help the credit card company from preventing the customers to be charged falsly!","3c69220f":"Now let us conduct some exploratory data analysis on our data!","d339aca7":"We notice that the feature time doesn't seem to have an impact in the frequency of frauds","9dc1c676":"The dataset does not contain any object data type, so we do not have to spend any time on conversion. Lets see if our data contains any null values!","2139a913":"So, we do not know what actually does V1,V2....V28 means due to data confidentiality, but what we know is they're going to help us draw insights from the data.","fe394607":"Looks like there a lot more instances of small fraud amounts than really large ones.","1cb29638":"Looking at precision for fraudulent cases (1) lets us know the percentage of cases that are getting correctly labeled. 'Precision' accounts for false-positives. 'Recall' accounts for false-negatives. Low numbers could mean that we are constantly calling clients asking them if they actually made the transaction which could be annoying.\n\nGoal: To get better percentages.\n\nOur Isolation Forest method (which is Random Forest based) was able to produce a better result. Looking at the f1-score 26% (or approx. 30%) of the time we are going to detect the fraudulent transactions."}}