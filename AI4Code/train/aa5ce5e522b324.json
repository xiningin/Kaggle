{"cell_type":{"ba4e857a":"code","709c5e28":"code","c43a1e29":"code","7acdb04d":"code","53dbecab":"code","b9058fa0":"code","76d64000":"code","e747a07a":"code","107c36eb":"code","4f44950e":"markdown","d9c54e93":"markdown","ac34b6cd":"markdown","c9d9b7e0":"markdown","23c251f8":"markdown","e06fdd3d":"markdown","6e1f95eb":"markdown","bd19f88f":"markdown","25972ca2":"markdown","8107d80b":"markdown","fbe23394":"markdown","51c79662":"markdown","e229001d":"markdown","5177e13b":"markdown","9781976d":"markdown","506686ba":"markdown"},"source":{"ba4e857a":"# base libs\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n# model libs\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import stats","709c5e28":"# dataframe information\ndef data_info(df):\n    \n    null_series = df.isnull().sum()\n    print(f\"######### MISSING VALUES:\")\n    print(f\"{null_series[null_series > 0]}\\n\")\n    \n    print(f\"######### LABEL BALANCE:\")\n    print(f\"{pd.value_counts(df['label'])}\\n\")\n    \n    print(f\"######### DESCRIBE:\")\n    print(f\"{df.describe()}\\n\")\n    \n    for column in df:\n        print('######### COLUMN NAME:', column)\n        print('TYPE:', df[column].dtypes)\n        print('LEN:', len(df[column]))\n        print('NUNIQUE:', df[column].nunique())\n        print('NaN:', df[column].isnull().values.sum())\n        print('')\n        \n    print(msno.bar(df, figsize=(16, 4)))  \n    \n\n# prepare neural network input\ndef df_to_nnetwork(df, normalization=True, verbose=False):\n    \n    # dataframe to neural network input format\n    dataset = df.values\n    nb_features = len(chosen_feat)\n\n    X = dataset[:,1:nb_features].astype(float)\n    Y = dataset[:,0]\n    if verbose:\n        print(f\"Raw data Stats:\")\n        print(f\"{stats.describe(X)}\\n\")\n\n    # data normalization\n    if normalization:\n        X = (X - X.min()) \/ (X.max() - X.min())\n        if verbose:\n            print(f\"Mean Normalized data Stats:\")\n            print(f\"{stats.describe(X)}\\n\")\n\n    # encode class values as integers\n    encoder = LabelEncoder()\n    encoder.fit(Y)\n    encoded_Y = encoder.transform(Y)\n\n    # convert integers to dummy variables (i.e. one hot encoded)\n    Y = to_categorical(encoded_Y)\n    if verbose:\n        print(f\"######### NEURAL NETWORK INPUT FORMAT:\")\n        print(f\"Data Shape: {X.shape}\")\n        print(f\"Labels Shape: {Y.shape}\")\n        \n    return X, Y\n\n\n# model build    \ndef build_model(X, Y, verbose=False):\n    \n    # definitions\n    neurons = 16\n    input_dim = len(X[0])\n    depth = 3\n\n    # define baseline model\n    def baseline_model():\n\n        # create model\n        model = Sequential()\n        for i in range(depth):\n            model.add(Dense(neurons, input_dim=input_dim, activation='relu'))\n        model.add(Dense(2, activation='softmax'))\n        # Compile model\n        model.compile(loss='categorical_crossentropy', \n                      optimizer='adam', \n                      metrics=['accuracy'])\n\n        return model\n        \n    model = baseline_model() \n    if verbose:\n        print(model.summary())\n           \n    return model  \n\n\n# quantitative model metrics\ndef result_metrics(data, labels, model):\n\n    y_pred = model.predict(data)\n    y_pred = (y_pred > 0.5)\n    print('Accuracy: %.2f%%' % (accuracy_score(labels, y_pred)*100))\n\n    cm = confusion_matrix(labels.argmax(axis=1), \n                          y_pred.argmax(axis=1))\n    print('\\nConfusion Matrix:\\n',cm)\n\n    print(\"\\nMetrics:\\n\", classification_report(labels, y_pred))\n    print(\"\")\n    \n\n# learning curves visualization    \ndef model_plots(history):\n     \n    plt.figure(figsize=(18, 6))\n    \n    # summarize history for accuracy\n    plt.subplot(1, 2, 1)\n    plt.plot(history['accuracy'])\n    plt.title(f\"model accuracy\")\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='best')\n\n    # summarize history for loss\n    plt.subplot(1, 2, 2)\n    plt.plot(history['loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper right')\n\n    plt.show()\n","c43a1e29":"# definitions\nlabel = 'SARS-Cov-2 exam result'\n\n# load data from excel\npath = '\/kaggle\/input\/covid19\/'\ndf = pd.read_excel(path+\"dataset.xlsx\")\n\nprint(f\"######### DATASET FEATURES:\")\nfeat_pool = []\nfor column in df:\n    if column != label:\n        feat_pool.append(column)\nprint(feat_pool)  \n\n# shuffle the rows reseting indexs\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\n\n# show dataset balance\nprint(f\"\\n######### LABELS BALANCING:\")\ndf[label].value_counts().plot.barh()","7acdb04d":"# data preprocess config\nbalance_data = True\ndrop_nan = True\n\n# feature list\nchosen_feat = [df['SARS-Cov-2 exam result'], \n               df['Leukocytes'], \n               df['Platelets'], \n               df['Eosinophils'],\n               df['Monocytes'],\n               #df['Neutrophils'],\n               #df['Hematocrit'],\n               #df['Hemoglobin']\n              ]\nkeys=['label', \n      'Leukocytes', \n      'Platelets', \n      'Eosinophils',\n      'Monocytes',\n      #'Neutrophils',\n      #'Hematocrit',\n      #'Hemoglobin'\n     ]\n\n# create a reduced dataframe with keys features \ndf = pd.concat(chosen_feat, axis=1, keys=keys)\n\n# drop NaN rows \nif drop_nan:\n    df = df.dropna(axis=0).reset_index(drop=True) \n\n# balancing data (same samples of positive and negatives)    \nif balance_data:\n    df_pos = df[df[\"label\"]==\"positive\"]\n    df_neg = df[df[\"label\"]==\"negative\"]\n    df_neg = df_neg.sample(n=len(df_pos), random_state=42)\n    # append a dataframe from another one\n    df = df_pos.append(df_neg, ignore_index=True)\n    df = df.sample(n=len(df), random_state=42).reset_index(drop=True)\nelse:\n    df = df.sample(n=len(df), random_state=42).reset_index(drop=True)\n    \n# general information about the data\ndata_info(df)    ","53dbecab":"# plot correlations between best features\nfeats_set = keys[1:]\nprint(\"Features Set Correlations:\", feats_set)\nsns.pairplot(df, hue='label')","b9058fa0":"# heat map\nprint(\"Features Heat Map:\")\nplt.figure(figsize=(10,8))\nmyBasicCorr = df[feats_set].corr('spearman')\nsns.heatmap(myBasicCorr, annot = True)","76d64000":"# prepare neural network input\ndata, labels = df_to_nnetwork(df, normalization=True, verbose=True)","e747a07a":"# train paramters\nbatch_size = 5\nepochs = 300\n\n# build the deep learning model\nmodel = build_model(data, labels, verbose=False)  \n\n# run!!\nHistory = model.fit(data, \n                    labels, \n                    batch_size=batch_size, \n                    epochs=epochs)","107c36eb":"# visualization of learning curves\nprint(\"\\nLearning Curves:\")\nmodel_plots(History.history)\n\n# quantitative metrics\nprint(f\"Features: {keys[1:]}\\n\")\nresult_metrics(data, labels, model)","4f44950e":"# Parser to Deep Learning Model Input","d9c54e93":"# Notebook Methods","ac34b6cd":"# Results:","c9d9b7e0":"<font size=\"4\">3- Replacing NaNs with mean values doesnt works either!! \nThis makes the data very biased because\nthe numbers of NaN values in this dataset is very high.<\/font>\n","23c251f8":"<font size=\"4\">1- My approach is a simple Neural Network model that can be easily extended to a more robust version but we are dealing with a very incomplete dataset so more robust neural network models will not help here anyway.<\/font>","e06fdd3d":"# Best Result:\n#### Accuracy: 87.35%, Features: ['Leukocytes', 'Platelets', 'Eosinophils', 'Monocytes']","6e1f95eb":"## Results with a reduced (drop rows with NaN) and balanced dataset using a deep learning approach. I believe this results provides a good and valuable north for what is the best set of features for a COVID-19 diagnostic.\n## Univariated Data:\n#### Accuracy: 73.49%, Feature: ['Leukocytes']\n#### Accuracy: 69.88%, Feature: ['Platelets'] \n#### Accuracy: 68.07%, Feature: ['Monocytes']\n#### Accuracy: 67.47%, Feature: ['Eosinophils']  \n#### Accuracy: 55.42%, Feature: ['Hemoglobin']\n#### Accuracy: 55.42%, Feature: ['Hematocrit']\n#### Accuracy: 52.00%, Feature: ['Neutrophils']\n## Multivariated Data (only statistical relevant features):\n#### Accuracy: 87.35%, Features: ['Leukocytes', 'Platelets', 'Eosinophils', 'Monocytes']\n#### Accuracy: 87.95%, Features: ['Leukocytes', 'Platelets', 'Monocytes']\n#### Accuracy: 85.54%, Features: ['Leukocytes', 'Platelets', 'Eosinophils', 'Monocytes', 'Hemoglobin']\n#### Accuracy: 83.73%, Features: ['Leukocytes', 'Platelets', 'Eosinophils', 'Hemoglobin']\n#### Accuracy: 81.93%, Features: ['Leukocytes', 'Platelets', 'Eosinophils']","bd19f88f":"# Results Metrics","25972ca2":"# Important notes:","8107d80b":"# Load Data","fbe23394":"# Challenge Answers:\n<font size=\"4\">TASK 1\n\u2022 Predict confirmed COVID-19 cases among suspected cases.\nBased on the results of laboratory tests commonly collected for a suspected COVID-19 case during a visit to the emergency room, would it be possible to predict the test result for SARS-Cov-2 (positive\/negative)?<\/font>","51c79662":"# Preprocessing Data","e229001d":"![best_result.png](attachment:best_result.png)","5177e13b":"# Model Build and Run ","9781976d":"<font size=\"4\">2- We should be very carefull with unbalanced data (much more negatives than positives) because\nthis alone can cause a heavy data overfitting (causing dangerous misleading interpretrations) especially \nusing neural networks in healthcare. So I preprocessed the data using only non NaN rows and equal amout of positives and negative labels. This reduces the amount of data and overall accuracy but the results are much more resilient for real world prediction. No model in task 1 should be beyond 90% because we have a very limited amount of realiable data.<\/font>\n","506686ba":"# Features Data Correlations"}}