{"cell_type":{"eae7e58e":"code","1ddfa54d":"code","b0a729b2":"code","9c06c3e4":"code","e66e942a":"markdown","1963da35":"markdown"},"source":{"eae7e58e":"import imblearn\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\n\n# Disabling Tensorflow warnings.\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \nimport tensorflow as tf","1ddfa54d":"def create_datasets(df, test_size=.2):\n    \"\"\"\n        Balancing the dataset with undersampling\n        and creating training and validation datasets.\n\n        Dataset with the calculated value of the psychotype saved separately\n        to ensure the possibility of comparing the SP-psychotypes\n        with the resulting prediction.\n    \"\"\"\n    # Balancing the dataset with undersampling.\n    under_sampler = imblearn.under_sampling.RandomUnderSampler()\n    balanced_df, _ = under_sampler.fit_resample(df, df['label'])\n\n    data, test_data = train_test_split(\n        balanced_df,\n        shuffle=True,\n        stratify=balanced_df['label'],\n        test_size=test_size\n    )\n\n    labels = data['label']\n    data = data.drop(columns=['label', 'psychotype'])\n    \n    val_labels = test_data['label']\n    val_data = test_data.drop(columns=['label', 'psychotype'])\n\n    return data, labels, val_data, val_labels, test_data\n\ndef create_compile_fit(\n    data,\n    labels,\n    batch_size=1000,\n    early_stop_delta=.01,\n    early_stop_patience=5,\n    epochs=50,\n    learning_rate=.001,\n    metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy')],\n    val_size=.2\n):\n    \"\"\"\n    Model creating, compiling and fitting.\n\n    Out - model instance and fitting history.\n\n    For our task, we use simple 32-16 architecture.\n    \"\"\"\n    inputs = tf.keras.Input(shape=data.shape[1])\n    x = tf.keras.layers.Dense(data.shape[1])(inputs)\n    x = tf.keras.layers.Dense(32, activation='relu')(x)\n    x = tf.keras.layers.Dense(16, activation='relu')(x)\n    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n        loss=tf.keras.losses.BinaryCrossentropy(),\n        metrics=metrics\n    )\n\n    history = model.fit(\n        data,\n        labels,\n        batch_size=batch_size,\n        callbacks=[\n            # Callback for stopping the learning process\n            # when there is no minimization of the loss function values.\n            tf.keras.callbacks.EarlyStopping(\n                monitor='val_loss', \n                verbose=0,\n                patience=early_stop_patience,\n                mode='min',\n                min_delta=early_stop_delta,\n                restore_best_weights=True\n            )\n        ],\n        epochs=epochs,\n        shuffle=True,\n        use_multiprocessing=True,\n        validation_split=val_size, \n        verbose=0\n    )\n\n    return model, history","b0a729b2":"df = pd.read_csv('\/kaggle\/input\/kpmiru-questionnaires-data\/kpmi_ru_data.csv')\n\n# Deleting columns with scales values.\ndf.drop(columns = ['e', 'f', 'i', 'j', 'n', 'p', 's', 't'], inplace=True)\n# Deletes response times for each question.\ndf.drop(columns = [f't{i}' for i in range(1, 64) if i != 61], inplace=True)\n# Formation of target label for question 6 and 33 answers through logical AND.\ndf['label'] = 0\ndf.loc[(df['q6'] == 2) & (df['q33'] == 2), 'label'] = 1\n# Removing answers on 6 and 33 question from the dataset.\ndf.drop(columns=['q6', 'q33'], inplace=True)\n\ndf","9c06c3e4":"BATCH_SIZE = 10000\nEARLY_STOP_DELTA = .001\nEARLY_STOP_PATIENCE = 20\nEPOCHS = 1000\nITERS = 100\nLEARNING_RATE = 0.0001\n# The minimum value of the forecast for attributing the target metric to the properties of NP-psychotypes.\nTARGET_PRED_BORDER = .8\nTEST_SIZE = .2\nVAL_SIZE = .1\n\nsum_eval_results = []\nprint('Iters: ', end='', flush=True)\n# To ensure representativeness, predictability, and leveling of random_seed,\n# data preparation, model creation\/fitting, and validation procedures\n# are performed several times.\nfor i in range(ITERS):\n    # Datasets creation at each iteration of the loop\n    # to level the imbalance of the original dataset\n    # and due to the use of RandomUnderSampler for data balancing.\n    data, labels, val_data, val_labels, test_data = create_datasets(\n        df=df,\n        test_size=TEST_SIZE\n    )\n    \n    model, history = create_compile_fit(\n        data,\n        labels,\n        batch_size=BATCH_SIZE,\n        early_stop_delta=EARLY_STOP_DELTA,\n        early_stop_patience=EARLY_STOP_PATIENCE,\n        epochs=EPOCHS,\n        learning_rate=LEARNING_RATE,\n        metrics=[\n            tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n            tf.keras.metrics.TruePositives(name='tp'),\n            tf.keras.metrics.FalsePositives(name='fp'),\n            tf.keras.metrics.TrueNegatives(name='tn'),\n            tf.keras.metrics.FalseNegatives(name='fn'), \n            tf.keras.metrics.Precision(name='precision'),\n            tf.keras.metrics.Recall(name='recall')\n        ],\n        val_size=VAL_SIZE\n    )\n\n    eval_results = model.evaluate(\n        val_data,\n        val_labels,\n        batch_size=BATCH_SIZE,\n        verbose=0\n    )\n    # Adding the number of epochs to the validation information.\n    eval_results.append(len(history.history['loss']))\n    sum_eval_results.append(eval_results)\n    \n    # Calculation of the target prediction\n    # and number of SJ-psychotypes with a prediction greater than the boundary.\n    test_data['pred'] = model.predict_on_batch(val_data)\n    target_profiles_cnt = len(\n        test_data[\n            test_data['psychotype'].str.contains('.N.P', regex=True)\n        ].index)\n    target_predicted_cnt = len(\n        test_data[\n            (test_data['psychotype'].str.contains('.N.P', regex=True)) \\\n            & (test_data['pred'] > TARGET_PRED_BORDER)\n        ].index)\n    eval_results.append(target_predicted_cnt\/target_profiles_cnt)\n\n    print(f'{i+1}..', end='', flush=True)\nprint('done.')\n\n# Preparation and printing of results.\nmetrics=model.metrics_names + ['epochs', 'target_pred']\nres = pd.DataFrame(sum_eval_results, columns=metrics)\nres.describe().transpose().drop(columns=['count']).round(2)","e66e942a":"This notebook demonstrates an example of creating a predictive psychodiagnostic metric based on the [KPMI](https:\/\/kpmi.ru) test.\n\nAs an MBTI-like test, KPMI identifies 16 personality types, which are divided into 4 groups. One of these groups, [with personality types SJ](https:\/\/www.16personalities.com\/personality-types#sentinels), includes people who have pronounced values of the S scale - the perception of information through the senses with an emphasis on facts; and the J scale, propensity to plan and control. Polar to the pair of SJ scales is the NP pair, which can be summarized by the words \u201cintuition, susceptibility to uncertainty, spontaneity\u201d.\n\nIn the KPMI test, among others, there are 2 questions that affect the values of the N and P scales:\n1. *You like people who: (1) follow generally accepted standards and do not attract attention \/ (2) so original that they don't care if they pay attention to them or not*.\n2. *You believe that following a daily routine: (1) helps you to work calmly \/ (2) limits your freedom.*.\n\nIn the code below, on the basis of logical AND, the metric of \"*inclination to originality, non-standard and novelty*\" in people with SJ psychotype is formed, which is formed when choosing the second answer to the both questions. Next, based on the [kpmi.ru dataset](https:\/\/www.kaggle.com\/pmenshih\/kpmiru-questionnaires-data), a model is built that makes a prediction of the target metric on the test dataset. After that, all positive predictions for SJ personality types are selected and their number is estimated. Such characteristics cannot be measured by default KPMI method and traditional psychodiagnostic methods without using machine learning algorithms on big data.","1963da35":"The `target_pred` parameter in the results obtained shows the proportion of people with a positive prediction of the target metric among all respondents with the SJ psychotype."}}