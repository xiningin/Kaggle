{"cell_type":{"902d075f":"code","80090a4b":"code","d81e39dd":"code","aaf6a6a8":"code","b3293472":"code","7b63ee51":"code","2dac784d":"code","654bfda6":"code","267ac42b":"code","f6493be0":"code","c418f6d9":"code","e27eb43f":"code","c5bf3f17":"code","ea2e8e9d":"code","1210654f":"code","834b34bb":"code","6244f5eb":"code","08e204d5":"code","3fb80c81":"code","609a1be9":"code","46eed349":"code","8774c1af":"code","69e29fc0":"code","dc410bac":"code","d8aed2ab":"code","d396e663":"code","570813f7":"code","8a5e7940":"code","bd6569ca":"code","a084c896":"code","849d0fa4":"code","0c947c17":"code","0b7625d4":"code","e730278c":"code","251c5209":"code","2a3497c0":"code","90a909ba":"code","40622583":"code","563c5d9c":"code","9c61840d":"code","8c71e468":"code","0fc14513":"code","6baa65e9":"code","0b68e5d4":"code","ffb93644":"code","25f59372":"code","e198f3e2":"code","cba725b7":"code","5b00cbb9":"code","9099061f":"code","ea32eec6":"markdown","8a0f7e23":"markdown","c3c3726c":"markdown","bc4b559d":"markdown","89e12f1e":"markdown","2ea7bbc2":"markdown","d0d80501":"markdown","360e7359":"markdown","0f7d61fa":"markdown","f5190b94":"markdown","f7dede0c":"markdown","f0de1492":"markdown","752665c0":"markdown","354285b4":"markdown","6fb366c0":"markdown","7c8c9d76":"markdown","b76fcfb2":"markdown"},"source":{"902d075f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","80090a4b":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom xgboost import XGBClassifier\n\n%matplotlib inline","d81e39dd":"df_train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/test.csv')\n\ntarget = 'target'\ny_train = df_train[target]\ndf_test_ids = df_test['id']","aaf6a6a8":"y_train.mean()","b3293472":"print(df_train.shape)\nprint(df_test.shape)","7b63ee51":"df_test_ids","2dac784d":"df = pd.concat([df_train.drop(columns=target), df_test], ignore_index = True)","654bfda6":"df.info()","267ac42b":"df.describe()","f6493be0":"df.describe(include=['O'])","c418f6d9":"sns.distplot(df_train[target]);","e27eb43f":"#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train[target].skew())\nprint(\"Kurtosis: %f\" % df_train[target].kurt())","c5bf3f17":"#https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n#correlation matrix\ndf_dummy = pd.get_dummies(df_train)[:5000]\ncorrmat = df_dummy.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","ea2e8e9d":"#correlation matrix\ncorrmat = df_train.sample(5000).corr()\nk = 10 #number of variables for heatmap\nf, ax = plt.subplots(figsize=(12, 9))\ncols = corrmat.nlargest(k, target)[target].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","1210654f":"#correlation heatmap of dataset\ndef correlation_heatmap(df, k):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    corrmat = df.corr()\n    cols = corrmat.nlargest(k, target)[target].index\n    _ = sns.heatmap(\n        \n        df[cols].corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(pd.get_dummies(df_train)[:100],7)","834b34bb":"#bivariate analysis saleprice\/grlivarea\nvar = 'cont8'\ndata = pd.concat([df_train[target], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y=target);","6244f5eb":"#https:\/\/www.kaggle.com\/gaetanlopez\/tps-complete-eda-single-lgb-tuning-strategy\ncols = df_train.select_dtypes(include='number').drop(columns=['id',target]).columns\n\nfig = plt.figure(figsize=(30,50))\ni=1\nfor cont in cols:\n    plt.subplot(len(cols), 3, i)\n    sns.histplot(df_train[cont])\n    i+=1\n    \n    plt.subplot(len(cols), 3, i)\n    plt.boxplot(x = df_train[cont])\n    i+=1\n\n    plt.subplot(len(cols), 3, i)\n    sns.violinplot(data = df_train, x = 'target', y = cont)\n    i+=1\n\n    plt.tight_layout()\n\nplt.show()","08e204d5":"var = 'cat16'\ndata = pd.concat([df_train[target], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=target, data=data) #NOTE: only first variable in the Target is selected\n#fig.axis(ymin=0, ymax=800000);\n#plt.xticks(rotation=90);","3fb80c81":"var = 'cat5'\ndf_train.groupby(var)[target].value_counts()","609a1be9":"MLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    #consumes a lot of RAM, may cause notebook to fail b\/c of te 16gb limit\n    #https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.GaussianProcessClassifier.html\n    #\"Note that this class thus does not implement a true multi-class Laplace approximation.\"\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    XGBClassifier()    \n    ]","46eed349":"df_dummies = pd.get_dummies(df).drop(columns='id')","8774c1af":"def MLA_test(X_initial, y_initial, f, t, verbose=0):\n    X = X_initial[f:t]    \n    y = y_initial[f:t]    \n    \n    #split dataset in cross-validation with this splitter class: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n    #note: this is an alternative to train_test_split\n    cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60\/30 split intentionally leaving out 10%\n    \n    #create table to compare MLA metrics\n    MLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\n    MLA_compare = pd.DataFrame(columns = MLA_columns)\n\n    #index through MLA and save performance to table\n    row_index = 0\n    for alg in MLA:\n        #set name and parameters\n        MLA_name = alg.__class__.__name__\n        MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n        MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n\n        #score model with cross validation: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n        cv_results = model_selection.cross_validate(alg, X, y, cv  = cv_split, n_jobs=-1, verbose=0, return_train_score=True)\n\n        training_score = cv_results['train_score'].mean()\n        test_score = cv_results['test_score'].mean()\n        if verbose == 1:\n            print('{}\/{}'.format(row_index+1, len(MLA)), MLA_name, \" - \", training_score, test_score)\n\n        MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n        MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = training_score\n        MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = test_score\n        #if this is a non-bias random sample, then +\/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n        MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n    \n        #save MLA predictions - see section 6 for usage\n        #alg.fit(data1[data1_x_bin], df[Target])\n        #MLA_predict[MLA_name] = alg.predict(df[data1_x_bin])\n\n        row_index+=1\n    \n    #print and sort table: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.sort_values.html\n    MLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n    \n    return MLA_compare","69e29fc0":"MLA_compare = MLA_test(df_dummies, y_train, 0, 5000, verbose=1)\nMLA_compare","dc410bac":"MLA_compare.values[0]","d8aed2ab":"model = None\nif model!=None:\n    cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60\/30 split intentionally leaving out 10%\n    cv_results = model_selection.cross_validate(model, X, y, cv  = cv_split, n_jobs=-1, verbose=0, return_train_score=True)\n\n    training_score = cv_results['train_score'].mean()\n    test_score = cv_results['test_score'].mean()\n    print(model.__class__.__name__, \" - \", training_score, test_score)","d396e663":"f = 0\nt = 10000\nmodel = MLA[4]\nmodel.fit(df_dummies[f:t], y_train[f:t])","570813f7":"X_test = df_dummies[df_train.shape[0]:]\nresults = model.predict(X_test)\ndf_results = pd.DataFrame({'id':df_test_ids, 'target':results})\ndf_results","8a5e7940":"df_results.to_csv('classifierSubmission.csv', index=False)","bd6569ca":"f = 100000\nt = 150000\n\nconf_mat = confusion_matrix(y_true=y_train[f:t], y_pred=model.predict(df_dummies[f:t]))\nprint('Confusion matrix:\\n', conf_mat)\n\nlabels = ['Class 0', 'Class 1']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","a084c896":"# Class count\ncount_class_0, count_class_1 = df_train.target.value_counts()\n\n# Divide by class\ndf_class_0 = df_train[df_train['target'] == 0]\ndf_class_1 = df_train[df_train['target'] == 1]","849d0fa4":"df_class_0_under = df_class_0.sample(count_class_1)\ndf_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n\nprint('Random under-sampling:')\nprint(df_test_under.target.value_counts())\n\ndf_test_under.target.value_counts().plot(kind='bar', title='Count (target)');","0c947c17":"df_class_1_over = df_class_1.sample(count_class_0, replace=True)\ndf_test_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n\nprint('Random over-sampling:')\nprint(df_test_over.target.value_counts())\n\ndf_test_over.target.value_counts().plot(kind='bar', title='Count (target)');","0b7625d4":"import tensorflow as tf","e730278c":"f = 0\nt = 300000\nX = df_dummies[f:t]\ny = y_train[f:t]","251c5209":"model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Dense(20, activation='relu', input_shape= (642,)))\n#model.add(tf.keras.layers.Dense(20, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='sgd', loss='mse', metrics=['accuracy'])\nhistory = model.fit(X, y, epochs=10, validation_split=0.3, shuffle=True)\n#history = model.fit(pd.get_dummies(df_test_under).drop(columns=['target','id']), df_test_under['target'], epochs=10, validation_split=0.3, shuffle=True)","2a3497c0":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","90a909ba":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","40622583":"#Sanity check\nf = 100000\nt = 150000\nmodel.evaluate(df_dummies[f:t], y_train[f:t])","563c5d9c":"y_pred = model.predict(df_dummies[df_train.shape[0]:])","9c61840d":"df_results = pd.DataFrame({'id':df_test_ids, 'target':y_pred.reshape(y_pred.shape[0])})","8c71e468":"df_results.head()","0fc14513":"df_results.to_csv('sequentialNNSubmission.csv', index=False)","6baa65e9":"import optuna\nfrom sklearn.metrics import roc_auc_score\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier","0b68e5d4":"f = 0\nt = 300000\nX = df_dummies[f:t]\ny = y_train[f:t]","ffb93644":"def objective(trial, data = X, target = y):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n    params = {\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 11, 333),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'max_depth': trial.suggest_int('max_depth', 5, 20),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.01, 0.02, 0.05, 0.005, 0.1]),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.5),\n        'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n        'random_state': 42,\n        'boosting_type': 'gbdt',\n        'metric': 'AUC',\n        #'device': 'gpu'\n    }\n    \n    model = LGBMClassifier(**params)  \n    model.fit(X_train, y_train, eval_set = [(X_val,y_val)], early_stopping_rounds = 222, verbose = False)\n    y_pred = model.predict_proba(X_val)[:,1]\n    roc_auc = roc_auc_score(y_val, y_pred)\n\n    return roc_auc","25f59372":"study = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, n_trials = 50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)","e198f3e2":"optuna.visualization.plot_optimization_history(study)","cba725b7":"paramsLGBM = study.best_trial.params\nparamsLGBM['boosting_type'] = 'gbdt'\nparamsLGBM['metric'] = 'AUC'\nparamsLGBM['random_state'] = 42","5b00cbb9":"from sklearn.model_selection import KFold\n\nfolds = KFold(n_splits = 10, shuffle = True, random_state = 42)\n\npredictions = np.zeros(len(X_test))\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n    \n    X_train, X_val = X.iloc[trn_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[trn_idx], y.iloc[val_idx]\n\n    model = LGBMClassifier(**paramsLGBM)\n   \n    model.fit(X_train, y_train, eval_set = [(X_val, y_val)], eval_metric = 'auc', verbose = False, early_stopping_rounds = 222)\n    \n    predictions += model.predict_proba(X_test)[:,1] \/ folds.n_splits ","9099061f":"submission = pd.DataFrame({'id': df_test_ids, 'target': predictions})\nsubmission.to_csv('submissionLGBM.csv', index = False)","ea32eec6":"# LGBM + Optuna\n\n- https:\/\/www.kaggle.com\/dmitryuarov\/catboost-vs-xgb-vs-lgbm-tps-mar-21\n- https:\/\/www.kaggle.com\/calebyenusah\/lgbm-and-optuna-tps-march-2021\n","8a0f7e23":"## Continious Variables","c3c3726c":"Even though classifiers achieved upto 84% accuracy in evaluation data, this success didn't translate to test data. Highest submission score was 76%.","bc4b559d":"We can see that model is misclassifying 1\/3 of the class 1. Seeing this inbalanced state, I decided to learn about resampling.\n\n## Resampling\n\n\"A widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and \/ or adding more examples from the minority class (over-sampling)\"","89e12f1e":"Submission of TF model achieved 86.915% accuracy.","2ea7bbc2":"# To Do and Resources:\n\n- [X] Check how much time XGBClassifier takes. It took unusually long, 84 seconds. Next highest was 4.1 seconds. Remove it if necessary\n- [X] Find out the score over time in best performing MLAs. See if there is overfitting.\n- [ ] https:\/\/towardsdatascience.com\/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n- [ ] https:\/\/towardsdatascience.com\/optimizing-hyperparameters-in-random-forest-classification-ec7741f9d3f6\n- [ ] https:\/\/www.datacamp.com\/community\/tutorials\/random-forests-classifier-python\n- [ ] Check if the nxn variable relation graph can be implemented (In notebook 4)\n\n### Statistics:\n\n- [ ] https:\/\/www.kaggle.com\/c\/porto-seguro-safe-driver-prediction\/discussion\/41037\n- [ ] https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html\n- [ ] https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.ShuffleSplit.html\n- [ ] https:\/\/en.wikipedia.org\/wiki\/Cross-validation_(statistics)\n\n### Imbalanced Data (!!!!!)\n\n- [X] https:\/\/www.kaggle.com\/rafjaa\/resampling-strategies-for-imbalanced-datasets\n\n### EDA Notebooks:\n\n- [X] https:\/\/www.kaggle.com\/sudalairajkumar\/winning-solutions-of-kaggle-competitions\n- [X] https:\/\/www.kaggle.com\/kanncaa1\/data-sciencetutorial-for-beginners\n- [X] https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n- [X] https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n- [X] https:\/\/www.kaggle.com\/calebyenusah\/lgbm-and-optuna-tps-march-2021\n\n### LGBM Notebooks:\n- [ ] https:\/\/www.kaggle.com\/gaetanlopez\/tps-complete-eda-single-lgb-tuning-strategy\n- [ ] https:\/\/www.kaggle.com\/rmiperrier\/lgbm-optuna\n- [ ] https:\/\/www.kaggle.com\/ekozyreff\/tps-2021-03-lightgbm-optuna-10-folds","d0d80501":"## Categorical Variables","360e7359":"# Classifier Test\nBorrowed code from [this notebook](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy).","0f7d61fa":"# TF Model","f5190b94":"# Data Exploration","f7dede0c":"## Random under-sampling\n","f0de1492":"Relation between categorical variable and the target:","752665c0":"Relation between continious variable and the target:","354285b4":"Achieved 89.249% with the LGBM + Optuna.","6fb366c0":"When I used resampling, accuracy droped to 70%.","7c8c9d76":"# Post-training Evaluation\n\nBorrowed code from [this notebook](https:\/\/www.kaggle.com\/rafjaa\/resampling-strategies-for-imbalanced-datasets).","b76fcfb2":"## Random over-sampling"}}