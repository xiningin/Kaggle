{"cell_type":{"5a3af265":"code","42abc11e":"code","2202d46b":"code","5de0ecf5":"code","33c2e41e":"code","7d2f66c1":"code","974a85da":"code","445bff02":"code","daf59090":"code","623a966c":"code","0520ca50":"code","44745d2a":"code","f0e3af4a":"code","4573ffdc":"code","e41ab6fa":"code","e90ffd31":"code","a5002ac6":"code","0ba0f8d1":"code","687a4eed":"code","b45c69c0":"code","22d6f3e3":"code","1a8dea86":"code","ee0c618f":"code","7e269c63":"code","a3f0a77a":"code","e20a3b53":"code","f5962edf":"code","42d5f6ec":"markdown","84fd1bea":"markdown","d621a8d8":"markdown","0776499f":"markdown","a16c28d4":"markdown","d84cc736":"markdown","1bca7be6":"markdown","2ba6133b":"markdown","054dfda5":"markdown","61474420":"markdown"},"source":{"5a3af265":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns","42abc11e":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","2202d46b":"df.describe()","5de0ecf5":"df.isnull().sum().max()","33c2e41e":"df.columns","7d2f66c1":"print('Normal', round(df['Class'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')","974a85da":"colors = [\"#0101DF\", \"#DF0101\"]\n\nsns.countplot('Class', data=df, palette=colors)\nplt.title('Class Distributions \\n (0: Normal || 1: Fraud)', fontsize=14)","445bff02":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\namount_val = df['Amount'].values\ntime_val = df['Time'].values\n\nsns.distplot(amount_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of Transaction Amount', fontsize=14)\nax[0].set_xlim([min(amount_val), max(amount_val)])\n\nsns.distplot(time_val, ax=ax[1], color='b')\nax[1].set_title('Distribution of Transaction Time', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])\n\nplt.show()","daf59090":"from sklearn.preprocessing import RobustScaler\n\n# RobustScaler is less prone to outliers.\n\nrob_scaler = RobustScaler()\n\ndf['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\ndf.drop(['Time','Amount'], axis=1, inplace=True)","623a966c":"scaled_amount = df['scaled_amount']\nscaled_time = df['scaled_time']\n\ndf.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndf.insert(0, 'scaled_amount', scaled_amount)\ndf.insert(1, 'scaled_time', scaled_time)\n\n# Amount and Time are Scaled!\n\ndf.head()","0520ca50":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\namount_val = df['scaled_amount'].values\ntime_val = df['scaled_time'].values\n\nsns.distplot(amount_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of Transaction Amount', fontsize=14)\nax[0].set_xlim([min(amount_val), max(amount_val)])\n\nsns.distplot(time_val, ax=ax[1], color='b')\nax[1].set_title('Distribution of Transaction Time', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])\n\nplt.show()","44745d2a":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\n\nprint('Normal', round(df['Class'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')\n\nX = df.drop('Class', axis=1)\ny = df['Class']\n\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor train_index, test_index in sss.split(X, y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n\n\n# Turn into an array\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 100)\n\nprint('Label Distributions: \\n')\nprint(train_counts_label\/ len(original_ytrain))\nprint(test_counts_label\/ len(original_ytest))","f0e3af4a":"df = df.sample(frac=1)\n\n# amount of fraud classes 492 rows.\nfraud_df = df.loc[df['Class'] == 1]\nnon_fraud_df = df.loc[df['Class'] == 0][:492]\n\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n# Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\n\nnew_df.head()","4573ffdc":"print('Distribution of the Classes in the subsample dataset')\nprint(new_df['Class'].value_counts()\/len(new_df))\n\nsns.countplot('Class', data=new_df, palette=colors)\nplt.title('Equally Distributed Classes', fontsize=14)\nplt.show()","e41ab6fa":"# Make sure we use the subsample in our correlation\n\nf, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n\n# Entire DataFrame\ncorr = df.corr()\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\nax1.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=14)\n\nsub_sample_corr = new_df.corr()\nsns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)\nax2.set_title('SubSample Correlation Matrix \\n (use for reference)', fontsize=14)\nplt.show()","e90ffd31":"f, axes = plt.subplots(ncols=4, figsize=(20,4))\n\n# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\nsns.boxplot(x=\"Class\", y=\"V17\", data=new_df, palette=colors, ax=axes[0])\naxes[0].set_title('V17 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V14\", data=new_df, palette=colors, ax=axes[1])\naxes[1].set_title('V14 vs Class Negative Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V12\", data=new_df, palette=colors, ax=axes[2])\naxes[2].set_title('V12 vs Class Negative Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V10\", data=new_df, palette=colors, ax=axes[3])\naxes[3].set_title('V10 vs Class Negative Correlation')\n\nplt.show()","a5002ac6":"f, axes = plt.subplots(ncols=4, figsize=(20,4))\n\n# Positive correlations (The higher the feature the probability increases that it will be a fraud transaction)\nsns.boxplot(x=\"Class\", y=\"V11\", data=new_df, palette=colors, ax=axes[0])\naxes[0].set_title('V11 vs Class Positive Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V4\", data=new_df, palette=colors, ax=axes[1])\naxes[1].set_title('V4 vs Class Positive Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V2\", data=new_df, palette=colors, ax=axes[2])\naxes[2].set_title('V2 vs Class Positive Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V19\", data=new_df, palette=colors, ax=axes[3])\naxes[3].set_title('V19 vs Class Positive Correlation')\n\nplt.show()","0ba0f8d1":"from scipy.stats import norm\n\nf, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))\n\nv14_fraud_dist = new_df['V14'].loc[new_df['Class'] == 1].values\nsns.distplot(v14_fraud_dist,ax=ax1, fit=norm, color='#FB8861')\nax1.set_title('V14 Distribution \\n (Fraud Transactions)', fontsize=14)\n\nv12_fraud_dist = new_df['V12'].loc[new_df['Class'] == 1].values\nsns.distplot(v12_fraud_dist,ax=ax2, fit=norm, color='#56F9BB')\nax2.set_title('V12 Distribution \\n (Fraud Transactions)', fontsize=14)\n\n\nv10_fraud_dist = new_df['V10'].loc[new_df['Class'] == 1].values\nsns.distplot(v10_fraud_dist,ax=ax3, fit=norm, color='#C5B3F9')\nax3.set_title('V10 Distribution \\n (Fraud Transactions)', fontsize=14)\n\nplt.show()","687a4eed":"# # -----> V14 Removing Outliers (Highest Negative Correlated with Labels)\nv14_fraud = new_df['V14'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nv14_iqr = q75 - q25\nprint('iqr: {}'.format(v14_iqr))\n\nv14_cut_off = v14_iqr * 1.5\nv14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\nprint('Cut Off: {}'.format(v14_cut_off))\nprint('V14 Lower: {}'.format(v14_lower))\nprint('V14 Upper: {}'.format(v14_upper))\n\noutliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\nprint('Feature V14 Outliers for Fraud Cases: {}'.format(len(outliers)))\nprint('V10 outliers:{}'.format(outliers))\n\nnew_df = new_df.drop(new_df[(new_df['V14'] > v14_upper) | (new_df['V14'] < v14_lower)].index)\nprint('----' * 44)\n\n# -----> V12 removing outliers from fraud transactions\nv12_fraud = new_df['V12'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)\nv12_iqr = q75 - q25\n\nv12_cut_off = v12_iqr * 1.5\nv12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off\nprint('V12 Lower: {}'.format(v12_lower))\nprint('V12 Upper: {}'.format(v12_upper))\noutliers = [x for x in v12_fraud if x < v12_lower or x > v12_upper]\nprint('V12 outliers: {}'.format(outliers))\nprint('Feature V12 Outliers for Fraud Cases: {}'.format(len(outliers)))\nnew_df = new_df.drop(new_df[(new_df['V12'] > v12_upper) | (new_df['V12'] < v12_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(new_df)))\nprint('----' * 44)\n\n\n# Removing outliers V10 Feature\nv10_fraud = new_df['V10'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v10_fraud, 25), np.percentile(v10_fraud, 75)\nv10_iqr = q75 - q25\n\nv10_cut_off = v10_iqr * 1.5\nv10_lower, v10_upper = q25 - v10_cut_off, q75 + v10_cut_off\nprint('V10 Lower: {}'.format(v10_lower))\nprint('V10 Upper: {}'.format(v10_upper))\noutliers = [x for x in v10_fraud if x < v10_lower or x > v10_upper]\nprint('V10 outliers: {}'.format(outliers))\nprint('Feature V10 Outliers for Fraud Cases: {}'.format(len(outliers)))\nnew_df = new_df.drop(new_df[(new_df['V10'] > v10_upper) | (new_df['V10'] < v10_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(new_df)))","b45c69c0":"f,(ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,6))\n\ncolors = ['#B3F9C5', '#f9c5b3']\n# Boxplots with outliers removed\n# Feature V14\nsns.boxplot(x=\"Class\", y=\"V14\", data=new_df,ax=ax1, palette=colors)\nax1.set_title(\"V14 Feature \\n Reduction of outliers\", fontsize=14)\nax1.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.5), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)\n\n# Feature 12\nsns.boxplot(x=\"Class\", y=\"V12\", data=new_df, ax=ax2, palette=colors)\nax2.set_title(\"V12 Feature \\n Reduction of outliers\", fontsize=14)\nax2.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.3), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)\n\n# Feature V10\nsns.boxplot(x=\"Class\", y=\"V10\", data=new_df, ax=ax3, palette=colors)\nax3.set_title(\"V10 Feature \\n Reduction of outliers\", fontsize=14)\nax3.annotate('Fewer extreme \\n outliers', xy=(0.95, -16.5), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)\n\n\nplt.show()","22d6f3e3":"from sklearn.svm import SVC\n\n# Undersampling before cross validating (prone to overfit)\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']\n\n\n# Our data is already scaled we should split our training and test sets\nfrom sklearn.model_selection import train_test_split\n\n# This is explicitly used for undersampling.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# Turn the values into an array for feeding the classification algorithms.\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values\n\n\n# Wow our scores are getting even high scores even when applying cross validation.\nfrom sklearn.model_selection import cross_val_score\n\nSVC().fit(X_train, y_train)\ntraining_score = cross_val_score(SVC(), X_train, y_train, cv=5)\nprint(\"Classifiers: SVC Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")\n","1a8dea86":"# Use GridSearchCV to find the best parameters.\nfrom sklearn.model_selection import GridSearchCV\n\n# Support Vector Classifier\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train, y_train)\n\n# SVC best estimator\nsvc = grid_svc.best_estimator_\n","ee0c618f":"svc_score = cross_val_score(svc, X_train, y_train, cv=5)\nprint('Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n","7e269c63":"# Let's Plot LogisticRegression Learning Curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator1, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, (ax1) = plt.subplots(1,1, figsize=(5,4), sharey=True)\n    if ylim is not None:\n        plt.ylim(*ylim)\n\n    # Third Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"Support Vector Classifier \\n Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n\n    return plt","a3f0a77a":"cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(svc, X_train, y_train, (0.87, 1.01), cv=cv, n_jobs=1)","e20a3b53":"from sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\n# Create a DataFrame with all the scores and the classifiers names.\n\nsvc_pred = cross_val_predict(svc, X_train, y_train, cv=5,\n                             method=\"decision_function\")\n","f5962edf":"from sklearn.metrics import roc_auc_score\n\nprint('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))","42d5f6ec":"<h2> Feature \ud655\uc778 <\/h2>\n\nPCA\ub85c \ubcc0\ud658\ub418\uc9c0 \uc54a\uc740 <b>'Time'<\/b>\uacfc <b>'Amount'<\/b>\uc758 Feature\ub97c \ud655\uc778\ud569\ub2c8\ub2e4.","84fd1bea":"<h2> Classifiers (UnderSampling):  <\/h2>\n<a id=\"classifiers\"><\/a>\n\n\ub370\uc774\ud130\ub97c \uad50\uc721 \ubc0f \ud14c\uc2a4\ud2b8 \uc138\ud2b8\ub85c \ubd84\ud560\ud558\uace0 Label\uacfc Feature\ub97c \ubd84\ub9ac\ud574\uc57c\ud569\ub2c8\ub2e4. <br>\n\ubcf8 \uc608\uc81c\uc5d0\uc11c\ub294 SVC \ubd84\ub958\uae30\uc5d0 \ub300\ud574\uc11c\ub9cc \ub2e4\ub8e8\uc5c8\uc2b5\ub2c8\ub2e4. <br>","d621a8d8":"<h2> Imbalenced data set <\/h2>\n\n\uc815\uc0c1\uac70\ub798 data\ub294 99.83%, \ubd80\uc815\uc0ac\uc6a9 data\ub294 0.17%\ub85c \uc2ec\ud55c data \ubd88\uade0\ud615\uc744 \uac16\uace0 \uc788\uc2b5\ub2c8\ub2e4. <br>\ndata\uac00 \ud55c \ucabd\uc73c\ub85c \uc2ec\uac01\ud558\uac8c \uce58\uc6b0\uce5c \uc0c1\ud0dc\uc5d0\uc11c \ubd84\ub958 \ubaa8\ub378\uc744 \ud559\uc2b5\ud558\uba74 \uce58\uc6b0\uce5c \uacb0\uacfc\ub97c \ub0b4\ub9b4 \uc218 \ubc16\uc5d0 \uc5c6\uc2b5\ub2c8\ub2e4. <br>\n\uc774\ub7ec\ud55c \uacbd\uc6b0 data\uc758 \uade0\ud615\uc744 \ub9de\ucd94\uae30 \uc704\ud574 under sampling, over sampling \ub4f1\uc758 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. <br>\n<ul>\n<li>Under sampling: data\uc758 \uc77c\ubd80\ub9cc sampling\ud574\uc11c \uc0ac\uc6a9\ud558\ub294 \uac83\uc73c\ub85c \uc911\uc694 \ud2b9\uc9d5\uc744 \ub2f4\uc9c0 \ubabb\ud560 \uc218 \uc788\ub294 \ub2e8\uc810\uc774 \uc788\uc74c <\/li>\n<li>Over sampling: data\ub97c \ucc0c\uadf8\ub7ec\ud2b8\ub9ac\uac70\ub098 \ub3cc\ub9ac\uba74\uc11c \uc0dd\uc131\ud558\uae30 \ub54c\ubb38\uc5d0 Overfitting\uc774 \uc77c\uc5b4\ub0a0 \uc218 \uc788\ub2e4\ub294 \ub2e8\uc810\uc774 \uc788\uc74c <\/li><\/ul>\n\n\ubcf8 \uc608\uc81c\uc5d0\uc11c\ub294 Under sampling\uc744 \uc774\uc6a9\ud55c \uc608\uc81c\ub97c \uc218\ud589\ud569\ub2c8\ub2e4. ","0776499f":"\uc591\uc758 \uc0c1\uad00\uad00\uacc4\uac00 \ub192\uc740 Column\uc758 data \ubd84\ud3ec","a16c28d4":"<h2> Scaling <\/h2>\n<a id=\"distributing\"><\/a>\n\uc774 \ub2e8\uacc4\uc5d0\uc11c\ub294 <b>Time<\/b>\uacfc <b>Amount<\/b>\ub85c \uad6c\uc131\ub41c \uc5f4\uc744 scaling\ud569\ub2c8\ub2e4. <br>\nsklearn.preprocessing\uc5d0\uc11c \uc81c\uacf5\ud558\ub294 RobustScaler\ub97c \uc0ac\uc6a9\ud558\uc600\uc9c0\ub9cc, StandardScaler\uc640 \ube44\uad50\ud574 \ubcf4\uc2dc\ub294 \uac83\ub3c4 \uc88b\uc2b5\ub2c8\ub2e4.","d84cc736":"<h2>Classifier\uc758 \uc815\ud655\ub3c4<\/h2>\n\nPrecision, Recall, F1 Score, ROC, etc <br>\nhttps:\/\/blog.naver.com\/trimurti\/221387953564","1bca7be6":"\uc74c\uc758 \uc0c1\uad00\uad00\uacc4\uac00 \ub192\uc740 Column\uc758 data \ubd84\ud3ec","2ba6133b":"<h2> Under sampling <\/h2>\n<a id=\"distributing\"><\/a>\n\uc815\uc0c1\/\ube44\uc815\uc0c1 \uac70\ub798\uc758 \uc815\ubcf4\ub7c9\uc744 \ub9de\ucd94\uae30 \uc704\ud574 \ub2e4\uc74c\uacfc \uac19\uc774 sampling\uc744 \uc218\ud589\ud569\ub2c8\ub2e4.   \n\nReference: KFold, StratifiedKFold <br>\nhttps:\/\/sgmath.tistory.com\/61","054dfda5":"<h1> Credit Fraud Detector <\/h1>\n\n<h2> Introduction <\/h2>\n\ubcf8 \ubb38\uc81c\uc758 data set\ub294 2\uc77c\ub3d9\uc548 \ubc1c\uc0dd\ud55c \uac70\ub798\ub97c \ub098\ud0c0\ub0b4\uba70 284,807\uac74 \uc911 492 \uac74\uc758 \ubd80\uc815\uc0ac\uc6a9\uc744 \ud3ec\ud568\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. <br>\n\uce74\ub4dc \uc0ac\uc6a9 \uc815\ubcf4\ub294 \uae30\ubc00 \uc720\uc9c0 \ubb38\uc81c\ub85c \ub370\uc774\ud130\uc758 \uae30\ub2a5\uacfc \ubc30\uacbd \uc815\ubcf4\ub97c \uc81c\uacf5\ud560 \uc218 \uc5c6\uae30 \ub54c\ubb38\uc5d0 PCA(Principal Component Analysis) \ubcc0\ud658 \uacb0\uacfc\ub85c \uc81c\uacf5\ud569\ub2c8\ub2e4. <br>\nColumn V1, V2,\u2026 V28\uc740 PCA\ub85c \uc5bb\uc740 \uc8fc\uc694 \uad6c\uc131 \uc694\uc18c\uc774\uba70, PCA\ub85c \ubcc0\ud658\ub418\uc9c0 \uc54a\uc740 \uc720\uc77c\ud55c feature\ub294 <b>'Time'<\/b>\uacfc <b>'Amount'<\/b>\uc785\ub2c8\ub2e4. <br>\n<b>'Class'<\/b>\ub294 Fraud\uc758 \uacbd\uc6b0 1, Normal\uc758 \uacbd\uc6b0 0\uc758 \uac12\uc744 \uac16\uc2b5\ub2c8\ub2e4.<br>\n\n\uc774 \uc5f0\uc2b5 \uc608\uc81c\ub294 \uae30\ubcf8\uc801\uc778 \ubd84\ub958 \ubb38\uc81c\ub97c \uacbd\ud5d8\ud574 \ubcfc \uc218 \uc788\ub3c4\ub85d \uac00\uc7a5 \ub9ce\uc740 Vote\ub97c \ubc1b\uc740 Janio Martinez\uac00 \uc791\uc131\ud55c <b>Credit Fraud || Dealing with Imbalanced Datasets<\/b>\uc758 \uc77c\ubd80\ub97c \ubc1c\ucdcc\ud558\uc5ec \uad6c\uc131\ud558\uc600\uc2b5\ub2c8\ub2e4.","61474420":"<h2> Data set \ud655\uc778 <\/h2>"}}