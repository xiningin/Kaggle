{"cell_type":{"a2a5b296":"code","a7f51748":"code","ea454418":"code","d77fbd52":"code","bb14a982":"code","aa77d2fe":"code","b4429c65":"code","c22c712a":"code","0000ba99":"code","77b1dc09":"code","bc6c2f65":"code","d84ded92":"code","791c3943":"code","9b138d95":"code","9be77ee9":"code","22e4f3fd":"code","f2e8058d":"code","7e3e9c4a":"code","a1f0c55b":"code","31f4b74a":"code","7a02248d":"code","cc65fe3c":"code","b5a52407":"code","ed7dd1f3":"markdown","fc6f07cc":"markdown","709c2af0":"markdown","9e8bfd6a":"markdown","10b96d2b":"markdown","876f5e8c":"markdown"},"source":{"a2a5b296":"import matplotlib.pyplot as plt\nimport seaborn as sns \nimport numpy as np\nimport pandas as pd\nimport numpy as np\nimport random as rnd\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.gridspec as gridspec\nfrom sklearn.preprocessing import StandardScaler\nfrom numpy import genfromtxt\nfrom scipy.stats import multivariate_normal\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score , average_precision_score\nfrom sklearn.metrics import precision_score, precision_recall_curve\n%matplotlib inline\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs,init_notebook_mode,plot,iplot\ninit_notebook_mode(connected=True)\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve","a7f51748":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs,init_notebook_mode,plot,iplot\ninit_notebook_mode(connected=True)","ea454418":"train_df = pd.read_csv(\"..\/input\/\/creditcard.csv\")\nprint(train_df.columns.values)","d77fbd52":"train_df.head(5)","bb14a982":"\n\ndef estimateGaussian(dataset):\n    mu = np.mean(dataset, axis=0)\n    sigma = np.cov(dataset.T)\n    return mu, sigma\n\ndef multivariateGaussian(dataset,mu,sigma):\n    p = multivariate_normal(mean=mu, cov=sigma)\n    return p.pdf(dataset)\n\n","aa77d2fe":"\ndef selectThresholdByCV(probs,gt):\n    best_epsilon = 0\n    best_f1 = 0\n    f = 0\n    farray = []\n    Recallarray = []\n    Precisionarray = []\n    epsilons = (0.0000e+00, 1.0527717316e-70, 1.0527717316e-50, 1.0527717316e-24)\n    #epsilons = np.asarray(epsilons)\n    #step = (probs.max() - probs.min()) \/ 1000\n    #for epsilon in np.arange(probs.min(), probs.max(), step):\n    for epsilon in epsilons:\n        predictions = (p_cv < epsilon)\n        f = f1_score(train_cv_y, predictions, average = \"binary\")\n        Recall = recall_score(train_cv_y, predictions, average = \"binary\")\n        Precision = precision_score(train_cv_y, predictions, average = \"binary\")\n        farray.append(f)\n        Recallarray.append(Recall)\n        Precisionarray.append(Precision)\n        print ('For below Epsilon')\n        print(epsilon)\n        print ('F1 score , Recall and Precision are as below')\n        print ('Best F1 Score %f' %f)\n        print ('Best Recall Score %f' %Recall)\n        print ('Best Precision Score %f' %Precision)\n        print ('-'*40)\n        if f > best_f1:\n            best_f1 = f\n            best_recall = Recall\n            best_precision = Precision\n            best_epsilon = epsilon    \n    fig = plt.figure()\n    ax = fig.add_axes([0.1, 0.5, 0.7, 0.3])\n    #plt.subplot(3,1,1)\n    plt.plot(farray ,\"ro\")\n    plt.plot(farray)\n    ax.set_xticks(range(5))\n    ax.set_xticklabels(epsilons,rotation = 60 ,fontsize = 'medium' )\n    ax.set_ylim((0,1.0))\n    ax.set_title('F1 score vs Epsilon value')\n    ax.annotate('Best F1 Score', xy=(best_epsilon,best_f1), xytext=(best_epsilon,best_f1))\n    plt.xlabel(\"Epsilon value\") \n    plt.ylabel(\"F1 Score\") \n    plt.show()\n    fig = plt.figure()\n    ax = fig.add_axes([0.1, 0.5, 0.9, 0.3])\n    #plt.subplot(3,1,2)\n    plt.plot(Recallarray ,\"ro\")\n    plt.plot(Recallarray)\n    ax.set_xticks(range(5))\n    ax.set_xticklabels(epsilons,rotation = 60 ,fontsize = 'medium' )\n    ax.set_ylim((0,1.0))\n    ax.set_title('Recall vs Epsilon value')\n    ax.annotate('Best Recall Score', xy=(best_epsilon,best_recall), xytext=(best_epsilon,best_recall))\n    plt.xlabel(\"Epsilon value\") \n    plt.ylabel(\"Recall Score\") \n    plt.show()\n    fig = plt.figure()\n    ax = fig.add_axes([0.1, 0.5, 0.9, 0.3])\n    #plt.subplot(3,1,3)\n    plt.plot(Precisionarray ,\"ro\")\n    plt.plot(Precisionarray)\n    ax.set_xticks(range(5))\n    ax.set_xticklabels(epsilons,rotation = 60 ,fontsize = 'medium' )\n    ax.set_ylim((0,1.0))\n    ax.set_title('Precision vs Epsilon value')\n    ax.annotate('Best Precision Score', xy=(best_epsilon,best_precision), xytext=(best_epsilon,best_precision))\n    plt.xlabel(\"Epsilon value\") \n    plt.ylabel(\"Precision Score\") \n    plt.show()\n    return best_f1, best_epsilon\n    ","b4429c65":"v_features = train_df.iloc[:,1:29].columns","c22c712a":"\n\nplt.figure(figsize=(12,8*4))\ngs = gridspec.GridSpec(7, 4)\nfor i, cn in enumerate(train_df[v_features]):\n    ax = plt.subplot(gs[i])\n    sns.distplot(train_df[cn][train_df.Class == 1], bins=50)\n    sns.distplot(train_df[cn][train_df.Class == 0], bins=50)\n    ax.set_xlabel('')\n    ax.set_title('feature: ' + str(cn))\nplt.show()\n\n","0000ba99":"#rnd_clf = RandomForestClassifier(n_estimators = 100 , criterion = 'entropy',random_state = 0)\n#rnd_clf.fit(train_df.iloc[:,1:29],train_df.iloc[:,30]);","77b1dc09":"#for name, importance in zip(train_df.iloc[:,1:29].columns, rnd_clf.feature_importances_):\n#   if importance > 0.020 :\n#        print('\"' + name + '\"'+',')","bc6c2f65":"train_df.drop(['V19','V21','V1','V2','V6','V5','V28','V27','V26','V25','V24','V23','V22','V20','V15','V13','V8', 'Time', 'Amount'], axis =1, inplace = True)","d84ded92":"train_df.head(5)","791c3943":"train_strip_v1 = train_df[train_df[\"Class\"] == 1]\ntrain_strip_v0 = train_df[train_df[\"Class\"] == 0]","9b138d95":"Normal_len = len (train_strip_v0)\nAnomolous_len = len (train_strip_v1)\n\nstart_mid = Anomolous_len \/\/ 2\nstart_midway = start_mid + 1\n\ntrain_cv_v1  = train_strip_v1 [: start_mid]\ntrain_test_v1 = train_strip_v1 [start_midway:Anomolous_len]\n\nstart_mid = (Normal_len * 60) \/\/ 100\nstart_midway = start_mid + 1\n\ncv_mid = (Normal_len * 80) \/\/ 100\ncv_midway = cv_mid + 1\n\ntrain_fraud = train_strip_v0 [:start_mid]\ntrain_cv    = train_strip_v0 [start_midway:cv_mid]\ntrain_test  = train_strip_v0 [cv_midway:Normal_len]\n\ntrain_cv = pd.concat([train_cv,train_cv_v1],axis=0)\ntrain_test = pd.concat([train_test,train_test_v1],axis=0)\n\n\nprint(train_fraud.columns.values)\nprint(train_cv.columns.values)\nprint(train_test.columns.values)\n\ntrain_cv_y = train_cv[\"Class\"]\ntrain_test_y = train_test[\"Class\"]\n\ntrain_cv.drop(labels = [\"Class\"], axis = 1, inplace = True)\ntrain_fraud.drop(labels = [\"Class\"], axis = 1, inplace = True)\ntrain_test.drop(labels = [\"Class\"], axis = 1, inplace = True)\n","9be77ee9":"mu, sigma = estimateGaussian(train_fraud)\np = multivariateGaussian(train_fraud,mu,sigma)\np_cv = multivariateGaussian(train_cv,mu,sigma)\np_test = multivariateGaussian(train_test,mu,sigma)","22e4f3fd":"print (p)\nprint(p_cv)\nprint(p_test)","f2e8058d":"fscore, ep= selectThresholdByCV(p_cv,train_cv_y)","7e3e9c4a":"print(fscore)\nprint(ep)","a1f0c55b":"predictions = (p_test < ep)\nRecall = recall_score(train_test_y, predictions, average = \"binary\")    \nPrecision = precision_score(train_test_y, predictions, average = \"binary\")\nF1score = f1_score(train_test_y, predictions, average = \"binary\")    \nprint ('F1 score , Recall and Precision for Test dataset')\nprint ('Best F1 Score %f' %F1score)\nprint ('Best Recall Score %f' %Recall)\nprint ('Best Precision Score %f' %Precision)\n","31f4b74a":"ig, ax = plt.subplots(figsize=(10, 10))\nax.scatter(train_test['V14'],train_test['V11'],marker=\"o\", color=\"lightBlue\")\nax.set_title('Anomalies(in red) vs Predicted Anomalies(in Green)')\nfor i, txt in enumerate(train_test['V14'].index):\n        if train_test_y.loc[txt] == 1 :\n            ax.annotate('*', (train_test['V14'].loc[txt],train_test['V11'].loc[txt]),fontsize=13,color='Red')\n        if predictions[i] == True :\n            ax.annotate('o', (train_test['V14'].loc[txt],train_test['V11'].loc[txt]),fontsize=15,color='Green')","7a02248d":"predictions = (p_cv < ep)\nRecall = recall_score(train_cv_y, predictions, average = \"binary\")    \nPrecision = precision_score(train_cv_y, predictions, average = \"binary\")\nF1score = f1_score(train_cv_y, predictions, average = \"binary\")    \nprint ('F1 score , Recall and Precision for Cross Validation dataset')\nprint ('Best F1 Score %f' %F1score)\nprint ('Best Recall Score %f' %Recall)\nprint ('Best Precision Score %f' %Precision)\n","cc65fe3c":"import itertools\nfrom sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,roc_curve,recall_score,classification_report \n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        #print(\"Normalized confusion matrix\")\n    else:\n        1#print('Confusion matrix, without normalization')\n\n    #print(cm)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","b5a52407":"cnf_matrix = confusion_matrix(train_cv_y,predictions)\n\nprint ('confusion matrix of test dataset = \\n',cnf_matrix)\n\nprint(classification_report(train_cv_y, predictions))\n\n# Plot non-normalized confusion matrix\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cnf_matrix\n                      , classes=class_names\n                      , title='Confusion matrix')\nplt.show()","ed7dd1f3":"**\nLets Visualize our predictions in below scatter plot\n**","fc6f07cc":"\n**Feature Importance**\n\nLets use Feqture importqnce to get rid of unwanted features whose existance will not improve our prediction model.\nI have used random forest classifier to identify the influential fetures. You can validate the below result with the feature analysis I conducted above","709c2af0":"1) Find out mu and Sigma for the dataframe variables passed to this function.\n2) Calculate Probability Distribution for the each row (I will explain why we need Probality for each row as we proceed)\n\nFormula:- if each example x has N dimensiona(features) then below formula is used to calculate the P value\nP(x) = p(x1,u1,sigma1^2)p(x2,u2,sigma2^2)p(x3,u3,sigma3^2).....p(xn,un,sigma'N'^2)","9e8bfd6a":"**Choosing Epsilon Values**\n\nI calculated P value for all the rows present in Normal Transaction and found the minimum P value by using below command\n\n**min(p)**\n\nsimilalrly we found the minimum P Value for rest of the datasets and found this value to be very close to 0 and then we found the max(p) value which is again somewhat far from 0. ","10b96d2b":"Epsilon value is the threshold value below which we will mark transaction as Anomalous.\n\nRewriting above sentense again P(x) for X if less than the epsilon value then mark that transaction as anomalous transaction.","876f5e8c":"\n\nIn the Anomalized technique we distribute this large dataset into 3 parts .\n\n1) Normal Transactons: classified as 0 , no anomalized transaction should be present here since it is not a supervised method\nHow to get this dataset :- 60% of normal transactions should be added here.\nFind out Epsilon by using min(Probability) command\n\n2) dataset for Cross validation : from the remaining normal transaction take 50 % (i.e. 20 % as a whole since we have already took the data in the first step) and add 50% of the Anomalized data with this .\n\n3) dataset for testing the algorithm :- this step is similar to what we did for Cross validattion.\nTest dataset = leftover normal transaction + leftover Anomalized data\n"}}