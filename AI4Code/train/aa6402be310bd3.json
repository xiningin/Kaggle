{"cell_type":{"ef7c0b2f":"code","acfdfa23":"code","e204f3d2":"code","6a229dee":"code","61dd14d8":"code","f408dc0d":"code","9369ab91":"code","adfd8b17":"code","98ef756b":"code","d0e02bd5":"code","f0459d9e":"code","19352483":"code","7fa48d7e":"code","8d4a1549":"code","9a41b326":"markdown","e0b2b52f":"markdown"},"source":{"ef7c0b2f":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport torch","acfdfa23":"x = torch.tensor([1.], requires_grad = True, dtype = torch.float32) \ny = torch.exp(-5. * x)\ny.backward()\nx.grad","e204f3d2":"x = np.random.rand(100, 1)\ny = 1 + 2.423 * x + 0.3 * np.random.randn(100, 1)","6a229dee":"plt.scatter(x , y)","61dd14d8":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = .2)","f408dc0d":"plt.scatter(X_train , y_train)","9369ab91":"plt.scatter(X_test , y_test)","adfd8b17":"\nX_train = torch.from_numpy(X_train)\nX_test = torch.from_numpy(X_test)\ny_train = torch.from_numpy(y_train)\ny_test = torch.from_numpy(y_test)\n\nX_train = X_train.float()\nX_test = X_test.float()\ny_train = y_train.float()\ny_test = y_test.float()","98ef756b":"#torch.manual_seed(10)\nclass LinearReg(torch.nn.Module):\n  \n  def __init__(self):\n    super().__init__()\n    self.neuron = torch.nn.Linear(1, 1)\n\n  \n  \n  def forward(self, x):\n    y = self.neuron(x)\n    return y\n\n\n\nmodel = LinearReg()\n\nn_epochs = 1000\nlosses = []\n\nmodel.train()\noptimizer = torch.optim.SGD(model.parameters(), lr = .03)\n\nloss = torch.nn.MSELoss()\n\nfor epoch in range(n_epochs):\n\n  yp = model(X_train)\n  \n  l = loss(yp, y_train)\n  l.backward(retain_graph = True)\n  \n  optimizer.step()\n  optimizer.zero_grad()\n  losses.append(l)\n\n\n  print(f'loss: {l.item()}')\n\n\n\n","d0e02bd5":"model.state_dict()","f0459d9e":"# y = 1 + 2.423","19352483":"plt.plot(losses)","7fa48d7e":"plt.plot(losses)\nplt.grid()","8d4a1549":"model.state_dict()","9a41b326":"![image.png](attachment:5b421e69-5cec-45ac-8281-4486b7545683.png)","e0b2b52f":"# Autograd"}}