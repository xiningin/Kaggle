{"cell_type":{"2720f166":"code","8d0afb00":"code","18aed295":"code","54537b73":"code","ccd6226f":"code","f2aa3885":"code","8cd1461c":"code","a8ed4c41":"code","3ddda49c":"code","1b734904":"code","b29d4eae":"code","25f1a0fe":"code","7fcff570":"code","c38e77bd":"code","2e3830d7":"code","b6058ecc":"code","f1191c67":"code","763dd725":"code","ceb0d9c2":"code","c2f6495b":"code","0c7bb10a":"code","3f1a7dc5":"code","243210b3":"code","e0df4cdd":"code","cc147574":"code","4d03a84a":"code","d92e2b75":"code","4a39d3ce":"code","4fd76ad5":"code","d37aa5ea":"code","0601ce87":"code","a6c9f1f5":"code","2163af4b":"code","7bf0964a":"code","fb51d801":"code","883989b4":"code","c56832f4":"code","40ebab79":"code","cff91ebd":"code","b36a0882":"code","05dba198":"code","c0fda507":"code","3e2b8d3c":"code","454477b5":"code","ee1d6c1c":"code","f42fba2d":"code","374f3c2c":"code","fea22fcc":"code","22a54051":"code","408fa6aa":"code","0aaa6b84":"code","f3d1c03a":"code","94037c01":"code","19ebc1c3":"code","710fa83d":"code","83f00133":"code","b47b36f5":"code","06a01026":"code","98b75469":"code","0165643e":"code","289717d1":"code","6c6e257a":"code","3153dbcf":"code","224ee9fb":"code","5877b4f4":"code","0ae2d412":"code","83d03b9a":"code","0c7efb28":"code","c846816a":"code","6ea893c8":"code","dadd76cb":"code","31ba52f7":"code","976f8dc8":"code","6eafcc09":"code","0401d83b":"markdown","48ed23d8":"markdown","1eeda3fa":"markdown","5f7612bc":"markdown","d7f48aba":"markdown","452d396b":"markdown","1cff22cb":"markdown","47aee2ca":"markdown"},"source":{"2720f166":"import numpy as np\nimport pandas as pd\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport warnings\n%matplotlib inline\nwarnings.simplefilter('ignore')\nfrom PIL import Image\n#\u753b\u50cf\u30b5\u30a4\u30ba\u3092\u5927\u304d\u304f\u3059\u308b\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 20,10\nimport xgboost as xgb\nimport re\nfrom sklearn.metrics import roc_auc_score","8d0afb00":"import sys\nimport pprint\npprint.pprint(sys.path)","18aed295":"df_test = pd.read_csv(\"..\/input\/titanic\/beginnersquest-with-titanicdata\/test.csv\")\ndf_train = pd.read_csv(\"..\/input\/titanic\/beginnersquest-with-titanicdata\/titanic_train.csv\")","54537b73":"df_train =df_train.rename(columns = {'Survived' : 'y'})\ndf_train['Honorifics'] = df_train['Name'].apply(lambda x:x.split(',')[1].split('.')[0].strip())","ccd6226f":"df_train[df_train[\"Embarked\"].isnull()]","f2aa3885":"df_train[(df_train['Pclass']==1)&(df_train['Embarked']==\"S\")].head()","8cd1461c":"df_train[(df_train['Pclass']==1)&(df_train['Embarked']==\"C\")].head()","a8ed4c41":"df_train[(df_train['Pclass']==1)&(df_train['Embarked']==\"Q\")].head()","3ddda49c":"df_train['Embarked']=df_train['Embarked'].fillna(\"Q\")","1b734904":"df_train.isna().sum()","b29d4eae":"df_test.isna().sum()","25f1a0fe":"df_test[df_test['Fare'].isnull()]","7fcff570":"train_fare01=df_train[(df_train['Pclass']==3)&(df_train['Embarked']==\"S\")]","c38e77bd":"train_fare01[\"Fare\"].mean()","2e3830d7":"df_test[\"Fare\"]=df_test['Fare'].fillna(14.6)","b6058ecc":"df_test.isna().sum()","f1191c67":"#Name\u306e\u656c\u79f0\ndf_test['Honorifics'] = df_test['Name'].apply(lambda x:x.split(',')[1].split('.')[0].strip())","763dd725":"#\u4e0d\u4f7f\u7528\nnot_use_col = [\"Cabin\",\"PassengerId\"]","ceb0d9c2":"print(df_train[\"Sex\"].unique())\nprint(df_train[\"Ticket\"].unique()[0:10])\nprint(df_train[\"Honorifics\"].unique()[0:10])\nprint(df_train[\"Embarked\"].unique()[0:10])\nprint(df_train[\"Pclass\"].unique()[0:10])","c2f6495b":"##Ticket\u306f\u6271\u3044\u305a\u3089\u305d\u3046\u306a\u306e\u3067\u3001\u7701\u304f\nnot_use_col += [\"Ticket\"]","0c7bb10a":"#Sex\u3068Embarked\u3068Pclass\u3092\u7f6e\u304d\u63db\u3048\u308b\ntrain_sex = pd.get_dummies(df_train['Sex'])\ntest_sex = pd.get_dummies(df_test['Sex'])\ntrain_embarked = pd.get_dummies(df_train['Embarked'])\ntest_embarked = pd.get_dummies(df_test['Embarked'])\ntrain_pclass = pd.get_dummies(df_train['Pclass'])\ntest_pclass = pd.get_dummies(df_test['Pclass'])","3f1a7dc5":"display(train_sex.head(1))\ndisplay(train_embarked.head(1))\ndisplay(train_pclass.head(1))","243210b3":"#\u7d50\u5408\ndf_train = pd.concat([df_train,train_sex,train_embarked,train_pclass],axis=1)\ndf_test = pd.concat([df_test,test_sex,test_embarked,test_pclass],axis=1)","e0df4cdd":"df_train.head(1)","cc147574":"#Sex\u3068Embarked\u3068Pclass\u3082\u5fc5\u8981\u306a\u304f\u306a\u3063\u305f\u306e\u3067\u8ffd\u52a0\u3057\u3066\u304a\u304f\nnot_use_col += [\"Sex\",\"Embarked\",\"Pclass\"]","4d03a84a":"#\u30c0\u30df\u30fc\u5316\ntrain_name = pd.get_dummies(df_train['Honorifics'])\ntest_name = pd.get_dummies(df_test['Honorifics'])","d92e2b75":"display(train_name.head(1))\ndisplay(test_name.head(1))","4a39d3ce":"#\u7d50\u5408\ndf_train = pd.concat([df_train,train_name],axis=1)\ndf_test = pd.concat([df_test,test_name],axis=1)","4fd76ad5":"#\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3068\u5b66\u7fd2\u30c7\u30fc\u30bf\u3067\u5171\u901a\u3067\u7528\u3044\u3089\u308c\u3066\u3044\u306a\u3044\u656c\u79f0\u3092\u7528\u3044\u308b\nnotsame_name = list(set(train_name.columns.tolist())^set(test_name.columns.tolist()))","d37aa5ea":"notsame_name","0601ce87":"#df_train\u306b\u306fDona\u304c\u3044\u306a\u3044\u306e\u30670\u306b\u3059\u308b\ndf_train[\"Dona\"]=0","a6c9f1f5":"from operator import itemgetter","2163af4b":"#notsame_name\u306fdf_test\u306b\u306f\u4f7f\u308f\u308c\u3066\u3044\u306a\u3044\u306e\u30670\u306b\u3059\u308b\nfor x in range(10):\n    df_test[itemgetter(x)(notsame_name)]=0","7bf0964a":"df_test.head(2)","fb51d801":"print(df_train.info())\nprint(df_test.info())","883989b4":"not_use_col","c56832f4":"#\u4e0d\u4f7f\u7528\nnot_use_col+=[\"Name\",\"Honorifics\"]","40ebab79":"#Age\u4ee5\u5916\u306etrain\u3068test\u30c7\u30fc\u30bf\u306e\u51e6\u7406\u5b8c\u4e86\ntrain = df_train.drop(not_use_col,axis=1)\ntest = df_test.drop(not_use_col,axis=1)","cff91ebd":"print(train.info())\nprint(test.info())","b36a0882":"#Age\u304c\u6b20\u640d\u3057\u3066\u3044\u306a\u3044\u884c\u3092\u53d6\u5f97\nage_data = train[~train[\"Age\"].isnull()].drop([\"y\"],axis=1)","05dba198":"#age_data\u3092train_x,test_x,train_y,test_y\u306b\u5206\u5272\ntrain_x,test_x,train_y,test_y = train_test_split(age_data.drop([\"Age\"],axis=1),age_data[\"Age\"],test_size=0.2,random_state=0)","c0fda507":"# xgboost\u30e2\u30c7\u30eb\u306e\u4f5c\u6210\ndtrain = xgb.DMatrix(train_x,label=train_y)\ndval = xgb.DMatrix(test_x,label=test_y)\n\n# \u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u63a2\u7d22\nparam = {\"objective\":\"reg:squarederror\"}\nnum_round = 500\n\nwatch_list = [(dtrain,\"train\"),(dval,\"val\")]\nmodel = xgb.train(param,dtrain,num_round,evals=watch_list,early_stopping_rounds=20)","3e2b8d3c":"#\u5b66\u7fd2\u306b\u4f7f\u3063\u305f\u5909\u6570\u3092\u53d6\u5f97\nage_col = train_x.columns.tolist()","454477b5":"#train\u3068test\u306e\u6b20\u640d\u3057\u3066\u3044\u308bAge\u3092\u4e88\u6e2c\npred_train_age = model.predict(xgb.DMatrix(train[train.Age.isnull()][age_col]),ntree_limit=model.best_ntree_limit)\npred_test_age = model.predict(xgb.DMatrix(test[test.Age.isnull()][age_col]),ntree_limit=model.best_ntree_limit)","ee1d6c1c":"#Age\u306e\u6b20\u640d\u3092\u57cb\u3081\u308b\ntrain.loc[train.Age.isnull(),\"Age\"] = pred_train_age\ntest.loc[test.Age.isnull(),\"Age\"] = pred_test_age","f42fba2d":"print(train.info())\nprint(test.info())","374f3c2c":"#train\u3068test\u306e\u30ab\u30e9\u30e0\u9806\u3092\u540c\u3058\u306b\u3059\u308b\ncol = list(train.columns.tolist())","fea22fcc":"test=test.reindex(columns=col).drop([\"y\"],axis=1)","22a54051":"print(train.info())\nprint(test.info())","408fa6aa":"#\u76ee\u7684\u5909\u6570\ntarget_col = [\"y\"]\n#\u8aac\u660e\u5909\u6570\nexplain_col = test.columns.tolist()\nnp.array(explain_col)","0aaa6b84":"# xgboost\u30e2\u30c7\u30eb\u306e\u4f5c\u6210\ntrain_x,val_x,train_y,val_y = train_test_split(train.drop([\"y\"],axis=1),train[\"y\"],test_size=0.2,random_state=0)\ndtrain = xgb.DMatrix(train_x,label=train_y)\ndval = xgb.DMatrix(val_x,label=val_y)\ndtest = xgb.DMatrix(test)\n\n# \u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u63a2\u7d22\nparam = {\"objective\":\"binary:logistic\",\"eval_metric\":\"auc\"}\nnum_round = 500\n\nwatch_list = [(dtrain,\"train\"),(dval,\"val\")]\nmodel = xgb.train(param,dtrain,num_round,evals=watch_list,early_stopping_rounds=20)","f3d1c03a":"pred_xgb = model.predict(dtest,ntree_limit=model.best_ntree_limit)","94037c01":"pred_xgb[0:5]","19ebc1c3":"train.head(1)","710fa83d":"#\u6a19\u6e96\u5316\u30fb\u6b63\u898f\u5316\u3059\u308b\nScaler1 = StandardScaler()\nScaler2 = StandardScaler()\n\ntrain_explain = pd.DataFrame(data = Scaler1.fit_transform(train[explain_col]),columns = explain_col)\n#Embarked\u3067\u524a\u9664\u3057\u305f\u4e8c\u500b\u306e\u305b\u3044\u3067index\u304c\u9023\u756a\u3067\u306f\u306a\u3044\n#train[target_col]\u306f\u9023\u756a\u306b\u76f4\u3057\u3066\u304a\u304f\ntrain = pd.concat([train_explain,train[target_col].reset_index(drop=True)],axis=1)\ntest = pd.DataFrame(data = Scaler2.fit_transform(test),columns = test.columns)","83f00133":"train.head(1)","b47b36f5":"train_x,val_x,train_y,val_y = train_test_split(train[explain_col],train[target_col],\n                                               test_size=0.2,random_state=0)","06a01026":"class Titanic(torch.utils.data.Dataset):\n    def __init__(self,dataframe,test=False):\n        self.df = dataframe\n        self.test = test\n    \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self,idx):\n        if self.test == True:#\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u306fSurvived\u306f\u306a\u3044\u306e\u30670\u3092\u8fd4\u3059\n            label = 0\n        else:\n            label = self.df.iloc[idx].y\n        features = self.df.iloc[idx][explain_col]\n        \n        #pytorch\u3067\u4f7f\u3046\u578b\u306b\u5909\u63db\u3059\u308b\n        label_tensor = torch.tensor(label,dtype=torch.long)#int\u578b\u306b\u5bfe\u5fdc\n        features_tensor = torch.from_numpy(features.values.astype(np.float32))#float\u578b\u306b\u5bfe\u5fdc\n            \n        return label_tensor,features_tensor","98b75469":"train_dataset = Titanic(pd.concat([train_x,train_y],axis=1))\nvalid_dataset = Titanic(pd.concat([val_x,val_y],axis=1))","0165643e":"#train\u306e\u4e00\u884c\u76ee\u306e\u30c7\u30fc\u30bf\u3092pytorch\u3067\u6271\u3048\u308b\u578b\u306b\u3057\u3066\u8fd4\u3059\ntrain_dataset[0]","289717d1":"class Net(nn.Module):\n    def __init__(self,num_feature):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(num_feature, 512)#\u7279\u5fb4\u91cf\u306e\u500b\u6570\n        self.fc2 = nn.Linear(512, 512)\n        self.fc3 = nn.Linear(512,1)#1\u500b\u306e\u7279\u5fb4\u91cf\u3092\u8fd4\u3059\n        self.dropout = nn.Dropout(p=0.3)#dropout\u5c64\u306e\u8ffd\u52a0\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = self.fc3(x)\n        return F.sigmoid(x)","6c6e257a":"# GPU\u304c\u6709\u52b9\u304b\u3069\u3046\u304b\u3092\u78ba\u8a8d\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","3153dbcf":"#len(train)%batch==1\u306b\u306a\u308b\u3068\u30a8\u30e9\u30fc\u304c\u51fa\u308b\u304b\u3089\u6ce8\u610f\nbatch_size = 32\n#valid\u306f\uff11\u500b\u305a\u3064\u691c\u8a3c\u3059\u308b\nvalid_batch_size = 1\nnum_epochs = 30\n\n#fold\u6bce\u306bmodel\u3092\u521d\u671f\u5316\nmodel = Net(len(explain_col))\n#model\u3092GPU\u306b\u6e21\u3059\nmodel = model.to(device)\n#\u640d\u5931\u95a2\u6570\ncriterion = nn.BCELoss()\n#\u6700\u9069\u5316\u624b\u6cd5\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\nmax_auc = 0\n\nepoch_loss = []\nepoch_auc = []\n\nfor epoch in range(num_epochs):#\u4f55\u56de\u5b66\u7fd2\u3059\u308b\u304b\uff1f\n    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)\n    #valid\u306f\u30b7\u30e3\u30c3\u30d5\u30eb\u3057\u306a\u3044\n    valid_dataloader = torch.utils.data.DataLoader(valid_dataset, valid_batch_size, shuffle=False)\n    \n    #\u8a08\u7b97\u3057\u305floss\u3092\u4fdd\u7ba1\u3059\u308b\n    losses = []\n    ####\u3053\u3053\u306b\u5b66\u7fd2\u30b3\u30fc\u30c9####\n    for label,data in train_dataloader:\n        #data\u3092GPU\u306b\u6e21\u3059\n        data = data.to(device)\n        #label\u3092GPU\u306b\u6e21\u3059\n        label = label.to(device)\n        #\u4e88\u6e2c\u7d50\u679c\u3092\u53d6\u5f97\n        out = model(data)\n        #loss\u3092\u8a08\u7b97 batch_size\u3092\u307e\u3068\u3081\u3066\u8a08\u7b97\u3059\u308b\n        loss = criterion(out.squeeze(),label.float())\n        #loss\u3092\u683c\u7d0d\n        losses.append(loss)\n        #optimizer\u306e\u521d\u671f\u5316\n        optimizer.zero_grad()\n        #\u8aa4\u5dee\u9006\u4f1d\u64ad\n        loss.backward()\n        #\u6700\u9069\u5316\n        optimizer.step()\n    #1epoch\u306eloss\u306e\u5e73\u5747\n    loss_mean = sum(losses)\/len(train_dataloader)\n    \n    ##\u8a55\u4fa1\u30b3\u30fc\u30c9\u3092\u66f8\u304f###\n    #vaild\u306f\u91cd\u307f\u3092\u5f04\u3089\u306a\u3044\n    with torch.no_grad():\n        #\u4e88\u6e2c\u78ba\u7387\u3092\u683c\u7d0d\u3059\u308b\n        pred_list = []\n        #\u6b63\u89e3\u30c7\u30fc\u30bf\u3092\u683c\u7d0d\u3059\u308b\n        label_list = []\n        for label,data in valid_dataloader:\n            #data\u3092GPU\u306b\u6e21\u3059\n            data = data.to(device)\n            #\u4e88\u6e2c\u7d50\u679c\u3092\u53d6\u5f97\n            out = model(data)\n            #\u6700\u5927\u5024\u3092\u53d6\u5f97\n            pred = out.item()\n            pred_list.append(pred)\n            label_list.append(label)\n        #auc\u3092\u8a08\u7b97\u3059\u308b\n        pred_list = np.array(pred_list)\n        label_list = np.array(label_list)\n        auc = roc_auc_score(label_list,pred_list)\n        \n    epoch_loss.append(loss_mean.item())\n    epoch_auc.append(auc)\n    \n    if auc > max_auc:\n        max_auc = auc\n        #\u6700\u9ad8\u7cbe\u5ea6\u306e\u30e2\u30c7\u30eb\u3092\u4fdd\u7ba1\n        best_model = model\n        print(\"epoch{}\/{}:loss:{}:auc:{}\".format(epoch,num_epochs,loss_mean,auc))","224ee9fb":"#\u53ef\u8996\u5316\u3059\u308b\ndf_plot = pd.DataFrame()\ndf_plot[\"loss\"] = epoch_loss\ndf_plot[\"auc\"] = epoch_auc\ndf_plot.plot()","5877b4f4":"test_dataset = Titanic(test,test=True)\n#shuffle\u306fFalse\ntestloader = torch.utils.data.DataLoader(test_dataset, 1, shuffle=False)\n#\u7d50\u679c\u306e\u683c\u7d0d\npred_list = []\n#\u4e88\u6e2c\nfor label,data in testloader:\n    with torch.no_grad():\n        data = data.to(device)\n        out = best_model(data)\n        pred = out.item()\n    pred_list.append(pred)","0ae2d412":"np.array(pred_list)[0:5]","83d03b9a":"pred_xgb[0:5]","0c7efb28":"import random\nfrom operator import itemgetter\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom six import StringIO\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.tree import plot_tree\n\nfrom IPython.display import Image\nimport seaborn as sns\n\nfrom statistics import stdev","c846816a":"train_x = train.drop(columns=[\"y\"])\ntrain_y = train[[\"y\"]]\n\ncol_name = list(train_x.columns.values)","6ea893c8":"tree_model = DecisionTreeClassifier(\n    criterion=\"entropy\",            # Entropy\u57fa\u6e96\u306e\u5834\u5408\u306f\"entropy\u201d\n    splitter=\"best\",             # \u5206\u5272\u3092\u30e9\u30f3\u30c0\u30e0\u3067\u884c\u3046\u5834\u5408\u306f\"random\"\n    random_state=17,             # \u540c\u3058\u5206\u5272\u30b9\u30b3\u30a2\u306e\u6642\u306b\u30e9\u30f3\u30c0\u30e0\u306b\u9078\u3076seed\u3092\u56fa\u5b9a\n    max_depth=6,                 # \u6c7a\u5b9a\u6728\u306e\u6df1\u3055\u306e\u6700\u5927\u5024\n    min_samples_split=10,       # \u5206\u5272\u3059\u308b\u6700\u5c0f\u30c7\u30fc\u30bf\u6570\n    min_samples_leaf=10         # \u672b\u7aef\u30ce\u30fc\u30c9\u306b\u8a72\u5f53\u3059\u308b\u6700\u5c0f\u30b5\u30f3\u30d7\u30eb\u6570\n)\ntree_model = tree_model.fit(train_x, train_y)","dadd76cb":"pred_test=tree_model.predict_proba(test)[:,1]","31ba52f7":"np.array(pred_test)[0:5]","976f8dc8":"#XGBoost\u306e\u4e88\u6e2c\u3068NN\u306e\u4e88\u6e2c\u3068\u6c7a\u5b9a\u6728\u306e\u4e88\u6e2c\u3092\u8db3\u3057\u30663\u3067\u5272\u308b\npred = (pred_list+pred_xgb+pred_test)\/3\n\n#\u63d0\u51fa\nsubmission=pd.DataFrame(pred)\nsubmission=submission.reset_index()\nsubmission['index']=submission['index']+1\nsubmission.columns=[\"Id\",\"Predicted\"]\nsubmission.head()","6eafcc09":"submission.to_csv(\n    path_or_buf=\"submissin.csv\",\n    sep=\",\",\n    index=False,\n    header=True\n)","0401d83b":"### \u30c0\u30df\u30fc\u5316","48ed23d8":"\u6b21\u306bdf_tast\u306e\u6b20\u640d\u3092\u57cb\u3081\u308b(Fare)","1eeda3fa":"Embarked\u306e\u6b20\u640d\uff12\u3064\u3092\u57cb\u3081\u308b","5f7612bc":"# \u6c7a\u5b9a\u6728\u3092\u7528\u3044\u305f\u30e2\u30c7\u30eb","d7f48aba":"# XGBoost\u3092\u7528\u3044\u305f\u30e2\u30c7\u30eb","452d396b":"# Age\u3092\u4e88\u6e2c\u3059\u308b","1cff22cb":"\u4f7f\u308f\u308c\u3066\u3044\u306a\u3044\u540d\u524d\u3092\uff10\u3067\u57cb\u3081\u308b","47aee2ca":"# NN\u3092\u7528\u3044\u305f\u30e2\u30c7\u30eb"}}