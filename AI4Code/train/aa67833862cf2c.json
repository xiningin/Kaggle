{"cell_type":{"9c12f403":"code","ea5752a0":"code","bf98c63d":"code","232e6f80":"code","7ac9afa2":"markdown","c25e3626":"markdown","b6494fae":"markdown","f69c4d64":"markdown","1409a9c2":"markdown","11410d76":"markdown","21ee5482":"markdown"},"source":{"9c12f403":"import pandas as pd\nimport numpy as np\nimport pandas as pd\nimport datatable as dt\nnp.random.seed(42)\nimport tensorflow as tf\ntf.random.set_seed(42)\n\n\n# Data ===============================================================================================================================\n# datatable loads the data much faster\ndatatable_df = dt.fread('\/kaggle\/input\/jane-street-market-prediction\/train.csv')\ntrain = datatable_df.to_pandas()\ndel datatable_df\n\nfeatures = [c for c in train.columns if 'feature' in c]\ntrain = train[train['weight']>0]\ntrain['action'] = (train['resp']>0)*1\n\nfor col in features:\n    mean = np.mean(train[col])\n    train[col] = train[col].fillna(mean)\n\n# Leaving 50 days between training and test to avoid data leakage.\n# Idea taken from this notebook https:\/\/www.kaggle.com\/gogo827jz\/jane-street-xgboost-grouptimesplitkfold?scriptVersionId=48297430\n    \nX_train = train[train['date']<=350][features].values.astype('float32')\nX_test = train[train['date']>400][features].values.astype('float32')\n\ny_train = train[train['date']<=350]['action'].values.astype('int')\ny_test = train[train['date']>400]['action'].values.astype('int')","ea5752a0":"import tensorflow as tf\nfrom kerastuner.tuners import RandomSearch, Hyperband\nfrom tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n\n# Build Model ============================================================================================================================\ndef build_model(hp):\n    model = tf.keras.Sequential()\n    for i in range(hp.Int('num_layers', 2, 5)):\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n                                            min_value=32,\n                                            max_value=2048,\n                                            step=32),\n                               activation='relu'\n                                       ))\n        model.add(tf.keras.layers.Dropout(rate=hp.Float('dropout_' + str(i),\n                                                 min_value=0.0,\n                                                 max_value=0.5,\n                                    )))\n    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(\n                                            hp.Float(\n                                                    'learning_rate',\n                                                    min_value=1e-4,\n                                                    max_value=1e-2,\n                                                    sampling='LOG',\n                                                    default=1e-3\n                                                )\n                                        ),\n            loss=tf.keras.losses.BinaryCrossentropy(label_smoothing = 1e-2),\n        metrics='acc')\n    return model\n\n# You can either use hyperband or Randomsearch or even Bayesian optimization\ntuner = RandomSearch(\n    build_model,\n    seed=1,\n    objective='val_acc',\n    max_trials=1,  #change this\n    executions_per_trial=1,   #change this\n    directory='JS_NN_randomsearch',\n    project_name='JS_keras_tuner')\n\n# tuner = Hyperband(\n#     build_model,\n#     seed=1,\n#     objective='val_acc',\n#     max_epochs = 40,\n#     executions_per_trial=2,\n#     directory='JS_NN_hyperband',\n#     project_name='JS_keras_tuner'\n# )\n\n# Add an early stop so the tuner isn't spending time on a suboptimal model\nes = EarlyStopping(monitor = 'val_acc', min_delta = 1e-4, patience = 5)\n\n#This is the tuner object\ntuner.search(X_train, y_train,\n             epochs=10, # small number for quick run\n             callbacks = [es], #,rlr,ckp],\n             batch_size = 4096,\n             validation_data=(X_test, y_test))","bf98c63d":"# Tuner summary\ntuner.results_summary()","232e6f80":"best_model = tuner.get_best_models(num_models=1)[0]\nbest_model.predict(X_test[0:1,])\nbest_model.summary()","7ac9afa2":"The below code is where we build an architecture and pass choices for hyperparameters. It takes a while to run this block depending on the space of possible parameter combinations. You can add more parameters or more choices. keras Tuner is slightly limited when compared to Hyperopt, but I found it to be more intuitive and easier to understand","c25e3626":"# Keras Tuner for hyperparameter optimization","b6494fae":"The tuner summary output is ugly and I haven't yet found a way to display it better. But you can extract the best model. The best model is empty when you extract so it needs to either train again or at least pass it through prediction and then you can print it just like a regular NN architecture. ","f69c4d64":"What we get is the most optimal architecture and not necessarily best model (weights), so for that we need to train it again with k-fold. There is an amazing notebook (which inspired this one) https:\/\/www.kaggle.com\/gogo827jz\/jane-street-neural-network-starter for NN training and submitting.","1409a9c2":"The tuner object stores all the tested models. You can display a summary or pull up top score models.","11410d76":"There are a number of automated tuners available for tuning a model, and by tuning I mean even the architecture - from number of layers, drop outs to learning rates. We can of course build innumerable for loops, but a more elegant option is always welcome.\n\nKeras tuner is a keras wrapper around hyperopt. Here is a quick code for implementing the tuner. Hope you find this useful.","21ee5482":"Please upvote if you find it useful.\nGood luck!"}}