{"cell_type":{"e9155a3c":"code","428bab54":"code","fbd160da":"code","81b24924":"code","4cad4cae":"code","c329c342":"code","54da4a77":"code","6815bf68":"code","08d929ae":"code","6f3f4567":"code","e0bf6f86":"code","cdcbea93":"code","0223fab8":"code","c7327167":"code","3346fe5b":"code","cb840c50":"code","6f43c248":"code","b7c3a1a5":"markdown","77c77143":"markdown","87c209fb":"markdown","5f2cf7d6":"markdown","9e0ff9eb":"markdown","240aed78":"markdown","35da2a6e":"markdown","80df2c60":"markdown","79e84100":"markdown","d84ffe84":"markdown","a0e8e514":"markdown","753a6ff8":"markdown","54045378":"markdown","7cdd1ea3":"markdown","7730a370":"markdown","830655bd":"markdown","e59cc173":"markdown","c46e746d":"markdown","c1df0708":"markdown"},"source":{"e9155a3c":"import numpy as np\nimport pandas as pd","428bab54":"dataset = pd.read_csv('..\/input\/titanic\/train.csv')\nX = dataset.iloc[:,[2,4,5,9,11]].values\ny = dataset.iloc[:, 1].values","fbd160da":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(X[:,2:3])\nX[:,2:3] = imputer.transform(X[:, 2:3])\n\nimputer_2 = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nimputer_2.fit(X[:, [4]])\nX[:, [4]] = imputer_2.transform(X[:, [4]])","81b24924":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nX[:,1] = le.fit_transform(X[:,1])\n\nX[:, 4] = le.fit_transform(X[:,4])","4cad4cae":"# Fearure Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX=sc.fit_transform(X)","c329c342":"X","54da4a77":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.03, random_state = 0 )","6815bf68":"from xgboost import XGBClassifier\nclassifier = XGBClassifier()\nclassifier.fit(X_train, y_train)","08d929ae":"y_pred = classifier.predict(X_test)","6f3f4567":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_train, classifier.predict(X_train))\nprint(cm)","e0bf6f86":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","cdcbea93":"dataset1 = pd.read_csv('..\/input\/titanic\/test.csv')\nX_test = dataset1.iloc[:,[1,3,4,8,10]].values\n\n# No y ","0223fab8":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer.fit(X_test[:,2:3])\nX_test[:,2:3] = imputer.transform(X_test[:, 2:3])\n\nimputer_2 = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nimputer_2.fit(X[:, [4]])\nX[:, [4]] = imputer_2.transform(X[:, [4]])\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nX_test[:,1] = le.fit_transform(X_test[:,1])\nX_test[:, 4] = le.fit_transform(X_test[:,4])\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_test=sc.fit_transform(X_test)","c7327167":"y_pred = classifier.predict(X_test)","3346fe5b":"final = dataset1.iloc[:,0:2].values\n\nfor i in range(len(y_pred)):\n  final[i,1] = y_pred[i]","cb840c50":"np.savetxt(\"titanic_prediction_Rahman.csv\", final, delimiter = ',')","6f43c248":"final","b7c3a1a5":"Then we fit our **classifier**","77c77143":"##### Then It's tiem to import the** data**.\n####### And we slice the data into independent and dependent variables.","87c209fb":"Now time to test our classifier","5f2cf7d6":"## Now the fun part","9e0ff9eb":"We will solve this problem following some simple steps using **xgboost**.","240aed78":"let's predict","35da2a6e":"now we encode texts","80df2c60":"### Let's Start","79e84100":"Not bad!","d84ffe84":"looks good. let's measure the accuracy.","a0e8e514":"now let's follow the same steps like before","753a6ff8":"let's see how our data table looks","54045378":"now feature scaling (xgBoost needs feature scaling)","7cdd1ea3":"Now we take care of missing data.","7730a370":"let's export the table to a csv file","830655bd":"now let's split our data into train and test sets.\n(I prefered to know the accuracy before submitting. but we will use only 3% for test)","e59cc173":"Let's see the result in confusion matrix","c46e746d":"At first we import **libraries** we need. (We need more which we will import **later**).","c1df0708":"we have our prediction ready. Now let's make a new table with the PassengerID and prediction"}}