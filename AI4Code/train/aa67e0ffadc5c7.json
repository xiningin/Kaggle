{"cell_type":{"23d05ea5":"code","0c9e2e2c":"code","eb40f2ac":"code","d7f3529f":"code","f3cb0e45":"code","9e2e5d66":"code","d0610ecf":"code","c5a0dbc6":"code","0dfba163":"code","47f93866":"code","614868cb":"code","7dc92b5f":"code","7b5379f4":"code","a3a72894":"code","255f06c5":"code","9358ff95":"code","9f39466a":"code","ecd20f61":"code","a3a02269":"code","689e21aa":"code","82dd012d":"code","196c301b":"code","53a7ac0b":"code","a70521f5":"code","749b2eb0":"code","5eded891":"code","a0209865":"code","e131cade":"code","e4972828":"code","0618550d":"code","8709a4c7":"code","4fec0bf9":"code","7c567313":"code","460b434c":"code","12359f49":"code","774428b6":"code","13c1ef35":"code","0d4cb52e":"code","b650d315":"code","51b105f4":"code","1f615c9d":"code","98838189":"code","e0a4571b":"code","04f0f24c":"code","2b08563e":"code","1e84e3d9":"code","38261eae":"code","49b0e846":"code","53f9984c":"code","bf7de49b":"code","d6534aaa":"markdown","ea5e3297":"markdown","adb2dd16":"markdown","bf0aeb2c":"markdown","0474bac2":"markdown","9df14a64":"markdown","e4bbff28":"markdown","f48df4b3":"markdown","6770d780":"markdown","a647f65e":"markdown","4cb3bcbe":"markdown","4b4caa52":"markdown","2cc3e732":"markdown","da759651":"markdown","fb8527bb":"markdown","7868a54d":"markdown"},"source":{"23d05ea5":"import torch\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport random\nimport torch.nn.functional as F","0c9e2e2c":"num_epochs = 50\nbatch_size = 4096\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","eb40f2ac":"transforms_ = transforms.ToTensor()\ntrain_data = datasets.MNIST(root='.\/data', train=True, transform=transforms_, download=True)\ntest_data = datasets.MNIST(root='.\/data', train=False, transform=transforms_, download=True)\n\ntrain_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)","d7f3529f":"class FullyConnectedNet(nn.Module):\n    def __init__(self):\n        super(FullyConnectedNet,self).__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(256, 2)\n        self.relu = nn.ReLU()\n        self.fc4 = nn.Linear(2, 10)\n        self.bn1 = nn.BatchNorm1d(256)\n        self.bn3 = nn.BatchNorm1d(2)\n\n    def forward(self,x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.bn1(out)\n        out = self.fc2(out)\n        out = self.relu(out)\n        out = self.bn3(out)\n        out = self.fc4(out)\n        return out","f3cb0e45":"classifier_fc = FullyConnectedNet().to(device)\nloss_function = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(classifier_fc.parameters(), lr=1e-1)\n\nfor epoch in range(num_epochs):\n    running_loss_train = 0.\n    running_corrects_train = 0\n    classifier_fc.train()\n    for i ,(images,labels) in enumerate(train_loader):    \n        images = images.view(-1,28*28).to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        outputs = classifier_fc(images)\n        loss = loss_function(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        _, preds = torch.max(outputs, 1)\n        running_loss_train += loss.item() * images.size(0)\n        running_corrects_train += torch.sum(preds == labels.data)\n    epoch_loss_train = running_loss_train \/ len(train_data)\n    epoch_acc_train = running_corrects_train.double() \/ len(train_data)\n\n    classifier_fc.eval()\n    running_loss_test = 0.\n    running_corrects_test = 0\n    for i ,(images,labels) in enumerate(test_loader):    \n        images = images.view(-1,28*28).to(device)\n        labels = labels.to(device)\n        outputs = classifier_fc(images)\n        loss = loss_function(outputs, labels)\n        _, preds = torch.max(outputs, 1)\n        running_loss_test += loss.item() * images.size(0)\n        running_corrects_test += torch.sum(preds == labels.data)\n    epoch_loss_test = running_loss_test \/ len(test_data)\n    epoch_acc_test = running_corrects_test.double() \/ len(test_data)         \n\n    print(f'Epoch {epoch+1}\/{num_epochs}\\t'\n          f'Train loss: {epoch_loss_train:.4f} | '\n          f'Train accuracy: {epoch_acc_train:.4f} | '\n          f'Test loss: {epoch_loss_test:.4f} | '\n          f'Test accuracy: {epoch_acc_test:.4f}')","9e2e5d66":"class FullyConnectedNet_features(nn.Module):\n    def __init__(self):\n        super(FullyConnectedNet_features,self).__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(256, 2)\n        self.fc4 = nn.Linear(2, 10)\n        self.bn1 = nn.BatchNorm1d(256)\n        self.bn3 = nn.BatchNorm1d(2)\n\n    def forward(self,x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.bn1(out)\n        out = self.fc2(out)\n        features = out\n        out = self.relu(out)\n        out = self.bn3(out)\n        out = self.fc4(out)\n        return out, features","d0610ecf":"clr = [\n    '#1f77b4',  # muted blue\n    '#ff7f0e',  # safety orange\n    '#2ca02c',  # cooked asparagus green\n    '#d62728',  # brick red\n    '#9467bd',  # muted purple\n    '#8c564b',  # chestnut brown\n    '#e377c2',  # raspberry yogurt pink\n    '#7f7f7f',  # middle gray\n    '#bcbd22',  # curry yellow-green\n    '#17becf'   # blue-teal\n]","c5a0dbc6":"classifier_with_features = FullyConnectedNet_features().to(device)\nclassifier_with_features.load_state_dict(classifier_fc.state_dict())\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=len(train_data))\ntest_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=len(test_data))\n\nclassifier_with_features.eval()\n\nwith torch.no_grad():\n    images, train_labels = next(iter(train_loader))\n    _, train_features = classifier_with_features(images.view(-1,28*28).to(device))\n    train_features = train_features.cpu()\n\n    images, test_labels = next(iter(test_loader))\n    _, test_features = classifier_with_features(images.view(-1,28*28).to(device))\n    test_features = test_features.cpu()\n\nfigure(num=None, figsize=(15, 6), dpi=80, facecolor='w', edgecolor='k')\n\nplt.subplot(121)\nfor i in range(10): plt.scatter(train_features[train_labels==i].T[0], train_features[train_labels==i].T[1], c=clr[i], s=.1)\n\nplt.subplot(122)\nfor i in range(10): plt.scatter(test_features[test_labels==i].T[0], test_features[test_labels==i].T[1], c=clr[i],s=1, label=f'{i}')\nplt.legend()","0dfba163":"class CNNClassifier(nn.Module):\n    def __init__(self):\n        super(CNNClassifier,self).__init__()\n        self.conv1 = nn.Conv2d(1, 8, 5)\n        self.maxpool = nn.MaxPool2d(2, stride=2)\n        self.conv2 = nn.Conv2d(8, 16, 5)\n        self.fc1 = nn.Linear(16*4*4, 32)\n        self.fc2 = nn.Linear(32, 2)\n        self.relu = nn.ReLU()\n        self.fc3 = nn.Linear(2, 10)\n        self.bn1 = nn.BatchNorm2d(8)\n        self.bn2 = nn.BatchNorm2d(16)\n        self.bn4 = nn.BatchNorm1d(2)\n        self.bn3 = nn.BatchNorm1d(32)\n\n    def forward(self,x):\n        out = self.conv1(x)\n        out = self.maxpool(out)\n        out = self.relu(out)\n        out = self.bn1(out)\n        out = self.conv2(out)\n        out = self.maxpool(out)\n        out = self.relu(out)\n        out = self.bn2(out)\n        out = out.view(out.size()[0], -1)\n        out = self.fc1(out)\n        out = self.relu(out)\n        out = self.bn3(out)\n        out = self.fc2(out)\n        features = out\n        out = self.relu(out)\n        out = self.bn4(out)        \n        out = self.fc3(out) \n        return out, features","47f93866":"data_transforms = transforms.Compose([\n    transforms.Resize(size=(28, 28)), \n    transforms.ToTensor(), \n    transforms.Normalize((0.1307,), (0.3081,))\n])\n\ntrain_data = datasets.MNIST(root='.\/data', train=True, transform=data_transforms, download=True)\ntest_data = datasets.MNIST(root='.\/data', train=False, transform=data_transforms, download=True)\n\ntrain_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)","614868cb":"cnn_classifier = CNNClassifier().to(device)\nloss_function = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(cnn_classifier.parameters(), lr=0.1)","7dc92b5f":"for epoch in range(num_epochs):\n    running_loss_train = 0.\n    running_corrects_train = 0\n    cnn_classifier.train()\n    for i ,(images,labels) in enumerate(train_loader):    \n        images = images.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        outputs, features = cnn_classifier(images)\n        loss = loss_function(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        _, preds = torch.max(outputs, 1)\n        running_loss_train += loss.item() * images.size(0)\n        running_corrects_train += torch.sum(preds == labels.data)\n    epoch_loss_train = running_loss_train \/ len(train_data)\n    epoch_acc_train = running_corrects_train.double() \/ len(train_data)\n\n    cnn_classifier.eval()\n    running_loss_test = 0.\n    running_corrects_test = 0\n    for i ,(images,labels) in enumerate(test_loader):    \n        images = images.to(device)\n        labels = labels.to(device)\n        outputs, features = cnn_classifier(images)\n        loss = loss_function(outputs, labels)\n        _, preds = torch.max(outputs, 1)\n        running_loss_test += loss.item() * images.size(0)\n        running_corrects_test += torch.sum(preds == labels.data)\n    epoch_loss_test = running_loss_test \/ len(test_data)\n    epoch_acc_test = running_corrects_test.double() \/ len(test_data)         \n\n    print(f'Epoch {epoch+1}\/{num_epochs}\\t'\n          f'Train loss: {epoch_loss_train:.4f} | '\n          f'Train accuracy: {epoch_acc_train:.4f} | '\n          f'Test loss: {epoch_loss_test:.4f} | '\n          f'Test accuracy: {epoch_acc_test:.4f}')","7b5379f4":"train_loader = DataLoader(dataset=train_data, batch_size=len(train_data))\ntest_loader = DataLoader(dataset=test_data, batch_size=len(test_data))\ncnn_classifier.eval()\n\nwith torch.no_grad():\n    images, train_labels = next(iter(train_loader))\n    _, train_features = cnn_classifier(images.to(device))\n    train_features = train_features.cpu()\n    \n    images, test_labels = next(iter(test_loader))\n    _, test_features = cnn_classifier(images.to(device))\n    test_features = test_features.cpu()\n\n\nfigure(num=None, figsize=(15, 6), dpi=80, facecolor='w', edgecolor='k')\nplt.subplot(121)\nfor i in range(10): plt.scatter(train_features[train_labels==i].T[0], train_features[train_labels==i].T[1], c=clr[i], s=.1)\n\nplt.subplot(122)\nfor i in range(10): plt.scatter(test_features[test_labels==i].T[0], test_features[test_labels==i].T[1], c=clr[i], s=1, label=f'{i}')\nplt.legend()","a3a72894":"class SiameseNet(nn.Module):\n    def __init__(self):\n        super(SiameseNet,self).__init__()\n        self.conv1 = nn.Conv2d(1, 8, 5)\n        self.maxpool = nn.MaxPool2d(2, stride=2)\n        self.conv2 = nn.Conv2d(8, 8, 5)\n        self.fc1 = nn.Linear(8*4*4, 2)\n        self.relu = nn.ReLU()\n        self.bn1 = nn.BatchNorm2d(8)\n        self.bn2 = nn.BatchNorm2d(8)\n\n    def forward(self,x1, x2):\n        out1 = self.conv1(x1)\n        out1 = self.maxpool(out1)\n        out1 = self.relu(out1)\n        out1 = self.bn1(out1)\n        out1 = self.conv2(out1)\n        out1 = self.maxpool(out1)\n        out1 = self.relu(out1)\n        out1 = self.bn2(out1)\n        out1 = out1.view(out1.size()[0], -1)\n        out1 = self.fc1(out1)\n        \n        out2 = self.conv1(x2)\n        out2 = self.maxpool(out2)\n        out2 = self.relu(out2)\n        out2 = self.bn1(out2)\n        out2 = self.conv2(out2)\n        out2 = self.maxpool(out2)\n        out2 = self.relu(out2)\n        out2 = self.bn2(out2)\n        out2 = out2.view(out2.size()[0], -1)\n        out2 = self.fc1(out2)\n\n        return out1, out2","255f06c5":"class siameseMNIST(Dataset):\n    def __init__(self, phase):\n        self.samples = []\n        self.phase = phase\n        self.train_indices = []\n        self.test_indices = []\n        for i in range(10):\n            self.train_indices.append([x for x, item in enumerate(train_data.targets == i) if item])\n            self.test_indices.append([x for x, item in enumerate(test_data.targets == i) if item])\n\n    def __getitem__(self, index):\n        if self.phase == 'train':\n            x1, label = train_data.__getitem__(index)\n            if random.random() > 0.5:\n                x2, _ = train_data.__getitem__(random.choice(self.train_indices[label]))\n                pair_label = 1\n            else:\n                all_classes = list(range(10))\n                _ = all_classes.pop(label)\n                diff_class = random.choice(all_classes)\n                x2, _ = train_data.__getitem__(random.choice(self.train_indices[diff_class]))\n                pair_label = 0\n            return x1, x2, pair_label \n\n        else:\n            x1, label = test_data.__getitem__(index)\n            if random.random() > 0.5:\n                x2, _ = test_data.__getitem__(random.choice(self.test_indices[label]))\n                pair_label = 1\n            else:\n                all_classes = list(range(10))\n                _ = all_classes.pop(label)\n                diff_class = random.choice(all_classes)\n                x2, _ = test_data.__getitem__(random.choice(self.test_indices[diff_class]))\n                pair_label = 0\n            return x1, x2, pair_label\n\n    def __len__(self):\n        if self.phase == 'train': return 60000\n        return 10000","9358ff95":"siamese_train_data = siameseMNIST(phase='train')\nsiamese_test_data = siameseMNIST(phase='test')\n\ntrain_loader = DataLoader(dataset=siamese_train_data, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=siamese_test_data, batch_size=batch_size, shuffle=False)","9f39466a":"class ContrastiveLoss(nn.Module):\n    def __init__(self, margin):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n        self.eps = 1e-9\n\n    def forward(self, output1, output2, target):\n        distances = (output2 - output1).pow(2).sum(1)\n        losses = 0.5 * (target.float() * distances +\n                        (1 + -1 * target).float() * F.relu(self.margin - (distances + self.eps).sqrt()).pow(2))\n        return losses.mean()","ecd20f61":"siamese_net = SiameseNet().to(device)\nloss_function = ContrastiveLoss(1.)\noptimizer = torch.optim.Adam(siamese_net.parameters(), lr=1e-1)","a3a02269":"for epoch in range(num_epochs):\n    siamese_net.train()\n    running_loss_train = 0.\n    for i ,(images_1, images_2,labels) in enumerate(train_loader):    \n        images_1 = images_1.to(device)\n        images_2 = images_2.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        output_1, output_2 = siamese_net(images_1, images_2)\n        loss = loss_function(output_1, output_2, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss_train += loss.item() * images_1.size(0)\n    epoch_loss_train = running_loss_train \/ len(siamese_train_data)\n\n    siamese_net.eval()\n    running_loss_test = 0.\n    for i ,(images_1, images_2,labels) in enumerate(test_loader):    \n        images_1 = images_1.to(device)\n        images_2 = images_2.to(device)\n        labels = labels.to(device)\n        output_1, output_2 = siamese_net(images_1, images_2)\n        loss = loss_function(output_1, output_2, labels)\n        running_loss_test += loss.item() * images_1.size(0)\n    epoch_loss_test = running_loss_test \/ len(siamese_test_data)\n\n    print(f'Epoch {epoch+1}\/{num_epochs} | '\n          f'Train loss: {epoch_loss_train} | '\n          f'Test loss: {epoch_loss_test}')\n    ","689e21aa":"class siamese_test(nn.Module):\n    def __init__(self):\n        super(siamese_test,self).__init__()\n        self.conv1 = nn.Conv2d(1, 8, 5)\n        self.maxpool = nn.MaxPool2d(2, stride=2)\n        self.conv2 = nn.Conv2d(8, 8, 5)\n        self.fc1 = nn.Linear(8*4*4, 2)\n        self.relu = nn.ReLU()\n        self.bn1 = nn.BatchNorm2d(8)\n        self.bn2 = nn.BatchNorm2d(8)\n\n    def forward(self,x1):\n        out1 = self.conv1(x1)\n        out1 = self.maxpool(out1)\n        out1 = self.relu(out1)\n        out1 = self.bn1(out1)\n        out1 = self.conv2(out1)\n        out1 = self.maxpool(out1)\n        out1 = self.relu(out1)\n        out1 = self.bn2(out1)\n        out1 = out1.view(out1.size()[0], -1)\n        out1 = self.fc1(out1)\n        return out1","82dd012d":"siamese_test_net = siamese_test().to(device)\nsiamese_test_net.load_state_dict(siamese_net.state_dict())\n\ntrain_loader = DataLoader(dataset=train_data, batch_size=len(train_data))\ntest_loader = DataLoader(dataset=test_data, batch_size=len(test_data))\n\nsiamese_test_net.eval()\n\nwith torch.no_grad():\n    images, train_labels = next(iter(train_loader))\n    train_features = siamese_test_net(images.to(device))\n    train_features = train_features.cpu()\n    \n    images, test_labels = next(iter(test_loader))\n    test_features = siamese_test_net(images.to(device))\n    test_features = test_features.cpu()\n\nfigure(num=None, figsize=(15, 6), dpi=80, facecolor='w', edgecolor='k')\n\nplt.subplot(121)\nfor i in range(10): plt.scatter(train_features[train_labels==i].T[0], train_features[train_labels==i].T[1], c=clr[i], s=.1)\n\nplt.subplot(122)\nfor i in range(10): plt.scatter(test_features[test_labels==i].T[0], test_features[test_labels==i].T[1], c=clr[i], s=1, label=f'{i}')\nplt.legend()","196c301b":"class TripletLoss(nn.Module):\n    def __init__(self, margin):\n        super(TripletLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, anchor, positive, negative, size_average=True):\n        distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)\n        distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)\n        losses = F.relu(distance_positive - distance_negative + self.margin)\n        return losses.mean() if size_average else losses.sum()","53a7ac0b":"class tripletMNIST(Dataset):\n    def __init__(self, phase):\n        self.samples = []\n        self.phase = phase\n        self.train_indices = []\n        self.test_indices = []\n        for i in range(10):\n            self.train_indices.append([x for x, item in enumerate(train_data.targets == i) if item])\n            self.test_indices.append([x for x, item in enumerate(test_data.targets == i) if item])\n\n    def __getitem__(self, index):\n        if self.phase == 'train':\n            anchor, label = train_data.__getitem__(index) ## __getitem__ of MNIST dataset class\n            pos, _ = train_data.__getitem__(random.choice(self.train_indices[label]))\n            all_classes = list(range(10))\n            _ = all_classes.pop(label)\n            diff_class = random.choice(all_classes)\n            neg, _ = train_data.__getitem__(random.choice(self.train_indices[diff_class]))\n            return anchor, pos, neg\n\n        if self.phase == 'test':\n            anchor, label = test_data.__getitem__(index)\n            pos, _ = test_data.__getitem__(random.choice(self.test_indices[label]))\n            all_classes = list(range(10))\n            _ = all_classes.pop(label)\n            diff_class = random.choice(all_classes)\n            neg, _ = test_data.__getitem__(random.choice(self.test_indices[diff_class]))\n            return anchor, pos, neg\n            \n    def __len__(self):\n        if self.phase == 'train': return 60000\n        return 10000","a70521f5":"triplet_train_data = tripletMNIST(phase='train')\ntriplet_test_data = tripletMNIST(phase='test')\n\ntrain_loader = DataLoader(dataset=triplet_train_data, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=triplet_test_data, batch_size=batch_size, shuffle=False)","749b2eb0":"class TripletNet(nn.Module):\n    def __init__(self):\n        super(TripletNet,self).__init__()\n        self.conv1 = nn.Conv2d(1, 8, 5)\n        self.maxpool = nn.MaxPool2d(2, stride=2)\n        self.conv2 = nn.Conv2d(8, 8, 5)\n        self.fc1 = nn.Linear(8*4*4, 2)\n        self.relu = nn.ReLU()\n        self.bn1 = nn.BatchNorm2d(8)\n        self.bn2 = nn.BatchNorm2d(8)\n\n    def forward(self, anchor, pos, neg):\n        anchor_embd = self.conv1(anchor)\n        anchor_embd = self.maxpool(anchor_embd)\n        anchor_embd = self.relu(anchor_embd)\n        anchor_embd = self.bn1(anchor_embd)\n        anchor_embd = self.conv2(anchor_embd)\n        anchor_embd = self.maxpool(anchor_embd)\n        anchor_embd = self.relu(anchor_embd)\n        anchor_embd = self.bn2(anchor_embd)\n        anchor_embd = anchor_embd.view(anchor_embd.size()[0], -1)\n        anchor_embd = self.fc1(anchor_embd)\n\n        pos_embd = self.conv1(pos)\n        pos_embd = self.maxpool(pos_embd)\n        pos_embd = self.relu(pos_embd)\n        pos_embd = self.bn1(pos_embd)\n        pos_embd = self.conv2(pos_embd)\n        pos_embd = self.maxpool(pos_embd)\n        pos_embd = self.relu(pos_embd)\n        pos_embd = self.bn2(pos_embd)\n        pos_embd = pos_embd.view(pos_embd.size()[0], -1)\n        pos_embd = self.fc1(pos_embd)\n\n        neg_embd = self.conv1(neg)\n        neg_embd = self.maxpool(neg_embd)\n        neg_embd = self.relu(neg_embd)\n        neg_embd = self.bn1(neg_embd)\n        neg_embd = self.conv2(neg_embd)\n        neg_embd = self.maxpool(neg_embd)\n        neg_embd = self.relu(neg_embd)\n        neg_embd = self.bn2(neg_embd)\n        neg_embd = neg_embd.view(neg_embd.size()[0], -1)\n        neg_embd = self.fc1(neg_embd)\n\n        return anchor_embd, pos_embd, neg_embd","5eded891":"triplet_net = TripletNet().to(device)\nloss_function = TripletLoss(1.)\noptimizer = torch.optim.Adam(triplet_net.parameters(), lr=1e-1)","a0209865":"for epoch in range(num_epochs):\n    triplet_net.train()\n    running_loss_train = 0.\n    for i ,(anchor, pos, neg) in enumerate(train_loader):  \n        anchor = anchor.to(device)\n        pos = pos.to(device)\n        neg = neg.to(device)\n        optimizer.zero_grad()\n        anchor_embd, pos_embd, neg_embd = triplet_net(anchor, pos, neg)\n        loss = loss_function(anchor_embd, pos_embd, neg_embd)\n        loss.backward()\n        optimizer.step()\n        running_loss_train += loss.item() * anchor.size(0)\n    epoch_loss_train = running_loss_train \/ len(triplet_train_data)\n\n    triplet_net.eval()\n    running_loss_test = 0.\n    for i ,(anchor, pos, neg) in enumerate(test_loader):  \n        anchor = anchor.to(device)\n        pos = pos.to(device)\n        neg = neg.to(device)\n        anchor_embd, pos_embd, neg_embd = triplet_net(anchor, pos, neg)\n        loss = loss_function(anchor_embd, pos_embd, neg_embd)\n        running_loss_test += loss.item() * anchor.size(0)\n    epoch_loss_test = running_loss_test \/ len(triplet_test_data)\n\n    print(f'Epoch {epoch+1}\/{num_epochs} | '\n        f'Train loss: {epoch_loss_train} | '\n        f'Test loss: {epoch_loss_test}')","e131cade":"class TripletNetTest(nn.Module):\n    def __init__(self):\n        super(TripletNetTest,self).__init__()\n        self.conv1 = nn.Conv2d(1, 8, 5)\n        self.maxpool = nn.MaxPool2d(2, stride=2)\n        self.conv2 = nn.Conv2d(8, 8, 5)\n        self.fc1 = nn.Linear(8*4*4, 2)\n        self.relu = nn.ReLU()\n        self.bn1 = nn.BatchNorm2d(8)\n        self.bn2 = nn.BatchNorm2d(8)\n\n    def forward(self, anchor):\n        anchor_embd = self.conv1(anchor)\n        anchor_embd = self.maxpool(anchor_embd)\n        anchor_embd = self.relu(anchor_embd)\n        anchor_embd = self.bn1(anchor_embd)\n        anchor_embd = self.conv2(anchor_embd)\n        anchor_embd = self.maxpool(anchor_embd)\n        anchor_embd = self.relu(anchor_embd)\n        anchor_embd = self.bn2(anchor_embd)\n        anchor_embd = anchor_embd.view(anchor_embd.size()[0], -1)\n        anchor_embd = self.fc1(anchor_embd)\n        return anchor_embd","e4972828":"triplet_test_net = TripletNetTest().to(device)\ntriplet_test_net.load_state_dict(triplet_net.state_dict())\n\ntrain_loader = DataLoader(dataset=train_data, batch_size=len(train_data))\ntest_loader = torch.utils.data.DataLoader(dataset=test_data, batch_size=len(test_data))\ntriplet_test_net.eval()\n\nwith torch.no_grad():\n    images, train_labels = next(iter(train_loader))\n    train_features = triplet_test_net(images.to(device))\n    train_features = train_features.cpu()\n    \n    images, test_labels = next(iter(test_loader))\n    test_features = triplet_test_net(images.to(device))\n    test_features = test_features.cpu()\n\n\nfigure(num=None, figsize=(15, 6), dpi=80, facecolor='w', edgecolor='k')\nplt.subplot(121)\nfor i in range(10): plt.scatter(train_features[train_labels==i].T[0], train_features[train_labels==i].T[1], c=clr[i], s=.1)\n\nplt.subplot(122)\nfor i in range(10): plt.scatter(test_features[test_labels==i].T[0], test_features[test_labels==i].T[1], c=clr[i], s=1, label=f'{i}')\nplt.legend()","0618550d":"class tripletMNIST(Dataset):\n    def __init__(self, phase):\n        self.samples = []\n        self.phase = phase\n        self.train_indices = []\n        self.test_indices = []\n        for i in range(10):\n            self.train_indices.append([x for x, item in enumerate(train_data.targets == i) if item])\n            self.test_indices.append([x for x, item in enumerate(test_data.targets == i) if item])\n\n    def __getitem__(self, index):\n        if self.phase == 'train':\n            anchor, label = train_data.__getitem__(index) ## __getitem__ of MNIST dataset class\n            pos, _ = train_data.__getitem__(random.choice(self.train_indices[label]))\n            all_classes = list(range(10))\n            _ = all_classes.pop(label)\n            diff_class = random.choice(all_classes)\n            neg, _ = train_data.__getitem__(random.choice(self.train_indices[diff_class]))\n            return anchor, pos, neg\n\n        if self.phase == 'test':\n            anchor, label = test_data.__getitem__(index)\n            pos, _ = test_data.__getitem__(random.choice(self.test_indices[label]))\n            all_classes = list(range(10))\n            _ = all_classes.pop(label)\n            diff_class = random.choice(all_classes)\n            neg, _ = test_data.__getitem__(random.choice(self.test_indices[diff_class]))\n            return anchor, pos, neg\n            \n    def __len__(self):\n        if self.phase == 'train': return 60000\n        return 10000\n\n    \n\nclass tripletMNIST5(Dataset):\n    def __init__(self, phase):\n        self.samples = []\n        self.phase = phase\n        self.train_indices = []\n        self.test_indices = []\n        for i in range(5):\n            self.train_indices.append([x for x, item in enumerate(train_data.targets == i) if item])\n            self.test_indices.append([x for x, item in enumerate(test_data.targets == i) if item])\n\n    def __getitem__(self, index):\n        if self.phase == 'train':\n            label = 9\n            labels_ = [5,6,7,8,9]\n            while label in labels_:\n                anchor, label = train_data.__getitem__(index) ## __getitem__ of MNIST dataset class\n                index = random.randint(0, 60000-1)\n            pos, _ = train_data.__getitem__(random.choice(self.train_indices[label]))\n            all_classes = list(range(5))\n            _ = all_classes.pop(label)\n            diff_class = random.choice(all_classes)\n            neg, _ = train_data.__getitem__(random.choice(self.train_indices[diff_class]))\n            return anchor, pos, neg\n\n        if self.phase == 'test':\n            label = 9\n            labels_ = [5,6,7,8,9]\n            while label in labels_:\n                anchor, label = test_data.__getitem__(index)\n                index = random.randint(0, 10000-1)\n            pos, _ = test_data.__getitem__(random.choice(self.test_indices[label]))\n            all_classes = list(range(5))\n            _ = all_classes.pop(label)\n            diff_class = random.choice(all_classes)\n            neg, _ = test_data.__getitem__(random.choice(self.test_indices[diff_class]))\n            return anchor, pos, neg\n            \n    def __len__(self):\n        if self.phase == 'train': return 60000\n        return 9000","8709a4c7":"triplet_train_data = tripletMNIST5(phase='train')\ntriplet_test_data = tripletMNIST5(phase='test')\n\ntrain_loader = DataLoader(dataset=triplet_train_data, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=triplet_test_data, batch_size=batch_size, shuffle=False)","4fec0bf9":"class TripletNet(nn.Module):\n    def __init__(self):\n        super(TripletNet,self).__init__()\n        self.conv1 = nn.Conv2d(1, 8, 5)\n        self.maxpool = nn.MaxPool2d(2, stride=2)\n        self.conv2 = nn.Conv2d(8, 8, 5)\n        self.fc1 = nn.Linear(8*4*4, 2)\n        self.relu = nn.ReLU()\n        self.bn1 = nn.BatchNorm2d(8)\n        self.bn2 = nn.BatchNorm2d(8)\n\n    def forward(self, anchor, pos, neg):\n        anchor_embd = self.conv1(anchor)\n        anchor_embd = self.maxpool(anchor_embd)\n        anchor_embd = self.relu(anchor_embd)\n        anchor_embd = self.bn1(anchor_embd)\n        anchor_embd = self.conv2(anchor_embd)\n        anchor_embd = self.maxpool(anchor_embd)\n        anchor_embd = self.relu(anchor_embd)\n        anchor_embd = self.bn2(anchor_embd)\n        anchor_embd = anchor_embd.view(anchor_embd.size()[0], -1)\n        anchor_embd = self.fc1(anchor_embd)\n\n        pos_embd = self.conv1(pos)\n        pos_embd = self.maxpool(pos_embd)\n        pos_embd = self.relu(pos_embd)\n        pos_embd = self.bn1(pos_embd)\n        pos_embd = self.conv2(pos_embd)\n        pos_embd = self.maxpool(pos_embd)\n        pos_embd = self.relu(pos_embd)\n        pos_embd = self.bn2(pos_embd)\n        pos_embd = pos_embd.view(pos_embd.size()[0], -1)\n        pos_embd = self.fc1(pos_embd)\n\n        neg_embd = self.conv1(neg)\n        neg_embd = self.maxpool(neg_embd)\n        neg_embd = self.relu(neg_embd)\n        neg_embd = self.bn1(neg_embd)\n        neg_embd = self.conv2(neg_embd)\n        neg_embd = self.maxpool(neg_embd)\n        neg_embd = self.relu(neg_embd)\n        neg_embd = self.bn2(neg_embd)\n        neg_embd = neg_embd.view(neg_embd.size()[0], -1)\n        neg_embd = self.fc1(neg_embd)\n\n        return anchor_embd, pos_embd, neg_embd","7c567313":"triplet_net = TripletNet().to(device)\nloss_function = TripletLoss(1.)\noptimizer = torch.optim.Adam(triplet_net.parameters(), lr=1e-1)","460b434c":"for epoch in range(num_epochs):\n    triplet_net.train()\n    running_loss_train = 0.\n    for i ,(anchor, pos, neg) in enumerate(train_loader):  \n        anchor = anchor.to(device)\n        pos = pos.to(device)\n        neg = neg.to(device)\n        optimizer.zero_grad()\n        anchor_embd, pos_embd, neg_embd = triplet_net(anchor, pos, neg)\n        loss = loss_function(anchor_embd, pos_embd, neg_embd)\n        loss.backward()\n        optimizer.step()\n        running_loss_train += loss.item() * anchor.size(0)\n    epoch_loss_train = running_loss_train \/ len(triplet_train_data)\n\n    triplet_net.eval()\n    running_loss_test = 0.\n    for i ,(anchor, pos, neg) in enumerate(test_loader):  \n        anchor = anchor.to(device)\n        pos = pos.to(device)\n        neg = neg.to(device)\n        anchor_embd, pos_embd, neg_embd = triplet_net(anchor, pos, neg)\n        loss = loss_function(anchor_embd, pos_embd, neg_embd)\n        running_loss_test += loss.item() * anchor.size(0)\n    epoch_loss_test = running_loss_test \/ len(triplet_test_data)\n\n    print(f'Epoch {epoch+1}\/{num_epochs} | '\n        f'Train loss: {epoch_loss_train} | '\n        f'Test loss: {epoch_loss_test}')","12359f49":"class TripletNetTest(nn.Module):\n    def __init__(self):\n        super(TripletNetTest,self).__init__()\n        self.conv1 = nn.Conv2d(1, 8, 5)\n        self.maxpool = nn.MaxPool2d(2, stride=2)\n        self.conv2 = nn.Conv2d(8, 8, 5)\n        self.fc1 = nn.Linear(8*4*4, 2)\n        self.relu = nn.ReLU()\n        self.bn1 = nn.BatchNorm2d(8)\n        self.bn2 = nn.BatchNorm2d(8)\n\n    def forward(self, anchor):\n        anchor_embd = self.conv1(anchor)\n        anchor_embd = self.maxpool(anchor_embd)\n        anchor_embd = self.relu(anchor_embd)\n        anchor_embd = self.bn1(anchor_embd)\n        anchor_embd = self.conv2(anchor_embd)\n        anchor_embd = self.maxpool(anchor_embd)\n        anchor_embd = self.relu(anchor_embd)\n        anchor_embd = self.bn2(anchor_embd)\n        anchor_embd = anchor_embd.view(anchor_embd.size()[0], -1)\n        anchor_embd = self.fc1(anchor_embd)\n        return anchor_embd","774428b6":"triplet_test_net = TripletNetTest().to(device)\ntriplet_test_net.load_state_dict(triplet_net.state_dict())\n\ntrain_loader = DataLoader(dataset=train_data, batch_size=len(train_data))\ntest_loader = DataLoader(dataset=test_data, batch_size=len(test_data))\ntriplet_test_net.eval()\n\nwith torch.no_grad():\n    images, train_labels = next(iter(train_loader))\n    train_features = triplet_test_net(images.to(device))\n    train_features = train_features.cpu()\n    \n    images, test_labels = next(iter(test_loader))\n    test_features = triplet_test_net(images.to(device))\n    test_features = test_features.cpu()\n\n\nfigure(num=None, figsize=(15, 12), dpi=80, facecolor='w', edgecolor='k')\nplt.subplot(221)\nfor i in range(5): plt.scatter(train_features[train_labels==i].T[0], train_features[train_labels==i].T[1], c=clr[i], s=.1)\n\nplt.subplot(222)\nfor i in range(5): plt.scatter(test_features[test_labels==i].T[0], test_features[test_labels==i].T[1], c=clr[i], s=1, label=f'{i}')\n\nplt.subplot(223)\nfor i in range(5, 10): plt.scatter(train_features[train_labels==i].T[0], train_features[train_labels==i].T[1], c=clr[i], s=.1)\n\nplt.subplot(224)\nfor i in range(5, 10): plt.scatter(test_features[test_labels==i].T[0], test_features[test_labels==i].T[1], c=clr[i], s=1, label=f'{i}')\n    \n    \nplt.legend()","13c1ef35":"figure(num=None, figsize=(15, 6), dpi=80, facecolor='w', edgecolor='k')\nplt.subplot(121)\nfor i in range(10): plt.scatter(train_features[train_labels==i].T[0], train_features[train_labels==i].T[1], c=clr[i], s=.1)\n\nplt.subplot(122)\nfor i in range(10): plt.scatter(test_features[test_labels==i].T[0], test_features[test_labels==i].T[1], c=clr[i], s=1, label=f'{i}')\n\nplt.legend()","0d4cb52e":"for i in range(5): print(train_features[train_labels==i].T[0].mean(), train_features[train_labels==i].T[1].mean())","b650d315":"torch.save(triplet_net.state_dict(), 'five_class_triplet.pt')","51b105f4":"class tripletMNIST(Dataset):\n    def __init__(self, phase):\n        self.samples = []\n        self.phase = phase\n        self.train_indices = []\n        self.test_indices = []\n        for i in range(10):\n            self.train_indices.append([x for x, item in enumerate(train_data.targets == i) if item])\n            self.test_indices.append([x for x, item in enumerate(test_data.targets == i) if item])\n\n    def __getitem__(self, index):\n        if self.phase == 'train':\n            anchor, label = train_data.__getitem__(index) ## __getitem__ of MNIST dataset class\n            pos, _ = train_data.__getitem__(random.choice(self.train_indices[label]))\n            all_classes = list(range(10))\n            _ = all_classes.pop(label)\n            diff_class = random.choice(all_classes)\n            neg, _ = train_data.__getitem__(random.choice(self.train_indices[diff_class]))\n            return anchor, pos, neg\n\n        if self.phase == 'test':\n            anchor, label = test_data.__getitem__(index)\n            pos, _ = test_data.__getitem__(random.choice(self.test_indices[label]))\n            all_classes = list(range(10))\n            _ = all_classes.pop(label)\n            diff_class = random.choice(all_classes)\n            neg, _ = test_data.__getitem__(random.choice(self.test_indices[diff_class]))\n            return anchor, pos, neg\n            \n    def __len__(self):\n        if self.phase == 'train': return 60000\n        return 10000\n\n    \n\nclass tripletMNIST5(Dataset):\n    def __init__(self, phase):\n        self.samples = []\n        self.phase = phase\n        self.train_indices = []\n        self.test_indices = []\n        for i in range(5):\n            self.train_indices.append([x for x, item in enumerate(train_data.targets == i) if item])\n            self.test_indices.append([x for x, item in enumerate(test_data.targets == i) if item])\n\n    def __getitem__(self, index):\n        if self.phase == 'train':\n            label = 9\n            labels_ = [5,6,7,8,9]\n            while label in labels_:\n                anchor, label = train_data.__getitem__(index) ## __getitem__ of MNIST dataset class\n                index = random.randint(0, 60000-1)\n            pos, _ = train_data.__getitem__(random.choice(self.train_indices[label]))\n            all_classes = list(range(5))\n            _ = all_classes.pop(label)\n            diff_class = random.choice(all_classes)\n            neg, _ = train_data.__getitem__(random.choice(self.train_indices[diff_class]))\n            return anchor, pos, neg\n\n        if self.phase == 'test':\n            label = 9\n            labels_ = [5,6,7,8,9]\n            while label in labels_:\n                anchor, label = test_data.__getitem__(index)\n                index = random.randint(0, 10000-1)\n            pos, _ = test_data.__getitem__(random.choice(self.test_indices[label]))\n            all_classes = list(range(5))\n            _ = all_classes.pop(label)\n            diff_class = random.choice(all_classes)\n            neg, _ = test_data.__getitem__(random.choice(self.test_indices[diff_class]))\n            return anchor, pos, neg\n            \n    def __len__(self):\n        if self.phase == 'train': return 60000\n        return 9000","1f615c9d":"triplet_train_data = tripletMNIST5(phase='train')\ntriplet_test_data = tripletMNIST5(phase='test')\n\ntrain_loader = DataLoader(dataset=triplet_train_data, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=triplet_test_data, batch_size=batch_size, shuffle=False)","98838189":"class TripletNet(nn.Module):\n    def __init__(self):\n        super(TripletNet,self).__init__()\n        self.conv1 = nn.Conv2d(1, 8, 5)\n        self.maxpool = nn.MaxPool2d(2, stride=2)\n        self.conv2 = nn.Conv2d(8, 8, 5)\n        self.fc1 = nn.Linear(8*4*4, 8)\n        self.relu = nn.ReLU()\n        self.bn1 = nn.BatchNorm2d(8)\n        self.bn2 = nn.BatchNorm2d(8)\n\n    def forward(self, anchor, pos, neg):\n        anchor_embd = self.conv1(anchor)\n        anchor_embd = self.maxpool(anchor_embd)\n        anchor_embd = self.relu(anchor_embd)\n        anchor_embd = self.bn1(anchor_embd)\n        anchor_embd = self.conv2(anchor_embd)\n        anchor_embd = self.maxpool(anchor_embd)\n        anchor_embd = self.relu(anchor_embd)\n        anchor_embd = self.bn2(anchor_embd)\n        anchor_embd = anchor_embd.view(anchor_embd.size()[0], -1)\n        anchor_embd = self.fc1(anchor_embd)\n\n        pos_embd = self.conv1(pos)\n        pos_embd = self.maxpool(pos_embd)\n        pos_embd = self.relu(pos_embd)\n        pos_embd = self.bn1(pos_embd)\n        pos_embd = self.conv2(pos_embd)\n        pos_embd = self.maxpool(pos_embd)\n        pos_embd = self.relu(pos_embd)\n        pos_embd = self.bn2(pos_embd)\n        pos_embd = pos_embd.view(pos_embd.size()[0], -1)\n        pos_embd = self.fc1(pos_embd)\n\n        neg_embd = self.conv1(neg)\n        neg_embd = self.maxpool(neg_embd)\n        neg_embd = self.relu(neg_embd)\n        neg_embd = self.bn1(neg_embd)\n        neg_embd = self.conv2(neg_embd)\n        neg_embd = self.maxpool(neg_embd)\n        neg_embd = self.relu(neg_embd)\n        neg_embd = self.bn2(neg_embd)\n        neg_embd = neg_embd.view(neg_embd.size()[0], -1)\n        neg_embd = self.fc1(neg_embd)\n\n        return anchor_embd, pos_embd, neg_embd","e0a4571b":"triplet_net = TripletNet().to(device)\nloss_function = TripletLoss(1.)\noptimizer = torch.optim.Adam(triplet_net.parameters(), lr=1e-1)","04f0f24c":"for epoch in range(num_epochs):\n    triplet_net.train()\n    running_loss_train = 0.\n    for i ,(anchor, pos, neg) in enumerate(train_loader):  \n        anchor = anchor.to(device)\n        pos = pos.to(device)\n        neg = neg.to(device)\n        optimizer.zero_grad()\n        anchor_embd, pos_embd, neg_embd = triplet_net(anchor, pos, neg)\n        loss = loss_function(anchor_embd, pos_embd, neg_embd)\n        loss.backward()\n        optimizer.step()\n        running_loss_train += loss.item() * anchor.size(0)\n    epoch_loss_train = running_loss_train \/ len(triplet_train_data)\n\n    triplet_net.eval()\n    running_loss_test = 0.\n    for i ,(anchor, pos, neg) in enumerate(test_loader):  \n        anchor = anchor.to(device)\n        pos = pos.to(device)\n        neg = neg.to(device)\n        anchor_embd, pos_embd, neg_embd = triplet_net(anchor, pos, neg)\n        loss = loss_function(anchor_embd, pos_embd, neg_embd)\n        running_loss_test += loss.item() * anchor.size(0)\n    epoch_loss_test = running_loss_test \/ len(triplet_test_data)\n\n    print(f'Epoch {epoch+1}\/{num_epochs} | '\n        f'Train loss: {epoch_loss_train} | '\n        f'Test loss: {epoch_loss_test}')","2b08563e":"class TripletNetTest(nn.Module):\n    def __init__(self):\n        super(TripletNetTest,self).__init__()\n        self.conv1 = nn.Conv2d(1, 8, 5)\n        self.maxpool = nn.MaxPool2d(2, stride=2)\n        self.conv2 = nn.Conv2d(8, 8, 5)\n        self.fc1 = nn.Linear(8*4*4, 8)\n        self.relu = nn.ReLU()\n        self.bn1 = nn.BatchNorm2d(8)\n        self.bn2 = nn.BatchNorm2d(8)\n\n    def forward(self, anchor):\n        anchor_embd = self.conv1(anchor)\n        anchor_embd = self.maxpool(anchor_embd)\n        anchor_embd = self.relu(anchor_embd)\n        anchor_embd = self.bn1(anchor_embd)\n        anchor_embd = self.conv2(anchor_embd)\n        anchor_embd = self.maxpool(anchor_embd)\n        anchor_embd = self.relu(anchor_embd)\n        anchor_embd = self.bn2(anchor_embd)\n        anchor_embd = anchor_embd.view(anchor_embd.size()[0], -1)\n        anchor_embd = self.fc1(anchor_embd)\n        return anchor_embd","1e84e3d9":"triplet_test_net = TripletNetTest().to(device)\ntriplet_test_net.load_state_dict(triplet_net.state_dict())\n\ntrain_loader = DataLoader(dataset=train_data, batch_size=len(train_data))\ntest_loader = DataLoader(dataset=test_data, batch_size=len(test_data))\ntriplet_test_net.eval()\n\nwith torch.no_grad():\n    images, train_labels = next(iter(train_loader))\n    train_features = triplet_test_net(images.to(device))\n    train_features = train_features.cpu()\n    \n    images, test_labels = next(iter(test_loader))\n    test_features = triplet_test_net(images.to(device))\n    test_features = test_features.cpu()","38261eae":"torch.mean(train_features, dim=0)","49b0e846":"for i in range(5): print(train_features[train_labels==i].T[0].mean(), train_features[train_labels==i].T[1].mean())","53f9984c":"for i in range(5): print(torch.mean(train_features[train_labels==i], dim=0))","bf7de49b":"torch.save(triplet_net.state_dict(), 'mnist_triplet_8__.pt')","d6534aaa":"# Plot 2D features learnt by fully connected NN","ea5e3297":"# Siamese network -Training\n- While training, we pass two images through CNN and calculate loss using embeddings of pair of image.\n- forward method takes two images as input, passes each image through the same CNN and returns embeddings of each image","adb2dd16":"# Train on five classes, test on all","bf0aeb2c":"# Fully connected NN","0474bac2":"## Dataset class\n- returns pair of images with label indicating if pair is positive(simialr) or negative(disimilar)","9df14a64":"## Few hyper-parametes","e4bbff28":"Colors for vizualization","f48df4b3":"# Train Fully connected NN","6770d780":"# Imports","a647f65e":"## Contrastive Loss","4cb3bcbe":"# Similar excercise for CNN","4b4caa52":"# Triplet loss","2cc3e732":"# MNIST dataset provided in pytorch for training and validation","da759651":"We are going to visualize the features extracted by fully connected NN in pre-final layer. Our initial implentation does not return features in forward method, so we are writing new class for fully connected NN. We will copy weights of trained NN in newly implemented NN class.","fb8527bb":"# Class for test-time siamese","7868a54d":"Metric Learning on MNIST using PyTorch.\n\nSTEPS:\n- Train a fully connected using MNIST\n\n- The architecture of NN: 786, 256, 2, 10\n\n- Why 2 neurons at the pre-final layer?\n    - Because we want to show embeddings of each image in 2D..\n\n- Visualize embedding\n    - Mostly elongated clusters.\n\n- Same exercise for CNN with 2 neurons at pre-final layer\n\n- Similar visualization\n\n- Train Siamese network\n    - Dataset class implementation\n    - Custom Loss function\n    - Difference between train-time and test-time\n\n- Visualize embeddings\n    - Clusters with nice circular shape can be visualized.\n\n- Repeat training and visualization for Triplet network\n    - Training is faster. Clusters are better than siamese.\n    \n- Use trained NN on Jetson Nano for inference"}}