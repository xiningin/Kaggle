{"cell_type":{"ad5fd516":"code","23d1cf08":"code","0f7e2b23":"code","5306385c":"code","c7b946f4":"code","c335aa15":"code","ee3efcf3":"code","3442fc6e":"code","94bd83cc":"code","8f6cf414":"code","bd51aed9":"code","b64e6f24":"markdown","c3ee9aed":"markdown","d5cf92e1":"markdown","82a1312f":"markdown","ca89eb89":"markdown","b6c163ae":"markdown","2bc92952":"markdown","0e3d8150":"markdown","0e5dca22":"markdown","e9ddeba5":"markdown"},"source":{"ad5fd516":"import torch\nimport torch.nn as nn\nimport numpy as np","23d1cf08":"inputs = torch.Tensor([[0.1, 0.1, 0.1, 0.1, 0.8]]).float() # pred as 4\ntargets = torch.Tensor([3]).long()\n\nloss_func = torch.nn.CrossEntropyLoss()\nprint(loss_func(inputs, targets))","0f7e2b23":"inputs = torch.Tensor([[0.8, 0.1, 0.1, 0.1, 0.1]]).float() # pred as 0\ntargets = torch.Tensor([3]).long()\n\nloss_func = torch.nn.CrossEntropyLoss()\nprint(loss_func(inputs, targets))","5306385c":"def pos_weight(pred_tensor, pos_tensor, neg_weight=1, pos_weight=1):\n    # neg_weight for when pred position < target position\n    # pos_weight for when pred position > target position\n    gap = torch.argmax(pred_tensor, dim=1) - pos_tensor\n    gap = gap.type(torch.float32)\n    return torch.where(gap < 0, -neg_weight * gap, pos_weight * gap).detach()","c7b946f4":"inputs = torch.Tensor([[0.1, 0.1, 0.1, 0.1, 0.8]]).float() # pred as 4\ntargets = torch.Tensor([3]).long()\n\npos_weight(inputs, targets, 1, 1)","c335aa15":"inputs = torch.Tensor([[0.8, 0.1, 0.1, 0.1, 0.1]]).float() # pred as 0\ntargets = torch.Tensor([3]).long()\n\npos_weight(inputs, targets, 1, 1)","ee3efcf3":"def loss_fn(start_logits, end_logits, start_positions, end_positions):\n    loss_fct = nn.CrossEntropyLoss(reduce='none') # do reduction later\n    \n    start_loss = loss_fct(start_logits, start_positions) * pos_weight(start_logits, start_positions, 1, 1)\n    end_loss = loss_fct(end_logits, end_positions) * pos_weight(end_logits, end_positions, 1, 1)\n    \n    start_loss = torch.mean(start_loss)\n    end_loss = torch.mean(end_loss)\n    \n    total_loss = (start_loss + end_loss)\n    return total_loss","3442fc6e":"# argmax pred for the start is 3, target is 1\n# argmax pred for the end is 3, target is 3\nstart = torch.Tensor([[0.1, 0.1, 0.1, 0.8, 0.1]]).float()\nstart_target = torch.Tensor([1]).long()\n\nend = torch.Tensor([[0.1, 0.1, 0.1, 0.8, 0.1]]).float()\nend_target = torch.Tensor([3]).long()","94bd83cc":"loss_fn(start, end, start_target, end_target)","8f6cf414":"# argmax pred for the start is 2, target is 1\n# argmax pred for the end is 3, target is 3\nstart = torch.Tensor([[0.1, 0.1, 0.8, 0.1, 0.1]]).float()\nstart_target = torch.Tensor([1]).long()\n\nend = torch.Tensor([[0.1, 0.1, 0.1, 0.8, 0.1]]).float()\nend_target = torch.Tensor([3]).long()","bd51aed9":"loss_fn(start, end, start_target, end_target)","b64e6f24":"The larger the gap is, the more penalty it'll get.","c3ee9aed":"Thanks laevatein for great notebook: https:\/\/www.kaggle.com\/laevatein\/tweat-the-loss-function-a-bit","d5cf92e1":"Now let's add more penalty when our \"argmax prediction\" is far away from our target.   \n\nThis function also allows you to add different penalty to \"argmax prediction\" before\/after the target.","82a1312f":"As you can see, the loss is the same, but 4 is much closer to 3 than 0. Then how can we optimise the loss function a bit?","ca89eb89":"# Penalty based on the position gap","b6c163ae":"Now it's time to combine them. A simply tweak on the original `loss_fn` is all we need.  \n\nYou can either multiple `loss_fct` with `pos_weight`, or squeeze `pos_weight` first and then add them together.","2bc92952":"We usually use the loss function used is the `CrossEntropyLoss`.  \n\nHowever, one drawback of `CrossEntropyLoss` if that it doesn't care \"the position of the error\".   \n\nWe can see from a simple example.   \n\nLet's say we got a sentence of length 5 (index starts from 0), and the correct answer is position 3.   \n\nThe first prediction has its highest probability at position 4, while the second prediction put it at position 0.","0e3d8150":"# Start from CrossEntropyLoss","0e5dca22":"As you can see, the loss for the second one is smaller, which also aligns with the jaccard.","e9ddeba5":"# Combine them"}}