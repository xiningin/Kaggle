{"cell_type":{"f6266713":"code","21344b6f":"code","216aaf36":"code","ca66cdea":"code","2370360a":"code","aa78de1b":"code","915befa1":"code","3235aee6":"code","21d8e223":"code","54a4d5a4":"code","5fb2c921":"code","a6be8690":"code","6b0d6e22":"code","54109f8e":"code","e01de29a":"markdown","9593d801":"markdown","60f17310":"markdown","e3a66c13":"markdown"},"source":{"f6266713":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom transformers import AutoModel\nfrom transformers import AutoTokenizer\nfrom transformers import AutoConfig\n\nfrom sklearn.model_selection import KFold","21344b6f":"# Config dict\ncfg = {\n    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n    'max_len': 512,\n    'learning_rate': 2e-5,\n    'num_epochs': 3\n}","216aaf36":"cfg['device']","ca66cdea":"# Read raw csv data to a pandas df\ntrain_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')","2370360a":"# Load the BERT tokenizer\ntokenizer = AutoTokenizer.from_pretrained('..\/input\/huggingface-bert\/bert-base-uncased')","aa78de1b":"class CommonLitDataset(Dataset):\n    \"\"\" Dataset loader class for pytorch \"\"\"\n    \n    def __init__(self, df, tokenizer, max_len, test=False):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.test = test\n        \n        \n    def __len__(self):\n        return len(self.df)\n    \n    \n    def __getitem__(self, idx):\n        text = self.df.loc[idx, 'excerpt']\n        \n        # encode the text and truncate if necessary\n        inputs = self.tokenizer.encode_plus(\n            text,                                 \n            add_special_tokens=True,\n            padding='max_length',\n            max_length=self.max_len,\n            truncation=True\n        )\n        \n        # Define the BERT inputs\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs['token_type_ids']\n        \n        if self.test == False:\n            # Define the BERT outputs\n            target = self.df.loc[idx, ['target']]\n        \n            return {\n                'ids': torch.tensor(ids),\n                'mask': torch.tensor(mask),\n                'token_type_ids': torch.tensor(token_type_ids),\n                'target': torch.torch.FloatTensor(target)\n            }\n        \n        return {\n                'ids': torch.tensor(ids),\n                'mask': torch.tensor(mask),\n                'token_type_ids': torch.tensor(token_type_ids)\n            }","915befa1":"def RMSE(y_pred, y_true):\n    metric = nn.MSELoss()\n    return torch.sqrt(metric(y_pred, y_true))","3235aee6":"class CommonLitModel(nn.Module):\n    \n    def __init__(self, name):\n        super(CommonLitModel, self).__init__()\n        self.name = name\n        \n        if name == 'BERT':\n            self.bert = AutoModel.from_pretrained('..\/input\/huggingface-bert\/bert-base-uncased')\n            # Output from BERT\n            self.in_features = self.bert.pooler.dense.out_features\n        \n        self.dropout = nn.Dropout()\n        self.layer_norm = nn.LayerNorm(self.in_features)\n        self.fc = nn.Linear(self.in_features, 1)\n    \n    \n    def forward(self, ids, mask, token_type_ids):\n        \n        if self.name == 'BERT':\n            _, output = self.bert(ids,\n                                 attention_mask=mask,\n                                 token_type_ids=token_type_ids,\n                                 return_dict=False)\n            \n        output = self.layer_norm(output)\n        output = self.dropout(output)\n        output = self.fc(output)\n        return output\n","21d8e223":"def train_valid (model, optimizer, criterion, datasets, num_epochs=10):\n    \n    model.to(cfg['device'])\n    \n    for idx, (train, test) in enumerate(datasets):\n        print(f'\\nSPLIT {idx + 1}:')\n        \n        train_dataloader = DataLoader(dataset=train, shuffle=True, batch_size=16)\n        test_dataloader = DataLoader(dataset=test, shuffle=False, batch_size=1)\n        \n        # Train the model\n        model.train()\n        for epoch in range(num_epochs):\n\n            for idx, data in enumerate(train_dataloader):\n\n                optimizer.zero_grad()\n    \n                X_train = {key: value.to(cfg['device']) for (key, value) in data.items() if key != 'target'}\n                y_train = data['target'].to(cfg['device'])\n            \n                output = model(X_train['ids'],\n                               X_train['mask'],\n                               X_train['token_type_ids'])\n\n                torch.cuda.empty_cache()\n                loss = criterion(output, y_train)\n\n                if idx % 140 == 0:\n                    print(f'\\nTRAIN RMSE: {loss}')\n\n                loss.backward()\n                optimizer.step()\n                \n        # Validate the model\n        model.eval()\n        for idx, data in enumerate(test_dataloader):\n            \n            X_test = {key: value.to(cfg['device']) for (key, value) in data.items()}\n            y_test = data['target'].to(cfg['device'])\n\n            with torch.no_grad():\n                output = model(X_test['ids'],\n                                X_test['mask'],\n                                X_test['token_type_ids'])\n                \n            loss = criterion(output, y_test)\n                \n            if idx % 20 == 0:\n                print(f'\\nVALID RMSE: {loss}')\n\n            torch.cuda.empty_cache()","54a4d5a4":"# Split the dataframe in a 10 fold cross validation manner\nkf = KFold(n_splits = 10, shuffle = True, random_state = 4)\ndatasets = []\n\nfor train_index, test_index in kf.split(train_df):\n    \n    train = train_df.iloc[train_index].reset_index(drop=True)\n    test =  train_df.iloc[test_index].reset_index(drop=True)\n    \n    datasets.append((CommonLitDataset(train, tokenizer, cfg['max_len'], test=False),\n                    CommonLitDataset(test, tokenizer, cfg['max_len'], test=False)))","5fb2c921":"model = CommonLitModel('BERT')\ncriterion = RMSE\noptimizer = torch.optim.AdamW(model.parameters(), lr=cfg['learning_rate'])\n\ntorch.cuda.empty_cache()\ntrain_valid(model, optimizer, criterion, datasets, num_epochs=cfg['num_epochs'])","a6be8690":"def test (model, dataloader):\n    \n    model.eval()\n    model.to(cfg['device'])\n    output_list = []\n    \n    for idx, data in enumerate(dataloader):\n            \n        X_train = {key: value.to(cfg['device']) for (key, value) in data.items()}\n        \n        with torch.no_grad():\n            output = model(X_train['ids'],\n                            X_train['mask'],\n                            X_train['token_type_ids'])\n        \n        output_list.append(output.item())\n        torch.cuda.empty_cache()\n    \n    return output_list\n            ","6b0d6e22":"dataset = CommonLitDataset(test_df, tokenizer, cfg['max_len'], test=True)\ndataloader = DataLoader(dataset=dataset, shuffle=False, batch_size=1)\n\ntorch.cuda.empty_cache()\noutputs = test(model, dataloader)","54109f8e":"# Save the output\noutput_data = {\n    'id': test_df['id'],\n    'target': outputs\n}\n\noutput_df = pd.DataFrame(output_data, columns=['id', 'target'])\noutput_df.to_csv('.\/submission.csv', index = False, header=True)","e01de29a":"## Criterion (RMSE)","9593d801":"## Dataset class","60f17310":"## TODO\n- Look into early-stopping","e3a66c13":"## Model class"}}