{"cell_type":{"7bdece5a":"code","028458cb":"code","edcf4f5c":"code","dc05ed93":"code","c8c069a8":"code","4621363f":"code","d2f068ab":"code","748133af":"code","d18d5e91":"code","46a0d2b1":"code","387ec299":"code","a1d9b83d":"code","3e9e2c30":"code","932a1765":"code","a0462eae":"code","1154337a":"code","4e3a44e2":"code","1a40083e":"code","2c507e7d":"code","924833e0":"markdown","41de88a5":"markdown","2419c60a":"markdown","b7be7fe3":"markdown","d3f4c7d7":"markdown","62395824":"markdown","a0a4c7c5":"markdown","7a77ef38":"markdown"},"source":{"7bdece5a":"#collapse\n%load_ext autoreload\n%autoreload 2","028458cb":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import models, transforms\nimport torch.optim as optim\n\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageEnhance\nfrom tqdm.notebook import tqdm\nimport gc\n\n%matplotlib inline","edcf4f5c":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # setting the deivice","dc05ed93":"# model that need to be visualised\n# we will visualise resnet-50 model\nmodel = models.resnet50(pretrained=True)","c8c069a8":"#hide\nmodel.to(device)","4621363f":"# here we will visualise the filters of first convolution layers of model\n# i.e conv1\nplt.figure(figsize=(10, 10))\nfor i, filter_weight in enumerate(model.conv1.weight):\n    plt.subplot(8, 8, i + 1)\n    v = filter_weight\n    min, max = torch.min(v), torch.max(v)\n    v = (v - min) \/ (max - min)\n    plt.imshow(v.permute(1, 2, 0).detach().cpu().numpy(), cmap='gray')\n    plt.axis('off')\n    plt.suptitle('1st Convolutional Layer Weights\/Filters')","d2f068ab":"class SaveFeatures:\n    def __init__(self, module):\n        self.hook = module.register_forward_hook(self.hook_fn)\n        \n    def hook_fn(self, module, inp, out):\n        self.features = out\n        \n    def close(self):\n        self.hook.remove()","748133af":"# returns a dummy image \ndef get_image(height, width):\n    image = np.uint8(np.random.uniform(150, 180, (height, width, 3)))\/255\n    return image","d18d5e91":"#collapse\nclass FilterVisualizer():\n    def __init__(self, model, module, size=(56, 56),\n                 upscaling_steps=12, upscaling_factor=0.95):\n        self.size, self.upscaling_steps, self.upscaling_factor = size, upscaling_steps, upscaling_factor\n        self.module = module\n        self.model = model.eval()\n\n    def visualize(self, filter, lr=0.01, opt_steps=20, blur=None):\n        sz = self.size\n        sz1 = sz[0]\n        sz2 = sz[1]\n        \n        activations = SaveFeatures(self.module)  # register hook\n        \n        img = get_image(*sz)\n        for _ in range(self.upscaling_steps):  # scale the image up upscaling_steps times\n            img_var = torch.tensor(img).to(device)\n            img_var = img_var.unsqueeze(0).permute(0, 3, 1, 2).float().requires_grad_()\n            \n\n            optimizer = optim.Adam([img_var], lr=lr, weight_decay=1e-6)\n            for n in range(opt_steps):  # optimize pixel values for opt_steps times\n                optimizer.zero_grad()\n                self.model(img_var)\n                loss = -activations.features[0, filter].mean()\n                loss.backward()\n                optimizer.step()\n\n            img = img_var.data.cpu().numpy()[0].transpose(1,2,0)\n            sz1 = int(self.upscaling_factor * sz1)\n            sz2 = int(self.upscaling_factor * sz2)  # calculate new image size\n            \n            img = cv2.resize(img, (sz1, sz2), interpolation = cv2.INTER_CUBIC)  # scale image up\n            if blur is not None: img = cv2.blur(img,(blur,blur), 0)  # blur image to reduce high frequency patterns\n              \n        activations.close()\n        \n        return np.clip(img, 0, 1)","46a0d2b1":"f1 = FilterVisualizer(model, model.conv1, size=(56, 56),\n                      upscaling_steps=20, upscaling_factor=1.11)","387ec299":"# here we will visualise the filters of first convolution layers of model\n# i.e conv1\n\nplt.figure(figsize=(20, 20))\nfor i in tqdm(range(64)):\n    plt.subplot(8, 8, i + 1)\n    v = f1.visualize(i, opt_steps=20, lr=0.01, blur=5)\n    min, max = np.min(v), np.max(v)\n    v = (v - min) \/ (max - min)\n    plt.imshow(v)\n    plt.axis('off')\n    plt.suptitle('model.conv1 Excitory Patterns', size='xx-large')","a1d9b83d":"f2 = FilterVisualizer(model, model.layer1[0].conv1,\n                      upscaling_steps=15, upscaling_factor=1.11)","3e9e2c30":"# here we will visualise the filters of first convolution layers of model\n# i.e conv1\n\nplt.figure(figsize=(20, 20))\nfor i in tqdm(range(64)):\n    plt.subplot(8, 8, i + 1)\n    v = f2.visualize(i, blur=5)\n    min, max = np.min(v), np.max(v)\n    v = (v - min) \/ (max - min)\n    plt.imshow(v)\n    #plt.axis('off')\n    plt.suptitle('model.layer1[0].conv1 Excitory Patterns', size='xx-large')","932a1765":"f3 = FilterVisualizer(model, model.layer3[0].conv2,\n                      upscaling_steps=15, upscaling_factor=1.11)","a0462eae":"# here we will visualise the filters of first convolution layers of model\n# i.e conv1\n\nplt.figure(figsize=(20, 20))\nfor i in tqdm(range(64)):\n    plt.subplot(8, 8, i + 1)\n    v = f3.visualize(i, blur=5)\n    min, max = np.min(v), np.max(v)\n    v = (v - min) \/ (max - min)\n    plt.imshow(v)\n    plt.axis('off')\n    plt.suptitle('model.layer3[0].conv2 Excitory Patterns', size='xx-large')","1154337a":"f4 = FilterVisualizer(model, model.fc,\n                      upscaling_steps=25, upscaling_factor=1.11)","4e3a44e2":"#collapse\n# to brighten image\ndef brighten(np_image, factor=1):\n  im = Image.fromarray((np_image*255).astype(np.uint8))\n  enhancer = ImageEnhance.Brightness(im)\n  return enhancer.enhance(factor)","1a40083e":"#collapse\nclasses = {0: 'tench, Tinca tinca',\n           1: 'goldfish, Carassius auratus',\n           2: 'great white shark, white shark, man-eater, man-eating shark, Carcharodon carcharias',\n           3: 'tiger shark, Galeocerdo cuvieri',\n           4: 'hammerhead, hammerhead shark',\n           5: 'electric ray, crampfish, numbfish, torpedo',\n           6: 'stingray',\n           7: 'cock',\n           8: 'hen',\n           9: 'ostrich, Struthio camelus',\n           10: 'brambling, Fringilla montifringilla',\n           11: 'goldfinch, Carduelis carduelis',\n           12: 'house finch, linnet, Carpodacus mexicanus',\n           13: 'junco, snowbird',\n           14: 'indigo bunting, indigo finch, indigo bird, Passerina cyanea',\n           15: 'robin, American robin, Turdus migratorius'}","2c507e7d":"# here we will visualise the filters of first convolution layers of model\n# i.e conv1\n\nplt.figure(figsize=(20, 20))\nfor i in tqdm(range(16)):\n    plt.subplot(4, 4, i + 1)\n    v = f4.visualize(i, blur=7)\n    min, max = np.min(v), np.max(v)\n    v = (v - min) \/ (max - min)\n    plt.imshow(v)\n    plt.axis('off')\n    plt.title(f'{classes[i]}')\n    plt.suptitle('Output Layer Excitory Patterns')","924833e0":"### To visualize filters of convnet there are different techniques out there. One that we are going to see here is very straight forward. what we do here is we simply plot weights of filters of first convolutional layer of network. We do this because first layer operates directly on images having 3 channels(RGB) so therfore every filters in that layer will have 3 channels too. Thats why we can plot those filters directly as image.","41de88a5":"### Hooks are just a callback function that are executed during forward\/backward pass. We need hooks because activations are only present in memory till the computation of gradients and after that they are deleted. So explicitly stores those activations.\n","2419c60a":"## The above visualizations are from last fully-connected layer of resnet50 and these are class specific. This visualization is not very clear but we can find some class specific parts in above images. If you want to read more about visualizations of neural nets you should read this [blog](https:\/\/distill.pub\/2017\/feature-visualization\/). And you can also see more detailed visualizations of some famous architectures at OpenAi's [microscope](https:\/\/microscope.openai.com\/models). If you had visited above given links and wish to create visualizations like those than you should use **lucid** library if you are **tensorflow** user or **lucent** if you are **pytorch** user.","b7be7fe3":"### As from the above visualization of filters we can see what does convnets learn in its early layers. Filters like above are used to extract very basic low level things like edges with different oreintations and color blobs.  \n### In fact, if you visualize initial layers of different convnet archictecture you will find somewhat similar figures as shown above. This gives us the evidence that convnets learns about images in different level of hierarchy from very low level in early layers to more abstract level in deeper layers.\n***","d3f4c7d7":"### Visualizing weights directly for layers other than first is not trivial so there is another way to visualize what the particular filter in convnet is looking for. This method is based on optimisation of randomly initialised image fed to the convnet. This is similar to classifying image using convnet. The only difference here is we update image instead of weights of convnet after each backward pass and target for convnet is chosen by us that is which filters activations we want to be highest. Highest activation of a particular filter suggest that convnet is looking at the pattern in the image for which the filter was trained for.","62395824":"## In this blogpost we will visualize the filters of resnet-50 architecture","a0a4c7c5":"### Here we generate an image with random values.","7a77ef38":"# How to Visualize Filters of Convolutional Neural Networks \ud83e\udd16"}}