{"cell_type":{"62ec70ef":"code","00fbd02b":"code","03d878d8":"code","62e63e55":"code","0173ab51":"code","df9bd962":"code","9f36c314":"code","c37a3136":"code","209244dc":"code","b3840416":"code","65f3982f":"code","323fe20d":"code","76c85280":"code","6bb27e82":"code","ab579178":"code","71e1b789":"code","c21a328c":"code","9b03f0ed":"code","e7ea299a":"code","4b52aba3":"code","35d06349":"code","2db1b1e7":"code","97a57064":"code","8e1284de":"code","66dc81bf":"code","21bf28c7":"markdown","ed8efcc6":"markdown","a689cdd7":"markdown","5c4de684":"markdown","355f119e":"markdown","42ab977d":"markdown","b6579efa":"markdown","df828a1c":"markdown","4af2aa31":"markdown","6634ea39":"markdown","8330482e":"markdown","99544004":"markdown","fa622393":"markdown","6febabad":"markdown","0fb66a65":"markdown","76ac23a1":"markdown","8aa0d523":"markdown","bb747ef6":"markdown"},"source":{"62ec70ef":"# import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\n\nnp.random.seed(2)\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, MaxPool2D, BatchNormalization, Activation\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom keras.regularizers import L1L2\ndense_regularizer = L1L2(l2=0.0001)\n\nimport os\nos.chdir('\/kaggle\/working')\n\nsns.set(style='white', context='notebook', palette='deep')","00fbd02b":"# Load the data\ntrain = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")","03d878d8":"# Create Y df (labels) and drop the labels from X\nY = train[\"label\"]\nX = train.drop(labels = [\"label\"],axis = 1) ","62e63e55":"# Check the distribution of labels in the training set\ng = sns.countplot(Y, color = 'darkviolet')\nY.value_counts()","0173ab51":"# Normalize the data for greyscale pixels\nX = X \/ 255.0\ntest = test \/ 255.0","df9bd962":"# Reshape images\nX = X.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)","9f36c314":"# Encode labels to one hot vectors\nY = to_categorical(Y, num_classes = 10)","c37a3136":"j = 0\nfor i in range(9):\n    plt.subplot(330 + (j+1))\n    j=j+1\n    fig = plt.imshow(test[i][:,:,0], cmap=plt.get_cmap('Purples'))\n    fig.axes.get_xaxis().set_visible(False)\n    fig.axes.get_yaxis().set_visible(False)","209244dc":"# Set the random seed\nrandom_seed = 2\n\n# Split the train and the validation set for the fitting\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size = 0.1, random_state=random_seed)","b3840416":"# Set the CNN model \n\nmodel1 = Sequential()\n\nmodel1.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', activation ='relu', input_shape = (28,28,1)))\nmodel1.add(Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same', activation ='relu'))\nmodel1.add(MaxPool2D(pool_size=(2,2)))\nmodel1.add(Dropout(0.25))\n\nmodel1.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\nmodel1.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\nmodel1.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel1.add(Dropout(0.25))\n\nmodel1.add(Flatten())\nmodel1.add(Dense(256, activation = \"relu\"))\nmodel1.add(Dropout(0.5))\nmodel1.add(Dense(10, activation = \"softmax\"))\n\n# Compile the model\nmodel1.compile(optimizer = 'adam' , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","65f3982f":"# define number of epochs and batch size\nepochs = 5\nbatch_size = 86\n\n# fit the model\nmodel1.fit(\n    X_train, \n    Y_train,\n    batch_size=batch_size, \n    epochs=epochs,\n    validation_data=(X_val, Y_val),\n    verbose=2\n)","323fe20d":"# predict results\nresults = model1.predict(test)\n\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\n\n# create the submission file\nsubmission1 = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\n# save submission file in output folder\nsubmission1.to_csv(\"sub1.csv\",index=False)","76c85280":"# recompile model1 with a new optimizer\nmodel1.compile(optimizer = 'RMSprop' , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","6bb27e82":"# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","ab579178":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(X_train)","71e1b789":"# define number of epochs and batch size\nepochs = 5\nbatch_size = 86\n\nhistory = model1.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (X_val,Y_val),\n                              verbose = 2, steps_per_epoch=X_train.shape[0] \/\/ batch_size\n                              , callbacks=[learning_rate_reduction])","c21a328c":"# predict results\nresults = model1.predict(test)\n\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\n\n# create the submission file\nsubmission2 = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\n# save submission file in output folder\nsubmission2.to_csv(\"sub2.csv\",index=False)","9b03f0ed":"# Set the CNN model 2\n\ndef Model_2(x=None):\n    # we initialize the model\n    model = Sequential()\n\n    # Conv Block 1\n    model.add(Conv2D(64, (5, 5), input_shape=(28,28,1),  padding='same', kernel_regularizer=dense_regularizer,kernel_initializer=\"he_normal\"))\n    model.add(BatchNormalization())\n    model.add(Activation('elu'))\n    model.add(Conv2D(64, (5, 5),   padding='same', kernel_regularizer=dense_regularizer,kernel_initializer=\"he_normal\"))\n    model.add(BatchNormalization())\n    model.add(Activation('elu'))\n    model.add(Conv2D(64, (5, 5),  padding='same', kernel_regularizer=dense_regularizer,kernel_initializer=\"he_normal\"))\n    model.add(BatchNormalization())\n    model.add(Activation('elu'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n    model.add(Dropout(0.3))\n\n    # Conv Block 2\n    model.add(Conv2D(128, (3, 3),  padding='same', kernel_regularizer=dense_regularizer,kernel_initializer=\"he_normal\"))\n    model.add(BatchNormalization())\n    model.add(Activation('elu'))\n    model.add(Conv2D(128, (3, 3),  padding='same', kernel_regularizer=dense_regularizer,kernel_initializer=\"he_normal\"))\n    model.add(BatchNormalization())\n    model.add(Activation('elu'))\n    model.add(Conv2D(128, (3, 3),  padding='same', kernel_regularizer=dense_regularizer,kernel_initializer=\"he_normal\"))\n    model.add(BatchNormalization())\n    model.add(Activation('elu'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n    model.add(Dropout(0.2))\n\n    # Conv Block 3\n    model.add(Conv2D(256, (3, 3),  padding='same', kernel_regularizer=dense_regularizer,kernel_initializer=\"he_normal\"))\n    model.add(BatchNormalization())\n    model.add(Activation('elu'))\n    model.add(Conv2D(256, (3, 3),  padding='same', kernel_regularizer=dense_regularizer,kernel_initializer=\"he_normal\"))\n    model.add(BatchNormalization())\n    model.add(Activation('elu'))\n    model.add(Conv2D(256, (3, 3),  padding='same', kernel_regularizer=dense_regularizer,kernel_initializer=\"he_normal\"))\n    model.add(BatchNormalization())\n    model.add(Activation('elu'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n    model.add(Dropout(0.2))\n\n    # Conv Block 4\n    model.add(Conv2D(512, (3, 3),  padding='same', kernel_regularizer=dense_regularizer,kernel_initializer=\"he_normal\"))\n    model.add(BatchNormalization())\n    model.add(Activation('elu'))\n    model.add(Conv2D(512, (3, 3),  padding='same', kernel_regularizer=dense_regularizer,kernel_initializer=\"he_normal\"))\n    model.add(BatchNormalization())\n    model.add(Activation('elu'))\n    model.add(MaxPooling2D(pool_size=(3, 3), strides=(3, 3)))\n\n    # FC layers\n    model.add(Flatten())\n    model.add(Dense(10, activation='softmax', kernel_regularizer=dense_regularizer,kernel_initializer=\"he_normal\"))\n\n    return model\n\nmodel2 = Model_2()","e7ea299a":"# Define the optimizer for model 2\noptimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n\n# compile model 2\nmodel2.compile(optimizer = optimizer , loss = \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])","4b52aba3":"# define new Y df for use with the new model\nY = train[\"label\"]\nY = np.array(Y)","35d06349":"# Stratified K-Fold\nk_fold = StratifiedKFold(n_splits=10, random_state=12, shuffle=True)\n\nbatch_size = 86\nepochs = 10\n\nfor k_train_index, k_test_index in k_fold.split(X, Y):\n    #model.fit(X[k_train_index,:], Y[k_train_index], epochs=5)\n    model2.fit_generator(datagen.flow(X[k_train_index,:],Y[k_train_index], batch_size=batch_size),\n                              epochs = epochs, validation_data = (X[k_test_index,:],Y[k_test_index]),\n                              verbose = 2, steps_per_epoch=X[k_train_index,:].shape[0] \/\/ batch_size\n                              , callbacks=[learning_rate_reduction])","2db1b1e7":"val_loss, val_acc = model2.evaluate(X, Y)\nval_acc","97a57064":"# predict results\nresults = model2.predict(test)\n\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\n\n# create the submission file\nsubmission3 = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\n# save submission file in output folder\nsubmission3.to_csv(\"sub3.csv\",index=False)","8e1284de":"# Plot confusion matrix\n\nY = train[\"label\"]\nY = to_categorical(Y, num_classes = 10)\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.BuPu):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\nY_pred = model2.predict(X)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10))","66dc81bf":"# Display some error results \n\n# Errors are difference between predicted labels and true labels\nerrors = (Y_pred_classes - Y_true != 0)\n\nY_pred_classes_errors = Y_pred_classes[errors]\nY_pred_errors = Y_pred[errors]\nY_true_errors = Y_true[errors]\nX_val_errors = X[errors]\n\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 2\n    ncols = 3\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)), cmap=plt.get_cmap('Purples'))\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n\n# Probabilities of the wrong predicted numbers\nY_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 6 errors \nmost_important_errors = sorted_dela_errors[-6:]\n\n# Show the top 6 errors\ndisplay_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)","21bf28c7":"These are 6 errors our model makes when it's asked to predict over the whole training dataset. A few of them are incorrect predictions even most humans would likely make and a couple look like incorrectly labelled examples in the training set. These are examples of errors that it's usually unrealistic to expect a machine learning model to eliminate so I think we can be pretty confident that we have a very robust model.","ed8efcc6":"Training for 30 epochs, this strategy achieves a validation score of 0.9926 and gives a leaderboard score of 0.9939 - already pretty good!","a689cdd7":"# Create train and validation sets","5c4de684":"**Reshaping** - The raw datset has 784 pixel-value columns per image. We need to change the shape of the pixel set from a 1 x 784 flat line into a 28 x 28 x 1 square image, where height and width are 28 pixels with a depth of 1.","355f119e":"Plot the first 9 images in the test set.","42ab977d":"# Evaluate the model","b6579efa":"We have a relatively balanced training set so there's no need to re-sample.","df828a1c":"# Strategy 2 - Add data augmentation and a learning rate annealer\n\nNext we'll use the same model but we'll augment the training set by applying random roations up to 10 degrees, random zooms up to x10% and random vertical and horizontal shifts up to x10%.\n\nWe'll also add a learning rate annealer that reduces the learning rate after 3 epochs if the score on the validation set hasn't improved.","4af2aa31":"**Normalization** - Each value in the dataset is a greyscale value between 0 and 255. It's best to normalise the data so that each value is between 0 and 1 before applying any models.","6634ea39":"# Strategy 1 - Simple CNN Architecture\n\nLet's define a relatively simple CNN model. This is a relatively simple architecture: [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out","8330482e":"**One hot encode** the label values (digits from 0-9).","99544004":"Training for 30 epochs, strategy 2 improves validation score to 0.9962 but the leaderboard score drops to 0.99378. We've over-fitted this time.","fa622393":"# Strategy 3 - Build a more complex CNN architecture and train using a k-fold cross validation scheme.\n\nLastly, we'll build a more complicated CNN model and instead of using a simple train \/ validation split, we'll use a k-fold strategy that let's every member of the training set spend some part of the validation set.","6febabad":"Training 10 folds for 10 epochs each, strategy 3 improves validation score to 0.9998. The leaderboard score is 0.99596.","0fb66a65":"# Load and Process Data","76ac23a1":"# Visualizing the Data","8aa0d523":"Do a quick check of how many of each label we have in the training set. An unblanced dataset could cause problems with our model's prediction ability.","bb747ef6":"# Introduction to CNN with MNIST\n\nThis notebook will look at three strategies for solving the MNIST Digit Recognizer computer vision competition. \n\n*Note*: The top scores in this competition are achieved by dowloading the full MNIST dataset from an external source. Using the full dataset as a training set will mean that your model will have seen every image from the competition test set. The scores of 100% accuracy take each image from the test set and just lookup the most similar image from the full dataset. Neither of these approaches would be possible in real life. It seems like the maximum 'non-cheating' score in this competition is around 0.997.\n\nThe three strategies explored in this notebook are:\n\n1. Reltively simple LeNet CNN architecture\n2. LeNet CNN architecture with data augmentation and learning rate annealing\n3. A more complex CNN architecture with data augmentation, learning rate annealing, and k-fold cross validation"}}