{"cell_type":{"01e1d334":"code","4a6b9ded":"code","3c259517":"code","6dba1fcc":"code","59660bc7":"code","a8334cfb":"code","aba151a9":"code","f07662ae":"code","f43c9a43":"code","3fa58cc8":"code","f4aad94e":"code","f8bcbfd6":"code","d2d63d71":"code","879e3c64":"code","ada4b5d3":"code","6de83658":"code","c2340fd5":"code","4e23ec1d":"code","d38137f3":"code","dd294ef4":"code","f2e19517":"code","6f427b31":"markdown","6629994e":"markdown"},"source":{"01e1d334":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split, GridSearchCV,cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale \nfrom sklearn import model_selection\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import BaggingRegressor\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')","4a6b9ded":"# Import data and split it","3c259517":"hit = pd.read_csv(\"..\/input\/hittlers\/Hitters.csv\")\ndf = hit.copy()\ndf = df.dropna()\ndms = pd.get_dummies(df[['League', 'Division', 'NewLeague']])\ny = df[\"Salary\"]\nX_ = df.drop(['Salary', 'League', 'Division', 'NewLeague'], axis=1).astype('float64')\nX = pd.concat([X_, dms[['League_N', 'Division_W', 'NewLeague_N']]], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.25, \n                                                    random_state=42)","6dba1fcc":"#Set and fit the model","59660bc7":"from sklearn.ensemble import GradientBoostingRegressor","a8334cfb":"gbm_model= GradientBoostingRegressor()\ngbm_model.fit(X_train,y_train)","aba151a9":"#Important Parameters\n#learning_rate\n#n_estimators should begin with 1000 for GBM and XG Boost\n#max_depth\n#subsample","f07662ae":"#Prediction","f43c9a43":"y_pred=gbm_model.predict(X_test)","3fa58cc8":"np.sqrt(mean_squared_error(y_pred,y_test))","f4aad94e":"#Model Tuning","f8bcbfd6":"gbm_params = {\n    'learning_rate': [0.001, 0.01, 0.1, 0.2],\n    'max_depth': [3, 5, 8,50,100],\n    'n_estimators': [200, 500, 1000, 2000],\n    'subsample': [1,0.5,0.75],\n}","d2d63d71":"gbm= GradientBoostingRegressor()\ngbm_cv_model= GridSearchCV(gbm,gbm_params,cv=10,n_jobs=-1,verbose=2)  #verbose gives us some information \ngbm_cv_model.fit(X_train,y_train)","879e3c64":"gbm_cv_model.best_params_","ada4b5d3":"gbm_tuned_model= GradientBoostingRegressor(learning_rate= 0.2,\n                                           max_depth= 3,\n                                           n_estimators=500, subsample= 0.75)","6de83658":"gbm_tuned_model.fit(X_train,y_train)","c2340fd5":"y_pred=gbm_tuned_model.predict(X_test)\nnp.sqrt(mean_squared_error(y_pred,y_test))    #test error","4e23ec1d":"# We found 413 for KNN, \n#          367 for SVR,\n#          363 for Artifical Neural Network.\n#          376 for CART\n#          349 for Bagged Trees\n#          350 for Random Forest\n#And now,  344 for GBM\n\n#In these models, the best one is GBM model for \"hitters\" data set, till now.","d38137f3":"# VARIABLES' IMPORTANCE LEVEL  (BONUS PART)","dd294ef4":"Importance = pd.DataFrame({\"Importance\": gbm_tuned_model.feature_importances_*100},\n                         index = X_train.columns)","f2e19517":"Importance.sort_values(by = \"Importance\", \n                       axis = 0, \n                       ascending = True).plot(kind =\"barh\", color = \"r\")\n\nplt.xlabel(\"Variables' Importance Level\")","6f427b31":"#### SPECIAL THANKS TO https:\/\/github.com\/mvahit\/DSMLBC","6629994e":"# GBM STEPS\n\n### 1)Import and split\n### 2)Set and fit the model\n### 3)Predict\n### 4)Model Tuning\n### 5)Find best params, set and fit the model again, find final RMSE."}}