{"cell_type":{"727c759c":"code","4b1bffd6":"code","96a5786f":"code","74b7946c":"code","a89a3d5a":"code","11076699":"code","1e29504b":"code","c5167402":"code","18e0b2d1":"code","aa7c6ea3":"code","0ae93f1f":"code","c41ee9b3":"code","093552e8":"code","3ade9d33":"code","c25df9d6":"code","ac6b5c12":"code","ac31da80":"code","86d786d5":"code","b7a5f42a":"code","ff25cc6c":"code","17fa4a3c":"code","de951c36":"code","9bfa1072":"code","7113dbdb":"code","5a2bb5ef":"code","1d54a1a9":"markdown","964d3e3a":"markdown","61f62b30":"markdown","88466935":"markdown","ad09606b":"markdown","79b85f05":"markdown","932a2d95":"markdown","680877fc":"markdown","66a23d2f":"markdown","998525b4":"markdown","5dae0fa0":"markdown","49ded8a6":"markdown","66c2b3cc":"markdown","4a43208f":"markdown","c92129e8":"markdown","24d09b8f":"markdown","a906ce2c":"markdown","bdca8504":"markdown"},"source":{"727c759c":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport gc\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint","4b1bffd6":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n \n    return df","96a5786f":"train = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv')\npl_df = pd.read_csv('..\/input\/tps12-pseudolabels\/tps12-pseudolabels_v1.csv')\n\ntrain = pd.concat([train, pl_df], axis=0).reset_index(drop = True)\ntrain","74b7946c":"del pl_df\ngc.collect()","a89a3d5a":"train = train.drop(columns = ['Id', 'Soil_Type7', 'Soil_Type15'])\ntest = test.drop(columns = ['Id', 'Soil_Type7', 'Soil_Type15'])","11076699":"print('<----------Value Count of Missing Values in Train Data---------->\\n', train.isna().sum())\nprint('<----------Value Count of Missing Values in Test Data---------->\\n', test.isna().sum())","1e29504b":"train['Cover_Type'].value_counts()","c5167402":"train = train.drop(index = train[train['Cover_Type'] == 5].index).reset_index(drop = True)\ntrain['Cover_Type'].value_counts()","18e0b2d1":"train[\"Aspect\"][train[\"Aspect\"] < 0] += 360\ntrain[\"Aspect\"][train[\"Aspect\"] > 359] -= 360\n\ntest[\"Aspect\"][test[\"Aspect\"] < 0] += 360\ntest[\"Aspect\"][test[\"Aspect\"] > 359] -= 360","aa7c6ea3":"train.loc[train[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\ntest.loc[test[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n\ntrain.loc[train[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\ntest.loc[test[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n\ntrain.loc[train[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\ntest.loc[test[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n\ntrain.loc[train[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\ntest.loc[test[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n\ntrain.loc[train[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\ntest.loc[test[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n\ntrain.loc[train[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\ntest.loc[test[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255","0ae93f1f":"train['Sum_Hydrology'] = np.abs(train['Horizontal_Distance_To_Hydrology']) + np.abs(train['Vertical_Distance_To_Hydrology'])\ntrain['Sub_Hydrology'] = np.abs(train['Horizontal_Distance_To_Hydrology']) - np.abs(train['Vertical_Distance_To_Hydrology'])\n\ntest['Sum_Hydrology'] = np.abs(test['Horizontal_Distance_To_Hydrology']) + np.abs(test['Vertical_Distance_To_Hydrology'])\ntest['Sub_Hydrology'] = np.abs(test['Horizontal_Distance_To_Hydrology']) - np.abs(test['Vertical_Distance_To_Hydrology'])","c41ee9b3":"from sklearn.preprocessing import LabelEncoder\n\n\nle = LabelEncoder()\ny = le.fit_transform(train['Cover_Type'])\n\ngc.collect()","093552e8":"train = train.drop(columns = ['Cover_Type'])\n\ncols = train.columns\n\n# Scaling\nrb = RobustScaler()\n\ntrain[cols] = rb.fit_transform(train[cols].values)\ntest[cols] = rb.transform(test[cols].values)","3ade9d33":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)\ntrain = train.values\ntest = test.values\ngc.collect()","c25df9d6":"from tensorflow.keras.utils import to_categorical\n\ntarget = to_categorical(y)","ac6b5c12":"def scaled_dot_product_attention(q, k, v, mask):\n  matmul_qk = tf.matmul(q, k, transpose_b=True)\n\n  # scale matmul_qk\n  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n  scaled_attention_logits = matmul_qk \/ tf.math.sqrt(dk)\n    \n  if mask is not None:\n    scaled_attention_logits += (mask * -1e9)\n\n  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n\n  output = tf.matmul(attention_weights, v)\n\n  return output, attention_weights","ac31da80":"class MultiHeadAttention(tf.keras.layers.Layer):\n  def __init__(self, d_model, num_heads):\n    super(MultiHeadAttention, self).__init__()\n    self.num_heads = num_heads\n    self.d_model = d_model\n\n    assert d_model % self.num_heads == 0\n\n    self.depth = d_model \/\/ self.num_heads\n\n    self.wq = tf.keras.layers.Dense(d_model)\n    self.wk = tf.keras.layers.Dense(d_model)\n    self.wv = tf.keras.layers.Dense(d_model)\n\n    self.dense = tf.keras.layers.Dense(d_model)\n\n  def split_heads(self, x):\n    \"\"\"Split the last dimension into (num_heads, depth).\n    Transpose the result such that the shape is (num_heads, seq_len, depth)\n    \"\"\"\n    x = tf.reshape(x, (-1, self.num_heads, self.depth))\n    return tf.transpose(x, perm=[0, 2, 1])\n\n  def call(self, v, k, q, mask):\n    batch_size = tf.shape(q)[0]\n\n    q = self.wq(q)  # (batch_size, seq_len, d_model)\n    k = self.wk(k)  # (batch_size, seq_len, d_model)\n    v = self.wv(v)  # (batch_size, seq_len, d_model)\n\n    q = self.split_heads(q)  # (batch_size, num_heads, seq_len_q, depth)\n    k = self.split_heads(k)  # (batch_size, num_heads, seq_len_k, depth)\n    v = self.split_heads(v)  # (batch_size, num_heads, seq_len_v, depth)\n\n    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n    scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n\n    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1])  # (batch_size, seq_len_q, num_heads, depth)\n\n    concat_attention = tf.reshape(scaled_attention,\n                                  (-1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n    return output, attention_weights","86d786d5":"def point_wise_feed_forward_network(d_model, dff):\n  return tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n  ])","b7a5f42a":"class EncoderLayer(tf.keras.layers.Layer):\n  def __init__(self, d_model, num_heads, dff, rate=0.1):\n    super(EncoderLayer, self).__init__()\n\n    self.mha = MultiHeadAttention(d_model, num_heads)\n    self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n    self.dropout1 = tf.keras.layers.Dropout(rate)\n    self.dropout2 = tf.keras.layers.Dropout(rate)\n\n  def call(self, x, training, mask = None):\n\n    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n    attn_output = self.dropout1(attn_output, training=training)\n    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n\n    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n    ffn_output = self.dropout2(ffn_output, training=training)\n    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n\n    return out2","ff25cc6c":"d_model = 54 # Embedding Dimension of our data\ndropout_rate = 0.1","17fa4a3c":"def get_model():\n    inputs = tf.keras.layers.Input(shape = (54))\n\n    x = EncoderLayer(d_model, 6, 512, dropout_rate)(inputs)\n    x = EncoderLayer(d_model, 6, 256, dropout_rate)(x)\n    x = EncoderLayer(d_model, 6, 128, dropout_rate)(x)\n    x = EncoderLayer(d_model, 6, 32, dropout_rate)(x)\n    \n    outputs = tf.keras.layers.Dense(6, activation = 'softmax')(x)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"encoder\")\n    \n    return model","de951c36":"model = get_model()\ntf.keras.utils.plot_model(model, show_shapes = True)","9bfa1072":"EPOCH = 2\nBATCH_SIZE = 256\nNUM_FOLDS = 5\n\nkf = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state=2021)\ntest_preds = []\n\nfor fold, (train_idx, test_idx) in enumerate(kf.split(train, y)):\n    print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n\n    checkpoint_filepath = f\"folds{fold}.hdf5\"\n    X_train, X_valid = train[train_idx], train[test_idx]\n    y_train, y_valid = target[train_idx], target[test_idx]\n\n    model = get_model()\n    model.compile(optimizer = \"adam\",\n                  loss = \"categorical_crossentropy\",\n                  metrics = ['accuracy'])\n\n    lr = ReduceLROnPlateau(monitor = \"val_loss\",\n                           factor = 0.5,\n                           patience = 1,\n                           verbose = 1)\n\n    es = EarlyStopping(monitor = \"val_loss\",\n                       patience = 2,\n                       verbose = 1,\n                       restore_best_weights = True)\n\n    sv = ModelCheckpoint(checkpoint_filepath,\n                         monitor = 'val_loss',\n                         verbose = 1,\n                         save_best_only = True,\n                         save_weights_only = True,\n                         mode = 'auto',\n                         save_freq = 'epoch',\n                         options = None)\n\n    model.fit(X_train,\n              y_train,\n              validation_data = (X_valid, y_valid),\n              epochs = EPOCH,\n              batch_size = BATCH_SIZE,\n              callbacks = [lr, es, sv])\n\n    test_preds.append(model.predict(test))\n\n    del X_train, X_valid, y_train, y_valid, model\n    gc.collect()","7113dbdb":"sub = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv')\nsub['Cover_Type'] = le.inverse_transform(np.argmax(np.array(test_preds).sum(axis = 0), axis = 1))\nsub","5a2bb5ef":"sub.to_csv('sub.csv', index = 0)","1d54a1a9":"## **Model Builder**","964d3e3a":"# **Modeling**","61f62b30":"## **Check Missing Values**\n\n* **Looking good!**","88466935":"# **Memory Reduce Func**\n\n* **If you don't use some memory reducing strategy, you can face some OOM**","ad09606b":"# **Scaling**\n\n* **Robust Scaling**","79b85f05":"* **Soil_Type7 & Soil_Type15 \u2192 useless**","932a2d95":"## **Aspect**\n\n* **Aspect means angle. Good for rescaling**\n* **Hillshade needs to rescale to 0 ~ 255**\n\nFrom [gulshanmishra Kernel](https:\/\/www.kaggle.com\/gulshanmishra\/tps-dec-21-tensorflow-nn-feature-engineering)\n\nThank you for sharing nice notebook :)","680877fc":"# **Target Encoding**\n\n### **Need to use inverse_transform at the end for submission**","66a23d2f":"## **Feed Forward Net**","998525b4":"# **Pseudolabeling**\n\n* **[reference](https:\/\/www.kaggle.com\/remekkinas\/tps-12-nn-tpu-pseudolabeling-0-95661\/notebook)**","5dae0fa0":"# **Training**\n\n### **Pseudolabeling using [public data](https:\/\/www.kaggle.com\/remekkinas\/tps12-pseudolabels?select=tps12-pseudolabels_v1.csv)**\n#### **Thanks for sharing Remek Kinas!**\n\n#### **You can increase the EPOCH for better result**","49ded8a6":"## **Encoder Block**","66c2b3cc":"[](https:\/\/data-science-blog.com\/wp-content\/uploads\/2022\/01\/mha_visualization-930x1030.png)","4a43208f":"## **Check Target Column**\n\n* **There is only one row of class5! We need to drop this**","c92129e8":"## **Multi-Head Attention**\n\n* **Removed Batch Size (No Time Step here)**","24d09b8f":"# **TPS Dec - Multi Head Attention**\n\n* This notebook is based on [tensorflow homepage](https:\/\/www.tensorflow.org\/)","a906ce2c":"## **Interaction Features**\n\n* **Sum of Hydrology**\n* **Subtraction of Hydrology**","bdca8504":"# **Feature Engineering**\n\n* **Check Missing Values**\n* **Check Target Column**"}}