{"cell_type":{"e9698412":"code","85632b36":"code","59b3be67":"code","59d0ea03":"code","e58b2089":"code","cd5fb71d":"code","28d959fe":"code","6c0f002c":"code","7f9840f4":"code","e2d64e93":"code","f8a530e8":"code","1e97c220":"code","d5d680cf":"code","dcabad0b":"code","ff2be910":"code","f99f2f4c":"code","82390939":"code","b83ee5de":"code","51cefec2":"code","3167fe0b":"code","122433e9":"code","8506ade5":"markdown","8a489c13":"markdown","972af146":"markdown","f29c2adc":"markdown","4c336257":"markdown","bbad285c":"markdown","6f0a5f13":"markdown","75583a0f":"markdown","41358905":"markdown","c6830873":"markdown","9ad647c3":"markdown","e6157253":"markdown","b9b81af9":"markdown","50f061ba":"markdown","c74b9c27":"markdown","d8f9f247":"markdown","48d9f77f":"markdown","547f529f":"markdown","1f970258":"markdown"},"source":{"e9698412":"import matplotlib.pyplot as plt\nimport wordcloud\nimport logging\nimport collections\nimport re\nimport nltk\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\n\n\nlogger = logging.getLogger('FraudEmails')\n\n#Minimum occurrence of a word to appear in the WordCloud\nfrequency = 100\n#Number of words in WordCloud\nnumb_of_words = 400","85632b36":"#Open and convert file to string (ignore strange characters)\ntry:\n    with open('..\/input\/fraudulent-email-corpus\/fradulent_emails.txt','r',encoding='utf-8',errors='ignore') as file:\n        text = file.read()\nexcept Exception as e:\n    logger.error('Process failed with error: '+repr(e))\nfinally:\n    file.close()\n\n","59b3be67":"#Delete everything between From r [.*?] Status: ?O\nemails = re.sub('From.*?Status: ?O','',text,flags=re.DOTALL)","59d0ea03":"#Convert to lower case\nemails = emails.lower()","e58b2089":"#Tokenize into string and remove stop words (the, a, on...)\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntokens = tokenizer.tokenize(emails)\nemailWordList = list(filter(lambda word: word not in nltk.corpus.stopwords.words('english'), tokens))","cd5fb71d":"#Remove anything less than two characters\nemailWordList = list(filter(lambda word: len(word)>2, emailWordList))","28d959fe":"#HTML tags modified\nhtmldf = pd.read_csv('..\/input\/html-tags\/html_tags.txt', sep='\\t').dropna()\nhtmldf['Tag'] = htmldf['Tag'].str.strip()\nhtmldf.head()","6c0f002c":"#Remove HTML tags\nfinalEmailFormat = [word for word in emailWordList if '<'+word.strip()+'>' not in htmldf['Tag'].to_list()]","7f9840f4":"#Correspond every entry to natural language tag\ntags = nltk.pos_tag(finalEmailFormat)","e2d64e93":"#Every word dataframe\ndf = pd.DataFrame(tags, columns=['word','tag'])\ndf.head()","f8a530e8":"#Verb dataframe\ndfverb = df[df['tag']=='VB']\ndfverb = dfverb.word.value_counts().reset_index()\ndfverb = dfverb[dfverb['word']>frequency]\ndfverb.head()","1e97c220":"#Noun dataframe\ndfnoun = df[df['tag']=='NN']\ndfnoun = dfnoun.word.value_counts().reset_index()\ndfnoun = dfnoun[dfnoun['word']>frequency]\ndfnoun.head()","d5d680cf":"#Adjective dataframe\ndfadj = df[df['tag']=='JJ']\ndfadj = dfadj.word.value_counts().reset_index()\ndfadj = dfadj[dfadj['word']>frequency]\ndfadj.head()","dcabad0b":"#Noun plural dataframe\ndfnounpl = df[df['tag']=='NNS']\ndfnounpl = dfnounpl.word.value_counts().reset_index()\ndfnounpl = dfnounpl[dfnounpl['word']>frequency]\ndfnounpl.head()","ff2be910":"#Keep VB (verb), NN (singular nouns), JJ (adjectives), NNS (plural nouns)\ntoKeep = ['VB','NN','JJ','NNS']\n\n#Get only tag if tag is associated with above list\nforWordCloud = [tag[0] for tag in tags if tag[1] in toKeep]\n","f99f2f4c":"#Create dictionary with count of each entry\nscamDict = collections.Counter(forWordCloud)\n","82390939":"#Remove website words\ntoRemove = ['nbsp','http','charset','iso','html','www']\n\nfor removal in toRemove:\n    del scamDict[removal]","b83ee5de":"#Remove less than 'frequency' instances\nscamDict = {k: v for k, v in scamDict.items() if v > frequency}\n","51cefec2":"#Open mask and create word cloud\nafricaMask = np.array(Image.open('..\/input\/africaoutline\/Africa-outline.jpg'))\nscamCloud = wordcloud.WordCloud(width=1600,height=1600,\n                                max_words=numb_of_words,mask=africaMask,\n                                contour_width=1,contour_color='blue').generate_from_frequencies(scamDict)\n","3167fe0b":"#Create image\nplt.figure(figsize=(30,30))\nplt.imshow(scamCloud)\nplt.axis('off')\nplt.savefig('.\/ScamWordCloud.png',bbox_inches='tight',pad_inches=0,dpi=133)\nplt.show()","122433e9":"#Output formatted word frequency\noutputformatdf = pd.DataFrame.from_dict(scamDict, orient='index').sort_values(by=[0],ascending=False)\noutputformatdf.to_csv('.\/WordFrequency_Processed.csv',header=False)\noutputformatdf.head()","8506ade5":"Every entry in the list will now be corresponded with an `nltk` tag. The full list of possible tags can be found [here]( https:\/\/www.ling.upenn.edu\/courses\/Fall_2003\/ling001\/penn_treebank_pos.html).","8a489c13":"## Conclusion\n\nAnalyzing the corpus was a nice challenge! It was my first experience in using natural language processing which is so useful. In my first few commits of this notebook, my `toRemove` list was around 100 entries long! Initially there was a lot of manual work in getting the final output and so switching over to using `nltk` saved a lot of time and effort.\n\nThe Github repository for this project can be found [here](https:\/\/github.com\/JSemelhago\/FraudEmailAnalysis.git). There, I tried a crack at creating an LSTM (long short-term memory, a type of recurrent neural network) unit. Unfortunately, I do not have the architecture to train a neural net of this size, especially at a higher number of epochs, but it was nice to get a preliminary understanding of `keras` and `tensorflow`.\n","972af146":"### Output\n\n`scamDict` now contains the final format for our word cloud (which uses the `wordcloud` package). First, I create a mask using a high resolution outline of Africa (which contains regions that are the most referred to in the corpus).","f29c2adc":"Using list comprehension again, we will create the matrix of words and their counts for the final word cloud.","4c336257":"In our first instance of using natural language processing, we use the `nltk` package to remove **stop words** which are the most common words of a language. These include *the, a, on, which...*.","bbad285c":"I will now create multiple dataframes based off of the following tags (producing the more interesting words):\n\n1. VB = Verb\n2. NN = Noun\n3. JJ = Adjective\n4. NNS = Noun (plural)\n\nThe dataframes will all have a count of how many times a word is present and keep it in the dataframe if it is above the given `frequency` variable.","6f0a5f13":"And using list comprehension, I surround every word with `<>` and check if it's in the HTML tag dataframe.","75583a0f":"### Formatting the Corpus\n\nTaking a look at corpus, the general format alternates between the metadata of the email and the body of the email. All the metadata comes in the format as `From...Status:` so using regular expression, we can delete all the metadata and keep the email bodies.","41358905":"There were a lot of HTML tags that appeared, but I couldn't use the `html.parser` module which can detect tags given HTML data. To counter this, I copied a table from [w3schools](https:\/\/www.w3schools.com\/TAGS\/default.ASP) which contains a list of HTML tags.","c6830873":"## Analyzing the Code\n\nI leave it up to the user as to how many words they want to see in the final word cloud and what the frequency of each word is. Keep in mind that decreasing the frequency and\/or increasing the number of words may result in strange words being picked up in the final word cloud.","9ad647c3":"# Word Cloud of Scam Emails","e6157253":"I also outputted what words were used in the word cloud just for reference.","b9b81af9":"Using `collections`, a dictionary is created from list matrix above.","50f061ba":"To get an idea of what the word-tag structure looks like, I created a dataframe of it.","c74b9c27":"I decided to removed words that are less than two characters. Single characters existed during the splitting process (i.e. `e-mail`->`[e, mail]`). Two character words weren't too much of a problem but a few instances came up of words such as `co` and `go` being overrepresented - words that I feel wouldn't contribute much to the word cloud.","d8f9f247":"There were some characters throwing errors that should be ignored. ","48d9f77f":"From a quick glance of the above dataframes, there were a few anomalies that weren't quite caught in my stages of formatting. Thankfully, they were pretty easy to catch and are all components of a website URL which was easy to notice. ","547f529f":"Then I attribute it to a `matplotlib` plot which can be plotted.","1f970258":"## Introduction\n\nThe purpose of this notebook is to format the fraudulent email corpus into a visual representation using natural language processing.\n\nThe first thing that came to mind was to use a word cloud but this requires some formatting of the corpus.\n"}}