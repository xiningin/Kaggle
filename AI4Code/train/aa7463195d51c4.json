{"cell_type":{"f0448547":"code","fba75b10":"code","11220d03":"code","ead2b098":"code","a996cd42":"code","d1b257f3":"code","d6f054cf":"code","57b3d6bb":"code","cb871ddc":"code","11642195":"code","9030b542":"code","67bffc66":"code","b133810e":"code","231121e9":"code","22fc7779":"code","abaf968d":"code","1447e334":"code","837a647a":"code","3d543dd1":"code","7cc26933":"code","7f6a88a2":"code","e037ef38":"code","7b04e9b9":"code","4fca3c68":"code","3d9388e2":"code","e57cbe1c":"code","4859cdda":"code","af2bec49":"code","d4bf59fc":"markdown","31cb955c":"markdown","9a7d88e3":"markdown","3c833bc1":"markdown","97d83095":"markdown","c9ae92e6":"markdown","9fbd6629":"markdown","4e3bfad0":"markdown","73a99a0d":"markdown","647f2796":"markdown","0797e457":"markdown","3d7c64f5":"markdown","2b19df03":"markdown","c29aa35e":"markdown","2a2b160d":"markdown","14e80549":"markdown","80b30150":"markdown"},"source":{"f0448547":"import pandas as pd\nimport torch\nimport transformers\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom transformers import DistilBertModel, DistilBertTokenizer\nfrom sklearn import metrics\nfrom tqdm import tqdm\nimport numpy as np\nimport re","fba75b10":"# Setting up the device for GPU usage\n\nfrom torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'","11220d03":"df_train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")[[\"text\",\"target\"]]","ead2b098":"df_train.head()","a996cd42":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_numbers(text):\n    text = ''.join([i for i in text if not i.isdigit()])         \n    return text\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_username(text):\n    url = re.compile(r'@[A-Za-z0-9_]+')\n    return url.sub(r'',text)","d1b257f3":"def pre_process_text(text):\n    text = remove_URL(text)\n    text = remove_numbers(text)\n    text = remove_html(text)\n    text = remove_username(text)\n    return \" \".join(text.split())","d6f054cf":"pre_process_text(\"Why can't gay men donate blood? http:\/\/t.co\/v2Etl8P9eQ http:\/\/t.co\/NLnyzeljbw\")","57b3d6bb":"# longest tweet length\nmax(df_train['text'].apply(len))","cb871ddc":"# Defining some key variables that will be used later on in the training\nMAX_LEN = 160\nBATCH_SIZE = 16\nEPOCHS = 1\nLEARNING_RATE = 1e-05\n\nBERT_PATH = '\/kaggle\/input\/distillbert-huggingface-model\/'\nMODEL_PATH = \"distilbert-base-uncased-pytorch_model.bin\"\ntokenizer = DistilBertTokenizer.from_pretrained(\n    BERT_PATH,\n    do_lower_case=True\n)","11642195":"class tweet_Dataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __getitem__(self, index):\n        tweet = str(self.data.text[index])\n        tweet = pre_process_text(tweet)\n        inputs = self.tokenizer.encode_plus(\n            tweet,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'targets': torch.tensor(self.data.target[index], dtype=torch.float)\n        }\n        \n    def __len__(self):\n        return len(self.data)","9030b542":"# Creating the dataset and dataloader for the neural network\n\ntrain_size = 0.85\ntrain_dataset=df_train.sample(frac=train_size,random_state=200).reset_index(drop=True)\nvalid_dataset=df_train.drop(train_dataset.index).reset_index(drop=True)\n\n\nprint(\"FULL Dataset: {}\".format(df_train.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"VALID Dataset: {}\".format(valid_dataset.shape))\n\ntraining_set = tweet_Dataset(train_dataset, tokenizer, MAX_LEN)\nvalidation_set = tweet_Dataset(valid_dataset, tokenizer, MAX_LEN)","67bffc66":"training_set[0]","b133810e":"train_params = {'batch_size': BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\nvalid_params = {'batch_size': BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntrain_dl = DataLoader(training_set, **train_params)\nvalid_dl = DataLoader(validation_set, **valid_params)","231121e9":"class DistillBERTClass(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.distill_bert = transformers.DistilBertModel.from_pretrained(BERT_PATH)\n        self.drop = torch.nn.Dropout(0.3)\n        self.out = torch.nn.Linear(768, 1)\n    \n    def forward(self, ids, mask):\n        distilbert_output = self.distill_bert(ids, mask)\n        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n        pooled_output = hidden_state[:, 0]  # (bs, dim)\n        output_1 = self.drop(pooled_output)\n        output = self.out(output_1)\n        return output","22fc7779":"model = DistillBERTClass()\nmodel.to(device)","abaf968d":"def loss_fn(outputs, targets):\n    return nn.BCEWithLogitsLoss()(outputs, targets)\noptimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)","1447e334":"def eval_fn(data_loader, model):\n    model.eval()\n    fin_targets = []\n    fin_outputs = []\n    with torch.no_grad():\n        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n            ids = d[\"ids\"]\n            mask = d[\"mask\"]\n            targets = d[\"targets\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n\n            outputs = model(ids=ids, mask=mask)\n            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n        fin_outputs = np.array(fin_outputs) >= 0.5\n        f1 = metrics.f1_score(fin_targets, fin_outputs)\n    return f1","837a647a":"def fit(num_epochs, model, loss_fn, opt, train_dl, valid_dl):\n    \n    for epoch in range(num_epochs):\n        model.train()\n        for _,data in enumerate(train_dl, 0):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            targets = data['targets'].to(device, dtype = torch.float)\n            outputs = model(ids, mask).squeeze()\n            loss = loss_fn(outputs, targets)\n            loss.backward()\n            opt.step()\n            opt.zero_grad()\n\n        valid_acc = eval_fn(valid_dl, model)\n        print('Epoch [{}\/{}], Train Loss: {:.4f} and Validation f1 {:.4f}'.format(epoch+1, num_epochs, loss.item(),valid_acc))","3d543dd1":"fit(7, model, loss_fn, optimizer, train_dl,valid_dl)","7cc26933":"def sentence_prediction(sentence):\n    max_len = MAX_LEN\n    tweet = str(sentence)\n    tweet = \" \".join(tweet.split())\n    inputs = tokenizer.encode_plus(\n            tweet,\n            None,\n            add_special_tokens=True,\n            max_length=max_len,\n            pad_to_max_length=True,\n        )\n\n    ids = inputs[\"input_ids\"]\n    mask = inputs[\"attention_mask\"]\n\n\n    ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n    mask = torch.tensor(mask, dtype=torch.long).unsqueeze(0)\n\n    ids = ids.to(device, dtype=torch.long)\n    mask = mask.to(device, dtype=torch.long)\n\n    outputs = model(ids=ids, mask=mask)\n\n    outputs = torch.sigmoid(outputs).cpu().detach().numpy()\n    return outputs[0][0] > 0.5","7f6a88a2":"sentence_prediction(\"We are experiencing slight tremors in London right now\")","e037ef38":"sentence_prediction(\"Lol this movie is an absolute disaster!\")","7b04e9b9":"df_test = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")[[\"id\",\"text\"]]","4fca3c68":"df_test.head()","3d9388e2":"df_test['target'] = df_test['text'].apply(sentence_prediction)","e57cbe1c":"df_test['target'] = df_test['target'].astype(int)","4859cdda":"df_test.head()","af2bec49":"# submission\ndf_test.to_csv(\"submission.csv\", columns = ['id','target'], index=False)","d4bf59fc":"### Pytorch module class for our model","31cb955c":"Bert Encoder(pooled output) -> Dropout -> Linear layer -> Final Output","9a7d88e3":"A few pre-processing functions that will be used to clean up the tweets while we create the PyTorch dataset object.","3c833bc1":"### Evaluation function","97d83095":"### Prediction on test set","c9ae92e6":"### Training function","9fbd6629":"The longest tweet is 157, for the bert input length we'll take that into consideration.","4e3bfad0":"### Class for creating the dataset.","73a99a0d":"In this notebook I use distill-bert-uncased from huggingface and use pytorch to fine-tune it to the classification task at hand.","647f2796":"### Importing requirements","0797e457":"### Reading in test set","3d7c64f5":"Reading in the train dataframe","2b19df03":"### Saving submission file","c29aa35e":"### Function to predict on a new tweet","2a2b160d":"### Pre-processing \/ Cleaning functions","14e80549":"An example of how a tweet gets cleaned","80b30150":"### Training for 7 epochs."}}