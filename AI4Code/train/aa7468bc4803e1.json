{"cell_type":{"7fd72dea":"code","4877af44":"code","74040689":"code","589c95b8":"code","d10fb9a9":"code","9e8489d1":"code","468d7875":"code","5aedee9c":"code","8b510618":"code","52c1164a":"code","9e7aa8ef":"code","27b6622e":"code","64011fd3":"markdown","59713dd7":"markdown","ae164233":"markdown","e714e67b":"markdown","6f7ffe33":"markdown","2f9ec899":"markdown","414a6873":"markdown","38df76bf":"markdown","0c832bce":"markdown","3870126d":"markdown","84da8cf6":"markdown","d9fa5eff":"markdown"},"source":{"7fd72dea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4877af44":"data = pd.read_csv('..\/input\/column_2C_weka.csv')\ndata.head()","74040689":"# As you can see there is no labels in data\nx = data['pelvic_radius']\ny = data['degree_spondylolisthesis']\nplt.figure(figsize=(13,5))\nplt.scatter(x,y)\nplt.xlabel('pelvic_radius')\nplt.ylabel('degree_spondylolisthesis')\nplt.show()","589c95b8":"df = data.loc[:, ['degree_spondylolisthesis', 'pelvic_radius']]\ndf.head()","d10fb9a9":"# which k value to choose\nfrom sklearn.cluster import KMeans\nwcss = []\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(df)\n    wcss.append(kmeans.inertia_) # kmeans.inertia : calculate wcss\n    \nplt.plot(range(1,15), wcss, '-o')\nplt.xlabel('number of k (cluster) value')\nplt.ylabel('wcss')\nplt.show()","9e8489d1":"# for k=2, lets write KMeans\nfrom sklearn.cluster import KMeans\nkmeans2 = KMeans(n_clusters = 2)\nclusters =kmeans2.fit_predict(df) # fit first and then predict\n\n# add labels for df\ndf['label'] = clusters","468d7875":"# plot\nplt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'],c = clusters)\nplt.xlabel('pelvic_radius')\nplt.xlabel('degree_spondylolisthesis')\nplt.show()","5aedee9c":"# if we choose k=3\nfrom sklearn.cluster import KMeans\nkmeans3 = KMeans(n_clusters = 3)\nclusters =kmeans3.fit_predict(df) # fit first and then predict\n\n# add labels for df\ndf['label'] = clusters\n\n# plot\nplt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'],c = clusters)\nplt.xlabel('pelvic_radius')\nplt.xlabel('degree_spondylolisthesis')\nplt.show()","8b510618":"# if we choose k=4\nfrom sklearn.cluster import KMeans\nkmeans4 = KMeans(n_clusters = 4)\nclusters =kmeans4.fit_predict(df) # fit first and then predict\n\n# add labels for df\ndf['label'] = clusters\n\n# plot\nplt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'],c = clusters)\nplt.xlabel('pelvic_radius')\nplt.xlabel('degree_spondylolisthesis')\nplt.show()","52c1164a":"# plot\ncolors = [0 if i=='Abnormal' else 1 for i in data['class']] # to create colors\nplt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'],c = colors)\nplt.xlabel('pelvic_radius')\nplt.ylabel('degree_spondylolisthesis')\nplt.show()","9e7aa8ef":"# DENDOGRAM \n# here we will try to predict how many clusters we have \nfrom scipy.cluster.hierarchy import linkage, dendrogram # linkage: create dendrogram\ndf1 = data.loc[:, ['pelvic_radius', 'degree_spondylolisthesis']]\nmerg = linkage(df1, method='ward') # ward: cluster icindeki yayilimlari minimize et (wcss gibi bisey)\ndendrogram(merg, leaf_rotation=90)\nplt.xlabel('data points')\nplt.ylabel('euclidian distance')\nplt.show()","27b6622e":"from sklearn.cluster import AgglomerativeClustering\n\nhierarcical_cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\ncluster = hierarcical_cluster.fit_predict(df1)\n\n# add label for df1\ndf1['label'] = cluster\n\n#plot\nplt.scatter(df1['pelvic_radius'],df1['degree_spondylolisthesis'],c = cluster)\nplt.xlabel('pelvic_radius')\nplt.ylabel('degree_spondylolisthesis')\nplt.show()\n","64011fd3":"**Lets find optimum k value**","59713dd7":"**1. K Means Algorithm**\n\n1.  We choose a k value \n2. Then it is randomly created k centroids\n3. Every data point is clusterd according to the nearest centroid\n4. By taking average of all data points that belog to a centroid, it is created new centroids.\n5. Using these new centroids repeat 3 and 4\n6. Finally, when centroids remain stationary, the algorith stops there.\n7. As a result, according to these centroids, data is clustered","ae164233":"**Original data is as follow**","e714e67b":"**for k=4**","6f7ffe33":"**How k value is selected**\n\n1. For k=1, run KMeans algorithm\n2. For each cluster (k cluster we have), it is calculated WCSS (within cluster sum of squares) value\n3. repeat 1 and 2 for 1<k<15\n4. obtain k vs WCSS plot\n5. Using elbow rule,  choose the optimum k value to be used in K Means Algorithm","2f9ec899":"**Unsupervised Learning:** We have data that has hidden labels and our aim is to find these labels of the data","414a6873":"**2. Hierarcical Clustering**\n\n1. Assign each data point as a cluster\n2. Create a new cluster by choosing the closest two clusters  \n3. repeat 2 until it remains only one cluster","38df76bf":"* vertical lines are clusters\n* height on dendogram: distance between merging cluster\n* method= 'single' : closest points of clusters\n* we are going to choose the highest distance between merging clusters, which are not cut by horizontal line\n* this suggest that choose 3 clusters (draw a horizontal line approx from euc. distance=400, it cuts at 3 points)","0c832bce":"using elbow rule we can select k=2, 3 or 4 (the elbow point is not quite obvious here)","3870126d":"**for k=2**","84da8cf6":"**CONCLUSION**\n\n* We wee that the predictions that we made using Kmeans algorihm does not suit well for this problem\n* But for Hierarcical Clustering, we have predicted the original data better as compared to the Kmeans. ","d9fa5eff":"**for k=3**"}}