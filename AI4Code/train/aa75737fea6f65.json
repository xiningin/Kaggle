{"cell_type":{"91103140":"code","9ec576d9":"code","28680804":"code","c7a2d14b":"code","fe50dea9":"code","2b61a715":"code","16aa44ff":"code","c28b96fe":"code","655ed452":"code","583d4e7b":"code","09e6d893":"code","7a8385e4":"code","33e31578":"code","96daacc3":"code","669536d6":"code","e7cf09de":"code","e08f35a8":"code","903eaa2a":"code","e774d6e8":"code","a16de348":"code","57b2d6c9":"code","f2875706":"code","3e1d3796":"code","53a62671":"code","f1207389":"code","5d34e54d":"code","1b533719":"code","22ebb640":"code","0ec28309":"code","fc6d5a47":"code","422c3305":"code","e73ff567":"markdown","9c76ff92":"markdown","1591a8ba":"markdown","61ed47ad":"markdown","913184f2":"markdown","e56b20ff":"markdown","bad09219":"markdown","e6de11aa":"markdown","7fbc6b2e":"markdown","230169fc":"markdown","d1f3294d":"markdown","2783f693":"markdown","1a8377bf":"markdown","a0de483f":"markdown","bcf6462e":"markdown","b1d9728f":"markdown","570e795e":"markdown","f67661f2":"markdown","38bd7133":"markdown","5e4205ce":"markdown","9bf7b0fa":"markdown","1f9e6ba0":"markdown","ff5b9cff":"markdown"},"source":{"91103140":"import pandas as pd\n#gender_submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n\ntrain_y = train['Survived']\ntrain_x = train.drop('Survived', axis=1)","9ec576d9":"import numpy as np\n\ndef fillna_age(dataframe):    \n    for index, df in dataframe.iterrows():\n        if pd.isna( df.Age ):\n            if 'Miss' in df[\"Name\"]:\n                dataframe.at[index,\"Age\"] = 17\n            elif 'Mrs' in  df[\"Name\"]:\n                dataframe.at[index, \"Age\"]= 25\n            elif 'Master' in  df[\"Name\"]:\n                dataframe.at[index, \"Age\"]= 4\n            elif 'Mr' in  df[\"Name\"]:\n                dataframe.at[index, \"Age\"]= 41\n            else :\n                dataframe.at[index, \"Age\"]=  dataframe[\"Age\"].median()\n\n    return dataframe\n\ndef fillna_fare(dataframe):    \n    for index, df in dataframe.iterrows():\n        if pd.isna( df.Fare ):\n            if df.Pclass == 3:\n                dataframe.at[index,\"Fare\"] = df.FamilyNum * 7\n            if df.Pclass == 2:\n                dataframe.at[index,\"Fare\"] = df.FamilyNum * 14\n            else :\n                dataframe.at[index,\"Fare\"] = df.FamilyNum * 30\n\n    return dataframe\n\n\n\ndef add_title(dataframe):\n\n    dataframe[\"Title\"] = float(0)\n\n    for index, df in dataframe.iterrows():\n\n        tmp = df['Name']\n        tmp = tmp.replace('Mlle', 'Miss')\n        tmp = tmp.replace('Ms', 'Miss')\n        tmp = tmp.replace('Mme', 'Mrs')\n\n        tmp_val= 1 #0.25\n        if (\"Mr.\" in tmp):\n            tmp_val = 0 # 0.156673\n        elif (\"Miss.\" in tmp):\n            tmp_val = 4 # 0.697802\n        elif (\"Mrs.\" in tmp):\n            tmp_val = 5 # 0.792000\n        elif (\"Master.\" in tmp ):\n            tmp_val = 3 # 0.575000\n        elif (\"Misc.\" in tmp):\n            tmp_val = 2 # 0.444444\n\n        dataframe.at[index, \"Title\"] = tmp_val\n\n    return dataframe\n\ndef add_isalone(dataframe):\n\n    dataframe[\"IsAlone\"] = int(0)\n\n    for index, df in dataframe.iterrows():\n\n        if df.FamilyNum == 1:\n            dataframe.at[index, \"IsAlone\"] = 1\n\n    return dataframe\n\n\n# pos = left or right\ndef add_cabinpos(dataframe):    \n\n    dataframe[\"CabinPos\"] = 0\n\n    for index, df in dataframe.iterrows():\n\n        if not(pd.isna( df.Cabin )): \n            tmp = df.Cabin[-1:]\n            if (tmp=='1' or tmp=='3' or tmp=='5' or tmp=='7' or tmp=='9'):\n                dataframe.at[index, \"CabinPos\"]= 1\n            elif (tmp=='2' or tmp=='4' or tmp=='6' or tmp=='8' or tmp=='0'):\n                dataframe.at[index, \"CabinPos\"]= -1\n\n    return dataframe\n\ndef add_familynum(dataframe):\n    dataframe[\"FamilyNum\"] = 0\n\n    for index, df in dataframe.iterrows():\n        dataframe.at[index, \"FamilyNum\"] = df.SibSp\t+ df.Parch + 1\n\n    return dataframe\n\n\ndef add_isbigfamily(dataframe):\n    dataframe[\"IsBigFamily\"] = int(0)\n\n    for index, df in dataframe.iterrows():\n\n        if df.FamilyNum >= 5:\n            dataframe.at[index, \"IsBigFamily\"] = 1\n\n    return dataframe\n\n\n\ndef create_familysurviverate( dataframe ):\n\n    name_array = []\n    family_num = []\n    family_survive_count = []\n    for _, df in dataframe.iterrows():\n        if (df.SibSp + df.Parch) > 0:\n\n            name = df.Name\n            name_split = name.split(\" \")\n            name_split_first = name_split[0].replace(',','')\n\n            if name_split_first in name_array:\n                family_num          [name_array.index(name_split_first)] = family_num[name_array.index(name_split_first)] + 1\n                family_survive_count[name_array.index(name_split_first)] = family_survive_count[name_array.index(name_split_first)] + df.Survived\n            else:\n                name_array.append(name_split_first)\n                family_num.append(1)\n                family_survive_count.append(df.Survived)\n\n    name_survive_ratio = []            \n    for i, _ in enumerate(family_num):\n        name_survive_ratio.append( family_survive_count[i] \/ float(family_num[i]))\n\n    fname_svv_dict = {}  \n    for index, name_split_first in enumerate(name_array):\n        fname_svv_dict[name_split_first] = name_survive_ratio[index]\n    \n    # print(fname_svv_dict )\n\n    return fname_svv_dict\n\n\ndef add_familysurviverate( dataframe, fname_svv_dict ):\n    dataframe[\"FamilySurviveRate\"] = 0.50\n\n    # refs. https:\/\/ja.wikipedia.org\/wiki\/%E3%82%BF%E3%82%A4%E3%82%BF%E3%83%8B%E3%83%83%E3%82%AF%E5%8F%B7%E6%B2%88%E6%B2%A1%E4%BA%8B%E6%95%85\n    default_table = [[0.97, 0.86, 0.46], [0.33, 0.08, 0.16]]\n    \n    for index, row in dataframe.iterrows():\n        \n        if not row.IsAlone: \n            name = row.Name\n            name_split = name.split(\" \")\n            name_split_first = name_split[0].replace(',','')\n\n            if name_split_first in fname_svv_dict:\n                dataframe.at[index, \"FamilySurviveRate\"] = fname_svv_dict[name_split_first]\n        else :\n            s_ix = 0 if(row.Sex == 'female')else 1\n            c_ix = int(row.Pclass) - 1\n            dataframe.at[index, \"FamilySurviveRate\"] = default_table[s_ix][c_ix]\n\n    return dataframe\n\ndef conv_cabin_ch2i( dataframe ):\n    dataframe[\"Cabin\"] = dataframe[\"Cabin\"].fillna('D') # D=center = 0\n    dataframe.loc[:, \"Cabin\"][ dataframe[\"Cabin\"].str.contains('A') ] = 'A'\n    dataframe.loc[:, \"Cabin\"][ dataframe[\"Cabin\"].str.contains('B') ] = 'B'\n    dataframe.loc[:, \"Cabin\"][ dataframe[\"Cabin\"].str.contains('C') ] = 'C'\n    dataframe.loc[:, \"Cabin\"][ dataframe[\"Cabin\"].str.contains('D') ] = 'D'\n    dataframe.loc[:, \"Cabin\"][ dataframe[\"Cabin\"].str.contains('E') ] = 'E'\n    dataframe.loc[:, \"Cabin\"][ dataframe[\"Cabin\"].str.contains('F') ] = 'F'\n    dataframe.loc[:, \"Cabin\"][ dataframe[\"Cabin\"].str.contains('G') ] = 'G'\n    dataframe.loc[:, \"Cabin\"][ dataframe[\"Cabin\"].str.contains('T') ] = 'T'\n\n    # adjust ticket class fillna\n    dataframe.loc[:, \"Cabin\"][ dataframe[\"Cabin\"] == 'A' ] = int(0) \n    dataframe.loc[:, \"Cabin\"][ dataframe[\"Cabin\"] == 'B' ] = int(1)\n    dataframe.loc[:, \"Cabin\"][ dataframe[\"Cabin\"] == 'C' ] = int(2)\n    dataframe.loc[:, \"Cabin\"][ dataframe[\"Cabin\"] == 'D' ] = int(3)\n    dataframe.loc[:, \"Cabin\"][ dataframe[\"Cabin\"] == 'E' ] = int(4)\n    dataframe.loc[:, \"Cabin\"][ dataframe[\"Cabin\"] == 'F' ] = int(5)\n    dataframe.loc[:, \"Cabin\"][ dataframe[\"Cabin\"] == 'G' ] = int(6)\n    dataframe.loc[:, \"Cabin\"][ dataframe[\"Cabin\"] == 'T' ] = int(2)\n    dataframe = dataframe.astype({\"Cabin\": \"float\"})\n    \n    return dataframe\n\n\ndef process_df( dataframe, fname_svv_dict ):\n    \n    dataframe = dataframe.drop('Ticket', axis = 1)\n\n    dataframe = add_title( dataframe )\n    dataframe = fillna_age( dataframe )\n    dataframe = add_cabinpos( dataframe )\n    dataframe = add_familynum( dataframe )\n    dataframe = add_isalone( dataframe )\n    dataframe = add_isbigfamily( dataframe )\n    dataframe = fillna_fare( dataframe )\n    dataframe.loc[:, \"Fare\"] = dataframe[\"Fare\"] \/ dataframe[\"FamilyNum\"]\n    dataframe = add_familysurviverate( dataframe, fname_svv_dict )\n\n \n    dataframe.loc[:, \"Sex\"][dataframe[\"Sex\"] == \"male\"]   =  -1\n    dataframe.loc[:, \"Sex\"][dataframe[\"Sex\"] == \"female\"] =  1\n    dataframe.loc[:, \"Embarked\"][dataframe[\"Embarked\"] == \"S\" ] = 1\n    dataframe.loc[:, \"Embarked\"][dataframe[\"Embarked\"] == \"C\" ] = 2\n    dataframe.loc[:, \"Embarked\"][dataframe[\"Embarked\"] == \"Q\"] =  3\n    dataframe[\"Embarked\"] = dataframe[\"Embarked\"].fillna(0)\n    dataframe = dataframe.astype({\"Sex\": \"float\"})\n\n    dataframe = conv_cabin_ch2i( dataframe )\n    dataframe = dataframe.drop('Name', axis=1)\n    \n    return dataframe\n\n","28680804":"fname_svv_dict = create_familysurviverate( train )\ntrain_x = process_df( train_x, fname_svv_dict )\n\ntrain_x","c7a2d14b":"from sklearn import svm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance","fe50dea9":"def do_PermutationImportance( target_model ):\n    Xtrn, Xval, ytrn, yval = train_test_split( train_x.values, train_y.values, shuffle=True )\n\n    perm = PermutationImportance( target_model.fit( Xtrn, ytrn ), n_iter=10 ).fit(Xval, yval)\n\n    return perm\n\nperm = do_PermutationImportance( svm.SVC(kernel='rbf', C=3000, gamma=0.00007) )\n\nperm_importances = np.array( [perm.feature_importances_] )   \nperm_std         = np.array( [perm.feature_importances_std_])\neli5.show_weights(perm, feature_names = train_x.columns.tolist())","2b61a715":"perm = do_PermutationImportance( svm.SVC(kernel='rbf', C=3000, gamma=0.00007) )\nperm_importances = np.append( perm_importances, [perm.feature_importances_],     axis=0 )\nperm_std         = np.append( perm_std        , [perm.feature_importances_std_], axis=0 )\neli5.show_weights(perm, feature_names = train_x.columns.tolist())","16aa44ff":"for _ in range(48):\n    perm = do_PermutationImportance( svm.SVC(kernel='rbf', C=3000, gamma=0.00007) )\n    perm_importances = np.append( perm_importances, [perm.feature_importances_],     axis=0 )\n    perm_std         = np.append( perm_std        , [perm.feature_importances_std_], axis=0 )\n\neli5.show_weights(perm, feature_names = train_x.columns.tolist())","c28b96fe":"for _ in range(50):\n    perm = do_PermutationImportance( RandomForestClassifier(n_estimators=300, max_depth=7) )\n    perm_importances = np.append( perm_importances, [perm.feature_importances_],     axis=0 )\n    perm_std         = np.append( perm_std        , [perm.feature_importances_std_], axis=0 )\neli5.show_weights(perm, feature_names = train_x.columns.tolist())","655ed452":"for _ in range(50):\n    perm = do_PermutationImportance( xgb.XGBClassifier(n_estimators=300, max_depth=7) )\n    perm_importances = np.append( perm_importances, [perm.feature_importances_],     axis=0 )\n    perm_std         = np.append( perm_std        , [perm.feature_importances_std_], axis=0 )\neli5.show_weights(perm, feature_names = train_x.columns.tolist())","583d4e7b":"for _ in range(50):\n    perm = do_PermutationImportance( lgb.LGBMClassifier(n_estimators=300, max_depth=7) )\n    perm_importances = np.append( perm_importances, [perm.feature_importances_],     axis=0 )\n    perm_std         = np.append( perm_std        , [perm.feature_importances_std_], axis=0 )\neli5.show_weights(perm, feature_names = train_x.columns.tolist())","09e6d893":"%matplotlib inline\n \nimport numpy as np\nimport matplotlib.pyplot as plt\n\nlabel = train_x.columns.tolist()\nlabel[-1] = 'SurviveRate' # \u6587\u5b57\u6570\u9577\u3044\u305f\u3081\nfig = plt.figure(figsize=(20, 5))\nax = fig.add_subplot(1,1,1)\nax.set_yscale('log')\nx_list = np.array([ i for i in range(len(perm_importances[0]))])\nplt.bar(x_list-0.2, perm_importances.mean(axis=0), width=0.2, label='importance_mean', tick_label=label, align=\"center\")\nplt.bar(x_list+0.0, perm_importances.max(axis=0),  width=0.2, label='importance_max' )\nplt.bar(x_list+0.2, perm_std.mean(axis=0),         width=0.2, label='importance_std')\nplt.legend()","7a8385e4":"fig = plt.figure(figsize=(20, 5))\nax = fig.add_subplot(1,1,1)\nax.set_xlim([-0.05, 0.05])\nplt.hist(perm_importances, label=label, bins=30)\nplt.legend()","33e31578":"drop_list = [ \"IsBigFamily\", \"PassengerId\"]\ntrain_x = train_x.drop(drop_list, axis=1)","96daacc3":"from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold, cross_validate\n\nk_num = 5\n\nhistory=[] \ndef search_hp( max_evals, score, space ):\n    trials = Trials()\n    fmin(score, space, algo=tpe.suggest, trials=trials, max_evals=max_evals)\n    \n    print(\"\\n\"*3)\n    print(\"--searched--\")\n    print(\"\\n\"*3)    \n\n    global history\n    history = sorted(history, key=lambda tpl: tpl[1])\n    for _hist in history[:10]:\n        print( \"score:\"+ str(_hist[1]) + \" ... params:\" + str(_hist[0]) )\n","669536d6":"def svm_score(params):\n    model = svm.SVC(**params)\n    \n    kf = StratifiedKFold(n_splits=k_num, shuffle=True, random_state=0)\n    _scores = cross_validate(model, X=train_x, y=train_y.values, cv=kf)\n    # \u6700\u5c0f\u5316\u306a\u306e\u3067\u7b26\u53f7\u3092\u53cd\u8ee2\u3059\u308b\n    _score = -1 * _scores['test_score'].mean()      \n    print( \"param:\" + str(params),  \"score:\" + str(_score) )\n    history.append((params, _score))\n\n    return {'loss':_score, 'status':STATUS_OK}\n\nsvm_space = {\n    'C':          hp.choice('C',         [1000, 2000, 3000, 4000, 5000] ),\n    'degree':     hp.choice('degree',    [3,4,5,6,7,8,9,10] ),    \n    'gamma':      hp.loguniform('gamma', np.log(1e-8), np.log(1.0)),    \n    'tol':        hp.loguniform('tol',   np.log(1e-8), np.log(1.0)),\n}\n\nsvm_best_score, svm_best_prm = search_hp(50, svm_score, svm_space)","e7cf09de":"def rf_score(params):\n    model = RandomForestClassifier(**params)\n    \n    kf = StratifiedKFold(n_splits=k_num, shuffle=True, random_state=0)\n    _scores = cross_validate(model, X=train_x, y=train_y.values, cv=kf)\n    # \u6700\u5c0f\u5316\u306a\u306e\u3067\u7b26\u53f7\u3092\u53cd\u8ee2\u3059\u308b\n    _score = -1 * _scores['test_score'].mean()    \n    print( \"param:\" + str(params),  \"score:\" + str(_score) )\n    history.append((params, _score))\n\n    return {'loss':_score, 'status':STATUS_OK}\n    \nrf_space = {\n    'n_estimators' :     hp.choice('n_estimators',     [10, 30, 100, 300, 1000, 1500, 3000]),\n    'max_depth':         hp.choice('max_depth',        [ 3, 5, 7, 9, 15, 20, None ]),\n    'min_samples_split': hp.choice('min_samples_split',[ 0.1, 0.2, 0.4, 0.8, 1.0]),\n    'min_samples_leaf':  hp.choice('min_samples_leaf', [ 1, 2, 3 ]),\n    'max_features':      hp.choice('max_features',     ['auto', 'sqrt', 'log2', None]),\n    'n_jobs': hp.choice('n_jobs', [-1] )\n}","e08f35a8":"history=[] \nsearch_hp(50, rf_score, rf_space)","903eaa2a":"import numpy as np \n\ndef xgb_score(params):\n    model = xgb.XGBClassifier(**params)\n    \n    kf = StratifiedKFold(n_splits=k_num, shuffle=True, random_state=0)\n    _scores = cross_validate(model, X=train_x, y=train_y.values, cv=kf)\n    # \u6700\u5c0f\u5316\u306a\u306e\u3067\u7b26\u53f7\u3092\u53cd\u8ee2\u3059\u308b\n    _score = -1 * _scores['test_score'].mean()    \n    print( \"param:\" + str(params),  \"score:\" + str(_score) )\n    history.append((params, _score))\n    return {'loss':_score, 'status':STATUS_OK}\n    \nxgb_space = {\n    'n_estimators' :     hp.choice('n_estimators',      [10, 30, 100, 300, 1000, 3000]),\n    'max_depth':         hp.choice('max_depth',         [3,4,5,6,7,8,9,10] ),\n    'subsample':         hp.quniform('subsample',        0.1, 0.95, 0.05 ),\n    'colsample_bytree':  hp.quniform('colsample_bytree', 0.5,  1.0, 0.05 ),\n    'learning_rate':     hp.loguniform('learning_rate',  np.log(1e-4), np.log(1e-1)),\n    'gamma':             hp.loguniform('gamma',          np.log(1e-8), np.log(1.0)),\n    'alpha':             hp.loguniform('alpha',          np.log(1e-8), np.log(1.0)),\n    'lambda':            hp.loguniform('lambda',         np.log(1e-8), np.log(1.0)),    \n    'n_jobs': hp.choice('n_jobs', [-1] )\n}\n\nhistory=[]\nsearch_hp(50, xgb_score, xgb_space)","e774d6e8":"def lgbm_score(params):\n    model = lgb.LGBMClassifier(**params)\n    \n    kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n    _scores = cross_validate(model, X=train_x, y=train_y.values, cv=kf)\n    # \u6700\u5c0f\u5316\u306a\u306e\u3067\u7b26\u53f7\u3092\u53cd\u8ee2\u3059\u308b\n    _score = -1 * _scores['test_score'].mean()    \n    print( \"param:\" + str(params),  \"score:\" + str(_score) )\n    history.append((params, _score))\n    return {'loss':_score, 'status':STATUS_OK}\n    \nlgbm_space = {\n    'num_leaves':        hp.choice('num_leaves',        [3,7,15,31,63,127] ),    \n    'n_estimators' :     hp.choice('n_estimators',      [10, 30, 100, 300, 1000, 3000]),\n    'max_depth':         hp.choice('max_depth',         [3,4,5,6,7,8,9,-1] ),\n    'subsample_for_bin': hp.choice('subsample_for_bin', [2000, 20000, 200000, 500000] ),\n    'subsample':         hp.quniform('subsample',        0.1, 0.95, 0.05 ),\n    'min_split_gain':    hp.quniform('min_split_gain',   0.5,  1.0, 0.05 ),\n    'colsample_bytree':  hp.quniform('colsample_bytree', 0.5,  1.0, 0.05 ),    \n    'learning_rate':     hp.loguniform('learning_rate',  np.log(1e-4), np.log(1e-1)),\n    'reg_alpha':         hp.loguniform('reg_alpha',      np.log(1e-8), np.log(1.0)),\n    'reg_lambda':        hp.loguniform('reg_lambda',     np.log(1e-8), np.log(1.0))\n}\n\nhistory=[]\nsearch_hp(100, lgbm_score, lgbm_space)","a16de348":"svm_score = 0.8810669387439546\nrf_score  = 0.8889258302134169\nxgb_score = 0.8911543318697476\nlgb_score = 0.896838610827374\nsvm_prms = {'C': 5000, 'degree': 7, 'gamma': 5.461685273552484e-05, 'tol': 0.006054430142255137}\nrf_prms  = {'max_depth': 7, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 0.1, 'n_estimators': 300, 'n_jobs': -1}\nxgb_prms = {'alpha': 1.6802879665271848e-06, 'colsample_bytree': 0.8, 'gamma': 0.0003259964944445497, 'lambda': 0.0023499634809638274, 'learning_rate': 0.09495947657869509, 'max_depth': 3, 'n_estimators': 300, 'n_jobs': -1, 'subsample': 0.7000000000000001}\nlgb_prms = {'colsample_bytree': 0.55, 'learning_rate': 0.024285603061513013, 'max_depth': 9, 'min_split_gain': 0.8, 'n_estimators': 1000, 'num_leaves': 7, 'reg_alpha': 0.5850700627702747, 'reg_lambda': 8.262269772946297e-05, 'subsample': 0.45, 'subsample_for_bin': 500000}\n\nsvm_prms[\"probability\"] = True","57b2d6c9":"svm_model = svm.SVC               (**svm_prms).fit(train_x, train_y.values)\nrf_model  = RandomForestClassifier(**rf_prms) .fit(train_x, train_y.values)\nxgb_model = xgb.XGBClassifier     (**xgb_prms).fit(train_x, train_y.values)\nlgb_model = lgb.LGBMClassifier    (**lgb_prms).fit(train_x, train_y.values)","f2875706":"def create_submission_csv(my_model, test_features, filename):\n    pred = my_model.predict(test_features)\n    passenger_id = np.array(test[\"PassengerId\"]).astype(int)\n    my_solution = pd.DataFrame(pred, passenger_id, columns = [\"Survived\"])\n    my_solution.to_csv(filename, index_label = [\"PassengerId\"])","3e1d3796":"test_x = test\ntest_x = process_df( test_x, fname_svv_dict )\ntest_x = test_x.drop(drop_list, axis=1)","53a62671":"create_submission_csv( svm_model, test_x, \"svm_result.csv\")\ncreate_submission_csv( rf_model,  test_x, \"rf_result.csv\")\ncreate_submission_csv( xgb_model, test_x, \"xgb_result.csv\")\ncreate_submission_csv( lgb_model, test_x, \"lgb_result.csv\")","f1207389":"pd.read_csv( \"rf_result.csv\")","5d34e54d":"from sklearn.metrics import accuracy_score\n\ndef create_ensemble_feature( w_model_list, w_score_list, x ):\n    _proba = np.zeros( (len(x), len(w_model_list)) )\n    ws_mean = np.array(w_score_list).mean()\n    \n    for i in range( len(w_model_list) ):\n        _w_proba = w_model_list[i].predict_proba(x) * (w_score_list[i] - ws_mean + 1.0)\n        _proba[:,i] = _w_proba[:,1] #\n        \n    return _proba\n","1b533719":"skf  = StratifiedKFold(n_splits=k_num, shuffle=False, random_state=54)\nskf_split = skf.split(train_x, train_y.values)\n\nw_model_list2d = [None]*k_num\nw_score_list   = [svm_score,  rf_score,  xgb_score,  lgb_score]\n\nk = 0\nfor train_index, valid_index in skf_split:\n        \n    # create weak model \n    X_train = train_x.iloc[train_index]\n    y_train = train_y.values[train_index]\n\n    _svm_model = svm.SVC               (**svm_prms).fit(X_train, y_train)\n    _rf_model  = RandomForestClassifier(**rf_prms) .fit(X_train, y_train)\n    _xgb_model = xgb.XGBClassifier     (**xgb_prms).fit(X_train, y_train)\n    _lgb_model = lgb.LGBMClassifier    (**lgb_prms).fit(X_train, y_train)\n        \n    w_model_list = [_svm_model, _rf_model, _xgb_model, _lgb_model]\n    w_model_list2d[k] = w_model_list\n    k+=1\n\ndef _ensemble_score(params):\n    _log_loss = 0.0\n    _acc = 0.0\n    k = 0\n    \n    skf = StratifiedKFold(n_splits=k_num, shuffle=False, random_state=54)    \n    skf_split = skf.split(train_x, train_y.values)\n    for train_index, valid_index in skf_split:\n\n        # ensemble learning\n        X_train, X_valid = train_x.iloc[train_index],   train_x.iloc[valid_index]\n        y_train, y_valid = train_y.values[train_index], train_y.values[valid_index]\n        \n        _train_proba = create_ensemble_feature(w_model_list2d[k], w_score_list, X_train)\n        ensemble_model = lgb.LGBMClassifier(**params).fit(_train_proba, y_train)\n\n        # validate ensemble learning\n        _valid_proba = create_ensemble_feature(w_model_list2d[k], w_score_list, X_valid)\n        ensemble_pred = ensemble_model.predict(_valid_proba)\n        \n        print(y_valid[:10], ensemble_pred[:10])\n        \n        _log_loss += log_loss(y_valid, ensemble_pred) \n        _acc      += accuracy_score(y_valid, ensemble_pred)\n\n    print( \"param:\" + str(params),  \"score:\" + str(_log_loss\/k_num) + \", \" + str(_acc\/k_num) )\n    history.append((params, _log_loss\/k_num))\n    return {'loss':_log_loss\/k_num, 'status':STATUS_OK}    \n    \nensemble_space = lgbm_space\n\nhistory=[]\nsearch_hp(50, _ensemble_score, ensemble_space)","22ebb640":"ensemble_valid_score = 0.9394948980188929\nensemble_prms = {'colsample_bytree': 0.75, 'learning_rate': 0.009116960816409152, 'max_depth': -1, 'min_split_gain': 0.8, 'n_estimators': 1000, 'num_leaves': 7, 'reg_alpha': 0.005028522047266823, 'reg_lambda': 3.41676672214522e-07, 'subsample': 0.75, 'subsample_for_bin': 2000}","0ec28309":"%matplotlib inline\n \nimport numpy as np\nimport matplotlib.pyplot as plt\n\nlabel = [\"svm\", \"random_forest\", \"xgboost\", \"lgbm\", \"ensemble\"]\nfig = plt.figure(figsize=(10, 5))\nax = fig.add_subplot(1,1,1)\nax.set_ylim([0.8,1.0])\nx_list = [i for i in range(5)] \ny_list = [svm_score,  rf_score,  xgb_score,  lgb_score, ensemble_valid_score]\nplt.bar(label, y_list, align=\"center\")","fc6d5a47":"_svm_model = svm.SVC               (**svm_prms).fit(train_x, train_y)\n_rf_model  = RandomForestClassifier(**rf_prms) .fit(train_x, train_y)\n_xgb_model = xgb.XGBClassifier     (**xgb_prms).fit(train_x, train_y)\n_lgb_model = lgb.LGBMClassifier    (**lgb_prms).fit(train_x, train_y)\n        \nw_model_list = [_svm_model, _rf_model, _xgb_model, _lgb_model]\nw_score_list = [ svm_score,  rf_score,  xgb_score,  lgb_score]\n_train_proba = create_ensemble_feature(w_model_list, w_score_list, train_x)\nensemble_model = lgb.LGBMClassifier(**ensemble_prms).fit(_train_proba, train_y)\n\n_test_proba = create_ensemble_feature(w_model_list, w_score_list, test_x)\ncreate_submission_csv( ensemble_model, _test_proba, \"ensemble_result.csv\")","422c3305":"pd.read_csv( \"ensemble_result.csv\")","e73ff567":"## Output submission files.(weak models)","9c76ff92":"## My Feature Engineering","1591a8ba":"score:-0.8911543318697476 ... params:{'alpha': 1.6802879665271848e-06, 'colsample_bytree': 0.8, 'gamma': 0.0003259964944445497, 'lambda': 0.0023499634809638274, 'learning_rate': 0.09495947657869509, 'max_depth': 3, 'n_estimators': 300, 'n_jobs': -1, 'subsample': 0.7000000000000001}","61ed47ad":"### Do Feature Enginnering\n\n* Add Parameters\n* Convert type.(string feature -> float)","913184f2":"### Result\n","e56b20ff":"### XGB Tuning","bad09219":"### LightGBM Tuning","e6de11aa":"## Read Datafiles","7fbc6b2e":"score:-0.8810669387439546 ... params:{'C': 5000, 'degree': 7, 'gamma': 5.461685273552484e-05, 'tol': 0.006054430142255137}","230169fc":"### RF Tuning","d1f3294d":"And, Other model too.","2783f693":"I do Bayesian optimization using hyperopt.","1a8377bf":"# 2. Tuning Models with Bayesian optimization","a0de483f":"# 3. Ensemble Learning\n\nUse ensemble learning to combine multiple weak models.  \nA model with higher generalization performance can be created. (It may be possible)","bcf6462e":"A parameter with a small influence tends to have a different result every time it is executed.  \nBut,the standard deviation (value to the right of \u00b1) does not change much.","b1d9728f":"score:-0.8968386108273748 ... params:{'colsample_bytree': 0.55, 'learning_rate': 0.024285603061513013, 'max_depth': 9, 'min_split_gain': 0.8, 'n_estimators': 1000, 'num_leaves': 7, 'reg_alpha': 0.5850700627702747, 'reg_lambda': 8.262269772946297e-05, 'subsample': 0.45, 'subsample_for_bin': 500000}","570e795e":"score:-0.8889258302134169 ... params:{'max_depth': 7, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 0.1, 'n_estimators': 300, 'n_jobs': -1}","f67661f2":"# 1. Permutation Importance\n\nThis method is  measuring parameters importance by randomly changing parameters.  \nDetail ...   \nhttps:\/\/www.kaggle.com\/dansbecker\/permutation-importance","38bd7133":"Here are three simple and useful techniques.  \n* Permutation importance\n* Bayesian Optimization\n* Ensemble Learning\n\n\n\"Permutation importance\" is a simple and strange feature engineering.  \nhttps:\/\/www.kaggle.com\/dansbecker\/permutation-importance\n\n\"Bayesian Optimization\" is a useful & quickly at searching hyper parameter.  \n\n\"Ensemble Learning\" is a simple Idea techniques. If you use it well, It's powerful method!  \n... But it needs a lot of cost( = computational resources), and tuning so difficult.  \nOften the accuracy drops.(It means tuning failure)  ","5e4205ce":"### SVM Tuning","9bf7b0fa":"So, run it many times and take the average.","1f9e6ba0":"## DROP PARAMS\n\nIneffective parameters have a small mean value or a negative value.  \n\nAnd, \"PassengerId\" is a meaningless parameter.\n\nThat parameters like a noise, have a bad effect.","ff5b9cff":"## Consideration\n\n\"Fare\" include \"Pclass\" feature.  \n\nAnd, \"SurviveRate\" feature include \"FamilyNum\", \"IsAlone\", \"IsBigFamily\", \"Parch\", \"SibSp\".  \n\nThe parameters to be referenced differ depending on the model, and basically I want to keep duplicate parameters.\n\nBut, \"IsBigFamily\" feature's effect is very small, so deleted it.\n\n\n... and, \"PassengerId\" is just noise param.  \nThese two parameters are not related to whether passengers survive."}}