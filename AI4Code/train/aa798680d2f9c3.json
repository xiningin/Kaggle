{"cell_type":{"0e868986":"code","30c6be56":"code","8b4c21d7":"code","943a54ad":"code","516804c0":"code","7ec404bf":"code","964b504a":"code","c916844d":"code","183af989":"code","d4021ff2":"code","41606b48":"code","1fb3cc7a":"code","e6e860b2":"code","2566d26a":"markdown","419558c1":"markdown","98025389":"markdown","df76ff1e":"markdown"},"source":{"0e868986":"# Standard imports \nimport tensorflow as tf\nimport keras \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport os\nimport sys\nimport random\nimport time","30c6be56":"from keras.preprocessing.image import ImageDataGenerator\ntest_dir = '..\/input\/neuron-cy5-images\/neuron cy5 test data\/Neuron Cy5 Test Data'\nimage_size = 2048\ninput_size = 331\n\n# Keras data generator to load image samples in batches\ndata_gen = ImageDataGenerator(samplewise_center=True,\n                              samplewise_std_normalization=True)\ntest_gen = data_gen.flow_from_directory(test_dir,\n                                        target_size=(image_size, image_size),\n                                        color_mode='grayscale',\n                                        class_mode='categorical',\n                                        batch_size=1,\n                                        shuffle=True)\n\nclasses = dict((v, k) for k, v in test_gen.class_indices.items())\nnum_classes = len(classes)\nnum_samples = len(test_gen)","8b4c21d7":"from tensorflow.python.keras.models import Model\nfrom tensorflow.python.keras.applications import VGG19\nfrom tensorflow.python.keras.layers import GlobalMaxPooling2D, Dense, Reshape\n\n# Create a VGG19 architecture\npretrained_model = VGG19(include_top=False,\n                         pooling='none',\n                         input_shape=(input_size, input_size, 3),\n                         weights=None)\nx = GlobalMaxPooling2D()(pretrained_model.output)\nx = Dense(2048, activation='relu')(x)\nx = Dense(2048, activation='relu')(x)\noutput = Dense(num_classes, activation='softmax')(x) \nvgg19_model = Model(pretrained_model.input, output)\n\n# Create new model with modified config which accepts the input shape: [input_size, input_size, 1]\ncfg = vgg19_model.get_config()\ncfg['layers'][0]['config']['batch_input_shape'] = (None, input_size, input_size, 1)\nmodel = Model.from_config(cfg)\n\n# Load in the weights from training\nweights_dir = '..\/input\/fitting-deeper-networks-vgg19\/VGG19_weights.h5'\nmodel.load_weights(weights_dir)","943a54ad":"def unaug_img(img):\n    '''\n    Returns image array with pixel intensities confined to [0,1]\n    '''\n    img -= img.min()\n    img \/= img.max()\n    return img","516804c0":"def s_window_pred(img, model=model):\n    output_size = (image_size\/\/input_size)+1\n    total_overlap = (output_size*input_size-image_size)\n    overlap = total_overlap\/(output_size-1)\n    preds = np.empty([output_size,output_size,2])\n    for i in range(output_size):\n        c_x = np.floor(i*(input_size-overlap)).astype(int)\n        for j in range(output_size):\n            c_y = np.floor(j*(input_size-overlap)).astype(int)\n            preds[i,j,:] = model.predict(img[:,c_x:c_x+input_size, c_y:c_y+input_size,:])\n    return preds                                                 ","7ec404bf":"# Load an image using the generator\nX, y = next(test_gen)\n# Generate sliding windows prediction\nsw_map = s_window_pred(X)\nsw_map = np.uint8(255*sw_map[...,1])\nsw_map = cv2.resize(sw_map, (image_size, image_size))\n# Apply colourmap to sliding windows map\nsw_map = cv2.applyColorMap(sw_map, cv2.COLORMAP_JET) # Red indicates \"Treated\"\n# Fix image values in [0, 255] and combine with sliding windows map\nX = np.uint8(255*unaug_img(X))\nX = cv2.cvtColor(X[0], cv2.COLOR_GRAY2RGB)\nclass_map = cv2.addWeighted(X, 0.7, sw_map, 0.3, 0)\n\n# Display image\nplt.figure(figsize=(15, 15))\nplt.title('Sliding windows classification map: \\nActual class: '+classes[y[0,1]], fontsize=20)\nplt.imshow(class_map)\nplt.axis('off');","964b504a":"# Registering GuidedRelu as a tensorflow gradient\ntry:\n    @tf.RegisterGradient('GuidedRelu')\n    def _guided_backprop(op, grad):\n        dtype = op.outputs[0].dtype\n        gate_g = tf.cast(grad > 0., dtype)\n        gate_y = tf.cast(op.outputs[0] > 0, dtype)\n        return gate_y * gate_g * grad\nexcept KeyError: #KeyError is raised if 'GuidedRelu' has already been registered as a gradient\n    pass","c916844d":"from tensorflow.python.keras.activations import linear\n\ncfg = model.get_config()\ng = tf.get_default_graph()\n# Compiling the model within this loop implements Guided Backprop\nwith g.gradient_override_map({'Relu': 'GuidedRelu'}):\n    # Copying model using it's config\n    guid_model = Model.from_config(cfg)\n    # Replacing the activation on the last layer with linear\n    guid_model.layers[-2].activation = linear # Index=-2 due to Reshape layer\n    guid_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[])","183af989":"from tensorflow.python.keras import backend as K\ndef generate_saliency(img, class_pred, model=guid_model):\n    '''\n    Generates the Saliency map of img with respect to class_pred, given the Keras model.\n    '''\n    inp = model.input\n    class_outp = model.output[:, class_pred]\n    sal = K.gradients(tf.reduce_sum(class_outp), inp)[0]\n    # Keras function returning the saliency map given an image input\n    sal_fn = K.function([inp], [sal])\n    # Generating the saliency map and normalizing it\n    img_sal = sal_fn([np.resize(img, (1, input_size, input_size, 1))])[0]\n    img_sal = np.maximum(img_sal, 0)\n    img_sal \/= img_sal.max()\n    return img_sal","d4021ff2":"def lcm(a,b):\n    '''\n    Returns the lowest common multiple of inputs a & b.\n    '''\n    from math import gcd\n    return (a*b)\/\/gcd(a,b)","41606b48":"# Loading optimal thresholds\nfile_dir = '..\/input\/youden-s-j-statistic-for-threshold-determination\/optimum_thresholds.csv'\nopt_thresholds = pd.read_csv(file_dir, sep=',', header=None).iloc[1:,1:].values\n\ndef generate_mapping(inp_img, opt_thresholds=opt_thresholds):\n    '''\n    Given an image, returns an explanation in the form of a Saliency map coloured by the sliding windows map.\n    '''\n    img = np.copy(inp_img)\n    map_size = img.shape[1]\n    # Load prediction map  \n    pred_map = s_window_pred(img)[...,1]\n    # Round predictions to 0 and 1 if confident and 0.5 if not\n    rounded_map = np.heaviside(pred_map-1+opt_thresholds[0], 1)\/2 + np.heaviside(pred_map-opt_thresholds[1], 1)\/2\n    # Produce coloured heatmap where red=treated, green=unsure and blue=untreated\n    heatmap = np.zeros(np.append(rounded_map.shape,[3]))\n    heatmap[np.where(rounded_map==1)] = [0, 0, 1]\n    heatmap[np.where(rounded_map==1\/2)] = [0, 1, 0]\n    heatmap[np.where(rounded_map==0)] = [1, 0, 0]\n    heatmap = cv2.resize(heatmap, (map_size,map_size))\n    #Reshape image to 3D tensor\n    img = np.reshape(img, [image_size, image_size, 1])\n    img_small = cv2.resize(img, (input_size, input_size))\n    # Generate saliency\n    saliencys = [generate_saliency(img_small, i) for i in range(num_classes)]\n    # Resize image and convert to RGB\n    img = np.uint8(255*unaug_img(img))\n    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB) \n    sal = cv2.resize(saliencys[0][0,...,0], (map_size, map_size))\n    # Histogram equalize for visualisation\n    sal = cv2.equalizeHist(np.uint8(255*sal))\n    sal = cv2.cvtColor(sal, cv2.COLOR_GRAY2RGB)\/255\n    #Combine saliency and heatmap\n    mapping = np.multiply(heatmap, sal)\n    #Normalize\n    for i in range(mapping.shape[2]):\n        if mapping[...,i].max() != 0:\n            mapping[...,i] \/= mapping[...,i].max()\n    mapping = np.uint8(255*mapping)\n    return cv2.addWeighted(img, 0.7, mapping, 0.3, 0)","1fb3cc7a":"def generate_prediction(inp_img, model=model):\n    '''\n    Generate prediction  on full-sized image by resizing\n    '''\n    img = np.copy(inp_img)\n    img = np.squeeze(img, axis=(0,-1))\n    img = cv2.resize(img, (input_size, input_size))\n    img = np.reshape(img, [1, input_size, input_size, 1])\n    pred = model.predict(img, steps=1)\n    return pred[0]","e6e860b2":"# Load random sample\nX, y = next(test_gen)\n# Generate prediction\ny_pred = generate_prediction(X)\n# Generate explanation\nstart = time.time()\nfinal_map = generate_mapping(X)\nprint('Time elapsed: '+str(time.time()-start))\nclass_pred = classes[y_pred[1].round()]\nfig, ax = plt.subplots(1, 2, figsize=(20, 10))\n# Show image\nax[0].set_title('Actual class: '+classes[y[0, 1]]+'\\n', fontsize=15)\nax[0].imshow(unaug_img(X[0,...,0]), cmap='gray')\nax[0].axis('off')\n# Show mapping\nax[1].set_title('Predicted class: '+str(class_pred)+'\\nConfidence: '+str(y_pred.max()), fontsize=15);\nax[1].imshow(final_map);\nax[1].axis('off');","2566d26a":"The sliding windows algorithm is now implemented: performing 49 classifications across the original image and returning a 7x7 array of predictions corresponding to different regions of the image.","419558c1":"Now the function to create the mapping is created. The Saliency Map is colored by the regional classification to give an visualisation of which neurons indicate \"Treated\" and which indicate \"Untreated\". \n\nHowever, the regional classification is sometimes unsure and outputs values close to 0.5. To remove these the optimal thresholds produced by maximising Youden's J Statistic are used. If a prediction is above the threshold for a class is it coloured Red for \"Treated\" and Blue for \"Untreated\". If the prediction is not above either of these then the region is coloured Green for Uncertain.","98025389":"Now that we have a model that performs well and has been verified to extract meaningful features, a map to produce the features of an image which most contribute to the classification of a class can be generated.\n\nThe Saliency Map produced before performs well however it isn't able to discrimate which parts of an image correspond to \"Treated\" or \"Untreated\", rather it is better at localising where the Neurons are in the image.\n\nA technique commonly used for object detection, the Sliding Windows algorithm, performs very well to classify regions of an image separately and for this reason it is implemented alongside the Saliency Map to determine which Neurites look \"Treated\" and which look \"Untreated\".","df76ff1e":"For the purpose of generating the Saliency Map, Guided BackPropagation is implemented."}}