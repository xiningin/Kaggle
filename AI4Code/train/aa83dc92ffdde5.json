{"cell_type":{"d2c3eb59":"code","dcddb7ff":"code","947a22d3":"code","5e673546":"code","08b9bbb3":"code","3077df9d":"code","a42b3cbc":"code","7cdc5030":"code","5095ef54":"code","d7c41e0a":"code","d449d327":"code","7c84c3e3":"code","33c98c7c":"code","143cd203":"code","1f9cfad2":"code","9d67065b":"code","04081c96":"code","e28c574d":"code","f8272abc":"code","92cae048":"code","d5713b13":"code","0758fd25":"code","ec2ae071":"code","a3f94789":"code","b67ad7d9":"code","e5dda941":"code","c8cf2741":"code","9edbca6a":"code","c58e28c6":"markdown","f5a96934":"markdown","e3a340aa":"markdown","46e2db29":"markdown","fc81fff0":"markdown","d1351f0a":"markdown","5d6a68b5":"markdown","461219b4":"markdown","46553f51":"markdown","14831280":"markdown","e7245be1":"markdown"},"source":{"d2c3eb59":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dcddb7ff":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\n\nfrom itertools import product\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\n\nimport warnings\nwarnings.filterwarnings('ignore')","947a22d3":"data = pd.read_csv('\/kaggle\/input\/heart-failure-prediction\/heart.csv')\ndata.head()","5e673546":"data.info()","08b9bbb3":"obj_col = data.select_dtypes('object').columns.tolist()\n# indices = product([0, 1], [0, 1, 2])\n_, axes = plt.subplots(2, 3, figsize=(18, 8), sharey=True)\nplt.suptitle('Hist per Flag', fontsize=30, color='b')\n\ntarget = 'HeartDisease'\nprint(data[target].value_counts())\nsns.countplot(x=target, data=data, ax=axes[0, 0])\n\nsns.countplot(x=obj_col[0], data=data, hue=target, ax=axes[0, 1])\nsns.countplot(x=obj_col[1], data=data, hue=target, ax=axes[0, 2])\nsns.countplot(x=obj_col[2], data=data, hue=target, ax=axes[1, 0])\nsns.countplot(x=obj_col[3], data=data, hue=target, ax=axes[1, 1])\nsns.countplot(x=obj_col[4], data=data, hue=target, ax=axes[1, 2])\n    \nplt.tight_layout()","3077df9d":"# to see all correlation\ndata_num = data[data.select_dtypes(exclude='object').columns]\nsns.pairplot(data_num, hue=target)","a42b3cbc":"# check the number \ncorr = data.corr()\nsns.heatmap(corr, annot=True)","7cdc5030":"var = 'FastingBS'\nsns.countplot(x=var, data=data, hue=target)","5095ef54":"data_copy = data.copy()\n\n# Encoding the obj columns\ndata_copy['Sex'] = data_copy['Sex'].replace({'M': 1, 'F': 0})\ndata_copy['ExerciseAngina'] = data_copy['ExerciseAngina'].replace({'Y': 1, 'N': 0})\ndata_encoded = pd.get_dummies(data_copy, drop_first=True)\ndata_encoded.head()","d7c41e0a":"scaler = StandardScaler()\n\nX = data_encoded.drop('HeartDisease', axis=1).values\ny = data_encoded['HeartDisease'].values\n\n# scaling X\nX_scaled = scaler.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y)","d449d327":"X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train)","7c84c3e3":"model = RandomForestClassifier(n_estimators=200, random_state=123, criterion='entropy')\n\nscores = cross_val_score(estimator=model, X=X_train, y=y_train, cv=10)\nnp.mean(scores)","33c98c7c":"kfold = KFold(n_splits=10).split(X_train, y_train)\n\nscores = []\nfor train, test in kfold:\n    model.fit(X_train[train], y_train[train])\n    score = model.score(X_train[test], y_train[test])\n    \n    scores.append(score)\n\nprint(np.mean(scores))","143cd203":"model.score(X_valid, y_valid)","1f9cfad2":"col_importance_dict = {}\nfor col, importance in zip(data_encoded.columns, model.feature_importances_):\n    col_importance_dict[col] = importance\n    \ncol_importance_dict","9d67065b":"data_copy = data.copy()\ndata_copy = data_copy.drop(['ChestPainType', 'RestingECG'], axis=1)\ndata_copy_encoded = pd.get_dummies(data_copy, drop_first=True)","04081c96":"scaler = StandardScaler()\n\nX = data_copy_encoded.drop('HeartDisease', axis=1).values\ny = data_copy_encoded['HeartDisease'].values\n\n# scaling X\nX_scaled = scaler.fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y)","e28c574d":"model = RandomForestClassifier(n_estimators=200, random_state=123, criterion='entropy')\n\nscores = cross_val_score(estimator=model, X=X_train, y=y_train, cv=10)\nprint('Score is ', np.mean(scores))","f8272abc":"X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train)","92cae048":"kfold = KFold(n_splits=10).split(X_train, y_train)\n\nscores = []\nfor train, test in kfold:\n    model.fit(X_train[train], y_train[train])\n    score = model.score(X_train[test], y_train[test])\n    \n    scores.append(score)\n\nprint('Train score is ', np.mean(scores))\n\nprint('Valid score is ', model.score(X_valid, y_valid))","d5713b13":"# we seach how much importances each col has\ncol_importance_dict = {}\nfor col, importance in zip(data_encoded.columns, model.feature_importances_):\n    col_importance_dict[col] = importance\n    \ncol_importance_dict","0758fd25":"xgb_train = xgb.DMatrix(X_train, label=y_train)\nxgb_valid = xgb.DMatrix(X_valid, label=y_valid)\n\nxbg_param = {\n   # predict 0 or 1\n   'objective': 'binary:logistic',\n   'max_depth':1,\n   'eval_metric': 'logloss'\n}\n\nbst = xgb.train(xbg_param, xgb_train, num_boost_round=150)\npreds = bst.predict(xgb_valid)\n\ny_pred = np.where(preds > 0.5, 1, 0)\n\nprint(accuracy_score(y_true=y_valid, y_pred=y_pred))\nprint(f1_score(y_true=y_valid, y_pred=y_pred))","ec2ae071":"param_dist = {\n    'n_estimators': [100, 150, 200], \n    'criterion':['gini', 'entropy'], \n    'max_depth':[1, 5, None], \n    'max_features': ['auto', 'sqrt', 'log2']\n}\n\nmodel = RandomForestClassifier(random_state=123)\n\nclf = RandomizedSearchCV(model, param_dist, cv=10)\nsearch = clf.fit(X_train, y_train)\nsearch.score(X_train, y_train)","a3f94789":"search.score(X_valid, y_valid)","b67ad7d9":"model_bst = search.best_estimator_\ny_pred = model_bst.predict(X_train)\n\nroc_auc_score(y_true=y_train, y_score=y_pred)","e5dda941":"y_pred_valid = search.predict(X_valid)\nroc_auc_score(y_true=y_valid, y_score=y_pred_valid)","c8cf2741":"search.best_params_","9edbca6a":"model_bst.score(X_test, y_test)","c58e28c6":"<div style=\"background-color:lightyellow;padding:15px 0 0 10px\">\n    <h3>We can say\n        <ul>\n            <li>Male tends to be sick more than Female!<\/li> \n            <li>ASY (Chest Pain Type) is related to heart disease!<\/li>\n            <li>Flat of ST_Slope is related to heart disease!<\/li>\n        <\/ul>\n    <\/h3>\n<\/div>","f5a96934":"### OK, we've done preparing.\n#### go analysis the data","e3a340aa":"#### ChestPain something's  importances don't look necessary??","46e2db29":"### First, We seek the data","fc81fff0":"#### OK, we don't have any columns which have nulls!","d1351f0a":"### OK, that's not enough acccuracy for me, haha. of course, we didn't use lightGBM,  simple logistics, adaboost, or someting\n### and we didn't try parameter engineering enough.\n#### So, if you think the better way to get more accuracy, feel free to let me know!!","5d6a68b5":"### Each col hasn't the high corr with others.  \n### So, we don't have to the multico for now.","461219b4":"<div style=\"background-color:lightyellow;padding:15px 0 0 10px\">\n    <h3>We can say\n        <ul>\n            <li>There isn't strong correlation with cols<\/li> \n            <li>FastingBS (fasting blood sugar) flag 1 cause heart disease<\/li>\n        <\/ul>\n    <\/h3>\n<\/div>","46553f51":"### I think this model isn't over fitting ","14831280":"#### This time, Random forest is better than xgb.\n#### So, we search hyperparameters","e7245be1":"### I think this model isn't over fitting."}}