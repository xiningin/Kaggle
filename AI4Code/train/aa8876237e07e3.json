{"cell_type":{"97052507":"code","c4928c17":"code","3efdd430":"code","48cd43f3":"code","25a716f2":"code","0c43458d":"code","8f8361f3":"code","cc1ec605":"code","d283ea39":"code","84950298":"markdown","8f8a5f90":"markdown","bbbd4f87":"markdown","309a58de":"markdown","2230f2e4":"markdown"},"source":{"97052507":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if dirname.split('\/')[-1] not in ['train', 'test']:\n            print(os.path.join(dirname, filename))","c4928c17":"train_df = pd.read_csv('\/kaggle\/input\/feedback-prize-2021\/train.csv')\ntrain_df.head()","3efdd430":"def read_essay_txt(essay_id, path='train'):\n    essay_file_path = f\"..\/input\/feedback-prize-2021\/{path}\/{essay_id}.txt\"\n    with open(essay_file_path, 'r') as essay_file:\n        return essay_file.read()","48cd43f3":"texts = {id_: read_essay_txt(id_) for id_ in train_df.id.unique()}\ntexts_len = {id_: len(text.split()) for id_, text in texts.items()}","25a716f2":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nsns.histplot(texts_len.values());\nprint('Mean Length:', np.mean(list(texts_len.values())))\nprint('Length > 1024: {:.2f}%'.format(np.mean([len_ > 1024 for len_ in texts_len.values()]) * 100))","0c43458d":"train_df.discourse_type.unique()","8f8361f3":"from matplotlib import colors\n\nlabel_color_dict = {\n    'Lead': 'royalblue',\n    'Position': 'violet',\n    'Evidence': 'crimson',\n    'Claim': 'magenta',\n    'Counterclaim': 'darkorange',\n    'Rebuttal': 'lime',\n    'Concluding Statement': 'red'\n}\n\n\nlabel_rgb_dict = {label: colors.to_rgb(color) for label, color in label_color_dict.items()}\n\nlabel_rgb_dict","cc1ec605":"def get_text_arr(text_id, max_len=1024):\n    text_arr = np.zeros((max_len, 3))\n    text = texts[text_id]\n\n    token_label_dict = {}\n    for i, row in train_df.query('id == @text_id')[['discourse_type', 'predictionstring']].iterrows():\n        for token in row['predictionstring'].split(' '):\n            token_label_dict[int(token)] = row['discourse_type']\n\n    for i in range(max_len):\n        if i in token_label_dict.keys():\n            text_arr[i] = label_rgb_dict[token_label_dict[i]]\n    \n    dims = (int(np.sqrt(max_len)), int(np.sqrt(max_len)), 3)\n    text_arr = np.reshape(text_arr, dims)\n    \n    return text_arr","d283ea39":"text_ids = train_df.id.unique()\n\nncols = 5\nnrows = 10\nmax_len = 1024\n\nfor nrow in range(nrows):\n    fig, axes = plt.subplots(1, ncols, figsize=(20, 5))\n    for i, text_id in enumerate(text_ids[nrow*ncols:(nrow*ncols)+ncols]):\n        axes[i].imshow(get_text_arr(text_id, max_len))\n        axes[i].set_title(text_id)","84950298":"Now that we have the rgb values, the next part is supposed to be easy. For each text we shall create a 2d array of of dims `(1024, 3)` then reshape this array into a 3d array of dims `(32, 32, 3)`.\n\nThe easiest way I can think of is to use the prediction string of each text to create a dict where keys are token ids and values are label. If the a token isn't in the dict, then we shall skip it and not set it's value in the array, and hence it will remain `(0, 0, 0)` which is black.","8f8a5f90":"It seems that the mean is 421 tokens, and almost 1% of texts are larger than 1024. **So I'll stick with 1024.**\n\n## Getting RGB values\n\nNow I need to set an rgb value for each label, then I need to make a 3d array `(32, 32, 3)` for each text, and then I can visualize them.","bbbd4f87":"# How can I implement this?\n\nFirst I think that either need to impose a cut off on the length of texts as when using transformers, so maybe this cutoff can be 1024 words. Or maybe I can use the the dimensions of the largest text as default.\n\nSo let's check the length of texts to get an idea about how we shall proceed.","309a58de":"### And that's it. Thanks for reading.","2230f2e4":"# Introduction\n\nIn this notebook, I'll explore the texts by making images using the labels of their words. I don't know if this has any particular use case in this competition, but I thought it would be fun to implement it anyways, and maybe someone can use it. I don't know. Anyways, let's get going."}}