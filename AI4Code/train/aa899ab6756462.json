{"cell_type":{"9e99f273":"code","2f420eb4":"code","dea7a85f":"code","fcdecd51":"code","9fa42610":"code","73577997":"code","f9b4f5c9":"code","789ad289":"code","62eff974":"code","1f238b6f":"code","87b18cd6":"code","5243c717":"code","0a9b68f8":"code","de893eee":"code","2773bdce":"code","14c744ea":"code","9149c84e":"code","bfd44f1f":"code","fd32cb95":"code","26ab360d":"code","6b9a6562":"code","62b22376":"code","25fc29ee":"code","c0bbc6a5":"code","73d47495":"code","298a2a1e":"code","d676de85":"code","9ae69be5":"code","7af0cebc":"code","8d3b8aba":"code","4e03a426":"code","b7aab3fc":"code","bc6a3ac9":"code","f0b1447b":"code","26e6e19a":"code","0b3cf6c9":"code","12c2392d":"code","40329e81":"code","193bd9f6":"code","9feaa792":"code","3a855059":"code","d10431bc":"code","221de919":"code","db2f2255":"code","959b4288":"code","2b4d51e8":"code","b7c31fb3":"code","8e0fdb9c":"code","cc3e84b0":"code","ac86d95b":"code","5f4eff80":"code","7b9e2bb9":"code","d5b535fb":"code","c75dda4e":"code","29beabd4":"code","c7a75588":"code","d2b5a00f":"code","e9a559f5":"code","b21311ed":"code","d2509fc5":"code","6b01b015":"code","f18b2dad":"code","506cf5e8":"code","94371cf5":"code","da3ab89d":"code","b6d4dd86":"code","0f32d0b1":"code","96956f2d":"code","a3a80f76":"code","da25c953":"code","2ec1ca72":"code","7297ade2":"code","cc3e875a":"code","a587e99d":"code","92d6caee":"code","51fd08bf":"code","b462bc17":"code","f6dbe57d":"code","0f483d3f":"code","003c54dd":"code","84437f81":"code","393ba34c":"code","423d3ba9":"code","010ecc88":"code","e4b52645":"code","c794fe48":"code","996c1d22":"code","50d545dc":"code","a6c32349":"code","1189a8d5":"code","ed53cc18":"code","d5b111f8":"code","68b75ad8":"code","180aa97b":"code","5ef098a0":"code","100b0ebf":"code","2cf3fc99":"code","677466c1":"code","eb5a3271":"code","0d58873e":"code","906687ad":"markdown","f42d15d3":"markdown","680e0109":"markdown","143e0a80":"markdown","d03975e5":"markdown","767e7a8a":"markdown","7dc4b1cb":"markdown","51da02de":"markdown","a1ff9aa2":"markdown","69f65ac7":"markdown","34901657":"markdown","5755e2a7":"markdown","55a83de2":"markdown","ea2a4e4c":"markdown","9368d839":"markdown","f2164034":"markdown","ec1a1438":"markdown","d245a889":"markdown","f2cf488d":"markdown","a1f11048":"markdown","5a6214da":"markdown","9801d21d":"markdown","557176b7":"markdown","3e0caa4e":"markdown","428cbd35":"markdown","e5c4a766":"markdown","1407472d":"markdown","69996ef8":"markdown","8aa8a1f6":"markdown","218426db":"markdown","15dc11db":"markdown","9785774d":"markdown","f49e3ed9":"markdown","23e53715":"markdown","a765381d":"markdown","39afec08":"markdown"},"source":{"9e99f273":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2f420eb4":"import numpy as np\nimport pandas as pd\npd.options.mode.chained_assignment = None\n\nimport warnings\nwarnings.simplefilter(action =\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats\nimport math\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\nimport sklearn.model_selection as GridSearchCV\nimport sklearn.model_selection as ms\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom mlxtend.regressor import StackingCVRegressor\n","dea7a85f":"train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","fcdecd51":"train.head()","9fa42610":"test.head()","73577997":"# Analyse statically insight of train data\ntrain.describe()","f9b4f5c9":"# Analyse statically insight of test data\ntest.describe()","789ad289":"train.info()","62eff974":"test.info()","1f238b6f":"print(f\"The train data size: {train.shape}\")\nprint(f\"The test data size: {test.shape}\")","87b18cd6":"diff_train_test = set(train.columns) - set(test.columns)\ndiff_train_test","5243c717":"train[\"SalePrice\"].describe()","0a9b68f8":"print(f\"Skewness of SalePrice: {train['SalePrice'].skew()}\")\nprint(f\"Kurtosis of SalePrice: {train['SalePrice'].kurt()}\")","de893eee":"# Plot a histogram and kernel density estimate for SalePrice target\nsns.distplot(train[\"SalePrice\"], color = \"#330033\");\nplt.xlabel(\"Sale price\", fontsize = 14, color = \"#330033\" );","2773bdce":"sns.distplot(np.log1p(train[\"SalePrice\"]));","14c744ea":"ax = sns.distplot(train[\"SalePrice\"], bins=20, kde=False, fit=stats.norm);\nplt.title(\"Distribution of SalePrice\")\n\n# Get the fitted parameters used by sns\n(mu, sigma) = stats.norm.fit(train[\"SalePrice\"])\nprint(\"mu={:.2f}, sigma={:.2f}\".format(mu, sigma))\n\n# Legend and labels \nplt.legend([\"Normal dist. fit ($\\mu=${:.2f}, $\\sigma=${:.2f})\".format(mu, sigma)])\nplt.ylabel(\"Frequency\")\n\n# Cross-check this is indeed the case - should be overlaid over black curve\nx_dummy = np.linspace(stats.norm.ppf(0.01), stats.norm.ppf(0.99), 100)\nax.plot(x_dummy, stats.norm.pdf(x_dummy, mu, sigma))\nplt.legend([\"Normal dist. fit ($\\mu=${:.2f}, $\\sigma=${:.2f})\".format(mu, sigma), \"cross-check\"]);","9149c84e":"numeric_features = train.select_dtypes(include=[np.number])\ncorr_numeric_features = numeric_features.corr()","bfd44f1f":"# Correlation Numeric featurs with output variable(SalePrice)\ncorr_target = abs(corr_numeric_features[\"SalePrice\"])\nprint(f\"Correlation between numeric featurs and SalePrice:\\n{corr_target.sort_values()}\")","fd32cb95":"relevant_features = corr_target[corr_target > 0.6]\nrelevant_features\nprint(f\"Selecting highly correlated numeric features with SalePrice:\\n{relevant_features}\")","26ab360d":"highly_correlated_visualization = sns.pairplot(train[[\"OverallQual\",  \"TotalBsmtSF\", \"1stFlrSF\", \"GrLivArea\", \"GarageCars\", \"GarageArea\", \"SalePrice\"]])","6b9a6562":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train[\"SalePrice\"].to_frame()\n\n#Combine train and test sets\nconcat_data = pd.concat((train, test), sort=False).reset_index(drop=True)\n#Drop the target \"SalePrice\" and Id columns\nconcat_data.drop([\"SalePrice\"], axis=1, inplace=True)\nconcat_data.drop([\"Id\"], axis=1, inplace=True)\nprint(\"Total size is :\",concat_data.shape)","62b22376":"concat_data.head()","25fc29ee":"concat_data.info()","c0bbc6a5":"# Count the null columns\nnull_columns = concat_data.columns[concat_data.isnull().any()]\nconcat_data[null_columns].isnull().sum()","73d47495":"df = concat_data.drop(columns=[\"Alley\", \"PoolQC\", \"Fence\", \"MiscFeature\", \"FireplaceQu\"], axis = 1) ","298a2a1e":"print(f\"The full data size: {df.shape}\")","d676de85":"# Count the null columns\nnull_columns = df.columns[df.isnull().any()]\ndf[null_columns].isnull().sum()","9ae69be5":"numeric_features = df.select_dtypes(include=[np.number])\nnumeric_features.dtypes","7af0cebc":"print(f\"Numerical features: {numeric_features.shape}\")","8d3b8aba":"unique_list_numeric_features = [(item, np.count_nonzero(df[item].unique())) for item in numeric_features]\nprint(f\"Unique numeric features:\\n{unique_list_numeric_features}\")","4e03a426":"# Corralation between Numeric features \ncorr_numeric_features = numeric_features.corr()\n\n#Using Pearson Correlation\nplt.figure(figsize=(30, 30))\n\nsns.heatmap(corr_numeric_features, cbar=True, square= True, fmt='.1f', annot=True, annot_kws={'size':15}, cmap=\"Blues\")\n\nplt.show()","b7aab3fc":"# Count the null columns' numeric_features in data set\nnull_columns_numeric_features = numeric_features.columns[numeric_features.isnull().any()]\nprint(f\"Missing values in numerical features: \\n{numeric_features[null_columns_numeric_features].isnull().sum()}\")","bc6a3ac9":"cols_absence_zero = [\"LotFrontage\", \"MasVnrArea\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\",\"TotalBsmtSF\", \"BsmtFullBath\", \"BsmtHalfBath\"]\ndf[cols_absence_zero] = df[cols_absence_zero].replace(to_replace = np.nan, value = 0) ","f0b1447b":"df[\"GarageYrBlt\"] = df.apply(\n    lambda row: row[\"YearBuilt\"] if np.isnan(row[\"GarageYrBlt\"]) else row[\"GarageYrBlt\"],\n    axis=1\n)","26e6e19a":"def ReplaceNanWithMedian(df, featureName):\n    median = df.loc[:,featureName].median()\n\n    df[featureName] = df.apply(lambda row: median if np.isnan(row[featureName]) else row[featureName], axis=1)","0b3cf6c9":"ReplaceNanWithMedian(df, \"GarageCars\")\nReplaceNanWithMedian(df, \"GarageArea\")","12c2392d":"categoricals = df.select_dtypes(exclude=[np.number])\ncategoricals.dtypes","40329e81":"print(f\"Categorical features: {categoricals.shape}\")","193bd9f6":" # Count the null columns\nnull_columns = categoricals.columns[categoricals.isnull().any()]\nprint(f\"Missing values in categorical features: \\n{categoricals[null_columns].isnull().sum()}\")","9feaa792":"cols_absence_none = [\"MasVnrType\", \"BsmtQual\", \"BsmtExposure\",\"BsmtFinType1\", \"BsmtFinType2\",\"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"BsmtCond\"]\ndf[cols_absence_none] = df[cols_absence_none].replace(to_replace = np.nan, value = \"None\") ","3a855059":"def ReplaceNanWithMostFrequent(df, featureName):\n    df[featureName] = df.apply(lambda x:x.fillna(x.value_counts().index[0]))","d10431bc":"ReplaceNanWithMostFrequent(df, \"MSZoning\")\nReplaceNanWithMostFrequent(df, \"Utilities\")\nReplaceNanWithMostFrequent(df, \"Exterior1st\")\nReplaceNanWithMostFrequent(df, \"Exterior2nd\")\nReplaceNanWithMostFrequent(df, \"MasVnrType\")\nReplaceNanWithMostFrequent(df, \"Electrical\")\nReplaceNanWithMostFrequent(df, \"KitchenQual\")\nReplaceNanWithMostFrequent(df, \"Functional\")\nReplaceNanWithMostFrequent(df, \"SaleType\")","221de919":" # Check the null columns in data set\nnull_columns = df.columns[df.isnull().any()]\ndf[null_columns].isnull().sum()","db2f2255":"# Check if the all of the columns have 0 null values.\nsum(df.isnull().sum() != 0)","959b4288":"print(f\"The Shape of all data: {df.shape}\")","2b4d51e8":"# Transforming some numerical variables that are really categorical\ndf[\"MSSubClass\"] = df[\"MSSubClass\"].apply(str)\n\n#Changing OverallCond into a categorical variable\ndf[\"OverallCond\"] = df[\"OverallCond\"].astype(str)\n\n#Year and month sold are transformed into categorical features.\ndf[\"YrSold\"] = df[\"YrSold\"].astype(str)\ndf[\"MoSold\"] = df[\"MoSold\"].astype(str)","b7c31fb3":"final_df = pd.get_dummies(df).reset_index(drop=True)","8e0fdb9c":"print(f\"Original dataset shape: {df.shape}\")\nprint(f\"Encoded dataset shape: {final_df.shape}\")\nprint(f\"We have: {final_df.shape[1] - df.shape[1]} new encoded features\")","cc3e84b0":"TrainData = final_df[:ntrain] \nTestData = final_df[ntrain:]","ac86d95b":"TrainData.shape, TestData.shape","5f4eff80":"fig, ax_arr = plt.subplots(3, 2, figsize=(14, 14))\n\nax_arr[0, 0].scatter(x = train[\"OverallQual\"], y = train[\"SalePrice\"], color=\"#330033\", alpha=.3)\nax_arr[0, 0].set_title(\"House price Vs Overall material and finish quality\", fontsize=14, color=\"#330033\")\n\nax_arr[0, 1].scatter(x = train[\"TotalBsmtSF\"], y = train[\"SalePrice\"], color=\"#330033\", alpha=.3)\nax_arr[0, 1].set_title(\"House price Vs Total square feet of basement area\", fontsize=14, color=\"#330033\")\n\nax_arr[1, 0].scatter(x = train[\"1stFlrSF\"], y = train[\"SalePrice\"], color=\"#330033\", alpha=.3)\nax_arr[1, 0].set_title(\"House price Vs First Floor square feet\", fontsize=14, color=\"#330033\")\n\nax_arr[1, 1].scatter(x = train[\"GrLivArea\"], y = train[\"SalePrice\"], color=\"#330033\", alpha=.3)\nax_arr[1, 1].set_title(\"House price Vs Above grade (ground) living area square feet\", fontsize=14, color=\"#330033\")\n\nax_arr[2, 0].scatter(x = train[\"LotArea\"], y = train[\"SalePrice\"], color=\"#330033\", alpha=.3)\nax_arr[2, 0].set_title(\"House price Vs Lot size in square feet\", fontsize=14, color=\"#330033\")\n\nax_arr[2, 1].scatter(x = train[\"YearBuilt\"], y = train[\"SalePrice\"], color=\"#330033\", alpha=.3)\nax_arr[2, 1].set_title(\"House price Vs Original construction date\", fontsize=14, color=\"#330033\")\n\nplt.tight_layout()\n\nplt.show()","7b9e2bb9":"OverallQual_visualization = sns.swarmplot(y = \"SalePrice\", x = \"OverallQual\", data = train, size = 7)\n# remove the top and right line in graph\nsns.despine()\nOverallQual_visualization.figure.set_size_inches(14,10)\nplt.show()","d5b535fb":"TotalBsmtSF_visualization = sns.swarmplot(y = \"SalePrice\", x = \"TotalBsmtSF\", data = train, size = 7)\n# remove the top and right line in graph\nsns.despine()\nTotalBsmtSF_visualization.figure.set_size_inches(14,10)\nplt.show()","c75dda4e":"StFlrSF_visualization = sns.swarmplot(y = \"SalePrice\", x = \"1stFlrSF\", data = train, size = 7)\n# remove the top and right line in graph\nsns.despine()\nStFlrSF_visualization.figure.set_size_inches(14,10)\nplt.show()","29beabd4":"LivArea_visualization = sns.swarmplot(y = \"SalePrice\", x = \"GrLivArea\", data = train, size = 7)\n# remove the top and right line in graph\nsns.despine()\nLivArea_visualization.figure.set_size_inches(14,10)\nplt.show()","c7a75588":"LotArea_visualization = sns.swarmplot(y = \"SalePrice\", x = \"LotArea\", data = train, size = 7)\n# remove the top and right line in graph\nsns.despine()\nLotArea_visualization.figure.set_size_inches(14,10)\nplt.show()","d2b5a00f":"YearBuilt_visualization = sns.swarmplot(y = \"SalePrice\", x = \"YearBuilt\", data = train, size = 7)\n# remove the top and right line in graph\nsns.despine()\nYearBuilt_visualization.figure.set_size_inches(14,10)\nplt.show()","e9a559f5":"MasVnrArea_visualization = sns.swarmplot(y = \"SalePrice\", x = \"MasVnrArea\", data = train, size = 7)\n# remove the top and right line in graph\nsns.despine()\nMasVnrArea_visualization.figure.set_size_inches(14,10)\nplt.show()","b21311ed":"train_df = TrainData[(TrainData[\"GrLivArea\"] < 4600) & (TrainData[\"MasVnrArea\"] < 1500)]\nprint(f\"We removed: {TrainData.shape[0]- train_df.shape[0]} outliers\")","d2509fc5":"print(f\"Encoded dataset shape: {final_df.shape}\")","6b01b015":"target = train[[\"SalePrice\"]]","f18b2dad":"pos = [1298,523, 297]\ntarget.drop(target.index[pos], inplace=True)","506cf5e8":"print(\"We make sure that both train and target sets have the same row number after removing the outliers:\")\nprint(f\"Train: {train_df.shape[0]} rows\")\nprint(f\"Target: {target.shape[0]} rows\")","94371cf5":"print(f\"Skewness before log transform: {target.SalePrice.skew()}\")\nprint(f\"Kurtosis before log transform: {target.SalePrice.kurt()}\")","da3ab89d":"target[\"SalePrice\"] = np.log1p(target[\"SalePrice\"])","b6d4dd86":"print(f\"Skewness after log transform: {target.SalePrice.skew()}\")\nprint(f\"Kurtosis after log transform: {target.SalePrice.kurt()}\")","0f32d0b1":"final_df = final_df.loc[:,~final_df.columns.duplicated()]","96956f2d":"x = train_df\ny = np.array(target)","a3a80f76":"# Split the data set into train and test sets \nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)","da25c953":"x_train.shape, x_test.shape, y_train.shape, y_test.shape","2ec1ca72":"scaler = RobustScaler()","7297ade2":"# transform \"x_train\"\nx_train = scaler.fit_transform(x_train)\n# transform \"x_test\"\nx_test = scaler.transform(x_test)\n#Transform the test set\nX_test= scaler.transform(TestData)","cc3e875a":"ridge = Ridge()\nparameters = {\"alpha\":[x for x in range(1,101)]}\nridge_regressor = ms.GridSearchCV(ridge, param_grid = parameters, scoring = \"neg_mean_squared_error\", cv = 15)\nridge_regressor_mod = ridge_regressor.fit(x_train, y_train)\nprint(f\"Best parameter for Ridge regression: {ridge_regressor_mod.best_params_}\")","a587e99d":"ridge = Ridge(alpha = 13)\nridge_mod = ridge.fit(x_train, y_train)\npred_ridge = ridge_mod.predict(x_test) \nmse_ridge = mean_squared_error(y_test,pred_ridge)\nrmse_ridge = np.sqrt(mean_squared_error(y_test, pred_ridge))\nscore_ridge_train = ridge_mod.score(x_train, y_train)\nscore_ridge_test = ridge_mod.score(x_test, y_test)","92d6caee":"print(f\"Mean Square Error for Ridge regression = {mse_ridge}\")\nprint(f\"Root Mean Square Error for Ridge regression = {rmse_ridge}\")\nprint(f\"R^2(coefficient of determination) on training set = {score_ridge_train}\")\nprint(f\"R^2(coefficient of determination) on testing set = {score_ridge_test}\")","51fd08bf":"# Print coefficients\nprint(f\"Ridge coefficient:\\n {ridge.coef_}\") ","b462bc17":"parameters= {\"alpha\":[0.0001,0.0009,0.001,0.002,0.003,0.01,0.1,1,10,100]}\n\nlasso = Lasso()\nlasso_reg = ms.GridSearchCV(lasso, param_grid = parameters, scoring = \"neg_mean_squared_error\", cv = 15)\nlasso_reg.fit(x_train,y_train)\n\nprint(\"The best value of Alpha for Lasso regression is: \",lasso_reg.best_params_)","f6dbe57d":"lasso = Lasso(alpha = 0.0009)\nlasso_mod = lasso.fit(x_train, y_train)\npred_lasso = lasso_mod.predict(x_test) \n\nmse_lasso = mean_squared_error(y_test,pred_lasso)\nrmse_lasso = np.sqrt(mean_squared_error(y_test, pred_lasso))\nscore_lasso_train = lasso_mod.score(x_train, y_train)\nscore_lasso_test = lasso_mod.score(x_test, y_test)","0f483d3f":"print(f\"Mean Square Error for Lasso regression = {mse_lasso}\")\nprint(f\"Root Mean Square Error for Lasso regression = {rmse_lasso}\")\nprint(f\"R^2(coefficient of determination) on training set = {score_lasso_train}\")\nprint(f\"R^2(coefficient of determination) on testing set = {score_lasso_test}\")","003c54dd":"# Print coefficients\nprint(f\"Lasso coefficient:\\n {lasso.coef_}\")","84437f81":"coefs = pd.Series(lasso_mod.coef_, index = x.columns)\nplt.figure(figsize=(20, 20))\n\nimp_coefs = pd.concat([coefs.sort_values().head(10), coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\", color = \"#800080\")\nplt.xlabel(\"Lasso coefficient\", weight = \"bold\")\nplt.title(\"Feature importance in the Lasso Model\", weight = \"bold\", color = \"#800080\")\nplt.show()","393ba34c":"print(\"Lasso kept \",sum(coefs != 0), \"important features and dropped the other \", sum(coefs == 0),\" features\")","423d3ba9":"alphas = [0.0005]\nl1ratio = [0.9]\n\nelastic_net_cv = ElasticNetCV(cv = 5, max_iter = 1e7, alphas = alphas,  l1_ratio = l1ratio)\nelastic_mod = elastic_net_cv.fit(x_train, y_train.ravel())\npred_elastic = elastic_mod.predict(x_test) \n\nmse_elastic = mean_squared_error(y_test, pred_elastic)\nrmse_elastic = np.sqrt(mean_squared_error(y_test, pred_elastic))\nscore_elastic_train = elastic_mod.score(x_train, y_train)\nscore_elastic_test = elastic_mod.score(x_test, y_test)","010ecc88":"print(f\"Mean Square Error for Elastic Net CV regression = {mse_elastic}\")\nprint(f\"Root Mean Square Error for Elastic Net CV regression = {rmse_elastic}\")\nprint(f\"R^2(coefficient of determination) on training set = {score_elastic_train}\")\nprint(f\"R^2(coefficient of determination) on testing set = {score_elastic_test}\")","e4b52645":"# Print coefficients\nprint(f\"Elastic Net CV coefficient:\\n {lasso.coef_}\")","c794fe48":"svr = SVR(C = 20, epsilon = 0.008, gamma = 0.0003,)\nsvr_mod = svr.fit(x_train, y_train.ravel())\npred_svr = svr_mod.predict(x_test) \n\nmse_svr = mean_squared_error(y_test, pred_svr)\nrmse_svr = np.sqrt(mean_squared_error(y_test, pred_svr))\nscore_svr_train = svr_mod.score(x_train, y_train)\nscore_svr_test = svr_mod.score(x_test, y_test)","996c1d22":"print(f\"Mean Square Error for SVR = {mse_svr}\")\nprint(f\"Root Mean Square Error for SVR = {rmse_svr}\")\nprint(f\"R^2(coefficient of determination) on training set = {score_svr_train}\")\nprint(f\"R^2(coefficient of determination) on testing set = {score_svr_test}\")","50d545dc":"gbr = GradientBoostingRegressor(n_estimators = 3000, learning_rate = 0.05, max_depth = 4, max_features = \"sqrt\", min_samples_leaf = 15, min_samples_split = 10, loss = \"huber\", random_state = 42)\ngbr_mod = gbr.fit(x_train, y_train.ravel())\npred_gbr = gbr_mod.predict(x_test) \n\nmse_gbr = mean_squared_error(y_test, pred_gbr)\nrmse_gbr = np.sqrt(mean_squared_error(y_test, pred_gbr))\nscore_gbr_train = gbr_mod.score(x_train, y_train)\nscore_gbr_test = gbr_mod.score(x_test, y_test)","a6c32349":"print(f\"Mean Square Error for Gradient Boosting regression = {mse_gbr}\")\nprint(f\"Root Mean Square Error for Gradient Boosting regression = {rmse_gbr}\")\nprint(f\"R^2(coefficient of determination) on training set = {score_gbr_train}\")\nprint(f\"R^2(coefficient of determination) on testing set = {score_gbr_test}\")","1189a8d5":"lightgbm = LGBMRegressor(objective = \"regression\", \n                                       num_leaves = 4,\n                                       learning_rate = 0.01, \n                                       n_estimators = 5000,\n                                       max_bin = 200, \n                                       bagging_fraction = 0.75,\n                                       bagging_freq = 5, \n                                       bagging_seed = 7,\n                                       feature_fraction = 0.2,\n                                       feature_fraction_seed = 7,\n                                       verbose = -1,\n                                       )\nlightgbm_mod = lightgbm.fit(x_train, y_train.ravel())\npred_lightgbm = lightgbm_mod.predict(x_test) \n\nmse_lightgbm = mean_squared_error(y_test, pred_lightgbm)\nrmse_lightgbm = np.sqrt(mean_squared_error(y_test, pred_lightgbm))\nscore_lightgbm_train = lightgbm_mod.score(x_train, y_train)\nscore_lightgbm_test = lightgbm_mod.score(x_test, y_test)","ed53cc18":"print(f\"Mean Square Error for LGBMRegressor = {mse_lightgbm}\")\nprint(f\"Root Mean Square Error for LGBMRegressor = {rmse_lightgbm}\")\nprint(f\"R^2(coefficient of determination) on training set = {score_lightgbm_train}\")\nprint(f\"R^2(coefficient of determination) on testing set = {score_lightgbm_test}\")","d5b111f8":"xgboost = XGBRegressor(learning_rate = 0.01, n_estimators = 3460,\n                                     max_depth = 3, min_child_weight = 0,\n                                     gamma = 0, subsample = 0.7,\n                                     colsample_bytree = 0.7,\n                                     objective = \"reg:squarederror\", nthread=-1,\n                                     scale_pos_weight = 1, seed = 27,\n                                     reg_alpha = 0.00006)\nxgboost_mod = xgboost.fit(x_train, y_train)\npred_xgboost = xgboost_mod.predict(x_test) \n\nmse_xgboost = mean_squared_error(y_test, pred_xgboost)\nrmse_xgboost = np.sqrt(mean_squared_error(y_test, pred_xgboost))\nscore_xgboost_train = xgboost_mod.score(x_train, y_train)\nscore_xgboost_test = xgboost_mod.score(x_test, y_test)","68b75ad8":"print(f\"Mean Square Error for xgboost regression = {mse_lightgbm}\")\nprint(f\"Root Mean Square Error for xgboost regression = {rmse_lightgbm}\")\nprint(f\"R^2(coefficient of determination) on training set = {score_lightgbm_train}\")\nprint(f\"R^2(coefficient of determination) on testing set = {score_lightgbm_test}\")","180aa97b":"vote = VotingRegressor([(\"Ridge\", ridge_mod), (\"Lasso\", ridge_mod), (\"Elastic Net CV\", elastic_net_cv), \n                        (\"SVR\", svr), (\"GradientBoostingRegressor\", gbr), (\"LGBMRegressor\", lightgbm), (\"XGBRegressor\", xgboost)])\nvote_mod = vote.fit(x_train, y_train.ravel())\nvote_pred = vote_mod.predict(x_test)\n\nprint(f\"Root Mean Square Error test for ENSEMBLE METHODS: {np.sqrt(mean_squared_error(y_test, vote_pred))}\")","5ef098a0":"stack_gen = StackingCVRegressor(regressors = [ridge_mod, ridge_mod, elastic_net_cv, svr, gbr, lightgbm, xgboost, vote],\n                                meta_regressor = xgboost,\n                                use_features_in_secondary = True)\nstack_mod = stack_gen.fit(x_train, y_train.ravel())\nstack_pred = stack_mod.predict(x_test)\n\nprint(f\"Root Mean Square Error test for STACKING REGRESSOR: {np.sqrt(mean_squared_error(y_test, vote_pred))}\")","100b0ebf":"averaged_preds = (vote_pred*0.3 + stack_pred*0.5 + pred_lasso*0.2)\n\nprint(f\"Root Mean Square Error test for STACKING REGRESSOR: {np.sqrt(mean_squared_error(y_test, averaged_preds))}\")","2cf3fc99":"# Visualize the result in a plot with averaging predict.\n\nplt.figure(figsize=(20,8))\n\nx_ax = range(len(x_test))\nplt.scatter(x_ax, y_test, s = 5, color=\"#422d42\", label = \"Original\")\nplt.plot(x_ax, averaged_preds, lw = 0.8, color = \"#9c6d9c\", label = \"Predicted\")\nplt.legend()\nplt.show()","677466c1":"#VotingRegressor to predict the final Test\nvote_test = vote.predict(X_test)\nfinal1 = np.expm1(vote_test)\n\n#StackingRegressor to predict the final Test\nstack_test = stack_gen.predict(X_test)\nfinal2 = np.expm1(stack_test)\n\n#LassoRegressor to predict the final Test\nlasso_test = lasso.predict(X_test)\nfinal3 = np.expm1(lasso_test)","eb5a3271":"test[\"Id\"].value_counts()","0d58873e":"final = (0.2*final1 + 0.6*final2 + 0.2*final3)\n\nfinal_submission = pd.DataFrame({\"Id\": test[\"Id\"], \"SalePrice\": final})\nfinal_submission.to_csv(\"final_submission.csv\", index=False)\nfinal_submission.head()","906687ad":"# Data claening","f42d15d3":"# Checking skewness for \"SalePrice\" is therefore an important process because it can give us clearer idea of the orientation\n","680e0109":"### Replacing missing values for columns:  \"GarageCars\", \"GarageArea\" with Median\n","143e0a80":"### One Hot Encoding: Convert categorical variables into dummy variable\n","d03975e5":"### Regularization reduces the magnitude of the features\u2019 coefficients to improve the accuracy of a model.","767e7a8a":"### Plotting the scatter diagram between sale price(dependent) and highly correlated numeric features but two featurs \"GarageArea\" and \"GarageCars\" are not useful for the model. We replace them with \"LotArea\" and \"YearBuilt\":","7dc4b1cb":"## Working with Numeric Features ","51da02de":"# SVR","a1ff9aa2":"# Working with outliers","69f65ac7":"# Remove any duplicated column names\n","34901657":"# lightgbm.LGBMRegressor","5755e2a7":"# Stacking: to avoid fitting on the same data twice , and is effective in reducing overfitting.","55a83de2":"# Features engineering","ea2a4e4c":"# Lasso regression","9368d839":"# Define numerical features","f2164034":"# Submission","ec1a1438":"*Thank you for your consideration sincerely.*","d245a889":"# Ridge regression model","f2cf488d":"# Elastic Net CV Regression ","a1f11048":"# ENSEMBLE METHODS","5a6214da":"### Replacing missing values for columns: \"LotFrontage\", \"MasVnrArea\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\",\"TotalBsmtSF\", \"BsmtFullBath\", \"BsmtHalfBath\" With \"0\" because \"Na\" value in those columns represent the absence of what is being measured:\n","9801d21d":"### Replacing missing values for columns: \"MasVnrType\", \"BsmtQual\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"BsmtCond\" With \"None\" because \"Na\" value in those columns represent the absence of what is being measured:","557176b7":"# The principal of this challenge is, predict the final sale price. So get the info about the column of \"SalePrice\":\n","3e0caa4e":"### Droping columns \"Alley\", \"PoolQC\", \"Fence\", \"MiscFeature\", \"FireplaceQu\" because they have less impact on the result.\n","428cbd35":"# The task is in this kaggle competition: Predict sales prices","e5c4a766":"### Scince we have outliers for scaling data using the mean and variance of the data is likely to not work very well. In this case, we can use robust_scale and RobustScaler as drop-in replacements instead.","1407472d":"### Selecting highly correlated features with SalePrice\n","69996ef8":"# XGBoost regression","8aa8a1f6":"# Working with non-numeric Features","218426db":"# Gradient Boosting regression","15dc11db":"# Get SalePrice with normally distributed, by using log transformation and it may give us better picture of the data.\n","9785774d":"# Averaging Approach","f49e3ed9":"### Replacing missing values for \"GarageYrBlt\" column with the value of the year as the house was built.","23e53715":"# The SalePrice is skewed to the right. solving this problem by using g(1+x) tranform to fix the skew.\n","a765381d":"### Replacing missing values by the most common value for columns: \"MSZoning\", \"Utilities\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"Electrical\", \"KitchenQual\", \"Functional\", \"SaleType\"","39afec08":"# Lets check that test dataset has all the columns in train dataset except SalePrice"}}