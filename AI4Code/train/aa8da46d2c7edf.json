{"cell_type":{"52c4d390":"code","9fa23625":"code","7e3e0e0b":"code","022d459e":"code","a3220c34":"code","6aad50bb":"code","92cf2413":"code","fc79e102":"code","80fcbb75":"code","36c9a653":"code","9162e658":"code","9aa6214c":"code","3e7d5a7e":"code","50cea680":"code","4cb02ef7":"code","248ce9ad":"code","82ab01ba":"code","f8dfbe77":"code","73393669":"code","799519af":"code","32209b49":"code","445d5646":"code","e6bff3de":"code","38f10355":"code","deaeca96":"code","c40efb49":"code","9d16b08e":"code","e75b0c74":"code","217aec72":"code","aee18688":"code","a85487b7":"code","418dabc2":"code","5175ddad":"code","3862a688":"code","f4faf223":"code","b2a9760b":"code","23f9d25a":"code","b7ab2bb1":"code","55e24fc2":"code","aef6ebc5":"code","aa770ae9":"code","f230b658":"code","7b5c3f58":"code","89d9721b":"code","54f5f5b1":"code","f84879ff":"code","deb89c7c":"code","c39ca851":"markdown","f4532080":"markdown","58132032":"markdown","0d20eb24":"markdown","9e28c49c":"markdown","b3849af0":"markdown","8038f46a":"markdown","b4f0da40":"markdown","bfed1073":"markdown","b3a59b37":"markdown","67775d71":"markdown","7f13b168":"markdown","bec9b465":"markdown","0aa3fa44":"markdown","080a6dcc":"markdown","1b37389d":"markdown","82b9bf61":"markdown","82602040":"markdown","cd1f8476":"markdown","a771ea20":"markdown","5b962bf3":"markdown","691e34c5":"markdown","1603df3a":"markdown","4690bc86":"markdown","81efae61":"markdown","6011b00e":"markdown","d70740c2":"markdown","bbdd33de":"markdown","9cb607af":"markdown","b66fd7d2":"markdown","becf75de":"markdown","c322479d":"markdown","f2263aea":"markdown","3b3e5b45":"markdown","aebbf4b7":"markdown","e2650268":"markdown","4d340331":"markdown","36abd4be":"markdown","9488d3cb":"markdown","6c624ab9":"markdown","82aa1093":"markdown","43303661":"markdown","de669866":"markdown","fe781892":"markdown","e6603295":"markdown","137a58ba":"markdown","b070d8c3":"markdown","915597be":"markdown","99cb7cf5":"markdown","d8a9f137":"markdown","82f9786e":"markdown","84dc5ebd":"markdown","799454e9":"markdown","7e73f821":"markdown","15e1b5af":"markdown","4c821a47":"markdown","2bb940cd":"markdown","86452698":"markdown","aa493654":"markdown","dde7219c":"markdown","a847e927":"markdown","b4e31360":"markdown","5e306061":"markdown","81254449":"markdown","ae9c4763":"markdown","241f3013":"markdown","f028d9d7":"markdown","04049b36":"markdown","43ce3aac":"markdown","e44196c4":"markdown","4242033d":"markdown","2eb24b1b":"markdown","d47ccae4":"markdown","561d166e":"markdown","be3b87d8":"markdown","a54b13f6":"markdown","6616e9db":"markdown","5521cd68":"markdown","132f37b8":"markdown","1c93915e":"markdown","85ec72f6":"markdown","45d09343":"markdown","e2cce492":"markdown","08f97e96":"markdown","e7b6a169":"markdown","d36a3d08":"markdown","9afb7e6b":"markdown","6e37b73e":"markdown","e839d8fa":"markdown","5ab9c090":"markdown","a9741ec9":"markdown","9fc543cf":"markdown","60f48034":"markdown","9ff09174":"markdown","9a31c940":"markdown","89b6d6e3":"markdown","195f583a":"markdown","d45020da":"markdown","55d8eeb1":"markdown","270bea94":"markdown","e8403ea3":"markdown","8a744876":"markdown","500fb366":"markdown","bedaac1a":"markdown","d1e4d17f":"markdown","5cfa068e":"markdown","75f0d6ee":"markdown","1b82719b":"markdown","e59babf1":"markdown","43b8e6ba":"markdown","10cae79e":"markdown","df5bea71":"markdown","fc6f1569":"markdown","3d696752":"markdown","a27d9ab3":"markdown","4d921f87":"markdown","df9c4699":"markdown","2e6a47f9":"markdown","74c1d715":"markdown","5fb14a0b":"markdown","bd46a6f2":"markdown","5a6e34e2":"markdown"},"source":{"52c4d390":"import numpy as np\nimport pandas as pd\n\nfrom scipy.optimize import minimize\nfrom scipy.stats import multivariate_normal, binom\n\nfrom functools import partial\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.animation import FuncAnimation\nimport seaborn as sns\n\n%matplotlib inline\n\nfrom IPython.display import HTML\n\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK\nfrom hyperopt.pyll import scope as ho_scope\nfrom hyperopt.pyll.stochastic import sample as ho_sample\n\n# ================================\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, PowerTransformer\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\nfrom sklearn.metrics import roc_auc_score, mean_squared_error, r2_score\n\nfrom sklearn import datasets\nfrom sklearn.utils import shuffle","9fa23625":"# Seed for pseudo-random generator\nSEED = 21\n\n# Plotting limits\nx_limits = (-10, 10)\ny_limits = (-10, 10)\n\n# --------------------------------\n\n# Toy function\nfct = lambda x, y: 5*np.exp(-((x - 3)**2 + (y - 4)**2)\/4) + \\\n                   2*np.exp(-np.sqrt((x + 3)**2 + (y - 3)**2)\/2) + \\\n                   1*np.exp(-np.sin(np.sqrt((x - 5)**2 + (y + 3)**2)\/1))\n\n# Defining mesh\nxy_mesh = np.meshgrid(np.linspace(*x_limits, 2001), np.linspace(*x_limits, 2001))\nfct_mesh = fct(xy_mesh[0], xy_mesh[1])","7e3e0e0b":"fig0 = plt.figure(figsize=(8, 8))\n\ncntr = plt.contourf(xy_mesh[0], xy_mesh[1], fct_mesh, levels=30, cmap=plt.cm.ocean)\n_ = fig0.colorbar(cntr, ax=plt.gca(), fraction=0.05, pad=0.05, aspect=18)\n\nplt.xlabel('$x$', fontsize=12)\nplt.ylabel('$y$', fontsize=12)\nplt.title('Target function', fontsize=16)\n\nplt.gca().grid(True)\nplt.gca().set_aspect('equal')","022d459e":"# Number of points in Grid Search per coordinate\nn_pts = 14\n\n# Building GS grid and evaluating function\nxy_GS = np.meshgrid(np.linspace(*x_limits, n_pts), np.linspace(*y_limits, n_pts))\nfct_GS = fct(xy_GS[0], xy_GS[1])\n\n# Best point obtained by Grid Search\nbest_GS = (fct_GS.max(), (xy_GS[0].flatten()[fct_GS.argmax()], \n                          xy_GS[1].flatten()[fct_GS.argmax()]))\n\n# --------------------------------\n\n# Setting up random seed for reproducibility\nnp.random.seed(SEED)\n\n# Sampling points from prior distributions for x and y\ny_std = 5\nxy_RS = [np.random.uniform(*x_limits, n_pts**2).reshape(n_pts, n_pts), \n         np.random.normal(0, y_std, n_pts**2).reshape(n_pts, n_pts)]\nfct_RS = fct(xy_RS[0], xy_RS[1])\n\n# Best point obtained by Random Search\nbest_RS = (fct_RS.max(), (xy_RS[0].flatten()[fct_RS.argmax()], \n                          xy_RS[1].flatten()[fct_RS.argmax()]))","a3220c34":"# Initial guess and list of traversed points\nxy_M = [(0.0, 0.0)]\n\n# Function to dump current point in minimization algorithm to list\ndef f_current_point(x):\n    xy_M.append(x)\n    return None\n\n# Starting maximization (minus sign since most optimization problems are \n# formulated in terms of finding minimum)\nres_M = minimize(lambda x: -fct(*x), x0=xy_M[0], method='L-BFGS-B', \n                   bounds=[x_limits, y_limits], options={'maxiter': n_pts**2}, \n                   callback=f_current_point)\n\nprint(res_M)\n\n# Best point obtained by L-BFGS-B\nxy_M = np.array(xy_M).T\nbest_M = (-res_M.fun, res_M.x)","6aad50bb":"# Setting up hyperparameters space\nhp_space = {'x': hp.uniform('x', *x_limits), \n            'y': hp.normal('y', 0, 5)}\n\n# Setting up number of evaluations and Trials object\nn_evals = n_pts**2\ntrls = Trials()\n\n# Running optimization\nres_HO = fmin(lambda hps: -fct(hps['x'], hps['y']), space=hp_space, algo=tpe.suggest, \n                trials=trls, max_evals=n_evals, rstate=np.random.RandomState(SEED))\n\n# Restoring search history and best point\nxy_HO = [np.array([x['misc']['vals']['x'] for x in trls.trials]), \n         np.array([x['misc']['vals']['y'] for x in trls.trials])]\nbest_HO = (-trls.best_trial['result']['loss'], (space_eval(hp_space, res_HO)['x'], \n                                                space_eval(hp_space, res_HO)['y']))","92cf2413":"# Setting up layout and graph labels\nfig0, axs = plt.subplots(nrows = 2, ncols = 2, figsize = (16, 16))\nlabels = ('Grid Search', 'Random Search', 'L-BFGS-B', 'Hyperopt')\n\n# Plotting\nfor (xy, xlabel, xbest, ax) in zip((xy_GS, xy_RS, xy_M, xy_HO), \n                                   labels, \n                                   (best_GS, best_RS, best_M, best_HO), \n                                    axs.flatten()):\n    # Function contour and points from search history\n    cntr = ax.contourf(xy_mesh[0], xy_mesh[1], fct_mesh, levels=30, cmap=plt.cm.ocean)\n    ax.plot(xy[0], xy[1], linewidth=0, marker='.', color='red')\n    \n    # Marking best result\n    ax.plot(x_limits, [xbest[1][1]]*2, linewidth=1, linestyle='--', color='red')\n    ax.plot([xbest[1][0]]*2, y_limits, linewidth=1, linestyle='--', color='red')\n    \n    # Formatting axes\n    ax.set_xlim(x_limits)\n    ax.set_ylim(y_limits)\n    \n    ax.set_xlabel('$x$', fontsize = 14)\n    ax.set_ylabel('$y$', fontsize = 14)\n    \n    ax.set_title('{0}\\n Best point: ({1:.2f}, {2:.2f}), function value: {3:.3f}'.format(\n        xlabel, *xbest[1], xbest[0]), fontsize=16)\n    \n    ax.grid(True)\n    ax.set_aspect('equal')\n\n# Adding colorbar\n_ = fig0.colorbar(cntr, ax=axs, fraction=0.03, pad=0.02, aspect=32)","fc79e102":"steps = 2000\n\ntrls = Trials()\ntrls_random = Trials()\n\n# ----------------\n\nres_HO = fmin(lambda hps: -fct(hps['x'], hps['y']), space = hp_space, algo = tpe.suggest, \n              trials = trls, max_evals = steps, rstate = np.random.RandomState(SEED))\n\nxy_HO2 = [np.array([x['misc']['vals']['x'] for x in trls.trials]), \n          np.array([x['misc']['vals']['y'] for x in trls.trials])]\n\n\nres_RS = fmin(lambda hps: -fct(hps['x'], hps['y']), space = hp_space, algo = tpe.rand.suggest, \n              trials = trls_random, max_evals = steps, rstate = np.random.RandomState(SEED))\n\nxy_RS2 = [np.array([x['misc']['vals']['x'] for x in trls_random.trials]), \n          np.array([x['misc']['vals']['y'] for x in trls_random.trials])]","80fcbb75":"# Setting up figure\nfig0, axs = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\nlabels = ['Random Search', 'Hyperopt']\n\n# Setting up animation frames\nn_frames = 20\nn_frame_pts = steps\/\/n_frames\n\n# Initial axes formatting\nfor (ax, xlabel) in zip(axs.flatten(), labels):\n    ax.contourf(xy_mesh[0], xy_mesh[1], fct_mesh, levels=30, cmap=plt.cm.ocean)\n    \n    ax.set_xlim(x_limits)\n    ax.set_xlim(y_limits)\n    \n    ax.set_title(xlabel, fontsize=16)\n    \n    ax.set_xlabel('$x$', fontsize=14)\n    ax.set_ylabel('$y$', fontsize=14)\n\n    ax.grid(True)\n    ax.set_aspect('equal')\n    \n# Adding colorbar\n_ = fig0.colorbar(cntr, ax=axs, fraction=0.03, pad=0.02, aspect=15)\n\n# Points and iteration indicator are the only things that change\npts0 = axs[0].plot([], [], linewidth=0, marker='o', color='red', markersize=5)\npts1 = axs[1].plot([], [], linewidth=0, marker='o', color='red', markersize=5)\nfig0_tl = fig0.suptitle('Batch {0}'.format(0), fontsize=18)\n\n\ndef f_update_anim(frame, n_frames):\n    \"\"\"\n    TODO\n    \"\"\"\n    \n    # Determine indexes of points within iteration\n    idx0 = int(frame*len(trls)\/n_frames)\n    idx1 = int((frame + 1)*len(trls)\/n_frames)\n    \n    # Plot new data\n    pts0[0].set_data(xy_RS2[0][idx0:idx1], xy_RS2[1][idx0:idx1])\n    pts1[0].set_data(xy_HO2[0][idx0:idx1], xy_HO2[1][idx0:idx1])\n    fig0_tl.set_text('Batch {0}'.format(frame + 1, n_frames))\n    \n    return pts0[0], pts1[0], fig0_tl\n\nplt.close()\n\n# Set up animation\nani1 = FuncAnimation(fig0, lambda fr: f_update_anim(fr, n_frames), frames=n_frames, interval=500, blit=False)","36c9a653":"HTML(ani1.to_jshtml())","9162e658":"# !pip install hyperopt\n# !conda install -c conda-forge hyperopt","9aa6214c":"from hyperopt import fmin, hp, tpe, Trials, space_eval\nfrom hyperopt.pyll import scope as ho_scope\nfrom hyperopt.pyll.stochastic import sample as ho_sample","3e7d5a7e":"hp_space = hp.uniform('C', low=0.01, high=100)","50cea680":"hp_space = hp.loguniform('C', low=-4*np.log(10), high=4*np.log(10))","4cb02ef7":"hp_space = {\n    'n_estimators': ho_scope.int(hp.quniform('n_estimators', low=100, high=300, q=25)), \n    'criterion': hp.choice('criterion', ['gini', 'entropy']), \n    'max_features': hp.uniform('max_features', low=0.25, high=0.75)\n}","248ce9ad":"ho_sample(hp_space)","82ab01ba":"hp_space = {\n    'type': hp.choice('type', [\n        {\n            'type': 'logit', \n            'C': hp.loguniform('C', low=-4*np.log(10), high=4*np.log(10))\n        }, \n        {\n            'type': 'random_forest', \n            'n_estimators': ho_scope.int(hp.quniform('n_estimators', low=100, high=300, q=25)), \n            'criterion': hp.choice('criterion', ['gini', 'entropy']), \n            'max_features': hp.uniform('max_features', low=0.25, high=0.75)\n        }\n    ])\n}\n\nho_sample(hp_space)","f8dfbe77":"# Defining mixture distributions\ndists = [\n    [\n        (0.5, multivariate_normal(mean=[0.0, 0.5], \n                                  cov=[[1.0, -0.8], \n                                       [-0.8, 3.0]])), \n        (0.5, multivariate_normal(mean=[6.5, 0.0], \n                                  cov=[[1.0, 0.9], \n                                       [0.9, 2.0]]))\n    ], \n    [\n        (0.3, multivariate_normal(mean=[2.0, 6.0], \n                                  cov=[[1.0, 0.5], \n                                       [0.5, 2.0]])), \n        (0.4, multivariate_normal(mean=[4.0, 2.0], \n                                  cov=[[1.0, 1.5], \n                                       [1.5, 5.0]])), \n        (0.3, multivariate_normal(mean=[7.0, 6.0], \n                                  cov=[[1.0, 0.3], \n                                       [0.3, 1.0]]))\n    ]\n]\n\n# Class weights\nweights = np.array([0.75, 0.25])\n\n# ================================\n\ndef f_sample_from_mixture(mix_dist, n_samples=1, random_state=None):\n    \"\"\"\n    Draws sample points from mixture of different distributions\n    \n    Parameters:\n    ----------------\n    mix_dist : list of tuples (w, d), where w is relative weight of distribution, d is scipy distribution\n        object (which has rvs() method to draw sample). NOTE: w's must sum to 1!\n    n_samples : int, number of samples to draw from mixed distribution\n    random_state : int, NumPy pseudo-random generator seed\n    \n    Returns:\n    ----------------\n    NumPy array of shape (n_samples, x), where each row is a sample point. x is determined by dimensionality\n        of distributions\n    \"\"\"\n    \n    # Setting preudo-random generator seed\n    if not random_state is None:\n        np.random.seed(random_state)\n    \n    # Drawing particular distribution component from which further to draw a point\n    dkeys = np.random.choice(range(len(mix_dist)), size=n_samples, replace=True, \n                             p=[x[0] for x in mix_dist])\n    \n    res = []\n    \n    # Drawing samples from mixture components\n    for (xkey, xdist) in enumerate(x[1] for x in mix_dist):\n        res.append(xdist.rvs(size=(dkeys == xkey).sum()))\n    \n    # Shaking up samples\n    return np.random.permutation(np.vstack(res))","73393669":"n_samples = 5000\n\nnp.random.seed(SEED)\n\n# Drawing number of objects of class 1 then generating features and class labels\nn_class1 = np.random.binomial(n=n_samples, p=weights[1])\nX1 = np.vstack([\n    f_sample_from_mixture(dists[0], n_samples - n_class1, SEED), \n    f_sample_from_mixture(dists[1], n_class1, SEED)\n])\ny1 = np.hstack([np.zeros(n_samples - n_class1), np.ones(n_class1)])\n\n# Shuffling samples (otherwise we have all 0's followed by all 1's)\nX1, y1 = shuffle(X1, y1, random_state=SEED)","799519af":"# Define plotting boundaries for later, and meshgrid for contour plots\nxlim2 = (-4, 12)\nylim2 = (-6, 10)\n\nxy_mesh2 = np.meshgrid(np.arange(xlim2[0], xlim2[1] + 0.01, 0.01), \\\n                       np.arange(ylim2[0], ylim2[1] + 0.01, 0.01))\n\n# Reshaping meshgrid into shape that we can pass to classifier from sklearn\nxy_mesh2_features = np.array([xy_mesh2[0].ravel(), xy_mesh2[1].ravel()]).T\n\n# --------------------------------\n\n# Plotting sampled points\nplt.figure(figsize=(8, 8))\nflag = y1 == 1\n\nplt.scatter(X1[~flag, 0], X1[~flag, 1], label='Class 0', marker='.', color='orange')\nplt.scatter(X1[flag, 0], X1[flag, 1], label='Class 1', marker='.', color='green')\n\nplt.xlim(xlim2)\nplt.ylim(ylim2)\n\nplt.gca().set_aspect('equal')\nplt.gca().grid(True)\nplt.legend()","32209b49":"X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=1500, \n                                                        stratify=y1, random_state=SEED)\n\nprint('Test set size: {0}x{1}'.format(*X1_test.shape))\nprint('Training set size: {0}x{1}'.format(*X1_train.shape))","445d5646":"hp_space_clf1 = {\n    'poly': {\n        'degree': 1 + hp.randint('degree', 1 + 1)\n    }, \n    'clf': {\n        'C': hp.loguniform('C', -4.0*np.log(10.0), 4.0*np.log(10.0)), \n        'class_weight': hp.choice('class_weight', [None, 'balanced'])\n    }\n}\n\n# Draw random sample to see if hyperspace is correctly defined\nho_sample(hp_space_clf1)","e6bff3de":"def f_clf1(hps):\n    \"\"\"\n    Constructs estimator\n    \n    Parameters:\n    ----------------\n    hps : sample point from search space\n    \n    Returns:\n    ----------------\n    model : sklearn.Pipeline.pipeline with hyperparameters set up as per hps\n    \"\"\"\n    \n    # Assembing pipeline\n    model = Pipeline([\n        ('poly', PolynomialFeatures(**hps['poly'], include_bias=False)), \n        ('scale', StandardScaler()), \n        ('clf', LogisticRegression(**hps['clf'], solver='liblinear', max_iter=25000, random_state=SEED))\n    ])\n    \n    return model","38f10355":"def f_to_min1(hps, X, y, ncv=5):\n    \"\"\"\n    Target function for optimization\n    \n    Parameters:\n    ----------------\n    hps : sample point from search space\n    X : feature matrix\n    y : target array\n    ncv : number of folds for cross-validation\n    \n    Returns:\n    ----------------\n    : target function value (negative mean cross-val ROC-AUC score)\n    \"\"\"\n    \n    model = f_clf1(hps)\n    cv_res = cross_val_score(model, X, y, cv=StratifiedKFold(ncv, random_state=SEED), \n                             scoring='roc_auc', n_jobs=-1)\n    \n    return -cv_res.mean()","deaeca96":"trials_clf1 = Trials()\nbest_clf1 = fmin(partial(f_to_min1, X=X1_train, y=y1_train), \n                 hp_space_clf1, algo=tpe.suggest, max_evals=100, \n                 trials=trials_clf1, rstate=np.random.RandomState(SEED))","c40efb49":"# Building and fitting classifier with best parameters\nclf1 = f_clf1(space_eval(hp_space_clf1, best_clf1)).fit(X1_train, y1_train)\n\n# Calculating performance on validation set\nclf1_val_score = roc_auc_score(y1_test, clf1.predict_proba(X1_test)[:, 1])\nprint('Cross-val score: {0:.5f}; validation score: {1:.5f}'.\\\n      format(-trials_clf1.best_trial['result']['loss'], clf1_val_score))\nprint('Best parameters:')\nprint(space_eval(hp_space_clf1, best_clf1))","9d16b08e":"def f_wrap_space_eval(hp_space, trial):\n    \"\"\"\n    Utility function for more consise optimization history extraction\n    \n    Parameters:\n    ----------------\n    hp_space : hyperspace from which points are sampled\n    trial : hyperopt.Trials object\n    \n    Returns:\n    ----------------\n    : dict(\n        k: v\n    ), where k - label of hyperparameter, v - value of hyperparameter in trial\n    \"\"\"\n    \n    return space_eval(hp_space, {k: v[0] for (k, v) in trial['misc']['vals'].items() if len(v) > 0})\n\n\ndef f_unpack_dict(dct):\n    \"\"\"\n    Unpacks all sub-dictionaries in given dictionary recursively. There should be no duplicated keys \n    across all nested subdictionaries, or some instances will be lost without warning\n    \n    Parameters:\n    ----------------\n    dct : dictionary to unpack\n    \n    Returns:\n    ----------------\n    : unpacked dictionary\n    \"\"\"\n    \n    res = {}\n    for (k, v) in dct.items():\n        if isinstance(v, dict):\n            res = {**res, **f_unpack_dict(v)}\n        else:\n            res[k] = v\n            \n    return res","e75b0c74":"fig0 = plt.figure(figsize=(16, 8))\ngs = gridspec.GridSpec(nrows=4, ncols=4)\n\n# ================================\n# Plotting optimization history\n\nax = fig0.add_subplot(gs[:2, :2])\nax.plot(range(1, len(trials_clf1) + 1), [-x['result']['loss'] for x in trials_clf1], \n        color='red', marker='.', linewidth=0)\n\nax.set_xlabel('Iteration', fontsize=12)\nax.set_ylabel('ROC-AUC', fontsize=12)\nax.set_title('Optimization history', fontsize=14)\n\nax.grid(True)\n\n# ================================\n# Plotting sampled points\n\nsamples = [f_unpack_dict(f_wrap_space_eval(hp_space_clf1, x)) for x in trials_clf1.trials]\n\nax = fig0.add_subplot(gs[2:, 0])\nax.hist(np.log([x['C'] for x in samples]), bins=20, rwidth=0.9, \n        range=(-4*np.log(10), 4*np.log(10)))\n\nax.set_xlabel('$\\ln C$', fontsize=12)\nax.set_ylabel('Counts', fontsize=12)\n\nax.grid(True)\n\n# ----------------\n\nax = fig0.add_subplot(gs[2, 1])\nsns.countplot(x=[x['degree'] for x in samples], ax=ax)\n\nax.set_xlabel('polynomial features degree', fontsize=12)\nax.set_ylabel('Counts', fontsize=12)\n\nax.grid(True)\n\n# ----------------\n\nax = fig0.add_subplot(gs[3, 1])\nsns.countplot(x=[str(x['class_weight']) for x in samples], ax=ax)\n\nax.set_xlabel('class_weight', fontsize=12)\nax.set_ylabel('Counts', fontsize=12)\n\nax.grid(True)\n\n# ================================\n# Plotting decision boundary\n\nax = fig0.add_subplot(gs[:, 2:])\n\n# plt.contour is a bit tricky with legend labels, so it has to be constructed manually\nlegends = []\n\n# Scattering sampled points and keeping their legend items\npts_class0, = ax.plot(X1[~flag, 0], X1[~flag, 1], label='Class 0', \n                      linewidth=0, marker='.', markersize=3, color='orange')\nlegends.append(pts_class0)\npts_class1, = ax.plot(X1[flag, 0], X1[flag, 1], label='Class 1', \n                      linewidth=0, marker='.', markersize=3, color='green')\nlegends.append(pts_class1)\n\n# Decision boundary is build via plt.contour with single level line = 0.5, legend item added separately\ncntr = ax.contour(xy_mesh2[0], xy_mesh2[1], \n                  clf1.predict_proba(xy_mesh2_features)[:, 1].\\\n                      reshape(xy_mesh2[0].shape[0], -1), \n                  levels = [0.5], colors='red', linewidths=2)\nlegends.append(cntr.legend_elements()[0][0])\n\nax.set_xlabel('$x$', fontsize=12)\nax.set_ylabel('$y$', fontsize=12)\nax.set_title('Classifier decision boundary', fontsize=14)\n\nax.set_xlim(xlim2)\nax.set_ylim(ylim2)\n\nax.set_aspect('equal')\nax.grid(True)\nax.legend(legends, ['Class 0', 'Class 1', 'Decision boundary'])\n\n# ================================\n\nplt.tight_layout()","217aec72":"hp_space_clf2 = {\n    # type refers to classifier type: either logit or SVM\n    'clf_type': hp.choice('clf_type', [\n        {\n            'type': 'logit', \n            'poly': {\n                'degree': 1 + hp.randint('degree', 3 + 1), \n                'interaction_only': hp.choice('interaction_only', [False, True])\n            }, \n            'clf': {\n                'C': hp.loguniform('logit.C', -4.0*np.log(10.0), 4.0*np.log(10.0)), \n                'class_weight': hp.choice('logit.class_weight', [None, 'balanced'])\n            }\n        }, \n        {\n            'type': 'SVM', \n            'clf': {\n                'C': hp.loguniform('svm.C', -4.0*np.log(10.0), 4.0*np.log(10.0)), \n                'class_weight': hp.choice('svm.class_weight', [None, 'balanced']), \n                'kernel': 'rbf', \n                'gamma': hp.choice('svm.gamma', ['auto', 'scale'])\n            }\n        }\n    ])\n}\n\n# Draw random sample to see if hyperspace is correctly defined\nho_sample(hp_space_clf2)","aee18688":"def f_clf2(hps):\n    \"\"\"\n    Constructs estimator\n    \n    Parameters:\n    ----------------\n    hps : sample point from search space\n    \n    Returns:\n    ----------------\n    model : sklearn.Pipeline.pipeline with hyperparameters set up as per hps\n    \"\"\"\n    \n    if hps['clf_type']['type'] == 'logit':\n        model = Pipeline([\n            ('poly', PolynomialFeatures(**hps['clf_type']['poly'], include_bias=False)), \n            ('scale', StandardScaler()), \n            ('clf', LogisticRegression(**hps['clf_type']['clf'], \n                                       solver='liblinear', max_iter=25000, random_state=SEED))\n        ])\n    elif hps['clf_type']['type'] == 'SVM':\n        model = Pipeline([\n            ('scale', StandardScaler()), \n            ('clf', SVC(**f_unpack_dict(hps['clf_type']['clf']), probability=True, random_state=SEED))\n        ])\n    else:\n        raise KeyError('Unknown classifier type hyperparameter value: {0}'.format(hps['clf_type']['type']))\n    \n    return model","a85487b7":"def f_to_min2(hps, X, y, ncv=5):\n    \"\"\"\n    Target function for optimization\n    \n    Parameters:\n    ----------------\n    hps : sample point from search space\n    X : feature matrix\n    y : target array\n    ncv : number of folds for cross-validation\n    \n    Returns:\n    ----------------\n    : dict(\n        'loss' : target function value (negative mean cross-validation ROC-AUC score)\n        'cv_std' : cross-validation ROC-AUC score standard deviation\n        'status' : status of function evaluation\n    )\n    \"\"\"\n    \n    model = f_clf2(hps)\n    cv_res = cross_val_score(model, X, y, cv=StratifiedKFold(ncv, random_state=SEED), \n                             scoring='roc_auc', n_jobs=-1)\n    \n    return {\n        'loss': -cv_res.mean(), \n        'cv_std': cv_res.std(), \n        'status': STATUS_OK\n    }","418dabc2":"trials_clf2 = Trials()\nbest_clf2 = fmin(partial(f_to_min2, X=X1_train, y=y1_train), \n                 hp_space_clf2, algo=tpe.suggest, max_evals=300, \n                 trials=trials_clf2, rstate=np.random.RandomState(SEED))","5175ddad":"# Building and fitting classifier with best parameters\nclf2 = f_clf2(space_eval(hp_space_clf2, best_clf2)).fit(X1_train, y1_train)\n\n# Calculating performance on validation set\nclf2_val_score = roc_auc_score(y1_test, clf2.predict_proba(X1_test)[:, 1])\nprint('Cross-val score: {0:.5f} +\/- {1:.5f}; validation score: {2:.5f}'.\\\n      format(-trials_clf2.best_trial['result']['loss'], trials_clf2.best_trial['result']['cv_std'], \n             clf2_val_score))\nprint('Best parameters:')\nprint(space_eval(hp_space_clf2, best_clf2))","3862a688":"fig0 = plt.figure(figsize=(16, 8))\ngs = gridspec.GridSpec(nrows=4, ncols=4)\n\n# ================================\n# Plotting optimization history\n\nax = fig0.add_subplot(gs[:2, :2])\nax.plot(range(1, len(trials_clf2) + 1), [-x['result']['loss'] for x in trials_clf2], \n        color='red', marker='.', linewidth=0)\n\nax.set_xlabel('Iteration', fontsize=12)\nax.set_ylabel('ROC-AUC', fontsize=12)\nax.set_title('Optimization history', fontsize=14)\n\nax.grid(True)\n\n# ================================\n# Plotting sampled points\n\nsamples = [f_unpack_dict(f_wrap_space_eval(hp_space_clf2, x)) for x in trials_clf2.trials]\n\nax = fig0.add_subplot(gs[2:, 0])\nsns.countplot(x=[x['type'] for x in samples], ax=ax)\n\nax.set_xlabel('Classifier type', fontsize=12)\nax.set_ylabel('Counts', fontsize=12)\n\nax.grid(True)\n\n# ----------------\n\nax = fig0.add_subplot(gs[2, 1])\nax.hist(np.log([x['C'] for x in samples if x['type'] == 'SVM']), bins=20, rwidth=0.9, \n        range=(-4*np.log(10), 4*np.log(10)))\n\nax.set_xlabel('SVM, $\\ln C$', fontsize=12)\nax.set_ylabel('Counts', fontsize=12)\n\nax.grid(True)\n\n# ----------------\n\nax = fig0.add_subplot(gs[3, 1])\nax.hist(np.log([x['C'] for x in samples if x['type'] == 'logit']), bins=20, rwidth=0.9, \n        range=(-4*np.log(10), 4*np.log(10)))\n\nax.set_xlabel('logit, $\\ln C$', fontsize=12)\nax.set_ylabel('Counts', fontsize=12)\n\nax.grid(True)\n\n# ================================\n# Plotting decision boundary\n\nax = fig0.add_subplot(gs[:, 2:])\n\n# plt.contour is a bit tricky with legend labels, so it has to be constructed manually\nlegends = []\n\n# Scattering sampled points and keeping their legend items\npts_class0, = ax.plot(X1[~flag, 0], X1[~flag, 1], label='Class 0', \n                      linewidth=0, marker='.', markersize=3, color='orange')\nlegends.append(pts_class0)\npts_class1, = ax.plot(X1[flag, 0], X1[flag, 1], label='Class 1', \n                      linewidth=0, marker='.', markersize=3, color='green')\nlegends.append(pts_class1)\n\n# Decision boundary is build via plt.contour with single level line = 0.5, legend item added separately\ncntr = ax.contour(xy_mesh2[0], xy_mesh2[1], \n                  clf2.predict_proba(xy_mesh2_features)[:, 1].\\\n                      reshape(xy_mesh2[0].shape[0], -1), \n                  levels = [0.5], colors='red', linewidths=2)\nlegends.append(cntr.legend_elements()[0][0])\n\nax.set_xlabel('$x$', fontsize=12)\nax.set_ylabel('$y$', fontsize=12)\nax.set_title('Classifier decision boundary', fontsize=14)\n\nax.set_xlim(xlim2)\nax.set_ylim(ylim2)\n\nax.set_aspect('equal')\nax.grid(True)\nax.legend(legends, ['Class 0', 'Class 1', 'Decision boundary'])\n\n# ================================\n\nplt.tight_layout()","f4faf223":"dset = datasets.fetch_california_housing()\ntot_df = pd.DataFrame(\n    {k: dset['data'][:, ic] for (k, ic) in zip(dset['feature_names'], range(dset['data'].shape[1]))}\n)\ntarget = dset['target']\n\ntot_df.head()","b2a9760b":"tot_df.drop(['Latitude', 'Longitude'], axis=1, inplace=True, errors='ignore')","23f9d25a":"orig_features = list(tot_df.columns)\nprint(orig_features)\n\nlog_features = []\nfor x_feature in orig_features:\n    log_features.append('_'.join([x_feature, 'log']))\n    tot_df[log_features[-1]] = np.log(tot_df[x_feature])\n    \nprint(log_features)","b7ab2bb1":"Xdf_tv, Xdf_test, y_tv, y_test = train_test_split(tot_df, target, test_size=5000, random_state=SEED)\nXdf_train, Xdf_val, y_train, y_val = train_test_split(Xdf_tv, y_tv, test_size=5000, random_state=SEED)\n\nprint('Test set size: {0}x{1}'.format(*Xdf_test.shape))\nprint('Validation set size: {0}x{1}'.format(*Xdf_val.shape))\nprint('Training set size: {0}x{1}'.format(*Xdf_train.shape))","55e24fc2":"hp_space_reg = {\n    'use_aveoccup': hp.choice('use_aveoccup', [False, True]), \n    'use_logs': hp.choice('use_logs', [False, True]), \n    'poly': {\n        'degree': 1 + hp.randint('degree', 2 + 1)\n    }, \n    'clf': {\n        'max_leaf_nodes': hp.choice('max_leaf_nodes', [\n            {\n                'max_leaf_nodes': None\n            }, \n            {\n                'max_leaf_nodes': ho_scope.int(hp.quniform('max_leaf_nodes_constrained', 100, 500, q=100))\n            }\n        ]), \n        'min_samples_split': 2 + hp.randint('min_samples_split', 8 + 1), \n        'min_samples_leaf': 1 + hp.randint('min_samples_leaf', 9 + 1), \n        'max_features': hp.uniform('rf.max_features', 0.2, 0.8)\n    }\n}\n\n# Draw random sample to see if hyperspace is correctly defined\nho_sample(hp_space_reg)","aef6ebc5":"def f_reg_rf(hps, n_trees=100, oob_score=True):\n    \"\"\"\n    Constructs estimator\n    \n    Parameters:\n    ----------------\n    hps : sample point from search space\n    n_trees : int > 0, number of trees in RandomForestClassifier\n    oob_score : boolean, whether to calculate R2 score on OOB samples\n    \n    Returns:\n    ----------------\n    model : sklearn.Pipeline.pipeline with hyperparameters set up as per hps\n    \"\"\"\n    \n    global orig_features, log_features\n    \n    if hps['use_aveoccup']:\n        if hps['use_logs']:\n            features = orig_features + log_features\n        else:\n            features = orig_features\n    else:\n        if hps['use_logs']:\n            features = orig_features[:-1] + log_features[:-1]\n        else:\n            features = orig_features[:-1]\n\n    # ================\n    \n    model = Pipeline([\n        ('feature_selector', ColumnTransformer(transformers=[\n            (\n                'select', \n                PolynomialFeatures(**hps['poly'], interaction_only=True, include_bias=False), \n                features\n            )\n        ], remainder='drop')), \n        ('clf', RandomForestRegressor(**f_unpack_dict(hps['clf']), n_estimators=n_trees, oob_score=oob_score, \n                                      criterion='mse', bootstrap=True, n_jobs=-1, random_state=SEED))\n    ])\n    \n    return model","aa770ae9":"def f_to_min_reg(hps, X, y, X_test, y_test, n_trees=100):\n    \"\"\"\n    Target function for optimization\n    \n    Parameters:\n    ----------------\n    hps : sample point from search space\n    X : training feature matrix\n    y : training target array\n    X_test : validation feature matrix\n    y_test : validation target array\n    n_trees : int > 0, number of trees in RandomForestClassifier\n    \n    Returns:\n    ----------------\n    : dict(\n        'loss' : target function value (negative OOB R2 score)\n        'status' : status of function evaluation\n        'test_rmse' : validation set RMSE\n        'test_r2' : validation set R2\n    )\n    \"\"\"\n    \n    model = f_reg_rf(hps, n_trees, True).fit(X, y)\n    \n    return {\n        'loss': -model.steps[-1][1].oob_score_, \n        'status': STATUS_OK, \n        'test_rmse': np.sqrt(mean_squared_error(y_test, model.predict(X_test))), \n        'test_r2': r2_score(y_test, model.predict(X_test))\n    }","f230b658":"trials_reg = Trials()\nbest_reg = fmin(partial(f_to_min_reg, X=Xdf_train, y=y_train, X_test=Xdf_val, y_test=y_val), \n                hp_space_reg, algo=tpe.suggest, max_evals=20, \n                trials=trials_reg, rstate=np.random.RandomState(SEED))","7b5c3f58":"best_reg = fmin(partial(f_to_min_reg, X=Xdf_train, y=y_train, X_test=Xdf_val, y_test=y_val), \n                hp_space_reg, algo=tpe.suggest, max_evals=200, \n                trials=trials_reg, rstate=np.random.RandomState(SEED))","89d9721b":"# Displaying best result\nprint('OOB R2 score: {0:.4f}; validation set R2: {1:.4f}, validation set RMSE: {2:.4f}'.\\\n      format(-trials_reg.best_trial['result']['loss'], trials_reg.best_trial['result']['test_r2'], \n             trials_reg.best_trial['result']['test_rmse']))\nprint('Best parameters:')\nprint(space_eval(hp_space_reg, best_reg))","54f5f5b1":"fig0 = plt.figure(figsize=(16, 8))\nfig0.suptitle('Optimization history', fontsize=16)\ngs = gridspec.GridSpec(nrows=2, ncols=4)\n\n# ================================\n# Plotting optimization histories\n\nax = fig0.add_subplot(gs[0, :2])\nax.plot(range(1, len(trials_reg) + 1), [-x['result']['loss'] for x in trials_reg], \n        color='red', marker='.', linewidth=0, label='OOB')\nax.plot(range(1, len(trials_reg) + 1), [x['result']['test_r2'] for x in trials_reg], \n        color='blue', marker='.', linewidth=0, label='validation set')\n\nax.set_xlabel('Iteration', fontsize=12)\nax.set_ylabel('$R_2$', fontsize=12)\n\nax.legend()\nax.grid(True)\n\n# --------------------\n\nax = fig0.add_subplot(gs[1, :2])\nax.plot(range(1, len(trials_reg) + 1), [x['result']['test_rmse'] for x in trials_reg], \n        color='red', marker='.', linewidth=0)\n\nax.set_xlabel('Iteration', fontsize=12)\nax.set_ylabel('Validation set RMSE', fontsize=12)\n\nax.grid(True)\n\n# ================================\n# Plotting sampled points\n\nsamples = [f_unpack_dict(f_wrap_space_eval(hp_space_reg, x)) for x in trials_reg.trials]\n\n# ----------------\n\nax = fig0.add_subplot(gs[0, 2])\nsns.countplot(x=[x['use_aveoccup'] for x in samples], ax=ax)\n\nax.set_xlabel('use_aveoccup', fontsize=12)\nax.set_ylabel('Counts', fontsize=12)\n\nax.grid(True)\n\n# ----------------\n\nax = fig0.add_subplot(gs[0, 3])\nsns.countplot(x=[x['use_logs'] for x in samples], ax=ax)\n\nax.set_xlabel('use_logs', fontsize=12)\nax.set_ylabel('Counts', fontsize=12)\n\nax.grid(True)\n\n# ----------------\n\nax = fig0.add_subplot(gs[1, 2])\nsns.countplot(x=[x['degree'] for x in samples], ax=ax)\n\nax.set_xlabel('polynomial features degree', fontsize=12)\nax.set_ylabel('Counts', fontsize=12)\n\nax.grid(True)\n\n# ----------------\n\nax = fig0.add_subplot(gs[1, 3])\nsns.countplot(x=[str(x['max_leaf_nodes']) for x in samples], ax=ax)\n\nax.set_xlabel('max_leaf_nodes', fontsize=12)\nax.set_ylabel('Counts', fontsize=12)\n\nax.grid(True)\n\n# # ================================\n\nfig0.tight_layout(rect=[0, 0, 1, 0.96])","f84879ff":"reg = f_reg_rf(space_eval(hp_space_reg, best_reg), n_trees=2000, oob_score=True)\nreg.fit(pd.concat([Xdf_train, Xdf_val], axis=0), np.hstack([y_train, y_val]))\nreg_pred = reg.predict(Xdf_test)\n\nprint('Model OOB R2 score: {0:.4f}'.format(reg.steps[-1][1].oob_score_))\nprint('Model test set R2 score: {0:.4f}'.format(r2_score(y_test, reg_pred)))\nprint('Model test set RMSE: {0:.4f}'.format(np.sqrt(mean_squared_error(y_test, reg_pred))))","deb89c7c":"fig0 = plt.figure(figsize=(16, 4))\nfig0.suptitle('Test set performance', fontsize=16)\ngs = gridspec.GridSpec(nrows=1, ncols=2)\n\n# ================================\n\nax = fig0.add_subplot(gs[0, 0])\nax.hist([y_test, reg_pred], bins=35, range=(0, 5), rwidth=0.95, label=['ground truth', 'prediction'], \n        density=True)\n\nax.set_xlabel('target', fontsize=12)\nax.set_ylabel('density', fontsize=12)\n\nax.grid(True)\nax.legend()\n\n# ================================\n\nax = fig0.add_subplot(gs[0, 1])\nax.hist(y_test - reg.predict(Xdf_test), bins=50, rwidth=0.9, density=True)\n\nax.set_xlabel('error', fontsize=12)\nax.set_ylabel('density', fontsize=12)\n\nax.grid(True)","c39ca851":"Test set accuracy and cross-validation scores are consistent. We also see that polynomial features of degree 2 are selected as best value.","f4532080":"Indeed SVM with RBF kernel works wonderfully on this dataset. Now we take a look at optimization history.","58132032":"#### Preparing new features","0d20eb24":"Obviously, this represents prior info that $C$ is uniformly distributed on interval $[0.01, 100]$. However, this is not the best prior for $C$. What is more important is the magnitude of $C$, so let's instead set up our prior as uniformly distributed logarithm:","9e28c49c":"#### Running optimization","b3849af0":"  - There is a rule of thumb that for proper search space exploration you would require around 25 runs per dimension to converge. It is entirely heuristical, but might help to estimate how many steps you would need;\n  - Always back up your results into `Trials` object, and possibly pickle it to hard drive. It is very frustrating to lose several hours of optimization due to a power cut or OS freezing;\n  - Check carefully sign of your target function. Shouldn't a minus be there?\n  - Don't use full capacity of your model for hyperparameter tuning. This means: your don't need 1000 trees in your XGBoost, nor do you need all 8 million samples of your training set. It is very unlikely that optimization on a reduced set\/capacity would yield significantly different results, while computation time would be much lower;\n  - **MOST IMPORTANT**: Never substitute proper feature engineering with extensive hyperparameter optimization. Former is much more important - untuned model with great features easily outperforms heavily optimized model without intelligent design. Do some EDA, try things, find out what works, what might work and what is garbage, and then, before you go to sleep - launch optimization for the night. Not the other way around.","8038f46a":"Since there are only two features, we can easily visualize them on 2D plane.","b4f0da40":"Latest version on package is available on PyPi, and can be easily installed with `pip` or `conda`:","bfed1073":"Note, that since we labeled our hyperparameters with the same keywords as they passed to estimator or transformer constructor - we can just use unpacking operator `**`, without using explicit statements like `LogisticRegression(C=hps['C'], ...`. It is a good practice to ensure you wont't forget to pass a hyperparameter value to model.","b3a59b37":"#### Splitting dataset","67775d71":"**Pros**:\n\n  - More efficient search space exploration, likely to yield better results in less time;\n  - Ability to build search spaces of arbitrary complexity, mixing all possible types of variables and branching into subspaces as needed;\n  - Ability to make custom-tailored target function, so that you can optimize any aspect of your model as you desire.\n\n**Cons**:\n  - Complex search spaces might take a lot of time to get properly explored. Each additional dimension, on average, would require additional runs for optimization to converge;\n  - Hyperparameters are treated independently - that might be one of greatest weaknesses of `hyperopt`. If some of hyperparameters are tightly coupled with strong anticorrelation - you might even get worse results than in case of simple Random Search;\n  - TPE takes some time to analyze evaluation history and suggest a new point, and the longer the history - the longer the time it takes. This is negligibly small if one step takes several minutes, but might be noticeable if evaluation is quick and runs for many trials.","7f13b168":"The [GitHub repo](https:\/\/github.com\/hyperopt\/hyperopt) contains all the necessary documentation and some examples. To set up and run optimization problem you need to import at least the following:","bec9b465":"One of the most exciting features of `hyperopt` is it's ability to design search spaces of arbitary complexity. You can mix real-valued parameters, integers, boolean variables and even strings, and you can also specify some prior information you have on them in form of probability distribution.\n\nOn Github repo there is a [basic tutorial](https:\/\/github.com\/hyperopt\/hyperopt\/wiki\/FMin) which covers essential steps to some extent.\n\nSimplest example. You are optimizing regularization parameter $C$ for Logistic Regression. Your one-dimensional hyperspace might be determined as follows:","0aa3fa44":"We'll practice on rather simple problem. We'll generate points of two classes from known distribution, which is a mixture of two 2-D normal distributions.","080a6dcc":"All right, we added logarithms of features. We also see that `AveOccup` (and it's logarithm) are the last on the lists of features, which will be used later.","1b37389d":"Branching search space into subspaces can be done with `hp.choice`. It's elements might be atomic values, or hyperspaces themselves as well!","82b9bf61":"<center>\n<img src=\"https:\/\/habrastorage.org\/files\/fd4\/502\/43d\/fd450243dd604b81b9713213a247aa20.jpg\" \/>\n    \n## [mlcourse.ai](https:\/\/mlcourse.ai) \u2013 Open Machine Learning Course \n\n<center>Author: Alexey Serov (@fanvacoolt)","82602040":"Since our hyperparameters space expanded significantly, to explore it thoroughly, we'll run it for 300 rounds.","cd1f8476":"Unlike in previous models, we'll use OOB $R_2$ score as target, so that we don't need to train 5 forests for cross-validation. This speeds up things considerably. We also use validation set to calculate both RMSE and $R_2$ on it. Note that, again, since we want $R_2$ to be maximized - the minus sign must be added.","a771ea20":"This is the algorithm used to adjust probability density in search space based on target function evaluation history. The internal workings of it are well beyond the scope of this tutorial. Describing it in two words: it uses bayesian approach to update posterior hyperparameters distribution. If you would like to dive in - [here](https:\/\/papers.nips.cc\/paper\/4443-algorithms-for-hyper-parameter-optimization.pdf) is a good start.","5b962bf3":"#### Running optimization","691e34c5":"#### Target function","1603df3a":"#### Model performance on test set","4690bc86":"Here we're splitting dataset into train, validation and test parts.","81efae61":"#### Setting up animation","6011b00e":"###### Drawing samples","d70740c2":"As usual, split dataset into two parts.","bbdd33de":"From the report we see that maximum indeed is close to $(3, 4)$, and it took $69$ function evaluations to converge, even if there are only $11$ iterations.","9cb607af":"### Analyzing results","b66fd7d2":"We used here `hyperopt.pyll.scope.int` function, because `hp.quniform` returns real value, even if it is quantified by integer.\n\nNote that when our parameter search space is multidimensional - we define it as dictionary. There are two label per each parameter - key of a dictionary and label inside `hp.***` distribution. You can name them as you please, but keep in mind that duplicate labels are not allowed.\n\n`hyperopt.pyll.stochastic.sample` draws a random point from search space, as you can see below.","becf75de":"We need a little helper function here. If classifier type happens to be SVM - there will be nested sub-dictionaries in it, so we can't use unpacking operator starightaway. `f_unpack_dict` deals with this, unpacking all the nested subdictionaries.","c322479d":"All right, enough with the theory. Let's run some models!","f2263aea":"## What's next?","3b3e5b45":"### Visualizing results","aebbf4b7":"###### Function to draw samples","e2650268":"Well, distributions are not exactly same, but look fairly close. For some reason our regressor have no low-value predictions at all - worth investigating. But that's for another time.","4d340331":"## Case 3: Tuning regression and backing up optimization history.","36abd4be":"#### Defining model","9488d3cb":"Sooner or later (most likely sooner) in machine learning we all come to the phase of model hyperparameters tuning. Even the most simple linear models, like logistic regression, could benefit from properly chosen regularization coefficient, and for more complex ones the efficiency gain could be even more dramatic. \n\nHyperparameters are set by a data scientist, and there is no definitive way to know beforehand which values are optimal (though there are some heuristics to ease the search). You can only try different values and see which one is best.\nThere are three classic ways to do this:\n  - **Grid search**, implemented, for example, in `GridSearchCV` class from `sklearn.model_selection` module. No doubt you know it already. Just feed in the model, the list of parameters you want to tune plus their possible values and the data, along with cross-validation scheme and desired metrics, and go get some coffee while it crunches all possible combinations of hyperparameters. This could take a **LONG** time even for simpler models.\n  - **Random search**. Implementation could be found in `RandomizedSearchCV` class from `sklearn.model_selection` module. The name speaks for itself - feed it the same data, only this time you might not only list the values, but give distributions as well - and it will sample random points from hyperparameters space. Not guaranteed to find the best combination, but is deemed much more cost-effective than grid search.\n  - **Numerical optimization**. There are many algorithms for numerical search of a function minimum (most well known to us is gradient descent); however, they are poorly suited for hyperparameters optimization. These algorithms are usually intended to minimize functions like $f:\\mathbb{R}^n\\to\\mathbb{R}$, while hyperparameters spaces might have both real-valued and discrete components, and even have distinct dimensionalities for different hyperparameters choice.","6c624ab9":"The classification quality is very high. Seems that our polynomial features are kicking off here!","82aa1093":"We can clearly see the propensity of TPE-driven optimization to sample points with lower target function value. Sampled points histograms are clearly non-uniform, so we see posterior densities adjusted. Note also that sometimes TPE samples points from regions with poor performance, which makes it unlikely to get stuck in local minimum. Win-win!","43303661":"### Generating samples","de669866":"### Visualizing hyperopt sampling strategy","fe781892":"#### Grid Search and Random Search","e6603295":"One last example to show you how you can construct complex hyperspaces. Say, you are not sure which classifier to use - Random Forest or Logistic Regression. You can treat it as a hyperparameter as well. We can define additional `type` parameter like this:","137a58ba":"### Tree of Parzen Estimators (TPE)","b070d8c3":"Let's read the data into pandas DataFrame and take initial look.","915597be":"#### Defining search space","99cb7cf5":"#### Plotting optimization history","d8a9f137":"We can pickle `Trials` object to hard drive and safely shut down the computer, and afterwards read it back and resume optimization. Given that it usually takes a lot of time to properly explore complex hyperspaces you might want to do that in several runs, when you do not use your computer heavily.\n\nTake a look on best hyperparameters set.","82f9786e":"Let's run hyperopt for some time and see how sampling distribution evolves for both `hyperopt` and random search.","84dc5ebd":"Now that's a complex hyperspace! It branches into two subspaces of different dimensionalities based on `clf_type`, and these subspaces are quite distinct. Moreover, subspace for SVM branches once more when we choose more appropriate kernel.\n\n**Important notes**:\n  - Here we have parameters `C` and `class_weight` in both logit and SVM constructors. Note how we preceded labels in hyperspace elements with prefixes. If we don't do that, we'll get `DuplicateLabel` exception upon trying to run `fmin`. `hyperopt` does not allow this, so each *internal label* (i.e. label within `hp.***` object) must be unique. However, sample from hyperspace is indexed by key values from dictionary, and these might be duplicated without any problem, as long as you don't have repeated keys within one (sub)-dictionary. \n  - Why don't we use a single dimension for `C` instead of making two separate dimensions for each classifier? Well, it is clear that optimal value of `C` for logit might be quite different of that for SVM. Distribution of common `C` gets adjusted with every trial of model, no matter classifier type. Eventually, of course, one of them will prevail, making `C` assume optimal value for it, but do we really need this additional obstacles for algorithm convergence?","799454e9":"Below are two helper functions that help to deal with how `hyperopt` stores trials. First wraps evaluation history into list of dicts, so we can easily extract parameter we're interested in from it. Second takes a dict and unpacks recursively any nested sub-dicts that it might have, which useful with complex hyperspaces that branch into different subspaces.","7e73f821":"## Importing modules","15e1b5af":"Fitting linear logistic regression is no fun. It has only one and a half parameter to tweak, so we won't see much action there. \n\nIt is clear that our dataset has highly non-linear separation boundary. We will take this into account by adding polynomial features to our classifier. We also won't forget that linear models are sensitive to features magnitudes, hence a pipeline with a scaler would be desirable.\n\nSo we create a 3-dimensional search space: degree of polynomial features, which for now we assume can be either $1$ or $2$, logit regularization constant $C$ and `class_weight` parameter, since we have a slight classes disbalance.","4c821a47":"We'll also make target function a bit more complex this time. `fmin` can accept functions which return not only a single number but also a dictionary. Two items are mandatory: `'loss'` - target function value, and `'status'`, which tells if function evaluated correctly. We'll add one more value: standard deviation of CV score to observe it's consistency.\n\n**Note**: `'status'` element must return `0` if function evaluation was successful; however, it is better to use constant `STATUS_OK` supplied by `hyperopt` for better readability.","2bb940cd":"## Using `hyperopt` to tune model hyperparameters.","86452698":"All right, now it's even more illustrative (don't forget to press \"run\" button above). We sampled 2000 points using random search and hyperopt, divided them in batches of 100 and plotted each batch separately. With every passing iteration points tend to cluster around global maximum for hyperopt, while any batch of points for Random Search is indistinguishable from any other (as expected).\n\nAll right. I hope by now you understand what does hyperopt do, and why we might want to use it. Let's see what we have to do to make it work, and then we'll apply it to machine learning problems.","aa493654":"### Defining search space","dde7219c":"# <center> Using `hyperopt` for hyperparameters optimization","a847e927":"#### Plotting optimization history","b4e31360":"Now, depending on hyperparameters selection, our model will operate only on given set of features, and should one choice be more efficient - it will be favored by TPE sampler. Also, `n_estimators` is not tuned, since obviously the more trees there are - the higher is accuracy. We'll fix it to $100$ during optimization phase - it should not affect optimal hyperparameters choosing, but greatly increases performance.","5e306061":"The function has a global maximum somewhere around $(3, 4)$ and local maximum somewhere close to $(-3.5, 3.5)$. Now we'll perform a grid search, a random search, a numerical maximization using [L-BFGS-B method](https:\/\/en.wikipedia.org\/wiki\/Limited-memory_BFGS) and hyperopt optimization.\n\nSay, for some reason we know nothing of variable $x$, we are only sure that it is constrained to $[-10, 10]$ interval, but we have strong evidence saying that maximum point of $y$ is more likely to lie closer to $0$. For stochastic algorithms (Random Search and hyperopt), we will assign uniform prior distribution to $x$ and normal to $y$: \n$$x \\sim \\mathrm{U}[-10, 10], y \\sim \\mathcal{N}(0, 5).$$","81254449":"## Conclusion","ae9c4763":"### Running optimization and backing up results","241f3013":"### Preparation","f028d9d7":"We define two hyperparameters: one for whether to use `AveOccup`, another for whether to use logarithms of features. We also add polynomial features generation, and some tree-controlling hyperparameters (for example, `max_leaf_nodes` might be unconstrained or limited).","04049b36":"#### Defining model","43ce3aac":"### `Trials` object","e44196c4":"All right, now our prior for $C$ has a logarithm uniformly distributed on $[-4\\ln{10}, 4\\ln{10}]$, and $C$ is constrained to $[10^{-4}, 10^{4}]$.\n\nSay, you decided to switch to Random Forest. It has many parameters you might tune. Suppose you want to tune number of trees, optimization criterion and number of features for bagging. Let's do it:","4242033d":"Let's consider a toy example for a start. We'll take the following function in 2D plane:\n\n$$f(x, y) = 5e^{-\\frac{(x - 3)^2 + (y - 4)^2}{4}} + 2e^{-\\frac{\\sqrt{(x + 3)^2 + (y - 3)^2}}{2}} + e^{-\\sin\\left({\\sqrt{(x - 5)^2 + (y + 3)^2}}\\right)},$$\n\nand see how different methods look for it's maximum.","2eb24b1b":"Which function do we pass to `fmin` as target? Well, since we are in machine learning business, and aim to pick best possible hyperparameter combination - our function must accept a sample from hyperspace and return desired metric value. In other words, it receives a set of hyperparameters values, creates model, then runs cross-validation and returns mean metric value, or fits model on training set and returns metric on validation set - it is you who determines this.\n\n**HINT**: You must understand which value of selected metric is better, and keep in mind that `fmin` will attempt to minimize it. If you run regression, then you want your RMSE as low as possible - no tricks there. But if you run classification - the higher your ROC-AUC, the better, so don't forget the minus sign! Otherwise you'll get what you asked for - the set of hyperparameter values that yield the worst performance :).","d47ccae4":"#### Defining search space","561d166e":"### Plotting samples and splitting dataset into train, validation and test subsets","be3b87d8":"Diving in:\n  - **fmin** - central function of `hyperopt`, it performs guided search for a function minimum. It returns the point from parameters hyperspace that yielded the lowest loss. Most important arguments are:\n    - *fn* - function to minimize. Must accept a sample from parameters hyperspace and return real number (it might also return a dictionary, which has to contain two mandatory records: *loss* and *status*. Other elements you can design youself, if you want to, say, keep track of some other value during optimization. More on that later);\n    - *space* - parameters hyperspace to sample from;\n    - *algo* - algorithm which suggests new point given the points history. There are two options - `tpe.suggest` (guided search) and `tpe.rand.suggest` (essentially random search. If you want it);\n    - *max_evals* - number of points to sample;\n    - *trials* - object of class `Trial` to keep track of optimization history;\n    - *rstate* - pseudo-random generator seed, analogous to `random_state` from `sklearn`, for reproducibility;\n    - *show_progressbar* - well, kind of explains itself, doesn't it?\n  - **hp** - submodule to construct parameters hyperspace;\n  - **tpe** - submodule, which component `tpe.suggest` is the heart on hyperopt. It is an algorithm for guided points sampling, named Tree of Parzen Estimators;\n  - **Trials** - class, instances of which keep track of entire optimization process (stuff like parameters values, loss values, etc.);\n  - **space_eval** (*optional*) - sometimes useful for restoring parameters values from hyperspace in case of complex hyperspaces;\n  - **scope** - (*optional*) - submodule, among other things used for parameters type transformation. Say, you draw something from normal distribution, which comes out as `float`, but your model accepts only integer types - `scope` rushes for help;\n  - **sample** - (*optional*) - function to draw sample from hyperspace. Not required, but useful for testing whether you described your hyperspace correctly before running `fmin`.","a54b13f6":"All right, now let's see what we've got. We'll plot search history of all four methods, points distribution on 2D plane and compare the results.","6616e9db":"### Case 1: logistic regression with polynomial features","5521cd68":"Using full capacity of model here - 2000 trees.","132f37b8":"Of these the random search usually shows the best results when time is limited. Grid search is guaranteed to find the best possible combinations of those you've given it, but it might take forever. And even the best possible combination might be far from optimal. Each time you make the grid more dense or expand the borders - the computation time grows as well.","1c93915e":"## Introducing `hyperopt`","85ec72f6":"#### Defining target function","45d09343":"### Hints","e2cce492":"#### Gathering sample points","08f97e96":"#### Defining toy function and meshgrid for plotting","e7b6a169":"**NOTE**: I am not giving here a detailed description of how to set up hyperopt optimization - this section is intended purely to demonstrate how it works. It will be fully covered in subsequent sections, when we solve a machine learning hyperparametes tuning problem. For now, you can safely ignore all the unfamiliar code.","d36a3d08":"#### Plotting function graph\n\nLet's build a contour plot to see how our function behaves in region $[-10, 10]\\times[-10, 10]$.","9afb7e6b":"Well, we've got $R_2 \\approx 0.7$, which is not so bad, given that it is for rather small forests of 100 trees. Let's see how our optimization fared.","6e37b73e":"#### `hyperopt` optimization","e839d8fa":"Easy, right? Now if hyperopt draws `logit` - your hyperspace will be one-dimensional, but when it is `random_forest` - it will have three parameters (well, actually, 2 and 4 since `type` is also a parameter, albeit constant for each classifier).\n\nTheoretically speaking, there is no limit of hyperspace complexity, however, bear in mind that it might take a lot of samples to properly inspect extremely complex ones, even with hyperopt.","5ab9c090":"Classes are not linearly separable, so there seems to be enough space for hyperparameters tweaking.\n\nWe split dataset into train and validation parts.","a9741ec9":"#### Model performance on test set","9fc543cf":"Yep, accuracy has increased, not so much in absolute value, but significantly in relative terms. Let's see what gave us this dramatic efficiency gain.","60f48034":"First step in model construction is creating a list of features to use, then using `ColumnTransformer` to select them and generate polynomial combinations. Note, that here we use `interaction_only = True`, as tree-based models don't care about monotonic transformations (such as squaring positive variables). Also, we'll use a cool feature of Random Forest Regressor - it's ability to estimate own performance using Out-of-Bag samples, that's why for hyperparameter optimization we're setting `oob_score=True`.","9ff09174":"### Running different optimization methods","9a31c940":"Congratulations, you've reached the end of this far too long tutorial! Time for recap.\n\n  - `hyperopt` is a method of stochastic target function minimization, which uses bayesian approach for intelligent points sampling from search space. It adjusts prior distribution using history of target function evaluations, concentrating probability mass in the region where function performs better;\n  - To run optimization task you have to define your search space and target function;\n  - It is used extensively in machine learning, since it usually yields better results than Random Search (which is better than Grid Search) with same amount of time invested.","89b6d6e3":"Interesting. Seems that `AveOccup` is a useful feature after all, while logarithms are not favored. Also, constraining tree depth looks beneficial, as well as adding cubic feature combinations.\n\nLast step - to see how model performed on test set.","195f583a":"Ok, now we've covered most essential stuff. We can define hyperspaces and target functions of arbitrary complexity and succesfully traverse them to find optimal point. What else is there to learn?\n\nOne particularly annoying thing is inability to stop optimization at any given point. We pass `max_evals` parameter to `fmin` and until it exhaust all sample points - we have no info about what is happening (thankfully, in latest version (0.1.2 at the moment of writing this) authors have added progress bar with current best target function value). We can always interrupt `fmin`, but in this case we'll receive no output. Luckily, history is saved in `Trials` object, and we can extract it with a little hassle.\n\nActually, `Trials` objects have another very cool feature: using them, we can resume optimization from last recorded step. Since it keeps all evaluation history, `fmin` takes it into account and you don't start from square one again. You can run optimization for, say, 100 steps, look at results, and if you think there is more potential - just pass to `fmin` your `Trials` instance and add another 100 steps. Pretty nice, isn't it?\n\nLet's test it on a simple regression (for a change) problem. We'll load dataset with California houses prices and unleash the power of `hyperopt` on it.","d45020da":"Well-well-well. What do we have here?\n  - **Grid Search** is pretty straightforward - a uniform mesh over the hyperspace, best point not far from global maximum;\n  - **Random Search** performed rather poorly - majority of points concentrated in areas of low function value; it also gave worst result here;\n  - **Numerical maximization** converged to global maximum - quite expected, since it was solving the problem it was designed to solve. Best result;\n  - **Hyperopt** has high points density around global maximum, and, in fact, performed almost as good as numerical maximization - $5.528$ against $5.541$.\n  \nFrom the graphs one can easily see the idea behind hyperopt - regions in hyperspace, points from which performed better get higher chance to be sampled from again. We'll illustrate this in more detail once more by showing the evolution of hyperopt sampling strategy as number of points grows.","55d8eeb1":"### Test set performance","270bea94":"There is a package [hyperopt-sklearn](https:\/\/github.com\/hyperopt\/hyperopt-sklearn), which eases setting up optimization tasks using `sklearn` models. Many of estimators are wrapped with target functions and default and configurable search spaces. I personally don't use it, but it is very well worth exploring.\n\n`hyperopt` can be run in parallel, but this takes some tambourine playing to set up. You'll have to use MongoDB trials objects instead of native ones. [Here](https:\/\/blog.goodaudience.com\/on-using-hyperopt-advanced-machine-learning-a2dde2ccece7) is an introductory article on how to set this up. Again, might greatly accelerate things, or might not, if you use all your cores for speeding up a single optimization step (say, multicore RandomForest fitting).\n\n`hyperopt` is not the only library for bayesian optimization, and in fact some people prefer other ways, especially due to `hyperopt` treating hyperparameters as independent. There is [SMAC](https:\/\/automl.github.io\/SMAC3\/stable\/manual.html) and other methods, highlighted in [paper](https:\/\/arxiv.org\/pdf\/1807.02811.pdf). As with ML, there is no silver bullet.\n\nThat would be all from me. Many thanks for reading, hope you liked it!","e8403ea3":"#### Defining search space","8a744876":"#### Performing numerical optimization","500fb366":"Here we're defining our own functionality to draw samples (seems simpler than trying to make `sklearn` generate what you need in 2D space).","bedaac1a":"### Loading data","d1e4d17f":"Right... 20 steps for a fairly multidimensional hyperspace. Unlikely that we explored it properly. I'd say another 180 (200 in total) runs wouldn't hurt. But in order to use already accumulated data, and not start from scratch - we pass `Trials` object to `fmin`, and it picks up from where it left.","5cfa068e":"All right, let's run optimization for 100 rounds using TPE algorithm, meaning that we use TPE to suggest next sample values based on previous function evaluations. We'll use `Trials` class objects to keep track of optimization history.\n\n**Note**: We're binding `X` and `y` arguments of target function to `X1_train` and `y1_train` respectively, using `functools.partial`, since target function of `fmin` may accept only a search space point.","75f0d6ee":"Here we separated parameters for polynomial feature generation for the sake of more clear and concise code. Note how `degree` parameter is implemented: `hp.randint(label, x)` returns random integer between $0$ and $x - 1$, we have to manually add 1 to result (`degree == 0` doesn't make any sense, while `degree == 1` means no polynomial features). Any mathematical expression is permitted with sampled values. $C$ initially has loguniform distribution, and `class_weight` is a discrete random variable with two options.\n\n**NOTE**: here `degree` assumes either $1$ or $2$, so we could have easily implemented it with `hp.choice`; however `hp.randint` suits better here, since we're dealing with ordinal numbers, and order actually has meaning here.\n\nNow, let's implement model constructor.","1b82719b":"Ok, now we have to define function to minimize. We'll stick with cross-validation score on train set. Our function should take a sample from search space and return negative mean ROC-AUC score. As noted above, it is very important to return negative score here, since otherwise we'll seek for hyperparameters that minimize ROC-AUC.","e59babf1":"We'll generate $5000$ points.","43b8e6ba":"Adding polynomial features sure helped a lot, but can we push quality even further? We can increase maximum allowable polynomial degree, but how about trying different classifiers? Maybe SVM with RBF kernel? We treat classifier type as hyperparameter as well.","10cae79e":"#### Defining model","df5bea71":"## Case 2: Choosing between logit and SVM","fc6f1569":"We'll run optimization for 20 steps for the start.","3d696752":"We won't be performing any EDA here - that's not the point of this tutorial. We'll also drop latitude and longitude as features requiring special treatment and useless as they're now.","a27d9ab3":"We'll animate the results to look at each generation separately.","4d921f87":"SVM heavily prevails over logit in this case - more than $5\/6$ of all samples were drawn from SVM subspace. You can also see, as noted above, that sampling distribution of regularization constant $C$ is different for both classifiers. In case of logit it is nearly uniform, as `hyperopt` hasn't have enough trials to explore it properly, while for SVM there is a clear peak.","df9c4699":"We won't be choosing between different regressors this time, and focus on one model: Random Forest. Aside from regressor hyperparameters tuning it might as well use `hyperopt` for feature selection, in a fashion.\n\nSay, we're not sure if `AveOccup` is useful, or just noisy. Also we would like to see if there is a benefit from adding logarithms of the features. Let's find out.","2e6a47f9":"#### Defining target function","74c1d715":"## Internal workings of `hyperopt`","5fb14a0b":"Unfortunately, common implementation of random search completely ignores information on the trials already computed, and each new sample is drawn from the same initial distribution. Fortunately, there is a way to account for them. Say, you were tuning $C$ - regularization parameter for logistic regression. If one particular value gives really bad results - the points in vicinity will also perform poorly, so there is really very little need to sample from this region. We would like to incorporate this information into our strategy - in other words, we want to get more points from the regions with high probability of yielding good result and get less points from elsewhere. That's exactly what `hyperopt` module can do.","bd46a6f2":"### Target function","5a6e34e2":"Instances of this class keep track of optimization history, recording all drawn points and function evaluation results. It is not mandatory to use it, but highly advisable."}}