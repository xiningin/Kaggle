{"cell_type":{"aec587db":"code","ce30a0b2":"code","4c55cd80":"code","13769073":"code","60c83f19":"code","e74e9747":"code","f4411be7":"code","e05616d6":"code","6e56a9d2":"code","480b3c5e":"code","c40f9000":"code","715b7c6d":"code","27053da3":"code","46ea8cb4":"code","8215489a":"code","b4700484":"code","cbf4be5a":"code","56fbedda":"code","f5162584":"code","d757e696":"code","ada43ed0":"code","09b2bf32":"code","2a180ec9":"code","fb77d520":"code","acb6707a":"code","29d6c492":"code","0ff7b36e":"code","797a5487":"code","5646747d":"code","b0451655":"markdown","4ed0d076":"markdown","03d476bd":"markdown","8c129c9b":"markdown","8c0c7ec3":"markdown","efd83be4":"markdown","3bbccd68":"markdown","2d130e5c":"markdown","fe79db96":"markdown","53d89e4b":"markdown","c745aee3":"markdown","88afd46a":"markdown","70c4bf41":"markdown","07babe9c":"markdown","9476eca6":"markdown","945b4a1b":"markdown","a3ca3668":"markdown","a3dbd8d2":"markdown","70bfd871":"markdown","1c7782c9":"markdown","51cd4fe1":"markdown"},"source":{"aec587db":"import pandas as pd\nimport numpy as np\n\n# What do we say to python warnings? NOT TODAY\nimport warnings\nwarnings.filterwarnings('ignore')","ce30a0b2":"def select_features(df, target, th):\n    \"\"\"\n    Select features.\n    \"\"\"\n    # Select rows with our target value\n    proc_df = df[df[target].isna() == False]\n    \n    # Remove useless columns\n    to_drop = [col for col in proc_df.columns if ((\"(F)\" in col) or (\"(fpm)\" in col) or (\"Thermal\" in col) or (\"Air movement\" in col)) and (col != target)] + [\"Database\", \"Publication (Citation)\", \"Data contributor\"]\n    proc_df = proc_df.drop(to_drop, axis=1)\n    \n    # Remove columns with a lot of missing values\n    # Get columns with less than 30% of data missing\n    s = (proc_df.isna().sum() \/ len(proc_df) * 100 < th)\n    to_keep = list(s[s].index)\n    proc_df = proc_df[to_keep]\n    \n    return proc_df","4c55cd80":"def group_koppen_categories(category):\n    if \"A\" in category:\n        return \"A\"\n    elif \"B\" in category:\n        return \"B\"\n    elif \"C\" in category:\n        return \"C\"\n    elif \"D\" in category:\n        return \"D\"\n    elif \"E\" in category:\n        return \"E\"\n    else:\n        return None\n    \ndef handle_categorical_features(df):\n\n    from sklearn.impute import SimpleImputer\n    from sklearn.preprocessing import OneHotEncoder\n    \n    # Drop climate\n    df.drop(\"Climate\", axis=1, inplace=True)\n    # Group koppen climate\n    df[\"Koppen climate classification\"] = [group_koppen_categories(category) for category in df[\"Koppen climate classification\"]]\n    # Fill missing cooling strategy in city Harbin\n    df.loc[df.City == \"Harbin\", \"Cooling startegy_building level\"] = \"Naturally Ventilated\"\n    # Fill missing city in Malaysia\n    df.loc[df.Country == \"Malaysia\", \"City\"] = \"Kota Kinabalu\"\n    \n    # Input mode by city in Season and Cooling strategy\n    # Get list of categorical variables with missing values\n    cols = [\"Season\", \"Cooling startegy_building level\"]\n    # Input mode for each city\n    for city in df.City.unique():\n        # Filter data of selected city\n        temp = df.loc[df.City == city, cols]\n        # Create imputer\n        imputer = SimpleImputer(strategy='most_frequent')\n        # Input missing values with mode\n        imputed = pd.DataFrame(imputer.fit_transform(temp))\n        # Rename columns and index\n        imputed.columns = temp.columns\n        imputed.index = temp.index\n        # Replace in dataframe\n        df.loc[df.City == city, cols] = imputed\n        \n    # Encode\n    # Get list of categorical variables\n    s = (df.dtypes == 'object')\n    cols = list(s[s].index)\n    # One Hot Encoder\n    for col in cols:\n        # Create encoder\n        OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n        # Transform\n        OH_cols = pd.DataFrame(OH_encoder.fit_transform(df[[col]]))\n        # Get categories names and rename\n        names = OH_encoder.categories_\n        OH_cols.columns = names\n        OH_cols.index = df.index\n        # Add encoded columns\n        df[list(names[0])] = OH_cols\n\n    # Drop un-encoded column\n    df.drop(cols, axis=1, inplace=True)\n    \n    return df\n\n    \ndef get_balance_dataset_index(df, target):\n    \"\"\"\n    df: the dataset to balance\n    target: the name of the target column\n    \"\"\"\n    \n    # Get count for each category\n    value_counts = df.value_counts(target).to_dict()\n    \n    # List comprehension to find the key with the minimum value (count)\n    min_category = [k for k,v in value_counts.items() if v == min(value_counts.values())][0]\n    min_count = [v for k,v in value_counts.items() if v == min(value_counts.values())][0]\n    \n    # For each category in your target\n    dfs = []\n    for key in value_counts:\n        if key == min_category:\n            df1 = df[df[target] == min_category]\n        else:\n            df1 = df[df[target] == key].sample(min_count, random_state=55)\n        dfs.append(df1)\n    \n    dfb = pd.concat(dfs)\n    print(f\"Your balance dataset: {dfb.value_counts(target).to_dict()}\")\n    \n    return dfb.index","13769073":"target = \"Thermal sensation acceptability\"\n\n# Load original data\ndf_raw = pd.read_csv(\"\/kaggle\/input\/ashrae-global-thermal-comfort-database-ii\/ashrae_db2.01.csv\", low_memory=False)\n# Select features\ndf = select_features(df_raw, target, 25)\n# Handle categorical features\ndf = handle_categorical_features(df)\n#get indexes from balanced dataset, this is to use always the same rows to predict\nidx = get_balance_dataset_index(df, target)","60c83f19":"df.head()","e74e9747":"df.info()","f4411be7":"cols = [\"Clo\",\"Met\", \"Air temperature (C)\", \"Relative humidity (%)\", \n        \"Air velocity (m\/s)\", \"Outdoor monthly air temperature (C)\"]","e05616d6":"df1 = df_raw[cols]","6e56a9d2":"df1.isna().sum() \/ len(df1) * 100","480b3c5e":"cols","c40f9000":"df1 = df_raw[cols + [\"City\", \"Country\"]]","715b7c6d":"df1.loc[df1.Country == \"Malaysia\", \"City\"] = \"Kota Kinabalu\"","27053da3":"# Input mean by city in numerical features\n# Input mode for each city\nfor city in df1.City.unique():\n    \n    # Filter data of selected city\n    temp = df1.loc[df1.City == city, cols]\n    # Serie with the mean per column\n    means = temp.mean()\n    # Fill the missing values with the mean\n    temp = temp.fillna(means)\n    # Replace in dataframe\n    df1.loc[df1.City == city, cols] = temp","46ea8cb4":"df1.isna().sum() \/ len(df1) * 100","8215489a":"# there are cities with all null\ndf1 = df1.fillna(df1.mean())\ndf1.isna().sum() \/ len(df1) * 100","b4700484":"cols","cbf4be5a":"# We drop the original columns\ndf = df.drop(cols, axis=1)\n# we don't need the columns City and Country anymore, we have it encoded\ndf[cols] = df1.drop([\"City\", \"Country\"], axis=1) ","56fbedda":"df.head()","f5162584":"df.info()","d757e696":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import roc_auc_score, classification_report","ada43ed0":"dfb = df.loc[idx]","09b2bf32":"dfb.head()","2a180ec9":"X = dfb.drop(\"Thermal sensation acceptability\", axis=1)\ny = dfb[\"Thermal sensation acceptability\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=55)","fb77d520":"# Define the model\nmy_model = XGBClassifier(n_estimators=50, learning_rate=0.1, n_jobs=4, random_state=55, \n                         objective=\"binary:logistic\", eval_metric=\"auc\", use_label_encoder=False)\n\n# Perform cross validation with 5 folds\nprint(\"Training...\")\nscores = cross_val_score(my_model, \n                          X_train, y_train,\n                          cv=5,\n                          scoring='roc_auc')\nprint(\"...done.\")","acb6707a":"print(f\"Scores: {scores}\")\nprint(f\"Mean scores: {np.mean(scores)}\")","29d6c492":"# This is the model we are going to train (a simple one)\nmy_model = XGBClassifier(n_estimators=50, learning_rate=0.1, n_jobs=4, random_state=55, objective=\"binary:logistic\")\n\n# Train\nprint(\"Training...\")\nmy_model.fit(X_train, y_train, verbose=False)\nprint(\"...done\")","0ff7b36e":"# And we predict\nprediction = pd.DataFrame({\"y_pred\": my_model.predict(X_test), \"y_real\": y_test})\nroc_auc_score(prediction.y_real, prediction.y_pred)","797a5487":"print(classification_report(prediction.y_real, prediction.y_pred))","5646747d":"def handle_numerical_features(df, df_raw):\n    \"\"\"\n    Input mean by city.\n    \"\"\"\n    \n    cols = ['Clo',\n             'Met',\n             'Air temperature (C)',\n             'Relative humidity (%)',\n             'Air velocity (m\/s)',\n             'Outdoor monthly air temperature (C)',\n             'City',\n             'Country']\n\n    df1 = df_raw[cols]\n\n    # Input mode in missing Malaysia city\n    df1.loc[df1.Country == \"Malaysia\", \"City\"] = \"Kota Kinabalu\"\n\n    cols = ['Clo',\n             'Met',\n             'Air temperature (C)',\n             'Relative humidity (%)',\n             'Air velocity (m\/s)',\n             'Outdoor monthly air temperature (C)']\n\n    # Input mean by city in numerical features\n    # Input mode for each city\n    for city in df1.City.unique():\n        # Filter data of selected city\n        temp = df1.loc[df1.City == city, cols]\n        # Serie with the mean per column\n        means = temp.mean()\n        # Fill the missing values with the mean\n        temp = temp.fillna(means)\n        # Replace in dataframe\n        df1.loc[df1.City == city, cols] = temp\n\n    # there are cities with all null\n    df1 = df1.fillna(df1.mean())\n\n    # Add to dataset\n    df = df.drop(cols, axis=1)\n    df[cols] = df1.drop([\"City\",\"Country\"], axis=1) # we don't need the column City anymore, we have it encoded\n    \n    return df","b0451655":"We are getting better, but there are still some gaps. Why this happens? because some cities don't have information about these features, a mean on a list of missing values cannot be calculated. In this gaps we are going to input the column mean.","4ed0d076":"# Model\nNow is time to train the model We are going the follow the same process as before:\n1. Balance data set\n2. Split in train and test\n3. 5-folds cross validation training\n4. Training with whole training data set\n5. Prediction of test data set","03d476bd":"Pretty close to our previous models.","8c129c9b":"Remember from last notebook that missing city in Malaysia, we are going to quickly fill that information (is the same process we follow before, you con omit this part).","8c0c7ec3":"| Notebook           | Categorical features | Missing values in categorical | Missing values in numerical | Feature engineering | ROC-AUC score |\n|--------------------|----------------------|-------------------------------|-----------------------------|---------------------|---------------|\n| Preprocessing pt.2 | One Hot Encoding     | Mode input                    | 0 input                     | -                   |0.6591         |\n| Preprocessing pt.3 | One Hot Encoding     | Mode Input                    | Mean input                  | -                   |0.6466         |","efd83be4":"## Split and cross validation\nLet's split our data. Don't forget to set a `random_state` if you want to replicate the process and obtain the same results.","3bbccd68":"We are going to input the **mean** in the numerical features **by city**. We have to select the numerical features and the city to have this information in the dataset; we are going to select them from `df_raw`, in this data set the features City and Country are not encoded.","2d130e5c":"## Train and predict\nLet's see what happens when we train with the whole training data set.","fe79db96":"## Input\nNow let's focus on the other numerical features. In our [previous notebook](https:\/\/www.kaggle.com\/ponybiam\/classification-preprocessing-pt-2), while handling categorical features, we input the mode using the package [SimpleImputer from SciKitLearn](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.SimpleImputer.html). Here we could use it too, but instead we are going to use pandas methods to input the mean in the missing gaps. In general, there are several tools to perform these pre-processing tasks, you can use the one of your preference.","53d89e4b":"# Introduction\n\nThis is a serie of notebooks thar should be visited in order, they are all linked in the table of content. In this notebook we are going to handle numerical features and run a classification model.\n\n### Content table\n- [Preprocessing pt. 1: data transformation & EDA](https:\/\/www.kaggle.com\/ponybiam\/classification-preprocessing-pt-1\/)\n- [Preprocessing pt. 2: encoding categorical variables](https:\/\/www.kaggle.com\/ponybiam\/classification-preprocessing-pt-2)\n- **Preprocessing pt. 3: handling missing values** (you are here) \n    - [Load data](#Load-data)\n    - [Missing values](#Missing-values)\n        - [Drop](#Drop)\n        - [Input](#Input)\n    - [Model](#Model)\n        - [Balance dataset](#Balance-dataset)\n        - [Split and cross validation](#Split-and-cross-validation)\n        - [Train and predict](#Train-and-predict)\n- [Feature engineering pt. 1: simple features](https:\/\/www.kaggle.com\/ponybiam\/classification-feature-engineering-pt-1)\n- [Feature engineering pt. 2: clustering & PCA](https:\/\/www.kaggle.com\/ponybiam\/classification-feature-engineering-pt-2)\n- [Feature engineering pt. 3: target encoding](https:\/\/www.kaggle.com\/ponybiam\/classification-feature-engineering-pt-3)","c745aee3":"# Missing values\nFirst, let's write down the numerical features we are going to work with. Remember, in our [first notebook](https:\/\/www.kaggle.com\/ponybiam\/classification-preprocessing-pt-1\/) we decided to use only the numerical features in Celsius degrees.","88afd46a":"# (Optional) One function to do it all\nThis part is optional. We are going to write functions that follows all the steps we performed here. These functions will be used in the following notebooks.","70c4bf41":"# Load data\nThe functions used here were defined in previous notebooks, they replicate the process followed there to use always the same dataset, you can ignore them.","07babe9c":"And we select them from our raw data:","9476eca6":"And we cross-validate with our training data:","945b4a1b":"## Balance dataset\nWe are going to use the function we defined before! it's at the beginning of this notebook.","a3ca3668":"# Final considerations about preprocessing\nPreprocessing is an important step when training machine learning models and maybe one of the most important. Here we ate focusing on the techniques and didn't use much of a criteria to select features or input the missing values. Here are thing you can try out and we really encourage to do it:\n\n- First of all, we choose one binary target to use it as example and we removed the other ones to focus on the one we choose. You could try to predict that target column but keeping the others ones; they are highly related features that can improve the model.\n- Here we selected some features without thinking that much, but a good exploration of the data usually shows information and relationship between the features that can give you a good lead on which of the features use. Of course you can use all of the data you have, but when you have big datasets is easy to use all your memory really quick, more features means more data, and that means more memory used in training.\n- Missing data is something you will always encounter. Here we used a simple technique to imput them but have in mind that there a lot of other resources really easy to implement. [ScikitLearn offers a bunch of them](https:\/\/scikit-learn.org\/stable\/modules\/impute.html).\n- If you have categorical features in your data set encoding them is non-optional. The techniques we saw here will do the work, but if you are interested there are some other resources out there. [This article](https:\/\/scikit-learn.org\/stable\/modules\/impute.html) is a good start to research them.","a3dbd8d2":"# Notebook goal\nThe goal of this notebook is to handle the numerical features: we are going to fill the missing values and, at the end, train a predictive model. Here will be shown only one of the many possibilities in machine learning, feel free to experiment and play around with the data on your own. We will be focusing on the techniques and not on the model performance.","70bfd871":"The score is lower than before! all that work imputing missing values for nothing? :( But that could happens, life is not always easy. Fortunately, we still have some cards to play!\n\nFinally, let's check out the classification report:","1c7782c9":"An now let's use `pandas` to fill the missing values. We are going to use the [method `fillna`](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.fillna.html). There are some methods you can choose (like copying the previous value) or you can pass a number to fill the gaps. In this case, we are goin to pass the mean by city to fill them. Check out the following loop:","51cd4fe1":"And finally we can put togheter our encoded categorical features and our all-missing-filled numerical ones."}}