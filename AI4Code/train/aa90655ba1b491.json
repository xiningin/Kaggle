{"cell_type":{"f9c7941d":"code","635b0042":"code","9b8012c7":"code","d77b8df3":"code","ef63237f":"code","d18de60b":"code","ac2c8e9c":"code","09e39f10":"code","bba1939a":"markdown","f781ae58":"markdown","13af5899":"markdown","304582f5":"markdown","86fa9737":"markdown","05e6c2d3":"markdown","105a4dff":"markdown"},"source":{"f9c7941d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nprint('Files in this dataset:')\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","635b0042":"import os\nfrom pathlib import Path\nimport pandas as pd\n\nimport numpy as np\nimport librosa\nimport librosa.core\nimport librosa.feature\n\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom scipy import signal\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nsns.set_palette(\"husl\")\n\ndef file_load(wav_name, mono=False):\n    \"\"\"\n    load .wav file.\n\n    wav_name : str\n        target .wav file\n    sampling_rate : int\n        audio file sampling_rate\n    mono : boolean\n        When load a multi channels file and this param True, the returned data will be merged for mono data\n\n    return : np.array( float )\n    \"\"\"\n    try:\n        return librosa.load(wav_name, sr=None, mono=mono)\n    except:\n        print(\"file_broken or not exists!! : {}\".format(wav_name))","9b8012c7":"! wget https:\/\/raw.githubusercontent.com\/daisukelab\/dcase2020_task2_variants\/master\/file_info.csv\n\ndf = pd.read_csv('file_info.csv')\ndf.file = df.file.map(lambda f: str(f).replace('\/data\/task2\/dev', '\/kaggle\/input\/dc2020task2'))\ntypes = df.type.unique()\n\ndf.head()","d77b8df3":"agg = df[['file', 'type', 'split']].groupby(['type', 'split']).agg('count')\nfig = plt.figure(figsize=(12.0, 6.0))\ng = sns.barplot(x=\"type\", y=\"file\", hue=\"split\", data=agg.reset_index())\nplt.ylabel(\"machine type\")\nplt.ylabel(\"number of files\")\nplt.show()\nagg.transpose()","ef63237f":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in sorted(filenames):\n        x = np.load(Path(dirname)\/filename)\n        _, mtype, split = Path(filename).stem.split('-')\n        print(f\"{mtype}'s {split} split numpy data file {filename}, shape (files, n_mels, frames): {x.shape}\")","d18de60b":"df.groupby(['type', 'split']).describe()","ac2c8e9c":"for t in types:\n    for split in ['train', 'test']:\n        type_df = df[df['type'] == t][df.split == split].reset_index()\n        file_path = Path(f'\/kaggle\/input\/dc2020task2prep\/dc2020t2l1-{t}-{split}.npy')\n        X = np.load(file_path)\n        # visualize\n        R = 2\n        fig, ax = plt.subplots(R, 4, figsize = (15, 5*R\/\/2))\n        print(f'=== Machine type [{t}], {split} set ===')\n        for i in range(R * 4):\n            file_index = i % 4 + ((i \/\/ 8) * 4)\n            if (i % 8) < 4:\n                ax[i\/\/4, i%4].set_title(Path(type_df.file[file_index]).name)\n                ax[i\/\/4, i%4].plot(X[file_index])\n            else:\n                ax[i\/\/4, i%4].violinplot(X[file_index, ::4, :].T)\n                ax[i\/\/4, i%4].get_xaxis().set_visible(False)\n        plt.show()","09e39f10":"for t in types:\n    for split in ['train', 'test']:\n        type_df = df[df['type'] == t][df.split == split].reset_index()\n        file_path = Path(f'\/kaggle\/input\/dc2020task2prep\/dc2020t2l1-{t}-{split}.npy')\n        X = np.load(file_path)\n        # visualize\n        R = 2\n        fig, ax = plt.subplots(R, 1, figsize = (15, 2.5*R))\n        print(f'=== Machine type [{t}], {split} set ===')\n        for i in range(R * 1):\n            file_index = i\n            file_path = Path(type_df.file[file_index])\n            mels = X[i]\n            ax[i].set_title(file_path.name)\n            ax[i].imshow(mels)\n            ax[i].axis('off')\n        plt.show()","bba1939a":"## Check\/confirm all samples completely\n\nDo all samples have consistent property?\n\nFollowings are checked here:\n\n- sampling rate\n- frame length, duration","f781ae58":"## Shape of numpy files\n\nThis preprocessed dataset has 12 numpy files, one file corresponds to train or test split for one of machine types.\n\nAll the samples in one of original folder were converted into log mel spectrogram, then packed into single numpy array and saved to a file.\n\nThen the first dimension is number of files, the second dimension is `n_mels` and the last dimension is number of frames.","13af5899":"## Spectrograms\n\nSamples from each machine types are shown here.","304582f5":"## Number of samples\n\nLet's check number of original audio samples for each machine types.","86fa9737":"## Download file information\n\nThe `file_info.csv` is created by a script [get_file_info.py](https:\/\/github.com\/daisukelab\/dcase2020_task2_variants\/blob\/master\/get_file_info.py) on the repository.\n\nRunning the script to create here takes long, then simply download a copy.","05e6c2d3":"Now we could confirm that:\n\n- Data format is consistent among files in one machine type.\n- Only ToyCar has longer audio 11s, all others are 10s long.\n- Sampling rate is consistently 16,000 Hz.\n\n## Frequency Bins Statistics\n\nThe followings will show visual basic statistics of each frequency bins in a log mel spectrogram data.","105a4dff":"# DCASE 2020 Task 2 Preprocessed Dataset EDA\n\nThis is a Kaggle notebook version from [github repository](https:\/\/github.com\/daisukelab\/dcase2020_task2_variants\/blob\/master\/data_eda.ipynb), will perform simple exploratory data analysis on the [DCASE 2020 Task 2 dataset](https:\/\/zenodo.org\/record\/3678171#.XnRgCi3AOL4)."}}