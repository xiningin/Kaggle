{"cell_type":{"8c8888ef":"code","2deb5924":"code","4c9ae436":"code","bdca239e":"code","33b625a9":"code","8aaae064":"code","cc062e7a":"code","c84f8b39":"code","46bbeb47":"code","e9854613":"code","876101c8":"code","009c57b5":"code","c833c93d":"code","65f70a55":"code","8fe2ec33":"code","ce02b821":"code","0444d733":"code","fd23b9c9":"code","0073acc4":"code","3f1b29c9":"code","cf539ebb":"code","c84059a0":"code","139caa24":"code","d50e5c25":"code","c47b6216":"code","a9c6a0ac":"code","debde64a":"code","24e92327":"code","8fb59e54":"code","5b488538":"markdown"},"source":{"8c8888ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        pass\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2deb5924":"from IPython.core.magic import register_cell_magic\n@register_cell_magic\ndef skip(line, cell=None):\n    '''Skips execution of the current line\/cell if line evaluates to True.'''\n    if eval(line):\n        return\n        \n    get_ipython().run_cell(cell)","4c9ae436":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow_datasets.public_api as tfds\nimport tensorflow as tf\nimport glob\nimport dill\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport random","bdca239e":"tf.random.set_seed(123)\nnp.random.seed(123)\nrandom.seed(123)","33b625a9":"# NEW on TPU in TensorFlow 24: shorter cross-compatible TPU\/GPU\/multi-GPU\/cluster-GPU detection code\ntpu = None\ntry: # detect TPUs\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept ValueError: # detect GPUs\n    #strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n    #strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # for clusters of multi-GPU machines\n\n    \n#strategy,tpu = tf.distribute.MirroredStrategy(devices=[\"TPU:0\", \"TPU:1\",\"TPU:2\"]),True\n\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n\n\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","8aaae064":"data_path = '..\/input\/optiver-realized-volatility-prediction'\nGCS_PATH = '..\/input\/orvptfrecords'\nif tpu:\n    GCS_PATH = KaggleDatasets().get_gcs_path('orvptfrecords')","cc062e7a":"class TrainDataset(tfds.core.GeneratorBasedBuilder):\n    VERSION = tfds.core.Version('0.1.0')\n    \n    def _split_generators(self, dl_manager):\n        return [\n            tfds.core.SplitGenerator(\n                    name=f'train',\n                    gen_kwargs={\n                    },\n            )\n        ]\n    \n    def _info(self):\n        return tfds.core.DatasetInfo(\n            builder=self,\n            description=(\"\"),\n            features=tfds.features.FeaturesDict({\n                \"stock_id\": tfds.features.Tensor(shape=(),dtype=tf.float64),\n                \"book\": tfds.features.Tensor(shape=(None,9,),dtype=tf.float64),\n                \"trade\": tfds.features.Tensor(shape=(None,4,),dtype=tf.float64),\n                \"target\": tfds.features.Tensor(dtype=tf.float64 ,shape=(1,)),\n            }),\n        )\n    \n    def _generate_examples(self,**args):\n        pass","c84f8b39":"BATCH_SIZE_PER_REPLICA = 256\nif tpu:\n    BATCH_SIZE_PER_REPLICA = 256\nGLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync\nBUFFER_SIZE = 50000\n\nprefetch = 30\nMAX_SEQ = 600\nTRAIN = False\n\nepochs = 60\nLR = 1e-3","46bbeb47":"train_df = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')","e9854613":"def calculate_wap(bid_price1,ask_price1,bid_size1,ask_size1,\n                 bid_price2,ask_price2,bid_size2,ask_size2):\n    a1 = bid_price1 * ask_size1 + ask_price1 * bid_size1\n    b1 = bid_size1 + ask_size1\n    a2 = bid_price2 * ask_size2 + ask_price2 * bid_size2\n    b2 = bid_size2 + ask_size2\n    x = (a1\/b1 + a2\/b2)\/ 2\n    return x[:,tf.newaxis]\n\n\ndef calculate_wap2(bid_price1,ask_price1,bid_size1,ask_size1,\n                 bid_price2,ask_price2,bid_size2,ask_size2):\n        \n    a1 = bid_price1 * ask_size1 + ask_price1 * bid_size1\n    a2 = bid_price2 * ask_size2 + ask_price2 * bid_size2\n    b = bid_size1 + ask_size1 + bid_size2+ ask_size2\n    \n    x = (a1 + a2)\/ b\n    return x[:,tf.newaxis]\n\ndef calculate_wap3(bid_price1,ask_price1,bid_size1,ask_size1,):\n    a1 = bid_price1 * ask_size1 + ask_price1 * bid_size1\n    b1 = bid_size1 + ask_size1\n    x = a1\/b1\n    return x[:,tf.newaxis]\n\ndef calculate_wap4(bid_price2,ask_price2,bid_size2,ask_size2,):\n    a2 = bid_price2 * ask_size2 + ask_price2 * bid_size2\n    b2 = bid_size2 + ask_size2\n    x = a2\/b2\n    return x[:,tf.newaxis]\n\ndef tf_diff(a):\n    return a[1:]-a[:-1]\n\ndef calculate_log_return(wap):\n    log_return = tf.math.log(wap)\n    log_return = tf.concat([log_return,tf.constant([[0.]],dtype=tf.float64)],axis=0)\n    log_return = tf_diff(log_return)\n    return log_return\n\ndef realized_volatility(log_return):\n    rv = tf.math.sqrt(tf.reduce_sum(log_return**2))\n    return rv","876101c8":"def features_builder(stock_id,book,trade):\n    #time_id = book[:,0]\n    seconds_in_bucket = book[:,0]\n    bid_price1 = book[:,1]\n    ask_price1 = book[:,2]\n    bid_price2 = book[:,3]\n    ask_price2 = book[:,4]\n    bid_size1 = book[:,5]\n    ask_size1 = book[:,6]\n    bid_size2 = book[:,7]\n    ask_size2 = book[:,8]\n    \n    #Book features\n    \n    # book_size\n    book_size = tf.cast(tf.shape(book)[0],tf.float64)\n    \n    #wap\n    wap = calculate_wap(bid_price1,ask_price1,bid_size1,ask_size1,bid_price2,ask_price2,bid_size2,ask_size2)\n    wap2 = calculate_wap2(bid_price1,ask_price1,bid_size1,ask_size1,bid_price2,ask_price2,bid_size2,ask_size2)\n    wap3 = calculate_wap3(bid_price1,ask_price1,bid_size1,ask_size1,)\n    wap4 = calculate_wap4(bid_price2,ask_price2,bid_size2,ask_size2,)\n    #log_return\n    log_return = calculate_log_return(wap)\n    log_return2 = calculate_log_return(wap2)\n    log_return3 = calculate_log_return(wap3)\n    log_return4 = calculate_log_return(wap4)\n    # rv\n    rv = realized_volatility(log_return)\n    rv2 = realized_volatility(log_return2)\n    rv3 = realized_volatility(log_return3)\n    rv4 = realized_volatility(log_return4)\n    rv = tf.repeat(rv,tf.shape(book)[0])[:,tf.newaxis]\n    rv2 = tf.repeat(rv2,tf.shape(book)[0])[:,tf.newaxis]\n    rv3 = tf.repeat(rv3,tf.shape(book)[0])[:,tf.newaxis]\n    rv4 = tf.repeat(rv4,tf.shape(book)[0])[:,tf.newaxis]\n    \n    book_data = [wap,wap2,wap3,wap4,\n                 log_return,log_return2,log_return3,log_return4,\n                 rv,rv2,rv3,rv4,\n                 tf.repeat(stock_id,tf.shape(book)[0])[:,tf.newaxis],\n                 tf.repeat(book_size,tf.shape(book)[0])[:,tf.newaxis]\n          ]\n    book_data = tf.concat(book_data,axis=-1)\n    \n    # Trade features\n    #time_id\tseconds_in_bucket\tprice\tsize\torder_count\n    #time_id = trade[:,0]\n    seconds_in_bucket = trade[:,0]\n    price = trade[:,1]\n    size = trade[:,2]\n    order_count = trade[:,3]\n    #price_log_return\n    price_log_return = calculate_log_return(tf.reshape(price,(-1,1)))\n    #trade_size\n    trade_size = tf.cast(tf.shape(trade)[0],tf.float64)\n    \n    trade_data = [\n        #price_log_return,\n        tf.repeat(trade_size,tf.shape(trade)[0])[:,tf.newaxis]\n          ]\n    trade_data = tf.concat(trade_data,axis=-1)\n    \n    return book_data,trade_data\n    \n","009c57b5":"def get_datasets():\n    builder = TrainDataset(data_dir=GCS_PATH)\n    # The following line download the dataset\n    builder.download_and_prepare()\n    dataset = builder.as_dataset()['train']\n    \n    size = len(dataset)\n\n    # pad,shuffle and bacth\n    def preprecoss(x):\n        stock_id,book,trade,target = x['stock_id'],x['book'],x['trade'],x['target']\n        \n        book_data,trade_data = features_builder(stock_id,book,trade)\n        \n        book = tf.concat([book,book_data],axis=-1)\n        trade = tf.concat([trade,trade_data],axis=-1)\n        \n        p1 = [[0,MAX_SEQ-tf.shape(book)[0]],[0,0]]\n        p2 = [[0,MAX_SEQ-tf.shape(trade)[0]],[0,0]]\n        \n        book = tf.pad(book,p1, constant_values=0.)\n        trade = tf.pad(trade,p2, constant_values=0.)\n        \n        return (book,trade),target\n    \n    def shape_fix(inputs,target):\n        book,trade = inputs\n        book = tf.reshape(book,(MAX_SEQ,9+14))\n        trade = tf.reshape(trade,(MAX_SEQ,4+1))\n        return (book,trade),target\n    \n    \n    dataset = dataset.repeat().shuffle(BUFFER_SIZE).map(preprecoss,num_parallel_calls=AUTO)\n    if tpu:\n        dataset = dataset.map(shape_fix,num_parallel_calls=AUTO)\n    dataset = dataset.batch(BATCH_SIZE_PER_REPLICA).prefetch(prefetch)\n    if tpu:\n        dataset = strategy.experimental_distribute_datasets_from_function(lambda x: dataset)\n        \n    return dataset,size\/\/GLOBAL_BATCH_SIZE","c833c93d":"def build_model():\n    n_seq = MAX_SEQ#None if not tpu else MAX_SEQ\n    book_input = tf.keras.layers.Input(shape=(n_seq,9+14))\n    trade_input = tf.keras.layers.Input(shape=(n_seq,4+1))\n\n    book = tf.keras.layers.Masking()(book_input)\n    trade = tf.keras.layers.Masking()(trade_input)\n\n    book = tf.keras.layers.BatchNormalization()(book)\n    trade = tf.keras.layers.BatchNormalization()(trade)\n\n    book = tf.keras.layers.GRU(256)(book)\n    book = tf.keras.layers.Dropout(0.1)(book)\n\n    trade = tf.keras.layers.GRU(256)(trade)\n    trade = tf.keras.layers.Dropout(0.1)(trade)\n    \n    model = tf.keras.layers.concatenate([book,trade])\n    \n    \n    for _ in range(10):\n        model = keras.layers.Dense(256, activation=keras.activations.swish)(model)\n        model = tf.keras.layers.Dropout(0.2)(model)\n    \n     \n    model = tf.keras.layers.Dense(1,activation=None)(model)\n\n    model = tf.keras.Model([book_input,trade_input],model)\n    return model","65f70a55":"build_model().summary()","8fe2ec33":"def rmspe(y_true,y_pred):\n    elements = ((y_true - y_pred) \/ y_true) ** 2\n    elements = tf.reduce_sum(elements)\/tf.cast(tf.size(y_pred),tf.float32)\n    return tf.sqrt(elements)","ce02b821":"train_dataset,train_step = get_datasets()","0444d733":"%%skip not TRAIN\n\nwith strategy.scope():\n    \n    #model\n    model = build_model()\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(LR),\n        loss=rmspe,\n    )\n    #callbacks\n    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=f'weights.h5',\n                                                                   save_weights_only=True,\n                                                                   monitor='loss',\n                                                                   mode='min',verbose=True,\n                                                                   save_best_only=True)\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss',mode='min', patience=5)\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=2, min_lr=LR\/100)\n    terminate_onNaN = tf.keras.callbacks.TerminateOnNaN()\n    # dataset\n    train_dataset,train_step = get_datasets()\n    # Train\n    history = model.fit(train_dataset,\n                        steps_per_epoch=train_step,\n                        epochs=epochs,\n                        callbacks=[model_checkpoint_callback,early_stopping,reduce_lr,terminate_onNaN],\n                       )","fd23b9c9":"%%skip not TRAIN\npd.DataFrame(history.history).loss.plot()","0073acc4":"%%skip TRAIN\n! cp  ..\/input\/realized-volatility-keras-rnn-baseline-tpu\/weights.h5 .\/","3f1b29c9":"%%skip tpu\nmodel  = build_model()\nmodel.load_weights(f'.\/weights.h5')","cf539ebb":"class TestDataset():\n    \n    def __init__(self,):\n        self.test = pd.read_csv(data_path+'\/test.csv')\n        self.stock_ids = self.test.stock_id.unique()\n        \n        \n    def getStockData(self,stock_id):\n        # Read data\n        book_path = f'{data_path}\/book_test.parquet\/stock_id={stock_id}'\n        trade_path = f'{data_path}\/trade_test.parquet\/stock_id={stock_id}'\n        book_df = pd.read_parquet(book_path)\n        trade_df = pd.read_parquet(trade_path)\n        \n        book_df_grp = book_df.groupby('time_id')\n        trade_df_grp = trade_df.groupby('time_id')\n        \n        data = []\n        for time_id in self.test.time_id.unique():\n            try:\n                book_items = book_df_grp.get_group(time_id)\n                del book_items['time_id']\n            except:\n                continue\n            try:\n                trade_items = trade_df_grp.get_group(time_id)\n                del trade_items['time_id']\n                trade_items = trade_items.values\n            except:\n                # If No trade\n                trade_items = np.zeros((1,4),dtype=np.float64)\n                \n            \n            row_id = self.test[(self.test.stock_id == stock_id) & (self.test.time_id == time_id) ].row_id.values\n            if len(row_id) == 0:\n                continue\n            row_id = row_id[0]\n            \n            item = row_id,stock_id,book_items.values,trade_items\n            data.append(item)\n            \n        return data\n    \n    def gen(self):\n        for stock_id in self.stock_ids:\n            for item in self.getStockData(stock_id):\n                yield item\n        \n    def __len__(self):\n        return len(self.train)","c84059a0":"%%skip tpu\ntest_data = TestDataset()","139caa24":"%%skip tpu\nfor x in test_data.gen():\n    break","d50e5c25":"def test_preprecoss(row_id,stock_id,book,trade):\n    book_data,trade_data = features_builder(stock_id,book,trade)\n        \n    book = tf.concat([book,book_data],axis=-1)\n    trade = tf.concat([trade,trade_data],axis=-1)\n        \n    p1 = [[0,MAX_SEQ-tf.shape(book)[0]],[0,0]]\n    p2 = [[0,MAX_SEQ-tf.shape(trade)[0]],[0,0]]\n        \n    book = tf.pad(book,p1, constant_values=0.)\n    trade = tf.pad(trade,p2, constant_values=0.)\n        \n    return row_id,(book,trade)","c47b6216":"%%skip tpu\ntest_dataset = tf.data.Dataset.from_generator(test_data.gen,\n                                         output_signature=(\n                                             tf.TensorSpec(shape=(), dtype=tf.string),\n                                             tf.TensorSpec(shape=(),dtype=tf.float64),\n                                             tf.TensorSpec(shape=(None,9,),dtype=tf.float64),\n                                             tf.TensorSpec(shape=(None,4,),dtype=tf.float64),\n                                         )\n                                        )\n\ntest_dataset = test_dataset.map(test_preprecoss,num_parallel_calls=AUTO)\ntest_dataset = test_dataset.batch(64).prefetch(prefetch)","a9c6a0ac":"%%skip tpu\ndef predict_fn(r,X):\n    return r, model(X,training=False)\ntest_dataset = test_dataset.map(predict_fn).prefetch(10)","debde64a":"%%skip tpu\nids,targets= [],[]\nfor (row_id,y) in test_dataset:\n    ids.extend(row_id.numpy().flatten())\n    targets.extend(y.numpy().flatten())\n    \nids = [s.decode('ascii') for s in ids]","24e92327":"%%skip tpu\n# add missing rows\nmiss_idx = ~test_data.test.row_id.isin(ids)\nmiss = test_data.test.loc[miss_idx,'row_id'].values\nids.extend(miss)\ntargets.extend([0 for _ in miss])\n#targets.extend([train_df.target.min() for _ in miss])","8fb59e54":"%%skip tpu\ndf = pd.DataFrame({'row_id':ids,'target':targets})\ndf.to_csv('submission.csv',index=False)\ndf.head(10)","5b488538":"\n* Dataset conversion to TFRecords : https:\/\/www.kaggle.com\/tchaye59\/orvp-dataset\n* TFRecords : https:\/\/www.kaggle.com\/tchaye59\/orvptfrecords"}}