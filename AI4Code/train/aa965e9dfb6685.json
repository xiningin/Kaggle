{"cell_type":{"92d846ef":"code","cb0c0489":"code","78c409a9":"code","3378628b":"code","1f9b89a8":"code","95c902d7":"code","d6a869f1":"code","2c68e6c5":"code","ebc86988":"code","1b2cc772":"code","c1db530c":"code","650c7f62":"code","6eb52d00":"code","47c8b9ad":"code","cee5c798":"code","bfa8cd2e":"code","0be6b486":"markdown","1b578b0e":"markdown","e5e0828f":"markdown","9b225b38":"markdown","794e1076":"markdown","a419b8cb":"markdown","bb461f6f":"markdown","5f31635a":"markdown","85f17eed":"markdown","9a6d2cbc":"markdown","3f382356":"markdown","43e483b5":"markdown","5ca4496f":"markdown","80830ecd":"markdown","56f89406":"markdown","64b7723b":"markdown","0fd9f5cc":"markdown","6b52ab45":"markdown","0080c495":"markdown"},"source":{"92d846ef":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\n!pip install -q pycocotools\nfrom pycocotools.coco import COCO\nimport tensorflow as tf\nimport requests\nimport io\nimport cv2","cb0c0489":"ver = \"val\"  # \"train\"\ndir_name=\"\/kaggle\/input\/coco-2017-dataset\/coco2017\/\"\nimg_dir=f\"{dir_name}\/{ver}2017\/\"\nannotations_file=f\"{dir_name}\/annotations\/instances_{ver}2017.json\"\n\n# Annotations are stored using JSON. COCO API can be used to access and manipulate all anotations.\ncoco=COCO(annotations_file)  # instantiating COCO API\nimgIds = coco.getImgIds()  # grab all img ids\nprint(f\"Total number of images: {len(imgIds)}\")\ncats = coco.loadCats(coco.getCatIds())  # list all categories\nprint(f\"Total categories: {len(cats)}\")\nannIds = coco.getAnnIds()  # grab all annotation ids\nprint(f\"Total annotations: {len(annIds)}\")","78c409a9":"filtClass = ['person']  # can add more categories\nfiltClassId = coco.getCatIds(catNms=filtClass)  # get their category id\nimgIds2 = coco.getImgIds(catIds=filtClassId)  # get filtered img ids\nannIds2 = coco.getAnnIds(catIds=filtClassId, iscrowd=False)  # iscrowd indicates multi instance objects will be grouped together\nprint(f\"number of filtered images: {len(imgIds2)}\")\nprint(f\"Total annotations: {len(annIds2)}\")\n# grab some of filtered annotations\nex_anns2 = coco.loadAnns(annIds2[:3])\nprint(ex_anns2)","3378628b":"ex_img_id = 397133\n# load img for this id\nex_img = coco.loadImgs(ex_img_id)\nprint(ex_img,'\\n')\n# load annotations id for this image id\nex_ann_ids = coco.getAnnIds(ex_img_id)\nprint(ex_ann_ids,'\\n')\n# load all annotations for the given ann ids\nex_anns = coco.loadAnns(ex_ann_ids)\nprint(ex_anns[0])","1f9b89a8":"# get img from flickr url, requires lib: request, io, matplotlib\ndef display_url_img(url):\n    response = requests.get(url).content\n    img = plt.imread(io.BytesIO(response), format='JPG')\n    plt.imshow(img)\n\ndisplay_url_img(ex_img[0]['flickr_url'])\ncoco.showAnns(ex_anns, draw_bbox=False)","95c902d7":"def _bytes_feature(value):\n  \"\"\"Returns a bytes_list from a string \/ byte.\"\"\"\n  if isinstance(value, type(tf.constant(0))):\n    value = value.numpy() # BytesList won't unpack a string from an EagerTensor. intended for the image data\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _float_feature(value):\n  \"\"\"Returns a float_list from a float \/ double.\"\"\"\n  return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\ndef _int64_feature(value):\n  \"\"\"Returns an int64_list from a bool \/ enum \/ int \/ uint.\"\"\"\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))","d6a869f1":"# returns all the annotation data for a single image id\ndef get_annotations(imgId, catIds):\n    # grab ids of annotations for objects detected in an image id\n    annIds=coco.getAnnIds(imgIds=imgId, catIds=catIds)\n    # loads all annotations for an ann Id\n    anns=coco.loadAnns(annIds)\n    \n    # initialize lists\n    segmentations=[]\n    segmentation_len=[]\n    bboxes=[]\n    catIds=[]\n    iscrowd_list=[]\n    area_list=[]\n    annotation_ids=[]\n    for ann in anns:\n        try:\n            catId=ann['category_id']\n            bbox=ann['bbox']\n            segmentation=ann['segmentation'][0]\n            iscrowd=ann['iscrowd']\n            area=ann['area']\n            annotation_id=ann['id']\n        except:\n            continue\n        if((not None in bbox) and (None!=catId)):\n            catIds.append(catId)\n            segmentations.append(segmentation)\n            segmentation_len.append(len(segmentation))\n            bboxes.append(bbox)\n            iscrowd_list.append(iscrowd)\n            area_list.append(area)\n            annotation_ids.append(annotation_id)\n    return annotation_ids,catIds,sum(segmentations,[]),area_list,sum(bboxes,[]),iscrowd_list,len(anns),segmentation_len","2c68e6c5":"# using chris's way with opencv\ndef process_image(imgFile):\n    img = cv2.imread(img_dir + imgFile)\n    # img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) # Fix incorrect colors (for images in BGR format)\n    img = cv2.resize(img, (img_size,img_size), interpolation=cv2.INTER_AREA)\n    img = cv2.imencode('.jpg', img, (cv2.IMWRITE_JPEG_QUALITY, 94))[1].tostring()\n    return img","ebc86988":"def create_tfrecord(dataset, SIZE, catIds):\n    # number of tfrecord files, right part adds 1 if left not divided equally\n    CT = len(imgIds)\/\/SIZE + int(len(imgIds)%SIZE!=0)\n    for j in range(CT):\n        print('\\nWriting TFRecord %i of %i...'%(j,CT))\n        CT2 = min(SIZE,len(imgIds)-j*SIZE)  # CT2=SIZE unless for last file with remaining images\n        examples = []\n        with tf.io.TFRecordWriter('%s%.2i-%i.tfrec'%(ver,j,CT2)) as writer:\n            for k in range(CT2):  # loop through the batch\n                img = coco.loadImgs(imgIds[SIZE*j+k])[0]  # loadImgs expect a batch, so it returns an array\n                image_string = process_image(img['file_name'])\n                # grab all 8 annotations\n                annotation_ids, catIds, segmentations, area, bboxes, iscrowd, object_len, segmentation_len = get_annotations(img['id'], catIds)\n                # Create a Features message using tf.train.Example\n                example = tf.train.Example(\n                    features=tf.train.Features(\n                        feature={\n                            # image data\n                            'image': _bytes_feature(image_string),  \n                            # image properties\n                            'height': _int64_feature([img['height']]),\n                            'width': _int64_feature([img['width']]),\n                            'id': _int64_feature([img['id']]),\n                            # strings need to be encoded\n                            'file_name': _bytes_feature(tf.compat.as_bytes(img['file_name'])),\n                            'flickr_url': _bytes_feature(tf.compat.as_bytes(img['flickr_url'])),\n                            # list of annotations\n                            'annotation_ids': _int64_feature(annotation_ids),\n                            'category_ids': _int64_feature(catIds),\n                            'segmentations': _float_feature(segmentations),\n                            'area': _float_feature(area),\n                            'bboxes': _float_feature(bboxes),\n                            'iscrowd': _int64_feature(iscrowd),\n                            # additional info\n                            'object_len': _int64_feature([object_len]),\n                            'segmentation_len': _int64_feature(segmentation_len),\n                        }\n                    )\n                )\n                writer.write(example.SerializeToString())\n                if k%400==0: print(k,', ',end='')  # print progress of every 400th image","1b2cc772":"version = [\"val\", \"train\"]\ndir_name = \"\/kaggle\/input\/coco-2017-dataset\/coco2017\/\"\nimg_size = 256\nprint(f\"image resized to: {img_size}x{img_size}\")\n# add category names to filter, ex: 'person','bike', if no filter leave empty\nfiltClass = ['person']\n\n# uncomment to see all category names\n# allCat = coco.loadCats(coco.getCatIds())\n# print([c['name'] for c in allCat])","c1db530c":"for ver in version:\n    img_dir=f\"{dir_name}\/{ver}2017\/\"\n    annotations_file=f\"{dir_name}\/annotations\/instances_{ver}2017.json\"\n\n    # Annotations are stored using JSON. COCO API can be used to access and manipulate all anotations.\n    coco=COCO(annotations_file)  # instantiating COCO API\n    if filtClass:\n        print('grabing filtered class')\n        catIds = coco.getCatIds(catNms=filtClass)  # get their category id\n        imgIds = coco.getImgIds(catIds=catIds)  # grab a set of category\n    else:\n        catIds = coco.getCatIds()  # get all category ids\n        imgIds = coco.getImgIds()  # grab all img ids\n        \n    print(f\"Total number of {ver} images: {len(imgIds)}\")\n    cats = coco.loadCats(catIds)  # list of categories\n    cat_len = len(cats)  # total number of categories in the dataset\n    print(f\"Total number of {ver} categories: {cat_len}\")\n\n    if ver==\"train\":\n        SIZE = 4000 # makes about 30 files\n    else:\n        SIZE = 2000 # makes 3 files for val set\n    \n    create_tfrecord(imgIds, SIZE, catIds)\nprint(\"\\nTFRecords sucessfully created\")\n\n# check the file size\n%ls -s","650c7f62":"def parse(feature):\n    features = tf.io.parse_single_example(\n        feature,\n        features={\n            'image': tf.io.FixedLenFeature([], tf.string),\n            'height': tf.io.FixedLenFeature([], tf.int64),\n            'width': tf.io.FixedLenFeature([], tf.int64),\n            'id': tf.io.FixedLenFeature([], tf.int64),\n            'file_name': tf.io.FixedLenFeature([], tf.string),\n            'flickr_url': tf.io.FixedLenFeature([], tf.string),\n            'annotation_ids': tf.io.VarLenFeature(tf.int64),\n            'category_ids': tf.io.VarLenFeature(tf.int64),\n            'segmentations': tf.io.VarLenFeature(tf.float32),\n            'area': tf.io.VarLenFeature(tf.float32),\n            'bboxes': tf.io.VarLenFeature(tf.float32),\n            'iscrowd': tf.io.VarLenFeature(tf.int64),\n            'object_len': tf.io.FixedLenFeature([], tf.int64),\n            'segmentation_len': tf.io.VarLenFeature(tf.int64),\n        }\n    )\n    \n    print('Image id:')\n    print(features['id'])\n    print('\\nfile_name:')\n    print(features['file_name'])\n    print('\\nflickr_url:')\n    print(features['flickr_url'])\n    print(\"\\nnum of objects:\")\n    print(features['object_len'])\n    print(\"\\nheight:\")\n    print(features['height'])\n    print(\"\\nwidth:\")\n    print(features['width'])\n    print(\"\\nlist of category ids:\")\n    print(features['category_ids'])\n    print(\"\\niscrowd:\")\n    print(features['iscrowd'])\n    print(\"\\nlist of area:\")\n    print(features['area'])\n    print(\"\\nlist of annotation_ids:\")\n    print(features['annotation_ids'])\n    \n    objects = features['object_len']\n    bboxes = features['bboxes']\n    bboxes = tf.sparse.to_dense(bboxes)\n    bboxes = tf.reshape(bboxes, [objects, 4])\n    \n    print(\"\\nbboxes:\")\n    print(bboxes)\n    \n    print(\"\\nsegmentation lengths:\")\n    print(features['segmentation_len'])\n    \n    segmentations = features['segmentations']\n    segmentations = tf.sparse.to_dense(segmentations)\n    segmentation_lengths=tf.sparse.to_dense(features['segmentation_len'])\n    \n    segs=[]\n    start=0\n    for i in segmentation_lengths:\n        segs.append(tf.slice(segmentations,[start,],[i,]))\n        start+=i\n    print(\"\\nSegmentations:\")    \n    print(segs)\n    \n    image = tf.image.decode_jpeg(features['image'], channels=3)\n    \n    anns=[]\n    for i in range(0,len(segs)):\n        #plt.gca().add_patch(Rectangle((i[0],i[1]),i[2],i[3],linewidth=1,edgecolor='r',facecolor='none'))\n        ann={}\n        ann['segmentation']=[segs[i].numpy().tolist()]\n        ann['bbox']=bboxes[i].numpy().tolist()\n        ann['image_id']=features['id'].numpy()  # needed for coco.showAnns\n        anns.append(ann)\n    return image, anns\n","6eb52d00":"# grab all TFRecord files and create a dataset\ndataset = tf.data.TFRecordDataset(tf.io.gfile.glob('.\/*.tfrec'))\nfor i in dataset.take(1):\n    res_image, res_ann = parse(i)","47c8b9ad":"# convert seg polygons to mask, resize, and store each instance in seperate dimension\ndef toMask(anns):\n    train_mask = np.zeros((img_size,img_size,len(anns)))\n    for i in range(len(anns)):\n        ann_mask = coco.annToMask(anns[i])\n        ann_mask = cv2.resize(ann_mask, (img_size,img_size))\n        train_mask[:,:,i] = np.maximum(ann_mask, train_mask[:,:,i])\n    return train_mask\n\n# store mask in same 2D plane, different color\ndef toMaskSamePlane(anns):\n    train_mask = np.zeros((img_size,img_size))\n    for i in range(len(anns)):\n        ann_mask = coco.annToMask(anns[i])\n        ann_mask = cv2.resize(ann_mask*(i+1), (img_size,img_size))\n        train_mask = np.maximum(ann_mask, train_mask)\n    return train_mask","cee5c798":"# show mask in different planes\nmask = toMask(res_ann)\n\nf = plt.figure(figsize=(15,4))\nax = f.add_subplot(141)\nax2 = f.add_subplot(142)\nax3 = f.add_subplot(143)\nax4 = f.add_subplot(144)\nax.imshow(res_image)\nax2.imshow(mask[:,:,0])\nax3.imshow(mask[:,:,1])\nax4.imshow(mask[:,:,2])\n","bfa8cd2e":"# show mask in same plane different color\nmask2 = toMaskSamePlane(res_ann)\nf = plt.figure(figsize=(10,5))\nax1 = f.add_subplot(121)\nax2 = f.add_subplot(122)\nax1.imshow(res_image)\nax2.imshow(mask2)","0be6b486":"### Parameters","1b578b0e":"you can see that the number of images containing only category `1` (person) is less than overall image. Furthermore the annotations must be filtered, and you can notice in the first 3 anns that the 'category_id' only contains `1`","e5e0828f":"### Size comparison\nfor 2000 imgs\/file, size of `val00-2000.tfrec`:\n* 128:  20832\n* 256:  59972 \n* 384: 119312 \n* 512: 194504","9b225b38":"# Efficiency for TPU\n* ideally you want each TFRecord file to be about 100MB to get more efficient processing by TPU\n* Some ops can't be done by TPU, so it needs to be added here","794e1076":"### 3. Example image","a419b8cb":"## Take 1 example","bb461f6f":"## Viewing segmentation masks\ninspired from this amazing [COCO tutorial](https:\/\/towardsdatascience.com\/master-the-coco-dataset-for-semantic-image-segmentation-part-2-of-2-c0d1f593096a) by viraf, [github code](https:\/\/github.com\/virafpatrawala\/COCO-Semantic-Segmentation\/blob\/master\/COCOdataset_SemanticSegmentation_Demo.ipynb)","5f31635a":"# Preview the output\nparse the tfrecord file using `tf.io.parse_single_example`. `FixedLenFeature` is used if the size of data is only 1 element. `VarLenFeature` are for features that contains a list with varying num of elements","85f17eed":"# Thank You\nAny comments or questions are welcome","9a6d2cbc":"# Addressing the problem\n* image is resized but segmentation mask and bounding boxes remains the same.\n    - solution: use coco's annToMask method during TFRecord loading to generate 2D mask, then resize that.\n    - alternative: convert to mask and resize before creating tfrecord, then put each instance in a seperate 3rd dimension `[:,:,0]`, `[:,:,1]` but this might bloat file size\n* stacked object has collision\n    - coco.annToMask where each object is different in pixel value, resulting in slight different color\n    - but it creates another problem, when objects are on top of each other, the segmen area is loss bcz all objects are stacked on the same plane\n    - solution would be using the 3rd dimension for object instance","3f382356":"### 4. COCO Data structure\nFull description on [COCO website](https:\/\/cocodataset.org\/#format-data).\n\n\n\n","43e483b5":"### TFRecord Data Type\nfeatures are bytes if contains string (including image data that uses str64), int64 for integer num, and float for floats. To learn more see tensorflow's [reference](https:\/\/www.tensorflow.org\/tutorials\/load_data\/tfrecord) <br>\nfor single value features (ex: width, height, object_len) must be wrapped in a list\n","5ca4496f":"### 2. Filtered class\nwhat if we only want a certain set of class in the dataset?","80830ecd":"from image properties we can take\n* id: image id\n* width: pixel width of image\n* height: pixel height of image\n* file_name: to access each file\n* flickr_url: for quick load from url\n\nfrom annotations, we should take everything except image_id\n* id: annotation id\n* category_id: reference from category list\n* segmentation: polygon containing mask boundaries\n* area: total area of mask\/segmentation\n* bbox: [x_coordinate, y_coordinate, width and height]\n* iscrowd: 1 means it is an object group (ex: crowd of people)\n\nin addition, these are required to properly parse from TFRecord files\n* object_len: num of objects detected in the image (int)\n* segmentation_len: polygon lengths for each annotation (list)","56f89406":"# COCO EDA\n\n### 1. Global methods\nsee coco's [github page](https:\/\/github.com\/cocodataset\/cocoapi\/blob\/master\/PythonAPI\/pycocotools\/coco.py) to learn more","64b7723b":"### Wait, why TFRecords?\n* to use TPUs more efficiently\n* seperate data preparation from model crafting\n* create 1 time data preparation","0fd9f5cc":"Highest branch consist of:\n```\n{\n  \"info\": info,\n  \"images\": [image],\n  \"annotations\": [annotation],\n  \"licenses\": [license],\n}\n```\nImage array consist of:\n```\nimage{\n\"id\": int, \"width\": int, \"height\": int, \"file_name\": str, \"license\": int, \"flickr_url\": str, \"coco_url\": str, \"date_captured\": datetime,\n}\n```\nAnnotations from `instances_train.json` consist of:\n```\nannotation{\n\"id\": int, \"image_id\": int, \"category_id\": int, \"segmentation\": RLE or [polygon], \"area\": float, \"bbox\": [x,y,width,height], \"iscrowd\": 0 or 1,\n}\n```","6b52ab45":"### Create files","0080c495":"# Writing TFRecords\n* workflow inspired from [this](https:\/\/www.kaggle.com\/cdeotte\/how-to-create-tfrecords) notebook\n* feature format mostly from [this](https:\/\/www.kaggle.com\/karthikeyanvijayan\/coco-object-detection-dataset-in-tfrecord) notebook"}}