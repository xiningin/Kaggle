{"cell_type":{"be184581":"code","e14aa99b":"code","f73b3388":"code","2dcd8321":"code","991a89ac":"code","11bfe235":"code","ecf92cd3":"code","51cb8615":"code","50b49364":"code","fe0ede9c":"code","ff522974":"code","83a10a6c":"code","2b9568dc":"code","24596adb":"code","a76c40af":"code","16cfb101":"code","1f4116cd":"code","d446600e":"code","21bf9074":"code","77c96f46":"code","7af9b721":"code","febea7cf":"code","b84b547c":"code","bfd9a920":"code","28d3b96e":"code","d81f670d":"code","845e8892":"code","b4e2f282":"code","0992720c":"code","b3b44828":"code","3fb2f0af":"code","ac895116":"code","b4cc8056":"code","c2542dce":"code","b95f467f":"code","d5745830":"markdown","5ee04022":"markdown","14614a46":"markdown","bdbf5930":"markdown","bc5b8e80":"markdown","c5b835f1":"markdown","318ee0dd":"markdown","70331d6f":"markdown","405318d9":"markdown","6231d334":"markdown","947f1c2a":"markdown","b42eeb30":"markdown","ee5d7447":"markdown","7687eaf3":"markdown","fb59d77c":"markdown","1772d656":"markdown","6516e826":"markdown","9546c364":"markdown"},"source":{"be184581":"import numpy as np\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pylab as plt\n%matplotlib inline             \nfrom statsmodels.tsa.stattools import adfuller\n'''\nadfuller. Augmented Dickey-Fuller unit root test. The Augmented Dickey-Fuller test can be used to test for a unit root in a univariate process \nin the presence of serial correlation.\n\n'''\nfrom statsmodels.tsa.stattools import acf, pacf\n'''\nacf. Calculate the autocorrelation function. If True, returns the Ljung-Box q statistic for each autocorrelation coefficient.\n\nIf a number is given, the confidence intervals for the given level are returned. ... 05, 95 % confidence intervals are returned where the standard deviation is computed according to 1\/sqrt(len(x)). \nReturns pacf ndarray. Partial autocorrelations, nlags elements, including lag zero.\n\n'''\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom matplotlib.pylab import rcParams\n'''\nEach time Matplotlib loads, it defines a runtime configuration (rc) containing the default styles for every plot element you create. \nThis configuration can be adjusted at any time using the plt. \nmatplotlibrc file, which you can read about in the Matplotlib documentation.\n\n'''\nrcParams['figure.figsize'] = 10, 6\n","e14aa99b":"data = pd.read_csv('..\/input\/air-passengers\/AirPassengers.csv')\ndata.head()","f73b3388":"data.dtypes","2dcd8321":"\n# parse strings \ndata['Month'] = pd.to_datetime(data['Month'], infer_datetime_format=True)\n","991a89ac":"data.dtypes","11bfe235":"indexedData = data.set_index(['Month'])\nindexedData.head()","ecf92cd3":"# plot graph\nplt.xlabel('Date')\nplt.ylabel('Number of air passengers')\nplt.plot(indexedData)","51cb8615":"# Datermine rolling statistics \nrolmean = indexedData.rolling(window=12).mean() # window size 12 denotes 12 months, giving rolling mean at yearly level\n# rolmean.head()\nrolstd = indexedData.rolling(window=12).std()\n","50b49364":"print(rolmean,rolstd)\n","fe0ede9c":"# plot rolling statistics\norig = plt.plot(indexedData, color='g', label='Original')\nmean = plt.plot(rolmean, color='r', label='Rolling Mean')\nstd = plt.plot(rolstd, color='b', label='Rolling Std')\nplt.legend(loc='best')\nplt.title(\"Rolling Mean & Standard Deviation\")\nplt.show(block=False)","ff522974":"# perform augmented dickey-fuller test\nprint(\"Results of Dickey Fuller Test:\")\ndftest = adfuller(indexedData[\"#Passengers\"], autolag='AIC')\n'''\nautolag{\u201cAIC\u201d, \u201cBIC\u201d, \u201ct-stat\u201d, None } Method to use when automatically determining the lag length among the values 0, 1,\nmaxlag. If \u201cAIC\u201d (default) or \u201cBIC\u201d, then the number of lags is chosen to minimize the corresponding information criterion\n\n'''\ndfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\nfor key,value in dftest[4].items():\n    dfoutput['Critical Value (%s)'%key] = value\n    \nprint(dfoutput)","83a10a6c":"# estimating trend\nindexedData_logscale = np.log(indexedData)\nplt.plot(indexedData_logscale)","2b9568dc":"# the below transformation is required to make series stationary\nmovingAverage = indexedData_logscale.rolling(window=12).mean()\nmovingSTD = indexedData_logscale.rolling(window=12).std()\nplt.plot(indexedData_logscale)\nplt.plot(movingAverage, color='r')\nplt.title(\"movingAverage & movingSTD\")\n\nplt.show()","24596adb":"datasetLogScaleMinusMovingAverage = indexedData_logscale - movingAverage\ndatasetLogScaleMinusMovingAverage.head(12)\n","a76c40af":"# Remove Nan values\ndatasetLogScaleMinusMovingAverage.dropna(inplace=True)\ndatasetLogScaleMinusMovingAverage.head(10)","16cfb101":"def test_stationarity(timeseries):\n    \n    #Determine rolling statistics\n    movingAverage = timeseries.rolling(window=12).mean()\n    movingSTD = timeseries.rolling(window=12).std()\n    \n    #Plot rolling statistics\n    orig = plt.plot(timeseries, color='g', label='Original')\n    mean = plt.plot(movingAverage, color='r', label='Rolling Mean')\n    std = plt.plot(movingSTD, color='b', label='Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n    \n    #Perform Dickey\u2013Fuller test:\n    print('Results of Dickey Fuller Test:')\n    dftest = adfuller(timeseries['#Passengers'], autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput)\n","1f4116cd":"test_stationarity(datasetLogScaleMinusMovingAverage)","d446600e":"exponentialDecayWeightedAverage = indexedData_logscale.ewm(halflife=12, min_periods=0, adjust=True).mean()\n'''\nThe ewm() function is used to provide exponential weighted functions. Specify decay in terms of center of mass, \u03b1=1\/(1+com), \nfor com\u22650. Specify decay in terms of span, \u03b1=2\/(span+1), for span\u22651. Specify decay in terms of half-life, \u03b1=1\u2212exp(log(0.5)\/halflife),forhalflife>0.\n'''\nplt.plot(indexedData_logscale)\nplt.plot(exponentialDecayWeightedAverage,color='g')\nplt.show()","21bf9074":"datasetLogScaleMinusExponentialMovingAverage = indexedData_logscale - exponentialDecayWeightedAverage\ntest_stationarity(datasetLogScaleMinusExponentialMovingAverage)\n","77c96f46":"datasetLogDiffShifting = indexedData_logscale - indexedData_logscale.shift()\nplt.plot(datasetLogDiffShifting,color='orange')\nplt.show()","7af9b721":"datasetLogDiffShifting.dropna(inplace=True)\ntest_stationarity(datasetLogDiffShifting)\n","febea7cf":"decomposition = seasonal_decompose(indexedData_logscale) \n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.subplot()\nplt.plot(indexedData_logscale, label='Original')\nplt.legend(loc='best')\nplt.show()","b84b547c":"plt.subplot()\nplt.plot(trend, label='Trend',color='red')\nplt.legend(loc='best')\nplt.show()","bfd9a920":"plt.subplot()\nplt.plot(seasonal, label='Seasonality',color='orange')\nplt.legend(loc='best')\nplt.show()","28d3b96e":"plt.subplot()\nplt.plot(residual, label='Residuals',color='brown')\nplt.legend(loc='best')\nplt.show()","d81f670d":"'''\nthere can be cases where an observation simply consisted of trend and seasonality. in that case, there won't be\nany residual component and would be a null or Nan. heance, we also remove such cases\n\n'''\ndecomposedLogData = residual\ndecomposedLogData.dropna(inplace=True)\n# test_stationarity(decomposedLogData)","845e8892":"#ACF & PACF plots\n\nlag_acf = acf(datasetLogDiffShifting, nlags=20)\nlag_pacf = pacf(datasetLogDiffShifting, nlags=20, method='ols')\n\n# plot ACF:\nplt.subplot(121)\nplt.plot(lag_acf)\nplt.axhline(y=0, linestyle='--',color='red')\n# axhline() Function. The axhline() function in pyplot module of matplotlib library is used to add a horizontal line across the axis.\nplt.axhline(y=-1.96\/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\nplt.title('Autocorrelation Function')  \n\n#Plot PACF\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0, linestyle='--', color='red')\nplt.axhline(y=-1.96\/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(datasetLogDiffShifting)), linestyle='--', color='gray')\nplt.title('Partial Autocorrelation Function')\n            \nplt.tight_layout()            ","b4e2f282":"'''\nIn fact, the AR and MA components are identical, combining a general autoregressive model AR(p) and general moving average model MA(q). \nAR(p) makes predictions using previous values of the dependent variable. \nMA(q) makes predictions using the series mean and previous errors.\n\n\nNon-seasonal ARIMA models are generally denoted ARIMA(p,d,q) where parameters p, d, and q are non-negative integers, \np is the order (number of time lags) of the autoregressive model, \nd is the degree of differencing (the number of times the data have had past values subtracted), and \nq is the order of the moving-average\n\ndisp = If True, convergence information is printed. For the default l_bfgs_b solver, disp controls the frequency of the output during the iterations. \ndisp < 0 means no output in this case.\n\n\n'''\n#AR Model\n#making order=(2,1,0) gives RSS=1.5023\nmodel = ARIMA(indexedData_logscale, order=(2,1,0))\nresults_AR = model.fit(disp=-1)\nplt.plot(datasetLogDiffShifting)\nplt.plot(results_AR.fittedvalues, color='Gold')\nplt.title('RSS: %.4f'%sum((results_AR.fittedvalues - datasetLogDiffShifting['#Passengers'])**2))\nprint('Plotting AR model')","0992720c":"#MA Model\nmodel = ARIMA(indexedData_logscale, order=(0,1,2))\nresults_MA = model.fit(disp=-1)\nplt.plot(datasetLogDiffShifting,color='orange')\nplt.plot(results_MA.fittedvalues, color='green')\nplt.title('RSS: %.4f'%sum((results_MA.fittedvalues - datasetLogDiffShifting['#Passengers'])**2))\nprint('Plotting MA model')","b3b44828":"# AR+I+MA = ARIMA model\nmodel = ARIMA(indexedData_logscale, order=(2,1,2))\nresults_ARIMA = model.fit(disp=-1)\nplt.plot(datasetLogDiffShifting,color='blue')\nplt.plot(results_ARIMA.fittedvalues, color='red')\nplt.title('RSS: %.4f'%sum((results_ARIMA.fittedvalues - datasetLogDiffShifting['#Passengers'])**2))\nprint('Plotting ARIMA model')","3fb2f0af":"predictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy = True)\npredictions_ARIMA_diff.head()","ac895116":"#Convert to cumulative sum\npredictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\npredictions_ARIMA_diff_cumsum.head()","b4cc8056":"predictions_ARIMA_log = pd.Series(indexedData_logscale['#Passengers'].iloc[0], index=indexedData_logscale.index)\npredictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum, fill_value=0)\npredictions_ARIMA_log.head()","c2542dce":"# Inverse of log is exp\npredictions_ARIMA = np.exp(predictions_ARIMA_log)\nplt.plot(indexedData)\nplt.plot(predictions_ARIMA)\nplt.show()","b95f467f":"#We have 144(existing data of 12 yrs in months) data points. \n#And we want to forecast for additional 120 data points or 10 yrs.\n# results_ARIMA.plot_predict(1,264) \n#x=results_ARIMA.forecast(steps=120)\nresults_ARIMA.plot_predict(1,264) \n","d5745830":"By combining AR & MA into ARIMA, we see that RSS value has decreased from either case to 1.0292, indicating ARIMA to be better than its individual component models.   \n\nWith the ARIMA model built, we will now generate predictions. But, before we do any plots for predictions ,we need to reconvert the predictions back to original form. This is because, our model was built on log transformed data.","5ee04022":"From above graph, it seems that exponential decay is not holding any advantage over log scale as both the corresponding curves are similar. But, in statistics, inferences cannot be drawn simply by looking at the curves. Hence, we perform the ADCF test again on the decay series below.\n\n","14614a46":"From the above graph, we see that rolling mean itself has a trend component even though rolling standard deviation is fairly constant with time. For our time series to be stationary, we need to ensure that both the rolling statistics ie: mean & std. dev. remain time invariant or constant with time. Thus the curves for both of them have to be parallel to the x-axis, which in our case is not so.\n\nTo further augment our hypothesis that the time series is not stationary, let us perform the ADCF test.\n\n","bdbf5930":"### Exponential Decay Transformation","bc5b8e80":"From the ACF graph, we see that curve touches y=0.0 line at x=2. Thus, from theory, Q = 2\nFrom the PACF graph, we see that curve touches y=0.0 line at x=2. Thus, from theory, P = 2\n\nARIMA is AR + I + MA. Before, we see an ARIMA model, let us check the results of the individual AR & MA model. Note that, these models will give a value of RSS. Lower RSS values indicate a better model.","c5b835f1":"For a Time series to be stationary, its ADCF test should have:\n\n* p-value to be low (according to the null hypothesis)\n* The critical values at 1%,5%,10% confidence intervals should be as close as possible to the Test Statistics\n\nFrom the above ADCF test result, we see that p-value(at max can be 1.0) is very large. Also critical values are no where close to the Test Statistics. Hence, we can safely say that our Time Series at the moment is not stationary\n\nData Transformation to achieve Stationarity \nThere are a couple of ways to achieve stationarity through data transformation like taking  log10 , loge , square, square root, cube, cube root, exponential decay, time shift and so on ...\n\nIn our notebook, lets start of with log transformations. Our objective is to remove the trend component. Hence, flatter curves( ie: paralle to x-axis) for time series and rolling mean after taking log would say that our data transformation did a good job.\n\n","318ee0dd":"### Prediction & Reverse Transformations","70331d6f":"### Time Shift Transformation","405318d9":"From above graph, we observe that our intuition that \"subtracting two related series having similar trend components will make the result stationary\" is true. We find that:\n\n* p-value has reduced from 0.99 to 0.022.\n* The critical values at 1%,5%,10% confidence intervals are pretty close to the Test Statistic. Thus, from above 2 points, we can say that our given series is stationary.\n\nBut, in the spirit of getting higher accuracy, let us explore & try to find a better scale than our current log.\n\nLet us try out Exponential decay.\nFor further info, refer to my answer 12 at the top of the notebook on it.\n\n","6231d334":"### Time Series with ARIMA","947f1c2a":"From above graph, we see that even though rolling mean is not stationary, it is still better than the previous case, where no transfromation were applied to series. So we can atleast say that we are heading in the right direction.\n\nWe know from above graph that both the Time series with log scale as well as its moving average have a trend component. Thus we can apply a elementary intuition: subtraction one from the other should remove the trend component of both. Its like:\n\n![image.png](attachment:33b39524-b371-49f2-870f-131703576800.png)","b42eeb30":"From the plot below, we can see that there is a Trend compoenent in th series. Hence, we now check for stationarity of the data\n\n","ee5d7447":"Build a model to forecast the demand(passenger traffic) in Airplanes. The data is classified in date\/time and the passengers travelling per month\n\n","7687eaf3":"From above 2 graphs, we can see that, visually this is the best result as our series along with rolling statistic values of moving avg & moving std. dev. is very much flat & stationary. But, the ADCF test shows us that:\n\n* p-value of 0.07 is not as good as 0.005 of exponential decay.\n* Test Statistic value not as close to the critical values as that for exponential decay.\n\nWe have thus tried out 3 different transformation: log, exp decay & time shift. For simplicity, we will go with the log scale. The reason for doing this is that we can revert back to the original scale during forecasting.\n\nLet us now break down the 3 components of the log scale series using a system libary function. Once, we separate our the components, we can simply ignore trend & seasonality and check on the nature of the residual part.\n\n","fb59d77c":"We observe that the Time Series is stationary & also the series for moving avg & std. dev. is almost parallel to x-axis thus they also have no trend.\nAlso,\n\np-value has decreased from 0.022 to 0.005.\nTest Statistic value is very much closer to the Critical values.\nBoth the points say that our current transformation is better than the previous logarithmic transformation. Even though, we couldn't observe any differences by visually looking at the graphs, the tests confirmed decay to be much better.\nBut lets try one more time & find if an even better solution exists. We will try out the simple time shift technique, which is simply:\n\n![image.png](attachment:1f55e875-e2c9-437e-bbd4-521451f5df24.png)\n\n","1772d656":"What is Time series analysis - \n\nA. Time Series is a series of observations taken at specified time intervals usually equal intervals. Analysis of the series helps us to predict future values based on previous observed values. In Time series, we have only 2 variables, time & the variable we want to forecast.\n\n","6516e826":"### Building Models","9546c364":"We see that our predicted forecasts are very close to the real time series values indicating a fairly accurate model."}}