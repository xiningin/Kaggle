{"cell_type":{"7239b618":"code","18a074cb":"code","a59976fd":"code","63bc3772":"code","ae9c4a17":"code","df7d8d7c":"code","768d3464":"code","1813edf5":"code","2473f9ab":"code","ed0fdb49":"code","fcb0ea25":"code","aa0759bf":"code","d778a50b":"code","e895b3c7":"code","713b3a86":"code","dc6bae62":"code","95902707":"code","8f426986":"code","358e4d56":"code","f4e0c93b":"code","9069e984":"code","7d4d3ebd":"code","c80e4c54":"code","3af3b8f0":"code","74038e37":"code","5a947cc2":"code","58b5f9c3":"code","958c0ccb":"code","8b2c9e14":"code","4a0bf647":"code","ce889661":"code","7d324ea7":"code","3938631a":"code","6b08fe27":"code","dbb50771":"code","70f24a80":"code","35af4a52":"code","32633452":"code","918fb845":"code","cb943e07":"code","ccba3281":"code","e94265e8":"code","75071411":"code","5ae22286":"code","dc458f91":"code","f906385d":"code","dcee5579":"code","77b35227":"code","bd19eb74":"code","81a742f2":"code","1f1d9ded":"code","412d39fb":"code","cbaa802d":"code","06135c5a":"code","e6f6463a":"markdown","d4e42f4b":"markdown","d1e6b537":"markdown","fde2bbfc":"markdown","be3971e8":"markdown","eb6982e3":"markdown","437cddb9":"markdown","b0d6a670":"markdown","fa6b23de":"markdown","72bf6e04":"markdown","b4261b1b":"markdown","49748134":"markdown","f7616eb9":"markdown","cb9955d3":"markdown","7649b879":"markdown","c820d0b9":"markdown","639ed5b3":"markdown","60796fc6":"markdown","8421866d":"markdown","00e328de":"markdown","8d8b06bc":"markdown","b959dd53":"markdown","86fce0bd":"markdown"},"source":{"7239b618":"%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', None)\n#pd.set_option('display.max_rows', None)\n\nimport gc\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, average_precision_score\nfrom sklearn.metrics import precision_recall_curve, plot_precision_recall_curve, auc\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom plotly import tools\nfrom plotly.offline import iplot\nimport plotly.graph_objs as go\n#to link plotly to pandas\nimport cufflinks as cf\ncf.go_offline()\ncf.set_config_file(offline = False, world_readable = True)\n\nfrom IPython.display import display\n\nplt.rcParams[\"figure.figsize\"] = (12, 8)\nplt.rcParams['axes.titlesize'] = 16\nplt.style.use('seaborn-whitegrid')\nsns.set_palette('Set2')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom time import time, strftime, gmtime\nstart = time()\nimport datetime\nprint(str(datetime.datetime.now()))\n\nimport warnings\nwarnings.simplefilter('ignore')","18a074cb":"bank_trans = pd.read_csv('\/kaggle\/input\/banksim1\/bs140513_032310.csv')\nprint(bank_trans.shape)\nbank_trans.head()","a59976fd":"bank_network = pd.read_csv('\/kaggle\/input\/banksim1\/bsNET140513_032310.csv')\nprint(bank_network.shape)\nbank_network.head()","63bc3772":"bank_trans.info()","ae9c4a17":"bank_trans.isna().any()","df7d8d7c":"cols_to_change = [c for c in bank_trans.columns if c not in ['step', 'amount', 'fraud']]\ncols_to_change","768d3464":"bank_trans[cols_to_change] = bank_trans[cols_to_change].applymap(lambda x: x.replace(\"'\", \"\"))","1813edf5":"for col in bank_trans.columns:\n    print(f\"Feature {col.upper()} has {bank_trans[col].nunique()} items\")","2473f9ab":"bank_trans = bank_trans.drop(['zipcodeOri', 'zipMerchant'], axis = 1)\nbank_trans.head(2)","ed0fdb49":"#Viz Utils\n\ndef countplot_helper(data: pd.DataFrame, col: str, title: str = None, hue: str = None):\n    plt.figure(figsize = (16, 12))\n    plt.title(title)\n    ax = sns.countplot(data = data, x = col, order = data[col].value_counts().index,\n                       hue = hue if hue else None)\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x() + p.get_width() \/ 2.0, height + 3,\n                f\"{round(100 * height \/ len(data[col]), 2)}%\",\n                ha = 'center')\n    if data[col].nunique() > 5:\n        plt.xticks(rotation = 45)\n    plt.show()","fcb0ea25":"countplot_helper(bank_trans, 'fraud', title = 'No. of Geniune\/Fraud Transactions')","aa0759bf":"countplot_helper(bank_trans, 'age', title = 'Feature - Age Distribution', hue = 'fraud')","d778a50b":"countplot_helper(bank_trans, 'gender', title = 'Feature - Gender Distribution', hue = 'fraud')","e895b3c7":"countplot_helper(bank_trans, 'category', title = 'Feature - Category Distribution')","713b3a86":"countplot_helper(bank_trans, 'category', title = 'Feature - Category Distribution', hue = 'fraud')","dc6bae62":"fraud_cats = list(bank_trans[bank_trans['fraud'] == 1]['category'].unique())\nprint(\"The categories where fraud has been detected:\")\nprint(*fraud_cats, sep = '\\n')","95902707":"temp = bank_trans[bank_trans['category'] != 'es_transportation']\ncountplot_helper(temp, 'category', title = 'Feature - Category Distribution without ES_TRANSPORTATION', hue = 'fraud')","8f426986":"fig, (ax1, ax2) = plt.subplots(ncols = 2, figsize = (16, 8))\nax1.set_title('Boxplot of Amount')\nsns.boxplot(data = bank_trans, x = 'fraud', y = 'amount', hue = 'fraud', ax = ax1)\n\nax2.set_title('Log Distribution of Amount')\nsns.distplot(np.log1p(bank_trans[bank_trans['fraud'] == 0]['amount']), label = 'No Fraud', ax = ax2)\nsns.distplot(np.log1p(bank_trans[bank_trans['fraud'] == 1]['amount']), label = 'Fraud', ax = ax2)\nax2.legend()\nplt.show()","358e4d56":"print('Fraud Transaction:')\nprint(f\"Max. Transaction Amount: {bank_trans[bank_trans['fraud'] == 1]['amount'].max()}\")\nprint(f\"Min. Transaction Amount: {bank_trans[bank_trans['fraud'] == 1]['amount'].min()}\")\nprint(f\"Avg. Transaction Amount: {bank_trans[bank_trans['fraud'] == 1]['amount'].mean()}\")\nprint(f\"Median Transaction Amount: {bank_trans[bank_trans['fraud'] == 1]['amount'].median()}\")\nprint('\\nNo Fraud Transaction:')\nprint(f\"Max. Transaction Amount: {bank_trans[bank_trans['fraud'] == 0]['amount'].max()}\")\nprint(f\"Min. Transaction Amount: {bank_trans[bank_trans['fraud'] == 0]['amount'].min()}\")\nprint(f\"Avg. Transaction Amount: {bank_trans[bank_trans['fraud'] == 0]['amount'].mean()}\")\nprint(f\"Median Transaction Amount: {bank_trans[bank_trans['fraud'] == 9]['amount'].median()}\")","f4e0c93b":"def bivariate_plots(df, x, y):\n    fig, ax = plt.subplots()\n    plt.scatter(df[x], df[y], c = 'blue', edgecolors = 'none', alpha = 0.5)\n    plt.xlabel(x)\n    plt.ylabel(y)\n    plt.title(f'{x} vs. {y}')\n    plt.show()","9069e984":"plt.title('Density Distribution plot of Step')\nsns.kdeplot(bank_trans[bank_trans['fraud'] == 0]['step'], shade = True, label = 'No Fraud')\nsns.kdeplot(bank_trans[bank_trans['fraud'] == 1]['step'], shade = True, label = 'Fraud')\nplt.legend()\nplt.show()","7d4d3ebd":"fraud_df = bank_trans[bank_trans['fraud'] == 1].copy()\nnofraud_df = bank_trans[bank_trans['fraud'] == 0].copy()\nfraud_df.shape, nofraud_df.shape","c80e4c54":"fig, (ax1, ax2) = plt.subplots(ncols = 2, figsize = (20, 8))\nfraud_df.groupby('step')['amount'].mean().plot(kind = 'line', label = 'Fraud', color = 'violet', legend = True, ax = ax1)\nax1.set_ylabel('Avg. Amount')\nnofraud_df.groupby('step')['amount'].mean().plot(kind = 'line', label = 'No Fraud', color = 'blue', legend = True, ax = ax2)\nax2.set_ylabel('Avg. Amount')\nplt.suptitle('Step Vs Avg. Transaction Amount', fontsize = 16)\nplt.show()","3af3b8f0":"def subplots_helper(df: pd.DataFrame, col: str):\n    colors = plt.rcParams['axes.prop_cycle']()\n    for i, gender in enumerate(df[col].unique()):\n        fig, (ax1, ax2) = plt.subplots(ncols = 2, nrows = 1, figsize = (20, 8))\n        c = next(colors)['color']\n        fraud_df[fraud_df[col] == gender].groupby('step')['amount'].mean().plot(kind = 'line', label = 'Fraud', color = c, legend = True, ax = ax1)\n        ax1.set_ylabel('Avg. Amount')\n        c = next(colors)['color']\n        nofraud_df[nofraud_df[col] == gender].groupby('step')['amount'].mean().plot(kind = 'line', label = 'No Fraud', color = c, legend = True, ax = ax2)\n        ax2.set_ylabel('Avg. Amount')\n        plt.suptitle(f'Step Vs Avg. Transaction Amount for {col}: {gender}', y = 0.95, fontsize = 16)\n        plt.figtext(0.5, 0.01, f\"Figure {i + 1}\", ha = 'center', fontsize = 16, bbox = {\"facecolor\": \"grey\", \"alpha\": 0.5, \"pad\": 5})\n        plt.show()\n    return None","74038e37":"subplots_helper(bank_trans, 'gender')","5a947cc2":"subplots_helper(bank_trans, 'age')","58b5f9c3":"def cols_to_dict(col: str, kind: str = None):\n    temp_dict = {}\n    for val in fraud_df[col].unique():\n        temp_dict[val] = len(nofraud_df[nofraud_df[col] == val])\n    temp = pd.Series(temp_dict).to_frame(col)\n    temp.iplot(kind = 'line' if kind else 'bar', \n              xTitle = col.title(), \n              yTitle = 'No. of Transactions', \n              title = f'No. of Normal Transactions done by {col.title()} in Fraudulent Transaction')\n    return None","958c0ccb":"cols_to_dict('merchant')","8b2c9e14":"cols_to_dict('customer', kind = 'line')","4a0bf647":"countplot_helper(fraud_df, col = 'merchant')","ce889661":"temp = fraud_df[fraud_df['merchant'] == 'M480139044'][['category', 'amount']]\nprint(f\"Top Fraud category: {temp['category'].unique()}, Total amount transacted: {temp['amount'].sum()}\")\ntemp = fraud_df[fraud_df['merchant'] == 'M980657600'][['category', 'amount']]\nprint(f\"Top Fraud category: {temp['category'].unique()}, Total amount transacted: {temp['amount'].sum()}\")","7d324ea7":"fraud_df.groupby('category')['amount'].sum().sort_values().iplot(kind = 'bar', \n                                                  xTitle = 'Category', \n                                                  yTitle = 'Amount', \n                                                  title = 'Total Fradulent Amount Transacted in each Category', \n                                                  color = 'green')","3938631a":"fraud_df[fraud_df['category'] == 'es_travel']['merchant'].unique()","6b08fe27":"fraud_df[fraud_df['category'] == 'es_travel'][['merchant', \n                                               'amount']].groupby('merchant')['amount'].sum().iplot(kind = 'bar', \n                                                                                                               xTitle = 'Merchants', \n                                                                                                               yTitle = 'Amount', \n                                                                                                               title = 'Total Fraudulent Amount Transacted in es_travel Category by Merchants', \n                                                                                                               color = 'red')","dbb50771":"def pivotplot_helper(df: pd.DataFrame, cat: str = None):\n    if cat is None:\n        print('Please enter the category to plot')\n        return\n    colors = ['brown', 'pink', 'red', 'blue', 'green', 'orange', 'purple']\n    temp = df[df['category'] == cat][['merchant', 'amount', 'fraud']]\n    temp = pd.pivot_table(data = temp, columns = ['merchant', 'fraud'], \n                          values = ['amount'], aggfunc = 'sum', fill_value = 0)\n\n    temp.T.iplot(kind = 'bar', \n                xTitle = 'Merchant - Fraud', \n                yTitle = 'Amount', \n                title = f'Total Amount Transacted by Merchant-Fraud\/noFraud for {cat} Category', \n                color = np.random.choice(colors, 1))\n    return None","70f24a80":"for cat in fraud_cats:\n    pivotplot_helper(bank_trans, cat = cat)","35af4a52":"#Frequency Encoding customer and merchant features\nfor col in ['customer', 'merchant']:\n    print(f\"Frequency Encoding: {col} - {bank_trans[col].nunique()}\")\n    freq = bank_trans[col].value_counts()\n    bank_trans[col] = bank_trans[col].apply(lambda x: freq[x])","32633452":"#Label Encoding age, gender, category\nfor col in ['gender', 'age', 'category']:\n    print(f\"Label Encoding: {col} - {bank_trans[col].nunique()}\")\n    le = LabelEncoder()\n    bank_trans[col] = le.fit_transform(bank_trans[col])","918fb845":"bank_trans['amount'] = StandardScaler().fit_transform(np.array(bank_trans['amount']).reshape(-1, 1))","cb943e07":"def plot_confusion(mat):\n    plt.figure(figsize = (8, 4))\n    sns.heatmap(pd.DataFrame(mat), annot = True, annot_kws = {\"size\": 25}, cmap = 'Blues', fmt = 'g')\n    plt.title('Confusion matrix', y = 1.1, fontsize = 22)\n    plt.ylabel('Actual', fontsize = 18)\n    plt.xlabel('Predicted', fontsize = 18)\n    plt.show()","ccba3281":"sample = bank_trans.sample(frac = 1).reset_index(drop = True)\nsample.shape","e94265e8":"X = sample.drop(['fraud', 'step'], axis = 1)\ny = sample['fraud'].copy()","75071411":"#scale_pos_weight - sample Weights\nnum_pos_samples = y.value_counts().values[1]\nnum_neg_samples = y.value_counts().values[0]\nnum_neg_samples \/ num_pos_samples, np.sqrt(num_neg_samples \/ num_pos_samples)","5ae22286":"def plot_pr_curve(precision, recall, avg_precision):\n    plt.figure(figsize = (8, 6))\n    plt.plot(recall, precision, label = f\"Avg. PR: {round(avg_precision, 2)}\")\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.ylim([0.5, 1.05])\n    plt.xlim([0.55, 1.0])\n    plt.title('Precision-Recall Curve')\n    plt.legend()\n    plt.show()","dc458f91":"from xgboost import XGBClassifier\nimport xgboost as xgb\n\nxgb_params = {\n         'objective': 'binary:logistic',\n         'lambda': 0.0030282073258141168, \n         'alpha': 0.01563845128469084,\n         'colsample_bytree': 0.55,\n         'subsample': 0.7,\n         'learning_rate': 0.01,\n         'max_depth': 9,\n         'random_state': 2020, \n         'min_child_weight': 257,\n         'eval_metric': 'aucpr',\n         'seed': 2021,\n         'scale_pos_weight': np.sqrt(num_neg_samples \/ num_pos_samples) #np.sqrt()?\n         }","f906385d":"n_folds = 4\npreds_xg = []\n\nskf = StratifiedKFold(n_splits = n_folds)\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(X, y)):\n    print(f\"Fold: {i + 1}\")\n    Xtrain, ytrain = X.iloc[trn_idx], y[trn_idx]\n    Xvalid, yvalid = X.iloc[val_idx], y[val_idx]\n    \n    xg_train = xgb.DMatrix(Xtrain, label = ytrain)\n    xg_valid = xgb.DMatrix(Xvalid, label = yvalid)\n\n    clf = xgb.train(xgb_params,\n                              xg_train,\n                              10000,\n                              verbose_eval = 200,\n                              evals = [(xg_train, 'train'), (xg_valid, 'valid')],\n                              early_stopping_rounds = 100)\n\n    valid_preds = clf.predict(xg_valid)\n    \n    avg_precision = average_precision_score(yvalid, valid_preds)\n    print(f'\\nAvg. Precision-Recall Score: {avg_precision}')\n    \n    precision, recall, _ = precision_recall_curve(yvalid, valid_preds)\n    plot_pr_curve(precision, recall, avg_precision)\n    \n    conf_mat = confusion_matrix(yvalid, valid_preds > 0.5)\n    plot_confusion(conf_mat)\n    \n    print(f\"Classification Report: \\n{classification_report(yvalid, valid_preds > 0.5, labels = [0, 1])}\")\n    \n    preds_xg.append(valid_preds)\n    print()","dcee5579":"xgb.plot_importance(clf);","77b35227":"temp_fraud = bank_trans[bank_trans['fraud'] == 1].reset_index(drop = True)\ntemp_nofraud = bank_trans[bank_trans['fraud'] == 0].reset_index(drop = True)\ntemp_nofraud = temp_nofraud.sample(n = 216000).reset_index(drop = True)\ndf = pd.concat([temp_fraud, temp_nofraud]).reset_index(drop = True)\ndf.shape","bd19eb74":"countplot_helper(df, 'fraud', title = 'Target Countplot after UnderSampling')","81a742f2":"X = df.drop(['fraud', 'step'], axis = 1)\ny = df['fraud'].copy()","1f1d9ded":"#scale_pos_weight - sample Weights\nnum_pos_samples = y.value_counts().values[1]\nnum_neg_samples = y.value_counts().values[0]\nxgb_params['scale_pos_weight'] = np.sqrt(num_neg_samples \/ num_pos_samples) #set the new class weights\n\nnum_neg_samples \/ num_pos_samples, np.sqrt(num_neg_samples \/ num_pos_samples)","412d39fb":"n_folds = 4\npreds_xg = []\n\nskf = StratifiedKFold(n_splits = n_folds)\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(X, y)):\n    print(f\"Fold: {i + 1}\")\n    Xtrain, ytrain = X.iloc[trn_idx], y[trn_idx]\n    Xvalid, yvalid = X.iloc[val_idx], y[val_idx]\n    \n    xg_train = xgb.DMatrix(Xtrain, label = ytrain)\n    xg_valid = xgb.DMatrix(Xvalid, label = yvalid)\n\n    clf = xgb.train(xgb_params,\n                              xg_train,\n                              10000,\n                              verbose_eval = 200,\n                              evals = [(xg_train, 'train'), (xg_valid, 'valid')],\n                              early_stopping_rounds = 100)\n\n    valid_preds = clf.predict(xg_valid)\n    \n    avg_precision = average_precision_score(yvalid, valid_preds)\n    print(f'\\nAvg. Precision-Recall Score: {avg_precision}')\n    \n    precision, recall, _ = precision_recall_curve(yvalid, valid_preds)\n    plot_pr_curve(precision, recall, avg_precision)\n    \n    conf_mat = confusion_matrix(yvalid, valid_preds > 0.5)\n    plot_confusion(conf_mat)\n    \n    print(f\"Classification Report: \\n{classification_report(yvalid, valid_preds > 0.5, labels = [0, 1])}\")\n    \n    preds_xg.append(valid_preds)\n    print()","cbaa802d":"xgb.plot_importance(clf);","06135c5a":"finish = time()\nprint(strftime(\"%H:%M:%S\", gmtime(finish - start)))","e6f6463a":"# Data Cleaning\nBefore proceeding its better to remove the quotes from the strings in the columns","d4e42f4b":"__Check for Missing values__","d1e6b537":"- es_travel is where most fraudulent transactions happen\n- Which merchants were affected?","fde2bbfc":"- The average amount of transaction is higher in Fraud compared to No Fraud","be3971e8":"# UnderSampling Majority Class\n\n- The original dataset has positive\/negative sampels in the ratio 1:80\n-  We will undersample the majority so that we get a ratio of 1:30","eb6982e3":"- __Top fradulent transactions happen in es_health and es_sportandtoys category with total amount transacted 664,804 and 505,311 respectively.__\n- What is the amount transacted in each category?","437cddb9":"- __Recall has improved well after undersampling and also the Precision__\n- __Average Precision-Recall Score has also improved well__\n- We can further improve the performance by doing feature engineering (create more features)\n- Also, if only high Recall is desired, we can do so by setting the model parameter *scale_pos_weight* to just neg\/pos instead of sqrt(neg\/pos)","b0d6a670":"- There % of Fraudulent transactions in the dataset is 1.21% of the total transactions\n- Heavily imbalanced dataset","fa6b23de":"- Most of the transactions has happened in 'es_transportation' category\n- It would be interesting to find out in which category the fradulent transactions happens","72bf6e04":"- __Of the total fradulent transaction, 43% happens with merchants M480139044 and M980657600__\n\n- What category of items that customers buy with these merchants and how much amount transacted?","b4261b1b":"- The average amount spent during every 'step' is higher for fradulent transactions than normal ones\n- Lets check how gender and age influences the transactions","49748134":"- There is no Fraud transaction in 'es_transportation'\n- Let's remove the es_transportation category and check the distribution","f7616eb9":"## When to use PR AUC?\n\n#### When two classes are equally important\n\nAUC would be the metric to use if the goal of the model is to perform equally well on both classes. Image classification between cats & dogs is a good example because the performance on cats is equally important on dogs.\n\n#### When minority class is more important\n\nPR AUC would be the metric to use if the focus of the model is to identify correctly as many positive samples as possible.","cb9955d3":"The 'zipcodeOri' and 'zipMerchant' has only one unique item in them, we can drop these two features from the dataset","7649b879":"# Precision-Recall Curve\n\nPrecision-Recall is a useful measure of success of prediction when the classes are very imbalanced.\n\nThe precision-recall curve shows the tradeoff between precision and recall for different threshold. __A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate.__ High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).\n\nA system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the training labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels. An ideal system with high precision and high recall will return many results, with all results labeled correctly.\n\n[Ref](https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_precision_recall.html)","c820d0b9":"# Traning and Prediction","639ed5b3":"# EDA","60796fc6":"- From above plots its clear that across figures\/features, the amount transacted is higher in Fraudulent cases\n- The bank could take special interest in high value transactions and do a check before its approved (most banks do this now)","8421866d":"#### Cardinality is high for the features 'customer' and 'merchant', we use Frequency Encoding for this","00e328de":"# Data Pre-processing","8d8b06bc":"- __From the above plots we can get which merchants are affected by fraudulent transaction by looking at the total amount transacted, may be the bank should flag these merchants and monitor transactions closely__\n- __We can do the same analysis for customer feature but flagging customers would be a bad idea for PR__","b959dd53":"- __Our Xgboost model does a good job by keeping the False Negatives low (high Recall) which is preferred in Fraud\/Spam\/Churn detection setup__\n- __When using *'scale_pos_weight' = sqrt(neg\/pos)*, the precision has increased (lesser FP) but the recall has decresed a bit (higher FN) compared to using *'scale_pos_weight' = neg\/pos*__\n- From the feature importance plot, merchant and amount are the most important ones, which confirms what we observed from the plots in the EDA part\n    <font color = 'green'>\n    \n    - Flagging the merchants where high fraudulent transactions happens\n    - Checking whenever high amount in transacted\n    \n    <\/font>\n\n- We will attempt undersampling below, to check whether we can improve our model performance","86fce0bd":"__Who are the Merchants where most number of fradulent transaction happens?__"}}