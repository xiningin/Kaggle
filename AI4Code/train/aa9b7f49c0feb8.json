{"cell_type":{"24763d31":"code","35f11b97":"code","c4092dab":"code","cd578a4b":"code","917069f0":"code","18ed69e2":"code","6f5357e5":"code","579b0981":"code","7bfdd9dd":"code","0437164d":"code","d4d564f5":"code","191939cf":"code","6683093a":"code","cb24f930":"code","37991cdf":"code","315a8b5e":"code","b183c981":"code","c3ddea20":"code","de15db0a":"code","41557fe5":"code","656726a7":"code","abd00a6a":"code","15a2250c":"code","6a67c00c":"code","fe999676":"code","8704227f":"code","ae4cc4ef":"code","f973cb33":"code","4190311f":"code","b11b2918":"code","5aad29d5":"code","58496cfa":"code","cb71f627":"code","ec026fa4":"code","ecbd2029":"code","3b32343f":"markdown","7311b059":"markdown","bc1706e6":"markdown","475a2a96":"markdown","7ba8b157":"markdown","7126248d":"markdown","b8000bef":"markdown","428fa9de":"markdown","bac178da":"markdown","beb0fb0d":"markdown","3eb75adb":"markdown","03cafb5b":"markdown"},"source":{"24763d31":"# libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler","35f11b97":"df = pd.read_csv(\"..\/input\/weather-dataset-rattle-package\/weatherAUS.csv\")","c4092dab":"df.info()","cd578a4b":"# getting categorical features\ncFeatures = [columns_name for columns_name in df.columns if df[columns_name].dtype == \"O\"]\nprint(f\"the number of categorical features present in the dataset is {len(cFeatures)}  \")\nprint(f\"the categorical features present are as follows {cFeatures} \")","917069f0":"# getting numberical features\nnFeatures = [columns_name for columns_name in df.columns if df[columns_name].dtype != \"O\"]\nprint(f\"the number of numerical features present in the dataset is {len(nFeatures)}  \")\nprint(f\"the numerical features present are as follows {nFeatures} \")","18ed69e2":"# need to check cardinality of some of the features\n# this is something that will only really be an issue for categorical features, so we'll examine those\nfor category in cFeatures:\n    count = len(df[category].unique())\n    print(f\"the cardinality of the column {category} is {count} \")","6f5357e5":"# we can deal with the cardinality of date by making a column for day, month, and year\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"]) # to_datetime converts to type date\n# creating the new columns\ndf[\"year\"] = df[\"Date\"].dt.year\ndf[\"month\"] = df[\"Date\"].dt.month\ndf[\"day\"] = df[\"Date\"].dt.day\n# then need to drop the \"Date\" column\ndf.drop(\"Date\", axis = 1, inplace = True)\n# and then take a look at what the dataframe looks like now\ndf.info() ","579b0981":"# now we have three more numeric columns, and our highest cardinality for cat variables is Location at 49\n# updating out nFeatures from before\ncFeatures = [columns_name for columns_name in df.columns if df[columns_name].dtype == \"O\"]\nprint(f\"the number of categorical features present in the dataset: {len(cFeatures)}  \")\nprint(f\"the categorical features present are as follows {cFeatures} \")","7bfdd9dd":"# dealing with missing values\n# sum up how many NAs for each categorical variable\ndf[cFeatures].isnull().sum() # Total missing values  ","0437164d":"# want to fill in missing values, can use median or mode or mean. \n# choosing mode but can change later and see how it affects the accuracy rate \ncategorical_nulls = [features for features in cFeatures if df[features].isnull().sum()]  # list will return with categorical features\nfor variable in categorical_nulls:\n    df[variable] = df[variable].fillna(df[variable].mode()[0])     #each cat_feature is filled by most repeated value ","d4d564f5":"# now we check how many missing values are left, if any (there are none!)\ndf[cFeatures].isnull().sum()","191939cf":"# now we handle missing values for numerical variables\n# getting numerical features -- we have more now, so re-running the code from above\nnFeatures = [columns_name for columns_name in df.columns if df[columns_name].dtype != \"O\"]\nprint(f\"the number of numerical features present in the dataset: {len(nFeatures)}  \")\nprint(f\"the numerical features present are as follows {nFeatures} \")","6683093a":"num_null = df[nFeatures].isnull().sum() # checking how many null values\n# so we can see\nnum_null","cb24f930":"# we can see that not all of the variables have null values because of our date conversion\n# so getting just the variables that have null\/missing values\nonly_num_null = [column for column in nFeatures if df[column].isnull().sum()]\nmissing_num = df[only_num_null].isnull().sum()\n# now we have only those with missing values\nmissing_num","37991cdf":"# for numerical, don't want to immediately impute something like the mean or mode for missing values\n# need to first check for outliers\nplt.figure(figsize=(12,10))\ndf.boxplot(only_num_null,rot = 90)","315a8b5e":"# there are indeed outliers (can see from the boxplot), which we will handle using the IQR method\n# this is a drawback of filling in missing values because often outliers can tell us something important about the data\n# for example, we have a lot of outliers for rainfall,which we are trying to predict, which means we might\n# have trouble predicting rain for \"outlying\" events later on. \n# for loop to get rid of outliers\nfor feature in only_num_null:\n    q1 = df[feature].quantile(0.25)\n    q3 = df[feature].quantile(0.75)\n    IQR = q3-q1\n    lower_limit = q1 - (IQR*1.5)\n    upper_limit = q3 + (IQR*1.5)\n    df.loc[df[feature]<lower_limit, feature] = lower_limit\n    df.loc[df[feature]>upper_limit, feature] = upper_limit","b183c981":"# now, we'll look at the boxplot again\nplt.figure(figsize=(12,10))\ndf.boxplot(only_num_null,rot = 90) # we can see that there are no longer any outliers","c3ddea20":"# since we've gotten rid of outliers, we can now fill in missing cells with the mean \nfor features in only_num_null:\n    df[features] = df[features].fillna(df[features].mean())\n\n# making sure that worked:\ndf[only_num_null].isnull().sum()","de15db0a":"# first I\"ll take a look at the target variable\nsns.countplot(x = \"RainTomorrow\", data = df) \n# we can see that most days, it did not rain the next day\n# in some cases it might be necessary to balance the data, but in this case it reflects reality\n# (in australia, it is supposed to rain less days than it is not)","41557fe5":"# make histograms for all numerical variables at once\ndf_num = df.select_dtypes(include = ['float64'])\ndf_num.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8)","656726a7":"# we need to convert our categorical variables to something our model can understand\n\n# getting final round up of categorical feautures\ncategorical = [i for i in df.columns if df[i].dtype == \"O\"]\n\ncategorical","abd00a6a":"# now to perform encoding, we'll make a function\ndef encoding(feature):\n    mapping = {} \n    unique = list(df[feature].unique()) # getting unique qualitative values\n    for index in range(len(unique)): \n        mapping[unique[index]] = index # converting to a numerical representation\n    return mapping                     # of qualitative values\n\n# applying the function to our data\nfor category in categorical:\n    df[category].replace(encoding(category), inplace = True)\n    \nprint(df.head()) # and this finishes our encoding","15a2250c":"y = df[\"RainTomorrow\"]  # dependent or target \nX = df.drop([\"RainTomorrow\"],axis = 1)  # indepedent or input","6a67c00c":"# because our dataset is imbalanced (yes\/no imbalance, more days it doesn't rain) will use SMOTE to balance data\nsmote_object = SMOTE()\n# now creating x and y\nX, y = smote_object.fit_resample(X, y)","fe999676":"# split the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)","8704227f":"# model\nforest1 = RandomForestClassifier(n_estimators=200)\n\n# training the model\nforest1.fit(X_train, y_train)","ae4cc4ef":"# predictions \nforest1_predictions = forest1.predict(X_test)","f973cb33":"# check accuracy\nprint(\"Accuracy: \" + str(metrics.accuracy_score(y_test, forest1_predictions)*100) +\"%\")","4190311f":"feature_importance = pd.Series(forest1.feature_importances_,index=X.columns).sort_values(ascending=False)\nfeature_importance","b11b2918":"plt.rcParams[\"figure.figsize\"] = (10, 10)\nsns.barplot(x=feature_importance, y=feature_importance.index)\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","5aad29d5":"#RainToday, year, and WindDir9am will be dropped because they're under 0.02\nX = X.drop([\"RainToday\", \"year\", \"WindDir9am\"],axis = 1)","58496cfa":"# split the updated data into train and test\nX_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X, y, test_size = 0.3, random_state = 0)","cb71f627":"# trying a model with 100 trees first\nforest2 = RandomForestClassifier(n_estimators=200)\n\n# training the model\nforest2.fit(X_train_2, y_train_2)","ec026fa4":"# predictions \nforest2_predictions = forest2.predict(X_test_2)","ecbd2029":"# check accuracy\nprint(\"Accuracy: \" + str(metrics.accuracy_score(y_test_2, forest2_predictions)*100) +\"%\")\n# accuracy went down -- can conclude that model is better with all of its features, \n# even the one with the lowest amount of feature importance","3b32343f":"After addressing cardinality, it's also important to get any null values that may exist out of the way before moving on to data preparation. We'll begin by filling in missing values for categorical values.","7311b059":"# Data Preparation\n\nOur last step before building the model is data preparation. We split the dataset and then apply SMOTE to our data because of the imbalance of \"Yes\" and \"No\" values in our target variable RainTomorrow.\n\nSMOTE stands for Synthetic Minority Oversampling Technique. It's a statistical technique for increasing the number of cases in your data in a balanced way. This specific module works by generating new instances from existing minority cases that we supply as input. This module should be used for imbalanced datasets-- like ours!","bc1706e6":"**OBJECTIVE: to build a classification model that can predict the target variable \"RainTomorrow\" with a high degree of accuracy**\n\n**MODEL CHOSEN**: Random Forests\n\n*More About Random Forests*\n* Random Forests are a supervised ensemble learning algorithm consisting of many Decision Trees\n* The algorithm uses bagging and feature randomness when building each individual tree to try and create an uncorrelated forest of trees whose prediction by committee is more accurate than that of any individual trees.\n* In a classification problem, each tree votes and the most popular class is chosen as the final result.\n\n**DATASET**: The Dataset used is the Rain in Australia dataset on Kaggle. You can find it [here](https:\/\/www.kaggle.com\/jsphyg\/weather-dataset-rattle-package).\n* The target variable, RainTomorrow, is binary, with a value of \"Yes\" if the rain for that day was 1 mm or more, and \"No\" if it was not.\n* The rest of the data is a mixture of categorical and numerical values. For a detailed explanation of the rest of the variables, follow the link to the dataset.\n\nBrief written explanations and interpretations will be given throughout this report. Additional information can be found in the form of comments throughout the code documentation. \n\nThe report will begin by loading all required libraries and data.","475a2a96":"# Data Preprocessing\n\nFor the preprocessing stage of the report, we'll first examine which features are categorical and which are numerical.","7ba8b157":"# Random Forest Model With Feature Selection\n\nNext, we'll build a second Random Forest Model that only runs on the features deemed important once Feature Importance was analyzed. We can see that accuracy slightly decreases on this new Random Forest Model. Thus we can hesitantly conclude that while each feature has a differing level of helpfullness when it comes to determining whether it will rain tomorrow in Australia, it's still necessary to include ALL features in order to achieve the highest possible degree of accuracy.","7126248d":"# Random Forest Feature Importance\n\nWhat's nice about the Random Forest module from the sklearn package is the built in attribute \"feature importances\". We'll use this to investigate which features are most helpful in classification, and which are not. ","b8000bef":"Next, we'll need to address the null values for numerical variables. However, this is a bit more complicated than with categorical values-- before imputing a mean, median, or mode; we'll need to check for outliers. This is an important step because outliers can skew the mean\/median\/mode of our dataset, which means that after imputing our skewed statistical measure, our dataset becomes irrepresentative of the population it's supposed to be representative of. \n\nHowever, there are also drawbacks to getting rid of outliers. For this specific dataset, when thinking about weather, it's important to note that extreme events such as rainstorms and drought that might be helpful in predicting RainTomorrow for those specific time periods will be marked outlying events through our analysis. \n\nIn this specific situation, the benefit of removing outliers far outweighs the cost-- for a relatively small dataset such as this one, we can't afford to just delete missing cells from the dataset. However, a more sophisticated analysis of weather in Australia with more available data would stop here and address the missing values using different methodology (and, most likey, have a better accuracy rate as a result).\n\nIf you're interested in a more detailed discussion of outliers, check out this [article](https:\/\/medium.com\/@curryrowan\/a-closer-look-at-outliers-58f553ebab2d).","428fa9de":"It's also a good idea to check the cardinality of our categorical features. High values of cardinality can increase run time and make our model more difficult to interpret. For a more detailed explanation of the drawbacks of cardinality, check out this [article](https:\/\/towardsdatascience.com\/dealing-with-features-that-have-high-cardinality-1c9212d7ff1b).","bac178da":"As we can see, the cardinality for the column Date is a cause for concern. We'll deal with this by converting the column Date into three new numeric columns: year, day, and month.","beb0fb0d":"# Encoding\n\nFor our next step, we'll need to convert the categorical variables into a form that our Random Forest model will be able to understand. This is called encoding-- where we replace each categorical variable with numerical values. This will be done by creating a function that counts how many possible values each categorical variable has and then maps each qualitative value to it's corresponding unique index. The function then uses those numbers as the new 'qualitative' value.  ","3eb75adb":"# Random Forest Model\n\nFinally, we build our Random Forest Model. Here we have the luxury of using 200 trees, which is one of the reasons our accuracy rate has a relatively high degree of correctness. However, the Random Forest Model can be slow in generating predictions due to the sheer number of decision trees. This is because whenever a prediction is made, all the trees in the forest have to make a prediction for the same given input and then perform voting on it. \n\nFor this specific report, that's totally fine, our dataset is small. But for large datasets that need to be processed quickly, a different algorithm or a smaller number of trees would need to be used.","03cafb5b":"# Exploratory Data Analysis\n\nWe'll conduct some brief exploratory analysis before moving on. The most important thing to notice from these preliminary visualizations is the imbalance between \"Yes\" and \"No\" values for RainTomorrow. This will be important later on."}}