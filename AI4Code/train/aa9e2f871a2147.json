{"cell_type":{"7b5fb99a":"code","362533d9":"code","3113c5b7":"code","8d25a9b2":"code","3e74e049":"code","5c156ab8":"code","32e46493":"code","1713f04c":"code","a492f7ba":"code","1180ae10":"code","d6115d30":"code","59308743":"code","a6070c4b":"code","c7670a62":"code","1000c770":"code","3f517c29":"code","60f94a1c":"code","08bc4f9b":"code","75786079":"code","915d78d3":"code","1c12b8ee":"code","32892914":"code","6ace2d90":"code","31f48997":"code","9089f5be":"code","f2e3fde5":"code","363d6269":"code","a91f7cc9":"code","d86bfc19":"code","b1b59e74":"code","67f1af87":"code","9e45ebd0":"code","965457f0":"code","0cae359f":"code","c9483f9f":"code","6fb220f3":"code","1831fc52":"code","88846538":"code","3a423059":"code","ac62c227":"code","70ce1700":"code","a3736834":"code","c169f4ad":"code","049937b0":"code","a55285ed":"code","67280044":"code","184a11e8":"code","d7f4c4d1":"code","67967621":"code","da105e73":"code","25cb0f38":"code","2783bf63":"code","86459d4a":"markdown","aa4ed800":"markdown","a639a69a":"markdown","08406978":"markdown","9bf48d9f":"markdown","55eb6866":"markdown","b4cd6d53":"markdown","f0d1823d":"markdown","cfd3b5db":"markdown","9ea8b01c":"markdown","3f9a5812":"markdown","f3ff2265":"markdown","094acb9f":"markdown","6add795e":"markdown","a4f4d3c8":"markdown","01d3fc0e":"markdown","8f31c459":"markdown","e2a39170":"markdown","c3e5f3cc":"markdown","ca2da4c5":"markdown","23dbe285":"markdown","55f3dfe9":"markdown","724d03f7":"markdown","1c3590c0":"markdown","675e1490":"markdown","646ad9e5":"markdown","15df743b":"markdown","28c2ae3b":"markdown","b784b6b5":"markdown","3e2eb102":"markdown","9b71df1f":"markdown","ca1b82a8":"markdown","f309eff1":"markdown","e0014c4f":"markdown","29e6aeda":"markdown","ca0d38cb":"markdown","e7806351":"markdown","aaa946a9":"markdown","1fba66bd":"markdown"},"source":{"7b5fb99a":"#Data Manipulation \nimport numpy as np\nimport pandas as pd\n\n#Data Visualization \nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n%matplotlib inline","362533d9":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest =  pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","3113c5b7":"print(\"Train data shape \",train.shape)","8d25a9b2":"print(\"Test data shape \",test.shape)","3e74e049":"train.head()","5c156ab8":"plt.style.use(style='ggplot')\nplt.rcParams['figure.figsize'] = (10, 6)","32e46493":"train.SalePrice.describe()","1713f04c":"print(\"Skew is : \",train.SalePrice.skew())\nplt.hist(train.SalePrice,color='blue')\nplt.plot()","a492f7ba":"target=np.log(train.SalePrice)","1180ae10":"print(\"Skew is : \",target.skew())","d6115d30":"plt.hist(target,color='green')\nplt.plot()","59308743":"numeric_features = train.select_dtypes(include=[np.number])\nnumeric_features.dtypes","a6070c4b":"corr=numeric_features.corr()\nprint(corr['SalePrice'].sort_values(ascending=False)[:5])\nprint(corr['SalePrice'].sort_values(ascending=False)[-5:])","c7670a62":"train.OverallQual.unique()","1000c770":"quality_pivot=train.pivot_table(index='OverallQual',values='SalePrice',aggfunc=np.median)","3f517c29":"quality_pivot","60f94a1c":"quality_pivot.plot(kind='bar',color='lightgreen')\nplt.xlabel('Overall Quality')\nplt.ylabel('Median Sale Price')\nplt.xticks(rotation=0)\nplt.show()","08bc4f9b":"plt.scatter(train['GrLivArea'],target,color='green')","75786079":"plt.scatter(train['GarageArea'],target,color='green')","915d78d3":"train=train[train['GarageArea']<1200]","1c12b8ee":"plt.scatter(x=train['GarageArea'], y=np.log(train.SalePrice))\nplt.xlim(-200,1600) # This forces the same scale as before\nplt.ylabel('Sale Price')\nplt.xlabel('Garage Area')\nplt.show()","32892914":"sns.heatmap(train.isnull(),cmap='Purples')","6ace2d90":"nulls=pd.DataFrame(data=train.isnull().sum().sort_values(ascending=False)[:20])\n\nnulls","31f48997":"print(\"Unique Values : \",train.MiscFeature.unique())","9089f5be":"categoricals = train.select_dtypes(exclude=[np.number])\ncategoricals.describe()","f2e3fde5":"print (\"Original: \\n\")\nprint (train.Street.value_counts(), \"\\n\")","363d6269":"train['enc_street'] = pd.get_dummies(train.Street, drop_first=True)\ntest['enc_street']=pd.get_dummies(train.Street,drop_first=True)","a91f7cc9":"print(train.enc_street.value_counts(),'\\n')","d86bfc19":"train.SaleCondition.unique()","b1b59e74":"condition_pivot=train.pivot_table(index='SaleCondition',values='SalePrice',aggfunc=np.median)","67f1af87":"condition_pivot.plot(kind='bar',color='blue')\nplt.xlabel('Sale Condition')\nplt.ylabel('Median Sale Price')\nplt.show()","9e45ebd0":"def encode(val):\n    if val == 'Partial': \n        return 1 \n    else : \n        return 0\n    \ntrain['enc_condition']=train.SaleCondition.apply(encode)\ntest['enc_condition']=test.SaleCondition.apply(encode)    ","965457f0":"condition_pivot=train.pivot_table(index='enc_condition',values='SalePrice',aggfunc=np.median)","0cae359f":"condition_pivot.plot(kind='bar',color='green')\nplt.xlabel('Sale Condition')\nplt.ylabel('Median Sale Price')\nplt.show()","c9483f9f":"train.head()","6fb220f3":"data = train.select_dtypes(include=[np.number]).interpolate().dropna()","1831fc52":"sum(data.isnull().sum() != 0)","88846538":"y=np.log(train.SalePrice)\nX=data.drop(['SalePrice','Id'],axis=1)","3a423059":"from sklearn.model_selection import train_test_split","ac62c227":"X_train, X_test, y_train, y_test = train_test_split( X, y,test_size=.33)","70ce1700":"from sklearn import linear_model\nlr = linear_model.LinearRegression()","a3736834":"model = lr.fit(X_train, y_train)","c169f4ad":"print (\"R^2 is: \\n\", model.score(X_test, y_test))","049937b0":"predictions = model.predict(X_test)","a55285ed":"from sklearn.metrics import mean_squared_error\nprint ('RMSE is: \\n', mean_squared_error(y_test, predictions))","67280044":"actual_values = y_test\nplt.scatter(predictions, actual_values, alpha=.7,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Linear Regression Model')\nplt.show()","184a11e8":"submission = pd.DataFrame()\nsubmission['Id'] = test.Id","d7f4c4d1":"feats = test.select_dtypes(\n        include=[np.number]).drop(['Id'], axis=1).interpolate()","67967621":"predictions = model.predict(feats)","da105e73":"final_predictions = np.exp(predictions)","25cb0f38":"submission['SalePrice'] = final_predictions","2783bf63":"submission.to_csv('\/submission1.csv', index=False)","86459d4a":"Notice that there are many homes with 0 for Garage Area, indicating that they don\u2019t have a garage. We\u2019ll transform other features later to reflect this assumption. There are a few outliers as well. Outliers can affect a regression model by pulling our estimated regression line further away from the true population regression line. So, we\u2019ll remove those observations from our data. ","aa4ed800":"Wrangling the non-numeric Features\nLet\u2019s now consider the non-numeric features.","a639a69a":"The count column indicates the count of non-null observations, while unique counts the number of unique values. top is the most commonly occurring value, with the frequency of the top value shown by freq.\n\nFor many of these features, we might want to use one-hot encoding to make use of the information for modeling.\n\n### Transforming and engineering features\n\nWhen transforming features, it\u2019s important to remember that any transformations that you\u2019ve applied to the training data before fitting the model must be applied to the test data.\n\nOur model expects that the shape of the features from the train set match those from the test set. This means that any feature engineering that occurred while working on the train data should be applied again on the test set.\n\nTo demonstrate how this works, consider the Street data, which indicates whether there is Gravel or Paved road access to the property.","08406978":"Check if the all of the columns have 0 null values.","9bf48d9f":"Next, we\u2019ll consider rmse. To do so, use the model we have built to make predictions on the test data set.\n\n","55eb6866":"Notice that the median sales price strictly increases as Overall Quality increases.\n\nNext, let\u2019s use plt.scatter() to generate some scatter plots and visualize the relationship between the Ground Living Area GrLivArea and SalePrice.","b4cd6d53":"The documentation can help us understand the missing values. In the case of PoolQC, the column refers to Pool Quality. Pool quality is NaN when PoolArea is 0, or there is no pool.\nWe can find a similar relationship between many of the Garage-related columns.\n\nLet\u2019s take a look at one of the other columns, MiscFeature. We\u2019ll use the Series.unique() method to return a list of the unique values.","f0d1823d":"Lets Check Skewness in dataset ","cfd3b5db":"The first five features are the most positively correlated with SalePrice, while the next five are the most negatively correlated.\n\nLet\u2019s dig deeper on OverallQual. We can use the .unique() method to get the unique values.","9ea8b01c":"Now we\u2019ll transform the predictions to the correct form. Remember that to reverse log() we do exp().\nSo we will apply np.exp() to our predictions becasuse we have taken the logarithm previously.","3f9a5812":"#### Exploring Sales Data ","f3ff2265":"__Problem Statment__ \nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\n__Goal__\nIt is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable. \n\n__Metric__\nSubmissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)\n\n__File descriptions__\ntrain.csv - the training set\ntest.csv - the test set\ndata_description.txt - full description of each column, originally prepared by Dean De Cock but lightly edited to match the column names used here\nsample_submission.csv - a benchmark submission from a linear regression on year and month of sale, lot square footage, and number of bedrooms","094acb9f":"Next, we\u2019ll consider rmse. To do so, use the model we have built to make predictions on the test data set.\n\n","6add795e":"The DataFrame.corr() method displays the correlation (or relationship) between the columns. We\u2019ll examine the correlations between the features and the target.","a4f4d3c8":"Next, we need to fit the model. First instantiate the model and next fit the model. Model fitting is a procedure that varies for different types of models. Put simply, we are estimating the relationship between our predictors and the target variable so we can make accurate predictions on new data.\n\nWe fit the model using X_train and y_train, and we\u2019ll score with X_test and y_test. The lr.fit() method will fit the linear regression on the features and target variable that we pass.","01d3fc0e":"These values describe whether or not the house has a shed over 100 sqft, a second garage, and so on. We might want to use this information later","8f31c459":"We can use the documentation to find out what these values indicate:\n\nMiscFeature: Miscellaneous feature not covered in other categories<br>\n\n<ul>\n   <li>Elev Elevator<br>\n   <li>Gar2 2nd Garage (if not described in garage section)<br>\n   <li>Othr Other<br>\n   <li>Shed Shed (over 100 SF)<br>\n   <li>TenC Tennis Court<br>\n   <li> NA   None<br>","e2a39170":"#### Make a submission\n\nWe\u2019ll need to create a csv that contains the predicted SalePrice for each observation in the test.csv dataset.","c3e5f3cc":"At first glance, we see that increases in living area correspond to increases in price. We will do the same for GarageArea.","ca2da4c5":"__Test Data__","23dbe285":"We will use the train_test_split() function from scikit-learn to create a training set and a hold-out set.","55f3dfe9":"We can view this relationship graphically with a scatter plot.","724d03f7":"The OverallQual data are integer values in the interval 1 to 10 inclusive.\n\nWe can create a pivot table to further investigate the relationship between OverallQual and SalePrice. The Pandas docs demonstrate how to accomplish this task. We set index='OverallQual' and values='SalePrice'. We chose to look at the median here.","1c3590c0":"Now that we\u2019ve transformed the target variable, let\u2019s consider our features. First, we\u2019ll check out the numerical features and make some plots. The .select_dtypes() method will return a subset of columns matching the specified data types.","675e1490":"We will first create a __Linear Regression model__.","646ad9e5":"#### Evaluate the performance and visualize results\n\nEach competition might evaluate the submissions differently. In this competition, Kaggle will evaluate our submission using root-mean-squared-error (RMSE). We\u2019ll also look at The r-squared value. The r-squared value is a measure of how close the data are to the fitted regression line. It takes a value between 0 and 1, 1 meaning that all of the variance in the target is explained by the data. In general, a higher r-squared value means a better fit.\n\n","15df743b":"Notice that Partial has a significantly higher Median Sale Price than the others. We will encode this as a new feature. We select all of the houses where SaleCondition is equal to Patrial and assign the value 1, otherwise assign 0","28c2ae3b":"![](http:\/\/)### House Prices: Basic Linear Regression \n<a href='House Prices: Advanced Regression Techniques'> Kaggle Link <\/a>\n\nThis is my very first kaggle Competition , I have tried to keep the Notebook Simple and clear with all needed explanation.","b784b6b5":"Let\u2019s check out the size of the Data\n\n__Train Data__ ","3e2eb102":"### Build a linear model","9b71df1f":"In the Street column, the unique values are Pave and Grvl, which describe the type of road access to the property. In the training set, only 5 homes have gravel access. Our model needs numerical data, so we will use one-hot encoding to transform the data into a Boolean column.\n\nWe create a new column called enc_street. The pd.get_dummies() method will handle this for us.\n\nAs mentioned earlier, we need to do this on both the train and test data.","ca1b82a8":"__Read Train and Test Data__ ","f309eff1":"Note we have one less column in test data set as compared to train i.e Sales Price ","e0014c4f":"We will seprate features and Target column to train the model \n\nWe will assign the features to X and the target variable to y. We use np.log() to transform the y variable for the model.\n\nWe won\u2019t include SalePrice for obvious reasons, and Id is just an index with no relationship to SalePrice.","29e6aeda":"We\u2019ll log in to our Kaggle account and go to the submission page to make a submission.\nWe will use the DataFrame.to_csv() to create a csv to submit.\nThe first column must the contain the ID from the test data.","ca0d38cb":"Handling Null Values","e7806351":"__Import Necessary Libraries__","aaa946a9":"Now lets try using np.log to transform  train.SalesPrice calculate the skewness a second time, as well as re-plot the data. A value closer to 0 means that we have improved the skewness of the data. We can see visually that the data will more resembles a normal distribution.","1fba66bd":"Before we prepare the data for modeling, we need to deal with the missing data. We\u2019ll fill the missing values with an average value and then assign the results to data. This is a method of interpolation. The DataFrame.interpolate() method makes this simple.\n\nThis is a quick and simple method of dealing with missing values, and might not lead to the best performance of the model on new data. Handling missing values is an important part of the modeling process, where creativity and insight can make a big difference."}}