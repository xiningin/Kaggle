{"cell_type":{"9aa07e58":"code","58dca188":"code","672b6811":"code","a7e1968e":"code","3b8d282c":"code","c0131120":"code","cf052d27":"code","5668f4c1":"code","e17acf23":"markdown","d7c4c08f":"markdown","7f37ee28":"markdown","3b37e43b":"markdown","c50b3bfd":"markdown","c3976ef4":"markdown","f0dfdd87":"markdown","f6e9c3ce":"markdown","f4b15f87":"markdown","3a1c9b21":"markdown","b1464195":"markdown","65dc381d":"markdown","11983e03":"markdown","68a90773":"markdown"},"source":{"9aa07e58":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nprint(tf.__version__)","58dca188":"def func(x, y):\n    return - 5.5 * tf.exp(- 20.0 * (x - 0.3)**2 - 40.0 * (y - 0.3)**2) - 3.5 * tf.exp(- 15.0 * (x - 0.6)**2 - 10.0 * (y - 0.85)**2) - 2.0 * tf.sin(2.0 * (x - y))\n\n\nx = np.linspace(0, 1, 400)\nX, Y = np.meshgrid(x, x)\nZ = func(X, Y)\n\nplt.figure(figsize=(6, 4.7))\nplt.contourf(X, Y, Z, 60, cmap='RdGy')\nplt.xlabel('x', fontsize=19)\nplt.ylabel('y', fontsize=19)\nplt.tick_params(axis='both', which='major', labelsize=14)\ncbar = plt.colorbar()\ncbar.ax.tick_params(labelsize=14) ","672b6811":"def constr(a, b):\n    assert b > a\n    return lambda x: tf.clip_by_value(x, a, b)\n\n\nx = tf.Variable(0.0, trainable=True, dtype=tf.float64, name='x', constraint=constr(0, 1))\ny = tf.Variable(0.0, trainable=True, dtype=tf.float64, name='y', constraint=constr(0, 1))\n\n\ndef objective():\n    return - 5.5 * tf.exp(- 20.0 * (x - 0.3)**2 - 40.0 * (y - 0.3)**2) - 3.5 * tf.exp(- 15.0 * (x - 0.6)**2 - 10.0 * (y - 0.85)**2) - 2.0 * tf.sin(2.0 * (x - y))\n\n\ndef optimize(start, verbose=False, method='SGD'):\n    x.assign(start[0])\n    y.assign(start[1])\n\n    if method == 'SGD':\n        opt = tf.keras.optimizers.SGD(learning_rate=0.01)\n \n    if method == 'ADAM':\n        opt = tf.keras.optimizers.Adam(\n            learning_rate=0.1,\n            beta_1=0.9,\n            beta_2=0.999,\n            epsilon=1e-07,\n            amsgrad=False,\n        )\n\n    obj_vals = []\n    coords = []\n\n    for i in range(50):\n        if verbose and i % 5 == 0:\n            print(f'step: {i}, obj = {objective().numpy():.4f}, x = {x.numpy():.4f}, y = {y.numpy():.4f}')\n        obj_vals.append(objective().numpy())\n        coords.append((x.numpy(), y.numpy()))\n        opt.minimize(objective, var_list=[x, y])\n        \n    return obj_vals, coords","a7e1968e":"def plot_res(obj_vals, coords):\n    plt.figure(figsize=(16, 6))\n    plt.subplot(121)\n    plt.contourf(X, Y, Z, 60, cmap='RdGy')\n    plt.xlabel('x', fontsize=19)\n    plt.ylabel('y', fontsize=19)\n    plt.tick_params(axis='both', which='major', labelsize=14)\n    cbar = plt.colorbar()\n    cbar.ax.tick_params(labelsize=14) \n\n    xcoord = [x[0] for x in coords]\n    ycoord = [x[1] for x in coords]\n    plt.plot(xcoord, ycoord, '.-')\n    plt.plot(xcoord[-1], ycoord[-1], \"y*\", markersize=12)\n\n    plt.subplot(122)\n    plt.plot(obj_vals, '.-')\n    plt.plot([len(obj_vals) - 1], obj_vals[-1], \"y*\", markersize=12)\n    plt.xlabel('Step', fontsize=17)\n    plt.ylabel('Objective', fontsize=17)\n    plt.tick_params(axis='both', which='major', labelsize=14)\n    plt.show()","3b8d282c":"obj_vals, coords = optimize([0.25, 0.65], verbose=True, method='SGD')\nplot_res(obj_vals, coords)","c0131120":"obj_vals, coords = optimize([0.2, 0.65], method='SGD')  \nplot_res(obj_vals, coords)","cf052d27":"obj_vals, coords = optimize([0.2, 0.65], method='ADAM')  \nplot_res(obj_vals, coords)","5668f4c1":"x.assign(0.25)\ny.assign(0.65)\nopt = tf.keras.optimizers.SGD(learning_rate=0.01)\n\nfor i in range(30):\n    with tf.GradientTape() as tape:\n        z = func(x, y)\n    grads = tape.gradient(z, [x, y])\n    processed_grads = [g for g in grads]\n    grads_and_vars = zip(processed_grads, [x, y])\n    if i % 5 == 0:\n        print(f\"step {i}, z = {z.numpy():.2f}, x = {x.numpy():.2f}, y = {y.numpy():.2f},  grads0 = {grads[0].numpy():.2f}, grads1 = {grads[1].numpy():.2f}\")\n    opt.apply_gradients(grads_and_vars)\n","e17acf23":"In this article, we looked at the gradient descent method and its implementation with TensorFlow library. Advanced algorithms like \"ADAM\" generally work better than simple GD. And the result of the optimization depends on the starting point.","d7c4c08f":"Finding a global minimum of a given quantity is a common problem. When it is possible to acquire function gradients using automatic differentiation the gradient descent (GD) algorithm (https:\/\/en.wikipedia.org\/wiki\/Gradient_descent) could be a good solution. Here we will take a look at gradient-based algorithms and explore convergence. Many libraries provide automatic differentiation and GD algorithm implementation, we are going to use TensorFlow because it provides GPU support, works well with a big number of variables and large tensors.","7f37ee28":"# Conclusion.","3b37e43b":"An example above is an artificial real-valued function of two variables func(x, y) that has one global and several local minimums.","c50b3bfd":"Last Updated on February 6, 2020","c3976ef4":"# References.\n* https:\/\/en.wikipedia.org\/wiki\/Gradient_descent\n* https:\/\/arxiv.org\/abs\/1412.6980\n* https:\/\/www.tensorflow.org\/","f0dfdd87":"Here we can see that trajectory smoothly converges to the local minimum. Let's change the starting point.","f6e9c3ce":"Here it is! Now we can see that trajectory is converging smoothly down into the global minimum point.","f4b15f87":"In the code below, there is a function objective() that depends on two variables x, y which are constrained with constr(a, b) and take values from interval [0, 1]. A starting point \"start\" is chosen, then we create object opt = tf.keras.optimizers.SGD() and run opt.minimize() method several times. We explore 2 alghoritms, a simple gradient descent and \"ADAM\" alghoritm given by tf.keras.optimizers.SGD() and tf.keras.optimizers.Adam() respectively.","3a1c9b21":"It's descending to the global minimum now. However, we can see that having found the global minimum area the trajectory jumps inside it back and forth and seems nonconverging. This is a known problem with GD that can be on the one hand solved by carefully tunning learning_rate (trajectory update step). Another way to deal with that is to take into account previous values of the trajectory or so-called momentums (https:\/\/distill.pub\/2017\/momentum\/). An algorithm that uses first and second momentums is \"ADAM\" (https:\/\/arxiv.org\/abs\/1412.6980). This algorithm is more than just GD with momentums and more details can be found in the article. Let's try it out.","b1464195":"In case you need to process gradients by hand before and then apply them explicitly, the example of the code above does so.","65dc381d":"Let's first use simple GD alghoritm.","11983e03":"# Gradient descent optimization with TensorFlow 2.","68a90773":"# Applying gradients."}}