{"cell_type":{"ac9a76cf":"code","b705e403":"code","aa33dd9c":"code","52163678":"code","a6ed307e":"code","eabe0d4c":"code","e8711782":"code","63420e9f":"code","45a352dd":"code","ce289c92":"code","7c72d8d0":"code","904fe07c":"code","0525644e":"code","866fb7c6":"code","8e97a992":"code","6d893741":"code","fe23270a":"code","845eb378":"code","3ab7b362":"code","8d3272c5":"code","5ff80cd6":"code","89654b3a":"code","e67bbec2":"code","273455e9":"code","f7af7194":"code","8a235ce0":"code","94e89d38":"code","309948bb":"code","c53eee8f":"code","52364219":"markdown","2c82aa8b":"markdown","c3c38301":"markdown","edb2abdd":"markdown","44fd3462":"markdown","d99605c1":"markdown","23e02c50":"markdown","166e1365":"markdown","27703ee1":"markdown","eff06a9f":"markdown","19c826c0":"markdown"},"source":{"ac9a76cf":"import numpy as np\nimport pandas as pd \n\nimport matplotlib.pyplot as plt \nfrom tqdm import tqdm\nimport random\n\n\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator, load_img\nfrom keras.utils import to_categorical\n\nimport tensorflow as tf\n\nfrom sklearn.model_selection import train_test_split\n\nimport os","b705e403":"def read_to_pandas(path):\n    filename = []\n    category = []\n\n    folders = [x for x in os.listdir(path)]\n    folders.sort()\n\n    i = 0\n    for folder in folders:\n        \n        folder = os.path.join(path,folder)\n        files = os.listdir(folder)\n\n        for file in tqdm(files,desc=folder.split('\/')[-1]):\n            \n            file = os.path.join(folder, file)\n            filename += [file]\n            category += [str(i)]\n\n        i += 1\n    \n    df = pd.DataFrame({\n        'filename': filename,\n        'category': category\n    })\n    \n    return df\n        ","aa33dd9c":"def read_refer(path):\n    refer = {}\n    folders = [x for x in os.listdir(path)]\n    folders.sort()\n    i = 0\n    \n    for folder in folders:\n        refer.update({str(i):folder})\n        i +=1 \n        \n    return refer","52163678":"def show_img(df):\n    i = 0\n    plt.figure(figsize=(12,12))\n    for index, row in df.iterrows():\n        filename = row['filename']\n        category = row['category']\n\n        img = load_img(filename)\n        plt.subplot(5, 6, i + 1)\n        plt.imshow(img)\n        plt.xlabel('(' + \"{}. {}\".format(category,refer[category]) + ')' )\n\n        i += 1\n\n    plt.tight_layout()\n    plt.show()\n","a6ed307e":"monkey_path = \"..\/input\/10-monkey-species\/training\/training\"\nmonkey_valid_path = \"..\/input\/10-monkey-species\/validation\/validation\"\n\ncactus_path = \"..\/input\/cactus-aerial-photos\/training_set\/training_set\"\ncactus_valid_path = \"..\/input\/cactus-aerial-photos\/training_set\/training_set\"\n\ndf = read_to_pandas(monkey_path)\ntest_df = read_to_pandas(monkey_valid_path)\n\nrefer = read_refer(monkey_path)\n\ndf.sample(n=5)['category'].tolist()\nshow_img(df.sample(n=18))","eabe0d4c":"class_mode = 'categorical'\n\ndef split_data(df,test_size=0.2):\n    df = df.sample(frac=1)\n\n    train_df, validation_df = train_test_split(df, test_size=test_size, random_state=42)\n    train_df = train_df.reset_index(drop=True)\n    validation_df = validation_df.reset_index(drop=True)\n    \n    return train_df, validation_df\n\ndef train_datagen():\n    train_datagen = ImageDataGenerator(\n        rotation_range=10,\n        shear_range=0.05,\n        zoom_range=[0.85,1],\n        horizontal_flip=True,\n        validation_split=0.1,\n        rescale=1.\/255\n    )\n    return train_datagen\n    \ndef test_datagen():\n    test_datagen = ImageDataGenerator(rescale=1.\/255)\n    return test_datagen\n\ndef generator_from(datagen, data_df, IMAGE_SIZE, batch_size):\n    generator = datagen.flow_from_dataframe(\n        data_df, \n        \"\", \n        x_col='filename',\n        y_col='category',\n        target_size=IMAGE_SIZE,\n        class_mode=class_mode,\n        batch_size=batch_size\n    )\n    return generator","e8711782":"def generator_example(generator):\n    plt.figure(figsize=(12, 12))\n    for i in range(0, 6):\n        plt.subplot(5, 3, i+1)\n        plt.imshow(generator[0][0][0])\n        break\n    plt.tight_layout()\n    plt.show()\n    \n    print(generator[0][1][0])","63420e9f":"image_size = (128, 128)\nbatch_size = 64\nclass_mode = 'categorical'\n\ntrain_df,valid_df = split_data(df)\n#valid_df, test_df = split_data(valid_df)\ntrain_gen = generator_from(train_datagen(),train_df,image_size, batch_size)\nvalid_gen = generator_from(test_datagen(),valid_df,image_size, batch_size)\n\ngenerator_example(train_gen)","45a352dd":"from keras.optimizers import Adam \nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nlog_dir =''\ndef tensorboard_callback():\n    import datetime\n    log_dir=\".\/logs\/fit\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)#, histogram_freq=1)\n    return tensorboard_callback\n\ndef modelcheckpoint():\n    modelcheckpoint = ModelCheckpoint(filepath='.\/model.weights.best.hdf5',\n                                  monitor=\"val_acc\", \n                                  verbose = 1, \n                                  save_best_only=True)\n    return modelcheckpoint\n\ndef earlystop():\n    earlystop = EarlyStopping(patience=10)\n\ndef learning_rate_reduction():\n    learning_rate_reduction= ReduceLROnPlateau(monitor='val_acc', \n                                            patience=5, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\n    return learning_rate_reduction\n","ce289c92":"epochs=30\nloss='categorical_crossentropy'\noptimizer=Adam(0.001)\ncallbacks = [modelcheckpoint()]","7c72d8d0":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n\ndef model_1(input_shape,n_out):\n    model = Sequential()\n\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(rate=0.2))\n\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(rate=0.2))\n\n    model.add(Conv2D(128, (3, 3), activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(rate=0.2))\n\n    model.add(Flatten())\n    model.add(Dense(2048, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(rate=0.75))\n\n    model.add(Dense(n_out, activation='softmax'))\n\n    return model","904fe07c":"from keras.models import Sequential, load_model\nfrom keras.layers import Activation\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nfrom keras.layers import Dense\nfrom keras.layers import Input\nfrom keras.layers import BatchNormalization\nfrom keras.layers import Conv2D\nfrom keras.models import Model\nfrom keras.applications import InceptionResNetV2\n\ndef model_2(input_shape, n_out):\n        \n    path = 'C:\\\\vs2015shareddata\\\\model_weight\\\\inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5'\n    path = \"..\/input\/inceptionresnetv2\/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n    \n    pretrain_model = InceptionResNetV2(\n        include_top=False, \n        weights=path, \n        input_shape=input_shape)    \n    \n    pretrain_model.trainable = False\n    \n    input_tensor = Input(shape=input_shape)\n    bn = BatchNormalization()(input_tensor)\n    x = pretrain_model(bn)\n    x = Conv2D(128, kernel_size=(1,1), activation='relu')(x)\n    x = Flatten()(x)\n    x = Dropout(0.5)(x)\n    x = Dense(512, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    output = Dense(n_out, activation='softmax')(x)\n    model = Model(input_tensor, output)\n    \n    return model","0525644e":"def model_3(input_shape, n_out):  \n    # Pre-trained model with MobileNetV2# Pre-trained model with MobileNetV2\n    path = 'C:\\\\vs2015shareddata\\\\model_weight\\\\mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_96_no_top.h5'\n    path = 'imagenet'\n    \n    base_model = keras.applications.MobileNetV2(\n        input_shape=input_shape,\n        include_top=False,\n        weights=path\n    )\n    \n    base_model.trainable = False\n    \n    maxpool_layer = keras.layers.GlobalMaxPooling2D()\n    prediction_layer = keras.layers.Dense(n_out, activation='softmax')\n    \n    model = keras.models.Sequential([\n        base_model,\n        maxpool_layer,\n        prediction_layer\n    ])\n    \n    return model","866fb7c6":"def model_4(input_shape, n_out):  \n    \n    from keras.applications.resnet50 import ResNet50\n    path = ''\n    path = '..\/input\/resnet50\/resnet50_weights_tf_dim_ordering_tf_kernels.h5'\n    \n    resnet_model = ResNet50(\n        input_shape=input_shape,\n        include_top=False,\n        weights=path\n    )\n    \n    resnet_model.trainable = False\n    \n    dense1 = keras.layers.Dense(1024, activation='relu')\n    dropout = keras.layers.Dropout(rate = 0.5)\n    output = keras.layers.Dense(n_out, activation='softmax')\n    \n    model = keras.models.Sequential([\n        resnet_model,\n        dense1,\n        dropout,\n        output\n    ])\n    \n    return model","8e97a992":"model = model_2(train_gen[0][0][0].shape,len(refer))\nmodel.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])","6d893741":"model.summary()","fe23270a":" def train_1(epochs=epochs,initial_epoch = 0):\n    history = model.fit_generator(\n        train_datagen.flow(\n            train_df['filename'], train_df['category'], \n            batch_size=batch_size),\n        epochs = epochs, \n        initial_epoch = initial_epoch,\n        validation_data = (valid_df['filename'],valid_df['category']),\n        verbose = 1, \n        steps_per_epoch=x_train.shape[0] \/\/ batch_size\n    )\n    return history\n\ndef train_2(epochs=epochs,initial_epoch = 0):\n    history = model.fit_generator(\n        train_gen, \n        epochs=epochs,\n        initial_epoch = initial_epoch,\n        validation_data=valid_gen,\n        validation_steps=valid_df.shape[0]\/\/batch_size,\n        steps_per_epoch=train_df.shape[0]\/\/batch_size,\n        callbacks=callbacks\n    )\n    return history","845eb378":"history = train_2(epochs = 50)","3ab7b362":"model.layers[2].trainable = True\nmodel.compile(loss=loss, optimizer=Adam(lr = 0.0001), metrics=['accuracy'])\nft_history = train_2(epochs = 100,initial_epoch = 50)","8d3272c5":"def show_result(history):\n    plt.figure(figsize=(10, 4))\n    plt.subplot(1,2,1)\n    plt.plot(history.history['loss'], color='b', label=\"Training loss\")\n    plt.plot(history.history['val_loss'], color='r', label=\"validation loss\")\n    legend = plt.legend(loc='best', shadow=True)\n\n    plt.subplot(1,2,2)\n    plt.plot(history.history['acc'], color='b', label=\"Training accuracy\")\n    plt.plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\n    legend = plt.legend(loc='best', shadow=True)\n\n    plt.tight_layout()\n    plt.show()\n\n    #plt.xticks(np.arange(1, epochs, 1))\n    #plt.yticks(np.arange(0, 1, 0.1))\n","5ff80cd6":"model.save_weights(\".\/model.h5\")\nmodel.load_weights('.\/model.weights.best.hdf5')\n\ntest_gen = generator_from(test_datagen(),test_df,image_size, batch_size)\nscore = model.evaluate_generator(test_gen,steps = 100)\n\nprint('loss: {:.4f}'.format(score[0]),'acc: {:.4f}'.format(score[1]))\nshow_result(history)","89654b3a":"import numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix\n\ndef predict_df(test_df, test_gen):\n    predict = []\n    predict = model.predict_generator(test_gen, steps=np.ceil(test_df.shape[0]\/batch_size))\n    \n    #print(test_gen[0][0].shape)\n    #print(test_df.shape[0])\n    #predict = [[int(i <= 0.5),int(i > 0.5)] for i in predict]\n\n    test_df[[str(i) for i in range(len(refer))]] = pd.DataFrame(predict, index=test_df.index )\n\n    test_df['true'] = test_gen.classes\n    test_df['result'] = [np.argmax(i) for i in predict]\n    test_df['prob'] = [i[np.argmax(i)] for i in predict]\n    \n    return test_df\n\ndef show_predict(test_df):\n    sample_test = test_df.sample(n = 18)\n    sample_test = sample_test.reset_index(drop=True)\n\n    plt.figure(figsize=(12,12))\n    for index, row in sample_test.iterrows():\n        filename = row['filename']\n        result = row['result']\n        probability = row['prob']\n        img = load_img(filename)\n        plt.subplot(6, 3, index + 1)\n        plt.imshow(img)\n        plt.xlabel(filename.split('\/')[-2] + '(' + \"{}\".format(refer[str(result)]) + ' - ' + \"{:.2f}\".format(probability) + ')')\n    plt.tight_layout()\n    plt.show()\n    \ndef save_predict(test_df):\n    submission_df = test_df.copy()\n    submission_df['id'] = submission_df['filename'].str.split('\/').str[-1].str.split('.').str[0]\n    submission_df['prob'] = submission_df['prob'].astype(np.float16)\n    submission_df.drop(['filename'], axis=1, inplace=True)\n    submission_df.to_csv('submission.csv', index=False)","e67bbec2":"def get_matrix(df):\n    true_label = list(df['category'])\n    pred_label = list(df['result'])\n    output = np.array([[0 ] * len(refer)]* len(refer))\n    for i in range(len(true_label)):\n        a = int(true_label[i])\n        b = int(pred_label[i])\n        output[a][b] += 1\n    return output        \n\ndef show_matrix(cm,normalize=False):\n    classes = refer.values()\n    title=None\n    \n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Only use the labels that appear in the data\n    #classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax","273455e9":"result_df = predict_df(test_df,test_gen)\n\nshow_matrix(get_matrix(result_df))\n#show_predict(result_df)","f7af7194":"save_predict(result_df)\noutput_df = pd.read_csv('submission.csv')\noutput_df.sample(n=5)","8a235ce0":"def get_CAM(filename, predicted_label):\n    \"\"\"\n    This function is used to generate a heatmap for a sample image prediction.\n    \n    Args:\n        processed_image: any sample image that has been pre-processed using the \n                       `preprocess_input()`method of a keras model\n        predicted_label: label that has been predicted by the network for this image\n    \n    Returns:\n        heatmap: heatmap generated over the last convolution layer output \n    \"\"\"\n    processed_image = load_img(filename)\n    \n    # we want the activations for the predicted label\n    predicted_output = model.output[:, predicted_label]\n    \n    # choose the last conv layer in your model\n    last_conv_layer = model.get_layer('block5_conv3')\n    \n    # get the gradients wrt to the last conv layer\n    grads = K.gradients(predicted_output, last_conv_layer.output)[0]\n    \n    # take mean gradient per feature map\n    grads = K.mean(grads, axis=(0,1,2))\n    \n    # Define a function that generates the values for the output and gradients\n    evaluation_function = K.function([model.input], [grads, last_conv_layer.output[0]])\n    \n    # get the values\n    grads_values, conv_ouput_values = evaluation_function([processed_image])\n    \n    # iterate over each feature map in yout conv output and multiply\n    # the gradient values with the conv output values. This gives an \n    # indication of \"how important a feature is\"\n    for i in range(512): # we have 512 features in our last conv layer\n        conv_ouput_values[:,:,i] *= grads_values[i]\n    \n    # create a heatmap\n    heatmap = np.mean(conv_ouput_values, axis=-1)\n    \n    # remove negative values\n    heatmap = np.maximum(heatmap, 0)\n    \n    # normalize\n    heatmap \/= heatmap.max()\n    \n    return heatmap","94e89d38":"def show_random_sample(idx):\n    \"\"\"\n    This function is used to select a random sample from the validation dataframe.\n    It generates prediction for the same. It also stores the heatmap and the intermediate\n    layers activation maps.\n    \n    Arguments:\n        idx: random index to select a sample from validation data\n    \n    Returns:\n        activations: activation values for intermediate layers\n    \"\"\"\n    # select the sample and read the corresponding image and label\n    sample_image = cv2.imread(valid_df.iloc[idx]['image'])\n    sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n    sample_image = cv2.resize(sample_image, (img_rows, img_cols))\n    sample_label = valid_df.iloc[idx][\"label\"]\n    \n    # pre-process the image\n    sample_image_processed = np.expand_dims(sample_image, axis=0)\n    sample_image_processed = preprocess_input(sample_image_processed)\n    \n    # generate activation maps from the intermediate layers using the visualization model\n    activations = vis_model.predict(sample_image_processed)\n    \n    # get the label predicted by our original model\n    pred_label = np.argmax(model.predict(sample_image_processed), axis=-1)[0]\n    \n    # choose any random activation map from the activation maps \n    sample_activation = activations[0][0,:,:,32]\n    \n    # normalize the sample activation map\n    sample_activation-=sample_activation.mean()\n    sample_activation\/=sample_activation.std()\n    \n    # convert pixel values between 0-255\n    sample_activation *=255\n    sample_activation = np.clip(sample_activation, 0, 255).astype(np.uint8)\n    \n    \n    \n    # get the heatmap for class activation map(CAM)\n    heatmap = get_CAM(sample_image_processed, pred_label)\n    heatmap = cv2.resize(heatmap, (sample_image.shape[0], sample_image.shape[1]))\n    heatmap = heatmap *255\n    heatmap = np.clip(heatmap, 0, 255).astype(np.uint8)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    super_imposed_image = heatmap * 0.5 + sample_image\n    super_imposed_image = np.clip(super_imposed_image, 0,255).astype(np.uint8)\n\n    f,ax = plt.subplots(2,2, figsize=(15,8))\n    ax[0,0].imshow(sample_image)\n    ax[0,0].set_title(f\"True label: {sample_label} \\n Predicted label: {pred_label}\")\n    ax[0,0].axis('off')\n    \n    ax[0,1].imshow(sample_activation)\n    ax[0,1].set_title(\"Random feature map\")\n    ax[0,1].axis('off')\n    \n    ax[1,0].imshow(heatmap)\n    ax[1,0].set_title(\"Class Activation Map\")\n    ax[1,0].axis('off')\n    \n    ax[1,1].imshow(super_imposed_image)\n    ax[1,1].set_title(\"Activation map superimposed\")\n    ax[1,1].axis('off')\n    plt.show()\n    \n    return activations","309948bb":"get_CAM(valid_df['filename'][0],valid_df['category'][0])","c53eee8f":"def plot_conv_weights(weights, input_channel=0):\n    # Get the lowest and highest values for the weights.\n    # This is used to correct the colour intensity across\n    # the images so they can be compared with each other.\n    w_min = np.min(weights)\n    w_max = np.max(weights)\n\n    # Number of filters used in the conv. layer.\n    num_filters = weights.shape[3]\n\n    # Number of grids to plot.\n    # Rounded-up, square-root of the number of filters.\n    num_grids = math.ceil(math.sqrt(num_filters))\n    \n    # Create figure with a grid of sub-plots.\n    fig, axes = plt.subplots(8, 8)\n\n    # Plot all the filter-weights.\n    for i, ax in enumerate(axes.flat):\n        # Only plot the valid filter-weights.\n        if i<num_filters:\n            # Get the weights for the i'th filter of the input channel.\n            # See new_conv_layer() for details on the format\n            # of this 4-dim tensor.\n            img = weights[:, :, input_channel, i]\n\n            # Plot image.\n            ax.imshow(img, vmin=w_min, vmax=w_max,\n                      interpolation='nearest', cmap='seismic')\n        \n        # Remove ticks from the plot.\n        ax.set_xticks([])\n        ax.set_yticks([])\n    \n    # Ensure the plot is shown correctly with multiple plots\n    # in a single Notebook cell.\n    plt.show()","52364219":"# Hyperparameter Adjust\n## 0. Summary\nTo minimize what essential to look, I created this notebook for hyperparameter adjustment\n\nFunction are actually full code but minimized.\n* [1. Install dependencies](#1)\n* [2. Input Data](#2)\n* [3. Data Generator](#3)\n* [4. Setting](#4)\n* [5. Model](#5)\n* [6. Train](#6)\n* [7. Evaluate](#7)\n* [8. Prediction](#8)\n* [9. Visualize](#9)","2c82aa8b":"<a id='6'><\/a>\n## 6. Train","c3c38301":"<a id='5'><\/a>\n## 5. Model","edb2abdd":"<a id='3'><\/a>\n## 3. Data Generator","44fd3462":"<a id='4'><\/a>\n## 4. Setting ","d99605c1":"<a id = '9'><\/a>\n## 9. Visualize","23e02c50":"<a id='2'><\/a>\n## 2. Input Data","166e1365":"Fine-tune","27703ee1":"<a id='1'><\/a>\n## 1. Install dependencies","eff06a9f":"<a id='8'><\/a>\n## 8. Prediction","19c826c0":"<a id='7'><\/a>\n## 7. Evaluate"}}