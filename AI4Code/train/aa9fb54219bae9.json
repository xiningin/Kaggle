{"cell_type":{"96212a7e":"code","ed405e94":"code","0e78e65d":"code","6b214d29":"code","327a8fa2":"code","9757b9db":"code","eefe7554":"code","b9d198be":"markdown","bf4d8110":"markdown","44bbd897":"markdown"},"source":{"96212a7e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import make_classification\n%matplotlib inline","ed405e94":"# Generating and visualizing data \nx, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_informative=1,\n                             n_clusters_per_class=1, random_state=14)\n\nsns.scatterplot(x[:,0],x[:,1],hue=y.reshape(-1))\n\nx_test=x[:500]\ny_test=y[:500]\nx=x[500:]\ny=y[500:]","0e78e65d":"#Building the model\nclass logistic_regression:\n    def sigmoid(self,x): return 1\/(1+np.exp(-x))\n    def __init__(self,features=2,lambda_=0.1,regulaization=\"None\"):\n        self.features=features\n        self.weights=np.random.rand(features)\n        self.bias=0.\n        self.regulaization=regulaization\n        self.lambda_=lambda_\n    def compute_cost(self,y,y_hat):\n        term1=np.sum(np.multiply(y,np.log(1-y_hat)))\n        term2=np.sum(np.multiply(1-y,np.log(y_hat)))\n        reg=0.\n        if(self.regulaization==\"L1\"):\n            reg=self.lambda_*sum(np.abs(self.weights))\n        if(self.regulaization==\"L2\"):\n            reg=sum((self.weights**2))*self.lambda_\n        return -(term1+term2)\/len(y)-reg\n    def fit(self,x,y,print_every_nth_epoch=100,learning_rate=0.01,epochs=1000):\n        assert x.shape[1]==self.features,\"Invalid number of features.\"\n        assert x.shape[0]==y.shape[0],\"unequal number of sizes\"\n        n=x.shape[0]\n        for epoch in range(epochs):\n            y_hat=np.dot(x,self.weights)+self.bias\n            y_hat=self.sigmoid(y_hat)\n            diff=y-y_hat\n            grad_w=+np.dot(x.T,diff)*learning_rate\/n\n            if(self.regulaization==\"L1\"):\n                signs=np.where(self.weights>0,1,-1)\n                grad_w=grad_w+signs*self.lambda_\n            if(self.regulaization==\"L2\"):\n                grad_w=grad_w+self.lambda_*self.weights*2\n            self.weights-=grad_w\n            grad_b=+np.sum(diff)*learning_rate\/n\n            self.bias-=grad_b\n            if((epoch+1)%print_every_nth_epoch==0):\n                print(\"--------- epoch {} -------> loss={} ----------\".format((epoch+1),self.compute_cost(y,y_hat)))\n    def evaluate(self,x,y):\n        pred=self.predict(x)\n        pred=np.where(pred>0.5,1,0)\n        diff=np.abs(y-pred)\n\n        return(sum(diff)\/len(diff))\n    def predict(self,x):\n        return self.sigmoid(np.dot(x,self.weights)+self.bias)","6b214d29":"#To visualize the decision boundry\ndef visualize(model,title):\n    print(\"accuracy = {}\".format(model.evaluate(x_test,y_test)))\n    x1=np.arange(-5,6,0.2)\n    x2=np.arange(-5,4,0.2)\n    for i in range(len(x1)):\n        for j in range(len(x2)):\n            pred=model.predict([np.array(np.array([x1[i],x2[j]]))])[0]\n            if(pred>0.5):\n                plt.scatter(x1[i],x2[j],c=\"c\")\n            else:\n                plt.scatter(x1[i],x2[j],c=\"b\")\n    plt.title(title)\n    plt.show()","327a8fa2":"#Training and visualizing model\nmodel=logistic_regression(lambda_=0.01)\nmodel.fit(x,y,)\nvisualize(model,\"Vanilla Logistic Regression\")","9757b9db":"#Training and visualizing ridge model\nmodel1=logistic_regression(regulaization=\"L2\",lambda_=0.01)\nmodel1.fit(x,y,)\nvisualize(model1,\"Ridge Regression\")","eefe7554":"#Training and visualizing lasso model\nmodel2=logistic_regression(regulaization=\"L1\",lambda_=0.001)\nmodel2.fit(x,y,)\nvisualize(model2,\"Lasso Regression\")","b9d198be":"### Lasso Regression\n- Same as ridge regression, it penalizes weights\n- It adds sum(abs(weights))*lambda to the loss function\n- Unlike ridge regression, it can minimize slope to zero thus helping in feature regression","bf4d8110":"### Ridge Regression\n- It penalizes slope decreasing dependency on features\n- We add sum(weights^2)*lambda to the loss function where lambda is a hyperparameter\n- It trades bias for lower variance","44bbd897":"## Logistic Regression\n- Best Fit line represented by y=x1.w1+x2.w2+....+xn.wn\n- Prediction carried out by plugging points in best fit line equation and followed by sigmoid function, where sign of output determines classification\n- Fit of line calculated using log loss.\n- Parameters computed using gradient descent"}}