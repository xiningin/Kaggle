{"cell_type":{"c08476d3":"code","f1107319":"code","fa8f9e6d":"code","c2a04a03":"code","0ad64557":"code","1c4fa0b2":"code","8a63d6a7":"code","ab73d894":"code","bace4b76":"code","a2d0c0b9":"code","e4b3b379":"code","e04804a6":"code","e7347445":"code","68b009f2":"code","080c6964":"code","cb6e090a":"code","c3e0bc90":"code","7da927b9":"code","5c166b1a":"code","4eb078e9":"code","0eed82dc":"code","f4266299":"markdown","d9140823":"markdown","e01f7024":"markdown","25074075":"markdown","6106f6f7":"markdown","8d34cd3d":"markdown","a322feae":"markdown","9cbf2f32":"markdown","994e296b":"markdown","75a783a5":"markdown","c96b2b77":"markdown"},"source":{"c08476d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f1107319":"!pip install pyspark","fa8f9e6d":"import pandas as pd\nfrom pyspark import SparkContext\nfrom pyspark.sql import SparkSession, SQLContext\n\nsc= SparkContext(master= 'local', appName= 'Fake and real news')\nss= SparkSession(sc)","c2a04a03":"# Funtion for conver Pandas Dataframe to Spark Dataframe\nfrom pyspark.sql.types import StringType, StructField, StructType\ndef read_data(path):\n  schema= StructType(\n      [StructField('title',StringType(),True),\n      StructField('text',StringType(),True),\n      StructField('subject',StringType(),True),\n      StructField('date',StringType(),True)])\n  pd_df= pd.read_csv(path)\n  sp_df= ss.createDataFrame(pd_df, schema= schema)\n  return sp_df","0ad64557":"# Read data set\npath_true= '\/kaggle\/input\/fake-and-real-news-dataset\/True.csv'\npath_fake= '\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv'\ntrue_df= read_data(path_true)\nfake_df= read_data(path_fake)","1c4fa0b2":"# Number of news true\ntrue_df.count()","8a63d6a7":"# Number of news fake\nfake_df.count()","ab73d894":"# Concatenate 2 data sets into one and shuffle data set\nfrom pyspark.sql.functions import lit, rand\ndata= true_df.withColumn('fake', lit(0)).union(fake_df.withColumn('fake', lit(1))).orderBy(rand())","bace4b76":"# Check again\ndata.groupBy('fake').count().show()","a2d0c0b9":"# Check the values of the subject column\ndata.select('subject').distinct().show()","e4b3b379":"data.show(5)","e04804a6":"from pyspark.ml.feature import SQLTransformer, RegexTokenizer, StopWordsRemover, CountVectorizer, Imputer, IDF\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler\nStopWordsRemover.loadDefaultStopWords('english')\n\n# 0. Extract tokens from title\ntitle_tokenizer= RegexTokenizer(inputCol= 'title', outputCol= 'title_words',\n                                pattern= '\\\\W', toLowercase= True)\n# 1. Remove stop words from title\ntitle_sw_remover= StopWordsRemover(inputCol= 'title_words', outputCol= 'title_sw_removed')\n# 2. Compute Term frequency from title\ntitle_count_vectorizer= CountVectorizer(inputCol= 'title_sw_removed', outputCol= 'tf_title')\n# 3. Compute Term frequency-inverse document frequency from title\ntitle_tfidf= IDF(inputCol= 'tf_title', outputCol= 'tf_idf_title')\n# 4. Extract tokens from text\ntext_tokenizer= RegexTokenizer(inputCol= 'text', outputCol= 'text_words',\n                                pattern= '\\\\W', toLowercase= True)\n# 5. Remove stop words from text\ntext_sw_remover= StopWordsRemover(inputCol= 'text_words', outputCol= 'text_sw_removed')\n# 6. Compute Term frequency from text\ntext_count_vectorizer= CountVectorizer(inputCol= 'text_sw_removed', outputCol= 'tf_text')\n# 7. Compute Term frequency-inverse document frequency text\ntext_tfidf= IDF(inputCol= 'tf_text', outputCol= 'tf_idf_text')\n# 8. StringIndexer subject\nsubject_str_indexer= StringIndexer(inputCol= 'subject', outputCol= 'subject_idx')\n# 9. VectorAssembler\nvec_assembler= VectorAssembler(inputCols=['tf_idf_title', 'tf_idf_text', 'subject_idx'], outputCol= 'features')","e7347445":"from pyspark.ml.classification import RandomForestClassifier\n# 10 Random Forest Classifier\nrf= RandomForestClassifier(featuresCol= 'features', labelCol= 'fake', predictionCol= 'fake_predict', maxDepth= 7, numTrees= 20)","68b009f2":"from pyspark.ml import Pipeline\nrf_pipe= Pipeline(stages=[title_tokenizer, # 0\n                title_sw_remover, # 1\n                title_count_vectorizer, # 2\n                title_tfidf, # 3\n                text_tokenizer, # 4\n                text_sw_remover, # 5\n                text_count_vectorizer, # 6\n                text_tfidf, # 7\n                subject_str_indexer, # 8\n                vec_assembler, # 9\n                rf]) # 10 model","080c6964":"train, test= data.randomSplit([0.8, 0.2])","cb6e090a":"rf_model= rf_pipe.fit(train)","c3e0bc90":"# Function for evaluating classification model\nfrom pyspark.ml.evaluation import  MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n\naccuracy= MulticlassClassificationEvaluator(labelCol= 'fake', predictionCol= 'fake_predict', metricName= 'accuracy')\nf1= MulticlassClassificationEvaluator(labelCol= 'fake', predictionCol= 'fake_predict', metricName= 'f1')\nareaUnderROC= BinaryClassificationEvaluator(labelCol= 'fake', metricName= 'areaUnderROC')\n\ndef classification_evaluator(data_result):\n    data_result.crosstab(col1= 'fake_predict', col2= 'fake').show()\n    print('accuracy:' ,accuracy.evaluate(data_result))\n    print('f1:' ,f1.evaluate(data_result))\n    print('areaUnderROC:' ,areaUnderROC.evaluate(data_result))","7da927b9":"# Predict on training data set\nrf_train_result= rf_model.transform(train)","5c166b1a":"classification_evaluator(rf_train_result)","4eb078e9":"# Predict on test data set\nrf_test_result= rf_model.transform(test)","0eed82dc":"classification_evaluator(rf_test_result)","f4266299":"### 8.2 evaluation of final model fit on the test data set","d9140823":"## 7. Fitting model","e01f7024":"## 3. Create objects for processing data","25074075":"## 4. Create object for Random Forest Classifier model","6106f6f7":"## 2. Check the data set","8d34cd3d":"## 8. Evaluate classification model","a322feae":"# NLP for detect fake news using PySpark","9cbf2f32":"## 6. Splitting the dataset into the training set and test set","994e296b":"## 1. Read data set  \nReading data using the SparkSession.read.csv method causes structural errors for the data file. So read the data using pandas.read_csv method and convert to Spark Dataframe.\n","75a783a5":"### 8.1 Evaluation of final model fit on the training data set","c96b2b77":"## 5. Create Pipeline for processing and fitting data to model"}}