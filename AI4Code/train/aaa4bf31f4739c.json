{"cell_type":{"b2b82711":"code","b0cf8237":"code","93d2374d":"code","eed2740f":"code","67b112a2":"code","1e82f6a3":"code","c765ef1a":"code","8e3d3bed":"code","d0d9d61b":"code","7bb48ec4":"code","1029ff5a":"code","1058d123":"code","be5339ac":"code","cfc7db38":"code","19b8e13b":"code","256b144b":"code","b776c3e4":"code","8afdb863":"code","7740a466":"code","f512ca30":"code","f0b10df8":"code","1b0655e3":"code","d7fb3bd5":"code","2ffe8657":"code","a1f1c4e8":"code","4722f1a2":"code","fbb61603":"code","91ecb2d5":"code","3c80ccac":"code","12ec57b6":"code","a77623df":"code","2763fca4":"code","91f9e801":"code","2ef2faa4":"code","3d00c078":"code","3d8d20a0":"code","b33210da":"code","17b3d8c8":"code","fab8c502":"code","2acfa548":"code","002d2f7d":"code","ffee9717":"markdown","34e345d5":"markdown","f9d7ba55":"markdown"},"source":{"b2b82711":"gpu_info = !nvidia-smi\ngpu_info = '\\n'.join(gpu_info)\nif gpu_info.find('failed') >= 0:\n  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n  print('and then re-execute this cell.')\nelse:\n  print(gpu_info)","b0cf8237":"import collections\nimport os\nimport random\nimport re\nimport string\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport nltk.corpus # for stopwords\nimport numpy as np\nimport pandas as pd\n\n# visualization lib \nimport matplotlib.pyplot as plt \nfrom PIL import Image\nfrom plotly import graph_objs, express, figure_factory\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport seaborn as sns\n\n# ml\nimport spacy.util\nimport tensorflow as tf\nimport torch\nimport torch.nn as nn\nimport tokenizers\nimport transformers\nfrom sklearn.model_selection import StratifiedKFold\n\n%matplotlib inline","93d2374d":"print('TF version', tf.__version__)","eed2740f":"train = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv') ","67b112a2":"print(train.shape)\nprint(test.shape)","1e82f6a3":"train.info()","c765ef1a":"train.describe()","8e3d3bed":"train.dropna(inplace=True)","d0d9d61b":"train.info()","7bb48ec4":"test.info()","1029ff5a":"# selected_text\uac00 \ubaa8\ub450 text \ub370\uc774\ud130\uc758 sub \uc778\uc9c0 \ud655\uc778\nlen(train.apply(lambda x:x.selected_text in x.text, axis=1))","1058d123":"temp = train.groupby('sentiment').count()['text'].reset_index().sort_values(by='text', ascending=False)","be5339ac":"temp.style.background_gradient(cmap='Purples')","cfc7db38":"plt.figure(figsize=(12, 6))\nsns.countplot(x='sentiment', data=train)","19b8e13b":"fig = graph_objs.Figure(graph_objs.Funnelarea(\n                    text = temp.sentiment,\n                    values = temp.text,\n                    title = {\n                        \"position\": \"top center\",\n                        \"text\": \"Funnel-Chart of Sentiment Distribution\"\n                    }\n                ))\nfig.show()","256b144b":"# selected_text\uc640 text Jaccard Similiarity\ub85c \uc720\uc0ac\ub3c4 \ucd94\ucd9c\ud558\uae30\ndef jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n","b776c3e4":"\nresult_jaccard = []\n\nfor idx, row in train.iterrows():\n    sent1 = row.text\n    sent2 = row.selected_text\n    \n    jaccard_score = jaccard(sent1, sent2)\n    result_jaccard.append([sent1, sent2, jaccard_score])\n    \njaccard = pd.DataFrame(result_jaccard, columns=[\"text\", \"selected_text\", \"jaccard_score\"])\ntrain = train.merge(jaccard, how='outer')","8afdb863":"train['num_word_selected'] = train['selected_text'].apply(lambda x: len(str(x).split()))\ntrain['num_word_text'] = train['text'].apply(lambda x: len(str(x).split()))\ntrain['difference_in_words'] = train['num_word_text'] - train['num_word_selected']","7740a466":"train.head()","f512ca30":"# selected text\uc640 text\uc758 word \uac1c\uc218 \ube44\uad50\nhist_data = [train['num_word_selected'], train['num_word_text']]\n\nfig, axes = plt.subplots(figsize=(12, 6))\nsns.countplot(train['num_word_selected'], ax=axes, color='blue', alpha=0.3, label='selected_text')\nsns.countplot(train['num_word_text'], ax=axes, color='red', alpha=0.3, label='text')\naxes.legend()\nfig.show()","f0b10df8":"# positive, negative \uc758 selected_text\uc640 text\uc758 \ub2e8\uc5b4 \uac1c\uc218 \ucc28\uc774 \nplt.figure(figsize=(12, 6))\np1 = sns.kdeplot(\n        train[train['sentiment'] == 'positive']['difference_in_words'],\n        shade=True,\n        color='b',\n        label='positive').set_title('Kernel Distribution of Difference in Number of words')\np2 = sns.kdeplot(\n        train[train['sentiment'] == 'negative']['difference_in_words'],\n        shade=True,\n        color='r',\n        label='positive')","1b0655e3":"# neutral\uc758 selected_text\uc640 text\uc758 \ub2e8\uc5b4\uac1c\uc218 \ucc28\uc774 \nplt.figure(figsize=(12, 6))\nsns.distplot(train[train['sentiment'] == 'neutral']['difference_in_words'], kde=False)","d7fb3bd5":"# positive, negative\uc758 jaccard score \ucc28\uc774\nplt.figure(figsize=(12, 6))\np1 = sns.distplot(\n        train[train['sentiment'] == 'positive']['jaccard_score'],\n#         shade=True,\n        color='b',\n        label='positive'\n        ).set_title('KDE of Jaccard Scores across different Sentiments')\np2 = sns.distplot(\n        train[train['sentiment'] == 'negative']['jaccard_score'],\n#         shade=True,\n        color='r',\n        label='negative')\nplt.legend(labels=['positive', 'negative'])","2ffe8657":"plt.figure(figsize=(12, 6))\np1 = sns.kdeplot(\n        train[train['sentiment'] == 'positive']['jaccard_score'],\n        shade=True,\n        color='b',\n        label='positive'\n        ).set_title('KDE of Jaccard Scores across different Sentiments')\np2 = sns.kdeplot(\n        train[train['sentiment'] == 'negative']['jaccard_score'],\n        shade=True,\n        color='r',\n        label='negative')\nplt.legend(labels=['positive', 'negative'])","a1f1c4e8":"MAX_LEN = 96","4722f1a2":"MAX_LEN = 96\nPATH = '..\/input\/tf-roberta\/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n                vocab = PATH + 'vocab-roberta-base.json',\n                merges = PATH + 'merges-roberta-base.txt',\n                lowercase = True,\n                add_prefix_space = True\n            )\n","fbb61603":"unique_sentiment = train.sentiment.unique()\nprint(unique_sentiment)\nsentiment_id = collections.defaultdict(int)\nfor idx, sentiment in enumerate(unique_sentiment):\n    sentiment_id[sentiment] = idx\n","91ecb2d5":"shape0 = train.shape[0]\ninput_ids = np.ones((shape0, MAX_LEN), dtype='int32')\nattention_mask = np.zeros((shape0, MAX_LEN), dtype='int32')\ntoken_type_ids = np.zeros((shape0, MAX_LEN), dtype='int32')\nstart_tokens = np.zeros((shape0, MAX_LEN), dtype='int32')\nend_tokens = np.zeros((shape0, MAX_LEN), dtype='int32')\n\nfor k in range(shape0):\n    # text2\uc5d0\uc11c text1\uc758 \uc704\uce58\ub97c \ucc3e\uc544 chars\uc5d0 \ud574\ub2f9 \uc704\uce58\uc5d0 1\uc774\ub77c\uace0 mark\n    text1 = \" \" + \" \".join(train.loc[k, 'text'].split())\n    text2 = \" \".join(train.loc[k, 'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx: idx + len(text2)] = 1\n    if text1[idx - 1] == ' ':\n        chars[idx - 1] = 1\n    # text1 \ubb38\uc7a5 tokenizer encoding\ud558\uc5ec \uc800\uc7a5 \n    enc = tokenizer.encode(text1)\n#     print(enc)\n    # offset\uc5d0 enc\uc5d0 \uc800\uc7a5\ub418\uc5b4 \uc788\ub294 \uac01 \ub2e8\uc5b4 \uae38\uc774 \uc800\uc7a5 (\ub2e8\uc5b4 \ub9e8\ucc98\uc74c char\uc5d0 ' '\ud3ec\ud568\ub418\uc5b4 \uce74\uc6b4\ud305\ub428)\n    offsets = []\n    idx = 0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n#         print(w)\n        offsets.append((idx, idx + len(w)))\n        idx += len(w)\n        \n    # \ub2e8\uc5b4 \uc778\ub371\uc2f1\n    toks = []\n#     print(chars)\n#     print(text2)\n    for i, (a, b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i)\n#         print(chars[a:b], sm)\n#     print(toks)\n#     print('')\n#     print('')\n    s_tok = sentiment_id[train.loc[k, 'sentiment']]\n#     print('sentiment_id: ', s_tok)\n#     print('before input_ids: ', input_ids)\n    input_ids[k, :len(enc.ids) + 5] = [0] + enc.ids + [2, 2] + [s_tok] + [2]\n#     print('after input_ids: ', input_ids)\n#     print('before attention_mask: ', attention_mask)\n    attention_mask[k, :len(enc.ids)+5] = 1\n#     print('after attention_mask: ', attention_mask)\n    if len(toks) > 0:\n        start_tokens[k, toks[0]+1] = 1\n        end_tokens[k, toks[-1]+1] = 1\n        ","3c80ccac":"print(train['text'][1])\nprint(input_ids[1])\nprint(attention_mask[1])\nprint(sentiment_id[1])\nprint(start_tokens[1])\nprint(end_tokens[1])","12ec57b6":"enc = tokenizer(train['text'][1])\nenc\n# print(tokenizer.decode(enc['input_ids']))","a77623df":"def build_roberta():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    \n    config = transformers.RobertaConfig.from_pretrained(PATH + 'config-roberta-base.json')\n    bert_model = transformers.TFRobertaModel.from_pretrained(PATH + 'pretrained-roberta-base.h5', config=config)\n    \n    x = bert_model(ids, attention_mask=att, token_type_ids=tok)\n    \n    x1 = tf.keras.layers.Dropout(0.1)(x[0])\n    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0])\n    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n    \n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1, x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n    \n    return model\n\n\n    \n    ","2763fca4":"model = build_roberta()\nmodel.summary()","91f9e801":"VER = 'v0'\nDISPLAY=1\nval_start = np.zeros((input_ids.shape[0], MAX_LEN))\nval_end = np.zeros((input_ids.shape[0], MAX_LEN))\n\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=777)\nbest_model = 0\nhistory = []\nfor fold, (idx_train, idx_val) in enumerate(skf.split(input_ids, train.sentiment.values)):\n    print(fold, (idx_train, idx_val))\n    tf.keras.backend.clear_session()\n    model = build_roberta()\n    sv = tf.keras.callbacks.ModelCheckpoint(\n            '\/kaggle\/working\/%s-roberta-%i.h5'%(VER,fold),\n            monitor='val_loss',\n            verbose=1,\n            save_best_only=True,\n            save_weight_only=True,\n            mode='auto',\n            save_freq='epoch'\n            )\n    \n    history.append(\n        model.fit([input_ids[idx_train,], attention_mask[idx_train,], token_type_ids[idx_train, ]], [start_tokens[idx_train,], end_tokens[idx_train,]],\n                  epochs=3,\n                  batch_size=32,\n                  verbose=1,\n                  callbacks=[sv],\n                  validation_data=([input_ids[idx_val,], attention_mask[idx_val,], token_type_ids[idx_val,]], [start_tokens[idx_val,], end_tokens[idx_val,]])))\n      \n    model.load_weights('\/kaggle\/working\/%s-roberta-%i.h5'%(VER,fold))\n    val_start[idx_val,], val_end[idx_val,] = model.predict(\n                                [input_ids[idx_val,],attention_mask[idx_val,],token_type_ids[idx_val,]],verbose=1)","2ef2faa4":"plt.plot(history[0].history['loss'])\nplt.plot(history[0].history['val_loss'])\nplt.title('Model Loss')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","3d00c078":"from transformers import BartModel, BartConfig\n# Initializing a BART facebook\/bart-large style configuration\nconfiguration = BartConfig()\n# Initializing a model from the facebook\/bart-large style configuration\nmodel = BartModel(configuration)\n# Accessing the model configuration\nconfiguration = model.config","3d8d20a0":"from transformers import BartTokenizer, BartModel\nimport torch\ntokenizer = BartTokenizer.from_pretrained('facebook\/bart-large')\nmodel = BartModel.from_pretrained('facebook\/bart-large')\ninputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\noutputs = model(**inputs)\nlast_hidden_states = outputs.last_hidden_state\n\ntokenizer = BartTokenizer.from_pretrained('facebook\/bart-large')\nmodel = BartModel.from_pretrained('facebook\/bart-large')","b33210da":"from transformers import BartTokenizer, BartModel, AdamW\nimport torch.nn as nn\n\n\nclass BartQA(nn.Module):\n    def __init__(self, bart, config):\n        super(BartQA, self).__init__()\n        self.bart = bart\n        self.qa = nn.Linear(config.hidden_size, 2)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input_ids, attention_mask):\n        output = self.bart(input_ids, attention_mask=attention_mask)\n        qa_output = self.qa(output[0])\n        print('output[0].shape: ', output[0].shape)\n        print('qa_output.shape: ', qa_output.shape)\n        start, end = qa_output.split(1, dim=-1)\n        print('start.shape, end.shape: ', start.shape, end.shape)\n        start_logits = start.squeeze(-1)\n        end_logit = end.squeeze(-1)\n        print('start_logits.shape, end_logit.shape: ', start_logits.shape, end_logit.shape)\n        return start_logits, end_logit\n\n\n\nbartqa = BartQA(model, model.config)\nbartqa.to('cuda')\n\n","17b3d8c8":"from torch.utils.data import  TensorDataset, DataLoader","fab8c502":"input_ids_tensor = torch.tensor(input_ids, dtype=torch.long)\nattention_mask_tensor = torch.tensor(attention_mask, dtype=torch.float32)\nstart_tokens_tensor = torch.tensor(start_tokens, dtype=torch.long)\nend_tokens_tensor = torch.tensor(end_tokens, dtype=torch.long)","2acfa548":"train_data = TensorDataset(input_ids_tensor, attention_mask_tensor, start_tokens_tensor, end_tokens_tensor)\ntrain_dataloader = DataLoader(train_data, shuffle=True, batch_size=2)","002d2f7d":"device = 'cuda'\n\nmodel.train()\ntotal_loss = 0\ntotal_preds = []\nfor step, batch in enumerate(train_dataloader):\n    # progress update after every 50 batches.\n    if step % 50 == 0 and not step == 0:\n        print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n\n    batch = [r.to(device) for r in batch]\n    sent_id, mask, start, end = batch\n    print(len(sent_id), len(mask), len(start), len(end))\n    model.zero_grad()\n\n    preds = bartqa(sent_id, mask)\n    print(preds[0].shape, start.shape)\n    print(preds[1].shape, end.shape)\n    loss_fct = nn.CrossEntropyLoss(ignore_index=1)\n    start_loss = loss_fct(preds[0], start)\n    end_loss = loss_fct(preds[1], end)\n    loss = (start_loss + end_loss) \/ 2\n    total_losss = total_loss + loss.item()\n\n    loss.backward()\n\n    nn.utils.clip_grad_norm(model.paramters(), 1.0)\n\n    optimizer.step()\n\n    preds = preds.detach().cpu().numpy()\n\n    total_preds.append(preds)\n\n    avg_loss = total_loss \/ len(train_dataloader)\n\n    total_preds = np.concatenate(total_preds, axis=0)\n\n    print(avg_loss, total_preds)","ffee9717":"### **BART**","34e345d5":"## Modeling  \n- roBerta\n- Bart","f9d7ba55":"### roBERTa (tensorflow)\n"}}