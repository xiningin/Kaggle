{"cell_type":{"f2986e2a":"code","9338af7c":"code","45edfe16":"code","e65d90db":"code","0444d48b":"code","a398004d":"code","666763ab":"code","99891123":"code","8dd25e09":"code","f679febb":"code","93db6d14":"code","a1ef34af":"code","6f1d8aba":"code","7af46a68":"code","00327374":"code","01259c51":"code","eec1a854":"code","b3a0e6cc":"code","ebca1358":"code","5419534e":"code","bcfec52c":"code","f8ea9ad0":"code","02c2dc05":"code","82a2c465":"code","b4c18cef":"code","6c852672":"code","6c39eadd":"markdown","1d70ff3c":"markdown","9686a6fa":"markdown","8ab1f250":"markdown","1e2c8b7c":"markdown"},"source":{"f2986e2a":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom gensim.models import Word2Vec\nfrom sklearn.model_selection import KFold\nimport numpy as np\nfrom tqdm import tqdm\nimport os,logging,pickle,random\nfrom sklearn import metrics as skmetrics\nimport warnings\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# from torch import nn as nn\n# from torch.nn import functional as F\n# import torch,time,os\nwarnings.filterwarnings(\"ignore\")","9338af7c":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.constraints import max_norm\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Activation\nfrom tensorflow.keras.layers import Conv1D, Add, MaxPooling1D, BatchNormalization\nfrom tensorflow.keras.layers import Embedding, Bidirectional, GlobalMaxPooling1D\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM","45edfe16":"import tensorflow as tf\ngpus = tf.config.list_physical_devices(device_type='GPU')\ncpus = tf.config.list_physical_devices(device_type='CPU')\nprint(gpus, cpus)","e65d90db":"data = pd.read_csv(\"..\/input\/protein-dataset\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/protein-dataset\/test_withid.csv\")","0444d48b":"data[\"sequence\"] = data[\"xulie\"].apply(lambda x : x.upper())\ndf_test[\"sequence\"] = df_test[\"xulie\"].apply(lambda x : x.upper())\ndata[\"seqLen\"] = data[\"xulie\"].apply(lambda x : len(x))\ndf_test[\"seqLen\"] = df_test[\"xulie\"].apply(lambda x : len(x))","a398004d":"df_test.head()","666763ab":"def integer_encoding(data):\n    \"\"\"\n    - Encodes code sequence to integer values.\n    - 20 common amino acids are taken into consideration\n    and rest 4 are categorized as 0.\n    \"\"\"\n\n    encode_list = []\n    for row in data['sequence'].values:\n        row_encode = []\n        for code in row:\n            row_encode.append(char_dict.get(code, 0))\n        encode_list.append(np.array(row_encode))\n\n    return encode_list\nitemCounter = {}\nfor seq in data[\"sequence\"]:\n    for i in seq:\n        itemCounter[i] = itemCounter.get(i,0)+1\ncodes = list(itemCounter.keys())\ncodes.sort()\ncodes","99891123":"def create_dict(codes):\n    char_dict = {}\n    for index, val in enumerate(codes):\n        char_dict[val] = index+1\n\n    return char_dict\n\nchar_dict = create_dict(codes)\n\nprint(char_dict)\nprint(\"Dict Length:\", len(char_dict))","8dd25e09":"max_length = 300\ndata_encode = integer_encoding(data)\ndata_pad = pad_sequences(data_encode, maxlen=max_length, padding='post', truncating='post')\ndata_ohe = to_categorical(data_pad, 23)","f679febb":"data_ohe.shape","93db6d14":"def plot_seq_count(df, data_name):\n    sns.distplot(df['seqLen'].values)\n    plt.title(f'Sequence char count: {data_name}')\n    plt.grid(True)\nplt.subplot(1, 2, 1)\nplot_seq_count(data, 'Train')\n\nplt.subplot(1, 2, 2)\nplot_seq_count(df_test, 'Test')\n\nplt.subplots_adjust(right=3.0)\nplt.show()","a1ef34af":"# \u53cc\u5411LSTM\ndef BiLSTM():  \n    x_input = Input(shape=(max_length,))\n    emb = Embedding(23, 128, input_length=max_length)(x_input)\n    bi_rnn = Bidirectional(CuDNNLSTM(64, kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01)))(emb)\n    x = Dropout(0.3)(bi_rnn)\n\n    # softmax \n    x_output = Dense(245, activation='softmax')(x)\n\n    model1 = Model(inputs=x_input, outputs=x_output)\n    return model1\n    \n\n#model1.summary()","6f1d8aba":"def residual_block(data, filters, d_rate):\n    \"\"\"\n    _data: input\n    _filters: convolution filters\n    _d_rate: dilation rate\n    \"\"\"\n\n    shortcut = data\n\n    bn1 = BatchNormalization()(data)\n    act1 = Activation('relu')(bn1)\n    conv1 = Conv1D(filters, 1, dilation_rate=d_rate, padding='same', kernel_regularizer=l2(0.001))(act1)\n\n    #bottleneck convolution\n    bn2 = BatchNormalization()(conv1)\n    act2 = Activation('relu')(bn2)\n    conv2 = Conv1D(filters, 3, padding='same', kernel_regularizer=l2(0.001))(act2)\n\n    #skip connection\n    x = Add()([conv2, shortcut])\n\n    return x\n\n# model\n\ndef ProtCNN():    \n    #Input(shape=(100, 21))\n    x_input = Input(shape=(max_length, 23))\n\n    #initial conv\n    conv = Conv1D(128, 1, padding='same')(x_input) \n\n    # per-residue representation\n    res1 = residual_block(conv, 128, 2)\n    res2 = residual_block(res1, 128, 3)\n\n    x = MaxPooling1D(3)(res2)\n    x = Dropout(0.5)(x)\n\n    # softmax classifier\n    x = Flatten()(x)\n    x_output = Dense(245, activation='softmax', kernel_regularizer=l2(0.0001))(x)\n\n    model2 = Model(inputs=x_input, outputs=x_output)\n    return model2\n#odel2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n#odel2.summary()","7af46a68":"load_model(self.saved_model, custom_objects={\"f1_score \": f1_score })","00327374":"from keras.models import load_model\ndef generate_submission(model_name):\n    model = load_model(f'.\/model_{model_name}\/model_{model_name}.h5', custom_objects={\"f1\": f1 })\n    test_encode = integer_encoding(df_test)\n    test_pad = pad_sequences(test_encode, maxlen=max_length, padding='post', truncating='post')\n    test_ohe = to_categorical(test_pad, 23)\n    if \"LSTM\" in model_name:\n        y_pred = model.predict(test_pad,batch_size = 256)\n    else:\n        y_pred = model.predict(test_ohe,batch_size = 256)\n    a = np.argmax(y_pred, axis=1)\n    final_pred = le.inverse_transform(a.reshape(-1, 1))\n    ans = []\n    for i in list(final_pred):\n        ans.append(i.strip()[0] + \".\" + i.strip()[1:])\n    df_test[\"category_id\"] = ans\n    output = df_test[[\"sample_id\", \"category_id\"]]\n    output.to_csv(f\"submission_{model_name}.csv\", index = None)","01259c51":"le = LabelEncoder()\nle.fit(data[\"label\"])\ny = le.transform(data[\"label\"])\ny = to_categorical(y)\n#y[trainIdList].shape","eec1a854":"import os\ndef create_dir_not_exist(path):\n    if not os.path.exists(path):\n        os.mkdir(path)\ncreate_dir_not_exist(\".\/model_BiLSTM\")\ncreate_dir_not_exist(\".\/model_ProtCNN\")","b3a0e6cc":"from tensorflow.keras import backend as K\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","ebca1358":"model_name = \"BiLSTM\"\ntest_encode = integer_encoding(df_test)\ntest_pad = pad_sequences(test_encode, maxlen=max_length, padding='post', truncating='post')\ntest_ohe = to_categorical(test_pad, 23)\nif \"LSTM\" in model_name:\n    y_pred = model.predict(test_pad,batch_size = 256)\nelse:\n    y_pred = model.predict(test_ohe,batch_size = 256)\na = np.argmax(y_pred, axis=1)\nfinal_pred = le.inverse_transform(a.reshape(-1, 1))\nans = []\nfor i in list(final_pred):\n    ans.append(i.strip()[0] + \".\" + i.strip()[1:])\ndf_test[\"category_id\"] = ans\noutput = df_test[[\"sample_id\", \"category_id\"]]\noutput.to_csv(f\"submission_{model_name}.csv\", index = None)","5419534e":"kf = KFold(n_splits=30)\nhistories = []\nle = LabelEncoder()\ny = le.fit_transform(data[\"label\"])\ny = to_categorical(y)\nmax_length = 300 # \u6307\u5b9a\u5e8f\u5217\u957f\u5ea6\nmodel = BiLSTM()\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', f1])\nfor fold_id, (trainIdList, validIdList) in enumerate(kf.split(data)):\n    print(f\"==============Fold{fold_id}==================\")\n    print(\"TRAIN:\", len(trainIdList), \"TEST:\", len(validIdList))\n    \n    df_train = data.iloc[trainIdList]\n    df_val = data.iloc[validIdList]\n    df_train['seq_char_count']= df_train['sequence'].apply(lambda x: len(x))\n    df_val['seq_char_count']= df_val['sequence'].apply(lambda x: len(x))\n    \n    train_encode = integer_encoding(df_train)\n    val_encode = integer_encoding(df_val) \n\n    train_pad = pad_sequences(train_encode, maxlen=max_length, padding='post', truncating='post')\n    val_pad = pad_sequences(val_encode, maxlen=max_length, padding='post', truncating='post')\n    \n    train_ohe = to_categorical(train_pad, 23)\n    val_ohe = to_categorical(val_pad, 23)\n    \n    y_train = y[trainIdList]\n    y_val = y[validIdList]\n\n    es = EarlyStopping(monitor='val_loss', patience=50, verbose=1)\n    history1 = model.fit(\n        train_pad, y_train,\n        epochs=300, batch_size=256,\n        validation_data=(val_pad, y_val),\n        callbacks=[es]\n        )\n    histories.append(history1)\nmodel.save(f'.\/model_BiLSTM\/model_BiLSTM.h5')\ngenerate_submission(\"BiLSTM\")","bcfec52c":"kf = KFold(n_splits=30)\nhistories = []\nle = LabelEncoder()\ny = le.fit_transform(data[\"label\"])\ny = to_categorical(y)\nmax_length = 300 # \u6307\u5b9a\u5e8f\u5217\u957f\u5ea6\nmodel = ProtCNN()\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', f1])\nfor fold_id, (trainIdList, validIdList) in enumerate(kf.split(data)):\n    print(f\"==============Fold{fold_id}==================\")\n    print(\"TRAIN:\", len(trainIdList), \"TEST:\", len(validIdList))\n    \n    \n    df_train = data.iloc[trainIdList]\n    df_val = data.iloc[validIdList]\n    df_train['seq_char_count']= df_train['sequence'].apply(lambda x: len(x))\n    df_val['seq_char_count']= df_val['sequence'].apply(lambda x: len(x))\n    \n    train_encode = integer_encoding(df_train)\n    val_encode = integer_encoding(df_val) \n\n    train_pad = pad_sequences(train_encode, maxlen=max_length, padding='post', truncating='post')\n    val_pad = pad_sequences(val_encode, maxlen=max_length, padding='post', truncating='post')\n    \n    train_ohe = to_categorical(train_pad, 23)\n    val_ohe = to_categorical(val_pad, 23)\n    \n    y_train = y[trainIdList]\n    y_val = y[validIdList]\n\n    es = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n    history1 = model.fit(\n        train_ohe, y_train,\n        epochs=20, batch_size=256,\n        validation_data=(val_ohe, y_val),\n        callbacks=[es]\n        )\n    histories.append(history1)\nmodel.save(f'.\/model_ProtCNN\/model_ProtCNN.h5')\ngenerate_submission(\"ProtCNN\")","f8ea9ad0":"# model = load_model(f'.\/model_BiLSTM\/model_ProtCNN.h5')\n# test_encode = integer_encoding(df_test)\n# test_pad = pad_sequences(test_encode, maxlen=max_length, padding='post', truncating='post')\n# test_ohe = to_categorical(test_pad, 23)\n# y_pred = model.predict(test_ohe,batch_size = 256)","02c2dc05":"# a = np.argmax(y_pred, axis=1)\n# final_pred = le.inverse_transform(a.reshape(-1, 1))","82a2c465":"# ans = []\n# for i in list(final_pred):\n#     ans.append(i.strip()[0] + \".\" + i.strip()[1:])\n# df_test[\"category_id\"] = ans","b4c18cef":"# output = df_test[[\"sample_id\", \"category_id\"]]\n# output.to_csv(\"submission_pro.csv\", index = None)","6c852672":"# def residual_block(data, filters, d_rate):\n#     \"\"\"\n#     _data: input\n#     _filters: convolution filters\n#     _d_rate: dilation rate\n#     \"\"\"\n\n#     shortcut = data\n\n#     bn1 = BatchNormalization()(data)\n#     act1 = Activation('relu')(bn1)\n#     conv1 = Conv1D(filters, 1, dilation_rate=d_rate, padding='same', kernel_regularizer=l2(0.001))(act1)\n\n#     #bottleneck convolution\n#     bn2 = BatchNormalization()(conv1)\n#     act2 = Activation('relu')(bn2)\n#     conv2 = Conv1D(filters, 3, padding='same', kernel_regularizer=l2(0.001))(act2)\n\n#     #skip connection\n#     x = Add()([conv2, shortcut])\n\n#     return x\n\n# # model\n\n# def ProtCNN():    \n#     #Input(shape=(100, 21))\n#     x_input = Input(shape=(max_length, 23))\n\n#     #initial conv\n#     conv = Conv1D(128, 1, padding='same')(x_input) \n\n#     # per-residue representation\n#     res1 = residual_block(conv, 128, 2)\n#     res2 = residual_block(res1, 128, 3)\n\n#     x = MaxPooling1D(3)(res2)\n#     x = Dropout(0.5)(x)\n\n#     # softmax classifier\n#     x = Flatten()(x)\n#     x_output = Dense(245, activation='softmax', kernel_regularizer=l2(0.0001))(x)\n\n#     model2 = Model(inputs=x_input, outputs=x_output)\n#     return model2\n# #odel2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# #odel2.summary()","6c39eadd":"## \u6a21\u578b2 ProtCNN","1d70ff3c":"## \u6a21\u578b2 ProtCNN","9686a6fa":"## \u6a21\u578b1 \u53cc\u5411LSTM","8ab1f250":"## \u6a21\u578b1\u8bad\u7ec3\uff08\u672a\u8fdb\u884c\u6a21\u578b\u4fdd\u5b58\u548c\u6d4b\u8bd5\u96c6\u63a8\u65ad\uff09","1e2c8b7c":"## \u5e8f\u5217\u957f\u5ea6\u5206\u5e03"}}