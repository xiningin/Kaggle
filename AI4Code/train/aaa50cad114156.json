{"cell_type":{"8706c6c3":"code","0f0763a2":"code","c3985859":"code","81f088db":"code","01bb5920":"code","183cafa2":"code","e4bd098d":"code","0ddf5d33":"code","224508d1":"code","b4ebe1b2":"code","c5369905":"code","0719a75c":"code","b7510891":"code","a6a751ea":"code","c77e7771":"code","9149d9a6":"code","043b734b":"code","96a1dac7":"code","b5a19a49":"code","f0f897f5":"code","47c91f92":"code","a9ce78bf":"code","d85e8611":"code","a4b7e058":"code","454e0cd9":"code","48a28b97":"code","b326b924":"code","e37b1be8":"markdown","f7963409":"markdown","d9dc56dc":"markdown","2661682b":"markdown","e439b0ca":"markdown","d810fc10":"markdown","2bc257f4":"markdown","97697a85":"markdown","126af751":"markdown","2813bc98":"markdown","ad1efc8d":"markdown","147545da":"markdown","8f5ce7a9":"markdown","d4ad6cec":"markdown","0ec01731":"markdown","c808a86e":"markdown","d8a759a8":"markdown","eff7fdb3":"markdown","239c6bc2":"markdown","b041ecbf":"markdown","a3604475":"markdown","c48e922d":"markdown","0d1342a6":"markdown","0c7f2247":"markdown","ea6a2a92":"markdown"},"source":{"8706c6c3":"import numpy as np\n\nimport pandas as pd\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom pandas.io.json import json_normalize\n\nimport json\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('fivethirtyeight')\n\nfrom sklearn.preprocessing import Imputer\n\nfrom sklearn import preprocessing\n\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor","0f0763a2":"train = pd.read_csv('..\/input\/train.csv',dtype={'fullVisitorId': 'str'})\ntest = pd.read_csv('..\/input\/test.csv',dtype={'fullVisitorId': 'str'})","c3985859":"print(train.head())","81f088db":"train_device = pd.DataFrame(list(train.device.apply(json.loads)))\ntrain_geoNetwork = pd.DataFrame(list(train.geoNetwork.apply(json.loads)))\ntrain_totals = pd.DataFrame(list(train.totals.apply(json.loads)))\ntrain_trafficSource = pd.DataFrame(list(train.trafficSource.apply(json.loads)))\ntrain_trafficSource_adwordsClickInfo = pd.DataFrame(list(train_trafficSource['adwordsClickInfo'].apply(json.dumps).apply(json.loads)))\n\ntest_device = pd.DataFrame(list(test.device.apply(json.loads)))\ntest_geoNetwork = pd.DataFrame(list(test.geoNetwork.apply(json.loads)))\ntest_totals = pd.DataFrame(list(test.totals.apply(json.loads)))\ntest_trafficSource = pd.DataFrame(list(test.trafficSource.apply(json.loads)))\ntest_trafficSource_adwordsClickInfo = pd.DataFrame(list(test_trafficSource['adwordsClickInfo'].apply(json.dumps).apply(json.loads)))","01bb5920":"train = pd.concat(\n    [\n        train[\n            [\n                'channelGrouping',\n                'date',\n                'fullVisitorId',\n                'sessionId',\n                'socialEngagementType',\n                'visitId',\n                'visitNumber',\n                'visitStartTime'\n            ]\n        ],\n        train_device,\n        train_geoNetwork,\n        train_totals,\n        train_trafficSource,\n        train_trafficSource_adwordsClickInfo\n    ],\n    axis = 1\n)\n\n# columns with no data or are JSON columns which have been flattened\ntrain = train.drop(\n    [\n        'socialEngagementType',\n        'browserSize',\n        'browserVersion',\n        'flashVersion',\n        'mobileDeviceBranding',\n        'mobileDeviceInfo',\n        'mobileDeviceMarketingName',\n        'mobileDeviceModel',\n        'mobileInputSelector',\n        'operatingSystemVersion',\n        'screenColors',\n        'screenResolution',\n        'cityId',\n        'latitude',\n        'longitude',\n        'networkLocation',\n        'adNetworkType',\n        'criteriaParameters',\n        'gclId',\n        'isVideoAd',\n        'page',\n        'slot',\n        'targetingCriteria',\n        'adwordsClickInfo'\n    ],\n    axis = 1\n)\n\ntest = pd.concat(\n    [\n        test[[\n                'channelGrouping',\n                'date',\n                'fullVisitorId',\n                'sessionId',\n                'socialEngagementType',\n                'visitId',\n                'visitNumber',\n                'visitStartTime'\n        ]],\n        test_device,\n        test_geoNetwork,\n        test_totals,\n        test_trafficSource,\n        test_trafficSource_adwordsClickInfo\n    ], \n    axis = 1\n)\n\n# columns with no data or are JSON columns which have been flattened\ntest = test.drop(\n    [\n        'socialEngagementType',\n        'browserSize',\n        'browserVersion',\n        'flashVersion',\n        'mobileDeviceBranding',\n        'mobileDeviceInfo',\n        'mobileDeviceMarketingName',\n        'mobileDeviceModel',\n        'mobileInputSelector',\n        'operatingSystemVersion',\n        'screenColors',\n        'screenResolution',\n        'cityId',\n        'latitude',\n        'longitude',\n        'networkLocation',\n        'adNetworkType',\n        'criteriaParameters',\n        'gclId',\n        'isVideoAd',\n        'page',\n        'slot',\n        'targetingCriteria',\n        'adwordsClickInfo'\n    ],\n    axis = 1\n)\n\ndel train_device\ndel train_geoNetwork\ndel train_totals\ndel train_trafficSource\ndel train_trafficSource_adwordsClickInfo\ndel test_device\ndel test_geoNetwork\ndel test_totals\ndel test_trafficSource\ndel test_trafficSource_adwordsClickInfo","183cafa2":"train['visitStartTime'].astype(str).astype(int)\ntrain['visitId'].astype(str).astype(int)\ntrain['transactionRevenue'].fillna(value = '0', inplace = True)\ntrain['transactionRevenue'] = train['transactionRevenue'].astype(int)\ntrain['visitNumber'].astype(str).astype(int)\ntrain['hits'] = train['hits'].astype(int)\ntrain['pageviews'].fillna(value = '0', inplace = True)\ntrain['pageviews'] = train['pageviews'].astype(int)\n\ntest['visitStartTime'].astype(str).astype(int)\ntest['visitId'].astype(str).astype(int)\ntest['visitNumber'].astype(str).astype(int)\ntest['hits'] = test['hits'].astype(int)\ntest['pageviews'].fillna(value = '0', inplace = True)\ntest['pageviews'] = test['pageviews'].astype(int)","e4bd098d":"unique_vals = train.nunique().sort_values(ascending = False)\nunique_vals = unique_vals.to_frame()\nunique_vals = unique_vals.reset_index()\nunique_vals.columns = ['column', 'cnt']\n\ndtypes = train.dtypes.to_frame()\ndtypes = dtypes.reset_index()\ndtypes.columns = ['column', 'type']\n\nprofile = pd.merge(\n    unique_vals,\n    dtypes,\n    on = 'column',\n    how = 'inner'\n)\n\nprint(profile.loc[profile['type'] == 'object'])","0ddf5d33":"networkDomain_encoder =  preprocessing.LabelEncoder()\nkeyword_encoder =  preprocessing.LabelEncoder()\nreferralPath_encoder =  preprocessing.LabelEncoder()\ncity_encoder =  preprocessing.LabelEncoder()\nvisitNumber_encoder =  preprocessing.LabelEncoder()\nsource_encoder =  preprocessing.LabelEncoder()\nregion_encoder =  preprocessing.LabelEncoder()\ndate_encoder =  preprocessing.LabelEncoder()\ncountry_encoder =  preprocessing.LabelEncoder()\nmetro_encoder =  preprocessing.LabelEncoder()\nbrowser_encoder =  preprocessing.LabelEncoder()\nadContent_encoder =  preprocessing.LabelEncoder()\nsubContinent_encoder =  preprocessing.LabelEncoder()\noperatingSystem_encoder =  preprocessing.LabelEncoder()\ncampaign_encoder =  preprocessing.LabelEncoder()\n\nnetworkDomain_encoder.fit(train['networkDomain'])\ntrain['keyword'].fillna(value = '0', inplace = True)\nkeyword_encoder.fit(train['keyword'])\ntrain['referralPath'].fillna(value = '0', inplace = True)\nreferralPath_encoder.fit(train['referralPath'])\ncity_encoder.fit(train['city'])\nvisitNumber_encoder.fit(train['visitNumber'])\nsource_encoder.fit(train['source'])\nregion_encoder.fit(train['region'])\ndate_encoder.fit(train['date'])\ncountry_encoder.fit(train['country'])\nmetro_encoder.fit(train['metro'])\nbrowser_encoder.fit(train['browser'])\n\ntrain['adContent'].fillna(value = '0', inplace = True)\nadContent_encoder.fit(train['adContent'])\nsubContinent_encoder.fit(train['subContinent'])\noperatingSystem_encoder.fit(train['operatingSystem'])\ncampaign_encoder.fit(train['campaign'])\n\ntrain['networkDomain_encoder'] = networkDomain_encoder.transform(train['networkDomain'])\ntrain['keyword_encoder'] = keyword_encoder.transform(train['keyword'])\ntrain['referralPath_encoder'] = referralPath_encoder.transform(train['referralPath'])\ntrain['city_encoder'] = city_encoder.transform(train['city'])\ntrain['visitNumber_encoder'] = visitNumber_encoder.transform(train['visitNumber'])\ntrain['source_encoder'] = source_encoder.transform(train['source'])\ntrain['region_encoder'] = region_encoder.transform(train['region'])\ntrain['date_encoder'] = date_encoder.transform(train['date'])\ntrain['country_encoder'] = country_encoder.transform(train['country'])\ntrain['metro_encoder'] = metro_encoder.transform(train['metro'])\ntrain['browser_encoder'] = browser_encoder.transform(train['browser'])\ntrain['adContent_encoder'] = adContent_encoder.transform(train['adContent'])\ntrain['subContinent_encoder'] = subContinent_encoder.transform(train['subContinent'])\ntrain['operatingSystem_encoder'] = operatingSystem_encoder.transform(train['operatingSystem'])\ntrain['campaign_encoder'] = campaign_encoder.transform(train['campaign'])\n\ntest_networkDomain_encoder =  preprocessing.LabelEncoder()\ntest_keyword_encoder =  preprocessing.LabelEncoder()\ntest_referralPath_encoder =  preprocessing.LabelEncoder()\ntest_city_encoder =  preprocessing.LabelEncoder()\ntest_visitNumber_encoder =  preprocessing.LabelEncoder()\ntest_source_encoder =  preprocessing.LabelEncoder()\ntest_region_encoder =  preprocessing.LabelEncoder()\ntest_date_encoder =  preprocessing.LabelEncoder()\ntest_country_encoder =  preprocessing.LabelEncoder()\ntest_metro_encoder =  preprocessing.LabelEncoder()\ntest_browser_encoder =  preprocessing.LabelEncoder()\ntest_adContent_encoder =  preprocessing.LabelEncoder()\ntest_subContinent_encoder =  preprocessing.LabelEncoder()\ntest_operatingSystem_encoder =  preprocessing.LabelEncoder()\ntest_campaign_encoder =  preprocessing.LabelEncoder()\n\ntest['keyword'].fillna(value = '0', inplace = True)\ntest_keyword_encoder.fit(test['keyword'])\n\ntest['referralPath'].fillna(value = '0', inplace = True)\ntest_referralPath_encoder.fit(test['referralPath'])\ntest_city_encoder.fit(test['city'])\ntest_visitNumber_encoder.fit(test['visitNumber'])\ntest_source_encoder.fit(test['source'])\ntest_region_encoder.fit(test['region'])\ntest_date_encoder.fit(test['date'])\ntest_country_encoder.fit(test['country'])\ntest_metro_encoder.fit(test['metro'])\ntest_browser_encoder.fit(test['browser'])\ntest_networkDomain_encoder.fit(test['networkDomain'])\ntest['adContent'].fillna(value = '0', inplace = True)\ntest_adContent_encoder.fit(test['adContent'])\ntest_subContinent_encoder.fit(test['subContinent'])\ntest_operatingSystem_encoder.fit(test['operatingSystem'])\ntest_campaign_encoder.fit(test['campaign'])\n\ntest['networkDomain_encoder'] = test_networkDomain_encoder.transform(test['networkDomain'])\ntest['keyword_encoder'] = test_keyword_encoder.transform(test['keyword'])\ntest['referralPath_encoder'] = test_referralPath_encoder.transform(test['referralPath'])\ntest['city_encoder'] = test_city_encoder.transform(test['city'])\ntest['visitNumber_encoder'] = test_visitNumber_encoder.transform(test['visitNumber'])\ntest['source_encoder'] = test_source_encoder.transform(test['source'])\ntest['region_encoder'] = test_region_encoder.transform(test['region'])\ntest['date_encoder'] = test_date_encoder.transform(test['date'])\ntest['country_encoder'] = test_country_encoder.transform(test['country'])\ntest['metro_encoder'] = test_metro_encoder.transform(test['metro'])\ntest['browser_encoder'] = test_browser_encoder.transform(test['browser'])\ntest['adContent_encoder'] = test_adContent_encoder.transform(test['adContent'])\ntest['subContinent_encoder'] = test_subContinent_encoder.transform(test['subContinent'])\ntest['operatingSystem_encoder'] = test_operatingSystem_encoder.transform(test['operatingSystem'])\ntest['campaign_encoder'] = test_campaign_encoder.transform(test['campaign'])","224508d1":"train_one_hot = train[\n    [\n        'channelGrouping',\n        'deviceCategory',\n        'isMobile',\n        'language',\n        'continent',\n        'medium',\n        'newVisits',\n        'visits',\n        'campaignCode',\n        'isTrueDirect',\n        'bounces'\n    ]\n]\n\ntrain_one_hot = pd.get_dummies(train_one_hot)\n\ntrain = pd.concat(\n    [\n        train,\n        train_one_hot\n    ],\n    axis = 1\n)\n\ntest_one_hot = test[\n    [\n        'channelGrouping',\n        'deviceCategory',\n        'isMobile',\n        'language',\n        'continent',\n        'medium',\n        'newVisits',\n        'visits',\n        'isTrueDirect',\n        'bounces'\n    ]\n]\n\ntest_one_hot = pd.get_dummies(test_one_hot)\n\ntest = pd.concat(\n    [\n        test,\n        test_one_hot\n    ],\n    axis = 1\n)\n\ndel train_one_hot\ndel test_one_hot","b4ebe1b2":"train['date'] = pd.to_datetime(train['date'], format = '%Y%m%d')\ntrain['month'] = pd.DatetimeIndex(train['date']).month\ntrain['year'] = pd.DatetimeIndex(train['date']).year\ntrain['day'] = pd.DatetimeIndex(train['date']).day\ntrain['quarter'] = pd.DatetimeIndex(train['date']).quarter\ntrain['weekday'] = pd.DatetimeIndex(train['date']).weekday\ntrain['weekofyear'] = pd.DatetimeIndex(train['date']).weekofyear\ntrain['is_month_start'] = pd.DatetimeIndex(train['date']).is_month_start\ntrain['is_month_end'] = pd.DatetimeIndex(train['date']).is_month_end\ntrain['is_quarter_start'] = pd.DatetimeIndex(train['date']).is_quarter_start\ntrain['is_quarter_end'] = pd.DatetimeIndex(train['date']).is_quarter_end\ntrain['is_year_start'] = pd.DatetimeIndex(train['date']).is_year_start\ntrain['is_year_end'] = pd.DatetimeIndex(train['date']).is_year_end\nprint(train[['month','day','year','quarter','weekday','weekofyear','date']].head())\n\ntest['date'] = pd.to_datetime(test['date'], format = '%Y%m%d')\ntest['month'] = pd.DatetimeIndex(test['date']).month\ntest['year'] = pd.DatetimeIndex(test['date']).year\ntest['day'] = pd.DatetimeIndex(test['date']).day\ntest['quarter'] = pd.DatetimeIndex(test['date']).quarter\ntest['weekday'] = pd.DatetimeIndex(test['date']).weekday\ntest['weekofyear'] = pd.DatetimeIndex(test['date']).weekofyear\ntest['is_month_start'] = pd.DatetimeIndex(test['date']).is_month_start\ntest['is_month_end'] = pd.DatetimeIndex(test['date']).is_month_end\ntest['is_quarter_start'] = pd.DatetimeIndex(test['date']).is_quarter_start\ntest['is_quarter_end'] = pd.DatetimeIndex(test['date']).is_quarter_end\ntest['is_year_start'] = pd.DatetimeIndex(test['date']).is_year_start\ntest['is_year_end'] = pd.DatetimeIndex(test['date']).is_year_end\nprint(test[['month','day','year','quarter','weekday','weekofyear','date']].head())","c5369905":"train['visitStartTime'] = pd.to_datetime(train['visitStartTime'], unit = 's')\ntrain['hour'] = pd.DatetimeIndex(train['visitStartTime']).hour\ntrain['minute'] = pd.DatetimeIndex(train['visitStartTime']).minute\nprint(train[['visitStartTime','hour','minute']].head())\n\ntest['visitStartTime'] = pd.to_datetime(test['visitStartTime'], unit = 's')\ntest['hour'] = pd.DatetimeIndex(test['visitStartTime']).hour\ntest['minute'] = pd.DatetimeIndex(test['visitStartTime']).minute\nprint(test[['visitStartTime','hour','minute']].head())","0719a75c":"train_staging = train.select_dtypes(exclude = 'object')\ntrain_staging = train_staging.select_dtypes(exclude = 'datetime')\ntrain_staging = train_staging.select_dtypes(exclude = 'bool')\n\nprint(train_staging.dtypes)\n\ntest_staging = test.select_dtypes(exclude = 'object')\ntest_staging = test_staging.select_dtypes(exclude = 'datetime')\ntest_staging = test_staging.select_dtypes(exclude = 'bool')\n\nprint(test_staging.dtypes)","b7510891":"train_staging_columns = train_staging.columns\n\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy = 'mean')\ntrain_staging = imputer.fit_transform(train_staging)\ntrain_staging = pd.DataFrame(\n    data = train_staging,\n    columns = train_staging_columns\n)\nprint(train_staging.isna().any())\n\ntest_staging_columns = test_staging.columns\n\ntest_staging = imputer.fit_transform(test_staging)\ntest_staging = pd.DataFrame(\n    data = test_staging,\n    columns = test_staging_columns\n)\nprint(test_staging.isna().any())","a6a751ea":"train_staging, test_staging = train_staging.align(test_staging, join = 'inner', axis = 1)\ntrain_staging['transactionRevenue'] = train['transactionRevenue']\ntest_staging['fullVisitorId'] = test['fullVisitorId']\ntrain_staging['fullVisitorId'] = train['fullVisitorId']","c77e7771":"print(train_staging.head())","9149d9a6":"train_agg = train_staging \\\n    .groupby(['fullVisitorId']) \\\n    .agg(['count','mean','min','max','sum']) \\\n    .reset_index()\n\ntest_agg = test_staging \\\n    .groupby(['fullVisitorId']) \\\n    .agg(['count','mean','min','max','sum']) \\\n    .reset_index()","043b734b":"columns_train = ['fullVisitorId']\n\n# Convert multi-level index from .agg() into clean columns\n# borrowing from: https:\/\/www.kaggle.com\/willkoehrsen\/introduction-to-manual-feature-engineering\nfor var in train_agg.columns.levels[0]:\n    if var != 'fullVisitorId':\n        for stat in train_agg.columns.levels[1][:-1]:\n            columns_train.append('%s_%s' % (var, stat))\n\ntrain_agg.columns = columns_train\n\ncolumns_test = ['fullVisitorId']\n\n# Convert multi-level index from .agg() into clean columns\n# borrowing from: https:\/\/www.kaggle.com\/willkoehrsen\/introduction-to-manual-feature-engineering\nfor var in test_agg.columns.levels[0]:\n    if var != 'fullVisitorId':\n        for stat in test_agg.columns.levels[1][:-1]:\n            columns_test.append('%s_%s' % (var, stat))\n\ntest_agg.columns = columns_test","96a1dac7":"del train_staging\ndel train\n\ndel test_staging\ndel test","b5a19a49":"print(train_agg.dtypes)","f0f897f5":"import math\n\ndef create_target(rev):\n    if rev == 0:\n        return 0\n    else:\n        return math.log(rev)\n\ntrain_agg['TARGET'] = train_agg['transactionRevenue_sum'].apply(create_target)\n\ntrain_agg = train_agg.drop(\n    [\n        'transactionRevenue_count',\n        'transactionRevenue_mean',\n        'transactionRevenue_min',\n        'transactionRevenue_max',\n        'transactionRevenue_sum'\n    ],\n    axis = 1\n)","47c91f92":"train_agg_corr = train_agg.corr()\nprint(train_agg_corr['TARGET'].sort_values(ascending = False))","a9ce78bf":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nid_train = train_agg['fullVisitorId']\nx = train_agg.drop(['TARGET','fullVisitorId'], axis = 1)\ny = train_agg['TARGET']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.33, random_state = 0)\n\nmodel = RandomForestRegressor()\nmodel.fit(x_train, y_train)\npredictions = model.predict(x_test)\n\nrms = sqrt(mean_squared_error(y_test, predictions))\n\nprint('RMSE train:', rms)","d85e8611":"importances = model.feature_importances_\nimportances_df = pd.DataFrame(\n    data = {'column' : x.columns, 'importance' : importances}\n)\n\nimportances_df = importances_df.sort_values(by = 'importance', ascending = False)\n\nimportances_df['weighted'] = importances_df['importance'] \/ importances_df['importance'].sum()\n\nplt.figure()\nplt.title('Feature Importances')\nplt.barh(\n    importances_df['column'].head(15),\n    importances_df['weighted'].head(15)\n)\nplt.show()","a4b7e058":"del train_agg\ndel Imputer\ndel RandomForestRegressor\ndel adContent_encoder\ndel auc\ndel browser_encoder\ndel campaign_encoder \ndel city_encoder \ndel columns_test \ndel columns_train\ndel country_encoder, create_target, date_encoder, imputer, json, json_normalize, keyword_encoder, math, metro_encoder, networkDomain_encoder, operatingSystem_encoder, parameters, plt, preprocessing\ndel referralPath_encoder\ndel region_encoder\ndel sns\ndel source_encoder\ndel stat\ndel subContinent_encoder\ndel test_adContent_encoder\ndel test_browser_encoder\ndel test_campaign_encoder\ndel test_city_encoder\ndel test_country_encoder\ndel test_date_encoder\ndel test_keyword_encoder\ndel test_metro_encoder\ndel test_networkDomain_encoder \ndel test_operatingSystem_encoder\ndel test_referralPath_encoder\ndel test_region_encoder\ndel test_source_encoder\ndel test_staging_columns\ndel test_subContinent_encoder\ndel test_visitNumber_encoder\ndel train_staging_columns\ndel train_test_split\ndel var\ndel visitNumber_encoder\ndel warnings","454e0cd9":"#import lightgbm as lightgbm\n#from sklearn.model_selection import train_test_split\n#from math import sqrt\n#from sklearn.metrics import mean_squared_error\n\n#x_train = lightgbm.Dataset(x_train)\n#y_train = lightgbm.Dataset(y_train)\n\n#parameters = {\n#    'num_leaves':31,\n#    'colsample_bytree' : .9,\n#    'metric':'l2_root',\n#    'learning_rate':0.03,\n#    'subsample' : 0.9, \n#    'random_state' : 1,\n#    'n_estimators': 1000\n#}\n\n#lgbm = lightgbm.train(\n#    parameters,\n#    x_train,\n#    y_train\n#)\n\n#p = lgbm.predict(x_test)\n\n#rms = sqrt(mean_squared_error(y_test, p))\n\n#print('LGBM RMSE train:', rms)","48a28b97":"predictions_test = model.predict(test_agg.drop(['fullVisitorId'], axis = 1))\n\nsubmission = pd.DataFrame({\n    \"fullVisitorId\": test_agg['fullVisitorId'].astype(str),\n    \"PredictedLogRevenue\": predictions_test\n    })\n\nsubmission['fullVisitorId'] = submission['fullVisitorId'].astype(str)\n\nimport csv\n\nsubmission.to_csv('submission_rf.csv', quoting=csv.QUOTE_NONNUMERIC, index = False)","b326b924":"#predictions_test_lgbm = lgbm.predict(test_agg.drop(['fullVisitorId'], axis = 1))\n\n#submission_lgbm = pd.DataFrame({\n#    \"fullVisitorId\": test_agg['fullVisitorId'].astype(str),\n#    \"PredictedLogRevenue\": predictions_test_lgbm\n#    })\n\n#submission_lgbm['fullVisitorId'] = submission_lgbm['fullVisitorId'].astype(str)\n\n#import csv\n\n#submission_lgbm.to_csv('submission_lgbm.csv', quoting=csv.QUOTE_NONNUMERIC, index = False)","e37b1be8":"# TRAIN LIGHTGBM\n* Did not result in better score, so disabling for now\n* Will continue experimenting with this...","f7963409":"# SUBMIT TO COMPETITION","d9dc56dc":"# THANKS!!","2661682b":"# CORRELATION CHECK\n* Now that we've gotten the data all cleaned up, let's see what kind of signal is in this data set out of the box!\n* You will see pageviews \/ hits are strongest correlations to the revenue the customer spends.  \n* Intuitively this makes sense: if they click around the site more, it's more likely it will end up in a transaction","e439b0ca":"# INTRO\n\n* I am currently working through this notebook.  The below is a basic trasnformation of the raw train.csv \/ test.csv files into a Machine Learning algorithm-friendly format (i.e. ints \/ floats only)\n\n* This data set is unique in that several of the columns including the TARGET are stored in a column of columns (stored via JSON)\n\n* I hope you find this helpful \/ interesting.  If you have any feedback please share in the comments or email me at jack.s.mengel@gmail.com\n\n* My next step is to try and improve my score with advanced feature engineering!","d810fc10":"# FLATTEN JSON COLUMNS\n\n* You will see below a few columns (i.e. device) represent additional columns!\n* Need to \"flatten\" this using json.loads","2bc257f4":"# DATE COLUMNS \n* Might as well get meta-data of the dates (weekday \/ is_month_end etc) and see if this generates a signal","97697a85":"# Let's convert text columns to numeric\n# keep as objects: sessionID\n# object columns that are obviously just numbers and should be converted straight up to ints: \n* visitStartTime\n* visitId\n* transactionRevenue\n* visitNumber\n* hits\n* pageviews","126af751":"# FLATTEN AGG() OUTPUT\n\n* Unfortunately the agg() method returns a dataframe with a multi-layer index.  We need to flatten this to make it useful\n* Basically just iterate through each column and name it using the column and metric on that column","2813bc98":"# AGGREGATE fullVisitorId SESSIONS\n\n* The competition calls for the resulting predictions to be on a customer level, not a transaction level\n* Therefore need to aggregate train \/ test data on a customer level for training of model\n* Below I am using groupby on fullVisitorId, then using a handful of metrics to aggregate their sessions for EACH COLUMN","ad1efc8d":"# CREATE TRAIN \/ TEST DataFrames\n* There are only two files in this data set: train \/ test.  \n\n# IMPORTANT!! NEED TO CONVERT fullVisitorId to a string -> leading 0's get shaved off otherwise!  Learned this the hard way.","147545da":"# REMOVE ALL UNNECESSARY COLUMNS\n* Store in train_staging and test_staging","8f5ce7a9":"# ALIGN TEST \/ TRAIN","d4ad6cec":"# Next, find columns with large amounts of unique values...","0ec01731":"# IMPORT LIBRARIES\n* Used in this notebook: pandas \/ sklearn\n* Have other libraries ready in case these are needed in upcoming EDA","c808a86e":"# AGAIN, COMMENTING OUT LGBM MODEL","d8a759a8":" # ...and label encode them.  Columns to label encode:\n* networkDomain          28064\n* gclId                  17774\n* keyword                 3659\n* referralPath            1475\n* city                     649\n* visitNumber              384\n* source                   380\n* region                   376\n* date                     366\n* hits                     274\n* country                  222\n* pageviews                214\n* metro                     94\n* browser                   54\n* adContent                 44\n* subContinent              23\n* operatingSystem           20\n* campaign 10","eff7fdb3":"# FILL NANS\n* Need to fill the NaN values for the Machine Learning algorithm!\n* Using Imputer from sci-kit learn","239c6bc2":"# TIME COLUMNS\n* Same as date columns!","b041ecbf":"# MORE MEMORY MANAGEMENT\n* Don't need train \/ test \/ train_staging \/ test_staging anymore as we aggregated using these DataFrames","a3604475":"# TRAIN RANDOM FOREST MODEL\n\n* Using all features to inform the model","c48e922d":"# DELETE UNNECESSARY OBJECTS","0d1342a6":"# One-hot encode the rest...","0c7f2247":"# ADD FLATTENED COLUMNS BACK TO TRAIN \/ TEST\n* Need to then bring these flattened columns back into the original train \/ test files and delete the columns you original JSON-formatted columns\n* NOTE: Some columns have no actual data -> deleting these as well\n* Given these are large-ish files, important to delete unnecessary objects \/ dataframes along the way here.  RAM gets full if you do not do so","ea6a2a92":"# NATURAL LOG\n* The competition calls for the TARGET to be the natural log of the actual amount spent\n* Using math library to convert the train data into natural log of itself"}}