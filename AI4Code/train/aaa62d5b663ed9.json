{"cell_type":{"b1494154":"code","14c2758b":"code","255f3be6":"code","15586ec8":"code","8813ad1c":"code","7d440a9d":"code","61d8e2b8":"code","d8bd62a6":"code","a5ebd9ce":"code","13d4ed9b":"code","198d362a":"code","758f5102":"code","77dd47b9":"code","4c881d88":"code","be51ab90":"code","55fc9b8a":"code","74cce1df":"code","cbd9d014":"code","f6eb1c81":"code","9950c56f":"code","922df72e":"code","83aebbbd":"code","7c5b8b48":"code","fdde7174":"code","e2d6124a":"code","7e35da23":"code","b033ceb7":"code","311aae5c":"code","0367c448":"code","9364cfe3":"code","c5ea91f8":"code","cf72817b":"code","88d8812c":"code","23ee434d":"code","1053ac2a":"code","59edf92e":"code","25922eb0":"code","dd981a27":"code","9ff021d5":"code","39f5642f":"code","c5259e3b":"code","0f73d510":"code","fafe66ad":"code","d5f33a18":"code","4eeb8503":"code","cce00a53":"code","f8f8062f":"code","4698e048":"markdown","31a44659":"markdown","959f4033":"markdown","6db0e8ae":"markdown","897c0640":"markdown","bc2f31d1":"markdown","22914965":"markdown","bf6e78a7":"markdown","3603332e":"markdown","5ffb824a":"markdown","fe2abc39":"markdown","0fcf5a59":"markdown","1a6b5007":"markdown","09927f06":"markdown","1be3458a":"markdown","e00d5fec":"markdown","6c735e70":"markdown","ef3368ef":"markdown","ded30247":"markdown","8e67602b":"markdown","aa461b2b":"markdown","d1b93185":"markdown","f713e801":"markdown"},"source":{"b1494154":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import wordnet\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n\nfrom tensorflow import string as tf_string\nfrom tensorflow import keras\nfrom keras.models import Model\nfrom keras.layers.experimental.preprocessing import TextVectorization\nfrom keras.layers import Input, Embedding, Dropout, Dense, Flatten, GRU, LSTM, Bidirectional\nfrom keras.layers import Conv2D, MaxPool2D, Reshape, Concatenate #, CuDNNLSTM, CuDNNGRU\nfrom tensorflow.compat.v1.keras.layers import CuDNNGRU, CuDNNLSTM\nfrom keras.callbacks import EarlyStopping\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical","14c2758b":"folder = '\/kaggle\/input'\nfor dirname, _, filenames in os.walk(folder):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","255f3be6":"swIV = pd.read_csv(folder + '\/star-wars-movie-scripts\/SW_EpisodeIV.txt', \n                   delim_whitespace=True, usecols=[1,2], names=['name', 'txt'], skiprows=[0])\nswV = pd.read_csv(folder + '\/star-wars-movie-scripts\/SW_EpisodeV.txt', \n                  delim_whitespace=True, usecols=[1,2], names=['name', 'txt'], skiprows=[0])\nswVI = pd.read_csv(folder + '\/star-wars-movie-scripts\/SW_EpisodeVI.txt', \n                   delim_whitespace=True, usecols=[1,2], names=['name', 'txt'], skiprows=[0])\nsw = pd.concat([swIV, swV, swVI], ignore_index=True)\nsw['universe'] = 'Star Wars'\nsw","15586ec8":"hp1 = pd.read_csv(folder + '\/harry-potter-dataset\/Harry Potter 1.csv', \n                  sep=';', encoding='ISO-8859-1', names=['name', 'txt'], skiprows=[0])\nhp2 = pd.read_csv(folder + '\/harry-potter-dataset\/Harry Potter 2.csv', \n                  sep=';', encoding='ISO-8859-1', names=['name', 'txt'], skiprows=[0])\nhp3 = pd.read_csv(folder + '\/harry-potter-dataset\/Harry Potter 3.csv', \n                  sep=';', encoding='ISO-8859-1', names=['name', 'txt'], skiprows=[0])\nhp = pd.concat([hp1, hp2, hp3], ignore_index=True)\nhp['universe'] = 'Harry Potter'\nhp","8813ad1c":"lr = pd.read_csv(folder + '\/lord-of-the-rings-data\/lotr_scripts.csv', \n                 usecols=[1,2], names=['name', 'txt'], skiprows=[0])\nlr['universe'] = 'Lord of the Rings'\nlr","7d440a9d":"rm = pd.read_csv('\/kaggle\/input\/rickmorty-scripts\/RickAndMortyScripts.csv', \n                 usecols=[4,5], names=['name', 'txt'], skiprows=[0])\nrm['universe'] = 'Rick and Morty'\nrm","61d8e2b8":"data = pd.concat([sw, hp, lr, rm], ignore_index=True)\ndata","d8bd62a6":"label_encoder = LabelEncoder()\ndata['label'] = label_encoder.fit_transform(data.universe)\nuniverses = data.groupby(['universe', 'label']).count().reset_index()\nuniverses.loc[universes.universe == 'Harry Potter', 'name'] = hp.name.value_counts().size\nuniverses.loc[universes.universe == 'Lord of the Rings', 'name'] = lr.name.value_counts().size\nuniverses.loc[universes.universe == 'Rick and Morty', 'name'] = rm.name.value_counts().size\nuniverses.loc[universes.universe == 'Star Wars', 'name'] = sw.name.value_counts().size\nuniverses.columns = ['universe', 'label', 'cnt_characters', 'cnt_dialogues']\nuniverses","a5ebd9ce":"print('The dataset contains', data.txt.size, 'dialogues between', universes.cnt_characters.sum(), \n      'characters from', universes.label.size, 'universes:\\n')\nfor index, row in universes.iterrows():\n    print('\\t', row.universe, '\\n\\t\\tCount of characters:', row.cnt_characters, \n          '\\n\\t\\tCount of dialogues:', row.cnt_dialogues, '\\n')","13d4ed9b":"data.txt.isna().value_counts()","198d362a":"data.dropna(inplace=True)\ndata['clean_txt'] = data['txt'].apply(lambda x: re.sub(r'[^A-Za-z]+', ' ', x))\ndata['clean_txt'] = data['clean_txt'].apply(lambda x: x.lower())\ndata['clean_txt'] = data['clean_txt'].apply(lambda x: x.strip())\n\nstop_words = stopwords.words('english')\ndata['clean_txt'] = data['clean_txt'].apply(lambda x: ' '.join([words for words in x.split() \n                                                                if words not in stop_words]))\nlem = wordnet.WordNetLemmatizer()\ndata['clean_txt'] = data['clean_txt'].apply(lambda x: ' '.join([lem.lemmatize(item, pos='v') \n                                                                for item in x.split()]))\ndata.head()","758f5102":"del_lines = data.loc[data.clean_txt == '']\ndata = data.loc[data.clean_txt != '']\ndel_lines.head()","77dd47b9":"universes['cnt_cleaned'] = data.label.value_counts()\nuniverses","4c881d88":"fig = plt.figure(figsize = (12,5))\nax = fig.add_subplot(111)\nsns.countplot(data.universe)\nplt.xlabel('Universe', size = 15)\nplt.ylabel('Count', size= 15)\nplt.xticks(size = 12)\nplt.title(\"Count of dialogues by universe\" , size = 20)\nplt.show()","be51ab90":"data['length'] = data['txt'].apply(lambda x: len(x))\n\nidx, row, col = 0, 2, 2\nfig, axes = plt.subplots(row, col, figsize=(15, 8))\nfig.suptitle('Length of texts', size=20)\ncolors = ['orange', 'green', 'brown', 'lightblue']\nfor r in range(row):\n    for c in range(col):\n        data.loc[data['label'] == idx].hist(ax=axes[r,c], column='length', by='universe', \n                                            bins=50, xrot=0, color=colors[idx])\n        idx += 1","55fc9b8a":"plt.figure(figsize=(12,5))\nfor col in data.universe.unique():\n    ax = sns.distplot(data.loc[(data['universe'] == col) & (data['length'] < 350), 'length'], kde = False)\n    \nax.legend(data.universe.unique())\nax.set_title('Distribution of length of texts', fontsize = 20)\nax.set_xlabel('length')\nsns.despine(left = True)\nax.set_ylabel('count')\nplt.show()","74cce1df":"fig, axes = plt.subplots(1, 2, figsize=(12, 5))\nfig.suptitle('Length of texts', size=20)\n\nsns.boxplot(ax=axes[0], x='universe', y='length', data=data, palette='coolwarm')\naxes[0].set_title('with outliers', size=15)\naxes[0].tick_params('x', rotation=15)\n\nsns.boxplot(ax=axes[1], x='universe', y='length', data=data, palette='coolwarm', showfliers=False)\naxes[1].set_title('no outliers', size=15)\naxes[1].tick_params('x', rotation=15)","cbd9d014":"X = data.clean_txt\ny = data.label\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1,\n                                                      random_state=13, stratify=y_train)\nprint('Train:', X_train.shape, y_train.shape)\nprint('Test:', X_test.shape, y_test.shape)\nprint('Validation:', X_valid.shape, y_valid.shape)\n\ny_train_vect = to_categorical(y_train)\ny_valid_vect = to_categorical(y_valid)\nprint('\\nEncoding labels example:')\nfor i in range(5):\n    print('  ', list(y_train)[i], '  ', y_train_vect[i])","f6eb1c81":"word_freq = {}\nfor txt in data.clean_txt:\n    words = pd.Series(txt.split(' ')).value_counts()\n    for word in words.index:\n        if word.index in word_freq:\n            word_freq[word.index] += words[word]\n        else: word_freq[word.index] = words[word]\nprint('Count of unique words:', len(word_freq))","9950c56f":"embedding_dim = 128 \nvocab_size = len(word_freq)\nsequence_length = 64 \nvect_layer = TextVectorization(max_tokens=vocab_size, output_mode='int',\n                               output_sequence_length=sequence_length)\nvect_layer.adapt(data.clean_txt.values)\n\nprint('Vocabulary example: ', vect_layer.get_vocabulary()[:10])\nprint('Vocabulary shape: ', len(vect_layer.get_vocabulary()))","922df72e":"input_layer = Input(shape=(1,), dtype=tf_string)\nx_v = vect_layer(input_layer)\nemb = Embedding(vocab_size, embedding_dim)(x_v)\nx = LSTM(64, activation='relu', return_sequences=True)(emb)\nx = GRU(64, activation='relu', return_sequences=True)(x)\nx = Flatten()(x)\nx = Dense(64, 'relu')(x)\nx = Dense(32, 'relu')(x)\nx = Dropout(0.2)(x)\noutput_layer = Dense(4, 'softmax')(x)\n\nmodel = Model(input_layer, output_layer)\nmodel.summary()\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])","83aebbbd":"es = EarlyStopping(monitor='val_loss', min_delta=0, patience=70, restore_best_weights=True)\nbatch_size = 512\nepochs = 50\nhistory = model.fit(X_train.values, y_train_vect, validation_data=(X_valid.values, y_valid_vect), \n                    callbacks=[es], epochs=epochs, batch_size=batch_size)","7c5b8b48":"def show_history(history):\n    plt.figure(figsize=(12,5))\n    for key in history.history.keys():\n        plt.plot(history.epoch, history.history[key], label=key)\n    plt.legend()\n    plt.tight_layout()","fdde7174":"show_history(history)","e2d6124a":"def class_report(y_test, y_pred_vect):\n    y_pred = np.argmax(y_pred_vect, axis=1)\n    test_accuracy = np.sum(y_pred == y_test.values) \/ y_test.size\n    print('Test accuracy:', test_accuracy)\n    print('Accuracy score: ', accuracy_score(y_test, y_pred))\n    print('F1 score: ', f1_score(y_test, y_pred, average='macro'), '\\n')\n    print(classification_report(y_true=y_test, y_pred=y_pred))\n\n    conf_mtx = confusion_matrix(y_test, y_pred)\n    df_conf_mtx = pd.DataFrame(conf_mtx, index=universes.universe, columns=universes.universe)\n    plt.figure(figsize=(12,5))\n    sns.heatmap(df_conf_mtx, fmt='d', annot=True, cmap='Blues')\n    plt.xlabel('Predicted label', size = 15)\n    plt.ylabel('True label', size= 15)\n    plt.title('Confusion matrix', size=20)\n    plt.show()","7e35da23":"class_report(y_test, model.predict(X_test))","b033ceb7":"input_layer = Input(shape=(1,), dtype=tf_string)\nx_v = vect_layer(input_layer)\nemb = Embedding(vocab_size, embedding_dim)(x_v)\nx = Bidirectional(LSTM(128, return_sequences=True))(emb)\nx = Dropout(0.5)(x)\nx = Bidirectional(LSTM(64))(x)\nx = Dropout(0.5)(x)\nx = Dense(32, 'relu')(x)\nx = Dropout(0.5)(x)\noutput_layer = Dense(4, 'softmax')(x)\n\nmodel_bdirect = Model(input_layer, output_layer)\nmodel_bdirect.summary()\nmodel_bdirect.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])","311aae5c":"es = EarlyStopping(monitor='val_loss', min_delta=0, patience=70, restore_best_weights=True)\nbatch_size = 512\nepochs = 50\nhistory_bdirect = model_bdirect.fit(X_train.values, y_train_vect, \n                                    validation_data=(X_valid.values, y_valid_vect), \n                                    callbacks=[es], epochs=epochs, batch_size=batch_size)","0367c448":"show_history(history_bdirect)","9364cfe3":"class_report(y_test, model_bdirect.predict(X_test))","c5ea91f8":"tokenizer_keras = Tokenizer(oov_token = \"<OOV>\")\ntokenizer_keras.fit_on_texts(data.clean_txt)\nword_index = tokenizer_keras.word_index\nvocab_token_size = len(word_index)\nprint('Vocabulary shape:', vocab_token_size)\nlist(word_index.items())[:10]","cf72817b":"def prepare_data(X, tokenizer, max_len):\n    sequences = tokenizer.texts_to_sequences(X)\n    padded = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n    return padded","88d8812c":"max_len = 512\nX_train_vect = prepare_data(X_train, tokenizer_keras, max_len)\nX_valid_vect = prepare_data(X_valid, tokenizer_keras, max_len)\nX_test_vect = prepare_data(X_test, tokenizer_keras, max_len)","23ee434d":"input_layer = Input(shape=(max_len,))\nemb = Embedding(vocab_token_size+1, embedding_dim, trainable=False)(input_layer)\nx = Bidirectional(LSTM(128, return_sequences=True))(emb)\nx = Dropout(0.5)(x)\nx = Bidirectional(LSTM(64))(x)\nx = Dropout(0.5)(x)\nx = Dense(32, 'relu')(x)\nx = Dropout(0.5)(x)\noutput_layer = Dense(4, 'softmax')(x)\n\nmodel_token = Model(input_layer, output_layer)\nmodel_token.summary()\nmodel_token.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])","1053ac2a":"es = EarlyStopping(monitor='val_loss', min_delta=0, patience=70, restore_best_weights=True)\nbatch_size = 512\nepochs = 50\nhistory_token = model_token.fit(X_train_vect, y_train_vect, \n                                validation_data=(X_valid_vect, y_valid_vect), \n                                callbacks=[es], epochs=epochs, batch_size=batch_size)","59edf92e":"show_history(history_token)","25922eb0":"class_report(y_test, model_token.predict(X_test_vect))","dd981a27":"glove_embeddings = np.load(folder + '\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl',\n                           allow_pickle=True)\nembedding_dim = len(glove_embeddings['the'])\nprint(\"There are\", len(glove_embeddings), \"words and\", embedding_dim,  \"dimensions in Glove Dictionary.\")","9ff021d5":"embedding_mtx = np.zeros((vocab_token_size+1, embedding_dim))\nfor word, idx in word_index.items():\n    if word in glove_embeddings:\n        embedding_mtx[idx] = glove_embeddings[word]\n        \ntokenized = pd.DataFrame([word_index]).T.reset_index()\ntokenized.columns = ['words','index']\ntemp_mtx = pd.DataFrame(embedding_mtx).reset_index()\ntemp_mtx = temp_mtx.drop(0, axis = 0)\ndf_embedding_mtx = pd.merge(tokenized, temp_mtx, on = 'index')\ndf_embedding_mtx","39f5642f":"input_layer = Input(shape=(max_len,))\nemb = Embedding(vocab_token_size+1, embedding_dim, weights=[embedding_mtx], \n                trainable=False)(input_layer)\nx = Bidirectional(LSTM(128, return_sequences=True))(emb)\nx = Dropout(0.5)(x)\nx = Bidirectional(LSTM(64))(x)\nx = Dropout(0.5)(x)\nx = Dense(32, 'relu')(x)\nx = Dropout(0.5)(x)\noutput_layer = Dense(4, 'softmax')(x)\n\nmodel_glove = Model(input_layer, output_layer)\nmodel_glove.summary()\nmodel_glove.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])","c5259e3b":"es = EarlyStopping(monitor='val_loss', min_delta=0, patience=70, restore_best_weights=True)\nbatch_size = 512\nepochs = 50\nhistory_glove = model_glove.fit(X_train_vect, y_train_vect, \n                                validation_data = (X_valid_vect, y_valid_vect),\n                                callbacks=[es], epochs=epochs, batch_size=batch_size)","0f73d510":"show_history(history_glove)","fafe66ad":"class_report(y_test, model_glove.predict(X_test_vect))","d5f33a18":"filter_sizes = [1,2,3,5]\nnum_filters = 42\n\ninput_layer = Input(shape=(max_len,))\nemb = Embedding(vocab_token_size+1, embedding_dim, weights=[embedding_mtx],\n                trainable=False)(input_layer)\nx = Reshape((max_len, embedding_dim, 1))(emb)\n\nconv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim),\n                             kernel_initializer='he_normal', activation='tanh')(x)\nconv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim),\n                             kernel_initializer='he_normal', activation='tanh')(x)\nconv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), \n                             kernel_initializer='he_normal', activation='tanh')(x)\nconv_3 = Conv2D(num_filters, kernel_size=(filter_sizes[3], embedding_dim),\n                             kernel_initializer='he_normal', activation='tanh')(x)\nmaxpool_0 = MaxPool2D(pool_size=(max_len - filter_sizes[0] + 1, 1))(conv_0)\nmaxpool_1 = MaxPool2D(pool_size=(max_len - filter_sizes[1] + 1, 1))(conv_1)\nmaxpool_2 = MaxPool2D(pool_size=(max_len - filter_sizes[2] + 1, 1))(conv_2)\nmaxpool_3 = MaxPool2D(pool_size=(max_len - filter_sizes[3] + 1, 1))(conv_3)\n\nz = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2, maxpool_3])   \nz = Flatten()(z)\nz = Dropout(0.1)(z)\noutput_layer = Dense(4, 'softmax')(z)\n\nmodel_cnn = Model(input_layer, output_layer)\nmodel_cnn.summary()\nmodel_cnn.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])","4eeb8503":"es = EarlyStopping(monitor='val_loss', min_delta=0, patience=70, restore_best_weights=True)\nbatch_size = 512\nepochs = 50\nhistory_cnn = model_cnn.fit(X_train_vect, y_train_vect, \n                            validation_data = (X_valid_vect, y_valid_vect),\n                            callbacks=[es], epochs=epochs, batch_size=batch_size)","cce00a53":"show_history(history_cnn)","f8f8062f":"class_report(y_test, model_cnn.predict(X_test_vect))","4698e048":"### *Length of dialogues is usually between 25 and 70 words. Dialogues in Harry Potter universe are considerably shorter than others. Serial Rick and Morty contains the longest dialogues but mostly about a little more. On the other hand, a few texts there are longer than thousand words.*","31a44659":"# Splitting data to training, validation and testing part","959f4033":"# ARD - project\n## *Determination of the universe according to the sentence of a character*\n\n**Author**: Brenda Lesniczakova, LES0045 <br>\n**Datasets**: Scripts from Star Wars, Rick and Morty, Harry Potter and Lord of the Rings","6db0e8ae":"# Summary\nDataset was split to 3 parts for training (72%), validation (8%) and testing (20%). Models trained on values of attributte **clean_txt**. Labels present universe of character dialogue and for better using was encoded to integer and represent as categoraical vector. The data was trained on 5 models:\n  1. base model with *LSTM* and *GRU* layers use vocabulary *TextVectorization* - accuracy: ~0.729, f1-score: ~0.711\n  2. model use *Bidirectional*(*LSTM*) layers, also use vocabulary *TextVectorization* - accuracy: ~0.724, f1-score: ~0.710\n  3. model use vocabulary *Tokenize* - but the accuracy results were much worse - accuracy: ~0.531, f1-score: ~0.432\n  4. model includes **GloVe Dictionary** to tokenized vocabulary - accuracy: ~0.739, f1-score: ~0.725\n  5. model use 2D convolution layers and operation max pooling - accuracy: ~0.737, f1-score: ~0.722\n\nIn general, models 2, 3 and 4 are built on the same foundation, main difference is used embedding layer. Models 1 and 2 there use vocabulary *TextVectorization*. Model 3 there use vocabulary *Tokenize*. Models 4 and 5 there use vocabulary *Tokenizer* with **GloVe dictionary**.\n\nModels 1-4 work with activation function *relu* and finally is used function *softmax* due to categorical label.\n\nBatch with size 512 prove to be the best and optimal count of epochs is 50 (more epochs make process of save and commit version too long and after 9 hours was cancelled).","897c0640":"# Vocabulary: Tokenizer","bc2f31d1":"## Star Wars: script of original trilogy (episodes 4-6)\nSource: https:\/\/www.kaggle.com\/xvivancos\/star-wars-movie-scripts","22914965":"# Vocabulary: TextVectorization","bf6e78a7":"# Model with embedding file","3603332e":"### *Some dialogues consist only of stop words. The data will be removed after cleaning.*","5ffb824a":"# Model","fe2abc39":"# Embedding file: GloVe Dictionary\nFile **glove.840B.300d.pkl** was imported from https:\/\/www.kaggle.com\/authman\/pickled-glove840b300d-for-10sec-loading","0fcf5a59":"# Model using CNN","1a6b5007":"# Classification report","09927f06":"# Dataset","1be3458a":"## Rick and Morty: script of first three seasons\nSource: https:\/\/www.kaggle.com\/andradaolteanu\/rickmorty-scripts","e00d5fec":"# All data import\nThe data is in the form of a CSV file. All datasets contain character names and their dialogues.","6c735e70":"# Graph of history","ef3368ef":"## Lord of the Rings: script of all three movies\nSource: https:\/\/www.kaggle.com\/paultimothymooney\/lord-of-the-rings-data","ded30247":"# Data summary\n*Count of characters and dialogues before and after cleaning data by universe.*","8e67602b":"### *There is one missing value in this dataset. The record with missing value will be removed for next processing.*","aa461b2b":"# Model - Bidirectional layer","d1b93185":"### *The script of Harry Potter is a little bit bigger than others, but tests with less records (using only one or two movie scripts) achieved worse results.*","f713e801":"## Harry Potter: script of first three movies\nSource: https:\/\/www.kaggle.com\/gulsahdemiryurek\/harry-potter-dataset"}}