{"cell_type":{"1fb13178":"code","c8177cba":"code","fda5f579":"code","06e00efb":"code","7d11f80e":"code","338c0ee4":"code","1d495dc4":"code","c2bc6fa6":"code","d783a9e9":"code","d8b7aed8":"code","5d364373":"code","1f1646ae":"code","f841818f":"code","8d5fc12d":"code","acef9ed5":"code","c2023b95":"code","eec80541":"code","4466d71a":"code","1c960ca0":"code","968cb5fe":"markdown","e7856eca":"markdown","f06be736":"markdown","af6a2084":"markdown","6fc72b52":"markdown","b989d2f7":"markdown","e9abe2f4":"markdown","e5a5debe":"markdown"},"source":{"1fb13178":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom numpy import zeros\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport random\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfor dirname, _, filenames in os.walk('\/kaggle\/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\nimport matplotlib.pyplot as plt     \nimport seaborn as sns  # visualization tool\n\nfrom tqdm import tqdm \n\nimport nltk, re\n#nltk.download('stopwords') # load english stopwords\n#nltk.download('wordnet')\nfrom nltk.corpus import stopwords\nSTOPWORDS = set(stopwords.words('english'))\n\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import wordnet as wn\nfrom nltk.tokenize import word_tokenize\n\nfrom collections import Counter\nfrom itertools import chain\nimport pickle\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\nwarnings.warn(\"deprecated\", DeprecationWarning)\nwarnings.simplefilter(\"ignore\")\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.utils import resample\nfrom sklearn.metrics import accuracy_score, roc_auc_score\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.layers import Input, Dense, Embedding, Dropout, LSTM, Bidirectional, Flatten, CuDNNLSTM, Conv1D, Average\nfrom keras.models import Model, Sequential\nfrom keras.optimizers import Adam\nfrom keras.constraints import unit_norm, max_norm\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate, BatchNormalization\nfrom keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\nimport tensorflow as tf","c8177cba":"def seed_everything(seed=1234):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_everything()","fda5f579":"PATH = '\/kaggle\/input\/nlp-getting-started\/'\n\ntrain = pd.read_csv(PATH + 'train.csv')\ntest = pd.read_csv(PATH + 'test.csv')","06e00efb":"## globals\n\nREPLACE_BY_SPACE_RE = re.compile('[\/(){}\\[\\]\\|@,;-_]')\nBAD_SYMBOLS_RE = re.compile('[!\"#$%&()*+,.\/:;<=>?@[\\\\]^`{|}~\\t\\n\u201c\u201d\u2019\\'\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014]')\nUNKNOWN = 'UNK'\nURL = 'URL'\n\n#Emoji patterns\nemoji_pattern = re.compile(\"[\"\n         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n         u\"\\U00002702-\\U000027B0\"\n         u\"\\U000024C2-\\U0001F251\"\n         \"]+\", flags=re.UNICODE)","7d11f80e":"def clean_tweet(tweet):\n\n    if pd.isnull(tweet):\n        return ''\n\n    tweet = tweet.lower()\n    tweet = tweet.replace('...', ' ... ').strip()\n    tweet = tweet.replace(\"'\", \" ' \").strip()\n    tweet = tweet.replace('%20', ' ').strip() \n\n    tweet = re.sub(r'https?:\/\/\\S+|www\\.\\S+', URL, tweet, flags=re.MULTILINE)\n    tweet = re.sub(REPLACE_BY_SPACE_RE,' ',tweet)\n    tweet = re.sub(BAD_SYMBOLS_RE,'',tweet)\n    tweet = re.sub(r'\\d+',' ',tweet)\n    tweet = re.sub(r'\\s+',' ',tweet)\n    tweet = re.sub(r'<.*?>', '', tweet)\n    tweet = emoji_pattern.sub(r'',tweet)\n    \n    # delete stopwords from text\n    tweet = ' '.join(w for w in nltk.wordpunct_tokenize(tweet))\n    tweet = ' '.join([i for i in tweet.split() if i not in STOPWORDS])\n    return tweet\n\ntrain['cleaned_text'] = train['text'].apply(clean_tweet)\ntest['cleaned_text'] = test['text'].apply(clean_tweet)","338c0ee4":"train.head(10)","1d495dc4":"## from: https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\n\n# WordNet lexical database for lemmatization\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\ndef create_corpus(df):\n    corpus=[]\n    tokenizer = TweetTokenizer()\n    for tweet in tqdm(df['cleaned_text']):\n        words = [word.lower() for word in tokenizer.tokenize(tweet) if((word.isalpha()==1))]\n        words = [wordnet_lemmatizer.lemmatize(word) for word in words] \n        corpus.append(words)\n    return corpus\n\nfull = pd.concat([train,test])\ncorpus = create_corpus(full)","c2bc6fa6":"MAX_LEN = 50\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\n\nX_train = tokenizer.texts_to_sequences(train['cleaned_text'])\nX_train = pad_sequences(X_train, maxlen=MAX_LEN, truncating='post', padding='post')    \ny_train = train['target']\n\nX_test = tokenizer.texts_to_sequences(test['cleaned_text'])\nX_test = pad_sequences(X_test, maxlen=MAX_LEN, truncating='post', padding='post')    \n\nword_index = tokenizer.word_index\nprint('Number of unique words:',len(word_index))","d783a9e9":"with open('..\/input\/\/glove-6b-50d-txt-glove-6b-50d-pkl\/glove.6B.50d.pkl', 'rb') as fp:\n#with open('..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl', 'rb') as fp:\n    glove = pickle.load(fp)\n    \nnum_words = len(word_index)+1\nembedding_matrix = np.zeros((num_words, MAX_LEN))\n\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    \n    emb_vec = glove.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec\n","d8b7aed8":"from sklearn.utils import class_weight\n\nclass_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n\nprint(class_weights) ","5d364373":"def build_model(embedding_matrix, units, drop_num = None):\n    words = Input(shape=(None,))\n    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n    x = SpatialDropout1D(0.5)(x)\n    x = Bidirectional(LSTM(units, return_sequences=True, kernel_constraint=unit_norm()))(x)\n    x = BatchNormalization()(x)\n    x = Bidirectional(LSTM(units, return_sequences=True, kernel_constraint=unit_norm()))(x)\n    x = Conv1D(units, kernel_size=3, padding='valid', kernel_initializer='glorot_uniform')(x)\n    x = BatchNormalization()(x)\n\n    x = concatenate([GlobalMaxPooling1D()(x), GlobalAveragePooling1D()(x)])    \n    x = add([x, Dense(units*2, activation='relu', kernel_constraint=unit_norm())(x)])\n    x = BatchNormalization()(x)\n    out = Dense(units, activation='relu', kernel_constraint=unit_norm())(x)\n\n    ## here is the implementation of multi-sample dropout\n    if drop_num is not None:\n        dense = []\n        FC = Dense(units, activation='relu', kernel_constraint=unit_norm())\n        xx = BatchNormalization()(out)\n        for p in np.linspace(0.2,0.8, drop_num):\n            xx = Dropout(p)(xx)\n            xx = FC(xx)\n            xx = Dense(units, activation='softmax')(xx)\n            dense.append(xx)\n        out = Average()(dense)    \n    \n    result = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inputs=words, outputs=result)\n    adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['acc'])\n    return model","1f1646ae":"from keras.utils import plot_model\n\nmodel = build_model(embedding_matrix, 128, 5)\nplot_model(model, to_file='model.png')","f841818f":"def fit_and_predict(model, trainX, trainy, testX, testy, epochs = 40, batch_size=256):\n    history = model.fit(trainX, trainy, epochs=epochs, verbose=1, validation_data=(testX, testy), batch_size=batch_size)\n    y_pred = model.predict(testX)\n    predictions = (y_pred > 0.5).astype(int).ravel()\n    return accuracy_score(testy, predictions), history\n\n# evaluate a single mlp model\ndef evaluate_model(fold, trainX, trainy, testX, testy, embedding_matrix, class_weights, epochs = 40, batch_size=256, units = 128):\n    ckpt = ModelCheckpoint(f'lstm_{fold}.hdf5', save_best_only = True, save_weights_only = True)\n    es = EarlyStopping(monitor='val_loss', patience=5, mode='min', baseline=None, restore_best_weights=False)\n    # define model\n    model = build_model(embedding_matrix, units)    \n    # fit model\n    history = model.fit(trainX, trainy, epochs=epochs, verbose=1, validation_data=(testX, testy), \n                        class_weight=class_weights, batch_size=batch_size, callbacks=[es, ckpt])\n    # make predictions\n    y_pred = model.predict(testX)\n    predictions = (y_pred > 0.5).astype(int).ravel()\n    return model, predictions, history\n\n# make an ensemble prediction for binary classification\ndef ensemble_predictions(members, testX):\n    # make predictions\n    yhats = [model.predict(testX) for model in members]\n    predictions = np.mean(np.array(yhats), axis=0)\n    return (predictions > 0.5).astype(int).ravel()\n \n# evaluate a specific number of members in an ensemble\ndef evaluate_n_members(members, n_members, testX, testy):\n    # select a subset of members\n    subset = members[:n_members]\n    # make prediction\n    yhat = ensemble_predictions(subset, testX)\n    # calculate accuracy\n    return accuracy_score(testy, yhat), yhat\n\n# splits data into train and test sets\ndef split_dataset(X, y, n_samples):\n    ix = [i for i in range(len(X))]\n    train_ix = resample(ix, replace=True, n_samples=n_samples)\n    test_ix = resample(ix, replace=True, n_samples=len(X)-len(train_ix))\n    return X[train_ix], y[train_ix], X[test_ix], y[test_ix]","8d5fc12d":"BATCH_SIZE = 512\n\n\ndef build_and_evaluate(embedding_matrix, train_samples, epochs, units, msd = None):\n    trainX, trainy, testX, testy = split_dataset(X_train, y_train, TRAIN_SAMPLES)\n    model = build_model(embedding_matrix, units, msd)\n    score, history = fit_and_predict(model, trainX, trainy, testX, testy, epochs = epochs, batch_size = BATCH_SIZE)\n    print(\"Accuracy: %.2f%%\" % (score))\n    return model, history, score\n\ndef run_experiment(epochs):\n    no_msd_res = dict( models = [], histories = [], scores = [] )\n    msd_res = dict( models = [], histories = [], scores = [] )\n    for epoch in epochs:\n        model, history, score = build_and_evaluate(embedding_matrix, TRAIN_SAMPLES, epoch, UNITS, MSD_LAYERS)\n        msd_res['models'].append(model)\n        msd_res['histories'].append(history)\n        msd_res['scores'].append(score)\n        \n        model, history, score = build_and_evaluate(embedding_matrix, TRAIN_SAMPLES, epoch, UNITS)\n        no_msd_res['models'].append(model)\n        no_msd_res['histories'].append(history)\n        no_msd_res['scores'].append(score)\n\n    return no_msd_res, msd_res","acef9ed5":"seed_everything(1111) \n\nTRAIN_SAMPLES = 5500\nUNITS = 128\nMSD_LAYERS = 5\n\nepochs = [20, 100]\nno_msd_res, msd_res = run_experiment(epochs)","c2023b95":"_, axes = plt.subplots(nrows = len(epochs), ncols = 2, figsize=(20, 16))\n\nfor i in range(len(epochs)):\n    axes[i,0].set_title(f'LOSS - EPOCH - {epochs[i]}')\n    axes[i,0].plot(msd_res['histories'][i].history['loss'], label='MSD train')\n    axes[i,0].plot(msd_res['histories'][i].history['val_loss'], label='MSD test')\n    axes[i,0].plot(no_msd_res['histories'][i].history['loss'], label='NO msd train')\n    axes[i,0].plot(no_msd_res['histories'][i].history['val_loss'], label='NO msd test')\n    axes[i,0].legend()\n\n    \n    msd = msd_res['scores'][i]\n    no_msd = no_msd_res['scores'][i]\n    axes[i,1].set_title(f'ACC - EPOCH - {epochs[i]}, MSD SCORE: {msd:.2f}, NO MSD SCORE: {no_msd:.2f}')\n    axes[i,1].plot(msd_res['histories'][i].history['acc'], label='MSD train')\n    axes[i,1].plot(msd_res['histories'][i].history['val_acc'], label='MSD test')\n    axes[i,1].plot(no_msd_res['histories'][i].history['acc'], label='NO msd train')\n    axes[i,1].plot(no_msd_res['histories'][i].history['val_acc'], label='NO msd test')\n    axes[i,1].legend()\n\nplt.show()","eec80541":"from mlxtend.plotting import plot_confusion_matrix\n\ny_pred = ensemble_predictions(no_msd_res['models'], X_train)\nfig, ax = plot_confusion_matrix(conf_mat=confusion_matrix(y_train, y_pred), show_absolute=True, show_normed=True, colorbar=True)\nax.set_title('NO MSD')\nplt.show()\n\ny_pred = ensemble_predictions(msd_res['models'], X_train)\nfig, ax = plot_confusion_matrix(conf_mat=confusion_matrix(y_train, y_pred), show_absolute=True, show_normed=True, colorbar=True)\nax.set_title('MSD')\nplt.show()","4466d71a":"## let's average both models and check the result\nall_models = []\nfor model in no_msd_res['models']:\n    all_models.append(model)\n\nfor model in msd_res['models']:\n    all_models.append(model)\n\ny_pred = ensemble_predictions(all_models, X_test)\n\nsample_sub = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nsub = pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pred})\nsub.to_csv('submission.csv',index=False)","1c960ca0":"sub['tweet'] = test['text']\nsub.head(25)","968cb5fe":"## basic cleanup","e7856eca":"## Preprocess text","f06be736":"## visualize the model layouts","af6a2084":"## getting class weights to balance target counts","6fc72b52":"Since overfitting was such an issue in my previous [notebook](https:\/\/www.kaggle.com\/vladlee\/disaster-tweets-glove-lstm-ensemble), I wanted to try new regularization technique which is also should accelerate training and improve generalization - Multi-Sample Dropout\n\nMulti-Sample Dropout for Accelerated Training and Better Generalization, [paper](https:\/\/arxiv.org\/pdf\/1905.09788.pdf)\n\nShout out to Marco Cerliani https:\/\/towardsdatascience.com\/multi-sample-dropout-in-keras-ea8b8a9bfd83\n\nMay be it's wrong dataset to try this technique (tell me, if so), I did not see a big difference. First dropout, right after the Embedding layer, has way bigger effect on overfitting.  Maybe on longer training intervals like in original paper, it would be more helpfull. ","b989d2f7":"## Submission","e9abe2f4":"## using GLOVE to create embedding vectors","e5a5debe":"## create full dictionary and vectorize"}}