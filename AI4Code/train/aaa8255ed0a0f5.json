{"cell_type":{"bab319cc":"code","24562893":"code","0dc1081f":"code","339590c4":"code","74e6d4ab":"code","72137963":"code","7be2bff2":"code","2a6acbbd":"code","e89833f7":"code","5806e57c":"code","2f1defd0":"code","5b591c5b":"code","4941cf02":"code","a871d9ff":"code","ebdd6379":"code","af6352d3":"code","cc81c940":"code","284e7d62":"code","11a51b94":"code","58dda4a7":"code","8dedb902":"code","c242fef5":"code","18453584":"code","3b9c6a05":"code","50a75af0":"code","08e3025e":"markdown","0722fc19":"markdown","62643e36":"markdown","3f850305":"markdown","e27622e7":"markdown","3de71291":"markdown","4f2eddf1":"markdown"},"source":{"bab319cc":"import os\nimport glob","24562893":"import numpy as np\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score","0dc1081f":"import torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader","339590c4":"UCD_SUB_IDS = np.arange(25)\nFILESET=['002','003','005','006','007','008','009','010','011','012',\n         '013','014','015','017','018','019','020','021','022','023',\n          '024','025','026','027','028']\nLABEL_LEN=[748, 882, 826, 808, 813, 768, 925, 907, 900, 864, 811, 774, 916, 789, 822, 852, 752, 913, 787, 861, 908, 711, 838, 893, 721]","74e6d4ab":"DATA_PATH = '\/kaggle\/input\/ucdprocessed'\nDATA_NAME = 'ucd'\nNUM_FOLDS = 25\nFOLD_ID = 4\nLABEL_INDEX = 2\nWINDOW_SIZE = 3\nBATCH_SIZE = 64\nLR = 1e-3\nEPOCHS = 50","72137963":"class TSNLSTM(nn.Module):\n    def __init__(self, seq_length=10, input_size=2560, n_units=128, dropout=0.5):\n        super(TSNLSTM, self).__init__()\n        self.seq_length = seq_length\n        self.n_units = n_units\n        self.rnn = nn.LSTM(input_size=input_size, hidden_size=n_units, num_layers=1, batch_first=True)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x):\n        x = x.view(-1, self.seq_length, 2560)  # batch first == True\n        _, (x, c) = self.rnn(x)\n        x = x.reshape(-1, self.n_units)\n        x = self.dropout(x)\n        return x","7be2bff2":"class TSNCNN(nn.Module):\n    def __init__(self, sampling_rate=100, dropout=0.5):\n        super(TSNCNN, self).__init__()\n        self.padding_edf = {  # same padding in tensorflow\n            'conv1': (22, 22),\n            'max_pool1': (2, 2),\n            'conv2': (3, 4),\n            'max_pool2': (0, 1),\n        }\n        first_filter_size = int(sampling_rate \/ 2.0)  # 100\/2 = 50, \u4e0e\u4ee5\u5f80\u4f7f\u7528\u7684Resnet\u76f8\u6bd4\uff0c\u8fd9\u91cc\u7684\u5377\u79ef\u6838\u66f4\u5927\n        first_filter_stride = int(sampling_rate \/ 16.0)  #  \u4e0e\u8bba\u6587\u4e0d\u540c\uff0c\u8bba\u6587\u7ed9\u51fa\u7684stride\u662f100\/4=25\n        self.cnn = nn.Sequential(\n            nn.ConstantPad1d(self.padding_edf['conv1'], 0),  # conv1\n            nn.Conv1d(in_channels=1, out_channels=128, kernel_size=first_filter_size, stride=first_filter_stride,\n                      bias=False),\n            nn.BatchNorm1d(num_features=128, eps=0.001, momentum=0.01),\n            nn.ReLU(inplace=True),\n            nn.ConstantPad1d(self.padding_edf['max_pool1'], 0),  # max p 1\n            nn.MaxPool1d(kernel_size=8, stride=8),\n            nn.Dropout(p=dropout),\n\n            nn.ConstantPad1d(self.padding_edf['conv2'], 0),  # conv2\n            nn.Conv1d(in_channels=128, out_channels=128, kernel_size=8, stride=1, bias=False),\n            nn.BatchNorm1d(num_features=128, eps=0.001, momentum=0.01),\n\n            nn.ReLU(inplace=True),\n            nn.ConstantPad1d(self.padding_edf['conv2'], 0),  # conv3\n            nn.Conv1d(in_channels=128, out_channels=128, kernel_size=8, stride=1, bias=False),\n            nn.BatchNorm1d(num_features=128, eps=0.001, momentum=0.01),\n\n            nn.ReLU(inplace=True),\n            nn.ConstantPad1d(self.padding_edf['conv2'], 0),  # conv4\n            nn.Conv1d(in_channels=128, out_channels=128, kernel_size=8, stride=1, bias=False),\n            nn.BatchNorm1d(num_features=128, eps=0.001, momentum=0.01),\n            nn.ReLU(inplace=True),\n            nn.ConstantPad1d(self.padding_edf['max_pool2'], 0),  # max p 2\n            nn.MaxPool1d(kernel_size=4, stride=4),\n            nn.Flatten(),\n            nn.Dropout(p=dropout)\n        )\n\n    def forward(self, x):\n        x = self.cnn(x)\n        return x","2a6acbbd":"class TsnCnnTsnLstm(nn.Module):\n    def __init__(self, sampling_rate=100, dropout=0.5):\n        super(TsnCnnTsnLstm, self).__init__()\n        self.cnn = TSNCNN()\n        self.rnn = TSNLSTM(seq_length=WINDOW_SIZE)\n        self.fc = nn.Linear(128, 1)\n        self.sigmoid = nn.Sigmoid()\n        #\u4e94\u5206\u7c7b\u662f5 \u4e8c\u5206\u7c7b\u662f1\n    def forward(self, x):\n        x = self.cnn(x)\n        x = self.rnn(x)\n        x = self.fc(x)\n#         x = self.sigmoid(x)\n        return x","e89833f7":"def get_subject_files(data_dir, dataset, files, sid):\n    #print(sid)\n    if \"mass\" in dataset:\n        reg_exp = f\".*-00{str(sid+1).zfill(2)} PSG.npz\"\n    elif \"sleepedf\" in dataset:\n        reg_exp = f\"S[C|T][4|7]{str(sid).zfill(2)}[a-zA-Z0-9]+\\.npz$\"\n    elif \"isruc\" in dataset:\n        reg_exp = f\"subject{sid+1}.npz\"\n    elif \"ucd\" in dataset:\n        subject_files = os.path.join(data_dir, 'ECG\/ucddb'+FILESET[sid]+'_V2.txt')\n        print(\"get subject files:\",subject_files)\n        return subject_files\n\n    else:\n        raise Exception(\"Invalid datasets.\")\n    # Get the subject files based on ID\n    subject_files = []\n    for i, f in enumerate(files):\n        pattern = re.compile(reg_exp)\n        if pattern.search(f):\n            subject_files.append(f)\n    return subject_files","5806e57c":"def split_train_test_files(data_dir, dataset, n_folds, fold_idx, validset=False, return_id=False):\n    subject_files = glob.glob(os.path.join(data_dir, \"*.txt\"))\n    print(\"\u83b7\u53d6\u6240\u6709\u7684subject files\uff1a\",subject_files)\n    #subject_files = glob.glob(os.path.join(data_dir, \"*.npz\"))\n    # Split training and test sets\n    if dataset == 'sleepedf':\n        sids = edf_sub_ids\n    elif dataset == 'sleepedfx':\n        sids = edfx_sub_ids\n    elif dataset == 'ucd':\n        sids = UCD_SUB_IDS\n\n    fold_pids = np.array_split(sids, n_folds)\n    test_sids = fold_pids[fold_idx]\n    train_sids = np.setdiff1d(sids, test_sids)\n    if not validset:\n\n        print(\"Train SIDs: ({}) {}\".format(len(train_sids), train_sids))\n        print(\"Test SIDs: ({}) {}\".format(len(test_sids), test_sids))\n        # Get corresponding files\n        train_files = np.hstack([get_subject_files(data_dir=data_dir, dataset=dataset, files=subject_files, sid=sid) for sid in train_sids])\n        test_files = np.hstack([get_subject_files(data_dir=data_dir, dataset=dataset, files=subject_files, sid=sid) for sid in test_sids])\n        if return_id:\n            return train_files, test_files, train_sids, test_sids\n        #print(\"split get \u540e\u7684train_files:\",train_files)\n        return train_files, test_files\n\n    else:\n        # Further split training set as validation set (10%)\n        n_valids = round(len(train_sids) * 0.10)\n        # Set random seed to control the randomness\n        np.random.seed(2021+fold_idx)\n        valid_sids = np.random.choice(train_sids, size=n_valids, replace=False)\n        train_sids = np.setdiff1d(train_sids, valid_sids)\n        print(\"Train SIDs: ({}) {}\".format(len(train_sids), train_sids))\n        print(\"Valid SIDs: ({}) {}\".format(len(valid_sids), valid_sids))\n        print(\"Test SIDs: ({}) {}\".format(len(test_sids), test_sids))\n        train_files = np.hstack(\n            [get_subject_files(data_dir=data_dir, dataset=dataset, files=subject_files, sid=sid) for sid in train_sids])\n        valid_files = np.hstack(\n            [get_subject_files(data_dir=data_dir, dataset=dataset, files=subject_files, sid=sid) for sid in valid_sids])\n        test_files = np.hstack([get_subject_files(data_dir=data_dir, dataset=dataset, files=subject_files, sid=sid) for sid in test_sids])\n        if return_id:\n            return train_files, valid_files,  test_files, train_sids, valid_sids, test_sids\n        return train_files, valid_files, test_files","2f1defd0":"class SleepDataset(Dataset):\n    def __init__(self, data_dir, file_list, window_size, label_idx, dataset, sequence=True, return_sid=False):\n        super(SleepDataset, self).__init__()\n        \n        self.data_dir = data_dir\n        self.window_size = window_size\n        self.label_idx = label_idx\n        self.return_sid = return_sid\n        self.ni_data_list, self.ni_label_list, self.sid_list = [], [], []  # ni_data_list \u4e2d\u7684\u5143\u7d20\u4e3a\u4e00\u665a\u7761\u7720\u6570\u636e N * C * 3000, torch.float\n        for file in tqdm(file_list):\n            #print(\"file:\",file)\n            '''\n            npz_data = np.load(file)\n            self.ni_data_list.append(torch.from_numpy(np.expand_dims(npz_data['x'], 1)).float())\n            print(\"self.data\u6bcf\u6b21append\u7684\u5185\u5bb9\uff1a\",torch.from_numpy(np.expand_dims(npz_data['x'], 1)).float().shape)\n            self.ni_label_list.append(torch.from_numpy(npz_data['y']).long())\n            print(\"self.label\u6bcf\u6b21append\u7684\u5185\u5bb9\uff1a\",torch.from_numpy(npz_data['y']).long().shape)\n            self.sid_list.append(torch.ones_like(self.ni_label_list[-1]).long() * int(os.path.basename(file)[3:5]))\n            #\u6bcf30s\u7684\u6837\u672c\u6240\u5728\u7684\u4e2a\u4f53id\n            '''\n            filename=os.path.basename(file)[5:8]\n            index=FILESET.index(filename)\n            #print(\"index:\",index)\n            #npz_data = np.load(file)\n            epoch,label=self.__get_epoch__(index)\n            self.ni_data_list.append(epoch.float())\n            #print(\"self.data\u6bcf\u6b21append\u7684\u5185\u5bb9\uff1a\",epoch.shape )\n            self.ni_label_list.append(label.long())\n            #print(\"self.label\u6bcf\u6b21append\u7684\u5185\u5bb9\uff1a\", label.shape)\n            self.sid_list.append(torch.ones_like((self.ni_label_list[-1]).long() * int(os.path.basename(file)[5:8])))\n\n\n\n        if sequence:\n            self.data, self.label, self.sid = self.__slide_window()\n        else:\n            self.data, self.label, self.sid = torch.cat(self.ni_data_list, dim=0), torch.cat(self.ni_label_list, dim=0), torch.cat(self.sid_list, dim=0)\n\n        #print(\"file_list:\",file_list)\n        #print(\"self.data:\",self.data.shape)\n        #print(\"self.label:\",self.label.shape)\n        #print(\"self.sid:\",self.sid.shape)\n\n\n    # todo \u6807\u51c6\u5316\u6570\u636e\n    # todo \u5206\u89e3\u6570\u636e\n\n\n    def __slide_window(self):\n        data, label, sid = [], [], []\n        for ni_data, ni_label, ni_sid in zip(self.ni_data_list, self.ni_label_list, self.sid_list):\n            data.append(ni_data.unfold(dimension=0, size=self.window_size, step=1))\n            label.append(ni_label.unfold(dimension=0, size=self.window_size, step=1)[:, self.label_idx])\n            sid.append(ni_sid.unfold(dimension=0, size=self.window_size, step=1)[:, self.label_idx])\n        #print(\"data shape:\",data[0].shape)\n        #print(\"label shape:\", label[0].shape)\n        #print(\"sid shape:\", sid[0].shape)\n        return torch.cat(data, dim=0).permute(0, 3, 1, 2), torch.cat(label, dim=0), torch.cat(sid, dim=0)\n\n\n    # def __augment(self, x, y):\n    #     return x, y\n\n\n    def __getitem__(self, index):\n        if self.return_sid:\n            return self.data[index], self.label[index], self.sid[index]\n        return self.data[index], self.label[index]\n\n    def __len__(self):\n        return self.data.shape[0]\n\n    def __get_epoch__(self,index):\n        fname = os.path.join(self.data_dir, 'ECG\/ucddb' + FILESET[index] + '_V2.txt')\n#         fname = ('\/data\/SongYudan\/Tinysleepnet\/data\/UCD\/ECG\/ucddb' + fileset[index] + '_V2.txt')\n#         f = open(fname)\n#         length = LABEL_LEN[index]\n#         line = f.readline()\n#         RRlist = []\n#         while line:\n#             RRlist.append(float(line))\n#             line = f.readline()\n            \n        with open(fname, 'r') as f:\n            lines = f.readlines()\n        RRlist = list(map(lambda x: float(x.strip('\\n')), lines))\n        length = LABEL_LEN[index]\n\n        epoch = []\n        for i in range(LABEL_LEN[index]):\n            epoch.append(RRlist[i * 128 * 30:(i + 1) * 128 * 30])\n        # \u6807\u7b7e\u4e0e\u547c\u5438\u4e8b\u4ef6\u7684\u5bf9\u5e94\u4fe1\u606f\uff1a0-\u65e0 1-H(O) 2-H(C) 3-H(M) 4-A(O) 5-A(C) 6-A(M) 7-POSSIBLE 8-PB ENENT\n        #0-0\uff1b1\u30012\u30013\u30014\u30015\u30016-1\uff1b7\u30018\u5220\u53bb\n        \n        fname2 = os.path.join(self.data_dir, 'apnea_label\/ucddb' + FILESET[index] + '_apnea.txt')\n#         fname2 = ('\/data\/SongYudan\/Tinysleepnet\/data\/UCD\/apnea_label\/ucddb' + fileset[index] + '_apnea.txt')\n        f2 = open(fname2)\n        line2 = f2.readline()\n        k = 0\n        label = [0 for i in range(length)]\n        while line2:\n\n            l = int(line2)\n\n            if (l > 0 and l < 7):\n                label[k] = 1\n            if (l == 0):\n                label[k] = 0\n            if(l==7 or l==8):\n                label[k] = l\n            k += 1\n            line2 = f2.readline()\n\n        delete_index = []\n        for i in range(length):\n            if (label[i] > 6):\n                delete_index.append(i)\n        new_delete_index=list(reversed(delete_index))\n        for i in new_delete_index:\n            del epoch[i]\n            del label[i]\n\n        epoch = np.array(epoch)\n        #epoch_reshape = epoch.reshape(length - len(delete_index), 128 * 30, 1)\n        epoch_reshape = epoch.reshape(length - len(delete_index), 1,128 * 30)\n        label = np.array(label)\n        epoch_final=torch.from_numpy(epoch_reshape)\n        label_final=torch.from_numpy(label)\n        \n        return epoch_final, label_final","5b591c5b":"train_files, test_files = split_train_test_files(DATA_PATH, DATA_NAME, NUM_FOLDS, FOLD_ID)","4941cf02":"print(train_files)","a871d9ff":"print(test_files)","ebdd6379":"train_dataset = SleepDataset(DATA_PATH, train_files, window_size=WINDOW_SIZE, label_idx=LABEL_INDEX, dataset=DATA_NAME)","af6352d3":"train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)","cc81c940":"model = TsnCnnTsnLstm().cuda()","284e7d62":"optimizer = optim.Adam(model.parameters(), lr=LR, betas=(0.9, 0.999), weight_decay=1e-3)\ncriterion = nn.BCEWithLogitsLoss().cuda()","11a51b94":"for epoch in range(EPOCHS):\n    model.train()\n    \n    losses = []\n    with tqdm(train_loader, desc=f'EPOCH [{epoch+1}\/{EPOCHS}]') as loader:\n        for train_x, train_y in loader:\n            train_x, train_y = train_x.cuda(non_blocking=True), train_y.cuda(non_blocking=True)\n            train_y_hat0 = model(train_x.view(-1, train_x.shape[2], train_x.shape[3]))\n            train_y_hat = train_y_hat0.view(-1)\n            \n            train_y = train_y.float()\n            loss = criterion(train_y_hat, train_y)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), max_norm=5, norm_type=2)\n            optimizer.step()\n            \n            losses.append(loss.item())\n            \n            loader.set_postfix({'Loss': np.mean(losses)})","58dda4a7":"test_dataset = SleepDataset(DATA_PATH, test_files, window_size=WINDOW_SIZE, label_idx=LABEL_INDEX, dataset=DATA_NAME)","8dedb902":"test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)","c242fef5":"model.eval()\n\npreds, truths = [], []\n\nfor test_x, test_y in tqdm(test_loader):\n    with torch.no_grad():\n        test_x = test_x.cuda(non_blocking=True)\n        test_y_hat0 = model(test_x.view(-1, test_x.shape[2], test_x.shape[3]))\n        test_y_hat = test_y_hat0.view(-1)\n        test_y_hat = torch.sigmoid(test_y_hat)\n        \n        test_y_hat = test_y_hat.cpu().numpy()\n        y_out = np.zeros_like(test_y_hat)\n        y_out[test_y_hat > 0.5] = 1\n    \n    preds.append(y_out)\n    truths.append(test_y.numpy())","18453584":"preds = np.concatenate(preds)\ntruths = np.concatenate(truths)","3b9c6a05":"acc = accuracy_score(truths, preds)\nf1 = f1_score(truths, preds, average='macro')","50a75af0":"print(acc, f1)","08e3025e":"# Backbones","0722fc19":"# Training","62643e36":"# Parameters","3f850305":"# Dataset","e27622e7":"# Model","3de71291":"# Evaluation","4f2eddf1":"# Packages"}}