{"cell_type":{"304bcde1":"code","83906362":"code","6cdb97ca":"code","d3be8eb6":"code","344208f3":"code","1fffdf40":"code","73d9cb95":"code","c7a5fce6":"code","1cc8456e":"markdown","a952f867":"markdown","a616d72b":"markdown","df9d5bcc":"markdown","40bbf5ef":"markdown","37ab05d5":"markdown","dd351622":"markdown","a69b2697":"markdown","42f08a24":"markdown"},"source":{"304bcde1":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.optimizers import SGD\n\n# Generate dummy data\nimport numpy as np\nimport pandas as pd\n\nX_train = np.random.random((1000, 3))\ny_train = pd.get_dummies(np.argmax(X_train[:, :3], axis=1)).values\nX_test = np.random.random((100, 3))\ny_test = pd.get_dummies(np.argmax(X_test[:, :3], axis=1)).values\n\n\n# Reusable model fit wrapper.\ndef epocher(batch_size=500, epochs=10, callbacks=None):\n    # Build the model.\n    clf = Sequential()\n    clf.add(Dense(9, activation='relu', input_dim=3))\n    clf.add(Dense(9, activation='relu'))\n    clf.add(Dense(3, activation='softmax'))\n    clf.compile(loss='categorical_crossentropy', optimizer=SGD())\n\n    # Perform training.\n    clf.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=[callbacks])\n    return clf","83906362":"from keras.callbacks import LambdaCallback\n\nhistory = []\nweight_history = LambdaCallback(on_epoch_end=lambda batch, logs: history.append((batch, logs)))\nclf = epocher(callbacks=weight_history)","6cdb97ca":"clf.history.history","d3be8eb6":"from keras.callbacks import Callback\n\nclass WeightHistory(Callback):\n    def __init__(self):\n        self.tape = []\n        \n    def on_epoch_end(self, batch, logs={}):\n        self.tape.append(self.model.get_weights()[0])\n\nwh = WeightHistory()\nclf = epocher(callbacks=wh)","344208f3":"wh.tape[0]","1fffdf40":"keras.callbacks.TerminateOnNaN","73d9cb95":"clf.get_config()","c7a5fce6":"my_config = clf.get_config()\nSequential.from_config(my_config)","1cc8456e":"You can actually get the same information a bit more conveniently by running `clf.history.history`:","a952f867":"To go the other way, and load a model from a config file:","a616d72b":"For non-trivial applications, classed callbacks are the way to go. We can even use them to perform updates to the model parameters inline: a feature we will demonstrate in the next notebook.\n\nThe last set of callbacks are precomputed ones. These are preexisting recipes that are built into Keras, that fit a handful of useful cases that occur often enough that templates are useful. For example, there's a checkpointing callback, and a stop-on-Nan callback. To get familiar with these, [browse the Keras documentation](https:\/\/keras.io\/callbacks\/).","df9d5bcc":"## Demonstration\n\nFirst, a simple reusable neural network model wrapper.","40bbf5ef":"# Keras callbacks and config files\n\n## Introduction to callbacks\n\nKeras callbacks allow user-defined methods to be invoked at certain predetermined points in the model fitting process. This functional pattern is known as the **callback** pattern. Specifically, you can define and execute methods during any of the following points, ordered by granularity:\n\n* `on_epoch_begin`\n* `on_epoch_end`\n* `on_batch_begin`\n* `on_batch_end`\n* `on_train_begin`\n* `on_train_end`\n\nThese callbacks can be passed to a model during `fit` time. They allow us to modify the classifier as it is run, or to log and retain historical information about the model, or to do both.\n\nThere are basically three sets of callbacks. Lambda callbacks is a quick-and-dirty callback structure useful for small things. Name callbacks allow you to implement meatier modifications on your model. Finally, a significant number of precomputed callbacks exist, which express a handful of particularly common and useful recipes, like checkpointing, Tensorboard integration, and other similar things.","37ab05d5":"The `LambdaCallback` allows invoking an anonymous function over the course of classifier training. By default the callback is provided with a counter value and a log value. The counter keeps track of which iteration of the training process we are at. If the callback is defined on batches this is the batch number, on epochs this is the epoch number, and on trainings this is the forward plus backwards pass number. The logs contains a running...log...of values that specified as being under observation. At a minimum it will contain the loss values at the callback runtime.\n\nHere is a minimal example showing how we can use this callback to historify that information:","dd351622":"If we look at our objet now we can see the history that we retained!","a69b2697":"## Config files\n\nKeras models can be serialized to JSON files using the `get_config()` function.","42f08a24":"As you might notice from the akward convention, however, this seems like a slightly more private API. In practice I would rely on using the callbacks, which are an accepted approach, and treating history-dot-history as an impermanent implementation detail.\n\nNext, here's an implementation of a classed `Callback`. The following code cell demonstrates a recipe for recording a tape of weights over time on the model."}}