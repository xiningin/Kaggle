{"cell_type":{"a35b4d05":"code","d0df3021":"code","d9b84f3a":"code","72567439":"code","0c4906de":"code","f1926c6a":"code","2ba949e4":"code","0a57bdd9":"code","186f24f9":"code","fe472790":"code","f136e82d":"code","bc6e69fa":"code","484fb8e4":"code","6627e00d":"code","04e14b4a":"code","b50b985e":"code","6e80a9c9":"code","ed2e1994":"code","f68e1082":"code","7d1efe0d":"code","24cf0ea4":"code","a0d8adb5":"code","d6302743":"code","c4d02c13":"code","c3192121":"code","7437465c":"code","62d9902f":"code","57c3d561":"code","bec4b815":"code","5a4eaea3":"code","9d1028ff":"code","964ce4ef":"code","38003156":"code","03a7cf3e":"code","4654f930":"markdown","0faa5e2d":"markdown","e9f6638a":"markdown","c379f8bd":"markdown","9b0c649d":"markdown","173b50f6":"markdown","e1831ff1":"markdown","f6197fee":"markdown","572fa551":"markdown","10cba470":"markdown","2c635283":"markdown","f6eb7e09":"markdown","521f822d":"markdown","6ddbc6ba":"markdown","93c32366":"markdown","7fc934cf":"markdown","f18d2383":"markdown","876c637f":"markdown","4bf5a73e":"markdown","609db564":"markdown","62a2cbdb":"markdown","9f3ba9bd":"markdown"},"source":{"a35b4d05":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d0df3021":"data = pd.read_csv('..\/input\/Churn_Modelling.csv')","d9b84f3a":"data.head()","72567439":"data.info()","0c4906de":"data.drop(['RowNumber', 'CustomerId', 'Surname', 'Geography'], axis=1, inplace=True)","f1926c6a":"data.Gender = [1 if each == 'Male' else 0 for each in data.Gender]","2ba949e4":"data.sample(5)","0a57bdd9":"plt.figure(figsize=[5,5])\nsns.set(style='darkgrid')\nax = sns.countplot(x='Exited', data=data, palette='Set3')\ndata.loc[:,'Exited'].value_counts()","186f24f9":"y = data.Exited.values\nx_data = data.drop(['Exited'], axis=1)","fe472790":"x_data.describe()","f136e82d":"x = (x_data - np.min(x_data)) \/ (np.max(x_data)-np.min(x_data))\nx.head()","bc6e69fa":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.10, random_state=7)","484fb8e4":"print('x_train shape: ', x_train.shape)\nprint('y_train shape: ', y_train.shape)\nprint('x_test shape: ', x_test.shape)\nprint('y_test shape: ', y_test.shape)","6627e00d":"from sklearn.linear_model import LogisticRegression\n\n# Defining the model\nlr = LogisticRegression()\n\n# Training the model:\nlr.fit(x_train, y_train)\n\n# Predicting target values by using x_test and our model:\ny_pred0 = lr.predict(x_test)","04e14b4a":"# Confusion matrix for visulalization of our prediction accuracy:\nfrom sklearn.metrics import confusion_matrix\n\n# Creating the confusion matrix:\nlr_cm = confusion_matrix(y_test, y_pred0)\n\n#Visualization:\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(lr_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='.0f', ax=ax, cmap='BrBG')\nplt.title('Logistic Regression Classification Confusion Matrix')\nplt.xlabel('y_pred')\nplt.ylabel('y_test')\nplt.show()","b50b985e":"score_lr = lr.score(x_test, y_test)\nprint(score_lr)","6e80a9c9":"from sklearn.neighbors import KNeighborsClassifier\n\n# Defining the model with a k number of 13:\nknn = KNeighborsClassifier(n_neighbors=13)\n\n# Training the model:\nknn.fit(x_train, y_train)\n\n# Predicting target values by using x_test and our model:\ny_pred1 = knn.predict(x_test)","ed2e1994":"# Confusion matrix for visualization our prediction accuracy:\nfrom sklearn.metrics import confusion_matrix\n\n# Creating the confusion matrix:\nknn_cm = confusion_matrix(y_test, y_pred1)\n\n# Visualization:\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(knn_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='.0f', ax=ax, cmap='BrBG')\nplt.title('KNN Classification Confusion Matrix')\nplt.xlabel('y_pred')\nplt.ylabel('y_test')\nplt.show()","f68e1082":"score_knn = knn.score(x_test, y_test)\nprint(score_knn)","7d1efe0d":"from sklearn.svm import SVC\n\n# Defining SVM model\nsvm = SVC(random_state=2)\n\n# Training model:\nsvm.fit(x_train, y_train)\n\n# Predicting target values by using x_test and our model:\ny_pred2 = svm.predict(x_test)","24cf0ea4":"# Confusion matrix for visualization our prediction accuracy:\nfrom sklearn.metrics import confusion_matrix\n\n# Creating the confusion matrix:\nsvm_cm = confusion_matrix(y_test, y_pred2)\n\n# Visualization:\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(svm_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='.0f', ax=ax, cmap='BrBG')\nplt.title('SVM Classification Confusion Matrix')\nplt.xlabel('y_pred')\nplt.ylabel('y_test')\nplt.show()","a0d8adb5":"score_svm = svm.score(x_test, y_test)\nprint(score_svm)","d6302743":"from sklearn.naive_bayes import GaussianNB\n\n# Defining model:\nnb = GaussianNB()\n\n# Training the model:\nnb.fit(x_train, y_train)\n\n# Predicting:\ny_pred3 = nb.predict(x_test)","c4d02c13":"# Confusion matrix for visualization our prediction accuracy:\nfrom sklearn.metrics import confusion_matrix\n\n# Creating the confusion matrix:\nnb_cm = confusion_matrix(y_test, y_pred3)\n\n# Visualization:\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(nb_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='.0f', ax=ax, cmap='BrBG')\nplt.title('Naive Bayes Classification Confusion Matrix')\nplt.xlabel('y_pred')\nplt.ylabel('y_test')\nplt.show()","c3192121":"score_nb = nb.score(x_test, y_test)\nprint(score_nb)","7437465c":"from sklearn.tree import DecisionTreeClassifier\n\n# Defining the model:\ndt = DecisionTreeClassifier()\n\n# Training:\ndt.fit(x_train, y_train)\n\n# Predicting:\ny_pred4 = dt.predict(x_test)","62d9902f":"# Confusion matrix for visualization our prediction accuracy:\nfrom sklearn.metrics import confusion_matrix\n\n# Creating the confusion matrix:\ndt_cm = confusion_matrix(y_test, y_pred4)\n\n# Visualization:\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(dt_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='.0f', ax=ax, cmap='BrBG')\nplt.title('Decision Tree Classification Confusion Matrix')\nplt.xlabel('y_pred')\nplt.ylabel('y_test')\nplt.show()","57c3d561":"score_dt = dt.score(x_test, y_test)\nprint(score_dt)","bec4b815":"from sklearn.ensemble import RandomForestClassifier\n\n# Defining:\nrf = RandomForestClassifier(n_estimators=100, random_state=3)\n\n# Training:\nrf.fit(x_train, y_train)\n\n# Predicting:\ny_pred5 = rf.predict(x_test)","5a4eaea3":"# Confusion matrix for visualization our prediction accuracy:\nfrom sklearn.metrics import confusion_matrix\n\n# Creating the confusion matrix:\nrf_cm = confusion_matrix(y_test, y_pred5)\n\n# Visualization:\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(rf_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='.0f', ax=ax, cmap='BrBG')\nplt.title('Random Forest Classification Confusion Matrix')\nplt.xlabel('y_pred')\nplt.ylabel('y_test')\nplt.show()","9d1028ff":"score_rf = rf.score(x_test, y_test)\nprint(score_rf)","964ce4ef":"data_scores = pd.Series([score_lr, score_knn, score_svm, score_nb, score_dt, score_rf], \n                        index=['logistic_regression_score', 'knn_score', 'svm_score', 'naive_bayes_score', 'decision_tree_score', 'random_forest_score']) \ndata_scores","38003156":"d = {'y_test': y_test, 'log_reg_pred': y_pred0,'knn_prediction': y_pred1, \n     'svm_prediction': y_pred2, 'naive_bayes_prediction': y_pred3, \n     'decision_tree_prediction': y_pred4, 'random_forest_prediction': y_pred5}\ndata01 = pd.DataFrame(data=d)\ndata01.T","03a7cf3e":"fig = plt.figure(figsize=(15,15))\n\nax1 = fig.add_subplot(3, 3, 1) # row, column, position\nax1.set_title('Logistic Regression Classification')\n\nax2 = fig.add_subplot(3, 3, 2) # row, column, position\nax2.set_title('KNN Classification')\n\nax3 = fig.add_subplot(3, 3, 3)\nax3.set_title('SVM Classification')\n\nax4 = fig.add_subplot(3, 3, 4)\nax4.set_title('Naive Bayes Classification')\n\nax5 = fig.add_subplot(3, 3, 5)\nax5.set_title('Decision Tree Classification')\n\nax6 = fig.add_subplot(3, 3, 6)\nax6.set_title('Random Forest Classification')\n\nsns.heatmap(data=lr_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='.0f', ax=ax1, cmap='BrBG')\nsns.heatmap(data=knn_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='.0f', ax=ax2, cmap='BrBG')   \nsns.heatmap(data=svm_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='.0f', ax=ax3, cmap='BrBG')\nsns.heatmap(data=nb_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='.0f', ax=ax4, cmap='BrBG')\nsns.heatmap(data=dt_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='.0f', ax=ax5, cmap='BrBG')\nsns.heatmap(data=rf_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='.0f', ax=ax6, cmap='BrBG')\nplt.show()","4654f930":"Now it's time to compare our models. \n\nFirst of all compare the accuracies:","0faa5e2d":"**11. COMPARISON OF ALGORITHMS:**","e9f6638a":"**Question: ** How many of the customers left the bank?","c379f8bd":"**8. NAIVE BAYES ALGORITHM:**","9b0c649d":"I don't need some of columns for my analysis, like customer ID, surname, etc.","173b50f6":"**4. SPLITTING DATA FOR TRAINING AND TESTING**\n\nI am going to split my data set into as train (x_train, y_train) and test (x_test, y_test) datas.\n\nThen I am going to teach my machine learning algorithms by using trainig data set.\n\nLater I will use my trained model to predict my test data (y_pred).\n\nFinally I will compare my predictions (y_pred) with my test data (y_test).","e1831ff1":"**5. LOGISTIC REGRESSION CLASSIFICATION:**","f6197fee":"**6. KNN CLASSIFICATION ALGORITHM:**","572fa551":"Now separete target feature (y) from other features (x_data).","10cba470":"**SKLEARN LIBRARY CLASSIFICATION ALGORITHMS COMPARISON**","2c635283":"I will change customers' gender from Male\/Female to 1\/0.","f6eb7e09":"**7. SUPPORT VECTOR MACHINE (SVM) ALGORITHM:**\n\nI will use the same x_train and y_train data sets to teach a SVM model. ","521f822d":"From the accuracy comparison I can see random forest classification gave the best result.\n\nNow I want to see y_test and my models' y_pred values manually:","6ddbc6ba":"**2. SEPARETING FEATURES AND TARGET**","93c32366":"I can see the x_data features has a large scale of numbers. So I need to normalize all the features between 0 and 1.","7fc934cf":"**1. EXPLORATORY DATA ANALYSIS (EDA)**\n\nFirst of all I need to understand and prepare my data set.","f18d2383":"**3. NORMALIZATION PROCESS**","876c637f":"**10. RANDOM FOREST CLASSIFICATION ALGORITHM:**","4bf5a73e":"Now I can start inplying machine learning algorithms one by one.","609db564":"Finally I want to show my models' confusion matrixes side by side:","62a2cbdb":"**9. DECISION TREE ALGORITHM:**","9f3ba9bd":"The data set I used in this kernel contains details of a bank's customers. \n\nThe target variable is a binary variable reflecting the fact whether the customer left the bank (closed his account) or s\/he continues to be a customer.\n\nIn order to predict target variables, I will use sklearn classification algorithms:\n* Logistic Regression Classification\n* K-Nearest Neighbour (KNN) Classification\n* Support Vector Machine (SVM) Classification\n* Naive Bayes Classification \n* Decision Tree Classification\n* Random Forest Classification\n\nAfter implementing each model, I used score and confusion matrix methods to compare models."}}