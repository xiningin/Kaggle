{"cell_type":{"62f3aca8":"code","b72b9c8d":"code","754c5a16":"code","ff443eff":"code","5dbd81c6":"code","42e5e25d":"code","636f69ff":"code","13ce9912":"code","8c503a65":"code","6320d85f":"code","6b8d789b":"code","9fd111a6":"code","2f261b9c":"code","62ce3b0d":"code","c83ca7fe":"code","7a725065":"code","66bdfded":"code","152a8ed1":"code","0064d5a4":"code","fc70d11f":"code","51d34cca":"code","7a5ee70f":"code","9fe7e847":"code","8d489da6":"code","54d1d3eb":"code","95a933e0":"code","63e41db4":"code","ff763b00":"code","a42af489":"code","468f0f3e":"code","deaf2127":"code","47eb4619":"code","0b017c9f":"code","d8ca23d8":"code","107fb8e5":"code","2a39fb97":"code","9ba928a7":"code","085d557b":"code","d892bc8a":"code","658972af":"code","7e99e7d6":"code","30d8ab05":"code","865d6d39":"code","6b52c806":"code","3545473f":"code","06ede67c":"code","e6ece248":"code","4c25a0b5":"code","4e6592a7":"code","8e4c0ef7":"code","592c6f38":"code","b34b4a03":"code","b4b18772":"code","70617779":"code","fbc7c169":"code","a61bd8ee":"code","e16b5136":"code","8cc71416":"code","a8514e1f":"code","a87910b1":"code","34a07726":"code","193e5fd8":"code","b7f192a2":"code","c808c946":"code","bf0efa14":"code","4c931918":"code","099465a1":"code","5bc74872":"code","1897075d":"code","d6937b03":"markdown"},"source":{"62f3aca8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n#import numpy as np # linear algebra\n#import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n#import os\n#print(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b72b9c8d":"# coding:utf8","754c5a16":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas_profiling as pdpr\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler # \u5f52\u4e00\u5316 \u6807\u51c6\u5316\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder # \u6807\u7b7e\u7f16\u7801 \u72ec\u70ed\u7f16\u7801\nfrom sklearn.preprocessing import Normalizer # \u6b63\u89c4\u5316(l1,l2)\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis # LDA \u964d\u7ef4\nfrom sklearn.decomposition import PCA\n%matplotlib inline","ff443eff":"df = pd.read_csv(\"..\/input\/HR_comma_sep.csv\")","5dbd81c6":"df.head()","42e5e25d":"pdpro = pdpr.ProfileReport(df)","636f69ff":"pdpro","13ce9912":"plt.title('SALARY')\nplt.xlabel('salary')\nplt.ylabel('number')\nplt.axis([0,4,0,10000])\nplt.xticks(np.arange(len(df['salary'].value_counts()))+0.5,df['salary'].value_counts().index)\nplt.bar(np.arange(len(df['salary'].value_counts()))+0.5,df['salary'].value_counts(),width=0.3,color = 'bry')\nfor x,y in zip(np.arange(len(df['salary'].value_counts()))+0.5,df['salary'].value_counts()):\n    plt.text(x,y,y,ha='center',va = 'bottom')","8c503a65":"sns.set_style(style='whitegrid')  #darkgrid, whitegrid, dark, white, ticks\nsns.set_context(context='paper',font_scale=1) # context : dict, None, or one of {paper, notebook, talk, poster}\nsns.set_palette(sns.color_palette(\"RdBu\"))\nsns.countplot(x = 'salary',hue = 'sales',data = df) \n# sns.countplot(x=None, y=None, hue=None, data=None, order=None, hue_order=None, orient=None, color=None, palette=None, saturation=0.75, dodge=True, ax=None, **kwargs\n\n# plt.title('SALARY')\n# plt.xlabel('salary')\n#plt.ylabel('number')\n#plt.axis([0,4,0,10000])\n# plt.xticks(np.arange(len(df['salary'].value_counts()))+0.5,df['salary'].value_counts().index)\n# plt.bar(np.arange(len(df['salary'].value_counts()))+0.5,df['salary'].value_counts(),width=0.3)\n# for x,y in zip(np.arange(len(df['salary'].value_counts()))+0.5,df['salary'].value_counts()):\n#   plt.text(x,y,y,ha='center',va = 'bottom')","6320d85f":"# fig = plt.figure()\n# ax1 = fig.add_subplot(2,2,1)\n# ax2 = fig.add_subplot(2,2,2)\n# ax3 = fig.add_subplot(2,2,3)\n","6b8d789b":"df.head(1)","9fd111a6":"fig = plt.figure()\nax1 = fig.add_subplot(2,2,1)\nsns.distplot(df['satisfaction_level'],bins = 10)\nax2 = fig.add_subplot(2,2,2)\nsns.distplot(df['last_evaluation'],bins = 10)\nax3 = fig.add_subplot(2,2,3)\nsns.distplot(df['number_project'],bins = np.arange(df['number_project'].min(),df['number_project'].max(),1),hist = False)\nplt.subplots_adjust(right = 1.2,top = 1.2)\nax4 = fig.add_subplot(2,2,4)\nax4.set_ylabel('eeee')\nsns.distplot(df['average_montly_hours'],bins = 10) #np.arange(df['average_montly_hours'].min(),df['average_montly_hours'].max()+20,20) ,\n             # kde = False,axlabel=('Hours'))\n","2f261b9c":"sns.boxplot(y = df['time_spend_company'],  saturation=0.75,  whis=3)\n","62ce3b0d":"sub_df = df.groupby('time_spend_company').mean()\nsub_df","c83ca7fe":"sns.pointplot(sub_df.index,sub_df['left'])","7a725065":"sns.pointplot(x = 'time_spend_company',y = 'left',data = df)","66bdfded":"lbs=df['sales'].value_counts().index\nexplodes = [0.1 if i == 'sales' else 0 for i in lbs]\nplt.pie(df['sales'].value_counts(normalize= True),labels =lbs,autopct='%1.1f%%',explode=explodes)","152a8ed1":"lbs=df['salary'].value_counts().index\nexplodes = [0.1 if i == 'low' else 0 for i in lbs]\nplt.pie(df['salary'].value_counts(normalize= True),labels =lbs,autopct='%1.1f%%',explode=explodes)","0064d5a4":"#import math ","fc70d11f":"#import scipy.stats as ss#","51d34cca":"# norm_dist = ss.norm.rvs(size =20)","7a5ee70f":"# norm_dist","9fe7e847":"# ss.normaltest(norm_dist) # \u57fa\u4e8e\u504f\u5ea6\u548c\u5cf0\u5ea6\u7684\u68c0\u9a8c\u6cd5  \u8fd4\u56de\u7edf\u8ba1\u503c\u548cP\u503c  \u68c0\u9a8c\u662f\u5426\u6b63\u592a\u5206\u5e03","8d489da6":"# ss.chi2_contingency([[15,95],[85,5]]) # \u5361\u65b9\u68c0\u9a8c","54d1d3eb":"# ss.ttest_ind(ss.norm.rvs(size = 100),ss.norm.rvs(size = 100))","95a933e0":"# ss.f_oneway([49,50,52,54,56],[56,54,52,51,48],[54,51,52,48,47]) # \u65b9\u5dee\u68c0\u9a8c","63e41db4":"#from statsmodels.graphics.api import qqplot\n# matplotlib","ff763b00":"# plt.show(qqplot(ss.norm.rvs(size = 100)))  # qq\u56fe\u68c0\u9a8c\u5206\u5e03\u662f\u5426\u6b63\u592a\u5206\u5e03","a42af489":"# pandas \u76f8\u5173\u7cfb\u6570\n# s1 = pd.Series(np.random.randn(10))\n# s2 = pd.Series(np.random.randn(10))","468f0f3e":"# s1.corr(s2)","deaf2127":"# df = pd.DataFrame(np.random.randn(4,14))","47eb4619":"# s1.corr(s2,method='spearman')","0b017c9f":"# s1.corr(s2)","d8ca23d8":"# s1.corr(s2,'kendall')","107fb8e5":"# df =pd.DataFrame([s1,s2]).T","2a39fb97":"# df.corr()","9ba928a7":"# df","085d557b":"# x =np.arange(10).astype(np.float).reshape((10,1))\n# y = x*3+4+np.random.random((10,1))","d892bc8a":"# from sklearn.linear_model import LinearRegression # \u7ebf\u6027\u56de\u5f52","658972af":"# reg = LinearRegression() ###\u7ebf\u6027\u56de\u5f52\n# res = reg.fit(x,y)# \u62df\u5408","7e99e7d6":"# y_pred = reg.predict(x) # \u9884\u6d4b\u503c","30d8ab05":"# y_pred","865d6d39":"# reg.coef_  # \u53c2\u6570","6b52c806":"# reg.intercept_  # \u622a\u8ddd","3545473f":"# PCA\u53d8\u6362 \u4e3b\u6210\u5206\u5206\u6790\n","06ede67c":"#from sklearn.decomposition import  PCA ","e6ece248":"# lower_dim = PCA(n_components=1) # \u7528\u7684\u662f\u5947\u5f02\u503c\u5206\u89e3\u7684\u65b9\u6cd5\u964d\u7ef4","4c25a0b5":"# lower_dim.fit(data)","4e6592a7":"# lower_dim.explained_variance_ratio_","8e4c0ef7":"# lower_dim.fit_transform(data)","592c6f38":"#\u4e3b\u6210\u5206\u5206\u6790PCA\n\n# import pandas as pd\n# import numpy as np\n# def myPCA(data,n_components = 10000000000):\n#     mean_vals = np.mean(data,axis = 0) # \u6bcf\u4e2a\u5c5e\u6027\u7684\u5747\u503c \u9488\u5bf9\u5217\n#     mid = data - mean_vals # \n#     cov_mat = np.cov(mid,rowvar = False) # \u8ba1\u7b97\u534f\u65b9\u5dee\u77e9\u9635 rowvar = False \u9488\u5bf9\u5217\n#     from scipy import linalg # \u6c42\u534f\u65b9\u5dee\u77e9\u9635\u7684\u7279\u5f81\u503c\u548c\u7279\u5f81\u5411\u91cf\n#     eig_vals,eig_vects = linalg.eig(np.mat(cov_mat)) #eig:\u8ba1\u7b97\u65b9\u9635\u7684\u7279\u5f81\u503c\u548c\u7279\u5f81\u5411\u91cf mat:\u5c06\u8f93\u5165\u89e3\u91ca\u4e3a\u77e9\u9635\u3002\n#     eig_val_index = np.argsort(eig_vals) # \u8fd4\u56de\u5c06\u5bf9\u6570\u7ec4\u6392\u5e8f\u7684\u7d22\u5f15 \u4e0b\u6807\n#     eig_val_index =(eig_val_index[:-(n_components + 1):-1])\n#     eig_vects = eig_vects[:,eig_val_index]\n#     low_dim_mat = np.dot(mid,eig_vects)\n#     return low_dim_mat,eig_vals\n# data = np.array([np.array([2.5,1.5,2.2,1.9,3.1,2,3.2,1,1.5,1.1]),np.array([2.4,0.7,2.9,2.2,3,2.7,1.6,1.1,1.6,0.9])]).T\n# print(myPCA(data,n_components=1))","b34b4a03":"# #\u4ea4\u53c9\u5206\u6790\n# pandas\n# numpy\n# %matplotlib\n# senborn\n# scipy.stats\n","b4b18772":"# sales_indices = df.groupby(by = 'sales').indices\n# sales_values = df['left'].iloc[sales_indices['sales']].values\n# technical_values = df['left'].iloc[sales_indices['technical']].values\n# print(ss.ttest_ind(sales_values,technical_values)[1])\n# sales_keys = list(sales_indices.keys())\n# sales_t_mat =np.zeros([len(sales_keys),len(sales_keys)])\n# for i in range(len(sales_keys)):\n#     for j in range(len(sales_keys)):\n#         p_value = ss.ttest_ind(df['left'].iloc[sales_indices[sales_keys[i]]].values,df['left'].iloc[sales_indices[sales_keys[j]]].values)[1]\n#         if p_value < 0.05:\n#             sales_t_mat[i][j] = -1\n#         else:\n#             sales_t_mat[i][j] = p_value\n# sns.heatmap(sales_t_mat,xticklabels = sales_keys,yticklabels = sales_keys)","70617779":"# \u900f\u89c6\u8868\npiv_tb = pd.pivot_table(df,values = 'left',index = ['promotion_last_5years','salary'],\\\n                       columns = ['Work_accident'],aggfunc = np.mean)  #aggfunc \u805a\u5408\u65b9\u5f0f \u53c2\u6570 \u4e3a\u4e00\u4e2a\u51fd\u6570\n\nsns.heatmap(piv_tb,vmin = 0, vmax = 1,cmap = sns.color_palette(n_colors = 6)) # cmap \u989c\u8272","fbc7c169":"# \u5206\u7ec4\u5206\u6790 \n# 1 \u79bb\u6563\u503c\nsns.barplot(x = 'salary',y = 'left',hue = 'sales',data =df)\n# 2 \u8fde\u7eed\u503c\n","a61bd8ee":"# sns.barplot(list(range(len(df['satisfaction_level']))),df['satisfaction_level'].sort_values())","e16b5136":"# \u76f8\u5173\u7cfb\u6570\n# sns.heatmap(df.corr(),vmin=-1, vmax=1, cmap=sns.color_palette())","8cc71416":"df.head()","a8514e1f":"# import numpy as np\n# import pandas as pd\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n# import pandas_profiling as pdpr\n# from sklearn.preprocessing import MinMaxScaler, StandardScaler  # \u5f52\u4e00\u5316 \u6807\u51c6\u5316\n# from sklearn.preprocessing import LabelEncoder, OneHotEncoder  # \u6807\u7b7e\u7f16\u7801 \u72ec\u70ed\u7f16\u7801\n# from sklearn.preprocessing import Normalizer # \u6b63\u89c4\u5316(l1,l2)\n# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis # LDA \u964d\u7ef4\n\n\n# s1  : satisfaction_level     --  False:MinMaxScaler, True:StandardScaler\n# le  : last_evaluation        --  False:MinMaxScaler, True:StandardScaler\n# npr : number_project         --  False:MinMaxScaler, True:StandardScaler\n# amh : average_montly_hours   --  False:MinMaxScaler, True:StandardScaler\n# tsc : time_spend_company     --  False:MinMaxScaler, True:StandardScaler\n# wa  : Work_accident          --  False:MinMaxScaler, True:StandardScaler\n# pl5 : promotion_last_5years  --  False:MinMaxScaler, True:StandardScaler\n# dep : departmeent  --- False:LableEncoder ,  True:OneHotEncoder\n# salary : salary --- False:LableEncoder ,  True:OneHotEncoder\n\n\ndef hr_preprocessing(sl=False, le=False, npr=False, amh=False, tsc=False, wa=False, pl5=False, dep=False, sal=False,low_d=False,low_n=1):\n    df = pd.read_csv(\"..\/input\/HR_comma_sep.csv\")\n    df.rename(columns={'sales': 'department', 'average_montly_hours': 'average_monthly_hours'}, inplace=True)\n    # 1,\u5f97\u5230\u6807\u6ce8\n    label = df['left']\n    df = df.drop('left', axis=1)\n    # \u6e05\u6d17\u6570\u636e\n    # df = df.dropna(subset = ['satisfaction_level','last_evaluation'])\n    # df = df[df[]]\n    # \u7279\u5f81\u9009\u62e9\n    # \u7279\u5f81\u5904\u7406\n    scaler_lst = [sl, le, npr, amh, tsc, wa, pl5]\n    column_lst = ['satisfaction_level', 'last_evaluation', 'number_project', 'average_monthly_hours',\n                  'time_spend_company', 'Work_accident', 'promotion_last_5years']\n    for i in range(len(scaler_lst)):\n        if not scaler_lst[i]:\n            df[column_lst[i]] = MinMaxScaler().fit_transform(df[column_lst[i]].values.\\\n                                                             reshape(-1, 1)).reshape(1, -1)[0]\n        else:\n            df[column_lst[i]] = StandardScaler().fit_transform(df[column_lst[i]].\\\n                                                               values.reshape(-1, 1)).reshape(1, -1)[0]\n    scaler_lst = [sal, dep]\n    column_lst = ['salary', 'department']\n    for i in range(len(scaler_lst)):\n        if not scaler_lst[i]:\n            if column_lst[i] == 'salary':\n                df[column_lst[i]] = [map_salary(s) for s in df['salary'].values]\n            else:\n                df[column_lst[i]] = LabelEncoder().fit_transform(df[column_lst[i]])\n            df[column_lst[i]] = MinMaxScaler().fit_transform(df[column_lst[i]].values.\n                                                                 reshape(-1,1)).reshape(1, -1)[0]\n        else:\n            df = pd.get_dummies(df, columns=[column_lst[i]])  # pandas \u4e2dOneHotEncoder \u5904\u7406\n    if low_d:\n        return PCA(n_components=low_n).fit_transform(df.values)\n    # return LinearDiscriminantAnalysis(n_components=low_n) LDA \u7ebf\u6027\u5224\u522b\u5f0f\u5206\u6790 \u964d\u7ef4\n    \n    return df,label\n\n\nd = dict([(\"low\", 0), (\"medium\", 1), (\"high\", 2)])\n\n\ndef map_salary(s):\n    return d.get(s, 0)\n\n\ndef hr_modeling(features,label):\n    # \u5207\u5206\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\n    from sklearn.model_selection import train_test_split\n    f_v = features.values\n    l_v = label.values\n    X_tt, X_validation, Y_tt, Y_validation   = train_test_split(f_v, l_v, test_size=0.2) #\u9a8c\u8bc1\u96c6\n    X_train, X_test, Y_train, Y_test = train_test_split(X_tt, Y_tt, test_size=0.25)\n    print(len(X_train), len(X_validation), len(X_test))\n\n\ndef main():\n    features,label=hr_preprocessing()\n    hr_modeling(features,label)\n    # print(hr_preprocessing(dep=True, sal=True, low_d=False, ))\n    # print(hr_preprocessing(sl=True, le=True, npr=True, amh=True, tsc=True, wa=True, pl5=True, dep=True, sal=True))\n\n\n\nif __name__ == \"__main__\":\n\n\n    main()\n","a87910b1":"# \u71b5\n# s1 = pd.Series(['x1','x1','x2','x2','x2','x2','x2','x2','x2','x2'])\n# s2 = pd.Series(['y1','y1','y2','y2','y1','y2','y2','y2','y2','y2'])\n# def getEntropy(s):\n#     if not isinstance(s,pd.core.series.Series):\n#         s = pd.Series(s)\n#     prt_ary = pd.groupby(s,by = s).count().values\/float(len(s))\n#     return -(np.log2(prt_ary)*prt_ary).sum()\n# print('Entropy:',getEntropy(s2))\n# def getCondEntropy(s1,s2):\n#     d = dict()\n#     for i in list(range(len(s1))):\n#         d[s1[i]] = d.get(s1[i],[]) + [s2[i]]\n#     return sum([getEntropy(d[k])*len(d[k])\/float(len(s1)) for k in d])\n# print('CondEntropy',getCondEntropy(s1,s2))\n# def getEntropyGain(s1,s2):\n#     return getEntropy(s2)-getCondEntropy(s1,s2)\n# print('EntropyGain',getEntropyGain(s2,s1))\n# def getEntropyGainRatio(s1,s2):\n#     return getEntropyGainRatio(s1,s2)\/getEntropy(s2)\n# print('EntropyGainRation',getEntropyGainRatio(s2,s1))\n# import math\n# def getDiscreteCorr(s1,s2):\n#     return getEntropyGain(s1,s2)\/math.sqrt(getEntropy(s1))*getEntropy(s2))\n# print('DiscreteCorr',getDiscreteCorr(s2,s1))\n# Gini \u7cfb\u6570\n# def getProbSS(s):\n#     if not isinstance(s,pd.core.series.Series):\n#         s = pd.Series(s)\n#     prt_ary = pd.groupby(s,by = s).count().values\/float(len(s))\n#     return sum(prt_ary**2)\n\n# def getGini(sq,s2):\n#     d = dict()\n#     for i in list(range(len(s1))):\n#         d[s1[i]] = d.get(s1[i],[]) + [s2[i]]\n#         return 1 - sum(([getProbSS(d[k])*len(d[k])\/float(len(s1)) for k in d]))\n# print('Gini',getGini(s2,s1))","34a07726":"# def getEntropyGainRatio(s1,s2):\n#     return getEntropyGainRatio(s1,s2)\/getEntropy(s2)\n# print('EntropyGainRation',getEntropyGainRatio(s2,s1))","193e5fd8":"#df1 = pd.DataFrame({'a':ss.norm.rvs(size = 10),'b':ss.norm.rvs(size = 10),'c':ss.norm.rvs(size = 10),'d':np.random.randint(low = 0,high = 2,size  = 10)})","b7f192a2":"# \u7279\u5f81\u9009\u62e9\n\n# from sklearn.svm import SVR\n# from sklearn.tree import DecisionTreeRegressor\n# x = df1.loc[:,['a','b','c']]\n# y = df1.loc[:,['d']]\n# from sklearn.feature_selection import SelectKBest,RFE,SelectFromModel # \u7279\u5f81\u9009\u62e9\u5305feature_selection SelectKBest:\u8fc7\u6ee4\u601d\u60f3 ,RFE:\u5305\u88f9\u601d\u60f3  SelectFromModel:\u5d4c\u5165\u601d\u60f3\n# skb = SelectKBest( k = 2)\n# skb.fit(x,y) #\u62df\u5408\n# skb.transform(x)\n# rfe = RFE(estimator = SVR(kernel = 'linear'),n_features_to_select=2,step = 1)\n# rfe.fit_transform(x,y)\n# sfm =SelectFromModel(estimator = DecisionTreeRegressor(),threshold=0.1)  # threshold\u91cd\u8981\u56e0\u5b50\u4f4e\u4e8e\u591a\u5c81\u88ab\u53bb\u6389  \u9700\u8981feature_importances_ \u6709\u8fd9\u4e2a\u6a21\u578b\u624d\u80fd\u7528\n# sfm.fit_transform(x,y)","c808c946":"# lst1 = [6,10,23,45,67,12,25,31,43,60,83]\n# pd.qcut(lst1,q = 3,labels = ['low','medium','high']) # \u7b49\u6df1\n","bf0efa14":"# pd.cut(lst1,bins = 3) # \u7b49\u8ddd","4c931918":"# # \u5f52\u4e00\u5316 \u548c\u6807\u51c6\u5316\n# from sklearn.preprocessing import MinMaxScaler,StandardScaler\n# MinMaxScaler().fit_transform(np.array([1,4,10,15,21]).reshape(-1,1)) # \u5f52\u4e00\u5316 o---1\u4e4b\u95f4\n# StandardScaler().fit_transform(np.array([1,0,0,0,0,0,0,0,0]).reshape(-1,1))# \u6807\u51c6\u5316 \u5747\u503c\u4e3a0 \u6807\u51c6\u5dee\u4e3a1","099465a1":"# from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n# # \u5f52\u4e00\u5316\n# LabelEncoder().fit_transform(np.array(['down','up','up','down','med','dog']).reshape(-1,1))","5bc74872":"# OneHotEnconder \u72ec\u70ed\u7f16\u7801\n# lb_encoder = LabelEncoder()\n# lb_tran_f = lb_encoder.fit_transform(np.array(['red','yellow','blue','green']))\n# oht_enconder = OneHotEncoder().fit(lb_tran_f.reshape(-1,1))\n# oht_enconder.transform(lb_encoder.transform(np.array(['yellow','blue','green','green','red'])).reshape(-1,1)).toarray()","1897075d":"# from sklearn.preprocessing import Normalizer  # \u6b63\u89c4\u5316\n# Normalizer(norm = 'l2').fit_transform(np.array([[1,1,3,-1,2]]))\n# Normalizer(norm = 'l1').fit_transform(np.array([[1,1,3,-1,2]]))\n# # LDA\u964d\u7ef4 Linear Discriminant Analysis \u7ebf\u6027\u5224\u522b\u5f0f\u5206\u6790\n# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n# x = np.array([[-1,1],[-2,-1],[-3,-2],[1,1],[2,1],[3,2]])\n# y = np.array([1,1,1,2,2,2])\n# LinearDiscriminantAnalysis(n_components = 1).fit_transform(x,y)\n# clf = LinearDiscriminantAnalysis(n_components = 1).fit(x,y) # fisher clssfile \u5206\u7c7b\u5668\n# clf.predict([[0.8,1]])","d6937b03":"* sns \u7ed8\u5236\u76f4\u65b9\u56fe\u67f1\u72b6\u56fe"}}