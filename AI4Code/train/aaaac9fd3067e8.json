{"cell_type":{"53594fd7":"code","51dd0b9a":"code","b246883e":"code","51f4642c":"code","4423ee71":"code","e8b83c35":"code","234074fc":"code","d1a2ac61":"code","8d261539":"code","34573fb4":"code","4927111c":"code","f2fcc578":"code","b77f20fd":"code","6dc12927":"code","e0c84488":"code","7c942afd":"code","d9a389a9":"code","d2ef4f56":"code","e01bacf4":"code","5942306b":"code","f661d4d3":"code","ff7691ae":"code","56811634":"code","cf914659":"code","055779e2":"code","3842c867":"code","eec0932b":"code","c028e29b":"code","76018289":"code","c7810da9":"code","65b058fc":"code","dbed64a1":"code","4260be44":"code","bd4ed800":"code","cdec9533":"code","8686d075":"code","1baabaaf":"code","0ffd0a85":"code","c8e01be1":"code","51d5b048":"code","f6d32113":"code","cf856679":"code","83df77e0":"code","76dcd687":"code","28803c71":"code","dd02fefc":"code","29515c7b":"code","9d806f9e":"code","fdac33ea":"code","a86eebe6":"code","bafc45f8":"code","ff7894b7":"code","36068752":"code","02d44dd0":"code","3e88f1e2":"code","b8834713":"code","b7caf9a7":"code","3d206d74":"code","f9f4cc53":"code","4d4df378":"code","8c385071":"code","815b1f5d":"code","a8691597":"code","0055e11c":"code","be6893db":"code","e8e9ed69":"code","e0772dc5":"code","74b7b3df":"code","ce2fd63a":"code","d99b2362":"code","12fdbfe7":"code","a1feb96c":"code","7d5f54c7":"code","c5847ecc":"code","fa89fc3c":"code","cc62603b":"code","07212e93":"code","6b24cd7a":"code","596a70c6":"code","3227f01f":"code","644c9372":"code","59711bd4":"code","2748eaec":"code","ecf4bcc8":"code","563a6904":"code","6dea4274":"code","8d9a2b3b":"code","75556734":"code","f37c0e88":"code","152994b8":"code","8b214422":"code","3cad4dd3":"code","3738e09c":"code","3cc2fb7d":"code","afd8644d":"code","6dbff1fc":"code","651f06f6":"code","a7a12608":"code","b8d62b2f":"code","6b21606d":"code","38c6c596":"markdown","ee8bcd31":"markdown","27836756":"markdown","91be19a6":"markdown","c34b7ae0":"markdown","997de3b1":"markdown","87389c96":"markdown","2439f257":"markdown","7f19efbe":"markdown","908e2ad2":"markdown","3bba52ba":"markdown","81bc3ee2":"markdown","2b67bea0":"markdown","03c908c9":"markdown","3cc25988":"markdown","ec9deacd":"markdown","bfc12fb8":"markdown","449746ef":"markdown","f63717a1":"markdown","eac4fbb9":"markdown","a6b5d8c5":"markdown","d8c3fa6f":"markdown","35d1e569":"markdown","8008bf69":"markdown","1d8e0428":"markdown","ddadf0b0":"markdown","0adf87fa":"markdown","8aef84a1":"markdown"},"source":{"53594fd7":"#importing dataset\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n#reading dataset\ndef read_dataset(file_name):\n    csv_=pd.read_csv(file_name)\n    return csv_\n\n#droping coloumns\ndef drop_coloumn(dataframe,label):\n    X = dataframe.drop(label, axis = 1)\n    y = dataframe[label]\n    return X,y","51dd0b9a":"#reading dataset\nSantander=read_dataset(\"..\/input\/santander-customer-satisfaction\/train.csv\")\nX_train,y_train=drop_coloumn(Santander,'TARGET')\n\nprint(X_train.shape, y_train.shape)","b246883e":"from sklearn.feature_selection import VarianceThreshold\nconstant_filter = VarianceThreshold(threshold=0)\nconstant_filter.fit(X_train)","51f4642c":"constant_list = [not temp_feat for temp_feat in constant_filter.get_support()]","4423ee71":"X_train_filter = constant_filter.transform(X_train)","e8b83c35":"X_train_filter.shape,X_train.shape","234074fc":"quasi_constant_filter = VarianceThreshold(threshold=0.01)\nquasi_constant_filter.fit(X_train)","d1a2ac61":"constant_list = [not temp_feat for temp_feat in quasi_constant_filter.get_support()]","8d261539":"X_train_quasi_filter  = quasi_constant_filter.transform(X_train)","34573fb4":"X_train_quasi_filter.shape","4927111c":"X_train_T = X_train_quasi_filter.T","f2fcc578":"type(X_train_T)","b77f20fd":"X_train_T = pd.DataFrame(X_train_T)","6dc12927":"X_train_T.shape","e0c84488":"X_train_T.duplicated().sum()","7c942afd":"duplicated_features = X_train_T.duplicated()","d9a389a9":"features_to_keep = [not index for index in duplicated_features]","d2ef4f56":"X_train_unique = X_train_T[features_to_keep].T","e01bacf4":"X_train_unique.shape, X_train.shape","5942306b":"import seaborn as sns\nfrom sklearn.feature_selection import chi2\ntitanic=sns.load_dataset('titanic')","f661d4d3":"titanic.drop(labels = ['age', 'deck'], axis = 1, inplace = True)","ff7691ae":"titanic = titanic.dropna()","56811634":"data = titanic[['pclass', 'sex', 'sibsp', 'parch', 'embarked', 'who', 'alone']].copy()","cf914659":"#encoding\nsex = {'male': 0, 'female': 1}\ndata['sex'] = data['sex'].map(sex)\n\nports = {'S': 0, 'C': 1, 'Q': 2}\ndata['embarked'] = data['embarked'].map(ports)\n\nwho = {'man': 0, 'woman': 1, 'child': 2}\ndata['who'] = data['who'].map(who)\n\nalone = {True: 1, False: 0}\ndata['alone'] = data['alone'].map(alone)","055779e2":"X_train = data.copy()\ny_train = titanic['survived']","3842c867":"f_score = chi2(X_train, y_train)","eec0932b":"import pandas as pd\np_values=pd.Series(f_score[1])\np_values.index=X_train.columns\np_values","c028e29b":"p_values.sort_values(ascending=False).plot.bar(figsize=(20, 8))","76018289":"from sklearn.datasets import load_wine\nimport pandas as pd\ndata=load_wine()\n\nX_train = pd.DataFrame(data.data)\ny_train = data.target\n\nX_train.columns = data.feature_names\nX_train.head()\n\nprint(X_train.shape,y_train.shape)","c7810da9":"from sklearn.feature_selection import f_classif\nf_score = f_classif(X_train, y_train)","65b058fc":"import pandas as pd\np_values=pd.Series(f_score[1])\np_values.index=X_train.columns\np_values\n\np_values.sort_index(ascending=False)","dbed64a1":"p_values.sort_values(ascending=False).plot.bar(figsize=(20, 8))","4260be44":"#reading dataset\nSantander=read_dataset(\"..\/input\/santander-customer-satisfaction\/train.csv\")\nX_train,y_train=drop_coloumn(Santander,'TARGET')\n\nprint(X_train.shape, y_train.shape)","bd4ed800":"def correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        \n        \n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","cdec9533":"import seaborn as sns\nimport matplotlib.pyplot as plt\n#Using Pearson Correlation\nplt.figure(figsize=(12,10))\ncor = X_train.corr()\nsns.heatmap(cor, annot=True)\nplt.show()","8686d075":"corr_features = correlation(X_train, 0.9)\nlen(set(corr_features))","1baabaaf":"X_train.drop(corr_features,axis=1)","0ffd0a85":"from sklearn.datasets import load_wine\nimport pandas as pd\ndata=load_wine()\n\nX_train = pd.DataFrame(data.data)\ny_train = data.target\n\nX_train.columns = data.feature_names\nX_train.head()\n\nprint(X_train.shape,y_train.shape)","c8e01be1":"from sklearn.feature_selection import mutual_info_classif\n# determine the mutual information\nmutual_info = mutual_info_classif(X_train,y_train)\nmutual_info","51d5b048":"mutual_info = pd.Series(mutual_info)\nmutual_info.index = X_train.columns\nmutual_info.sort_values(ascending=False)","f6d32113":"mutual_info.sort_values(ascending=False).plot.bar(figsize=(20, 8))","cf856679":"from sklearn.feature_selection import SelectKBest\n#No we Will select the  top 5 important features\nsel_five_cols = SelectKBest(mutual_info_classif, k=5)\nsel_five_cols.fit(X_train, y_train)\nX_train.columns[sel_five_cols.get_support()]","83df77e0":"from sklearn.datasets import load_boston\nfrom sklearn.feature_selection import mutual_info_regression","76dcd687":"boston = load_boston()","28803c71":"X_train = pd.DataFrame(data = boston.data, columns=boston.feature_names)\nX_train.head()","dd02fefc":"y_train = boston.target","29515c7b":"mi = mutual_info_regression(X_train, y_train)\nmi = pd.Series(mi)\nmi.index = X_train.columns\nmi.sort_values(ascending=False, inplace = True)","9d806f9e":"mi.plot.bar(figsize=(20, 8))","fdac33ea":"sel = SelectKBest(mutual_info_regression, k = 9).fit(X_train, y_train)\nX_train.columns[sel.get_support()]","a86eebe6":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","bafc45f8":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS","ff7894b7":"from sklearn.datasets import load_wine","36068752":"data = load_wine()","02d44dd0":"data.keys()","3e88f1e2":"X_train = pd.DataFrame(data.data)\ny_train = data.target","b8834713":"X_train.columns = data.feature_names\nX_train.head()","b7caf9a7":"X_train.isnull().sum()","3d206d74":"sfs = SFS(RandomForestClassifier(n_estimators=100, random_state=0, n_jobs = -1),\n         k_features = 7,\n          forward= True,\n          floating = False,\n          verbose= 2,\n          scoring= 'accuracy',\n          cv = 4,\n          n_jobs= -1\n         ).fit(X_train, y_train)","f9f4cc53":"sfs.k_feature_names_","4d4df378":"sfs.k_score_ ","8c385071":"sfs = SFS(RandomForestClassifier(n_estimators=100, random_state=0, n_jobs = -1),\n         k_features = (1, 8),\n          forward= False,\n          floating = False,\n          verbose= 2,\n          scoring= 'accuracy',\n          cv = 4,\n          n_jobs= -1\n         ).fit(X_train, y_train)","815b1f5d":"sfs.k_feature_names_","a8691597":"sfs.k_score_ ","0055e11c":"from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS","be6893db":"efs = EFS(RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1),\n         min_features= 4,\n          max_features= 8,\n          scoring='accuracy',\n          cv = None,\n          n_jobs=-1\n         ).fit(X_train, y_train)","e8e9ed69":"efs.best_score_","e0772dc5":"efs.best_feature_names_","74b7b3df":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","ce2fd63a":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Lasso, LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel","d99b2362":"titanic = sns.load_dataset('titanic')","12fdbfe7":"titanic.isnull().sum()","a1feb96c":"titanic.drop(labels = ['age', 'deck'], axis = 1, inplace = True)","7d5f54c7":"titanic = titanic.dropna()\ntitanic.isnull().sum()","c5847ecc":"titanic.head()","fa89fc3c":"data = titanic[['pclass', 'sex', 'sibsp', 'parch', 'embarked', 'who', 'alone']].copy()","cc62603b":"data.head()","07212e93":"sex = {'male': 0, 'female': 1}\ndata['sex'] = data['sex'].map(sex)","6b24cd7a":"ports = {'S': 0, 'C': 1, 'Q': 2}\ndata['embarked'] = data['embarked'].map(ports)","596a70c6":"who = {'man': 0, 'woman': 1, 'child': 2}\ndata['who'] = data['who'].map(who)","3227f01f":"alone = {True: 1, False: 0}\ndata['alone'] = data['alone'].map(alone)","644c9372":"X_train = data.copy()\ny_train = titanic['survived']","59711bd4":"sel = SelectFromModel(LogisticRegression(C = 0.05, penalty = 'l1', solver = 'liblinear'))\nsel.fit(X_train, y_train)","2748eaec":"sel.get_support()","ecf4bcc8":"features = X_train.columns[sel.get_support()]\nfeatures","563a6904":"X_train_l1 = sel.transform(X_train)","6dea4274":"X_train_l1.shape, X_train.shape","8d9a2b3b":"sel = SelectFromModel(LogisticRegression(C = 0.05, penalty = 'l2', solver = 'liblinear'))\nsel.fit(X_train, y_train)","75556734":"sel.get_support()","f37c0e88":"features = X_train.columns[sel.get_support()]\nfeatures","152994b8":"X_train_l1 = sel.transform(X_train)","8b214422":"X_train_l1.shape","3cad4dd3":"from sklearn.datasets import load_wine\nimport pandas as pd\ndata = load_wine()\n\ndata.keys()\n\nX = pd.DataFrame(data.data)\ny = data.target\n\nX.columns = data.feature_names\nX.head()","3738e09c":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, random_state=0)\nrf.fit(X, y)","3cc2fb7d":"importance = pd.concat([pd.Series(X.columns), pd.Series(rf.feature_importances_)], axis = 1)","afd8644d":"importance.columns = ['features', 'importance']","6dbff1fc":"importance.sort_values(by = 'importance', ascending = False, inplace = True)","651f06f6":"X[importance['features'][0:8]]","a7a12608":"from sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\nmodel = RandomForestClassifier(n_estimators=100, random_state=0)\n\nmodel.fit(X,y)","b8d62b2f":"print(model.feature_importances_)","6b21606d":"#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","38c6c596":"## 3.Wrapper Method\n\nwe perform wrapper method using mlxtend library<br>\nhttp:\/\/rasbt.github.io\/mlxtend\/<br>\n<br>\n<br>\nfor installing<br>\n<i><b>pip install mlxtend<\/b><\/i><br>","ee8bcd31":"## End Note\n<img src=\"https:\/\/i.ibb.co\/K7Jwh58\/difference-table.png\" alt=\"difference-table\" border=\"0\">","27836756":"<b>feature has been reduced from 371 to 336<\/b>","91be19a6":"<b>in Santander dataset we have 3 duplicate columns <\/b>\ncolumn has been reduced from 370 to 256","c34b7ae0":"### 4.1. Lasso Regression","997de3b1":"<img src=\"https:\/\/i.ibb.co\/cTX0DDp\/feature-type-data.png\" alt=\"feature-type-data\" border=\"0\">","87389c96":"##### 2.2.2.1 Classification","2439f257":"### Table of content\n1. Introduction\n    * 1.1 what is Feature Selection?\n    * 1.2 benefit of Feature Selection\n<br>\n<br>\n2. Filter Method\n    * 2.1 Univariant\n        * 2.1.1 Constant removal\n        * 2.1.2 Quasi Constant Removal\n        * 2.1.3 Duplicate Feature Removal\n        * 2.1.4 Information Gain\n            *  2.1.4.1 Information Gain for Classification\n            *  2.1.4.2 Information Gain for Regression\n        * 2.1.5 Chi-square Test\n        * 2.1.6 Anova Test \n    * 2.2 Multi variant\n        * 2.2.1 Correlation\n        * 2.2.2 Information Gain for regression and classification\n<br>\n<br>\n3. Wrapper Method\n    * 3.1. Forward Feature Selection\n    * 3.2. Backward Feature Selection\n    * 3.3. Recursive Feature Selection\n<br>\n<br>\n4. Embedding Method\n    * 4.1. Lasso Regression\n    * 4.2. Ridge Regression\n    * 4.3 Feature Importance\n<br>\n<br>\n\n### <i>Dataset Used<\/i>\n<img src=\"https:\/\/i.ibb.co\/yXbLFmh\/data-used.png\" alt=\"data-used\" border=\"0\">\n\n\n## 1.Introduction\n<img src=\"https:\/\/i.ibb.co\/mv8Vxch\/feature-selection.png\" alt=\"feature-selection\" border=\"0\">\n<br>\n<br>\n\n### 1.1 What is Feature Selection?\n\nIn feature selection we choose those feature which contribute most to our prediction variable<br>\nHaving irrelevant feature decrease the accuracy of model.<br>\ntraining the model on the feature can take lot of the time <br>\n\n\nwhen the curse of dimension kicked in model performance reduced\n<img src=\"https:\/\/i.ibb.co\/s90zgTM\/curse-of-dim.png\" alt=\"curse-of-dim\" border=\"0\">\n\n<img src=\"https:\/\/i.ibb.co\/kQVNMZ6\/what-is-feature-importance.png\" alt=\"what-is-feature-importance\" border=\"0\">\n\n### 1.2 Benefit of Feature Selection\n\n* Model with less number of features have higher explainability\n* It is easier to implement machine learning models with reduced features\n* It reduces overfitting\n* Training time of models with fewer features is significantly lower\n* Models with fewer features are less prone to errors","7f19efbe":"<b>feature after Forward feature selection<\/b>","908e2ad2":"#### 2.1.4 Chi-square (\u03c72) Test\nIt is a statistical test applied to the groups of categorical features to evaluate the likelihood of correlation or association between them using their frequency distribution.\n\n<b>Note-<\/b>This score should be used to evaluate categorical variables in a classification task.","3bba52ba":"### 4.3 Feature Importance","81bc3ee2":"#### 2.2.1 Correlation Coefficient\n\nPearson\u2019s Correlation: It is used as a measure for quantifying linear dependence between two continuous variables X and Y. Its value varies from -1 to +1. Pearson\u2019s correlation is given as:\n\n<img src=\"https:\/\/i.ibb.co\/yBRFr3F\/person-correlation.png\" alt=\"person-correlation\" border=\"0\">","2b67bea0":"<img src=\"https:\/\/i.ibb.co\/qjw6jL5\/wrapper-method.png\" alt=\"wrapper-method\" border=\"0\">","03c908c9":"### 4.2. Ridge Regression","3cc25988":"### 3.3. Recursive Feature Selection\nThis is the most robust feature selection method covered so far. This is a brute-force evaluation of each feature subset. This means that it tries every possible combination of the variables and returns the best performing subset","ec9deacd":"#### 2.1.2 Quasi Constant Removal\nQuasi Constant is like constant feature in constant feature we remove the 0 variance while in quasi we also remove feature having equal to 0.1 variance","bfc12fb8":"##### 2.2.2.2 Regression","449746ef":"#### 2.1.3 Duplicate Feature Removal\nsometime we have duplicate columns.we take the first column and remove the second duplicate column\n<img src=\"https:\/\/i.ibb.co\/WcJR2Vb\/Duplicate-Feature-Removal.png\" alt=\"Duplicate-Feature-Removal\" border=\"0\">","f63717a1":"### 3.2. Backward Feature Selection\nIn backward elimination, we start with the all the independent variables and then remove the insignificant feature which minimized our criterion function.we repeat this process untill we get our k features <br>\nk=number of desire features\n\n<img src=\"https:\/\/i.ibb.co\/hCyDgpg\/backward-feature-selection.png\" alt=\"backward-feature-selection\" border=\"0\">","eac4fbb9":"<b>feature after backward feature selection<\/b>","a6b5d8c5":"## 2.Filter Method\nFilter Method use statistical test to choose the subset of features which minimize the loss. Filter Method does not depended on machine learning models.<br>\n\n#### advantages of filter Method\n* less computationally expensive\n* uses statistical algorithms like chi-square\n* uses individual feature predictive power\n\n### 2.1 Uni-variant\nThe univariate filter method use only individual features to find the best subset of features and are ranked based upon specific criteria \n<br>\nUni-variant Methods\n* Constant Removal\n* Quasi Constant Removal\n* Duplicate Feature Removal\n* Chi-square Test\n* Anova Test ","d8c3fa6f":"#### 2.2.2 Information Gain\nMutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n\n1. If X and Y are independent, then no information about Y can be obtained by knowing X or vice versa. Hence their mutual information is 0.\n2. If X is a deterministic function of Y, then we can determine X from Y and Y from X with mutual information 1.\n3. When we have Y = f(X,Z,M,N), 0 < mutual information < 1\n\nThe mutual information between two random variables X and Y can be stated formally as follows:\n\n<img src=\"https:\/\/i.ibb.co\/bNkrQzR\/information-gain.png\" alt=\"information-gain\" border=\"0\">","35d1e569":"#### 2.1.5 Anova Test \n\nANOVA stands for Analysis of variance. It is similar to LDA except for the fact that it is operated using one or more categorical independent features and one continuous dependent feature. It provides a statistical test of whether the means of several groups are equal or not.\n\n<b>Note-This score should be used to evaluate continuous variables in a classification task.<\/b>","8008bf69":"## 4.Embedding Method\n<img src=\"https:\/\/i.ibb.co\/7Y627DX\/Emedding-Method.png\" alt=\"Emedding-Method\" border=\"0\">\n\n\nEmbedding Method also called Regularization Approach <br>\n* Lasso-L1 regularization <br>\n* Ridge-L2 regularization <br>\n* Elastic Nets-L1 and L2 regularization <br>","1d8e0428":"### 3.1. Forward Feature Selection\n\nIn forward Feature selection, we start with a null model and then start fitting the model with each individual feature one at a time and select the feature which maximize our criterion function.we repeat this process untill we get our k features <br>\nk=number of desire features\n\n<img src=\"https:\/\/i.ibb.co\/Cv6RDhZ\/forward-feature-selection.png\" alt=\"forward-feature-selection\" border=\"0\">","ddadf0b0":"#### 1.1.1 Constant Removal\nIn constant feature we remove the feature which has 0 variance.Constant features provide no information that can help in classification.","0adf87fa":"<b>after quasi constant removal feature has been reduced from 371 to 274<\/b>","8aef84a1":"### 2.2 Multi-variant"}}