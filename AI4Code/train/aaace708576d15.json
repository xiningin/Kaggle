{"cell_type":{"bc9d12fe":"code","f98c2950":"code","b868989b":"code","396d0910":"code","20441bc5":"code","2da8f2b4":"code","44554296":"code","6c37c421":"code","3d1f4f82":"code","267c2a79":"code","cab32d2d":"code","0bee889c":"code","dd6a1fa6":"code","f0fc5e58":"code","6f7271db":"code","d09ba29d":"code","53ed499e":"code","8269ccd9":"code","ed24329a":"code","2bf3d7b6":"code","9f6bc451":"code","f1e73a8a":"code","d297a176":"code","94a0d078":"code","d7b8fa69":"code","6e09e132":"code","a2ac0255":"code","ce092427":"code","d9268a44":"code","9c26f962":"code","a3531c7e":"code","7c36c180":"code","f615357a":"code","0f60a0b3":"code","499c965b":"code","7d91e472":"code","d4d4f354":"code","0d60247e":"code","a5edd9b7":"code","1eb343dd":"code","4bd6257d":"code","0802d7f1":"code","b3ae891a":"code","9714c126":"code","48bd2d4b":"code","43274477":"code","beda0e7b":"code","8d2a5198":"code","91b3e115":"code","4676a0c2":"code","5d9a93e3":"code","a3942c80":"code","82549539":"code","cb7419e3":"code","b684bc9b":"code","df4143be":"code","0deb4294":"code","83ccef71":"code","f8114c84":"code","947bcec0":"code","a84354b5":"code","8505d527":"code","23d477cd":"code","29df19a4":"code","a1386a1a":"code","8edff60f":"code","34c8b9cf":"code","a9c0578e":"code","d1434cba":"code","22dc8fc5":"code","32dd7a67":"code","a674ee41":"code","01aee083":"code","abb23cb9":"code","1c8594e1":"code","45dd5c4d":"markdown","4204717e":"markdown","322a06b6":"markdown","edbcabf3":"markdown","d923654e":"markdown","0ccc7595":"markdown","f1a3dfa3":"markdown","445ade38":"markdown","0bf736c4":"markdown","50ccaadd":"markdown","0a019338":"markdown","573422a6":"markdown","3c6fd64f":"markdown","7b9f54a7":"markdown","ea79128a":"markdown","735a3b62":"markdown","ab1493b1":"markdown","492f9e02":"markdown","a16244b7":"markdown","21c15d7c":"markdown","eb462ba0":"markdown","06a37e0c":"markdown","397e49be":"markdown","db75b4d0":"markdown"},"source":{"bc9d12fe":"#import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn as sk\n%matplotlib inline\nimport os as os","f98c2950":"os.chdir(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\")\nprint(os.listdir(\"..\/house-prices-advanced-regression-techniques\/\"))","b868989b":"#load the data into a Pandas dataframe\ntrain_features_df=pd.read_csv(\"train.csv\")\ntest_features_df=pd.read_csv(\"test.csv\") ","396d0910":"train_features_df.dtypes","20441bc5":"#Preview of dataframes\ntrain_features_df.head(7)","2da8f2b4":"test_features_df.head(7)","44554296":"train_features_df.shape","6c37c421":"test_features_df.shape","3d1f4f82":"#Use info to see length and datatypes\ntrain_features_df.info()","267c2a79":"test_features_df.info()","cab32d2d":"train_df=train_features_df","0bee889c":"#look for duplicate data\ntrain_df.duplicated().sum()","dd6a1fa6":"test_features_df.duplicated().sum()","f0fc5e58":"#Check for entries with SalePrice<=0\n(train_df.SalePrice<=0).sum() ","6f7271db":"\n# Copy of the input Data frame\n\ntrain_data=train_df\ntraindata_df=train_df.copy()\ntest_data=test_features_df ","d09ba29d":"len(traindata_df) ","53ed499e":"#Identify numerical and categorical varibales\ntrain_data.columns\n#len(train_data.columns)","8269ccd9":"test_features_df.columns","ed24329a":"# Split into numerical and Categorical features\n\nnumeric_cols = [ c for c in train_data.columns if train_data.dtypes[c] != 'object' ]\ncategorical_cols = [ c for c in train_data.columns if train_data.dtypes[c] == 'object' ]","2bf3d7b6":"categorical_cols","9f6bc451":"#Summarize numericaland categorical variables separately\ntrain_data.describe(include=[np.number])","f1e73a8a":"\ntrain_data.describe(include=[np.object])","d297a176":"train_data.describe(include=['O'])","94a0d078":"#Visualize target variable distribution and boxplot\nplt.figure(figsize=(12,6))\nplt.subplot(1,2,1)\nsns.boxplot(train_data.SalePrice)\nplt.subplot(1,2,2)\nsns.distplot(train_data.SalePrice,bins=20)\nplt.show()","d7b8fa69":"print('The skew is '+ str(train_data.SalePrice.skew()))\nprint('The Kurtosis is '+ str(train_data.SalePrice.kurt()))","6e09e132":"\nstat = train_data.SalePrice.describe()\nstat","a2ac0255":"#Use 1.5 IQR rule to find outliers\nstat = train_data.SalePrice.describe()\nprint(stat)\nIQR = stat['75%'] - stat['25%']\nupper = stat['75%'] + 1.5*IQR\nlower = stat['25%'] - 1.5*IQR\nprint('The upper and lower bounds of suspected outliers are {} and {}.'.format(upper,lower))","ce092427":"print(\" The upper limit is \" + str(upper))\nprint(\" The lower limit is \" + str(lower))","d9268a44":"#Check potential outlier below lower bound\nprint(train_data[train_data.SalePrice < 3937.5])\nprint(train_data[train_data.SalePrice > 340037.5 ])","9c26f962":" \n#train_data.head()","a3531c7e":"#Define a function to plot the relation between features and the target\ndef plot_feature(df,col):\n    plt.figure(figsize=[25,5])\n    plt.subplot(1,3,2)\n    if df[col].dtype=='int64':\n        #mean=df.groupby([col])['SalePrice'].mean().plot()\n        print(col)\n        #df.plot.scatter(x=col, y='SalePrice')#, ylim=(0,800000))\n        plt.scatter(x=col, y='SalePrice',data=train_data)\n        plt.xlabel(col)\n        plt.ylabel(\"Mean SalePrice\")\n        plt.subplot(1,3,1)\n        #df[col].value_counts().sort_index().plot()\n        sns.distplot(df[col],bins=20)\n        plt.xlabel(col)\n        plt.ylabel(\"Counts\")\n    else:\n        mean=df.groupby(by=[col])['SalePrice'].mean().sort_values(ascending=True).index\n        sns.boxplot(x=col,y=\"SalePrice\",data=df,order=mean)\n        plt.xticks(rotation=20)\n        plt.subplot(1,3,1)\n        df[col].value_counts().sort_index().plot()\n        plt.xlabel(col)\n        plt.ylabel(\"Counts\")\n    ","7c36c180":"train_data['TotalBsmtSF'].hist()","f615357a":"\nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\ntrain_data['TotalBsmtSF'].value_counts().sort_index().plot()\n \n\nplt.subplot(1,2,2)\nplt.scatter(x='TotalBsmtSF', y='SalePrice',data=train_data)# ylim=(0,800000))\n","0f60a0b3":"numeric_cols","499c965b":"#numeric_cols.remove( (\"Id\",\"MSSubClass\",\"LotArea\",\"OverallQual\",\"OverallCond\",\"YearBuilt\",\"YearRemodAdd\",\"BsmtFinSF1\",\"BsmtFinSF2\"))\n# for col in numeric_cols:\n#     plot_feature(train_data,col)","7d91e472":"#plot_feature(train_data,\"CentralAir\")\nfor col in categorical_cols:\n    plot_feature(train_data,col)\n    ","d4d4f354":"\nplt.figure(figsize=[35,15])\nsns.boxplot(x=\"Neighborhood\",y=\"SalePrice\",data=train_data)","0d60247e":"numeric_cols","a5edd9b7":"#Find relations between features\nsns.set()\ncols = [\"GrLivArea\",\"SalePrice\",\"WoodDeckSF\",\"OverallQual\"]\nsns.pairplot(train_data[cols],size = 2.5)","1eb343dd":"plt.scatter(x='GrLivArea',y=\"SalePrice\",data=train_data)","4bd6257d":"# len(train_data)\n# np.log1p(train_data['SalePrice'])","0802d7f1":"#plt.scatter(y=np.log1p(train_data['SalePrice']),x=\"GrLivArea\",data=train_data)","b3ae891a":"train_data=train_data.drop(train_data[ (train_data['SalePrice'] < 300000) & (train_data['GrLivArea'] > 4000) ].index )","9714c126":"#combine data for handling missing values\n\nnrows_train = train_data.shape[0]\nnrows_test = test_data.shape[0]\nall_data=pd.concat([train_data,test_data]).reset_index(drop=True)\n\nall_data.drop(['SalePrice'], axis=1, inplace=True)\n\nprint(\"all_data size is : {}\".format(all_data.shape)) ","48bd2d4b":"# needed in future\nnrows_train","43274477":"#find missing values\n\nTotal=all_data.isnull().count()\ncounts=all_data.isnull().sum()\n\nsummary = pd.concat([counts,counts\/Total],axis =1,keys=[\"Count\",\"Percentage\"])\nsummary.sort_values(by=\"Percentage\",ascending=False)","beda0e7b":"all_data['MiscFeature']=all_data['MiscFeature'].fillna(\"None\")\nall_data['Alley']=all_data['Alley'].fillna(\"None\")\nall_data['Fence']=all_data['Fence'].fillna(\"None\")\nall_data['FireplaceQu']=all_data['FireplaceQu'].fillna(\"None\")\nall_data['GarageFinish']=all_data['GarageFinish'].fillna(\"None\")\nall_data['GarageQual']=all_data['GarageFinish'].fillna(\"None\")\nall_data['GarageType']=all_data['GarageType'].fillna(\"None\")\nall_data['BsmtCond']=all_data['BsmtCond'].fillna(\"None\")\nall_data['BsmtExposure']=all_data['BsmtExposure'].fillna(\"Nobase\")\nall_data['BsmtQual']=all_data['BsmtQual'].fillna(\"None\")\nall_data['BsmtFinType2']=all_data['BsmtFinType2'].fillna(\"None\")\nall_data['BsmtFinType1']=all_data['BsmtFinType1'].fillna(\"None\")\nall_data['GarageCond']=all_data['GarageCond'].fillna(\"None\")\nall_data['PoolQC']=all_data['PoolQC'].fillna(\"None\")\nall_data['MasVnrType']=all_data['MasVnrType'].fillna(\"None\")\nall_data['MasVnrArea']=all_data['MasVnrArea'].fillna(0)\nall_data['Functional']=all_data['Functional'].fillna(\"Typ\")\n#all_data['Utilities']=all_data['Utilities'].fillna(\"AllPub\")\nall_data=all_data.drop(['Utilities'],axis=1)\nall_data['MSZoning']=all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\nall_data['Electrical']=all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\nfor col in ['GarageYrBlt','GarageArea','GarageCars']:\n    all_data[col]=all_data[col].fillna(0)\n    \nfor col in ['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath']:\n    all_data[col]=all_data[col].fillna(0)\n    \nfor col in ['SaleType','Exterior1st','Exterior2nd','KitchenQual']:\n    all_data[col]=all_data[col].fillna(all_data[col].mode()[0])","8d2a5198":"# Before treaing missing values\nsns.boxplot(x=\"MiscFeature\",y=\"SalePrice\",data=train_data)\n","91b3e115":"# After treaing missing values\nsns.boxplot(x=\"MiscFeature\",y=train_data['SalePrice'],data=all_data[:1458])","4676a0c2":"#Data frame length after dropping 2 outliers\nlen(all_data.dropna())","5d9a93e3":"#check if any more missing values are there\n\nTotal=all_data.isnull().count()\ncounts=all_data.isnull().sum()\n\nsummary = pd.concat([counts,counts\/Total],axis =1,keys=[\"Count\",\"Percentage\"])\nsummary.sort_values(by=\"Percentage\",ascending=False)","a3942c80":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC,LassoCV,LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error \n ","82549539":"linreg = LinearRegression()","cb7419e3":"all_data.head(7)","b684bc9b":"all_data.shape","df4143be":"#new_data created for converting categorical to numerical values\nnew_data=pd.get_dummies(all_data)","0deb4294":"new_data.head(7)","83ccef71":"#train input\ntrain_model_input = new_data[:1458]\n\n#train output\ntrain_model_output=train_data['SalePrice']\n\n#log train output\ntrain_model_log_output=np.log1p(train_data['SalePrice']).values\n\n#test data\ntest_model_data = new_data[1458:]","f8114c84":"#defin\nn_folds = 5\ndef rmsle_cv(model,data_input,data_output):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(data_input.values)\n    rmse= np.sqrt(-cross_val_score(model, data_input.values, data_output, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","947bcec0":"linreg.fit(train_model_input, train_model_output) \n","a84354b5":"rmsle_cv(linreg,train_model_input,train_model_output).mean()","8505d527":"train_model_prediction= linreg.predict(new_data[:1458])","23d477cd":"print('Mean Squared Error:', mean_squared_error(train_model_output, train_model_prediction)) \nprint('Mean Squared Error in log terms :', mean_squared_error(np.log(train_model_output) , np.log(train_model_prediction) )) ","29df19a4":"linreg_log = LinearRegression()\ntrain_model_log_output=np.log(train_data['SalePrice']) \nlinreg_log.fit(train_model_input, train_model_log_output)\ntrain_model_log_prediction= linreg_log.predict(new_data[:1458])\nprint('Mean Squared Error:', mean_squared_error(np.exp(train_model_log_output) , np.exp(train_model_log_prediction) )) \nprint('Mean Squared Error in log terms :', mean_squared_error(train_model_log_output , train_model_log_prediction )) \n","a1386a1a":"train_model_log_prediction","8edff60f":"n_folds = 5\n#rmsle_cv(linreg).mean()\nrmsle_cv(linreg_log,train_model_input,train_model_log_output).mean()","34c8b9cf":"#X_train, X_test, y_train, y_test = train_test_split(train_model_input, train_model_log_output, test_size=0.33, random_state=42)","a9c0578e":"Final_prediction=linreg_log.predict(test_model_data)","d1434cba":"Final_prediction","22dc8fc5":"test_data","32dd7a67":"results=pd.concat([test_data['Id'],pd.Series(np.expm1(Final_prediction))],axis=1,keys=['Id','SalePrice'])","a674ee41":"results.head(5)","01aee083":"!pwd","abb23cb9":"print(os.listdir(\"\/kaggle\/working\"))\n","1c8594e1":"results.to_csv('\/kaggle\/working\/submissions_linreg_3.csv',index=False)","45dd5c4d":"# 10.Check the output","4204717e":"# Split the Whole Data set to Train and Test Split \n","322a06b6":"# Calculate errors on Cross validated set","edbcabf3":"#  8.Create the model ","d923654e":"# 6.Removing Outliers","0ccc7595":"#    4. Explore the data (EDA) ","f1a3dfa3":"By examining the above data it is clear that eventhough the jobType is JUNIOR, all these employees has atleast 18 years of experience and majority of them has masters and doctoral degree. So the data should be good and no need to remove any entries","445ade38":"# 7.Handle Missing Values","0bf736c4":"# 5.Data Visualizations","50ccaadd":" # Fit the Linear Regression Model","0a019338":"From the above plots we can find a positive relation between jobType and salary. Higher the job position,higher is the salary","573422a6":"# Predict the output for training Set","3c6fd64f":"# *****************THE END********************* #","7b9f54a7":"# Define the cross validation function ","ea79128a":"The goal of this project is to predict the house prices by analyzing the train data set and doing prediction on the test data set.\n\nThe tool used id Python 3 with it libraries and packages for data manipulation, data visulisation and  developing predictive modelling algorithms.","735a3b62":"#  2. Load the data and do exploratory analysis ","ab1493b1":"From the above visualization we can infer that although most of the data are somewhere between 75 and 150, there are some potential outliers","492f9e02":"# Fit the model with log data","a16244b7":"# 9.Predict the final output\n","21c15d7c":"# 1. Goal of the Project ","eb462ba0":"# Cross Validation Error","06a37e0c":"# Calculate the Error (MSE)","397e49be":"Since there is no data with Sale Price below lower bound, we don't need to remove any entries","db75b4d0":"#  3. Clean the data "}}