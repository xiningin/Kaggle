{"cell_type":{"ae034064":"code","5b592e98":"code","c2115957":"code","015a53c9":"code","93168508":"code","18400374":"code","f80eaf74":"code","fa1fc2e7":"code","b90adfbd":"code","7649424c":"code","a5dee283":"code","aa4f8e00":"code","7b46fd8a":"code","5e2e7b03":"code","5287fdea":"code","b49d7fc1":"code","78cc5865":"code","34c7c882":"markdown","76d38161":"markdown","e36e2359":"markdown"},"source":{"ae034064":"import numpy as np\nimport pandas as pd","5b592e98":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain.sample(10)","c2115957":"test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest.sample(10)","015a53c9":"print('train shape:', train.shape)\nprint('test shape:', test.shape)","93168508":"print(train.info())\nprint(test.info())","18400374":"#there are empty values for age, cabin, and fare, we will replace them using median, and mode\n\n# for efficient data cleaning\ndata_cleaner = [train,test]\n\nfor dataset in data_cleaner:\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n    dataset['Fare'].fillna(dataset['Fare'].mode()[0], inplace = True)","f80eaf74":"print(train.info())\nprint(test.info())","fa1fc2e7":"#https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n\nfor dataset in data_cleaner:   \n    \n    # create variable family size by adding siblings and parent\/children\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    \n    # alone passengers usually survive better\n    dataset['IsAlone'] = 0\n    #dataset[dataset['FamilySize'] == 1]['IsAlone'] = 1\n    dataset['IsAlone'].loc[dataset['FamilySize'] == 1] = 1   \n    \n    # title\n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n    \ntrain.head()","b90adfbd":"train['Title'].value_counts()","7649424c":"# clean up titles\nfor dataset in data_cleaner:   \n    for i in range(len(dataset)):\n        if dataset['Title'][i] not in ['Mr', 'Miss', 'Mrs', 'Master', 'Dr', 'Rev']:\n            dataset['Title'][i] = 'Others'\n\ntrain.sample(5)","a5dee283":"train = train.drop(['Name', 'Ticket', 'Cabin'],axis = 1)\none_hot_encoded_training_predictors = pd.get_dummies(train)\ntrain = one_hot_encoded_training_predictors.align(one_hot_encoded_training_predictors,\n                                                                    join='left', \n                                                                    axis=1)\ntrain = train[0]","aa4f8e00":"test = test.drop(['Name', 'Ticket', 'Cabin'],axis = 1)\none_hot_encoded_training_predictors = pd.get_dummies(test)\ntest = one_hot_encoded_training_predictors.align(one_hot_encoded_training_predictors,\n                                                                    join='left', \n                                                                    axis=1)\ntest = test[0]","7b46fd8a":"# train test spilt\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html\n\nfrom sklearn.model_selection import train_test_split\n\nX = train.drop(['Survived'], axis = 1)\ny = train['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","5e2e7b03":"#htps:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html\nfrom sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\ny_pred = gnb.fit(X_train, y_train)\npredictions = gnb.predict(X_test)\n\nfrom sklearn.metrics import accuracy_score\n\naccuracy_score(y_test, predictions)","5287fdea":"test['Survived'] = gnb.predict(test)","b49d7fc1":"submission = test[['PassengerId', 'Survived']]\nsubmission.to_csv('submission.csv', index=False)","78cc5865":"submission.head()","34c7c882":"### Imputing Missing Values","76d38161":"> ### Feature Engineering","e36e2359":"> ### Training data with Naive Bayes\n\nWhy Naive Bayes? Eventually I want to turn this into a graphical model kernel to apply some bayesian network ;) stay tuned"}}