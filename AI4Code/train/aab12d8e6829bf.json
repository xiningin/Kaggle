{"cell_type":{"2eb5653a":"code","4a7eb6fb":"code","adfa2554":"code","0392b43c":"code","0abdc0bd":"code","c0d06d1a":"code","eba30009":"code","5ddf440d":"code","4e19259f":"code","08e28e90":"code","4caa2733":"code","52ae28e8":"code","e54250fc":"code","0c820306":"code","65160a2a":"code","9e683705":"code","e15d7ecb":"code","69ba8bc2":"code","3bc35c81":"code","27f81774":"code","4fefad64":"code","90468873":"code","917ea98b":"code","e067a776":"code","ee404c67":"code","92013733":"code","549efb32":"code","7ad373aa":"code","d9ba216b":"code","255f74f5":"code","54167830":"code","b33db516":"code","962da782":"code","523bbc2f":"code","f0dd3d1a":"code","fc8e26cb":"code","ffb90893":"code","5a52110f":"code","c7bd0052":"code","bb500f9a":"code","f94298a1":"code","ca39cf1b":"code","d7990a2a":"code","a885021a":"code","ea11beb1":"code","5a110fca":"markdown","f39059bc":"markdown","66d0a7db":"markdown","58e2b800":"markdown","9768fa2d":"markdown","716f798d":"markdown","32f1e07b":"markdown"},"source":{"2eb5653a":"# impots \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport itertools\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.ensemble import GradientBoostingClassifier \nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn import tree\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n%matplotlib inline","4a7eb6fb":"df = pd.read_csv('..\/input\/heart.csv')\n#print first 10 rows\ndf.head(10)","adfa2554":"df.info()","0392b43c":"healthy = df[(df['target'] ==0) ].count()[1]\nsick = df[(df['target'] ==1) ].count()[1]\nprint (\"num of pepole without heart deacise: \"+ str(healthy))\nprint (\"num of pepole with chance for heart deacise: \"+ str(sick))","0abdc0bd":"# we will nurmaize the data and split it to test and train.\n# we choose to splite 30-70 because we have a small data set and we want to have enught validetionn examples.\n# split data table into data X and class labels y\n\nX = df.iloc[:,0:13].values\ny = df.iloc[:,13].values\n#nurmalize the data\nfrom sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X)\ndfNorm = pd.DataFrame(X_std, index=df.index, columns=df.columns[0:13])\n# # add non-feature target column to dataframe\ndfNorm['target'] = df['target']\ndfNorm.head(10)\n\nX = dfNorm.iloc[:,0:13].values\ny = dfNorm.iloc[:,13].values","c0d06d1a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size=0.3, random_state=0)\n\nX_train.shape, y_train.shape, X_test.shape , y_test.shape","eba30009":"# # calculate the correlation matrix\ncorr = dfNorm.corr()\n\n# plot the heatmap\nfig = plt.figure(figsize=(5,4))\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns,\n            linewidths=.75)","5ddf440d":"from mlxtend.feature_selection import SequentialFeatureSelector as SFS\ndef sfs_features(algo_namem,features_nums):\n    sfs_name=SFS(algo_namem, \n                k_features=features_nums, \n                forward=True, \n                floating=False,\n                scoring='accuracy',\n                cv=5)\n    return sfs_name","4e19259f":"results_test = {}\nresults_train = {}\nlist_algos=[]","08e28e90":"def prdict_date(algo_name,X_train,y_train,X_test,y_test,atype='',verbose=0):\n    algo_name.fit(X_train, y_train)\n    Y_pred = algo_name.predict(X_test)\n    acc_train = round(algo_name.score(X_train, y_train) * 100, 2)\n    acc_val = round(algo_name.score(X_test, y_test) * 100, 2)\n    \n    results_test[str(algo_name)[0:str(algo_name).find('(')]+'_'+str(atype)] = acc_val\n    results_train[str(algo_name)[0:str(algo_name).find('(')]+'_'+str(atype)] = acc_train\n    list_algos.append(str(algo_name)[0:str(algo_name).find('(')])\n    if verbose ==0:\n        print(\"acc train: \" + str(acc_train))\n        print(\"acc test: \"+ str(acc_val))\n    else:\n        return Y_pred","4caa2733":"def print_fitures(sfs_name='sfs1',verbose=0):\n    a= (sfs_name.k_feature_idx_[0],sfs_name.k_feature_idx_[1],sfs_name.k_feature_idx_[2])\n    if verbose ==0:\n        print('Selected features:', sfs_name.k_feature_idx_)\n        for i in range (len (sfs_name.k_feature_idx_)):\n            print (df.iloc[:,sfs_name.k_feature_idx_[i]].name)\n    return a","52ae28e8":"from sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy import interp\n\ndef roc_graph_cv(algo_name,X,y,cvn=5):\n    # Run classifier with cross-validation and plot ROC curves\n    cv = StratifiedKFold(n_splits=cvn)\n    classifier =algo_name\n\n    tprs = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100)\n\n    i = 0\n    for train, test in cv.split(X, y):\n        probas_ = classifier.fit(X[train], y[train].ravel()).predict_proba(X[test])\n        # Compute ROC curve and area the curve\n        fpr, tpr, thresholds = roc_curve(y[test].ravel(), probas_[:, 1])\n        tprs.append(interp(mean_fpr, fpr, tpr))\n        tprs[-1][0] = 0.0\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        plt.plot(fpr, tpr, lw=1, alpha=0.3,\n                 label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n\n        i += 1\n    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n             label='Luck', alpha=.8)\n\n    mean_tpr = np.mean(tprs, axis=0)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n    std_auc = np.std(aucs)\n    plt.plot(mean_fpr, mean_tpr, color='b',\n             label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n             lw=2, alpha=.8)\n\n    std_tpr = np.std(tprs, axis=0)\n    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n                     label=r'$\\pm$ 1 std. dev.')\n\n    plt.xlim([-0.05, 1.05])\n    plt.ylim([-0.05, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()","e54250fc":"### helping function\n\ndef conf(algo_name,X_test, y_test):\n    y_pred = algo_name.predict(X_test)\n    forest_cm = metrics.confusion_matrix(y_pred, y_test, [1,0])\n    sns.heatmap(forest_cm, annot=True, fmt='.2f',xticklabels = [\"1\", \"0\"] , yticklabels = [\"1\", \"0\"] )\n    plt.ylabel('True class')\n    plt.xlabel('Predicted class')\n    plt.title(str(algo_name)[0:str(algo_name).find('(')])\n","0c820306":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(X)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2'])\nfinalDf = pd.concat([principalDf, df[['target']]], axis = 1)\n\nfig = plt.figure(figsize = (8,6))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\ntargets = [1,0]\ncolors = ['r',  'b']\nfor target, color in zip(targets,colors):\n    indicesToKeep = finalDf['target'] == target\n    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n               , finalDf.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()","65160a2a":"### LDA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda = LinearDiscriminantAnalysis()\nprdict_date(lda,X_train,y_train,X_test,y_test)","9e683705":"#predictusing sfs:\nsfs_1=sfs_features(lda,(1,5))\nsfs_1 = sfs_1.fit(X, y)\nselectedFeatures = print_fitures(sfs_1)\n\n#plot3D(sfs_1.k_feature_idx_[0],sfs_1.k_feature_idx_[1],sfs_1.k_feature_idx_[2],'knn')\n\nX_train_sfs = sfs_1.transform(X_train)\nX_test_sfs = sfs_1.transform(X_test)\nprint ('\\n')\nprdict_date(lda,X_train_sfs,y_train,X_test_sfs,y_test,'sfs')\n\n\n","e15d7ecb":"print(classification_report(y_test, lda.predict(X_test_sfs)))\nconf(lda,X_test_sfs, y_test)\n","69ba8bc2":"roc_graph_cv(lda,X[:,selectedFeatures],y)","3bc35c81":"### RANDOM FOREST\n# Train: Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=50, random_state = 0)\nprdict_date(random_forest,X_train,y_train,X_test,y_test)","27f81774":"feature_importance = random_forest.feature_importances_\nfeat_importances = pd.Series(random_forest.feature_importances_, index=df.columns[:-1])\nfeat_importances = feat_importances.nlargest(13)\n\nfeature = df.columns.values.tolist()[0:-1]\nimportance = sorted(random_forest.feature_importances_.tolist())\n\n\nx_pos = [i for i, _ in enumerate(feature)]\n\nplt.barh(x_pos, importance , color='dodgerblue')\nplt.ylabel(\"feature\")\nplt.xlabel(\"importance\")\nplt.title(\"feature_importances\")\n\nplt.yticks(x_pos, feature)\n\nplt.show()","4fefad64":"#taking the best 5 features give as smaller result.\ncurrlist =[2,12,10,9,11]\n# print (currlist)\n\nrandom_forest = RandomForestClassifier(n_estimators=100, random_state = 0)\nprdict_date(random_forest,X_train[:,currlist],y_train,X_test[:,currlist],y_test,'FS')\n","90468873":"sfs_1=sfs_features(random_forest,(1,5))\nsfs_1 = sfs_1.fit(X, y)\nselectedFeatures = print_fitures(sfs_1)\n\n\nX_train_sfs = sfs_1.transform(X_train)\nX_test_sfs = sfs_1.transform(X_test)\n\nprint (\"\\n\")\nprdict_date(random_forest,X_train_sfs,y_train,X_test_sfs,y_test,'sfs')\n","917ea98b":"print(classification_report(y_test, random_forest.predict(X_test_sfs)))\nconf(random_forest,X_test_sfs, y_test)","e067a776":"roc_graph_cv(random_forest,X[:,selectedFeatures],y)","ee404c67":"### DECISION TREE\n#  descion tree\ndect = tree.DecisionTreeClassifier()\n\nprdict_date(dect,X_train,y_train,X_test,y_test)","92013733":"sfs_1=sfs_features(dect,(1,5))\nsfs_1 = sfs_1.fit(X, y)\nselectedFeatures = print_fitures(sfs_1)\n\nX_train_sfs = sfs_1.transform(X_train)\nX_test_sfs = sfs_1.transform(X_test)\n\nprdict_date(dect,X_train_sfs,y_train,X_test_sfs,y_test,'sfs')\n","549efb32":"print(classification_report(y_test, dect.predict(X_test_sfs)))\nconf(dect,X_test_sfs, y_test)","7ad373aa":"roc_graph_cv(dect,X[:,selectedFeatures],y)","d9ba216b":"# Gradient Boosting\n# Train: Gradient Boosting\ngbc = GradientBoostingClassifier(loss='exponential', learning_rate=0.03, n_estimators=75 , max_depth=6)\nprdict_date(gbc,X_train,y_train,X_test,y_test)\n","255f74f5":"sfs_1=sfs_features(gbc,(1,5))\nsfs_1 = sfs_1.fit(X, y)\nselectedFeatures = print_fitures(sfs_1)\n\n\nX_train_sfs = sfs_1.transform(X_train)\nX_test_sfs = sfs_1.transform(X_test)\n\nprint (\"\\n\")\nprdict_date(gbc,X_train_sfs,y_train,X_test_sfs,y_test,'sfs')","54167830":"print(classification_report(y_test, gbc.predict(X_test_sfs)))\nconf(gbc,X_test_sfs, y_test)","b33db516":"roc_graph_cv(gbc,X[:,selectedFeatures],y)","962da782":"#### KNN \n##to choose the right K we build a loop witch examen all the posible values for K. \nfrom sklearn import model_selection\n\n#Neighbors\nneighbors = [x for x in list(range(1,50)) if x % 2 == 0]\n\n#Create empty list that will hold cv scores\ncv_scores = []\n\n#Perform 10-fold cross validation on training set for odd values of k:\nseed=123\nfor k in neighbors:\n    k_value = k+1\n    knn = KNeighborsClassifier(n_neighbors = k_value, weights='uniform', p=2, metric='euclidean')\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    scores = model_selection.cross_val_score(knn, X_train, y_train, cv=kfold, scoring='accuracy')\n    cv_scores.append(scores.mean()*100)\n    #print(\"k=%d %0.2f (+\/- %0.2f)\" % (k_value, scores.mean()*100, scores.std()*100))\n\noptimal_k = neighbors[cv_scores.index(max(cv_scores))]\nprint(( \"The optimal number of neighbors is %d with %0.1f%%\" % (optimal_k, cv_scores[optimal_k])))\n\nplt.plot(neighbors, cv_scores)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Train Accuracy')\nplt.show()","523bbc2f":"cv_preds = []\n\n#Perform 10-fold cross validation on testing set for odd values of k\nseed=123\nfor k in neighbors:\n    k_value = k+1\n    knn = KNeighborsClassifier(n_neighbors = k_value, weights='uniform', p=2, metric='euclidean')\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    preds = model_selection.cross_val_predict(knn, X_test, y_test, cv=kfold)\n    cv_preds.append(metrics.accuracy_score(y_test, preds)*100)\n    #print(\"k=%d %0.2f\" % (k_value, 100*metrics.accuracy_score(test_y, preds)))\n\noptimal_k = neighbors[cv_preds.index(max(cv_preds))]\nprint(\"The optimal number of neighbors is %d with %0.1f%%\" % (optimal_k, cv_preds[optimal_k]))\n\nplt.plot(neighbors, cv_preds)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Test Accuracy')\nplt.show()","f0dd3d1a":"# KNN\nknn = KNeighborsClassifier(n_neighbors = 6)\nprdict_date(knn,X_train,y_train,X_test,y_test)","fc8e26cb":"sfs_1=sfs_features(knn,(1,5))\nsfs_1 = sfs_1.fit(X, y)\nselectedFeatures = print_fitures(sfs_1)\n\nX_train_sfs = sfs_1.transform(X_train)\nX_test_sfs = sfs_1.transform(X_test)\n\nprdict_date(knn,X_train_sfs,y_train,X_test_sfs,y_test,'sfs')\nprint(classification_report(y_test, knn.predict(X_test_sfs)))\nconf(knn,X_test_sfs, y_test)","ffb90893":"roc_graph_cv(knn,X[:,selectedFeatures],y)\n","5a52110f":"### SVM \n#  SVM\nsvm = SVC(kernel='linear', probability=True)\nprdict_date(svm,X_train,y_train,X_test,y_test,'linear')    ","c7bd0052":"#  SVM\nsvm = SVC(kernel='poly', probability=True)\nprdict_date(svm,X_train,y_train,X_test,y_test,'poly')","bb500f9a":"#  SVM\nsvm = SVC(kernel='rbf', probability=True)\nprdict_date(svm,X_train,y_train,X_test,y_test,'rbf')","f94298a1":"sfs_1=sfs_features(svm,(1,5))\nsfs_1 = sfs_1.fit(X, y)\nselectedFeatures = print_fitures(sfs_1)\n\nX_train_sfs = sfs_1.transform(X_train)\nX_test_sfs = sfs_1.transform(X_test)\n\nprdict_date(svm,X_train_sfs,y_train,X_test_sfs,y_test,'sfs')\nprint(classification_report(y_test, svm.predict(X_test_sfs)))\nconf(svm,X_test_sfs, y_test)\n","ca39cf1b":"roc_graph_cv(svm,X[:,selectedFeatures],y)","d7990a2a":"# print (results_test)\n\ndf_test =pd.DataFrame(list(results_test.items()),\n                      columns=['algo_name','acc_test'])\ndf_train =pd.DataFrame(list(results_train.items()),\n                      columns=['algo_name','acc_train'])\ndf_results = df_test.join(df_train.set_index('algo_name'), on='algo_name')\ndf_results.sort_values('acc_test',ascending=False)","a885021a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\n# set jupyter's max row display\npd.set_option('display.max_row', 100)\n\n# set jupyter's max column width to 50\npd.set_option('display.max_columns', 50)\n\n# Load the dataset\nax = df_results[['acc_test', 'acc_train']].plot(kind='barh',\n              figsize=(10,7), color=['dodgerblue', 'slategray'], fontsize=13);\nax.set_alpha(0.8)\nax.set_title(\"The Best ALGO with SFS is?\",\nfontsize=18)\nax.set_xlabel(\"ACC\", fontsize=18)\nax.set_ylabel(\"Algo Names\", fontsize=18)\nax.set_xticks([0,10,20,30,40,50,60,70,80,90,100,110])\nax.set_yticklabels(df_results.iloc[:,0].values.tolist())\n\n# set individual bar lables using above list\nfor i in ax.patches:\n    # get_width pulls left or right; get_y pushes up or down\n    ax.text(i.get_width()+7, i.get_y()+.1, \\\n            str(round((i.get_width()), 2)), fontsize=11, color='dimgrey')\n\n# invert for largest on top \nax.invert_yaxis()","ea11beb1":"### NUERAL NETWORK\n\nxtrain = X_train\nytrain = y_train\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\ntorch.manual_seed(42)\n\n#hyperparameters\n# h1 = 6\n# h2 = 4\n# lr = 0.0023\n# num_epoch =7000\n\nh1 = 6\nh2 = 4\nlr = 0.0023\nnum_epoch =7000\n\n#build model\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(13, h1)\n        self.fc2 = nn.Linear(h1, h2)\n        self.fc3 = nn.Linear(h2, 2)\n    \n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        #return  F.log_softmax(x, dim=1)\n        return x\nnet = Net()\n\n#choose optimizer and loss function\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.7)\n\nloss_per_epoch = []\n\n#train\nfor epoch in range(num_epoch):\n    X = Variable(torch.Tensor(xtrain).float())\n    Y = Variable(torch.Tensor(ytrain).long())\n\n    #feedforward - backprop\n    optimizer.zero_grad()\n    out = net(X)\n    loss = criterion(out, Y)\n    loss.backward()\n    optimizer.step()\n\n#     if (epoch) % 50 == 0:\n#         print ('Epoch [%d\/%d] Loss: %.4f' \n#                    %(epoch+1, num_epoch, loss.data[0]))\n    loss_per_epoch.append( loss.item()) \n        \nepochs = np.arange(1,num_epoch + 1)\nplt.plot(epochs, loss_per_epoch, label='Training')\nplt.ylabel('Average Loss')\nplt.xlabel('Epochs')\nplt.title('heart')\nplt.legend()\nplt.show()       \n\nxtest = X_test\nytest = y_test\n\n#get prediction\nX = Variable(torch.Tensor(xtest).float())\nY = torch.Tensor(ytest).long()\nout = net(X)\n_, predicted = torch.max(out.data, 1)\n\n\n#get prediction train\nXt = Variable(torch.Tensor(xtrain).float())\nYt = torch.Tensor(ytrain).long()\nout = net(Xt)\n_, predicted_train = torch.max(out.data, 1)\n\n#get accuration\nprint('Accuracy of the network %d %%' % (100 * torch.sum(Y==predicted) \/ len(y_test)))\nprint('Accuracy of the train %d %%' % (100 * torch.sum(Yt==predicted_train) \/ len(y_train)))\n","5a110fca":"### <font color='#891a1f'> our \"goal\" predict the presence of heart disease in the patient.<\/font>","f39059bc":"### helping functions:","66d0a7db":"# <font color='#891a1f'>Table Reasults<\/font>\n****************************************","58e2b800":"## Define The Algorithems\nfirst we will run eatch algorithem on all the features <br>\nthen we will use SFS to compere and cheack improvment.\nwe will use sfs to take aoutimaticly from 1 to 5 features as the algorithem sujests","9768fa2d":"# <font color='#b20c14'>Heart<\/font>\n","716f798d":"## PCA\n> Principal component analysis (PCA) is a technique used to emphasize variation and bring out strong patterns in a dataset.<br>\nwe use it first make data easy to explore and visualize.","32f1e07b":"we can see that we have a small 303 rows data set. our data has no nulls and no other chars to represent it.\nall our data is numeric - therefore, no enumeration needed."}}