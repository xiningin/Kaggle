{"cell_type":{"9f6ef6dd":"code","c5e1b715":"code","ad1ff6e6":"code","b9493c22":"code","ebabcd72":"code","d594e7fd":"code","ec097255":"code","ffce50fe":"code","95b35773":"code","86499547":"code","2576d0af":"code","0bb6db52":"code","c3ca9728":"code","ca7b399b":"code","2c8e6c3b":"code","84cade06":"code","b557a41b":"code","46bed152":"code","1b1d5060":"code","79da4b58":"code","cc399170":"code","0b3e6a3d":"code","e7255a89":"code","736daa86":"code","555d9d1a":"code","c9831260":"code","037f7d2f":"code","b7d98fd3":"code","3068198c":"code","323ed694":"code","1d0dc12d":"code","0fd5df5c":"code","0889c02e":"code","72adf9a8":"code","593edec8":"code","33719661":"code","871d04b5":"code","b7fb6da7":"code","a1e09d2a":"code","969c8939":"code","bd926be3":"code","02eae9f3":"code","b8e1d4c9":"code","eb50c214":"code","7ec91c96":"code","95a1e2b6":"code","8ace6f77":"code","39067406":"code","f821411a":"code","d98732ef":"code","38d4c7e1":"code","173535a4":"code","75dcbcf9":"code","78db55bb":"code","a3912056":"code","907a8eec":"code","b52b0b86":"code","0f0791c1":"code","6885c43c":"code","c490e878":"code","5bdc0b3f":"code","b934ce9b":"code","646b9055":"code","892f50ee":"code","46d28197":"code","d3a67106":"code","8d9313b3":"code","70b13a47":"code","bbb0d462":"code","0b375613":"markdown","8566e2a4":"markdown","eae2b8a6":"markdown","b471f0b0":"markdown","58cd427b":"markdown","28056a74":"markdown","5e047dfe":"markdown","b6028fcd":"markdown","4eea8e22":"markdown","a2438e00":"markdown","e436b747":"markdown","4be961bd":"markdown","e9805241":"markdown","f8fd9edd":"markdown","664ab141":"markdown","cb8f0372":"markdown","3b3257d0":"markdown","ac3d2913":"markdown","01fa3e3d":"markdown","2c5533a3":"markdown","4ab1fce9":"markdown","c2c5eca4":"markdown","9deea9b9":"markdown","eefa4521":"markdown","74e497de":"markdown","16d5bf84":"markdown","50546518":"markdown","9d47a4ee":"markdown","6a716bd5":"markdown","47205b8b":"markdown","4e06fd81":"markdown","3255287d":"markdown","bd2edcb8":"markdown","d7d966a9":"markdown","5ebedba8":"markdown","4239818d":"markdown","bc668c52":"markdown"},"source":{"9f6ef6dd":"# Import Packages\nimport os\nimport re\nimport nltk\nimport json\nimport torch\nimport nltk.corpus  \nimport pandas as pd\nimport numpy as np\nfrom copy import deepcopy\nfrom nltk.stem import PorterStemmer\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nnltk.download('punkt')\nnltk.download('stopwords')\n\nfrom pathlib import Path","c5e1b715":"\n# Text Preprocessing `clean_sent()`----------------------------------------------------------------------\nporter_stemmer = PorterStemmer()\ndef clean_sent(sentence):\n    \"\"\"\n    Clean the sentence\n    :param sentence: text to to be cleaned\n    :return: text that has been cleaned\n    \"\"\"\n    #nltk.FreqDist(words).most_common(10)\n    stopwords = set(nltk.corpus.stopwords.words('english'))\n    words = sentence.split()\n    # Lowercase all words (default_stopwords are lowercase too)\n    words = [word.lower() for word in words]\n    #words = sentence\n    words = [word for word in words if len(word) > 1]\n    # Remove numbers\n    words = [word for word in words if not word.isnumeric()]\n    # Remove punctuation\n    words = [word for word in words if word.isalpha()]\n    # Remove stopwords\n    words = [word for word in words if word not in stopwords]\n    # Porter\n    words = [porter_stemmer.stem(word) for word in words]\n    #fdist = nltk.FreqDist(words_lc)   \n    return \" \".join(words)\n\n\n## Data Load----------------------------------------------------------------------\ndef format_name(author):\n    middle_name = \" \".join(author['middle'])\n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n\n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n\n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:\n            name_ls.append(name)\n\n    return \", \".join(name_ls)\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n\n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n\n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n\n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'],\n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n\n    return \"; \".join(formatted)\n\ndef load_files(dirname):\n    filenames = os.listdir(dirname)\n    raw_files = []\n\n    for filename in tqdm(filenames):\n        filename = dirname + filename\n        file = json.load(open(filename, 'rb'))\n        raw_files.append(file)\n\n    return raw_files\n\ndef clean_pdf_files(file_list, keyword_list):\n    nth_paper=0\n    cleaned_files=[]\n    for file in file_list:\n        with open(file) as f:\n            file=json.load(f)\n        features = [\n            file['paper_id'],\n            file['metadata']['title'],\n            format_authors(file['metadata']['authors']),\n            format_authors(file['metadata']['authors'],\n                           with_affiliation=True),\n            format_body(file['abstract']),\n            format_body(file['body_text']),\n            format_bib(file['bib_entries']),\n            file['metadata']['authors'],\n            file['bib_entries']\n        ]\n        if(nth_paper%1000)==0:\n            print(nth_paper)\n        nth_paper=nth_paper+1\n\n        has_keyword = False\n        for keyword in keyword_list:\n            if keyword in features[5]:\n                has_keyword = True\n                break\n        if has_keyword == True:\n            cleaned_files.append(features)\n    col_names = ['paper_id', 'title', 'authors',\n                 'affiliations', 'abstract', 'text',\n                 'bibliography','raw_authors','raw_bibliography']\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    return clean_df\n\n\n# Similarity ----------------------------------------------------------------------\ndef calc_simlarity_score(question_list, text_list,threshold=None, top=None):\n    \"\"\"\n    Calculate the cosine similarity score for each pair of question and text with given questions and text\n    \n    :param question_list: a list - questions based on which the relevant articles are searched for \n    :param text_list: a list - the list of paper contents that the relevant articles are searched upon\n    :param text_list: a list - the list of paper contents that the relevant articles are searched upon\n    :param threshold: a decimal - the minimum cosine similarity score between questions and returned papers\n    :param top: a number - the number of papers with highest score to be returned\n    \n    :return: dic - A dictionary with question index as key, and relevant paper index and similarity score as value\n    :return: sim_matrix - A matrix with cosine similarity score of all pairs of questions and papers\n    \"\"\"\n    if (threshold==None)  and  (top==None):\n        raise ValueError(\"Parameter `threshold` and `top` cannot both be None\")\n    dic = {}\n    tfidf = TfidfVectorizer()\n    corpus_tfidf_matrix = tfidf.fit_transform(text_list)\n    ques_tfidf_matrix = tfidf.transform(question_list)\n    sim_matrix = cosine_similarity(corpus_tfidf_matrix, ques_tfidf_matrix)\n    for ques_idx in range(sim_matrix.shape[1]):\n        dic[ques_idx] = []\n        if threshold != None:\n            if (threshold>1) or (threshold <0):\n                raise ValueError(\"Please enter a value from 0 to 1 for parameter `threshold`\")\n            for paper_idx in range(sim_matrix.shape[0]):\n                score = sim_matrix[paper_idx, ques_idx]\n                if score >= threshold:\n                    dic[ques_idx].append((paper_idx, score))\n            dic[ques_idx]=sorted(dic[ques_idx], key=lambda i: i[1], reverse=True)\n        elif top != None:\n            top_paper_idx_list = sorted(range(len(sim_matrix[:, ques_idx])), key=lambda i: sim_matrix[:,0][i], reverse=True)[:top]\n            dic[ques_idx] = [(top_idx, sim_matrix[top_idx, ques_idx]) for top_idx in top_paper_idx_list]\n    return dic, sim_matrix\n\n# Retrieve relevant paper----------------------------------------------------------------------\ndef retrieve_paper(df, dic):\n    df_dic={}\n    for ques_idx in dic:\n        new_df = df.iloc[[item[0] for item in dic[ques_idx]], :]\n        new_df['score'] = [item[1] for item in dic[ques_idx]]\n        new_df['question'] = questions[ques_idx]\n        df_dic[ques_idx]=new_df.copy()\n    return df_dic\n","ad1ff6e6":"# Set parameters\npath = '\/kaggle\/input\/CORD-19-research-challenge\/document_parses\/pdf_json'\nkeyword_list = ['novel coronavirus', 'novel-coronavirus', 'coronavirus-2019', \n                'sars-cov-2', 'sarscov2', 'covid-19', 'covid19',\n                '2019ncov', '2019-ncov', 'wuhan']\n\n# Get list of file paths\nfile_list = [os.path.join(r, file)  for r, _, f in os.walk(path)  for file in f]\n\n# Clean \uff08This takes ~15 min\uff09\nclean_pdf_df = clean_pdf_files(file_list, keyword_list)","b9493c22":"# Append additional info from metadata to main df\nmetadata = pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\")\nclean_pdf_df = clean_pdf_df.merge(metadata[['sha', 'title', 'authors', 'abstract', 'doi', 'publish_time', 'journal']], \n                                  how ='left', left_on='paper_id', right_on='sha')\n\n# Clean columns\nclean_pdf_df['title_x'] = clean_pdf_df['title_x'].fillna(clean_pdf_df['title_y'])\nclean_pdf_df['authors_x'] = clean_pdf_df['authors_x'].fillna(clean_pdf_df['authors_y'])\nclean_pdf_df['abstract_x'] = clean_pdf_df['abstract_x'].fillna(clean_pdf_df['abstract_y'])\nclean_pdf_df = clean_pdf_df.drop(['sha', 'title_y', 'authors_y', 'abstract_y'], axis=1)\nclean_pdf_df = clean_pdf_df.rename(columns={'title_x': 'title', 'authors_x': 'authors', 'abstract_x': 'abstract'})","ebabcd72":"clean_pdf_df['text_cleaned'] = clean_pdf_df.apply(lambda row: clean_sent(row['text']), axis=1)","d594e7fd":"clean_pdf_df.to_pickle((\"\/kaggle\/working\/clean_pdf_df.pkl\"))\n# clean_pdf_df = pd.read_pickle(\"\/kaggle\/working\/clean_pdf_df.pkl\")","ec097255":"# set text\ntext_cleaned = clean_pdf_df['text_cleaned']\n\n# set questions\npath = '\/kaggle\/input\/CORD-19-research-challenge\/Kaggle\/target_tables\/1_population\/'\nfile_list = sorted(list(Path(path).glob('*.csv')))\nquestions = [file.name.split(\".csv\")[0].strip('_') for file in file_list]\nfor i,q in enumerate(questions):\n    print(\"Question\" ,i + 1,\":\",q)","ffce50fe":"questions_alt = [\n    'How are patients from poorer areas or lower socialeconomic status or live in proverty managed',\n    'How to reach marginalized, rural, low-income and disadvantaged populations',\n    'How to control the spread in communities',\n    'How to communicate with target high risk population',\n    'How to combat and overcome resource failures or shortages',\n    'How to create hospital infrastructure to prevent nosocomial outbreaks',\n]","95b35773":"# questions_cleaned = [clean_sent(ques) for ques in questions]\nquestions_cleaned = [clean_sent(ques) for ques in questions_alt]","86499547":"# Select relevant paper to \ndic, sim_matrix = calc_simlarity_score(questions_cleaned, text_cleaned, threshold=0.13)#, top=10)\nrelevant_paper_dic = retrieve_paper(clean_pdf_df, dic)","2576d0af":"[relevant_paper_dic[i].shape[0] for i in relevant_paper_dic.keys()]","0bb6db52":"file_list = [os.path.join(r, file)  for r, _, f in os.walk(path)  for file in f]\ntable_cols_dic={}\ntable_dic={}\ntarget_table_dic={}\nfor i,file in enumerate(file_list):\n    df=pd.read_csv(file)\n    cols=list(df.columns)\n    table_cols_dic[i]=cols[1:]\n    table_dic[i]=df\n    target_table_dic[i]=pd.DataFrame(columns=cols)\n_ = [print(table_cols_dic[key]) for key in table_cols_dic.keys()]","c3ca9728":"for key in target_table_dic.keys():\n    target_table_dic[key][['Date', 'Study', 'Journal']] = relevant_paper_dic[key][['publish_time', 'title', 'journal']]\n    target_table_dic[key]['Study Link'] = \"https:\/\/doi.org\/\" + relevant_paper_dic[key]['doi']\n    relevant_paper_dic[key]=relevant_paper_dic[key].reset_index(drop=True)\n    target_table_dic[key]=target_table_dic[key].reset_index(drop=True)\n    target_table_dic[key]['Added on'] = \"10-Jun-2020\"","ca7b399b":"STUDY_TYPES = [\n    char.lower() for char in [\n        'Systematic review', \n        'meta-analysis', \n        'Prospective observational', \n        'Retrospective observational', \n        'Observational', \n        'Cross-sectional', \n        'Case series', \n        'Expert review', \n        'Editorial', \n        'Simulation', \n    ]\n]","2c8e6c3b":"index = 0","84cade06":"print(questions[index])\nprint(questions_alt[index])","b557a41b":"excerpt_list=[]\nfor i, row in relevant_paper_dic[index].iterrows():    \n    # Divide to sentences\n    sent_list = [sent.strip(' ') for sent in re.split(' \\.|\\.(?=[A-Z])|\\. (?=[A-Z])|\\n', row['text'])]\n    \n    \n    # Find relevant sentences\n    relevant_sent_list = [\n        sent for sent in sent_list \n        if any([kw in sent.lower() for kw in [\n            'underhoused', 'poor', 'underpaid', 'socialeconomic', 'proverty', 'unemployed', 'low income', 'no income', 'disparities', \n            'metropolitan'\n        ]])\n    ]\n    print(f\"# relevant sentences: {len(relevant_sent_list)}\")\n    # Join sentences to form excerpt\n    excerpt_string = '; \\n'.join([sent for sent in relevant_sent_list])\n    excerpt_list.append(excerpt_string)\n    \ntarget_table_dic[index][\"Excerpt\"]=excerpt_list","46bed152":"pd.set_option('display.max_colwidth', None)","1b1d5060":"target_table_dic[index] = target_table_dic[index][target_table_dic[index]['Excerpt'].str.len().gt(0)]","79da4b58":"target_table_dic[index] = target_table_dic[index].iloc[:2, 1:]","cc399170":"target_table_dic[index].loc[target_table_dic[index].index == 1, 'Study'] = \"Letter to the Editor\"","0b3e6a3d":"target_table_dic[index].loc[:, 'Addressed Population'] = [\n    'urban and rural elderly people',\n    'rural areas of the USA',\n]\ntarget_table_dic[index].loc[:, 'Challenge'] = [\n    'Profound health disparities exist in the time of emerging epidemic',\n    'The trickle-down effect for people with CF living in poorly-resourced rural counties with limited financial capital and limited pathways for individual economic and social advancement may be significant',\n]\n\ntarget_table_dic[index].loc[:, 'Proposed Solution'] = [\n    'Health care providers, especially those serving vulnerable population, should be vigilant about undiagnosed patients',\n    'None proposed',\n]\ntarget_table_dic[index].loc[:, 'Measure of Evidence'] = [\n    'p=0.01 and p=0.05',\n    'None provided',\n]","e7255a89":"study_types = []\nfor i, row in target_table_dic[index].iterrows():\n    paper_data = relevant_paper_dic[index][relevant_paper_dic[index]['doi'] == row['Study Link'][16:]].iloc[0]\n    for st in STUDY_TYPES:\n        if st in paper_data['text'].lower():\n            study_types.append(st)\n            break\nprint(study_types)","736daa86":"target_table_dic[index]","555d9d1a":"index = 1","c9831260":"print(questions[index])\nprint(questions_alt[index])","037f7d2f":"excerpt_list=[]\nfor i, row in relevant_paper_dic[index].iterrows():    \n    # Divide to sentences\n    sent_list = [sent.strip(' ') for sent in re.split(' \\.|\\.(?=[A-Z])|\\. (?=[A-Z])|\\n', row['text'])]\n    \n    \n    # Find relevant sentences\n    relevant_sent_list = [\n        sent for sent in sent_list \n        if any([kw in sent.lower() for kw in [\n            'marginalize', 'disadvantage', 'poverty', 'reach', 'statistical significan', 'confidence interval'\n        ]])\n    ]\n    print(f\"# relevant sentences: {len(relevant_sent_list)}\")\n    # Join sentences to form excerpt\n    excerpt_string = '; \\n'.join([sent for sent in relevant_sent_list])\n    excerpt_list.append(excerpt_string)\n    \ntarget_table_dic[index][\"Excerpt\"]=excerpt_list","b7d98fd3":"study_types = []\nfor i, row in target_table_dic[index].iterrows():\n    paper_data = relevant_paper_dic[index][relevant_paper_dic[index]['doi'] == row['Study Link'][16:]].iloc[0]\n    for st in STUDY_TYPES:\n        if st in paper_data['text'].lower():\n            study_types.append(st)\n            break\nprint(study_types)","3068198c":"target_table_dic[index].loc[:, 'Study Type'] = [\n    'Systematic review and meta-analysis',\n]\ntarget_table_dic[index].loc[:, 'Addressed Population'] = [\n    'vulnerable and disadvantaged populations',\n]\ntarget_table_dic[index].loc[:, 'Challenge'] = [\n    'When the distribution of power, money, and resources systematically disadvantages some individuals and groups, this leads to avoidable differences in health outcomes, referred to as health inequity, within countries but also between countries',\n]\n\ntarget_table_dic[index].loc[:, 'Solution'] = [\n    'None proposed',\n]\ntarget_table_dic[index].loc[:, 'Measure of Evidence'] = [\n    'None provided',\n]","323ed694":"target_table_dic[index]","1d0dc12d":"index = 2","0fd5df5c":"print(questions[index])\nprint(questions_alt[index])","0889c02e":"excerpt_list=[]\n\nfor i, row in relevant_paper_dic[index].iterrows():    \n    # Divide to sentences\n    sent_list = [sent.strip(' ') for sent in re.split(' \\.|\\.(?=[A-Z])|\\. (?=[A-Z])|\\n', row['text'])]\n    \n    \n    # Find relevant sentences\n    relevant_sent_list = [\n        sent for sent in sent_list \n        if any([re.search(pat, sent.lower()) is not None for pat in [\n            '(control)(.*)(spread)', '(spread)(.*)( communit)', '(control)(.*)(virus)', '(virus)(.*)(communit)'\n        ]])\n    ]\n    if len(relevant_sent_list) > 0:\n        relevant_sent_list += [\n        sent for sent in sent_list \n        if (any([re.search(pat, sent.lower()) is not None for pat in [\n            \"(statistical)(.*)(signific)\", \"confidence interval\", \"evidence\"\n        ]])) and not sent in relevant_sent_list\n    ]\n    print(f\"# relevant sentences: {len(relevant_sent_list)}\")\n    # Join sentences to form excerpt\n    excerpt_string = '; \\n'.join([sent for sent in relevant_sent_list])\n    excerpt_list.append(excerpt_string)\n    \ntarget_table_dic[index][\"Excerpt\"]=excerpt_list","72adf9a8":"target_table_dic[index] = target_table_dic[index][target_table_dic[index]['Excerpt'].str.len().gt(0)]","593edec8":"target_table_dic[index] = target_table_dic[index].iloc[:15, :]","33719661":"target_table_dic[index] = target_table_dic[index].iloc[[i for i in range(15) if i != 3], :]","871d04b5":"target_table_dic[index].loc[target_table_dic[index].index == 4, 'Study'] = \"Community pharmacist in public health emergencies: Quick to action against the coronavirus 2019-nCoV outbreak\"\ntarget_table_dic[index].loc[target_table_dic[index].index == 4, 'Journal'] = \"Elsevier Public Health Emergency Collection\"\ntarget_table_dic[index].loc[target_table_dic[index].index == 4, 'Study Link'] = \"https:\/\/doi.org\/10.1016\/j.sapharm.2020.02.003\"","b7fb6da7":"target_table_dic[index] = target_table_dic[index].iloc[[i for i in range(14) if i != 9], :]","a1e09d2a":"target_table_dic[index].loc[target_table_dic[index].index == 16, 'Study'] = \"Editorial: The explosive epidemic outbreak of novel coronavirus disease 2019 (COVID-19) and the persistent threat of respiratory tract infectious diseases to global health security\"\ntarget_table_dic[index].loc[target_table_dic[index].index == 16, 'Journal'] = \"Current Opinion in Pulmonary Medicine\"","969c8939":"study_types = []\nfor i, row in target_table_dic[index].iterrows():\n    try:\n        if pd.isnull(row['Study Link']):\n            paper_data = relevant_paper_dic[index][relevant_paper_dic[index]['title'] == row['Study']].iloc[0]\n        else:\n            paper_data = relevant_paper_dic[index][relevant_paper_dic[index]['doi'] == row['Study Link'][16:]].iloc[0]\n        found = False\n        for st in STUDY_TYPES:\n            if st in paper_data['text'].lower():\n                study_types.append(st)\n                found = True\n                break\n        if not found:\n            study_types.append(None)\n    except:\n        study_types.append(None)\nprint(study_types)","bd926be3":"study_types[1] = \"Systematic review and meta-analysis\"\nstudy_types[11] = \"Editorial\"\ntarget_table_dic[index].loc[:, 'Study Type'] = study_types\ntarget_table_dic[index].loc[:, 'Addressed Population'] = [\n    'Not specified',\n    \"Not specified\",\n    \"Not specified\",\n    \"England and Wales\",\n    \"Not specified\",\n    \"China\",\n    \"United States\",\n    \"Not specified\",\n    \"Not specified\",\n    \"Not specified\",\n    \"Not specified\",\n    \"haematology patients\",\n    \"China\",\n]\ntarget_table_dic[index].loc[:, 'Challenge'] = [\n    'Contain the spread of virus',\n    \"epidemic prevention and control\",\n    'avoid or minimize the last stage of \"community transmission\"',\n    \"containing outbreaks\",\n    \"SARS-CoV-2 probably started to spread locally long before community transmission was officially recognized and control measures for social distancing and air travel restrictions were implemented\",\n    \"prevent and control the spread of COVID -19 in China\",\n    \"Healthcare disparities and challenges\",\n    \"prevent spread from imported cases and re-establishment of community transmission\",\n    \"contain community transmission\", \n    \"prevention and control of SARS Coronavirus-2 disease\",\n    \"public health infection control\",\n    \"reduce haematology patients' risk during ongoing care in the middle of COVID-19\",\n    \"control the spread of the epidemic\",\n]\n\ntarget_table_dic[index].loc[:, 'Solution'] = [\n    'the international community, by contrast, views virus sharing as a critical element of risk communication and, by extension, a global risk management issue',\n    \"This indicates that after controlling other factors affecting the COVID-19, the spread of information has significantly reduced the spread of COVID-19 in the country; \",\n    \"Routine hand hygiene is essential for preventing the spread of the virus and requires the correct use of alcohol-based formulation.\\nHowever, community pharmacists encountered a major issue soon after the news about the potential spread of 2019-CoV to the citya foreseeable short supply of hand hygiene products which were all imported products\",\n    \"imposing additional targeted lockdowns or mass-testing at the community-level can be effective at containing outbreaks\",\n    \"the mortality rate will probably not exceed 1% of the total number of infected individuals.\",\n    \"In the prevention and control of the COVID-19 epidemic, traffic control and social distancing measures have played a very good role in controlling the spread of the epidemic, but it also makes it difficult to deliver supplies equipment and humanitarian aid to the affected areas\",\n    \"we need to engage communities to make it more of a reality for all Americans\",\n    \"Suppression measures can keep transmission and prevalence low, decreasing the effective reproduction number (R e ). 36 Once R e is below 1 in a community, spread in that community should eventually stop; \",\n    \"community quarantine is also needed with rigorous implementation of social distancing\",\n    \"we simulate the optimal systems when all the three control variables (such as educational campaign, social distancing and treatment control) for coronavirus-2 disease (COVID 19) are employed\",\n    \"A specific molecular test for SARS-CoV-2 was developed and a flurry of investigations and research on COVID-19 outbreak rapidly defined the epidemiological, virologic, and clinical features and provided evidence of humanto-human transmission in community, household, and hospital settings\",\n    \"In the case of widespread community infection and substantial pressures on hospitals, it will be hard to justify the prolonged immunosuppression, close follow-up and additional demands on hospital services that come with performing allogeneic (and to a lesser extent, autologous) bone marrow transplants\",\n    \"The Wuhan closure, work stoppage, school suspension and community closed management, as well as the first-level response to major public health events launched nationwide, are measures that effectively controlled the movement of people and the spread of the epidemic; \",\n]\ntarget_table_dic[index].loc[:, 'Measure of Evidence'] = [\n    'None provided',\n    \"statistically significant\",\n    'None provided',\n    'None provided',\n    'None provided',\n    \"None provided\",\n    \"None provided\",\n    \"None provided\",\n    \"None provided\",\n    \"None provided\",\n    \"None provided\",\n    \"None provided\",\n    \"None provided\",\n]","02eae9f3":"target_table_dic[index]","b8e1d4c9":"index = 3","eb50c214":"print(questions[index])\nprint(questions_alt[index])","7ec91c96":"excerpt_list=[]\nfor i, row in relevant_paper_dic[index].iterrows():    \n    # Divide to sentences\n    sent_list = [sent.strip(' ') for sent in re.split(' \\.|\\.(?=[A-Z])|\\. (?=[A-Z])|\\n', row['text'])]\n    \n    \n    # Find relevant sentences\n    relevant_sent_list = [\n        sent for sent in sent_list \n        if any([re.search(pat, sent.lower()) is not None for pat in [\n            '(communicat)(.*)(target)', '(communicat)(.*)(high[- ]risk)', '(communicat)(.*)(vulnerable)', '(communicat)(.*)(population)'\n        ]])\n    ]\n    if len(relevant_sent_list) > 0:\n        relevant_sent_list += [\n        sent for sent in sent_list \n        if (any([re.search(pat, sent.lower()) is not None for pat in [\n            \"(statistical)(.*)(signific)\", \"confidence interval\", \"evidence\"\n        ]])) and not sent in relevant_sent_list\n    ]\n    print(f\"# relevant sentences: {len(relevant_sent_list)}\")\n    # Join sentences to form excerpt\n    excerpt_string = '; \\n'.join([sent for sent in relevant_sent_list])\n    excerpt_list.append(excerpt_string)\n    \ntarget_table_dic[index][\"Excerpt\"]=excerpt_list","95a1e2b6":"target_table_dic[index] = target_table_dic[index][target_table_dic[index]['Excerpt'].str.len().gt(0)]","8ace6f77":"target_table_dic[index] = target_table_dic[index].iloc[[1], :]","39067406":"study_types = []\nfor i, row in target_table_dic[index].iterrows():\n    try:\n        if pd.isnull(row['Study Link']):\n            paper_data = relevant_paper_dic[index][relevant_paper_dic[index]['title'] == row['Study']].iloc[0]\n        else:\n            paper_data = relevant_paper_dic[index][relevant_paper_dic[index]['doi'] == row['Study Link'][16:]].iloc[0]\n        found = False\n        for st in STUDY_TYPES:\n            if st in paper_data['text'].lower():\n                study_types.append(st)\n                found = True\n                break\n        if not found:\n            study_types.append(None)\n    except:\n        study_types.append(None)\nprint(study_types)","f821411a":"target_table_dic[index].loc[:, 'Study Type'] = study_types\ntarget_table_dic[index].loc[:, 'Addressed Population'] = [\"marginalized patient populations and families\"]\ntarget_table_dic[index].loc[:, 'Challenge'] = [\n    'patients and families might feel abandoned, as well as how poor communication and lack of resources to attend to psychosocial needs are often overlooked in life-threatening illness',\n]\n\ntarget_table_dic[index].loc[:, 'Solution'] = [\n    \"when communities finally got involved, not only in research and intervention design, but also in the implementation and the evaluation of solutions, and in building trust in the community about the recommendations for protection\", \n]\ntarget_table_dic[index].loc[:, 'Measure of Evidence'] = [\n    'None provided',\n]","d98732ef":"target_table_dic[index]","38d4c7e1":"index = 4","173535a4":"print(questions[index])\nprint(questions_alt[index])","75dcbcf9":"excerpt_list=[]\nfor i, row in relevant_paper_dic[index].iterrows():    \n    # Divide to sentences\n    sent_list = [sent.strip(' ') for sent in re.split(' \\.|\\.(?=[A-Z])|\\. (?=[A-Z])|\\n', row['text'])]\n    \n    \n    # Find relevant sentences\n    relevant_sent_list = [\n        sent for sent in sent_list \n        if any([re.search(pat, sent.lower()) is not None for pat in [\n            '(resource)(.*)(fail)', '(resource)(.*)(short)', '(combat)(.*)(fail)', '(overcom)(.*)(fail)', '(short)(.*)(resource)'\n        ]])\n    ]\n    if len(relevant_sent_list) > 0:\n        relevant_sent_list += [\n        sent for sent in sent_list \n        if (any([re.search(pat, sent.lower()) is not None for pat in [\n            \"(statistical)(.*)(signific)\", \"confidence interval\", \"evidence\", 'population'\n        ]])) and not sent in relevant_sent_list\n    ]\n    print(f\"# relevant sentences: {len(relevant_sent_list)}\")\n    # Join sentences to form excerpt\n    excerpt_string = '; \\n'.join([sent for sent in relevant_sent_list])\n    excerpt_list.append(excerpt_string)\n    \ntarget_table_dic[index][\"Excerpt\"]=excerpt_list","78db55bb":"target_table_dic[index] = target_table_dic[index][target_table_dic[index]['Excerpt'].str.len().gt(0)]","a3912056":"study_types = []\nfor i, row in target_table_dic[index].iterrows():\n    try:\n        if pd.isnull(row['Study Link']):\n            paper_data = relevant_paper_dic[index][relevant_paper_dic[index]['title'] == row['Study']].iloc[0]\n        else:\n            paper_data = relevant_paper_dic[index][relevant_paper_dic[index]['doi'] == row['Study Link'][16:]].iloc[0]\n        found = False\n        for st in STUDY_TYPES:\n            if st in paper_data['text'].lower():\n                study_types.append(st)\n                found = True\n                break\n        if not found:\n            study_types.append(None)\n    except:\n        study_types.append(None)\nprint(study_types)","907a8eec":"target_table_dic[index].loc[:, 'Addressed Population'] = [\"Not specified\"]\ntarget_table_dic[index].loc[:, 'Challenge'] = [\"The surge in infections has led to a severe shortage of medical resources\"]\ntarget_table_dic[index].loc[:, 'Solution'] = [\"Besides, we observe that there are only two possible stationary state: the whole healthy and the whole infected of the population\"]\ntarget_table_dic[index].loc[:, 'Measure of Evidence'] = [\"Not provided\"]","b52b0b86":"target_table_dic[index]","0f0791c1":"index = 5","6885c43c":"print(questions[index])\nprint(questions_alt[index])","c490e878":"excerpt_list=[]\nfor i, row in relevant_paper_dic[index].iterrows():    \n    # Divide to sentences\n    sent_list = [sent.strip(' ') for sent in re.split(' \\.|\\.(?=[A-Z])|\\. (?=[A-Z])|\\n', row['text'])]\n    \n    \n    # Find relevant sentences\n    relevant_sent_list = [\n        sent for sent in sent_list \n        if any([re.search(pat, sent.lower()) is not None for pat in [\n            '(hospital)(.*)(infrastructure)', '(nosocomial)', '(infrastructure)(.*)(hospital)'\n        ]])\n    ]\n    if len(relevant_sent_list) > 0:\n        relevant_sent_list += [\n        sent for sent in sent_list \n        if (any([re.search(pat, sent.lower()) is not None for pat in [\n            \"(statistical)(.*)(signific)\", \"confidence interval\", \"evidence\"\n        ]])) and not sent in relevant_sent_list\n    ]\n    print(f\"# relevant sentences: {len(relevant_sent_list)}\")\n    # Join sentences to form excerpt\n    excerpt_string = '; \\n'.join([sent for sent in relevant_sent_list])\n    excerpt_list.append(excerpt_string)\n    \ntarget_table_dic[index][\"Excerpt\"]=excerpt_list","5bdc0b3f":"target_table_dic[index] = target_table_dic[index][target_table_dic[index]['Excerpt'].str.len().gt(0)]","b934ce9b":"target_table_dic[index] = target_table_dic[index].iloc[:9, :]","646b9055":"target_table_dic[index] = target_table_dic[index].iloc[[i for i in range(9) if i != 1], :]","892f50ee":"target_table_dic[index].loc[target_table_dic[index].index == 2, 'Study'] = \"Inside China and COVID-19: Questions and answers\"\ntarget_table_dic[index].loc[target_table_dic[index].index == 2, 'Journal'] = \"Elsevier Public Health Emergency Collection\"\ntarget_table_dic[index].loc[target_table_dic[index].index == 2, 'Study Link'] = \"https:\/\/doi.org\/10.1016\/j.tmaid.2020.101640\"","46d28197":"target_table_dic[index] = target_table_dic[index].iloc[[i for i in range(8) if i != 5], :]","d3a67106":"study_types = []\nfor i, row in target_table_dic[index].iterrows():\n    try:\n        if pd.isnull(row['Study Link']):\n            paper_data = relevant_paper_dic[index][relevant_paper_dic[index]['title'] == row['Study']].iloc[0]\n        else:\n            paper_data = relevant_paper_dic[index][relevant_paper_dic[index]['doi'] == row['Study Link'][16:]].iloc[0]\n        found = False\n        for st in STUDY_TYPES:\n            if st in paper_data['text'].lower():\n                study_types.append(st)\n                found = True\n                break\n        if not found:\n            study_types.append(None)\n    except:\n        study_types.append(None)\nprint(study_types)","8d9313b3":"target_table_dic[index].loc[:, 'Study Type'] = study_types\ntarget_table_dic[index].loc[:, 'Addressed Population'] = [\n    \"SARS infected population\",\n    \"China\",\n    \"Hong Kong\",\n    \"Thailand\", \n    \"Wuhan\", \n    \"Hong Kong\", \n    \"Hong Kong\", \n]\ntarget_table_dic[index].loc[:, 'Challenge'] = [\n    \"Nosocomial Transmission of SARS\",\n    \"The implementation of standard precautions measures in the prevention and control of nosocomial infections was insufficient\", \n    \"nosocomial acquisition of SARS-CoV\", \n    \"A number of nosocomial outbreaks of MERS-CoV have been reported\",\n    \"two separate nosocomial outbreaks of SARS-CoV-2 involving 20 medical staff in a hospital within one week\", \n    \"prevent importation and nosocomial transmission of SARS-CoV-2 in Hong Kong\",\n    \"Nosocomial coronavirus infections\", \n]\n\ntarget_table_dic[index].loc[:, 'Solution'] = [\n    \"As noted, nosocomial transmission is highly efficient, and HCWs were sometimes the predominant victims\",\n    \"From the perspective of nosocomial infection control, all medical staff must enquire about the epidemiological history of patients, particularly patients with fever and\/or clinical characteristics of pneumonia; Only with all of these efforts coordinated can transmission of virus be cut within the family, the risk of nosocomial infections minimized, and the epidemic finally controlled.\",\n    \"Administrative support to infection control is important to prevent nosocomial outbreak but it is not well studied in recent years [19]; With the support of hospital administration, the infrastructure of hospitals was improved from the period of SARS-CoV to SARS-CoV-2 by the provision of 1400 AIIRs in Hong Kong;\",\n    \"Strict infection control precaution is the key to prevent nosocomial spread of emerging diseases\",\n    \"Not Provided\",\n    \"Vigilance in hand hygiene practice, wearing of surgical masks in the hospital, and appropriate use of PPE in patient care, especially performing AGPs, are the key infection control measures to prevent nosocomial transmission of SARS-CoV-2, even before the availability of effective antiviral agents and vaccine.; \",\n    \"This layout of distinct and separate sectors has proven efficient in the prevention of nosocomial RSV infections\", \n]\ntarget_table_dic[index].loc[:, 'Measure of Evidence'] = [\n    \"Not Provided\",\n    \"Not Provided\",\n    \"statistically significant\", \n    \"Not Provided\",\n    \"Not Provided\",\n    \"statistically significant\",\n    \"Not Provided\",\n]","70b13a47":"target_table_dic[index]","bbb0d462":"for i in target_table_dic:\n    target_table_dic[i].to_csv(f\"\/kaggle\/working\/table{i}_{questions[i]}.csv\")","0b375613":"### [Q5] \"What are recommendations for combating_overcoming resource failures","8566e2a4":"### [Q4] \"Modes of communicating with target high-risk populations\"","eae2b8a6":"### [Q2] \"Measures to reach marginalized and disadvantaged populations\"","b471f0b0":"# PART 2: Process","58cd427b":"### Manual work","28056a74":"## Step 1: Import Packages & Helper Functions","5e047dfe":"### 3.2 Select relevant papers by cosine similarity","b6028fcd":"### [Q1] \"Management of patients who are underhoused or otherwise lower social economic status\"","4eea8e22":"### [Q6] \"What are ways to create hospital infrastructure to prevent nosocomial outbreaks\"","a2438e00":"### 2.3 Data preprocessing","e436b747":"### Manual work","4be961bd":"- Use lowercase for all content\n- Remove numbers, punctuations, stopwords\n- Remove the commoner morphological and inflexional endings from words in English","e9805241":"# Step 4: Create target table for questions","f8fd9edd":"# CORD-19 Research [task 1: Population]\n\nThis notebook is for task 1 only","664ab141":"## Summary\n\n### Q1: Management of patients who are underhoused or otherwise lower social economic status\n- Challenges:\n    - profound health disparities exist\n        - medical resource in rural vs metropolitan areas\n        - personal financial status\n        - limited pathways for individual economic and social advancement\n- Proposed Solutions:\n    - Health care providers should be vigilant about undiagnosed patients\n\n### Q2: Measures to reach marginalized and disadvantaged populations\n- Challenges:\n    - health inequality exsits not only within countries but also between countries\n- Proposed Solutions:\n    - None\n\n### Q3: Methods to control the spread in communities\n- Challenges:\n    - Minimize community transmission\n    - Reduce the risk for patients of other diseases\n    - Public infection control\n- Proposed Solutions:\n    - recognize that virus sharing is a critical element of risk communication\n    - spread of incormation can significantly reduce the spread of COVID-19\n    - routine hand hygiene\n    - Need to solve shortage of supply of hand hygiene products\n    - targeted lockdowns or mess-testing at the community-level\n    - Actual morality may by low, at around 1% (which suggests that the high morality might be due to shortage of hospital resources)\n    - engage communicaties\n    - Need to control R_0 at below 1 for the virus to eventually stop spreading\n    - community quarantine and social distancing\n    - better testing\n\n### Q4: Modes of communicating with target high-risk populations\n- Challenges:\n    - poor communication and lack of resources are overlooked in life-threatening illness, and patients might feel abandoned\n- Proposed Solution\n    - involve the community, not only in research and intervention design, but also in the implementation and the evaluation of solutions, and in building trust in the community about the recommendations for protection\n\n### Q5: What are recommendations for combating_overcoming resource failures\n- Challenges:\n    - The surge in infections has led to a severe shortage of medical resources\n- Solution:\n    - we observe that there are only two possible stationary state: the whole healthy and the whole infected of the population\n    \n### Q6: What are ways to create hospital infrastructure to prevent nosocomial outbreaks\n- Challenges:\n    - nosocomial outbreaks\n- Proposed Solutions:\n    - all medical staff must enquire about the epidemiological history of patients, particularly patients with fever and\/or clinical characteristics of pneumonia\n    - Administrative support to infection control\n    - improve the hospital infastructure\n    - Strict infection control precaution\n    - Vigilance in hand hygiene practice, wearing of surgical masks in the hospital, and appropriate use of PPE in patient care, especially performing AGPs\n    - Hospital layout of distinct and separate sectors\n    \n    \n    \n    \nContacts\n- Kaitan Sun, kaitan9095@gmail.com\n- Jiangxue Han, jxhan0317@gmail.com\n- Richard Luo, ruize.luo@outlook.com\n- Hanying Gan, hanying.gan@outlook.com","cb8f0372":"## Output","3b3257d0":"# PART 3: Output & Summary","ac3d2913":"### 2.1 Load covid-19 relevant papers into dataframe","01fa3e3d":"#### Conclusion\n","2c5533a3":"### Reprasing questions to include more keywords","4ab1fce9":"### Manual work","c2c5eca4":"This part of the code is based on https:\/\/www.kaggle.com\/tizili0307\/research-documentation","9deea9b9":"Reference\n- The code for paper loading functions is based on: https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv\n- The code for TF-IDF cosine similarity is based on: https:\/\/github.com\/mbulusu\/Duplicate-Document-Detection-Meetup-Presentation\n- The code for Bert is based on: https:\/\/colab.research.google.com\/drive\/1uSlWtJdZmLrI3FCNIlUHFxwAJiSu2J0-","eefa4521":"# PART 1: Introduction\n\n![image.png](attachment:image.png)\n\nWith the ongoing development of COVID-19 situation, the number of research publications on it has been growing rapidly, making it increasingly difficult for researchers to locate the most relevant findings for their topics. This project aims to help the medical community find the information they seek by building data mining and search tools, which can ultimately support the ongoing fight against this pandemic. This notebook contains a basic pipeline of using NLP models and keyword searching to find answers to high priority scientific questions regarding the COVID-19.","74e497de":"### [Q3] \"Methods to control the spread in communities\"","16d5bf84":"### Manual work","50546518":"## Step 2: Load Data","9d47a4ee":"### Manual work","6a716bd5":"- Function `format_name(author)`, `format_affiliation(affiliation)`, `format_authors(authors, with_affiliation=False)`, `format_body(body_text)`, and `format_bib(bibs)` each converts the paper-related information to standard format.\n- Function `load_files(dirname)` loads the papers from json and saves them to a list.\n- Function `clean_pdf_files(file_list, keyword_list)` takes a list of file and converts it to a dataframe with cleaned columns.\n- Function `clean_sent(sentence)` cleans the text by lowercasing all words, removing numbers, punctuations, and stopwords, and stemming all words.\n- Function `calc_simlarity_score(question_list, text_list,threshold=None, top=None)` calculates the similarity score between the TF-IDF matrix of the article and that of questions.\n- Function `retrieve_paper(df, dic)` retrieves the information of relevant papers for each question in the specific topic.\n\n\nThe script for loading papers is based on: https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv\n\nThe code for TF-IDF cosine similarity is based on: https:\/\/github.com\/mbulusu\/Duplicate-Document-Detection-Meetup-Presentation","47205b8b":"### 3.3 Add common columns to all ****target tables","4e06fd81":"- Target Table Overview","3255287d":"## Step 3: Paper Selection by `Cosine Similarity`\n\nNote: Based on https:\/\/www.kaggle.com\/tizili0307\/research-documentation\n\nThe relevant papers for each question are selected by calculating the cosine similarity score based on the TF-IDF pair of articles and questions.\n\nTF-IDF is a statistical measure that evaluates how important a word is to a document when compared against the other document in the corpus. In this project, both articles and questions are converted to TF-IDF matrix so that the information can be captured with fewer words and can be used for calculation.\n\nCosine similarity is a commonly used metric to measure the similarity between two vectors by taking the dot product divided by the product of two vector\u2019s length. In this project, cosine similarity is used to measure the similarity between the TF-IDF matrix pair of documents and questions. In this way, the level of relevance can be quantified, which ultimately helps us to identify the most relevant papers for each question against more than 6,000 articles.","bd2edcb8":"### Manual work","d7d966a9":"### 2.4 (Optional) Save dataframe","5ebedba8":"### 2.2 Combine selected papers with the metadata\u00b6","4239818d":"### Load relevant papers\n- Load the papers and related information to the data structure.\n- Filter the papers to only articles related to COVID-19.","bc668c52":"Find questions"}}