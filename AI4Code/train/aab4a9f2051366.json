{"cell_type":{"c7c75e7e":"code","fab1e6cd":"code","92eb19a5":"code","15748067":"code","87672197":"code","c77b2e8a":"code","1e1ade8a":"code","4529cebe":"code","f4b7b479":"code","6b40cb16":"code","54159c24":"code","2f166c75":"code","d2769eb5":"code","10d037a9":"code","4a4308cd":"code","524254e5":"code","2938f8c3":"code","4be85df5":"code","5cdf7ba2":"code","294b80d4":"code","08768029":"code","7b4a9cc1":"code","e686ac6b":"code","74e6edef":"code","a1c58583":"code","4b402eaf":"code","639b6756":"code","a6adf42a":"code","47558e08":"code","e3dc4df7":"code","8ad1ff13":"code","406d73f7":"markdown","be7219ad":"markdown","0b160264":"markdown","4bcb2d0e":"markdown","9004111f":"markdown","e242581e":"markdown","d3d9f501":"markdown","6fa49e23":"markdown","267db4bc":"markdown","e2a83214":"markdown","bb146894":"markdown","d9f8c43a":"markdown","619377de":"markdown","df70a31e":"markdown","f8f4bf88":"markdown","6873045f":"markdown","c816ff0e":"markdown","87848e97":"markdown"},"source":{"c7c75e7e":"# Manipulation Libraries\n\nfrom termcolor import cprint\nimport os\nfrom glob import glob\nimport random\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n\n# supporting libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nfrom PIL import Image\nimport random\n\n#importing pytorch and associated libraries\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader","fab1e6cd":"# unzipping train folder\n!unzip -q ..\/input\/datasciencebowl\/train.zip","92eb19a5":"# unzipping test folder\n!unzip -q ..\/input\/datasciencebowl\/test.zip","15748067":"#unzipping sample submission file\n!unzip ..\/input\/datasciencebowl\/sampleSubmission.csv.zip","87672197":"# Class distribution in PIE_CHART \n\nclass_names = []\nclass_count = []\n\nfor name in os.listdir('.\/train\/'):\n    class_names.append(name)\n    class_count.append(len(os.listdir(f'.\/train\/{name}')))\nplt.figure(figsize=(10, 10))\nplt.pie(class_count, labels = class_names)\nplt.title('Class Distribution (Train)')\nplt.show()","c77b2e8a":"# Class distribution in BAR-PLOT\n\nplt.figure(figsize=(20, 8))\nsns.barplot(class_names, class_count, palette = 'Blues')\nplt.title('Class Distribution (Train)')\nplt.xticks(rotation=90)\nplt.show()","1e1ade8a":"# fixing the seeds\n\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)","4529cebe":"# Neural Network blocks and models\n\nclass Conv(nn.Module):\n    def __init__(self, in_channels, out_channels, kerel_size = 3, stride = 1, padding = 0):\n        super(Conv, self).__init__()\n        self.seq = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kerel_size, stride, padding),\n            nn.ReLU(),\n            nn.BatchNorm2d(out_channels),\n        )\n    def forward(self, x):\n        return self.seq(x)\n\nclass PlankNet(nn.Module):\n    def __init__(self, in_channels, num_classes, H = 128, W = 128):\n        super(PlankNet, self).__init__()\n        self.model = nn.Sequential(\n            Conv(in_channels, 16, 4), #125\n            nn.MaxPool2d(2), #62\n            Conv(16, 32, 3), #60\n            Conv(32, 64, 3), #58\n            nn.Dropout(0.1),\n            nn.MaxPool2d(2), # 29\n            Conv(64, 128), # 27\n            nn.Dropout(0.2),\n            Conv(128, 64, 3), # 25\n            Conv(64, 32, 3), # 23\n            nn.Flatten(),\n            nn.Linear(32*23*23 , 4096),\n            nn.Linear(4096, num_classes),\n        )\n    def forward(self, x):\n        return self.model(x)\nnum_classes = len(class_names)\nmodel = PlankNet(3, num_classes, 128, 128)","f4b7b479":"rand_data = torch.rand(1, 3, 128, 128)\nprint(model(rand_data).shape)","6b40cb16":"cprint(model, \"blue\")","54159c24":"# Model layers overview\n\nfor name, param in model.named_parameters():\n    print(f\"{name} : {param.shape}\")","2f166c75":"#Generating csv file to gather data of images \n\ndef generate_csv(root,train = True, img_ext = 'jpg'):\n    df = pd.DataFrame(columns = ['path', 'class'])\n    if train:\n        for index,label in enumerate(os.listdir(root)):\n            links = glob(f\"{root}\/{label}\/*{img_ext}\")\n            temp_df = pd.DataFrame({'path': links, 'class': np.ones(len(links), dtype='float32')*index})\n            df = pd.concat([df, temp_df], axis = 0)\n    else:\n        links = glob(f\"{root}\/*{img_ext}\")\n        temp_df = pd.DataFrame({'path': links, 'class': np.ones(len(links), dtype = 'float32')})\n        df = pd.concat([df, temp_df], axis = 0)\n        \n    return df\n        ","d2769eb5":"train_csv = generate_csv('.\/train')\ntrain_csv.head()","10d037a9":"test_csv = generate_csv('.\/test',train = False)\ntest_csv.head()","4a4308cd":"# sorting the image by their names\n\ntest_csv.sort_values('path', inplace = True)\ntest_csv.head()","524254e5":"# Genrating dataset classes\n\ndef load_image(path , H, W):\n    img = cv2.imread(path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (H,W))\n    return img\n\ndef transform():\n    return transforms.Compose([\n        transforms.RandomRotation(90),\n        transforms.ToTensor()\n    ])\n\n\nclass PlanktonDataset(Dataset):\n    def __init__(self, df, H = 128, W = 128, transform = None):\n        super(PlanktonDataset, self).__init__()\n        self.df = df\n        self.H = H\n        self.W = W\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        path = self.df.iloc[index, 0]\n        img = load_image(path, H = self.H, W = self.W)\n        label = self.df.iloc[index, 1]\n        img = Image.fromarray(img)\n        if self.transform != None:\n            img = self.transform()(img)\n        else:\n            img = transforms.ToTensor()(img)\n        return (img, (label, path))\n    \n# Creating the train and test datasets.   \ntrain_ds = PlanktonDataset(train_csv, 128, 128, transform = transform)\ntest_ds = PlanktonDataset(test_csv, 128, 128)","2938f8c3":"# data access format\n\nimage, label = test_ds.__getitem__(3)\nimage.shape, label[0], label[1]","4be85df5":"# Train-Validation Split :\n\nspl_idx = int(train_ds.__len__() * 0.75)\nprint(f\"Splitting index : {spl_idx}\")\ntrain_ds, val_ds = torch.utils.data.random_split(train_ds,[spl_idx, train_ds.__len__() - spl_idx] )","5cdf7ba2":"print(f\"Size of train dataset : {train_ds.__len__()}\")\nprint(f\"Size of validation dataset : {val_ds.__len__()}\")\nprint(f\"Size of test dataset : {test_ds.__len__()}\")","294b80d4":"# Creating data loaders specifying the batch size\n\nBATCH_SIZE = 64\ntrain_dl = DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle = True)\nval_dl = DataLoader(val_ds, batch_size = BATCH_SIZE, shuffle = True)\ntest_dl = DataLoader(test_ds, batch_size = BATCH_SIZE, shuffle = False)","08768029":"# Chossing training hyperparameters and also the optimizer and loss\n\nEPOCHS = 30\ncriterion = nn.CrossEntropyLoss()\noptim = torch.optim.Adam(params = model.parameters(), lr = 1e-4)","7b4a9cc1":"train_loss = []\nval_loss = []","e686ac6b":"# Model Training...\n\nmodel = model.cuda()  # Putting the model inside GPU\nbest_loss = np.inf\nfor epoch in range(EPOCHS):\n    print(f\"Epoch {epoch + 1} : \\n\")\n    TR_LOSS = 0.0\n    VAL_LOSS = 0.0\n    model.train()\n    \n    # Train Data Forward & Backward Pass\n    \n    for index, (train_patch, (labels, _)) in enumerate(train_dl):\n        optim.zero_grad()\n        train_patch = train_patch.cuda()\n        labels = labels.long().cuda()\n        op = model(train_patch)\n        tloss = criterion(op, labels)\n        TR_LOSS += tloss.item()\n        train_loss.append(tloss.item())\n        tloss.backward()\n        optim.step()\n        \n        if index % 100 == 99:\n            print(f\"         Step {index + 1} Loss : {'%.4f'%(tloss.item())}\")   \n    model.eval()\n    \n    # Validation Checking ( Only Forward Pass )\n    \n    with torch.no_grad():\n        for index, (val_patch, (labels, _)) in enumerate(val_dl):\n            val_patch = val_patch.cuda()\n            labels = labels.long().cuda()\n            op = model(val_patch)\n            vloss = criterion(op, labels)\n            VAL_LOSS += vloss.item()\n            val_loss.append(vloss.item())\n    print(f\"\\n     Training Loss : {'%.4f'%(TR_LOSS)}  ||  Validation Loss : {'%.4f'%(VAL_LOSS)}\\n\")\n    \n    if VAL_LOSS < best_loss :      # Model Updationg\n        cprint(\"Model Updation : Success!\\n\", 'green')\n        torch.save(model, 'best_model.pth')\n        best_loss = VAL_LOSS\n    else:\n        cprint(\"Model Updation : Failed!\\n\", 'red')\ncprint('Training completed...', 'green')","74e6edef":"plt.figure(figsize=(20,8))\nplt.plot(train_loss)\nplt.title('Train Loss', size = 20)\nplt.xlabel('STEPS')\nplt.ylabel('LOSS')\nplt.show()","a1c58583":"plt.figure(figsize=(20,8))\nplt.plot(val_loss)\nplt.title('Validation Loss', size = 20)\nplt.xlabel('STEPS')\nplt.ylabel('LOSS')\nplt.show()","4b402eaf":"best_model = torch.load('.\/best_model.pth')\ncprint(best_model, 'blue')","639b6756":"# to filter only the names of the images\n\ndef preprocess_names(names_list):\n    name_list = []\n    for name in names_list:\n        name_list.append(name.split('\/')[-1])\n    return name_list","a6adf42a":"# Test data generating function\n\ndef create_submission_file(test_dataloader, model, class_names):\n    df = pd.DataFrame()\n    model.eval()\n    sf_layer = nn.Softmax(dim = 1)\n    with torch.no_grad():\n        for index, (test_patch, attr) in enumerate(test_dataloader):  # loading test data\n            paths = preprocess_names(attr[1])\n            test_patch = test_patch.cuda()\n            op = model(test_patch)\n            op = sf_layer(op)\n            paths = np.array(list(paths))   #adding the filenames in a list\n            op = op.cpu().detach().numpy()\n            patch_df = pd.DataFrame(op, columns = class_names)  #creating the patch dataframe\n            patch_df.insert(0, 'image', paths)\n            df = pd.concat([df, patch_df], axis = 0)    # joining the patch dataframe with the main one.\n            \n            if index % 100 == 99:\n                print(f\"{index + 1} Steps Completed...\\n\")    \n    print('Test Dataframe Generated...\\n')\n    \n    return df","47558e08":"best_model = best_model.cuda()\npred_df = create_submission_file(test_dl, best_model, class_names)\npred_df.head()","e3dc4df7":"# Checking if all the imae names are different\nassert pred_df['image'].nunique() == pred_df.shape[0], \" Submission format not correct!\"\ncprint('Submission correcty created !', 'green')","8ad1ff13":"pred_df.to_csv('submission.csv', index = False)","406d73f7":"# Libraries :\n---","be7219ad":"# Prediction :\n---\n\n In this step we will load the best fitted model and then cretae the submission dataframe and then poplate that data on a `.csv` file.","0b160264":"<div align=\"center\"><img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcR_Q8nZAYAx2FCzHlmWUnZjOjVHtSGQUehJ9w&usqp=CAU\"><\/div>","4bcb2d0e":"# Model Evaluation :\n---\n\nNow, we should check if the model is overfitting or not. But in this scenario we can see that the loss function has too many local minima but the model hparams are quite successful to cross those and reach a satisfactory point.","9004111f":"### Creating `submission.csv` file","e242581e":"# Dataset Generation :\n---\n\nNow we will be creating the dataset which will be fed through the CNN-Model.\nThe training data will be splitted in training and validation for checking the model performance.\n\nFirstly in this step we will create the dataframe eith which we will acces each data elements and it's additional features.\nThen we will create the dataset class and then fit that though dataloader which will create batches of data to feed into the model.","d3d9f501":"Now, we need to do a last sanity chcek if the dataframe is okay or not.","6fa49e23":"# Class Distribution :\n---\n\n We should be aware of the class distributions cause every single class it's significance in this scenario.","267db4bc":"# Model Generation :\n---\n\nThe model we are going to train for this chalenge will be a custom Convoltuional Neural Network. Primarily we will be updating some conv layers then we will flatten the whole image and then we will feed that through fully connected networks to generate outputs.","e2a83214":"<div align=\"center\"><h1>HURRAH!<\/h1><\/div>","bb146894":"After checking the input format we can see that the images of the train folder are stored in the folders as per their class names and the test data has only one folder containing all the images.\n\nThe sample submission has the name of the images and the the probabilty of the classes. So, we need to calculate the the softmax prediction of the classes.\n\nThus the problem can be achieved by a simple classification using a softmax layer at the bottom of the prediction.","d9f8c43a":" # Unzipping data :\n ---\n \n Now we are unzipping the train, test and the sample submission to check which type of data we should be feeding to the model and what should be the output format.","619377de":"The train-validation split is taken 3:1 (75% - 25%)","df70a31e":"As we can see that the names are not sorted as shown n the sample submission , so we need to sort the label names of the test data.","f8f4bf88":"![](http:\/\/prod-upp-image-read.ft.com\/4fec4450-0417-11e5-a70f-00144feabdc0)\n\n<div align=\"center\"><u><h1>Plankton Classification Challenge<\/h1><\/u><\/div>\n <div align=\"center\"><h3>This notebook has been prepared on the Plankton Classification Challenge .<\/h3><\/div>\n <div align=\"center\">This not only holds the solution but also is a nice<\/div>\n  <div align=\"center\">starter notebook for image classification with PyTorch<\/div>","6873045f":"At first we need to understand the steps that how we are going to solve the whole challenge by conquering every small step.","c816ff0e":"# Model Compilation, Hyperparameter Tuning , Training :\n---\n\nIn this step we'll be feeding the data to the CNN-Model. Do, keep in mind this trainer has been made to only surpass GPU devices and will not work on the cpu devices.\n\n**Note :** If you want to run the trainer on cpu , just comment the `.cuda()` portions.","87848e97":"Now, We've completed the whole project. \n## Do ***UPVOTE*** this notebook.\nFollow me on [GitHub](https:\/\/github.com\/sagnik1511) and also in [Kaggle](https:\/\/kaggle.com\/sagnik1511)."}}