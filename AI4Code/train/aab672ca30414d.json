{"cell_type":{"168e1227":"code","a1325aee":"code","0ed853fb":"code","de926452":"code","fc48176d":"code","35e900c3":"code","6849c146":"code","9a7dae57":"code","a8fe07bb":"code","d3086d5d":"code","aae011b8":"code","e1193c38":"code","ca26d43d":"code","b6e77cea":"code","544b96b0":"code","1a603fc2":"code","589d28e6":"code","bc2ea899":"code","e1d8e2ad":"code","9fa96bb6":"code","eebe7636":"code","d999631c":"code","63c59cd3":"code","767c9437":"code","e2c0e4c7":"code","693018ba":"code","9cacfa9e":"code","02f31ba3":"code","d3bab706":"code","a2fd9778":"code","60e9ad84":"code","4720e2e1":"code","f5aaec85":"code","61336eba":"code","44e23852":"code","f390bc37":"code","6e8c8a0a":"code","bef7316c":"code","f70f2d66":"code","443337d2":"code","d0051d63":"code","e3a366e0":"code","bda690eb":"code","14ad73c5":"code","9cf9e8ea":"code","27885747":"code","874b5f9a":"code","d8603164":"code","9a2d3dd3":"code","47fcab1e":"markdown","9a588279":"markdown","664a85ce":"markdown","3cf18d04":"markdown","578b68aa":"markdown","d5cce4b5":"markdown","fe788836":"markdown","af80c63c":"markdown","a0ba8a07":"markdown","fdded050":"markdown","0c005aaa":"markdown","8e395a25":"markdown","4b436e44":"markdown","dd420ece":"markdown","2532c56a":"markdown","f81de2f9":"markdown","9ec61e2d":"markdown","b556f0e9":"markdown","8fbaf3b6":"markdown","6d6b2fbd":"markdown","9fc3deef":"markdown","32ffbcdd":"markdown","fb54ab05":"markdown","eb84595b":"markdown","095cf782":"markdown","eac523b3":"markdown","732a6483":"markdown","82b9ec11":"markdown"},"source":{"168e1227":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a1325aee":"#importing libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\n\n\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0ed853fb":"#import our train and test data \ntrain=pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv')\n","de926452":"#display some rows from our train data \ntrain.head()","fc48176d":"#shape of our training  data\ntrain.shape","35e900c3":"#some information about training data\ntrain.info()","6849c146":"# the number of nulls in training data\nna_counts=train.isnull().sum()\nna_counts","9a7dae57":"#plotting the number of null values in each feature\nplt.figure(figsize=[10,5]);\nsns.barplot(na_counts.index.values,na_counts);\nplt.xticks(rotation=90);","a8fe07bb":"#drop company_size , company_type\ntrain.drop(['company_size','company_type'],axis=1,inplace=True)\n","d3086d5d":"#dropping nulls in rest of our training data \ntrain.dropna(axis=0,inplace=True)\ntrain.info()","aae011b8":"#check for duplicates\ntrain.duplicated().sum()","e1193c38":"plt.figure(figsize=[15,7])\nplt.subplot(1,2,1)\nsorted_counts=train['gender'].value_counts()\nplt.pie(sorted_counts,labels=sorted_counts.index,startangle=90,counterclock=False,wedgeprops={'width':0.4})\nplt.axis('square');\nplt.subplot(1,2,2)\nsns.barplot(sorted_counts.index.values,sorted_counts)\n#plt.yticks(range(np.arange(0,len(train)+1000,2000))\nplt.yticks(range(0,14000,2000));","ca26d43d":"#number of relevant experiance in each unique one\nbase_color=sns.color_palette()[2]\nrects1=sns.countplot(data=train,x='relevent_experience',color=base_color);\n\n ","b6e77cea":"#creating waffle plot for education level\ndef percentage_blocks(df, var):\n    \"\"\"\n    Take as input a dataframe and variable, and return a Pandas series with\n    approximate percentage values for filling out a waffle plot.\n    \"\"\"\n    # compute base quotas\n    percentages = 100 * train[var].value_counts() \/ df.shape[0]\n    counts = np.floor(percentages).astype(int) # integer part = minimum quota\n    decimal = (percentages - counts).sort_values(ascending = False)\n\n    # add in additional counts to reach 100\n    rem = 100 - counts.sum()\n    for cat in decimal.index[:rem]:\n        counts[cat] += 1\n\n    return counts\n\n\nwaffle_counts = percentage_blocks(train, 'education_level')\n\nprev_count = 0\n# for each category,\nfor cat in range(waffle_counts.shape[0]):\n    # get the block indices\n    blocks = np.arange(prev_count, prev_count + waffle_counts[cat])\n    # and put a block at each index's location\n    x = blocks % 10 # use mod operation to get ones digit\n    y = blocks \/\/ 10 # use floor division to get tens digit\n    plt.bar(x = x, height = 0.8, width = 0.8, bottom = y)\n    prev_count += waffle_counts[cat]\n\n    \n# aesthetic wrangling\nplt.legend(waffle_counts.index, bbox_to_anchor = (1, 0.5), loc = 6);\nplt.axis('off');\nplt.axis('square');\n","544b96b0":"#showing some statistic about training hours\nax = sns.boxplot(data=train['training_hours'], orient=\"v\", palette=\"Set2\")","1a603fc2":"#showing the distribution of training hours\nbin_edges = np.arange(0, train['training_hours'].max()+20, 20)\nsns.distplot(train['training_hours'], bins = bin_edges, kde = False,\n            hist_kws = {'alpha' : 1})\n","589d28e6":"train.head()","bc2ea899":"#Number of each gender in each relevant experiance unique value\nsns.set(style=\"whitegrid\")\nax = sns.countplot(x=\"gender\", hue=\"relevent_experience\", data=train)\nplt.title('Number of each gender in each relevant experiance unique value');","e1d8e2ad":"#Number of each gender in each education level\nct_counts = train.groupby(['gender', 'education_level']).size()\n\n# Use Series.reset_index() to convert a series into a dataframe object\nct_counts = ct_counts.reset_index(name='count')\n \n# Use DataFrame.pivot() to rearrange the data, to have gender type on rows\nct_counts = ct_counts.pivot(index = 'gender', columns = 'education_level', values = 'count')\n \n \nsns.heatmap(ct_counts,annot=True,fmt='d');\nplt.title('Number of each gender in each education level');","9fa96bb6":"# Convert the \"gender\" column from a plain object type into an ordered categorical type\ngender_types = list(train['gender'].unique())\nvclasses = pd.api.types.CategoricalDtype(ordered=True, categories=gender_types)\ntrain['gender'] = train['gender'].astype(vclasses);\n\n# Plot the Seaborn's FacetGrid\ng = sns.FacetGrid(data = train, col = 'gender')\ng.map(plt.hist, \"training_hours\");\n","eebe7636":"#statics for each gender with training hours \nax=sns.boxplot(data=train,x='gender',y='training_hours')","d999631c":"#number of each gender in each target unique\nax = sns.countplot(y=\"gender\", hue=\"target\", data=train)\nplt.legend()","63c59cd3":"#statistic for each gender in every education level & training_hours\nax=sns.boxplot(data=train,x='gender',y='training_hours',hue='education_level')","767c9437":"#let's take alook again on our data set\ntrain.head()","e2c0e4c7":"train['gender']=train['gender'].replace({'Male':0,'Female':1,'Other':2})\ntrain['enrolled_university']=train['enrolled_university'].replace({'no_enrollment':0,'Full time course':1,'Part time course':2})\ntrain['relevent_experience']=train['relevent_experience'].replace({'No relevent experience':0,'Has relevent experience':1})\ntrain['education_level']=train['education_level'].replace({'Graduate':1,'Masters':2,'Phd':3})\n\ntrain['major_discipline']=train['major_discipline'].replace({'STEM':1, 'Humanities':2, 'Arts':3, 'Business Degree':4,'No Major':5,'Other':6})\n\ntrain['experience']=train['experience'].replace({'>20':30,'<1':0})\n\ntrain['last_new_job']=train['last_new_job'].replace({'>4':5,'1':1,'never':0,'4':4, '3':3, '2':2})\n","693018ba":"train['experience']=train['experience'].replace({'>20':30,'<1':0,'15':15,'13':13,'7':7, '5':5, '16':16, '4':4, '11':11, '18':18, '19':19, '12':12,\n       '10':10, '9':9, '2':2, '6':6, '14':14, '3':3, '8':8, '20':20, '17':17, '1':1})\n","9cacfa9e":"train['training_hours']=train['training_hours']\/train['training_hours'].max()\ntrain['experience']=train['experience']\/train['experience'].max()","02f31ba3":"train.head()","d3bab706":"#let's checking again for coulmns type to see the correlation\ntrain.info()","a2fd9778":"train.shape","60e9ad84":"#correlation matrix\nplt.figure(figsize=[10,10])\nsns.heatmap(train.corr(),annot=True,fmt='.3f')\nplt.title('Correlation factor between target and each column');","4720e2e1":"#our features and target\nX=train.drop(['enrollee_id','city','target'],axis=1) #these featured don't benefit us in our clsassification \ny=train['target']\n","f5aaec85":"from imblearn.over_sampling import RandomOverSampler\nfrom collections import Counter\n\n\nrand=RandomOverSampler(random_state=42)\nx_ros, y_ros = rand.fit_resample(X, y)\nprint(f\"Imbalanced target class: {Counter(y)} Balanced target class:{Counter(y_ros)}\")","61336eba":"#splitting our data \nX_train, X_test, y_train, y_test = train_test_split(x_ros, y_ros, test_size=0.33, random_state=44, shuffle =True)\n\n#Splitted Data\nprint('X_train shape is ' , X_train.shape)\nprint('X_test shape is ' , X_test.shape)\nprint('y_train shape is ' , y_train.shape)\nprint('y_test shape is ' , y_test.shape)","44e23852":"#Import Libraries\nfrom sklearn.ensemble import GradientBoostingClassifier\n \n\nGBCModel = GradientBoostingClassifier(n_estimators=500,max_depth=50,random_state=33) \nGBCModel.fit(X_train, y_train)\n\n#Calculating Details\nprint('GBCModel Train Score is : ' , GBCModel.score(X_train, y_train))\nprint('GBCModel Test Score is : ' , GBCModel.score(X_test, y_test))\n \n#Calculating Prediction\ny_pred = GBCModel.predict(X_test)\ny_pred_prob = GBCModel.predict_proba(X_test)\n ","f390bc37":"# applying  random forest\nRandomForestClassifierModel = RandomForestClassifier(criterion = 'gini',n_estimators=500,max_depth=10,random_state=33) #criterion can be also : entropy \nRandomForestClassifierModel.fit(X_train, y_train)\n\n#Calculating Details\nprint('RandomForestClassifierModel Train Score is : ' , RandomForestClassifierModel.score(X_train, y_train))\nprint('RandomForestClassifierModel Test Score is : ' , RandomForestClassifierModel.score(X_test, y_test))\nprint('RandomForestClassifierModel features importances are : ' , RandomForestClassifierModel.feature_importances_)\nprint('----------------------------------------------------')\n\n#Calculating Prediction\ny_pred = RandomForestClassifierModel.predict(X_test)\ny_pred_prob = RandomForestClassifierModel.predict_proba(X_test)\nprint('Predicted Value for RandomForestClassifierModel is : ' , y_pred[:10])\nprint('Prediction Probabilities Value for RandomForestClassifierModel is : ' , y_pred_prob[:10])","6e8c8a0a":"#calculating f1 score\n\nF1Score = f1_score(y_test, y_pred, average='micro') #it can be : binary,macro,weighted,samples\nprint('F1 Score is : ', F1Score)","bef7316c":"#Import Libraries\nfrom sklearn.metrics import classification_report\n#----------------------------------------------------\n\n#----------------------------------------------------\n#Calculating classification Report :  \n#classification_report(y_true, y_pred, labels=None, target_names=None,sample_weight=None, digits=2, output_dict=False)\n\nClassificationReport = classification_report(y_test,y_pred)\nprint('Classification Report is : ', ClassificationReport )","f70f2d66":"#Calculating Confusion Matrix\nCM = confusion_matrix(y_test, y_pred)\nprint('Confusion Matrix is : \\n', CM)\n\n# drawing confusion matrix\nsns.heatmap(CM, center = True,annot=True,fmt='.1f')\nplt.show()","443337d2":"#imprting our test data \ntest=pd.read_csv('..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv')","d0051d63":"#some rows from our test data\ntest.head()","e3a366e0":"test.info()","bda690eb":"# number of nulls \ntest.isnull().sum()","14ad73c5":"# dropping columns which we removed when training our model\ntest.drop(['company_size','company_type','enrollee_id','city'],axis=1,inplace=True)\ntest.head()","9cf9e8ea":"test['gender']=test['gender'].replace({'Male':0,'Female':1,'Other':2})\ntest['enrolled_university']=test['enrolled_university'].replace({'no_enrollment':0,'Full time course':1,'Part time course':2})\ntest['relevent_experience']=test['relevent_experience'].replace({'No relevent experience':0,'Has relevent experience':1})\ntest['education_level']=test['education_level'].replace({'Graduate':1,'High School':4,'Primary School':6,'Masters':2,'Phd':3})\n\ntest['major_discipline']=test['major_discipline'].replace({'STEM':1, 'Humanities':2, 'Arts':3, 'Business Degree':4,'No Major':5,'Other':6})\n\ntest['experience']=test['experience'].replace({'>20':30,'<1':0})\n\ntest['last_new_job']=test['last_new_job'].replace({'>4':5,'1':1,'never':0,'4':4, '3':3, '2':2})\ntest['experience']=test['experience'].replace({'>20':30,'<1':0,'15':15,'13':13,'7':7, '5':5, '16':16, '4':4, '11':11, '18':18, '19':19, '12':12,\n       '10':10, '9':9, '2':2, '6':6, '14':14, '3':3, '8':8, '20':20, '17':17, '1':1})\n","27885747":"test['gender'].fillna(test['gender'].mode()[0],inplace=True)\ntest['enrolled_university'].fillna(test['enrolled_university'].mode()[0],inplace=True)\ntest['education_level'].fillna(test['education_level'].mode()[0],inplace=True)\ntest['major_discipline'].fillna(test['major_discipline'].mode()[0],inplace=True)\ntest['last_new_job'].fillna(test['last_new_job'].mode()[0],inplace=True)\ntest['experience'].fillna(test['experience'].mean(),inplace=True)\n","874b5f9a":"test.info()","d8603164":"test.head()","9a2d3dd3":"test_pred= RandomForestClassifierModel.predict(test)\ntest_pred","47fcab1e":"One alternative univariate plot type that you might see for categorical data is the waffle plot, also known as the square pie chart. While the standard pie chart uses a circle to represent the whole, a waffle plot is plotted onto a square divided into a 10x10 grid. Each small square in the grid represents one percent of the data, and a number of squares are colored by category to indicate total proportions. Compared to a pie chart, it is much easier to make precise assessments of relative frequencies.","9a588279":"after that we will find few rows with missing values so we will drop it to start analyze our clean data","664a85ce":"Count of education level ","3cf18d04":"As we see there is overfitting when we use GradientBoostingClassifier, so we will try to use random forest","578b68aa":"If you find this kernal useful for you please upvote , to encourage me to do more","d5cce4b5":"# Treating our imbalance data \nAs we see from our visualization , we have an imbalance data in our training data set , since the 0 in target is more higher than 1 ","fe788836":"Number of each gender in each relevant experiance unique value","af80c63c":"From correlation matrix we can define which features are strong related to our target and which not.","a0ba8a07":"# Random forest","fdded050":"Let's see the number of each gender in each education level\n","0c005aaa":"# Importing libraries","8e395a25":"We will fill nulls with most frequent value in each column","4b436e44":"# Predicting our test data ","dd420ece":"number of  relevant_exoeriance in each unique one","2532c56a":"# EDA","f81de2f9":"#  GradientBoostingClassifier","9ec61e2d":"Correlation factor between target and each column","b556f0e9":"# Multivariate visualization","8fbaf3b6":"# Content \nThis note contain three main points :\n- EDA\n- Univariate visualization\n- Bivariate visualzation\n- Multivariate visualization\n\nAnd as we will find that, our data is imbalanced so we will try to treat this by \n\n-  imblearn \n\nthen we do our clsaaification ","6d6b2fbd":"Number of males and femals","9fc3deef":"# Handling our test data to predict it","32ffbcdd":"We note that there is many missing values in company_size and comany_type ,so if i dropped all the nulls in data , i will lose more than half my training dataset ,so i will drop these two columns","fb54ab05":"We shouldn't drop any rows from our test dataset because they are our finally target ,so we will handel our missing values","eb84595b":"Let's see the distribution of training ours for all our candidates ","095cf782":"# Univariate visualization","eac523b3":"# Handling our data to make our classification","732a6483":"# Bivariate visualization","82b9ec11":"First we will drop company size and company type as we droped them from our training data"}}