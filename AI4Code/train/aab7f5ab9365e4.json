{"cell_type":{"39a4da37":"code","35d0456c":"code","ee5c02fc":"code","2e372d30":"code","c920a559":"code","b70b7ce1":"code","8c757e42":"code","e9ef7b1e":"code","e2d07326":"code","991ea041":"code","f68868c3":"code","43e76734":"code","1ed1489e":"code","93cf8f1f":"code","8b7aaeda":"code","ac6cc85a":"markdown","bc20c2d7":"markdown","0147bb10":"markdown","1d321c02":"markdown","e9ac4176":"markdown","c645c10b":"markdown","25bbfe3d":"markdown","0de6cd7e":"markdown","73992d93":"markdown"},"source":{"39a4da37":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport os.path\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\n\nfrom sklearn.metrics import confusion_matrix, classification_report","35d0456c":"image_dir = Path('..\/input\/food41\/images')","ee5c02fc":"filepaths = list(image_dir.glob(r'**\/*.jpg'))\nlabels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n\nfilepaths = pd.Series(filepaths, name='Filepath').astype(str)\nlabels = pd.Series(labels, name='Label')\n\nimages = pd.concat([filepaths, labels], axis=1)\n\ncategory_samples = []\nfor category in images['Label'].unique():\n    category_slice = images.query(\"Label == @category\")\n    category_samples.append(category_slice.sample(100, random_state=1))\nimage_df = pd.concat(category_samples, axis=0).sample(frac=1.0, random_state=1).reset_index(drop=True)","2e372d30":"image_df","c920a559":"image_df['Label'].value_counts()","b70b7ce1":"train_df, test_df = train_test_split(image_df, train_size=0.7, shuffle=True, random_state=1)","8c757e42":"train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n    validation_split=0.2\n)\n\ntest_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n)","e9ef7b1e":"train_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=42,\n    subset='training'\n)\n\nval_images = train_generator.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=True,\n    seed=42,\n    subset='validation'\n)\n\ntest_images = test_generator.flow_from_dataframe(\n    dataframe=test_df,\n    x_col='Filepath',\n    y_col='Label',\n    target_size=(224, 224),\n    color_mode='rgb',\n    class_mode='categorical',\n    batch_size=32,\n    shuffle=False\n)","e2d07326":"pretrained_model = tf.keras.applications.MobileNetV2(\n    input_shape=(224, 224, 3),\n    include_top=False,\n    weights='imagenet',\n    pooling='avg'\n)\n\npretrained_model.trainable = False","991ea041":"inputs = pretrained_model.input\n\nx = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\nx = tf.keras.layers.Dense(128, activation='relu')(x)\n\noutputs = tf.keras.layers.Dense(101, activation='softmax')(x)\n\nmodel = tf.keras.Model(inputs, outputs)\n\n\nprint(model.summary())","f68868c3":"model.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nhistory = model.fit(\n    train_images,\n    validation_data=val_images,\n    epochs=100,\n    callbacks=[\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=3,\n            restore_best_weights=True\n        )\n    ]\n)","43e76734":"results = model.evaluate(test_images, verbose=0)\nprint(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))","1ed1489e":"predictions = np.argmax(model.predict(test_images), axis=1)\n\ncm = confusion_matrix(test_images.labels, predictions)\nclr = classification_report(test_images.labels, predictions, target_names=test_images.class_indices, zero_division=0)","93cf8f1f":"plt.figure(figsize=(30, 30))\nsns.heatmap(cm, annot=True, fmt='g', vmin=0, cmap='Blues', cbar=False)\nplt.xticks(ticks=np.arange(101) + 0.5, labels=test_images.class_indices, rotation=90)\nplt.yticks(ticks=np.arange(101) + 0.5, labels=test_images.class_indices, rotation=0)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.show()","8b7aaeda":"print(\"Classification Report:\\n----------------------\\n\", clr)","ac6cc85a":"# Train-Test Split","bc20c2d7":"# Creating Generators","0147bb10":"# Data Every Day  \n\nThis notebook is featured on Data Every Day, a YouTube series where I train models on a new dataset each day.  \n\n***\n\nCheck it out!  \nhttps:\/\/youtu.be\/35iRdqY01co","1d321c02":"# Modeling","e9ac4176":"# Getting Started","c645c10b":"# Task for Today  \n\n***\n\n## Food Image Classification  \n\nGiven *images of 101 different foods*, let's try to classifify the **food** present in a given image.\n\nWe will use a TensorFlow\/Keras pretrained MobileNetV2 CNN to make our predictions.","25bbfe3d":"# Training","0de6cd7e":"# Creating File DataFrame","73992d93":"# Results"}}