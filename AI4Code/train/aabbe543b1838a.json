{"cell_type":{"27aac927":"code","c5705099":"code","5e669594":"code","79eba16c":"code","10a5a9c8":"code","b4e703d8":"code","98ee8a4b":"code","6021e548":"code","a05d154a":"code","4209443a":"code","ea242b74":"code","be07132e":"code","316f5e2f":"code","2c19cac0":"code","16a8efba":"code","087c598a":"code","6bbead70":"code","794557b5":"code","9fb67b6d":"code","3751c6a1":"code","a34cfe1f":"code","1d17cc89":"code","b22f4930":"code","80c5c6b4":"code","20fbe2ae":"code","d4a7d9c6":"code","6560a5ec":"markdown","88d733df":"markdown","85ffbf3e":"markdown","1208a950":"markdown","a7afdbca":"markdown","cf1bcbbd":"markdown","cf738319":"markdown","ecb72784":"markdown","0819928a":"markdown","c6ed2d90":"markdown","cc2da160":"markdown","d81f1787":"markdown","2f5e541a":"markdown","5ccfd283":"markdown","18d353b9":"markdown","1b35fe6c":"markdown","572c78f4":"markdown","9628472f":"markdown"},"source":{"27aac927":"# Imports\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import mean_squared_error as mse\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.experimental import enable_iterative_imputer # For multivariate imputation\nfrom fancyimpute import IterativeImputer\nfrom sklearn.impute import IterativeImputer\n\n# I\/O\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c5705099":"# Loading data\ntrain_original = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_original = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\n# Making copies to clean so I keep the originals seperate\ntrain = train_original.copy()\ntest = test_original.copy()\n\n# Adding both to a list to streamline cleaning each df at the same time\ndfs = [train, test]\ndfs_names = ['Train','Test']","5e669594":"# New thing I came across - Pandas profiling\n#train_original.profile_report()","79eba16c":"# Regex to get titles (start with space and end with full stop)\nfor dataset in [train,test]:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    \n# Never seen pd.crosstab before but found this in the top rated Titanic notebook, very cool\npd.crosstab(train['Title'], train['Sex'])","10a5a9c8":"# Replacing some rare ones with just the word 'Rare', and combining some similar ones into 'Miss'\nfor df in [train,test]:\n    df['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col',\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    \nfor df in [train,test]:\n    df['Title'] = df['Title'].replace(['Mlle','Mme','Ms'], 'Miss') # Mlle = Mademoiselle\n    \ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","b4e703d8":"# regular expressions are something I need to properly learned, totally just cobbled this together from various stack overflow posts\nimport re\nfor df in [train,test]:\n    df['Ticket Extracted'] = df['Ticket'].str.extract(r'([a-zA-Z]+)')","98ee8a4b":"# How many unique values are there in each 'object' column i.e. what type of encoding to use\nfor i,j in enumerate(dfs):\n    print(dfs_names[i]+':')\n    for col in j.select_dtypes(include='object').columns:\n        print(f'Unique number of {col}: {j[col].nunique()}')","6021e548":"label_enc = LabelEncoder()\n\nto_encode = ['Sex','Embarked','Ticket Extracted','Title']\n\nfor df in [train, test]:\n    for i in to_encode:\n        df[i] = df[i].astype(str)\n        df[i] = label_enc.fit_transform(df[i])\n        \nfor df in [train,test]:\n    df.drop(['Name','Ticket','Cabin'],axis=1,inplace=True)","a05d154a":"# Getting dummy columns and dropping unwanted columns (not using this anymore, using above label encoding instead)\n\ndef get_dummies_and_drop(df,dummy_cols,drop_cols):\n    '''Function to create dummy columns and drop unwanted columns\n    \n    Keyword arguments:\n    df -- dataframe to use\n    dummy_cols -- List of columns to create dummies of\n    drop_cols -- List of columns to drop \n    '''\n    df_ = pd.get_dummies(df,columns=dummy_cols)\n    for i in to_drop:\n        try:\n            df_.drop(i,inplace=True,axis=1)\n        except:\n            pass\n    return df_\n\nto_dummy = ['Sex','Embarked','Ticket Extracted','Title']\nto_drop = ['Sex','Embarked','Name','Ticket','Cabin','Title']\n\n#train = get_dummies_and_drop(train,dummy_cols=to_dummy,drop_cols=to_drop)\n#test = get_dummies_and_drop(test,dummy_cols=to_dummy,drop_cols=to_drop)","4209443a":"# What columns have NA values?\n\ndef cols_with_na(df,df_name):\n    '''Print out columns with NA values, and save list of cols\n    Also returns list for further use\n    \n    Keyword arguments:\n    df -- dataframe to input\n    df_name -- string to be printed out to aid readability\n    '''\n    # Which columns have NA values?\n    cols_with_na = df.columns[df.isna().any()].to_list()\n    print(f'{df_name} columns with NA values: {cols_with_na}')\n    return(cols_with_na)\n\ntrain_na_cols = cols_with_na(train,'Train')\ntest_na_cols = cols_with_na(test,'Test')","ea242b74":"# Of columns with NA values, how many NAs are there?\n# I.e. if not many then I can impute, but if too many then may have to drop column\n\ndef percent_na_values(df,df_name,impute_thres):\n    '''Print out % of columns with NA values that is made up of NA values\n    Allows determination of if imputation is possible or if column needs to be dropped\n    \n    Keyword arguments:\n    df -- dataframe to input\n    df_name -- string to be printed out to aid readability\n    impute_thres -- threshold for determining whether to impute: if 25, will print \"ok to impute\" for cols with <= 25% na\n    '''\n    # Which columns have NA values?\n    cols_with_na = df.columns[df.isna().any()].to_list()\n    df_len = len(df)\n\n    # For cols with na, print % of col that is na, and print 'ok to impute' if thres < inputted threshold\n    for i in cols_with_na:\n        count_missing = df[i].isna().sum()\n        percent_missing = round(count_missing\/df_len*100,1)\n        print(f'Percent of {df_name} {i} missing = {percent_missing}')\n        if percent_missing <= impute_thres:\n            print(f'Ok to impute {i} (below chosen {impute_thres}% impute threshold)')\n        else:\n            print(f'Don\\'t impute {i}')\n        \npercent_na_values(train,'Train',impute_thres=25)\nprint('')\npercent_na_values(test,'Test',impute_thres=25)","be07132e":"# Deciding against using this for now\n\ndef multivariate_imputer(df):\n    \"\"\"Use sklearn's multivariate imputer to impute in a more sophisticated way than just the median\"\"\"\n    imp = IterativeImputer(max_iter=20, random_state=0,min_value=0)\n    imp.fit(df)\n    df_imputed = imp.transform(df)\n    df_imputed = pd.DataFrame(df_imputed,columns=df.columns)\n    return df_imputed\n\ntrain = multivariate_imputer(train)\ntest = multivariate_imputer(test)","316f5e2f":"imputer = SimpleImputer(strategy='most_frequent')\n\ndef impute_df(df):\n    df_imputed = pd.DataFrame(imputer.fit_transform(df))\n    df_imputed.columns = df.columns\n    df = df_imputed.copy() # to get back to normal naming convention\n    return df\n\n#train = impute_df(train)\n#test = impute_df(test)\n\nprint(f'Total NA in train: {train.isna().sum().sum()}')\nprint(f'Total NA in test: {test.isna().sum().sum()}')","2c19cac0":"# qcut to create age and fare bins\ndef bin_maker(df, col_name, num_bins):\n    '''Input dataframe, the name of the column and the number of qcut bins'''\n    df[f'{col_name} Bins'] = pd.qcut(df[col_name],q=num_bins,labels=False,duplicates='drop')\n\nfor i in [train,test]:\n    for j in ['Age','Fare']:\n        bin_maker(df=i,col_name=j,num_bins=10)","16a8efba":"def threshold_explainer(df,col,num_bins):\n    \"\"\"Prints the min and max for each qcut threshold\"\"\"\n    for i in np.arange(num_bins):\n        df_ = df[df[col+' Bins']==i].copy()\n        min_ = df_[col].min()\n        max_ = df_[col].max()\n        print(f'Min for {col} Bins group {i} is {min_} and max is {max_}')\n        \nthreshold_explainer(train,'Age',num_bins=10)\nprint('')\nthreshold_explainer(train,'Fare',num_bins=10)","087c598a":"# Add \"family\" column\nfor df in [train, test]:\n    df['Family'] = df['Parch'] + df['SibSp']","6bbead70":"# Correlation matrix - maybe I should have used label encoding rather than one-hot!!\ncorr_matrix = train.corr()\nfig = plt.figure(figsize=[15,10])\nsns.heatmap(corr_matrix,annot=True)","794557b5":"# Dropping age, fare and passenger ID as logistic regression wants all features to be similarly scaled \nfor df in [train,test]:\n    df.drop(['Age','Fare'],inplace=True,axis=1)","9fb67b6d":"# Making X and y\nX = train.copy()\nX.drop('Survived',inplace=True,axis=1)\ny = train['Survived'].copy()\n\n# Train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","3751c6a1":"def gridsearch(X,y,model,grid,cv):\n    '''Gridsearch function to perform a gridsearch with different models\n    And then return the best parameters for use\n    \n    Keyword arguments:\n    model -- model to use i.e. XGB\n    grid -- dictionary of parameters to try out\n    cv -- size of cross validation\n    '''\n    CV = GridSearchCV(estimator=model,param_grid=grid,cv=cv)\n    CV.fit(X, y)\n    print(CV.best_params_)\n    print('Best parameters returned for use')\n    return(CV.best_params_)","a34cfe1f":"xgb = XGBClassifier()\nxgb_grid = {\n    'learning_rate':[0.001,0.002,0.003,0.004,0.005],\n    'n_estimators':[60,70,80,90],\n    'max_depth':[2,3,4],\n    'subsample':[0.1,0.2,0.3,0.4]}\n\nforest = RandomForestClassifier()\nforest_grid = {\n    'max_depth':[4,5,6],\n    'max_leaf_nodes':[25,30,35],\n    'n_estimators':[550,600,650]}\n\n# Run the function\nxgb_params = gridsearch(X,y,xgb,xgb_grid,5)\nforest_params = gridsearch(X,y,forest,forest_grid,5)","1d17cc89":"xgb = XGBClassifier(learning_rate=xgb_params['learning_rate'],max_depth=xgb_params['max_depth'],\n                    n_estimators=xgb_params['n_estimators'],subsample=xgb_params['subsample'])\n\nforest = RandomForestClassifier(max_depth=forest_params['max_depth'],max_leaf_nodes=forest_params['max_leaf_nodes'],\n                                n_estimators=forest_params['n_estimators'])","b22f4930":"def cross_val(model_name,model,X,y,cv):\n    '''Cross validate a model and gives scores and average score\n    \n    Keyword arguments:\n    model_name -- string of the name, for printing out\n    model -- model i.e. xgb, forest\n    X -- data to use with no target\n    y -- target\n    cv -- number of cross validations\n    '''\n    scores = cross_val_score(model, X, y, cv=cv)\n    print(f'{model_name} Scores:')\n    for i in scores:\n        print(round(i,2))\n    print(f'Average {model_name} score: {round(scores.mean(),2)}')","80c5c6b4":"cross_val('XGB',xgb,X,y,5)\ncross_val('Forest',forest,X,y,5)","20fbe2ae":"xgb.fit(X,y)\n\npreds = xgb.predict(test)\n\ntest.PassengerId.astype(int)","d4a7d9c6":"output = pd.DataFrame({'PassengerId': test.PassengerId.astype(int), 'Survived': preds.astype(int)})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","6560a5ec":"# The model - Gridsearch","88d733df":"#### Conclusion:\n- Sex, embarked, ticket extracted, title to <font color=green>be encoded<\/font>\n- Name, ticket and cabin are too unique to have much meaning, and can <font color=green>be dropped<\/font>","85ffbf3e":"# Imports & Loading Data","1208a950":"## GridSearch\n\n- Working out the best parameters to use","a7afdbca":"# Making predictions and outputting","cf1bcbbd":"### <font color=purple>Fancy imputation<\/font>\n- What correlates with age, so I can say \"I don't know their age, but I know their _ and _ so I can make a guess\"?\n\n\n\n1. Class\n2. Title\n3. SibSp\n4. Parch","cf738319":"---\n\n---\n\n---\n\n# Early feature engineering - title & getting letters from tickets\n1. Get <font color=green>**titles**<\/font> of people from their name \n2. Extract <font color=green>letters from tickets<\/font>= in case this encodes cabin info\n\n**Will do more after the cleaning step**, once imputation has been used (i.e. making bins for ages)\n","ecb72784":"### <font color=purple>Feature eng 4. Family Column<\/font>","0819928a":"# The model - Cross validation","c6ed2d90":"## <font color=purple>Cleaning 2. Filling NAs (imputation)<\/font>","cc2da160":"## Train test split\n- Actually don't think I need this if I'm doing cross validation","d81f1787":"# Feature engineering continued - bins & family column\n\n(Now that I've done imputation)","2f5e541a":"# *<font color=purple>What<\/font> <font color=blue>this<\/font> <font color=red>notebook<\/font> <font color=green>contains<\/font>*\n\nThis notebook is still a **work in progress**, and at the moment contains:\n\n- Feature Engineering:\n    - <font color=green>**Titles**<\/font> (from names of passengers)\n    - Extracted <font color=green>**letters from tickets**<\/font> (could indicate the cabins people were in)\n    - <font color=green>**\"Family\"**<\/font> column\n    - Age and Ticket fare <font color=green>**bins**<\/font> (i.e. people who are ages 0 to 3 are in bin 1, etc)\n- Data cleaning, with some <font color=green>**snazzy functions**<\/font>\n- <font color=green>**Cross validation**<\/font>\n- Custom function to a) run <font color=green>**GridSearch**<\/font> and b) fit a model with the best parameters found by the GridSearch\n\nWhat this notebook doesn't contain right now:\n- Any EDA (exploratory data analysis), other than a heatmap\n\n---","5ccfd283":"### <font color=purple>Feature Eng 1. Making title column<\/font>","18d353b9":"## <font color=purple>Feature eng 3. Bins<\/font>\n- Creating bins for age and fare\n- qcut splits a column into equally sized bins, with the thresholds calculated automatically","1b35fe6c":"### <font color=purple>Simple imputation<\/font>","572c78f4":"# Cleaning - encoding categoricals and simple imputation\n### <font color=purple>Cleaning 1. Encoding categorical variables<\/font>","9628472f":"### <font color=purple>Feature Eng 2. Extracting letters from tickets, could be important<\/font>\n- This [forum post](https:\/\/www.encyclopedia-titanica.org\/cabins.html) implies ticket info is useful, and makes sense as it might contain cabin data which I'm sorely lacking"}}