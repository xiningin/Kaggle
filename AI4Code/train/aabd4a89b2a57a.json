{"cell_type":{"bd35a0fe":"code","1ce59034":"code","fc32a8fc":"code","ad3e903b":"code","3990d111":"code","21af7760":"code","503d32ff":"code","5fdc06b2":"code","159b4a2f":"code","e6d4246e":"code","c8ca559a":"code","18f1a0f2":"code","0d63c962":"code","e45cb28c":"code","e04d1ab6":"code","8aec393a":"code","2c906653":"code","2beb7a11":"code","ad2d597d":"code","b4cfa6ca":"code","b99e8f57":"code","759d4f55":"code","b411ce71":"code","8e3b6ff2":"code","52d17fef":"code","440527c5":"code","8320b44c":"code","880caf00":"code","f0db24a3":"markdown","73d5ddb0":"markdown","71e06f56":"markdown","afa77a56":"markdown","eaf84651":"markdown","a1aff115":"markdown","67e7cb27":"markdown","26558e54":"markdown","f701e7d3":"markdown","a1894cc7":"markdown","ca1eb802":"markdown","2eadae58":"markdown","26e74515":"markdown","eec9024a":"markdown","0cea82d1":"markdown","cff0681d":"markdown","eb3c6b1f":"markdown","bc05badf":"markdown","58682241":"markdown","11c3106a":"markdown","3fb13a91":"markdown","282c6f86":"markdown","797ff9e2":"markdown","7dbecabb":"markdown","40c2cd53":"markdown","5f004fea":"markdown"},"source":{"bd35a0fe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1ce59034":"star_wars = [125, 1977]\nraiders = [115, 1981]\nmean_girls = [97, 2004]","fc32a8fc":"def distance(movie1, movie2):\n    squared_difference = 0\n    for i in range(len(movie1)):\n        squared_difference += (movie1[i] - movie2[i]) ** 2\n    final_distance = squared_difference ** 0.5\n    return final_distance\n\nprint(distance(star_wars,raiders ))\nprint(distance(star_wars,mean_girls))","ad3e903b":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(12,10))\nplt.imshow(plt.imread(\"..\/input\/difference\/Capture.PNG\"))","3990d111":"release_dates = [1897, 1998, 2000, 1948, 1962, 1950, 1975, 1960, 2017, 1937, 1968, 1996, 1944, 1891, 1995, 1948, 2011, 1965, 1891, 1978]\ndef min_max_normalize(lst):\n    minimum = min(lst)\n    maximum = max(lst)\n    normalized = list()\n    for i in lst:\n        normalized.append((i-minimum)\/(maximum-minimum))\n    return normalized\nprint(min_max_normalize(release_dates))","21af7760":"df = pd.read_csv(\"..\/input\/the-movies-dataset\/movies_metadata.csv\")\ndf.head()","503d32ff":"df.info()","5fdc06b2":"def distance(movie1, movie2):\n    squared_difference = 0\n    for i in range(len(movie1)):\n        squared_difference += (movie1[i] - movie2[i]) ** 2\n    final_distance = squared_difference ** 0.5\n    return final_distance","159b4a2f":"distance([df.iloc[0][\"revenue\"],df.iloc[0][\"runtime\"],df.iloc[0][\"vote_average\"]],[df.iloc[1][\"revenue\"],df.iloc[1][\"runtime\"],df.iloc[0][\"vote_average\"]])\n#This is the distance calculation between the first and second filme in the dataset based on three columns in the dataset","e6d4246e":"def classify(unknown,dataset, k):\n    distances = list()\n    for row_number, row in dataset.iterrows():\n        movie = [row[\"revenue\"],row[\"runtime\"],row[\"vote_average\"]]\n        distance_to_point = distance(movie, unknown)\n        distances.append([distance_to_point,row[\"title\"]])\n    distances.sort()\n    neighbors = distances[:k]\n    return neighbors\nprint(classify([373554033, 91, 8],df,5))","c8ca559a":"df.isnull().sum()\ndf.drop([\"belongs_to_collection\",\"homepage\",\"tagline\",\"overview\",\"poster_path\",\"imdb_id\",\"original_language\"], axis=1,inplace=True)","18f1a0f2":"df[\"popularity\"].fillna(value=df[\"popularity\"].mode(),inplace=True)\ndf[\"popularity\"] = pd.to_numeric(df[\"popularity\"],errors ='coerce')\ndf.info()","0d63c962":"def label_encoder(row):\n    if row[\"popularity\"] > 2.92:\n        return 1\n    else:\n        return 0","e45cb28c":"df[\"label\"] = df.apply(lambda row: label_encoder(row), axis=1)\ndf.head(3)","e04d1ab6":"def classify(unknown,dataset,labels, k):\n    distances = list()\n    for row_number, row in dataset.iterrows():\n        movie = [row[\"revenue\"],row[\"runtime\"],row[\"vote_average\"]]\n        distance_to_point = distance(movie, unknown)\n        distances.append([distance_to_point,row[\"title\"],row[\"label\"]])\n    distances.sort()\n    neighbors = distances[0:k]\n    num_good = 0\n    num_bad = 0\n    for neighbor in neighbors:\n        label = neighbor[2]\n        if label == 0:\n            num_bad += 1\n        elif label == 1:\n            num_good += 1\n    if num_good > num_bad:\n        return 1\n    else:\n        return 0\nprint(classify([373554033, 91, 8],df,df[\"label\"],5))","8aec393a":"df.drop([\"id\",\"genres\",\"spoken_languages\",\"title\",\"original_title\",\"production_companies\",\"production_countries\"],axis=1,inplace=True)\ndf.head(3)","2c906653":"df.isnull().sum()","2beb7a11":"\ndf[\"release_date\"].fillna(\"2008-01-01\",inplace=True)\ndf[\"revenue\"].fillna(df[\"revenue\"].mean(),inplace=True)\ndf[\"runtime\"].fillna(df[\"runtime\"].mean(),inplace=True)\ndf[\"status\"].fillna(\"Released\",inplace=True)\ndf[\"video\"].fillna(\"False\",inplace=True)\ndf[\"popularity\"].fillna(df[\"popularity\"].mean(),inplace=True)\ndf[\"vote_average\"].fillna(df[\"vote_average\"].mean(),inplace=True)\ndf[\"vote_count\"].fillna(df[\"vote_count\"].mean(),inplace=True)","ad2d597d":"df.isnull().sum()","b4cfa6ca":"df[\"release_year\"] = df[\"release_date\"].apply(lambda x : int(x[:4]))\ndf[\"release_month\"] = df[\"release_date\"].apply(lambda x : x[5:7])\ndf = df[df[\"release_month\"]!= \"\"]\ndf[\"release_month\"] = df[\"release_month\"].apply(lambda x : int())\ndf.drop(\"release_date\",axis=1,inplace=True)\ndf[\"budget\"] = df[\"budget\"].apply(lambda x: float(x))\ndf.info()\n                                               ","b99e8f57":"for i in df.select_dtypes(\"object\").columns:\n    print(f\"Column {i} has these type of data: {df[i].nunique()}\")\n    print(\"***************************************************\")","759d4f55":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf[\"adult\"] = le.fit_transform(df[\"adult\"])\ndf_transformed = pd.get_dummies(df, columns=[\"status\",\"video\"])\ndf_transformed","b411ce71":"df_transformed.info()","8e3b6ff2":"X = df_transformed.drop(\"label\",axis=1)\ny = df_transformed[\"label\"]\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=101)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","52d17fef":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nX_train = ss.fit_transform(X_train)\nX_test = ss.transform(X_test)","440527c5":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train,y_train) # the algorithm fits with our data\npredictions=knn.predict(X_test)\nfrom sklearn.metrics import classification_report, confusion_matrix, silhouette_score\nprint(confusion_matrix(y_test,predictions))\nprint(\"**********************************\")\nprint(silhouette_score(X_test,y_test))\nprint(\"**********************************\")\nprint(classification_report(y_test,predictions))","8320b44c":"error_rate=list()\n#here we iterate meny different k values and plot their error rates \n#and discover which one is better than others and has the lowest error rate\nfor i in range(1,40):\n    knn=KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    prediction_i=knn.predict(X_test)\n    error_rate.append(np.mean(prediction_i != y_test))\n# Now we will plot the prediction error rates of different k values\nplt.figure(figsize=(15,10))\nplt.plot(range(1,40),error_rate, color=\"blue\", linestyle=\"--\",marker=\"o\",markerfacecolor=\"red\",markersize=10)\nplt.title(\"Error Rate vs K Value\")\nplt.xlabel=\"K Value\"\nplt.ylabel(\"Error Rate\")","880caf00":"knn=KNeighborsClassifier(n_neighbors=7)\nknn.fit(X_train, y_train)\npredictions=knn.predict(X_test)\nprint(confusion_matrix(y_test,predictions))\nprint(\"**********************************\")\nprint(silhouette_score(X_test,y_test))\nprint(\"**********************************\")\nprint(classification_report(y_test,predictions))","f0db24a3":"<font color=\"purple\">\nWe\u2019ve now written your own K-Nearest Neighbor classifier from scratch! However, rather than writing your own classifier every time, you can use Python\u2019s sklearn library. sklearn is a Python library specifically used for Machine Learning","73d5ddb0":"<font color=\"purple\">\nHere we split data into training and test set.","71e06f56":"<font color=\"purple\">\nOur goal now is to count the number of good movies and bad movies in the list of neighbors. If more of the neighbors were good, then the algorithm will classify the unknown movie as good. Otherwise, it will classify it as bad. Lets use popularity column to create a label column that shows 0 or 1 based on their popularity","afa77a56":"<font color=\"purple\">\nIf we have two features, this means we have 2D data, and we can simply calculate distance between two features by returning the square root of the total of square of the differences of each feature as follows:","eaf84651":"<font color=\"purple\">\nUsing this formula above, we can find the K-Nearest Neighbors of a point in N-dimensional space","a1aff115":"<font color=\"purple\">\nConsider the two dimensions of release date and budget. The maximum difference between two movies\u2019 release dates is about 125 years (The Lumi\u00e8re Brothers were making movies in the 1890s). However, the difference between two movies\u2019 budget can be millions of dollars.Another way of thinking about this is that the budget completely outweighs the importance of all other dimensions because it is on such a huge scale. The fact that two movies were 70 years apart is essentially meaningless compared to the difference in millions in the other dimension. Therefore, we need to normalize the data before applying K Nearest Neightbors Algorithm which makes every value be between 0 and 1.","67e7cb27":"<font color=\"purple\">\nLet\u2019s say this third dimension is the movie\u2019s budget. We now have to find the distance between these two points in three dimensions.","26558e54":"## 3. Finding the Nearest Neighbors","f701e7d3":"<font color=\"purple\">\nFirst of all we need to fill or ged tird of missing values for each columns","a1894cc7":"## 2. Data Normalization","ca1eb802":"<font color=\"purple\">\nAlthough k=1 is very good for our predictions, we will check whether there is better k value or not","2eadae58":"## 4. Classify Movie","26e74515":"## 5.1. Data Preprocessing","eec9024a":"<font color=\"purple\">\nNow all the columns are numerical as it is seen below:","0cea82d1":"<font color=\"purple\">\nThe next step is to deal with columns with non numerical values. ","cff0681d":"<font color=\"purple\">\nBy using this function we make all the data be between 0 and 1.","eb3c6b1f":"<font color=\"purple\">\nNow, with new k value, the algorithm has % 98 accuracy and the confusion matrix results are also better than before","bc05badf":"<font color=\"purple\">\nHere we rescale the data before applying the algorithm.","58682241":"<font color=\"purple\">\nOur classifier is now able to predict whether a movie will be good or bad and returns 1 if it is good or returns 0 if it is bad.","11c3106a":"<font color=\"purple\">\nK-Nearest Neighbors (KNN) is a classification algorithm. The central idea is that data points with similar attributes tend to fall into similar categories.If you have a dataset of points where the class of each point is known, you can take a new point with an unknown class, find it\u2019s nearest neighbors, and classify it.","3fb13a91":"<font color=\"purple\">\nAccording to figure,  k = 7 value give the lowest error rate. Now we will choose k=7 and evaluate its performance again","282c6f86":"## 1. Calculating Distance","797ff9e2":"<font color=\"purple\">\nAbove we see the 5 nearest neigbors to the example film from the entire dataset based on calculation of three features.We\u2019ve now found the k nearest neighbors, and have stored them in a list that looks like in the list above.","7dbecabb":"## 5.Using Sklearn Builtin K Neares Neighbors Algorithm","40c2cd53":"<font color=\"purple\">\nIn order to find the 5 nearest neighbors, we need to compare this new unclassified movie to every other movie in the dataset. This means we\u2019re going to be using the distance formula again and again. We ultimately want to end up with a sorted list of distances and the movies associated with those distances.","5f004fea":"<font color=\"purple\">\nSecondly we need to do some feature engineering."}}