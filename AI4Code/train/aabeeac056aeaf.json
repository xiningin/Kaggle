{"cell_type":{"d0ac6aa2":"code","e70b6369":"code","037cc1a4":"code","cf2cbce8":"code","982bdc9e":"code","2b034cfa":"code","1d7e3fd8":"code","3fa14995":"code","5500cf5d":"code","1e51d569":"markdown","59f49904":"markdown"},"source":{"d0ac6aa2":"import numpy as np\nimport pandas as pd\nimport os\nfrom PIL import Image\n\nTRAIN_DIR = '\/kaggle\/input\/alzheimers-dataset-4-class-of-images\/Alzheimer_s Dataset\/train\/'\nTEST_DIR = '\/kaggle\/input\/alzheimers-dataset-4-class-of-images\/Alzheimer_s Dataset\/test\/'\nIMAGE_SIZE = 176\nCLASSES = [\n    'NonDemented',\n    'VeryMildDemented',\n    'MildDemented',\n    'ModerateDemented',\n]\ntrain_images = {}\ntest_images = {}\n\nfor klass in CLASSES:\n    train_images[klass] = []\n    test_images[klass] = []\n\nfor klass in CLASSES:\n    for image in os.listdir(TRAIN_DIR + klass):\n        im = Image.open(TRAIN_DIR + klass + '\/' + image).convert('L')\n        train_images[klass].append(im.resize((IMAGE_SIZE, IMAGE_SIZE)))\n        \n    for image in os.listdir(TEST_DIR + klass):\n        im = Image.open(TEST_DIR + klass + '\/' + image).convert('L')\n        test_images[klass].append(im.resize((IMAGE_SIZE, IMAGE_SIZE)))\n\nfrom matplotlib import pyplot as plt\n\nplt.figure(1, figsize=(20, 4))\ni = 1\nfor klass in CLASSES:\n    avg_pic = np.zeros((IMAGE_SIZE, IMAGE_SIZE))\n    for pic in train_images[klass] + test_images[klass]:\n        avg_pic += np.array(pic)\n    avg_pic = avg_pic \/ (len(train_images[klass]) + len(test_images[klass]))\n    plt.subplot(1, 4, i)\n    i += 1\n    plt.imshow(avg_pic, cmap='gray')\n    plt.xlabel('%s average\\ntotally %d\/%d pics' % (klass, len(train_images[klass]), len(test_images[klass])))\nplt.show()","e70b6369":"class_weight = {}\nmax_classes = len(test_images['NonDemented']) \/ 100\nfor i, klass in enumerate(CLASSES):\n    class_weight[i] = max_classes \/ len(test_images[klass])\nclass_weight","037cc1a4":"from keras.utils import np_utils\nfrom skimage.exposure import equalize_adapthist as eq_hist\n\ndef equalize(im):\n    return eq_hist(np.array(im), clip_limit=0.03)\n\ntrain_data = np.array([equalize(i) for i in train_images[CLASSES[0]]])\ntrain_labels = np.zeros((len(train_images[CLASSES[0]]), 1))\nfor ind, klass in enumerate(CLASSES[1:], start=1):\n    klass_arr = np.array([equalize(i) for i in train_images[klass]])\n    train_data = np.concatenate([train_data, klass_arr], axis=0)\n    \n    labels_arr = np.ones((len(train_images[klass]), 1)) * ind\n    train_labels = np.concatenate([train_labels, labels_arr], axis=0)\n\ntest_data = np.array([equalize(i) for i in test_images[CLASSES[0]]])\ntest_labels = np.zeros((len(test_images[CLASSES[0]]), 1))\nfor ind, klass in enumerate(CLASSES[1:], start=1):\n    klass_arr = np.array([equalize(i) for i in test_images[klass]])\n    test_data = np.concatenate([test_data, klass_arr], axis=0)\n    \n    labels_arr = np.ones((len(test_images[klass]), 1)) * ind\n    test_labels = np.concatenate([test_labels, labels_arr], axis=0)\n\n    \ntrain_data = train_data.reshape((-1, IMAGE_SIZE, IMAGE_SIZE, 1))\ntest_data = test_data.reshape((-1, IMAGE_SIZE, IMAGE_SIZE, 1))\n\ntrain_labels = np_utils.to_categorical(train_labels)\ntest_labels = np_utils.to_categorical(test_labels)\n\ntrain_data.shape, train_labels.shape, test_data.shape, test_labels.shape","cf2cbce8":"from keras.utils.data_utils import Sequence\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.keras import balanced_batch_generator\n\np = np.random.permutation(train_data.shape[0])\ntrain_data = train_data[p]\ntrain_labels = train_labels[p]\n\nros = RandomOverSampler(random_state=42)\ntrain_ros_data, train_ros_labels = ros.fit_resample(train_data.reshape((-1, IMAGE_SIZE * IMAGE_SIZE)), train_labels)\ntest_ros_data, test_ros_labels = ros.fit_resample(test_data.reshape((-1, IMAGE_SIZE * IMAGE_SIZE)), test_labels)\n\ntrain_ros_data = train_ros_data.reshape((-1, IMAGE_SIZE, IMAGE_SIZE, 1))\ntest_ros_data = test_ros_data.reshape((-1, IMAGE_SIZE, IMAGE_SIZE, 1))\n\ntrain_ros_data.shape, train_ros_labels.shape, test_ros_data.shape, test_ros_labels.shape","982bdc9e":"from keras.callbacks import Callback\nfrom keras import backend\nfrom keras.models import load_model\nimport math\n\n# this callback applies cosine annealing, saves snapshots and allows to load them\nclass SnapshotEnsemble(Callback):\n    \n    __snapshot_name_fmt = \"snapshot_%d.hdf5\"\n    \n    def __init__(self, n_models, n_epochs_per_model, lr_max, verbose=1):\n        \"\"\"\n        n_models -- quantity of models (snapshots)\n        n_epochs_per_model -- quantity of epoch for every model (snapshot)\n        lr_max -- maximum learning rate (snapshot starter)\n        \"\"\"\n        self.n_epochs_per_model = n_epochs_per_model\n        self.n_models = n_models\n        self.n_epochs_total = self.n_models * self.n_epochs_per_model\n        self.lr_max = lr_max\n        self.verbose = verbose\n        self.lrs = []\n \n    # calculate learning rate for epoch\n    def cosine_annealing(self, epoch):\n        cos_inner = (math.pi * (epoch % self.n_epochs_per_model)) \/ self.n_epochs_per_model\n        return self.lr_max \/ 2 * (math.cos(cos_inner) + 1)\n\n    # when epoch begins update learning rate\n    def on_epoch_begin(self, epoch, logs={}):\n        # update learning rate\n        lr = self.cosine_annealing(epoch)\n        backend.set_value(self.model.optimizer.lr, lr)\n        # log value\n        self.lrs.append(lr)\n\n    # when epoch ends check if there is a need to save a snapshot\n    def on_epoch_end(self, epoch, logs={}):\n        if (epoch + 1) % self.n_epochs_per_model == 0:\n            # save model to file\n            filename = self.__snapshot_name_fmt % ((epoch + 1) \/\/ self.n_epochs_per_model)\n            self.model.save(filename)\n            if self.verbose:\n                print('Epoch %d: snapshot saved to %s' % (epoch, filename))\n                \n    # load all snapshots after training\n    def load_ensemble(self):\n        models = []\n        for i in range(self.n_models):\n            models.append(load_model(self.__snapshot_name_fmt % (i + 1)))\n        return models","2b034cfa":"from keras.layers import Dense, Flatten, BatchNormalization, Dropout, Conv2D, MaxPooling2D, LeakyReLU\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.regularizers import l2\nfrom keras.optimizers import Adam\n\nimagegen = ImageDataGenerator(\n    rotation_range=15,\n    width_shift_range=15,\n    height_shift_range=15,\n    zoom_range=0.2\n)\n\ndef create_cnn(filters=[32], kernels=[3, 3, 5], dropout=0.5, denses=[128], reg=.0001):\n\n    model = Sequential()\n    \n    for i, fil in enumerate(filters):\n        if i == 0:\n            model.add(Conv2D(fil, kernels[0], padding='same', kernel_regularizer=l2(reg), input_shape=(IMAGE_SIZE, IMAGE_SIZE, 1)))\n        else:\n            model.add(Conv2D(fil, kernels[0], padding='same', kernel_regularizer=l2(reg)))\n        model.add(LeakyReLU())\n        \n        for ker in kernels[1:]:\n            model.add(BatchNormalization())\n            model.add(Conv2D(fil, ker, padding='same', kernel_regularizer=l2(reg)))\n            model.add(LeakyReLU())\n            \n        model.add(MaxPooling2D())\n        model.add(BatchNormalization())\n        model.add(Dropout(dropout))\n\n    model.add(Flatten())\n    \n    for den in denses:\n        model.add(Dense(den, kernel_regularizer=l2(reg)))\n        model.add(LeakyReLU())\n        model.add(BatchNormalization())\n        model.add(Dropout(dropout))\n    \n    model.add(Dense(len(CLASSES), activation='softmax'))\n    \n    model.compile(\n        loss=\"categorical_crossentropy\",\n        optimizer=Adam(.001),\n        metrics=['acc']\n    )\n\n    return model","1d7e3fd8":"se_callback = SnapshotEnsemble(n_models=5, n_epochs_per_model=50, lr_max=.003)\n\nmodel = create_cnn(filters=[32, 64, 128], kernels=[3, 3, 5], dropout=0.5, denses=[128, 64], reg=.0001)\nhistory = model.fit_generator(\n    imagegen.flow(train_ros_data, train_ros_labels, batch_size=32),\n    steps_per_epoch=len(train_ros_data) \/ 32,\n    epochs=se_callback.n_epochs_total,\n    verbose=0,\n    callbacks=[se_callback],\n    validation_data=(test_ros_data, test_ros_labels)\n)","3fa14995":"h = history.history\nplt.figure(1, figsize=(16, 10))\n\nplt.subplot(121)\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.plot(h['loss'], label='training')\nplt.plot(h['val_loss'], label='validation')\nplt.legend()\n\nplt.subplot(122)\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.plot(h['acc'], label='training')\nplt.plot(h['val_acc'], label='validation')\nplt.legend()\n\nplt.show()","5500cf5d":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix, f1_score\n\nmodels = se_callback.load_ensemble()\ndef predict(models_ensemble, data):\n    y_pred = np.zeros((data.shape[0], len(CLASSES)))\n    for model in models_ensemble:\n        y_pred += model.predict(data)\n    y_pred = np.argmax(y_pred, axis=1)\n    return y_pred\n\ny_true = np.argmax(test_labels, axis=1)\ny_pred = predict(models, test_data)\n\ncm = confusion_matrix(y_true, y_pred)\nprint('F1s = %.3f; %.3f' % (f1_score(y_true, y_pred, average='micro'), f1_score(y_true, y_pred, average='macro')))\n\ncm_norm = cm.copy().astype(float)\nfor i in range(cm_norm.shape[0]):\n    cm_norm[i] = cm_norm[i] \/ cm_norm[i].sum() * 100\n\nplt.figure(1, figsize=(14, 6))\n\nplt.subplot(121)\nplt.title('Confusion matrix')\nsns.heatmap(pd.DataFrame(cm, index=CLASSES, columns=CLASSES), annot=True, fmt='d', cbar=False)\n\nplt.subplot(122)\nplt.title('Normalized confusion matrix')\nsns.heatmap(pd.DataFrame(cm_norm, index=CLASSES, columns=CLASSES), annot=True, fmt='.1f', cbar=False)\n\nplt.show()","1e51d569":"<img src=\"https:\/\/i.imgur.com\/dcc6Gsm.jpg\" width=\"300\">","59f49904":"Possible improvements (TODO):\n* more complex CNN\n* better data balancing\n* ..."}}