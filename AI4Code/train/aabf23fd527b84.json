{"cell_type":{"28f680d2":"code","a1d735bf":"code","b6929dd4":"code","bab4cc6f":"code","08de017e":"code","e4220da9":"code","ea78c652":"code","3312ad9f":"code","d017d569":"code","6faf935c":"code","9c88ec22":"code","3a6a8c64":"code","a478c093":"code","73b7c68b":"code","dbfd9c43":"code","6069627a":"code","5039bef0":"code","2aacb298":"code","3db18794":"code","af5c41b1":"code","270be1c5":"code","a0acea91":"code","f7d864de":"code","bf873ce4":"code","c9d71e95":"code","cbbe2722":"code","b7737818":"code","76898c47":"code","2b71bbd0":"code","2ef6591d":"code","1a38cf0e":"code","28703652":"code","5c7fde86":"code","b1d13a40":"code","ae22aab3":"markdown","c4bfc144":"markdown","c63c4e0e":"markdown","fa717049":"markdown","038b1002":"markdown","37a69775":"markdown","ffca7191":"markdown","43284a81":"markdown","534b9f0f":"markdown","d4ad9a30":"markdown","0d9de291":"markdown","e87a092e":"markdown","456bffe0":"markdown"},"source":{"28f680d2":"# Importing the libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","a1d735bf":"df = pd.read_csv('..\/input\/Reviews.csv')","b6929dd4":"df=df[:5000]","bab4cc6f":"df.head()","08de017e":"df.shape","e4220da9":"# Cleaning the texts\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\ncorpus = []\nfor i in range(0, 10000):\n    review = re.sub('[^a-zA-Z]', ' ', df['Text'][i])\n    review = review.lower()\n    review = review.split()\n    ps = PorterStemmer()\n    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n    review = ' '.join(review)\n    corpus.append(review)","ea78c652":"corpus=pd.DataFrame(corpus, columns=['Reviews']) \ncorpus.head()","3312ad9f":"result=corpus.join(df[['Score']])\nresult.head()","d017d569":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer()\ntfidf.fit(result['Reviews'])","6faf935c":"X = tfidf.transform(result['Reviews'])\nresult['Reviews'][1]","9c88ec22":"print([X[1, tfidf.vocabulary_['peanut']]])","3a6a8c64":"print([X[1, tfidf.vocabulary_['jumbo']]])","a478c093":"print([X[1, tfidf.vocabulary_['error']]])","73b7c68b":"result.dropna(inplace=True)\nresult[result['Score'] != 3]\nresult['Positivity'] = np.where(result['Score'] > 3, 1, 0)\ncols = [ 'Score']\nresult.drop(cols, axis=1, inplace=True)\nresult.head()","dbfd9c43":"result.groupby('Positivity').size()","6069627a":"from sklearn.model_selection import train_test_split\nX = result.Reviews\ny = result.Positivity\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)","5039bef0":"print(\"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(X_train),\n                                                                             (len(X_train[y_train == 0]) \/ (len(X_train)*1.))*100,\n                                                                            (len(X_train[y_train == 1]) \/ (len(X_train)*1.))*100))","2aacb298":"print(\"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(X_test),\n                                                                             (len(X_test[y_test == 0]) \/ (len(X_test)*1.))*100,\n                                                                            (len(X_test[y_test == 1]) \/ (len(X_test)*1.))*100))","3db18794":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score","af5c41b1":"def accuracy_summary(pipeline, X_train, y_train, X_test, y_test):\n    sentiment_fit = pipeline.fit(X_train, y_train)\n    y_pred = sentiment_fit.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(\"accuracy score: {0:.2f}%\".format(accuracy*100))\n    return accuracy","270be1c5":"cv = CountVectorizer()\nrf = RandomForestClassifier(class_weight=\"balanced\")\nn_features = np.arange(10000,25001,5000)\n\ndef nfeature_accuracy_checker(vectorizer=cv, n_features=n_features, stop_words=None, ngram_range=(1, 1), classifier=rf):\n    result = []\n    print(classifier)\n    print(\"\\n\")\n    for n in n_features:\n        vectorizer.set_params(stop_words=stop_words, max_features=n, ngram_range=ngram_range)\n        checker_pipeline = Pipeline([\n            ('vectorizer', vectorizer),\n            ('classifier', classifier)\n        ])\n        print(\"Test result for {} features\".format(n))\n        nfeature_accuracy = accuracy_summary(checker_pipeline, X_train, y_train, X_test, y_test)\n        result.append((n,nfeature_accuracy))\n    return result","a0acea91":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer()","f7d864de":"print(\"Result for trigram with stop words (Tfidf)\\n\")\nfeature_result_tgt = nfeature_accuracy_checker(vectorizer=tfidf,ngram_range=(1, 3))","bf873ce4":"from sklearn.metrics import classification_report\n\ncv = CountVectorizer(max_features=30000,ngram_range=(1, 3))\npipeline = Pipeline([\n        ('vectorizer', cv),\n        ('classifier', rf)\n    ])\nsentiment_fit = pipeline.fit(X_train, y_train)\ny_pred = sentiment_fit.predict(X_test)\n\nprint(classification_report(y_test, y_pred, target_names=['negative','positive']))","c9d71e95":"## K-fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = pipeline, X= X_train, y = y_train,\n                             cv = 10)\nprint(\"Random Forest Classifier Accuracy: %0.2f (+\/- %0.2f)\"  % (accuracies.mean(), accuracies.std() * 2))","cbbe2722":"from sklearn.feature_selection import chi2\n\ntfidf = TfidfVectorizer(max_features=30000,ngram_range=(1, 3))\nX_tfidf = tfidf.fit_transform(result.Reviews)\ny = result.Positivity\nchi2score = chi2(X_tfidf, y)[0]","b7737818":"plt.figure(figsize=(16,8))\nscores = list(zip(tfidf.get_feature_names(), chi2score))\nchi2 = sorted(scores, key=lambda x:x[1])\ntopchi2 = list(zip(*chi2[-20:]))\nx = range(len(topchi2[1]))\nlabels = topchi2[0]\nplt.barh(x,topchi2[1], align='center', alpha=0.5)\nplt.plot(topchi2[1], x, '-o', markersize=5, alpha=0.8)\nplt.yticks(x, labels)\nplt.xlabel('$\\chi^2$')\nplt.show();","76898c47":"from sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\nimport re","2b71bbd0":"max_fatures = 30000\ntokenizer = Tokenizer(nb_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(result['Reviews'].values)\nX1 = tokenizer.texts_to_sequences(result['Reviews'].values)\nX1 = pad_sequences(X1)","2ef6591d":"Y1 = pd.get_dummies(result['Positivity']).values\nX1_train, X1_test, Y1_train, Y1_test = train_test_split(X1,Y1, random_state = 42)\nprint(X1_train.shape,Y1_train.shape)\nprint(X1_test.shape,Y1_test.shape)","1a38cf0e":"embed_dim = 150\nlstm_out = 200\n\nmodel = Sequential()\nmodel.add(Embedding(max_fatures, embed_dim,input_length = X1.shape[1], dropout=0.2))\nmodel.add(LSTM(lstm_out, dropout_U=0.2,dropout_W=0.2))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nprint(model.summary())","28703652":"batch_size = 32\nmodel.fit(X1_train, Y1_train, nb_epoch = 7, batch_size=batch_size, verbose = 2)","5c7fde86":"score,acc = model.evaluate(X1_test, Y1_test, verbose = 2, batch_size = batch_size)\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))","b1d13a40":"pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\nfor x in range(len(X1_test)):\n    \n    result = model.predict(X1_test[x].reshape(1,X1_test.shape[1]),batch_size=1,verbose = 2)[0]\n   \n    if np.argmax(result) == np.argmax(Y1_test[x]):\n        if np.argmax(Y1_test[x]) == 0:\n            neg_correct += 1\n        else:\n            pos_correct += 1\n       \n    if np.argmax(Y1_test[x]) == 0:\n        neg_cnt += 1\n    else:\n        pos_cnt += 1\n\n\n\nprint(\"pos_acc\", pos_correct\/pos_cnt*100, \"%\")\nprint(\"neg_acc\", neg_correct\/neg_cnt*100, \"%\")","ae22aab3":"Next, I compose the LSTM Network. Note that embed_dim, lstm_out, batch_size, droupout_x variables are hyperparameters, their values are somehow intuitive, can be and must be played with in order to achieve good results. Please also note that I am using softmax as activation function. The reason is that our Network is using categorical crossentropy, and softmax is just the right activation method for that.","c4bfc144":"Among the three words, \u201cpeanut\u201d, \u201cjumbo\u201d and \u201cerror\u201d, tf-idf gives the highest weight to \u201cjumbo\u201d. Why? This indicates that \u201cjumbo\u201d is a much rarer word than \u201cpeanut\u201d and \u201cerror\u201d. This is how to use the tf-idf to indicate the importance of words or terms inside a collection of documents.","c63c4e0e":"# Train Test Split","fa717049":"Keras Embedding Layer\nKeras offers an Embedding layer that can be used for neural networks on text data.\n\nIt requires that the input data be integer encoded, so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API also provided with Keras.\n\nThe Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset.\n\nIt is a flexible layer that can be used in a variety of ways, such as:\n\nIt can be used alone to learn a word embedding that can be saved and used in another model later.\nIt can be used as part of a deep learning model where the embedding is learned along with the model itself.\nIt can be used to load a pre-trained word embedding model, a type of transfer learning.\nThe Embedding layer is defined as the first hidden layer of a network. It must specify 3 arguments:\n\nIt must specify 3 arguments:\n\ninput_dim: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\noutput_dim: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\ninput_length: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.\nFor example, below we define an Embedding layer with a vocabulary of 200 (e.g. integer encoded words from 0 to 199, inclusive), a vector space of 32 dimensions in which words will be embedded, and input documents that have 50 words each.\n\n\ne = Embedding(200, 32, input_length=50)\n1\ne = Embedding(200, 32, input_length=50)\nThe Embedding layer has weights that are learned. If you save your model to file, this will include weights for the Embedding layer.\n\nThe output of the Embedding layer is a 2D vector with one embedding for each word in the input sequence of words (input document).\n\nIf you wish to connect a Dense layer directly to an Embedding layer, you must first flatten the 2D output matrix to a 1D vector using the Flatten layer.","038b1002":"# TFIDF","37a69775":"\n# Text Cleaning or Preprocessing","ffca7191":"# Chi2 Feature Selection","43284a81":"# LSTM neural network","534b9f0f":"\nFinally measuring the number of correct guesses. It is clear that finding negative tweets goes very well for the Network but deciding whether is positive is not really.","d4ad9a30":"TFIDF is an information retrieval technique that weighs a term\u2019s frequency (TF) and its inverse document frequency (IDF). Each word has its respective TF and IDF score. The product of the TF and IDF scores of a word is called the TFIDF weight of that word.\n\nPut simply, the higher the TFIDF score (weight), the rarer the word and vice versa","0d9de291":"This dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories.","e87a092e":"# Sentiment Classification","456bffe0":"# Sentiment Classification with Natural Language Processing on LSTM "}}