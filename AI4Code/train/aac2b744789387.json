{"cell_type":{"9a2fa0e5":"code","d1898950":"code","98830821":"code","09325bd9":"code","07f49d7b":"code","0fd6f340":"code","9a976a94":"code","a346a488":"code","35604111":"code","78ea2baa":"code","49a50395":"code","f05cf32b":"code","4aff077c":"code","e44fabd0":"code","0b612a7a":"code","ad2bd6f0":"code","e776e956":"code","f3d66189":"code","cf0c5d5a":"code","2eae47be":"code","0680f2fb":"code","f9ba3015":"code","97d13d1c":"code","4e86cda4":"code","e6271741":"code","3a94d052":"code","4c4875a1":"code","2f123532":"code","ca62eaf5":"code","1fa6b2fc":"code","2b505d8f":"code","de5a9af9":"code","03f46d66":"code","07705b20":"code","300aab61":"code","c154e7d0":"code","f51c590c":"markdown","5e995884":"markdown","f908c4dd":"markdown","a12f42ef":"markdown","f59f0446":"markdown","8705a4ae":"markdown","219c0204":"markdown","956ddede":"markdown","11c309de":"markdown","4f156419":"markdown","5b5c6d3d":"markdown","3230593c":"markdown","74343a40":"markdown","9b2e0401":"markdown","25698f66":"markdown","aee84e57":"markdown","d5180fe2":"markdown","64525f1f":"markdown","2d15881a":"markdown","8a5e756a":"markdown","924aac63":"markdown","39eafd04":"markdown","07bceed6":"markdown","38b0c99b":"markdown","c950cb7c":"markdown","5f8f87cb":"markdown","7fae6c55":"markdown","b71f6458":"markdown","baaa7410":"markdown"},"source":{"9a2fa0e5":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d1898950":"pd.set_option('display.max_columns', None)\ncard = pd.read_csv(r'\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ncard.head()","98830821":"card.info()","09325bd9":"card.describe()","07f49d7b":"card.shape","0fd6f340":"# percentage of missing values in each column\nround(100 * (card.isnull().sum()\/len(card)),2).sort_values(ascending=False)","9a976a94":"# percentage of missing values in each row\nround(100 * (card.isnull().sum(axis=1)\/len(card)),2).sort_values(ascending=False)","a346a488":"card_d=card.copy()\ncard_d.drop_duplicates(subset=None, inplace=True)","35604111":"card.shape","78ea2baa":"card_d.shape","49a50395":"## Assigning removed duplicate datase to original \ncard=card_d\ncard.shape","f05cf32b":"del card_d","4aff077c":"card.info()\n","e44fabd0":"def draw_histograms(dataframe, features, rows, cols):\n    fig=plt.figure(figsize=(20,20))\n    for i, feature in enumerate(features):\n        ax=fig.add_subplot(rows,cols,i+1)\n        dataframe[feature].hist(bins=20,ax=ax,facecolor='midnightblue')\n        ax.set_title(feature+\" Distribution\",color='DarkRed')\n        ax.set_yscale('log')\n    fig.tight_layout()  \n    plt.show()\ndraw_histograms(card,card.columns,8,4)","0b612a7a":"card.Class.value_counts()","ad2bd6f0":"ax=sns.countplot(x='Class',data=card);\nax.set_yscale('log')","e776e956":"plt.figure(figsize = (40,10))\nsns.heatmap(card.corr(), annot = True, cmap=\"tab20c\")\nplt.show()","f3d66189":"card.shape","cf0c5d5a":"card.info()","2eae47be":"#Dropping Time as it's non business required data\nestimators=[ 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n\nX1 = card[estimators]\ny = card['Class']","0680f2fb":"col=X1.columns[:-1]\ncol","f9ba3015":"X = sm.add_constant(X1)\nreg_logit = sm.Logit(y,X)\nresults_logit = reg_logit.fit()","97d13d1c":"results_logit.summary()","4e86cda4":"def back_feature_elem (data_frame,dep_var,col_list):\n    \"\"\" Takes in the dataframe, the dependent variable and a list of column names, runs the regression repeatedly eleminating feature with the highest\n    P-value above alpha one at a time and returns the regression summary with all p-values below alpha\"\"\"\n\n    while len(col_list)>0 :\n        model=sm.Logit(dep_var,data_frame[col_list])\n        result=model.fit(disp=0)\n        largest_pvalue=round(result.pvalues,3).nlargest(1)\n        if largest_pvalue[0]<(0.0001):\n            return result\n            break\n        else:\n            col_list=col_list.drop(largest_pvalue.index)\n\nresult=back_feature_elem(X,card.Class,col)\n","e6271741":"result.summary()\n","3a94d052":"params = np.exp(result.params)\nconf = np.exp(result.conf_int())\nconf['OR'] = params\npvalue=round(result.pvalues,3)\nconf['pvalue']=pvalue\nconf.columns = ['CI 95%(2.5%)', 'CI 95%(97.5%)', 'Odds Ratio','pvalue']\nprint ((conf))","4c4875a1":"\nnew_features=card[['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V20','V21', 'V22', 'V23', 'V25', 'V26', 'V27','Class']]\nx=new_features.iloc[:,:-1]\ny=new_features.iloc[:,-1]\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.2,stratify=y,random_state=42)","2f123532":"from sklearn.linear_model import LogisticRegression\nlogreg=LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred=logreg.predict(x_test)","ca62eaf5":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","1fa6b2fc":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred)\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\");","2b505d8f":"TN=cm[0,0]\nTP=cm[1,1]\nFN=cm[1,0]\nFP=cm[0,1]\nsensitivity=TP\/float(TP+FN)\nspecificity=TN\/float(TN+FP)","de5a9af9":"print('The acuuracy of the model = TP+TN\/(TP+TN+FP+FN) =       ',(TP+TN)\/float(TP+TN+FP+FN),'\\n',\n\n'The Missclassification = 1-Accuracy =                  ',1-((TP+TN)\/float(TP+TN+FP+FN)),'\\n',\n\n'Sensitivity or True Positive Rate = TP\/(TP+FN) =       ',TP\/float(TP+FN),'\\n',\n\n'Specificity or True Negative Rate = TN\/(TN+FP) =       ',TN\/float(TN+FP),'\\n',\n\n'Positive Predictive value = TP\/(TP+FP) =               ',TP\/float(TP+FP),'\\n',\n\n'Negative predictive Value = TN\/(TN+FN) =               ',TN\/float(TN+FN),'\\n',\n\n'Positive Likelihood Ratio = Sensitivity\/(1-Specificity) = ',sensitivity\/(1-specificity),'\\n',\n\n'Negative likelihood Ratio = (1-Sensitivity)\/Specificity = ',(1-sensitivity)\/specificity)","03f46d66":"y_pred_prob=logreg.predict_proba(x_test)[:,:]\ny_pred_prob_df=pd.DataFrame(data=y_pred_prob, columns=['Prob of Not Fraud (0)','Prob of Fraud (1)'])\ny_pred_prob_df.head()","07705b20":"from sklearn.preprocessing import binarize\nfor i in range(0,11):\n    cm2=0\n    y_pred_prob_yes=logreg.predict_proba(x_test)\n    y_pred2=binarize(y_pred_prob_yes,i\/10)[:,1]\n    cm2=confusion_matrix(y_test,y_pred2)\n    print ('With',i\/10,'threshold the Confusion Matrix is ','\\n',cm2,'\\n',\n            'with',cm2[0,0]+cm2[1,1],'correct predictions and',cm2[1,0],'Type II errors( False Negatives)','\\n\\n',\n          'Sensitivity: ',cm2[1,1]\/(float(cm2[1,1]+cm2[1,0])),'Specificity: ',cm2[0,0]\/(float(cm2[0,0]+cm2[0,1])),'\\n\\n\\n')","300aab61":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_yes[:,1])\nplt.plot(fpr,tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve for Fraud classifier')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\nplt.grid(True)","c154e7d0":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_test,y_pred_prob_yes[:,1])","f51c590c":"# Logistic Regression","5e995884":"# Model Evaluation","f908c4dd":"### Note:\n- There are no missing \/ Null values either in columns or rows","a12f42ef":"## Feature Selection: Backward elemination (P-value approach)","f59f0446":"## Duplicate Check","8705a4ae":"Since the model is predicting Fraud too many type II errors is not advisable. A False Negative ( ignoring the probability of Fraud when there actualy is one) is more dangerous than a False Positive in this case. Hence inorder to increase the sensitivity, threshold can be lowered.","219c0204":"# EXPLORATORY DATA ANALYSIS","956ddede":"# Model Evaluation - Statistics","11c309de":"The confusion matrix shows 56642+54 = 56698 correct predictions and 19+41=50 incorrect ones.\n\nTrue Positives: 59\n\nTrue Negatives: 56641\n\nFalse Positives: 9 (Type I error)\n\nFalse Negatives: 41 ( Type II error)","4f156419":"## ROC Curve","5b5c6d3d":"# Confusion matrix","3230593c":"### Note\n- The heatmap clearly shows which all variable are multicollinear in nature, and which variable have high collinearity with the target variable.\n- We will refer this map back-and-forth while building the linear model so as to validate different correlated values along with p-value, for identifying the correct variable to select\/eliminate from the model.","74343a40":"# DATA QUALITY CHECK","9b2e0401":"### Lower the threshold","25698f66":"# Correlation Matrix","aee84e57":"## Check for NULL\/MISSING values","d5180fe2":"# Conclusion ","64525f1f":"Finding :\nDataset has 284807 rows and 31 columns.","2d15881a":"# Interpreting the results: Odds Ratio, Confidence Intervals and Pvalues","8a5e756a":"## Predicted probabilities of 0 (No Fraud) and 1 ( Fraud) for the test data with a default classification threshold of 0.5\n","924aac63":"### Note:\n\nThe results above show some of the attributes with P value higher than the preferred alpha(5%) and thereby showing low statistically significant relationship with the probability of heart disease. Backward elemination approach is used here to remove those attributes with highest Pvalue one at a time follwed by running the regression repeatedly until all attributes have P Values less than 0.05.","39eafd04":"* Accuracy of the model is 1","07bceed6":"- All attributes selected after the elimination process show Pvalues lower than 5% and thereby suggesting significant role in the fraud Prediction.\n\n- The Area under the ROC curve is 95.71 which is good \n\n- Overall model could be improved with more data.\n","38b0c99b":"### Note\n\n- There are 283253 records with no fraud status and 473 records with fraud status.","c950cb7c":"## Model accuracy","5f8f87cb":"- From the above statistics it is clear that the model is highly specific than sensitive. The negative values are predicted more accurately than the positives.","7fae6c55":"Logistic regression equation\n\nP=e^(\u03b20+\u03b21X1)\/1+e^(\u03b20+\u03b21X1)\n\n","b71f6458":"### Note:\n- Duplicate are found in the records","baaa7410":"A common way to visualize the trade-offs of different thresholds is by using an ROC curve, a plot of the true positive rate (# true positives\/ total # positives) versus the false positive rate (# false positives \/ total # negatives) for all possible choices of thresholds. A model with good classification accuracy should have significantly more true positives than false positives at all thresholds.\n\nThe optimum position for roc curve is towards the top left corner where the specificity and sensitivity are at optimum levels\n\nArea Under The Curve (AUC)\nThe area under the ROC curve quantifies model classification accuracy; the higher the area, the greater the disparity between true and false positives, and the stronger the model in classifying members of the training dataset. An area of 0.5 corresponds to a model that performs no better than random classification and a good classifier stays as far away from that as possible. An area of 1 is ideal. The closer the AUC to 1 the better"}}