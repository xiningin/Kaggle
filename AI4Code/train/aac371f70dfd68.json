{"cell_type":{"2b039a03":"code","dd7d59df":"code","47db56da":"code","62fc3ee5":"code","48459db2":"code","bdc7528e":"code","ecb9e52b":"code","9edb7681":"code","4454dfb3":"code","818a1f19":"code","aa33cbb8":"code","18a1c7b4":"code","20577b5a":"code","6bc91014":"code","4b394dd2":"code","89570123":"code","4c84f1cc":"code","cd07d652":"code","002072f3":"code","4bcab29d":"code","6d7d17a0":"code","ca4f5f63":"code","af3fef19":"code","61b5eac2":"code","8c193486":"code","c9943914":"code","a9663414":"code","c0095576":"code","faa44597":"code","0ff9e763":"markdown","339776fd":"markdown","4b51aa66":"markdown","524612d0":"markdown","02be4475":"markdown","f58d2faa":"markdown"},"source":{"2b039a03":"#Below are packages used in project\n\nimport numpy as np # used for numpy data structures viz. 1D and 2D arrays and math functions\nimport pandas as pd # To extract data from cav file and store in table like sttructure ( dataframe)\nimport datetime \nimport matplotlib.pyplot as plt #get 2D plots\nimport matplotlib #get 2D plots\nimport seaborn as sns #get 2D plots\nfrom sklearn.metrics import confusion_matrix #get confusion matrrix viz. actual vs predicted output for model evaluation\nimport math  #math functions\nimport xgboost as xgb #to implement XGBoost algorithm\nnp.random.seed(2019) #Seed is set so that results (random numbers) are repeated even after running code several times\nfrom scipy.stats import skew #statistical analysis\nfrom scipy import stats #statistical analysis\n\nimport statsmodels #statistical analysis\nfrom sklearn.metrics import accuracy_score #for model Evaluation\nfrom IPython.display import FileLink #For creating download link to submission csv file\n\n#to get plots below the code blocks\n%matplotlib inline \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","dd7d59df":"#Read input csv files using pandas functions\ntrain=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nsubmission=pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")\n\n#We will combine test and train for applying data tranformations \ntrain['train'] = 1 \ntest['train'] = 0\ndata = train.append(test,sort=False ,ignore_index=True)","47db56da":"#lets view few records in data\ndata.head()\n\n#In Data Preprocessing, we will consider each column and transform to make them usable for model application.\n#1. Age has lot many missing values, so need to impute Age\n#2. From name we can get title for each passenger and use it to impute missing values in Age field\n#3. Ticket and cabin column will be preprocessed and converted into categorical column\n#4. Fare and Embarked column have 2 and 1 missing value respectively, we will impute them using most repetitive values","62fc3ee5":"#lets analyse input data\ndata.describe()\n\n#It can be seen that we have lot of nulls in Age and survived.\n#Survived is expected to have so many nulls as we combined test and train.","48459db2":"data.Name.head()","bdc7528e":"#Age is very important variable and we need to impute missing values with best estimates.\n#Data is Name variable can be used to extract Titles and then we can impute Age missing values with mean of those groups\n#lets see how we will do this.\ndata['Title'] = data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\npd.crosstab(data['Title'], data['Sex'])\ndata = data.drop('Name',axis=1)\n\n#let's replace a few titles -> \"other\" and fix a few titles\ndata['Title'] = np.where((data.Title=='Capt') |(data.Title=='Dr') | (data.Title=='Countess') | (data.Title=='Don') | (data.Title=='Dona')\n                        | (data.Title=='Jonkheer') | (data.Title=='Lady') | (data.Title=='Sir') | (data.Title=='Major') | (data.Title=='Rev') | (data.Title=='Col'),'Other',data.Title)\ndata['Title'] = data['Title'].replace('Ms','Miss')\ndata['Title'] = data['Title'].replace('Mlle','Miss')\ndata['Title'] = data['Title'].replace('Mme','Mrs')","ecb9e52b":"#Lets observe how age is correlated with Title\nsns.boxplot(data = data, x = \"Title\", y = \"Age\")","9edb7681":"data.groupby('Title').Age.mean()","4454dfb3":"#We will impute Age missing values as below\ndata['Age'] = np.where((data.Age.isnull()) & (data.Title=='Master'),5,\n                        np.where((data.Age.isnull()) & (data.Title=='Miss'),22,\n                                 np.where((data.Age.isnull()) & (data.Title=='Mr'),32,\n                                          np.where((data.Age.isnull()) & (data.Title=='Mrs'),37,\n                                                  np.where((data.Age.isnull()) & (data.Title=='Other'),45,data.Age))))) \n\n#Converting Age to ","818a1f19":"#Ticket column is quite random but we can use information from it and check how it describes our dependent varaible i.e survided\nimport string\nTypeOfTicket = []\nfor i in range(len(data.Ticket)):\n    ticket = data.Ticket.iloc[i]\n    for c in string.punctuation:\n                ticket = ticket.replace(c,\"\")\n                splited_ticket = ticket.split(\" \")   \n    if len(splited_ticket) == 1:\n                TypeOfTicket.append('NO')\n    else: \n                TypeOfTicket.append(splited_ticket[0])\n            \ndata['TypeOfTicket'] = TypeOfTicket\n\ndata.TypeOfTicket.value_counts()\ndata['TypeOfTicket'] = np.where((data.TypeOfTicket!='NO') & (data.TypeOfTicket!='PC') & (data.TypeOfTicket!='CA') & \n                                (data.TypeOfTicket!='A5') & (data.TypeOfTicket!='SOTONOQ'),'other',data.TypeOfTicket)\ndata = data.drop('Ticket',axis=1)","aa33cbb8":"#Lets observe how age is correlated with Title\nsns.lineplot(data = data, x = \"TypeOfTicket\", y = \"Survived\")\n\n#It can be seen that typeOfTicket is certainly explaining Survived.\n#passengers holding certain type of tickets had better chance of survival","18a1c7b4":"data.Embarked.value_counts()\n# imputing value of Embarked with 'S' which has most number of occurences\ndata.Embarked=data.Embarked.fillna(\"S\")","20577b5a":"data[\"Cabin\"].head()","6bc91014":"# Replace the Cabin number by first letter and with 'M' if Null \ndata[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'M' for i in data['Cabin'] ])","4b394dd2":"sns.countplot(data[\"Cabin\"],order=['A','B','C','D','E','F','G','T','M'])\n#it cn be seen that most passengers have unkown cabin number which may suggest that \n#these passengers were not alloted any cabins","89570123":"#lets see Survival probablity vs Cabin \ng = sns.factorplot(y=\"Survived\",x=\"Cabin\",data=data,kind=\"bar\",order=['A','B','C','D','E','F','G','T','M'])\ng = g.set_ylabels(\"Survival Probability\")","4c84f1cc":"#Fill Fare NA with 0 \ndata.Fare=data.Fare.fillna(0)","cd07d652":"#check which columns are having nulls\ndata.isnull().sum()","002072f3":"#Convert all categorical variables to their dummies for model application\ndata = pd.get_dummies(data)","4bcab29d":"data.columns","6d7d17a0":"#split data into train and test\nfrom sklearn.model_selection import train_test_split\ntrainX, testX, trainY, testY = train_test_split(data[data.Survived.isnull()==False].drop('Survived',axis=1),data.Survived[data.Survived.isnull()==False],test_size=0.30, random_state=2019)","ca4f5f63":"#Model_eval will store the results of accuracy scroe of each model\nmodel_eval = pd.DataFrame({'Model': [],'Accuracy Score': []})","af3fef19":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(trainX, trainY)\ny_pred = model.predict(testX)\nfrom sklearn.metrics import accuracy_score\nres = pd.DataFrame({\"Model\":['LogisticRegression'],\n                    \"Accuracy Score\": [accuracy_score(y_pred,testY)]})\nmodel_eval = model_eval.append(res)\npd.crosstab(testY, y_pred, rownames=['Actual'], colnames=['Predicted'])","61b5eac2":"from sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(max_depth=4)\nmodel.fit(trainX, trainY)\ny_pred = model.predict(testX)\nfrom sklearn.metrics import accuracy_score\nres = pd.DataFrame({\"Model\":['DecisionTreeClassifier'],\n                    \"Accuracy Score\": [accuracy_score(y_pred,testY)]})\nmodel_eval = model_eval.append(res)\npd.crosstab(testY, y_pred, rownames=['Actual'], colnames=['Predicted'])","8c193486":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators=2500, max_depth=4)\nmodel.fit(trainX, trainY)\ny_pred = model.predict(testX)\nfrom sklearn.metrics import accuracy_score\nres = pd.DataFrame({\"Model\":['RandomForestClassifier'],\n                    \"Accuracy Score\": [accuracy_score(y_pred,testY)]})\nmodel_eval = model_eval.append(res)\npd.crosstab(testY, y_pred, rownames=['Actual'], colnames=['Predicted'])","c9943914":"from sklearn.svm import SVC\nmodel = SVC()\nmodel.fit(trainX, trainY)\ny_pred = model.predict(testX)\nfrom sklearn.metrics import accuracy_score\nres = pd.DataFrame({\"Model\":['SVC'],\n                    \"Accuracy Score\": [accuracy_score(y_pred,testY)]})\nmodel_eval = model_eval.append(res)\npd.crosstab(testY, y_pred, rownames=['Actual'], colnames=['Predicted'])","a9663414":"from xgboost.sklearn import XGBClassifier\nmodel = XGBClassifier(learning_rate=0.0001,n_estimators=2500,\n                                max_depth=4, min_child_weight=0,\n                                gamma=0, subsample=0.7,\n                                colsample_bytree=0.7,\n                                scale_pos_weight=1, seed=21,\n                                reg_alpha=0.00006)\nmodel.fit(trainX, trainY)\ny_pred = model.predict(testX)\nfrom sklearn.metrics import accuracy_score\nres = pd.DataFrame({\"Model\":['XGBClassifier'],\n                    \"Accuracy Score\": [accuracy_score(y_pred,testY)]})\nmodel_eval= model_eval.append(res)\npd.crosstab(testY, y_pred, rownames=['Actual'], colnames=['Predicted'])","c0095576":"#lets see how the models performed overall\nmodel_eval","faa44597":"#It can be seen that XGboost fared really good and we can use it to submit our results\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.model_selection import train_test_split\ntrainX = data[data.Survived.isnull()==False].drop(['Survived','train'],axis=1)\ntrainY = data.Survived[data.Survived.isnull()==False]\ntestX = data[data.Survived.isnull()==True].drop(['Survived','train'],axis=1)\nmodel = XGBClassifier(learning_rate=0.0001,n_estimators=2500,\n                                max_depth=4, min_child_weight=0,\n                                gamma=0, subsample=0.7,\n                                colsample_bytree=0.7,\n                                scale_pos_weight=1, seed=21,\n                                reg_alpha=0.00006)\nmodel.fit(trainX, trainY)\ntest = data[data.train==0]\ntest['Survived'] = model.predict(testX).astype(int)\ntest = test.reset_index()\ntest[['PassengerId','Survived']].to_csv(\"submissionXGB.csv\",index=False)\nFileLink(r'submissionXGB.csv')","0ff9e763":"Hello Friends! I am excited to present my perspective of Titanic problem. \n\nTargeted Audience\n1. Who wants to start with Kaggle and reach Top 5% in the competition\n2. Implement various classification algorithms inlcuing most trending one i.e XGBoost\n3. Understand Titanic use case and solve similar classification problem.\n\nLet me first summarize what I have done in this project.\n\n1. Data Preprocessing (Includes Exploratory data Analysis and Feature Engineering )\n2. Implement various classification algorithms\n3. Model Evaluation\n\nlastly Very important one. :P  Download Submission csv\nKindly UPVOTE if you find it helful. Thank You:)\n\nLets START","339776fd":"**1. Data Preprocessing (Includes Exploratory data Analysis and Feature Engineering )**","4b51aa66":"**3. Model Evaluation****","524612d0":"**2. Implement various classification algorithms","02be4475":"We will apply below classification algorithms in the order and compare their accuracy score\n1. Logistic Regression \n2. Decision Tree\n3. Random Forest\n4. SVC\n5. XGBoost","f58d2faa":"It can be seen that passengers not having cabins i.e M have low survival probality compared to others"}}