{"cell_type":{"63995951":"code","449e187a":"code","aa322f9f":"code","5c1e0a68":"code","f91bea3a":"code","37d0fd8a":"code","c7eed328":"code","6441e129":"code","851c70b5":"code","85f86759":"code","197b059f":"code","31097f5a":"code","29c78657":"code","71acabbd":"code","bba23ec4":"code","66c6ad50":"code","b3a57ab5":"code","15a102a7":"code","9e1ff0f5":"code","c7344ebf":"code","37d15c92":"code","8b211ccc":"code","b5a4fa11":"code","8f9ef0e8":"code","7372eecf":"code","91fd3ebe":"code","0ad2fd9c":"code","f63c0b29":"code","23074b40":"code","d6e5faf8":"code","a005a2e6":"code","5aeb8f59":"code","8fd2d228":"markdown","c0444006":"markdown","4ee5fef1":"markdown","fa26a814":"markdown","5c55cf22":"markdown","dda87e4b":"markdown","b69f9ffc":"markdown","c0752c5a":"markdown","34c3e2dc":"markdown","ea465f4d":"markdown","50981c24":"markdown","b6fa0878":"markdown","95453b2b":"markdown","54798c7a":"markdown","97501203":"markdown","0ceed87a":"markdown","1f07c335":"markdown","21f8c6cd":"markdown","b45a8ff7":"markdown","1adea6a5":"markdown","5db767db":"markdown","ad3fa045":"markdown","72f375d4":"markdown","893ab4f3":"markdown","73ffb13e":"markdown","659ca24e":"markdown","7807d5f7":"markdown","464f7073":"markdown","81c59d33":"markdown"},"source":{"63995951":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold","449e187a":"import nltk\nnltk.download('stopwords')","aa322f9f":"from nltk.stem import SnowballStemmer","5c1e0a68":"SnowballStemmer.languages","f91bea3a":"stemmer = SnowballStemmer('english')","37d0fd8a":"stemmer.stem('playing')","c7eed328":"stemmer.stem('plays')","6441e129":"stemmer.stem('played')","851c70b5":"stemmer_ignore_stopwords = SnowballStemmer('english', ignore_stopwords=True)","85f86759":"stemmer.stem('having')","197b059f":"stemmer_ignore_stopwords.stem('having')","31097f5a":"nltk.download('wordnet')","29c78657":"from nltk.stem import WordNetLemmatizer","71acabbd":"lemmatizer = WordNetLemmatizer()","bba23ec4":"lemmatizer.lemmatize('feet')","66c6ad50":"df = pd.read_csv('..\/input\/toxic-comments\/train.csv')","b3a57ab5":"df.head(2)","15a102a7":"print(df['comment_text'].values[0])","9e1ff0f5":"df['comment_text'].values[0]","c7344ebf":"import re\nfrom nltk.corpus import stopwords\nfrom tqdm import tqdm\n\n# creating a corpus with all the comments\ncorpus = []\ndf = df[:100]\nfor i in tqdm(range(len(df))):\n    comment = re.sub('[^a-zA-Z]', ' ', df['comment_text'][i])\n    comment = comment.lower()\n    comment = comment.split()\n    stemmer = SnowballStemmer('english')\n    lemmatizer = WordNetLemmatizer()\n    all_stopwords = stopwords.words('english')\n    comment = [stemmer.stem(word) for word in comment if not word in set(all_stopwords)]\n    comment = [lemmatizer.lemmatize(word) for word in comment]\n    comment = ' '.join(comment)\n    corpus.append(comment)","37d15c92":"corpus[0]","8b211ccc":"df['comment_text'].values[0]","b5a4fa11":"len(set(np.hstack([sentence.split() for sentence in df['comment_text'].values])))","8f9ef0e8":"len(set(np.hstack([sentence.split() for sentence in corpus])))","7372eecf":"comment = 'The food is not good'\ncomment = re.sub('[^a-zA-Z]', ' ', comment)\ncomment = comment.lower()\ncomment = comment.split()\nstemmer = SnowballStemmer('english')\nlemmatizer = WordNetLemmatizer()\nall_stopwords = stopwords.words('english')\ncomment = [stemmer.stem(word) for word in comment if not word in set(all_stopwords)]\ncomment = [lemmatizer.lemmatize(word) for word in comment]\ncomment = ' '.join(comment)\ncomment","91fd3ebe":"df = pd.read_csv('..\/input\/toxic-comments\/train.csv')","0ad2fd9c":"## Washing machine\ncorpus=[]\nfor i in tqdm(range(len(df))):\n    comment = re.sub('[^a-zA-Z]', ' ', df['comment_text'][i])\n    comment = comment.lower()\n    comment = comment.split()\n    stemmer = SnowballStemmer('english')\n    lemmatizer = WordNetLemmatizer()\n    all_stopwords = stopwords.words('english')\n    comment = [stemmer.stem(word) for word in comment if not word in set(all_stopwords)]\n    comment = [lemmatizer.lemmatize(word) for word in comment]\n    comment = ' '.join(comment)\n    corpus.append(comment)","f63c0b29":"df['cleaned_comment'] = corpus","23074b40":"df.head()","d6e5faf8":"df.to_csv('cleaned_train.csv', index=False)","a005a2e6":"def washing_machine(df):\n    corpus=[]\n    for i in tqdm(range(len(df))):\n        comment = re.sub('[^a-zA-Z]', ' ', df['comment_text'][i])\n        comment = comment.lower()\n        comment = comment.split()\n        stemmer = SnowballStemmer('english')\n        lemmatizer = WordNetLemmatizer()\n        all_stopwords = stopwords.words('english')\n        comment = [stemmer.stem(word) for word in comment if not word in set(all_stopwords)]\n        comment = [lemmatizer.lemmatize(word) for word in comment]\n        comment = ' '.join(comment)\n        corpus.append(comment)\n    df['cleaned_comment'] = corpus\n    return df","5aeb8f59":"# cleaned_df = washing_machine(df)","8fd2d228":"#### Let's download the 'wordnet' corpora","c0444006":"# Import SnowballStemmer","4ee5fef1":"### Let's see a bad case of removing stopwords","fa26a814":"## Wow Almost Half!!!!\n\nThat will be enough to answer the question.\n\nThis reduces the number of unique words and hence reduces the dimensionality of the problem.\n\nBut **Not Always**. <br>\nSome problems require the use of stopwords. For example excluding not from a sentence changes the sentiment of that sentence.","5c55cf22":"#### Now lets use the stemmer that ignore stopwords","dda87e4b":"# Lets import NLTK","b69f9ffc":"#### But considering the array, we can see there are some escape sequences like '\\n'. \n\n#### Also, we won't be needing any digits neither will be needing those stopwords.","c0752c5a":"# Lets clean out data and save it as a Dataset for others to use :)","34c3e2dc":"# Let's See the action of a stemmer\n\nWe will be using SnwoballStemmer for Stemming from the NLTK library.\n\nYou can also use other stememr like PorterStemmer, etc.","ea465f4d":"### For stopwords!!!\n\nStopwords are the most frequent words in a sentence.\n\nThose include 'have', 'is', 'are' etc.\n\nBut sometimes, stemming the stopwords drive the meaning of a sentence in a completely different direction. Depending on the problem statement we can either ignore stemming the stopwords or stem them. But according to this problem statement, since the severity of a toxic comment depends on vulgar keywords, we might not be using stopwords at all :)\n\nBut still let us see how a stemmer ignore stopwords. It's pretty simple :)","50981c24":"The output is stemmed perfectly","b6fa0878":"# What is Lemmatization?\n\nLemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.\n\nFor example, runs, running, ran are all forms of the word run, therefore run is the lemma of all these words. Because lemmatization returns an actual word of the language, it is used where it is necessary to get valid words.\n\n\nWe will be using **WordNetLemmatizer** for Lemmatizing","95453b2b":"### Now lets clean the data by removing unnecessary symbols and stopwords","54798c7a":"![stemming](https:\/\/user-images.githubusercontent.com\/74188336\/140915632-b20d6c02-86c6-41bb-a2fb-6f8abc8d19bd.png)","97501203":"# But why do we use Stemming and Lemmatizing and remove stopwords?\n\n#### Well to answer that, let us see the number of unique words before and after the process","0ceed87a":"### Great!! It performed perfectly","1f07c335":"I have taken only the first 100 samples of the dataset","21f8c6cd":"### function for generalization","b45a8ff7":"#### Stemming with the generalized stemmer","1adea6a5":"# Example of stemming","5db767db":"Great our problem is now solved !!! :D","ad3fa045":"# What is stemming?\n\nStemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language.\n\nStem (root) is the part of the word to which you add inflectional (changing\/deriving) affixes such as (-ed,-ize, -s,-de,mis). So stemming a word or sentence may result in words that are not actual words. Stems are created by removing the suffixes or prefixes used with a word.\n\n","72f375d4":"### We can see how much the sentence has changed","893ab4f3":"![nlp](https:\/\/user-images.githubusercontent.com\/74188336\/140924469-f7292676-d422-4871-98d3-59ce395e6e07.jpeg)","73ffb13e":"#### lets create a csv of cleaned comments","659ca24e":"### Available Languages.\n\nStememr are not available for all languages. \nSo, Let us check out the languages which are available.","7807d5f7":"### Complete opposite! :(\nThe original sentence was of negative sentiment : **'The food is not good'**\n\nThe output : **'food good'**","464f7073":"### Let us check out how good it is.\n\nWe will take the words in the image show above.\n","81c59d33":"### Printing shows that there are not unnecessary symbols"}}