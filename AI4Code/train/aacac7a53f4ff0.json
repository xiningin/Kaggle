{"cell_type":{"67c8673a":"code","dde8efe7":"code","96591d8b":"code","42831209":"code","cb230c7d":"code","e218bae9":"code","c4ffe205":"code","26796ab4":"code","3abd368e":"code","c78b12c6":"code","7c9aaec3":"code","5e6f853c":"code","ec822d87":"code","11a8d54c":"code","09ae5aae":"code","c015bdef":"code","c97435b6":"code","99041bcd":"code","1aeec323":"code","0ad33bb1":"code","8a84d7fe":"markdown","4857b08c":"markdown","3400f515":"markdown","e217eb51":"markdown","e472fbd0":"markdown","10c7e5ee":"markdown","fc47a1a2":"markdown","37e41ad3":"markdown","66097517":"markdown","f28f6e79":"markdown","8eca258a":"markdown","1be94385":"markdown","21147f7d":"markdown","58a345ed":"markdown"},"source":{"67c8673a":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, log_loss, roc_auc_score","dde8efe7":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv\")","96591d8b":"train.head()","42831209":"test.head()","cb230c7d":"train.pop(\"id\")\ntest_ids = test.pop(\"id\")","e218bae9":"train_targets = train.pop(\"claim\")","c4ffe205":"train_targets.head()","26796ab4":"sns.countplot(train_targets)","3abd368e":"train_desc = train.describe()\ntrain_desc.transpose()","c78b12c6":"test_desc = test.describe()\ntest_desc.transpose()","7c9aaec3":"desc_delta = train_desc - test_desc\ndesc_delta.transpose()","5e6f853c":"for data in [train, test]:\n    data['n_nans'] = data.isnull().sum(axis=1)\n    data['std'] = data.std(axis=1)\n    data['var'] = data.var(axis=1)\n    data['max'] = data.max(axis=1)\n    data['min'] = data.min(axis=1)","ec822d87":"train.head()","11a8d54c":"test.head()","09ae5aae":"columns = list(train.columns)\nfor item in train.columns:\n    if abs(train[item].max()) \/ (abs(train[item].min()) + 10e-10) > 20:\n        train[item] = np.sign(train[item]) * np.log2(np.abs(train[item]) + 1)\n        test[item] = np.sign(test[item]) * np.log2(np.abs(test[item]) + 1)\n    train_mean = train[item].mean()\n    train_std = train[item].std()\n    train[item] = (train[item] - train_mean) \/ train_std\n    test[item] = (test[item] - train_mean) \/ train_std\n    # Missing Value Imputation seems to have a bad effect to final results\n    #train[item].replace(np.NAN, train[item].mean(), inplace=True)\n    #test[item].replace(np.NAN, test[item].mean(), inplace=True)\n","c015bdef":"def evaluate(valid_targets, probs, name):\n    y_pred = np.array(probs > 0.5, dtype=int)\n    acc = accuracy_score(valid_targets, y_pred)\n    loss = log_loss(valid_targets, y_pred)\n    auc = roc_auc_score(valid_targets, probs)\n    print(\"Accuracy score: %.2f\"%(acc))\n    print(\"Log loss: %.2f\"%(loss))\n    print(\"AUC score:\", auc)\n    print(\"Classification report:\")\n    print(classification_report(valid_targets, y_pred))\n    return {\n        \"name\": name, \n        \"accuracy_score\": acc, \n        \"log_loss\": loss, \n        \"auc\": auc\n    }","c97435b6":"from sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(n_splits=10, shuffle=True)\ncats = []\nindex = 1\nfor train_indices, valid_indices in kfold.split(train, train_targets):\n    print(\"Training with Fold %d\"%(index))\n    train_features = train.iloc[train_indices]\n    train_labels = train_targets.iloc[train_indices]\n    valid_features = train.iloc[valid_indices]\n    valid_labels = train_targets.iloc[valid_indices]\n    cat_params = {\n        'iterations': 15000, \n        'loss_function': 'Logloss', \n        'depth': 8, \n        'task_type' : 'GPU',\n        'use_best_model': True,\n        'eval_metric': 'AUC',\n        'early_stopping_rounds': 1000,\n        'learning_rate': 0.03,\n        'border_count': 32,\n        'l2_leaf_reg': 3,\n        \"verbose\": 1000\n    }\n    cat = CatBoostClassifier(\n        **cat_params\n    )\n    cat.fit(train_features, train_labels, eval_set=[(valid_features, valid_labels)])\n    cats.append(cat)\n    probs = cat.predict_proba(valid_features)[:, 1]\n    result_cat = evaluate(valid_labels, probs, \"catboost\")\n    print(result_cat)\n    index += 1","99041bcd":"probs_list = []\nfor cat in cats:\n    probs = cat.predict_proba(test)[:, 1]\n    probs_list.append(probs)\nprobs_array = np.array(probs_list)\nmean_probs = probs_array.mean(axis=0)","1aeec323":"mean_probs.shape","0ad33bb1":"submission = pd.DataFrame({\"id\": list(test_ids), \"claim\": mean_probs})\nsubmission.to_csv(\"submission.csv\", index=False)","8a84d7fe":"### Get Train data Targets","4857b08c":"## EDA & Data Preprocessing","3400f515":"## Import datasets","e217eb51":"# CatBoost Tabular Playground Prediction(Sep 2021)\n## Table of Contents\n- Import Packages\n- Import Datasets\n- EDA & Preprocessing\n- Model Development & Evaluation\n- Submission","e472fbd0":"### Drop ID columns","10c7e5ee":"## Import Packages","fc47a1a2":"## Submisssion","37e41ad3":"Let's see statistic info between train set and test set and compare their differences, which are very small execpt for their total numbers.  Some of the parameters has a great difference, so I will apply log transformation to reduce their skewness.","66097517":"## Feature Scaling","f28f6e79":"## Using CatBoost","8eca258a":"## Model Development & Evaluation\n","1be94385":"### Evaluation Method","21147f7d":"### Add extra features","58a345ed":"The labels looks balanced. "}}