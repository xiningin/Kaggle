{"cell_type":{"8560bd32":"code","1ff6c9d6":"code","5ed68252":"code","243f02c1":"code","62294ba9":"code","fb8d5a93":"code","93463cf1":"code","0273552d":"code","96cfb5bb":"code","0c392ff9":"code","6e84984c":"code","61d55ff6":"code","b8f27524":"code","75093307":"code","af139d16":"code","ce4e22c3":"code","93a9adb3":"code","a5aa9c22":"code","7c29ebf1":"code","dd464606":"code","5ce12616":"code","019da9b1":"code","4499d423":"code","bfbc52ac":"code","c1d88767":"code","6da802aa":"code","44091a8d":"code","1723fd89":"code","61cb9b0c":"code","d70e3207":"code","84165e0d":"code","7ba0c978":"code","cf254b11":"code","58b5e724":"markdown","bc4bbaca":"markdown","8067b4c7":"markdown","64db4d41":"markdown","aa3a97b7":"markdown","3df1da89":"markdown","07ef8855":"markdown","b570d023":"markdown","70977b3a":"markdown","48cbf1eb":"markdown","2a14c512":"markdown","9f0c170f":"markdown","3d5db37e":"markdown","0677c1d9":"markdown","36a18077":"markdown","f4d337e1":"markdown","5f60f7d6":"markdown","b5bcd060":"markdown","7deac91c":"markdown","34ae9bff":"markdown","00cba94f":"markdown"},"source":{"8560bd32":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nfrom glob import glob\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.datasets import load_files \nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.models import Sequential\n\n\nfrom keras.utils import np_utils\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/flowers\/flowers\"))\n\n# Any results you write to the current directory are saved as output.","1ff6c9d6":"# Make a parent directory `data` and three sub directories `train`, `valid` and 'test'\n%rm -rf data # Remove if already present\n\n%mkdir -p data\/train\/daisy\n%mkdir -p data\/train\/tulip\n%mkdir -p data\/train\/sunflower\n%mkdir -p data\/train\/rose\n%mkdir -p data\/train\/dandelion\n\n%mkdir -p data\/valid\/daisy\n%mkdir -p data\/valid\/tulip\n%mkdir -p data\/valid\/sunflower\n%mkdir -p data\/valid\/rose\n%mkdir -p data\/valid\/dandelion\n\n%mkdir -p data\/test\/daisy\n%mkdir -p data\/test\/tulip\n%mkdir -p data\/test\/sunflower\n%mkdir -p data\/test\/rose\n%mkdir -p data\/test\/dandelion\n\n\n%ls data\/train\n%ls data\/valid\n%ls data\/test","5ed68252":"base_dir = \"..\/input\/flowers\/flowers\"\ncategories = os.listdir(base_dir)","243f02c1":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport random\nfrom shutil import copyfile\n\nplt.rcParams[\"figure.figsize\"] = (20,3)","62294ba9":"def train_valid_test(files):\n    \"\"\"This function splits the files in training, validation and testing sets with 60%, 20%\n    and 20% of data in each respectively\"\"\"\n    train_fles = files[:int(len(files)*0.6)]\n    valid_files = files[int(len(files)*0.6):int(len(files)*0.8)]\n    test_files = files[int(len(files)*0.8):]\n    return train_fles, valid_files, test_files","fb8d5a93":"def copy_files(files, src, dest):\n    \"\"\"This function copy files from src to dest\"\"\"\n    for file in files:\n        copyfile(\"{}\/{}\".format(src, file), \"{}\/{}\".format(dest, file))","93463cf1":"def plot_images(category, images):\n    \"\"\"This method plots five images from a category\"\"\"\n    for i in range(len(images)):\n        plt.subplot(1,5,i+1)\n        plt.title(category)\n        image = mpimg.imread(\"{}\/{}\/{}\".format(base_dir, category, images[i]))\n        plt.imshow(image)\n    plt.show()","0273552d":"total_images = []\nfor category in categories:\n    images = os.listdir(\"{}\/{}\".format(base_dir, category))\n    random.shuffle(images)\n    filtered_images = [image for image in images if image not in ['flickr.py', 'flickr.pyc', 'run_me.py']]\n    \n    total_images.append(len(filtered_images))\n    \n    \n    train_images, valid_images, test_images = train_valid_test(filtered_images)\n    \n    copy_files(train_images, \"{}\/{}\".format(base_dir, category), \".\/data\/train\/{}\".format(category))\n    copy_files(valid_images, \"{}\/{}\".format(base_dir, category), \".\/data\/valid\/{}\".format(category))\n    copy_files(test_images, \"{}\/{}\".format(base_dir, category), \".\/data\/test\/{}\".format(category))\n    plot_images(category, images[:5])\n    \n        ","96cfb5bb":"print(\"Total images: {}\".format(np.sum(total_images)))\nfor i in range(len(categories)):\n    print(\"{}: {}\".format(categories[i], total_images[i]))","0c392ff9":"y_pos = np.arange(len(categories))\nplt.bar(y_pos, total_images, width=0.2,color='b',align='center')\nplt.xticks(y_pos, categories)\nplt.ylabel(\"Image count\")\nplt.title(\"Image count in different categories\")\nplt.show()","6e84984c":"# define function to load train, valid and test datasets\ndef load_dataset(path):\n    data = load_files(path)\n    flower_files = np.array(data['filenames'])\n    print(data['target_names'])\n    flower_targets = np_utils.to_categorical(np.array(data['target']), 5)\n    return flower_files, flower_targets\n\n# load train, test, and validation datasets\ntrain_files, train_targets = load_dataset('data\/train')\nvalid_files, valid_targets = load_dataset('data\/valid')\ntest_files, test_targets = load_dataset('data\/test')\n\nprint('There are %d total flower categories.' % len(categories))\nprint('There are %s total flower images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\nprint('There are %d training flower images.' % len(train_files))\nprint('There are %d validation flower images.' % len(valid_files))\nprint('There are %d test flower images.' % len(test_files))\n","61d55ff6":"from keras.preprocessing import image                  \nfrom tqdm import tqdm","b8f27524":"def path_to_tensor(img_path):\n    # loads RGB image as PIL.Image.Image type\n    img = image.load_img(img_path, target_size=(224, 224))\n    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n    x = image.img_to_array(img)\n    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n    return np.expand_dims(x, axis=0)","75093307":"def paths_to_tensor(img_paths):\n    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n    return np.vstack(list_of_tensors)","af139d16":"from PIL import ImageFile                            \nImageFile.LOAD_TRUNCATED_IMAGES = True                 \n\n# pre-process the data for Keras\ntrain_tensors = paths_to_tensor(train_files).astype('float32')\/255\nvalid_tensors = paths_to_tensor(valid_files).astype('float32')\/255\ntest_tensors = paths_to_tensor(test_files).astype('float32')\/255","ce4e22c3":"simple_model = Sequential()\nprint(train_tensors.shape)\n\n### Define the architecture of the simple model.\nsimple_model.add(Conv2D(filters=16, kernel_size=2, strides=1, activation='relu', input_shape=(224,224,3)))\nsimple_model.add(GlobalAveragePooling2D())\nsimple_model.add(Dense(5, activation='softmax'))\nsimple_model.summary()","93a9adb3":"simple_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])","a5aa9c22":"# Create a `saved_models` directory for saving best model\n%mkdir -p saved_models","7c29ebf1":"from keras.callbacks import ModelCheckpoint  \n\n### number of epochs\nepochs = 50\n\ncheckpointer = ModelCheckpoint(filepath='saved_models\/weights.best.simple.hdf5', \n                               verbose=1, save_best_only=True)\n\nsimple_model.fit(train_tensors, train_targets, \n          validation_data=(valid_tensors, valid_targets),\n          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)","dd464606":"simple_model.load_weights('saved_models\/weights.best.simple.hdf5')","5ce12616":"# get index of predicted flower category for each image in test set\nflower_predictions = [np.argmax(simple_model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n\n# report test accuracy\ntest_accuracy = 100*np.sum(np.array(flower_predictions)==np.argmax(test_targets, axis=1))\/len(flower_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)","019da9b1":"model = Sequential()\nprint(train_tensors.shape)\n### Define architecture.\nmodel.add(Conv2D(filters=16, kernel_size=2, strides=1, activation='relu', input_shape=(224,224,3)))\nmodel.add(MaxPooling2D(pool_size=2, strides=2))\nmodel.add(Conv2D(filters=32, kernel_size=2, strides=1, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2, strides=2))\nmodel.add(Conv2D(filters=64, kernel_size=2, strides=1, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2, strides=2))\nmodel.add(GlobalAveragePooling2D())\nmodel.add(Dense(5, activation='softmax'))\nmodel.summary()","4499d423":"model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])","bfbc52ac":"from keras.callbacks import ModelCheckpoint  \n\n### number of epochs\nepochs = 50\n\ncheckpointer = ModelCheckpoint(filepath='saved_models\/weights.best.from_scratch.hdf5', \n                               verbose=1, save_best_only=True)\n\nmodel.fit(train_tensors, train_targets, \n          validation_data=(valid_tensors, valid_targets),\n          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)","c1d88767":"model.load_weights('saved_models\/weights.best.from_scratch.hdf5')","6da802aa":"# get index of predicted flower category for each image in test set\nflower_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n\n# report test accuracy\ntest_accuracy = 100*np.sum(np.array(flower_predictions)==np.argmax(test_targets, axis=1))\/len(flower_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)","44091a8d":"from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\nfrom keras.models import Model\n\ninception_resnet = InceptionResNetV2(weights=\"imagenet\",include_top=False, input_shape=(224,224,3))\nfor layer in inception_resnet.layers[:5]:\n    layer.trainable = False\n\noutput_model = inception_resnet.output\noutput_model = Flatten()(output_model)\noutput_model = Dense(200, activation='relu')(output_model)\noutput_model = Dropout(0.5)(output_model)\noutput_model = Dense(200, activation='relu')(output_model)\noutput_model = Dense(5, activation='softmax')(output_model)\n\nmodel = Model(inputs=inception_resnet.input, outputs=output_model)\nmodel.summary()","1723fd89":"model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])","61cb9b0c":"from keras.callbacks import ModelCheckpoint  \n\n### number of epochs\nepochs = 50\n\ncheckpointer = ModelCheckpoint(filepath='saved_models\/weights.best.inception_resnetv2.hdf5', \n                               verbose=1, save_best_only=True)\n\nmodel.fit(train_tensors, train_targets, \n          validation_data=(valid_tensors, valid_targets),\n          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)","d70e3207":"### load best weights\nmodel.load_weights('saved_models\/weights.best.inception_resnetv2.hdf5')","84165e0d":"# get index of predicted flower category for each image in test set \nflower_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n\n# report test accuracy\ntest_accuracy = 100*np.sum(np.array(flower_predictions)==np.argmax(test_targets, axis=1))\/len(flower_predictions)\nprint('Test accuracy: %.4f%%' % test_accuracy)","7ba0c978":"for i in range(5):\n    predicted = np.argmax(model.predict(np.expand_dims(test_tensors[i], axis=0)))\n    actual = np.argmax(test_targets[i])\n    print(\"Predicted: {}, Actual: {}, Name: {}\".format(predicted, actual, test_files[i].split(\"\/\")[2]))\n    image = mpimg.imread(test_files[i])\n    plt.imshow(image)\n    plt.show()","cf254b11":"%rm -rf data","58b5e724":"Delete created directory and files. It's necessary to have only few files otherwise Kaggle won't allow to commit a kernel.","bc4bbaca":"### Reorganize the data\nAll the flowers are stored in a directory flower and separated based on the category in sub-directory.\nWe can reorganize the data in such a way that we can easily use `load_files` from `sklearn`.","8067b4c7":"#### Load the best weight of the model","64db4d41":"---\n<a id=\"step3\"><\/a>\n## Step 3: Develop a CNN architecture from scratch","aa3a97b7":"Find all the categories of the flowers","3df1da89":"The flowers are present in dataset as follows: \n\n```\nflowers\n\u2502\n\u2514\u2500\u2500\u2500Daisy\n\u2502   \n\u2514\u2500\u2500\u2500Dandelion\n|\n\u2514\u2500\u2500\u2500Rose\n\u2502   \n\u2514\u2500\u2500\u2500Sunflower\n|\n\u2514\u2500\u2500\u2500Tulip\n```\n\nWe can create dataset for training, validation and testing to easily use `load_files` from `sklearn`\n\n```\ndata\n\u2502\n\u2514\u2500\u2500\u2500train\n|    \u2502\n|    \u2514\u2500\u2500\u2500Daisy\n|    \u2502   \n|    \u2514\u2500\u2500\u2500Dandelion\n|    |\n|    \u2514\u2500\u2500\u2500Rose\n|    \u2502   \n|    \u2514\u2500\u2500\u2500Sunflower\n|    |\n|    \u2514\u2500\u2500\u2500Tulip\n\u2514\u2500\u2500\u2500valid\n|    \u2502\n|    \u2514\u2500\u2500\u2500Daisy\n|    \u2502   \n|    \u2514\u2500\u2500\u2500Dandelion\n|    |\n|    \u2514\u2500\u2500\u2500Rose\n|    \u2502   \n|    \u2514\u2500\u2500\u2500Sunflower\n|    |\n|    \u2514\u2500\u2500\u2500Tulip\n\u2514\u2500\u2500\u2500test\n     \u2502\n     \u2514\u2500\u2500\u2500Daisy\n     \u2502   \n     \u2514\u2500\u2500\u2500Dandelion\n     |\n     \u2514\u2500\u2500\u2500Rose\n     \u2502   \n     \u2514\u2500\u2500\u2500Sunflower\n     |\n     \u2514\u2500\u2500\u2500Tulip\n```\n","07ef8855":"The `ptahs_to_tensor` applies `path_to_tensor` to all images and returns a list of tensors.","b570d023":"### Making Predictions with the simple model","70977b3a":"### Benchmark model's performance\nThe accuracy obtained from the benchmark model is 41.57%.","48cbf1eb":"#### Get the accuracy of the model","2a14c512":"### Observations\n- There are 4323 total images with approximately similar distribution in each category.\n- The dataset does not seem  to be imbalanced.\n- Accuracy can be used as a metric for model evaulation.","9f0c170f":"### Create a 4D tensor\nThe `path_to_tensor` function below takes a color image as input and returns a 4D tensor suitable for supplying to Keras CNN. The function first loads the image and then resizes it 224x224 pixels. The image then, is converted to an array and resized to a 4D tensor. The returned tensor will always have a shape of `(1, 224, 224, 3)` as we are dealing with a single image only in this function.","3d5db37e":"<a id=\"step2\"><\/a>\n## Step 2: Develop a Benchmark model\nUse a simple CNN to create a benchmark model.","0677c1d9":"#### Get the accuracy on test set","36a18077":"<a id=\"step4\"><\/a>\n## Step 4: Develop a CNN using Transfer Learning","f4d337e1":"### Statistics of flowers","5f60f7d6":"#### Load best weight of the model","b5bcd060":"# Steps\nThis kernel is designed in following ways:\n\n**[Step 1](#step1)**: Data Preprocessing\n\n**[Step 2](#step2)**:  Develop a Benchmark model\n\n**[Step 3](#step3)**: Develop a CNN architecture from scratch\n\n**[Step 4](#step4)**: Develop a CNN using Transfer Learning\n","7deac91c":"### Data Transformation\n\nKeras' CNNs require a 4D tensor as input with the shape as `(nb_samples, rows, columns, channels)` where\n- `nb_samples`: total number of samples or images\n- `rows`: number of rows of each image\n- `columns`: number of columns of each image\n- `channels`: number of channels of each image\n","34ae9bff":"---\n<a id='step1'><\/a>\n## Step 1: Data Preprocessing\n\n### Import Libraries\nHere we import a set of useful libraries","00cba94f":"### Pre-process the Data\nRescale the images by dividing every pixel in every image by 255."}}