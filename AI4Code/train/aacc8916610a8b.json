{"cell_type":{"2dd29477":"code","55a35da3":"code","c562fed8":"code","79860996":"code","792a8d09":"code","dff30b46":"code","83fb0760":"code","c5d4088d":"code","056208e6":"code","ef312190":"code","02e1fc10":"code","62e70b79":"code","35967260":"code","a7e34cd0":"code","19a55cbf":"code","ef0439b6":"code","265b10e2":"code","ddf34f9a":"code","c337ec29":"code","642b22c5":"code","2d20724a":"code","d7fb5d82":"code","8bcf3118":"code","0076682a":"code","15519aa1":"markdown","abb29ea3":"markdown","af524fc0":"markdown","6f0a240c":"markdown","b700f37d":"markdown","5acc8443":"markdown","afae3b2f":"markdown","7fdaac22":"markdown","968889e4":"markdown","4c457d66":"markdown","eeb4bff2":"markdown"},"source":{"2dd29477":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew,skewtest\nfrom sklearn.impute import KNNImputer\nfrom collections import Counter\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","55a35da3":"data = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndata.head()","c562fed8":"data.info()","79860996":"data.describe().T","792a8d09":"data.columns\n# I checked that if features has space or not in their names. We dont want to search for mistakes later.","dff30b46":"data.tail()","83fb0760":"data[[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\"]] = data[[\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\"]].replace(0,np.nan)\nprint(data.isnull().sum())","c5d4088d":"p = data.hist(figsize = (20,20),bins=25)\nplt.show()","056208e6":"c = sns.FacetGrid(data,col=\"Outcome\",height=6)\nc.map(sns.distplot,\"Glucose\",bins=25)\nplt.show()","ef312190":"c = sns.FacetGrid(data,col=\"Outcome\",height=6)\nc.map(sns.distplot,\"BloodPressure\",bins=25)\nplt.show()","02e1fc10":"c = sns.FacetGrid(data,col=\"Outcome\",height=6)\nc.map(sns.distplot,\"SkinThickness\",bins=25)\nplt.show()","62e70b79":"c = sns.FacetGrid(data,col=\"Outcome\",height=6)\nc.map(sns.distplot,\"Insulin\",bins=25)\nplt.show()","35967260":"c = sns.FacetGrid(data,col=\"Outcome\",height=6)\nc.map(sns.distplot,\"BMI\",bins=25)\nplt.show()","a7e34cd0":"corr = data.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(12, 8))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, annot=True, mask=mask, cmap=cmap, ax=ax,)\nplt.show()","19a55cbf":"imputer = KNNImputer(n_neighbors=5)\ndata_filled = imputer.fit_transform(data)\ndata_filled_df = pd.DataFrame(data_filled, index=data.index,columns=data.columns)","ef0439b6":"data_filled_df.isnull().sum()\n# All missing values are filled.","265b10e2":"corr = data_filled_df.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nf, ax = plt.subplots(figsize=(12, 8))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, annot=True, mask=mask, cmap=cmap, ax=ax,)\nplt.show()","ddf34f9a":"from sklearn.model_selection import train_test_split,GridSearchCV, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier, RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost as xgb","c337ec29":"X = data_filled_df.drop(\"Outcome\",axis=1)\ny = data_filled_df[\"Outcome\"]","642b22c5":"scaler = StandardScaler()\nX = scaler.fit_transform(X)","2d20724a":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n\nknn = KNeighborsClassifier()\n\nknn.fit(X_train,y_train)\n\nprint(\"Train Accuracy: \",knn.score(X_train,y_train))\nprint(\"Test Accuracy: \",knn.score(X_test,y_test))","d7fb5d82":"random_state = 42\nclassifier = [SVC(random_state = random_state),\n             LogisticRegression(random_state = random_state),\n             KNeighborsClassifier(),\n             xgb.XGBClassifier(seed=123, objective=\"reg:logistic\")]\n\nsvc_params = {\"kernel\" : [\"rbf\"],\n              \"gamma\": [0.001, 0.01, 0.1, 1],\n              \"C\": [1,10,50,100,200,300,1000]}\n\nlogreg_params = {\"C\":np.logspace(-3,3,7),\n                 \"penalty\": [\"l1\",\"l2\"]}\n\nknn_params = {\"n_neighbors\": np.arange(1,20,1),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\",\"minkowski\"]}\n\nxgb_params = {\"n_estimators\": np.arange(1,15,1),\n              \"max_depth\" : np.arange(1,10,1),\n              \"num_boost_round\": np.arange(1,10,1)}\n\nclassifier_params = [svc_params,\n                    logreg_params,\n                    knn_params,\n                    xgb_params]","8bcf3118":"cv_result = []\nbest_estimators = []\nfor i in range(len(classifier)):\n    clf = GridSearchCV(classifier[i], param_grid=classifier_params[i], cv = 5, scoring = \"accuracy\", n_jobs = -1,verbose=1)\n    clf.fit(X_train,y_train)\n    cv_result.append(clf.best_score_)\n    best_estimators.append(clf.best_estimator_)\n    print(cv_result[i])","0076682a":"cv_results = pd.DataFrame({\"ML Models\":[\"SVM\",\n                                        \"LogisticRegression\",\n                                        \"KNeighborsClassifier\",\n                                        \"XGBClassifier\"],\n                           \"Cross Validation Means\":cv_result})\nprint(cv_results)\n\n\ng = sns.barplot(\"Cross Validation Means\", \"ML Models\", data = cv_results)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Cross Validation Scores\")","15519aa1":"* We can see that we have some missing values there and they all showing with 0 so we don't know how many. Let's check other things about our data and then we can replace our missing values, in this case 0's, with NaN's.","abb29ea3":"### Hyperparameter Tuning, Grid Search","af524fc0":"* Now, we replaced all our missing values using KNNImputer. After the change, our correlation values have slightly changed. \n* We have our missing values filled, now we can start to build our models.\n\n### Modeling","6f0a240c":"* After we scale our data, we'll do a quick and simple KNN fit and see our accuracy.","b700f37d":"* Now our results are not satisfying, like expected. We have 67% test accuracy. Let's tune our hyperparameters with Grid Search and see where it goes.","5acc8443":"* Yes, we have missing data but which columns? We can see here that our Glucose, BloodPressure, SkinThcikness, Insulin and BMI features all have minimum values 0. This can't be right? Now we also found which features have missing values.","afae3b2f":"* Well, it's seems like we have some skewness in our data distributions. \n* What is skewness?\n* Skewness is asymmetry in a statistical distribution, in which the curve appears distorted or skewed either to the left or to the right. Skewness can be quantified to define the extent to which a distribution differs from a normal distribution.\n* Now let's check our distributions to outcomes, and see if we have same distribution.","7fdaac22":"* We have somewhat identical distributions. \n* Now let's check our correlation map and fill our missing values.","968889e4":"* So we improved our accuracy 10% with hyperparameter tuning, and the best one is XGBClassifier with 78.5% accuracy. \n\n* And thanks for all readers, this is my first notebook so if you've found any mistakes please tell me so i can improve myself. ","4c457d66":"### Fill Missing Value\n* So like i said, we have a few ways to fill our values. Calculating mean or median of each missing column is a way, and mostly preferred way. But with that much skewness in data if we add more that wont be good for us.\n* Let's use a different approach and use KNNImputer, and see if we have improvement.","eeb4bff2":"* So Glucose doesnt have much values alose BloodPressure too. But SkinThickness has one forth of data missing and Insulin is almost fourty percent missing value too. Before we dive in to our models and so we need to fill these missing values.\n\n* What are the choices we have here?\n\n* We have few ways that filling missing values but before that let's check our data distributions to understand our data."}}