{"cell_type":{"cd76375a":"code","497ee448":"code","97060dc6":"code","7a097692":"code","57320bf3":"code","b5ed6f5c":"code","baf270c6":"code","6a25eb60":"code","3fcf81cf":"code","ef15bb73":"code","0c819549":"code","6ab1ce97":"code","f7c2b00b":"code","3df574f6":"code","ca866dd8":"code","0cceae6f":"code","71e918b1":"code","706c98da":"code","6fbdc8df":"code","8050a650":"code","49884a2e":"code","2b054268":"code","f1edb101":"code","237f5ec4":"code","58eff184":"code","ccefa176":"code","3b7d9c91":"code","f7a23c21":"code","733aea46":"code","5871d1c9":"code","7e637f7b":"code","455ad76a":"code","21a535bd":"code","5a004a7c":"code","2770d6fd":"code","3ccaff64":"code","464beb40":"code","682910c8":"code","433eeb9b":"code","7ef1dd76":"code","f5c00d16":"code","a0bdd690":"markdown","3d4d26e4":"markdown","36ca936d":"markdown","288be4a6":"markdown","47c78a16":"markdown","7323496b":"markdown","380b7b59":"markdown"},"source":{"cd76375a":"#'''Importing Data Manipulation Modules'''\nimport numpy as np                 # Linear Algebra\nimport pandas as pd                # Data Processing, CSV file I\/O (e.g. pd.read_csv)\n\n#'''Seaborn and Matplotlib Visualization'''\nimport matplotlib                  # 2D Plotting Library\nimport matplotlib.pyplot as plt\nimport seaborn as sns              # Python Data Visualization Library based on matplotlib\nplt.style.use('fivethirtyeight')\n%matplotlib inline\n\n#'''Plotly Visualizations'''\nimport plotly as plotly                # Interactive Graphing Library for Python\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly.offline as py\ninit_notebook_mode(connected=True)\nimport os\n%pylab inline\nimport warnings\nwarnings.filterwarnings('ignore')","497ee448":"df = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","97060dc6":"df.head()","7a097692":"df.dtypes","57320bf3":"df.describe()","b5ed6f5c":"def quality(dataframe):\n    dataframe.loc[(dataframe['quality'] >= 2) & (dataframe['quality'] <= 6.5), 'quality'] = 0\n    \n    dataframe.loc[(dataframe['quality'] > 6.5) & (dataframe['quality'] <= 8), 'quality'] = 1\n           \n    return dataframe\n\nquality(df)","baf270c6":"x = df.drop(['quality'],axis = 1)","6a25eb60":"y = df['quality']","3fcf81cf":"labels = (df.quality.unique())\ncolors = ['Crimson', 'DarkBlue']\n\ntrace = go.Histogram(x=df.quality,marker=dict(color=colors,line=dict(color='black', width=2)),opacity=0.75)\nlayout = go.Layout(\n    title='Quality distribution',\n    xaxis=dict(\n        title='Bad wine - Great wine'\n    ),\n    yaxis=dict(\n        title='Count'\n    ),\n    bargap=0.2,\n    bargroupgap=0.1, paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor=\"rgb(243, 243, 243)\")\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)","ef15bb73":"data_dia = y\ndata = x\ndata_n_2 = (data - data.mean()) \/ (data.std())              # standardization\n\n\ndata = pd.concat([y,data_n_2],axis=1)\ndata = pd.melt(data,id_vars=\"quality\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"quality\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)","0c819549":"data_dia = y\ndata = x\ndata_n_2 = (data - data.mean()) \/ (data.std())              # standardization\ndata = pd.concat([y,data_n_2],axis=1)\ndata = pd.melt(data,id_vars=\"quality\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\ntic = time.time()\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"quality\", data=data,palette=[\"black\", \"silver\"])\n\nplt.xticks(rotation=90)","6ab1ce97":"f,ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","f7c2b00b":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import KFold\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)\naccuracies = {}","3df574f6":"x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.25, random_state=42)","ca866dd8":"from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nx_train = sc_X.fit_transform(x_train)\nx_test = sc_X.transform(x_test)","0cceae6f":"clf_rf = RandomForestClassifier(random_state=43)      \nclr_rf = clf_rf.fit(x_train,y_train)","71e918b1":"ac = accuracy_score(y_test,clf_rf.predict(x_test))\n\nprint('Accuracy is: ',ac)\ncm = confusion_matrix(y_test,clf_rf.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")","706c98da":"from sklearn.feature_selection import RFECV\n\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nclf_rf_1 = RandomForestClassifier(random_state = 42) \nrfecv = RFECV(estimator=clf_rf_1, step=1, cv=k_fold,scoring='accuracy')   #10-fold cross-validation\nrfecv = rfecv.fit(x_train, y_train)\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', x.columns[rfecv.support_])","6fbdc8df":"x_1 = df[['volatile acidity','citric acid','total sulfur dioxide','density','sulphates','alcohol']]","8050a650":"x_train, x_test, y_train, y_test = train_test_split(x_1,y, test_size=0.25, random_state=42)","49884a2e":"from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nx_train = sc_X.fit_transform(x_train)\nx_test = sc_X.transform(x_test)","2b054268":"clf_rf_1 = RandomForestClassifier(random_state=43)      \nclr_rf_1 = clf_rf_1.fit(x_train,y_train)","f1edb101":"ac = accuracy_score(y_test,clf_rf_1.predict(x_test))\naccuracies['Random_Forest'] = ac\n\nprint('Accuracy is: ',ac, '\\n')\ncm = confusion_matrix(y_test,clf_rf_1.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")\n\nprint('RFC Reports\\n',classification_report(y_test, clf_rf_1.predict(x_test)))","237f5ec4":"import matplotlib.pyplot as plt\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score of number of selected features\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","58eff184":"from sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression() \nlogmodel.fit(x_train,y_train)\n\nac = accuracy_score(y_test,logmodel.predict(x_test))\naccuracies['Logistic regression'] = ac\n\nprint('Accuracy is: ',ac, '\\n')\ncm = confusion_matrix(y_test,logmodel.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")\n\nprint('Logistic regression Reports\\n',classification_report(y_test, logmodel.predict(x_test)))\n\n","ccefa176":"from sklearn import model_selection\nfrom sklearn.neighbors import KNeighborsClassifier","3b7d9c91":"#Neighbors\nneighbors = np.arange(0,25)\n\n#Create empty list that will hold cv scores\ncv_scores = []\n\n#Perform 10-fold cross validation on training set for odd values of k:\nfor k in neighbors:\n    k_value = k+1\n    knn = KNeighborsClassifier(n_neighbors = k_value, weights='uniform', p=2, metric='euclidean')\n    kfold = model_selection.KFold(n_splits=10, random_state=123)\n    scores = model_selection.cross_val_score(knn, x_train, y_train, cv=k_fold, scoring='accuracy')\n    cv_scores.append(scores.mean()*100)\n    print(\"k=%d %0.2f (+\/- %0.2f)\" % (k_value, scores.mean()*100, scores.std()*100))\n\noptimal_k = neighbors[cv_scores.index(max(cv_scores))]\nprint (\"The optimal number of neighbors is %d with %0.1f%%\" % (optimal_k, cv_scores[optimal_k]))\n\nplt.plot(neighbors, cv_scores)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Train Accuracy')\nplt.show()","f7a23c21":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=9)\nknn.fit(x_train, y_train)\n\nac = accuracy_score(y_test,knn.predict(x_test))\naccuracies['KNN'] = ac\n\n\nprint('Accuracy is: ',ac, '\\n')\ncm = confusion_matrix(y_test,knn.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")\n\nprint('KNN Reports\\n',classification_report(y_test, knn.predict(x_test)))","733aea46":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(criterion='gini') #criterion = entopy, gini\ndtree.fit(x_train, y_train)\n\nac = accuracy_score(y_test,dtree.predict(x_test))\nprint('Accuracy is: ',ac, '\\n')\ncm = confusion_matrix(y_test,dtree.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")\n\nprint('DecisionTree Reports\\n',classification_report(y_test, dtree.predict(x_test)))","5871d1c9":"from sklearn.tree import plot_tree","7e637f7b":"plt.figure(figsize=(20,15))\nplot_tree(dtree,\n         filled=True,\n         rounded=True,\n         feature_names=x_1.columns)","455ad76a":"path = dtree.cost_complexity_pruning_path(x_train, y_train)\nccp_alphas = path.ccp_alphas\nccp_alphas = ccp_alphas[:-1]\n\ndtrees = []\n\nfor ccp_alpha in ccp_alphas:\n    dtree = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    dtree.fit(x_train,y_train)\n    dtrees.append(dtree)","21a535bd":"train_scores = [dtree.score(x_train,y_train) for dtree in dtrees]\ntest_scores = [dtree.score(x_test, y_test) for dtree in dtrees]\n\nfig, ax = plt.subplots()\nax.set_xlabel('alpha')\nax.set_ylabel('accuracy')\nax.set_title('Accuracy vs alpha for training and testing sets')\nax.plot(ccp_alphas, train_scores, marker = 'o', label = 'train', drawstyle='steps-post')\nax.plot(ccp_alphas, test_scores, marker = 'o', label = 'test', drawstyle='steps-post')\nax.legend()\nplt.show()\n","5a004a7c":"# create an array to store the results of each fold during cross validation\nf,ax = plt.subplots(figsize=(18, 8))\nalpha_loop_values = []\n\nfor ccp_alpha in ccp_alphas:\n    dtree = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    scores =cross_val_score(dtree, x_train, y_train, cv=kfold, scoring='accuracy')\n    alpha_loop_values.append([ccp_alpha, np.mean(scores),np.std(scores)])\n    \nalpha_results = pd.DataFrame(alpha_loop_values,\n                             columns=['alpha','mean_accuracy','std'])\nalpha_results.plot(x='alpha',\n                   y='mean_accuracy',\n                   yerr='std',\n                   marker='o',\n                   linestyle='--',\n                   ax=ax)","2770d6fd":"alpha_results[(alpha_results['alpha'] > 0.003)\n              & \n              (alpha_results['alpha'] < 0.004)]","3ccaff64":"dtree1 = DecisionTreeClassifier(random_state=42,\n                                ccp_alpha=0.003672)\ndtree1 = dtree1.fit(x_train, y_train)\n\nac = accuracy_score(y_test,dtree1.predict(x_test))\naccuracies['decisiontree'] = ac\n\n\nprint('Accuracy is: ',ac,'\\n')\ncm = confusion_matrix(y_test,dtree1.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")\n\nprint('Decision Tree Reports\\n',classification_report(y_test, dtree1.predict(x_test)))","464beb40":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {'C':[0.5,1,10,100],\n     'gamma': ['scale',1,0.1,0.01,0.001,0.0001],\n     'kernel': ['rbf']},\n]\n\noptimal_params = GridSearchCV(\n        SVC(),\n        param_grid,\n        cv = k_fold,\n        scoring='accuracy',\n        verbose = 0\n    )\noptimal_params.fit(x_train, y_train)\nprint(optimal_params.best_params_)","682910c8":"svc1= SVC(random_state = 42, C = 10, gamma = 1, kernel = 'rbf')\nsvc1.fit(x_train, y_train)\n\nac = accuracy_score(y_test,svc1.predict(x_test))\naccuracies['SVM'] = ac\n\n\nprint('Accuracy is: ',ac, '\\n')\ncm = confusion_matrix(y_test,svc1.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")\n\nprint('SVM report\\n',classification_report(y_test, svc1.predict(x_test)))","433eeb9b":"from sklearn.naive_bayes import GaussianNB\ngaussiannb= GaussianNB()\ngaussiannb.fit(x_train, y_train)\n\nac = accuracy_score(y_test,gaussiannb.predict(x_test))\naccuracies['GaussianNB'] = ac\n\n\nprint('Accuracy is: ',ac,'\\n')\ncm = confusion_matrix(y_test,gaussiannb.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")\n\nprint('GaussianNB report\\n',classification_report(y_test, gaussiannb.predict(x_test)))","7ef1dd76":"colors = [\"purple\", \"green\", \"orange\", \"magenta\",\"#CFC60E\",\"#0FBBAE\"]\n\nplt.rcParams['figure.figsize'] = (18,8)\n\nx=list(accuracies.keys())\ny=list(accuracies.values())\n\nbars = plt.bar(x, height=y, width=.4, color = colors)\n\nxlocs, xlabs = plt.xticks()\n\nxlocs=[i for i in x]\nxlabs=[i for i in x]\n\nplt.xlabel('Algorithms', size = 20)\nplt.ylabel('Accuracy %', size = 20)\nplt.xticks(xlocs, xlabs, size = 15)\n\nfor bar in bars:\n    yval = bar.get_height()\n    plt.text(bar.get_x() + .1, yval + .005, yval, size = 15)\n\nplt.show()","f5c00d16":"fig, ax_arr = plt.subplots(nrows = 2, ncols = 3, figsize = (20,15))\nfrom sklearn import metrics\n\n#RandomForest\nprobs = clf_rf_1.predict_proba(x_test)\npreds = probs[:,1]\nfprrfc, tprrfc, thresholdrfc = metrics.roc_curve(y_test, preds)\nroc_aucrfc = metrics.auc(fprrfc, tprrfc)\n\nax_arr[0,0].plot(fprrfc, tprrfc, 'b', label = 'AUC = %0.2f' % roc_aucrfc)\nax_arr[0,0].plot([0, 1], [0, 1],'r--')\nax_arr[0,0].set_title('ROC Random Forest ',fontsize=20)\nax_arr[0,0].set_ylabel('True Positive Rate',fontsize=20)\nax_arr[0,0].set_xlabel('False Positive Rate',fontsize=15)\nax_arr[0,0].legend(loc = 'lower right', prop={'size': 16})\n\n#LOGMODEL\nprobs = logmodel.predict_proba(x_test)\npreds = probs[:,1]\nfprlog, tprlog, thresholdlog = metrics.roc_curve(y_test, preds)\nroc_auclog = metrics.auc(fprlog, tprlog)\n\nax_arr[0,1].plot(fprlog, tprlog, 'b', label = 'AUC = %0.2f' % roc_auclog)\nax_arr[0,1].plot([0, 1], [0, 1],'r--')\nax_arr[0,1].set_title('ROC Logistic ',fontsize=20)\nax_arr[0,1].set_ylabel('True Positive Rate',fontsize=20)\nax_arr[0,1].set_xlabel('False Positive Rate',fontsize=15)\nax_arr[0,1].legend(loc = 'lower right', prop={'size': 16})\n\n#KNN\nprobs = knn.predict_proba(x_test)\npreds = probs[:,1]\nfprknn, tprknn, thresholdknn = metrics.roc_curve(y_test, preds)\nroc_aucknn = metrics.auc(fprknn, tprknn)\n\nax_arr[0,2].plot(fprknn, tprknn, 'b', label = 'AUC = %0.2f' % roc_aucknn)\nax_arr[0,2].plot([0, 1], [0, 1],'r--')\nax_arr[0,2].set_title('ROC KNN ',fontsize=20)\nax_arr[0,2].set_ylabel('True Positive Rate',fontsize=20)\nax_arr[0,2].set_xlabel('False Positive Rate',fontsize=15)\nax_arr[0,2].legend(loc = 'lower right', prop={'size': 16})\n\n#DECISION TREE\nprobs = dtree1.predict_proba(x_test)\npreds = probs[:,1]\nfprdtree, tprdtree, thresholddtree = metrics.roc_curve(y_test, preds)\nroc_aucdtree = metrics.auc(fprdtree, tprdtree)\n\nax_arr[1,0].plot(fprdtree, tprdtree, 'b', label = 'AUC = %0.2f' % roc_aucdtree)\nax_arr[1,0].plot([0, 1], [0, 1],'r--')\nax_arr[1,0].set_title('ROC Decision Tree ',fontsize=20)\nax_arr[1,0].set_ylabel('True Positive Rate',fontsize=20)\nax_arr[1,0].set_xlabel('False Positive Rate',fontsize=15)\nax_arr[1,0].legend(loc = 'lower right', prop={'size': 16})\n\n\n#Gaussiannb\n\nprobs = gaussiannb.predict_proba(x_test)\npreds = probs[:,1]\nfprgau, tprgau, thresholdgau = metrics.roc_curve(y_test, preds)\nroc_aucgau = metrics.auc(fprgau, tprgau)\n\nax_arr[1,1].plot(fprgau, tprgau, 'b', label = 'AUC = %0.2f' % roc_aucgau)\nax_arr[1,1].plot([0, 1], [0, 1],'r--')\nax_arr[1,1].set_title('ROC Gaussian ',fontsize=20)\nax_arr[1,1].set_ylabel('True Positive Rate',fontsize=20)\nax_arr[1,1].set_xlabel('False Positive Rate',fontsize=15)\nax_arr[1,1].legend(loc = 'lower right', prop={'size': 16})\n\n#All plots\nax_arr[1,2].plot(fprrfc, tprrfc, 'b', label = 'rfc', color='black')\nax_arr[1,2].plot(fprlog, tprlog, 'b', label = 'Logistic', color='blue')\nax_arr[1,2].plot(fprknn, tprknn, 'b', label = 'Knn', color='brown')\nax_arr[1,2].plot(fprdtree, tprdtree, 'b', label = 'Decision Tree', color='green')\nax_arr[1,2].plot(fprgau, tprgau, 'b', label = 'Gaussiannb', color='grey')\nax_arr[1,2].set_title('Receiver Operating Comparison ',fontsize=20)\nax_arr[1,2].set_ylabel('True Positive Rate',fontsize=20)\nax_arr[1,2].set_xlabel('False Positive Rate',fontsize=15)\nax_arr[1,2].legend(loc = 'lower right', prop={'size': 16})","a0bdd690":"# Random Forest","3d4d26e4":"# GaussianNB","36ca936d":"# SVM","288be4a6":"# KNN","47c78a16":"# Decision Tree","7323496b":"# Thanks for watching, upvote if you like:)","380b7b59":"# LogisticRegression"}}