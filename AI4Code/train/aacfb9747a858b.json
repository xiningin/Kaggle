{"cell_type":{"9abd31d2":"code","9a45ebeb":"code","85f82da2":"code","1aaa89c7":"code","f0fc3288":"code","0692d857":"code","6ada1bbc":"code","99d9e31b":"markdown","38b735f9":"markdown","bc15a82c":"markdown","d9e96090":"markdown"},"source":{"9abd31d2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.cluster import AgglomerativeClustering \nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\nimport scipy.cluster.hierarchy as shc","9a45ebeb":"raw_df = pd.read_csv('..\/input\/ccdata\/CC GENERAL.csv')\nraw_df = raw_df.drop('CUST_ID', axis = 1) \nraw_df.fillna(method ='ffill', inplace = True) \nraw_df.head(2)","85f82da2":"# Standardize data\nscaler = StandardScaler() \nscaled_df = scaler.fit_transform(raw_df) \n  \n# Normalizing the Data \nnormalized_df = normalize(scaled_df) \n  \n# Converting the numpy array into a pandas DataFrame \nnormalized_df = pd.DataFrame(normalized_df) \n  \n# Reducing the dimensions of the data \npca = PCA(n_components = 2) \nX_principal = pca.fit_transform(normalized_df) \nX_principal = pd.DataFrame(X_principal) \nX_principal.columns = ['P1', 'P2'] \n  \nX_principal.head(2)","1aaa89c7":"plt.figure(figsize =(6, 6)) \nplt.title('Visualising the data') \nDendrogram = shc.dendrogram((shc.linkage(X_principal, method ='ward'))) ","f0fc3288":"silhouette_scores = [] \n\nfor n_cluster in range(2, 8):\n    silhouette_scores.append( \n        silhouette_score(X_principal, AgglomerativeClustering(n_clusters = n_cluster).fit_predict(X_principal))) \n    \n# Plotting a bar graph to compare the results \nk = [2, 3, 4, 5, 6,7] \nplt.bar(k, silhouette_scores) \nplt.xlabel('Number of clusters', fontsize = 10) \nplt.ylabel('Silhouette Score', fontsize = 10) \nplt.show() ","0692d857":"agg = AgglomerativeClustering(n_clusters=3)\nagg.fit(X_principal)","6ada1bbc":"# Visualizing the clustering \nplt.scatter(X_principal['P1'], X_principal['P2'],  \n           c = AgglomerativeClustering(n_clusters = 3).fit_predict(X_principal), cmap =plt.cm.winter) \nplt.show() ","99d9e31b":"Building and Visualizing clustering model for k = 3","38b735f9":"<b>Dendograms <\/b> are used to divide a given cluster into many different clusters.","bc15a82c":"Determine the optimal number of clusters using <b> [Silhouette Score](https:\/\/www.kaggle.com\/vipulgandhi\/kmeans-detailed-explanation)<\/b>.","d9e96090":"# Hierarchical clustering\n\n<b>Hierarchical clustering <\/b> (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two types:\n\n- <b>Agglomerative <\/b>: This is a \"bottom-up\" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\n- <b>Divisive <\/b>: This is a \"top-down\" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.\n\nIn general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering are usually presented in a dendrogram.\n\n## Agglomerative Clustering\n\nInitially each data point is considered as an individual cluster. At each iteration, the similar clusters merge with other clusters until 1\/ K clusters are formed.\n\nThe main advantage is that we don\u2019t need to specify the number of clusters, this comes with a price: performance $O(n^3)$. In sklearn\u2019s implementation, we can specify the number of clusters to assist the algorithm\u2019s performance.\n\n### Algorithm\n- Compute the proximity matrix\n- Let each data point be a cluster\n- Repeat: Merge two closest clusters and update the proximity matrix until 1\/ K cluster remains\n\nEx. - We have six data points {A,B,C,D,E,F}.\n\n- In the initial step, we consider all the six data points as individual clusters as shown in the image below.\n\n![Screen%20Shot%202019-08-30%20at%2018.06.56.png](attachment:Screen%20Shot%202019-08-30%20at%2018.06.56.png)\n\n- The first step is to determine which elements to merge in a cluster. Usually, we want to take the two closest elements, according to the chosen distance.We construct a distance matrix at this stage, where the number in the i-th row j-th column is the distance between the i-th and j-th elements. Then, as clustering progresses, rows and columns are merged as the clusters are merged and the distances updated.\n\n#### Computation of proximity\/distance matrix\n\nThe choice of an appropriate metric will influence the shape of the clusters, as some elements may be close to one another according to one distance and farther away according to another. For example, in a 2-dimensional space, the distance between the point (1,0) and the origin (0,0) is always 1 according to the usual norms, but the distance between the point (1,1) and the origin (0,0) can be 2 under Manhattan distance, $\\sqrt2$ under Euclidean distance, or 1 under maximum distance.\n\nSome commonly used metrics for hierarchical clustering are:\n\n![Screen%20Shot%202019-08-30%20at%2018.03.52.png](attachment:Screen%20Shot%202019-08-30%20at%2018.03.52.png)\n\nFor text or other non-numeric data, metrics such as the Hamming distance or Levenshtein distance are often used.\n\n- Similar clusters are merged together and formed as a single cluster. Let\u2019s consider B,C, and D,E are similar clusters that are merged in step two. Now, we\u2019re left with four clusters which are A, BC, DE, F. To calculate the proximity between two clusters, we need to define the distance between them. Usually the distance is one of the following:\n     - The maximum distance between elements of each cluster (also called  <b> complete-linkage clustering <\/b>)\n     - The minimum distance between elements of each cluster (also called <b> single-linkage clustering <\/b>)\n     - The mean distance between elements of each cluster (also called <b> average linkage clustering <\/b>)\n     - The sum of all intra-cluster variance.\n- Again calculate the proximity of new clusters and merge the similar clusters to form new clusters A, BC, DEF.\n- Calculate the proximity of the new clusters. The clusters DEF and BC are similar and merged together to form a new cluster. We\u2019re now left with two clusters A, BCDEF.\n- Finally, all the clusters are merged together and form a single cluster.\n\n\n\nThe Hierarchical clustering Technique can be visualized using a Dendrogram.\nA Dendrogram is a tree-like diagram that records the sequences of merges or splits.\n\n![Screen%20Shot%202019-08-30%20at%2018.07.26.png](attachment:Screen%20Shot%202019-08-30%20at%2018.07.26.png)\n\n### Example\n"}}