{"cell_type":{"0c6ad65f":"code","c8a3956a":"code","e3268fea":"code","e4d93ed1":"code","1fd59f66":"code","79c513a9":"code","e37b6fba":"code","db5dd80c":"code","4a59fa07":"code","65c22550":"code","b869ec4d":"code","613fec02":"code","2f60b098":"code","0a727872":"code","67858a37":"code","06f1e59e":"code","50ebe995":"code","0c000f4b":"code","3b2f3198":"code","b4befd98":"code","df17c7e9":"code","66f46b9c":"code","ff0400fb":"code","a43a1605":"code","2f6e6ad3":"code","769061ce":"code","af7f776d":"markdown","6fc5b707":"markdown","fe01138d":"markdown","5b9b7a85":"markdown","3cb3fe9e":"markdown","2ca50eaa":"markdown","6e321bd9":"markdown","03453855":"markdown","da197949":"markdown","14fe38af":"markdown","7a1dc7fb":"markdown","7afbd5d3":"markdown","c88633c5":"markdown","ee1cadcf":"markdown","909dc05f":"markdown","5cb42a0d":"markdown","91e74fd6":"markdown","ed9b597d":"markdown","9ca4d694":"markdown","43cabba8":"markdown","4537fa3e":"markdown","7e3b361c":"markdown","187e7446":"markdown","908d0f18":"markdown"},"source":{"0c6ad65f":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import  StandardScaler\ndf= pd.read_csv('..\/input\/indian-food-101\/indian_food.csv')\ndf.head()","c8a3956a":"df.shape","e3268fea":"ingre = set()\nfor i in df['ingredients']:\n    ingre.update(str(i).lower().split(\",\"))\n    \nprint(\"Total unique ingredients in dataset\",len(ingre),sep=\": \")","e4d93ed1":"def count_ingredient(column):\n    return float(len(column.split(\",\")))\ndf['ingredient_count'] = df['ingredients'].apply(count_ingredient)\ndf.head()","1fd59f66":"df.drop('ingredients', axis=1, inplace=True)\ndf.head()","79c513a9":"df.describe()","e37b6fba":"df.describe(include='object')","db5dd80c":"df.replace(-1, np.NaN, inplace = True)\ndf.replace(\"-1\", np.NaN, inplace = True)\ndf.nunique()","4a59fa07":"data = df.dropna()","65c22550":"data.shape","b869ec4d":"data.describe()","613fec02":"data.dtypes","2f60b098":"num = (data.dtypes == 'float64')\nnumerical = list(num[num].index)\nprint(\"Numerical variables are:\")\nprint(numerical)","0a727872":"num_data = data[numerical]\nnum_data.head()","67858a37":"scaler = MinMaxScaler()\nnum_data_values = num_data.values\nnum_data_scaled = scaler.fit_transform(num_data_values)\nnormalized_df = pd.DataFrame(num_data_scaled)\nnormalized_df.head()","06f1e59e":"normalized_df.describe()","50ebe995":"std_scaler = StandardScaler()\nnum_data_values = num_data.values\nnum_data_std= std_scaler.fit_transform(num_data_values)\nstandardized_df = pd.DataFrame(num_data_std)\nstandardized_df.head()","0c000f4b":"standardized_df.describe()","3b2f3198":"data.describe(include='object')","b4befd98":"cat = (data.dtypes == 'object')\nobjects = list(cat[cat].index)\nprint(\"Categorical variables are:\")\nprint(objects)","df17c7e9":"cat_data = data[objects]\ncat_data.head()","66f46b9c":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ncat_data['course'] = label_encoder.fit_transform(cat_data['course'])\ncat_data['state'] = label_encoder.fit_transform(cat_data['state'])\ncat_data.head()","ff0400fb":"f_pro = {'sweet':1,'spicy':2, 'bitter':3, 'sour':4}\ncat_data = cat_data.replace({'flavor_profile':f_pro})\ncat_data.head()","a43a1605":"ndiet={'vegetarian':0,'non-vegitarian':1}\ncat_data= cat_data.replace({'diet':ndiet})\ncat_data.head()","2f6e6ad3":"cat_data = pd.get_dummies(cat_data,columns=['region'],prefix = ['cat'])\ncat_data.head()","769061ce":"cat_data.describe()","af7f776d":"Here, we are going to drop the NaN data and the new dataset is named as 'data'","6fc5b707":"**Conclusion:**\n\nThus, we preprocessed the data using normalization and standardization methods and convert the categorical data to numeric data for ease of building models.","fe01138d":"Here, I am storing numerical data into \"num_data\".","5b9b7a85":"![![image.png](attachment:image.png)](https:\/\/techondiary.files.wordpress.com\/2019\/02\/capture.png?w=660)","3cb3fe9e":"**Objective:**\n\nTo perform data preprocessing techniques.\n\n**Secondary Objectives:**\n\n* To study Feature Scaling with Normalization and Standardization.\n* To study conversion of catgorical data into numeric using various methods.\n* To handle missing or irrelevant data.","2ca50eaa":"* **Label Encoder** \n \nNow, I will encode two features that is 'course' and 'state' using label_encoder from scikitlearn.","6e321bd9":"* **Normalization using MinMaxScaler() from sklearn:**\n\n","03453855":"Here, as we can see the min prep_time and cook_time is -1 which is not a realistic value. So, we will replace all the -1 in numeric as well as in categorical to NaN value. Then check for the unique values.","da197949":"* **Data Standardisation using StandardScaler()**","14fe38af":"**Steps or techniques to perform data processing:**\n\n* Import Libraries to be used.\n* Import the dataset.\n* Remove the missing or irrelevant data.\n* Encode categorical data to numeric type.\n* Perform feature scaling techniques (Normalization and Standardization).\n* Then you can build your model with preprocessed data.\n\n\n","7a1dc7fb":"The shape which was (255,9) is now reduced to (180,9). ","7afbd5d3":"* **get_dummies()**\n\nIn this, we will use get_dummies() method. Here,cat_data is the dataframe and we use 'region' to specify which columns we want to be in dummy code. categorical variables in region are recoded into a set of separate binary variables (dummy variables). The next question is \u201cwhat is a dummy variable?\u201d. Typically, a dummy variable (or column) is one which has a value of one (1) when a categorical event occurs and zero (0) when it doesn\u2019t occur Furthermore, this re-coding is called \u201cdummy coding\u201d and involves the creation of a table called contrast matrix.","c88633c5":"Here, I have seperated categorical data from the base dataset.","ee1cadcf":"Data preprocessing in Machine Learning is a crucial step that enhances the quality of data to promote the extraction of meaningful insights from the data. Data preprocessing in Machine Learning refers to the technique of preparing (cleaning and organizing) the raw data to make it suitable for a building and training Machine Learning models.","909dc05f":"* **Replace() method**\n\nNow,using replace() method replacing the flavor_profile and diet with numerical values.","5cb42a0d":"Here, we can see it has 255 columns and 9 rows. Most of the data is of categorical type. In the next step, we will count the number of unique ingredients used. ","91e74fd6":"The first step is to load the libraries and the dataset. Here, I am going to use indian_food dataset. Let's see the details...","ed9b597d":"Next, we will perform feature scaling on this numeric data.\n\n**What is Feature scaling?**\n\nFeature Scaling is a technique to represent in the data in a fixed range. In our example prep_time and cook time variables have different range. That is prep_time should be less than cook_time. ","9ca4d694":"Next, I will be seperating the numeric data and categorical data.So that we could easily perform different operations.","43cabba8":"As we can see there are 425 unique ingredients used. Now, we will count the number of ingredient used for a particular dish. ","4537fa3e":"Now, we will encode the categorical data into numeric data using various methods","7e3b361c":"Now we have 10 columns. But, we will drop the ingrident column as we already have the count of ingredients.","187e7446":"**Why do we need data preprocessing in machine learning?**\n\nGenerally, real-world data is incomplete, inconsistent, inaccurate and often lacks specific attribute\/values. This is where data preprocessing is used \u2013 it helps to clean, format, and organize the raw data, thereby making it ready for building Machine Learning models. Preprocessing removes outliers and scales the features to an equivalent range. ","908d0f18":"# Data Preprocessing"}}