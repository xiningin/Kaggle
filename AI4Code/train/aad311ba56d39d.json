{"cell_type":{"7539cf54":"code","64f1012f":"code","cf45f4d2":"code","f5526c94":"code","f7ad26ea":"code","133fd86a":"code","c9d7a20d":"code","0166bfa0":"code","43c74893":"code","63b35a06":"code","3552e2b3":"code","8205ae88":"code","a12ff570":"code","6a903eb8":"code","ddceb00b":"code","ab14ea1e":"code","b1933abf":"code","7afcbd7d":"code","055003f0":"code","172f608b":"code","7bc7d4a1":"code","90e83fcf":"markdown","732f18cb":"markdown","ff48a385":"markdown","884102a3":"markdown","393f551f":"markdown","d4d4b04b":"markdown","f68e7966":"markdown"},"source":{"7539cf54":"!pip install tqdm\nimport os, datetime, importlib\n\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\n\nfrom typing import Union\nimport sklearn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import timedelta\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Conv1D, Dense, Activation, Dropout, Lambda, Multiply, Add, Concatenate\nfrom tensorflow.keras.optimizers import Adam\n\ntimestring = lambda : datetime.datetime.now().strftime(\"%H_%M_%S\")\n\nimport m5_helpers\nimport m5_models\nimport nbeats\n\n%matplotlib inline","64f1012f":"path = \"..\/input\/m5-forecasting-accuracy\"\n\ntrain_df = pd.read_csv(f'{path}\/sales_train_validation.csv')\ncalendar = pd.read_csv(f'{path}\/calendar.csv')\nprices = pd.read_csv(f'{path}\/sell_prices.csv')\n\ntrain_fold_df = train_df.iloc[:, :-28]\nvalid_fold_df = train_df.iloc[:, -28:]\n\ne = m5_helpers.WRMSSEEvaluator(train_fold_df, valid_fold_df, calendar, prices)","cf45f4d2":"# Use this is you want to fit on the full 42840 sequences\ndf_train = e.train_series\ndf_valid = e.valid_series\ndf = pd.concat([df_train, df_valid], axis=1)\ndf = df.reset_index().rename(columns={'index':'id'})","f5526c94":"length = len(df.columns)-1\nstart_day = datetime.datetime.strptime('2011-01-29', '%Y-%m-%d')\ndate_list = [(start_day + datetime.timedelta(days=x)).date() for x in range(length)]\ndf.columns = [df.columns[0]]+date_list\ndata_start_date = df.columns[1]\ndata_end_date = df.columns[-1]\n\ndate_to_index = pd.Series(index=pd.Index([pd.to_datetime(c) for c in df.columns[1:]]), data=[i for i in range(len(df.columns[1:]))])\n\nprint('Data ranges from %s to %s' % (data_start_date, data_end_date))\ndf.head()","f7ad26ea":"def plot_random_series(df, n_series):\n    \n    sample = df.sample(n_series, random_state=np.random.randint(100))\n    page_labels = sample['id'].tolist()\n    series_samples = sample.loc[:,data_start_date:data_end_date]\n    \n    plt.figure(figsize=(10,6))\n    \n    for i in range(series_samples.shape[0]):\n        pd.Series(series_samples.iloc[i]).astype(np.float64).plot(linewidth=1.5)\n    \n    plt.title('Randomly Selected Time Series')\n    plt.legend(page_labels)\n    \nplot_random_series(df, 6)","133fd86a":"pred_steps = 28\npred_length = timedelta(pred_steps)\n\nfirst_day = pd.to_datetime(data_start_date) \nlast_day = pd.to_datetime(data_end_date)\n\ntest_pred_start = last_day - pred_length + timedelta(1)\ntest_pred_end = last_day\n\nval_pred_start = test_pred_start - pred_length\nval_pred_end = test_pred_start - timedelta(days=1)\n\ntrain_pred_start = val_pred_start - pred_length\ntrain_pred_end = val_pred_start - timedelta(days=1)\n\nenc_length = train_pred_start - first_day\n\ntrain_enc_start = first_day\ntrain_enc_end = train_enc_start + enc_length - timedelta(1)\n\nval_enc_start = train_enc_start + pred_length\nval_enc_end = val_enc_start + enc_length - timedelta(1)\n\ntest_enc_start = val_enc_start + pred_length\ntest_enc_end = test_enc_start + enc_length - timedelta(1)\n\nprint('Train encoding:', train_enc_start, '-', train_enc_end)\nprint('Train prediction:', train_pred_start, '-', train_pred_end, '\\n')\nprint('Val encoding:', val_enc_start, '-', val_enc_end)\nprint('Val prediction:', val_pred_start, '-', val_pred_end, '\\n')\nprint('Test encoding:', test_enc_start, '-', test_enc_end)\nprint('Test prediction:', test_pred_start, '-', test_pred_end)\n\nprint('\\nEncoding interval:', enc_length.days)\nprint('Prediction interval:', pred_length.days)","c9d7a20d":"n_samples = None #Set this to a finite number to train on a reduced number of samples\nbatch_size = 64\nepochs = 5\nlearning_rate = 1e-3\n\nseries_array = df[df.columns[1:]].values\n\nencoder_input_data, encode_series_mean, decoder_input_data, decoder_target_data = m5_helpers.get_all_data(date_to_index, series_array, train_enc_start, train_enc_end, train_pred_start, train_pred_end, n_samples=n_samples)\n\ncallbacks = m5_helpers.make_callbacks('lstm', 'lstm_run_1')\n\nmodel, dec_model, encoder_inputs, encoder_states = m5_models.create_enc_dec(learning_rate=learning_rate, hidden_size = 32, dropout = 0)\n\nhistory = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n                     batch_size=batch_size,\n                     epochs=epochs,\n                     validation_split=0.2,\n                     callbacks=callbacks)","0166bfa0":"history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n                     batch_size=batch_size,\n                     epochs=epochs,\n                     validation_split=0.2,\n                     callbacks=callbacks)","43c74893":"m5_helpers.make_training_plot(history)","63b35a06":"encoder_model = Model(encoder_inputs, encoder_states)\n\nencoder_input_data, encode_series_mean, decoder_input_data, decoder_target_data = m5_helpers.get_all_data(date_to_index, series_array, val_enc_start, val_enc_end, val_pred_start, val_pred_end, shuffle=False)\n\nm5_helpers.predict_and_plot(encoder_model, encoder_input_data, decoder_target_data, encode_series_mean, sample_ind=0, enc_tail_len=250, lstm=True, dec_model=dec_model)","3552e2b3":"n_samples = None #Set this to a finite number to train on a reduced number of samples\nbatch_size = 256\nepochs = 5\nlearning_rate = 3e-4\n\nseries_array = df[df.columns[1:]].values\n# weights = e.weights[:first_n_samples][0]\n\nencoder_input_data, encode_series_mean, decoder_input_data, decoder_target_data = m5_helpers.get_all_data(date_to_index, series_array, train_enc_start, train_enc_end, train_pred_start, train_pred_end, n_samples=n_samples)\n\n# we append a lagged history of the target series to the input data, so that we can train with teacher forcing\nlagged_target_history = decoder_target_data[:,:-1,:1]\nencoder_input_data = np.concatenate([encoder_input_data, lagged_target_history], axis=1)\n\ncallbacks = m5_helpers.make_callbacks('wavenet_model', 'wavenet_1')\nscore_cb = m5_helpers.score_callback(e=e, ids=[x+'_validation' for x in df.id], val_encoder_input=encoder_input_data, val_encode_series_mean=encode_series_mean)\ncallbacks.insert(0, score_cb)\n\n# Create a simple wavenet model\nmodel = m5_models.create_simple_wave(learning_rate=learning_rate)\n\n# Create a larger more complex wavenet model\n# model = Model_Functions.create_full_wave(learning_rate=learning_rate)\n\nprint(model.summary())\nhistory = model.fit(encoder_input_data, decoder_target_data,\n                    batch_size=batch_size,\n                    epochs=epochs,\n                    validation_split=0.2,\n                    callbacks=callbacks)","8205ae88":"m5_helpers.make_training_plot(history)","a12ff570":"encoder_input_data, encode_series_mean, decoder_input_data, decoder_target_data = m5_helpers.get_all_data(date_to_index, series_array, val_enc_start, val_enc_end, val_pred_start, val_pred_end, shuffle=False)\n\nm5_helpers.predict_and_plot(model, encoder_input_data, decoder_target_data, encode_series_mean, 35000, enc_tail_len=250, lstm=False, dec_model=None)","6a903eb8":"batch_size = 1024\ninput_steps = 28*4\npred_steps=28\n\nval_frac=0.1\nseries_array = sklearn.utils.shuffle(df[df.columns[1:]].values, random_state=42) # shuffle once up front so don't have to waste time later\nweights = sklearn.utils.shuffle(e.weights[0], random_state=42)\ntrain_samples = int((1-val_frac)*series_array.shape[0])-1\nval_samples = int(val_frac**series_array.shape[0])\n\nfirst_valid_day = first_day + timedelta(days=input_steps)\nvalid_train_end_days = [first_valid_day + timedelta(days=x) for x in range((train_enc_end-first_valid_day).days + 1)]\n\ndef data_generator():\n    random_start_id = np.random.choice(train_samples-batch_size-val_samples, 1)[0]\n    random_train_enc_end_day = np.random.choice(valid_train_end_days, 1)[0]\n    random_train_enc_start_day = random_train_enc_end_day - timedelta(days=input_steps-1)\n    \n    random_train_pred_start_day = random_train_enc_end_day + timedelta(days=1)\n    random_train_pred_end_day = random_train_pred_start_day + timedelta(days=pred_steps) - timedelta(1)\n\n    encoder_input_data, encode_series_mean, decoder_input_data, decoder_target_data = m5_helpers.get_all_data(date_to_index, series_array, \n                                                                                                                    random_train_enc_start_day, random_train_enc_end_day, \n                                                                                                                    random_train_pred_start_day, random_train_pred_end_day, \n                                                                                                                    shuffle=True, n_samples=train_samples)\n    yield (encoder_input_data[random_start_id:random_start_id+batch_size,:,:], \n    decoder_target_data[random_start_id:random_start_id+batch_size,:,:],\n    weights[random_start_id:random_start_id+batch_size])\n\nseries_array = df[df.columns[1:]].values\nweights = e.weights[0]\n\nencoder_input_data, encode_series_mean, decoder_input_data, decoder_target_data = m5_helpers.get_all_data(date_to_index, series_array, val_pred_start - timedelta(days=input_steps), val_pred_start - timedelta(days=1), val_pred_start, val_pred_end, shuffle=False)\nvalidation_data = (encoder_input_data, decoder_target_data, weights)\nvalidation_dataset = tf.data.Dataset.from_tensor_slices(validation_data).batch(batch_size)","ddceb00b":"epochs = 5\nlearning_rate=3e-4\n\ncallbacks = m5_helpers.make_callbacks('nbeats_model', 'nbeats_run_1')\n\n# val_encoder_input = pd.concat([df.id, pd.DataFrame(encoder_input_data.reshape(-1,28))], axis=1, ignore_index=True)\n\nscore_cb = m5_helpers.score_callback(e=e, ids=[x+'_validation' for x in df.id], val_encoder_input=encoder_input_data, val_encode_series_mean=encode_series_mean)\ncallbacks.insert(0, score_cb)\n\nshapes = ((batch_size, input_steps, 1),(batch_size, pred_steps, 1),(batch_size,))\ndataset = tf.data.Dataset.from_generator(data_generator, (tf.float32, tf.float32, tf.float32), output_shapes=shapes).repeat()\n\nmodel = nbeats.NBeatsNet(backcast_length=input_steps, forecast_length=pred_steps,\n                  stack_types=(nbeats.NBeatsNet.GENERIC_BLOCK, nbeats.NBeatsNet.GENERIC_BLOCK), nb_blocks_per_stack=2,\n                  thetas_dim=(4, 4), share_weights_in_stack=False, hidden_layer_units=8)\n\nmodel.compile(Adam(learning_rate=learning_rate), loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\nhistory = model.fit(dataset, steps_per_epoch=200, validation_data=validation_dataset, callbacks=callbacks, epochs=epochs)","ab14ea1e":"m5_helpers.make_training_plot(history)","b1933abf":"encoder_input_data, encode_series_mean, decoder_input_data, decoder_target_data = m5_helpers.get_all_data(date_to_index, series_array, val_pred_start - timedelta(days=input_steps), val_pred_start - timedelta(days=1), val_pred_start, val_pred_end, shuffle=False)\n\nm5_helpers.predict_and_plot(model, encoder_input_data, decoder_target_data, encode_series_mean, sample_ind=1000, enc_tail_len=input_steps, lstm=False, dec_model=None, nbeats=True)","7afcbd7d":"predict_batch_size = 1024\nseries_array = pd.read_csv(f'{path}\/sales_train_validation.csv').iloc[:,6:].values\nbatches = series_array.shape[0]\/\/predict_batch_size+1\npreds = []\n\nencoder_input_data, encode_series_mean, decoder_input_data, decoder_target_data = m5_helpers.get_all_data(date_to_index, series_array, val_pred_start - timedelta(days=input_steps), val_pred_start - timedelta(days=1), val_pred_start, val_pred_end, shuffle=False)\npredictions = model.predict(encoder_input_data)","055003f0":"predictions = m5_helpers.untransform_series_decode(predictions, encode_series_mean)\npredictions = pd.DataFrame(predictions.reshape(-1,28))\npredictions['id']=train_df.id\npredictions = predictions[['id']+list(predictions.columns[:-1])]\npredictions.columns = ['id'] + ['d_' + str(x+1886) for x in np.arange(28)]\n\n# Score on val set\nprint('Score: ' + str(e.score(predictions.iloc[:,1:])))","172f608b":"# Look at which time series are contributing to this score\nprint(e.contributors)","7bc7d4a1":"# For submission\n# submitte_predictions = predictions.iloc[:,1:]\n# submitte_predictions.columns = ['F'+str(x+1) for x in np.arange(28)]\n\n# Make submissions file\nsample_submission = pd.DataFrame(pd.read_csv(f'{path}\/sample_submission.csv').id)\nsample_submission = sample_submission.merge(predictions, on='id', how='left')\nsample_submission.fillna(0, inplace=True)\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head()","90e83fcf":"Hi and welcome to my first ever kaggle contribution!\n\nThere are quite a number of public kernels demonstrating how to use gradient boosting methods to tackle this problem, most involving a large degree of feature engineering (lags etc). While these methods work very well, I always find there is something unsatisfying about having to do a large amount of work to shoe-horn time domain information into an underlying model (gradient boosting) that doesn't natively make use of it. To that end I thought would share a simple notebook that shows how to do multi-step forecasting using a variety of RNN\/CNN\/other architectures which natively use the sequential information inherent in time series. While at present my implementation of these methods don't score very well (no doubt in part to the sporadic nature of the time series in this competition), they can almost certainl be improved through the addition of extra features (at present they only use the sales information i.e. they have a single channel), architectural modifications to account for the large number of 0s, optimization of hyperparameters etc.\n\nNote this notebook re-uses alot of work orginally found in https:\/\/github.com\/JEddy92\/TimeSeries_Seq2Seq","732f18cb":"> ## Import Data \n#### Here we import the data and use the nice WRMSEEvaluator class provided by Dhananjay Raut.","ff48a385":"## Simple LSTM Model\n- We implement a simple encoder\/decoder architecture based on an LSTM\n- Callbacks are used to log tensorboard output and to checkpoint the model each time the performance improves on the validation set\n- Note that we are doing a very naive train\/val split here where correlation between time series' will lead to leakage into the validation set","884102a3":"## Generate model predictions","393f551f":"## Wavenet Model\n- We use the standard wavenet model where you can select between either a simplified wavenet architecture, or a more expressive one\n\nAdditions from the LSTM used above:\n- We add a callback to calculate the appropriately weighted WRMSE score on the validation set","d4d4b04b":"## N-BEATS\n\nWe use the N-BEATS implementation courtesy of (https:\/\/github.com\/philipperemy\/n-beats). \n\nOther additions:\n- Weights are used during training calculated via the WRMSEEvaluator class\n- We use a data generator to sample random subsequences to train on","f68e7966":"### Set up date ranges"}}