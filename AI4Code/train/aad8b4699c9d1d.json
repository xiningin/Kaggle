{"cell_type":{"125871af":"code","7627eed1":"code","07917829":"code","2d5350d2":"code","99b05d30":"code","c1b297c1":"code","c76c21eb":"code","408d900d":"code","0663399d":"code","e1ad90a8":"code","9ef35d81":"code","9e174bcd":"code","62bd593b":"code","4e35161f":"code","a1556195":"code","65df0178":"code","f950ad7a":"code","605a17d3":"code","69d763a5":"code","bd2d36dd":"code","b668afe5":"code","6bf20a1d":"markdown","e3a33231":"markdown","133fd324":"markdown","f04014eb":"markdown","88b6824c":"markdown","2b016f23":"markdown","39434c87":"markdown","f75c8d92":"markdown","4762edca":"markdown","3ab4c5ab":"markdown","25c34ae2":"markdown","05dec15a":"markdown","8b59a5c3":"markdown","2c90ba5f":"markdown","3f99e28b":"markdown","a7173f0f":"markdown","156cbddb":"markdown","87e1864c":"markdown","ccf61db6":"markdown","15c32037":"markdown","e4c72447":"markdown","f951e441":"markdown","6d1b16a0":"markdown","1a49ceef":"markdown","68dee3d4":"markdown","403507f8":"markdown"},"source":{"125871af":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy\n%matplotlib inline\nimport warnings; warnings.simplefilter('ignore')","7627eed1":"DATA = pd.read_csv(\"..\/input\/OpenData2018.csv\", sep=\";\")","07917829":"DATA.head()","2d5350d2":"grades = DATA[[\"UkrBall100\", \"mathBall100\", \"engBall100\", \"physBall100\", \"geoBall100\", \"histBall100\",\n               'chemBall100', 'bioBall100', 'fraBall100',  'deuBall100',  'spaBall100']].replace({0:np.nan})","99b05d30":"grades.describe()","c1b297c1":"dat = grades.histBall100 \nst = (dat.mean() - dat.min()) \/ dat.std() # calculate the statistics\n\nprint(\"Test Statistics: \", st)","c76c21eb":"def plot_distr(subject):\n    col = subject+\"Ball100\"\n    dat = DATA[col].replace({0:np.nan}).dropna()\n    sns.distplot(dat, bins=100, color=\"c\")\n    \nplot_distr(\"hist\") # History","408d900d":"def fit_distribution(data):\n    distributions = [\"expon\", \"weibull_max\", \"pareto\", \"genextreme\"]\n    dist_results = []\n    params = {}\n    \n    plt.hist(data, normed=True)\n    rX = np.linspace(100,200, 100)\n    \n    for dist_name in distributions:\n        # fit the distribution\n        dist = getattr(scipy.stats, dist_name)\n        param = dist.fit(data)\n        params[dist_name] = param\n        \n        # Use the Kolmogorov-Smirnov test\n        D, p = scipy.stats.kstest(data, dist_name, args=param)\n\n        dist_results.append((dist_name, p, D))\n        \n        rP = dist.pdf(rX, *param)\n        plt.plot(rX, rP, label=dist_name)\n        \n        print(dist_name.ljust(16) + (\"p-value: \"+str(p)).ljust(16) + \"D: \"+str(D))    \n    plt.legend()\n    plt.show()\n            \n    return params, dist_results\n\n\nparams, results = fit_distribution(list(grades.histBall100.dropna()))","0663399d":"# find minimum distance between the CDF's of the two samples\nbest_dist, best_p, best_d = (min(results, key=lambda item: item[2]))\n    \nprint(\"Most close fitting distribution: \"+str(best_dist))\nprint(\"It's p-value: \"+ str(best_p))\nprint(\"Parameters for the most close fit: \"+ str(params[best_dist]))","e1ad90a8":"regdata = DATA[[\"REGNAME\", \"UkrBall100\", \"mathBall100\", \"engBall100\", \"physBall100\", \"geoBall100\", \"histBall100\",\n               'chemBall100', 'bioBall100', 'fraBall100',  'deuBall100',  'spaBall100']].replace({0:np.nan})\n\ndef plot_region_avg(sub):\n    col1 = \"REGNAME\"\n    col2 = sub +\"Ball100\"\n    ss = regdata[[col1, col2]].dropna() # select subject and clean data\n    b1, b2 = [], []\n    \n    for region in sorted(set(ss[col1])):\n        people = ss[ss[col1] == region]\n        mean = people[col2].mean()\n        b1.append(region)\n        b2.append(mean)\n        #print(region, mean)\n    regData = pd.DataFrame( {col1: b1, sub: b2})\n    regplot = sns.barplot(x=\"REGNAME\", y=sub, data=regData)\n    regplot.set_xticklabels(regplot.get_xticklabels(), rotation=90)\n    plt.show()\n\nplot_region_avg(\"hist\")","9ef35d81":"plot_region_avg(\"math\")","9e174bcd":"plot_region_avg(\"phys\")","62bd593b":"plot_region_avg(\"Ukr\")","4e35161f":"sns.heatmap(grades.corr())\nplt.show()","a1556195":"# physics and mathematics\n\ndf = DATA[np.isfinite(DATA['physBall100'])]\ndf = df[np.isfinite(df['mathBall100'])]\nscipy.stats.linregress(df.mathBall100, df.physBall100)","65df0178":"def lin(sub1, sub2):\n    col1 = sub1+\"Ball100\"\n    col2 = sub2+\"Ball100\"\n    ss = DATA[[col1, col2]].replace({0:np.nan}).dropna() #select two subjects and clean data\n    b1, b2 = [], []\n    for ball in sorted(set(ss[col1])):\n        people = ss[ss[col1] == ball]\n        mean = people[col2].mean()\n        b1.append(ball)\n        b2.append(mean)\n    return pd.DataFrame( {sub1: b1, sub2: b2})\n\ns1, s2 = \"phys\", \"math\"\ndf = lin(s1, s2)\nsns.lmplot(s1, s2, df)\nplt.show()","f950ad7a":"# geography and spanish\n\ndf = DATA[np.isfinite(DATA['spaBall100'])]\ndf = df[np.isfinite(df['geoBall100'])]\nscipy.stats.linregress(df.spaBall100, df.geoBall100)","605a17d3":"s1, s2 = \"geo\", \"spa\"\ndf = lin(s1, s2)\nsns.lmplot(s1, s2, df)\nplt.show()","69d763a5":"df = DATA.replace({0:np.nan})\n\ndef plot_subject(subject):\n    count = [\"SEXTYPENAME\", \"ClassProfileNAME\", \"ClassLangName\"]\n    plt.figure(figsize=(18,8))\n    ax = sns.countplot(x=subject+'Ball100', hue=count[0], data=df, orient=\"h\")\n    #ax = sns.countplot(x=\"SEXTYPENAME\", hue=subject+'TestStatus', data=DATA, orient=\"h\")\n    ax.set_xticklabels(ax.get_xticklabels(), fontsize=7)\n    plt.tight_layout()\n    plt.show()\n    \nplot_subject(\"math\")\nplot_subject(\"hist\")","bd2d36dd":"plot_subject(\"Ukr\")","b668afe5":"plot_subject(\"phys\")","6bf20a1d":"# Fitting Distribution","e3a33231":"Here, regression result shows us that there is no dependency linear dependency between these two subjects, i.e. we would not be able to predict the results for spanish based on results for geography test.","133fd324":"The dataset includes information about every participant of independent testing: their id, birth date, sex, region, school, class (specialization), and test results for subjects they passed: test score, and the results in 12- and 200-point grade scale. You can see the detalized description in the attached xls file, *\"OpenDataInfo2018.xls\"*. For now, we will need just the information about the results of testing. The _grades_ dataframe includes just the test results in 200-point scale.","f04014eb":"# Data Overview","88b6824c":"We can see that people who wrote mathematics well enough have good results for physics, too. So, our hypothesis is linear dependency between these two grades. To test it, we need to select people who participated in testing in both of these subjects and run a linear regression.","2b016f23":"The first natural assumption that comes to mind is that grades are normally distributed, i.e. most of the people resulted somewhere around 150, there are some who did very well and some who almost failed. Let's take a look at the disributions of grades for some subjects.","39434c87":"# Boys vs. Girls","f75c8d92":"Let's take a look at the correlation between grades for several subjects.\n","4762edca":"The only hypothesis we accept is that Ukrainian students are unique and do not fit any known distribution.","3ab4c5ab":"Working with real-world data is hard. For example, I was not able to just \"drop all NaN's\" at the very beginning to have clear data, because this way I would have dropped everything from the table. The data is not stored very vell; there are some mistakes in the recordings, problems with outliers. I had to clean parts of the data every time I needed it.\n\nFrom the very beginning, when the research project was announced, and we were told to find some data ourselves, I knew that want to analyze some Ukrainian open data. There are many messy datasets over here, but the topic that grabbed my attention was education. It was also hard to come from simple data observation to making a hypothesis and finding ways to test them, to come up with some reasonable ideas. Few times I just wanted to leave it all and switch to some typical dataset from Kaggle, run a linear regression, make predictions, and that's it.\n\nI guess that some of the hypothesis proposed above may probably be irrelevant. For example, running linear regression for grades for related subjects. It is ok to talk about correlation in this case, but is it ok to talk about dependency? Anyway, the results gave some extra info and, what's more important, practice.\n\nIt is hard to fit real-world data to a known distribution. Maybe, it's unique for this particular case. But, as one can see above, the distribution isn't looking the way I expected it to be.\n\nSimply calculating basic statistics for every subject gives us a good overview of the data we have. Even though finding out the results of the testing was, literally, hurtful (i.e. my expectations for average grades were higher), some hypothesis were accepted. For example, we approved relations for grades for the natural sciences (e.g. mathematics and physics) and showed that there is no correlation between grades for Spanish language and geography.\n\nAnother conclusion: sometimes just visualizations can help \u2014 most of the times. As for me, the last bar plot is a great example of it. I was shocked when I saw how little girls are passing tests in physics.\n\nThe other strange observation is that the two regions which perform the best are Lviv region and Kyiv city. It would be interesting to find the reasons for such results.\n\nTo sum up, this project gave me the opportunity to practice hypothesis testing and practice approaches to working with data, finally understand what is p-value, find out about 80+ distributions that are implemented in SciPy and that I haven't heard about before.\n\nIt also lead me to some thoughts about our educational system, in general. In my opinion, the testing results presented here are bad. We need to find reasons for such results. We also need to think about the fact that all the pupils go to universities after schools. But the question is for what reason most of them are doing this? We should concentrate on quality, not quantity.","25c34ae2":"# Correlations","05dec15a":"As we can see, all the p-values are 0's. This means that either Python is not able to display such small values, or it is _impossible_ that our distribution fits any of the tested above. At leat, we can find the closest one, considering CDF's.","8b59a5c3":" It is not Normal. Now, we are not sure we'll be able to fit it to some known distribution, but will try fitting several distributions that are likely to appear here to see what we get.\n \nFor this, we first use _fit()_, which, according to docs, gives us the maximum likelihood estimates (MLE) for parameters for each of our possible distributions. After that, when we have the estimated parameters, we can get the theoretical distribution and use Kolmogorov-Smirnov test for goodness of fit. Here, we will test the following hypothesis:\n\n$H_0$: the two distributions are identical\n<br>\n$H_1$: the two distributions are different\n\n_kstest()_ performs a test of the distribution of an observed random variable against a given distribution. Under the null hypothesis the two distributions are identical. The alternative hypothesis is 'two-sided'. As a result, we get the p-value and corresponding test statistic, D. The p-value returned by the test is just the same as other p-values :) The D statistic is the absolute maximum distance (supremum) between the CDF's of the two samples. The closer this number is to 0, the more likely it is that the two samples were drawn from the same distribution.\n\nWe reject the null hypothesis that the two samples were drawn from the same distribution if the p-value is less than our significance level. _But there is no need to choose significance level here, because we completely fail and reject each of our hopes to fit at least some distribution._  \ud83e\udd37 Look:","2c90ba5f":"We can reject $H_0$. Bu I'm still not sure the distribution is exponential. Let's plot it, finally. _(Maybe I should have done it before (and I did), but the dataset is not very informative and I was not able to come up with other meaningfull hypothesis to test)_","3f99e28b":"# Independent testing in Ukraine - what does it depend on?","a7173f0f":"But for Ukrainian language, girls perform better.","156cbddb":"Some other interesting facts about boys and girls. For subjects, such as history or mathematics, boys and girls have basically the same results.","87e1864c":"# The aim of this kernel is to examine, analyze, and visualize open data about External independent testing in Ukraine.\nFor this, I will be using a dataset provided by Ukrainian center for educational quality assessment.","ccf61db6":"Let's take, for example, history. From the 75-percentille, which is only 154, I can guess that most of the people had not wery good results, and the distribution is skewed to the right. My other assumption now is that distribution is exponential. Let's deal with the numbers: we will test $H_0$ : the grades for the history test are normally distributed vs. $H_1$: the grades for the history test have exponential distribution. \n\nThe Normal distribution is a location-scale family. This means that we need to use a test that is invariant to scale and location. The only meaningful statistics I was able to find is the one mentioned by H.C. Thode. in his __[Testing for Normality](https:\/\/books.google.com.ua\/books?id=gbegXB4SdosC&pg=PA471&lpg=PA471&dq=most+powerful+location+and+scale+invariant+test+grubbs&source=bl&ots=9Zw7_jeECU&sig=8IJWRYp0eti5bDSQd0uvPpTvKn4&hl=uk&sa=X&ved=2ahUKEwjhzLao_97fAhXC8uAKHZ0IBTEQ6AEwAnoECAMQAQ#v=onepage&q=most%20powerful%20location%20and%20scale%20invariant%20test%20grubbs&f=false)__ (Section 4.2.4):\n$$T = \\frac{\\overline{x} - x_0}{s} $$\n\n- $\\overline{x}$ is the sample mean\n- $x_0$ is the minimum observation in the sample\n- $s$ is the standard deviation of the sample\n\nWe reject $H_0$ when test statiscic is large enough.\nAs I found later, this is just a one-sided Grubb's test, which is used to determine outlier. This makes sense for us: performing this test, under the assumption of normality, we are 'checking' if $x_0$ is the outlier. If it is, then, for us, this means that we reject null hypothesis of normality in favour of exponentiality.","15c32037":"Just calculating the basic statistics impresses. Average grades are, for most of the subjects, below 150 :(","e4c72447":"We can see that there is a slope, and the rvalue, which is a correlation coefficient, is even greater than 0.5 for mathematics and physics. The grades for these two subjects have linear dependency.","f951e441":"# Leading Regions","6d1b16a0":"# Conclusions","1a49ceef":"Let's first take a brief overview of the data we have. The dataset includes information about the participants and results of the External independent testing conducted in 2018. ","68dee3d4":"What about physics? Unfortunately, girls do not like physics.","403507f8":"Now, let's take a look at some other interesting facts we have noticed. For example, this is interesting that Kyiv city and Lviv region perform better, on average, for almost every subject, that other reegions of the country."}}