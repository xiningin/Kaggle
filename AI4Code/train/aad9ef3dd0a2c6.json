{"cell_type":{"6daaf394":"code","9c42be58":"code","3fe1397d":"code","8fa12fad":"code","68877dfd":"code","3f941ae9":"code","a5eba370":"code","d95b4ed4":"code","4d583fce":"code","ce2fc724":"code","e34205d2":"code","426ac8c2":"code","e6c73ba1":"code","6b2e0dc7":"code","a45b915e":"code","743b6b90":"code","11486d32":"code","ec53f17f":"code","f4248ac6":"code","d28ce983":"code","c1f70818":"code","3078a845":"code","72b42d9d":"code","a7ae651f":"code","4d24be5a":"markdown","d6461ea0":"markdown","00d602b0":"markdown","92bd745d":"markdown","9e7de7e5":"markdown","a29c9826":"markdown","a422ad43":"markdown"},"source":{"6daaf394":"import numpy as np\nimport pandas as pd","9c42be58":"us_yt = pd.read_csv('..\/input\/youtube-new\/USvideos.csv')\ndisplay(us_yt.head())\nprint(us_yt.columns)\n","3fe1397d":"#another curiousity of mine\nus_yt.category_id.nunique()#.unique_count().sum()\nus_yt.head()","8fa12fad":"corrolation_list = ['views', 'likes', 'dislikes', 'comment_count']\nhm_data = us_yt[corrolation_list].corr() \ndisplay(hm_data)\n#This will trim down the number of columns we are looking for correlations between\n#this exceeds the scope of the initial question but i want to know a little more for myself ","68877dfd":"import matplotlib as plt\nimport seaborn as sns\nimport matplotlib.pyplot\n","3f941ae9":"# to help you visualize these correlations, see the plots below\n# the lighter the colour the higher the correlation\n\nmatplotlib.pyplot.figure(figsize=(8,6))\nsns.heatmap(hm_data, annot=True);","a5eba370":"#matplotlib.pyplot.figure(figsize=(12,12))\nsns.pairplot(us_yt[['views', 'likes']], kind='reg',height=6);","d95b4ed4":"#to get a closer look at the scatterplot for likes\/views\nmatplotlib.pyplot.figure(figsize=(16,16))\nsns.scatterplot(x=us_yt['views'], y=us_yt['likes']);","4d583fce":"#For these datasets we will use a subset of the columns, certain columns have characters that\n#the dataframe can't currently handle and since those columns don't contain meaningful data for\n#this use case I'm electing to simply drop them \ncol_list = ['video_id', 'views', 'likes', 'dislikes', 'comment_count']\n\nus_yt = pd.read_csv('..\/input\/youtube-new\/USvideos.csv', usecols=col_list) #USA, remaking the dataframe in the same format as the others\nca_yt = pd.read_csv('..\/input\/youtube-new\/CAvideos.csv', usecols=col_list) #Canada\nde_yt = pd.read_csv('..\/input\/youtube-new\/DEvideos.csv', usecols=col_list) #Germany\nfr_yt = pd.read_csv('..\/input\/youtube-new\/FRvideos.csv', usecols=col_list) #France\ngb_yt = pd.read_csv('..\/input\/youtube-new\/GBvideos.csv', usecols=col_list) #United Kingdom (Great Brittain)\nin_yt = pd.read_csv('..\/input\/youtube-new\/INvideos.csv', usecols=col_list) #India\njp_yt = pd.read_csv('..\/input\/youtube-new\/JPvideos.csv', usecols=col_list) #Japan\nkr_yt = pd.read_csv('..\/input\/youtube-new\/KRvideos.csv', usecols=col_list) #South Korea\nmx_yt = pd.read_csv('..\/input\/youtube-new\/MXvideos.csv', usecols=col_list) #Mexico\nru_yt = pd.read_csv('..\/input\/youtube-new\/RUvideos.csv', usecols=col_list) #Russia","ce2fc724":"df_list = [us_yt, ca_yt, de_yt, fr_yt, gb_yt, in_yt, jp_yt, kr_yt, mx_yt, ru_yt]\ndf_name_list = ['United States', 'Canada', 'Germany', 'France', 'Great Brittain', 'India',\n                'Japan', 'South Korea', 'Mexico', 'Russia']\n","e34205d2":"views_df = pd.DataFrame(columns=['views', 'likes', 'dislikes', 'comment_count'])\nlikes_df = pd.DataFrame(columns=['views', 'likes', 'dislikes', 'comment_count'])\n\ndisplay(views_df, likes_df)","426ac8c2":"count = 0\nwhile count != 10:\n    print(df_name_list[count])\n    current_df = df_list[count]\n    _x = current_df[corrolation_list].corr()\n    display(_x)\n    views_df.loc[count] = _x.loc['views']\n    likes_df.loc[count] = _x.loc['likes']\n    count += 1\n# Hid output to keep the size of the notebook down, feel free to unhide it to see more of the data","e6c73ba1":"views_df = views_df.drop(axis=1, labels='views') #remove redundant column\nviews_df.index = df_name_list\ndisplay(views_df.style.background_gradient()) ## MasterData frame with all the correlation coefficients in relation to views ","6b2e0dc7":"likes_df = likes_df.drop(axis=1, labels='likes') #remove redundant column\nlikes_df.index = df_name_list\ndisplay(likes_df.style.background_gradient()) ## Master Data frame with all the corrolation coefficients in relation to likes","a45b915e":"matplotlib.pyplot.figure(figsize=(8,6))\nsns.heatmap(likes_df, annot=True, linewidths=.5);","743b6b90":"Master_df = pd.DataFrame(columns=['views', 'likes', 'dislikes', 'comment_count', 'country'])\ndisplay(Master_df)","11486d32":"#now to create a master dataframe with all the datapoints in it\ncount = 0\nentries = 0\nwhile count != 10:\n    current_df = df_list[count]\n    entries = entries + len(current_df)\n    country_name = df_name_list[count]\n    current_df['country'] = country_name\n    Master_df = pd.merge(Master_df, current_df, how='outer')\n    count += 1\nprint(entries)","ec53f17f":"print(entries) \nprint(len(Master_df)) #comparing the total number of entries to make sure I didn't lose any data points \ndisplay(Master_df.sample(10))\n","f4248ac6":"Master_df[corrolation_list].corr().style.background_gradient()","d28ce983":"matplotlib.pyplot.figure(figsize=(16,16))\nsns.scatterplot(x=Master_df[\"views\"], y=Master_df[\"likes\"], hue=Master_df[\"country\"])\nsns.despine(bottom=True, left=True);","c1f70818":"col_list = [\"title\"]\n\nus_yt = pd.read_csv('..\/input\/youtube-new\/USvideos.csv', usecols=col_list) #USA, remaking the dataframe in the same format as the others\nca_yt = pd.read_csv('..\/input\/youtube-new\/CAvideos.csv', usecols=col_list) #Canada\nde_yt = pd.read_csv('..\/input\/youtube-new\/DEvideos.csv', usecols=col_list) #Germany\nfr_yt = pd.read_csv('..\/input\/youtube-new\/FRvideos.csv', usecols=col_list) #France\ngb_yt = pd.read_csv('..\/input\/youtube-new\/GBvideos.csv', usecols=col_list) #United Kingdom (Great Brittain)\nin_yt = pd.read_csv('..\/input\/youtube-new\/INvideos.csv', usecols=col_list) #India\njp_yt = pd.read_csv('..\/input\/youtube-new\/JPvideos.csv', usecols=col_list) #Japan\nkr_yt = pd.read_csv('..\/input\/youtube-new\/KRvideos.csv', usecols=col_list) #South Korea\nmx_yt = pd.read_csv('..\/input\/youtube-new\/MXvideos.csv', usecols=col_list) #Mexico\nru_yt = pd.read_csv('..\/input\/youtube-new\/RUvideos.csv', usecols=col_list) #Russia","3078a845":"df_list = [us_yt, ca_yt, de_yt, fr_yt, gb_yt, in_yt, jp_yt, kr_yt, mx_yt, ru_yt]\ndf_name_list = ['United States', 'Canada', 'Germany', 'France', 'Great Brittain', 'India',\n                'Japan', 'South Korea', 'Mexico', 'Russia']\n","72b42d9d":"\n#for a in df_list:\ndef wordcld(a,j):\n    from wordcloud import WordCloud\n    import matplotlib.pyplot as plt\n    title_words = list(a[\"title\"].apply(lambda x: x.split()))\n    title_words = [x for y in title_words for x in y]\n    #print(df_name_list[j])\n    wc = WordCloud(width=1200, height=500, \n                                collocations=False, background_color=\"white\", \n                                colormap=\"tab20b\").generate(\" \".join(title_words))\n    plt.figure(figsize=(15,10))\n    plt.imshow(wc, interpolation='bilinear')\n    #plt.axis(\"off\")\n    plt.ylabel(df_name_list[j])","a7ae651f":"j=0\nwhile j<len(df_list):\n    #print(df_name_list[j])\n    wordcld(df_list[j],j)\n    j=j+1\n    ","4d24be5a":"<a id=\"part-1\"><\/a>\n## US Data\nWe'll start with a subset of the data (US) before expanding to include the entire dataset","d6461ea0":"This is the pearson correlation values:<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 = a strong positive correlation<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0 = no correlation<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -1 = strong negative correlation.\n\nThis shows that there is a strong positive correlation with number of likes and number of views, while there is a moderate correlation between views and comment count \n","00d602b0":"# SHOWING MOST COMMON WORDS USED IN TITLE FOR EACH COUTRY BY WORD CLOUD","92bd745d":"# <a id=\"part-2\"><\/a>\n## World Data\n\n# Now that we see the correlation coefficient for the US data, let's now see how much these trends stay the same in other countries.","9e7de7e5":"We can see that there is a high correlation between the number of likes on a video and the number of views. Also an interesting note: likes and comment count has an incredibly high correlation in Japan, Japan also has a weirdly high correlation between dislikes, likes and views (one could also rephrase this to a high correlation between reactions and views). There are other interesting little tidbits in the data.\n\n(obligatory correlation =\/= causation speil)Without seeing this data in timelapse one couldn't determine if a particular variable is the cause of the other. For instance: are videos that are watched frequently more likely to be liked by a viewer, or does a video receiving a large number of likes cause it to be recommended frequently. Or perhaps neither is the cause of the other and perhaps some confounding variable is the cause of both of these. ","a29c9826":"\n\nAs you can see below the trend below seems to follow for each country in the datasets","a422ad43":"# Correlations On Youtube TOTW\n\nThis notebook is to answer the task: Correlation between views and likes. This is my Third draft (content complete), I encourage constructive feedback. As time allows I will revisit this notebook and polish it up.\nThe conclusion is that there is a strong positive correlation between views and likes on youtube.\n\n\n[US Data](#part-1)\n\n[World Data](#part-2)\n\n[Final Visual](#part-3)"}}