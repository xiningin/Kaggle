{"cell_type":{"bcd92e61":"code","fd5c2031":"code","78cb219e":"code","dd746862":"code","f14c0978":"code","461b7963":"code","c914254a":"code","50317eda":"code","1d2adf27":"code","230a5fc0":"code","9a5def07":"code","2af08f22":"code","130107d6":"code","ff13665b":"code","8f3afec3":"code","120ddcb5":"code","5d0a3ade":"code","29ead473":"code","e1d30854":"code","e472b293":"code","85f4d1b5":"code","5291887b":"code","57ed61cb":"code","f6af99f0":"code","61dd9b63":"code","78eff3e9":"code","d414ce1e":"code","02e3050d":"code","c7036d53":"code","64cdf42d":"code","09c15f07":"code","da82afc4":"code","cc81843e":"markdown","f2d278b5":"markdown","fc5f2d5e":"markdown","198f3bce":"markdown","eff80305":"markdown","b0b9d115":"markdown","4f4ab805":"markdown","e134d941":"markdown","51f831f4":"markdown","5c423d50":"markdown","59b2dc36":"markdown","a602f12a":"markdown","c9d52061":"markdown","86bf1a9e":"markdown","ad267fe1":"markdown","ed8116a7":"markdown","34157633":"markdown","b6b40576":"markdown","2d521be8":"markdown","eb258489":"markdown","99d2d769":"markdown","e2a5d7e3":"markdown","70fba617":"markdown","8819c9d9":"markdown"},"source":{"bcd92e61":"# Imports \nimport numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap, LinearSegmentedColormap\nimport matplotlib.patches as mpatches\n\nfrom sklearn.preprocessing import normalize\nfrom sklearn.decomposition import NMF\n\n\n# Helper functions for displaying the digits\ndef show_as_image(sample, shape):\n    # shape = tuple e.g. (13,8)\n    bitmap = sample.reshape(shape)\n    plt.figure()\n    plt.imshow(bitmap, cmap='gray', interpolation='nearest')\n    plt.colorbar()\n    plt.show()\n    \n# Useful to be able to pass an axes, as subplots enable view of many images\ndef plot_on_axes(sample, shape, ax):\n    bitmap = sample.reshape(shape)\n    ax.imshow(bitmap, cmap='gray', interpolation='nearest')","fd5c2031":"train = pd.read_csv('\/kaggle\/input\/mnist-in-csv\/mnist_train.csv')\ntest = pd.read_csv('\/kaggle\/input\/mnist-in-csv\/mnist_test.csv')\ndata = pd.concat([train, test])\ndata.head()","78cb219e":"# Create feature (X) and label (y) numpy arrays\nX = np.array(data.loc[:, '1x1':])\ny = np.array(data.loc[:, 'label'])","dd746862":"n_examples = 10\nfig, axes = plt.subplots(10, n_examples, figsize=(n_examples, 10))\n\nfor digit in np.arange(10):\n    digit_indexes = np.where(y == digit)[0][:n_examples]\n    \n    for i, index in enumerate(digit_indexes): \n        plot_on_axes(X[index], (28, 28), axes[digit][i])\n        axes[digit][i].get_xaxis().set_visible(False)\n        axes[digit][i].get_yaxis().set_visible(False)\n        \nplt.show()","f14c0978":"model = NMF(n_components=15, init='nndsvd', random_state=2021, max_iter=1000)\nmodel.fit(X)\nnmf_features = model.transform(X)","461b7963":"def show_components(model, display_on_ax_func, images_per_row, figsize_tuple):\n    n_components = model.n_components_\n    n_rows = (n_components \/\/ images_per_row) \n    n_rows = n_rows if (n_components \/\/ images_per_row) == 0 else n_rows + 1\n    fig, axes = plt.subplots(n_rows, images_per_row, figsize=figsize_tuple)\n    \n    for i, ax in enumerate(axes.flatten()):\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n        \n        if i < n_components:\n            display_on_ax_func(model.components_[i], (28, 28), ax)\n        else:\n            ax.axis('off')\n            \n    plt.show()","c914254a":"show_components(model, plot_on_axes, 5, (10, 6))","50317eda":"nmf_features.shape","1d2adf27":"model.components_.shape","230a5fc0":"nmf_features[0]","9a5def07":"example_iloc = 0\nplt.figure(figsize=(10, 3))\nplt.bar(x=np.arange(len(nmf_features[example_iloc])) + 1, height=nmf_features[example_iloc])\nplt.xticks(np.arange(len(nmf_features[example_iloc])) + 1)\nplt.xlabel('NMF Component')\nplt.title('Feature importance for given sample')\nplt.show()","2af08f22":"five_features = nmf_features[np.where(y == 5)[0]]\nseven_features = nmf_features[np.where(y == 7)[0]]\n\nfig, axes = plt.subplots(2, 1, figsize=(10, 10))\naxes[0].boxplot(five_features)\naxes[0].set_title('Features for \"5\"s')\naxes[1].boxplot(seven_features)\naxes[1].set_title('Features for \"7\"s')\nplt.show()","130107d6":"show_as_image(model.components_[0], (28, 28))","ff13665b":"show_as_image(model.components_[13], (28, 28))","8f3afec3":"fig, axes = plt.subplots(2, 1, figsize=(10, 10))\n\naxes[0].bar(x=np.arange(15), height=np.mean(five_features, axis=0))\naxes[1].bar(x=np.arange(15), height=np.mean(seven_features, axis=0))\naxes[0].set_title('Average importance of features for \"5\"s')\naxes[1].set_title('Average importance of features for \"7\"s')\nplt.show()","120ddcb5":"top_four_features_five = np.mean(five_features, axis=0).argsort()[-4:][::-1]\ntop_four_features_seven = np.mean(seven_features, axis=0).argsort()[-4:][::-1]\n\n# Top 4 components, on average, for the digit 5\nprint(top_four_features_five)\nnp.mean(five_features, axis=0)[top_four_features_five]","5d0a3ade":"fig, axs = plt.subplots(1, 4, figsize=(12, 3.5))\nfig.suptitle('Key components in \"5\"s')\nfor i, feature in enumerate(top_four_features_five):\n    plot_on_axes(model.components_[feature], (28, 28), axs.flatten()[i])\n    axs.flatten()[i].set_title(f'Component {feature + 1}: {round(np.mean(five_features, axis=0)[feature], 2)}')\n    axs.flatten()[i].get_xaxis().set_visible(False)\n    axs.flatten()[i].get_yaxis().set_visible(False)\nplt.show() ","29ead473":"fig, axs = plt.subplots(1, 4, figsize=(12, 3.5))\nfig.suptitle('Key components in \"7\"s')\nfor i, feature in enumerate(top_four_features_seven):\n    plot_on_axes(model.components_[feature], (28, 28), axs.flatten()[i])\n    axs.flatten()[i].set_title(f'Component {feature + 1}: {round(np.mean(seven_features, axis=0)[feature], 2)}')\n    axs.flatten()[i].get_xaxis().set_visible(False)\n    axs.flatten()[i].get_yaxis().set_visible(False)\nplt.show() ","e1d30854":"sample_index = 0\nsample_features = nmf_features[sample_index]\ncomponents = model.components_\nmanually_reconstructed_image = np.dot(sample_features, components)\n\nfig, axes = plt.subplots(1, 3, figsize=(10, 4))\nplot_on_axes(manually_reconstructed_image, (28, 28), axes[0])\naxes[0].set_title(f'Manually reconstructed image')\naxes[0].get_xaxis().set_visible(False)\naxes[0].get_yaxis().set_visible(False)\n\nplot_on_axes(X[sample_index], (28, 28), axes[1])\naxes[1].set_title(f'Original sample')\naxes[1].get_xaxis().set_visible(False)\naxes[1].get_yaxis().set_visible(False)\n\nplot_on_axes(model.inverse_transform(nmf_features[sample_index]), (28, 28), axes[2])\naxes[2].set_title(f'Using `inverse_transform()`')\naxes[2].get_xaxis().set_visible(False)\naxes[2].get_yaxis().set_visible(False)\n\nplt.show()","e472b293":"# All pixel values of the reconstruction match the inverse_transform method\nsum(model.inverse_transform(nmf_features[0]) == manually_reconstructed_image)","85f4d1b5":"# Rebuild all samples \nX_rebuilt = model.inverse_transform(nmf_features)\n\n# Compare the original image to the normalized version of the rebuild\nX_diff = X - X_rebuilt\n\n# Find the differences between the rebuilt and original by taking the norm of the differences row-wise\nnorm_diffs = pd.Series(np.linalg.norm(X_diff, axis=1))\n\nmost_different = np.array(pd.Series(norm_diffs).nlargest(20).index)\nmost_similar = np.array(pd.Series(norm_diffs).nsmallest(20).index)","5291887b":"sample_index = most_different[0]\nsample_features = nmf_features[sample_index]\ncomponents = model.components_\nmanually_reconstructed_image = np.dot(sample_features, components)\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\nfig.suptitle('Sample with poor reconstruction')\nplot_on_axes(manually_reconstructed_image, (28, 28), axes[0])\naxes[0].set_title(f'Reconstructed image')\naxes[0].get_xaxis().set_visible(False)\naxes[0].get_yaxis().set_visible(False)\n\nplot_on_axes(X[sample_index], (28, 28), axes[1])\naxes[1].set_title(f'Original sample')\naxes[1].get_xaxis().set_visible(False)\naxes[1].get_yaxis().set_visible(False)\n\nplt.show()","57ed61cb":"y[most_different]","f6af99f0":"sample_index = most_similar[2]\nsample_features = nmf_features[sample_index]\ncomponents = model.components_\nmanually_reconstructed_image = np.dot(sample_features, components)\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\nfig.suptitle('Sample with better reconstruction')\nplot_on_axes(manually_reconstructed_image, (28, 28), axes[0])\naxes[0].set_title(f'Manually reconstructed image')\naxes[0].get_xaxis().set_visible(False)\naxes[0].get_yaxis().set_visible(False)\n\nplot_on_axes(X[sample_index], (28, 28), axes[1])\naxes[1].set_title(f'Original sample')\naxes[1].get_xaxis().set_visible(False)\naxes[1].get_yaxis().set_visible(False)\n\nplt.show()","61dd9b63":"y[most_similar]","78eff3e9":"seven_features = nmf_features[np.where(y == 7)[0]]\ntop_four_features_seven = np.mean(seven_features, axis=0).argsort()[-4:][::-1]\n\nfig, axs = plt.subplots(1, 4, figsize=(12, 3.5))\nfig.suptitle('Key components in \"7\"s')\nfor i, feature in enumerate(top_four_features_seven):\n    plot_on_axes(model.components_[feature], (28, 28), axs.flatten()[i])\n    axs.flatten()[i].set_title(f'Component {feature + 1}: {round(np.mean(seven_features, axis=0)[feature], 2)}')\n    axs.flatten()[i].get_xaxis().set_visible(False)\n    axs.flatten()[i].get_yaxis().set_visible(False)\nplt.show() ","d414ce1e":"top_four_seven_components = model.components_[top_four_features_seven]\n\n# Aggregate at pixel level to reconstruct the class\nmean = np.mean(top_four_seven_components, axis=0)\nmedian = np.median(top_four_seven_components, axis=0)\nminimum = np.min(top_four_seven_components, axis=0)\nmaximum = np.max(top_four_seven_components, axis=0)","02e3050d":"fig, axs = plt.subplots(1, 4, figsize=(12, 3.5))\nfig.suptitle('Reconstruction methods using top 4 components')\nmetrics = ['Mean', 'Median', 'Minimum', 'Maximum']\n\nfor i, metric in enumerate(metrics):\n    image_arr = eval(metric.lower())\n    plot_on_axes(image_arr, (28, 28), axs.flatten()[i])\n    axs.flatten()[i].set_title(f'{metric}')\n    axs.flatten()[i].get_xaxis().set_visible(False)\n    axs.flatten()[i].get_yaxis().set_visible(False)\nplt.show()","c7036d53":"model2 = NMF(n_components=45, init='nndsvd', random_state=2021, max_iter=2000)\nmodel2.fit(X)\nnmf_features2 = model2.transform(X)\nshow_components(model2, plot_on_axes, 5, (10, 18))","64cdf42d":"sample_index = 0\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 10))\n\nplot_on_axes(X[sample_index], (28, 28), axes[0])\naxes[0].set_title(f'Original sample')\naxes[0].get_xaxis().set_visible(False)\naxes[0].get_yaxis().set_visible(False)\n\nplot_on_axes(model2.inverse_transform(nmf_features2[sample_index]), (28, 28), axes[1])\naxes[1].set_title(f'Reconstructed image')\naxes[1].get_xaxis().set_visible(False)\naxes[1].get_yaxis().set_visible(False)\n\nplt.show()","09c15f07":"three_features = nmf_features2[np.where(y == 3)[0]]\nfeatures_display = np.mean(three_features, axis=0).argsort()[-10:][::-1]\n\nfig, axs = plt.subplots(2, 5, figsize=(12, 5.5))\nfig.suptitle('Key components in \"3\"s')\nfor i, feature in enumerate(features_display):\n    plot_on_axes(model2.components_[feature], (28, 28), axs.flatten()[i])\n    axs.flatten()[i].set_title(f'Component {feature + 1}: {round(np.mean(three_features, axis=0)[feature], 2)}')\n    axs.flatten()[i].get_xaxis().set_visible(False)\n    axs.flatten()[i].get_yaxis().set_visible(False)\nplt.show() ","da82afc4":"colors = [\"#8e44ad\", \"#2ecc71\", \"#3498db\", \"#f39c12\", \"#2c3e50\", \"#e74c3c\", \"#1abc9c\", \"#f1c40f\", \"#e84393\", \"#63cdda\"]\nplt.figure(figsize=(6, 6))\nplt.axis('off')\n\nhandles = []\n\nfor i, feature in enumerate(features_display[:len(colors)]):\n    # Create custom colourmap from white to color[N]\n    cmap1 = LinearSegmentedColormap.from_list(\"mycmap\", [\"white\", colors[i]])\n    sample = model2.components_[feature]\n    bitmap = sample.reshape((28, 28))\n    patch = mpatches.Patch(color=colors[i], label=f'Component {feature + 1}')\n    handles.append(patch)\n    \n    \n    # Mask those nearer white so that layering them works \n    bitmap = np.ma.masked_where(bitmap <0.01, bitmap)\n    \n    # Plot the sample in image space using custom cmap1\n    plt.imshow(bitmap, cmap=cmap1, interpolation='nearest')\n    \nplt.legend(handles=handles, loc='center', bbox_to_anchor=(1, 0.5, 0.25, 0))\nplt.title(f'Coloured representation of {len(colors)} components')\nplt.show()","cc81843e":"Note, I am using the `init='nndsvd'` here as reading the [documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.NMF.html) suggests that this is better for sparse matrices, of which our samples are.","f2d278b5":"Let's have a look at the top components for the digit 3 now and see whether the 'smaller' components have made a difference. ","fc5f2d5e":"# NMF Implementation - Pattern Detection in Digits\nInstead of using the MNIST data as an example of supervised learning and training a model to predict the digit given an image, I am instead going to use non-negative matrix factorisation (NMF) to extract patterns from the digit images. This is a method of dimensionality reduction so we are looking to turn the 784 pixel variables for the 28 x 28 images into a smaller subset of features that can be used alongside the model's components to maintain information about the original images and 'rebuild' the samples if necessary using matrix dot product. ","198f3bce":"The components of the model represent the patterns that are seen in the images, with their shape being weightings against each of the original variables (pixels in this case); that is why we are able to visualise them in the same shape as the images that we are using via the function `show_as_image` that is defined at the top of this script.   \n\nThe features are the new variables that contain a weighting for the corresponding component. We can, therefore, take an example and find out which components are the most influential. ","eff80305":"We can isolate and check the general importance at a digit-level, rather than a single example. ","b0b9d115":"While these components aren't the clear clustering of digits themselves (as you would expected in a more predictive approach), we can see that they are clearly parts of the digits and represent the common patterns that are seen throughout the data. With 15 components we are getting a mixture of small dashes and sweeping strokes that are more circular i.e. zeros and eights.  \n\nWe might consider increasing the number of components even further to try and break up the components into smaller patterns and smaller strokes.","4f4ab805":"## Examples of digits\nLet's take a look at some of the samples for each digit. ","e134d941":"## Combining top components to reconstruct classes\nI have thought that we can use top components for a given class to reconstruct a generic representation of that class. By combining those top components in such a way that we create an image that shows the major structure of a class by combining the most influential parts.  \n\nWith components illustrating the features of those types of images, it would make sense that there is a way to combine them back together to get an 'average' digit of each type, represented by the most influential components that comprise that digit.  \n\nLet's take 7s again as an example of this idea. ","51f831f4":"# More components, better interpretability? \nRight, time to increase the number of components that the model creates, and see whether that makes a difference to the ease that we can interpret the components visually. ","5c423d50":"While this doesn't necessarily provide actionable insight, it does show some intuition behind the way in which NMF extracts these intepretable components that represent patterns in the original samples. With 7s we can see that the information held in the top 4 components is enough for us to reconstruct a recognisable digit that generalises to the class that we know; this alludes to the effectiveness of the components that we are able to reduce the dimensions and still maintain the ability to build those samples back generally. However, we did see that on an individual basis, with just 15 components, that this wasn't as effective. ","59b2dc36":"As the patterns are now smaller strokes, it is more difficult to see how they might come together to create a digit. \n\nLet's take some components and plot them on top of one another to illustrate the way they represent different parts of the digit. When applied to images this is intuitive as the image 'completes' to represent a general digit class, with numeric data it is the same idea but is perhaps more difficult to visualise; the components represent patterns that are seen, and if you combine them you can use that to classify generally those samples that are similar to one another. ","a602f12a":"As we don't require test and train sets, I'm going to combine the train and test files into one larger structure that we can work with. ","c9d52061":"The components are much 'smaller' now and represent certain strokes and dots in places across the whole image. Combining these in a similar way to before should yield more accurate reconstructions because the inverse transform is creating an image from smaller pieces. ","86bf1a9e":"NMF will create both a set of components (n defined upon model creation) and features for input data. These features will, essentially, act as additional variables; this is where the dimensionality reduction comes in as you can compress the information stored in the 784 pixel columns into `n_components` columns.  \n\nYou can take the dot product between the features and the components and use that information to 'rebuild' the original samples. That is the way in which NMF works as a dimensionality reduction. It is like PCA but the returned components are interpretable to the images and pick out common patterns that are found across the input data.  ","ad267fe1":"Contrast this with the 14th component, that is more influential in \"7\"s than in \"5\"s where it scores much lower. ","ed8116a7":"As you can see above, you can achieve the same reconstruction using the `.inverse_transform()` method on the `model` instance itself. Below you can see that the results are the same when we compare the resultant arrays.","34157633":"## Reconstructing samples using components and features \nFundamentally NMF is performing dimensionality reduction, trying to store the information held in 784 variables in just 15 instead (defined via `n_components`). We have already looked at the components that have been created, but how well do these represent the original data? We can reconstruct the sample images from the features and components of the NMF model and compare them to see!  \n\nWe can reconstruct the image using the features and components by multiplying the matrices together using `np.dot`. Their shapes are compatible and shows how NMF finds the patterns (or components) and then creates an feature matrix that combines with those features to rebuild the original sample. This can also be done using a method on the model itself, `inverse_transform()`; see the [docs](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF.inverse_transform) for more information.","b6b40576":"The overall reconstruction isn't _that_ impressive here, but you begin to understand what is happening and what NMF is achieving here through creative two matrices in far fewer dimensions that the original data set.  \n\nI'm going to score the differences between the reconstructed and original samples and find those examples that score poorly (the difference between the original and reconstructed is high) and those that reconstruct well (differences are minimal). ","2d521be8":"We can see that there are a range of the different digits here under those that are underperforming when the model rebuilds the records with 15 components available. ","eb258489":"I think we can see the rough outline of a '3' here, but it would be good to test this out on all digits in the future. ","99d2d769":"Let's extract the components that, on average, have the highest influence for the digit 5. We can then print those out and get a feel for if they look like patterns that commonly appear when writing fives. ","e2a5d7e3":"Whereas, we can see above that the reconstruction works well with the digit 1, probably due to the simplicity of the original image. It would be interesting to see the performance of the model with additional components and some hyperparameter tuning to improve the resultant inversely transformed records. ","70fba617":"- - - \n\nThanks for taking a look over this pattern detection using NMF notebook. I will keep adding and improving this, and I want to work on some more visualisations to try and bring those components together in a more approachable way.  \n\nPlease leave a comment with any feedback and suggestions, or corrections for that matter, I would love to hear what people think! If you've enjoyed this then please consider **upvoting** this notebook, it would be greatly appreciated. \n","8819c9d9":"Here we can see the general trend differences in which patterns are most important for each digit. Take the first component, for example, this plays a larger part in \"5\"s than in \"7\"s which makes a lot of sense when you look at the shape of that component: "}}