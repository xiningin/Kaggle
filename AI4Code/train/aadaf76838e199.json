{"cell_type":{"ce76b1a1":"code","eb2823a2":"code","946c29d2":"code","ad4c92e7":"code","112c54a0":"code","b923413e":"code","5b4737e4":"code","7c07aa43":"code","58071f64":"code","b33e2f3b":"code","38101bc2":"code","7bb3a851":"code","5a277963":"code","de371d40":"code","6208d1ba":"code","b830d7b6":"code","49ffea19":"code","81ee0558":"code","f0213cd7":"code","8315c6cd":"code","a8a9110d":"code","6396cdaf":"code","6d2bcd47":"code","b910d549":"code","6abeeb3f":"code","16be0486":"code","c7d380f1":"code","a30fe61e":"code","7d47cf82":"code","06f08a4e":"code","12db47b8":"code","0acc1754":"code","74f8d9a5":"code","2132523a":"code","89219134":"code","7a396649":"code","236e7a36":"code","75b64f3d":"markdown","084133ba":"markdown","e4fe0d55":"markdown","55694893":"markdown","05561368":"markdown","680dcda4":"markdown","0b995f1a":"markdown","59efd88e":"markdown","1159db76":"markdown","1efec0da":"markdown","963fb288":"markdown","fbd1afe8":"markdown","92e4bfbb":"markdown","1028a259":"markdown","046c0e22":"markdown","e3076367":"markdown","0dc91806":"markdown","4ecb36d6":"markdown","06923095":"markdown","db4c1375":"markdown","3faadc64":"markdown","c84cf129":"markdown","ceef3c1f":"markdown","ac86862a":"markdown","18f86dbb":"markdown","a79a4bb1":"markdown","61a84eec":"markdown","33b96ab9":"markdown","c7654b94":"markdown"},"source":{"ce76b1a1":" !pip install git+https:\/\/github.com\/fastai\/fastai@2e1ccb58121dc648751e2109fc0fbf6925aa8887 2>\/dev\/null 1>\/dev\/null\n# !apt update && apt install -y libsm6 libxext6","eb2823a2":"from fastai.structured import rf_feat_importance","946c29d2":"from fastai.structured import train_cats,proc_df","ad4c92e7":"#RandomForest\nimport math \nimport pandas as pd\nimport numpy as np \nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n#Xgboost\nimport xgboost as xgb\n#CatBoost \nfrom catboost import CatBoostRegressor","112c54a0":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","b923413e":"Id = test['Id']","5b4737e4":"test_copy = test.copy()","7c07aa43":"test_copy[\"SalePrice\"] = np.nan","58071f64":"train_set_data = [train,test_copy]\ntrain_set_data = pd.concat(train_set_data)","b33e2f3b":"len(train_set_data) == len(train)+len(test)","38101bc2":"train_cats(train_set_data)","7bb3a851":"df, y, nas = proc_df(train_set_data, 'SalePrice',max_n_cat=10)","5a277963":"test_df = df[1460:2919]\ndf = df[0:1460]\ny=y[0:1460]","de371d40":"m = RandomForestRegressor(n_jobs=-1,verbose=0)\nm.fit(df, y)","6208d1ba":"fi = rf_feat_importance(m, df)","b830d7b6":"len(df.columns)","49ffea19":"def plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)","81ee0558":"plot_fi(fi[fi.imp>0.005])","f0213cd7":"df = df[fi[fi.imp>0.0005].cols]","8315c6cd":"def split_vals(a,n): return a[:n].copy(), a[n:].copy()\n\nn_valid = 400  \nn_trn = len(df)-n_valid\nX_train, X_valid = split_vals(df, n_trn)\ny_train, y_valid = split_vals(y, n_trn)\n\nX_train.shape, y_train.shape, X_valid.shape","a8a9110d":"def rmse(x,y): return math.sqrt(((np.log(x)-np.log(y))**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(X_train), y_train), rmse(m.predict(X_valid), y_valid),\n                m.score(X_train, y_train), m.score(X_valid, y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","6396cdaf":"rf_param_grid = {\n                 'max_depth' : [4, 6, 8,12],\n                 'n_estimators': [5,10,20,60,100],\n                 'max_features': ['sqrt', 'auto', 'log2'],\n                 'min_samples_split': [2, 3, 10,20],\n                 'min_samples_leaf': [1, 3, 10,18,25],\n                 'bootstrap': [True, False],\n                 }","6d2bcd47":"m = RandomForestRegressor()","b910d549":"m_r = RandomizedSearchCV(param_distributions=rf_param_grid, \n                                    estimator = m,  \n                                    verbose = 0, n_iter = 50, cv = 4)","6abeeb3f":"m_r.fit(X_train, y_train)","16be0486":"print_score(m_r)","c7d380f1":"xgb_classifier = xgb.XGBRegressor()\n","a30fe61e":"gbm_param_grid = {\n    'n_estimators': range(1,100),\n    'max_depth': range(1, 15),\n    'learning_rate': [.1,.13, .16, .19,.3,.6],\n    'colsample_bytree': [.6, .7, .8, .9, 1]\n}","7d47cf82":"xgb_random = RandomizedSearchCV(param_distributions=gbm_param_grid, \n                                    estimator = xgb_classifier, \n                                    verbose = 0, n_iter = 50, cv = 4)","06f08a4e":"xgb_random.fit(X_train,y_train)","12db47b8":"print_score(xgb_random)","0acc1754":"m_c = CatBoostRegressor(iterations=2000,learning_rate=0.1,depth=3,loss_function='RMSE',l2_leaf_reg=4,border_count=15,verbose=False)","74f8d9a5":"m_c.fit(X_train,y_train)","2132523a":"print_score(m_c)","89219134":"test_df = test_df[list(X_train.columns)]","7a396649":"y_pred = (m_c.predict(test_df) + m_r.predict(test_df)+xgb_random.predict(test_df))\/3","236e7a36":"submission = pd.DataFrame({\"Id\": Id,\"SalePrice\": y_pred})\nsubmission.to_csv('submission.csv', index=False)","75b64f3d":"### <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\">RandomForest<\/a>","084133ba":"### <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/\">Xgboost<\/a>","e4fe0d55":"### <a href=\"https:\/\/tech.yandex.com\/catboost\/doc\/dg\/concepts\/python-installation-docpage\/\">CatBoost<\/a>","55694893":"### For further parameter engineering they is a great python library \"<a href=\"https:\/\/github.com\/pandas-profiling\/pandas-profiling\">Pandas profiling<\/a>\" that Generates profile reports from a pandas DataFrame.","05561368":"#### Saving the Id column for later use ","680dcda4":"####  resplite the training and test set","0b995f1a":"#### optimizing hyperparameters of a random forest with the grid search ","59efd88e":"#### concatenate the saved Id with the predicted values to create a csv file for submussion ","1159db76":"![](submission.PNG)","1efec0da":"#### Import ML models ","963fb288":"<h3>Introduction<\/h3><br>\n<b>Hello,<\/b><br>\nIn this kernel I will be showing you the fastest way to make your first submission to a Kaggle competition I will be using the data from the <b>'House prices: Advanced Regression Techniques'<\/b> competition.<br> \nFor this competition, we are predicting the sale price of property. The data is splited into two parts Training and testing sets both contains 1470 observations and 80 features.<br>\n<b style=\"color:red\">This approach with no feature engineering does ok on leaderboard <\/b>\n<br><br><br>\n\nI would like to recommend some kernels and courses that helped me begin my journey on Kaggle:\n<ul>\n  <li>Machine learning <a href=\"https:\/\/course.fast.ai\/ml.html\">Fastai<\/a> it\u2019s free and available on YouTube<\/li>\n  <li>For advance feature engineering and parameter tuning those 2 kernels are well detailed \n      <a href=\"https:\/\/www.kaggle.com\/josh24990\/simple-stacking-approach-top-12-score\/notebook\">Simple stacking approach<\/a>,\n      <a href=\"https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\">Stacked Regressions<\/a>\n<\/li>\n<\/ul>  \n\n","fbd1afe8":"#### creating a function that evaluate the algorithmes performance <a href=\"https:\/\/en.wikipedia.org\/wiki\/Out-of-bag_error\">Learn more about OOB score<\/a> ","92e4bfbb":"#### proc_df will replace categories with their numeric codes, handle missing continuous values, and split the dependent variable into a separate variable for the max_n_cat is to create dummy variables for the categorical column with less or equal to 10 categories ","1028a259":"##### The below parameters come from <a href=\"https:\/\/www.kaggle.com\/josh24990\/simple-stacking-approach-top-12-score\/notebook\">this kernel <\/a>, we can optimizing hyperparameter but it will take a long time a lot of processing power (<a href=\"https:\/\/tech.yandex.com\/catboost\/doc\/dg\/concepts\/parameter-tuning-docpage\/\">catBoost parameter tuning<\/a>)","046c0e22":"#### get only the column used on the training set to predict on the test set ","e3076367":"#### combinion the traning and testing dataset to maintain consistency between the sets","0dc91806":"#### optimizing hyperparameters of a random forest with the grid search ","4ecb36d6":"<h4><a href=\"https:\/\/github.com\/fastai\/fastai\">Fastai download Installation guide\/documentation<\/a><\/h4>","06923095":"### combining the 3 models to predict on the test set  ","db4c1375":"#### rf_feat_importance that uses the feature_importances_ attribute from the RandomForestRegressor to return a dataframe with the columns and their importance in descending order.","3faadc64":"#### Splite the data to training set and a validation set ","c84cf129":"### Thank you for reading ( \u0361\u1d54 \u035c\u0296 \u0361\u1d54 )","ceef3c1f":"#### fitting the model to the training set ","ac86862a":"#### creating a plot of the most relevant features ","18f86dbb":"### Reading the data","a79a4bb1":"#### train_cats is a function in the fastai library that  convert strings to pandas categories","61a84eec":"#### Train a quick randomForest Resressor to check the feature importance ","33b96ab9":"## Imports","c7654b94":"#### keep only the column that have a acceptable information gain "}}