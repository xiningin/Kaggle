{"cell_type":{"a4856c4b":"code","badd4bc6":"code","486402f0":"code","ff368bc5":"code","d55a64b8":"code","7ec6a7f5":"code","9d7157b0":"code","2f7a2843":"code","8bddea6a":"code","c400322a":"code","22e630e6":"code","756427e6":"code","e9c87094":"code","7df87f5e":"code","8c2e4221":"code","4d33d921":"code","0971099b":"code","773da496":"code","821044bb":"code","12ce688e":"code","0aaa4e7d":"code","54a602be":"code","790965a1":"code","bdbf9472":"code","fad3a7ee":"code","c9742b5a":"code","261019db":"code","ddba0770":"code","04009eaf":"code","01ee58b2":"code","61d153ea":"code","d9cc8357":"markdown","3ddb7a08":"markdown","588f26a0":"markdown","4644d969":"markdown","c0f8298d":"markdown","afb853c5":"markdown"},"source":{"a4856c4b":"import numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom pandas import Series\nimport matplotlib.pyplot as plt\nimport warnings\nimport math\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('fivethirtyeight')\n\nimport os\nprint(os.listdir(\"..\/input\"))\n","badd4bc6":"# load the dataset\ndataframe = pd.read_excel('..\/input\/otanchatram\/madurai veg final merged (1).xlsx')\ndataframe =dataframe.T\nprint(dataframe.shape)\n\n#Naming the index\ndataframe.index.name = 'Date'\nprint(dataframe.head())\n\n#backing up orginal file\noriginal = dataframe\n\n#Index in DateTimeFormat\ndataframe.index","486402f0":"#IndexDate is changed to column variable\ndataframe.reset_index(level=0, inplace=True)","ff368bc5":"#Extract Features from Time\ndataframe['Date'] = pd.to_datetime(dataframe.Date, format = '%Y-%m-%d')","d55a64b8":"#Checking missing values\nprint (dataframe.isnull().sum())\nprint (dataframe.columns)\n\n#Forward filling the missing values.Assuming that the price of the product will be mostly nearer to the previous day price\ndataframe.fillna(method='ffill',inplace=True)\nprint(dataframe.isnull().sum())\n\n#Cleaning the values\n#removing the empty column\ndataframe.drop(dataframe.columns[-1],axis=1,inplace=True)\ndataframe['Button Mushrooms \/ Button Kaalan'] = dataframe['Button Mushrooms \/ Button Kaalan'].map({'100-140':'100'})\nprint(dataframe.head())","7ec6a7f5":"# Data Preprocessing for TS\nTS = original\nTS.fillna(method='ffill',inplace=True)\nprint(TS.isnull().sum())\n\n#removing the last empty column\nTS.drop(TS.columns[-1],axis=1,inplace=True)\nTS['Button Mushrooms \/ Button Kaalan'] = TS['Button Mushrooms \/ Button Kaalan'].map({'100-140':'100'})\nprint(TS.head())","9d7157b0":"#Tomato\ndf = dataframe[['Tomotto ottu \/ Thakkali Ottu']]\nTS = TS[['Tomotto ottu \/ Thakkali Ottu']]","2f7a2843":"plt.figure(figsize = (16,8))\nplt.plot(df)\nplt.title(\"Time Series - Tomato price 2015\")\nplt.xlabel(\"Days\")\nplt.ylabel(\"Price\")\nplt.legend(loc = 'best')","8bddea6a":"dataframe['Day'] = dataframe['Date'].dt.day\ndataframe['Month'] = dataframe['Date'].dt.month\n\ndataframe.groupby('Day')['Tomotto ottu \/ Thakkali Ottu'].mean().plot.bar()","c400322a":"dataframe.groupby('Month')['Tomotto ottu \/ Thakkali Ottu'].mean().plot.bar()","22e630e6":"temp = dataframe.groupby(['Month','Day'])['Tomotto ottu \/ Thakkali Ottu'].mean()\ntemp.plot(figsize =(15,5), title = \"Tomato\", fontsize = 14)","756427e6":"#Summary Statistics\nX = TS.iloc[:,0].values\nX1, X2 = X[0:104], X[104:]\nmean1, mean2 = X1.mean(), X2.mean()\nvar1, var2 = X1.var(), X2.var()\nprint('mean1=%f, mean2=%f' % (mean1, mean2))\nprint('variance1=%f, variance2=%f' % (var1, var2))","e9c87094":"#T-test\nfrom scipy import stats\n\nN = 104\na = X1\nb = X2\n\n## Calculate the Standard Deviation\n#Calculate the variance to get the standard deviation\n#For unbiased max likelihood estimate we have to divide the var by N-1, and therefore the parameter ddof = 1\nvar_a = a.var(ddof=1)\nvar_b = b.var(ddof=1)\n\n#std deviation\ns = np.sqrt((var_a + var_b)\/2)\n\n# Calculate the t-statistics\nt = (a.mean() - b.mean())\/(s*np.sqrt(2\/N))\n\n## Compare with the critical t-value\n#Degrees of freedom\ndf = 2*N - 2\n\n#p-value after comparison with the t \np = 1 - stats.t.cdf(t,df=df)\n\nprint(\"t = \" + str(t))\nprint(\"p = \" + str(2*p))\n\n# Cross Checking with the internal scipy function\nt2, p2 = stats.ttest_ind(a,b)\nprint(\"t = \" + str(t2))\nprint(\"p = \" + str(p2))","7df87f5e":"from statsmodels.tsa.stattools import adfuller\ndef test_stationarity(timeseries):\n    \n    #Determing rolling statistics\n    rolmean = timeseries.rolling(window=8,center=False).mean()\n    rolstd = timeseries.rolling(window=8,center=False).std()\n\n    #Plot rolling statistics:\n    orig = plt.plot(timeseries, color='blue',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n    \n    print ('Results of Dickey-Fuller Test:')\n    #adfuller() function accepts only 1d array of time series so first convert it using:\n    dk = timeseries.iloc[:,0].values\n    dftest = adfuller(dk, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print (dfoutput)","8c2e4221":"test_stationarity(TS)","4d33d921":"#Moving average\nmoving_avg = TS.rolling(window=8,center=False).mean()\nplt.plot(TS)\nplt.plot(moving_avg, color='red')","0971099b":"#Note that since we are taking average of last 8 values, rolling mean is not defined for first 7 values\nts_moving_avg_diff = TS - moving_avg\nts_moving_avg_diff.dropna(inplace=True)\ntest_stationarity(ts_moving_avg_diff)","773da496":"#Expotential weigghted moving average\nexpwighted_avg = TS.ewm(span=8,adjust=False).mean()\nplt.plot(TS)\nplt.plot(expwighted_avg, color='red')","821044bb":"ts_ewma_diff = TS - expwighted_avg\ntest_stationarity(ts_ewma_diff)","12ce688e":"#Differencing\n#we take the difference of the observation at a particular instant with that at the previous instant\nts_diff = TS - TS.shift()\nplt.plot(ts_diff)","0aaa4e7d":"ts_diff.dropna(inplace=True)\ntest_stationarity(ts_diff)","54a602be":"#Multiplicative Decomposition\n\nfrom pandas import Series\nfrom matplotlib import pyplot\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nresult = seasonal_decompose(TS['Tomotto ottu \/ Thakkali Ottu'], model='multiplicative', freq=10)\ntrend = result.trend\nseasonal = result.seasonal\nresidual = result.resid\nresult.plot()\npyplot.show()","790965a1":"ts_decompose = residual.to_frame()\nts_decompose = ts_decompose.dropna()","bdbf9472":"test_stationarity(ts_decompose)","fad3a7ee":"#ARIMA\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error\nmodel = ARIMA(TS, order=(10, 1, 1))  \nresults_AR = model.fit(disp=-2)\nplt.plot(ts_diff)\nplt.plot(results_AR.fittedvalues, color='red')","c9742b5a":"results_AR = results_AR.fittedvalues.to_frame(name='Tomotto ottu \/ Thakkali Ottu')\ntest_stationarity(results_AR)","261019db":"X = TS.values\nsize = int(len(X) * 0.66)\ntrain, test = X[0:size], X[size:len(X)]\nhistory = [x for x in train]\npredictions = list()\nfor t in range(len(test)):\n    model = ARIMA(history, order=(10,1,0))\n    model_fit = model.fit(disp=0)\n    output = model_fit.forecast()\n    yhat = output[0]\n    predictions.append(yhat)\n    obs = test[t]\n    history.append(obs)\n    print('predicted=%f, expected=%f' % (yhat, obs))\nerror = mean_squared_error(test, predictions)\nprint('Test MSE: %.3f' % error)\n# plot\npyplot.plot(test)\npyplot.plot(predictions, color='red')\npyplot.show()","ddba0770":"# normalize the dataset\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0, 1))\nTS_log = scaler.fit_transform(TS)\n","04009eaf":"# split into train and test sets\ntrain_size = int(len(TS) * 0.67)\ntest_size = len(TS) - train_size\ntrain, test = TS.iloc[0:train_size,:], TS.iloc[train_size:len(TS),:]\nprint(len(train), len(test))","01ee58b2":"import numpy\nimport matplotlib.pyplot as plt\nimport math\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n# convert an array of values into a dataset matrix\ndef create_dataset(dataset, look_back=1):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), 0]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, 0])\n    return numpy.array(dataX), numpy.array(dataY)\n# fix random seed for reproducibility\nnumpy.random.seed(7)\n# load the dataset\ndataset = TS\ndataset = dataset.astype('float32')\n# normalize the dataset\nscaler = MinMaxScaler(feature_range=(0, 1))\ndataset = scaler.fit_transform(dataset)\n# split into train and test sets\ntrain_size = int(len(dataset) * 0.67)\ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n# reshape into X=t and Y=t+1\nlook_back = 1\ntrainX, trainY = create_dataset(train, look_back)\ntestX, testY = create_dataset(test, look_back)\n# reshape input to be [samples, time steps, features]\ntrainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n\n# create and fit the LSTM network\nmodel = Sequential()\nmodel.add(LSTM(4, input_shape=(1, look_back)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(trainX, trainY, epochs=1, batch_size=1, verbose=2)\n# make predictions\ntrainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\n# invert predictions\ntrainPredict = scaler.inverse_transform(trainPredict)\ntrainY = scaler.inverse_transform([trainY])\ntestPredict = scaler.inverse_transform(testPredict)\ntestY = scaler.inverse_transform([testY])\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))\n","61d153ea":"# shift train predictions for plotting\ntrainPredictPlot = numpy.empty_like(dataset)\ntrainPredictPlot[:, :] = numpy.nan\ntrainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n# shift test predictions for plotting\ntestPredictPlot = numpy.empty_like(dataset)\ntestPredictPlot[:, :] = numpy.nan\ntestPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n# plot baseline and predictions\nplt.plot(scaler.inverse_transform(dataset))\nplt.plot(trainPredictPlot)\nplt.plot(testPredictPlot)\nplt.show()","d9cc8357":"We can split our time series into two (or more) partitions and compare the mean and variance of each group.\nIf they differ and the difference is statistically significant, the time series is likely non-stationary.","3ddb7a08":"*** Expotential weighted moving average***\n\nNote that here the parameter \u2018halflife\u2019 is used to define the amount of exponential decay. This is just an assumption here and would depend largely on the business domain.\n\n**Span** corresponds to what is commonly called an \u201cN-day EW moving average\u201d.Center of mass has a more physical interpretation and can be thought of in terms of span: \\(c = (s - 1) \/ 2\\).\n\n**Half-life** is the period of time for the exponential weight to reduce to one half.\n**Alpha** specifies the smoothing factor directly.","588f26a0":"If the calculated t-statistic is greater than the critical t-value, the test concludes that there is a statistically significant difference between the two populations.\nTherefore,here we'll accept the null hypothesis that there is statistically significant difference between the two populations.\n","4644d969":"Hi this is my 1st personal project. After so many years of interest, now I was able to take my happy first step. I know there is alot of scope to work on this project (like trying FB Prophet) and i'll be more happy to hear it.\n\n> **Im open to all comments and suggestions.**\n\n\nData Used - 308 price points in Madurai Wholesale Vegetable Market 2015\nTrain data - 206, Test Data - 102\nForecasted for only one variable - Tomato\n\nSteps:\n- Pre-processing the data (removing outliers, filling missing values, Converting into Date format)\n- Visualising the data\n- Checking stationarity\n- Used Moving Avg, Exponential Moving Avg, Multiplicative Decomposition\n- ARIMA and RNN (attained lowest MSE in RNN)\n\nThanks to Analytics Vidhya and machinelearningmastery.","c0f8298d":" ** Perform Dickey-Fuller test:** is used to check the stationarity\n  H0:If failed to be rejected, it suggests the time series has a unit root, meaning it is non-stationary. It has some time dependent structure.\n  H1:The null hypothesis is rejected; it suggests the time series does not have a unit root, meaning it is stationary. It does not have time-dependent structure.\n  p-value > 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.\n  p-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.","afb853c5":"**AR: Autoregression.** A model that uses the dependent relationship between an observation and some number of lagged observations.\nI: Integrated. The use of differencing of raw observations (e.g. subtracting an observation from an observation at the previous time step) in order to make the time series stationary.\n\n**MA:** Moving Average. A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations.\n\n**ARIMA(p,d,q):**\np: The number of lag observations included in the model, also called the lag order.\nd: The number of times that the raw observations are differenced, also called the degree of differencing.\nq: The size of the moving average window, also called the order of moving average."}}