{"cell_type":{"e414ec1a":"code","95d1465e":"code","9f0176da":"code","53870f7f":"code","f6b812e6":"code","a54c62d0":"code","fb6c9d0f":"code","c76f1b41":"code","421ec200":"code","030e6a53":"code","6acbc332":"code","3c6dc11b":"code","28e90f3c":"code","591410e1":"code","3a783718":"code","fe40c1cf":"code","9b5dafc8":"code","841d847e":"code","42554380":"code","7b31ce81":"code","af3492fc":"code","a5b4ac1d":"code","338d9dcc":"code","c18de7e8":"markdown","140bf3e3":"markdown","1b919920":"markdown","19c13521":"markdown","e2d48b33":"markdown","b3fc1e76":"markdown","ad01d330":"markdown","6eed8124":"markdown","8e19abf6":"markdown","361eb3bd":"markdown","e73a1788":"markdown","78ca8aff":"markdown","540e6aec":"markdown","209ea1c2":"markdown","d7f8a350":"markdown","92b69652":"markdown","e6081b50":"markdown","cb5db98d":"markdown","4bab82c0":"markdown","492b5e95":"markdown","a9fb75c6":"markdown","e87742ec":"markdown","db820a72":"markdown","c8e501e8":"markdown","86a83c2d":"markdown","f877da1b":"markdown","33c9b7f0":"markdown","d9f64b39":"markdown","dd26f09b":"markdown","f32afbb7":"markdown","0c7f9002":"markdown","f54d9f18":"markdown","390d8fa3":"markdown","01a3e762":"markdown","27c4e0ef":"markdown","b4b7bed7":"markdown"},"source":{"e414ec1a":"import numpy as np \nimport pandas as pd\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nimport tensorflow as tf\nfrom keras import preprocessing\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D,Dropout,Dense,Flatten,Conv2DTranspose,BatchNormalization,LeakyReLU,Reshape\n","95d1465e":"path_celeb = []\ntrain_path_celeb = \"\/kaggle\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba\/\"\nfor path in os.listdir(train_path_celeb):\n    if '.jpg' in path:\n        path_celeb.append(os.path.join(train_path_celeb, path))","9f0176da":"select_path=path_celeb[0:50000]  # \u9009\u53d6\u56fe\u50cf","53870f7f":"%%time\ncrop = (30, 55, 150, 175) #\u56fe\u50cf\u88c1\u526a\uff0c\u4f7f\u5f97\u9762\u90e8\u5c45\u4e8e\u56fe\u50cf\u4e2d\u5fc3\n# \u76ee\u6807\u56fe\u50cf\u5927\u5c0f64*64\nimages = [np.array((Image.open(path).crop(crop)).resize((64,64))) for path in select_path]\nfor i in range(len(images)):\n    images[i] = ((images[i] - images[i].min())\/(255 - images[i].min())) # \u5f52\u4e00\u5316\n    \nimages = np.array(images) ","f6b812e6":"X_train=images  # \u6837\u672c\u7279\u5f81\u96c6","a54c62d0":"X_train.shape","fb6c9d0f":"# \u4ece\u8bad\u7ec3\u96c6\u62bd\u53d620\u5e45\u4eba\u8138\u56fe\u50cf\uff0c\u89c2\u5bdf\nplt.figure(figsize=(10,15))\nfig,ax=plt.subplots(3,5)\nfig.suptitle(\"Real Images\")\nindex = 500\n\nfor i in range(3):\n    for j in range(5):\n            ax[i,j].imshow(X_train[index].reshape(64,64,3))           \n            index += 500            \nplt.tight_layout()\nplt.show()","c76f1b41":"noise_shape = 100","421ec200":"generator=Sequential()\ngenerator.add(Dense(4*4*512,input_shape=[noise_shape]))\ngenerator.add(Reshape([4,4,512]))\ngenerator.add(Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"))\ngenerator.add(LeakyReLU(alpha=0.2))\ngenerator.add(BatchNormalization())\ngenerator.add(Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"))\ngenerator.add(LeakyReLU(alpha=0.2))\ngenerator.add(BatchNormalization())\ngenerator.add(Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\"))\ngenerator.add(LeakyReLU(alpha=0.2))\ngenerator.add(BatchNormalization())\ngenerator.add(Conv2DTranspose(3, kernel_size=4, strides=2, padding=\"same\",\n                                 activation='sigmoid'))","030e6a53":"# \u505a\u5b8c\u4e86\u6a21\u578b\uff0c\u770b\u770b\u7ed3\u6784\u662f\u5fc5\u8981\u7684\ngenerator.summary()","6acbc332":"discriminator=Sequential()\ndiscriminator.add(Conv2D(32, kernel_size=4, strides=2, padding=\"same\",input_shape=[64,64, 3]))\ndiscriminator.add(Conv2D(64, kernel_size=4, strides=2, padding=\"same\"))\ndiscriminator.add(LeakyReLU(0.2)) \ndiscriminator.add(BatchNormalization())\ndiscriminator.add(Conv2D(128, kernel_size=4, strides=2, padding=\"same\"))\ndiscriminator.add(LeakyReLU(0.2))\ndiscriminator.add(BatchNormalization())\ndiscriminator.add(Conv2D(256, kernel_size=4, strides=2, padding=\"same\"))\ndiscriminator.add(LeakyReLU(0.2))\ndiscriminator.add(Flatten())\ndiscriminator.add(Dropout(0.5))\ndiscriminator.add(Dense(1,activation='sigmoid'))","3c6dc11b":"discriminator.summary()","28e90f3c":"DCGAN =Sequential([generator,discriminator])","591410e1":"# \u5148\u5b9a\u4e49\u5224\u522b\u5668\u7684\u4f18\u5316\u65b9\u6cd5\u548c\u635f\u5931\u51fd\u6570\ndiscriminator.compile(optimizer='adam',loss='binary_crossentropy')\ndiscriminator.trainable = False   # \u6839\u636e\u9700\u8981\u5f00\u5173","3a783718":"DCGAN.compile(optimizer='adam',loss='binary_crossentropy')","fe40c1cf":"DCGAN.summary()","9b5dafc8":"epochs = 300  \nbatch_size = 128\nD_loss=[] #\u5224\u522b\u5668\u635f\u5931\u8bb0\u5f55\nG_loss=[] #\u751f\u6210\u5668\u635f\u5931\u8bb0\u5f55","841d847e":"with tf.device('\/gpu:0'):\n    for epoch in range(epochs):   # epoch\u5faa\u73af\n        print(f\"\u5f53\u524d\u8fdb\u884c\u5230Epoch\uff1a{epoch+1}\")\n\n        # batch \u5faa\u73af\n        for i in range(X_train.shape[0]\/\/batch_size):\n\n            if (i)%100 == 0:\n                print(f\"\\t\u8fdb\u884c\u5230batch\uff1a {i} of {len(X_train)\/\/batch_size}\")\n            # \u6839\u636ebatch_size\u548c\u566a\u58f0\u5411\u91cf\u7ef4\u5ea6\uff0c\u4e3a\u751f\u6210\u5668\u51c6\u5907\u5747\u5300\u5206\u5e03\u7684\u566a\u58f0\u77e9\u9635\n            noise=np.random.uniform(-1,1,size=[batch_size,noise_shape])\n            # \u751f\u6210\u5668\u6279\u91cf\u751f\u6210\u4f2a\u9020\u56fe\u50cf\n            gen_image = generator.predict_on_batch(noise)  \n            # \u4ece\u8bad\u7ec3\u96c6\u4e2d\u6309\u7167batch_size\u6279\u91cf\u53d6\u6837\u672c\n            train_dataset = X_train[i*batch_size:(i+1)*batch_size]\n            # \u771f\u5b9e\u6837\u672c\u7684\u6807\u7b7e\n            train_label=np.ones(shape=(batch_size,1))  # \u771f\u5b9e\u6837\u672c\u6807\u7b7e\u4e3a1\n            discriminator.trainable = True  # \u5224\u522b\u5668\u53ef\u8bad\u7ec3\n            # \u5224\u522b\u5668\u57fa\u4e8e\u771f\u5b9e\u6837\u672c\u8bad\u7ec3\uff0c\u8fd4\u56de\u771f\u5b9e\u6837\u672c\u635f\u5931\n            d_loss1 = discriminator.train_on_batch(train_dataset,train_label)\n\n            # \u5224\u522b\u5668\u5728\u4f2a\u9020\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u8fd4\u56de\u4f2a\u9020\u6837\u672c\u7684\u635f\u5931\n            train_label=np.zeros(shape=(batch_size,1))  # \u4f2a\u9020\u6837\u672c\u6807\u7b7e\u4e3a0\n            d_loss2 = discriminator.train_on_batch(gen_image,train_label)\n\n            D_loss.append(d_loss1+d_loss2)    # \u8fd9\u662f\u4e00\u4e2a\u6279\u6b21\u8bad\u7ec3\u540e\uff0c\u5224\u522b\u5668\u7684\u603b\u635f\u5931\n            \n            # \u6839\u636ebatch_size\u548c\u566a\u58f0\u5411\u91cf\u7ef4\u5ea6\uff0c\u4e3a\u751f\u6210\u5668\u51c6\u5907\u5747\u5300\u5206\u5e03\u7684\u566a\u58f0\u77e9\u9635\n            noise=np.random.uniform(-1,1,size=[batch_size,noise_shape])\n            train_label=np.ones(shape=(batch_size,1))  # \u6b64\u65f6\u8981\u8bbe\u7f6e\u4f2a\u9020\u56fe\u7247\u7684\u6807\u7b7e\u4e3a1\n            discriminator.trainable = False  # \u6b64\u65f6\u8981\u8bbe\u4e3aFalse\uff0c\u5224\u522b\u5668\u505c\u6b62\u8bad\u7ec3\n            \n            # \u8bad\u7ec3\u751f\u6210\u5668\uff0c\u8fd4\u56de\u751f\u6210\u5668\u635f\u5931\n            g_loss = DCGAN.train_on_batch(noise, train_label)\n            \n            G_loss.append(g_loss)  # \u8fd9\u662f\u4e00\u4e2a\u6279\u6b21\u8bad\u7ec3\u540e\uff0c\u751f\u6210\u5668\u7684\u603b\u635f\u5931\n\n\n        if epoch % 5 == 0:  # \u6bcf\u9694\u4e94\u4ee3\uff0c\u89c2\u5bdf\u751f\u6210\u5668\u7684\u6548\u679c\n            samples = 10\n            x_fake = generator.predict(np.random.normal(loc=0, scale=1, size=(samples,100)))\n\n            for k in range(samples):\n                plt.subplot(2, 5, k+1)  # 2\u884c\u4e94\u5217\n                plt.imshow(x_fake[k].reshape(64,64,3))\n                plt.xticks([])\n                plt.yticks([])\n\n\n            plt.tight_layout()\n            plt.show()\n        print('Epoch: %d,  Loss: D_real = %.3f, D_fake = %.3f,  G = %.3f' %   (epoch+1, d_loss1, d_loss2, g_loss))        \nprint('\u8bad\u7ec3\u5b8c\u6210\uff01')","42554380":"for i in range(5):\n    plt.figure(figsize=(7,7))   \n    for k in range(20):\n            noise=np.random.uniform(-1,1,size=[100,noise_shape])\n            new_img=generator.predict(noise)   # \u6a21\u578b\u751f\u6210100\u5e45\u65b0\u56fe\u50cf\n            plt.subplot(5, 4, k+1)\n            plt.imshow(new_img[k].reshape(64,64,3))  # \u53ea\u663e\u793a\u524d20\u5e45\n            plt.xticks([])\n            plt.yticks([])\n \n    plt.tight_layout()\n    plt.show()","7b31ce81":"plt.figure(figsize=(10,10))\nplt.plot(G_loss,color='red',label='Generator_loss')  # \u751f\u6210\u5668\u7684\u635f\u5931\nplt.plot(D_loss,color='blue',label='Discriminator_loss')  # \u5224\u522b\u5668\u7684\u635f\u5931\nplt.legend()\nplt.xlabel('total batches')\nplt.ylabel('loss')\nplt.title('Model loss per batch')\nplt.show()","af3492fc":"model_path ='DCGAN'\nDCGAN.save(model_path)","a5b4ac1d":"model = tf.keras.models.load_model('DCGAN')","338d9dcc":"noise=np.random.uniform(-1,1,size=[1,noise_shape])\nnew_img=generator.predict(noise)   # \u6a21\u578b\u751f\u62101\u5e45\u65b0\u56fe\u50cf\nplt.imshow(new_img[0].reshape(64,64,3))\nplt.xticks([])\nplt.yticks([])","c18de7e8":"## 3 \u4e16\u754c\u540d\u4eba\u6570\u636e\u96c6\u9884\u5904\u7406","140bf3e3":"### 4.2 \u5224\u522b\u5668","1b919920":"\u30101\u3011\u751f\u6210\u5668\u7528tanh\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff0c\u7279\u5f81\u503c\u6620\u5c04\u5230[-1,1]\u533a\u95f4\uff0c\u672c\u5b9e\u9a8c\u7528sigmoid\u66ff\u6362<br\/>\n\u30102\u3011\u6743\u91cd\u521d\u59cb\u4e3a\u5747\u503c\u4e3a0\uff0c\u6807\u51c6\u5dee\u4e3a0.02\u7684\u6b63\u6001\u5206\u5e03<br\/>\n\u30103\u3011LeakyReLU\u7684\u659c\u7387\u8bbe\u7f6e\u4e3a0.2<br\/>\n\u30104\u3011\u91c7\u7528Adam\u4f18\u5316\u7b97\u6cd5\uff08\u751f\u6210\u5668\u548c\u5224\u522b\u5668\uff09<br\/>\n\u30105\u3011\u5b66\u4e60\u7387\u8bbe\u7f6e\u4e3a2e-4<br\/>\n\u30106\u3011momentum\u7684beta1\u8bbe\u7f6e\u4e3a0.5<br\/>\n<font color='red'>\u5f53\u7136\uff0c\u8fd8\u6709\u7ed3\u6784\u7684\u53d8\u5316\uff0c\u7528\u5377\u79ef\u6784\u5efa\u7f51\u7edc<\/font><br\/>\n#### \u751f\u6210\u5668\u7f51\u7edc\u7ed3\u6784\u5982\u56fe2\u6240\u793a\u3002","19c13521":"\u518d\u5b9a\u4e49DCGAN\u7684\u4f18\u5316\u65b9\u6cd5\u548c\u635f\u5931\u51fd\u6570<br\/>\n\u4e8b\u5b9e\u4e0a\u5c31\u662f\u5b9a\u4e49\u751f\u6210\u5668\u7684\u4f18\u5316\u65b9\u6cd5\u548c\u635f\u5931\u51fd\u6570\uff0c\u5e76\u628a\u751f\u6210\u5668\u548c\u5224\u522b\u5668\u5173\u8054\u8d77\u6765","e2d48b33":"#### \u56fe3\u663e\u793a\u4e86\u751f\u6210\u5668\u4e0e\u5224\u522b\u5668\u7684\u7ed3\u6784\u5bf9\u6bd4\uff0c\u751f\u6210\u5668\u4e0e\u5224\u522b\u5668\u5747\u4e3a\u5377\u79ef\u7f51\u7edc\uff0c\u4f46\u662f\u5377\u79ef\u8fc7\u7a0b\u4e0d\u540c\uff0c\u529f\u80fd\u4e0d\u540c\u3002","b3fc1e76":"#### \u6837\u672c\u96c6\u89c2\u5bdf","ad01d330":"![image.png](attachment:c5b928ec-ad4d-42b2-9ec9-6e88911dd95f.png)\n#### \u56fe3 \u751f\u6210\u5668\u4e0e\u5224\u522b\u5668\u7ed3\u6784\u5bf9\u6bd4","6eed8124":"### 4.1 \u751f\u6210\u5668","8e19abf6":"# \u57fa\u4e8e DCGAN \u751f\u6210\u5047\u7684\u4eba\u8138\u56fe\u50cf","361eb3bd":"\u6570\u636e\u96c6\u5305\u542b20\u591a\u4e07\u5e45\u4eba\u8138\u56fe\u50cf\uff0c\u4e3a\u4e86\u7f29\u51cf\u8ba1\u7b97\u89c4\u6a21\uff0c\u53ef\u4ee5\u6839\u636e\u9700\u8981\u9009\u62e9\u56fe\u50cf\u7528\u4e8e\u6a21\u578b\u8bad\u7ec3","e73a1788":"\u751f\u6210100\u5e45\u4eba\u8138\u56fe\u50cf","78ca8aff":"# **GAN\u7684\u635f\u5931\u51fd\u6570**\n\n\u751f\u6210\u5668\u5e0c\u671b\u4e0b\u9762\u7684\u635f\u5931\u51fd\u6570\u8d8a\u5c0f\u8d8a\u597d\uff0c\u5224\u522b\u5668\u5e0c\u671b\u4e0b\u9762\u7684\u635f\u5931\u51fd\u6570\u8d8a\u5927\u8d8a\u597d\n\n![image.png](attachment:d22b9e04-2dc7-451a-a99f-be2dbbc4e385.png)\n\nD(x) \uff1a\u5224\u522b\u5668\u5224\u65ad\u771f\u5b9e\u7684x\u662f\u771f\u5b9e\u56fe\u50cf\u7684\u6982\u7387\u503c\n\nEx \uff1a\u6240\u6709\u771f\u5b9e\u56fe\u50cf\u7684\u671f\u671b\u4f30\u8ba1\u503c\n\nG(z) \uff1a\u751f\u6210\u5668\u5bf9\u566a\u58f0\u56fe\u50cf z \u751f\u6210\u7684\u65b0\u56fe\u50cf\n\nD(G(z)) \uff1a\u5224\u522b\u5668\u4f30\u8ba1\u751f\u6210\u7684\u56fe\u50cfG(z)\u662f\u771f\u5b9e\u56fe\u50cf\u7684\u6982\u7387\n\nEz \uff1a\u6240\u6709\u8f93\u5165\u5230\u751f\u6210\u5668\u4e2d\u7684 z \u7684\u671f\u671b\u4f30\u8ba1\u503c (\u5373\u6240\u6709\u751f\u6210\u7684\u56fe\u50cf G(z) \u7684\u671f\u671b\u4f30\u8ba1\u503c)\n\n<font color='red'>\u751f\u6210\u5668\u4e0d\u80fd\u76f4\u63a5\u5f71\u54cd\u5230 log(D(x) \u7684\u503c\uff0c\u6240\u4ee5\u5bf9\u4e8e\u4e0a\u9762\u7684\u635f\u5931\u51fd\u6570\u800c\u8a00\uff0c\u6700\u5c0f\u5316\u635f\u5931\u76f8\u5f53\u4e8e\u6700\u5c0f\u5316 log(1 - D(G(z)))<\/font>","540e6aec":"\u5224\u522b\u5668\u662f\u4e00\u4e2a\u57fa\u4e8e CNN \u7684\u56fe\u7247\u5206\u7c7b\u5668\u3002","209ea1c2":"\u6709\u4e86\u751f\u6210\u5668\u548c\u5224\u522b\u5668\uff0c\u5c06\u4e8c\u8005\u6574\u5408\u4e3a\u4e00\u4e2a\u6574\u4f53\uff0c\u5373\u4e3aDCGAN","d7f8a350":"\u53ef\u4ee5\u6839\u636e\u8ba1\u7b97\u80fd\u529b\uff0c\u9002\u5f53\u589e\u52a0\u56fe\u50cf\u9009\u53d6\u6570\u91cf","92b69652":"## 9 \u6a21\u578b\u4fdd\u5b58","e6081b50":"GAN\u7684\u635f\u5931\u51fd\u6570\u66f2\u7ebf\uff0c\u4e0d\u662f\u7b80\u5355\u7684\u4e0b\u964d\u4e86\uff0c\u800c\u662f\u9700\u8981\u7528\u65b0\u773c\u5149\u533a\u522b\u5bf9\u5f85\u751f\u6210\u5668\u548c\u635f\u5931\u5668","cb5db98d":"## 5 \u5b9a\u4e49DCGAN","4bab82c0":"\u751f\u6210\u5668\u4f7f\u7528 tf.keras.layers.Conv2DTranspose\uff08\u4e0a\u91c7\u6837\uff09\u4ece\u79cd\u5b50\uff08\u968f\u673a\u566a\u58f0\uff09\u4e2d\u751f\u6210\u56fe\u50cf\u3002\u4ee5\u4e00\u4e2a\u4f7f\u7528\u8be5\u79cd\u5b50\u4f5c\u4e3a\u8f93\u5165\u7684 Dense \u5c42\u5f00\u59cb\uff0c\u7136\u540e\u591a\u6b21\u4e0a\u91c7\u6837\uff0c\u76f4\u81f3\u8fbe\u5230\u6240\u9700\u7684 64x64x3 \u7684\u56fe\u50cf\u5927\u5c0f\u3002\u8bf7\u6ce8\u610f\uff0c\u9664\u4e86\u8f93\u51fa\u5c42\u4f7f\u7528tanh\u4e4b\u5916\uff0c\u5176\u4ed6\u5c42\u5747\u4f7f\u7528 tf.keras.layers.LeakyReLU \u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\u3002","492b5e95":"\u8bad\u7ec3\u5faa\u73af\u7684\u903b\u8f91\u662f\uff1a\u751f\u6210\u5668\u6536\u5230\u4e00\u4e2a\u968f\u673a\u79cd\u5b50\u4f5c\u4e3a\u8f93\u5165\u3002\u8be5\u79cd\u5b50\u7528\u4e8e\u751f\u6210\u4e00\u4e2a\u56fe\u50cf\u3002\u5224\u522b\u5668\u968f\u540e\u88ab\u7528\u4e8e\u5bf9\u771f\u5b9e\u56fe\u50cf\uff08\u9009\u81ea\u8bad\u7ec3\u96c6\uff09\u548c\u4f2a\u9020\u56fe\u50cf\uff08\u7531\u751f\u6210\u5668\u751f\u6210\uff09\u8fdb\u884c\u5206\u7c7b\u3002\u4e3a\u6bcf\u4e00\u4e2a\u6a21\u578b\u8ba1\u7b97\u635f\u5931\uff0c\u5e76\u4f7f\u7528\u68af\u5ea6\u66f4\u65b0\u751f\u6210\u5668\u548c\u5224\u522b\u5668\u3002","a9fb75c6":"\u5b8c\u6210300\u4ee3\u8bad\u7ec3\uff0c\u9700\u89812\u5c0f\u65f6\u591a\u4e00\u4e9b\u3002","e87742ec":"## 2 \u5173\u4e8eDCGAN","db820a72":"\u5224\u522b\u5668\u6a21\u578b\u6700\u7ec8\u5c06\u88ab\u8bad\u7ec3\u4e3a\u5bf9\u771f\u5b9e\u56fe\u50cf\u8f93\u51fa\u6b63\u503c\uff0c\u5bf9\u4f2a\u9020\u56fe\u50cf\u8f93\u51fa\u8d1f\u503c\u3002\u5f53\u7136\u8fd9\u662f\u5224\u522b\u5668\u4e00\u53a2\u60c5\u613f\u7684\u7406\u60f3\u72b6\u6001\u3002","c8e501e8":"## 1 \u5173\u4e8eGAN","86a83c2d":"### \u53c2\u8003\n\nhttps:\/\/tensorflow.google.cn\/tutorials\/generative\/dcgan<br\/>\nhttps:\/\/www.kaggle.com\/sayakdasgupta\/fake-faces-with-dcgans\n","f877da1b":"## 4 \u521b\u5efa\u6a21\u578b\n\u751f\u6210\u5668\u548c\u5224\u522b\u5668\u5747\u4f7f\u7528 Keras Sequential API \u5b9a\u4e49\u3002","33c9b7f0":"\u8bad\u7ec3\u591a\u5c11\u4ee3\u624d\u5408\u9002\uff1f\u4ee3\u6570\u5c11\u4e86\u770b\u4e0d\u51fa\u6548\u679c\uff0c\u591a\u4e86\u65f6\u95f4\u6210\u672c\u9ad8\uff01","d9f64b39":"the dataset have more than 202k images of which only 50k are being selected for the training purpose","dd26f09b":"\u4f60\u73b0\u5728\u770b\u5230\u7684DCGAN\u7684\u7ed3\u6784\uff0c\u53ea\u5305\u542b\u4e24\u5c42\uff0c\u5206\u522b\u4ee3\u8868\u751f\u6210\u5668\u548c\u5224\u522b\u5668","f32afbb7":"## 6 \u8bad\u7ec3\u6a21\u578b","0c7f9002":"## 7 \u6a21\u578b\u6d4b\u8bd5","f54d9f18":"## 8 \u635f\u5931\u51fd\u6570\u66f2\u7ebf","390d8fa3":"![download%20%282%29.png](attachment:download%20%282%29.png)\n### \u56fe1\uff1aGAN\u6a21\u578b\u7ed3\u6784\u793a\u610f","01a3e762":"![image.png](attachment:7857bc9b-76d7-4688-8126-8a108b5d7844.png)\n#### \u56fe2 \u751f\u6210\u5668\u7ed3\u6784","27c4e0ef":"GAN\u5305\u62ec\u4e24\u90e8\u5206\u6784\u6210\n\n1. \u751f\u6210\u5668\uff08Generator\uff09 - \u751f\u6210\u65b0\u56fe\u50cf\n\n2. \u5224\u522b\u5668 \uff08Discriminator\uff09- \u57fa\u4e8e\u771f\u5b9e\u7684\u56fe\u50cf\u6570\u636e\u96c6\u533a\u5206\u771f\u5047\u56fe\u50cf\n\n\u5224\u522b\u5668\u8d1f\u8d23\u7279\u5f81\u63d0\u53d6\uff0c\u5c06\u771f\u5b9e\u56fe\u50cf\u548c\u751f\u6210\u56fe\u50cf\u7684\u7279\u5f81\u5206\u522b\u6620\u5c04\u5230\u771f\u5047\u6807\u7b7e\u3002\u751f\u6210\u5668\u5219\u76f8\u53cd\uff0c\u751f\u6210\u5668\u662f\u751f\u6210\u56fe\u50cf\u7279\u5f81\u3002\nGAN\u6a21\u578b\u7ed3\u6784\u5982\u56fe1\u6240\u793a\u3002<br\/>\n<font color ='red'>\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u751f\u6210\u5668\u5728\u751f\u6210\u903c\u771f\u56fe\u50cf\u65b9\u9762\u9010\u6e10\u53d8\u5f3a\uff0c\u800c\u5224\u522b\u5668\u5728\u8fa8\u522b\u8fd9\u4e9b\u56fe\u50cf\u7684\u80fd\u529b\u4e0a\u9010\u6e10\u53d8\u5f3a\u3002\u5f53\u5224\u522b\u5668\u4e0d\u518d\u80fd\u591f\u533a\u5206\u771f\u5b9e\u56fe\u7247\u548c\u4f2a\u9020\u56fe\u7247\u65f6\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u8fbe\u5230\u5e73\u8861\u3002<\/font>","b4b7bed7":"\u4e0a\u9762\u7684\u56fe\u50cf\u8bfb\u53d6\u3001\u88c1\u526a\uff0c\u9700\u8981\u8d39\u4e9b\u65f6\u95f4\uff0c\u8bf7\u8010\u5fc3\u7b49\u5f85...."}}