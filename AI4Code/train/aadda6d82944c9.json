{"cell_type":{"99b581f7":"code","9fcffb4c":"code","06c47ace":"code","02321d5e":"code","8ea2dce9":"code","effd3f2e":"code","1c40c83b":"code","8e64096b":"code","cf64a525":"markdown","4315c271":"markdown","be07e073":"markdown","8f2fd027":"markdown","f58370ae":"markdown","89f0dea9":"markdown"},"source":{"99b581f7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9fcffb4c":"#reading the files\ntrain = pd.read_csv(\"..\/input\/learn-together\/train.csv\")\ntest = pd.read_csv(\"..\/input\/learn-together\/test.csv\")\nprint(train.columns)","06c47ace":"#set aside the targets and id\ny = train.Cover_Type\ntest_id = test['Id']","02321d5e":"#dropping Ids\ntrain = train.drop(['Id'], axis = 1)\ntest = test.drop(['Id'], axis = 1)\n\n#prepare data for training the model\nX = train.drop(['Cover_Type'], axis = 1)\n\n#horizontal and vertical distance to hydrology can be easily combined\ncols = ['Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology']\nX['Distance_to_hydrology'] = X[cols].apply(np.linalg.norm, axis=1)\ntest['Distance_to_hydrology'] = test[cols].apply(np.linalg.norm, axis=1)\n\n#adding a few combinations of distance features to help enhance the classification\ncols = ['Horizontal_Distance_To_Roadways',\n        'Horizontal_Distance_To_Fire_Points',\n        'Horizontal_Distance_To_Hydrology']\n\ndef addDistFeatures(df):\n    df['distance_mean'] = df[cols].mean(axis=1)\n    df['distance_sum'] = df[cols].sum(axis=1)\n    df['distance_dif_road_fire'] = df[cols[0]] - df[cols[1]]\n    df['distance_dif_hydro_road'] = df[cols[2]] - df[cols[0]]\n    df['distance_dif_hydro_fire'] = df[cols[2]] - df[cols[1]]\n    return df\n\nX = addDistFeatures(X)\ntest = addDistFeatures(test)\n\n#persisting with the idea of adding simple combination of Hillshades\ncols = ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\nweights = pd.Series([0.299, 0.587, 0.114], index=cols)\nX['Hillshade'] = (X[cols]*weights).sum(1)\ntest['Hillshade'] = (test[cols]*weights).sum(1)\n\nprint(X.columns)","8ea2dce9":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3)","effd3f2e":"param_grid = {\"n_estimators\":  [int(x) for x in np.linspace(start = 10, stop = 200, num = 11)],\n              \"max_depth\": [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n              \"min_samples_split\": np.linspace(0.1, 1.0, 10, endpoint=True), #np.arange(1,150,1),\n              \"min_samples_leaf\": np.linspace(0.1, 0.5, 5, endpoint=True),  #np.arange(1,60,1),\n              \"max_leaf_nodes\": np.arange(2,60,1),\n              \"min_weight_fraction_leaf\": np.arange(0.1,0.4, 0.1)}\n\n#parameter tuning\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nimport warnings\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nclf = RandomForestClassifier(random_state=1)\n\ndef evaluate_param(clf, param_grid, metric, metric_abv):\n    data = []\n    for parameter, values in dict.items(param_grid):\n        for value in values:\n            d = {parameter:value}\n            warnings.filterwarnings('ignore') \n            clf = RandomForestClassifier(**d)\n            clf.fit(X_train, y_train)\n            x_pred = clf.predict(X_train)\n            train_score = metric(y_train, x_pred)\n            y_pred = clf.predict(X_val)\n            test_score = metric(y_val, y_pred)\n            data.append({'Parameter':parameter, 'Param_value':value, \n            'Train_'+metric_abv:train_score, 'Test_'+metric_abv:test_score})\n    df = pd.DataFrame(data)\n    _, axes = plt.subplots(nrows=2, ncols=3, figsize=(10,5))\n    for (parameter, group), ax in zip(df.groupby(df.Parameter), axes.flatten()):\n        group.plot(x='Param_value', y=(['Train_'+metric_abv,'Test_'+metric_abv]),\n        kind='line', ax=ax, title=parameter)\n        ax.set_xlabel('')\n    plt.tight_layout()\n    plt.show()\n\nevaluate_param(clf, param_grid, accuracy_score, 'ACC')","1c40c83b":"param_grid2 = {\"n_estimators\": [29,47,113,181],\n                #'max_leaf_nodes': [150,None],\n                #'max_depth': [20,None],\n                #'min_samples_split': [2, 5], \n                #'min_samples_leaf': [1, 2],\n              \"max_features\": ['auto','sqrt'],\n              \"bootstrap\": [True, False]}\n\nfrom sklearn.model_selection import GridSearchCV\ngrid = GridSearchCV(clf, param_grid2, refit=True, cv=5, verbose=0)\ngrid.fit(X_train, y_train)\nprint('Best parameters: ',grid.best_params_)\nprint('Best estimator: ',grid.best_estimator_)\ngrid_predictions = grid.predict(X_val) \n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_val, grid_predictions))","8e64096b":"test_pred = grid.predict(test)\noutput = pd.DataFrame({'Id': test_id, 'Cover_Type': test_pred.astype(int)})\noutput.to_csv('submission.csv', index=False)","cf64a525":"### Split","4315c271":"# Back to basics with insights gathered from fellow competitors","be07e073":"### Plot","8f2fd027":"### Preprocess","f58370ae":"### Tune","89f0dea9":"## Lessons learnt so far\n\n1. Don't touch categories that are already in binary format (like Wilderness Area, Soil Type), unless some value can be added\n2. Accuracy is a better metric than MAE since this is a classification problem\n3. There is a lot of detail in the features ([just look here](https:\/\/www.kaggle.com\/evimarp\/top-6-roosevelt-national-forest-competition)) that can be explored instead of number crunching through classifiers"}}