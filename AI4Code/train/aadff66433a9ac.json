{"cell_type":{"3b590272":"code","76ccf9ec":"code","4054c0f7":"code","22025709":"code","e1f16082":"code","c756d4d9":"code","18ee51e4":"code","545f3da2":"code","f2653117":"code","18841626":"code","1660a8af":"code","a41081f4":"code","7a86e762":"code","1049b641":"code","2d1c79c5":"code","1a7c0ae8":"code","796a6731":"code","85eaf40a":"code","608cf64f":"code","41926a82":"code","c24a00df":"code","73d91a32":"code","13f88b93":"code","c0d32a4b":"code","da5e80d0":"code","664df24a":"code","74016fa8":"code","89124559":"code","88c8b777":"code","ce184ad9":"code","40e5a9c0":"code","6014148c":"code","2302aa58":"code","a34d67fc":"code","5abbb20b":"code","d9cf6ee5":"code","816b8ee3":"code","73b2ecd2":"code","11699fc3":"code","805abf98":"code","0dd4fa97":"code","870a436d":"code","e5d93305":"code","885f6ee2":"code","72f8666f":"code","4cc70de7":"code","1c9a591a":"code","f5fd8e3b":"code","b1dd8843":"markdown","4299a7e6":"markdown","cafd2606":"markdown","6e373fb1":"markdown","b3c77903":"markdown","d548c934":"markdown","787b7a34":"markdown","68b4d72a":"markdown","c41a4487":"markdown","5fe4fafe":"markdown","6b941b02":"markdown","eb7a643b":"markdown","79902393":"markdown","15b5a579":"markdown","bc625118":"markdown","532c2eea":"markdown","0f90232a":"markdown","42210264":"markdown"},"source":{"3b590272":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","76ccf9ec":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\n\n#\n\nfrom scipy import stats\nfrom scipy.stats import skew, boxcox_normmax, norm\nfrom scipy.special import boxcox1p\n\n#\n\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\n\n#\n\nimport warnings\npd.options.display.max_columns = 250\npd.options.display.max_rows = 250\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')","4054c0f7":"# Loading datasets.\n\ntrain = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')","22025709":"print(train.shape, test.shape)","e1f16082":"train.head()","c756d4d9":"train.describe()","18ee51e4":"# Dropping unnecessary Id column.\n\ntrain.drop('Id', axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)","545f3da2":"# Backing up target variables and dropping them from train data.\n\ny = train['SalePrice'].reset_index(drop=True)\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test","f2653117":"# Display numerical correlations (pearson) between features on heatmap.\n\nsns.set(font_scale=1.1)\ncorrelation_train = train.corr()\nmask = np.triu(correlation_train.corr())\nplt.figure(figsize=(20, 20))\nsns.heatmap(correlation_train,\n            annot=True,\n            fmt='.1f',\n            cmap='coolwarm',\n            square=True,\n            mask=mask,\n            linewidths=1,\n            cbar=False)\n\nplt.show()","18841626":"# Merging train test features for engineering.\n\nfeatures = pd.concat([train_features, test_features]).reset_index(drop=True)\nprint(features.shape)","1660a8af":"\ndef missing_percentage(df):\n    \n    \"\"\"A function for returning missing ratios.\"\"\"\n    \n    total = df.isnull().sum().sort_values(\n        ascending=False)[df.isnull().sum().sort_values(ascending=False) != 0]\n    percent = (df.isnull().sum().sort_values(ascending=False) \/ len(df) *\n               100)[(df.isnull().sum().sort_values(ascending=False) \/ len(df) *\n                     100) != 0]\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])","a41081f4":"# Checking 'NaN' values.\n\nmissing = missing_percentage(features)\n\nfig, ax = plt.subplots(figsize=(20, 5))\nsns.barplot(x=missing.index, y='Percent', data=missing, palette='Reds_r')\nplt.xticks(rotation=90)\n\ndisplay(missing.T.style.background_gradient(cmap='Reds', axis=1))","7a86e762":"# List of 'NaN' including columns where NaN's mean none.\n\nnone_cols = [\n    'Alley', 'PoolQC', 'MiscFeature', 'Fence', 'FireplaceQu', 'GarageType',\n    'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond',\n    'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType'\n]\n\n# List of 'NaN' including columns where NaN's mean 0.\n\nzero_cols = [\n    'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath',\n    'BsmtHalfBath', 'GarageYrBlt', 'GarageArea', 'GarageCars', 'MasVnrArea'\n]\n\n# List of 'NaN' including columns where NaN's actually missing gonna replaced with mode.\n\nfreq_cols = [\n    'Electrical', 'Exterior1st', 'Exterior2nd', 'Functional', 'KitchenQual',\n    'SaleType', 'Utilities'\n]\n\n# Filling the list of columns above with appropriate values:\n\nfor col in zero_cols:\n    features[col].replace(np.nan, 0, inplace=True)\n\nfor col in none_cols:\n    features[col].replace(np.nan, 'None', inplace=True)\n\nfor col in freq_cols:\n    features[col].replace(np.nan, features[col].mode()[0], inplace=True)","1049b641":"# Filling 'MSZoning' according to MSSubClass.\n\nfeatures['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].apply(\n    lambda x: x.fillna(x.mode()[0]))","2d1c79c5":"# Filling 'MSZoning' according to Neighborhood.\n\nfeatures['LotFrontage'] = features.groupby(\n    ['Neighborhood'])['LotFrontage'].apply(lambda x: x.fillna(x.median()))","1a7c0ae8":"# Features which numerical on data but should be treated as category:\n\nfeatures['MSSubClass'] = features['MSSubClass'].astype(str)\nfeatures['YrSold'] = features['YrSold'].astype(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)","796a6731":"# Transforming rare values(less than 10) into one group.\n\nothers = [\n    'Condition1', 'Condition2', 'RoofMatl', 'Exterior1st', 'Exterior2nd',\n    'Heating', 'Electrical', 'Functional', 'SaleType'\n]\n\nfor col in others:\n    mask = features[col].isin(\n        features[col].value_counts()[features[col].value_counts() < 10].index)\n    features[col][mask] = 'Other'","85eaf40a":"def srt_box(y, df):\n    \n    '''A function for displaying categorical variables.'''\n    \n    fig, axes = plt.subplots(14, 3, figsize=(25, 80))\n    axes = axes.flatten()\n\n    for i, j in zip(df.select_dtypes(include=['object']).columns, axes):\n\n        sortd = df.groupby([i])[y].median().sort_values(ascending=False)\n        sns.boxplot(x=i,\n                    y=y,\n                    data=df,\n                    palette='plasma',\n                    order=sortd.index,\n                    ax=j)\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=18))\n\n        plt.tight_layout()","608cf64f":"# Displaying sale prices vs. categorical values:\n\nsrt_box('SalePrice', train)","41926a82":"# Converting some of the categorical values to numeric ones. Choosing similar values for closer groups to balance linear relations...\n\nneigh_map = {\n    'MeadowV': 1,\n    'IDOTRR': 1,\n    'BrDale': 1,\n    'BrkSide': 2,\n    'OldTown': 2,\n    'Edwards': 2,\n    'Sawyer': 3,\n    'Blueste': 3,\n    'SWISU': 3,\n    'NPkVill': 3,\n    'NAmes': 3,\n    'Mitchel': 4,\n    'SawyerW': 5,\n    'NWAmes': 5,\n    'Gilbert': 5,\n    'Blmngtn': 5,\n    'CollgCr': 5,\n    'ClearCr': 6,\n    'Crawfor': 6,\n    'Veenker': 7,\n    'Somerst': 7,\n    'Timber': 8,\n    'StoneBr': 9,\n    'NridgHt': 10,\n    'NoRidge': 10\n}\n\nfeatures['Neighborhood'] = features['Neighborhood'].map(neigh_map).astype(\n    'int')\next_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\nfeatures['ExterQual'] = features['ExterQual'].map(ext_map).astype('int')\nfeatures['ExterCond'] = features['ExterCond'].map(ext_map).astype('int')\nbsm_map = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\nfeatures['BsmtQual'] = features['BsmtQual'].map(bsm_map).astype('int')\nfeatures['BsmtCond'] = features['BsmtCond'].map(bsm_map).astype('int')\nbsmf_map = {\n    'None': 0,\n    'Unf': 1,\n    'LwQ': 2,\n    'Rec': 3,\n    'BLQ': 4,\n    'ALQ': 5,\n    'GLQ': 6\n}\n\nfeatures['BsmtFinType1'] = features['BsmtFinType1'].map(bsmf_map).astype('int')\nfeatures['BsmtFinType2'] = features['BsmtFinType2'].map(bsmf_map).astype('int')\nheat_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\nfeatures['HeatingQC'] = features['HeatingQC'].map(heat_map).astype('int')\nfeatures['KitchenQual'] = features['KitchenQual'].map(heat_map).astype('int')\nfeatures['FireplaceQu'] = features['FireplaceQu'].map(bsm_map).astype('int')\nfeatures['GarageCond'] = features['GarageCond'].map(bsm_map).astype('int')\nfeatures['GarageQual'] = features['GarageQual'].map(bsm_map).astype('int')","c24a00df":"# Plotting numerical features with polynomial order to detect outliers by eye.\n\ndef srt_reg(y, df):\n    fig, axes = plt.subplots(12, 3, figsize=(25, 80))\n    axes = axes.flatten()\n\n    for i, j in zip(df.select_dtypes(include=['number']).columns, axes):\n\n        sns.regplot(x=i,\n                    y=y,\n                    data=df,\n                    ax=j,\n                    order=3,\n                    ci=None,\n                    color='#e74c3c',\n                    line_kws={'color': 'black'},\n                    scatter_kws={'alpha':0.4})\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=10))\n\n        plt.tight_layout()\n","73d91a32":"srt_reg('SalePrice', train)","13f88b93":"\nfeatures = features.join(y)\nfeatures = features.drop(features[(features['OverallQual'] < 5)\n                                  & (features['SalePrice'] > 200000)].index)\nfeatures = features.drop(features[(features['GrLivArea'] > 4000)\n                                  & (features['SalePrice'] < 200000)].index)\nfeatures = features.drop(features[(features['GarageArea'] > 1200)\n                                  & (features['SalePrice'] < 200000)].index)\nfeatures = features.drop(features[(features['TotalBsmtSF'] > 3000)\n                                  & (features['SalePrice'] > 320000)].index)\nfeatures = features.drop(features[(features['1stFlrSF'] < 3000)\n                                  & (features['SalePrice'] > 600000)].index)\nfeatures = features.drop(features[(features['1stFlrSF'] > 3000)\n                                  & (features['SalePrice'] < 200000)].index)\n\ny = features['SalePrice']\ny.dropna(inplace=True)\nfeatures.drop(columns='SalePrice', inplace=True)","c0d32a4b":"# Creating new features  based on previous observations. There might be some highly correlated features now. You cab drop them if you want to...\n\nfeatures['TotalSF'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                       features['1stFlrSF'] + features['2ndFlrSF'])\nfeatures['TotalBathrooms'] = (features['FullBath'] +\n                              (0.5 * features['HalfBath']) +\n                              features['BsmtFullBath'] +\n                              (0.5 * features['BsmtHalfBath']))\n\nfeatures['TotalPorchSF'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                            features['EnclosedPorch'] +\n                            features['ScreenPorch'] + features['WoodDeckSF'])\n\nfeatures['YearBlRm'] = (features['YearBuilt'] + features['YearRemodAdd'])\n\n# Merging quality and conditions.\n\nfeatures['TotalExtQual'] = (features['ExterQual'] + features['ExterCond'])\nfeatures['TotalBsmQual'] = (features['BsmtQual'] + features['BsmtCond'] +\n                            features['BsmtFinType1'] +\n                            features['BsmtFinType2'])\nfeatures['TotalGrgQual'] = (features['GarageQual'] + features['GarageCond'])\nfeatures['TotalQual'] = features['OverallQual'] + features[\n    'TotalExtQual'] + features['TotalBsmQual'] + features[\n        'TotalGrgQual'] + features['KitchenQual'] + features['HeatingQC']\n\n# Creating new features by using new quality indicators.\n\nfeatures['QualGr'] = features['TotalQual'] * features['GrLivArea']\nfeatures['QualBsm'] = features['TotalBsmQual'] * (features['BsmtFinSF1'] +\n                                                  features['BsmtFinSF2'])\nfeatures['QualPorch'] = features['TotalExtQual'] * features['TotalPorchSF']\nfeatures['QualExt'] = features['TotalExtQual'] * features['MasVnrArea']\nfeatures['QualGrg'] = features['TotalGrgQual'] * features['GarageArea']\nfeatures['QlLivArea'] = (features['GrLivArea'] -\n                         features['LowQualFinSF']) * (features['TotalQual'])\nfeatures['QualSFNg'] = features['QualGr'] * features['Neighborhood']","da5e80d0":"# Observing the effects of newly created features on sale price.\n\ndef srt_reg(feature):\n    merged = features.join(y)\n    fig, axes = plt.subplots(5, 3, figsize=(25, 40))\n    axes = axes.flatten()\n\n    new_features = [\n        'TotalSF', 'TotalBathrooms', 'TotalPorchSF', 'YearBlRm',\n        'TotalExtQual', 'TotalBsmQual', 'TotalGrgQual', 'TotalQual', 'QualGr',\n        'QualBsm', 'QualPorch', 'QualExt', 'QualGrg', 'QlLivArea', 'QualSFNg'\n    ]\n\n    for i, j in zip(new_features, axes):\n\n        sns.regplot(x=i,\n                    y=feature,\n                    data=merged,\n                    ax=j,\n                    order=3,\n                    ci=None,\n                    color='#e74c3c',\n                    line_kws={'color': 'black'},\n                    scatter_kws={'alpha':0.4})\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=10))\n\n        plt.tight_layout()","664df24a":"srt_reg('SalePrice')","74016fa8":"# Creating some simple features.\n\nfeatures['HasPool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['Has2ndFloor'] = features['2ndFlrSF'].apply(lambda x: 1\n                                                     if x > 0 else 0)\nfeatures['HasGarage'] = features['QualGrg'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['HasBsmt'] = features['QualBsm'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['HasFireplace'] = features['Fireplaces'].apply(lambda x: 1\n                                                        if x > 0 else 0)\nfeatures['HasPorch'] = features['QualPorch'].apply(lambda x: 1 if x > 0 else 0)","89124559":"\nskewed = [\n    'LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n    'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea',\n    'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n    'ScreenPorch', 'PoolArea', 'LowQualFinSF', 'MiscVal'\n]\n# Finding skewness of the numerical features.\n\nskew_features = np.abs(features[skewed].apply(lambda x: skew(x)).sort_values(\n    ascending=False))\n\n# Filtering skewed features.\n\nhigh_skew = skew_features[skew_features > 0.3]\n\n# Taking indexes of high skew.\n\nskew_index = high_skew.index\n\n# Applying boxcox transformation to fix skewness.\n\nfor i in skew_index:\n    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))","88c8b777":"# Features to drop:\n\nto_drop = [\n    'Utilities',\n    'PoolQC',\n    'YrSold',\n    'MoSold',\n    'ExterQual',\n    'BsmtQual',\n    'GarageQual',\n    'KitchenQual',\n    'HeatingQC',\n]\n\n# Dropping features.\n\nfeatures.drop(columns=to_drop, inplace=True)","ce184ad9":"# Getting dummy variables for categorical data.\n\nfeatures = pd.get_dummies(data=features)","40e5a9c0":"print(f'Number of missing values: {features.isna().sum().sum()}')","6014148c":"\nfeatures.sample(5)","2302aa58":"features.describe()","a34d67fc":"# Separating train and test set.\n\ntrain = features.iloc[:len(y), :]\ntest = features.iloc[len(train):, :]","5abbb20b":"correlations = train.join(y).corrwith(train.join(y)['SalePrice']).iloc[:-1].to_frame()\ncorrelations['Abs Corr'] = correlations[0].abs()\nsorted_correlations = correlations.sort_values('Abs Corr', ascending=False)['Abs Corr']\nfig, ax = plt.subplots(figsize=(12,12))\nsns.heatmap(sorted_correlations.to_frame()[sorted_correlations>=.5], cmap='coolwarm', annot=True, vmin=-1, vmax=1, ax=ax);","d9cf6ee5":"def plot_dist3(df, feature, title):\n    \n    # Creating a customized chart. and giving in figsize and everything.\n    \n    fig = plt.figure(constrained_layout=True, figsize=(12, 8))\n    \n    # creating a grid of 3 cols and 3 rows.\n    \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n\n    # Customizing the histogram grid.\n    \n    ax1 = fig.add_subplot(grid[0, :2])\n    \n    # Set the title.\n    \n    ax1.set_title('Histogram')\n    \n    # plot the histogram.\n    \n    sns.distplot(df.loc[:, feature],\n                 hist=True,\n                 kde=True,\n                 fit=norm,\n                 ax=ax1,\n                 color='#e74c3c')\n    ax1.legend(labels=['Normal', 'Actual'])\n\n    # customizing the QQ_plot.\n    \n    ax2 = fig.add_subplot(grid[1, :2])\n    \n    # Set the title.\n    \n    ax2.set_title('Probability Plot')\n    \n    # Plotting the QQ_Plot.\n    stats.probplot(df.loc[:, feature].fillna(np.mean(df.loc[:, feature])),\n                   plot=ax2)\n    ax2.get_lines()[0].set_markerfacecolor('#e74c3c')\n    ax2.get_lines()[0].set_markersize(12.0)\n\n    # Customizing the Box Plot:\n    \n    ax3 = fig.add_subplot(grid[:, 2])\n    # Set title.\n    \n    ax3.set_title('Box Plot')\n    \n    # Plotting the box plot.\n    \n    sns.boxplot(df.loc[:, feature], orient='v', ax=ax3, color='#e74c3c')\n    ax3.yaxis.set_major_locator(MaxNLocator(nbins=24))\n\n    plt.suptitle(f'{title}', fontsize=24)","816b8ee3":"# Checking target variable.\n\nplot_dist3(train.join(y), 'SalePrice', 'Sale Price Before Log Transformation')","73b2ecd2":"# Setting model data.\n\nX = train\nX_test = test\ny = np.log1p(y)","11699fc3":"plot_dist3(train.join(y), 'SalePrice', 'Sale Price After Log Transformation')","805abf98":"# Loading neccesary packages for modelling.\n\nfrom sklearn.model_selection import cross_val_score, KFold, cross_validate\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV, TweedieRegressor\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor # This is for stacking part, works well with sklearn and others...","0dd4fa97":"# Setting kfold for future use.\n\nkf = KFold(10, random_state=42)\n# Some parameters for ridge, lasso and elasticnet.\n\nalphas_alt = [15.5, 15.6, 15.7, 15.8, 15.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [\n    5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008\n]\ne_alphas = [\n    0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007\n]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\n# ridge_cv\n\nridge = make_pipeline(RobustScaler(), RidgeCV(\n    alphas=alphas_alt,\n    cv=kf,\n))\n\n# lasso_cv:\n\nlasso = make_pipeline(\n    RobustScaler(),\n    LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kf))\n\n# elasticnet_cv:\n\nelasticnet = make_pipeline(\n    RobustScaler(),\n    ElasticNetCV(max_iter=1e7,\n                 alphas=e_alphas,\n                 cv=kf,\n                 random_state=42,\n                 l1_ratio=e_l1ratio))\n\n# svr:\n\nsvr = make_pipeline(RobustScaler(),\n                    SVR(C=21, epsilon=0.0099, gamma=0.00017, tol=0.000121))\n\n# gradientboosting:\n\ngbr = GradientBoostingRegressor(n_estimators=2900,\n                                learning_rate=0.0161,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=17,\n                                loss='huber',\n                                random_state=42)\n\n# lightgbm:\n\nlightgbm = LGBMRegressor(objective='regression',\n                         n_estimators=3500,\n                         num_leaves=5,\n                         learning_rate=0.00721,\n                         max_bin=163,\n                         bagging_fraction=0.35711,\n                         n_jobs=-1,\n                         bagging_seed=42,\n                         feature_fraction_seed=42,\n                         bagging_freq=7,\n                         feature_fraction=0.1294,\n                         min_data_in_leaf=8)\n\n# xgboost:\n\nxgboost = XGBRegressor(\n    learning_rate=0.0139,\n    n_estimators=4500,\n    max_depth=4,\n    min_child_weight=0,\n    subsample=0.7968,\n    colsample_bytree=0.4064,\n    nthread=-1,\n    scale_pos_weight=2,\n    seed=42,\n)\n\n\n# hist gradient boosting regressor:\n\nhgrd= HistGradientBoostingRegressor(    loss= 'least_squares',\n    max_depth= 2,\n    min_samples_leaf= 40,\n    max_leaf_nodes= 29,\n    learning_rate= 0.15,\n    max_iter= 225,\n                                    random_state=42)\n\n# tweedie regressor:\n \ntweed = make_pipeline(RobustScaler(),TweedieRegressor(alpha=0.005))\n\n\n# stacking regressor:\n\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr,\n                                            xgboost, lightgbm,hgrd, tweed),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","870a436d":"def model_check(X, y, estimators, cv):\n    \n    ''' A function for testing multiple estimators.'''\n    \n    model_table = pd.DataFrame()\n\n    row_index = 0\n    for est, label in zip(estimators, labels):\n\n        MLA_name = label\n        model_table.loc[row_index, 'Model Name'] = MLA_name\n\n        cv_results = cross_validate(est,\n                                    X,\n                                    y,\n                                    cv=cv,\n                                    scoring='neg_root_mean_squared_error',\n                                    return_train_score=True,\n                                    n_jobs=-1)\n\n        model_table.loc[row_index, 'Train RMSE'] = -cv_results[\n            'train_score'].mean()\n        model_table.loc[row_index, 'Test RMSE'] = -cv_results[\n            'test_score'].mean()\n        model_table.loc[row_index, 'Test Std'] = cv_results['test_score'].std()\n        model_table.loc[row_index, 'Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1\n\n    model_table.sort_values(by=['Test RMSE'],\n                            ascending=True,\n                            inplace=True)\n\n    return model_table","e5d93305":"# Setting list of estimators and labels for them:\n\nestimators = [ridge, lasso, elasticnet, gbr, xgboost, lightgbm, svr, hgrd, tweed]\nlabels = [\n    'Ridge', 'Lasso', 'Elasticnet', 'GradientBoostingRegressor',\n    'XGBRegressor', 'LGBMRegressor', 'SVR', 'HistGradientBoostingRegressor','TweedieRegressor'\n]","885f6ee2":"# Executing cross validation.\n\nraw_models = model_check(X, y, estimators, kf)\ndisplay(raw_models.style.background_gradient(cmap='summer_r'))","72f8666f":"# Fitting the models on train data.\n\nprint('=' * 20, 'START Fitting', '=' * 20)\nprint('=' * 55)\n\nprint(datetime.now(), 'StackingCVRegressor')\nstack_gen_model = stack_gen.fit(X.values, y.values)\nprint(datetime.now(), 'Elasticnet')\nelastic_model_full_data = elasticnet.fit(X, y)\nprint(datetime.now(), 'Lasso')\nlasso_model_full_data = lasso.fit(X, y)\nprint(datetime.now(), 'Ridge')\nridge_model_full_data = ridge.fit(X, y)\nprint(datetime.now(), 'SVR')\nsvr_model_full_data = svr.fit(X, y)\nprint(datetime.now(), 'GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y)\nprint(datetime.now(), 'XGboost')\nxgb_model_full_data = xgboost.fit(X, y)\nprint(datetime.now(), 'Lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)\nprint(datetime.now(), 'Hist')\nhist_full_data = hgrd.fit(X, y)\nprint(datetime.now(), 'Tweed')\ntweed_full_data = tweed.fit(X, y)\nprint('=' * 20, 'FINISHED Fitting', '=' * 20)\nprint('=' * 58)","4cc70de7":"# Blending models by assigning weights:\n\ndef blend_models_predict(X):\n    return ((0.1 * elastic_model_full_data.predict(X)) +\n            (0.1 * lasso_model_full_data.predict(X)) +\n            (0.1 * ridge_model_full_data.predict(X)) +\n            (0.1 * svr_model_full_data.predict(X)) +\n            (0.025 * gbr_model_full_data.predict(X)) +\n            (0.15 * xgb_model_full_data.predict(X)) +\n            (0.05 * lgb_model_full_data.predict(X)) +\n            (0.025 * hist_full_data.predict(X)) +\n            (0.1 * tweed_full_data.predict(X)) +\n            (0.25 * stack_gen_model.predict(X.values)))","1c9a591a":"submission = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')\n# Inversing and flooring log scaled sale price predictions\nsubmission['SalePrice'] = np.floor(np.expm1(blend_models_predict(X_test)))\n# Defining outlier quartile ranges\nq1 = submission['SalePrice'].quantile(0.0050)\nq2 = submission['SalePrice'].quantile(0.99)\n\n# Applying weights to outlier ranges to smooth them\nsubmission['SalePrice'] = submission['SalePrice'].apply(\n    lambda x: x if x > q1 else x * 0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x\n                                                        if x < q2 else x * 1.1)\nsubmission = submission[['Id', 'SalePrice']]","f5fd8e3b":"# Saving submission file\n\nsubmission.to_csv('mysubmission.csv', index=False)\nprint(\n    'Save submission',\n    datetime.now(),\n)\nsubmission.head()","b1dd8843":"Top 5 features where the most of missing values exist are PoolQC, MiscFeature, Alley, Fence and FireplaceQu.","4299a7e6":"### Missing Data\nAlright, first of all we need detect missing values, then wee need to get rid of them for the next steps of our work. So let's list our missing values and visualize them:\n","cafd2606":"### Stacking & Blending\nHere we fit every single estimator we have on the train data and then blend them by assigning weights to each model and sum the results.","6e373fb1":"### Categorical Data\n\n#### Observations\n1. MSZoing\n    * Floating village houses (I assume they are some kind of special area that retired community resides, has the highest median value.\n    * Residental low density houses comes second with the some outliers.\n    * Residental high and low seems similar meanwhile commercial is the lowest.\n    \n1. LandContour; Hillside houses seems a little bit higher expensive than the rest meanwhile banked houses are the lowest.\n\n1. Neighborhood;\n\n    * Northridge Heights, Northridge and Timberland are top 3 expensive places for houses.\n    * Somerset, Veenker, Crawford, Clear Creek, College Creek and Bloomington Heights seems above average.\n    * Sawyer West has wide range for prices related to similar priced regions.\n    * Old Town and Edwards has some outlier prices but they generally below average.\n    * Briardale, Iowa DOT and Rail Road, Meadow Village are the cheapest places for houses it seems...\n    \n    \n1. Conditions;\n\n    * Meanwhile having wide range of values being close to North-South Railroad seems having positive effect on the price.\n    * Being near or adjacent to positive off-site feature (park, greenbelt, etc.) increases the price.\n        These values are pretty similar but we can get some useful information from them.\n        \n        \n1. MasVnrType; Having stone masonry veneer seems better priced than having brick.\n\n1. Quality Features; There are many categorical quality values that affects the pricing on some degree, we're going to quantify them so we can create new features based on them. So we don't dive deep on them in this part.\n\n\n1. CentralAir; Having central air system has decent positive effect on sale prices.\n\n1. GarageType;\n\n    * Built-In (Garage part of house - typically has room above garage) garage typed houses are the most expensive ones.\n    * Attached garage types following the built-in ones.\n    * Car ports are the lowest\n    \n1. Misc; Sale type has some kind of effect on the prices but we won't get into details here. Btw... It seems having tennis court is really adding price to your house, who would have known :)\n\n\n**Alright, we're done with categorical data inspecting, I'm going to convert some of these categories to numerical ones, especially the ones where related to quality of the specific features.**","b3c77903":"### Droping feautures\nHere we dropping some unnecessary features had their use in feature engineering or not needed at all","d548c934":"### Numeric Data\nThere are many numeric features the inspect, one of the best ways to see how they effect sale prices is scatter plots.\n\n\n* Observations:\n* OverallQual; It's clearly visible that sale price of the house increases with overall quality. This confirms the correlation in first table we did at the beginning. (Pearson corr was 0.8)\n\n* OverallCondition; Looks like overall condition is left skewed where most of the houses are around 5\/10 condition. But it doesn't effect the price like quality indicator...\n\n* YearBuilt; Again new buildings are generally expensive than the old ones.\n\n* Basement; General table shows bigger basements are increasing the price but I see some outliers there...\n\n* GrLivArea; This feature is pretty linear but we can spot two outliers effecting this trend. There are some huge area houses with pretty cheap prices, there might be some reason behind it but we better drop them.\n\n* SaleDates; They seem pretty unimportant on sale prices, we can drop them...","787b7a34":"### Checking New Features\n","68b4d72a":"Separating train and test data\n","c41a4487":"So we have no missing values","5fe4fafe":"## Analysis Time!\n\n\nObservations:\n1. There's strong relation between overall quality of the houses and their sale prices.\n1. Again above grade living area seems strong indicator for sale price.\n1. Garage features, number of baths and rooms, how old the building is etc. also having effect on the price on various levels too.\n1. There are some obvious relations we gonna pass like total square feet affecting how many rooms there are or how many cars can fit into a garage vs. garage area etc.\n1. Overall condition of the house seems less important on the pricing, it's interesting and worth digging.","6b941b02":"#### Feature Engineering\nOk this is the part where we dig deeper into our completed dataset. There are no missing values so we're good to go! I'm going to start with grouping some values, these values are really rare and I'm thinking they do not add much, so if they appear less than 10 times in our observations they get into 'Other' group.","eb7a643b":"### Fixing missing values\n\n1. First we fill the NaN's in the columns where they mean 'None' so we gonna replace them with that,\n1. Then we fill numerical columns where missing values indicating there is no parent feature to measure, so we replace them with 0's.\n1. Even with these there are some actual missing data, by checking general trends of these features we can fill them with most frequent value(with mode).\n1. MSZoning part is little bit tricky I choose to fill them with most common type of the related MSSubClass type. It's not perfect but at least we decrease randomness a little bit.\n1. Again we fill the Lot Frontage with similar approach.\n","79902393":"# Modelling\n* Ridge,\n* Lasso,\n* Elasticnet,\n* Support Vector Regression (robust scaler on these before we run them because they really get effected by outliers.\n* Gradient Boosting Regressor\n* LightGBM Regressor\n* XGBoost Regressor\n* Hist Gradient Boosting Regressor\n* Tweedie Regressor: It's generalized linear model with a Tweedie distribution. We gonna use power of 0 because we expecting normal target distribution but you can try this or other generalized models like poisson regressor or gamma regressor.","15b5a579":"#### Outliers\nOk here we're going to drop some outliers we detected them just above","bc625118":"### Creating New Features\nWith basic approach by merging some important indicators and making them stronger.","532c2eea":"### Transforming the Data\nSome of the continious values are not distributed evenly and not fitting on normal distribution, we can fix them by using couple transformation approaches. We're going to use boxcox here","0f90232a":"# Cross Validation","42210264":"### Submission\nOur models are tuned, stacked, fitted and blended so we are ready to predict and submit our results. One last thing that I have seen on couple examples adding weights on some quantile levels. "}}