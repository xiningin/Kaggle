{"cell_type":{"026c74e4":"code","0b954077":"code","c08bacba":"code","5cb78f16":"code","40584971":"code","8f1ac596":"code","c6495540":"code","562cbfd1":"code","ba684449":"code","2c0818ef":"code","e4811104":"code","1106b81f":"code","59d00dee":"code","6d500be6":"code","1615a919":"code","da441c79":"code","6adfdb80":"code","1f75cb5a":"code","bd6c8022":"code","f5c92801":"code","fef64a12":"code","1ee7dd7d":"code","c8aedcc3":"code","1d67e040":"code","168bde87":"code","1cfe334d":"markdown","aaf1ac34":"markdown","b06b2188":"markdown","01b17a86":"markdown","096e4893":"markdown","9fe577f0":"markdown","adf43312":"markdown","1ee3a2d8":"markdown","9cf32b90":"markdown","f0ae4272":"markdown","f0e62628":"markdown","021e987d":"markdown"},"source":{"026c74e4":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.io import wavfile","0b954077":"frequency_sampling, audio_signal = wavfile.read(\"..\/input\/speech-emotion-recognition-en\/Ravdess\/audio_speech_actors_01-24\/Actor_07\/03-01-01-01-01-01-07.wav\")","c08bacba":"print('\\nSignal shape:', audio_signal.shape)\nprint('Signal Datatype:', audio_signal.dtype)\nprint('Signal duration:', round(audio_signal.shape[0] \/ \nfloat(frequency_sampling), 2), 'seconds')","5cb78f16":"audio_signal = audio_signal \/ np.power(2, 15)","40584971":"audio_signal = audio_signal [:100]\ntime_axis = 1000 * np.arange(0, len(audio_signal), 1) \/ float(frequency_sampling)","8f1ac596":"plt.plot(time_axis, audio_signal, color='blue')\nplt.xlabel('Time (milliseconds)')\nplt.ylabel('Amplitude')\nplt.title('Input audio signal')\nplt.show()","c6495540":"length_signal = len(audio_signal)\nhalf_length = np.ceil((length_signal + 1) \/ 2.0).astype(np.int)","562cbfd1":"signal_frequency = np.fft.fft(audio_signal)","ba684449":"signal_frequency = abs(signal_frequency[0:half_length]) \/ length_signal\nsignal_frequency **= 2","2c0818ef":"len_fts = len(signal_frequency)","e4811104":"if length_signal % 2:\n   signal_frequency[1:len_fts] *= 2\nelse:\n   signal_frequency[1:len_fts-1] *= 2","1106b81f":"signal_power = 10 * np.log10(signal_frequency)","59d00dee":"x_axis = np.arange(0, half_length, 1) * (frequency_sampling \/ length_signal) \/ 1000.0","6d500be6":"plt.figure()\nplt.plot(x_axis, signal_power, color='red')\nplt.xlabel('Frequency (kHz)')\nplt.ylabel('Signal power (dB)')\nplt.show()","1615a919":"!pip install SpeechRecognition","da441c79":"import speech_recognition as sr\nsr.__version__","6adfdb80":"r = sr.Recognizer()","1f75cb5a":"bangla1 = sr.AudioFile('..\/input\/speech-emotion-recognition-en\/Savee\/DC_a03.wav')","bd6c8022":"with bangla1 as source:\n    audio = r.record(source)","f5c92801":"type(audio)","fef64a12":"#using google web speech API\nr.recognize_google(audio)","1ee7dd7d":"bangla2 = sr.AudioFile('..\/input\/bangla-speech-recognition\/Dataset\/\u09b2\u09cd\u09af\u09be\u09aa\u099f\u09aa \u099a\u09be\u09b0\u09cd\u099c\u09be\u09b0 \u099a\u09be\u09b2\u09c1 \u09b9\u09cb\u0995\/Tonmoy 16.wav')","c8aedcc3":"with bangla2 as source:\n    audio2 = r.record(source)","1d67e040":"type(audio2)","168bde87":"r.recognize_google(audio2,language = \"bn-BN\")","1cfe334d":"# Recognizer class","aaf1ac34":"# Difficulties in developing a speech recognizer","b06b2188":"# Loading an audio file","01b17a86":"* Size of the vocabulary - Larger the size of vocabulary, the harder it is to perform speech recognition.\n\n* Channel Charateristics - Channel quality is also an important dimension. For example, human speech contains high bandwidth with full frequency range, while a telephone speech consists of low bandwidth with limited frequency range. It is harder in the latter.\n\n* Speaking mode - Ease of developing an ASR also depends on the speaking mode, that is whether the speech is in isolated word mode, or connected word mode, or in a continuous speech mode. Note that a continuous speech is harder to recognize.\n\n* Speaking style - A read speech may be in a formal style, or spontaneous and conversational with casual style. The latter is harder to recognize.\n\n* Speaker dependency - Speech can be speaker dependent, speaker adaptive, or speaker independent. A speaker independent is the hardest to build.\n\n* Type of noise -  Noise is another factor to consider while developing an ASR. Signal to noise ratio may be in various ranges, depending on the acoustic environment that observes less versus more background noise. If the signal to noise ratio is greater than 30dB, it is considered as high range.\n\n* Microphone characteristics - he quality of microphone may be good, average, or below average. Also, the distance between mouth and micro-phone can vary.\n\n","096e4893":"# Visualizing audio files","9fe577f0":"# Installing libraries","adf43312":"# How does speech recognition work?","1ee3a2d8":"![Automatic_Speech_Recognition_ASR.jpg](attachment:Automatic_Speech_Recognition_ASR.jpg)","9cf32b90":"> Speech recognition translates spoken languages into text.  is the ability of a computer software to identify words and phrases in spoken language and convert them to human readable text. Speech must be converted from physical sound to an electrical signal with a microphone, and then to digital data with an analog-to-digital converter. Once digitized, several models can be used to transcribe the audio to text. Most modern speech recognition systems rely on what is known as a Hidden Markov Model (HMM). This approach works on the assumption that a speech signal, when viewed on a short enough timescale (say, ten milliseconds), can be reasonably approximated as a stationary process\u2014that is, a process in which statistical properties do not change over time. The basic goal of speech processing is to provide an interaction between a human and a machine.","f0ae4272":"# Characterizing the audio signal\nConverting the time domain signal into frequency domain. This is important because it gives a lot of information about the signal. A mathematical tool like fourier transformation can be used for this.","f0e62628":"Speech processing system mainly has 3 tasks - \n\n\n1) First, speech recognition that allows the machine to catch the words, phrases and sentences we speak.\n\n2)Second, natural language processing to allow the machine to understand what we speak, and\n\n3) Third, speech synthesis to allow the machine to speak.","021e987d":"Almost all speech recognition systems rely on hidden Markov Model (HMM). This approach works on the assumption that a speech signal when viewed on a short enough timescale, for example - 10 milliseconds can be reasonably approximated as a stationary process, i.e, the process in which statistical properties do not change over time.\n\n*In a typical HMM, the speech signal is divided into 10-milliseconds fragments. The power spectrum of each fragement is essentially a plot of the signal's power as a function of frequency, is mapped to a vector of real numbers known as cepstral coefficients. The dimension of this ector is usually small, sometimes as low as 10, although more accurate systems may have dimension 32 or more. The final output of the HMM is a sequence of these vectors.*\n\n**To decode the speech into text, groups of vectors are matched to one or more phonemes\u2014a fundamental unit of speech. This calculation requires training, since the sound of a phoneme varies from speaker to speaker, and even varies from one utterance to another by the same speaker. A special algorithm is then applied to determine the most likely word (or words) that produce the given sequence of phonemes. One can imagine that this whole process may be computationally expensive. In many modern speech recognition systems, neural networks are used to simplify the speech signal using techniques for feature transformation and dimensionality reduction before HMM recognition. Voice activity detectors (VADs) are also used to reduce an audio signal to only the portions that are likely to contain speech. This prevents the recognizer from wasting time analyzing unnecessary parts of the signal.**"}}