{"cell_type":{"bc21f140":"code","179faf9e":"code","bea01ed1":"code","3f449b4c":"code","22aef69e":"code","4c5e676e":"code","6c85eeaf":"code","f5ee9752":"code","4f966800":"code","7df113ce":"code","c560f6d3":"code","094ea2a4":"code","94d054fd":"code","a37267b5":"code","789f75c1":"code","fcc0763c":"code","2bf8ead6":"code","00139eb7":"code","986009c9":"code","aedec6e5":"code","a842335b":"code","2574ffd8":"code","d1bae618":"code","f696f5da":"code","e4b3b91d":"code","d19dda5c":"code","99d22d54":"code","3a46ea07":"code","b2f8e142":"code","7183a941":"code","51d57bf4":"code","670d1a73":"code","1bcb5b30":"code","792aa93f":"code","306e035e":"code","f1b5429a":"code","94a3a7ee":"code","d469c4c0":"code","de637de5":"code","71aa7d95":"code","48291a91":"code","e30d5919":"code","2caa86ee":"code","ed23dce4":"code","af8bd75c":"code","98bf553f":"code","ff115746":"code","1bdd0fdc":"code","10974786":"code","aa578960":"code","1662d82f":"markdown","8bd9c02a":"markdown","186ef3b4":"markdown","1c5bfe76":"markdown","da905902":"markdown","b6da6860":"markdown","f33c8c34":"markdown","5ac205e3":"markdown","43b25b7a":"markdown","63f8d414":"markdown","3f43fe7a":"markdown","3ff79eee":"markdown","be12d82d":"markdown","625caa56":"markdown","5f4d4339":"markdown","101e3c69":"markdown","0b5013bd":"markdown","7220ce18":"markdown","18f46f1c":"markdown","4986e65a":"markdown","24f3c045":"markdown"},"source":{"bc21f140":"# The Packages we need for this project:\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy\n\nimport warnings\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, Ridge, Lasso, SGDRegressor\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import svm, gaussian_process\nfrom sklearn.metrics import mean_squared_error\n\n\n# The basic setup for this project:\n\nsns.set(style=\"ticks\")\n%matplotlib inline\nwarnings.filterwarnings('ignore')\n\n\n# The data we need for this project:\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n","179faf9e":"train.head()","bea01ed1":"train.shape, test.shape","3f449b4c":"train.info()","22aef69e":"test.info()","4c5e676e":"# Describe and visualize the dependent variables -- house prices:\n\nprint(train.shape)\ntrain['SalePrice'].describe()","6c85eeaf":"sns.set(rc = {'figure.figsize':(12,7)})\nsns.distplot(train['SalePrice'], rug=True)","f5ee9752":"# Have a look at log of price to promise the dependent variables can obey normal distribution which don't change the veracity of final result at the sametime:\n\ntrain['LogPrice'] = np.log(train['SalePrice'])\nsns.distplot(train['LogPrice'], rug=True)","4f966800":"# Use correlation matrix to describe the relationship among all features:\n\ncorrmat = train.corr()\nsns.set(rc = {'figure.figsize':(35,15)})\nsns.heatmap(corrmat, cmap=\"YlGnBu\", annot=True)","7df113ce":"# The correlation matrix among selected features:\n\ncols = ['LogPrice', 'OverallQual', 'YearBuilt', 'YearRemodAdd', 'TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'FullBath', 'TotRmsAbvGrd', 'GarageCars', 'GarageArea']\ncorrmat = train[cols].corr()\nsns.set(rc = {'figure.figsize':(12,7)})\nsns.heatmap(corrmat, cmap=\"YlGnBu\", annot=True)","c560f6d3":"# Decribe every single variables:\n\nprint(train['OverallQual'].describe())\nsns.set(rc = {'figure.figsize':(12,7)})\nsns.countplot(train['OverallQual'])","094ea2a4":"print(train['YearBuilt'].describe())\nsns.set(rc = {'figure.figsize':(35,15)})\np = sns.countplot(train['YearBuilt'])\np.set_xticklabels(p.get_xticklabels(), rotation=30)","94d054fd":"print(train['YearRemodAdd'].describe())\nsns.countplot(train['YearRemodAdd'])","a37267b5":"print(train['TotalBsmtSF'].describe())\nsns.set(rc = {'figure.figsize':(12,7)})\nsns.distplot(train['TotalBsmtSF'], rug=True)","789f75c1":"print(train['1stFlrSF'].describe())\nsns.distplot(train['1stFlrSF'], rug=True)","fcc0763c":"print(train['GrLivArea'].describe())\nsns.distplot(train['GrLivArea'], rug=True)","2bf8ead6":"print(train['FullBath'].describe())\nsns.countplot(train['FullBath'])","00139eb7":"print(train['TotRmsAbvGrd'].describe())\nsns.countplot(train['TotRmsAbvGrd'])","986009c9":"print(train['GarageCars'].describe())\nsns.countplot(train['GarageCars'])","aedec6e5":"print(train['GarageArea'].describe())\nsns.distplot(train['GarageArea'], rug=True)","a842335b":"# Find the missing data:\n\ntrain_data = train[cols]\nmissing_data = train_data.isnull().sum().sort_values(ascending=False)\nmissing_percent = (train_data.isnull().sum()*100\/train_data.isnull().count()).sort_values(ascending=False)\npd.concat([missing_data, missing_percent], axis=1, keys=['missing_data', 'missing_percent'])","2574ffd8":"# Standardize varibles with log function:\n\ntrain_data['LogGarageArea'] = train_data['GarageArea']\ntrain_data.loc[train_data['LogGarageArea']>0, 'LogGarageArea'] = np.log(train_data['LogGarageArea'])\nsns.distplot(train_data[train_data['LogGarageArea']>0]['LogGarageArea'], rug=True)","d1bae618":"train_data['LogGrLivArea'] = np.log(train_data['GrLivArea'])\nsns.distplot(train_data['LogGrLivArea'], rug=True)","f696f5da":"train_data['Log1stFlrSF'] = np.log(train_data['1stFlrSF'])\nsns.distplot(train_data['Log1stFlrSF'], rug=True)","e4b3b91d":"train_data['LogTotalBsmtSF'] = train_data['TotalBsmtSF']\ntrain_data.loc[train_data['LogTotalBsmtSF']>0, 'LogTotalBsmtSF'] = np.log(train_data['LogTotalBsmtSF'])\nsns.distplot(train_data[train_data['LogTotalBsmtSF']>0]['LogTotalBsmtSF'], rug=True)","d19dda5c":"train_data.head(10)","99d22d54":"# The quantitative variables:\n\ncolumns_quantity = ['LogTotalBsmtSF', 'Log1stFlrSF', 'LogGrLivArea', 'LogGarageArea']","3a46ea07":"# A positive correlation between 'LogTotalBsmtSF' and 'LogPrice':\n\nsns.scatterplot(x=train_data['LogTotalBsmtSF'], y=train_data['LogPrice'])","b2f8e142":"# A positive correlation between 'Log1stFlrSF' and 'LogPrice':\n\nsns.scatterplot(x=train_data['Log1stFlrSF'], y=train_data['LogPrice'])","7183a941":"# A positive correlation between 'LogGrLivArea' and 'LogPrice':\n\nsns.scatterplot(x=train_data['LogGrLivArea'], y=train_data['LogPrice'])","51d57bf4":"# A positive correlation between 'LogGarageArea' and 'LogPrice':\n\nsns.scatterplot(x=train_data['LogGarageArea'], y=train_data['LogPrice'])","670d1a73":"# The categorical variable:\n\ncolums_cate = ['OverallQual', 'YearBuilt', 'YearRemodAdd', 'FullBath', 'GarageCars', 'TotRmsAbvGrd']","1bcb5b30":"# A positive correlation between 'OverallQual' and 'LogPrice':\n\nsns.boxplot(train_data['OverallQual'], train_data['LogPrice'])","792aa93f":"# A positive correlation between 'FullBath' and 'LogPrice':\n\nsns.boxplot(train_data['FullBath'], train_data['LogPrice'])","306e035e":"# A positive correlation between 'GarageCars' and 'LogPrice' but it turns negative when 'GarageCars' is too large:\n\nsns.boxplot(train_data['GarageCars'], train_data['LogPrice'])","f1b5429a":"# A positive correlation between 'TotRmsAbvGrd' and 'LogPrice' but it turns negative when 'TotRmsAbvGrd' is too large:\n\nsns.boxplot(train_data['TotRmsAbvGrd'], train_data['LogPrice'])","94a3a7ee":"# A positive correlation tendency between 'YearBuilt' and 'LogPrice' :\n\nsns.set(rc = {'figure.figsize':(35,15)})\np = sns.boxplot(train_data['YearBuilt'], train_data['LogPrice'])\np.set_xticklabels(p.get_xticklabels(), rotation=30)","d469c4c0":"# A positive correlation tendency between 'YearRemodAdd' and 'LogPrice' \n\np = sns.boxplot(train_data['YearRemodAdd'], train_data['LogPrice'])\np.set_xticklabels(p.get_xticklabels(), rotation=30)","de637de5":"train_data.loc[train_data['YearRemodAdd']==train_data['YearBuilt'], 'RemodAdd'] = 0\ntrain_data['RemodAdd'].fillna(1, inplace=True)\np = sns.boxplot(hue=train_data['RemodAdd'], x=train_data['YearBuilt'], y=train_data['LogPrice'])\np.set_xticklabels(p.get_xticklabels(), rotation=30)","71aa7d95":"cols = ['LogPrice', 'OverallQual', 'YearBuilt', 'YearRemodAdd', 'FullBath', 'GarageCars', 'TotRmsAbvGrd', 'LogTotalBsmtSF', 'Log1stFlrSF', 'LogGrLivArea', 'LogGarageArea']\ncorrm = train_data[cols].corr()\nsns.set(rc = {'figure.figsize':(12,7)})\nsns.heatmap(corrm, cmap=\"YlGnBu\", annot=True)","48291a91":"cols = ['LogPrice', 'OverallQual', 'YearBuilt', 'RemodAdd', 'Log1stFlrSF', 'FullBath', 'LogGrLivArea', 'GarageCars']\nsns.pairplot(train_data[cols])","e30d5919":"# Data preparation:\n\ntrain_data['Cross'] = train_data['YearBuilt'] * train_data['RemodAdd']\ncols = ['LogPrice', 'OverallQual', 'YearBuilt', 'Cross', 'Log1stFlrSF', 'FullBath', 'LogGrLivArea', 'GarageCars']\nX = train_data[cols[1:]]\nY = train_data[cols[0]]\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n\n\n# Model Score:\n\ndef r_square(model, x, y):\n    print('R^2 is {}'.format(model.score(x, y)))\n    return model.score(x, y)\n    \ndef rmse(y_true, y_pred):\n    print('RMSE is {}'.format(mean_squared_error(y_true, y_pred)))\n    return mean_squared_error(y_true, y_pred)\n\nsns.set(rc = {'figure.figsize':(12,7)})","2caa86ee":"# Linear Regression with Ordinary Least Squares:\n\nlrm = LinearRegression().fit(x_train, y_train)\ncoe1 = pd.DataFrame(list(zip(X.columns, lrm.coef_)), columns=['Variables', 'Coefficients'])\nprint(coe1)\n\ny_train_pred = lrm.predict(x_test)\n\nr_square(lrm, x_test, y_test)\nrmse(y_test, y_train_pred)\n\nplt.scatter(y_test, y_train_pred, alpha=0.75)\nplt.xlabel('y_true'); plt.ylabel('y_predict')\n","ed23dce4":"# Linear Regression with Ridge Regression:\n\nalphas_rr=[0.0001, 0.0003, 0.0005, 0.0007, 0.0009, 0.01, 0.05, 0.1, 0.5, 1, 3, 5, 10, 30, 50, 100]\nr_squares = []\nrmses = []\n\nfor alpha in alphas_rr:\n    \n    rrm = Ridge(alpha).fit(x_train, y_train)\n    coe2 = pd.DataFrame(list(zip(X.columns, rrm.coef_)), columns=['Variables', 'Coefficients'])\n    print(coe2)\n\n    y_train_pred = rrm.predict(x_test)\n\n    print('alpha is {}'.format(alpha))\n    r_squares.append(r_square(rrm, x_test, y_test))\n    rmses.append(rmse(y_test, y_train_pred))\n\n    plt.scatter(y_test, y_train_pred, alpha=0.75)\n    plt.xlabel('y_true'); plt.ylabel('y_predict')\n    plt.show()\n    print()\n\nplt.plot(alphas_rr, rmses)","af8bd75c":"# Linear Regression with Lasso Regression:\n\nalphas_lar=[0.0001, 0.0003, 0.0005, 0.0007, 0.0009, 0.01, 0.05, 0.1]\nr_squares = []\nrmses = []\n\nfor alpha in alphas_lar:\n    \n    larm = Lasso(alpha).fit(x_train, y_train)\n    coe2 = pd.DataFrame(list(zip(X.columns, larm.coef_)), columns=['Variables', 'Coefficients'])\n    print(coe2)\n\n    y_train_pred = larm.predict(x_test)\n\n    print('alpha is {}'.format(alpha))\n    r_squares.append(r_square(rrm, x_test, y_test))\n    rmses.append(rmse(y_test, y_train_pred))\n\n    plt.scatter(y_test, y_train_pred, alpha=0.75)\n    plt.xlabel('y_true'); plt.ylabel('y_predict')\n    plt.show()\n    print()\n    \nplt.plot(alphas_lar, rmses)","98bf553f":"# Linear Regression with XGBoost:\n\nxgbr = XGBRegressor(max_depth=5, learning_rate=0.01, n_estimators=1460, objective='reg:linear').fit(x_train, y_train)\n\ny_train_pred = xgbr.predict(x_test)\n\nr_square(xgbr, x_test, y_test)\nrmse(y_test, y_train_pred)\n\nplt.scatter(y_test, y_train_pred, alpha=0.75)\nplt.xlabel('y_true'); plt.ylabel('y_predict')\n","ff115746":"# Use Linear Regression with XGBoost to predict data:\n\n# Data preparation:\ntest['Log1stFlrSF'] = np.log(test['1stFlrSF'])\ntest['LogGrLivArea'] = np.log(test['GrLivArea'])\ntest.loc[test['YearRemodAdd']==test['YearBuilt'], 'RemodAdd'] = 0\ntest['RemodAdd'].fillna(1, inplace=True)\ntest['Cross'] = test['YearBuilt'] * test['RemodAdd']\n\nfinal_cols = ['OverallQual', 'YearBuilt', 'Cross', 'Log1stFlrSF', 'FullBath', 'LogGrLivArea', 'GarageCars']\n\ntest_data = test[final_cols]\ntest_data.head()\n","1bdd0fdc":"# Model creation and data predition:\n\nmodel = XGBRegressor(max_depth=5, learning_rate=0.01, n_estimators=1460, objective='reg:linear').fit(X, Y)\n\ny_test_pred = model.predict(test_data)\nprint('Initiate Predictions are {}'.format(y_test_pred))\n\nfinal_predictions = np.exp(y_test_pred)\nprint('Final Predictions are {}'.format(final_predictions))\n","10974786":"# Form result:\n\nsubmission = pd.DataFrame({'Id':test['Id'], 'SalePrice':final_predictions})\nsubmission.head()","aa578960":"# Data submission:\n\nsubmission.to_csv('submission.csv', index=False)","1662d82f":"<font face='\u5fae\u8f6f\u96c5\u9ed1' size='3'>\n    This part, we can use the data processed above for machine learning and submission of prediction.<br\/>\n<\/font>","8bd9c02a":"<font face='\u5fae\u8f6f\u96c5\u9ed1' size='3'>\n    Before model, feature engineering is important to help us work smartly. We use simple method in this version. And maybe we will improve this part in the next moment.<br\/>\n    As the variables we choose have no missing data, we needn't do something else in this side.\n<\/font>","186ef3b4":"<font face='\u5fae\u8f6f\u96c5\u9ed1' size=4>1.  Packages Importing and Data Loading<\/font>","1c5bfe76":"<font face='\u5fae\u8f6f\u96c5\u9ed1' size='3'>\n    Next, we want to search the relationship between every variables we choose and price as well as to detect outliers. We Use scatter plot to discover the relationship between quantitative variables and price, and box plot is used to detect the relationship between categorical variables and price.\n<\/font>","da905902":"<font face='\u5fae\u8f6f\u96c5\u9ed1' size='3'>\n    As you see, some variables have strong correlation with others and always may indicate the same meaning in some sense. Thus, we can choose the most representative one of them to research or use PCA to transform them to a comprehensive variable. Here, we use the former firstly, and we may use the latter in the future. <br\/>\n    That is, <br\/>\n    YearBuilt\/YearRemodAdd: YearBuilt can be representitive, and we find that remodel can increase price so that we add 'RemodAdd' to control the influence of remodel or not.<br\/>\n    TotalBsmtSF\/1stFlrSF: We choose '1stFlrSF'.<br\/>\n    GrLivArea\/TotRmsAbvGrd: 'GrLivArea' is selected.<br\/>\n    GarageCars\/GarageArea: 'GarageCars' is on.<br\/>\n    Finally, the varibles we use for machine learning include 'OverallQual', 'YearBuilt', 'TotalBsmtSF', 'FullBath', 'GrLivArea' and 'GarageCars'.<br\/>    \n<\/font>","b6da6860":"<font face='\u5fae\u8f6f\u96c5\u9ed1' size=4>0.  Introduction<\/font>","f33c8c34":"<font face='\u5fae\u8f6f\u96c5\u9ed1' size=4>2.  Main Feature Analysis -- House Prices<\/font>","5ac205e3":"<font face='\u5fae\u8f6f\u96c5\u9ed1' size='3'>\n    We have looked for a chance to enhance the ability of data science. This is a perfect competition for data science students who are looking to expand their skill set before trying a featured competition. Besides, problem of house is always on the way in reality for most people. Thus, we do this research on kaggle.<br\/>\n    As we all know, we must definite what's the need and what's the goal when we start a project.<br\/>\n    Ask a home buyer to describe their dream house, price may be important in their mind. Of course, the features of house may have influence on their ideal price with different house. In this kernel, we will find relationship between these features and price. And by all of these, our goal is to predict the price with less loss as possible.<br\/>\n<\/font>","43b25b7a":"<font face='\u5fae\u8f6f\u96c5\u9ed1' size='3'>Now, we have got result with as less as loss we can. Ways of improvement are finding may with some more features and more methods in some part of this research.<\/font>","63f8d414":"<font face='\u5fae\u8f6f\u96c5\u9ed1' size='3'>\nThe deeper the color is, the deeper relationship  it indicates. So, we reserve those variables whose correlation coefficient with price is lager than 0.5. Fortunately, there are 10 variables remained which may explain price we expected before. They are:<br\/>\n<br\/>\nOverallQual: Rates the overall material and finish of the house. <br\/>\nYearBuilt: Original construction date.<br\/>\nYearRemodAdd: Remodel date (same as construction date if no remodeling or additions).<br\/>\nTotalBsmtSF: Total square feet of basement area.<br\/>\n1stFlrSF: First Floor square feet.<br\/>\nGrLivArea: Above grade (ground) living area square feet.<br\/>\nFullBath: Full bathrooms above grade.<br\/>\nTotRmsAbvGrd: Total rooms above grade (does not include bathrooms).<br\/>\nGarageCars: Size of garage in car capacity.<br\/>\nGarageArea: Size of garage in square feet.<br\/>\n<\/font>","3f43fe7a":"<font face='\u5fae\u8f6f\u96c5\u9ed1' size='5'>House Prices Prediction with Machine Learning<br\/><\/font>\n<font face='\u5fae\u8f6f\u96c5\u9ed1' size='3'>Ann Yan  -  Desember 2018<\/font><br\/>","3ff79eee":"<font face='\u5fae\u8f6f\u96c5\u9ed1' size='3'>\n    As the purpose for our research, we should focus on attributes that related to price now. So, we have had a look at the relationship between all variables and price firstly. And then, we research the features that more related to price detailedly.\n<\/font>","be12d82d":"<font face='\u5fae\u8f6f\u96c5\u9ed1' size=4>3.  Other Features Analysis <\/font>","625caa56":"<font face='\u5fae\u8f6f\u96c5\u9ed1' size='3'> \n    Obviously, the 'SalePrice' is our dependent variables in this project. So, let's have a look at it firstly. <br\/>\n    According to theories, linear regression model is based on the hypothesis that dependent variable must obey the normal distribution. Thus, we also have to have a look at the log of the price.<br\/>\n    We can find that, log of price obeys normal distribution, and we have a uniform data sets.\n<\/font>","5f4d4339":"<font face='\u5fae\u8f6f\u96c5\u9ed1' size=4>5.  Model Selection and Training<\/font>","101e3c69":"<font face='\u5fae\u8f6f\u96c5\u9ed1' size='3'>\n    Initially, to enhance readability for this project, we import all packages necessarily and load data we need. Then, we have a simple description on database as following.\n<\/font>","0b5013bd":"<font face='\u5fae\u8f6f\u96c5\u9ed1' size='3'>Because linear regression with XGBoost has the least loss, we choose this method. And the final result as following.<\/font><br\/>","7220ce18":"<font face='\u5fae\u8f6f\u96c5\u9ed1' size='3'>\n    And we use log function to standardize varibles in this kernel. The same, we'll explore more methods.\n<\/font>","18f46f1c":"<font face='\u5fae\u8f6f\u96c5\u9ed1' size=4>4. Data Processing and Feature Engineering<\/font>","4986e65a":"<font face='\u5fae\u8f6f\u96c5\u9ed1' size=4> 7.  Conclusion<\/font>","24f3c045":"<font face='\u5fae\u8f6f\u96c5\u9ed1' size=4>6.  Data Prediction and Submission<\/font>"}}