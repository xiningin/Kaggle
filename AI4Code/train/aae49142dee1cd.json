{"cell_type":{"9c33bdd9":"code","5219d680":"code","4ad0fca9":"code","3b888995":"code","582ed870":"code","04a53007":"code","baa0ddaf":"code","1d181b5b":"code","67b8c61a":"code","a645b178":"code","e3a622ae":"code","aeab7f8f":"code","2381765c":"code","51779f32":"code","16288f11":"code","2a5943b9":"code","cf5ffd11":"code","e199420f":"code","f17e3ab9":"code","40c064f2":"code","fb33d496":"code","652d5f4f":"code","0c899521":"code","6470e8ee":"code","7e7b5880":"markdown","372b6c4c":"markdown","15103f75":"markdown","85083c65":"markdown"},"source":{"9c33bdd9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nimport cv2\n\nfrom tensorflow.keras.applications import vgg16\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import layers, models, optimizers, callbacks\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.experimental import enable_hist_gradient_boosting  # noqa\nfrom sklearn.ensemble import HistGradientBoostingClassifier, VotingClassifier, RandomForestClassifier\nfrom sklearn.svm import SVC\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline, make_pipeline\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5219d680":"!ls ..\/input\/identify-the-dance-form\/","4ad0fca9":"train_labels = pd.read_csv(\"..\/input\/identify-the-dance-form\/train.csv\")\ntrain_labels.head()","3b888995":"img = cv2.imread('\/kaggle\/input\/identify-the-dance-form\/train\/404.jpg')\nplt.imshow(img)\nplt.title(train_labels[train_labels.Image==\"404.jpg\"].target.values[0])\nplt.show()","582ed870":"def load_data(df, path):\n    images = []\n    labels = []\n    for i in zip(df.values):\n        file = i[0][0]\n        label = i[0][1]\n        image = cv2.resize(cv2.imread(path+file), \n                           (224,224))\n        image = vgg16.preprocess_input(image)\n        images.append(image)\n        labels.append(label)\n    return np.array(images), np.array(labels)\n\nimage_path = \"\/kaggle\/input\/identify-the-dance-form\/train\/\"\n\nX, y = load_data(train_labels, image_path)","04a53007":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25)\n\nencoder = OneHotEncoder(sparse=False)\ny_train = encoder.fit_transform(y_train.reshape(-1,1))\ny_val = encoder.transform(y_val.reshape(-1,1))","baa0ddaf":"data_gen = ImageDataGenerator(rotation_range=30,\n                              width_shift_range=0.25,\n                              height_shift_range=0.25,\n                              shear_range=0.2,\n                              zoom_range=0.3,\n                              horizontal_flip=True,\n                              fill_mode='nearest')\n\ndata_gen.fit(X_train)","1d181b5b":"num_classes = len(np.unique(y))\n\nmodel = vgg16.VGG16(include_top=False, weights='imagenet', \n                    pooling='avg', input_shape = (224, 224, 3))\n\n\n\nfor layer in model.layers[:17]:\n    layer.trainable = False\n\n\n\nx = layers.Dense(1024, activation='relu')(model.output)\nx = layers.Dense(512, activation='relu')(x)\noutput = layers.Dense(num_classes, activation='softmax')(x)\n\nmodel = models.Model(model.input, output)\n\nmodel.compile(loss=\"categorical_crossentropy\",\n              optimizer=optimizers.RMSprop(lr = 0.001),\n              metrics=['accuracy'])","67b8c61a":"lr_reduce = callbacks.ReduceLROnPlateau(monitor='val_loss', \n                                        patience=2, \n                                        verbose=1, \n                                        factor=0.5, \n                                        min_lr=0.00001)\n\nhist = model.fit(data_gen.flow(X_train, y_train, batch_size=64),\n                 steps_per_epoch=len(X_train)\/64, epochs=50,\n                 validation_data=(X_val, y_val),\n                 callbacks = [lr_reduce])","a645b178":"plt.figure(figsize=(10,7))\nplt.subplot(121)\nplt.plot(hist.history[\"loss\"])\nplt.plot(hist.history[\"val_loss\"])\nplt.legend([\"train\",\"val\"])\n\nplt.subplot(122)\nplt.plot(hist.history[\"accuracy\"])\nplt.plot(hist.history[\"val_accuracy\"])\nplt.legend([\"train\",\"val\"])\n\nplt.show()","e3a622ae":"extractor = models.Model(model.input, model.layers[-3].output)","aeab7f8f":"X_new_train = extractor.predict(X_train)\nX_new_val = extractor.predict(X_val)\n\nscaler = StandardScaler()\nX_new_train = scaler.fit_transform(X_new_train)\nX_new_val = scaler.transform(X_new_val)\n\nprint(\"Train Shape:\", X_new_train.shape, \", Val Shape:\", X_new_val.shape)","2381765c":"y_new_train = encoder.inverse_transform(y_train).reshape(-1)\ny_new_val = encoder.inverse_transform(y_val).reshape(-1)","51779f32":"learning_rate = np.linspace(1e-3, 1, num=500)\n\nmax_iter = list(range(50,301))\n\nl2_regularization = np.linspace(0, 5, num=500)\n\nmax_leaf_nodes = list(range(10, 51))\n\nmax_depth = list(range(3, 21))\n\nmin_samples_leaf = list(range(10,51))\n\n# Create the random grid\nrandom_grid = {'learning_rate': learning_rate,\n               'max_iter': max_iter,\n               'max_leaf_nodes': max_leaf_nodes,\n               'l2_regularization': l2_regularization,\n               'max_depth': max_depth,\n               'min_samples_leaf':min_samples_leaf}\n\nrandom_grid = {'histgradientboostingclassifier__' + key: random_grid[key] for key in random_grid}\n\nsmote_par = {\"mohiniyattam\":60, \"odissi\":60, \"bharatanatyam\":60,\n             \"kathakali\":60, \"sattriya\":60, \"kathak\":60,\n             \"kuchipudi\":60, \"manipuri\":60}\nclf = make_pipeline(SMOTE(smote_par),\n                    HistGradientBoostingClassifier(loss=\"categorical_crossentropy\"))\n\ngb_random = RandomizedSearchCV(clf, random_grid, n_iter=50, cv=3,\n                              random_state=1, n_jobs=-1, refit=True)\n    \ngb_random.fit(X_new_train, y_new_train)","16288f11":"print(gb_random.best_score_)\ngb_random.best_estimator_","2a5943b9":"y_pred = gb_random.best_estimator_.predict(X_new_val)\n\nprint(classification_report(y_new_val,y_pred,\n      labels=np.unique(y_new_val)))\n\nsns.heatmap(confusion_matrix(y_new_val,y_pred), \n            xticklabels=np.unique(y_new_val),\n            yticklabels=np.unique(y_new_val),\n            annot=True, cbar=False, cmap=\"Blues\")\n\nplt.show()","cf5ffd11":"random_grid = {'C': [0.1, 1, 5, 10, 100], \n              'gamma': [1,0.1,0.01,0.001, 0.0005],\n              'degree': [2, 3, 4],\n              'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n\nrandom_grid = {'svc__' + key: random_grid[key] for key in random_grid}\n\nsmote_par = {\"mohiniyattam\":60, \"odissi\":60, \"bharatanatyam\":60,\n             \"kathakali\":60, \"sattriya\":60, \"kathak\":60,\n             \"kuchipudi\":60, \"manipuri\":60}\nclf = make_pipeline(SMOTE(smote_par),\n                    SVC())\n\nsvm_random = RandomizedSearchCV(clf, random_grid, n_iter=50, cv=3,\n                              random_state=1, n_jobs=-1, refit=True)\n    \nsvm_random.fit(X_new_train, y_new_train)","e199420f":"print(svm_random.best_score_)\nsvm_random.best_estimator_","f17e3ab9":"y_pred = svm_random.best_estimator_.predict(X_new_val)\n\nprint(classification_report(y_new_val,y_pred,\n      labels=np.unique(y_new_val)))\n\nsns.heatmap(confusion_matrix(y_new_val,y_pred), \n            xticklabels=np.unique(y_new_val),\n            yticklabels=np.unique(y_new_val),\n            annot=True, cbar=False, cmap=\"Blues\")\n\nplt.show()","40c064f2":"n_estimators = [int(x) for x in np.linspace(start = 100, stop = 300, num = 11)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum depth of the trees\nmax_depth = [int(x) for x in np.linspace(10, 50, num = 10)]\n\n# Minimum number of samples required for the split\nmin_samples_split = [2, 5, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4, 6, 8, 10]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nrandom_grid = {'randomforestclassifier__' + key: random_grid[key] for key in random_grid}\n\nsmote_par = {\"mohiniyattam\":60, \"odissi\":60, \"bharatanatyam\":60,\n             \"kathakali\":60, \"sattriya\":60, \"kathak\":60,\n             \"kuchipudi\":60, \"manipuri\":60}\n\nclf = make_pipeline(SMOTE(smote_par),\n                    RandomForestClassifier())\n\nrf_random = RandomizedSearchCV(clf, random_grid, n_iter=50, cv=3,\n                              random_state=1, n_jobs=-1, refit=True)\n    \nrf_random.fit(X_new_train, y_new_train)","fb33d496":"print(rf_random.best_score_)\nrf_random.best_estimator_","652d5f4f":"y_pred = rf_random.best_estimator_.predict(X_new_val)\n\nprint(classification_report(y_new_val,y_pred,\n      labels=np.unique(y_new_val)))\n\nsns.heatmap(confusion_matrix(y_new_val,y_pred), \n            xticklabels=np.unique(y_new_val),\n            yticklabels=np.unique(y_new_val),\n            annot=True, cbar=False, cmap=\"Blues\")\n\nplt.show()","0c899521":"ensemble = VotingClassifier([(\"hgb\",gb_random.best_estimator_[\"histgradientboostingclassifier\"]),\n                             (\"svm\",svm_random.best_estimator_[\"svc\"]),\n                             (\"rf\",rf_random.best_estimator_[\"randomforestclassifier\"])])\n\nclf = make_pipeline(SMOTE(smote_par),\n                    ensemble)\nclf.fit(X_new_train, y_new_train)\n\ny_pred = clf.predict(X_new_val)\n\nprint(classification_report(y_new_val,y_pred,\n      labels=np.unique(y_new_val)))","6470e8ee":"sns.heatmap(confusion_matrix(y_new_val,y_pred), \n            xticklabels=np.unique(y_new_val),\n            yticklabels=np.unique(y_new_val),\n            annot=True, cbar=False, cmap=\"Blues\")\n\nplt.show()","7e7b5880":"# Ensemble of the models\nLet's create an hard voting ensemble from the three previous models","372b6c4c":"# New model with hyper parameter optimization\n\nIn the last part we use a model which will be optimized using a 3-CV. And SMOTE will be used for data augmentation!","15103f75":"# Transfer Learning for Feature Extraction\n\nHere we will train\/fine tune our NN model using transfer learning and cut the model to the second last layer and put a more complex model for the final prediction. And a ImageDataGenerator is used for data augmentation to avoid overfitting of the train set. ","85083c65":"# Feature Extraction"}}