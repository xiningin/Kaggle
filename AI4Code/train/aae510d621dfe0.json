{"cell_type":{"6fea31c3":"code","5a124f74":"code","80f07774":"code","cf241d1e":"code","8da31c64":"code","ddff6243":"code","853b5d73":"code","6a7fd987":"code","e5597095":"code","ccdb93b4":"code","80cd9408":"code","10d3c7bf":"code","9716ae9d":"code","19ac54e5":"code","07f5155f":"code","11584dba":"code","5a18ef1e":"code","af6e2ca6":"code","51e8145d":"code","bdf4fd49":"code","8a94c65a":"code","dd2dd840":"code","1dde8d42":"code","20811b55":"code","7b03df42":"code","562d7299":"code","f6258cbf":"code","f0b6ff0f":"code","5f222054":"code","12aebd28":"code","31ac45fe":"code","29a4db8d":"code","895affd9":"code","61635f41":"code","5ef66b5f":"code","fa1a3406":"code","af2dad0b":"code","09cc464a":"code","0048ca81":"code","2d1fabed":"code","af4e1671":"code","8aaca885":"code","e1b7b2ed":"code","b34ea77c":"code","7dbf0d79":"code","2a062a34":"code","db54e0f4":"code","05767636":"code","74709fd6":"code","cbea00ae":"code","03106263":"code","279f35ea":"code","10cc0a8b":"code","73c89498":"code","7610fc8b":"code","7c7033c3":"code","73f11fc4":"code","c3833aab":"code","8281c4c6":"code","f8a22129":"markdown"},"source":{"6fea31c3":"#ECG time series anomaly detection adapted and modified from work by: https:\/\/curiousily.com\/posts\/time-series-anomaly-detection-using-lstm-autoencoder-with-pytorch-in-python\/","5a124f74":"!pip install -qq arff2pandas","80f07774":"!pip install missingno","cf241d1e":"from sklearn.metrics import confusion_matrix, classification_report\nimport torch\nimport missingno\nimport copy\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom torch import nn, optim\n\nimport torch.nn.functional as F\nfrom arff2pandas import a2p\nimport os\nimport shutil\nRANDOM_SEED = 10\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nfrom tqdm import tqdm\nfrom tensorflow.keras.models import Model\n","8da31c64":"path = '..\/input\/ecg-time-series'\nwith open(path + '\/ECG5000_TRAIN.arff') as f:\n\n  train = a2p.load(f)\n\nwith open(path + '\/ECG5000_TEST.arff') as f:\n\n  test = a2p.load(f)","ddff6243":"test.head()\n","853b5d73":"df = train.append(test)\n\ndf = df.sample(frac=1.0)\n\ndf.shape","6a7fd987":"CLASS_NORMAL = 1\n\nclass_names = ['Normal','R on T','PVC','SP','UB']","e5597095":"new_columns = list(df.columns)\n\nnew_columns[-1] = 'target'\n\ndf.columns = new_columns","ccdb93b4":"df.target.nunique()","80cd9408":"df.target.value_counts()","10d3c7bf":"ax = sns.countplot(df.target)\nax.set_xticklabels(class_names);","9716ae9d":"missingno.matrix(df)","19ac54e5":"def plot_ECG_class(data, class_name, ax, n_steps=10):\n  time_series_df = pd.DataFrame(data)\n\n  smooth_path = time_series_df.rolling(n_steps).mean()\n  path_deviation = time_series_df.rolling(2, win_type='gaussian').sum(std=1)#2 * time_series_df.rolling(n_steps).std()\n\n  under_line = (smooth_path - path_deviation)[0]\n  over_line = (smooth_path + path_deviation)[0]\n\n  ax.plot(smooth_path, linewidth=2)\n  ax.fill_between(\n    path_deviation.index,\n    under_line,\n    over_line,\n    alpha=.125\n  )\n  ax.set_title(class_name)","07f5155f":"classes = df.target.unique()    #This block calls above func\n\nfig, axs = plt.subplots(\n  nrows=len(classes) \/\/ 3 + 1,\n  ncols=3,\n  sharey=True,\n  figsize=(14, 8)\n)\n\nfor i, cls in enumerate(classes):\n  ax = axs.flat[i]\n  data = df[df.target == cls] \\\n    .drop(labels='target', axis=1) \\\n    .mean(axis=0) \\\n    .to_numpy()# cls first argument in class method? Why?\n  plot_ECG_class(data, class_names[i], ax)  #######\n\nfig.delaxes(axs.flat[-1])\nfig.tight_layout();","11584dba":"#Train autoencoder to reconstruct input ","5a18ef1e":"#first split up dataframe into a normal set and an anomalous set (merged anomalous classes), then further split norm and anom in a train test split ","af6e2ca6":"###Prep data for use in tensorflow as well as pytorch\n\n#train_val = train.values\n#test_val = test.values\nmin_val = train.min()\nmax_val = train.max()\n\n\ntrain_val = train.values\ntest_val = test.values\n\ntrain_norm_tf = train.copy()\ntest_norm_tf = test.copy()\n\n\ndf_tf = train_norm_tf.append(test_norm_tf)\nnew_columns_tf = list(df_tf.columns)\n\nnew_columns_tf[-1] = 'target'\n\ndf_tf.columns = new_columns_tf\ncols = list(df_tf.columns)\ncols.remove(\"target\")\nfor col in cols:\n    df_tf[col]=(df_tf[col] - df_tf[col].min()) \\\n    \/ (df_tf[col].max() - df_tf[col].min())   # normalisation\n\n\n\n","51e8145d":"#Tensorflow test\/train sets\n\n#labels = df_tf['target']\n\n\n#data = df_tf.loc[:, df_tf.columns != 'target']\n\n#train_data_tf, test_data_tf, train_labels_tf, test_labels_tf = train_test_split(\n #   data, labels, test_size=0.2, random_state=25\n#)\n\n\n","bdf4fd49":"norm_df = df[df.target == str(CLASS_NORMAL)].drop(labels='target', axis=1)  #norm here means healthy\nnorm_df_tf = df_tf[df_tf.target == str(CLASS_NORMAL)].drop(labels='target', axis=1)\n\nprint(norm_df.shape, \" = normal results shape vs all results shape = \" , df.shape)\nprint(norm_df_tf.shape, \" = normal results TF shape vs all results shape = \" , df_tf.shape)","8a94c65a":"df_tf.target.value_counts()","dd2dd840":"anomaly_df = df[df.target != str(CLASS_NORMAL)].drop(labels='target', axis=1)\nanomaly_df_tf = df_tf[df_tf.target != str(CLASS_NORMAL)].drop(labels='target', axis=1)\n\nanomaly_df.shape\n\n(2081, 140)","1dde8d42":"class AnomalyDetector(Model):   # Tensorflow \n  def __init__(self):\n    super(AnomalyDetector, self).__init__()\n    self.encoder = tf.keras.Sequential([\n      layers.Dense(32, activation=\"relu\"),\n      layers.Dense(16, activation=\"relu\"),\n      layers.Dense(8, activation=\"relu\")])\n\n    self.decoder = tf.keras.Sequential([\n      layers.Dense(16, activation=\"relu\"),\n      layers.Dense(32, activation=\"relu\"),\n      layers.Dense(140, activation=\"sigmoid\")])\n\n  def call(self, x):\n    encoded = self.encoder(x)\n    decoded = self.decoder(encoded)\n    return decoded\n\nautoencoder = AnomalyDetector()\nautoencoder.compile(optimizer='adam', loss='msle')\n\n#from tensorflow docs","20811b55":"normal_train_data, normal_test_data = train_test_split(\n    norm_df_tf,  test_size=0.2, random_state=21)\nanomalous_train_data, anomalous_test_data = train_test_split(\n    anomaly_df_tf,  test_size=0.2, random_state=21)\ntrain_data_tf, test_data_tf = train_test_split(\n    df_tf,  test_size=0.2, random_state=21)","7b03df42":"history = autoencoder.fit(normal_train_data, normal_train_data, \n          epochs=40, \n          batch_size=512,\n          validation_data=(normal_test_data, normal_test_data),\n          shuffle=True)\n# tensorflow simpler?","562d7299":"autoencoder.encoder.summary()\n","f6258cbf":"plt.plot(history.history[\"loss\"], label=\"Training Loss\")\nplt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\nplt.legend()\n","f0b6ff0f":"reconstructions = autoencoder.predict(normal_train_data)\ntrain_loss = tf.keras.losses.msle(reconstructions, normal_train_data)\ntrain_loss.shape\n","5f222054":"threshold = np.mean(train_loss) + np.std(train_loss)\nprint(\"Threshold: \", threshold)\n","12aebd28":"reconstructions = autoencoder.predict(anomalous_test_data)\ntest_loss = tf.keras.losses.mae(reconstructions, anomalous_test_data)\n\n","31ac45fe":"def predict(model, data, threshold):\n  reconstructions = model(data)\n  loss = tf.keras.losses.mae(reconstructions, data)\n  return tf.math.less(loss, threshold)\n\ndef print_stats(predictions, labels):\n  print(\"Accuracy = {}\".format(accuracy_score(labels, preds)))\n  print(\"Precision = {}\".format(precision_score(labels, preds)))\n  print(\"Recall = {}\".format(recall_score(labels, preds)))\n","29a4db8d":"preds = predict(autoencoder, test_data_tf, threshold)\nprint_stats(preds, test_labels)\n","895affd9":"#Now onto pytorch\n\ntrain_df, validation_df = train_test_split(      \n\n  norm_df,\n\n  test_size=0.15,\n\n  random_state=RANDOM_SEED\n\n)     ## Take the healthy set, prepare model to recognise normal PYTORCH\n\nvalidation_df, test_df = train_test_split(\n\n  validation_df,\n\n  test_size=0.33,\n\n  random_state=RANDOM_SEED\n\n)","61635f41":"def tensorflow_tensor(df_tf):\n    return tf.convert_to_tensor(df_tf)","5ef66b5f":"#Convert to tensor to prep for feeding into autoencoder for pytorch","fa1a3406":"def tensor_maker(df):\n\n  dftolist = df.astype(np.float32).to_numpy().tolist()\n\n  dataset = [torch.tensor(d).unsqueeze(1).float() for d in dftolist]  # unsqueeze Returns a new tensor with a\n                                                                        #dimension of size one inserted at the specified position.\n\n  n_seq, seq_len, n_features = torch.stack(dataset).shape\n\n  return dataset, seq_len, n_features\n\n","af2dad0b":"# 1D time series to 2D tensor (sequence elements x no of features)\n","09cc464a":"train_dataset, seq_len, n_features = tensor_maker(train_df)\n\nval_dataset, _, _ = tensor_maker(validation_df) # _ to ignore values when unpacking\n\ntest_normal_dataset, _, _ = tensor_maker(test_df)\n\ntest_anomaly_dataset, _, _ = tensor_maker(anomaly_df)","0048ca81":"#Questions, equivalent of PCA to lower dimensionality? or is the compressed step of the encoder enough?","2d1fabed":"#Encoder class, decoder class","af4e1671":"class Encoder(nn.Module):\n\n  def __init__(self, seq_len, n_features, embedding_dim=64): # Embedding like NLP \n\n    super(Encoder, self).__init__()\n\n    self.seq_len, self.n_features = seq_len, n_features\n\n    self.embedding_dim, self.hidden_dim = embedding_dim, int(1.5 * embedding_dim)    # Ideal hidden layer size?\n\n    self.rnn1 = nn.LSTM(   # recurrent neural network rnn, convolutional network maybe?\n\n      input_size=n_features,\n\n      hidden_size=self.hidden_dim,\n\n      num_layers=1,\n\n      batch_first=True\n\n    )\n\n    self.rnn2 = nn.LSTM(\n\n      input_size=self.hidden_dim,\n\n      hidden_size=embedding_dim,\n\n      num_layers=1,\n\n      batch_first=True\n\n    )\n\n  def forward(self, x):\n\n    x = x.reshape((1, self.seq_len, self.n_features))\n\n    x, (_, _) = self.rnn1(x)\n\n    x, (hidden_n, _) = self.rnn2(x)\n\n    return hidden_n.reshape((self.n_features, self.embedding_dim))","8aaca885":"class Decoder(nn.Module):\n\n  def __init__(self, seq_len, input_dim=64, n_features=1):\n\n    super(Decoder, self).__init__()\n\n    self.seq_len, self.input_dim = seq_len, input_dim\n\n    self.hidden_dim, self.n_features = int(1.5 * input_dim), n_features \n\n    self.rnn1 = nn.LSTM(\n\n      input_size=input_dim,\n\n      hidden_size=input_dim,\n\n      num_layers=1,\n\n      batch_first=True\n\n    )\n\n    self.rnn2 = nn.LSTM(\n\n      input_size=input_dim,\n\n      hidden_size=self.hidden_dim,\n\n      num_layers=1,\n\n      batch_first=True\n\n    )\n\n    self.output_layer = nn.Linear(self.hidden_dim, n_features)  # Why linear transformation?\n\n  def forward(self, x):\n\n    x = x.repeat(self.seq_len, self.n_features)\n\n    x = x.reshape((self.n_features, self.seq_len, self.input_dim))\n\n    x, (hidden_n, cell_n) = self.rnn1(x)\n\n    x, (hidden_n, cell_n) = self.rnn2(x)\n\n    x = x.reshape((self.seq_len, self.hidden_dim))\n\n    return self.output_layer(x)","e1b7b2ed":"#Create a class which contains the encoder, decoder classes","b34ea77c":"class RecurrentAutoencoder(nn.Module):\n\n  def __init__(self, seq_len, n_features, embedding_dim=64):\n    super(RecurrentAutoencoder, self).__init__()\n\n    self.encoder = Encoder(seq_len, n_features, embedding_dim).to(device)\n    self.decoder = Decoder(seq_len, embedding_dim, n_features).to(device)\n\n  def forward(self, x):\n    x = self.encoder(x)\n    x = self.decoder(x)\n\n    return x","7dbf0d79":"model = RecurrentAutoencoder(seq_len, n_features, 128)  # Instantiate controller class\n\nmodel = model.to(device) ","2a062a34":"def train_model(model, train_dataset, val_dataset, n_epochs):\n\n  optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)  \n\n  criterion = nn.L1Loss(reduction='sum').to(device) # Creates a criterion that measures the mean absolute error (MAE) \n    #between each element in the input xxx and target yyy .\n\n  history = dict(train=[], val=[])\n\n  best_model_wts = copy.deepcopy(model.state_dict())\n\n  best_loss = 10000.0\n\n  for epoch in range(1, n_epochs + 1):\n\n    model = model.train()\n\n    train_losses = []\n\n    for seq_true in train_dataset:\n\n      optimizer.zero_grad() #In PyTorch, we need to set the gradients to zero before starting to do backpropragation because PyTorch\n    #accumulates the gradients on subsequent backward passes. This is convenient while training rnn's - stackoverflow\n\n      seq_true = seq_true.to(device)\n\n      seq_pred = model(seq_true)\n\n      loss = criterion(seq_pred, seq_true)\n\n      loss.backward()\n\n      optimizer.step()\n\n      train_losses.append(loss.item())\n\n    val_losses = []\n\n    model = model.eval()\n\n    with torch.no_grad():\n\n      for seq_true in val_dataset:\n\n        seq_true = seq_true.to(device)\n\n        seq_pred = model(seq_true)\n\n        loss = criterion(seq_pred, seq_true)\n\n        val_losses.append(loss.item())\n\n    train_loss = np.mean(train_losses)\n\n    val_loss = np.mean(val_losses)\n\n    history['train'].append(train_loss)\n\n    history['val'].append(val_loss)\n\n    if val_loss < best_loss:\n\n      best_loss = val_loss\n\n      best_model_wts = copy.deepcopy(model.state_dict())\n\n    print(f'Epoch {epoch}: train loss {train_loss} val loss {val_loss}')\n\n  model.load_state_dict(best_model_wts)\n\n  return model.eval(), history","db54e0f4":"model, history = train_model(\n  model, \n  train_dataset, \n  val_dataset, \n  n_epochs=25\n)","05767636":"ax = plt.figure().gca()\n\nax.plot(history['train'])\nax.plot(history['val'])\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'])\nplt.title('Loss over training epochs')\nplt.show();","74709fd6":"MODEL_PATH = 'model.pth'\n\ntorch.save(model, MODEL_PATH)","cbea00ae":"def predict(model, dataset):\n  predictions, losses = [], []\n  criterion = nn.L1Loss(reduction='sum').to(device)\n  with torch.no_grad():\n    model = model.eval()\n    for seq_true in dataset:\n      seq_true = seq_true.to(device)\n      seq_pred = model(seq_true)\n\n      loss = criterion(seq_pred, seq_true)\n\n      predictions.append(seq_pred.cpu().numpy().flatten())\n      losses.append(loss.item())\n  return predictions, losses","03106263":"_, losses = predict(model, train_dataset)\n\nsns.distplot(losses, bins=50, kde=True);","279f35ea":"THRESHOLD = 40","10cc0a8b":"predictions, pred_losses = predict(model, test_normal_dataset)\nsns.distplot(pred_losses, bins=50, kde=True);","73c89498":"correct = sum(l <= THRESHOLD for l in pred_losses)\nprint(f'Correct normal predictions: {correct}\/{len(test_normal_dataset)}')\nprint((correct\/len(test_normal_dataset))*100, 'percent')","7610fc8b":"anomaly_dataset = test_anomaly_dataset[:len(test_normal_dataset)]","7c7033c3":"predictions, pred_losses = predict(model, anomaly_dataset)\nsns.distplot(pred_losses, bins=50, kde=True);","73f11fc4":"correct = sum(l > THRESHOLD for l in pred_losses)\nprint(f'Correct anomaly predictions: {correct}\/{len(anomaly_dataset)}' )\nprint((correct\/len(anomaly_dataset))*100, 'percent')","c3833aab":"def plot_prediction(data, model, title, ax):\n  predictions, pred_losses = predict(model, [data])\n\n  ax.plot(data, label='true')\n  ax.plot(predictions[0], label='reconstructed')\n  ax.set_title(f'{title} (loss: {np.around(pred_losses[0], 2)})')\n  ax.legend()","8281c4c6":"fig, axs = plt.subplots(\n  nrows=2,\n  ncols=6,\n  sharey=True,\n  sharex=True,\n  figsize=(22, 8)\n)\n\nfor i, data in enumerate(test_normal_dataset[:6]):\n  plot_prediction(data, model, title='Normal', ax=axs[0, i])\n\nfor i, data in enumerate(test_anomaly_dataset[:6]):\n  plot_prediction(data, model, title='Anomaly', ax=axs[1, i])\n\nfig.tight_layout();","f8a22129":"accuracy could be improved if model was run for more epochs, is the threshold correct?"}}