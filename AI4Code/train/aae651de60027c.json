{"cell_type":{"7f93b997":"code","dd4023e8":"code","926ad8cf":"code","cac464a4":"code","a3a87fd9":"code","2f99de50":"code","b9d4acec":"code","b6cda6ed":"code","bed6d08c":"code","bd92d2e2":"code","07d81ce0":"code","a65b246d":"code","140fe609":"code","c245b2d7":"code","9329d779":"code","486f81ab":"code","ac03eb67":"code","1a8005d6":"code","306bde10":"code","5c19037b":"code","1caee2b5":"code","e191525b":"code","5fffd1c3":"code","b18df3f7":"code","a74d1663":"code","555a38b0":"code","2528bfd6":"code","ccb7979d":"code","2ee1e009":"code","f8aff46f":"markdown","a65b6d26":"markdown","4b71af62":"markdown","66e847d5":"markdown","109e1db9":"markdown","f04e5556":"markdown","08fd0212":"markdown","40d1b870":"markdown","399c64ff":"markdown"},"source":{"7f93b997":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd \nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","dd4023e8":"df = pd.read_csv(\"\/kaggle\/input\/productivity-prediction-of-garment-employees\/garments_worker_productivity.csv\", \n                 parse_dates=['date'])\nprint(df.shape)\ndf.head()","926ad8cf":"# Check for duplicate records.\ndf.duplicated().sum()","cac464a4":"# Check datatypes, null values\ndf.info()","a3a87fd9":"# We only have null values in `work in process`\ndf.isnull().sum()","2f99de50":"# Separate categorical and numerical data for simplicity in analysis\ncategory = df.select_dtypes(include='object')\nnumerical = df.select_dtypes(exclude='object')","b9d4acec":"for c in category.columns:\n    print(f\"{c}\")\n    print(category[c].unique())\n    print()","b6cda6ed":"# Fix an error of department\ncategory.loc[:,'department'] = category.loc[:,'department'].str.strip()","bed6d08c":"numerical.describe()","bd92d2e2":"sns.catplot(kind='box', data=numerical, orient='h');","07d81ce0":"# Assemble a full dataframe\ndf2 = pd.concat([category, numerical], axis=1)","a65b246d":"# Check for missing values\ndf2[df2.isnull().any(axis=1)]","140fe609":"df2[df2.isnull().any(axis=1)]['department'].unique()\n\n# All the missing values are `work in process` of the finishing department\n# Finishing dep. might have to wait for the product from other departments. So, there's no work on process (`wip`=0)","c245b2d7":"# We then fill it with 0\ndf2['wip'] = df2['wip'].fillna(value=0.0)","9329d779":"# Drop date coloumn since it's unnecessary\ndf2.drop(['date'],axis=1,inplace=True)\n\n# Get dummies variables\ndf2_dummies = pd.get_dummies(df2, drop_first=True)","486f81ab":"# Save the features for later use\nfeatures = df2_dummies.drop(['actual_productivity'], axis=1).columns","ac03eb67":"from sklearn.feature_selection import SequentialFeatureSelector\nfrom sklearn.ensemble import RandomForestRegressor\n\nX, y = df2_dummies.drop(['actual_productivity'], axis=1), df2_dummies['actual_productivity']\nrf = RandomForestRegressor()","1a8005d6":"# We can do Forward or Backward selection\n# by specifying `direction` parameter\nsfs = SequentialFeatureSelector(rf, n_features_to_select=7, direction='forward')\nsfs.fit(X, y)","306bde10":"# Get the selected features\nfeatures[sfs.get_support()]","5c19037b":"# Transform to use only selected features\nX_selected = sfs.transform(X)","1caee2b5":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error as mse\n\nX_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=112)\n\nmodel = RandomForestRegressor().fit(X_train, y_train)\n\n# This is the result\ny_pred = model.predict(X_test)\nprint(f'mse = {mse(y_true=y_test, y_pred=y_pred)}')","e191525b":"# Compared to the distribution of target,\n# the result of model built with only 7 features\n# is fairly good.\ndf2['actual_productivity'].hist();","5fffd1c3":"from sklearn.feature_selection import RFE\n\nX, y = df2_dummies.drop(['actual_productivity'], axis=1), df2_dummies['actual_productivity']\n\nestimator = RandomForestRegressor()\nrfe = RFE(estimator, n_features_to_select=7, step=1)\nrfe.fit(X, y)","b18df3f7":"# Transform to use only selected features\nfeatures[rfe.support_]","a74d1663":"X_train, X_test, y_train, y_test = train_test_split(rfe.transform(X), y, test_size=0.2, random_state=112)\n\nmodel = RandomForestRegressor().fit(X_train, y_train)\n\n# This is the result\ny_pred = model.predict(X_test)\nprint(f'mse = {mse(y_true=y_test, y_pred=y_pred)}')","555a38b0":"X,y = df2.drop(['actual_productivity'], axis=1),df2['actual_productivity']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=112)\n\nX_train","2528bfd6":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nrf = RandomForestRegressor()\nsfs = SequentialFeatureSelector(rf, n_features_to_select=7, direction='forward')\n\nct = ColumnTransformer([\n    (\"ohe\", OneHotEncoder(), [0,1,2])\n], remainder='passthrough')\n\npipe = Pipeline([\n    (\"ohe\", ct),\n    (\"standardize\", StandardScaler()),\n    (\"feature selection\", sfs),\n    (\"model\", RandomForestRegressor())\n])","ccb7979d":"pipe.fit(X_train, y_train)","2ee1e009":"# This is the result\ny_pred = pipe.predict(X_test)\nprint(f'mse = {mse(y_true=y_test, y_pred=y_pred)}')","f8aff46f":"## Numerical","a65b6d26":"## Missing values","4b71af62":"`sklearn.feature_selection.SequentialFeatureSelector` is a transformer that performs Sequential Feature Selection. We can treat it like other transformers i.e. use it in `Pipeline`, `ColumnTransformer`.","66e847d5":"## Categorical","109e1db9":"# Add to Pipeline\n---\nWe can do one-hot encoding, standardize, feature selection by using `ColumnTransformer`, `Pipeline`. This section will show an example.","f04e5556":"# Feature selection\n---","08fd0212":"- quarter : A portion of the month. A month was divided into four quarters\n- team : Associated team number with the instance \n- no_of_workers : Number of workers in each team \n- no_of_style_change : Number of changes in the style of a particular product\n- targeted_productivity : Targeted productivity set by the Authority for each team for each day. \n- smv : Standard Minute Value, it is the allocated time for a task \n- wip : Work in progress. Includes the number of unfinished items for products \n- overtime : Represents the amount of overtime by each team in minutes\n- incentive : Represents the amount of financial incentive (in BDT) that enables or motivates a particular course of action.\n- idle_time : The amount of time when the production was interrupted due to several reasons \n- idle_men : The number of workers who were idle due to production interruption\n- actual_productivity : The actual % of productivity that was delivered by the workers. It ranges from 0-1.","40d1b870":"### 2) Recursive feature elimination\nGiven an estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. \n- First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through any specific attribute (such as coef_, feature_importances_). \n- Then, **the least important features are pruned** from current set of features. \n- That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n\nSFS differs from RFE in that it does not require the underlying model to expose a coef_ or feature_importances_ attribute. SFS is base on score obtained by model in each iteration rather than features' importance.","399c64ff":"### 1) Sequential Feature Selector\nForward-SFS is a greedy procedure that iteratively finds the best new feature to add to the set of selected features. \n- Concretely, we initially start with zero feature and **find the one feature that maximizes a cross-validated score** when an estimator is trained on this single feature. \n- Once that first feature is selected, we repeat the procedure by adding a new feature to the set of selected features. \n- The procedure stops when the desired number of selected features is reached, as determined by the n_features_to_select parameter.\n\nBackward-SFS follows the same idea but works in the opposite direction: instead of starting with no feature and greedily adding features, we start with all the features and greedily remove features from the set. The direction parameter controls whether forward or backward SFS is used."}}