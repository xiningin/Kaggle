{"cell_type":{"589d7974":"code","d2080e33":"code","c1dec445":"code","1da2ad94":"code","7934aff8":"code","fd9ba2a8":"code","2e7b18b9":"code","0a3da7dd":"code","2748dc25":"code","630571a6":"code","db409c9c":"code","d4b5739c":"code","d30fc4e4":"markdown","2b53bded":"markdown","965b51a6":"markdown","8dc18ae5":"markdown","728c2fbc":"markdown","637bf5ea":"markdown","1d148923":"markdown"},"source":{"589d7974":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport random\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\n\nimport time\nimport copy\nimport chainer\nimport chainer.functions as F\nimport chainer.links as L\nfrom plotly import tools\nfrom plotly.graph_objs import *\nfrom plotly.offline import init_notebook_mode, iplot, iplot_mpl\nfrom collections import deque\ninit_notebook_mode()\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d2080e33":"# if this flag is set, each step of the environment's state will be printed\nENVIRONMENT_DEBUG = False","c1dec445":"ds=pd.read_csv('..\/input\/USD_TRY Gemi Verileri.csv')\nds['Tarih']=pd.to_datetime(ds['Tarih'], errors='coerce')\nds['\u015eimdi']=pd.to_numeric(ds['\u015eimdi'].str.replace(',', '.'), errors='coerce')\nds['Fark %']=ds['Fark %'].str.replace('%', '')\nds['Fark %']=ds['Fark %'].str.replace(',', '.')\n\nds.head(10)","1da2ad94":"X=ds['\u015eimdi']\nY=ds['Tarih']\nX=np.array(X).reshape((len(X), 1))#i got some issues with shapes but i found this solution on stackoverflow\nY=np.array(Y).reshape((len(Y), 1))\nfig=plt.figure()\nax=fig.add_subplot(111)\nax.plot_date(Y, X, '.')\nplt.show()","7934aff8":"#split train and test data\n\ndate_split = 4000\ntrain = ds[:date_split]\ntest = ds[date_split:]\nlen(train), len(test)","fd9ba2a8":"class Environment:\n    \n    def __init__(self, data, tl, history_t=10):\n        self.data = data\n        self.history_t = history_t\n        self.tl_start = tl;\n        self.reset()\n        \n    def reset(self):\n        self.tl = self.tl_start\n        self.usd = 0;\n        self.done = False\n        self.profits = 0\n        self.current_position = \"none\";\n        self.history = [self.data.iloc[x, :]['Fark %'] for x in range(self.history_t)]\n        self.t = self.history_t;\n        self.last_tl = 0;\n        return self.history # obs\n    \n    def step(self, act):\n        #print(\"new step\");\n        reward = 0\n        # act = 0,1,2\n        if act == 0: # hold\n            self.current_position = self.current_position;\n        elif act == 1: # buy\n            if self.current_position == \"none\":\n                # open long\n                self.current_position = \"long\"\n                ## buy usd\n                self.last_tl = self.tl;\n                self.usd = self.tl\/(self.data.iloc[self.t, :]['\u015eimdi'])\n                self.tl = 0;\n                #print(\"bought usd, usd:\", self.usd)\n            else:\n                self.current_position = self.current_position;\n                \n        else: # sell\n            if self.current_position == \"long\":\n                self.current_position = \"none\"\n                ## sell usd\n                self.tl = self.usd*(self.data.iloc[self.t, :]['\u015eimdi'])\n                self.usd = 0; # dolarlar\u0131 s\u0131f\u0131rla\n                \n                self.profits = self.profits + (self.tl - self.last_tl);\n                \n                if (self.tl - self.last_tl) > 0:\n                    reward = 1\n                else:\n                    reward = -1\n                #print(\"sold usd, tl:\", self.tl)\n            else:\n                self.current_position = self.current_position;\n                \n        # set next time\n        #print(\"current position: \",self.current_position)\n        self.t += 1\n        #print(\"history before: \",self.history)\n        self.history.pop(0)\n        self.history.append(self.data.iloc[self.t, :]['Fark %'])\n        #print(\"history after: \",self.history)\n        \n        #print(\"reward: \",reward)\n            \n        if ENVIRONMENT_DEBUG == True:\n            print(\"t: \",(self.t-self.history_t),\" reward: \",reward,\" profits: \",self.profits, \" current position: \",\n              self.current_position,\" done: \",self.done)\n            \n        if self.t == (len(self.data)-1):\n            self.done = True \n            print(\"Total steps: \",(self.t-self.history_t),\" TotalProfit: \",self.profits,\" done: \",self.done)\n        \n        return self.history, reward, self.done, self.profits # obs, reward, done","2e7b18b9":"# Deep Q-learning Agent\nclass DQNAgent:\n    def __init__(self, state_size, action_size, hidden_layer_size):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.hidden_layer_size = hidden_layer_size\n        self.memory = deque(maxlen=2000)\n        self.gamma = 0.95    # discount rate\n        self.epsilon = 1.0  # exploration rate\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        self.learning_rate = 0.001\n        self.model = self._build_model()\n        \n    def _build_model(self):\n        # Neural Net for Deep-Q learning Model\n        model = Sequential()\n        model.add(Dense(self.hidden_layer_size, input_dim=self.state_size, activation='relu'))\n        model.add(Dense(self.hidden_layer_size, activation='relu'))\n        model.add(Dense(self.action_size, activation='linear'))\n        model.compile(loss='mse',\n                      optimizer=Adam(lr=self.learning_rate))\n        return model\n    \n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n        \n    def act(self, state):\n        if np.random.rand() <= self.epsilon:\n            return random.randrange(self.action_size)\n        act_values = self.model.predict(state)\n        return np.argmax(act_values[0])  # returns action\n    \n    def act_greedy(self, state):\n        act_values = self.model.predict(state)\n        return np.argmax(act_values[0])  # returns action\n    \n    def replay(self, batch_size):\n        minibatch = random.sample(self.memory, batch_size)\n        for state, action, reward, next_state, done in minibatch:\n            \n            target = reward\n            if not done:\n              target = reward + self.gamma * \\\n                       np.amax(self.model.predict(next_state)[0])\n            target_f = self.model.predict(state)\n            target_f[0][action] = target\n            self.model.fit(state, target_f, epochs=1, verbose=0)\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay","0a3da7dd":"if __name__ == \"__main__\":\n    # macros\n    EPISODES = 2500\n    STATE_SIZE = 90\n    \n    # profits list\n    total_profits = []\n    \n    # initialize environment and the agent\n    env = Environment(train,100,STATE_SIZE) #100tl, 60 history\n    agent = DQNAgent(STATE_SIZE,3,100)\n    \n    # Iterate the game\n    for e in range(EPISODES):\n        #check if the buy and sell actions are taken\n        actions_count = 0\n        \n        # reset state in the beginning of each game\n        state = env.reset()\n        state = np.reshape(state, [1, STATE_SIZE])\n        \n        # time_t represents each frame of the game\n        # the more time_t the more score\n        for time_t in range(5000):\n            \n            # Decide action\n            action = agent.act(state)\n            \n            if (action == 1) or (action == 2):\n                actions_count = actions_count +1\n           \n            # Advance the game to the next frame based on the action.\n            next_state, reward, done, profits = env.step(action)\n            next_state = np.reshape(next_state, [1, STATE_SIZE])\n            \n            # make rewards = profits (EXPERIMENTAL)\n            reward = profits\n            \n            # Remember the previous state, action, reward, and done\n            agent.remember(state, action, reward, next_state, done)\n           \n            # make next_state the new current state for the next frame.\n            state = next_state\n            \n            # done becomes True when the game ends\n            if done:\n                total_profits.append(profits)\n                # print the score and break out of the loop\n                # print(\"number of actions taken other than hold in this iteration is \",actions_count,\"\\n\")\n                # print(\"episode: {}\/{}, score: {}\".format(e, EPISODES, time_t))\n                break\n        # train the agent with the experience of the episode\n        agent.replay(32)","2748dc25":"def plot_profits(total_profits):\n    epoch_count = range(1, len(total_profits) + 1)\n    fig= plt.figure(figsize=(30,10))\n    plt.plot(epoch_count, total_profits,'b-')\n    plt.legend('Total Profits')\n    plt.xlabel('Epoch')\n    plt.ylabel('Total Profits')\n    plt.figure(figsize=(50,10))\n    plt.show();\n    ","630571a6":"plot_profits(total_profits)","db409c9c":"test = test.iloc[::-1]\ntest.head(10)","d4b5739c":"env_test = Environment(test,100,STATE_SIZE) #100tl, 60 history\n\n# Iterate the game\nfor e in range(1):\n    #check if the buy and sell actions are taken\n    actions_count = 0\n        \n    # reset state in the beginning of each game\n    state = env_test.reset()\n    state = np.reshape(state, [1, STATE_SIZE])\n        \n    # time_t represents each frame of the game\n    # the more time_t the more score\n    for time_t in range(5000):\n            \n        # Decide action\n        action = agent.act_greedy(state)\n            \n        if (action == 1) or (action == 2):\n                actions_count = actions_count +1\n           \n        # Advance the game to the next frame based on the action.\n        next_state, reward, done, profits = env_test.step(action)\n        next_state = np.reshape(next_state, [1, STATE_SIZE])\n            \n        # make rewards = profits (EXPERIMENTAL)\n        reward = profits\n            \n        # Remember the previous state, action, reward, and done\n        agent.remember(state, action, reward, next_state, done)\n           \n        # make next_state the new current state for the next frame.\n        state = next_state\n            \n        # done becomes True when the game ends\n        if done:\n            break","d30fc4e4":"> Train the DQN","2b53bded":"First, let's load our dataset. The dataset used here is,\nhttps:\/\/www.kaggle.com\/umar47\/usd-try. For preprocessing, again, thanks to the kaggle user https:\/\/www.kaggle.com\/umar47 and his kernel https:\/\/www.kaggle.com\/umar47\/preprocessing-on-usd-try.","965b51a6":"Now, let's define our envireonment.","8dc18ae5":"The agent class.","728c2fbc":"Now test the agent with real world data.","637bf5ea":"First, the reverse version of the data will be used. It is expected to make the learning stronger since from 2002 to this date, usd is increasing with respect to try.","1d148923":"Some flags for the debuging purposes are presented here."}}