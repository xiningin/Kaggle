{"cell_type":{"ee81e31a":"code","cfec7501":"code","43363964":"code","3af05ca3":"code","b5a51f33":"code","4af2c8ea":"code","c1677107":"code","b1ff7064":"code","ae202d59":"code","bee60fb5":"code","d2806e8b":"code","2f08fa19":"code","f045b476":"code","f7927ef0":"code","41e9af57":"code","ce0bc2f8":"code","9954e0aa":"code","ec803ef4":"code","585f1f7f":"code","026b5a96":"code","454eb93b":"code","06ddbf59":"code","598b4f05":"code","5aeef9dd":"code","536b9a6a":"code","3a2c0ae8":"code","fdb26245":"code","b0376c30":"code","72c99f2d":"code","a97cb3e3":"code","b129a7ca":"code","c5eb1a09":"code","144f534a":"code","6ccb3e21":"code","eac40187":"code","b98f5ab9":"code","12e7e0dc":"markdown","307a3a2d":"markdown","6b0654fe":"markdown","ddd0511e":"markdown","d19d9258":"markdown","4e9ada9e":"markdown","537e3744":"markdown","50fd3bc7":"markdown","f0f036a2":"markdown","4c0b77be":"markdown","3548db39":"markdown","49ec0b57":"markdown","64e55888":"markdown"},"source":{"ee81e31a":"!pip install fastai2\n!pip install fast_tabnet","cfec7501":"from fastai2.basics import *\nfrom fastai2.tabular.all import *\nfrom fast_tabnet.core import *\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n#plt.style.use(\"_background\")","43363964":"PATH = '\/kaggle\/input\/covid19-global-forecasting-week-3\/'\ntrain_df = pd.read_csv(PATH + 'train.csv', parse_dates=['Date'])\ntest_df = pd.read_csv(PATH + 'test.csv', parse_dates=['Date'])\n\nadd_datepart(train_df, 'Date', drop=False)\nadd_datepart(test_df, 'Date', drop=False)","3af05ca3":"PATH1 = '\/kaggle\/input\/covid19-country-data-wk3-release\/'\nmeta_df = pd.read_csv(PATH1 + 'Data Join - RELEASE.csv', thousands=\",\")\n\nPATH2 = '\/kaggle\/input\/countryinfo\/'\ncountryinfo = pd.read_csv(PATH2 + 'covid19countryinfo.csv', thousands=\",\", parse_dates=['quarantine', 'schools', 'publicplace', 'gathering', 'nonessential'])\ntestinfo = pd.read_csv(PATH2 + 'covid19tests.csv', thousands=\",\")\n\ncountryinfo.rename(columns={'region': 'Province_State', 'country': 'Country_Region'}, inplace=True)\ntestinfo.rename(columns={'region': 'Province_State', 'country': 'Country_Region'}, inplace=True)\ntestinfo = testinfo.drop(['alpha3code', 'alpha2code', 'date'], axis=1)\n\nPATH3 = '\/kaggle\/input\/covid19-forecasting-metadata\/'\ncontinent_meta = pd.read_csv(PATH3 + 'region_metadata.csv')\ncontinent_meta = continent_meta[['Country_Region' ,'Province_State', 'continent']]\n\ndef fill_unknown_state(df):\n    df.fillna({'Province_State': 'Unknown'}, inplace=True)\n    \nfor d in [train_df, test_df, meta_df, countryinfo, testinfo, continent_meta]:\n    fill_unknown_state(d)","b5a51f33":"idx_group = ['Country_Region', 'Province_State']\n\ndef day_reached_cases(df, name, no_cases=1):\n    \"\"\"For each country\/province get first day of year with at least given number of cases.\"\"\"\n    gb = df[df['ConfirmedCases'] >= no_cases].groupby(idx_group)\n    return gb.Dayofyear.first().reset_index().rename(columns={'Dayofyear': name})\n\ndef area_fatality_rate(df):\n    \"\"\"Get average fatality rate for last known entry, for each country\/province.\"\"\"\n    gb = df[df['Fatalities'] >= 22].groupby(idx_group)\n    res_df = (gb.Fatalities.last() \/ gb.ConfirmedCases.last()).reset_index()\n    return res_df.rename(columns={0 : 'FatalityRate'})","4af2c8ea":"def joined_data(df):\n    res = df.copy()\n    \n    fatality = area_fatality_rate(train_df)\n    first_nonzero = day_reached_cases(train_df, 'FirstCaseDay', 1)\n    first_fifty = day_reached_cases(train_df, 'First50CasesDay', 50)\n    \n    # Add external features\n    res = pd.merge(res, continent_meta, how='left')\n    res = pd.merge(res, meta_df, how='left')\n    res = pd.merge(res, countryinfo, how='left')\n    res = pd.merge(res, testinfo, how='left', left_on=idx_group, right_on=idx_group)\n    \n    # Add calculated features\n    res = pd.merge(res, fatality, how='left')\n    res = pd.merge(res, first_nonzero, how='left')\n    res = pd.merge(res, first_fifty, how='left')\n    return res\n\ntrain_df = joined_data(train_df)\ntest_df = joined_data(test_df)","c1677107":"# It turns out any country in train dataset has at least one case.\ntrain_df.FirstCaseDay.isna().sum()","b1ff7064":"def with_new_features(df, train=True):\n    res = df.copy()\n    add_datepart(res, 'quarantine', prefix='qua')\n    add_datepart(res, 'schools', prefix='sql')\n    \n    res['DaysSinceFirst'] = res['Dayofyear'] - res['FirstCaseDay']\n    res['DaysSince50'] = res['Dayofyear'] - res['First50CasesDay']\n    res['DaysQua'] = res['Dayofyear'] - res['quaDayofyear']\n    res['DaysSql'] = res['Dayofyear'] - res['sqlDayofyear']\n    \n    # Since we will take log of dependent variable, we won't make it nonzero.\n    if train:\n        res['ConfirmedCases'] += 1\n    return res\n    \ntrain_df = with_new_features(train_df)\ntest_df = with_new_features(test_df, train=False)","ae202d59":"PATH4 = '\/kaggle\/input\/covid19forecastforvalidset\/'\nvalid_preds = pd.read_csv(PATH4 + 'forecast.csv')","bee60fb5":"train_df.shape","d2806e8b":"test_with_preds = test_df.merge(valid_preds, how='left')\ntest_with_preds = test_with_preds[(test_with_preds['Date'] > train_df.Date.max()) & (test_with_preds['Date'] < '2020-04-13')]","2f08fa19":"train_len = train_df.shape[0]\ntrain_new = pd.concat([train_df, test_with_preds]).sort_values(by='Date')","f045b476":"train_new","f7927ef0":"# Categorical variables - only basic identifiers, some features like continent will be worth adding.\ncat_vars = ['Country_Region', 'Province_State',\n            'continent'\n#             'publicplace', 'gathering', 'nonessential'\n           ]\n\n# Continuous variables - just ones directly connected with time.\ncont_vars = ['DaysSinceFirst', 'DaysSince50', 'Dayofyear',\n            'DaysQua', 'DaysSql',\n            'TRUE POPULATION', \n            'testper1m', 'positiveper1m',\n            'casediv1m', 'deathdiv1m', \n            'FatalityRate',\n#             'density', 'urbanpop', 'medianage', 'hospibed','healthperpop', 'fertility',\n#             'smokers', 'lung', \n#             'continent_gdp_pc', 'continent_happiness', 'continent_Life_expectancy','GDP_region', \n#             'latitude', 'abs_latitude', 'longitude', 'temperature', 'humidity',\n            ]\n\n# We will predict only confirmed cases. \n# For fatalities, one could train another model but we won't do it - multiplying by average fatality in each area is enough for a sample submission.\ndep_var = 'ConfirmedCases'\n\ndf = train_new[cont_vars + cat_vars + [dep_var,'Date']].copy().sort_values('Date')","41e9af57":"# print(test_df.Date.min())\n# MAX_TRAIN_IDX = df[df['Date'] < test_df.Date.min()].shape[0]\nMAX_TRAIN_IDX = train_len","ce0bc2f8":"df1 = df.copy()\ndf1['ConfirmedCases'] = np.log(df1['ConfirmedCases'])","9954e0aa":"path = '\/kaggle\/working\/'\n\nprocs=[FillMissing, Categorify, Normalize]\n\nsplits = list(range(MAX_TRAIN_IDX)), (list(range(MAX_TRAIN_IDX, len(df))))\n\n%time to = TabularPandas(df1, procs, cat_vars.copy(), cont_vars.copy(), dep_var, y_block=TransformBlock(), splits=splits)","ec803ef4":"dls = to.dataloaders(bs=512, path=path)\ndls.show_batch()","585f1f7f":"to_tst = to.new(test_df)\nto_tst.process()\nto_tst.all_cols.head()","026b5a96":"emb_szs = get_emb_sz(to); print(emb_szs)","454eb93b":"dls.c = 1 # Number of outputs we expect from our network.\nmodel = TabNetModel(emb_szs, len(to.cont_names), dls.c, n_d=16, n_a=32, n_steps=1)\nopt_func = partial(Adam, wd=0.01, eps=1e-5)\nlearn = Learner(dls, model, MSELossFlat(), opt_func=opt_func, lr=3e-2, metrics=[rmse])","06ddbf59":"learn.lr_find()","598b4f05":"cb = SaveModelCallback()\nlearn.fit_one_cycle(100, cbs=cb)","5aeef9dd":"learn.load('model')\nlearn.fit_one_cycle(100, cbs=cb)","536b9a6a":"learn.load('model')\nlearn.fit_one_cycle(50, lr_max=7e-5, cbs=cb)","3a2c0ae8":"from xgboost import XGBRegressor","fdb26245":"X_train, y_train = to.train.xs, to.train.ys.values.ravel()\n\nxmodel1 = XGBRegressor(n_estimators=1000, n_jobs=-1)\nxmodel1.fit(X_train, y_train)","b0376c30":"ys = xmodel1.predict(to_tst.train.xs)","72c99f2d":"learn.load('model')","a97cb3e3":"tst_dl = dls.valid.new(to_tst)\ntst_dl.show_batch()","b129a7ca":"learn.metrics = []\ntst_preds,_ = learn.get_preds(dl=tst_dl)","c5eb1a09":"res0 = np.expm1(ys)\nres1 = np.expm1(tst_preds)\nres2 = list(map(lambda x: x[0], res1.numpy()))\nsubmit = pd.DataFrame({'ConfirmedCases': (res2 + res0)\/2})\nsubmit.index = test_df.ForecastId","144f534a":"fatality_series = test_df.FatalityRate.copy()\nfatality_series.index += 1\nfatality_series.fillna(0.02137, inplace=True)\n\nsubmit['Fatalities'] = (submit.ConfirmedCases > 69) * fatality_series * submit.ConfirmedCases","6ccb3e21":"submit.to_csv('submission.csv')","eac40187":"import seaborn as sns\n\nmin_date = test_df.Date.min()\nmax_date = train_df.Date.max()\n\nf, axes = plt.subplots(10, 1, figsize=(16, 60))\n\ndef plot_preds(country, ax):\n    subset = test_df[(test_df['Country_Region'] == country)]\n    \n    idx = subset.index\n    dates = subset.Date\n    predicted = submit.iloc[idx].ConfirmedCases\n    predicted.index = dates\n    \n    combined = pd.DataFrame({'pred': predicted})\n    \n    sns.lineplot(data=combined, ax=axes[ax]).set_title(country)\n\nplot_preds('Italy', 0)\nplot_preds('Spain', 1)\nplot_preds('Germany', 2)\nplot_preds('Poland', 3)\nplot_preds('Czechia', 4)\nplot_preds('Russia', 5)\nplot_preds('Iran', 6)\nplot_preds('Sweden', 7)\nplot_preds('Japan', 8)\nplot_preds('Belgium', 9)","b98f5ab9":"res = submit.iloc[test_df[(test_df['Country_Region'] == 'Italy')].index]\nres.index = test_df[(test_df['Country_Region'] == 'Italy')].Date\nres","12e7e0dc":"# Add temporal features\nSome basic features like number of days since the first case in each country\/province with analogous feature for 50 days may be worth adding.","307a3a2d":"# Fatalities\n\nWe calculated average fatality rate for each country and region. Although it is not the best predictor, we can use it here. \n\nFor countries that don't have any fatalities yet, we provide a magic value as they exceed another magic number of cases.","6b0654fe":"# Preparing submission","ddd0511e":"# Data preprocessing for the model\n\nBasically vanilla fast.ai stuff here, including taking log of dependent variable.","d19d9258":"# Add external features to input dataframes\nFor each country, we want to extract the first day when it reached at least 1 and 50 cases.\n\nAverage fatality rate will be calculated from last data available, simply taking deaths \/ cases.","4e9ada9e":"# Model\n\nBaseline fast.ai tabular learner with RMSE metrics (we took log before, so it is RMSLE)\n\nAs mentioned before, our dependent variable is number of confirmed cases. We will provide a simple estimate for fatalities later.","537e3744":"# Avoid leakage - take only non-overlapping values for training\n\nFor now, the only available data to validate our model is in training set. \n\nAs our test set starts on **26.03.2020**, we should take only rows before that date for training to avoid leakage.","50fd3bc7":"# [#masks4all](https:\/\/masks4all.co\/why-we-need-mandatory-mask-laws-masks4all\/)\n\n# Introduction\n\nThe goal of this notebook is to provide some basic [fast.ai](https:\/\/www.fast.ai\/) tabular model for COVID-19 dataset.\n\nAlthough it is not the best approach here, it requires reasonably small amount of code and obviously no feature engineering.\n\nThe solution utilizes mostly [fast.ai](https:\/\/www.fast.ai\/) library and stuff included in [this course](https:\/\/course.fast.ai\/)","f0f036a2":"# Example predictions\n\n","4c0b77be":"# Load input data","3548db39":"# Feature selection\nIn fast.ai we can easily select categorical and continuous variables for training.\n\nI decided not to choose any external data in baseline model. Adding numerical values from country data provided in this notebook doesn't seem to improve the validation score much.","49ec0b57":"# Metadata (continent, population, tests per million etc.)","64e55888":"# Conclusion\n\nAs neural networks are thought not to perform well on tabular data, which is true especially for some trivial architectures like the MLP used here, we cannot expect much from this model.\n\nWhat is interesting, additional continuous variables not dependent on time don't seem to provide any improvement in our validation score.\n\nPredictions of confirmed cases look quite legit for short time windows like the one in validation set (up to 10 days). For later dates especially in May, we can see some unreasonable exponential behaviour.\n\n# [#masks4all](https:\/\/masks4all.co\/why-we-need-mandatory-mask-laws-masks4all\/)"}}