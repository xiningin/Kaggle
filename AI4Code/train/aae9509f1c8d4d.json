{"cell_type":{"9c2af4e3":"code","12a391f0":"code","8ed15ca3":"code","ade608cc":"code","5d2e5378":"code","7e98428a":"code","ce55d747":"code","8f9136f4":"code","2ecec6b2":"code","d06c368c":"code","21551f1f":"code","6d36aeca":"code","52c9b09d":"code","de114791":"code","ed490415":"code","6bd12424":"code","c5f3010e":"code","e6a497fc":"code","573cf822":"code","8408c6a8":"code","04b0ea01":"code","3716fd8a":"code","4db7892a":"code","19e06cbc":"code","2e6526bc":"code","fbe0091c":"code","53efcc58":"code","ca47277a":"code","644baeb3":"code","8d17b8ff":"code","21232d4e":"code","39148a3c":"code","8323fcd2":"code","9f1a13b7":"code","a61f4d5b":"code","f7e0d4b7":"code","0bb126fb":"code","3c85ad9e":"code","20f5e923":"code","1fe62eed":"code","85d4d151":"code","10ff7527":"code","9c79c206":"code","5f2d5766":"code","fbabd0b5":"code","13bb51e9":"code","035532ee":"code","b5a6e92f":"code","0bd0e245":"code","87cc6c57":"code","baf0ff0a":"code","648a7fdd":"code","0bfe028a":"code","148afc5a":"code","8a057f68":"code","8bb69134":"code","e21f201e":"code","85118c9c":"code","68d23280":"code","9cb4224f":"code","c74b8613":"code","6b1e4de5":"code","cd8316a7":"code","9f711cd5":"code","3e3dfbfd":"code","84eafc2c":"code","a80ecf76":"code","478462b6":"code","7c7bd767":"code","6f575529":"code","2add7999":"code","0ad95821":"code","199ba135":"code","a67e8c19":"code","93e9c7ba":"code","8156e385":"code","4bfeac40":"code","0a16dca2":"code","6d864c17":"markdown","2cdb5510":"markdown","1e9d194c":"markdown","45b3db9f":"markdown","8b558488":"markdown","57a4b7f2":"markdown","4d8743e8":"markdown","1d552447":"markdown","70c9495c":"markdown","18341f25":"markdown","3283fad7":"markdown","0fe19732":"markdown","99ce2b28":"markdown","080e3468":"markdown","48808873":"markdown","12495e29":"markdown","616906a6":"markdown","68b60a21":"markdown"},"source":{"9c2af4e3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom statsmodels.tsa.stattools import grangercausalitytests\nimport seaborn as sns\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.api import VAR\nimport math\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.neighbors import KNeighborsRegressor","12a391f0":"%matplotlib inline","8ed15ca3":"df = pd.read_csv('..\/input\/into-the-future\/train.csv')\ndf_test = pd.read_csv('..\/input\/into-the-future\/test.csv')","ade608cc":"df.iloc[455]","5d2e5378":"df.tail()","7e98428a":"pd.isna(df).sum()","ce55d747":"df.info()","8f9136f4":"df['time'] = pd.to_datetime(df['time'])","2ecec6b2":"df.info()","d06c368c":"df.describe()","21551f1f":"fig, ax0 = plt.subplots(figsize=(15, 5))\nax0.plot(df['feature_1'], 'r')\nax0.plot(df['feature_2'], 'b')","6d36aeca":"# Scaling data\nscaler = MinMaxScaler()\ndf[['feature_1', 'feature_2']] = scaler.fit_transform(df[['feature_1', 'feature_2']])","52c9b09d":"df.head()","de114791":"df_time = df.copy()\ndf_time.index = df['time']\nfig, ax1 = plt.subplots(figsize=(15, 5))\nax1.plot(df_time['feature_1'], 'r')\nax1.plot(df_time['feature_2'], 'b')","ed490415":"df_data = df.drop(['id', 'time'], axis=1)\ndf_data.head()","6bd12424":"corr = df_data.corr()\nsns.heatmap(corr, vmax=-1.0, vmin=-0.6)","c5f3010e":"def is_GrangerCause(data=None, maxlag=5):\n    \"\"\"This function find if x2 Granger cause x1 vis versa \"\"\"    \n    gc = grangercausalitytests(data, maxlag=maxlag, verbose=False)\n    \n    for i in range(maxlag):\n        x=gc[i+1][0]\n        p1 = x['lrtest'][1] # pvalue for lr test\n        p2 = x['ssr_ftest'][1] # pvalue for ssr ftest\n        p3 = x['ssr_chi2test'][1] #pvalue for ssr_chi2test\n        p4 = x['params_ftest'][1] #pvalue for 'params_ftest'\n        \n        condition = ((p1 < 0.05 and p2 < 0.05) and (p3 < 0.05 and p4 < 0.05))\n        \n        if condition == True:\n            cols = data.columns\n            print('Yes: {} Granger causes {}'.format(cols[0], cols[1]))\n            print('maxlag = {}\\nResults: {}'.format(i, x))\n            break\n            \n        else:\n            if i == maxlag - 1:\n                cols = data.columns\n                print('No: {} does not Granger cause {}'.format(cols[0], cols[1]))","e6a497fc":"is_GrangerCause(df_data)","573cf822":"def adfuller_test(data, p_val=0.05):\n    print('\\n \\n ----------------- ADF-------------------')\n    index = ['Test Stats', 'p-value', 'Lags', 'Observations']\n    adf_test = adfuller(data, autolag='AIC')\n    adf = pd.Series(adf_test[0:4], index =index)\n    \n    for key, value in adf_test[4].items():\n        adf['Critical Value(%s)'%key] = value\n    print(adf)\n    \n    p = adf['p-value']\n    if p <= p_val:\n        print('Series is stationary')\n    else:\n        print('Series is not stationary')","8408c6a8":"# Applying test on both series\nadfuller_test(df_data['feature_1'])\nadfuller_test(df_data['feature_2'])","04b0ea01":"df_sup = pd.read_csv('..\/input\/into-the-future\/train.csv')\ndf_sup = df_sup.drop(['id', 'time',], axis=1)","3716fd8a":"df_sup.head()","4db7892a":"# Scaling\nscaler = StandardScaler()\ndf_sup[['feature_1', 'feature_2']] = scaler.fit_transform(df_sup[['feature_1', 'feature_2']])","19e06cbc":"df_sup.head()","2e6526bc":"df_sup.describe()","fbe0091c":"df_test = pd.read_csv('..\/input\/into-the-future\/test.csv')","53efcc58":"df_test = df_test.drop('time', axis=1)\nid_test = df_test['id']\nfeature_1_test = df_test['feature_1']","ca47277a":"df_test.head()","644baeb3":"df_test[['feature_1', 'id']] = scaler.transform(df_test[['feature_1', 'id']])","8d17b8ff":"df_test.head()","21232d4e":"df_test['feature_1'].plot()","39148a3c":"# Comparing df_test to df_sup\nfig, ax1 = plt.subplots(nrows=2, sharex= True, figsize=(15, 5))\nax1[0].plot(df_sup['feature_1'], 'r')\nax1[1].plot(df_test['feature_1'], 'b')","8323fcd2":"sns.relplot(x='feature_1', y='feature_2', data=df_sup, kind='scatter')","9f1a13b7":"df_sup['feature_1'] = df_sup[df_sup['feature_1'] < 3]\ndf_sup = df_sup.dropna()","a61f4d5b":"df_sup.describe()","f7e0d4b7":"sns.regplot(x='feature_1', y='feature_2', data=df_sup)","0bb126fb":"x_train,x_valid, y_train,  y_valid = train_test_split(df_sup['feature_1'], df_sup['feature_2'], test_size=0.2)","3c85ad9e":"x_train, y_train = pd.DataFrame(x_train, columns=['feature_1']), pd.DataFrame(y_train, columns=['feature_2'])\nx_valid, y_valid = pd.DataFrame(x_valid, columns=['feature_1']), pd.DataFrame(y_valid, columns=['feature_2'])","20f5e923":"Lr = LinearRegression()","1fe62eed":"Lr.fit(x_train, y_train)","85d4d151":"pred_valid = Lr.predict(x_valid)","10ff7527":"knn = KNeighborsRegressor()","9c79c206":"knn_param = {'n_neighbors': [1, 3, 5],\n            'leaf_size': [10, 20, 40, 50]}","5f2d5766":"grid_search = GridSearchCV(knn, knn_param, n_jobs=-1, cv=10)","fbabd0b5":"grid_search.fit(x_train, y_train)","13bb51e9":"grid_search.best_score_","035532ee":"grid_search.best_params_","b5a6e92f":"model = grid_search.best_estimator_","0bd0e245":"pred_valid = model.predict(x_valid)","87cc6c57":"r2_score(y_valid, pred_valid)","baf0ff0a":"plt.figure(figsize=(10, 10))\nplt.scatter(x_train, y_train, color = \"red\")\nplt.plot(x_train, model.predict(x_train), 'go')\nplt.title(\"Knn Fit\")\nplt.xlabel(\"feature_1\")\nplt.ylabel(\"feature_2\")\nplt.show()","648a7fdd":"x_test = pd.DataFrame(df_test['feature_1'], columns=['feature_1'])","0bfe028a":"pred_test = pd.DataFrame(model.predict(x_test), columns=['feature_2'])","148afc5a":"pred_test","8a057f68":"pred_test_df = pd.concat([df_test['id'], pred_test], axis=1, copy=False)","8bb69134":"pred_test_df","e21f201e":"pred_test_df[['id', 'feature_2']] = scaler.inverse_transform(pred_test_df[['id', 'feature_2']])","85118c9c":"pred_test_df['id'] = id_test","68d23280":"pred_test_df.shape","9cb4224f":"pred_test_df.head()","c74b8613":"len(id_test)","6b1e4de5":"len(pred_test_df['feature_2'])","cd8316a7":"pred_test_df['feature_2']","9f711cd5":"submission = pd.DataFrame({'id': id_test, 'feature_2':pred_test_df['feature_2'].values})","3e3dfbfd":"submission.head()","84eafc2c":"submission.to_csv(r'submisson.csv', index=False)","a80ecf76":"# adding timestamp\ndf_data.index = pd.to_datetime(df['time'])\ndf_data.head()","478462b6":"model = VAR(df_data, freq='10S')","7c7bd767":"result = model.fit(maxlags=30, ic='fpe')\nresult.summary()","6f575529":"# Forecasting\nlag_order = result.k_ar\nresult.forecast(df_data.values[-lag_order:], 6)","2add7999":"result.plot_forecast(375)","0ad95821":"# Evaluation\nfevd = result.fevd(5)\nfevd.summary()","199ba135":"test_predicted = result.forecast(df_data.values[-lag_order:], 375)\ntest_predicted_df = pd.DataFrame(test_predicted, index=df_test['id'], columns=df_data.columns)","a67e8c19":"test_predicted_df[['feature_1', 'feature_2']] = scaler.inverse_transform(test_predicted_df[['feature_1', 'feature_2']])","93e9c7ba":"test_predicted_df.head()","8156e385":"df_test.index = df_test['id']\ndf_test.head()","4bfeac40":"math.sqrt(mean_squared_error(df_test['feature_1'], test_predicted_df['feature_1']))","0a16dca2":"# Plotting data simulatneously to visualize quality of prediction\nfig, ax3 = plt.subplots(figsize=(15, 5))\nax3.plot(df_test['feature_1'], 'r')\nax3.plot(test_predicted_df['feature_1'], 'b')","6d864c17":"<h3> Visualize Model Fit","2cdb5510":"This plot reveals some pretty interesting facts about this dataset after scaling data usin min max scaler in bounds of [0, 1] we can clearly see that feature_1 is an approximation of <b> Exponential Function<\/b> whereas feature_2 is a <b> Logarithmic Function <\/b>, This information come will hnady when we will use supervised method to predict the FUTURE.<br> Now we will test for correlation between data.","1e9d194c":"Data is for 10 seconds in sequence for total 563 entries. There is 94 minutes of data in dataset. <br>\nLet's check for data quality.","45b3db9f":"Since Data is not perfectly linear we might be able to use some Non- linear models.","8b558488":"We have pretty decent r2_score on valid set let's try knn and see if we can make any diffrneces.\n<h3> KNN <\/h3>","57a4b7f2":"we have some outlier in feature_1 we need to remove it to make predictions more accurate.","4d8743e8":"# Method 2: VAR","1d552447":"Here we don't require id and time","70c9495c":"time column is not date time so we need to make it in datetime format.","18341f25":"Here Knn gets issue of overfitting but as data, Our model has perfectly covered the elements of the test data which is observable in Eda section of test set which means we can fit slightly overfitted knn model to the dataset., Instead if we use the Linear regression model we can eaisly fit the model but data fit is slightly off on edges such as -2.0 and 1.0.\n<h3> Prediction <\/h3>","3283fad7":"High negative correlation between feature_1 and feature_2, Now we need to test whether there is causality in feature_1 and feature_2 or not.","0fe19732":"# Checking stationarity in Data","99ce2b28":"# Importing Data","080e3468":"# Method 1 : Supervised Learning","48808873":"<h3> EDA Visualizing data <\/h3>","12495e29":"Data in both feature columns are not on same scale so we need to scale data.","616906a6":"# Approach\n2 approach can be used in this dataset.<br>\n<b> 1. Supervised Learning. <\/b> : Since test data have only one predictor, we can easily convert the problem in supervised learning. <br>\n<b> 2. Vector auuto regression (VAR). <\/b> : As data is in timeseries format we can use multivariate timeseries forecasting technique to predict the future. <br>\nAdvantagge of VAR method is we can further extend predictions even beyond given test dataset.","68b60a21":"# Plotting"}}