{"cell_type":{"cb9167c9":"code","6ca00dd0":"code","677b7376":"code","80cbf430":"code","662c45e3":"code","47071f81":"code","6cf575e0":"code","608b4730":"code","b9521fb9":"code","3ea20bac":"code","c366a646":"code","7f09a462":"code","6ca1fa00":"code","7d0afa64":"code","390ce560":"code","a2afd2b3":"code","21f60c99":"code","4a6f9cdb":"code","0ebc2024":"code","d18bdc3a":"code","4df5bc93":"code","f05cf757":"code","82e32d9d":"code","ecc8101f":"code","56763ef6":"code","43d5557a":"code","c2c9c606":"code","43876364":"code","2a71d999":"code","af317d32":"code","a7b6078a":"code","210de532":"code","e11372a0":"code","81002d7f":"code","ca66ad25":"code","f5f585f6":"code","86b8beb7":"code","f1d90997":"code","d71b652f":"code","16c1765b":"code","cd4a3124":"code","25eb7e22":"code","7252cfcd":"code","c8078ddd":"code","4e3e634d":"code","75a53269":"code","e4e2f2a4":"code","e42e45d8":"code","a8373ca6":"code","f04a011a":"code","c3031acf":"code","0aafb200":"code","7229300b":"markdown","25a7911d":"markdown","2a9ff318":"markdown","26591360":"markdown","f4627941":"markdown","554de6ae":"markdown","48c82a5c":"markdown","29c4c28d":"markdown","826ca08b":"markdown","1392d6a3":"markdown","973f2326":"markdown"},"source":{"cb9167c9":"# !conda install pytorch torchvision -c pytorch","6ca00dd0":"# !pwd\n# # !mkdir check_point_dir\n# # !pwd\n# !ls","677b7376":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pre\n# ssing Shift+Enter) will list all files under the input directory\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n# import torch.utils.data as Data\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sns\n\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split \n%matplotlib inline\n\nfrom functools import partial\nimport numpy as np\nimport os\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import random_split\nimport torchvision\nimport torchvision.transforms as transforms\n\nfrom ray import tune\nfrom ray.tune import CLIReporter\nfrom ray.tune.schedulers import ASHAScheduler\n\n\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\nimport random\n\n# import h2o\n# h2o.init()\n\nimport numpy as np\nimport imageio\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","80cbf430":"data=pd.read_csv(\"\/kaggle\/input\/fromnancy\/logP (1).txt\")\ndata.drop(columns=\"smiles\",inplace=True)  #discard the \"name\" column\nlen(data.columns),len(data)","662c45e3":"#the column \"a_nB\" has only one value 0, so it's safe to drop it.\ndata.drop(columns=\"a_nB\",inplace=True)\n\n#the column \"h_logp\" is said to be somehow dependent on \"logp\"(label), \n#so it should be dropped.\ndata.drop(columns=\"h_logP\",inplace=True)\n\n#two more features with very unbalanced value distribution:\"a_aro\" and \"b_ar\":\nfor f in [\"a_aro\",\"b_ar\"]:\n    plt.figure(figsize=(10,4))\n    data[f].plot.hist(bins=50)\n    plt.title(\"Histogram of \"+f)\n    plt.show()\n\ndata.drop(columns=\"a_aro\",inplace=True)\ndata.drop(columns=\"b_ar\",inplace=True)","47071f81":"#separate three different groups of features. The first group is continuous \nfeatures_continuous=[\"Weight\",\"apol\",\"bpol\",\"TPSA\",\"h_logS\",\"h_mr\"]\nfeatures_discrete_many=[\"zagreb\",\"b_single\",\"b_heavy\",\"b_count\",\"b_1rotN\",\"a_heavy\",\"a_hyd\",\n          \"a_nC\",\"a_count\",\"a_acc\",'a_nN','a_nO','a_nS']\nfeatures_discrete_unbalanced=['a_acid','a_base','a_nBr','a_nCl','a_nF','a_nI',\n 'a_nP','b_double','FCharge','rings']\n\n#the above grouping can be achieved by the following lines where features_discrete_many\n#and features_discrete_unbalanced are separated by hist[0]\/hist[1]=5\nhist0DIVhist1=5\nfeatures_continuous=[f for f in data.columns  if data[f].dtype!=\"int64\"];features_continuous.remove(\"logp\")\nfeatures_discrete_many=[f for f in data.columns if data[f].dtype==\"int64\" and data[f].value_counts().iloc[0]\/data[f].value_counts().iloc[1] <hist0DIVhist1]\nfeatures_discrete_unbalanced=[f for f in data.columns if data[f].dtype==\"int64\" and data[f].value_counts().iloc[0]\/data[f].value_counts().iloc[1] >= hist0DIVhist1]","6cf575e0":"#plot histogram for different groups\n# for f in features_continuous:\n# for f in features_discrete_many:\n# for f in features_discrete_unbalanced:\n#     plt.figure(figsize=(10,4))\n#     data[f].plot.hist(bins=50)\n#     plt.title(\"Histogram of \"+f)\n#     plt.show()","608b4730":"## in the following, we further discard the features from features_discrete_unbalanced\ndata1=data[[\"logp\"]+features_discrete_many+features_continuous]\n\ndef color_high_red(val):\n    \"\"\"\n    Takes a scalar and returns a string with\n    the css property `'color: red'` for negative\n    strings, black otherwise.\n    \"\"\"\n    color = 'red' if np.abs(val) >0.8 and not val==1 else 'black'\n    return 'color: %s' % color\n\n#find some highly correlated features\nthreshold=0.8 \nCorrMatrix=data1.corr();\nall_feature=list(CorrMatrix.columns); features_to_discard=[]\nfor i,f1 in list(enumerate(CorrMatrix.columns))[1:]:\n    for j in range(i):\n        if abs(CorrMatrix.iloc[i,j])>threshold and not all_feature[j] in features_to_discard:\n            features_to_discard.append(all_feature[j])\ndisplay(\"features to discard:\",features_to_discard )\ndisplay(\"correlation matrix before discarding highly correlated features:\")            \ndisplay(CorrMatrix.style.applymap(color_high_red))    \n\n#drop highly correlated features:\n# data1.drop(columns=[\"a_count\",\"b_count\",\"a_heavy\",\"b_heavy\",\"zagreb\",\"b_single\",\"b_1rotN\",\"bpol\",\"h_mr\"],inplace=True)\nfeatures_to_discard+=[\"b_1rotN\",\"bpol\",\"zagreb\"]\ndata1.drop(columns=features_to_discard,inplace=True)\ndisplay(\"correlation matrix after discarding highly correlated features:\")\ndisplay(data1.corr().style.applymap(color_high_red))\n\n\n# set features with int data type as categorical and use dummy encoding for them\ncategorical_features=[name for name in data1.columns if data1[name].dtype==\"int64\"]\ndata_dummy_encoded=pd.get_dummies(data1, columns=categorical_features,drop_first=True)\n\nprint(\"column names after encoding features with int values with dummy variables:\")\ndisplay(data_dummy_encoded.columns)\n\n# standardizer applied to all features(without dummy encoding)\nstandardizer = preprocessing.StandardScaler()\nx_pca = standardizer.fit_transform(data1.iloc[:,1:]) ","b9521fb9":"pca = PCA(n_components=5)\nprincipalComponents = pca.fit_transform(x_pca)\nfinalDf = pd.DataFrame(data = principalComponents)\nfinalDf.head()\npca.explained_variance_ratio_","3ea20bac":"%matplotlib inline\ncolor=[\"green\" if s>0 else \"red\" for s in data1.iloc[:,0]]\nplt.figure(figsize=(10,4))\nplt.scatter(x=finalDf.iloc[:,0],y=finalDf.iloc[:,1],c=color)\nplt.title(\"PCA 2D plot\")","c366a646":"%matplotlib inline\nfig = plt.figure(figsize=(20,6))\ngs = fig.add_gridspec(1, 2)\nax1 = fig.add_subplot(gs[0, 0])\ngreen=[s==\"green\" for s in color];\n# plt.figure(figsize=(10,4))\nax1.scatter(x=finalDf.iloc[:,0][green],y=finalDf.iloc[:,1][green],c=\"green\")\nax1.set_title(\"PCA 2D plot\")\n\nax2 = fig.add_subplot(gs[0, 1])\nred=[s==\"red\" for s in color];\nax2.scatter(x=finalDf.iloc[:,0][red],y=finalDf.iloc[:,1][red],c=\"red\")\nax2.set_title(\"PCA 2D plot\")","7f09a462":"# %matplotlib notebook\n\n# import matplotlib.pyplot as plt\n# from mpl_toolkits.mplot3d import axes3d    \n\n# ax = plt.axes(projection='3d')\n# color = data1[\"logp\"]\n\n# zdata = finalDf.iloc[:,0].values\n# xdata = finalDf.iloc[:,1].values\n# ydata = finalDf.iloc[:,2].values\n# ax.scatter3D(xdata, ydata, zdata,c=color);","6ca1fa00":"from sklearn.manifold import TSNE\n# X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\nX_embedded = TSNE(n_components=2,random_state=234).fit_transform(x_pca)\nX_embedded.shape","7d0afa64":"# from IPython.core.display import HTML\n# color=\"255,127,80\"  # coral\n# def print_color(c):\n#     source = \"<h1 style='color: rgb({0})'>Color Text<\/h1>\".format(c)\n#     return HTML(source)","390ce560":"%matplotlib inline\n# color=[\"green\" if s>=0 else \"red\" for s in data1.iloc[:,0]]\n\nregions_separators=[-1,1.5,3]\nregions=[\"logp<-1\"];\n[regions.append(str(regions_separators[s])+\"<logp<\"+str(regions_separators[s+1]))  \n    for s in range(len(regions_separators)-1)] \nregions.append(\"logp>\"+str(regions_separators[-1]))\ndisplay(regions)\ncolors=[(random.random(), random.random(), random.random()) for s in regions]\n\ncolors[0]=\"green\";colors[-1]=\"red\";\n\ncolor_regions=[colors[0]]*len(data1.iloc[:,0]);\nfor i in range(len(data1.iloc[:,0])):\n    for s in range(len(regions_separators)-1): \n        if regions_separators[s]<=data1.iloc[i,0] and data1.iloc[i,0]<regions_separators[s+1]:\n            color_regions[i]=colors[s+1]\n    if data1.iloc[i,0]>=regions_separators[-1]:\n        color_regions[i]=colors[-1]\n\n#     if data1.iloc[i,0]>=-2 and data1.iloc[i,0]<4:\n#         color[i]=\"yellow\"; \n#     elif data1.iloc[i,0]>=4:\n#         color[i]=\"red\"\n\nplt.figure(figsize=(10,4))\nplt.scatter(x=X_embedded[:,0],y=X_embedded[:,1],c=color_regions)\n_=plt.title(\"color plot of different logp regions\")","a2afd2b3":"#plot each region with different color\nX_embedded_df=pd.DataFrame(X_embedded)\n%matplotlib inline\nfig = plt.figure(figsize=(20,12))\ngs = fig.add_gridspec(len(regions)\/\/3+1, 3)\nfor i,r in enumerate(regions):\n    row,col=divmod(i, 3)\n    ax=fig.add_subplot(gs[row,col])\n    cr=[s==colors[i] for s in color_regions];\n    ax.scatter(x=X_embedded_df.iloc[:,0][cr],y=X_embedded_df.iloc[:,1][cr],c=colors[i])\n    ax.set_title(r)\n    plt.xlim([-100, 100])\nfig = plt.figure(figsize=(20,6))\nred=[s==\"red\" for s in colors];\ndata1.iloc[:,0].hist(bins=10)\nplt.title(\"histgram of logp\")\nplt.xlim([-5, 7])","21f60c99":"#features and label\n# x=data_dummy_encoded.loc[:,data_dummy_encoded.columns != 'logp']\n# y=data_dummy_encoded[\"logp\"];\n\nx=data1.loc[:,data1.columns != 'logp']\ny=data1[\"logp\"];\n\n#split data\nmsk = np.random.rand(len(x)) < 0.8\ntrain_x = x[msk];  train_y = y[msk];\ntest_x  = x[~msk];  test_y = y[~msk];\n\n\n#normalize train and test features\nfrom sklearn import preprocessing\nv = train_x.values #returns a numpy array\nmin_max_scaler = preprocessing.MinMaxScaler()\nv_scaled = min_max_scaler.fit_transform(v)\ntrain_x = pd.DataFrame(v_scaled,index=train_y.index)\n\nv = test_x.values #returns a numpy array\nmin_max_scaler = preprocessing.MinMaxScaler()\nv_scaled = min_max_scaler.fit_transform(v)\ntest_x = pd.DataFrame(v_scaled,index=test_y.index)\n\n#\u53d8\u6210torch\u9700\u8981\u7684\u5f62\u5f0f\ntrain_data = []; index=train_x.index;\nfor i in range(len(train_x)):\n    train_data.append([np.array(train_x.loc[index[i],:].to_list()), np.array([train_y[index[i]]])])\n    \ntest_data = []; index=test_x.index;\nfor i in range(len(test_x)):\n    test_data.append([np.array(test_x.loc[index[i],:].to_list()), np.array([test_y[index[i]]])])\n\ntrainloader = torch.utils.data.DataLoader(train_data, batch_size=16, shuffle=True, num_workers=4)\ntestloader = torch.utils.data.DataLoader(test_data, batch_size=16, shuffle=False, num_workers=4)\n    ","4a6f9cdb":"# %time\n\n# torch.manual_seed(3)    # reproducible\n# class Net(nn.Module):\n#     def __init__(self, l1=120, l2=84):\n#         super(Net, self).__init__()\n#         self.fc1 = nn.Linear(len(train_data[0][0]), l1)\n#         self.fc2 = nn.Linear(l1, l2)\n#         self.fc3 = nn.Linear(l2, 1)\n\n#     def forward(self, x):\n#         x = F.relu(self.fc1(x))\n#         x = F.relu(self.fc2(x))\n#         x = self.fc3(x)\n#         return x\n\n\n# # net = torch.nn.Sequential(\n# #         torch.nn.Linear(len(train_data[0][0]), 200),\n# #         torch.nn.LeakyReLU(),\n# #         torch.nn.Linear(200, 100),\n# #         torch.nn.LeakyReLU(),\n# #         torch.nn.Linear(100, 20),\n# #         torch.nn.LeakyReLU(),\n# #         torch.nn.Linear(20, 1),\n# #     )\n    \n# # net = Net(config[\"l1\"], config[\"l2\"])\n\n# # if checkpoint_dir:\n# #     model_state, optimizer_state = torch.load(\n# #         os.path.join(checkpoint_dir, \"checkpoint\"))\n# #     net.load_state_dict(model_state)\n# #     optimizer.load_state_dict(optimizer_state)\n    \n# ## full training function    \n# def train_cifar(config, checkpoint_dir=None,data_dir=None):\n#     net = Net(config[\"l1\"], config[\"l2\"])\n\n#     device = \"cpu\"\n#     if torch.cuda.is_available():\n#         device = \"cuda:0\"\n#         if torch.cuda.device_count() > 1:\n#             net = nn.DataParallel(net)\n#     net.to(device)\n\n# #     criterion = nn.CrossEntropyLoss()\n# #     optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9)\n    \n#     # print(net)  # net architecture\n# #     optimizer = torch.optim.SGD(net.parameters(), lr=0.005)\n#     optimizer = optim.SGD(net.parameters(), lr=config[\"lr\"], momentum=0.9) #\u7528\u65b0\u7684optimzer\n#     loss_func = torch.nn.MSELoss(reduction=\"mean\")  # this is for regression mean squared loss\uff0c\u4ee3\u66ff\u524d\u9762\u7684criteria\n#     Mean_error=[]\n#     epoch_range=range(20)\n    \n#     if checkpoint_dir:\n#         model_state, optimizer_state = torch.load(\n#             os.path.join(checkpoint_dir, \"checkpoint\"))\n#         net.load_state_dict(model_state)\n#         optimizer.load_state_dict(optimizer_state)    #\u5efa\u4e00\u4e2acheckpoint_dir\u6765\u5b58\u4e2d\u95f4\u72b6\u6001\n\n# #     trainset, testset = load_data(data_dir)   \n\n# #     test_abs = int(len(trainset) * 0.8)\n# #     train_subset, val_subset = random_split(\n# #         trainset, [test_abs, len(trainset) - test_abs])\n#     test_abs = int(len(train_data) * 0.8)         ##\u5206\u6210 train\u548cvalidation\u4e24\u4e2asubsets\n#     train_subset, val_subset = random_split(\n#         train_data, [test_abs, len(train_data) - test_abs])\n\n#     trainloader = torch.utils.data.DataLoader(\n#         train_subset,\n#         batch_size=int(config[\"batch_size\"]),\n#         shuffle=True,\n#         num_workers=8)\n#     valloader = torch.utils.data.DataLoader(\n#         val_subset,\n#         batch_size=int(config[\"batch_size\"]),\n#         shuffle=True,\n#         num_workers=8)\n\n#     for epoch in epoch_range:  # loop over the dataset multiple times\n#         running_loss = 0.0\n#         epoch_steps = 0\n#         err=0\n#         for i, data_b in enumerate(trainloader, 0):\n  \n#             inputs, labels = data_b\n#             inputs, labels = inputs.to(device), labels.to(device)\n#     #       inputs = inputs.view(-1, 1891)\n#             # Zero the parameter gradients\n#             optimizer.zero_grad()\n#             # Forward + backward + optimize\n#             outputs = net(inputs.float())\n#             loss = loss_func(outputs.double(), labels)\n\n#             err+=loss.data.item()\n#             loss.backward()\n#             optimizer.step()\n\n#             # print statistics\n# #             running_loss += loss.item()\n#             epoch_steps += 1\n         \n#         Mean_error.append(err\/(i+1))\n\n#         # Validation loss\n#         val_loss = 0.0\n#         val_steps = 0\n#         total = 0\n#         correct = 0\n#         for i, data in enumerate(valloader, 0):\n#             with torch.no_grad():\n#                 inputs, labels = data\n#                 inputs, labels = inputs.to(device), labels.to(device)\n\n# #                 outputs = net(inputs)\n#                 outputs = net(inputs.float())\n# #         loss = loss_func(outputs.double(), labels)\n                \n#                 loss = loss_func(outputs.double(), labels)\n#                 val_loss += loss.cpu().numpy()\n                \n# #                 _, predicted = torch.max(outputs.data, 1)\n# #                 total += labels.size(0)\n# #                 correct += (predicted == labels).sum().item()\n# #                 loss = criterion(outputs, labels)\n# #                 val_loss += loss.cpu().numpy()\n#                 val_steps += 1\n\n#         with tune.checkpoint_dir(epoch) as checkpoint_dir:\n#             path = os.path.join(checkpoint_dir, \"checkpoint\")\n#             torch.save((net.state_dict(), optimizer.state_dict()), path)\n\n#         tune.report(loss=(val_loss \/ val_steps))\n#         print(\"epoch \",epoch, \"MSE_train=\",Mean_error[-1], \"MSE_val=\",val_loss \/ val_steps)\n#     print(\"Finished Training\")\n\n# def test_accuracy(net, device=\"cpu\"):\n# #     trainset, testset = load_data()\n\n# #     testloader = torch.utils.data.DataLoader(\n# #         testset, batch_size=4, shuffle=False, num_workers=2)\n#     loss_func = torch.nn.MSELoss(reduction=\"mean\") \n    \n#     test_loss=0\n#     test_steps = 0\n#     with torch.no_grad():\n#         for data in testloader:\n          \n            \n#             inputs, labels = data\n#             inputs, labels = inputs.to(device), labels.to(device)\n#             outputs = net(inputs.float())\n#             loss = loss_func(outputs.double(), labels)\n#             test_loss += loss.cpu().numpy()\n#             test_steps+=1\n            \n\n#     return test_loss \/ test_steps\n\n# !pip uninstall dataclasses -y\n# num_samples=10;\n# max_num_epochs=10;\n# gpus_per_trial=0\n\n# config = {\n#     \"l1\": tune.sample_from(lambda _: 2**np.random.randint(4, 7)),\n#     \"l2\": tune.sample_from(lambda _: 2**np.random.randint(4, 7)),\n#     \"lr\": tune.loguniform(1e-4, 1e-1),\n#     \"batch_size\": tune.choice([2, 4, 8, 16])\n# }\n\n# scheduler = ASHAScheduler(\n#         metric=\"loss\",\n#         mode=\"min\",\n#         max_t=max_num_epochs,\n#         grace_period=1,\n#         reduction_factor=2)\n\n# reporter = CLIReporter(\n#         # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n#         metric_columns=[\"loss\",  \"training_iteration\"])\n\n# result = tune.run(\n#         partial(train_cifar, data_dir=\".\/\"),\n#         resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n#         config=config,\n#         num_samples=num_samples,\n#         scheduler=scheduler,\n#         progress_reporter=reporter)\n\n# best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n# print(\"Best trial config: {}\".format(best_trial.config))\n# print(\"Best trial final validation loss: {}\".format(\n#     best_trial.last_result[\"loss\"]))\n# # print(\"Best trial final validation accuracy: {}\".format(\n# #     best_trial.last_result[\"accuracy\"]))\n\n\n# best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n# device = \"cpu\"\n# if torch.cuda.is_available():\n#     device = \"cuda:0\"\n#     if gpus_per_trial > 1:\n#         best_trained_model = nn.DataParallel(best_trained_model)\n# best_trained_model.to(device)\n\n# best_checkpoint_dir = best_trial.checkpoint.value\n# model_state, optimizer_state = torch.load(os.path.join(\n#     best_checkpoint_dir, \"checkpoint\"))\n# best_trained_model.load_state_dict(model_state)\n\n# test_acc = test_accuracy(best_trained_model, device)\n# print(\"Best trial test set accuracy: {}\".format(test_acc))\n# net=best_trained_model\n","0ebc2024":"# config = {\n#     \"l1\": 64,\n#     \"l2\": 32,\n#     \"lr\":0.00066,\n#     \"batch_size\":  2\n# }\n# train_cifar(config,data_dir=\".\/\")\n\n# st trial config: {'l1': 64, 'l2': 32, 'lr': 0.0006583461551036207, 'batch_size': 2}\n# Best trial final validation loss: 0.27679049782137505","d18bdc3a":"torch.manual_seed(3)    # reproducible\n\n# net = torch.nn.Sequential(\n#         torch.nn.Linear(len(train_data[0][0]), 16),\n#         torch.nn.ReLU(),\n#         torch.nn.Linear(16, 32),\n#         torch.nn.ReLU(),\n#         torch.nn.Linear(32, 1),\n#     )\n\n\nnet = torch.nn.Sequential(\n        torch.nn.Linear(len(train_data[0][0]), 32),\n        torch.nn.LeakyReLU(),\n        torch.nn.Linear(32, 16),\n        torch.nn.LeakyReLU(),\n        torch.nn.Linear(16, 16),\n        torch.nn.LeakyReLU(),\n        torch.nn.Linear(16, 1),\n    )\n\n# print(net)  # net architecture\n# optimizer = torch.optim.Adam(net.parameters(), lr=0.007)\noptimizer = optim.SGD(net.parameters(), lr=0.007) #\u7528\u65b0\u7684optimzer\nloss_func = torch.nn.MSELoss(reduction=\"mean\")  # this is for regression mean squared loss\n\nMean_error=[]\nepoch_range=range(200)\nfor epoch in epoch_range: # loop over the dataset multiple times\n    err=0\n    \n    for i, data_b in enumerate(trainloader, 0):\n    # Get the inputs\n        inputs, labels = data_b\n#         inputs = inputs.view(-1, 1891)\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n        # Forward + backward + optimize\n        outputs = net(inputs.float())    \n        \n        loss = loss_func(outputs.double(), labels)\n        \n        err+=loss.data.item()\n        \n#         l1_crit = nn.L1Loss(size_average=False)\n#         reg_loss = 0\n#         for param in net.parameters():\n#             reg_loss += l1_crit(param,target=torch.zeros_like(param))\n#         factor = 0.02\n#         loss += factor * reg_loss\n\n        loss.backward()\n        optimizer.step()\n        \n    Mean_error.append(err\/(i+1))\n#     print(\"epoch=\",epoch,\"loss=\",loss.data.item())\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# epoch_range=range(500)\n# view data\nplt.figure(figsize=(10,4))\nplt.scatter(epoch_range, Mean_error, color = \"orange\")\nplt.title('convergence')\nplt.xlabel('epoch')\nplt.ylabel('Mean squared error')\nplt.show()","4df5bc93":"correct, total = 0, 0\npredictions = []\nloss_func = torch.nn.MSELoss(reduction=\"mean\")  # this is for regression mean squared loss\n\n\nnet.eval()\n\nLossTotal_train=[0];LossTotal_test=[0];\npredict_vector_train=[];real_label_train=[]\npredict_vector_test=[];real_label_test=[]\ninput_matrix_train=[torch.empty(0,train_x.shape[1])];\ninput_matrix_test=[torch.empty(0,test_x.shape[1])];\n\nfor class_ in [\"train\",\"test\"]:\n    loader=locals()[class_+\"loader\"]\n    LossTotal=locals()[\"LossTotal_\"+class_]\n    predict_vector=locals()[\"predict_vector_\"+class_]\n    real_label=locals()[\"real_label_\"+class_]\n    input_matrix=locals()[\"input_matrix_\"+class_]\n    \n    for i, data_b in enumerate(loader, 0):\n        inputs, labels = data_b\n        # inputs = inputs.view(-1, 32*32*3)\n        outputs = net(inputs.float())\n    #     _, predicted = torch.max(outputs.data, 1)\n        loss = loss_func(outputs.double(), labels)\n        predict_vector+=list(outputs.double().detach().numpy().T[0])\n        real_label+=list(labels.double().detach().numpy().T[0])\n        input_matrix[0]=torch.cat((input_matrix[0],inputs),0)\n        LossTotal[0]+=loss.data.item()\n        \n    LossTotal[0]\/= (i+1)\n    print(\"The R2 score for \"+class_+\" is:\",r2_score(real_label,predict_vector))\n    print('The Root Mean Squared Error (RMSE) for ' + class_+' is :',np.sqrt(LossTotal[0])) ","f05cf757":"# print(len(test_data)) \n# outputs.shape,labels.shape\n# outputs.numpy(),labels.numpy()\n# print(outputs.detach().numpy())\nplt.figure(figsize=(10,10))\n\ncolors_NN=[\"red\",\"blue\"]\nfor ci,class_ in enumerate([\"train\",\"test\"]):\n    loader=locals()[class_+\"loader\"]\n    predict_vector=locals()[\"predict_vector_\"+class_]\n    real_label=locals()[\"real_label_\"+class_]\n    LossTotal=locals()[\"LossTotal_\"+class_][0]\n    \n    \n    for i, data_b in enumerate(loader, 0):\n        inputs, labels = data_b\n        # inputs = inputs.view(-1, 32*32*3)\n        outputs = net(inputs.float())\n        plt.scatter(outputs.detach().numpy(),labels.numpy(),c=colors_NN[ci],s=2)\n    plt.scatter(outputs.detach().numpy(),labels.numpy(),c=colors_NN[ci],\n                label=class_+\",RMSE=\"+str(round(np.sqrt(LossTotal),3))+\", R2=\"+str(round(r2_score(real_label,predict_vector),3))\n                ,s=2)\n    plt.xlabel(\"predicted logp\")\n    plt.ylabel(\"real logp\")\n\n_=plt.legend()\n\nplt.figure(figsize=(10,4))\nplt.plot(range(100),real_label[:100],c=colors_NN[0],\n                label=\"real logp\",marker=\"*\")\nplt.plot(range(100),predict_vector[:100],c=colors_NN[1],\n                label=\"predicted logp\",marker=\".\")\nplt.title(\"the first 100 samples\")\nplt.xlabel(\"sample index in test set\")\nplt.ylabel(\"logp\")\n_=plt.legend()\n\n","82e32d9d":"print(sum(abs(np.array(predict_vector_test)-np.array(real_label_test))>0.5)\/len(real_label_test))\nprint(sum(abs(np.array(predict_vector_train)-np.array(real_label_train))>0.5)\/len(real_label_train))","ecc8101f":"input_train_df=pd.DataFrame(input_matrix_train[0].numpy(),columns=train_x.columns)\ninput_train_df[\"label\"]=real_label_train\nTrain_NN_0p5=input_train_df[abs(np.array(predict_vector_train)-np.array(real_label_train))>0.5]\nTrain_NN_0p3=input_train_df[abs(np.array(predict_vector_train)-np.array(real_label_train))>0.3]\n\ninput_test_df=pd.DataFrame(input_matrix_test[0].numpy(),columns=test_x.columns)\ninput_test_df[\"label\"]=real_label_test\nTest_NN_0p5=input_test_df[abs(np.array(predict_vector_test)-np.array(real_label_test))>0.5]\nTest_NN_0p3=input_test_df[abs(np.array(predict_vector_test)-np.array(real_label_test))>0.3]\nfor nth in range(len(Train_NN_0p5)):\n    if not abs(net(torch.tensor(Train_NN_0p5.iloc[nth,:-1].values).float().reshape(1,9)).double().item()-Train_NN_0p5.iloc[nth,-1])>0.5:\n         print(nth)\nfor nth in range(len(Test_NN_0p5)):\n    if not abs(net(torch.tensor(Test_NN_0p5.iloc[nth,:-1].values).float().reshape(1,9)).double().item()-Test_NN_0p5.iloc[nth,-1])>0.5:\n         print(nth)\nfor nth in range(len(Train_NN_0p3)):\n    if not abs(net(torch.tensor(Train_NN_0p3.iloc[nth,:-1].values).float().reshape(1,9)).double().item()-Train_NN_0p3.iloc[nth,-1])>0.3:\n         print(nth)\nfor nth in range(len(Test_NN_0p3)):\n    if not abs(net(torch.tensor(Test_NN_0p3.iloc[nth,:-1].values).float().reshape(1,9)).double().item()-Test_NN_0p3.iloc[nth,-1])>0.3:\n         print(nth)\nprint(\"If no numbers printed out, then Train_NN_0p5,Train_NN_0p3, Test_NN_0p5, Test_NN_0p3 are the \\\n      DataFrames that describe the records where the difference between prediction and real labels \\\n       are larger than 0.5, 0.3 for train and test sets, respectively.\")","56763ef6":"#features and label\nx=data1.loc[:, data1.columns != 'logp']\ny=data1[\"logp\"];\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split \n\n#split data\n# Splitting \ntrain_X, test_X, train_y, test_y = train_test_split(x, y, test_size = 0.2, random_state = 123)\n# msk = np.random.rand(len(x)) < 0.7\n# train_x = x[msk];  train_y = y[msk];\n# test_x  = x[~msk];  test_y = y[~msk];\n\n# c_features=list(set(features_discrete_many)-set(['a_count']))\n# Train_x[c_features]=Train_x[c_features].astype(\"category\")\n# Test_x[c_features]=Test_x[c_features].astype(\"category\")\n\n# parameters = {'C':[1, 15]}\nscaler = preprocessing.StandardScaler().fit(train_X)\n\nX_train_transformed = scaler.transform(train_X)\n# clf = GridSearchCV(SVR(epsilon=0.2), parameters).fit(X_train_transformed, train_y)\nX_test_transformed = scaler.transform(test_X)\n\nregr = RandomForestRegressor(max_depth=10, random_state=0)\nregr.fit(X_train_transformed, train_y)","43d5557a":"print(np.sqrt(sum((regr.predict(X_train_transformed)-train_y.values)**2)\/len(train_y)))\nprint(np.sqrt(sum((regr.predict(X_test_transformed)-test_y.values)**2)\/len(test_y)))","c2c9c606":"plt.figure(figsize=(7,7))\n\ncolors_RE=[\"red\",\"blue\"]\npre_train=[0];\npre_test=[0];\n\nalpha=[1,0.1]\nfor ir,class_ in enumerate([\"train\",\"test\"]):\n    \n    feature_transformed=locals()[\"X_\"+class_+\"_transformed\"]\n    ydata=locals()[class_+\"_y\"].values\n    pre=locals()[\"pre_\"+class_]\n    pre[0]=regr.predict(feature_transformed)  \n    \n    r2=r2_score(pre[0],ydata)\n    RMSE=np.sqrt(sum((pre[0]-ydata)**2)\/len(ydata))\n    plt.scatter(pre[0],ydata,c=colors_RE[ir],\n                label=class_+\",RMSE=\"+str(round(RMSE,3))+\",R2=\"+str(round(r2,3))\n                ,alpha=alpha[ir],s=10)\n    plt.xlabel(\"predicted logp\")\n    plt.ylabel(\"real logp\")\n    plt.title(\"Random Forest model\")\n\n_=plt.legend()\n\n\nplt.figure(figsize=(10,4))\nplt.plot(range(100),ydata[:100],c=colors_RE[0],\n                label=\"real logp\",marker=\"*\")\nplt.plot(range(100),pre[0][:100],c=colors_RE[1],\n                label=\"predicted logp\",marker=\".\")\nplt.title(\"the first 100 samples\")\nplt.xlabel(\"sample index in test set\")\nplt.ylabel(\"logp\")\n_=plt.legend()\n\n","43876364":"input_train_df=pd.DataFrame(X_train_transformed);input_train_df[\"label\"]=train_y.values\nTrain_RF_0p5=input_train_df[abs(pre_train[0]-train_y.values)>0.5]\nTrain_RF_0p3=input_train_df[abs(pre_train[0]-train_y.values)>0.3]\n\ninput_test_df=pd.DataFrame(X_test_transformed);input_test_df[\"label\"]=test_y.values\nTest_RF_0p5=input_test_df[abs(pre_test[0]-test_y.values)>0.5]\nTest_RF_0p3=input_test_df[abs(pre_test[0]-test_y.values)>0.3]\n\nfor nth in range(len(Train_RF_0p5)):\n    if not abs(regr.predict(torch.tensor(Train_RF_0p5.iloc[nth,:-1].values).float().reshape(1,9))[0]-Train_RF_0p5.iloc[nth,-1])>0.5:\n         print(nth)\nfor nth in range(len(Test_RF_0p5)):\n    if not abs(regr.predict(torch.tensor(Test_RF_0p5.iloc[nth,:-1].values).float().reshape(1,9))[0]-Test_RF_0p5.iloc[nth,-1])>0.5:\n         print(nth)\nfor nth in range(len(Train_RF_0p3)):\n    if not abs(regr.predict(torch.tensor(Train_RF_0p3.iloc[nth,:-1].values).float().reshape(1,9))[0]-Train_RF_0p3.iloc[nth,-1])>0.3:\n         print(nth)\nfor nth in range(len(Test_RF_0p3)):\n    if not abs(regr.predict(torch.tensor(Test_RF_0p3.iloc[nth,:-1].values).float().reshape(1,9))[0]-Test_RF_0p3.iloc[nth,-1])>0.3:\n         print(nth)\nprint(\"If no numbers printed out, then Train_RF_0p5,Train_RF_0p3, Test_RF_0p5, Test_RF_0p3 are the DataFrames that describe the records where the difference between prediction and real labels \\\n       are larger than 0.5, 0.3 for train and test sets, respectively.\")\n\n# pre_train[0]-train_y.values\n# pre_test[0],test_y","2a71d999":"print(\"percents of samples in test set that predicted logp deviates from real logp more than 0.5 and 0.3:\")\nprint(\" \"*13,round(len(Test_RF_0p5)\/len(test_y),3),\" \"*13,round(len(Test_RF_0p3)\/len(test_y),3))","af317d32":"#features and label\nx=data1.loc[:, data1.columns != 'logp']\ny=data1[\"logp\"];\n\n# Necessary imports \nimport numpy as np \nimport pandas as pd \nimport xgboost as xg \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import mean_squared_error as MSE \n\nfrom sklearn.model_selection import cross_val_score,cross_val_predict,cross_validate\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import preprocessing\n\n\n# Load the data \n# dataset = pd.read_csv(\"boston_house.csv\") \n# X, y = dataset.iloc[:, :-1], dataset.iloc[:, -1] \n\n# Splitting \n# train_X, test_X, train_y, test_y = train_test_split(x, y, test_size = 0.3, random_state = 123) \n\n# Instantiation \nxgb_r = xg.XGBRegressor(objective ='reg:squarederror', n_estimators = 200, seed = 12) \n\n# Fitting the model \nxgb_r.fit(train_X, train_y) \n\n# Predict the model \npred = xgb_r.predict(test_X) \n\n# RMSE Computation \n\n\nrmse = np.sqrt(MSE(train_y, xgb_r.predict(train_X) )) \nprint(\"RMSE for train set: % f\" %(rmse)) \n\nrmse = np.sqrt(MSE(test_y, pred)) \nprint(\"RMSE for test set: % f\" %(rmse)) \n\n\n\n# # GridSearch\n# parameters = {\"reg_alpha\":[0.01,0.05,0.1,0.5,1,3,5,7,9,10,12,15],\n#               \"reg_lambda\":[0.01,0.05,0.1,0.5,1,3,5,7,9,10,12,15]}\n\n\n# scaler = preprocessing.StandardScaler().fit(train_X)\n# # X_train_transformed = scaler.transform(train_X)\n# # clf = svm.SVC(C=1).fit(X_train_transformed, y_train)\n# clf_xgb = GridSearchCV(xg.XGBRegressor(objective ='reg:squarederror', n_estimators = 200, seed = 12,verbosity=1),\n#                    parameters).fit(train_X, train_y)\n# # X_test_transformed = scaler.transform(test_X)\n\n# print(\"RMSE for train set(optimized): % f\"%(np.sqrt(MSE(train_y, clf_xgb.best_estimator_.predict(train_X)))))\n# print(\"RMSE for test set(optimized): % f\"%(np.sqrt(MSE(test_y, clf_xgb.best_estimator_.predict(test_X)))))\n\n# print(np.sqrt(MSE(test_y, clf_xgb.best_estimator_.predict(test_X) ))) ","a7b6078a":"# print(np.sqrt(MSE(train_y, xgb_r.predict(train_X))))\n# print(np.sqrt(MSE(test_y, xgb_r.predict(test_X) ))) ","210de532":"# plt.scatter(pred,test_y,label=\"test\")\n# plt.scatter(xgb_r.predict(train_X) ,train_y,c=\"red\",label=\"train\")\n# _=plt.legend()\n\n#Unoptimized xgb Model\nplt.figure(figsize=(7,7))\npredict_train=xgb_r.predict(train_X)\ntrainLabel=\"train: RMSE=\"+str(round(np.sqrt(MSE(train_y, predict_train)),3 ))+\",R2=\"+str(round(r2_score(predict_train,train_y),3))\npredict_test=xgb_r.predict(test_X) \ntestLabel=\"test: RMSE=\"+str(round(np.sqrt(MSE(test_y, predict_test)) ,3))+\",R2=\"+str(round(r2_score(predict_test,test_y),3))\n\nplt.scatter(xgb_r.predict(train_X),train_y,c=\"red\",label=trainLabel,s=2)\nplt.scatter(xgb_r.predict(test_X),test_y,c=\"blue\",label=testLabel,s=2,alpha=0.3)\n\nplt.title(\"XGB regression model\")\nplt.xlabel(\"predicted logp\")\nplt.ylabel(\"real logp\")\n_=plt.legend()\n\n# #GridSearchCV optimized xgb Model\n# plt.figure(figsize=(7,7))\n# predict_train=clf_xgb.best_estimator_.predict(train_X)\n# trainLabel=\"train: RMSE=\"+str(round(np.sqrt(MSE(train_y, predict_train)),3 ))+\",R2=\"+str(round(r2_score(predict_train,train_y),3))\n# predict_test=clf_xgb.best_estimator_.predict(test_X) \n# testLabel=\"test: RMSE=\"+str(round(np.sqrt(MSE(test_y, predict_test)) ,3))+\",R2=\"+str(round(r2_score(predict_test,test_y),3))\n\n# plt.scatter(clf_xgb.predict(train_X),train_y,c=\"red\",label=trainLabel,s=2)\n# plt.scatter(clf_xgb.predict(test_X),test_y,c=\"blue\",label=testLabel,s=2,alpha=0.6)\n\n# plt.title(\"optimized XGB regression model\")\n# plt.xlabel(\"predicted logp\")\n# plt.ylabel(\"real logp\")\n# _=plt.legend()\n\n\nplt.figure(figsize=(10,5))\nplt.plot(range(100),test_y[:100],c=colors_RE[0],\n                label=\"real logp\",marker=\"*\")\nplt.plot(range(100),predict_test[:100],c=colors_RE[1],\n                label=\"predicted logp\",marker=\".\")\nplt.title(\"the first 100 test samples of optimized XGB regression model\")\nplt.xlabel(\"sample index\")\nplt.ylabel(\"logp\")\n_=plt.legend()\n\n","e11372a0":"from copy import deepcopy\npredict_train=xgb_r.predict(train_X);\npredict_test=xgb_r.predict(test_X);\n\ninput_train_df=deepcopy(train_X);input_train_df[\"label\"]=train_y.values\nTrain_XGB_0p5=input_train_df[abs(predict_train-train_y.values)>0.5]\nTrain_XGB_0p3=input_train_df[abs(predict_train-train_y.values)>0.3]\nTrain_XGB_0L2=input_train_df[abs(predict_train-train_y.values)<0.2]\nTrain_XGB_0L1=input_train_df[abs(predict_train-train_y.values)<0.1]\n\ninput_test_df=deepcopy(test_X);input_test_df[\"label\"]=test_y.values\nTest_XGB_0p5=input_test_df[abs(predict_test-test_y.values)>0.5]\nTest_XGB_0p3=input_test_df[abs(predict_test-test_y.values)>0.3]\nTest_XGB_0L2=input_test_df[abs(predict_test-test_y.values)<0.2]\nTest_XGB_0L1=input_test_df[abs(predict_test-test_y.values)<0.1]\n\nfor nth in range(len(Train_XGB_0p5)):\n    if not abs(xgb_r.predict(Train_XGB_0p5.iloc[nth,:-1].to_frame().T)[0]-Train_XGB_0p5.iloc[nth,-1])>0.5:\n         print(nth)\nfor nth in range(len(Test_XGB_0p5)):\n#     if not abs(net(torch.tensor(Test_XGB_0p5.iloc[nth,:-1].values).float().reshape(1,9)).double().item()-Test_XGB_0p5.iloc[nth,-1])>0.5:\n#          print(nth)\n     if not abs(xgb_r.predict(Test_XGB_0p5.iloc[nth,:-1].to_frame().T)[0]-Test_XGB_0p5.iloc[nth,-1])>0.5:\n         print(nth)\nfor nth in range(len(Train_XGB_0p3)):\n#     if not abs(net(torch.tensor(Train_XGB_0p3.iloc[nth,:-1].values).float().reshape(1,9)).double().item()-Train_XGB_0p3.iloc[nth,-1])>0.3:\n#          print(nth)\n     if not abs(xgb_r.predict(Train_XGB_0p3.iloc[nth,:-1].to_frame().T)[0]-Train_XGB_0p3.iloc[nth,-1])>0.3:\n         print(nth)\nfor nth in range(len(Test_XGB_0p3)):\n    if not abs(xgb_r.predict(Test_XGB_0p3.iloc[nth,:-1].to_frame().T)[0]-Test_XGB_0p3.iloc[nth,-1])>0.3:\n         print(nth)\nprint(\"If no numbers printed out, then Train_NN_0p5,Train_NN_0p3, Test_NN_0p5, Test_NN_0p3 are the \\\n      DataFrames that describe the records where the difference between prediction and real labels \\\n       are larger than 0.5, 0.3 for train and test sets, respectively.\")\n","81002d7f":"sum(abs((predict_test-test_y).values)>0.5)\/len(test_y)","ca66ad25":"#features and label\nx=data1.loc[:, data1.columns != 'logp']\ny=data1[\"logp\"];\n# Splitting \n# train_X, test_X, train_y, test_y = train_test_split(x, y, test_size = 0.2, random_state = 123)\n\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nregr = make_pipeline(StandardScaler(), SVR(C=10, epsilon=0.2))\nregr.fit(train_X, train_y)","f5f585f6":"print(np.sqrt(sum((regr.predict(train_X)-train_y.values)**2)\/len(train_y)))\nprint(np.sqrt(sum((regr.predict(test_X)-test_y.values)**2)\/len(test_y)))","86b8beb7":"plt.figure(figsize=(7,7))\npredict_train=regr.predict(train_X)\ntrain_RMSE=np.sqrt(sum((predict_train-train_y.values)**2)\/len(train_y))\npredict_test=regr.predict(test_X)\ntest_RMSE=np.sqrt(sum((predict_test-test_y.values)**2)\/len(test_y))\nr2=round(r2_score(predict_train,train_y.values),3)\ntrainLabel=\"train: RMSE=\"+str(round(train_RMSE,3))+\" R2=\"+str(r2)\nr2=round(r2_score(predict_test,test_y.values),3)\ntestLabel=\"test: RMSE=\"+str(round(test_RMSE,3))+\" R2=\"+str(r2)\n\nplt.scatter(predict_train,train_y,c=\"red\",label=trainLabel,s=2)\nplt.scatter(predict_test,test_y,c=\"blue\",label=testLabel,s=2)\n\nplt.title(\"SVR: SVM regression\")\n\nplt.xlabel(\"predicted logp\")\nplt.ylabel(\"real logp\")\n_=plt.legend()","f1d90997":"from sklearn.model_selection import cross_val_score,cross_val_predict,cross_validate\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import preprocessing\n\nparameters = {'C':[1,3,5,7,10,12,13,14, 15]}\nscaler = preprocessing.StandardScaler().fit(train_X)\nX_train_transformed = scaler.transform(train_X)\n# clf = svm.SVC(C=1).fit(X_train_transformed, y_train)\nclf_svm = GridSearchCV(SVR(epsilon=0.2), parameters).fit(X_train_transformed, train_y)\nX_test_transformed = scaler.transform(test_X)\n\n","d71b652f":"# plt.figure(figsize=(10,4))\n# train_RMSE=np.sqrt(sum((clf.best_estimator_.predict(X_train_transformed)-train_y.values)**2)\/len(train_y))\n# test_RMSE=np.sqrt(sum((clf.best_estimator_.predict(X_test_transformed)-test_y.values)**2)\/len(test_y))\n\n# trainLabel=\"train: RMSE=\"+str(round(train_RMSE,4))\n# testLabel=\"test: RMSE=\"+str(round(test_RMSE,4))\n# plt.scatter(clf.best_estimator_.predict(X_test_transformed),test_y,c=\"blue\",label=testLabel)\n# plt.scatter(clf.best_estimator_.predict(X_train_transformed),train_y,c=\"red\",label=trainLabel)\n# plt.title(\"SVR: SVM---GridSearchCV regression\")\n# _=plt.legend()\n\n\n\nplt.figure(figsize=(7,7))\npredict_train=clf_svm.best_estimator_.predict(X_train_transformed)\ntrain_RMSE=np.sqrt(sum((predict_train-train_y.values)**2)\/len(train_y))\npredict_test=clf_svm.best_estimator_.predict(X_test_transformed)\ntest_RMSE=np.sqrt(sum((predict_test-test_y.values)**2)\/len(test_y))\n\nr2=round(r2_score(predict_train,train_y.values),3)\ntrainLabel=\"train: RMSE=\"+str(round(train_RMSE,3))+\" R2=\"+str(r2)\nr2=round(r2_score(predict_test,test_y.values),3)\ntestLabel=\"test: RMSE=\"+str(round(test_RMSE,3))+\" R2=\"+str(r2)\n\nplt.scatter(predict_test,test_y,c=\"blue\",label=testLabel,s=2)\nplt.scatter(predict_train,train_y,c=\"red\",label=trainLabel,s=2)\nplt.title(\"Optimized SVR: SVM regression\")\n\nplt.xlabel(\"predicted logp\")\nplt.ylabel(\"real logp\")\n_=plt.legend()\n\n\n\nplt.figure(figsize=(10,4))\npre=predict_test\nplt.plot(range(pre[:100].shape[0]),test_y[:100],c=colors_RE[0],\n                label=\"real logp\",marker=\"*\")\nplt.plot(range(pre[:100].shape[0]),pre[:100],c=colors_RE[1],\n                label=\"predicted logp\",marker=\".\")\nplt.title(\"the first 100 test samples of optimized svm\/svr regression model\")\nplt.xlabel(\"sample index\")\nplt.ylabel(\"logp\")\n_=plt.legend()","16c1765b":"# clf_svm=clf","cd4a3124":"from copy import deepcopy\npredict_train=clf_svm.best_estimator_.predict(X_train_transformed)\npredict_test=clf_svm.best_estimator_.predict(X_test_transformed)\n\ninput_train_df=deepcopy(train_X);input_train_df[\"label\"]=train_y.values\nTrain_svm_0p5=input_train_df[abs(predict_train-train_y.values)>0.5]\nTrain_svm_0p3=input_train_df[abs(predict_train-train_y.values)>0.3]\nTrain_svm_0L2=input_train_df[abs(predict_train-train_y.values)<0.2]\nTrain_svm_0L1=input_train_df[abs(predict_train-train_y.values)<0.1]\n\ninput_test_df=deepcopy(test_X);input_test_df[\"label\"]=test_y.values\nTest_svm_0p5=input_test_df[abs(predict_test-test_y.values)>0.5]\nTest_svm_0p3=input_test_df[abs(predict_test-test_y.values)>0.3]\nTest_svm_0L2=input_test_df[abs(predict_test-test_y.values)<0.2]\nTest_svm_0L1=input_test_df[abs(predict_test-test_y.values)<0.1]\n\nfor nth in range(len(Train_svm_0p5)):\n    if not abs(clf_svm.predict(scaler.transform(Train_svm_0p5.iloc[nth,:-1].to_frame().T))[0]-Train_svm_0p5.iloc[nth,-1])>0.5:\n         print(nth)\nfor nth in range(len(Test_svm_0p5)):\n     if not abs(clf_svm.predict(scaler.transform(Test_svm_0p5.iloc[nth,:-1].to_frame().T))[0]-Test_svm_0p5.iloc[nth,-1])>0.5:\n             print(nth)\nfor nth in range(len(Train_svm_0p3)):\n      if not abs(clf_svm.predict(scaler.transform(Train_svm_0p3.iloc[nth,:-1].to_frame().T))[0]-Train_svm_0p3.iloc[nth,-1])>0.3:\n             print(nth)\nfor nth in range(len(Test_svm_0p3)):\n     if not abs(clf_svm.predict(scaler.transform(Test_svm_0p3.iloc[nth,:-1].to_frame().T))[0]-Test_svm_0p3.iloc[nth,-1])>0.3:\n             print(nth)\nprint(\"If no numbers printed out, then Train_svm_0p5,Train_svm_0p3, Test_svm_0p5, Test_svm_0p3 are the \\\n      DataFrames that describe the records where the difference between prediction and real labels \\\n       are larger than 0.5, 0.3 for train and test sets, respectively.\")\n","25eb7e22":"# The dataFrames with 0.5 and 0.3 deviations between predictions and real logp:\ndisplay(Train_svm_0p5.head())\ndisplay(Train_svm_0p3.head())\ndisplay(Test_svm_0p5.head())\ndisplay(Test_svm_0p3.head())\n\n# display(Train_XGB_0p5.head())\n# display(Train_XGB_0p3.head())\n# display(Test_XGB_0p5.head())\n# display(Test_XGB_0p3.head())\n\n\n# display(Train_RF_0p5.head())\n# display(Train_RF_0p3.head())\n# display(Test_RF_0p5.head())\n# display(Test_RF_0p3.head())\n\n\n# display(Train_NN_0p5.head())\n# display(Train_NN_0p3.head())\n# display(Test_NN_0p5.head())\n# display(Test_NN_0p3.head())\n\n","7252cfcd":"# svm and xgb data are original(features not scaled, while for rf and nn data, featurs are scaled ) \n# great overlap was found between xgb and svm: \nmodelNames=[\"NN\",\"RF\",\"XGB\",\"svm\"]\n\nUnionPercent_05_list=[]\nUnionPercent_03_list=[]\nfor  modelName1 in modelNames:\n    row05=[]; row03=[];\n    for modelName2 in modelNames:\n#     modelName2=\"XGB\"\n        model1=locals()[\"Train_\"+modelName1+\"_0p5\"]\n        model2=locals()[\"Train_\"+modelName2+\"_0p5\"]\n        perc=len(set(model1.index).intersection(set(model2.index)))\/ \\\n        min(len(set(model1.index)),len(set(model2.index)))\n        row05.append(perc)\n        \n        model1=locals()[\"Train_\"+modelName1+\"_0p3\"]\n        model2=locals()[\"Train_\"+modelName2+\"_0p3\"]\n        perc=len(set(model1.index).intersection(set(model2.index)))\/ \\\n        min(len(set(model1.index)),len(set(model2.index)))\n        row03.append(perc)\n    \n    UnionPercent_05_list.append(row05)\n    UnionPercent_03_list.append(row03)   \n\nPercent_03_df=pd.DataFrame(UnionPercent_03_list,modelNames,modelNames)\nPercent_05_df=pd.DataFrame(UnionPercent_05_list,modelNames,modelNames)\n\ndisplay(Percent_03_df)\ndisplay(Percent_05_df)","c8078ddd":"# svm and xgb data are original(features not scaled, while for rf and nn data, featurs are scaled ) \n# great overlap was found between xgb and svm: \nmodelNames=[\"NN\",\"RF\",\"XGB\",\"svm\"]\n\nUnionPercent_05_list=[]\nUnionPercent_03_list=[]\nfor  modelName1 in modelNames:\n    row05=[]; row03=[];\n    for modelName2 in modelNames:\n#     modelName2=\"XGB\"\n        model1=locals()[\"Test_\"+modelName1+\"_0p5\"]\n        model2=locals()[\"Test_\"+modelName2+\"_0p5\"]\n        perc=len(set(model1.index).intersection(set(model2.index)))\/ \\\n        min(len(set(model1.index)),len(set(model2.index)))\n        row05.append(perc)\n        \n        model1=locals()[\"Test_\"+modelName1+\"_0p3\"]\n        model2=locals()[\"Test_\"+modelName2+\"_0p3\"]\n        perc=len(set(model1.index).intersection(set(model2.index)))\/ \\\n        min(len(set(model1.index)),len(set(model2.index)))\n        row03.append(perc)\n    \n    UnionPercent_05_list.append(row05)\n    UnionPercent_03_list.append(row03)   \n\nPercent_03_df=pd.DataFrame(UnionPercent_03_list,modelNames,modelNames)\nPercent_05_df=pd.DataFrame(UnionPercent_05_list,modelNames,modelNames)\n\ndisplay(Percent_03_df)\ndisplay(Percent_05_df)","4e3e634d":"Test_XGB_0p5.head()","75a53269":"import seaborn as sns\nfig, axes = plt.subplots(2, 2, figsize=(15, 12), sharey=True,sharex=True)\nsns.set_theme(style=\"whitegrid\")\nfig.suptitle('common rows(between XGB\/svm) with deviations larger or smaller than a given number')\nx_feature=\"a_nS\"; y_feature=\"h_mr\"; hue_feature=\"a_acc\";\n\nsub_axes=list(np.array([[[i,j] for i in range(2)] for j in range(2)]).reshape(-1,2))\ntitles=[\"deviation>0.5\",\"deviation>0.3\",\"deviation<0.2\",\"deviation<0.1\"]\nfor pn in [\"0p5\",\"0p3\",\"0L2\",\"0L1\"]:\n    i,j=sub_axes.pop(0)\n    model1 = locals()[\"Test_XGB_\"+pn]\n    model2 = locals()[\"Test_svm_\"+pn]\n    Uset=set(model1.index).intersection(set(model2.index))\n    ax1= sns.swarmplot(ax=axes[i,j],x=x_feature, y=y_feature, hue=hue_feature,\n                       data=model1.loc[Uset,:])\n    _=ax1.set_title(titles.pop(0))","e4e2f2a4":"fig, axes = plt.subplots(2, 2, figsize=(15, 12), sharey=True,sharex=True)\nsns.set_theme(style=\"whitegrid\")\nfig.suptitle('common rows(between XGB\/svm) with deviations larger or smaller than a given number')\nx_feature=\"a_acc\"; y_feature=\"h_logS\"; hue_feature=\"a_nO\";\n\nsub_axes=list(np.array([[[i,j] for i in range(2)] for j in range(2)]).reshape(-1,2))\ntitles=[\"deviation>0.5\",\"deviation>0.3\",\"deviation<0.2\",\"deviation<0.1\"]\nfor pn in [\"0p5\",\"0p3\",\"0L2\",\"0L1\"]:\n    i,j=sub_axes.pop(0)\n    model1 = locals()[\"Test_XGB_\"+pn]\n    model2 = locals()[\"Test_svm_\"+pn]\n    Uset=set(model1.index).intersection(set(model2.index))\n    ax1= sns.swarmplot(ax=axes[i,j],x=x_feature, y=y_feature, hue=hue_feature,\n                       data=model1.loc[Uset,:])\n    _=ax1.set_title(titles.pop(0))","e42e45d8":"# sum(abs((predict_test-test_y).values)>0.5)\/len(test_y)\nmodel1.loc[Uset,:]","a8373ca6":"# train_RMSE=np.sqrt(sum((clf.best_estimator_.predict(X_train_transformed)-train_y.values)**2)\/len(train_y))\n# test_RMSE=np.sqrt(sum((clf.best_estimator_.predict(X_test_transformed)-test_y.values)**2)\/len(test_y))\n# train_RMSE,test_RMSE","f04a011a":"# X_y_train_h = h2o.H2OFrame(pd.concat([train_X, train_y], axis='columns'))\n# # X_y_train_h['dep_delayed_15min'] = X_y_train_h['dep_delayed_15min'].asfactor()\n# # ^ the target column should have categorical type for classification tasks\n# #   (numerical type for regression tasks)\n\n# X_test_h = h2o.H2OFrame(test_X)\n\n# X_y_train_h.describe()","c3031acf":"# from h2o.automl import H2OAutoML\n# SEED=17\n# aml = H2OAutoML(\n#     max_runtime_secs=(3600 * 8),  # 8 hours\n#     max_models=None,  # no limit\n#     seed=SEED,\n# )\n\n# aml.train(\n#     x=list(train_X.columns),\n#     y='logp',\n#     training_frame=X_y_train_h\n# )\n\n","0aafb200":"# lb = aml.leaderboard\n# model_ids = list(lb['model_id'].as_data_frame().iloc[:,0])\n# out_path = \".\"\n\n# for m_id in model_ids:\n#     mdl = h2o.get_model(m_id)\n#     h2o.save_model(model=mdl, path=out_path, force=True)\n\n# h2o.export_file(lb, os.path.join(out_path, 'aml_leaderboard.h2o'), force=True)","7229300b":"# TSNE","25a7911d":"#### Above data are the ratio of number of common rows in Train sets to the min of the numbers of rows of the two models for which the deviation of prediction is larger than real logp by 0.3 and 0.5,respectively. From the data, we found that svm and XGB share large overlap.\n","2a9ff318":"#### Above data are the ratios of number of common rows in Test sets to the min of the numbers of rows of the two models for which the deviation of prediction is larger than real logp by 0.3 and 0.5,respectively. From the data, we found that svm and XGB share large overlap.\n","26591360":"#### XGB is the best model we tried in this work.","f4627941":"# PCA","554de6ae":"# GridSearchCV of SVM","48c82a5c":"# Neural Network","29c4c28d":"# XGB","826ca08b":"# SVM:SVR","1392d6a3":"# Random Forest","973f2326":"# Data loading and feature selection"}}