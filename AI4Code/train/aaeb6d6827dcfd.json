{"cell_type":{"8d67684a":"code","99eb914e":"code","b1b53386":"code","69744a36":"code","1e6b7943":"code","59c1fb2b":"code","433b6566":"code","ca16adf8":"code","30f842da":"code","3a705880":"code","3c0145a3":"code","736da291":"code","11e91e6a":"code","c19b9231":"code","81c8adbc":"code","b5b9bf3c":"code","fffc1bce":"code","9fecdb5d":"code","43c9ca0f":"code","978c691f":"code","782c8439":"code","f403717e":"code","d1cea2f2":"code","362f8baa":"code","78e370a5":"code","e19e1543":"code","85bf9583":"code","33b0387d":"code","0f9c34ee":"code","f7b5d960":"code","00f3136a":"code","4cb303f7":"code","5ce6bd58":"code","3325fe64":"code","83cc797a":"code","4b5acd5b":"code","3920b699":"code","3430d8bb":"code","c5021f30":"code","68d5d3a3":"code","c0956e43":"code","dae8fad9":"code","661eaf9f":"code","83edf4db":"code","05723a4a":"code","9199b5d1":"code","a6355d53":"code","1cf3231c":"code","15e0a907":"code","be9f7cc8":"code","ae10f0ad":"code","92350d55":"code","f1ea4dff":"code","62d8e969":"code","1e747251":"code","2cecbc51":"code","be49dd1d":"code","2b7030e4":"code","faa56101":"code","c0881688":"code","3562cf4b":"markdown","b5f23371":"markdown","8c87b745":"markdown","c9f3c9da":"markdown","bf5758aa":"markdown","8ae33ebc":"markdown","1dc8a31b":"markdown","30894b71":"markdown","5af3e473":"markdown","f36ee4a4":"markdown","d83ec235":"markdown","7bfa17f9":"markdown","f247d6db":"markdown","f2267d6c":"markdown","a8d7b0e8":"markdown","48be31dd":"markdown","aa469f8d":"markdown","ef2241aa":"markdown","35a161dc":"markdown","c08c7c51":"markdown","2f441102":"markdown","46a43785":"markdown","39ba37f8":"markdown","922c4e48":"markdown"},"source":{"8d67684a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","99eb914e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","b1b53386":"sms_df=pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv',encoding='latin-1')","69744a36":"sms_df.head()","1e6b7943":"sms_df.shape","59c1fb2b":"sms_df.isnull().sum()","433b6566":"sms_df.isnull().mean()*100","ca16adf8":"sms_df.dropna(how='any',axis=1,inplace=True)","30f842da":"sms_df.head()","3a705880":"sms_df.columns=['Tag','Message']","3c0145a3":"sms_df.describe()","736da291":"sms_df.groupby('Tag').describe()","11e91e6a":"sms_df.info()","c19b9231":"sms_df['Tag'].unique()","81c8adbc":"sms_df['Tag']=np.where(sms_df['Tag']=='spam',1,0)","b5b9bf3c":"sms_df.head()","fffc1bce":"sms_df.describe()","9fecdb5d":"sms_df['Tag'].mean()*100","43c9ca0f":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score","978c691f":"X_train,X_test,y_train,y_test=train_test_split(sms_df['Message'],sms_df['Tag'],random_state=0)","782c8439":"# CountVectorizer\ncount=CountVectorizer().fit(X_train)\nX_train_Count=count.transform(X_train)\nX_test_count=count.transform(X_test)\n\n#TfidfVectorizer\n\nTfid=TfidfVectorizer().fit(X_train)\nX_train_Tfid=Tfid.transform(X_train)\nX_test_Tfid=Tfid.transform(X_test)","f403717e":"clf_count=MultinomialNB(alpha=0.1)\nclf_count.fit(X_train_Count,y_train)\npred=clf_count.predict(X_test_count)\nprint('ROC score by applying Countvectorizer:',roc_auc_score(y_test,pred))","d1cea2f2":"clf_Tfid=MultinomialNB(alpha=0.1)\nclf_Tfid.fit(X_train_Tfid,y_train)\npred=clf_Tfid.predict(X_test_Tfid)\nprint('ROC score by applying TfidfVectorizer:',roc_auc_score(y_test,pred))","362f8baa":"feature_names=np.array(count.get_feature_names())\ncount_coefficients=clf_count.coef_[0].argsort()\n\nprint('Smallest 20  Count vectorizer coefficients:\\n')\nprint(feature_names[count_coefficients[:20]])\nprint('\\n\\n')\nprint('Largest 20  Count vectorizer coefficients:\\n')\nprint(feature_names[count_coefficients[-21:-1]])","78e370a5":"feature_names=np.array(Tfid.get_feature_names())\nTfid_coefficients=clf_Tfid.coef_[0].argsort()\n\nprint('Smallest 20  Tfid vectorizer coefficients:\\n')\nprint(feature_names[Tfid_coefficients[:20]])\nprint('\\n\\n')\nprint('Largest 20  Tfid vectorizer coefficients:\\n')\nprint(feature_names[Tfid_coefficients[-21:-1]])","e19e1543":"from sklearn.dummy import DummyClassifier\n\ndummy=DummyClassifier(strategy='prior').fit(X_train_Count,y_train)","85bf9583":"dummy_predict=dummy.predict(X_test_count)\nroc_auc_score(y_test,dummy_predict)","33b0387d":"X_train_Count.shape","0f9c34ee":"sum1=X_train_Count.sum(axis=0)","f7b5d960":"len(count.get_feature_names())","00f3136a":"sum1.shape","4cb303f7":"# Countvectorizer Features\n\ndata=[]\n\nfor col,features in enumerate(count.get_feature_names()):\n    data.append([features,sum1[0,col]])\n    \nfeature_data=pd.DataFrame(data,columns=['Feature','Score'])\nfeature_data.sort_values(by='Score',inplace=True)\n\nprint('20 features with lowest score')\n\nprint(feature_data.head(20).sort_values(by='Score',ascending=False))\n\nprint('20 features with highest score')\nprint(feature_data.tail(20).sort_values(by='Score',ascending=False))","5ce6bd58":"#Tfidf\n\ndata1=[]\n\nTf_sum=X_train_Tfid.sum(axis=0)\n\nfor col,features in enumerate(Tfid.get_feature_names()):\n    data1.append([features,Tf_sum[0,col]])\n    \nfeature_data=pd.DataFrame(data1,columns=['Feature','Score'])\nfeature_data.sort_values(by='Score',inplace=True)\n\nprint('20 features with lowest score')\nprint('\\n')\n\nprint(feature_data.head(20).sort_values(by='Score',ascending=False))\nprint('\\n\\n')\nprint('20 features with highest score')\nprint('\\n')\nprint(feature_data.tail(20).sort_values(by='Score',ascending=False))","3325fe64":"sms_df['Message_length']=sms_df['Message'].apply(lambda x:len(x))\nsms_df.head()","83cc797a":"from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nexample_sentence='This is an example showing of stop word filteration.'\nwords=word_tokenize(example_sentence)\nprint('Before applying Stopwords:\\n\\n{}'.format(words))\nstop_words=set(stopwords.words('english'))\n\nw=[]\nfor i in words:\n    if i not in stop_words:\n        w.append(i)\nprint('\\n\\n')\nprint('After applying Stopwords:\\n\\n{}'.format(w))","4b5acd5b":"sms_df['Message_stop']=sms_df['Message'].apply(lambda x: ' '.join([w for w in x.split() if w not in stop_words ]))","3920b699":"sms_df['Message_length_stop']=sms_df['Message_stop'].apply(lambda x:len(x))\nsms_df.head()","3430d8bb":"sms_df.drop(['Message','Message_length'],axis=1,inplace=True)","c5021f30":"sms_df.head()","68d5d3a3":"plt.figure(figsize=(7,5))\nsns.countplot(x='Tag',data=sms_df)\nplt.title('Total Count of Spam and ham message\\n 1=Spam and 0=ham',size=15)","c0956e43":"sms_df['Number_count']=sms_df['Message_stop'].apply(lambda x:len(''.join([n for n in x if n.isdigit()])))","dae8fad9":"sms_df.head(6)","661eaf9f":"sms_df['Message_stop'][5] # 6th message contains exactly 4 digits","83edf4db":"sms_df[sms_df['Tag']==1].describe()","05723a4a":"sms_df[sms_df['Tag']==0].describe()","9199b5d1":"fig,ax=plt.subplots(1,2,figsize=(15,5))\n\nmsg_length_spam=sms_df.loc[sms_df['Tag']==1,'Message_length_stop']\nmsg_length_ham=sms_df.loc[sms_df['Tag']==0,'Message_length_stop']\n\nsns.distplot(msg_length_spam,ax=ax[0],color='r')\nax[0].set_title('Distribution of Message length of Spam',fontsize=14)\n\n\nsns.distplot(msg_length_ham,ax=ax[1],color='b')\nax[1].set_title('Distribution of Message length of ham',fontsize=14)\nplt.show()","a6355d53":"fig,ax=plt.subplots(1,2,figsize=(15,5))\n\nnumber_count_spam=sms_df.loc[sms_df['Tag']==1,'Number_count']\nnumber_count_ham=sms_df.loc[sms_df['Tag']==0,'Number_count']\n\nsns.distplot(number_count_spam,ax=ax[0],color='r')\nax[0].set_title('Distribution of numbers length of Spam',fontsize=14)\n\n\n\n# try:\n#     sns.distplot(number_count_ham,ax=ax[1],color='b')\n# except RuntimeError as re:\n#     if str(re).startswith(\"Selected KDE bandwidth is 0. Cannot estimate density.\"):\n#         sns.distplot(number_count_ham,ax=ax[1],color='b', kde_kws={'bw': 0.1})\n#         ax[1].set_title('Distribution of numbers length of ham',fontsize=14)\n#     else:\n#         raise re\n\nsns.distplot(number_count_ham,ax=ax[1],color='b', kde_kws={'bw': 0.1})\nax[1].set_title('Distribution of numbers length of ham',fontsize=14)\nplt.show()","1cf3231c":"X=sms_df['Message_stop']\ny=sms_df['Tag']\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0)","15e0a907":"# # CountVectorizer\ncount=CountVectorizer(min_df=5,ngram_range=[3,6],analyzer='char').fit(X_train)\nX_train_Count=count.transform(X_train)\nX_test_Count=count.transform(X_test)\n\n#TfidfVectorizer\n\nTfid=TfidfVectorizer(min_df=5,ngram_range=[3,6],analyzer='char').fit(X_train)\nX_train_Tfid=Tfid.transform(X_train)\nX_test_Tfid=Tfid.transform(X_test)","be9f7cc8":"clf=MultinomialNB(alpha=0.1)\nclf.fit(X_train_Count,y_train)\ntrain_pred=clf.predict(X_train_Count)\nprint('ROC score of Training by applying Countvectorizer:',roc_auc_score(train_pred,y_train))\npred_count=clf.predict(X_test_Count)\nprint('ROC score of Testing by applying Countvectorizer:',roc_auc_score(y_test,pred_count))","ae10f0ad":"# Predictions of MultinomialNB using CountVectorization\n\nx=['do you have plans for weekend?, let us meet at our usual place',\n  'Hii, you are our lucky customer, you have won 100000000 Rs, Please provide your account details we will transfer the amount',\n  'Your account is freezed please provide your account details to unfreeze the account',\n  'Hi, Pooja you have been selected for the First round of interview with Wipro, you need to visit our campus on Next Monday']\ndata=pd.Series(x)\ntrans=count.transform(data)\nclf.predict(trans)","92350d55":"clf=MultinomialNB(alpha=0.1)\nclf.fit(X_train_Tfid,y_train)\ntrain_pred=clf.predict(X_train_Tfid)\nprint('ROC score of Training by applying TFidVectorizer:',roc_auc_score(train_pred,y_train))\npred_tfid=clf.predict(X_test_Tfid)\nprint('ROC score of Testing by applying TFidVectorizer:',roc_auc_score(y_test,pred_tfid))","f1ea4dff":"# Predictions of MultinomialNB using TfidVectorization\n\nx=['do you have plans for weekend?, let us meet at our usual place',\n  'Hii, you are our lucky customer, you have won 100000000 Rs, Please provide your account details we will transfer the amount',\n  'Your account is freezed please provide your account details to unfreeze the account',\n  'Hi, Pooja you have been selected for the First round of interview with Wipro, you need to visit our campus on Next Monday for interview process']\ndata=pd.Series(x)\ntrans=Tfid.transform(data)\nclf.predict(trans)","62d8e969":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","1e747251":"print('confusion matrix ')\nconf_count=confusion_matrix(y_test,pred_count)\nprint('Confusion matrix for classifier using Countvectorizer:\\n\\n{}'.format(conf_count))\nconf_tfid=confusion_matrix(y_test,pred_tfid)\nprint('Confusion matrix for classifier using TFidVectorizer:\\n\\n{}'.format(conf_tfid))","2cecbc51":"print('classification report')\nconf_report=classification_report(y_test,pred_count)\nprint('Classification report for classifier using Countvectorizer:\\n\\n{}'.format(conf_report))\nconf_report=classification_report(y_test,pred_tfid)\nprint('Classification report for classifier using TFidVectorizer:\\n\\n{}'.format(conf_report))","be49dd1d":"from sklearn.linear_model import LogisticRegression\n\n#Countvectorizer\nlog_clf=LogisticRegression(C=100)\nlog_clf.fit(X_train_Count,y_train)\ntrain_pred=log_clf.predict(X_train_Count)\nprint(roc_auc_score(train_pred,y_train))\nlog_pre=log_clf.predict(X_test_Count)\nprint(roc_auc_score(y_test,log_pre))","2b7030e4":"#Tfid\n\nlog_clf=LogisticRegression(C=100)\nlog_clf.fit(X_train_Tfid,y_train)\ntrain_pred=log_clf.predict(X_train_Tfid)\nprint(roc_auc_score(train_pred,y_train))\nlog_pre=log_clf.predict(X_test_Tfid)\nprint(roc_auc_score(y_test,log_pre))","faa56101":"from sklearn.tree import DecisionTreeClassifier\n\n#Count\ntree_clf=DecisionTreeClassifier(max_depth=3)\ntree_clf.fit(X_train_Count,y_train)\ntrain_pred=tree_clf.predict(X_train_Count)\nprint(roc_auc_score(train_pred,y_train))\ntest_pred=tree_clf.predict(X_test_Count)\nprint(roc_auc_score(test_pred,y_test))","c0881688":"#Tfid\ntree_clf=DecisionTreeClassifier(max_depth=2)\ntree_clf.fit(X_train_Tfid,y_train)\ntrain_pred=tree_clf.predict(X_train_Tfid)\nprint(roc_auc_score(train_pred,y_train))\ntest_pred=tree_clf.predict(X_test_Count)\nprint(roc_auc_score(test_pred,y_test))","3562cf4b":"Making use of stopwords to the Message column to delete all the stopwords,and also checking the length of message length before and after applying **stopwords**","b5f23371":"The datafraem has 5572 Rows and 5 columns, we can check the total Null values in dataframe which is shown below","8c87b745":"Count= Total Count of Tag and Message is 5572.\n\nUnique=In Tag Column there are 2 unique characters and in Message 5169 meesages are unique\n\nTop= Tag Column as ham category as the majority class and in Message column **Sorry, I'll call later** is the Top message\n\nFreq= Ham has occured 4825 times and **Sorry, I'll call later** has occured 30 times ","c9f3c9da":"# Evaluate model again with new features and check the accuracy score","bf5758aa":"What percentage of Data are Spam?\n","8ae33ebc":"Most of unnamed columns have huge value of NaN it's best we drop all the unnamed columns","1dc8a31b":"spam ='Message which is labelled as Spam'\n\nham = 'Message which is labelled as Not Spam'","30894b71":"min_df[x]: The minimum number of times the term variable should appear in the document i.e the term variable wil be deleted which appears less than x times\n\nngram_range[min,max]: the grouping is done from min upto max times","5af3e473":"# Feature Engineering","f36ee4a4":"#                 Thank you.","d83ec235":"Since Tag column is Categorical we can convert it into numerical where:\n\nspam=1\n\nham=0","7bfa17f9":"we can see that there are high Count of Numbers in the Spam message than Non spam message. Which means that the spam message are also related to money where it says as winning a lottery or some value of amount will be credited etc.","f247d6db":"Length of messages for Spam is higher than the Ham message which means that the Spam Message has higher number of characters.\n\nSimilarly we can also plot for number_count to check whether the Spam message has high number count.","f2267d6c":"Deleting Message and Message_length column","a8d7b0e8":"With the message provided the Predicition of MultoMultinomialNB are Correctly predicted for both Count and Tfid Vectorizations\n\nLet's check with the other Classifiers","48be31dd":"###### Importing Libraries","aa469f8d":"##### Reading text based Dataset into pandas DataFrame","ef2241aa":"Except Column v1 and v2 all columns have large NaN values","35a161dc":"By checking with the DummyClassifier we can see that the Classifier is performing Good with Provided dataset no modification needs to de done to Dataset. Let's continue Feature Engineering with the same Dataset.","c08c7c51":"We can get the largest and smallest countvectorizer and Tfid features,  ","2f441102":"Before modifing the Message column let's apply MultinomialNB classifier after Counter and TFid Vectorization  and check Accuracy score.\n\nDuring Text processing,cleaning the text[preprocessing] is necessary, the cleaned text have to be converted into numerical format where each word is represented by a matrix. Which is also known as word embedding.\n\nTFIDF[Term frequency Inverse document frequency]  allows us to weight terms based on how important they are to a document.","46a43785":"Dataset is imbalanced As the number of ham count is higher than spam count, in such case when the dataset is applied to Machine Learning the Algorithm becomes highly biased\n\nWe have to consider the label which has less count here[ham] and extract the other label[spam] with the exact count so that the Data becomes Balanced.\n\nImbalanced dataset can be checked whether it is balanced or not with the help of DummyClassifier we can check with the dataset later","39ba37f8":"We can apply Confusion matrix and check the Performance of a Classifier","922c4e48":"We can also check the total number of digits in Message_stop column and add it as an extra feature"}}