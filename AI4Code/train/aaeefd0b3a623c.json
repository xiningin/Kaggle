{"cell_type":{"75f6fec4":"code","fb191b32":"code","85038bf1":"code","cf706679":"code","251c523e":"code","ec97e958":"code","56232682":"code","36fbd041":"code","71776c9c":"code","9a7faa06":"code","775b2f53":"code","26b26522":"code","6ed15907":"code","9cf6e5f6":"code","b05a4690":"code","36630993":"code","4569d978":"code","0c136389":"code","cb125e96":"code","eff87caa":"code","37ad9de9":"code","6e10f009":"code","67a4a987":"code","3818b738":"code","e8ed5e86":"code","b5f2629a":"code","197d4d53":"code","7eea3954":"code","d8621e4d":"code","db4671ae":"code","771118ca":"code","792cc5f9":"code","19c4ed3c":"code","a30ba378":"code","270385be":"code","599fc2b5":"code","c170dfff":"code","24d75f7b":"code","1d2dbffa":"code","78fe1ab8":"code","e88d568f":"code","863717a5":"code","6a5d3b7e":"code","7740c235":"code","65601fdb":"code","88ffa8a1":"code","49d2805d":"code","2191936d":"code","ddd10ef7":"code","b32d4d9b":"code","35512bae":"code","272e9e84":"code","fbdedb26":"code","c4cef277":"code","8ab60993":"code","2d451796":"code","cba205d7":"code","c754cd80":"code","dabb4bdb":"code","87c7591c":"code","d201086f":"code","162fa47c":"code","4738b6be":"code","cedd59f6":"code","cda55cee":"code","7caa5075":"code","355c67eb":"code","cd13a8ac":"code","10b75153":"code","872890e5":"code","6ede7eb6":"code","3ce0a7a3":"code","710cfba7":"code","65382aea":"code","cac280f6":"code","9e43bcac":"code","8484e132":"code","22478181":"code","3e4704a3":"code","d86c7f75":"code","49c9d4fb":"code","ae43dd63":"code","77ace3eb":"code","f9f8f117":"code","a965190f":"code","50379525":"code","832b240a":"code","4629ae13":"code","2e8a553d":"code","603da80d":"code","6a32073f":"code","7fb82152":"code","bc11584e":"code","0bfa89af":"code","49496af3":"code","7c5c3232":"code","801aa886":"code","a3f68d43":"code","2cda7181":"code","d080e46a":"code","25361947":"code","fc9674f4":"code","32c77e02":"markdown","fe2871c3":"markdown","b1f6fee7":"markdown","075bfc1d":"markdown","f456ad59":"markdown","5637a29b":"markdown","a0059e2e":"markdown","c0fa15b1":"markdown","bafeef0e":"markdown","281dbf89":"markdown","ebe1782d":"markdown","2f0d96e8":"markdown","4d8a4292":"markdown","d1b7101c":"markdown","3f2fc775":"markdown","ad31aa33":"markdown","8066ea96":"markdown","a9d73c1b":"markdown","bc003cfc":"markdown","972ae888":"markdown","96d6cb56":"markdown","2e7c4506":"markdown","bd4989a1":"markdown","521b3973":"markdown","791041f8":"markdown","6a7c0e5c":"markdown","a9525aa2":"markdown","f115baf6":"markdown","ae3fdf07":"markdown","76add851":"markdown","acf56996":"markdown","8dd3f0e6":"markdown","a5fdeea3":"markdown"},"source":{"75f6fec4":"import pandas as pd\nimport numpy as np","fb191b32":"#read in the movies dataset\ndf= pd.read_csv('..\/input\/movielens\/ratings.csv',header=0,sep=',', usecols = [0,1,2],encoding = \"ISO-8859-1\")\n#count number of rowsdf_pivot = df_small.pivot(index='Cust_Id',values='Rating',columns='Movie_Id')\nlen(df)","85038bf1":"df.head()","cf706679":"#count how many reviews there are with each rating\ncount_ratings = df.groupby('rating',as_index=False).count()\ncount_ratings['perc_total']=round(count_ratings['userId']*100\/count_ratings['userId'].sum(),1)\ncount_ratings","251c523e":"import seaborn as sns\n#plot distribution of ratings\ncolors = sns.color_palette(\"GnBu_d\")\nax = sns.barplot(x='rating', y='perc_total', data=count_ratings,palette=colors)\nax.set_title(label='% of reviews by Rating', fontsize=20)\nax","ec97e958":"#dictionarity of movies that I have chosen and rated\nmy_movies_dict = {'userId': [999999,999999,999999,999999,999999,999999,999999,999999,999999,999999,999999,999999,999999,999999,999999], \n                          'movieId': [318,104241,130073,111362,108945,108190,105884,69112,79132,103335,136020,136562,109374,112556,160438],\n                          'rating': [4,4,4,4,2,3.5,3.5,3,4,4,5,4,5,3,3]}\n#save as data frame\nmy_movies = pd.DataFrame.from_dict(my_movies_dict)\nmy_movies","56232682":"#attach to existing ratings data frame\ndf = df.append(my_movies)\n#check that new rows have been appended to the data set\ndf.loc[df['userId']==999999].head()","36fbd041":"#count number of unique users in the ratings data set\nn_users = df.userId.unique().shape[0]\n#count number of unitque movies in the ratings data set\nn_items = df.movieId.unique().shape[0]\nprint(len(df))\nprint(n_users) \nprint(n_items)","71776c9c":"#calculate sparsity as number of entries \/ total number of possible entries\nsparsity = round(1.0 - len(df)\/float(n_users*n_items),2)\n\nprint(\"The sparsity of the ratings data set is \" + str(sparsity*100) + \"%\")","9a7faa06":"#read in the movies dataset\nmovies= pd.read_csv('..\/input\/movielens\/movies.csv',header=0,sep=',',encoding = \"ISO-8859-1\")\n#view movies\nprint(movies.head())\nn_movies = len(movies)\nprint(n_movies)","775b2f53":"#Search for a particular movie\nmovies.loc[movies['title'].str.contains(\"Nym\")]","26b26522":"#read in the links\nlinks= pd.read_csv('..\/input\/movielens\/links.csv',header=0,sep=',',encoding = \"ISO-8859-1\")\n#view links\nlinks.head()","6ed15907":"#read in the tags\ntags= pd.read_csv('..\/input\/movielens\/tags.csv',header=0,sep=',',encoding = \"ISO-8859-1\")\n#view tags\nprint(tags.head())\nprint(len(tags))\nn_movies_with_tags = tags.movieId.unique().shape[0]\nprint(n_movies_with_tags)\nprint(\"There are \" + str(round(100*n_movies_with_tags\/n_movies,1)) + \" percent of movies with at least one tag\")","9cf6e5f6":"#create a string and append all genre values in the data frame\ngenre_list = \"\"\nfor index,row in movies.iterrows():\n        genre_list += row.genres + \"|\"\n#split the string into a list of values\ngenre_list_split = genre_list.split('|')\n#de-duplicate values\nnew_list = list(set(genre_list_split))\n#remove the value that is blank\nnew_list.remove('')\n#inspect list of genres\nnew_list","b05a4690":"movie_genres = movies.copy()\n#for each genre in the list\nfor genre in new_list:\n    #create a new column for the genre abd search if the genre is in the list of genres\n    movie_genres[genre] = movie_genres.apply(lambda _: int(genre in _.genres), axis=1)","36630993":"#inspect the final data frame\nmovie_genres.head()","4569d978":"#get the average rating per movie \navg_movie_rating = pd.DataFrame(df.groupby('movieId')['rating'].agg(['mean','count']))\n#create new movieId column from Index\navg_movie_rating['movieId']= avg_movie_rating.index\navg_movie_rating.head()","0c136389":"#get the number of movies by count\nmovies_with_rating = pd.DataFrame(avg_movie_rating.groupby('count')['movieId'].agg(['count']))\n#calculate the percentage of total movies that have a specific number of ratings\nmovies_with_rating['perc_total']=round(movies_with_rating['count']*100\/movies_with_rating['count'].sum(),1)\nmovies_with_rating.head()","cb125e96":"import seaborn as sns\nsns.distplot(avg_movie_rating['count'])\n#np.percentile(movie_ratings['count'],50)\n#len(movie_ratings)\nlen(avg_movie_rating.loc[avg_movie_rating['count']>=30])","eff87caa":"#calculate the percentile count\nnp.percentile(avg_movie_rating['count'],70)","37ad9de9":"#Get the average movie rating across all movies \navg_rating_all=df['rating'].mean()\navg_rating_all\n#set a minimum threshold for number of reviews that the movie has to have\nmin_reviews=30\nmin_reviews\nmovie_score = avg_movie_rating.loc[avg_movie_rating['count']>min_reviews]\nmovie_score.head()","6e10f009":"#create a function for weighted rating score based off count of reviews\ndef weighted_rating(x, m=min_reviews, C=avg_rating_all):\n    v = x['count']\n    R = x['mean']\n    # Calculation based on the IMDB formula\n    return (v\/(v+m) * R) + (m\/(m+v) * C)","67a4a987":"movie_score['weighted_score'] = movie_score.apply(weighted_rating, axis=1)\nmovie_score.head()","3818b738":"#join movie details to movie ratings\nmovie_score = pd.merge(movie_score,movie_genres,on='movieId')\n#join movie links to movie ratings\n#movie_score = pd.merge(movie_score,links,on='movieId')\nmovie_score.head()","e8ed5e86":"#list top scored movies over the whole range of movies\npd.DataFrame(movie_score.sort_values(['weighted_score'],ascending=False)[['title','count','mean','weighted_score']][:10])","b5f2629a":"def best_movies_by_genre(genre,top_n):\n    #return  print(\"The top \" + str(top_n) +\" \" + genre + \"movies are:\")\n    return pd.DataFrame(movie_score.loc[(movie_score[genre]==1)].sort_values(['weighted_score'],ascending=False)[['title','count','mean','weighted_score']][:top_n])","197d4d53":"#run function to return top recommended movies by genre\nbest_movies_by_genre('Action',10)  ","7eea3954":"#join the movie names to the movies data set\nratings_movies = pd.merge(df,movies,on='movieId')\n#print the new data set\nratings_movies.head()","d8621e4d":"def get_other_movies(movie_name):\n    #get all users who watched a specific movie\n    df_movie_users_series = ratings_movies.loc[ratings_movies['title']==movie_name]['userId']\n    #convert to a data frame\n    df_movie_users = pd.DataFrame(df_movie_users_series,columns=['userId'])\n    #get a list of all other movies watched by these users\n    other_movies = pd.merge(df_movie_users,ratings_movies,on='userId')\n    #get a list of the most commonly watched movies by these other user\n    other_users_watched = pd.DataFrame(other_movies.groupby('title')['userId'].count()).sort_values('userId',ascending=False)\n    other_users_watched['perc_who_watched'] = round(other_users_watched['userId']*100\/other_users_watched['userId'][0],1)\n    return other_users_watched[:10]","db4671ae":"get_other_movies('Gone Girl (2014)')","771118ca":"from sklearn.neighbors import NearestNeighbors","792cc5f9":"#only include movies with more than 10 ratings\nmovie_plus_10_ratings = avg_movie_rating.loc[avg_movie_rating['count']>=30]\nprint(len(df))","19c4ed3c":"filtered_ratings = pd.merge(movie_plus_10_ratings, df, on=\"movieId\")\nlen(filtered_ratings)","a30ba378":"#create a matrix table with movieIds on the rows and userIds in the columns.\n#replace NAN values with 0\nmovie_wide = filtered_ratings.pivot(index = 'movieId', columns = 'userId', values = 'rating').fillna(0)\nmovie_wide.head()","270385be":"#specify model parameters\nmodel_knn = NearestNeighbors(metric='cosine',algorithm='brute')\n#fit model to the data set\nmodel_knn.fit(movie_wide)","599fc2b5":"#select a random movie\n#query_index = np.random.choice(movie_wide.index)\nquery_index=96079\n#96079 for skyfall\n#get the list of user ratings for a specific userId\nquery_index_movie_ratings = movie_wide.loc[query_index,:].values.reshape(1,-1)\n#get the closest 6 movies and their distances from the movie specified\ndistances,indices = model_knn.kneighbors(query_index_movie_ratings,n_neighbors = 11)","c170dfff":"#movies.head(304)\nindices","24d75f7b":"#write a lopp that prints the similar movies for a specified movie.\nfor i in range(0,len(distances.flatten())):\n    #get the title of the random movie that was chosen\n    get_movie = movies.loc[movies['movieId']==query_index]['title']\n    #for the first movie in the list i.e closest print the title\n    if i==0:\n        print('Recommendations for {0}:\\n'.format(get_movie))\n    else: \n        #get the indiciees for the closest movies\n        indices_flat = indices.flatten()[i]\n        #get the title of the movie\n        get_movie = movies.loc[movies['movieId']==movie_wide.iloc[indices_flat,:].name]['title']\n        #print the movie\n        print('{0}: {1}, with distance of {2}:'.format(i,get_movie,distances.flatten()[i]))","1d2dbffa":"movie_genres.head()","78fe1ab8":"content_df = movie_genres.copy()\ncontent_df.set_index('movieId')\ncontent_df_drop = content_df.drop(columns=['movieId','title','genres'])\ncontent_df_drop = content_df_drop.as_matrix()\ncontent_df_drop","e88d568f":"# Import linear_kernel\nfrom sklearn.metrics.pairwise import linear_kernel\n\n# Compute the cosine similarity matrix\ncosine_sim = linear_kernel(content_df_drop,content_df_drop)","863717a5":"cosine_sim","6a5d3b7e":"#create a series of the movie id and title\nindicies = pd.Series(content_df.index, content_df['title'])\nindicies","7740c235":"idx = indicies[\"Skyfall (2012)\"]#\nsim_scores = list(enumerate(cosine_sim[idx]))\n# Sort the movies based on the similarity scores\nsim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n# Get the scores of the 10 most similar movies\nsim_scores = sim_scores[1:11]\n# Get the movie indices\nmovie_indices = [i[0] for i in sim_scores]\npd.DataFrame(content_df[['title','genres']].iloc[movie_indices])","65601fdb":"#get ordered list of movieIds\nitem_indices = pd.DataFrame(sorted(list(set(df['movieId']))),columns=['movieId'])\n#add in data frame index value to data frame\nitem_indices['movie_index']=item_indices.index\n#inspect data frame\nitem_indices.tail()","88ffa8a1":"#get ordered list of movieIds\nuser_indices = pd.DataFrame(sorted(list(set(df['userId']))),columns=['userId'])\n#add in data frame index value to data frame\nuser_indices['user_index']=user_indices.index\n#inspect data frame\nuser_indices.tail()","49d2805d":"#join the movie indices\ndf_with_index = pd.merge(df,item_indices,on='movieId')\n#join the user indices\ndf_with_index=pd.merge(df_with_index,user_indices,on='userId')\n#inspec the data frame\ndf_with_index.head()","2191936d":"#import train_test_split module\nfrom sklearn.model_selection import train_test_split\n#take 80% as the training set and 20% as the test set\ndf_train, df_test= train_test_split(df_with_index,test_size=0.2)\nprint(len(df_train))\nprint(len(df_test))","ddd10ef7":"def table_to_matrix(df):\n    #create an array with zeros\n    data_matrix = np.zeros((n_users, n_items))\n    #for every line in the data\n    for line in df.itertuples():\n        #line[2] is rating line[4] is movie_index and line[5] is user_index\n        data_matrix[line[5], line[4]] = line[2]\n    return data_matrix","b32d4d9b":"train_data_matrix = table_to_matrix(df_train)\ntest_data_matrix = table_to_matrix(df_test)","35512bae":"pd.DataFrame(train_data_matrix).head()","272e9e84":"from math import sqrt\ndf_test_benchmark = df_test.copy()\navg_rating_benchmark=df_train['rating'].mean()\ndf_test_benchmark['prediction']=avg_rating_benchmark\ndf_test_benchmark['diff'] = df_test_benchmark['prediction'] - df_test_benchmark['rating']\ndf_test_benchmark['diff_squared']=df_test_benchmark['diff']**2\nsqrt(df_test_benchmark['diff_squared'].sum()\/len(df_test_benchmark))\nprint(avg_rating_benchmark)","fbdedb26":"df_test_benchmark.head()","c4cef277":"#create a table with movieIds on the rows and userIds in the columns.\n#replace NAN values with 0\ndf_pivot = df_train.pivot(index = 'userId', columns = 'movieId', values = 'rating').fillna(0)","8ab60993":"import sklearn\nfrom sklearn.metrics.pairwise import pairwise_distances","2d451796":"#comput user similarities and item similarities with cosine\nuser_similarity = pairwise_distances(train_data_matrix, metric='cosine')\nitem_similarity = pairwise_distances(train_data_matrix.T, metric='cosine')","cba205d7":"#len(user_similarity[1])\nlen(item_similarity[1])\n#pd.DataFrame(item_similarity).head(10)\n#pd.DataFrame(user_similarity).head(10)","c754cd80":"#function to predict rating\ndef predict(ratings, similarity, type='user'):\n    if type == 'user':\n        mean_user_rating = ratings.mean(axis=1)\n        #We use np.newaxis so that mean_user_rating has same format as ratings\n        ratings_diff = (ratings - mean_user_rating[:, np.newaxis])\n        #prediction is equal to the mean rating + similarty . ratings difference \/ sum of absolute similarity\n        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) \/ np.array([np.abs(similarity).sum(axis=1)]).T\n    elif type == 'item':\n        #prediction is equal to the ratings x similarity \/ sum of the similarity\n        pred = ratings.dot(similarity) \/ np.array([np.abs(similarity).sum(axis=1)])\n    return pred","dabb4bdb":"user_prediction = predict(train_data_matrix,user_similarity,type='user')\nitem_prediction = predict(train_data_matrix,item_similarity,type='item')","87c7591c":"user_prediction.max()\nuser_prediction.min()","d201086f":"#convert the prediction matrices into data frames\nuser_pred_df = pd.DataFrame(user_prediction)\nitem_pred_df = pd.DataFrame(item_prediction)","162fa47c":"#inspect the predictions\nuser_pred_df.head(20)\n#item_pred_df.head()","4738b6be":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\ndef rmse(prediction, ground_truth):\n    #select prediction values that are non-zero and flatten into 1 array\n    prediction = prediction[ground_truth.nonzero()].flatten() \n    #select test values that are non-zero and flatten into 1 array\n    ground_truth = ground_truth[ground_truth.nonzero()].flatten()\n    #return RMSE between values\n    return sqrt(mean_squared_error(prediction, ground_truth))","cedd59f6":"print(\"User-based CF RMSE: \" + str(rmse(user_prediction, test_data_matrix)))\nprint(\"Item-based CF RMSE: \" + str(rmse(item_prediction, test_data_matrix)))","cda55cee":"#create validation set from training set as 10% of data\ndf_train_val, df_val= train_test_split(df_train,test_size=0.1)\nprint(len(df_train_val))\nprint(len(df_val))","7caa5075":"#create new test matrix and validation matrix\ntrain_data_matrix_2=table_to_matrix(df_train_val)\nvalidation_data_matrix=table_to_matrix(df_val)","355c67eb":"#calculate mean user rating\nmean_user_rating = train_data_matrix_2.mean(axis=1)[:, np.newaxis]\nratings_diff = (train_data_matrix_2 - mean_user_rating)","cd13a8ac":"import scipy.sparse as sp\nfrom scipy.sparse.linalg import svds","10b75153":"#Calculate the rmse sscore of SVD using different values of k (latent features)\nk_factors_list = [1,2,5,20,30,40,60,100,200]\nrmse_train_list = []\nrmse_val_list=[]\nfor i in k_factors_list:\n    #apply svd to the test data\n    #apply to original matrix or demeaned matrix\n    #u,s,vt = svds(train_data_matrix_2,k=i)\n    u,s,vt = svds(ratings_diff,k=i)\n    #get diagonal matrix\n    s_diag_matrix=np.diag(s)\n    #predict x with dot product of u s_diag and vt\n    #calculate dot product or dot product + mean user rating\n    #X_pred = np.dot(np.dot(u,s_diag_matrix),vt)\n    X_pred = np.dot(np.dot(u,s_diag_matrix),vt) + mean_user_rating\n    #calculate rmse score of matrix factorisation predictions\n    rmse_train_score = rmse(X_pred,train_data_matrix_2)\n    rmse_val_score = rmse(X_pred,validation_data_matrix)\n    rmse_train_list.append(rmse_train_score)\n    rmse_val_list.append(rmse_val_score)\n    print(\"Matrix Factorisation with \" + str(i) +\" latent features has a training RMSE of \" + str(round(rmse_train_score,3)) + \" and a validation RMSE of \" + str(round(rmse_val_score,3)))","872890e5":"#save list as something else (so new list can be created)\nrmse_train_demeaned = rmse_train_list\nrmse_val_demeaned=rmse_val_list","6ede7eb6":"#this was used for calculating the difference in error between using the original data set vs the demeaned data set\n#errors = pd.DataFrame.from_dict({'k_factors':k_factors_list,'rmse_original':rmse_val_original,'rmse_demeaned':rmse_val_demeaned})\n#errors['rmse_diff']=errors['rmse_original']- errors['rmse_demeaned']\n#errors","3ce0a7a3":"#plot the original vs demeaned data set or the validation vs training errors\nimport matplotlib.pyplot as plt\nplt.plot(k_factors_list,rmse_train_demeaned)\nplt.plot(k_factors_list,rmse_val_demeaned)\nplt.xlabel(\"number of latent factors\")\nplt.ylabel(\"rmse score\")\nplt.ylim(2.0, 4.0)\nplt.title(\"Matrix Factorisation - Validation & Validation RMSE Training vs Latent Factors\")","710cfba7":"mf_rmse_list =[]\n# data sample\ndata = df_with_index\ni=0\n# prepare cross validation\nkfold = KFold(10, True, 1)\n# enumerate splits\nfor train, test in kfold.split(data):\n    i=i+1\n    #save CV test and trining sets\n    cv_train = data.iloc[train]\n    cv_test = data.iloc[test]\n    #convert to matrices\n    cv_train_matrix = table_to_matrix(cv_train)\n    cv_test_matrix = table_to_matrix(cv_test)\n    #Calculate mean user rating\n    mean_user_rating = cv_train_matrix.mean(axis=1)[:, np.newaxis]\n    ratings_diff = (cv_train_matrix - mean_user_rating)\n    #apply svd to the test data\n    u,s,vt = svds(ratings_diff,k=30)\n    #get diagonal matrix\n    s_diag_matrix=np.diag(s)\n    #predict x with dot product of u s_diag and vt\n    X_pred = np.dot(np.dot(u,s_diag_matrix),vt) + mean_user_rating\n    #calculate rmse score of matrix factorisation predictions\n    rmse_test_score = rmse(X_pred,cv_test_matrix)\n    mf_rmse_list.append(rmse_test_score)\n    print(\"Final Matrix Factorisation Model for Fold + \" + str(i) + \" has an RMSE of \" + str(round(rmse_test_score,3)))","65382aea":"#calculate standard deviation of final RMSE score\nimport statistics as stats\nstats.stdev(mf_rmse_list)","cac280f6":"#plot distribution of test RMSE scores from Cross Validation\nsns.distplot(mf_rmse_list)","9e43bcac":"#Final Matrix Factorisation Model\n#apply svd to the test data\nu,s,vt = svds(ratings_diff,k=30)\n#get diagonal matrix\ns_diag_matrix=np.diag(s)\n#predict x with dot product of u s_diag and vt\nX_pred = np.dot(np.dot(u,s_diag_matrix),vt) + mean_user_rating\n#calculate rmse score of matrix factorisation predictions\nrmse_test_score = rmse(X_pred,test_data_matrix)\nprint(\"Final Matrix Factorisation Model with 30 latent features has a training RMSE of \" + str(round(rmse_test_score,3)))","8484e132":"#select predictions from test set\nprediction = X_pred[test_data_matrix.nonzero()].flatten() \n#select actual values form test set\nground_truth = test_data_matrix[test_data_matrix.nonzero()].flatten()","22478181":"#plot predictions vs test data\nplt.scatter(prediction,ground_truth,alpha=0.5)","3e4704a3":"#Convert predictions to a DataFrame\nmf_pred = pd.DataFrame(X_pred)\nmf_pred.head()","d86c7f75":"df_names = pd.merge(df,movies,on='movieId')\ndf_names.head()","49c9d4fb":"#choose a user ID\nuser_id = 999999\n#get movies rated by this user id\nusers_movies = df_names.loc[df_names[\"userId\"]==user_id]\n#print how many ratings user has made \nprint(\"User ID : \" + str(user_id) + \" has already rated \" + str(len(users_movies)) + \" movies\")\n#list movies that have been rated\nusers_movies","ae43dd63":"#get user in\nuser_index = df_train.loc[df_train[\"userId\"]==user_id]['user_index'][:1].values[0]\n#get movie ratings predicted for this user and sort by highest rating prediction\nsorted_user_predictions = pd.DataFrame(mf_pred.iloc[user_index].sort_values(ascending=False))\n#rename the columns\nsorted_user_predictions.columns=['ratings']\n#save the index values as movie id\nsorted_user_predictions['movieId']=sorted_user_predictions.index\nprint(\"Top 10 predictions for User \" + str(user_id))\n#display the top 10 predictions for this user\npd.merge(sorted_user_predictions,movies, on = 'movieId')[:10]","77ace3eb":"#take a % of trainign data (but for final model train on all)\ndf_train_sample = df_train.sample(frac=0.1)\n#count number of unique users\nnumUsers = df_train_sample.userId.unique().shape[0]\n#count number of unitque movies\nnumMovies = df_train_sample.movieId.unique().shape[0]\nprint(len(df_train_sample))\nprint(numUsers) \nprint(numMovies)","f9f8f117":"#Separate out the values of the df_train data set into separate variables\nUsers = df_train_sample['userId'].values\nMovies = df_train_sample['movieId'].values\nRatings = df_train_sample['rating'].values\nprint(Users),print(len(Users))\nprint(Movies),print(len(Movies))\nprint(Ratings),print(len(Ratings))","a965190f":"#import libraries\nimport keras\nfrom keras.layers import Embedding, Reshape, Merge\nfrom keras.models import Sequential\nfrom keras.optimizers import Adamax\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot","50379525":"n_latent_factors=40","832b240a":"#movie input is an array\nmovie_input = keras.layers.Input(shape=[1],name='Item')\n#create movie embeddings to transform movies to a number of latent factors\nmovie_embedding = keras.layers.Embedding(numMovies + 1, n_latent_factors, name='Movie-Embedding')(movie_input)\nmovie_vec = keras.layers.Flatten(name='FlattenMovies')(movie_embedding)\n#dropout layer to prevent overfitting\nmovie_vec = keras.layers.Dropout(0.5)(movie_vec)\n\n#user input is an array\nuser_input = keras.layers.Input(shape=[1],name='User')\nuser_vec = keras.layers.Flatten(name='FlattenUsers')(keras.layers.Embedding(numUsers + 1, n_latent_factors,name='User-Embedding')(user_input))\nuser_vec = keras.layers.Dropout(0.5)(user_vec)\n\n#prod = keras.layers.merge([movie_vec, user_vec], mode='dot',name='DotProduct')\nprod = keras.layers.dot([movie_vec, user_vec],axes=1,name='DotProduct')\n\nmodel = keras.Model([user_input, movie_input], prod)\n\n\nmodel.compile(optimizer='adam', loss='mse')","4629ae13":"model.summary()","2e8a553d":"#print model architecture\n#SVG(model_to_dot(model,  show_shapes=True, show_layer_names=True, rankdir='HB').create(prog='dot', format='svg'))","603da80d":"callbacks = [EarlyStopping('val_loss', patience=2),ModelCheckpoint('weights.h5', save_best_only=True)]","6a32073f":"history_model_one = model.fit([Users, Movies], Ratings,validation_split=0.1, epochs=30, verbose=2,batch_size=64,callbacks=callbacks)","7fb82152":"import matplotlib.pyplot as plt\npd.Series(history_model_one.history['val_loss']).plot(logy=True)\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Train Error\")","bc11584e":"import math\n# Show the best validation RMSE\nmin_val_loss, idx = min((val, idx) for (idx, val) in enumerate(history_model_one.history['val_loss']))\nprint('Minimum RMSE at epoch ' + str(idx+1) + ' = ' + str(round(math.sqrt(min_val_loss),2)))\n## Output","0bfa89af":"#os.listdir('..\/input')","49496af3":"# Load weights'..\/input\/ratings.csv\n#model.load_weights('..\/input\/final-project\/weights.h5')","7c5c3232":"# Function to predict the ratings given User ID and Movie ID\ndef predict_rating(user_id, movie_id):\n    return model.predict([np.array([user_id]), np.array([movie_id])])[0][0]","801aa886":"#predict the rating for user and movie\npredict_rating(500,500)","a3f68d43":"#apply prediction across whole test set\ndf_nn_test = df_test.copy()\ndf_nn_test['prediction'] =df_nn_test.apply(lambda x: predict_rating(x['userId'], x['movieId']), axis=1)","2cda7181":"plt.scatter(df_nn_test['rating'],df_nn_test['prediction'],alpha = 0.5)\nplt.xlabel(\"test ratings\")\nplt.ylabel(\"prediction\")","d080e46a":"#calculate rmse\nfrom math import sqrt\ndf_nn_test['diff'] = df_nn_test['prediction'] - df_nn_test['rating']\ndf_nn_test['diff_squared']=df_nn_test['diff']**2\nrmse_nn = sqrt(df_nn_test['diff_squared'].sum()\/len(df_nn_test))\nprint(rmse_nn)","25361947":"#choose a user ID\nTEST_USER = 500\n#get movies rated by this user id\nusers_movies = df_names.loc[df_names[\"userId\"]==TEST_USER]\n#print how many ratings user has made \nprint(\"User ID : \" + str(TEST_USER) + \" has already rated \" + str(len(users_movies)) + \" movies\")\n#list movies that have been rated\nusers_movies.head(10)","fc9674f4":"#get movies that were rated by the user\nuser_ratings = df[df['userId'] == TEST_USER][['userId', 'movieId', 'rating']]\n#get list of movie IDs not rated by the user\nrecommendations = df[df['movieId'].isin(user_ratings['movieId']) == False][['movieId']].drop_duplicates()\n#get a list of recommendations\nrecommendations['prediction'] = recommendations.apply(lambda x: predict_rating(500, x['movieId']), axis=1)\nrecommendations.sort_values(by='prediction',\n                          ascending=False).merge(movies,on='movieId',).head(30)","32c77e02":"## Add in my own recommendations\nHere I create a list of my own movie ratings so that I can use these later to see what my recommender predicts for me","fe2871c3":"## Explore data set\nHere I group the data set by ratings value and count how many ratings were given each value as well as the % that have each value. ","b1f6fee7":"## Benchmark Model\nThis benchmark model on the test set is going to predict all of the ratings as the mean rating for all movies. After this I will calulate the RMSE of this over simplistic prediction.","075bfc1d":"### Final Model Cross Validation Set (To test Robustnesss)","f456ad59":"Matrix Factorisation from scratch : http:\/\/www.quuxlabs.com\/blog\/2010\/09\/matrix-factorization-a-simple-tutorial-and-implementation-in-python\/","5637a29b":"Here I load in the movies data set, links and tags. I also write some code to search for a particular movie by its title (case sensitive).","a0059e2e":"### Final model with best parameters (without cross validation)","c0fa15b1":"## Importing Ratings Data Set\n\nHere I import the main ratings data set, print the number of rows and inspect the top few rows.","bafeef0e":"## Deep Learning - Predict Score for Test Set\nHere I loop through each row in the test set and use the model to predict the rating. From this I calcualate the RMSE score.","281dbf89":"## Enhancing the Movies Data Set","ebe1782d":"### Plot and Table Errors","2f0d96e8":"## Prediction - Matrix Factorisation\nA combination of matrix factorization and Restricted Boltxman Machines were being used in the Netflix recommender system in 2014. Here we use Singular Value Decomposition (SVD) to represent the original matrix as 3 separate matrices describing latent factors in the matrix. To make predictions the dot product of these 3 matrices is then taken. ","4d8a4292":"## Deep Learning - Predict Movies for User\nHere I make predictions using ratings predicted fromt he deep learning model on my user ID","d1b7101c":"### Create validation Set","3f2fc775":"## Prediction - Deep Learning Model\nThis uses a neural network in Keras to predict the movie ratings\nhttps:\/\/nipunbatra.github.io\/blog\/2017\/recommend-keras.html","ad31aa33":"## Prediction - Item and User Collaborative Filtering \n\nThis approach uses user-user collaborative filtering and item-user collaboartive filtering to predict user movie ratings. From this predictions can then be made by ordering movies by the predicted movie rating for the particular user. and RMSE score can be calculated from the test set by predicting all ratings and comparing them to the known ratings.\n\nThe approach again calculates a cosine similarity, this time between each user and between each item. In the user-user approach it then uses this to calculate a prediction based on the ratings of similar users (giving a higher weighting to more similar users) to calculate an average prediction.","8066ea96":"# Most commonly watched movie by people who watched X were\n\nThis recommender takes the approach of looking at at all users who have watched a particular movie and then counts the returns the most popular movie returned by that group.","a9d73c1b":"## Content Based Cosine Similarity","bc003cfc":"## Creating a Matrix","972ae888":"## Calculating Sparsity\nHere I calculate the sparsity of the data set which is defined as a count of zero elements divided by total possible elements in a matrix of user x movie ratings. This is an important measure as the sparsity of the data set can impact the accuracy of different recommender methods and models.","96d6cb56":"Now I append my movie ratings to the existing data set","2e7c4506":"## Recommendation Systems\n\nThis notebook looks at several implementation of recommendation systems for a movie recommender that can be extended for use with other product types.\nFour methods are explored including:\n- Simple non-personalised recommender systems\n- Collaborative Filtering (user-user and user-item)\n- Matrix Factorization\n- Neural Networks\n\n### Data Set\nThe data set used for this notebook is the 1M ratings data set from MovieLens. This contains 1M ratings of movies from 7120 movies and 14,025 Users. This data set includes:\n- movieId\n- userId\n- rating\n\nIn addition a data set of the movies includes the movie name and genres.\n\n- movieId\n- title\n- genres\n\n### Benchmarking Models\nThe non personalised recommender systems do not have a benchmarking metric - though in practice their effectiveness could be measured using A\/B tests and metrics such as click-throug-rate or converesion to a transaction  to compare agains other types of recommender systems.\n\nThe recommender systems that predict user ratings are benchmarked by calculating the 'root mean squared error' as a measure of how closely the prediction is to the actual ratings in a test set. A base benchmark is calculated by predicting all of the ratings as the average rating across all ratings and calculating the root mean squared error. This is the rmse score which the subsequent models aim to beat. ","bd4989a1":"### Plot Test vs predicted values","521b3973":"https:\/\/github.com\/khanhnamle1994\/movielens\nhttps:\/\/github.com\/khanhnamle1994\/movielens\/blob\/master\/CFModel.py\nhttps:\/\/keras.io\/layers\/merge\/","791041f8":"## NN Model ","6a7c0e5c":"## Calculate RMSE","a9525aa2":"Now I plot the above data set to display the % or ratings given each score","f115baf6":"## Recommend Top Average Rated Movie by Sector\n\nThe first recommendation system I will build is to recommend the best movie by genre based on its weighted average using a formula called the credibility formula.  This will produce a non personalised and generalized recommender where the same recommendations are made for every user. This particular formula is used for IMDB\u2019s top 250 movie recommendation list  at https:\/\/www.imdb.com\/chart\/top. As I have already re-formatted the genre for each movie the top movies overall can be recommended as well as top movies by genre.","ae3fdf07":"## Convert Genre Column into multiple columns\n\nHere i convert the genre column into multiple columns with one per genre.","76add851":"## Train and Test Set Split\nHere I create a training and a test set from all of the ratings . The first step is to order the movie and user IDs and assign a sequential number to them - so that they can be added to the training and test matrices in order and so that a mapping exists back from the movie and user ID to its index in the matrix. Scondly the train_test_split function is used to allocate 80% of ratings to the training set and 20% to the test set. Finally the training and the test set are both matrices of the same size i.e movies in rows and users in columns by adding them to a matrix in ascending order of userId and moviId using. \n","acf56996":"## K Nearest Neighbours on Ratings\n\nThis method uses K means to find the distance between movies, and recommends movies that have the smallest distance between them. It then allow you to specify a specific movie (or radomly select a movie to return the most similar movies. Cosine similarity is used to calculate the distances and the 'brute' algorithm which computes distances between all movie pairs.","8dd3f0e6":"## Matrix Factorization - Recommend Movies for 1 User","a5fdeea3":"### Test Model Parameters\nUse the validation set to choose a value for k"}}