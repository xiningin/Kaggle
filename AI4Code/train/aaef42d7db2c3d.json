{"cell_type":{"73e1b635":"code","3c589920":"code","bf94929b":"code","e744524c":"code","5ceef08e":"code","a3316ee2":"code","98ebc216":"code","82fe8c24":"code","6806153c":"code","0b96983b":"code","36acd742":"code","2caf576c":"markdown","ab57d7f6":"markdown","974ee43e":"markdown","37435557":"markdown","e9145d26":"markdown","0513f4a6":"markdown","a5a13864":"markdown","34b20333":"markdown","59a33c46":"markdown"},"source":{"73e1b635":"# Prepare Stack\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\n# ml import\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score as a_score\n\n# scrape stack\nimport requests\nfrom bs4 import BeautifulSoup","3c589920":"class WebCrawler(object):\n    \"\"\"\n    This class is responsible for scrape data from website.\n    Make Sure You have internet connection.\n    \"\"\"\n    def __init__(self):\n        self.url = 'http:\/\/worldpopulationreview.com\/countries\/'\n    \n    # get the header elements\n    def extract_header(self, links):\n        table_header = []\n        for item in links:\n            table_header.append(item.get('data-field'))\n        table_header = [item for item in table_header if item is not None]\n        return table_header\n    \n    # get the main data from html    \n    def country_population_gen(self):\n        page_as_text = requests.get(self.url).text\n        soup_obj = BeautifulSoup(page_as_text,'lxml')\n        ct_table = soup_obj.find('table',{'class':'table table-striped'})\n        links = ct_table.findAll('a')\n        table_header = self.extract_header(links)\n        country_list = []\n        country_stat = []\n        \n        for item in links:\n            country = ''.join(item.findAll(text=True))\n            if country not in table_header:\n                country_list.append(country) \n\n        for trs in ct_table.findAll('tr')[1:]:\n            tds = trs.findAll('td')\n            row = [tds[1].text, tds[2].text.replace(',', '')]\n            country_stat.append(row)\n            df = pd.DataFrame.from_records(country_stat)    \n            df.columns = ['country', 'population']\n            df['population'] = pd.to_numeric(df.population, errors='coerce')\n        return df        \n    ","bf94929b":"class MetricGenerator(object):\n    \"\"\"\n    This class is generating all the aggregated data for the poster visualizaiton\n    \"\"\"\n    def __init__(self):\n        self.df = pd.read_csv('..\/input\/multipleChoiceResponses.csv', low_memory=False, nrows=2)\n        self.sdf = pd.read_csv('..\/input\/multipleChoiceResponses.csv', low_memory=False)[1:]\n    \n    # create a map between column header and column description\n    def col_map(self):\n        df_s_col = list(self.df.columns)\n        df_s_col_label = self.df.loc[0].tolist()\n        col_map = {name:label for name, label in zip(df_s_col, df_s_col_label)}\n        return col_map\n    \n    # remap the gender\n    def gender_mapper(self, value):\n        if value == 'Male':\n            return 'M'\n        elif value == 'Female':\n            return 'F'\n        else:\n            return 'O'\n    # get the percentage and label of genders    \n    def decompose_gender(self):\n        gender = self.sdf.Q1.apply(lambda x: self.gender_mapper(x))\n        cross_tab = gender.value_counts()\/gender.shape*100\n        label = list(cross_tab.index)\n        value = cross_tab.tolist()\n        return label, value\n    \n    # get metric and frequencies for all required metrics\n    def prog_lang(self):\n        top_lang_df = self.sdf.groupby(\"Q18\").size().reset_index().rename(columns={\"Q18\":\"language\", 0:\"frequency\"}).nlargest(5, \"frequency\")\n        lang = top_lang_df.language.tolist()\n        lang_freq = top_lang_df.frequency.tolist()\n        return lang, lang_freq\n    \n    def ml_tool(self):\n        top_ml_tool = self.sdf.groupby(\"Q20\").size().reset_index().rename(columns={\"Q20\":\"ml_tool\", 0:\"frequency\"}).nlargest(5, \"frequency\")\n        ml_tool = top_ml_tool.ml_tool.tolist()\n        ml_tool_freq = top_ml_tool.frequency.tolist()\n        return ml_tool, ml_tool_freq\n    \n    def viz_tool(self):\n        top_viz_tool = self.sdf.groupby(\"Q22\").size().reset_index().rename(columns={\"Q22\":\"viz_tool\", 0:\"frequency\"}).nlargest(5, \"frequency\")\n        viz_tool = top_viz_tool.viz_tool.tolist()\n        viz_tool_freq = top_viz_tool.frequency.tolist()\n        return viz_tool, viz_tool_freq\n    \n    def applied_ml(self):\n        ml_pref = self.sdf.groupby(\"Q10\").size().reset_index().rename(columns={\"Q10\":\"pref_type\", 0:\"frequency\"}).nlargest(5, \"frequency\")\n        ml_pref_type = [\"Exploring\", \"Not Using\", \"Just Started\", \"Unknown\", \"Using\"]\n        ml_pref_freq = ml_pref.frequency.tolist()\n        return ml_pref_type, ml_pref_freq\n    \n    # few more dirth cleaning and joining the country and population data\n    def c_index(self, df):\n        name_map = {'United States of America':'United States',\n               'Others' : 'Others',\n               'Iran, Islamic Republic of...' : 'Iran',\n               'United Kingdom of Great Britain and Northern Ireland': 'United Kingdom',\n               'Hong Kong (S.A.R.)': 'Hong Kong',\n               'Viet Nam': 'Vietnam',\n               'Republic of Korea':'South Korea'}\n        country_list = df.country.tolist()\n        country_map = {}\n        for item in self.sdf.Q3.unique().tolist():\n            if item not in country_list:\n                country_map[item] = name_map.get(item)\n            else:\n                country_map[item] = item\n        country_by_survey = self.sdf.Q3.value_counts().reset_index()\n        country_by_survey['country'] = country_by_survey['index'].map(country_map)\n        survey_merged = pd.merge(country_by_survey, country_df, on = 'country', how= 'left')\n        survey_merged[\"population\"] = pd.to_numeric(survey_merged.population, errors='coerec')\n        survey_merged['custom_index'] = survey_merged.Q3\/survey_merged.population*10000000\n        survey_merged = survey_merged.sort_values(\"custom_index\", ascending=False)\n        return survey_merged","e744524c":"class PlotMaker(object):\n    \"\"\"\n    This class responsible for creating custom plots\n    \"\"\"\n    # method for creating pie plot\n    def pie_plot(self, axxr, gen_label, gen_value, colors):\n        patches, texts, autotexts = axxr.pie(gen_value,\n                                            labels=gen_label,\n                                            autopct='%1.1f%%',\n                                            #explode=[0.05,0.05,0.05], \n                                            startangle=0, \n                                            pctdistance=0.85,\n                                            colors=colors)\n        # plt.title('Survey Response Ratio, by Gender in 2018',fontsize=20)\n        # Format the text labels\n        for text in texts+autotexts:\n            text.set_fontsize(11)\n            text.set_fontweight('bold')\n        for text in autotexts:\n            text.set_color('white')\n        # draw circle\n        centre_circle = plt.Circle((0,0),0.4,fc='white')\n        axxr.add_artist(centre_circle)\n        return axxr\n    \n    # method for creating bar plot\n    def bar_plot(self, axxr, label, value, color, ylabel):\n        rng = len(label)\n        _ = axxr.bar(range(rng),value, align='center',color=color)\n        axxr.set_xticks(range(rng))\n        # axxr.set_xticklabels(labels=country_list)\n        axxr.set_xticklabels(label, rotation=15)\n        axxr.set_ylabel(ylabel)\n        # plt.xticks(range(10),top10_arrivals_countries,fontsize=18)\n        return axxr\n    \n    # method for creating minibar plot\n    def minibar_plot(self, axxr, label, value, color):\n        rng = len(label)\n        _ = axxr.bar(range(rng),value, align='center',color=color)\n        axxr.set_xticks(range(rng))\n        # axxr.set_xticklabels(labels=country_list)\n        axxr.set_xticklabels(label,fontdict={'fontweight': 'bold'} ,rotation=60)\n        axxr.yaxis.set_visible(False)\n        # plt.xticks(range(10),top10_arrivals_countries,fontsize=18)\n        return axxr","5ceef08e":"class DecisionSupportSystem(object):\n    \"\"\"\n    This class will generate data and build the model\n    \"\"\"\n    # remap the column for system\n    def __init__(self):\n        self.sdf = pd.read_csv('..\/input\/multipleChoiceResponses.csv', low_memory=False)[1:]\n        self.ml_col_list = [\"Q2\", \"Q4\", \"Q8\", \"Q37\",\"Q17\", \"Q20\", \"Q23\", \"Q24\",\"Q6\"]\n        self.age_map_q2 = {'18-21': 1, '22-24': 2, '25-29': 3, '30-34': 4, '35-39': 5, '40-44': 6, '45-49': 7, '50-54': 8, '55-59': 9, '60-69': 10, '70-79': 11, '80+': 12}\n        self.education_map_q4 = {'Doctoral degree' : 6, 'Bachelor\u2019s degree' : 4, 'Master\u2019s degree' : 5,\n                         'Professional degree' : 3, 'Some college\/university study without earning a bachelor\u2019s degree' : 2,\n                         'I prefer not to answer' : 0, 'No formal education past high school' : 1}\n        self.experience_map_q8 = {'0-1': 1, '1-2': 2, '2-3': 3, '3-4': 4, '4-5': 5, '5-10': 6, '10-15': 7, '15-20': 8, '20-25': 9, '25-30': 10, \"30 +\": 11}\n        self.course_map_q37 = {'DataCamp': 'datac', 'Coursera': 'cours', 'Kaggle Learn': 'kaggl', 'Udacity': 'udaci', \n                       'Other': 'other', 'developers.google.com': 'devel', 'Online University Courses': 'onlin', \n                       'Udemy': 'udemy', 'DataQuest': 'dataq', 'Fast.AI': 'fasta', 'TheSchool.AI': 'thesc'}\n        self.job_map_q6 = {'Consultant': 'consu', 'Data Scientist': 'datas', 'Data Analyst': 'dataa', 'Software Engineer': 'softw', \n                   'Research Assistant': 'resea', 'Chief Officer': 'chief', 'Research Scientist': 'resea', \n                   'Business Analyst': 'busin', 'Data Engineer': 'datae', 'Developer Advocate': 'devel', \n                   'Marketing Analyst': 'marke', 'Product\/Project Manager': 'produ', 'Principal Investigator': 'princ', \n                   'DBA\/Database Engineer': 'dbada', 'Statistician': 'stati', 'Data Journalist': 'dataj'}\n        \n        self.reverse_job_map = {'consu': 'Consultant', 'datas': 'Data Scientist', 'dataa': 'Data Analyst', \n                           'softw': 'Software Engineer','resea': 'Research Scientist', 'chief': 'Chief Officer', \n                           'busin': 'Business Analyst', 'datae': 'Data Engineer', 'devel': 'Developer Advocate', \n                           'marke': 'Marketing Analyst', 'produ': 'Product\/Project Manager', \n                           'princ': 'Principal Investigator', 'dbada': 'DBA\/Database Engineer', \n                           'stati': 'Statistician', 'dataj': 'Data Journalist'}\n\n        self.job_map_q6b = {'Consultant': 'consu', 'Data Scientist': 'datas', 'Data Analyst': 'dataa',\n                       'Research Assistant': 'resea', 'Research Scientist': 'resea', 'Business Analyst': 'busin', \n                       'Data Engineer': 'datae', 'Marketing Analyst': 'marke', 'Statistician': 'stati'}\n\n        self.lang_map_q17 = {'Java': 'java', 'Python': 'python', 'SQL': 'sql', 'Javascript\/Typescript': 'javascript\/typescript', \n                        'C#\/.NET': 'c#\/.net', 'R': 'r', 'MATLAB': 'matlab', 'C\/C++': 'c\/c++', 'Visual Basic\/VBA': 'visual_basic\/vba', \n                        'Bash': 'bash', 'Scala': 'scala', 'PHP': 'php', 'SAS\/STATA': 'sas\/stata', 'Other': 'other', 'Ruby': 'ruby', \n                        'Go': 'go', 'Julia': 'julia'}\n        self.code_time_map_q23 = {'0% of my time' : 1, '1% to 25% of my time': 2, '75% to 99% of my time' : 5, \n                         '50% to 74% of my time': 4, '25% to 49% of my time' : 3,'100% of my time': 6}\n        self.code_exp_map_q24 = {'I have never written code but I want to learn' : 2, '5-10 years' : 6,\n                        '3-5 years' : 5, '< 1 year' : 3, '1-2 years' : 4, '10-20 years': 7, '20-30 years': 8,\n                        '30-40 years': 9,'I have never written code and I do not want to learn' : 1, '40+ years': 10}\n        self.ml_pak_map_q20 = {'Scikit-Learn': 'scikit-learn', 'Keras': 'keras', 'TensorFlow': 'tensorflow', 'Caret': 'caret', \n                      'Spark MLlib': 'spark-mllib', 'Caffe': 'caffe', 'mlr': 'mlr', 'PyTorch': 'pytorch', \n                      'randomForest': 'randomforest', 'H20': 'h20', 'Xgboost': 'xgboost', 'lightgbm': 'lightgbm', \n                      'Fastai': 'fastai', 'Other': 'other', 'CNTK': 'cntk', 'catboost': 'catboost', \n                      'Prophet': 'prophet', 'Mxnet': 'mxnet'}\n        \n    # method to get the reverse map from the previous map\n    def reverse_map(self, dict_obj):\n        reverse_map = {value:key for key, value in dict_obj.items()}\n        return reverse_map\n    # method for one vs all encoding for response variable\n    def one_all_encode(self, response, job_code):\n        y_train = response.apply(lambda x: 1 if x==job_code else 0)\n        return y_train\n    \n    def prepare_ml_data(self, job_code):\n        # impute and clean the data for model\n        ml_df = self.sdf[self.ml_col_list]\n        ml_df =  ml_df.dropna(axis=0, subset=['Q6', 'Q8'])\n        ml_df[\"Q6\"] = ml_df.Q6.apply(lambda x: np.nan if x is np.nan else self.job_map_q6b.get(x, np.nan))\n        ml_df =  ml_df.dropna(axis=0, subset=['Q6'])\n        ml_df[\"Q2\"] = ml_df.Q2.map(self.age_map_q2)\n        ml_df[\"Q4\"] = ml_df.Q4.apply(lambda x: self.education_map_q4.get(x))\n        ml_df[\"Q8\"] = ml_df.Q8.apply(lambda x: 0 if x is np.nan else self.experience_map_q8.get(x))\n        ml_df[\"Q37\"] = ml_df.Q37.apply(lambda x: \"unkno37\" if x is np.nan else self.course_map_q37.get(x))\n        ml_df[\"Q17\"] = ml_df.Q17.apply(lambda x: \"unkno17\" if x is np.nan else self.lang_map_q17.get(x))\n        ml_df[\"Q20\"] = ml_df.Q20.apply(lambda x: \"unkno20\" if x is np.nan else self.ml_pak_map_q20.get(x))\n        ml_df[\"Q23\"] = ml_df.Q23.apply(lambda x: 0 if x is np.nan else self.code_time_map_q23.get(x))\n        ml_df[\"Q24\"] = ml_df.Q24.apply(lambda x: 0 if x is np.nan else self.code_exp_map_q24.get(x))\n        ml_df = ml_df[ml_df.Q4 != 0]\n        # get dummy variables\n        online_platform = pd.get_dummies(ml_df.Q37, drop_first=True)\n        programming = pd.get_dummies(ml_df.Q17, drop_first=True)\n        ml_pack = pd.get_dummies(ml_df.Q20, drop_first=True)\n        # remove rows with null value\n        ml_df = ml_df.drop([\"Q37\", \"Q17\", \"Q20\"], axis=1)\n        ml_df = pd.concat([ml_df, online_platform, programming, ml_pack], axis=1)\n        # remove un necessary columns\n        X_df = ml_df.drop(\"Q6\", axis=1)\n        y_series = self.one_all_encode(ml_df.Q6, job_code)\n        return X_df, y_series\n    \n    def fit_model(self,X_set, y_set):\n        # sorten the question map\n        prediction_label_map = {\"Q2\" : \"Age(year)\" ,\"Q4\" : \"Education Level\" ,\"Q8\" : \"Experience\", \"Q23\" : \"Time Spent on Coding\" ,\n                            \"Q24\" : \"Coding Experience\", \"p_1\" : \"Probability\", \"online_platform\" : \"Preferred Online Platform\", \n                            \"prog_lang\" : \"Preferrend Language\", \"mlp_pak\" : \"Preferred ML Platform\"}\n        # create train test split\n        X_train, X_test, y_train, y_test = train_test_split(X_set, y_set, test_size=0.2,random_state=101)\n        logmodel = LogisticRegression(C=100.0, random_state=1)\n        logmodel.fit(X_train, y_train)\n        predictions = logmodel.predict(X_test)\n        accuracy_value = a_score(y_test, predictions)\n        r_square_score = logmodel.score(X_set, y_set)\n        # get the probability from the model\n        prob_df = pd.DataFrame(logmodel.predict_proba(X_train), columns=[\"p_0\", \"p_1\"])\n        final_predict = pd.concat([X_train.reset_index(drop=True), prob_df], axis=1)\n        result = final_predict.nlargest(3, \"p_1\").rename_axis(\"_id\").reset_index()\n        result = result.drop_duplicates(['p_1'], keep='first')\n        # convert data from wide to long format\n        column_platform = ['_id', 'datac', 'dataq', 'devel', 'fasta', 'kaggl', 'onlin', 'other', 'thesc', 'udaci', 'udemy','unkno37']\n        column_programming = ['_id', 'c#\/.net', 'c\/c++', 'go', 'java', 'javascript\/typescript','julia', 'matlab', 'other', \n                              'php', 'python', 'r', 'ruby', 'sas\/stata', 'scala', 'sql', 'unkno17', 'visual_basic\/vba']\n        column_ml_pak = ['_id', 'caret', 'catboost','cntk', 'fastai', 'h20', 'keras', 'lightgbm', 'mlr', 'mxnet', \n              'other', 'prophet', 'pytorch', 'randomforest', 'scikit-learn', 'spark-mllib', \n              'tensorflow', 'unkno20', 'xgboost']\n        column_rest = ['_id', 'Q2', 'Q4', 'Q8', 'Q23', 'Q24', 'p_1']\n        # select the rows hiving value only 1 for hot encoding columns\n        result_plt = pd.melt(result[column_platform], id_vars=['_id'], var_name=\"online_platform\", value_name=\"platform_encoding\")\n        result_plt = result_plt[result_plt.platform_encoding == 1].set_index(\"_id\")\n        result_prg = pd.melt(result[column_programming], id_vars=['_id'], var_name=\"prog_lang\", value_name=\"lang_encoding\")\n        result_prg = result_prg[result_prg.lang_encoding == 1].set_index(\"_id\")\n        result_mlp = pd.melt(result[column_ml_pak], id_vars=['_id'], var_name=\"mlp_pak\", value_name=\"pak_encoding\")\n        result_mlp = result_mlp[result_mlp.pak_encoding == 1].set_index(\"_id\")\n        result_rest = result[column_rest].set_index(\"_id\")\n        result_top = result_rest.join(result_plt.drop(\"platform_encoding\", axis=1)).join(result_prg.drop(\"lang_encoding\", axis=1))\n        result_top = result_top.join(result_mlp.drop(\"pak_encoding\", axis=1)).reset_index(drop=True)\n        # get the actual values\n        result_top[\"Q2\"] = result_top.Q2.map(self.reverse_map(self.age_map_q2))\n        result_top[\"Q4\"] = result_top.Q4.map(self.reverse_map(self.education_map_q4))\n        result_top[\"Q8\"] = result_top.Q8.map(self.reverse_map(self.experience_map_q8))\n        result_top[\"Q23\"] = result_top.Q23.map(self.reverse_map(self.code_time_map_q23))\n        result_top[\"Q24\"] = result_top.Q24.map(self.reverse_map(self.code_exp_map_q24))\n        result_top[\"online_platform\"] = result_top.online_platform.apply(lambda x: self.reverse_map(self.course_map_q37).get(x, \"Not Found\"))\n        result_top[\"prog_lang\"] = result_top.prog_lang.map(self.reverse_map(self.lang_map_q17))    \n        result_top[\"mlp_pak\"] = result_top.mlp_pak.apply(lambda x: self.reverse_map(self.ml_pak_map_q20).get(x, \"Not Found\"))  \n        result_top[\"p_1\"] = result_top.p_1.apply(lambda x: str(x)[:4])\n        result_top = result_top.rename(columns=prediction_label_map)\n        return result_top, accuracy_value, r_square_score\n    # convert the result to string\n    def result_to_string(self, result_item):\n        string = ''\n        for key, value in result_item.items():\n            string = string + \"{} : {} \".format(key, value) + \"\\n\"\n        return string\n    \n    # finction to create the visual poster\n    def persona_poster(self, label, result_dict):\n        mpl.style.use('seaborn')\n        fontparams = {'size':12,'fontweight':'light', 'family':'monospace','style':'normal'}\n        fig = plt.figure(figsize=(9.9,14))\n        G = gridspec.GridSpec(10, 6)\n        head_axes = plt.subplot(G[0, :], facecolor='teal')\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(0.5, 0.5, 'Be like {} !'.format(label), color='white', ha='center', va='center', size=24)\n\n        foot_axes = plt.subplot(G[9, :], facecolor='teal')\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(0.5, 0.5, '\u00a9 Copyright 2018, XXX \\n source: https:\/\/www.kaggle.com\/kaggle\/kaggle-survey-2018 \\n\\nAuthor: DataPsycho', color='white', ha='center', va='center', size=14)\n\n        axes_12a = plt.subplot(G[1:3, :-4], facecolor='#333333')\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(0.5, 0.5, 'Model Info.', color='white', ha='center', va='center', size=24, alpha=1.0)\n\n        axes_12b = plt.subplot(G[1:3, 2:], facecolor='#333333')\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(0.5, 0.6, 'R Square : {:.2f} \\n \\n Test Accuracy: {:.2f}'.format(r_square, accuracy), color='white', ha='center', va='center', size=15, alpha=1.0, fontdict=fontparams)\n\n        axes_34a = plt.subplot(G[3:5, :-4], facecolor='#041f33')\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(0.5, 0.5, 'Bilkis', color='white', ha='center', va='center', size=24, alpha=1.0)\n\n        axes_34b = plt.subplot(G[3:5, 2:], facecolor='#041f33')\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(0.5, 0.5, dss.result_to_string(result_dict[0]), color='white', ha='center', va='center', size=14, alpha=1.0, fontdict=fontparams)\n\n        axes_56a = plt.subplot(G[5:7, :-4], facecolor='#45002d')\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(0.5, 0.5, 'Mofiz', color='white', ha='center', va='center', size=24, alpha=1.0)\n\n        axes_56b = plt.subplot(G[5:7, 2:], facecolor='#45002d')\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(0.5, 0.5, dss.result_to_string(result_dict[1]), color='white', ha='center', va='center', size=14, alpha=1.0, fontdict=fontparams)\n\n        axes_78a = plt.subplot(G[7:9, :-4], facecolor='#03314a')\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(0.5, 0.5, 'Ad\u00e9la', color='white', ha='center', va='center', size=24, alpha=1.0)\n\n        axes_78b = plt.subplot(G[7:9, 2:], facecolor='#03314a')\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(0.5, 0.5, dss.result_to_string(result_dict[2]), color='white', ha='center', va='center', size=14, alpha=1.0, fontdict=fontparams) \n        plt.tight_layout()\n        return plt.show()","a3316ee2":"crawler = WebCrawler()\nmetric_manager = MetricGenerator()\nplot_manager = PlotMaker()\ncountry_df = crawler.country_population_gen()\ncindex_df = metric_manager.c_index(country_df)","98ebc216":"print(\"Top 10 Countries According to C Index.\")\nprint(cindex_df.drop([\"index\", \"Q3\"], axis=1).head(10))","82fe8c24":"def hightlights_poster():\n    mpl.style.use('seaborn')\n    fig = plt.figure(figsize=(12.7,18))\n    G = gridspec.GridSpec(10, 5)\n    head_axes = plt.subplot(G[0, :], facecolor='teal')\n    plt.xticks(())\n    plt.yticks(())\n    plt.text(0.5, 0.5, 'Kaggle Survey Highlights, 2018', color='white', ha='center', va='center', size=24)\n\n    foot_axes = plt.subplot(G[9, :], facecolor='teal')\n    plt.xticks(())\n    plt.yticks(())\n    plt.text(0.5, 0.5, '\u00a9 Copyright 2018, XXX \\n source: https:\/\/www.kaggle.com\/kaggle\/kaggle-survey-2018 \\n\\nAuthor: DataPsycho', color='white', ha='center', va='center', size=14)\n\n    axes_12a = plt.subplot(G[1:3, :-3], facecolor='lightcoral')\n    plt.xticks(())\n    plt.yticks(())\n    plt.text(0.5, 0.5, 'Hire More Women !', color='black', ha='center', va='center', size=24, alpha=1.0)\n    plt.text(0.5, 0.3, 'Approximately each 5 resopnse \\nthere is only 1 woman responded, \\nWomen might have less participation in Data Science.', color='black', ha='center', va='center', size=12, alpha=1.0)\n\n    axes_12b = plt.subplot(G[1:3, 2:])\n    label, value = metric_manager.decompose_gender()\n    colors = ['navy','lightcoral', 'grey']\n    plot_manager.pie_plot(axes_12b, label, value, colors)\n    plt.text(0.0, 0.0, 'M : F \\n4 : 1', color='black', ha='center', va='center', size=12)\n\n    axes_34a = plt.subplot(G[3:5, 2:], facecolor='lightgrey')\n    bottom_country = cindex_df.nsmallest(10, 'custom_index').sort_values('custom_index', ascending=False)\n    country_list = bottom_country.country.tolist()\n    custom_index = bottom_country.custom_index.tolist()\n    plot_manager.bar_plot(axes_34a, country_list, custom_index, '#3b5998', 'C Index')\n\n    axes_34b = plt.subplot(G[3:5, :2], facecolor='#3b5998')\n    plt.xticks(())\n    plt.yticks(())\n    plt.text(0.5, 0.5, 'Together We can Go Far !', color='white', ha='center', va='center', size=20)\n    plt.text(0.5, 0.3, 'The bottom 10 Countries according to \\nC Index (responses per 10M people).', color='white', ha='center', va='center', size=12, alpha=1.0)\n\n    axes_56a = plt.subplot(G[5:7, :-3], facecolor='#2a4b5a')\n    plt.xticks(())\n    plt.yticks(())\n    plt.text(0.5, 0.5, 'Programmer\\'s Tools!', color='white', ha='center', va='center', size=24)\n    plt.text(0.5, 0.3, 'Python, Scikit-Learn, Matplot \\nis the ultimate bundle.', color='white', ha='center', va='center', size=12, alpha=1.0)\n\n    axes_56b = plt.subplot(G[5:7, 2:], facecolor='lightgrey')\n    lang, lang_freq = metric_manager.prog_lang()\n    plot_manager.bar_plot(axes_56b, lang, lang_freq, '#2a4b5a',  'no. of responses')\n    subax1 = inset_axes(axes_56b, width=2, height=1, loc=1)\n    ml_tool, ml_tool_freq = metric_manager.ml_tool()\n    plot_manager.minibar_plot(subax1, ml_tool, ml_tool_freq, '#2a4b5a')\n    subax2 = inset_axes(axes_56b, width=2, height=1, loc=9)\n    viz_tool, viz_tool_freq = metric_manager.viz_tool()\n    plot_manager.minibar_plot(subax2, viz_tool, viz_tool_freq, '#2a4b5a')\n\n    axes_78a = plt.subplot(G[7:9, :-3], facecolor='#4a2339')\n    plt.xticks(())\n    plt.yticks(())\n    plt.text(0.5, 0.5, 'ML: Still A Buzzword?', color='white', ha='center', va='center', size=24)\n    plt.text(0.5, 0.3, 'Dawn of Applied ML implies \\nproper ML culture has not been stablished yet.', color='white', ha='center', va='center', size=12, alpha=1.0)\n\n    axes_78b = plt.subplot(G[7:9, 2:], facecolor='lightgrey')\n    ml_pref_type, ml_pref_freq = metric_manager.applied_ml()\n    plot_manager.bar_plot(axes_78b, ml_pref_type, ml_pref_freq, '#4a2339',  'no. of responses')\n    plt.tight_layout()\n    return plt.show()\n\nhightlights_poster()","6806153c":"# attach the class\ndss = DecisionSupportSystem()\n# assign a profession\nprofession =  \"datas\"\n# get the actual label\nprofession_label = dss.reverse_job_map.get(profession).upper()\n# get the prepared data\nX_df, y_series = dss.prepare_ml_data(profession)\n# fit the model and get required metrics\nresult, accuracy, r_square = dss.fit_model(X_df, y_series)\n# convert result to dict object\nresult_dict = result.to_dict(orient=\"records\")\n# create the poster\ndss.persona_poster(profession_label, result_dict)","0b96983b":"profession =  \"dataa\"\nprofession_label = dss.reverse_job_map.get(profession).upper()\nX_df, y_series = dss.prepare_ml_data(profession)\nresult, accuracy, r_square = dss.fit_model(X_df, y_series)\nresult_dict = result.to_dict(orient=\"records\")\nresult\ndss.persona_poster(profession_label, result_dict)","36acd742":"profession =  \"resea\"\nprofession_label = dss.reverse_job_map.get(profession).upper()\nX_df, y_series = dss.prepare_ml_data(profession)\nresult, accuracy, r_square = dss.fit_model(X_df, y_series)\nresult_dict = result.to_dict(orient=\"records\")\ndss.persona_poster(profession_label, result_dict)","2caf576c":"\n# <div style=\"text-align:center\"><span style=\"color:teal; font-family:Georgia; font-size:2em;\">Building Personas with Top Skills for a Job Role<\/span><\/div>\n*__Abstract -__\nThis notebook describes concept of building a Data Driven Decision Support System,  able to create persona with job profile with specific skill set and can suggest what we need to know if we want to be in that certain job  position.  Through the study we were able to analyse the 2018 Kaggle Machine Learning & Data Science Survey data to build a backend system which is able to suggest a best possible combinatin of selected variables to explain a job description . First we will go through some descriptive study and then we will use logistic regression to fit a model, which can reverse predict the behaviour of a persona based no job descrption.*\n\n__Keywords: Data Visualization, Decisions Support System, EDA, Logistic Regression Modeling, Building Persona __\n\n## Introduction\nDecision  support  Systems (DSS) mostly  focuses  on  developing  systems  to  help  decision makers in any specific field.  System can provide interactive or static report daily or based on any time interval which can help a decision makers to gain more insights so that they can __solve problems more efficiently__ [8]. In our context we will build a DSS based on kaggle survey data to see which kind of skill we need to be in a job role. In the following section We will be using logistic regression to build a model capable of explaining different skill of a persona should have to be in a job.\n\n## Methodology\n\n### Arts in Data Scinece\nThe way of working in acdamic and industry is totally different. In acdamic theoritical knowledge and simulation is preferable (in general). In the other hand in industry we deals with more realistic problems. Also there is a mojor difference bettween presenting data. For example scientific paper focus on less visually appealing but more in depth visuals where as visuals for marketing, it must have to be visually appealing and descriptive.\n\nIn the first section of analysis we will build a poster like visual with basic exploration of the data. To make it as poster we will keep the aspect ration of __A4 size paper (8.3 : 11.7)__. We are going to use some advance features of matplotlib (supplot, grids, insert axes in subplot etc.) [1]. Most of the features can be found in __Matplotlib Official Docuentation__ [3] and __Real Python Matplotlib Guide__ [4]. Some of the advance features is also borrowed frome the __Matplotlib 2.x by Example__ [1, 2]. \n\n### Binary Logistic Regression:\nWe will consider job description (Q6) as our dependent variable and traing one vs all logistic model on some selected job descriptions. The following explanatory variable will be considered:\n+ \"Q2\" : \"Age(year)\" \n+ \"Q4\" : \"Education Level\" \n+ \"Q8\" : \"Experience\"\n+  \"Q23\" : \"Time Spent on Coding\" \n+ \"Q24\" : \"Coding Experience\"\n+ \"Q37\" : \"Online Platform\"\n+ \"Q17\" : \"Preferrend Language\" \n+ \"Q20\" : \"ML Platform\"\n\nAfter fitting the model each time we can easily generate fitted probability for each sample $p(Y = 1 | X)$. From the fitted values we will select top 3 samples as personas based on having highest probability. The following list contains the job description and the custom labels associated with that particular description. __Age, Education level, Experience, Time Spent on Coding, Coding and Experience is considered as ordinal categorical variable and rest of the variable is considered as nominal categorical variable__. So all the ordinal variable has been remaped from 1 to maximum level and ordinal variable has been converted to dummy in the application end [5]. Here is the list of job role and associated short level we are considering for our DSS application:\n\n+ 'Consultant': 'consu'\n+ 'Data Scientist': 'datas'\n+ 'Data Analyst': 'dataa'\n+ 'Research Assistant': 'resea'\n+ 'Research Scientist': 'resea'\n+ 'Business Analyst': 'busin'\n+ 'Data Engineer': 'datae'\n+ 'Marketing Analyst': 'marke'\n+ 'Statistician': 'stati'\n \nHere we are merging two type of researcher in one label. The model is trained with the subset of the data that matches with the following job description. We will use the concept of building poster to represent the personas for few job roles as final result from the DSS.\n\n#### How the whole system works?\nThe whole system can be thought as API. We will input any of the job label as in put to a method and we will get processed data. Then we will provide that data to another method to fit the model and prepare the visual part. The visual part consist of model information and the top ranked (top 3) personas based on the model.\n\n\n","ab57d7f6":"### DSS: Application 2\nIn that section we will look at the 3 personas Bilskis, Mofiz, Ad\u00e9la generated from the system we have build for few selected job positions. We can think all of the three personas are working on a specific job possition and if we wish to work on any of the job position what the skill set we need. For example the below chart reflects 3 personas who have highest probabability (based on the probality value of being in class 1) of being __Data Scientist__. \n\nSo here is the take away, We should learn python, should code more than 50 percent of time for 3 to 5 years to be a good data scientist. We can have online tutorial from Fast.AI to know advance stuff. Bachelor's or Master's degree is enough to start working as data scientist. The following reports for other job position can be explained as the same manner, they just refrect diffenrent job positions.\n\nIn the backend the system relabel the data based on our need and fit a one vs all model. Then its calculate fitted probability from the model (Ex: $p(Data \\ Scientist \\ | \\ Q2, Q4, ..., Q37)$)","974ee43e":"# References:\n[1] Allen Chi Shing Yu, Claire Yik Lok Chung, Aldrin Kay Yuen Yim; Matplotlib 2.x by Example, PacktPub, 2017\n\n[2] https:\/\/www.python-course.eu\/matplotlib_multiple_figures.php \n\n[3] https:\/\/matplotlib.org\/api\/_as_gen\/matplotlib.pyplot.subplot.html\n\n[4] https:\/\/realpython.com\/python-matplotlib-guide\/\n\n[5]  Sebastian Raschka; Python Machine Learning Second Edition, PacktPub, 2017 \n\n[6] Adela B\u02c6ara and Ion Lungu; Improving  Decision  Support  Systems  with  Data  Mining Techniques INTECH Open Access Publisher, 2012\n\n[7] Gareth James,  Daniela Witten, Trevor Hastie, Robert Tibshirani; An Introduction to Statistical Learning with Applications in R, Springer, 2013\n\n[8] Efraim Turban, Ramesh Sharda, and Dursun Delen; Decision  support  and  business  intelligence systems. Pearson Education India, 2011.","37435557":"## Application","e9145d26":"### <div style=\"text-align:left\"><span style=\"color:brown;\"> Important Note <\/span><\/div>\nComplex decision support system with more interactivity need more experience user other wise decision making can be mis-leading. But in our case the DSS is pretty simple and not supporting any such big decision making process[6, 8].\n\n### When Overfitting is Preferred?\nThe model we have created explain the preset and will not be used to predict future. In such cases the general rule of overfitting is not that much important. In such condition we wnat to overfit our model with its best. So that we can get much better accuracy using the present data. Which is kind of a funny fact.\n\n### Further Improvement\nIt was just a quick and short example of use of DSS with real world application. There is a lot of assumption has been made when building the logistic model. So as a part of further imporvement we should investigate the signicicancy of the parameter (i.e. t-test). Also there can other ML model can have much better accuray than Logistic Regression which also we need to evaluate, we can add more attribute or delete attribute to see if we can get some better result [7].","0513f4a6":"#### <div style=\"text-align:center\"><span style=\"color:teal; font-family:Georgia; font-size:2em;\">Some criticism about the survey questionnaire :<\/span><\/div>\nIt's just my personal thinking that kaggle should know.\n+ As we have differnt job role and how much time we spent in what section in a project then I would like to know which data cleaning tool we are using (Spark, Pandas, dplyr, etl etc.), we are familier with that fact data cleaning and transformation takes most of the times in a data science project.\n+ I would like to see the preference on deep learning frameworks seperately.\n+ Rather that which data we use (numerical, categorical etc.) I am more interested in which specific industry has more use case of ML and DL (Computer Vision, Auto Encoder etc.)\n+ As we are considering Business Analyst and other Analysit in the survey they might be using other tools for visualizaiton and only stick on excel and sql. So I would like to know that too.\n\nThanks,\n\n*__DataPsycho__*","a5a13864":"# Conclusion\nWe started with basic exploratory analysis and were able to see how to improve data visualization, more focusing on industrial use rather than publication. Then we build a DSS can predict personas fits best for a job and at the end it does tell some story about the present data science industry based on overall and personal point of view. __The author deliberately preferred the more visual element over writting descriptive findings and left that on reader to make their own decision as it is about Decision Support System__.","34b20333":"# Class Based Modules\nThis section consist the main objects, entity and methods used to build the systems.","59a33c46":"### General Exploration: Application 1\nThe poster it self explain everything. But there is extra data has been added, about what we need to know. The world population data data has been scraped from http:\/\/worldpopulationreview.com\/countries\/. Let's discuss reason of adding that data. When we see in the [surevy challange eda](https:\/\/www.kaggle.com\/paultimothymooney\/2018-kaggle-machine-learning-data-science-survey) we notice that there is such big resopnses from USA, India and China. In common sense it is expected, country with more population suppose to have more response. So we have introduced the following index: $$C \\ Index = {Total \\ Response} \\ \/ \\ {Population} * 10000000$$ (response per 10M people) and calculate that index for each country. Now in the below image we can see the bottom 10 countries according to C Index. The justification of putting bottom 10 countries to raise awareness among those countries. The top countries with C index is as followes:\n\n__*But point to be noted here, if we assume that the country having higher C index suppose to have better data scientist community. Then we can find good data science community in europe*__. \n"}}