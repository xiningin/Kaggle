{"cell_type":{"51235bdc":"code","f16f138f":"code","2973943c":"code","00fc98d6":"code","aca99474":"code","5edaa04d":"code","f9811e07":"code","528fb378":"code","06a629b9":"code","0d35382a":"code","5de03016":"code","bb76e203":"code","e3f11d8e":"code","85cc63f1":"code","0d83368d":"code","5cb3fbc0":"code","a1b48ec2":"code","32d287e2":"code","6a11d883":"code","b4b9558f":"code","f34c97d4":"code","85494d87":"code","6279dbbb":"code","dee3085e":"code","ea8bb40d":"code","6a0b00bb":"code","6a8a9cc1":"code","e625b09a":"code","b7cea533":"code","dcc2fde0":"code","e8d91ff3":"code","51a226e1":"code","dc2f2210":"code","fc9b21ee":"code","1ebf432a":"code","75bb4c28":"code","7e5c79ee":"code","61b754bd":"code","6f7a908f":"code","9ff5d892":"code","70498090":"code","33970d46":"code","11b940b6":"code","e2c6703f":"code","3af84b1a":"code","1ac6fffb":"code","75cfc19d":"code","95702d2b":"code","3811a7f8":"code","3c7d6207":"code","f882a1e0":"code","9742490b":"code","685d81eb":"code","c3d52d51":"code","bdb2d5c4":"markdown","e234f3e5":"markdown","c5b30621":"markdown","ec844a6c":"markdown","e8fcbf77":"markdown","4686a407":"markdown","3dcb6efe":"markdown","847c3c23":"markdown","682a2317":"markdown","d834619c":"markdown","e69e7a29":"markdown","7a331e89":"markdown","635094de":"markdown","24e564d8":"markdown","26981ac5":"markdown","55746795":"markdown","8ac5e954":"markdown","6fe09b8d":"markdown"},"source":{"51235bdc":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom collections import Counter\nfrom IPython.core.display import display, HTML\nsns.set_style('darkgrid')","f16f138f":"dataset = pd.read_csv('\/kaggle\/input\/glass\/glass.csv')\ndataset.head()","2973943c":"corr = dataset.corr()\n#Plot figsize\nfig, ax = plt.subplots(figsize=(10, 8))\n#Generate Heat Map, allow annotations and place floats in map\nsns.heatmap(corr, cmap='coolwarm', annot=True, fmt=\".2f\")\n#Apply xticks\nplt.xticks(range(len(corr.columns)), corr.columns);\n#Apply yticks\nplt.yticks(range(len(corr.columns)), corr.columns)\n#show plot\nplt.show()","00fc98d6":"f, axes = plt.subplots(1,2,figsize=(14,4))\n\nsns.distplot(dataset['RI'], ax = axes[0])\naxes[0].set_xlabel('Refractive Index', fontsize=14)\naxes[0].set_ylabel('Count', fontsize=14)\naxes[0].yaxis.tick_left()\n\nsns.violinplot(x = 'Type', y = 'RI', data = dataset, hue = 'Type', dodge = False, ax = axes[1])\naxes[1].set_xlabel('Type of glass', fontsize=14)\naxes[1].set_ylabel('Refractive Index', fontsize=14)\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\naxes[1].legend(bbox_to_anchor=(1.15, 1), loc=2, borderaxespad=0.)\n\nplt.show()","aca99474":"f, axes = plt.subplots(1,2,figsize=(14,4))\n\nsns.distplot(dataset['Na'], ax = axes[0])\naxes[0].set_xlabel('Sodium', fontsize=14)\naxes[0].set_ylabel('Count', fontsize=14)\naxes[0].yaxis.tick_left()\n\nsns.boxplot(x = 'Type', y = 'Na', data = dataset, hue = 'Type', dodge = False, ax = axes[1])\naxes[1].set_xlabel('Type of glass', fontsize=14)\naxes[1].set_ylabel('Sodium', fontsize=14)\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\naxes[1].legend(bbox_to_anchor=(1.15, 1), loc=2, borderaxespad=0.)\n\nplt.show()","5edaa04d":"f, axes = plt.subplots(1,2,figsize=(14,4))\n\nsns.distplot(dataset['Mg'], ax = axes[0])\naxes[0].set_xlabel('Magnesium', fontsize=14)\naxes[0].set_ylabel('Count', fontsize=14)\naxes[0].yaxis.tick_left()\n\nsns.violinplot(x = 'Type', y = 'Mg', data = dataset, hue = 'Type', dodge = False, ax = axes[1])\naxes[1].set_xlabel('Type of glass', fontsize=14)\naxes[1].set_ylabel('Magnesium', fontsize=14)\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\naxes[1].legend(bbox_to_anchor=(1.15, 1), loc=2, borderaxespad=0.)\n\nplt.show()","f9811e07":"f, axes = plt.subplots(1,2,figsize=(14,4))\n\nsns.distplot(dataset['Al'], ax = axes[0])\naxes[0].set_xlabel('Aluminum', fontsize=14)\naxes[0].set_ylabel('Count', fontsize=14)\naxes[0].yaxis.tick_left()\n\nsns.violinplot(x = 'Type', y = 'Al', data = dataset, hue = 'Type', dodge = False, ax = axes[1])\naxes[1].set_xlabel('Type of glass', fontsize=14)\naxes[1].set_ylabel('Aluminum', fontsize=14)\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\naxes[1].legend(bbox_to_anchor=(1.15, 1), loc=2, borderaxespad=0.)\n\nplt.show()","528fb378":"f, axes = plt.subplots(1,2,figsize=(14,4))\n\nsns.distplot(dataset['Si'], ax = axes[0])\naxes[0].set_xlabel('Silicon', fontsize=14)\naxes[0].set_ylabel('Count', fontsize=14)\naxes[0].yaxis.tick_left()\n\nsns.violinplot(x = 'Type', y = 'Si', data = dataset, hue = 'Type', dodge = False, ax = axes[1])\naxes[1].set_xlabel('Type of glass', fontsize=14)\naxes[1].set_ylabel('Silicon', fontsize=14)\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\naxes[1].legend(bbox_to_anchor=(1.15, 1), loc=2, borderaxespad=0.)\n\nplt.show()","06a629b9":"f, axes = plt.subplots(1,2,figsize=(14,4))\n\nsns.distplot(dataset['K'], ax = axes[0])\naxes[0].set_xlabel('Potassium', fontsize=14)\naxes[0].set_ylabel('Count', fontsize=14)\naxes[0].yaxis.tick_left()\n\nsns.violinplot(x = 'Type', y = 'K', data = dataset, hue = 'Type', dodge = False, ax = axes[1])\naxes[1].set_xlabel('Type of glass', fontsize=14)\naxes[1].set_ylabel('Potassium', fontsize=14)\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\naxes[1].legend(bbox_to_anchor=(1.15, 1), loc=2, borderaxespad=0.)\n\nplt.show()","0d35382a":"f, axes = plt.subplots(1,2,figsize=(14,4))\n\nsns.distplot(dataset['Ca'], ax = axes[0])\naxes[0].set_xlabel('Calcium', fontsize=14)\naxes[0].set_ylabel('Count', fontsize=14)\naxes[0].yaxis.tick_left()\n\nsns.violinplot(x = 'Type', y = 'Ca', data = dataset, hue = 'Type', dodge = False, ax = axes[1])\naxes[1].set_xlabel('Type of glass', fontsize=14)\naxes[1].set_ylabel('Calcium', fontsize=14)\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\naxes[1].legend(bbox_to_anchor=(1.15, 1), loc=2, borderaxespad=0.)\n\nplt.show()","5de03016":"f, axes = plt.subplots(1,2,figsize=(14,4))\n\nsns.distplot(dataset['Ca'], ax = axes[0])\naxes[0].set_xlabel('Barium', fontsize=14)\naxes[0].set_ylabel('Count', fontsize=14)\naxes[0].yaxis.tick_left()\n\nsns.violinplot(x = 'Type', y = 'Ca', data = dataset, hue = 'Type', dodge = False, ax = axes[1])\naxes[1].set_xlabel('Type of glass', fontsize=14)\naxes[1].set_ylabel('Barium', fontsize=14)\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\naxes[1].legend(bbox_to_anchor=(1.15, 1), loc=2, borderaxespad=0.)\n\nplt.show()","bb76e203":"f, axes = plt.subplots(1,2,figsize=(14,4))\n\nsns.distplot(dataset['Fe'], ax = axes[0])\naxes[0].set_xlabel('Iron', fontsize=14)\naxes[0].set_ylabel('Count', fontsize=14)\naxes[0].yaxis.tick_left()\n\nsns.violinplot(x = 'Type', y = 'Fe', data = dataset, hue = 'Type', dodge = False, ax = axes[1])\naxes[1].set_xlabel('Type of glass', fontsize=14)\naxes[1].set_ylabel('Iron', fontsize=14)\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\naxes[1].legend(bbox_to_anchor=(1.15, 1), loc=2, borderaxespad=0.)\n\nplt.show()","e3f11d8e":"X = dataset.drop('Type', axis = 1).values\ny = dataset['Type'].values.reshape(-1,1)","85cc63f1":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)","0d83368d":"print(\"Shape of X_train: \",X_train.shape)\nprint(\"Shape of X_test: \", X_test.shape)\nprint(\"Shape of y_train: \",y_train.shape)\nprint(\"Shape of y_test\",y_test.shape)","5cb3fbc0":"# Fitting Logistic Regression to the Training set\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\nclassifier_lr = LogisticRegression()\nsteps = [\n    ('scalar', StandardScaler()),\n    ('model', LogisticRegression())\n]\n\nlr_pipe = Pipeline(steps)","a1b48ec2":"parameters = { 'model__C' : [1,10,100,1000,10000],\n               'model__fit_intercept' : [True],\n               'model__multi_class' : ['auto'],\n               'model__tol' : [0.0001],\n               'model__solver' : ['newton-cg', 'lbfgs', 'sag', 'saga'],\n               'model__n_jobs' : [-1],\n               'model__max_iter' : [5000],\n               'model__random_state': [42] \n}\nclassifier_lr = GridSearchCV(lr_pipe, parameters, iid=False, cv = 3)\nclassifier_lr = classifier_lr.fit(X_train, y_train.ravel())","32d287e2":"from sklearn.metrics import accuracy_score\n\ny_pred_lr_train = classifier_lr.predict(X_train)\naccuracy_lr_train = accuracy_score(y_train, y_pred_lr_train)\nprint(\"Training set: \", accuracy_lr_train)\n\ny_pred_lr_test = classifier_lr.predict(X_test)\naccuracy_lr_test = accuracy_score(y_test, y_pred_lr_test)\nprint(\"Test set: \", accuracy_lr_test)","6a11d883":"from sklearn.metrics import confusion_matrix\nsns.heatmap(confusion_matrix(y_test, y_pred_lr_test), annot=True, cmap = 'viridis', fmt='.0f')\nplt.show()","b4b9558f":"# Fitting classifier to the Training set\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier_knn = KNeighborsClassifier()\nsteps = [\n    ('scalar', StandardScaler()),\n    ('model', KNeighborsClassifier())\n]\nknn_pipe = Pipeline(steps)","f34c97d4":"parameters = { 'model__algorithm' : ['brute'],\n               'model__leaf_size' : [30,50,70,90,110],\n               'model__metric' : ['minkowski'],\n               'model__p' : [1],\n               'model__n_neighbors' : [3,5,11,19],\n               'model__weights' : ['uniform', 'distance'],\n               'model__n_jobs' : [-1]\n}\nclassifier_knn = GridSearchCV(knn_pipe, parameters, iid=False, cv = 3)\nclassifier_knn = classifier_knn.fit(X_train, y_train.ravel())","85494d87":"y_pred_knn_train = classifier_knn.predict(X_train)\naccuracy_knn_train = accuracy_score(y_train, y_pred_knn_train)\nprint(\"Training set: \", accuracy_knn_train)\n\ny_pred_knn_test = classifier_knn.predict(X_test)\naccuracy_knn_test = accuracy_score(y_test, y_pred_knn_test)\nprint(\"Test set: \", accuracy_knn_test)","6279dbbb":"from sklearn.metrics import confusion_matrix\nsns.heatmap(confusion_matrix(y_test, y_pred_knn_test), annot=True, cmap = 'viridis', fmt='.0f')\nplt.show()","dee3085e":"from sklearn.svm import SVC\nclassifier_svm = SVC()\nsteps = [\n    ('scalar', StandardScaler()),\n    ('model', SVC())\n]\nsvm_linear_pipe = Pipeline(steps)","ea8bb40d":"parameters = { 'model__kernel' : ['linear'],\n               'model__C' : [1,10,100,1000,10000],\n               'model__random_state' : [42]\n}\nclassifier_svm_linear = GridSearchCV(svm_linear_pipe, parameters, iid=False, cv = 3)\nclassifier_svm_linear = classifier_svm_linear.fit(X_train, y_train.ravel())","6a0b00bb":"y_pred_svm_linear_train = classifier_svm_linear.predict(X_train)\naccuracy_svm_linear_train = accuracy_score(y_train, y_pred_svm_linear_train)\nprint(\"Training set: \", accuracy_svm_linear_train)\n\ny_pred_svm_linear_test = classifier_svm_linear.predict(X_test)\naccuracy_svm_linear_test = accuracy_score(y_test, y_pred_svm_linear_test)\nprint(\"Test set: \", accuracy_svm_linear_test)","6a8a9cc1":"from sklearn.metrics import confusion_matrix\nsns.heatmap(confusion_matrix(y_test, y_pred_svm_linear_test), annot=True, cmap = 'viridis', fmt='.0f')\nplt.show()","e625b09a":"# Fitting classifier to the Training set\nfrom sklearn.svm import SVC\nclassifier_svm_kernel = SVC()\nsteps = [\n    ('scalar', StandardScaler()),\n    ('model', SVC())\n]\nsvm_kernel_pipe = Pipeline(steps)","b7cea533":"parameters = { 'model__kernel' : ['rbf', 'poly', 'sigmoid'],\n               'model__C' : [1,10,100,1000,10000],\n               'model__gamma' : [0.001, 0.01, 0.1, 1, 'scale'],\n               'model__random_state' : [42],\n               'model__degree' : [1,2,3]\n}\nclassifier_svm_kernel = GridSearchCV(svm_kernel_pipe, parameters, iid=False, cv = 3)\nclassifier_svm_kernel = classifier_svm_kernel.fit(X_train, y_train.ravel())","dcc2fde0":"y_pred_svm_kernel_train = classifier_svm_kernel.predict(X_train)\naccuracy_svm_kernel_train = accuracy_score(y_train, y_pred_svm_kernel_train)\nprint(\"Training set: \", accuracy_svm_kernel_train)\n\ny_pred_svm_kernel_test = classifier_svm_kernel.predict(X_test)\naccuracy_svm_kernel_test = accuracy_score(y_test, y_pred_svm_kernel_test)\nprint(\"Test set: \", accuracy_svm_kernel_test)","e8d91ff3":"from sklearn.metrics import confusion_matrix\nsns.heatmap(confusion_matrix(y_test, y_pred_svm_kernel_test), annot=True, cmap = 'viridis', fmt='.0f')\nplt.show()","51a226e1":"# Fitting classifier to the Training set\nfrom sklearn.naive_bayes import GaussianNB\nclassifier_nb = GaussianNB()\nclassifier_nb.fit(X_train, y_train.ravel())","dc2f2210":"y_pred_nb_train = classifier_nb.predict(X_train)\naccuracy_nb_train = accuracy_score(y_train, y_pred_nb_train)\nprint(\"Training set: \", accuracy_nb_train)\n\ny_pred_nb_test = classifier_nb.predict(X_test)\naccuracy_nb_test = accuracy_score(y_test, y_pred_nb_test)\nprint(\"Test set: \", accuracy_nb_test)","fc9b21ee":"from sklearn.metrics import confusion_matrix\nsns.heatmap(confusion_matrix(y_test, y_pred_nb_test), annot=True, cmap = 'viridis', fmt='.0f')\nplt.show()","1ebf432a":"# Fitting classifier to the Training set\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier_dt = DecisionTreeClassifier()\n\nsteps = [\n    ('scalar', StandardScaler()),\n    ('model', DecisionTreeClassifier())\n]\ndt_pipe = Pipeline(steps)","75bb4c28":"# Applying Grid Search to find the best model and the best parameters\nparameters = [ { \"model__max_depth\": np.arange(1,21),\n                 \"model__min_samples_leaf\": [1, 5, 10, 20, 50, 100],\n                 \"model__min_samples_split\": np.arange(2, 11),\n                 \"model__criterion\": [\"gini\"],\n                 \"model__random_state\" : [42]}\n            ]\nclassifier_dt = GridSearchCV(estimator = dt_pipe,\n                           param_grid  = parameters,\n                           cv = 3,\n                           iid = False,\n                           n_jobs = -1)\nclassifier_dt = classifier_dt.fit(X_train, y_train.ravel())","7e5c79ee":"y_pred_dt_train = classifier_dt.predict(X_train)\naccuracy_dt_train = accuracy_score(y_train, y_pred_dt_train)\nprint(\"Training set: \", accuracy_dt_train)\n\ny_pred_dt_test = classifier_dt.predict(X_test)\naccuracy_dt_test = accuracy_score(y_test, y_pred_dt_test)\nprint(\"Test set: \", accuracy_dt_test)","61b754bd":"from sklearn.metrics import confusion_matrix\nsns.heatmap(confusion_matrix(y_test, y_pred_dt_test), annot=True, cmap = 'viridis', fmt='.0f')\nplt.show()","6f7a908f":"# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nclassifier_rf = RandomForestClassifier()\n\nsteps = [\n    ('scalar', StandardScaler()),\n    ('model', RandomForestClassifier())\n]\nrf_pipe = Pipeline(steps)","9ff5d892":"parameters =  { \"model__n_estimators\": [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n                \"model__max_features\": [\"auto\", \"sqrt\"],\n                \"model__max_depth\": np.linspace(10, 110, num = 11),\n                \"model__min_samples_split\": [2, 5, 10],\n                \"model__min_samples_leaf\": [1, 2, 4],\n                \"model__bootstrap\": [True, False],\n                \"model__criterion\": [\"gini\"],\n                \"model__random_state\" : [42] }\n            \nclassifier_rf = RandomizedSearchCV(estimator = rf_pipe,\n                                  param_distributions = parameters,\n                                  n_iter = 100,\n                                  cv = 3,\n                                  random_state=42,\n                                  verbose = 4,\n                                  n_jobs = -1)\nclassifier_rf = classifier_rf.fit(X_train, y_train.ravel())","70498090":"y_pred_rf_train = classifier_rf.predict(X_train)\naccuracy_rf_train = accuracy_score(y_train, y_pred_rf_train)\nprint(\"Training set: \", accuracy_rf_train)\n\ny_pred_rf_test = classifier_rf.predict(X_test)\naccuracy_rf_test = accuracy_score(y_test, y_pred_rf_test)\nprint(\"Test set: \", accuracy_rf_test)","33970d46":"from sklearn.metrics import confusion_matrix\nsns.heatmap(confusion_matrix(y_test, y_pred_rf_test), annot=True, cmap = 'viridis', fmt='.0f')\nplt.show()","11b940b6":"# Importing the Keras libraries and packages\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense","e2c6703f":"# Feature Scaling\nsc_X = StandardScaler()\nX_train_scaled = sc_X.fit_transform(X_train)\nX_test_scaled = sc_X.fit_transform(X_test)\nprint(X_train_scaled.shape)\nprint(X_test_scaled.shape)","3af84b1a":"# Defining a function to encode output column\nfrom keras.utils import to_categorical\ndef encode(data):\n    print('Shape of data (BEFORE encode): %s' % str(data.shape))\n    encoded = to_categorical(data)\n    print('Shape of data (AFTER  encode): %s\\n' % str(encoded.shape))\n    return encoded","1ac6fffb":"y_train_encoded = encode(y_train)","75cfc19d":"y_test_encoded = encode(y_test)","95702d2b":"y_train_encoded = np.delete(y_train_encoded, [0,4], axis = 1)\ny_test_encoded = np.delete(y_test_encoded, [0,4], axis = 1)\nprint(y_train_encoded[2])\nprint(y_test_encoded[2])","3811a7f8":"# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu'))\n\n# Adding the second hidden layer\nclassifier.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'relu'))\n\n# Adding the output layer\nclassifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'softmax'))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nhistory = classifier.fit(X_train_scaled, y_train_encoded, validation_data=(X_test_scaled, y_test_encoded), batch_size = 100, epochs = 1150)","3c7d6207":"f, axes = plt.subplots(1,2,figsize=(14,4))\n\naxes[0].plot(history.history['loss'])\naxes[0].plot(history.history['val_loss'])\naxes[0].set_xlabel('Loss', fontsize=14)\naxes[0].set_ylabel('Epuch', fontsize=14)\naxes[0].yaxis.tick_left()\naxes[0].legend(['Train', 'Test'], loc='upper left')\n\naxes[1].plot(history.history['acc'])\naxes[1].plot(history.history['val_acc'])\naxes[1].set_xlabel('Accuracy', fontsize=14)\naxes[1].set_ylabel('Epoch', fontsize=14)\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\naxes[1].legend(['Train', 'Test'], loc='upper left')\n\nplt.show()","f882a1e0":"print(\"Training set: \", history.history.get('acc')[-1])\nprint(\"Test set: \", history.history.get('val_acc')[-1])","9742490b":"models = [('Logistic Regression', accuracy_lr_train, accuracy_lr_test),\n          ('KNN', accuracy_knn_train, accuracy_knn_test),\n          ('SVM (Linear)', accuracy_svm_linear_train, accuracy_svm_linear_test),\n          ('SVM (Kernel)', accuracy_svm_kernel_train, accuracy_svm_kernel_test),\n          ('Naive Bayes', accuracy_nb_train, accuracy_nb_test),\n          ('Decision Tree Classification', accuracy_dt_train, accuracy_dt_test),\n          ('Random Forest Classification', accuracy_rf_train, accuracy_rf_test),\n          ('ANN', history.history.get('acc')[-1], history.history.get('val_acc')[-1]),\n         ]","685d81eb":"predict = pd.DataFrame(data = models, columns=['Model', 'Training Accuracy', 'Test Accuracy'])\npredict","c3d52d51":"f, axes = plt.subplots(2,1, figsize=(14,10))\n\npredict.sort_values(by=['Training Accuracy'], ascending=False, inplace=True)\n\nsns.barplot(x='Training Accuracy', y='Model', data = predict, palette='Blues_d', ax = axes[0])\n#axes[0].set(xlabel='Region', ylabel='Charges')\naxes[0].set_xlabel('Training Accuracy', size=16)\naxes[0].set_ylabel('Model')\naxes[0].set_xlim(0,1.0)\naxes[0].set_xticks(np.arange(0, 1.1, 0.1))\n\npredict.sort_values(by=['Test Accuracy'], ascending=False, inplace=True)\n\nsns.barplot(x='Test Accuracy', y='Model', data = predict, palette='Greens_d', ax = axes[1])\n#axes[0].set(xlabel='Region', ylabel='Charges')\naxes[1].set_xlabel('Test Accuracy', size=16)\naxes[1].set_ylabel('Model')\naxes[1].set_xlim(0,1.0)\naxes[1].set_xticks(np.arange(0, 1.1, 0.1))\n\nplt.show()","bdb2d5c4":"### <span id=\"12\"><\/span> ** Artificial Neural Network (ANN) **","e234f3e5":"## <span id=\"2\"><\/span> ** 2. Importing Libraries and Reading the Dataset **","c5b30621":"### <span id=\"9\"><\/span> ** Naive Bayes **","ec844a6c":"### <span id=\"10\"><\/span> ** Decision Tree Classification **","e8fcbf77":"<hr\/>\n[**Tolgahan Cepel**](https:\/\/www.kaggle.com\/tolgahancepel)\n<hr\/>\n<font color=green>\n1. [Overview](#1)\n1. [Importing Libraries and Reading the Dataset](#2)\n1. [Data Visualization and Preprocessing](#3)\n1. [Classification Models](#4) \n    * [Logistic Regression](#5) \n    * [K-Nearest Neighbors(K-NN)](#6)\n    * [Support Vector Machine (SVM - Linear)](#7)\n    * [Support Vector Machine (SVM - Kernel)](#8)\n    * [Naive Bayes](#9) \n    * [Decision Tree Classification](#10) \n    * [Random Forest Classification](#11)\n    * [Artificial Neural Network (ANN)](#12) \n1. [Comparing the Results](#13)\n    * [Visualizing Models Performance](#14) \n1. [Conclusion](#15)\n<hr\/>","4686a407":"### <span id=\"8\"><\/span> ** Support Vector Machine (SVM - Kernel) **","3dcb6efe":"### <span id=\"11\"><\/span> ** Random Forest Classification **","847c3c23":"## <span id=\"1\"><\/span> ** 1. Overview **","682a2317":"## <span id=\"3\"><\/span> ** 3. Data Visualization and Preprocessing **","d834619c":"### <span id=\"6\"><\/span> ** K-Nearest Neighbors (K-NN) **","e69e7a29":"Columns:\n- <b> RI: <\/b> refractive index \n- <b> NA: <\/b> Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10)\n- <b> Mg: <\/b> Magnesium\n- <b> Al: <\/b> Aluminum\n- <b> K: <\/b> Potassium\n- <b> Ca: <\/b> Calcium\n- <b> Ba: <\/b> Barium\n- <b> Fe: <\/b> Iron\n- <b> Type of glass: <\/b> 1 building_windows_float_processed -- 2 building_windows_non_float_processed -- 3 vehicle_windows_float_processed -- 4 vehicle_windows_non_float_processed (none in this database) -- 5 containers -- 6 tableware -- 7 headlamps","7a331e89":"### <span id=\"7\"><\/span> ** Support Vector Machine (SVM - Linear) **","635094de":"In this kernel, I have built 8 classification models using Glass Identification Dataset. These are logistic, k-nn, svm(linear), svm(kernel), naive bayes, decision tree, random forest and artificial neural network. Then measured and visualized the performance of the models. Please make a comment and let me know how to improve model performance, visualization or something in this kernel. This will also help me on my future works.\n\n<b><font color=\"red\">Don't forget to <\/font><\/b> <b><font color=\"green\">UPVOTE <\/font><\/b> if you liked this kernel, thank you. \ud83d\ude42\ud83d\udc4d","24e564d8":"### <span id=\"5\"><\/span> ** Logistic Regression **","26981ac5":"## <span id=\"13\"><\/span> ** 5. Comparing the Results **","55746795":"## <span id=\"4\"><\/span> ** 4. Classification Models **","8ac5e954":"## <span id=\"15\"><\/span> ** 6. Conclusion **","6fe09b8d":"### <span id=\"14\"><\/span> ** Visualizing Models Performance **"}}