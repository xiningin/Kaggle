{"cell_type":{"bfc15385":"code","f0da6afe":"code","bc7cd438":"code","2a4d9faf":"code","497f6a92":"code","77c43d30":"code","18c45d43":"code","970799c5":"code","6d10bee0":"code","cd81e209":"code","49714aaf":"code","2c6c224e":"code","5adbcef9":"code","58bbc53c":"code","f6934c79":"code","8115f05e":"code","4581aa3f":"code","49853e15":"code","df964f3f":"code","fb14e78e":"code","ca71a348":"code","15111eed":"code","1556a353":"code","d0cf1276":"code","6eb20c81":"code","926c6652":"code","4956623c":"code","7b339089":"code","810a3be0":"code","18363ddb":"code","e71d7ecb":"code","471053fb":"code","eda6adf9":"code","64b5f4dc":"code","99bedd56":"code","c250f56a":"code","0b546d84":"code","e5536a60":"code","84f1ffce":"code","efd39b60":"code","be5269b8":"code","314079eb":"code","7a060123":"code","19a8b5d3":"code","e353da69":"code","97c813eb":"code","8a4d71a8":"code","3b41645d":"code","b302a26b":"code","88c9bf23":"markdown","90669e49":"markdown"},"source":{"bfc15385":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f0da6afe":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option(\"display.max_colwidth\", 200)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport re\n\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\nfrom gensim.models import LsiModel\nfrom gensim.models.coherencemodel import CoherenceModel\n\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport matplotlib.pyplot as plt\nfrom nltk.stem import WordNetLemmatizer \nimport nltk\n\nfrom sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\n\nimport spacy\n\nimport pyLDAvis\nimport pyLDAvis.gensim\nimport pyLDAvis.sklearn\n\nfrom pprint import pprint\nfrom sklearn.decomposition import NMF","bc7cd438":"import time\nstart_time = time.time()","2a4d9faf":"nltk.download('stopwords')\nnltk.download('wordnet')","497f6a92":"train_1 = pd.read_csv('\/kaggle\/input\/npr-data\/npr.csv')\n#train = train_1[:5000]\ntrain = train_1\ntrain.shape","77c43d30":"print(train.columns)\ntrain.head()","18c45d43":"# Convert to list\ndata = train['Article'].values.tolist()\n\ndata = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n\ndata = [re.sub('\\s+', ' ', sent) for sent in data]\n\ndata = [re.sub(\"\\'\", \"\", sent) for sent in data]\n\npprint(data[:1])","970799c5":"def sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n\ndata_words = list(sent_to_words(data))\n\nprint(data_words[:1])","6d10bee0":"def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https:\/\/spacy.io\/api\/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append(\" \".join([token.lemma_ if token.lemma_ not in ['-PRON-'] else '' for token in doc if token.pos_ in allowed_postags]))\n    return texts_out\n\n# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n# Run in terminal: python3 -m spacy download en\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\n# Do lemmatization keeping only Noun, Adj, Verb, Adverb\ndata_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(data_lemmatized[:2])","cd81e209":"print(len(data_words))\nprint(len(data_lemmatized))","49714aaf":"n_features = 5000\nn_components = 12\nn_top_words = 10\n\n# ignore terms that have a document frequency strictly higher than 95%, \n# ignore terms that have a document frequency strictly lower than 2\ntfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n                                   max_features=n_features,\n                                   stop_words='english')\ntfidf = tfidf_vectorizer.fit_transform(data_lemmatized)\n\n# alpha=0 means no regularization, l1_ratio=.5, the penalty is a combination of L1 and L2\nnmf = NMF(n_components=n_components, random_state=1, alpha=.1, l1_ratio=.5).fit(tfidf)\nnmf_output = nmf.fit_transform(tfidf)","2c6c224e":"topicnames = [\"Topic\" + str(i) for i in range(nmf.n_components)]","5adbcef9":"docnames = [\"Doc\" + str(i) for i in range(len(data))]","58bbc53c":"df_document_topic = pd.DataFrame(np.round(nmf_output, 2), columns=topicnames, index=docnames)","f6934c79":"# Get dominant topic for each document\ndominant_topic = np.argmax(df_document_topic.values, axis=1)\ndf_document_topic['dominant_topic'] = dominant_topic","8115f05e":"df_topic_distribution = df_document_topic['dominant_topic'].value_counts().reset_index(name=\"Num Documents\")\ndf_topic_distribution.columns = ['Topic Num', 'Num Documents']\ndf_topic_distribution","4581aa3f":"# Topic-Keyword Matrix\ndf_topic_keywords = pd.DataFrame(nmf.components_)\n\n# Assign Column and Index\ndf_topic_keywords.columns = tfidf_vectorizer.get_feature_names()\ndf_topic_keywords.index = topicnames","49853e15":"# View\ndf_topic_keywords.head()","df964f3f":"# Show top n keywords for each topic\ndef show_topics(vectorizer=tfidf_vectorizer, lda_model=nmf, n_words=10):\n    keywords = np.array(vectorizer.get_feature_names())\n    topic_keywords = []\n    for topic_weights in lda_model.components_:\n        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n        topic_keywords.append(keywords.take(top_keyword_locs))\n    return topic_keywords\n\n#topic_keywords = show_topics(vectorizer=tfidf_vectorizer, lda_model=nmf, n_words=10)        \ntopic_keywords = show_topics()        \n\n# Topic - Keywords Dataframe\ndf_topic_keywords = pd.DataFrame(topic_keywords)\ndf_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\ndf_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\ndf_topic_keywords","fb14e78e":"type(df_topic_keywords)","ca71a348":"elapsed_time = time.time() - start_time\ntime.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))","15111eed":"# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a random sample dataframe\ndf = df_topic_keywords\n#df = df_topic_distribution\n\n# create a link to download the dataframe\ncreate_download_link(df)","1556a353":"data[13]","d0cf1276":"df_document_topic.loc['Doc13']","6eb20c81":"import time\nstart_time = time.time()","926c6652":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option(\"display.max_colwidth\", 200)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport re\n\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\nfrom gensim.models import LsiModel\nfrom gensim.models.coherencemodel import CoherenceModel\n\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport matplotlib.pyplot as plt\nfrom nltk.stem import WordNetLemmatizer \nimport nltk\n\nimport spacy\n\nimport pyLDAvis\nimport pyLDAvis.gensim\n\nfrom pprint import pprint","4956623c":"nltk.download('stopwords')\nnltk.download('wordnet')","7b339089":"documents = train['Article'].values.tolist()\nprint(type(documents))\nprint(type(documents[0]))\nprint(len(documents))","810a3be0":"documents[1]","18363ddb":"def preprocess_data1(doc_set):\n    \"\"\"\n    Input  : docuemnt list\n    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n    Output : preprocessed text\n    \"\"\"\n    # initialize regex tokenizer\n    tokenizer = RegexpTokenizer(r'\\w+')\n    # create English stop words list\n    en_stop = set(stopwords.words('english'))\n    #en_stop = set(stopwords)\n    # Create p_stemmer of class PorterStemmer\n    lemmatizer = WordNetLemmatizer()\n    # list for tokenized documents in loop\n    texts = []\n    # loop through document list\n    for i in doc_set:\n        # clean and tokenize document string\n        raw = i.lower()\n        tokens = tokenizer.tokenize(raw)\n        # remove stop words from tokens\n        stopped_tokens = [i for i in tokens if not i in en_stop]\n        # stem tokens\n        stemmed_tokens = [lemmatizer.lemmatize(i) for i in stopped_tokens]\n        # add tokens to list\n        texts.append(stemmed_tokens)\n    return texts","e71d7ecb":"clean_text=preprocess_data1(documents)","471053fb":"print(type(clean_text))\nprint(type(clean_text[0]))\nprint(len(clean_text))\nclean_text[1]","eda6adf9":"def prepare_corpus(doc_clean):\n    \"\"\"\n    Input  : clean document\n    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n    Output : term dictionary and Document Term Matrix\n    \"\"\"\n    \n    global doc_term_matrix\n    \n    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n    dictionary = corpora.Dictionary(doc_clean)\n    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n    # generate LDA model\n    return dictionary,doc_term_matrix","64b5f4dc":"def create_gensim_lda_model(doc_clean,number_of_topics,words):\n    \n    global lsamodel\n    \"\"\"\n    Input  : clean document, number of topics and number of words associated with each topic\n    Purpose: create LSA model using gensim\n    Output : return LSA model\n    \"\"\"\n    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n    # generate LSA model\n    #ldamodel = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n    return lsamodel","99bedd56":"number_of_topics=12\nwords=10\nmodel=create_gensim_lda_model(clean_text,number_of_topics,words)","c250f56a":"pprint(lsamodel.print_topics())","0b546d84":"optimal_model = lsamodel\nmodel_topics = optimal_model.show_topics(formatted=False)\npprint(optimal_model.print_topics(num_words=10))","e5536a60":"def format_topics_sentences(ldamodel=lsamodel, corpus=doc_term_matrix, texts=documents):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row in enumerate(ldamodel[corpus]):\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=doc_term_matrix, texts=documents)\n\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n\n# Show\ndf_dominant_topic.head(10)\n","84f1ffce":"df_dominant_topic.shape","efd39b60":"df_dominant_topic[13:14]","be5269b8":"# Group top 5 sentences under each topic\nsent_topics_sorteddf_mallet = pd.DataFrame()\n\nsent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n\nfor i, grp in sent_topics_outdf_grpd:\n    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n                                            axis=0)\n\n# Reset Index    \nsent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n\n# Format\nsent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n\n# Show\nsent_topics_sorteddf_mallet.head()\n","314079eb":"sent_topics_sorteddf_mallet.shape","7a060123":"elapsed_time = time.time() - start_time\ntime.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))","19a8b5d3":"type(model_topics)","e353da69":"model_topics","97c813eb":"df = pd.DataFrame(df_dominant_topic['Keywords'].unique())","8a4d71a8":"df","3b41645d":"# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n# create a random sample dataframe\ndf = df\n#df = df_topic_distribution\n\n# create a link to download the dataframe\ncreate_download_link(df)","b302a26b":"elapsed_time = time.time() - start_time\ntime.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))","88c9bf23":"#### LSA","90669e49":"#### NMF"}}