{"cell_type":{"ed55c73c":"code","16ad52f9":"code","193b58a3":"code","a5b1b7ab":"code","c10d8a9c":"code","40a1c77d":"code","3b8b9da7":"code","2c2c4f7e":"code","bfcadb9a":"code","2dc4d0a4":"code","58617fed":"code","da822b88":"code","6e88db72":"code","0a199fd9":"code","e33eab38":"code","4a1ca0d9":"code","4e11ea30":"code","54974c4e":"code","bca35438":"code","1b5363ca":"code","a7393fbb":"code","e75f41cd":"code","e00d6562":"code","2b7bd1a9":"code","7f0e3062":"code","0ad7eee2":"code","3ff4a164":"code","533c2a03":"code","6dca913a":"code","1285c58f":"code","e659d387":"code","dc7593de":"code","a6606f16":"code","90cd8c31":"code","66b83630":"code","5c89cfd2":"code","43316c64":"code","3e1cefdb":"code","c0c2d59f":"code","fce99947":"code","1e70a934":"code","e4b75073":"code","7e825d0f":"code","a982025c":"code","66eeea44":"code","d2f483b5":"code","d63fc552":"code","a57ce60c":"code","0486f1dc":"code","c83a6864":"code","20f1a0bd":"code","e733a728":"code","a67567c5":"code","cf2c319f":"code","94d49fd1":"code","772253f9":"code","a8bf31cc":"code","5780b532":"code","13d10b15":"code","9be846a4":"code","24a5d797":"code","a33727ce":"code","edbb067f":"code","5a3fce2d":"code","5b4658cd":"code","35ece315":"markdown","01ed1aa7":"markdown","e4791ee3":"markdown","40922630":"markdown","7308a5c7":"markdown","f0b3af87":"markdown","72419a5c":"markdown","b4d4cd3c":"markdown","2606c887":"markdown","f58332c9":"markdown","f710ec51":"markdown","3fde474c":"markdown","49c615f4":"markdown","023b8a9c":"markdown","b0e1dd41":"markdown","60327dd1":"markdown","3af4d7c3":"markdown","37ec32f0":"markdown","bbba02b3":"markdown","ab0b87e7":"markdown","51cbff9b":"markdown","1e6dec14":"markdown","64c41ca9":"markdown","33c66990":"markdown","1b69a38d":"markdown","880fa03b":"markdown","f79787e3":"markdown","4d132598":"markdown","b339f9ce":"markdown"},"source":{"ed55c73c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","16ad52f9":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split as tts, RandomizedSearchCV,StratifiedShuffleSplit\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, power_transform\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report, r2_score, precision_recall_curve, roc_auc_score, roc_curve, auc, f1_score, recall_score, precision_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.feature_selection import SelectKBest, RFE, RFECV, chi2   #chi2 aka. chi square is used when working with 2 categorical columns.\nfrom sklearn.decomposition import PCA\nfrom scipy import stats\nimport statsmodels.api as sm\nimport pprint\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","193b58a3":"missing_values = [\"n\/a\", \"na\", \"--\", \"NONE\", \"None\", \"none\", \"NA\", \"N\/A\",'inf','-inf']\ndata = pd.read_csv('..\/input\/mushroom-classification\/mushrooms.csv', na_values=missing_values)\ndata.head()","a5b1b7ab":"split = StratifiedShuffleSplit(n_splits = 1, random_state=42, test_size=0.2) #n_splits = 1, because I want to divide data into train and test sets\nfor train_index, test_index in split.split(data, data['class']):\n  stratified_train_data = data.loc[train_index]\n  stratified_test_data = data.loc[test_index]\n\nprint(stratified_train_data.shape , stratified_test_data.shape)","c10d8a9c":"stratified_test_data.drop(['class'],1,inplace=True)","40a1c77d":"le = LabelEncoder()\nstratified_train_data[[\"cap-shape\",'cap-surface','cap-color','bruises','odor','gill-attachment','gill-spacing','gill-size','gill-color','stalk-shape','stalk-root','stalk-surface-above-ring','stalk-surface-below-ring','stalk-color-above-ring','stalk-color-below-ring','veil-type','veil-color','ring-number','ring-type','spore-print-color','population','habitat']] = stratified_train_data[[\"cap-shape\",'cap-surface','cap-color','bruises','odor','gill-attachment','gill-spacing','gill-size','gill-color','stalk-shape','stalk-root','stalk-surface-above-ring','stalk-surface-below-ring','stalk-color-above-ring','stalk-color-below-ring','veil-type','veil-color','ring-number','ring-type','spore-print-color','population','habitat']].apply(le.fit_transform)","3b8b9da7":"stratified_train_data['class'] = stratified_train_data['class'].replace('p',0)\nstratified_train_data['class'] = stratified_train_data['class'].replace('e',1)","2c2c4f7e":"def correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if (corr_matrix.iloc[i, j]) > threshold:\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","bfcadb9a":"corr_features = correlation(stratified_train_data.iloc[:,:22], 0.6)\nprint(\"Number of correlated features are :\",len(set(corr_features)))\nprint(\"Independent correlated features are to be deleted are :\",corr_features)","2dc4d0a4":"stratified_train_data.head()","58617fed":"stratified_train_data = stratified_train_data[['cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n       'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',\n       'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n       'stalk-surface-below-ring', 'stalk-color-above-ring',\n       'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number',\n       'ring-type', 'spore-print-color', 'population', 'habitat','class']]","da822b88":"stratified_train_data.columns","6e88db72":"print('FEATURES CORRELATION TO TARGET VALUES :')\ntrain_data_corr = stratified_train_data[stratified_train_data.columns[1:]].corr()['class'][:]\nprint(train_data_corr)\nprint(\"=================================================\")\nprint('DELETING FETAURES THAT ARE LESS CORRELATED TO TARGET VARIABLES BETWEEN -0.1 & 0.1')\ntrain_data_corr.drop(train_data_corr[(train_data_corr.values > -0.1 ) & (train_data_corr.values < 0.1)].index, inplace=True)\nprint(train_data_corr)\nprint(\"=================================================\")\nprint(\"PRINTING THE DELETED COLUMN NAMES\")\nnew_train_data = stratified_train_data.columns[~stratified_train_data.columns.isin(train_data_corr.index)]\nprint(new_train_data)","0a199fd9":"#DELETING ALL THE UNWANTED COLUMNS AND ALSO DELETING THE 'veil-type' COLUMN AS IT IS USELESS FOR US\nstratified_train_data.drop(['cap-shape','cap-color','veil-type','veil-color', 'spore-print-color', 'ring-type'],1,inplace=True)","e33eab38":"X = stratified_train_data.drop('class',1)\ny = stratified_train_data['class']","4a1ca0d9":"select_K_Best = SelectKBest(k=9, score_func=chi2)\nselected_features = select_K_Best.fit(X, y)\nindices_selected = selected_features.get_support(indices=True)\ncolnames_selected = [X.columns[i] for i in indices_selected]\n\nX = X[colnames_selected]","4e11ea30":"print(\"The 9 most important features chosen by SelectKBest are : \\n {} \".format(X.columns))","54974c4e":"def calc_vif(X):\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n    return(vif)","bca35438":"calc_vif(X)","1b5363ca":"X['stalk-surface'] = X['stalk-surface-above-ring'] + X['stalk-surface-below-ring']\nX['stalk-surface']","a7393fbb":"X.head()","e75f41cd":"X.drop(['stalk-surface-above-ring','stalk-surface-below-ring'],1,inplace= True)","e00d6562":"calc_vif(X)","2b7bd1a9":"skewness_of_df = pd.DataFrame(X.skew())\nskewness_of_df","7f0e3062":"def boxcox_transformation(df,column):\n  try:\n    for column in df:\n      if ((df[column].skew() > 1.0) or (df[column].skew() < -1.0).any()):\n        plt.figure(figsize=(15,6))\n        plt.subplot(1, 2, 1)\n        df[column].hist()\n\n        plt.subplot(1, 2, 2)\n        stats.probplot(df[column], dist=\"norm\", plot=plt)\n        print(df[column].skew())\n \n        df[column], params = stats.boxcox(df[column]+1)\n\n        plt.figure(figsize=(15,6))\n        plt.subplot(2, 2, 1)\n        df[column].hist()\n\n        plt.subplot(2, 2, 2)\n        stats.probplot(df[column], dist=\"norm\", plot=plt)\n        print(data[column].skew())\n\n        return boxcox_transformation\n  except TypeError:\n       print(\"\")\n","0ad7eee2":"column = ['bruises','gill-spacing','gill-size','gill-color','stalk-root','population','habitat','stalk-surface']\nboxcox_transformation(X,column)","3ff4a164":"pca = PCA(n_components = 7,random_state=42)\ntransformed_data = pca.fit_transform(X)\nX = pd.DataFrame(data = transformed_data, columns = ['PCA1','PCA2','PCA3','PCA4','PCA5','PCA6','PCA7'])\nX","533c2a03":"print(pca.explained_variance_ratio_)\nprint(pca.noise_variance_)","6dca913a":"X_train, X_test, y_train, y_test = tts(X, y, random_state=42,test_size=0.3)","1285c58f":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","e659d387":"def evaluate(model, X_test, y_test, X_train, y_train):\n    y_pred = model.predict(X_test)\n    errors = abs(y_pred - y_test)\n    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n    print(classification_report(y_test,y_pred))\n    print(confusion_matrix(y_test,y_pred))\n    print('Recall Score = ',recall_score(y_test, y_pred))\n    print('Precision Score = ',precision_score(y_test, y_pred))\n\n    return evaluate","dc7593de":"def train_auc_roc_curve(model, X_test, y_test, X_train, y_train):\n  y_pred = model.predict(X_test)\n  print(\"roc curve :\",roc_curve(y_test,y_pred))\n  base_fpr,base_tpr,base_threshold = roc_curve(y_train, model.predict(X_train))\n  plt.plot([0,1])\n  plt.plot(base_fpr,base_tpr)\n  print(\"auc score :\",auc(base_fpr,base_tpr))\n\n  return train_auc_roc_curve","a6606f16":"def test_auc_ruc_curve(model, X_test, y_test):\n  test_fpr, test_tpr, test_threshold = roc_curve(y_test,model.predict(X_test))\n  test_auc = auc(test_fpr, test_tpr)\n  print(test_auc)\n  plt.plot([0,1])\n  plt.plot(test_fpr, test_tpr)\n\n  return test_auc_ruc_curve","90cd8c31":"default_logistic_model = LogisticRegression(random_state = 1)\ndefault_logistic_model.fit(X_train, y_train)\nbase_accuracy = evaluate(default_logistic_model, X_test, y_test, X_train, y_train)","66b83630":"train_auc_roc_curve(default_logistic_model, X_test, y_test, X_train, y_train)","5c89cfd2":"logistic = LogisticRegression(random_state=42)","43316c64":"random_parameters = ({'C' : [0.001, 0.01, 0.1, 1.0],\n                      'penalty' : ['l2'],\n                      'solver' : ['lbfgs', 'newton-cg', 'saga'],\n                      'max_iter' : [300,400,500,600,700,900,1000]})\n\nrandom_search_logistic = RandomizedSearchCV(logistic, param_distributions= random_parameters, n_iter=60, cv=5)\nrandom_search_logistic.fit(X,y)","3e1cefdb":"print(random_search_logistic.best_estimator_)\nprint(random_search_logistic.best_params_)","c0c2d59f":"logistic = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=600,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=42, solver='newton-cg', tol=0.0001, verbose=0,\n                   warm_start=False)\nlogistic.fit(X, y)","fce99947":"evaluate(logistic, X_test, y_test, X_train, y_train)","1e70a934":"train_auc_roc_curve(logistic, X_test, y_test, X_train, y_train)","e4b75073":"default_decision_tree_model = DecisionTreeClassifier(random_state=42)\ndefault_decision_tree_model.fit(X_train, y_train)\nbase_accuracy = evaluate(default_decision_tree_model, X_test, y_test, X_train, y_train)","7e825d0f":"train_auc_roc_curve(default_decision_tree_model, X_test, y_test, X_train, y_train)","a982025c":"dtree_classifier = DecisionTreeClassifier(random_state=42)","66eeea44":"path = dtree_classifier.cost_complexity_pruning_path(X_train, y_train)\nalphas = path['ccp_alphas']\nalphas","d2f483b5":"random_dtree_parameters = ({'ccp_alpha' : alphas,\n                             'criterion' : ['gini','entropy'],\n                            'splitter' : ['best','random'],\n                            'max_depth' : [8,10,12,15,20,24,32],\n                            'min_samples_leaf' : [2,3,5],\n                            'max_features' : ['auto', 'sqrt', 'log2']})\n\nrandom_search_dtree = RandomizedSearchCV(dtree_classifier, param_distributions= random_dtree_parameters, n_iter=60, cv=5)\nrandom_search_dtree.fit(X,y)","d63fc552":"print(random_search_dtree.best_estimator_)\nprint(random_search_dtree.best_params_)","a57ce60c":"dtree_classifier = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n                       max_depth=20, max_features='auto', max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=3, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, presort='deprecated',\n                       random_state=42, splitter='best')\ndtree_classifier.fit(X,y)","0486f1dc":"evaluate(dtree_classifier, X_test, y_test, X_train, y_train)","c83a6864":"train_auc_roc_curve(dtree_classifier, X_test, y_test, X_train, y_train)","20f1a0bd":"stratified_test_data.head()","e733a728":"le = LabelEncoder()\nstratified_test_data[[\"cap-shape\",'cap-surface','cap-color','bruises','odor','gill-attachment','gill-spacing','gill-size','gill-color','stalk-shape','stalk-root','stalk-surface-above-ring','stalk-surface-below-ring','stalk-color-above-ring','stalk-color-below-ring','veil-type','veil-color','ring-number','ring-type','spore-print-color','population','habitat']] = stratified_test_data[[\"cap-shape\",'cap-surface','cap-color','bruises','odor','gill-attachment','gill-spacing','gill-size','gill-color','stalk-shape','stalk-root','stalk-surface-above-ring','stalk-surface-below-ring','stalk-color-above-ring','stalk-color-below-ring','veil-type','veil-color','ring-number','ring-type','spore-print-color','population','habitat']].apply(le.fit_transform)","a67567c5":"stratified_test_data.drop(['cap-shape','cap-color','veil-type','veil-color', 'spore-print-color', 'ring-type'],1,inplace=True)","cf2c319f":"stratified_test_data.head()","94d49fd1":"print(stratified_test_data.columns)","772253f9":"stratified_test_data['stalk-surface'] = stratified_test_data['stalk-surface-above-ring'] + stratified_test_data['stalk-surface-below-ring']\nstratified_test_data['stalk-surface']","a8bf31cc":"stratified_test_data.drop(['stalk-color-above-ring', 'stalk-color-below-ring','stalk-surface-above-ring','stalk-surface-below-ring','cap-surface','odor', 'gill-attachment', 'stalk-shape', 'ring-number'],1,inplace=True)","5780b532":"print(\"Remaining Columns are \\n: {}\". format(stratified_test_data.columns))","13d10b15":"column = ['bruises','gill-spacing','gill-size','gill-color','stalk-root','population','habitat','stalk-surface']\nboxcox_transformation(stratified_test_data,column)","9be846a4":"test_pca = PCA(n_components = 7,random_state=42)\ntransformed_test_data = test_pca.fit_transform(stratified_test_data)\ntest_x = pd.DataFrame(data = transformed_test_data, columns = ['PCA1','PCA2','PCA3','PCA4','PCA5','PCA6','PCA7'])\ntest_x","24a5d797":"sc = StandardScaler()\ntest_x = sc.fit_transform(test_x)\nprint(test_x)","a33727ce":"test_auc_ruc_curve(default_logistic_model, X_test, y_test)","edbb067f":"test_auc_ruc_curve(logistic, X_test, y_test)","5a3fce2d":"test_auc_ruc_curve(default_decision_tree_model, X_test, y_test)","5b4658cd":"test_auc_ruc_curve(dtree_classifier, X_test, y_test)","35ece315":"**The explained_variance_ratio gives the variance of every column. In our case the remaining columns are 7 as passed in n_components = 7.**\n\n**The noise_variance gives the overall noise in the data**","01ed1aa7":"**Stratified Shuffle split will not create sampling bias as it will choose some samples from all the target classes.**","e4791ee3":"### Testing Tuned Decision Tree ","40922630":"**Label Encoding the data as ML model does not accept non-numerical values**","7308a5c7":"**cost_complexity_pruning_path helps to find different ccp values which will be later used in random search for choosing the best one**\n","f0b3af87":"# Feature Selection","72419a5c":"**Models like linear regression and logistic regression assume that data follows gaussian distribution so to use that we transform skewed data to normal distributed \ndata. But SVM, Neural Network, Tree based and boosting does not require to transform data.**","b4d4cd3c":"## Applying Classification models","2606c887":"**Importing Data**","f58332c9":"**Applying Principal Component Analysis (PCA), this helps to handle multicollinearity in data as column stalk-surface has high multicollinearity.**","f710ec51":"### Testing Tuned Logistic Regression","3fde474c":"**DELETING ALL THE UNWANTED COLUMNS AND ALSO DELETING THE 'veil-type' COLUMN AS IT IS USELESS FOR US**","49c615f4":"### Using SelectKBest library of sklearn to select features that are most important using chi-square hypothesis testing.\n\n\n> chi sqaure hypothesis testing uses two columns for working. Here Two columns will be feature and another is target.\n\n\n> This will be done iteratively with every feature until the k value of SelectKBest is reached.\n\n\n\n","023b8a9c":"## Default Logistic Regression","b0e1dd41":"## Tuned Logistic Regression ","60327dd1":"### Testing Default Logistic Regression","3af4d7c3":"### Default Decision tree classifier","37ec32f0":"### Tuned Decision Tree Classifier","bbba02b3":"### Default Decision Tree Classifier","ab0b87e7":"**My Conclusion\n\n**Models applied :\n\n** 1.Logistic Regression (Default and Tuned)**\n\n   2.Decision Tree Classifier (Default and Tuned)**\n* Default models run better than tuned models","51cbff9b":"# Using Stratified Test data","1e6dec14":"**Creating a Pearson Correlation function for deleting column with above threshold value. In this case I have chosen 0.6 or 60%**","64c41ca9":"## Logistic Regression","33c66990":"## Decision Tree Classifier ","1b69a38d":"## Calculating Multicollinearity in data\n\n* Using Variance Inflation Factor to check the multicollinearity or dependency in independent columns","880fa03b":"## Pricipal Component Analysis","f79787e3":"**Using Stratified Train Data**","4d132598":"**Creating Train and Test sets using Stratified Shuffle Split**\n","b339f9ce":"**Deleting columns which have feature to target correlation between -0.1 to 0.1 as they are very less correlated to target**"}}