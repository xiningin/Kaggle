{"cell_type":{"9efa3624":"code","c3a75215":"code","b813b840":"code","044801a1":"code","ec0ae907":"code","e9735706":"code","1fb75950":"code","5b0ebcc3":"code","bfc2a329":"code","ed4e30a0":"code","299eb984":"code","c46203ff":"code","ddbb3452":"code","9559bb91":"code","a919ab7f":"code","e70fde42":"code","a273eb2f":"code","34f09ade":"code","ab34c53e":"code","5d7e16b3":"code","2a411066":"code","5e40ec6c":"code","a4ad7c69":"code","830c0955":"code","63b7be31":"code","4a4d659a":"code","6c6f2d3c":"markdown","930ad154":"markdown","5aac2dd2":"markdown","96121938":"markdown","15b2a68b":"markdown","00ecc19f":"markdown","a1e046ef":"markdown","a5266ec0":"markdown","b1fd01f9":"markdown","e92ebb5d":"markdown","14ba4199":"markdown","c6c5677b":"markdown","2596da3f":"markdown","c20a1fca":"markdown","095b5956":"markdown","0703a5d9":"markdown","c0f63c89":"markdown","92df880a":"markdown","b8ae2fbc":"markdown","e0b19230":"markdown","4955e79a":"markdown","0c9e9d1d":"markdown","003f24d4":"markdown","c87fbe18":"markdown"},"source":{"9efa3624":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","c3a75215":"import pandas as pd \npd.set_option('display.max_rows',None)\n\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split","b813b840":"data = pd.read_csv('..\/input\/headbrain\/headbrain.csv')\ndata.head()","044801a1":"x = data['Head Size(cm^3)']\ny = data['Brain Weight(grams)']\nn = len(y)","ec0ae907":"train_x, test_x, train_y, test_y = train_test_split(x,y, test_size=0.3)\ntrain_x.shape , train_y.shape","e9735706":"plt.scatter(train_x, train_y)\nplt.xlabel('Head Size(cm^3)')\nplt.ylabel('Brain Weight(grams)')\nplt.show()","1fb75950":"mean_x = np.mean(train_x)\nmean_y = np.mean(train_y)\nnum = 0\ndenom = 0\n\n# for i in range(n):\n#     num = num + (x[i] - mean_x)*(y[i] - mean_y)\n#     denom = denom + ((x[i] - mean_x))**2\n    \nnum = np.dot(np.subtract(train_x,mean_x), np.subtract(train_y,mean_y))\ndenom = np.dot(np.subtract(train_x,mean_x), np.subtract(train_x,mean_x))\n\nm = num\/denom\nc = mean_y - (m*mean_x)\nprint(m,c)","5b0ebcc3":"min_x = np.min(train_x)-100\nmax_x = np.max(train_x)+100\nx_dummy = np.linspace(min_x,max_x,1000)\ny_dummy = m * x_dummy + c\n\nplt.scatter(train_x,train_y,color='g')\nplt.plot(x_dummy,y_dummy,color='r')\nplt.title('Simple Linear Regression')\nplt.xlabel('Head size cm^3')\nplt.ylabel('Brain weight in grams')","bfc2a329":"sum_pred = 0\nsum_act = 0\n\nfor xi,yi in zip(train_x, train_y):\n    y_pred = (m * xi + c)\n    sum_pred += (y_pred - mean_y)**2\n    sum_act += (yi - mean_y)**2\n\n# r2 = 1-(sum_pred\/sum_act)\nr2 = sum_pred\/sum_act\nprint(r2)\n\n# Here we can observe that we got R**2> 0.5 . so we have good model","ed4e30a0":"def predict(x):\n    return m*x+c\n\nprint(predict(4177))","299eb984":"x = data['Head Size(cm^3)'].values\ny = data['Brain Weight(grams)'].values\nn = len(y)\n\nx = x.reshape((len(x),1)) # Converting into 2d array\ny = y.reshape((len(y),1))\n\ntrain_x, test_x, train_y, test_y = train_test_split(x,y, test_size=0.3)\n\ntrain_x.shape , train_y.shape","c46203ff":"from sklearn import linear_model\n\nreg = linear_model.LinearRegression(normalize=True)\nreg.fit(train_x, train_y) # accepts 2d array\n","ddbb3452":"y_predict = reg.predict(test_x)","9559bb91":"df = pd.DataFrame({'Actual': test_y.flatten(), 'Predicted': y_predict.flatten()})\ndf.head()","a919ab7f":"plt.scatter(test_x,test_y,color='g')\nplt.plot(test_x,y_predict,color='r')\nplt.title('Simple Linear Regression')\nplt.xlabel('Head size cm^3')\nplt.ylabel('Brain weight in grams')","e70fde42":"from sklearn.metrics import mean_absolute_error\n\naccuracy2 = mean_absolute_error(test_y, y_predict)\naccuracy2","a273eb2f":"from sklearn.metrics import r2_score\n\naccuracy = r2_score(test_y, y_predict)\nprint(accuracy)\n\nweights = reg.coef_\nintercept = reg.intercept_\nprint(weights, intercept)","34f09ade":"x = data['Head Size(cm^3)']\ny = data['Brain Weight(grams)']\nx.shape, y.shape","ab34c53e":"def gradient_descent(x, y, m, c, alpha, iterations, n):\n\n    # Performing Gradient Descent \n    for i in range(iterations): \n        y_guess = m*x + c  # The current predicted value of Y\n        cost = 1\/n * np.sum((y - y_guess)**2) # Cost function to check convergence of theta\n        D_m = (-2\/n) * np.sum(x * (y- y_guess))  # Derivative wrt m\n        D_c = (-2\/n) * np.sum(y - y_guess)  # Derivative wrt c\n        m = m - alpha * D_m  # Update m\n        c = c - alpha * D_c  # Update c\n        costs.append(cost)\n    return m,c, costs","5d7e16b3":"n = len(x)\nm = 0\nc = 0\ncosts = []\nalpha = 0.000000009 # The learning Rate\niterations = 30 # The number of iterations to perform gradient descent\n\nm,c, costs = gradient_descent(x, y, m, c, alpha, iterations, n)","2a411066":"plt.plot(costs)\nplt.ylabel('cost')\nplt.xlabel('iterations (per hundreds)')\nplt.title('Cost reduction over time')\nplt.show()","5e40ec6c":"\ny_guess = m*x+c\n\nplt.scatter(x,y)\nplt.xlabel('Head Size(cm^3)')\nplt.ylabel('Brain Weight(grams)')\nplt.plot([min(x), max(x)], [min(y), max(y)], color='red')\nplt.show()","a4ad7c69":"from sklearn.metrics import r2_score\n\naccuracy = r2_score(y, y_guess)\nprint(accuracy)","830c0955":"def predict(x_):\n    return m*x_+c","63b7be31":"print(predict(4747))","4a4d659a":"data[data['Head Size(cm^3)'] == 4747]","6c6f2d3c":"# **Calculating R Square**","930ad154":"**Collecting Data**","5aac2dd2":"**It shows, that we have learning rate neighter too small nor too large, and after some iterations, m and c are constant.**","96121938":"# Calculating Mean Absolute Error","15b2a68b":"# Visualizing the Output","00ecc19f":"# Importing Libraries","a1e046ef":"# **Predicting for testing dataset**","a5266ec0":"#  Importing the data","b1fd01f9":"# Debugging Theta","e92ebb5d":"# Calculating R Square","14ba4199":"**Visualizing test dataset**","c6c5677b":"R Square > 0.5, thus we can go with our model","2596da3f":"# Comparing predicted and guessed value","c20a1fca":"# Segregating variables: Independent and Dependent variables","095b5956":"# Calculating R Square","0703a5d9":"**Segregating variables: Independent and Dependent variables**","c0f63c89":"# Calculating m and c manually","92df880a":"# **Implementing Linear Regression - Library**","b8ae2fbc":"# Splitting the data into train set and test set","e0b19230":"# Building model for training dataset","4955e79a":"# **Calculating Linear Regression Using Gradient Descent**","0c9e9d1d":"# Predicting Output","003f24d4":"# Creating dummy dataset","c87fbe18":"# Implementing Linear Regression -Manual"}}