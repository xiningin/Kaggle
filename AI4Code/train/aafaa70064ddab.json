{"cell_type":{"aa832260":"code","660ba82a":"code","2f00023e":"code","695ce227":"code","f9e61603":"code","62014048":"code","9bc022d9":"code","1ab3a900":"code","585a38ad":"code","3bd2fb1e":"code","bcdaad5a":"code","adb7cc32":"code","ed1a1063":"code","ba5d906b":"code","13e3af21":"code","4490d6e4":"code","a3769dd1":"code","5007cd3f":"code","f7f2e510":"code","262bb08c":"code","1cda1964":"code","135938cf":"code","9400dc3b":"code","5dcfa869":"code","2d18f2a1":"code","a0d06275":"code","c6fa0b4a":"code","86c127a3":"code","124b9ee1":"code","5fa08edb":"code","2e467d6a":"code","6384c49f":"code","e665c159":"code","1a71ad75":"code","a05d794f":"code","438bdbb3":"code","740d074f":"code","95b2f232":"code","1839d232":"code","5c952936":"markdown","ba767056":"markdown","26a2d724":"markdown","877db39c":"markdown","87aa5791":"markdown","a1478301":"markdown","abb926d1":"markdown","a13c1e4a":"markdown","361f4d8a":"markdown"},"source":{"aa832260":"import numpy as np # linear algebra\nimport pandas as pd # data processing\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","660ba82a":"import numpy as np \nimport matplotlib.pyplot as plt\nimport pandas as pd","2f00023e":"data=pd.read_csv('..\/input\/titanic\/train.csv')","695ce227":"data.head()","f9e61603":"data.info()","62014048":"data.columns","9bc022d9":"cols_to_drop= ['PassengerId','Name','Ticket','Cabin','Embarked']","1ab3a900":"clean_data=data.drop(cols_to_drop,axis=1)","585a38ad":"clean_data.head()","3bd2fb1e":"from sklearn.preprocessing import LabelEncoder","bcdaad5a":"le=LabelEncoder()","adb7cc32":"clean_data[\"Sex\"]=le.fit_transform(clean_data[\"Sex\"])\nclean_data.head()","ed1a1063":"clean_data=clean_data.fillna(clean_data['Age'].mean())\n","ba5d906b":"clean_data.info()","13e3af21":"input_cols=['Pclass','Sex','Age','SibSp','Parch','Fare']\nout_cols=['Survived']\n\nX=clean_data[input_cols]\ny=clean_data[out_cols]\n\nX.shape,y.shape","4490d6e4":"def entropy(col):\n    data,counts =np.unique(col, return_counts=True)\n    ## total items are also needed to find the prob\n    N= float(col.shape[0])\n    \n    ent = 0.0\n    \n    for count in counts:\n        p = count \/ N               ## predicting the Probability\n        ent += p* np.log2(p)\n        \n    return -ent\n    ","a3769dd1":"col = np.array([4,4,3,3,2,2,1,2,2])\nentropy(col)","5007cd3f":"def divide_data(x_data, fkey, fval):\n    x_right = pd.DataFrame([], columns=x_data.columns)\n    x_left = pd.DataFrame([], columns=x_data.columns)\n    \n    for xi in range(x_data.shape[0]):\n        val = x_data[fkey].iloc[xi]\n        \n        if val > fval:\n            x_right = x_right.append(x_data.loc[xi])\n        else:\n            x_left = x_left.append(x_data.loc[xi])\n            \n    return x_left,x_right","f7f2e510":"# We are making a Binary Tree ,henc spilt node into 2.\n# If rain will come or not ,Lets say split this across two probabilities. fkey=Probabilities\n# we want to split like prob < 0.5(will come) &prob > 0.5(will not come): fval=0.5\n\ndef information_gain(x_data, fkey, fval):\n    left, right = divide_data(x_data, fkey, fval)\n    \n    # %age of examples  in left and right\n    l = float(left.shape[0]) \/ x_data.shape[0]\n    r = float(right.shape[0]) \/ x_data.shape[0]\n    \n    hs = entropy(x_data.Survived)\n    \n    igain = hs- (l * entropy(left.Survived) + r * entropy(right.Survived))\n    return igain\n    ","262bb08c":"for f in X.columns:\n    print(f)\n    print(information_gain(clean_data ,f, clean_data[f].mean()))","1cda1964":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split","135938cf":"sk_tree = DecisionTreeClassifier(criterion='entropy', max_depth=5)","9400dc3b":"X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.33, random_state=42)","5dcfa869":"sk_tree.fit(X_train, y_train)","2d18f2a1":"sk_tree.score(X_test, y_test)","a0d06275":"sk_tree.predict(X_test[:10])\n","c6fa0b4a":"y_test[:10]","86c127a3":"class DecisionTree:\n    \n    def __init__(self, depth=0, max_depth = 5):\n        self.left = None\n        self.right = None\n        self.fkey = None\n        self.fval = None\n        self.max_depth = max_depth \n        self.depth = depth\n        self.target = None\n## when I am going to predict at partcular Node , say leaf Node has 50 examples and it has 40 True\n## and 10 false then target of this node is False(80%)\n\n    def fit(self, X_train):\n        features = ['Pclass','Sex','Age','SibSp','Parch','Fare']\n        info_gains = []\n        \n        for ix in features:\n            # calculating the info gain for each feature\n            i_gain = information_gain(X_train, ix, X_train[ix].mean())\n            info_gains.append(i_gain)\n   \n        #taking the feature with max IG\n        self.fkey = features[np.argmax(info_gains)]\n        self.fval = X_train[self.fkey].mean()\n          ##print(\"Choosing the Feature:\" self.fkey)\n    \n    \n        # create the data\n        # split the data\n        data_left , data_right = divide_data(X_train, self.fkey, self.fval)\n    \n        # reset_index with reset the index again from starting for each Subpart\n        data_left = data_left.reset_index(drop=True)\n        data_right = data_right.reset_index(drop=True)\n    \n        # reached the Leaf Node    \n        if data_left.shape[0] == 0 or data_right.shape[0] == 0:\n            if X_train.Survived.mean() >= 0.5:\n                self.target = \"Survived\"\n            else:\n                self.target =  \"Dead\"\n            return\n    \n        ## Stop early when depth >= max-depth\n        if self.depth >= self.max_depth:\n            if X_train.survived.mean():\n                self.target = \"Survived\"\n            else:\n                self.target = \"Dead\"\n            return\n    \n        ## Recursion\n        self.left = DecisionTree(depth=self.depth + 1)\n        self.left.fit(data_left)\n    \n        self.right = DecisionTree(depth=self.depth + 1)\n        self.right.fit(data_left)\n                \n        \n    \n    def predict(self, test):\n        if test[self.fkey] > self.fval:\n            # go to right\n            # base case\n            if self.right is None:\n                return self.target\n            # Recursive Case\n            return self.right.predict(test)\n        else:\n            if self.left is None:\n                return self.target\n            return self.left.predict(test)\n","124b9ee1":"dt = DecisionTree()","5fa08edb":"## splitting the data into train and test and then predicting on test data\n\nsplit = int(0.7*clean_data.shape[0])\ntrain_data = clean_data[:split]\ntest_data = clean_data[split:]\ntest_data = test_data.reset_index(drop=True)","2e467d6a":"dt.fit(train_data)","6384c49f":"y_pred = []\nfor i in range(test_data.shape[0]):\n    y_pred.append(dt.predict(test_data.loc[i]))","e665c159":"y_pred[:10]","1a71ad75":"y_actual = test_data[out_cols]","a05d794f":"y_actual[:10]","438bdbb3":"data[split:][:10]","740d074f":"!pip install --upgrade scikit-learn==0.20.3","95b2f232":"import pydotplus\nfrom sklearn.externals.six import StringIO \nfrom IPython.display import Image\nfrom sklearn.tree import export_graphviz           ## will show me the graph\nimport graphviz","1839d232":"dot_data = StringIO()\nexport_graphviz(sk_tree, out_file=dot_data, filled=True, rounded=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\nImage(graph.create_png())","5c952936":"# Information Gain","ba767056":"### So Decision Tree is visualized for Sklearn tree and its giving the entropy, number of samples,and range of values.","26a2d724":"# Decision Tree using Sklearn","877db39c":"# Entropy","87aa5791":"# Custom Implementation","a1478301":"# Visualising DT","abb926d1":"# Thanks for visiting the kernel.Hope you like it.Please show an upvote of Motivation if you liked it.","a13c1e4a":"### So we can see that Pclass has Information Gain of 0.075 and Sex has I.G of 0.21 which is highest among all.\nThat means Gender Column has highest priority when comes the question of Surviving .So this is the Root.","361f4d8a":"So Our Decision Tree model is predicting Our first person not Survived while it is Survived. Similarly for others."}}