{"cell_type":{"f894f9e0":"code","5b4e9092":"code","e773577f":"code","8d2e7329":"markdown","0176cc9f":"markdown"},"source":{"f894f9e0":"import numpy as np\nimport os\nimport PIL\nimport PIL.Image\nimport pandas as pd\nimport pathlib\nimport glob\nfrom PIL import Image\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(11.7,8.27)})\npalette = sns.color_palette(\"bright\", 28)\n\ndef preprocess_image_data_to_df(classes_path):\n    data = []\n    target = []\n    i = 0\n    for entry in glob.iglob(classes_path):\n        for image in glob.iglob(entry+'\/*.jpg'):\n            img = Image.open(image)\n            img = img.resize((8, 8))\n            #convert to greyscale\n            img = img.convert('L')\n            img = np.array(img)\n            #255 white turns into black\n            img = 255 - img\n            #scaling\n            img = img\/10\n            #turn image 8x8 into row vector\n            img = img.ravel()\n            img = img.tolist()\n            #create dataset one row per cycle\n            data.append(img)\n            target.append(i)\n        #creating the lables as integers\n        i = i+1\n    data = pd.DataFrame(data)\n    #creating new column from the target list\n    data[\"target\"] = target\n    #shuffeling the data\n    data = data.sample(frac=1)\n    print(data)\n    return data\n                            \n","5b4e9092":"def svm_model(X_train, X_test, y_train, y_test):\n    from sklearn import svm\n    svm_classifier = svm.SVC(kernel='rbf',gamma=0.001,C=5)\n    svm_classifier.fit(X_train, y_train)\n    predicted = svm_classifier.predict(X_test)\n    accuracy = (len(X_test[predicted==y_test])\/len(X_test))*100\n    return  accuracy\n\ndef gaussian_naive_bayes(X_train, X_test, y_train, y_test):\n    from sklearn.naive_bayes import GaussianNB\n    GNB_classifier = GaussianNB()\n    GNB_classifier.fit(X_train, y_train)\n    predicted = GNB_classifier.predict(X_test)\n    accuracy = (len(X_test[predicted == y_test]) \/ len(X_test)) * 100\n    return accuracy\n\ndef decision_tree(X_train, X_test, y_train, y_test):\n    from sklearn import tree\n    dt_classifier = tree.DecisionTreeClassifier()\n    dt_classifier.fit(X_train, y_train)\n    predicted = dt_classifier.predict(X_test)\n    accuracy = (len(X_test[predicted == y_test]) \/ len(X_test)) * 100\n    return accuracy\n\ndef random_forest(X_train, X_test, y_train, y_test):\n    from sklearn.ensemble import RandomForestClassifier\n    RF_classifier = RandomForestClassifier(max_depth=2, random_state=0)\n    RF_classifier.fit(X_train, y_train)\n    predicted = RF_classifier.predict(X_test)\n    accuracy = (len(X_test[predicted == y_test]) \/ len(X_test)) * 100\n    return accuracy\n\ndef k_nearest_neighbors(X_train, X_test, y_train, y_test):\n    from sklearn.neighbors import KNeighborsClassifier\n    KNN_classifier = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n    KNN_classifier.fit(X_train, y_train)\n    predicted = KNN_classifier.predict(X_test)\n    accuracy = (len(X_test[predicted == y_test]) \/ len(X_test)) * 100\n    return accuracy\n\ndef stochastic_gradient_decend(X_train, X_test, y_train, y_test):\n    from sklearn.linear_model import SGDClassifier\n    sgd_classifier = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\n    sgd_classifier.fit(X_train, y_train)\n    predicted = sgd_classifier.predict(X_test)\n    accuracy = (len(X_test[predicted == y_test]) \/ len(X_test)) * 100\n    return accuracy\n\ndef xgboost(X_train, X_test, y_train, y_test):\n    from xgboost import XGBClassifier\n    model = XGBClassifier()\n    model.fit(X_train, y_train)\n    # make predictions for test data\n    predicted = model.predict(X_test)\n    accuracy = (len(X_test[predicted == y_test]) \/ len(X_test)) * 100\n    return accuracy\n\ndef using_tsne(X_train,y_train):\n\n    tsne = TSNE()\n    X_embedded = tsne.fit_transform(X_train)\n    sns.scatterplot(X_embedded[:, 0], X_embedded[:, 1], hue=y_train, legend='full', palette=palette)\n    plt.show()","e773577f":"if __name__ == \"__main__\":\n    classes_path = \"\/kaggle\/input\/handwrittenmathsymbols\/extracted_images\/*\"\n    pr = preprocess_image_data_to_df(classes_path)\n    pr = pr.sample(frac=1)\n    data = pr.iloc[:,:-1]\n    target = pr.iloc[:,-1]\n    X_train, X_test, y_train, y_test = train_test_split(pr.iloc[:,:-1], pr.iloc[:,-1], test_size=0.3, shuffle=False)\n\n\n    svm_model_accuracy = svm_model(X_train, X_test, y_train, y_test)\n    print(\"svm accuracy:\" + str(svm_model_accuracy))\n    \n    gaussian_naive_bayes_accuracy = gaussian_naive_bayes(X_train, X_test, y_train, y_test)\n    print(\"gaussian naive bayes accuracy:\" + str(gaussian_naive_bayes_accuracy))\n\n    decision_tree_accuracy = decision_tree(X_train, X_test, y_train, y_test)\n    print(\"decision tree accuracy:\" + str(decision_tree_accuracy))\n\n    random_forest_accuracy = random_forest(X_train, X_test, y_train, y_test)\n    print(\"random forest accuracy:\" + str(random_forest_accuracy))\n\n    knn_accuracy = k_nearest_neighbors(X_train, X_test, y_train, y_test)\n    print(\"knn accuracy:\"+ str(knn_accuracy))\n\n    sgd = stochastic_gradient_decend(X_train, X_test, y_train, y_test)\n    print(\"stochastic gradient decent accuracy:\" + str(sgd))\n\n    XGboost = xgboost(X_train, X_test, y_train, y_test)\n    print(\"xgboost accuracy:\"+ str(XGboost))","8d2e7329":"Preprocessing the image data:","0176cc9f":"Functions defining various machine learning models:"}}