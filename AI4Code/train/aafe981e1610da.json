{"cell_type":{"e15c4ed7":"code","a8c50834":"code","df41aad4":"code","c447ebe4":"code","11f0f9ed":"code","3833853e":"code","a165eca7":"code","55a215dc":"code","4e3f5026":"code","8705994d":"code","105baa8b":"code","f204f270":"code","99f5d39d":"code","3057acc3":"code","cd4c7adf":"code","99bf6e47":"code","feb41164":"code","e8142516":"code","dd30612c":"code","a76d00e6":"code","4bcd514b":"code","402c1440":"code","b4af7403":"code","648595c3":"code","0915e234":"code","2f4f678e":"code","3d5aa29e":"code","b38a8c8f":"markdown","fedcf5d6":"markdown","71ddf27d":"markdown","43c6d6bf":"markdown","cb8b2aff":"markdown","37de7dc7":"markdown","e08877b9":"markdown","d1afd28c":"markdown","83f9a546":"markdown","ea0ed707":"markdown","6ab7864e":"markdown","eb4a0e58":"markdown"},"source":{"e15c4ed7":"import pandas as pd\nimport numpy as np","a8c50834":"df = pd.read_csv(\"..\/input\/usp-pj01\/train_Iris.csv\")\ndf.sample(5) # vizualizando 5 linhas do data frame aleatoriamente","df41aad4":"df.shape # qtde de linhas e colunas do banco de dados","c447ebe4":"df['Species'].value_counts() # verificando a quantidade de observa\u00e7\u00f5es por classe\n# em quais casos balancear os dados?","11f0f9ed":"df.isna().sum() # verificando linhas com NaN\n# data.dropna() para retirar linhas com NaN","3833853e":"df.duplicated().sum()  # verificando valores duplicados","a165eca7":"df.describe() # verificando estat\u00edsticas do data frame","55a215dc":"import matplotlib.pyplot as plt\nimport seaborn as sns","4e3f5026":"df1 = df.drop('Id', axis = 1) # retirando a coluna id para n\u00e3o interferir no plot dos gr\u00e1ficos","8705994d":"df1.columns","105baa8b":"sns.pairplot(df1, hue = \"Species\")\nplt.show()","f204f270":"j=0\nplt.figure(figsize=(10, 10))\n\nfor i in df1.columns[:-1]:\n    j = j + 1\n    plt.subplot(2,2,j)\n    plt.hist(df1[i],density=True)\n    plt.title(i)","99f5d39d":"j = 0\nplt.figure(figsize=(16, 16))\n\nfor i in df1.columns[:-1]:\n    j = j + 1\n    plt.subplot(2, 2, j)\n    sns.boxplot(x = 'Species', y = df1[i], data = df1)","3057acc3":"from sklearn.model_selection import train_test_split\n\ndf = pd.read_csv(\"..\/input\/usp-pj01\/train_Iris.csv\") #carregando arquivo","cd4c7adf":"X = df.iloc[:, :-1] ### Features\/recursos ou vari\u00e1veis independentes\ny = df.iloc[:, -1] ### Labels\/r\u00f3tulos ou vari\u00e1veis dependentes\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2) ## divis\u00e3o de 30% para o conjunto teste","99bf6e47":"#treina o modelo usando o algoritmo \u00c1rvore de Decis\u00e3o do sklearn\n# nota: o modelo de decis\u00e3o de Sklearn, baseando em features, \u00e9 poss\u00edvel determinar qual \u00e9 o tipo de flor. \n\nfrom sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\n\nprev = dtc.predict(X_test)\n\nprev\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score\n\n# Pelo menos tr\u00eas m\u00e9tricas \nprint(\"M\u00e9tricas de avalia\u00e7\u00e3o \\n\")\nprint(\"Accuracy: %s\" % (accuracy_score(y_test, prev)))\nprint(\"F1 score: %s\" % (f1_score(y_test, prev, average = \"weighted\")))\nprint(\"Precission: %s\" % (precision_score(y_test, prev, average = \"weighted\")))","feb41164":"# Modelo \n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# dividir o data set em treino e teste \n# X: features \n# y: labels \n\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n\n\n# Normalizacao\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\n\nk = 5 # numero de vizinhos\n# ajusta o modelo k-vizinhos\nmodel = KNeighborsClassifier(n_neighbors=k, metric = 'euclidean')\nmodel.fit(X_train,y_train)\n# faz a predi\u00e7\u00e3o no conjunto de teste\ny_pred = model.predict(X_test)\n\n# Pelo menos tr\u00eas m\u00e9tricas \n\nprint(\"Accuracy: %s\" % (accuracy_score(y_test, y_pred)))\nprint(\"F1 score: %s\" % (f1_score(y_test, y_pred, average = \"weighted\")))\nprint(\"Precission: %s\" % (precision_score(y_test, y_pred, average = \"weighted\")))","e8142516":"df = pd.read_csv(\"..\/input\/usp-pj01\/train_Iris.csv\") # Passando o data frame para array numpy\ndf = df.to_numpy() # passando o data frame para array numpy\nnrol, ncol = df.shape\ny = df[:,-1]\nX = df[:,0:ncol-1]","dd30612c":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler().fit(X) # normalizando os dados\nX = scaler.transform(X)","a76d00e6":"# conjuntos de treinamento e teste\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 100)","4bcd514b":"from sklearn.naive_bayes import GaussianNB\n\n# ajustando o modelo\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)","402c1440":"# realizando predi\u00e7\u00e3o\ny_pred = model.predict(X_test) \n# calculando m\u00e9tricas\nprint(\"Accuracy: %s\" % (accuracy_score(y_pred, y_test)))\nprint(\"F1 score: %s\" % (f1_score(y_pred, y_test, average = \"weighted\")))\nprint(\"Precission: %s\" % (precision_score(y_pred, y_test, average = \"weighted\")))","b4af7403":"df = pd.read_csv(\"..\/input\/usp-pj01\/train_Iris.csv\")\nteste = pd.read_csv(\"..\/input\/usp-pj01\/test_Iris.csv\")\nteste.sample(5)","648595c3":"teste_id = teste.Id","0915e234":"y_train = df.iloc[:, -1]\nX_train = df.iloc[:, :-1]\n\nX_test = teste.copy()","2f4f678e":"model = DecisionTreeClassifier()\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)","3d5aa29e":"teste_id = pd.DataFrame(data = {\"Id\": teste_id, \"Category\": y_pred})\nteste_id.to_csv(\"envio.csv\", index = False)\n\nteste_id.head()","b38a8c8f":"Pelo fato do conjunto de dados n\u00e3o ser grande, n\u00e3o vamos retirar os outliers","fedcf5d6":"### Obtemos aproximadamente 97% de precis\u00e3o\/acur\u00e1cia neste processo de aprendizado de m\u00e1quina","71ddf27d":"## Divis\u00e3o do conjunto de dados em treino e teste","43c6d6bf":"No classificador Naive Bayes assumimos a normalidade dos dados","cb8b2aff":"## Grupo Sparkanos\n### Gabriel Victor de Jesus Lima - 10844226\n### Vitor Pinho Iecks Ponce - 10785968\n### Luiz Claudio Olivato - 3658437\n### Andrey In\u00e1cio Paulino - 10818741\n### Eduardo Almeida M Neiva - 10817027","37de7dc7":"## Vamos usar o classificador \u00e1rvore de decis\u00e3o para envio por ter apresentado uma melhor acur\u00e1cia","e08877b9":"## Treino do modelo com classificadores","d1afd28c":"### Classifica\u00e7\u00e3o ","83f9a546":"# Leitura dos dados e an\u00e1lise explorat\u00f3ria\n","ea0ed707":"#### Classificador K-vizinhos mais pr\u00f3ximos","6ab7864e":"#### Aqui notamos a redu\u00e7\u00e3o de precis\u00e3o\/acur\u00e1cia do modelo, com o processo de normaliza\u00e7\u00e3o dos dados, obtemos aproximadamente 94% de precis\u00e3o\/acur\u00e1cia.\n##### H\u00e1 oportunidade para mais estudo, testes e investiga\u00e7\u00e3o","eb4a0e58":"# Classificador Naive Bayes"}}