{"cell_type":{"8fccd967":"code","4fcb4d6a":"code","99bd3da9":"code","24ed8807":"code","876aa116":"code","560d6a02":"code","b43a77b2":"code","f41aeb00":"code","93edf803":"code","c4926a35":"code","9019fddf":"code","beba2710":"code","4f749e17":"code","c4ef2c77":"code","6d1f8eaa":"markdown","d1bb3513":"markdown","ead4a025":"markdown","796fba1d":"markdown","a105127c":"markdown","502881b6":"markdown","c78ce8c6":"markdown","6c28f079":"markdown","02193c12":"markdown","61ad0721":"markdown","8c1a647e":"markdown","ce880198":"markdown","4098e9ea":"markdown","d38d6ee7":"markdown","0149bd2b":"markdown","a12302b1":"markdown","042f3b4a":"markdown","5e98d6f3":"markdown","7fcd4902":"markdown","269388f9":"markdown","67c6ffef":"markdown","805cdc5d":"markdown","a46473da":"markdown","0e9429dd":"markdown","7d816ab9":"markdown","bdbeb75b":"markdown","dca4bf1d":"markdown","61897391":"markdown","22cd3fdb":"markdown","96fff286":"markdown","372df294":"markdown","4751e6d6":"markdown"},"source":{"8fccd967":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Ridge\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4fcb4d6a":"boston = load_boston()\ndata = boston.data\ntarget = boston.target","99bd3da9":"# train\/test split\nX_train, X_test, y_train, y_test = train_test_split(data, \n                                                    target, \n                                                    shuffle=True,\n                                                    test_size=0.2, \n                                                    random_state=15)","24ed8807":"def calc_train_error(X_train, y_train, model):\n    '''returns in-sample error for already fit model.'''\n    predictions = model.predict(X_train)\n    mse = mean_squared_error(y_train, predictions)\n    rmse = np.sqrt(mse)\n    return mse\n    \ndef calc_validation_error(X_test, y_test, model):\n    '''returns out-of-sample error for already fit model.'''\n    predictions = model.predict(X_test)\n    mse = mean_squared_error(y_test, predictions)\n    rmse = np.sqrt(mse)\n    return mse\n    \ndef calc_metrics(X_train, y_train, X_test, y_test, model):\n    '''fits model and returns the RMSE for in-sample error and out-of-sample error'''\n    model.fit(X_train, y_train)\n    train_error = calc_train_error(X_train, y_train, model)\n    validation_error = calc_validation_error(X_test, y_test, model)\n    return train_error, validation_error","876aa116":"lr = LinearRegression(fit_intercept=True)\n\ntrain_error, test_error = calc_metrics(X_train, y_train, X_test, y_test, lr)\ntrain_error, test_error = round(train_error, 3), round(test_error, 3)\n\nprint('train error: {} | test error: {}'.format(train_error, test_error))\nprint('train\/test: {}'.format(round(test_error\/train_error, 1)))","560d6a02":"# intermediate\/test split (gives us test set)\nX_intermediate, X_test, y_intermediate, y_test = train_test_split(data, \n                                                                  target, \n                                                                  shuffle=True,\n                                                                  test_size=0.2, \n                                                                  random_state=15)\n\n# train\/validation split (gives us train and validation sets)\nX_train, X_validation, y_train, y_validation = train_test_split(X_intermediate,\n                                                                y_intermediate,\n                                                                shuffle=False,\n                                                                test_size=0.25,\n                                                                random_state=2018)","b43a77b2":"# delete intermediate variables\ndel X_intermediate, y_intermediate\n\n# print proportions\nprint('train: {}% | validation: {}% | test {}%'.format(round(len(y_train)\/len(target),2),\n                                                       round(len(y_validation)\/len(target),2),\n                                                       round(len(y_test)\/len(target),2)))","f41aeb00":"alphas = [0.001, 0.01, 0.1, 1, 10]\nprint('All errors are RMSE')\nprint('-'*76)\nfor alpha in alphas:\n    # instantiate and fit model\n    ridge = Ridge(alpha=alpha, fit_intercept=True, random_state=99)\n    ridge.fit(X_train, y_train)\n    # calculate errors\n    new_train_error = mean_squared_error(y_train, ridge.predict(X_train))\n    new_validation_error = mean_squared_error(y_validation, ridge.predict(X_validation))\n    new_test_error = mean_squared_error(y_test, ridge.predict(X_test))\n    # print errors as report\n    print('alpha: {:7} | train error: {:5} | val error: {:6} | test error: {}'.\n          format(alpha,\n                 round(new_train_error,3),\n                 round(new_validation_error,3),\n                 round(new_test_error,3)))","93edf803":"# train\/test split\nX_train, X_test, y_train, y_test = train_test_split(data, \n                                                    target, \n                                                    shuffle=True,\n                                                    test_size=0.2, \n                                                    random_state=15)\n\n# instantiate model\nridge = Ridge(alpha=0.11, fit_intercept=True, random_state=99)\n\n# fit and calculate errors\nnew_train_error, new_test_error = calc_metrics(X_train, y_train, X_test, y_test, ridge)\nnew_train_error, new_test_error = round(new_train_error, 3), round(new_test_error, 3)","c4926a35":"print('ORIGINAL ERROR')\nprint('-' * 40)\nprint('train error: {} | test error: {}\\n'.format(train_error, test_error))\nprint('ERROR w\/REGULARIZATION')\nprint('-' * 40)\nprint('train error: {} | test error: {}'.format(new_train_error, new_test_error))","9019fddf":"from sklearn.model_selection import cross_val_score\nfrom sklearn import linear_model\nalphas = [1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1]\n\nval_errors = []\nfor alpha in alphas:\n    lasso = linear_model.Lasso(alpha=alpha, fit_intercept=True, random_state=77)\n    errors = np.sum(-cross_val_score(lasso, \n                                     data, \n                                     y=target, \n                                     scoring='neg_mean_squared_error', \n                                     cv=10, \n                                     n_jobs=-1))\n    val_errors.append(np.sqrt(errors))","beba2710":"# RMSE\nprint(val_errors)","4f749e17":"print('best alpha: {}'.format(alphas[np.argmin(val_errors)]))","c4ef2c77":"from sklearn.model_selection import KFold\n\nK = 10\nkf = KFold(n_splits=K, shuffle=True, random_state=42)\n\nfor alpha in alphas:\n    train_errors = []\n    validation_errors = []\n    for train_index, val_index in kf.split(data, target):\n        \n        # split data\n        X_train, X_val = data[train_index], data[val_index]\n        y_train, y_val = target[train_index], target[val_index]\n\n        # instantiate model\n        lasso = linear_model.Lasso(alpha=alpha, fit_intercept=True, random_state=77)\n        \n        #calculate errors\n        train_error, val_error = calc_metrics(X_train, y_train, X_val, y_val, lasso)\n        \n        # append to appropriate list\n        train_errors.append(train_error)\n        validation_errors.append(val_error)\n    \n    # generate report\n    print('alpha: {:6} | mean(train_error): {:7} | mean(val_error): {}'.\n          format(alpha,\n                 round(np.mean(train_errors),4),\n                 round(np.mean(validation_errors),4)))","6d1f8eaa":"Hmm, e\u011fitim hatam\u0131z test hatas\u0131ndan biraz daha d\u00fc\u015f\u00fck. Asl\u0131nda, test hatas\u0131 1,1 kat veya% 10 daha k\u00f6t\u00fc. Bu b\u00fcy\u00fck bir fark de\u011fil ama ara\u015ft\u0131rmaya de\u011fer.  \n\nBu bizi hangi b\u00f6lgeye sokuyor?  \n\nBu do\u011fru, Y\u00fcksek Varyans b\u00f6lgesinde her biri \u00e7ok hafif. Bu, modelimizin biraz daha yorucu oldu\u011fu anlam\u0131na geliyor. Yine, bu modelimizin biraz fazla karma\u015f\u0131kl\u0131\u011fa sahip oldu\u011fu anlam\u0131na gelir.  \n\nNe yaz\u0131k ki, bu noktada s\u0131k\u0131\u015f\u0131p kald\u0131k.  \n\nMuhtemelen d\u00fc\u015f\u00fcn\u00fcyorsun, \u201cHey bekle, hay\u0131r de\u011filiz. Bir veya iki \u00f6zellik b\u0131rak\u0131p ard\u0131ndan e\u011fitim hatas\u0131n\u0131 ve test hatas\u0131n\u0131 yeniden hesaplayabiliyorum. \u201d  \n\nPeki ne yap\u0131yoruz?  \n\nBa\u015fa d\u00f6nmeliyiz. Verilerimizi \u00fc\u00e7 veri k\u00fcmesine ay\u0131rmam\u0131z gerekiyor: e\u011fitim, do\u011frulama, test.  \n\nTest setinin, modelinizden memnun kalana kadar dokunmad\u0131\u011f\u0131n\u0131z veriler oldu\u011funu unutmay\u0131n. Test seti, modelinizin nas\u0131l genelle\u015fece\u011fini g\u00f6rmek i\u00e7in yaln\u0131zca B\u0130R kez kullan\u0131l\u0131r. Bu kadar.  \n\nTamam, Do\u011fruluma Seti denilen \u015feye bir bakal\u0131m.","d1bb3513":"## Do\u011frulama Seti\nBirinden \u00fc\u00e7 veri k\u00fcmesi \u00e7ok i\u015f gibi g\u00f6r\u00fcn\u00fcyor ama buna de\u011fer oldu\u011funa s\u00f6z veriyorum. \u0130lk \u00f6nce, bunun pratikte nas\u0131l yap\u0131ld\u0131\u011f\u0131n\u0131 g\u00f6relim.","ead4a025":"G\u00f6rsel bir ki\u015fiyseniz, verilerimiz bu \u015fekilde b\u00f6l\u00fcmlere ayr\u0131lm\u0131\u015ft\u0131r.  \n\n![train-validate-test.png](attachment:train-validate-test.png)\n\n","796fba1d":"**Sklearn & CV**  \nBunu sklearn\u2019de yapman\u0131n iki yolu var, ondan ne almak istedi\u011finizi bekliyorlar.  \n\nSize g\u00f6sterece\u011fim ilk y\u00f6ntem, ilgilendi\u011finiz tek \u015fey do\u011frulama hatas\u0131ysa g\u00fczel \u00e7al\u0131\u015fan cross_val_score\u2019dur.  \n\n\u0130kinci y\u00f6ntem, tren ve do\u011frulama hatalar\u0131na ihtiyac\u0131n\u0131z varsa m\u00fckemmel olan KFold\u2019dur.  \n\n\u0130\u015fleri ilgin\u00e7 tutmak i\u00e7in LASSO adl\u0131 yeni bir model deneyelim.  ","a105127c":"Burada birka\u00e7 \u00f6nemli \u00e7\u0131kar\u0131m var. \u0130lk olarak, do\u011frulama hatas\u0131 taraf\u0131ndan sergilenen U \u015feklindeki davran\u0131\u015fa dikkat edin.  \n19.796\u2019da ba\u015flar, iki ad\u0131m a\u015fa\u011f\u0131 iner ve sonra geri gider. Ayr\u0131ca, do\u011frulama hatas\u0131 ile test hatas\u0131n\u0131n birlikte hareket etme e\u011filiminde oldu\u011funa dikkat edin, ancak hi\u00e7bir \u015fekilde ili\u015fki m\u00fckemmel de\u011fildir.  \nAlfa ba\u015flang\u0131\u00e7ta artt\u0131k\u00e7a her iki hatan\u0131n da azald\u0131\u011f\u0131n\u0131 g\u00f6r\u00fcyoruz, ancak do\u011frulama hatas\u0131 tekrar y\u00fckselirken test hatas\u0131 azalmaya devam ediyor.  \nM\u00fckemmel de\u011fil.  \nAsl\u0131nda \u00e7ok k\u00fc\u00e7\u00fck bir veri k\u00fcmesiyle u\u011fra\u015ft\u0131\u011f\u0131m\u0131zla \u00e7ok ilgili. Her \u00f6rnek, bir milyon veya daha fazla kayd\u0131 olan bir veri k\u00fcmemize sahip oldu\u011fumuzdan \u00e7ok daha b\u00fcy\u00fck bir veri oran\u0131n\u0131 temsil eder.   \nHer neyse, do\u011frulama hatas\u0131, \u00f6zellikle veri k\u00fcmesi boyutu artt\u0131k\u00e7a test hatas\u0131 i\u00e7in iyi bir proxy\u2019dir. K\u00fc\u00e7\u00fck ve orta \u00f6l\u00e7ekli veri k\u00fcmeleriyle, \u00e7apraz do\u011frulamay\u0131 kullanarak daha iyisini yapabiliriz. Bunun hakk\u0131nda k\u0131saca konu\u015faca\u011f\u0131z.  \n\nModelimizi ayarlad\u0131\u011f\u0131m\u0131za g\u00f6re, test verileri d\u0131\u015f\u0131ndaki t\u00fcm verilere yeni bir s\u0131rt regresyon modeli yerle\u015ftirelim. Ard\u0131ndan, test hatas\u0131n\u0131 kontrol edece\u011fiz ve t\u00fcm \u00f6zelliklere sahip orijinal do\u011frusal regresyon modelimizle kar\u015f\u0131la\u015ft\u0131raca\u011f\u0131z.","502881b6":"![bias-variance-tradeoff.png](attachment:bias-variance-tradeoff.png)","c78ce8c6":"Cross_val_score \u00e7\u0131kt\u0131s\u0131n\u0131 KFold ile kar\u015f\u0131la\u015ft\u0131rarak, genel e\u011filimin ge\u00e7erli oldu\u011funu g\u00f6rebiliriz \u2013 10\u2019luk bir alfa en b\u00fcy\u00fck do\u011frulama hatas\u0131na neden olur. Neden farkl\u0131 de\u011ferler elde etti\u011fimizi merak edebilirsiniz. Bunun nedeni, verilerin farkl\u0131 \u015fekilde b\u00f6l\u00fcnm\u00fc\u015f olmas\u0131d\u0131r. B\u00f6lme prosed\u00fcr\u00fcn\u00fc KFold ile kontrol edebiliriz, ancak cross_val_score ile kontrol edemeyiz. Bu nedenle, iki prosed\u00fcr\u00fc ayr\u0131nt\u0131l\u0131 bir b\u00f6lme aramas\u0131 yapmadan veya algoritmay\u0131 kendimiz s\u0131f\u0131rdan yazmadan m\u00fckemmel bir \u015fekilde senkronize etmenin bir yolu yok. \u00d6nemli olan, her birinin bize, yaln\u0131zca do\u011frulama hatas\u0131 ya da e\u011fitim ve do\u011frulama hatas\u0131n\u0131n bir kombinasyonu olsun, ihtiyac\u0131m\u0131z olan her \u015feyi hesaplamak i\u00e7in uygun bir y\u00f6ntem vermesidir.  \n\nG\u00fcncelleme: sklearn, sizin i\u00e7in e\u011fitim ve do\u011frulama hatalar\u0131n\u0131 yakalayacak cross_validate ad\u0131nda bir y\u00f6nteme sahiptir. Hatta her bir katlama i\u00e7in bir modelin ne kadar s\u00fcrece\u011fini ve her do\u011frulama setinde modeli puanlamak i\u00e7in ge\u00e7en s\u00fcreyi t\u00fck\u00fcrecektir.  ","6c28f079":"Hangisinin d\u00f6nd\u00fcr\u00fcr: en iyi alfa: 0.1\n\n**K-Fold**","02193c12":"## Uygulama\nForest Fire veri k\u00fcmesinin do\u011frusal bir regresyon modelini olu\u015ftural\u0131m. Modelimizin tam olarak uyuyor mu, \u00e7ok uyuyor mu yoksa uyuyor mu oldu\u011funu ara\u015ft\u0131r\u0131r\u0131z. Durumun alt\u0131nda veya \u00fczerinde ise, bunu d\u00fczeltebilece\u011fimiz bir yoldan bakar\u0131z.  \n\nModel olu\u015fturma zaman\u0131.  \n\nNot: E\u011fitim hatas\u0131n\u0131 g\u00f6stermek i\u00e7in train_error\u2019\u0131, do\u011frulama hatas\u0131n\u0131 g\u00f6stermek i\u00e7in test_error\u2019\u0131 kullanaca\u011f\u0131m.  ","61ad0721":"## Y\u00fcksek Bias\nSola do\u011fru k\u0131sa \u00e7izgilerle \u00e7izilen ve High Bias olarak etiketlenen dikd\u00f6rtgen kutu ilgi duyulan ilk b\u00f6lgedir.    \nBurada E\u011fitim Hatas\u0131 ve Do\u011frulama Hatas\u0131\u2019n\u0131n y\u00fcksek oldu\u011funu fark edeceksiniz. Ayr\u0131ca birbirlerine yak\u0131n olduklar\u0131n\u0131 fark edeceksiniz. Bu b\u00f6lge, modelin verilerdeki i\u00e7sel e\u011filimi ger\u00e7ekten ortaya \u00e7\u0131karmak i\u00e7in gereken esnekli\u011fe sahip olmad\u0131\u011f\u0131 b\u00f6lge olarak tan\u0131mlanmaktad\u0131r. Makine \u00f6\u011frenim konu\u015fmas\u0131nda, yetersizdir, yani her yerde fakir bir i\u015f yap\u0131yor ve genelle\u015fmiyor. Model, e\u011fitim setinde pek iyi de\u011fil.  \n\nBunu nas\u0131l d\u00fczeltirsiniz?    \n\nTabii ki model karma\u015f\u0131kl\u0131\u011f\u0131n\u0131 ekleyerek. Ba\u015fka bir g\u00f6nderi alt\u0131nda oldu\u011funuzu veya bunlara uymad\u0131\u011f\u0131n\u0131z\u0131 fark etti\u011finizde ne yapaca\u011f\u0131n\u0131z konusunda daha fazla ayr\u0131nt\u0131ya girece\u011fim. \u015eimdilik, do\u011frusal regresyon kulland\u0131\u011f\u0131n\u0131z\u0131 varsayarsak, ba\u015flamak i\u00e7in iyi bir yer ek \u00f6zellikler eklemektir. Modelinize parametrelerin eklenmesi, modelinizi Golidlocks B\u00f6lgesine itebilecek esnekli\u011fi sa\u011flar.  ","8c1a647e":"Bir do\u011frulama katlamas\u0131 olarak egzersiz verilerinizin bir y\u00fczdesini segmentlere ay\u0131r\u0131rs\u0131n\u0131z.  \n\nTeknik not: Terminolojiye dikkat edin. Baz\u0131 insanlar do\u011frulama kat\u0131n\u0131 test katlamas\u0131 olarak adland\u0131r\u0131r. Ne yaz\u0131k ki, terimleri birbirinin yerine kullan\u0131rlar, bu da kafa kar\u0131\u015ft\u0131r\u0131c\u0131d\u0131r ve bu nedenle do\u011fru de\u011fildir. Bunu yapma. Test seti, sonunda varsa, yaln\u0131zca sonunda t\u00fcketilen saf verilerdir.  \n\nVeriler do\u011frulama kat\u0131nda b\u00f6l\u00fcmlere ayr\u0131ld\u0131ktan sonra, kalan egzersiz verilerine yeni bir model yerle\u015ftirirsiniz. \u0130deal olarak, tren ve do\u011frulama hatas\u0131n\u0131 hesaplars\u0131n\u0131z. Ancak baz\u0131 ki\u015filer yaln\u0131zca do\u011frulama hatas\u0131na bakarlar.  \n\n\u0130lk do\u011frulama kat\u0131na dahil edilen veriler bir daha asla do\u011frulama katlamas\u0131n\u0131n bir par\u00e7as\u0131 olmayacakt\u0131r. \u0130lk yinelemeyle ayn\u0131 veri y\u00fczdesini b\u00f6l\u00fcmlere ay\u0131ran yeni bir do\u011frulama katlamas\u0131 olu\u015fturulur. Daha sonra s\u00fcre\u00e7 tekrar eder \u2013 yeni bir modele uyun, \u00f6nemli metrikleri hesaplay\u0131n ve tekrarlay\u0131n. Algoritma, bu i\u015flem K kez oldu\u011funda sona erer. Bu nedenle, do\u011frulama setindeki t\u00fcm veri noktalar\u0131n\u0131 e\u011fitim setlerinde bir kez ve bir\u00e7ok kez ziyaret ederek, do\u011frulama hatas\u0131n\u0131n K tahminleriyle sonu\u00e7lan\u0131rs\u0131n\u0131z. Son ad\u0131m, regresyon i\u00e7in do\u011frulama hatalar\u0131n\u0131 ortalamakt\u0131r. Bu, belirli bir modelin ne kadar iyi performans g\u00f6sterece\u011fine dair iyi bir tahmin verir.  \n\nYine, bu y\u00f6ntem, k\u00fc\u00e7\u00fck ve orta boyutlu veri k\u00fcmelerinde hiperparametrelerin ayarlanmas\u0131 i\u00e7in paha bi\u00e7ilmezdir. Teknik olarak bir test setine bile ihtiyac\u0131n\u0131z yok. Verilere sahip de\u011filseniz bu harika bir \u015fey. B\u00fcy\u00fck veri k\u00fcmeleri i\u00e7in, basit bir tren \/ do\u011frulama \/ test b\u00f6l\u00fcnme stratejisi kullan\u0131n ve hiperparametrelerinizi \u00f6nceki b\u00f6l\u00fcmde yapt\u0131\u011f\u0131m\u0131z gibi ayarlay\u0131n.\n\nPekala, bakal\u0131m K-katlamal\u0131 CV i\u015f ba\u015f\u0131nda.  ","ce880198":"## **\u00d6zet**\nHiperparametrelerinizi ayarlad\u0131ktan sonra ne yapars\u0131n\u0131z? M\u00fcmk\u00fcn oldu\u011funca fazla bilgi elde edebilmek i\u00e7in t\u00fcm veriler \u00fczerinde yeni bir model e\u011fitmeniz yeterlidir. Bu \u015fekilde, modeliniz gelecekteki veriler \u00fczerinde en iyi tahmin g\u00fcc\u00fcne sahip olacakt\u0131r. G\u00f6rev tamamland\u0131!  \n\nBias-Varyans modelinin uygun olmayan bir bias modelinde, y\u00fcksek varyansl\u0131 modelin fazla olan bir bias modelini tart\u0131\u015ft\u0131k.  \nAyr\u0131ca, ayarlama amac\u0131yla verileri \u00fc\u00e7 gruba ay\u0131rabilece\u011fimizi \u00f6\u011frendik. \u00d6zellikle, \u00fc\u00e7 grup e\u011fitim, do\u011frulama ve testtir.  \nTest setinin, bir modelin daha \u00f6nce hi\u00e7 g\u00f6r\u00fclmedi\u011fi veriler \u00fczerinde ne kadar iyi oldu\u011funu kontrol etmek i\u00e7in yaln\u0131zca bir kez kullan\u0131ld\u0131\u011f\u0131n\u0131 unutmay\u0131n.  \nBu \u00fc\u00e7 gruplu b\u00f6l\u00fcnme, b\u00fcy\u00fck veri k\u00fcmeleri i\u00e7in son derece iyi \u00e7al\u0131\u015f\u0131r, ancak k\u00fc\u00e7\u00fck ve orta boyutlu veri k\u00fcmeleri i\u00e7in iyi de\u011fildir. Bu durumda, \u00e7apraz do\u011frulamay\u0131 (CV) kullan\u0131n. CV, modellerinizi ayarlaman\u0131za ve k\u00fc\u00e7\u00fck veri \u00f6rne\u011finden m\u00fcmk\u00fcn oldu\u011funca fazla sinyal alman\u0131za yard\u0131mc\u0131 olabilir. Unutmay\u0131n, CV ile bir test setine ihtiyac\u0131n\u0131z yoktur. Bir K-katlama yakla\u015f\u0131m\u0131 kullanarak, do\u011frulama hatas\u0131n\u0131 kontrol etmek i\u00e7in K-test setlerinin e\u015fde\u011ferini elde edersiniz. Bu, bias-varyans rejiminde nerede oldu\u011funuzu te\u015fhis etmenize yard\u0131mc\u0131 olur.","4098e9ea":"## Goldilocks B\u00f6lgesi  \n\u00c7izgisiz orta b\u00f6lgeye Goldilocks B\u00f6lgesi ad\u0131n\u0131 verdim.  \nModeliniz, verilerde bulunan deseni almak i\u00e7in tam do\u011fru miktarda esnekli\u011fe sahiptir, ancak ger\u00e7ekten sadece e\u011fitim verilerini ezberleyecek kadar esnek de\u011fildir.  \nBu b\u00f6lge hem d\u00fc\u015f\u00fck hem de birbirine yak\u0131n olan E\u011fitim Hatas\u0131 ve Do\u011frulama Hatas\u0131 ile i\u015faretlenmi\u015ftir.  \nBu, modelinizin ya\u015famas\u0131 gereken yer.","d38d6ee7":"## **Oryantasyon**  \nX ekseni model karma\u015f\u0131kl\u0131\u011f\u0131n\u0131 temsil eder. Bunun modelinizin ne kadar esnek oldu\u011fu ile ilgili. Bir modele karma\u015f\u0131kl\u0131k katan baz\u0131 \u015feyler \u015funlard\u0131r: ek \u00f6zellikler, polinom terimlerinin artt\u0131r\u0131lmas\u0131 ve a\u011fa\u00e7 tabanl\u0131 modeller i\u00e7in derinli\u011fin artt\u0131r\u0131lmas\u0131. Bu \u00e7ok ayr\u0131nt\u0131l\u0131 bir listeden uzakta de\u011fil ama \u00f6z\u00fc almal\u0131s\u0131n.  \n\nY ekseni model hatas\u0131n\u0131 g\u00f6sterir. Genellikle, Regresyon ve \u00c7apraz Entropi i\u00e7in Ortalama Karesel Hata (MSE) veya S\u0131n\u0131fland\u0131rma Hassasiyeti olarak \u00f6l\u00e7\u00fcl\u00fcr.  \n\nMavi e\u011fri E\u011fitim Hatas\u0131. Sadece azald\u0131\u011f\u0131na dikkat edin. Ac\u0131\u00e7a a\u00e7\u0131k olmas\u0131 gereken, model karma\u015f\u0131kl\u0131\u011f\u0131n\u0131n eklenmesinin daha k\u00fc\u00e7\u00fck ve daha k\u00fc\u00e7\u00fck e\u011fitim hatalar\u0131na yol a\u00e7mas\u0131d\u0131r. Bu \u00f6nemli bir bulgu.  \n\nYe\u015fil e\u011fri U \u015feklindedir. Bu e\u011fri Do\u011frulama Hatas\u0131 anlam\u0131na gelir. Trende dikkat edin. \u0130lk \u00f6nce azal\u0131r, minimum vurur ve artar. Do\u011frulama Hatas\u0131\u2019n\u0131n tam olarak ne oldu\u011fu ve nas\u0131l hesaplanaca\u011f\u0131 hakk\u0131nda daha detayl\u0131 olarak konu\u015faca\u011f\u0131z.","0149bd2b":"## Model Ayarlama\n\nKarma\u015f\u0131kl\u0131\u011f\u0131 azaltmam\u0131z gerekiyor. Bunu yapman\u0131n bir yolu, d\u00fczenlile\u015ftirme kullanmakt\u0131r.  \nD\u00fczenlemenin \u015fu anda nas\u0131l \u00e7al\u0131\u015ft\u0131\u011f\u0131na dair cesur cesaretlere girmeyece\u011fim \u00e7\u00fcnk\u00fc gelecekteki bir g\u00f6nderide bunu ele alaca\u011f\u0131m.  \nD\u00fczenlemenin, model parametrelerini belirlemeye s\u0131n\u0131rlar getiren bir k\u0131s\u0131tl\u0131 optimizasyon bi\u00e7imi oldu\u011funu bilin.  \nEtkili bir modele bias eklememi etkili bir \u015fekilde sa\u011fl\u0131yor.  \nBias miktar\u0131n\u0131 lambda veya alfa denilen bir hiperparametre ile kontrol edebilirim (her ikisini de g\u00f6r\u00fcrs\u00fcn\u00fcz, ancak sklearn alfa kullan\u0131yor, \u00e7\u00fcnk\u00fc lambda bir Python anahtar s\u00f6zc\u00fc\u011f\u00fcd\u00fcr), normalle\u015ftirme g\u00fcc\u00fcn\u00fc tan\u0131mlar.","a12302b1":"## Veri Al ve Haz\u0131rla","042f3b4a":"![kfold-cross-validation.png](attachment:kfold-cross-validation.png)","5e98d6f3":"## **Teori**\n## **Bias-Varyans Tradeoff**\nBu b\u00f6l\u00fcme \u00e7ok dikkat edin. T\u00fcm makine \u00f6\u011frenmesinde en \u00f6nemli kavramlardan biridir. Bu kavram\u0131 anlamak, her t\u00fcr modeli te\u015fhis etmenize yard\u0131mc\u0131 olacakt\u0131r, bunlar do\u011frusal regresyon, XGBoost veya Konvol\u00fcsyonel Sinir A\u011flar\u0131d\u0131r.\n\nE\u011fitim hatas\u0131n\u0131 ve test hatas\u0131n\u0131 nas\u0131l hesaplayaca\u011f\u0131m\u0131z\u0131 zaten biliyoruz. \u015eimdiye kadar, modelimizin ne kadar iyi genellenece\u011fini \u00f6l\u00e7menin bir yolu olarak test hatas\u0131n\u0131 kullan\u0131yorduk. Bu iyi bir ilk ad\u0131md\u0131, ancak yeterince iyi de\u011fil. Daha iyisini yapabiliriz. Modelimizi ayarlayabiliriz. A\u015fa\u011f\u0131 inelim.\n\nModelimizde neler olup bitti\u011fini anlamak i\u00e7in e\u011fitim hatas\u0131n\u0131 ve do\u011frulama hatas\u0131 olarak adland\u0131r\u0131lan bir \u015feyi kar\u015f\u0131la\u015ft\u0131rabiliriz \u2013 daha bir dakika i\u00e7inde do\u011frulama hatas\u0131yla ilgili. Her birinin de\u011ferine ba\u011fl\u0131 olarak, modelimiz \u00fc\u00e7 b\u00f6lgeden birinde olabilir:\n\n1) **High Bias** \u2013 underfitting  \n2) **Goldilocks Zone** \u2013 just right  \n3) **High Variance** \u2013 overfitting  ","7fcd4902":"Her bir alfa ile ili\u015fkili do\u011frulama hatalar\u0131n\u0131 kontrol edelim.","269388f9":"## Verileri, Modeli Kurma ve Hatalar\u0131 Hesaplama","67c6ffef":"## cross_val_score","805cdc5d":"## Kanonik \u00c7izim\nBu fikirleri eve g\u00f6t\u00fcrmek i\u00e7in bir plana daha bakal\u0131m.  \n\n![bias-and-variance-targets.jpg](attachment:bias-and-variance-targets.jpg)\n\n\nBir ok\u00e7uluk yar\u0131\u015fmas\u0131na kat\u0131ld\u0131\u011f\u0131n\u0131z\u0131 d\u00fc\u015f\u00fcn\u00fcn. Hedefin hangi k\u0131sm\u0131na vurdu\u011funuza g\u00f6re bir puan al\u0131rs\u0131n\u0131z: k\u0131rm\u0131z\u0131 daire i\u00e7in 0 (bullseye), mavi i\u00e7in 1 ve bir s\u00fcre i\u00e7in 2. Ama\u00e7, puan\u0131n\u0131z\u0131 en aza indirmektir ve bunu m\u00fcmk\u00fcn oldu\u011funca \u00e7ok say\u0131da bullseye vurarak yapars\u0131n\u0131z.  \n\nOk\u00e7uluk metaforu, bir model olu\u015fturarak neyi ba\u015farmaya \u00e7al\u0131\u015ft\u0131\u011f\u0131m\u0131z\u0131 a\u00e7\u0131klamak i\u00e7in kullan\u0131\u015fl\u0131 bir analogdur. Farkl\u0131 veri k\u00fcmeleri g\u00f6z \u00f6n\u00fcne al\u0131nd\u0131\u011f\u0131nda (farkl\u0131 oklara e\u015fde\u011fer), g\u00f6zlemlenen verilere (aka hedefler) olabildi\u011fince yakla\u015fan bir model istiyoruz.  \n\nGrafi\u011fin \u00fcst D\u00fc\u015f\u00fck Bias \/ D\u00fc\u015f\u00fck Varyans k\u0131sm\u0131 ideal durumu temsil eder. Buras\u0131 Goldilocks B\u00f6lgesi. Modelimiz t\u00fcm faydal\u0131 bilgileri \u00e7\u0131kard\u0131 ve genelle\u015ftiriyor. Bunu biliyoruz, \u00e7\u00fcnk\u00fc model do\u011fru ve \u00f6ng\u00f6r\u00fclemeyen verileri tahmin ederken bile \u00e7ok az farkl\u0131l\u0131k g\u00f6steriyor. Model, farkl\u0131 r\u00fczgar h\u0131zlar\u0131na, mesafelerine ve ayd\u0131nlatma ko\u015fullar\u0131na uyum sa\u011flayabilen bir ok\u00e7u gibi olduk\u00e7a iyi ayarlanm\u0131\u015f.  \n\nGrafi\u011fin D\u00fc\u015f\u00fck Bias \/ Y\u00fcksek Varyansl\u0131 k\u0131sm\u0131, fazla uydurmay\u0131 temsil eder. Modelimiz e\u011fitim verilerinde iyi i\u015f \u00e7\u0131kar\u0131yor, ancak belirli veri k\u00fcmeleri i\u00e7in y\u00fcksek farkl\u0131l\u0131klar g\u00f6r\u00fcyoruz. Bu, \u00e7ok kat\u0131 ko\u015fullar alt\u0131nda e\u011fitim alm\u0131\u015f bir ok\u00e7unun \u00f6zlemi\u015ftir \u2013 belki r\u00fczgar\u0131n olmad\u0131\u011f\u0131 yerlerde, mesafe tutarl\u0131 ve ayd\u0131nlatma daima ayn\u0131d\u0131r. Bu \u00f6zelliklerin herhangi birindeki herhangi bir de\u011fi\u015fiklik ok\u00e7unun do\u011frulu\u011funu artt\u0131r\u0131r. Ok\u00e7u tutarl\u0131l\u0131ktan yoksundur.  \n\nGrafi\u011fin Y\u00fcksek Bias \/ D\u00fc\u015f\u00fck Varyansl\u0131 k\u0131sm\u0131, donat\u0131y\u0131 temsil eder. Bizim modelimiz belirli bir veri setinde yetersiz kalmaktad\u0131r. Asl\u0131nda, besledi\u011finiz verilere bak\u0131lmaks\u0131z\u0131n zay\u0131f bir \u015fekilde yapmas\u0131 o kadar k\u00f6t\u00fc, dolay\u0131s\u0131yla k\u00fc\u00e7\u00fck farkl\u0131l\u0131klar. Bir analog olarak, tutarl\u0131 bir \u015fekilde ate\u015f etmeyi \u00f6\u011frenmi\u015f ancak hedefi vurmay\u0131 \u00f6\u011frenmemi\u015f bir ok\u00e7u d\u00fc\u015f\u00fcn\u00fcn. Bu, e\u011fitim verilerinin hedefinin ortalama de\u011ferini her zaman tahmin eden bir modele ayk\u0131r\u0131d\u0131r.  \n\nGrafi\u011fin Y\u00fcksek Bias \/ Y\u00fcksek Varyansl\u0131 b\u00f6l\u00fcm\u00fcnde, bildi\u011fim kadar\u0131yla makine \u00f6\u011frenmede analog yok. Bias ve varyans aras\u0131nda bir tradeoff var. Bu nedenle, her ikisinin de y\u00fcksek olmas\u0131 m\u00fcmk\u00fcn de\u011fildir.  \n\nPekala, teoriyi \u00e7\u00f6zd\u00fc\u011f\u00fcm\u00fcz i\u00e7in bunu pratikte g\u00f6rmek i\u00e7in vites de\u011fi\u015ftirelim.  ","a46473da":"Hangi alfa de\u011feri bize en k\u00fc\u00e7\u00fck do\u011frulama hatas\u0131n\u0131 verdi?","0e9429dd":"\u0130\u015fte 10 kat CV i\u00e7in genel fikir:","7d816ab9":"A\u00e7\u0131k\u00e7a s\u00f6yleyeyim: bu y\u00f6ntem k\u00fc\u00e7\u00fck ila orta boyutlu veri k\u00fcmelerinde harika \u00e7al\u0131\u015f\u0131r. Bu kesinlikle b\u00fcy\u00fck bir veri k\u00fcmesinde denemek istedi\u011finiz t\u00fcrden bir \u015fey de\u011fildir (onlarca veya y\u00fcz milyonlarca sat\u0131r ve \/ veya s\u00fctun d\u00fc\u015f\u00fcn\u00fcn). Pekala, hadi \u015fimdi yolumdan \u00e7ekilelim.  \n\nE\u011fitim \/ test b\u00f6l\u00fcm\u00fc hakk\u0131nda yaz\u0131da g\u00f6rd\u00fc\u011f\u00fcm\u00fcz gibi, daha k\u00fc\u00e7\u00fck veri k\u00fcmelerini nas\u0131l b\u00f6ld\u00fc\u011f\u00fcn\u00fcz \u00f6nemli bir fark yarat\u0131yor; sonu\u00e7lar b\u00fcy\u00fck \u00f6l\u00e7\u00fcde de\u011fi\u015febilir. Rastgele durum bir hiperparametre olmad\u0131\u011f\u0131ndan (ciddi olarak, l\u00fctfen bunu yapmay\u0131n), m\u00fcmk\u00fcn olan verilerden her son bit sinyalini \u00e7\u0131karmak i\u00e7in bir yola ihtiyac\u0131m\u0131z var. Bu y\u00fczden, sadece bir e\u011fitim \/ do\u011frulama b\u00f6l\u00fcm\u00fc yerine, onlar\u0131 K yapal\u0131m.  \n\nBu teknik uygun \u015fekilde K-kat \u00e7apraz ge\u00e7erlili\u011fi olarak adland\u0131r\u0131l\u0131r. Yine, K ihtiyac\u0131n\u0131z olan ka\u00e7 tren \/ do\u011frulama b\u00f6ce\u011fini temsil eder. K\u2019nin nas\u0131l se\u00e7ilece\u011fi konusunda sert ve h\u0131zl\u0131 bir kural yoktur, ancak daha iyi ve daha k\u00f6t\u00fc se\u00e7enekler vard\u0131r. Veri k\u00fcmenizin boyutu b\u00fcy\u00fcd\u00fck\u00e7e, K i\u00e7in 3 veya 5 gibi daha k\u00fc\u00e7\u00fck de\u011ferlerle kurtulabilirsiniz. Veri k\u00fcmeniz k\u00fc\u00e7\u00fck oldu\u011funda, 10 gibi daha b\u00fcy\u00fck bir say\u0131 se\u00e7mek yayg\u0131nd\u0131r. Yine, bunlar sadece temel kurallard\u0131r  ","bdbeb75b":"## Y\u00fcksek Varyans\nKesik dikd\u00f6rtgen kutu sa\u011fda ve Y\u00fcksek Varyans etiketli, Y\u00fcksek Bias b\u00f6lgesinin \u00e7evresidir. Burada model o kadar esnekli\u011fe sahiptir ki, esasen e\u011fitim verilerini ezberlemeye ba\u015flar. Beklendi\u011fi gibi, bu yakla\u015f\u0131m d\u00fc\u015f\u00fck E\u011fitim Hatalar\u0131na yol a\u00e7maktad\u0131r. Ancak tren \/ test noktas\u0131nda belirtildi\u011fi gibi, bir arama tablosu genelle\u015fmiyor, bu y\u00fczden bu b\u00f6lgede Y\u00fcksek Do\u011frulama Hatas\u0131 g\u00f6r\u00fcyoruz.   \nE\u011fitim Hatalar\u0131n\u0131z d\u00fc\u015f\u00fck, ancak Do\u011frulama Hatalar\u0131n\u0131z y\u00fcksekken bu b\u00f6lgede oldu\u011funuzu biliyorsunuz. Ba\u015fka bir deyi\u015fle, ikisi aras\u0131nda b\u00fcy\u00fck bir delta varsa, fazladan uyduruyorsunuz.\n\nBunu nas\u0131l d\u00fczeltirsiniz?\n\nModel karma\u015f\u0131kl\u0131\u011f\u0131n\u0131 azaltarak. Yine, tam olarak ne yap\u0131laca\u011f\u0131 hakk\u0131nda ayr\u0131 bir g\u00f6nderide daha fazla ayr\u0131nt\u0131ya girece\u011fim. \u015eimdilik d\u00fczenlile\u015ftirme veya b\u0131rakma \u00f6zellikleri kullanmay\u0131 d\u00fc\u015f\u00fcn\u00fcn.","dca4bf1d":" ### \u015eimdi de verileri \u015fu \u015fekilde e\u011fitim \/ test setine b\u00f6lelim:","61897391":"## Hata raporlar\u0131","22cd3fdb":"## Kurulum\nE\u011fitim ve test hatas\u0131n\u0131 hesaplamam\u0131z gerekti\u011fini biliyoruz, bu y\u00fczden devam edelim ve bunun i\u00e7in i\u015flevler olu\u015ftural\u0131m.  \nBiz varken bizim i\u00e7in g\u00fczel bir rapor olu\u015fturacak bir meta-fonksiyon ekleyelim.  \nAyr\u0131ca, K\u00f6k Ortalama Kare Hatas\u0131 (RMSE) bizim se\u00e7im \u00f6l\u00e7\u00fct\u00fcm\u00fcz olacakt\u0131r.","96fff286":"E\u011fitim setinin t\u00fcm verilerin% 60\u2019\u0131n\u0131, do\u011frulama setinin% 20\u2019sini ve test setinin% 20\u2019sini olu\u015fturdu\u011fu yukar\u0131daki grafikle g\u00f6sterilen \u00fc\u00e7 veri setimiz var. Ger\u00e7ek test setini hi\u00e7bir \u015fekilde de\u011fi\u015ftirmedi\u011fime dikkat edin. Ayn\u0131 ba\u015flang\u0131\u00e7 b\u00f6l\u00fcnmesini ve ayn\u0131 rastgele durumu kulland\u0131m. Bu \u015fekilde, daha \u00f6nce olu\u015fturdu\u011fumuz do\u011frusal regresyon modeline uyacak ve ayarlayaca\u011f\u0131m\u0131z modeli kar\u015f\u0131la\u015ft\u0131rabiliriz.  \n\nYan not: Verilerinizi nas\u0131l oranland\u0131raca\u011f\u0131n\u0131z konusunda zor ve h\u0131zl\u0131 bir kural yoktur. Modelinizin, besledi\u011finiz verileri s\u0131n\u0131rland\u0131r\u0131rsan\u0131z \u00f6\u011frenebilecekleriyle s\u0131n\u0131rl\u0131 oldu\u011funu bilin. Ancak, test k\u00fcmeniz \u00e7ok k\u00fc\u00e7\u00fckse, modelinizin nas\u0131l performans g\u00f6sterece\u011fine ili\u015fkin kesin bir tahmin sa\u011flamaz. \u00c7apraz do\u011frulama, bu durumu kolayl\u0131kla ele almam\u0131za izin verir, ancak daha sonra.  \n\nModelimize uyma ve ayarlama zaman\u0131.  ","372df294":"E\u011fitim hatas\u0131 \u00e7ok k\u00fc\u00e7\u00fck bir art\u0131\u015f ve test hatas\u0131 k\u00fc\u00e7\u00fck bir d\u00fc\u015f\u00fc\u015f ile birlikte.  \nKesinlikle do\u011fru y\u00f6nde ilerliyoruz. Belki de bekledi\u011fimiz de\u011fi\u015fimin b\u00fcy\u00fckl\u00fc\u011f\u00fc de\u011fil, sadece burada bir noktaya de\u011finmeye \u00e7al\u0131\u015f\u0131yoruz.  \nBunun k\u00fc\u00e7\u00fck bir veri k\u00fcmesi oldu\u011funu unutmay\u0131n.  \nAyr\u0131ca, \u00c7apraz Do\u011frulama ad\u0131 verilen bir \u015feyi kullanarak daha iyisini yapabilece\u011fimizi s\u00f6yledi\u011fimi unutmay\u0131n. \u015eimdi bunun hakk\u0131nda konu\u015fma zaman\u0131.","4751e6d6":"## Cross-Validation (\u00c7apraz Do\u011frulama)"}}