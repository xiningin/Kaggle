{"cell_type":{"6adc698f":"code","a60c2127":"code","0fcd7dc2":"code","94f61196":"code","a45cd280":"code","e27c85cf":"code","e280ca2a":"code","86eca535":"code","7b839a78":"code","d9d2b442":"code","ed191051":"code","3dbe5598":"code","7ec00c5f":"code","810fbd1b":"code","977ed161":"code","36e360eb":"code","45e35ffa":"code","53482845":"code","4b5a8cb9":"code","8d3a1642":"code","7fb7ab35":"code","4bf376c1":"code","b5c90d93":"code","d79e9b1d":"code","f1d9fd6c":"code","4c368af9":"code","e7c4d292":"code","38d4e4fb":"code","2a7b3fe6":"code","faaf108e":"code","0d0ec414":"markdown","493d37e6":"markdown","9ae76cc4":"markdown","d10ee8bc":"markdown","1e645b88":"markdown","9a1bcad4":"markdown","c07044a6":"markdown","2c5225b4":"markdown"},"source":{"6adc698f":"import matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\ntf.__version__","a60c2127":"# we load VGG19 here just to find out the name of the layers\n# we won't be using the network object initialized below\nnetwork = tf.keras.applications.VGG19(include_top=False, #only convolution and maxpooling layers to include\n                                     weights='imagenet' #include weights\n                                    )\nnetwork.summary()","0fcd7dc2":"len(network.layers)","94f61196":"content_image = tf.keras.preprocessing.image.load_img('..\/input\/deepdream\/chaves.jpeg')\nplt.axis('off')\nplt.imshow(content_image);","a45cd280":"type(content_image)","e27c85cf":"content_image = tf.keras.preprocessing.image.img_to_array(content_image)\ntype(content_image), content_image.shape","e280ca2a":"#normalize the image\ncontent_image = content_image \/ 255","86eca535":"# convert int obatch format\ncontent_image = content_image[tf.newaxis, :]\ncontent_image.shape","7b839a78":"style_image = tf.keras.preprocessing.image.load_img('..\/input\/deepdream\/tarsila_amaral.jpg')\nplt.axis('off')\nplt.imshow(style_image);","d9d2b442":"style_image = tf.keras.preprocessing.image.img_to_array(style_image)\n\n#normalize the image\nstyle_image = style_image \/ 255\n\n# convert int obatch format\nstyle_image = style_image[tf.newaxis, :]\n\ntype(style_image), style_image.shape","ed191051":"content_layers = ['block4_conv2'] #1 content layer\nstyle_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1'] #5 style layers","3dbe5598":"num_content_layers = len(content_layers)\nnum_style_layers = len(style_layers)\nnum_content_layers, num_style_layers","7ec00c5f":"network.input","810fbd1b":"#function to return activation values from a layer\ndef vgg_layers(layer_names):\n    vgg = tf.keras.applications.VGG19(include_top=False, #load the model to use the layer_names activation output\n                                    weights='imagenet')\n    vgg.trainable = False # not going to train the network. Only weights is to be used\n    \n    outputs =[vgg.get_layer(name).output for name in layer_names] #get activations from the specified layers\n    \n    network = tf.keras.Model(inputs = [vgg.input], outputs=outputs)\n    return network","977ed161":"# test the above function\nstyle_extractor = vgg_layers(style_layers)\nstyle_extractor.summary()\n# seems the network is same as VGG19 (not like that, here there are 5 output layers while in VGG19  there is only one output layer)","36e360eb":"network.outputs #only one output for VGG19","45e35ffa":"style_outputs = style_extractor(style_image) #see below that there are 5 output layers\nstyle_outputs","53482845":"len(style_outputs) #gives activation outputs of the 5 layers given the style image as input","4b5a8cb9":"style_outputs[0].shape, style_outputs[1].shape, style_outputs[2].shape, style_outputs[3].shape, style_outputs[4].shape, ","8d3a1642":"def gram_matrix(layer_activation): #activation value of a layer\n    result = tf.linalg.einsum('bijc,bijd->bcd', layer_activation, layer_activation) #code extracted from documentation of style transfer\n    input_shape = tf.shape(layer_activation)\n    num_locations = tf.cast(input_shape[1] * input_shape[2], tf.float32) # for normalization\n    \n    return result \/ num_locations #normalized value","7fb7ab35":"gram_matrix(style_outputs[0])","4bf376c1":"class StyleContentModel(tf.keras.models.Model):\n    def __init__(self, style_layers, content_layers):\n        super().__init__()\n        self.vgg = vgg_layers(style_layers + content_layers)\n        self.style_layers = style_layers\n        self.content_layers = content_layers\n        self.num_style_layers = len(style_layers)\n        self.vgg.trainable = False\n    #call can be invoked as obj()\n    def call(self, inputs): #inputs - content image\n        inputs = inputs * 255.0 #undo normalization since preprocessing is done with rspect to VGG19 (which is done below)\n        preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n        outputs = self.vgg(preprocessed_input) #contains activation outputs of style and content layers\n        style_outputs = outputs[:self.num_style_layers] #activation outputs of style layers\n        content_outputs = outputs[self.num_style_layers:] #activation outputs of content layers\n        \n        style_outputs = [gram_matrix(style_output) for style_output in style_outputs]\n        \n        content_dict = {content_name: value for content_name, value in zip(self.content_layers, content_outputs)}\n        style_dict = {style_name: value for style_name, value in zip(self.style_layers, style_outputs)}\n        \n        return {'content':content_dict, 'style':style_dict}","b5c90d93":"style_layers, content_layers","d79e9b1d":"extractor = StyleContentModel(style_layers, content_layers)\nresults = extractor(content_image)\nresults","f1d9fd6c":"for key, value in results.items():\n    print(key,'-', value.keys())","4c368af9":"style_targets = extractor(style_image)['style'] #5 activation layers' outputs\ncontents_targets = extractor(content_image)['content'] #1 activation layer's output\nlen(style_targets), len(contents_targets)\n\n# generated image is compared with original content image to minimize the content loss\n# also, generated image is compared with original style image to minimize the style loss","e7c4d292":"transferred_image = tf.Variable(content_image) #copy of content image which is to be style transfered from style image during training\n\ncontent_weights = 1 #units of contents needed in the final result\nstyle_weights = 1000 #units of styles needed in the final result","38d4e4fb":"optimizer = tf.optimizers.Adam(learning_rate = 0.02)","2a7b3fe6":"from sklearn.metrics import mean_absolute_error, mean_squared_error\n\nepochs = 5000\ndisplay_epoch = 500\n\nfor epoch in range(epochs):\n    with tf.GradientTape() as tape:\n    #get gradients since we need to update the pixels of images and not the weights using gradients as like traditional NN\n        outputs = extractor(transferred_image)\n        content_outputs = outputs['content']\n        style_outputs = outputs['style']\n        \n        # loss is calculated since transfer is to be done only to certain\n        # amount since more the transfer may cause the image to transfer totally\n        content_loss = tf.add_n([tf.reduce_mean((content_outputs[name] - contents_targets[name])**2) for name in content_outputs.keys()])\n        style_loss = tf.add_n([tf.reduce_mean((style_outputs[name] - style_targets[name])**2) for name in style_outputs.keys()])\n        #single metric to combine both loss\n        total_loss = (content_loss * content_weights \/ num_content_layers) + (style_loss * style_weights \/ num_style_layers)\n    \n    #calculate gradient of loss with respect to image\n    gradient = tape.gradient(total_loss, transferred_image)\n    \n    optimizer.apply_gradients([(gradient, transferred_image)]) #apply the optimizer on the image to tune the model\n    \n    transferred_image.assign(tf.clip_by_value(transferred_image, 0.0, 1.0)) # clip the values in the range [0, 1] \n    \n    if((epoch +1) % display_epoch == 0): #display interval\n        print('Epoch : {} | Content Loss : {} | Style Loss : {} | Total Loss : {}'.format(epoch+1, content_loss, style_loss, total_loss))\n        plt.axis('off')\n        plt.imshow(tf.squeeze(transferred_image, axis = 0)); # shape = (1, 448, 598, 3) => (448, 589, 3)\n        plt.show()","faaf108e":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(30, 10))\nax1.axis('off')\nax1.imshow(tf.squeeze(content_image, axis=0))\nax1.set_title('Content')\n\nax2.axis('off')\nax2.imshow(tf.squeeze(style_image, axis=0))\nax2.set_title('Style')\n\nax3.axis('off')\nax3.imshow(tf.squeeze(transferred_image, axis=0))\nax3.set_title('Transferred Image')","0d0ec414":"# Building Model\n\nFrom the VGG19 network, it is found that the below layers outputs contents and style information\nTherefore, the style layers' outputs from one image are extracted and applied to content layers of another image to transfer style","493d37e6":"# Load VGG19","9ae76cc4":"# Load And Preprocess Images","d10ee8bc":"# Import Libraries","1e645b88":"# Visualizing the Results","9a1bcad4":"# Training","c07044a6":"## Style Image","2c5225b4":"## Content Image"}}