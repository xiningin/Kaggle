{"cell_type":{"1e0de700":"code","0dc92a3f":"code","7ea6c5e3":"code","48a36544":"code","b1ca3020":"code","269c6a25":"code","88b4c06e":"code","e12f2aa7":"code","c302ae16":"code","38c9b03d":"code","c8a0cc3d":"code","4b9a5036":"code","816f2147":"code","d0053088":"code","8336189a":"code","b4987011":"code","0f6e5ec8":"code","db30b83b":"code","849b7880":"code","316fe399":"code","dd6f2cb3":"code","196531cb":"code","0a95d6ad":"code","c038916f":"code","4cd44c34":"code","63d70760":"code","cb59f929":"code","5b6c5cb1":"code","a05bb3c6":"code","3b50d6ec":"code","dc8ea372":"code","d05dc765":"code","399ad273":"code","6065d183":"markdown","b1d1468b":"markdown","c7ea369e":"markdown","414cd4c5":"markdown","5d5845d1":"markdown","6525af5d":"markdown","4d3e1e01":"markdown","af5160a2":"markdown","638a3921":"markdown","98e2bd56":"markdown","6021dc5a":"markdown","375a4028":"markdown","35e512bc":"markdown","34c3ff80":"markdown","0ef0cb08":"markdown","79e2efb8":"markdown","83f8ba1f":"markdown","6749e488":"markdown","849b1deb":"markdown","3faae1ff":"markdown","8749defc":"markdown","ee4531b6":"markdown"},"source":{"1e0de700":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0dc92a3f":"# Importing libraries and resources \nimport plotly.graph_objects as go\nimport seaborn as sns\n%matplotlib inline\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom sklearn.preprocessing import LabelBinarizer\nimport statsmodels.api as sm\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeRegressor \nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn import ensemble\n","7ea6c5e3":"# Reading the data\ndata=pd.read_csv(\"\/kaggle\/input\/california-housing-prices\/housing.csv\")","48a36544":"# Visualizing the data and the dataset dimensions\nprint(data.shape)\ndata.head()","b1ca3020":"# Ocean proximity to dummie\n\ndummies=LabelBinarizer().fit_transform(data['ocean_proximity'])\ndata=data.join(pd.DataFrame(dummies,columns=[\"<1H OCEAN\",\"INLAND\",\"ISLAND\",\"NEAR BAY\",\"NEAR OCEAN\"]))\n\ndata.tail(2)","269c6a25":"# Missing data\ndata.isnull().sum()","88b4c06e":"# filling missing data\n\nfor i in data['ocean_proximity'].unique():\n    median=data[data['ocean_proximity']==i]['total_bedrooms'].median()\n    data.loc[data['ocean_proximity']==i,'total_bedrooms'] =  data[data['ocean_proximity']==i]['total_bedrooms'].fillna(median)\n    \ndata.isnull().sum()","e12f2aa7":"# Map chart: Visualizing the Median House Value\n\ncalifornia_img=mpimg.imread('\/kaggle\/input\/housingprices\/california.png')\n\n\ndata.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", title='Median House Value',\n    s=data['population']\/100, label=\"population\",\n    c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),\n    colorbar=True, alpha=0.7, figsize=(10,7),\n)\n\n\nplt.ylabel(\"Latitude\", fontsize=10)\nplt.xlabel(\"Longitude\", fontsize=10)\n\nplt.imshow(california_img, extent=[-124.55, -113.80, 32.45, 42.05], alpha=1)\n\nplt.legend()\nplt.show()","c302ae16":"# interactive map chart with plotly \n\nfig = go.Figure(data=go.Scattergeo(\n        lon = data['longitude'],\n        lat = data['latitude'],\n        text = data['median_house_value'],\n        mode = 'markers',\n        marker_color = data['median_house_value'],\n        ))\n\nfig.update_layout(\n        title = 'Median House Value in California',\n        geo_scope='usa')\n    \nfig.show()","38c9b03d":"# ocean_proximity analysis \n\nfig=plt.figure(figsize=(17, 4))\n\nplt.subplot(131)\ng = sns.countplot(data=data,x=\"ocean_proximity\",palette=\"Blues\",orient=\"v\",dodge=True).set_title('Ocean Proximity Count')\n\nplt.subplot(132)\nsns.boxplot( x=data[\"ocean_proximity\"], y=data[\"median_house_value\"], palette=\"Blues\").set_title('Median House Value Boxplot by Ocean Proximity')\n\nplt.tight_layout()\nplt.show()\n","c8a0cc3d":"# Histograma de median_house_value\nfrom scipy.stats import iqr\nfig = plt.figure(figsize=(20, 6))\n\nbin_width = 2 * iqr(data[\"median_house_value\"]) \/ len(data)**(1\/3)\nnum_bins = (np.max(data[\"median_house_value\"]) - np.min(data[\"median_house_value\"])) \/ bin_width\n\nplt.subplot(131)\n(sns.distplot(data[\"median_house_value\"], bins = \"fd\", norm_hist = True, kde = False, color = \"skyblue\", hist_kws = dict(alpha = 1))\n    .set(xlabel = \"median_house_value\", ylabel = \"Density\", title = \"Median House Value Histogram\"));\n\nplt.subplot(132)\nsns.boxplot(y=data[\"median_house_value\"], color=\"skyblue\").set_title('Median House Value Boxplot')\n\nplt.tight_layout()\nplt.show()\n","4b9a5036":"# Descriptive analysis \ndata[['longitude','latitude','housing_median_age','total_rooms','total_bedrooms','population','households','median_income','median_house_value']].describe()","816f2147":"# Histogram\n\nfig = plt.figure(figsize=(15, 5))\n\nplt.subplot(131)\n\n(sns.distplot(data[\"housing_median_age\"], bins = \"fd\", norm_hist = True, kde = False, color = \"skyblue\", hist_kws = dict(alpha = 1))\n    .set(xlabel = \"Housing Median Age\", ylabel = \"Density\", title = \"Median House Age Histogram\"));\n\nplt.subplot(132)\n\n(sns.distplot(data[\"total_rooms\"], bins = \"fd\", norm_hist = True, kde = False, color = \"skyblue\", hist_kws = dict(alpha = 1))\n    .set(xlabel = \"Total Rooms\", ylabel = \"Density\", title = \"Total Rooms Histogram\"));\n\nplt.subplot(133)\n\n(sns.distplot(data[\"total_bedrooms\"], bins = \"fd\", norm_hist = True, kde = False, color = \"skyblue\", hist_kws = dict(alpha = 1))\n    .set(xlabel = \"Total Bedrooms\", ylabel = \"Density\", title = \"Total Bedrooms Histogram\"));\n\nplt.tight_layout()\nplt.show()\n\n# Boxplot\n\nfig = plt.figure(figsize=(15, 5))\n\nplt.subplot(131)\nsns.boxplot(y=data[\"housing_median_age\"], color=\"skyblue\").set_title('Median House Age Boxplot')\n\nplt.subplot(132)\nsns.boxplot(y=data[\"total_rooms\"], color=\"skyblue\").set_title('Total Rooms Boxplot')\n\nplt.subplot(133)\nsns.boxplot(y=data[\"total_bedrooms\"], color=\"skyblue\").set_title('Total Bedrooms Boxplot')\n\nplt.tight_layout()\nplt.show()\n\n\n# Histogram\n\nfig = plt.figure(figsize=(15, 5))\n\nplt.subplot(131)\n\n(sns.distplot(data[\"population\"], bins = \"fd\", norm_hist = True, kde = False, color = \"skyblue\", hist_kws = dict(alpha = 1))\n    .set(xlabel = \"Population\", ylabel = \"Density\", title = \"Population Histogram\"));\n\nplt.subplot(132)\n\n(sns.distplot(data[\"households\"], bins = \"fd\", norm_hist = True, kde = False, color = \"skyblue\", hist_kws = dict(alpha = 1))\n    .set(xlabel = \"Households\", ylabel = \"Density\", title = \"Households Histogram\"));\n\nplt.subplot(133)\n\n(sns.distplot(data[\"median_income\"], bins = \"fd\", norm_hist = True, kde = False, color = \"skyblue\", hist_kws = dict(alpha = 1))\n    .set(xlabel = \"Median Income\", ylabel = \"Density\", title = \"Median Income Histogram\"));\n\nplt.tight_layout()\nplt.show()\n\n\n# Boxplot \n\nfig = plt.figure(figsize=(15, 5))\n\nplt.subplot(131)\nsns.boxplot(y=data[\"population\"], color=\"skyblue\").set_title('Population Boxplot')\n\nplt.subplot(132)\nsns.boxplot(y=data[\"households\"], color=\"skyblue\").set_title('Households Boxplot')\n\nplt.subplot(133)\nsns.boxplot(y=data[\"median_income\"], color=\"skyblue\").set_title('Median Income Boxplot')\n\nplt.tight_layout()\nplt.show()\n","d0053088":"(data[['longitude','latitude','housing_median_age','total_rooms','total_bedrooms','population','households','median_income','median_house_value']]\n .corr().style.background_gradient( axis=None))","8336189a":"# Removing categorical column  \ndata=data.drop(['ocean_proximity'],axis=1)\nprint(data.shape)\ndata.head()","b4987011":"#  splitting the data into attributes and labels:\n#  response variable: meadian_house_value\n\nX = data.drop(['median_house_value'],axis=1).values\ny = data.iloc[:, 8].values","0f6e5ec8":"# splitting the data into training and testing sets:\n# 20% for testing \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)","db30b83b":"# feature scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","849b7880":"# Linear Regression \n\nfrom sklearn.linear_model import LinearRegression\n\nregressor_linear = LinearRegression()\n\nregressor_linear.fit(X_train,y_train)\n\ny_pred_lr = regressor_linear.predict(X_test) \n","316fe399":"print('Mean Absolute Error:', np.round(metrics.mean_absolute_error(y_test, y_pred_lr),2))\n\nprint('Mean Squared Error:', np.round(metrics.mean_squared_error(y_test, y_pred_lr),2))\n\nprint('Root Mean Squared Error:', np.round(np.sqrt(metrics.mean_squared_error(y_test, y_pred_lr)),2))","dd6f2cb3":"# residual plot\nplt.figure(figsize=(12,7))\n(sns.distplot((y_test-y_pred_lr), bins = \"fd\", norm_hist = True, kde = False, color = \"skyblue\", hist_kws = dict(alpha = 1))\n    .set(xlabel = \"(y_test-y_pred)\", ylabel = \"Density\", title = \"Regression Tree Residual Plot\"));","196531cb":"# training the model\nregressor_tree = DecisionTreeRegressor(random_state = 123)  \nregressor_tree.fit(X_train, y_train)\ny_pred_tree = regressor_tree.predict(X_test) ","0a95d6ad":"# evaluating the algorithm \n\nprint('Mean Absolute Error:', np.round(metrics.mean_absolute_error(y_test, y_pred_tree),2))\n\nprint('Mean Squared Error:', np.round(metrics.mean_squared_error(y_test, y_pred_tree),2))\n\nprint('Root Mean Squared Error:', np.round(np.sqrt(metrics.mean_squared_error(y_test, y_pred_tree)),2))","c038916f":"# residual plot\nplt.figure(figsize=(12,7))\n(sns.distplot((y_test-y_pred_tree), bins = \"fd\", norm_hist = True, kde = False, color = \"skyblue\", hist_kws = dict(alpha = 1))\n    .set(xlabel = \"(y_test-y_pred)\", ylabel = \"Density\", title = \"Regression Tree Residual Plot\"));","4cd44c34":"# training the model\n\nregressor_bagging = BaggingRegressor(n_estimators=200,random_state=123)\nregressor_bagging=regressor_bagging.fit(X_train,y_train)\ny_pred_bagging = regressor_bagging.predict(X_test)\n","63d70760":"# evaluating the algorithm \n\nprint('Mean Absolute Error:', np.round(metrics.mean_absolute_error(y_test, y_pred_bagging),2))\n\nprint('Mean Squared Error:', np.round(metrics.mean_squared_error(y_test, y_pred_bagging),2))\n\nprint('Root Mean Squared Error:', np.round(np.sqrt(metrics.mean_squared_error(y_test, y_pred_bagging)),2))\n\n","cb59f929":"# residual plot\nplt.figure(figsize=(12,7))\n(sns.distplot((y_test-y_pred_bagging), bins = \"fd\", norm_hist = True, kde = False, color = \"skyblue\", hist_kws = dict(alpha = 1))\n    .set(xlabel = \"(y_test-y_pred)\", ylabel = \"Density\", title = \"RF Residual Plot\"));\n","5b6c5cb1":"# training the model\nregressor_rf = RandomForestRegressor(n_estimators=200, random_state=123)\nregressor_rf.fit(X_train, y_train)\ny_pred_rf = regressor_rf.predict(X_test)","a05bb3c6":"# evaluating the algorithm \n\nprint('Mean Absolute Error:', np.round(metrics.mean_absolute_error(y_test, y_pred_rf),2))\n\nprint('Mean Squared Error:', np.round(metrics.mean_squared_error(y_test, y_pred_rf),2))\n\nprint('Root Mean Squared Error:', np.round(np.sqrt(metrics.mean_squared_error(y_test, y_pred_rf)),2))\n","3b50d6ec":"# residual plot\nplt.figure(figsize=(12,7))\n(sns.distplot((y_test-y_pred_rf), bins = \"fd\", norm_hist = True, kde = False, color = \"skyblue\", hist_kws = dict(alpha = 1))\n    .set(xlabel = \"(y_test-y_pred)\", ylabel = \"Density\", title = \"RF Residual Plot\"));\n","dc8ea372":"# setting the parameters\nparams = {\n    'n_estimators': 200,\n    'learning_rate': 0.015,\n    'max_depth': 10,\n    'min_samples_split': 2,\n    'loss': 'ls',\n}\n\n# training the model\nregressor_boosting = ensemble.GradientBoostingRegressor(**params)\nregressor_boosting.fit(X_train, y_train)\ny_pred_boosting=regressor_boosting.predict(X_test)","d05dc765":"# evaluating the algorithm \n\nprint('Mean Absolute Error:', np.round(metrics.mean_absolute_error(y_test, y_pred_boosting),2))\n\nprint('Mean Squared Error:', np.round(metrics.mean_squared_error(y_test, y_pred_boosting),2))\n\nprint('Root Mean Squared Error:', np.round(np.sqrt(metrics.mean_squared_error(y_test, y_pred_boosting)),2))\n\n","399ad273":"# residual plot\nplt.figure(figsize=(12,7))\n(sns.distplot((y_test-y_pred_boosting), bins = \"fd\", norm_hist = True, kde = False, color = \"skyblue\", hist_kws = dict(alpha = 1))\n    .set(xlabel = \"(y_test-y_pred)\", ylabel = \"Density\", title = \"Boosting Residual Plot\"));\n\n","6065d183":"Random Forest is an evolution of bagging. The Random Forest model provide an <b>improvement over bagged trees<\/b> by way of a small tweak that <b>decorrelates the trees<\/b>.\n\nAs in bagging, we build a number of decision trees on bootstrapped training samples. But when building these decision trees, each time a split in a tree is considered, a <b>random sample of m predictors is chosen as split candidates from the full set of p predictors<\/b>. The idea behind this process is to decorrelate the trees, for example: Suppose that there is one very strong predictor in the data set, along with a number of other moderately strong predictors. Then in the collection of bagged trees, most or all of the trees will use this strong predictor in the top split. Consequently, all of the bagged trees will look quite similar to each other. Hence the predictions from the bagged trees will be highly correlated.\n\nIn Random Forest <i>p<\/i> is the full set of predictors and <i> m <\/i> is the predictors taken at each split. So, <b>the main difference between bagging and random forests is the choice of predictor subset size m<\/b>. For instance, if a random forest is built using m = p, then this amounts simply to bagging.\n\n    ","b1d1468b":"There are missing data for the variable total bedrooms!\nTo overcome this issue we will calculate the median for total bedrooms based upon categories of ocean proximity column.","c7ea369e":"# Linear Regression","414cd4c5":"The map chart indicates that the price of the houses is more expensive near to the ocean. The Boxplot is inline with the map, once we can easily note that houses inland are cheaper.\nThe island houses are much more expensive and outnumbered.\n","5d5845d1":"The charts and descriptive analysis above help us to visualize the distribution of all numerical variables. ","6525af5d":"# Boosting\n\nAs showed earlier, bagging involves creating multiple copies of the original training data set using the bootstrap, fitting a separate decision tree to each copy, and then combining all of the trees in order to create a single predictive model. Like bagging, <b>boosting involves combining a large number of decision trees, but boosting does not involve bootstrap sampling<\/b>. So, how does improve the perfomance? The key idea behind boosting is to <b>fit the new predictor to the residual errors made by the previous regression tree<\/b>. That is, we fit a tree using the current residuals, rather than the outcome Y , as the response. We then add this new decision tree into the fitted function in order to update the residuals. This process allows to slowly improve the predictions in areas where it does not perform well.\n\nBoosting has three tuning parameters: \n<i>\n1. The number of trees B. We can use cross-validation to select B. \n\n2. The shrinkage parameter \u03bb, a small positive number (typical values are 0.01 or 0.001). This controls the rate at which boosting learns. \n\n3. The number $d$ of splits in each tree.\n\n<\/i>\n\nHere we can see a general Boosting algorithm for regression problems. \n\n1. Set $\\hat{f}(x)=0$ and $r_{i}=y_{i}$ for $i$ in the training set.\n\n\n2. For b in 1,2,3...,B, repeat: \n\n   (a) Fit a tree $\\hat{f}^{b}$ with $d$ splits ($d$+1 terminal nodes) to the training data (X,$r$).\n   \n   (b) Update  $\\hat{f}$ by adding in a shrunken version of the new tree: $ \\hat{f}(x) \\leftarrow \\hat{f}(x) +\\lambda \\hat{f}^{b}(x)  $\n   \n   (c) Update the residuals: $r_{i} \\leftarrow r_{i} +\\lambda \\hat{f}^{b}(x_{i}) $\n   \n   \n\n3. Output the boosted model: $\\hat{f}(x) = \\sum_{b=1}^{B}\\lambda \\hat{f}^{b}(x_{i})$\n\n\nUnlike bagging and random forests, boosting can overfit if B is too large. Therefore it is important to make sure we are using validation splits\/cross-validation to make sure we are not overfitting our Boosting models. \n\nThe main idea behind Boosting is <b>to combine weak learner after weak learner<\/b>, our final model is able to account for a lot of the error from the original model <b>and reduces this error over time.<\/b>\n\nIn this case, we are going to use gradient boosting.\n","4d3e1e01":"As discussed above, <b>Decision Trees suffer from high variance<\/b>, bagging is a method that aims improve the perfome of the Decision Trees. An intuitive alternative to reduce the variance and hence <b>increase the prediction accuracy is to take many training sets from the population<\/b>, build a separate prediction model using each training set and avarage the resulting predictions. Where B is the number of separate training sets.\n<p style=\"font-size:120%;\">\\begin{equation*}{\\hat{f}}_{avg}(x)=\\frac{1}{B}\\sum_{b=1}^{B}\\hat{f}^{b}(x)\\end{equation*}<\/p>\n\nOf course, this is not practical because we <b>generally do not have access to multiple training sets<\/b>.\nInstead, we can <b>bootstrap, by taking repeated samples from the (single) training data set<\/b>. In this approach we generate B different bootstrapped training data sets.\n\n<p style=\"font-size:120%;\">\\begin{equation*}{\\hat{f}}_{bag}(x)=\\frac{1}{B}\\sum_{b=1}^{B}\\hat{f}^{*b}(x)\\end{equation*}<\/p>\n\nBefore continue into Bagging, lets go deeper into the concept of <b>Boostrap.<\/b>\n\nThe figure bellow is a graphical illustration of the bootstrap approach on a small sample containing n = 3 observations. Each bootstrap dataset contains n observations, sampled with replacement from the original dataset. Each bootstrap dataset is used to obtain an estimate of \u03b1.\n\nAltough Bootstrapping can usefull for many statistical learning methods, here we are foccusing on <b>applying bagging for Decision Trees in regression problems.<\/b>. So, how does it works? \n<i>\n1. Select a orginal dataset (training set).\n\n2. Randomly select samples from the origial dataset allowing replacement to create a new dataset with the same size as the original dataset. Each re-sample will be called a Bootstrapped dataset and we will have a total of B Bootstrapped datasets. \n\n3. Generate a decision tree (regression tree) for each bootstrapped dataset considering all variables (we will discuss the details of it on Random Forest) to obtain an estimate of \u03b1.\n\n4. As its regression problem, we should average the predictions of all B trees to get our bagging prediction result. \n<\/i>\nDuring the <b>bagging process<\/b>, all the <b>trees are grown deep<\/b>, and are not pruned. Hence <b>each individual tree has high variance, but low bias. Averaging these B trees reduces the variance. <\/b>\n\nThe number of trees B is not a critical parameter with bagging; using a <b>very large value of B will not lead to overfitting.<\/b>\n\nLearn more: https:\/\/www.youtube.com\/watch?v=J4Wdy0Wc_xQ\n\n","af5160a2":"The charts above show the median_house_value distribution. \nAn interesting fact is that there is a high frequency on the value of 50000. Most likely, this value is a cap, hence all the median_house_value above 50000 was reduced to this value. ","638a3921":"# Bagging ","98e2bd56":"We can see above, that the variable regressor_linear is a Linear Regression model trained from the variables X_train and y_train. To train the model means that we are looking for the line that better fits the training data, to do so we will use the <b>predict()<\/b> function. \n\n![title](linear.png)\n","6021dc5a":"Linear regression models are useful for prediction and to explain variation in the response variable. In this case, we will focus on prediction. So we will fit a predictive model to an observed data set of values of the response and explanatory variables. \n\n<p style=\"font-size:120%;\">\\begin{equation*}Y= \\beta _{0} + \\beta _{1}X_{1}+\\beta _{2}X_{2}+\\varepsilon\\end{equation*}<\/p>\n\nTo run the linear model we will use a class from Scikit-learn called <b> Linear Regression() <\/b> this class has a function called <b> fit() <\/b>, which will train our data.","375a4028":"The variable ocean_proximty is Categorical. As we will run regression models, it's important to create a set o binaries (or dummies) variables to represent ocean_proximty. This transformation will make the results interpretable (mostly for linear regression). So, let's do it! ","35e512bc":"# Understanding the dataset","34c3ff80":"Decision Trees methods involve stratifying or segmenting the predictor space into a number of simple regions. Decision Trees are <b>simple and useful for interpretation<\/b>, however they are typically <b>not competitive in terms of prediction accuracy<\/b>.","0ef0cb08":"# Training and Test Set","79e2efb8":"1. longitude: A measure of how far west a house is; a higher value is farther west\n2. latitude: A measure of how far north a house is; a higher value is farther north\n3. housingMedianAge: Median age of a house within a block; a lower number is a newer building\n4. totalRooms: Total number of rooms within a block\n5. totalBedrooms: Total number of bedrooms within a block\n6. population: Total number of people residing within a block\n7. households: Total number of households, a group of people residing within a home unit, for a block\n8. medianIncome: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n9. medianHouseValue: Median house value for households within a block (measured in US Dollars)\n10. oceanProximity: Location of the house w.r.t ocean\/sea","83f8ba1f":"# Regression Trees","6749e488":"The correlation matrix is a good way to visualize how the variables behave along with each other. It's also very useful for linear regression problems, once it helps us to select regressors high correlated with the response variables. As well as it will help us to avoid multicollinearity between the regressors.","849b1deb":"Decision trees can be applied to both <b>regression and classification problems<\/b>. At this task, we are going to use <b>Regression Trees<\/b>\n\n<p style=\"color:green;\">Advantages of Decision Trees: <\/p>\n\n1. Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression!\n2. Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches seen in previous chapters.\n3. Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small).\n4. Trees can easily handle qualitative predictors without the need to create dummy variables, \n\n<p style=\"color:red;\">Disadvantages of Decision Trees: <\/p>\n\n1. Unfortunately, trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches seen in this book.\n2. Additionally, trees can be very non-robust. In other words, a small change in the data can cause a large change in the final estimated tree (high variance). \n\nHowever, by aggregating many decision trees the predictive performance of trees can be substantially improved. We will evaluate these models in the next cells. \n","3faae1ff":"On the step above, we could select just some variable aiming to improve the model's performance, by removing some high correlated regressors or by testing the performance of each model. But we are not going to do it, we will simply run all the models (Linear Regression, Decision Trees, Random Forest and Boosting) with all the variables. \n\nThe idea here is to explain and show how each model works, without trying to improve the prediction performance. (At least for now). ","8749defc":"Now, we will try to predict the median_house_value. So we will split the data into training and test set.","ee4531b6":"# Random Forest"}}