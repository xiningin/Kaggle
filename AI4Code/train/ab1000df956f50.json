{"cell_type":{"49de2e29":"code","3b551cc5":"code","1afd2891":"code","e75da24c":"code","7bc7de17":"code","a672dd53":"code","fd8869f7":"code","a3224879":"code","e14a384e":"code","36c9e471":"markdown","42b8726c":"markdown","b17af114":"markdown"},"source":{"49de2e29":"!git clone https:\/\/github.com\/sattree\/gpr_pub.git\n!wget http:\/\/nlp.stanford.edu\/software\/stanford-corenlp-full-2018-10-05.zip\n!unzip stanford-corenlp-full-2018-10-05.zip\n!pip install https:\/\/github.com\/huggingface\/neuralcoref-models\/releases\/download\/en_coref_sm-3.0.0\/en_coref_sm-3.0.0.tar.gz\n!pip install stanfordcorenlp\n!pip install allennlp --ignore-installed greenlet\n# Huggingface neuralcoref model has issues with spacy-2.0.18\n!conda install -y cymem==1.31.2 spacy==2.0.12","3b551cc5":"from IPython.core.display import display, HTML\n# Add css styles and js events to DOM, so that they are available to rendered html\ndisplay(HTML(open('gpr_pub\/visualization\/highlight.css').read()))\ndisplay(HTML(open('gpr_pub\/visualization\/highlight.js').read()))","1afd2891":"import pandas as pd\n\nimport en_coref_sm\nfrom stanfordcorenlp import StanfordCoreNLP\nfrom nltk.parse.corenlp import CoreNLPParser\nfrom allennlp.predictors.predictor import Predictor\n\nfrom gpr_pub import visualization","e75da24c":"# Instantiate stanford corenlp server\nSTANFORD_CORENLP_PATH = 'stanford-corenlp-full-2018-10-05\/'\nPORT = 9090\ntry:\n    server = StanfordCoreNLP(STANFORD_CORENLP_PATH, port=PORT, quiet=True)\nexcept OSError as e:\n    print('The port is occupied, probably an instance is already running.')\n    server = StanfordCoreNLP('http:\/\/localhost', port=PORT, quiet=True)\n    \nSTANFORD_SERVER_URL = server.url\nALLENNLP_COREF_MODEL_PATH = 'https:\/\/s3-us-west-2.amazonaws.com\/allennlp\/models\/coref-model-2018.02.05.tar.gz'","7bc7de17":"# create model instances\nstanford_model = CoreNLPParser(url=STANFORD_SERVER_URL)\nallennlp_model = Predictor.from_path(ALLENNLP_COREF_MODEL_PATH)\nhuggingface_model = en_coref_sm.load()\n\n# If annotators are not preloaded, stanford model can take a while for the first call and may even timeout\n# make a dummy call to the server\ntry:\n    stanford_model.api_call('This is a dummy text.', properties={'annotators': 'coref'})\nexcept:\n    pass","a672dd53":"# Load GPR data\ntrain = pd.read_csv('gpr_pub\/data\/gap-development.tsv', sep='\\t')\n# normalizing column names\ntrain.columns = map(lambda x: x.lower().replace('-', '_'), train.columns)\nwith pd.option_context('display.max_rows', 10, 'display.max_colwidth', 15):\n    display(train)","fd8869f7":"# 'proref' is a special case that handles highlighting pronoun references that may be present\n# in ground truth or predictions\n# the renderer expects ['text', 'pronoun', 'pronoun_offset', 'a_coref', 'a_offset', 'b_coref', 'b_offset'] \n# keys (columns) to be present in the input (row)\nrow = train.loc[0]\nvisualization.render(row, proref=True, jupyter=True)","a3224879":"rows = []\nfor idx, row in train.iterrows():\n    data = stanford_model.api_call(row.text, properties={'annotators': 'coref'})\n    html = visualization.render(data, stanford=True, jupyter=False)\n    rows.append({'sample_idx': idx, \n                 'model': 'Stanford',\n                 'annotation': html})\n    \n    data = allennlp_model.predict(row.text)\n    html = visualization.render(data, allen=True, jupyter=False)\n    rows.append({'sample_idx': idx, \n                 'model': 'AllenNlp',\n                 'annotation': html})\n    \n    data = huggingface_model(row.text)\n    html = visualization.render(data, huggingface=True, jupyter=False)\n    rows.append({'sample_idx': idx, \n                 'model': 'Huggingface',\n                 'annotation': html})\n    \n    # Special rendering for labelled pronouns, either gold or predicted\n    # labels in 'a_coref', 'b_coref'\n    html = visualization.render(row, proref=True, jupyter=False)\n    rows.append({'sample_idx': idx, \n                 'model': 'GPR',\n                 'annotation': html})\n    \n    break\n\ndf = pd.DataFrame(rows).groupby(['sample_idx', 'model']).agg(lambda x: x)\ns = df.style.set_properties(**{'text-align': 'left'})\ndisplay(HTML(s.render(justify='left')))","e14a384e":"!rm -r stanford-corenlp-full-2018-10-05\/\n!rm -r gpr_pub\/\n!rm stanford-corenlp-full-2018-10-05.zip","36c9e471":"**Coreference visualization for jupyter notebooks (Code repository: https:\/\/github.com\/sattree\/gpr_pub)**\n\nAllenNLP style highlighting of mention clusters (https:\/\/demo.allennlp.org\/coreference-resolution\/NjA2MjY3**) extended to stanford corenlp, huggingface, and pronoun resolutions\n***\nThis notebook demonstrates two contributions made to the gpr visualization task:\n1. Extend the visualization code logic for rendering in jupyter notebooks - allennlp functionality provides these visualizations only through a web app interface and is natively built in js.\n1. Extend the visualization api to cover and provide a uniform interface for stanford and huggingface coref apis, and to also handle pronoun resolution labels.\n\nVisualization renderer has a displacy (spacy) style api interface, again aimed at maintaining uniformity in interfaces.\n\nI found allennlp entity highlighting and linking type of visualizations to be better suited for longer text snippets as opposed to the spacy dependency style visualizations offered by huggingface.\n\nThis kernel is the first in a tri-series of self-contained installments to introduce the GPR problem.\n1. **Coref visualization**\n1. Reproducing GAP results - achieves a logloss score of 0.84\n1. A better baseline - without any training\n\nBy no means am I implying that this series is a comprehensive coverage of the problem. There are numerous wonderful kernels available in the competition to that effect. The aim of this series is to provide a good starting point for fellow participants to hit the ground running.\n***","42b8726c":"Download and install all dependencies\n* gpr_pub - contains code for visualizations","b17af114":"Hope you find these visualizations useful for your projects!\n\nStay tuned for the 2nd and 3rd installments..."}}