{"cell_type":{"a44f88aa":"code","3a2d3ced":"code","bb63a487":"code","827a2149":"code","e43d6534":"code","f959f324":"code","48540195":"code","aafcc7c1":"code","15a52f19":"code","64358a83":"code","bc261f5d":"code","4b8951ea":"code","fa1a2941":"code","c92c2f1f":"code","e1034f8e":"code","52053ae3":"code","5ea0e2a1":"markdown","4653818e":"markdown","210caf82":"markdown","26fa6b83":"markdown","6bde1e5a":"markdown","674fb3b3":"markdown","260580a6":"markdown","e8b704ee":"markdown","abd04763":"markdown","294fa1fb":"markdown","a347243b":"markdown","482209d4":"markdown","08bf7419":"markdown","13112cc8":"markdown","e4e09744":"markdown","de5ba6e4":"markdown"},"source":{"a44f88aa":"# references:\n# https:\/\/www.kaggle.com\/antmarakis\/cnn-baseline-model\n# http:\/\/www.wildml.com\/2015\/12\/implementing-a-cnn-for-text-classification-in-tensorflow\/\n# https:\/\/github.com\/yoonkim\/CNN_sentence\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nseed = 12345\nimport random\nimport numpy as np\nfrom tensorflow import set_random_seed\n\nrandom.seed(seed)\nnp.random.seed(seed)\nset_random_seed(seed)","3a2d3ced":"train,test,sampleSubmission = pd.read_csv('..\/input\/train.tsv', sep = '\\t'),pd.read_csv('..\/input\/test.tsv', sep = '\\t'),pd.read_csv('..\/input\/sampleSubmission.csv')\n\n# train,test,sampleSubmission = pd.read_csv('all\/train.tsv', sep = '\\t'),pd.read_csv('all\/test.tsv', sep = '\\t'),pd.read_csv('all\/sampleSubmission.csv')\ntrain.head(3)\n","bb63a487":"# From Yoon Kim work:\n# https:\/\/github.com\/yoonkim\/CNN_sentence\/blob\/master\/process_data.py\nimport re\ndef clean_str(string):\n    \"\"\"\n    Tokenization\/string cleaning for all datasets except for SST.\n    \"\"\"\n    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n    string = re.sub(r\"\\'s\", \" \\'s\", string) \n    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n    string = re.sub(r\"\\'re\", \" \\'re\", string) \n    string = re.sub(r\"\\'d\", \" \\'d\", string) \n    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n    string = re.sub(r\",\", \" , \", string) \n    string = re.sub(r\"!\", \" ! \", string) \n    string = re.sub(r\"\\(\", \" \\( \", string) \n    string = re.sub(r\"\\)\", \" \\) \", string) \n    string = re.sub(r\"\\?\", \" \\? \", string) \n    string = re.sub(r\"\\s{2,}\", \" \", string)    \n    return string.strip().lower()\n\nphrases = [clean_str(s) for s in train['Phrase']]","827a2149":"type(train['Phrase'])\nlen(phrases[2].split())\nphrases[2].split()","e43d6534":"from keras.utils import to_categorical\nY = to_categorical(train['Sentiment'].values)\nprint(Y[155:165])\nprint(train['Sentiment'].values[155:165])","f959f324":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# tokenizer = Tokenizer(num_words=max_features)\nt = Tokenizer()\nt.fit_on_texts(phrases)\nvocab_size = len(t.word_index) + 1\nX = t.texts_to_sequences(phrases)\n# print(X)\nmax_length = max([len(test.split()) for test in phrases ])\nX = pad_sequences(X,maxlen=max_length,padding = 'post')\n# print(X)\nprint(X.shape)","48540195":"from keras.layers import Embedding\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Activation\nfrom keras.layers import Flatten, Conv1D, SpatialDropout1D, MaxPooling1D,AveragePooling1D, merge, concatenate, Input, Dropout\n\n# ONE LAYER\ndef model(output_dim=8, max_length=50, y_dim=5, num_filters=5, filter_sizes = [3,5], pooling = 'max', pool_padding = 'valid', dropout = 0.2):\n    # Input Layer\n#     embed_input = Input(shape=(max_length,output_dim))\n    embed_input = Input(shape=(max_length,))\n    x = Embedding(vocab_size,output_dim,input_length=max_length)(embed_input)\n#     x = SpatialDropout1D(0.2)(x)\n    ## concat\n    pooled_outputs = []\n    for i in range(len(filter_sizes)):\n        conv = Conv1D(num_filters, kernel_size=filter_sizes[i], padding='valid', activation='relu')(x)\n        if pooling=='max':\n            conv = MaxPooling1D(pool_size=max_length-filter_sizes[i]+1, strides=1, padding = pool_padding)(conv)\n        else:\n            conv = AveragePooling1D(pool_size=max_length-filter_sizes[i]+1, strides=1, padding = pool_padding)(conv)            \n        pooled_outputs.append(conv)\n    merge = concatenate(pooled_outputs)\n        \n    x = Flatten()(merge)\n    x = Dropout(dropout)(x)\n#     predictions = Dense(y_dim, activation = 'sigmoid')(x)\n    predictions = Dense(y_dim, activation = 'softmax')(x) # TEST\n    \n    model = Model(inputs=embed_input,outputs=predictions)\n\n    model.compile(optimizer='adam',loss = 'categorical_crossentropy', metrics = ['acc'])\n    print(model.summary())\n    \n    from keras.utils import plot_model\n    plot_model(model, to_file='shared_input_layer.png')\n    \n    return model\n\n\nmodel = model(output_dim=16, max_length=max_length,y_dim=5,filter_sizes = [3,4,5],pooling = 'max',dropout=0.5)\nfrom IPython.display import Image\nImage(filename='shared_input_layer.png') \n","aafcc7c1":"# ## PREVIOUS MODELS\n# def model1(output_dim=8, max_length=50, y_dim=5, filter_sizes = [3]):\n#     model = Sequential()\n#     model.add(Embedding(vocab_size,output_dim,input_length=max_length))\n#     model.add(Flatten())\n#     model.add(Dense(y_dim, activation = 'sigmoid'))\n\n#     model.compile(optimizer='adam',loss = 'categorical_crossentropy', metrics = ['acc'])\n#     print(model.summary())\n#     return model\n\n# def model2(output_dim=8, max_length=50, y_dim=5, num_filters=5, filter_sizes = [3,5]):\n#     model = Sequential()\n#     model.add(Embedding(vocab_size,output_dim,input_length=max_length))\n\n#     model.add(SpatialDropout1D(0.2))\n\n#     ## GOOD\n#     model.add(Conv1D(num_filters, kernel_size=filter_sizes[0], padding='valid', activation='relu'))\n#     model.add(MaxPooling1D(pool_size=max_length-filter_sizes[0]+1, strides=1, padding='valid'))\n\n#     model.add(Flatten())\n#     model.add(Dense(y_dim, activation = 'sigmoid'))\n\n#     model.compile(optimizer='adam',loss = 'categorical_crossentropy', metrics = ['acc'])\n#     print(model.summary())\n#     return model\n\n","15a52f19":"from sklearn.model_selection import train_test_split\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.25, random_state=seed)","64358a83":"epochs = 10\nbatch_size = 32\n\nmodel.fit(X_train,Y_train,epochs = epochs, validation_data=(X_val,Y_val), batch_size=batch_size, verbose = 1)\nloss,accuracy = model.evaluate(X_val,Y_val)\nprint('Accuracy: %f' % (accuracy*100))","bc261f5d":"# epochs = 10\n# batch_size = 32\n\n# model_avg = model(output_dim=16, max_length=max_length,y_dim=5,filter_sizes = [3,4,5],pooling = 'avg',dropout=0.5)\n\n# model_avg.fit(X_train,Y_train,epochs = epochs, validation_data=(X_val,Y_val), batch_size=batch_size, verbose = 1)\n# loss,accuracy = model_avg.evaluate(X_val,Y_val)\n# print('Accuracy: %f' % (accuracy*100))\n","4b8951ea":"# ONE LAYER\ndef model_constant_poolsize(output_dim=8, max_length=50, y_dim=5, num_filters=5, filter_sizes = [3,5], pooling = 'max', pool_padding = 'valid', pool_size=5,dropout=0.4):\n    # Input Layer\n#     embed_input = Input(shape=(max_length,output_dim))\n    embed_input = Input(shape=(max_length,))\n    x = Embedding(vocab_size,output_dim,input_length=max_length)(embed_input)\n#     x = SpatialDropout1D(0.2)(x)\n    ## concat\n    pooled_outputs = []\n    for i in range(len(filter_sizes)):\n        conv = Conv1D(num_filters, kernel_size=filter_sizes[i], padding='same', activation='relu')(x)\n        if pooling=='max':\n            conv = MaxPooling1D(pool_size=pool_size, strides=1, padding = pool_padding)(conv)\n        else:\n            conv = AveragePooling1D(pool_size=pool_size, strides=1, padding = pool_padding)(conv)            \n        pooled_outputs.append(conv)\n    merge = concatenate(pooled_outputs)\n        \n    x = Flatten()(merge)\n    x = Dropout(dropout)(x)\n#     predictions = Dense(y_dim, activation = 'sigmoid')(x)\n    predictions = Dense(y_dim, activation = 'softmax')(x) # TEST\n    \n    model = Model(inputs=embed_input,outputs=predictions)\n\n    model.compile(optimizer='adam',loss = 'categorical_crossentropy', metrics = ['acc'])\n    print(model.summary())\n    \n    from keras.utils import plot_model\n    plot_model(model, to_file='shared_input_layer_model_constant_poolsize.png')\n    \n    return model\n\n\nmodel_constant_poolsize = model_constant_poolsize(output_dim=16, max_length=max_length,y_dim=5,filter_sizes = [3,4,5],pooling = 'max', pool_size=5,dropout = 0.5)\nfrom IPython.display import Image\nImage(filename='shared_input_layer_model_constant_poolsize.png') \n","fa1a2941":"epochs = 10\nmodel_constant_poolsize.fit(X_train,Y_train,epochs = epochs, validation_data=(X_val,Y_val), batch_size=batch_size, verbose = 1)\nloss,accuracy = model_constant_poolsize.evaluate(X_val,Y_val)\nprint('Accuracy: %f' % (accuracy*100))\n","c92c2f1f":"# pre processing\ntest_phrases = [clean_str(s) for s in test['Phrase']]\n# text to tokens\nX_test = t.texts_to_sequences(test_phrases)\nX_test = pad_sequences(X_test,maxlen=max_length,padding = 'post')","e1034f8e":"# sampleSubmission['Sentiment'] = model.predict_classes(X_test,verbose=1)\nsampleSubmission['Sentiment'] = model_constant_poolsize.predict(X_test,verbose=1).argmax(axis=-1)\nsampleSubmission.to_csv('sub_cnn_constant_maxpool.csv', index=False)\n\n\nsampleSubmission['Sentiment'] = model.predict(X_test,verbose=1).argmax(axis=-1)\nsampleSubmission.to_csv('sub_cnn.csv', index=False)","52053ae3":"# train\nmodel_constant_poolsize.fit(X,Y,epochs = epochs, validation_data=(X_val,Y_val), batch_size=batch_size, verbose = 1)\n\n# submission\nsampleSubmission['Sentiment'] = model_constant_poolsize.predict(X_test,verbose=1).argmax(axis=-1)\nsampleSubmission.to_csv('sub_cnn_constant_maxpool_FULL.csv', index=False)","5ea0e2a1":"Same model where max pooling layer creates catches 5 words at a time.\nPrevious models maxpool was on the entire text, outputing 1 feature.\nThis will require 'same' convolution padding so all filters outputs will be same length and concatenate will be on same size metrices.\nWe also have more weights so lets increase dropout.","4653818e":"## Same with average pooling","210caf82":"# Create Submission","26fa6b83":"# Sentence preprocessing","6bde1e5a":"Val_acc here is meaningless as we are validating on subset of train data (definitely not best practice)\nTODO:\nCheck missclassified samples","674fb3b3":"# Embedding Layer","260580a6":"# Input","e8b704ee":"## Text to Tokens","abd04763":"- 52 words max length (52 length vector)\n- embedding dim of 8 ([52,8] embedding layer)\n- 3 parallel convolution layers to capture 3,4 and 5 words ([50,5],[49,5],[48,5] covolution output)\n- Each convolution layer is followed by a maxpool to capture 1 feature ([1,5] output from each convolution layer)\n- concatenate parallel lines\n- flatten and dense layer 20% dropout rate (15 neurons to 5 outputs)\n\n","294fa1fb":"# Train","a347243b":"## Submission on model trained on full data","482209d4":"Looks like best results came from the model where maxpooling outputs more than one feature.\nWe'll score test data on that model","08bf7419":"## Word Embedding + Model","13112cc8":"# One Hot Labels","e4e09744":"# Train\/Validatio Split","de5ba6e4":"Paralel convolution layers followed by maxpool (sizes 3,4 and 5) concatenated to dense layer.\n\nReference: Yoon Kim https:\/\/github.com\/yoonkim\/CNN_sentence"}}