{"cell_type":{"b5576f63":"code","23aec264":"code","3e8c2f44":"code","b63bc194":"code","3f99846f":"code","ff65f8cb":"code","ae9408ef":"code","1be17e14":"code","9b7d6268":"code","3effc995":"code","7bf5aa70":"code","e4ee252c":"code","8d9c9a47":"code","7bc7456c":"code","aa0dd1fa":"code","bfa941aa":"code","e893d3a3":"code","777c6fe2":"code","d1f67c1f":"code","e9498445":"code","7bb57fd4":"code","8c39ced2":"markdown","b37038ce":"markdown","281969f6":"markdown","f03c1571":"markdown","386d961f":"markdown","b5a880d0":"markdown","99d2b70c":"markdown","6da968ac":"markdown"},"source":{"b5576f63":"import os\nimport math\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split","23aec264":"# Load the data from the excel file and look at column names\nos.chdir(\"\/kaggle\/input\")\norig = pd.read_csv('user-knowledge\/User Knowledge.csv')\norig.columns","3e8c2f44":"# Keep only the columns containing the data about student's knowledge\nknowledge = orig.iloc[:,:5]\nknowledge.head()","b63bc194":"# Plot histograms of the featuers to visualize the data\nknowledge.hist(bins=50, figsize = (8,8))\nplt.show()","3f99846f":"# Perform k-Means Clustering with values of k from 1 to 10 and plot k v\/s Within Cluster Sum of Squares\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=400, n_init=20, random_state=0)\n    kmeans.fit(knowledge)\n    wcss.append(kmeans.inertia_)\n\nplt.plot(range(1, 11), wcss)\nplt.title('Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","ff65f8cb":"# K-Means Clustering with 3 clusters\nkmeans = KMeans(n_clusters=3, init='k-means++', max_iter=400, n_init=20, random_state=0)\nkmeans.fit(knowledge)\nk_class = kmeans.predict(knowledge)","ae9408ef":"# Using PCA and filtering 3 principal components for data visualization\npca = PCA(n_components=3)\nprincipalComponents = pca.fit_transform(knowledge)\nPDF = pd.DataFrame(data = principalComponents, columns = ['PC1', 'PC2', 'PC3'])","1be17e14":"# Add a column 'Class' to the data sets\nPDF.loc[:, 'Cluster'] = pd.Series(k_class)\nknowledge_class = knowledge.copy()\nknowledge_class['Class'] = k_class","9b7d6268":"# Count of points in each cluster\nPDF['Cluster'].value_counts()","3effc995":"# Assign a color to each cluster\nPDF['Color'] = PDF['Cluster'].map({0 : 'red', 1 : 'blue', 2 : 'green'})","7bf5aa70":"# Plot the first 2 principal components and color by cluster\na1 = PDF['PC1']\na2 = PDF['PC2']\na3 = PDF['PC3']\nc1 = PDF['Color']\nplt.scatter(a1, a2, c = c1, alpha=0.3, cmap='viridis')","e4ee252c":"# 3-D plot of the data using 3 principal components\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(a1, a2, a3, alpha = 0.4, c = c1)","8d9c9a47":"knowledge_class.groupby(['Class']).mean()","7bc7456c":"# Slipt the data into train and test data sets\nX = knowledge_class.iloc[:, :-1]\nY = knowledge_class.iloc[:, -1]\nxTrain, xTest, yTrain, yTest = train_test_split(X, Y, test_size = 0.25, random_state = 0)","aa0dd1fa":"# KNN for various values of k and plot of k v\/s accuracy\nfrom sklearn.neighbors import KNeighborsClassifier\naccuracy = []\nfor i in range(1,12):\n    knn = KNeighborsClassifier(n_neighbors = i).fit(xTrain, yTrain)\n    accuracy.append(knn.score(xTest, yTest))\n\nplt.plot(range(1,12), accuracy)\nplt.xlabel('k')\nplt.ylabel('Accuracy') \nplt.title('k v\/s Accuracy for KNN')","bfa941aa":"# KNN model and evaluation for optimal value of k (8 in this case)\nknn = KNeighborsClassifier(n_neighbors = accuracy.index(max(accuracy))+1).fit(xTrain, yTrain)\nknn_predictions = knn.predict(xTest)\nknn_accuracy = knn.score(xTest, yTest)\nknn_accuracy","e893d3a3":"knn_CM = confusion_matrix(yTest, knn_predictions) # KNN Confusion Matrix\nknn_CM","777c6fe2":"# Decision Tree Classifier and evaluation for optimal value of k\nfrom sklearn.tree import DecisionTreeClassifier\ndtree_model = DecisionTreeClassifier(max_depth = 2).fit(xTrain, yTrain) \ndtree_predictions = dtree_model.predict(xTest)\ndt_accuracy = dtree_model.score(xTest, yTest)\ndt_accuracy","d1f67c1f":"DT_CM = confusion_matrix(yTest, dtree_predictions) # Decision Tree confusion Matrix\nDT_CM","e9498445":"# Gaussian Naive Bayes model and evaluation for optimal value of k\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB().fit(xTrain, yTrain)\ngnb_predictions = gnb.predict(xTest)\ngnb_accuracy = gnb.score(xTest, yTest)\ngnb_accuracy","7bb57fd4":"NB_CM = confusion_matrix(yTest, gnb_predictions) # Naive Bayes confusion Matrix\nNB_CM","8c39ced2":"In this analysis, we will explore K-Means clustering and look closely at the elbow method.","b37038ce":"Next, we want to perform classification on unseen data and the new categorical target values of class. We can use multiclass classification methods in Machine Learning on this data. The data appears to be well separated in space as seen from the plots. First we will split the data into training and test sets. Then, we will train the Machine Learning models on the trainnig data and evaluate their performance on the test data. There are numerous ways to evaluate performance of the model. Here, we will use the most simple metric, accuracy to evaluate our models. \n\nThe algorithms to be used for this multi-class classification task and the reason why they were selected from the list of all algorithms are stated below:\n* KNN (K-Nearest Neighbors) - KNN uses distance as the metric and the labels for the dataset were also obtained using distance as the metric when we applied K-Means Clustering. Thus, KNN may perform well on this dataset.\n* Decision Tree Classifier - We almost always want to apply a few Machine Learning methods to any dataset and compare them based on a suitable evaluation metric rather than selecting one final model based only on intusion. Although decision tess may not perform best on a small data such as this one, they are highly interpretable.\n* Naive Bayes - Based on assumption that variables are independent and making a probabilistic estimation using  amaximum likelihood hypothesis, this algorithm is highly efficient as compared to other Machine Lerning models.","281969f6":"In the elbow method, the optimal number of clusters is chosen as the point beyond which the rate of decrease of the within clusters sum of squares starts to fall significantly. In some cases, we need not use the elbow method if we are certain about the number of clusters required. For example, in this case, suppose that we wanted to form 3 clusters of student's knowledge to be able to classify them in three different groups and potentially use different strategies to help them better their knowledge.","f03c1571":"We conclude  that the Naive Bayes classifier performed better than KNN and Decision Tree classifier based on the results of accuracy as can be verified by comparing the confusion matrices.","386d961f":"**THEORY**\n\nThe most common methods used for identifying clusters or classes in unlabelled data are: 1) K-Means Clustering and 2) Hierarchical Clustering. While both are used for the same purpose, their underlying techniques are different.\n\n***Comparison***: It is natural to wonder which medhod to choose when performing a clustering task. There are several points of cmparison between the two: While Hierarchical Clustering is highly interpretable by looking at the dendograms, it has a higher time complexiy O(n^2) as compated to K-Means Clustering which has a linear time complexity O(n). Even by iterating K-Means for different initial clusters, it would be more efficient for clustering large amounts of data. In contrast, K-Means clustering requires the data to be continuous while Hierarchical Clustering can be run on categorical data by defining a similarity metric  rather than distance.\n\nNote: If one of the features has a range of values much larger than the others, clustering will be completely dominated by that one feature. Hence, it is important to ensure that the range of the variables is similar by normalizing the data before clustering.\n\n***Number of clusters***: Sometimes we might know exactly what is the number of clusters required for further analysis. For example, while clustering the data for physical features of people for clustering them into small, medium and large sized, we know that k is 3. However in some cases we might not be pre-decided about the number of clusters. In those cases, if using K-Means Clustering, we may use the 'elbow method' to choose the optimal number of clusters or use our judgement to choose where to draw the line in the dendograms obtained from Hierarchical Clustering.","b5a880d0":"**Clustering and Multiclass Classification (Predictive Modeling \/ Machine Learning)**","99d2b70c":"Let us look at how to the 3 classes differ by calculating their averages on each column.","6da968ac":"**INTRODUCTION:**\n\nThe data contains real information about the student's knowledge status about the subject of Electrical DC Machines.It has been obtained from UCI ML Repo. It was the Ph.D. Thesis of Dr. Hamdi Tolga Kahraman back in 2009. It is an unlabelled dataset containing 5 features explained below:\n\n\nSTG (The degree of study time for goal object materials)\n<br>\nSCG (The degree of repetition number of user for goal object materials)\n<br>\nSTR (The degree of study time of user for related objects with goal object)\n<br>\nLPR (The exam performance of user for related objects with goal object)\n<br>\nPEG (The exam performance of user for goal objects)"}}