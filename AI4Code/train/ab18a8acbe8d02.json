{"cell_type":{"7a151ade":"code","6571ec23":"code","df9c7f88":"code","a7b89080":"code","2f4c0210":"code","6bc21335":"code","0558e4e1":"code","5db40129":"code","bdb60bce":"code","42a43108":"code","f096d745":"code","6d08e868":"code","dd38c8ed":"code","57aa7f75":"code","ddd9803f":"code","9fdf42af":"code","44e778d5":"code","92491736":"code","1776a9ea":"code","f9a5a00c":"code","2222650c":"code","bba30b01":"code","aec2d67a":"code","a9018717":"code","cecc77ad":"code","c2c30b7c":"code","c6d60637":"code","54ad9092":"code","66f0dda4":"code","00b0bd68":"code","bcace6ef":"code","fa937af7":"code","77617192":"code","71f65a08":"code","b7371162":"code","93a4c0c1":"code","407bb983":"markdown","12c5483b":"markdown","80ecc93f":"markdown","7c6ec9a2":"markdown","a09aed90":"markdown","ca8e6a00":"markdown","91615321":"markdown","f087a398":"markdown","c6d39c87":"markdown","a9af6676":"markdown","c63f0bb0":"markdown","2cc99a58":"markdown","e49de530":"markdown","d0ed1e22":"markdown","668b5877":"markdown","ba197be2":"markdown","0034d3a8":"markdown","637b79fb":"markdown","136db952":"markdown","7bf99d6a":"markdown","6b8f0a18":"markdown","c37f6d27":"markdown","8df83d75":"markdown","98d91811":"markdown","ffa7ab74":"markdown","7dd4dd28":"markdown","f9f31ec0":"markdown","2d6c56ec":"markdown","d80776fd":"markdown","23859165":"markdown","b0beb3ae":"markdown","822162ce":"markdown"},"source":{"7a151ade":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os","6571ec23":"pip install keras","df9c7f88":"pip install --upgrade tensorflow","a7b89080":"pip install --upgrade tensorflow-gpu","2f4c0210":"import os\nfrom glob import glob\nimport random\nimport time\nimport tensorflow\nimport datetime\nos.environ['KERAS_BACKEND'] = 'tensorflow'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' # 3 = INFO, WARNING, and ERROR\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import FileLink\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sns \n%matplotlib inline\nfrom IPython.display import display, Image\nimport matplotlib.image as mpimg\nimport cv2\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_files       \nfrom keras.utils import np_utils\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import log_loss\n\nfrom tensorflow import keras \n\nfrom keras.models import Sequential, Model\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling2D\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing import image\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.applications.vgg16 import VGG16\n\n\ndataset = pd.read_csv('..\/input\/state-farm-distracted-driver-detection\/driver_imgs_list.csv')\ndataset.head(5)","6bc21335":"# Groupby subjects\nby_drivers = dataset.groupby('subject') \n# Groupby unique drivers\nunique_drivers = by_drivers.groups.keys() # drivers id\nprint('There are : ',len(unique_drivers), ' unique drivers')\nprint('There is a mean of ',round(dataset.groupby('subject').count()['classname'].mean()), ' images by driver.')","0558e4e1":"NUMBER_CLASSES = 10 # 10 classes","5db40129":"# Read with opencv\ndef get_cv2_image(path, img_rows, img_cols, color_type=3):\n    \"\"\"\n    Function that return an opencv image from the path and the right number of dimension\n    \"\"\"\n    if color_type == 1: # Loading as Grayscale image\n        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    elif color_type == 3: # Loading as color image\n        img = cv2.imread(path, cv2.IMREAD_COLOR)\n    img = cv2.resize(img, (img_rows, img_cols)) # Reduce size\n    return img\n\n# Loading Training dataset\ndef load_train(img_rows, img_cols, color_type=3):\n    \"\"\"\n    Return train images and train labels from the original path\n    \"\"\"\n    train_images = [] \n    train_labels = []\n    # Loop over the training folder \n    for classed in tqdm(range(NUMBER_CLASSES)):\n        print('Loading directory c{}'.format(classed))\n        files = glob(os.path.join('..\/input\/state-farm-distracted-driver-detection\/imgs\/train\/c' + str(classed), '*.jpg'))\n        for file in files:\n            img = get_cv2_image(file, img_rows, img_cols, color_type)\n            train_images.append(img)\n            train_labels.append(classed)\n    return train_images, train_labels \n\ndef read_and_normalize_train_data(img_rows, img_cols, color_type):\n    \"\"\"\n    Load + categorical + split\n    \"\"\"\n    X, labels = load_train(img_rows, img_cols, color_type)\n    y = np_utils.to_categorical(labels, 10) #categorical train label\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # split into train and test\n    x_train = np.array(x_train, dtype=np.uint8).reshape(-1,img_rows,img_cols,color_type)\n    x_test = np.array(x_test, dtype=np.uint8).reshape(-1,img_rows,img_cols,color_type)\n    \n    return x_train, x_test, y_train, y_test\n\n# Loading validation dataset\ndef load_test(size=200000, img_rows=64, img_cols=64, color_type=3):\n    \"\"\"\n    Same as above but for validation dataset\n    \"\"\"\n    path = os.path.join('..\/input\/state-farm-distracted-driver-detection\/imgs\/test', '*.jpg')\n    files = sorted(glob(path))\n    X_test, X_test_id = [], []\n    total = 0\n    files_size = len(files)\n    for file in tqdm(files):\n        if total >= size or total >= files_size:\n            break\n        file_base = os.path.basename(file)\n        img = get_cv2_image(file, img_rows, img_cols, color_type)\n        X_test.append(img)\n        X_test_id.append(file_base)\n        total += 1\n    return X_test, X_test_id\n\ndef read_and_normalize_sampled_test_data(size, img_rows, img_cols, color_type=3):\n    test_data, test_ids = load_test(size, img_rows, img_cols, color_type)   \n    test_data = np.array(test_data, dtype=np.uint8)\n    test_data = test_data.reshape(-1,img_rows,img_cols,color_type)\n    return test_data, test_ids","bdb60bce":"img_rows = 64 # dimension of images\nimg_cols = 64\ncolor_type = 1 # grey\nnb_test_samples = 200\n\n# loading train images\nx_train, x_test, y_train, y_test = read_and_normalize_train_data(img_rows, img_cols, color_type)\n\n# loading validation images\ntest_files, test_targets = read_and_normalize_sampled_test_data(nb_test_samples, img_rows, img_cols, color_type)","42a43108":"# Statistics\n# Load the list of names\nnames = [item[17:19] for item in sorted(glob(\"..\/input\/state-farm-distracted-driver-detection\/imgs\/train\/*\/\"))]\ntest_files_size = len(np.array(glob(os.path.join('..\/input\/state-farm-distracted-driver-detection\/imgs\/test', '*.jpg'))))\nx_train_size = len(x_train)\ncategories_size = len(names)\nx_test_size = len(x_test)\nprint('There are %s total images.\\n' % (test_files_size + x_train_size + x_test_size))\nprint('There are %d training images.' % x_train_size)\nprint('There are %d total training categories.' % categories_size)\nprint('There are %d validation images.' % x_test_size)\nprint('There are %d test images.'% test_files_size)","f096d745":"import plotly.express as px\n\npx.histogram(dataset, x=\"classname\", color=\"classname\", title=\"Number of images by categories \")","6d08e868":"# Find the frequency of images per driver\ndrivers_id = pd.DataFrame((dataset['subject'].value_counts()).reset_index())\ndrivers_id.columns = ['driver_id', 'Counts']\npx.histogram(drivers_id, x=\"driver_id\",y=\"Counts\" ,color=\"driver_id\", title=\"Number of images by subjects \")","dd38c8ed":"activity_map = {'c0': 'Safe driving', \n                'c1': 'Texting - right', \n                'c2': 'Talking on the phone - right', \n                'c3': 'Texting - left', \n                'c4': 'Talking on the phone - left', \n                'c5': 'Operating the radio', \n                'c6': 'Drinking', \n                'c7': 'Reaching behind', \n                'c8': 'Hair and makeup', \n                'c9': 'Talking to passenger'}\n\n\nplt.figure(figsize = (12, 20))\nimage_count = 1\nBASE_URL = '..\/input\/state-farm-distracted-driver-detection\/imgs\/train\/'\nfor directory in os.listdir(BASE_URL):\n    if directory[0] != '.':\n        for i, file in enumerate(os.listdir(BASE_URL + directory)):\n            if i == 1:\n                break\n            else:\n                fig = plt.subplot(5, 2, image_count)\n                image_count += 1\n                image = mpimg.imread(BASE_URL + directory + '\/' + file)\n                plt.imshow(image)\n                plt.title(activity_map[directory])","57aa7f75":"def create_submission(predictions, test_id, info):\n    \"\"\"\n    Submission function for participating to the competition\n    \"\"\"\n    result = pd.DataFrame(predictions, columns=['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n    result.loc[:, 'img'] = pd.Series(test_id, index=result.index)\n    \n    now = datetime.datetime.now()\n    \n    if not os.path.isdir('kaggle_submissions'):\n        os.mkdir('kaggle_submissions')\n\n    suffix = \"{}_{}\".format(info,str(now.strftime(\"%Y-%m-%d-%H-%M\")))\n    sub_file = os.path.join('kaggle_submissions', 'submission_' + suffix + '.csv')\n    \n    result.to_csv(sub_file, index=False)\n    \n    return sub_file","ddd9803f":"# Number of batch size and epochs\nbatch_size = 40 #40\nnb_epoch = 6 #10","9fdf42af":"models_dir = \"saved_models\"\nif not os.path.exists(models_dir):\n    os.makedirs(models_dir)\n    \ncheckpointer = ModelCheckpoint(filepath='saved_models\/weights_best_vanilla.hdf5', \n                               monitor='val_loss', mode='min',\n                               verbose=1, save_best_only=True)\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n#callbacks = [checkpointer, es]","44e778d5":"def create_model():\n    model = Sequential()\n\n    ## CNN 1\n    model.add(Conv2D(32,(3,3),activation='relu',input_shape=(img_rows, img_cols, color_type)))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32,(3,3),activation='relu',padding='same'))\n    model.add(BatchNormalization(axis = 3))\n    model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n    model.add(Dropout(0.3))\n\n    ## CNN 2\n    model.add(Conv2D(64,(3,3),activation='relu',padding='same'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64,(3,3),activation='relu',padding='same'))\n    model.add(BatchNormalization(axis = 3))\n    model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n    model.add(Dropout(0.3))\n\n    ## CNN 3\n    model.add(Conv2D(128,(3,3),activation='relu',padding='same'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(128,(3,3),activation='relu',padding='same'))\n    model.add(BatchNormalization(axis = 3))\n    model.add(MaxPooling2D(pool_size=(2,2),padding='same'))\n    model.add(Dropout(0.5))\n\n    ## Output\n    model.add(Flatten())\n    model.add(Dense(512,activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(128,activation='relu'))\n    model.add(Dropout(0.25))\n    model.add(Dense(10,activation='softmax'))\n\n    return model","92491736":"model = create_model()\n\n# More details about the layers\nmodel.summary()\n\n# Compiling the model\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])","1776a9ea":"history = model.fit(x_train, y_train, \n          validation_data=(x_test, y_test),\n          epochs=nb_epoch, batch_size=batch_size, verbose=1)\n\n#model.load_weights('saved_models\/weights_best_vanilla.hdf5')\nprint('History of the training',history.history)","f9a5a00c":"def plot_train_history(history):\n    \"\"\"\n    Plot the validation accuracy and validation loss over epochs\n    \"\"\"\n    # Summarize history for accuracy\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('Model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n\n    # Summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n    \nplot_train_history(history)","2222650c":"def plot_test_class(model, test_files, image_number, color_type=1):\n    \"\"\"\n    Function that tests or model on test images and show the results\n    \"\"\"\n    img_brute = test_files[image_number]\n    img_brute = cv2.resize(img_brute,(img_rows,img_cols))\n    plt.imshow(img_brute, cmap='gray')\n\n    new_img = img_brute.reshape(-1,img_rows,img_cols,color_type)\n\n    y_prediction = model.predict(new_img, batch_size=batch_size, verbose=1)\n    print('Y prediction: {}'.format(y_prediction))\n    print('Predicted: {}'.format(activity_map.get('c{}'.format(np.argmax(y_prediction)))))\n    \n    plt.show()","bba30b01":"score1 = model.evaluate(x_test, y_test, verbose=1)","aec2d67a":"print('Loss: ', score1[0])\nprint('Accuracy: ', score1[1]*100, ' %')","a9018717":"for i in range(10):\n    plot_test_class(model, test_files, i)","cecc77ad":"# Using ImageDataGenerator from keras\ntrain_datagen = ImageDataGenerator(rescale = 1.0\/255, \n                                   shear_range = 0.2, \n                                   zoom_range = 0.2, \n                                   horizontal_flip = True, \n                                   validation_split = 0.2)\n\ntest_datagen = ImageDataGenerator(rescale=1.0\/ 255, validation_split = 0.2)","c2c30b7c":"nb_train_samples = x_train.shape[0]\nnb_validation_samples = x_test.shape[0]\ntraining_generator = train_datagen.flow(x_train, y_train, batch_size=batch_size)\nvalidation_generator = test_datagen.flow(x_test, y_test, batch_size=batch_size)","c6d60637":"#checkpoint = ModelCheckpoint('saved_models\/weights_best_vanilla.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nhistory_v2 = model.fit_generator(training_generator,\n                         steps_per_epoch = nb_train_samples \/\/ batch_size,\n                         epochs = nb_epoch, \n                         verbose = 1,\n                         validation_data = validation_generator,\n                         validation_steps = nb_validation_samples \/\/ batch_size)","54ad9092":"#model2.load_weights('saved_models\/weights_best_va nilla.hdf5')\nplot_train_history(history_v2)","66f0dda4":"# Evaluate and compare the performance of the new model\nscore2 = model.evaluate_generator(validation_generator, nb_validation_samples \/\/ batch_size)\nprint(\"Loss for model 1\",score1[0])\nprint(\"Loss for model 2 (data augmentation):\", score2[0])\n\nprint(\"Test accuracy for model 1\",score1[1])\nprint(\"Test accuracy for model 2 (data augmentation):\", score2[1])","00b0bd68":"def vgg_std16_model(img_rows, img_cols, color_type=3):\n    \"\"\"\n    Architecture and adaptation of the VGG16 for our project\n    \"\"\"\n    nb_classes = 10\n    # Remove fully connected layer and replace\n    vgg16_model = VGG16(weights=\"imagenet\", include_top=False)\n    for layer in vgg16_model.layers:\n        layer.trainable = False\n    \n    x = vgg16_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation='relu')(x)\n    predictions = Dense(nb_classes, activation = 'softmax')(x) # add dense layer with 10 neurons and activation softmax\n    model = Model(vgg16_model.input,predictions)\n    return model","bcace6ef":"# Load the VGG16 network\nprint(\"Loading network...\")\nmodel_vgg16 = vgg_std16_model(img_rows, img_cols)\nmodel_vgg16.summary()\nmodel_vgg16.compile(loss='categorical_crossentropy',\n                         optimizer='rmsprop',\n                         metrics=['accuracy'])","fa937af7":"training_generator = train_datagen.flow_from_directory('..\/input\/state-farm-distracted-driver-detection\/imgs\/train', \n                                                 target_size = (img_rows, img_cols), \n                                                 batch_size = batch_size,\n                                                 shuffle=True,\n                                                 class_mode='categorical', subset=\"training\")\n\nvalidation_generator = test_datagen.flow_from_directory('..\/input\/state-farm-distracted-driver-detection\/imgs\/test', \n                                                   target_size = (img_rows, img_cols), \n                                                   batch_size = batch_size,\n                                                   shuffle=False,\n                                                   class_mode='categorical', subset=\"validation\")\nnb_train_samples = 17943\nnb_validation_samples = 4481","77617192":"epoch=6","71f65a08":"# Training the new Model\n#checkpoint = ModelCheckpoint('saved_models\/weights_best_vgg16.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nhistory_v3 = model_vgg16.fit_generator(training_generator,\n                         steps_per_epoch = nb_train_samples \/\/ batch_size,\n                         epochs = epoch, \n                         verbose = 1,\n                         validation_data = validation_generator,\n                         validation_steps = nb_validation_samples \/\/ batch_size)","b7371162":"#model_vgg16.load_weights('saved_models\/weights_best_vgg16.hdf5')\nplot_train_history(history_v3)","93a4c0c1":"# Evaluate the performance of the new model with Transfer learning\nscore3 = model_vgg16.evaluate_generator(validation_generator, nb_validation_samples \/\/ batch_size, verbose = 1)\n\nprint(\"Test Score with simple CNN:\", score1[0])\nprint(\"Test Accuracy with simple CNN\", score1[1])\nprint('--------------------------------------')\nprint(\"Test Score with Data Augmentation:\", score2[0])\nprint(\"Test Accuracy with Data Augmentation:\", score2[1])\nprint('--------------------------------------')\nprint(\"Test Score with Transfer Learning:\", score3[0])\nprint(\"Test Accuracy with Transfer Learning:\", score3[1])","407bb983":"### Images overview","12c5483b":"## Part 4 : Data Augmentation  <a class=\"anchor\" id=\"chapter4\"><\/a>","80ecc93f":"Number of images by category","7c6ec9a2":"- Transfer learning is a machine learning method where a model developed for a task is reused as the starting point for a model on a second task.\n- That's why we train a CNN with Transfer Learning (VGG, MobileNet) in order to reduce training time without sacrificing accuracy. \n- VGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper \u201cVery Deep Convolutional Networks for Large-Scale Image Recognition\u201d. The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes","a09aed90":"### Some functions for loading and normalization","ca8e6a00":"## Part 2 : EDA  <a class=\"anchor\" id=\"chapter2\"><\/a>","91615321":"### Data visualisation","f087a398":"Create a vanilla CNN model\nBuilding the model\nI'll develop the model with a total of 4 Convolutional layers, then a Flatten layer and then 2 Dense layers. I'll use the optimizer as rmsprop, and loss as categorical_crossentropy.","c6d39c87":"#### Prediction on test set","a9af6676":"### Technology used :\n- Python libraries : OpenCV, Tenserflow, Scikit Learn, Pandas,...\n- Computer vision techniques : CNN, Data Augmentation, Transfer Learning","c63f0bb0":"#### Training model","2cc99a58":"# Computer Vision Project","e49de530":"## Distraction detection during driving\n\nThe main goal of this notebook is to show how can we detect the distraction of a driver using a dataset of thousand of driver images. \nMore precisely we want to classify the activity the driver is performing (driving, texting, talking, operating the radio etc)\n\n### Table of Contents\n\n* [Part 1 : Loading Dataset ](#chapter1)\n    \n* [Part 2 : EDA](#chapter2)\n\n* [Part 3 : Performing a CNN model for classification](#chapter3)\n\n* [Part 4 : Data augmentation for increase robustness](#chapter4)\n\n* [Part 5 : Transfer Learning for increasing accuracy](#chapter4)","d0ed1e22":"## Part 5 : Transfer Learning with VGG  <a class=\"anchor\" id=\"chapter5\"><\/a>","668b5877":"## Some check about libraries","ba197be2":"## Conclusion","0034d3a8":"#### Data augmentation makes our model more robust.","637b79fb":"* ### CNN are really powerfull in computer vision in order to classify images. (We predict well more than 90% of the images)\n* ### And differents techniques exist in order to improve its performance like Data augmentation and Transfer learning.","136db952":"So now we create another model with data augmentation with the input in order to increase accuracy.\nWe generate more images using ImageDataGenerator and split the training data into 80% train and 20% validation split.","7bf99d6a":"Image data augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset. Training deep learning neural network models on more data can result in more skillful models, and the augmentation techniques can create variations of the images that can improve the ability of the fit models to generalize what they have learned to new images.\n\nFor that we will use the ImageDataGenerator from keras.","6b8f0a18":"![test.png](attachment:6f127c57-e907-438f-8673-5e47f80eb8b3.png)","c37f6d27":"### Training with data augmentation","8df83d75":"--> It is well distributed","98d91811":"## Part 1 : Loading Dataset  <a class=\"anchor\" id=\"chapter1\"><\/a>","ffa7ab74":"#### Good accuracy but we can do better !","7dd4dd28":"Let's take a look at the various images in the dataset.\n* I'll plot an image for each of the 10 classes.","f9f31ec0":"## Part 3 : CNN Model  <a class=\"anchor\" id=\"chapter3\"><\/a>\nArchitecture :\n- 3 Convolutionnal layers (with Relu, Maxpooling and dropout)\n- A flatten layer\n- 2 Dense layers with Relu and Dropouts\n- 1 Dense layer with softmax for the classification","2d6c56ec":"### Conclusion for Data Augmentation","d80776fd":"The initial Dataset is composed of a thousand of images labelized as above. The goal of this notebook is to show how to classify them using computer vision techniques","23859165":"## Dataset sample","b0beb3ae":"The 10 classes to classify are :\n- c0: safe driving\n- c1: texting - right\n- c2: talking on the phone - right\n- c3: texting - left\n- c4: talking on the phone - left\n- c5: operating the radio\n- c6: drinking\n- c7: reaching behind\n- c8: hair and makeup\n- c9: talking to passenger","822162ce":"![test.jpeg](attachment:0d1d1222-6cb6-40ac-af75-380ec42047bc.jpeg)\n\nExample of data augmentation (Shift, sampling, resizing etc)"}}