{"cell_type":{"977b29db":"code","bffbb550":"code","f9f194ef":"code","cad85878":"code","eec3bc1b":"code","968c45c4":"code","26e89571":"code","2a6da969":"code","2d06a861":"code","c2a64eb4":"code","8d66e162":"code","1a82852e":"code","313d80d8":"code","d63eb933":"code","3c8fb45f":"code","0e575b78":"code","62737ea4":"code","39452afd":"code","4d6a70aa":"code","96da3605":"code","ebf08269":"code","211d88a6":"code","6a2b8167":"code","12ca7696":"code","523af33b":"code","eea6f9fb":"code","4973f5cf":"code","68fd48f5":"code","8b75f973":"code","7f4863af":"code","37cb9053":"code","97e47e12":"code","5257b549":"code","ab66c1a0":"code","9986f88b":"code","4873a2e7":"code","b7aad404":"code","d0050c38":"code","bff6f547":"code","53962a81":"code","9c258a7b":"code","24552df0":"code","54cbec89":"code","b377853e":"code","23086955":"code","0816eb17":"code","718c18c1":"code","033b3d3a":"code","7c77dbb9":"code","583c25a9":"code","0f13f4bb":"code","dd3cdfa9":"code","9abd0b4c":"code","267bb5ec":"code","d3968422":"code","1149ff8b":"code","bae27330":"code","ad0bfc06":"code","79b67e62":"code","14b7ea2f":"code","33e94b30":"code","a8343f69":"code","036b6ae9":"code","7f5f096e":"code","9c4f97c7":"code","e1aaaed0":"code","e521ce09":"code","b9e6bc92":"code","7c9d169f":"code","7da16bbb":"code","90ddd084":"code","4091e6b7":"code","bb8f69b3":"code","4592eaea":"markdown","262d732f":"markdown","a7b4c2b6":"markdown","501f0158":"markdown","0273b98d":"markdown","e5a146ae":"markdown","244ed370":"markdown","6e486801":"markdown","9135d19b":"markdown","d41881a5":"markdown","460a427b":"markdown","7107d822":"markdown","ba8c07d3":"markdown","2c19b437":"markdown","aa13c9d3":"markdown","ba9c9f90":"markdown","4aacaf4e":"markdown","219b6668":"markdown","00f774fa":"markdown","0b718a14":"markdown","d19cabe0":"markdown","6807ac8b":"markdown","4d7e6ace":"markdown","1c8c4053":"markdown","c0d77736":"markdown","890a3770":"markdown","2a66671f":"markdown","566aa5ae":"markdown","5c9414c2":"markdown","5c0006b4":"markdown","ea978003":"markdown","5388cbbb":"markdown","712fda1e":"markdown","9e1e97ce":"markdown","c996da12":"markdown","8184da92":"markdown","e3af9c73":"markdown","a0d09561":"markdown","64a24a66":"markdown"},"source":{"977b29db":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn import linear_model\nfrom sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, accuracy_score\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.model_selection import KFold\nimport math\nfrom sklearn.svm import SVR\nimport lightgbm\nfrom xgboost.sklearn import XGBRegressor\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom lightgbm import LGBMRegressor\nfrom sklearn import neighbors\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn import preprocessing\nfrom sklearn import utils\nfrom sklearn.ensemble import AdaBoostRegressor\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","bffbb550":"#Cr\u00e9er un Pandas DataFrame \u00e0 partir des fichiers CSV:\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n#enregistrer la colonne  'Id'\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#puis la supprimer puisqu\"elle inutile dans le processus de la prediction \ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#mettre train et test dans une seule dataset\ndata = train.append(test,sort=False)\n#taille des datasets:\nprint(train.shape)\nprint(test.shape)# test ne contient pas la colonne 'SalePrice'\nprint(data.shape)\n\ndata.head()","f9f194ef":"#quelques statisques sur la 'target variable' 'SalePrice'\ndata.SalePrice.describe() #on constate que le prix de vente moyen est ~ 180921$","cad85878":"#distribution de la variable 'SalePrice' \nplt.hist(train['SalePrice'], color= 'r')\nplt.title('Distribution de sales price des maisons', fontsize = 24)\nplt.ylabel('observation', fontsize = 20)\nplt.xlabel('sales price', fontsize = 20)\n\nplt.show()","eec3bc1b":"#correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);\n#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","968c45c4":"#prendre juste les valeurs num\u00e9riques\nnumeric_features = data.select_dtypes(include = [np.number])\n#features with the most correlation with the predictor variable\ncorr = numeric_features.corr()\nprint(corr['SalePrice'].sort_values(ascending = False)[:5], '\\n')\nprint(corr['SalePrice'].sort_values(ascending = False)[-5:])","26e89571":"#log transforming transformer sale price en une distribution gaussienne\ntarget = np.log(data.SalePrice)\n#definir\nplt.scatter(x=data['GarageArea'], y=target)\nplt.ylabel('Sale Price')\nplt.xlabel('Garage Area')\nf1=plt.show()\nf1\n#definir\nplt.scatter(x=data['OverallQual'], y=target)\nplt.ylabel('Sale Price')\nplt.xlabel('OverallQual')\nf2=plt.show()\nf2\n#definir\nplt.scatter(x=data['GrLivArea'], y=target)\nplt.ylabel('Sale Price')\nplt.xlabel('GrLivArea')\nf3=plt.show()\nf3\n#definir\nplt.scatter(x=data['GarageCars'], y=target)\nplt.ylabel('Sale Price')\nplt.xlabel('GarageCars')\nf4=plt.show()\nf4","2a6da969":"#v\u00e9rifier l'existence des valeurs nulles:\ndata[data.isnull().any(axis=1)]","2d06a861":"#maintenant on va afficher le nombre des valeurs manquantes pour chaque collone 'feature':\na=data.isnull().sum().sort_values(ascending=False)[:25]\nb=a\/2919*100\nnulls = pd.DataFrame({'Nb_val_nulles': a,'pourcentage': b})\nnulls.index.name = 'Features'\nnulls","c2a64eb4":"#les colonnes'features' qui ont plus que 1000 valeur manquantes doivent etre supprim\u00e9es:\ndata = data.dropna(axis=1, how='any', thresh = 1000)\ndata.shape","8d66e162":"#rempacer les valeurs manquantes par la veleur mean:\ndata = data.fillna(data.mean())\n#train = train.select_dtypes(include= [np.number]).interpolate().dropna()\n#v\u00e9rifier l'existence de valeurs nulles\ndata[data.isnull().any(axis=1)]","1a82852e":"#remplacer les valeurs de type object par des integers\ndata = pd.get_dummies(data)\ndata.shape","313d80d8":"#Verifying missing values\ndata[data.isnull().any(axis=1)]\n#sum(data.isnull().sum() != 0)","d63eb933":"#Supprimer les variables corr\u00e9l\u00e9es  entre eux: car ils donnent tous les memes informations:\ncovariance = data.corr()\nallFeatures = [i for i in covariance]\nsetOfDroppedFeatures = set() \nfor i in range(len(allFeatures)) :\n    for j in range(i+1,len(allFeatures)): \n        feature1=allFeatures[i]\n        feature2=allFeatures[j]\n        if abs(covariance[feature1][feature2]) > 0.8: #If the correlation between the features is > 0.8\n            setOfDroppedFeatures.add(feature1)\ndata2 = data.drop(setOfDroppedFeatures, axis=1)\ndata2.shape","3c8fb45f":"#supprimer les features qui ont la minimale correlation avec notre target variable'SalePrice':\nnonCorrelated = [column for column in data2 if abs(data2[column].corr(data2[\"SalePrice\"])) < 0.045]\ndata2 = data2.drop(nonCorrelated, axis=1)\ndata2.shape","0e575b78":"from sklearn.decomposition import PCA\nfrom sklearn.datasets import load_boston\nfrom sklearn.preprocessing import StandardScaler\nboston = load_boston()\nX = boston[\"data\"]\nscaler = StandardScaler()\nx_scaled= scaler.fit_transform(X)\nn_col=data.shape[1]\npca=PCA(n_components = n_col)\ndata3=pca.fit_transform(data)\n#test_com=pca.fit_transform(data)\ndata3.shape\n#test_com.shape","62737ea4":"#on separe les datastes (Because removing outliers \u21d4 removing rows, and we don't want to remove rows from test set)\nnewTrain = data.iloc[:1460]\nnewTest = data.iloc[1460:]\n#Apr\u00e9s reduction PCA\n\"\"\"newTrain3 = data3.iloc[:1460]\nnewTest3 = data3.iloc[1460:]\"\"\"\n#Apr\u00e9s reduction COR\nnewTrain2 = data2.iloc[:1460]\nnewTest2 = data2.iloc[1460:]","39452afd":"#Second, definir une fonction quie retourne les valeurs des outliers via la methode  percentile()\n\ndef outliers_iqr(ys):\n    quartile_1, quartile_3 = np.percentile(ys, [25, 75]) #Get 1st and 3rd quartiles (25% -> 75% of data will be kept)\n    iqr = quartile_3 - quartile_1\n    lower_bound = quartile_1 - (iqr * 1.5) #Get lower bound\n    upper_bound = quartile_3 + (iqr * 1.5) #Get upper bound\n    return np.where((ys > upper_bound) | (ys < lower_bound)) #Get outlier values\n\n#Third, supprimer les ouliers juste de train dataset \n\ntrainWithoutOutliers = newTrain #We can't change train while running through it\nfor column in newTrain:\n    outlierValuesList = np.ndarray.tolist(outliers_iqr(newTrain[column])[0]) #outliers_iqr() returns an array\n    trainWithoutOutliers = newTrain.drop(outlierValuesList) #Drop outlier rows\n    \ntrainWithoutOutliers = newTrain\nprint(outlierValuesList)\nprint(trainWithoutOutliers.shape)\n\n#Apr\u00e9s reduction\ntrainWithoutOutliers2 = newTrain2\nfor column in newTrain2:\n    outlierValuesList2 = np.ndarray.tolist(outliers_iqr(newTrain2[column])[0]) #outliers_iqr() returns an array\n    trainWithoutOutliers2 = newTrain2.drop(outlierValuesList2) #Drop outlier rows\n    \ntrainWithoutOutliers2 = newTrain2\nprint(outlierValuesList2)\nprint(trainWithoutOutliers2.shape)","4d6a70aa":"X = trainWithoutOutliers.drop(\"SalePrice\", axis=1) #supprimer la colonne SalePrice \nY = np.log1p(newTrain[\"SalePrice\"])\n\n#Apr\u00e9 reduction PCA\nX2 = trainWithoutOutliers2.drop(\"SalePrice\", axis=1) #supprimer la colonne SalePrice \nY2 = np.log1p(newTrain2[\"SalePrice\"])","96da3605":"#splitting the data into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, Y, random_state = 42, test_size = .33)\n#Apr\u00e9s reduction\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X2, Y2, random_state = 42, test_size = .33)","ebf08269":"#\u00e7a va me servire pour faire la prediction\nnewTest = newTest.drop(\"SalePrice\", axis=1) \n#apres reduction\nnewTest2 = newTest2.drop(\"SalePrice\", axis=1) ","211d88a6":"rmse_val = [] #to store rmse values for different k\nfor K in range(20):\n    K = K+1\n    model = neighbors.KNeighborsRegressor(n_neighbors = K)\n\n    model.fit(X_train, y_train) \n    \n    predictions=model.predict(X_test) #make prediction on test set\n    error = mean_squared_error(y_test, predictions) #calculate rmse\n    rmse_val.append(error) #store rmse values\n    print('RMSE value for k= ' , K , 'is:', error)","6a2b8167":"# on choisi K=5 puisque cette valeur donne RMSE minimal\n#cr\u00e9er le mod\u00e8le\nknn = neighbors.KNeighborsRegressor(n_neighbors = 5)\n#fitting linear regression on the data\nknn_fit = knn.fit(X_train, y_train)","12ca7696":"#R square value\nprint('R square is: {}'.format(knn_fit.score(X, Y)))\n#predicting on the test set\npredictionknn = knn_fit.predict(X_test)\n#evaluating the model on mean square error\nknnRMS=mean_squared_error(y_test, predictionknn)\nprint('RMSE is {}'.format(knnRMS))","523af33b":"rmse_val = [] #to store rmse values for different k\nfor K in range(20):\n    K = K+1\n    model = neighbors.KNeighborsRegressor(n_neighbors = K)\n\n    model.fit(X_train2, y_train2) \n    \n    predictions=model.predict(X_test2) #make prediction on test set\n    error = mean_squared_error(y_test2, predictions) #calculate rmse\n    rmse_val.append(error) #store rmse values\n    print('RMSE value for k= ' , K , 'is:', error)\n    \n# on choisi K=5 puisque cette valeur donne RMSE minimal\n#cr\u00e9er le mod\u00e8le\nknn = neighbors.KNeighborsRegressor(n_neighbors = 5)\n#fitting linear regression on the data\nknn_fit2 = knn.fit(X_train2, y_train2)\n\n#R square value\nprint('R square is: {}'.format(knn_fit2.score(X2, Y2)))\n#predicting on the test set\npredictionknn2 = knn_fit2.predict(X_test2)\n#evaluating the model on mean square error\nknnRMS2=mean_squared_error(y_test2, predictionknn2)\nprint('RMSE is {}'.format(knnRMS2))","eea6f9fb":"lab_enc = preprocessing.LabelEncoder()\ny_train_encoded = lab_enc.fit_transform(y_train)\nprint(y_train_encoded)\nprint(utils.multiclass.type_of_target(y_train))\nprint(utils.multiclass.type_of_target(y_train.astype('int')))\nprint(utils.multiclass.type_of_target(y_train_encoded))","4973f5cf":"#cr\u00e9er le mod\u00e8le\nlogir = LogisticRegression(random_state = 0)\n#fitting linear regression on the data\nlogir_fit = logir.fit(X_train, y_train_encoded)","68fd48f5":"#R square value\n#print('R square is: {}'.format(logir_fit.score(X_test, y_test)))\n#predicting on the test set\npredictionlogir = logir_fit.predict(X_test)\n#evaluating the model on mean square error\nlogirRMS= mean_squared_error(y_test, predictionlogir)\nprint('RMSE is {}'.format(logirRMS))","8b75f973":"lab_enc = preprocessing.LabelEncoder()\ny_train_encoded2 = lab_enc.fit_transform(y_train2)\nprint(y_train_encoded2)\nprint(utils.multiclass.type_of_target(y_train2))\nprint(utils.multiclass.type_of_target(y_train2.astype('int')))\nprint(utils.multiclass.type_of_target(y_train_encoded2))\n\n#cr\u00e9er le mod\u00e8le\nlogir = LogisticRegression(random_state = 0)\n#fitting linear regression on the data\nlogir_fit2 = logir.fit(X_train2, y_train_encoded2)\n\n#R square value\n#print('R square is: {}'.format(logir_fit.score(X_test, y_test)))\n#predicting on the test set\npredictionlogir2 = logir.predict(X_test2)\nlogirRMS2= mean_squared_error(y_test2, predictionlogir2)\nprint('RMSE is {}'.format(logirRMS2))","7f4863af":"#cr\u00e9er le mod\u00e8le\nlr = linear_model.LinearRegression()\n#fitting linear regression on the data\nlr_fit = lr.fit(X_train, y_train)","37cb9053":"#R square value\nprint('R square is: {}'.format(lr_fit.score(X, Y)))\n#predicting on the test set\npredictionlr = lr_fit.predict(X_test)\n#evaluating the model on mean square error\nlrRMSE=mean_squared_error(y_test, predictionlr)\nprint('RMSE is {}'.format(lrRMSE))","97e47e12":"pred1=np.expm1(lr_fit.predict(newTest))\nsub1 = pd.DataFrame() #Create a new DataFrame for submission\nsub1['Id'] = test_ID\nsub1['SalePrice'] = pred1\nsub1.to_csv(\"submissionlin.csv\", index=False) #Convert DataFrame to .csv file\nsub1","5257b549":"#cr\u00e9er le mod\u00e8le\nlr = linear_model.LinearRegression()\n#fitting linear regression on the data\nlr_fit2 = lr.fit(X_train2, y_train2)\n\n#R square value\nprint('R square is: {}'.format(lr_fit2.score(X2, Y2)))\n#predicting on the test set\npredictionlr2 = lr_fit2.predict(X_test2)\n#evaluating the model on mean square error\nlrRMSE2=mean_squared_error(y_test2, predictionlr2)\nprint('RMSE is {}'.format(lrRMSE2))\n\n\"\"\"pred1=np.expm1(lr_fit.predict(newTest2))\nsub1 = pd.DataFrame() #Create a new DataFrame for submission\nsub1['Id'] = test_ID\nsub1['SalePrice'] = pred1\nsub1.to_csv(\"submissionlin.csv\", index=False) #Convert DataFrame to .csv file\nsub1\"\"\"","ab66c1a0":"kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\nalphas_Rd = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphasL = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]","9986f88b":"#cr\u00e9er mod\u00e8le\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphasL, random_state=42, cv=kfolds))","4873a2e7":"#fitting\nlasso_fit=lasso.fit(X_train, y_train)","b7aad404":"#prediction on train set\ny_train_predict = lasso_fit.predict(X_train)\n#prediction on test set\ny_test_predict = lasso_fit.predict(X_test)\n#MSE trainset\nlasso_train = mean_squared_error(y_train, y_train_predict)\nprint('Mean square error on the Train set is: {}'.format(lasso_train))\n#MSE test set\nlassoRMS = mean_squared_error(y_test, y_test_predict)\nprint('Mean square error on the Test set is: {}'.format(lassoRMS))","d0050c38":"pred5=np.expm1(lasso_fit.predict(newTest))\nsub5 = pd.DataFrame() #Create a new DataFrame for submission\nsub5['Id'] = test_ID\nsub5['SalePrice'] = pred5\nsub5.to_csv(\"submissionlassofin1.csv\", index=False) #Convert DataFrame to .csv file\nsub5","bff6f547":"\n#fitting\nlasso_fit2=lasso.fit(X_train2, y_train2)\ny_train_predict2 = lasso_fit2.predict(X_train2)\n#prediction on test set\ny_test_predict2 = lasso_fit2.predict(X_test2)\n#MSE trainset\nlasso_train2 = mean_squared_error(y_train2, y_train_predict2)\nprint('Mean square error on the Train set is: {}'.format(lasso_train2))\n#MSE test set\nlassoRMS2 = mean_squared_error(y_test2, y_test_predict2)\nprint('Mean square error on the Test set is: {}'.format(lassoRMS2))\n\"\"\"pred5=np.expm1(lasso_fit.predict(newTest))\nsub5 = pd.DataFrame() #Create a new DataFrame for submission\nsub5['Id'] = test_ID\nsub5['SalePrice'] = pred4\nsub5.to_csv(\"submissionlassofin1.csv\", index=False) #Convert DataFrame to .csv file\nsub5\"\"\"","53962a81":"#c\u00e9re modele\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_Rd, cv=kfolds))","9c258a7b":"#fiting\nridge_fit=ridge.fit(X_train, y_train)","24552df0":"#prediction on train set\ny_train_predict = ridge_fit.predict(X_train)\n#prediction on test set\ny_test_predict = ridge_fit.predict(X_test)\n#MSE trainset\nridge_train = mean_squared_error(y_train, y_train_predict)\nprint('Mean square error on the Train set is: {}'.format(ridge_train))\n#MSE test set\nridgeRMS = mean_squared_error(y_test, y_test_predict)\nprint('Mean square error on the Test set is: {}'.format(ridgeRMS))","54cbec89":"#fiting\nridge_fit2=ridge.fit(X_train2, y_train2)\n#prediction on train set\ny_train_predict2 = ridge_fit2.predict(X_train2)\n#prediction on test set\ny_test_predict2 = ridge_fit2.predict(X_test2)\n#MSE trainset\nridge_train2 = mean_squared_error(y_train2, y_train_predict2)\nprint('Mean square error on the Train set is: {}'.format(ridge_train))\n#MSE test set\nridgeRMS2 = mean_squared_error(y_test2, y_test_predict2)\nprint('Mean square error on the Test set is: {}'.format(ridgeRMS2))","b377853e":"#cr\u00e9er mod\u00e8le\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003,))","23086955":"#fiting\nsvr_fit=svr.fit(X_train, y_train)","0816eb17":"#prediction on train set\ny_train_predict = svr_fit.predict(X_train)\n#prediction on test set\ny_test_predict = svr_fit.predict(X_test)\n#MSE trainset\nsvr_train = mean_squared_error(y_train, y_train_predict)\nprint('Mean square error on the Train set is: {}'.format(svr_train))\n#MSE test set\nsvrRMS = mean_squared_error(y_test, y_test_predict)\nprint('Mean square error on the Test set is: {}'.format(svrRMS))","718c18c1":"#fiting\nsvr_fit2=svr.fit(X_train2, y_train2)\n#prediction on train set\ny_train_predict2 = svr_fit2.predict(X_train2)\n#prediction on test set\ny_test_predict2 = svr_fit2.predict(X_test2)\n#MSE trainset\nsvr_train2 = mean_squared_error(y_train2, y_train_predict2)\nprint('Mean square error on the Train set is: {}'.format(svr_train2))\n#MSE test set\nsvrRMS2 = mean_squared_error(y_test2, y_test_predict2)\nprint('Mean square error on the Test set is: {}'.format(svrRMS2))","033b3d3a":"#Gradient boosting regressor model\ngbr = GradientBoostingRegressor(n_estimators= 1000, max_depth= 2, learning_rate= .01)\n#fiting\ngbr_fit=gbr.fit(X_train, y_train)","7c77dbb9":"#prediction on train set\ny_train_predict = gbr_fit.predict(X_train)\n#prediction on test set\ny_test_predict = gbr_fit.predict(X_test)\n#MSE trainset\nest_train = mean_squared_error(y_train, y_train_predict)\nprint('Mean square error on the Train set is: {}'.format(est_train))\n#MSE test set\ngrboRMS = mean_squared_error(y_test, y_test_predict)\nprint('Mean square error on the Test set is: {}'.format(grboRMS))","583c25a9":"pred4=np.expm1(gbr_fit.predict(newTest))\nsub4 = pd.DataFrame() #Create a new DataFrame for submission\nsub4['Id'] = test_ID\nsub4['SalePrice'] = pred4\nsub4.to_csv(\"submissiongbr.csv\", index=False) #Convert DataFrame to .csv file\nsub4","0f13f4bb":"#fiting\ngbr_fit2=gbr.fit(X_train2, y_train2)\n#prediction on train set\ny_train_predict2 = gbr_fit2.predict(X_train2)\n#prediction on test set\ny_test_predict2 = gbr_fit2.predict(X_test2)\n#MSE trainset\nest_train2 = mean_squared_error(y_train2, y_train_predict2)\nprint('Mean square error on the Train set is: {}'.format(est_train2))\n#MSE test set\ngrboRMS2 = mean_squared_error(y_test, y_test_predict)\nprint('Mean square error on the Test set is: {}'.format(grboRMS2))\n\"\"\"pred4=np.expm1(gbr_fit.predict(newTest))\nsub4 = pd.DataFrame() #Create a new DataFrame for submission\nsub4['Id'] = test_ID\nsub4['SalePrice'] = pred4\nsub4.to_csv(\"submissiongbr.csv\", index=False) #Convert DataFrame to .csv file\nsub4\"\"\"","dd3cdfa9":"# cr\u00e9er mod\u00e9le\nxgboost = XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)","9abd0b4c":"# fiting\nxgboost_fit=xgboost.fit(X_train, y_train)","267bb5ec":"#prediction on train set\ny_train_predict = xgboost_fit.predict(X_train)\n#prediction on test set\ny_test_predict = xgboost_fit.predict(X_test)\n#MSE trainset\nxgboost_train = mean_squared_error(y_train, y_train_predict)\nprint('Mean square error on the Train set is: {}'.format(xgboost_train))\n#MSE test set\nxgboostRMS = mean_squared_error(y_test, y_test_predict)\nprint('Mean square error on the Test set is: {}'.format(xgboostRMS))","d3968422":"pred4=np.expm1(xgboost_fit.predict(newTest))\nsub4 = pd.DataFrame() #Create a new DataFrame for submission\nsub4['Id'] = test_ID\nsub4['SalePrice'] = pred4\nsub4.to_csv(\"submissionxgb2.csv\", index=False) #Convert DataFrame to .csv file\nsub4","1149ff8b":"# fiting\nxgboost_fit2=xgboost.fit(X_train2, y_train2)\n#prediction on train set\ny_train_predict2 = xgboost_fit2.predict(X_train2)\n#prediction on test set\ny_test_predict2 = xgboost_fit2.predict(X_test2)\n#MSE trainset\nxgboost_train2 = mean_squared_error(y_train2, y_train_predict2)\nprint('Mean square error on the Train set is: {}'.format(xgboost_train2))\n#MSE test set\nxgboostRMS2 = mean_squared_error(y_test2, y_test_predict2)\nprint('Mean square error on the Test set is: {}'.format(xgboostRMS2))\n\"\"\"pred4=np.expm1(xgboost_fit.predict(newTest))\nsub4 = pd.DataFrame() #Create a new DataFrame for submission\nsub4['Id'] = test_ID\nsub4['SalePrice'] = pred4\nsub4.to_csv(\"submissionxgb2.csv\", index=False) #Convert DataFrame to .csv file\nsub4\"\"\"","bae27330":"lightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       )","ad0bfc06":"# fiting\nlightgbm_fit=lightgbm.fit(X_train, y_train)","79b67e62":"#prediction on train set\ny_train_predict = lightgbm_fit.predict(X_train)\n#prediction on test set\ny_test_predict = lightgbm_fit.predict(X_test)\n#MSE trainset\nlightgbm_train = mean_squared_error(y_train, y_train_predict)\nprint('Mean square error on the Train set is: {}'.format(lightgbm_train))\n#MSE test set\nlightgbmRMS = mean_squared_error(y_test, y_test_predict)\nprint('Mean square error on the Test set is: {}'.format(lightgbmRMS))","14b7ea2f":"pred3=np.expm1(lightgbm_fit.predict(newTest))\nsub3 = pd.DataFrame() #Create a new DataFrame for submission\nsub3['Id'] = test_ID\nsub3['SalePrice'] = pred3\nsub3.to_csv(\"submissionlight4.csv\", index=False) #Convert DataFrame to .csv file\nsub3","33e94b30":"# fiting\nlightgbm_fit2=lightgbm.fit(X_train2, y_train2)\n#prediction on train set\ny_train_predict2 = lightgbm_fit2.predict(X_train2)\n#prediction on test set\ny_test_predict2 = lightgbm_fit2.predict(X_test2)\n#MSE trainset\nlightgbm_train2 = mean_squared_error(y_train2, y_train_predict2)\nprint('Mean square error on the Train set is: {}'.format(lightgbm_train2))\n#MSE test set\nlightgbmRMS2 = mean_squared_error(y_test2, y_test_predict2)\nprint('Mean square error on the Test set is: {}'.format(lightgbmRMS2))\n\"\"\"pred3=np.expm1(lightgbm_fit.predict(newTest))\nsub3 = pd.DataFrame() #Create a new DataFrame for submission\nsub3['Id'] = test_ID\nsub3['SalePrice'] = pred3\nsub3.to_csv(\"submissionlight4.csv\", index=False) #Convert DataFrame to .csv file\nsub3\"\"\"","a8343f69":"#hyperparameter tuning\n#ada=AdaBoostRegressor()\n#search_grid={'n_estimators':[500,1000,2000],'learning_rate':[.001,0.01,.1],'random_state':[1]}\n#search=GridSearchCV(estimator=ada,param_grid=search_grid,scoring='neg_mean_squared_error',n_jobs=1,cv=5)\n#search.fit()","036b6ae9":"#cr\u00e9er le mod\u00e8le\nadab=AdaBoostRegressor(n_estimators=500,learning_rate=0.001,random_state=1)\nscore=np.mean(cross_val_score(adab,X_train, y_train,scoring='neg_mean_squared_error',cv=7,n_jobs=1))\nscore","7f5f096e":"# fiting\nadab_fit=adab.fit(X, Y)","9c4f97c7":"#prediction on train set\ny_train_predict = adab_fit.predict(X_train)\n#prediction on test set\ny_test_predict = adab_fit.predict(X_test)\n#MSE trainset\nadab_train = mean_squared_error(y_train, y_train_predict)\nprint('Mean square error on the Train set is: {}'.format(adab_train))\n#MSE test set\nadabRMS = mean_squared_error(y_test, y_test_predict)\nprint('Mean square error on the Test set is: {}'.format(adabRMS))","e1aaaed0":"\"\"\"pred4=np.expm1(xgboost_fit.predict(newTest))\nsub4 = pd.DataFrame() #Create a new DataFrame for submission\nsub4['Id'] = test_ID\nsub4['SalePrice'] = pred4\nsub4.to_csv(\"submissionadab.csv\", index=False) #Convert DataFrame to .csv file\nsub4\"\"\"","e521ce09":"#cr\u00e9er le mod\u00e8le\nadab=AdaBoostRegressor(n_estimators=500,learning_rate=0.001,random_state=1)\nscore2=np.mean(cross_val_score(adab,X_train2, y_train2,scoring='neg_mean_squared_error',cv=7,n_jobs=1))\nscore2\n\n# fiting\nadab_fit2=adab.fit(X2, Y2)\n\n#prediction on train set\ny_train_predict2 = adab_fit2.predict(X_train2)\n#prediction on test set\ny_test_predict2 = adab_fit2.predict(X_test2)\n#MSE trainset\nadab_train2 = mean_squared_error(y_train2, y_train_predict2)\nprint('Mean square error on the Train set is: {}'.format(adab_train2))\n#MSE test set\nadabRMS2 = mean_squared_error(y_test2, y_test_predict2)\nprint('Mean square error on the Test set is: {}'.format(adabRMS2))\n\n\"\"\"pred4=np.expm1(xgboost_fit.predict(newTest))\nsub4 = pd.DataFrame() #Create a new DataFrame for submission\nsub4['Id'] = test_ID\nsub4['SalePrice'] = pred4\nsub4.to_csv(\"submissionadab.csv\", index=False) #Convert DataFrame to .csv file\nsub4\"\"\"","b9e6bc92":"from sklearn.ensemble import RandomForestRegressor\n#Cr\u00e9er model\nforest = RandomForestRegressor(n_estimators=100)\n#fitting\nforest.fit(X, Y)\ny_test_predict = forest.predict(X_test)","7c9d169f":"#MSE test set\nforestRMS = mean_squared_error(y_test, y_test_predict)\nprint('Mean square error on the Test set is: {}'.format(forestRMS))","7da16bbb":"pred4=np.expm1(forest.predict(newTest))\nsub4 = pd.DataFrame() #Create a new DataFrame for submission\nsub4['Id'] = test_ID\nsub4['SalePrice'] = pred4\nsub4.to_csv(\"submissionforestfin.csv\", index=False) #Convert DataFrame to .csv file\nsub4","90ddd084":"from sklearn.ensemble import RandomForestRegressor\n#Cr\u00e9er model\nforest = RandomForestRegressor(n_estimators=100)\n#fitting\nforest.fit(X2, Y2)\ny_test_predict2 = forest.predict(X_test2)\n\n#MSE test set\nforestRMS2 = mean_squared_error(y_test2, y_test_predict2)\nprint('Mean square error on the Test set is: {}'.format(forestRMS2))\n\n\"\"\"pred4=np.expm1(forest.predict(newTest))\nsub4 = pd.DataFrame() #Create a new DataFrame for submission\nsub4['Id'] = test_ID\nsub4['SalePrice'] = pred4\nsub4.to_csv(\"submissionforestfin.csv\", index=False) #Convert DataFrame to .csv file\nsub4\"\"\"","4091e6b7":"recap = pd.DataFrame({'RMSE': pd.Series([knnRMS,logirRMS,lrRMSE,lassoRMS,ridgeRMS,svrRMS,grboRMS,xgboostRMS,lightgbmRMS, adabRMS, forestRMS], index = ['knn','logiReg','linReg','Lasso','Ridge','SVR','gradboost','Xgboost','lgbm', 'adaboost', 'rdForest']), 'RMSE2': pd.Series([knnRMS2,logirRMS2,lrRMSE2,lassoRMS2,ridgeRMS2,svrRMS2,grboRMS2,xgboostRMS2,lightgbmRMS2, adabRMS2, forestRMS2], index = ['knn','logiReg','linReg','Lasso','Ridge','SVR','gradboost','Xgboost','lgbm', 'adaboost', 'rdForest'])})","bb8f69b3":"recap","4592eaea":"* ***visualiser les ouliers*** ","262d732f":"##### Apres reduction","a7b4c2b6":"### PCA","501f0158":"##### Apres reduction","0273b98d":"#### Apres reduction","e5a146ae":"### La r\u00e9gression logistique","244ed370":"## 2-Apprentissage du mod\u00e8le","6e486801":"### Light Gradient Boosting Machine","9135d19b":"##### Apres reduction","d41881a5":"#### Apres reduction","460a427b":"### KNN","7107d822":"#### Apres reduction","ba8c07d3":"## *1.2-stastical analysis*","2c19b437":"### Random Forest","aa13c9d3":"# Comparaison","ba9c9f90":"##### apres Red","4aacaf4e":"### 1.3.2 transformer les string en int","219b6668":"###  AdaBoost","00f774fa":" ## 1-Exploration et pr\u00e9paration des donn\u00e9es","0b718a14":"##### Apres reduction ","d19cabe0":"##### Apres reduction","6807ac8b":"### 1.3.3 Les ouliers","4d7e6ace":"### Corr\u00e9lation","1c8c4053":"## *1.1-Data ingestion*","c0d77736":"### gradiant boosting","890a3770":"### LASSO","2a66671f":"## 2-agr\u00e9gation","566aa5ae":"#### Apres reduction","5c9414c2":"### La r\u00e9gression d\u2019ar\u00eates(Ridge)","5c0006b4":"## Reduction de dimentionnalit\u00e9","ea978003":"* ***distribution de la variable 'SalePrice'***","5388cbbb":"### linear regression:","712fda1e":"### SVR","9e1e97ce":"## *1.3 Data cleaning:*","c996da12":"##### Apres reduction","8184da92":"### 1.3.1 les valeurs nulles","e3af9c73":"* ***La cor\u00e9lation avec 'Sale Price*** ","a0d09561":"### Data processing:","64a24a66":"### xgboost"}}