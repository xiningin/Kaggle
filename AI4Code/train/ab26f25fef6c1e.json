{"cell_type":{"e2b05f7d":"code","b9981336":"code","2b12b241":"code","63c2f582":"code","f8920b82":"code","d1dd70eb":"code","1f178740":"code","ce7f9383":"code","9dc31a24":"code","df672df8":"code","d25811af":"code","6a37eedf":"code","9a31003c":"code","fe69f3bb":"code","4096f99d":"code","55e6ff10":"code","1fe729e9":"code","1a64b08d":"markdown","9b5904a2":"markdown","049932dd":"markdown","591d0861":"markdown","f2914508":"markdown","c1659bfc":"markdown","09f8c877":"markdown","8733a1e4":"markdown"},"source":{"e2b05f7d":"!git clone https:\/\/github.com\/YuanGongND\/ast.git --quiet\n!pip install llvmlite --quiet\n!pip install wget --quiet\n!pip install zipp --quiet\n!pip install wandb --upgrade --quiet\n!pip install nnAudio --quiet\n!pip install pytorch_lightning --quiet","b9981336":"!pip install timm==0.4.5","2b12b241":"import sys\nsys.path.append('.\/ast\/src')","63c2f582":"import os\nimport glob\nimport wandb\nimport shutil\n\nfrom PIL import Image\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport timm\nimport torch\nfrom models import ASTModel\nimport pytorch_lightning as pl\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom nnAudio.Spectrogram import CQT1992v2\n\nfrom tqdm.notebook import tqdm\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","f8920b82":"%cd .\/ast\/\n\nbase_model = ASTModel(label_dim=1,\n                     fstride=5, tstride=5, \\\n                     input_fdim=69, input_tdim=193, \\\n                     imagenet_pretrain=True, audioset_pretrain=False, \\\n                     model_size='base384')","d1dd70eb":"%cd \/kaggle\/working\n\ntrain = pd.read_csv('..\/input\/g2net-gravitational-wave-detection\/training_labels.csv')\ntest = pd.read_csv('..\/input\/g2net-gravitational-wave-detection\/sample_submission.csv')\n\ndef get_train_file_path(image_id):\n    return \"..\/input\/g2net-gravitational-wave-detection\/train\/{}\/{}\/{}\/{}.npy\".format(\n        image_id[0], image_id[1], image_id[2], image_id)\n\ndef get_test_file_path(image_id):\n    return \"..\/input\/g2net-gravitational-wave-detection\/test\/{}\/{}\/{}\/{}.npy\".format(\n        image_id[0], image_id[1], image_id[2], image_id)\n\ntrain['file_path'] = train['id'].apply(get_train_file_path)\ntest['file_path'] = test['id'].apply(get_test_file_path)\n\ndisplay(train.head(2))\ndisplay(test.head(2))","1f178740":"wandb.login()","ce7f9383":"# ====================================================\n# CFG\n# ====================================================\nclass Config:\n    debug = False\n    num_workers = 4\n    epochs = 3\n    lr = 1e-4\n    weight_decay = 1e-6\n    batch_size = 32\n    seed = 1234\n    target_size = 1\n    n_folds = 5\n    target_col = 'target'\n    LOSS = torch.nn.BCEWithLogitsLoss()\n    epochs = 10\n    dev_run = False\n\nif Config.debug:\n    train = train.sample(n=50000, random_state=Config.seed).reset_index(drop=True)\n    Config.epochs = 1\n    Config.dev_run = True\n    \n\npl.seed_everything(Config.seed)","9dc31a24":"train, valid = train_test_split(train, test_size=0.3, stratify=train['target'])","df672df8":"# ====================================================\n# Dataset\n# ====================================================\nclass TrainDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.file_names = df['file_path'].values\n        self.labels = df[Config.target_col].values\n        self.wave_transform = CQT1992v2(sr=2048, fmin=20, fmax=1024, hop_length=64)\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def apply_qtransform(self, waves, transform):\n        waves = np.hstack(waves)\n        waves = waves \/ np.max(waves)\n        waves = torch.from_numpy(waves).float()\n        image = transform(waves)\n        return image\n\n    def __getitem__(self, idx):\n        file_path = self.file_names[idx]\n        waves = np.load(file_path)\n        image = self.apply_qtransform(waves, self.wave_transform)\n        if self.transform:\n            image = image.squeeze().numpy()\n            image = self.transform(image=image)['image']\n        label = torch.tensor(self.labels[idx]).float()\n        return image[0], label","d25811af":"# ====================================================\n# Transforms\n# ====================================================\ndef get_transforms(*, data):\n    \n    if data == 'train':\n        return A.Compose([\n            ToTensorV2(),\n        ])\n\n    elif data == 'valid':\n        return A.Compose([\n            ToTensorV2(),\n        ])","6a37eedf":"train_dataset = TrainDataset(train, transform=get_transforms(data='train'))\n\ntrain_dl = DataLoader(train_dataset, \n                      batch_size=Config.batch_size,\n                      num_workers=Config.num_workers,\n                      shuffle=True,\n                      pin_memory=True)\n\nvalid_dataset = TrainDataset(valid, transform=get_transforms(data='valid'))\n\nvalid_dl = DataLoader(valid_dataset,\n                     batch_size=Config.batch_size,\n                     num_workers=Config.num_workers,\n                     pin_memory=True)","9a31003c":"sample = None\nfor i in train_dl:\n    print(i[0].shape, i[1])\n    sample = i[0]\n    break","fe69f3bb":"class Classifier(pl.LightningModule):\n    \n    def __init__(self):\n        super(Classifier, self).__init__()\n        self.model = base_model\n        \n    def forward(self, x):\n        output = self.model(x)\n        return output\n    \n    def training_step(self, batch, batch_no):\n        images, labels = batch\n        outputs = self(images)\n        loss = Config.LOSS(outputs.view(-1), labels)\n        return loss\n    \n    def validation_step(self, batch, batch_no):\n        images, labels = batch\n        outputs = self(images)\n        loss = Config.LOSS(outputs.view(-1), labels)\n        return loss\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=Config.lr, \n                                     weight_decay=Config.weight_decay)\n        return optimizer","4096f99d":"model = Classifier()","55e6ff10":"wandb_logger = pl.loggers.WandbLogger(project='G2Net')\n\ntrainer = pl.Trainer(gpus=1, max_epochs=Config.epochs, fast_dev_run=Config.dev_run, logger=wandb_logger)","1fe729e9":"trainer.fit(model, train_dl, valid_dl)","1a64b08d":"# Data Loading","9b5904a2":"# MODEL","049932dd":"# Simple train valid split","591d0861":"# <a href='https:\/\/github.com\/YuanGongND\/ast'>AST: Audio Spectrogram Transformer<\/a>\n\n**This don't need to resize your input. Also I don't know what the hyper-parameters in the model exactly mean So most likely I made a huge mistake somewhere in the hyperparameters but I have made a quick implementation of the model for the community. Here you go.** ","f2914508":"# Dataset","c1659bfc":"# Transforms","09f8c877":"# Importing Libraries","8733a1e4":"# CFG"}}