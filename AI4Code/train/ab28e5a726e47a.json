{"cell_type":{"0a622144":"code","1f449469":"code","c1a247bc":"code","71fbb4e1":"code","336990cf":"code","ec9ef17e":"code","a0ac9fd0":"code","73fdaf96":"code","d399522a":"code","32e0aa37":"code","21db7a52":"code","2fbd0848":"code","0114831b":"code","19085f20":"code","b08de25f":"code","4992844f":"code","7a0c4556":"markdown","fae234c8":"markdown"},"source":{"0a622144":"# Locally I created classes with a similar behaviour that we have in this competition\n# It slowed me down during validation phase and I decided to refactor code for lags creation\n\n# from twosigmanews import *\nfrom kaggle.competitions import twosigmanews","1f449469":"import pandas as pd\nimport numpy as np\nimport gc\nfrom resource import getrusage, RUSAGE_SELF\nfrom datetime import date, datetime\n\nimport multiprocessing\nfrom multiprocessing import Pool, cpu_count\n\nimport warnings\nwarnings.filterwarnings('ignore')","c1a247bc":"global STARTED_TIME\nSTARTED_TIME = datetime.now()\n\n# It's better to use cpu_count from the system - who knows what happens during test phase\nglobal N_THREADS\nN_THREADS=multiprocessing.cpu_count()\n\nprint(f'N_THREADS: {N_THREADS}')","71fbb4e1":"# FILTERDATE - start date for the train data\nFILTERDATE = date(2007, 1, 1)\n\n# SAMPLEDATE - I use it for sampling and fast sanity check of scripts\nSAMPLEDATE = None\n# SAMPLEDATE = date(2007, 1, 30)","336990cf":"global N_LAG, RETURN_FEATURES\n\n# Let's try how it works for 1-year lags\nN_LAG = np.sort([5, 10, 20, 252])\n\n# Features for lags calculation\nRETURN_FEATURES = [\n    'returnsOpenPrevMktres10',\n    'returnsOpenPrevRaw10',\n    'open',\n    'close']","ec9ef17e":"# Tracking time and memory usage\nglobal MAXRSS\nMAXRSS = getrusage(RUSAGE_SELF).ru_maxrss\ndef using(point=\"\"):\n    global MAXRSS, STARTED_TIME\n    print(str(datetime.now()-STARTED_TIME).split('.')[0], point, end=' ')\n    max_rss = getrusage(RUSAGE_SELF).ru_maxrss\n    if max_rss > MAXRSS:\n        MAXRSS = max_rss\n    print(f'max RSS {MAXRSS\/1024\/1024:.1f}Gib')\n    gc.collect();","a0ac9fd0":"# I've slightly optimized this function from https:\/\/www.kaggle.com\/qqgeogor\/eda-script-67\n# It uses just one loop ofr N_LAG\n\nglobal FILLNA\nFILLNA = -1\n\ndef create_lag(df_code):\n    prevlag = 1    \n    for window in N_LAG:\n        rolled = df_code[RETURN_FEATURES].shift(prevlag).rolling(window=window)\n        # Mean is not so stable as median if you have assets with very high\/low beta risk factor\n        # df_code = df_code.join(rolled.mean().add_suffix(f'_lag_{window}_mean'))\n        df_code = df_code.join(rolled.median().add_suffix(f'_lag_{window}_median'))\n        df_code = df_code.join(rolled.max().add_suffix(f'_lag_{window}_max'))\n        df_code = df_code.join(rolled.min().add_suffix(f'_lag_{window}_min'))\n\n        # We also have an idea to make lags uncorrelated - in this case you may uncomment\n        # the following line. N_LAG have to be sorted\n\n        # prevlag = window\n    return df_code.fillna(FILLNA)\n\ndef generate_lag_features(df):\n    global RETURN_FEATURES, N_THREADS\n    all_df = []\n    df_codes = df.groupby('assetCode')\n    df_codes = [df_code[1][['time','assetCode']+RETURN_FEATURES] for df_code in df_codes]\n    \n    pool = Pool(N_THREADS)\n    all_df = pool.map(create_lag, df_codes)\n    \n    new_df = pd.concat(all_df)  \n    new_df.drop(RETURN_FEATURES,axis=1,inplace=True)\n    pool.close()\n    \n    return new_df","73fdaf96":"# Pre-processing of dataframe, this functions is the same for train and test periods\n# In production we had more calculations\n\ndef preparedf(df):\n    df['time'] = df['time'].dt.date\n    return df","d399522a":"# The following functions are used for initialization and expanding of numpy arrays\n# for storing historical data of all assets.\n\n# It helps to have very fast lags creation.\n\n# Initialization of history array\ndef initialize_values(items=5000, features=4, history=15):\n    return np.ones((items, features, history))*np.nan\n\n# Expanding of history array for new assets\ndef add_values(a, items=100):\n    return np.concatenate([a, initialize_values(items, a.shape[1], a.shape[2])])\n\n# codes dictionary maps assetCode to the index in the history array\n# if we found new code - we have to store it and expand history\ndef get_code(a):\n    global codes, history\n    try: \n        return codes[a]\n    except KeyError:\n        codes[a] = len(codes)\n        if len(codes) > history.shape[0]:\n            history = add_values(history, 100)\n        return codes[a]\n\n# list2codes returns numpy array of indexes of assetCodes (for each day)\ndef list2codes(l):\n    return np.array([get_code(a) for a in l])","32e0aa37":"env = twosigmanews.make_env()\n(market_train_df, news_train_df) = env.get_training_data()\nusing('Done')","21db7a52":"print('Dataframe pre-processing')\nmarket_train_df = preparedf(market_train_df)\nusing('Done')","2fbd0848":"# Dataframe filtering\nprint('DF Filtering')\nmarket_train_df = market_train_df.loc[market_train_df['time']>=FILTERDATE]\n\nif SAMPLEDATE is not None:\n    market_train_df = market_train_df.loc[market_train_df['time']<=SAMPLEDATE]  \nusing('Done')","0114831b":"print('Lag features generation')\nnew_df = generate_lag_features(market_train_df)\nusing('Done')","19085f20":"print('DF Merging')\nmarket_train_df = pd.merge(market_train_df,new_df,how='left',on=['time','assetCode'])\ndel new_df\nusing('Done')","b08de25f":"print('Preparation for the prediction')\n# codes maps assetCodes with indexes into history array\ncodes = dict(\n    zip(market_train_df.assetCode.unique(), np.arange(market_train_df.assetCode.nunique()))\n)\n# history stores information for all assets, required features\n# np.max(LAG)+1 - to store information for maximum beriod and the current day (+1)\nhistory = initialize_values(len(codes), len(RETURN_FEATURES), np.max(N_LAG)+1)\n\n# Get the latest information for assets\nlatest_events = market_train_df.groupby('assetCode').tail(np.max(N_LAG)+1)\n# but we may have different informations size for different assets\nlatest_events_size = latest_events.groupby('assetCode').size()\n\n# Filling the history array\nfor s in latest_events_size.unique():\n    for i in range(len(RETURN_FEATURES)):\n        # l is a Dataframe with assets with the same history size for each asset\n        l = latest_events[\n            latest_events.assetCode.isin(latest_events_size[latest_events_size==s].index.values)\n        ].groupby('assetCode')[RETURN_FEATURES[i]].apply(list)\n\n        # v is a 2D array contains history information of feature RETURN_FEATURES[i] \n        v = np.array([k for k in l.values])\n\n        # r contains indexes (in the history array) of all assets\n        r = list2codes(l.index.values)\n\n        # Finally, filling history array\n        history[r, i, -s:] = v\n        del l, v, r\n\ndel latest_events, latest_events_size\nusing('Done')","4992844f":"print('Prediction')\n#prediction\ndays = env.get_prediction_days()\nn_days = 0\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    if n_days % 100 == 0:\n        using(f'{n_days}')\n    # Test data preprocessing    \n    market_obs_df = preparedf(market_obs_df)\n\n    # Getting indexes of asses for the current day\n    r = list2codes(market_obs_df.assetCode.values)\n\n    # Shifting history by 1 for assets of the current day\n    history[r, :, :-1] = history[r, :, 1:] \n\n    # Filling history with a new data\n    history[r, :, -1] = market_obs_df[RETURN_FEATURES].values\n\n    prevlag = 1    \n    for lag in np.sort(N_LAG):\n        lag_median = np.median(history[r, : , -lag:-prevlag], axis=2)\n        lag_median[np.isnan(lag_median)] = FILLNA\n        lag_max = history[r, : , -lag-1:-prevlag].max(axis=2)\n        lag_max[np.isnan(lag_max)] = FILLNA\n        lag_min = history[r, : , -lag-1:-prevlag].min(axis=2)\n        lag_min[np.isnan(lag_min)] = FILLNA\n\n        for ix in range(len(RETURN_FEATURES)):\n            market_obs_df[f'{RETURN_FEATURES[ix]}_lag_{lag}_median'] = lag_median[:, ix]\n            market_obs_df[f'{RETURN_FEATURES[ix]}_lag_{lag}_min'] = lag_min[:, ix]\n            market_obs_df[f'{RETURN_FEATURES[ix]}_lag_{lag}_max'] = lag_max[:, ix]\n            \n#         prevlag=lag\n\n    confidence = 0\n    \n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':confidence})\n\n    predictions_template_df = predictions_template_df.merge(preds,how='left')\\\n    .drop('confidenceValue',axis=1)\\\n    .fillna(0)\\\n    .rename(columns={'confidence':'confidenceValue'})\n    \n    env.predict(predictions_template_df)\n    \nusing('Prediction done')\n\n# env.write_submission_file()\n# using('Done')","7a0c4556":"Let's start","fae234c8":"This kernel shows how to prepare lags separately for train and test phases"}}