{"cell_type":{"2eb16d7b":"code","78ddf27c":"code","3ce1ff24":"code","b09ae7a0":"code","2d7bd70e":"code","9a6108b4":"code","66b48dc0":"code","d5ca5ce1":"code","96997137":"code","3799b3cd":"code","f24afdd9":"code","535f7732":"code","9d4c1fdc":"code","7c35b473":"code","0c2f5e8e":"code","68d23024":"code","c7235017":"code","528611c9":"code","87500820":"code","85721c53":"code","aa650643":"code","0d6c205c":"code","afa1759e":"code","cb407d84":"code","97d0e6d7":"code","10693cdf":"markdown","8e4f535f":"markdown","5f68579f":"markdown","c407fd31":"markdown","1b8cec47":"markdown","dc0ecd60":"markdown","b05fca32":"markdown","a35cc3b2":"markdown","7848510d":"markdown","b306aac5":"markdown","12ef07f0":"markdown","66207da6":"markdown","e9397016":"markdown","b404f181":"markdown"},"source":{"2eb16d7b":"import numpy as np\nimport pandas as pd \n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#SMOTE \nimport imblearn\nfrom imblearn.over_sampling import SMOTE\n\n#LDA \nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.preprocessing import StandardScaler\n\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.options.mode.chained_assignment = None\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","78ddf27c":"data = pd.read_csv('..\/input\/glass\/glass.csv')\ndata.head()","3ce1ff24":"#get information about the data types\ndata.info()","b09ae7a0":"data.isnull().sum()","2d7bd70e":"data.describe()","9a6108b4":"dups = data.duplicated()\nprint('Number of duplicate rows: %d' % dups.sum())","66b48dc0":"#drop duplicated value\nprint('Number of rows before discarding duplicates = %d' % data.shape[0])\n\ndata2 = data.drop_duplicates()\nprint('Number of rows after discarding duplicates = %d' % data2.shape[0])","d5ca5ce1":"data2.Type.value_counts()","96997137":"sns.set(style = 'whitegrid', font_scale = 1.8)\nplt.subplots(figsize = (12,7))\nsns.countplot(x = 'Type', data = data2, palette = 'Pastel1')","3799b3cd":"target_class_name = 'Type'\nfeatures = ['RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe']\n\nsns.set(style = 'whitegrid', font_scale = 1.2, palette = 'Paired')\nplt.subplots(figsize = (20,15))\n\nfor i in range(1,10):\n    plt.subplot(3,3,i)\n    sns.boxplot(x = target_class_name, y = features[i-1], data = data2)\n","f24afdd9":"correlation = data2[features].corr()\nmask = np.zeros_like(correlation)\nmask[np.triu_indices_from(mask)] = True\nplt.subplots(figsize = (10, 10))\nsns.heatmap(correlation, mask = mask, cmap = 'YlGnBu', annot = True, linewidth = .5, square = True)","535f7732":"X = pd.DataFrame(data2.drop([\"Type\"], axis = 1),\n            columns=['RI','Na','Mg','Al','Si','K','Ca','Ba','Fe'])\ny = data2.Type\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 40, stratify = y)","9d4c1fdc":"knn = KNeighborsClassifier()\nknn_params = {'n_neighbors': np.arange(1,50)}\nknn_cv_model = GridSearchCV(knn, knn_params, cv = 10).fit(X_train, y_train)\n\nn_neig = knn_cv_model.best_params_['n_neighbors']","7c35b473":"knn_tuned = KNeighborsClassifier(n_neighbors = n_neig).fit(X_train, y_train)\ny_pred = knn_tuned.predict(X_test)\nknn_accuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy score for KNN: {}'.format(knn_accuracy))","0c2f5e8e":"#Confusion Matrix of KNN\nsns.heatmap(confusion_matrix(y_test, y_pred), annot = True, lw = 2, cbar = False, cmap = 'GnBu')\nplt.ylabel('True values')\nplt.xlabel('Predicted values')\nplt.title('Confusion Matrix')\nplt.show()","68d23024":"#Classification report for KNN\nprint(classification_report(y_test, y_pred))","c7235017":"sns.set(style = 'whitegrid', font_scale = 1.8)\nplt.subplots(figsize = (12,7))\nsns.countplot(x = y, palette = 'Pastel1').set_title('Before SMOTE')","528611c9":"sm = SMOTE(sampling_strategy = 'not majority', random_state = 42)\nx_res, y_res = sm.fit_resample(X, y)\ny_res_df = pd.DataFrame(y_res)\n\n\nsns.set(style = 'whitegrid', font_scale = 1.8)\nplt.subplots(figsize = (12,7))\nsns.countplot(x = y_res_df.Type, palette = 'Pastel1').set_title('After SMOTE')\n","87500820":"print(x_res.shape)\nprint(y_res.shape)\n\ny_res.value_counts()","85721c53":"X_train, X_test, y_train, y_test = train_test_split(x_res, y_res, test_size = .2, random_state = 40, stratify = y_res)\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n","aa650643":"lda = LDA(n_components = 5)\nX_train = lda.fit_transform(X_train, y_train)\nX_test = lda.transform(X_test)\n\nprint(X_train.shape)\nprint(X_test.shape)","0d6c205c":"knn = KNeighborsClassifier()\nknn_params = {'n_neighbors': np.arange(1,50)}\nknn_cv_model = GridSearchCV(knn, knn_params, cv = 10).fit(X_train, y_train)\nn_neighbor = knn_cv_model.best_params_['n_neighbors']","afa1759e":"knn_tuned = KNeighborsClassifier(n_neighbors = n_neighbor).fit(X_train,y_train)\nknn_y_pred = knn_tuned.predict(X_test)\nknn_acc = accuracy_score(y_test, knn_y_pred)\nprint('Accuracy score for KNN: {}'.format(knn_acc))","cb407d84":"#Confusion Matrix of KNN\nsns.heatmap(confusion_matrix(y_test, knn_y_pred), annot = True, lw = 2, cbar = False, cmap = 'GnBu')\nplt.ylabel('True values')\nplt.xlabel('Predicted values')\nplt.title('Confusion Matrix')\nplt.show()","97d0e6d7":"#Classification report for KNN\nprint(classification_report(y_test, knn_y_pred))","10693cdf":"The statistical results of the data can help us make some inferences about the dataset.\nWith describe() method, many information such as the mean values of the data, standard deviation values etc are displayed. \n\nFor example it is seen that the highest mean value is in Si feature.","8e4f535f":"It is set to be 80% training data and 20% test data.","5f68579f":"### KNN Classification again","c407fd31":"An alternative to using classification accuracy is to use precision and recall metrics.\n\n##### -> Precision quantifies the number of positive class predictions that actually belong to the positive class.\n##### -> Recall quantifies the number of positive class predictions made out of all positive examples in the dataset.\n##### -> F-Measure provides a single score that balances both the concerns of precision and recall in one number.","1b8cec47":"### **correlation coefficients**\nCorrelation is the relationship between two variables. Values above 0.70, 0.80 indicate a high correlation. Finding highly correlated features in the dataset can be misleading for the model. \nAttention should be paid to feature selection while applying the model.\n\n\nThere is a high correlation of 0.81 between Ca and RI in the dataset.\n##### Ba - Al -> 0.48\n##### Ba - Na -> 0.33\nThere is a negative correlation between Si and RI with a value of -0.54.","dc0ecd60":"### K-Nearest Neighbors Classification\nIt is a widely used supervised machine learning algorithm. In the work of the algorithm, a value of k is determined, the meaning of k value is the number of elements to look at. When a value comes, the distance between the value is calculated by taking the nearest k elements. Euclidean, Manhattan, Minkowski and Hamming functions are used for distance calculation. After the distances are calculated, they are sorted and the corresponding value is assigned to the appropriate class.\n\nWith the help of GridSearchCV, after calculating the best value in the range of (1, 50) of the number of neighbors to be given to the model, the accuracy value was calculated as 0.7674.","b05fca32":"### SMOTE (Synthetic Minority Over-sampling Technique)\n\nOne of the methods that can be used to eliminate the imbalance in the data set is resampling. It has two methods:\n\nThe first method is to increase the data belonging to the minority classes to obtain classes with an equal number of data. It is called oversampling.\n\nAnother method is to obtain a balanced data set by extracting the data belonging to the weighted class from the data set. It is called undersampling.\n\n\nThe oversampling method was preferred because the number of samples in the data set used is small and the undersampling process will cause data loss. In order to apply this method, the SMOTE technique in imbalanced-learn library was used and synthetic data belonging to minority classes were produced.\n","a35cc3b2":"### **Checking null values**\nMissing values in the dataset can cause problems. Before classification, the missing values problem should be solved.\nThere is no null value in the dataset.","7848510d":"### **Checking dataset imbalanced**\n\nThe sets of data in which classes are not evenly distributed, approximately the same number for its class are called imbalanced datasets.The high accuracy value of the model can be caused by a certain class.\n\nvalue_counts() method shows how many samples it is for the glass type. Type 2 and Type 1 have more samples than other types.","b306aac5":"Accuracy is the number of correctly predicted data points out of all the data points.\nAs a performance measure, accuracy is inappropriate for imbalanced classification problems. Therefore, it is necessary to evaluate the results of other metrics together.\n\nThe confusion matrix that enables the calculation of the metrics that measure the success of the classification models was created.","12ef07f0":"#### Standard Scaler\n\nSituations such as the distribution of data and the scale differences between features are factors that affect the operation of algorithms. In this case, pulling the features into a common data range enables us to obtain more accurate results.","66207da6":"### **Checking outliers**","e9397016":"### LDA (Linear Discriminant Analysis)\nIt is used as a size reduction technique. It reduces the size of the data set, maximizing the difference between classes. The goal is to prevent overfitting and reduce computational costs.\n\nIn the data set, 9 attributes were reduced to 5 significant components with the help of LDA.","b404f181":"### **Checking duplicate values**\nRepeated data may prevent the model from producing correct results.\nduplicated() method, it is checked whether there is a repeating value in the dataset. \n"}}