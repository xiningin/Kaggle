{"cell_type":{"0e9477f0":"code","b8d672d7":"code","7244066f":"code","69e7aea3":"code","2006139d":"code","eaa5937a":"code","24f2290e":"code","7b987f2b":"code","6a18e0b0":"code","d7cd43d2":"code","f46ba22e":"code","18975f1b":"code","d7c16b18":"code","215af0de":"code","e180c82d":"markdown","4593ef4f":"markdown","833057ff":"markdown","c3fb78d1":"markdown","c7e1d8c5":"markdown","d0505c28":"markdown","8a3fc71f":"markdown","2453d6aa":"markdown","b1143841":"markdown","5e08f1fb":"markdown","08e26e46":"markdown"},"source":{"0e9477f0":"# Path to TF model\n# We will use model from https:\/\/www.kaggle.com\/khanhlvg\/cots-detection-w-tensorflow-object-detection-api for demo purposes\nMODEL_DIR = '..\/input\/cots-detection-w-tensorflow-object-detection-api\/cots_efficientdet_d0'\n\n# Detection parameters\nDETECTION_THRESHOLD  = 0.3\nMAX_BOXES = 10","b8d672d7":"import numpy as np\nimport pandas as pd \nimport torch\nfrom tqdm import tqdm\nimport sys\nimport tensorflow as tf\n\nfrom PIL import Image\nfrom PIL import ImageColor\nfrom PIL import ImageDraw\nfrom PIL import ImageFont\nfrom PIL import ImageOps\n\nsys.path.append('..\/input\/tensorflow-great-barrier-reef')","7244066f":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport importlib\nimport cv2 \n\nimport ast\nimport shutil\nimport sys\nimport time\n\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\n\nfrom PIL import Image\nfrom IPython.display import display","69e7aea3":"start_time = time.time()\ntf.keras.backend.clear_session()\ndetect_fn_tf_odt = tf.saved_model.load(os.path.join(os.path.join(MODEL_DIR, 'output'), 'saved_model'))\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint('Elapsed time: ' + str(elapsed_time) + 's')","2006139d":"# Install ffmpeg for video compression\n%cd \/kaggle\/working\n\n! tar -xf ..\/input\/ffmpeg-static-build\/ffmpeg-git-amd64-static.tar.xz\n\nimport subprocess\n\nFFMPEG_BIN = \"\/kaggle\/working\/ffmpeg-git-20191209-amd64-static\/ffmpeg\"","eaa5937a":"# https:\/\/www.tensorflow.org\/hub\/tutorials\/object_detection\ndef draw_bounding_box_on_image(image,\n                               ymin,\n                               xmin,\n                               ymax,\n                               xmax,\n                               color,\n                               font,\n                               thickness=4,\n                               display_str_list=()):\n    \"\"\"Adds a bounding box to an image.\"\"\"\n    draw = ImageDraw.Draw(image)\n    im_width, im_height = image.size\n    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n                                ymin * im_height, ymax * im_height)\n    draw.line([(left, top), (left, bottom), (right, bottom), (right, top),\n             (left, top)],\n            width=thickness,\n            fill=color)\n\n    # If the total height of the display strings added to the top of the bounding\n    # box exceeds the top of the image, stack the strings below the bounding box\n    # instead of above.\n    display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n    # Each display_str has a top and bottom margin of 0.05x.\n    total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n\n    if top > total_display_str_height:\n        text_bottom = top\n    else:\n        text_bottom = top + total_display_str_height\n    # Reverse list and print from bottom to top.\n    for display_str in display_str_list[::-1]:\n        text_width, text_height = font.getsize(display_str)\n        margin = np.ceil(0.05 * text_height)\n        draw.rectangle([(left, text_bottom - text_height - 2 * margin),\n                        (left + text_width, text_bottom)],\n                       fill=color)\n        draw.text((left + margin, text_bottom - text_height - margin),\n                  display_str,\n                  fill=\"black\",\n                  font=font)\n        text_bottom -= text_height - 2 * margin\n\n\ndef draw_boxes(image, boxes, class_names, scores, color, max_boxes=MAX_BOXES, min_score=DETECTION_THRESHOLD):\n    \"\"\"Overlay labeled boxes on an image with formatted scores and label names.\"\"\"\n    colors = list(ImageColor.colormap.values())\n\n    font = ImageFont.load_default()\n    for i in range(min(boxes.shape[0], max_boxes)):\n        if scores[i] >= min_score and len(boxes[i]) > 1:\n            #print(boxes[i])\n            ymin, xmin, ymax, xmax = tuple(boxes[i])\n            display_str = \"{}: {}%\".format(class_names[i],\n                                         int(100 * scores[i]))\n            #color = colors[hash(class_names[i]) % len(colors)]\n            image_pil = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n            draw_bounding_box_on_image(\n              image_pil,\n              ymin,\n              xmin,\n              ymax,\n              xmax,\n              color,\n              font,\n              display_str_list=[display_str])\n            np.copyto(image, np.array(image_pil))\n    return image","24f2290e":"# Modified from https:\/\/www.kaggle.com\/remekkinas\/yolox-inference-on-kaggle-for-cots-lb-0-507\n\n%cd \/kaggle\/working\n\nfrom sklearn.model_selection import GroupKFold\n\ndef get_bbox(annots):\n    bboxes = [list(annot.values()) for annot in annots]\n    return bboxes\n\ndef get_path(row):\n    row['image_path'] = f'{ROOT_DIR}\/train_images\/video_{row.video_id}\/{row.video_frame}.jpg'\n    return row\n\nROOT_DIR  = '\/kaggle\/input\/tensorflow-great-barrier-reef\/'\n\ndf = pd.read_csv(\"\/kaggle\/input\/tensorflow-great-barrier-reef\/train.csv\")\n\n\ndf[\"num_bbox\"] = df['annotations'].apply(lambda x: str.count(x, 'x'))\ndf_train = df\n\n#Annotations \ndf_train['annotations'] = df_train['annotations'].progress_apply(lambda x: ast.literal_eval(x))\ndf_train['bboxes'] = df_train.annotations.progress_apply(get_bbox)\n\ndf_train = df_train.progress_apply(get_path, axis=1)\n\nkf = GroupKFold(n_splits = 5) \ndf_train = df_train.reset_index(drop=True)\ndf_train['fold'] = -1\nfor fold, (train_idx, val_idx) in enumerate(kf.split(df_train, y = df_train.video_id.tolist(), groups=df_train.sequence)):\n    df_train.loc[val_idx, 'fold'] = fold\n\ndf_train.head(5)","7b987f2b":"df_test = df_train[df_train.fold == 4]","6a18e0b0":"image_paths = df_test.image_path.tolist()\n\ngt = []\nfor i, row in df_test.iterrows():\n    if len(row['bboxes']) > 1:\n        x0 = row['bboxes'][0][0] \/ 1280\n        y0 = row['bboxes'][0][1] \/ 720\n        x1 = x0 + row['bboxes'][0][2] \/ 1280\n        y1 = y0 + row['bboxes'][0][3] \/ 720\n        gt.append([y0, x0, y1, x1])\n    else:\n        gt.append([])","d7cd43d2":"def load_img(path):\n    img = tf.io.read_file(path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    return img\n\ndef detect(image_np):\n    input_tensor = np.expand_dims(image_np, 0)\n    start_time = time.time()\n    detections = detect_fn_tf_odt(input_tensor)\n    return detections\n\ndef run_detector(detector, path, color):\n    img = load_img(path)#load_image_into_numpy_array(path)\n\n    ##converted_img  = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n    start_time = time.time()\n    result = detect(img)\n    ##result = detector(converted_img)\n    end_time = time.time()\n\n    result = {key:value.numpy()[0] for key,value in result.items()}\n\n    #print(\"Found %d objects.\" % len(result[\"detection_scores\"]))\n    #print(\"Inference time: \", end_time-start_time)\n\n    detection_classes = ['cots']*len(result[\"detection_boxes\"])\n    image_with_boxes = draw_boxes(\n        img.numpy(), result[\"detection_boxes\"],\n        detection_classes, result[\"detection_scores\"],\n        color)\n\n    return image_with_boxes\n\n\ndef add_correct_box(img, ind, color):\n    detection_classes_t = ['cots']\n    detection_scores_t = [1]\n    image_with_boxes = draw_boxes(\n        img, np.array([gt[ind]]),\n        detection_classes_t, detection_scores_t,\n        color)\n    return image_with_boxes","f46ba22e":"%cd \/kaggle\/working\n\nvideo_size = (1280, 720)\nCOCO_CLASSES = (\"starfish\")\n\nout1 = cv2.VideoWriter('Video.avi',cv2.VideoWriter_fourcc(*'DIVX'), 15, video_size)\n\nfor i in tqdm(range(1250, 2200)):\n    # Test a small video first. For the full video, substitute \"tqdm(range(start, finish))\" with \"tqdm(range(len(image_paths)))\"\n    TEST_IMAGE_PATH = image_paths[i]\n    img = cv2.imread(TEST_IMAGE_PATH)\n    out_image = run_detector(detect_fn_tf_odt, TEST_IMAGE_PATH, color='#ff0000')\n    out_image = add_correct_box(out_image, i, color='#00ff00')\n    out_image = cv2.cvtColor(out_image, cv2.COLOR_BGR2RGB)\n    out1.write(out_image)\n    \n# Finalize AVI\nout1.release()","18975f1b":"AVI2MP4 = \"-ac 2 -b:v 2000k -c:a aac -c:v libx264 -b:a 160k -vprofile high -bf 0 -strict experimental -f mp4\"\n\ncommand = f\"{FFMPEG_BIN} -i Video.avi {AVI2MP4} Video.mp4\"\nsubprocess.call(command, shell=True)","d7c16b18":"from IPython.display import HTML\nfrom base64 import b64encode\n\ndef play(filename):\n    html = ''\n    video = open(filename,'rb').read()\n    src = 'data:video\/mp4;base64,' + b64encode(video).decode()\n    html += '<video width=800 controls autoplay loop><source src=\"%s\" type=\"video\/mp4\"><\/video>' % src \n    return HTML(html)\n\nplay('Video.mp4')","215af0de":"# Cleanup\n\n!rm *.avi\n!rm -r ffmpeg*","e180c82d":"### Utility Functions","4593ef4f":"# Define the Model Here","833057ff":"# Show off your video!\n\n* Green boxes are ground truth\n* Red boxes are model inference","c3fb78d1":"## Importing the Training Dataset and selecting videos\n\n* Note that it is important to also include videos with NO COTS, so you can understand where false positive detections may arise","c7e1d8c5":"### Define image paths and ground truth bounding boxes","d0505c28":"# TensorFlow Object Detection - Make compact videos\n\n**<span style=\"color:red\">If you liked this notebook, please don't forget to upvote it!<\/span>**\n\n* This notebook is based on https:\/\/www.kaggle.com\/alexchwong\/yolov5-is-all-you-need-make-compact-videos from alexchwong (version 3). Please upvote it as well!\n\nI used these notebooks to create this one:\n* https:\/\/www.kaggle.com\/alexchwong\/yolov5-is-all-you-need-make-compact-videos\n* https:\/\/www.kaggle.com\/khanhlvg\/cots-detection-w-tensorflow-object-detection-api\n* https:\/\/www.kaggle.com\/bamps53\/create-annotated-video\n\nAlso I used code from:\n* https:\/\/www.tensorflow.org\/hub\/tutorials\/object_detection\n\nPlease upvote them as well!\n","8a3fc71f":"# Creating Videos","2453d6aa":"## Inference on Validation Videos and Recording the Video","b1143841":"### Select the validation dataset\n\n* Take care not to shuffle the videos! Keep them in the original order by not sorting the data frame","5e08f1fb":"## Convert AVI to compressed mp4 for more convenient downloading\n\n* The created AVI is a large file. Compress the video file so you can download it and watch it locally","08e26e46":"## Install ffmpeg for Kaggle"}}