{"cell_type":{"25c8e85d":"code","873cd577":"code","e457ac11":"code","527bc9be":"code","450be875":"code","afd71d62":"code","74a58eba":"code","6a67c281":"code","f6696801":"code","63e15662":"code","7bc7c5f8":"code","98e1a520":"code","c229fa18":"code","757189f9":"code","aee53121":"code","7f152c13":"code","a83489c7":"code","fdceb738":"code","bdff7a10":"code","9701d77b":"code","85386f9f":"code","b8d1b9a8":"code","943e9f8b":"code","219bb531":"code","097a8199":"code","1fa1df34":"code","94df9236":"code","1766ee15":"code","5c4bb790":"code","107b345b":"code","696d4bc2":"code","51337178":"code","9222406c":"code","6cd0d624":"markdown","fc3c1ae3":"markdown","e737761f":"markdown","032bb0b0":"markdown","d573755e":"markdown","3d43cc7c":"markdown","5fc0a9d3":"markdown","43017472":"markdown","aaf9f078":"markdown","dbc3542d":"markdown","fad2e03b":"markdown","25214c0b":"markdown","a13dcb31":"markdown","aee9d238":"markdown","a5ef02b6":"markdown","a76f7f9f":"markdown","26d84798":"markdown","3360f94d":"markdown","de2fd8db":"markdown","5197f31c":"markdown","129b6347":"markdown","3955a07d":"markdown","c751b130":"markdown","508ff220":"markdown","6b202274":"markdown","7976e1cd":"markdown","b2b44fee":"markdown","a6f505da":"markdown","777250b3":"markdown","48bd63d3":"markdown","589129cc":"markdown","01699275":"markdown"},"source":{"25c8e85d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","873cd577":"df = pd.read_csv('\/kaggle\/input\/advertising-dataset\/advertising.csv')","e457ac11":"# first five rows \ndf.head()","527bc9be":"#drop radio and newspaper column from df\n\ndf.drop(columns=['Radio','Newspaper'],inplace=True,axis=1)","450be875":"# View the dimensions of df\n\ndf.shape","afd71d62":"# View the top 5 rows of df\n\ndf.head()","74a58eba":"# View dataframe summary\n\ndf.info()","6a67c281":"df.describe()","f6696801":"# Declare feature variable and target variable\n# TV and Sales data values are given by X and y respectively.\n\nX = df.TV\ny = df.Sales","63e15662":"import matplotlib.pyplot as plt\nimport seaborn as sns","7bc7c5f8":"# Visualise the relationship between the features and the response using scatterplots\n\nplt.figure(figsize=(10,5))\n\nplt.scatter(X,y)\n\nplt.title('Relationship between TV and Sales')\n\nplt.xlabel('TV')\nplt.ylabel('Sales')\nplt.show()","98e1a520":"# plot a pairplot also for df\n\nsns.pairplot(df)","c229fa18":"# Print the dimensions of X and y\nprint(X.shape)\nprint(y.shape)","757189f9":"X","aee53121":"y","7f152c13":"# Reshape X and y\nX = np.array(X)\nX = np.reshape(X,(-1,1))\n\ny = np.array(y)\ny = np.reshape(y,(-1,1))","a83489c7":"# Print the dimensions of X and y after reshaping\nprint(X.shape)\nprint(y.shape)","fdceb738":"# run it if you want\n\n#X","bdff7a10":"# run it if you want\n\n#y","9701d77b":"# import train_test_split module\nfrom sklearn.model_selection import train_test_split\n\n# Split X and y into training and test data sets with test_size=0.3 and random_state=42\n\nX_train,y_train,X_test,y_test = train_test_split(X,y,test_size=0.3,random_state=42)","85386f9f":"# print shapes of X_train,y_train, X_test, y_test\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","b8d1b9a8":"# import LinearRegression module\nfrom sklearn.linear_model import LinearRegression as lir\n\n\n# Instantiate the linear regression object lm\nlm = lir()\n# Fit and train the model using training data sets\nlm.fit(X_train,X_test)\n# Predict on the test data\ny_pred_test = lm.predict(y_train)","943e9f8b":"# Compute model slope and intercept\nprint('Estimated model slope, a: {}'.format(lm.coef_))\nprint('Estimated model intercept, b: {}'.format(lm.intercept_))","219bb531":"# So comment below, our fitted regression line here is ?\n\ny = 0.05483488 * X + 7.20655455","097a8199":"df.TV.head()","1fa1df34":"# Predicting Sales values on first five 5 TV  datasets only\npred_x = lm.predict(X)\npred_x[:5]","94df9236":"# prediction for all X present in the dataset\n\n# if you want to see it remove # from pred_x\n#pred_x","1766ee15":"# import mean_squared_error module\nfrom sklearn.metrics import mean_squared_error as mse\n\n\n# Calculate and print Root Mean Square Error(RMSE)\nmse = mse(y_test,y_pred_test)\nrmse = np.sqrt(mse)\n\nprint(f'RMSE value:: {rmse}')","5c4bb790":"# import r2_score module\nfrom sklearn.metrics import r2_score\n\n\n# Calculate and print r2_score\nr2 = r2_score(y_test,y_pred_test)\nprint(f'R2 Score value:: {r2 }')","107b345b":"# Plot the Regression Line between X and Y as shown in below output.\n\nplt.figure(figsize=(20,8))\nplt.scatter(y_train,y_test,color='blue',label='Scatter Plot')\nplt.plot(y_train,y_pred_test,color='red',linewidth=4,label='Regression Line')\n\nplt.legend(loc=4)\nplt.title('Sales and Advertising Relation')\nplt.xlabel('Sales')\nplt.ylabel('Advertising')\n\nplt.show()","696d4bc2":"X_test.shape,X_train.shape,y_train.shape,y_test.shape","51337178":"# Plotting residual errors\n\nplt.figure(figsize=(20,8))\nplt.scatter(lm.predict(X_train),lm.predict(X_train) - X_test,color='blue',label='train_data')\nplt.scatter(lm.predict(y_train),lm.predict(y_train) - y_test,color='red',label='test_data')\n\nplt.hlines(xmin=0,xmax=50,y=0,linewidth=4)\nplt.legend(loc=4)\nplt.title('Residual Error')\n\nplt.show()","9222406c":"# Checking for Overfitting or Underfitting the data by calculation score using score function.\nprint(f'Training set score:: {lm.score(X_train,X_test)}')\nprint(f'Test set score:: {lm.score(y_train,y_test)}')\n","6cd0d624":"* As TV's value increases sales increases vise-versa\n    * i.e shows positive linear relation","fc3c1ae3":"# `Model Performance`\n### `Regression Metrics`\nTo Evaluate Model Performance\n\nFor Regression Problems there are two ways to Compute Model Performance\n\n1. **`RMSE (Root Mean Square Error)`**\n    * RMSE is standard deviation of residuals, it gives us standard deviation of unexplained variance by model `calculated by taking square root of Mean Squared Error`\n    * RMSE is an absolute measure of fit, gives us how spread residuals are, given by standard deviation of the residuals\n    * `More concentrated data is around regression line, lower the residuals and hence lower standard deviation of residuals`\n    * It results in lower values of RMSE as `lower values of RMSE indicate better fit of data` ","e737761f":"# Understanding the Data Let's start with the following steps:\n\n* Importing data using the pandas library\n* Understanding the structure of the data","032bb0b0":"----\n----\n# [To Multiple Linear Regression](https:\/\/www.kaggle.com\/mukeshmanral\/multiple-linear-regression-basic) one step ahead\n----\n-----","d573755e":"Can also do prediction for all values of TV available in dataset","3d43cc7c":"# `Linear Regression`\nUsed to find Linear Relationship between dependent and one or more independent variables<br> \n`Linear Regression` is a `Supervised Learning Algo` => `solves Regression problems` where we try to predict a `Continuous Variable`\n\nLinear Regression can be further classified into two types:\n1. `Simple Linear Regression`\n2. `Multiple Linear Regression` ","5fc0a9d3":"# Reshaping X and y\n* Since we are working with only one feature variable, so we need to reshape using Numpy reshape() method\n\n`Example`: If you have an array of shape (3,2) then reshaping it with (-1, 1), then the array will get reshaped in such a way that the resulting array has only 1 column and this is only possible by having 6 rows, hence, (6,1)","43017472":"# Model slope and intercept term\n* `Model slope is given by lm.coef_` \n* `Model intercept is given by lm.intercept_.`\n\n`For example`: <br>\nif estimated model slope and intercept values are 1.60509347 and -11.16003616\n\nSo, equation of fitted regression line will be:-\n\n`y = 1.60509347 * x - 11.16003616`","aaf9f078":"# Simple Linear Regression - Model Assumptions\n\n\nThe Linear Regression Model is based on several assumptions which are listed below:-\n\n1. `Linear relationship` \n2. `Multivariate normality` \n3. `No or little multicollinearity` \n4. `No auto-correlation` \n5. `Homoscedasticity`\n\n------\n------\n1. `Linear relationship`\nThe relationship between response and feature variables should be linear. This linear relationship assumption can be tested by plotting a scatter-plot between response and feature variables.\n\n2. `Multivariate normality`\nThe linear regression model requires all variables to be multivariate normal. A multivariate normal distribution means a vector in multiple normally distributed variables, where any linear combination of the variables is also normally distributed.\n\n3. `No or little multicollinearity`\nIt is assumed that there is little or no multicollinearity in the data. Multicollinearity occurs when the features (or independent variables) are highly correlated.\n\n4. `No auto-correlation`\nAlso, it is assumed that there is little or no auto-correlation in the data. Autocorrelation occurs when the residual errors are not independent from each other.\n5. `Homoscedasticity`\nHomoscedasticity describes a situation in which the error term (that is, the noise in the model) is the same across all values of the independent variables. It means the residuals are same across the regression line. It can be checked by looking at scatter plot.","dbc3542d":"**`R2` = 1 - (sum squared regression error`SS regression` \/ `SS total`sum squared total error)**\n\n**sum squared total error`SS total` = sum over all data points(each data point`yi`-`ybar`mean value)`^2`**\n\n**sum squared regression error`SS regression` = sum over all data points (each datapoint`yi` - `y regression`regression value)`^2`**","fad2e03b":"# 1. Simple Linear Regression (`SLR`)\n\nIt is the simplest model in machine learning. It models the linear relationship between the independent and dependent variables. \n\n`This application of Linear Regression is based on the TV and Sales data` <br>\n* In data we have one `Independent or Input Variable` which represents `TV`, `denoted by X` \n* In data we have one `Dependent or Output Variable` which represents `Sales`,`denoted by y`  <br>\n    I am trying to build a Linear Relationship between these variables. This linear relationship can be modelled by mathematical equation of the form:-           \n                 \n                     Y = \u03b20  + \u03b21X    -------------   (1)\n                 \n\nIn this equation: \n* `X` is `Independent Variables`\n* `Y` is `Dependent Variables`\n* `\u03b21` is `Coefficient for independent variable`\n* `\u03b20` is `Constant Term`\n* `\u03b20` and `\u03b21` are called `Parameters of Model`\n \nFor simplicity, compare above equation with basic linear equation of form:-\n \n                   y = mx + c       -------------   (2)\n\n* `m` =  `\u03b21` => `Slope` of line\n* `c` =  `\u03b20` => `Intercept` of line\n\n\nIn this Simple Linear Regression Model, I am trying to fit a line which estimates Linear Relationship between X and Y<br> \nSo, the question of fitting reduces to estimating the parameters of the model \u03b20 and \u03b21\n\n\n\n# Ordinary Least Square Method\n\nOne can draw a scatter plot between X and y which shows relationship between them <br>\n\n`My task is to find a line which best fits this scatter plot \nThis Line will help us to predict value of any Target variable for any given Feature variable This line is called **Regression line**` \n\n\n`We can define an error function for any line. Then, the Regression Line is one which minimizes error function, such an error function is also called a Cost function`\n\nBy below chart you might understand more clearly","25214c0b":"# Train test split\n* Split dataset into two sets namely - train set and test set\n\n* Model learn relationships from training data and predict on test data","a13dcb31":"![image.png](attachment:a776a7d1-3351-429c-87f4-4248c8d4f311.png)","aee9d238":"[More into Over\/Under fitting](https:\/\/www.pinterest.com\/pin\/791155859531008328\/)","a5ef02b6":"# Checking for Overfitting and Underfitting\nWill see training set score and test set score\n\n* Can expect training set score to be 0.7996 which is averagely good. So, model learned relationships quite appropriately from training data \n* Model performs good on test data as test score will be 0.8149\n    * It is a clear sign of good fit\/ balanced fit \n    * Hence, we can validated our finding that linear regression model provides good fit to data\n\n**`Underfitting:`** \n* Model underfitts training data when model performs poorly on training data\n* This is because model is unable to capture relationship between input examples (often called X) and target values (often called Y)\n\n**`Overfitting:`** \n* Model overfitts training data when you see that model performs well on training data but does not perform well on evaluation data\n* This is because model is memorizing data it has seen and is unable to generalize to unseen examples\n","a76f7f9f":"![image.png](attachment:352f094b-9ed1-4e9a-81a2-1f03b5137a92.png)","26d84798":"* RMSE value has been found to be 2.2758 \n    * Means standard Deviation for our prediction is 2.2758 which is quite less\n* Sometimes we can also expect RMSE to be less than 2.2758 ,can say model is good fit to data\n\n##### `In business decisions, the benchmark for R2 score value is 0.7`\n* It means if R2 score value >= 0.7, then model is good enough to deploy on unseen data\n* if R2 score value < 0.7 then model is not good enough to deploy\n    \n       \n* Our R2 score value has been found to be 0.814855389208679 \n    * `It means that this model explains 81.49 % of the variance in our dependent variable`\n    * So R2 score value confirms that the model is good enough to deploy because it provides good fit to data\n    \n    \n* See in figure Regression line fits data quite well","3360f94d":"# Residual analysis\n* Linear regression model may not represent data appropriately, model may be a poor fit to data\n    * So we should validate our model by defining and examining residual plots\n\n    * `Difference between observed value of dependent variable (y) and predicted value (y\u0302i) is called residual and is denoted by e or error`\n    * Scatter-plot of these residuals is called residual plot\n\nIf data points in a residual plot are randomly dispersed around horizontal axis and an approximate zero residual mean, a linear regression model may be appropriate for data. Otherwise a non-linear model may be more appropriate\n\nIf we take a look at both generated \u2018Residual errors\u2019 plot(below), we can clearly see that train data plot pattern is non-random. Same is the case with the test data plot pattern. So, it suggests a better-fit for a non-linear model","de2fd8db":"I am trying to show how one can apply Linear Regression <br>\nI will consider only two columns as this notebook is a walkthroug for everyone who want to Understand `Linear Regression` in some applied way.","5197f31c":"Model is not overfitting or underfitting. But before we jump into any conclusion on this, it is always better to do `Cross Validation`\n* I will try to show Cross Validation in any next notebook [Will add the like soon...]() or maybe in this notebook","129b6347":"# Performing Simple Linear Regression\nLinear Regression Equation\n`y = c + m_1x_1 + m_2x_2 + ... + m_nx_n`\n\n-  $y$ is the response\n-  $c$ is the intercept\n-  $m_1$ is the coefficient for the first feature\n-  $m_n$ is the coefficient for the nth feature<br>\n\nIn our case:\n`$y = c + m_1 \\times TV$`\n\n$m$ = model **coefficients** or **model parameters**","3955a07d":"\n2. **`R-Squared Value`**\n    * `(R2) Correlation` explains strength of relationship between an independent and dependent variable \n    * `R-square` explains to what extent variance of one variable explains variance of second variable. It may also be known as the coefficient of determination\n    * `If R2 of a model is 0.50, then approximately half of observed variation can be explained by model's inputs`\n    * `In general higher R2 Score value better model fits data` \n    * `Its value ranges from 0 to 1`\n    * So we want its value to be as close to 1\n    * `Its value can become negative if our model is wrong`\n    \n[Important Link](http:\/\/www.fairlynerdy.com\/what-is-r-squared\/)","c751b130":"# Difference in dimensions of X and y after reshaping\nYou can see the difference in diminsions of X and y before and after reshaping.\n\nIt is essential in this case because getting the feature and target variable right is an important precursor to model building.","508ff220":"# Mechanics of the model\n* It is good to understand the generic structure of modeling using scikit-learn library\n* Broad steps to build any model can be divided as follows:\n    * Split dataset into two sets \u2013 training set and test set \n    * Instantiate regressor lm and fit it on training set with fit method\n    * In above step model learned relationships between training data (X_train, y_train)\n* Now model is ready to make predictions on test data (X_test)\n* So predict on test data using predict method","6b202274":"* Same shows positive linear relation between both TV and Sales","7976e1cd":"# Making predictions\nTo make prediction on an individual TV value, \n\n                lm.predict(Xi)\n\nwhere: \n\n`Xi` = `TV data value of ith observation`","b2b44fee":"#  Checking dimensions of X and y","a6f505da":"Above is the Linear Model","777250b3":"# Variables\n* Independent\n* Dependent \n\n**`1. Independent variable`** <br>\nIndependent variable is also called Input or Feature or Predictor Variable and is denoted by X\n\nIt can be denoted it as:-\n**`Independent or Input variable (X) = Feature variable = Predictor variable`**\n\n**`2. Dependent variable`** <br>\nDependent variable is also called Output or Target or Response Variable and is denoted by y.\n\nIt can be denoted it as:-\n**`Dependent or Output variable (y) = Target variable = Response variable`**","48bd63d3":"# Interpretation and Conclusion","589129cc":"# Note:\n**`I generally try to update my notebooks by adding more information and external links, so stay together its a long journey`**\n","01699275":"# EDA\nVisualizing Relationship between X and y by plotting a scatterplot between X and y"}}