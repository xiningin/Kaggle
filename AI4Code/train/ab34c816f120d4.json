{"cell_type":{"42fb791f":"code","4478a52c":"code","0d182d8b":"code","6e8c7a47":"code","60f78bf5":"code","b9424f9a":"code","7d798d35":"code","5a47376f":"code","06ff6e97":"code","8a638f49":"markdown","4dda7679":"markdown","ca088898":"markdown","6606355b":"markdown","986caedc":"markdown","d9c562f9":"markdown"},"source":{"42fb791f":"import random\nsuccess = 2\nvals, pattern = [], ['p', 'p', 'o', 'o','p','o','p']\n\nfor x in range(0,101):\n    pulls, avg = 0, 0\n    for step in pattern:\n        if step == 'o':\n            pulls += 1\n        if step == 'p':\n            chance = x*(0.97**pulls)\/100\n            avg += sum(random.choices([1, 0], k=100, weights=[chance, 1-chance]))\/100\n            pulls += 1\n    vals.append(avg)\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(8, 8))\nax.plot(vals, label='Avg Candies in 100 trials')\nax.plot([2]*101, linewidth=10, alpha=0.5, label='Actual Candies acquired in the given conditions')\n\nbest_score, X = 100, 0\nfor x in range(len(vals)):\n    if abs(vals[x] - success) < best_score:\n        X = x\n        best_score = abs(vals[x] - success)\n\nax.scatter([X], [success], color='black', label=f'X = {X}, Avg = {vals[X]}')\nax.set_xlabel('Initial Threshold -->')\nax.set_ylabel('Avg. Candies after 100 trial runs -->')\nax.set_title('PulledSelf=4, Opponent=3, Candies=2')\nax.legend()","4478a52c":"%%writefile submission.py\n\nimport pandas, numpy, random\n\nbandits = [{'pulled': 0, 'success': 0, 'opp': 0} for i in range(100)]\nlast_reward = 0\nprobabs = [1 for i in range(100)]\npatterns = [[] for i in range(100)]\nopp_actions = []\nmy_actions = []\narr = []\n\ndef update(obs):\n    \"\"\"Just updating stuff, nothing interesting here :\/ \"\"\"\n    global bandits, last_reward, patterns, opp_actions, my_actions, arr\n    \n    bandits[obs.lastActions[obs.agentIndex]]['pulled'] += 1\n    my_actions.append(obs.lastActions[obs.agentIndex])\n    \n    bandits[obs.lastActions[1 - obs.agentIndex]]['opp'] += 1\n    opp_actions.append(obs.lastActions[1 - obs.agentIndex])\n    \n    bandits[obs.lastActions[obs.agentIndex]]['success'] += (obs.reward > last_reward)*1\n    \n    patterns[obs.lastActions[obs.agentIndex]].append('s')\n    patterns[1 - obs.lastActions[obs.agentIndex]].append('o')\n    \n    arr.append(probabs[obs.lastActions[obs.agentIndex]])\n    \n\n    \ndef compute(obs):\n    \"\"\"Updating the threshold of the bandit pulled in the previous step\"\"\"\n    global probabs, bandits, last_reward, patterns\n    \n    best_probab = 100\n    best_score = 100\n    ind = obs.lastActions[obs.agentIndex]\n    success = bandits[ind]['success']\n            \n    for i in range(0, 102, 2):\n        pulls, avg = 0, 0\n        for step in patterns[ind]:\n            if step == 'o':\n                pulls += 1\n            if step == 's':\n                chance = i*(0.97**pulls)\/100\n                avg += sum(random.choices([1, 0], k=100, weights=[chance, 1-chance]))\/100\n                pulls += 1\n        \n        if abs(avg - success) < best_score:\n            best_score = abs(avg - success)\n            best_probab = i\n    \n#     print(\"Updating \", ind, \"From\", probabs[ind], \"to\", best_probab*(0.99**(bandits[ind]['pulled'] + bandits[ind]['opp']))\/100)\n    probabs[ind] = (best_probab)*(0.97**(bandits[ind]['pulled'] + bandits[ind]['opp']))\/100 \n                \n\n\n        \ndef agent(obs, conf):\n    \"\"\"Main agent\"\"\"\n    global last_reward, bandits, probabs, patterns, opp_actions, my_actions, arr\n\n    if obs.step == 0:\n        return random.randint(0, 99)\n    \n    update(obs) #Update all the information\n\n    compute(obs) #Update threshold of the bandit pulled previously\n\n    if obs.reward > last_reward: #Re-pull if rewarded\n        last_reward = obs.reward\n        return obs.lastActions[obs.agentIndex]\n\n    last_reward = obs.reward\n    \n    #Max threshold and the bandit\n    maxpred, bnd = max(probabs), int(numpy.argmax(probabs))\n    \n    if obs.step > 3: #Repeat if already repeated last few steps, decrease significance as game progresses\n        if opp_actions[-1] == opp_actions[-2] and opp_actions[-1] == opp_actions[-3] and random.random() < 0.6 - 0.6*(obs.step\/2000):\n            return opp_actions[-1]\n\n        if my_actions[-1] == my_actions[-2] and my_actions[-1] == my_actions[-3] and random.random() < 0.6 - 0.6*(obs.step\/2000):\n            return my_actions[-1]\n        \n    if obs.step >= 1995:\n        numpy.save('arr.npy', arr)\n    #Pull the bandit with the max expected threshold    \n    return bnd\n    ","0d182d8b":"%%writefile thompson.py\n\nimport numpy as np\nimport pandas as pd\n\npost_a = None\npost_b = None\nbandit = None\ntotal_reward = 0\nc = 3\ndf_steps = []\ndf_probs = []\n\ndef agent(observation, configuration):\n    global reward_sums, total_reward, bandit, post_a, post_b, c\n    global df_steps, df_probs\n    \n    n_bandits = configuration.banditCount\n\n    if observation.step == 0:\n        post_a = np.ones(n_bandits)\n        post_b = np.ones(n_bandits)\n    else:\n        r = observation.reward - total_reward\n        total_reward = observation.reward\n\n        # Update Gaussian posterior\n        post_a[bandit] += r\n        post_b[bandit] += (1 - r)\n\n    samples = np.random.beta(post_a, post_b)\n    bandit = int(np.argmax(samples))\n    \n    df_steps.append(observation.step)\n    df_probs.append(samples[bandit] * 100)\n    # if observation.step > 1995:\n    #     df = pd.DataFrame(df_probs, index=df_steps, columns=['probs'])\n    #     df.to_csv(\"thompson.csv\")\n        \n    return bandit\n","6e8c7a47":"%%writefile vegas_pull.py\n\nimport numpy as np\nimport pandas as pd\nimport random, os, datetime, math\nfrom collections import defaultdict\n\ntotal_reward = 0\nbandit_dict = {}\n\ndef set_seed(my_seed=42):\n    os.environ['PYTHONHASHSEED'] = str(my_seed)\n    random.seed(my_seed)\n    np.random.seed(my_seed)\n\ndef get_next_bandit():\n    best_bandit = 0\n    best_bandit_expected = 0\n    for bnd in bandit_dict:\n        expect = (bandit_dict[bnd]['win'] - bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'] - (bandit_dict[bnd]['opp']>0)*1.5 + bandit_dict[bnd]['op_continue']) \\\n                 \/ (bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp']) \\\n                * math.pow(0.97, bandit_dict[bnd]['win'] + bandit_dict[bnd]['loss'] + bandit_dict[bnd]['opp'])\n        if expect > best_bandit_expected:\n            best_bandit_expected = expect\n            best_bandit = bnd\n    return best_bandit\n\nmy_action_list = []\nop_action_list = []\n\nop_continue_cnt_dict = defaultdict(int)\n\ndef multi_armed_probabilities(observation, configuration):\n    global total_reward, bandit_dict\n\n    my_pull = random.randrange(configuration['banditCount'])\n    if 0 == observation['step']:\n        set_seed()\n        total_reward = 0\n        bandit_dict = {}\n        for i in range(configuration['banditCount']):\n            bandit_dict[i] = {'win': 1, 'loss': 0, 'opp': 0, 'my_continue': 0, 'op_continue': 0}\n    else:\n        last_reward = observation['reward'] - total_reward\n        total_reward = observation['reward']\n        \n        my_idx = observation['agentIndex']\n        my_last_action = observation['lastActions'][my_idx]\n        op_last_action = observation['lastActions'][1-my_idx]\n        \n        my_action_list.append(my_last_action)\n        op_action_list.append(op_last_action)\n        \n        if 0 < last_reward:\n            bandit_dict[my_last_action]['win'] = bandit_dict[my_last_action]['win'] +1\n        else:\n            bandit_dict[my_last_action]['loss'] = bandit_dict[my_last_action]['loss'] +1\n        bandit_dict[op_last_action]['opp'] = bandit_dict[op_last_action]['opp'] +1\n        \n        if observation['step'] >= 3:\n            if my_action_list[-1] == my_action_list[-2]:\n                bandit_dict[my_last_action]['my_continue'] += 1\n            else:\n                bandit_dict[my_last_action]['my_continue'] = 0\n            if op_action_list[-1] == op_action_list[-2]:\n                bandit_dict[op_last_action]['op_continue'] += 1\n            else:\n                bandit_dict[op_last_action]['op_continue'] = 0\n        \n        if last_reward > 0:\n            my_pull = my_last_action\n        else:\n            if observation['step'] >= 4:\n                if (my_action_list[-1] == my_action_list[-2]) and (my_action_list[-1] == my_action_list[-3]):\n                    if random.random() < 0.5:\n                        my_pull = my_action_list[-1]\n                    else:\n                        my_pull = get_next_bandit()\n                else:\n                    my_pull = get_next_bandit()\n            else:\n                my_pull = get_next_bandit()\n    \n    return my_pull","60f78bf5":"%%writefile bayesian_ucb.py\n\nimport numpy as np\nfrom scipy.stats import beta\nimport pandas as pd\n\npost_a, post_b, bandit = [None] * 3\ntotal_reward = 0\nc = 3\ndf_steps = []\ndf_probs = []\n\ndef agent(observation, configuration):\n    global total_reward, bandit, post_a, post_b, c\n    global df_steps, df_probs\n    if observation.step == 0:\n        post_a, post_b = np.ones((2, configuration.banditCount))\n    else:\n        r = observation.reward - total_reward\n        total_reward = observation.reward\n        # Update Gaussian posterior\n        post_a[bandit] += r\n        post_b[bandit] += 1 - r\n    \n    bound = post_a \/ (post_a + post_b) + beta.std(post_a, post_b) * c\n    bandit = int(np.argmax(bound))\n    \n    df_probs.append(100 * post_a[bandit]\/(post_a[bandit] + post_b[bandit]))\n    df_steps.append(observation.step)\n    \n    # if observation.step > 1995:\n    #     df = pd.DataFrame(df_probs, index=df_steps, columns=['probs'])\n    #     df.to_csv(\"bayesian_ucb.csv\")\n    return bandit","b9424f9a":"!pip install kaggle-environments --upgrade -q\nfrom kaggle_environments import make\nenv = make(\"mab\", debug=True)","7d798d35":"import matplotlib.pyplot as plt\nimport numpy as np\n\ngame = env.run(['submission.py', 'bayesian_ucb.py'])\n\nbests = [max(game[k][0]['observation']['thresholds']) for k in range(2000)]\n\nfig, [ax1, ax2] = plt.subplots(2, 1, figsize=(16, 12))\n\nactual_regr_op = []\nactual_regr_me = []\nfor i in range(2000):\n    actual_regr_op.append(game[i][0]['observation']['thresholds'][game[i][1]['action']])\n    actual_regr_me.append(game[i][0]['observation']['thresholds'][game[i][0]['action']])\n\nax1.set_title('Thresholds of the bandits chosen every step')\nax1.scatter([i for i in range(2000)], actual_regr_op, alpha=0.7,label='Op')\nax1.scatter([i for i in range(2000)], actual_regr_me, alpha=0.7, label='Me')\nax1.plot(bests, color='black', label='Max Threshold')\nax1.legend()\n\nexpect = np.load('arr.npy')\n\nax2.set_title('Expected vs Actual Thresholds of the chosen bandits')\nax2.scatter([i for i in range(len(expect))], [actual_regr_me[i] for i in range(len(expect))], alpha=0.7, label='Actual')\nax2.scatter([i for i in range(len(expect))], [expect[i]*100 for i in range(len(expect))], alpha=0.7, label='Expected')\nax2.legend()","5a47376f":"import matplotlib.pyplot as plt\nimport numpy as np\n\ngame = env.run(['submission.py', 'thompson.py'])\n\nbests = [max(game[k][0]['observation']['thresholds']) for k in range(2000)]\n\nfig, [ax1, ax2] = plt.subplots(2, 1, figsize=(16, 12))\n\nactual_regr_op = []\nactual_regr_me = []\nfor i in range(2000):\n    actual_regr_op.append(game[i][0]['observation']['thresholds'][game[i][1]['action']])\n    actual_regr_me.append(game[i][0]['observation']['thresholds'][game[i][0]['action']])\n\nax1.set_title('Thresholds of the bandits chosen every step')\nax1.scatter([i for i in range(2000)], actual_regr_op, alpha=0.7,label='Op')\nax1.scatter([i for i in range(2000)], actual_regr_me, alpha=0.7, label='Me')\nax1.plot(bests, color='black', label='Max Threshold')\nax1.legend()\n\nexpect = np.load('arr.npy')\n\nax2.set_title('Expected vs Actual Thresholds of the chosen bandits')\nax2.scatter([i for i in range(len(expect))], [actual_regr_me[i] for i in range(len(expect))], alpha=0.7, label='Actual')\nax2.scatter([i for i in range(len(expect))], [expect[i]*100 for i in range(len(expect))], alpha=0.7, label='Expected')\nax2.legend()","06ff6e97":"import matplotlib.pyplot as plt\nimport numpy as np\n\ngame = env.run(['submission.py', 'vegas_pull.py'])\n\nbests = [max(game[k][0]['observation']['thresholds']) for k in range(2000)]\n\nfig, [ax1, ax2] = plt.subplots(2, 1, figsize=(16, 12))\n\nactual_regr_op = []\nactual_regr_me = []\nfor i in range(2000):\n    actual_regr_op.append(game[i][0]['observation']['thresholds'][game[i][1]['action']])\n    actual_regr_me.append(game[i][0]['observation']['thresholds'][game[i][0]['action']])\n\nax1.set_title('Thresholds of the bandits chosen every step')\nax1.scatter([i for i in range(2000)], actual_regr_op, alpha=0.7,label='Op')\nax1.scatter([i for i in range(2000)], actual_regr_me, alpha=0.7, label='Me')\nax1.plot(bests, color='black', label='Max Threshold')\nax1.legend()\n\nexpect = np.load('arr.npy')\n\nax2.set_title('Expected vs Actual Thresholds of the chosen bandits')\nax2.scatter([i for i in range(len(expect))], [actual_regr_me[i] for i in range(len(expect))], alpha=0.7, label='Actual')\nax2.scatter([i for i in range(len(expect))], [expect[i]*100 for i in range(len(expect))], alpha=0.7, label='Expected')\nax2.legend()","8a638f49":"# <h><center>Predicting Thresholds by Emulating Bandits<\/center><\/h>\n\n<p> Hello Everyone! <br> Just publishing an idea that came to my mind as I was enjoying some nice tea early morning, but unfortunately didn't perform as well as I had hoped it would. Regardless, It felt a little unique and I still think it has a lot of potential, that I'm not doing proper justice towards. <\/p>\n\nSo Here's the plan:\n\nWe assume the bandits initial threshold to be X. Each time the bandit is pulled, X decreases by 3%, i.e, the threshold now becomes \\\\(X * 0.97 \\\\) . We can then generalize the threshold at any point to \n<br> <center> $$ X_{initial} * 0.97 ^ {n_{pulls}}$$ <br> <br>\n \nNow this was basic knowledge, but how do we know what X is? We *assume* multiple values for X, then do some trial runs, and find out which fits best. For a quick example, let's take some bandit, which has been pulled 7 times, 4 times by us, thrice by opponent, and has given us 2 candies. Let the order of pulls be such: <br> \n    <b><center> P, P, O, O, P, O, P (P= Us, O = Opp)<\/center><\/b>\n<br> Now we run step by step, everytime we hit an opponents pull, we decrease X by 3%. When we hit our pull, we 'pull' from the bandit, and then decrease the threshold by 3%. We run this hundreds of times for each X, and try to find out the value of X which would be most plausible for our scenario. Here are the results: \n","4dda7679":"We can all agree that the performances were quite bad, but that more has to do with the bad rule-based agent, and not the predictor, or maybe I just like to think so. I tinkered around with it quite a bit, but unfortunately couldn't come up with an agent that has a good win-rate against other famous public agents, so I didn't really use it in the contest. Still, I think this is worth looking into, and I hope if not me, someone else can come up with the proper way to use it.\n\nHave a nice day!","ca088898":"As can be seen above, the most probable value of initial threshold is 54, then, present threshold of the bandit, will be: <br> <center>\n$$ X_{final} = X_{initial}*0.97 ^ {n_{pulls}}$$<br> \n            $$ = 54 * 0.97 ^ {7}$$ <br>\n            $$ = 43.6 $$<\/center> <br>\n Now let's play a few matches against some other agents :)\n \n Code: ","6606355b":"# <center>Against Bayesian UCB<\/center>","986caedc":"# <center>Against Thompson Sampling<\/center>","d9c562f9":"# <center>Against Vegas-Pull v2<\/center>"}}