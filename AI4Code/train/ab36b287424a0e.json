{"cell_type":{"edbd59cd":"code","f4d9a8c8":"code","1db0b07e":"code","bc94a555":"code","22f105e9":"code","daccde6b":"code","11ef5773":"code","e42f3ea0":"code","b3081eed":"code","0286a46a":"code","e62bdb7b":"code","bc9b69cf":"code","2152ee9d":"code","987fe87f":"code","a6fabfc8":"code","1bee075b":"code","b40d938b":"code","3eefad99":"code","9cc1e59b":"markdown","73c87faa":"markdown","cb641eed":"markdown","164eb7a5":"markdown","bd65306c":"markdown","0db9b339":"markdown","1859cd5b":"markdown","33e3aabc":"markdown","50ccc987":"markdown","f25d43cd":"markdown"},"source":{"edbd59cd":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom pandas_profiling import ProfileReport","f4d9a8c8":"df = pd.read_csv('..\/input\/datasetknn\/DataSetKNN.csv')\ndf.head()","1db0b07e":"report = ProfileReport(df)\nreport","bc94a555":"df.info()","22f105e9":"sns.pairplot(df,hue='TARGET CLASS',palette='coolwarm')","daccde6b":"scaler = StandardScaler()\n\nscaler.fit(df.drop('TARGET CLASS',axis=1))\n\nscaled_features = scaler.transform(df.drop('TARGET CLASS',axis=1))","11ef5773":"df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])\ndf_feat.head()","e42f3ea0":"X = df_feat\ny = df['TARGET CLASS']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)","b3081eed":"knn = KNeighborsClassifier(n_neighbors=1)\n\nknn.fit(X_train,y_train)","0286a46a":"pred = knn.predict(X_test)","e62bdb7b":"print(confusion_matrix(y_test,pred))","bc9b69cf":"print(classification_report(y_test,pred))","2152ee9d":"error_rate = []\n\nfor i in range(1,40):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","987fe87f":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","a6fabfc8":"knn = KNeighborsClassifier(n_neighbors=31)","1bee075b":"knn.fit(X_train,y_train)\npred = knn.predict(X_test)","b40d938b":"print(confusion_matrix(y_test,pred))","3eefad99":"print(classification_report(y_test,pred))","9cc1e59b":"- Para melhor resultado das previs\u00f5es \u00e9 necessario mudar o valor de n_neighbors. A melhor forma de encontrar o valor \u00e9 por meio de um loop que teste os varios valores.","73c87faa":"# KNN","cb641eed":"# n_neighbors","164eb7a5":"# Testando o novo valor","bd65306c":"# Previs\u00f5es e Avalia\u00e7\u00f5es do modelo","0db9b339":"# K-nearest neighbors\n\nPara este projeto, o K-nearest neighbors sera usado para prever a classe alvo do conjunto de dados.\n\n---\n\n# Dados\n- Todos os dados usados s\u00e3o artificiais.","1859cd5b":"# Padronizar as Vari\u00e1veis","33e3aabc":"- Analisando os dados obitidos pelo loop.","50ccc987":"- Converter os recursos dimensionados em um dataframe.\n- verificar o cabe\u00e7alho do dataframe para ter certeza de que o dimensionamento funcionou.","f25d43cd":"- Ajustar o escalonador aos recursos"}}