{"cell_type":{"627e1955":"code","517d50c4":"code","8f17029d":"code","b32579a2":"code","54e1eb5a":"code","ed3aba50":"code","f0cd6640":"code","6482996d":"code","cdb76c2b":"code","e3c7841d":"code","89720bdd":"code","9a8ae825":"code","3998fbaa":"code","9e389362":"code","38892da1":"code","5268c890":"code","900861b8":"markdown"},"source":{"627e1955":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","517d50c4":"# Hyper parameters\nvocab_size = 1000\nembedding_dim = 16\nmax_length = 120\ntrunc_type = 'post'\npadding_type = 'post'\noov_tok = '<OOV>'\ntraining_portion = .8","8f17029d":"sentences = []\nlabels = []\nstopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\nprint(len(stopwords))","b32579a2":"import csv\nwith open(\"\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\", 'r') as csvfile:\n    csv_reader = csv.reader(csvfile, delimiter=',')\n    next(csv_reader, None)\n    \n    for row in csv_reader:\n        labels.append(row[1])\n        sentence = row[0]\n        \n        for word in stopwords:\n            token = ' ' + word + ' '\n            sentence = sentence.replace(token, ' ').replace('  ', ' ')\n            \n        sentences.append(sentence)\n\n    \nprint(len(labels))\nprint(len(sentences))\nprint(sentences[0])\nprint(labels[0])","54e1eb5a":"train_size = int(len(sentences) * training_portion)\n\ntrain_sentences = sentences[:train_size]\ntrain_labels = labels[:train_size]\n\nvalidation_sentences = sentences[train_size:]\nvalidation_labels = labels[train_size:]\n\nprint(train_size)\nprint(len(train_sentences))\nprint(len(train_labels))\nprint(len(validation_sentences))\nprint(len(validation_labels))","ed3aba50":"import tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(\n    num_words=vocab_size,\n    oov_token=oov_tok\n)\ntokenizer.fit_on_texts(train_sentences)\nword_index = tokenizer.word_index\n\ntrain_sequences = tokenizer.texts_to_sequences(train_sentences)\ntrain_padded = pad_sequences(\n    train_sequences,\n    padding=padding_type,\n    maxlen=max_length\n)\n\nprint(len(train_sequences[0]))\nprint(len(train_padded[0]))\n\nprint(len(train_sequences[1]))\nprint(len(train_padded[1]))\n\nprint(len(train_sequences[10]))\nprint(len(train_padded[10]))","f0cd6640":"validation_sequences = tokenizer.texts_to_sequences(validation_sentences)\nvalidation_padded = pad_sequences(\n    validation_sequences,\n    padding=padding_type,\n    maxlen=max_length\n)\n\nprint(len(validation_sequences))\nprint(validation_padded.shape)","6482996d":"label_tokenizer = Tokenizer()\nlabel_tokenizer.fit_on_texts(labels)\n\ntraining_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\nvalidation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))\n\nprint(training_label_seq[0])\nprint(training_label_seq[1])\nprint(training_label_seq[2])\nprint(training_label_seq.shape)\n\nprint(validation_label_seq[0])\nprint(validation_label_seq[1])\nprint(validation_label_seq[2])\nprint(validation_label_seq.shape)","cdb76c2b":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dense(6, activation='softmax')\n])\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","e3c7841d":"num_epochs = 30\nhistory = model.fit(\n    train_padded,\n    training_label_seq,\n    epochs=num_epochs,\n    validation_data=(validation_padded, validation_label_seq),\n    verbose=2)","89720bdd":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()\n\nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","9a8ae825":"reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_sentence(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])","3998fbaa":"e = model.layers[0]\nweights = e.get_weights()[0]\nprint(weights.shape)","9e389362":"import io\n\nout_v = io.open('vecs.tsv', 'w', encoding='utf-8')\nout_m = io.open('meta.tsv', 'w', encoding='utf-8')\nfor word_num in range(1, vocab_size):\n    word = reverse_word_index[word_num]\n    embeddings = weights[word_num]\n    out_m.write(word + \"\\n\")\n    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\nout_v.close()\nout_m.close()","38892da1":"# viewing on tensorboard projector\nfrom tensorboard.plugins import projector\n\n# Set up a logs directory, so Tensorboard knows where to look for files\nlog_dir='\/kaggle\/working\/logs\/imdb-example\/'\nif not os.path.exists(log_dir):\n    os.makedirs(log_dir)\n\n# Save Labels separately on a line-by-line manner.\nwith open(os.path.join(log_dir, '\/kaggle\/working\/meta.tsv'), \"w\") as f:\n    for subwords in word_index:\n        f.write(\"{}\\n\".format(subwords))\n    for unknown in range(1, vocab_size - len(word_index)):\n        f.write(\"unknown #{}\\n\".format(unknown))\n\nweights = tf.Variable(model.layers[0].get_weights()[0][1:])\ncheckpoint = tf.train.Checkpoint(embedding=weights)\ncheckpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n\n# Set up config.\nconfig = projector.ProjectorConfig()\nembedding = config.embeddings.add()\nembedding.tensor_name = \"embedding\/.ATTRIBUTES\/VARIABLE_VALUE\"\nembedding.metadata_path = '\/kaggle\/working\/meta.tsv'\nprojector.visualize_embeddings(log_dir, config)","5268c890":"# %tensorflow_version only exists in Colab.\ntry:\n    %tensorflow_version 2.x\nexcept Exception:\n    pass\n\n%load_ext tensorboard\n# Now run tensorboard against on log data we just saved\n%tensorboard --logdir \/kaggle\/working\/logs\/imdb-example\/","900861b8":"Visualize the embeddings\nTo visualize the embeddings, upload them to the embedding projector.\n\nOpen the Embedding Projector(http:\/\/projector.tensorflow.org\/) (this can also run in a local TensorBoard instance).\nClick on \"Load data\".\nUpload the two files you created above: vecs.tsv and meta.tsv.\n\nThe embeddings you have trained will now be displayed. You can search for words to find their closest neighbors. For example, try searching for \"beautiful\". You may see neighbors like \"wonderful\".\n\nSample images for this model when viewed in Embedding Projector\n![Screenshot (803).png](attachment:090a305d-65fb-418e-83f3-9c2bc051d116.png)\n![Screenshot (805).png](attachment:2ddc6d87-0e45-4b7c-b019-77cd134b7b30.png)"}}