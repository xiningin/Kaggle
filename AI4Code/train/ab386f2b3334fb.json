{"cell_type":{"bf3db43d":"code","1d2bc5f8":"code","499a4e91":"code","ae78cb52":"code","8b0b323c":"code","004b6e15":"code","92ff1cce":"code","f93ee523":"code","55b83d05":"code","68ecf435":"code","66719ce4":"code","47ac0103":"code","cc5b281b":"code","edc5ea9e":"code","f7de92f7":"code","98169a31":"code","93f81673":"code","5ff964c4":"code","5417fdb8":"code","981895fd":"code","abaee950":"code","1f46b471":"code","0024ee7e":"code","58aca6b8":"code","8d945a82":"code","8d0d583f":"code","978c94c4":"code","7148acc3":"code","ec2cd506":"code","cc268b0f":"code","ce0e71c4":"code","65ddf0a3":"code","080bf5d5":"code","13366193":"code","32d18406":"code","fd612058":"code","b4fb9d68":"code","b11512ec":"code","f541e1af":"code","6e067c13":"code","1272e8a3":"code","c9d76040":"code","14c2e6e1":"code","b260045e":"code","e94fa44e":"code","8a36c605":"code","f1d63cdb":"code","3a8c2456":"code","65e5bb7c":"markdown","3d5af1dd":"markdown","4a550b4e":"markdown","81915d2d":"markdown","c07ddbf3":"markdown","1979ba78":"markdown","23905457":"markdown","e0c94f74":"markdown","4960c474":"markdown","56bbf48c":"markdown","b33cccb1":"markdown","d812785d":"markdown","5e2f43d0":"markdown","e4d715b4":"markdown","94c63c29":"markdown","3dcbf763":"markdown","c13a2cd2":"markdown","cea2cc92":"markdown","e87188e3":"markdown","16dfd4ae":"markdown","f1af8ad6":"markdown","c8a62121":"markdown","71b28752":"markdown","106dcb06":"markdown","a6cb1dbf":"markdown","96767302":"markdown","f7067fb3":"markdown","cb013bbf":"markdown","9b66a7ce":"markdown","b00d2921":"markdown","22d280e5":"markdown","043b3416":"markdown","5306cda0":"markdown","55a1fb95":"markdown","ac61df4e":"markdown","3699edd9":"markdown","84e81d6d":"markdown","52688f6e":"markdown","7cfee7a5":"markdown","11df620e":"markdown","1be16ad3":"markdown","5cfc6cdf":"markdown","80fff34c":"markdown","bbe67461":"markdown","fe957a62":"markdown"},"source":{"bf3db43d":"# generics\nimport pandas as pd\nimport numpy as np\nimport random\n\n# visu\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\n# texts\nimport re\nimport unicodedata\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical, plot_model\n\n# Model\nfrom tensorflow.keras import layers, Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# NLTK\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\n\n# sklearn\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn import metrics","1d2bc5f8":"df_train = pd.read_csv(\"\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv\", encoding=\"latin_1\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv\", encoding=\"latin_1\")","499a4e91":"df_train.sample(5)","ae78cb52":"max_tweet_length = 0\ntweet_length = []\n#\nfor tweet in df_train[\"OriginalTweet\"]:\n    tweet_length.append(len(tweet))\n    if len(tweet) > max_tweet_length:\n        max_tweet_length = len(tweet)\nprint(\"Longest tweet: \" + str(max_tweet_length) + \" characters\")","8b0b323c":"parameters = {'axes.labelsize': 20,\n              'axes.titlesize': 30}\n#\nplt.rcParams.update(parameters)\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.set_size_inches(18.5, 6)\nsns.histplot(tweet_length, palette='Blues', stat='density', bins=50, ax=ax1);\nsns.kdeplot(tweet_length, color='red', ax=ax1)\nax1.set_xlabel('Character count per tweet');\ndf_train[\"Sentiment\"].reset_index().groupby(\"Sentiment\").count().rename(columns={\"index\": \"Count\"}).sort_values(by= \n       \"Count\").plot(kind=\"barh\", legend=False, \n        ax=ax2).grid(axis='x')\nax1.tick_params(axis='x', labelsize=16)\nax1.tick_params(axis='y', labelsize=16)\nax1.set_ylabel(\"\")\nax1.set_title(\"Tweet length distribution\", color =\"#292421\")\nax2.tick_params(axis='x', labelsize=16)\nax2.tick_params(axis='y', labelsize=16)\nax2.set_ylabel(\"\")\nax2.set_title(\"Tweet sentiment count\", color =\"#292421\")\nfig.tight_layout(pad=2.0)\nplt.rcParams.update(parameters)","004b6e15":"def set_3_classes(x):\n  if x==\"Extremely Negative\":\n    return \"Negative\"\n  elif x==\"Extremely Positive\":\n    return \"Positive\"\n  else:\n    return x","92ff1cce":"df_train[\"Sentiment\"] = df_train[\"Sentiment\"].apply(set_3_classes)\ndf_test[\"Sentiment\"] = df_test[\"Sentiment\"].apply(set_3_classes)","f93ee523":"fig, ax = plt.subplots()\nfig.suptitle(\"Count\", fontsize=12)\ndf_train[\"Sentiment\"].reset_index().groupby(\"Sentiment\").count().sort_values(by= \n       \"index\").plot(kind=\"barh\", legend=False, \n        ax=ax).grid(axis='x')\nplt.show()","55b83d05":"df_train[\"CleanTweet\"] = df_train[\"OriginalTweet\"]\ndf_train.sample(3)","68ecf435":"def clean_eol_tabs(df, label):\n    \"\"\" text lowercase\n        removes \\n\n        removes \\t\n        removes \\r \"\"\"\n    df[label] = df[label].str.lower()\n    df[label] = df[label].apply(lambda x: x.replace(\"\\n\", \" \"))\n    df[label] = df[label].apply(lambda x: x.replace(\"\\r\", \" \"))\n    df[label] = df[label].apply(lambda x: x.replace(\"\\t\", \" \"))\n    return df\n#\ndf_train = clean_eol_tabs(df_train, \"CleanTweet\")","66719ce4":"def remove_emails(df, label):\n    \"\"\" This function removes email adresses\n        inputs:\n         - text \"\"\"\n    df[label] = df[label].apply(lambda x: re.sub(r\"\"\"(?:[a-z0-9!#$%&'*+\/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+\/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\", \" \", x))\n    return df\n#\ndf_train = remove_emails(df_train, \"CleanTweet\")","47ac0103":"def remove_mentions(df, label):\n    \"\"\" This function removes mentions (Twitter - starting with @) from texts\n        inputs:\n         - text \"\"\"\n    df[label] = df[label].apply(lambda x: re.sub(r\"@([a-zA-Z0-9_.-]{1,100})\", \" \", x))\n    return df\n#\ndf_train = remove_mentions(df_train, \"CleanTweet\")","cc5b281b":"def remove_hyperlinks(df, label):\n    \"\"\" This function removes hyperlinks from texts\n        inputs:\n         - text \"\"\"\n    df[label] = df[label].apply(lambda x: re.sub(r\"http\\S+\", \" \", x))\n    return df\n#\ndf_train = remove_hyperlinks(df_train, \"CleanTweet\")","edc5ea9e":"def remove_hashtags(df, label):\n    \"\"\" This function removes hashtags\n        inputs:\n         - text \"\"\"\n    df[label] = df[label].apply(lambda x: re.sub(r\"#\\w+\", \" \", x))\n    return df\n#\ndf_train = remove_hashtags(df_train, \"CleanTweet\")","f7de92f7":"def remove_html_tags(df, label):\n    \"\"\" This function removes html tags from texts\n        inputs:\n         - text \"\"\"\n    df[label] = df[label].apply(lambda x: re.sub(r\"<.*?>\", \" \", x))\n    return df\n#\ndf_train = remove_html_tags(df_train, \"CleanTweet\")","98169a31":"def remove_numbers(df, label):\n    \"\"\" This function removes numbers from a text\n        inputs:\n         - text \"\"\"\n    df[label] = df[label].apply(lambda x: re.sub(r\"\\d+\", \" \", x))\n    return df\n#\ndf_train = remove_numbers(df_train, \"CleanTweet\")","93f81673":"def encode_unknown(df, label):\n    \"\"\" This function encodes special caracters \"\"\"\n    df[label] = df[label].apply(lambda x: unicodedata.normalize(\"NFD\", x).encode('ascii', 'ignore').decode(\"utf-8\"))\n    return df\n#\ndf_train = encode_unknown(df_train, \"CleanTweet\")","5ff964c4":"def clean_punctuation_no_accent(df, label):\n    \"\"\" This function removes punctuation and accented characters from texts in a dataframe \n        To be appplied to languages that have no accents, ex: english \n    \"\"\"\n    df[label] = df[label].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))\n    return df\n#\ndf_train = clean_punctuation_no_accent(df_train, \"CleanTweet\")","5417fdb8":"def remove_stop_words(text, stopwords=set(stopwords.words('english'))):\n    \"\"\" This function removes stop words from a text\n        inputs:\n         - stopword list\n         - text \"\"\"\n\n    # prepare new text\n    text_splitted = text.split(\" \")\n    text_new = list()\n    \n    # stop words updated\n    #stopwords = stopwords.union({\"amp\", \"grocery store\", \"covid\", \"supermarket\", \"people\", \"grocery\", \"store\", \"price\", \"time\", \"consumer\"})\n    \n    # loop\n    for word in text_splitted:\n        if word not in stopwords:\n            text_new.append(word)\n    return \" \".join(text_new)\n\ndef clean_stopwords(df, label):\n    \"\"\" This function removes stopwords \"\"\"\n    df[label] = df[label].apply(lambda x: remove_stop_words(x))\n    return df\n#\ndf_train = clean_stopwords(df_train, \"CleanTweet\")","981895fd":"def more_cleaning(df, label):\n    \"\"\" This function\n     1) removes remaining one-letter words and two letters words\n     2) replaces multiple spaces by one single space\n     3) drop empty lines \"\"\"\n    df[label] = df[label].apply(lambda x: re.sub(r'\\b\\w{1,2}\\b', \" \", x))\n    df[label] = df[label].apply(lambda x: re.sub(r\"[ \\t]{2,}\", \" \", x))\n    df[label] = df[label].apply(lambda x: x if len(x) != 1 else '')\n    df[label] = df[label].apply(lambda x: np.nan if x == '' else x)\n    df = df.dropna(subset=[label], axis=0).reset_index(drop=True).copy()\n    return df\n#\ndf_train = more_cleaning(df_train, \"CleanTweet\")","abaee950":"def lemmatize_one_text(text):\n    \"\"\" This function lemmatizes words in text (it changes word to most close root word)\n        inputs:\n         - lemmatizer\n         - text \"\"\"\n\n    # initialize lemmatizer\n    lemmatizer = WordNetLemmatizer()\n    \n    # tags\n    lem_tags = ['a', 'r', 'n', 'v']\n\n    # prepare new text\n    text_splitted = text.split(\" \")\n    text_new = list()\n\n    # change bool\n    changed = ''\n    \n    # loop\n    for word in text_splitted:\n        text_new.append(lemmatizer.lemmatize(word))\n        #changed = ''\n        #for tag in lem_tags:\n        #    if lemmatizer.lemmatize(word, tag) != word:\n        #        changed = tag\n        #if changed == '':\n        #    text_new.append(word)\n        #else:\n        #    text_new.append(lemmatizer.lemmatize(word, changed))\n\n    return \" \".join(text_new)\n\ndef lemmatize(df, label):\n    \"\"\" This function lemmatizes texts \"\"\"\n    df[label] = df[label].apply(lambda x: lemmatize_one_text(x))\n    return df\n#\ndf_train = lemmatize(df_train, \"CleanTweet\")","1f46b471":"df_train.sample(5)","0024ee7e":"df_test[\"CleanTweet\"] = df_test[\"OriginalTweet\"]\ndf_test = clean_eol_tabs(df_test, \"CleanTweet\")\ndf_test = remove_emails(df_test, \"CleanTweet\")\ndf_test = remove_mentions(df_test, \"CleanTweet\")\ndf_test = remove_hyperlinks(df_test, \"CleanTweet\")\ndf_test = remove_hashtags(df_test, \"CleanTweet\")\ndf_test = remove_html_tags(df_test, \"CleanTweet\")\ndf_test = remove_numbers(df_test, \"CleanTweet\")\ndf_test = encode_unknown(df_test, \"CleanTweet\")\ndf_test = clean_punctuation_no_accent(df_test, \"CleanTweet\")\ndf_test = clean_stopwords(df_test, \"CleanTweet\")\ndf_test = more_cleaning(df_test, \"CleanTweet\")\ndf_test = lemmatize(df_test, \"CleanTweet\")","58aca6b8":"df_test.sample(3)","8d945a82":"tweet_num = random.randint(0, df_train.shape[0])\nprint(\"############################# Original Tweet #############################\")\nprint(df_train.iloc[tweet_num].at[\"OriginalTweet\"])\nprint(\"\\n\")\nprint(\"############################# Clean Tweet ################################\")\nprint(df_train.iloc[tweet_num].at[\"CleanTweet\"])","8d0d583f":"tweet_num = random.randint(0, df_train.shape[0])\nprint(\"############################# Original Tweet #############################\")\nprint(df_train.iloc[tweet_num].at[\"OriginalTweet\"])\nprint(\"\\n\")\nprint(\"############################# Clean Tweet ################################\")\nprint(df_train.iloc[tweet_num].at[\"CleanTweet\"])","978c94c4":"fig, (ax1, ax2) = plt.subplots(1, 2)\nfig.set_size_inches(18.5, 5)\nfig.suptitle('Sentiment repartition among tweets in train and test sets')\ndf_train[\"Sentiment\"].value_counts().plot(kind=\"bar\", ax=ax1);\ndf_test[\"Sentiment\"].value_counts().plot(kind=\"bar\", ax=ax2);","7148acc3":"all_words_positive = \" \".join([text for text in df_train[df_train[\"Sentiment\"]==\"Positive\"][\"CleanTweet\"]])\nall_words_neutral = \" \".join([text for text in df_train[df_train[\"Sentiment\"]==\"Neutral\"][\"CleanTweet\"]])\nall_words_negative = \" \".join([text for text in df_train[df_train[\"Sentiment\"]==\"Negative\"][\"CleanTweet\"]])","ec2cd506":"wordcloud_positive = WordCloud(width=800, height=600, max_font_size=120, background_color=\"white\", colormap=\"Greens\").generate(all_words_positive)\nwordcloud_neutral = WordCloud(width=800, height=600, max_font_size=120, background_color=\"white\", colormap=\"YlOrBr\").generate(all_words_neutral)\nwordcloud_negative = WordCloud(width=800, height=600, max_font_size=120, background_color=\"white\", colormap=\"Reds\").generate(all_words_negative)","cc268b0f":"parameters = {'axes.labelsize': 12,\n              'axes.titlesize': 10}\n#\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3)\nfig.set_size_inches(18.5, 7)\nax1.imshow(wordcloud_positive, interpolation='bilinear')\nax1.axis(\"off\")\nax1.set_title(\"WordCloud of positive tweets\", fontsize=12)\nax2.imshow(wordcloud_neutral, interpolation='bilinear')\nax2.axis(\"off\")\nax2.set_title(\"WordCloud of neutral tweets\", fontsize=12)\nax3.imshow(wordcloud_negative, interpolation='bilinear')\nax3.axis(\"off\")\nax3.set_title(\"WordCloud of negative tweets\", fontsize=12)\nplt.rcParams.update(parameters)\nplt.show()","ce0e71c4":"df_train_encoded = df_train.copy()\ndf_test_encoded = df_test.copy()\n#\nprint(\"train set shape: \" + str(df_train_encoded.shape))\nprint(\"test set shape: \" + str(df_test_encoded.shape))","65ddf0a3":"map_sentiment = {\"Neutral\":0, \"Positive\":1,\"Negative\":2}\ndf_train_encoded['Sentiment'] = df_train_encoded['Sentiment'].map(map_sentiment)\ndf_test_encoded['Sentiment']  = df_test_encoded['Sentiment'].map(map_sentiment)","080bf5d5":"y_train = df_train['Sentiment'].copy()\ny_test = df_test['Sentiment'].copy()\n#\ny_train_encoded = to_categorical(df_train_encoded['Sentiment'], 3)\ny_test_encoded = to_categorical(df_test_encoded['Sentiment'], 3)\n#\ny_train_mapped = df_train_encoded['Sentiment'].copy()\ny_test_mapped = df_test_encoded['Sentiment'].copy()\n#\nX_train = df_train_encoded[['CleanTweet']].copy()\nX_test = df_test_encoded[['CleanTweet']].copy()","13366193":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train[\"CleanTweet\"])\nvocab_length = len(tokenizer.word_index) + 1\nvocab_length","32d18406":"X_train = tokenizer.texts_to_sequences(X_train[\"CleanTweet\"])\nX_test = tokenizer.texts_to_sequences(X_test[\"CleanTweet\"])","fd612058":"print(\"First tweet encoded:\")\nprint(X_train[0])\nprint(\"\\nSecond tweet encoded:\")\nprint(X_train[1])\nprint(\"\\nThird tweet encoded:\")\nprint(X_train[2])","b4fb9d68":"max_word_count = 0\nword_count = []\n#\nfor encoded_tweet in X_train:\n    word_count.append(len(encoded_tweet))\n    if len(encoded_tweet) > max_word_count:\n        max_word_count = len(encoded_tweet)\nprint(\"Maximum number of word in one tweet: \" + str(max_word_count) + \" words\")","b11512ec":"parameters = {'axes.labelsize': 20,\n              'axes.titlesize': 30}\n#\nplt.rcParams.update(parameters)\nfig, ax1 = plt.subplots(1, 1)\nfig.set_size_inches(18.5, 8)\nsns.histplot(word_count, palette='Blues', stat='density', bins=30, ax=ax1);\nsns.kdeplot(word_count, color='red', ax=ax1)\nax1.set_xlabel('Word count per tweet');\nax1.tick_params(axis='x', labelsize=16)\nax1.tick_params(axis='y', labelsize=16)\nax1.set_ylabel(\"\")\nax1.set_title(\"Tweet length distribution\", color =\"#292421\")\nfig.tight_layout(pad=2.0)\nplt.rcParams.update(parameters)","f541e1af":"X_train = pad_sequences(X_train, maxlen=max_word_count, padding='post')\nX_test = pad_sequences(X_test, maxlen=max_word_count, padding='post')\nX_train.shape","6e067c13":"print(\"First tweet encoded:\", \"Size = \", len(X_train[0]))\nprint(X_train[0])\nprint(\"\\nSecond tweet encoded:\", \"Size = \", len(X_train[1]))\nprint(X_train[1])\nprint(\"\\nThird tweet encoded:\", \"Size = \", len(X_train[2]))\nprint(X_train[2])","1272e8a3":"model_LSTM = Sequential()\nmodel_LSTM.add(layers.Embedding(vocab_length, output_dim=32, input_length=max_word_count, mask_zero=True))\nmodel_LSTM.add(layers.LSTM(100))\nmodel_LSTM.add(layers.Dense(64, activation=\"relu\"))\nmodel_LSTM.add(layers.Dense(32, activation=\"relu\"))\nmodel_LSTM.add(layers.Dense(16, activation=\"relu\"))\nmodel_LSTM.add(layers.Dense(3, activation='softmax'))\nmodel_LSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model_LSTM.summary())","c9d76040":"es = EarlyStopping(patience=10, monitor='val_accuracy', restore_best_weights=True)\nhistory = model_LSTM.fit(X_train,\n                         y_train_encoded,\n                         validation_data=(X_test, y_test_encoded),\n                         epochs=30,\n                         batch_size=16,\n                         verbose=1,\n                         callbacks=[es]\n                        )","14c2e6e1":"predicted = model_LSTM.predict(X_test)\ny_pred = predicted.argmax(axis=-1)","b260045e":"acc_score = accuracy_score(y_test_mapped, y_pred)\nauc_score = roc_auc_score(y_test_mapped, predicted, multi_class=\"ovr\")","e94fa44e":"report = classification_report(y_test_mapped, y_pred, target_names=list(y_test.unique()), output_dict=True)\naccuracy_col = ([\"\"]*3) + [round(acc_score, 2)]\nroc_auc_col = ([\"\"]*3) + [round(auc_score, 2)]\naccuracy_col = pd.Series(accuracy_col, index=list(report[\"Neutral\"].keys()))\nroc_auc_col = pd.Series(roc_auc_col, index=list(report[\"Neutral\"].keys()))\ndf_report = pd.DataFrame(report)[[\"Neutral\", \"Positive\", \"Negative\", \"macro avg\", \"weighted avg\"]].apply(lambda x: round(x, 2))\ndf_report[\"accuracy\"] = accuracy_col\ndf_report[\"roc_auc\"] = roc_auc_col\ndf_report","8a36c605":"## Plot confusion matrix\ncm = confusion_matrix(y_test_mapped, y_pred)\nfig, ax = plt.subplots()\nfig.set_size_inches(12, 8)\nsns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, cbar=False)\nax.set(xticklabels=list(y_test.unique()), yticklabels=list(y_test.unique()), title=\"Confusion matrix\")\nax.tick_params(axis='x', labelsize=16)\nax.tick_params(axis='y', labelsize=16)\nax.set_ylabel(\"True\", color=\"royalblue\", fontsize=35, fontweight=700)\nax.set_xlabel(\"Prediction\", color=\"royalblue\", fontsize=35, fontweight=700)\nplt.yticks(rotation=0);","f1d63cdb":"y_test_array = pd.get_dummies(y_test_mapped, drop_first=False).values\nclasses = y_train.unique()","3a8c2456":"fig, ax = plt.subplots(nrows=1, ncols=2)\nfig.set_size_inches(18.5, 5)\n## Plot roc\nfor i in range(len(classes)):\n    fpr, tpr, thresholds = roc_curve(y_test_array[:,i], predicted[:,i])\n    ax[0].plot(fpr, tpr, lw=3, \n              label='{0} (area (AUC) = {1:0.2f})'.format(classes[i], metrics.auc(fpr, tpr))\n               )\nax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\nax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n          xlabel='False Positive Rate', \n          ylabel=\"True Positive Rate (Recall)\", \n          title=\"Receiver operating characteristic\")\nax[0].legend(loc=\"lower right\")\nax[0].grid(True)\n\n## Plot precision-recall curve\nfor i in range(len(classes)):\n    precision, recall, thresholds = metrics.precision_recall_curve(\n                 y_test_array[:,i], predicted[:,i])\n    ax[1].plot(recall, precision, lw=3, \n               label='{0} (area ={1:0.2f})'.format(classes[i], metrics.auc(recall, precision))\n              )\nax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n          ylabel=\"Precision\", title=\"Precision-Recall curve\")\nax[1].legend(loc=\"best\")\nax[1].grid(True)\nplt.show()","65e5bb7c":"<b>Classification report:<\/b>","3d5af1dd":"# 5. Cleaning tweets","4a550b4e":"### Let's apply all of these cleaning on test data set as well:","81915d2d":"# 3.  Data overview","c07ddbf3":"### Let's have a look on before\/after cleaning on several tweets:","1979ba78":"<b>Confusion Matrix:<\/b>","23905457":"<div style=\"font-size:15pt; color:#104E8B; font-weight:700; width:80%; display:block; margin:auto; text-align:center\">Thanks for reading, I hope you enjoyed it. If there is anything wrong or if you have any suggestions for improvement, please feel free to comment, I'll be glad to get feedback to improve.<\/div>","e0c94f74":"Now every encoded tweet has the same length, the data is ready for the model.","4960c474":"# 8. Feature and target preparation","56bbf48c":"<b>Removing end-of-line, tabulation and carriage return. Turning into lower case:<\/b>","b33cccb1":"# 11. Model Evaluation","d812785d":"<b>Removing punctuations and special characters:<\/b><br>\n*Note this function will remove punctuation AND accented characters. Thus it is not necessary usable on languages that have accented characters. But for english it is ok.*","5e2f43d0":"<b>The model is composed of:<\/b>\n1. <b>An embedding layer with parameters<\/b>\n    * input dim = vocabulary size\n    * output dim = 32\n    * input length = size of padded sequences\n    * mask_zero = True to ignore 0 (from padding)\n2. <b>An LSTM (Long Short Term Memory) Layer with parameter<\/b>\n    * units = 100 (don't ask me why, the resulting accuracy is almost the same regardless this value)\n3. <b>Three Dense layers<\/b>\n4. <b>An output dense layer with parameters<\/b>\n    * units = 3 (output dim)\n    * activation = softmax (for multiclassification problem)\n\n<b>Compilation with parameters:<\/b>\n1. loss = categorical_crossentropy (for multiclassification problem)\n2. optimizer = adam\n3. metrics = accuracy","e4d715b4":"# 1.  Imports","94c63c29":"<b>Removing one and two letters words, removing unnecessary spaces, droping empty lines:<\/b>","3dcbf763":"<b>Lemmatizing words:<\/b><br>\n*Note: Here the lemmatizer works only for its default parameter which is <b>nouns<\/b>. That is to say, it will only find the closest root for nouns and will not work on verbs or adjectives ect ... I tried with lemmatization of everything but the accuracy was lower*","c13a2cd2":"<b>Removing e-mails:<\/b>","cea2cc92":"Here I set an early stopping after 10 epochs and set the parameter <i><b>restore_best_weights<\/b><\/i> to <b style=\"color:green\">True<\/b> so that the weights of best score on monitored metric - here <b>val_accuracy<\/b> (accuracy on test set) - are restored when training stops. This way the model has the best accuracy possible on unseen data.","e87188e3":"So the longest tweet we have is composed of 37 words. We are going to pad the sequences with a maximum length of 37.","16dfd4ae":"<b>ROC and precision-recall curves<\/b>","f1af8ad6":"<b>Removing html tags:<\/b>","c8a62121":"# 7. Sentiment encoding","71b28752":"The <b>texts_to_sequences<\/b> function first transforms a text into list of words. Then, thanks to the dictionnary previously created by the tokenizer (see above), transforms list of list of words into list of list of numbers","106dcb06":"<b>Removing stop words. Here, the list is from nltk stopwords library:<\/b>","a6cb1dbf":"Each tweets has differents length. Thus the result of the <b>texts_to_sequences<\/b> function will be a list of list of numbers of different length: ","96767302":"# 2.  Loading data","f7067fb3":"<b>Prediction on test set:<\/b>","cb013bbf":"<b>Encode unknown characters:<\/b>","9b66a7ce":"# 10. Model","b00d2921":"<b>Removing hashtags:<\/b>","22d280e5":"It looks good","043b3416":"<b>Calculation of accuracy and Area Under (ROC) Curve - AUC - scores:<\/b>","5306cda0":"<b>Removing hyperlinks:<\/b>","55a1fb95":"<b>Word cloud in each sentiment categories:<\/b>","ac61df4e":"<b>Removing numbers:<\/b>","3699edd9":"Let's have a look at the encoded 3 tweets after padding:","84e81d6d":"# 6.  Looking at data","52688f6e":"To feed the deep learning model, we need all these lists to be the same length. Thus we need to apply padding. In other words, we are going to add several zeros (0) at the end of the shortest tweets so that at the end, all of our lists have the same length. <br><br>\nFirst let's get the maximum number of words in one tweet:","7cfee7a5":"# 4.  Turning 5 categories into 3 categories\nHere we convert <b>extremely positive<\/b> tweets into <b style=\"color: green\">positive<\/b> and <b>extremely negative<\/b> tweets into <b style=\"color: red\">negative<\/b>.","11df620e":"We can see that after epoch 2, the accuracy on test set - val_accuracy - doesn't increase any more while accuracy on train set continues to increase untill almost 100%! The model is overfitting from epoch 2 and is not able to generalize well on unseen data from there.","1be16ad3":"# 9. Tokenization, sequences, padding","5cfc6cdf":"<div style=\"display: block; height: 500px; overflow:hidden;position: relative\">\n     <img src=\"https:\/\/imgur.com\/6I1AHP5.jpg\" style=\"position: absolute;top: 0px;\">\n<\/div>","80fff34c":"<b>Removing mentions:<\/b>","bbe67461":"<b>Sentiment repartition:<\/b>","fe957a62":"<b>The result of tokenizer is a dictionnary with:<\/b><br>\n* key = word<br>\n* value = unique number"}}