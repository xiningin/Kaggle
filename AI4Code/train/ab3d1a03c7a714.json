{"cell_type":{"0a829bf1":"code","ab28e949":"code","50929889":"code","31e7df50":"code","1172652c":"code","8301dad4":"code","cf4da845":"code","a4399c31":"code","fcae7b7d":"code","d9e65e13":"code","5e90d176":"code","679732a5":"code","18c70259":"code","b63e9315":"code","bc25ff64":"markdown","8e08c848":"markdown","ba6b4a59":"markdown","216247cc":"markdown","247cda6f":"markdown","5cf8905a":"markdown","f8cae486":"markdown","04c8da55":"markdown","50d23bde":"markdown","44641d8e":"markdown","240c1858":"markdown","530bf672":"markdown","bdd8dba4":"markdown","5b103501":"markdown"},"source":{"0a829bf1":"import os\n\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sea\n\nimport tensorflow as tf\nfrom tensorflow.keras.applications.mobilenet import MobileNet\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.mobilenet import preprocess_input\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.regularizers import l2\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.utils import shuffle\nfrom sklearn import svm\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec","ab28e949":"sea.set_style(\"darkgrid\")","50929889":"classes = [\"daisy\", \"dandelion\", \"rose\", \"sunflower\", \"tulip\"]\npath = \"\/kaggle\/input\/flowers-recognition\/flowers\"\n\nfile_path = [os.path.join(path, \"daisy\/100080576_f52e8ee070_n.jpg\"),\n             os.path.join(path, \"dandelion\/10043234166_e6dd915111_n.jpg\"),\n             os.path.join(path, \"rose\/10090824183_d02c613f10_m.jpg\"),\n             os.path.join(path, \"sunflower\/1008566138_6927679c8a.jpg\"),\n             os.path.join(path, \"tulip\/100930342_92e8746431_n.jpg\")]\n\nfig = plt.figure(figsize=(10, 12))\ngs = gridspec.GridSpec(nrows=3, ncols=2, figure=fig)\n\nfor i in range(5):\n    y, x = i\/\/2, i%2 \n    ax = fig.add_subplot(gs[y,x])\n    ax.imshow(image.load_img(file_path[i]))\n    ax.axis(\"off\")\n    ax.title.set_text(classes[i])","31e7df50":"# load pretrained MobileNet\nmodel = MobileNet(input_shape=(224,224,3), include_top=True)\n\nmodel.summary()","1172652c":"vector = model.get_layer(\"reshape_2\").output\nfeature_extractor = tf.keras.Model(model.input, vector)","8301dad4":"# create empty feature and label lists\nX_list = []\nY_list = []\n\nfor f in range(5):    \n    folder_path = os.path.join(path, classes[f])\n    for file in os.listdir(folder_path):    \n        file_path = os.path.join(folder_path, file)\n        \n        # check file extension, skip file if not jpg\n        if not(file.endswith(\".jpg\")):\n            continue\n        \n        # load image\n        img = image.load_img(file_path, target_size=(224,224))\n        # convert image to numpy array\n        img_arr = image.img_to_array(img)\n        # add 1 more dimension\n        img_arr_b = np.expand_dims(img_arr, axis=0)\n        # preprocess image\n        input_img = preprocess_input(img_arr_b)\n        # extract feature\n        feature_vec = feature_extractor.predict(input_img)\n    \n        X_list.append(feature_vec.ravel())\n        Y_list.append(f)","cf4da845":"X = np.asarray(X_list, dtype=np.float32)\nY = np.asarray(Y_list, dtype=np.float32)\n\nfor s in range(100):\n    X, Y = shuffle(X, Y)\n    \nprint(\"Shape of feature matrix X\")\nprint(X.shape)\nprint(\"\\nShape of label matrix Y\")\nprint(Y.shape)\n\nclass_types, counts = np.unique(Y, return_counts=True)\n\nprint(\"\\nClass labels\")\nprint(class_types)\nprint(\"\\nClass counts\")\nprint(counts)","a4399c31":"train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.2,\n                                                    stratify=Y,\n                                                    random_state=0)\n\nprint(\"Shape of train_X\")\nprint(train_X.shape)\nprint(\"\\nShape of test_X\")\nprint(test_X.shape)","fcae7b7d":"svm_lin = svm.SVC(C=1.0, kernel=\"linear\")\nsvm_lin.fit(train_X, train_Y)\ny_pred = svm_lin.predict(test_X)\nprint(classification_report(test_Y, y_pred,\n                            target_names=classes))","d9e65e13":"svm_nlin = svm.SVC(C=1.0, kernel=\"rbf\")\nsvm_nlin.fit(train_X, train_Y)\ny_pred = svm_nlin.predict(test_X)\nprint(classification_report(test_Y, y_pred,\n                            target_names=classes))","5e90d176":"n_encoder = OneHotEncoder(sparse=False)\n\n# fit encoder to train_Y\nn_encoder.fit(train_Y.reshape(-1,1))\n# transform train_Y\ne_train_Y = n_encoder.transform(train_Y.reshape(-1,1))\n# transform test_Y\ne_test_Y = n_encoder.transform(test_Y.reshape(-1,1))","679732a5":"def create_model():\n    model = Sequential()\n    model.add(Dense(256, input_dim=1000, activation=\"relu\"))\n    model.add(Dropout(0.3))\n    model.add(Dense(5, kernel_regularizer=l2(0.1), activation=\"linear\"))\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n                 loss=\"categorical_hinge\")\n    return model\n\nepoch = 100\nmodel = create_model()\nhistory = model.fit(train_X, e_train_Y,\n                    validation_split = 0.15,\n                    epochs=epoch, batch_size=64, verbose=1)","18c70259":"e = np.linspace(1, epoch, epoch)\nfig, axes = plt.subplots(nrows=1, ncols=1, figsize=(8, 6))\nsea.lineplot(x = e, y = history.history[\"loss\"],\n             ax=axes, label=\"train\");\nsea.lineplot(x = e, y = history.history[\"val_loss\"],\n             ax=axes, label=\"val\");\naxes.set_ylabel(\"Categorical Hinge Loss\")\naxes.set_xlabel(\"epoch\");","b63e9315":"y_pred = np.argmax(model.predict(test_X), axis=-1);\nprint(classification_report(test_Y, y_pred,\n                            target_names=classes))","bc25ff64":"Performance of **neural network model** is evaluated on **test_X**.","8e08c848":"## Split Data\n\nDataset is splitted into train and test sets. We want stratified splits with same class distributions.","ba6b4a59":"## SVM with Linear Kernel\n\nFirst we try linear **SVM** from **scikit-learn**.","216247cc":"## SVM with Nonlinear Kernel\n\nSecond classifier is **SVM** with nonlinear kernel. Default nonlinear kernel is **rbf** in **scikit-learn**.","247cda6f":"The output of **reshape_2** layer is a 1000 elements vector. **MobileNet** **predictions** layer takes this vector as input. We use this vector as our feature and try to do classification.\n\nA new model **feature_extractor** is created.","5cf8905a":"## Prepare Data with Transfer Learning\n\n**Flowers Recognition** dataset will be used to compare different methods. In dataset, there are 5 classes: daisy, dandelion, rose, sunflower and tulip. Images belonging to a specific class are collected in the corresponding folder. Dimensions of the images are not standardized. One sample from each class is shown below.","f8cae486":"All files in 5 folders are scanned one by one. There are some non-image files in dataset folders. Extension of each file is checked and file is skipped if its extension is not **jpg**. Images are read, resized, converted to numpy array and preprocessed. Then, **feature_extractor** takes image and produces corresponding feature vector. \n\nPreprocessing step is similar to standardization used for tabular data. The aim is to scale pixel values to [-1,1] interval. Pixel values of 8-bit images are in the interval [0,255]. If image is in **RGB** format as in our case, then there are 3 channels (one channel for each color) which means every pixel has three subpixels. In **MobileNet** preprocessing step, each subpixel value is divided by 127.5, this way pixel values are scaled to [0,2] interval. Then 1 is subtracted from scaled values to shift them to [-1,1] interval.","04c8da55":"There isn't any serious imbalance problem. There are 4323 samples.","50d23bde":"**Support Vector Machine (SVM)** is one of the most used supervised machine learning methods. It can be used for regression and classification. We focus on classification in this notebook. **SVM** is a linear classifier in its original form. It determines a hyperplane as a decision boundary. Nonlinear kernels are used to handle nonlinear cases. Data is transformed to higher dimensional space and this way it is expected to be linearly seperable. In scikit-learn implementation, the kernels can be **linear**, **poly**, **rbf**, **sigmoid** or **custom**. \n\nOutline of the work is as follows:\n\n* Prepare Data with Transfer Learning\n* Split Data\n* SVM with Linear Kernel\n* SVM with Nonlinear Kernel\n* Neural Network with Hinge Loss","44641d8e":"## Neural Network with Hinge Loss\n\nAs the third method, we will try a **neural network** with **categorical hinge loss**. The number of neurons in the output layer is same with the number of classes in our dataset. We use **one-hot-encoder** to transfom train and test labels.","240c1858":"Loss curves are drawn for training and validation using **Keras** history object.","530bf672":"Lists are converted to numpy arrays and shuffled. Then shape of the dataset is printed and occurrences of each class label is counted.","bdd8dba4":"**Linear SVM** has the lowest performance. **Nonlinear SVM** and **neural network model** are very close considering the classification reports.\n\n**Nonlinear SVM** uses nonlinear kernels to move the data to a higher dimensional space. This is done to make the data linearly separable. On the other hand, a neural network transforms data nonlinearly in each layer. And neural network has the flexibility to learn the specific transformation needed for the data under consideration. ","5b103501":"Feature vectors will be extracted from flower images. Three methods mentioned in the introduction will be trained on these features and their performances will be compared.\n\nTransfer learning is used for feature extraction. Pretrained **MobileNet** is loaded from **Keras** applications and used to get feature vector for each image in the dataset. **MobileNet** is loaded with specific input resolution. Then model structure is observed using **summary** function."}}