{"cell_type":{"85676fc4":"code","48017fc7":"code","73266880":"code","8c0b2a69":"code","20b94064":"code","15f198a2":"code","5dd1e099":"code","0aab2f40":"code","45c068bf":"code","b3bdaccb":"code","f88c717b":"code","862d5f3c":"code","ce295857":"code","412357ab":"code","e51952d7":"code","91d09cdd":"code","5ac612d4":"code","7d4dd4ba":"code","daf210b1":"code","0028e19c":"code","cad889c9":"code","63568382":"code","32a0d12b":"markdown","0c009be8":"markdown","813f40f2":"markdown","1d3d8267":"markdown","cd39fe56":"markdown","f7557cfb":"markdown","e6804591":"markdown","5a0b6de2":"markdown","b0e1d6c5":"markdown","c328e4e1":"markdown","d6f901c8":"markdown","7728ed0e":"markdown","618d9f1b":"markdown","f1802d55":"markdown","6e8b2b30":"markdown","ea1641af":"markdown","2f9b50cc":"markdown","bb239401":"markdown","118e7671":"markdown"},"source":{"85676fc4":"import warnings\nwarnings.filterwarnings(\"ignore\")","48017fc7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfilepath = '..\/input\/diamonds\/diamonds.csv'\ndiamond_data = pd.read_csv(filepath)\n# Drop the first column since it has indexes we don't need\ndiamond_data.drop(diamond_data.columns[0], axis=1, inplace=True)\n\nnum_cols = diamond_data.select_dtypes(exclude=['object']).columns\ncat_cols = np.setdiff1d(diamond_data.columns, num_cols)","73266880":"for feature in diamond_data.columns:\n    isnull = diamond_data.loc[diamond_data[feature].isnull()].shape[0]\n    print(feature + \" - \" + str(isnull))","8c0b2a69":"diamond_data[num_cols].describe()","20b94064":"diamond_data.drop(diamond_data[diamond_data.x == 0].index, inplace=True)\ndiamond_data.drop(diamond_data[diamond_data.y == 0].index, inplace=True)\ndiamond_data.drop(diamond_data[diamond_data.z == 0].index, inplace=True)\ndiamond_data.reindex()\ndiamond_data.describe()","15f198a2":"diamond_data[cat_cols].describe()","5dd1e099":"import seaborn as sns\nsns.distplot(diamond_data.price, kde=False)","0aab2f40":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\nsns.scatterplot(x=diamond_data['carat'], y=diamond_data['price'], ax=ax1)\nsns.scatterplot(x=diamond_data['depth'], y=diamond_data['price'], ax=ax2)\nsns.scatterplot(x=diamond_data['table'], y=diamond_data['price'], ax=ax3)","45c068bf":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\nsns.stripplot(x=diamond_data['clarity'], y=diamond_data['price'], ax=ax1)\nsns.stripplot(x=diamond_data['color'], y=diamond_data['price'], ax=ax2)\nsns.stripplot(x=diamond_data['cut'], y=diamond_data['price'], ax=ax3)","b3bdaccb":"clarity_cut = diamond_data[['clarity', 'cut']]\nclarity_cut_concat = clarity_cut['clarity'].map(str) + '_' + clarity_cut['cut'].map(str)\ncl_cut_counts = clarity_cut.assign(concat=clarity_cut_concat).groupby(['clarity', 'cut']).concat.count()\n\nclarity_color = diamond_data[['clarity', 'color']]\nclarity_color_concat = clarity_color['clarity'].map(str) + '_' + clarity_color['color'].map(str)\ncl_col_counts = clarity_color.assign(concat=clarity_color_concat).groupby(['clarity', 'color']).concat.count()\n\ncolor_cut = diamond_data[['color', 'cut']]\ncolor_cut_concat = color_cut['color'].map(str) + '_' + color_cut['cut'].map(str)\ncol_cut_counts = color_cut.assign(concat=color_cut_concat).groupby(['color', 'cut']).concat.count()\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 5))\nsns.heatmap(data=cl_cut_counts.unstack(), ax=ax1)\nsns.heatmap(data=cl_col_counts.unstack(), ax=ax2)\nsns.heatmap(data=col_cut_counts.unstack(), ax=ax3)","f88c717b":"clarity_dict = {'FL': 11,'IF': 10, 'VVS1': 9, 'VVS2': 8, 'VS1': 7,\n                'VS2': 6, 'SI1': 5, 'SI2': 4, 'I1': 3, 'I2': 2, 'I3': 1}\ncolor_dict = {'D': 7, 'E': 6, 'F': 5, 'G': 4, 'H': 3, 'I': 2, 'J': 1}\ncut_dict = {'Fair': 1, 'Good': 2, 'Very Good': 3, 'Premium': 4, 'Ideal': 5}\n\ndiamond_data['clarity'] = diamond_data['clarity'].map(clarity_dict)\ndiamond_data['cut'] = diamond_data['cut'].map(cut_dict)\ndiamond_data['color'] = diamond_data['color'].map(color_dict)\ndiamond_data[['clarity', 'cut', 'color']].head()","862d5f3c":"diamond_data['xyz'] = diamond_data['x'] * diamond_data['y'] * diamond_data['z']\nxyz_carat = diamond_data['xyz'] * diamond_data['carat']\ndiamond_data['xyz_carat'] = xyz_carat.map(np.log)\n\ndepth_table = diamond_data['depth'] * diamond_data['table']\ndiamond_data['depth_table'] = depth_table.map(np.log)\n\ncarat_depth = diamond_data['depth'] * diamond_data['carat']\ndiamond_data['carat_depth'] = carat_depth.map(np.log)\n\ncarat_table = diamond_data['carat'] * diamond_data['table']\ndiamond_data['carat_table'] = carat_table.map(np.log)\n\ndiamond_data.describe()","ce295857":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\nsns.scatterplot(x=diamond_data['xyz_carat'], y=diamond_data['price'], ax=ax1)\nsns.scatterplot(x=diamond_data['carat_table'], y=diamond_data['price'], ax=ax2)\nsns.scatterplot(x=diamond_data['carat_depth'], y=diamond_data['price'], ax=ax3)","412357ab":"import itertools\nfrom sklearn.preprocessing import LabelEncoder\n\ninteractions = pd.DataFrame(index=diamond_data.index)\nfor col1, col2 in itertools.combinations(cat_cols, 2):\n    col_name = col1 + '_' + col2\n    interaction = diamond_data[col1].map(str) + '_' + diamond_data[col2].map(str)\n    encoder = LabelEncoder()\n    interactions[col_name] = encoder.fit_transform(interaction)\n    \ndiamond_data = diamond_data.join(interactions)\ndiamond_data.head()","e51952d7":"def train_valid_test_split(data, train_percent, valid_percent):\n    np.random.seed(6)\n    perm = np.random.permutation(diamond_data.index)\n    n = len(data.index)\n    train_end = int(train_percent * n)\n    valid_end = int(valid_percent * n) + train_end\n    train = data.loc[perm[:train_end]]\n    valid = data.loc[perm[train_end:valid_end]]\n    test = data.loc[perm[valid_end:]]\n    return train, valid, test\n\ntrain, valid, test = train_valid_test_split(diamond_data, 0.7, 0.2)\ntrain_X = train.drop('price', axis=1)\ntrain_y = train.price\nvalid_X = valid.drop('price', axis=1)\nvalid_y = valid.price\ntest_X = test.drop('price', axis=1)\ntest_y = test.price","91d09cdd":"from sklearn.metrics import mean_absolute_error\n\ndef evaluate_model(model, train_X, train_y, valid_X, valid_y):\n    model.fit(train_X, train_y)\n    predictions = model.predict(valid_X)\n    return mean_absolute_error(valid_y, predictions)","5ac612d4":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\n\nmodels = [('DecisionTreeRegressor', DecisionTreeRegressor()),\n          ('RandomForestRegressor', RandomForestRegressor()),\n          ('LinearRegression', LinearRegression()),\n          ('XGBRegressor', XGBRegressor())]\n\nfor model_name, model in models:\n    print('mae for ' + model_name + \": \", end='')\n    print(evaluate_model(model, train_X, train_y, valid_X, valid_y))","7d4dd4ba":"model = RandomForestRegressor()","daf210b1":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\n\nparams = [6, 8, 10, 12, 14, 15, 16, 17]\nfor k in params:\n    selector = SelectKBest(f_regression, k=k)\n    X_new = selector.fit_transform(train_X, train_y)\n    selected_features = pd.DataFrame(selector.inverse_transform(X_new),\n                                     index=train_X.index, columns=train_X.columns)\n    selected_columns = selected_features.columns[selected_features.var() != 0]\n    print('mae for k = {}: '.format(k), end='')\n    print(evaluate_model(model, train_X[selected_columns], train_y, valid_X[selected_columns], valid_y))","0028e19c":"selector = SelectKBest(f_regression, k=15)\nX_new = selector.fit_transform(train_X, train_y)\nselected_features = pd.DataFrame(selector.inverse_transform(X_new),\n                                     index=train_X.index, columns=train_X.columns)\nselected_columns = selected_features.columns[selected_features.var() != 0]\n    \ntrain_X = train_X[selected_columns]\nvalid_X = valid_X[selected_columns]\ntest_X = test_X[selected_columns]\nselected_columns","cad889c9":"n_estimators = range(5, 96, 15)\n\nfor num in n_estimators:\n    print('mae for n_estimators = {}: '.format(num), end='')\n    print(evaluate_model(RandomForestRegressor(n_estimators=num),train_X, train_y, valid_X, valid_y))","63568382":"print('mae on test data: ')\nprint(evaluate_model(RandomForestRegressor(n_estimators=50),train_X, train_y, test_X, test_y))","32a0d12b":"Looks like there's no correspondence between depth and table features and price. I'll try to do some feature engineering to make a use of them.","0c009be8":"Time to add some generic numerical features.","813f40f2":"Now it's time to tune the regressor itself.","1d3d8267":"There's a pretty big difference between prices...","cd39fe56":"Split data into train, validation and test datasets.","f7557cfb":"Uh, my pulse is raising... Final testing on test data.","e6804591":"Fortunately, there are no NaN values.","5a0b6de2":"Yep, it could be better but could be worse either \ud83e\udd23","b0e1d6c5":"I'll combine categorical features, later it will be more clear which ones are useful. It's better to fit LaberEncoder only on training data, but I don't know does it matter in my case, to be fair \ud83d\ude03","c328e4e1":"I'll try running different models with default parameters to see which one performs best. Then I'll do some hyperparameter tuning.","d6f901c8":"Seems like there's a relationship between new features and diamond price but plots look kinda same so I guess I'll drop some of these features later\ud83e\udd14","7728ed0e":"I'll leave 15 features because mae was a little bit lower with this parameter. I have a feeling that I can improve results by creating more generic features but at the moment I don't have much experience to figure out how to get more of them and which ones will be better.","618d9f1b":"Plot relationships between categorical data and price.","f1802d55":"It took a plenty of time to figure out how to plot a heatmap from categorical data but it worth it \u270a","6e8b2b30":"\\*I tried changing params such as min_samples_split and min_samples_leaf but mae was higher\\*","ea1641af":"A plenty of entries have zero length, width or depth, it seems not so real.","2f9b50cc":"Let's try selecting useful features to improve model.","bb239401":"I'll do label encoding by hand because the order of features is important.","118e7671":"Let's read the data from .csv first."}}