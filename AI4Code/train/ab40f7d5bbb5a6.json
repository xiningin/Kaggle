{"cell_type":{"84fcc06c":"code","060e4dbc":"code","232282b3":"code","193fc0e1":"code","2f70a6a5":"code","84891a0c":"code","daa31df9":"code","26fab8a6":"code","0adea834":"code","7c7fcfb1":"code","9a793af4":"code","52bec4b8":"code","68a6c350":"markdown","d08861cd":"markdown","d4dc7350":"markdown","19f8bef0":"markdown","dae3bad8":"markdown","ec56435b":"markdown","145bf25a":"markdown"},"source":{"84fcc06c":"#import sys\n#!{sys.executable} -m conda install tensorflow-gpu -y\n#!{sys.executable} -m pip install keras==2.1.4","060e4dbc":"import keras\nimport tensorflow as tf\nprint(keras.__version__)\nprint(tf.__version__)","232282b3":"## some config values \nembed_size = 300 # how big is each word vector\nmax_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 50 # max number of words in a question to use\n\nRANDOM_SEED = 42\n","193fc0e1":"import os\nimport time\nimport numpy as np # linear algebra\nimport random as rn\n\nnp.random.seed(RANDOM_SEED)\nrn.seed(RANDOM_SEED)\n\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import f1_score, roc_auc_score\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import *\nfrom keras.optimizers import *\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.callbacks import *\nfrom keras.initializers import *\n\nfrom tensorflow import set_random_seed\n \nset_random_seed(RANDOM_SEED)\n\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\nK.set_session(sess)\n\nfrom tensorflow.python.client import device_lib\ndevice_lib.list_local_devices()\n","2f70a6a5":"def load_and_prec():\n    train_df = pd.read_csv(\"..\/input\/train.csv\")\n    test_df = pd.read_csv(\"..\/input\/test.csv\")\n    print(\"Train shape : \",train_df.shape)\n    print(\"Test shape : \",test_df.shape)\n    \n    ## fill up the missing values\n    train_X = train_df[\"question_text\"].fillna(\"_##_\").values\n    test_X = test_df[\"question_text\"].fillna(\"_##_\").values\n\n    ## Tokenize the sentences\n    tokenizer = Tokenizer(num_words=max_features)\n    tokenizer.fit_on_texts(list(train_X))\n    train_X = tokenizer.texts_to_sequences(train_X)\n    test_X = tokenizer.texts_to_sequences(test_X)\n\n    ## Pad the sentences \n    train_X = pad_sequences(train_X, maxlen=maxlen)\n    test_X = pad_sequences(test_X, maxlen=maxlen)\n\n    ## Get the target values\n    train_y = train_df['target'].values\n    \n    #shuffling the data\n    np.random.seed(RANDOM_SEED)\n    trn_idx = np.random.permutation(len(train_X))\n\n    train_X = train_X[trn_idx]\n    train_y = train_y[trn_idx]\n    \n    return train_X, test_X, train_y, tokenizer.word_index","84891a0c":"def load_glove(word_index):\n    EMBEDDING_FILE = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding='utf-8'))\n\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n\n    # word_index = tokenizer.word_index\n    nb_words = min(max_features, len(word_index))\n    np.random.seed(RANDOM_SEED)\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= max_features: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n            \n    return embedding_matrix \n    ","daa31df9":"def get_model(embedding_matrix):\n    \n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    #x = SpatialDropout1D(0.1, seed = RANDOM_SEED)(x)\n    \n    #x = Bidirectional(CuDNNLSTM(40, return_sequences=True, kernel_initializer=glorot_uniform(seed = RANDOM_SEED), \\\n    #                                recurrent_initializer=Orthogonal(seed = RANDOM_SEED)))(x)\n        \n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    \n    conc = concatenate([avg_pool, max_pool])\n    \n    x = Dense(16, activation=\"relu\", kernel_initializer=he_uniform(seed=RANDOM_SEED))(conc)\n    #x = Dropout(0.1, seed = RANDOM_SEED)(x)\n    outp = Dense(1, activation=\"sigmoid\", kernel_initializer=he_uniform(seed=RANDOM_SEED))(x)    \n\n    model = Model(inputs=inp, outputs=outp)\n    optimizer = Adam()\n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    \n    return model","26fab8a6":"# https:\/\/www.kaggle.com\/strideradu\/word2vec-and-gensim-go-go-go\ndef train_pred(model, train_X, train_y, val_X, val_y, epochs=2, callback=None):\n    for e in range(epochs):\n        \n        np.random.seed(RANDOM_SEED + e)\n        trn_idx = np.random.permutation(len(train_X))\n        #print('trn_idx[:1000].sum()', trn_idx[:10000].sum())\n        X = train_X[trn_idx]\n        y = train_y[trn_idx]\n        \n        model.fit(X, y, batch_size=512, epochs=1, validation_data=(val_X, val_y), callbacks = callback, verbose=0, shuffle = False)\n        pred_val_y = model.predict([val_X], batch_size=1024, verbose=0)\n        \n        best_score = 0.0\n        best_score = metrics.f1_score(val_y, (pred_val_y > 0.33).astype(int))\n        print(\"Epoch: \", e, \"-    Val F1 Score: {:.4f}\".format(best_score))\n\n    pred_test_y = model.predict([test_X], batch_size=1024, verbose=0)\n    print('='*100)\n    return pred_val_y, pred_test_y, best_score","0adea834":"train_X, test_X, train_y, word_index = load_and_prec()\nembedding_matrix = load_glove(word_index)","7c7fcfb1":"epochs = 5","9a793af4":"%%time\nset_random_seed(RANDOM_SEED)\n\nsplits = list(StratifiedKFold(n_splits=4, shuffle=True, random_state=RANDOM_SEED).split(train_X, train_y))\nfor idx, (train_idx, valid_idx) in list(enumerate(splits))[:1]:\n        X_train = train_X[train_idx]\n        y_train = train_y[train_idx]\n        X_val = train_X[valid_idx]\n        y_val = train_y[valid_idx]\n        model = get_model(embedding_matrix)\n        \n        pred_val_y, pred_test_y, best_score = train_pred(model, X_train, y_train, X_val, y_val, epochs = epochs)\n        \n        for l in model.layers:\n            for w in l.get_weights():\n                print(w.shape, w.sum())","52bec4b8":"set_random_seed(RANDOM_SEED)\n\nsplits = list(StratifiedKFold(n_splits=4, shuffle=True, random_state=RANDOM_SEED).split(train_X, train_y))\nfor idx, (train_idx, valid_idx) in list(enumerate(splits))[:1]:\n        X_train = train_X[train_idx]\n        y_train = train_y[train_idx]\n        X_val = train_X[valid_idx]\n        y_val = train_y[valid_idx]\n        model = get_model(embedding_matrix)\n        \n        pred_val_y, pred_test_y, best_score = train_pred(model, X_train, y_train, X_val, y_val, epochs = epochs)\n        \n        for l in model.layers:\n            for w in l.get_weights():\n                print(w.shape, w.sum())","68a6c350":"**Load embeddings**","d08861cd":"# This should output the same weights if everything is deterministic. \n# This works fine for me on local kernels even after adding more depth\/epochs\/Cudnn etc\n\nThis actually just ran deterministically for the first time I have seen in many many tries. Right before this and I was going to publish it. Good thing I checked.","d4dc7350":"This is a bare bones kernel that shows that even a very simple NN pipeline is non deterministic on kernels. \n\nThis is without any Cudnn usage so that is not the reason\n\n Once everything is seeded properly I am unable to get non deterministic behaviour locally. So it must be something in the kernel environment.\n \n Even Cudnn behaves deterministically in my local environment.  Would love if others can verify running this kernel locally on a GPU.\n \n Things to keep in mind when you try to make your code deterministic. Cudnn has 2 initializers that need to be seeded. Also .fit has shuffle by default so I turn that off and shuffle manually. Also dropouts take a seed value. With all these things I can get deterministic behaviour (locally) even when running a loop to try hyper parameter changes.\n\n For a bit I thought it was the versioning since I thought I had non deterministic behaviour locally on 2.2.4 but I have not been able to duplicate that so seems to be a deadend. At this point I am throwing in the towel. Maybe someone else has some more ideas on the cause and remedy. At least we should stop blamming Cudnn.\n\n The biggest issue with this is that they presumablly will only be running these one time for final submissions. With this much variance I think it will put alot of luck in play.**\n","19f8bef0":"**Train and predict**","dae3bad8":"**Load packages and data**","ec56435b":"**Main part: load, train, pred and blend**","145bf25a":"**LSTM models**"}}