{"cell_type":{"ec97e0a7":"code","a29a036a":"code","4a55c9e5":"code","f667ab3c":"code","34e3309f":"code","0d120b8d":"code","d5a9c017":"code","e9141d2e":"code","75e30321":"code","f96af957":"code","893a84ea":"code","eab78e5f":"code","b990e08a":"code","12e12d19":"code","21fe2dbd":"code","ca809031":"code","d8379d47":"code","c243430c":"code","d9a8e7c3":"code","dc76c0fe":"code","9bb876a0":"code","3af6dd4f":"code","be9a1bc5":"code","6d7f19ca":"code","738ec59c":"code","cac28c56":"code","e8f6fbe0":"code","57551285":"code","e00a8280":"code","304fb289":"code","949ffefb":"code","d757167a":"code","d943feab":"code","44336aad":"code","f37a6712":"code","7ef9f98e":"code","0a4bb705":"code","d1057cec":"code","d1c1704e":"code","f6fe031d":"code","316786b7":"code","4876f13c":"code","26c25e7c":"code","0823ff6d":"code","2786ddce":"code","6f04eb5d":"code","fdbc3de7":"code","bdfd5c0a":"code","6f5479b7":"code","3b693c33":"code","faf0e725":"code","0960e869":"code","0e8f6a22":"code","3750f206":"code","96650feb":"code","f65cf2e2":"code","08ef4d26":"code","ccea560e":"code","97b3b24c":"code","8ed8f351":"code","15be8092":"code","80bc2f60":"code","90f90f11":"markdown","278125bc":"markdown","62b59647":"markdown","e9fd5a87":"markdown","6c852e12":"markdown","d839e132":"markdown","2e1efa17":"markdown","5ac02311":"markdown","efbec5e5":"markdown","f49aba28":"markdown","7f38d659":"markdown","e9c92bae":"markdown","166d9953":"markdown","c82b4b27":"markdown","b439078c":"markdown","45ffadaa":"markdown","1a5ba543":"markdown","4f71dc30":"markdown","97b8a9dc":"markdown","c235996c":"markdown","e7676f40":"markdown","5d95fdcb":"markdown","076f45aa":"markdown","ec861d59":"markdown","c471096f":"markdown","86b8ed18":"markdown","ac499b68":"markdown","105e72b9":"markdown","72b88176":"markdown","50735251":"markdown","0919c4f4":"markdown","4584b6a0":"markdown","fa94f88f":"markdown","c569eb8e":"markdown","fb9eb586":"markdown","37262008":"markdown","8f65993e":"markdown","c7ff7d78":"markdown","c4a9b71a":"markdown","d45d97a1":"markdown","984d6698":"markdown","2d113c64":"markdown","c97eafa4":"markdown","b75a9f7e":"markdown","e868f0ae":"markdown","aca7252b":"markdown","06bfc03d":"markdown","845942d8":"markdown"},"source":{"ec97e0a7":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","a29a036a":"dataset = pd.read_csv('..\/input\/clicked-on-add\/advertising.csv')","4a55c9e5":"dataset.head()","f667ab3c":"dataset.keys()","34e3309f":"dataset.head()","0d120b8d":"# removing spaces in column names...since it will get easier during model deployment..\ndataset.columns = dataset.columns.str.replace(' ', '_')","d5a9c017":"dataset.keys()","e9141d2e":"# checking datatype in each column.\n\ndataset.info()","75e30321":"# Numerical coumn insights.\n\ndataset.describe().transpose()","f96af957":"# checking for null values..\n\ndataset.isna().sum()","893a84ea":"# cross checking for null values..Making a list of missing value types\n\nmissing_values = [\"n\/a\", \"na\", \"--\",\" \"]\ndf = pd.read_csv('..\/input\/clicked-on-add\/advertising.csv', na_values = missing_values)","eab78e5f":"df.isna().sum()","b990e08a":"dataset.shape","12e12d19":"# seaborn pairplot\n\nsns.pairplot(data = dataset, hue = 'Clicked_on_Ad')\nplt.show()","21fe2dbd":"# Checking columnswise data visualization.\n# taking non object datatype column..\n\ndf1 = dataset.select_dtypes(exclude=[np.object])\nfor i, col in enumerate(df1.columns):\n    sns.set_style('whitegrid')\n    plt.figure(i)\n    fig, ax =plt.subplots(figsize=(10,5))\n    sns.set(font_scale = 1.2)\n    sns.kdeplot(df1[col], shade=True)\n    plt.show()","ca809031":"# correlation matrix.\n\nprint(sns.heatmap(dataset.corr()<-0.8, annot=True))","d8379d47":"print(sns.heatmap(dataset.corr()<-0.8, annot=True))","c243430c":"# Checking for total no. of unique values in object datatypes..\n\ndf2 = dataset.select_dtypes(include=[np.object])\nfor i in df2.columns:\n    unique_value = df2[i].nunique()\n    print(\"total unique values in '{}' : {}\".format(i, unique_value))","d9a8e7c3":" # We will drop two columns from main dataset..1) Ad Topic Line & 2) Timestamp. Since this information is not helpful.\n    \ndataset1 = dataset.drop(['Ad_Topic_Line','Timestamp'], axis=1)","dc76c0fe":"dataset1.head()","9bb876a0":"# Converting categorical datatyps...\n\ndataset1[\"Country\"] = dataset1.Country.astype('category').cat.codes\ndataset1[\"City\"] = dataset1.City.astype('category').cat.codes","3af6dd4f":"dataset1.head()","be9a1bc5":"x = dataset1.drop(['Clicked_on_Ad'], axis=1)","6d7f19ca":"y = dataset1.loc[:,['Clicked_on_Ad']]","738ec59c":"x.head()","cac28c56":"y.head()","e8f6fbe0":"# We will use recursive feature elimination technique(RFE)\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression()\nrfe = RFE(log_reg,7)\nrfe_fit = rfe.fit(x,y)","57551285":"rfe_fit.ranking_","e00a8280":"len(rfe_fit.ranking_)  # which equals to no. of columns in x dataset","304fb289":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.8, test_size = 0.2, random_state =15)\n\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, train_size = 0.9, test_size = 0.1, random_state = 15)\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(x_valid.shape)\nprint()\nprint(y_train.shape)\nprint(y_test.shape)\nprint(y_valid.shape)","949ffefb":"list1 = [y_train, y_test, y_valid]\nm=1\nfor i in list1:\n    plt.title(m)\n    sns.countplot(x='Clicked_on_Ad', data = i)\n    plt.show()\n    m +=1","d757167a":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\n\nparameters = [{'solver':['newton-cg','lbfgs','liblinear','sag','saga']},{'random_state':[10]},\n              {'C':[[i for i in np.geomspace(1e-3, 1e1, num=20)]]}]","d943feab":"# using GridSearchCV\n\nfrom sklearn.model_selection import GridSearchCV\n\n# help(GridSearchCV)\ngrid_search = GridSearchCV(estimator= model, param_grid= parameters, scoring='accuracy', n_jobs= -1)\n\ngrid_search = grid_search.fit(x_train, y_train)","44336aad":"accuracy = grid_search.best_score_\naccuracy","f37a6712":"estimator = grid_search.best_estimator_\nestimator","7ef9f98e":"grid_search.best_params_","0a4bb705":"from sklearn.linear_model import LogisticRegression\n\n# creating instance.. \nmodel1 = LogisticRegression(penalty='l2', solver = 'newton-cg', random_state=10)\n\n# fitting model..\nmodel1.fit(x_train, y_train)\n\n# predicting results for y variable using dependent variable set x_train\ny_pred = model1.predict(x_train)","d1057cec":"# Generating predict values in probabilites % format..\n# first value is correspondance to '0' & second value is with '1'\n\npredict_proba1 = model1.predict_proba(x_train).round(2)\npredict_proba1","d1c1704e":"# getting beta coefficients or beta weights.\n\nmodel1.coef_","f6fe031d":"# Accuracy on Train\n\nprint(\"The Training Accuracy is: \", model1.score(x_train, y_train))\n\n# Accuracy on Test\nprint(\"The Testing Accuracy is: \", model1.score(x_test, y_test))","316786b7":"# classification report..\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_train, y_pred))","4876f13c":"# confusion matrix..\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_train, y_pred)\ncm","26c25e7c":"# writing function to plot confusion matrix:\n\ndef plot_confusion_matrix(cm, classes=None, title='Confusion matrix'):\n    \n    \"\"\"Plots a confusion matrix.\"\"\"\n    if classes is not None:\n        sns.heatmap(cm, cmap=\"YlGnBu\", xticklabels=classes, yticklabels=classes, vmin=0., vmax=1., \n                    annot=True, annot_kws={'size':30})\n    else:\n        sns.heatmap(cm, vmin=0., vmax=1.)\n    plt.title(title)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","0823ff6d":"# plotting confusion-matrix % value\n\ncm = confusion_matrix(y_train, y_pred)\ncm_norm = cm \/ cm.sum(axis=1)\n\nplot_confusion_matrix(cm_norm, classes = model1.classes_, title='Confusion matrix')","2786ddce":"\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import plot_precision_recall_curve\nimport matplotlib.pyplot as plt\n\n# for training datasets.. since we used x_train to find y_pred.\ndisp = plot_precision_recall_curve(model1, x_train, y_train)\ndisp.ax_.set_title('Precision\/recall tradeoff (training dataset)')\nplt.show()\n\n# for testing datasets.. since we used x_train to find y_pred.\ndisp = plot_precision_recall_curve(model1, x_test, y_test)\ndisp.ax_.set_title('Precision\/recall tradeoff (testing dataset)')\nplt.show()","6f04eb5d":"# easy way to plot\n\n# from sklearn.metrics import plot_roc_curve\n# disp1 = plot_roc_curve(model1, x_train, y_train)\n# plt.show()","fdbc3de7":"# writing function to reuse..key \ndef roc(y_model,y_predicted):\n    from sklearn.metrics import roc_curve, roc_auc_score\n    fpr, tpr, thresholds = roc_curve(y_model,y_predicted)\n    auc = roc_auc_score(y_model,y_predicted)\n    plt.figure(figsize=(8,6))\n    plt.plot(fpr, tpr, linewidth=2, label='Logistic Regression(area = %0.2f)'%auc)\n    plt.plot([0, 1], [0, 1], \"k--\")\n    plt.axis([0, 1, 0, 1])\n    plt.legend(loc='lower right')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(label= 'Roc Curve')","bdfd5c0a":"print('roc curve with training dataset')\nroc(y_train, y_pred)","6f5479b7":"y_predict_t = model1.predict(x_test)\nroc(y_test, y_predict_t)","3b693c33":"from sklearn.metrics import log_loss\n\n# Running Log loss on training\n# predict_proba1 = model1.predict_proba(x_train)  # we have already done this step no.5\nprint(\"The Log Loss on Training is: \", log_loss(y_train, predict_proba1))\n\n# Running Log loss on testing set\npred_proba_t = model1.predict_proba(x_test)\nprint(\"The Log Loss on Testing Dataset is: \", log_loss(y_test, pred_proba_t))","faf0e725":"# we will find out best c value which will give min. log loss & better accuracy..\n\nC_list = np.geomspace(1e-5, 1e2, num=20) #  log space\n\nfrom sklearn.linear_model import LogisticRegressionCV \nmodel2 = LogisticRegressionCV(random_state=10, solver='newton-cg', Cs= C_list)\nmodel2.fit(x_train, y_train)\n\nprint(\"The accuaracy is:\", model2.score(x_test, y_test))\npred_proba_t = model2.predict_proba(x_test)\nlog_loss2 = log_loss(y_test, pred_proba_t)\nprint(\"The Logistic Loss is: \", log_loss2)\n\nprint(\"The optimal C parameter is: \", model2.C_)","0960e869":"from sklearn.dummy import DummyClassifier\n\n# class sklearn.dummy.DummyClassifier(*, strategy='warn', random_state=None, constant=None) ...fyi only\n\nstrategies = ['stratified','most_frequent','prior', 'uniform', 'constant']\ntest_score = []\n\nfor s in strategies:\n    if s == 'constant':\n        dummy_clf = DummyClassifier(strategy=s, random_state=15, constant = 'break')\n    else:\n        dummy_clf = DummyClassifier(strategy=s, random_state=15)\n        dummy_clf.fit(x_train, y_train)\n        score = dummy_clf.score(x_test, y_test)\n        test_score.append(score)\n\n        pred_proba_t = dummy_clf.predict_proba(x_test)\n        log_loss2 = log_loss(y_test, pred_proba_t)\n        \n        print(\"when strategy is '{}'\".format(s))\n        print(\"the Testing Acc:\", score)\n        print(\"the Log Loss:\", log_loss2)\n        print('-'*30)","0e8f6a22":"dummy_score = pd.DataFrame(list(zip(strategies, test_score)), columns =['strategies', 'test_score'])","3750f206":"plt.figure(figsize=(8,6))\nax = sns.stripplot(x=\"strategies\", y=\"test_score\", linewidth=3,data=dummy_score)","96650feb":"from sklearn.linear_model import LogisticRegression\n\nfinal_model = LogisticRegression(penalty='l2', solver = 'newton-cg', random_state=10, C=0.00162378)\nfinal_model.fit(x_train, y_train)\nfinal_predict = final_model.predict(x_valid)\n\nscore = final_model.score(x_valid, y_valid)\n\npred_proba_t3 = final_model.predict_proba(x_valid)\nlog_loss3 = log_loss(y_valid, pred_proba_t3)\n\nprint(\"Testing Acc:\", score)\nprint(\"Log Loss:\", log_loss3)","f65cf2e2":"final_model","08ef4d26":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_valid, final_predict)","ccea560e":"# we have created function to take user input values..Copy-paste this code in streamlit python file.\n\ndef inputs(Daily_Time_Spent_on_Site, Age, Area_Income,Daily_Internet_Usage, City, Male, Country):\n    \n    new_data=pd.DataFrame({'Daily_Time_Spent_on_Site':Daily_Time_Spent_on_Site,\"Age\":Age,\"Area_Income\":Area_Income, \"Daily_Internet_Usage\":Daily_Internet_Usage,\n                           \"City\":City, \"Male\":Male, \"Country\":Country},index=[1])\n    \n    new_data[[\" Daily_Time_Spent_on_Site\",\"Area_Income\",\"Daily_Internet_Usage\"]] = df[[\" Daily_Time_Spent_on_Site\",\"Area_Income\",\"Daily_Internet_Usage\"]].astype('float')\n    \n    new_data[[\"Age\",\"City\",\"Male\",\"Country\"]] = df[[\"Age\",\"City\",\"Male\",\"Country\"]].astype('int')\n    final_predict = final_model.predict(new_data)\n    return(final_predict).values","97b3b24c":"# categorical codes & label dictonary to use in deployment\n\n# for city column.\n\nlabels1 = dataset['City'].astype('category').cat.categories.tolist()\ncity_codes = {k: v for k,v in zip(labels1,list(range(1,len(labels1)+1)))}\n\n# checking how dict looks like \nout1 = dict(list(city_codes.items())[0: 3])\nprint(\"dict looks like: \" + str(out1))\n\n# for country column..\n\nlabels2 = dataset['Country'].astype('category').cat.categories.tolist()\ncountry_codes = {k: v for k,v in zip(labels2,list(range(1,len(labels2)+1)))}\n\n# checking how dict looks like \nout2 = dict(list(country_codes .items())[0: 3])\nprint(\"dict looks like: \" + str(out2))","8ed8f351":"# creating country city dataframe to sort further..\ncountry_filter = pd.DataFrame({'Country':dataset.Country, 'Cities': dataset.City})","15be8092":"# creating dict of country + city..which includes Country as key and Cities in that country as values\n#.....ignore city names not matching to the real city names in the world..since it is practice purpose dataset..not real data\n\ncountry_city_dict = country_filter.groupby(['Country'])['Cities'].apply(lambda grp: list(grp.value_counts().index)).to_dict()","80bc2f60":"import pickle\n\n# Pickling fianl_predict \nfilename = 'fianl_model.p'    # provide file name\noutfile = open(filename,'wb') #creating empty file..it will saved as model_predict.py in same working directory\npickle.dump(final_model,outfile) # dumping pickled object in file created.  \noutfile.close()\n\n\n# Pickling unique value dictionary..\ndict1 = open('country_codes1.p', 'wb') \npickle.dump(country_codes, dict1)                      \ndict1.close() \n\ndict2 = open('city_codes1.p', 'wb') \npickle.dump(city_codes, dict2)                      \ndict2.close() \n\ndict3 = open('country_city_dict.p', 'wb')\npickle.dump(country_city_dict, dict3)","90f90f11":"## 1. Import required libraries.","278125bc":" note: correlation matrix also one of the technique to selct features.","62b59647":"Since there are mno. of categorical fvalues..so generating insights for logistic regression by considering huge no. of categorical features will not helpful.","e9fd5a87":" ### 1. Checking for null values \/ shape \/ info.","6c852e12":" #### Use ROC curve whenever the negative class is rare or when you care more about the false negatives than the false positives","d839e132":" ### 3. Separating depedent & independent variables..","2e1efa17":" ### Classifiaction report","5ac02311":" ### 2. Exploratory Data Analysis.","efbec5e5":"Instead of plotting precision versus recall, the ROC curve plots the true positive rate (another name for recall) against the false positive rate. The false positive rate (FPR) is the ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the true negative rate, which is the ratio of negative instances that are correctly classified as negative.\n\n\nThe TNR is also called specificity. Hence the ROC curve plots sensitivity (recall) versus 1 - specificity.","f49aba28":" ### Confusion Matrix.","7f38d659":"## Check Here my GitHUb for detail codes & deployment files:","e9c92bae":"\n #### Useful when data is imbalanced in nature.\n #### Use PR curve whenever the positive class is rare or when you care more about the false positives than the false negatives\n \n Increasing in precision reduces recall and vice versa.\n \n Precision-Recall is a useful measure of success of prediction when the classes are very imbalanced. In information retrieval, precision is a measure of result relevancy, while recall is a measure of how many truly relevant results are returned.\n***\n\nThe precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision relates to a low false positive rate, and high recall relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).\n***\n\nA system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the training labels. A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels. An ideal system with high precision and high recall will return many results, with all results labeled correctly.\n***","166d9953":"# Step 9: Model Deployment","c82b4b27":"that is, it shows all columns are importatnt features..","b439078c":" ### Performance Measurement\n \n     it includes..precsion, recall, confusion matrix, f1 score, PR Curve, ROC curve etc. Except curve you will get all in classification report or ou may check theorotical formulaes for more.","45ffadaa":"both correlation heatmap shows there is no multicollinearity in numerical datatype indepedendent variables.","1a5ba543":"# Step 5: Select & train the model","4f71dc30":"\nSince we are executing supervised machine learning algorithm we must split our dataset.","97b8a9dc":" ### 2. checking all splitted data is balnced or imbalanced","c235996c":" #### We have found out solver, C value. We can use them and build our final model.","e7676f40":" #### Note: if dummy classifier accuracy is < model accuracy. Then our model is on right track\n\nWe see that the accuracy was almost 100%. A cross validation strategy is recommended for a better estimate of the accuracy, if it is not too CPU costly. For more information see the Cross-validation: evaluating estimator performance section. Moreover if you want to optimize over the parameter space, it is highly recommended to use an appropriate methodology; see the Tuning the hyper-parameters of an estimator section for details.\n\nMore generally, when the accuracy of a classifier is too close to random, it probably means that something went wrong: features are not helpful, a hyperparameter is not correctly tuned, the classifier is suffering from class imbalance, etc\u2026","5d95fdcb":"## 2. Load Dataset","076f45aa":"# logistic regression notebook\n\n This is the basic\/complete logistic regression notebook. Performed all basic steps. From data loading to model deployment.\n ***\n### Best logistic regression Examples using python..\n\n<a href=\"https:\/\/github.com\/ShrikantUppin\/2_logistic-regression-notebook\/blob\/main\/clicked%20on%20Ad%20.ipynb\/\" target=\"_blank\">1. User Clicked on Advertise or not?<\/a>\n***\n### Model-app Deployment link:\n\n<a href=\"https:\/\/clicked-on-ad-logistic-regress.herokuapp.com\/\" target=\"_blank\">1.https:\/\/clicked-on-ad-logistic-regress.herokuapp.com<\/a>\n\n![](https:\/\/github.com\/ShrikantUppin\/Logistic-Regression-Complete-Notebook\/blob\/main\/streamlit.png?raw=true)","ec861d59":"# Step1: Probelm Formulation.","c471096f":" ###  Features information\n \n#### Independent Variables:\n ***\n 1. Daily Time Spent on Site.: consumer time on site in minutes\n 2. Age.: cutomer age in years\n 3. Area Income.: AVG .Area income (COUNTRYWISE) in INR\n 4. Daily Internet Usage: Avg. minutes a day consumer is on the internet\n 5. Ad Topic Line: Headline of the advertisement\n 6. City: City of consumer\n 7. Male: Consumer male or not?\n 8. Country: Country of consumer\n 9. Timestamp: Time at which consumer clicked on Ad or closed window.\n\n#### Dependet Variable:\n ***\n 10. Clicked on Ad: 0 or 1 indicated clicking on Ad","86b8ed18":" ### 1 . Grid Search for Hyperparameter Tuning","ac499b68":"plot shows dummy classifier accuracy with diff strategies. which much lesser than our logistic regression model accuracy (0.98). Hence we have correctly hypertuned parameters.","105e72b9":" ### Log-loss","72b88176":" ### I) Precison vs Recall curve.","50735251":" ### 1. Splitting dataset into train, test, valid sets.","0919c4f4":"# Step 4: Split the raw data","4584b6a0":"So  GridSearchCV has given solver: newton-cg","fa94f88f":" ### Problem Statement: Determine wether particular internet user clicked on advertise or not.","c569eb8e":"# Complete Logistic Regression Model\n***","fb9eb586":"# Step 6: Model Evaluation","37262008":" ### II) Receiver Operating Characteristic (ROC) Curve.","8f65993e":"# Step 8: Fianl Model","c7ff7d78":" ### Dummy Classifiers: Estimator score method: quantifying the quality of predictions \n \n this is only for checking how our model deviates with dummy classifiers that overfit or underfit.\n before making final model always prefer to test on dummy classifiers.\n \n the reason behind using dummy classifier , just compare model accuracy and dummy classifier accuracy. If model accuracy is   much greater than dummy classifier then our model said to be good.\n \n### click here fore more info:\nhttps:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#dummy-estimators","c4a9b71a":"* We will loop over parameter C (Inverse of regularization strength).\n* We will loop over parameter C (Inverse of regularization strength).\n* Inverse of regularization strength helps to avoid overfitting - it penalizes large values of your parameters\n* It also helps to find Global Minimum by moving to better \"solutions\" from local minimum to global minimum\n* The values of C to search should be n-equally-spaced values in log space ranging from 1e-5 to 1e2\n\nNote: GirdSerchCV also be used as hypertuning parameter. But here we are going to find best value for 'C'","d45d97a1":"* We will make separate python file for deployment code.\n* From this jupyter notebook, will pickle model3 & use it in deployment file.","984d6698":"#  Step 3: Data preprocessing","2d113c64":" ### What is need to be solve?\n \n * we want to find out whether or not a particular internet user clicked on an Advertisement. \n \n \n ### What you are going to do?\n * We will try to create a logistic regresssion model that will predict whether or not they will click on an ad based off the features of that user.","c97eafa4":" ### Accuracy","b75a9f7e":"# Step2: Gather the Raw-Data & understand features clearly","e868f0ae":" ### 2. Categorical data handling.","aca7252b":"# Step 7: Parameter Tuning","06bfc03d":" ### 4.  Feature Selection","845942d8":" ### 2. Implementing Logistic regressiion Model"}}