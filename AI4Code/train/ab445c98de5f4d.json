{"cell_type":{"d7e6ab4b":"code","e7632758":"code","56ec7e4b":"code","b798a9e1":"code","b731fff0":"code","64d718ba":"code","035376c3":"code","74ed0a14":"code","ab637de2":"code","5d303b31":"code","f82aacc8":"code","bd9ffc54":"code","b93cd29d":"code","73e456fa":"code","eac42dd1":"code","0920196b":"code","5dcda712":"code","21e20992":"code","2371d9ff":"code","3620e06d":"code","1be7b6f1":"code","667e1b2c":"code","7f4b076d":"markdown","a9d077aa":"markdown"},"source":{"d7e6ab4b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e7632758":"import pandas as pd\nimport numpy as np\nimport sklearn\nfrom sklearn import linear_model\nfrom sklearn import neighbors\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt  \nimport seaborn as seabornInstance \n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_digits\nimport seaborn as sns\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error","56ec7e4b":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","b798a9e1":"train_complete = train_data.fillna(train_data.mean())\n\ntest_complete = test_data.fillna(test_data.mean())","b731fff0":"le = preprocessing.LabelEncoder()\ntrain_complete[\"Sex\"] = le.fit_transform(train_complete[\"Sex\"])\nle = preprocessing.LabelEncoder()\ntest_complete[\"Sex\"] = le.fit_transform(test_complete[\"Sex\"])","64d718ba":"train_complete = train_complete.replace('S', value = 0)\ntrain_complete = train_complete.replace('C', value = 1)\ntrain_complete = train_complete.replace('Q', value = 2)\ntrain_complete['Embarked'] = train_complete['Embarked'].fillna(3)\n\ntest_complete = test_complete.replace('S', value = 0)\ntest_complete = test_complete.replace('C', value = 1)\ntest_complete = test_complete.replace('Q', value = 2)\ntest_complete['Embarked'] = test_complete['Embarked'].fillna(3)","035376c3":"train_features = ['Pclass', 'Sex', 'SibSp', 'Fare', 'Age','Parch','Embarked',]\nX0 = train_complete[train_features]\nX1 = X0.iloc[:594]\nX2 = X0.iloc[594:]\n\nXTest = test_complete[train_features]","74ed0a14":"train_survived = ['Survived']\ny0 = train_complete[train_survived]\ny1 = y0.iloc[:594]\ny2 = y0.iloc[594:]\n","ab637de2":"corr = X0.corr()\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)\nfig.colorbar(cax)\nticks = np.arange(0,len(X0.columns),1)\nax.set_xticks(ticks)\nplt.xticks(rotation=90)\nax.set_yticks(ticks)\nax.set_xticklabels(X0.columns)\nax.set_yticklabels(X0.columns)\nplt.show()","5d303b31":"import matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X0,y0)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X0.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","f82aacc8":"Titanic_model = RandomForestClassifier(n_estimators=1200, max_depth=10, random_state= 1)\nTitanic_model.fit(X0, y0.values.ravel())\n\n\nTest_pred = (Titanic_model.predict(XTest))","bd9ffc54":"print(Test_pred)","b93cd29d":"#mean_squared_error(y2, Test_pred)","73e456fa":"#mean_absolute_error(y2, Test_pred)","eac42dd1":"#from pprint import pprint\n#print('Parameters currently in use:\\n')\n#pprint(Titanic_model.get_params())","0920196b":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)","5dcda712":"#rf_random = RandomizedSearchCV(estimator = Titanic_model, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n#rf_random.fit(X1, y1)","21e20992":"#rf_random.best_params_","2371d9ff":"#pred_round = [round(num) for num in Test_pred]\n#pred_int = [int(num) for num in pred_round]\n#print (pred_int)","3620e06d":"#mean_absolute_error(y2, pred_round)","1be7b6f1":"#from sklearn.metrics import classification_report, confusion_matrix\n#print(confusion_matrix(y2, pred_round))\n#print(classification_report(y2, pred_round))","667e1b2c":"output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': Test_pred})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","7f4b076d":"X0 = whole dataset\nX1 = train data\nX2 = val data","a9d077aa":"y0 = whole survive data\ny1 = train survive\ny2 = val survive"}}