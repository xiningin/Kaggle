{"cell_type":{"d9e55680":"code","800c4a27":"code","c42b2be5":"code","2691298a":"code","cd86c3d5":"code","aeff88b5":"code","7b8cd765":"code","3e0536ea":"code","f57f5dd1":"code","46f593f7":"code","ed2687cd":"code","8c2882ed":"code","8e97fe62":"code","3580165d":"markdown"},"source":{"d9e55680":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","800c4a27":"colnames=['class', 'Alcohol', 'Malic_acid', 'Ash', 'Alcalinity_of_ash', 'Magnesium', 'Total_phenols', 'Flavanoids', 'Nonflavanoid_phenols', 'Proanthocyanins', 'Color_intensity', 'Hue', 'OD280\/OD315', 'Proline']\ndataset = pd.read_csv(\"..\/input\/Wine.csv\",names=colnames, header=None)","c42b2be5":"dataset.head()","2691298a":"dataset.shape","cd86c3d5":"dataset['class'].value_counts()","aeff88b5":"# Univariate graphs to see the distribution\ndataset.hist(figsize=(20, 15))\nplt.show()","7b8cd765":"# Correlation Matrix\nplt.subplots(figsize=(20, 15))\nsns.heatmap(dataset.drop('class', axis=1).corr(), annot=True)","3e0536ea":"X = dataset.iloc[:,1:]\ny = dataset.iloc[:,0]\n\n# Feature Scaling\nfrom  sklearn.preprocessing  import StandardScaler\nslc= StandardScaler()\nX = slc.fit_transform(X)\n\n# Spliting data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","f57f5dd1":"# Test options and evaluation metric\nnum_folds = 10\nseed = 0\nscoring = 'accuracy'","46f593f7":"from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.model_selection import KFold, cross_val_score\n\n# Spot-Check Algorithms (Classification)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Spot-Check Ensemble Models (Classification)\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier\nfrom xgboost.sklearn import XGBClassifier\n\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('SVM', SVC()))\n\nmodels.append(('AB', AdaBoostClassifier()))\nmodels.append(('GBM', GradientBoostingClassifier()))\nmodels.append(('ET', ExtraTreesClassifier()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('XGB',XGBClassifier()))\n\n# evaluate each model in turn\nresults = {}\naccuracy = {}\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n    results[name] = (cv_results.mean(), cv_results.std())\n    model.fit(X_train, y_train)\n    _ = model.predict(X_test)\n    accuracy[name] = accuracy_score(y_test, _)","ed2687cd":"results","8c2882ed":"accuracy","8e97fe62":"# Finalizing the model and comparing the test, predict results\n\nmodel = SVC(random_state=seed)\n\nmodel.fit(X_train, y_train) \ny_predict = model.predict(X_test)\n\nprint(classification_report(y_test, y_predict))\n\ncm= confusion_matrix(y_test, y_predict)\nsns.heatmap(cm, annot=True)","3580165d":"**SVC and Logistic Regression predicts with 100% accuracy**"}}