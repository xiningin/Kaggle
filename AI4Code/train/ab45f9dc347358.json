{"cell_type":{"9fb682bb":"code","dc0c087b":"code","59345bf7":"code","335fb8f3":"code","15edac69":"code","cad52936":"code","a1364317":"code","82a876d7":"code","95a0083e":"code","017c59ab":"code","67657bc9":"code","0af22be7":"code","c759cefb":"code","7dac5d7c":"code","7d257e3f":"code","93faa99e":"code","fc4b9b51":"code","d628bdfe":"code","a01cb192":"code","745d3e4c":"code","60b3e946":"code","b0b942e3":"code","f971b7df":"code","8ae28f32":"code","786c6adb":"code","da15ca23":"code","4bc117bd":"code","2428b6e2":"code","6f0ab543":"code","d4fe6a03":"code","a390772d":"code","ef22e4a4":"code","22e343ac":"code","8ab5a3f3":"code","bb312056":"code","dcbda419":"code","99b9c8c6":"code","4f55f09b":"code","16e3fe11":"code","7cd2eca7":"code","9021c5a5":"code","aff495b1":"code","8c96d90c":"markdown","a4688e8c":"markdown","890344dd":"markdown","b65604f9":"markdown","00b7dc54":"markdown","3214bcf9":"markdown","d498dd95":"markdown","7a8b80d9":"markdown","c5b21aa2":"markdown","9c281096":"markdown","5f8f96c6":"markdown","3279fd6c":"markdown","5a5ecb50":"markdown","0388a681":"markdown","0f806524":"markdown","1ea8fbf5":"markdown","e0d23c97":"markdown","1efe3a8c":"markdown","8f4faaad":"markdown","0b0d3fc8":"markdown","5a78ba63":"markdown","d3171ef6":"markdown","58964b03":"markdown","82571d9f":"markdown","b8ea65aa":"markdown","c9287c98":"markdown","b783cd82":"markdown","6031e47a":"markdown","d3ab3770":"markdown","1ec8af67":"markdown","3c4d328a":"markdown"},"source":{"9fb682bb":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import *\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.naive_bayes import GaussianNB\n# !pip install ppscore\n# import ppscore as pps\nimport statsmodels.api as sm\nfrom scipy.stats import probplot\nfrom sklearn import metrics               \nfrom sklearn import preprocessing\nfrom sklearn import utils\nfrom sklearn.metrics import mean_absolute_error\n%matplotlib inline\n!pip install PyCustom\nimport PyCustom\nfrom statistics import mean, stdev\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score, roc_curve, auc, confusion_matrix, plot_confusion_matrix\nfrom sklearn.model_selection import KFold, cross_val_score, cross_validate, StratifiedKFold\nfrom sklearn.svm import SVC \nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import SelectKBest\n\n# Avoids scroll-in-the-scroll in the entire Notebook\nfrom IPython.display import Javascript\ndef resize_colab_cell():\n  display(Javascript('google.colab.output.setIframeHeight(0, true, {maxHeight: 5000})'))\nget_ipython().events.register('pre_run_cell', resize_colab_cell)","dc0c087b":"df = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/akhil14shukla\/IME672A-Course-Project\/master\/hmeq.csv\")","59345bf7":"(df.head())","335fb8f3":"print(df.shape)\n(df.info())","15edac69":"(df.describe())","cad52936":"sns.set(style=\"whitegrid\")\n# plt.style.use('white')\nplt.rcParams.update({\"grid.linewidth\":0.5, \"grid.alpha\":0.5})\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)","a1364317":"# Heatmap for null\/missing values\nsns.heatmap(df.isnull(),cbar=False,yticklabels=False,cmap = 'viridis')","82a876d7":"print(df[\"REASON\"].value_counts())\nprint(df[\"JOB\"].value_counts())","95a0083e":"# We can fill the missing values with the mode, i.e. \"Other\", or we can fill the missing values depending on the distribution of the non-null values. \ndf[\"REASON\"].fillna(\"Other reason\",inplace=True)\ndf[\"JOB\"].fillna(df[\"JOB\"].mode()[0],inplace=True)\ndf[\"DEROG\"].fillna(value=0,inplace=True)        # Filling the missing value with the mode\ndf[\"DELINQ\"].fillna(value=0,inplace=True)       # Filling the missing value with the mode\n# print(df[\"JOB\"].isna().sum())","017c59ab":"# label_encoder = preprocessing.LabelEncoder()\n# df['JOB']= label_encoder.fit_transform(df['JOB'])\n# df['REASON']= label_encoder.fit_transform(df['REASON'])\ndf = df.join(pd.get_dummies(df[\"JOB\"]))\ndf = df.join(pd.get_dummies(df[\"REASON\"]))\ndf.drop([\"JOB\",\"REASON\"],axis=1,inplace=True)","67657bc9":"fig, axes = plt.subplots(5, 4, figsize=(25, 20),)\nfor i,ax in zip(df.columns,axes.flat):\n    sns.boxplot(data=df, x=i,ax=ax)     # we can also use violin plot \nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.show()","0af22be7":"fig, axes = plt.subplots(5, 4, figsize=(25, 20))\nfor i,ax in zip(df.columns,axes.flat):\n    sns.histplot(data=df,x=i,ax=ax,kde=True)\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.show()\n","c759cefb":"# sns.pairplot(df, hue=\"BAD\")","7dac5d7c":"fig, axes = plt.subplots(5, 4, figsize=(25, 20))\nfor i,ax in zip(df.columns,axes.flat):\n    probplot(df[i],dist='norm',plot=ax)\n    ax.set_title(i)\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.show()\n","7d257e3f":"from scipy.stats import yeojohnson\nfig, axes = plt.subplots(5, 4, figsize=(25, 20))\nfor i,ax in zip(df.columns,axes.flat):\n    probplot(np.power(df[i],1\/8),dist='norm',plot=ax)\n    ax.set_title(i)\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.show()","93faa99e":"# df = df_temp.copy()\n# df_temp = df_temp.join(pd.DataFrame({\"LOAN\":yeojohnson(df[\"LOAN\"])[0].reshape(-1)}))\n# df_temp\n# # pd.DataFrame({\"LOAN\":yeojohnson(df[\"LOAN\"])[0].reshape(-1)})\n","fc4b9b51":"plt.figure(figsize=(20,20))\nheat_map = sns.heatmap(df.corr(),cmap = colormap,annot=True)\nheat_map.set_yticklabels(heat_map.get_yticklabels(), rotation=0)\n# sns.color_palette(\"mako\", as_cmap=True)\nplt.show()","d628bdfe":"plt.figure(figsize=(25,25))\nimport PyCustom as pc\nmatrix_df = PyCustom.pps(df,[\"BAD\",\"JOB\",\"REASON\"])[['Feature', 'Target', 'PPS']].pivot(columns='Feature', index='Target', values='PPS')\nsns.heatmap(matrix_df, vmin=0, vmax=1, cmap=colormap, linewidths=0.5,annot=True)\n# plt.show()","a01cb192":"df[\"PROBINC\"] = df.MORTDUE\/df.DEBTINC # adding new feature, (current debt on mortgage)\/(debt to income ratio)","745d3e4c":"df_orig = df.copy()","60b3e946":"df_temp = df.copy()\ndf_temp[\"LOAN\"] = yeojohnson(df[\"LOAN\"])[0]          # transforming LOAN using yeo-johnson method\ndf_1 = df_temp.copy()\ndf_temp[\"MORTDUE\"] = np.power(df[\"MORTDUE\"],1\/8)     # transforming MORTDUE by raising it to 1\/8\ndf_temp[\"YOJ\"] = np.log(df[\"YOJ\"]+10) \ndf_temp[\"VALUE\"] = np.log(df[\"VALUE\"]+10) \ndf_temp[\"CLNO\"] = np.log(df[\"CLNO\"]+10) \ndf_2 = df_temp.copy()","b0b942e3":"# print(\"Ratio of Number of defaulters\/ Number of non-defaulters : \", df[df.BAD==1].sum().sum()\/df[df.BAD==0].sum().sum())","f971b7df":"df_3 = df_2.copy()","8ae28f32":"df_2.drop([\"HomeImp\", \"Other reason\",\"Sales\", \"Self\"],axis=1,inplace=True)","786c6adb":"df.name = 'df : original with one added feature (PROBINC)'\ndf_1.name = 'df_1 : transformed just LOAN feature'\ndf_2.name = 'df_2 : transformed LOAN, and other features too, and dropped other features (MORTDUE, YOJ), which seemed unimportant from the visualisation section'\ndf_3.name = 'df_3 :  contains transformed features, LOAN and others (MORTDUE,YOJ, etc.)'","da15ca23":"def preprocess(df):\n  df1 = df.copy()\n  # Numerosity Reduction, with a threshold of 4 null values\n  df1.dropna(axis=0,thresh=4,inplace=True)\n  # Filling the rest of the null values using interpolated values, mode and median\n  df1.fillna(value=df1.interpolate(),inplace=True)\n  df1.fillna(value=df1.mode(),inplace=True)\n  df1.fillna(value=0,inplace=True)\n  # Taking out the target column before using #standard scaler\n  y = df[\"BAD\"]\n  df1.drop([\"BAD\"],axis=1,inplace=True)\n\n  # Using Standard Scaler, as it might also take care of some outliers\n  sscaler = StandardScaler()\n  scaled_features = sscaler.fit_transform(df1)\n\n  # Standard Scaler retuen a numpy array, convertig it back into a DataFrame, for ease of understanding\n  scaled_features_df = pd.DataFrame(scaled_features, index=df1.index, columns=df1.columns)\n  return scaled_features_df,y\n\ndef preprocess_min_max(df):\n  df1 = df.copy()\n  # Numerosity Reduction, with a threshold of 4 null values\n  df1.dropna(axis=0,thresh=4,inplace=True)\n  # Filling the rest of the null values using interpolated values, mode and median\n  df1.fillna(value=df1.interpolate(),inplace=True)\n  df1.fillna(value=df1.mode(),inplace=True)\n  df1.fillna(value=0,inplace=True)\n  # Taking out the target column before using #standard scaler\n  y = df[\"BAD\"]\n  df1.drop([\"BAD\"],axis=1,inplace=True)\n\n  # Using Standard Scaler, as it might also take care of some outliers\n  sscaler = MinMaxScaler()\n  scaled_features = sscaler.fit_transform(df1)\n\n  # Standard Scaler retuen a numpy array, convertig it back into a DataFrame, for ease of understanding\n  scaled_features_df = pd.DataFrame(scaled_features, index=df1.index, columns=df1.columns)\n  return scaled_features_df,y","4bc117bd":"x1,y1 = preprocess_min_max(df)\nchi_scores = chi2(x1,y1)  \np_values = pd.Series(chi_scores[1],index = x1.columns)\np_values.sort_values(ascending = False , inplace = True)\np_values.plot.bar()","2428b6e2":"def remove_outlier_IQR(df):\n    Q1=df.quantile(0.25)\n    Q3=df.quantile(0.75)\n    IQR=Q3-Q1\n    df_final = df[~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]\n    return df_final\n\ndf3=remove_outlier_IQR(df)\nprint(\"Number of outliers removed : \" , df.shape[0]-df3.shape[0])","6f0ab543":"fig, axes = plt.subplots(5, 4, figsize=(15, 10),)\nfor i,ax in zip(df3.columns,axes.flat):\n    sns.boxplot(x = df3[i],ax=ax)     # we can also use violin plot \nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.show()","d4fe6a03":"pca = PCA(n_components=6)\npca_df,pca_y = preprocess(df_2)\nprincipalComponents = pca.fit_transform(pca_df)\nprincipalDf = pd.DataFrame(data = principalComponents)\nprint(pca.explained_variance_)","a390772d":"\n# kfold_model = LogisticRegression()\n# kf = StratifiedKFold(n_splits=5 ,shuffle=True, random_state=1)\n# kf_scores1 = []\n# kf_scores2 = []\n# xmat1 = X1_rfe.values\n# xmat2 = X2_rfe.values\n# ymat1 = y1.values\n# ymat2 = y2.values\n# for train_index, test_index in kf.split(xmat1, ymat1):\n#     X_train, y_train = xmat1[train_index] , ymat1[train_index]\n#     kfold_model.fit(X_train,y_train)\n#     y_pre = kfold_model.predict(xmat1[test_index])\n#     kf_scores1.append(accuracy_score(ymat1[test_index], y_pre))\n# print(mean(kf_scores1))\n\n# for train_index, test_index in kf.split(xmat2, ymat2):\n#     X_train, y_train = xmat2[train_index] , ymat2[train_index]\n#     kfold_model.fit(X_train,y_train)\n#     y_pre = kfold_model.predict(xmat2[test_index])\n#     kf_scores2.append(accuracy_score(ymat2[test_index], y_pre))\n\n# print(mean(kf_scores2))","ef22e4a4":"# defining plot_roc function function\n\ndef plot_roc(y_test, y_score):\n    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n\n    plt.figure()\n    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % auc(fpr, tpr))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(\"ROC plot for loan defaulter prediction\")\n    plt.legend(loc=\"lower right\")\n    plt.show()","22e343ac":"def plotconfusionmatrix(y, y_pred):\n    cf_matrix = confusion_matrix(y, y_pred)\n    group_names = ['True Neg','False Pos','False Neg','True Pos']\n    group_counts = [\"{0:0.0f}\".format(value) for value in\n                    cf_matrix.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in cf_matrix.flatten()\/np.sum(cf_matrix)]\n    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n    plt.show()","8ab5a3f3":"def model_analysis(model,x_test, y_true,y_pred):\n    plot_roc(y_true, y_pred)\n    plotconfusionmatrix(y_true, y_pred)\n    print(classification_report(y_test, y_pred))","bb312056":"def build_and_analysis(df,models):\n  scaled_features_df,y = preprocess(df)\n  x_train, x_test, y_train, y_test = train_test_split(scaled_features_df,y)\n  for i in models:\n    model = i()\n    model.fit(x_train,y_train)\n    print(str(type(model)).split(\".\")[-1][:-2])\n    print(\"Accuracy on Training Dataset : \",model.score(x_train,y_train))\n    print(\"Accuracy on CV Dataset : \",model.score(x_test,y_test))\n    model_analysis(model,x_test,y_test,model.predict(x_test))\n    print(\"\\n \\n \\n \\n\")","dcbda419":"# Dividing the dataset into training and cross-validation\n# y = df[\"BAD\"]\n# df.drop([\"BAD\"],axis=1,inplace=True)\nscaled_features_df,y = preprocess(df_2)\nx_train, x_test, y_train, y_test = train_test_split(scaled_features_df,y)","99b9c8c6":"build_and_analysis(df_2,[LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, SVC, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier])","4f55f09b":"def show_values_on_bars(axs, h_v=\"v\", space=0.4):\n    def _show_on_single_plot(ax):\n        if h_v == \"v\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() \/ 2\n                _y = p.get_y() + p.get_height()\n                value = int(p.get_height())\n                ax.text(_x, _y, value, ha=\"center\") \n        elif h_v == \"h\":\n            for p in ax.patches:\n                _x = p.get_x() + p.get_width() + float(space)\n                _y = p.get_y() + p.get_height()\n                value = int(p.get_width())\n                ax.text(_x, _y, value, ha=\"left\")\n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)\n\ndef analysis(df,y):\n  x_train,x_test, y_train,y_test = train_test_split(df,y)\n  classifiers = [\n      KNeighborsClassifier(3),\n      SVC(probability=True),\n      DecisionTreeClassifier(),\n      RandomForestClassifier(),\n      AdaBoostClassifier(),\n      GradientBoostingClassifier(),\n      GaussianNB(),\n      LinearDiscriminantAnalysis(),\n      QuadraticDiscriminantAnalysis(),\n      LogisticRegression()\n  ]\n\n  log_cols = [\"Classifier\", \"Accuracy\"]\n  log = pd.DataFrame(columns=log_cols)\n\n  acc_dict = {}\n\n  for clf in classifiers:\n      name = clf.__class__.__name__\n      clf.fit(x_train, y_train)\n      train_predictions = clf.predict(x_test)\n      acc = accuracy_score(y_test, train_predictions)\n\n      if name in acc_dict:\n          acc_dict[name] += acc\n      else:\n          acc_dict[name] = acc\n\n  for clf in acc_dict:\n      acc_dict[clf] = acc_dict[clf]\n      log_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns = log_cols)\n      log = log.append(log_entry)\n\n  plt.xlabel('Accuracy')\n  plt.title('Classifier Accuracy')\n\n  sns.set_color_codes(\"muted\")\n  barplots = sns.barplot(x = 'Accuracy', y = 'Classifier', data = log, color = \"b\")\n  for p in barplots.patches:\n    barplots.annotate(\"%.4f\" % p.get_width(), xy=(p.get_width(), p.get_y()+p.get_height()\/2),\n            xytext=(5, 0), textcoords='offset points', ha=\"left\", va=\"center\")\n  plt.show()\n  print(acc_dict)","16e3fe11":"for i in [df, df_1,df_2,df_3]:\n  x1,y1 = preprocess(i)\n  print(\"For dataset \", i.name)\n  analysis(x1,y1)\n  print()\n  print()","7cd2eca7":"x1,y1 = preprocess_min_max(df)\ndf_new = pd.DataFrame(SelectKBest(chi2, k=8).fit_transform(x1,y1))","9021c5a5":"x_train, x_test, y_train, y_test = train_test_split(df_new,y1)","aff495b1":"lr = LogisticRegression()       # tuning hyperparameters : penalty=\"l1\",solver=\"liblinear\",max_iter=100,C=0.004\nlr.fit(x_train,y_train)\nprint(\"Accuracy on Training Dataset : \",lr.score(x_train,y_train))\nprint(\"Accuracy on CV Dataset : \",lr.score(x_test,y_test))\nmodel_analysis(lr,x_test,y_test,lr.predict(x_test))","8c96d90c":"We can consider dropping Self and Other Reason as there PPS score is 0 for every other column.","a4688e8c":"Testing the models on cross-validation dataset, and comparing with training dataset","890344dd":"Taking csv data as input","b65604f9":"Importing the required libraries","00b7dc54":"Most of the data is already numerical, only two are of strings type (_REASON_ and _JOB_).","3214bcf9":"Comparing various transformaton, to observe their effect on performance","d498dd95":"## **Please Upvote if you liked the notebook**","7a8b80d9":"Building the Model","c5b21aa2":"Chi-Square Test for ","9c281096":"_REASON_ and _JOB_ are categorical attributes and of string type. Some plots work only with numerical values, so we need to convert them to numericals. Using Label Encoding on these two variables to convert them to numerical values. (We can also use One-Hot-Encoding)","5f8f96c6":"Plotting Histograms to see the distribution of each attribute","3279fd6c":"Yeo-johnson transformation\n* y = ((x + 1)**lmbda - 1) \/ lmbda,                for x >= 0, lmbda != 0\n* log(x + 1),                                  for x >= 0, lmbda = 0\n* ((-x + 1)**(2 - lmbda) - 1) \/ (2 - lmbda),  for x < 0, lmbda != 2\n* log(-x + 1),                                for x < 0, lmbda = 2","5a5ecb50":"All the attributes are of numerical type,now, we can use plots to understand the distribution of all the the attributes.\n<br><br>Plotting Boxplots for each attribute in df","0388a681":"\n\n```\n# This is formatted as code\n```\n\nTransforming a basic data transformation(x to the power 1\/8) technique to try converting the current distribution to near normal distribution","0f806524":"Plotting Heatmap of correlation Matrix, to understand the type of linear relation between attributes.<br>\nWe will again plot Heatmap of correlation after cleaning and transforming the attributes.<br>\n","1ea8fbf5":"The model result was not that good after using PCA with 6 Principal Components","e0d23c97":"We can drop MORTDUE or VALUE, as they are highly correlated and they are PPS score is also greater than 0.5\nDropping MORTDUE, because VALUE is a better predictor of BAD","1efe3a8c":"Improving visuals of plots","8f4faaad":"Chi-square future selection on Logistic Regression","0b0d3fc8":"* df -> original with one added feature\n* df_1 -> transformed just _LOAN_ feature\n* df_2 -> transformed LOAN, and other features too, and dropped other features (MORTDUE, YOJ), which seemed unimportant from the visualisation section\n* df_3 -> contains transformed features, LOAN and others (MORTDUE,YOJ, etc.)","5a78ba63":"**Our objective is to minimize company loss, predicting the risk of client default, a good recall rate is desirable because we want to identify the maximum amount of clients that are indeed prone to stop paying their debts, thus, we are pursuing a small number of False Negatives.**","d3171ef6":"## Data Cleaning and Transformation","58964b03":"Observations from the above plots:\n* The **scale of each attribute is different**, we need to normalize all the features. \n* Some attributes have **skewed distribution**\n* Some attributes have a lot of **outliers** (DEBTINC, LOAN, MORTDUE, VALUE)\n[\/\/]: # (Hello)\nFor normalizing, we can use Min-Max Scaler, but attributes like LOAN, MORTDUE, VALUE have a lot of outliers (from boxplot), so we will also try Z-Score Normalization (preferred).\n<br>\nFor fixing the skewness, we need to transform the attributes. Our basic transformation did improve the distribution of some attributes like _LOAN_, _MORTDUE_ and _VALUE_.","82571d9f":"**We need to fill these two variable's null values and One-Hot-Encode or Label Encode them to plot their distribution efficiently.<br><br>**\nReason\/Meaning of null values in REASON and JOB, and how we will fill these:<br><br>\n\nWe can replace the missing value with \n* a new value _\"Unknown\"_ \n* values based on the distribution of non-missing values\n* the most frequent value  \n* the predicted value using other attributes and Decision Tree.\n<br><br>\n[\/\/]: # (Hello)\nREASON - This shows the reason why the person is taking the loan. There are two available values : Debt consolidation and Home Improvement. The missing value must be showing that the Reason of taking the loan was not either of the two given optins and hence was left empty. So, **filling the missing values with a new value _\"Other reason\"_, for this attribute**.<br><br>\n\n\nJOB - This tells the occupation of the applicant. There are 6 unique values for this attribute, and the value _\"Other\"_ is the most frequent. The _\"Other\"_ value is already present, so it rules out the reasoning used above. Out of the remaining options, we can **replace the missing values with the most frequent value for simplicity**<br><br>\n\nDEROG - Around 4700 have value _0_. So, **replacing the missing values with the mode** (i.e. 0) \n<br><br>\nDELINQ - Around 5200 have same value (_0_). **Replacing the missing with the mode value**. ","b8ea65aa":"## Understanding the Data\n\n \n**Initial understanding of data i.e types of variables present, missing values and their distribution**","c9287c98":"NUmber of outliers removed are too large, this decreases the data quality.","b783cd82":"# Objective:\nPredict whether a person will default on loan or not, based on the given attributes.\n## Meta Data-\nThe data set HMEQ reports characteristics and delinquency information for 5,960 home equity loans. A home equity loan is a loan where the obligor uses the equity of his or her home as the underlying collateral. The data set has the following characteristics:\n\n\u25a0 BAD: 1 = applicant defaulted on loan or seriously delinquent; 0= applicant paid loan (Target Variable){Assymetric Binnary Nominal}\n\n\u25a0 LOAN: Amount of the loan request{Ratio-Scaled Numeric}\n\n\u25a0 MORTDUE: Amount due on existing mortgage{Ratio-Scaled Numeric}\n\n\u25a0 VALUE: Value of current property{Ratio-Scaled Numeric}\n\n\u25a0 REASON: DebtCon = debt consolidation; Homelmp home improvement{Nominal}\n\n\u25a0 JOB: Occupational categories{Nominal}\n\n\u25a0 YOJ: Years at present job\n\n\u25a0 DEROG: Number of major derogatory reports\n\n\u25a0 DELINQ: Number of delinquent credit lines\n\n\u25a0 CLAGE: Age of oldest credit line in months\n\n\u25a0 NINQ: Number of recent credit inquiries\n\n\u25a0 CLNO: Number of credit lines\n\n\u25a0 DEBTINC: Debt-to-income ratio{Ratio-Scaled Numeric}","6031e47a":"## Explaining the data<br>\nFeatures of the Data:","d3ab3770":"Analysis of Transformations:","1ec8af67":"**Numerosity Reduction** : Apart from the above needed steps, many tuple\/observations might have many missing values in their attributes. We can consider dropping them to improve the data quality. For this we need to decide a threshold value, such that the data quality is also improved and a lot of data isn't lost.<br>\n**Feature Reduction** - Dropping columns with same value for most of the observations (_DELINQ_ and _DEROG_), and after considering their Correlation and Predictive Power Score(_REASON_ and _JOB_). ","3c4d328a":"PCA can be performed for both the numeric continous variables and the categorical and numeric discrete variables but it is most effective and designed for numeric continous variables."}}