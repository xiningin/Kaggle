{"cell_type":{"659f0f55":"code","5ca6ecbd":"code","10bbf681":"code","0a038c43":"code","2d908ea4":"code","ee15a7e0":"code","19e1f7f7":"code","2821f6ce":"code","14310274":"code","aa0ba98e":"code","0dedfbdf":"code","d375bb29":"code","7d4cecd8":"code","716deb59":"code","cddfbe89":"code","21b538c8":"code","ad16ab07":"code","33a0f675":"code","73d71ebe":"code","092ef5e4":"code","b48c6e94":"code","7f980858":"code","23e6e14f":"code","eaa83188":"code","4d993fa5":"code","87d4791e":"code","34a28172":"code","1fc0eb47":"code","457b8682":"code","5a7edd28":"code","27dee18e":"code","e2c5e7b0":"code","0bfb2e79":"code","6d589a8d":"code","d6032077":"code","0054e1a2":"code","e713a80f":"code","22e90304":"code","d01fbb78":"code","05d30621":"code","e38248a7":"code","6511be4f":"code","e7953b1e":"code","f1f9fe03":"code","4d42af2a":"code","15dcd0cb":"code","3a5eb55b":"code","52f90fd3":"code","b379bd9c":"code","d3f37860":"code","f1e27855":"code","2cf893d7":"code","94a345de":"code","e4568165":"code","2dcededa":"code","38cd3d36":"code","4b5a6dbc":"code","d1cdffa4":"code","c91aac9c":"code","7d65b591":"code","9066358e":"code","72ab7f13":"markdown","db73204e":"markdown","57daaa6f":"markdown","f0466b11":"markdown","53d94d49":"markdown","51256ff5":"markdown","ba21c18d":"markdown","a4a11d07":"markdown","d40b6747":"markdown","022527c9":"markdown","35b5a502":"markdown","92fe96b7":"markdown","b78dfa89":"markdown","16e4e9ae":"markdown"},"source":{"659f0f55":"import os\nimport numpy as np\nimport pandas as pd","5ca6ecbd":"df=pd.read_csv('\/kaggle\/input\/customerdata\/CustomerData.csv')\ndf.head()","10bbf681":"df.shape # 14 features are present based on dimension","0a038c43":"df.info()","2d908ea4":"numerical_data=df.drop(['CustomerID','City','FavoriteChannelOfTransaction','FavoriteGame'],axis=1)\ncategorical_data=df[['City','FavoriteChannelOfTransaction','FavoriteGame']]","ee15a7e0":"numerical_data.head()","19e1f7f7":"categorical_data.head()","2821f6ce":"#Distribution in data #Variance\nnumerical_data.var().plot(kind='bar',logy=True)","14310274":"from sklearn.preprocessing import MinMaxScaler\nsc=MinMaxScaler()","aa0ba98e":"pd.DataFrame(sc.fit_transform(numerical_data),columns=numerical_data.columns).var().plot(kind='bar')","0dedfbdf":"categorical_data.head()","d375bb29":"set(categorical_data.City)","7d4cecd8":"categorical_data.City.value_counts(normalize=True).plot(kind='pie')","716deb59":"categorical_data.FavoriteChannelOfTransaction.value_counts(normalize=True)#.plot(kind='pie')","cddfbe89":"print(categorical_data.FavoriteGame.value_counts(normalize=True))\ncategorical_data.FavoriteGame.value_counts(normalize=True).plot(kind='pie')","21b538c8":"import seaborn as sns\nsns.pairplot(numerical_data)","ad16ab07":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,6))\ncorr=numerical_data.corr()\n#mask\nmask=np.triu(np.ones_like(corr))\nsns.heatmap(corr,annot=True,mask=mask,cmap='YlGnBu')","33a0f675":"df.head()","73d71ebe":"sns.catplot(x='City',y='TotalRevenueGenerated',data=df,ci=99,kind='point',aspect=2)","092ef5e4":"sns.catplot(x='FavoriteChannelOfTransaction',y='TotalRevenueGenerated',data=df,ci=99,kind='point',aspect=2)","b48c6e94":"sns.catplot(x='FavoriteGame',y='TotalRevenueGenerated',data=df,ci=99,kind='point',aspect=2)","7f980858":"sns.catplot(x='NoOfChildren',y='TotalRevenueGenerated',data=df,ci=99,kind='point',aspect=2)","23e6e14f":"sns.catplot(x='FrquncyOfPurchase',y='TotalRevenueGenerated',data=df,ci=99,kind='point',aspect=2)","eaa83188":"df.head()","4d993fa5":"df.drop(['CustomerID'],axis=1,inplace=True)","87d4791e":"df.head()","34a28172":"df_new=df[df.MinAgeOfChild <100]\ndf_new=df_new[df_new.MaxAgeOfChild <100]","1fc0eb47":"df_new.head()","457b8682":"df_new[df_new.MinAgeOfChild >100]\ndf_new[df_new.MaxAgeOfChild >100]","5a7edd28":"#split data into dependent and Independent \nX=df_new.iloc[:,:-1]\ny=df_new.iloc[:,-1:]","27dee18e":"X.head()","e2c5e7b0":"y.head()","0bfb2e79":"x_num=X.drop(['City','FavoriteChannelOfTransaction','FavoriteGame'],axis=1)\nx_cat=X[['City','FavoriteChannelOfTransaction','FavoriteGame']]","6d589a8d":"#for categorical, we need to create n-1 dummies\nx_cat=pd.get_dummies(data=x_cat,drop_first=True)\nx_cat.info()","d6032077":"x_cat.head()","0054e1a2":"from sklearn.preprocessing import StandardScaler # z-score\nscx = StandardScaler()\nscy = StandardScaler()","e713a80f":"x_num_norm = pd.DataFrame(scx.fit_transform(x_num.values),columns=x_num.columns)","22e90304":"x_cat.reset_index(inplace=True) #because index is different from x_num_norm","d01fbb78":"x_norm =pd.concat([x_num_norm,x_cat],axis=1)","05d30621":"y_norm = pd.DataFrame(scy.fit_transform(y.values),columns=y.columns)","e38248a7":"x_norm.isnull().sum()","6511be4f":"import pickle","e7953b1e":"os.mkdir('preprocessing')","f1f9fe03":"pickle.dump(x_norm,open('preprocessing\/x_norm.pickle','wb'))\npickle.dump(y_norm,open('preprocessing\/y_norm.pickle','wb'))","4d42af2a":"# loading preprocessed data\nx = pickle.load(open('preprocessing\/x_norm.pickle','rb'))\ny = pickle.load(open('preprocessing\/y_norm.pickle','rb'))","15dcd0cb":"x.drop('index',axis=1,inplace=True) # drop index","3a5eb55b":"# convert into array\nx = x.values\ny = y.values","52f90fd3":"x.shape , y.shape","b379bd9c":"from tensorflow.keras import Sequential\nfrom tensorflow.keras import layers","d3f37860":"import tensorflow as tf","f1e27855":"# multiple linear regression\ndef multi_regression():\n    model = Sequential([layers.Dense(units=1,input_shape=(13,))])\n    loss = tf.keras.losses.mean_squared_error\n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n    # complie\n    model.compile(loss=loss,optimizer=optimizer,metrics=['mse'])\n    return model","2cf893d7":"model = multi_regression()\nmodel.summary()","94a345de":"# split the data into train and test\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)","e4568165":"x_train.shape,x_test.shape,y_train.shape,y_test.shape","2dcededa":"# training multiple regression\nhistory = model.fit(x_train,y_train,batch_size=100,epochs=200,validation_data=(x_test,y_test))","38cd3d36":"hist = history.history\ndf_hist = pd.DataFrame(hist)","4b5a6dbc":"# visualizing loss\ndf_hist[['loss','val_loss']].plot()","d1cdffa4":"from tensorflow.keras.regularizers import l1","c91aac9c":"def lasso_regression(penality):\n    model = Sequential([\n                      layers.Dense(units=1,input_shape=(13,),kernel_regularizer=l1(penality))\n  ])\n  # loss and optimizer\n    loss = tf.keras.losses.mean_squared_error\n    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n  # compile\n    model.compile(optimizer=optimizer,loss=loss)\n    return model","7d65b591":"# different penality factors\npenality = [1e-10,3e-10,6e-10,9e-10]\n#             [1e-9,3e-9,6e-9,9e-9,\n#             1e-8,3e-8,6e-8,9e-8,\n#             1e-7,3e-7,6e-7,9e-7,\n#             1e-6,3e-6,6e-6,9e-6,\n#             1e-5,3e-5,6e-5,9e-5,\n#             1e-4,3e-4,6e-4,9e-4,\n#             1e-3,3e-3,6e-3,9e-3,\n#             1e-2,3e-2,6e-2,9e-2,\n#             1e-1,3e-1,6e-1,1,3,6,10]","9066358e":"# training model for each penality factor\nloss_values = [] \nfor lam in penality:\n    print('Running the model for lambda = %f'%lam)\n  # initilizing model with lambda\n    model_lasso = lasso_regression(lam)\n  # fitting the model\n    history = model_lasso.fit(x_train,y_train,batch_size=100,epochs=500,validation_data=(x_test,y_test))\n  # visualize loss for different iteration or epochs\n    df_history = pd.DataFrame(history.history)\n    df_history[['loss','val_loss']].plot()\n    plt.show()\n  #  save the weight \n    weights = model_lasso.get_weights() # coloumn \n    try:\n        weight_penality = np.concatenate((weight_penality,weights[0].T),axis=0)\n    except:\n        weight_penality = weights[0].T # rows\n  # loss\n    loss_values.append(df_history.iloc[-1].to_dict())","72ab7f13":"# Advanced Linear Regression","db73204e":"# *Problem Statement:*\nfor child, Toy company which sells educational tablets and gaming system both online and offline wants to analyse customer data","57daaa6f":"Observations:\nMulticolinearity \n* FrequncyOfPurchase,NoOfUnitsPurchased,NoOfGamesBought,FrequencyOFPlay,NoOfGamesPlayed.\n* we need to remove some  multicolinearity between the independent variables\n* FrequncyOfPurchase,NoOfUnitsPurchased has high correlation with Total Revenue Generated","f0466b11":"Obeservation:\nTenure has large variance and MinAgeofChild has low variance","53d94d49":"### **Multiple Linear Regression**\n$\\hat y = a + b_1 X_1 + b_2 X_2 + ... + b_n X_n $","51256ff5":"# Conculsion:\n1. CustomerID is unique key and has to drop it.\n1. City,FavoriteChannelOfTransaction,FavoriteGame are categorical variables for that we need to create  n\u22121  dummies.\n1. Tenure has highest variance whereas MinAgeOfChild has least variance\n1. From pairplot MinAgeofChild and MaxAgeofChild has outlier.\n1. There is MultiColinearity effect between the variables.\n\n* FreqOfPurchase & NoOfUnitsPurchased\n* FreqOfPurchase & NoOfGamesBought\n* NoOfUnitsPurchased & NoOfGamesBought\n* FreqOfPlay & NoOfGamesPlay\n1. FactorAnalysis:\n* Categorical data\n  City,FavoriateChannelTransaction has good correlation wrt Total Revenue Generated\n* Numerical data\n FreqOfPurchased, NoOfUnitsPurchased, NoOfGamesBought has good correlation wrt Total Revenue Generated","ba21c18d":"# *Observations*\n1. CustomerID- Nominal(where ranking doesn't make sense) and unique\n1. City,FavoriteChannelOfTransaction,FavoriteGame - Categorical information\n1. Rest are numerical information","a4a11d07":"# **Lasso Regression**","d40b6747":"# *Common question in Data Analysis*\n1. Dimensions\n1. Features and Datatypes\n1. Categorical and Numerical\n1. Distribution in data\n1. Correlation between the features \n1. Factor Analysis\n","022527c9":"# Data Normalization","35b5a502":"# Factor Analysis","92fe96b7":"# Correlations between the Features","b78dfa89":"# > Data Preprocessing","16e4e9ae":"# *Data Analysis*"}}