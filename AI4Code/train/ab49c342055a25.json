{"cell_type":{"51979765":"code","844fe06f":"code","d7eb7010":"code","1dfaa8e5":"code","75d73cae":"code","0e8c1fd8":"code","f6995f6f":"code","9e0d04bc":"code","1c93d04c":"code","2d42b6f4":"code","89c6d210":"code","b1da41d9":"markdown","060ac6af":"markdown","5a6211df":"markdown","95b1ed88":"markdown","f78f8498":"markdown"},"source":{"51979765":"# Data Wrangling\nimport pandas as pd\nfrom pandas import Series, DataFrame\nimport numpy as np\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pylab as plt\nfrom matplotlib import font_manager, rc\n\n# Preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import FeatureUnion\n\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Modeling\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.ensemble import VotingClassifier\nfrom vecstack import stacking\nfrom scipy.stats.mstats import gmean\n\n# Evaluation\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.model_selection import LeaveOneOut\nfrom sklearn.model_selection import ShuffleSplit\n\n# Utility\nimport os\nimport time\nimport random\nimport warnings; warnings.filterwarnings(\"ignore\")\nfrom IPython.display import Image\nimport pickle\nfrom itertools import combinations\nimport gc\nfrom tqdm import tqdm\nimport platform\nimport datetime\n\n# For DNN modeling\nimport tensorflow as tf\nimport keras\nfrom keras import backend as K\nfrom keras.layers import * #Input, Dense\nfrom keras.models import * #Model\nfrom keras.optimizers import *\nfrom keras.initializers import *\nfrom keras.regularizers import *\nfrom keras.utils.np_utils import *\nfrom keras.utils.vis_utils import * #model_to_dot\nfrom keras.callbacks import EarlyStopping","844fe06f":"tr_train = pd.read_csv('..\/input\/kml2020\/X_train.csv', encoding='cp949')\ntr_test = pd.read_csv('..\/input\/kml2020\/X_test.csv', encoding='cp949')\ny_train = pd.read_csv('..\/input\/kml2020\/y_train.csv').gender\nIDtest = tr_test.cust_id.unique()\n\ntr_train.head()","d7eb7010":"def make_goods_dict(df_train, df_test):\n    \n    data = pd.concat([df_train,df_test], axis = 0)\n    goods_list = np.array(df_train.query('gds_grp_nm != \"\uc0c1\ud488\uad70\ubbf8\uc9c0\uc815\"')['goods_id'].unique())\n    temp = df_train.query('gds_grp_nm != \"\uc0c1\ud488\uad70\ubbf8\uc9c0\uc815\" and gds_grp_nm != \"\uae30\ud0c0\" and goods_id in @goods_list')\\\n    [['goods_id','gds_grp_nm','gds_grp_mclas_nm' ]].drop_duplicates()\n    \n    goods_dict = {}\n    for index in range(temp.shape[0]):\n        goods_dict[temp.iloc[index][0]] = [temp.iloc[index][1], temp.iloc[index][2]]\n        \n    return goods_dict","1dfaa8e5":"def goods_tag(df, goods_dict):\n    gds_grp_nm = []\n    gds_grp_mclas_nm = []\n    \n    for index in range(df.shape[0]):\n        if df.iloc[index]['gds_grp_nm'] == '\uc0c1\ud488\uad70\ubbf8\uc9c0\uc815' :\n            try : \n                key = df.iloc[index]['goods_id']\n                nm_mclas = goods_dict.get(key)\n                gds_grp_nm.append(nm_mclas[0])\n                gds_grp_mclas_nm.append(nm_mclas[1])\n            except:\n                gds_grp_nm.append(df.iloc[index]['gds_grp_nm'])\n                gds_grp_mclas_nm.append(df.iloc[index]['gds_grp_mclas_nm'])\n        else :\n            gds_grp_nm.append(df.iloc[index]['gds_grp_nm'])\n            gds_grp_mclas_nm.append(df.iloc[index]['gds_grp_mclas_nm'])\n            \n    df = pd.DataFrame({'gds_grp_nm' :gds_grp_nm , 'gds_grp_mclas_nm' : gds_grp_mclas_nm})\n    return df","75d73cae":"def replace_col(df, df_goods_map):\n    # \uc5f4 \ub300\uccb4\n    df = df.drop(['gds_grp_nm','gds_grp_mclas_nm'],axis = 1)\n    df = pd.concat([df, df_goods_map ], axis = 1)\n    df = df[['cust_id', 'tran_date', 'store_nm', 'goods_id','gds_grp_nm','gds_grp_mclas_nm', 'amount']]\n    return df","0e8c1fd8":"def goods_operator(df_train,df_test):\n    # \uc704\uc758 \ud568\uc218\ub4e4\uc744 \uc790\ub3d9\uc801\uc73c\ub85c \uc2e4\ud589\ud55c\ub2e4.\n    goods_dict = make_goods_dict(df_train, df_test)\n    df_train = replace_col(df_train, goods_tag(df_train, goods_dict))\n    df_test = replace_col(df_test, goods_tag(df_test, goods_dict))\n    return df_train, df_test","f6995f6f":"%%time\n\ntr_train_new, tr_test_new = goods_operator(tr_train, tr_test)","9e0d04bc":"features = ['goods_id', 'gds_grp_nm', 'gds_grp_mclas_nm']\n\ntr_all = pd.concat([tr_train_new, tr_test_new])\ntrain = []\ntest = []\n\nfor f in features:\n    for d,q in zip([train, test], ['cust_id not in @IDtest', 'cust_id in @IDtest']):\n        d.append(pd.pivot_table(tr_all, index='cust_id', columns=f, values='amount',\n                                aggfunc=lambda x: np.where(len(x) >=1, 1, 0), fill_value=0)                 \n                 .reset_index()\n                 .query(q)\n                 .drop(columns=['cust_id']).values)\n \ntrain, test = np.hstack(train),  np.hstack(test)","1c93d04c":"# Set hyper-parameters for power mean ensemble \nN = 20\np = 3.5\npreds = []\naucs = []\n\nfor i in tqdm(range(N)):    \n    X_train, X_test = train, test\n\n    ##### STEP 1: Randomize Seed\n    SEED = int(time.time() * 10000000 % 10000000) #np.random.randint(1, 10000)              \n    random.seed(SEED)       \n    np.random.seed(SEED)     \n    if tf.__version__[0] < '2':  \n        tf.set_random_seed(SEED)\n    else:\n        tf.random.set_seed(SEED)\n\n    ##### STEP 2: Build DAE #####\n    \n    # Define the encoder dimension\n    encoding_dim = 128\n\n    # Input Layer\n    input_dim = Input(shape = (X_train.shape[1], ))\n\n    # Encoder Layers\n    noise = Dropout(0.5)(input_dim) # for Denoising\n    encoded1 = Dense(512, activation = 'relu')(noise)\n    encoded2 = Dense(256, activation = 'relu')(encoded1)\n    encoded3 = Dense(128, activation = 'relu')(encoded2)\n    encoded4 = Dense(encoding_dim, activation = 'relu')(encoded3)\n\n    # Decoder Layers\n    decoded1 = Dense(128, activation = 'relu')(encoded4)\n    decoded2 = Dense(256, activation = 'relu')(decoded1)\n    decoded3 = Dense(512, activation = 'relu')(decoded2)\n    decoded4 = Dense(X_train.shape[1], activation = 'linear')(decoded3)\n\n    # Combine Encoder and Deocder layers\n    autoencoder = Model(inputs = input_dim, outputs = decoded4)\n\n    # Compile the model\n    autoencoder.compile(optimizer = 'adam', loss = 'mse')\n\n    # Train the model\n    history = autoencoder.fit(X_train, X_train, epochs=20, batch_size=64, \n                              shuffle=True, validation_data=(X_test,X_test), verbose=0)\n\n    print(f'DAE learning curve {i+1}\/{N}')\n    plt.plot(history.history[\"loss\"], label=\"train loss\")\n    plt.plot(history.history[\"val_loss\"], label=\"validation loss\")\n    plt.legend()\n    plt.title(\"Loss\")\n    plt.show()\n\n    ##### STEP 3: Reduce Dimension #####\n        \n    # Use a middle Bottleneck Layer to Reduce Dimension\n    model = Model(inputs=input_dim, outputs=encoded4)\n    X_train = model.predict(X_train)\n    X_test = model.predict(X_test)\n\n    ##### STEP 4: Build a DNN Model\n\n    # Define the Model architecture\n    max_features = X_train.shape[1]\n\n    # Define the Model architecture\n    model = Sequential()\n    model.add(Dense(32, activation='relu', input_shape=(max_features,), kernel_regularizer=l2(0.01)))   \n    model.add(Dropout(0.7))\n    model.add(Dense(16, activation='relu'))\n    model.add(Dropout(0.7))\n    model.add(Dense(1, activation='sigmoid'))\n\n    # Train the Model\n    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(curve='ROC', name='roc_auc')])\n    train_x, valid_x, train_y, valid_y = train_test_split(X_train, y_train, test_size=0.2)\n    history = model.fit(train_x, train_y, epochs=100, batch_size=256, \n                        validation_data=(valid_x,valid_y), callbacks=[tf.keras.callbacks.EarlyStopping(monitor= 'val_roc_auc', patience=15, mode='max')], verbose=0)\n\n    print(f'DNN learning curve {i+1}\/{N}')\n    plt.plot(history.history[\"loss\"], label=\"train loss\")\n    plt.plot(history.history[\"val_loss\"], label=\"validation loss\")\n    plt.legend()\n    plt.title(\"Loss\")\n    plt.show()\n    \n    # Make Prediction\n    auc = roc_auc_score(valid_y, model.predict(valid_x).flatten())\n    aucs.append(auc)\n    print('AUC', auc)\n    preds.append(model.predict(X_test).flatten())     ","2d42b6f4":"### Validate the Models\nprint('\\nValidation Summary:')\naucs = pd.Series(aucs)\nprint(aucs.sort_values(ascending=False))\nprint('mean={:.5f}, std={:.3f}'.format(aucs.mean(), aucs.std()))   ","89c6d210":"# Power mean ensemble\nTHRESHOLD = 0.78  # Use only models whose AUC exceeds this value\n\nsns.heatmap(pd.DataFrame(preds).filter(aucs[aucs.sort_values(ascending=False) > THRESHOLD].index.to_list(), axis=0).T.corr(), annot=True)\nplt.show()\n\npred = 0\nn = 0\nfor i in range(N):\n    if aucs.iloc[i] > THRESHOLD:\n        pred = pred + preds[i]**p \n        n += 1\npred = pred \/ n    \npred = pred**(1\/p)\n\n# Make a submission file\n\nsubmissions = pd.concat([pd.Series(IDtest, name=\"cust_id\"), pd.Series(pred, name=\"gender\")] ,axis=1)\nsubmissions.to_csv('submission_DAE(batchsize_256).csv', index=False)","b1da41d9":"<font color='tomato'><font color=\"#CC3D3D\"><p>\n# DNN with DAE    \n### This approach has the following characteristics:\n* No feature engineering\n* Applying BOW to raw transactions\n* Applying Denoising Autoencoder (DAE) to reduce the number of features\n* Creating multiple DNNs by changing random seeds and Ensembling the models with power mean    \n    \n<img align='left' src='https:\/\/img1.daumcdn.net\/thumb\/R720x0.q80\/?scode=mtistory2&fname=http%3A%2F%2Fcfile24.uf.tistory.com%2Fimage%2F992860505BDC99320A2AA2' width=600>      ","060ac6af":"### Ensemble Models & Make Submissions","5a6211df":"<font color='tomato'><font color=\"#CC3D3D\"><p>\n# End","95b1ed88":"### Make BOW-based Features\n","f78f8498":"### Build Models with DAE"}}