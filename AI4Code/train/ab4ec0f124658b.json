{"cell_type":{"78b01b8c":"code","5e0b218c":"code","b5a9b7fa":"code","7f09fe32":"code","b2812e3b":"code","7c4cda32":"code","dde2db87":"code","19747bf3":"code","79bbf6b4":"code","6141aaac":"code","67e624c7":"code","c1f105a3":"code","1e605e80":"code","ec5b427a":"code","4cbd07e2":"code","225ad0e2":"code","1722201a":"code","4099ff18":"code","eefa2f10":"code","b7eb6451":"code","eb4eaab4":"code","43bc31d6":"code","9894cc7c":"code","cfcbcb9a":"code","2ce0fb4f":"code","9f4e2474":"code","abb18cbc":"code","cb9ef15d":"code","880a11db":"code","d27333a7":"code","873baf30":"code","6d6945ae":"code","a219710a":"code","77ccdf7d":"code","a9b1fe4d":"code","13084cbe":"code","77d441fb":"code","bbf2c2aa":"code","112be40f":"code","ebc85603":"code","642502fd":"code","6dccea04":"code","1068ec1a":"code","ba4a53f5":"code","04129217":"code","03e85493":"code","fb056a3e":"code","bd728eb6":"code","d6136587":"code","0bb971af":"code","8ce1d74e":"code","49b9ebe9":"code","01ebea24":"code","a6828aae":"code","c0dedf50":"code","b554533b":"code","841df399":"code","31c664e1":"code","bf1aa030":"code","1d4af765":"code","5a817fa8":"code","62427e44":"code","758800b7":"code","7625971b":"code","197c44a7":"code","f7b222dc":"code","745f17e9":"code","74adc99f":"code","169119e4":"code","0e186a43":"code","f8f26cc3":"code","ff58e1b9":"code","0e5013fc":"code","d0b31dff":"code","a84a84c5":"code","e8aa5739":"code","28c815b1":"code","2d3a540a":"code","bb598be8":"code","d834c9fa":"code","5776cb40":"code","b44d5fa7":"code","d4874ea6":"code","ccbf6dd8":"code","ab121409":"markdown","6adc591b":"markdown","849df9c8":"markdown","79fc3fc6":"markdown","2effa189":"markdown","1b0d652e":"markdown","a5aa9251":"markdown","c2c19dd5":"markdown","1eacffb2":"markdown","038303c9":"markdown","4a17d258":"markdown","54f60f63":"markdown"},"source":{"78b01b8c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5e0b218c":"# First load data\n\ndf = pd.read_csv(\"\/kaggle\/input\/trip-advisor-hotel-reviews\/tripadvisor_hotel_reviews.csv\")","b5a9b7fa":"print(df.head()","7f09fe32":"train_df = df.iloc[:][:15000]\ntest_df = df.iloc[:][15000:]","b2812e3b":"print(train_df.describe())","7c4cda32":"ratings = train_df[\"Rating\"]\nbins = [1, 2, 3, 4, 5, 6]\nplt.hist(ratings, bins, rwidth=0.8)\nplt.savefig(\"histogram.png\")","dde2db87":"amount_ratings = len(test_df)\nrobots_predictions = list([5 for i in range(amount_ratings)])","19747bf3":"def accuracy(predictions, real):\n    amount_correct = list(map(lambda x: x[0] == x[1], zip(predictions, real))).count(True)\n    return amount_correct \/ len(predictions) ","79bbf6b4":"accuracy(robots_predictions, list(test_df[\"Rating\"]))","6141aaac":"from nltk.tokenize import word_tokenize # we need to import nltk\nreviews = list(train_df[\"Review\"])\nwords = list(map(lambda review: word_tokenize(review), reviews))","67e624c7":"reviews[0].count(\",\")","c1f105a3":"amount_words = list(map(lambda w: len(w), words))\namount_sentences = list(map(lambda w: len(w), sentences))\namount_commas = list(map(lambda w: w.count(\",\"), reviews))","1e605e80":"amount_commas[:10]","ec5b427a":"df[\"Amount Words\"] = amount_words\ndf[\"Amount Sentences\"] = amount_sentences\ndf[\"Commas\"] = amount_commas","4cbd07e2":"#df.drop(\"Exclamation Marks\", axis=1, inplace=True)","225ad0e2":"df.head(10)","1722201a":"from matplotlib import pyplot as plt\nimport seaborn as sn","4099ff18":"corrMatrix = df.corr()\nfig, ax = plt.subplots(figsize= (15,10))\nsn.heatmap(corrMatrix, annot=True)\nplt.show()","eefa2f10":"words[0]","b7eb6451":"start_df = df.copy()","eb4eaab4":"all_words = []\nfor ws in words:\n    all_words.extend(ws)","43bc31d6":"all_unique_words = list(set(all_words))","9894cc7c":"word_sample = all_words[:50_000]\nunique_words_sample = list(set(word_sample))\nlen(unique_words_sample)","cfcbcb9a":"count_word = {}\nfor i, word in enumerate(unique_words_sample):\n    if i % 1_000 == 0:\n        print(i)\n    count_word[word] = word_sample.count(word)","2ce0fb4f":"most_used_words = list(map(lambda x: x[0],sorted(count_word.items(), key= lambda x: x[1], reverse=True)))","9f4e2474":"unique_words = list(map(lambda x: set(x), words))","abb18cbc":"word_sample = all_words[:50_000]\nunique_words_sample = list(set(word_sample))\nlen(unique_words_sample)\ncount_word = {}\nfor i, word in enumerate(unique_words_sample):\n    if i % 1_000 == 0:\n        print(i)\n    count_word[word] = word_sample.count(word)\nmost_used_words = list(map(lambda x: x[0],sorted(count_word.items(), key= lambda x: x[1], reverse=True)))\nunique_words = list(map(lambda x: set(x), words))\nimportant_words = most_used_words[:1_000]","cb9ef15d":"for i, unique_word in enumerate(important_words):\n    if i % 100 == 0:\n        print(f\"{i} \/ {len(important_words)}\")\n    df[\"amount \" + unique_word] = list(map(lambda w: int(unique_word in w), unique_words))","880a11db":"df.head()","d27333a7":"df[\"Rating\"] = df[\"Rating\"].apply(lambda x: x-1)","873baf30":"df[\"Rating\"].describe()","6d6945ae":"y = list(df[\"Rating\"])","a219710a":"train_df = df.copy()","77ccdf7d":"train_df.drop(\"Rating\", axis=1, inplace=True)\ntrain_df.drop(\"Review\", axis=1, inplace=True)","a9b1fe4d":"train_df.head()","13084cbe":"X = train_df.values","77d441fb":"from sklearn import model_selection\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33)","bbf2c2aa":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression().fit(X_train, y_train)","112be40f":"model.score(X_test, y_test)","ebc85603":"y_predict = model.predict(X_test)","642502fd":"print(y_predict[:10])\nprint(y_test[:10])","6dccea04":"from sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors=10).fit(X_train, y_train)\nmodel.score(X_test, y_test)","1068ec1a":"X_train","ba4a53f5":"X_train = np.array(list(map(lambda x: np.array(x), X_train)))\ny_train = np.array(y_train)\nX_test = np.array(list(map(lambda x: np.array(x), X_test)))\ny_test = np.array(y_test)","04129217":"y_train","03e85493":"from tensorflow import keras\nimport tensorflow as tf\nmodel = keras.Sequential()\nmodel.add(keras.layers.Dense(len(train_df.columns)))\nmodel.add(keras.layers.Dense(6))\n","fb056a3e":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","bd728eb6":"test_x = np.array([np.array([100 for _ in range(len(train_df.columns))])])","d6136587":"test_y = np.array([np.array([1])])","0bb971af":"model.fit(X_train, y_train)","8ce1d74e":"from tensorflow import keras\nimport tensorflow as tf\nmodel = keras.Sequential()\nmodel.add(keras.layers.Dense(len(train_df.columns)))\nmodel.add(keras.layers.Dense(255, activation = \"tanh\"))\nmodel.add(keras.layers.Dense(255, activation=\"relu\"))\n# tanh and then relu has the best result\nmodel.add(keras.layers.Dense(5))\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nmodel.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))","49b9ebe9":"test = {\"hi\": 2 }","01ebea24":"test.get(\"haha\", 0)","a6828aae":"def create_word_count_dict(words):\n    word_count = {}\n    for w in words:\n        if w in word_count:\n            word_count[w] += 1\n        else:\n            word_count[w] = 1\n    return word_count","c0dedf50":"create_word_count_dict(words[0])","b554533b":"word_counts = list(map(lambda w: create_word_count_dict(w),words))","841df399":"word_sample = all_words[:100_000]\nunique_words_sample = list(set(word_sample))\nlen(unique_words_sample)\ncount_word = {}\nfor i, word in enumerate(unique_words_sample):\n    if i % 1_000 == 0:\n        print(i)\n    count_word[word] = word_sample.count(word)\nmost_used_words = list(map(lambda x: x[0],sorted(count_word.items(), key= lambda x: x[1], reverse=True)))\nunique_words = list(map(lambda x: set(x), words))\n","31c664e1":"important_words = most_used_words[:2_000]","bf1aa030":"df_copy = start_df.copy()","1d4af765":"for i, unique_word in enumerate(important_words):\n    if i % 100 == 0:\n        print(f\"{i} \/ {len(important_words)}\")\n    df_copy[\"amount \" + unique_word] = list(map(lambda w: w.get(unique_word, 0), word_counts))","5a817fa8":"df_copy.head()","62427e44":"train_df = df_copy.copy()\ntrain_df[\"Rating\"] = train_df[\"Rating\"].apply(lambda x: x-1)\nprint(train_df[\"Rating\"].describe())\ny = train_df[\"Rating\"]\ntrain_df.drop(\"Rating\", axis=1, inplace=True)\ntrain_df.drop(\"Review\", axis=1, inplace=True)\nX = train_df.values","758800b7":"X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33)\nX_train = np.array(list(map(lambda x: np.array(x), X_train)))\ny_train = np.array(y_train)\nX_test = np.array(list(map(lambda x: np.array(x), X_test)))\ny_test = np.array(y_test)","7625971b":"from tensorflow import keras\nimport tensorflow as tf\nmodel = keras.Sequential()\nmodel.add(keras.layers.Dense(len(train_df.columns)))\nmodel.add(keras.layers.Dense(255, activation = \"tanh\"))\nmodel.add(keras.layers.Dense(255, activation=\"relu\"))\n# tanh and then relu has the best result\nmodel.add(keras.layers.Dense(5))\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nmodel.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))","197c44a7":"words[:10]","f7b222dc":"two_words = []","745f17e9":"def get_all_two_words(words):\n    two_words = []\n    for i in range(1, len(words)):\n        two_words.append((words[i-1], words[i]))\n    return two_words\n        ","74adc99f":"\" \".join(words[0])","169119e4":"get_all_two_words(words[0])","0e186a43":"two_words_ls = list(map(get_all_two_words, words))","f8f26cc3":"for i in range(1, 6):\n    print(str(i) + \":\" + str(list(y).count(i)))","ff58e1b9":"all_two_words = []\nfor two_words in two_words_ls:\n    all_two_words.extend(two_words)","0e5013fc":"new_df_copy = start_df.copy()","d0b31dff":"new_df_copy = df_copy.copy()","a84a84c5":"two_words_sample = all_two_words[:100_000]\nunique_two_words_sample = list(set(two_words_sample))\ncount_word = {}\nprint(len(unique_two_words_sample))\nfor i, word in enumerate(unique_two_words_sample):\n    if i % 1_000 == 0:\n        print(i)\n    count_word[word] = two_words_sample.count(word)\nmost_used_two_words = list(map(lambda x: x[0],sorted(count_word.items(), key= lambda x: x[1], reverse=True)))","e8aa5739":"important_two_words= most_used_two_words[:5_000]","28c815b1":"important_two_words[:20]","2d3a540a":"def create_two_word_count_dict(words):\n    word_count = {}\n    for w in words:\n        if w in word_count:\n            word_count[w] += 1\n        else:\n            word_count[w] = 1\n    return word_count","bb598be8":"two_words_count_ls = list(map(create_two_word_count_dict, two_words_ls))","d834c9fa":"for i, unique_word in enumerate(important_two_words):\n    if i % 100 == 0:\n        print(f\"{i} \/ {len(important_two_words)}\")\n    new_df_copy[\"amount \" + str(unique_word)] = list(map(lambda w: w.get(unique_word, 0), two_words_count_ls))","5776cb40":"new_df_copy.head()","b44d5fa7":"train_df = new_df_copy.copy()\ntrain_df[\"Rating\"] = train_df[\"Rating\"].apply(lambda x: x-1)\nprint(train_df[\"Rating\"].describe())\ny = train_df[\"Rating\"]\ntrain_df.drop(\"Rating\", axis=1, inplace=True)\ntrain_df.drop(\"Review\", axis=1, inplace=True)\nX = train_df.values","d4874ea6":"X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.33)\nX_train = np.array(list(map(lambda x: np.array(x), X_train)))\ny_train = np.array(y_train)\nX_test = np.array(list(map(lambda x: np.array(x), X_test)))\ny_test = np.array(y_test)","ccbf6dd8":"\"\"\"from tensorflow import keras\nimport tensorflow as tf\nmodel = keras.Sequential() \nmodel.add(keras.layers.Dense(len(train_df.columns)))\nmodel.add(keras.layers.Dense(1_000, activation = \"tanh\"))\nmodel.add(keras.layers.Dense(1_000, activation=\"relu\"))\n# tanh and then relu has the best result\nmodel.add(keras.layers.Dense(5))\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nmodel.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n\"\"\"","ab121409":"two words next to each other","6adc591b":"# NLP with Trip Advisor Hotel Reviews\n## How to classify Text","849df9c8":"Explore data with df.head() - shows first 5 rows.","79fc3fc6":"# Could count words insteacd of 0 1","2effa189":"## same result with tanh and sigmoid","1b0d652e":"Therefore I used matplotlib. The histogram shows, that there are much more 5 star ratings. circa 6 000 5 star ratings from a total of 15 k ratings.\nNow we can build a naive algorithm to classify the reviews. Our \"robot\" will always predict 5 stars.","a5aa9251":"We achieved an accuracy of 48% this is pretty good. Our robot always predicted 5 stars, without knowing anything about the text written.\nNow let's try improving our robot so he can predict better.\n\nTherefore our robot will look at the text.\n\nFirst of all we have to transform the text to words, we are going to use nltk for that. It is a very useful libary for NLP.","c2c19dd5":"The Ratings are from 1 to 5. The average( or mean) is 3.95. The Median is 4. \nThe inter-quartile range is 3 - 5 and the standard deviation is 1.23.\nWe can also plot the distribution with a histogram.","1eacffb2":"With training.describe() - we can get more information about the numerical columns in the dataframe.","038303c9":"We can see that each row has a review and a rating.\n\nNow we will start developing our \"classification robot\". To correctly score our \"robot\" we need to split our data in training and testing.\nThe training set will contain 15 000 rows and the test set about 5 000.\n","4a17d258":"First of all load data from kaggle trip advisor hotel reviews.\nLoad Data as pandas Datafram","54f60f63":"Checkout my website [My NLP Blog](http:\/\/www.itman.solutions\/en\/blogs\/nlp-with-trip-advisor-reviews)"}}