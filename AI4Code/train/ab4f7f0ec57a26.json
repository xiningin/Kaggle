{"cell_type":{"6880d5e3":"code","320a64ab":"code","8dc3f9cf":"code","9db3f087":"code","6c67a2ba":"code","9c041545":"code","13829545":"code","92199910":"code","0f1a1f87":"code","6dd365c0":"code","beba71db":"code","afa76727":"code","3cfa9da5":"code","af226e24":"code","57359c96":"code","56d0c623":"code","1b014ddc":"code","cf8a2654":"code","75f0958f":"code","5033c00b":"code","a48341ca":"code","4077fb60":"code","d6cae301":"code","3dbff882":"code","2f7ddc7e":"code","509b8d3d":"code","ba00040a":"code","785ad916":"code","ca93bc1a":"code","9d910fc4":"code","7af9d2e1":"code","389cb23c":"code","94b25cd8":"code","51365738":"code","9b432990":"code","4cb7938b":"code","5bae9fe1":"code","509a2583":"code","97347d0a":"code","d43f15b0":"code","4ebe04f0":"code","4cdbd1d7":"code","d7737796":"code","88fa63cd":"code","929bfb35":"code","25b86a32":"code","17b1f86f":"code","17c956ed":"code","e712f113":"code","8483cbf1":"code","7158a327":"code","5cb243fe":"code","44d7f3fd":"code","2c357325":"code","269b4479":"code","10dc68fc":"code","e65718eb":"markdown","1f6d2164":"markdown","7043ba9b":"markdown","a15ef0c6":"markdown","cb39df52":"markdown","04246d54":"markdown","a1ae38a7":"markdown","5f252f1b":"markdown","ac4894db":"markdown","b840ff43":"markdown","379e29cb":"markdown","d9af2a9a":"markdown","c4f369d7":"markdown","c7bbeb84":"markdown","59221025":"markdown","3b343cd3":"markdown","858671fd":"markdown","4584e3ba":"markdown","86cbf03b":"markdown","279e61d8":"markdown","8a692e6b":"markdown","7b97ad06":"markdown","f3f638be":"markdown"},"source":{"6880d5e3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","320a64ab":"import os\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport textwrap\n\n%matplotlib inline\nplt.rcParams['figure.figsize']=10,6\nplt.rcParams['axes.grid']=True\nplt.gray()","8dc3f9cf":"des = pd.read_csv('\/kaggle\/input\/chai-time-data-science\/Description.csv')\nepsd = pd.read_csv('\/kaggle\/input\/chai-time-data-science\/Episodes.csv')\nytb = pd.read_csv('\/kaggle\/input\/chai-time-data-science\/YouTube Thumbnail Types.csv')\nanchor = pd.read_csv('\/kaggle\/input\/chai-time-data-science\/Anchor Thumbnail Types.csv')\n\nprint('Description_shape...'+str(des.shape))\nprint('Episode_shape...'+str(epsd.shape))\nprint('Youtube_shape...'+str(ytb.shape))\nprint('Thumbail_shape...'+str(anchor.shape))","9db3f087":"des.head()","6c67a2ba":"epsd.head()","9c041545":"from pandas_profiling import ProfileReport\nprof = ProfileReport(epsd)\nprof.to_file(output_file='Episode.html')","13829545":"prof","92199910":"# Genersting the pie chart\ndef pie_chart(df,col,path):\n  label = df[col].value_counts().index.tolist()\n  fig = plt.figure(figsize=(10,6))\n  ax = (df[col].value_counts()*100.0 \/len(df))\\\n  .plot.pie(startangle=90,autopct='%.1f%%', labels =label, fontsize=12)                                                                           \n  ax.set_title('% '+str(col))\n  # plt.savefig(path+str(col1)+'.png')\n  plt.show()\n\n# Relation between categorical variable\ndef categorical_summarized(path,dataframe, x=None, y=None, hue=None,palette='Set1', verbose=True):\n    '''\n    Helper function that gives a quick summary of a given column of categorical data\n    Arguments\n    =========\n    dataframe: pandas dataframe\n    x: str. horizontal axis to plot the labels of categorical data, y would be the count\n    y: str. vertical axis to plot the labels of categorical data, x would be the count\n    hue: str. if you want to compare it another variable (usually the target variable)\n    palette: array-like. Colour of the plot\n    Returns\n    =======\n    Quick Stats of the data and also the count plot\n    '''\n    if x == None:\n        column_interested = y\n    else:\n        column_interested = x\n    series = dataframe[column_interested]\n    print(series.describe())\n    print('mode: ', series.mode())\n    if verbose:\n        print('='*80)\n        print(series.value_counts())\n\n    sns.countplot(x=x, y=y, hue=hue, data=dataframe, palette=palette)\n    plt.xlabel(str(x),Weight='bold',fontsize=12)\n    plt.ylabel(\"Count\",weight='bold',fontsize=12)\n    if x== None:\n        plt.title('Relation_between'+str(y)+'_'+'and'+'_'+str(hue))\n#         g= plt.savefig(path+str(y)+'_'+str(hue)+'.png')\n        labels = dataframe[column_interested].value_counts().index.tolist()\n        labels.sort()\n        labels=[textwrap.fill(text,10) for text in labels]\n        pos = np.arange(len(labels)) \n        plt.yticks(pos, labels)\n    else:\n        plt.title('Relation_between'+'_'+str(x)+'_'+'and'+'_'+str(hue),weight='bold',fontsize=14)\n#         g= plt.savefig(path+str(x)+'_'+str(hue)+'.png')\n        labels = dataframe[column_interested].value_counts().index.tolist()\n        labels.sort()\n        labels=[textwrap.fill(text,10) for text in labels]\n        pos = np.arange(len(labels)) \n        plt.xticks(pos, labels)\n        plt.legend(loc='upper right')\n    plt.show()\n\n # This helper fucntion help to find relation between categorical and numerical variable and visualize stack chart   \ndef stack_plot_sum(df,x_axis,y_axis,hue,title,path):\n  t1 = df.groupby([x_axis,hue])[[y_axis]].count().add_prefix('sum_of_').reset_index()\n  t2 = t1.pivot(x_axis,hue,'sum_of_'+y_axis)\n  ax = t2.plot(kind='bar',stacked=True)\n  plt.xticks(rotation=90)\n#   ax.legend([\"Not_Fraud\", \"Fraud\"])\n  plt.xlabel(str(x_axis),fontweight =\"bold\",fontsize=14)\n  plt.ylabel('Frequency',fontweight =\"bold\",fontsize=14)\n  plt.title(title,fontweight =\"bold\",fontsize=16)\n  # plt.savefig(path+title+'.png')\n  plt.show()\n\n    \n# Relation between the categorical variable    \ndef rel_cat(df,x_axis,y_axis,path,stacked=None):\n    temp =pd.crosstab(df[x_axis],df[y_axis])\n    temp.plot(kind='bar',stacked=stacked,grid=False)\n    plt.xlabel(str(x_axis),weight='bold',fontsize=12)\n    plt.ylabel(str(y_axis),weight='bold',fontsize=12)\n    plt.title(str(x_axis)+'_'+'and'+'_'+str(y_axis),weight='bold',fontsize=14)\n    plt.xticks(rotation=0,fontsize=12)\n    plt.yticks(fontsize=12)\n    labels = df[x_axis].value_counts().index.tolist()\n    labels.sort()\n    labels=[textwrap.fill(text,10) for text in labels]\n    pos = np.arange(len(labels)) \n    plt.xticks(pos, labels)\n#     plt.legend()\n#     plt.savefig(path+str(x_axis)+'_'+'and'+'_'+str(y_axis)+'.jpg')    \n    plt.show()\n    \n\n# Helper fucntion helps to find the relation between the categorical and numerical variable\ndef rel_num_cat(df,x_axis,y_axis,palette,path):\n    t =df.groupby([x_axis])[y_axis].sum().reset_index()\n    heroes = t.sort_values(by=y_axis,ascending=False)[:20].reset_index(drop=True)\n#     heroes\n\n    fig,ax = plt.subplots(figsize=(18,6))\n    g= sns.barplot(x=heroes['heroes'],y=heroes[y_axis],data=heroes,ax=ax,palette=palette)\n    plt.xlabel(str(x_axis),weight='bold',fontsize=12)\n    plt.ylabel(str(y_axis),weight='bold',fontsize=12)\n    plt.title('Number of '+str(y_axis)+'_'+'for_each'+'_'+str(x_axis),weight='bold',fontsize=14)\n    plt.xticks(rotation=0)\n    for index, row in heroes.iterrows():\n        g.text(row.name,row[1], round(row[1]), color='black', ha=\"center\")\n    labels = heroes[x_axis].tolist()\n    # labels.sort()\n    labels=[textwrap.fill(text,10) for text in labels]\n    pos = np.arange(len(labels)) \n    plt.xticks(pos, labels)\n#     plt.savefig('Number of '+str(y_axis)+ 'for each'+ str(x_axis)+'.jpg')\n    plt.show()","0f1a1f87":"epsd.head()","6dd365c0":"epsd.info()","beba71db":"pie_chart(epsd,'heroes_gender',8)","afa76727":"pie_chart(epsd,'category',8)","3cfa9da5":"pie_chart(epsd,'flavour_of_tea',8)","af226e24":"# sns.set_style(\"white\")\nsns.countplot(y=epsd['heroes_nationality'],data=epsd,palette='winter')\nplt.xlabel('Count',weight='bold',fontsize=12)\nplt.ylabel('Heros_Nationality',weight='bold',fontsize=12)\nplt.title('Count of Heros from different Nationality',weight='bold',fontsize=14)\nplt.show()","57359c96":"f, ax = plt.subplots(figsize=(10,6))\nsns.countplot(y=epsd['recording_time'],data=epsd,palette='winter_r',ax=ax)\nplt.xlabel('Count',weight='bold',fontsize=12)\nplt.ylabel('Recording_time',weight='bold',fontsize=12)\nplt.title('Count of recoding time',weight='bold',fontsize=14)\nfor p in ax.patches:\n    width = p.get_width()\n    ax.text(width+1.,\n            p.get_y()+p.get_height()\/3. + 0.2,\n            '{:1.2f}'.format(width),\n            ha=\"center\")\nplt.show()","56d0c623":"c_palette = ['tab:blue', 'tab:orange','tab:green','tab:red']\ncategorical_summarized(8,epsd,x='category',hue='heroes_gender',palette='winter')","1b014ddc":"rel_cat(epsd,'flavour_of_tea','recording_time',8,stacked=True)","cf8a2654":"epsd.columns","75f0958f":"epsd['category'].value_counts()","5033c00b":"plt.rcParams['figure.figsize']=18,6\nrel_cat(epsd,'heroes_nationality','category',8,stacked=True)","a48341ca":"plt.rcParams['figure.figsize']=18,6\ncategorical_summarized(8,epsd,x='flavour_of_tea',hue='heroes_gender',palette='winter')","4077fb60":"categorical_summarized(8,epsd,x='flavour_of_tea',hue='category',palette='Blues_r')","d6cae301":"t =epsd.groupby(['heroes'])['youtube_subscribers'].sum().reset_index()\nheroes = t.sort_values(by='youtube_subscribers',ascending=False)[:10].reset_index(drop=True)\nheroes","3dbff882":"t =epsd.groupby(['heroes'])['youtube_subscribers'].sum().reset_index()\nheroes = t.sort_values(by='youtube_subscribers',ascending=False)[:20].reset_index(drop=True)\nheroes\n\nfig,ax = plt.subplots(figsize=(18,6))\ng= sns.barplot(x=heroes['heroes'],y=heroes['youtube_subscribers'],data=heroes,ax=ax,palette='Blues_r')\nplt.xlabel('Heros',weight='bold',fontsize=12)\nplt.ylabel('youtube_subscribers',weight='bold',fontsize=12)\nplt.title('Number of Subscribers for each Heroes',weight='bold',fontsize=14)\nplt.xticks(rotation=0)\nfor index, row in heroes.iterrows():\n    g.text(row.name,row.youtube_subscribers, round(row.youtube_subscribers), color='black', ha=\"center\")\nlabels = heroes['heroes'].tolist()\n# labels.sort()\nlabels=[textwrap.fill(text,10) for text in labels]\npos = np.arange(len(labels)) \nplt.xticks(pos, labels)\nplt.show()","2f7ddc7e":"plt.rcParams['figure.figsize']=10,6\nrel_cat(epsd,'recording_time','category',8,stacked=True)","509b8d3d":"plt.rcParams['figure.figsize']=18,6\ncategorical_summarized(8,epsd,x='recording_time',hue='category',palette='BuPu_r')","ba00040a":"x = epsd['episode_id']\ny= epsd['episode_duration']\n\n# Color palette\nblue, = sns.color_palette(\"muted\", 1)\n\n# Make the plot\nfig,ax = plt.subplots(figsize=(24,8))\nax.plot(x, y, color=blue, lw=3)\nax.fill_between(x, 0, y, alpha=.3)\nax.set(xlim=(0, len(x)-1), ylim=(0, None), xticks=x)\nplt.show()\n","785ad916":"rel_num_cat(epsd,'heroes','spotify_starts','winter',8)","ca93bc1a":"rel_num_cat(epsd,'heroes','anchor_plays','brg',8)","9d910fc4":"rel_num_cat(epsd,'heroes','youtube_likes','hsv',8)","7af9d2e1":"rel_num_cat(epsd,'heroes','youtube_watch_hours','gnuplot',8)","389cb23c":"data_epd = epsd\ndata_epd['avg_duration']=  data_epd['youtube_watch_hours']*60\/ data_epd['youtube_views']\nrel_num_cat(data_epd,'heroes','avg_duration','twilight_shifted',8)","94b25cd8":"epsd.columns","51365738":"rel_num_cat(epsd,'heroes','youtube_nonimpression_views','Blues_r',8)\n# Jeremy Howard and Parul having  youtube non impression views","9b432990":"rel_num_cat(epsd,'heroes','youtube_impression_views','winter',8)","4cb7938b":"rel_num_cat(epsd,'heroes','youtube_dislikes','Spectral',8)","5bae9fe1":"rel_num_cat(epsd,'heroes','youtube_subscribers','Spectral',8)","509a2583":"t1 = epsd.groupby(['episode_name','category'])[['youtube_ctr']].sum().add_prefix('sum_of_').reset_index()\nt1.head()","97347d0a":"plt.figure(figsize=(24,6))\nsns.lineplot(x='episode_name',y='sum_of_youtube_ctr',hue='category',data=t1)\nplt.xticks(rotation=90)\nplt.show()\n\n# Episode wise category","d43f15b0":"epsd.head()","4ebe04f0":"epsd['spotify_listeners'].plot()\nepsd['apple_listeners'].plot()","4cdbd1d7":"des.head()","d7737796":"from sklearn.feature_extraction.text import CountVectorizer\nimport re\n# Tutorial about Python regular expressions: https:\/\/pymotw.com\/2\/re\/ import string\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport random\nimport string","88fa63cd":"\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    #remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(tokenized_text)\n    return combined_text\n","929bfb35":"# Applying the cleaning function to both Descriptions datasets\ndes['text_clean'] = des['description'].apply(str).apply(lambda x: text_preprocessing(x))","25b86a32":"# Analyzing Text statistics\n\ndes['text_len'] = des['text_clean'].astype(str).apply(len)\ndes['text_word_count'] = des['text_clean'].apply(lambda x: len(str(x).split()))","17b1f86f":"plt.rcParams['figure.figsize']=10,6\nsns.distplot(des['text_len'],color='red')\nplt.xlabel('text_len',weight='bold',fontsize=12)\nplt.ylabel('Count',weight='bold',fontsize=12)\nplt.title('Distribution of description',weight='bold',fontsize=14)\nplt.show()","17c956ed":"#source of code : https:\/\/medium.com\/@cristhianboujon\/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\ndef get_top_n_words(corpus, n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","e712f113":"#Distribution of top unigrams\ndes_unigrams = get_top_n_words(des['text_clean'],20)\n\ndf1 = pd.DataFrame(des_unigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='g')\nplt.ylabel('words',weight='bold',fontsize=12)\nplt.xlabel('Counts',weight='bold',fontsize=12)\nplt.title('Top 20 unigrams in description text',weight='bold',fontsize=14)\nplt.show()","8483cbf1":"def get_top_n_gram(corpus,ngram_range,n=None):\n    vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","7158a327":"#Distribution of top Bigrams\ndes_bigrams = get_top_n_gram(des['text_clean'],(2,2),20)\n\ndf1 = pd.DataFrame(des_bigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='r',alpha=0.5)\nplt.ylabel('words',weight='bold',fontsize=12)\nplt.xlabel('Counts',weight='bold',fontsize=12)\nplt.title('Top 20 bigrams in description text',weight='bold',fontsize=14)\nplt.show()","5cb243fe":"#Distribution of top Trigrams\ndes_trigrams = get_top_n_gram(des['text_clean'],(3,3),20)\n\ndf1 = pd.DataFrame(des_trigrams, columns = ['Text' , 'count'])\ndf1.groupby('Text').sum()['count'].sort_values(ascending=True).plot(kind='barh',color='b',alpha=0.5)\nplt.ylabel('words',weight='bold',fontsize=12)\nplt.xlabel('Counts',weight='bold',fontsize=12)\nplt.title('Top 20 trigrams in description text',weight='bold',fontsize=14)\nplt.show()","44d7f3fd":"#source of code : https:\/\/medium.com\/@cristhianboujon\/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d\ndef get_top_n_words(corpus, n=None):\n    \"\"\"\n    List the top n words in a vocabulary according to occurrence in a text corpus.\n    \"\"\"\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","2c357325":"top_words_in_description_text = get_top_n_words(des['text_clean'])\n\np1 = [x[0] for x in top_words_in_description_text[:20]]\np2 = [x[1] for x in top_words_in_description_text[:20]]","269b4479":"# Top Descriptions word\nsns.barplot(x=p2,y=p1,palette='winter')\n# plt.xticks(rotation=45)\nplt.yticks(fontsize=12)\nplt.ylabel('Words',weight='bold',fontsize=12)\nplt.xlabel('Counts',weight='bold',fontsize=12)\nplt.title('Top 20 description Words',weight='bold',fontsize=14)\nplt.show()","10dc68fc":"from wordcloud import WordCloud\nfig,ax = plt.subplots(figsize=[40,30])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(des['text_clean']))\nax.imshow(wordcloud1)\nax.axis('off')\nax.set_title('Descriptions text',fontsize=20);","e65718eb":"### Now we are checking the porfile report the episode the table\n- From the profile report we can get the whole summary of the data such as missing value,interation ,correlation etc","1f6d2164":"### Necessary funtions","7043ba9b":"> Ginger Chai,Masala chai,Sulemani Chai,Herbal chai and Kesar Rose chai are mostly used in the Episodes> ","a15ef0c6":"It is showing that CTDS  dosen't have female Kaggler and it having very less count of female in other category","cb39df52":"### Importing Necessary Library","04246d54":"### Checking the Description and Episode tabel","a1ae38a7":"#### Most ML Heroes are from India,USA,Canada,Germany,Russia and France","5f252f1b":"##### We can seet the datastype of each variable","ac4894db":"### Ginger tea are mostly have by the Kaggler","b840ff43":"### Distribution of category ","379e29cb":"### Ginger tea , Apple Cinnamon, Masala Chai, Paan Rose Green Tea are mostly have by Males Heroes","d9af2a9a":"Kesar Rose Chai are mostly have in the night and on the other hand, masala tea are have during the morning","c4f369d7":"## Lets Work on description dataset so that we can get some information ","c7bbeb84":"- We can clearly see the distribution,correlation missing etc for episode data set\n- this will help us in understanding the overview of the dataset of we are going to work","59221025":"### Distribuition of the Flavour tea used in the eahc Episodes","3b343cd3":"> Here Kaggle and Industry covers more than 75% of the category","858671fd":"### Distribution of the Heroes based on Gender","4584e3ba":"> Industry having most of ther recording at night where as the Kagglers having recording at afternoon, Evening, Morning and Night","86cbf03b":"> Showing the top 20 kaggler having subscribers and \n> Jeremy Howard having most number of subscribers than other\n","279e61d8":"*Here male heroes are much more the female ML heroes*","8a692e6b":"### Most of the Recordings the happen at Night","7b97ad06":"### Loading the Data ","f3f638be":"Reference: https:\/\/www.kaggle.com\/parulpandey\/how-to-explore-the-ctds-show-data,\n           https:\/\/www.kaggle.com\/vpkprasanna\/insights-on-chai-time-data-science"}}