{"cell_type":{"8677e18d":"code","1afe8220":"code","92a9795a":"code","6e1d314e":"code","421d1d37":"code","e4d31f43":"code","13f2be10":"code","69417fba":"code","aa25f852":"code","bc1f795d":"code","417fab12":"code","1a89567c":"code","20da45a2":"code","70205043":"code","e7cb4b0e":"code","97610731":"code","d7ec6c5b":"code","35450f32":"code","c371e5b8":"code","9ac09d7e":"code","b60c7da0":"code","b010b73d":"code","d747cd94":"code","5a8a7965":"code","eed2c56c":"code","c2f665ca":"code","221b9c7d":"code","b997d27d":"code","4638f7fc":"code","15c6cfae":"code","0e06b1da":"code","072d7934":"code","dbe63b65":"code","de527ac1":"code","8e0eb5c6":"code","f29b859d":"code","cb2f96db":"code","340a6fbb":"code","f6214233":"code","9d37667b":"code","20f84f5e":"code","fc45449c":"code","1c952faa":"code","2aedbc82":"code","98f22c13":"code","8ef4aecd":"code","6c0f14a0":"code","c92828d6":"code","c4565891":"code","829fe233":"code","a1903503":"code","4a7f4672":"code","0f822d34":"code","6130f766":"code","22de4605":"code","b101a23a":"code","1a1a98b6":"code","c5ea5bc9":"code","de19a8f4":"code","d54ba621":"code","21cd70ea":"code","9afe177e":"code","2a3cd06d":"code","13f90b48":"markdown","a95225d5":"markdown","387d9cb1":"markdown","091e11e3":"markdown","de80ef0b":"markdown","3e53ce24":"markdown","18112e13":"markdown","73305558":"markdown","e0197ae8":"markdown","a58a62ff":"markdown"},"source":{"8677e18d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1afe8220":"# Read .csv file into pandas\ndata = pd.read_csv('\/kaggle\/input\/employee-future-prediction\/Employee.csv')\ndata.head()","92a9795a":"# Shape of data\nprint(\"The shape of data is \" ,data.shape)\n# Info of data\ndata.info()","6e1d314e":"#checking missing data\ndata.isnull().sum()","421d1d37":"# Checing for value counts \ndata['LeaveOrNot'].value_counts()","e4d31f43":"# Checing for value counts \ndata['Education'].value_counts()","13f2be10":"# Checing for value counts \ndata['Gender'].value_counts()","69417fba":"# Checing for value counts \ndata['City'].value_counts()","aa25f852":"# Checing for value counts \ndata['EverBenched'].value_counts()","bc1f795d":"# education & Loan Status\nimport seaborn as sns\nsns.countplot(x='Education',hue='LeaveOrNot',data=data)","417fab12":"# marital status & Loan Status\nsns.countplot(x='Gender',hue='LeaveOrNot',data=data)","1a89567c":"# marital status & Loan Status\nsns.countplot(x='City',hue='LeaveOrNot',data=data)","20da45a2":"# marital status & Loan Status\nsns.countplot(x='EverBenched',hue='LeaveOrNot',data=data)","70205043":"# marital status & Loan Status\nsns.countplot(x='Age',hue='LeaveOrNot',data=data)","e7cb4b0e":"# marital status & Loan Status\nsns.countplot(x='ExperienceInCurrentDomain',hue='LeaveOrNot',data=data)","97610731":"# convert categorical columns to numerical values\ndata.replace({'EverBenched':{'No':0,'Yes':1},'City':{'Bangalore':0,'Pune':1,'New Delhi':2},\n              'Gender':{'Male':0,'Female':1},'Education':{'Bachelors':0,'Masters':1,'PHD':2}},inplace=True)","d7ec6c5b":"# lets see the data one time \ndata.head()","35450f32":"# separating the data and label\nX = data.drop(columns=['LeaveOrNot'],axis=1)\ny = data['LeaveOrNot']","c371e5b8":"# Shape of X & Y\nprint(\" Shape of X is\", X.shape)\nprint(\" Shape of X is\", y.shape)","9ac09d7e":"# splitting the data into testing and training data.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,stratify=y , random_state = 0)","b60c7da0":"print(\" Shape of X_train is\", X_train.shape)\nprint(\" Shape of X_test is\", X_test.shape)\nprint(\" Shape of y_train is\", y_train.shape)\nprint(\" Shape of y_test is\", y_test.shape)","b010b73d":"# Feature scaling the data \n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","d747cd94":"# fitting data to Logistic Regression model\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\nlogistic_model = LogisticRegression()\nlogistic_model.fit(X_train, y_train)\ny_pred = logistic_model.predict(X_test)\n\nlogistic_model_train_acc = accuracy_score(y_train, logistic_model.predict(X_train))\nlogistic_model_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Logistic Regression Model is {logistic_model_train_acc}\")\nprint(f\"Test Accuracy of Logistic Regression Model is {logistic_model_test_acc}\")","5a8a7965":"# confusion matrix \nconfusion_matrix(y_test, y_pred)","eed2c56c":"# classification report\nprint(classification_report(y_test, y_pred))","c2f665ca":"# KNN Classifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\n\ny_pred = knn.predict(X_test)\n\nknn_train_acc = accuracy_score(y_train, knn.predict(X_train))\nknn_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of KNN Model is {knn_train_acc}\")\nprint(f\"Test Accuracy of KNN Model is {knn_test_acc}\")","221b9c7d":"# confusion matrix \nconfusion_matrix(y_test, y_pred)","b997d27d":"# classification report\nprint(classification_report(y_test, y_pred))","4638f7fc":"# Support Classifier\n\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(X_train, y_train)\n\ny_pred = svc.predict(X_test)\n\nsvc_train_acc = accuracy_score(y_train, svc.predict(X_train))\nsvc_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of SVC Model is {svc_train_acc}\")\nprint(f\"Test Accuracy of SVC Model is {svc_test_acc}\")","15c6cfae":"# confusion matrix\nconfusion_matrix(y_test, y_pred)","0e06b1da":"# classification report\nprint(classification_report(y_test, y_pred))","072d7934":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier()\ndtc.fit(X_train, y_train)\n\ny_pred = dtc.predict(X_test)\n\ndtc_train_acc = accuracy_score(y_train, dtc.predict(X_train))\ndtc_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Decision Tree Model is {dtc_train_acc}\")\nprint(f\"Test Accuracy of Decision Tree Model is {dtc_test_acc}\")","dbe63b65":"# confusion matrix\n\nconfusion_matrix(y_test, y_pred)","de527ac1":"# classification report\nprint(classification_report(y_test, y_pred))","8e0eb5c6":"# hyper parameter tuning\n\nfrom sklearn.model_selection import GridSearchCV\n\ngrid_params = {\n    'criterion' : ['gini', 'entropy'],\n    'max_depth' : [3, 5, 7, 10],\n    'min_samples_split' : range(2, 10, 1),\n    'min_samples_leaf' : range(2, 10, 1)\n}\n\ngrid_search = GridSearchCV(dtc, grid_params, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search.fit(X_train, y_train)","f29b859d":"dtc = grid_search.best_estimator_\ny_pred = dtc.predict(X_test)\ndtc_train_acc = accuracy_score(y_train, dtc.predict(X_train))\ndtc_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Decesion Tree Model is {dtc_train_acc}\")\nprint(f\"Test Accuracy of Decesion Tree Model is {dtc_test_acc}\")","cb2f96db":"# best parameters and best score\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)","340a6fbb":"from sklearn.ensemble import RandomForestClassifier\n\nrand_clf = RandomForestClassifier(criterion = 'entropy', max_depth = 7, max_features = 'sqrt', min_samples_leaf = 3, min_samples_split = 2, n_estimators = 180)\nrand_clf.fit(X_train, y_train)\n\ny_pred = rand_clf.predict(X_test)\n\nrand_clf_train_acc = accuracy_score(y_train, rand_clf.predict(X_train))\nrand_clf_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Random Forest Model is {rand_clf_train_acc}\")\nprint(f\"Test Accuracy of Random Forest Model is {rand_clf_test_acc}\")","f6214233":"#Ada Boost Classifie\nfrom sklearn.ensemble import AdaBoostClassifier\n\nada = AdaBoostClassifier(base_estimator = dtc)\n\nparameters = {\n    'n_estimators' : [50, 70, 90, 120, 180, 200],\n    'learning_rate' : [0.001, 0.01, 0.1, 1, 10],\n    'algorithm' : ['SAMME', 'SAMME.R']\n}\n\ngrid_search = GridSearchCV(ada, parameters, n_jobs = -1, cv = 10, verbose = 1)\ngrid_search.fit(X_train, y_train)","9d37667b":"print(grid_search.best_params_)\nprint(grid_search.best_score_)","20f84f5e":"ada = AdaBoostClassifier(base_estimator = dtc, algorithm = 'SAMME.R', learning_rate = 0.001, n_estimators = 200)\nada.fit(X_train, y_train)\n\nada_train_acc = accuracy_score(y_train, ada.predict(X_train))\nada_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Ada Boost Model is {ada_train_acc}\")\nprint(f\"Test Accuracy of Ada Boost Model is {ada_test_acc}\")","fc45449c":"# confusion matrix\nconfusion_matrix(y_test, y_pred)","1c952faa":"# classification report\nprint(classification_report(y_test, y_pred))","2aedbc82":"#Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier()\n\nparameters = {\n    'loss': ['deviance', 'exponential'],\n    'learning_rate': [0.001, 0.1, 1, 10],\n    'n_estimators': [100, 150, 180, 200]\n}\n\ngrid_search = GridSearchCV(gb, parameters, cv = 5, n_jobs = -1, verbose = 1)\ngrid_search.fit(X_train, y_train)","98f22c13":"# best parameter and best score\n\nprint(grid_search.best_params_)\nprint(grid_search.best_score_)\n","8ef4aecd":"gb = GradientBoostingClassifier(learning_rate = 0.1, loss = 'deviance', n_estimators = 180)\ngb.fit(X_train, y_train)\n\ny_pred = gb.predict(X_test)\n\ngb_train_acc = accuracy_score(y_train, gb.predict(X_train))\ngb_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Gradient Boosting Classifier Model is {gb_train_acc}\")\nprint(f\"Test Accuracy of Gradient Boosting Classifier Model is {gb_test_acc}\")","6c0f14a0":"# Stochastic Gradient Boosting (SGB)\nsgbc = GradientBoostingClassifier(learning_rate = 0.1, subsample = 0.9, max_features = 0.75, loss = 'deviance',\n                                  n_estimators = 100)\n\nsgbc.fit(X_train, y_train)\n\ny_pred = sgbc.predict(X_test)\n\nsgbc_train_acc = accuracy_score(y_train, sgbc.predict(X_train))\nsgbc_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of SGB Model is {sgbc_train_acc}\")\nprint(f\"Test Accuracy of SGB Model is {sgbc_test_acc}\")","c92828d6":"#Cat boost\nfrom catboost import CatBoostClassifier\n\ncat = CatBoostClassifier(iterations = 180, learning_rate = 0.1)\ncat.fit(X_train, y_train)\n\ny_pred = cat.predict(X_test)","c4565891":"cat_train_acc = accuracy_score(y_train, cat.predict(X_train))\ncat_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of Cat Boost Classifier Model is {cat_train_acc}\")\nprint(f\"Test Accuracy of Cat Boost Classifier Model is {cat_test_acc}\")","829fe233":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(booster = 'gblinear', learning_rate = 1, n_estimators = 10)\nxgb.fit(X_train, y_train)\n\ny_pred = xgb.predict(X_test)\n\nxgb_train_acc = accuracy_score(y_train, xgb.predict(X_train))\nxgb_test_acc = accuracy_score(y_test, y_pred)\n\nprint(f\"Training Accuracy of XGB Model is {xgb_train_acc}\")\nprint(f\"Test Accuracy of XGB Model is {xgb_test_acc}\")","a1903503":"X_train.shape","4a7f4672":"# let's divide our dataset into training set and holdout set by 50% \n\nfrom sklearn.model_selection import train_test_split\n\ntrain, val_train, test, val_test = train_test_split(X, y, test_size = 0.5, random_state = 355)","0f822d34":"# let's split the training set again into training and test dataset\n\nX_train, X_test, y_train, y_test = train_test_split(train, test, test_size = 0.2, random_state = 355)","6130f766":"svm = SVC()\nsvm.fit(X_train, y_train)","22de4605":"# using Logistic Regression and SVM algorithm as base models.\n# Let's fit both of the models first on the X_train and y_train data.\n\nlr = LogisticRegression(solver='liblinear')\nlr.fit(X_train, y_train)","b101a23a":"predict_val1 = lr.predict(val_train)\npredict_val2 = svm.predict(val_train)","1a1a98b6":"predict_val = np.column_stack((predict_val1, predict_val2))\n","c5ea5bc9":"#Let's get the prediction of all the base models on test set X_set\npredict_test1 = lr.predict(X_test)\npredict_test2 = svm.predict(X_test)","de19a8f4":"# Let's stack the prediction values for validation set together as 'predict_set'\npredict_test = np.column_stack((predict_test1, predict_test2))","d54ba621":"rand_clf = RandomForestClassifier()\nrand_clf.fit(predict_val, val_test)","21cd70ea":"stacking_acc = accuracy_score(y_test, rand_clf.predict(predict_test))\nprint(stacking_acc)","9afe177e":"models = ['Logistic Regression', 'SVC', 'Decision Tree', 'Random Forest','Ada Boost', 'Gradient Boosting', 'SGB', 'XgBoost', 'Stacking', 'Cat Boost']\nscores = [logistic_model_test_acc, svc_test_acc, dtc_test_acc, rand_clf_test_acc, ada_test_acc, gb_test_acc, sgbc_test_acc, xgb_test_acc, stacking_acc, cat_test_acc]\n\nmodels = pd.DataFrame({'Model' : models, 'Score' : scores})\n\n\nmodels.sort_values(by = 'Score', ascending = False)","2a3cd06d":"import matplotlib.pyplot as plt\nplt.figure(figsize = (18, 8))\n\nsns.barplot(x = 'Model', y = 'Score', data = models)\nplt.show()","13f90b48":"# **\ud83d\udd75\ufe0f Model Traning**\n\n**We will train the model with different models also apply some boosting and stacking \ud83d\ude09**","a95225d5":"# **\ud83d\udcc8 Data Visualization**","387d9cb1":"# **\ud83d\udc40 Data Transformation**","091e11e3":"# **\ud83d\udcc8 Exploratory Data Analysis**","de80ef0b":"**We can also use label encoding and One hot encoding but we are not using this time because we will simply train our model in DTC & Random forest lets check the accuracy with out Onhe Hot Encoding :)**","3e53ce24":"***We can see that Cat Boost gives the best accuracy, So we will use this model for production. If you found this notebook please upvote & also give your valuable suggestion for further improvment. \u2764\ufe0f***\n\n<img src= \"https:\/\/www.funimada.com\/assets\/images\/cards\/big\/thank-you-10.gif\" alt =\"Titanic\" style='width: 800px;'>\n\n","18112e13":"# **\u2728 Model Preparation**","73305558":"**We will convert columns into numerical value by mapping. Here we have !** \n\n**EverBenched:**\n> NO --> 0\n\n> Yes --> 1\n\n**City:**\n> Bangalore --> 0\n\n> Pune --> 1\n\n> New Delhi --> 2\n\n**Gender:**\n> Male --> 0\n\n> Female --> 1\n\n**Education:**\n> Bachelors --> 0\n\n> Masters --> 1\n\n> PHD --> 2\n    \n    ","e0197ae8":"# **\u26cf\ufe0f Model Selection**\n\n**We will compare the accuracy of different models then we will select our best model for production.**","a58a62ff":"# **\ud83d\ude80 Getting Started**\n\n### **Title: Employee Future Prediction**\n\n**0 --> Not Leave**\n\n**1 --> Leave**\n"}}