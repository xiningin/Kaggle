{"cell_type":{"c2d66548":"code","83c854ce":"code","0e687bb5":"code","aa127e45":"code","e7ffc60c":"code","8edefecd":"code","7df0d78e":"code","1453d6ae":"code","1f366ed4":"code","ffad4f18":"code","bd83d5c7":"code","8a6a92b7":"code","56c45792":"code","cb1b950e":"code","a7b53855":"code","34daf6b8":"code","c7d677e5":"code","4d616d8f":"code","a9026b43":"code","57b55289":"code","63945d50":"code","ddaf6b6c":"code","cd557dd6":"code","ed124c25":"code","12cfb115":"code","60f1d50c":"code","30103d6e":"code","c3d5bba8":"code","bd586022":"markdown","dc6dfb1f":"markdown","df411471":"markdown","741502e9":"markdown","33de529b":"markdown"},"source":{"c2d66548":"import numpy as np\nimport pandas as pd\nfrom keras.layers import (Input, Embedding, Conv1D, Activation, GlobalMaxPooling1D, BatchNormalization,\n                          Concatenate, CuDNNLSTM, Flatten, Dropout, Dense)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom keras.models import Model, Sequential\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.initializers import Constant\nfrom keras.metrics import top_k_categorical_accuracy\nfrom keras.utils import to_categorical, Sequence, plot_model\nimport matplotlib.pyplot as plt","83c854ce":"news = pd.read_pickle('..\/input\/stopwords-removal-and-lemmatization\/data.pkl')\nnews = shuffle(news, random_state=29)\nnews.head()","0e687bb5":"news['category'] = news['category'].apply(lambda x: \"WORLDPOST\" if x == \"THE WORLDPOST\" else x)\nlabels = list(set(news['category']))\nnews['category'] = news['category'].apply(labels.index)\nX_train, X_test, y_train, y_test = train_test_split(news['text'], news['category'], test_size=0.3, random_state=4)\nX_holdout, X_test, y_holdout, y_test = train_test_split(X_test, y_test, test_size=0.66, random_state=98)\ny_train, y_test, y_holdout = list(y_train), list(y_test), list(y_holdout)","aa127e45":"tokenizer = Tokenizer(lower=True)\ntokenizer.fit_on_texts(news['text'])\nX_train = tokenizer.texts_to_sequences(X_train)\nX_holdout = tokenizer.texts_to_sequences(X_holdout)\nX_test = tokenizer.texts_to_sequences(X_test)","e7ffc60c":"def get_embed_mat(EMBEDDING_FILE):\n    embeddings_index = {}\n    with open(EMBEDDING_FILE, 'r') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = coefs\n        f.close()\n    \n    word_index = tokenizer.word_index\n    num_words = len(word_index) + 1\n    all_embs = np.stack(embeddings_index.values())\n    embedding_matrix = np.random.normal(all_embs.mean(), all_embs.std(), \n                                        (num_words, embed_dim))\n    for word, i in word_index.items():\n        if i >= num_words:\n            break\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n    return num_words, embedding_matrix\n\nEMBEDDING_FILE = '..\/input\/glove-global-vectors-for-word-representation\/glove.6B.200d.txt'\nembed_dim = 200\nnum_words, embedding_matrix = get_embed_mat(EMBEDDING_FILE)","8edefecd":"plt.hist([len(x) for x in X_train + X_test+X_holdout], bins=100)\nplt.ylabel('Number of sequences')\nplt.xlabel('Length')\nplt.show()","7df0d78e":"plt.hist([len(x) for x in X_train + X_holdout + X_test], bins=100)\nplt.ylabel('Number of sequences')\nplt.xlabel('Length')\nplt.show()","1453d6ae":"max_length = 50\nprint('Sequence length:', max_length)","1f366ed4":"num_classes = len(labels)\nlayers = []\nfilters = [2, 3, 5]\n\nsequence_input1 = Input(shape=(max_length, ), dtype='int32')\nembedding_layer_static1 = Embedding(num_words, embed_dim, embeddings_initializer=Constant(embedding_matrix),\n                            input_length=max_length, trainable=False)(sequence_input1)\n\nfor sz in filters:\n    conv_layer1 = Conv1D(filters=256, kernel_size=sz)(embedding_layer_static1)\n    batchnorm_layer1 = BatchNormalization()(conv_layer1)\n    act_layer1 = Activation('relu')(batchnorm_layer1)\n    pool_layer1 = GlobalMaxPooling1D()(act_layer1)\n    layers.append(pool_layer1)\n\nmerged1 = Concatenate(axis=1)(layers)\n\ndrop1 = Dropout(0.5)(merged1)\ndense1 = Dense(512, activation='relu')(drop1)\nout1 = Dense(num_classes, activation='softmax')(dense1)\n\ncnn_static = Model(sequence_input1, out1)\ncnn_static.summary()","ffad4f18":"def top_3_acc(y_true, y_pred):\n    return top_k_categorical_accuracy(y_true, y_pred, k=3)\n\nclass dataseq(Sequence):\n    def __init__(self, X, y, batch_size, padding='post'):\n        self.x, self.y = X, y\n        self.batch_size = batch_size\n        self.m = len(self.y)\n        self.padding = padding\n\n    def __len__(self):\n        return int(np.ceil(len(self.x) \/ float(self.batch_size)))\n\n    def __getitem__(self, idx):\n        batch_x = self.x[idx * self.batch_size:min((idx + 1) * self.batch_size, self.m)]\n        batch_y = self.y[idx * self.batch_size:min((idx + 1) * self.batch_size, self.m)]\n\n        return pad_sequences(batch_x, maxlen=max_length, truncating='post', padding=self.padding), to_categorical(\n            batch_y, num_classes=num_classes)\n\ncnn_static.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc', top_3_acc])\n\nbatch_size = 128\ncnn_static_history = cnn_static.fit_generator(dataseq(X_train, y_train, batch_size), epochs=15, verbose=2,\n                              validation_data = dataseq(X_holdout, y_holdout, batch_size), shuffle=True)","bd83d5c7":"plt.plot(cnn_static_history.history['loss'], label='train')\nplt.plot(cnn_static_history.history['val_loss'], label='holdout')\nplt.title('CNN - Static learning curve')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(loc='upper right')\nplt.show()","8a6a92b7":"cnn_static.evaluate_generator(dataseq(X_test, y_test, batch_size))","56c45792":"layers = []\n\nembedding_layer_dynamic1 = Embedding(num_words, embed_dim, embeddings_initializer=Constant(embedding_matrix),\n                            input_length=max_length)(sequence_input1)\n\nfor sz in filters:\n    conv_layer2 = Conv1D(filters=256, kernel_size=sz)(embedding_layer_dynamic1)\n    batchnorm_layer2 = BatchNormalization()(conv_layer2)\n    act_layer2 = Activation('relu')(batchnorm_layer2)\n    pool_layer2 = GlobalMaxPooling1D()(act_layer2)\n    layers.append(pool_layer2)\n\nmerged2 = Concatenate(axis=1)(layers)\n\ndrop2 = Dropout(0.5)(merged2)\ndense2 = Dense(512, activation='relu')(drop2)\nout2 = Dense(num_classes, activation='softmax')(dense2)\n\ncnn_dynamic = Model(sequence_input1, out2)","cb1b950e":"cnn_dynamic.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc', top_3_acc])\ncnn_dynamic_history = cnn_dynamic.fit_generator(dataseq(X_train, y_train, batch_size), epochs=5, verbose=2,\n                              validation_data = dataseq(X_holdout, y_holdout, batch_size), shuffle=True)","a7b53855":"plt.plot(cnn_dynamic_history.history['loss'], label='train')\nplt.plot(cnn_dynamic_history.history['val_loss'], label='holdout')\nplt.title('CNN - Dynamic learning curve')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(loc='upper right')\nplt.show()","34daf6b8":"cnn_dynamic.evaluate_generator(dataseq(X_test, y_test, batch_size))","c7d677e5":"sequence_input2 = Input(shape=(max_length, ), dtype='int32')\nembedding_layer_static2 = Embedding(num_words, embed_dim, embeddings_initializer=Constant(embedding_matrix),\n                            input_length=max_length, trainable=False)(sequence_input2)\nlstm1 = CuDNNLSTM(500, return_sequences=True)(embedding_layer_static2)\ndrop3 = Dropout(0.5)(lstm1)\nlstm2 = CuDNNLSTM(200)(drop3)\ndrop4 = Dropout(0.5)(lstm2)\nout3 = Dense(num_classes, activation='softmax')(drop4)\nlstm_static = Model(sequence_input2, out3)\nlstm_static.summary()","4d616d8f":"lstm_static.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc', top_3_acc])\nlstm_static_history = lstm_static.fit_generator(dataseq(X_train, y_train, batch_size, 'pre'), epochs=5, verbose=2,\n                              validation_data = dataseq(X_holdout, y_holdout, batch_size, 'pre'))","a9026b43":"plt.plot(lstm_static_history.history['loss'], label='train')\nplt.plot(lstm_static_history.history['val_loss'], label='test')\nplt.title('LSTM - Static learning curve')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(loc='upper right')\nplt.show()","57b55289":"lstm_static.evaluate_generator(dataseq(X_test, y_test, batch_size, 'pre'))","63945d50":"embedding_layer_dynamic2 = Embedding(num_words, embed_dim, embeddings_initializer=Constant(embedding_matrix),\n                           input_length=max_length)(sequence_input2)\nlstm3 = CuDNNLSTM(500, return_sequences=True)(embedding_layer_dynamic2)\ndrop5 = Dropout(0.5)(lstm3)\nlstm4 = CuDNNLSTM(200)(drop5)\ndrop6 = Dropout(0.5)(lstm4)\nout4 = Dense(num_classes, activation='softmax')(drop6)\nlstm_dynamic = Model(sequence_input2, out4)","ddaf6b6c":"lstm_dynamic.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc', top_3_acc])\nlstm_dynamic_history = lstm_dynamic.fit_generator(dataseq(X_train, y_train, batch_size, 'pre'), epochs=4, verbose=2,\n                              validation_data = dataseq(X_holdout, y_holdout, batch_size, 'pre'))","cd557dd6":"plt.plot(lstm_dynamic_history.history['loss'], label='train')\nplt.plot(lstm_dynamic_history.history['val_loss'], label='test')\nplt.title('LSTM - Dynamic learning curve')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(loc='upper right')\nplt.show()","ed124c25":"lstm_dynamic.evaluate_generator(dataseq(X_test, y_test, batch_size, 'pre'))","12cfb115":"models = [cnn_static, cnn_dynamic, lstm_static, lstm_dynamic]\n\nfor i in range(len(models)):\n    for layer in models[i].layers:\n        layer.trainable = False\n\ninput_layers = [sequence_input1, sequence_input2]\noutput_layers = [model.output for model in models]\nensemble_merge = Concatenate()(output_layers)\nensemble_dense1 = Dense(120, activation='relu')(ensemble_merge)\nensemble_dense2 = Dense(80, activation='relu')(ensemble_dense1)\noutput = Dense(num_classes, activation='softmax')(ensemble_dense2)\nmodel = Model(inputs=input_layers, outputs=output)","60f1d50c":"class meta_dataseq(Sequence):\n    def __init__(self, X, y, batch_size):\n        self.x, self.y = X, y\n        self.batch_size = batch_size\n        self.m = len(self.y)\n\n    def __len__(self):\n        return int(np.ceil(len(self.x) \/ float(self.batch_size)))\n\n    def __getitem__(self, idx):\n        batch_x = self.x[idx * self.batch_size:min((idx + 1) * self.batch_size, self.m)]\n        batch_y = to_categorical(self.y[idx * self.batch_size:min((idx + 1) * self.batch_size, self.m)],\n                                 num_classes=num_classes)\n        \n        return [pad_sequences(batch_x, maxlen=max_length, truncating='post', padding='post'),\n                 pad_sequences(batch_x, maxlen=max_length, truncating='post', padding='pre')], batch_y\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc', top_3_acc])\nmodel_history = model.fit_generator(meta_dataseq(X_holdout, y_holdout, batch_size), epochs=20, verbose=2,\n                              validation_data = meta_dataseq(X_test, y_test, batch_size))","30103d6e":"plt.plot(model_history.history['loss'], label='train')\nplt.plot(model_history.history['val_loss'], label='test')\nplt.title('Meta-classifer learning curve')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(loc='upper right')\nplt.show()","c3d5bba8":"plot_model(model, to_file='model.png')\nmodel_json = model.to_json()\nwith open('model.json', 'w') as json_file:\n    json_file.write(model_json)\nmodel.save_weights('model.h5')\nprint('Saved model to disk')","bd586022":"**Ensemble**","dc6dfb1f":"**LSTM - Static**","df411471":"**LSTM - Dynamic**","741502e9":"**CNN - Dynamic**","33de529b":"**CNN - Static**"}}