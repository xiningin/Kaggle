{"cell_type":{"f204558d":"code","635c1a8e":"code","b6997239":"code","abb739a8":"code","25be1e39":"code","2d2768f0":"code","4aec82b1":"code","3781a3f6":"code","794ff160":"code","93adc4f5":"code","651822ec":"code","7327c68e":"code","8a7aeafc":"code","4a99e2be":"code","08c558de":"code","92c7bdc7":"code","945b54d0":"code","b6885ce8":"code","b28c8315":"code","46901b64":"code","4dae717b":"code","fd2a2a0e":"code","c7f5243f":"code","b6243ba2":"code","9d08e58a":"code","2f598c6f":"code","e15eda11":"code","9c725d1e":"code","68e5eb7f":"code","ba4b18f2":"code","57a448cf":"code","a84fb001":"code","879f55d5":"code","2dc6269b":"code","1ecefd22":"code","1bc31ebd":"code","0d39898b":"code","db87f580":"code","37f9b0d1":"code","df3e855b":"code","4403758c":"code","d6068a42":"code","41112630":"code","5b733a56":"code","2258aa98":"code","775d3cfe":"code","c6790898":"code","5cccc4e0":"code","228fee5e":"code","a3ff21b4":"code","c6c8b6b6":"code","a264ab2a":"markdown","a9abb346":"markdown","b9560dc9":"markdown","6f1bfb33":"markdown","00818824":"markdown","8b293e0f":"markdown","8b763fd4":"markdown","48c7ec08":"markdown","2ff533a3":"markdown","22d8dd28":"markdown","06dde069":"markdown","e932b2c0":"markdown","dfb7111f":"markdown","69844539":"markdown","d3f11a1f":"markdown","f50be4e5":"markdown","2b172b1e":"markdown","d358412f":"markdown","eb06335d":"markdown","8e2e7c15":"markdown","27ddc8e1":"markdown","82415941":"markdown","b89a6431":"markdown","e373db1d":"markdown","29e2115c":"markdown","245cf4af":"markdown","7e13a08f":"markdown","b92ed3da":"markdown","3c1c1416":"markdown","cf66f0d1":"markdown","9b839b98":"markdown","f558d900":"markdown","b275dcbe":"markdown","db1d4195":"markdown","516814c0":"markdown","8e988dc7":"markdown","7d28d7fd":"markdown","2637d874":"markdown","4475d63a":"markdown","55c82bd3":"markdown","4f1893ef":"markdown","42342b5e":"markdown","e256ddd3":"markdown","d0ba2942":"markdown","c35f341b":"markdown","c938b26c":"markdown"},"source":{"f204558d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler,OneHotEncoder, OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom collections import Counter\nfrom xgboost import XGBRegressor\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","635c1a8e":"filePath1 = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\"\nfilePath2 = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\"\n\nhome_train = pd.read_csv(filePath1,index_col=\"Id\")\nhome_test = pd.read_csv(filePath2,index_col=\"Id\")","b6997239":"home_train.head()","abb739a8":"home_train.shape","25be1e39":"home_test.shape","2d2768f0":"home_train.columns","4aec82b1":"numCol=home_train.select_dtypes(exclude=\"object\").columns\nnumCol","3781a3f6":"len(numCol)","794ff160":"home_train.select_dtypes(exclude=\"object\").describe()","93adc4f5":"catCol = home_train.select_dtypes(include=\"object\").columns\ncatCol","651822ec":"home_train.select_dtypes(include=\"object\").describe()","7327c68e":"len(catCol)","8a7aeafc":"target = home_train.SalePrice\nsns.distplot(target)\nplt.title(\"Examine sales prices and it's skew\")\nplt.show()","4a99e2be":"num_attributes=home_train.select_dtypes(exclude=\"object\").drop(\"SalePrice\",axis=1).copy()\n\nfig = plt.figure(figsize=(12,18))\n\nfor i in range(len(num_attributes.columns)):\n    try:\n        fig.add_subplot(9,4,i+1)\n        sns.distplot(num_attributes.iloc[:,i].dropna(),kde=True,rug=True)\n        plt.xlabel(num_attributes.columns[i])\n    except ValueError and RuntimeError:\n        pass\n\nplt.tight_layout()\nplt.show()","08c558de":"f = plt.figure(figsize=(12,18))\n\n\nfor i in range(len(num_attributes.columns)):\n    try:\n        f.add_subplot(9,4,i+1)\n        sns.boxplot(num_attributes.iloc[:,i])\n    except ValueError and RuntimeError:\n        pass\n\nplt.tight_layout()\nplt.show()","92c7bdc7":"f = plt.figure(figsize=(12,18))\n\n\nfor i in range(len(num_attributes.columns)):\n    try:\n        f.add_subplot(9,4,i+1)\n        sns.scatterplot(num_attributes.iloc[:,i],target)\n    except ValueError and RuntimeError:\n        pass\n\nplt.tight_layout()\nplt.show()","945b54d0":"f = plt.figure(figsize=(15,10))\nsns.heatmap(home_train.corr(),annot=False,fmt=\".2f\")\nplt.show()","b6885ce8":"correlation = home_train.corr()\n\ncorrelation[\"SalePrice\"].sort_values(ascending=False).head(16)","b28c8315":"catCol = home_train.select_dtypes(include=\"object\").columns\ncatCol","46901b64":"g=sns.factorplot(x=\"OverallQual\",y=\"SalePrice\",data=home_train,kind=\"bar\",size=6)\ng.set_ylabels(\"Sale Prices\")\ng.add_legend()\nplt.xticks(rotation=45)\nplt.show()","4dae717b":"g=sns.factorplot(x=\"Neighborhood\",y=\"SalePrice\",data=home_train,kind=\"box\",size=6)\ng.set_ylabels(\"Sale Prices\")\ng.add_legend()\nplt.xticks(rotation=45)\nplt.show()","fd2a2a0e":"g=sns.factorplot(x=\"Neighborhood\",y=\"SalePrice\",data=home_train,kind=\"bar\",size=6)\ng.set_ylabels(\"Sale Prices\")\ng.add_legend()\nplt.xticks(rotation=45)\nplt.show()","c7f5243f":"g=sns.factorplot(x=\"HouseStyle\",y=\"SalePrice\",data=home_train,kind=\"bar\",size=6)\ng.set_ylabels(\"Sale Prices\")\ng.add_legend()\nplt.xticks(rotation=45)\nplt.show()","b6243ba2":"def detectOutliers(df,features):\n    outlier_indices=[]\n    for c in features:\n        Q1=np.percentile(df[c],25)\n        Q2=np.percentile(df[c],75)\n        IQR = Q2-Q1\n        outlierStep = IQR*1.5\n        outlierListCol = df[(df[c] < Q1-outlierStep) | (df[c]>Q2+outlierStep)].index\n        outlier_indices.extend(outlierListCol)\n    outlier_indices=Counter(outlier_indices)\n    multiple_outliers=list(i for i,v in outlier_indices.items() if v>2)\n    return multiple_outliers","9d08e58a":"outliers=home_train.loc[detectOutliers(home_train,numCol)]\noutliers","2f598c6f":"outliers.shape","e15eda11":"home_train.dropna(axis=0,subset=[\"SalePrice\"],inplace=True)\ny = home_train.SalePrice\nhome_train.drop([\"SalePrice\"],axis=1,inplace=True)\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(home_train,y,train_size=0.8,test_size=0.2,random_state=0)","9c725d1e":"print(home_train.shape[0],home_test.shape[0])","68e5eb7f":"[col for col in home_train.columns if col not in home_test.columns]","ba4b18f2":"categorical_col = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and X_train_full[cname].dtype==\"object\"]\nnumerical_col = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in [\"int64\",\"float64\"]]\n\nmissing_columns_values = X_train_full.isnull().sum()\nprint(\"Columns list which has missing values : \\n{}\".format(missing_columns_values[missing_columns_values>0]))","57a448cf":"missing_numeric_columns_values = X_train_full[numerical_col].isnull().sum()\nprint(\"Numerical Columns list which has missing values : \\n{}\".format(missing_numeric_columns_values[missing_numeric_columns_values>0]))","a84fb001":"constant_num_cols = ['GarageYrBlt', 'MasVnrArea',\"LotFrontage\"]\n# I imputate them how using neededNumCols.I calculate mean of neededNumCols and assing to constant_num_cols\n\nmeanNumCols = list(set(numerical_col).difference(constant_num_cols))\n\nconstant_categorical_cols = ['Alley', 'MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu',\n                             'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature']\n# Samely , I imputate how most frequently which categotycal data\n\nmostFrqCol = list(set(categorical_col).difference(constant_categorical_cols))  # I imputate as most frequently with pipeline\n\nmy_cols = constant_num_cols+meanNumCols+constant_categorical_cols+mostFrqCol","879f55d5":"X_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = home_test[my_cols].copy()","2dc6269b":"numerical_transformer_m = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())])\n\nnumerical_transformer_c = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n    ('scaler', StandardScaler())])\n\n\n\ncategorical_transformer_mf = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown = 'ignore', sparse = False))\n])\n\n\ncategorical_transformer_c = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='NA')),\n    ('onehot', OneHotEncoder(handle_unknown = 'ignore', sparse = False))\n])\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num_mean', numerical_transformer_m, meanNumCols),\n        ('num_constant', numerical_transformer_c, constant_num_cols),\n        ('cat_mf', categorical_transformer_mf, mostFrqCol),\n        ('cat_c', categorical_transformer_c, constant_categorical_cols)\n    ])","1ecefd22":"model = RandomForestRegressor(n_estimators=100,random_state=0)\n\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ])\n\nmy_pipeline.fit(X_train,y_train)\n\npreds = my_pipeline.predict(X_valid)\n\nprint('MAE:', mean_absolute_error(y_valid, preds))","1bc31ebd":"scores = -1 * cross_val_score(my_pipeline,X_train,y_train,cv=5,scoring='neg_mean_absolute_error')\nscores","0d39898b":"def get_score(n_estimators):\n    my_pipeline = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        (\"model\",RandomForestRegressor(n_estimators=n_estimators,random_state=0))\n    ])\n    scores = -1 * cross_val_score(my_pipeline,X_train,y_train,cv=3,scoring='neg_mean_absolute_error')\n    return scores.mean()","db87f580":"estimators = np.arange(50,450,50)\n\nresults = {}\n\nfor i in range(1,9):\n    results[i*50] = get_score(i*50)","37f9b0d1":"results","df3e855b":"plt.plot(list(results.keys()), list(results.values()))\nplt.show()","4403758c":"model = RandomForestRegressor(n_estimators=350,random_state=0)\n\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ])\n\nmy_pipeline.fit(X_train,y_train)\n\npreds = my_pipeline.predict(X_valid)\n\nprint('MAE:', mean_absolute_error(y_valid, preds))","d6068a42":"my_second_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\nmy_second_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', my_second_model)\n                     ])\nmy_second_pipeline.fit(X_train,y_train)\n\npreds2 = my_second_pipeline.predict(X_valid)\nprint('MAE:', mean_absolute_error(y_valid, preds2))","41112630":"def get_score2(n_estimators):\n    my_pipeline2 = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        (\"model\",XGBRegressor(n_estimators=n_estimators,learning_rate=0.05, n_jobs=4))\n    ])\n    scores2 = -1 * cross_val_score(my_pipeline2,X_train,y_train,cv=3,scoring='neg_mean_absolute_error')\n    return scores2.mean()","5b733a56":"estimators2 = np.arange(50,1600,50)\n\nresults2 = {}\n\nfor i in range(1,30):\n    results2[i*50] = get_score2(i*50)","2258aa98":"sorted(results2.items(),key=lambda x:x[1])","775d3cfe":"plt.plot(list(results2.keys()),list(results2.values()))\nplt.show()","c6790898":"my_second_model = XGBRegressor(n_estimators=350, learning_rate=0.05, n_jobs=4)\nmy_second_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', my_second_model)\n                     ])\nmy_second_pipeline.fit(X_train,y_train)\n\npreds2 = my_second_pipeline.predict(X_valid)\nprint('MAE:', mean_absolute_error(y_valid, preds2))","5cccc4e0":"X=home_train.copy()\nX_test = home_test.copy()\n\nX_tr = X[my_cols].copy()\nX_te = X_test[my_cols].copy()","228fee5e":"my_second_model = XGBRegressor(n_estimators=350, learning_rate=0.05, n_jobs=4)\nmy_second_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', my_second_model)\n                     ])\nmy_second_pipeline.fit(X_tr,y)\n\npreds2 = my_second_pipeline.predict(X_te)\n","a3ff21b4":"output = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds2})","c6c8b6b6":"compression_opts = dict(method=\"zip\",archive_name=\"submission.csv\")\noutput.to_csv(\"submission.zip\",index=False,compression=compression_opts)","a264ab2a":"### Way to improvementing model called Cross Validation","a9abb346":"<a id=\"12\"><\/a>\n## XGBoost","b9560dc9":"**As you know , we make everything to reach the best result,and I am going to use another model to get best solution called XGBoost**","6f1bfb33":"### Now , finally I will predict with test data with my model","00818824":"**Now,I will start to preprocessing.Up to now , we made visualize and overview data e.g but We will jump to preprocessing ,create model and training them slightly.**","8b293e0f":"**Now , I will develop my model with pipeline , and handle missing values SimpleImputer and One-Hot-Encoder inside pipeline.In other words , I will preprocessing for numerical and categorical values**","8b763fd4":"**Find categorical data**","48c7ec08":"**Distributions of others attributes**\n\n* with distplot\n* with boxplot\n* with scatter\n* with heatmap","2ff533a3":"**I will prefer ensemle leraning members models because they give more small error.Like random foresst or XGBoost e.g. reason why they use lots of different methods to implement data**","22d8dd28":"<a id=\"5\"><\/a>\n### Outliers Detection","06dde069":"<a id=\"1\"><\/a>\n## Imports Library","e932b2c0":"I will detect outliers indexes in data , this is one of the important preprocessing steps.","dfb7111f":"If data is a numerical imputate constant or mean , else imputate constant or most frequent them.","69844539":"I am going to different methods for imputation columns do how divide according to whether has missing values.Then imputate values as constant,mean or most frequently.","d3f11a1f":"<a id=\"7\"><\/a>\n## Define Model","f50be4e5":"**One of the effectively way to evulate your model is cross-validation.And I am going to test my moel with them**","2b172b1e":"**Change Model Parameters**","d358412f":"<a id=\"4\"><\/a>\n## Data Cleaning & Preprocessing","eb06335d":"<a id=\"15\"><\/a>\n## Final Predict Process","8e2e7c15":"<a id=\"ek2\"><\/a>\n### Overview Data","27ddc8e1":"**Examine sales prices(target) on plot.This result,plot is important for some algorithms**","82415941":"Firstly I will use random forest model","b89a6431":"### It's time refresh our model","e373db1d":"**Describing numerical features**","29e2115c":"<a id=\"3\"><\/a>\n### Investigate Numerical Columns","245cf4af":"## Trying Another Model","7e13a08f":"Inside the codes, I used try - except structer ,it's reason why before cleaning data I have some error.So ,I have to use them.","b92ed3da":"<a id=\"ek1\"><\/a>\n#### Reading Input File","3c1c1416":"<a id=\"11\"><\/a>\n### Change Model Parameters","cf66f0d1":"<a id=\"ek3\"><\/a>\n### Univariate Variable Analysis","9b839b98":"<a id=\"ek4\"><\/a>\n### Investigate Categorical Columns","f558d900":"<a id=\"6\"><\/a>\n### Preprocessing","b275dcbe":"<a id=\"8\"><\/a>\n## Random Forest","db1d4195":"We get whole columns exclude which as objects","516814c0":"<a id=\"13\"><\/a>\n## Compare Parameters","8e988dc7":"We get objects columns","7d28d7fd":"<a id=\"2\"><\/a>\n### Exploratory Data Analysis\n\nThis section occur these steps :\n\n* Overview Data\n* Explore Numerical Data\n* Explore Categorical Data\n* Investigate Numerical Columns\n* Investigate Categorical Columns\n* Explore correlation between columns\n* Outliers Detection\n* Find missing\/null values in numerical columns","2637d874":"<a id=\"9\"><\/a>\n## Cross Validation\n","4475d63a":"I will use plot to see estimator's results and decided to use inside.","55c82bd3":"I will not drop outliers from data because Unfortunately, we can only get a large validation set by removing rows from our training data, and smaller training datasets mean worse models!.","4f1893ef":"**Look at whole columns**","42342b5e":"**Find Numerical Columns**","e256ddd3":"**I will show correlation between target columns to others**","d0ba2942":"# Introduction\n\nThis kernel is gonna be my first competition kernel,so I'm excited and I am going to try explain understandable visualize and predict and I will deploy that I know.I aimed that improve my model with spesicif parameters(cross validation,fitting e.g).If I mention content of competitions , I will try to predict houses's sale according to given features.Actually it's gonna be nice.Then,let's start.\n\n### Content\n\n1. [Imports Library](#1)\n    * [Reading Input File](#ek1)\n2. [Exploratory Data Analysis](#2)\n    * [Overview Data](#ek2)\n    * [Investigate Numerical Columns](#3)\n        * [Univariate Variable Analysis](#ek3)\n    * [Investigate Categorical Columns](#ek4)\n3. [Data Cleaning & Preprocessing](#4)\n    * [Outliers Detection](#5)\n    * [Preprocessing](#6)\n    * [Define Model](#7)\n        * [Random Forest](#8)\n            * [Cross Validation](#9)\n            * [Compare Parameters](#10)\n            * [Change Model Parameters](#11)\n        * [XGBoost](#12)\n            * [Compare Parameters](#13)\n            * [Change Model Parameters](#14)\n    * [Final Predict Process](#15)       \n           ","c35f341b":"<a id=\"10\"><\/a>\n## Compare Parameters","c938b26c":"<a id=\"14\"><\/a>\n## Change Model Parameters"}}