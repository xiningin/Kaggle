{"cell_type":{"883a8758":"code","f26835fd":"code","0fc6e70c":"code","42291323":"code","c9045da0":"code","264c4e2b":"code","cd5a3b00":"code","9fc6d88a":"code","bcabc93f":"code","05d8917a":"code","815bd2fb":"code","ff6c5125":"code","6d50550e":"code","3a307880":"code","57fd5b9e":"code","994c47a5":"code","74c3bfa7":"markdown","31b8f7ee":"markdown","c108c8a4":"markdown","6de31b53":"markdown","231f3e11":"markdown","8ac36fed":"markdown","a66e5697":"markdown","7b09ae9a":"markdown"},"source":{"883a8758":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom tqdm.notebook import tqdm\nimport gzip\nimport pickle\nfrom timeit import time","f26835fd":"# IR metrics \n\ndef mean_reciprocal_rank(rs):\n    \"\"\"Score is reciprocal of the rank of the first relevant item\n    First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n    Example from http:\/\/en.wikipedia.org\/wiki\/Mean_reciprocal_rank\n    >>> rs = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n    >>> mean_reciprocal_rank(rs)\n    0.61111111111111105\n    >>> rs = np.array([[0, 0, 0], [0, 1, 0], [1, 0, 0]])\n    >>> mean_reciprocal_rank(rs)\n    0.5\n    >>> rs = [[0, 0, 0, 1], [1, 0, 0], [1, 0, 0]]\n    >>> mean_reciprocal_rank(rs)\n    0.75\n    Args:\n        rs: Iterator of relevance scores (list or numpy) in rank order\n            (first element is the first item)\n    Returns:\n        Mean reciprocal rank\n    \"\"\"\n    rs = (np.asarray(r).nonzero()[0] for r in rs)\n    return np.mean([1. \/ (r[0] + 1) if r.size else 0. for r in rs])\n\n\ndef r_precision(r):\n    \"\"\"Score is precision after all relevant documents have been retrieved\n    Relevance is binary (nonzero is relevant).\n    >>> r = [0, 0, 1]\n    >>> r_precision(r)\n    0.33333333333333331\n    >>> r = [0, 1, 0]\n    >>> r_precision(r)\n    0.5\n    >>> r = [1, 0, 0]\n    >>> r_precision(r)\n    1.0\n    Args:\n        r: Relevance scores (list or numpy) in rank order\n            (first element is the first item)\n    Returns:\n        R Precision\n    \"\"\"\n    r = np.asarray(r) != 0\n    z = r.nonzero()[0]\n    if not z.size:\n        return 0.\n    return np.mean(r[:z[-1] + 1])\n\n\ndef precision_at_k(r, k):\n    \"\"\"Score is precision @ k\n    Relevance is binary (nonzero is relevant).\n    >>> r = [0, 0, 1]\n    >>> precision_at_k(r, 1)\n    0.0\n    >>> precision_at_k(r, 2)\n    0.0\n    >>> precision_at_k(r, 3)\n    0.33333333333333331\n    >>> precision_at_k(r, 4)\n    Traceback (most recent call last):\n        File \"<stdin>\", line 1, in ?\n    ValueError: Relevance score length < k\n    Args:\n        r: Relevance scores (list or numpy) in rank order\n            (first element is the first item)\n    Returns:\n        Precision @ k\n    Raises:\n        ValueError: len(r) must be >= k\n    \"\"\"\n    assert k >= 1\n    r = np.asarray(r)[:k] != 0\n    if r.size != k:\n        raise ValueError('Relevance score length < k')\n    return np.mean(r)\n\n\ndef average_precision(r):\n    \"\"\"Score is average precision (area under PR curve)\n    Relevance is binary (nonzero is relevant).\n    >>> r = [1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n    >>> delta_r = 1. \/ sum(r)\n    >>> sum([sum(r[:x + 1]) \/ (x + 1.) * delta_r for x, y in enumerate(r) if y])\n    0.7833333333333333\n    >>> average_precision(r)\n    0.78333333333333333\n    Args:\n        r: Relevance scores (list or numpy) in rank order\n            (first element is the first item)\n    Returns:\n        Average precision\n    \"\"\"\n    r = np.asarray(r) != 0\n    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n    if not out:\n        return 0.\n    return np.mean(out)\n\n\ndef mean_average_precision(rs):\n    \"\"\"Score is mean average precision\n    Relevance is binary (nonzero is relevant).\n    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1]]\n    >>> mean_average_precision(rs)\n    0.78333333333333333\n    >>> rs = [[1, 1, 0, 1, 0, 1, 0, 0, 0, 1], [0]]\n    >>> mean_average_precision(rs)\n    0.39166666666666666\n    Args:\n        rs: Iterator of relevance scores (list or numpy) in rank order\n            (first element is the first item)\n    Returns:\n        Mean average precision\n    \"\"\"\n    return np.mean([average_precision(r) for r in rs])\n\n\ndef dcg_at_k(r, k, method=0):\n    \"\"\"Score is discounted cumulative gain (dcg)\n    Relevance is positive real values.  Can use binary\n    as the previous methods.\n    Example from\n    http:\/\/www.stanford.edu\/class\/cs276\/handouts\/EvaluationNew-handout-6-per.pdf\n    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n    >>> dcg_at_k(r, 1)\n    3.0\n    >>> dcg_at_k(r, 1, method=1)\n    3.0\n    >>> dcg_at_k(r, 2)\n    5.0\n    >>> dcg_at_k(r, 2, method=1)\n    4.2618595071429155\n    >>> dcg_at_k(r, 10)\n    9.6051177391888114\n    >>> dcg_at_k(r, 11)\n    9.6051177391888114\n    Args:\n        r: Relevance scores (list or numpy) in rank order\n            (first element is the first item)\n        k: Number of results to consider\n        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n    Returns:\n        Discounted cumulative gain\n    \"\"\"\n    r = np.asfarray(r)[:k]\n    if r.size:\n        if method == 0:\n            return r[0] + np.sum(r[1:] \/ np.log2(np.arange(2, r.size + 1)))\n        elif method == 1:\n            return np.sum(r \/ np.log2(np.arange(2, r.size + 2)))\n        else:\n            raise ValueError('method must be 0 or 1.')\n    return 0.\n\n\ndef ndcg_at_k(r, k, method=0):\n    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n    Relevance is positive real values.  Can use binary\n    as the previous methods.\n    Example from\n    http:\/\/www.stanford.edu\/class\/cs276\/handouts\/EvaluationNew-handout-6-per.pdf\n    >>> r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n    >>> ndcg_at_k(r, 1)\n    1.0\n    >>> r = [2, 1, 2, 0]\n    >>> ndcg_at_k(r, 4)\n    0.9203032077642922\n    >>> ndcg_at_k(r, 4, method=1)\n    0.96519546960144276\n    >>> ndcg_at_k([0], 1)\n    0.0\n    >>> ndcg_at_k([1], 2)\n    1.0\n    Args:\n        r: Relevance scores (list or numpy) in rank order\n            (first element is the first item)\n        k: Number of results to consider\n        method: If 0 then weights are [1.0, 1.0, 0.6309, 0.5, 0.4307, ...]\n                If 1 then weights are [1.0, 0.6309, 0.5, 0.4307, ...]\n    Returns:\n        Normalized discounted cumulative gain\n    \"\"\"\n    dcg_max = dcg_at_k(sorted(r, reverse=True), k, method)\n    if not dcg_max:\n        return 0.\n    return dcg_at_k(r, k, method) \/ dcg_max","0fc6e70c":"def show_side_by_side(figs, limit = 10, figsize=(20, 4), grid = False, titles = None,\n                      norm = mpl.colors.Normalize(0, 1, clip=True), cmap = None):\n    ''' Show images side by side. Each line contains limit images. Each image\n        may have a title. \n    '''\n    minval = min(limit, figs.shape[0])\n    plt.figure(figsize = figsize)\n    for i in range(minval):\n        subplot = plt.subplot(1, limit, i + 1)\n        extent = (0, figs[i].shape[1], figs[i].shape[0], 0)\n        subplot.imshow(figs[i], extent = extent, norm = norm, cmap = cmap)\n        if titles is not None:\n            subplot.set_title(titles[i])\n        \n        if grid:\n            subplot.grid(color='gray', linestyle='-', linewidth=1)\n        else:\n            subplot.get_xaxis().set_visible(False)\n            subplot.get_yaxis().set_visible(False)\n\n    plt.show()","42291323":"def save_zipped_pickle(obj, filename, protocol=-1):\n    with open(filename, 'wb') as f:\n        pickle.dump(obj, f, protocol)\n\ndef load_zipped_pickle(filename):\n    with open(filename, 'rb') as f:\n        loaded_object = pickle.load(f)\n        return loaded_object\n\ndef plotimages(imgs, n_row = 1, n_col = 5, cmap='gray', titles = None):\n    _, axs = plt.subplots(n_row, n_col, figsize=(12, 6))\n    axs = axs.flatten()\n    for i, (img, ax) in enumerate(zip(imgs, axs)):\n        if titles is not None:\n            ax.set_title(titles[i])\n        ax.imshow(img, cmap=cmap)\n        ax.axis('off')\n    plt.show()","c9045da0":"saved_file_train = '..\/input\/triplet\/multi0123_train.pck'\nsaved_file_test = '..\/input\/triplet\/multi0123_test.pck'\n\n(data_train, labels_train, hot_label_train) = load_zipped_pickle(saved_file_train)\n(data_test, labels_test, hot_labels_test) = load_zipped_pickle(saved_file_test)","264c4e2b":"X = data_train.reshape(53952,28,28)\nidxs = np.random.choice(range(X.shape[0]), size=10, replace=False)\n\nsample = X[idxs]\nplotimages(sample, n_row=2, titles = labels_train[idxs])","cd5a3b00":"EPS = 1e-10   # a constant used to avoid zero in log operations","9fc6d88a":"# amostra da GS. Note que ele tem a opcao de fazer discretiza\u00e7\u00e3o na \u00faltima fun\u00e7\u00e3o\n# Note que nada disso eh usado neste c\u00f3digo\n\ndef sample_gumbel(shape, eps=1e-20):\n    \"\"\"Sample from Gumbel(0, 1)\"\"\"\n    U = tf.random.uniform(shape, minval=0, maxval=1)\n    return -tf.math.log(-tf.math.log(U + eps) + eps)\n\ndef gumbel_softmax_sample(logits, temperature):\n    \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n    y = logits + sample_gumbel(tf.shape(logits))\n    return tf.nn.softmax( y \/ temperature)\n\ndef gumbel_softmax(logits, temperature, hard=False):\n    \"\"\"Sample from the Gumbel-Softmax distribution and optionally discretize\"\"\"\n    y = gumbel_softmax_sample(logits, temperature)\n    if hard:\n        k = tf.shape(logits)[-1]\n        y_hard = tf.cast(tf.equal(y, tf.reduce_max(y, -1, keepdims=True)), tf.float32)\n        y = tf.stop_gradient(y_hard - y) + y\n    return y\n\ndef l2f(v):\n    return tf.reduce_sum(tf.pow(v, 2), axis = 1) \/ 2","bcabc93f":"class pnash(tf.keras.Model):\n    \n    def __init__(self, latent_dim, img_size, nlabels, temperature=1.0,\\\n                 keep_prob=0.5, learning_rate=0.001, decay_rate=0.96):\n        \n        super(pnash, self).__init__()\n        \n        self.latent_dim = latent_dim    # lantent dimension Z\n        self.img_size = img_size    # hidden dimension      \n        self.nlabels = nlabels          # label count\n        self.gamma = 0.5                # param for Eq 5, KL cost\n        self.tau = temperature          # gumbel softmax param\n        self.keep_prob = keep_prob      # Dropout param\n        self.use_cross_entropy = True\n\n        # inference network q(z|x)\n        # input -> Dense(hidden_dim) | Dense(hidden_dim) | Dropout | Dense(latent_dim) -> z\n      \n        \n        try:\n            #salve os pesos \n            self.encoder = tf.keras.models.load_model()\n            self.pred_label = tf.keras.models.load_model()\n            self.decoder = tf.keras.models.load_model()\n            \n            print('Peso carregado!')\n        except:\n            print('Peso n\u00e3o encontrado!')\n            self.encoder = self.create_encoder()\n            self.pred_label = self.create_pred_label()\n            self.decoder = self.create_decoder()\n            \n     # optimizer\n        lr = 0.0001 #tf.train.exponential_decay(learning_rate, step, 10000, decay_rate, staircase=True, name=\"lr\")\n        self.optimizer1 = tf.optimizers.Adam(learning_rate = lr)\n        self.optimizer2 = tf.optimizers.Adam(learning_rate = lr)\n    \n    def create_encoder(self):\n        x_input = tf.keras.layers.Input(shape=(28,28,1))\n\n        x = tf.keras.layers.Conv2D(64, (3,3), padding=\"same\", strides=2)(x_input)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.LeakyReLU()(x)\n\n        x = tf.keras.layers.Conv2D(64, (3,3), padding=\"same\", strides=2)(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.LeakyReLU()(x)\n\n        x = tf.keras.layers.Conv2D(64, (3,3), padding=\"same\", strides=1)(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.LeakyReLU()(x)\n\n        x = tf.keras.layers.Flatten()(x)\n        x = tf.keras.layers.Dense(512)(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.LeakyReLU()(x)\n\n        out = tf.keras.layers.Dense(1*self.latent_dim)(x)\n\n       \n        rede = tf.keras.models.Model(inputs = x_input, outputs = out)\n        \n        return rede\n\n    def create_pred_label(self):\n        x_input = tf.keras.layers.Input(shape=(self.latent_dim))\n\n\n        out = tf.keras.layers.Dense(self.nlabels, activation=\"sigmoid\",\\\n                  kernel_initializer = tf.random_uniform_initializer(minval = -np.sqrt(6.0 \/ (2*self.latent_dim + self.nlabels)),\\\n                  ),\\\n                  bias_initializer=tf.constant_initializer(0.0))(x_input)\n\n        pred_rede = tf.keras.models.Model(inputs = x_input, outputs  = out)\n\n        return pred_rede\n\n    def create_decoder(self):\n        x_input = tf.keras.layers.Input(shape=(self.latent_dim))\n        x = tf.keras.layers.Dense(7*7*64)(x_input)\n        x = tf.keras.layers.Reshape(target_shape = (7,7,64))(x)\n\n        x = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=(3,3),padding=\"same\", strides=2)(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.LeakyReLU()(x)\n\n        x = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=(3,3), padding=\"same\", strides=2)(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.LeakyReLU()(x)\n\n\n        x = tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=(3,3), activation='relu', padding=\"same\", strides=1)(x)\n        out = tf.keras.layers.LeakyReLU()(x)\n\n        modelo = tf.keras.models.Model(inputs = x_input, outputs = out)\n\n        return modelo\n\n    \n    def reconst_err(self, logprob_word, docs):\n        'erro de reconstru\u00e7\u00e3o'\n        return -tf.reduce_sum(logprob_word * docs, axis=[1,2,3], keepdims=False)\n    \n    def kl_loss(self, q_z, cat_dim=2):\n        'loss de Kullback-Leibler -- n eh usada neste codigo'\n        log_q_z = tf.math.log(q_z + EPS)\n        return tf.reduce_mean(tf.reduce_sum(q_z * (log_q_z - tf.math.log(self.gamma)), axis=-1))\n    \n    def pred_loss(self, labels, tag_prob):\n        if not self.use_cross_entropy:\n            return tf.reduce_sum(tf.pow(tag_prob - labels, 2), axis=1)\n        else:\n            return -tf.reduce_mean(tf.reduce_sum(labels * tf.math.log(tf.maximum(tag_prob, 1e-10))\\\n                           + (1 - labels) * tf.math.log(tf.maximum(1 - tag_prob, 1e-10)), axis=1))\n    \n    def arm_grad(self, docs, mu):\n          # get docs' codes (z)\n        q = self.encoder(docs)\n        \n        score1 = tf.sigmoid(-q)\n        z1 = 0.5 * (tf.sign(mu - score1) + 1.0) # sequence of 0s, 1s according to prob score\n        # get probs of pixels\/words in reconstructed doc\/img decoded from z\n        logprob_w1 = tf.nn.log_softmax(self.decoder(z1), axis=-1)\n        # repeat the same for the 2o doc\/img\n        score2 = 1 - score1 # ?? -- check paper\n        z2 = 0.5 * (tf.sign(score2 - mu) + 1.0)\n        logprob_w2 = tf.nn.log_softmax(self.decoder(z2), axis=-1)\n      \n        # calculate reconstructions errors \n       \n        F1 = tf.reshape(self.reconst_err(logprob_w1, docs),[tf.shape(docs)[0],1])\n        F2 = tf.reshape(self.reconst_err(logprob_w2, docs),[tf.shape(docs)[0],1])\n        \n        # GRADIENTE WITH ARM\n        g = tf.tile(F1 - F2, [1, self.latent_dim]) * (mu - 0.5) # 1 X latent_dim\n        \n        g_delta = tf.convert_to_tensor(g, dtype=tf.float32)\n        \n        return g_delta\n\n    def update(self, docs1, docs2, sgn, oh_y1, oh_y2, kl_weight, sim_weight, tag_weight):\n        \n        docs1 = docs1\/255\n        docs1 = docs1.reshape(docs1.shape[0],28,28,1)\n        docs2 = docs2\/255\n        docs2 = docs2.reshape(docs2.shape[0],28,28,1)\n        \n        #print(\"Apos reshape\", docs2.shape)\n        \n        # get random matrix of numbers uniformely distributed in range 0 to 1\n        # matrix has shape given by (batch size, latent dim)\n        mu = tf.random.uniform([tf.shape(docs1)[0], self.latent_dim])\n        g_delta1 = self.arm_grad(docs1, mu)\n        g_delta2 = self.arm_grad(docs2, mu)\n        \n        # gradient tape indicates for TF which vars it should record\n        # to calculate the gradients later\n        with tf.GradientTape(persistent=True) as tape:\n            \n            # first doc\n            logits1 = self.encoder(docs1) # f_theta(x)_k\n            score1 = tf.sigmoid(logits1) # sigmoid(f_theta(x)_k)\n            z1 = 0.5 * (tf.sign(score1 - mu) + 1.0)\n            logprob_w1 = tf.nn.log_softmax(self.decoder(z1), axis=-1)\n            # Eq 5 for doc1\n            kl1 = tf.reduce_mean(tf.reduce_sum(score1 * (tf.math.log(score1 + EPS) - tf.math.log(self.gamma + EPS))\\\n                               + (1 - score1) * (tf.math.log(1 - score1 + EPS) - tf.math.log(1 - self.gamma + EPS))))\n            reconstr_err1 = tf.reduce_mean(self.reconst_err(logprob_w1, docs1))\n            \n            # second doc\n            logits2 = self.encoder(docs2)\n            score2 = tf.sigmoid(logits2)\n            z2 = 0.5 * (tf.sign(score2 - mu) + 1.0)\n            logprob_w2 = tf.nn.log_softmax(self.decoder(z2), axis=-1)\n            # Eq 5 for doc 2\n            kl2 = tf.reduce_mean(tf.reduce_sum(score2 * (tf.math.log(score2 + EPS) - tf.math.log(self.gamma + EPS))\\\n                               + (1 - score2) * (tf.math.log(1 - score2 + EPS) - tf.math.log(1 - self.gamma + EPS))))\n            reconstr_err2 = tf.reduce_mean(self.reconst_err(logprob_w2, docs2))\n            \n            # build loss\n            #value = y1 & y2, axis=1\n            sgn = 2 * tf.cast(sgn, tf.float32) - 1\n \n            #print(sgn)\n            tag_prob1 = self.pred_label(tf.stop_gradient(z1-score1)+score1)\n            #print(\"Tag_prob1\", len(tag_prob1))\n            #print(\"hotencoding\", oh_y1)\n            pred_loss1 = self.pred_loss(oh_y1, tag_prob1)\n            tag_prob2 = self.pred_label(tf.stop_gradient(z2-score2)+score2)\n            pred_loss2 = self.pred_loss(oh_y2, tag_prob2)\n            \n            reconstr_err = reconstr_err1 + reconstr_err2\n            \n            \n            # eq 8 ?\n            loss_fn = tf.reduce_mean(tf.reduce_sum(g_delta1 * logits1 +\n                                                   g_delta2 * logits2, axis=1)) +\\\n                      kl_weight * (kl1 + kl2) +\\\n                      tag_weight * (pred_loss1 + pred_loss2) +\\\n                      sim_weight * sgn * l2f(score1 - score2) # Eq 7 -- pairwise\n                      #sim_weight * sgn * tf.nn.l2_loss(score1 - score2) # Eq 7 -- pairwise\n\n            #tf.print(sgn, score1, score2, y1, y2)\n            #tf.print(sgn.shape, score1.shape, score2.shape, y1.shape, y2.shape)\n            #tf.print(sgn * tf.nn.l2_loss(score1 - score2))\n            #print(loss_fn)\n            #return\n            \n            loss_fn2 = reconstr_err + tag_weight * (pred_loss1 + pred_loss2)\n        \n        grad = tape.gradient(loss_fn , self.encoder.variables) \n        self.optimizer1.apply_gradients(\n        (grad, var) \n        for (grad, var) in zip(grad, self.encoder.variables) \n        if grad is not None\n        ) \n        # decoder variables\n        var_list = [v for v in self.decoder.variables]\n        for v in self.pred_label.variables:\n            var_list.append(v)\n        grad2 = tape.gradient(loss_fn2 , var_list) \n        self.optimizer2.apply_gradients(\n        (grad2, var) \n        for (grad2, var) in zip(grad2, var_list) \n        if grad2 is not None\n        ) \n        #print(loss_fn)\n        \n  \n    def transform(self, docs):\n      'transforma documento em c\u00f3digo: documento -> z'\n      z_data = []\n      for i in tqdm(range(len(docs))):\n          # get z code for each doc\n          doc = docs[i].reshape(28,28,1)\n          logits = self.encoder(doc[np.newaxis, :])\n          score = tf.sigmoid(logits)\n          mu = 0.5 #tf.random.uniform([1, self.latent_dim])\n          z = 0.5 * (tf.sign(score - mu) + 1.0)\n          z_data.append(z.numpy()[0])\n      return z_data\n","05d8917a":"def train(data_train, labels_train, hot_label_train):\n  # initialization\n  latent_dim = 128\n  img_size = 28\n  keep_prob = 0.8\n  nlabels = 4\n  #n_feas = 784\n  learner = pnash(latent_dim, img_size , nlabels, 1.0, keep_prob)\n\n  # labels\n  ntrain = len(data_train)\n  labels = labels_train\n\n  # training\n  nepoch = 50\n  nbatch = 200\n  kl_weight = 0.01\n  #sim_weight = 0.0009\n  sim_weight = 0.0\n  tag_weight = 0.1\n  tag_inc = 0.1\n  max_tag_weight = 10.\n  kl_inc = 1 \/ 5000. # set the annealing rate for KL loss\n  idxlist = list(range(ntrain))\n\n  for epoch in tqdm(range(nepoch)):\n      idx = np.random.permutation(ntrain)\n      np.random.shuffle(idxlist)\n      for i in range(0, ntrain, nbatch):\n          # sample 2 minibatches of data\n          end_idx = min(i + nbatch, ntrain)\n          docs1 = data_train[idxlist[i:end_idx]]\n          docs2 = data_train[idx[idxlist[i:end_idx]]]\n          \n          y1 = labels[idxlist[i:end_idx]]\n          y2 = labels[idx[i:end_idx]]\n          \n         \n          sgn = []\n          for k in range(0, len(y1)):\n              label1 = list(y1[k])\n              label2 = list(y2[k])\n\n              intersection = list(set(label1).intersection(label2))\n              union = list(set(label1).union(label2))\n\n              percent = 2*(len(intersection)\/len(union)) -1\n              sgn.append(percent)\n      \n          #sgn = 2 * tf.cast(y1==y2, tf.float32) - 1\n          # update weights\n          #print(docs1,\"\\n\")\n          #print(y2, \"\\n\")\n          \n          #print(labels_train[idxlist[i:end_idx]], \"\\n\")\n          #print(labels_train[idx[i:end_idx]], \"\\n\")\n          #sign = np.sum(y1 & y2, axis=1).astype(int)\n          learner.update(docs1, docs2, sgn, \n                        hot_label_train[idxlist[i:end_idx]], \n                        hot_label_train[idx[i:end_idx]],\n                        kl_weight, sim_weight, tag_weight)\n \n          #return\n          #kl_weight = min(kl_weight + kl_inc, 1.0)\n          tag_weight = min(tag_weight + tag_inc, max_tag_weight)\n          learner.tau = max(learner.tau * 0.96, 0.1)\n  return learner","815bd2fb":"learner = train(data_train, labels_train, hot_label_train)","ff6c5125":"# run experiment here\nzTrain = learner.transform(data_train\/255)\nzTest = learner.transform(data_test\/255)\nzTrain = np.array(zTrain)\nzTest = np.array(zTest)\n\ncbTrain = zTrain.astype(int) \ncbTest = zTest.astype(int) ","6d50550e":"class HashNumber(object):\n    ''' Represent a long integer value '''\n    \n    def __init__(self, bin_array):\n        self.n_bits = len(bin_array)\n        \n        if self.n_bits > 32:\n            assert(len(bin_array) % 32 == 0)  \n\n        self.bin_code = []\n        for i in range(0, len(bin_array), 32):\n            self.bin_code.append(self._bitarray_to_bytes(bin_array[i:i+32]))\n         \n    def distance(self, other):\n        assert(len(self.bin_code) == len(other.bin_code))\n        d = 0\n        for i in range(len(self.bin_code)):\n            d += self._hamming_distance(self.bin_code[i], other.bin_code[i])\n        return d\n    \n    def _bitarray_to_bytes(self, s):\n        intstr = ''.join([str(i) for i in s])\n        v = int(intstr, 2)\n        return v\n\n    def _hamming_distance(self, b1, b2):\n        return bin(b1^b2).count(\"1\")  #realiza opera\u00e7\u00e3o de ou exclusivo e conta o numero de 1s do valor resultante\n        \ndef run_topK_retrieval_experiment(codeTrain, codeTest, labels_train, gnd_train, labels_test, gnd_test,\n                                  data_train = None, data_test = None, \n                                  TopK=100):\n\n    y_train = gnd_train.astype(int)\n    y_test = gnd_test.astype(int)\n    \n    assert(codeTrain.shape[1] == codeTest.shape[1])\n    assert(y_train.shape[1] == y_test.shape[1])\n    assert(codeTrain.shape[0] == y_train.shape[0])\n    assert(codeTest.shape[0] == y_test.shape[0])\n    \n    cbTrain = [HashNumber(bitarray) for bitarray in codeTrain]\n    cbTest = [HashNumber(bitarray) for bitarray in codeTest]\n\n    p_at_k = []\n    avg_p = []\n    ndcg_score = []\n    bin_ndcg_score = []\n    avg_r = []\n\n    with tqdm(total=len(cbTest)) as pbar:\n        for idx, test_bin_code in enumerate(cbTest):\n            #print(\"Tamanho dos codigos binarios\", test_bin_code.bin_code)\n            Dist = np.array([test_bin_code.distance(bincode) for bincode in cbTrain]) #calcula as distancias entre as imagens de consulta e as imagens da base\n            #print(np.argsort(Dist))\n            TopDocIdx = np.argsort(Dist)[:TopK] #ordena os indices do array de acordo com os valores de distancias e seleciona as 10 primeiras\n            #print(\"distancia calculadas para a primeira imagem\", Dist[33835])\n\n            #if idx == 0:\n            #    print(codeTrain[TopDocIdx])\n\n            # show query and top-k answers if data_train is provided (show only\n            # the 10 first queries)\n            if (data_train is not None) and (idx < 10):\n                #print(idx, TopDocIdx, data_test[idx].shape, data_train[TopDocIdx].shape)  \n                dists = ['dist'] + [str(i) for i in Dist[TopDocIdx]]      \n                q_and_answers = np.concatenate((data_test[idx].reshape(1, -1), \n                                                data_train[TopDocIdx, :]))\n                show_side_by_side(q_and_answers.reshape(-1, 28, 28), \n                                  limit = 10 + 1, titles = dists, figsize=(10, 2))\n\n            # count number of matching labels\n            #num_matches = np.sum(y_test[idx] & y_train[TopDocIdx], axis=1).astype(int) #verifica os matches nos top-k\n            num_matches = []\n            label1 = list(labels_test[idx])\n            #print(\"label da query\", label1)\n            for j in  labels_train[TopDocIdx]:\n                label2 = list(j)\n\n                intersection = list(set(label1).intersection(label2))\n                union = list(set(label1).union(label2))\n\n                percent = (len(intersection)\/len(union))\n\n                #if(percent > 0):\n                num_matches.append(percent)\n\n            num_relevant_items = []\n            for k in  labels_train:\n                label2 = list(k)\n                \n                intersection = list(set(label1).intersection(label2))\n                union = list(set(label1).union(label2))\n\n                percent = (len(intersection)\/len(union))\n                if(percent > 0):\n                    num_relevant_items.append(percent)\n\n            #print(num_matches)\n            #print(num_relevant_items)\n            #break\n            #num_relevant_items = np.sum(np.sum(y_test[idx] & y_train, axis=1) > 0) #numero de items relevantes na base toda\n            num_matches = np.asarray(num_matches, dtype=np.float32)\n\n            #num_matches = np.sum(num_matches)\n            num_relevant_items = np.asarray(num_relevant_items, dtype=np.float32)\n            num_relevant_items = np.sum(num_relevant_items)\n            #print(\"matches\", num_matches)\n            #print(\"relevantes\", num_relevant_items)\n            #break\n\n            relevance = (num_matches > 0).astype(int)\n\n            # This measurement is Recall at K\n            if num_relevant_items > 0:\n                avg_r.append(np.sum(num_matches) \/ float(num_relevant_items))\n            else:\n                avg_r.append(0.)\n\n            p_at_k.append(precision_at_k(relevance, TopK))\n            avg_p.append(average_precision(relevance))\n            bin_ndcg_score.append(ndcg_at_k(relevance, TopK))\n            ndcg_score.append(ndcg_at_k(num_matches, TopK))\n            \n            pbar.update(1)\n\n    avg_prec_at_k = np.mean(p_at_k)\n    avg_recall_at_k = np.mean(avg_r) \n    avg_ndcg = np.mean(ndcg_score)\n    print('\\nPrec@K = {:.4f}, Recall@K = {:.4f}, NDCG@K = {:.4f}'.format(avg_prec_at_k, \n                                                                         avg_recall_at_k, \n                                                                         avg_ndcg))\n    return avg_prec_at_k, avg_recall_at_k, avg_ndcg","3a307880":"cbTrain.shape, cbTest.shape, hot_label_train.shape, hot_labels_test.shape","57fd5b9e":"TopK=100\nprint('Retrieve Top{} candidates using hamming distance'.format(TopK))\n\nresults = run_topK_retrieval_experiment(cbTrain, cbTest[0:999], labels_train, hot_label_train, \n                                        labels_test[0:999], hot_labels_test[0:999], data_train, data_test[0:999], TopK)","994c47a5":"results","74c3bfa7":"# M\u00e9tricas de Avalia\u00e7\u00e3o","31b8f7ee":"Abaixo, apenas algumas m\u00e9trias de IR, como NDCG, precis\u00e3o em k, etc.","c108c8a4":"Aqui vem a implementa\u00e7\u00e3o do m\u00e9todo supervisionado descrito no paper. \u00c9 um m\u00e9todo pairwaise, com um VAE de bernoulli que usa informa\u00e7\u00e3o supervisionada e usa uma t\u00e9cnica de Monte Carlos para a atualiza\u00e7\u00e3o de gradientes.","6de31b53":"Abaixo eu estou pegando todos os c\u00f3digos (Z) gerados para as imagens do treino. Para cada bit dos c\u00f3digos (coluna) estou calculando a m\u00e9dia. Eu esperaria algo sempre em torno de 50%, o que significaria que espalha bem o c\u00f3digo entre todos os bits. Contudo, obtive sempre mais que 50%, e em alguns casos, em torno de 60%.","231f3e11":"Abaixo, obtive os topK = 10 imagens mais parecidas (answers) com cada imagem de teste (query). Eu estou exibindo, em cada linha abaixo, a imagem query seguida das top10 imagens answers. Acima de cada imagem est\u00e1 a dist\u00e2ncia de hamming para a query. Dist\u00e2ncia 0 significa colis\u00e3o com a query. H\u00e1 bastante colis\u00e3o, mas lembre o espa\u00e7o s\u00f3 tem 256 c\u00f3digos e foi treinado com >1500 imagens. O resultado pode n\u00e3o ser grande coisa, mas \u00e9 importante notar que a cole\u00e7\u00e3o de treino que estou usando \u00e9 bem pequena.","8ac36fed":"# Carregamento da Base\n","a66e5697":"# PSH with Bernoulli VAE and Self Control Gradient Estimator","7b09ae9a":"Abaixo temos os c\u00f3digos gerados para o Treino. Vamos fazer algumas an\u00e1lises simples destes c\u00f3digos:"}}