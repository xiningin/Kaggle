{"cell_type":{"461067c6":"code","2841d27d":"code","b329ce34":"code","cff82e09":"code","1bff0e4b":"code","611f80bd":"code","e7194beb":"code","6376b778":"code","8733dda4":"code","c09dd4ef":"code","cdc6de49":"code","a9da4e84":"code","c2969984":"code","aff919d7":"code","add26dbf":"code","35e0cc63":"code","17b89a65":"code","adf24e7c":"code","e681936a":"code","01428440":"code","c3dc046d":"code","8995225b":"code","90b16456":"code","87156a9a":"code","f39a2769":"code","701341bd":"code","6c866cd6":"code","fb8c551b":"code","a1b8b15e":"code","9d731a8f":"code","df238ef3":"code","d2522fab":"code","db42896f":"code","a5070794":"code","75f9a85e":"code","31204b52":"code","c8252b8d":"code","bf8039ca":"code","14fa876a":"code","5a473050":"markdown","18ec9400":"markdown","588f733e":"markdown","cc541325":"markdown","a8bb6d3c":"markdown","5ee66e6c":"markdown","f3ec0964":"markdown","b18e9025":"markdown","12afd695":"markdown","aea5c7c5":"markdown","e5a2dde8":"markdown","b0a801a4":"markdown","60fd710a":"markdown","94f427b8":"markdown","fa169162":"markdown","8a59ae8f":"markdown","6e95eb96":"markdown","0f784f45":"markdown","7e2c1e77":"markdown","3c71c375":"markdown","07c87e9c":"markdown","cda64045":"markdown","be8acb7e":"markdown","4ef19e9f":"markdown","d2c9c166":"markdown","fb2d0ff6":"markdown","fe0ed465":"markdown","e3d3d6dd":"markdown","431a1a75":"markdown","492164d2":"markdown","38ace5b6":"markdown","25dc7b4d":"markdown","cce204d0":"markdown","73e8b0ba":"markdown","51f3f005":"markdown","e16009b9":"markdown","0a5808a7":"markdown","b8e37817":"markdown","5a46fec8":"markdown"},"source":{"461067c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport missingno as msno \nimport scipy\nfrom scipy.sparse import hstack\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_squared_error\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n%matplotlib inline\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2841d27d":"data = pd.read_csv('\/kaggle\/input\/amazon-fine-food-reviews\/Reviews.csv')","b329ce34":"data.head()","cff82e09":"plt.title('10 Most Active Users')\ndata['UserId'].value_counts(sort=True).nlargest(10).plot.bar()","1bff0e4b":"print('There are', str(data.shape[0]), 'records in total.')","611f80bd":"plt.title('10 Most Rated Products')\ndata['ProductId'].value_counts(sort=True).nlargest(10).plot.bar()","e7194beb":"HelpfulnessNumerator0 = data[data['HelpfulnessNumerator'] == 0]['HelpfulnessNumerator'].value_counts()\nHelpfulnessNumerator1 = data[data['HelpfulnessNumerator'] == 1]['HelpfulnessNumerator'].value_counts()\nHelpfulnessNumerator2 = data[data['HelpfulnessNumerator'] == 2]['HelpfulnessNumerator'].value_counts()\nHelpfulnessNumerator3 = data[data['HelpfulnessNumerator'] == 3]['HelpfulnessNumerator'].value_counts()\nHelpfulnessNumeratorMoreThan3 = data[data['HelpfulnessNumerator'] > 3]['HelpfulnessNumerator'].value_counts()\n\nlabels = '0', '1', '2', '3', 'more than 3'\nsizes = [HelpfulnessNumerator0.values.item(), HelpfulnessNumerator1.values.item(), HelpfulnessNumerator2.values.item(), \n         HelpfulnessNumerator3.values.item(), HelpfulnessNumeratorMoreThan3.values.sum()]\nexplode = (0, 0, 0, 0, 0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\nfig, ax = plt.subplots()\nax.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.title(' Portions of Amount of Helpfulness Labels')\nplt.show()","6376b778":"HelpfulnessDenominator0 = data[data['HelpfulnessDenominator'] == 0]['HelpfulnessDenominator'].value_counts()\nHelpfulnessDenominator1 = data[data['HelpfulnessDenominator'] == 1]['HelpfulnessDenominator'].value_counts()\nHelpfulnessDenominator2 = data[data['HelpfulnessDenominator'] == 2]['HelpfulnessDenominator'].value_counts()\nHelpfulnessDenominator3 = data[data['HelpfulnessDenominator'] == 3]['HelpfulnessDenominator'].value_counts()\nHelpfulnessDenominatorMoreThan3 = data[data['HelpfulnessDenominator'] > 3]['HelpfulnessDenominator'].value_counts()\n\nlabels = '0', '1', '2', '3', 'more than 3'\nsizes = [HelpfulnessDenominator0.values.item(), HelpfulnessDenominator1.values.item(), HelpfulnessDenominator2.values.item(), \n         HelpfulnessDenominator3.values.item(), HelpfulnessDenominatorMoreThan3.values.sum()]\nexplode = (0, 0, 0, 0, 0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\nfig, ax = plt.subplots()\nax.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.title(' Portions of Amount of Comments Watched')\nplt.show()","8733dda4":"plt.title('Scores')\ndata['Score'].value_counts().plot.bar()","c09dd4ef":"fig = plt.figure(figsize=(14, 10))\nax = fig.add_subplot(121)\ntext = data.Summary.values\nwordcloud = WordCloud(\n    background_color = 'black',\n    stopwords = STOPWORDS).generate(str(text))\nplt.title('Summary Keywords')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\n\nax = fig.add_subplot(122)\ntext = data.Text.values\nwordcloud = WordCloud(\n    background_color = 'white',\n    stopwords = STOPWORDS).generate(str(text))\nplt.title('Text Keywords')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()","cdc6de49":"msno.matrix(data)","a9da4e84":"msno.heatmap(data)","c2969984":"# fives = data.loc[data['Score'] == 5]\n# fives = fives.sample(frac=0.5)\n# data = pd.concat([data.loc[data['Score'] != 5], fives])","aff919d7":"data['Text'].loc[data['Text'].isna()] = ''\ndata['Summary'].loc[data['Summary'].isna()] = ''","add26dbf":"X_train, X_valid, y_train, y_valid = train_test_split(data.drop('Score', axis=1), data['Score'], test_size=0.2, random_state=42)","35e0cc63":"X_train['Helpful'] = X_train['HelpfulnessNumerator']\nX_train['Unhelpful'] = X_train['HelpfulnessDenominator'] -X_train['HelpfulnessNumerator']\n\nX_valid['Helpful'] = X_valid['HelpfulnessNumerator']\nX_valid['Unhelpful'] = X_valid['HelpfulnessDenominator'] - X_valid['HelpfulnessNumerator']\n\nscaler = StandardScaler()\n\n# only fit on the train set\nscalerFitter = scaler.fit(X_train[['Helpful', 'Unhelpful', 'Time']])\nX_train[['Helpful', 'Unhelpful', 'Time']] = scalerFitter.transform(X_train[['Helpful', 'Unhelpful', 'Time']])\nX_valid[['Helpful', 'Unhelpful', 'Time']] = scalerFitter.transform(X_valid[['Helpful', 'Unhelpful', 'Time']])\n\nX_train = X_train.drop(['HelpfulnessDenominator','HelpfulnessNumerator'], axis=1)\nX_valid = X_valid.drop(['HelpfulnessDenominator','HelpfulnessNumerator'], axis=1)","17b89a65":"text_vectorizer = TfidfVectorizer(input='content', analyzer='word', stop_words='english', ngram_range=(1, 2))\nsummary_vectorizer = TfidfVectorizer(input='content', analyzer='word', stop_words='english', ngram_range=(1, 2))\n\ntext_fitter = text_vectorizer.fit(data['Text'])\ntext_matrix_train = text_fitter.transform(X_train['Text'])\ntext_matrix_valid = text_fitter.transform(X_valid['Text'])\n\nsummary_fitter = summary_vectorizer.fit(data['Summary'])\nsummary_matrix_train = summary_fitter.transform(X_train['Summary'])\nsummary_matrix_valid = summary_fitter.transform(X_valid['Summary'])","adf24e7c":"# the shape of text and summary matrices\ntext_matrix_train, summary_matrix_train, text_matrix_valid, summary_matrix_valid","e681936a":"# features in text\ntext_vectorizer.get_feature_names()","01428440":"# features in summary\nsummary_vectorizer.get_feature_names()","c3dc046d":"OHE = OneHotEncoder(sparse=True)\nID_fitter = OHE.fit(data[['ProductId', 'UserId']])\nIDs_train = ID_fitter.transform(X_train[['ProductId', 'UserId']])\nIDs_valid = ID_fitter.transform(X_valid[['ProductId', 'UserId']])","8995225b":"numerical_train = scipy.sparse.csr_matrix(X_train[['Helpful', 'Unhelpful', 'Time']].values)\nnumerical_valid = scipy.sparse.csr_matrix(X_valid[['Helpful', 'Unhelpful', 'Time']].values)","90b16456":"X_train = hstack([text_matrix_train, summary_matrix_train, numerical_train, IDs_train])\nX_valid = hstack([text_matrix_valid, summary_matrix_valid, numerical_valid, IDs_valid])","87156a9a":"from imblearn.over_sampling import RandomOverSampler\n\nros = RandomOverSampler(random_state=42)\nX_train, y_train = ros.fit_resample(X_train, y_train)","f39a2769":"plt.title('Scores')\ny_train.value_counts().plot.bar()","701341bd":"def CVKFold(k, X, y, model):\n    np.random.seed(1)\n    #reproducibility\n    \n    highest_accuracy = float('inf')\n    best_model = None\n\n    kf = KFold(n_splits = k,shuffle =True)\n    #CV loop\n    \n    for train_index,test_index in kf.split(X):#generation of the sets\n    #generate the sets    \n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        #model fitting\n        model.fit(X_train,y_train)\n        y_test_pred = model.predict(X_test)\n    \n        test_accuracy = mean_squared_error(y_test_pred, y_test)\n        print(\"The accuracy is \" + str(test_accuracy))\n        if test_accuracy < highest_accuracy:\n          best_model = model\n          highest_accuracy = test_accuracy\n\n    print(\"The highest accuracy is \" + str(highest_accuracy))\n    return best_model, highest_accuracy","6c866cd6":"# Logistics Regression\nmodel = LogisticRegression(random_state = 0)\nmodel = model.fit(X_train, y_train)\nclf_Log, accuracy_Log = CVKFold(3, X_train, y_train, model)\n# Decision Tree\nmodel = DecisionTreeClassifier(random_state = 0, max_depth=20)\nclf_DTree, accuracy_DTree = CVKFold(3, X_train, y_train, model)\n# # Random Forest\nmodel = RandomForestClassifier(random_state = 0, max_depth=20)\nclf_RF, accuracy_RF = CVKFold(3, X_train, y_train, model)","fb8c551b":"accuracies = {accuracy_Log: clf_Log, accuracy_DTree: clf_DTree, accuracy_RF: clf_RF}\nclf = accuracies[min([accuracy_Log, accuracy_DTree, accuracy_RF])]","a1b8b15e":"print('The most accurate classifier is:', clf)","9d731a8f":"from sklearn.preprocessing import label_binarize\n\ny_valid = label_binarize(y_valid, classes=[1, 2, 3, 4, 5])\ny_train = label_binarize(y_train, classes=[1, 2, 3, 4, 5])","df238ef3":"from sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import roc_curve, auc\n\nmodel = OneVsRestClassifier(clf)\ny_score = model.fit(X_train, y_train).decision_function(X_valid)\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(5):\n    fpr[i], tpr[i], _ = roc_curve(y_valid[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\n# Compute micro-average ROC curve and ROC area\nfpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_valid.ravel(), y_score.ravel())\nroc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])","d2522fab":"plt.figure()\nlw = 2\nplt.plot(fpr[2], tpr[2], color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()","db42896f":"from scipy import interp\nfrom itertools import cycle\n\nn_classes = 5\n\n# First aggregate all false positive rates\nall_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n# Then interpolate all ROC curves at this points\nmean_tpr = np.zeros_like(all_fpr)\nfor i in range(n_classes):\n    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n# Finally average it and compute AUC\nmean_tpr \/= n_classes\n\nfpr[\"macro\"] = all_fpr\ntpr[\"macro\"] = mean_tpr\nroc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n# Plot all ROC curves\nplt.figure()\nplt.plot(fpr[\"micro\"], tpr[\"micro\"],\n         label='micro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"micro\"]),\n         color='deeppink', linestyle=':', linewidth=4)\n\nplt.plot(fpr[\"macro\"], tpr[\"macro\"],\n         label='macro-average ROC curve (area = {0:0.2f})'\n               ''.format(roc_auc[\"macro\"]),\n         color='navy', linestyle=':', linewidth=4)\n\ncolors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\nfor i, color in zip(range(n_classes), colors):\n    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n             label='ROC curve of class {0} (area = {1:0.2f})'\n             ''.format(i, roc_auc[i]))\n\nplt.plot([0, 1], [0, 1], 'k--', lw=lw)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Some extension of Receiver operating characteristic to multi-class')\nplt.legend(loc=\"lower right\")\nplt.show()","a5070794":"from sklearn.metrics import precision_score\n\nprint('Precision Score: ')\nprecision_score(y_valid, model.predict(X_valid), average='macro')","75f9a85e":"from sklearn.metrics import recall_score\n\nprint('Recall Score: ')\nrecall_score(y_valid, model.predict(X_valid), average='macro')","31204b52":"from sklearn.metrics import f1_score\n\nprint('F-1 Score: ')\nf1_score(y_valid, model.predict(X_valid), average='macro')","c8252b8d":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\n\n# For each class\nprecision = dict()\nrecall = dict()\naverage_precision = dict()\nfor i in range(n_classes):\n    precision[i], recall[i], _ = precision_recall_curve(y_valid[:, i],\n                                                        y_score[:, i])\n    average_precision[i] = average_precision_score(y_valid[:, i], y_score[:, i])\n\n# A \"micro-average\": quantifying score on all classes jointly\nprecision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_valid.ravel(), y_score.ravel())\naverage_precision[\"micro\"] = average_precision_score(y_valid, y_score, average=\"micro\")\nprint('Average precision score, micro-averaged over all classes: {0:0.2f}'.format(average_precision[\"micro\"]))","bf8039ca":"plt.figure()\nplt.step(recall['micro'], precision['micro'], where='post')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title(\n    'Average precision score, micro-averaged over all classes: AP={0:0.2f}'\n    .format(average_precision[\"micro\"]))","14fa876a":"colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])\n\nplt.figure(figsize=(7, 8))\nf_scores = np.linspace(0.2, 0.8, num=4)\nlines = []\nlabels = []\nfor f_score in f_scores:\n    x = np.linspace(0.01, 1)\n    y = f_score * x \/ (2 * x - f_score)\n    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n\nlines.append(l)\nlabels.append('iso-f1 curves')\nl, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\nlines.append(l)\nlabels.append('micro-average Precision-recall (area = {0:0.2f})'\n              ''.format(average_precision[\"micro\"]))\n\nfor i, color in zip(range(n_classes), colors):\n    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n    lines.append(l)\n    labels.append('Precision-recall for class {0} (area = {1:0.2f})'\n                  ''.format(i, average_precision[i]))\n\nfig = plt.gcf()\nfig.subplots_adjust(bottom=0.25)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Extension of Precision-Recall curve to multi-class')\nplt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n\n\nplt.show()","5a473050":"### Receiver Operating Characteristic (ROC)","18ec9400":"The original data set contains 9 columns, and column \"Id\" serves as the unique identifier of each row entry which in other words, is irrelevant to the modeling.","588f733e":"Commenters incline to rate a movie 5.0\/5.0, so unlike previous features, a rating of 5.0\/5.0 might cause a side effect when implementing machine learning that the classifier tends to predict an un labeled movie 5\/0\/5.0. I will avoid this happen in the following chapters.","cc541325":"#### Recall","a8bb6d3c":"## Random Oversampling","5ee66e6c":"## Missing Value Detection\n-----\nAs the overview suggests, there are missing values inside our data set. So it is necessary to research their properties and make sure no distortions caused by them.","f3ec0964":"I found the most active user among the dataset has no more than 3000 records in it. Compared to the total amount of over 1 million records, this is not a very significant portion. Therefore we have to worry about whether a certain user is able to make too big an influence to the entire prediction. This is the same in terms of the column \"ProductId\" because its most frequently seen entry has no more than 3000 records either.","b18e9025":"#### One vs Rest ROC","12afd695":"## Downsize the data whose score is 5.0\/5.0\n------\nLike I showed before, there are too many score of 5.0\/5.0 insides. To avoid side effect of it (a predictor tends to set the score of an unknown record as the most commonly seen one), I am going to cut off the scale of the dataset whose score is 5.0\/","aea5c7c5":"## Standardize the numerical features\n-----\nStandardization helps to reduce the influence of outliers and converge faster.","e5a2dde8":"#### Multiclass ROC","b0a801a4":"## Concatenate all the features\n\n### make dense series to a sparse matrix","60fd710a":"#### Plot for Each Class","94f427b8":"## Separate a ratio of train set for the use of validation\n---\nThe reason that I do splitting at the first place is that standardardization before splitting is efficient in terms of avoiding the effect of outliers.","fa169162":"# **Amazon Fine Food Reviews Prediction**\n-------\nThis project examines the realtionship between the star rating score between user reviews from Amazon Movie Reviews using the available features. The features include unique identifier for the product\/user, the number of users who found the review helpful, the number of users who indicated whether they found the review helpful, the timestamp for the review, the brief summary of the review and the text of the review. In the rest of this notebook, I'm going to utilize these features on the prediction of the star rating score. This project, if successful, is beneficial to estimate any unlisted movie's popularity or reputation, which could be further used in the recommendation system in this field. ","8a59ae8f":"### HelpfulnessDenominator","6e95eb96":"### Precision-Recall Curve","0f784f45":"### stack all the sparse matrices above acording to their row indices\n----\nSparse matrix helps solve the memory consuming trouble of the one-hot encoding.","7e2c1e77":"## One-hot Encoding to Categorical Features\n------\nEncoding is a method to digitalize the categorical features. The reason I chose one-hot encoding over label encoding is due to the fact that label encoding labels each unique identifier a different on a basis of \"first appear, first encode\". In other words, the identifier which appears earlier gets a smaller value. This would conflict with the fact that identifier cannot be measureable feature. The only disvantage of one-hot encoding, on the other hand, is that it consumes too many memories. I will show how to handle this situation.","3c71c375":"#### Average Plot","07c87e9c":"#### Precision","cda64045":"In \"HelpfulnessNumerator\" and \"HelpfulnessDenominator\", none of a single value makes up more than 50%, so I can safely put that none of the value is too overwhelming to affect the performance of the predictor.","be8acb7e":"There is not too much overlapping between the two word clouds, basically because summary is more consise than its text and so that it contains less information. However, since they have few in common, it is save to asusme there does not exist a strong correlation between the two. Thus I am going to use both of them as my factors of the predictor.","4ef19e9f":"## Binarize the target values","d2c9c166":"## Impute the missing values by making blanks as void strings\n-----\nThe easiest imputation.","fb2d0ff6":"# Data Preprocessing","fe0ed465":"### HelpfulnessNumerator","e3d3d6dd":"## Implement the classifier\n---\nI chose Logistics regression as my model because of its efficiency and am going to check its accuracy with K-fold cross validation","431a1a75":"## Calculate TF-IDF and Vectorize\n-----\nThis is the baseline step. By vectorizing the text values, we can access to more information about this dataset so that a higher accuracy is obtained.","492164d2":"## Import train data","38ace5b6":"### Score","25dc7b4d":"### Users","cce204d0":"### Summary and Text\n-----\nIn this step I plotted out the word cloud images of both \"Summary\" and \"Text\", in search of any valuable information.","73e8b0ba":"It seems that there almost no missing values insides other properties. According to the following heatmap, the missing values in other columns only exist in the property \"Summary\" and \"Text\", while neither of each is strongly correlated to one another. This makes the problem simple because I can just apply a simple imputation on them without considering other situations.","51f3f005":"## Import libraries","e16009b9":"### Products","0a5808a7":"## EDA\n-------\nI am going to discover some significant properties among the data by carrying out an exploratory data analysis.","b8e37817":"#### F-1 score","5a46fec8":"# Testing & Measuring"}}