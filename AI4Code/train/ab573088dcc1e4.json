{"cell_type":{"df2bdeb8":"code","1b6fba71":"code","873aa858":"code","4904d5bd":"code","a0dafefc":"code","0443ac72":"code","1672a146":"code","01c59266":"code","6a595438":"code","61418f28":"code","d974d312":"code","b2ff72af":"code","f0dd0e23":"code","8ff332a6":"code","0a29d206":"code","10fccc1a":"code","ab7d02ed":"code","768a4157":"code","e225bb3f":"markdown","78172af3":"markdown","b6f2b027":"markdown","7465b91d":"markdown","b0d3574b":"markdown","cd270884":"markdown","edcdbb2d":"markdown","45183b9a":"markdown","587c847d":"markdown","4a5ba2e4":"markdown"},"source":{"df2bdeb8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport time\nimport pickle\nfrom sklearn.model_selection import train_test_split","1b6fba71":"df_train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ndf_test = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","873aa858":"# grayscale normalization\n\nfor column in tqdm(df_test.columns):\n    df_train[column] = df_train[column]\/255\n    df_test[column] = df_test[column]\/255","4904d5bd":"def pict_from_array(numarray, width, height):\n    \"\"\"\n    Shows plot of array image\n    :param numarray: numpy array to plot\n    :param width: width of picture\n    :param height: height of picture\n    \n    :return: plotted image\n    \"\"\"\n    numarray = np.reshape(numarray, (width, height))\n    plt.imshow(numarray)\n    plt.show","a0dafefc":"pict_from_array(df_test.loc[0].to_numpy(), 28, 28)","0443ac72":"y = df_train['label'].to_numpy()\nX = df_train.loc[:,'pixel0':'pixel783'].to_numpy()","1672a146":"z = np.zeros((len(y), 10))","01c59266":"for key, value in enumerate(y):\n    z[key][value] = 1","6a595438":"X_train, X_test, y_train, y_test = train_test_split(X, z, test_size=0.33, random_state=13)","61418f28":"neurons_5 = [1,1,1,1,1] # size of 5\nconversion_matrix = np.random.randn(5, 3)\nneurons_3 = np.dot(neurons_5, conversion_matrix) # size of 3\nneurons_3","d974d312":"class NeuralNetwork:\n    \n    '''\n    Neural network from scratch\n    Self-education purpose only. \n    Use with caution! And have fun\n    '''\n    \n    def __init__(self, neurons = [784, 512, 128, 64, 10], lr=0.001, epoch=10, \\\n                 weights = None, momentum = 0.9):\n        '''\n        Here we build our network\n        \n        :param neurons: array with sizes of neuron layers\n                        neurons[0] = number of pixels = 28*28\n                        neurons[-1:] = number of classes = 10\n                        all berween are hidden layers\n        :param lr: learning_rate\n        :param epoch: number of epochs\n        :param weights: if weights == None, then initialize random\n        '''\n        self.input = neurons[0]\n        self.hidden_1 = neurons[1]\n        self.hidden_2 = neurons[2]\n        self.hidden_3 = neurons[3]\n        self.output = neurons[4]\n        self.lr = lr\n        self.epoch = epoch\n        \n        if weights == None:\n            self.weights = {\n                # We need to make from default 784 neurons 512\n                # Only way we can do that by multiplying by matrix of shape 512\u00d7784\n                'w0': np.random.randn(self.hidden_1, self.input) * np.sqrt(1. \/ self.hidden_1),\n                # Multiplying by matrix of shape 128\u00d7512\n                'w1': np.random.randn(self.hidden_2, self.hidden_1) * np.sqrt(1. \/ self.hidden_2),\n                # Multiplying by matrix of shape 64\u00d7128\n                'w2': np.random.randn(self.hidden_3, self.hidden_2) * np.sqrt(1. \/ self.hidden_3),\n                # Multiplying by matrix of shape 10\u00d764\n                'w3': np.random.randn(self.output, self.hidden_3) * np.sqrt(1. \/ self.output),\n            }\n        else:\n            self.weights = weights\n        \n        # It will contain all current neuron calculations\n        self.neurons = {}\n        # Gradient for momentum\n        self.prev_grad = None\n        self.momentum = momentum\n            \n        \n            \n    \n    def softmax(self, x, derivative = False):\n        '''\n        Softmax activation function\n        https:\/\/en.wikipedia.org\/wiki\/Softmax_function\n        \n        Basic softmax should look like this:\n        def softmax(x):\n            \"\"\"Compute the softmax of vector x.\"\"\"\n            exps = np.exp(x)\n            return exps \/ np.sum(exps)\n        But code below is more numerically stable\n        \n        '''\n        exps = np.exp(x - x.max())\n        if derivative == True:\n            return exps \/ np.sum(exps, axis=0) * (1 - exps \/ np.sum(exps, axis=0))\n        return exps \/ np.sum(exps, axis=0)\n    \n    def relu(self, x, derivative = False):\n        '''\n        RELU activation function\n        https:\/\/en.wikipedia.org\/wiki\/Rectifier_(neural_networks)\n        I used Relu, because I've read that with this activation function,\n        training process goes much faster\n        \n        :param x:\n        :param derivative: if False, than \n        \n        '''\n        if derivative == True:\n            # if x == 0 derivative should be undefined, but I'll make it 0\n            x[x<=0] = 0\n            x[x>0] = 1\n            return x\n        else:\n            return np.maximum(0, x)\n    \n    \n    def forward(self, x):\n        '''\n        Forward pass\n        \n        :param x: input neurons\n        :return :\n        '''\n        \n        # All new keys in dictionary will be l# or a# \n        # (# - based on layer number, l or a based on linear or activation layer)\n        \n        # Working with input\n        self.neurons['l_input'] = x\n        \n        self.neurons['l0'] = np.dot(self.weights['w0'], self.neurons['l_input'])\n        self.neurons['a0'] = self.relu(self.neurons['l0']) # activation of first layer\n        \n        # Create hidden layer\n        self.neurons['l1'] = np.dot(self.weights['w1'], self.neurons['a0'])\n        self.neurons['a1'] = self.relu(self.neurons['l1'])\n        \n        # Create hidden layer\n        self.neurons['l2'] = np.dot(self.weights['w2'], self.neurons['a1'])\n        self.neurons['a2'] = self.relu(self.neurons['l2'])\n        \n        # Working with output\n        self.neurons['l3'] = np.dot(self.weights['w3'], self.neurons['a2'])\n        self.neurons['a3'] = self.softmax(self.neurons['l3']) # return probability of 0 to 10\n        \n        return self.neurons['a3']\n    \n    def backward(self, y_true, y_pred):\n        '''\n        y_pred - predicted y\n        y_true - real y\n        '''\n        \n        # Computation of gradients\n        gradients = {}\n        \n        error = 2 * (y_pred - y_true) \/ y_pred.shape[0] * self.softmax(self.neurons['l3'], derivative=True)\n        gradients['w3'] = np.outer(error, self.neurons['a2'])\n        \n        error = np.dot(self.weights['w3'].T, error)*self.relu(self.neurons['l2'], derivative=True)\n        gradients['w2'] = np.outer(error, self.neurons['a1'])\n        \n        error = np.dot(self.weights['w2'].T, error)*self.relu(self.neurons['l1'], derivative=True)\n        gradients['w1'] = np.outer(error, self.neurons['a0'])\n        \n        error = np.dot(self.weights['w1'].T, error)*self.relu(self.neurons['l0'], derivative=True)\n        gradients['w0'] = np.outer(error, self.neurons['l_input'])\n        \n        return gradients\n        \n    def sgd_with_momentum(self, gradient, momentum=0.9):\n        '''\n        expl\n        '''\n        \n        for key, value in gradient.items():\n            if self.prev_grad == None:\n                self.weights[key] -= self.lr * value\n            else:\n                self.weights[key] -= (self.lr * value + self.lr * self.prev_grad[key] * momentum)\n\n        self.prev_grad = gradient.copy()\n        \n        \n        \n    def compute_accuracy(self, x_val, y_val):\n        '''\n        '''\n        predictions = []\n\n        for x, y in zip(x_val, y_val):\n            output = self.forward(x)\n            pred = np.argmax(output)\n            predictions.append(pred == np.argmax(y))\n            \n        return np.mean(predictions)\n    \n    def train(self, x_train, y_train, x_val, y_val):\n        score = 0\n        start_time = time.time()\n        for iteration in range(self.epoch):\n            for x,y in zip(x_train, y_train):\n                output = self.forward(x)\n                gradident = self.backward(y, output)\n                self.sgd_with_momentum(gradident, self.momentum)\n            \n            accuracy = self.compute_accuracy(x_val, y_val)\n\n            if accuracy > score:\n                with open('.\/weights.pkl', 'wb') as f:\n                    pickle.dump(self.weights, f, pickle.HIGHEST_PROTOCOL)\n                print('Accuracy improved from {:.2f}% to {:.2f}%. File with weights updated'.format(score*100, accuracy * 100))\n                score = accuracy\n            \n            print('Epoch: {}\/{}, Time Spent: {:.2f}s, Accuracy: {:.2f}%'.format(\n                iteration+1, self.epoch, time.time() - start_time, accuracy * 100\n            ))\n\n        return self.weights","b2ff72af":"# Training from scratch\n# Just uncoment 2 lines below and do not run Loading model section\n# dnn = NeuralNetwork()\n# model_weights = dnn.train(X_train, y_train, X_test, y_test)","f0dd0e23":"# Loading weights (it was training for about 70 epoches)\nwith open('..\/input\/nn-from-scratch-mnist\/weights_95_8.pkl', 'rb') as f:\n    weigths = pickle.load(f)","8ff332a6":"# I'll run only one epoch to see results\ndnn = NeuralNetwork(weights = weigths, epoch=1)\nmodel_weights = dnn.train(X_train, y_train, X_test, y_test)","0a29d206":"# Prepare data to fit\nnp_test = df_test.to_numpy()","10fccc1a":"image_id = []\nlabel = []\nj=1\nfor i in tqdm(np_test):\n    label.append(np.argmax(dnn.forward(i)))\n    image_id.append(j)\n    j+=1","ab7d02ed":"df_test_answ = pd.DataFrame(list(zip(image_id, label)),\n               columns =['ImageId', 'Label'])","768a4157":"df_test_answ.to_csv('.\/answer.csv', index = False)","e225bb3f":"We need to make everything numpy. All our calculations will use numpy","78172af3":"# Class of it's own","b6f2b027":"# Some theory\nI'm going to make a class with neural net below, but I'm going to explain some things.<br>\nI suppose, you have seen pictures where one set of neurons convert to different number of neurons<br>\nIn our network we will convert 784 neurons to 512 then to 128 then to 64 then to 10<br>\nLet's take a look at small example and convert 5 neurons to 3 neurons. It's made with matrix multiplications. We need to multiply vector with size of 5, by matrix (5,3)","7465b91d":"# Training","b0d3574b":"And we need will make it a few times<br>\nThis is linear calculation<br>\nThen we need to add non-linear activation. I'll use RELU (some people say it works pretty good)<br>And we will make it multiple times<br>\n<br>Than we need to train our model. I've made some notebooks for better understanding how training works\nhttps:\/\/www.kaggle.com\/konstantinsuspitsyn\/gradient-descent-for-formulas-like-y-a-x-b\nI'll use Gradient Descent with momentum (people say, that it works faster then Adam for MNIST)","cd270884":"After uploading, score is 95.8%. It's definitely not the best, model is not the fastest, but it was built from scratch","edcdbb2d":"## Loading model","45183b9a":"Let's look at the picture","587c847d":"## Fitting data","4a5ba2e4":"# Introduction\nIt's much easier to manage something when you know how it works<br>\nThe goal of this Notebook is building neural network from scratch without any special libraries such as TensorFlow or PyTorch.<br>\nIt's not going to have the best score or work really fast. It's going to be pretty simple, comparing to modern CNN, but it's going to be written from scratch using only NumPy"}}