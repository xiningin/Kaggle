{"cell_type":{"62fad258":"code","78935e99":"code","7135162c":"code","00e8b6dc":"code","c0d9ad36":"code","5fdff1f2":"code","9fe30668":"code","cf9c5666":"code","c77e21a8":"code","bbf2957c":"code","06377934":"code","790dd46d":"code","61a640df":"code","2a3cd30b":"code","ccd92eae":"code","d6e4e24d":"code","f227e84d":"code","c6b14f22":"code","a2b42287":"code","a6e33d21":"code","b0c4a0ff":"code","7119a5f9":"code","f692d41d":"code","dd94bb31":"code","a880f728":"code","f241e363":"code","10206b24":"code","601df1ac":"code","2718c760":"code","79f84617":"code","9f4f6a19":"code","c33a2ad9":"code","3a8afcdd":"code","3b61cc3c":"code","3a7f9962":"code","e60c2ed2":"code","eea8ddac":"code","468fdf15":"code","6a374ce0":"code","8bb5b319":"code","42c8b5b5":"code","0cf60843":"code","b9900af0":"code","6cb1c561":"code","53de50b3":"code","406d1b9c":"code","f901a48d":"code","7a9fc985":"code","fd1934f6":"code","6589a06a":"code","238ae3f1":"code","869a6a1d":"code","b6757321":"code","d384e794":"code","cf076ae0":"code","9a39fdfb":"code","396dcc79":"code","0cde0d2b":"code","efb2ae04":"code","7d86be1c":"code","27360de8":"code","1b162ed0":"code","8ca92776":"code","39db6ad5":"code","dd7f6450":"code","730c6f68":"code","4150e498":"code","b2921431":"code","0569603f":"code","dad9504f":"code","fc4462f1":"code","94fcfb4f":"code","7ca72922":"code","7e1d5fa7":"code","54ad1c73":"code","98dbc3e5":"code","0772b03d":"code","725e7fc4":"code","01fa89ac":"code","ec6319e3":"code","fc106f2c":"code","2a508d91":"code","10fe5b87":"code","043d1770":"code","2e4fe6e1":"code","5303f81c":"code","63ff8143":"code","4443f8f4":"code","78c8dcc4":"code","28f77663":"code","649e11c9":"code","3f535276":"code","ac223057":"code","d6a5cdaa":"code","d8fcc612":"code","067adebd":"code","319103ee":"code","9a31a02b":"code","f6d5f003":"code","0eb7fd29":"code","c01c6555":"code","13dbcb11":"code","f0ff8583":"code","ba433ccf":"code","9a0431f7":"code","6dbbe1cb":"code","7a481cba":"code","d37220c5":"code","a806c407":"code","b63c5337":"code","5d0e5e84":"code","0fcd833f":"code","09be2fa4":"code","1533a4c9":"code","aee83852":"code","2e5a7ef4":"code","83af873f":"code","b0dbb8c6":"code","2f3bfd00":"code","63ed23a7":"code","5d56f299":"code","69b122a4":"code","577946b0":"code","fb90953f":"code","4e0b67e2":"code","0444f41f":"code","f9062d69":"code","c731b9c1":"code","879fd6f4":"code","00899ac3":"markdown","9281e5ef":"markdown","08badcfd":"markdown","614095f2":"markdown","5a886fc3":"markdown","4c30c297":"markdown","60ab599e":"markdown","0ea44762":"markdown","7a075e0b":"markdown","e5f20027":"markdown","cee9c034":"markdown","8802fc08":"markdown","c1c08639":"markdown","db4b5841":"markdown","3e50cf79":"markdown","20411d00":"markdown","ad50e3b1":"markdown","ab7fdb11":"markdown","1a03901f":"markdown","b2ad93a3":"markdown","8eb5e481":"markdown","919c2a77":"markdown","40f02ebc":"markdown","c1db8d44":"markdown","4e5601d8":"markdown","284828eb":"markdown","a4f548e4":"markdown","fc8584a9":"markdown","b84fc83b":"markdown","4fc0795d":"markdown","841266a8":"markdown","bab4164b":"markdown","630ad4b9":"markdown","75cd5887":"markdown","e44ba369":"markdown","03cfe6e6":"markdown","414b61d7":"markdown","d8a56864":"markdown","1d887819":"markdown","92a04f70":"markdown","ac5e28a7":"markdown","1f646a67":"markdown","435b3ebc":"markdown","e26f5b1b":"markdown","7197de91":"markdown","96852a35":"markdown","e002af70":"markdown","f5b02b9c":"markdown","37a58b6f":"markdown","d1d2fcf5":"markdown","d0d8986c":"markdown","6f87db26":"markdown","afb078e2":"markdown","f5f16847":"markdown","9e0fe1ae":"markdown","82e67a29":"markdown","84bdaa0f":"markdown","783883de":"markdown","2c02308e":"markdown","df8d67b3":"markdown","11887806":"markdown","2a06d371":"markdown"},"source":{"62fad258":"# Importing the libraries\nimport pandas as pd        # for data manipulation\nimport seaborn as sns      # for statistical data visualisation\nimport numpy as np         # for linear algebra\nimport matplotlib.pyplot as plt      # for data visualization\nfrom scipy import stats        # for calculating statistics\n\n# Importing various machine learning algorithm from sklearn\n\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score,KFold\nfrom sklearn.metrics import mean_absolute_error,roc_curve,auc,accuracy_score,mean_squared_error,r2_score\nfrom scipy.stats import zscore\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.linear_model import LinearRegression,Lasso,Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import VotingRegressor,RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor,BaggingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.utils import resample\nfrom math import sqrt","78935e99":"dataframe= pd.read_csv(\"concrete (1).csv\")  # Reading the data\ndataframe.head()   # showing first 5 datas","7135162c":"dataframe.shape","00e8b6dc":"dataframe.info()","c0d9ad36":"dataframe.isnull().sum()","5fdff1f2":"dataframe.apply(lambda x: len(x.unique()))","9fe30668":"dataframe.describe()","cf9c5666":"dataframe.skew()","c77e21a8":"print('Range of values',dataframe.cement.max()-dataframe.cement.min())\nprint('Interquartile range: ',dataframe.cement.describe()['75%']-dataframe.cement.describe()['25%'])","bbf2957c":"q3=dataframe.cement.describe()['75%']\nq1=dataframe.cement.describe()['25%']\niqr=dataframe.cement.describe()['75%']-dataframe.cement.describe()['25%']\nprint('Upper outliers starts from',q3+1.5*iqr)\nprint('Lower outliers starts from',q1-1.5*iqr)\n","06377934":"print('Number of Upper outliers are : ',dataframe[dataframe['cement']>586.4375].cement.count(),'(',dataframe[dataframe['cement']>586.4375].cement.count()\/len(dataframe),'%)')\nprint('Number of Lower outliers are : ',dataframe[dataframe['cement']<-44.0625].cement.count(),'(',dataframe[dataframe['cement']<-44.0625].cement.count()\/len(dataframe),'%)')","790dd46d":"fig,(a1,a2)=plt.subplots(1,2,figsize=(13,5))\n\nsns.distplot(dataframe.cement,ax=a1)\na1.set_title('Cement')\na1.set_ylabel('strengh')\nsns.boxplot(x='cement',data=dataframe,ax=a2)\n","61a640df":"print('Range of values',dataframe.slag.max()-dataframe.slag.min())\nprint('Interquartile range: ',dataframe.slag.describe()['75%']-dataframe.slag.describe()['25%'])","2a3cd30b":"q3=dataframe.slag.describe()['75%']\nq1=dataframe.slag.describe()['25%']\niqr=dataframe.slag.describe()['75%']-dataframe.slag.describe()['25%']\nprint('Upper outliers starts from',q3+1.5*iqr)\nprint('Lower outliers starts from',q1-1.5*iqr)\n","ccd92eae":"print('Number of Upper outliers are : ',dataframe[dataframe['slag']>357.357].slag.count(),'(',dataframe[dataframe['slag']>357.375].slag.count()\/len(dataframe),'%)')\nprint('Number of Lower outliers are : ',dataframe[dataframe['slag']<-214.42499999999998].slag.count(),'(',dataframe[dataframe['slag']<-214.42499999999998].slag.count()\/len(dataframe),'%)')","d6e4e24d":"fig,(a1,a2)=plt.subplots(1,2,figsize=(13,5))\n\nsns.distplot(dataframe.slag,ax=a1)\na1.set_title('Slag')\na1.set_ylabel('strengh')\nsns.boxplot(x='slag',data=dataframe,ax=a2)","f227e84d":"print('Range of values',dataframe.ash.max()-dataframe.ash.min())\nprint('Interquartile range: ',dataframe.ash.describe()['75%']-dataframe.ash.describe()['25%'])","c6b14f22":"q3=dataframe.ash.describe()['75%']\nq1=dataframe.ash.describe()['25%']\niqr=dataframe.ash.describe()['75%']-dataframe.ash.describe()['25%']\nprint('Upper outliers starts from',q3+1.5*iqr)\nprint('Lower outliers starts from',q1-1.5*iqr)\n","a2b42287":"print('Number of Upper outliers are : ',dataframe[dataframe['ash']>295.75].ash.count(),'(',dataframe[dataframe['ash']> 295.75].ash.count()\/len(dataframe),'%)')\nprint('Number of Lower outliers are : ',dataframe[dataframe['ash']<-177.45].ash.count(),'(',dataframe[dataframe['ash']<-177.45].ash.count()\/len(dataframe),'%)')","a6e33d21":"fig,(a1,a2)=plt.subplots(1,2,figsize=(13,5))\n\nsns.distplot(dataframe.ash,ax=a1)\na1.set_title('Ash ')\na1.set_ylabel('strengh')\nsns.boxplot(x='ash',data=dataframe,ax=a2)","b0c4a0ff":"print('Range of values',dataframe.water.max()-dataframe.water.min())\nprint('Interquartile range: ',dataframe.water.describe()['75%']-dataframe.water.describe()['25%'])","7119a5f9":"q3=dataframe.water.describe()['75%']\nq1=dataframe.water.describe()['25%']\niqr=dataframe.water.describe()['75%']-dataframe.water.describe()['25%']\nprint('Upper outliers starts from',q3+1.5*iqr)\nprint('Lower outliers starts from',q1-1.5*iqr)\n","f692d41d":"print('Number of Upper outliers are : ',dataframe[dataframe['water']>232.64999999999998].water.count(),'(',dataframe[dataframe['water']> 232.64999999999998].water.count()\/len(dataframe),'%)')\nprint('Number of Lower outliers are : ',dataframe[dataframe['water']<124.25000000000001].water.count(),'(',dataframe[dataframe['water']<124.25000000000001].water.count()\/len(dataframe),'%)')","dd94bb31":"fig,(a1,a2)=plt.subplots(1,2,figsize=(13,5))\n\nsns.distplot(dataframe.water,ax=a1)\na1.set_title('Water')\na1.set_ylabel('strengh')\nsns.boxplot(x='water',data=dataframe,ax=a2)","a880f728":"print('Range of values',dataframe.superplastic.max()-dataframe.superplastic.min())\nprint('Interquartile range: ',dataframe.superplastic.describe()['75%']-dataframe.superplastic.describe()['25%'])","f241e363":"q3=dataframe.superplastic.describe()['75%']\nq1=dataframe.superplastic.describe()['25%']\niqr=dataframe.superplastic.describe()['75%']-dataframe.superplastic.describe()['25%']\nprint('Upper outliers starts from',q3+1.5*iqr)\nprint('Lower outliers starts from',q1-1.5*iqr)\n","10206b24":"print('Number of Upper outliers are : ',dataframe[dataframe['superplastic']>25.5].superplastic.count(),'(',dataframe[dataframe['superplastic']> 25.5].superplastic.count()\/len(dataframe),'%)')\nprint('Number of Lower outliers are : ',dataframe[dataframe['superplastic']<-15.299999999999999].superplastic.count(),'(',dataframe[dataframe['superplastic']<-15.299999999999999].superplastic.count()\/len(dataframe),'%)')","601df1ac":"fig,(a1,a2)=plt.subplots(1,2,figsize=(13,5))\n\nsns.distplot(dataframe.superplastic,ax=a1)\na1.set_title('Super plastic')\na1.set_ylabel('strengh')\nsns.boxplot(x='superplastic',data=dataframe,ax=a2)","2718c760":"fig, ax2 = plt.subplots(3, 3, figsize=(16, 16))\nsns.distplot(dataframe['cement'],ax=ax2[0][0])\nsns.distplot(dataframe['slag'],ax=ax2[0][1])\nsns.distplot(dataframe['ash'],ax=ax2[0][2])\nsns.distplot(dataframe['water'],ax=ax2[1][0])\nsns.distplot(dataframe['superplastic'],ax=ax2[1][1])\nsns.distplot(dataframe['coarseagg'],ax=ax2[1][2])\nsns.distplot(dataframe['fineagg'],ax=ax2[2][0])\nsns.distplot(dataframe['age'],ax=ax2[2][1])\nsns.distplot(dataframe['strength'],ax=ax2[2][2])","79f84617":"sns.regplot(x='cement',y='strength',data=dataframe)","9f4f6a19":"sns.regplot(x='slag',y='strength',data=dataframe)","c33a2ad9":"sns.regplot(x='age',y='strength',data=dataframe)","3a8afcdd":"sns.regplot(x='superplastic',y='strength',data=dataframe)","3b61cc3c":"sns.pairplot(dataframe)","3a7f9962":"corelation=dataframe.corr()\ncorelation","e60c2ed2":"plt.figure(figsize=(20,20))\na=sns.heatmap(corelation,annot=True)","eea8ddac":"dataframe.boxplot(figsize=(35,15))","468fdf15":"for columns in dataframe.columns[:-1]:\n    q1 = dataframe[columns].quantile(0.25)\n    q3 = dataframe[columns].quantile(0.75)\n    iqr = q3 - q1\n    \n    low = q1-1.5*iqr\n    high = q3+1.5*iqr\n    dataframe.loc[(dataframe[columns] < low) | (dataframe[columns] > high), columns] = dataframe[columns].median()","6a374ce0":"\ndataframe.boxplot(figsize=(35,15))\n","8bb5b319":"dataframe.columns","42c8b5b5":"z_dataframe=dataframe.apply(zscore)\nz_dataframe","0cf60843":"features=['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg',\n       'fineagg', 'age']\n    \nX=z_dataframe[features]\nY=z_dataframe['strength']         ","b9900af0":"train_X,test_X,train_y,test_y=train_test_split(X,Y,test_size=0.3,random_state=1)\ntrain_X.count() ","6cb1c561":"train_X.head()","53de50b3":"test_X.count()","406d1b9c":"test_X.head()","f901a48d":"linear_reg=LinearRegression(n_jobs=1)\nlinear_reg","7a9fc985":"l_model=linear_reg.fit(train_X,train_y)\npredict=l_model.predict(test_X)\nprint(predict)\n","fd1934f6":"print('Performance on training data using Linear Model:',linear_reg.score(train_X,train_y))\nprint('Performance on testing data using Linear Model:',linear_reg.score(test_X,test_y))\nacc_DT=r2_score(test_y, predict)\nprint('Accuracy DT: ',acc_DT)\nprint('MSE: ',mean_squared_error(test_y, predict))","6589a06a":"results = pd.DataFrame({'Method':['Linear Regression'], 'accuracy': acc_DT},index={'1'})\nresults = results[['Method', 'accuracy']]\nresults","238ae3f1":"R_model=Ridge(alpha=.3)\nR_model=R_model.fit(train_X , train_y)","869a6a1d":"R_predict=R_model.predict(test_X)\nR_predict[0:100]","b6757321":"print('Performance on training data using Ridge Model:',R_model.score(train_X,train_y))\nprint('Performance on testing data using Ridge Model:',R_model.score(test_X,test_y))\nacc_DT=r2_score(test_y, R_predict)\nprint('Accuracy DT: ',acc_DT)\nprint('MSE: ',mean_squared_error(test_y, R_predict))","d384e794":"L_results = pd.DataFrame({'Method':['Ridge Regression'], 'accuracy': acc_DT},index={'2'})\nresults=pd.concat([results,L_results])\nresults = results[['Method', 'accuracy']]\nresults","cf076ae0":"L_model=Lasso(alpha=.2)\nL_model=L_model.fit(train_X , train_y)","9a39fdfb":"L_predict=L_model.predict(test_X)\nL_predict[0:100]","396dcc79":"print('Performance on training data using Lasso Model:',L_model.score(train_X,train_y))\nprint('Performance on testing data using Lasso Model:',L_model.score(test_X,test_y))\nacc_DT=r2_score(test_y, L_predict)\nprint('Accuracy DT: ',acc_DT)\nprint('MSE: ',mean_squared_error(test_y, L_predict))","0cde0d2b":"L_results = pd.DataFrame({'Method':['Lasso Regression'], 'accuracy': acc_DT},index={'3'})\nresults=pd.concat([results,L_results])\nresults = results[['Method', 'accuracy']]\nresults","efb2ae04":"dtree = DecisionTreeRegressor(random_state=1)\ndtree=dtree.fit(train_X , train_y)","7d86be1c":"dtree_predict=dtree.predict(test_X)\ndtree_predict[0:100]","27360de8":"print('Performance on training data using Lasso Model:',dtree.score(train_X,train_y))\nprint('Performance on testing data using DT:',dtree.score(test_X,test_y))\nacc_DT=r2_score(test_y, dtree_predict)\nprint('Accuracy : ',acc_DT)\nprint('Mean Sqaure Error: ',mean_squared_error(test_y, dtree_predict))","1b162ed0":"cr_results = pd.DataFrame({'Method':['Decision Tree'], 'accuracy': acc_DT},index={'4'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","8ca92776":"cross_val_score(dtree,train_X,train_y,cv=10).mean()","39db6ad5":"kfold= KFold(n_splits=10,shuffle=True,random_state=1)\nkfold_acc=cross_val_score(dtree,train_X,train_y,cv=kfold).mean()\nprint(\"Cross Val : \",kfold_acc)","dd7f6450":"cr_results = pd.DataFrame({'Method':['Cross Val Decision Tree'], 'accuracy': kfold_acc},index={'5'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","730c6f68":"imp=pd.DataFrame(data=dtree.feature_importances_,index=list(X.columns))\nimp","4150e498":"df=z_dataframe.copy()\ndf","b2921431":"x=df.drop(['ash','coarseagg','fineagg','strength'] , axis=1)\ny=df.strength\ntrain_X,test_X,train_y,test_y=train_test_split(x,y,test_size=0.3,random_state=1)\ntrain_X.count() ","0569603f":"dtree2 = DecisionTreeRegressor(random_state=1)\ndtree2=dtree2.fit(train_X , train_y)","dad9504f":"dtree_predict2=dtree2.predict(test_X)\ndtree_predict2[0:100]","fc4462f1":"print('Performance on training data using DT:',dtree2.score(train_X,train_y))\nprint('Performance on testing data using DT:',dtree2.score(test_X,test_y))\nacc_DT=r2_score(test_y, dtree_predict2)\nprint('Accuracy : ',acc_DT)\nprint('Mean Sqaure Error: ',mean_squared_error(test_y, dtree_predict2))","94fcfb4f":"dt2_results = pd.DataFrame({'Method':['2nd Decision Tree'], 'accuracy': acc_DT},index={'6'})\nresults=pd.concat([results,dt2_results])\nresults = results[['Method', 'accuracy']]\nresults","7ca72922":"features=['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg',\n       'fineagg', 'age']\n    \nX=z_dataframe[features]\nY=z_dataframe['strength']         \ntrain_X,test_X,train_y,test_y=train_test_split(X,Y,test_size=0.3,random_state=1)\ntrain_X.count() ","7e1d5fa7":"dtree_reg = DecisionTreeRegressor( max_depth = 4,random_state=1,min_samples_leaf=5)\ndtree_reg.fit(train_X, train_y)\n","54ad1c73":"dtree_reg_pred = dtree_reg.predict(test_X)\nprint('Performance on training data using DT:',dtree_reg.score(train_X,train_y))\nprint('Performance on testing data using DT:',dtree_reg.score(test_X,test_y))\nacc_RDT=r2_score(test_y, dtree_reg_pred)\nprint('Accuracy DT: ',acc_RDT)\nprint('MSE: ',mean_squared_error(test_y,dtree_reg_pred))","98dbc3e5":"RegDtree_result = pd.DataFrame({'Method':['Regularised Decision Tree'], 'accuracy': [acc_RDT]},index={'7'})\nresults = pd.concat([results, RegDtree_result])\nresults = results[['Method', 'accuracy']]\nresults","0772b03d":"kfold= KFold(n_splits=10,shuffle=True,random_state=1)\nkfold_acc=cross_val_score(dtree_reg,train_X,train_y,cv=kfold).mean()\nprint(\"Cross Val : \",kfold_acc)","725e7fc4":"cr_results = pd.DataFrame({'Method':['Cross Val Regularised Decision Tree'], 'accuracy': kfold_acc},index={'8'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","01fa89ac":"x=df.drop(['ash','coarseagg','fineagg','strength'] , axis=1)\ny=df.strength\ntrain_X,test_X,train_y,test_y=train_test_split(x,y,test_size=0.3,random_state=1)\ntrain_X.count()","ec6319e3":"dtree_reg = DecisionTreeRegressor( max_depth = 4,random_state=1,min_samples_leaf=5)\ndtree_reg.fit(train_X, train_y)","fc106f2c":"re_dtree_predict = dtree_reg.predict(test_X)\nprint('Performance on training data using DT:',dtree_reg.score(train_X,train_y))\nprint('Performance on testing data using DT:',dtree_reg.score(test_X,test_y))\nacc_RDT=r2_score(test_y, re_dtree_predict)\nprint('Accuracy DT: ',acc_RDT)\nprint('MSE: ',mean_squared_error(test_y, re_dtree_predict))","2a508d91":"cr_results = pd.DataFrame({'Method':['2nd Pruned Decision Tree '], 'accuracy': acc_RDT},index={'9'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","10fe5b87":"features=['cement', 'slag', 'ash', 'water', 'superplastic', 'coarseagg',\n       'fineagg', 'age']\n    \nX=z_dataframe[features]\nY=z_dataframe['strength']         \ntrain_X,test_X,train_y,test_y=train_test_split(X,Y,test_size=0.3,random_state=1)\ntrain_X.count() ","043d1770":"rfr_model=RandomForestRegressor(random_state=1)\nrfr_model=rfr_model.fit(train_X,train_y)\nrfr_model","2e4fe6e1":"rfr_predict = rfr_model.predict(test_X)\nprint('Performance on training data using Lasso Model:',rfr_model.score(train_X,train_y))\nprint('Performance on testing data using RF:',rfr_model.score(test_X,test_y))\nacc_RDT=r2_score(test_y, rfr_predict)\nprint('Accuracy RF: ',acc_RDT)\nprint('MSE: ',mean_squared_error(test_y, rfr_predict))","5303f81c":"cr_results = pd.DataFrame({'Method':['Random Forest Regressor '], 'accuracy': acc_RDT},index={'10'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","63ff8143":"kfold= KFold(n_splits=10,shuffle=True,random_state=1)\nkfold_acc=cross_val_score(rfr_model,train_X,train_y,cv=kfold).mean()\nprint(\"Cross Val : \",kfold_acc)","4443f8f4":"cr_results = pd.DataFrame({'Method':['Cross Val Random Forest Regressor '], 'accuracy': kfold_acc},index={'11'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","78c8dcc4":"ada_model=AdaBoostRegressor(random_state=1)\nada_model=ada_model.fit(train_X,train_y)\nada_model","28f77663":"ada_predict = ada_model.predict(test_X)\nprint('Performance on training data using Ada Boosting:',ada_model.score(train_X,train_y))\nprint('Performance on testing data using Ada Boosting:',ada_model.score(test_X,test_y))\nacc_RDT=r2_score(test_y, ada_predict)\nprint('Accuracy Ada Boostig: ',acc_RDT)\nprint('MSE: ',mean_squared_error(test_y, ada_predict))","649e11c9":"cr_results = pd.DataFrame({'Method':['Ada Boosting'], 'accuracy': acc_RDT},index={'12'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","3f535276":"kfold= KFold(n_splits=10,shuffle=True,random_state=1)\nkfold_acc=cross_val_score(ada_model,train_X,train_y,cv=kfold).mean()\nprint(\"Cross Val : \",kfold_acc)","ac223057":"cr_results = pd.DataFrame({'Method':['Cross Val Ada Boosting '], 'accuracy': kfold_acc},index={'13'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","d6a5cdaa":"gb_model=GradientBoostingRegressor(random_state=1)\ngb_model=gb_model.fit(train_X,train_y)\ngb_model","d8fcc612":"gb_predict = gb_model.predict(test_X)\nprint('Performance on training data using Gradient Boosting:',gb_model.score(train_X,train_y))\nprint('Performance on testing data using Gradient Boosting:',gb_model.score(test_X,test_y))\nacc_RDT=r2_score(test_y, gb_predict)\nprint('Accuracy Gradient Boostig: ',acc_RDT)\nprint('MSE: ',mean_squared_error(test_y, gb_predict))","067adebd":"cr_results = pd.DataFrame({'Method':['Gradient Boosting'], 'accuracy': acc_RDT},index={'14'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","319103ee":"kfold= KFold(n_splits=10,shuffle=True,random_state=1)\nkfold_acc=cross_val_score(gb_model,train_X,train_y,cv=kfold).mean()\nprint(\"Cross Val : \",kfold_acc)","9a31a02b":"cr_results = pd.DataFrame({'Method':['Cross Val Gradient Boosting '], 'accuracy': kfold_acc},index={'15'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","f6d5f003":"svr_model=SVR(kernel=\"linear\")\nsvr_model=svr_model.fit(train_X,train_y)\nsvr_model","0eb7fd29":"svr_predict = svr_model.predict(test_X)\nprint('Performance on training data using SVR:',svr_model.score(train_X,train_y))\nprint('Performance on testing data using SVR:',svr_model.score(test_X,test_y))\nacc_RDT=r2_score(test_y, svr_predict)\nprint('Accuracy SVR: ',acc_RDT)\nprint('MSE: ',mean_squared_error(test_y, svr_predict))","c01c6555":"cr_results = pd.DataFrame({'Method':['SVR '], 'accuracy': acc_RDT},index={'16'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","13dbcb11":"kfold= KFold(n_splits=10,shuffle=True,random_state=1)\nkfold_acc=cross_val_score(svr_model,train_X,train_y,cv=kfold).mean()\nprint(\"Cross Val : \",kfold_acc)","f0ff8583":"cr_results = pd.DataFrame({'Method':['Cross Val SVR '], 'accuracy': kfold_acc},index={'17'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","ba433ccf":"bag_model=BaggingRegressor(random_state=1)\nbag_model=bag_model.fit(train_X,train_y)\nbag_model","9a0431f7":"bag_predict = bag_model.predict(test_X)\nprint('Performance on training data using Bagging Regressor:',bag_model.score(train_X,train_y))\nprint('Performance on testing data using Bagging Regressor :',bag_model.score(test_X,test_y))\nacc_RDT=r2_score(test_y, bag_predict)\nprint('Accuracy Bagging Regressor : ',acc_RDT)\nprint('MSE: ',mean_squared_error(test_y, bag_predict))","6dbbe1cb":"cr_results = pd.DataFrame({'Method':['Bagging Regressor '], 'accuracy': acc_RDT},index={'18'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","7a481cba":"kfold= KFold(n_splits=10,shuffle=True,random_state=1)\nkfold_acc=cross_val_score(bag_model,train_X,train_y,cv=kfold).mean()\nprint(\"Cross Val : \",kfold_acc)","d37220c5":"cr_results = pd.DataFrame({'Method':['Cross Val Bagging Regressor '], 'accuracy': kfold_acc},index={'19'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","a806c407":"error=[]\nfor i in range(1,30):\n    knn_model = KNeighborsRegressor(n_neighbors=i)\n    knn_model.fit(train_X,train_y)\n    pred_i = knn_model.predict(test_X)\n    error.append(np.mean(pred_i!=test_y))\n  ","b63c5337":"\nplt.figure(figsize=(12,6))\nplt.plot(range(1,30),error,color='red', linestyle='dashed',marker='o',markerfacecolor='blue',markersize=10)\nplt.title('Error Rate K Value')\nplt.xlabel('K Value')\nplt.ylabel('Mean error')","5d0e5e84":"knn_model = KNeighborsRegressor(n_neighbors=3)\nknn_model.fit(train_X,train_y)","0fcd833f":"knn_predict = knn_model.predict(test_X)\nprint('Performance on training data using KNN:',knn_model.score(train_X,train_y))\nprint('Performance on testing data using KNN :',knn_model.score(test_X,test_y))\nacc_RDT=r2_score(test_y, knn_predict)\nprint('Accuracy KNN : ',acc_RDT)\nprint('MSE: ',mean_squared_error(test_y, knn_predict))","09be2fa4":"cr_results = pd.DataFrame({'Method':['KNN Regressor '], 'accuracy': acc_RDT},index={'20'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","1533a4c9":"kfold= KFold(n_splits=10,shuffle=True,random_state=1)\nkfold_acc=cross_val_score(knn_model,train_X,train_y,cv=kfold).mean()\nprint(\"Cross Val : \",kfold_acc)","aee83852":"cr_results = pd.DataFrame({'Method':['Cross Val KNN Regressor '], 'accuracy': kfold_acc},index={'21'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","2e5a7ef4":"\nL=LinearRegression()\nK=KNeighborsRegressor(n_neighbors=3)\nS=SVR(kernel='linear')","83af873f":"ebl=VotingRegressor(estimators=[('L',L),('K',K),('S',S)])\nebl.fit(train_X,train_y)","b0dbb8c6":"ebl_predict = ebl.predict(test_X)\nprint('Performance on training data using Ensemble technique:',ebl.score(train_X,train_y))\nprint('Performance on testing data using Ensemble technique :',ebl.score(test_X,test_y))\nacc_RDT=r2_score(test_y, ebl_predict)\nprint('Accuracy Ensemble : ',acc_RDT)\nprint('MSE: ',mean_squared_error(test_y, ebl_predict))","2f3bfd00":"cr_results = pd.DataFrame({'Method':['Ensemble '], 'accuracy': acc_RDT},index={'22'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","63ed23a7":"kfold= KFold(n_splits=10,shuffle=True,random_state=1)\nkfold_acc=cross_val_score(ebl,train_X,train_y,cv=kfold).mean()\nprint(\"Cross Val : \",kfold_acc)","5d56f299":"cr_results = pd.DataFrame({'Method':['Cross Val Ensemble '], 'accuracy': kfold_acc},index={'23'})\nresults=pd.concat([results,cr_results])\nresults = results[['Method', 'accuracy']]\nresults","69b122a4":"values=z_dataframe.values","577946b0":"values","fb90953f":"scores=list()   \nfor i in range(1000):\n    # prepare train and test sets\n    train = resample(values, n_samples=len(z_dataframe))  # Sampling with replacement \n    test = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n    \n    \n     # fit model\n    gbm_model = GradientBoostingRegressor(n_estimators=50)\n    # fit against independent variables and corresponding target values\n    gbm_model.fit(train[:,:-1], train[:,-1]) \n    # Take the target column for all rows in test set\n\n    y_test = test[:,-1]    \n    # evaluate model\n    # predict based on independent variables in the test data\n    score = gbm_model.score(test[:, :-1] , y_test)\n    predictions = gbm_model.predict(test[:, :-1])  \n\n    scores.append(score)\nscores","4e0b67e2":"plt.hist(scores)\nplt.show()","0444f41f":"alpha = 0.95                             # for 95% confidence \np = ((1.0-alpha)\/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\nlower = max(0.0, np.percentile(scores, p))  \np = (alpha+((1.0-alpha)\/2.0)) * 100\nupper = min(1.0, np.percentile(scores, p))\nprint('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))","f9062d69":"scores=list()   \nfor i in range(1000):\n    # prepare train and test sets\n    train = resample(values, n_samples=len(z_dataframe))  # Sampling with replacement \n    test = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n    \n    \n     # fit model\n    gbm_model = RandomForestRegressor(n_estimators=50)\n    # fit against independent variables and corresponding target values\n    gbm_model.fit(train[:,:-1], train[:,-1]) \n    # Take the target column for all rows in test set\n\n    y_test = test[:,-1]    \n    # evaluate model\n    # predict based on independent variables in the test data\n    score = gbm_model.score(test[:, :-1] , y_test)\n    predictions = gbm_model.predict(test[:, :-1])  \n\n    scores.append(score)\nscores","c731b9c1":"plt.hist(scores)\nplt.show()","879fd6f4":"alpha = 0.95                             # for 95% confidence \np = ((1.0-alpha)\/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\nlower = max(0.0, np.percentile(scores, p))  \np = (alpha+((1.0-alpha)\/2.0)) * 100\nupper = min(1.0, np.percentile(scores, p))\nprint('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))","00899ac3":"<h3>The bootstrap random forest classification model performance is between 84.3%-90.3% which is better than other classification algorithms.<\/h3>","9281e5ef":"<li><b>Cement vs other independent attributes:<\/b> This attribute does not have any significant relation with slag, ash, water, superplatic, coarseagg,fineagg and age. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.<\/li>\n<li><b>Slag vs other independent attributes: <\/b> This attribute also does not have any significant relation with ash, water, superplatic, coarseagg,fineagg and age. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.<\/li>\n<li><b>Ash vs other independent attributes:<\/b>  This attribute also does not have any significant relation with water, superplatic, coarseagg,fineagg and age. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.<\/li>\n<li><b>Water vs other independent attributes: <\/b> This attribute have negative linear relationship with superplastic and fineagg. It does not have any significant relationship with other independent atributes. This is true as Superplasticizers allows the reduction of water in the concrete upto the extent of 30% without reducing the workability.<\/li>\n<li><b>Superplastic vs other independent attributes:<\/b> This attribute have negative linear relationship with water only. It does not have any significant relationship with other independent attributes.\ncoarseagg vs other independent attributes:This attribute also does not have any significant relation with any other attributes. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.<\/li>\n<li><b>Fineagg vs other independent attributes:<\/b> It has negative linear relationship with water. It does not have any significant relation with any other attributes. It almost spread like a cloud. If we had calculated the r value it would have come close to 0.<\/li>","08badcfd":"Still a overfitting model without any signficant change in accuracy score.","614095f2":"So, cement, age and water are significant attributes.\nHere, ash, coarseagg, fineagg, superplastic and slag are the less significant variable.These will impact less to the strength column.","5a886fc3":" It is linearly related to the cement. The relationship is positive and we can see that for a given value of cement we have a multiple values of strength. Which one should we pick we don't know. Hence Cement though it has positive relationship with the strength, it is not a very good predictor. It is a weak predictor.","4c30c297":"<h3>Obv<\/h3>\n<li>Ash has two gaussians and rightly skewed.<\/li>\n","60ab599e":"<h3>Obv<\/h3>\nWe can see that<br>\nAge is positively skewed (right skewed)<br>\nCoarseagg and Fineagg is slightly left skewed<br>\nOthers are slightly right skewed<br>\n\n<h2>Univariate analysis<\/h2>\n<h2>Cement<\/h2>","0ea44762":"<h3>Cement Vs Strength<\/h3>","7a075e0b":"a. After applying all the models we can see that Random Forest Regressor, Random Forest Regressor k fold, Gradient Boost Regressor, Gradient Boost Regressor k fold, Bagging Regressor are giving better results as compared to other models.<br><br>","e5f20027":"<h3>Obv<\/h3>\n<li>There is nothing much to infer but we can see cement attribute have almost normal curve.<\/li>\n","cee9c034":"<h2>Multivariate Analysis<\/h2>","8802fc08":"\n<h2>Corelation of Attributes<\/h2>","c1c08639":"<h2>Ridge Regression Model<\/h2>","db4b5841":"<h3>Managing the Outliers<\/h3>","3e50cf79":"<h2>Gradient Boosting<\/h2>","20411d00":"<h2>Random Forest<\/h2>","ad50e3b1":"The data given has  9 columns and consist of 1030 data. And all the data is read correctly.<br>\nThere are eight independent variables and one dependent variable (i.e strenght)","ab7fdb11":"<h2>Water<\/h2>","1a03901f":"<h3>Obv<\/h3>\n<li>Here most of the column share high collinearity both negatively and positively. <\/li>\n<li>water shows significant negative relationship with superplastic and fineagg. It also shows some kind of positive relationship with slag and age.<\/li>\n<li>This should not be the case in the model.<\/li>\n<li>As data are correlated, we can drop some of the columns which are corelated.<\/li>\n\n\n","b2ad93a3":"<h2>PairPlot<\/h2>","8eb5e481":"This is a overfitting model as the dataset is performing 99% accurate in trainnig data.<br>The accuracy on test data is less.","919c2a77":"We can see there are 2 outliers on the right","40f02ebc":"There are no missing values. ","c1db8d44":"No much findings.","4e5601d8":"<h2>Slag<\/h2>","284828eb":"<h3>Obv<\/h3>\n    The above information shows the following<br>\n    a. The attributes are either int or float  <br>\n    <br>\n       \n","a4f548e4":"Random Forest","fc8584a9":"Hence no independent attribute is useful to predict dependent values single handedly.","b84fc83b":"<h2>Boot Strap Sampling<\/h2>","4fc0795d":"<h3> Splitting the Data<\/h3>\nSplitting the model in 7:3 ratio","841266a8":"<h2>Bagging Regression<\/h2>\n    ","bab4164b":"We can see there are 10 outliers on the right","630ad4b9":"<h2> SVR <\/h2>","75cd5887":"<h2>Lasso Regression Model<\/h2>","e44ba369":"There is no outliers in Cement data.","03cfe6e6":"<h2>Linear Regression Model<\/h2>","414b61d7":" there are no outliers.","d8a56864":"<h3>Super Plastic Vs Strength<\/h3>","1d887819":"<h3>Given:<\/h3>\nThe actual concrete compressive strength (MPa) for a given mixture under a\nspecific age (days) was determined from laboratory. Data is in raw form (not\nscaled). The data has 8 quantitative input variables, and 1 quantitative output\nvariable, and 1030 instances (observations).\n<h3>Objective:<\/h3>\nModeling of strength of high performance concrete using Machine Learning","92a04f70":"<h2>Super Plastic<\/h2>","ac5e28a7":"<h2>Pruning the DTree<\/h2>","1f646a67":"<h3>Obv<\/h3>\n<li>Slag has two gausssians and rightly skewed.It shows the presence of outlies.<\/li>\n","435b3ebc":"This model is also overfitting but the accuracy score is pretty good.","e26f5b1b":"<h3>We can infer that:<\/h3>\nWe can see that cement,slag,ash are left skewed.<br>\n\n\n\n","7197de91":"<h2>Ash<\/h2>","96852a35":"For a given value of Superplastic, we have different values of strength. Hence, It is not a good predictor.","e002af70":"<h3>Obv<\/h3>\n<li>Water has atleast guassians and slighly left skewed.It shows the presence of outlies.<\/li>\n","f5b02b9c":"Normalising the data","37a58b6f":"<h3>Slag Vs Strength<\/h3>","d1d2fcf5":"<h2>KNN<\/h2>","d0d8986c":"<h2>Dropping some features<\/h2>","6f87db26":"<h3>Age Vs Strength<\/h3>","afb078e2":"<h2>Dtree<\/h2>","f5f16847":"Here we can see n_neighnors=3 is suitable","9e0fe1ae":"For a given value of age, we have different values of strength. Hence, It is not a good predictor.","82e67a29":"As we can see all the attributes in the same scale(unit) except the age attribute. Therefore, we need to scale  the attributes. ","84bdaa0f":"<h3>Obv<\/h3>\n<li>superplastic has multiple gaussians and rightly skewed.It shows the presence of outlies.<\/li>\n","783883de":"<h2>Ada Boosting<\/h2>\n","2c02308e":"<h2>1. Reading the data<\/h2>","df8d67b3":"We can see there are 4 outliers on the right and 5 outliers on left.","11887806":"<h2>Preparing the data for modeling<\/h2>","2a06d371":"<h2>Ensemble<\/h2>"}}