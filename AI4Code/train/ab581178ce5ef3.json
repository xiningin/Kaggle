{"cell_type":{"bb0814b2":"code","af3f16d1":"code","24d2a630":"code","34f68487":"code","862990b8":"code","9ba9230e":"code","9c1dba3a":"code","776aa54a":"code","c33c3f54":"code","21d61aeb":"code","9902bd03":"code","35e57683":"code","c8736cc6":"code","2db4875d":"code","19033099":"code","0b70e0e6":"code","9dabcbda":"code","3db2c1b9":"code","92c2ff00":"code","4ec69b75":"code","68176fab":"code","134a1d00":"code","e3c7d143":"code","150aea36":"code","7c960c7e":"code","11a53ef9":"code","920942c2":"code","62b1121e":"code","facd558a":"code","2a65c223":"code","aeb3edb1":"code","b70eedbe":"code","66523b75":"markdown","308610ce":"markdown","170c2e0c":"markdown","ab85be0c":"markdown","e92cdac0":"markdown","2c439b24":"markdown","946fabf1":"markdown","cbd9de43":"markdown","594f9549":"markdown","1e234f1b":"markdown","1a53004b":"markdown","ea690905":"markdown","295023bc":"markdown","14b1de05":"markdown","7aed3537":"markdown","ff266856":"markdown","934a283b":"markdown","587e4c51":"markdown","c1587476":"markdown","a87a7ff1":"markdown","4a5203ca":"markdown","e39fd501":"markdown","d2efa6b5":"markdown","96871d51":"markdown","3878dc1a":"markdown","bb9ad114":"markdown","c2490d57":"markdown","9f396ed9":"markdown","c158800f":"markdown","d5775933":"markdown","54e70fb6":"markdown","b0fdb992":"markdown","135addeb":"markdown","102f815b":"markdown","703d560f":"markdown","5ccbf41f":"markdown","cf23f3a6":"markdown","22cdf7e4":"markdown","a2f0863a":"markdown","001586be":"markdown","b2f5d488":"markdown","f71b5b4f":"markdown","b646334b":"markdown","e56d6718":"markdown","1885dfa8":"markdown","bee859b0":"markdown","85ff3d3f":"markdown","506b5707":"markdown","0c8150cc":"markdown"},"source":{"bb0814b2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom google.cloud import bigquery #For BigQuery\nfrom bq_helper import BigQueryHelper #For BigQuery\nfrom sklearn.ensemble import RandomForestClassifier #for the model\nfrom sklearn.metrics import roc_curve, auc #for model evaluation\nfrom sklearn.metrics import classification_report #for model evaluation\nfrom sklearn.metrics import confusion_matrix #for model evaluation\nfrom sklearn.model_selection import train_test_split #for data splitting\nimport eli5 #for purmutation importance\nfrom eli5.sklearn import PermutationImportance\nimport shap #for SHAP values\nfrom pdpbox import pdp, info_plots #for partial plots\nnp.random.seed(123) #ensure reproducibility\n\npd.options.mode.chained_assignment = None  #hide any pandas warnings","af3f16d1":"us_traffic = BigQueryHelper(\"bigquery-public-data\", \"nhtsa_traffic_fatalities\")\nus_traffic.head(\"accident_2015\")","24d2a630":"accidents_query_2015 = \"\"\"SELECT month_of_crash,\n                                 day_of_week,\n                                 hour_of_crash,\n                                 manner_of_collision_name,\n                                 light_condition_name,\n                                 land_use_name,\n                                 latitude,\n                                 longitude,\n                                 atmospheric_conditions_1_name,\n                                 number_of_drunk_drivers,\n                                 number_of_fatalities\n                          FROM `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015`\n                          WHERE number_of_fatalities = 1\n                          AND longitude < 0\n                          AND longitude > -140\n                          LIMIT 5000\n                      \"\"\" \n\naccidents_query_2016 = \"\"\"SELECT month_of_crash,\n                                 day_of_week,\n                                 hour_of_crash,\n                                 manner_of_collision_name,\n                                 light_condition_name,\n                                 land_use_name,\n                                 latitude,\n                                 longitude,\n                                 atmospheric_conditions_1_name,\n                                 number_of_drunk_drivers,\n                                 number_of_fatalities\n                          FROM `bigquery-public-data.nhtsa_traffic_fatalities.accident_2016`\n                          WHERE number_of_fatalities = 1\n                          AND longitude < 0\n                          AND longitude > -140\n                          LIMIT 5000\n                      \"\"\" \n\naccidents_query_multiple_2015 = \"\"\"SELECT month_of_crash,\n                                          day_of_week,\n                                          hour_of_crash,\n                                          manner_of_collision_name,\n                                          light_condition_name,\n                                          land_use_name,\n                                          latitude,\n                                          longitude,\n                                          atmospheric_conditions_1_name,\n                                          number_of_drunk_drivers,\n                                          number_of_fatalities\n                                    FROM `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015`\n                                    WHERE number_of_fatalities > 1\n                                    AND longitude < 0\n                                    AND longitude > -140\n                      \"\"\" \n\naccidents_query_multiple_2016 = \"\"\"SELECT month_of_crash,\n                                          day_of_week,\n                                          hour_of_crash,\n                                          manner_of_collision_name,\n                                          light_condition_name,\n                                          land_use_name,\n                                          latitude,\n                                          longitude,\n                                          atmospheric_conditions_1_name,\n                                          number_of_drunk_drivers,\n                                          number_of_fatalities\n                                    FROM `bigquery-public-data.nhtsa_traffic_fatalities.accident_2016`\n                                    WHERE number_of_fatalities > 1\n                                    AND longitude < 0\n                                    AND longitude > -140\n                      \"\"\" ","34f68487":"accidents_2015 = us_traffic.query_to_pandas(accidents_query_2015)\naccidents_2015_multiple = us_traffic.query_to_pandas(accidents_query_multiple_2015)\n\naccidents_2016 = us_traffic.query_to_pandas(accidents_query_2016)\naccidents_2016_multiple = us_traffic.query_to_pandas(accidents_query_multiple_2016)\n\nframes = [accidents_2015, accidents_2015_multiple, accidents_2016, accidents_2016_multiple]\naccidents_all = pd.concat(frames)","862990b8":"accidents_all['number_of_fatalities'].hist()","9ba9230e":"accidents_all['drunk_driver_involved'] = 0\naccidents_all['drunk_driver_involved'][accidents_all['number_of_drunk_drivers'] > 1] = 1\naccidents_all = accidents_all.drop('number_of_drunk_drivers', 1)\n\naccidents_all['Multiple_fatalities'] = 0\naccidents_all['Multiple_fatalities'][accidents_all['number_of_fatalities'] > 1] = 1\naccidents_all = accidents_all.drop('number_of_fatalities', 1)","9c1dba3a":"accidents_all.groupby(['land_use_name']).size()","776aa54a":"accidents_all = accidents_all[accidents_all['hour_of_crash'] != 99]\naccidents_all = accidents_all[accidents_all['manner_of_collision_name'] != 'Unknown']\naccidents_all = accidents_all[accidents_all['light_condition_name'] != 'Unknown']\naccidents_all = accidents_all[accidents_all['atmospheric_conditions_1_name'] != 'Unknown']\naccidents_all = accidents_all[accidents_all['land_use_name'] != 'Unknown']","c33c3f54":"accidents_all = accidents_all[accidents_all['land_use_name'] != 'Trafficway Not in State Inventory']","21d61aeb":"x = accidents_all['longitude']\ny = accidents_all['latitude']\n\nplt.plot(x, y)","9902bd03":"accidents_all.head(10)","35e57683":"accidents_all['month_of_crash'] = accidents_all['month_of_crash'].astype('category')\naccidents_all['day_of_week'] = accidents_all['day_of_week'].astype('category')\naccidents_all['hour_of_crash'] = accidents_all['hour_of_crash'].astype('category')\naccidents_all['drunk_driver_involved'] = accidents_all['drunk_driver_involved'].astype('category')\n\naccidents_all.dtypes","c8736cc6":"accidents_all = pd.get_dummies(accidents_all, drop_first=True) #from the reduntant dummy categories","2db4875d":"accidents_all.head()","19033099":"accidents_all.shape","0b70e0e6":"X_train, X_test, y_train, y_test = train_test_split(accidents_all.drop('Multiple_fatalities', 1), accidents_all['Multiple_fatalities'], test_size = .3, random_state=25)","9dabcbda":"model = RandomForestClassifier()\nmodel.fit(X_train, y_train)","3db2c1b9":"y_predict = model.predict(X_test)\ny_pred_quant = model.predict_proba(X_test)[:, 1]\ny_pred_bin = model.predict(X_test)","92c2ff00":"confusion_matrix = confusion_matrix(y_test, y_pred_bin)\nconfusion_matrix","4ec69b75":"total=sum(sum(confusion_matrix))\n\nsensitivity = confusion_matrix[0,0]\/(confusion_matrix[0,0]+confusion_matrix[0,1])\nprint('Sensitivity : ', sensitivity )\n\nspecificity = confusion_matrix[1,1]\/(confusion_matrix[1,0]+confusion_matrix[1,1])\nprint('Specificity : ', specificity)","68176fab":"fpr, tpr, thresholds = roc_curve(y_test, y_pred_quant)\n\nfig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.rcParams['font.size'] = 12\nplt.title('ROC curve for diabetes classifier')\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.grid(True)","134a1d00":"auc(fpr, tpr)","e3c7d143":"perm = PermutationImportance(model, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = X_test.columns.tolist())","150aea36":"X_test_sample = X_test.iloc[:200]\n\nbase_features = accidents_all.columns.values.tolist()\nbase_features.remove('Multiple_fatalities')\n\nfeat_name = 'manner_of_collision_name_Front-to-Front'\npdp_dist = pdp.pdp_isolate(model=model, dataset=X_test_sample, model_features=base_features, feature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","7c960c7e":"inter1  =  pdp.pdp_interact(model=model, dataset=X_test_sample, model_features=base_features, features=['land_use_name_Urban', 'manner_of_collision_name_Front-to-Front'])\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=['land_use_name_Urban', 'manner_of_collision_name_Front-to-Front'], plot_type='contour')\nplt.show()","11a53ef9":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test_sample)\n\nshap.summary_plot(shap_values[1], X_test_sample, plot_type=\"bar\")","920942c2":"shap.summary_plot(shap_values[1], X_test_sample)","62b1121e":"def accident_risk_factors(model, accident):\n\n    explainer = shap.TreeExplainer(model)\n    shap_values = explainer.shap_values(accident)\n    shap.initjs()\n    return shap.force_plot(explainer.expected_value[1], shap_values[1], accident)","facd558a":"data_for_prediction = X_test.iloc[1,:].astype(float)\naccident_risk_factors(model, data_for_prediction)","2a65c223":"data_for_prediction = X_test.iloc[5,:].astype(float)\naccident_risk_factors(model, data_for_prediction)","aeb3edb1":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X_test_sample)\n\nshap.dependence_plot('latitude', shap_values[1], X_test_sample, interaction_index=\"manner_of_collision_name_Front-to-Front\")","b70eedbe":"shap_values = explainer.shap_values(X_train.iloc[:100])\nshap.force_plot(explainer.expected_value[1], shap_values[1], X_train.iloc[:100])","66523b75":"## Permutation Importance","308610ce":"We only have two possibilities, 0 and 1 (not front-to-front and front-to-front, respectively). As you can see, if an accident manner is front-to-front, the outcome increases by ~25%\n\nWe can also create 2D plots by selecting two variables. Let's see how the front-to-front variable interacts with Urban land use,","170c2e0c":"# Introduction\n\n### Computer says what?\n\nWith the rise of any new technology comes the inevitable rise of problems and criticisms. With machine-learning, it's become an almost mandatory retort in some circles to decry *\"yes, but it's a black box!\"*. Or, as I once saw a salesman say when talking about a new medical device that uses deep learning, *\"no-one knows how it works!\".*\n\nThe idea of **machine-learning explainability** is not new, but in my experience, you had to previously plug examples in, see what you got out, and try to infer what was going on. For example, creating a Titanic prediction model, running through a fictional passenger, and seeing the probability of survival that came out.\n\nToday, a raft of tools and techniques are available to help and (crucially) visualize this process. I won't get into the details, as the [course by Dan Becker on Kaggle Learn](https:\/\/www.kaggle.com\/learn\/machine-learning-explainability) does that infinitely better that I could. Having made my way through that course, I've applied some of these ideas to the Kaggle [US Traffic Fatality Records](https:\/\/www.kaggle.com\/usdot\/nhtsa-traffic-fatalities) dataset. I also found the official shap library GitHub page useful [here](https:\/\/github.com\/slundberg\/shap).\n\n### The Dataset\n\nThis dataset (accessible via BigQuery) contains multiple tables, including data on the passengers, drivers, visibility at the time, damage done to the vehicle, etc. I'm going to stick to the main tables of the accident details, which give information on crash characteristics and environmental conditions at the time of the crash. There is one record per crash.\n\n\n### The Question\n\nAt first, I considered putting together a model to see what factors led to a fatal accident, before realising that *all* of the accidents were fatal (the clue is in the title, I guess). So, next I wondered if there were factors that could distinguish accidents involving a single fatality and those that involved multiple fatalities. The emphasis isn't so much on creating an amazing model, but I wanted to see if I could get some predictive power, which I could then explore future with such tools as permutation importance, partial plots and SHAP values.\n","ab85be0c":"<a id='section2'><\/a>","e92cdac0":"These work by showing the influence of the values of every variable in a single row, compared to their baseline values (learn more [here](https:\/\/www.kaggle.com\/dansbecker\/shap-values)). Let's take a look at the overall values,","2c439b24":"Review with a [confusion matrix](https:\/\/www.dataschool.io\/simple-guide-to-confusion-matrix-terminology\/),","946fabf1":"There are a few variables with unknown values. For example,","cbd9de43":"# The Model\n\nI'm going to create a fairly basic model (random forest), to see if there is any predictive ability,","594f9549":"Double-check that the longitude and latitude values are sensible,","1e234f1b":"One-hot encode the data,","1a53004b":"Let's also look at the sensitivity and specificity (recall and precision are also often used to evaluate binary classification problems),","ea690905":"Arguably higher overall SHAP values at the lower and upper latitudes?\n\nFinally, let's take an ensemble of the individual row-plots and plot them all together (imagine rotating the plots and stacking them horizontally),","295023bc":"These looks similar to the permutation importance list.\n\nOne of my favourite plots from all of the explainability tools is the summary plot, which shows the effect from the individual rows,","14b1de05":"# Contents\n\n1. [Introduction ](#section1)\n2. [The Data](#section2)\n3. [The Model](#section3)\n4. [The Explanation](#section4)\n5. [Conclusion](#section5)","7aed3537":"**Permutation importance** is the first tool for understanding a machine-learning model, and involves shuffling individual variables in the validation data (after a model has been fit), and seeing the effect on accuracy. Learn more [here](https:\/\/www.kaggle.com\/dansbecker\/permutation-importance).\n\nLet's take a look,","ff266856":"Finally, let's create a [ROC plot](https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic),","934a283b":"Now the training data looks like this,","587e4c51":"# The Data\n\nFirst, let's load the relavent libraries, including those for BigQuery,","c1587476":"# Conclusion\n","a87a7ff1":"<a id='section3'><\/a>","4a5203ca":"Now, let's merge them into a single dataframe,","e39fd501":"Let's take a look at the data,","d2efa6b5":"Some things really jump out here. For example, \n\n- The top feature, 'not collision with a motor vehicle', shows that when this is true (red), the probability of multiple fatalities is reduced. That makes a lot of sense\n- Front-to-front collision again. All the cases in blue (corresponding to 'not front-to-front') reduce the impact, and all the cases in red (corresponding to front-to-front) increase the impact\n- The land use, urban vs rural (or *is urban* vs *not urban*), are directly opposite each other, which makes sense\n- Longitude and latitude are jumbled together and don't seem to have a great deal of impact\n- Days of the week 7 is interesting. According to the data dictionary, this is Sunday, and it's showing that accidents on this day tend to increase the impact. Is this people out drinking at the weekend? (perhaps early Sunday morning, i.e. after a Saturday night out?)\n\nBelow is a function (written in one of the ML explainability exercises) that creates another type of plot. This is for an individual accident, and show the factors that increase (red) and decrease (blue) the impact compared to a baseline prediction,","96871d51":"Quick check on the shape,","3878dc1a":"It looks like the lowest probability is when the accident occurs on urban land and isn't a front-to-front accident (bottom-right corner).","bb9ad114":"# What Causes an Accident to Involve Multiple Fatalities?","c2490d57":"<a id='section5'><\/a>","9f396ed9":"Very cool. Hover over to see the influence of the different variables.","c158800f":"We can look at the SHAP values of two variables in the sort of plot below. These are a little limited with this dataset as it's mainly made up of categorical variables. Below shows latitude and front-the-front collisions,","d5775933":"# The Explanation","54e70fb6":"Sort out any data types into categorical,","b0fdb992":"I'm going to create a new variable called **'multiple_fatalities'** that is 0 for single fatalities and 1 for anything greater than 1. I'm also going to change the 'number_of_drunk_drivers' into a binary category (any drunk drivers involved or not?), because the number of drunk drivers could give the model a clue as to how many people are involved in the accident, which might give it an unfair advantage,","135addeb":"<a id='section4'><\/a>","102f815b":"<a id='section1'><\/a>","703d560f":"And check the area-under-the-curve (AUC),","5ccbf41f":"As a rule of thumb, an AUC can be classed as follows,\n\n- 0.90 - 1.00 = excellent\n- 0.80 - 0.90 = good\n- 0.70 - 0.80 = fair\n- 0.60 - 0.70 = poor\n- 0.50 - 0.60 = fail\n","cf23f3a6":"That looks about right.\n\nThe training data now looks like this,","22cdf7e4":"OK, so something is working. The model seems to be able to distinguish between single and multiple-fatality accidents. Now, let's see if we can figure out how it's working.","a2f0863a":"Let's take a quick look and the distribution,","001586be":"No doubt many people reading this could do a better job at creating a machine-learning model to predict accident severity. However, it seems to be working on some level, and the various explainability tools have highlighted some interesting points.\n\nImagine if a hospital could predict the severity of an accident, and consequently plan better, by knowing a few features of the crash scene? An interesting thought!","b2f5d488":"Much lower than baseline. The accident occuring on urban land is the largest factor in reducing the probability.","f71b5b4f":"I'm going to remove them,","b646334b":"OK, there is a lot of data there, both in terms of the number of accidents and the number of columns. Let's limit it. I'll pick out variables that I think may have an influence on the severity of the accident (which is, in essence, the question I'm asking), and limit to 5000 rows each from the two years available (2015 and 2016). Note that there is a large imbalance between accidents involving 1 fatality and those involving multiple fatalities Therefore, I'm getting 10,000 single-fatality accidents and *all* the multiple-fatality accidents. I've also set some limits on the latitude and longitude, as I noticed some odd values,","e56d6718":"I'm also going to remove a handful of rows with 'Trafficway Not in State Inventory' as the land use,","1885dfa8":"## SHAP Values","bee859b0":"So, it looks like variables relating to the manner of the accident are the most important. That seems to make sense.\n\n## Partial Dependence Plots\n\nNext, I'm going to use a **Partial Dependence Plot** (learn more [here](https:\/\/www.kaggle.com\/dansbecker\/partial-plots)). These plots vary a single variable in a single row across a range of values and see what effect it has on the outcome. It does this for several rows and plots the average effect. Let's take a look at the front-to-front collision variable, which was near the top of the permutation importance list,","85ff3d3f":"Here we see an output of 0.7 compared to a baseline of 0.3221. It looks like the biggest factor increasing the probability is the front-to-front collision.\n\nHere is another,","506b5707":"Get the predictions of the test set,","0c8150cc":"Let's look at the first accident,"}}