{"cell_type":{"bcd900cd":"code","0a9800ed":"code","63b084ee":"code","bf69a1b6":"code","5430e48f":"code","645c63e0":"code","fb0d89e4":"code","dd38f378":"code","d44f06be":"code","3dde6334":"code","6d396f81":"code","ca713061":"code","e02631f4":"code","520a6764":"code","da3b4bf9":"code","cef706d3":"code","920ffb47":"code","89222e97":"code","7554f403":"code","437468dd":"code","442c59ad":"code","eda1e754":"code","9a249fbb":"code","2dfba9ea":"code","90795161":"code","a14beab6":"code","4342d3e7":"code","175d17ff":"code","2f5200d1":"code","d6188091":"code","d84cf26e":"code","22f51647":"code","4cf273a8":"code","274a2c9f":"markdown","bd0a61de":"markdown","da1b6234":"markdown","c1512134":"markdown","c2b7fedd":"markdown","1f584efd":"markdown","a2de9c84":"markdown","0e684c5a":"markdown","9138b5ec":"markdown","42fb8496":"markdown","17c78c66":"markdown"},"source":{"bcd900cd":"## Added log to y values \n#Thanks to Ambrosm and Remek Kinas \n#https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model\/comments","0a9800ed":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\n\n#import lightgbm as lgb \n#import xgboost as xgb\n#from catboost import CatBoostRegressor\n\nfrom sklearn.linear_model import LinearRegression,HuberRegressor,SGDRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n\nimport optuna\nimport math","63b084ee":"PRODUCT_SPLIT= False\nDate_Augmentation = True\n\n#No OPTUNA Needed for Huber\n\n##Add DATA\nADD_2014 = False \n\n#Holidays\nHOLIDAYS = True\nNEXT_HOLIDAY = True\n\nPOST_PROCESSING = False\n\nVAL_SPLIT = \"2017-12-31\" #\"2018-05-31\"","bf69a1b6":"EPOCHS = 10000    \nEARLY_STOPPING = 30\nDEVICE = \"cpu\"\n\nSCALER_NAME = \"MinMax\"  #None MinMax Standard\nSCALER = MinMaxScaler()  #MinMaxScaler\n\nMODEL_TYPE = \"Huber\" #lightgbm catboost\n","5430e48f":"train = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/train.csv\",index_col = 0)\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/test.csv\",index_col = 0)\ngdp_df = pd.read_csv('..\/input\/gdp-20152019-finland-norway-and-sweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\ngdp_df.set_index('year', inplace=True)\n\nif HOLIDAYS:\n    holidays = pd.read_csv(\"..\/input\/holidays-finland-norway-sweden-20152019\/Holidays_Finland_Norway_Sweden_2015-2019.csv\",usecols = [\"Date\",\"Country\",\"Name\"]                      )\n    holidays.rename(columns = {\"Date\":\"date\",\"Country\":\"country\",\"Name\":\"holiday\"},inplace= True)\n    holidays[\"holiday\"]= 1\n    holidays[\"holiday\"]= holidays[\"holiday\"].astype(\"int32\")\n    holidays[\"date\"] = pd.to_datetime(holidays[\"date\"])","645c63e0":"#Make date\ntrain[\"date\"] = pd.to_datetime(train[\"date\"])\ntest[\"date\"] = pd.to_datetime(test[\"date\"])","fb0d89e4":"## Adding in Synthetic 2014 data = 2015 *0.98\nif ADD_2014:\n    train_2014= train [train[\"date\"]<\"2016-01-01\"]\n    train_2014[\"date\"] = train_2014[\"date\"] - pd.DateOffset(years=1)\n    train_2014[\"num_sold\"] = train_2014[\"num_sold\"]*0.98\n    train = pd.concat([train_2014,train],axis=0,ignore_index=True)","dd38f378":"train.head()","d44f06be":"def public_hols(df):\n    df = pd.merge(df, holidays, how='left', on=['date', 'country'])\n    df.fillna(value = 0,inplace=True)\n    return df\n\nif HOLIDAYS:\n    train = public_hols(train)\n    test = public_hols(test)","3dde6334":"def get_gdp(row):\n    country = 'GDP_' + row.country\n    return gdp_df.loc[row.date.year, country]","6d396f81":"def engineer(df):\n    \"\"\"Return a new dataframe with the engineered features\"\"\"\n    \n    \n    #get GDP from file \n    df[\"gdp\"] = df.apply(get_gdp, axis=1)   #improves Huber & Tweedie\n    \n    df[\"day\"] = df[\"date\"].dt.day\n    df[\"dayofweek\"] = df[\"date\"].dt.dayofweek\n    df[\"month\"] = df[\"date\"].dt.month\n    df[\"year\"] = df[\"date\"].dt.year\n    \n    #play around with if Tree model - each varies \n    #df['dayofyear'] = df['date'].dt.dayofyear                ### This can cause noise - remove but keep inverse\n    df['inverse_dayofyear'] = 365 - df['date'].dt.dayofyear    # good for all \n    df.loc[df[\"year\"] == 2016 , \"inverse_dayofyear\"] = df.loc[df[\"year\"] == 2016 , \"inverse_dayofyear\"]+1     #Leap year in 2016\n    \n    df['quarter'] = 'Q' + df['date'].dt.quarter.astype(str)      # Good for trees & Huber bad for Tweedie\n    #df['daysinmonth'] = df['date'].dt.days_in_month           ## Also reduces performance\n     \n    #encoding\n    for country in ['Finland', 'Norway']:\n        df[country] = df.country == country\n    df['KaggleRama'] = df.store == 'KaggleRama'\n    for product in ['Kaggle Mug', 'Kaggle Sticker']:\n        df[product] = df['product'] == product\n    \n    df[\"Friday\"] = df[\"dayofweek\"] ==4\n    df[\"Sat_sun\"] = (df[\"dayofweek\"] ==5) |(df[\"dayofweek\"] ==6)\n    \n    df.drop([\"country\",\"store\",\"product\"], axis =1, inplace = True)\n    \n    # Seasonal variations (Fourier series)\n    # The three products have different seasonal patterns\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 21):\n        df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * math.pi * k)\n        df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * math.pi * k)\n        df[f'mug_sin{k}'] = df[f'sin{k}'] * df['Kaggle Mug']\n        df[f'mug_cos{k}'] = df[f'cos{k}'] * df['Kaggle Mug']\n        df[f'sticker_sin{k}'] = df[f'sin{k}'] * df['Kaggle Sticker']\n        df[f'sticker_cos{k}'] = df[f'cos{k}'] * df['Kaggle Sticker']\n\n    return df","ca713061":"train = engineer(train)\ntest = engineer(test)\n\n#Enconding the last features \ncategorical_feats = [\n    #country\",\"store\",\"product\",\n                     \"quarter\",\n                     #Friday\",\n                     #Sat_Sun\"\n                    ]\n\n# Huber improves with dropping first\ntrain = pd.get_dummies(train,columns= categorical_feats,drop_first=True)\ntest = pd.get_dummies(test,columns= categorical_feats,drop_first=True)","e02631f4":"#Calculate the days till the next holiday\ndef next_holiday(x):\n    i=1\n    while sum(holidays[\"date\"] == pd.Timestamp(x) + pd.DateOffset(days=i)) ==0:\n        i+=1\n        # break in case there is no future holiday\n        if i >200:\n            i=0\n            break\n            break\n    return i\n\nif NEXT_HOLIDAY:\n    holidays[\"date\"] = pd.to_datetime(holidays[\"date\"])\n    train[\"to_holiday\"] = train[\"date\"].apply(lambda x : next_holiday(x))\n    test[\"to_holiday\"] = test[\"date\"].apply(lambda x : next_holiday(x))","520a6764":"def SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)","da3b4bf9":"def scale_data(X_train, X_test, test):\n    scaler= SCALER\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\n    test = scaler.transform(test)\n    \n    return X_train, X_test, test","cef706d3":"prior_2017 = train[train[\"date\"]<=VAL_SPLIT].index\nafter_2017 = train[train[\"date\"]>VAL_SPLIT].index","920ffb47":"train.index = train[\"date\"]\ntrain.drop(\"date\",axis=1,inplace=True)\n\ntest.index = test[\"date\"]\ntest.drop(\"date\",axis=1,inplace=True)","89222e97":"X = train.drop(\"num_sold\", axis=1)\ny= train[\"num_sold\"]","7554f403":"X_train = train.iloc[prior_2017,:].drop(\"num_sold\", axis=1)\nX_test = train.iloc[after_2017,:].drop(\"num_sold\", axis=1)\ny_train= train.iloc[prior_2017,:][\"num_sold\"]\ny_test= train.iloc[after_2017,:][\"num_sold\"]","437468dd":"X_train,X_test,test =scale_data(X_train,X_test,test)","442c59ad":"params = {\"epsilon\":1.3}","eda1e754":"## CREATE lightgbm model\ndef fit_model(X_train,y_train,X_test,y_test):\n    \n    model = HuberRegressor(max_iter=EPOCHS, **params)\n    model.fit(X_train,np.log1p(y_train))\n\n\n    test_predictions = np.expm1(model.predict(X_test))\n    print(\"SMAPE:\", SMAPE(y_test,test_predictions))\n    \n    return test_predictions, model","9a249fbb":"test_predictions, model = fit_model(X_train,y_train,X_test,y_test)","2dfba9ea":"final_predictions = np.expm1(model.predict(test))","90795161":"print(\"SMAPE :\",SMAPE(y_test,test_predictions) )\nprint(f\"\\n EPOCHS: {EPOCHS}\")\nprint(f\"\\n SCALER: {SCALER_NAME}\")\nprint(f\"\\n 2014 Data: {ADD_2014}\")\nprint(f\"\\n POST_PROCESSING: {POST_PROCESSING}\")","a14beab6":"final_predictions","4342d3e7":"sub = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv\",index_col = 0)","175d17ff":"if POST_PROCESSING:\n    # from previous run we are under predicting, lets scale the values upwards\n    print(\"Scaling predictions \")\n    print(\"preds_prior:\", final_predictions)\n    \n    sub[\"num_sold\"] = final_predictions*1.143\n    \n    print(\"preds after:\", np.array(sub[\"num_sold\"]))\nelse:\n    sub[\"num_sold\"] = final_predictions","2f5200d1":"sub.to_csv(\"submission.csv\")","d6188091":"sub.head()","d84cf26e":"sub.head()","22f51647":"#for visual only\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/test.csv\",index_col = 0)\ntest[\"date\"] = pd.to_datetime(test[\"date\"])\n\nfig,ax = plt.subplots(2,1, figsize=(25,20),sharey= True)\n\ndiff = y_test - test_predictions\nsns.lineplot(ax=ax[0], data= y_test, label=\"Train Actual\",ci=None)\nsns.lineplot(ax=ax[0], data = y_test,x = y_test.index , y = test_predictions, label =\"Validation Prediction\" ,ci=None)\nsns.lineplot(ax=ax[0],data =sub, x= test[\"date\"], y = \"num_sold\",label=\"Final Prediction\" ,ci=None) \n\nax[0].set_title(f\"Actual and Predicted Sales for {MODEL_TYPE}\")\n\nsns.lineplot(ax=ax[1], data = diff, label =\"Residuals\" )\nax[1].set_title(f\"Residuals for {MODEL_TYPE} for 2018\")\n\nplt.show()","4cf273a8":"plt.figure(figsize=(25,10))\n\nsns.lineplot(data= train[\"num_sold\"] ,label=\"Train Actual\",ci=None)\nsns.lineplot(data =sub, x= test[\"date\"], y = \"num_sold\",label=\"Final Prediction\" ,ci=None) \nplt.title(\"Actual and Predicted Sales\")\n\nplt.show()","274a2c9f":"#To DO\n1. Run a model for each product\n1. Recursive model \n1. Add Further Feature engineering - ambrosm https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model\/\n1. SARIMAX model","bd0a61de":"Thanks to [ambrosm](https:\/\/www.kaggle.com\/anirudhg15)\n\nhttps:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model\/notebook#More-feature-engineering-(advanced-model)","da1b6234":"obj is the objective function of the algorithm, i.e. what it's trying to maximize or minimize, e.g. \"regression\" means it's minimizing squared residuals.\n\nMetric and eval are essentially the same. They are used for Early stopping ","c1512134":"# Split and Scale","c2b7fedd":"# Final Train","1f584efd":"# Training Visualization","a2de9c84":"# Post Processing & Submission ","0e684c5a":"# Libraries","9138b5ec":"# Functions ","42fb8496":"# Load Data","17c78c66":"# Run model"}}