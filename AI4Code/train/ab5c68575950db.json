{"cell_type":{"8427e39e":"code","bc814a42":"code","504700db":"code","a991e8bb":"code","654372c8":"code","da736964":"code","f168cb1f":"code","d4efe8dc":"code","548d23e5":"code","0790815c":"code","65af8799":"code","5e90fe04":"code","1f30cfa4":"code","304f12c0":"code","40addc0e":"code","54cb7981":"code","e8836642":"code","d96076c9":"code","663e265a":"code","ef19fcc2":"code","e7ac3513":"code","a8dab25d":"code","23e4aebb":"code","4c482eab":"code","7c740208":"code","d6ce543d":"code","b5947d6b":"code","03882320":"code","3b33324e":"code","5f47b5db":"code","13946123":"code","ecf34bc7":"code","d62dd056":"code","f573d3c2":"code","8ea7fe44":"markdown","44d3ba14":"markdown","b227a448":"markdown","71a06957":"markdown","6aeb277f":"markdown","09f6fed6":"markdown","1562751d":"markdown","15135617":"markdown","4d039791":"markdown","eada5c5c":"markdown","aa1d41dc":"markdown","333f884a":"markdown","b4883c6e":"markdown","a19370b0":"markdown","cc24cf86":"markdown","4b86b94c":"markdown","6583f5a4":"markdown","60d24781":"markdown","afe76135":"markdown","923d9448":"markdown","2345f01d":"markdown"},"source":{"8427e39e":"import numpy as np \nimport pandas as pd \nimport json\nimport bq_helper\nfrom pandas.io.json import json_normalize\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\nimport lightgbm as lgb\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error","bc814a42":"# https:\/\/www.kaggle.com\/julian3833\/1-quick-start-read-csv-and-flatten-json-fields\nJSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\ndef load_df(csv_path='..\/input\/train.csv'):\n\n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'})\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n\n    return df\n        ","504700db":"%%time\ntrain = load_df(\"..\/input\/train.csv\")","a991e8bb":"%%time\ntest = load_df(\"..\/input\/test.csv\")","654372c8":"cols_to_drop = [col for col in train.columns if train[col].nunique() == 1]\ntrain.drop(cols_to_drop, axis=1, inplace=True)\ntest.drop([col for col in cols_to_drop if col in test.columns], axis=1, inplace=True)\nprint(f'Dropped {len(cols_to_drop)} columns.')","da736964":"# converting columns into more reasonable format\nfor col in ['visitNumber', 'totals.hits', 'totals.pageviews', 'totals.transactionRevenue']:\n    train[col] = train[col].astype(float)","f168cb1f":"train.head()","d4efe8dc":"for col in train.columns:\n    if train[col].isnull().sum() > 0:\n        rate = train[col].isnull().sum() * 100 \/ train.shape[0]\n        print(f'Column {col} has {rate:.4f}% missing values.')\n    if train[col].dtype == 'object':\n        if (train[col] == 'not available in demo dataset').sum() > 0:\n            rate = (train[col] == 'not available in demo dataset').sum() * 100 \/ train.shape[0]\n            print(f'Column {col} has {rate:.4f}% values not available in dataset.')","548d23e5":"plt.hist(np.log1p(train.loc[train['totals.transactionRevenue'].isna() == False, 'totals.transactionRevenue']));\nplt.title('Distribution of revenue');","0790815c":"grouped = train.groupby('fullVisitorId')['totals.transactionRevenue'].sum().reset_index()\ngrouped = grouped.loc[grouped['totals.transactionRevenue'].isna() == False]\nplt.hist(np.log(grouped.loc[grouped['totals.transactionRevenue'] > 0, 'totals.transactionRevenue']));\nplt.title('Distribution of total revenue per user');","65af8799":"counts = train.loc[train['totals.transactionRevenue'] > 0, 'fullVisitorId'].value_counts()\nprint('There are {0} paying users ({1} total) in train data.'.format(len(counts), train['fullVisitorId'].nunique()))\nprint('{0} users ({1:.4f}% of paying) have 1 paid transaction.'.format(np.sum(counts == 1), 100 * np.sum(counts == 1) \/ len(counts)))\nprint('{0} users ({1:.4f}% of paying) have 2 paid transaction.'.format(np.sum(counts == 2), 100 * np.sum(counts == 2) \/ len(counts)))\nprint('')\nprint('Count of non-zero transactions per user:')\ncounts.head(10)","5e90fe04":"train['totals.transactionRevenue'] = train['totals.transactionRevenue'].fillna(0)\ntrain['totals.transactionRevenue'] = np.log1p(train['totals.transactionRevenue'])\nsns.set(rc={'figure.figsize':(20, 16)})\ntrain_ = train.loc[train['totals.transactionRevenue'] > 0.0]\nsns.boxplot(x=\"device.deviceCategory\", y=\"totals.transactionRevenue\", hue=\"channelGrouping\",  data=train_)\nplt.title(\"Total revenue by device category and channel.\");\nplt.xticks(rotation='vertical')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()\n","1f30cfa4":"train['date'] = pd.to_datetime(train['date'].apply(lambda x: str(x)[:4] + '-' + str(x)[4:6] + '-' + str(x)[6:]))\ntest['date'] = pd.to_datetime(test['date'].apply(lambda x: str(x)[:4] + '-' + str(x)[4:6] + '-' + str(x)[6:]))","304f12c0":"train_['totals.transactionRevenue'].min(), train.shape, train_.shape","40addc0e":"train_ = train.loc[train['totals.transactionRevenue'] > 0]\nfig, ax1 = plt.subplots(figsize=(16, 8))\nplt.title(\"Trends of transactions number by paying and non-paying users\");\ntrain.groupby(['date'])['totals.transactionRevenue'].count().plot(color='brown')\nax1.set_ylabel('Transaction count', color='b')\nplt.legend(['Non-paying users'])\nax2 = ax1.twinx()\ntrain_.groupby(['date'])['totals.transactionRevenue'].count().plot(color='gold')\nax2.set_ylabel('Transaction count', color='g')\nplt.legend(['Paying users'], loc=(0.875, 0.9))\nplt.grid(False)","54cb7981":"fig, ax1 = plt.subplots(figsize=(16, 8))\nplt.title(\"Trends transaction count and total value by paying users\")\ntrain.groupby(['date'])['totals.transactionRevenue'].sum().plot(color='purple')\nax1.set_ylabel('Transaction count', color='b')\nplt.legend(['Transaction count'])\nax2 = ax1.twinx()\ntrain_.groupby(['date'])['totals.transactionRevenue'].count().plot(color='gold')\nax2.set_ylabel('Natural logarithm of sum of transactions', color='g')\nplt.legend(['Transaction sum'], loc=(0.875, 0.9))\nplt.grid(False)","e8836642":"print(f'First date in train set is {train[\"date\"].min()}. Last date in train set is {train[\"date\"].max()}.')\nprint(f'First date in test set is {test[\"date\"].min()}. Last date in test set is {test[\"date\"].max()}.')","d96076c9":"fig, ax = plt.subplots(2, 2, figsize = (16, 12))\nprint('Mean revenue per transaction')\nsns.pointplot(x=\"device.browser\", y=\"totals.transactionRevenue\", hue=\"device.isMobile\", data=train_, ax = ax[0, 0])\nsns.pointplot(x=\"device.deviceCategory\", y=\"totals.transactionRevenue\", hue=\"device.isMobile\", data=train_, ax = ax[0, 1])\nsns.pointplot(x=\"device.operatingSystem\", y=\"totals.transactionRevenue\", hue=\"device.isMobile\", data=train_, ax = ax[1, 0])\nsns.pointplot(x=\"device.isMobile\", y=\"totals.transactionRevenue\", data=train_, ax = ax[1, 1])\nplt.xticks(rotation=30)","663e265a":"def show_count_sum(df, col):\n    return df.groupby(col).agg({'totals.transactionRevenue': ['count', 'mean']}).sort_values(('totals.transactionRevenue', 'count'), ascending=False).head()\nshow_count_sum(train_, 'geoNetwork.subContinent')","ef19fcc2":"show_count_sum(train_.loc[train_['geoNetwork.subContinent'] == 'Northern America'], 'geoNetwork.metro')","e7ac3513":"show_count_sum(train_.loc[train_['geoNetwork.subContinent'] == 'Northern America'], 'geoNetwork.networkDomain')","a8dab25d":"show_count_sum(train_.loc[train_['geoNetwork.subContinent'] == 'Northern America'], 'geoNetwork.region')","23e4aebb":"show_count_sum(train_, 'trafficSource.medium')","4c482eab":"del grouped, counts, train_","7c740208":"# time based\ntrain['month'] = train['date'].dt.month\ntrain['day'] = train['date'].dt.day\ntrain['weekday'] = train['date'].dt.weekday\n\ntrain['month_unique_user_count'] = train.groupby('month')['fullVisitorId'].transform('nunique')\ntrain['day_unique_user_count'] = train.groupby('day')['fullVisitorId'].transform('nunique')\n\ntest['month'] = test['date'].dt.month\ntest['day'] = test['date'].dt.day\ntest['weekday'] = test['date'].dt.weekday\n\ntest['month_unique_user_count'] = test.groupby('month')['fullVisitorId'].transform('nunique')\ntest['day_unique_user_count'] = test.groupby('day')['fullVisitorId'].transform('nunique')","d6ce543d":"# device based\n\ntrain['browser_category'] = train['device.browser'] + train['device.deviceCategory']\ntrain['browser_operatingSystem'] = train['device.browser'] + train['device.operatingSystem']\n\ntest['browser_category'] = test['device.browser'] + test['device.deviceCategory']\ntest['browser_operatingSystem'] = test['device.browser'] + test['device.operatingSystem']","b5947d6b":"train['visitNumber'] = np.log1p(train['visitNumber'])\ntest['visitNumber'] = np.log1p(test['visitNumber'])\n\ntrain['totals.hits'] = np.log1p(train['totals.hits'])\ntest['totals.hits'] = np.log1p(test['totals.hits'].astype(int))\n\ntrain['totals.pageviews'] = np.log1p(train['totals.pageviews'].fillna(0))\ntest['totals.pageviews'] = np.log1p(test['totals.pageviews'].astype(float).fillna(0))","03882320":"num_cols = ['visitNumber', 'totals.hits', 'totals.pageviews', 'month_unique_user_count', 'day_unique_user_count']\nno_use = [\"visitNumber\", \"date\", \"fullVisitorId\", \"sessionId\", \"visitId\", \"visitStartTime\", 'totals.transactionRevenue', 'trafficSource.referralPath']\ncat_cols = [col for col in train.columns if col not in num_cols and col not in no_use]","3b33324e":"for col in cat_cols:\n    print(col)\n    lbl = LabelEncoder()\n    lbl.fit(list(train[col].values.astype('str')) + list(test[col].values.astype('str')))\n    train[col] = lbl.transform(list(train[col].values.astype('str')))\n    test[col] = lbl.transform(list(test[col].values.astype('str')))","5f47b5db":"train = train.sort_values('date')\nX = train.drop(no_use, axis=1)\ny = train['totals.transactionRevenue']\nX_test = test.drop([col for col in no_use if col in test.columns], axis=1)\n# I use TimeSeriesSplit as we have time series\ntscv = TimeSeriesSplit(n_splits=5)","13946123":"params = {\"objective\" : \"regression\", \"metric\" : \"rmse\",\n              \"num_leaves\" : 32, \"learning_rate\" : 0.05, \"bagging_fraction\" : 0.8, \"feature_fraction\" : 0.8, \"bagging_frequency\" : 5}\n\n# Cleaning and defining parameters for LGBM\nmodel = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)","ecf34bc7":"prediction = np.zeros(test.shape[0])\n\nfor fold_n, (train_index, test_index) in enumerate(tscv.split(X)):\n    print('Fold:', fold_n)\n    X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n    \n\n    model.fit(X_train, y_train, \n            eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric='rmse',\n            verbose=100, early_stopping_rounds=100)\n    \n    y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n    prediction += y_pred\n\nprediction \/= 5","d62dd056":"lgb.plot_importance(model, max_num_features=30);","f573d3c2":"test[\"PredictedLogRevenue\"] = np.expm1(prediction)\nsub = test.groupby(\"fullVisitorId\").agg({\"PredictedLogRevenue\" : \"sum\"}).reset_index()\nsub[\"PredictedLogRevenue\"] = np.log1p(sub[\"PredictedLogRevenue\"])\nsub[\"PredictedLogRevenue\"] = sub[\"PredictedLogRevenue\"].apply(lambda x : 0.0 if x < 0 else x)\nsub[\"PredictedLogRevenue\"] = sub[\"PredictedLogRevenue\"].fillna(0.0)\nsub.to_csv(\"lgb.csv\", index=False)","8ea7fe44":"### Devices\n\nLet's see which devices bring most revenue!","44d3ba14":"## General information\nThis kernel is dedicated to EDA of Google Analytics Customer Revenue Prediction  competition as well as feature engineering. For now basic data is used and not data from BigQuery.\n\nIn this dataset we can see customers which went to Google Merchandise Store, info about them and their transactions. We need to predict the natural log of the sum of all transactions per user.","b227a448":"Most transactions are from California or New York.","71a06957":"We can see several thing from this overview:\n- less than 2% of all transactions bring revenue. This percentage is more or less reasonable;\n- some columns have full representation only in BigQuery, so it is necessary to use it;\n- traffic sourse in some cases is unknown, we'll need to find a way to fill these values;\n- there several groups of features: visitor activity, geodata and device info, source of traffic;","6aeb277f":"## Data exploration","09f6fed6":"## Feature analysis","1562751d":"Most of features related to traffic have a lot of missing values, so I'll skip them for now.","15135617":"### Feature processing","4d039791":"Some of columns aren't available in this dataset, let's drop them.","eada5c5c":"It seems that devices on Chrome OS and Macs bring most profit.\n\nThere is an interesting fact - there are desktop devices which are considered to be mobile devices. Is it an error?","aa1d41dc":"Most paying users made only 1 transaction, but there are several users, who had a lot of transactions. Regular customers?","333f884a":"### Traffic source","b4883c6e":"We can see that revenue comes mostly from desktops. Social, Affiliates and others aren't as profitable as other channels.","a19370b0":"It isn't surprising that trends of sum and count of paid transactions are almost similar. It is worth noticing that there are several periods when the number of non-paying users was significantly higher that the number of paying users, but it didin't influence total sums.\n\nTrain and test period don't intersect.","cc24cf86":"Obviously most transactions come from America","4b86b94c":"### Revenue","6583f5a4":"The plots are quite similar.","60d24781":"## Feature engineering","afe76135":"At first let's create some features","923d9448":"Here we can see the distribution of transaction revenue. But it would be more useful to see a distribution of total revenue per user.","2345f01d":"### geoNetwork"}}