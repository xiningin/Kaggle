{"cell_type":{"4aba36da":"code","b338caeb":"code","83539b89":"code","45c09eb1":"code","f4399a5d":"code","7dab76e6":"code","da520641":"code","37e18749":"code","84c6bb2e":"code","3d10656f":"code","e1ea4f8b":"code","1532892b":"code","75acc35b":"code","392c0342":"code","529aa21f":"code","717fdcc8":"code","2e99868a":"code","c28e2154":"code","1eebef79":"code","9504d6f2":"code","12c1fd86":"code","e6285d7c":"code","98903760":"code","f419c6d7":"code","102e4b51":"code","73d5d4c8":"code","33bfea40":"code","4ac2d7d5":"code","41b9323f":"code","b5c5df05":"code","f107fff4":"code","ad783343":"code","db6a173b":"code","01a732de":"code","b0ecf827":"code","7582997f":"code","f81b9cbe":"code","e494ac9a":"code","b2b4f270":"code","1c309018":"code","062c0f65":"code","f64c2a22":"code","67185419":"code","c83d68d6":"code","df8cfde4":"code","6d23dc67":"code","8780975b":"code","d8ef9a16":"code","3764902b":"code","bae453d5":"code","4c97904f":"code","a88b9142":"code","78f412f3":"code","d8ed204a":"code","af725a71":"code","fe86331e":"code","93b81664":"code","87448179":"code","97aa9946":"code","5c947bc2":"code","45a5900c":"code","23cb3192":"code","9baaeb97":"code","af9e01e3":"code","6cc737f3":"code","4e766e97":"code","97dc396a":"code","d87a0bb7":"code","1398e871":"code","9ce32271":"code","165ad04d":"code","8e759494":"code","dfc160de":"code","421c97b5":"code","2060c417":"code","5dcfd271":"code","1836157c":"code","1d819e2d":"code","3fb52000":"markdown","84ce71f2":"markdown","d4346a64":"markdown","6e559b7a":"markdown","fd922019":"markdown","eac613a3":"markdown","33d7ccf3":"markdown","950627fc":"markdown","4e996f04":"markdown","51d8f27b":"markdown","9ca90356":"markdown","50d52405":"markdown"},"source":{"4aba36da":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Here we are just importing which are important for starting with.. and will add-on once I need more when reaching towards Modeling and Prdiction.","b338caeb":"# Get File Path\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","83539b89":"train_data = pd.read_csv('\/kaggle\/input\/cat-in-the-dat\/train.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/cat-in-the-dat\/sample_submission.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/cat-in-the-dat\/test.csv')","45c09eb1":"train_data.head()","f4399a5d":"print(train_data.shape, test_data.shape, sample_submission.shape)","7dab76e6":"test_data['id'].head()","da520641":"test_data['id'].tail()","37e18749":"sample_submission['id'].head()","84c6bb2e":"train_data.info()","3d10656f":"train_data.describe()","e1ea4f8b":"train_data.describe(include='all')","1532892b":"train_data.describe(include=[np.object])","75acc35b":"# write a function to get the distinct value in each categorical value\ndef get_Unique_Values(list_cat_var) :\n    cat_dict = dict()\n    for i in list_cat_var:\n        cat_dict[i] = list(train_data[i].unique())\n    return cat_dict","392c0342":"print(get_Unique_Values(['bin_3', 'bin_4'])) \nprint('-'*80)\nprint(get_Unique_Values(['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'])) \nprint('-'*80)\n#print(get_Unique_Values(['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9'])) \nprint('-'*80)\nprint(get_Unique_Values(['ord_1', 'ord_2'])) \nprint('-'*80)\nprint(get_Unique_Values(['ord_3', 'ord_4'])) \nprint('-'*80)\nprint(get_Unique_Values(['ord_5'])) \nprint('-'*80)\nprint(get_Unique_Values(['day', 'month'])) ","529aa21f":"# Binary encoding\ntrain_data['bin_3'] = [0 if x == 'F' else 1 for x in train_data['bin_3']]\ntrain_data['bin_4'] = [0 if x == 'N' else 1 for x in train_data['bin_4']]\n\ntest_data['bin_3'] = [0 if x == 'F' else 1 for x in test_data['bin_3']]\ntest_data['bin_4'] = [0 if x == 'N' else 1 for x in test_data['bin_4']]\n\nprint(get_Unique_Values(['bin_3', 'bin_4'])) ","717fdcc8":"#Hard coded Label encoding\nprint(get_Unique_Values(['ord_1', 'ord_2'])) \n\ntrain_data['ord_1'] = [0 if x == 'Novice' else 1 if x == 'Contributor' else 2 if x == 'Expert' else 3 if x == 'Master' else 4 for x in train_data['ord_1']]\ntrain_data['ord_2'] = [0 if x == 'Freezing' else 1 if x == 'Cold' else 2 if x == 'Warm' else 3 if x == 'Hot' else 4 if x == 'Boiling Hot' else 5 for x in train_data['ord_2']]\n\ntest_data['ord_1'] = [0 if x == 'Novice' else 1 if x == 'Contributor' else 2 if x == 'Expert' else 3 if x == 'Master' else 4 for x in test_data['ord_1']]\ntest_data['ord_2'] = [0 if x == 'Freezing' else 1 if x == 'Cold' else 2 if x == 'Warm' else 3 if x == 'Hot' else 4 if x == 'Boiling Hot' else 5 for x in test_data['ord_2']]\n\nprint('------- After Hard coded Label Encoding -------')\nprint(get_Unique_Values(['ord_1', 'ord_2'])) ","2e99868a":"# for ord_5, as mentioned will separate the value into two new features, and then will drop ord_5.\ntrain_data[\"ord_5a\"]=train_data[\"ord_5\"].str[0]\ntrain_data[\"ord_5b\"]=train_data[\"ord_5\"].str[1]\ntrain_data.drop(['ord_5'], axis=1, inplace = True)\n\ntest_data[\"ord_5a\"]=test_data[\"ord_5\"].str[0]\ntest_data[\"ord_5b\"]=test_data[\"ord_5\"].str[1]\ntest_data.drop(['ord_5'], axis=1, inplace = True)\n\nprint(get_Unique_Values(['ord_5a', 'ord_5b'])) ","c28e2154":"import string\nprint('ASCII for a : ', string.ascii_letters.index('a'))\nprint('ASCII for A : ', string.ascii_letters.index('A'))\nprint('ASCII for E : ', string.ascii_letters.index('E'))\nprint('ASCII for j : ', string.ascii_letters.index('j'))","1eebef79":"# ascii encoding for ord_3, ord_4, ord_5a, ord_5b\ntrain_data['ord_3'] = train_data['ord_3'].apply(lambda x: string.ascii_letters.index(x))\ntrain_data['ord_4'] = train_data['ord_4'].apply(lambda x: string.ascii_letters.index(x))\ntrain_data['ord_5a'] = train_data['ord_5a'].apply(lambda x: string.ascii_letters.index(x))\ntrain_data['ord_5b'] = train_data['ord_5b'].apply(lambda x: string.ascii_letters.index(x))\n\ntest_data['ord_3'] = test_data['ord_3'].apply(lambda x: string.ascii_letters.index(x))\ntest_data['ord_4'] = test_data['ord_4'].apply(lambda x: string.ascii_letters.index(x))\ntest_data['ord_5a'] = test_data['ord_5a'].apply(lambda x: string.ascii_letters.index(x))\ntest_data['ord_5b'] = test_data['ord_5b'].apply(lambda x: string.ascii_letters.index(x))\n\nprint(get_Unique_Values(['ord_3', 'ord_4'])) \nprint('-'*80)\nprint(get_Unique_Values(['ord_5a', 'ord_5b'])) ","9504d6f2":"train_data.head()","12c1fd86":"# Lets transform the Categorical Features ['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'] into Number using get_dummies function (One Hot Encoding)\nprint(train_data.shape)\nprint('-'*20)\ntrain_data = pd.get_dummies(train_data, columns=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'], drop_first=True)\n\ntest_data = pd.get_dummies(test_data, columns=['nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4'], drop_first=True)\n","e6285d7c":"print(train_data.shape)\n#train_data.describe(include = 'object')\ntrain_data.columns","98903760":"#Leave one out encoding high cardinal variables\nhigh_cardinal_vars = ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\nhigh_cardinal_vars","f419c6d7":"train_data['nom_5'].describe()","102e4b51":"#from category_encoders import TargetEncoder, HashingEncoder, LeaveOneOutEncoder\n#trgt_encoder = TargetEncoder(cols=high_cardinal_vars, smoothing=0, return_df=True)\n#hashing_encoder = HashingEncoder(cols = high_cardinal_vars)\n# loo_encoder = LeaveOneOutEncoder(cols=high_cardinal_vars)\n#train_data = loo_encoder.fit_transform(train_data.drop(['target'], axis = 1), train_data['target'])\n","73d5d4c8":"# taken reference from https:\/\/maxhalford.github.io\/blog\/target-encoding-done-the-right-way\/ \ndef calc_smooth_mean(df, by, on, m):# df => Data Frame; by => column on which encoding is required; on => target field; m => weight\n    # Compute the global mean\n    mean = df[on].mean()\n\n    # Compute the number of values and the mean of each group\n    agg = df.groupby(by)[on].agg(['count', 'mean'])\n    counts = agg['count']\n    means = agg['mean']\n\n    # Compute the \"smoothed\" means\n    smooth = (counts * means + m * mean) \/ (counts + m)\n\n    # Replace each value by the according smoothed mean\n    return df[by].map(smooth)","33bfea40":"train_data['nom_5'] = calc_smooth_mean(train_data, by='nom_5', on='target', m=8)\ntrain_data['nom_6'] = calc_smooth_mean(train_data, by='nom_6', on='target', m=8)\ntrain_data['nom_7'] = calc_smooth_mean(train_data, by='nom_7', on='target', m=8)\ntrain_data['nom_8'] = calc_smooth_mean(train_data, by='nom_8', on='target', m=8)\ntrain_data['nom_9'] = calc_smooth_mean(train_data, by='nom_9', on='target', m=8)\n \ntest_data['nom_5'] = calc_smooth_mean(train_data, by='nom_5', on='target', m=8)\ntest_data['nom_6'] = calc_smooth_mean(train_data, by='nom_6', on='target', m=8)\ntest_data['nom_7'] = calc_smooth_mean(train_data, by='nom_7', on='target', m=8)\ntest_data['nom_8'] = calc_smooth_mean(train_data, by='nom_8', on='target', m=8)\ntest_data['nom_9'] = calc_smooth_mean(train_data, by='nom_9', on='target', m=8)    ","4ac2d7d5":"train_data.columns","41b9323f":"print(get_Unique_Values(['nom_5'])) ","b5c5df05":"#print( train_data['nom_5'].value_counts().keys().tolist() )\n#print( train_data['nom_5'].value_counts().tolist() )\n\n#print( train_data['nom_5'].value_counts().to_frame() )\n# nm1 = train_data['nom_5'].value_counts().to_frame() \n# # print(nm1.dtypes)\n# print(nm1['nom_5'].head())\n# print('TOTAL ', nm1.count())\n# # print('MORE THAN 2000 : ', nm1.loc[nm1['nom_5'] > 2000].count())\n# # print('MORE THAN 1000 : ', nm1[(nm1['nom_5'] > 1000) ].count()  )\n# # print('BETWEEN 1000 and 2000 : ', nm1[(nm1['nom_5'] > 1000) & (nm1['nom_5'] < 2000)].count())\n# # print('LESS THAN 1000 : ', nm1[(nm1['nom_5'] < 1000) ].count())\n# print(nm1[(nm1['nom_5'] < 1000) ].index)","f107fff4":"# df = pd.DataFrame({'FIELD_1': ['f710fca39', '1fd0233cd', '005dd4ce3', '5331f98fb', '005dd4ce3', 'f710fca39', 'eb0004a0b'], \n#                          'B': [400        , 500        , 600        , 700        , 800        , 900        , 111]})\n# new_df = pd.DataFrame({'CNT': [225, 150, 80, 230],'ID': ['f710fca39', '1fd0233cd', '5331f98fb', '005dd4ce3']})\n# new_df.set_index('ID', inplace=  True)\n# print(df)\n# print(new_df)\n# print('-'*50)\n# # Ask is : Update the column \"FIELD_1\" as 2 if the CNT is more than 200 in new_df; 1 when the CNT is >100 and <200; 0 when CNT <100.\n# print(df.loc[df['FIELD_1'].isin(new_df.index & new_df['CNT'] > 200 ),'FIELD_1']  )\n\n# print(df)","ad783343":"# train_nom5 = train_data['nom_5'].value_counts().to_frame() \n# test_nom5 = test_data['nom_5'].value_counts().to_frame() ","db6a173b":"# #train_data['nom_5'] = [0 if x in train_nom5[(train_nom5['nom_5'] > 2000)].index else 1 if x in train_nom5[(train_nom5['nom_5'] > 1000) & (train_nom5['nom_5'] < 2000)].index else 2 for x in train_data['nom_5']]\n# train_data['nom_5'] = \n# train_data['nom_5'].head(10)","01a732de":"# Lets get the % of each null values.\n#total = train_data.isnull().sum().sort_values(ascending=False)\n#percent_1 = train_data.isnull().sum()\/train_data.isnull().count()*100\n#percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n#missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n#missing_data.head(5)\n# Cool.. No NaN Values in train_data","b0ecf827":"# Lets get the % of each null values.\n#total = test_data.isnull().sum().sort_values(ascending=False)\n#percent_1 = test_data.isnull().sum()\/test_data.isnull().count()*100\n#percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n#missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n#missing_data.head(5)\n# Cool.. No NaN Values in test_data","7582997f":"#Using Pearson Correlation\n\nplt.figure(figsize=(20,10))\ncor = train_data.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","f81b9cbe":"#Correlation with output variable\ncor_target = abs(cor[\"target\"])\n\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.5]\nrelevant_features\n\n# Seems none of the numeric feature have much correlation with our target variable.\n# Correlation coefficients whose magnitude are between 0.5 and 0.7 indicate variables which can be considered moderately correlated. Correlation coefficients whose magnitude are between 0.3 and 0.5 indicate variables which have a low correlation.","e494ac9a":"## Taken reference from https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables\n# #Get list of categorical variables\n# s = (train_data.dtypes == 'object')\n# train_data_cat_var = list(s[s].index)\n\n# s = (test_data.dtypes == 'object')\n# test_data_cat_var = list(s[s].index)\n\n# print(\"Categorical variables from train_data:\", train_data_cat_var)\n# print(\"-\"*30)\n# print(\"Categorical variables from test_data:\", test_data_cat_var)","b2b4f270":"# list(train_data['bin_3'].unique() )\n#train_data['bin_3'].value_counts() \n#train_data['bin_3'].unique().sum()\n#train_data.groupby('bin_3').size()\n#len(train_data['bin_3'].unique())","1c309018":"# write a function to get the count of distinct value in each categorical value\n# def get_Unique_Count(list_cat_var) :\n#     cat_dict = dict()\n#     for i in list_cat_var:\n#         cat_dict[i] = len(train_data[i].unique())\n#     return cat_dict","062c0f65":"# print(get_Unique_Count(list(train_data_cat_var))) \n# print(get_Unique_Count(list(test_data_cat_var))) ","f64c2a22":"# Dropping off un-used features.\n# train_data.drop(['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_3', 'ord_4', 'ord_5'], axis = 1, inplace = True)\n# test_data.drop(['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_3', 'ord_4', 'ord_5'], axis = 1, inplace = True)","67185419":"# removing un-used features from our categorical features.\n# print(len(train_data_cat_var))\n# train_data_cat_var = [ele for ele in train_data_cat_var if ele not in  ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_3', 'ord_4', 'ord_5']]\n# print(len(train_data_cat_var))","c83d68d6":"# print(len(test_data_cat_var))\n# test_data_cat_var = [ele for ele in test_data_cat_var if ele not in  ['nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_3', 'ord_4', 'ord_5']]\n# print(len(test_data_cat_var))","df8cfde4":"# Lets transform the Categorical Features into Number using get_dummies function (One Hot Encoding)\n# final_train_data = pd.get_dummies(train_data, columns=train_data_cat_var, drop_first=True)\n# print(final_train_data.shape, train_data.shape)\n# final_train_data.head()","6d23dc67":"# final_test_data = pd.get_dummies(test_data, columns=test_data_cat_var, drop_first=True)\n# print(final_test_data.shape, test_data.shape)\n# final_test_data.head()","8780975b":"# Defining Feature and Target.\nfeatures = train_data.drop(['target'], axis = 1).columns\ntarget = train_data[\"target\"]\nprint(\"Features\", features)\nprint('--'*10)\nprint (\"Target\", target.head())","d8ef9a16":"test_data.columns","3764902b":"# split the train_data into 2 DF's aka X_train, X_test, y_train, y_test.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_data[features], target, test_size=0.2)\n\nprint (X_train.shape, y_train.shape)\nprint (X_test.shape, y_test.shape)","bae453d5":"# test_data \nX_test_df  = test_data[features].copy()\nX_test_df.head()","4c97904f":"# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","a88b9142":"# ROC and AUR Curve related importing the libraries\nfrom sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, accuracy_score, classification_report","78f412f3":"# # Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nY_pred_lr = logreg.predict(X_test)\n#print(Y_pred_lr)\n\n","d8ed204a":"logreg_score = round(logreg.score(X_train, y_train) * 100, 2)\nprint(\"Score (LogisticRegression)\", logreg_score)\n","af725a71":"logreg_accuracy_score = round(accuracy_score(y_test, Y_pred_lr) * 100, 2)\nprint(\"Accuracy Score (LogisticRegression)\", logreg_accuracy_score)","fe86331e":"logreg_confusion_matrix = confusion_matrix(y_test, Y_pred_lr)\nlogreg_confusion_matrix","93b81664":"logreg_roc_auc = roc_auc_score(y_test, Y_pred_lr)\nlogreg_roc_auc","87448179":"# # Getting False Positive Rate (fpr); True Positive Rate (tpr) and threshold.\nfpr_logreg, tpr_logreg, threshold_logreg = roc_curve(y_test,logreg.predict_proba(X_test)[:,1])\nprint('False Positive Rate : ', fpr_logreg)\nprint('True Positive Rate : ', tpr_logreg)\nprint('Threshold : ', threshold_logreg)","97aa9946":"# Plotting the ROC Curve\nplt.figure()\nplt.plot(fpr_logreg, tpr_logreg, label = 'Logistic Regression Model (aread = %0.2f)' %logreg_roc_auc)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc = 'lower right')\nplt.show()","5c947bc2":"# Support Vector Machines\n\n# svc = SVC(gamma='auto')\n# svc.fit(X_train, y_train)\n# Y_pred_svc = svc.predict(X_test)\n","45a5900c":"#svc_roc_auc = roc_auc_score(y_test, Y_pred_svc)\n#print('ROC AUR Score for SVC Model : ', svc_roc_auc)\n\n# Getting False Positive Rate (fpr); True Positive Rate (tpr) and threshold.\n#fpr_svc, tpr_svc, threshold_svc = roc_curve(y_test,svc.predict_proba(X_test)[:,1])\n#print('False Positive Rate : ', fpr_svc)\n#print('True Positive Rate : ', tpr_svc)\n#print('Threshold : ', threshold_svc)","23cb3192":"# # Plotting the ROC Curve for Logistic Regression and SVC Model\n\n# plt.figure()\n# plt.plot(fpr_logreg, tpr_logreg, label = 'Logistic Regression Model (aread = %0.2f)' %logreg_roc_auc)\n# plt.plot(fpr_svc, tpr_svc, label = 'SVC Model (aread = %0.2f)' %svc_roc_auc)\n# plt.plot([0,1], [0,1], 'r--')\n# plt.xlabel('False Positive Rate')\n# plt.ylabel('True Positive Rate')\n# plt.legend(loc = 'lower right')\n# plt.show()\n","9baaeb97":"# KNN\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\nY_pred_knn = knn.predict(X_test)","af9e01e3":"knn_roc_auc = roc_auc_score(y_test, Y_pred_knn)\nprint('ROC AUR Score for KNN Model : ', knn_roc_auc)\n\n# Getting False Positive Rate (fpr); True Positive Rate (tpr) and threshold.\nfpr_knn, tpr_knn, threshold_knn = roc_curve(y_test,knn.predict_proba(X_test)[:,1])\nprint('False Positive Rate : ', fpr_knn)\nprint('True Positive Rate : ', tpr_knn)\nprint('Threshold : ', threshold_knn)","6cc737f3":"# Plotting the ROC Curve for Logistic Regression ; SVC ; KNN Model\nplt.figure()\nplt.plot(fpr_logreg, tpr_logreg, label = 'Logistic Regression Model (aread = %0.2f)' %logreg_roc_auc)\n#plt.plot(fpr_svc, tpr_svc, label = 'SVC Model (aread = %0.2f)' %svc_roc_auc)\nplt.plot(fpr_knn, tpr_knn, label = 'KNN Model (aread = %0.2f)' %knn_roc_auc)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc = 'lower right')\nplt.show()","4e766e97":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\nY_pred_gnb = gaussian.predict(X_test)","97dc396a":"gnb_roc_auc = roc_auc_score(y_test, Y_pred_gnb)\nprint('ROC AUR Score for Gaussian Naive Bayes Model : ', gnb_roc_auc)\n\n# Getting False Positive Rate (fpr); True Positive Rate (tpr) and threshold.\nfpr_gnb, tpr_gnb, threshold_gnb = roc_curve(y_test,gaussian.predict_proba(X_test)[:,1])\nprint('False Positive Rate : ', fpr_gnb)\nprint('True Positive Rate : ', tpr_gnb)\nprint('Threshold : ', threshold_gnb)","d87a0bb7":"# Plotting the ROC Curve for Logistic Regression ; SVC ; KNN; Gaussian Naive Bayes Model\nplt.figure()\nplt.plot(fpr_logreg, tpr_logreg, label = 'Logistic Regression Model (aread = %0.2f)' %logreg_roc_auc)\n#plt.plot(fpr_svc, tpr_svc, label = 'SVC Model (aread = %0.2f)' %svc_roc_auc)\nplt.plot(fpr_knn, tpr_knn, label = 'KNN Model (aread = %0.2f)' %knn_roc_auc)\nplt.plot(fpr_gnb, tpr_gnb, label = 'Gaussian Naive Bayes Model (aread = %0.2f)' %gnb_roc_auc)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc = 'lower right')\nplt.show()","1398e871":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=10)\nrandom_forest.fit(X_train, y_train)\nY_pred_rf = random_forest.predict(X_test)","9ce32271":"rf_roc_auc = roc_auc_score(y_test, Y_pred_rf)\nprint('ROC AUR Score for Gaussian Naive Bayes Model : ', rf_roc_auc)\n\n# Getting False Positive Rate (fpr); True Positive Rate (tpr) and threshold.\nfpr_rf, tpr_rf, threshold_rf = roc_curve(y_test,random_forest.predict_proba(X_test)[:,1])\nprint('False Positive Rate : ', fpr_rf)\nprint('True Positive Rate : ', tpr_rf)\nprint('Threshold : ', threshold_rf)","165ad04d":"# Plotting the ROC Curve for Logistic Regression ; SVC ; KNN; Gaussian Naive Bayes Model\nplt.figure(figsize = (10, 10))\nplt.plot(fpr_logreg, tpr_logreg, label = 'Log Reg Model (aread = %0.2f)' %logreg_roc_auc)\n#plt.plot(fpr_svc, tpr_svc, label = 'SVC Model (aread = %0.2f)' %svc_roc_auc)\nplt.plot(fpr_knn, tpr_knn, label = 'KNN Model (aread = %0.2f)' %knn_roc_auc)\nplt.plot(fpr_gnb, tpr_gnb, label = 'G N Bayes Model (aread = %0.2f)' %gnb_roc_auc)\nplt.plot(fpr_rf, tpr_rf, label = 'R F Model (aread = %0.2f)' %rf_roc_auc)\nplt.plot([0,1], [0,1], 'r--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend(loc = 'lower right')\nplt.show()","8e759494":"modelling_score = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'ROC AUR Score': [0, knn_roc_auc, logreg_roc_auc, \n              rf_roc_auc, gnb_roc_auc, 0, \n              0, 0, 0]})","dfc160de":"modelling_score.sort_values(by='ROC AUR Score', ascending=False)","421c97b5":"# Predicting on actual test_data\nY_pred_test_df = random_forest.predict(X_test_df)\nY_pred_test_df ","2060c417":"X_test_df.head()","5dcfd271":"submission = pd.DataFrame( { 'id': X_test_df.id , 'target': Y_pred_test_df } )","1836157c":"print(\"Submission File Shape \",submission.shape)\nsubmission.head()","1d819e2d":"submission.to_csv( '\/kaggle\/working\/submission1.csv' , index = False )","3fb52000":"# Encoding techniques\n* bin_3, bin_4  :- Convert T\/F & Y\/N to 1\/0\n* nom_0 - nom_4 :- Encode using One hot encoding\n* nom_5 - nom_9 :- Target encode them as they are high cardinal variables\n* ord_1, ord_2  :- Convert into numerical order using hard coded values as Label encoder might not be able to understand the order\n* ord_3, ord_4  :- Encode using ascii as they are alphabetical values\n* ord_5         :- Separate two alphabets and then do encoding using ascii.\n\nTaken reference from https:\/\/www.kaggle.com\/ruchibahl18\/categorical-data-encoding-techniques#Encoding-techniques ","84ce71f2":"Taken Reference from Quora:\n\nOne would be to cluster them based on the response; you can sort them by response, then split them however you like; perhaps let a fairly shallow decision tree handle it. Now you have far fewer categories.\n\nAnother is to target encode them. Replace each category in a variable with the mean response given that category. Now you have 1 continuous feature instead of a bunch of categories.\n\nAnother is to group them by frequency. The most frequent categories may dominate, and the least frequent may be numerous but each have few samples. You can e.g. leave the top 5 categories alone and group the rest into a new category. Now you have 6 categorical variables.","d4346a64":"# Submission","6e559b7a":"# Introduction\nThis Playground competition will give you the opportunity to try different encoding schemes for different algorithms to compare how they perform.\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.","fd922019":"# Handling Categorical Features","eac613a3":"# Identifying best Model from above","33d7ccf3":"# Correlation Heatmap","950627fc":"This is a fork of my original code \"Categorical Feature Encoding Challenge\". Here i am trying to do more better by re-visting the data.","4e996f04":"From above we see that there are some categorical variables which has more than 10 unique value such as nom_5; nom_6; nom_7; nom_8; nom_9; ord_3; ord_4; ord_5. So we will not be using these to transform.\nWill just transoform remaining as we have limited counts, also it is recommended to transform any categorical variable if the max unique is less than 15, but here we will stick max to 10.","51d8f27b":"# Modeling","9ca90356":"From my previous challenge... i experienced that the test and train categorical variables may have different set of values.. so better to check at first. If found we can merge the data-set and then transform them.","50d52405":"Observation from above : \nThere are multiple categorical variables which are as follows\n* bin_3, bin_4  :- binary cols (T & F and Y & N respectively)\n* nom_0 - nom_4 :- nominal columns ( with no order)\n* nom_5 - nom_9 :- nominal columns with high cardinality\n* ord_1 - ord_5 :- Ordered columns\n\nWe have to use different ways to treat these columns and convert them into numerical data"}}