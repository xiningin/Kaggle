{"cell_type":{"59b7f167":"code","f9946224":"code","d64eb839":"code","4150762f":"code","0c0e2478":"code","e8ab8190":"code","f49cd94a":"code","db16fc80":"code","d840b432":"markdown","5bf0d1ae":"markdown","757edb93":"markdown","a06b2bb3":"markdown","ade46d88":"markdown"},"source":{"59b7f167":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f9946224":"import pandas as pd\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\ndata = pd.read_csv('\/kaggle\/input\/heart-failure-prediction\/heart.csv')\nprint('\\n\\n Descriptive Statistics \\n\\n',data.describe().T,'\\n\\n')\nprint(data.info())\n\ndef IV_calc(data, var):\n    if data[var].dtypes == 'object':\n        dataf = data.groupby([var])['HeartDisease'].agg(['count','sum'])\n        dataf.columns = ['Total','bad']\n        dataf['good'] = dataf['Total'] - dataf['bad']\n        dataf['bad_per'] = dataf['bad']\/dataf['bad'].sum()\n        dataf['good_per'] = dataf['good']\/dataf['good'].sum()\n        dataf['I_V'] = (dataf['good_per'] - dataf['bad_per'])*np.log(dataf['good_per']\/dataf['bad_per'])\n        return dataf\n    else:\n        data['bin_var'] = pd.qcut(data[var].rank(method='first'),10)\n        dataf = data.groupby(['bin_var'])['HeartDisease'].agg(['count','sum'])\n        dataf.columns = ['Total','bad']\n        dataf['good'] = dataf['Total'] - dataf['bad']\n        dataf['bad_per'] = dataf['bad']\/dataf['bad'].sum()\n        dataf['good_per'] = dataf['good']\/dataf['good'].sum()\n        dataf['I_V'] = (dataf['good_per'] - dataf['bad_per'])*np.log(dataf['good_per']\/dataf['bad_per'])\n        return dataf","d64eb839":"discrete_columns = []\ncontinuous_columns = []\n\nfor col in data.columns:\n    if col != 'HeartDisease':     \n        if data[col].dtype == 'object':\n            discrete_columns.append(col)\n        else:\n            continuous_columns.append(col)\n\ntotal_columns = discrete_columns + continuous_columns\n\nIv_list = []\n\nfor col in total_columns:\n    assigned_data = IV_calc(data=data, var=col)\n    iv_val = round(assigned_data['I_V'].sum(),3)\n    dt_type = data[col].dtypes\n    Iv_list.append((iv_val, col, dt_type))\n    \nIv_list = sorted(Iv_list, reverse=True)\n\nfor iv in Iv_list:\n    print(iv[1],':',iv[0])","4150762f":"dummy_sex = pd.get_dummies(data['Sex'], prefix='sex')\ndummy_CPT = pd.get_dummies(data['ChestPainType'], prefix='chest_p_t')\ndummy_RECG = pd.get_dummies(data['RestingECG'], prefix='rest_ecg')\ndummy_angina = pd.get_dummies(data['ExerciseAngina'], prefix='angina')\ndummy_STslope = pd.get_dummies(data['ST_Slope'], prefix = 'st_slope')\n\ndata_continuous = data[continuous_columns]\ndata_new = pd.concat([dummy_sex,dummy_CPT,dummy_RECG,dummy_angina,dummy_STslope,data_continuous, data['HeartDisease']],axis=1)","0c0e2478":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data_new.drop(['HeartDisease'],axis=1),data_new['HeartDisease'], train_size = 0.7, random_state=42)\ny_train = pd.DataFrame(y_train)\ny_test = pd.DataFrame(y_test)\n\nremove_cols_extra_dummy = ['sex_F','chest_p_t_ASY','rest_ecg_LVH','angina_N','st_slope_Down']\nremove_cols_insig = []\nremove_cols = list(set(remove_cols_extra_dummy+remove_cols_insig))","e8ab8190":"import statsmodels.api as sm\n\nlogistic_model = sm.Logit(y_train, sm.add_constant(x_train.drop(remove_cols,axis=1))).fit()\nprint(logistic_model.summary())\n\ncnames = x_train.drop(remove_cols, axis=1).columns\nfor i in np.arange(0,len(cnames)):\n    xvars = list(cnames)\n    yvar = xvars.pop(i)\n    mod = sm.OLS(x_train.drop(remove_cols, axis=1)[yvar], sm.add_constant(x_train.drop(remove_cols, axis=1)[xvars]))\n    res = mod.fit()\n    vif = 1\/(1-res.rsquared)\n    print(yvar, round(vif,3))\n    \ny_pred = pd.DataFrame(logistic_model.predict(sm.add_constant(x_train.drop(remove_cols, axis=1))))\ny_pred.columns = ['probs']\nboth = pd.concat([y_train, y_pred], axis=1)\n\nzeros = both[['HeartDisease','probs']][both['HeartDisease']==0]\nones = both[['HeartDisease','probs']][both['HeartDisease']==1]","f49cd94a":"def df_crossjoin(df1,df2,**kwargs):\n    df1['_tmpkey'] = 1\n    df2['_tmpkey'] = 1\n    res = pd.merge(df1,df2,on='_tmpkey',**kwargs).drop('_tmpkey',axis=1)\n    res.index = pd.MultiIndex.from_product((df1.index, df2.index))\n    df1.drop('_tmpkey',axis=1,inplace=True)\n    df2.drop('_tmpkey', axis=1, inplace=True)\n    return res\n\njoined_data = df_crossjoin(ones, zeros)\njoined_data['concordant_pair'] = 0\njoined_data.loc[joined_data['probs_x']>joined_data['probs_y'],'concordant_pair'] = 1\njoined_data['discordant_pair'] = 0\njoined_data.loc[joined_data['probs_x']<joined_data['probs_y'],'discordant_pair'] = 1\njoined_data['tied_pair'] = 0\njoined_data.loc[joined_data['probs_x']==joined_data['probs_y'],'tied_pair'] = 1\n\np_conc = (sum(joined_data['concordant_pair'])*1)\/(joined_data.shape[0])\np_disc = (sum(joined_data['discordant_pair'])*1)\/(joined_data.shape[0])\nc_statistic = 0.5 + (p_conc-p_disc)\/2\n\nprint(c_statistic)","db16fc80":"import matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.metrics import auc, accuracy_score\n\nfpr, tpr, thresholds = metrics.roc_curve(both['HeartDisease'],both['probs'], pos_label = 1)\nroc_auc = auc(fpr, tpr)\nplt.figure()\nlw=2\nplt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC Curve (area = %0.2f)' % roc_auc)\nplt.plot([0,1],[0,1],color='navy',lw=lw, linestyle = '--')\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.05])\nplt.xlabel('False Positive Rate (1-Specificity)')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve - Heart Disease')\nplt.legend(loc='lower right')\nplt.show()\n\nfor i in list(np.arange(0,1,0.1)):\n    both['y_pred'] = 0\n    both.loc[both['probs']>i,'y_pred'] = 1\n    print('Threshold:',round(i,2),'Train Accuracy:',round(accuracy_score(both['HeartDisease'],both['y_pred']),4))\n\nboth['y_pred'] = 0\nboth.loc[both['probs']>0.5,'y_pred'] = 1\nprint('\\n\\nTrain Confusion Matrix', pd.crosstab(both['HeartDisease'],both['y_pred'],rownames = ['Actual'], colnames = ['Predicted']))\n\ny_pred_test = pd.DataFrame(logistic_model.predict(sm.add_constant(x_test.drop(remove_cols,axis=1))))\ny_pred_test.columns = ['probs']\nboth_test = pd.concat([y_test,y_pred_test],axis=1)\nboth_test['y_pred'] = 0\nboth_test.loc[both_test['probs']>0.5, 'y_pred'] = 1\n\ntest_crosstab = pd.crosstab(both_test['HeartDisease'],both_test['y_pred'],rownames = ['Actual'], colnames = ['Predicted'])\nprecision = round(test_crosstab[1][1] \/ (test_crosstab[1][1] + test_crosstab[1][0]),3)\nrecall =round(test_crosstab[1][1] \/ (test_crosstab[1][1] + test_crosstab[0][1]),3)\nf_score = round(2*(precision*recall)\/(precision+recall),3)\n\n\nprint('\\n\\nTest Confusion Matrix \\n', test_crosstab)\nprint('\\n\\nTest Accuracy:', round(accuracy_score(both_test['HeartDisease'],both_test['y_pred']),4))\nprint('Precision:',precision)\nprint('Recall:',recall)\nprint('F1 Score:', f_score)","d840b432":"Split data into 70% training and 30% testing with random state set to 42 to create reproducible results.\n\nOne dummy variable from each discrete column can be removed by default.\n\nInsignificant columns can be removed if they are either insignificant or multi-collinear.","5bf0d1ae":"Splitting columns into Discrete and Continuous for use in creating dummy variables.\n\nRanking columns by Information Value shows ST_Slope to have the most predictive power as opposed to RestingECG which has the weakest.","757edb93":"Summary of information relating to VIF (Variance Inflation Factors) and IV (Independent Variables)","a06b2bb3":"Pair is concordant if probability against the 1 class is higher than the 0. \nPair is discordant if probability against the 1 class is less than the 0.\nIf both probabilities are the same, they are a tied pair.\n\nC-Statistic indicates the performance of a model. A value >= 0.7 indicates a good model. ","ade46d88":"AUC is 0.93 indicating an excellent ability to diagnose patients with and without Heart Disease.\n\nA threshold probability between classes is optimal at 0.5 producing the following test results:\n\n* Test Accuracy: 0.88\n\n* Precision: 0.917\n\n* Recall: 0.878\n\n* F1 Score:0.897"}}