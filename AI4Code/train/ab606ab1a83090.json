{"cell_type":{"5d536485":"code","9a6feb9f":"code","f5bd3f90":"code","fa2d7edd":"code","c1ef5424":"code","da88438c":"code","81bf74e0":"code","9bf4dc40":"code","45c368d1":"code","2955509d":"code","c7f2c746":"code","bf659cef":"code","e9460e8b":"code","411da474":"code","558fd578":"code","33210cfa":"code","69942e5e":"code","a106ae3d":"code","24ab4e7c":"code","7c7be2b1":"code","e69eb51c":"code","998d85a1":"code","0c9ecd61":"code","b3ea0e5d":"code","95f06a03":"code","de41a451":"code","d2f43042":"code","34528d9a":"code","3adb4ab6":"code","251927e4":"code","c68111ac":"code","fce4282f":"code","2c3f6830":"code","7495c34d":"code","35267adc":"code","3f733ffa":"code","9d71389c":"code","58371dc9":"code","7b11521d":"code","ccc55630":"code","b57c910d":"code","48d757c2":"code","a1ca0327":"markdown","93c28c81":"markdown","fe0d5a2e":"markdown","9a5a6c3c":"markdown","170779d0":"markdown","cc793dde":"markdown","c7a6f4e2":"markdown","ed61f596":"markdown","de200a31":"markdown","e50c86bc":"markdown","b877179d":"markdown","fa991925":"markdown","e9722092":"markdown","0b56686e":"markdown","684519ec":"markdown","eb2b9763":"markdown","d5c2262f":"markdown","617a8410":"markdown","c7fec863":"markdown","6b3bbb4a":"markdown","d5bbadb9":"markdown","588520f2":"markdown","72a9403c":"markdown","1ef4a346":"markdown","3ba841ee":"markdown","48d5e225":"markdown","7326d1b4":"markdown","5c30dcfc":"markdown","7bc41e4d":"markdown","c2b85c67":"markdown","46627ab4":"markdown","d14b47ec":"markdown","708ecea7":"markdown","3f04fec3":"markdown","9e95346d":"markdown","c9de4049":"markdown","d01d8b7a":"markdown","17e2b83b":"markdown","ae3ab9f9":"markdown","90bd499f":"markdown","f4645acc":"markdown","ac42841b":"markdown","695fb383":"markdown"},"source":{"5d536485":"import torch\nimport numpy as np","9a6feb9f":"x = np.array([-1, -2, 1, 2]) # a numpy array\nprint(x)\nprint(type(x))","f5bd3f90":"x = torch.Tensor(x) # now x is a torch Tensor\nprint(x)","fa2d7edd":"y = torch.Tensor([0,1,2,-1]) # we can initialize some tensor directly from a python list\nprint(y)","c1ef5424":"z = x + y # normal tensor operation, the result is a Tensor\nprint(z)\nprint(type(z))\nprint(z.type())","da88438c":"x = torch.Tensor(5,4) \nprint(x)\nx = torch.zeros(5,4) # As default, a tensor in pytorch is a float tensor (torch.float32)\nprint(x)\nx = torch.ones([5,4], dtype=torch.int64) \nprint(x)\nx = torch.Tensor([[5,4]]) # Initialize it from python list \nprint(x)\nprint(x.shape)","81bf74e0":"x = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) \nprint(x)\nprint(x[:,2])\nprint(x[1:3,])","9bf4dc40":"x = torch.Tensor(3,4) \nprint(x.shape)\nx = x.view(2,6)\nprint(x)\nprint(x.shape)","45c368d1":"from torch.autograd import Variable\nimport torch.nn.functional as F","2955509d":"a = torch.Tensor([3])\nw = Variable(torch.Tensor([2]),requires_grad=True)\nb = Variable(torch.Tensor([3]),requires_grad=True)\nz = a * w\ny = z + b\nprint(y.data) # data of a variable y is a tensor\n","c7f2c746":"print(z.grad_fn)  \nprint(y.grad_fn)","bf659cef":"torch.__version__","e9460e8b":"a = torch.Tensor([3]) # Normal tensor - multi array\nprint(a)\nprint(a.grad_fn)\n\nw = torch.Tensor([2]) # A variable - A tensor which contains other information\nw.requires_grad = True\n\nb = torch.Tensor([3])\nb.requires_grad = True\nz = a * w\ny = z + b\nprint(y)\nprint(y.data) # data of a variable y is a normal tensor","411da474":"y.backward() # y = a * w + b","558fd578":"print(b.grad)  \nprint(w.grad) # w and b are variables","33210cfa":"print(z.grad)\nprint(a.grad) # z and a are normal tensors, no gradient information included","69942e5e":"def mean_squared_error(t, y):  # Mean squared error in numpy\n    # t: target label\n    # y: predicted value\n    n, m = y.shape\n    return np.sum((y - t)**2) \/ (n * m)\n    \ndef dMSE_dy(t, y): # Derivative of mean squared error w.r.t y in numpy\n    n, m = y.shape\n    return 2 * (y-t) \/ (n * m)\n\ndef mean_squared_error_PT(t, y): # Mean squared error - Pytorch version\n    # t: target label\n    # y: predicted value\n    n, m = y.shape\n    return torch.sum((y - t)**2) \/ (n * m)\n\ny = np.random.randn(4,2)\nt = np.random.randn(4,2)\n\n# forward test\nmy_mse = mean_squared_error(t, y)\n\n# warp y, t to be tensors\/variable\ntt = torch.Tensor(t)\nyt = Variable(torch.Tensor(y),requires_grad=True)\npt_mse = mean_squared_error_PT(tt,yt)\n\nprint(\"My Mean of Squared Error: \" + str(my_mse))\nprint(\"Pytorch Mean of Squared Error: \" + str(pt_mse))\n\n# backward test\nprint(\"===============================\")\nmy_dmse_dy = dMSE_dy(t, y)\n\npt_mse.backward() # Pytorch will calculate it for us\n\nprint(\"My Derivatives: \" + str(my_dmse_dy))\nprint(\"Pytorch Derivative: \" + str(yt.grad))","a106ae3d":"def sigmoid(z):\n    return 1 \/ (1 + np.exp(-z))\n\ndef sigmoid_prime(z):\n    return sigmoid(z) * (1 - sigmoid(z))\n\nz = np.random.randn(4,2)\nz_prime = sigmoid_prime(z)\n\nzt = torch.Tensor(z)\nzt.requires_grad = True\npt_sigmoid = torch.sigmoid(zt)\nprint(\"Sigmoid Prime: \" + str(z_prime))\n\npt_sigmoid.backward() # THIS WILL RAISE AN ERROR\nprint(\"Pytorch Sigmoid Prime: \" + str(zt.grad))\n\n","24ab4e7c":"def sigmoid(z):\n    return 1 \/ (1 + np.exp(-z))\n\ndef sigmoid_prime(z):\n    return sigmoid(z) * (1 - sigmoid(z))\n\nz = np.random.randn(4,2)\nz_prime = sigmoid_prime(z)\n\nzt = torch.Tensor(z)\nzt.requires_grad = True\npt_sigmoid = torch.sigmoid(zt)\nprint(\"Sigmoid Prime: \" + str(z_prime))\n\n# This works because L = sum(all sigmoid(zt_i)) \n# => dL\/dzt_i = dL\/dsigmoid * dsigmoid\/dzt_i = dsigmoid\/dzt_i \n# (dL\/dsigmoid = 1)\npt_sigmoid.sum().backward() \nprint(\"Pytorch Sigmoid Prime: \" + str(zt.grad))\n\n","7c7be2b1":"from torch import nn\n","e69eb51c":"x = Variable(torch.randn(2,5), requires_grad=True)\nprint (x)\n","998d85a1":"lin = nn.Linear(5, 3) # a linear transformation","0c9ecd61":"z = lin(x) # forward the data x to the linear transformation\nprint(z)","b3ea0e5d":"y = torch.sigmoid(z)\nprint(y.grad_fn)","95f06a03":"y = torch.tanh(z)\nt = y.sum()\nt.backward()\nprint(x.grad) # dt\/dx","de41a451":"# Input x, output y\nx = Variable(torch.randn(5, 3))\ny = Variable(torch.randn(5, 2))\n\n# Build a linear layer\nlin = nn.Linear(3, 2)\nprint ('w: ', lin.weight)\nprint ('b: ', lin.bias)\n","d2f43042":"criterion = nn.MSELoss()","34528d9a":"optimizer = torch.optim.SGD(lin.parameters(), lr=0.01) # do updates on weights of the linear layer","3adb4ab6":"z = lin(x)","251927e4":"loss = criterion(z, y)\nprint('loss: ', loss.data.item())","c68111ac":"loss.backward()\n\nprint ('dL\/dw: ', lin.weight.grad) \nprint ('dL\/db: ', lin.bias.grad)","fce4282f":"# update weights one time\noptimizer.step()\n\n# Equal to:\n# lin.weight.data.sub_(0.01 * lin.weight.grad.data)  (w = w - 0.01 * dloss\/dw)\n# lin.bias.data.sub_(0.01 * lin.bias.grad.data)  (bias = bias - 0.01 * dloss\/dbias)\n\n# Print out the loss after optimization.\nz = lin(x)\nloss = criterion(z, y)\nprint('loss after 1 step optimization: ', loss.data.item()) # The loss should be smalller than in the step 2 above\n# Those weights should be changed\nprint ('w: ', lin.weight)\nprint ('b: ', lin.bias)","2c3f6830":"import torch\nfrom torch import nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\n# Build a sigmoid layer\nclass MySigmoid(nn.Module): # inheriting from nn.Module!\n    \n    def __init__(self, input_size, output_size):\n        # just inherit the init of the module\n        super(MySigmoid, self).__init__()\n        \n        # Your lego building here\n        # NOTE: The non-linearity sigmoid doesn't have any parameter so we do not put it here\n        # the layer's parameters will be all parameters from learnable modules building the layer\n        self.linear = nn.Linear(input_size, output_size)\n        \n    def forward(self, x):\n        # Pass the input through the linear layer,\n        # then pass that through the sigmoid.\n        return torch.sigmoid(self.linear(x))\n    \ntorch.manual_seed(1234)\n\n# Input x, output y, test on t\nx = Variable(torch.randn(5, 3))\ny = Variable(torch.randn(5, 2))\nt = Variable(torch.zeros(5, 3))\n# WHAT WILL HAPPEN IF WE DO THE FOLLOWING??? TRY IT YOURSELF!!!\n#x = Variable(torch.Tensor(5,3))\n#y = Variable(torch.Tensor(5,2))\n#t = Variable(torch.Tensor(5,3)) \n\n# Create the network comprise of only that sigmoid layer\nmodel = MySigmoid(3,2)\n\n# Define loss and learning method\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n\n# Train many times\ndef train(x, y, epoch):\n    # Forward pass is different from train to test (e.g. dropout, batch norm)\n    # So it's a good habit if you explicitly speak out\n    model.train()\n    for i in range(epoch):\n        # Pytorch accumulates gradients.  We need to clear them out before a new batch\n        model.zero_grad()\n        \n        # Forward pass and calculate errors\n        z = model(x) # This is a shortcut for calling model.forward(x)\n        loss = criterion(z, y)  # This is a shortcut for calling criterion.forward(z, y)\n        print('Epoch {}: Loss: {}'.format(i+1, loss.data.item()))\n              \n        # Do backward pass and update weights\n        loss.backward()\n        optimizer.step()\n\n# Test\ndef test(t):\n    # Say that we do testing\n    model.eval()\n    return model(t) # Do the forward pass on the new input\n\ntrain(x, y, 50)\n\n# Do the test:\nprint(\"==================\")\nprint(\"Apply on new data:\")\nprint(t.data)\nprint(\"Result:\")\nprint(test(t).data)\n\n","7495c34d":"from torchvision import datasets, transforms\n\n# Download MNIST traing data to directory ..\/Data for the first time\n# Read the training dataset from directory ..\/Data if there was such data\n# Convert data to Tensor and do normalization (mean, std)\n# transforms.Compose: compose of many image transformations\nmnist = datasets.MNIST(root='..\/Data',\n                       train=True,\n                       download=True,\n                       transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1,), (0.4,))])\n                      )\n\n# Examine data: select one data pair (the first data instance)\nimage, label = mnist[0]\nprint (image.size())\nprint (label)\n\n# Use DataLoader to easily divide a dataset into train\/valid\/test with mini-batches and shuffle...\ntrain_loader = torch.utils.data.DataLoader(mnist, batch_size=32, shuffle=True)\n\n# Actual usage of data loader is as below.\nfor images, labels in train_loader:\n    # Your training code will be written here\n    pass","35267adc":"import torch\nimport numpy as np\nfrom torch.utils.data import Dataset\n\nclass MyMNIST(Dataset):\n    \"\"\"My custom MNIST dataset.\"\"\"\n    \n    TRAIN = 0\n    VALID = 1\n    TEST = 2\n\n    def __init__(self, csv_file, data=TRAIN, one_hot=False, transforms=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file\n            root_dir (string): Directory with all the images.\n            one_hot (boolean): Do the one-hot encoding on the labels or not\n            transform (callable, optional): Optional transform to be applied on a sample.\n        \"\"\"\n        \n        # load from csv_file (full path)\n        all_data = np.genfromtxt(csv_file, delimiter=\",\", dtype=\"uint8\")\n    \n        # There are 5000 instances (5000 lines in csv file)\n        # First 3000 lines will be training set\n        # The next 1000 lines will be validation set\n        # The last 1000 lines will be the test set\n        # You can modify this\n        train, test = all_data[:4000], all_data[4000:]\n        train, valid = train[:3000], train[3000:]\n        \n        # We can implement the shuffle easily as follows, \n        # but we would like to use this utility from DataLoader\n        # from numpy.random import shuffle\n        # shuffle(train)\n        # ...\n        \n \n        # The label of the digits is always the first fields\n        if data == self.TRAIN:\n            self.input = train[:, 1:]\n            self.label = train[:, 0]\n        elif data == self.VALID:\n            self.input = valid[:, 1:]\n            self.label = valid[:, 0]\n        else:\n            self.input = test[:, 1:]\n            self.label = test[:, 0]\n    \n        \n        # One-hot encoding:\n        if one_hot:\n            self.label = np.array(map(one_hot_encoding, self.label))\n           \n        # Apply the transformations:\n        for i, transfunction in enumerate(transforms):\n            self = transfunction(self)\n        \n    def __len__(self):\n        return len(self.input)\n\n    def __getitem__(self, idx):\n        return self.input[idx], self.label[idx]\n\n\n# One-hot encoding\ndef one_hot_encoding(idx):\n    one_hot_array = np.zeros(10)\n    one_hot_array[idx] = 1\n    return one_hot_array\n\n####################################################\n# We can also implement the transformation ourselves\n\n# This normalizer only works for our MNIST data: divide the pixel values to 255\n# We can implement other general normalizers, e.g. standard normalizer, by:\n# mean = np.mean(data.input)\n# std = np.std(data.input)\n# data.input = (data.input - mean)\/std\n\nclass Normalizer(object):\n\n    def __call__(self, data):\n        data.input = 1.0*data.input\/255\n        return data\n\n# This convert from numpy arrays of data to pytorch tensors\nclass ToTensor(object):\n    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n\n    def __call__(self, data):\n        data.input = torch.from_numpy(data.input).float()\n        data.label = torch.from_numpy(data.label).long()\n        return data","3f733ffa":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\n# Some helper function to draw the image\ndef plot(img):\n    plt.imshow(img.view(28,28).numpy())\n    \n    \nmy_mnist = MyMNIST(csv_file='..\/input\/mnistseven\/mnist_seven.csv',\n                data=MyMNIST.TEST,\n                one_hot=False,\n                transforms=[Normalizer(),\n                            ToTensor()])\n\n\n# Examine data: select one data pair\nimage, label = my_mnist[3]\nplot(image)\nprint(\"Label: \" + str(label))\n\ntrain_loader = torch.utils.data.DataLoader(my_mnist, batch_size=32, shuffle=True)\n\n# Actual usage of data loader is as below.\nfor images, labels in train_loader:\n    # Your training code will be written here\n    pass\n","9d71389c":"import torch\nfrom torch import nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\n\n# Define custom network: 3 layers\nclass MyFNN(nn.Module):\n    \n    def __init__(self, input_size, hidden_sizes, output_size, dropout=0.3):\n        super(MyFNN, self).__init__()\n        \n        self.model = nn.Sequential()  # Container of modules\n        \n        # Dynamically build network\n        for i, hidden_size in enumerate(hidden_sizes):\n            self.model.add_module(\"linear\" + str(i+1), nn.Linear(input_size, hidden_size))\n            self.model.add_module(\"sigmoid\" + str(i+1), nn.Sigmoid())\n            self.model.add_module(\"dropout\" + str(i+1), nn.Dropout(dropout))\n            input_size = hidden_size\n        \n        self.model.add_module(\"linear\" + str(len(hidden_sizes)+1), nn.Linear(input_size, output_size, bias=False))\n        # If you use CrossEntropyLoss instead of NLLLoss, do not need to add this LogSoftmax layer\n        # self.model.add_module(\"logsoftmax\", nn.LogSoftmax())  \n    \n    def forward(self, x):\n        return self.model.forward(x)\n\n    \nbatch_size = 64\ntorch.manual_seed(1234)\n\n# Two-hidden-layer Feedforward Network with default dropout\nnet = MyFNN(784, [100, 50], 10)\n\n\n#criterion = nn.NLLLoss()  # Use Negative Log Likelihood\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n\ndef train(epoch, train_data):\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    \n    for i in range(epoch):\n        num_batch = len(train_data)\/\/batch_size + 1\n        for batch_idx, (images, labels) in enumerate(train_loader):\n            images = images.view(-1, 28*28)\n            \n            #Zero grads before each optimizing step\n            optimizer.zero_grad()\n            \n            # Forward pass and calculate errors\n            y = net(images)\n            loss = criterion(y, labels)\n            if (batch_idx+1) % 100 == 0:\n                print('Iteration {0:d}\/{1:d}: Loss: {2:.2f}'.format(batch_idx+1, num_batch, loss.data.item()))\n            # Do backward pass and update weights\n            loss.backward()\n            optimizer.step()\n        print('Epoch {}: Loss: {}'.format(i+1, loss.data.item()))\n\n\n# Train\nfrom torchvision import datasets, transforms\ntrain_data = datasets.MNIST(root='..\/Data', \n                            train=True, \n                            transform=transforms.ToTensor(),  \n                            download=True)\nnet.train()    \ntrain(5, train_data)\n\n\n# Test\ntest_data = datasets.MNIST(root='..\/Data', \n                            train=False, \n                            transform=transforms.ToTensor(),  \n                            download=True)\n\nnet.eval()\n\ncorrect = 0\ntotal = 0 # accumulated loss over mini-batches\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n\nfor images, labels in test_loader:\n    images = images.view(-1, 28*28)\n    y = net(images)\n    _, predicts = torch.max(y.data, 1)\n    total += labels.size(0)\n    correct += (predicts == labels).sum()\n   \nprint('Accuracy on the test images: %d %%' % (100 * correct \/ total))\n    ","58371dc9":"import torch\nfrom torch import nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\n\n# Define custom network: 3 layerstorch.max\nclass MyFNN(nn.Module):\n    \n    def __init__(self, input_size, hidden_sizes, output_size, dropout=0.3):\n        super(MyFNN, self).__init__()\n        \n        self.model = nn.Sequential()  # Container of modules\n        \n        # Dynamically build network\n        for i, hidden_size in enumerate(hidden_sizes):\n            self.model.add_module(\"linear\" + str(i+1), nn.Linear(input_size, hidden_size))\n            self.model.add_module(\"sigmoid\" + str(i+1), nn.Sigmoid())\n            self.model.add_module(\"dropout\" + str(i+1), nn.Dropout(dropout))\n            input_size = hidden_size\n        \n        self.model.add_module(\"linear\" + str(len(hidden_sizes)+1), nn.Linear(input_size, output_size, bias=False))\n        # If you use CrossEntropyLoss instead of NLLLoss, do not need to add this LogSoftmax layer\n        # self.model.add_module(\"logsoftmax\", nn.LogSoftmax())  \n    \n    def forward(self, x):\n        return self.model.forward(x)\n\n\nbatch_size = 64\ntorch.manual_seed(1234)\n    \n# Two-hidden-layer Feedforward Network with default dropout\nnet = MyFNN(784, [100, 50], 10)\n\n\n#criterion = nn.NLLLoss()  # Use Negative Log Likelihood\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n\ndef train(epoch, train_data):\n    train_loader = torch.utils.data.DataLoader(my_mnist, batch_size=batch_size, shuffle=True)\n    \n    for i in range(epoch):\n        num_batch = len(train_data)\/\/batch_size + 1\n        for batch_idx, (images, labels) in enumerate(train_loader):\n            images = images.view(-1, 28*28)\n            \n            #Zero grads before each optimizing step\n            optimizer.zero_grad()\n            \n            # Forward pass and calculate errors\n            y = net(images)\n            loss = criterion(y, labels)\n            if (batch_idx+1) % 100 == 0:\n                print('Iteration {0:d}\/{1:d}: Loss: {2:.2f}'.format(batch_idx+1, num_batch, loss.data.item()))\n            # Do backward pass and update weights\n            loss.backward()\n            optimizer.step()\n        print('Epoch {}: Loss: {}'.format(i+1, loss.data.item()))\n\n\n# Train\nfrom torchvision import transforms\ntrain_data = MyMNIST(csv_file='..\/input\/mnistseven\/mnist_seven.csv',\n                    data=MyMNIST.TRAIN,\n                    one_hot=False,\n                    transforms=[Normalizer(),\n                                ToTensor()])\n\n\nnet.train()    \ntrain(35, train_data)\n\n\n# Test\ntest_data = MyMNIST(csv_file='..\/input\/mnistseven\/mnist_seven.csv',\n                    data=MyMNIST.TEST,\n                    one_hot=False,\n                    transforms=[Normalizer(),\n                                ToTensor()])\n\nnet.eval()\n\ncorrect = 0\ntotal = 0 # accumulated loss over mini-batches\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n\nfor images, labels in test_loader:\n    images = images.view(-1, 28*28)\n    y = net(images)\n    _, predicts = torch.max(y.data, 1)\n    total += labels.size(0)\n    correct += (predicts == labels).sum()\n   \nprint('Accuracy on the test images: %d %%' % (100 * correct \/ total))\n    ","7b11521d":"cuda = torch.device('cuda')     # Default CUDA-enable GPU device\ncuda1 = torch.device('cuda:1')  # The second available GPU device (0-indexed)\n\na = torch.tensor([1., 2.], device=cuda)\nprint(a)\n\nb = torch.tensor([1., 2.])\nprint(b)\n\nb1 = b.cuda() # Convert CPU tensor to GPU tensor (and store it on the default)\nprint(b1)\n\n# Convert tensor in one device to another device\nb2 = b.to(device=cuda)\nprint(b2)\n\n# The following command will raise an error since a is a GPU tensor and b is a CPU tensor\n#c1 = a + b\n#print(c1)\n\n# Convert tensor in one device to another device\nc1 = a.to(device=torch.device('cpu')) + b\nprint(c1)\n","ccc55630":"import torch\nfrom torch import nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\n\n# Define custom network: 3 layerstorch.max\nclass MyFNN(nn.Module):\n    \n    def __init__(self, input_size, hidden_sizes, output_size, dropout=0.3):\n        super(MyFNN, self).__init__()\n        \n        self.model = nn.Sequential()  # Container of modules\n        \n        # Dynamically build network\n        for i, hidden_size in enumerate(hidden_sizes):\n            self.model.add_module(\"linear\" + str(i+1), nn.Linear(input_size, hidden_size))\n            self.model.add_module(\"sigmoid\" + str(i+1), nn.Sigmoid())\n            self.model.add_module(\"dropout\" + str(i+1), nn.Dropout(dropout))\n            input_size = hidden_size\n        \n        self.model.add_module(\"linear\" + str(len(hidden_sizes)+1), nn.Linear(input_size, output_size, bias=False))\n        # If you use CrossEntropyLoss instead of NLLLoss, do not need to add this LogSoftmax layer\n        # self.model.add_module(\"logsoftmax\", nn.LogSoftmax())  \n    \n    def forward(self, x):\n        return self.model.forward(x)\n\nbatch_size = 64\ntorch.manual_seed(1234)\n\n# Two-hidden-layer Feedforward Network with default dropout\nnet = MyFNN(784, [100, 50], 10).cuda()\n\n\n#criterion = nn.NLLLoss()  # Use Negative Log Likelihood\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n\ndef train(epoch, train_data):\n    train_loader = torch.utils.data.DataLoader(my_mnist, batch_size=batch_size, shuffle=True)\n    \n    for i in range(epoch):\n        num_batch = len(train_data)\/\/batch_size + 1\n        for batch_idx, (images, labels) in enumerate(train_loader):\n            images = images.view(-1, 28*28).cuda()\n            labels = labels.cuda()\n            \n            #Zero grads before each optimizing step\n            optimizer.zero_grad()\n            \n            # Forward pass and calculate errors\n            y = net(images)\n            loss = criterion(y, labels)\n            if (batch_idx+1) % 100 == 0:\n                print('Iteration {0:d}\/{1:d}: Loss: {2:.2f}'.format(batch_idx+1, num_batch, loss.data.item()))\n            # Do backward pass and update weights\n            loss.backward()\n            optimizer.step()\n        print('Epoch {}: Loss: {}'.format(i+1, loss.data.item()))\n\n\n# Train\nfrom torchvision import transforms\ntrain_data = MyMNIST(csv_file='..\/input\/mnistseven\/mnist_seven.csv',\n                    data=MyMNIST.TRAIN,\n                    one_hot=False,\n                    transforms=[Normalizer(),\n                                ToTensor()])\n\n\nnet.train()    \ntrain(35, train_data)\n\n\n# Test\ntest_data = MyMNIST(csv_file='..\/input\/mnistseven\/mnist_seven.csv',\n                    data=MyMNIST.TEST,\n                    one_hot=False,\n                    transforms=[Normalizer(),\n                                ToTensor()])\n\nnet.eval()\n\ncorrect = 0\ntotal = 0 # accumulated loss over mini-batches\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n\nfor images, labels in test_loader:\n    images = images.view(-1, 28*28).cuda()\n    labels = labels.cuda()\n    y = net(images)\n    _, predicts = torch.max(y.data, 1)\n    total += labels.size(0)\n    correct += (predicts == labels).sum()\n   \nprint('Accuracy on the test images: %d %%' % (100 * correct \/ total))\n    ","b57c910d":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)","48d757c2":"import torch\nfrom torch import nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\n\n# Define custom network: 3 layerstorch.max\nclass MyFNN(nn.Module):\n    \n    def __init__(self, input_size, hidden_sizes, output_size, dropout=0.3):\n        super(MyFNN, self).__init__()\n        \n        self.model = nn.Sequential()  # Container of modules\n        \n        # Dynamically build network\n        for i, hidden_size in enumerate(hidden_sizes):\n            self.model.add_module(\"linear\" + str(i+1), nn.Linear(input_size, hidden_size))\n            self.model.add_module(\"sigmoid\" + str(i+1), nn.Sigmoid())\n            self.model.add_module(\"dropout\" + str(i+1), nn.Dropout(dropout))\n            input_size = hidden_size\n        \n        self.model.add_module(\"linear\" + str(len(hidden_sizes)+1), nn.Linear(input_size, output_size, bias=False))\n        # If you use CrossEntropyLoss instead of NLLLoss, do not need to add this LogSoftmax layer\n        # self.model.add_module(\"logsoftmax\", nn.LogSoftmax())  \n    \n    def forward(self, x):\n        return self.model.forward(x)\n\n# Get the available device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\nbatch_size = 64\ntorch.manual_seed(1234)\n\n# Two-hidden-layer Feedforward Network with default dropout\nnet = MyFNN(784, [100, 50], 10).to(device)\n\n\n#criterion = nn.NLLLoss()  # Use Negative Log Likelihood\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n\n\ndef train(epoch, train_data):\n    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n    \n    for i in range(epoch):\n        num_batch = len(train_data)\/\/batch_size + 1\n        for batch_idx, (images, labels) in enumerate(train_loader):\n            images = images.view(-1, 28*28).to(device)\n            labels = labels.to(device)\n            \n            #Zero grads before each optimizing step\n            optimizer.zero_grad()\n            \n            # Forward pass and calculate errors\n            y = net(images)\n            loss = criterion(y, labels)\n            if (batch_idx+1) % 100 == 0:\n                print('Iteration {0:d}\/{1:d}: Loss: {2:.2f}'.format(batch_idx+1, num_batch, loss.data.item()))\n            # Do backward pass and update weights\n            loss.backward()\n            optimizer.step()\n        print('Epoch {}: Loss: {}'.format(i+1, loss.data.item()))\n\n\n# Train\nfrom torchvision import datasets, transforms\ntrain_data = datasets.MNIST(root='..\/Data', \n                            train=True, \n                            transform=transforms.ToTensor(),  \n                            download=True)\nnet.train()    \ntrain(5, train_data)\n\n\n# Test\ntest_data = datasets.MNIST(root='..\/Data', \n                            train=False, \n                            transform=transforms.ToTensor(),  \n                            download=True)\n\nnet.eval()\n\ncorrect = 0\ntotal = 0 # accumulated loss over mini-batches\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n\nfor images, labels in test_loader:\n    images = images.view(-1, 28*28).to(device)\n    labels = labels.to(device)\n    y = net(images)\n    _, predicts = torch.max(y.data, 1)\n    total += labels.size(0)\n    correct += (predicts == labels).sum()\n   \nprint('Accuracy on the test images: %d %%' % (100 * correct \/ total))\n    ","a1ca0327":"## GPU run on our custom data\nTo run on GPUs, normally we need to convert the data (training, testing) and the model (architecture) only","93c28c81":"### Variables not only contain data but also how it's formed","fe0d5a2e":"# 0. Preface on Pytorch","9a5a6c3c":"This tutorial is best used with the lecture on Deep Learning of the Praktikum. Credits go to [official pytorch tutorials](http:\/\/pytorch.org\/tutorials\/).","170779d0":"# 2. Pytorch Variables\n","cc793dde":"### Now we can do auto differentiation","c7a6f4e2":"* Choose topology: How many layers, which activation functions\n* Choose error\/cost\/loss function\n* Choose updating\/learning methods (sgd\/rmsprop\/adadelta\/adam...)\n* Then define the architecture using the \"language\" of Pytorch: Pytorch will build the computational graph for you.","ed61f596":"### Use `view()` to reshape Tensors:","de200a31":"# 6. Custom Modules","e50c86bc":"### In Pytorch, tensors are initialized in a weird way, you are the ones who have to initialize it properly:","b877179d":"# 9. Complete example on MNIST (CPU version)","fa991925":"### Choose Learning method","e9722092":"## Device-independent run","0b56686e":"### 3. Backward pass (and calculate the derivatives of the loss w.r.t weights of the linear layer)","684519ec":"# 8. Custom Dataset","eb2b9763":"Most of the time we want to create our neural architecture ourself from the basic blocks of pytorch. Two things we have to do: inherit `nn.Module` and override the `__init__()` and `forward()` methods (You also have to override `backward()` method if you create the architecture based on a new activation function which hasn't been implemented in pytorch before - this is out of the scope of this tutorial).","d5c2262f":"# 11. Use GPUs\nTensors stored in CPUs are different to tensors stored in GPUs. Depending on the device (currently Pytorch supports only CPUs and CUDA-enable GPUs) that tensors are stored and preprocessed differently, but in general, Pytorch wraps other operations to be transparent to the device as much as possible.  \nThe following cell can be run only when you run it on a GPU-enabled device. ","617a8410":"### Non-linearity","c7fec863":"# 7. Using Datasets ","6b3bbb4a":"Now we can do a complete Feedforward Network to recognize handwritten digits trained and test on MNIST data","d5bbadb9":"# 4. Building a neural network","588520f2":"### 2. Compute Loss:","72a9403c":"Now we can do similar things as the `torchvision.datasets.MNIST`:\n","1ef4a346":"### 4. Update weights of the linear layer using Learning method","3ba841ee":"### You can access and slice Tensors like you do with numpy arrays","48d5e225":"# 5. Training a neural network\n\n1. Forward pass to calculate the outputs of the network\n2. Compute the loss\n3. Do the backward pass to calculate the error terms\n4. Update the weights based on the specified learning method (sgd\/rmsprop\/adadelta\/adam...)","7326d1b4":"Pytorch is a more powerful python version of [Torch](http:\/\/torch.ch\/). Torch is a Scientific Computing framework, not just a Deep Learning framework, but it is famous for Deep Learning application because of its handy Neural Network library (`torch.nn`) and its executing speed. A notable drawback of Torch is its language, Lua, which we are often not familiar with. We can see Pytorch in the way as a Torch framework written in python (and many more awesome features).","5c30dcfc":"### With Pytorch, we can do backward from a scalar variable only (e.g. some loss)","7bc41e4d":"### Linear layer","c2b85c67":"### Topology","46627ab4":"[torchvision](http:\/\/pytorch.org\/docs\/master\/torchvision\/datasets.html) package (not a default pytorch package) contains popular datasets and tools to work easier with datasets, especially computer vision ones. You have to install it with conda or pip separately. Here is the example to load MNIST data and feed to a network.","d14b47ec":"### Choose Loss function","708ecea7":"# 1. Torch Tensor as Numpy Array","3f04fec3":"# <center> Pytorch Tutorial <\/center> ","9e95346d":"# 3. Pytorch modules","c9de4049":"### 1. Forward pass:","d01d8b7a":"### Gradients of more complicated functions can be calculated easily: ","17e2b83b":"### But from Pytorch 0.4, Tensor and Variable are merged. Tensors have the same properties and methods as Variable. So now you can use Tensor instead of Variable or you can still warp Tensor in a Variable as before.","ae3ab9f9":"To work with provided datasets of the Praktikum projects, you should know how to preprocess data. The best way is to make a custom dataset and utilize DataLoader for your code. Here is the example to create a MNIST Dataset from a csv file exactly to what we have for the Neuronale Netze course. Similar to custom module, we have to inherit `Dataset` object and override `__len__()` and `__getitem__()` methods.","90bd499f":"# 12. Complete example of MNIST (run on available devices)","f4645acc":"In Torch and Pytorch Deep Learning regime, the main object is Tensors (multi-dimentional arrays). Pytorch wraps Tensors and the operations between them in the way that they work very similar to Numpy arrays (but can run both on CPUs and GPUs):","ac42841b":"## Variable == Tensor (Pytorch 0.4)\nIn previous versions of Pytorch prior to 0.4, Tensors works as Numpy arrays but both on GPUs and CPUs, and Pytorch variables contain their data (a Tensor) and other information (essentially, a Pytorch Variable is a node in the computational graph).","695fb383":"# 10. Example on our custom MNIST data\n\nThis data is exactly the same as the data we used to train in the exercise of the Neuronale Netze course (which includes only 3000 data instances for training and 1000 instances for testing). You can see how much easier using a good deep learning framework to do tasks like this."}}