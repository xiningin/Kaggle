{"cell_type":{"3bc88136":"code","afed3998":"code","d6e7eec5":"code","aae62d6e":"code","b3f42ed2":"code","145005a7":"code","ab171a1b":"code","75aea0eb":"code","5744d13c":"code","bff09247":"code","504ac14a":"code","ff640d57":"code","153006cf":"code","9ffbfc5a":"code","a77dbb62":"code","2a596341":"code","95a97eba":"code","ddf978ec":"code","923c16b3":"code","1ef95119":"code","a4813629":"code","97387645":"code","7fedbade":"code","1f21066d":"code","f41d0690":"code","d62b95c5":"code","264cb5b5":"code","470950f5":"code","fc21ed06":"code","bf858e1e":"code","ee0f3636":"code","5393ec09":"code","5131a601":"code","d0007f87":"code","12527494":"code","8f9c487a":"code","e93788ea":"code","b3c4a9c1":"code","c976f53c":"code","c7003622":"markdown","10e1ea4f":"markdown","6e48a8cb":"markdown","707ed750":"markdown","9df60cfc":"markdown","b7003013":"markdown","3dd758c2":"markdown","7a8bacb7":"markdown"},"source":{"3bc88136":"import spacy\n\nimport pandas as pd\nimport os\nimport numpy as np\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\n\n# fix random seed for reproducibility\nnp.random.seed(7)\n\n## For basic data preproc: https:\/\/realpython.com\/python-keras-text-classification\/\nfrom keras.preprocessing.text import Tokenizer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.preprocessing import LabelBinarizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom keras.utils.np_utils import to_categorical # https:\/\/stackoverflow.com\/questions\/36927025\/how-to-use-keras-multi-layer-perceptron-for-multi-class-classification\n\nfrom keras.models import Sequential\nfrom keras import layers\n\nfrom keras.layers import Embedding,GlobalMaxPool1D,Dense, Conv1D, LSTM, Dropout, SpatialDropout1D, Input, merge, Multiply, Dot\nfrom keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, Bidirectional, MaxPooling1D\nfrom keras import regularizers, optimizers\n\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.initializers import Constant\nfrom keras.models import Model\nfrom keras import optimizers\n\npd.set_option('max_colwidth',300)\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam\n\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\nfrom sklearn.preprocessing import OneHotEncoder\nfrom tqdm import tqdm\nimport re\n\nfrom sklearn.utils import shuffle\nfrom tqdm import tqdm\n#nltk.download('stopwords')\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, f1_score,precision_recall_fscore_support\n\n## https:\/\/www.kaggle.com\/nikhilroxtomar\/lstm-cnn-1d-with-lstm-attention\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, concatenate\nfrom keras.layers import CuDNNGRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D\nfrom keras.layers import Add, BatchNormalization, Activation, CuDNNLSTM, Dropout\nfrom keras.layers import *\nfrom keras.models import *\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau","afed3998":"# # https:\/\/www.kaggle.com\/danmoller\/keras-training-with-float16-test-kernel-2 # float 16. may give error with batchnorm\n# # gives errors also with cudalstm..\n# import keras\n# # keras.backend.set_floatx('float16')\n# keras.backend.set_floatx('float32')","d6e7eec5":"print(os.listdir(\"..\/input\"))\nprint(os.listdir(\"..\/input\/concept-net-numberbatch\"))\nprint(os.listdir(\"..\/input\/dbpedia-classes\"))","aae62d6e":"TARGET_COL = \"l3\"\n\nmaxlen = 150\nEMBEDDING_DIR = \"..\/input\/concept-net-numberbatch\"\nEMBEDDING_FILE = \"numberbatch-en.txt\"\nEMBEDDING_DIM = 300\n\n# TRAIN_FILE = os.path.join(\"..\/input\",\"dbpedia_train.csv\")\n# VAL_FILE = os.path.join(\"..\/input\",\"dbpedia_val.csv\")\n\nTRAIN_FILE = \"..\/input\/dbpedia-classes\/DBPEDIA_train.csv\"\nVAL_FILE = \"..\/input\/dbpedia-classes\/DBPEDIA_val.csv\"\nTEST_FILE = \"..\/input\/dbpedia-classes\/DBPEDIA_test.csv\"","b3f42ed2":"## https:\/\/realpython.com\/python-keras-text-classification\/\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\ndef plot_history(history):\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training acc')\n    plt.plot(x, val_acc, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()","145005a7":"### https:\/\/www.kaggle.com\/fareise\/multi-head-self-attention-for-text-classification\n\nclass Position_Embedding(Layer):\n    \n    def __init__(self, size=None, mode='sum', **kwargs):\n        self.size = size\n        self.mode = mode\n        super(Position_Embedding, self).__init__(**kwargs)\n        \n    def call(self, x):\n        if (self.size == None) or (self.mode == 'sum'):\n            self.size = int(x.shape[-1])\n        batch_size,seq_len = K.shape(x)[0],K.shape(x)[1]\n        position_j = 1. \/ K.pow(10000., \\\n                                 2 * K.arange(self.size \/ 2, dtype='float32' \\\n                               ) \/ self.size)\n        position_j = K.expand_dims(position_j, 0)\n        position_i = K.cumsum(K.ones_like(x[:,:,0]), 1)-1 \n        position_i = K.expand_dims(position_i, 2)\n        position_ij = K.dot(position_i, position_j)\n        position_ij = K.concatenate([K.cos(position_ij), K.sin(position_ij)], 2)\n        if self.mode == 'sum':\n            return position_ij + x\n        elif self.mode == 'concat':\n            return K.concatenate([position_ij, x], 2)\n        \n    def compute_output_shape(self, input_shape):\n        if self.mode == 'sum':\n            return input_shape\n        elif self.mode == 'concat':\n            return (input_shape[0], input_shape[1], input_shape[2]+self.size)\n\n\n'''\noutput dimention: [batch_size, time_step, nb_head*size_per_head]\nevery word can be represented as a vector [nb_head*size_per_head]\n'''\nclass Attention(Layer):\n\n    def __init__(self, nb_head, size_per_head, **kwargs):\n        self.nb_head = nb_head\n        self.size_per_head = size_per_head\n        self.output_dim = nb_head*size_per_head\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.WQ = self.add_weight(name='WQ', \n                                  shape=(input_shape[0][-1], self.output_dim),\n                                  initializer='glorot_uniform',\n                                  trainable=True)\n        self.WK = self.add_weight(name='WK', \n                                  shape=(input_shape[1][-1], self.output_dim),\n                                  initializer='glorot_uniform',\n                                  trainable=True)\n        self.WV = self.add_weight(name='WV', \n                                  shape=(input_shape[2][-1], self.output_dim),\n                                  initializer='glorot_uniform',\n                                  trainable=True)\n        super(Attention, self).build(input_shape)\n        \n    def Mask(self, inputs, seq_len, mode='mul'):\n        if seq_len == None:\n            return inputs\n        else:\n            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n            mask = 1 - K.cumsum(mask, 1)\n            for _ in range(len(inputs.shape)-2):\n                mask = K.expand_dims(mask, 2)\n            if mode == 'mul':\n                return inputs * mask\n            if mode == 'add':\n                return inputs - (1 - mask) * 1e12\n                \n    def call(self, x):\n        if len(x) == 3:\n            Q_seq,K_seq,V_seq = x\n            Q_len,V_len = None,None\n        elif len(x) == 5:\n            Q_seq,K_seq,V_seq,Q_len,V_len = x\n        Q_seq = K.dot(Q_seq, self.WQ)\n        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n        K_seq = K.dot(K_seq, self.WK)\n        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n        V_seq = K.dot(V_seq, self.WV)\n        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))\n        A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) \/ self.size_per_head**0.5\n        A = K.permute_dimensions(A, (0,3,2,1))\n        A = self.Mask(A, V_len, 'add')\n        A = K.permute_dimensions(A, (0,3,2,1))    \n        A = K.softmax(A)\n        O_seq = K.batch_dot(A, V_seq, axes=[3,2])\n        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))\n        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n        O_seq = self.Mask(O_seq, Q_len, 'mul')\n        return O_seq\n        \n    def compute_output_shape(self, input_shape):\n        return (input_shape[0][0], input_shape[0][1], self.output_dim)","ab171a1b":"train = pd.read_csv(TRAIN_FILE,usecols=[\"text\",TARGET_COL])#.sample(frac=0.5) # downsample for speed\nval =  pd.read_csv(VAL_FILE,usecols=[\"text\",TARGET_COL])#.sample(frac=0.6)\n\ntest =  pd.read_csv(TEST_FILE,usecols=[\"text\",TARGET_COL])\n\nprint(train.shape)\nprint(val.shape)\ntrain.tail()","75aea0eb":"# print(\"total samples:\",train.shape[0] + val.shape[0] + test.shape[0]) # 337739\n# print(\"total samples with deduplicated text (check for multilabel)\",pd.concat([train.text,val.text,test.text]).drop_duplicates().shape[0]) # 337739","5744d13c":"N_CLASSES = train[TARGET_COL].nunique()\nprint(N_CLASSES)\n\ntrain[TARGET_COL].value_counts(normalize=True)*100","bff09247":"encoder = LabelEncoder()\ny_train = encoder.fit_transform(train[TARGET_COL])\ny_val = encoder.transform(val[TARGET_COL]) # transform, not fit! \ny_test = encoder.transform(test[TARGET_COL])\n\n\n# # onehot encode ### fix these if changing from train_labels to y_train .. \n# onehot_encoder = OneHotEncoder(sparse=False)\n# integer_encoded = integer_encoded.reshape(len(train_labels), 1)\n# onehot_encoded = onehot_encoder.fit_transform(train_labels)","504ac14a":"## https:\/\/stackoverflow.com\/questions\/36927025\/how-to-use-keras-multi-layer-perceptron-for-multi-class-classification\n\n# y_train = to_categorical(train[TARGET_COL])\n# y_val = to_categorical(val[TARGET_COL])\n\ny_train = to_categorical(y_train)\ny_val = to_categorical(y_val)\ny_test = to_categorical(y_test)","ff640d57":"sentences_train = train[\"text\"].values\nsentences_val = val[\"text\"].values\nsentences_test = test[\"text\"].values","153006cf":"## https:\/\/realpython.com\/python-keras-text-classification\/\n\ntokenizer = Tokenizer()#num_words=341234\ntokenizer.fit_on_texts(sentences_train)","9ffbfc5a":"X_train = tokenizer.texts_to_sequences(sentences_train)\nX_val = tokenizer.texts_to_sequences(sentences_val)\nX_test = tokenizer.texts_to_sequences(sentences_test)\n\nvocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n\nprint(sentences_train[2])\nprint(X_train[2])","a77dbb62":"word_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","2a596341":"X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_val = pad_sequences(X_val, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)","95a97eba":"embeddings_index = {}\n\nf = open(os.path.join(EMBEDDING_DIR, EMBEDDING_FILE))\n\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:(EMBEDDING_DIM+1)], dtype='float32') # take first K dims only\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","ddf978ec":"## At this point we can leverage our embedding_index dictionary and our word_index to compute our embedding matrix:\n\nembedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","923c16b3":"## define pretrained embedding layer, not trainable\n\nembedding_layer = Embedding(len(word_index) + 1,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            input_length=maxlen,\n                            trainable=False)","1ef95119":"# F! score with keras\n# doesn't make sense to implement per batch, as metrics are calculated per batch\n# https:\/\/www.kaggle.com\/guglielmocamporese\/macro-f1-score-keras\n\nimport tensorflow as tf\nimport keras.backend as K\n\n# def f1(y_true, y_pred):\n#     y_pred = K.round(y_pred)\n#     tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n#     # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n#     fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n#     fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n#     p = tp \/ (tp + fp + K.epsilon())\n#     r = tp \/ (tp + fn + K.epsilon())\n\n#     f1 = 2*p*r \/ (p+r+K.epsilon())\n#     f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n#     return K.mean(f1)\n\n### https:\/\/github.com\/keras-team\/keras\/issues\/6507\n\ndef precision(y_true, y_pred):\n    # Calculates the precision\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision\n\n\ndef recall(y_true, y_pred):\n    # Calculates the recall\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef fbeta_score(y_true, y_pred, beta=1):\n    # Calculates the F score, the weighted harmonic mean of precision and recall.\n\n    if beta < 0:\n        raise ValueError('The lowest choosable beta is zero (only precision).')\n        \n    # If there are no true positives, fix the F score at 0 like sklearn.\n    if K.sum(K.round(K.clip(y_true, 0, 1))) == 0:\n        return 0\n\n    p = precision(y_true, y_pred)\n    r = recall(y_true, y_pred)\n    bb = beta ** 2\n    fbeta_score = (1 + bb) * (p * r) \/ (bb * p + r + K.epsilon())\n    return fbeta_score\n\ndef fmeasure(y_true, y_pred):\n    # Calculates the f-measure, the harmonic mean of precision and recall.\n    return fbeta_score(y_true, y_pred, beta=1)\n","a4813629":"## save on boiler plate - do eval. \n## note : could be cleaner - read in list of tuples, allowing print of eval of train ,eval, test. \n\ndef model_evaluation(model,X, y,subset=\"data\"):\n    loss_score, accuracy_score,fbeta_score_score,recall_score,precision_score = model.evaluate(X, y, verbose=True)\n    print(subset)\n    print(f\"loss {loss_score:.3f}, accuracy {100*accuracy_score:.2f}%, fbeta {100*fbeta_score_score:.2f}%, recall {100*recall_score:.2f}%, precision {100*precision_score:.2f}%\")\n","97387645":"## set up callbacks and optimizer defaults\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3,\n                              patience=2, min_lr=0.004)\n\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=4,restore_best_weights=True)\n# model.fit(X_train, Y_train, callbacks=[reduce_lr])\n\n\nadam = optimizers.Adam(lr=0.08,decay=0.0005)","7fedbade":"#### https:\/\/www.kaggle.com\/fareise\/multi-head-self-attention-for-text-classification\n\nconfig = {\n    \"trainable\": False,\n    \"max_len\": maxlen,\n    \"max_features\": 95000,\n    \"embed_size\": EMBEDDING_DIM,\n    \"units\": 128,\n    \"num_heads\": 8,\n    \"dr\": 0.25,\n    \"epochs\": 2,\n    \"model_checkpoint_path\": \"best_weights\",\n}\n\ndef build_model(config):\n    inp = Input(shape = (config[\"max_len\"],))\n    \n    x = embedding_layer(inp)\n    \n    x = Position_Embedding()(x)\n    x = Dropout(config[\"dr\"])(x) # new     \n    x = Attention(config[\"num_heads\"], config[\"units\"])([x, x, x])  #output: [batch_size, time_step, nb_head*size_per_head]\n\n    x = GlobalMaxPooling1D()(x)\n    x = Dropout(config[\"dr\"])(x)\n    x = Dense(N_CLASSES, activation='softmax')(x)\n\n    \n    model = Model(inputs = inp, outputs = x)\n    model.compile(\n        loss = \"categorical_crossentropy\", \n        #optimizer = Adam(lr = config[\"lr\"], decay = config[\"lr_d\"]), \n        optimizer = \"adam\",\n        metrics = [\"accuracy\",fbeta_score,\n                           recall,\n                          precision],\n    )\n    \n    return model\n\nmodel = build_model(config)\n\n# ### model train + history + run\n\nhistory = model.fit(X_train, y_train,\n                    epochs=8,\n                    verbose=1,\n                    validation_data=(X_val, y_val),\n#                     validation_split=0.1,\n                    batch_size=128\n                     , callbacks=[earlystop]\n                   )\n# loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n# print(\"Training Accuracy: {:.4f}\".format(accuracy))\n# loss, accuracy = model.evaluate(X_val, y_val, verbose=False)\n# print(\"Validation Accuracy:  {:.4f}\".format(accuracy))\n# plot_history(history)\n\n# score = model.evaluate(X_test, y_test,\n#                        verbose=1)\n# print('Test score:', score[0])\n# print('Test accuracy:', score[1])\n\n\n# model_evaluation(model,X_train, y_train,subset=\"TRAIN\")\nmodel_evaluation(model,X_val, y_val,subset=\"val\")\nmodel_evaluation(model,X_test, y_test,subset=\"test\")","1f21066d":"# like build model, with batch norm, +- pooling\ndef build_model_2(config):\n    inp = Input(shape = (config[\"max_len\"],))\n    \n    x = embedding_layer(inp)\n    \n    x = Position_Embedding()(x)\n    x = Dropout(config[\"dr\"])(x) # new     \n    x = Attention(config[\"num_heads\"], config[\"units\"])([x, x, x])  #output: [batch_size, time_step, nb_head*size_per_head]\n\n#     x = BatchNormalization()(GlobalMaxPooling1D()(x))\n#     x = BatchNormalization()(Dropout(config[\"dr\"])(x))\n    x = BatchNormalization()(Flatten()(x))\n    x = Dense(N_CLASSES, activation='softmax')(x)\n\n    \n    model = Model(inputs = inp, outputs = x)\n    model.compile(\n        loss = \"categorical_crossentropy\", \n        optimizer = \"adam\",\n        metrics = [\"accuracy\",fbeta_score,\n                           recall,precision]\n    )\n    \n    return model\n\nmodel = build_model(config)\n\n# ### model train + history + run\n\nhistory = model.fit(X_train, y_train,\n                    epochs=8,\n                    verbose=1,\n                    validation_data=(X_val, y_val),\n#                     validation_split=0.1,\n                    batch_size=128\n                     , callbacks=[earlystop]\n                   )\n\nprint(\"\\n Metrics Eval \\n\")\n# model_evaluation(model,X_train, y_train,subset=\"TRAIN\")\nmodel_evaluation(model,X_val, y_val,subset=\"val\")\nmodel_evaluation(model,X_test, y_test,subset=\"test\")","f41d0690":"## attention based biLSTM:\n### https:\/\/www.kaggle.com\/danofer\/different-embeddings-with-attention-fork\n## https:\/\/www.kaggle.com\/suicaokhoailang\/lstm-attention-baseline-0-672-lb\n## could try use GRU\n\n# https:\/\/www.kaggle.com\/qqgeogor\/keras-lstm-attention-glove840b-lb-0-043\n\n# + batch norm wit hdropout example (drop bias to save on calc time) : https:\/\/github.com\/ranamit112\/alpha-zero-general\/blob\/master\/othello\/keras\/OthelloNNet.py\n\n\nclass Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight((input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim\n    \n    \n\ninp = Input(shape=(maxlen,))\nx = embedding_layer(inp)\nx = Bidirectional(CuDNNLSTM(256, return_sequences=True))(x)\nx = BatchNormalization()(x) # new\nx = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\nx = BatchNormalization()(x) # new\nx = Attention(maxlen)(x)\n# x = Dropout(0.25)(x) \nx = BatchNormalization()(Dropout(0.35)(x))# new\nx = Dense(1024, activation=\"elu\")(x)\nx = BatchNormalization()(x) # new\nx = Dropout(0.5)(x) # new\n# x = BatchNormalization()(Dropout(0.2)(x))# new\nx = Dense(N_CLASSES, activation='softmax')(x)\n\nmodel = Model(inputs = inp, outputs = x)\nmodel.compile(\n    loss = \"categorical_crossentropy\", \n    optimizer = \"adam\",\n    metrics = [\"accuracy\",\n               fbeta_score,\n#                            recall,\n#                           precision\n              ],\n)\n\n\n# model = build_model(config)\n# ### model train + history + run\n\nhistory = model.fit(X_train, y_train,\n                    epochs=5,\n                    verbose=1,\n                    validation_data=(X_val, y_val),\n#                     validation_split=0.1,\n                    batch_size=128\n                   , callbacks=[earlystop])\n# loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n# print(\"Training Accuracy: {:.4f}\".format(accuracy))\n# loss, accuracy = model.evaluate(X_val, y_val, verbose=False)\n# print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n# plot_history(history)\n\n\n# score = model.evaluate(X_test, y_test,\n#                        verbose=1)\n# print('Test score:', score[0])\n# print('Test accuracy:', score[1])\n\n\nloss, accuracy, f1 = model.evaluate(X_val, y_val, verbose=False)\nprint(\"Val Accuracy:  {:.4f}\".format(accuracy))\nprint(\"Val f1:  {:.4f}\".format(f1))\n\n\nloss, accuracy, f1 = model.evaluate(X_test, y_test, verbose=True)\nprint(\"test loss:  {:.4f}\".format(loss))\nprint(\"test Accuracy:  {:.4f}\".format(accuracy))\nprint(\"test f1:  {:.4f}\".format(f1))\n\nplot_history(history)\n\n\n\n# model_evaluation(model,X_train, y_train,subset=\"TRAIN\")\n# model_evaluation(model,X_val, y_val,subset=\"val\")\n# model_evaluation(model,X_test, y_test,subset=\"test\")","d62b95c5":"### ALT BiLSTM + attention:\n\n## attention based biLSTM:\n### https:\/\/www.kaggle.com\/danofer\/different-embeddings-with-attention-fork\n## https:\/\/www.kaggle.com\/suicaokhoailang\/lstm-attention-baseline-0-672-lb\n## https:\/\/www.kaggle.com\/qqgeogor\/keras-lstm-attention-glove840b-lb-0-043\n\n## + batch norm wit hdropout example (drop bias to save on calc time) : https:\/\/github.com\/ranamit112\/alpha-zero-general\/blob\/master\/othello\/keras\/OthelloNNet.py\n\n\n## uses \"Attention\" implementation from previous cell\n\n\ninp = Input(shape=(maxlen,))\nx = embedding_layer(inp)\nx = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\nx = BatchNormalization()(x) # new\nx = Dropout(0.2)(x) \nx = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\nx = BatchNormalization()(x) # new\n\nx = Attention(maxlen)(x)\n# x = Dropout(0.4)(x) \n# x = BatchNormalization()(x) # new\n\n# x = BatchNormalization()(Dropout(0.25)(x))# new\nx = Dense(1024, activation=\"elu\")(x)\nx = BatchNormalization()(x) # new\nx = Dropout(0.5)(x) # new\n# x = BatchNormalization()(Dropout(0.2)(x))# new\nx = Dense(N_CLASSES, activation='softmax')(x)\n\nmodel = Model(inputs = inp, outputs = x)\nmodel.compile(\n    loss = \"categorical_crossentropy\", \n    optimizer = \"adam\",\n    metrics = [\"accuracy\",\n               fbeta_score,\n#                            recall,\n#                           precision\n              ],\n)\n\n\n# model = build_model(config)\n# ### model train + history + run\n\nhistory = model.fit(X_train, y_train,\n                    epochs=8,\n                    verbose=1,\n                    validation_data=(X_val, y_val),\n                    batch_size=128\n                   , callbacks=[earlystop])\n\n\nloss, accuracy, f1 = model.evaluate(X_val, y_val, verbose=False)\nprint(\"Val Accuracy:  {:.4f}\".format(accuracy))\nprint(\"Val f1:  {:.4f}\".format(f1))\n\n\nloss, accuracy, f1 = model.evaluate(X_test, y_test, verbose=True)\nprint(\"test loss:  {:.4f}\".format(loss))\nprint(\"test Accuracy:  {:.4f}\".format(accuracy))\nprint(\"test f1:  {:.4f}\".format(f1))\n\nplot_history(history)\n\n\n\n# model_evaluation(model,X_train, y_train,subset=\"TRAIN\")\n# model_evaluation(model,X_val, y_val,subset=\"val\")\n# model_evaluation(model,X_test, y_test,subset=\"test\")","264cb5b5":"# ###  BiLSTM without attention:\n\n\n# inp = Input(shape=(maxlen,))\n# x = embedding_layer(inp)\n# x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n# x = BatchNormalization()(x) # new\n# x = Dropout(0.2)(x) \n# x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n# x = BatchNormalization()(x) # new\n\n# # x = Attention(maxlen)(x)\n# x = Flatten()(x) # new , instead of attention reshaping\n\n# # x = Dropout(0.4)(x) \n# # x = BatchNormalization()(x) # new\n\n# # x = BatchNormalization()(Dropout(0.25)(x))# new\n# x = Dense(1024, activation=\"elu\")(x)\n# x = BatchNormalization()(x) # new\n# x = Dropout(0.5)(x) # new\n# # x = BatchNormalization()(Dropout(0.2)(x))# new\n# x = Dense(N_CLASSES, activation='softmax')(x)\n\n# model = Model(inputs = inp, outputs = x)\n# model.compile(\n#     loss = \"categorical_crossentropy\", \n#     optimizer = \"adam\",\n#     metrics = [\"accuracy\",\n#                fbeta_score,\n# #                            recall,\n# #                           precision\n#               ],\n# )\n\n\n# # model = build_model(config)\n# # ### model train + history + run\n\n# history = model.fit(X_train, y_train,\n#                     epochs=8,\n#                     verbose=1,\n#                     validation_data=(X_val, y_val),\n#                     batch_size=128\n#                    , callbacks=[earlystop])\n\n\n# loss, accuracy, f1 = model.evaluate(X_val, y_val, verbose=False)\n# print(\"Val Accuracy:  {:.4f}\".format(accuracy))\n# print(\"Val f1:  {:.4f}\".format(f1))\n\n\n# loss, accuracy, f1 = model.evaluate(X_test, y_test, verbose=True)\n# print(\"test loss:  {:.4f}\".format(loss))\n# print(\"test Accuracy:  {:.4f}\".format(accuracy))\n# print(\"test f1:  {:.4f}\".format(f1))\n\n# plot_history(history)\n\n\n\n# # model_evaluation(model,X_train, y_train,subset=\"TRAIN\")\n# # model_evaluation(model,X_val, y_val,subset=\"val\")\n# # model_evaluation(model,X_test, y_test,subset=\"test\")\n\n\n# # without attention, model is clearly worse (overfits more)\n# Val Accuracy:  0.9066\n# Val f1:  0.9105\n# 60794\/60794 [==============================] - 24s 392us\/step\n# test loss:  0.3807\n# test Accuracy:  0.9049\n# test f1:  0.9090\n","470950f5":"## 128 batch size , ~.35 dropout\n\n# Epoch 12\/20\n# 240942\/240942 [==============================] - 168s 699us\/step - loss: 0.1163 - acc: 0.9620 - fbeta_score: 0.9627 - val_loss: 0.2641 - val_acc: 0.9331 - val_fbeta_score: 0.9356\n# Val Accuracy:  0.9304\n# Val f1:  0.9330\n# 60794\/60794 [==============================] - 33s 542us\/step\n# test loss:  0.2580\n# test Accuracy:  0.9287\n# test f1:  0.9320","fc21ed06":"# ### https:\/\/www.kaggle.com\/nikhilroxtomar\/lstm-cnn-1d-with-lstm-attention\n\n# def attention_3d_block(inputs, name):\n#     # inputs.shape = (batch_size, time_steps, input_dim)\n#     print(\"inputs.shape[1].value\",inputs.shape[1].value)\n#     TIME_STEPS = inputs.shape[1].value\n#     SINGLE_ATTENTION_VECTOR = False\n    \n#     input_dim = int(inputs.shape[2])\n#     print(\"input_dim.shape\",input_dim.shape)\n#     a = Permute((2, 1))(inputs)\n#     a = Reshape((input_dim, TIME_STEPS))(a) # this line is not useful. It's just to know which dimension is what.\n#     a = Dense(TIME_STEPS, activation='softmax')(a)\n#     if SINGLE_ATTENTION_VECTOR:\n#         a = Lambda(lambda x: K.mean(x, axis=1))(a)\n#         a = RepeatVector(input_dim)(a)\n#     a_probs = Permute((2, 1), name=name)(a)\n#     output_attention_mul = Multiply()([inputs, a_probs])\n#     return output_attention_mul\n\n\n# def model1(init):\n#     x = init\n#     x = Conv1D(64, 3,strides=2,padding='same',activation='relu')(x)\n#     x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x) #CuDNN\n#     x = attention_3d_block(x, 'attention_vec_1')\n#     x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x) #CuDNN\n#     x = attention_3d_block(x, 'attention_vec_2')\n#     x = GlobalMaxPool1D()(x)\n#     out = Dense(64, activation=\"relu\")(x)\n#     return out\n\n# def m2_block(init, filter, kernel, pool):\n#     x = init\n    \n#     x = Conv1D(filter, kernel, padding='same', kernel_initializer='he_normal', activation='elu')(x)\n#     skip = x\n#     x = Conv1D(filter, kernel, padding='same', kernel_initializer='he_normal', activation='elu')(x)\n#     x = Conv1D(filter, kernel, padding='same')(x)\n#     x = BatchNormalization()(x)\n#     x = Add()([x, skip])\n#     x = Activation('elu')(x) # vs relu\n#     x = MaxPooling1D(pool)(x)\n    \n#     x = Flatten()(x)\n#     x = BatchNormalization()(x)\n    \n#     return x\n\n# def model2(init):\n#     #init = Reshape((maxlen, embed_size, 1))(init)\n    \n#     # pool = maxlen - filter + 1\n#     x0 = m2_block(init, 32, 1, maxlen - 1 + 1)\n#     x1 = m2_block(init, 32, 2, maxlen - 2 + 1)\n#     x2 = m2_block(init, 32, 3, maxlen - 3 + 1)\n#     x3 = m2_block(init, 32, 5, maxlen - 5 + 1)\n    \n#     x = concatenate([x0, x1, x2, x3])\n#     x = Dropout(0.1)(x)  # vs 0.5\n#     out = Dense(64, activation=\"relu\")(x)\n#     return out\n\n\n# def get_model():\n#     inp = Input(shape=(maxlen, ))\n#     #x = Embedding(max_features, embed_size)(inp)\n# #     x = Embedding(input_dim=max_features, output_dim= embed_size , input_length=maxlen,weights=[embedding_matrix], trainable=False)(inp)\n#     x = embedding_layer(inp)\n    \n#     out1 = model1(x)\n#     out2 = model2(x)\n    \n#     conc = concatenate([out1, out2])\n    \n#     #conc = out1\n#     x = Dropout(0.15)(conc)  # vs 0.5\n#     x = Dense(64, activation='elu')(x) # relu\n#     x = Reshape((x.shape[1].value, 1))(x)\n#     x = CuDNNLSTM(32)(x)\n\n#     x = Dense(64, activation=\"elu\")(x) # relu\n#     outp = Dense(N_CLASSES, activation='softmax')(x)\n    \n#     model = Model(inputs=inp, outputs=outp)\n#     model.compile(loss='categorical_crossentropy',\n#                   optimizer='adam',\n#                   metrics=['acc'])    \n\n#     return model\n\n# # model = get_model()\n\n# #################\n# inp = Input(shape=(maxlen, ))\n# #x = Embedding(max_features, embed_size)(inp)\n# #     x = Embedding(input_dim=max_features, output_dim= embed_size , input_length=maxlen,weights=[embedding_matrix], trainable=False)(inp)\n# x = embedding_layer(inp)\n\n# out1 = model1(x)\n# out2 = model2(x)\n\n# conc = concatenate([out1, out2])\n\n# #conc = out1\n# x = Dropout(0.25)(conc)  # vs 0.5\n# x = Dense(64, activation='elu')(x) # relu\n# x = Reshape((x.shape[1].value, 1))(x)\n# #     x = CuDNNLSTM(32)(x)\n# x = LSTM(32)(x)\n\n# x = Dense(128, activation=\"elu\")(x) # relu\n# outp = Dense(N_CLASSES, activation='softmax')(x)\n\n# model = Model(inputs=inp, outputs=outp)\n# model.compile(loss='categorical_crossentropy',\n#               optimizer='adam',\n#               metrics=['acc'])    \n# #################\n\n\n# model.summary()\n\n\n\n# history = model.fit(X_train, y_train,\n#                     epochs=5,\n#                     verbose=1,\n# #                     validation_split=0.1,\n#                     batch_size=128\n#                    , callbacks=[earlystop])\n# loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n# print(\"Training Accuracy: {:.4f}\".format(accuracy))\n# loss, accuracy = model.evaluate(X_val, y_val, verbose=False)\n# print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n# plot_history(history)\n\n\n# score = model.evaluate(X_val, y_val,\n#                        verbose=1)\n# print('Test score:', score[0])\n# print('Test accuracy:', score[1])\n","bf858e1e":"# ### huge speed difference if embedding is not trainable..\n\n# # embedding_dim = 80\n\n# model = Sequential()\n# # model.add(layers.Dropout(0.2))\n# model.add(embedding_layer)\n# model.add(layers.GlobalMaxPool1D())\n# model.add(layers.Flatten()) # gives error\n# # model.add(layers.Dropout(0.35))\n# model.add(layers.Dense(128, activation='relu'))\n# # model.add(layers.Dense(N_CLASSES, activation='relu'))\n# model.add(layers.Dense(N_CLASSES, activation='softmax')) # Instead of 1 sigmoid output (binary classification)\n# model.compile(optimizer='adam',\n#               loss='categorical_crossentropy',\n#               metrics=['accuracy'])\n# model.summary()","ee0f3636":"# ### model train + history + run\n\n# history = model.fit(X_train, y_train,\n#                     epochs=5,\n#                     verbose=1,\n# #                     validation_data=(X_test, y_test),\n#                     validation_split=0.1,\n#                     batch_size=128)\n# loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n# print(\"Training Accuracy: {:.4f}\".format(accuracy))\n# loss, accuracy = model.evaluate(X_val, y_val, verbose=False)\n# print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n# plot_history(history)\n\n\n# score = model.evaluate(X_val, y_val,\n#                        verbose=1)\n# print('Test score:', score[0])\n# print('Test accuracy:', score[1])","5393ec09":"## basic 1D CNN:\n\n# embedding_dim = 80\n\nmodel = Sequential()\n# model.add(layers.Dropout(0.1))\n# model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen,\n#                           trainable=False))\nmodel.add(embedding_layer)\n\n# model.add(layers.Dropout(0.1))\nmodel.add(layers.Conv1D(128, 3, activation='relu',padding=\"same\"))\n# model.add(layers.GlobalMaxPooling1D())# this being activated gives bug..\nmodel.add(layers.Conv1D(128, 5, activation='relu')) # added\n# model.add(layers.GlobalMaxPooling1D()) # added\nmodel.add(layers.Dense(512, activation='relu'))\nmodel.add(layers.Dropout(0.1))\n# model.add(layers.Dense(256, activation='relu'))\n# model.add(layers.Dropout(0.1))\n# model.add(layers.Dense(1, activation='sigmoid')) #orig\nmodel.add(layers.Dense(N_CLASSES, activation='softmax')) #new\n\nmodel.compile(optimizer=adam, #'adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy',fbeta_score])\n\n\n\n### model train + history + run\n\nhistory = model.fit(X_train, y_train,\n                    epochs=5,\n                    verbose=1,\n#                     validation_data=(X_test, y_test),\n                    validation_split=0.1,\n                    batch_size=128\n                   , callbacks=[reduce_lr,earlystop])\n\nmodel.summary()\n\nscore = model.evaluate(X_val, y_val,\n                       verbose=1)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])","5131a601":"# ## Another  1D CNN:\n# ##TODO: use pretrained embeddings : https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/pretrained_word_embeddings.py\n\n\n# # embedding_dim = 50\n\n# model = Sequential()\n# # model.add(layers.Dropout(0.1))\n# # model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen\n# #                           ,trainable=False\n# #                           ))\n\n# model.add(embedding_layer)\n\n# # model.add(layers.Dropout(0.1))\n# model.add(layers.Conv1D(128, 5, activation='relu'))\n\n# model.add(MaxPooling1D(2))\n# # model.add(layers.Dropout(0.1))\n\n# model.add(layers.Conv1D(64, 5, activation='relu'))\n\n# # model.add(MaxPooling1D(2))\n# # model.add(layers.Dropout(0.1))\n\n# # model.add(layers.Conv1D(64, 5, activation='relu'))\n\n# # model.add(layers.GlobalMaxPooling1D())# this being activated gives bug..\n# # model.add(layers.Conv1D(32, 5, activation='relu')) # added\n# model.add(layers.GlobalMaxPooling1D()) # added\n# # model.add(layers.Dropout(0.2))\n# model.add(layers.Dense(32, activation='relu'))\n# # model.add(layers.Dropout(0.1))\n# # model.add(layers.Dense(1, activation='sigmoid')) #orig\n# model.add(layers.Dense(N_CLASSES, activation='softmax')) #new\n\n# model.compile(optimizer=adam, #'adam',\n#               loss='categorical_crossentropy',\n#               metrics=['accuracy'])\n\n\n\n# ### model train + history + run\n\n# history = model.fit(X_train, y_train,\n#                     epochs=2,\n#                     verbose=1,\n# #                     validation_data=(X_test, y_test),\n#                     validation_split=0.1,\n#                     batch_size=128\n#                    , callbacks=[reduce_lr,earlystop])\n\n# model.summary()\n\n# score = model.evaluate(X_val, y_val,\n#                        verbose=1)\n# print('Test score:', score[0])\n# print('Test accuracy:', score[1])","d0007f87":"## Another  1D CNN:\n##TODO: use pretrained embeddings : https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/pretrained_word_embeddings.py\n\n\n\nmodel = Sequential()\n# model.add(layers.Dropout(0.1))\nmodel.add(embedding_layer)\n# model.add(layers.Dropout(0.1))\nmodel.add(layers.Conv1D(128, 4, activation='relu',padding='valid'))\n\n# model.add(MaxPooling1D(2))\n# model.add(layers.Dropout(0.1))\n\nmodel.add(layers.Conv1D(64, 6, activation='relu'))\n\n# model.add(MaxPooling1D(2))\n# # model.add(layers.Dropout(0.1))\n\n# model.add(layers.Conv1D(64, 4, activation='relu'))\n# # model.add(MaxPooling1D(2))\n\n# model.add(layers.Conv1D(32, 5, activation='relu')) # added\n\n# model.add(layers.GlobalMaxPooling1D()) # added\n\nmodel.add(layers.Flatten())\n# model.add(layers.Dropout(0.1))\nmodel.add(layers.Dense(128, activation='relu'))\n# model.add(layers.Dense(128, activation='relu'))\n# model.add(layers.Dropout(0.2))\nmodel.add(layers.Dense(N_CLASSES, activation='softmax')) #new\n\nmodel.compile(optimizer=adam, #'adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.summary()\n\n### model train + history + run\n\nhistory = model.fit(X_train, y_train,\n                    epochs=3,\n                    verbose=1,\n#                     validation_data=(X_test, y_test),\n                    validation_split=0.1,\n                    batch_size=128\n                   , callbacks=[reduce_lr,earlystop])\n\n\n\nscore = model.evaluate(X_val, y_val,\n                       verbose=1)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])","12527494":"## imdb cnn-lstm example\n## https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/imdb_cnn_lstm.py\n","8f9c487a":"## bi-lstm\n##https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/imdb_bidirectional_lstm.py\n\nmodel = Sequential()\nmodel.add(embedding_layer)\nmodel.add(Bidirectional(CuDNNLSTM(128)))\nmodel.add(Dropout(0.25))\nmodel.add(layers.Dense(512))\nmodel.add(Dropout(0.25))\nmodel.add(layers.Dense(N_CLASSES, activation='softmax')) #new\n\n\n# try using different optimizers and different optimizer configs\n\nmodel.compile(optimizer=adam, #'adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy',fbeta_score])\n\n\nmodel.summary()\n\n\n### model train + history + run\n\nhistory = model.fit(X_train, y_train,\n                    epochs=12,\n                    verbose=1,\n                    validation_split=0.1,\n                    batch_size=128\n                   , callbacks=[reduce_lr,earlystop])\n\nscore = model.evaluate(X_val, y_val,\n                       verbose=1)\nprint('Test score:', score[0])\nprint('Test accuracy:', score[1])\n","e93788ea":"## defaultish network https:\/\/blog.keras.io\/using-pre-trained-word-embeddings-in-a-keras-model.html\n\nsequence_input = Input(shape=(maxlen,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\nx = Conv1D(256, 6, activation='elu',padding=\"same\")(embedded_sequences)\nx = BatchNormalization()(x)\n# x = MaxPooling1D(3)(x)\nx = Conv1D(128, 6, activation='elu')(x)\nx = BatchNormalization()(x)\nx = MaxPooling1D(2)(x)\nx = Conv1D(128, 5, activation='elu')(x)\nx = BatchNormalization()(x)\nx = GlobalMaxPooling1D()(x)  # global max pooling\n# x = Flatten()(x)\nx = Dense(1024, activation='elu')(x)\nx = BatchNormalization()(x)\npreds = Dense(N_CLASSES, activation='softmax')(x)\n\nmodel = Model(sequence_input, preds)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam', #rmsprop\n              metrics=['accuracy',fbeta_score])\n\nhistory = model.fit(X_train, y_train,\n                    epochs=20,\n                    verbose=1,\n                    validation_data=(X_val, y_val),\n                    batch_size=128\n                   , callbacks=[earlystop])\n\n\nloss, accuracy, f1 = model.evaluate(X_val, y_val, verbose=False)\nprint(\"Val Accuracy:  {:.4f}\".format(accuracy))\nprint(\"Val f1:  {:.4f}\".format(f1))\n\n\nloss, accuracy, f1 = model.evaluate(X_test, y_test, verbose=True)\nprint(\"test loss:  {:.4f}\".format(loss))\nprint(\"test Accuracy:  {:.4f}\".format(accuracy))\nprint(\"test f1:  {:.4f}\".format(f1))\n","b3c4a9c1":"# # #### attention \n# # ## https:\/\/github.com\/philipperemy\/keras-attention-mechanism\n\n# ### older keras version, update to use merge\/multiply layer\n\n# # inputs = Input(shape=(input_dim,))\n\n# # # ATTENTION PART STARTS HERE\n# # attention_probs = Dense(input_dim, activation='softmax', name='attention_vec')(inputs)\n# # attention_mul = merge([inputs, attention_probs], output_shape=32, name='attention_mul', mode='mul')\n# # # ATTENTION PART FINISHES HERE\n\n# # attention_mul = Dense(64)(attention_mul)\n# # output = Dense(1, activation='sigmoid')(attention_mul)\n# # model = Model(input=[inputs], output=output)\n\n\n\n# ### https:\/\/www.reddit.com\/r\/learnmachinelearning\/comments\/adhsfm\/keras_mergelayer1_layer2_modemul\/\n# # merged = keras.layers.Multiply()([tanh_out, sigmoid_out])\n# # Here merged is actually a layer so first you're creating a Multiply object and then calling it. It would be equivalent to this:\n# import keras\n# multiply_layer = keras.layers.Multiply()\n# # multiply_layer = keras.layers.dot()\n# # merged = multiply_layer([layer1, layer2])\n\n\n# sequence_input = Input(shape=(maxlen,), dtype='int32')\n# embedded_sequences = embedding_layer(sequence_input)\n# seq_v = keras.layers.Flatten()(embedded_sequences)\n# attention_probs = Dense(10500, activation='softmax', name='attention_vec')(seq_v)\n# # attention_mul = merge([embedded_sequences, attention_probs], output_shape=maxlen, name='attention_mul', mode='mul')\n# # https:\/\/www.reddit.com\/r\/learnmachinelearning\/comments\/adhsfm\/keras_mergelayer1_layer2_modemul\/\n# # attention_mul = multiply_layer([embedded_sequences, attention_probs])\n# # attention_mul = keras.layers.dot([seq_v, attention_probs],axes=0,normalize=False)\n\n# attention_mul = multiply_layer([seq_v, attention_probs])\n\n\n# # attention_mul = Flatten(attention_mul)\n# attention_mul = Dense(64)(attention_mul)\n\n# preds = Dense(N_CLASSES, activation='softmax')(attention_mul)\n\n# model = Model(sequence_input, preds)\n# model.compile(loss='categorical_crossentropy',\n#               optimizer='adam',\n#               metrics=['accuracy'])\n\n# model.summary()\n\n# model.fit(X_train, y_train, verbose=1,\n#                     validation_split=0.1,\n#           epochs=3, batch_size=128)\n","c976f53c":"# #### attention \n# ## https:\/\/github.com\/philipperemy\/keras-attention-mechanism\n\n### older keras version, update to use merge\/multiply layer\n\n# inputs = Input(shape=(input_dim,))\n\n# # ATTENTION PART STARTS HERE\n# attention_probs = Dense(input_dim, activation='softmax', name='attention_vec')(inputs)\n# attention_mul = merge([inputs, attention_probs], output_shape=32, name='attention_mul', mode='mul')\n# # ATTENTION PART FINISHES HERE\n\n# attention_mul = Dense(64)(attention_mul)\n# output = Dense(1, activation='sigmoid')(attention_mul)\n# model = Model(input=[inputs], output=output)\n\n\n\n### https:\/\/www.reddit.com\/r\/learnmachinelearning\/comments\/adhsfm\/keras_mergelayer1_layer2_modemul\/\n# merged = keras.layers.Multiply()([tanh_out, sigmoid_out])\n# Here merged is actually a layer so first you're creating a Multiply object and then calling it. It would be equivalent to this:\nimport keras\nmultiply_layer = keras.layers.Multiply()\n# multiply_layer = keras.layers.dot()\n# merged = multiply_layer([layer1, layer2])\n\n\nsequence_input = Input(shape=(maxlen,), dtype='int32')\nseq_v = embedding_layer(sequence_input)\n# seq_v = keras.layers.Flatten()(seq_v)\nattention_probs = Dense(maxlen, activation='softmax', name='attention_vec')(seq_v)\n# attention_mul = merge([embedded_sequences, attention_probs], output_shape=maxlen, name='attention_mul', mode='mul')\n# https:\/\/www.reddit.com\/r\/learnmachinelearning\/comments\/adhsfm\/keras_mergelayer1_layer2_modemul\/\n# attention_mul = multiply_layer([embedded_sequences, attention_probs])\n# attention_mul = keras.layers.dot([seq_v, attention_probs],axes=0,normalize=False)\n\nattention_mul = multiply_layer([seq_v, attention_probs])\n\n\n# attention_mul = Flatten(attention_mul)\nattention_mul = Dense(64)(attention_mul)\n\npreds = Dense(N_CLASSES, activation='softmax')(attention_mul)\n\nmodel = Model(sequence_input, preds)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy',fbeta_score])\n\nmodel.summary()\n\nmodel.fit(X_train, y_train, verbose=1,\n                    validation_split=0.1,\n          epochs=1,\n          batch_size=128)\n","c7003622":"## Pretrained embeddings\n* code from : https:\/\/blog.keras.io\/using-pre-trained-word-embeddings-in-a-keras-model.html\n* start with conceptnet en : https:\/\/github.com\/commonsense\/conceptnet-numberbatch\nhttps:\/\/conceptnet.s3.amazonaws.com\/downloads\/2017\/numberbatch\/numberbatch-en-17.06.txt.gz","10e1ea4f":"##### DBPedia classification\n\n* Note: needs adding of pretrained word vectors\n* Kernel uses downsampled data by default\n* not  all code snippets working\n\n### external resources:\n* https:\/\/blog.keras.io\/using-pre-trained-word-embeddings-in-a-keras-model.html\n* XLNet\n* BERT\n* Keras examples on IMDB: https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/imdb_cnn_lstm.py , https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/imdb_bidirectional_lstm.py\n* more imdb: https:\/\/machinelearningmastery.com\/sequence-classification-lstm-recurrent-neural-networks-python-keras\/\n\n\n* Example \"Wide anddeep\" model (e.g. categorical variables, or OHE BOW text + embeddings learn). : https:\/\/colab.research.google.com\/github\/sararob\/keras-wine-model\/blob\/master\/keras-wide-deep.ipynb\n\n\n* naive Attention (lstm, dense): https:\/\/github.com\/philipperemy\/keras-attention-mechanism\n\n\n* https:\/\/www.kaggle.com\/fareise\/multi-head-self-attention-for-text-classification\n\n* attention based biLSTM:\n    * https:\/\/www.kaggle.com\/danofer\/different-embeddings-with-attention-fork\n    * https:\/\/www.kaggle.com\/suicaokhoailang\/lstm-attention-baseline-0-672-lb\n\n* https:\/\/www.kaggle.com\/nikhilroxtomar\/lstm-cnn-1d-with-lstm-attention # not working here? ","6e48a8cb":"#### not attention models: ","707ed750":"###### Attention + biLSTM","9df60cfc":"## Models\n\n* start with fastText like:\n\n* use early stopping and maybe LR plateau : https:\/\/keras.io\/callbacks\/#reducelronplateau\n\n* https:\/\/github.com\/keras-team\/keras\/issues\/3938\n* The predict_classes method is only available for the Sequential class, not for the Model clas.\n    * With the Model class, you can use the predict method which will give you a vector of probabilities and then get the argmax of this vector (with `np.argmax(y_pred,axis=1))`.","b7003013":"### callbacks and hyperparams: ","3dd758c2":"## Transform text data\n* Preprocess\n*max seq_len\n","7a8bacb7":"#### Process target col + tokenize + pad sequences\n* https:\/\/realpython.com\/python-keras-text-classification\/\n* Or use labelBinarizer for multiclass? (seems simpler):\n    * https:\/\/stackoverflow.com\/a\/50502803\/1610518"}}