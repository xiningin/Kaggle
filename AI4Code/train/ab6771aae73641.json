{"cell_type":{"73963447":"code","3a1887a2":"code","ca78f5bc":"code","b35478d5":"code","9d59315c":"code","1d3fd41b":"code","3f1dcd5f":"code","aba49c9e":"code","cdd57b17":"code","44b54191":"code","b091055a":"code","42e38cfb":"code","f742f78b":"code","4ba94ee4":"code","3816952b":"code","a2046068":"code","8f139267":"code","27be8a75":"code","bc3510db":"code","f3e2a233":"code","0c98f74b":"code","a1fba06f":"code","dbef1fa9":"code","7d4220aa":"code","eccbc19c":"code","b07a987d":"code","f8f1be3e":"code","70937c44":"code","fc8e2b5f":"code","f54864dc":"code","e3fd8e4d":"code","029c219b":"code","28d5c37e":"code","27d9e230":"code","78f60c0f":"code","550e27c3":"code","2a8a3db4":"code","a17eaac5":"code","04857f88":"code","1a4b9b1b":"code","da47ef2f":"code","e00957eb":"code","fa3c4499":"markdown","96e977c6":"markdown","c6f4681f":"markdown","6d4280a1":"markdown","4bbbdb33":"markdown","414d2b6f":"markdown","3d7f3a77":"markdown","15ca02a8":"markdown","668490ed":"markdown"},"source":{"73963447":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport datetime as dt","3a1887a2":"#read all columns and rows\npd.set_option('display.max_columns', None);\npd.set_option('display.max_rows', None);\n\n#read dataset\n# we have two different sheet in our excel file. We choose: sheet_name=\"Year 2010-2011\"\nretail=pd.read_excel(\"..\/input\/online-retail-ii-dataset\/online_retail_II.xlsx\", sheet_name=\"Year 2010-2011\")","ca78f5bc":"df = retail.copy()","b35478d5":"df.shape","9d59315c":"df.head()","1d3fd41b":"df.info()","3f1dcd5f":"df['Country'].value_counts().head()","aba49c9e":"df['Country'].nunique()","cdd57b17":"#We can see how many unique values are in each column\n\nfor i in df.columns:\n    print(i, df[i].nunique())","44b54191":"df['total_price']=df['Price']*df['Quantity']\ndf.head()","b091055a":"df.groupby('Description').agg({'total_price':'sum'}).sort_values('total_price',ascending = False).head()","42e38cfb":"df.groupby('Description').agg({'Quantity':'sum'}).sort_values('Quantity',ascending = False).head()","f742f78b":"#which bills earned the most money\ndf.groupby('Invoice').agg({'total_price':'sum'}).sort_values('total_price',ascending = False).head(3)","4ba94ee4":"#top 3 countries making the most money\ndf.groupby('Country').agg({'total_price':'sum'}).sort_values('total_price', ascending = False).head(3)","3816952b":"df.isnull().sum()","a2046068":"df.isnull().sum().sum()","8f139267":"#ratio of missing values to all data\ndf.isnull().sum().sum()\/df.shape[0]","27be8a75":"df.dropna(inplace = True)\n#check na value\ndf.isna().sum()","bc3510db":"df.groupby('Customer ID').agg({'total_price':'sum'}).head()","f3e2a233":"#Why is the total spend of the customer with id 12346.0 zero?\ndf[df['Customer ID']==12346.0]","0c98f74b":"#lets look at how many return invoice we have\n\ndf[df['Invoice'].astype('str').str.get(0) == 'C'].shape","a1fba06f":"#lets continue with invoice do not start with C\ndf = df[df['Invoice'].astype('str').str.get(0) != 'C']","dbef1fa9":"df.shape","7d4220aa":"# change type for CustomerId\ndf['Customer ID'] = df['Customer ID'].astype('int')\ndf.head()","eccbc19c":"df['InvoiceDate'].max()","b07a987d":"df['InvoiceDate'].min()","f8f1be3e":"#we need only date value\ntoday_date = dt.datetime(2011,12,10)\ntoday_date","70937c44":"recency = today_date - df.groupby('Customer ID').agg({'InvoiceDate':'max'})\nrecency.head()","fc8e2b5f":"#only take days value in column InvoiceDate\nrecency['InvoiceDate']=[i.days for i in recency['InvoiceDate']]\nrecency.head()","f54864dc":"#change the columns name\nrecency.rename(columns ={'InvoiceDate':'Recency'}, inplace = True)\nrecency.head()","e3fd8e4d":"frequency = df.groupby('Customer ID').agg({'InvoiceDate':'nunique'})\nfrequency.rename(columns = {'InvoiceDate':'Frequency'}, inplace = True)\nfrequency.head()","029c219b":"monetary = df.groupby('Customer ID').agg({'total_price':'sum'})\nmonetary.rename(columns={'total_price': 'Monetary'}, inplace= True)\nmonetary.head()","28d5c37e":"print(recency.shape, frequency.shape, monetary.shape)","27d9e230":"#concat colums\nrfm = pd.concat([recency, frequency, monetary], axis=1)\nrfm.head()","78f60c0f":"#set range\n#small recency value is good for us so we did a reverse sequence\nrfm[\"RecencyScore\"] = pd.qcut(rfm['Recency'], 5, labels = [5, 4, 3, 2, 1])\nrfm[\"FrequencyScore\"] = pd.qcut(rfm['Frequency'].rank(method='first'), 5, labels = [1, 2, 3, 4, 5])\nrfm[\"MonetaryScore\"] = pd.qcut(rfm['Monetary'], 5, labels = [1, 2, 3, 4, 5])","550e27c3":"rfm.head()","2a8a3db4":"rfm['RFM']=rfm['RecencyScore'].astype('str')+rfm['FrequencyScore'].astype('str')+rfm['MonetaryScore'].astype('str')\nrfm.head()","a17eaac5":"seg_map={r'[1-2][1-2]': \"Hibernating\", r'[1-2][3-4]': \"At Risk\", r'[1-2]5': \"Can't Lose\", r'3[1-2]': \"About to Sleep\",\n        r'33': \"Need Attention\", r'[3-4][4-5]': \"Loyal Customers\", r'41': \"Promising\", r'51': \"New Customers\",\n        r'[4-5][2-3]': \"Potential Loyalist\", r'5[4-5]': \"Champions\"}\nrfm[\"Segment\"]=rfm[\"RecencyScore\"].astype(str)+ rfm[\"FrequencyScore\"].astype(str)\nrfm[\"Segment\"]=rfm[\"Segment\"].replace(seg_map,regex=True)\nrfm.head()","04857f88":"rfm[['Recency','Frequency','Monetary','Segment']].groupby('Segment').agg({'max','min','mean','count'})","1a4b9b1b":"rfm[rfm[\"Segment\"]==\"Need Attention\"].index","da47ef2f":"dff=pd.DataFrame()\ndff[\"Need_Attention_ID\"]=rfm[rfm[\"Segment\"]==\"Need Attention\"].index\ndff.head()","e00957eb":"dff.to_csv(\"Need_Attention.csv\")","fa3c4499":"MONETARY : a customer\u2019s average order value, or what a customer has spent in the past year.","96e977c6":"RFM SCORE","c6f4681f":"FREQUENCY :refers to how often a specific customer purchases from your business","6d4280a1":"If an invoice started with C, it shows that it was calcelled. The total expenditure appeared to be zero, as return invoice and sales invoice take place together.","4bbbdb33":"RECENCY : a measurement of when a customer has last purchased from your business.\n\n* Recency Score = Today - last purchased day\n* max() : Last day of shopping\n* min() : First day of shopping","414d2b6f":"Actually 25 percent is a value that should be examined. But we will continue by removing na values in this study.","3d7f3a77":"NA VALUES","15ca02a8":"DATA UNDERSTANDING","668490ed":"**CUSTOMER SEGMENTATION WITH RFM ANALYSIS**"}}