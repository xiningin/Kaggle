{"cell_type":{"75410b1f":"code","6c3933cb":"code","c48ce7bf":"code","3478b712":"code","6f680e50":"code","8093b5f5":"code","5268605a":"code","9d1ed9a2":"code","09de9acc":"code","8bf34aa2":"code","f15cd36f":"code","57338014":"code","c3cdc52b":"code","5fd798d4":"code","bf44a908":"code","b72f212d":"code","e2434a37":"code","1c3dcdd3":"code","7ad13a7b":"code","f655b4c9":"code","bd231f56":"code","3de4ec68":"code","1962884f":"code","f5a71cad":"code","20a899be":"code","83498740":"markdown","35a8143c":"markdown","4715c047":"markdown","71849568":"markdown","14b2416d":"markdown","19b21cd6":"markdown","9b2c70b5":"markdown","98a68313":"markdown","2ee22c14":"markdown","88df67fd":"markdown","37df9ca5":"markdown","50d3a51a":"markdown","f76a0b8d":"markdown","d5412f1f":"markdown","5f9561ad":"markdown"},"source":{"75410b1f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6c3933cb":"df=pd.read_csv(\"..\/input\/kathmandu-post-article\/articles_the_kathmandu_post (1).csv\")\ndf.head()","c48ce7bf":"# Plotly imports for interactive visualizations\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls","3478b712":"All_words = df['Article_headlines'].str.split(expand=True).unstack().value_counts()\ndata = [go.Bar(x = All_words.index.values[2:50],\n                y = All_words.values[2:50],\n                marker= dict(colorscale='Jet', color = All_words.values[2:100]),\n                text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Top 50 (Uncleaned) Word frequencies in dataset'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')","6f680e50":"# Import the wordcloud library\nfrom wordcloud import WordCloud\n# Join the different processed titles together.\nlong_string = ','.join(list(df['Article_headlines'].values))\n# Create a WordCloud object\nwordcloud = WordCloud(background_color=\"white\", max_words=5000, contour_width=3, contour_color='steelblue')\n# Generate a word cloud\nwordcloud.generate(long_string)\n# Visualize the word cloud\nwordcloud.to_image()","8093b5f5":"import nltk\n#Tokenization\n# Storing the first text element as a string\nfirst_text = df.Article_headlines.values[0]\nprint(first_text)\nprint(\"=\"*100)\nprint(first_text.split(\" \"))","5268605a":"stopwords = nltk.corpus.stopwords.words('english')\nlen(stopwords),stopwords#179 stop words","9d1ed9a2":"#strips out singular words as well as punctuations into separate elements\nfirst_text_list = nltk.word_tokenize(first_text)\nprint(first_text_list)","09de9acc":"first_text_list_cleaned = [word for word in first_text_list if word.lower() not in stopwords]\nprint(first_text_list_cleaned)\nprint(\"=\"*100)\nprint(\"Length of original list: {0} words\\n\"\n      \"Length of list after stopwords removal: {1} words\"\n      .format(len(first_text_list), len(first_text_list_cleaned)))","8bf34aa2":"#Lemmatization\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\nimport nltk\n\ndef text_cleaner(text):\n    stop_words = set(stopwords.words('english'))\n    f_words = [w for w in nltk.word_tokenize(text) if not w in stop_words] \n    punctuations = '''''\u201d\u2019\u201c!()-[]{};:'\"\\,\u201d,<>.\/?@#$%^&*_~'''\n    fp_words = [w for w in f_words if not w in punctuations] #word list after removing punctuation\n    fp_words_stem = [wordnet_lemmatizer.lemmatize(words, pos=\"v\") for words in fp_words] #wordlist after lemmatization    \n    fp_sent = ' '.join(word for word in fp_words_stem)\n    return fp_sent","f15cd36f":"df['Article_headlines_clean'] = df['Article_headlines'].apply(text_cleaner)\n\ndf['Article_headlines_clean'] =df['Article_headlines_clean'].str.replace(\"[^a-zA-Z#]\", \" \")\n## keeping word with word length gretater than 3.\ndf['Article_headlines_clean'] =df['Article_headlines_clean'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))","57338014":"# Load the regular expression library\nimport re\n# Convert the titles to lowercase\ndf['Article_headlines_clean'] = df['Article_headlines_clean'].map(lambda x: x.lower())\ndf['Article_headlines_clean'].head(50)","c3cdc52b":"df","5fd798d4":"All_words = df['Article_headlines_clean'].str.split(expand=True).unstack().value_counts()\ndata = [go.Bar(x = All_words.index.values[2:50],\n                y = All_words.values[2:50],\n                marker= dict(colorscale='Jet', color = All_words.values[2:100]),\n                text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Top 50 (cleaned) Word frequencies in dataset'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\npy.iplot(fig, filename='basic-bar')","bf44a908":"All_words","b72f212d":"# Import the wordcloud library\nfrom wordcloud import WordCloud\n# Join the different processed titles together.\nlong_string = ','.join(list(df['Article_headlines_clean'].values))\n# Create a WordCloud object\nwordcloud = WordCloud(background_color=\"white\", max_words=4000, contour_width=3, contour_color='steelblue')\n# Generate a word cloud\nwordcloud.generate(long_string)\n# Visualize the word cloud\nwordcloud.to_image()","e2434a37":"df.to_csv(\"processed_and_clean_article_ktm_post.csv\",index=False)","1c3dcdd3":"from sklearn.feature_extraction.text import CountVectorizer\ncount = CountVectorizer(stop_words='english', max_df=.1, max_features=2000)\nX = count.fit_transform(df['Article_headlines_clean'].values) #feature matrix","7ad13a7b":"from sklearn.decomposition import LatentDirichletAllocation\nlda = LatentDirichletAllocation(n_components=5,\n                                random_state=11, \n                                learning_method='batch',\n                               )\n# fit transform the feature matrix\nX_topics = lda.fit_transform(X)","f655b4c9":"lda.components_.shape","bd231f56":"print(X_topics.shape)  # (no_of_doc,no_of_topics)\nprint(X_topics)","3de4ec68":"# composition of doc 0 for eg\nprint(\"Document 0: \")\nfor i,topic in enumerate(X_topics[0]):\n  print(\"Topic \",i+1,\": \",topic*100,\"%\")","1962884f":"n_top_words = 10\nfeature_names = count.get_feature_names()\nfor topic_idx, topic in enumerate(lda.components_):\n  print('Topic %d:' % (topic_idx + 1))\n  print(\" \".join([feature_names[i]\n  for i in topic.argsort()\n    [:-n_top_words-1:-1]]))## To get high score value to low score value","f5a71cad":"import pyLDAvis.sklearn\npanel = pyLDAvis.sklearn.prepare(lda, X, count, mds='tsne')\npyLDAvis.display(panel)","20a899be":"from sklearn.model_selection import GridSearchCV\n\n# Define Search Param\nsearch_params = {'n_components': [2, 3, 4, 5, 10, 15, 20, 25], 'learning_decay': [.2,0.4,0.5,0.6,.7]}\n\n# Init the model\nlda = LatentDirichletAllocation()\n\n# Init Grid Search class\nmodel = GridSearchCV(lda, search_params)\n\nmodel.fit(X)\nbest_lda_model = model.best_estimator_\nprint(\"Best model's params: \", model.best_params_)\n","83498740":"## For Better Model Selection: Optimization part:(Optional)","35a8143c":"### Applying helper function which was created above","4715c047":"## Text pre-processing","71849568":"## For interactive topic model visualization\n[Click the link to learn more.](https:\/\/nbviewer.jupyter.org\/github\/bmabey\/pyLDAvis\/blob\/master\/notebooks\/pyLDAvis_overview.ipynb#topic=0&lambda=1&term=)","14b2416d":"## Creating Wordcloud for the visual representation of most common words from cleaned data","19b21cd6":"## Lets check composition of doc 0","9b2c70b5":"## Creating Helper function to get clean text after removing stopwords, punctuation, and applying lemmatization","98a68313":"### Tokenizing","2ee22c14":"## Creating Wordcloud for the visual representation of most common words from unclean data","88df67fd":"## Generating Topics","37df9ca5":"## Get the visual representation of most common words from clean data","50d3a51a":"## Visualizations of word frequencies in dataset for top50 words","f76a0b8d":"## Loading CSV data ","d5412f1f":"### Stopword removal","5f9561ad":"## LDA "}}