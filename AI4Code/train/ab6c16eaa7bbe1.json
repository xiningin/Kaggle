{"cell_type":{"09aa247b":"code","a88aab00":"code","63254c31":"code","91446dd0":"code","f2cabcdc":"code","39fd999e":"code","f35a3396":"code","36754a38":"code","bb0677d0":"code","d5de59d3":"code","2c05103d":"code","c7d3f765":"code","9672590f":"code","2e1e605c":"code","df63c02d":"code","499eaa68":"markdown","3e526948":"markdown","4089b8c8":"markdown"},"source":{"09aa247b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a88aab00":"from bs4 import BeautifulSoup\nfrom gensim.models import word2vec\nfrom nltk.corpus import stopwords\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import make_scorer, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\nfrom tqdm import tqdm_notebook\nimport re","63254c31":"tsvread_params = {\n    \"delimiter\": '\\t',\n    \"quoting\": 3,\n    \"header\": 0\n}\ntrain = pd.read_csv(\"..\/input\/labeledTrainData.tsv\", **tsvread_params)\ntest = pd.read_csv(\"..\/input\/testData.tsv\", **tsvread_params)\nunlabeled_train = pd.read_csv(\"..\/input\/unlabeledTrainData.tsv\", **tsvread_params)","91446dd0":"print(\"Train data shape:\", train.shape)\nprint(\"Test data shape:\", test.shape)\nprint(\"Unlabeled data shape:\", unlabeled_train.shape)\nprint(\"Columns:\", train.columns.values)","f2cabcdc":"def clean_review(raw_text, remove_stops=True, result_as_list=True):\n    # remove HTML\n    text = BeautifulSoup(raw_text, \"lxml\").get_text()\n    \n    # remove non-letters\n    letters_text = re.sub(r\"[^a-zA-Z]\", ' ', text)\n    \n    # lower case and split\n    words = letters_text.lower().split()\n    \n    # remove stop words\n    if remove_stops:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n    if not result_as_list:\n        words = ' '.join(words)\n    return words","39fd999e":"sentences = [clean_review(review, remove_stops=False) for review in unlabeled_train[\"review\"].values]","f35a3396":"num_features = 3000\nmodel_params = {\n    \"size\": num_features, # Word vector dimensionality\n    \"min_count\": 40,      # Minimum word count\n    \"workers\": 4,         # Number of threads to run in parallel\n    \"window\": 10,         # Context window size\n    \"sample\": 1e-3,       # Downsample setting for frequent words\n}\n\nw2v = word2vec.Word2Vec(sentences, **model_params)","36754a38":"w2v.doesnt_match(\"king count queen princess\".split())","bb0677d0":"w2v.most_similar(\"alien\")","d5de59d3":"def makeFeatureVec(words, model, num_features):\n    # Function to average all of the word vectors in a given\n    # paragraph\n    #\n    # Pre-initialize an empty numpy array (for speed)\n    featureVec = np.zeros((num_features,),dtype=\"float32\")\n    #\n    nwords = 0.\n    # \n    # Index2word is a list that contains the names of the words in \n    # the model's vocabulary. Convert it to a set, for speed \n    index2word_set = set(model.wv.index2word)\n    #\n    # Loop over each word in the review and, if it is in the model's\n    # vocaublary, add its feature vector to the total\n    for word in words:\n        if word in index2word_set: \n            nwords = nwords + 1.\n            featureVec = np.add(featureVec, model[word])\n    # \n    # Divide the result by the number of words to get the average\n    featureVec = np.divide(featureVec,nwords)\n    return featureVec\n\n\ndef getAvgFeatureVecs(reviews, model, num_features):\n    # Given a set of reviews (each one a list of words), calculate \n    # the average feature vector for each one and return a 2D numpy array \n    # \n    # Initialize a counter\n    counter = 0\n    # \n    # Preallocate a 2D numpy array, for speed\n    reviewFeatureVecs = np.zeros((len(reviews), num_features), dtype=\"float32\")\n    # \n    # Loop through the reviews\n    for review in tqdm_notebook(reviews, desc=\"Reviews preprocessed\"):\n        # Call the function (defined above) that makes average feature vectors\n        reviewFeatureVecs[counter] = makeFeatureVec(review, model, num_features)\n        #\n        # Increment the counter\n        counter += 1\n    return reviewFeatureVecs","2c05103d":"print(\"Creating average feature vecs for train reviews\")\nclean_train_reviews = []\nfor review in train[\"review\"]:\n    clean_train_reviews.append(clean_review(review))\n\ntrain_features = getAvgFeatureVecs(clean_train_reviews, w2v, num_features)\n\nprint(\"Creating average feature vecs for test reviews\")\nclean_test_reviews = []\nfor review in test[\"review\"]:\n    clean_test_reviews.append(clean_review(review))\n\ntest_features = getAvgFeatureVecs(clean_test_reviews, w2v, num_features)","c7d3f765":"model = RandomForestClassifier(n_estimators=100)","9672590f":"model.fit(train_features, train[\"sentiment\"])","2e1e605c":"y_pred = model.predict(test_features)","df63c02d":"output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": y_pred})\noutput.to_csv(\"out.csv\", index=False, quoting=3)","499eaa68":"# Word2Vec training","3e526948":"# Feature Extraction","4089b8c8":"# Learning"}}