{"cell_type":{"9ecc445d":"code","295815af":"code","8e1f6aec":"code","a5475d76":"code","c366c606":"code","ea29d63c":"code","3e2c9b91":"code","b02fb277":"code","9fe8398e":"code","15308ed5":"code","a9ba25f1":"code","866accb2":"code","d7c85689":"code","7e3d1117":"code","37a409b7":"code","c38c368b":"code","df30a6f9":"code","75ffa86d":"code","ba5e8347":"code","05f09aaa":"code","f62b15e7":"code","dff1803d":"code","6fb1bcb8":"code","b249e4cd":"code","15e79af8":"markdown","784c0202":"markdown","6c046fbd":"markdown","46a5db6c":"markdown","e9a2061a":"markdown","f3870b44":"markdown","f4f340ce":"markdown","e3a146ea":"markdown","ed1b3cc6":"markdown","39280fca":"markdown","95915e22":"markdown","63ba3d6a":"markdown","cb85f160":"markdown","860ce831":"markdown","180f0f81":"markdown"},"source":{"9ecc445d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","295815af":"#Importing all the libraries to be used\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np \nimport pandas as pd\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline    \nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.metrics import precision_score, recall_score, plot_confusion_matrix, classification_report, accuracy_score, f1_score\nfrom sklearn import metrics","8e1f6aec":"#Loading data\ndata = pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\")\ndata.info()","a5475d76":"# Dropping the redundent looking collumns (for this project)\nto_drop = [\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\"]\ndata = data.drop(data[to_drop], axis=1)\n# Renaming the columns because I feel fancy today \ndata.rename(columns = {\"v1\":\"Target\", \"v2\":\"Text\"}, inplace = True)\ndata.head()","c366c606":"#Palette\ncols= [\"#E1F16B\", \"#E598D8\"] \n#first of all let us evaluate the target and find out if our data is imbalanced or not\nplt.figure(figsize=(12,8))\nfg = sns.countplot(x= data[\"Target\"], palette= cols)\nfg.set_title(\"Count Plot of Classes\", color=\"#58508d\")\nfg.set_xlabel(\"Classes\", color=\"#58508d\")\nfg.set_ylabel(\"Number of Data points\", color=\"#58508d\")","ea29d63c":"#Adding a column of numbers of charachters,words and sentences in each msg\ndata[\"No_of_Characters\"] = data[\"Text\"].apply(len)\ndata[\"No_of_Words\"]=data.apply(lambda row: nltk.word_tokenize(row[\"Text\"]), axis=1).apply(len)\ndata[\"No_of_sentence\"]=data.apply(lambda row: nltk.sent_tokenize(row[\"Text\"]), axis=1).apply(len)\n\ndata.describe().T\n\n#PS. At this step, I tokenised the words and sentences and used the length of the same. \n#More on Tokenizing later in the notebook.","3e2c9b91":"plt.figure(figsize=(12,8))\nfg = sns.pairplot(data=data, hue=\"Target\",palette=cols)\nplt.show(fg)","b02fb277":"#Dropping the outliers. \ndata = data[(data[\"No_of_Characters\"]<350)]\ndata.shape","9fe8398e":"plt.figure(figsize=(12,8))\nfg = sns.pairplot(data=data, hue=\"Target\",palette=cols)\nplt.show(fg)","15308ed5":"#Lets have a look at a sample of texts before cleaning\nprint(\"\\033[1m\\u001b[45;1m The First 5 Texts:\\033[0m\",*data[\"Text\"][:5], sep = \"\\n\")","a9ba25f1":"# Defining a function to clean up the text\ndef Clean(Text):\n    sms = re.sub('[^a-zA-Z]', ' ', Text) #Replacing all non-alphabetic characters with a space\n    sms = sms.lower() #converting to lowecase\n    sms = sms.split()\n    sms = ' '.join(sms)\n    return sms\n\ndata[\"Clean_Text\"] = data[\"Text\"].apply(Clean)\n#Lets have a look at a sample of texts after cleaning\nprint(\"\\033[1m\\u001b[45;1m The First 5 Texts after cleaning:\\033[0m\",*data[\"Clean_Text\"][:5], sep = \"\\n\")","866accb2":"data[\"Tokenize_Text\"]=data.apply(lambda row: nltk.word_tokenize(row[\"Clean_Text\"]), axis=1)\n\nprint(\"\\033[1m\\u001b[45;1m The First 5 Texts after Tokenizing:\\033[0m\",*data[\"Tokenize_Text\"][:5], sep = \"\\n\")","d7c85689":"# Removing the stopwords function\ndef remove_stopwords(text):\n    stop_words = set(stopwords.words(\"english\"))\n    filtered_text = [word for word in text if word not in stop_words]\n    return filtered_text\n\ndata[\"Nostopword_Text\"] = data[\"Tokenize_Text\"].apply(remove_stopwords)\n\nprint(\"\\033[1m\\u001b[45;1m The First 5 Texts after removing the stopwords:\\033[0m\",*data[\"Nostopword_Text\"][:5], sep = \"\\n\")","7e3d1117":"lemmatizer = WordNetLemmatizer()\n# lemmatize string\ndef lemmatize_word(text):\n    #word_tokens = word_tokenize(text)\n    # provide context i.e. part-of-speech\n    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in text]\n    return lemmas\n\ndata[\"Lemmatized_Text\"] = data[\"Nostopword_Text\"].apply(lemmatize_word)\nprint(\"\\033[1m\\u001b[45;1m The First 5 Texts after lemitization:\\033[0m\",*data[\"Lemmatized_Text\"][:5], sep = \"\\n\")","37a409b7":"#Creating a corpus of text feature to encode further into vectorized form\ncorpus= []\nfor i in data[\"Lemmatized_Text\"]:\n    msg = ' '.join([row for row in i])\n    corpus.append(msg)\n    \ncorpus[:5]\nprint(\"\\033[1m\\u001b[45;1m The First 5 lines in corpus :\\033[0m\",*corpus[:5], sep = \"\\n\")","c38c368b":"#Changing text data in to numbers. \ntfidf = TfidfVectorizer()\nX = tfidf.fit_transform(corpus).toarray()\n#Let's have a look at our feature \nX.dtype","df30a6f9":"#Label encode the Target and use it as y\nlabel_encoder = LabelEncoder()\ndata[\"Target\"] = label_encoder.fit_transform(data[\"Target\"])","75ffa86d":"#Setting values for labels and feature as y and X(we already did X in vectorizing...)\ny = data[\"Target\"] \n# Splitting the testing and training sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","ba5e8347":"#Testing on the following classifiers\nclassifiers = [MultinomialNB(), \n               RandomForestClassifier(),\n               KNeighborsClassifier(), \n               SVC()]\nfor cls in classifiers:\n    cls.fit(X_train, y_train)\n\n# Dictionary of pipelines and model types for ease of reference\npipe_dict = {0: \"NaiveBayes\", 1: \"RandomForest\", 2: \"KNeighbours\",3: \"SVC\"}","05f09aaa":"# Cossvalidation \nfor i, model in enumerate(classifiers):\n    cv_score = cross_val_score(model, X_train,y_train,scoring=\"accuracy\", cv=10)\n    print(\"%s: %f \" % (pipe_dict[i], cv_score.mean()))","f62b15e7":"# Model Evaluation\n# creating lists of varios scores\nprecision =[]\nrecall =[]\nf1_score = []\ntrainset_accuracy = []\ntestset_accuracy = []\n\nfor i in classifiers:\n    pred_train = i.predict(X_train)\n    pred_test = i.predict(X_test)\n    prec = metrics.precision_score(y_test, pred_test)\n    recal = metrics.recall_score(y_test, pred_test)\n    f1_s = metrics.f1_score(y_test, pred_test)\n    train_accuracy = model.score(X_train,y_train)\n    test_accuracy = model.score(X_test,y_test)\n  \n    #Appending scores\n    precision.append(prec)\n    recall.append(recal)\n    f1_score.append(f1_s)\n    trainset_accuracy.append(train_accuracy)\n    testset_accuracy.append(test_accuracy)\n","dff1803d":"# initialise data of lists.\ndata = {'Precision':precision,\n'Recall':recall,\n'F1score':f1_score,\n'Accuracy on Testset':testset_accuracy,\n'Accuracy on Trainset':trainset_accuracy}\n# Creates pandas DataFrame.\nResults = pd.DataFrame(data, index =[\"NaiveBayes\", \"RandomForest\", \"KNeighbours\",\"SVC\"])","6fb1bcb8":"cmap2 = ListedColormap([\"#E2CCFF\",\"#E598D8\"])\nResults.style.background_gradient(cmap=cmap2)","b249e4cd":"cmap = ListedColormap([\"#E1F16B\", \"#E598D8\"])\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15,10))\n\nfor cls, ax in zip(classifiers, axes.flatten()):\n    plot_confusion_matrix(cls, \n                          X_test, \n                          y_test, \n                          ax=ax, \n                          cmap= cmap,   \n                          )\n    ax.title.set_text(type(cls).__name__)\nplt.tight_layout()  \nplt.show()","15e79af8":"*Of course, I already knew it would just zoom into, but I still replotted it because look at all those pretty colours! (:*\n\n<a id=\"4\"><\/a>\n# <p style=\"background-color:#E598D8;font-family:newtimeroman;color:#E1F16B;font-size:150%;text-align:center;border-radius:20px 60px;\">DATA PREPREPROCESSING<\/p>\n\n![Yellow and Purple Illustrated Voice Talent Minimalist Marketing Presentation (1).png](attachment:6d6dddaa-caf9-430d-8329-2765158e4eb4.png)","784c0202":"**The dataset consists of 5,574 messages in English. The data is designated as being ham or spam.  Dataframe has two columns. The first column is \"Target\" indicating the class of message as ham or spam and the second \"Text\" column is the string of text.** \n\n<a id=\"3\"><\/a>\n# <p style=\"background-color:#E598D8;font-family:newtimeroman;color:#E1F16B;font-size:150%;text-align:center;border-radius:20px 60px;\">DATA EXPLORATION<\/p>","6c046fbd":"<a id=\"4.1\"><\/a>\n# <p style=\"background-color:#E1F16B;font-family:newtimeroman;color:#E598D8;text-align:center;font-size:80%;border-radius:20px 60px;\">CLEANING TEXT<\/p>\n\nThe data cleaning process NLP is crucial. The computer doesn\u2019t understand the text. for the computer, it is just a cluster of symbols. To further process the data we need to make the data cleaner. \n\n* In the first step we extract only the alphabetic characters by this we are removing punctuation and numbers. \n* In the next step, we are converting all the characters into lowercase. \n\nThis text will be then used in further procrssing","46a5db6c":"<a id=\"4.3\"><\/a>\n# <p style=\"background-color:#E1F16B;font-family:newtimeroman;color:#E598D8;text-align:center;font-size:80%;border-radius:20px 60px;\">REMOVING STOPWORDS<\/p>\n\n**Stopwords** are frequently occurring words(*such as few, is, an, etc*). These words hold meaning in sentence structure, but do not contribute much to language processing in NLP. For the purpose of removing redundancy in our processing, I am removing those. NLTK library has a set of default stopwords that we will be removing. ","e9a2061a":"<a id=\"4.2\"><\/a>\n# <p style=\"background-color:#E1F16B;font-family:newtimeroman;color:#E598D8;text-align:center;font-size:80%;border-radius:20px 60px;\">TOKENIZATION<\/p>\n\n**Tokenization** is breaking complex data into smaller units called tokens. It can be done by splitting paragraphs into sentences and sentences into words. \nI am splitting the Clean_Text into words at this step.","f3870b44":"# <p style=\"background-color:#E598D8;font-family:newtimeroman;color:#E1F16B;font-size:150%;text-align:center;border-radius:20px 60px;\">SPAM OR HAM<\/p>\n\n![Yellow and Purple Illustrated Voice Talent Minimalist Marketing Presentation.png](attachment:533b549e-fb7a-45a6-a23a-921606faf44c.png)\n\n   <a id='top'><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<p style=\"background-color:#E598D8;font-family:newtimeroman;color:#E1F16B;font-size:150%;text-align:center;border-radius:20px 60px;\">TABLE OF CONTENTS<\/p>   \n    \n* [1. IMPORTING LIBRARIES](#1)\n    \n* [2. LOADING DATA](#2)\n    \n* [3. DATA EXPLORATION](#3)\n    * [3.1 FEATURE ENGINEERING](#3.1)\n    * [3.2 OUTLIER DETECTION](#3.2)\n    \n    \n* [4. DATA PREPREPROCESSING](#4)\n    \n    * [4.1 CLEANING TEXT](#4.1)\n    * [4.2 TOKENIZATION](#4.2)\n    * [4.3 REMOVING STOPWORDS](#4.3)\n    * [4.4 LEMMATIZATION](#4.4) \n    \n    \n* [5. VECTORIZATION](#5) \n      \n* [6. MODEL BUILDING](#6)\n    \n* [7. EVALUATING MODELS](#7)\n    \n* [8. END](#8)\n\n<p style=\"background-color:#E1F16B;font-family:newtimeroman;color:#E598D8;font-size:150%;text-align:center;border-radius:20px 60px;\">START<\/p>","f4f340ce":"<a id=\"5\"><\/a>\n# <p style=\"background-color:#E598D8;font-family:newtimeroman;font-size:150%;color:#E1F16B;text-align:center;border-radius:20px 60px;\">VECTORIZE<\/p>\n\n**TF-IDF** in NLP stands for Term Frequency \u2013 Inverse document frequency. In NLP cleaned data needs to be converted into a numerical format where each word is represented by a matrix. This is also known as word embedding or Word vectorization.\n\nTerm Frequency (TF) = (Frequency of a term in the document)\/(Total number of terms in documents)\nInverse Document Frequency(IDF) = log( (total number of documents)\/(number of documents with term t))\nI will be using TfidfVectorizer() to vectorize the preprocessed data. \n\n**Steps in the Vectorizing:**\n* Creating a corpus of lemmatized text\n* Converting the corpus in vector form\n* Label Encoding the classes in Target\n \n*Note: So far we have been stalking up columns in our data for the purpose of explanation*","e3a146ea":"<a id=\"7\"><\/a>\n# <p style=\"background-color:#E598D8;font-family:newtimeroman;font-size:150%;color:#E1F16B;text-align:center;border-radius:20px 60px;\">EVALUATING MODELS<\/p>\n**Testing the models on Testset**\n* Accuracy Report\n* Confusion Matrix","ed1b3cc6":" <a id=\"1\"><\/a>\n# <p style=\"background-color:#E598D8;font-family:newtimeroman;color:#E1F16B;font-size:150%;text-align:center;border-radius:20px 60px;\">IMPORTING LIBRARIES<\/p>","39280fca":"<a id=\"6\"><\/a>\n# <p style=\"background-color:#E598D8;font-family:newtimeroman;font-size:150%;color:#E1F16B;text-align:center;border-radius:20px 60px;\">MODEL BUILDING<\/p>\n\n**Steps involved in the Model Building**\n* Setting up features and target as X and y\n* Splitting the testing and training sets\n* Build a pipeline of model for four different classifiers.\n  1. Na\u00efve Bayes\n  2. RandomForestClassifier\n  3. KNeighborsClassifier\n  4. Support Vector Machines\n* Fit all the models on training data\n* Get the cross-validation on the training set for all the models for accuracy","95915e22":"**Note:** From the above countplot the data imbalance is quite evident. \n\n<a id=\"3.1\"><\/a>\n# <p style=\"background-color:#E1F16B;font-family:newtimeroman;color:#E598D8;text-align:center;font-size:80%;border-radius:20px 60px;\">FEATURE ENGINEERING<\/p>\n\nFor the purpose of data exploration, I am creating new features \n\n* No_of_Characters: Number of characters in the text message\n* No_of_Words: Number of words in the text message\n* No_of_sentence: Number of sentences in the text message  ","63ba3d6a":"<a id=\"4.4\"><\/a>\n# <p style=\"background-color:#E1F16B;font-family:newtimeroman;color:#E598D8;text-align:center;font-size:80%;border-radius:20px 60px;\">LEMMATIZATION<\/p>\n\n**Stemming** is the process of getting the root form of a word. Stem or root is the part to which inflectional affixes are added. The stem of a word is created by removing the prefix or suffix of a word. It goes back to the etymology of the word. Languages evolve over time. Many different languages branch into each other; for example, English is a derivative of Latin. Thus, stemming a word takes it back to the root word. \n\n**lemmatization** also converts a word to its root form. However, the difference is that lemmatization ensures that the root word belongs to the language one is dealing with, in our case it is English. If we use lemmatization the output would be in English. ","cb85f160":"**Note:** From the pair plot, we can see a few outliers all in the class ham. This is interesting as we could put a cap over one of these. As they essentially indicate the same thing ie the length of SMS. \n\nNext, I shall be dropping the outliers\n\n<a id=\"3.2\"><\/a>\n# <p style=\"background-color:#E1F16B;font-family:newtimeroman;color:#E598D8;text-align:center;font-size:80%;border-radius:20px 60px;\">OUTLIER DETECTION<\/p>","860ce831":"\n**<span style=\"color:#E598D8;\"> If you liked this Notebook, please do upvote.<\/span>**\n\n**<span style=\"color:#E598D8;\"> If you have any suggestions or questions, feel free to comment!<\/span>**\n\n**<span style=\"color:#E598D8;\"> Best Wishes!<\/span>**\n\n<a id=\"8\"><\/a>\n# <p style=\"background-color:#E598D8;font-family:newtimeroman;font-size:150%;color:#E1F16B;text-align:center;border-radius:20px 60px;\">END<\/p>","180f0f81":"<a id=\"2\"><\/a>\n# <p style=\"background-color:#E598D8;font-family:newtimeroman;color:#E1F16B;font-size:150%;text-align:center;border-radius:20px 60px;\">LOADING DATA<\/p>"}}