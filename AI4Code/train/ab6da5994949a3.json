{"cell_type":{"8874a5b9":"code","0c1b4955":"code","8c5bf638":"code","59bc2549":"code","40abe981":"code","5e40bc46":"code","89e60886":"code","738fbd39":"code","7bf37313":"code","0dea381b":"code","453dfdac":"code","6f8d181b":"code","602287b3":"code","2e7a1b7c":"code","f78413e7":"code","3129a033":"code","6064b375":"code","88b560b1":"code","cfaaa667":"code","7456a3a5":"code","19f0bb57":"code","80c6cf8b":"code","383ed812":"code","3ff30686":"code","42968854":"code","62304011":"code","1a60bfea":"code","b228ed0a":"code","6596c4da":"code","96df345f":"code","505dd10f":"code","cafc31d5":"code","8a37a684":"code","f410d81e":"code","30d02443":"code","7ffb5ff2":"code","6de6fe90":"code","67766588":"code","8dd5b3e5":"code","44992740":"code","8323467f":"code","3bc0d564":"code","e5495490":"code","15e2b15e":"code","c42726fb":"code","70330caf":"code","2a5525a3":"code","c89ebf13":"code","c60b491e":"code","3b76d7b7":"code","3e630e41":"code","21a310a2":"code","9999fdf2":"code","16651440":"code","ea5dd400":"markdown","a44b517a":"markdown","558e036e":"markdown","16a35318":"markdown","fe56ef02":"markdown","56e17ffe":"markdown","fa45ebcb":"markdown","0bc5ce7a":"markdown","a65af68e":"markdown","a20acac8":"markdown","6670469b":"markdown","1f510d5b":"markdown","11797122":"markdown","5d8d439c":"markdown","9b38328b":"markdown","47acf0f5":"markdown","43389066":"markdown","4bddf28a":"markdown","6e58b30d":"markdown","328f27b9":"markdown","f44bcfe4":"markdown","b524ac3e":"markdown","c09b32af":"markdown","9b109c02":"markdown","25fb57ef":"markdown","e4596756":"markdown","e67e69c5":"markdown","0997d8af":"markdown","79919ee3":"markdown","1101f97c":"markdown","9666ff5f":"markdown","ee54e513":"markdown","d07dcc83":"markdown","e7010078":"markdown","32aa63b5":"markdown","ad897e4d":"markdown","9718f7d8":"markdown","62d625fb":"markdown","01071f82":"markdown","52d07c94":"markdown","34f99f2b":"markdown","b3c081ea":"markdown","4601fefa":"markdown","b5dea38d":"markdown","e166fc9e":"markdown","cf1d31f2":"markdown","6d946dcb":"markdown","c41714c4":"markdown","5001eecb":"markdown","fea17a61":"markdown","87f1cde8":"markdown","a9507f93":"markdown","6f65c911":"markdown","fdac65ab":"markdown"},"source":{"8874a5b9":"import numpy as np \nimport pandas as pd\nimport warnings\nwarnings.simplefilter(\"ignore\")\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline","0c1b4955":"dataset=pd.read_csv(\"..\/input\/mushrooms.csv\")\ndataset.head()","8c5bf638":"dataset.isnull().sum()","59bc2549":"dataset.describe()","40abe981":"X=dataset.drop('class',axis=1) #Predictors\ny=dataset['class'] #Response\nX.head()","5e40bc46":"from sklearn.preprocessing import LabelEncoder\nEncoder_X = LabelEncoder() \nfor col in X.columns:\n    X[col] = Encoder_X.fit_transform(X[col])\nEncoder_y=LabelEncoder()\ny = Encoder_y.fit_transform(y)","89e60886":"X.head()","738fbd39":"y","7bf37313":"X=pd.get_dummies(X,columns=X.columns,drop_first=True)\nX.head()","0dea381b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","453dfdac":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","6f8d181b":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\n\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)","602287b3":"def visualization_train(model):\n    sns.set_context(context='notebook',font_scale=2)\n    plt.figure(figsize=(16,9))\n    from matplotlib.colors import ListedColormap\n    X_set, y_set = X_train, y_train\n    X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                     np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n    plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.6, cmap = ListedColormap(('red', 'green')))\n    plt.xlim(X1.min(), X1.max())\n    plt.ylim(X2.min(), X2.max())\n    for i, j in enumerate(np.unique(y_set)):\n        plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                    c = ListedColormap(('red', 'green'))(i), label = j)\n    plt.title(\"%s Training Set\" %(model))\n    plt.xlabel('PC 1')\n    plt.ylabel('PC 2')\n    plt.legend()\ndef visualization_test(model):\n    sns.set_context(context='notebook',font_scale=2)\n    plt.figure(figsize=(16,9))\n    from matplotlib.colors import ListedColormap\n    X_set, y_set = X_test, y_test\n    X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1, stop = X_set[:, 0].max() + 1, step = 0.01),\n                         np.arange(start = X_set[:, 1].min() - 1, stop = X_set[:, 1].max() + 1, step = 0.01))\n    plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n                 alpha = 0.6, cmap = ListedColormap(('red', 'green')))\n    plt.xlim(X1.min(), X1.max())\n    plt.ylim(X2.min(), X2.max())\n    for i, j in enumerate(np.unique(y_set)):\n        plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n                    c = ListedColormap(('red', 'green'))(i), label = j)\n    plt.title(\"%s Test Set\" %(model))\n    plt.xlabel('PC 1')\n    plt.ylabel('PC 2')\n    plt.legend()","2e7a1b7c":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense","f78413e7":"classifier = Sequential()","3129a033":"classifier.add(Dense(8, kernel_initializer='uniform', activation= 'relu', input_dim = 2))\nclassifier.add(Dense(6, kernel_initializer='uniform', activation= 'relu'))\nclassifier.add(Dense(5, kernel_initializer='uniform', activation= 'relu'))\nclassifier.add(Dense(4, kernel_initializer='uniform', activation= 'relu'))\nclassifier.add(Dense(1, kernel_initializer= 'uniform', activation= 'sigmoid'))\nclassifier.compile(optimizer= 'adam',loss='binary_crossentropy', metrics=['accuracy'])","6064b375":"classifier.fit(X_train,y_train,batch_size=10,epochs=100)","88b560b1":"y_pred=classifier.predict(X_test)\ny_pred=(y_pred>0.5)","cfaaa667":"from sklearn.metrics import confusion_matrix,classification_report\nprint(confusion_matrix(y_test, y_pred))","7456a3a5":"print(classification_report(y_test, y_pred))","19f0bb57":"visualization_train(model='ANN')","80c6cf8b":"visualization_test(model='ANN')","383ed812":"from sklearn.model_selection import cross_val_predict, cross_val_score\nfrom sklearn.metrics import confusion_matrix,classification_report,accuracy_score","3ff30686":"def print_score(classifier,X_train,y_train,X_test,y_test,train=True):\n    if train == True:\n        print(\"Training results:\\n\")\n        print('Accuracy Score: {0:.4f}\\n'.format(accuracy_score(y_train,classifier.predict(X_train))))\n        print('Classification Report:\\n{}\\n'.format(classification_report(y_train,classifier.predict(X_train))))\n        print('Confusion Matrix:\\n{}\\n'.format(confusion_matrix(y_train,classifier.predict(X_train))))\n        res = cross_val_score(classifier, X_train, y_train, cv=10, n_jobs=-1, scoring='accuracy')\n        print('Average Accuracy:\\t{0:.4f}\\n'.format(res.mean()))\n        print('Standard Deviation:\\t{0:.4f}'.format(res.std()))\n    elif train == False:\n        print(\"Test results:\\n\")\n        print('Accuracy Score: {0:.4f}\\n'.format(accuracy_score(y_test,classifier.predict(X_test))))\n        print('Classification Report:\\n{}\\n'.format(classification_report(y_test,classifier.predict(X_test))))\n        print('Confusion Matrix:\\n{}\\n'.format(confusion_matrix(y_test,classifier.predict(X_test))))","42968854":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression()\n\nclassifier.fit(X_train,y_train)","62304011":"print_score(classifier,X_train,y_train,X_test,y_test,train=True)","1a60bfea":"print_score(classifier,X_train,y_train,X_test,y_test,train=False)","b228ed0a":"visualization_train('Logistic Reg')","6596c4da":"visualization_test('Logistic Reg')","96df345f":"from sklearn.svm import SVC\nclassifier = SVC(kernel='rbf',random_state=42)\n\nclassifier.fit(X_train,y_train)","505dd10f":"print_score(classifier,X_train,y_train,X_test,y_test,train=True)","cafc31d5":"print_score(classifier,X_train,y_train,X_test,y_test,train=False)","8a37a684":"visualization_train('SVC')","f410d81e":"visualization_test('SVC')","30d02443":"from sklearn.neighbors import KNeighborsClassifier as KNN\n\nclassifier = KNN()\nclassifier.fit(X_train,y_train)","7ffb5ff2":"print_score(classifier,X_train,y_train,X_test,y_test,train=True)","6de6fe90":"print_score(classifier,X_train,y_train,X_test,y_test,train=False)","67766588":"visualization_train('K-NN')","8dd5b3e5":"visualization_test('K-NN')","44992740":"from sklearn.naive_bayes import GaussianNB as NB\n\nclassifier = NB()\nclassifier.fit(X_train,y_train)","8323467f":"print_score(classifier,X_train,y_train,X_test,y_test,train=True)","3bc0d564":"print_score(classifier,X_train,y_train,X_test,y_test,train=False)","e5495490":"visualization_train('Naive Bayes')","15e2b15e":"visualization_test('Naive Bayes')","c42726fb":"from sklearn.tree import DecisionTreeClassifier as DT\n\nclassifier = DT(criterion='entropy',random_state=42)\nclassifier.fit(X_train,y_train)","70330caf":"print_score(classifier,X_train,y_train,X_test,y_test,train=True)","2a5525a3":"print_score(classifier,X_train,y_train,X_test,y_test,train=False)","c89ebf13":"visualization_train('Decision Tree')","c60b491e":"visualization_test('Decision Tree')","3b76d7b7":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 50, criterion = 'entropy', random_state = 42)\nclassifier.fit(X_train, y_train)","3e630e41":"print_score(classifier,X_train,y_train,X_test,y_test,train=True)","21a310a2":"print_score(classifier,X_train,y_train,X_test,y_test,train=False)","9999fdf2":"visualization_train('Random Forest')","16651440":"visualization_test('Random Forest')","ea5dd400":"## Visualising the Decision Tree Test set results","a44b517a":"## Visualising the Logistic Regression Test set results","558e036e":"### Class column is response and rest columns are predictors.\n### Seprating Predictors and Response","16a35318":"## Random Forest Test Results","fe56ef02":"## Decision Tree Test Results","56e17ffe":"## Visualising the K-NN Training set results","fa45ebcb":"## Initializing ANN","0bc5ce7a":"## Classification Report","a65af68e":"## Confusion Matrix","a20acac8":"# Artificial Neural Networks (ANN)","6670469b":"## Visualising the SVC Training set results","1f510d5b":"## SVC Test Results","11797122":"# Naive Bayes Classification Model\n## Fitting Naive Bayes classifier to the Training set","5d8d439c":"## Predicting the Test Set Results","9b38328b":"## Fitting ANN to Training Set","47acf0f5":"### Functions to visualize Training & Test Set Results.","43389066":"## Visualising the K-NN Test set results","4bddf28a":"## Visualising the Naive Bayes Training set results","6e58b30d":"# Results :\n| Classifier | Logistic Reg| SVC | K-NN | Naive Bayes | Decision Tree | Random Forest |\n| --- | --- | --- | --- | --- | --- | --- |\n| Train accuracy score | 0.9057 | 0.9289 | 0.9430 | 0.8980 | 1.0000 | 0.9991 |\n| Average accuracy score | 0.9057 | 0.9281 | 0.9314 | 0.8982 | 0.8920 | 0.9288 |\n| SD | 0.0097 | 0.0112 | 0.0097 | 0.0114 | 0.0128 | 0.0106 |\n| Test accuary score | 0.9028 | 0.9258 | 0.9307 | 0.8966 | 0.9016 | 0.9295 |","328f27b9":"### Importing the Libraries","f44bcfe4":"## SVC Training Results","b524ac3e":"### Splitting the dataset into the Training set and Test set","c09b32af":"### Checking for nulls","9b109c02":"## Naive Bayes Test Results","25fb57ef":"Hi, I'm going to apply 6 supervised machine learning classification models and an ANN model on the given dataset to classify mushrooms as poisonous or eatable.\n1. Logistic Regression\n2. Support Vector machines (SVC)\n3. K-Nearest Neighbours(K-NN)\n4. Naive Bayes classifier\n5. Decision Tree Classifier\n6. Random Forest Classifier\n7. Artificial Neural Networks\n\nI'll proceed by converting categorical variables into dummy\/indicator variables, then reducing dimensions using Princple Component Analysis to reduce 23 categorical variables (which will become 95 variables after conversion)\nto only 2 variables (Principle Components) and training different classification models over these two principle components. Finally, I'll visualize the outputs so that decision boundaries of different models can be seen in 2D-plane. Here the preference is not given to accuracy as the goal is to visualize the decision boundaries. For greater accuracy one can choose more than two variables.","e4596756":"## Decision Tree Training Results","e67e69c5":"## Visualizing ANN Test Set results","0997d8af":"## Logistic Regression Test Results","79919ee3":"# Decision Tree Classification Model\n## Fitting Decision Tree classifier to the Training set","1101f97c":"## K-NN Test Results","9666ff5f":"### Feature Scaling","ee54e513":"### Applying PCA with  n_components = 2","d07dcc83":"## Visualising the Random Forest Test set results","e7010078":"## Visualising the Naive bayes Test set results","32aa63b5":"### Description of Dataset","ad897e4d":"## Random Forest Training Results","9718f7d8":"## Visualising the SVC Test set results","62d625fb":"## Visualising the Decision tree Training set results","01071f82":"## Logistic Regression Training Results","52d07c94":"## Adding Layers","34f99f2b":"# Random Forest Classification Model\n## Fitting Random Forest classifier to the Training set","b3c081ea":"### Creating a func to evaluate model's performance.","4601fefa":"### Encoding categorical data\n### Label encoding","b5dea38d":"## Visualising the Logistic Regression Training set results","e166fc9e":"## K-NN Training  Results","cf1d31f2":"# Logistic Regression Model\n## Fitting Logistic Regression model to the Training set","6d946dcb":"# Support Vecor (SVC) Classification Model\n## Fitting SVC to the Training set","c41714c4":"# Poisonous = 1\n# Eatable = 0","5001eecb":"### Importing the Dataset","fea17a61":"## Visualising the Random Forest Training set results","87f1cde8":"## Naive Bayes Training Results","a9507f93":"## Visualizing ANN Training Set results","6f65c911":"# K Nearest Neighbors (K-NN) Classification Model\n## Fitting K-NN to the Training set","fdac65ab":"### Getting dummy variables"}}