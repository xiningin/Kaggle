{"cell_type":{"8090f7e6":"code","9de4c16b":"code","3097d41d":"code","a28f09fa":"code","816576e1":"code","542bf7a3":"code","1ae195e1":"code","b51e4eaa":"code","d79a49e4":"code","235e3e8e":"code","961c92a6":"code","de61be60":"code","d24069a0":"code","49900bb7":"code","233718a9":"code","a68cbd64":"code","75bbe6ff":"code","4b9fc48e":"code","48637aa5":"code","ef30ab16":"code","63c00791":"code","f6fbfa08":"code","bb4561a4":"code","de9aa86a":"markdown","f47bb2b6":"markdown","0729f9e1":"markdown","d3aeaf7b":"markdown","6861579e":"markdown","04e75e9b":"markdown","9ec46b0a":"markdown","fd4919e2":"markdown","5265d155":"markdown","94b5a712":"markdown","b6a0227e":"markdown","0a25911f":"markdown","960c3ec5":"markdown","6f038c70":"markdown","2519a9e2":"markdown","e3726275":"markdown","3a7ee810":"markdown"},"source":{"8090f7e6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","9de4c16b":"lower, higher = -10, 10\npoints = 100\nclusters = 5\nwidth = 1.2\nrandom_state = 1\n\nnp.random.seed(random_state)\n\ndata = []\nfor ix in range(clusters):\n    cx, cy = np.random.randint(lower, higher), np.random.randint(lower, higher), \n    x = np.random.normal(cx, width, size=(points,))\n    y = np.random.normal(cy, width, size=(points,))\n    data.append(\n        pd.DataFrame({\n            'x': x,\n            'y': y,\n            'label': ix + 1,\n        })\n    )\ndata = pd.concat(data)\n\nX = data.drop('label', axis=1)\ny = data.drop(['x', 'y'], axis=1)\n\nfig, ax = plt.subplots(1,1, figsize=(10,6))\nax.set_aspect('equal')\nplt.axis('off')\nfig.tight_layout()\nfor group, group_data in data.groupby('label'):\n    plt.plot(group_data.x, group_data.y, 'o', ms=5, label=f'Class {group}')\nlgd = ax.legend(loc=8, fontsize=20, frameon=False, markerscale=2)","3097d41d":"def euclidean_distance(vector1, vector2):\n    return np.sqrt(np.sum((vector1 - vector2)**2))\n\n# test function\nvec1 = np.array([3, 0])\nvec2 = np.array([0, 4])\n\n# this is the 3:4:5 triangle and therefore, it should return 5 (Long live Pythagoras)\neuclidean_distance(vec1, vec2)","a28f09fa":"def squared_difference(dataset, vector, columns=['x', 'y']):\n    return ((dataset[columns[0]] - vector[columns[0]])**2 +\n                  (dataset[columns[1]] - vector[columns[1]])**2)","816576e1":"number_of_clusters = len(y.label.unique())\ncentroids = X.sample(number_of_clusters, random_state=random_state)\nnumber_of_clusters","542bf7a3":"def cluster_dataset(dataset, centroids):\n    distances = pd.concat([\n        ((dataset - centroid)**2).sum(axis=1)\n        for ix, centroid in centroids.iterrows()],\n        axis=1,\n    )\n    return dataset.assign( cluster = distances.idxmin(axis=1)), distances.min(axis=1).sum()","1ae195e1":"clustered, _ = cluster_dataset(X, centroids)","b51e4eaa":"clustered.cluster.unique()","d79a49e4":"fig, ax = plt.subplots(1,1, figsize=(10,6))\nax.set_aspect('equal')\nplt.axis('off')\nfig.tight_layout()\nfor group, group_data in clustered.groupby('cluster'):\n    plt.plot(group_data.x, group_data.y, 'o', ms=5, label=f'Class {group + 1}')\n\nfor ix, centroid in centroids.iterrows():\n    ax.plot(centroid.x, centroid.y, 'ro', ms=10)\nlgd = ax.legend(loc=8, fontsize=20, frameon=False, markerscale=2)\n# xlim = ax.set_xlim([-5, 26])    \n","235e3e8e":"def update_centroids(clustered_dataset):\n    new_centroids = clustered_dataset.groupby('cluster').mean().reset_index(drop=True)\n    return new_centroids\n","961c92a6":"centroids = update_centroids(clustered)","de61be60":"centroids","d24069a0":"from time import sleep\nfrom IPython.display import clear_output\n\ndef plot(clustered, centroids, iteration, total_sum):\n    clear_output(wait=True)\n    fig, ax = plt.subplots(1,1, figsize=(10,6))\n    ax.set_aspect('equal')\n    plt.axis('off')\n    fig.tight_layout()\n    for group, group_data in clustered.groupby('cluster'):\n        ax.plot(group_data.x, group_data.y, 'o', ms=5, label=f'Class {group + 1}')\n    for ix, centroid in centroids.iterrows():\n        ax.plot(centroid.x, centroid.y, 'ro', ms=15)\n    lgd = ax.legend(loc=8, fontsize=20, frameon=False, markerscale=2)\n    ax.set_title(f'Iteration {iteration} (sum = {total_sum})')\n    plt.show()\n\n    \ndef cluster_data(dataset, number_of_clusters, max_iter=20, show=False, pause=0.5):\n    ds = dataset.copy()\n    centroids = ds.sample(number_of_clusters, random_state=random_state)\n    previous_sum = pd.Series(range(len(centroids)))\n    for iteration in range(max_iter):\n        clustered, total_sum = cluster_dataset(ds, centroids)\n        centroids = update_centroids(clustered)\n        if show:\n            plot(clustered, centroids, iteration + 1, total_sum)\n            sleep(pause)\n        if (total_sum - previous_sum).sum() == 0:  # this is not the best method\n            break\n        previous_sum = total_sum\n    return clustered, centroids, total_sum\n            \nclust, cent, total_sum = cluster_data(X, number_of_clusters= 5, show=True)","49900bb7":"result = {}\nfor num_of_clust in range(2, 10):\n    clust, cent, total_sum = cluster_data(X, number_of_clusters=num_of_clust, show=False)\n    result[num_of_clust] = total_sum\n    \nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\nax.plot(list(result.keys()), list(result.values()), 'bo--')\n_ = ax.set_xlabel('Number of clusters (k)', fontsize=20)\n_ = ax.set_ylabel('Loss', fontsize=20)","233718a9":"result.values()","a68cbd64":"combined = pd.read_parquet('..\/input\/titanic-family-survivabillity\/titanic_family_survivabillity.parquet')\n\ntrain = combined.loc[combined['set'] == 'train'].drop('set', axis=1).reset_index(drop=True)\ntest = combined.loc[combined['set'] == 'test'].drop(['set', 'Survived'], axis=1).reset_index(drop=True)\n\ntrain['Survived'] = train['Survived'].astype(np.int)\n\ncolumns = ['Pclass', 'Sex',  'Fare', 'family_survival', 'family_size']","75bbe6ff":"clust, cent, total_sum = cluster_data(train[columns], number_of_clusters=100)\n\nclust['Survived'] = train['Survived']\nsurvival_mapper = clust.groupby('cluster')['Survived'].mean().round().astype(np.int).to_dict()\nclust['group_survived'] = clust.cluster.map(survival_mapper)\nclust","4b9fc48e":"accuracy = (clust['Survived'] == clust['group_survived']).mean()\naccuracy  # train dataset accuracy","48637aa5":"survival_map = clust.groupby('cluster')['Survived'].mean().round().astype(np.int).to_dict()\n\npredict, loss = cluster_dataset(test[columns], cent)\n\npredict['Survived'] = predict.cluster.map(survival_map)\npredict['PassengerId'] = test['PassengerId']\npredict = predict[['PassengerId', 'Survived']].sort_values('PassengerId').reset_index(drop=True)\npredict","ef30ab16":"predict.to_csv('results_algorithm_from_scratch.csv', index=False)","63c00791":"from sklearn.cluster import KMeans","f6fbfa08":"km = KMeans(\n    n_clusters=47,\n    init='random',\n    n_init=32,\n    max_iter=600, \n    tol=1e-5,\n    random_state=2020,\n)\n\n_ = km.fit(train[['Pclass', 'Sex',  'Fare', 'family_survival', 'family_size']])","bb4561a4":"df = train.copy()\ndf['cluster'] = km.predict(df[columns])\nmapper = df.groupby('cluster')['Survived'].mean().round().to_dict()\n\ndf = test.copy()\ndf['Survived'] = km.predict(test[['Pclass', 'Sex',  'Fare', 'family_survival', 'family_size']])\ndf['Survived'] = df.Survived.map(mapper).astype(np.int)\ndf = df[['PassengerId', 'Survived']].sort_values('PassengerId').reset_index(drop=True)\n\ndf.to_csv('results_scikit_algorithm.csv', index=False)","de9aa86a":"The Scikit-learn algorithm scores almost the same: 0.796","f47bb2b6":"On Kaggle, it scores 0.794. Not bad at all!","0729f9e1":"Next we need a function to cluster our dataset according to the center points we have randomly chosen.","d3aeaf7b":"We can visualize this in steps:","6861579e":"We start with the dataset we have created during my KNN Notebook:","04e75e9b":"The next step in k-means is to update the centroids to have the center averaged over the found cluster. This will move the centroids to the new position and we can repeat this step to converge to the perfect position.","9ec46b0a":"Next, we need to define a distance function. A very common function is the Euclidean distance.","fd4919e2":"# 3. Solution using Scikit-Learn","5265d155":"Lets plot our clustered dataset and include the centers:","94b5a712":"# 1. Create a k-means algorithm","b6a0227e":"As we already know the amount of clusters, lets just set that. For k-means we need a starting point. We will randomly select the number of clusters amount of points from our dataset.","0a25911f":"The elbow of the graph is around 5, which is exactly the amount of clusters we defined in the beginning. Of course, this is a very nice toy dataset and with real data this might not be so clear. Still, nice to have a confirmation!","960c3ec5":"Lets first generate a toy dataset to apply k-means on:","6f038c70":"The euclidean distance calculates the square root, however, the squared distance is very similar and computationally a bit easier.","2519a9e2":"# 2. Predict Titanic Survivabillity","e3726275":"This is a great method to identify groups, however we need to provide the amount of clusters. One way to select this amount is using the Elbow method. This is a visual method in which we plot the average distance of all datapoints to their centroids as function of number of clusters (k). These will go down fast in the beginning and eventually converge to a flat line. This is because we get clusters with only one or a few points and it cannot get much better. The \"elbow\" of the graph, i.e. the corner is approximately the best k-value. For this, we do need to have the proper distance, i.e. the euclidean distance, as we are summing many distances.","3a7ee810":"# k-means-clustering from scratch\nIn This notebook we write our own k-means from scratch in Python and apply it to predict the survivability on the famous Titanic dataset. The method is very similar to k-nearest-neighbor, however now we find groups of similar data.\n\n### Content\n1. Create a k-means algorithm\n2. Predict Titanic Survivabillity\n3. Solution using Scikit-Learn"}}