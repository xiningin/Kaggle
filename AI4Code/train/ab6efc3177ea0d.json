{"cell_type":{"7cf7801b":"code","24986dd6":"code","320da19c":"code","ec94323c":"code","4253d320":"code","a9fc50fc":"code","48406454":"code","d696f11e":"code","98d88466":"code","9a13febd":"code","f54daec2":"code","3a75da1b":"code","218a341e":"code","b94ac7dd":"code","4f979da7":"code","db7d6790":"code","c4d5c87d":"code","69e39a08":"code","36e142ab":"code","c3ae75a0":"code","0eb985ef":"code","911e5fb0":"code","c4638660":"code","42b54d48":"code","3740c4a6":"code","d15e07a7":"code","0e14934c":"code","37c91a1c":"code","b9814493":"code","c3e17349":"code","d988853c":"code","b359c3b1":"code","0a388e0b":"code","2a3aa9dd":"code","82f40571":"code","5753b19d":"code","aef1dbdb":"code","f8e042d9":"code","87e8ad5d":"code","b85cec89":"code","cbdaa286":"code","a938ca07":"code","43a69204":"code","08ae332e":"code","8b9b62d8":"code","2cad6726":"code","1da1bc1a":"markdown","80c11a7b":"markdown","05a25908":"markdown","2fc49e0e":"markdown","86b2fcb4":"markdown","2f0c0e29":"markdown","347583e0":"markdown","ecaa11d5":"markdown","f2f9909d":"markdown","848ceb0d":"markdown","47ac7b0a":"markdown","dd863e82":"markdown","d8780399":"markdown","7230b91e":"markdown","1757d0f8":"markdown","8c4449a3":"markdown","f3197885":"markdown","741e719e":"markdown","5b48b1b7":"markdown","24306af4":"markdown","59edf9fe":"markdown","58c8b19a":"markdown","bd553926":"markdown","960ba6cd":"markdown","4457f4cd":"markdown"},"source":{"7cf7801b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","24986dd6":"df0 = pd.read_csv(\"..\/input\/0.csv\",header = None)\ndf1 = pd.read_csv(\"..\/input\/1.csv\")\ndf2 = pd.read_csv(\"..\/input\/2.csv\")\ndf3 = pd.read_csv(\"..\/input\/3.csv\")\ndf0.head()","320da19c":"col_names = list()\nfor i in range(0,65):\n    if i == 64:\n        col_names.append(\"class\")\n    else:\n        col_names.append(\"sensor\"+str(i+1))\n    ","ec94323c":"df0.columns = col_names\ndf1.columns = col_names\ndf2.columns = col_names\ndf3.columns = col_names","4253d320":"print(df0.shape)\nprint(df1.shape)\nprint(df2.shape)\nprint(df3.shape)","a9fc50fc":"df = pd.concat([df0,df1,df2,df3],ignore_index=True)\nprint(df.tail())\nprint(df.shape)","48406454":"df.info()","d696f11e":"total = df.isnull().sum().sort_values(ascending = False)\npercentage = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending = False)\nmissing_data = pd.concat([total,percentage],axis = 1, keys = [\"Total\",\"Percentage\"])\nmissing_data","98d88466":"print(df.shape)\ndf.dropna(how = \"any\", inplace = True)\nprint(df.shape)","9a13febd":"sensor1 = df.sensor1.unique()\nlen(sensor1)","f54daec2":"#For special columns\ndf.drop_duplicates(subset=[col_name],keep = \"first\",inplace = True)","3a75da1b":"print(df.shape)\ndf.drop_duplicates(keep = \"first\",inplace = True)\nprint(df.shape)","218a341e":"df.describe()","b94ac7dd":"#boxplot\nimport seaborn as sns\nsns.boxplot(x = df['sensor32'])","4f979da7":"#Z score\ndef detect_outlier(data):\n    outliers=[]\n    \n    threshold=3\n    mean = np.mean(data)\n    std  = np.std(data)\n    \n    \n    for i in data:\n        z_score= (i - mean)\/std         #z = (x-mean)\/std\n        if np.abs(z_score) > threshold:\n            outliers.append(i)\n    return outliers","db7d6790":"outlier = detect_outlier(df.sensor1)\nlen(outlier)","c4d5c87d":"#Compute number of outliers for all columns\nnumber_outliers = []\nx = np.arange(1,66,1) \n\nfor i in df.columns:\n    outliers = detect_outlier(df[i])\n    c = len(outliers)\n    number_outliers.append(c)\nplt.plot(x,number_outliers)\nplt.show()","69e39a08":"#\u0131qr (q3-q1) ---> asl\u0131nda boxplot gibi oluyor.\ndef detect_outlier2(data):\n    outliers = []\n    \n    data = sorted(data)\n    q1, q3 = np.percentile(data,[25,75])\n    iqr = q3 - q1\n    \n    lower_bound = q1 - (1.5 * iqr) \n    upper_bound = q3 + (1.5 * iqr)\n    \n    for i in data:\n        if lower_bound <= i <= upper_bound:\n            continue\n        else:\n            outliers.append(i)\n    \n    return outliers","36e142ab":"outlier = detect_outlier2(df.sensor1)\nlen(outlier)","c3ae75a0":"number_outliers2 = []\nx = np.arange(1,66,1) \n\nfor i in df.columns:\n    outliers = detect_outlier2(df[i])\n    c = len(outliers)\n    number_outliers2.append(c)\nplt.plot(x,number_outliers2)\nplt.xlabel(\"Columns\")\nplt.ylabel(\"Number of Outliers\")\nplt.show()","0eb985ef":"#Z-score\nfrom scipy import stats\n\nz = np.abs(stats.zscore(df))\nprint(z)","911e5fb0":"threshold = 3\nprint(np.where(z > threshold))","c4638660":"zdata = df[(z <= 3).all(axis=1)]\nprint(zdata.shape)","42b54d48":"#next step is to standardize our data - using MinMaxScaler\ny = df[\"class\"]\nx_data1 = df.drop([\"class\"],axis = 1)\n\nfrom sklearn import preprocessing\n\nscaler = preprocessing.MinMaxScaler()\nscaler.fit(x_data1)\n\nx_data1 = pd.DataFrame(scaler.transform(x_data1), index=x_data1.index, columns=x_data1.columns)\nx_data1.iloc[4:10]","3740c4a6":"#other way for normalization\ny = df[\"class\"]\nx_data2 = df.drop([\"class\"],axis = 1)\n\nx = (x_data2 - np.min(x_data2)) \/ (np.max(x_data2) - np.min(x_data2)).values\nx_data2 = pd.DataFrame(scaler.transform(x), index=x.index, columns=x.columns)\nx_data2.iloc[4:10]\n","d15e07a7":"sns.countplot(x='class', data=df)","0e14934c":"#0 and 1lerin indeksleri\ncount = 0\nfor i in y:\n    if i == 0 or i == 1:\n        count+=1\n    else:\n        break\nprint(count)","37c91a1c":"y = y[0:5812].values.reshape(-1,1)\nx = x_data1[0:5812].values\n\nprint(y.shape)\nprint(x.shape)","b9814493":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3, random_state = 42) ","c3e17349":"a=0\nb=0\nc=0\nd=0\nfor i in y_train:\n    if i ==0:\n        a+=1\n    else:\n        b+=1\nfor i in y_test:\n    if i ==0:\n        c+=1\n    else:\n        d+=1\n    \nprint(\"0 class for train: \",a,\"\\n1 class for train: \",b,\"\\n0 class for test: \",c,\"\\n1 class for test: \",d)","d988853c":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression()\nlog_reg.fit(x_train,y_train)\n\nprint(\"Test Accuracy {}\".format(log_reg.score(x_test,y_test)))","b359c3b1":"x5 = df.drop([\"class\"],axis=1)\nx5 = x5.values\ny5 = df[\"class\"].values.reshape(-1,1)\n\nx5 = x5[0:5812]\ny5 = y5[0:5812]","0a388e0b":"from sklearn.model_selection import train_test_split\nx_train1, x_test1, y_train1, y_test1 = train_test_split(x5,y5,test_size = 0.3, random_state = 42) ","2a3aa9dd":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(x_train1,y_train1)\n\nprint(\"Test Accuracy: {}\".format(lr.score(x_test1,y_test1)))","82f40571":"sns.countplot(x=\"class\",data=zdata)","5753b19d":"y6 = zdata[\"class\"].values.reshape(-1,1)\ny6[2846]","aef1dbdb":"x6 = zdata.drop([\"class\"],axis=1)\nx6 = x6.values","f8e042d9":"from sklearn.model_selection import train_test_split\nx_train2, x_test2, y_train2, y_test2 = train_test_split(x6,y6,test_size = 0.3, random_state = 42) ","87e8ad5d":"from sklearn.linear_model import LogisticRegression\n\nlr2 = LogisticRegression()\nlr2.fit(x_train2,y_train2)\n\nprint(\"Accuracy: {}\".format(lr2.score(x_test2,y_test2)))","b85cec89":"y = df[\"class\"].values.reshape(-1,1)\nx = df.drop([\"class\"],axis = 1).values","cbdaa286":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3, random_state = 42) ","a938ca07":"#for K=3;\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)  #n_neighbor = k\nknn.fit(x_train,y_train)\n\n#Accuracy for K=7\nprint(\"K={} iken accuracy: {}\".format(3,knn.score(x_test,y_test)))","43a69204":"#for K=7;\nknn = KNeighborsClassifier(n_neighbors=7)\nknn.fit(x_train,y_train)\n\n#Accuracy for K=7\nprint(\"K={} iken accuracy: {}\".format(3,knn.score(x_test,y_test)))\n\n#Alttaki hatay\u0131 almamak i\u00e7in np.ravel(y_train,order=\"C\") yap\u0131yoruz nedenine bak! Alttaki kodda var.\n#Another way : model = knn.fit(train_fold, train_y.values.reshape(-1,))\n#A\u00e7\u0131klamas\u0131 net bir \u015fekilde burda: https:\/\/www.w3resource.com\/numpy\/manipulation\/ravel.php","08ae332e":"#Find the best K value\nk_value = []\naccuracy = []\n\nfor i in range(1,22):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train,np.ravel(y_train,order='C'))\n    \n    score = knn.score(x_test,y_test)\n    k_value.append(i)\n    accuracy.append(score)\nfor i,j in zip(k_value,accuracy):\n    print(i,j)\n\n#Find K value for Max accuracy\nplt.plot(range(1,22),accuracy,color = \"blue\")\nplt.xlabel(\"K value\")\nplt.ylabel(\"Accuracy\")\nplt.show()","8b9b62d8":"\"\"\"\n#Find max accuracy with range 1-200 for K\nmax_accuracy = 0\n\nfor i in range(1,200):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train,np.ravel(y_train,order=\"C\"))\n    score = knn.score(x_test,y_test)\n    \n    if score > max_accuracy:\n        k,max_accuracy = i,score\n    else:\n        continue\nprint(k,\":\",max_accuracy)\"\"\"","2cad6726":"#random_state verme bakal\u0131m kendisi s\u00fcrekli de\u011fi\u015fsin sonu\u00e7 ne kadar de\u011fi\u015fiyor!\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=42) \n\nknn = KNeighborsClassifier(n_neighbors=7)\nknn.fit(x_train,np.ravel(y_train,order=\"C\"))\n\nprint(\"K=7  Accuracy: {}\".format(knn.score(x_test,y_test)))","1da1bc1a":"* Removing Outlier(Measurement Error or Entry Wrong Data)","80c11a7b":"* Remove Missing Data","05a25908":"We can have an idea about outliers by comparing maximum, minimum and mean values.","2fc49e0e":"**EDA and DATA CLEANING**\n","86b2fcb4":"Change Column Names","2f0c0e29":"* Duplicates","347583e0":"Outliers are more than z-score.","ecaa11d5":"The dropna() method performs the removal of any row of data set if it has an nan value. But, Any rows do not removed because of no nan values.","f2f9909d":"#Bu i\u015flem cok uzun s\u00fcrd\u00fc bunun i\u00e7in range i k\u00fc\u00e7\u00fclt ve tek say\u0131lar yap!!!!","848ceb0d":"* Remove Unneccessary Columns\n\nAll columns are necessary for machine and deep learning.","47ac7b0a":"Outlierlar\u0131n c\u0131kar\u0131lm\u0131\u015f haline","dd863e82":"**Logistic Regression**\n* For Including Outliers","d8780399":"Test size d\u00fc\u015f\u00fcn 0.2 olunca accuracy 0.6844 ten 0.6877 oldu.\nve Randomstate de\u011ferini 42 verince y\u00fczde 69.46 ya \u00e7\u0131kt\u0131.","7230b91e":"No differences among two process.","1757d0f8":"* Normalization","8c4449a3":"**Ad\u0131mlar**\n#KNN = K nearest neighbour(en yak\u0131n K kom\u015fu)\n#1) K de\u011ferini se\u00e7\n#2) K de\u011feri kadar en yak\u0131n data noktalar\u0131n\u0131 bul\n#3) K en yak\u0131n kom\u015fu aras\u0131na hangi class tan ka\u00e7 tane var hesapla\n#4) Test etti\u011fimiz point ya da data hangi class a ait tahmin et\n\n**A\u00e7\u0131klama**\n#Mesela yukar\u0131daki grafikte(radius- texture)  bir test noktas\u0131 seciyoruz. Mesela x=20, y=30.\n#Daha sonra K de\u011ferini secece\u011fiz. Bu \u015fu demek. Diyelim K =3. Secti\u011fim test noktas\u0131na en yak\u0131n 3 tane nokta bul ve onlar\u0131 sec.\n#3(K) tane komsu aras\u0131nda hangi classtan kac tane var => kotu=3 , iyi=0\n#Kotu class \u0131ma daha yak\u0131n oldugu i\u00e7in bu test noktama k\u00f6t\u00fc diyorum.\n\n**\u00d6nemli**\n#KNN ile en \u00f6nemli \u015fey normalization yapmakt\u0131r. Aksi taktirde featurelar birbirini domine edebilir. Bunu kesin yapmal\u0131y\u0131z yoksa data m\u0131z\u0131n accuracy cok d\u00fc\u015f\u00fck c\u0131kabilir.\n#K de\u011ferini \u00e7ift verdik ve esit say\u0131da iyi ve k\u00f6t\u00fc c\u0131kt\u0131 bu durumda algoritma s\u0131n\u0131fland\u0131rma yapamaz, k de\u011ferini bir art\u0131r\u0131r ve ya azalt\u0131r.\n#KNN algoritmas\u0131 kendisi zaten en y\u00fcksek score i\u00e7in K de\u011ferini kendi i\u00e7inde kendisi belirler.\n#Hoca cevap:  K de\u011ferini deneyerek se\u00e7iyoruz. ve Ama ger\u00e7ek hayatta sadece en iyi score de\u011feri bizim k de\u011ferini se\u00e7memiz i\u00e7in yeterli olmuyor. Bu nedenle ger\u00e7ekten deneyerek buluyoruz :)\n\n\n**Notlar**\n#1) Data scientists usually choose as an odd number if the number of classes is 2 and another simple approach to select k is set k=sqrt(n).\n#2)K value should be odd\n#3)K value must not be multiples of the number of classes\n#4)Should not be too small or too large","f3197885":"Concatenate Data Frames","741e719e":"Only I choose 0 and 1 class. Because of logistic regression used for binary classes.","5b48b1b7":"Add Datasets","24306af4":"Now,Dataframe is ready for EDA and Data Cleaning.(Outlier,Nan values,Duplicates,Normalization)","59edf9fe":"**Sonu\u00e7**\n#Random_state(yani randomlugu) de\u011fi\u015ftirince de accuracy de\u011fi\u015fiyor. Mesela 1 yap\u0131nca %95 oluyor.\n#K de\u011feri de\u011fi\u015fince de accuracy de\u011fi\u015fiyor.\n#test_size de\u011fi\u015fince de accuracy de\u011fi\u015fiyor.","58c8b19a":"**KNN Algorithms**","bd553926":"Bu normalizasyonsuz deneme!","960ba6cd":"There are no duplicate rows.\n\nIt is not neccessary duplicate process for this data. Because dataset form from numeric datas.","4457f4cd":"There is not any missing data(nan values). All of datas is null."}}