{"cell_type":{"4468efc6":"code","a8876f8d":"code","50a6f074":"code","3644cf83":"code","b236bba6":"code","41f2b8e9":"code","a5adb435":"code","9ea832bb":"code","1d635b99":"code","1ddcc8bf":"code","0fe50f92":"code","88ca8bc8":"code","6c9a87a2":"code","28eaa2e4":"code","35792326":"code","f58fac05":"code","47a1657e":"code","3c057aba":"code","4774bcd9":"code","564f6a9a":"code","35816462":"code","39862342":"code","2777467b":"code","40a9391f":"markdown","b8ac46df":"markdown","817ddd00":"markdown","fc0a3c51":"markdown","09056ae7":"markdown","84d1ef61":"markdown","e017f8c9":"markdown","4f78eb5e":"markdown","bfad0870":"markdown","41288cd7":"markdown","4881c5fa":"markdown","2fc95e2f":"markdown","d8ee3fbb":"markdown","fac95ebd":"markdown","dcd9de39":"markdown","2f22e183":"markdown","b3b52106":"markdown","b7405385":"markdown","d10d0e4c":"markdown","abf2e55e":"markdown","e49a1750":"markdown","a8039098":"markdown","7f9e1d8b":"markdown","26d4e49e":"markdown","2507ac52":"markdown","982e8850":"markdown","2f666cda":"markdown","e61f6cbd":"markdown"},"source":{"4468efc6":"from pathlib import Path # for creating paths to the data files\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # visualization","a8876f8d":"# path to folder with the data\nDATA_DIR = Path(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\")","50a6f074":"train_raw = pd.read_csv(DATA_DIR \/ \"train.csv\", index_col = \"Id\")\ntrain_raw.head()","3644cf83":"train_raw.info()","b236bba6":"# missing data\nsns.heatmap(data = train_raw.isnull(), cbar = False)","41f2b8e9":"# most columns do not have missing data. Which do?\nmissing_counts = train_raw.isnull().sum()\nmissing_cols = missing_counts[missing_counts > 0]\nmissing_cols","a5adb435":"sns.regplot(x = \"YearBuilt\", y = \"SalePrice\", data = train_raw)","9ea832bb":"sns.displot(x = \"SalePrice\", data = train_raw)","1d635b99":"train_y = train_raw[\"SalePrice\"]\ntrain_y","1ddcc8bf":"train_X = train_raw.drop(\"SalePrice\", axis = 1)\ntrain_X.head()","0fe50f92":"# split categorical and continuous columns\ntrain_X_cat = train_X.select_dtypes(include=['object']).copy()\ntrain_X_cont = train_X.select_dtypes(exclude=['object']).copy()\n\n# fill missing continuous data\ncont_means = train_X_cont.mean()\ntrain_X_cont = train_X_cont.fillna(cont_means)\n\n#fill missing categorical data\ncat_modal = train_X_cat.mode().squeeze() # get most common value (mode) and convert to series\ntrain_X_cat = train_X_cat.fillna(cat_modal)\n\n# convert categorical to numeric\ntrain_X_cat = train_X_cat.astype(\"category\").apply(\n    lambda x: x.cat.codes\n)\n\n# join the two back up:\ntrain_X_transformed = train_X_cont.join(train_X_cat)","88ca8bc8":"from sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor()\nrf.fit(train_X_transformed, train_y)","6c9a87a2":"rf.feature_importances_","28eaa2e4":"named_importances = zip(train_X.columns, rf.feature_importances_)\n\nfor n,i in sorted(named_importances, key = lambda x: x[1], reverse = True):\n    print(n, \"=\", i)","35792326":"from sklearn.model_selection import cross_val_score","f58fac05":"rf1 = RandomForestRegressor()\nscores = cross_val_score(rf1, train_X_transformed, train_y, cv=5)\nprint(\"%0.2f coefficient of determination (R-squared) with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))","47a1657e":"rf2 = RandomForestRegressor(n_estimators = 20, max_depth = 3)\nscores = cross_val_score(rf2, train_X_transformed, train_y, cv=5)\nprint(\"%0.2f coefficient of determination (R-squared) with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))","3c057aba":"# rf3 = RandomForestRegressor(n_estimators = 1000)\n# scores = cross_val_score(rf3, train_X_transformed, train_y, cv=5)\n# print(\"%0.2f coefficient of determination (R-squared) with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))","4774bcd9":"rf_best = RandomForestRegressor(\n    n_estimators=200\n)\nrf_best.fit(train_X_transformed, train_y)","564f6a9a":"test_raw = pd.read_csv(DATA_DIR \/ \"test.csv\", index_col = \"Id\")\ntest_raw.head()","35816462":"# split categorical and continuous columns\ntest_X_cat = test_raw.select_dtypes(include=['object']).copy()\ntest_X_cont = test_raw.select_dtypes(exclude=['object']).copy()\n\n# fill missing continuous data\ntest_X_cont = test_X_cont.fillna(cont_means)\n\n#fill missing categorical data\ntest_X_cat = test_X_cat.fillna(cat_modal)\n\n# convert categorical to numeric\ntest_X_cat = test_X_cat.astype(\"category\").apply(\n    lambda x: x.cat.codes\n)\n\n# join the two back up:\ntest_X_transformed = test_X_cont.join(test_X_cat)","39862342":"test_y = rf_best.predict(test_X_transformed)\ntest_X_transformed[\"SalePrice\"] = test_y","2777467b":"test_X_transformed.reset_index()[[\"Id\", \"SalePrice\"]].to_csv(\"submissions.csv\", index=False)","40a9391f":"Divide features into categorical and continuous","b8ac46df":"Another useful feature of a random forest model is it can tell us which variables were most important in making the trees:","817ddd00":"Then we need to preprocess the test set using exactly the same steps that we used for the training set. \n\nIn our case, that means just repeating the code to impute missing values:","fc0a3c51":"# Make predictions on test data","09056ae7":"We can use decision trees (i.e. a flow chart) to predict the slope and intercept of a regression model:\n\n![](https:\/\/dinhanhthi.com\/img\/post\/ML\/random-forest-decision-tree\/r2.jpg)","84d1ef61":"At this point we would normally remove part of the dataset to be our \"test\" set (for evaluating our final model). However, because this is a competition, Kaggle has already done this for us, and stored that test set in a separate file (which we will load later).","e017f8c9":"# Impute Missing Data","4f78eb5e":"Finally we can predict `SalePrice` for the test data using our best model.","bfad0870":"Different parameters that you can set for the RandomForestRegressor estimator are listed on the documentation page: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html","41288cd7":"If you create a notebook associated with a competition then the relevant data will be automatically attached. If not, you can always attach data manually.","4881c5fa":"First we need to load the file of test data:","2fc95e2f":"# Load data","d8ee3fbb":"Then instead of using the `fit()` method on the full dataset, we pass the model object to the `cross_val_score` function, and use the parameter `cv` to state how many times to split up the data.","fac95ebd":"# Fit a Random Forest model","dcd9de39":"# Data preprocessing","2f22e183":"### Your turn...\n\nMake a graph (or several) to explore some of the variation in the data.\n\nYou might find the following tips helpful:\n* A scatter plot can be created with the `sns.scatterplot(...)` function. At a minimum you will need to supply arguments to the `x`, `y`, and `data` parameters.\n    * If you wish to add a linear model regression line to your scatter plot, then try the `sns.regplot(...)` function instead.\n* A histogram can be created with the `sns.displot(...)` function. At a minimum you will need to supply arguments to the `x` and `data` parameters.\n\nFor example:","b3b52106":"# Exploratory Data Analysis","b7405385":"# The missing step: Cross-validation\n\nBut how good is this particular model? And well does it make predictions compared to a different model?\n\nFor that we need to evaluate the model on the dataset. Unfortunately we don't want to measure the model's performance on the same data that we used to create the model in the first place.\n\nWe will use \"cross-validation\" to repeatedly build the model on part of the training data and then evaluate it on the part that we left out:","d10d0e4c":"Make sure to use the same parameter settings here that you used in the model that did best in cross validation...","abf2e55e":"A common issue with datasets is that they will be missing some data. Unfortunately we cannot create a predictive model if some data is missing.","e49a1750":"The model from a single tree may look quite jagged...\n\n![](https:\/\/miro.medium.com\/max\/691\/1*Edq1ZIOo26Iqp3CuUTWotA.png)","a8039098":"To make predictions to the Kaggle competition from this notebook, we need to save the output dataframe to a file called `submissions.csv`.","7f9e1d8b":"This initial code loads the libraries we will use.","26d4e49e":"Now save a version of this notebook, which will re-run all the code. Then go to the Viewer page for this notebook, navigate to the data tab, and make your submission.","2507ac52":"## Going further\n\nTo learn more about machine learning techniques:\n\n* Book: Hands-On Machine Learning with Scikit-Learn, Keras, and Tensorflow (2nd edition) by Aur\u00e9lien G\u00e9ron - read online for free at O'Reilly\/Safari Books by creating an account through the University Library: https:\/\/infoguides.gmu.edu\/ebooks\/collections\n\nTo learn more about competing in Kaggle competitions:\n\n* Free Coursera course: How to win a data science competition - https:\/\/www.coursera.org\/learn\/competitive-data-science\n\nSuggestions for things to try with this competition:\n* Look at the tutorials for this competition: https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/overview\/tutorials\n* Feature selection, i.e. you might get a better model by using fewer features.\n* Feature engineering, i.e. you might get a better model by recombining some features into new features.\n* Use different types of regression models (see the references above for good suggestions)","982e8850":"# Now go back and fit a Random Forest model on the full dataset with the best parameters","2f666cda":"...but we can take the average of many randomly different trees (i.e. a \"random forest\" of decision trees) to smooth this line out.","e61f6cbd":"We can then make a different model with different parameters for the Random Forest algorithm and compare cross-validation scores:"}}