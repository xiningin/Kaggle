{"cell_type":{"5204f0c9":"code","de1d06ab":"code","38f68d2c":"code","8aaa60ff":"code","3e38d2f3":"code","181525c0":"code","1649b3a3":"code","787a48f1":"code","adb10b8f":"code","9dd1f3da":"code","01698db3":"code","1cba01ee":"code","3c7240c6":"code","2dfc3598":"code","cf2dad89":"code","4931a79d":"code","c046f062":"code","925649e4":"code","03c2380a":"code","a27f4005":"code","def297c9":"code","be7d6f9d":"code","a300ecff":"code","a937c229":"code","7e0e643b":"code","813ab5c7":"code","3080d4db":"code","d998baa9":"code","1fc139c4":"code","1c581bb9":"code","7ce1fcee":"code","8f307382":"code","5a9296bb":"code","3d6c7ffc":"code","8ad5aa5a":"code","517bcc06":"code","03797c3b":"code","1f847552":"code","4ef843e4":"code","03926f02":"code","13414f61":"code","75ba688f":"code","2142debd":"code","e232ffa0":"code","201257b7":"markdown","5d5915a8":"markdown","364ed780":"markdown","4a24c366":"markdown","96815fad":"markdown","97ba0a5f":"markdown","dcd6eea8":"markdown","7e3dc5aa":"markdown","12ff13d0":"markdown","fb0c1ed0":"markdown","0fe74f58":"markdown","9737ae27":"markdown","7d6f1e6b":"markdown"},"source":{"5204f0c9":"#https:\/\/www.kaggle.com\/cdeotte\/high-scoring-lgbm-malware-0-702-0-775\n#https:\/\/www.kaggle.com\/fabiendaniel\/detecting-malwares-with-lgbm\n#https:\/\/www.kaggle.com\/humananalog\/xgboost-lasso\n#https:\/\/www.kaggle.com\/ogrellier\/good-fun-with-ligthgbm\n#https:\/\/www.kaggle.com\/mlisovyi\/modular-good-fun-with-ligthgbm\/output","de1d06ab":"\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport gc\nfrom tqdm import tqdm","38f68d2c":"kaggle=1\n\nif kaggle==0:\n    train=pd.read_csv(\"train.csv\")\n    test=pd.read_csv(\"test.csv\")\n    sample_submission=pd.read_csv(\"sample_submission.csv\")\n    \nelse:\n    train=pd.read_csv(\"..\/input\/cat-in-the-dat\/train.csv\")\n    test=pd.read_csv(\"..\/input\/cat-in-the-dat\/test.csv\")\n    sample_submission=pd.read_csv(\"..\/input\/cat-in-the-dat\/sample_submission.csv\")","8aaa60ff":"train.head()","3e38d2f3":"train.shape,test.shape","181525c0":"train['target'].value_counts()","1649b3a3":"train.dtypes","787a48f1":"test.dtypes","adb10b8f":"#convert all the columns to category datatype:\nfor f in train.columns:\n    if f==\"id\" or f==\"target\": continue\n    print(f'Converting {f} into category datatype\\n')\n    train[f]=train[f].astype('category')\n    test[f]=test[f].astype('category')","9dd1f3da":"## For binary columns , the cardinality will be 2.Lets separate them out .\nbinary_columns=[c for c in train.columns if train[c].nunique()==2]","01698db3":"binary_columns","1cba01ee":"categorical_columns=[c for c in train.columns if (c not in binary_columns)]","3c7240c6":"cardinality=[]\nfor c in categorical_columns:\n    if c=='id':continue\n    cardinality.append([c,train[c].nunique()])\ncardinality.sort(key=lambda x:x[1],reverse=True)\n","2dfc3598":"cardinality","cf2dad89":"# Columns that can be safely label encoded\ngood_label_cols = [col for col in categorical_columns if \n                   set(train[col]) == set(test[col])]","4931a79d":"## from https:\/\/www.kaggle.com\/fabiendaniel\/detecting-malwares-with-lgbm\ndef frequency_encoding(variable):\n    t = pd.concat([train[variable], test[variable]]).value_counts().reset_index()\n    t = t.reset_index()\n    t.loc[t[variable] == 1, 'level_0'] = np.nan\n    t.set_index('index', inplace=True)\n    max_label = t['level_0'].max() + 1\n    t.fillna(max_label, inplace=True)\n    return t.to_dict()['level_0']","c046f062":"#frequency_encoded_columns=['nom_9','nom_8','nom_7','nom_6','nom_5','ord_5','ord_4']","925649e4":"for variable in tqdm(good_label_cols):\n    freq_encod_dict=frequency_encoding(variable)\n    train[variable+'_FE']=train[variable].map(lambda x:freq_encod_dict.get(x,np.nan))\n    test[variable+'_FE']=test[variable].map(lambda x:freq_encod_dict.get(x,np.nan))\n    categorical_columns.remove(variable)","03c2380a":"#https:\/\/www.kaggle.com\/vprokopev\/mean-likelihood-encodings-a-comprehensive-study\n\ndef factorize(train, test, features, na_value=-9999, full=False, sort=True):\n    \"\"\"Factorize categorical features.\n    Parameters\n    ----------\n    train : pd.DataFrame\n    test : pd.DataFrame\n    features : list\n           Column names in the DataFrame to be encoded.\n    na_value : int, default -9999\n    full : bool, default False\n        Whether use all columns from train\/test or only from train.\n    sort : bool, default True\n        Sort by values.\n    Returns\n    -------\n    train : pd.DataFrame\n    test : pd.DataFrame\n    \"\"\"\n\n    for column in features:\n        if full:\n            vs = pd.concat([train[column], test[column]])\n            labels, indexer = pd.factorize(vs, sort=sort)\n        else:\n            labels, indexer = pd.factorize(train[column], sort=sort)\n\n        train[column+'_LE'] = indexer.get_indexer(train[column])\n        test[column+'_LE'] = indexer.get_indexer(test[column])\n\n        if na_value != -1:\n            train[column] = train[column].replace(-1, na_value)\n            test[column] = test[column].replace(-1, na_value)\n\n    return train, test","a27f4005":"# indexer = {}\n# for col in tqdm(categorical_columns):\n#     if col == 'id': continue\n#     _, indexer[col] = pd.factorize([train[col],test[col]])","def297c9":"#categorical_columns.remove('id')\ntrain,test=factorize(train,test,categorical_columns,full=True)","be7d6f9d":"#train,test=factorize(train,test,frequency_encoded_columns,full=True)","a300ecff":"# for col in tqdm(categorical_columns):\n#     if col=='id':continue\n#     train[col+'_LE']=indexer[col].get_indexer(train[col])\n#     test[col+'_LE']=indexer[col].get_indexer(test[col])\n    ","a937c229":"binary_columns","7e0e643b":"train_cat_dum=pd.DataFrame()\ntest_cat_dum=pd.DataFrame()\nfor c_ in binary_columns:\n    if c_=='target':continue\n    train_cat_dum=pd.concat([train_cat_dum,pd.get_dummies(train[c_],prefix=c_).astype(np.uint8)],axis=1)\n    test_cat_dum=pd.concat([test_cat_dum,pd.get_dummies(test[c_],prefix=c_).astype(np.uint8)],axis=1)","813ab5c7":"train_cat_dum.head()","3080d4db":"train=pd.concat([train,train_cat_dum],axis=1)\ntest=pd.concat([test,test_cat_dum],axis=1)","d998baa9":"train.head()","1fc139c4":"train.columns,test.columns","1c581bb9":"cols_to_remove=['id', 'bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4', 'nom_0', 'nom_1',\n       'nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9',\n       'ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5', 'day', 'month','id_LE']","7ce1fcee":"train=train.drop(cols_to_remove,axis=1)\ntest=test.drop(cols_to_remove,axis=1)","8f307382":"train.shape","5a9296bb":"test.shape","3d6c7ffc":"## Importing required libraries:\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score","8ad5aa5a":"y=train['target']\ndel train['target']","517bcc06":"n_folds=5","03797c3b":"folds=StratifiedKFold(n_splits=5,shuffle=True,random_state=1234)\nfeats=[f for f in train.columns if f not in ['id']]","1f847552":"oof_preds = np.zeros(train.shape[0])\nsub_preds = np.zeros(test.shape[0])\n    \nfeature_importance_df = pd.DataFrame()\ncategorical_features=[c for c in train.columns if c not in ['id_LE']]","4ef843e4":"# param = {'num_leaves': 60,\n#          'min_data_in_leaf': 60, \n#          'objective':'binary',\n#          'max_depth': -1,\n#          'learning_rate': 0.1,\n#          \"boosting\": \"gbdt\",\n#          \"feature_fraction\": 0.8,\n#          \"bagging_freq\": 1,\n#          \"bagging_fraction\": 0.8 ,\n#          \"bagging_seed\": 11,\n#          \"metric\": 'auc',\n#          \"lambda_l1\": 0.1,\n#          \"random_state\": 133,\n#          \"verbosity\": -1}","03926f02":"#params after bayesian optimisation:\n\nparam = {'num_leaves': 31,\n         'min_data_in_leaf': 69, \n         'objective':'binary',\n         'max_depth': 4,\n         'learning_rate': 0.06,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.33,\n         \"metric\": 'auc',\n         \"lambda_l1\": 0.01,\n         \"random_state\": 133,\n         \"verbosity\": -1}","13414f61":"for n_folds,(train_idx,valid_idx) in enumerate(folds.split(train.values,y.values)):\n    print(\"fold n\u00b0{}\".format(n_folds+1))\n    trn_data = lgb.Dataset(train.iloc[train_idx][feats],\n                           label=y.iloc[train_idx],\n                           categorical_feature=categorical_features\n                          )\n    val_data = lgb.Dataset(train.iloc[valid_idx][feats],\n                           label=y.iloc[valid_idx],categorical_feature=categorical_features\n                          )\n\n    num_round = 10000\n    clf = lgb.train(param,\n                    trn_data,\n                    num_round,\n                    valid_sets = [trn_data, val_data],\n                    verbose_eval=100,\n                    early_stopping_rounds = 200)\n    \n    #clf.fit(train_x,train_y,eval_set=[(train_x,train_y),(valid_x,valid_y)],verbose=500,eval_metric=\"auc\",early_stopping_rounds=100)\n    \n    oof_preds[valid_idx]=clf.predict(train.iloc[valid_idx][feats],num_iteration=clf.best_iteration)\n    sub_preds+=clf.predict(test[feats],num_iteration=clf.best_iteration)\/folds.n_splits\n    \n    fold_importance_df=pd.DataFrame()\n    fold_importance_df['features']=feats\n    fold_importance_df['importance']=clf.feature_importance(importance_type='gain')\n    fold_importance_df['folds']=n_folds+1\n    print(f'Fold {n_folds+1}: Most important features are:\\n')\n    for i in np.argsort(fold_importance_df['importance'])[-5:]:\n        print(f'{fold_importance_df.iloc[i,0]}-->{fold_importance_df.iloc[i,1]}')\n    \n    feature_importance_df=pd.concat([feature_importance_df,fold_importance_df],axis=0)\n    \n    print('Fold %2d AUC : %.6f' % (n_folds + 1, roc_auc_score(y.iloc[valid_idx], oof_preds[valid_idx])))\n    del clf\n    gc.collect()\n    \n\n\nprint('Full auc score %.6f' % (roc_auc_score(y,oof_preds)))\n\ntest['target']=sub_preds\n              ","75ba688f":"sample_submission['target']=sub_preds","2142debd":"sample_submission.head()","e232ffa0":"sample_submission.to_csv(\"sample_submission.csv\",index=False)","201257b7":"### Import Necessary libraries:","5d5915a8":"We have been provided with a dataset that only has categorical variables and we are asked to try out different encoding schemes and compare how they perform.The competition is binary classification challenge with only categorical variables to train on.","364ed780":"### Reading the data","4a24c366":"### Building the model","96815fad":"We see that there are 7 columns with high cardinality.Feature encoding for these columns may include frequency encoding which is based on the ranking of categories based on the frequency of occurence in the group.We check if the cols have same levels in both test and train.We encode only those columns.","97ba0a5f":"# Categorical Feature Challenge","dcd6eea8":"We see that the train dataset has 25 categorical columns with varying degree of cardinality.\n\nLet check the distribution of the target value to understand whether the dataset is balanced or not.","7e3dc5aa":"### References","12ff13d0":"Now,we have taken care of all the categorical variables.Lets build the model and with 5 fold cross validation .Before this ,lets delete the original categorical columns.","fb0c1ed0":"We see that the target has lot of 0's than 1's.Its an unbalanced problem.","0fe74f58":"### Label Encoding","9737ae27":"### Cardinality of the columns","7d6f1e6b":"Now we do one hot encoding for all the binary categorical variables."}}