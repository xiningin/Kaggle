{"cell_type":{"35acac34":"code","1da3b586":"code","822cc3f3":"code","7b577145":"code","dcd9bd06":"code","7ca3aa7a":"code","a274ebcd":"code","9c293672":"code","6e1cc306":"code","08ce7315":"code","f0cee523":"code","ae5ed9e8":"code","a6b3c935":"code","911b4a08":"code","078063ef":"code","5911c491":"code","77bf3079":"code","71a636c6":"code","c9409174":"code","20064076":"code","d1537281":"code","5fca3a24":"code","38500820":"code","476199ee":"markdown","9e1f9c32":"markdown","ccce90d7":"markdown","eeb4066a":"markdown","b8925552":"markdown","3ffcf60e":"markdown","592e8956":"markdown","cab9817f":"markdown","3583d890":"markdown","4d9479f3":"markdown","3dd4a5a2":"markdown","a88a2c71":"markdown","9aa86f3c":"markdown","e4d60ad0":"markdown"},"source":{"35acac34":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1da3b586":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","822cc3f3":"dataset = pd.read_csv('\/kaggle\/input\/iris-flower-dataset\/IRIS.csv')","7b577145":"dataset.info()","dcd9bd06":"dataset.isnull().sum()","7ca3aa7a":"sns.countplot(x= 'species', data= dataset)","a274ebcd":"dataset.species.value_counts()","9c293672":"sns.heatmap(data= dataset.corr(), annot= True)","6e1cc306":"x = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values","08ce7315":"x","f0cee523":"y","ae5ed9e8":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state= 0)","a6b3c935":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","911b4a08":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier","078063ef":"from sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import cross_val_score","5911c491":"models = []\nmodels.append(['Logistic Regreesion', LogisticRegression(random_state=0)])\nmodels.append(['SVM', SVC(random_state=0)])\nmodels.append(['KNeighbors', KNeighborsClassifier()])\nmodels.append(['Naive Bayes', GaussianNB()])\nmodels.append(['Decision Tree', DecisionTreeClassifier(random_state=0)])\nmodels.append(['Random Forest', RandomForestClassifier(random_state=0)])\nmodels.append(['XGBoost', XGBClassifier()])\n\nlst_1= []\n\nfor m in range(len(models)):\n  lst_2= []\n  model = models[m][1]\n  model.fit(x_train, y_train)\n  y_pred = model.predict(x_test)\n  cm = confusion_matrix(y_test, y_pred)\n  accuracies = cross_val_score(estimator = model, X = x_train, y = y_train, cv = 10)\n  print(models[m][0])\n  print(cm)\n  print('Accuracy Score',accuracy_score(y_test, y_pred))\n  print('')\n  print(\"Mean Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n  print('')\n  print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n  print('')\n  print('-----------------------------------')\n  print('')\n  lst_2.append(models[m][0])\n  lst_2.append((accuracy_score(y_test, y_pred))*100) \n  lst_2.append(accuracies.mean()*100)\n  lst_2.append(accuracies.std()*100)\n  lst_1.append(lst_2)","77bf3079":"df = pd.DataFrame(lst_1, columns= ['Model', 'Accuracy', 'Mean Accuracy', 'Std. Deviation'])","71a636c6":"df.sort_values(by= ['Accuracy', 'Mean Accuracy'], inplace= True, ascending= False)","c9409174":"df","20064076":"lr = LogisticRegression()\nsvm = SVC()\nknn = KNeighborsClassifier()","d1537281":"data = [(lr, [{'C': [0.01, 0.1, 0.5, 1.0], 'random_state':[0]}]),\n        (svm, [{'C': [0.1, 0.5, 1.0], 'kernel': ['linear', 'rbf'], 'random_state':[0]}]),\n        (knn, [{'n_neighbors': [2, 3, 5, 7, 10], 'metric': ['euclidean', 'manhattan', 'chebyshev', 'minkowski']}])]","5fca3a24":"from sklearn.model_selection import GridSearchCV","38500820":"for i,j in data:\n  grid = GridSearchCV(estimator = i , param_grid = j , scoring = 'accuracy',cv = 10)\n  grid.fit(x_train,y_train)\n  best_accuracy = grid.best_score_\n  best_parameters = grid.best_params_\n  print('{} \\nBestAccuracy : {:.2f}%'.format(i,best_accuracy*100))\n  print('BestParameters : ',best_parameters)\n  print('')","476199ee":"**Making Data Frame.**","9e1f9c32":"# **Data Preprocessing**","ccce90d7":"# **Heat Map Correlation**","eeb4066a":"# **Importing Libraries**","b8925552":"# **Count Plot for species in Iris**","3ffcf60e":"# **Importing Dataset**","592e8956":"**Therefore, after applying GridSearch we can confirm that LogisticRegression and KNeighborsClassifier is best suited model for the dataset and both gives the accuracy of 95.00%.**","cab9817f":"**Below shows the values of models in Descending Order.**","3583d890":"**Checking if there are any NULL values.**","4d9479f3":"# **Summary**","3dd4a5a2":"**Applying Grid Search on Top 3 above models for best parameters and model selection.**\n\n1. Logistic Regression\n2. SVM\n3. KNeighbors","a88a2c71":"# **Splitting dataset into Train and Test set**","9aa86f3c":"# **Selection of Models**","e4d60ad0":"# **Feature Scaling**"}}