{"cell_type":{"9e81cda2":"code","25d08282":"code","337e2303":"code","34c100f6":"code","51b44b62":"code","d5d4bccb":"code","9cc28562":"code","3b44d0c5":"code","b947491a":"code","b510f0cd":"code","6a911553":"code","d7c84fa4":"code","ad8125f2":"code","34a12fce":"code","4141446b":"code","d8052a91":"code","d7e05b56":"code","23043c09":"code","9486438c":"code","23d63d95":"code","f7343dc7":"code","bb293eae":"code","f086118a":"code","75c69bf5":"code","26f39e6d":"code","c3f78a17":"code","3da3c9e4":"code","7bc01fd9":"code","daa50fe1":"code","876513b3":"code","4f11e9e5":"code","2e9cb6ad":"code","dae467c3":"code","d31217ec":"code","8969b2b3":"code","7c6a6b63":"code","0043079c":"code","3406bacd":"code","7d8ab659":"code","7d5fdf7a":"code","ad8e17ab":"code","6504dcce":"code","f764839d":"code","4280328e":"code","d8e76fe2":"code","df42ee0f":"code","96c2f1b6":"code","e1c6ac7e":"code","81688785":"code","a400c44d":"code","2d754526":"code","87ee3dd2":"code","fca0326c":"code","e45634a8":"markdown","5d703ee3":"markdown","541cabc8":"markdown","23a8b17d":"markdown","d1a67f1d":"markdown","04562d3a":"markdown","fe6831ac":"markdown","3f8c911f":"markdown","c70336e4":"markdown","bd49799f":"markdown","2bc09dea":"markdown","f7210ab6":"markdown","3ab585f6":"markdown","6cbc617c":"markdown","53863839":"markdown","22ca6400":"markdown","d266a4a3":"markdown","abd44752":"markdown","9031cd05":"markdown","185b3277":"markdown","fde34501":"markdown","6e19cc0c":"markdown","35837639":"markdown","913f45bb":"markdown","0bb210f2":"markdown","4483fb4d":"markdown","d2ec5b8e":"markdown","f0f6043e":"markdown","8286082d":"markdown","dd94b1be":"markdown","f92ca226":"markdown","248da9fb":"markdown","cf215a87":"markdown","f4f43e38":"markdown","cf6a0167":"markdown","54f637ee":"markdown","2316d68c":"markdown"},"source":{"9e81cda2":"from IPython.display import Image\nImage(\"..\/input\/heartpredictionimage\/HeartAttackPrediction_Image.png\")","25d08282":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas_profiling as pp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport time\n\n%matplotlib inline\nwarnings.filterwarnings('ignore')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","337e2303":"o2sat = pd.read_csv(\"..\/input\/heart-attack-analysis-prediction-dataset\/o2Saturation.csv\")\no2sat.head()","34c100f6":"o2sat.mean()","51b44b62":"o2sat.value_counts()","d5d4bccb":"o2sat.describe()","9cc28562":"plt.figure(figsize=(14,6))\nsns.histplot(data=o2sat['98.6'])","3b44d0c5":"heart = pd.read_csv (\"..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")\nheart.head()","b947491a":"pp.ProfileReport(heart, explorative = True)","b510f0cd":"heart.shape # Get quick snapshot of number of rows and features.","6a911553":"heart[heart.duplicated()] #understand which row is duplicate","d7c84fa4":"df_processed = heart.copy()\n\ndf_processed.drop_duplicates(inplace = True)\ndf_processed.reset_index(drop = True, inplace = True)\ndf_processed.shape","ad8125f2":"df_processed[df_processed.duplicated()]","34a12fce":"df_processed.hist(figsize=(18,10))\nplt.show()","4141446b":"plt.figure(figsize=(16,8))\nsns.heatmap(df_processed.corr(),annot=True,cmap=\"PuBuGn\")","d8052a91":"df_processed.sex.value_counts(normalize=True)","d7e05b56":"df_processed.age.hist(figsize=(16,8),bins=30)","23043c09":"def age_category_values(df):\n    p=round(df.max()\/5)*5\n    q=round(df.min()\/5)*5\n    L=[i for i in range(q,p,5)]\n    dicts={}\n    M=[]\n    for a in range(len(L)):\n        dicts[L[a]]=0\n    for j in df:\n        for k in L:\n            if j<k:\n                dicts[k]+=1\n                break\n    for b in dicts:\n        M.append(([b-5,b],dicts[b]))\n    return M\n\nage_category_values(df_processed.age)","9486438c":"s=0\nfor i in df_processed.age:\n    if 40 <= i:\n        s+=1\nx = len(df_processed)\nprint(100*s\/x)","23d63d95":"df_processed.cp.value_counts()","f7343dc7":"df_processed.cp.hist()","bb293eae":"df_processed.cp.value_counts(normalize=True)","f086118a":"df_processed.trtbps.hist()","75c69bf5":"def category_values(df, step):\n    p = round(df.max()\/step)*step\n    q = round(df.min()\/step)*step\n    L=[i for i in range(q,p+(2*step),step)]\n    dicts={}\n    M=[]\n    for a in range(len(L)):\n        dicts[L[a]]=0\n    for j in df:\n        for k in L:\n            if j<k:\n                dicts[k]+=1\n                break\n    for b in dicts:\n        M.append(([b-step,b],dicts[b]))\n    return M\n\ncategory_values(df_processed.trtbps,10)","26f39e6d":"sns.histplot(data=df_processed,x=\"trtbps\", bins=(80,90,100,110,120,130,140,150,160,170,180,190,200))","c3f78a17":"df_processed.info()","3da3c9e4":"df1_processed = pd.get_dummies(df_processed, columns = ['sex', 'cp', 'fbs', 'restecg', 'exng', 'slp', 'caa', 'thall'])","7bc01fd9":"df1_processed.head()","daa50fe1":"from sklearn.preprocessing import StandardScaler\nstandardScaler = StandardScaler()\n\ncolumns_for_scaling = ['age', 'trtbps', 'chol', 'thalachh', 'oldpeak']\n\ndf1_processed[columns_for_scaling] = standardScaler.fit_transform(df1_processed[columns_for_scaling])","876513b3":"df1_processed.head()","4f11e9e5":"from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n\nfrom sklearn.linear_model import LogisticRegression     # Logistic Regression\nfrom sklearn.neighbors import KNeighborsClassifier      # KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier     # Random Forest\nfrom sklearn.ensemble import GradientBoostingClassifier # GBM\nimport xgboost as xgb\nfrom xgboost import XGBClassifier                       # XGBoost\nfrom lightgbm import LGBMClassifier                     # Light GBM\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve\nfrom sklearn.model_selection import ShuffleSplit, GridSearchCV\n#from sklearn.metrics import mean_squared_error, r2_score","2e9cb6ad":"SEED = 124\nx = df1_processed.drop(\"output\",axis=1)\ntarget = df1_processed[\"output\"]\n\nx_train,x_test,y_train,y_test = train_test_split(x,target,test_size=0.25,random_state = SEED)","dae467c3":"x_train.shape","d31217ec":"x_test.shape","8969b2b3":"# For us, x --> Independent Feature Set, target --> Target feature\n# We will try with 10 fold Cross Validation of dataset\n\nknn_scores = []\nfor k in range(1,21): \n    knn_classifier = KNeighborsClassifier(n_neighbors = k)\n    score = cross_val_score(knn_classifier,x,target,cv=10)\n    knn_scores.append(score.mean())","7c6a6b63":"plt.plot([k for k in range(1, 21)], knn_scores, color = 'blue')\nfor i in range(1,21):\n    plt.text(i, round(knn_scores[i-1],3), (i, round(knn_scores[i-1],2)))\nplt.xticks([i for i in range(1, 21)])\nplt.xlabel('Number of Neighbors (K)')\nplt.ylabel('Scores')\nplt.title('K Neighbors Classifier scores for different K values')","0043079c":"# We are trying to execute with the best k value found above\nknn_classifier = KNeighborsClassifier(n_neighbors = 10)\nscore = cross_val_score(knn_classifier,x,target,cv=10)","3406bacd":"score.mean() ## Accuracy score from kNN","7d8ab659":"rf_classifier= RandomForestClassifier(n_estimators = 10)\n\nscore = cross_val_score(rf_classifier,x,target,cv=10)","7d5fdf7a":"score.mean() ## Accuracy score from RF","ad8e17ab":"lgbm_classifier = LGBMClassifier()\n\nscore = cross_val_score(lgbm_classifier,x,target,cv=5)\n","6504dcce":"score.mean() ## Accuracy score from LightGBM","f764839d":"def print_score(classifier, x_train, y_train, x_test, y_test, train=True):\n    if train:\n        pred = classifier.predict(x_train)\n        classifier_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n        print(\"Train Result:\\n--------------------------------------------\")\n        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"Classification Report:\\n{classifier_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n        \n    elif train==False:\n        pred = classifier.predict(x_test)\n        classifier_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n        print(\"Test Result:\\n---------------------------------------------\")        \n        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"Classification Report:\\n{classifier_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")","4280328e":"x_train.head()","d8e76fe2":"y_train.head()","df42ee0f":"knn_classifier = KNeighborsClassifier(n_neighbors=20)\nknn_classifier.fit(x_train, y_train)\n\nprint_score(knn_classifier, x_train, y_train, x_test, y_test, train=True)\nprint_score(knn_classifier, x_train, y_train, x_test, y_test, train=False)","96c2f1b6":"test_score = accuracy_score(y_test, knn_classifier.predict(x_test)) * 100\ntrain_score = accuracy_score(y_train, knn_classifier.predict(x_train)) * 100\n\nresults_df = pd.DataFrame(data=[[\"Tuned k-Nearest Neighbors\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nresults_df","e1c6ac7e":"from sklearn.model_selection import GridSearchCV\n\nparams = {\"C\": np.logspace(-4, 4, 20),\n          \"solver\": [\"liblinear\"]}\n\nlr_classifier = LogisticRegression()\n\nlr_cv = GridSearchCV(lr_classifier, params, scoring=\"accuracy\", n_jobs=-1, verbose=1, cv=5)\nlr_cv.fit(x_train, y_train)\nbest_params = lr_cv.best_params_\nprint(f\"Best parameters: {best_params}\")\nlr_classifier = LogisticRegression(**best_params)\n\nlr_classifier.fit(x_train, y_train)\n\nprint_score(lr_classifier, x_train, y_train, x_test, y_test, train=True)\nprint_score(lr_classifier, x_train, y_train, x_test, y_test, train=False)","81688785":"test_score = accuracy_score(y_test, lr_classifier.predict(x_test)) * 100\ntrain_score = accuracy_score(y_train, lr_classifier.predict(x_train)) * 100\n\ntuning_results_df = pd.DataFrame(data=[[\"Tuned Logistic Regression\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\n\nz = results_df.append(tuning_results_df, ignore_index=True)\nz","a400c44d":"n_estimators = [100]\nmax_features = ['auto', 'sqrt']\nmax_depth = [5]\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\n\nparams_rf = {'n_estimators': n_estimators, 'max_features': max_features,\n               'max_depth': max_depth, 'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap}\n\nrf_classifier = RandomForestClassifier(random_state = SEED)\n\nrf_cv = GridSearchCV(rf_classifier, params_rf, scoring=\"accuracy\", cv=3, verbose=2, n_jobs=-1)\n\n\nrf_cv.fit(x_train, y_train)\nbest_params = rf_cv.best_params_\nprint(f\"Best parameters: {best_params}\")\n\nrf_classifier = RandomForestClassifier(**best_params)\nrf_classifier.fit(x_train, y_train)\n\nprint_score(rf_classifier, x_train, y_train, x_test, y_test, train=True)\nprint_score(rf_classifier, x_train, y_train, x_test, y_test, train=False)","2d754526":"test_score = accuracy_score(y_test, rf_classifier.predict(x_test)) * 100\ntrain_score = accuracy_score(y_train, rf_classifier.predict(x_train)) * 100\n\nresults_df = pd.DataFrame(data=[[\"Tuned Random Forest Classifier\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nz = z.append(results_df, ignore_index=True)\nz","87ee3dd2":"n_estimators = [100]\nmax_depth = [2, 3, 5]\nbooster = ['gbtree', 'gblinear']\nbase_score = [0.99]\nlearning_rate = [0.05]\nmin_child_weight = [1, 2, 3]\n\nparams = {\n    'n_estimators': n_estimators, 'max_depth': max_depth,\n    'learning_rate' : learning_rate, 'min_child_weight' : min_child_weight, \n    'booster' : booster, 'base_score' : base_score\n                      }\n\nxgb_classifier = XGBClassifier()\n\nxgb_cv = GridSearchCV(xgb_classifier, params, cv=3, scoring = 'accuracy',n_jobs =-1, verbose=1)\n\n\nxgb_cv.fit(x_train, y_train)\nbest_params = xgb_cv.best_params_\nprint(f\"Best paramters: {best_params}\")\n\nxgb_classifier = XGBClassifier(**best_params)\nxgb_classifier.fit(x_train, y_train)\n\nprint_score(xgb_classifier, x_train, y_train, x_test, y_test, train=True)\nprint_score(xgb_classifier, x_train, y_train, x_test, y_test, train=False)","fca0326c":"test_score = accuracy_score(y_test, xgb_classifier.predict(x_test)) * 100\ntrain_score = accuracy_score(y_train, xgb_classifier.predict(x_train)) * 100\n\nresults_df = pd.DataFrame(data=[[\"Tuned XGBoost Classifier\", train_score, test_score]], \n                          columns=['Model', 'Training Accuracy %', 'Testing Accuracy %'])\nz = z.append(results_df, ignore_index=True)\nz","e45634a8":"# 6. Conclusion \/ Interpretations:\n\n* Since our objective is to predict whether a person is prone to heart attack or not based on the dataset and information available, we have approached it accordingly and explored with initial data analysis followed by feature engineering and few methods.\n* We analyzed few algorithms and compared with their accuracy percentage points. Both training and testing p.p are compared to just get a feel of how they are performing (though we will be only interested in the testing accuracy p.p.)\n* We will further experiment more with additional feature engineering and models to be analyzed with various options to see what works better and why.\n* In any business problem solving, we will have to see data and context\/need and then only can state which algorithm will perform better given the scenario. Time is also important and we will have to consider trade off between time and optimum solution accordingly.\n* More effort will always be towards EDA and Feature Engineering which are important.","5d703ee3":"# 5.4 Hyperparameter Tuning for XGBoost","541cabc8":"# 3. DATA PREPARATION ","23a8b17d":"# 3c. Analysis on \"Sex\" feature","d1a67f1d":"# Processing with Dummy variables\n\n### We observed that we need to convert some categorical variables into dummy variables and scale all the values before training the Machine Learning models. With regards to this, we will use the get_dummies method to create dummy columns for categorical variables.","04562d3a":"# 5.1 Hyperparameter Tuning for kNN","fe6831ac":"# 2. DATA UNDERSTANDING\n\n#### There are 2 files provided as inputs.\n* o2saturation.csv\n* heart.csv\n\n#### Description of dataset features are captured below.\n\n* age : Age of the patient\n* sex : Sex of the patient\n    * 1: Male\n    * 0: Female\n* cp : Chest Pain type\n    * Value 0: typical angina\n    * Value 1: atypical angina\n    * Value 2: non-anginal pain\n    * Value 3: asymptomatic\n* trtbps : resting blood pressure (in mm Hg)\n* chol : cholestoral in mg\/dl fetched via BMI sensor\n* fbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n    * 1: True (i.e. Fasting Blood Sugar > 120mg\/dl)\n    * 0: False (i.e. Fasting Blood Sugar < 120mg\/dl)\n* rest_ecg : resting electrocardiographic results\n    * Value 0: normal\n    * Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n    * Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n* thalach : maximum heart rate achieved\n* exng: exercise induced angina (1 = yes; 0 = no)\n* oldpeak: Previous peak\n* slp: the slope of the peak exercise ST segment \n* caa: number of major vessels (0-4) - 0\/1\/2\/3\/4\n* thall: thallium stress result - 0\/1\/2\/3 etc\n* output: (This is the TARGET variable)\n    * 0= less chance of heart attack \n    * 1= more chance of heart attack \n","3f8c911f":"Overall, not much high correlation between variables. \n\nOutput (Target variable) - is correlated more relatively with cp, thalachh, slp (positively) and exng, oldpeak, caa (negatively).\n","c70336e4":"#### The tuned model with Training and Testing Accuracy percetage points are captured above.\n\n#### We will continue to explore further with multiple experiments.","bd49799f":"# 4. MODEL DEVELOPMENT","2bc09dea":"# 1. BUSINESS UNDERSTANDING\n\n### Objective: Predict whether a person is prone to heart attack or not based on the information available.\n\n* a) Perform Exploratory Data Analysis on the information \/ dataset available to gather insights around it. \n* b) Additionally, perform predict if a person is prone to heart attack or not.","f7210ab6":"* Male ~ 68.2%\n* Female ~ 31.8%","3ab585f6":"# 3b. Correlation using Histogram","6cbc617c":"This is just an example of one of AutoEDA (Automated Exploratory Data Analysis) considered for quick insights \/ analysis. We can always consider any such methods \/ approaches which we can use and see if it helps in our current context. For example other AutoEDA libraries could be - SweetViz, LUX, AutoViz, DataPrep, DTale etc.\n\nIn any case, these will provide some quick insights and save time for us. Post that, we can focus more in depth into certain areas such as correlation or interaction between features to understand more and take actions as part of our Data Preparation \/ Feature Engineering steps.","53863839":"# 2b. Read from Datasets\n\n### O2 Saturation Dataset","22ca6400":"# 3a. Since there are duplicates, let's remove them.","d266a4a3":"We can observe that approximately 89% values range between 96.5 to 98.6. Most common values are 98.6","abd44752":"# 5.3 Hyperparameter Tuning for Random Forest","9031cd05":"# 5. MODEL TUNING\n\nLet's focus on the fine tuning of hyper parameters and explore which combinations works in an optimum manner.\nBased on that, we will consider those parameter values and re-execute our model and evaluate the performance.","185b3277":"### Observations in the Heart Dataset: \n    * 14 columns\/features and 303 rows\/observations\n    * It indicated no missing values\n    * 1 duplicate row\n    * A column \"oldpeak\" has almost 32.7% zero values\n    ","fde34501":"#### Now, we can see that the features are scaled appropriately.","6e19cc0c":"# 3f. Analysis on \"trtbps\" feature\n\n### trtbps - Resting Blood Pressure","35837639":"# 4c. Light GBM Classifier","913f45bb":"#### So, we have removed the 1 duplicate. We will proceed with this dataframe. (df_processed)","0bb210f2":"# 5.2 Hyperparameter Tuning for Logistic Regression","4483fb4d":"# 4b. Random Forest Classifier","d2ec5b8e":"### Let's prepare Independent and Target variables ","f0f6043e":"# Scaling\n\n* We noticed that following features\/columns are needed to be normalized\/scaled.\n    * age\n    * trtbps\n    * chol\n    * thalachh\n    * oldpeak","8286082d":"# 3b. Let's visualize through a histogram","dd94b1be":"### Heart Attack Dataset","f92ca226":"# 4a. Classification Models - kNearestNeighbor","248da9fb":"# 3e. Analysis on \"CP\" feature\n\n### CP - Chest Pain Type\n\n* Value 0: Typical angina: chest pain related decrease blood supply to the heart\n* Value 1: Atypical angina: chest pain not related to heart\n* Value 2: Non-anginal pain: typically esophageal spasms (non heart related)\n* Value 3: Asymptomatic: chest pain not showing signs of disease","cf215a87":"#### Interpretation: Around 95% of people above Age of 40 are having heart attack","f4f43e38":"# 2a. Get required libraries","cf6a0167":"# 2c. Profile Report Analysis to understand features, distributions & correlations","54f637ee":"# 3d. Analysis on \"Age\" feature","2316d68c":"#### We observed from the Profile report about some of the features and data representations. Now we will further do EDA and Data Preparation to have pre-processing, more charts \/ visualization prior to making the data ready for model development phase.\n"}}