{"cell_type":{"0fb5b025":"code","23cfe73f":"code","c1c7d558":"code","5207e984":"code","d457535c":"code","5acbfc40":"code","fa19ab6f":"code","cf6a9e93":"code","c443312e":"code","29bfd39c":"code","b244e155":"code","8676743f":"code","a78553b5":"code","383c9fac":"code","74b22cfb":"code","bf5db8f8":"code","1695d72e":"code","129585fe":"code","e809c010":"code","b7050099":"code","b1b67b8d":"code","8a2f8812":"code","bb2a2240":"code","95c0fe55":"code","847a9ba1":"code","16852727":"code","67628340":"code","8ece52e8":"code","1c566b42":"code","0548b7e1":"code","c061b5b0":"code","c7eda746":"code","abb91be2":"code","dac5c5d5":"code","efbe19be":"code","9bacd2f4":"markdown","39f3ed17":"markdown","5aa3eacc":"markdown","328c7992":"markdown","897b616a":"markdown","fbca722e":"markdown","cfe7042a":"markdown","ac6d07b0":"markdown","b6fc11ac":"markdown","da097ef9":"markdown","720d62dc":"markdown","3aefdaa1":"markdown","521e82a1":"markdown","b7ac57fc":"markdown","43e2271a":"markdown","4a59cc7d":"markdown","dfd0e479":"markdown","9b784ebc":"markdown","eb3c0b49":"markdown","e913da83":"markdown","27e6f3d6":"markdown","8be7d7e2":"markdown","2a9c9456":"markdown","9e7829ed":"markdown","3d471fde":"markdown","134489b4":"markdown","fe836b66":"markdown"},"source":{"0fb5b025":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","23cfe73f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve","c1c7d558":"data = pd.read_csv('..\/input\/Combined_News_DJIA.csv')\ndata.head()","5207e984":"combined=data.copy()\ncombined['Combined']=combined.iloc[:,2:27].apply(lambda row: ''.join(str(row.values)), axis=1)","d457535c":"train = data[data['Date'] < '2015-01-01']\ntest = data[data['Date'] > '2014-12-31']","5acbfc40":"print(\"Length of train is\",len(train))\nprint(\"Length of test is\", len(test))","fa19ab6f":"trainheadlines = []\nfor row in range(0,len(train.index)):\n    trainheadlines.append(' '.join(str(x) for x in train.iloc[row,2:27]))\ntestheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))","cf6a9e93":"train = combined[combined['Date'] < '2015-01-01']\ntest = combined[combined['Date'] > '2014-12-31']","c443312e":"non_decrease = train[train['Label']==1]\ndecrease = train[train['Label']==0]\nprint(len(non_decrease)\/len(train))","29bfd39c":"non_decrease_test = test[test['Label']==1]\ndecrease_test = test[test['Label']==0]\nprint(len(non_decrease_test)\/len(test))","b244e155":"import matplotlib.pyplot as plt\n%matplotlib inline\nfrom wordcloud import WordCloud,STOPWORDS\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\ndef to_words(content): ### function to clean the words\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", content) ### get only letters\n    words = letters_only.lower().split()             ### lowercase       \n    stops = set(stopwords.words(\"english\"))         ### remove stopwords such as 'the', 'and' etc.         \n    meaningful_words = [w for w in words if not w in stops] ### get meaningful words\n    return( \" \".join( meaningful_words )) ","8676743f":"non_decrease_word=[]\ndecrease_word=[]\nfor each in non_decrease['Combined']:\n    non_decrease_word.append(to_words(each))\n\nfor each in decrease['Combined']:\n    decrease_word.append(to_words(each))","a78553b5":"wordcloud1 = WordCloud(background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(decrease_word[1])\nplt.figure(1,figsize=(8,8))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.title(\"Words which indicate a fall in DJIA \")\nplt.show()","383c9fac":"wordcloud2 = WordCloud(background_color='green',\n                      width=3000,\n                      height=2500\n                     ).generate(non_decrease_word[3])\nplt.figure(1,figsize=(8,8))\nplt.imshow(wordcloud2)\nplt.axis('off')\nplt.title(\"Words which indicate a rise\/stable DJIA \")\nplt.show()","74b22cfb":"example = train.iloc[3,3]\nprint(example)","bf5db8f8":"example2 = example.lower()\nprint(example2)","1695d72e":"example3 = CountVectorizer().build_tokenizer()(example2)\nprint(example3)","129585fe":"pd.DataFrame([[x,example3.count(x)] for x in set(example3)], columns = ['Word', 'Count'])","e809c010":"basicvectorizer = CountVectorizer()\nbasictrain = basicvectorizer.fit_transform(trainheadlines)\nprint(basictrain.shape)","b7050099":"testheadlines = []\nfor row in range(0,len(test.index)):\n    testheadlines.append(' '.join(str(x) for x in test.iloc[row,2:27]))\n","b1b67b8d":"basictest = basicvectorizer.transform(testheadlines)\nprint(basictest.shape)","8a2f8812":"Classifiers = [\n    LogisticRegression(C=0.1,solver='liblinear',max_iter=2000),\n    KNeighborsClassifier(3),\n    RandomForestClassifier(n_estimators=500,max_depth=9),\n    ]","bb2a2240":"Accuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(basictrain,train['Label'])\n        pred = fit.predict(basictest)\n        prob = fit.predict_proba(basictest)[:,1]\n    except Exception:\n        fit = classifier.fit(basictrain,train['Label'])\n        pred = fit.predict(basictest)\n        prob = fit.predict_proba(basictest)[:,1]\n    accuracy = accuracy_score(pred,test['Label'])\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    fpr, tpr, _ = roc_curve(test['Label'],prob)","95c0fe55":"df=pd.DataFrame(columns = ['Model', 'Accuracy'],index=np.arange(1, len(df)+1))\ndf.Model=Model\ndf.Accuracy=Accuracy\ndf","847a9ba1":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf=TfidfVectorizer()\ntrain_text = []\ntest_text = []\nfor each in train['Combined']:\n    train_text.append(to_words(each))\n\nfor each in test['Combined']:\n    test_text.append(to_words(each))\ntrain_features = tfidf.fit_transform(train_text)\ntest_features = tfidf.transform(test_text)","16852727":"Classifiers = [\n    LogisticRegression(C=0.1,solver='liblinear',max_iter=2000),\n    KNeighborsClassifier(3),\n    SVC(kernel=\"rbf\", C=0.25, probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=500,max_depth=9),\n    AdaBoostClassifier(),\n    ]","67628340":"dense_features=train_features.toarray()\ndense_test= test_features.toarray()\nAccuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(train_features,train['Label'])\n        pred = fit.predict(test_features)\n        prob = fit.predict_proba(test_features)[:,1]\n    except Exception:\n        fit = classifier.fit(dense_features,train['Label'])\n        pred = fit.predict(dense_test)\n        prob = fit.predict_proba(dense_test)[:,1]\n    accuracy = accuracy_score(pred,test['Label'])\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    fpr, tpr, _ = roc_curve(test['Label'],prob)\n    ","8ece52e8":"df=pd.DataFrame(columns = ['Model', 'Accuracy'],index=np.arange(1, len(df)+1))\ndf.Model=Model\ndf.Accuracy=Accuracy\ndf","1c566b42":"advancedvectorizer = CountVectorizer(ngram_range=(2,2))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)\nprint(advancedtrain.shape)\n","0548b7e1":"advancedtest = advancedvectorizer.transform(testheadlines)","c061b5b0":"Accuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(advancedtrain,train['Label'])\n        pred = fit.predict(advancedtest)\n        prob = fit.predict_proba(advancedtest)[:,1]\n    except Exception:\n        fit = classifier.fit(advancedtrain,train['Label'])\n        pred = fit.predict(advancedtest)\n        prob = fit.predict_proba(advancedtest)[:,1]\n    accuracy = accuracy_score(pred,test['Label'])\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    fpr, tpr, _ = roc_curve(test['Label'],prob)\n    ","c7eda746":"df=pd.DataFrame(columns = ['Model', 'Accuracy'],index=np.arange(1, len(df)+1))\ndf.Model=Model\ndf.Accuracy=Accuracy\ndf","abb91be2":"advancedvectorizer = CountVectorizer(ngram_range=(3,3))\nadvancedtrain = advancedvectorizer.fit_transform(trainheadlines)\nprint(advancedtrain.shape)\nadvancedtest = advancedvectorizer.transform(testheadlines)","dac5c5d5":"Accuracy=[]\nModel=[]\nfor classifier in Classifiers:\n    try:\n        fit = classifier.fit(advancedtrain,train['Label'])\n        pred = fit.predict(advancedtest)\n        prob = fit.predict_proba(advancedtest)[:,1]\n    except Exception:\n        fit = classifier.fit(advancedtrain,train['Label'])\n        pred = fit.predict(advancedtest)\n        prob = fit.predict_proba(advancedtest)[:,1]\n    accuracy = accuracy_score(pred,test['Label'])\n    Accuracy.append(accuracy)\n    Model.append(classifier.__class__.__name__)\n    fpr, tpr, _ = roc_curve(test['Label'],prob)","efbe19be":"df=pd.DataFrame(columns = ['Model', 'Accuracy'],index=np.arange(1, len(df)+1))\ndf.Model=Model\ndf.Accuracy=Accuracy\ndf","9bacd2f4":"The technique we just used is known as a **bag-of-words** model. We essentially placed all of our headlines into a \"bag\" and counted the words as we pulled them out.\n\nHowever,  a single word doesn't always have enough meaning by itself.\n\nWe need to consider the rest of the words in the sentence as well!","39f3ed17":"### N - gram model","5aa3eacc":"The accuracy does not seem to increase and it looks like we have hit our maximum accuracy point at 56%.","328c7992":"We can see that the occurrence of non-decrease situation is almost equal to that of a decrease market.","897b616a":"**Model Fitting**","fbca722e":"##### n=2","cfe7042a":"As we can see, there has been a slight improvement from the previous scores.","ac6d07b0":"#### Simple EDA","b6fc11ac":"Lets first join all the headlines for each row together.","da097ef9":"This time we have 611,140 unique variables representing three-word combinations!","720d62dc":"The tool we'll be using is CountVectorizer, which takes a single list of strings as input, and produces word counts for each one.","3aefdaa1":"The **Label** variable will be a **1** if the DJIA stayed the same or rose on that date or **0** if the DJIA fell on that date.\n","521e82a1":"##### Count Vectorizer","b7ac57fc":"## Model fitting","43e2271a":"### Basic Model Training and Testing","4a59cc7d":"## Text Preprocessing","dfd0e479":"##### n=3","9b784ebc":"##### Lower Case","eb3c0b49":"Lets try to improve the score with more models and feature Selection.","e913da83":"#### Train and Test Split","27e6f3d6":"**TFIDF Model**\n","8be7d7e2":"This time we have 366,721 unique variables representing two-word combinations!","2a9c9456":"Our resulting table contains counts for 31,675 different words!","9e7829ed":"## Advanced Modeling","3d471fde":"We are getting much better results now and we are getting an accuracy of 56.08%.","134489b4":"The process involved:\n\n- Converting the headline to lowercase letters\n- Splitting the sentence into a list of words\n- Removing punctuation and meaningless words\n","fe836b66":"### Feature Extraction"}}