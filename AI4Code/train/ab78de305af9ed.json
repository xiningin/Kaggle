{"cell_type":{"ccebbceb":"code","042b8cee":"code","a70debd1":"code","75f33d90":"code","13452280":"code","859df30f":"code","862cfb35":"code","43c45de3":"code","bbd7a555":"code","a7c759cc":"code","6ca388f2":"code","fbaf4095":"code","f522ad9a":"code","2e3b448a":"code","518dfaeb":"code","a37091c0":"code","9f6dae3d":"code","54650112":"code","838ef9b1":"code","a694317a":"code","872ef041":"code","c7d20dec":"code","bfbae529":"code","27790f95":"code","ac4767bb":"code","414263c1":"code","df2cca92":"code","61bc4799":"code","8c7f8a7d":"code","3ed7c2a5":"code","a073ce7e":"code","1d7a1899":"code","c88bbdb3":"code","bf3c7e8f":"code","8343cc3e":"code","29995288":"code","2d462a24":"code","bc56f7f3":"code","a2df89d6":"code","cbb09734":"code","d50cc64c":"code","c7c7a20c":"code","34aa666e":"code","08bc5911":"code","587c8657":"code","55db006b":"code","fd9dd6c2":"code","e5997271":"code","77effbe5":"code","8bfee867":"code","1b618113":"code","61663bdd":"code","f731de54":"code","d233afdb":"code","113946b7":"code","2e7d91f1":"code","06515a01":"code","01d55987":"code","d433d4a1":"code","7f32739a":"code","0769156a":"code","0f1e862b":"code","3998a2e5":"code","0975d918":"code","bc11ed97":"code","89ebaf34":"code","f33409ea":"code","4d1cc647":"code","87aca99d":"code","c2311407":"code","23f25ec1":"code","05de35ab":"code","a37376c2":"code","60fd9a9a":"code","4993e5dc":"code","8b5c830d":"code","026acd41":"code","90ef0afa":"code","863f7dc9":"code","a500c2e1":"code","b05ed78d":"code","5dfa150f":"code","e20a2ad8":"code","b6525047":"code","e755b069":"code","b9b934d9":"code","3571e124":"code","c70f3977":"code","b7b55428":"code","a0fe44f0":"code","2bade04c":"code","205242ee":"code","92f374c5":"code","cfa5c78d":"code","3f92785c":"code","8cbb1b1c":"code","14440783":"code","a5542bec":"code","e27a2766":"code","025f02a7":"code","f56fc25a":"code","152e1001":"code","1ed5fca0":"code","da0ba6fd":"code","2bbc8061":"code","e4f54124":"code","ecaca933":"code","5db6ee25":"markdown","c793ab6a":"markdown","3072adf4":"markdown","bfd6f475":"markdown","e220370c":"markdown","912af287":"markdown","e0ad5d2f":"markdown","f7da3c22":"markdown","394b33d7":"markdown","6273137b":"markdown","0e6532f4":"markdown","fdaf6df4":"markdown","6ba6edb1":"markdown","3cc0d112":"markdown","edca4e03":"markdown","ce88bc7c":"markdown","79c6f16e":"markdown","e9ff07e7":"markdown","0c50ee1b":"markdown","7b90c095":"markdown","7b974d46":"markdown","816e30d7":"markdown","aa5c63d8":"markdown","b07302a1":"markdown","358c1ab8":"markdown","6711290a":"markdown","0c7cf88c":"markdown","9549cfe3":"markdown","6a93daba":"markdown","0d819551":"markdown","a04c52fa":"markdown","68b09a1f":"markdown","0902febf":"markdown","6845f377":"markdown","977466ed":"markdown","6014bd79":"markdown","a8c834fb":"markdown","f016618d":"markdown","26d0c08c":"markdown","94a71754":"markdown","669a6ecf":"markdown","d1838aa4":"markdown","e6a71a2a":"markdown","487a30c9":"markdown","c7ba667a":"markdown","701f6616":"markdown","8923ea7f":"markdown","a3bd10fc":"markdown","07650b3d":"markdown","6b8fe42b":"markdown","5bd48281":"markdown","14a87c4d":"markdown","3654297f":"markdown","9664f31f":"markdown","931971b8":"markdown","b2181bf2":"markdown","42ed1319":"markdown","8c0634ae":"markdown","a9754fe4":"markdown","571fc474":"markdown","481cce33":"markdown","5b9008f1":"markdown","cf332029":"markdown","e84d1512":"markdown","8fb42594":"markdown","40b0c089":"markdown","78f2bf99":"markdown","56cc3a37":"markdown","cd6c9584":"markdown","edea21f0":"markdown","f9e60733":"markdown","6909e0b0":"markdown","f639a85e":"markdown","812549c1":"markdown","f8431047":"markdown","d7de1c9e":"markdown","7ead2505":"markdown","e023687a":"markdown","d77883e3":"markdown","38d4753c":"markdown","a894ddb4":"markdown","3cf9d12c":"markdown","c235afe6":"markdown","8fa83dfa":"markdown","e6216f47":"markdown","e5d94001":"markdown","e0988a15":"markdown","f279f049":"markdown","09513063":"markdown","27a8a22c":"markdown","a17f0998":"markdown","dfb0595d":"markdown","3e020e16":"markdown","ca63beb1":"markdown","37b3e6ce":"markdown","b3f6e8e3":"markdown","185d3fd4":"markdown","2e7254bf":"markdown","41a93dd7":"markdown","82088a8b":"markdown","3ace38ea":"markdown","6bf22cfc":"markdown","92bdab6a":"markdown","82f2e712":"markdown","c1c28365":"markdown","1961d273":"markdown","eb1122e5":"markdown","19185ef6":"markdown","2dfe5231":"markdown","e6daf65d":"markdown","f393c70e":"markdown","14969fcc":"markdown"},"source":{"ccebbceb":"# importing libraries for data handling and analysis\nimport pandas as pd\nfrom pandas.plotting import scatter_matrix\nfrom pandas import ExcelWriter\nfrom pandas import ExcelFile\nfrom openpyxl import load_workbook\nimport numpy as np\nfrom scipy.stats import norm, skew\nfrom scipy import stats\nimport statsmodels.api as sm\nimport warnings\nwarnings.filterwarnings('ignore')","042b8cee":"# importing libraries for data visualisations\nimport seaborn as sns\nfrom matplotlib import pyplot\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport matplotlib\n%matplotlib inline\ncolor = sns.color_palette()\nfrom IPython.display import display\npd.options.display.max_columns = None\n# Standard plotly imports\nfrom plotly import plotly as py\nimport plotly as py\nimport plotly.figure_factory as ff\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n#py.initnotebookmode(connected=True) # this code, allow us to work with offline plotly version\n# Using plotly + cufflinks in offline mode\nimport cufflinks as cf\ncf.set_config_file(offline=True)\nimport cufflinks\ncufflinks.go_offline(connected=True)","a70debd1":"# sklearn modules for preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n# from imblearn.over_sampling import SMOTE  # SMOTE\n# sklearn modules for ML model selection\nfrom sklearn.model_selection import train_test_split  # import 'train_test_split'\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n# Libraries for data modelling\nfrom sklearn import svm, tree, linear_model, neighbors\nfrom sklearn import naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Common sklearn Model Helpers\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n# from sklearn.datasets import make_classification\n\n# sklearn modules for performance metrics\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve\nfrom sklearn.metrics import auc, roc_auc_score, roc_curve, recall_score, log_loss\nfrom sklearn.metrics import f1_score, accuracy_score, roc_auc_score, make_scorer\nfrom sklearn.metrics import average_precision_score","75f33d90":"# importing misceallenous libraries\nimport os\nimport re\nimport sys\nimport timeit\nimport string\nfrom datetime import datetime\nfrom time import time\nfrom dateutil.parser import parse\n# ip = get_ipython()\n# ip.register_magics(jupyternotify.JupyterNotifyMagics)","13452280":"import os\nprint(os.listdir(\"..\/input\"))","859df30f":"# Read Excel file\ndf_sourcefile = pd.read_csv('..\/input\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\nprint(\"Shape of dataframe is: {}\".format(df_sourcefile.shape))","862cfb35":"# Make a copy of the original sourcefile\ndf_HR = df_sourcefile.copy()","43c45de3":"# Dataset columns\ndf_HR.columns","bbd7a555":"# Dataset header\ndf_HR.head()","a7c759cc":"# let's break down the columns by their type (i.e. int64, float64, object)\ndf_HR.columns.to_series().groupby(df_HR.dtypes).groups","6ca388f2":"# Columns datatypes and missign values\ndf_HR.info()","fbaf4095":"df_HR.describe()","f522ad9a":"df_HR.hist(figsize=(20,20))\nplt.show()","2e3b448a":"(mu, sigma) = norm.fit(df_HR.loc[df_HR['Attrition'] == 'Yes', 'Age'])\nprint(\n    'Ex-exmployees: average age = {:.1f} years old and standard deviation = {:.1f}'.format(mu, sigma))\n(mu, sigma) = norm.fit(df_HR.loc[df_HR['Attrition'] == 'No', 'Age'])\nprint('Current exmployees: average age = {:.1f} years old and standard deviation = {:.1f}'.format(\n    mu, sigma))","518dfaeb":"plt.figure(figsize=(15,6))\nplt.style.use('seaborn-colorblind')\nplt.grid(True, alpha=0.5)\nsns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'No', 'Age'], label = 'Active Employee')\nsns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'Yes', 'Age'], label = 'Ex-Employees')\nplt.xlim(left=18, right=60)\nplt.xlabel('Age (years)')\nplt.ylabel('Density')\nplt.title('Age Distribution in Percent by Attrition Status');","a37091c0":"# Education Field of employees\ndf_HR['EducationField'].value_counts()","9f6dae3d":"df_EducationField = pd.DataFrame(columns=[\"Field\", \"% of Leavers\"])\ni=0\nfor field in list(df_HR['EducationField'].unique()):\n    ratio = df_HR[(df_HR['EducationField']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] \/ df_HR[df_HR['EducationField']==field].shape[0]\n    df_EducationField.loc[i] = (field, ratio*100)\n    i += 1\n    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \ndf_EF = df_EducationField.groupby(by=\"Field\").sum()\ndf_EF.iplot(kind='bar',title='Leavers by Education Field (%)')","54650112":"# Gender of employees\ndf_HR['Gender'].value_counts()","838ef9b1":"print(\"Normalised gender distribution of ex-employees in the dataset: Male = {:.1f}%; Female {:.1f}%.\".format((df_HR[(df_HR['Attrition'] == 'Yes') & (\n    df_HR['Gender'] == 'Male')].shape[0] \/ df_HR[df_HR['Gender'] == 'Male'].shape[0])*100, (df_HR[(df_HR['Attrition'] == 'Yes') & (df_HR['Gender'] == 'Female')].shape[0] \/ df_HR[df_HR['Gender'] == 'Female'].shape[0])*100))","a694317a":"df_Gender = pd.DataFrame(columns=[\"Gender\", \"% of Leavers\"])\ni=0\nfor field in list(df_HR['Gender'].unique()):\n    ratio = df_HR[(df_HR['Gender']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] \/ df_HR[df_HR['Gender']==field].shape[0]\n    df_Gender.loc[i] = (field, ratio*100)\n    i += 1\n    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \ndf_G = df_Gender.groupby(by=\"Gender\").sum()\ndf_G.iplot(kind='bar',title='Leavers by Gender (%)')","872ef041":"# Marital Status of employees\ndf_HR['MaritalStatus'].value_counts()","c7d20dec":"df_Marital = pd.DataFrame(columns=[\"Marital Status\", \"% of Leavers\"])\ni=0\nfor field in list(df_HR['MaritalStatus'].unique()):\n    ratio = df_HR[(df_HR['MaritalStatus']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] \/ df_HR[df_HR['MaritalStatus']==field].shape[0]\n    df_Marital.loc[i] = (field, ratio*100)\n    i += 1\n    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \ndf_MF = df_Marital.groupby(by=\"Marital Status\").sum()\ndf_MF.iplot(kind='bar',title='Leavers by Marital Status (%)')","bfbae529":"# Distance from Home\nprint(\"Distance from home for employees to get to work is from {} to {} miles.\".format(df_HR['DistanceFromHome'].min(),\n                                                                                       df_HR['DistanceFromHome'].max()))","27790f95":"print('Average distance from home for currently active employees: {:.2f} miles and ex-employees: {:.2f} miles'.format(\n    df_HR[df_HR['Attrition'] == 'No']['DistanceFromHome'].mean(), df_HR[df_HR['Attrition'] == 'Yes']['DistanceFromHome'].mean()))","ac4767bb":"plt.figure(figsize=(15,6))\nplt.style.use('seaborn-colorblind')\nplt.grid(True, alpha=0.5)\nsns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'No', 'DistanceFromHome'], label = 'Active Employee')\nsns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'Yes', 'DistanceFromHome'], label = 'Ex-Employees')\nplt.xlabel('DistanceFromHome')\nplt.xlim(left=0)\nplt.ylabel('Density')\nplt.title('Distance From Home Distribution in Percent by Attrition Status');","414263c1":"# The organisation consists of several departments\ndf_HR['Department'].value_counts()","df2cca92":"df_Department = pd.DataFrame(columns=[\"Department\", \"% of Leavers\"])\ni=0\nfor field in list(df_HR['Department'].unique()):\n    ratio = df_HR[(df_HR['Department']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] \/ df_HR[df_HR['Department']==field].shape[0]\n    df_Department.loc[i] = (field, ratio*100)\n    i += 1\n    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \ndf_DF = df_Department.groupby(by=\"Department\").sum()\ndf_DF.iplot(kind='bar',title='Leavers by Department (%)')","61bc4799":"# Employees have different business travel commitmnent depending on their roles and level in the organisation\ndf_HR['BusinessTravel'].value_counts()","8c7f8a7d":"df_BusinessTravel = pd.DataFrame(columns=[\"Business Travel\", \"% of Leavers\"])\ni=0\nfor field in list(df_HR['BusinessTravel'].unique()):\n    ratio = df_HR[(df_HR['BusinessTravel']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] \/ df_HR[df_HR['BusinessTravel']==field].shape[0]\n    df_BusinessTravel.loc[i] = (field, ratio*100)\n    i += 1\n    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \ndf_BT = df_BusinessTravel.groupby(by=\"Business Travel\").sum()\ndf_BT.iplot(kind='bar',title='Leavers by Business Travel (%)')","3ed7c2a5":"# Employees in the database have several roles on-file\ndf_HR['JobRole'].value_counts()","a073ce7e":"df_JobRole = pd.DataFrame(columns=[\"Job Role\", \"% of Leavers\"])\ni=0\nfor field in list(df_HR['JobRole'].unique()):\n    ratio = df_HR[(df_HR['JobRole']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] \/ df_HR[df_HR['JobRole']==field].shape[0]\n    df_JobRole.loc[i] = (field, ratio*100)\n    i += 1\n    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \ndf_JR = df_JobRole.groupby(by=\"Job Role\").sum()\ndf_JR.iplot(kind='bar',title='Leavers by Job Role (%)')","1d7a1899":"df_HR['JobLevel'].value_counts()","c88bbdb3":"df_JobLevel = pd.DataFrame(columns=[\"Job Level\", \"% of Leavers\"])\ni=0\nfor field in list(df_HR['JobLevel'].unique()):\n    ratio = df_HR[(df_HR['JobLevel']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] \/ df_HR[df_HR['JobLevel']==field].shape[0]\n    df_JobLevel.loc[i] = (field, ratio*100)\n    i += 1\n    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \ndf_JL = df_JobLevel.groupby(by=\"Job Level\").sum()\ndf_JL.iplot(kind='bar',title='Leavers by Job Level (%)')","bf3c7e8f":"df_HR['JobInvolvement'].value_counts()","8343cc3e":"df_JobInvolvement = pd.DataFrame(columns=[\"Job Involvement\", \"% of Leavers\"])\ni=0\nfor field in list(df_HR['JobInvolvement'].unique()):\n    ratio = df_HR[(df_HR['JobInvolvement']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] \/ df_HR[df_HR['JobInvolvement']==field].shape[0]\n    df_JobInvolvement.loc[i] = (field, ratio*100)\n    i += 1\n    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \ndf_JI = df_JobInvolvement.groupby(by=\"Job Involvement\").sum()\ndf_JI.iplot(kind='bar',title='Leavers by Job Involvement (%)')","29995288":"print(\"Number of training times last year varies from {} to {} years.\".format(\n    df_HR['TrainingTimesLastYear'].min(), df_HR['TrainingTimesLastYear'].max()))","2d462a24":"plt.figure(figsize=(15,6))\nplt.style.use('seaborn-colorblind')\nplt.grid(True, alpha=0.5)\nsns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'No', 'TrainingTimesLastYear'], label = 'Active Employee')\nsns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'Yes', 'TrainingTimesLastYear'], label = 'Ex-Employees')\nplt.xlabel('TrainingTimesLastYear')\nplt.ylabel('Density')\nplt.title('Training Times Last Year Distribution in Percent by Attrition Status');","bc56f7f3":"df_HR['NumCompaniesWorked'].value_counts()","a2df89d6":"df_NumCompaniesWorked = pd.DataFrame(columns=[\"Num Companies Worked\", \"% of Leavers\"])\ni=0\nfor field in list(df_HR['NumCompaniesWorked'].unique()):\n    ratio = df_HR[(df_HR['NumCompaniesWorked']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] \/ df_HR[df_HR['NumCompaniesWorked']==field].shape[0]\n    df_NumCompaniesWorked.loc[i] = (field, ratio*100)\n    i += 1\n    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \ndf_NC = df_NumCompaniesWorked.groupby(by=\"Num Companies Worked\").sum()\ndf_NC.iplot(kind='bar',title='Leavers by Num Companies Worked (%)')","cbb09734":"print(\"Number of Years at the company varies from {} to {} years.\".format(\n    df_HR['YearsAtCompany'].min(), df_HR['YearsAtCompany'].max()))","d50cc64c":"plt.figure(figsize=(15,6))\nplt.style.use('seaborn-colorblind')\nplt.grid(True, alpha=0.5)\nsns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'No', 'YearsAtCompany'], label = 'Active Employee')\nsns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'Yes', 'YearsAtCompany'], label = 'Ex-Employees')\nplt.xlabel('YearsAtCompany')\nplt.xlim(left=0)\nplt.ylabel('Density')\nplt.title('Years At Company in Percent by Attrition Status');","c7c7a20c":"print(\"Number of Years in the current role varies from {} to {} years.\".format(\n    df_HR['YearsInCurrentRole'].min(), df_HR['YearsInCurrentRole'].max()))","34aa666e":"plt.figure(figsize=(15,6))\nplt.style.use('seaborn-colorblind')\nplt.grid(True, alpha=0.5)\nsns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'No', 'YearsInCurrentRole'], label = 'Active Employee')\nsns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'Yes', 'YearsInCurrentRole'], label = 'Ex-Employees')\nplt.xlabel('YearsInCurrentRole')\nplt.xlim(left=0)\nplt.ylabel('Density')\nplt.title('Years In Current Role in Percent by Attrition Status');","08bc5911":"print(\"Number of Years since last promotion varies from {} to {} years.\".format(\n    df_HR['YearsSinceLastPromotion'].min(), df_HR['YearsSinceLastPromotion'].max()))","587c8657":"plt.figure(figsize=(15,6))\nplt.style.use('seaborn-colorblind')\nplt.grid(True, alpha=0.5)\nsns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'No', 'YearsSinceLastPromotion'], label = 'Active Employee')\nsns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'Yes', 'YearsSinceLastPromotion'], label = 'Ex-Employees')\nplt.xlabel('YearsSinceLastPromotion')\nplt.xlim(left=0)\nplt.ylabel('Density')\nplt.title('Years Since Last Promotion in Percent by Attrition Status');","55db006b":"print(\"Total working years varies from {} to {} years.\".format(\n    df_HR['TotalWorkingYears'].min(), df_HR['TotalWorkingYears'].max()))","fd9dd6c2":"plt.figure(figsize=(15,6))\nplt.style.use('seaborn-colorblind')\nplt.grid(True, alpha=0.5)\nsns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'No', 'TotalWorkingYears'], label = 'Active Employee')\nsns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'Yes', 'TotalWorkingYears'], label = 'Ex-Employees')\nplt.xlabel('TotalWorkingYears')\nplt.xlim(left=0)\nplt.ylabel('Density')\nplt.title('Total Working Years in Percent by Attrition Status');","e5997271":"print(\"Number of Years wit current manager varies from {} to {} years.\".format(\n    df_HR['YearsWithCurrManager'].min(), df_HR['YearsWithCurrManager'].max()))","77effbe5":"plt.figure(figsize=(15,6))\nplt.style.use('seaborn-colorblind')\nplt.grid(True, alpha=0.5)\nsns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'No', 'YearsWithCurrManager'], label = 'Active Employee')\nsns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'Yes', 'YearsWithCurrManager'], label = 'Ex-Employees')\nplt.xlabel('YearsWithCurrManager')\nplt.xlim(left=0)\nplt.ylabel('Density')\nplt.title('Years With Curr Manager in Percent by Attrition Status');","8bfee867":"df_HR['WorkLifeBalance'].value_counts()","1b618113":"df_WorkLifeBalance = pd.DataFrame(columns=[\"WorkLifeBalance\", \"% of Leavers\"])\ni=0\nfor field in list(df_HR['WorkLifeBalance'].unique()):\n    ratio = df_HR[(df_HR['WorkLifeBalance']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] \/ df_HR[df_HR['WorkLifeBalance']==field].shape[0]\n    df_WorkLifeBalance.loc[i] = (field, ratio*100)\n    i += 1\n    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \ndf_WLB = df_WorkLifeBalance.groupby(by=\"WorkLifeBalance\").sum()\ndf_WLB.iplot(kind='bar',title='Leavers by WorkLifeBalance (%)')","61663bdd":"df_HR['StandardHours'].value_counts()","f731de54":"df_HR['OverTime'].value_counts()","d233afdb":"df_OverTime = pd.DataFrame(columns=[\"OverTime\", \"% of Leavers\"])\ni=0\nfor field in list(df_HR['OverTime'].unique()):\n    ratio = df_HR[(df_HR['OverTime']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] \/ df_HR[df_HR['OverTime']==field].shape[0]\n    df_OverTime.loc[i] = (field, ratio*100)\n    i += 1\n    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \ndf_OT = df_OverTime.groupby(by=\"OverTime\").sum()\ndf_OT.iplot(kind='bar',title='Leavers by OverTime (%)')","113946b7":"print(\"Employee Hourly Rate varies from ${} to ${}.\".format(\n    df_HR['HourlyRate'].min(), df_HR['HourlyRate'].max()))","2e7d91f1":"print(\"Employee Daily Rate varies from ${} to ${}.\".format(\n    df_HR['DailyRate'].min(), df_HR['DailyRate'].max()))","06515a01":"print(\"Employee Monthly Rate varies from ${} to ${}.\".format(\n    df_HR['MonthlyRate'].min(), df_HR['MonthlyRate'].max()))","01d55987":"print(\"Employee Monthly Income varies from ${} to ${}.\".format(\n    df_HR['MonthlyIncome'].min(), df_HR['MonthlyIncome'].max()))","d433d4a1":"plt.figure(figsize=(15,6))\nplt.style.use('seaborn-colorblind')\nplt.grid(True, alpha=0.5)\nsns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'No', 'MonthlyIncome'], label = 'Active Employee')\nsns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'Yes', 'MonthlyIncome'], label = 'Ex-Employees')\nplt.xlabel('Monthly Income')\nplt.xlim(left=0)\nplt.ylabel('Density')\nplt.title('Monthly Income in Percent by Attrition Status');","7f32739a":"print(\"Percentage Salary Hikes varies from {}% to {}%.\".format(\n    df_HR['PercentSalaryHike'].min(), df_HR['PercentSalaryHike'].max()))","0769156a":"plt.figure(figsize=(15,6))\nplt.style.use('seaborn-colorblind')\nplt.grid(True, alpha=0.5)\nsns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'No', 'PercentSalaryHike'], label = 'Active Employee')\nsns.kdeplot(df_HR.loc[df_HR['Attrition'] == 'Yes', 'PercentSalaryHike'], label = 'Ex-Employees')\nplt.xlabel('PercentSalaryHike')\nplt.xlim(left=0)\nplt.ylabel('Density')\nplt.title('Percent Salary Hike in Percent by Attrition Status');","0f1e862b":"print(\"Stock Option Levels varies from {} to {}.\".format(\n    df_HR['StockOptionLevel'].min(), df_HR['StockOptionLevel'].max()))","3998a2e5":"print(\"Normalised percentage of leavers by Stock Option Level: 1: {:.2f}%, 2: {:.2f}%, 3: {:.2f}%\".format(\n    df_HR[(df_HR['Attrition'] == 'Yes') & (df_HR['StockOptionLevel'] == 1)\n          ].shape[0] \/ df_HR[df_HR['StockOptionLevel'] == 1].shape[0]*100,\n    df_HR[(df_HR['Attrition'] == 'Yes') & (df_HR['StockOptionLevel'] == 2)\n          ].shape[0] \/ df_HR[df_HR['StockOptionLevel'] == 1].shape[0]*100,\n    df_HR[(df_HR['Attrition'] == 'Yes') & (df_HR['StockOptionLevel'] == 3)].shape[0] \/ df_HR[df_HR['StockOptionLevel'] == 1].shape[0]*100))","0975d918":"df_StockOptionLevel = pd.DataFrame(columns=[\"StockOptionLevel\", \"% of Leavers\"])\ni=0\nfor field in list(df_HR['StockOptionLevel'].unique()):\n    ratio = df_HR[(df_HR['StockOptionLevel']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] \/ df_HR[df_HR['StockOptionLevel']==field].shape[0]\n    df_StockOptionLevel.loc[i] = (field, ratio*100)\n    i += 1\n    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \ndf_SOL = df_StockOptionLevel.groupby(by=\"StockOptionLevel\").sum()\ndf_SOL.iplot(kind='bar',title='Leavers by Stock Option Level (%)')","bc11ed97":"df_HR['EnvironmentSatisfaction'].value_counts()","89ebaf34":"df_EnvironmentSatisfaction = pd.DataFrame(columns=[\"EnvironmentSatisfaction\", \"% of Leavers\"])\ni=0\nfor field in list(df_HR['EnvironmentSatisfaction'].unique()):\n    ratio = df_HR[(df_HR['EnvironmentSatisfaction']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] \/ df_HR[df_HR['EnvironmentSatisfaction']==field].shape[0]\n    df_EnvironmentSatisfaction.loc[i] = (field, ratio*100)\n    i += 1\n    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \ndf_Env = df_EnvironmentSatisfaction.groupby(by=\"EnvironmentSatisfaction\").sum()\ndf_Env.iplot(kind='bar',title='Leavers by Environment Satisfaction (%)')","f33409ea":"# Job Satisfaction was captured as: 1 'Low' 2 'Medium' 3 'High' 4 'Very High'\ndf_HR['JobSatisfaction'].value_counts()","4d1cc647":"df_JobSatisfaction = pd.DataFrame(columns=[\"JobSatisfaction\", \"% of Leavers\"])\ni=0\nfor field in list(df_HR['JobSatisfaction'].unique()):\n    ratio = df_HR[(df_HR['JobSatisfaction']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] \/ df_HR[df_HR['JobSatisfaction']==field].shape[0]\n    df_JobSatisfaction.loc[i] = (field, ratio*100)\n    i += 1\n    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \ndf_JS = df_JobSatisfaction.groupby(by=\"JobSatisfaction\").sum()\ndf_JS.iplot(kind='bar',title='Leavers by Job Satisfaction (%)')","87aca99d":"df_HR['RelationshipSatisfaction'].value_counts()","c2311407":"df_RelationshipSatisfaction = pd.DataFrame(columns=[\"RelationshipSatisfaction\", \"% of Leavers\"])\ni=0\nfor field in list(df_HR['RelationshipSatisfaction'].unique()):\n    ratio = df_HR[(df_HR['RelationshipSatisfaction']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] \/ df_HR[df_HR['RelationshipSatisfaction']==field].shape[0]\n    df_RelationshipSatisfaction.loc[i] = (field, ratio*100)\n    i += 1\n    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \ndf_RS = df_RelationshipSatisfaction.groupby(by=\"RelationshipSatisfaction\").sum()\ndf_RS.iplot(kind='bar',title='Leavers by Relationship Satisfaction (%)')","23f25ec1":"df_HR['PerformanceRating'].value_counts()","05de35ab":"print(\"Normalised percentage of leavers by Stock Option Level: 3: {:.2f}%, 4: {:.2f}%\".format(\n    df_HR[(df_HR['Attrition'] == 'Yes') & (df_HR['PerformanceRating'] == 3)\n          ].shape[0] \/ df_HR[df_HR['StockOptionLevel'] == 1].shape[0]*100,\n    df_HR[(df_HR['Attrition'] == 'Yes') & (df_HR['PerformanceRating'] == 4)].shape[0] \/ df_HR[df_HR['StockOptionLevel'] == 1].shape[0]*100))","a37376c2":"df_PerformanceRating = pd.DataFrame(columns=[\"PerformanceRating\", \"% of Leavers\"])\ni=0\nfor field in list(df_HR['PerformanceRating'].unique()):\n    ratio = df_HR[(df_HR['PerformanceRating']==field)&(df_HR['Attrition']==\"Yes\")].shape[0] \/ df_HR[df_HR['PerformanceRating']==field].shape[0]\n    df_PerformanceRating.loc[i] = (field, ratio*100)\n    i += 1\n    #print(\"In {}, the ratio of leavers is {:.2f}%\".format(field, ratio*100))    \ndf_PR = df_PerformanceRating.groupby(by=\"PerformanceRating\").sum()\ndf_PR.iplot(kind='bar',title='Leavers by Performance Rating (%)')","60fd9a9a":"# Attrition indicates if the employee is currently active ('No') or has left the company ('Yes')\ndf_HR['Attrition'].value_counts()","4993e5dc":"print(\"Percentage of Current Employees is {:.1f}% and of Ex-employees is: {:.1f}%\".format(\n    df_HR[df_HR['Attrition'] == 'No'].shape[0] \/ df_HR.shape[0]*100,\n    df_HR[df_HR['Attrition'] == 'Yes'].shape[0] \/ df_HR.shape[0]*100))","8b5c830d":"df_HR['Attrition'].iplot(kind='hist', xTitle='Attrition',\n                         yTitle='count', title='Attrition Distribution')","026acd41":"# Find correlations with the target and sort\ndf_HR_trans = df_HR.copy()\ndf_HR_trans['Target'] = df_HR_trans['Attrition'].apply(\n    lambda x: 0 if x == 'No' else 1)\ndf_HR_trans = df_HR_trans.drop(\n    ['Attrition', 'EmployeeCount', 'EmployeeNumber', 'StandardHours', 'Over18'], axis=1)\ncorrelations = df_HR_trans.corr()['Target'].sort_values()\nprint('Most Positive Correlations: \\n', correlations.tail(5))\nprint('\\nMost Negative Correlations: \\n', correlations.head(5))","90ef0afa":"# Calculate correlations\ncorr = df_HR_trans.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n# Heatmap\nplt.figure(figsize=(15, 10))\nsns.heatmap(corr,\n            vmax=.5,\n            mask=mask,\n            # annot=True, fmt='.2f',\n            linewidths=.2, cmap=\"YlGnBu\")","863f7dc9":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n# Create a label encoder object\nle = LabelEncoder()","a500c2e1":"print(df_HR.shape)\ndf_HR.head()","b05ed78d":"# Label Encoding will be used for columns with 2 or less unique values\nle_count = 0\nfor col in df_HR.columns[1:]:\n    if df_HR[col].dtype == 'object':\n        if len(list(df_HR[col].unique())) <= 2:\n            le.fit(df_HR[col])\n            df_HR[col] = le.transform(df_HR[col])\n            le_count += 1\nprint('{} columns were label encoded.'.format(le_count))","5dfa150f":"# convert rest of categorical variable into dummy\ndf_HR = pd.get_dummies(df_HR, drop_first=True)","e20a2ad8":"print(df_HR.shape)\ndf_HR.head()","b6525047":"# import MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0, 5))\nHR_col = list(df_HR.columns)\nHR_col.remove('Attrition')\nfor col in HR_col:\n    df_HR[col] = df_HR[col].astype(float)\n    df_HR[[col]] = scaler.fit_transform(df_HR[[col]])\ndf_HR['Attrition'] = pd.to_numeric(df_HR['Attrition'], downcast='float')\ndf_HR.head()","e755b069":"print('Size of Full Encoded Dataset: {}'. format(df_HR.shape))","b9b934d9":"# assign the target to a new dataframe and convert it to a numerical feature\n#df_target = df_HR[['Attrition']].copy()\ntarget = df_HR['Attrition'].copy()","3571e124":"type(target)","c70f3977":"# let's remove the target feature and redundant features from the dataset\ndf_HR.drop(['Attrition', 'EmployeeCount', 'EmployeeNumber',\n            'StandardHours', 'Over18'], axis=1, inplace=True)\nprint('Size of Full dataset is: {}'.format(df_HR.shape))","b7b55428":"# Since we have class imbalance (i.e. more employees with turnover=0 than turnover=1)\n# let's use stratify=y to maintain the same ratio as in the training dataset when splitting the dataset\nX_train, X_test, y_train, y_test = train_test_split(df_HR,\n                                                    target,\n                                                    test_size=0.25,\n                                                    random_state=7,\n                                                    stratify=target)  \nprint(\"Number transactions X_train dataset: \", X_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions X_test dataset: \", X_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)","a0fe44f0":"# selection of algorithms to consider and set performance measure\nmodels = []\nmodels.append(('Logistic Regression', LogisticRegression(solver='liblinear', random_state=7,\n                                                         class_weight='balanced')))\nmodels.append(('Random Forest', RandomForestClassifier(\n    n_estimators=100, random_state=7)))\nmodels.append(('SVM', SVC(gamma='auto', random_state=7)))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('Decision Tree Classifier',\n               DecisionTreeClassifier(random_state=7)))\nmodels.append(('Gaussian NB', GaussianNB()))","2bade04c":"acc_results = []\nauc_results = []\nnames = []\n# set table to table to populate with performance results\ncol = ['Algorithm', 'ROC AUC Mean', 'ROC AUC STD', \n       'Accuracy Mean', 'Accuracy STD']\ndf_results = pd.DataFrame(columns=col)\ni = 0\n# evaluate each model using cross-validation\nfor name, model in models:\n    kfold = model_selection.KFold(\n        n_splits=10, random_state=7)  # 10-fold cross-validation\n\n    cv_acc_results = model_selection.cross_val_score(  # accuracy scoring\n        model, X_train, y_train, cv=kfold, scoring='accuracy')\n\n    cv_auc_results = model_selection.cross_val_score(  # roc_auc scoring\n        model, X_train, y_train, cv=kfold, scoring='roc_auc')\n\n    acc_results.append(cv_acc_results)\n    auc_results.append(cv_auc_results)\n    names.append(name)\n    df_results.loc[i] = [name,\n                         round(cv_auc_results.mean()*100, 2),\n                         round(cv_auc_results.std()*100, 2),\n                         round(cv_acc_results.mean()*100, 2),\n                         round(cv_acc_results.std()*100, 2)\n                         ]\n    i += 1\ndf_results.sort_values(by=['ROC AUC Mean'], ascending=False)","205242ee":"fig = plt.figure(figsize=(15, 7))\nfig.suptitle('Algorithm Accuracy Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(acc_results)\nax.set_xticklabels(names)\nplt.show()","92f374c5":"fig = plt.figure(figsize=(15, 7))\nfig.suptitle('Algorithm ROC AUC Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(auc_results)\nax.set_xticklabels(names)\nplt.show()","cfa5c78d":"kfold = model_selection.KFold(n_splits=10, random_state=7)\nmodelCV = LogisticRegression(solver='liblinear',\n                             class_weight=\"balanced\", \n                             random_state=7)\nscoring = 'roc_auc'\nresults = model_selection.cross_val_score(\n    modelCV, X_train, y_train, cv=kfold, scoring=scoring)\nprint(\"AUC score (STD): %.2f (%.2f)\" % (results.mean(), results.std()))","3f92785c":"param_grid = {'C': np.arange(1e-03, 2, 0.01)} # hyper-parameter list to fine-tune\nlog_gs = GridSearchCV(LogisticRegression(solver='liblinear', # setting GridSearchCV\n                                         class_weight=\"balanced\", \n                                         random_state=7),\n                      iid=True,\n                      return_train_score=True,\n                      param_grid=param_grid,\n                      scoring='roc_auc',\n                      cv=10)\n\nlog_grid = log_gs.fit(X_train, y_train)\nlog_opt = log_grid.best_estimator_\nresults = log_gs.cv_results_\n\nprint('='*20)\nprint(\"best params: \" + str(log_gs.best_estimator_))\nprint(\"best params: \" + str(log_gs.best_params_))\nprint('best score:', log_gs.best_score_)\nprint('='*20)","8cbb1b1c":"## Confusion Matrix\ncnf_matrix = metrics.confusion_matrix(y_test, log_opt.predict(X_test))\nclass_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","14440783":"print('Accuracy of Logistic Regression Classifier on test set: {:.2f}'.format(log_opt.score(X_test, y_test)*100))","a5542bec":"# Classification report for the optimised Log Regression\nlog_opt.fit(X_train, y_train)\nprint(classification_report(y_test, log_opt.predict(X_test)))","e27a2766":"log_opt.fit(X_train, y_train) # fit optimised model to the training data\nprobs = log_opt.predict_proba(X_test) # predict probabilities\nprobs = probs[:, 1] # we will only keep probabilities associated with the employee leaving\nlogit_roc_auc = roc_auc_score(y_test, probs) # calculate AUC score using test dataset\nprint('AUC score: %.3f' % logit_roc_auc)","025f02a7":"rf_classifier = RandomForestClassifier(class_weight = \"balanced\",\n                                       random_state=7)\nparam_grid = {'n_estimators': [50, 75, 100, 125, 150, 175],\n              'min_samples_split':[2,4,6,8,10],\n              'min_samples_leaf': [1, 2, 3, 4],\n              'max_depth': [5, 10, 15, 20, 25]}\n\ngrid_obj = GridSearchCV(rf_classifier,\n                        iid=True,\n                        return_train_score=True,\n                        param_grid=param_grid,\n                        scoring='roc_auc',\n                        cv=10)\n\ngrid_fit = grid_obj.fit(X_train, y_train)\nrf_opt = grid_fit.best_estimator_\n\nprint('='*20)\nprint(\"best params: \" + str(grid_obj.best_estimator_))\nprint(\"best params: \" + str(grid_obj.best_params_))\nprint('best score:', grid_obj.best_score_)\nprint('='*20)","f56fc25a":"importances = rf_opt.feature_importances_\nindices = np.argsort(importances)[::-1] # Sort feature importances in descending order\nnames = [X_train.columns[i] for i in indices] # Rearrange feature names so they match the sorted feature importances\nplt.figure(figsize=(15, 7)) # Create plot\nplt.title(\"Feature Importance\") # Create plot title\nplt.bar(range(X_train.shape[1]), importances[indices]) # Add bars\nplt.xticks(range(X_train.shape[1]), names, rotation=90) # Add feature names as x-axis labels\nplt.show() # Show plot","152e1001":"importances = rf_opt.feature_importances_\ndf_param_coeff = pd.DataFrame(columns=['Feature', 'Coefficient'])\nfor i in range(44):\n    feat = X_train.columns[i]\n    coeff = importances[i]\n    df_param_coeff.loc[i] = (feat, coeff)\ndf_param_coeff.sort_values(by='Coefficient', ascending=False, inplace=True)\ndf_param_coeff = df_param_coeff.reset_index(drop=True)\ndf_param_coeff.head(10)","1ed5fca0":"## Confusion Matrix\ncnf_matrix = metrics.confusion_matrix(y_test, rf_opt.predict(X_test))\nclass_names=[0,1] # name  of classes\nfig, ax = plt.subplots()\ntick_marks = np.arange(len(class_names))\nplt.xticks(tick_marks, class_names)\nplt.yticks(tick_marks, class_names)\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.1)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","da0ba6fd":"print('Accuracy of RandomForest Regression Classifier on test set: {:.2f}'.format(rf_opt.score(X_test, y_test)*100))","2bbc8061":"# Classification report for the optimised RF Regression\nrf_opt.fit(X_train, y_train)\nprint(classification_report(y_test, rf_opt.predict(X_test)))","e4f54124":"rf_opt.fit(X_train, y_train) # fit optimised model to the training data\nprobs = rf_opt.predict_proba(X_test) # predict probabilities\nprobs = probs[:, 1] # we will only keep probabilities associated with the employee leaving\nrf_opt_roc_auc = roc_auc_score(y_test, probs) # calculate AUC score using test dataset\nprint('AUC score: %.3f' % rf_opt_roc_auc)","ecaca933":"# Create ROC Graph\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, log_opt.predict_proba(X_test)[:,1])\nrf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test, rf_opt.predict_proba(X_test)[:,1])\nplt.figure(figsize=(14, 6))\n\n# Plot Logistic Regression ROC\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n# Plot Random Forest ROC\nplt.plot(rf_fpr, rf_tpr, label='Random Forest (area = %0.2f)' % rf_opt_roc_auc)\n# Plot Base Rate ROC\nplt.plot([0,1], [0,1],label='Base Rate' 'k--')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Graph')\nplt.legend(loc=\"lower right\")\nplt.show()","5db6ee25":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Problem-Definition\" data-toc-modified-id=\"Problem-Definition-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Problem Definition<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Project-Overview\" data-toc-modified-id=\"Project-Overview-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;<\/span>Project Overview<\/a><\/span><\/li><li><span><a href=\"#Problem-Statement\" data-toc-modified-id=\"Problem-Statement-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;<\/span>Problem Statement<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Dataset-Analysis\" data-toc-modified-id=\"Dataset-Analysis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Dataset Analysis<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Importing-Python-libraries\" data-toc-modified-id=\"Importing-Python-libraries-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;<\/span>Importing Python libraries<\/a><\/span><\/li><li><span><a href=\"#Importing-the-data\" data-toc-modified-id=\"Importing-the-data-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;<\/span>Importing the data<\/a><\/span><\/li><li><span><a href=\"#Data-Description-and-Exploratory-Visualisations\" data-toc-modified-id=\"Data-Description-and-Exploratory-Visualisations-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;<\/span>Data Description and Exploratory Visualisations<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;<\/span>Overview<\/a><\/span><\/li><li><span><a href=\"#Numerical-features-overview\" data-toc-modified-id=\"Numerical-features-overview-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;<\/span>Numerical features overview<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Feature-distribution-by-target-attribute\" data-toc-modified-id=\"Feature-distribution-by-target-attribute-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;<\/span>Feature distribution by target attribute<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Age\" data-toc-modified-id=\"Age-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;<\/span>Age<\/a><\/span><\/li><li><span><a href=\"#Education\" data-toc-modified-id=\"Education-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;<\/span>Education<\/a><\/span><\/li><li><span><a href=\"#Gender\" data-toc-modified-id=\"Gender-2.4.3\"><span class=\"toc-item-num\">2.4.3&nbsp;&nbsp;<\/span>Gender<\/a><\/span><\/li><li><span><a href=\"#Marital-Status\" data-toc-modified-id=\"Marital-Status-2.4.4\"><span class=\"toc-item-num\">2.4.4&nbsp;&nbsp;<\/span>Marital Status<\/a><\/span><\/li><li><span><a href=\"#Distance-from-Home\" data-toc-modified-id=\"Distance-from-Home-2.4.5\"><span class=\"toc-item-num\">2.4.5&nbsp;&nbsp;<\/span>Distance from Home<\/a><\/span><\/li><li><span><a href=\"#Department\" data-toc-modified-id=\"Department-2.4.6\"><span class=\"toc-item-num\">2.4.6&nbsp;&nbsp;<\/span>Department<\/a><\/span><\/li><li><span><a href=\"#Role-and-Work-Conditions\" data-toc-modified-id=\"Role-and-Work-Conditions-2.4.7\"><span class=\"toc-item-num\">2.4.7&nbsp;&nbsp;<\/span>Role and Work Conditions<\/a><\/span><\/li><li><span><a href=\"#Years-at-the-Company\" data-toc-modified-id=\"Years-at-the-Company-2.4.8\"><span class=\"toc-item-num\">2.4.8&nbsp;&nbsp;<\/span>Years at the Company<\/a><\/span><\/li><li><span><a href=\"#Years-With-Current-Manager\" data-toc-modified-id=\"Years-With-Current-Manager-2.4.9\"><span class=\"toc-item-num\">2.4.9&nbsp;&nbsp;<\/span>Years With Current Manager<\/a><\/span><\/li><li><span><a href=\"#Work-Life-Balance-Score\" data-toc-modified-id=\"Work-Life-Balance-Score-2.4.10\"><span class=\"toc-item-num\">2.4.10&nbsp;&nbsp;<\/span>Work-Life Balance Score<\/a><\/span><\/li><li><span><a href=\"#Pay\/Salary-Employee-Information\" data-toc-modified-id=\"Pay\/Salary-Employee-Information-2.4.11\"><span class=\"toc-item-num\">2.4.11&nbsp;&nbsp;<\/span>Pay\/Salary Employee Information<\/a><\/span><\/li><li><span><a href=\"#Employee-Satisfaction-and-Performance-Information\" data-toc-modified-id=\"Employee-Satisfaction-and-Performance-Information-2.4.12\"><span class=\"toc-item-num\">2.4.12&nbsp;&nbsp;<\/span>Employee Satisfaction and Performance Information<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Target-Variable:-Attrition\" data-toc-modified-id=\"Target-Variable:-Attrition-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;<\/span>Target Variable: Attrition<\/a><\/span><\/li><li><span><a href=\"#Correlation\" data-toc-modified-id=\"Correlation-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;<\/span>Correlation<\/a><\/span><\/li><li><span><a href=\"#EDA-Concluding-Remarks\" data-toc-modified-id=\"EDA-Concluding-Remarks-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;<\/span>EDA Concluding Remarks<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Pre-processing-Pipeline\" data-toc-modified-id=\"Pre-processing-Pipeline-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Pre-processing Pipeline<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Encoding\" data-toc-modified-id=\"Encoding-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Encoding<\/a><\/span><\/li><li><span><a href=\"#Feature-Scaling\" data-toc-modified-id=\"Feature-Scaling-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;<\/span>Feature Scaling<\/a><\/span><\/li><li><span><a href=\"#Splitting-data-into-training-and-testing-sets\" data-toc-modified-id=\"Splitting-data-into-training-and-testing-sets-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;<\/span>Splitting data into training and testing sets<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Building-Machine-Learning-Models\" data-toc-modified-id=\"Building-Machine-Learning-Models-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Building Machine Learning Models<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Baseline-Algorithms\" data-toc-modified-id=\"Baseline-Algorithms-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Baseline Algorithms<\/a><\/span><\/li><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>Logistic Regression<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Fine-tuning\" data-toc-modified-id=\"Fine-tuning-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;<\/span>Fine-tuning<\/a><\/span><\/li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;<\/span>Evaluation<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Random-Forest-Classifier\" data-toc-modified-id=\"Random-Forest-Classifier-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;<\/span>Random Forest Classifier<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Fine-tuning\" data-toc-modified-id=\"Fine-tuning-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;<\/span>Fine-tuning<\/a><\/span><\/li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;<\/span>Evaluation<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#ROC-Graphs\" data-toc-modified-id=\"ROC-Graphs-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;<\/span>ROC Graphs<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Concluding-Remarks\" data-toc-modified-id=\"Concluding-Remarks-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Concluding Remarks<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Risk-Category\" data-toc-modified-id=\"Risk-Category-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>Risk Category<\/a><\/span><\/li><li><span><a href=\"#Strategic-Retention-Plan\" data-toc-modified-id=\"Strategic-Retention-Plan-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>Strategic Retention Plan<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>","c793ab6a":"> Let's plot a heatmap to visualize the correlation between Attrition and these factors.","3072adf4":"> Distance from home for employees to get to work varies from 1 to 29 miles. There is no discernable strong correlation between Distance from Home and Attrition Status as per the KDE plot below.","bfd6f475":"#### Pay\/Salary Employee Information","e220370c":"# Employee Churn Model with a Strategic Retention Plan: a HR Analytics Case Study","912af287":"### Strategic Retention Plan","e0ad5d2f":"### Problem Statement","f7da3c22":"## Concluding Remarks","394b33d7":"### Feature Scaling","6273137b":"#### Role and Work Conditions","0e6532f4":"A strategic **\"Retention Plan\"** should be drawn for each **Risk Category** group. In addition to the suggested steps for each feature listed above, face-to-face meetings between a HR representative and employees can be initiated for **medium-** and **high-risk employees** to discuss work conditions. Also, a meeting with those employee's Line Manager would allow to discuss the work environment within the team and whether steps can be taken to improve it.","fdaf6df4":"> Let's evaluate each model in turn and provide accuracy and standard deviation scores","6ba6edb1":"#### Distance from Home","3cc0d112":"#### Years With Current Manager","edca4e03":"> The dataset features three marital status: Married (673 employees), Single (470 employees), Divorced (327 employees). <br>\nSingle employees show the largest proportion of leavers at 25%.","ce88bc7c":"![title](https:\/\/images.unsplash.com\/photo-1523006520266-d3a4a8152803?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1422&q=80)","79c6f16e":"> Relationship Satisfaction was captured as: 1 'Low', 2 'Medium', 3 'High', 4 'Very High'.","e9ff07e7":"> Several Education Fields are represented in the dataset, namely: Human Resources, Life Sciences, Marketing, Medical, Technical Degree, and a miscellaneous category Other. Here, I plot the normalized % of Leavers for each Education Field.","0c50ee1b":"> **Classification Accuracy** is the number of correct predictions made as a ratio of all predictions made. <br> \nIt is the most common evaluation metric for classification problems. However, it is often **misused** as it is only really suitable when there are an **equal number of observations in each class** and all predictions and prediction errors are equally important. It is not the case in this project, so a different scoring metric may be more suitable.","7b90c095":"> There is a feature for the number of companies the employee has worked at. <br>\n> 0 likely indicates that according to records, the employee has only worked at this company","7b974d46":"#### Age","816e30d7":"> Let's take a look at some of most significant correlations. It is worth remembering that correlation coefficients only measure linear correlations.","aa5c63d8":"### Logistic Regression","b07302a1":"> Let's create a kernel density estimation (KDE) plot colored by the value of the target. A kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. It will allow us to identify if there is a correlation between the Age of the Client and their ability to pay it back.","358c1ab8":"> AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. The green line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner). <br>","6711290a":"In this section, we undertake data pre-processing steps to prepare the datasets for Machine Learning algorithm implementation.","0c7cf88c":"### Data Description and Exploratory Visualisations","9549cfe3":"- The stronger indicators of people leaving include:\n    - **Monthly Income**: people on higher wages are less likely to leave the company. Hence, efforts should be made to gather information on industry benchmarks in the current local market to determine if the company is providing competitive wages.\n    - **Over Time**: people who work overtime are more likelty to leave the company. Hence efforts  must be taken to appropriately scope projects upfront with adequate support and manpower so as to reduce the use of overtime.\n    - **YearsWithCurrManager**: A large number of leavers leave 6 months after their Current Managers. By using Line Manager details for each employee, one can determine which Manager have experienced the largest numbers of employees resigning over the past year. Several metrics can be used here to determine whether action should be taken with a Line Manager: \n        - number of employees under managers showing high turnover rates: this would indicate that the organisation's structure may need to be revisit to improve efficiency\n        - number of years the Line Manager has been in a particular position: this may indicate that the employees may need management training or be assigned a mentor (ideally an Executive) in the organisation\n        - patterns in the employees who have resigned: this may indicate recurring patterns in employees leaving in which case action may be taken accordingly.\n    - **Age**: Employees in relatively young age bracket 25-35 are more likely to leave. Hence, efforts should be made to clearly articulate the long-term vision of the company and young employees fit in that vision, as well as provide incentives in the form of clear paths to promotion for instance.\n    - **DistanceFromHome**: Employees who live further from home are more likely to leave the company. Hence, efforts should be made to provide support in the form of company transportation for clusters of employees leaving the same area, or in the form of Transportation Allowance. Initial screening of employees based on their home location is probably not recommended as it would be regarded as a form of discrimination as long as employees make it to work on time every day.\n    - **TotalWorkingYears**: The more experienced employees are less likely to leave. Employees who have between 5-8 years of experience should be identified as potentially having a higher-risk of leaving.\n    - **YearsAtCompany**: Loyal companies are less likely to leave. Employees who hit their two-year anniversary should be identified as potentially having a higher-risk of leaving.","6a93daba":"> Random Forest helped us identify the Top 10 most important indicators (ranked in the table below).","0d819551":"> The data indicates that employees may have access to some Training. A feature indicates how many years it's been since the employee attended such training.","a04c52fa":"> Employee Performance Rating was captured as: 1 'Low' 2 'Good' 3 'Excellent' 4 'Outstanding'","68b09a1f":"### Target Variable: Attrition","0902febf":"> A feature related to \"Work-Life Balance\" was captured as: 1 'Bad' 2 'Good' 3 'Better' 4 'Best'. The data indicates that the largest normalised proportion of Leavers had \"Bad\" Work-Life Balance.","6845f377":"> Let's first use a range of **baseline** algorithms (using out-of-the-box hyper-parameters) before we move on to more sophisticated solutions. The algorithms considered in this section are: **Logistic Regression**, **Random Forest**, **SVM**, **KNN**, **Decision Tree Classifier**, **Gaussian NB**.","977466ed":"#### Education","6014bd79":"### Importing Python libraries","a8c834fb":"> The resulting dataframe has **49 columns** for 1,470 employees.","f016618d":"Understanding why and when employees are most likely to leave can lead to actions to improve employee retention as well as possibly planning new hiring in advance. I will be usign a step-by-step systematic approach using a method that could be used for a variety of ML problems. This project would fall under what is commonly known as \"**HR Anlytics**\", \"**People Analytics**\". <br>","26d0c08c":"#### Gender","94a71754":"> Feature Scaling using MinMaxScaler essentially shrinks the range such that the range is now between 0 and n. Machine Learning algorithms perform better when input numerical variables fall within a similar scale. In this case, we are scaling between 0 and 5.","669a6ecf":"> A few observations can be made based on the information and histograms for numerical features:\n - Many histograms are tail-heavy; indeed several distributions are right-skewed (e.g. MonthlyIncome DistanceFromHome, YearsAtCompany). Data transformation methods may be required to approach a normal distribution prior to fitting a model to the data.\n - Age distribution is a slightly right-skewed normal distribution with the bulk of the staff between 25 and 45 years old.\n - EmployeeCount and StandardHours are constant values for all employees. They're likely to be redundant features.\n - Employee Number is likely to be a unique identifier for employees given the feature's quasi-uniform distribution.","d1838aa4":"### Splitting data into training and testing sets","e6a71a2a":"**Random Forest** is a popular and versatile machine learning method that is capable of solving both regression and classification. Random Forest is a brand of Ensemble learning, as it relies on an ensemble of decision trees. It aggregates Classification (or Regression) Trees. A decision tree is composed of a series of decisions that can be used to classify an observation in a dataset.\n\nRandom Forest fits a number of decision tree classifiers on various **sub-samples of the dataset** and use **averaging** to improve the predictive accuracy and control over-fitting. Random Forest can handle a large number of features, and is helpful for estimating which of your variables are important in the underlying data being modeled.","487a30c9":"> All employees have a standard 80-hour work commitment","c7ba667a":"![title](https:\/\/images.unsplash.com\/photo-1498409785966-ab341407de6e?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1360&q=80)","701f6616":"> The data provided has no missing values. In HR Analytics, employee data is unlikely to feature large ratio of missing values as HR Departments typically have all personal and employment data on-file. However, the type of documentation data is being kept in (i.e. whether it is paper-based, Excel spreadhsheets, databases, etc) has a massive impact on the accuracy and the ease of access to the HR data.","8923ea7f":"> Let's take a closer look at using the Random Forest algorithm. I'll fine-tune the Random Forest algorithm's hyper-parameters by cross-validation against the AUC score.","a3bd10fc":"> **Area under ROC Curve** (or AUC for short) is a performance metric for binary classification problems. <br>\nThe AUC represents a **model\u2019s ability to discriminate between positive and negative classes**. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model as good as random.","07650b3d":"> The age distributions for Active and Ex-employees only differs by one year. <br>\nThe average age of ex-employees is **33.6** years old, while **37.6** is the average age for current employees.","6b8fe42b":"#### Overview","5bd48281":"> As shown above, \"Monthly Rate\", \"Number of Companies Worked\" and \"Distance From Home\" are positively correlated to Attrition; <br> while \"Total Working Years\", \"Job Level\", and \"Years In Current Role\" are negatively correlated to Attrition.","14a87c4d":"> The feature 'Attrition' is what this Machine Learning problem is about. We are trying to predict the value of the feature 'Attrition' by using other related features associated with the employee's personal and professional history. ","3654297f":"> Some employees have overtime commitments. The data clearly show that there is significant larger portion of employees with OT that have left the company.","9664f31f":"Let's summarise the findings from this EDA: <br>\n\n> - The dataset does not feature any missing or erroneous data values, and all features are of the correct data type. <br>\n- The strongest positive correlations with the target features are: **Performance Rating**, **Monthly Rate**, **Num Companies Worked**, **Distance From Home**. \n- The strongest negative correlations with the target features are: **Total Working Years**, **Job Level**, **Years In Current Role**, and **Monthly Income**.\n- The dataset is **imbalanced** with the majoriy of observations describing Currently Active Employees. <br>\n- Several features (ie columns) are redundant for our analysis, namely: EmployeeCount, EmployeeNumber, StandardHours, and Over18. <br>\n\nOther observations include: <br>\n> - Single employees show the largest proportion of leavers, compared to Married and Divorced counterparts. <br>\n- About 10% of leavers left when they reach their 2-year anniversary at the company. <br>\n- Loyal employees with higher salaries and more responsbilities show lower proportion of leavers compared to their counterparts. <br>\n- People who live further away from their work show higher proportion of leavers compared to their counterparts.<br>\n- People who travel frequently show higher proportion of leavers compared to their counterparts.<br>\n- People who have to work overtime show higher proportion of leavers compared to their counterparts.<br>\n- Employee who work as Sales Representatives show a significant percentage of Leavers in the submitted dataset.<br>\n- Employees that have already worked at several companies previously (already \"bounced\" between workplaces) show higher proportion of leavers compared to their counterparts.<br>","931971b8":"### Correlation","b2181bf2":"![title](https:\/\/images.unsplash.com\/photo-1441422454217-519d3ee81350?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1489&q=80)","42ed1319":"#### Fine-tuning","8c0634ae":"#### Years at the Company","a9754fe4":"> Let's take a closer look at using the Logistic Regression algorithm. I'll be using 10 fold Cross-Validation to train our Logistic Regression Model and estimate its AUC score.","571fc474":"## Pre-processing Pipeline","481cce33":"> The Confusion matrix provides us with a much more detailed representation of the accuracy score and of what's going on with our labels - we know exactly which\/how labels were correctly and incorrectly predicted","5b9008f1":"In this case study, a HR dataset was sourced from [IBM HR Analytics Employee Attrition & Performance](https:\/\/www.ibm.com\/communities\/analytics\/watson-analytics-blog\/hr-employee-attrition\/) which contains employee data for 1,470 employees with various information about the employees. I will use this dataset to predict when employees are going to quit by understanding the main drivers of employee churn. <br>\n\nAs stated on the [IBM website](https:\/\/www.ibm.com\/communities\/analytics\/watson-analytics-blog\/hr-employee-attrition\/) *\"This is a fictional data set created by IBM data scientists\"*. Its main purpose was to demonstrate the IBM Watson Analytics tool for employee attrition.","cf332029":"#### Evaluation","e84d1512":"> Based on our ROC AUC comparison analysis, **Logistic Regression** and **Random Forest** show the highest mean AUC scores. We will shortlist these two algorithms for further analysis. See below for more details on these two algos.","8fb42594":"> The data features employee data from three departments: Research & Development, Sales, and Human Resources.","40b0c089":"## Problem Definition","78f2bf99":"Employee turn-over (also known as \"employee churn\") is a costly problem for companies. The true cost of replacing an employee\ncan often be quite large. A study by the [Center for American Progress](https:\/\/www.americanprogress.org\/wp-content\/uploads\/2012\/11\/CostofTurnover.pdf) found that companies typically pay about one-fifth of an employee\u2019s salary to replace that employee, and the cost can significantly increase if executives or highest-paid employees are to be replaced. In other words, the cost of replacing employees for most employers remains significant. This is due to the amount of time spent to interview and find a replacement, sign-on bonuses, and the loss of productivity for several months while the new employee gets accustomed to the new role. <br>","56cc3a37":"### Baseline Algorithms","cd6c9584":"### Project Overview","edea21f0":"#### Department","f9e60733":"> As shown on the chart above, we see this is an imbalanced class problem. Indeed, the percentage of Current Employees in our dataset is 83.9% and the percentage of Ex-employees is: 16.1%\n\n> Machine learning algorithms typically work best when the number of instances of each classes are roughly equal. We will have to address this target feature imbalance prior to implementing our Machine Learning algorithms.","6909e0b0":"![title](https:\/\/cdn-images-1.medium.com\/max\/1600\/0*vRhSdZ_k4wrP6Bl8.jpg)","f639a85e":"> Machine Learning algorithms can typically only have numerical values as their predictor variables. Hence Label Encoding becomes necessary as they encode categorical labels with numerical values. To avoid introducing feature importance for categorical features with large numbers of unique values, we will use both Lable Encoding and One-Hot Encoding as shown below.","812549c1":"> Let's import the dataset and make of a copy of the source file for this analysis. <br> The dataset contains 1,470 rows and 35 columns.","f8431047":"### Random Forest Classifier","d7de1c9e":"> The dataset contains several numerical and categorical columns providing various information on employee's personal and employment details.","7ead2505":"#### Numerical features overview","e023687a":"> A preliminary look at the relationship between Business Travel frequency and Attrition Status shows that there is a largest normalized proportion of Leavers for employees that travel \"frequently\". Travel metrics associated with Business Travel status were not disclosed (i.e. how many hours of Travel is considered \"Frequent\").","d77883e3":"![title](https:\/\/images.unsplash.com\/photo-1535017584024-2f4bead257df?ixlib=rb-1.2.1&auto=format&fit=crop&w=1350&q=80)","38d4753c":"> GridSearchCV allows use to fine-tune hyper-parameters by searching over specified parameter values for an estimator.","a894ddb4":"> Instead of getting binary estimated target features (0 or 1), a probability can be associated with the predicted target. <br> The output provides a first index referring to the probability that the data belong to **class 0** (employee not leaving), and the second refers to the probability that the data belong to **class 1** (employee leaving).\n\n> The resulting AUC score is higher than that best score during the optimisation step. Predicting probabilities of a particular label provides us with a measure of how likely an employee is to leave the company.","3cf9d12c":"> In this section, we will provide data visualizations that summarizes or extracts relevant characteristics of features in our dataset. Let's look at each column in detail, get a better understanding of the dataset, and group them together when appropriate.","c235afe6":"### Encoding","8fa83dfa":"As the company generates more data on its employees (on New Joiners and recent Leavers) the algorithm can be re-trained using the additional data and theoritically generate more accurate predictions to identify **high-risk employees** of leaving based on the probabilistic label assigned to each feature variable (i.e. employee) by the algorithm.","e6216f47":"### Feature distribution by target attribute","e5d94001":"Employees can be assigning a \"Risk Category\" based on the predicted label such that:\n- **Low-risk** for employees with label < 0.6\n- **Medium-risk** for employees with label between 0.6 and 0.8\n- **High-risk** for employees with label > 0.8 <br>","e0988a15":"> Prior to implementating or applying any Machine Learning algorithms, we must decouple training and testing datasets from our master dataframe.","f279f049":"#### Marital Status","09513063":"> Environment Satisfaction was captured as: 1 'Low' 2 'Medium' 3 'High' 4 'Very High'. <br> \nProportion of Leaving Employees decreases as the Environment Satisfaction score increases.","27a8a22c":"#### Evaluation","a17f0998":"> Gender distribution shows that the dataset features a higher relative proportion of male ex-employees than female ex-employees, with normalised gender distribution of ex-employees in the dataset at 17.0% for Males and 14.8% for Females.","dfb0595d":"#### Work-Life Balance Score","3e020e16":"In this study, we will attempt to solve the following problem statement is: <br>\n> ** What is the likelihood of an active employee leaving the company? <br>\nWhat are the key indicators of an employee leaving the company? <br>\nWhat policies or strategies can be adopted based on the results to improve employee retention? **\n\nGiven that we have data on former employees, this is a standard **supervised classification problem** where the label is a binary variable, 0 (active employee), 1 (former employee). In this study, our target variable Y is the probability of an employee leaving the company. <br>","ca63beb1":"### EDA Concluding Remarks","37b3e6ce":"## Building Machine Learning Models","b3f6e8e3":"### Importing the data","185d3fd4":"## Dataset Analysis","2e7254bf":"> Job Satisfaction was captured as: 1 'Low' 2 'Medium' 3 'High' 4 'Very High'. <br> \nProportion of Leaving Employees decreases as the Job Satisfaction score increases.","41a93dd7":"### ROC Graphs","82088a8b":"* **I hope you enjoyed reading this Kernel as much as I had writing it. **","3ace38ea":"#### Employee Satisfaction and Performance Information","6bf22cfc":"> Employees have an assigned level within the organisation which varies from 1 (staff) to 5 (managerial\/director). Employees with an assigned Job Level of \"1\" show the largest normalized proportion of Leavers.","92bdab6a":"#### Fine-tuning","82f2e712":"### Risk Category","c1c28365":"> Random Forest allows us to know which features are of the most importance in predicting the target feature (\"attrition\" in this project). Below, we plot features by their importance.","1961d273":"> Several Job Roles are listed in the dataset: Sales Executive, Research Scientist, Laboratory Technician, Manufacturing Director, Healthcare Representative, Manager, Sales Representative, Research Director, Human Resources.","eb1122e5":"> As shown above, the results from GridSearchCV provided us with fine-tuned hyper-parameter using ROC_AUC as the scoring metric.","19185ef6":"> The Confusion matrix provides us with a much more detailed representation of the accuracy score and of what's going on with our labels - we know exactly which\/how labels were correctly and incorrectly predicted","2dfe5231":"> A ranking is associated to the employee's Job Involvement :1 'Low' 2 'Medium' 3 'High' 4 'Very High'. The plot below indicates a negative correlation with the Job Involvement of an employee and the Attrition Status. In other words, employees with higher Job Involvement are less likely to leave.","e6daf65d":"> The resulting AUC score is higher than that best score during the optimisation step. Predicting probabilities of a particular label provides us with a measure of how likely an employee is to leave the company.","f393c70e":"> As shown above, the fine-tuned Logistic Regression model showed a higher AUC score compared to the Random Forest Classifier. <br>","14969fcc":"**Logistic Regression** is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. Logistic Regression is classification algorithm that is not as sophisticated as the ensemble methods or boosted decision trees method discussed below. Hence, it provides us with a good benchmark. "}}