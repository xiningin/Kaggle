{"cell_type":{"14f4ab61":"code","bae47716":"code","c3b98908":"code","48534c3a":"code","dcc130ed":"code","50ce2b9f":"code","53041a72":"code","dd6182b4":"code","13205cc6":"code","530746b0":"code","806553c0":"code","30ebf9b3":"code","44eb9c46":"code","69aff655":"code","6c064091":"code","66873923":"code","75d20906":"code","89a56b33":"code","53b69301":"code","be1efbad":"code","ae226f95":"code","19390c6f":"code","6ad90269":"markdown","acaeb675":"markdown","d86fd9da":"markdown","93b14434":"markdown","f968e2d3":"markdown","2686b106":"markdown","df5b0994":"markdown","ce8b040d":"markdown","c12c7a10":"markdown","35fb9d86":"markdown","8eb7d4c2":"markdown"},"source":{"14f4ab61":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bae47716":"import pandas as pd\n\n## Train set\ntr = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv')\ntr.head(5)","c3b98908":"## Validation data set\nval = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-severity-rating\/validation_data.csv')\nval.head(5)","48534c3a":"## Sample submission data set\nsub = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-severity-rating\/sample_submission.csv')\nsub.head(5)\n","dcc130ed":"## Top 5 less toxic comments\n\nval['less_toxic'].value_counts().head(5)\n","50ce2b9f":"tr['text'].unique()","53041a72":"# ## There are several unwanted punctuation marks in the text.\n# ## Some comments have repeated dots and words. We have to clean them\n\n# tr['text'] = tr['text'].apply(lambda x:x.split('...')[0])\n# tr['text']","dd6182b4":"import re\nimport string\ndef clean(d):\n    ## lowercase the reviews\n    d = d.apply(lambda x:x.lower())\n    ## remove punctuation marks\n    d = d.apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))\n    # Removing extra spaces\n    d = d.apply(lambda x: re.sub(' +',' ',x))\n    ## Remove line breaks\n    d = d.replace('\\n',' ').replace('\\r',' ').replace('...',' ')\n    # Remove special characters\n    d = d.apply(lambda x:re.sub('[^a-zA-z0-9\\s]','',x))\n\n    ## Look at the text after cleaning \n    d.head(5)","13205cc6":"clean(tr['text'])","530746b0":"import spacy\n\nnlp = spacy.load('en_core_web_sm',disable=['parser', 'ner'])\n\n# Lemmatization with stopwords removal\ntr['text']=tr['text'].apply(lambda x: ' '.join([token.lemma_ for token in list(nlp(x)) if (token.is_stop==False)]))","806553c0":"tr['text'].head(5)","30ebf9b3":"from textblob import TextBlob\ntr['polarity'] = tr['text'].apply(lambda x: round(TextBlob(x).sentiment.polarity),2)\nprint(\" 3 Comments which are positive (highest polarity)\")\nfor index, t in enumerate(tr.iloc[tr['polarity'].sort_values(ascending = True)[:3].index]['text']):\n    print(index+1,t,'\\n')\n# tr[['polarity','text']].head(3)\ntr['polarity_category'] = ['positive' if score > 0\n                           else 'negative' if score < 0 \n                              else 'neutral' \n                                  for score in tr['polarity']]\n","44eb9c46":"tr.groupby(by=['polarity_category']).describe()","69aff655":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.barplot(x = 'polarity_category', y ='polarity',data = tr)\nplt.title('Sentiment analysis of comments')","6c064091":"tr.head(5)","66873923":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nt = Tokenizer()\ninput_text = tr['text'].values\nt.fit_on_texts(input_text)\ntxt_sequences = t.texts_to_sequences(input_text)\ntxt_vectors = pad_sequences(txt_sequences, maxlen = 512)\ntxt_vectors.shape\n\nprint('Text tokens count ',len(t.word_index))\n","75d20906":"import tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\nfrom tensorflow.keras.losses import BinaryCrossentropy\nvocab_size = 60000\nembedding_dim = 100\nmodel = Sequential([\n    Embedding(vocab_size, embedding_dim,name='embedding'),\n    LSTM(64),\n    Dropout(0.2),\n    Dense(16,activation = 'relu'),\n    Dropout(0.2),\n    Dense(1,activation = 'sigmoid')\n])\nmodel.compile(optimizer='adam',loss=BinaryCrossentropy(from_logits = True),\n             metrics = ['accuracy'])\nmodel.summary()","89a56b33":"from tensorflow.keras.utils import plot_model\n\nplot_model(model)","53b69301":"from keras.callbacks import ModelCheckpoint, EarlyStopping\nimport datetime\n\ncp_file = '.\/lstm_model.h5'\ncp = ModelCheckpoint(cp_file, \n                     monitor='loss', \n                     verbose=0, \n                     save_best_only=True, mode='min')\n\nes = EarlyStopping(patience=3, \n                   monitor='loss', \n                   #restore_best_weights=True, \n                   mode='min', \n                   verbose=1)\nlogdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\ntensorboard_callback = tf.keras.callbacks.TensorBoard(\"logs\")\n\n# model train\nhistory = model.fit(txt_vectors,tr['polarity'].values,\n                    batch_size= 500, \n                    epochs= 10,\n                    validation_split=0.1,\n                    callbacks=[es, cp, tensorboard_callback],\n                    shuffle=True,\n                    )\n\n\n\n","be1efbad":"\n# %reload_ext tensorboard\n# !kill 302\n# %tensorboard --logdir logs","ae226f95":"preds = model.predict(txt_vectors)\nplt.hist(preds,label='Model Prediction')\nplt.legend()","19390c6f":"from scipy.stats import rankdata\nu = pd.DataFrame()\nu['comment_id'] = tr['comment_id']\nu['score'] = rankdata(preds)\nu.to_csv('submission.csv',index=False)","6ad90269":"# Load dataset","acaeb675":"Introducing the feature Polarity to score toxicity of comments.\n\nSentiment analysis is the analysis of how much a piece of text is positive and opinionated.\n","d86fd9da":"# Submission","93b14434":"# Prediction","f968e2d3":"# Data cleaning","2686b106":"## Stopwords removal & Lemmatization\n\nStopwords are the most commonly occuring words in the text carrying no meaning. Ex: 'I', 'This','on','there','here','is','in'.\n\nWe will use spacy , a nltk library to remove stopwords from the cleaned train set.\n\nLemmatization removes inflectional endings only and returns the base or dictionary form of a word, which is known as the lemma .\nEx: rained, rainning, has rained, going to rain will be reduced to 'rain'","df5b0994":"# EDA on Comments to score dataset","ce8b040d":"## Build the deep learning model\n\nAn lstm model can remember, learn and memorise sequences of padded vectors.","c12c7a10":"# Text Tokenization\n\nTokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.","35fb9d86":"# Feature engineering","8eb7d4c2":"# Train the model\n\n"}}