{"cell_type":{"ca9390f1":"code","21af8894":"code","ce761f17":"code","bb2dd3bc":"code","61501394":"code","aadaf171":"code","4f3693fc":"code","0209e517":"code","fc7cbe4c":"code","11ee3542":"code","5d65e548":"code","6d6ecc56":"code","210de5dd":"code","1202ac9d":"code","70af9139":"code","a2e3d5bf":"code","d582677a":"code","78e4c50d":"code","8d48a0ce":"code","d163dcfa":"code","3dd4b9dd":"code","31a82c3e":"code","3d260e22":"code","0cd0c443":"code","939abf62":"code","759630dc":"code","69483d7a":"code","cd2097fe":"code","79bc9403":"code","e619af5b":"code","9b5c032c":"code","91c5de30":"code","5d134d9f":"code","fdbda6da":"code","bb5ae34f":"code","8e6b73aa":"code","c04d8b56":"markdown","49f7576f":"markdown","86d5331f":"markdown","e1891d06":"markdown","1289b2f0":"markdown","02bf29a3":"markdown","09f81692":"markdown","fbf32c26":"markdown","078ed85d":"markdown","1cd7732f":"markdown","e50c7697":"markdown","994c9760":"markdown","5c70f67a":"markdown","f429a060":"markdown","faa69ae2":"markdown","4a6e0e43":"markdown","24ba9d8b":"markdown","4ae2f7c1":"markdown","4e3edd5d":"markdown","385e95d9":"markdown","11051f6e":"markdown","18458738":"markdown","a4a5ff58":"markdown","9e4b7dcd":"markdown","147a13a4":"markdown","7a9833ae":"markdown","02394977":"markdown","63d93a8f":"markdown","09af21f2":"markdown","122c4cd3":"markdown","9b17d5ce":"markdown","22af37af":"markdown","b9f9d59f":"markdown","fedc3747":"markdown","25711d3b":"markdown","693dd31a":"markdown","f963fd0c":"markdown","cbc126c3":"markdown","db418d0d":"markdown","9b4865f4":"markdown","82717ac1":"markdown","d6e14003":"markdown","7b8f4ceb":"markdown","50375370":"markdown","ca928d70":"markdown","e9b0c754":"markdown","0a4e8f73":"markdown","a9739c8f":"markdown","16e79b67":"markdown","e93aef66":"markdown","ad676710":"markdown","68709907":"markdown","448a66da":"markdown","621574f8":"markdown","716a7f4c":"markdown","ea90ffdc":"markdown","8b4a8169":"markdown","49cf41a0":"markdown"},"source":{"ca9390f1":"import numpy as np #do roznych obliczen\nimport pandas as pd #do operacji na danych\nimport matplotlib.pyplot as plt #do wykresow\nimport seaborn as sns #do jeszcze ladniejszych wykresow\n%matplotlib inline \npd.set_option('display.max_columns', 500)","21af8894":"#Wczytujemy dane treningowe i testowe w postaci *.csv.\n#header=0 - informuje, gdzie znajduje si\u0119 wiersz z nazwami kolumn\n#index_col=0 - informuje, gdzie znajduje si\u0119 kolumna, po kt\u00f3rej indeksujemy nasz zbi\u00f3r\ndf_train = pd.read_csv('..\/input\/train.csv', header=0,index_col=0)\ndf_test = pd.read_csv('..\/input\/test.csv', header=0,index_col=0)\n\n#Polaczenie zbioru treningowego z testowym, by mozna je bylo razem modyfikowac\ndf_all=pd.concat([df_train, df_test], sort=False)\n\n#Nadanie nazw dataframe'om\ndf_train.name='Train Dataset'\ndf_all.name='All Dataset'\ndf_test.name='Test Dataset'","ce761f17":"#Funkcja ulatwiajaca wyswietlanie informacji o zbiorze w konsoli\ndef print_some_info(df):\n    '''Funkcja do wyswietlania informacji o datasecie w formie zbiorczej'''\n    \n    names_print=['Przykladowy sampel', 'Zestawienie nazw kolumn, liczby wystapien i typow danych', 'Wymiar datasetu',\n                 'Brak danych w poszczegolnych kolumnach']\n    n_len=[int((100-len(name))\/2) for name in names_print ]\n    \n    print('\\n \\n Zestawienie dla datasetu: '+ '*'*len(df.name) + df.name  + '*'*len(df.name) )\n    \n    print('\\n'+'~'*n_len[0] + names_print[0]  + '~'*n_len[0]+'\\n')\n    print(df.sample(4))\n    print('\\n'+'~'*n_len[1] + names_print[1]  + '~'*n_len[1]+'\\n')\n    print(df.info())\n    print('\\n'+'~'*n_len[2] + names_print[2]  + '~'*n_len[2]+'\\n')\n    print(df.shape)\n    print('\\n'+'~'*n_len[3] + names_print[3]  + '~'*n_len[3]+'\\n')\n    print(df.isnull().sum())\ndf_all.describe\n\n#Wywolanie funkcji dla datesetow\nprint_some_info(df_all)\nprint_some_info(df_train)\nprint_some_info(df_test)\n\n#Inne przydatne funkcje do wstepnej analizy zbioru:\n#df.head(n) #pokazuje n pierwszych wierszy df\n#df.tail(n) #pokazuje n ostatnich wierszy df\n#df.describe() #opis statystyczny\n#df[-5:], df[:10] #filtrowanie po wierszach (5 ostatnich, 10 pierwszych)\n#df[[\"Age\",\"Pclass\"]][5:30] #filtrowanie po wierszach z wyborem kolumn\n#df[(df['Age'] > 5.0) & (df['Age'] < 7.0 ) ] #filtorwanie po kolumnie\n#df[(df['Cabin'].str.contains('B2',na=False)) ] #flitrowanie po tekscie\n#df[df['Embarked'].isnull()] #filtrowanie po pustych wartosciach\n#df.select_dtypes(include=[np.int, np.float]).head() #flitrowanie po typie danych\n#df.groupby(['Pclass','Sex'])['Survived'].sum() # grupowanie po kategorii","bb2dd3bc":"#Sprawdzam wartosci jakim zakodowana jest plec\nprint('~~~Unikalne wartosci dla kategorii Sex~~~')\nprint(df_all.Sex.unique())\nprint('\\n')\n\n#Sprawdzam czy nie ma brakujacych danych w zbiorze\nprint('~~~Ilosc pustych danych w zbiorze~~~')\nprint(df_train.Sex.isnull().sum())\nprint('\\n')\n\n#Sprawdzam liczbe mezczyzn i kobiet na statku (test+train)\nprint('~~~Liczba mezczyzn i kobiet na statku (test+train)~~~')\nprint(df_all.Sex.value_counts())\nprint('\\n')\n\n#Sprawdzam liczbe mezczyzn i kobiet na statku (tylko train)\nprint('~~~Liczba mezczyzn i kobiet na statku (train)~~~')\nprint(df_train.Sex.value_counts())\nprint('\\n')\n\n#Udzial kobiet i mezczyzn wsrod tych, ktorzy przetrwali\nprint('~~~Liczba mezczyzn i kobiet wsrod uratowanych~~~')\nprint(df_train.groupby(['Sex'])['Survived'].sum())\nprint('\\n')\n\n#Stosunek przezywalnosci kabiet i mezczyzn\nprint('~~~Stosunek przezywalnosc kobiet i mezczyzn~~~')\nprint(df_train.groupby(['Sex'])['Survived'].mean()*100)\nprint('\\n')\n\n","61501394":"#Plotowanie wykresu z zaleznoscia przyzywalnosci od plci\nplt.figure(figsize=(4,4))\nsns.set_style(\"whitegrid\")\nax = sns.barplot(x=\"Sex\", y=\"Survived\", data=df_train).set(title = 'Stosunek prze\u017cywalno\u015bci w zale\u017cno\u015bci od p\u0142ci',\n                                                           xlabel = 'P\u0142e\u0107', ylabel = 'Stosunek prze\u017cywalno\u015bci',\n                                                           xticklabels=['M\u0119\u017cczyzna', 'Kobieta'])","aadaf171":"#Zastapienie nazw plci 'male' przez 0, 'female' przez 1\ndef replace_sex_names_with_code(df):\n    df.Sex.replace(['male', 'female'], [0, 1], inplace = True)\n    return (df)\n\nfor i in [df_all, df_train, df_test]:\n    replace_sex_names_with_code(i)\n\n#Inne sposoby zakodowania danych\n#df_train['_Sex'] = pd.Categorical(df_train.Sex).codes #utworzenie nowej kolumny z zakodowanymi plciami\n#sprawdz get_dummies()\n#dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n#df['sex_cat'] = pd.factorize( df['Sex'] )[0]","4f3693fc":"#Sprawdzam wartosci jakim zakodowana jest klasa\nprint('~~~Unikalne wartosci dla kategorii Pclass~~~')\nprint(df_all.Pclass.unique())\nprint('\\n')\n\n#Sprawdzam czy nie ma brakujacych danych w zbiorze\nprint('~~~Ilosc pustych danych w zbiorze~~~')\nprint(df_all.Pclass.isnull().sum())\nprint('\\n')\n\n#Liczba pasazerow poszczegolnych klas (zbior train)\nprint('~~~Liczba pasazerow w danej klasie (train)~~~')\nprint(df_train.Pclass.value_counts())\nprint('\\n')\n\n#Udzial poszczegolnych klas wsrod tych, ktorzy przetrwali\nprint('~~~Stosunek uratowanych z poszczegolnych klas~~~')\nprint(df_train.groupby(['Pclass'])['Survived'].mean()*100)\nprint('\\n')\n\n#Udzial poszczegolnych klas wsrod tych, ktorzy przetrwali z uwzglednieniem plci\nprint('~~~Stosunek uratowanych z poszczegolnych klas z uwzglednieniem plci~~~')\nprint(df_train.groupby(['Pclass', 'Sex'])['Survived'].mean()*100)\nprint('\\n')","0209e517":"#Plotowanie wykresu z zaleznoscia przyzywalnosci od plci\nfig, ax =plt.subplots(1,2, figsize=(10,5))\n#fig(figsize=(4,4))\nsns.set_style(\"whitegrid\")\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=df_train, ax=ax[0]).set(title = 'Stosunek prze\u017cywalno\u015bci w zale\u017cno\u015bci klasy',\n                                                           xlabel = 'Klasa', ylabel = 'Stosunek prze\u017cywalno\u015bci')\nsns.barplot(x=\"Pclass\", y=\"Survived\",hue='Sex', data=df_train, ax=ax[1]).set(title = 'Stosunek prze\u017cywalno\u015bci w zale\u017cno\u015bci klasy i p\u0142ci',\n                                                           xlabel = 'P\u0142e\u0107 i klasa', ylabel = 'Stosunek prze\u017cywalno\u015bci')\n                                                          ","fc7cbe4c":"print('~~~Przykladowy wydruk zmiennej Name~~~')\nprint(df_all.Name.head(4))\nprint('\\n')\n\n#Liczba unikalnych wartosci\nprint('~~~Liczba unikalnych wartosci dla kategorii Age~~~')\nprint(df_all.Name.nunique())\nprint('\\n')\n\n#Powtarzajace sie wartosci\nprint('~~~Powtarzajace sie wartosci~~~')\nprint(df_all[df_all.Name.duplicated(keep=False)])\nprint('\\n')\n\n#Sprawdzam czy nie ma brakujacych danych w zbiorze\nprint('~~~Ilosc pustych danych w zbiorze Name~~~')\nprint(df_all.Name.isnull().sum())\nprint('\\n')","11ee3542":"#Ze zbioru wyluskuje nazwiska (pierwszy czlon Name), oraz zapisuje je tylko za pomoca malych liter\n#by te same nazwiska nie byly traktowane jako rozne ze wzgledu na wielkosc liter\n#Nastepnie obliczam ile osob nosi takie samo nazwisko i drukuje przylad\nprint('\\n~~~Liczba o tym samym nazwisku (wydruk pierwszych 5)~~~')\nLastNameSum=df_train['Name'].map(lambda x: x.split(',')[0].lower()).value_counts().reset_index().rename(index=str, columns={\"index\": \"_Name\",\"Name\": \"counts\" })\nprint(LastNameSum.head(5))\n\n#Tworze nowa kolumne w DF o nazwie _Name, ktora przechowuje nazwiska pasazerow\ndf_train['_Name']=df_train['Name'].map(lambda x: x.split(',')[0].lower())\ndf_all['_Name']=df_all['Name'].map(lambda x: x.split(',')[0].lower())\n\n#Sprawdzam ile jest podzbiorow o danej liczbie osob, ktore tworza pseudo-rodzine (maja takie samo nazwisko), \n#ile jest pseudo-rodzin o liczbie czlonkow 9,8,7, etc.\nprint('\\n~~~Liczba osob o takim samym nazwisku | Liczba takich przypadkow~~~')\nprint(LastNameSum['counts'].value_counts())\n\n#Tworze zestawienie ilosci osob o takim samym nazwisku i ile osob przetrwalo\nLastNameSum=LastNameSum.sort_values('_Name').reset_index().set_index('_Name')\nLastNameSurvivedSum=df_train.groupby(['_Name'])['Survived'].sum().reset_index()\nLastNameSurvivedSum=LastNameSurvivedSum.sort_values('_Name').reset_index().set_index('_Name')\nLastNameResult = pd.concat([LastNameSum['counts'], LastNameSurvivedSum['Survived']], axis=1, join_axes=[LastNameSum.index]).sort_values('counts', ascending=False)\nprint('\\n~~~Liczba o tym samym nazwisku | Liczba tych co przetrwali (wydruk pierwszych 5)~~~~~~')\nprint(LastNameResult.head(5))\n\n#Tworze zestawienie ile osob jest w pozbiorze o danej wielkosci grupy (liczbie osob o tym samym nazwisku), ile z tych osob przetrwalo\nSurvivedDependLastNameSizeGroup=LastNameResult.groupby(['counts'])['Survived'].sum()\nPeopleNumberDependLastNameSizeGroup=LastNameResult.sort_values('counts')\nPeopleNumberDependLastNameSizeGroup=LastNameResult['counts'].value_counts(sort=False)\nPeopleNumberDependLastNameSizeGroup=PeopleNumberDependLastNameSizeGroup.index*PeopleNumberDependLastNameSizeGroup\nSurvivedPercent=SurvivedDependLastNameSizeGroup\/PeopleNumberDependLastNameSizeGroup*100\nLastNameResultWithNumbers = pd.concat([PeopleNumberDependLastNameSizeGroup, SurvivedDependLastNameSizeGroup, SurvivedPercent], axis=1, join_axes=[PeopleNumberDependLastNameSizeGroup.index]).rename(index=str, columns={0: \"All\",1: \"Survived [%]\" })\nprint('\\n~~~Rozmiar grupy o tym samym nazwisku | Wszyscy  | Uratowani | Uratowani w [%]~~~~~~')\nprint(LastNameResultWithNumbers)\n\n#Wykres rozkladu ilosci wszystkich pasazerow oraz tych co przetrwali w zaleznosci ile osob mialo takie samo nazwisko\nax=LastNameResultWithNumbers.plot(kind='bar',y=['All', 'Survived'], label=['Wszyscy', 'Uratowani'])\nplt.legend()\nax.set_xlabel('Liczba osob o takim samym nazwisku')\nax.set_ylabel('Suma osob')\nax.set_title('Rozklad liczby pasazerow o takim samym nazwisku')\nplt.show()","5d65e548":"#Sprawdzenie liczebnosci osob o konkretnym tytule\ndf_all['_Title']=df_all['Name'].map(lambda x: x.split(', ')[1].split('.')[0].lower() )\nprint('\\n~~~Ilosc osob o danym tytule~~~')\nprint(df_all['_Title'].value_counts())\n\n#Sprawdzenie rozkladu wieku w zaleznosci od tytuly\nprint('\\n~~~Tytul|Srednia|Mediana|Min|Max wieku w zaleznosci od tytulu~~~')\nprint(df_all.groupby('_Title')['Age'].agg([np.mean, np.median, np.min, np.max]))\n\n#Agregacja tytulow i ich zakodowanie\npopular_titles = [\"mr\", \"miss\", \"mrs\", \"master\", \"dr\", \"rev\"]\ndf_all['Title'] = df_all['_Title'].map(lambda x: x if x in popular_titles else \"other\")\ndf_all['Title_encoded'] = pd.factorize( df_all['Title'] )[0]\nprint('\\n~~~Ilosc osob o danym tytule po agregacji~~~')\nprint(df_all['Title'].value_counts())\n\n#Rozklad wieku po agregacji, zebranie mediany wieku, by uzupelnic puste pola w wieku\nprint('\\n~~~Tytul|Srednia|Mediana|Min|Max wieku w zaleznosci od tytulu~~~')\nprint(df_all.groupby('Title')['Age'].agg([np.mean, np.median, np.min, np.max]))\nmissing_ages=df_all.groupby('Title')['Age'].agg([np.mean, np.median, np.min, np.max]).to_dict()['median']\n\n#Sprawdzenie szans przezycia w zaleznosci od tytulu\nprint('\\n~~~Szansa przezycia w zaleznosci od tytulu~~~')\nprint(df_all.groupby('Title')['Survived'].agg([np.mean]))\n\n#Alternatywny sposob wyznaczenie tytulow\n#pat = r\",\\s([^ .]+)\\.?\\s+\"\n#df_all['Title'] =  df_all['Name'].str.extract(pat,expand=True)[0]\n#df_all.groupby('Title')['Title'].count()\n#df_all.loc[df_all['Title'].isin(['Mille','Ms','Lady']),'Title'] = 'Miss'\n#df_all.loc[df_all['Title'].isin(['Mme','Sir']),'Title'] = 'Mrs'\n#df_all.loc[~df_all['Title'].isin(['Miss','Master','Mr','Mrs']),'Title'] = 'Other' # NOT IN\n#df_all['_Title'] = pd.Categorical(df_all.Title).codes\n#df_all.groupby('Title')['Title'].count()","6d6ecc56":"#Sprawdzam wartosci jakim zakodowana jest klasa\nprint('~~~Unikalne wartosci dla kategorii Age~~~')\nprint(df_all.Age.unique())\nprint('\\n')\n\n#Liczba unikalnych wartosci\nprint('~~~Liczba unikalnych wartosci dla kategorii Age~~~')\nprint(df_all.Age.nunique())\nprint('\\n')\n\n\n#Sprawdzam czy nie ma brakujacych danych w zbiorze\nprint('~~~Ilosc pustych danych w zbiorze Age~~~')\nprint(df_all.Age.isnull().sum())\nprint('\\n')\n\n#Statystyka opisowa \nprint('~~~Statystyka opisowa wieku (Age)~~~')\nprint(df_all.Age.describe())\nprint('\\n')","210de5dd":"#Wyplotowanie histogramu z podzialem na 30 zakresow\nfig, ax=plt.subplots()\nplt_all=plt.hist(df_all['Age'],bins = 30,  range = [0,100],label='Wszyscy')\nplt_survived=plt.hist(df_all[df_all['Survived']==1]['Age'], bins=30, range=[0,100], label='Uratowani')\nplt.legend()\nax.set_xlabel('Wiek')\nax.set_ylabel('Liczba pasazerow')\nax.set_title('Rozklad wieku pasazerow')\nplt.show()","1202ac9d":"#Uzupelniam puste miejsca wiekiem z tytulow\ndf_all['Age'] = df_all.apply( lambda x: x['Age'] if str(x['Age']) != 'nan' else missing_ages[x['Title']], axis=1 )\nprint('\\n~~~Puste miejsca po imputacji w zmiennej Age~~~')\nprint(df_all.Age.isnull().sum())","70af9139":"#Podzial wieku na podzbiory do ciecia\nage_bins = [0,1, 3, 8, 15, 20,30, 40,60, 100]\ndf_all['Age_cut']=pd.cut(df_all[\"Age\"], bins=age_bins)\n\n#Ilosc osob w danym przedziale wiekowym\nprint('\\n~~~Ilosc osob w danym przedziale wiekowym~~~')\nprint(df_all['Age_cut'].value_counts())\n\n#Szansa przezycia od przedzialu wiekowego\nprint('\\n~~~Szansa przezycia w zaleznosci od przedzialu wieku~~~')\nprint(df_all.groupby('Age_cut')['Survived'].agg(np.mean))\n\n#Encoding grup wiekowych\nage_bins = [0,1, 2,3, 5,8,12, 15,18, 20,25,30,35, 40,50,60, 100]\ndf_all['Age_cut']=pd.cut(df_all[\"Age\"], bins=age_bins)\ndf_all['Age_encoded'] = pd.factorize( df_all['Age_cut'] )[0]","a2e3d5bf":"#Utworzenie nowej zmiennej Family_size\ndf_all['Family_size']=df_all['SibSp']+df_all['Parch']+1\n\nprint('\\n~~~Wielkosc rodziny | Liczba pasazerow~~~')\nprint(df_all.Family_size.value_counts())\n\nprint('\\n~~~Wielkosc rodziny | Szanse na przezycie~~~')\nprint(df_all.groupby('Family_size')['Survived'].mean())\n\n#Utworzenie zmiennej IsAlone informujacej czy pasazer podrozowal sam\ndf_all['IsAlone']=df_all.apply(lambda x: 1 if x['Family_size']==1 else 0, axis=1)\n\nprint('\\n~~~Zaleznosc miedzy podrozowaniem samemu a przezyciem')\nprint(df_all.groupby('IsAlone')['Survived'].mean())","d582677a":"print('\\n~~~Satystyka opisowa - Fare - Cena biletu~~~')\nprint(df_all.Fare.describe())\n\n#Wydrukowanie informacji o osobie, ktorej ceny biletu brakuje\nprint('\\n~~~Wiersz, w ktorym brakuje ceny biletu~~~')\nprint(df_all[df_all.Fare.isnull()][['Name', 'Pclass', 'Age', 'Embarked']])\n\n#Pozyskanie wartosci klasy, zaokretowania i oplaty\n#missPclass=df_all.loc[df_all.Fare.isnull(), 'Pclass'].values\n#missEmbarked=df_all.loc[df_all.Fare.isnull(), 'Embarked'].values\n#missFare_Age=df_all.loc[df_all.Fare.isnull(), 'Age'].values\n\n#Ceny biletow pasazerow o podobnej specyfice to tego, ktorego ceny brakuje\nprint('\\n~~~Sprawdzenie cen biletow pasazerow w podobnym wieku, klasie i zaokretowaniu~~~')\nval = df_all[(df_all['Pclass'] == 3)&(df_all['Embarked'] == 'S')&(df_all['Age'] > 60.5)][['Age','Fare']];\nprint(val.groupby('Age').agg(['min','max','count','mean','median']))\n\n#Obliczenie sredniej ceny biletow dla osob o podobnej specyfice do brakujacej\nval_mean=val.groupby('Age').agg(np.mean).mean()\nprint('\\n~~~Srednia cena biletu osob o podobnej specyfice={}'.format(val_mean.values))\n\n#Uzupelnieni pustych miejsc\ndf_all.loc[df_all.Fare.isnull(),'Fare']=val_mean.values\n\n\n#Wstawienie danych w konkretnym miejscu\n#df_all.at[1044,'Fare']=val_mean.values\n#df_all['Fare']=df_all.apply(lambda x: x['Fare'] if str(x['Fare']) != 'nan' else val_mean.values, axis=1 )","78e4c50d":"# Skalowanie Fare\nfrom sklearn import preprocessing\ndf_all['_Fare'] = preprocessing.scale(df_all[['Fare']])[:,0]\n#df_all['_Fare'] = preprocessing.normalize(df_all[['Fare']], norm='l2')\n\n#Wyplotowanie histogramu z Fare przed skalowaniem z podzialem na 100 zakresow\nfig, ax=plt.subplots()\nplt_all_fare=plt.hist(df_all['Fare'], bins=100)\nplt_survived_fare=plt.hist(df_all[df_all['Survived']==1]['Fare'], bins=100)\nplt.legend()\nax.set_xlabel('Fare - cena biletu')\nax.set_ylabel('Liczba pasazerow')\nax.set_title('Rozklad cen biletow')\nplt.show()\n\n#Wyplotowanie histogramu z Fare po skalowaniu z podzialem na 100 zakresow\nfig, ax=plt.subplots()\nplt_all_fare=plt.hist(df_all['_Fare'], bins=100)\nplt_survived_fare=plt.hist(df_all[df_all['Survived']==1]['_Fare'], bins=100)\nplt.legend()\nax.set_xlabel('Fare - cena biletu - po skalowaniu')\nax.set_ylabel('Liczba pasazerow')\nax.set_title('Rozklad cen biletow - po skalowaniu')\nplt.show()\n\n#Pociecie na przedzialy, by sprawdzic wplyw ceny biletu na przezycie\nfare_bins = [0,7,10, 20, 30, 50, 70,100,200,600]\ndf_all['Fare_cut']=pd.cut(df_all[\"Fare\"], bins=fare_bins)\n\n#Zakodowanie przedzialow, byc moze poprawia wynik\ndf_all['_Fare_encoded'] = df_all['Fare_cut'].cat.codes\n\n#Ilosc osob w danym przedziale ceny biletu\nprint('\\n~~~Ilosc osob w danym przedziale ceny biletu Fare~~~')\nprint(df_all['Fare_cut'].value_counts())\n\n#Ilosc osob w danym przedziale ceny biletu z uwzglednieniem klasy\nprint('\\n~~~Ilosc osob w danym przedziale ceny biletu Fare z uwzgledniem klasy~~~')\nprint(df_all.groupby('Fare_cut')['Pclass'].value_counts())\n\n#Szansa przezycia od przedzialu cenowego biletu\nprint('\\n~~~Szansa przezycia w zaleznosci od przedzialu cenowego biletu~~~')\nprint(df_all.groupby(['Fare_cut', 'Pclass'])['Survived'].agg(np.mean))","8d48a0ce":"print('\\n~~~Przykladowy wydruk informacji o Cabin~~~')\nprint(df_all.Cabin.sample(4))\n\n#Uzyskanie tylko typow kabin (pierwsz litera) + brakujace nazwane jako missing\ndf_all['_Cabin'] =df_all.Cabin.astype(str).str[0]\ndf_all.loc[df_all._Cabin=='n', '_Cabin']='missing'\n\nprint('\\n~~~Zestawienie typow Cabin (pierwsza litera)')\nprint(df_all._Cabin.value_counts())\n\nprint('\\n~~~Zestawienie typow Cabin i klas')\nprint(df_all.groupby('_Cabin')['Pclass'].value_counts())\n\n#Zakodowanie typow kabin\ndf_all['_Cabin_encoded']=pd.Categorical(df_all['_Cabin']).codes","d163dcfa":"#Sprawdzam dla jakich wierszy brakuje danych\nprint('\\n~~~Dla jakich wierszy brakuje zmiennej Embarked~~~')\nprint(df_all[df_all.Embarked.isnull()][['Name', 'Fare', 'Pclass', 'Ticket']])\n\n#Jakie zaokretowanie maja osoby o tym samym typie kabiny i podobnej cenie biletu\nprint('\\n~~~Jakie zaokretowanie maja osoby o tym samym typie kabiny i podobnej cenie biletu')\nprint(df_all[(df_all['_Cabin']=='B')&(df_all['Fare']>79)]['Embarked'].value_counts())\n\nprint('\\n~~~Ilosc pasazerow z konkretnych portow z podzialem na klasy')\nprint(df_all.groupby('Embarked')['Pclass'].value_counts())\n\nprint('\\n~~~Zestawienie portow zaokretowania, klasy i mediany ceny biletu~~~')\nprint(df_all.groupby(['Embarked','Pclass'])['Fare'].median())\n\nprint('\\n~~~Zestawienie portow zaokretowania i szans na przezycie~~~')\nprint(df_all.groupby(['Embarked'])['Survived'].mean())\n\nprint('\\n~~~Zestawienie portow zaokretowania i pasazerow w konkretnych typach kabin')\nprint(df_all.groupby(['Embarked'])['_Cabin'].value_counts())\n\n#Zakodowanie zmiennej Embarked - portow za pomoca int-ow\ndf_all['_Embarked_encoded']=pd.Categorical(df_all['Embarked']).codes\n","3dd4b9dd":"print('\\n~~~Przykladowe numery biletow~~~')\nprint(df_all.Ticket.sample(5))\n\nprint('\\n~~~Liczba unikalnych nr biletow~~~')\nprint(df_all.Ticket.nunique())\n\n#Zmienna ile osob ma taki sam nr biletu\ndf_all['_TicketCounts'] = df_all.groupby(['Ticket'])['Ticket'].transform('count')","31a82c3e":"#Wybranie kolumn, ktore beda uzyte do treningu\ncolumns_names=['Survived', 'Pclass', 'Sex', 'Age_encoded', 'SibSp', 'Parch', 'Title_encoded', 'Family_size','IsAlone','_Fare_encoded','_Cabin_encoded', '_Embarked_encoded', '_TicketCounts', '_Fare' ]\n\n#Sprawdzenie korelacji miedzy zmiennymi\ncorr = df_all[columns_names].corr()\n#print(corr)\n\n#Heatmap\nplt.figure(figsize=(8, 8))\nsns.heatmap(corr)","3d260e22":"#Import roznych modeli ML do testow\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn import linear_model\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Zebranie estymatow w jednym dictionarze\nclassifiers = {\n    'LogisticRegression':  linear_model.LogisticRegression(),\n    'SVC': SVC(class_weight='balanced'),\n    'LinearSVC':  LinearSVC(),\n    'GaussianNB': GaussianNB(),\n    'XGBoost': XGBClassifier(),#max_depth=3, n_estimators=15, subsample=0.8, random_state=2018),\n    'DecisionTree': DecisionTreeClassifier(),\n    'RandomForest': RandomForestClassifier(),#(n_estimators=100),\n    'KNeighbours': KNeighborsClassifier(),\n    'NeuralNetwork': MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)\n}","0cd0c443":"#Wybranie kolumn, ktore beda uzyte do wstepnych obliczen\ncols=['Pclass', 'Sex', 'Age_encoded', 'Family_size','_Fare', 'Title_encoded']#, 'IsAlone']#'_Fare_encoded']#,'_Cabin_encoded']\n\n#Wydzielenie zbioru treningowego z polaczonego datasetu\ndef get_train_data(df_all, cols):\n    SURV = 891\n    X = df_all[:SURV][cols] \n    Y = df_all[:SURV]['Survived']\n    return X,Y\n\nX,Y=get_train_data(df_all, cols)","939abf62":"#Chwilowe wylaczenie warningow, zeby nie zaburzaly widoku wykresow\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Import learning_curve, ktora pozwala na cross-validation (zwraca wyniki dla treningu, \n#testu dla roznych zbiorow treningowych i testowych) z dodatkowym uwzglednieniem rozmiaru zbioru treningowego\nfrom sklearn.model_selection import learning_curve\n\n#Import ShuffleSplit, ktory umozliwa losowe wybranie zbioru treningowego i walidacyjny do CV, z uwzgledniem\n#rozmiaru zbioru walidacyjnego\nfrom sklearn.model_selection import ShuffleSplit\n\n#Funkcja plot_learning_curve odpowiedzialna jest za wyplotowanie wykresow\n#krzywej treningowej oraz krzywej walidacyjnej, wykorzystujac learning_curves,\n#pokazuje jak zmienia sie wynik treningu i CV w zaleznosci od rozmiaru zbioru\n#treningowego, dodatkowo dodany, by zwracala wartosc srednia wyniku dla CV\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5), plot=True):\n    \n    '''Generate a simple plot of the test and training learning'''\n    if plot==True:\n        plt.figure()\n        plt.title(title)\n        if ylim is not None:\n            plt.ylim(*ylim)\n        plt.xlabel(\"Training examples\")\n        plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    if plot==True:\n        plt.grid()\n\n        plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n        plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n        plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n        plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n\n        plt.legend(loc=\"best\")\n    return plt, test_scores_mean","759630dc":"def test_models(classifiers, X, Y, plot=True):\n    '''Funkcja jako argument przyjmuje rodzaj modelu w slowniku, zbior X oraz zbior Y'''\n    test_score={} #przechowuje usrednione wyniki CV dla roznych modeli\n    for c in classifiers:\n        title = \"Learning Curves \" + c \n        cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0) #CV 100 iteracji z 20% losowym zbiorem testowym\n        estimator = classifiers[c] \n        ax, test_score[c] = plot_learning_curve(estimator, title, X, Y, ylim=(0.7, 1.01), cv=cv, n_jobs=4, plot=plot)\n        if plot==True:\n            plt.show()\n    print('\\n~~~Usrednione wyniki testu dla roznych modeli, z uwzgledniem wielkosci zbioru treningowego do 20% do 100% oraz srednia dla wszystkich treningow~~~')\n    df_scores=pd.DataFrame.from_dict(test_score).T.rename(index=str, columns={0: '20_Train', 1: '40_Train', 2: '60_Train', 3: '80_Train', 4: '100_Train'})\n    df_scores['Mean']=df_scores.mean(numeric_only=True, axis=1)\n    df_scores\n    return df_scores\n\ntest_models(classifiers,X, Y)","69483d7a":"#Wszystkie utworzone, zmodyfikowane, zeskalowane, zakodowane lub pierwotne kolumny(, ktore zawieraja tylko liczby calkowite)\ncols_all=['Pclass', 'Sex', 'Age_encoded', 'Family_size',\n          '_Fare', 'Title_encoded' ,'_Cabin_encoded','_Embarked_encoded',\n          '_TicketCounts', 'SibSp', 'Parch','IsAlone','_Fare_encoded']\n\n#Wydzielenie zbioru treningowego z polaczonego datasetu\nX,Y=get_train_data(df_all, cols_all)\n\n#Wynik XGBoosta dla wszystkich kolumn\nxgb_clas={'XGBoost': XGBClassifier(),}\ntest_models(xgb_clas, X, Y, plot=False)\n","cd2097fe":"#Plot Feature Importance korzystajac z wbudowanych funkcji XGBoosta\nfrom numpy import loadtxt\nfrom xgboost import plot_importance\n\n# fit na danych\nmodel = XGBClassifier()\nmodel.fit(X, Y)\n\n# plot feature importance\nplot_importance(model)\nplt.show()\n\n","79bc9403":"#Wszystkie utworzone, zmodyfikowane, zeskalowane, zakodowane lub pierwotne kolumny(, ktore zawieraja tylko liczby calkowite)\ncols_cut1=['Pclass', 'Sex',  'Family_size',\n          '_Fare', 'Title_encoded' ,'_Embarked_encoded'\n          ,'_Cabin_encoded','Age_encoded','_TicketCounts']\n\n#Wydzielenie zbioru treningowego z polaczonego datasetu\nX,Y=get_train_data(df_all, cols_cut1)","e619af5b":"#Na podstawie https:\/\/machinelearningmastery.com\/feature-importance-and-feature-selection-with-xgboost-in-python\/\n#Zadaniem jest:\n#wyznaczenie feature importance\n#a potem z uzyciem SelectFromModel ze zbioru wybieramy zmienne, ktore sa uzyte do obliczenia accuracy\n#zaczynamy od jednej zmiennej z najwyzszym feature importance, a potem dwie zmienne, 3 zmienne i tak az uzwglednimy wszystkie\n#sprawdzamy jak uwzglednienie kolejnych zmiennych wplywa na accuracy zbiorcze\n\nfrom numpy import sort\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import SelectFromModel\n\n# podzial na train i test\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=7)\n\n# fit na train\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\n\n#Drukowanie feature importance wraz z nazwami cech, po sortowaniu, by wiedziec dla czego drukowane sa accuracy\nfeature_importance_with_names=zip(X.columns,model.feature_importances_)\nprint('\\n~~~Posortowane feature importance od najwiekszego do najmniejszego~~~\\n')\nprint(sorted(feature_importance_with_names, key=lambda x: x[1], reverse=True))\nprint ('\\n')\n\n# predykcja i wydruk accuracy\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_test, predictions)\n\nprint('\\n~~~Accuracy z uwzglednieniem wszystkich zmiennych~~~\\n')\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n\nprint('\\n~~~Wynik accuracy w zaleznosci od ilosci uwzglednionych zmiennych~~~\\n')\n\n# posortowanie wg wartosci feture importance\nthresholds = sort(model.feature_importances_)\n\nfor thresh in thresholds:\n    \n    \n    #wybor zmiennych, ktore feature importance jest wyzszy niz threshold\n    #czyli zaczyna od najwiekszej wartosci, ktora spelnia 1 zmienna (_Fare)\n    #potem mniejszy threshold, ktory spelniaja juz 2 zmienne (_Fare i Age_encode)\n    #itd. az uwzgledni wszystkie zmienne\n    #wiec najpierw do obliczen accuracy wykorzystuje 1 zmienna, potem 2,..., a na koniec wszystkie\n    selection = SelectFromModel(model, threshold=thresh, prefit=True)\n    select_X_train = selection.transform(X_train)\n    \n    # train model\n    selection_model = XGBClassifier()\n    selection_model.fit(select_X_train, y_train)\n    \n    # eval model\n    select_X_test = selection.transform(X_test)\n    y_pred = selection_model.predict(select_X_test)\n    \n    #print\n    predictions = [round(value) for value in y_pred]\n    accuracy = accuracy_score(y_test, predictions)\n    print(\"FeatImportance=%.3f, FeatNumber=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))\n\n \n\nxgb_clas={'XGBoost': XGBClassifier()}\ntest_models(xgb_clas, X, Y, plot=False)\n    ","9b5c032c":"from sklearn.model_selection import train_test_split\nfrom operator import itemgetter\nfrom sklearn.model_selection import cross_val_score\n\n#Obliczenie accuracy przy uwzglednieniu tylko 1 zmiennej\ndef get_accuracy_for_one_feature(df_all, cols_cut1):\n    score = {}\n    model=XGBClassifier()\n    for i in range(0,len(cols_cut1),1):\n        cols_cut2=[cols_cut1[i]]\n        X,Y=get_train_data(df_all, cols_cut2)    \n        score[cols_cut2[0]]=cross_val_score(model, X, Y, cv=5).mean()\n\n\n    #posortowanie od najwiekszego accuracy do najmniejszego\n    print('\\n~~~Accuracy przy uwzglednieniu tylko jednej zmiennej~~~')\n    return sorted(score.items(), key=itemgetter(1))[::-1]\n    \n\nget_accuracy_for_one_feature(df_all, cols_cut1)","91c5de30":"#Normalizacja Fare w 0-1\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import preprocessing\n\nx = df_all['Fare'].values #returns a numpy array\nx=x.reshape(df_all.shape[0],1)\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\ndf_all['_Fare_norm'] = pd.DataFrame(x_scaled)","5d134d9f":"#Sprawdzenie wyniku accuracy dla pojedynczej zmiennej\ncol_cut_with_fare=cols_cut1.copy()\ncol_cut_with_fare.append('_Fare_norm')\nget_accuracy_for_one_feature(df_all, col_cut_with_fare)","fdbda6da":"print('\\n~~~Zmienne pozostawione do dalszego tuningu XGBoosta:~~~\\n')\nprint(cols_cut1)","bb5ae34f":"from sklearn.model_selection import GridSearchCV\n\n#zastosowanie tylko ponizszych 5 zmiennych zapewnia wynik na poziomie 0.7894\n#cols_cut12=['Pclass', 'Sex', 'Age_encoded', 'Family_size','_Fare']\n\ncols_cut12=cols_cut1\nX, Y = get_train_data(df_all, cols_cut12)\n\n#Parametry do sprawdzenia i wybrania najlepszych\ncv_params = {'max_depth': [2,3,4,5],\n             'n_estimators': range(10,210,50),\n            'learning_rate': [0.01,0.02,0.05,0.07,0.1]}\n\n#parametry podane do XGBoosta\n#ind_params = {'learning_rate': 0.05, 'seed':0, 'subsample': 0.8, 'colsample_bytree': 0.8, \n#             'objective': 'binary:logistic'}\n\nxgb= XGBClassifier().fit(X, Y)\noptimized_xgb = GridSearchCV(xgb, cv_params, scoring = 'accuracy', cv = 5, n_jobs = -1)\noptimized_xgb.fit(X, Y)\nprint('\\n~~~Accuracy po optymalizacji~~~\\n')\noptimized_xgb.score(X,Y)","8e6b73aa":"#Predykcja dla wyniku testowego i submit\n#Wyznaczenie zbioru testowego\nSURV = 891\nXp = df_all[SURV:][cols_cut12]\n\n#Utworzenie pliku wynikowego\nresult = pd.DataFrame({'PassengerID': df_all[SURV:].index })\nresult['Survived'] = optimized_xgb.predict(Xp).T.astype(int)\nresult[['PassengerID','Survived']].to_csv('submission.csv',index=False)","c04d8b56":"## 2.3 Zmienna Name i pozyskanie Title","49f7576f":"#### Wnioski z powy\u017cszego wykresu:\n* Zmienne, kt\u00f3re zale\u017c\u0105 od innych maj\u0105 niewielke znaczenie i mo\u017cna je wyrzuci\u0107\n* S\u0105 to SibSp, Parch, IsAlone, kt\u00f3re zale\u017c\u0105 od Family_Size oraz Fare_encoded, kt\u00f3re zale\u017cy od Fare\n* Najwi\u0119kszy udzia\u0142 w Feature Importance maj\u0105 po kolei: Fare, Age_encoded, Title_encoded, Ticet_Counts, Family_size, a wi\u0119c zmienne, kt\u00f3ry warto\u015bci wahaj\u0105 od -1 do oko\u0142o 10, wi\u0119c nale\u017cy sprawdzi\u0107, czy nie zaburzaj\u0105 one wyniku przez to, \u017ce nie s\u0105 0-1\n","86d5331f":"### 4.1.2 Wybranie zmiennych, kt\u00f3re b\u0119d\u0105 u\u017cyte do wst\u0119pnego treningu oraz wydzielenie zbioru treningowego\n* wybrane zmienna do wst\u0119pnego treningu: Pclass - klasa, Sex - zakodowana p\u0142e\u0107 (0,1), Age_encoded - zakodowany wiek, Family_size (zakodowany rozmiar rodziny),  _Fare - przeskalowana warto\u015b\u0107 ceny biletu, Title_encoded - zakodowany tytu\u0142, IsAlone - czy osoba podr\u00f3\u017cuje sama (0,1), pozosta\u0142e kolumny wst\u0119pnie odrzucono, bo zani\u017ca\u0142y wynik zar\u00f3wno podczas wst\u0119pnej CV jak i podczas submitu wyniku do rankingu g\u0142\u00f3wnego\n* wydzielenie zbioru treningowego z DataFrame zawieraj\u0105cego zbi\u00f3r treningowu i testowy, kt\u00f3re zosta\u0142y z\u0142\u0105czone na pocz\u0105tku analizy (dataset treningowy zawiera\u0142 warto\u015b\u0107 w kolumnie Survived i liczy\u0142 SURV=891 wierszy, natomiast zbi\u00f3r testowy nie zawiera\u0142 tej danej i znajdowa\u0142 si\u0119 poni\u017cej wiersza 891)\n* podzielenie zbioru na zmienne predykcyjne X z wybranymi zmiennymi oraz zbi\u00f3r, kt\u00f3rego warto\u015b\u0107 przewidujemy Y zawieraj\u0105cy kolumn\u0119 Survived","e1891d06":"#### Wnioski z analizy zmiennej Cabin\n* Nazwy kabin w wi\u0119kszo\u015bci posiadaj\u0105 osoby z pierwszej klasy\n* Brakuje danych o kabinie g\u0142\u00f3wnie dla 3 klasy\n* Zmienna _Cabin_encoded zawiera zakodowane grupy kabin","1289b2f0":"#### Wnioski z zmienej Embarked - zaokretowania\n* najwi\u0119cej pasa\u017cer\u00f3w zaokr\u0119towano w porcie S i byli to pasa\u017cerowie g\u0142\u00f3wnie 3 klasy, z brakuj\u0105cymi informacjami o kabinie\n* w porcie Q zaokr\u0119towano najmniej pasa\u017cer\u00f3w, g\u0142\u00f3wnie 3 klasa, zn\u00f3w braki w informacji o kabinach\n* w porcie C najwi\u0119kszy udzia\u0142 pasa\u017cer\u00f3w stanowi\u0105 posiadacze biletu na pierwsz\u0105 klas\u0119, ale te\u017c jest sporo os\u00f3b z klasy 3, bez informacji o kabinach\n* mediana cen bilet\u00f3w jest podobna dla klasy 2 i 3 we wszystkich portach i oscyluje 12-15 dolarow dla klasy 2 i oko\u0142o 8 dla klasy 3, dla klasy 1 ta warto\u015b\u0107 ma wi\u0119ksz\u0105 rozbie\u017cno\u015b\u0107 od oko\u0142o 50-90 dolar\u00f3w, ale mo\u017ce to wynika\u0107 z niewielkiej liczby pasa\u017cer\u00f3w pierwszej klasy w portcie Q\n* najwi\u0119ksze szanse na prze\u017cycie mieli zaokr\u0119towni w porcie C, co mo\u017ce wynika\u0107, z tego \u017ce to pasa\u017cerowie g\u0142\u00f3wnie 1-szej klasy\n* dla port\u00f3w Q i S warto\u015bci szans na prze\u017cycie oscyluj\u0105 mi\u0119dzy 33-38%\n\n\nWydaje si\u0119, \u017ce zmienna Embarked nie wniesie nowych zale\u017cno\u015bci i nie wp\u0142ynie znacznie na popraw\u0119 wyniku predykcji szans na prze\u017cycie.","02bf29a3":"#### Wnioski z dalszej analizy wieku z podzia\u0142em na podzbiory\n* najwi\u0119ksze szanse na prze\u017cycie mia\u0142y noworodki 0-1, a p\u00f3\u017aniej dzieci 3-8 lat\n* najmniejsze szanse mieli ludzie w sile wieku 20-30lat oraz staruszkowie 60-100lat","09f81692":"### 4.1.4 Przeprowadzenie cross-validation z r\u00f3\u017cnym rozmiarem zbioru treningowego dla r\u00f3\u017cnych modeli\n* CV dla r\u00f3\u017cnych rozmiar\u00f3w zbioru treningowe z u\u017cycie plot_learning_curves\n* przetestowanie r\u00f3\u017cnych modeli klasyfikacyjnych\n* por\u00f3wnanie wykres\u00f3w krzywych treningowych i CV dla r\u00f3\u017cnych modeli\n* por\u00f3wnanie wynik\u00f3w u\u015brednionego wyniku testu z kilku CV dla r\u00f3\u017cnych modeli\n* wyb\u00f3r modelu do dalszego tuningu","fbf32c26":"### 5.3 Wp\u0142yw parametr\u00f3w na accuracy\n* Odrzucam zmienne SibSp, Parch, IsAlone, oraz Fare_encoded, kt\u00f3re nie maj\u0105 ma\u0142y wp\u0142yw na wynik\n* Sprawdzamy, jak uzwgl\u0119dnienie kolejnych zmiennych\/ cech wg Feature Importance wp\u0142ywa na wynik accuracy\n* ","078ed85d":"#### Z wst\u0119pnego przegl\u0105du zmiennej Name (imion i nazwisk) mo\u017cna wywnioskowa\u0107, \u017ce:\n   * nie ma brak\u00f3w w danych\n   * tylko 2 imiona i nazwiska si\u0119 powtarzaj\u0105, ale s\u0105 to r\u00f3\u017cne osoby (inny wiek, inny nr biletu)\n   * pierwszy cz\u0142on zmiennej Name to nazwisko, wi\u0119c mo\u017cna jeszcze sprawdzi\u0107 ilo\u015b\u0107 os\u00f3b o takim samym nazwisku, czy to rodzina\n   * drugi cz\u0142on w Name (po przecinku) to tytu\u0142, kt\u00f3ry mo\u017ce \u015bwiadczy\u0107 o wieku, statusie osoby i wyci\u0105gniemy z niego kolejn\u0105 zmienn\u0105 (bazuj\u0105c na innych opracowaniach tego datasetu)\n    ","1cd7732f":"### Wnioski z analizy zmiennej Pclass\n* Klasa, w kt\u00f3rej znajdowa\u0142 si\u0119 pasa\u017cer mia\u0142a znaczny wp\u0142yw na szanse uratowania, im wy\u017csza klasa tym wi\u0119ksza by\u0142a szansa (dla zbioru treningowego: 1-klasa 63% prze\u017cy\u0142o, 2-klasa 47%, 3-klasa 24%).\n* Im wy\u017csza by\u0142a klasa tym mniej pasa\u017cer\u00f3w si\u0119 w niej znajdowa\u0142o (dla zbioru treningowego)\n* Im wy\u017csza by\u0142a klasa tym wi\u0119ksze szanse na prze\u017cycie mieli tak\u017ce m\u0119\u017cczy\u017ani, ale w\u015br\u00f3d uratowanych dominowa\u0142y kobiety (1-klasa - ponad 96% kobiet i 36% m\u0119\u017cczyzn uratowanych, 2-klasa 92% uratowanych kobiet i tylko 15% m\u0119\u017cczyzn, 3-klasa 50% kobiet i 13% m\u0119\u017cczyzn uratowanych)\n* Zmienna stanowi wa\u017cny predyktor i nie mo\u017ce zosta\u0107 pomini\u0119ta.\n* W kolumnie Pclass nie by\u0142o brak\u00f3w danych, a zmienne wyznaczone s\u0105 przez liczby 1,2,3 i nie by\u0142y ju\u017c poddawne encodingowi.","e50c7697":"## 5.5 Ponowna normalizacja Fare\n* normalizacja Fare w zakresie 0-1, by sprawdzi\u0107 jaki wp\u0142yw na uzyskiwany wynik mia\u0142 jej brak","994c9760":"### Z wst\u0119pnego przegl\u0105du zmiennej Age - wieku:\n* widoczne s\u0105 znaczne braki w miejscach okre\u015blaj\u0105cych wiek, dlatego konieczne b\u0119dzie uzupe\u0142nienie brak\u00f3w.\n* warto\u015bci wieku wyra\u017cone s\u0105 jako liczby zmiennoprzecinkowe z dok\u0142adno\u015bi\u0105 do 2 miejsc po przecinku oraz wyst\u0119puje prawie 100 r\u00f3\u017cnorodnych warto\u015bci. Taki zbi\u00f3r nie nadaje si\u0119 do przeprowadzenia treningu. \n* najliczniejsz\u0105 grup\u0119 stanowi\u0142y osoby w wieku 20-40 lat i tak\u017ce \u015bmiertelno\u015b\u0107 w tej grupie by\u0142a najwi\u0119ksza\n* pojedyncze osoby o wieku powy\u017cej 60 lat, kt\u00f3re mog\u0105 zaburza\u0107 predykcj\u0119\n* konieczny podzia\u0142 os\u00f3b na kilka zakodowanych grup wiekowych","5c70f67a":"### Dalej przyj\u017cymy si\u0119 zmiennej Fare:\n* sprawdzimy rozk\u0142ad Fare\n* zale\u017cno\u015b\u0107 mi\u0119dzy Fare, a innymi zmiennymi\n* wp\u0142yw warto\u015bci Fare na szans\u0119 na przetrwanie (Survived)\n* przeprowadzimy normalizacj\u0119 zmiennej Fare","f429a060":"## 6. Tuning XGBoosta\nZ u\u017cyciem GridSearchCV testujemy:\n*  r\u00f3\u017cne maksymalne g\u0142\u0119bko\u015bci drzewa (max_depth)\n* r\u00f3\u017cn\u0105 ilo\u015b\u0107 drzew (n_estimators)\n* r\u00f3\u017cn\u0105 min_child_weight","faa69ae2":"## 2.7 Cabin - nr kabiny\n* zmienna Cabin posiada znaczne braki danych\n* przeprowadzimy proces zast\u0105pienie pustych miejsc zmienn\u0105 = 'missing'\n* oraz dodatkowo wyci\u0105gniemy pierwsz\u0105 liter\u0119 z nazwy kabiny, bo mo\u017ce ona definiowa\u0107 po\u0142o\u017cenie na statku\n* sprawdzimy jaka jest zale\u017cno\u015b\u0107 mi\u0119dzy nazw\u0105 kabiny","4a6e0e43":"## 5.4 Wp\u0142yw pojedynczych zmiennych na accuracy (z pomini\u0119ciem pozosta\u0142ych)\n* u\u017cyjemy najprostszej metody, a wi\u0119c przywidywania z uwzgl\u0119dnieniem tylko 1 kolumny\n* przeprowadzimy CV i sprawdzimy accuracy, gdy uwzgledniamy do przewidywnia, tylko konkretna zmienna\n","24ba9d8b":"### Wnioski z tuningu XGBoosta\n* XGBoost pomimo wyniku oko\u0142o 90% dla zbioru testowego przewiduje oko\u0142o 76-77% -> overfitting\n* Pomimo u\u017cycia GridSeachCV nie uda\u0142o si\u0119 uzyska\u0107 dobrego wyniku, niezb\u0119dne jest g\u0142\u0119bsze zag\u0142\u0119bienie si\u0119 w parametry XGBoosta, bo r\u00f3\u017cnica mi\u0119dzy zastosowaniem optymalizacji, a jej brak to zaledwie 1% r\u00f3\u017cnicy\n* Jednocze\u015bnie zastosowanie tylko 5 zmiennych: Pclass, Sex, Age_encoded, Family_size oraz _Fare zapewni\u0142o wynik na poziomie 78.94%, co zapewnia oko\u0142o miejsce w top3000 z hakiem\n* Zatem b\u0142\u0105d nadal tkwi w zmiennych, ale na tym zako\u0144czymy ten wst\u0119pny projekt","4ae2f7c1":"## 2.9 Ticket - nr biletu\n* Sprawdzenie  przyk\u0142adowych bilet\u00f3w\n* Sprawdzenie unikalnej liczby bilet\u00f3w\n* Wprowadznie nowej zmiennej, kt\u00f3ra zawiera informacje ile os\u00f3b jeszcze posiada taki sam nr biletu (_TicketCounts)","4e3edd5d":"### Utworzenie pliku z predykcj\u0105 do submitu","385e95d9":"### Wnioski z przegladu wielkosci rodziny:\n* najwi\u0119cej os\u00f3b podr\u00f3\u017cowa\u0142o samotnie i mia\u0142o oko\u0142o 30% szans na prze\u017cycie\n* podr\u00f3\u017cuj\u0105cy rodzin\u0105 mieli 50% szans na prze\u017cycie\n","11051f6e":"#### Wnioski z analizy Fare\n*  najwi\u0119kszy udzia\u0142 maj\u0105 bilety w przedziale ceny 7-10\n* mediana ceny biletu wynosi oko\u0142o 14\n* 75% bilet\u00f3w zamyka si\u0119 w cenie 31\n* mo\u017cna zauwa\u017cy\u0107, \u017ce tanie bilety 0-10 dominuj\u0105 w klasie 3, 10-20 to przedzia\u0142 przej\u015bciowy mi\u0119dzy 3 a 2-g\u0105 klas\u0105, a powy\u017cej 30 dominuj\u0105 bilety w pierwszej klasie, przy czym pojawiaj\u0105 si\u0119 pojedyncze przypadki z 3-klasy, by\u0107 mo\u017ce s\u0105 to jakie\u015b bilety grupowe\n* najmniejsze szanse na przetrwanie nios\u0142o posiadanie biletu w zakresie 0-10, a powy\u017cej Fare=70 szanse wzrasta\u0142y po\u017cej 70% (przy za\u0142o\u017ceniu, \u017ce podr\u00f3\u017cowa\u0142o si\u0119 w 1-szej klasie\n* utworzono 2 dodatkowe zmienne [  __ Fare] - zawieraj\u0105c\u0105 przeskalowane warto\u015bci Fare oraz zmienn\u0105 [_Fare_encoded] zawieraj\u0105c\u0105 zakodowane warto\u015bci przedzia\u0142\u00f3w Fare, by du\u017ce warto\u015bci Fare nie wp\u0142ywa\u0142y negatywnie na proces treningu\n","18458738":"## 2.5 Sibsp & Parch \u2013 liczba ma\u0142\u017conk\u00f3w, lub rodze\u0144stwa na pok\u0142adzie oraz liczba dzieci\/rodzic\u00f3w na pok\u0142adzie pos\u0142u\u017cy wyznaczenie wielko\u015bci rodziny Family_size oraz zmiennej IsAlone\n* Na pocz\u0105tku sprawdzili\u015bmy, \u017ce nie brakuje danych w zmiennych Sibsp oraz Parch, dlatego pominiemy ten pkt\n* Z zmiennej Sibsp i Parch tworzymy zmienn\u0105 Familiy_size informuj\u0105c\u0105 o wielko\u015bci rodziny bez uwzgl\u0119dnienia samego pasa\u017cera\nFamily_size=Sibsp+Parch\n","a4a5ff58":"## 2.2 Zmienna Pclass - Klasa, w kt\u00f3rej podr\u00f3\u017cowa\u0142 pasa\u017cer\n\nPrzebieg dzia\u0142ania:\n* Sprawdzenie czy zmienna ma unikalne warto\u015bci, czy trzeba j\u0105 zakodowa\u0107\n* Sprawdzenie czy nie ma brak\u00f3w danych\n* Sprawdzenie liczby pasa\u017cer\u00f3w z poszczeg\u00f3lnych klas\n* Sprawdzenie stosunku uratowanych w zale\u017cno\u015bci od przynale\u017cno\u015bci do danej klasy oraz p\u0142ci (w konsoli oraz na wykresach)\n","9e4b7dcd":"# 3 Sprawdzenie korelacji mi\u0119dzy zmiennymi\n","147a13a4":"#### Wnioski po analizie tytu\u0142\u00f3w:\n* dominuj\u0105ce tytu\u0142y to Mr, Miss, Mrs, Master oraz kilka os\u00f3b z tytu\u0142em Rev, Dr i tylko te tytu\u0142y uwzgl\u0119dniamy bez zmian, bo pozosta\u0142e wyst\u0119puj\u0105 pojedynczo lub po dwa i podlegaj\u0105 agreacji w grup\u0119 other\n* widoczny jest du\u017cy rozrzut w wieku dla wi\u0119kszo\u015bci grup\n* Dr - zazwyczaj m\u0119\u017cczyzna w sile wieku oko\u0142o 50tki, ale najm\u0142odszy w tej grupie ma tylko 23 lata\n* Master - ch\u0142opiec, grupa o medianie r\u00f3wnej 4 lata, ale najstarszy ma ponad 14 lat, a najm\u0142odszy jest 4 miesi\u0119cznym noworodkiem\n* Miss - okre\u015blenie panny (?), ale te\u017c jako ma\u0142ej dziewczynki, jak i kobiety w podesz\u0142ym wieku, mediana wieku 22lata, ale zakres wieku od noworodka (2 miesi\u0105ce) do starszej kobiety (63lata)\n* Mr - okre\u015blenie m\u0119\u017cczyzny, mediana 29 lat, najmniej 11lat, najwi\u0119cej 80lat\n* Mrs - okre\u015blenie kobiety, mediana 35.5 lat, najmniej 14 lat, nawi\u0119cej 76\n* Rev - osoba w \u015brednim wieku, mediana 41.5, najmniej 27, najwi\u0119cej 57'\n* Po raz kolejny widoczne, \u017ce bycie kobiet\u0105 gwarantowa\u0142o du\u017co wi\u0119ksze szanse na prze\u017cycie, bo posiadaczki tytu\u0142\u00f3w Miss oraz Mrs mia\u0142y odpowiednio prawie 70% oraz ponad 79% szans na prze\u017cycie, du\u017ce szanse mieli te\u017c mali ch\u0142opcy - Master - 58% szans. Najmniej mieli posiadacze tytu\u0142y Rev (?) 0% oraz m\u0119\u017cczy\u017ani - Master - oko\u0142o 16%\n* Dodatko zebrano mediane wieku, dla tytu\u0142u, by uzupe\u0142ni\u0107 brakuj\u0105ce informacje o wieku","7a9833ae":"## 1.2 Wczytanie danych i z\u0142\u0105czenie zbior\u00f3w","02394977":"## 2.6 Fare - op\u0142ata za bilety\n* statyska opisowa cen bilet\u00f3w\n* przegl\u0105d brakuj\u0105cych danych Fare, opis osoby, dla kt\u00f3rej brakuje tych informacji\n* na podstawie zmiennych Pclass, Age, Embarked ( i os\u00f3b o podobnych warto\u015bciach w tych zmiennych) oszacowanie brakujacej ceny biletu\n* histogram z rozk\u0142adem cen bilet\u00f3w","63d93a8f":"### 4.1.3 Definicja modu\u0142\u00f3w i funkcji do treningu i cross-validation\n* Definicja funkcji plot_learning_curve, kt\u00f3ra odpowiedzialna jest za trening i cross-validation dla r\u00f3\u017cnych rozmiar\u00f3w zbioru treningowego i wyplotowanie krzywych treningowych i krzywych cross-validation oraz u\u015brednione wyniki testu\n* Funkcja b\u0119dzie u\u017cyta, by wybra\u0107 najlepszy model do dalszych modyfikacji ","09af21f2":"# 2.Oczyszczenie danych, wst\u0119pna wizualizacja i wst\u0119pny Feature Engineering\nW kolejnym rozdziale przyj\u017cymy si\u0119 ka\u017cdej zmiennej, kt\u00f3ra jest u\u017cywana do predykcji. Przeprowadzimy wst\u0119pn\u0105 wizualizacj\u0119, interpretacj\u0119 zmiennej, oczy\u015bcimy dane, przeprowadzimy kategoryzacj\u0119, encoding, odrzucimy zb\u0119dne zmienne i przeprowadzimy wst\u0119pny Feature Engineering.","122c4cd3":"### 5.1 Za\u0142adowanie zbioru z wszystkimi zmiennymi, kt\u00f3re mog\u0105 by\u0107 u\u017cyte do analizy","9b17d5ce":"### Sprawdzenie os\u00f3b o tym samym nazwisku (by\u0107 mo\u017ce rodzin)\n* Sprawdzenie ile os\u00f3b ma takie same nazwisko\n* Ile jest podzbior\u00f3w o konkretnej liczbie os\u00f3b o takim samym nazwisku\n* Ilu z tych podgrup si\u0119 uratowa\u0142o\n* Jaki by\u0142 wp\u0142yw powtarzalno\u015bci nazwisk na szanse uratowania","22af37af":"## 1.3 Wst\u0119pny przegl\u0105d danych\nPrzyj\u017cyjmy si\u0119 teraz co zawiera nasz zbi\u00f3r. G\u0142\u00f3wnym celem jest przewidzenie warto\u015bci warto\u015bci zmiennej Survival, a wi\u0119c informacji czy pasa\u017cer prze\u017cy\u0142, czy te\u017c nie, przy u\u017cyciu 9-ciu pozosta\u0142ych zmiennych (predyktor\u00f3w), kt\u00f3re zosta\u0142y opisane poni\u017cej:\n\n<br>-PassengerID - unikalny numer pasa\u017cera\n<br>-Survival - informacja czy dany pasa\u017cer prze\u017cy\u0142 katastrof\u0119, czy te\u017c nie. 0 - Nie, 1 - Tak\n<br>-Pclass \u2013 klasa | (1 = pierwsza; 2 = druga; 3 = trzecia).\n<br>-Name \u2013 imi\u0119 i nazwisko pasa\u017cera.\n<br>-Sex \u2013 p\u0142e\u0107 pasa\u017cera.\n<br>-Sibsp \u2013 liczba ma\u0142\u017conk\u00f3w, lub rodze\u0144stwa na pok\u0142adzie.\n<br>-Parch \u2013 liczba rodzic\u00f3w, lub dzieci na pok\u0142adzie.\n<br>-Ticket \u2013 numer biletu.\n<br>-Fare \u2013 op\u0142ata za bilet.\n<br>-Cabin \u2013 kabina.\n<br>-Embarked \u2013 port startowy (C = Cherbourg; Q = Queenstown; S = Southampton).\n","b9f9d59f":"### 4.1.1 Wybranie i import r\u00f3\u017cnych modeli ML do klasyfikacji\n* import modeli Logistycznej Regresji,Naive Bayesian, Support Vector Machine do klasyfikacji,  drzewo decyzyjne, las losowy, ,sieci neuronowe,  k-najbli\u017cych s\u0105siad\u00f3w, XGBoost\n* utworzenie s\u0142\u00f3wnika z r\u00f3\u017cnymi modelami","fedc3747":"# 4. Wst\u0119pna budowa modelu","25711d3b":"### Podzial wieku na mniejsze podbiory i zakodowanie\nDziel\u0119 zbi\u00f3r na przedzia\u0142y wiekowe:\n* 0-3lat\n* 3-8lat\n* 8-15 lat\n* 14-20lat\n* 20-40lat\n* 40-60lat\n* 60-100lat\n<br>Nast\u0119pnie robi\u0119 encoding grup wiekowych\n","693dd31a":"## 4.1 Wst\u0119pna predykcja, wybranie modelu do dalszej optymalizacji\n* Wybranie i import wst\u0119pnych modeli ML do klasyfikacji\n* Wybranie zmiennych do predykcji\n* Wydzielenie zbioru treningowego, podzia\u0142 na zbi\u00f3r zmiennych predykcyjnych X oraz zmienn\u0105 przewidywan\u0105 Y\n* Przeprowadzenie cross-validation, dla r\u00f3\u017cnych modeli, zbadanie wyniku testowego dla r\u00f3\u017cnych modeli, krzywe treningowe\n* Wybranie modelu do dalszej analizy\n","f963fd0c":"### Uzupe\u0142nienie brak\u00f3w w wieku\nWykrzystuje mediane wieku dla danego tytu\u0142u","cbc126c3":"#### Wnioski z analizy Ticket:\n* wi\u0119kszo\u015b\u0107 pasa\u017cer\u00f3w posiada\u0142a unikalny bilet (tylko oni byli do niego przypisani)\n* utworzono pomocnicz\u0105 zmienn\u0105 _TicketCounts, kt\u00f3ra zawiera informacje, kto jeszcze podr\u00f3\u017cowa\u0142 na takim samym bilecie, a wi\u0119c \u015bwiadczy to , \u017ce taka osoba nie podr\u00f3\u017cowa\u0142a sama\n","db418d0d":"### Wnioski z uwzglednienia tylko pojedynczych zmiennych\n* Najlepiej z przewidywanie radz\u0105 sobie zmienne Title_encoded oraz Sex\n* Pomimo, \u017ce _Fare mia\u0142o wysokie warto\u015bci Feature Importance przewiduje tylko w 3 co do kolejno\u015bci dok\u0142adno\u015bci\u0105, a warto\u015b\u0107 feature importance tej zmiennej mo\u017ce wynika\u0107 z du\u017cych warto\u015bci, kt\u00f3re przyjmuje -> mo\u017cna znormalizowa\u0107 w 0-1 dodatkowo\n* Zaskakuj\u0105co dobrze przewiduje zmienna Cabin_encoded, ale mo\u017ce spowodowane znaczn\u0105 ilo\u015bci\u0105 brakuj\u0105cych danych\n* Kiepsko wypada Age_encoded, by\u0107 mo\u017ce przyj\u0119te przedzia\u0142y nie s\u0105 zbyt dobre\n","9b4865f4":"## 1.1 Import niezb\u0119dnych modu\u0142\u00f3w","82717ac1":" #### Z przegl\u0105du nazwisk widoczne jest:\n* Najwi\u0119kszy udzia\u0142 pasa\u017cer\u00f3w stanowi\u0142y osoby, kt\u00f3re mia\u0142y unikalne nazwisko, nie by\u0142o na pok\u0142adzie nikogo innego o takim samym nazwisku. By\u0107 mo\u017ce podr\u00f3\u017cowa\u0142y same lub  ich rodzina nosi\u0142a inne nazwisko. W\u015br\u00f3d nich przetrwa\u0142o oko\u0142o 36%.\n* Najwi\u0119ksze szanse na prze\u017cycie mia\u0142y osoby nale\u017c\u0105ce do grupy 2-osobowej (52%). Mo\u017cna przypuszcza\u0107, \u017ce by\u0142y to ma\u0142\u017ce\u0144stwa, a taki du\u017cy  procent uratowanych wynika\u0142 ze wzgl\u0119du na liczb\u0119 uratowanych kobiet.\n* Mo\u017cna zauwa\u017cy\u0107 tendencj\u0119 (z wyj\u0105tkiem grupy 6 osobowej), \u017ce im wi\u0119ksza liczba os\u00f3b o takim samym nazwisku, tym mniej by\u0142o takim os\u00f3b.\n* Dalsza analiza i tworzenie nowych zmiennych z nazwisk nie b\u0119dzie prowadzone, bo wiem nale\u017ca\u0142o, by sprawdzi\u0107, czy osoby by\u0142y spokrewnione, a ta niejako zawiera si\u0119 ju\u017c w zmiennej SibSig, wi\u0119c dublowaliby\u015bmy dane, tak\u017ce nale\u017ca\u0142oby sprawdzi\u0107 p\u0142e\u0107, a tu dublowaliby\u015bmy zmienn\u0105 Sex. Analiz\u0119 przeprowadzono tylko pogl\u0105dowo i nie b\u0119dzie dalej u\u017cywana zmienna z zestawieniem nazwisk.\n","d6e14003":"## 2.4 Zmienna Age - Wiek\n### Wst\u0119pny przegl\u0105d","7b8f4ceb":"# 7. Wnioski\n1. Uda\u0142o si\u0119 uzyska\u0107 model z accuracy 0.77, przy uwzgl\u0119dnieniu wybranych w punkcie 5.5 zmiennych\n2. Lepszy wynik uda\u0142o si\u0119 pomijaj\u0105 wi\u0119kszo\u015b\u0107 zmiennych, a zostawiaj\u0105c tylko p\u0142e\u0107, klas\u0119 podr\u00f3\u017cnego, zakodowany wiek, wielko\u015b\u0107 rodziny i przeskalowane ceny biletu - wynik 0.7894\n3. Optymalizacja parametr\u00f3w XGBoosta za pomoc\u0105 GridSearchCV zapewni\u0142a niewielk\u0105 1% popraw\u0119 w przypadku wymienionym w pkt 1 i 0% w przypadku z pkt-u 2-ego.\n4. G\u0142\u00f3wny problem tkwi prawdopodobnie w zmiennych i ich podziale, konieczny by\u0142by dalszy Feature Engineering\n5. Jedynymi utworzonymi zmiennymi, kt\u00f3re dobrze si\u0119 sprawdzi\u0142y by\u0142o Family_size oraz _Fare (przeskalowane Fare)","50375370":"### Wy\u015bwietlmy teraz podstawowe informacje o datasetcie\n- Przyk\u0142adowy sampel z 3 wierszami, by zapozna\u0107 si\u0119 z wygl\u0105dem datasetu (df.sample(3))\n- Zestawienie typ\u00f3w danych zbioru wraz z nazwami kolumn oraz liczb\u0105 wyst\u0105pie\u0144 (df.info())\n- Wymiary datasetu (df.shape)\n- Zestawienie brak\u00f3w w danych (df.isnull().sum())","ca928d70":"## 2.8 Zmienna Embarked - zaokr\u0119towanie\n* sprawdzenie brakuj\u0105cych danych\n* uzupe\u0142nienie brakuj\u0105cych danych, bazuj\u0105\u0107 na podobnych pasa\u017cerach\n* sprawdzenie zale\u017cno\u015bci mi\u0119dzy portem, a klas\u0105, cen\u0105 biletu i szansami na prze\u017cycie\n* zakodowanie zmiennych","e9b0c754":"### 4.1.5 Wnioski z powy\u017cszego punktu i wyb\u00f3r modelu\n* widoczny znaczny overfitting (przetrenowanie, high variance) dla dla drzewa decyzyjnego i lasu losowego ( znaczna rozbie\u017cno\u015b\u0107 mi\u0119dzy wynikiem dla treningu i CV)\n* widoczny underfitting (high bias) dla prostych modeli (Regresji Logistycznej (logistic regression), Liniowego SVM (Linear SVC), Naiwnego Bayesiana z rozk\u0142adem Gaussa (GausianNB) i sieci neuronowych (neural networks))\n* ciekawie wynik daje K-najbli\u017cszych s\u0105siad\u00f3w, wydaje si\u0119, \u017ce je\u015bli dost\u0119pny by\u0142by wi\u0119kszy zbi\u00f3r to m\u00f3g\u0142by osi\u0105gn\u0105\u0107 lepszy wynik, jednak dla tego daje oko\u0142o accuracy oko\u0142o 0.8\n* godne uwagi po przyj\u017ceniu si\u0119 s\u0105 2 algorytmy - SVC oraz XGBoost, przy czym w ka\u017cdym przypadku rozmiaru zbioru treningowego XGBoost uzyskiwa\u0142 wy\u017csze accuracy oraz jest zwyci\u0119skim algorytmem wielku konkurencji na Kaggle, dlatego do dalszej analizy wybierzemy ten algorytm","0a4e8f73":"### Wnioski z wst\u0119pnego przegl\u0105du Feature Importance:\n* Odrzucenie zmiennych o niewielkiej warto\u015bci Feature Importance przyczyni\u0142o si\u0119 do niewielkiego wzrostu accuracy\n* Przegl\u0105d wp\u0142ywu ilo\u015bci uwzgl\u0119dnionych cech na accuracy nie daje odpowiedzi, kt\u00f3re cechy zmodyfikowa\u0107, bo np.: uwzgl\u0119dniene opr\u00f3cz Fare tak\u017ce Age_encoded, powoduje spadek accuracy (przy uwzgl\u0119dnieniu tylko tych 2 cech), ale nie uwzgl\u0119dnienie Age_encoded powoduje znaczny spadek ca\u0142kowitego accuracy\n* Przyj\u017cymy si\u0119 wp\u0142ywowi pojedynczych cech (z pomini\u0119ciem pozosta\u0142ych) na accuracy","a9739c8f":"### Pozyskanie tytu\u0142\u00f3w (Title) z Name, wykorzystanie patern\u00f3w\n* Pozyskanie tytu\u0142\u00f3w z zmiennej Name\n* Obliczenie \u015bredniego wieku dla danego tytu\u0142u\n* Sprawdzenie szans na prze\u017cycie w zale\u017cno\u015bci od tytu\u0142u\n* Encoding","16e79b67":"### Wnioski z kategorii Sex (P\u0142e\u0107)\n* Zmienna p\u0142e\u0107 odgrywa\u0142a du\u017c\u0105 rol\u0119, je\u015bli chodzi o prze\u017cycie (Survived). Prze\u017cy\u0142o ponad 74% kobiet i tylko niewiele ponad 18% m\u0119\u017cczyzn.\n* Jest to istotna zmienna predykcyjna.\n* Wprowadzono encoding, przypisano warto\u015b\u0107 0 dla 'male' (m\u0119\u017cczyzny) i 1 - 'female' dla kobiety, by modele mog\u0142y dzia\u0142a\u0107 na liczbach\n","e93aef66":"### Wst\u0119pny przegl\u0105d Name (imion i nazwisk)\n* Sprawdzenie czy nie ma brak\u00f3w danych\n* Czy warto\u015bci s\u0105 unikalne\n* Sprawdzenie formy w jakiej wyst\u0119puj\u0105 dane Name","ad676710":"#### Wnioski z heatmapy i korelacji\n* widoczne s\u0105 zale\u017cno\u015bci, kt\u00f3re zosta\u0142y ujawnione ju\u017c podczas podczas obr\u00f3bki zmiennych\n* wp\u0142yw klasy, ceny bilety, p\u0142ci, tytu\u0142em na zwi\u0119kszenie szans na prze\u017cycie (wy\u017csza klasa, wy\u017csza cena, kobieta, tytu\u0142 zwi\u0105zany z kobiet\u0105 -> wi\u0119ksze szanse na przetrwanie)\n* zwi\u0105zek mi\u0119dzy cen\u0105 biletu a klas\u0105 (wy\u017csza klasa->wy\u017csza cena)\n* zale\u017cno\u015bci, \u017ce podr\u00f3\u017cowanie samemu nios\u0142o ze sob\u0105 wi\u0119ksze ryzko \u015bmierci\n* pozytywna korelacja mi\u0119dzy ilo\u015bci\u0105 podr\u00f3\u017cnych na jeden bilet, a jego cen\u0105 (wy\u017csza cena za wi\u0119cej os\u00f3b 0->logiczne)\n* oraz korelacje mi\u0119dzy zmiennymi i ich nowo utworzonymi pochodnymi (np. wielko\u015b\u0107 rodziny od ilo\u015bci dzieci, ma\u0142\u017con\u00f3w, braci etc)\n\n","68709907":"### Wnioski po normalizacji Fare\n* Uzyskane wyniki s\u0105 gorsze ni\u017c bez normalizacji\n* Wst\u0119pnie zawieszamy pomys\u0142 Feature Enginneringu i sprawdzamy jak na wynik mo\u017cemy wp\u0142yn\u0105 tuningiem modelu XGBoost\n* Do dalszej analizy i tuningu XGBoosta pozostawiono zmienne:","448a66da":"## Po wst\u0119pnej analizie mo\u017cna zauwa\u017cy\u0107, \u017ce:\n1.  W zbiorze treningowym brakuje wi\u0119kszo\u015bci (687 z 891) warto\u015bci dotycz\u0105cych zmiennej Cabin (nr kabiny), ale za pewien substytut mo\u017cna informacj\u0119 o klasie Pclass. I zmienn\u0105 Cabin trzeba b\u0119dzie usun\u0105\u0107.\n2. Brakuje tak\u017ce cz\u0119\u015bci informacji o wieku (dla zbioru treningowego 177 z 891), kt\u00f3re trzeba b\u0119dzie imputowa\u0107.\n3. Brakuje pojedynczych danych dla zmiennych Ebarked i Fare.\n4.  Zbi\u00f3r treningowy zawiera kompletne informacja o zmiennej wynikowej (czy kto\u015b prze\u017cy\u0142, czy nie) - Survived, wi\u0119c nie b\u0119dzie trzeba wyrzuca\u0107 wierszy ze zbioru.\n5. Zmienne Sex, Embarked to zmienne kategoryczne i trzeba b\u0119dzie je odpowiednio zakodowa\u0107.\n6. Zmienne Age, Fare podane s\u0105 jako liczby zmiennoprzecinkowy (float64), ale dla p\u00f3\u017aniejszych oblicze\u0144 zapewne b\u0119dzie stworzy\u0107 tak\u017ce z nich zmienne kategoryczne i zakodowa\u0107.\n","621574f8":"# 0. Wstep\nNotebook stanowi m\u00f3j pierwszy w\u0142asny projekt na Kaggle opr\u00f3cz tutorialu z ML.\nDotyczy on oczywi\u015bcie wszystkim znanego zadania Titanica na Kagglu, a wi\u0119c przewidzenia czy pasa\u017cer prze\u017cyje lub czy zginie podczas katastrofy na Titanicu, kt\u00f3ry zaton\u0105\u0142 w 1912 roku, w zale\u017cno\u015bci od r\u00f3\u017cnych cech, kt\u00f3re zostan\u0105 om\u00f3wione w kolejnym punkcie. \u0179r\u00f3d\u0142a, na kt\u00f3rych bazowa\u0142em poda\u0142em na ko\u0144cu notebooka.\n\n### Spis tre\u015bci:\n1. Wczytanie danych i wst\u0119pny ich przegl\u0105d\n2. Oczyszczenie danych, wst\u0119pna wizualizacja i wst\u0119pny Feature Engineering\n3. Sprawdzenie zale\u017cno\u015bci mi\u0119dzy zmiennymi\n4. Wst\u0119pny model, cross-validation, krzywe uczenia (learning_curves), por\u00f3wnanie r\u00f3\u017cnych modeli i wyb\u00f3r jednego (XGBoosta)\n5. Feature Importance - sprawdzenie jak poszczeg\u00f3lne cechy wp\u0142ywaj\u0105 na wynik, odrzucenie zb\u0119dnych\n6. Pr\u00f3ba tuningu parametr\u00f3w XGBoosta z u\u017cyciem GridSearchCV\n7. Wnioski\n\n### Stosowane rozwi\u0105zania:\nW notebooku przedstawiono r\u00f3\u017cne operacje na danych z pomoc\u0105 Pandasa, wizualizacje z pomoc\u0105 Matplotlib oraz Seaborn, liczne operacje z u\u017cyciem Sklearn: imputacje pustych danych, normalizacje, skalowanie danych, tworzenie modeli ML, tworzenie cross-validation, operanie i analiza krzywych uczenia,dob\u00f3r i odrzucenie zmiennych z u\u017cyciem feature importance, sprawdzanie ich zale\u017cno\u015b\u0107i mi\u0119dzy sob\u0105 (heatmapy) oraz ich wp\u0142yw na wynik ,modelowanie z u\u017cyciem XGBoosta,  pr\u00f3ba optymalizacji i  tuning jego parametr\u00f3w z u\u017cyciem GridSeachCV.\n\n### \u0179r\u00f3d\u0142a poza stackoverflow, dokumentacji bibliotek:\n* https:\/\/alexiej.github.io\/kaggle-titanic\/\n* https:\/\/machinelearningmastery.com\n* https:\/\/github.com\/dataworkshop\/webinar-titanic\n","716a7f4c":"### 5.2 Wykres wp\u0142ywu zmiennych na wynik (Feature Importance)","ea90ffdc":"# 1 .Wczytanie danych i wst\u0119pny ich przegl\u0105d i wizualizacja","8b4a8169":"# 5. Features Importance - wyb\u00f3r zmiennych do dalszych analiz\n* w punkcie 4. przeprowadzili\u015bmy analiz\u0119 na wybranych kolumnach, kt\u00f3rych wyb\u00f3r dokonano metod\u0105 pr\u00f3b i b\u0142\u0119d\u00f3w, teraz jednak dok\u0142adnie przyj\u017cymy si\u0119 wp\u0142ywowi poszczeg\u00f3lnych zmiennych na wynik w XGBoostcie\n* dokonamy wyboru, kt\u00f3re zmienne b\u0119dziemy dalej wykorzystywa\u0107","49cf41a0":"## 2.1 Sex - p\u0142e\u0107 pasa\u017cera\nW poni\u017cszym podrozdziale przeprowadzono nast\u0119puj\u0105ce czynno\u015bci:\n* Sprawdzono jak oznaczone s\u0105 rodzaje p\u0142ci i czy s\u0105 to warto\u015bci unikalne, czy trzeba je modyfikowa\u0107 (df_all.Sex.unique())\n* Sprawdzono czy nie wyst\u0119puj\u0105 braki danych w kategorii p\u0142ci (df_train.Sex.isnull().sum())\n* Sprawdzono liczb\u0119 m\u0119\u017cczyzn i kobiet w ca\u0142kowitym zbiorze oraz jak\u0105 liczb\u0119 ich uratowono (df_train.groupby(['Sex'])['Survived'].sum())\n* Wyplotowano wykres ze stosunkiem jaki udzia\u0142 m\u0119\u017cczyzn i kobiet prze\u017cy\u0142 katastrof\u0119 za pomoc\u0105 Seaborn\n* Podmieniono nazwy okr\u0119\u015blaj\u0105ce p\u0142cie na warto\u015bci ca\u0142kowite (0 - m\u0119\u017cczyzna, 1 - kobieta) (df.Sex.replace(['male', 'female'], [0, 1], inplace = True))\n"}}