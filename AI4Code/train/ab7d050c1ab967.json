{"cell_type":{"d334f240":"code","3aae83c7":"code","523e58d5":"code","d2b623d9":"code","c205b2a6":"code","4b7b4c13":"code","360378e2":"code","bd0ffc24":"code","b6b7bb84":"code","d8623d20":"code","d7e285d7":"code","de0ebdb1":"code","259c9d0f":"code","42225ada":"code","fc3f7d12":"code","117e26ed":"code","501cf8b0":"code","ba8a1140":"code","44c846ee":"code","31fa4011":"code","814e6185":"code","19a9c0bb":"code","b37b3405":"code","76b29be1":"code","47a3edc6":"code","84154720":"code","774d9e1f":"code","1f29bc0b":"code","148a9d81":"code","38289d34":"code","edf84444":"code","cf6f9bcf":"code","10fa7e05":"markdown","30eb44ab":"markdown","fbce7a92":"markdown","2650dbdf":"markdown","61bb3722":"markdown","8aea8777":"markdown","cf2f234f":"markdown","7993227a":"markdown"},"source":{"d334f240":"# Load libraries\nimport numpy as np\nfrom matplotlib import pyplot\nfrom pandas import read_csv, set_option\n# from pandas.tools.plotting import scatter_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score","3aae83c7":"# load dataset\ndataset = read_csv('..\/input\/sonar.all-data.csv', header=None)","523e58d5":"# shape\ndataset.shape","d2b623d9":"dataset.describe()","c205b2a6":"dataset.isnull().sum()","4b7b4c13":"# types\nset_option('display.max_rows', 500)\ndataset.dtypes","360378e2":"# peek at data\nset_option('display.width', 100)\ndataset.head(20)","bd0ffc24":"# describe data\nset_option('precision', 3)\ndataset.describe()","b6b7bb84":"# class distribution\ndataset.groupby(60).size()","d8623d20":"# histograms\ndataset.hist(sharex=False, sharey=False, xlabelsize=1, ylabelsize=1, figsize=(12,12))\npyplot.show()","d7e285d7":"# density\ndataset.plot(kind='density', subplots=True, layout=(8,8), sharex=False, legend=False, fontsize=1, figsize=(12,12))\npyplot.show()","de0ebdb1":"# box and whisker\n#dataset.plot(kind='box', subplots=True, layout=(8,8), sharex=False, sharey=False, fontsize=1)\n#pyplot.show()","259c9d0f":"# correlation matrix\nfig = pyplot.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(dataset.corr(), vmin=-1, vmax=1, interpolation='none')\nfig.colorbar(cax)\nfig.set_size_inches(10,10)\npyplot.show()","42225ada":"# split out validation dataset for the end\narray = dataset.values\nX = array[:,0:-1].astype(float)\nY = array[:,-1]\nvalidation_size = 0.2\nseed = 7\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)","fc3f7d12":"# test options\nnum_folds = 10\nseed = 7\nscoring = 'accuracy'","117e26ed":"# spot check some algorithms\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('SVM', SVC()))","501cf8b0":"results = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","ba8a1140":"# compare algorithms\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\nfig.set_size_inches(8,6)\npyplot.show()","44c846ee":"# standardized the dataset\npipelines = []\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()), ('LR', LogisticRegression())])))\npipelines.append(('ScaledLDA', Pipeline([('Scaler', StandardScaler()), ('LDA', LinearDiscriminantAnalysis())])))\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()), ('KNN', KNeighborsClassifier())])))\npipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()), ('CART', DecisionTreeClassifier())])))\npipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()), ('NB', GaussianNB())])))\npipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()), ('SVM', SVC())])))","31fa4011":"results = []\nnames = []\nfor name, model in pipelines:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","814e6185":"# compare scaled algorithms\nfig = pyplot.figure()\nfig.suptitle('Scaled Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\nfig.set_size_inches(8,6)\npyplot.show()","19a9c0bb":"# KNN algorithm tuning\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nneighbors = [1,3,5,7,9,11,13,15,17,19,21]\nparam_grid = dict(n_neighbors=neighbors)\nmodel = KNeighborsClassifier()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)","b37b3405":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nranks = grid_result.cv_results_['rank_test_score']\nfor mean, stdev, param, rank in zip(means, stds, params, ranks):\n    print(\"#%d %f (%f) with: %r\" % (rank, mean, stdev, param))","76b29be1":"# SVM algorithm tuning\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nc_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5, 1.7, 2.0]\nkernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\nparam_grid = dict(C=c_values, kernel=kernel_values)\nmodel = SVC()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)","47a3edc6":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nranks = grid_result.cv_results_['rank_test_score']\nfor mean, stdev, param, rank in zip(means, stds, params, ranks):\n    print(\"#%d %f (%f) with: %r\" % (rank, mean, stdev, param))","84154720":"# ensembles\nensembles = []\n# Boosting methods\nensembles.append(('AB', AdaBoostClassifier()))\nensembles.append(('GBM', GradientBoostingClassifier()))\n# Bagging methods\nensembles.append(('RF', RandomForestClassifier()))\nensembles.append(('ET', ExtraTreesClassifier()))","774d9e1f":"results = []\nnames = []\nfor name, model in ensembles:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","1f29bc0b":"# compare ensemble algorithms\nfig = pyplot.figure()\nfig.suptitle('Ensemble Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\nfig.set_size_inches(8,6)\npyplot.show()","148a9d81":"# prepare model\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nmodel = SVC(C=1.5) # rbf is default kernel\nmodel.fit(rescaledX, Y_train)","38289d34":"# estimate accuracy on validation set\nrescaledValidationX = scaler.transform(X_validation)\npredictions = model.predict(rescaledValidationX)\nprint(accuracy_score(Y_validation, predictions))\nprint(confusion_matrix(Y_validation, predictions))\nprint(classification_report(Y_validation, predictions))","edf84444":"predictions","cf6f9bcf":"Y_validation","10fa7e05":"The accuracy on the validation set was 85.7%. Very close to our original estimates.","30eb44ab":"Parameters of SVM are C and kernel. Try a number of kernels with various values of C with less bias and more bias (less than and greater than 1.0 respectively","fbce7a92":"# Evaluate Algorithms","2650dbdf":"GBM might be worthy of further study, but for now SVM shows a lot of promise as a low complexity and stable model for this problem.\n\nFinalize Model with best parameters found during tuning step.","61bb3722":"An example of a binary classification ML project","8aea8777":"# Analyze data","cf2f234f":"Algorith Tuning: KNN and SVM show as the most promising options","7993227a":"Right now SVM is proving the best with accuracy of 86.7% over KNN's best of 84.9%. (But what about variance? KNN seemed to indicate a tighter variance during spot checking).\n\nLet's try some ensemble methods. No standardization on data this time. Because apparantly all four ensembles we are using are based on decision trees and thus are less sensitive to data distributions. (Ok. Nice tip!)"}}