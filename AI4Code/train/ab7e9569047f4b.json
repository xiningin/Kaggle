{"cell_type":{"15709841":"code","b2a4fccb":"code","4a722133":"code","b0b502d0":"code","234f4e92":"code","a0762c66":"code","8d4f91eb":"code","1a9e1004":"code","d93a8a66":"markdown","992df245":"markdown","2527bc81":"markdown"},"source":{"15709841":"import os\nimport tensorflow as tf\nimport pandas as pd\nfrom transformers import TFGPT2LMHeadModel, GPT2Tokenizer\nimport numpy as np\nimport time\n","b2a4fccb":"class CFG: \n    model_name = 'gpt2' # obviously, moving to a larger model helps -> e.g. \"gpt2-large\"\n    # for the sake speedy demonstration, keep one possible output - \n    # but an obvious extension is generating more and selecting the one most similar (in readability score) to the input\n    nof_outputs = 1 \n    seed = 34\n    MAX_LEN = 70\n    nof_rows = 10\n\ntf.random.set_seed(CFG.seed)","4a722133":"# quick score for comparison\ndef syllable_count(word):\n    count = 0\n    vowels = \"aeiouy\"\n    if word[0] in vowels:\n        count += 1\n    for index in range(1, len(word)):\n        if word[index] in vowels and word[index - 1] not in vowels:\n            count += 1\n            if word.endswith(\"e\"):\n                count -= 1\n    if count == 0:\n        count += 1\n    return count\n\ndef fleisch(passage):\n    nof_char = len(passage)\n    nof_words = len(passage.split(' '))\n    nof_sent = passage.count('.')\n    nof_syl = syllable_count(passage)\n\n    fl = 206  - ((0.1 + nof_words) \/ (0.1 + nof_sent)) - 84 * (nof_syl \/ nof_words)\n    return fl\n\ndef generate_paragraph(xinput):\n    lix = len(xinput)\n\n    input_ids = tokenizer.encode(xinput, return_tensors='tf')\n    sample_outputs = GPT2.generate(input_ids, do_sample = True,  max_length = lix, \n                                   temperature = .85, \n                                   top_k = 50, \n                                   top_p = 0.85, \n    #                               num_return_sequences = CFG.nof_outputs\n                                  )\n    xoutput = tokenizer.decode(sample_outputs[0], skip_special_tokens = True)[lix:  2 * lix]\n\n\n    return xoutput","b0b502d0":"tokenizer = GPT2Tokenizer.from_pretrained(CFG.model_name)\nGPT2 = TFGPT2LMHeadModel.from_pretrained(CFG.model_name, pad_token_id=tokenizer.eos_token_id)\n","234f4e92":"# load the training data\nxtrain = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv', encoding = 'utf8')\n","a0762c66":"# # process single paragraph by sentence - HORRIBLY SLOW ATM\n\n# xinput = xtrain['excerpt'][0]\n\n# xoutput = ''\n\n# nof_sentences = len(xinput.split('.'))\n\n# for ii in range(nof_sentences):\n\n#     input_sequence = xinput.split('.')[ii]\n\n#     len_inp = len(input_sequence)\n\n#     input_ids = tokenizer.encode(input_sequence, return_tensors='tf')\n\n#     len_inp = len(input_sequence)\n\n#     # only process non-empty sentences\n#     if len_inp:\n#         ## topk + top p\n#         sample_outputs = GPT2.generate(input_ids, do_sample = True,  max_length = 2*len_inp, temperature = .7, top_k = 50, top_p = 0.85, num_return_sequences = 3)\n\n#         print('input: ' + input_sequence)\n\n#         fin_score = 10 ** 10\n#         fin_output = ''\n\n#         for i, sample_output in enumerate(sample_outputs):\n#             outtext = tokenizer.decode(sample_output, skip_special_tokens = True)[(len_inp + 1): (2 * len_inp  + 1) ]\n#             dist = np.abs(fleisch(input_sequence) - fleisch(outtext))\n#             if dist < fin_score:\n#                 fin_score = dist\n#                 fin_output = outtext\n#             print('')\n\n#         print('output: ' + fin_output)\n#         xoutput += '. ' + fin_output\n#         print('---')","8d4f91eb":"start_time = time.time()\n# for testing purposes, run on a small subset\n\nxaug = xtrain.loc[0:CFG.nof_rows]\n\nxaug['new_xrc'] = xaug['excerpt'].apply(generate_paragraph)\n\nprint(\"--- %s seconds ---\" % (time.time() - start_time))\n","1a9e1004":"xaug.to_csv('train_augmented.csv', index = False)","d93a8a66":"# Functions","992df245":"# Model","2527bc81":"# Config"}}