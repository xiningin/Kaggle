{"cell_type":{"b876bbae":"code","bda6d370":"code","3a4b30ad":"code","fa5dba4f":"code","4a978638":"code","9759d893":"code","47bbdf78":"code","7da43130":"code","2b74c505":"code","7017eeb1":"code","faa3d60b":"code","5a1fba00":"code","5b4623d6":"code","976db4e5":"code","30418a82":"code","25708a9f":"code","57e4b7ab":"markdown","d7e03a91":"markdown","9ba8b964":"markdown","79e048ab":"markdown","18849ac2":"markdown","f9a2b6d9":"markdown","22ecb855":"markdown","9d72a20c":"markdown","8df01e75":"markdown","0ad081ba":"markdown","b3940c08":"markdown","7d583848":"markdown","92f3ac48":"markdown"},"source":{"b876bbae":"from sklearn import datasets\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","bda6d370":"boston_data = datasets.load_boston()\ndf = pd.DataFrame(boston_data.data,columns=boston_data.feature_names)\ndf['target'] = pd.Series(boston_data.target)                            #Appending the target feature to the dataset\ndf.head()","3a4b30ad":"df.info()","fa5dba4f":"df.shape","4a978638":"Y = df['target']\ny = Y.copy(deep=True) \ndf.drop('target', axis = 1, inplace = True)","9759d893":"df.describe()","47bbdf78":"#Feature Scaling\n#Unscaled features results in dominance of a particular feature\/features having higher \"weight\" \n#and thus reduces the accuracy of the model on data the model is not tested on.\n#Performing mean normalisation(Z-Score)\nfor i in df.columns:\n    if i == 'CHAS':\n        #We dont normalise a categorical data(chas only has binary values)\n        continue\n    df[i] = (df[i]-(df[i].mean()))\/(df[i].std())","7da43130":"#Dataset with normalised values\ndf.head()","2b74c505":"sns.scatterplot(data = y)","7017eeb1":"x = df.to_numpy()\nprint(x.shape, y.shape)\ntarget = y.to_numpy()\nnumExamples = x.shape[0]\nnumFeatures = x.shape[1]","faa3d60b":"x = np.append(np.ones((numExamples,1)),x, axis = 1)   #Adding unit bias\nx.shape","5a1fba00":"theta = np.zeros((numFeatures + 1,1)) #Initializing theta values as 0.","5b4623d6":"theta.shape","976db4e5":"iterations = [200,300,400,500,700,600,1000]\nalpha = [.001 , .003, .01, .03, .1 ,.3]\nfor i in iterations:\n    for j in alpha:\n        for iters in range(i):\n                h= x@theta                                            #Hypothesis Function\n                target = np.reshape(target, (len(target),1))\n                error = h-target\n                J = ((error**2).sum())*(1\/(2*target.shape[0]))        #Cost Function \n                gradient = (x.T@error)*(j\/target.shape[0])            #Gradient\n                theta = theta-gradient                                #simultaneously updating theta\n        print(\"for\",i,\"interations and alpha =\",j, \"cost is\",J.sum())\n        theta = np.zeros((numFeatures + 1,1))                         #Resetting the values of theta to zeros","30418a82":"#Choosing the optimal combination of number of iterations and alpha. (300,.03)\ntheta = np.zeros((numFeatures + 1,1))\niterations = 300\nalpha = 0.03\nJ_History = np.zeros((iterations, 1))\ntheta_history = np.zeros((iterations,len(theta)))\nfor i in range(iterations):\n    h= x@theta                                                    #Hypothesis Function\n    target = np.reshape(target, (len(target),1)) \n    error = h-target\n    J = ((error**2).sum())*(1\/(2*target.shape[0]))                #Cost Function\n    gradient = (x.T@error)*(alpha\/target.shape[0])                #Gradient\n    theta = theta-gradient\n    J_History[i] = J.sum()                                        #Appending values of Cost Function to the array\nplt.plot(range(iterations), J_History)\nplt.xlabel('Number of iterations')\nplt.ylabel('Cost function')\nplt.title('Cost function constantly decreases with increase in number of iterations')","25708a9f":"theta #Final values of theta obtained by our vectorized liner regression madel","57e4b7ab":"## The above plot shows how the price for houses are capped at 50,000. This can prove to limit the accuracy of our model for houses whose prices are more than 50,000, both on real life and training example. This capping of the highest price is one limitation of the Boston Dataset. ","d7e03a91":"### Loadind the Boston dataset from sklearn.datasets","9ba8b964":"### Converting datasets into matrices to execute vectorised Linear Regression","79e048ab":"### Multivariate Linear Regression on the Boston Housing Prices dataset. No existing machine learning algorithm is used.\n### **To my surprise, I couldn't find vectorization of linear regression to this widely known dataset in particular, so hopefully, I'm making a significant contribution to the Machine Learning community.** \n### Why is vectorization important? To take complete advantage of computational power of computers, the most efficient way of implementing an algorithm is vectorizing the computations as it enables us to attain parallelized computations hence tapping into the limits of the system. It saves noteworthy time whose efficiency comes into play when dealing with Big data, where seconds in these small datasets translate to days.\n### Thus, vectorization saves us on huge amount of training time and improves our algorithm.","18849ac2":"### 506 examples and 13 features.","f9a2b6d9":"# Vectorized Linear Regression\n## The model is implemented solely over Numpy and Pandas. No other python libraries are used. \n## The goal is to reduce the Cost function by obtaining the optimal value of parameter thetas, guided by Gradient Descent. ","22ecb855":"506 examples with 13 features and 1 target variable.","9d72a20c":"### Converting datasets into numpy array to implement vectorized Linear Regression model","8df01e75":"### Since features have varying distribution we'll have to normalise them to make sure one feature does not dominate other features on deciding the target value.\n### Z-Score or mean normalisation equates the mean of feature to 0 and standard deviation to 1.","0ad081ba":"## Breakdown of each feature :-","b3940c08":"crim-\nper capita crime rate by town.\n\nzn-\nproportion of residential land zoned for lots over 25,000 sq.ft.\n\nindus-\nproportion of non-retail business acres per town.\n\nchas-\nCharles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n\nnox-\nnitrogen oxides concentration (parts per 10 million).\n\nrm-\naverage number of rooms per dwelling.\n\nage-\nproportion of owner-occupied units built prior to 1940.\n\ndis-\nweighted mean of distances to five Boston employment centres.\n\nrad-\nindex of accessibility to radial highways.\n\ntax-\nfull-value property-tax rate per $10,000.\n\nptratio-\npupil-teacher ratio by town.\n\nblack-\n1000(Bk - 0.63)^2 where Bk is the proportion of african-american by town.\n\nlstat-\nlower status of the population (percent).\n\ntarget-\nmedian value of owner-occupied homes in $1000s.","7d583848":"#### If this notebook helped you in learning, an upvote would be huge! \n#### Thank you :)","92f3ac48":"## New set of values for the 13 features can be multiplied by theta to predict the price of the house corresponding to the values of the 13 features. "}}