{"cell_type":{"5d155320":"code","5f1528ca":"code","efd71cb1":"code","6871ea7b":"code","840248ed":"code","c6d71ca7":"code","bcd1db07":"code","b49473d9":"code","efa35f78":"code","5937fb86":"code","758ba256":"code","a886f7c6":"code","ec8c580b":"code","8f7ee26a":"code","3fa87496":"code","52d45c7a":"code","82a47671":"code","83f8423f":"code","93568e72":"code","89ddcc68":"code","9904f36f":"code","be103407":"code","173c71e0":"code","4ea47a2a":"code","d3fb8cc5":"code","2d80aa67":"code","d0fadb81":"code","88d5fc68":"markdown","e76eead0":"markdown","d81a5335":"markdown","5c3f1285":"markdown","283b874b":"markdown","4af8594a":"markdown"},"source":{"5d155320":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5f1528ca":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","efd71cb1":"train_fp = '..\/input\/tabular-playground-series-jan-2022\/train.csv'\ntrain_df = pd.read_csv(train_fp)\n\ntest_fp = '..\/input\/tabular-playground-series-jan-2022\/test.csv'\ntest_df = pd.read_csv(test_fp)","6871ea7b":"train_df","840248ed":"train_df.info()\nprint(\" \")\ntrain_df.isnull().sum()\n","c6d71ca7":"test_df","bcd1db07":"test_df.info()\nprint(\" \")\ntest_df.isnull().sum()","b49473d9":"train_copy = train_df.copy()\ntest_copy = test_df.copy()","efa35f78":"def convert_dates(df):\n    df['date'] = pd.to_datetime(df['date'])\n    \n    df['day'] = df.date.dt.day\n    df['month'] = df.date.dt.month\n    df['year'] = df.date.dt.year\n    df['weekday'] = df.date.dt.weekday\n    return df\n\ntrain = convert_dates(train_copy)\ntest = convert_dates(test_copy)","5937fb86":"cat_cols = train.select_dtypes('object').columns.tolist()\ntrain = pd.get_dummies(train, columns=cat_cols)\ntest  = pd.get_dummies(test, columns=cat_cols)","758ba256":"#test1 = test_copy.drop(columns=['row_id'])","a886f7c6":"X = train.drop(columns=['row_id', 'date', 'num_sold'])\ny = train_copy.num_sold\n\ntest.drop(columns=['row_id', 'date'], inplace=True)","ec8c580b":"display(X)","8f7ee26a":"'''\n# All categorical columns\nobject_cols = [col for col in X.columns if X[col].dtype == \"object\"]\n\n# Columns that can be safely label encoded\ngood_label_cols = [col for col in object_cols if \n                   set(X[col]) == set(X[col])]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n        \nprint('Categorical columns that will be label encoded:', good_label_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)\n'''","3fa87496":"'''\nfrom sklearn.preprocessing import LabelEncoder\n\ncountry_le = LabelEncoder()\nX['country_le'] = country_le.fit_transform(X['country'])\ntest['country_le'] = country_le.transform(test['country'])\n\nstore_le = LabelEncoder()\nX['store_le'] = store_le.fit_transform(X['store'])\ntest['store_le'] = store_le.transform(test['store'])\n\nproduct_le = LabelEncoder()\nX['product_le'] = product_le.fit_transform(X['product'])\ntest['product_le'] = product_le.transform(test['product'])\n'''","52d45c7a":"#from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\n\n#X_train, X_val, y_train, y_val = train_test_split(X2, y, random_state=0, test_size=0.2)","82a47671":"#'''\nimport optuna\nfrom optuna.samplers import TPESampler\nfrom hyperopt import STATUS_OK,Trials,fmin,hp,tpe\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom sklearn.metrics import log_loss, accuracy_score, mean_absolute_error, r2_score, roc_auc_score\nfrom optuna.integration import XGBoostPruningCallback\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\n\ndef smape(actual, predicted):\n    numerator = np.abs(predicted - actual)\n    denominator = (np.abs(actual) + np.abs(predicted)) \/ 2\n    \n    return np.mean(numerator \/ denominator)*100\n\n\ndef objective(trial, X=X, y=y):\n    \n    X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0, test_size=0.2)\n    \n    params = {\n        \"objective\": trial.suggest_categorical('objective', [\"reg:squarederror\"]),\n        \"eval_metric\": trial.suggest_categorical('eval_metric', [\"mape\"]),\n        \"use_label_encoder\": trial.suggest_categorical('use_label_encoder', [False]),\n        \"n_estimators\": trial.suggest_categorical('n_estimators', [40000]),\n        \"learning_rate\": trial.suggest_loguniform('learning_rate', 0.15, 1.0),\n        \"subsample\": trial.suggest_float('subsample', 0.1, 1, step=0.01),\n        \"colsample_bytree\": trial.suggest_float('colsample_bytree', 0.05, 1, step=0.01),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 8),\n        \"booster\": trial.suggest_categorical('booster', [\"gbtree\"]),\n        \"gamma\": trial.suggest_float('gamma', 0, 100, step=0.1),\n        \"tree_method\": trial.suggest_categorical('tree_method', [\"gpu_hist\"]),\n        \"reg_lambda\": trial.suggest_loguniform('reg_lambda', 0.1, 100),\n        \"reg_alpha\": trial.suggest_loguniform('reg_alpha', 0.1, 100),\n        \"random_state\": trial.suggest_categorical('random_state', [42]),\n        \"n_jobs\": trial.suggest_categorical('n_jobs', [4]),\n        \"min_child_weight\": trial.suggest_categorical(\"min_child_weight\", [256]),\n            }\n    \n    #opt_params = params\n    #opt_params['n_estimators'] = 80000\n    \n    model = XGBRegressor(**params)\n\n    model.fit(\n        X_train, \n        y_train,\n        early_stopping_rounds=100,\n        eval_set=[(X_val, y_val)],\n        #eval_metric='auc',\n        verbose=False\n    )\n\n    yhat = model.predict(X_val)\n    return smape(y_val, yhat)\n\nstudy = optuna.create_study(direction=\"minimize\", sampler=TPESampler())\nstudy.optimize(objective, n_trials=100)\nprint(study.best_params)\n#'''","83f8423f":"print(\"Best value: {:.5f}\".format(study.best_value))\nprint(\"Best params:\")\n\nfor key, value in study.best_params.items():\n    print(\"{}: {}\".format(key, value))\n","93568e72":"xgb_params = study.best_params\nxgb_params","89ddcc68":"#from xgboost import XGBRegressor\n\n#xgb = XGBRegressor()\n\n#model = xgb.fit(X_train, y_train)","9904f36f":"#preds = model.predict(X_val)","be103407":"#from sklearn.metrics import mean_absolute_error, r2_score\n\n#print(\"Mean absolute error: {}\\n\".format(mean_absolute_error(y_val, preds)))\n#print(\"r2 score: {}\".format(r2_score(y_val, preds)))\n","173c71e0":"#final_preds = model.predict(test_le)","4ea47a2a":"#from sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import TimeSeriesSplit\n\n#kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n\n#preds = []\n#scores = []\n\n#%%time\nfolds = TimeSeriesSplit(10)\n\npreds = np.zeros(len(test))\nscores = []\n\nfor fold, (idx_train, idx_valid) in enumerate(folds.split(X, y)):\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n    \n    xgb = XGBRegressor(**xgb_params, \n          predictor='gpu_predictor', \n          gpu_id=0)\n\n    xgb.fit(\n        X_train, \n        y_train,\n        eval_metric='rmse',\n        early_stopping_rounds=100,\n        eval_set=[(X_valid, y_valid)], \n        verbose=False\n        )\n    \n    pred_valid = xgb.predict(X_valid)#[:,1]\n    #fpr, tpr, _ = roc_curve(y_valid, pred_valid)\n    score = smape(y_valid, pred_valid)\n    scores.append(score)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('||'*40)\n    \n    #test_preds = xgb.predict(test)#[:,1]\n    #preds.append(test_preds)\n    \n    preds += xgb.predict(test) \/ folds.n_splits\n    \nprint(f\"Overall Validation Score: {np.mean(scores)}\")\n","d3fb8cc5":"sample_fp = '..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv'\nsample = pd.read_csv(sample_fp)","2d80aa67":"submission = sample.copy()\n\nsubmission['num_sold'] = preds\n\nsubmission","d0fadb81":"submission.to_csv('submission.csv', index=False)","88d5fc68":"<h1>Submission<\/h1>","e76eead0":"<h1>K-Fold Cross Validation<\/h1>","d81a5335":"<h1>Data preprocessing and encoding<\/h1>\n<h3>Converting objects to dates using to_datetime and date<\/h3>","5c3f1285":"<center><h1>Tabular Playground Series - January 2022<\/h1><\/center>\n<center><h2>XGBoost + Optuna (Time Series)<\/h2><\/center>\n<center><h2>By Tariq Hussain<\/h2><\/center>","283b874b":"<h1>Importing and viewing data<\/h1>","4af8594a":"<h1>Optuna and hyperparameter tuning<\/h1>"}}