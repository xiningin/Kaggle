{"cell_type":{"876cb33a":"code","80eb25ce":"code","1763def6":"code","805a310f":"code","a6e94ab4":"code","ace67b08":"code","b1a43bdf":"code","92ade6cb":"code","7c1f74dc":"code","12997170":"code","91b9fff5":"code","5ee52a3c":"code","5c5e165f":"code","0891d7e8":"code","3116bcc7":"code","05057c53":"code","c7bda005":"markdown","8914960f":"markdown","979284c4":"markdown","a863478f":"markdown","bf46f3c6":"markdown","f9bc00fb":"markdown","7871089f":"markdown","4b70aa93":"markdown"},"source":{"876cb33a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","80eb25ce":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import accuracy_score","1763def6":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df\n","805a310f":"train = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv')\n\ntrain = reduce_memory_usage(train)\ntest = reduce_memory_usage(test)","a6e94ab4":"x_data = train.drop(['Id', 'Cover_Type'], axis=1)\nx_test = test.drop('Id', axis=1)\n\ny_data = train.Cover_Type","ace67b08":"scaler = StandardScaler() # I tried QuantileTransformer, but StandardScaler seems to be better by 0.005\nx_data = scaler.fit_transform(x_data)\nx_test = scaler.transform(x_test)","b1a43bdf":"encoder = OneHotEncoder()\ny_data = encoder.fit_transform(y_data.values[:, np.newaxis]).toarray()","92ade6cb":"from sklearn.model_selection import train_test_split\n\nx_train, x_val, y_train, y_val = train_test_split(x_data, y_data)","7c1f74dc":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models","12997170":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, BatchNormalization\n\n\nINPUT_SHAPE = x_train.shape[1:]\nNUM_CLASSES = train.Cover_Type.nunique()\n\ndef build_model():\n    model = Sequential([\n        Dense(units=300, kernel_initializer=\"lecun_normal\", activation=\"selu\", input_shape=INPUT_SHAPE),\n        BatchNormalization(),\n        Dense(units=200, kernel_initializer=\"lecun_normal\", activation=\"selu\"),\n        BatchNormalization(),\n        Dense(units=100, kernel_initializer=\"lecun_normal\", activation=\"selu\"),\n        BatchNormalization(),\n        Dense(units=50, kernel_initializer=\"lecun_normal\", activation=\"selu\"),\n        BatchNormalization(),\n        Dense(units=NUM_CLASSES, activation=\"softmax\")\n    ])\n\n    model.compile(\n        optimizer=\"adam\",\n        loss=\"categorical_crossentropy\",\n        metrics=[\"accuracy\"]\n    )\n\n    return model","91b9fff5":"# model = models.Sequential()\n# model.add(layers.Dense(512, activation='relu', input_shape=(54, )))\n# model.add(layers.Dense(256, activation='relu'))\n# model.add(layers.Dense(128, activation='relu'))\n# model.add(layers.Dense(32, activation='relu'))\n# model.add(layers.Dense(7, activation='softmax'))","5ee52a3c":"# model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])","5c5e165f":"model = build_model()","0891d7e8":"model.fit(x_train, y_train, batch_size=512, epochs=100)","3116bcc7":"y_pred = model.predict(x_val)\naccuracy_score(y_val.argmax(axis=1), y_pred.argmax(axis=1))","05057c53":"y_test = model.predict(x_test)\ny_test = y_test.argmax(axis=1).reshape(-1,) + 1\n\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv')\nsubmission.Cover_Type = y_test\nsubmission.to_csv('submission.csv', index=False)","c7bda005":"## Predict and save","8914960f":"## Validation","979284c4":"## train val split","a863478f":"## Data preprocessing","bf46f3c6":"## Train model","f9bc00fb":"## Import Library ","7871089f":"### Simple Keras Neural Networks ","4b70aa93":"## Read data"}}