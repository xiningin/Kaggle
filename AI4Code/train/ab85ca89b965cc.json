{"cell_type":{"5133d948":"code","64a07a54":"code","46e35a71":"code","9a2976ea":"code","b764d424":"code","c9b736c3":"code","57eb6eef":"code","b85b7209":"code","0c4032b4":"code","f4c5635f":"code","b90ac075":"code","a65c40b8":"code","f8106b7c":"code","72de98ea":"code","581916a8":"code","951ad49f":"code","b29d5499":"code","6fd1825a":"code","d5f816e6":"code","0594fd3c":"code","ab8f5224":"code","599cbe2e":"code","207c9aca":"code","d550edbe":"code","2231ca52":"code","138f6ce9":"code","c78e63fe":"code","d4a99ac4":"code","f004fbbb":"code","6a2d6a1e":"markdown"},"source":{"5133d948":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\n\nfrom catboost import CatBoost\nfrom catboost import CatBoostRegressor\nfrom catboost import Pool\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","64a07a54":"def pre_load_data():\n    train = pd.read_csv(\"..\/input\/ykc-cup-1st\/train.csv\")\n    test = pd.read_csv(\"..\/input\/ykc-cup-1st\/test.csv\")\n    sample_submission = pd.read_csv(\"..\/input\/ykc-cup-1st\/sample_submission.csv\")\n    print('train: ', train.shape)\n    print('test: ', test.shape)\n    print('sample_submission: ', sample_submission.shape)\n    return train, test, sample_submission","46e35a71":"def fe_label_encode(train, test, category_feats):\n    for c in category_feats:\n        le = LabelEncoder()\n        train[c] = le.fit_transform(train[c].fillna(\"na\"))\n        test[c] = le.transform(test[c].fillna(\"na\"))\n    print('fe_label_encode done train: ', train.shape)\n    print('fe_label_encode done test: ', test.shape)","9a2976ea":"train, test, sample_submission = pre_load_data()","b764d424":"# Sunday -> 3\n# Monday -> 1\n# Tuesday -> 5\n# Wednesday -> 6\n# Thursday -> 4\n# Friday -> 0\n# Saturday -> 2","c9b736c3":"day_of_week_1_day_shift = {\n    'Sunday': 'Saturday',\n    'Saturday': 'Friday',\n    'Friday': 'Thursday',\n    'Thursday': 'Wednesday',\n    'Wednesday': 'Tuesday',\n    'Tuesday': 'Monday',\n    'Monday': 'Sunday'\n}\n\nday_of_week_2_day_shift = {\n    'Sunday': 'Friday',\n    'Saturday': 'Thursday',\n    'Friday': 'Wednesday',\n    'Thursday': 'Tuesday',\n    'Wednesday': 'Monday',\n    'Tuesday': 'Sunday',\n    'Monday': 'Saturday'\n}\n\nday_of_week_3_day_shift = {\n    'Sunday': 'Thursday',\n    'Saturday': 'Wednesday',\n    'Friday': 'Tuesday',\n    'Thursday': 'Monday',\n    'Wednesday': 'Sunday',\n    'Tuesday': 'Saturday',\n    'Monday': 'Friday'\n}\n\nday_of_week_4_day_shift = {\n    'Sunday': 'Wednesday',\n    'Saturday': 'Tuesday',\n    'Friday': 'Monday',\n    'Thursday': 'Sunday',\n    'Wednesday': 'Saturday',\n    'Tuesday': 'Friday',\n    'Monday': 'Thursday'\n}\n\nday_of_week_5_day_shift = {\n    'Sunday': 'Tuesday',\n    'Saturday': 'Monday',\n    'Friday': 'Sunday',\n    'Thursday': 'Saturday',\n    'Wednesday': 'Friday',\n    'Tuesday': 'Thursday',\n    'Monday': 'Wednesday'\n}\n\nday_of_week_6_day_shift = {\n    'Sunday': 'Monday',\n    'Saturday': 'Sunday',\n    'Friday': 'Saturday',\n    'Thursday': 'Friday',\n    'Wednesday': 'Thursday',\n    'Tuesday': 'Wednesday',\n    'Monday': 'Tuesday'\n}","57eb6eef":"# Sunday -> 3\n# Monday -> 1\n# Tuesday -> 5\n# Wednesday -> 6\n# Thursday -> 4\n# Friday -> 0\n# Saturday -> 2\n\ndef fe_pipeline(train, test):\n    train['is_test'] = 0\n    test['is_test'] = 1\n\n    data = pd.concat([train, test], sort=False)\n    data.reset_index(drop=True, inplace=True)\n\n    data['shift_1'] = data['day_of_week'].map(day_of_week_1_day_shift)\n    data['shift_2'] = data['day_of_week'].map(day_of_week_2_day_shift)\n    data['shift_3'] = data['day_of_week'].map(day_of_week_3_day_shift)\n    data['shift_4'] = data['day_of_week'].map(day_of_week_4_day_shift)\n    data['shift_5'] = data['day_of_week'].map(day_of_week_5_day_shift)\n    data['shift_6'] = data['day_of_week'].map(day_of_week_6_day_shift)\n    \n    data['shift_1'] = data['store_id'] + '_' + data['shift_1']\n    data['shift_2'] = data['store_id'] + '_' + data['shift_2']\n    data['shift_3'] = data['store_id'] + '_' + data['shift_3']\n    data['shift_4'] = data['store_id'] + '_' + data['shift_4']\n    data['shift_5'] = data['store_id'] + '_' + data['shift_5']\n    data['shift_6'] = data['store_id'] + '_' + data['shift_6']\n\n    id_target = data[['id', 'log_visitors']].set_index('id')\n    data['shift_1'] = data['shift_1'].map(id_target['log_visitors'])\n    data['shift_2'] = data['shift_2'].map(id_target['log_visitors'])\n    data['shift_3'] = data['shift_3'].map(id_target['log_visitors'])\n    data['shift_4'] = data['shift_4'].map(id_target['log_visitors'])\n    data['shift_5'] = data['shift_5'].map(id_target['log_visitors'])\n    data['shift_6'] = data['shift_6'].map(id_target['log_visitors'])\n\n\n    shift_week = ['shift_{}'.format(idx) for idx in range(1, 7)]\n    data['visitor_week_mean'] = data[shift_week].mean(axis=1)\n    data['visitor_week_std'] = data[shift_week].std(axis=1)\n    data['visitor_week_max'] = data[shift_week].max(axis=1)\n    data['visitor_week_min'] = data[shift_week].min(axis=1)\n    data['visitor_week_median'] = data[shift_week].median(axis=1)\n    data['max_min_diff'] = data['visitor_week_max'] - data['visitor_week_min']\n\n    train = data[data['is_test']==0].reset_index(drop=True)\n    test = data[data['is_test']==1].reset_index(drop=True)\n    del train['is_test']\n    del test['is_test']\n    \n    print('fe_pipeline done, train: {}, test: {}'.format(train.shape, test.shape))\n    return train, test","b85b7209":"train, test = fe_pipeline(train, test)\n\n# train['genre_area'] = train['genre_name'] + '_' + train['area_name']\n# test['genre_area'] = test['genre_name'] + '_' + test['area_name']\n\ncategory_feats = [\"store_id\", \"genre_name\", \"area_name\", \"day_of_week\"]\nfe_label_encode(train, test, category_feats)\n\ntrain.head()","0c4032b4":"# \u66dc\u65e5\u6301\u3061\n\n# train['is_test'] = 0\n# test['is_test'] = 1\n\n# data = pd.concat([train, test], sort=False)\n# data.reset_index(drop=True, inplace=True)\n\n# data['Sunday'] = data['store_id'] + '_Sunday'\n# data['Monday'] = data['store_id'] + '_Monday'\n# data['Tuesday'] = data['store_id'] + '_Tuesday'\n# data['Wednesday'] = data['store_id'] + '_Wednesday'\n# data['Thursday'] = data['store_id'] + '_Thursday'\n# data['Friday'] = data['store_id'] + '_Friday'\n# data['Saturday'] = data['store_id'] + '_Saturday'\n\n# print(data.shape)\n# data.head()\n\n# id_target = data[['id', 'log_visitors']].set_index('id')\n# day_of_week_cols = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n# for day_of_week in day_of_week_cols:\n#     target_idx = data[data['day_of_week'] != day_of_week].index\n#     not_target_idx = data[data['day_of_week'] == day_of_week].index\n#     data.loc[target_idx, day_of_week] = data.loc[target_idx, day_of_week].map(id_target['log_visitors'])\n#     data.loc[not_target_idx, day_of_week] = np.nan\n#     data[day_of_week] = data[day_of_week].astype('float')\n\n\n# data['visitor_week_mean'] = data[day_of_week_cols].mean(axis=1)\n# data['visitor_week_std'] = data[day_of_week_cols].std(axis=1)\n\n# train = data[data['is_test']==0].reset_index(drop=True)\n# test = data[data['is_test']==1].reset_index(drop=True)\n# del train['is_test']\n# del test['is_test']","f4c5635f":"def model_get_feats(train, excluded_feats):\n    features = [col for col in train.columns if col not in excluded_feats]\n    print('features len: ', len(features))\n    return features","b90ac075":"def model_get_fold(train, n_split=5, fold_type=None, seed=24):\n    if fold_type == 'kfold':\n        kfold = KFold(n_splits=n_split, shuffle = True, random_state=seed)\n        fold = kfold.split(train)\n    elif fold_type == 'stratified':\n        skfold = StratifiedKFold(n_splits=n_split, shuffle = True, random_state=seed)\n        fold = skfold.split(train, train['store_id'])\n    else:\n        print('invalid fold_type: ', fold_type)\n    return fold\n\n# def model_target_encoding():\n    ","a65c40b8":"def after_preparation_importance(feats, model, n_fold):\n    imp_df = pd.DataFrame()\n    imp_df[\"feature\"] = feats\n    imp_df[\"importance\"] = model.feature_importance(importance_type='gain', iteration=model.best_iteration)\n    imp_df[\"fold\"] = n_fold + 1\n    return imp_df\n\ndef after_display_importances(feature_importance_df_):\n    cols = feature_importance_df_[[\"feature\", \"importance\"]]\\\n        .groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n        \n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n\n    plt.figure(figsize=(8, 10))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.show()","f8106b7c":"def model_run_catboost(train, test, features, category_features):\n    n_split = 3\n#     fold = model_get_fold(train, n_split, fold_type='kfold', seed=427)\n    fold = model_get_fold(train, n_split, fold_type='stratified', seed=427)\n\n    preds_cv = []\n    preds_test = []\n\n    for idx, (train_idx, valid_idx) in enumerate(fold):\n        ## train data\n        x_train = train.loc[train_idx, features]\n        y_train = train.loc[train_idx, 'log_visitors']\n\n        ## valid data\n        x_valid = train.loc[valid_idx, features]\n        y_valid = train.loc[valid_idx, 'log_visitors']\n        \n        train_pool = Pool(x_train, y_train, cat_features=category_features)\n        valid_pool = Pool(x_valid, y_valid, cat_features=category_features)\n        test_pool = Pool(test[features], cat_features=category_features) \n        \n        model = CatBoostRegressor(\n            loss_function=\"RMSE\",\n            eval_metric=\"RMSE\",\n            learning_rate=0.05,\n            iterations=1000,\n            random_seed=27,\n            depth=7,\n            early_stopping_rounds=100\n        )\n        \n        model = model.fit(\n            train_pool,\n            eval_set=valid_pool,\n            use_best_model=True,\n            verbose=100,\n#             plot=True\n        )\n        \n        # valid\n        valid_preds = model.predict(valid_pool)\n        valid_rmse = mean_squared_error(valid_preds, y_valid)**0.5\n        preds_cv.append(valid_rmse)\n        train.loc[valid_idx, 'catboost_pred'] = valid_preds\n\n        # test\n        pred_test = model.predict(test_pool)\n        preds_test.append(pred_test)\n\n        print('Valid Metric: {:.5f}'.format(valid_rmse))\n\n    print('Valid OverAll: {:.5f}'.format(np.mean(preds_cv)))\n    return preds_test, preds_cv","72de98ea":"excluded_feats = ['log_visitors', 'id', 'lgb_pred', 'catboost_pred']\ncatboost_features = model_get_feats(train, excluded_feats)\n\ncategory_features = ['day_of_week', 'genre_name', 'area_name'] \ncatboost_preds_test, catboost_preds_cv = model_run_catboost(train, test, catboost_features, category_features)","581916a8":"# Valid OverAll: 0.10199\n# Valid OverAll: 0.33256 LB: 0.32056 \u30ab\u30c6\u30b4\u30ea\u7cfb\u306e\u307f\n# Valid OverAll: 0.22411\n# Valid OverAll: 0.29982\n# Valid OverAll: 0.29802 LB: 0.28003 store_id\u629c\u304d depth 7\n# Valid OverAll: 0.29947 feats\u304b\u3089store_id\u629c\u304d depth 7","951ad49f":"def model_run_lgb(train, test, lgb_params, features):\n    n_split = 3\n#     fold = model_get_fold(train, n_split, fold_type='kfold', seed=24)\n    fold = model_get_fold(train, n_split, fold_type='stratified', seed=24)\n\n    preds_cv = []\n    preds_test = []\n    feature_importance_df = pd.DataFrame()\n\n    for idx, (train_idx, valid_idx) in enumerate(fold):\n        print(\"--------fold {} -------\".format(idx+1))\n\n        ## train data\n        x_train = train.loc[train_idx, features]\n        y_train = train.loc[train_idx, 'log_visitors']\n\n        ## valid data\n        x_valid = train.loc[valid_idx, features]\n        y_valid = train.loc[valid_idx, 'log_visitors']\n\n\n    #     # target encoding\n    #     target_encoding_category = ['genre_area']\n    #     for category in target_encoding_category:\n    #         # valid\n    #         data_tmp = pd.DataFrame({category: x_train[category].values, 'target': y_train.values.flatten()})\n    #         target_mean = data_tmp.groupby(category)['target'].mean()\n    #         x_valid.loc[:, category] = x_valid[category].map(target_mean)\n\n    #         # test\n    #         test_data_tmp = pd.DataFrame({category: train[category].values, 'target': train[target].values.flatten()})\n    #         test_target_mean = test_data_tmp.groupby(category)['target'].mean()\n    #         test.loc[:, category] = test[category].map(test_target_mean)\n\n    #         # train\n    #         tmp = np.repeat(np.nan, len(x_train))\n    #         kf_encoding = KFold(n_splits=5, shuffle=True, random_state=777)\n    #         for idx_1, idx_2 in kf_encoding.split(x_train):\n    #             target_mean = data_tmp.iloc[idx_1].groupby(category)['target'].mean()\n    #             tmp[idx_2] = x_train[category].iloc[idx_2].map(target_mean)\n\n    #         x_train.loc[:, category] = tmp\n\n\n        lgb_train = lgb.Dataset(x_train.values, label=y_train)\n        lgb_valid = lgb.Dataset(x_valid.values, label=y_valid)\n\n        model = lgb.train(\n            lgb_params,\n            lgb_train,\n            valid_sets=[lgb_train, lgb_valid],\n            valid_names=['train', 'valid'],\n            num_boost_round=3000,\n            early_stopping_rounds=100,\n            verbose_eval=100\n        )\n\n        # valid\n        valid_preds = model.predict(x_valid.values)\n        valid_rmse = mean_squared_error(valid_preds, y_valid)**0.5\n        preds_cv.append(valid_rmse)\n        train.loc[valid_idx, 'lgb_pred'] = valid_preds\n\n        # test\n        pred_test = model.predict(test[features].values)\n        preds_test.append(pred_test)\n\n        # importance\n        fold_importance_df = after_preparation_importance(features, model, n_split)\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n        print('Valid Metric: {:.5f}'.format(valid_rmse))\n\n    print('Valid OverAll: {:.5f}'.format(np.mean(preds_cv)))\n    return feature_importance_df, preds_test, preds_cv\n\ndef model_run_lgb_by_all_data(train, test, lgb_params, features, num_boost_round):\n    preds_test = []\n    feature_importance_df = pd.DataFrame()\n    \n    x_train = train[features]\n    y_train = train['log_visitors']\n    lgb_train = lgb.Dataset(x_train.values, label=y_train)\n    \n    model = lgb.train(\n        lgb_params,\n        lgb_train,\n        valid_sets=[lgb_train],\n        valid_names=['train'],\n        num_boost_round=num_boost_round,\n        verbose_eval=10\n    )\n\n    pred_test = model.predict(test[features].values)\n    preds_test.append(pred_test)\n    \n    fold_importance_df = after_preparation_importance(features, model, 1)\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    return feature_importance_df, preds_test","b29d5499":"lgb_params = {\n    'boost': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'learning_rate': 0.05, \n    \"max_depth\": 14, # default: -1\n    \"lambda_l1\": 0.01, # default: 0\n    \"lambda_l2\": 0.01, # default: 0\n#     \"bagging_fraction\": 0.7,\n#     \"bagging_freq\": 5,\n#     \"feature_fraction\": 0.8, \n    'verbosity': 1,\n}\n\nexcluded_feats = ['log_visitors', 'id', 'lgb_pred', 'catboost_pred', 'store_id']\nlgb_features = model_get_feats(train, excluded_feats)\n\n# feature_importance_df, preds_test, preds_cv = model_run_lgb(train, test, lgb_params, lgb_features)\n\nnum_boost_round = 140\n_feature_importance_df, preds_test_140 = model_run_lgb_by_all_data(train, test, lgb_params, lgb_features, num_boost_round)\n\nnum_boost_round = 150\n_feature_importance_df, preds_test_150 = model_run_lgb_by_all_data(train, test, lgb_params, lgb_features, num_boost_round)\n\nnum_boost_round = 160\n_feature_importance_df, preds_test_160 = model_run_lgb_by_all_data(train, test, lgb_params, lgb_features, num_boost_round)","6fd1825a":"# Valid OverAll: 0.30996 LB: 0.28299\n# Valid OverAll: 0.30887 LB: 0.28610 area_genre\u3067target encoding\n# Valid OverAll: 0.30224 LB: 0.28163\n# Valid OverAll: 0.30102 LB: 0.28185 shift\u3067\u6301\u3064\u3088\u3046\u306b\u3057\u305f\n# Valid OverAll: 0.30113","d5f816e6":"# after_display_importances(feature_importance_df)","0594fd3c":"# catboost_score = mean_squared_error(train['catboost_pred'].values, train['log_visitors'].values)**0.5\n# lgb_score = mean_squared_error(train['lgb_pred'].values, train['log_visitors'].values)**0.5\n# ensemble_score = mean_squared_error(0.6*train['catboost_pred'].values + 0.4*train['lgb_pred'].values, train['log_visitors'].values)**0.5\n# print('catboost: {:.5}'.format(catboost_score))\n# print('lgb: {:.5}'.format(lgb_score))\n# print('ensemble: {:.5}'.format(ensemble_score))","ab8f5224":"# catboost: 0.29802\n# lgb: 0.30103\n# ensemble: 0.29718","599cbe2e":"# final_pred = np.mean(np.vstack(preds_test), axis = 0)\n# sample_submission[\"log_visitors\"] = final_pred\n# sample_submission.to_csv(\"submission.csv\", index = False)\n\n# final_pred = np.mean(np.vstack(catboost_preds_test), axis = 0)\n# sample_submission[\"log_visitors\"] = final_pred\n# sample_submission.to_csv(\"submission.csv\", index = False)\n\n# catboost_test = np.mean(np.vstack(catboost_preds_test), axis = 0)\n# lgb_test = np.mean(np.vstack(preds_test), axis = 0)\n\n# final_pred = 0.6*catboost_test + 0.4*lgb_test\n# sample_submission[\"log_visitors\"] = final_pred\n# sample_submission.to_csv(\"submission.csv\", index = False)\n\n\nlgb_test = (preds_test_140[0] + preds_test_150[0] + preds_test_160[0]) \/ 3\ncatboost_test = np.mean(np.vstack(catboost_preds_test), axis = 0)\n\n\nfinal_pred = 0.65*catboost_test + 0.35*lgb_test\nsample_submission[\"log_visitors\"] = final_pred\nsample_submission.to_csv(\"submission.csv\", index = False)","207c9aca":"# new_train, new_test, sample_submission = pre_load_data()\n# new_test['log_visitors'] = final_pred\n\n# new_train, new_test = fe_pipeline(new_train, new_test)\n# del new_test['log_visitors']\n# category_feats = [\"store_id\", \"genre_name\", \"area_name\", \"day_of_week\"]\n# fe_label_encode(new_train, new_test, category_feats)\n\n# new_train.head()","d550edbe":"# logistic_feats = [\n#     \"shift_1\", \n#     \"shift_2\", \n#     \"shift_3\",\n#     'shift_4',\n#     'shift_5',\n#     'shift_6',\n#     'visitor_week_mean',\n#     'visitor_week_std'\n# ]","2231ca52":"# new_train.columns","138f6ce9":"# from sklearn import linear_model\n\n## lasso\n# n_split = 3\n# fold = model_get_fold(new_train, n_split, fold_type='stratified')\n\n# preds_cv = []\n# new_preds_test = []\n\n# for idx, (train_idx, valid_idx) in enumerate(fold):\n#     print(\"--------fold {} -------\".format(idx+1))\n\n#     ## train data\n#     x_train = new_train.loc[train_idx, logistic_feats].fillna(0)\n#     y_train = new_train.loc[train_idx, target]\n\n#     ## valid data\n#     x_valid = new_train.loc[valid_idx, logistic_feats].fillna(0)\n#     y_valid = new_train.loc[valid_idx, target]\n\n#     clf = linear_model.Lasso(alpha=0.1).fit(x_train, y_train)\n    \n#     # valid\n#     valid_preds = clf.predict(x_valid)\n#     valid_rmse = mean_squared_error(valid_preds, y_valid)**0.5\n#     preds_cv.append(valid_rmse)\n\n#     # test\n#     pred_test = clf.predict(new_test[logistic_feats].fillna(0).values)\n#     new_preds_test.append(pred_test)\n\n#     print('Valid Metric: {:.5f}'.format(valid_rmse))\n\n# print('Valid OverAll: {:.5f}'.format(np.mean(preds_cv)))\n\n## lgb\n# lgb_params = {\n#     'boost': 'gbdt',\n#     'objective': 'regression',\n#     'metric': 'rmse',\n#     'learning_rate': 0.05, \n#     \"max_depth\": 3, # default: -1\n#     \"lambda_l1\": 10, # default: 0\n#     'verbosity': 1,\n# }\n# excluded_feats = target + ['id', 'visitor_week_mean', 'visitor_week_std',\n#        'visitor_week_max', 'visitor_week_min', 'visitor_week_median',\n#        'max_min_diff']\n# features = [col for col in new_train.columns if col not in excluded_feats]\n# print('features len: ', len(features))\n\n# new_feature_importance_df, new_preds_test = model_run_lgb(new_train, new_test, lgb_params, features)","c78e63fe":"# Valid OverAll: 0.20349 LB: 0.30741\n# Valid OverAll: 0.24286","d4a99ac4":"# after_display_importances(new_feature_importance_df)","f004fbbb":"# new_final_pred = np.mean(np.vstack(new_preds_test), axis = 0)\n# sample_submission[\"log_visitors\"] = new_final_pred\n# sample_submission.to_csv(\"submission.csv\", index = False)","6a2d6a1e":"# Pseudo Labeling"}}