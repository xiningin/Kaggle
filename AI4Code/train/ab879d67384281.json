{"cell_type":{"baaea868":"code","b027c62d":"code","27faee08":"code","3e650e87":"code","dc1ff47a":"code","59516f4d":"code","871628ab":"code","1d4b1b4d":"code","86d9e36a":"code","225a980e":"code","626d2873":"code","a4ac561e":"code","d6d68d5c":"code","2a616462":"code","1fd288ac":"code","e483ad94":"code","fe545a6d":"code","65f5a1f1":"code","7116c7f4":"code","35fd9e2a":"code","59c44629":"code","8c690b2d":"markdown","3f9efd0c":"markdown","50347f83":"markdown","298541ba":"markdown","bba68bfc":"markdown","8907eca0":"markdown","269395a3":"markdown","133ed238":"markdown","fd3ade94":"markdown","003b71c0":"markdown","b9ed2ff3":"markdown"},"source":{"baaea868":"!pip install -U sentence-transformers","b027c62d":"from sentence_transformers import SentenceTransformer\nsentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n\nmodel = SentenceTransformer('sentence-transformers\/msmarco-distilbert-base-v4')\nembeddings = model.encode(sentences)\nprint(embeddings)","27faee08":"from transformers import AutoTokenizer, AutoModel\nimport torch\n\n\n#Mean Pooling - Take attention mask into account for correct averaging\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) \/ torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\n# Sentences we want sentence embeddings for\nsentences = ['This is an example sentence', 'Each sentence is converted']\n\n# Load model from HuggingFace Hub\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers\/msmarco-distilbert-base-v4')\nmodel = AutoModel.from_pretrained('sentence-transformers\/msmarco-distilbert-base-v4')\n\n# Tokenize sentences\nencoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n\n# Compute token embeddings\nwith torch.no_grad():\n    model_output = model(**encoded_input)\n\n# Perform pooling. In this case, max pooling.\nsentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n\nprint(\"Sentence embeddings:\")\nprint(sentence_embeddings)","3e650e87":"# !pip install sentence-transformers","dc1ff47a":"import json\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom sentence_transformers import SentenceTransformer","59516f4d":"model = SentenceTransformer('msmarco-distilbert-base-v4', device=\"cuda\")","871628ab":"df = pd.read_json(\"..\/input\/corpusjson\/corpus.json\")","1d4b1b4d":"df.head()","86d9e36a":"df.shape","225a980e":"df.columns","626d2873":"# Here we ancode all startup descriptions\n# We do encoding in batches, as this reduces overhead costs and significantly speeds up the process\nvectors = []\nbatch_size = 64\nbatch = []\nfor row in tqdm(df.itertuples()):\n    description =row.file_data\n    batch.append(description)\n    if len(batch) >= batch_size:\n        vectors.append(model.encode(batch))  # Text -> vector encoding happens here\n        batch = []\n\nif len(batch) > 0:\n    vectors.append(model.encode(batch))\n    batch = []\n\nvectors = np.concatenate(vectors)","a4ac561e":"# Now we have all our descriptions converted into vectors.\n# We have 40474 vectors of 768 dimentions. The output layer of the model has this dimension\nvectors.shape","d6d68d5c":"# You can download this saved vectors and continue with rest part.\nnp.save(\".\/final_vector_list.npy\", vectors, allow_pickle=False)","2a616462":"# You can download this Model and continue with rest part.\nmodel.save(\".\/model\")","1fd288ac":"from sklearn.metrics.pairwise import cosine_similarity","e483ad94":"# Take a random description as a query\nsample_query = df.iloc[12345].file_data\nprint(sample_query)","fe545a6d":"# Convert query description into a vector.\nquery_vector = model.encode(sample_query)  ","65f5a1f1":"# Look for the most similar vectors, manually score all vectors\nscores = cosine_similarity([query_vector], vectors)[0] ","7116c7f4":"# Select top-5 with vectors the largest scores\ntop_scores_ids = np.argsort(scores)[-5:][::-1] ","35fd9e2a":"# Displaying Selected top-5 vectors with the largest scores\ntop_scores_ids ","59c44629":"# Check if result similar to the query\nfor top_id in top_scores_ids:\n    print(\"New_story: \")\n    print(df.iloc[top_id].file_data)\n    print(\"-------------------------------------------------------------------------------------\")","8c690b2d":"To install Sentence Transformer use this command:\n\n**!pip install -U sentence-transformers**","3f9efd0c":"- Importing library","50347f83":"### SBERT\n- Sentence-Transformers is a state-of-the-art Python framework for embedding sentences, text and images. These embeddings can then be used for classification or clustering. For example, with the help of cosine similarity between embeddings, you can find sentences with similar meanings. This can be useful for semantic text similarity, semantic search, or paraphrase mining.","298541ba":"- Since I have already install sentence transformer, I am commenting it","bba68bfc":"# Conslusion:\n\n- I think This performed very well on this dataset and I am also getting the desired output\n- Since the transformer are huge in size its computationally expensive and takes a bit long time in training but performs wells!!","8907eca0":"![](https:\/\/ichi.pro\/assets\/images\/max\/640\/1*Bi-39aWOdFElYQOI7DSnWg.png)","269395a3":"![](https:\/\/towardsml.files.wordpress.com\/2019\/09\/bert.png)","133ed238":"- Evaluation ","fd3ade94":"### Usage (HuggingFace Transformers)\n- Without sentence-transformers, you can use the model like this: First, you pass your input through the transformer model, then you have to apply the right pooling-operation on-top of the contextualized word embeddings.\n\n","003b71c0":"# Actual Impletation","b9ed2ff3":"I will be using\n**sentence-transformers\/msmarco-distilbert-base-v4**"}}