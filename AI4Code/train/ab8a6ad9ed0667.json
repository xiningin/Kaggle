{"cell_type":{"cc057854":"code","007b1018":"code","09db49a5":"code","0c3fa88a":"code","de7a26c5":"code","511ed7b3":"code","3ccadb77":"code","50d7ffc2":"code","6acd4e7c":"code","da033c91":"code","3ebb7eff":"code","37e078cd":"code","bfbd6585":"code","02b2975f":"code","ad7087a8":"code","4c2ee3e5":"code","644574ce":"code","71f52fb9":"code","7ea0cd56":"code","41b0ce00":"code","585013e3":"code","da2d3c06":"code","d7721593":"code","7ef2921f":"code","5e705231":"code","e631e222":"code","c8df9a55":"code","b254268b":"code","a7bf7a10":"code","c5a936f5":"code","ffc1f302":"code","ea52d456":"code","0c5cd397":"code","e624cb19":"markdown","7d71cd88":"markdown","20ef6cc8":"markdown","c7dbb6a0":"markdown","33dc0a03":"markdown","b0f5c103":"markdown","5cf42341":"markdown","1fd6b00b":"markdown","ff459476":"markdown","3feaa819":"markdown","577c0c1b":"markdown","2e1b6eb1":"markdown"},"source":{"cc057854":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","007b1018":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', 500)\n%matplotlib inline","09db49a5":"df=pd.read_csv('\/kaggle\/input\/heart-failure-prediction\/heart.csv')","0c3fa88a":"df","de7a26c5":"total = df.isnull().sum().sort_values(ascending=False)\npercent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(12)","511ed7b3":"df.dtypes","3ccadb77":"df['HeartDisease'].value_counts()","50d7ffc2":"from sklearn.model_selection import train_test_split\ntrain_set, test_set=train_test_split(df, test_size=.2, random_state=42)","6acd4e7c":"train_y=train_set[['HeartDisease']]\ntrain_x=train_set.drop(columns='HeartDisease')\ntest_y=test_set[['HeartDisease']]\ntest_x=test_set.drop(columns='HeartDisease')","da033c91":"#Let's see our correlation matrix\ncorrmat = train_set.corr()\nf, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(corrmat, vmax=.8, square=True, annot=True);","3ebb7eff":"from sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nnumcols=train_x.select_dtypes(include=['float64','int64']).columns\ncatcols=train_x.select_dtypes(include=['object']).columns\ntransformer = ColumnTransformer(transformers=[('cat', OneHotEncoder(), catcols),('num',StandardScaler(),numcols)])\ncattransformer= ColumnTransformer(transformers=[('cat', OneHotEncoder(), catcols)],remainder='passthrough')","37e078cd":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.pipeline import Pipeline","bfbd6585":"from sklearn import metrics\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn import model_selection\n\nacc={}\nscores=['accuracy','roc_auc','precision','recall','f1']\nfor model in [DummyClassifier, LogisticRegression,GaussianNB,KNeighborsClassifier,SVC]:\n    if model==LogisticRegression:\n        cls=model(solver='liblinear')\n    else:\n        cls=model()\n    pipe=Pipeline(steps=[\n        (\"preprocess\",cattransformer),\n        (\"model\",cls)\n    ])\n    kfold=model_selection.KFold(n_splits=10)\n#    train_x_std=transformer.fit_transform(train_x)\n #   print(train_x_std)\n    s=model_selection.cross_validate(pipe,train_x,train_y,scoring=scores,cv=kfold,verbose=1)\n    print(\n        f\"{model.__name__:22} Fit Time: \"\n        f\"{s['score_time'].mean():.3f}s  Accuracy: \"\n        f\"{s['test_accuracy'].mean():.3f}  ROC-AUC: \"\n        f\"{s['test_roc_auc'].mean():.3f}  Precision: \"\n        f\"{s['test_precision'].mean():.3f}  Recall: \"\n        f\"{s['test_recall'].mean():.3f}  F1: \"\n        f\"{s['test_f1'].mean():.3f}\"\n    )\n    acc.update({model.__name__:s['test_accuracy'].mean()})","02b2975f":"for model in [xgb.XGBClassifier, lgb.LGBMClassifier,DecisionTreeClassifier, RandomForestClassifier]:\n    if model==xgb.XGBClassifier:\n        cls=model(eval_metric='mlogloss')\n    else:\n        cls=model()\n    pipe=Pipeline(steps=[\n        (\"preprocess\",cattransformer),\n        (\"model\",cls)\n    ])\n    kfold=model_selection.KFold(n_splits=10)\n    s=model_selection.cross_validate(pipe,train_x,train_y,scoring=scores,cv=kfold,verbose=1)\n    print(\n        f\"{model.__name__:22} Fit Time: \"\n        f\"{s['score_time'].mean():.3f}s  Accuracy: \"\n        f\"{s['test_accuracy'].mean():.3f}  ROC-AUC: \"\n        f\"{s['test_roc_auc'].mean():.3f}  Precision: \"\n        f\"{s['test_precision'].mean():.3f}  Recall: \"\n        f\"{s['test_recall'].mean():.3f}  F1: \"\n        f\"{s['test_f1'].mean():.3f}\"\n    )\n    acc.update({model.__name__:s['test_accuracy'].mean()})\n    ","ad7087a8":"import operator\nprint(\"Best Models (in order of accuracy):\")\nacc_list = sorted(acc.items(), key=lambda x: x[1], reverse=True)\nfor i in acc_list:\n    print(\n        f\"{i[0]:30} \"\n        f\"{i[1]:.3f}\"\n    )","4c2ee3e5":"from mlxtend.classifier import StackingClassifier\nclfs=[\n    x()\n    for x in [\n        LogisticRegression,\n        GaussianNB,\n        KNeighborsClassifier,\n        SVC,\n        RandomForestClassifier,\n        xgb.XGBClassifier, \n        lgb.LGBMClassifier,\n        DecisionTreeClassifier\n    ]\n]\nstack=StackingClassifier(\n    classifiers=clfs,\n    meta_classifier=RandomForestClassifier()\n)\npipe=Pipeline(steps=[\n        (\"preprocess\",transformer),\n        (\"model\",stack)\n])\nkfold=model_selection.KFold(n_splits=10)\ns=model_selection.cross_validate(pipe,train_x,train_y,scoring=scores,cv=kfold,verbose=1)\nprint(\n        f\"Stack Fit Time: \"\n        f\"{s['score_time'].mean():.3f}s  Accuracy: \"\n        f\"{s['test_accuracy'].mean():.3f}  ROC-AUC: \"\n        f\"{s['test_roc_auc'].mean():.3f}  Precision: \"\n        f\"{s['test_precision'].mean():.3f}  Recall: \"\n        f\"{s['test_recall'].mean():.3f}  F1: \"\n        f\"{s['test_f1'].mean():.3f}\"\n)","644574ce":"accnew={}\nfrom sklearn.model_selection import GridSearchCV\nparams = [{\n        'model__penalty': ['l1','l2'],\n        'model__C': np.arange(0,1,.01),\n        'model__solver':['liblinear','saga']\n    }]\npipe=Pipeline(steps=[\n        (\"preprocess\",transformer),\n        (\"model\",LogisticRegression())\n])\nlingrid = GridSearchCV(estimator=pipe,           \n                      param_grid=params, \n                      cv=5,\n                      scoring='accuracy',\n                      verbose=0) \nprint(\"Linear Regression\")\nlingrid.fit(train_x, train_y)\naccnew.update({'Logistic Regression':lingrid.best_score_})\nprint(lingrid.best_score_)\n#print(kgrid.best_estimator_)\nprint(lingrid.best_params_)","71f52fb9":"from sklearn.model_selection import GridSearchCV\nparams = [{\n        'model__n_neighbors':  np.arange(1,50,1),\n        'model__p':[1,2],\n        'model__weights':['uniform','distance']\n    }]\npipe=Pipeline(steps=[\n        (\"preprocess\",transformer),\n        (\"model\",KNeighborsClassifier())\n])\nkgrid = GridSearchCV(estimator=pipe,           \n                      param_grid=params, \n                      cv=5,\n                      verbose=0) \nprint(\"K Nearest Neighbors\")\nkgrid.fit(train_x, train_y)\naccnew.update({'KNN':kgrid.best_score_})\nprint(kgrid.best_score_)\nprint(kgrid.best_params_)\n#print(list(zip(kgrid.cv_results_['params'],kgrid.cv_results_['mean_test_score'])))","7ea0cd56":"params = [{\n        'model__var_smoothing': np.arange(0,1,.01),\n    }]\npipe=Pipeline(steps=[\n        (\"preprocess\",transformer),\n        (\"model\",GaussianNB())\n])\nbaygrid = GridSearchCV(estimator=pipe,           \n                      param_grid=params, \n                      cv=5,\n                      scoring='accuracy',\n                      verbose=0)\nprint(\"Naive Bayes\")\nbaygrid.fit(train_x, train_y)\naccnew.update({'Naive Bayes':baygrid.best_score_})\nprint(baygrid.best_score_)\n#print(kgrid.best_estimator_)\nprint(baygrid.best_params_)\n#print(list(zip(baygrid.cv_results_['params'],baygrid.cv_results_['mean_test_score'])))","41b0ce00":"params = [{\n        'model__C': np.arange(.1,1,.1),\n        'model__degree':np.arange(1,5,1),\n        'model__gamma':['auto','scale'],\n        'model__kernel':['linear','poly','rbf','sigmoid','precomputed'],\n        'model__probability':[True],         \n    }]\npipe=Pipeline(steps=[\n        (\"preprocess\",transformer),\n        (\"model\",SVC())\n])\nsvcgrid = GridSearchCV(estimator=pipe,           \n                      param_grid=params, \n                      cv=5,\n                      verbose=0) \nprint(\"SVC\")\nsvcgrid.fit(train_x, train_y)\naccnew.update({'SVC':svcgrid.best_score_})\nprint(svcgrid.best_score_)\nprint(svcgrid.best_params_)\n#print(list(zip(svcgrid.cv_results_['params'],svcgrid.cv_results_['mean_test_score'])))","585013e3":"from sklearn.model_selection import RandomizedSearchCV\nparams = [{\n'model__max_depth': np.arange(1,51,1),\n'model__min_samples_split': np.arange(1,51,1),\n'model__n_estimators': np.arange(50,101,1),\n    }]\npipe=Pipeline(steps=[\n        (\"preprocess\",cattransformer),\n        (\"model\",RandomForestClassifier())\n])\n#fgrid = GridSearchCV(estimator=pipe,           \n#                      param_grid=params, \n#                      cv=5,\n#                      verbose=1) \nfgrid=RandomizedSearchCV(pipe,params,n_iter=100,cv=5,verbose=1)\nprint(\"Random Forest\")\nfgrid.fit(train_x, train_y)\naccnew.update({'Random Forest':fgrid.best_score_})\nprint(fgrid.best_score_)\nprint(fgrid.best_params_)\n#print(list(zip(fgrid.cv_results_['params'],fgrid.cv_results_['mean_test_score'])))","da2d3c06":"params = {\n    'model__max_depth': np.arange(1,11,1),\n    'model__n_estimators': np.arange(10,101,10),\n    'model__learning_rate': [.5,0.1,0.05,0.01],\n    'model__booster':['gbtree','gblinear','dart'],\n    'model__reg_alpha':np.arange(.1,5,.5),\n    'model__reg_lambda':np.arange(1,11,1),\n    'model__verbosity':[0],\n    'model__objective':['binary:logistic'],\n    'model__eval_metric':['mlogloss'],\n    \n}\npipe=Pipeline(steps=[\n        (\"preprocess\",cattransformer),\n        (\"model\",xgb.XGBClassifier())\n])\nxgrid=RandomizedSearchCV(pipe,params,n_iter=500,cv=5,verbose=1)\n#xgrid = GridSearchCV(estimator=pipe,         \n  #                    param_grid=params, \n  #                    cv=5,\n   #                   verbose=1) \nprint(\"XGBoost\")\nxgrid.fit(train_x, train_y)\naccnew.update({'XGBoost':xgrid.best_score_})\nprint(xgrid.best_score_)\nprint(xgrid.best_params_)\n#print(list(zip(xgrid.cv_results_['params'],xgrid.cv_results_['mean_test_score'])))","d7721593":"#LGBM\nparams = {\n    'model__num_leaves':np.arange(2,21,1),\n    'model__min_child_samples': np.arange(5,101,5),\n    'model__n_estimators':np.arange(5,101,5),\n    'model__boosting_type':['gbdt','dart','goss'],\n    'model__learning_rate': [.5,0.1,0.05,0.01],\n    'model__objective':['binary'],\n    'model__reg_alpha':np.arange(.1,5,.5),\n    'model__reg_lambda':np.arange(1,11,1),\n}\npipe=Pipeline(steps=[\n        (\"preprocess\",cattransformer),\n        (\"model\",lgb.LGBMClassifier())\n])\n#lgrid = GridSearchCV(estimator=pipe,         \n  #                    param_grid=params, \n    #                  cv=5,\n    #                  verbose=1) \nlgrid=RandomizedSearchCV(pipe,params,n_iter=500,cv=5,verbose=1)\nlgrid.fit(train_x, train_y)\nprint(\"LGBM\")\naccnew.update({'LGBM':lgrid.best_score_})\nprint(lgrid.best_score_)\nprint(lgrid.best_params_)\n#print(list(zip(lgrid.cv_results_['params'],lgrid.cv_results_['mean_test_score'])))","7ef2921f":"import operator\nprint(\"Best Models (in order of accuracy) on Training Set:\")\nacc_list2 = sorted(accnew.items(), key=lambda x: x[1], reverse=True)\nfor i in acc_list2:\n    print(\n        f\"{i[0]:30} \"\n        f\"{i[1]:.3f}\"\n    )","5e705231":"accfinal={}","e631e222":"linpipe=Pipeline(steps=[\n        (\"preprocess\",transformer),\n        (\"model\",LogisticRegression())\n])\nlinpipe.set_params(**lingrid.best_params_)\nlinpipe.fit(train_x,train_y)\nprint(\"Logistic Regression\")\nscore=linpipe.score(test_x,test_y)\naccfinal.update({'Logistic Regression':score})\nscore","c8df9a55":"kpipe=Pipeline(steps=[\n        (\"preprocess\",transformer),\n        (\"model\",KNeighborsClassifier())\n])\nkpipe.set_params(**kgrid.best_params_)\nkpipe.fit(train_x,train_y)\nprint(\"K Nearest Neighbors\")\nscore=kpipe.score(test_x,test_y)\naccfinal.update({'K Nearest Neighbors':score})\nscore","b254268b":"baypipe=Pipeline(steps=[\n        (\"preprocess\",transformer),\n        (\"model\",GaussianNB())\n])\nbaypipe.set_params(**baygrid.best_params_)\nbaypipe.fit(train_x,train_y)\nprint(\"Naive Bayes\")\nscore=baypipe.score(test_x,test_y)\naccfinal.update({'Naive Bayes':score})\nscore","a7bf7a10":"svcpipe=Pipeline(steps=[\n        (\"preprocess\",transformer),\n        (\"model\",SVC())\n])\nsvcpipe.set_params(**svcgrid.best_params_)\nsvcpipe.fit(train_x,train_y)\nprint(\"SVC\")\nscore=svcpipe.score(test_x,test_y)\naccfinal.update({'SVC':score})\nscore","c5a936f5":"fpipe=Pipeline(steps=[\n        (\"preprocess\",cattransformer),\n        (\"model\",RandomForestClassifier())\n])\nfpipe.set_params(**fgrid.best_params_)\nfpipe.fit(train_x,train_y)\nprint(\"Random Forest\")\nscore=fpipe.score(test_x,test_y)\naccfinal.update({'Random Forest':score})\nscore","ffc1f302":"xpipe=Pipeline(steps=[\n        (\"preprocess\",cattransformer),\n        (\"model\",xgb.XGBClassifier())])\nxpipe.set_params(**xgrid.best_params_)\nxpipe.fit(train_x,train_y)\nprint(\"XGBoost\")\nscore=xpipe.score(test_x,test_y)\naccfinal.update({'XGBoost':score})\nscore","ea52d456":"lpipe=Pipeline(steps=[\n        (\"preprocess\",cattransformer),\n        (\"model\",lgb.LGBMClassifier())])\nlpipe.set_params(**lgrid.best_params_)\nlpipe.fit(train_x,train_y)\nprint(\"LGBM\")\nscore=lpipe.score(test_x,test_y)\naccfinal.update({'LGBM':score})\nscore","0c5cd397":"import operator\nprint(\"Best Models (in order of accuracy) on Test Set:\")\nacc_list3 = sorted(accfinal.items(), key=lambda x: x[1], reverse=True)\nfor i in acc_list3:\n    print(\n        f\"{i[0]:30} \"\n        f\"{i[1]:.3f}\"\n    )","e624cb19":"There are a lot of categorical variables that need encoding.","7d71cd88":"Here are the models that don't need standardized input:","20ef6cc8":"Does stacking our classifiers improve performance?","c7dbb6a0":"No.\n\nNow we'll tune our parameters using grid search:","33dc0a03":"Which are much, much better than the random chance of the Dummy Regressor.","b0f5c103":"Finally, we'll take our models, train them with the optimal parameters, and then evaluate the test set.","5cf42341":"Here's our final results:","1fd6b00b":"The classes are close enough to 50\/50 that we won't need to use category weighting.","ff459476":"We'll set up a pipeline that can transform categories and standardize numerical columns.\n\nWe'll also make a pipeline that only transforms categories for models that don't need standardized input.","3feaa819":"Here are the models that require standardized input:","577c0c1b":"Luckily, no missing values!","2e1b6eb1":"For the larger models with more hyperparameters, we'll use randomzied grid search to save time. I'll likely update this portion in the future to try to fine tune it better."}}