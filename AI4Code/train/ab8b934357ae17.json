{"cell_type":{"2039412a":"code","c99e2976":"code","9fad21f8":"code","852f968b":"code","911d77b6":"code","1a5967c2":"code","391d8742":"code","483e1046":"code","1775afcb":"code","ab93ac1b":"code","c716bbbc":"code","c4d02cfb":"code","e3259e43":"markdown","4fda8dea":"markdown","40c04d42":"markdown","22f7089b":"markdown","c63d59af":"markdown","ab639cdf":"markdown","3764bbcf":"markdown","71db0160":"markdown","d74bd9e0":"markdown","625dd901":"markdown","f6b0b119":"markdown"},"source":{"2039412a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c99e2976":"import numpy as np\n\nimport torch\nimport torch.nn as nn\ntorch.manual_seed(0)\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom scipy import stats\nimport pandas as pd\n\nimport wandb\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"WANDB_KEY\")\n\nwandb.login(key=secret_value_0)\nwandb.init(project='titanic_kaggle', save_code=True)","9fad21f8":"titanic_training_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntitanic_test_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntitanic_training_data.head()","852f968b":"def clean_titanic(df, train=True):\n    df[\"Cabin\"] = df[\"Cabin\"].apply(lambda x: pd.isna(x)).astype(bool)\n    df[\"Embarked\"] = df[\"Embarked\"].apply(lambda x: pd.isna(x)).astype(bool)\n    df[\"AgeNan\"] = df[\"Age\"].apply(lambda x: pd.isna(x)).astype(bool)\n    df = pd.concat([df, pd.get_dummies(df['Sex'], dtype='bool', prefix='sex_'), pd.get_dummies(df['Pclass'], dtype='bool', prefix='pclass_')], axis=1)\n    df = df.drop(['PassengerId', 'Name','Ticket','Sex','Pclass'], axis=1)\n    if train:\n        df = df.drop(['Survived'], axis=1)\n    numeric_features = df.dtypes[(df.dtypes != 'object') & (df.dtypes != 'bool')].index\n    df[numeric_features] = df[numeric_features].apply(lambda x: (x - x.mean()) \/ (x.std()))\n    df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].mean())\n    df[\"Fare\"] = df[\"Fare\"].fillna(df[\"Fare\"].mean())\n    return df\n\nlabels = torch.tensor(titanic_training_data[\"Survived\"].values, dtype=torch.float32)\ntitanic_training_data = clean_titanic(titanic_training_data)\ntitanic_training_data.head()","911d77b6":"titanic_data_tensor = torch.tensor(titanic_training_data.astype('float').values, dtype=torch.float32)\ntitanic_data_tensor.shape","1a5967c2":"dataset = torch.utils.data.TensorDataset(titanic_data_tensor, labels)","391d8742":"labels.shape","483e1046":"training_size = int(0.7 * len(dataset))\nvalidation_size = len(dataset) - training_size\ntrain, val = torch.utils.data.random_split(dataset, [training_size, validation_size], generator=torch.Generator().manual_seed(0))\ndata_loader_train = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True)\ndata_loader_val = torch.utils.data.DataLoader(val, batch_size=32, shuffle=True)","1775afcb":"#TODO Xavier Uniform to the weight and set the bias to 0\ndef init_layer(m):\n    torch.nn.init.xavier_normal_(m.weight)\n    torch.nn.init.constant_(m.bias, 0)\n    return m","ab93ac1b":"class LinearModel(nn.Module):\n    def __init__(self, inputSize):\n        super(LinearModel, self).__init__()\n        self.linear = init_layer(torch.nn.Linear(inputSize, 1))\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.linear(x)\n        return self.sigmoid(x)","c716bbbc":"num_epochs = 400 # should be more than enought, but can be changed\nlr = 3e-2 # e.q to 0.003, you can change it if needed\nmomentum = 0.9\n\nnet = LinearModel(titanic_data_tensor.shape[1])\n\ncriterion = torch.nn.BCELoss()\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)  #torch.optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n\n#logging configuration and model\nwandb.config.learningrate = lr\nwandb.config.num_iterations = num_epochs\nwandb.config.optimizer = \"Adam\"\nwandb.config.criterion = \"BCELoss\"\n#wandb.config.momentum = momentum\nwandb.watch(net, log=\"all\", criterion=criterion, log_freq=1,  log_graph=(True)) #log frequency depend on your training\n\n\nfor epoch in range(num_epochs):\n    training_loss = 0\n    #TODO TRAINING LOOP\n    for train_features, train_labels in data_loader_train:\n        l = criterion(net(train_features), train_labels.unsqueeze(1))\n        training_loss += l.item()\n        optimizer.zero_grad() # please don't forget!\n        l.backward() # remember: You need to tell wrt to what the gradient is computed\n        optimizer.step() # do a step in the gradient direction\n    \n    validation_loss = 0\n    correct_classified = 0\n    #TODO VALIDATION LOOP\n    with torch.no_grad():\n        for val_features, val_labels in data_loader_val:\n            out = net(val_features)\n            l = criterion(out, val_labels.unsqueeze(1))\n            validation_loss += l.item()\n            \n            predicted = torch.ge(out, 0.5)\n            correct_classified += int(predicted.eq(val_labels.unsqueeze(1)).sum().item())\n            \n    acc = correct_classified \/ validation_size         \n    wandb.log({'training_loss': training_loss, 'validation_loss': validation_loss, 'accuracy': acc})\n\n# SAVE THE MODEL\ntorch.save(net.state_dict(), 'my_model_' + wandb.run.name + '_' + wandb.run.id +'.pt')\nprint(\"finished...\")\n","c4d02cfb":"titanic_test_data_cleaned = clean_titanic(titanic_test_data, train=False)\ntitanic_data_tensor = torch.tensor(titanic_test_data_cleaned.astype('float').values, dtype=torch.float32)\n\nwith torch.no_grad():\n    net.eval()\n    test_pred = torch.LongTensor()\n    for i, data in enumerate(titanic_data_tensor):\n        output = net(data)\n        predicted = torch.ge(output, 0.5)\n        test_pred = torch.cat((test_pred, predicted), dim=0)\n    \n    \n    out_df = pd.DataFrame(np.c_[titanic_test_data['PassengerId'].values, test_pred.numpy()], columns=['PassengerId', 'Survived'])\n    out_df.to_csv('submission.csv', index=False)","e3259e43":"Initialize the network (call it `net`, it would make things easier later), the loss, the optimizer, and write the training loop\n\nDon't forget to check the validation loss and save your model at the end of each epoch!","4fda8dea":"Create a `TensorDataset` to get tuple of data and label","40c04d42":"Dataframe needs to be cleaned. Knowing if some information are unknown can be very important to determine if someone survived","22f7089b":"We first need to read the datasets","c63d59af":"Layer initialization using Xavier Uniform on the weights and a constant 0 value on the bias","ab639cdf":"Create the LinearModel with one Linear layer and Sigmoid applied to the output","3764bbcf":"We then split between the training and validation set","71db0160":"This loop computes the prediction on the test dataset and creates a submission file\n\nYou then just have to click the submit button to get your score. Lucky you!","d74bd9e0":"We then transform the data from numpy (pandas representation) into torch's `Tensor`","625dd901":"Import all the needed library and init Weights and Biases","f6b0b119":"# Titanic - Machine Learning from Disaster\n\n\nKaggle link: https:\/\/www.kaggle.com\/c\/titanic"}}