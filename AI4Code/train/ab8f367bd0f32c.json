{"cell_type":{"366fc1e8":"code","f8424ae3":"markdown","7f479109":"markdown","11eb90a4":"markdown","e6ae8026":"markdown","4c78ded1":"markdown"},"source":{"366fc1e8":"## lets assume below linear equation \n##  y = mx + b \n\nimport numpy as np\n\ndef gradient_decent(x,y):\n    m_curr = b_curr = 0\n    iterations = 1000\n    learning_rate = 0.05\n    n = len(x)\n    for i in range(iterations):\n        y_pred = m_curr * x + b_curr\n        cost = 1\/n * sum([val ** 2 for val in (y - y_pred)])\n        md = - 2\/n * sum (x * (y - y_pred))    ### partial m derivative\n        bd = - 2\/n * sum (y - y_pred)          ### partial b derivative\n        m_curr = m_curr - learning_rate * md\n        b_curr = b_curr - learning_rate * bd\n        print(\"m {}, b {}, cost {} iterations {}\".format(m_curr, b_curr, cost, i))\n        \n\nx = np.array([1,2,3,4,5,6])\ny = np.array([5,7,9,11,13,15])\n\ngradient_decent(x,y)\n    ","f8424ae3":"**Please appreciate by upvoting**","7f479109":"**Problem Statement** : How to calculate global minima by applying gradient descent algorithms using python for the linear equation y = mx + b \n\n![image.png](attachment:a1dbae24-36d4-4992-bf75-50bf40ca5e2a.png)","11eb90a4":"Key mathmatical formula :\n![image.png](attachment:b37c3a4c-f466-44b6-9bbd-1a1ad6233ab8.png)","e6ae8026":"**Learning Rates :** Learning rate is used to scale the magnitude of parameter updates during gradient descent. ","4c78ded1":"**Calculate gradient descent using python code**"}}