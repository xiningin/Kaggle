{"cell_type":{"52a49266":"code","e7397ce4":"code","2fd4fe78":"code","8bae9c8f":"code","383d77c3":"code","8cd85dfc":"code","1fb25ef5":"code","69c13521":"code","29c70e7a":"code","3457c16d":"code","a3e4cf5e":"code","181c77fc":"code","23f9bb7e":"code","824d3b10":"code","9341631a":"code","eab440ff":"code","92a6b1a9":"code","e5b98012":"code","98974534":"code","a8700bb5":"code","ca29b94d":"code","31177bb8":"code","5af1733f":"code","ad5521a9":"code","8e819b5e":"code","1729af48":"markdown","7c7996f5":"markdown","53ab552c":"markdown","3afd2962":"markdown","a9673157":"markdown","65ea1b02":"markdown","b2f0c735":"markdown","c824042b":"markdown","3dea9173":"markdown","2c2dc473":"markdown","31db7f32":"markdown","77f71cd2":"markdown","33697826":"markdown","394a4912":"markdown","240c9872":"markdown","8dcab46e":"markdown","309f89a9":"markdown","387ebfbb":"markdown","57e19e98":"markdown","78339178":"markdown","6c6b87e3":"markdown"},"source":{"52a49266":"##Import the required packages. These include pandas, numpy,scikit-learn and optuna\n!pip install bayesian-optimization -q\nfrom bayes_opt import BayesianOptimization\nfrom fastai.tabular.all import *\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport fastai\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e7397ce4":"train_ = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest_ = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/test.csv')","2fd4fe78":"train_.shape","8bae9c8f":"from collections import Counter\nCounter(train_['target'])","383d77c3":"bs = int(train_.shape[0]*0.8\/100)\nbs","8cd85dfc":"train_.columns","1fb25ef5":"df = train_[['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'cat10', 'cat11', 'cat12', 'cat13', 'cat14', 'cat15', 'cat16', 'cat17', 'cat18', 'cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10', 'target']]","69c13521":"def fit_with(lr:float, wd:float, dp:float, n_layers:float, layer_1:float, layer_2:float, layer_3:float):\n\n  print(lr, wd, dp)\n  if int(n_layers) == 2:\n    layers = [int(layer_1), int(layer_2)]\n  elif int(n_layers) == 3:\n    layers = [int(layer_1), int(layer_2), int(layer_3)]\n  else:\n    layers = [int(layer_1)]\n  config = tabular_config(embed_p=float(dp),\n                          ps=float(wd))\n  learn = tabular_learner(dls, layers=layers, metrics=RocAucBinary(), config = config)\n\n  with learn.no_bar() and learn.no_logging():\n    learn.fit(5, lr=float(lr))\n\n  auc = float(learn.validate()[1])\n\n  return auc","29c70e7a":"cat_names = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'cat10', 'cat11', 'cat12', 'cat13', 'cat14', 'cat15', 'cat16', 'cat17', 'cat18']\ncont_names = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10']\nprocs = [Categorify, FillMissing, Normalize]\ny_names = 'target'\ny_block = CategoryBlock()\nsplits = TrainTestSplitter(test_size=0.2, random_state=42, stratify=df['target'])(range_of(df))","3457c16d":"to = TabularPandas(df, procs=procs, cat_names=cat_names, cont_names=cont_names,\n                   y_names=y_names, y_block=y_block, splits=splits)","a3e4cf5e":"dls = to.dataloaders(bs=bs)\n","181c77fc":"hps = {'lr': (1e-05, 1e-01),\n      'wd': (4e-4, 0.4),\n      'dp': (0.01, 0.5),\n       'n_layers': (1,3),\n       'layer_1': (50, 200),\n       'layer_2': (100, 1000),\n       'layer_3': (200, 2000)}","23f9bb7e":"optim = BayesianOptimization(\n    f = fit_with, # fit function as defined above\n    pbounds = hps, # our hyper parameters \n    verbose = 3, \n    random_state=42\n)\n%time optim.maximize(n_iter=10) #Run the optimizer for 10 iterations","824d3b10":"print(optim.max)\n","9341631a":"params = {'target': 0.8899, 'params': {'dp': 0.118, 'layer_1': 172.7, 'layer_2': 198.3, 'layer_3': 1.934e+0, 'lr': 0.001991 , 'n_layers': 2.116, 'wd': 0.08398}}\nparams","eab440ff":"params = {'target': 0.8899,\n 'params': {'dp': 0.118,\n  'layer_1': 172,\n  'layer_2': 198,\n  'lr': 0.001991,\n  'n_layers': 2,\n  'wd': 0.08398}}","92a6b1a9":"config = tabular_config(embed_p=float(0.118),\n                          ps=float(0.08398))\nlayers = [172,198]\ndls = to.dataloaders(bs=bs)\nlearn = tabular_learner(dls, layers=layers, metrics=RocAucBinary(), config = config)\n","e5b98012":"learn.lr_find(stop_div=False, num_it=200)\n","98974534":"learn.fit_one_cycle(10,  0.001991)\n","a8700bb5":"test_df = test_[['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'cat10', 'cat11', 'cat12', 'cat13', 'cat14', 'cat15', 'cat16', 'cat17', 'cat18', 'cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10']]","ca29b94d":"ids = test_.id.to_list()","31177bb8":"dl = learn.dls.test_dl(test_df)\nresults = learn.get_preds(dl=dl)","5af1733f":"predictions = []\nfor i in range(200000):\n    predictions.append(int(np.argmax(results[0][i])))\n\n    \n    ","ad5521a9":"resultf = pd.DataFrame()\nresultf['id'] = ids\nresultf['target'] = predictions\nresultf.to_csv('submission.csv', index=False)","8e819b5e":"resultf","1729af48":"Get the list of ids from the test set and subset the test_ dataframe for prediction with the trained tabular learner","7c7996f5":"In this notebook I will be using bayes_opt, a general purpose bayesian optimization framework to optimize hyperparameters of a fastai tabular learner with categorical embedding layers. I will do my best to account for the evaluation metric along the way. The links to the notebooks which this is based on, including Jeremy Howard's lecture, Zachary Mueller's repo and the bayes_opt repo as well as some other resources are provided at the end.","53ab552c":"Define a fit function that the Bayesian optimization framework will optimize. The hyperparameters we are optimizing for in this manner are the number of layers, the number of neurons per each such layer, learning rate, drop out and weight decay","3afd2962":"Notice that I do the train test split stratified on the target column","a9673157":"They don't particularly agree. However, in this case I will go with what we found in the hyperparamter search. I prefer not to risk going with too large a learning rate as it could risk overstepping global minima and bouncing around in the parameter space.","65ea1b02":"I read the train set, test set to be loaded to the python runtime.\n","b2f0c735":"Create the submission data set ","c824042b":"Some useful links:\nZachary Mueller's code:\nhttps:\/\/github.com\/muellerzr\/Practical-Deep-Learning-for-Coders-2.0\/blob\/master\/Tabular%20Notebooks\/02_Bayesian_Optimization.ipynb\n\nJeremy Howard's lesson:\nhttps:\/\/course.fast.ai\/videos\/?lesson=7\n\nBayesian optimization framework used in this example:\nhttps:\/\/github.com\/fmfn\/BayesianOptimization[](http:\/\/)\n","3dea9173":"It's impossible to have bits and pieces\/ fractional neuron numbers in an MLP. So this requires some interpretation. A cleaned up set of hyperparameters on my 500 iteration search is as follows","2c2dc473":"Get the feature+target columns of the training set and assign it to a different variable","31db7f32":"Set up the optimizer and run the search. Note that we intend to maximize the auc so we seek for the maximum solution for 10 iterations","77f71cd2":"Let's train a tabular learner with these hyperparameters for a few cycles using fit one cycle using the discovered learning rate","33697826":"Specify ranges for each of the hyperparameters discussed above. This is pretty much directly taken from Zachary Mueller's notebook. Feel free to play around with this but it's a good place to start.","394a4912":"Assign lists of categorical variables and continuous variables to separate variables, specify preprocessing procs, the target and the correct type of data block for the target.","240c9872":"As a general rule of thumb, for exploratory work, I like to keep the mini batch size to around 1% of my training set size. Also ensure that you typecast the batch size to an integer, otherwise fastai will throw an error.","8dcab46e":"For the sake of brevity I only ran this for 10 iterations on the Kaggle environment. I ran this in a gpu enabled Colab pro instance for 500 iterations and the best I managed to get was as follows.","309f89a9":"Also I couldn't stress enough how important it is to pick the right metric to optimize. Since the competition guidelines specifically ask for auc and this is a binary target, RocAucBinary() is the approproate metric compatible with fastai here. Optimizing for accuracy might not lead to the best score in the confines of this tabular playground exercise.","387ebfbb":"Inspecting class imbalance. I will use stratfied sampling in train test split in a subsequent split to ensure this class imbalance is equally represented in the train and validation data sets.","57e19e98":"Let's inspect the best hyperparameters as discovered by our search","78339178":"Let's see if the learning rate finder's estimation for the learning rate matches what we found using the bayesian hyperparameter search","6c6b87e3":"Inspect the columns of the training data set "}}