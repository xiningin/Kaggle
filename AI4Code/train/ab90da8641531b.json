{"cell_type":{"05b159fd":"code","fbf601a9":"code","bce46b20":"code","ee4835a0":"code","d6096a83":"code","01ddbb13":"code","5fddfa4d":"code","c773f9e2":"code","d89c8ce2":"code","a72e981a":"code","b2b2b40e":"code","3b551887":"code","6265c2b2":"code","359a1aeb":"code","e32248d8":"code","09d5125a":"code","f9cb3e1d":"code","f15bda1d":"code","52eddc3f":"code","c534efac":"code","cbc09c6f":"code","503110ef":"code","90b3187e":"code","40c6351b":"code","02241ab2":"code","bb757c23":"code","6dd3e175":"code","bd10dbd3":"code","1ae6c018":"markdown","840b9e72":"markdown","dee13ead":"markdown","a3f8338c":"markdown","d6d32238":"markdown","f52517d7":"markdown","b388e3bf":"markdown","363846bd":"markdown","c497ed9c":"markdown","8e12c9dd":"markdown","e895423b":"markdown","7e4277e7":"markdown","edabf076":"markdown","badcd0b2":"markdown","c440f1f2":"markdown","fed6639e":"markdown"},"source":{"05b159fd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport gc\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","fbf601a9":"DATA_ROOT = '..\/input\/'\nGAP_DATA_FOLDER = os.path.join(DATA_ROOT, 'gap-coreference')\nSUB_DATA_FOLDER = os.path.join(DATA_ROOT, 'gendered-pronoun-resolution')\nFAST_TEXT_DATA_FOLDER = os.path.join(DATA_ROOT, 'fasttext-crawl-300d-2m')","bce46b20":"test_df_path = os.path.join(GAP_DATA_FOLDER, 'gap-development.tsv')\ntrain_df_path = os.path.join(GAP_DATA_FOLDER, 'gap-test.tsv')\ndev_df_path = os.path.join(GAP_DATA_FOLDER, 'gap-validation.tsv')\n\ntrain_df = pd.read_csv(train_df_path, sep='\\t')\ntest_df = pd.read_csv(test_df_path, sep='\\t')\ndev_df = pd.read_csv(dev_df_path, sep='\\t')\n\n# pd.options.display.max_colwidth = 1000","ee4835a0":"test_df.head()","d6096a83":"spacy_model = \"en_core_web_lg\"","01ddbb13":"from spacy.lang.en import English\nfrom spacy.pipeline import DependencyParser\nimport spacy\nfrom nltk import Tree\n\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing import text as ktext","5fddfa4d":"nlp = spacy.load(spacy_model)\n\ndef bs(list_, target_):\n    lo, hi = 0, len(list_) -1\n    \n    while lo < hi:\n        mid = lo + int((hi - lo) \/ 2)\n        \n        if target_ < list_[mid]:\n            hi = mid\n        elif target_ > list_[mid]:\n            lo = mid + 1\n        else:\n            return mid + 1\n    return lo\n\ndef bs_(list_, target_):\n    lo, hi = 0, len(list_) -1\n    \n    while lo < hi:\n        mid = lo + int((hi - lo) \/ 2)\n        \n        if target_ < list_[mid]:\n            hi = mid\n        elif target_ > list_[mid]:\n            lo = mid + 1\n        else:\n            return mid\n    return lo\n\ndef ohe_dist(dist, buckets):\n    idx = bs_(buckets, dist)\n    oh = np.zeros(shape=(len(buckets),), dtype=np.float32)\n    oh[idx] = 1\n    \n    return oh","c773f9e2":"num_pos_features = 45","d89c8ce2":"def extrac_positional_features(text, char_offset1, char_offset2):\n    doc = nlp(text)\n    max_len = 64\n    \n    # char offset to token offset\n    lens = [token.idx for token in doc]\n    mention_offset1 = bs(lens, char_offset1) - 1\n    mention_offset2 = bs(lens, char_offset2) - 1\n    \n    # token offset to sentence offset\n    lens = [len(sent) for sent in doc.sents]\n    acc_lens = [len_ for len_ in lens]\n    pre_len = 0\n    for i in range(0, len(acc_lens)):\n        pre_len += acc_lens[i]\n        acc_lens[i] = pre_len\n    sent_index1 = bs(acc_lens, mention_offset1)\n    sent_index2 = bs(acc_lens, mention_offset2)\n    \n    sent1 = list(doc.sents)[sent_index1]\n    sent2 = list(doc.sents)[sent_index2]\n    \n    # buckets\n    bucket_dist = [1, 2, 3, 4, 5, 8, 16, 32, 64]\n    \n    # relative distance\n    dist = mention_offset2 - mention_offset1\n    dist_oh = ohe_dist(dist, bucket_dist)\n    \n    # buckets\n    bucket_pos = [0, 1, 2, 3, 4, 5, 8, 16, 32]\n    \n    # absolute position in the sentence\n    sent_pos1 = mention_offset1 + 1\n    if sent_index1 > 0:\n        sent_pos1 = mention_offset1 - acc_lens[sent_index1-1]\n    sent_pos_oh1 = ohe_dist(sent_pos1, bucket_pos)\n    sent_pos_inv1 = len(sent1) - sent_pos1\n    assert sent_pos_inv1 >= 0\n    sent_pos_inv_oh1 = ohe_dist(sent_pos_inv1, bucket_pos)\n    \n    sent_pos2 = mention_offset2 + 1\n    if sent_index2 > 0:\n        sent_pos2 = mention_offset2 - acc_lens[sent_index2-1]\n    sent_pos_oh2 = ohe_dist(sent_pos2, bucket_pos)\n    sent_pos_inv2 = len(sent2) - sent_pos2\n    if sent_pos_inv2 < 0:\n        print(sent_pos_inv2)\n        print(len(sent2))\n        print(sent_pos2)\n        raise ValueError\n    sent_pos_inv_oh2 = ohe_dist(sent_pos_inv2, bucket_pos)\n    \n    sent_pos_ratio1 = sent_pos1 \/ len(sent1)\n    sent_pos_ratio2 = sent_pos2 \/ len(sent2)\n    \n    return dist_oh, sent_pos_oh1, sent_pos_oh2, sent_pos_inv_oh1, sent_pos_inv_oh2","a72e981a":"def create_dist_features(df, text_column, pronoun_offset_column, name_offset_column):\n    text_offset_list = df[[text_column, pronoun_offset_column, name_offset_column]].values.tolist()\n    num_features = num_pos_features\n    \n    pos_feature_matrix = np.zeros(shape=(len(text_offset_list), num_features))\n    for text_offset_index in range(len(text_offset_list)):\n        text_offset = text_offset_list[text_offset_index]\n        dist_oh, sent_pos_oh1, sent_pos_oh2, sent_pos_inv_oh1, sent_pos_inv_oh2 = extrac_positional_features(text_offset[0], text_offset[1], text_offset[2])\n        \n        feature_index = 0\n        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(dist_oh)] = np.asarray(dist_oh)\n        feature_index += len(dist_oh)\n        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_oh1)] = np.asarray(sent_pos_oh1)\n        feature_index += len(sent_pos_oh1)\n        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_oh2)] = np.asarray(sent_pos_oh2)\n        feature_index += len(sent_pos_oh2)\n        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_inv_oh1)] = np.asarray(sent_pos_inv_oh1)\n        feature_index += len(sent_pos_inv_oh1)\n        pos_feature_matrix[text_offset_index, feature_index:feature_index+len(sent_pos_inv_oh2)] = np.asarray(sent_pos_inv_oh2)\n        feature_index += len(sent_pos_inv_oh2)\n    \n    return pos_feature_matrix","b2b2b40e":"max_len = 50 # longer than 99% of the sentences","3b551887":"seq_list = list()\ndef extract_sents(text, char_offset_p, char_offset_a, char_offset_b, id):\n    global max_len\n    global seq_list\n    \n    seq_list.append(list())\n    \n    doc = nlp(text)\n    token_lens = [token.idx for token in doc]\n    \n    char_offsets = [char_offset_p, char_offset_a, char_offset_b]\n    sent_list = list()\n    \n    for char_offset in char_offsets:\n        # char offset to token offset\n        mention_offset = bs(token_lens, char_offset) - 1\n        # mention_word\n        mention = doc[mention_offset]\n    \n        # token offset to sentence offset\n        lens = [len(sent) for sent in doc.sents]\n        acc_lens = [len_ for len_ in lens]\n        pre_len = 0\n        for i in range(0, len(acc_lens)):\n            pre_len += acc_lens[i]\n            acc_lens[i] = pre_len\n        sent_index = bs(acc_lens, mention_offset)\n        # mention sentence\n        sent = list(doc.sents)[sent_index]\n        \n        # absolute position in the sentence\n        sent_pos = mention_offset + 1\n        if sent_index > 0:\n            sent_pos = mention_offset - acc_lens[sent_index-1]\n        \n        # clip the sentence if it is longer than max length\n        if len(sent) > max_len:\n            # make sure the mention is in the sentence span\n            if sent_pos < max_len-1:\n                sent_list.append(sent[0:max_len].text)\n                sent_list.append(sent_pos)\n                seq_list[-1].append(sent[0:max_len])\n            else:\n                sent_list.append(sent[sent_pos-max_len+2 : min(sent_pos+2, len(sent))].text)\n                sent_list.append(max_len-2)\n                seq_list[-1].append(sent[sent_pos-max_len+2 : min(sent_pos+2, len(sent))])\n        else:\n            sent_list.append(sent.text)\n            sent_list.append(sent_pos)\n            seq_list[-1].append(sent)\n        \n    return pd.Series([id] + sent_list, index=['ID', 'Pronoun-Sent', 'Pronoun-Sent-Offset', 'A-Sent', 'A-Sent-Offset', 'B-Sent', 'B-Sent-Offset'])\n\ndef add_sent_columns(df, text_column, pronoun_offset_column, a_offset_column, b_offset_column):\n    global seq_list\n    seq_list = list()\n    sent_df = df.apply(lambda row: extract_sents(row.loc[text_column], row[pronoun_offset_column], row[a_offset_column], row[b_offset_column], row['ID']), axis=1)\n    df = df.join(sent_df.set_index('ID'), on='ID')\n    return df, seq_list","6265c2b2":"seq_list = list()\ntrain_df, train_tokenized = add_sent_columns(train_df, 'Text', 'Pronoun-offset', 'A-offset', 'B-offset')\nseq_list = list()\ntest_df, test_tokenized = add_sent_columns(test_df, 'Text', 'Pronoun-offset', 'A-offset', 'B-offset')\nseq_list = list()\ndev_df, dev_tokenized = add_sent_columns(dev_df, 'Text', 'Pronoun-offset', 'A-offset', 'B-offset')\n\n# df apply will call the first row twice, remove the first one\ntrain_tokenized = train_tokenized[1:]\ntest_tokenized = test_tokenized[1:]\ndev_tokenized = dev_tokenized[1:]","359a1aeb":"embed_size = 300\nmax_features = 80000\n\n# generate word index\nword_index = dict()\nidx = 1\nfor text_ in train_tokenized+test_tokenized+dev_tokenized:\n    for sent_ in text_:\n        for word_ in sent_:\n            if word_.text not in word_index and nlp.vocab.has_vector(word_.text):\n                word_index[word_.text] = idx\n                idx += 1\n\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words + 1, embed_size))\n        \nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = None\n    if nlp.vocab.has_vector(word):\n        embedding_vector = nlp.vocab.vectors[nlp.vocab.strings[word]]\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n        \nprint(embedding_matrix.shape)\n\n# generate pos tag index\npos_index = dict()\nidx = 1\nfor text_ in train_tokenized+test_tokenized+dev_tokenized:\n    for sent_ in text_:\n        for word_ in sent_:\n            if word_.pos not in pos_index:\n                pos_index[word_.pos] = idx\n                idx += 1\n\ndef sentences_to_sequences(tokenized_):\n    return list(map(\n        lambda sent_tokenized: list(map(\n            lambda token_: word_index[token_.text] if token_.text in word_index else 0,\n            sent_tokenized\n        )),\n        tokenized_\n    ))\n\ndef poses_to_sequences(tokenized_):\n    return list(map(\n        lambda sent_tokenized: list(map(\n            lambda token_: pos_index[token_.pos] if token_.pos in pos_index else 0,\n            sent_tokenized\n        )),\n        tokenized_\n    ))\n    ","e32248d8":"import seaborn as sns\n\nsns.distplot(train_df['Pronoun-Sent'].map(lambda ele: len(ele.split(\" \"))), kde_kws={\"label\": \"P\"})\n\nsns.distplot(train_df['A-Sent'].map(lambda ele: len(ele.split(\" \"))), kde_kws={\"label\": \"A\"})\n\nsns.distplot(train_df['B-Sent'].map(lambda ele: len(ele.split(\" \"))), kde_kws={\"label\": \"B\"})","09d5125a":"train_p_tokenized = sentences_to_sequences([row[0] for row in train_tokenized])\ntrain_a_tokenized = sentences_to_sequences([row[1] for row in train_tokenized])\ntrain_b_tokenized = sentences_to_sequences([row[2] for row in train_tokenized])\n\ntest_p_tokenized = sentences_to_sequences([row[0] for row in test_tokenized])\ntest_a_tokenized = sentences_to_sequences([row[1] for row in test_tokenized])\ntest_b_tokenized = sentences_to_sequences([row[2] for row in test_tokenized])\n\ndev_p_tokenized = sentences_to_sequences([row[0] for row in dev_tokenized])\ndev_a_tokenized = sentences_to_sequences([row[1] for row in dev_tokenized])\ndev_b_tokenized = sentences_to_sequences([row[2] for row in dev_tokenized])\n\nseq_p_train = sequence.pad_sequences(train_p_tokenized, maxlen = max_len, padding='post')\nseq_a_train = sequence.pad_sequences(train_a_tokenized, maxlen = max_len, padding='post')\nseq_b_train = sequence.pad_sequences(train_b_tokenized, maxlen = max_len, padding='post')\n\nseq_p_test = sequence.pad_sequences(test_p_tokenized, maxlen = max_len, padding='post')\nseq_a_test = sequence.pad_sequences(test_a_tokenized, maxlen = max_len, padding='post')\nseq_b_test = sequence.pad_sequences(test_b_tokenized, maxlen = max_len, padding='post')\n\nseq_p_dev = sequence.pad_sequences(dev_p_tokenized, maxlen = max_len, padding='post')\nseq_a_dev = sequence.pad_sequences(dev_a_tokenized, maxlen = max_len, padding='post')\nseq_b_dev = sequence.pad_sequences(dev_b_tokenized, maxlen = max_len, padding='post')\n\ntrain_p_pos = poses_to_sequences([row[0] for row in train_tokenized])\ntrain_a_pos = poses_to_sequences([row[1] for row in train_tokenized])\ntrain_b_pos = poses_to_sequences([row[2] for row in train_tokenized])\n\ntest_p_pos = poses_to_sequences([row[0] for row in test_tokenized])\ntest_a_pos = poses_to_sequences([row[1] for row in test_tokenized])\ntest_b_pos = poses_to_sequences([row[2] for row in test_tokenized])\n\ndev_p_pos = poses_to_sequences([row[0] for row in dev_tokenized])\ndev_a_pos = poses_to_sequences([row[1] for row in dev_tokenized])\ndev_b_pos = poses_to_sequences([row[2] for row in dev_tokenized])\n\npos_p_train = sequence.pad_sequences(train_p_pos, maxlen = max_len, padding='post')\npos_a_train = sequence.pad_sequences(train_a_pos, maxlen = max_len, padding='post')\npos_b_train = sequence.pad_sequences(train_b_pos, maxlen = max_len, padding='post')\n\npos_p_test = sequence.pad_sequences(test_p_pos, maxlen = max_len, padding='post')\npos_a_test = sequence.pad_sequences(test_a_pos, maxlen = max_len, padding='post')\npos_b_test = sequence.pad_sequences(test_b_pos, maxlen = max_len, padding='post')\n\npos_p_dev = sequence.pad_sequences(dev_p_pos, maxlen = max_len, padding='post')\npos_a_dev = sequence.pad_sequences(dev_a_pos, maxlen = max_len, padding='post')\npos_b_dev = sequence.pad_sequences(dev_b_pos, maxlen = max_len, padding='post')\n\nindex_p_train = train_df['Pronoun-Sent-Offset'].values\nindex_a_train = train_df['A-Sent-Offset'].values\nindex_b_train = train_df['B-Sent-Offset'].values\n\nindex_p_test = test_df['Pronoun-Sent-Offset'].values\nindex_a_test = test_df['A-Sent-Offset'].values\nindex_b_test = test_df['B-Sent-Offset'].values\n\nindex_p_dev = dev_df['Pronoun-Sent-Offset'].values\nindex_a_dev = dev_df['A-Sent-Offset'].values\nindex_b_dev = dev_df['B-Sent-Offset'].values\n\npa_pos_tra = create_dist_features(train_df, 'Text', 'Pronoun-offset', 'A-offset')\npa_pos_dev = create_dist_features(dev_df, 'Text', 'Pronoun-offset', 'A-offset')\npa_pos_test = create_dist_features(test_df, 'Text', 'Pronoun-offset', 'A-offset')\n\npb_pos_tra = create_dist_features(train_df, 'Text', 'Pronoun-offset', 'B-offset')\npb_pos_dev = create_dist_features(dev_df, 'Text', 'Pronoun-offset', 'B-offset')\npb_pos_test = create_dist_features(test_df, 'Text', 'Pronoun-offset', 'B-offset')","f9cb3e1d":"X_train = [seq_p_train, seq_a_train, seq_b_train, pos_p_train, pos_a_train, pos_b_train, index_p_train, index_a_train, index_b_train, pa_pos_tra, pb_pos_tra]\nX_dev = [seq_p_dev, seq_a_dev, seq_b_dev, pos_p_dev, pos_a_dev, pos_b_dev, index_p_dev, index_a_dev, index_b_dev, pa_pos_dev, pb_pos_dev]\nX_test = [seq_p_test, seq_a_test, seq_b_test, pos_p_test, pos_a_test, pos_b_test, index_p_test, index_a_test, index_b_test, pa_pos_test, pb_pos_test]","f15bda1d":"def _row_to_y(row):\n    if row.loc['A-coref']:\n        return 0\n    if row.loc['B-coref']:\n        return 1\n    return 2\n\ny_tra = train_df.apply(_row_to_y, axis=1)\ny_dev = dev_df.apply(_row_to_y, axis=1)\ny_test = test_df.apply(_row_to_y, axis=1)","52eddc3f":"import numpy as np\nfrom keras import backend\nfrom keras import layers\nfrom keras import models\n\nfrom keras import initializers, regularizers, constraints, activations\nfrom keras.engine import Layer\nimport keras.backend as K\nfrom keras.layers import merge\n\nimport tensorflow as tf","c534efac":"def _dot_product(x, kernel):\n    \"\"\"\n    Wrapper for dot product operation, in order to be compatible with both\n    Theano and Tensorflow\n    Args:\n        x (): input\n        kernel (): weights\n    Returns:\n    \"\"\"\n    if K.backend() == 'tensorflow':\n        # todo: check that this is correct\n        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n    else:\n        return K.dot(x, kernel)\n    \n    \nclass AttentionWeight(Layer):\n    \"\"\"\n        This code is a modified version of cbaziotis implementation:  GithubGist cbaziotis\/AttentionWithContext.py\n        Attention operation, with a context\/query vector, for temporal data.\n        Supports Masking.\n        Follows the work of Yang et al. [https:\/\/www.cs.cmu.edu\/~diyiy\/docs\/naacl16.pdf]\n        \"Hierarchical Attention Networks for Document Classification\"\n        by using a context vector to assist the attention\n        # Input shape\n            3D tensor with shape: `(samples, steps, features)`.\n        # Output shape\n            2D tensor with shape: `(samples, steps)`.\n        :param kwargs:\n        Just put it on top of an RNN Layer (GRU\/LSTM\/SimpleRNN) with return_sequences=True.\n        The dimensions are inferred based on the output shape of the RNN.\n        Example:\n            model.add(LSTM(64, return_sequences=True))\n            model.add(AttentionWeight())\n        \"\"\"\n\n    def __init__(self,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        super(AttentionWeight, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        shape1 = input_shape[0]\n        shape2 = input_shape[1]\n\n        self.W = self.add_weight((shape2[-1], shape1[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        if self.bias:\n            self.b = self.add_weight((shape2[-1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, inputs, mask=None):\n        x = inputs[0]\n        u = inputs[1]\n        \n        uit = _dot_product(x, self.W)\n\n        if self.bias:\n            uit += self.b\n\n        uit = K.tanh(uit)\n        ait = K.batch_dot(uit, u)\n\n        a = K.exp(ait)\n\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a *= K.cast(mask, K.floatx())\n\n        # in some cases especially in the early stages of training the sum may be almost zero\n        # and this results in NaN's. A workaround is to add a very small positive number \u03b5 to the sum.\n        # a \/= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n        \n        return a\n\n    def compute_output_shape(self, input_shape):\n        if not isinstance(input_shape, list) or len(input_shape) != 2:\n            raise ValueError('A `Dot` layer should be called '\n                             'on a list of 2 inputs.')\n        shape1 = list(input_shape[0])\n        shape2 = list(input_shape[1])\n        \n        return shape1[0], shape1[1]\n\n    def get_config(self):\n        config = {\n            'W_regularizer': regularizers.serialize(self.W_regularizer),\n            'b_regularizer': regularizers.serialize(self.b_regularizer),\n            'W_constraint': constraints.serialize(self.W_constraint),\n            'b_constraint': constraints.serialize(self.b_constraint),\n            'bias': self.bias\n        }\n        base_config = super(AttentionWeight, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n    \n    \nclass FeatureSelection1D(Layer):\n    \"\"\"\n        Normalize feature along a specific axis.\n        Supports Masking.\n\n        # Input shape\n            A ND tensor with shape: `(samples, timesteps, features)\n            A 2D tensor with shape: [samples, num_selected_features]\n        # Output shape\n            ND tensor with shape: `(samples, num_selected_features, features)`.\n        :param kwargs:\n        \"\"\"\n\n    def __init__(self, num_selects, **kwargs):\n\n        self.num_selects = num_selects\n        self.supports_masking = True\n        super(FeatureSelection1D, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n\n        super(FeatureSelection1D, self).build(input_shape)\n\n    def compute_mask(self, input, input_mask=None):\n        # don't pass the mask to the next layers\n        return None\n\n    def call(self, inputs, mask=None):\n        if not isinstance(inputs, list) or len(inputs) != 2:\n            raise ValueError('FeatureSelection1D layer should be called '\n                             'on a list of 2 inputs.')\n\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a = K.cast(mask, K.floatx()) * inputs[0]\n        else:\n            a = inputs[0]\n\n        b = inputs[1]\n\n        a = tf.batch_gather(\n            a, b\n        )\n\n        return a\n\n    def compute_output_shape(self, input_shape):\n        if not isinstance(input_shape, list) or len(input_shape) != 2:\n            raise ValueError('A `FeatureSelection1D` layer should be called '\n                             'on a list of 2 inputs.')\n        shape1 = list(input_shape[0])\n        shape2 = list(input_shape[1])\n\n        if shape2[0] != shape1[0]:\n            raise ValueError(\"batch size must be same\")\n\n        if shape2[1] != self.num_selects:\n            raise ValueError(\"must conform to the num_select\")\n\n        return (shape1[0], self.num_selects, shape1[2])\n\n    def get_config(self):\n        config = {\n            'num_selects': self.num_selects\n        }\n        base_config = super(FeatureSelection1D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","cbc09c6f":"from keras import callbacks as kc\nfrom keras import optimizers as ko\nfrom keras import initializers, regularizers, constraints\n\nimport matplotlib.pyplot as plt\nfrom IPython.display import SVG\n\nhistories = list()\nfile_paths = list()\ncos = list()","503110ef":"def build_e2e_birnn_attention_model(\n        voca_dim, time_steps, pos_tag_size, pos_tag_dim, extra_feature_dims, output_dim, rnn_dim, model_dim, mlp_dim,\n        item_embedding=None, rnn_depth=1, mlp_depth=1,\n        drop_out=0.5, rnn_drop_out=0., rnn_state_drop_out=0.,\n        trainable_embedding=False, gpu=False, return_customized_layers=False):\n    \"\"\"\n    Create A End-to-End Bidirectional RNN Attention Model.\n\n    :param voca_dim: vocabulary dimension size.\n    :param time_steps: the length of input\n    :param extra_feature_dims: the dimention size of the auxilary feature\n    :param output_dim: the output dimension size\n    :param model_dim: rrn dimension size\n    :param mlp_dim: the dimension size of fully connected layer\n    :param item_embedding: integer, numpy 2D array, or None (default=None)\n        If item_embedding is a integer, connect a randomly initialized embedding matrix to the input tensor.\n        If item_embedding is a matrix, this matrix will be used as the embedding matrix.\n        If item_embedding is None, then connect input tensor to RNN layer directly.\n    :param rnn_depth: rnn depth\n    :param mlp_depth: the depth of fully connected layers\n    :param drop_out: dropout rate of fully connected layers\n    :param rnn_drop_out: dropout rate of rnn layers\n    :param rnn_state_drop_out: dropout rate of rnn state tensor\n    :param trainable_embedding: boolean\n    :param gpu: boolean, default=False\n        If True, CuDNNLSTM is used instead of LSTM for RNN layer.\n    :param return_customized_layers: boolean, default=False\n        If True, return model and customized object dictionary, otherwise return model only\n    :return: keras model\n    \"\"\"\n    \n    # sequences inputs\n    if item_embedding is not None:\n        inputp = models.Input(shape=(time_steps,), dtype='int32', name='inputp')\n        inputa = models.Input(shape=(time_steps,), dtype='int32', name='inputa')\n        inputb = models.Input(shape=(time_steps,), dtype='int32', name='inputb')\n        inputs = [inputp, inputa, inputb]\n        \n        if isinstance(item_embedding, np.ndarray):\n            assert voca_dim == item_embedding.shape[0]\n            embed_dim = item_embedding.shape[1]\n            emb_layer = layers.Embedding(\n                voca_dim, item_embedding.shape[1], input_length=time_steps,\n                weights=[item_embedding, ], trainable=trainable_embedding,\n                mask_zero=False, name='embedding_layer0'\n            )\n        elif utils.is_integer(item_embedding):\n            embed_dim = item_embedding\n            emb_layer = layers.Embedding(\n                voca_dim, item_embedding, input_length=time_steps,\n                trainable=trainable_embedding,\n                mask_zero=False, name='embedding_layer0'\n            )\n        else:\n            raise ValueError(\"item_embedding must be either integer or numpy matrix\")\n\n        xs = list(map(\n            lambda input_: emb_layer(input_),\n            inputs\n        ))\n    else:\n        inputp = models.Input(shape=(time_steps, voca_dim), dtype='float32', name='inputp')\n        inputa = models.Input(shape=(time_steps, voca_dim), dtype='float32', name='inputa')\n        inputb = models.Input(shape=(time_steps, voca_dim), dtype='float32', name='inputb')\n        embed_dim = voca_dim\n        xs = [inputp, inputa, inputb]\n        \n    # pos tag\n    inputposp = models.Input(shape=(time_steps,), dtype='int32', name='inputposp')\n    inputposa = models.Input(shape=(time_steps,), dtype='int32', name='inputposa')\n    inputposb = models.Input(shape=(time_steps,), dtype='int32', name='inputposb')\n    inputpos = [inputposp, inputposa, inputposb]\n    pos_emb_layer = layers.Embedding(\n        pos_tag_size, pos_tag_dim, input_length=time_steps,\n        trainable=True, mask_zero=False, name='pos_embedding_layer0'\n    )\n    xpos = list(map(\n        lambda input_: pos_emb_layer(input_),\n        inputpos\n    ))\n    \n    embed_concate_layer = layers.Concatenate(axis=2, name=\"embed_concate_layer\")\n    for i in range(len(xs)):\n        xs[i] = embed_concate_layer([xs[i], xpos[i]])\n    \n    # mention position in the sentence\n    inputpi = models.Input(shape=(1,), dtype='int32', name='inputpi')\n    inputai = models.Input(shape=(1,), dtype='int32', name='inputai')\n    inputbi = models.Input(shape=(1,), dtype='int32', name='inputbi')\n    xis = [inputpi, inputai, inputbi]\n    \n    # addtional mention-pair features\n    inputpa = models.Input(shape=(extra_feature_dims,), dtype='float32', name='inputpa')\n    inputpb = models.Input(shape=(extra_feature_dims,), dtype='float32', name='inputpb')\n    xextrs = [inputpa, inputpb]\n    \n    # rnn\n    birnns = list()\n    rnn_batchnorms = list()\n    rnn_dropouts = list()\n    if gpu:\n        # rnn encoding\n        for i in range(rnn_depth):\n            rnn_dropout = layers.SpatialDropout1D(rnn_drop_out)\n            birnn = layers.Bidirectional(\n                layers.CuDNNGRU(rnn_dim, return_sequences=True),\n                name='bi_lstm_layer' + str(i))\n            rnn_batchnorm = layers.BatchNormalization(name='rnn_batch_norm_layer' + str(i))\n            \n            birnns.append(birnn)\n            rnn_dropouts.append(rnn_dropout)\n            rnn_batchnorms.append(rnn_batchnorm)\n        \n        xs_ = list()\n        for x_ in xs:\n            for i in range(len(birnns)):\n                x_ = rnn_dropouts[i](x_)\n                x_ = birnns[i](x_)\n                x_ = rnn_batchnorms[i](x_)\n            xs_.append(x_)\n        xs = xs_\n    else:\n        # rnn encoding\n        for i in range(rnn_depth):\n            birnn = layers.Bidirectional(\n                layers.GRU(rnn_dim, return_sequences=True, dropout=rnn_drop_out,\n                            recurrent_dropout=rnn_state_drop_out),\n                name='bi_lstm_layer' + str(i))\n            rnn_batchnorm = layers.BatchNormalization(name='rnn_batch_norm_layer' + str(i))\n            \n            birnns.append(birnn)\n            rnn_batchnorms.append(rnn_batchnorm)\n            \n        xs_ = list()\n        for x_ in xs:\n            for i in range(len(birnns)):\n                x_ = birnns[i](x_)\n                x_ = rnn_batchnorms[i](x_)\n            xs_.append(x_)\n        xs = xs_\n    \n    # attention aggregated rnn embedding + mention rnn embedding + mention-pair features\n    select_layer = FeatureSelection1D(1, name='boundary_selection_layer')\n    flatten_layer1 = layers.Flatten('channels_first', name=\"flatten_layer1\")\n    permute_layer = layers.Permute((2, 1), name='permuted_attention_x')\n    attent_weight = AttentionWeight(name=\"attention_weight\")\n    focus_layer = layers.Dot([2, 1], name='focus' + '_layer')\n    reshape_layer = layers.Reshape((1, rnn_dim*2), name=\"reshape_layer\")\n    concate_layer = layers.Concatenate(axis=1, name=\"attention_concate_layer\")\n    atten_dropout_layer = layers.Dropout(drop_out, name='attention_dropout_layer')\n    map_layer1 = layers.Dense(model_dim, activation=\"relu\", name=\"map_layer1\")\n    #map_layer2 = layers.TimeDistributed(layers.Dense(model_dim, activation=\"relu\"), name=\"map_layer2\")\n    map_layer2 = map_layer1\n    flatten_layer = layers.Flatten('channels_first', name=\"flatten_layer\")\n    for i in range(len(xs)):\n        if i == 0:\n            map_layer = map_layer1\n        else:\n            map_layer = map_layer2\n            \n        select_ = select_layer([xs[i], xis[i]])\n        flatten_select_ = flatten_layer1(select_)\n        att = attent_weight([xs[i], flatten_select_])\n        \n        focus = focus_layer([permute_layer(xs[i]), att])\n        xs[i] = concate_layer([select_, reshape_layer(focus)])\n        xs[i] = flatten_layer(xs[i])\n        xs[i] = atten_dropout_layer(xs[i])\n        xs[i] = map_layer(xs[i])\n    \n    feature_dropout_layer = layers.Dropout(rate=drop_out, name=\"feature_dropout_layer\")\n    feature_map_layer = layers.Dense(model_dim, activation=\"relu\",name=\"feature_map_layer\")\n    xextrs = [feature_map_layer(feature_dropout_layer(xextr)) for xextr in xextrs]\n    \n    x = layers.Concatenate(axis=1, name=\"concat_feature_layer\")(xs + xextrs)\n    x = layers.Dropout(drop_out, name='dropout_layer')(x)\n\n    # MLP Layers\n    for i in range(mlp_depth - 1):\n        x = layers.Dense(mlp_dim, activation='selu', kernel_initializer='lecun_normal', name='selu_layer' + str(i))(x)\n        x = layers.AlphaDropout(drop_out, name='alpha_layer' + str(i))(x)\n\n    outputs = layers.Dense(output_dim, activation=\"softmax\", name=\"softmax_layer0\")(x)\n\n    model = models.Model([inputp, inputa, inputb] + inputpos + xis + [inputpa, inputpb], outputs)\n\n    if return_customized_layers:\n        return model, {'FeatureSelection1D': FeatureSelection1D, 'AttentionWeight': AttentionWeight}\n\n    return model","90b3187e":"voca_dim = embedding_matrix.shape[0]\npos_tag_size = len(pos_index)\ntime_steps = max_len\n\nembed_dim = embedding_matrix.shape[1]\npos_tag_dim = 5\nextra_feature_dims = num_pos_features\noutput_dim = 3\nrnn_dim = 50\nmodel_dim = 10\nmlp_dim = 10\nrnn_depth = 1\nmlp_depth=1\ndrop_out=0.2\nrnn_drop_out=0.5\ngpu = False\nreturn_customized_layers=True\n\nmodel, co = build_e2e_birnn_attention_model(\n        voca_dim, time_steps, pos_tag_size, pos_tag_dim, extra_feature_dims, output_dim, rnn_dim, model_dim, mlp_dim,\n        item_embedding=embedding_matrix, rnn_depth=rnn_depth, mlp_depth=mlp_depth,\n        drop_out=drop_out, rnn_drop_out=rnn_drop_out, rnn_state_drop_out=rnn_drop_out,\n        trainable_embedding=False, gpu=gpu, return_customized_layers=return_customized_layers)\ncos.append(co)","40c6351b":"print(model.summary())","02241ab2":"adam = ko.Nadam(clipnorm=1.0)\nmodel.compile(adam, loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"])\n\nfile_path = \"best_e2e_rnn_atten_model.hdf5\"\ncheck_point = kc.ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1, save_best_only = True, mode = \"min\")\nearly_stop = kc.EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience=5)\nhistory = model.fit(X_train, y_tra, batch_size=30, epochs=40, validation_data=(X_dev, y_dev), callbacks = [check_point, early_stop])\n\nfile_paths.append(file_path)\nhistories.append(np.min(np.asarray(history.history['val_loss'])))\n\ndel model, history\ngc.collect()","bb757c23":"print(\"load best model: \" + str(file_paths[np.argmin(histories)]))\nmodel = models.load_model(\n    file_paths[np.argmin(histories)], cos[np.argmin(histories)])","6dd3e175":"y_preds = model.predict(X_test, batch_size = 1024, verbose = 1)\n\nsub_df_path = os.path.join(SUB_DATA_FOLDER, 'sample_submission_stage_1.csv')\nsub_df = pd.read_csv(sub_df_path)\nsub_df.loc[:, 'A'] = pd.Series(y_preds[:, 0])\nsub_df.loc[:, 'B'] = pd.Series(y_preds[:, 1])\nsub_df.loc[:, 'NEITHER'] = pd.Series(y_preds[:, 2])\n\nsub_df.head()","bd10dbd3":"sub_df.to_csv(\"submission.csv\", index=False)","1ae6c018":"# Explore Features for Building Mention-Pair Distributed Representation","840b9e72":"# Define Keras Layers","dee13ead":" ##  Position Features","a3f8338c":"# Build and Train Model","d6d32238":"### Create Vocab and Embedding Matrix","f52517d7":"### Build Model","b388e3bf":"## End-to-End RNN Attention Model","363846bd":"### Define Model","c497ed9c":"This Kernel implements a modified version of **a state-of-art end-to-end neural correference resolution model** published in 2017: https:\/\/www.aclweb.org\/anthology\/D17-1018.\nThis completition only focus on a specific case of  the generic reference resolution problem, and we only need pick out the correct mention from two candidates, which simplifies the model implementation.\n\nYou can compare the result of this model  with the result by other non-RNN based DL models implemented in another kernel: https:\/\/www.kaggle.com\/keyit92\/coreference-resolution-by-mlp-cnn-coattention-nn. \n","8e12c9dd":"## Create Train, Dev and Test Data","e895423b":"# Import Data","7e4277e7":"### Train Model","edabf076":"Select the surrounding 100 words around the mention in the sentence.","badcd0b2":"## Extract Sentences","c440f1f2":"Encode the absolute positions in the sentence and the relative position between the pronoun and the entities.","fed6639e":"###  Make Prediction"}}