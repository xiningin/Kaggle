{"cell_type":{"c6f7ebd0":"code","88e1ae1c":"code","5a6b84e9":"code","0f127d94":"code","0b93fc53":"code","d20e92c4":"code","0ff1ce3b":"code","068e3610":"code","95bf0f5f":"code","69d6bebd":"code","1dcd588e":"code","c18d8ed9":"code","531922b3":"code","6262933c":"code","24a0dd3e":"code","de3faf19":"code","bebaff87":"code","eeb8465f":"code","f1032f1e":"code","642f6e73":"code","dddc5cff":"code","928fe6b3":"code","54d9dc25":"code","cfffd585":"code","d53ba395":"code","378bbdee":"code","d3e5cd09":"code","f52783bc":"code","eeebc6d5":"code","a9b5c15b":"code","e43c0e23":"code","610d7558":"code","0f302605":"code","71c828f3":"code","50df09f6":"code","660896a9":"code","572e74c5":"code","d268b3cc":"code","2d5a51ef":"code","7ee341c2":"code","79b9c454":"code","7b0d4b23":"markdown","874e78c5":"markdown"},"source":{"c6f7ebd0":"import seaborn as sns\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom bs4 import BeautifulSoup","88e1ae1c":"from gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nfrom tqdm import tqdm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nimport sqlite3\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n#from sklearn.externals import joblib\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import classification_report\nfrom prettytable import PrettyTable","5a6b84e9":"df = pd.read_csv('..\/input\/real-or-fake-fake-jobposting-prediction\/fake_job_postings.csv',  engine = 'python')","0f127d94":"df.head()","0b93fc53":"del df['job_id']\ndel df['salary_range']","d20e92c4":"df.fillna(\" \",inplace = True)","0ff1ce3b":"df['textdata'] = df['title'] + ' ' + df['location'] + ' ' + df['department'] + ' ' + df['company_profile'] + ' ' + df['description'] + ' ' + df['requirements'] + ' ' + df['benefits'] + ' ' + df['employment_type'] + ' ' + df['required_education'] + ' ' + df['industry'] + ' ' + df['function'] ","068e3610":"del df['title']\ndel df['location']\ndel df['department']\ndel df['company_profile']\ndel df['description']\ndel df['requirements']\ndel df['benefits']\ndel df['employment_type']\ndel df['required_experience']\ndel df['required_education']\ndel df['industry']\ndel df['function']","95bf0f5f":"df.head()","69d6bebd":"import re\n\nimport nltk\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords","1dcd588e":"stop = set(stopwords.words('english'))\nlen(stop)","c18d8ed9":"import re,string,unicodedata\npunctuation = list(string.punctuation)\nstop.update(punctuation)","531922b3":"def cleaner(phrase):\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can't\", 'can not', phrase)\n  \n  # general\n    phrase = re.sub(r\"n\\'t\",\" not\", phrase)\n    phrase = re.sub(r\"\\'re'\",\" are\", phrase)\n    phrase = re.sub(r\"\\'s\",\" is\", phrase)\n    phrase = re.sub(r\"\\'ll\",\" will\", phrase)\n    phrase = re.sub(r\"\\'d\",\" would\", phrase)\n    phrase = re.sub(r\"\\'t\",\" not\", phrase)\n    phrase = re.sub(r\"\\'ve\",\" have\", phrase)\n    phrase = re.sub(r\"\\'m\",\" am\", phrase)\n    \n    return phrase","6262933c":"cleaned_title = []\n\nfor sentance in tqdm(df['textdata'].values):\n    sentance = str(sentance)\n    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    sentance = cleaner(sentance)\n    sentance = re.sub(r'[?|!|\\'|\"|#|+]', r'', sentance)\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stop)\n    cleaned_title.append(sentance.strip())","24a0dd3e":"df['textdata'] = cleaned_title","de3faf19":"df.head()","bebaff87":"from nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.stem import LancasterStemmer,WordNetLemmatizer\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet","eeb8465f":"from wordcloud import WordCloud,STOPWORDS","f1032f1e":"plt.figure(figsize = (20,20)) # Text that is not fraudulent(0)\nwc = WordCloud(width = 1600 , height = 800 , max_words = 3000).generate(\" \".join(df[df.fraudulent == 0].textdata))\nplt.imshow(wc , interpolation = 'bilinear')","642f6e73":"plt.figure(figsize = (20,20)) # Text that is  fraudulent(1)\nwc = WordCloud(width = 1600 , height = 800 , max_words = 3000).generate(\" \".join(df[df.fraudulent == 1].textdata))\nplt.imshow(wc , interpolation = 'bilinear')","dddc5cff":"import seaborn as sns\nsns.countplot(x = \"fraudulent\", data=df)","928fe6b3":"from sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split","54d9dc25":"X_Train, X_test, y_Train, y_test = train_test_split(df.textdata, df.fraudulent, random_state=0, stratify=df.fraudulent, test_size=0.1)","cfffd585":"X_Train.shape","d53ba395":"X_train, X_cross, y_train, y_cross = train_test_split(X_Train, y_Train, random_state=0, stratify=y_Train, test_size=0.1)","378bbdee":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB","d3e5cd09":"y_train.shape\n#X_train.shape","f52783bc":"tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n#transformed train reviews\ntv_train_reviews=tv.fit_transform(X_train)\n#transformed test reviews\n","eeebc6d5":"tv_cross_reviews=tv.transform(X_cross)\nprint('Tfidf_train:',tv_train_reviews.shape)\nprint('Tfidf_test:',tv_cross_reviews.shape)","a9b5c15b":"tv_test_reviews=tv.transform(X_test)","e43c0e23":"alpha_set=[0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000]\nTrain_AUC_BOW = []\nCrossVal_AUC_BOW = []\nfor i in alpha_set:\n    naive_b=MultinomialNB(alpha=i)\n    naive_b.fit(tv_train_reviews, y_train)\n    Train_y_pred =  naive_b.predict(tv_train_reviews)\n    Train_AUC_BOW.append(roc_auc_score(y_train,Train_y_pred))\n    CrossVal_y_pred =  naive_b.predict(tv_cross_reviews)\n    CrossVal_AUC_BOW.append(roc_auc_score(y_cross,CrossVal_y_pred))","610d7558":"from numpy import math\nAlpha_set=[]\nfor i in range(len(alpha_set)):\n    Alpha_set.append(math.log(alpha_set[i]))","0f302605":"plt.plot(Alpha_set, Train_AUC_BOW, label='Train AUC')\nplt.scatter(Alpha_set, Train_AUC_BOW)\nplt.plot(Alpha_set, CrossVal_AUC_BOW, label='CrossVal AUC')\nplt.scatter(Alpha_set, CrossVal_AUC_BOW)\nplt.legend()\nplt.xlabel(\"alpha : hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.show()","71c828f3":"optimal_alpha=alpha_set[CrossVal_AUC_BOW.index(max(CrossVal_AUC_BOW))]\nprint(optimal_alpha)","50df09f6":"Classifier1=MultinomialNB(alpha=optimal_alpha)\nClassifier1.fit(tv_train_reviews, y_train)","660896a9":"auc_train_bow = roc_auc_score(y_train,Classifier1.predict(tv_train_reviews))\nprint (\"AUC for Train set\", auc_train_bow)\n\nauc_test_bow = roc_auc_score(y_test,Classifier1.predict(tv_test_reviews))\nprint (\"AUC for Test set\",auc_test_bow)","572e74c5":"from sklearn.metrics import accuracy_score, log_loss, f1_score\n\npreds = Classifier1.predict(tv_test_reviews)\n\nacc = accuracy_score(y_test, preds)\n\nf1 = f1_score(y_test, preds, average='macro')\n\nprint ('Accuracy is : ', acc)\nprint ('F1 Score is :', f1)","d268b3cc":"print('Confusion Matrix of Test Data')\nTest_mat=confusion_matrix(y_test,preds)\nprint (Test_mat)","2d5a51ef":"from sklearn import metrics\nprint(metrics.classification_report(y_test, preds))","7ee341c2":"cm_cv = pd.DataFrame(Test_mat, index=[0,1], columns=[0,1])\ncm_cv.index.name = 'Actual'\ncm_cv.columns.name = 'Predicted'","79b9c454":"plt.figure(figsize = (10,10))\nsns.heatmap(cm_cv,cmap= \"Blues\",annot = True )","7b0d4b23":"Thanks to https:\/\/www.kaggle.com\/madz2000 for his notebook https:\/\/www.kaggle.com\/madz2000\/text-classification-using-keras-nb-97-accuracy \nThe visualizations were borrowed from the notebook. ","874e78c5":"This is my first Kernel on Kaggle, Please upvote if it was of any help.\nLeave a comment if there are any queries.\nThanks.."}}