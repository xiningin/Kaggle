{"cell_type":{"bb4d824b":"code","233acf37":"code","95a58df5":"code","313252d6":"code","887a0e09":"code","d9d7734b":"code","92920b45":"code","4689dadd":"code","2d8a9077":"code","6b7d0827":"code","fdf957c3":"code","f69cc444":"code","80887b1e":"markdown","9714a365":"markdown","ab0fccdd":"markdown","48a7f0f5":"markdown","bf18cde0":"markdown","d219fc7a":"markdown","d894f61f":"markdown","890f28ec":"markdown","bc97b0a8":"markdown"},"source":{"bb4d824b":"# Kaggle environment comes with many python libraries preinstalled\nfrom IPython.display import display\nimport numpy as np      # linear algebra\nimport pandas as pd     # data processing\nimport matplotlib.pyplot as plt   # used for data visualization \nimport seaborn as sns             # used for data visualization\nimport warnings\nimport time\n# ignore all warnings\nwarnings.filterwarnings('ignore')","233acf37":"input_dir = '..\/input\/titanic'\ntrain_dir = '..\/input\/titanic\/train.csv'\ntest_dir = '..\/input\/titanic\/test.csv'","95a58df5":"train_data = pd.read_csv(train_dir)\nprint(train_data.shape)\ntrain_data.head()","313252d6":"\"\"\"\nonly choosing the features that seem useful \n\nnote when the no of features are high pd.drop can be used to remove the columns inplace\ninstead of creating a new dataframe\n\n\"\"\"\ntraindf = train_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Survived']]\ntraindf.head()","887a0e09":"# Checking the unique values and their frequency\ntraindf.Sex.replace({\"male\": \"0\", \"female\": \"1\"}, inplace=True)\nprint(\"All the Values in Embarked column along with their frequency\\n\")\ndisplay(traindf.Embarked.value_counts())\nprint(\"\\nAll the Values in Parch column along with their frequency\\n\")\ndisplay(traindf.Parch.value_counts())\nprint(\"\\nAll the Values in SibSp column along with their frequency\\n\")\ndisplay(traindf.SibSp.value_counts())\ntraindf.head()","d9d7734b":"# Finding the nan values in the dataframe\nprint(traindf.isna().sum())\ntraindf.dropna(inplace = True)\nprint()\nprint(traindf.isna().sum())\n# Usually most algoritms need us to fix mixing values by either imputing them or dropping the\n# rows or columns with missing values with pd.dropna() or similar functions.\n# Luckily xgboost can handle missing values :) but for simplicity I dropped the columns with nans","92920b45":"# Converting to categorical and getting dummy variables\ndf_Embarked = pd.get_dummies(traindf.Embarked)\ntraindf = pd.concat([traindf, df_Embarked], axis = 1)\ntraindf.drop(['Embarked'], axis = 1, inplace = True)\ntraindf.head()","4689dadd":"# Convert the datatype of Sex column from object to int\ntraindf = traindf.astype({'Sex': int})\nprint(traindf.dtypes)","2d8a9077":"traindf.corr()","6b7d0827":"# number of survivors vs no of people who didn't survive\n\nax = sns.countplot(traindf.Survived,label=\"Count\")      \nDead, Survived = traindf.Survived.value_counts()\nprint('Number of Survivors: ',Survived)\nprint('Number of People who didn\\'t survive : ',Dead)","fdf957c3":"plotY = traindf.Survived\nplotX = traindf.drop('Survived', axis = 1)\nplotX_Std = (plotX - plotX.mean()) \/ (plotX.std()) # standardization\nplotX_Std = pd.concat([plotX_Std, plotY], axis = 1)\nplot_data = pd.melt(plotX_Std,id_vars=\"Survived\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(10,10))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"Survived\", data=plot_data,split=True, inner=\"quart\")\nplt.xticks(rotation=90)","f69cc444":"\"\"\"\nAs we have a pretty good idea about how sex affects the target variable, \nwe can study the effect of other features on the output\n\"\"\"\nsns.set(style=\"white\")\nplotdf = traindf.loc[:,['Sex','Age', 'Pclass','Survived']]\ng = sns.PairGrid(plotdf, diag_sharey=False)\ng.map_lower(sns.kdeplot, cmap=\"Blues_d\")\ng.map_upper(plt.scatter)\ng.map_diag(sns.kdeplot, lw=3)","80887b1e":"## Additional Resources \nhttps:\/\/towardsdatascience.com\/exploratory-data-analysis-8fc1cb20fd15\n\nhttps:\/\/matplotlib.org\/stable\/tutorials\/index.html\n\nhttps:\/\/www.geeksforgeeks.org\/python-seaborn-tutorial\/\n","9714a365":"## Loading the Data","ab0fccdd":"## Here we see various features like *Passangerid*, *Pclass*, *Name* and so on\n\n### A few things to note here:\n\n1) **PassengerId** is of no value when it comes to predicting the label\n\n2) Our target label is **Survived**\n\n3) **Name** column can be dropped since we can't predict if a person survives based on their name\n\n4) The **Ticket** column can be dropped since I doubt any effort in finding a correlation between the random string values and the target label is worth it.","48a7f0f5":"### The table above shows the correlation between various features\n\nThe diagonal is always 1 since every feature has perfect corelation with itself. In layman terms if any object is compard to itself, it is always a perfect match. When represented as a function x = y, if x increases by 1 so does y and so on. if x = -y, the magnitude of correlation is 1 but the sign is negetive since if x increases in +ve direction, y increases in negetive direction and vice versa\n\nWe have to focus on the **Survived** row and column as it is our target variable.\nThe greater the absolute value of the table either (positive or negetive), the greater the correlation; that is if its closer to 1 or -1, the more the dependency of the output on the given feature and the closer it is to 0, the lower the effect of the feature on the output.\n\nFrom the table we can see that **Sex** and **Pclass** have some effect on the output\n\nDon't worry this table is just to give an idea of which feature might be useful.\nWe can plot graphs to get a better understanding.","bf18cde0":"### Interpretation\n\nThe Diagonal Graphs simply represnt the distribution of the class that is how many entities belong to which subclass. For example in the 16th graph **Survived** X **Survived** we see the peak at 0 is higher which indicates the number of people who didn't survive is higher.\n\nThe graph **Survived** X **Age** gives us a similar intuition. The lower circular figure is more dense between 25 to 45 which corresponds to a 0 in **Survived**. It means that people in this age group are more likely to die than any other age group which makes sense cause elderly people and children are given more preference when it comes to rescue operations.\n\nThe graph of **Survived** X **Pclass** gives us the same inference that we previously derived. A 3rd class passanger is more likely to die than a first class one. It is represented my dense concentric ciruclar pattern corresponding to 0 in **Survived** and 3 in **Pclass**\n\nThe graph **Survived** X **Sex** shows women are more likely to survive than men","d219fc7a":"### Now how do we interpret the data?\nWhat we are looking for is peaks and how close they are to each other.\nIf they are close that means its not a good way to distribute the data\nIf they are far away, it means that they are good features to predict the output. Also the size of the peak is an indicator of the magnitude of the target value distribution.\n\nHere we see in ***PClass*** the peak of the blue section is high when its value is 3(note the value in the graph is around 1 as we applied standardization which squishes all values between 1 and -1). The blue section represents non survivors. The graph tells when a person has a 3rd class ticket, the person is less likely survive as indicated by the blue peak. It makes sense intuitively as high class passangers are more likely to get priority when it comes to rescue operations.\n\nSimilarly in the ***Sex*** section we see a clear distinctions in the peaks which indicate the male population has a far higher mortality rate than the females.","d894f61f":"## Plot the Data","890f28ec":"# Welcome to My Notebook","bc97b0a8":"# Preprocessing and Exploration\n\nPreprocessing is a vital for the success of any algorithm. Its like the marination we do before cooking a great dish\n\nPreprocessing heavily depends on the algorithm you are using as well as the data itself.\nXGboost, Random Forest are some of the golden algorithms that don't require much preprocssing \n\nHowever we need to convert the string data into categorical features.\n\nAlso in the columns like Pclass, we need to convert it into categorical feature. Having continuous numerical representation might lead the algorithm to misinterpret that the higher value has higher weight eg Pclass 3 has more weight than Pclass 1 when in reality they might just be the opposite."}}