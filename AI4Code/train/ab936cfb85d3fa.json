{"cell_type":{"d67d6342":"code","baa78435":"code","b97aed12":"code","a57704a5":"code","50488119":"code","274abe61":"code","21809cb5":"code","3a2963b5":"code","419d3426":"code","f35e8c78":"code","6d36c708":"code","197debf9":"code","631176e5":"code","c7dd2df6":"code","559091d8":"code","ea9a2812":"code","68227a62":"code","1961920d":"code","82afe87b":"code","5e707d1d":"code","d19e716f":"code","7e87f26d":"code","72bf0f76":"code","27584f74":"code","0dcb79ee":"markdown","7e5853da":"markdown","5229187d":"markdown","266e16ad":"markdown","4ae658c3":"markdown"},"source":{"d67d6342":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","baa78435":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nfrom collections import defaultdict\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nimport torch\ntorch.cuda.empty_cache()\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import Adam\nfrom transformers import BertModel, BertTokenizer\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","b97aed12":"train_data = pd.read_csv('\/kaggle\/input\/spooky-author-identification\/train.zip')\ntest_data = pd.read_csv('\/kaggle\/input\/spooky-author-identification\/test.zip')\ntrain_data","a57704a5":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n\n                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n\n                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n\n                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n\n                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n\n                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n\n                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n\n                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n\n                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n\n                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n\n                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n\n                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n\n                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n\n                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n\n                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n\n                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n\n                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n\n                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n\n                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n\n                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n\n                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n\n                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n\n                           \"you're\": \"you are\", \"you've\": \"you have\"}","50488119":"StopWords = set(stopwords.words('english'))\ndef text_clean(text):\n    text = re.sub(' +', ' ', text).strip() #Remove leading, trailing and in-between whitespaces\n    text = ' '.join([contraction_mapping[word] if word in contraction_mapping else word for word in text.split()])\n    text = re.sub('[^a-zA-Z]', ' ', text) #Remove non-alphabetical characters\n    #text = ' '.join([word.lower() for word in text.split() if (word not in StopWords) and (len(word) > 2)])\n    return text\n\ntrain_data['text'] = train_data['text'].apply(text_clean)\ntest_data['text'] = test_data['text'].apply(text_clean)","274abe61":"#Lemmatization\nlemmatizer = WordNetLemmatizer()\ndef lemmatize(text):\n    lem_text = ''\n    for word in text.split():\n        lem_word = lemmatizer.lemmatize(word)\n        lem_word = lemmatizer.lemmatize(word, pos='v')\n        lem_text = lem_text + ' ' + lem_word\n    return lem_text\n\ntrain_data['text'] = train_data['text'].apply(lemmatize)\ntest_data['text'] = test_data['text'].apply(lemmatize)\ntrain_data","21809cb5":"#Distribution of Train Target Labels\nsns.countplot(x = train_data['author'])","3a2963b5":"#Encoding Labels and Preparing Training, Validation Sets.\nencoder = LabelEncoder()\ntrain_data['author'] = encoder.fit_transform(train_data['author'])\nX_train, X_val, y_train, y_val = train_test_split(train_data['text'], train_data['author'], train_size=0.8)\ntrain_df = pd.DataFrame()\nval_df = pd.DataFrame()\ntrain_df['text'] = X_train\ntrain_df['author'] = y_train\nval_df['text'] = X_val\nval_df['author'] = y_val","419d3426":"#Load the BERT model and tokenizer to be used\ntokenizer = BertTokenizer.from_pretrained('bert-base-cased')\nbert_model = BertModel.from_pretrained('bert-base-cased')","f35e8c78":"#Custom Dataset class, subclass of the Dataset Module.\n#Necessarily contains the below 3 methods: __init__, __len__, __getitem__\nclass CustomDataset(Dataset):\n    def __init__(self, text, label, tokenizer):\n        self.text  = text\n        self.label = label\n        self.tokenizer  = tokenizer\n    \n    def __len__(self):\n        #Number of examples\n        return len(self.label)\n    \n    def __getitem__(self, idx):\n        #Return a dict of text, labels, tokenized data at specific idx\n        sample_text = str(self.text[idx])\n        sample_label = self.label[idx]\n        tokenizer_dict = self.tokenizer.encode_plus(\n        sample_text,\n        add_special_tokens=True,\n        max_length=512,\n        padding='max_length',\n        return_token_type_ids=False,\n        return_attention_mask=True,\n        truncation=True,\n        return_tensors='pt' #pytorch tensor format\n        )\n        return {\n            'text' : sample_text,\n            'input_ids': tokenizer_dict['input_ids'].flatten(),\n            'attn_mask': tokenizer_dict['attention_mask'].flatten(),\n            'label' : torch.tensor(sample_label, dtype=torch.int64)\n        }","6d36c708":"#Same as above for test data, sans targets\nclass CustomTestDataset(Dataset):\n    def __init__(self, text, tokenizer):\n        self.text = text\n        self.tokenizer = tokenizer\n        \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, idx):\n        test_seq = self.text[idx]\n        tokenizer_dict = self.tokenizer.encode_plus(\n            test_seq,\n            add_special_tokens = True,\n            max_length = 512,\n            padding = 'max_length',\n            truncation = True,\n            return_token_type_ids = False,\n            return_attention_mask = True,\n            return_tensors = 'pt'\n        )\n        return {'text':test_seq,\n                'input_ids':tokenizer_dict['input_ids'].flatten(),\n                'attn_mask':tokenizer_dict['attention_mask'].flatten()}\n        ","197debf9":"#Instantiate custom datasets by passing constructor arguments\ndata_train = CustomDataset(train_df['text'].to_numpy(), train_df['author'].to_numpy(), tokenizer)\ndata_val = CustomDataset(val_df['text'].to_numpy(), val_df['author'].to_numpy(), tokenizer)\ndata_test = CustomTestDataset(test_data['text'].to_numpy(), tokenizer)\n\n#Create iterable dataloaders, which create batches of given size returned as the return type of __getitem__ (dict here)\ntrain_dataloader = DataLoader(data_train, batch_size=4)\nval_dataloader = DataLoader(data_val, batch_size=4)\ntest_dataloader = DataLoader(data_test, batch_size=4)\n#How a batch looks:\nnext(iter(train_dataloader))","631176e5":"#Create custom NN, always inheriting from nn.Module\nclass BERT_Classifier(nn.Module):\n    def __init__(self, num_classes):\n        super(BERT_Classifier, self).__init__()\n        self.num_classes = num_classes\n        self.bert_layer = bert_model\n        self.dropout = nn.Dropout(p=0.2)\n        self.linear = nn.Linear(self.bert_layer.config.hidden_size, self.num_classes) #Outputs logits for num_classes\n        \n    def forward(self, input_ids, attn_mask):\n        last_hidden, pooled_out = self.bert_layer(input_ids=input_ids, attention_mask= attn_mask, return_dict=False)\n        #return_dict=False required in v4 for working like v3.\n        dropout = self.dropout(pooled_out)\n        logits = self.linear(dropout)\n        #probs = nn.Softmax(dim=1)(logits)\n        return logits","c7dd2df6":"#Instantiate model, transfer to gpu device if available\nmodel = BERT_Classifier(3).to(device)","559091d8":"#Trial feed forward on one smaple batch.\ninput_ids = next(iter(train_dataloader))['input_ids'].to(device)\nattn_mask = next(iter(train_dataloader))['attn_mask'].to(device)\nlogits = model(input_ids, attn_mask)\nprobs = nn.Softmax(dim=1)(logits)\ntorch.max(probs, dim=1)","ea9a2812":"#Training Essentials\noptimizer = Adam(model.parameters(), lr=2e-5)\nloss = nn.CrossEntropyLoss().to(device) #Categorical CrossEntropy\nepochs = 7","68227a62":"#Train Loop, iterates over all batches\ndef train_loop(model, dataloader, loss, optimizer):\n    model = model.train() #Setting to train mode activates Dropouts, Batch Norm, etc\n    batch_losses = []\n    pred_correct = 0\n    for batch in dataloader: #Each batch as a Dict from __getitem__, with dict values having batch-size num of elements\n        input_ids = batch['input_ids'].to(device)\n        attn_mask = batch['attn_mask'].to(device)\n        ground_truths = batch['label'].to(device)\n        logits = model(input_ids, attn_mask)\n        output = nn.Softmax(dim=1)(logits) #Generate probabilites on output of NN\n        prob, labels = torch.max(output, dim=1) #Extract class labels from max(logits) or max(probs)\n        batch_loss = loss(logits, ground_truths) #Compute Loss\n        batch_losses.append(batch_loss.item())\n        pred_correct += torch.sum(labels == ground_truths) #Compute num of correct predictions for accuracy\n        \n        #Backprop Steps - Essential.\n        batch_loss.backward() #Performs backprop\n        optimizer.step() #Updates parameters\n        optimizer.zero_grad() #Resets gradients to zero for next batch\n    \n    return np.mean(batch_losses), pred_correct.double() \/ len(train_df) #Return train loss, train acc","1961920d":"#Validation Loop, almost same - No backprop\ndef validation_loop(model, dataloader, loss):\n    model = model.eval() #Deactivates dropouts, batch norm etc\n    batch_losses = []\n    pred_correct = 0\n    with torch.no_grad(): #Disable Autograd - No parameter optimization during validation\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attn_mask = batch['attn_mask'].to(device)\n            ground_truths = batch['label'].to(device)\n            logits = model(input_ids, attn_mask)\n            output = nn.Softmax(dim=1)(logits)\n            prob, labels = torch.max(output, dim=1)\n            batch_loss = loss(logits, ground_truths)\n            batch_losses.append(batch_loss.item())\n            pred_correct += torch.sum(labels == ground_truths)\n    \n    return np.mean(batch_losses), pred_correct.double() \/ len(val_df) #Returns Val Loss, Val Acc","82afe87b":"#To extract predictions on test data\ndef get_predictions(model, dataloader):\n    model.eval()\n    pred_labels = []\n    pred_probs = []\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attn_mask = batch['attn_mask'].to(device)\n            logits = model(input_ids, attn_mask)\n            probs = nn.Softmax(dim=1)(logits)\n            pred_probs.extend(probs)\n            _, labels = torch.max(probs, dim=1)\n            pred_labels.extend(labels)\n    \n    return pred_labels, pred_probs","5e707d1d":"#Running the train, validation loops over all epochs. All batches over each epoch.\nhistory = defaultdict(list)\nfor epoch in range(epochs):\n    print('Epoch ', epoch+1, ' of', epochs, ' :')\n    train_loss, train_acc = train_loop(model, train_dataloader, loss, optimizer)\n    print('Training Loss: ', train_loss, '    Training Acc: ', train_acc)\n    val_loss, val_acc = validation_loop(model, val_dataloader, loss)\n    print('Val Loss: ', val_loss, '    Val Acc: ', val_acc)\n    history['train_loss'].append(train_loss)\n    history['train_acc'].append(train_acc)\n    history['val_loss'].append(val_loss)\n    history['val_acc'].append(val_acc)","d19e716f":"plt.figure(1)\nplt.plot(history['train_loss'], label='Train Loss')\nplt.plot(history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid()\nplt.show()\nplt.figure(2)\nplt.plot(history['train_acc'], label='Train Accuracy')\nplt.plot(history['val_acc'], label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid()\nplt.show()","7e87f26d":"y_pred, y_prob = get_predictions(model, test_dataloader)","72bf0f76":"y_prob = torch.stack(y_prob).cpu().numpy()","27584f74":"df = pd.DataFrame()\ndf['id'] = test_data['id']\ndf['EAP'] = y_prob[:,0]\ndf['HPL'] = y_prob[:,1]\ndf['MWS'] = y_prob[:,2]\ndf.to_csv('Submission.csv', index=False)","0dcb79ee":"# V. Making Predictions and Generate Submissions","7e5853da":"# I. Importing Libraries and Reading Data","5229187d":"# II. Text Preprocessing","266e16ad":"# III. Preparing Data for Model\n* Train-Validation Splitting\n* Creating Custom Dataset Class for Train, Test(no target labels)\n* Creating an iterable wrapper called DataLoader for train, val, test data while splitting into batches.","4ae658c3":"# IV. Model Definition and Training"}}