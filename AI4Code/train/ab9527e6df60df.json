{"cell_type":{"4afc74a7":"code","8625f5b6":"code","f45e66bb":"code","145f3d70":"code","b827a391":"code","364c20f2":"code","b6069706":"code","00a24fba":"code","1775df5f":"code","6593ebfa":"code","85553397":"code","cd2cc805":"code","4121b5e6":"code","499ba0d5":"code","292db0f9":"code","9c5c8231":"code","14e4193e":"code","842bfaad":"code","1aaf06e9":"code","1e4738cf":"code","378bd9e4":"code","3d7061a2":"code","c5dd97ee":"code","0569ad01":"code","c2e5070d":"code","4ddea95d":"code","7b256bff":"code","52fcb5db":"code","066a08f0":"code","836a5b71":"code","dde94a3d":"code","19a5364e":"code","77166d0b":"code","72a3f497":"code","8cf67a0f":"code","2ca08dfa":"code","484e5edc":"code","6e1cd3d5":"code","d42fe452":"code","1cecb78e":"code","43c95b03":"code","135ba95c":"code","a1ba4cf7":"code","67b3de79":"code","efbb8c2d":"code","55342ecc":"code","43c2abe8":"code","ca371e24":"code","5f6cdac9":"code","d6bf75ac":"code","bfb80dbe":"code","71054abf":"code","f89586b3":"code","4f3475c8":"code","bd911b13":"code","6a5cc165":"code","e24518d4":"code","d49f1472":"code","1a8f6929":"code","4eee57b4":"code","f8590fb3":"code","17934801":"code","7dcedc2a":"code","b0360685":"code","e658cd50":"code","44109806":"code","421aa583":"code","49e425bb":"code","145303d4":"code","7b3efed3":"code","403744dd":"code","2dab36dd":"code","6d215619":"code","38488a5b":"code","d174ab3d":"code","e1523e9c":"code","f8d51b23":"code","4cfbf251":"markdown","c6a86623":"markdown","329aa980":"markdown","874eb448":"markdown","ed742d27":"markdown","e670e77c":"markdown","7b77b4e1":"markdown","aab21f7e":"markdown","b4e92123":"markdown","5b6adbd8":"markdown","6a06bdbd":"markdown","1344bc16":"markdown","6cfdfe9b":"markdown","a43f398b":"markdown","bac456b0":"markdown","d18806e4":"markdown","a6b3a723":"markdown","bb736d30":"markdown","f17adb8d":"markdown","3a249b04":"markdown","b1e2f746":"markdown","95b408b9":"markdown","e6c3047f":"markdown","dc03a94e":"markdown","5eaf0d13":"markdown","b2d42d10":"markdown","bd8d4be7":"markdown","e508cff1":"markdown","f6c6cc50":"markdown","e1377bd5":"markdown","392165ae":"markdown","cd57e304":"markdown","0546ea3a":"markdown","16cab5d9":"markdown","efd36c1b":"markdown","215f08a3":"markdown","7a785b73":"markdown","b0233d5c":"markdown","c2c56d19":"markdown"},"source":{"4afc74a7":"import numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\nimport json\n\nfrom scipy.stats import boxcox, skew\nfrom statsmodels.api import qqplot\nfrom sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import cross_validate, train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import IsolationForest\nimport xgboost as xgb\nimport lightgbm as lgb\n\nsns.set_theme(style='darkgrid')","8625f5b6":"data = pd.read_csv('\/kaggle\/input\/lianjia\/new.csv', low_memory=False)","f45e66bb":"data.head()","145f3d70":"data.info()","b827a391":"# get description for numerical features\ndata.describe()","364c20f2":"# remove the first 3 chinese characters\nfloor_col = data.floor.apply(lambda x: str(x)[3:])\ndata['floor'] = floor_col","b6069706":"floor_col.value_counts()","00a24fba":"plt.figure(figsize=(14,5))\nsns.barplot(x=floor_col.value_counts().index, y=floor_col.value_counts()).set_title('Floor values')\nplt.xticks(rotation=45)\nplt.show()\n\n# note the \u1e79value","1775df5f":"floor_u_indexes = data.index[data['floor']=='\u1e79'].tolist()\ndata = data.drop(index=floor_u_indexes)","6593ebfa":"# renaming certain columns to better reflect their data\ndata = data.rename(columns={\n    'livingRoom': 'bedRoom',\n    'drawingRoom': 'livingRoom',\n    'floor': 'buildingHeight',\n    'tradeTime': 'soldDate',\n    'constructionTime': 'constructionDate',\n    'Cid': 'communityId',\n    'price': 'pricePerSM',\n    'square': 'squareMeters'\n})\n# data.info()","85553397":"data['constructionDate'].value_counts().index","cd2cc805":"construction_d_indexes = data.index[data['constructionDate']=='\u03b4\u05aa'].tolist()\ndata = data.drop(index=construction_d_indexes)","4121b5e6":"# changing data types\ndata = data.astype({\n    'buildingType': 'object',\n    'renovationCondition': 'object',\n    'buildingStructure': 'object',\n    'elevator': 'object',\n    'subway': 'object',\n    'communityId': 'object',\n    'fiveYearsProperty': 'object',\n    'district': 'object',\n})\n\ndata = data.astype({\n    'buildingHeight': 'int32',\n    'soldDate': 'datetime64',\n    'constructionDate': 'int32',\n    'livingRoom': 'int32',\n    'bedRoom': 'int32',\n    'bathRoom': 'int32',\n})\n\n# changing unnecessarily big int64s to int32s\ndata = data.astype({col: 'int32' for col in data.select_dtypes('int64').columns})\n\n# changing unnecessarily big float64s to float32s\ndata = data.astype({col: 'float32' for col in data.select_dtypes('float64').columns})\n\n# removing totalPrice as it is irrelevant\nif 'totalPrice' in data.columns:\n    data = data.drop('totalPrice', axis=1)\n\ndata.info()","499ba0d5":"data.describe()","292db0f9":"data['soldDateYear'] = data['soldDate'].dt.year.astype('int32')\ndata['soldDateMonth'] = data['soldDate'].dt.month.astype('int32')\ndata['soldDateDay'] = data['soldDate'].dt.day.astype('int32')\ndata.info()","9c5c8231":"fig, axs = plt.subplots(1,3,figsize=(21, 7))\naxs[0].bar(data.soldDateYear.value_counts().index, data.soldDateYear.value_counts())\naxs[0].set_title('Sold Year')\naxs[1].bar(data.soldDateMonth.value_counts().index, data.soldDateMonth.value_counts())\naxs[1].set_title('Sold Month')\naxs[2].bar(data.soldDateDay.value_counts().index, data.soldDateDay.value_counts())\naxs[2].set_title('Sold Day')","14e4193e":"data = data.drop('soldDate', axis=1)\ndata.info()","842bfaad":"# Group the data\ndt = data.groupby(['soldDateYear', 'soldDateMonth', 'soldDateDay'])['pricePerSM'].mean().reset_index()\ndt.drop(dt[dt['soldDateYear']<2010].index, inplace=True) # the years prior to 2010 only have a few points, while the ones after have at least one for every month\ndt['soldDate'] = list(zip(dt.soldDateYear, dt.soldDateMonth))\ndt.reset_index(inplace=True)\n\n# Plot it with the right x ticks\nfig = plt.figure(figsize=(20,10))\nfig.suptitle('House Prices Over Time', fontsize=20)\nsns.boxplot(data=dt, x='soldDate',y='pricePerSM', color=\"r\", saturation=0.7)\nplt.xticks(np.arange(0, len(dt['soldDate'].unique()), 12), labels=range(2010, 2018+1))\nplt.show()","1aaf06e9":"data = data.astype({\n    'soldDateYear': 'object',\n    'soldDateMonth': 'object',\n    'soldDateDay': 'object',\n})","1e4738cf":"def viz_numerical_data(data):\n    num_data_cols = data.describe().columns\n    num_of_cols = 5\n    num_of_rows = int(math.ceil(num_data_cols.size\/num_of_cols))\n    fig, axs = plt.subplots(num_of_rows, num_of_cols, figsize=(num_of_cols*5, num_of_rows*5))\n    fig_row = -1\n\n    for i, col in enumerate(num_data_cols):\n        if i%num_of_rows == 0:\n            fig_row += 1\n        axs[i%num_of_rows, fig_row].hist(data[col])\n        axs[i%num_of_rows, fig_row].title.set_text(col)","378bd9e4":"viz_numerical_data(data)","3d7061a2":"# dropping the url and ids as they're not useful for this vizualisation\ndt = data.drop(['url', 'id', 'communityId'], axis=1)","c5dd97ee":"cat_cols = dt.dtypes[dt.dtypes == \"object\"].index\nnum_of_rows = int((cat_cols.size\/3))\ncat_cols.size","0569ad01":"cat_cols = dt.dtypes[dt.dtypes == \"object\"].index\nnum_of_cols = 4\nnum_of_rows = int(math.ceil(cat_cols.size\/num_of_cols))\nfig, axs = plt.subplots(num_of_rows, num_of_cols, figsize=(num_of_cols*5, num_of_rows*5))\nfig_row = -1\n\nfor i, col in enumerate(cat_cols):\n    if i%num_of_rows == 0:\n        fig_row += 1\n    axs[i%num_of_rows, fig_row].bar(dt[col].value_counts(sort=False, ascending=True).index, dt[col].value_counts(sort=False, ascending=True))\n    axs[i%num_of_rows, fig_row].title.set_text(col)","c2e5070d":"plt.figure(figsize=(14,10))\nsns.scatterplot(x='Lng', y='Lat', hue='pricePerSM', palette='viridis_r', data=data)","4ddea95d":"sns.relplot(kind='scatter', x=\"Lng\", y=\"Lat\", hue=\"pricePerSM\", col='district', palette='viridis_r', col_wrap=5, data=data)    ","7b256bff":"# Compute mean price by district\ndistrict_price = data[[\n    'pricePerSM',\n    'district']].groupby(\"district\").mean('pricePerSM').reset_index()\ndistrict_price.rename(columns = {'pricePerSM':'districtPrice'}, inplace = True)\ndistrict_price = district_price.astype({\n    'district': 'object',\n    'districtPrice': 'float32',\n})\n\n# Add the district mean to a copy of the dataset\ndt = data.copy()\ndt = pd.merge(dt, district_price, on='district')\ndt.info()","52fcb5db":"plt.figure(figsize=(14,10))\nsns.scatterplot(x='Lng', y='Lat', hue='districtPrice', palette='viridis_r', data=dt)","066a08f0":"data = dt","836a5b71":"# for analysing the relation between columns and the price, and seeing their clear outliers\ndef plot_data_with_price(dataset, col, discrete=False):\n    if discrete:\n        fig, ax = plt.subplots(1,2,figsize=(14,6))\n        sns.stripplot(x=col, y='pricePerSM', data=dataset, ax=ax[0])\n        sns.countplot(dataset[col], ax=ax[1])\n        fig.suptitle(str(col) + ' vs Price')\n    else:\n        fig, ax = plt.subplots(1,2,figsize=(12,6))\n        sns.scatterplot(x=col, y='pricePerSM', data=dataset, ax=ax[0])\n        sns.distplot(dataset[col], kde=False, ax=ax[1])\n        fig.suptitle(str(col) + ' vs Price')","dde94a3d":"sns.heatmap(data[data.describe().columns].corr())","19a5364e":"top_features = data.corr()[['pricePerSM']].sort_values(by=['pricePerSM'],ascending=False)\nplt.figure(figsize=(5,10))\nsns.heatmap(top_features,cmap='rainbow',annot=True,annot_kws={\"size\": 16},vmin=-1)","77166d0b":"plot_data_with_price(data, 'communityAverage')","72a3f497":"dt = data.drop(data[(data['communityAverage'] < 60000) & (data['pricePerSM'] > 90000)].index).copy()\nplot_data_with_price(dt, 'communityAverage')","8cf67a0f":"data = dt","2ca08dfa":"nulls = data.isnull().sum()\nnulls = pd.DataFrame(nulls[nulls > 0].sort_values(ascending = False))\nnulls.reset_index(level=0, inplace=True)\nnulls.columns = ['Feature','Number of Missing Values']\nnulls['Percentage of Missing Values'] = (100.0*nulls['Number of Missing Values'])\/len(data)\nnulls","484e5edc":"dt = data.copy()","6e1cd3d5":"dt['communityAverage'].fillna(dt.groupby('district')['communityAverage'].transform('mean'), inplace=True)\ndt['communityAverage'].isnull().sum()","d42fe452":"dt['buildingType'].fillna(dt.groupby('district')['buildingType'].transform(lambda x: x.mode()[0]), inplace=True)\ndt['buildingType'].isnull().sum()","1cecb78e":"dt['DOM'].isnull().sum()","43c95b03":"dt['DOM'].fillna(dt.groupby(['district', 'renovationCondition', 'buildingType'])['DOM'].transform('mean'), inplace=True)\ndt['DOM'].isnull().sum()","135ba95c":"feature = 'DOM'\nfig, axs = plt.subplots(1,2,figsize=(14,7), sharey=True)\nsns.histplot(data=data, x=feature, ax=axs[0]).set_xlim(left=0, right=80)\nsns.histplot(data=dt, x=feature, ax=axs[1]).set_xlim(left=0, right=80)\nfig.suptitle(f'NaN Handling: {feature}', fontsize=20)","a1ba4cf7":"feature = 'communityAverage'\nfig, axs = plt.subplots(1,2,figsize=(14,7), sharey=True)\nsns.histplot(data=data, x=feature, ax=axs[0])\nsns.histplot(data=dt, x=feature, ax=axs[1])\nfig.suptitle(f'NaN Handling: {feature}', fontsize=20)","67b3de79":"dt = dt.astype({\n    'buildingType': 'int32',\n    'fiveYearsProperty': 'int32',\n    'elevator': 'int32',\n    'subway': 'int32',\n})\ndt = dt.astype({\n    'buildingType': 'object',\n    'fiveYearsProperty': 'object',\n    'elevator': 'object',\n    'subway': 'object',\n})","efbb8c2d":"feature = 'buildingType'\nfig = plt.figure(figsize=(7,7))\n_, _, patches = plt.hist([data[feature], dt[feature]], color=['r','b'], alpha=0.6, bins=np.arange(1,6)-0.6)\nplt.title(f'NaN Handling: {feature}', fontsize=20)\nplt.xlabel(feature)\nplt.ylabel('Count')\nplt.xticks(ticks=[1,2,3,4])\n\n# add the annotations\nfor patch, d in zip(patches, [data[feature].value_counts(), dt[feature].value_counts()]):\n    row = 1\n    for pp in patch:\n        x = pp.get_x()\n        y = pp.get_y() + pp.get_height() + 1000\n        plt.text(x, y, d[row])\n        row += 1\n\nplt.show()","55342ecc":"data = dt","43c2abe8":"dt = data.drop(['url', 'id', 'communityId'], axis=1)\n\nclf = IsolationForest(max_samples=500)\noutliers = clf.fit_predict(dt)\noutliers = pd.DataFrame(outliers, columns = ['Top'])\n\noutliers[outliers['Top'] == 1].index.values\ndt = dt.iloc[outliers[outliers['Top'] == 1].index.values]\ndt.reset_index(drop = True, inplace = True)\n\nprint(\"Number of Outliers:\", outliers[outliers['Top'] == -1].shape[0])\nprint(\"Number of rows without outliers:\", dt.shape[0])\nprint(f\"Percentage trimmed out: {100 - dt.shape[0]*100\/data.shape[0]: .1f}%\")","ca371e24":"data = dt","5f6cdac9":"dt = data.copy()","d6bf75ac":"numerical_cols = dt.describe().columns\nnumerical_cols","bfb80dbe":"'''\ndesc = dt.describe()\nfor d in desc:\n    # if the min value is 0, then add the max value to every row\n    if desc[d].loc['min'] == 0:\n        max_x = desc[d].loc['max']\n        dt[d] = dt[d].apply(lambda x: x + max_x)\n    \n    # if the min value is negative, add that value to every row\n    if desc[d].loc['min'] < 0:\n        min_x = desc[d].loc['min']\n        dt[d] = dt[d].apply(lambda x: x + min_x)\n\ndt.describe()\n'''","71054abf":"def get_skewed_feats(data, numerical_cols):\n    skewed_feats = data[numerical_cols].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n    skewness = pd.DataFrame({'Skew' :skewed_feats})\n    print(skewness)\n    skewness = skewness[abs(skewness) > 0.75].dropna()\n    \n    print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n    return skewness\n\nskewness = get_skewed_feats(dt, numerical_cols)","f89586b3":"'''\nlambda = -1. is a reciprocal transform.\nlambda = 0.0 is a log transform.\nlambda = 0.5 is a square root transform.\n'''\n\ndef get_boxbox_lambdas(skew_data, skewness, numerical_cols):\n    lambdas = {}\n    skewed_cols = skewness.index\n    for col in skewed_cols:\n        _, l = boxcox(skew_data[col]+1)\n        if abs(l) > 2: # protect against exaggerated lambda values\n            l = 0\n        lambdas[col] = l\n    \n    print(json.dumps(lambdas, indent=2))\n    \n    return lambdas\n    \ndef boxcox_transform(skew_data, skewness, numerical_cols, lambdas):\n    skewed_cols = skewness.index\n    for col in skewed_cols:\n        skew_data[col] = boxcox(skew_data[col]+1, lambdas[col])\n\n    skewness = get_skewed_feats(skew_data, numerical_cols)\n    \n    return skew_data, skewness","4f3475c8":"lambdas = get_boxbox_lambdas(dt, skewness, numerical_cols)\ndt, skewness = boxcox_transform(dt, skewness, numerical_cols, lambdas)","bd911b13":"viz_numerical_data(dt)","6a5cc165":"data = dt","e24518d4":"dt = data.copy()","d49f1472":"numerical_cols = dt.describe().columns\nnumerical_cols","1a8f6929":"scaler = StandardScaler()\ndt[numerical_cols] = scaler.fit_transform(dt[numerical_cols])","4eee57b4":"viz_numerical_data(dt)","f8590fb3":"data = dt","17934801":"dt = pd.get_dummies(data)\ndt.info()","7dcedc2a":"data = dt","b0360685":"x = data.drop('pricePerSM', axis = 1)\ny = data['pricePerSM']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\nprint(f'Training set:\\t\\t{x_train.shape}')\nprint(f'Testing set:\\t\\t{x_test.shape}')\nprint(f'Training target:\\t{y_train.shape}')\nprint(f'Testing target:\\t\\t{y_test.shape}')","e658cd50":"# cross validation\nk_folds = 3\nscoring = {'rmse': 'neg_root_mean_squared_error', 'r2': 'r2'}\n\ndef cv(model):\n    scores = cross_validate(model, x_train, y_train, cv=k_folds, scoring=scoring, return_train_score=True)\n    \n    scores['test_rmse'] = -scores['test_rmse']\n    scores['train_rmse'] = -scores['train_rmse']\n    scores['rmse'] = scores['test_rmse'].mean()\n    scores['r2'] = scores['test_r2'].mean()\n    \n    print(model)\n    print(f\"RMSE:\\t{scores['rmse']:.3f} +-{scores['test_rmse'].std():.3f}\")\n    print(f\"R2:\\t{scores['r2']:.3f} +-{scores['test_r2'].std():.3f}\")\n\n    return scores","44109806":"a = 0.0005\n\nlasso = Lasso(alpha=a)\n# ridge = KernelRidge(alpha=a) # ridge can't seem to handle this dataset well. Likely has to do with its SVD step and how it can explode the size of the input dataset\ne_net = ElasticNet(alpha=a)\ng_boost = GradientBoostingRegressor(loss='huber')\nxgb_regressor = xgb.XGBRegressor()\nlgb_regressor = lgb.LGBMRegressor()","421aa583":"# not following through with grid search as it doesn't seem necessary for this case\n# reg_cv = GridSearchCV(xgb_regressor, {\"colsample_bytree\":[1.0], \"min_child_weight\":[1.0,1.2], 'max_depth': [3,4,6], 'n_estimators': [500,1000]})\n# reg_cv.fit(x_train,y_train)","49e425bb":"scores = pd.DataFrame({'Model': [], 'RMSE': [], 'R2': []})","145303d4":"cv_score = cv(lasso)\nscores = scores.append({'Model': 'lasso', 'RMSE': cv_score['rmse'], 'R2': cv_score['r2']}, ignore_index=True)","7b3efed3":"cv_score = cv(e_net)\nscores = scores.append({'Model': 'e_net', 'RMSE': cv_score['rmse'], 'R2': cv_score['r2']}, ignore_index=True)","403744dd":"cv_score = cv(g_boost)\nscores = scores.append({'Model': 'g_boost', 'RMSE': cv_score['rmse'], 'R2': cv_score['r2']}, ignore_index=True)","2dab36dd":"cv_score = cv(lgb_regressor)\nscores = scores.append({'Model': 'lgb_regressor', 'RMSE': cv_score['rmse'], 'R2': cv_score['r2']}, ignore_index=True)","6d215619":"cv_score = cv(xgb_regressor)\nscores = scores.append({'Model': 'xgb_regressor', 'RMSE': cv_score['rmse'], 'R2': cv_score['r2']}, ignore_index=True)","38488a5b":"ax = scores.plot(x='Model', y=['RMSE', 'R2'], kind='bar', figsize=(14,10))\nax.set_title('Base Regression Models', fontsize=20)\nax.set(xlabel=None)\nplt.xticks(rotation=45)\nplt.show","d174ab3d":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","e1523e9c":"averaged_model = AveragingModels(models = (g_boost, lgb_regressor, xgb_regressor))\ncv_score = cv(averaged_model)\nscores = scores.append({'Model': 'AveragedModel', 'RMSE': cv_score['rmse'], 'R2': cv_score['r2']}, ignore_index=True)","f8d51b23":"ax = scores.plot(x='Model', y=['RMSE', 'R2'], kind='bar', figsize=(14,10))\nax.set_title('Base Regression Models', fontsize=20)\nax.set(xlabel=None)\nplt.xticks(rotation=45)\nplt.show","4cfbf251":"For the model building, different types of regression models will be tried for their accuracy, and later ensembled to improve on their results.\n\nThe method applied here is following the successful example given in this [notebook](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard#Modelling).","c6a86623":"Scale the numerical features to work on a 0-1 interval","329aa980":"## Handling Null Values","874eb448":"## Categorical Transformations\n\nOne hot encoding transforms all column types into numericals, so it's best to do this after all the numerical data has been processed.","ed742d27":"The distributions all look more normal after the transformations","e670e77c":"We're happy with the changes, so we apply them to the main dataset.","7b77b4e1":"## Categorical Data Viz","aab21f7e":"## Normalising\n\nSome numerical features don't follow a gaussian distribution; that should be remedied to better fit the regression model using a boxcox transformation","b4e92123":"Seeing closer the correlation between the house's price and the remaining features, we can see that none stand out as excessively related to it.\n\nIt would, however, be a good idea to manually inspect outliers in the communityAverage column.","5b6adbd8":"Now that all NaNs have been removed, we can also remove the floating points from some categorical labels","6a06bdbd":"We can more clearly conclude that the center has the most expensive zoning, with the west side tending to be more expensive in general.\n\nWe can also note that while the area outside the center quickly drops off in price all around, theres a zone to the east that maintains the same high price as other zones near the center. ","1344bc16":"### Columns with nulls\n- **DOM** (Days On Market), null might be 0\n- buildingType\n- elevator\n- fiveYearsProperty\n- subway\n- communityAverage\n\n### Notes\n\n- totalPrice: might be irrelevant, since there's price(perSquare) and square for each house\n- buildingType, renovationCondition, buildingStructure, elevator, subway, Cid, fiveYearProperty, district: is num and should be converted to an object\n- tradeTime, constructionTime: convert to dates\n- livingRoom -> bedRoom\n- drawingRoom -> livingRoom\n- nr of rooms needs to be a numerical\n- communityAverage: assuming as the average price in the community\n- lat, lng: might instead change these two into only one column that indicates the district code\n- floor:\n    - meaning the total number of floors in the building (\u5171x\u5c42)\n    - issue between \u5e95\uff0c\u4f4e\uff0c\u4e2d\uff0c\u9ad8\u697c\u5c42: lianjia only says the total amount of floors and if the house in question is a bottom, low, middle or high floor\n    - since the characters weren't parsed correctly when creating this dataset, the house floor value is lost and only the building max height is kept\n    - need to remove the unintelligible chinese characters\n    - need to remove the 32 rows with \u1e79","6cfdfe9b":"We can see that the number of living rooms, bedrooms and bathrooms is very related to the size of the house (squareMeters). Also, the community average and the house's price also share a correlation, as expected.\n\nThese values don't seem too correlated to justify abolishing ones in favour of other so far.","a43f398b":"Lets visualise the changes in price over the recorded time of the dataset 2010 -> 2018","bac456b0":"### Outliers","d18806e4":"We can see a few outliers in the top left of the first graph, the ones around <60000 communityAverage and >90000 pricePerSm.","a6b3a723":"Separate the train and test data after the preprocessing is done","bb736d30":"We now have a pretty good understanding of the data distribution, especially how most features are not gaussian and are very focused on just one or two values.","f17adb8d":"Good to convert the soldDate datetime column into 3 numerical columns representing the year, month, and day.\n\nAfter the analysis has been done, these columns will be turned into categorical data, since the regression model works best with gaussian distributions and it's unrealistic to have dates follow that kind of distribution.","3a249b04":"# Data Preprocessing","b1e2f746":"The visualisation is done, so we can convert the dates to their final form as categorical features.","95b408b9":"## Ensemble\n\nFor the ensemble model, each model's prediction will be averaged into a single stacked prediction. Overall, it barely fails to overperform the top performing model.","e6c3047f":"We can see that only 3 features have NaNs, and that DOM has around half of its values as null.\n\n- with regards to communityAverage, it can be easily filled with median values based on the communityAverage values on the same districts\n- buildingType could be filled by assigning the most popular building type for the correspending district that the building is on (mode)","dc03a94e":"## Imports","5eaf0d13":"# Modelling","b2d42d10":"# Objective\n\n- make an analysis of the data related to the beijing house prices\n- predict house prices through regression methods","bd8d4be7":"## Outliers with Isolation Forest\n\nApply the isolation forest algorithm to handle general outliers in all columns.","e508cff1":"## Understanding Spatial Data","f6c6cc50":"Boxcox only accepts values >0, so we must first move some columns' values to the right.\n\nThe code below fixes that issue, however since there's only cases where the minimum value is 0 we can simply +1 when passing it to boxcox.","e1377bd5":"notice the '\u03b4\u05aa' ... it must be removed","392165ae":"## Exploring Dates","cd57e304":"## Numerical Data Viz","0546ea3a":"## Top Features\n### Correlations","16cab5d9":"## Scalling","efd36c1b":"## Base Models","215f08a3":"# Data Analysis\n\n### Visualisation, Cleaning, Exploration, Feature Engineering","7a785b73":"Happy with the way the NaN values were filled without affecting the features' distribution too much.","b0233d5c":"After further analysis of the DOM null values, on cases such as [this](https:\/\/bj.lianjia.com\/ershoufang\/101100881269.html) the listing date on the website comes after the tradeTime of this entry (id=101100881269), thus we can assume that the DOM values were made to be calculated automatically with the listing date and the sold date values. Since the dataset is already a few years old, it's likely that lianjia's website structure changed in the meantime and made these DOM calculations impossible.\n\nWe can still attempt to secure hypothetical DOM mean values based on **district**, **renovationCondition**, and **buildingType**","c2c56d19":"We can see that for most houses this is true:\n- located in district 7\n- with either building structure 2 or 4\n- have easy access to a subway and have an elevator\n- the seller has the property for more than 5 years\n- likely that it's a building type 4"}}