{"cell_type":{"df18462d":"code","30100822":"code","c42529ba":"code","d70cc4d5":"code","ea0fdef8":"code","16cafe6b":"code","94aaab82":"code","3538aa25":"code","1812a492":"code","483004bd":"code","ec9557ec":"code","90b7b6e7":"code","c7655fd5":"code","a622c9e4":"code","cc657fd6":"code","26d8ac9a":"code","eb0dc013":"code","22ce098f":"code","ac99d6b7":"code","f4241370":"code","087ae7ed":"markdown","3a337e38":"markdown","4e5fc9be":"markdown","0e218c0b":"markdown","5bf34fb8":"markdown","68a3d8fc":"markdown","54338134":"markdown","9d71b46a":"markdown","406d5ceb":"markdown","1332de50":"markdown","970c731b":"markdown","d92b2b59":"markdown","61de4341":"markdown"},"source":{"df18462d":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport gc\nimport sys\n\nfrom tqdm.notebook import tqdm\ntqdm().pandas()\npd.set_option('display.max_colwidth', None)\n\n# Set seed for experiment reproducibility\nseed = 1024\ntf.random.set_seed(seed)\nnp.random.seed(seed)\n\ndef print_size(var):  \n    print('%.2fMB' % (sys.getsizeof(var)\/1024\/1024))","30100822":"GLOVE_FILE = 'glove.840B.300d\/glove.840B.300d.txt'","c42529ba":"!unzip -n \/kaggle\/input\/quora-insincere-questions-classification\/embeddings.zip {GLOVE_FILE} -d .","d70cc4d5":"def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\ndef get_lines_count(file_name): return sum(1 for _ in open(file_name, encoding=\"utf8\", errors='ignore'))\ndef load_vec(file_name): return dict(get_coefs(*o.split(\" \")) for o in tqdm(open(file_name, encoding=\"utf8\", errors='ignore'), total=get_lines_count(file_name)) if len(o) > 100)","ea0fdef8":"train_data = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/test.csv')","16cafe6b":"# train_data = train_data[0:100000]\n# test_data = test_data[0:10000]","94aaab82":"train_data.head()","3538aa25":"negative, positive = np.bincount(train_data['target'])\ntotal = negative + positive\nprint('total: {}    positive: {} ({:.2f}% of total)'.format(total, positive, 100 * positive \/ total))","1812a492":"import re\n\ndef clean_tag(text):\n    if '[math]' in text:\n        text = re.sub('\\[math\\].*?math\\]', '[formula]', text) #replacing with [formuala]\n\n    if 'http' in text or 'www' in text:\n        text = re.sub('(?:(?:https?|ftp):\\\/\\\/)?[\\w\/\\-?=%.]+\\.[\\w\/\\-?=%.]+', '[url]', text) #replacing with [url]\n    return text\n\ncontraction_mapping = {\"We'd\": \"We had\", \"That'd\": \"That had\", \"AREN'T\": \"Are not\", \"HADN'T\": \"Had not\", \"Could've\": \"Could have\", \"LeT's\": \"Let us\", \"How'll\": \"How will\", \"They'll\": \"They will\", \"DOESN'T\": \"Does not\", \"HE'S\": \"He has\", \"O'Clock\": \"Of the clock\", \"Who'll\": \"Who will\", \"What'S\": \"What is\", \"Ain't\": \"Am not\", \"WEREN'T\": \"Were not\", \"Y'all\": \"You all\", \"Y'ALL\": \"You all\", \"Here's\": \"Here is\", \"It'd\": \"It had\", \"Should've\": \"Should have\", \"I'M\": \"I am\", \"ISN'T\": \"Is not\", \"Would've\": \"Would have\", \"He'll\": \"He will\", \"DON'T\": \"Do not\", \"She'd\": \"She had\", \"WOULDN'T\": \"Would not\", \"She'll\": \"She will\", \"IT's\": \"It is\", \"There'd\": \"There had\", \"It'll\": \"It will\", \"You'll\": \"You will\", \"He'd\": \"He had\", \"What'll\": \"What will\", \"Ma'am\": \"Madam\", \"CAN'T\": \"Can not\", \"THAT'S\": \"That is\", \"You've\": \"You have\", \"She's\": \"She is\", \"Weren't\": \"Were not\", \"They've\": \"They have\", \"Couldn't\": \"Could not\", \"When's\": \"When is\", \"Haven't\": \"Have not\", \"We'll\": \"We will\", \"That's\": \"That is\", \"We're\": \"We are\", \"They're\": \"They' are\", \"You'd\": \"You would\", \"How'd\": \"How did\", \"What're\": \"What are\", \"Hasn't\": \"Has not\", \"Wasn't\": \"Was not\", \"Won't\": \"Will not\", \"There's\": \"There is\", \"Didn't\": \"Did not\", \"Doesn't\": \"Does not\", \"You're\": \"You are\", \"He's\": \"He is\", \"SO's\": \"So is\", \"We've\": \"We have\", \"Who's\": \"Who is\", \"Wouldn't\": \"Would not\", \"Why's\": \"Why is\", \"WHO's\": \"Who is\", \"Let's\": \"Let us\", \"How's\": \"How is\", \"Can't\": \"Can not\", \"Where's\": \"Where is\", \"They'd\": \"They had\", \"Don't\": \"Do not\", \"Shouldn't\":\"Should not\", \"Aren't\":\"Are not\", \"ain't\": \"is not\", \"What's\": \"What is\", \"It's\": \"It is\", \"Isn't\":\"Is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n\ndef clean_contractions(text):\n    specials = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    \n    text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(\" \")])\n    return text\n\npuncts = [\",\",\".\",'\"',\":\",\")\",\"(\",\"-\",\"!\",\"?\",\"|\",\";\",\"'\",\"$\",\"&\",\"\/\",\"[\",\"]\",\">\",\"%\",\"=\",\"#\",\"*\",\"+\",\"\\\\\",\"\u2022\",\"~\",\"@\",\"\u00a3\",\"\u00b7\",\"_\",\"{\",\"}\",\"\u00a9\",\"^\",\"\u00ae\",\"`\",\"<\",\"\u2192\",\"\u00b0\",\"\u20ac\",\"\u2122\",\"\u203a\",\"\u2665\",\"\u2190\",\"\u00d7\",\"\u00a7\",\"\u2033\",\"\u2032\",\"\u2588\",\"\u2026\",\"\u201c\",\"\u2605\",\"\u201d\",\"\u2013\",\"\u25cf\",\"\u25ba\",\"\u2212\",\"\u00a2\",\"\u00ac\",\"\u2591\",\"\u00a1\",\"\u00b6\",\"\u2191\",\"\u00b1\",\"\u00bf\",\"\u25be\",\"\u2550\",\"\u00a6\",\"\u2551\",\"\u2015\",\"\u00a5\",\"\u2593\",\"\u2014\",\"\u2039\",\"\u2500\",\"\u2592\",\"\uff1a\",\"\u2295\",\"\u25bc\",\"\u25aa\",\"\u2020\",\"\u25a0\",\"\u2019\",\"\u2580\",\"\u00a8\",\"\u2584\",\"\u266b\",\"\u2606\",\"\u00af\",\"\u2666\",\"\u00a4\",\"\u25b2\",\"\u00b8\",\"\u22c5\",\"\u2018\",\"\u221e\",\"\u2219\",\"\uff09\",\"\u2193\",\"\u3001\",\"\u2502\",\"\uff08\",\"\u00bb\",\"\uff0c\",\"\u266a\",\"\u2569\",\"\u255a\",\"\u30fb\",\"\u2566\",\"\u2563\",\"\u2554\",\"\u2557\",\"\u25ac\",\"\u2764\",\"\u2264\",\"\u2021\",\"\u221a\",\"\u25c4\",\"\u2501\",\"\u21d2\",\"\u25b6\",\"\u2265\",\"\u255d\",\"\u2661\",\"\u25ca\",\"\u3002\",\"\u2708\",\"\u2261\",\"\u263a\",\"\u2714\",\"\u21b5\",\"\u2248\",\"\u2713\",\"\u2663\",\"\u260e\",\"\u2103\",\"\u25e6\",\"\u2514\",\"\u201f\",\"\uff5e\",\"\uff01\",\"\u25cb\",\"\u25c6\",\"\u2116\",\"\u2660\",\"\u258c\",\"\u273f\",\"\u25b8\",\"\u2044\",\"\u25a1\",\"\u2756\",\"\u2726\",\"\uff0e\",\"\u00f7\",\"\uff5c\",\"\u2503\",\"\uff0f\",\"\uffe5\",\"\u2560\",\"\u21a9\",\"\u272d\",\"\u2590\",\"\u263c\",\"\u263b\",\"\u2510\",\"\u251c\",\"\u00ab\",\"\u223c\",\"\u250c\",\"\u2109\",\"\u262e\",\"\u0e3f\",\"\u2266\",\"\u266c\",\"\u2727\",\"\u3009\",\"\uff0d\",\"\u2302\",\"\u2716\",\"\uff65\",\"\u25d5\",\"\u203b\",\"\u2016\",\"\u25c0\",\"\u2030\",\"\\x97\",\"\u21ba\",\"\u2206\",\"\u2518\",\"\u252c\",\"\u256c\",\"\u060c\",\"\u2318\",\"\u2282\",\"\uff1e\",\"\u3008\",\"\u2399\",\"\uff1f\",\"\u2620\",\"\u21d0\",\"\u25ab\",\"\u2217\",\"\u2208\",\"\u2260\",\"\u2640\",\"\u2654\",\"\u02da\",\"\u2117\",\"\u2517\",\"\uff0a\",\"\u253c\",\"\u2740\",\"\uff06\",\"\u2229\",\"\u2642\",\"\u203f\",\"\u2211\",\"\u2023\",\"\u279c\",\"\u251b\",\"\u21d3\",\"\u262f\",\"\u2296\",\"\u2600\",\"\u2533\",\"\uff1b\",\"\u2207\",\"\u21d1\",\"\u2730\",\"\u25c7\",\"\u266f\",\"\u261e\",\"\u00b4\",\"\u2194\",\"\u250f\",\"\uff61\",\"\u25d8\",\"\u2202\",\"\u270c\",\"\u266d\",\"\u2523\",\"\u2534\",\"\u2513\",\"\u2728\",\"\\xa0\",\"\u02dc\",\"\u2765\",\"\u252b\",\"\u2120\",\"\u2712\",\"\uff3b\",\"\u222b\",\"\\x93\",\"\u2267\",\"\uff3d\",\"\\x94\",\"\u2200\",\"\u265b\",\"\\x96\",\"\u2228\",\"\u25ce\",\"\u21bb\",\"\u21e9\",\"\uff1c\",\"\u226b\",\"\u2729\",\"\u272a\",\"\u2655\",\"\u061f\",\"\u20a4\",\"\u261b\",\"\u256e\",\"\u240a\",\"\uff0b\",\"\u2508\",\"\uff05\",\"\u254b\",\"\u25bd\",\"\u21e8\",\"\u253b\",\"\u2297\",\"\uffe1\",\"\u0964\",\"\u2582\",\"\u272f\",\"\u2587\",\"\uff3f\",\"\u27a4\",\"\u271e\",\"\uff1d\",\"\u25b7\",\"\u25b3\",\"\u25d9\",\"\u2585\",\"\u271d\",\"\u2227\",\"\u2409\",\"\u262d\",\"\u250a\",\"\u256f\",\"\u263e\",\"\u2794\",\"\u2234\",\"\\x92\",\"\u2583\",\"\u21b3\",\"\uff3e\",\"\u05f3\",\"\u27a2\",\"\u256d\",\"\u27a1\",\"\uff20\",\"\u2299\",\"\u2622\",\"\u02dd\",\"\u220f\",\"\u201e\",\"\u2225\",\"\u275d\",\"\u2610\",\"\u2586\",\"\u2571\",\"\u22d9\",\"\u0e4f\",\"\u2601\",\"\u21d4\",\"\u2594\",\"\\x91\",\"\u279a\",\"\u25e1\",\"\u2570\",\"\\x85\",\"\u2662\",\"\u02d9\",\"\u06de\",\"\u2718\",\"\u272e\",\"\u2611\",\"\u22c6\",\"\u24d8\",\"\u2752\",\"\u2623\",\"\u2709\",\"\u230a\",\"\u27a0\",\"\u2223\",\"\u2751\",\"\u25e2\",\"\u24d2\",\"\\x80\",\"\u3012\",\"\u2215\",\"\u25ae\",\"\u29bf\",\"\u272b\",\"\u271a\",\"\u22ef\",\"\u2669\",\"\u2602\",\"\u275e\",\"\u2017\",\"\u0702\",\"\u261c\",\"\u203e\",\"\u271c\",\"\u2572\",\"\u2218\",\"\u27e9\",\"\uff3c\",\"\u27e8\",\"\u00b7\",\"\u2717\",\"\u265a\",\"\u2205\",\"\u24d4\",\"\u25e3\",\"\u0361\",\"\u201b\",\"\u2766\",\"\u25e0\",\"\u2704\",\"\u2744\",\"\u2203\",\"\u2423\",\"\u226a\",\"\uff62\",\"\u2245\",\"\u25ef\",\"\u263d\",\"\u220e\",\"\uff63\",\"\u2767\",\"\u0305\",\"\u24d0\",\"\u2198\",\"\u2693\",\"\u25a3\",\"\u02d8\",\"\u222a\",\"\u21e2\",\"\u270d\",\"\u22a5\",\"\uff03\",\"\u23af\",\"\u21a0\",\"\u06e9\",\"\u2630\",\"\u25e5\",\"\u2286\",\"\u273d\",\"\u26a1\",\"\u21aa\",\"\u2741\",\"\u2639\",\"\u25fc\",\"\u2603\",\"\u25e4\",\"\u274f\",\"\u24e2\",\"\u22b1\",\"\u279d\",\"\u0323\",\"\u2721\",\"\u2220\",\"\uff40\",\"\u25b4\",\"\u2524\",\"\u221d\",\"\u264f\",\"\u24d0\",\"\u270e\",\";\",\"\u2424\",\"\uff07\",\"\u2763\",\"\u2702\",\"\u2724\",\"\u24de\",\"\u262a\",\"\u2734\",\"\u2312\",\"\u02db\",\"\u2652\",\"\uff04\",\"\u2736\",\"\u25bb\",\"\u24d4\",\"\u25cc\",\"\u25c8\",\"\u275a\",\"\u2742\",\"\uffe6\",\"\u25c9\",\"\u255c\",\"\u0303\",\"\u2731\",\"\u2556\",\"\u2749\",\"\u24e1\",\"\u2197\",\"\u24e3\",\"\u267b\",\"\u27bd\",\"\u05c0\",\"\u2732\",\"\u272c\",\"\u2609\",\"\u2589\",\"\u2252\",\"\u2625\",\"\u2310\",\"\u2668\",\"\u2715\",\"\u24dd\",\"\u22b0\",\"\u2758\",\"\uff02\",\"\u21e7\",\"\u0335\",\"\u27aa\",\"\u2581\",\"\u258f\",\"\u2283\",\"\u24db\",\"\u201a\",\"\u2670\",\"\u0301\",\"\u270f\",\"\u23d1\",\"\u0336\",\"\u24e2\",\"\u2a7e\",\"\uffe0\",\"\u274d\",\"\u2243\",\"\u22f0\",\"\u264b\",\"\uff64\",\"\u0302\",\"\u274b\",\"\u2733\",\"\u24e4\",\"\u2564\",\"\u2595\",\"\u2323\",\"\u2738\",\"\u212e\",\"\u207a\",\"\u25a8\",\"\u2568\",\"\u24e5\",\"\u2648\",\"\u2743\",\"\u261d\",\"\u273b\",\"\u2287\",\"\u227b\",\"\u2658\",\"\u265e\",\"\u25c2\",\"\u271f\",\"\u2320\",\"\u2720\",\"\u261a\",\"\u2725\",\"\u274a\",\"\u24d2\",\"\u2308\",\"\u2745\",\"\u24e1\",\"\u2667\",\"\u24de\",\"\u25ad\",\"\u2771\",\"\u24e3\",\"\u221f\",\"\u2615\",\"\u267a\",\"\u2235\",\"\u235d\",\"\u24d1\",\"\u2735\",\"\u2723\",\"\u066d\",\"\u2646\",\"\u24d8\",\"\u2236\",\"\u269c\",\"\u25de\",\"\u0bcd\",\"\u2739\",\"\u27a5\",\"\u2195\",\"\u0333\",\"\u2237\",\"\u270b\",\"\u27a7\",\"\u220b\",\"\u033f\",\"\u0367\",\"\u2505\",\"\u2964\",\"\u2b06\",\"\u22f1\",\"\u2604\",\"\u2196\",\"\u22ee\",\"\u06d4\",\"\u264c\",\"\u24db\",\"\u2555\",\"\u2653\",\"\u276f\",\"\u264d\",\"\u258b\",\"\u273a\",\"\u2b50\",\"\u273e\",\"\u264a\",\"\u27a3\",\"\u25bf\",\"\u24d1\",\"\u2649\",\"\u23e0\",\"\u25fe\",\"\u25b9\",\"\u2a7d\",\"\u21a6\",\"\u2565\",\"\u2375\",\"\u230b\",\"\u0589\",\"\u27a8\",\"\u222e\",\"\u21e5\",\"\u24d7\",\"\u24d3\",\"\u207b\",\"\u239d\",\"\u2325\",\"\u2309\",\"\u25d4\",\"\u25d1\",\"\u273c\",\"\u264e\",\"\u2650\",\"\u256a\",\"\u229a\",\"\u2612\",\"\u21e4\",\"\u24dc\",\"\u23a0\",\"\u25d0\",\"\u26a0\",\"\u255e\",\"\u25d7\",\"\u2395\",\"\u24e8\",\"\u261f\",\"\u24df\",\"\u265f\",\"\u2748\",\"\u21ac\",\"\u24d3\",\"\u25fb\",\"\u266e\",\"\u2759\",\"\u2664\",\"\u2209\",\"\u061b\",\"\u2042\",\"\u24dd\",\"\u05be\",\"\u2651\",\"\u256b\",\"\u2553\",\"\u2573\",\"\u2b05\",\"\u2614\",\"\u2638\",\"\u2504\",\"\u2567\",\"\u05c3\",\"\u23a2\",\"\u2746\",\"\u22c4\",\"\u26ab\",\"\u030f\",\"\u260f\",\"\u279e\",\"\u0342\",\"\u2419\",\"\u24e4\",\"\u25df\",\"\u030a\",\"\u2690\",\"\u2719\",\"\u2199\",\"\u033e\",\"\u2118\",\"\u2737\",\"\u237a\",\"\u274c\",\"\u22a2\",\"\u25b5\",\"\u2705\",\"\u24d6\",\"\u2628\",\"\u25b0\",\"\u2561\",\"\u24dc\",\"\u2624\",\"\u223d\",\"\u2558\",\"\u02f9\",\"\u21a8\",\"\u2659\",\"\u2b07\",\"\u2671\",\"\u2321\",\"\u2800\",\"\u255b\",\"\u2755\",\"\u2509\",\"\u24df\",\"\u0300\",\"\u2656\",\"\u24da\",\"\u2506\",\"\u239c\",\"\u25dc\",\"\u26be\",\"\u2934\",\"\u2707\",\"\u255f\",\"\u239b\",\"\u2629\",\"\u27b2\",\"\u279f\",\"\u24e5\",\"\u24d7\",\"\u23dd\",\"\u25c3\",\"\u2562\",\"\u21af\",\"\u2706\",\"\u02c3\",\"\u2374\",\"\u2747\",\"\u26bd\",\"\u2552\",\"\u0338\",\"\u265c\",\"\u2613\",\"\u27b3\",\"\u21c4\",\"\u262c\",\"\u2691\",\"\u2710\",\"\u2303\",\"\u25c5\",\"\u25a2\",\"\u2750\",\"\u220a\",\"\u2608\",\"\u0965\",\"\u23ae\",\"\u25a9\",\"\u0bc1\",\"\u22b9\",\"\u2035\",\"\u2414\",\"\u260a\",\"\u27b8\",\"\u030c\",\"\u263f\",\"\u21c9\",\"\u22b3\",\"\u2559\",\"\u24e6\",\"\u21e3\",\"\uff5b\",\"\u0304\",\"\u219d\",\"\u239f\",\"\u258d\",\"\u2757\",\"\u05f4\",\"\u0384\",\"\u259e\",\"\u25c1\",\"\u26c4\",\"\u21dd\",\"\u23aa\",\"\u2641\",\"\u21e0\",\"\u2607\",\"\u270a\",\"\u0bbf\",\"\uff5d\",\"\u2b55\",\"\u2798\",\"\u2040\",\"\u2619\",\"\u275b\",\"\u2753\",\"\u27f2\",\"\u21c0\",\"\u2272\",\"\u24d5\",\"\u23a5\",\"\\u06dd\",\"\u0364\",\"\u208b\",\"\u0331\",\"\u030e\",\"\u265d\",\"\u2273\",\"\u2599\",\"\u27ad\",\"\u0700\",\"\u24d6\",\"\u21db\",\"\u258a\",\"\u21d7\",\"\u0337\",\"\u21f1\",\"\u2105\",\"\u24e7\",\"\u269b\",\"\u0310\",\"\u0315\",\"\u21cc\",\"\u2400\",\"\u224c\",\"\u24e6\",\"\u22a4\",\"\u0313\",\"\u2626\",\"\u24d5\",\"\u259c\",\"\u2799\",\"\u24e8\",\"\u2328\",\"\u25ee\",\"\u2637\",\"\u25cd\",\"\u24da\",\"\u2254\",\"\u23e9\",\"\u2373\",\"\u211e\",\"\u250b\",\"\u02fb\",\"\u259a\",\"\u227a\",\"\u0652\",\"\u259f\",\"\u27bb\",\"\u032a\",\"\u23ea\",\"\u0309\",\"\u239e\",\"\u2507\",\"\u235f\",\"\u21ea\",\"\u258e\",\"\u21e6\",\"\u241d\",\"\u2937\",\"\u2256\",\"\u27f6\",\"\u2657\",\"\u0334\",\"\u2644\",\"\u0368\",\"\u0308\",\"\u275c\",\"\u0321\",\"\u259b\",\"\u2701\",\"\u27a9\",\"\u0bbe\",\"\u02c2\",\"\u21a5\",\"\u23ce\",\"\u23b7\",\"\u0332\",\"\u2796\",\"\u21b2\",\"\u2a75\",\"\u0317\",\"\u2762\",\"\u224e\",\"\u2694\",\"\u21c7\",\"\u0311\",\"\u22bf\",\"\u0316\",\"\u260d\",\"\u27b9\",\"\u294a\",\"\u2041\",\"\u2722\"];\n\ndef clean_punct(x):\n    for punct in puncts:\n        if punct in x:\n            x = x.replace(punct, f' {punct} ')\n    return x\n\ndef data_cleaning(x):\n    x = clean_tag(x)\n    x = clean_contractions(x)\n    x = clean_punct(x)\n    return x\n\ntrain_data['preprocessed_question_text'] = train_data['question_text'].progress_map(lambda x: data_cleaning(x))\ntest_data['preprocessed_question_text'] = test_data['question_text'].progress_map(lambda x: data_cleaning(x))","483004bd":"from tensorflow import keras\nimport spacy\n\nnlp = spacy.load('en_core_web_lg', disable=['parser','ner','tagger'])\n\nvocab_freq = {}\nword2index = {}\nlemma_dict = {}\n\nsentences = pd.concat([train_data[\"preprocessed_question_text\"], test_data[\"preprocessed_question_text\"]])\ndocs = nlp.pipe(sentences, n_threads = 2)\nword_sequences = []\n\nfor doc in tqdm(docs, total=len(sentences)):\n    word_seq = []\n    for token in doc:\n        if token.is_punct or token.is_space:\n            continue\n        try:\n            vocab_freq[token.text] += 1\n        except KeyError:\n            vocab_freq[token.text] = 1\n        if token.text not in word2index:\n            word2index[token.text] = len(vocab_freq)\n            lemma_dict[token.text] = token.lemma_\n        word_seq.append(word2index[token.text])\n    word_sequences.append(word_seq)\n\nvocab_size = len(word2index) + 1\n\nprint('Found %s unique tokens.' % vocab_size)","ec9557ec":"MAX_SENTENCE_LENGTH = 100\n\nmax_text_len = len(max(word_sequences, key=len))\nprint(\"max text length in data: \", max_text_len)\n\npercentage = 100 * sum(1 for seq in word_sequences if len(seq) > MAX_SENTENCE_LENGTH)\/total\nprint(\"percentage of sentences that's length longer than max length > %d in data: %.4f%%\" % (MAX_SENTENCE_LENGTH, percentage))\n\nX_data = word_sequences[:len(train_data)]\nX_data = keras.preprocessing.sequence.pad_sequences(X_data, maxlen=MAX_SENTENCE_LENGTH)\nprint('Shape of data tensor:', X_data.shape)\n\nX_test_data = word_sequences[len(train_data):]\nX_test_data = keras.preprocessing.sequence.pad_sequences(X_test_data, maxlen=MAX_SENTENCE_LENGTH)\n\nY_data = train_data['target']\n\ndel word_sequences, docs, sentences, train_data\ngc.collect()","90b7b6e7":"from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\nfrom nltk.stem.lancaster import LancasterStemmer\n \nEMBEDDING_DIM = 300\n\nps = PorterStemmer()\nlc = LancasterStemmer()\nsb = SnowballStemmer('english')\nlm = WordNetLemmatizer() \n\ndef correction(word):\n    return list(candidates(word))[0]\n\ndef candidates(word):\n    \"Generate possible spelling corrections for word.\"\n    return (known([word]) or known(edits1(word)) or [word])\n\ndef known(words):\n    \"The subset of `words` that appear in the dictionary of WORDS.\"\n    return set(w for w in words if w in word2index)\n\ndef edits1(word):\n    \"All edits that are one edit away from `word`.\"\n    letters = 'abcdefghijklmnopqrstuvwxyz'\n    splits = [(word[:i], word[i:])        for i in range(len(word) + 1)]\n    deletes = [L + R[1:]                  for L, R in splits if R]  \n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]  \n    replaces = [L + c + R[1:]             for L, R in splits if R for c in letters]\n    inserts = [L + c + R                  for L, R in splits for c in letters]\n    return set(deletes + transposes + replaces + inserts)\n\ndef load_embedding(word2vec):\n    oov_count = 0\n    vocab_count = 0\n    embedding_weights = np.zeros((vocab_size, EMBEDDING_DIM))\n    unknown_vector = np.zeros((EMBEDDING_DIM,), dtype=np.float32) - 1.\n    unknown_words = {}\n\n    for key, i in tqdm(word2index.items()):\n        word = key\n        if word in word2vec:\n            vocab_count += vocab_freq[key]\n            embedding_weights[i] = word2vec[word]\n            continue\n        word = key.lower()         #Lower\n        if word in word2vec:\n            vocab_count += vocab_freq[key]\n            embedding_weights[i] = word2vec[word]\n            continue\n        word = key.upper()         #Upper\n        if word in word2vec:\n            vocab_count += vocab_freq[key]\n            embedding_weights[i] = word2vec[word]\n            continue\n        word = key.capitalize()    #Capitalize \n        if word in word2vec:\n            vocab_count += vocab_freq[key]\n            embedding_weights[i] = word2vec[word]\n            continue\n        word = ps.stem(key)        #PorterStemmer\n        if word in word2vec:\n            vocab_count += vocab_freq[key]\n            embedding_weights[i] = word2vec[word]\n            continue\n        word = lc.stem(key)        #LancasterStemmer\n        if word in word2vec:\n            vocab_count += vocab_freq[key]\n            embedding_weights[i] = word2vec[word]\n            continue\n        word = sb.stem(key)        #SnowballStemmer\n        if word in word2vec:\n            vocab_count += vocab_freq[key]\n            embedding_weights[i] = word2vec[word]\n            continue\n        word = lemma_dict[key]     #Lemmanization\n        if word in word2vec: \n            vocab_count += vocab_freq[key]\n            embedding_weights[i] = word2vec[word]\n            continue\n        if len(key) > 1:\n            word = correction(key)\n            if word in word2vec: \n                vocab_count += vocab_freq[key]\n                embedding_weights[i] = word2vec[word]\n                continue\n\n        try:\n            unknown_words[key] += 1\n        except KeyError:\n            unknown_words[key] = 1\n            \n        embedding_weights[i] = unknown_vector\n        oov_count += vocab_freq[key]\n\n    print('Top 10 Null word embeddings: ')\n    print(list(unknown_words.items())[:10])\n    print('\\n')\n    print('Null word embeddings: %d' % np.sum(np.sum(embedding_weights, axis=1) == -1 * EMBEDDING_DIM))\n    print('Null word embeddings percentage: %.2f%%' % (100 * oov_count \/ vocab_count))\n    \n    return embedding_weights","c7655fd5":"print('loading glove_vec')\nglove_vec = load_vec(GLOVE_FILE)\nglove_weights = load_embedding(glove_vec)\ndel glove_vec\ngc.collect()","a622c9e4":"import tensorflow as tf\n\nstrategy = None\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\n    print('Use TPU')\nexcept ValueError:\n    if len(tf.config.list_physical_devices('GPU')) > 0:\n        strategy = tf.distribute.MirroredStrategy()\n        print('Use GPU')\n    else:\n        strategy = tf.distribute.get_strategy()\n        print('Use CPU')","cc657fd6":"def f1_smart(y_true, y_pred):\n    args = np.argsort(y_pred)\n    tp = y_true.sum()\n    fs = (tp - np.cumsum(y_true[args[:-1]])) \/ np.arange(y_true.shape[0] + tp - 1, tp, -1)\n    res_idx = np.argmax(fs)\n    return 2 * fs[res_idx], (y_pred[args[res_idx]] + y_pred[args[res_idx + 1]]) \/ 2\n\ndef plot_history(history):\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'])\n    plt.show()\n\nweight_for_0 = (1 \/ negative) * (total) \/ 2.0 \nweight_for_1 = (1 \/ positive) * (total) \/ 2.0\n\nclass_weight = {0: weight_for_0, 1: weight_for_1}\n\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor='val_loss', \n    verbose=1,\n    patience=1,\n    mode='min',\n    restore_best_weights=True)\n\ncheckpoint = keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, mode='min')\n\nreduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1)","26d8ac9a":"from tensorflow.keras import layers\n\nclass TransformerLayer(layers.Layer):\n    def __init__(self, num_heads, hidden_size, dropout_rate=0.1, **kwargs):\n        super(TransformerLayer, self).__init__(**kwargs)\n        self.num_heads = num_heads\n        self.hidden_size = hidden_size\n        self.dropout_rate = dropout_rate\n        \n    def build(self, input_shape):\n        self.att = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=input_shape[2])\n        self.ffn = keras.Sequential(\n            [layers.Dense(self.hidden_size, activation=\"relu\"), layers.Dense(input_shape[2]),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(self.dropout_rate)\n        self.dropout2 = layers.Dropout(self.dropout_rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n    \n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            'num_heads': self.num_heads,\n            'hidden_size': self.hidden_size,\n            'dropout_rate': self.dropout_rate\n        })\n        return config\n    \nclass PositionEmbedding(layers.Layer):\n    def __init__(self, embeding_dim, **kwargs):\n        super(PositionEmbedding, self).__init__(**kwargs)\n        self.embeding_dim = embeding_dim\n        \n    def build(self, input_shape): \n        self.max_length = input_shape[-1]\n        self.position_embedding = layers.Embedding(input_dim=self.max_length, output_dim=self.embeding_dim, name='position_embedding')\n\n    def call(self, inputs):\n        positions = tf.range(start=0, limit=self.max_length, delta=1)\n        output = self.position_embedding(positions)\n        return output\n    \n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            'embeding_dim': self.embeding_dim\n        })\n        return config","eb0dc013":"import tensorflow_addons as tfa\nfrom keras import backend as K\nfrom keras.layers import *\nfrom keras.models import *\nfrom keras.initializers import Constant\n\ndef create_model(units):\n    output_bias = Constant(np.log([positive\/negative]))\n    \n    x_input = Input(shape=(MAX_SENTENCE_LENGTH,))\n    posistion_x = PositionEmbedding(EMBEDDING_DIM)(x_input)\n    x = Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SENTENCE_LENGTH, weights=[glove_weights], trainable=False)(x_input)\n    x = SpatialDropout1D(0.2)(x)\n    rnn = Bidirectional(GRU(units, return_sequences=True))(x)\n    att = TransformerLayer(2, 128)(posistion_x + x)\n    x = Concatenate()([rnn, att])\n    \n    x = GlobalAveragePooling1D()(x)\n    x_output = Dense(1, activation='sigmoid', bias_initializer=output_bias)(x)\n    \n    model = Model(inputs=x_input, outputs=x_output)\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n    \n    return model","22ce098f":"from sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom IPython.display import Image\nfrom keras.utils import plot_model\n\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n\nbest_f1score = 0\n\nwith strategy.scope():\n    model = create_model(64)\n    model.summary()\n    \n    plot_model(model, 'model.png', show_shapes=True, show_layer_names=True)\n    \n    for index, (train_index, valid_index) in enumerate(kfold.split(X_data, Y_data)):\n        if index > 1:\n            break\n        X_train, X_val, Y_train, Y_val = X_data[train_index], X_data[valid_index], Y_data[train_index], Y_data[valid_index]\n        history = model.fit(\n            X_train, Y_train, \n            epochs=5, \n            batch_size=128, \n            validation_data=(X_val, Y_val),\n            callbacks=[reduce_lr], \n            class_weight=class_weight\n        )\n        plot_history(history)\n        Y_pred = model.predict(X_val)\n        f1, threshold = f1_smart(Y_val.to_numpy(), np.squeeze(Y_pred))\n        best_f1score = max(best_f1score, f1)\n        print('Optimal F1: {:.4f} at threshold: {:.4f}\\n'.format(f1, threshold))\n        \nprint(f'{\"#\" * 30} best f1score: {best_f1score} {\"#\" * 30}')","ac99d6b7":"Image(\"model.png\")","f4241370":"# best_model = keras.models.load_model('best_model.h5', custom_objects={\"TransformerLayer\": TransformerLayer, \"PositionEmbedding\": PositionEmbedding})\n\nY_test = (model.predict(X_test_data) > threshold).astype(\"int32\")\n\nprint('Write results to submission.csv')\nsubmit_data = pd.DataFrame({'qid': test_data.qid, 'prediction': Y_test.reshape(-1)})\nsubmit_data.to_csv('submission.csv', index=False)\n\n!head submission.csv","087ae7ed":"It's not necessary using entire dataset to train if you just run it quickly.","3a337e38":"Fill the embedding layer's weights with word2vec we had loaded before.","4e5fc9be":"Word vectorizing. converting words into numbers so that can be fed into neural network.","0e218c0b":"Predict on the test dataset and write to the file named as submission.csv.","5bf34fb8":"Define the max sentence length. The length should be longer than most sentences in the dataset, otherwise it will lose a lot of useful features.","68a3d8fc":"References:\n\nhttps:\/\/www.kaggle.com\/ronaksvijay\/qiqc-nn","54338134":"Let's see how imbalanced the dataset is.","9d71b46a":"# Introduction\n**This notebook introduced how to solve an imbalanced text classification problem with LSTM networks and word embedding.**","406d5ceb":"Load the train and test dataset.","1332de50":"Import some required libraries.","970c731b":"Unzip the word2vec. It may take several minutes.","d92b2b59":"Let's see what's in the dataset and print the first 5 rows in train data.","61de4341":"Define model. I usually use metrics like f1-score, auc for binary classification model. There has three layers in the model. The first layer is Embedding layer that turns the X_train data(now that's the word indexes of vocabulary) into EMBEDDING_DIM dimensional vectors. The second layer is a bidirectinal LSTM that is well-suited to process data base on time-series. The last layer is output layer with a sigmoid activation function. I had set the [bias_initializer](http:\/\/https:\/\/www.tensorflow.org\/tutorials\/structured_data\/imbalanced_data#optional_set_the_correct_initial_bias) parameter for the output layer due to the imbalanced dataset."}}