{"cell_type":{"56ad6b8c":"code","b916a471":"code","dc23cec5":"code","91802e65":"code","19613785":"code","9e850c01":"code","a67235b1":"code","5aa81295":"code","9020bc6e":"code","3ae3b3e6":"code","a0c8b8a8":"code","1b18c217":"code","a826080f":"code","d2881ac4":"markdown","6def37b6":"markdown","121d183c":"markdown","e3bd408a":"markdown","c4c0e238":"markdown"},"source":{"56ad6b8c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers as L\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold\nimport joblib\n","b916a471":"data = pd.read_csv('..\/input\/song-popularity-prediction\/train.csv')\nprint(data.shape)\ndata.head()","dc23cec5":"test = pd.read_csv('..\/input\/song-popularity-prediction\/test.csv')\nX_test = test.drop(['id'], axis=1)","91802e65":"X = data.drop(['id', 'song_popularity'], axis=1)\ny = data['song_popularity']","19613785":"skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)","9e850c01":"class TabularNetworkConfig():\n    def __init__(\n        self, \n        target_feature_name, \n        target_feature_labels, \n        numeric_feature_names, \n        categorical_features_with_vocabulary,\n        num_outputs,\n        out_activation,\n        hidden_units, \n        dropout_rate,\n        embedding_dim,\n    ):\n        self.TARGET_FEATURE_NAME = target_feature_name\n        self.TARGET_FEATURE_LABELS = target_feature_labels\n        self.NUMERIC_FEATURE_NAMES = numeric_feature_names\n        self.CATEGORICAL_FEATURES_WITH_VOCABULARY = categorical_features_with_vocabulary\n        self.CATEGORICAL_FEATURE_NAMES = list(self.CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n        self.FEATURE_NAMES = self.NUMERIC_FEATURE_NAMES + self.CATEGORICAL_FEATURE_NAMES\n        self.NUM_OUT = num_outputs\n        self.OUT_ACTIVATION = out_activation\n        self.HIDDEN_UNITS = hidden_units\n        self.DROPOUT_RATE = dropout_rate\n        self.EMBEDDING_DIM = embedding_dim\n\nclass BaseTabularNetwork():\n    @staticmethod\n    def get_inputs(config):\n        return {\n        feature_name: L.Input(\n            name=feature_name,\n            shape=(),\n            dtype=(tf.float32 if feature_name in config.NUMERIC_FEATURE_NAMES else tf.string),\n        )\n        for feature_name in config.FEATURE_NAMES\n    }\n    \n    @staticmethod\n    def encode_inputs(inputs, config, use_embeddings=False, prefix=''):\n        encoded_features = []\n        for feature_name in inputs:\n            if feature_name in config.CATEGORICAL_FEATURE_NAMES:\n                vocabulary = config.CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n                lookup = L.StringLookup(\n                    vocabulary=vocabulary,\n                    mask_token=None,\n                    num_oov_indices=0,\n                    output_mode=\"int\" if use_embeddings else \"binary\",\n                    name=f\"{prefix}{feature_name}_lookup\"\n                )\n                if use_embeddings:\n                    encoded_feature = lookup(inputs[feature_name])\n                    embedding = L.Embedding(\n                        input_dim=len(vocabulary), output_dim=config.EMBEDDING_DIM,\n                        name=f\"{prefix}{feature_name}_embeddings\"\n                    )\n                    encoded_feature = embedding(encoded_feature)\n                else:\n                    encoded_feature = lookup(L.Reshape((1, ), name=f\"{prefix}{feature_name}_reshape\")(inputs[feature_name]))\n            else:\n                encoded_feature = L.Reshape((1, ), name=f\"{prefix}{feature_name}_reshape\")(inputs[feature_name])\n            encoded_features.append(encoded_feature)\n\n        all_features = L.Concatenate(name=f\"{prefix}inputs_concatenate\")(encoded_features)\n        return all_features","a67235b1":"class DeepTabularNetwork(BaseTabularNetwork):\n    @classmethod\n    def from_config(cls, name, config):\n        inputs = cls.get_inputs(config)\n        features = cls.encode_inputs(inputs, config)\n        \n        for i, units in enumerate(config.HIDDEN_UNITS):\n            features = L.Dense(units, name=f\"block_{i+1}_dense\")(features)\n            features = L.BatchNormalization(name=f\"block_{i+1}_b_norm\")(features)\n            features = L.ReLU(name=f\"block_{i+1}_relu\")(features)\n            features = L.Dropout(config.DROPOUT_RATE, name=f\"block_{i+1}_dropout\")(features)\n\n        outputs = L.Dense(units=config.NUM_OUT, activation=config.OUT_ACTIVATION, name=\"outputs\")(features)\n        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n        return model\n\nclass WideAndDeepTabularNetwork(BaseTabularNetwork):\n    @classmethod\n    def from_config(cls, name, config):\n        inputs = cls.get_inputs(config)\n        wide = cls.encode_inputs(inputs, config, prefix=\"wide_\")\n        deep = cls.encode_inputs(inputs, config, use_embeddings=True, prefix=\"deep_\")\n        \n        for i, units in enumerate(config.HIDDEN_UNITS):\n            deep = L.Dense(units, name=f\"block_{i+1}_dense\")(deep)\n            deep = L.BatchNormalization(name=f\"block_{i+1}_b_norm\")(deep)\n            deep = L.ReLU(name=f\"block_{i+1}_relu\")(deep)\n            deep = L.Dropout(config.DROPOUT_RATE, name=f\"block_{i+1}_dropout\")(deep)\n\n        merged = L.Concatenate(name=\"network_concatenate\")([wide, deep])\n        outputs = L.Dense(units=config.NUM_OUT, activation=config.OUT_ACTIVATION, name=\"outputs\")(merged)\n        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n        return model\n    \nclass DeepAndCrossTabularNetwork(BaseTabularNetwork):\n    @classmethod\n    def from_config(cls, name, config):\n        inputs = cls.get_inputs(config)\n        x0 = cls.encode_inputs(inputs, config, use_embeddings=True)\n        \n        cross = x0\n        for i in range(len(config.HIDDEN_UNITS)):\n            units = cross.shape[-1]\n            x = L.Dense(units, name=f\"cross_{i+1}_dense\")(cross)\n            cross = L.Lambda(lambda x: x[0] * x[1] + x[2], name=f\"cross_{i+1}\")((x0, x, cross))\n        cross = L.BatchNormalization(name=\"cross_b_norm\")(cross)\n        \n        deep = x0\n        for i, units in enumerate(config.HIDDEN_UNITS):\n            deep = L.Dense(units, name=f\"block_{i+1}_dense\")(deep)\n            deep = L.BatchNormalization(name=f\"block_{i+1}_b_norm\")(deep)\n            deep = L.ReLU(name=f\"block_{i+1}_relu\")(deep)\n            deep = L.Dropout(config.DROPOUT_RATE, name=f\"block_{i+1}_dropout\")(deep)\n        \n\n        merged = L.Concatenate(name=\"network_concatenate\")([cross, deep])\n        outputs = L.Dense(units=config.NUM_OUT, activation=config.OUT_ACTIVATION, name=\"outputs\")(merged)\n        model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n        return model    ","5aa81295":"model_config = TabularNetworkConfig(\n    target_feature_name=\"song_popularity\", \n    target_feature_labels=[\"0\", \"1\"], \n    numeric_feature_names=[\n        'song_duration_ms', 'acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness',\n        'speechiness', 'tempo', 'audio_valence'\n    ], \n    categorical_features_with_vocabulary={\n        'key': list(map(str, range(12))),\n        'audio_mode': [\"0\", \"1\"],\n        'time_signature': [\"2\", \"3\", \"4\", \"5\"]   \n    },\n    num_outputs=1,\n    out_activation=\"sigmoid\",\n    hidden_units=[64, 64],\n    dropout_rate=0.3,\n    embedding_dim=32\n)\n\nMAX_EPOCHS  = 250\n\nget_callbacks = lambda : [\n    keras.callbacks.EarlyStopping(min_delta=1e-4, patience=10, verbose=1, restore_best_weights=True),\n    keras.callbacks.ReduceLROnPlateau(patience=3, verbose=1)\n]","9020bc6e":"keras.utils.plot_model(\n    DeepTabularNetwork.from_config(\"deep_network\", model_config),\n    show_shapes=True, rankdir=\"LR\", to_file=\"deep_model.png\"\n)","3ae3b3e6":"keras.utils.plot_model(\n    WideAndDeepTabularNetwork.from_config(\"wide_deep_network\", model_config), \n    show_shapes=True, rankdir=\"LR\", to_file=\"wide_deep_model.png\"\n)","a0c8b8a8":"keras.utils.plot_model(\n    DeepAndCrossTabularNetwork.from_config(\"deep_cross_model\", model_config), \n    show_shapes=True, rankdir=\"LR\", to_file=\"deep_cross_model.png\"\n)","1b18c217":"preds_deep = []\npreds_wide_deep = []\npreds_deep_cross = []\n\nfor fold, (train_index, valid_index) in enumerate(skf.split(X, y)):\n    X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n\n    mean_imputer = Pipeline([\n        ('imputer', SimpleImputer(strategy='median')), \n        ('scaler', StandardScaler())\n    ]).fit(X_train[model_config.NUMERIC_FEATURE_NAMES])\n    mode_imputer = SimpleImputer(strategy='most_frequent').fit(X_train[model_config.CATEGORICAL_FEATURE_NAMES])\n    \n    X_train = pd.concat([\n        pd.DataFrame(\n            mean_imputer.transform(X_train[model_config.NUMERIC_FEATURE_NAMES]), \n            columns=model_config.NUMERIC_FEATURE_NAMES\n        ),\n        pd.DataFrame(\n            mode_imputer.transform(X_train[model_config.CATEGORICAL_FEATURE_NAMES]).astype(float).astype(int), \n            columns=model_config.CATEGORICAL_FEATURE_NAMES\n        ).astype(str),\n    ], axis=1)\n    X_valid = pd.concat([\n        pd.DataFrame(\n            mean_imputer.transform(X_valid[model_config.NUMERIC_FEATURE_NAMES]), \n            columns=model_config.NUMERIC_FEATURE_NAMES\n        ),\n        pd.DataFrame(\n            mode_imputer.transform(X_valid[model_config.CATEGORICAL_FEATURE_NAMES]).astype(float).astype(int), \n            columns=model_config.CATEGORICAL_FEATURE_NAMES\n        ).astype(str),\n    ], axis=1)\n    X_test_ = pd.concat([\n        pd.DataFrame(\n            mean_imputer.transform(X_test[model_config.NUMERIC_FEATURE_NAMES]), \n            columns=model_config.NUMERIC_FEATURE_NAMES\n        ),\n        pd.DataFrame(\n            mode_imputer.transform(X_test[model_config.CATEGORICAL_FEATURE_NAMES]).astype(float).astype(int), \n            columns=model_config.CATEGORICAL_FEATURE_NAMES\n        ).astype(str),\n    ], axis=1)\n        \n    data_train = tf.data.Dataset.from_tensor_slices((\n    {col: X_train[col].values.tolist() for col in model_config.FEATURE_NAMES}, \n        y_train.values.tolist()\n    )).batch(1024)\n    data_valid = tf.data.Dataset.from_tensor_slices((\n        {col: X_valid[col].values.tolist() for col in model_config.FEATURE_NAMES}, \n        y_valid.values.tolist()\n    )).batch(1024)\n    data_test = tf.data.Dataset.from_tensor_slices((\n        {col: X_test_[col].values.tolist() for col in model_config.FEATURE_NAMES}\n    )).batch(1024)\n    \n    deep_model = DeepTabularNetwork.from_config(\"deep_network\", model_config)\n    deep_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n    deep_model.fit(data_train, validation_data=data_valid, callbacks=get_callbacks(), epochs=MAX_EPOCHS)  \n    preds_deep.append(deep_model.predict(data_test))\n    \n    wide_deep_model = WideAndDeepTabularNetwork.from_config(\"wide_deep_network\", model_config)\n    wide_deep_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n    wide_deep_model.fit(data_train, validation_data=data_valid, callbacks=get_callbacks(), epochs=MAX_EPOCHS)\n    preds_wide_deep.append(wide_deep_model.predict(data_test))\n    \n    deep_cross_model = DeepAndCrossTabularNetwork.from_config(\"deep_cross_model\", model_config)\n    deep_cross_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n    deep_cross_model.fit(data_train, validation_data=data_valid, callbacks=get_callbacks(), epochs=MAX_EPOCHS) \n    preds_deep_cross.append(deep_cross_model.predict(data_test))","a826080f":"submissions = pd.read_csv('..\/input\/song-popularity-prediction\/sample_submission.csv')\nsubmissions['song_popularity'] = np.array(preds_deep).mean(axis=0)\nsubmissions.to_csv('preds_deep.csv', index=False)\n\nsubmissions['song_popularity'] = np.array(preds_wide_deep).mean(axis=0)\nsubmissions.to_csv('preds_wide_deep.csv', index=False)\n\nsubmissions['song_popularity'] = np.array(preds_deep_cross).mean(axis=0)\nsubmissions.to_csv('preds_deep_cross.csv', index=False)","d2881ac4":"# Deep, Wide and Cross Networks\n\nEven though deep learning has attained tremendous success on data domains such as images, audio and texts.\nGDBT still rule the domain of tabular data.\n\nIn this note we will discuss 3 Neural Network Architectures for tabular deep learning\n1. Deep Network: a multi-layer feed-forward network, where the categorical features are one-hot encoded.\n2. Wide and Deep Model ([Wide & Deep Learning: Better Together with TensorFlow](https:\/\/ai.googleblog.com\/2016\/06\/wide-deep-learning-better-together-with.html)):\n    * The wide part of the model a linear model, while the deep part of the model is a multi-layer feed-forward network.\n    * A sparse representation of the input features is used in the wide part of the model and the dense representation of the input features for the deep part of the model.\n3. Deep and Cross Model ([Deep & Cross Network for Ad Click Predictions](https:\/\/arxiv.org\/pdf\/1708.05123.pdf)):\n    * The deep part of this model is the same. \n    * The key idea of the cross part is to apply explicit feature crossing in an efficient way, where the degree of cross features grows with layer depth.","6def37b6":"# Submissions","121d183c":"# Model","e3bd408a":"# Data","c4c0e238":"# Training"}}