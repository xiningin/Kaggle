{"cell_type":{"25dd87ed":"code","5a3152f4":"code","e2f13120":"code","821cf59d":"code","182210b7":"code","94ce0f22":"code","f11282a8":"code","c32c3b0f":"markdown","19d08e5e":"markdown","0915a12e":"markdown","ac8538a8":"markdown","fa3a5d24":"markdown","41ad45ce":"markdown"},"source":{"25dd87ed":"## import libraries\nimport json, nltk, random\nimport numpy as np\nimport re\nfrom nltk import bigrams, trigrams\nfrom collections import Counter, defaultdict\nfrom nltk.tokenize import sent_tokenize\nfrom tqdm import tqdm","5a3152f4":"## ~ 1 MM articles https:\/\/en.wikipedia.org\/wiki\/List_of_Wikipedias\nMAX_ARTICLES_TO_READ = 100000\narticle_count = 0\nsentences = []\n\n\n\nfor article in tqdm(open('\/kaggle\/input\/wikipedia-ptbr\/ptwiki-latest.json', 'r'), total=MAX_ARTICLES_TO_READ):\n    article = json.loads(article)\n    article_count = article_count + 1\n    if article_count > MAX_ARTICLES_TO_READ: ## first 100,000 articles to fit in kaggle kernel's RAM\n        break\n    if 'section_texts' in article:\n        for section in article['section_texts']:\n            sentences.extend(sent_tokenize(section))","e2f13120":"random.sample(sentences,5)","821cf59d":"model = defaultdict(lambda: defaultdict(lambda: 0))\n\ndef space_punctuation(text):\n    s = re.sub('([.,!?()])', r' \\1 ', text)\n    s = re.sub('\\s{2,}', ' ', s)\n    return s\n\nfor sentence in tqdm(sentences):\n    text = space_punctuation(sentence)\n    for w1, w2, w3 in trigrams(text.split(), pad_right=True, pad_left=True):\n        model[(w1, w2)][w3] += 1\n","182210b7":"for w1_w2 in tqdm(model):\n    total_count = float(sum(model[w1_w2].values()))\n    for w3 in model[w1_w2]:\n        model[w1_w2][w3] \/= total_count","94ce0f22":"sorted(model['Clube', 'Atl\u00e9tico'].items(), key=lambda item: item[1], reverse=True)[:10]","f11282a8":"text = [\"A\", \"Petrobras\"]\nTOP_N = 10\nP_THRESHOLD = 0.01\n\nfor i in range(20):\n    sentence_finished = False\n\n    while not sentence_finished:\n        try:\n            top_n = sorted(model[tuple(text[-2:])].items(), key=lambda item: item[1], reverse=True)[:TOP_N]\n            next_word = random.sample(top_n,1)\n            text.append(next_word[0][0])\n            if tuple(text[-2:]) not in model or next_word[0][0] == '.' or next_word[0][1] < P_THRESHOLD:\n                sentence_finished = True\n        except:\n            sentence_finished = True\nprint (' '.join([t for t in text if t]))\n","c32c3b0f":"## Essay generator for language model\n\nHere we test our language model capabilities for essay generation. From a seed sentence (actually a 2 word start), we sample next word from the most likely next words (like the test above) and keep moving and going until we generate K sentences. \nWe Consider a sentence completed when there's no 2-gram into our model or the selected next word is a period or when the selected word has a small probability to occur.","19d08e5e":"Normalize counts by total count to get probabilities instead of absolute values","0915a12e":"Here we can test the top-10 most likely tokens (words) to appear after \"Clube Atl\u00e9tico\". ","ac8538a8":"# Language Model using Wikipedia in Portuguese\n\nThis notebook creates an 3-gram language model using 100,000 Portuguese Wikipedia articles and generates a random text based in this model. This is a rudimentary model compared to those [GPT-3](https:\/\/www.theguardian.com\/commentisfree\/2020\/sep\/08\/robot-wrote-this-article-gpt-3) like press release, but it runs into a laptop with 16 GB RAM :-)\n\nFor this toy example we aren't dealing with cased letters, punctuation, numbers, html tags or other usual pre processing steps for NLP.","fa3a5d24":"Here we parse each sentence with a sliding window of size 3 and compute each occurence of this 3-gram, but keeping it on an structutre of dictionary of dictionary, using the first 2 tokens (words) as a key and the third token for the next key.\n\nFor instance, if the 3-gram 'clube atletico mineiro' and 'clube altetico barcelona' appears once on the wikipedia sentences, our model will store:\n<pre>\n{('clube','atletico'):\n    {'mineiro':1},\n    {'barcelona':1}\n}\n<\/pre>","41ad45ce":"Here we extract sentences from the articles. "}}