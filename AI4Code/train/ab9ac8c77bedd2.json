{"cell_type":{"12f38729":"code","f6af360c":"code","26a12801":"code","da349813":"code","5488ad93":"code","8c2dcf99":"code","929aed60":"code","6e08ddbd":"code","ff0e814a":"code","a1373041":"code","23627707":"code","5ea2acf8":"code","aa0f9259":"code","e1c15d5a":"code","e2912f0e":"code","19b9ec3c":"code","716ebd73":"code","08a1dfe3":"code","5fca5cb1":"code","3cdb34a9":"code","1cdbf776":"code","d1acac8d":"code","a6559a93":"code","8b9b7b92":"code","0516255c":"code","2620a86a":"code","80c2c9e6":"code","fed62f08":"code","9e600d49":"code","582bf2cf":"code","9a17580b":"code","a5fb7fad":"code","5a4c8425":"code","4167507b":"code","85c3d9a3":"code","849b794f":"code","bb912608":"code","e19779e5":"code","f0eba8f9":"code","2bef773a":"code","8afc4e46":"code","ab37d371":"code","14fa34b1":"code","025838d0":"code","bacad01e":"code","147e2998":"code","e90ad063":"code","9677c072":"code","574d1e26":"code","5e3fdeca":"code","0d110e84":"code","cc32535a":"code","2bcd0163":"code","f38b84a4":"code","4ea2ece5":"code","12cff0d9":"code","bae25729":"code","07ea71a6":"code","d4893747":"code","3908c616":"code","e4075bd5":"code","7c9988d6":"code","c247bbc9":"code","bc01084f":"code","c2d1c7fb":"code","b6cccee3":"code","04846498":"code","fa8f1ded":"code","25714460":"code","45860f67":"code","f1fcad01":"code","408ec17c":"code","aaad3902":"code","14011557":"code","dc63d941":"code","8b850dda":"code","3951207f":"code","0d3df5f4":"code","7e644f99":"code","adbb5b31":"code","2ba59f30":"code","23ae0f5d":"code","83bf4cc3":"code","a0015be2":"code","048349c5":"code","dabfccf3":"code","f90f8cbd":"code","4485a5d7":"code","f033c8b7":"code","42cd67c5":"markdown","696f5baf":"markdown","eea3afb6":"markdown","3515abcd":"markdown","c052ee7b":"markdown","63a761ca":"markdown","1dced66f":"markdown","094b8b1a":"markdown","0165fe38":"markdown","e471ecc3":"markdown","6d4b26a7":"markdown","27bbb044":"markdown","fddcd991":"markdown","78e8dc00":"markdown","54205751":"markdown","f157a24d":"markdown","af0f8164":"markdown","bf62beb1":"markdown","270cbc57":"markdown","e44613f3":"markdown","00ea8b59":"markdown","beee6a18":"markdown","a015ecdc":"markdown","feff5ce2":"markdown","032dbf49":"markdown"},"source":{"12f38729":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","f6af360c":"Raw_train_df=pd.read_csv(\"\/kaggle\/input\/chronic-kidney-disease\/kidney_disease_train.csv\")\nRaw_test_df=pd.read_csv(\"\/kaggle\/input\/chronic-kidney-disease\/kidney_disease_test.csv\")","26a12801":"#Raw_train_df=pd.read_csv(\"kidney_disease_train.csv\")\n#Raw_test_df=pd.read_csv(\"kidney_disease_test.csv\")","da349813":"Raw_test_df.head()","5488ad93":"Raw_train_df.head()","8c2dcf99":"print(f'Shape of Train Data {Raw_train_df.shape}')\nprint(f'\\nShape of Test Data {Raw_test_df.shape}')","929aed60":"#one Extra Field is because of Train Flag,which separates the Train and Test Data\nRaw_train_df.shape\n","6e08ddbd":"Raw_train_df.describe().T","ff0e814a":"Raw_train_df.skew()","a1373041":"# From the Description of Data given in the Problem statement. Below are Numerical and Categorocal(nominal) columns\/variables\n# Excluding the Target variable \"Classification\"\ncat_var=['sg','al','su','rbc','pc','pcc','ba','htn','dm','cad','appet','pe','ane']\nnum_var=['id','age','bp','bgr','bu','sc','sod','pot','hemo','pcv','wc','rc']\nprint(f'Number of Categirical Variables including Label is {len(cat_var)}')\nprint(f'Number of Numerical Variables including Label is {len(num_var)}')\n\n","23627707":"Raw_train_df.dtypes","5ea2acf8":"Raw_train_df[Raw_train_df['wc'].map(lambda x:type(x)==str)].wc.value_counts()\n","aa0f9259":"Raw_train_df[Raw_train_df['rc'].map(lambda x:type(x)==str)].rc.value_counts()","e1c15d5a":"train_df=Raw_train_df.copy()\ntrain_df['wc']=pd.to_numeric(train_df['wc'],errors='coerce')\ntrain_df['rc']=pd.to_numeric(train_df['rc'], errors='coerce')\n","e2912f0e":"#Now All Numeric Columns\/variables are of type Intezers\/floats.\ntrain_df.dtypes","19b9ec3c":"#Missing Values For Each Column (Count in True Row). \ntrain_df.apply(lambda x: x.isna().value_counts()). T","716ebd73":"for feature in cat_var:\n  plt.figure(figsize = (5,5))\n  sns.countplot(x = feature, hue = 'classification', data = train_df, order = train_df[feature].value_counts().index)\n  plt.title(feature)\n","08a1dfe3":"# Checking for garbage or wrong values in Categorical variables.\nfor col in cat_var:\n    print(f\"Values counts for {col} are \\n {train_df[col].value_counts()}\")","5fca5cb1":"#Removing the Extra tab character\ntrain_df['dm']=train_df.dm.replace(\"\\tno\",\"no\")\ntrain_df['dm']=train_df.dm.replace(\"\\tyes\",\"yes\")\n\ntrain_df['cad']=train_df.cad.replace(\"\\tno\",\"no\")","3cdb34a9":"#Cross Checking the Replacement\nprint(train_df.dm.value_counts())\nprint(train_df.cad.value_counts())","1cdbf776":"#Missing Values For Each Column (Count in True Column). \n\ntrain_df.apply(lambda x: x.isna().value_counts()). T","d1acac8d":"for col in num_var:\n    print(f'Imputing for {col} with {train_df[col].median()}')\n    train_df[col]=train_df[col].fillna(train_df[col].median())\n","a6559a93":"for col in cat_var:\n    print(f'Imputing for {col} with {train_df[col].mode()[0]}')\n    train_df[col]=train_df[col].fillna(train_df[col].mode()[0])","8b9b7b92":"#Cross checking for any more Missing Values For Each Column (Count in True Column). \n\ntrain_df.apply(lambda x: x.isna().value_counts()). T","0516255c":"train_num_df=train_df[num_var].copy()","2620a86a":"train_num_df.head()","80c2c9e6":"train_num_df.shape","fed62f08":"from scipy.stats import zscore\ntrain_num_zscore=train_num_df.apply(zscore)","9e600d49":"train_num_zscore[~(np.abs(train_num_zscore) < 3).all(axis=1)].shape","582bf2cf":"\ntrain_num_zscore[~(np.abs(train_num_zscore) < 3).all(axis=1)]","9a17580b":"(~(np.abs(train_num_zscore) < 3)).sum(axis=0)","a5fb7fad":"from scipy import stats\nfor col in num_var:\n    print(f'Imputing for {col} with {train_df[col].median()}')\n    train_df.loc[(np.abs(stats.zscore(train_num_df[col])) >= 3), col] = train_df[col].median()\n","5a4c8425":"train_df.head()","4167507b":"train_df.dtypes","85c3d9a3":"cat_var","849b794f":"# Creating a Dictionary to Replace the Non Numerical values with Numeric for the Categorical variables identified above.\ncat_nom_dict = {\"rbc\":     {\"normal\": 1, \"abnormal\": 0},\n                \"pc\":     {\"normal\": 1, \"abnormal\": 0},\n                \"pcc\":     {\"present\": 1, \"notpresent\": 0},\n                \"ba\":     {\"present\": 1, \"notpresent\": 0},\n                \"htn\":     {\"yes\": 1, \"no\": 0},\n                \"dm\":     {\"yes\": 1, \"no\": 0},\n                \"cad\":     {\"yes\": 1, \"no\": 0},\n                \"pe\":     {\"yes\": 1, \"no\": 0},\n                \"ane\":     {\"yes\": 1, \"no\": 0},\n                \"appet\":     {\"good\": 1, \"poor\": 0},\n                \"classification\":     {\"ckd\": 1, \"notckd\": 0} \n               }","bb912608":"cat_nom_dict","e19779e5":"train_df.replace(cat_nom_dict, inplace=True)","f0eba8f9":"train_df.head()","2bef773a":"train_df.dtypes","8afc4e46":"train_df.drop('id',axis=1,inplace=True)","ab37d371":"train_df.head()","14fa34b1":"# Plotting Heat map with Co Relation numbers between each Features. Degault will be Pearson Co relation co efficient\nplt.figure(figsize=(25, 25))\nTrain_df_corr = train_df.corr()\nsns.heatmap(Train_df_corr, \n            xticklabels = Train_df_corr.columns.values,\n            yticklabels = Train_df_corr.columns.values,\n            annot = True);","025838d0":"drop_feat=['hemo','pcv','htn']\ntrain_df.drop(drop_feat,axis=1,inplace=True)","bacad01e":"train_df.head()","147e2998":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler","e90ad063":"y = train_df['classification']\nX = train_df.drop(['classification'], axis = 1)","9677c072":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)","574d1e26":"print('X train shape: ', X_train.shape)\nprint('X test shape: ', X_test.shape)\nprint('y train shape: ', y_train.shape)\nprint('y test shape: ', y_test.shape)","5e3fdeca":"norm = MinMaxScaler().fit(X_train)","0d110e84":"X_train_norm = norm.transform(X_train)","cc32535a":"type(X_train_norm)","2bcd0163":"X_train_norm","f38b84a4":"from sklearn.linear_model import LogisticRegression","4ea2ece5":"clf = LogisticRegression(random_state=0,max_iter=500).fit(X_train, y_train)\ny_pred = clf.predict(X_test)","12cff0d9":"y_pred","bae25729":"from sklearn.metrics import classification_report, confusion_matrix","07ea71a6":"print(confusion_matrix(y_test, y_pred))","d4893747":"print(classification_report( y_test, y_pred))","3908c616":"param_grid = [    \n    {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n    'C' : np.logspace(-4, 4, 20),\n    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n    'max_iter' : [100, 1000,2500, 5000]\n    }\n]","e4075bd5":"from sklearn.model_selection import RandomizedSearchCV","7c9988d6":"clf_random = RandomizedSearchCV(clf, param_distributions = param_grid, cv = 3, verbose=True, n_jobs=-1)","c247bbc9":"best_clf_random = clf_random.fit(X_train,y_train)","bc01084f":"best_clf_random.best_estimator_","c2d1c7fb":"clf = LogisticRegression(C=4.281332398719396, class_weight=None, dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=100, multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False).fit(X_train, y_train)\ny_pred = clf.predict(X_test)","b6cccee3":"print(confusion_matrix(y_test, y_pred))","04846498":"print(classification_report( y_test, y_pred))","fa8f1ded":"param_grid_cv = [    \n    {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n    'C' : np.logspace(3, 4, 20),\n    'solver' : ['lbfgs','newton-cg','liblinear','sag','saga'],\n    'max_iter' : [80, 100, 120, 150]\n    }\n]","25714460":"from sklearn.model_selection import GridSearchCV","45860f67":"clf_grid = GridSearchCV(clf, param_grid= param_grid_cv, cv = 3, verbose=True, n_jobs=-1)\nbest_clf_grid = clf_grid.fit(X_train, y_train)","f1fcad01":"best_clf_grid.best_estimator_","408ec17c":"y_pred = best_clf_grid.best_estimator_.predict(X_test)","aaad3902":"print(confusion_matrix(y_test, y_pred))","14011557":"print(classification_report(y_test, y_pred))","dc63d941":"from sklearn.tree import DecisionTreeClassifier\ndt_model = DecisionTreeClassifier(random_state=23)\ndt_model.fit(X_train, y_train)\ny_pred_dt = dt_model.predict(X_test)","8b850dda":"print(confusion_matrix(y_test, y_pred_dt))","3951207f":"print(classification_report(y_test, y_pred_dt))","0d3df5f4":"param_grid_random = [    \n    {'splitter' : ['best', 'random'],\n     'max_depth' : np.linspace(1, 32, 32, endpoint=True),\n     'min_samples_split' : np.linspace(1, 10, 10, endpoint=True),\n     'min_samples_leaf' : np.linspace(0.1, 0.5, 10, endpoint=True),\n     'max_features' : list(range(1,X_train.shape[1])),\n    }\n]","7e644f99":"dt_model_random = RandomizedSearchCV(dt_model, param_distributions = param_grid_random, cv = 3, verbose=True, n_jobs=-1)\ndt_model_random.fit(X_train, y_train)","adbb5b31":"y_pred = dt_model_random.best_estimator_.predict(X_test)","2ba59f30":"print(confusion_matrix(y_test, y_pred))","23ae0f5d":"print(classification_report(y_test, y_pred))","83bf4cc3":"from sklearn.ensemble import RandomForestClassifier","a0015be2":"# Running Random Forest Regressor Model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nrf = RandomForestRegressor(n_estimators = 51, random_state = 1)\nmodel = rf.fit(X_train, y_train)\nnp.sqrt(mean_squared_error(y_train, model.predict(X_train)))\nr2_score(y_train, model.predict(X_train))","048349c5":"rf = RandomForestClassifier(n_estimators = 100, random_state = 1)\nrf.fit(X_train, y_train)\ny_train = rf.predict(X_train)\ny_pred = rf.predict(X_test)\nprint('\\n')\nprint('Classification Report for Train Set\\n')\nprint(classification_report(y_train, y_train))\nprint('Classification Report for Test Set\\n')\nprint(classification_report(y_test, y_pred))","dabfccf3":"from sklearn.svm import SVC\n\nX_train, X_test, y_train, y_test = train_test_split(Raw_train_df.drop(['classification'], axis = 1),\n    Raw_train_df['classification'],\n    test_size=0.3,\n    random_state=0)\nX_train.shape, X_test.shape","f90f8cbd":"classifier = SVC(kernel = 'linear', random_state = 1)\nclassifier.fit(X_train, y_train)\ny_train_pred = classifier.predict(X_train)\ny_test_pred = classifier.predict(X_test)\nprint('\\n')\nprint('Classification Report for Train Set\\n')\nprint(classification_report(y_train, y_train_pred))\nprint('Classification Report for Test Set\\n')\nprint(classification_report(y_test, y_test_pred))\n","4485a5d7":"submission_df = pd.DataFrame({'PatientId' :['id'], 'class': y_pred.tolist()})","f033c8b7":"submission_df.to_csv(\"submission.csv\", header = True, index= False)","42cd67c5":"Considering absolute Co efficients greater than 0.6\n\n1) pc,hemo are highly co related with 0.82 \n\n2) rc,hemo are highly co related with 0.67\n\n3) rc,pcv are highly co related with 0.72\n\n4) dm,htn are highly co related with 0.64\n\n5) hemo,htn are highly co related with 0.61\n\nWe can drop hemo,pcv,htm Features.\n","696f5baf":"using grid search we have improved our model ","eea3afb6":"here we can see that our recall has improved from 90 to 92 which is considerably good ","3515abcd":"From The above Describe Function(for Numerical Variables as detected by pandas. there might be some other numerical variables which are detected as Objects due to some wrong entries in data.), we are able to find below observations.\n\n1) Ideally there should be total 280 count for each Column, From \"Count\" field, we could see the missing values.\n\n2) Id, seems to be a just a numerical identifier for each patient, which is starting from 0 and ending with 399 . So this can be removed, as it doesnt show any significance.\n\n3) Age Columns Varies from 2 to 90 years. Also, the Mean is Slightly less than the median.The skewness also points out the same, that its slightly skewed towards left.\n\n4) BP Column Varies from 50 to 180. Also, the 2nd Quartile and 3rd Quartile are same. This means,that there are many entries with bp value as 70 ( at least 25% of data). The mean is more than Median,which points out \nthat the distribution is right skewed with skewness > 2.\n\n5)sg,al,su are Categorical Variables(Nominal, as they dont have any order importance)\n\n6) bgr ranges from 22 to 490 and its right skewed with skewness 1.96\n\n7)bu ranges from 10 to 391 and the mean is greater than median , so its right skewed with skewness 2.95.\n\n8)sc ranges from 0.4 to 76 and heavily right skewed with skewness of 8.28.\n\n9)sod seems to be left skewed with skewness of -7.1, but there are many number of missing values.\n\n10) pot is  right skewed (skewness 9.86) and there are many missing values.\n\n11) hemo is slightly left skewed.\n\n12) pcv is slightly left skewed.","c052ee7b":"1) For \"sg\" Values less than or equal to 1.015, there are only ckd cases ( that means,  lower the sg values, more chances of Chronic kidney disease)\n\n2) For \"ai\/\"su\" Values greater than or equal to 1, there are only ckd cases ( that means,  higher the ai\/su values, more chances of Chronic kidney disease)\n\n3) for \"rbc\/pc\", if they are abnormal, than its CKD. \n\n4) for \"pcc\/ba\", if they are present, than its CKD. \n\n5) There seems to be extra charcters ( spaces) for columns \"dm\/cad\". We need to trim the values in these columns and correct them.\n\n6)  for \"htn\/dm\/cad\/pe\/ane\", if they are yes, than its CKD. \n\n7) for \"appt\", if its good, than its CKD. ","63a761ca":"There are Many columns with missing values, with 'rbc' with maximum of 107. we need to do proper imputation during pre processing as we cannot ignore the columns with more number of missing numbers.","1dced66f":"The Id Column is just an identifier . So We can drop it.","094b8b1a":"Since the size of the train data is small, with just 280 rows and we have many such columns with missing values. We cannot go with Deleting the missing rows. \nTry to impute the missing values with median for continous variables and Mode for Categorical variables.","0165fe38":"In Earlier section, we have removed the garbage data in case of numerical columns, where we had \"?\", \"\\t\". For these we have replaced with Nan. \n\nSo We need to treat the data in case of Categorical variables. In the EDA, we have observed that \"dm\/cad\" . columns has extra spaces. lets fix them","e471ecc3":"If we observe from the type of the for Numerical variables, ('wc','rc') are identified as objects by pandas instead of intezers\/floats. lets explore the reason for this.","6d4b26a7":"#### Analysing the Distribution of Categorical variables with respect to Traget variable.","27bbb044":"from this output we can infer that the number of false negatives are more which we should try to reduce.","fddcd991":"Based on Zscore Analysis, there are around 25 Columns with outliers. \n\nAlso Count of number of outliers for each Column are mentioned above.\n\nbgr\/bu columns has higest number of outliers(7)","78e8dc00":"using these hyperparameters we try to improve our model.","54205751":"this model gives us considerably good accuracy and the value of recall for 0 is 97 whereas for 1 we have 90 we will try to improve this . ","f157a24d":"###  Missing Values Imputation","af0f8164":"Train Data has 280 Rows and 26 Columns ( \"classification\" Column  is the label)\n\nTest Data has 120 Rows and 25 Columns ( without label Column, which needs to be predicted)","bf62beb1":"## Model Building","270cbc57":"### Outlier Treatment","e44613f3":"our model has improved little from 5 false postives to 4 ","00ea8b59":"Description\nThe data was taken over a 2-month period in India with 25 features ( eg, red blood cell count, white blood cell count, etc). The target is the 'classification', which is either 'ckd' or 'notckd' - ckd=chronic kidney disease. Use machine learning techniques to predict if a patient is suffering from a chronic kidney disease or not.\n\nCredit goes to Mansoor Iqbal (https:\/\/www.kaggle.com\/mansoordaku) from where the dataset has been collected. For the purpose of creating a challenge, certain modifications have been done to the dataset.\n\nOriginal dataset can be acquired from the link Chronic KIdney Disease (https:\/\/www.kaggle.com\/mansoordaku\/ckdisease)\n\n## ** Feature Details**\nAttribute Information:\n\nWe use 25 + class = 26 ( 12 numeric ,14 nominal)\n\nId(numerical) - Patient Id\nAge(numerical) - age in years\n\nBlood Pressure(numerical) - bp in mm\/Hg\n\nSpecific Gravity(nominal) - sg - (1.005,1.010,1.015,1.020,1.025)\n\nAlbumin(nominal) - al - (0,1,2,3,4,5)\n\nSugar(nominal) - su - (0,1,2,3,4,5)\n\nRed Blood Cells(nominal) - rbc - (normal,abnormal)\n\nPus Cell (nominal) - pc - (normal,abnormal)\n\nPus Cell clumps(nominal) - pcc - (present,notpresent)\n\nBacteria(nominal) - ba - (present,notpresent)\n\nBlood Glucose Random(numerical) - bgr in mgs\/dl\n\nBlood Urea(numerical) -bu in mgs\/dl\n\nSerum Creatinine(numerical) - sc in mgs\/dl\n\nSodium(numerical) - sod in mEq\/L\n\nPotassium(numerical) - pot in mEq\/L\n\nHemoglobin(numerical) - hemo in gms\n\nPacked Cell Volume(numerical)\n\nWhite Blood Cell Count(numerical) - wc in cells\/cumm\n\nRed Blood Cell Count(numerical) - rc in millions\/cmm\n\nHypertension(nominal) - htn - (yes,no)\n\nDiabetes Mellitus(nominal) - dm - (yes,no)\n\nCoronary Artery Disease(nominal) - cad - (yes,no)\n\nAppetite(nominal) - appet - (good,poor)\n\nPedal Edema(nominal) - pe - (yes,no)\n\nAnemia(nominal) - ane - (yes,no)\n\nClass (nominal)- class - (ckd,notckd)\n\nAcknowledgements\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Chronic_Kidney_Disease\n\n\n\n\n","beee6a18":"Out of the 14 Categorical Variables, 3 of them ( sg,al,su) are ordinal categorical Variables. These are already in Numeric Form. \n\nRemaining 11 Categorical Variables (rbc,pc,pcc,ba,htn,dm,cad,appet,pe,ane,classification) are nominal. They are only has 2 values each ( i.e normal\/abnormal, yes\/no etc..). So We can convert them to ","a015ecdc":"## Dimensionality Reduction","feff5ce2":"#### Converting Non Numeric Categorical Variables into Numeric.","032dbf49":"However for column 'rc'\/'wc;, there is garbage characters (\"\\t\"  and \" ? \") character in it. So we need to replace it with Nan before type casting them to numeric"}}