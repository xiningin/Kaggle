{"cell_type":{"002e1f5a":"code","b41982de":"code","a3bdd78d":"code","88f32a68":"code","f30fdcf1":"code","fe05b433":"code","7eab90bc":"code","c1470d99":"markdown","43ee0815":"markdown","b5e67896":"markdown","dbb88211":"markdown","7fb0ff50":"markdown","3e4270a4":"markdown"},"source":{"002e1f5a":"import numpy as np\nimport pandas as pd\nimport json\nfrom sklearn import preprocessing\nfrom datasketch import MinHash, MinHashLSHForest","b41982de":"with open('..\/input\/train.json') as file:\n    training_data = json.load(file)\n\nlsh = MinHashLSHForest(num_perm=128)\ntrain, y_train_str = {}, {}\nfor entry in training_data:\n    # Compute min-hash and add to LSH\n    min_hash = MinHash(num_perm=128)\n    for e in entry['ingredients']:\n        min_hash.update(e.encode('utf-8'))\n    lsh.add(entry['id'], min_hash)\n    \n    # We may want to use the raw data later on\n    train[entry['id']] = entry['ingredients']\n    y_train_str[entry['id']] = entry['cuisine']\n\nlsh.index()","a3bdd78d":"label_encoder = preprocessing.LabelEncoder()\nlabel_encoder.fit(list(y_train_str.values()))\ny_train = {eid: label_encoder.transform([y_train_str[eid]])[0] for eid in y_train_str}","88f32a68":"def jaccard_similarity(x, y):\n    return len(set(x) & set(y)) \/ len(set(x) | set(y))","f30fdcf1":"N_NEAREST_NEIGHBORS = 100","fe05b433":"with open('..\/input\/test.json') as file:\n    test_data = json.load(file)\n    \npredictions = {}\nfor entry in test_data:\n    # Create the min-hash for the given item and find its nearest neighbors\n    min_hash = MinHash(num_perm=128)\n    for e in entry['ingredients']:\n        min_hash.update(e.encode('utf-8'))\n        \n    nearest_neighbors = lsh.query(min_hash, N_NEAREST_NEIGHBORS)\n    similarities, cuisines, cuisines_str = [], [], []\n    for nn in nearest_neighbors:\n        similarities.append(jaccard_similarity(train[nn], entry['ingredients']))\n        cuisines.append(y_train[nn])\n        cuisines_str.append(y_train_str[nn])\n    \n    weighted_votes = np.bincount(cuisines, weights=similarities)\n    prediction_idx = np.argmax(weighted_votes)\n    prediction = label_encoder.classes_[prediction_idx]\n    \n    predictions[entry['id']] = prediction","7eab90bc":"predictions_df = pd.DataFrame(list(predictions.items()), columns=['id', 'cuisine'])\npredictions_df.to_csv('predictions.csv', index=False)","c1470d99":"To predict on the test set, the K nearest neighbors are found. Each neighbor then votes for its cuisine with the Jaccard similarity to the test point. The cusine with the most votes is assigned to the test point.","43ee0815":"In order to make working with numpy simpler, we convert the target labels (cuisine) to integers.","b5e67896":"We will use an arbitrary number of neighbors for demonstrative purposes.","dbb88211":"# A nearest neighbor based approach\n\nI've noticed a lot of approaches take a very NLP approach to this problem, however, this really doesn't seem to me like an NLP problem (I haven't looked into the data too much, so this may be incorrect). For each recipie, we are given a set of ingredients used and then we have to determine which cuisine the recipie belongs to. Given that we are given sets of ingredients, which are typically a word or two, there is no real structure here that NLP algorithms can take.\n\nGiven that we're working with sets, a natural choice of metric is the Jaccard similarity, combined with a simple k-nearest neighbors classifier. This, to me, seems much simpler than fancy GloVe or word2vec embeddings, combined with neural nets or SVMs.\n\nIn this notebook, I implement a simple weighted voting scheme with k-nearest neighbors using the Jaccard similarity index. I use an arbitrary LSH approximate nearest neighbor library because I did not want to wait for the exact nearest neighbors to be computed.\n\nFirst, we have to build up the LSH index. We will use this to query the nearest neighbor for every test point.","7fb0ff50":"We will use pandas to write our predictions to a csv.","3e4270a4":"This approach doesn't boast particularly high accuracy, but given its simplicity, it doesn't do too poorly either. I merely tried to demonstrate that there is no need to instantly jump to the most sophisticated methods like neural nets before exploring simpler options.\n\nThis approach could probably be improved upon as well to achieve better results e.g. the LSH could likely be tuned for higher recall, we could use a weighted min-hashes using tf-idf weights, so that rarer words would have a larger impact in nearest neighbor search.\n\nAs stated, the end goal of this notebook is not to achieve the best results, but to show that somewhat decent results can be achieved with minimal effort and using a very conceptually and programatically simple technique."}}