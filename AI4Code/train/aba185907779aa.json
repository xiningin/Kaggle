{"cell_type":{"3d76b26f":"code","0943d640":"code","9917fc0d":"code","67eb93a7":"code","95b53414":"code","8f0bdb55":"code","5d1ae658":"code","818d9590":"code","8bd124a6":"code","38da74dd":"code","2bca818e":"code","4a2248de":"code","fb48fef5":"code","42df5b7e":"code","73048808":"code","b4e2380b":"code","ef572fa7":"code","8133d060":"code","0438c3dc":"code","7379ae7a":"code","45e8a9c4":"code","ed0e3711":"code","c1cff79d":"code","eb532818":"code","22e68f9a":"code","82b7e54d":"code","8964711d":"code","fcf8126c":"code","749f388f":"code","34a3b25b":"code","f98f96b2":"code","6035eb86":"code","0086b309":"code","eeebff57":"code","725dd30f":"code","10050ec7":"code","94c48180":"code","5357e57b":"code","452d02fd":"code","4dccb778":"code","1b6be46d":"code","ba2333a8":"code","b7e443a4":"code","c5d3a9d0":"markdown","cacc8b50":"markdown","db7be4c9":"markdown","9bb8f59d":"markdown","47a635df":"markdown","c0af3b91":"markdown","39bad071":"markdown","4e4777a6":"markdown","df248a84":"markdown","4f2611db":"markdown","99468572":"markdown","acf906ed":"markdown","aac25abb":"markdown","e2f0a4c4":"markdown","3b1e27dc":"markdown"},"source":{"3d76b26f":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os, gc\nimport numpy as np\nimport pandas as pd\nfrom collections import defaultdict,Counter\nimport random\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('fivethirtyeight')\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport keras.backend as K\nprint(\"Tensorflow version: \",tf.__version__)\n\n# from transformers import *\n\nfrom sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\nfrom sklearn.metrics import mean_squared_error\n\nfrom tqdm import tqdm \nimport itertools\n\nSEED = 2020\n\ndef seed_everything(SEED):\n    np.random.seed(SEED)\n    tf.random.set_seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n\nseed_everything(SEED)","0943d640":"AUGMENTED = True\nOHE = False","9917fc0d":"PATH = '\/kaggle\/input\/stanford-covid-vaccine'\nos.listdir(PATH)","67eb93a7":"train = pd.read_json(os.path.join(PATH,'train.json'),lines=True).drop('index',axis=1)\ntest = pd.read_json(os.path.join(PATH,'test.json'),lines=True).drop('index',axis=1)\nsub = pd.read_csv(os.path.join(PATH,'sample_submission.csv'))","95b53414":"if AUGMENTED:\n    train = pd.read_json('..\/input\/openvaccine\/train.json').drop('index',axis=1)\n    test = pd.read_json('..\/input\/openvaccine\/test.json').drop('index',axis=1)\n    test = test[test.cnt==1]","8f0bdb55":"def get_bppm(id_):\n    return np.load(os.path.join(PATH,f\"bpps\/{id_}.npy\"))","5d1ae658":"train.head().T","818d9590":"target_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\nfeature_cols = ['sequence', 'structure', 'predicted_loop_type']","8bd124a6":"# Filter the train data\n# train = train[train.signal_to_noise>1]\n# train = train[train.SN_filter==1]","38da74dd":"# def get_stratify_group(row):\n#     snf = row['SN_filter']\n#     snr = row['signal_to_noise']\n    \n#     if snf == 0:\n#         if snr<0:\n#             snr_c = 0\n#         elif 0<= snr < 2:\n#             snr_c = 1\n#         elif 2<= snr < 4:\n#             snr_c = 2\n#         elif 4<= snr < 5.5:\n#             snr_c = 3\n#         elif 5.5<= snr < 10:\n#             snr_c = 4\n#         elif snr >= 10:\n#             snr_c = 5\n            \n#     else: # snf == 1\n#         if snr<2.5:\n#             snr_c = 6\n#         elif 2.5<= snr < 7.5:\n#             snr_c = 7\n#         elif 7.5<= snr < 11:\n#             snr_c = 8\n#         elif snr >= 11:\n#             snr_c = 9\n        \n#     return '{}'.format(snr_c)\n\ndef get_stratify_group(row):\n    snf = row['SN_filter']\n    snr = row['signal_to_noise']\n    \n    if snf == 0:\n        if snr<0:\n            snr_c = 0\n        elif 0<= snr < 2:\n            snr_c = 1\n        elif 2<= snr < 4:\n            snr_c = 2\n        elif 4<= snr < 5.5:\n            snr_c = 3\n        elif 5.5<= snr < 10:\n            snr_c = 4\n        elif snr >= 10:\n            snr_c = 5\n            \n    else: # snf == 1\n        if snr < 2:\n            snr_c = 6\n        elif 2<= snr < 3:\n            snr_c = 7\n        elif 3<= snr < 4:\n            snr_c = 8\n        elif 4<= snr < 5:\n            snr_c = 9\n        elif 5<= snr < 6:\n            snr_c = 10\n        elif 6<= snr < 7:\n            snr_c = 11\n        elif 7<= snr < 8:\n            snr_c = 12\n        elif 8<= snr < 9:\n            snr_c = 13\n        elif 9<= snr < 10:\n            snr_c = 14\n        elif snr >= 10:\n            snr_c = 15\n        \n    return snr_c\n\n\ntrain['snr_stratify_group'] = train.apply(get_stratify_group,axis=1)","2bca818e":"# Some new features\n\n# Next or Prev loop type\ndef getNextOrPrevLoop(df,NEXT=True):\n    data = []\n    for index in range(df.shape[0]):\n        predicted_loop_type = df.iloc[index]['predicted_loop_type']\n        if NEXT:\n            predicted_loop_type = predicted_loop_type[::-1]\n        \n        prevLoop = '-'\n        nextLoop = [prevLoop]\n        for i in range(1,len(predicted_loop_type)):\n            curr = predicted_loop_type[i]\n            prev = predicted_loop_type[i-1]\n            \n            if curr != prev:\n                prevLoop = prev\n            \n            nextLoop.append(prevLoop)\n        \n        if NEXT:\n            nextLoop = nextLoop[::-1]\n\n        data.append(\"\".join(nextLoop))\n    \n    return data\n\n\ndef getPairedWith(df):\n    data = []\n    bppsData = []\n    absDistData = []\n    \n    # Adjacent pair type\n    prevPairData = []\n    nextPairData = []\n\n    for index in range(df.shape[0]):\n        sequence = df.iloc[index]['sequence']\n        structure = df.iloc[index]['structure']\n        \n        bpps = get_bppm(df.iloc[index]['id'])\n\n        pairedWith = ['#']*len(sequence)\n        bpp = [-1]*len(sequence)\n        abs_dist = [-1]*len(sequence)\n        pairIndex = [-1]*len(sequence)\n        \n        stack = []\n        for i in range(len(sequence)):\n            if structure[i] == '(':\n                stack.append(i)\n            elif structure[i] == ')':\n                j = stack.pop(-1)\n                \n                pairIndex[i] = j\n                pairIndex[j] = i\n                \n                pairedWith[j] = sequence[i]\n                pairedWith[i] = sequence[j]\n                \n                bpp[j] = bpps[j][i]\n                bpp[i] = bpps[i][j]\n                \n                abs_dist[j] = abs(i-j)\n                abs_dist[i] = abs(i-j)\n        \n        prevPair = [-1]*len(sequence)\n        nextPair = [-1]*len(sequence)\n        for i in range(len(sequence)):\n            if i-1>=0 and pairIndex[i-1]!=-1:\n                prevPair[i] = (sequence[i-1],sequence[pairIndex[i-1]])\n            if i+1<len(sequence) and pairIndex[i+1]!=-1:\n                nextPair[i] = (sequence[i+1],sequence[pairIndex[i+1]])\n        \n        prevPairData.append(prevPair)\n        nextPairData.append(nextPair)\n        \n        data.append(\"\".join(pairedWith))\n        bppsData.append(bpp)\n        absDistData.append(abs_dist)\n    \n    return data, bppsData, absDistData, prevPairData, nextPairData\n\ndef getBPPFeatures(df):\n    maxs = []\n    means = []\n    argmaxs = []\n    sums = []\n    nonzero = []\n    \n    for index in range(df.shape[0]):\n        bpps = get_bppm(df.iloc[index].id)\n        \n        maxs.append(np.max(bpps,axis=-1))\n        means.append(np.mean(bpps,axis=-1))\n        argmaxs.append(np.argmax(bpps,axis=-1))\n        sums.append(np.sum(bpps,axis=-1))\n        nonzero.append((bpps>0).mean(axis=-1))\n        \n    return maxs, means, argmaxs, sums, nonzero\n\ndef getNeighbourBPP(df):\n    dataB = [] # BPP with Prev Base\n    dataA = [] # BPP with Next Base\n    \n    for index in range(df.shape[0]):\n        bpps = get_bppm(df.iloc[index].id)\n        seq_length = df.iloc[index].seq_length\n        \n        B = [-1]*seq_length\n        A = [-1]*seq_length\n        \n        for i in range(seq_length):\n            \n            if i-1>=0:\n                B[i] = bpps[i][i-1]\n            if i+1<seq_length:\n                A[i] = bpps[i][i+1]\n            \n        dataB.append(B)\n        dataA.append(A)\n    \n    return dataB, dataA\n\n# Minimum Pair Distance from Stem-ends\ndef getMinPairDist(df):\n    data = []\n    \n    for index in range(df.shape[0]):\n        structure = df.iloc[index].structure\n        seq_length = df.iloc[index].seq_length\n        \n        dist = [-1]*seq_length\n        \n        a = 0\n        if structure[0] in ['(',')']:\n            dist[0] = a\n        for i in range(1,seq_length):\n            curr = structure[i]\n            if curr in ['(',')']:\n                if curr==structure[i-1]:\n                    a += 1\n                else:\n                    a = 0\n                dist[i] = a\n        \n        structure = structure[::-1]\n        \n        a = 0\n        if structure[0] in ['(',')']:\n            dist[seq_length - 0 - 1] = a\n        for i in range(1,seq_length):\n            curr = structure[i]\n            if curr in ['(',')']:\n                if curr==structure[i-1]:\n                    a += 1\n                else:\n                    a = 0\n                dist[seq_length - i-1] = min(a,dist[seq_length - i-1])\n        \n        data.append(dist)\n            \n    return data","4a2248de":"\"\"\"https:\/\/www.kaggle.com\/mrkmakr\/covid-ae-pretrain-gnn-attn-cnn\"\"\"\n\ndef return_ohe(n, i):\n    tmp = [0] * n\n    tmp[i] = 1\n    return tmp\n\ndef getOHEInput(df):\n    \n    ## get node features, which is one hot encoded\n    mapping = {}\n    vocab = [\"A\", \"G\", \"C\", \"U\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_node = np.stack(df[\"sequence\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n    \n    if 'pairedWith' in df.columns:\n        mapping = {}\n        vocab = [\"A\", \"G\", \"C\", \"U\", \"#\"]\n        for i, s in enumerate(vocab):\n            mapping[s] = return_ohe(len(vocab), i)\n        X_paired = np.stack(df['pairedWith'].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n        \n        X_node = np.concatenate([X_node,X_paired],axis=2)\n\n    mapping = {}\n    vocab = [\"S\", \"M\", \"I\", \"B\", \"H\", \"E\", \"X\"]\n#     if AUGMENTED:\n#         vocab = ['s','h','f','t','i','m']\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_loop = np.stack(df[\"predicted_loop_type\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n    \n    mapping = {}\n    vocab = [\".\", \"(\", \")\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_structure = np.stack(df[\"structure\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n    \n    X_node = np.concatenate([X_node, X_loop, X_structure], axis = 2)\n    \n    ## interaction\n    a = np.sum(X_node * (2 ** np.arange(X_node.shape[2])[None, None, :]), axis = 2)\n    vocab = sorted(set(a.flatten()))\n    # print(vocab)\n    ohes = []\n    for v in vocab:\n        ohes.append(a == v)\n    ohes = np.stack(ohes, axis = 2)\n    X_node = np.concatenate([X_node, ohes], axis = 2).astype(np.float32)\n    \n    # print(X_node.shape)\n    \n    return X_node","fb48fef5":"s = set(['A','C','G','U'])\npairTypeDict = dict(zip(list(itertools.product(s,s)),range(len(s)**2)))\n\npairTypeDict","42df5b7e":"new_features = []\nseparate_features = []\n\n# train['nextLoopType'] = getNextOrPrevLoop(train)\n# train['prevLoopType'] = getNextOrPrevLoop(train,NEXT=False)\n\n# test['nextLoopType'] = getNextOrPrevLoop(test)\n# test['prevLoopType'] = getNextOrPrevLoop(test,NEXT=False)\n\n# new_features += ['nextLoopType','prevLoopType']\n\ntrain_pw_f = getPairedWith(train)\ntest_pw_f = getPairedWith(test)\n\ntrain['pairedWith'],test['pairedWith'] = train_pw_f[0], test_pw_f[0]; new_features += ['pairedWith']\ntrain['bpp'], test['bpp'] = train_pw_f[1], test_pw_f[1]; separate_features += ['bpp']\ntrain['abs_dist'], test['abs_dist'] = train_pw_f[2], test_pw_f[2]; separate_features += ['abs_dist']\n\nfeature_cols += new_features\n\ntrain_bpp_f = getBPPFeatures(train)\ntest_bpp_f = getBPPFeatures(test)\n\ntrain_neighbour_bpp = getNeighbourBPP(train)\ntest_neighbour_bpp = getNeighbourBPP(test)\n\ntrain['bpp_maxs'], test['bpp_maxs'] = train_bpp_f[0], test_bpp_f[0]; separate_features += ['bpp_maxs']\ntrain['bpp_means'], test['bpp_means'] = train_bpp_f[1], test_bpp_f[1]; separate_features += ['bpp_means']\n# train['bpp_argmaxs'], test['bpp_argmaxs'] = train_bpp_f[2], test_bpp_f[2]; separate_features += ['bpp_argmaxs']\ntrain['bpp_sums'], test['bpp_sums'] = train_bpp_f[3], test_bpp_f[3]; separate_features += ['bpp_sums']\n\n# train['bpp_before'], test['bpp_before'] = train_neighbour_bpp[0], test_neighbour_bpp[0]; separate_features += ['bpp_before']\ntrain['bpp_after'], test['bpp_after'] = train_neighbour_bpp[1], test_neighbour_bpp[1]; separate_features += ['bpp_after']\n\ntrain['bpp_nonzero'], test['bpp_nonzero'] = train_bpp_f[4], test_bpp_f[4]; separate_features += ['bpp_nonzero']\n\n\n# Adjacent Pair Type\n# train['prevPairType'], test['prevPairType'] = train_pw_f[3], test_pw_f[3]; separate_features += ['prevPairType']\n# train[['prevPairType']] = train[['prevPairType']].applymap(lambda pairs: [pairTypeDict[pair] if pair!=-1 else -1 for pair in pairs])\n# test[['prevPairType']] = test[['prevPairType']].applymap(lambda pairs: [pairTypeDict[pair] if pair!=-1 else -1 for pair in pairs])\n\n# train['nextPairType'], test['nextPairType'] = train_pw_f[4], test_pw_f[4]; separate_features += ['nextPairType']\n# train[['nextPairType']] = train[['nextPairType']].applymap(lambda pairs: [pairTypeDict[pair] if pair!=-1 else -1 for pair in pairs])\n# test[['nextPairType']] = test[['nextPairType']].applymap(lambda pairs: [pairTypeDict[pair] if pair!=-1 else -1 for pair in pairs])\n\n# train['min_pair_dist_from_stem_end'], test['min_pair_dist_from_stem_end'] = getMinPairDist(train), getMinPairDist(test); separate_features += ['min_pair_dist_from_stem_end']\n\ntrain.head()","73048808":"token2int = {x:i for i, x in enumerate('#().ACGUBEHIMSX')}\n\n# if AUGMENTED:\n#     token2int = {x:i for i, x in enumerate('#().ACGUshftim')}","b4e2380b":"def preprocess_inputs(df, cols=feature_cols, sep=separate_features, token2int=token2int):\n    \n    if OHE:\n        X_f = getOHEInput(df[cols])\n        X_s = np.transpose(\n            np.array(\n            df[sep]\n            .values\n            .tolist()\n            ),\n            (0, 2, 1)\n        )\n        \n        return np.concatenate([X_f, X_s], axis = 2).astype(np.float32)\n        \n        \n    df1 = df[cols].applymap(lambda seq: [token2int[x] for x in seq])\n    \n    df2 = df[sep] \n    \n    return np.transpose(\n        np.array(\n            pd.concat([df1,df2],axis=1)\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )","ef572fa7":"X = preprocess_inputs(train,feature_cols)\n\nif AUGMENTED:\n    train[target_cols] = train[target_cols].applymap(lambda x: x[1:-1].split(\", \"))\n\ny = np.array(train[target_cols].values.tolist(),dtype=np.float32).transpose((0, 2, 1))\n\nprint(\"Shape of X: \",X.shape)\nprint(\"Shape of y: \",y.shape)","8133d060":"public_df = test.query(\"seq_length == 107\").copy()\nprivate_df = test.query(\"seq_length == 130\").copy()\n\nX_public = preprocess_inputs(public_df,feature_cols)\nX_private = preprocess_inputs(private_df,feature_cols)\n\nprint(\"Public Test size: \",X_public.shape)\nprint(\"Private Test size: \",X_private.shape)\n\nassert X_public.shape[1:] == X.shape[1:], \"Train & Test features shape not same\"","0438c3dc":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n    \nexcept ValueError:\n    tpu = None\n    #If TPU not found try with GPUs\n    gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n    \n# Select appropriate distribution strategy for hardware\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Running on TPU ', tpu.master())  \n    \nelif len(gpus) > 0:\n    strategy = tf.distribute.MirroredStrategy(gpus) # this works for 1 to multiple GPUs\n    print('Running on ', len(gpus), ' GPU(s) ')\n\nelse:\n    strategy = tf.distribute.get_strategy()\n    print('Running on CPU')\n\n# How many accelerators do we have ?\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n    \nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync","7379ae7a":"FOLDS = 5\nEPOCHS = 100\nBATCH_SIZE = 32\nVERBOSE = 2\n\nLR = 0.00004\nWARMUP = 5\n\nCYCLIC_LRS = True\nCYCLIC_BLR = 1e-3\nCYCLIC_MLR = 8e-3\nCYCLIC_STEP = int((FOLDS-1)*(X.shape[0]\/(FOLDS*BATCH_SIZE))*8)\nprint(\"Cyclic Step: \",CYCLIC_STEP)\n\nEXPDECAY_LRS = False\nEXP_LRSTART = 5e-3\nEXP_LRMAX = 1.25e-2\nEXP_LRMIN = 1e-3\nEXP_RAMPUP = 5\nEXP_SUSTAIN = 2\nEXP_DECAY = 0.8\n\nINFER_TEST = True\nDISPLAY_PLOT = True\n\nDROPOUT = 0.4\nEMBED_DIM = 100\n\nTRANSFORMERS = False\n\nSKF_SNR = True\n\nprint(\"Replicas: \",REPLICAS)","45e8a9c4":"# Gaussian Noise\nP = 0.0\ndef add_noise(X,y):\n    \n    if np.random.choice([True,False],p=[P,1-P]):\n        y += tf.random.normal(tf.shape(y), 0, 1, tf.float32,seed=SEED)\n    \n    return X, y","ed0e3711":"def getTrainGenerator(X_,y_):\n    traindata_generator = (\n        tf.data.Dataset.from_tensor_slices((X_,y_))\n        .cache()\n        .map(add_noise,num_parallel_calls=AUTO)\n        .batch(BATCH_SIZE*REPLICAS)\n        .prefetch(AUTO)\n    )\n    \n    return traindata_generator","c1cff79d":"def rmse(y_actual, y_pred):\n    mse = tf.keras.losses.mean_squared_error(y_actual, y_pred)\n    return K.sqrt(mse)\n\ndef mcrmse(y_actual, y_pred, num_scored=len(target_cols)):\n    score = 0\n    for i in range(num_scored):\n        score += rmse(y_actual[:, :, i], y_pred[:, :, i]) \/ num_scored\n    return score","eb532818":"def get_angles(pos, i, d_model):\n    angle_rates = 1 \/ np.power(10000, (2 * (i\/\/2)) \/ np.float32(d_model))\n    return pos * angle_rates\n\n\ndef positional_encoding(position, d_model):\n    angle_rads = get_angles( np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n    \n    # apply sin to even indices in the array; 2i\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    \n    # apply cos to odd indices in the array; 2i+1\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    pos_encoding = angle_rads[np.newaxis, ...]\n    return tf.cast(pos_encoding, dtype=tf.float32)","22e68f9a":"class MultiHeadAttention(tf.keras.layers.Layer):\n    \n    def __init__(self, d_model = 512, num_heads = 8, causal=False, dropout=0.0):\n        super(MultiHeadAttention, self).__init__()\n\n        assert d_model % num_heads == 0\n        depth = d_model \/\/ num_heads\n\n        self.w_query = tf.keras.layers.Dense(d_model)\n        self.split_reshape_query = tf.keras.layers.Reshape((-1,num_heads,depth))  \n        self.split_permute_query = tf.keras.layers.Permute((2,1,3))      \n\n        self.w_value = tf.keras.layers.Dense(d_model)\n        self.split_reshape_value = tf.keras.layers.Reshape((-1,num_heads,depth))\n        self.split_permute_value = tf.keras.layers.Permute((2,1,3))\n\n        self.w_key = tf.keras.layers.Dense(d_model)\n        self.split_reshape_key = tf.keras.layers.Reshape((-1,num_heads,depth))\n        self.split_permute_key = tf.keras.layers.Permute((2,1,3))\n\n        self.attention = tf.keras.layers.Attention(causal=causal, dropout=dropout)\n        self.join_permute_attention = tf.keras.layers.Permute((2,1,3))\n        self.join_reshape_attention = tf.keras.layers.Reshape((-1,d_model))\n\n        self.dense = tf.keras.layers.Dense(d_model)\n\n    def call(self, inputs, mask=None, training=None):\n        q = inputs[0]\n        v = inputs[1]\n        k = inputs[2] if len(inputs) > 2 else v\n\n        query = self.w_query(q)\n        query = self.split_reshape_query(query)    \n        query = self.split_permute_query(query)                 \n\n        value = self.w_value(v)\n        value = self.split_reshape_value(value)\n        value = self.split_permute_value(value)\n\n        key = self.w_key(k)\n        key = self.split_reshape_key(key)\n        key = self.split_permute_key(key)\n\n        if mask is not None:\n            if mask[0] is not None:\n                mask[0] = tf.keras.layers.Reshape((-1,1))(mask[0])\n                mask[0] = tf.keras.layers.Permute((2,1))(mask[0])\n            if mask[1] is not None:\n                mask[1] = tf.keras.layers.Reshape((-1,1))(mask[1])\n                mask[1] = tf.keras.layers.Permute((2,1))(mask[1])\n\n        attention = self.attention([query, value, key], mask=mask)\n        attention = self.join_permute_attention(attention)\n        attention = self.join_reshape_attention(attention)\n\n        x = self.dense(attention)\n\n        return x\n    \n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'w_q': self.w_query,\n            's_r_q':self.split_reshape_query, \n            's_p_q':self.split_permute_query,\n            \n            'w_v':self.w_value,\n            's_r_v':self.split_reshape_value,\n            's_p_v':self.split_permute_value,\n\n            'w_k':self.w_key,\n            's_r_k':self.split_reshape_key,\n            's_p_k':self.split_permute_key,\n\n            'attn':self.attention,\n            'p_attn':self.join_permute_attention,\n            'r_attn':self.join_reshape_attention,\n\n            'dense':self.dense\n        })\n        \n        return config","82b7e54d":"class EncoderLayer(tf.keras.layers.Layer):\n    \n    def __init__(self,  d_model = 512, num_heads = 8, dff = 2048, dropout = 0.0):\n        super(EncoderLayer, self).__init__()\n\n        self.multi_head_attention =  MultiHeadAttention(d_model, num_heads)\n        self.dropout_attention = tf.keras.layers.Dropout(dropout)\n        self.add_attention = tf.keras.layers.Add()\n        self.layer_norm_attention = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dense1 = tf.keras.layers.Dense(dff, activation='relu')\n        self.dense2 = tf.keras.layers.Dense(d_model)\n        self.dropout_dense = tf.keras.layers.Dropout(dropout)\n        self.add_dense = tf.keras.layers.Add()\n        self.layer_norm_dense = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n    def call(self, inputs, mask=None, training=None):\n        # print(mask)\n        attention = self.multi_head_attention([inputs,inputs,inputs], mask = [mask,mask])\n        attention = self.dropout_attention(attention, training = training)\n        x = self.add_attention([inputs , attention])\n        x = self.layer_norm_attention(x)\n        # x = inputs\n\n        ## Feed Forward\n        dense = self.dense1(x)\n        dense = self.dense2(dense)\n        dense = self.dropout_dense(dense, training = training)\n        x = self.add_dense([x , dense])\n        x = self.layer_norm_dense(x)\n\n        return x\n\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'mha': self.multi_head_attention,\n            'drop_attn': self.dropout_attention,\n            'add_attn': self.add_attention,\n            'ln_attn': self.layer_norm_attention,\n            'dense1': self.dense1,\n            'dense2': self.dense2,\n            'drop_dense': self.dropout_dense,\n            'add_dense': self.add_dense,\n            'ln_dense': self.layer_norm_dense\n        })\n        \n        return config","8964711d":"def gru_layer(hidden_dim, dropout):\n    return tf.keras.layers.Bidirectional(\n                                tf.keras.layers.GRU(hidden_dim,\n                                dropout=dropout,\n                                return_sequences=True,\n                                kernel_initializer='orthogonal'))\n\n\ndef lstm_layer(hidden_dim, dropout):\n    return tf.keras.layers.Bidirectional(\n                                tf.keras.layers.LSTM(hidden_dim,\n                                dropout=dropout,\n                                return_sequences=True,\n                                kernel_initializer='orthogonal'))\n\ndef res(x, unit, kernel = 3, rate = 0.1):\n    h = tf.keras.layers.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n    h = tf.keras.layers.LayerNormalization()(h)\n    h = tf.keras.layers.LeakyReLU()(h)\n    h = tf.keras.layers.Dropout(rate)(h)\n    return tf.keras.layers.Add()([x, h])\n\ndef forward(x, unit, kernel = 3, rate = 0.1):\n#     h = L.Dense(unit, None)(x)\n    h = tf.keras.layers.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n    h = tf.keras.layers.LayerNormalization()(h)\n    h = tf.keras.layers.Dropout(rate)(h)\n#         h = tf.keras.activations.swish(h)\n    h = tf.keras.layers.LeakyReLU()(h)\n    h = res(h, unit, kernel, rate)\n    return h\n\n\ndef build_model(seq_len=107, pred_len=68, dropout=DROPOUT,\n                embed_dim=EMBED_DIM, hidden_dim=128):\n    \n    inputs = tf.keras.layers.Input(shape=(seq_len, X.shape[2]))\n    \n    fs = len(feature_cols)\n    if OHE:\n        fs = X.shape[2] - len(separate_features)\n#         conv1 = forward(inputs[:,:,:fs], 256, 3, 0.0)\n        conv1 = tf.keras.layers.Conv1D(256, 3, padding='same', activation='elu')(inputs[:,:,:fs])\n#         conv2 = tf.keras.layers.Conv1D(128, 6, padding='same', activation='elu')(conv1)\n#         conv3 = tf.keras.layers.Conv1D(32, 12, padding='same', activation='elu')(conv2)\n#         conv4 = tf.keras.layers.Conv1D(16, 24, padding='same', activation='elu')(conv3)\n#         hidden = tf.keras.layers.Concatenate(axis=-1)([conv1,conv2])\n        hidden = tf.keras.layers.SpatialDropout1D(.2)(conv1)\n    else:\n        embed = tf.keras.layers.Embedding(input_dim=len(token2int), output_dim=embed_dim)(inputs[:,:,:fs])\n        reshaped = tf.reshape(embed, shape=(-1, embed.shape[1], embed.shape[2] * embed.shape[3]))\n        hidden = tf.keras.layers.SpatialDropout1D(.2)(reshaped)\n        \n    if len(separate_features)==1:\n        hidden = tf.keras.layers.Concatenate(axis=-1)([hidden,tf.keras.backend.expand_dims(inputs[:,:,fs:],axis=-1)])\n    else:\n        hidden = tf.keras.layers.Concatenate(axis=-1)([hidden,inputs[:,:,fs:]])\n    \n#     conv1 = forward(hidden, 256, 3, 0.0)\n    conv1 = tf.keras.layers.Conv1D(256, 3, padding='same', activation='elu')(hidden)\n    \n    hidden = gru_layer(hidden_dim, dropout)(conv1)\n    \n    hidden = tf.keras.layers.Concatenate(axis=-1)([hidden,conv1])\n    \n#     hidden = gru_layer(hidden_dim, dropout)(hidden)\n    \n#     hidden = gru_layer(hidden_dim, dropout)(hidden)\n#     hidden = gru_layer(hidden_dim, dropout)(hidden)\n    \n    \n    hidden = gru_layer(hidden_dim, dropout)(hidden)\n    hidden = lstm_layer(hidden_dim, dropout)(hidden)\n#     hidden = lstm_layer(hidden_dim, dropout)(hidden)\n\n    # BLOCK 1\n#     hidden_1 = gru_layer(hidden_dim, dropout)(hidden)\n#     hidden_1 = gru_layer(hidden_dim, dropout)(hidden_1)\n#     hidden_1 = gru_layer(hidden_dim, dropout)(hidden_1)\n        \n    # BLOCK 2\n#     hidden_2 = lstm_layer(hidden_dim, dropout)(hidden)\n#     hidden_2 = lstm_layer(hidden_dim, dropout)(hidden_2)\n#     hidden_2 = lstm_layer(hidden_dim, dropout)(hidden_2)\n        \n    # BLOCK 3\n#     hidden_3 = gru_layer(hidden_dim, dropout)(hidden)\n#     hidden_3 = lstm_layer(hidden_dim, dropout)(hidden_3)\n#     hidden_3 = gru_layer(hidden_dim, dropout)(hidden_3)\n    \n    # BLOCK 4\n#     hidden_4 = lstm_layer(hidden_dim, dropout)(hidden)\n#     hidden_4 = gru_layer(hidden_dim, dropout)(hidden_4)\n#     hidden_4 = lstm_layer(hidden_dim, dropout)(hidden_4)\n    \n    #only making predictions on the first part of each sequence\n#     truncated_1 = hidden_1[:, :pred_len]\n#     truncated_2 = hidden_2[:, :pred_len]\n#     truncated_3 = hidden_3[:, :pred_len]\n#     truncated_4 = hidden_4[:, :pred_len]\n    \n#     out_1 = tf.keras.layers.Dense(5, activation='linear')(truncated_1)\n#     out_2 = tf.keras.layers.Dense(5, activation='linear')(truncated_2)\n#     out_3 = tf.keras.layers.Dense(5, activation='linear')(truncated_3)\n#     out_4 = tf.keras.layers.Dense(5, activation='linear')(truncated_4)\n    \n#     out = tf.keras.layers.Concatenate(axis=-1)([truncated_1,truncated_2,truncated_3,truncated_4])\n#     out = tf.keras.layers.Dense(128, activation='elu')(out)\n#     out = tf.keras.layers.Dropout(0.4)(out)\n    \n#     out = tf.keras.layers.Dense(5, activation='linear')(out)\n\n#     hidden = tf.keras.layers.Conv1D(512, 3, padding='same', activation='elu')(hidden)\n    \n    truncated = hidden[:, :pred_len]\n    \n    out = tf.keras.layers.Dense(5, activation='linear')(truncated)\n\n    model = tf.keras.Model(inputs=inputs, outputs=out)\n\n    #some optimizers\n    adam = tf.optimizers.Adam()\n    radam = tfa.optimizers.RectifiedAdam()\n    lookahead = tfa.optimizers.Lookahead(adam, sync_period=6)\n    ranger = tfa.optimizers.Lookahead(radam, sync_period=6)\n    \n    model.compile(optimizer=adam, loss=mcrmse)\n    \n    return model","fcf8126c":"def build_transformer(hidden_dim=128,seq_len=107, pred_len=68, embed_dim=100, d_model = 128, num_heads = 4, dff = 256, maximum_position_encoding = 10000, dropout = 0.4):\n    \n    inputs = tf.keras.layers.Input(shape=(seq_len, X.shape[2]))\n    \n    # Input Embeddings\n    fs = len(feature_cols)\n    if OHE:\n        fs = X.shape[2] - len(separate_features)\n        conv1 = tf.keras.layers.Conv1D(256, 3, padding='same', activation='elu')(inputs[:,:,:fs])\n        hidden = tf.keras.layers.SpatialDropout1D(.2)(conv1)\n    else:\n        embed = tf.keras.layers.Embedding(input_dim=len(token2int), output_dim=embed_dim)(inputs[:,:,:fs])\n        reshaped = tf.reshape(embed, shape=(-1, embed.shape[1], embed.shape[2] * embed.shape[3]))\n        hidden = tf.keras.layers.SpatialDropout1D(.2)(reshaped)\n        \n    if len(separate_features)==1:\n        hidden = tf.keras.layers.Concatenate(axis=-1)([hidden,tf.keras.backend.expand_dims(inputs[:,:,fs:],axis=-1)])\n    else:\n        hidden = tf.keras.layers.Concatenate(axis=-1)([hidden,inputs[:,:,fs:]])\n\n    # Input Embeddings\n    # embedding = tf.keras.layers.Embedding(input_dim=len(token2int), output_dim=embed_dim)(inputs[:,:,:len(feature_cols)])\n    # reshaped = tf.reshape(embedding, shape=(-1, embedding.shape[1], embedding.shape[2] * embedding.shape[3]))\n    # hidden = tf.keras.layers.SpatialDropout1D(.2)(reshaped)\n    \n    # Numerical Features\n#     if len(separate_features)==1:\n#         hidden = tf.keras.layers.Concatenate(axis=-1)([hidden,tf.keras.backend.expand_dims(inputs[:,:,len(feature_cols):],axis=-1)])\n#     else:\n#         hidden = tf.keras.layers.Concatenate(axis=-1)([hidden,inputs[:,:,len(feature_cols):]])\n    \n    x = tf.keras.layers.Conv1D(d_model, 3, padding='same', activation='elu')(hidden)\n    \n    # RNNs\n    x = gru_layer(d_model\/\/2, dropout)(x)\n    x = lstm_layer(d_model, dropout)(x)\n    x = gru_layer(d_model\/\/2, dropout)(x)\n\n    # Positional Encoding\n#     scaling_factor = tf.keras.backend.constant(np.sqrt(d_model), shape = (1,1,1))\n#     x = tf.keras.layers.Multiply()([x,scaling_factor])\n#     pos = positional_encoding(maximum_position_encoding, d_model)\n#     x = tf.keras.layers.Add()([x, pos[: , :tf.shape(x)[1], :]])\n    \n    # Encoding Layers\n    x = EncoderLayer(d_model, num_heads, dff, dropout)(x)\n    # x = EncoderLayer(d_model, num_heads, dff, dropout)(x)\n    \n    # RNNs\n    # x = gru_layer(d_model\/\/2, dropout)(x)\n    # x = gru_layer(d_model\/\/2, dropout)(x)\n    \n    # FFN\n    truncated = x[:,:pred_len,:]\n    out = tf.keras.layers.Dense(5,activation='linear')(truncated)\n    \n    # Model\n    model = tf.keras.Model(inputs=inputs, outputs=out)\n\n    # Optimizers\n    adam = tf.optimizers.Adam()\n    radam = tfa.optimizers.RectifiedAdam()\n    lookahead = tfa.optimizers.Lookahead(adam, sync_period=6)\n    ranger = tfa.optimizers.Lookahead(radam, sync_period=6)\n    \n    # Compilation\n    model.compile(optimizer=adam, loss=mcrmse)\n    \n    return model","749f388f":"if not TRANSFORMERS:\n    plot = tf.keras.utils.plot_model(build_model(),show_shapes=True)\nelse:\n    plot = tf.keras.utils.plot_model(build_transformer(),show_shapes=True)\n\nplot","34a3b25b":"def get_cosine_schedule_with_warmup(lr, num_warmup_steps, num_training_steps, num_cycles=0.5):\n    \"\"\"\n    Modified version of the get_cosine_schedule_with_warmup from huggingface.\n    (https:\/\/huggingface.co\/transformers\/_modules\/transformers\/optimization.html#get_cosine_schedule_with_warmup)\n\n    Create a schedule with a learning rate that decreases following the\n    values of the cosine function between 0 and `pi * cycles` after a warmup\n    period during which it increases linearly between 0 and 1.\n    \"\"\"\n\n    def lrfn(epoch):\n        if epoch < num_warmup_steps:\n            return (float(epoch) \/ float(max(1, num_warmup_steps))) * lr\n        progress = float(epoch - num_warmup_steps) \/ float(max(1, num_training_steps - num_warmup_steps))\n        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))) * lr\n\n    return tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n\n\ndef build_lrfn(lr_start=EXP_LRSTART, lr_max=EXP_LRMAX, \n               lr_min=EXP_LRMIN, lr_rampup_epochs=EXP_RAMPUP, \n               lr_sustain_epochs=EXP_SUSTAIN, lr_exp_decay=EXP_DECAY):\n    \n    lr_max = lr_max * BATCH_SIZE\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        \n        return lr\n    \n    return lrfn\n\n\nclass CyclicLR(tf.keras.callbacks.Callback):\n    \n    def __init__(self,base_lr=0.25e-3,max_lr=1e-3,stepsize=8):\n        super().__init__()\n        \n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.stepsize = stepsize\n        self.iterations = 0\n        self.history = {}\n        \n    def clr(self):\n        cycle = np.floor((1+self.iterations)\/(2*self.stepsize))\n        x = np.abs(self.iterations\/self.stepsize - 2*cycle + 1)\n        \n        return self.base_lr + (self.max_lr - self.base_lr)*(np.maximum(0,1-x))*(1\/(2.**(cycle-1)))\n    \n    def on_train_begin(self,logs={}):\n        tf.keras.backend.set_value(self.model.optimizer.lr, self.base_lr)\n    \n    def on_batch_end(self,batch,logs=None):\n        logs = logs or {}\n        \n        self.iterations += 1\n        \n        self.history.setdefault('lr', []).append(tf.keras.backend.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        tf.keras.backend.set_value(self.model.optimizer.lr, self.clr())","f98f96b2":"def stratified_group_k_fold(X, y, groups, k, seed=None):\n    \n    \"\"\"https:\/\/www.kaggle.com\/jakubwasikowski\/stratified-group-k-fold-cross-validation\"\"\"\n    \n    labels_num = np.max(y)+1\n    y = y.astype('int32')\n    y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n    y_distr = Counter()\n    for label, g in zip(y, groups):\n        y_counts_per_group[g][label] += 1\n        y_distr[label] += 1\n\n    y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n    groups_per_fold = defaultdict(set)\n\n    def eval_y_counts_per_fold(y_counts, fold):\n        y_counts_per_fold[fold] += y_counts\n        std_per_label = []\n        for label in range(labels_num):\n            label_std = np.std([y_counts_per_fold[i][label] \/ y_distr[label] for i in range(k)])\n            std_per_label.append(label_std)\n        y_counts_per_fold[fold] -= y_counts\n        return np.mean(std_per_label)\n    \n    groups_and_y_counts = list(y_counts_per_group.items())\n    random.Random(seed).shuffle(groups_and_y_counts)\n\n    for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n        best_fold = None\n        min_eval = None\n        for i in range(k):\n            fold_eval = eval_y_counts_per_fold(y_counts, i)\n            if min_eval is None or fold_eval < min_eval:\n                min_eval = fold_eval\n                best_fold = i\n        y_counts_per_fold[best_fold] += y_counts\n        groups_per_fold[best_fold].add(g)\n\n    all_groups = set(groups)\n    for i in range(k):\n        train_groups = all_groups - groups_per_fold[i]\n        test_groups = groups_per_fold[i]\n\n        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n\n        yield train_indices, test_indices","6035eb86":"if len(tf.config.list_physical_devices('GPU')):\n    print('Training on GPU')\nelse:\n    print('Not on GPU')","0086b309":"%%time\n\nhistories = []\nprivate_preds = np.zeros((private_df.shape[0], 130, 5))\npublic_preds = np.zeros((public_df.shape[0], 107, 5))\n\noof_pred = []\noof_tar = []\noof_val = []\noof_ids = []\n\nskf = StratifiedKFold(FOLDS, random_state = SEED)\n\nskf_y = train['SN_filter']\nif SKF_SNR:\n    print(\"#### Stratified on SNRs\")\n    skf_y = train['snr_stratify_group']\n    print(\"unique SNR Groups: \",skf_y.nunique())\n    \nsplits = skf.split(train, skf_y)\nif AUGMENTED:\n    # splits = GroupKFold(FOLDS).split(train,groups=skf_y)\n    print(\"#### Grouped by train.ids\")\n    splits = stratified_group_k_fold(train,skf_y,train.id,k=FOLDS,seed=SEED)\n\nfor fold, (idxT, idxV) in enumerate(splits):\n    \n    print()\n    print(\"#\"*50)\n    print(\"#### FOLD: \",fold)\n\n    X_train, y_train = X[idxT], y[idxT]\n    X_val, y_val = X[idxV], y[idxV]\n    \n    # Filters of Validation Data\n    filter_ = train.iloc[idxV].SN_filter==1\n    if AUGMENTED:\n        filter_ = (train.iloc[idxV].SN_filter==1) & (train.iloc[idxV].cnt == 1)\n    \n    X_val, y_val = X_val[filter_], y_val[filter_]\n    \n    oof_ids.append(train.iloc[idxV][filter_]['id'].values)\n    oof_tar.append(y[idxV][filter_])\n    \n    print(\"#### Train Shape: \",X_train.shape)\n    print(\"#### Validation Shape: \",X_val.shape)\n    \n#     continue\n\n#     snr = train.iloc[idxT]['signal_to_noise'].values\n#     weights = np.abs(snr)\n#     weights[weights>1], weights[weights<1] = 1, 2\n    \n    modelCheckpoint = tf.keras.callbacks.ModelCheckpoint(f'WEIGHTS_FOLD{fold}.h5',monitor='val_loss',mode='min')\n    csvLogger = tf.keras.callbacks.CSVLogger(f'TRAININGLOGS_FOLD{fold}.csv')\n    \n    # LR SCHEDULE\n    lr_schedule = get_cosine_schedule_with_warmup(lr=LR,num_warmup_steps=WARMUP,num_training_steps=EPOCHS)\n\n    if CYCLIC_LRS:\n        lr_schedule = CyclicLR(base_lr=CYCLIC_BLR,max_lr=CYCLIC_MLR,stepsize=CYCLIC_STEP)\n    elif EXPDECAY_LRS:\n        lrfn = build_lrfn()\n        lr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)\n    \n    if not TRANSFORMERS:\n        model = build_model()\n    else:\n        model = build_transformer()\n    \n    model.summary()\n    print()\n    print()\n    \n    if AUGMENTED:\n        print(\"#### Training on Augmented Data: \")\n        history = model.fit(getTrainGenerator(X_train,y_train), \n                        validation_data=(X_val,y_val),\n                        batch_size=BATCH_SIZE,\n                        epochs=EPOCHS,\n#                         sample_weight = weights,\n                        callbacks=[lr_schedule,modelCheckpoint,csvLogger],\n                        verbose = VERBOSE)\n    else:\n        history = model.fit(X_train, y_train, \n                            validation_data=(X_val,y_val),\n                            batch_size=BATCH_SIZE,\n                            epochs=EPOCHS,\n#                             sample_weight = weights,\n                            callbacks=[lr_schedule,modelCheckpoint,csvLogger],\n                            verbose = VERBOSE)\n\n    histories.append(history)\n    \n    model.load_weights(f'WEIGHTS_FOLD{fold}.h5')\n    \n    # VAL PREDICTIONS\n    val_preds = model.predict(X_val)\n    oof_val.append(np.min(history.history['val_loss']))\n    \n    oof_pred.append(val_preds)\n    \n    if INFER_TEST:\n\n        #load best model and predict\n        if not TRANSFORMERS:\n            public_model = build_model(seq_len=107, pred_len=107)\n        else:\n            public_model = build_transformer(seq_len=107, pred_len=107)\n            \n        public_model.load_weights(f'WEIGHTS_FOLD{fold}.h5')\n        public_pred = public_model.predict(X_public) \/ FOLDS\n\n        if not TRANSFORMERS:\n            private_model = build_model(seq_len=130, pred_len=130)\n        else:\n            private_model = build_transformer(seq_len=130, pred_len=130)\n        \n        private_model.load_weights(f'WEIGHTS_FOLD{fold}.h5')\n        private_pred = private_model.predict(X_private) \/ FOLDS\n\n        public_preds += public_pred\n        private_preds += private_pred\n\n        del public_model, private_model\n    \n    print(\"#\"*50)\n    \n    # PLOT TRAINING\n    if DISPLAY_PLOT:\n        \n        plt.figure(figsize=(20,8))\n        \n        plt.plot(np.arange(EPOCHS),history.history['loss'],'-o',label='Train Loss',color='#cc0044')\n        plt.plot(np.arange(EPOCHS),history.history['val_loss'],'-o',label='Val Loss',color='#6600ff')\n        \n        x_ = np.argmin( history.history['val_loss'] ); y_ = np.min( history.history['val_loss'] )\n        \n        xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n        \n        plt.scatter(x_,y_,s=200,color='#6600ff'); plt.text(x_-0.03*xdist,y_+0.05*ydist,f'min loss - {y_}',size=14)\n        \n        plt.ylabel('RMSE Loss',size=14)\n        plt.title(f'FOLD - {fold} Training Losses & Validation Losses')\n        plt.legend()\n        \n        plt.show()\n        \n    del model\n    z = gc.collect()","eeebff57":"# Leakage-Check\nfor i, ids in enumerate(oof_ids):\n    print(\"#ids: \",len(set(ids)))\n    for j in range(i+1,len(oof_ids)):\n        print(f'Fold_{i} ids intersection Fold_{j} ids: ',len(set(ids).intersection(set(oof_ids[j]))))\n    print()\n\nprint(\"Total ids: \",np.concatenate(oof_ids).shape)","725dd30f":"plt.figure(figsize=(5,5))\n\nsns.barplot(x=[f'Fold_{fold}' for fold in range(FOLDS)],y=oof_val)\nplt.title('OOF RMSE')\n\nplt.show()","10050ec7":"oof_rmse = np.mean(oof_val)\n\nprint(\"Val Losses: \",oof_val)\n\nprint(f\"Mean Loss: {oof_rmse} +\/- {np.std(oof_val)}\")","94c48180":"oof_pred_ = np.transpose(np.array(np.concatenate(oof_pred,axis=0)),(0,2,1))\noof_tar_ = np.transpose(np.concatenate(oof_tar),(0,2,1))\n\noof_df = pd.DataFrame({\n    'id': np.concatenate(oof_ids)\n})\n\nfor i, target in enumerate(target_cols):\n    oof_df[f'{target}'] = list(oof_tar_[:,i,:])\n    oof_df[f'pred_{target}'] = list(oof_pred_[:,i,:])\n\noof_df.head()","5357e57b":"def preprocess(df):\n    \n    data = []\n    \n    for index in tqdm(range(df.shape[0])):\n        sample = df.iloc[index]\n        \n        seq_length = 68\n        \n        for i in range(seq_length):\n            preprocessed = {\n                'id': sample.id,\n                'id_seqpos': sample.id + '_' + str(i),\n            }\n            \n                \n            for col in target_cols:\n                preprocessed[col] = sample[col][i]\n                preprocessed[f'pred_{col}'] = sample[f'pred_{col}'][i]\n            \n            data.append(preprocessed)\n    \n    data = pd.DataFrame(data)\n    \n    return data\n\noof_df_ = preprocess(oof_df)\n\noof_df_.head()","452d02fd":"scored_cols = ['reactivity','deg_Mg_50C','deg_Mg_pH10']\n\nscored_rmse = 0\n\nfor col in scored_cols:\n    col_rmse = np.sqrt(np.mean((oof_df_[col] - oof_df_[f'pred_{col}'])**2))\n    print(f\"{col} RMSE: \",col_rmse)\n    scored_rmse += col_rmse\n    \nscored_rmse \/= len(scored_cols)\n\nprint(\"\\nRMSE on Scored Targets (Whole Train data): \",scored_rmse)\nprint()\nprint(\"#\"*50)\nprint()\n\nSN_FILTER = train[['id','SN_filter']]\nif AUGMENTED:\n    SN_FILTER = train[train.cnt==1][['id','SN_filter']]\n\noof_df_filtered = oof_df_.merge(SN_FILTER,on='id',how='left')\noof_df_filtered = oof_df_filtered[oof_df_filtered.SN_filter==1]\n\nfiltered_scored_rmse = 0\n\nfor col in scored_cols:\n    col_rmse = np.sqrt(np.mean((oof_df_filtered[col] - oof_df_filtered[f'pred_{col}'])**2))\n    print(f\"{col} RMSE: \",col_rmse)\n    filtered_scored_rmse += col_rmse\n    \nfiltered_scored_rmse \/= len(scored_cols)\n\nprint(\"RMSE on Scored Targets (Filtered Data): \",filtered_scored_rmse)","4dccb778":"oof_df_.to_csv(f'oof_{oof_rmse}_{scored_rmse}_{filtered_scored_rmse}.csv',index=False)","1b6be46d":"test_preds = []\n\nfor df, preds in [(public_df, public_preds), (private_df, private_preds)]:\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n\n        single_df = pd.DataFrame(single_pred, columns=target_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n        test_preds.append(single_df)\n\ntest_preds_df = pd.concat(test_preds)\ntest_preds_df.head()","ba2333a8":"submission = sub[['id_seqpos']].merge(test_preds_df, on=['id_seqpos'])\n\nsubmission.head()","b7e443a4":"submission.to_csv(f'submission_{oof_rmse}_{scored_rmse}_{filtered_scored_rmse}.csv', index=False)","c5d3a9d0":"## 4. Model","cacc8b50":"## 7. Inference","db7be4c9":"## 3. Preprocessing","9bb8f59d":"## 0. Was that you, RNN? \ud83e\udd14\n\n![I Doubt!](https:\/\/media.giphy.com\/media\/puOukoEvH4uAw\/giphy.gif)\n\n---\n\n### Was considering the mRNA sequence as a Sequence rather than a Graph Structre was a Mistake?\n\n**From:**\n![PublicLBRank.jpg](attachment:PublicLBRank.jpg)\n\n\n**To:**\n![PrivateLBRank.jpg](attachment:PrivateLBRank.jpg)\n\nThere could be a lot of other things that didn't go well with the private LB like, some features: [see this by @its7171](https:\/\/www.kaggle.com\/its7171\/dangerous-features).\n\n---\n\nAnyway, I learned a lot of new things and I will comeback even more stronger! \u270c\ud83c\udffb\n\n### Mistakes:\n\n* Didn\u2019t explore more about Test-set predictions & Train-set predictions after training.\n* Distance Based Features?? Overfitted for small sequence (Train & Public)??\n* Didn\u2019t change BPPs Features for Augmented data, used the same features as before.\n\n### INFERENCE after the Fall\n\n* Maybe, RNNs o\/p has higher target values than other models like GCN and Transformers.  ?? (https:\/\/www.kaggle.com\/c\/stanford-covid-vaccine\/discussion\/189182#1039989)\n* TTA by predicting probable different structures and average the predictions.\n* Use of BPPs matrix to get some kind of Embeddings. (https:\/\/www.kaggle.com\/c\/stanford-covid-vaccine\/discussion\/189236)\n* Use of external libraries to generate bpps matrix & structures\n* Self Supervised training - (Randomly dropping and predicting dropped nucleotides)\n* More weight to scored columns is loss function\n* Sample weights using errors or SNR.\n* GCN with Attention using BPPs and other Features\n\n\n### Some of the Top Solutions\n* [13th place](https:\/\/www.kaggle.com\/c\/stanford-covid-vaccine\/discussion\/189585)\n* [11th place](https:\/\/www.kaggle.com\/c\/stanford-covid-vaccine\/discussion\/189571)\n* [23rd place](https:\/\/www.kaggle.com\/c\/stanford-covid-vaccine\/discussion\/189594)\n* [BERT Solution](https:\/\/www.kaggle.com\/c\/stanford-covid-vaccine\/discussion\/189580)\n* [7th place](https:\/\/www.kaggle.com\/c\/stanford-covid-vaccine\/discussion\/189564)\n* [3rd place](https:\/\/www.kaggle.com\/c\/stanford-covid-vaccine\/discussion\/189574)\n* [1st place](https:\/\/www.kaggle.com\/c\/stanford-covid-vaccine\/discussion\/189620)\n\n\n### Things to look into\n* GCNs - (Edge Conv)\n* Transformers\n* Lookahead & RAdam Optimizers","47a635df":"## 6. Training","c0af3b91":"## 1. Import Necessary Libraries","39bad071":"### Validation Scheme","4e4777a6":"### Positional Encoding","df248a84":"### GRU-LSTMs","4f2611db":"### Transformers","99468572":"### Multiheaded Attention","acf906ed":"### Encoder-Layer","aac25abb":"## 5. LR Schedulers","e2f0a4c4":"## 2. Load Data","3b1e27dc":"### 8. Test set Predictions"}}