{"cell_type":{"69697665":"code","0e94bfc4":"code","5450a45a":"code","ad22ee1a":"code","0ee1cc61":"code","559661e5":"code","5e2b2196":"code","96c32048":"code","0ebf69e3":"code","5ea102ab":"code","0f92caf6":"code","059a45aa":"code","9ecf47b0":"code","96df0200":"code","36d600a1":"code","0347a3b0":"code","72c3f9f8":"code","1f44a9c0":"code","9d240943":"code","af108d3f":"code","be819703":"code","1d68d030":"code","e904829a":"code","f74fc690":"code","c120fff1":"code","ebaa3d8f":"code","844616ad":"code","7a94f029":"code","616aecb9":"code","7b70b2ed":"code","c209908c":"code","5fa54a7a":"code","8e8051d7":"code","b4267821":"code","d3f4a0dc":"code","7186ddbf":"markdown","a20805a4":"markdown","1a83a0a9":"markdown","ab2c8793":"markdown","e2f6d894":"markdown","f70a5f0e":"markdown","4a8213ac":"markdown","7ec888bf":"markdown","cb089eff":"markdown","8509cd73":"markdown","31f10e96":"markdown"},"source":{"69697665":"!rm -r \/opt\/conda\/lib\/python3.6\/site-packages\/lightgbm\n!git clone --recursive https:\/\/github.com\/Microsoft\/LightGBM","0e94bfc4":"!apt-get install -y -qq libboost-all-dev","5450a45a":"!ls","ad22ee1a":"%%bash\ncd LightGBM\nrm -r build\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=\/usr\/local\/cuda\/lib64\/libOpenCL.so -DOpenCL_INCLUDE_DIR=\/usr\/local\/cuda\/include\/ ..\nmake -j$(nproc)","0ee1cc61":"!cd LightGBM\/python-package\/;python3 setup.py install --precompile","559661e5":"!mkdir -p \/etc\/OpenCL\/vendors && echo \"libnvidia-opencl.so.1\" > \/etc\/OpenCL\/vendors\/nvidia.icd\n!rm -r LightGBM","5e2b2196":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","96c32048":"from tqdm import tqdm\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation, NMF\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.experimental import enable_hist_gradient_boosting  # noqa\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import roc_auc_score\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoost, Pool\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader","0ebf69e3":"is_local = False\nif is_local:\n    base_path = '..\/data\/homework-for-students4plus'\nelse:\n    base_path = '..\/input\/homework-for-students4plus'\n\ntrain_path = f'{base_path}\/train.csv'\ntest_path = f'{base_path}\/test.csv'\nsample_submission_path = f'{base_path}\/sample_submission.csv'\ndescription_path = f'{base_path}\/description.csv'\n\nspi_path = f'{base_path}\/spi.csv'\nus_gdp_by_state_path = f'{base_path}\/US_GDP_by_State.csv'\nstatelatlong_path = f'{base_path}\/statelatlong.csv'\nfree_zipcode_database_path = f'{base_path}\/free-zipcode-database.csv'\n\n\nTARGET = 'loan_condition'\nSEED = 0\n\ngrade_dict = {\n    'A': 1,\n    'B': 2,\n    'C': 3,\n    'D': 4,\n    'E': 5,\n    'F': 6,\n    'G': 7\n}\nsub_grade_dict = {\n    'A1': 1,\n    'A2': 2,\n    'A3': 3,\n    'A4': 4,\n    'A5': 5,\n    'B1': 6,\n    'B2': 7,\n    'B3': 8,\n    'B4': 9,\n    'B5': 10,\n    'C1': 11,\n    'C2': 12,\n    'C3': 13,\n    'C4': 14,\n    'C5': 15,\n    'D1': 16,\n    'D2': 17,\n    'D3': 18,\n    'D4': 19,\n    'D5': 20,\n    'E1': 21,\n    'E2': 22,\n    'E3': 23,\n    'E4': 24,\n    'E5': 25,\n    'F1': 26,\n    'F2': 27,\n    'F3': 28,\n    'F4': 29,\n    'F5': 30,\n    'G1': 31,\n    'G2': 32,\n    'G3': 33,\n    'G4': 34,\n    'G5': 35\n}\nemp_length_dict = {\n    'Nan': 0,\n    '< 1 year': 1,\n    '1 year': 2,\n    '2 years': 3,\n    '3 years': 4,\n    '4 years': 5,\n    '5 years': 6,\n    '6 years': 7,\n    '7 years': 8,\n    '8 years': 9,\n    '9 years': 10,\n    '10+ years': 11,\n}\nmonth_dict = {\n    'Nan': -1,\n    'Jan': 1,\n    'Feb': 2,\n    'Mar': 3,\n    'Apr': 4,\n    'May': 5,\n    'Jun': 6,\n    'Jul': 7,\n    'Aug': 8,\n    'Sep': 9,\n    'Oct': 10,\n    'Nov': 11,\n    'Dec': 12\n}","5ea102ab":"train_df = pd.read_csv(train_path)\ntest_df = pd.read_csv(test_path)\ntrain_df['istrain'] = 1\ntest_df['istrain'] = 0\n\nconcat_df = pd.concat([train_df, test_df], axis=0)\n\ndescription = pd.read_csv(description_path)\nus_gdp_by_state = pd.read_csv(us_gdp_by_state_path)\nstatelatlong = pd.read_csv(statelatlong_path)\nfree_zipcode_database = pd.read_csv(free_zipcode_database_path)","0f92caf6":"def merge_us_gdp_by_state(concat_df, us_gdp_by_state, statelatlong):\n    us_gdp_by_state = us_gdp_by_state.rename(columns={'State': 'City'})\n    us_gdp_by_state = pd.merge(us_gdp_by_state, statelatlong, on=\"City\", how=\"left\")\n    us_gdp_by_state = us_gdp_by_state.rename(columns={\"State\": \"addr_state\"})\n\n    us_gdp_by_state_group = us_gdp_by_state.groupby('addr_state').agg(\n        {\n            'State & Local Spending': [\"min\", \"max\", \"mean\"],\n            'Gross State Product': [\"min\", \"max\", \"mean\"],\n            'Real State Growth %': [\"min\", \"max\", \"mean\"],\n            'Population (million)': [\"min\", \"max\", \"mean\"]\n        }\n    )\n    us_gdp_by_state_group.columns = ['_'.join(col).strip() for col in us_gdp_by_state_group.columns.values]\n    us_gdp_by_state_group.reset_index(inplace=True)\n    concat_df = pd.merge(concat_df, us_gdp_by_state_group, on='addr_state', how=\"left\")\n\n    return concat_df\n\ndef merge_free_zipcode_database(concat_df, free_zipcode_database):\n    free_zipcode_database = free_zipcode_database.rename(columns={'State': 'addr_state'})\n    free_zipcode_database_group = free_zipcode_database.groupby('addr_state').agg(\n        {\n            'TaxReturnsFiled': [\"sum\", \"mean\"],\n            'EstimatedPopulation': [\"sum\", \"mean\"],\n            'TotalWages': [\"sum\", \"mean\"],\n        }\n    )\n\n    free_zipcode_database_group.columns = ['_'.join(col).strip() for col in free_zipcode_database_group.columns.values]\n    free_zipcode_database_group.reset_index(inplace=True)\n\n    free_zipcode_database_group['TaxReturnsFiled_sum_divide_EstimatedPopulation_sum'] = free_zipcode_database_group['TaxReturnsFiled_sum'] \/ free_zipcode_database_group['EstimatedPopulation_sum']\n    free_zipcode_database_group['TotalWages_sum_divide_EstimatedPopulation_sum'] = free_zipcode_database_group['TotalWages_sum'] \/ free_zipcode_database_group['EstimatedPopulation_sum']\n\n    concat_df = pd.merge(concat_df, free_zipcode_database_group, on='addr_state', how='left')\n\n    return concat_df","059a45aa":"def count_encoder(df, col):\n    df[f'{col}_count_enc'] = df.groupby([col])['ID'].transform('count')\n    return df\n\ndef FE_loan_amnt_annual_inc(concat_df):\n    FE_cols = [\n            'annual_inc',\n            'loan_amnt'\n        ]\n    for col in FE_cols:\n        # count_encoding\n        concat_df = count_encoder(concat_df, col)\n        # 1\u6841\u76ee\u30012\u6841\u76ee\u3092\u62bd\u51fa\n        concat_df[f'{col}_1digit'] = concat_df[col].fillna(0).astype(int).apply(lambda x: int(str(x)[-1]))\n        concat_df[f'{col}_2digit'] = concat_df[col].fillna(0).astype(int).apply(lambda x: int(str(x)[-2:]) if x >= 10 else 0)\n    # \u5c0f\u6570\u90e8\u3092\u53d6\u5f97\n    concat_df['annual_inc_decimal'] = concat_df[\"annual_inc\"].fillna(0).astype(str).apply(lambda x : x.split(\".\")[1]).astype(int)\n    # \u52a0\u6e1b\u4e57\u9664\n    concat_df['dti_times_annual_inc'] = concat_df['dti'] * concat_df['annual_inc']\n    concat_df['loan_amnt_divide_installment'] = concat_df['loan_amnt'] \/ concat_df['installment']\n\n    return concat_df\n\ndef custom_ordinal_encoding(concat_df):\n    concat_df['grade'] = concat_df['grade'].apply(lambda x: grade_dict[x])\n    concat_df['sub_grade'] = concat_df['sub_grade'].apply(lambda x: sub_grade_dict[x])\n    concat_df['emp_length'] = concat_df['emp_length'].fillna('Nan')\n    concat_df['emp_length'] = concat_df['emp_length'].apply(lambda x: emp_length_dict[x])\n\n    return concat_df\n\ndef target_encoding(concat_df, train_df, test_df):\n    cat_cols = [\n        'addr_state',\n        # 'application_type',\n        'emp_length',\n        'grade',\n        'home_ownership',\n        'initial_list_status',\n        'purpose',\n        'sub_grade',\n        'title',\n    ]\n    target_enc_cols = [f'{col}_target_enc' for col in cat_cols]\n    for col in cat_cols:\n        fold = StratifiedKFold(n_splits=5, random_state=SEED, shuffle=True)\n        train_df[f'{col}_target_enc'] = np.nan\n        for fold_, (train_idx, val_idx) in enumerate(fold.split(train_df, train_df[TARGET])):\n            train_group = train_df.loc[train_idx, :].groupby(col)[TARGET].mean()\n            train_mean = train_df.loc[train_idx, TARGET].mean()\n            train_df.loc[val_idx, f'{col}_target_enc'] = train_df.loc[val_idx, col].map(train_group).fillna(train_mean)\n        train_group = train_df.groupby(col)[TARGET].mean()\n        train_mean = train_df[TARGET].mean()\n        test_df[f'{col}_target_enc'] = test_df[col].map(train_group).fillna(train_mean)\n    concat_df_tmp = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n    concat_df = pd.merge(concat_df, concat_df_tmp[[\"ID\"] + target_enc_cols], how='left', on=\"ID\")\n\n    return concat_df\n\ndef combination_features(concat_df):\n    times_cols = [\n        ('dti', 'open_acc'),\n        ('open_acc', 'revol_util'),\n        ('dti', 'loan_amnt'),\n    ]\n    divide_cols = [\n        ('annual_inc', 'loan_amnt'),\n        ('annual_inc', 'open_acc'),\n        ('annual_inc', 'dti'),\n        ('installment', 'loan_amnt'),\n        ('loan_amnt', 'annual_inc'),\n        ('dti', 'annual_inc'),\n        ('open_acc', 'tot_cur_bal'),\n        ('tot_cur_bal', 'open_acc'),\n        ('open_acc', 'annual_inc'),\n        ('dti', 'tot_cur_bal'),\n        ('open_acc', 'total_acc'),\n        ('loan_amnt', 'tot_cur_bal'),\n        ('loan_amnt', 'revol_bal'),\n        ('revol_bal', 'dti'),\n\n    ]\n    for cols in times_cols:\n        concat_df[f'{cols[0]}_times_{cols[1]}'] = concat_df[cols[0]] * concat_df[cols[1]]\n    for cols in divide_cols:\n        concat_df[f'{cols[0]}_divide_{cols[1]}'] = concat_df[cols[0]] \/ concat_df[cols[1]]\n        concat_df[f'{cols[0]}_divide_{cols[1]}'] = concat_df[f'{cols[0]}_divide_{cols[1]}'].replace(np.inf, np.nan)\n    return concat_df","9ecf47b0":"def transform_month_year_col(df, col):\n    df[col] = df[col].fillna('Nan-0')\n    df[f'{col}_month'] = df[col].apply(lambda x: month_dict[x.split(\"-\")[0]])\n    df[f'{col}_year'] = df[col].apply(lambda x: x.split(\"-\")[1]).astype(int)\n    df = df.drop([col], axis=1)\n\n    return df\n\ndef transform_zip_code(df, col):\n    df[f'{col}_1'] = df[col].apply(lambda x: x[0]).astype(int)\n    df[f'{col}_2_3'] = df[col].apply(lambda x: x[1:3]).astype(int)\n    df[col] = df[col].apply(lambda x: x[0:3]).astype(int)\n    \n    return df\n\ndef ordinal_encoding(df, col):\n    df[col] = df[col].fillna(\"NaN\")\n    encoder = OrdinalEncoder()\n    df[col] = encoder.fit_transform(df[[col]])\n    return df\n\ndef log_normalizing(df, col):\n    df[col] = df[col].fillna(df[col].median())\n    df[col] = df[col].apply(np.log1p)\n    scaler = StandardScaler()\n    df[col] = scaler.fit_transform(df[[col]])\n    return df\n\ndef normalizing(df, col):\n    df[col] = df[col].fillna(df[col].median())\n    scaler = StandardScaler()\n    df[col] = scaler.fit_transform(df[[col]])\n    return df\n\ndef text_preprocess(df, col, n_components):\n    tfidf_vectorizer = TfidfVectorizer(\n        max_features=10000,\n        ngram_range=(1, 2), \n    )\n    truncated_svd = TruncatedSVD(\n        n_components=n_components,\n        random_state=SEED\n    )\n    df[col] = df[col].fillna('')\n    tfidf_matrix = tfidf_vectorizer.fit_transform(df[col])\n    svd_df = pd.DataFrame(truncated_svd.fit_transform(tfidf_matrix))\n    svd_df = svd_df.add_prefix(f'SVD_{col}_')\n    df = df.drop([col], axis=1)\n    df = pd.concat([df.reset_index(drop=True), svd_df], axis=1)\n    \n    return df","96df0200":"def baseline_preprocess(concat_df):\n    month_year_cols = [\n        'earliest_cr_line',\n        'issue_d'\n    ]\n    cat_cols = [\n        'addr_state',\n        'application_type',\n        'home_ownership',\n        'initial_list_status',\n        'purpose',\n        #'title',# test\u306bnunique\u6570\u304c\u5c11\u306a\u3044\u306e\u3067\u5de5\u592b\u5fc5\u8981\u307e\u305f\u306f\u5229\u7528\u3057\u306a\u3044\n    ]\n\n    num_cols = [\n        'acc_now_delinq',\n        'annual_inc',\n        'collections_12_mths_ex_med',\n        'delinq_2yrs',\n        'dti',\n        'inq_last_6mths',\n        'installment',\n        'loan_amnt', #\u30d4\u30fc\u30af\u90e8\u5206\u3092\u30ab\u30c6\u30b4\u30ea\u5316\u3057\u305f\u3044\n        'mths_since_last_delinq',\n        'mths_since_last_major_derog',\n        'mths_since_last_record',\n        'open_acc',\n        'pub_rec',\n        'revol_bal',\n        'revol_util',\n        'tot_coll_amt',# train\u3068test\u306e\u5206\u5e03\u7570\u306a\u308b\u306e\u3067\u6ce8\u610f\n        'tot_cur_bal',# train\u3068test\u306e\u5206\u5e03\u7570\u306a\u308b\u306e\u3067\u6ce8\u610f\n        'total_acc'\n    ]\n    text_cols = [\n        'emp_title'\n    ]\n    remove_features = ['title']\n    \n    # zip_code\u306e\u5909\u63db\n    concat_df = transform_zip_code(concat_df, 'zip_code') \n    # month_year_cols\u306e\u5909\u63db\n    for col in month_year_cols:\n        concat_df = transform_month_year_col(concat_df, col)\n\n    # category\u3092ordinal encoding\u306b\u5909\u63db\n    for col in cat_cols:\n        concat_df = ordinal_encoding(concat_df, col)\n\n    # text\u3092tf-idf\u5909\u63db\n    for col in text_cols:\n        concat_df = text_preprocess(concat_df, col, 20)\n    \n    concat_df = concat_df.drop(remove_features, axis=1)\n    \n    return concat_df","36d600a1":"def feature_engineering(concat_df, train_df, test_df):\n    concat_df = merge_us_gdp_by_state(concat_df, us_gdp_by_state, statelatlong)\n    concat_df = merge_free_zipcode_database(concat_df, free_zipcode_database)\n    concat_df = FE_loan_amnt_annual_inc(concat_df)\n    concat_df = custom_ordinal_encoding(concat_df)\n    concat_df = target_encoding(concat_df, train_df, test_df)\n    concat_df = baseline_preprocess(concat_df)\n    concat_df = combination_features(concat_df)\n    return concat_df","0347a3b0":"concat_df = feature_engineering(concat_df, train_df, test_df)","72c3f9f8":"concat_df.head().T","1f44a9c0":"def create_nn_df(concat_df):\n    concat_nn_df = concat_df.copy()\n    cat_cols = [\n        \n    ]\n    enc_cat_cols = [\n        'addr_state',\n        'application_type',\n        'home_ownership',\n        'initial_list_status',\n        'purpose',\n        'grade',\n        'sub_grade',\n        'emp_length',\n        'zip_code',\n        'loan_amnt_1digit',\n        'loan_amnt_2digit',\n        'annual_inc_decimal',\n        'zip_code_1',\n        'zip_code_2_3',\n    ]\n    log_num_cols = [\n        'loan_amnt',\n        'delinq_2yrs',\n        'inq_last_6mths',\n        'pub_rec',\n        'collections_12_mths_ex_med',\n        'acc_now_delinq',\n    ]\n    \n    useless_cols = [\n        'ID',\n        'istrain',\n        TARGET,\n    ]\n    \n    for col in enc_cat_cols:\n        concat_nn_df = ordinal_encoding(concat_nn_df, col)\n    \n    for col in log_num_cols:\n        concat_nn_df = log_normalizing(concat_nn_df, col)\n    \n    for col in concat_df.columns:\n        if col not in cat_cols + enc_cat_cols + log_num_cols + useless_cols:\n            concat_nn_df = normalizing(concat_nn_df, col)\n\n    return concat_nn_df","9d240943":"concat_nn_df = create_nn_df(concat_df)","af108d3f":"concat_nn_df.head()","be819703":"train_df = concat_df[concat_df['istrain'] == 1]\ntest_df  = concat_df[concat_df['istrain'] == 0]\n\ntrain_x = train_df.drop([TARGET, 'istrain'], axis=1)\ntrain_y = train_df[TARGET]\ntest_x = test_df.drop([TARGET, 'istrain'], axis=1)","1d68d030":"def training_lightgbm(max_depth, output_key):\n    fold = StratifiedKFold(n_splits=5, random_state=SEED, shuffle=True)\n    sub = pd.DataFrame()\n    sub[\"ID\"] = pd.read_csv(sample_submission_path)['ID']\n    sub[TARGET] = 0\n    oof = pd.DataFrame()\n    oof[\"ID\"] = train_x[\"ID\"]\n    oof[TARGET] = np.nan\n    use_cols = [col for col in train_x.columns if col != 'ID']\n\n    for fold_, (train_idx, val_idx) in tqdm(enumerate(fold.split(train_x, train_y))):\n        train_data = lgb.Dataset(train_x.loc[train_idx, use_cols], label=train_y[train_idx])\n        valid_data = lgb.Dataset(train_x.loc[val_idx, use_cols], label=train_y[val_idx])\n\n        lgb_params = {\n            'boosting_type': 'gbdt',\n            'objective': 'binary',\n            'num_leaves': 2**max_depth,\n            'metric': 'auc',\n            'max_depth': -1,\n            'learning_rate': 0.01,\n            'min_data_in_leaf': 800,\n            'verbose': -1,\n            'n_estimators': 5000,\n            'subsample': 0.8,\n            'subsample_freq': 1,\n            'colsample_bytree': 0.8,\n            'random_state': SEED,\n            'device': 'gpu',\n            'gpu_platform_id': 0,\n            'gpu_device_id': 0\n        }\n        estimator = lgb.train(\n            lgb_params,\n            train_data,\n            valid_sets = [valid_data],\n            verbose_eval = 100,\n            early_stopping_rounds=50\n        )\n\n        oof.loc[val_idx, TARGET] = estimator.predict(train_x.loc[val_idx, use_cols])\n        sub[TARGET] = sub[TARGET] + estimator.predict(test_x.loc[:, use_cols]) \/ 5\n    print(roc_auc_score(train_y, oof[TARGET]))\n    oof.to_csv(f'oof_{output_key}.csv', index=False)\n    sub.to_csv(f'submit_{output_key}.csv', index=False)\n\ntraining_lightgbm(7, 'lightgbm_7')\ntraining_lightgbm(8, 'lightgbm_8')\ntraining_lightgbm(9, 'lightgbm_9')","e904829a":"def training_catboost(max_depth, output_key):\n    fold = StratifiedKFold(n_splits=5, random_state=SEED, shuffle=True)\n    sub = pd.DataFrame()\n    sub[\"ID\"] = pd.read_csv(sample_submission_path)['ID']\n    sub[TARGET] = 0\n    oof = pd.DataFrame()\n    oof[\"ID\"] = train_x[\"ID\"]\n    oof[TARGET] = np.nan\n    use_cols = [col for col in train_x.columns if col != 'ID']\n\n    for fold_, (train_idx, val_idx) in tqdm(enumerate(fold.split(train_x, train_y))):\n        train_data = Pool(train_x.loc[train_idx, use_cols], label=train_y[train_idx])\n        valid_data = Pool(train_x.loc[val_idx, use_cols], label=train_y[val_idx])\n        test_data = Pool(test_x.loc[:, use_cols])\n        cat_params = {\n            'objective': 'Logloss',\n            'max_depth': max_depth,\n            'custom_metric': 'AUC',\n            'eval_metric': 'AUC',\n            'learning_rate': 0.01,\n            'verbose': 0,\n            'n_estimators': 10000,\n            'random_state': SEED,\n            'early_stopping_rounds': 50,\n            'task_type': 'GPU'\n        }\n        estimator = CatBoost(\n            cat_params\n        )\n        estimator.fit(train_data, eval_set=valid_data, early_stopping_rounds=50)\n        oof.loc[val_idx, TARGET] = estimator.predict(valid_data, prediction_type='Probability')[:,1]\n        sub[TARGET] = sub[TARGET] + estimator.predict(test_data, prediction_type='Probability')[:,1] \/ 5\n    print(roc_auc_score(train_y, oof[TARGET]))\n    oof.to_csv(f'oof_{output_key}.csv', index=False)\n    sub.to_csv(f'submit_{output_key}.csv', index=False)\n\ntraining_catboost(7, 'catboost_7')\ntraining_catboost(8, 'catboost_8')\ntraining_catboost(9, 'catboost_9')","f74fc690":"def training_xgboost(max_depth, output_key):\n    fold = StratifiedKFold(n_splits=5, random_state=SEED, shuffle=True)\n    sub = pd.DataFrame()\n    sub[\"ID\"] = pd.read_csv(sample_submission_path)['ID']\n    sub[TARGET] = 0\n    oof = pd.DataFrame()\n    oof[\"ID\"] = train_x[\"ID\"]\n    oof[TARGET] = np.nan\n    use_cols = [col for col in train_x.columns if col != 'ID']\n    \n    for fold_, (train_idx, val_idx) in tqdm(enumerate(fold.split(train_x, train_y))):\n        train_data = xgb.DMatrix(train_x.loc[train_idx, use_cols], label=train_y[train_idx])\n        valid_data = xgb.DMatrix(train_x.loc[val_idx, use_cols], label=train_y[val_idx])\n        test_data = xgb.DMatrix(test_x.loc[:, use_cols])\n        xgb_params = {\n            'objective': 'binary:logistic',\n            'max_depth': max_depth,\n            'eval_metric': 'auc',\n            'learning_rate': 0.02,\n            'min_child_weight': 100,\n            'verbosity': 0,\n            'subsample': 0.8,\n            'colsample_bytree': 0.8,\n            'random_state': SEED,\n            'tree_method': 'gpu_hist'\n        }\n        estimator = xgb.train(\n            xgb_params,\n            train_data,\n            num_boost_round=5000,\n            evals=[(valid_data, \"valid\")],\n            verbose_eval = 100,\n            early_stopping_rounds=50\n        )\n        oof.loc[val_idx, TARGET] = estimator.predict(valid_data)\n        sub[TARGET] = sub[TARGET] + estimator.predict(test_data) \/ 5\n    print(roc_auc_score(train_y, oof[TARGET]))\n    oof.to_csv(f'oof_{output_key}.csv', index=False)\n    sub.to_csv(f'submit_{output_key}.csv', index=False)\n\ntraining_xgboost(7, 'xgboost_7')\ntraining_xgboost(8, 'xgboost_8')\ntraining_xgboost(9, 'xgboost_9')","c120fff1":"train_nn_df = concat_nn_df[concat_nn_df['istrain'] == 1]\ntest_nn_df  = concat_nn_df[concat_nn_df['istrain'] == 0]\n\ntrain_nn_x = train_nn_df.drop([TARGET, 'istrain'], axis=1)\ntrain_nn_y = train_nn_df[TARGET]\ntest_nn_x = test_nn_df.drop([TARGET, 'istrain'], axis=1)","ebaa3d8f":"# https:\/\/github.com\/yashu-seth\/pytorch-tabular\nclass TabularDataset(Dataset):\n    def __init__(self, X, y=None, cat_cols=None,):\n\n        self.n = X.shape[0]\n        if y is not None:\n            self.y = y.reset_index(drop=True).values.reshape(-1, 1)\n        else:\n            self.y = None\n        \n        self.cat_cols = cat_cols if cat_cols else []\n        self.cont_cols = [\n            col for col in X.columns if col not in self.cat_cols\n        ]\n\n        if self.cont_cols:\n            self.cont_X = X[self.cont_cols].reset_index(drop=True).astype(np.float32).values\n        else:\n            self.cont_X = np.zeros((self.n, 1))\n\n        if self.cat_cols:\n            self.cat_X = X[cat_cols].reset_index(drop=True).astype(np.int64).values\n        else:\n            self.cat_X = np.zeros((self.n, 1))\n        self.cont_X = torch.Tensor(self.cont_X)\n        self.cat_X = torch.Tensor(self.cat_X)\n        \n        self.cont_X = self.cont_X.to(torch.float)\n        self.cat_X = self.cat_X.to(torch.long)\n        \n        if y is not None:\n            self.y = torch.Tensor(self.y)\n            self.y = self.y.to(torch.float)\n        \n    def __len__(self):\n        \"\"\"\n        Denotes the total number of samples.\n        \"\"\"\n        return self.n\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Generates one sample of data.\n        \"\"\"\n        if self.y is not None:\n            return [self.cont_X[idx], self.cat_X[idx], self.y[idx]]\n        else:\n            return [self.cont_X[idx], self.cat_X[idx]]\n\nclass FullyConnectedModule(nn.Module):\n    def __init__(\n        self,\n        input_dim,\n        output_dim\n    ):\n        super(FullyConnectedModule, self).__init__()\n        self.linear_layer = nn.Linear(input_dim, output_dim)\n        self.bn_layer = nn.BatchNorm1d(output_dim)\n\n    def forward(self, X):\n        X = self.linear_layer(X)\n        X = F.relu(self.bn_layer(X))\n        return X\n\n\nclass SkipNN(nn.Module):\n    def __init__(\n        self,\n        input_dim\n    ):\n        super(SkipNN, self).__init__()\n        # linear layers\n        self.fc_layer1 = FullyConnectedModule(input_dim, 256)\n        self.fc_layer2 = FullyConnectedModule(256, 256)\n        self.fc_layer3 = FullyConnectedModule(256, 256)\n        self.fc_layer4 = FullyConnectedModule(256, input_dim)\n        # Batch norm layers\n\n        # dropout layers\n        self.dropout_layer = nn.Dropout(0.5)\n    def forward(self, X):\n        X_skip = self.fc_layer1(X) # input_dim -> 256\n        X = self.fc_layer2(X_skip) # 256 -> 256\n        X = self.dropout_layer(X) # 256\n        X = self.fc_layer3(X) + X_skip # 256 -> 256\n        X = self.fc_layer4(X) # 256 -> input_dim\n        return X        \n\nclass FeedForwardNN(nn.Module):\n    def __init__(\n        self,\n        emb_dims,\n        no_of_cont,\n        lin_layer_sizes,\n        output_size,\n        emb_dropout,\n        lin_layer_dropouts,\n    ):\n\n        \"\"\"\n        Parameters\n        ----------\n        emb_dims: List of two element tuples\n        This list will contain a two element tuple for each\n        categorical feature. The first element of a tuple will\n        denote the number of unique values of the categorical\n        feature. The second element will denote the embedding\n        dimension to be used for that feature.\n        no_of_cont: Integer\n        The number of continuous features in the data.\n        lin_layer_sizes: List of integers.\n        The size of each linear layer. The length will be equal\n        to the total number\n        of linear layers in the network.\n        output_size: Integer\n        The size of the final output.\n        emb_dropout: Float\n        The dropout to be used after the embedding layers.\n        lin_layer_dropouts: List of floats\n        The dropouts to be used after each linear layer.\n        \"\"\"\n\n        super().__init__()\n\n        # Embedding layers\n        self.emb_layers = nn.ModuleList([nn.Embedding(x, y) for x, y in emb_dims])\n\n        no_of_embs = sum([y for x, y in emb_dims])\n        self.no_of_embs = no_of_embs\n        self.no_of_cont = no_of_cont\n        # Linear Layers\n        first_lin_layer = nn.Linear(\n            self.no_of_embs + self.no_of_cont, lin_layer_sizes[0]\n        )\n\n        self.lin_layers = nn.ModuleList(\n            [first_lin_layer]\n            + [\n                nn.Linear(lin_layer_sizes[i], lin_layer_sizes[i + 1])\n                for i in range(len(lin_layer_sizes) - 1)\n            ]\n        )\n\n        for lin_layer in self.lin_layers:\n            nn.init.kaiming_normal_(lin_layer.weight.data)\n\n        # Output Layer\n        self.output_layer = nn.Linear(lin_layer_sizes[-1], output_size)\n        nn.init.kaiming_normal_(self.output_layer.weight.data)\n\n        # Batch Norm Layers\n        self.first_bn_layer = nn.BatchNorm1d(self.no_of_cont)\n        self.bn_layers = nn.ModuleList(\n            [nn.BatchNorm1d(size) for size in lin_layer_sizes]\n        )\n\n        # Dropout Layers\n        self.emb_dropout_layer = nn.Dropout(emb_dropout)\n        self.droput_layers = nn.ModuleList(\n            [nn.Dropout(size) for size in lin_layer_dropouts]\n        )\n\n    def forward(self, cont_data, cat_data):\n        if self.no_of_embs != 0:\n            #print(cat_data)\n            x = [\n                emb_layer(cat_data[:, i]) for i, emb_layer in enumerate(self.emb_layers)\n            ]\n            x = torch.cat(x, 1)\n            x = self.emb_dropout_layer(x)\n        \n        if self.no_of_cont != 0:\n            normalized_cont_data = self.first_bn_layer(cont_data)\n\n            if self.no_of_embs != 0:\n                x = torch.cat([x, normalized_cont_data], 1)\n            else:\n                x = normalized_cont_data\n        for lin_layer, dropout_layer, bn_layer in zip(\n            self.lin_layers, self.droput_layers, self.bn_layers\n        ):\n\n            x = F.relu(lin_layer(x))\n            x = bn_layer(x)\n            x = dropout_layer(x)\n\n        x = self.output_layer(x)\n        return x\n\nclass FeedForwardNNAddSkipNN(nn.Module):\n    def __init__(\n        self,\n        emb_dims,\n        no_of_cont,\n        lin_layer_sizes,\n        output_size,\n        emb_dropout,\n        lin_layer_dropouts,\n    ):\n\n        \"\"\"\n        Parameters\n        ----------\n        emb_dims: List of two element tuples\n        This list will contain a two element tuple for each\n        categorical feature. The first element of a tuple will\n        denote the number of unique values of the categorical\n        feature. The second element will denote the embedding\n        dimension to be used for that feature.\n        no_of_cont: Integer\n        The number of continuous features in the data.\n        lin_layer_sizes: List of integers.\n        The size of each linear layer. The length will be equal\n        to the total number\n        of linear layers in the network.\n        output_size: Integer\n        The size of the final output.\n        emb_dropout: Float\n        The dropout to be used after the embedding layers.\n        lin_layer_dropouts: List of floats\n        The dropouts to be used after each linear layer.\n        \"\"\"\n\n        super().__init__()\n\n        # Embedding layers\n        self.emb_layers = nn.ModuleList([nn.Embedding(x, y) for x, y in emb_dims])\n\n        no_of_embs = sum([y for x, y in emb_dims])\n        self.no_of_embs = no_of_embs\n        self.no_of_cont = no_of_cont\n        self.skip_layer = SkipNN(self.no_of_embs + self.no_of_cont)\n        # Linear Layers\n        first_lin_layer = nn.Linear(\n            self.no_of_embs + self.no_of_cont, lin_layer_sizes[0]\n        )\n\n        self.lin_layers = nn.ModuleList(\n            [first_lin_layer]\n            + [\n                nn.Linear(lin_layer_sizes[i], lin_layer_sizes[i + 1])\n                for i in range(len(lin_layer_sizes) - 1)\n            ]\n        )\n\n        for lin_layer in self.lin_layers:\n            nn.init.kaiming_normal_(lin_layer.weight.data)\n\n        # Output Layer\n        self.output_layer = nn.Linear(lin_layer_sizes[-1], output_size)\n        nn.init.kaiming_normal_(self.output_layer.weight.data)\n\n        # Batch Norm Layers\n        self.first_bn_layer = nn.BatchNorm1d(self.no_of_cont)\n        self.bn_layers = nn.ModuleList(\n            [nn.BatchNorm1d(size) for size in lin_layer_sizes]\n        )\n\n        # Dropout Layers\n        self.emb_dropout_layer = nn.Dropout(emb_dropout)\n        self.droput_layers = nn.ModuleList(\n            [nn.Dropout(size) for size in lin_layer_dropouts]\n        )\n\n    def forward(self, cont_data, cat_data):\n        if self.no_of_embs != 0:\n            #print(cat_data)\n            x = [\n                emb_layer(cat_data[:, i]) for i, emb_layer in enumerate(self.emb_layers)\n            ]\n            x = torch.cat(x, 1)\n            x = self.emb_dropout_layer(x)\n        \n        if self.no_of_cont != 0:\n            normalized_cont_data = self.first_bn_layer(cont_data)\n\n            if self.no_of_embs != 0:\n                x = torch.cat([x, normalized_cont_data], 1)\n            else:\n                x = normalized_cont_data\n        x = self.skip_layer(x)\n        for lin_layer, dropout_layer, bn_layer in zip(\n            self.lin_layers, self.droput_layers, self.bn_layers\n        ):\n\n            x = F.relu(lin_layer(x))\n            x = bn_layer(x)\n            x = dropout_layer(x)\n\n        x = self.output_layer(x)\n        return x    ","844616ad":"def training_nn(use_skip_nn, output_key):\n    fold = StratifiedKFold(n_splits=5, random_state=SEED, shuffle=True)\n    sub = pd.DataFrame()\n    sub[\"ID\"] = pd.read_csv(sample_submission_path)['ID']\n    sub[TARGET] = 0\n    oof = pd.DataFrame()\n    oof[\"ID\"] = train_x[\"ID\"]\n    oof[TARGET] = np.nan\n    use_cols = [col for col in train_x.columns if col != 'ID']\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    batch_size = 1024\n    # category\u306e\u8a2d\u5b9a\n    cat_cols = [\n        'grade',\n        'sub_grade',\n        'emp_length',\n        'addr_state',\n        'application_type',\n        'home_ownership',\n        'initial_list_status',\n        'purpose',\n        'zip_code',\n        'loan_amnt_1digit',\n        'loan_amnt_2digit',\n        'annual_inc_decimal',\n        'zip_code_1',\n        'zip_code_2_3',\n    ]\n    emb_dims = []\n    for col in cat_cols:\n        emb_dim = concat_nn_df[col].nunique()\n        emb_dims.append((emb_dim, int(max(2, min(emb_dim \/ 2, 50)))))\n    no_of_cont = len(use_cols) - len(cat_cols)\n    # \u305d\u306e\u4ed6\u30d1\u30e9\u30e1\u30fc\u30bf\n    lin_layer_sizes = [256, 256]\n    emb_dropout = 0.5\n    lin_layer_dropouts = [0.5, 0.5]\n    \n    for fold_, (train_idx, val_idx) in tqdm(enumerate(fold.split(train_nn_x, train_nn_y))):\n        auc_tmp = 0\n        # \u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u5b9a\u7fa9\n        dataset_train = TabularDataset(\n            train_nn_x.loc[train_idx, use_cols],\n            train_nn_y[train_idx],\n            cat_cols=cat_cols\n        )\n        dataset_valid = TabularDataset(\n            train_nn_x.loc[val_idx, use_cols],\n            train_nn_y[val_idx],\n            cat_cols=cat_cols\n        )\n        dataset_test = TabularDataset(\n            test_nn_x.loc[:, use_cols],\n            None,\n            cat_cols=cat_cols\n        )\n        # \u30c7\u30fc\u30bf\u30ed\u30fc\u30c0\u30fc\u5b9a\u7fa9\n        dataloader_train = DataLoader(dataset_train, batch_size, shuffle=True)\n        dataloader_valid = DataLoader(dataset_valid, batch_size, shuffle=False)\n        dataloader_test = DataLoader(dataset_test, batch_size, shuffle=False)\n        # model\u5b9a\u7fa9\n        if use_skip_nn :\n            estimator = FeedForwardNNAddSkipNN(\n                emb_dims=emb_dims,\n                no_of_cont=no_of_cont,\n                lin_layer_sizes=lin_layer_sizes,\n                output_size=1,\n                emb_dropout=emb_dropout,\n                lin_layer_dropouts=lin_layer_dropouts\n            ).to(device)\n        else: \n            estimator = FeedForwardNN(\n                emb_dims=emb_dims,\n                no_of_cont=no_of_cont,\n                lin_layer_sizes=lin_layer_sizes,\n                output_size=1,\n                emb_dropout=emb_dropout,\n                lin_layer_dropouts=lin_layer_dropouts\n            ).to(device)\n        \n        criterion = nn.BCEWithLogitsLoss()\n        optimizer = torch.optim.Adam(estimator.parameters(), lr=0.01)\n        # epoch\u6570\u3060\u3051\u30eb\u30fc\u30d7\n        for i in range(15):\n            running_loss = 0.0\n            estimator.train()\n            # 1epoch\u5b66\u7fd2\n            for X_num_batch, X_cat_batch, y_batch in dataloader_train:\n                X_num_batch = X_num_batch.to(device)\n                X_cat_batch = X_cat_batch.to(device)\n                y_batch = y_batch.to(device)\n                preds = estimator(X_num_batch, X_cat_batch)\n                loss = criterion(preds, y_batch)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                running_loss += loss.item() \/ dataset_train.__len__()\n            # validation\u8a55\u4fa1\n            estimator.eval()\n            predictions = np.empty((0, 1))\n            for X_num_batch, X_cat_batch, y_batch in dataloader_valid:\n                X_num_batch = X_num_batch.to(device)\n                X_cat_batch = X_cat_batch.to(device)\n                output = torch.sigmoid(estimator(X_num_batch, X_cat_batch)).data.cpu().numpy()\n                predictions = np.concatenate([predictions, output], axis=0)\n            print(i, \": \", running_loss,\", \", roc_auc_score(train_nn_y.loc[val_idx], predictions.reshape(-1)))\n            # early stopping\u306e\u305f\u3081\u306b\u73fe\u5728\u304c\u6700\u9ad8\u30b9\u30b3\u30a2\u304b\u3069\u3046\u304b\u5224\u5b9a\n            if roc_auc_score(train_nn_y.loc[val_idx], predictions.reshape(-1)) < auc_tmp:\n                continue\n            # \u73fe\u6642\u70b9\u3067\u306e\u6700\u9ad8\u30b9\u30b3\u30a2\u306e\u5834\u5408\u306f\u4e88\u6e2c\u5024\u66f4\u65b0\n            auc_tmp = roc_auc_score(train_nn_y.loc[val_idx], predictions.reshape(-1))\n            oof.loc[val_idx, TARGET] = predictions.reshape(-1)\n            # test\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u4e88\u6e2c\u8a08\u7b97\n            predictions_test = np.empty((0, 1))\n            for X_num_batch, X_cat_batch in dataloader_test:\n                X_num_batch = X_num_batch.to(device)\n                X_cat_batch = X_cat_batch.to(device)\n                output = torch.sigmoid(estimator(X_num_batch, X_cat_batch)).data.cpu().numpy()\n                predictions_test = np.concatenate([predictions_test, output], axis=0)\n        sub[TARGET] = sub[TARGET] + predictions_test.reshape(-1)\/5\n    oof.to_csv(f'oof_{output_key}.csv', index=False)\n    sub.to_csv(f'submit_{output_key}.csv', index=False)\n    print(roc_auc_score(train_nn_y, oof[TARGET]))\ntraining_nn(False, 'nn_mlp')\n# training_nn(True, 'nn_skip')","7a94f029":"output_key = [\n    'lightgbm_7',\n    'lightgbm_8',\n    'lightgbm_9',\n    'catboost_7',\n    'catboost_8',\n    'catboost_9',\n    'xgboost_7',\n    'xgboost_8',\n    'xgboost_9',\n    'nn_mlp',\n    # 'nn_skip'\n]\nsub = pd.DataFrame()\nsub[\"ID\"] = pd.read_csv(sample_submission_path)['ID']\noof = pd.DataFrame()\noof[\"ID\"] = train_df[\"ID\"]\nfor key in output_key:\n    sub[key] = pd.read_csv(f'submit_{key}.csv')[TARGET]\n    oof[key] = pd.read_csv(f'oof_{key}.csv')[TARGET]","616aecb9":"oof.drop([\"ID\"], axis=1).corr('spearman')","7b70b2ed":"for key in output_key:\n    print(key, \": \", roc_auc_score(train_y, oof[key]))","c209908c":"sub","5fa54a7a":"print(roc_auc_score(train_y, oof.drop([\"ID\"], axis=1).mean(axis=1)))\nsubmit = pd.DataFrame()\nsubmit[\"ID\"] = pd.read_csv(sample_submission_path)['ID']\nsubmit[TARGET] = sub.drop([\"ID\"], axis=1).mean(axis=1)\nsubmit.to_csv(\"averaging.csv\", index=False)","8e8051d7":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression().fit(oof.drop([\"ID\"], axis=1), train_y)\nprint(roc_auc_score(train_y, clf.predict_proba(oof.drop([\"ID\"], axis=1))[:,1] ))\nsubmit = pd.DataFrame()\nsubmit[\"ID\"] = pd.read_csv(sample_submission_path)[\"ID\"]\nsubmit[TARGET] =  clf.predict_proba(sub.drop([\"ID\"], axis=1))[:,1]\nsubmit.to_csv(\"stacking.csv\", index=False)","b4267821":"print(roc_auc_score(train_y, np.sum(oof.drop([\"ID\"], axis=1).rank(), axis=1) \/ (oof.drop([\"ID\"], axis=1).shape[1] * oof.shape[0])))\nsubmit = pd.DataFrame()\nsubmit[\"ID\"] = pd.read_csv(sample_submission_path)['ID']\nsubmit[TARGET] =  np.sum(sub.drop([\"ID\"], axis=1).rank(), axis=1) \/ (sub.drop([\"ID\"], axis=1).shape[1] * sub.shape[0])\nsubmit.to_csv(\"rank_average.csv\", index=False)","d3f4a0dc":"oof.drop([\"ID\"], axis=1).shape[1]","7186ddbf":"## \u30c7\u30fc\u30bf\u306e\u30ed\u30fc\u30c9","a20805a4":"## \u30c7\u30fc\u30bf\u30d1\u30b9\u3001\u30b0\u30ed\u30fc\u30d0\u30eb\u5909\u6570\u306e\u5b9a\u7fa9","1a83a0a9":"### xgboost","ab2c8793":"### NN\u7528\u306b\u7279\u5fb4\u91cf\u4f5c\u6210","e2f6d894":"### lightgbm","f70a5f0e":"### \u7279\u5fb4\u91cf\u52a0\u5de5","4a8213ac":"## ensemble","7ec888bf":"## \u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406","cb089eff":"## \u30e2\u30c7\u30ea\u30f3\u30b0","8509cd73":"## neural network","31f10e96":"### catboost"}}