{"cell_type":{"da60b760":"code","7bf3ef88":"code","9f37d518":"code","482bed9e":"code","dbc8e596":"code","e16890f2":"code","9bbf68e7":"code","82243872":"code","85da10c7":"code","48bc2a56":"code","12e44a6a":"code","6088d582":"code","ebc6bde2":"code","7796f65e":"code","b4ab5e42":"code","1628ea46":"code","96578aaf":"code","20aa3958":"code","c8ada8fb":"code","7d1a6131":"code","3f23b3ae":"code","5e7c1caf":"code","0033ae49":"code","e87f6a79":"code","923be59b":"code","e50fb0b3":"markdown","1df03f36":"markdown","94a0a802":"markdown","d014f262":"markdown","1cd9be7a":"markdown"},"source":{"da60b760":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")","7bf3ef88":"data = pd.read_csv(\"..\/input\/entity-annotated-corpus\/ner_dataset.csv\", encoding=\"latin1\")\ndata = data.drop(['POS'], axis =1)\ndata = data.fillna(method=\"ffill\")\ndata.head(10)","9f37d518":"words = set(list(data['Word'].values))\nwords.add('PADword')\nn_words = len(words)\nn_words","482bed9e":"tags = list(set(data[\"Tag\"].values))\nn_tags = len(tags)\nn_tags","dbc8e596":"tags","e16890f2":"class SentenceGetter(object):\n    \n    def __init__(self, data):\n        self.n_sent = 1\n        self.data = data\n        self.empty = False\n        agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(),s[\"Tag\"].values.tolist())]\n        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n        self.sentences = [s for s in self.grouped]\n    \n    def get_next(self):\n        try:\n            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n            self.n_sent += 1\n            return s\n        except:\n            return None","9bbf68e7":"getter = SentenceGetter(data)\nsent = getter.get_next()\nprint(sent)","82243872":"sentences = getter.sentences\nprint(len(sentences))","85da10c7":"largest_sen = max(len(sen) for sen in sentences)\nprint('biggest sentence has {} words'.format(largest_sen))","48bc2a56":"%matplotlib inline\nplt.hist([len(sen) for sen in sentences], bins= 50)\nplt.show()","12e44a6a":"words2index = {w:i for i,w in enumerate(words)}\ntags2index = {t:i for i,t in enumerate(tags)}\nprint(words2index['London'])\nprint(tags2index['B-geo'])","6088d582":"max_len = 50\nX = [[w[0]for w in s] for s in sentences]\nnew_X = []\nfor seq in X:\n    new_seq = []\n    for i in range(max_len):\n        try:\n            new_seq.append(seq[i])\n        except:\n            new_seq.append(\"PADword\")\n    new_X.append(new_seq)\nnew_X[15]","ebc6bde2":"from keras.preprocessing.sequence import pad_sequences\ny = [[tags2index[w[1]] for w in s] for s in sentences]\ny = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tags2index[\"O\"])\ny[15]","7796f65e":"from sklearn.model_selection import train_test_split\nX_tr, X_te, y_tr, y_te = train_test_split(new_X, y, test_size=0.1, random_state=2018)","b4ab5e42":"batch_size = 32\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nimport tensorflow_hub as hub\nfrom tensorflow.compat.v1.keras import backend as K\nsess = tf.Session()\nK.set_session(sess)","1628ea46":"elmo_model = hub.Module(\"https:\/\/tfhub.dev\/google\/elmo\/2\", trainable=True)\nsess.run(tf.global_variables_initializer())\nsess.run(tf.tables_initializer())","96578aaf":"def ElmoEmbedding(x):\n    return elmo_model(inputs={\n                            \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n                            \"sequence_len\": tf.constant(batch_size*[max_len])\n                      },\n                      signature=\"tokens\",\n                      as_dict=True)[\"elmo\"]","20aa3958":"\nfrom keras.models import Model, Input\nfrom keras.layers.merge import add\nfrom keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda","c8ada8fb":"\ninput_text = Input(shape=(max_len,), dtype=tf.string)\nembedding = Lambda(ElmoEmbedding, output_shape=(max_len, 1024))(input_text)\nx = Bidirectional(LSTM(units=512, return_sequences=True,\n                       recurrent_dropout=0.2, dropout=0.2))(embedding)\nx_rnn = Bidirectional(LSTM(units=512, return_sequences=True,\n                           recurrent_dropout=0.2, dropout=0.2))(x)\nx = add([x, x_rnn])  # residual connection to the first biLSTM\nout = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(x)","7d1a6131":"model = Model(input_text, out)\nmodel.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])","3f23b3ae":"X_tr, X_val = X_tr[:1213*batch_size], X_tr[-135*batch_size:]\ny_tr, y_val = y_tr[:1213*batch_size], y_tr[-135*batch_size:]\ny_tr = y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)\ny_val = y_val.reshape(y_val.shape[0], y_val.shape[1], 1)","5e7c1caf":"history = model.fit(np.array(X_tr), y_tr, validation_data=(np.array(X_val), y_val),\n                    batch_size=batch_size, epochs=3, verbose=1)","0033ae49":"X_te = X_te[:149*batch_size]\ntest_pred = model.predict(np.array(X_te), verbose=1)","e87f6a79":"\nidx2tag = {i: w for w, i in tags2index.items()}\n\ndef pred2label(pred):\n    out = []\n    for pred_i in pred:\n        out_i = []\n        for p in pred_i:\n            p_i = np.argmax(p)\n            out_i.append(idx2tag[p_i].replace(\"PADword\", \"O\"))\n        out.append(out_i)\n    return out\n\ndef test2label(pred):\n    out = []\n    for pred_i in pred:\n        out_i = []\n        for p in pred_i:\n            out_i.append(idx2tag[p].replace(\"PADword\", \"O\"))\n        out.append(out_i)\n    return out\n    \npred_labels = pred2label(test_pred)\ntest_labels = test2label(y_te[:149*32])","923be59b":"i = 390\np = model.predict(np.array(X_te[i:i+batch_size]))[0]\np = np.argmax(p, axis=-1)\nprint(\"{:15} {:15}: ({})\".format(\"Word\", \"Predicted tag\", \"True tag\"))\nprint(\"=\"*44)\nfor w, true, pred in zip(X_te[i], y_te[i], p):\n    if w != \"PADword\":\n        print(\"{:15}:{:15} ({})\".format(w, tags[pred], tags[true]))","e50fb0b3":"length distribution of the sentences. ","1df03f36":"# Data set\nAtfirst read the dataset. The ner_dataset.csv contains sentence no, word, parts of speech and tags for the NER task. As we do not need the POSs, we drop this column. ","94a0a802":"Now we get the unique words and the unique tags. Later we need to pad the sentences to the maximum length. For this we use a dummy word 'PADword'.","d014f262":"Now we need a way to make touples containing words and tags. For this, we declear the SentenceGetter class. We also declear a method get_next() for debugging and visualization purpose.","1cd9be7a":"Let's have a look how a sentence looks like after converting it into a list of (word, tag) tuples. Letter we fetch all the sentences as a list of tuples. "}}