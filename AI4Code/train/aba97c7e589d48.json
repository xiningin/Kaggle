{"cell_type":{"c2601144":"code","bd698d57":"code","d738a1e2":"code","a0dd88f6":"code","189368a1":"code","fea3ea22":"code","e9bb1e09":"code","f74098f3":"code","7423a904":"code","a2d97038":"code","cabbed64":"code","38622647":"code","7946053a":"code","307e7d0c":"code","8b1f740e":"code","2f0a0cae":"code","5977e701":"code","b659afeb":"code","47e1d48c":"code","03e209d0":"code","deb22bc4":"code","eee1ebc4":"code","1492e497":"code","fdedd688":"code","bef19003":"code","3dcc71c3":"code","5b58ab78":"code","43e8009d":"code","7f07c150":"code","fc778a2b":"code","fbf7e4e9":"code","d3e5998b":"code","19b1cd19":"code","0222fc48":"code","2105c9b6":"code","f48fecdb":"code","befb8114":"code","fb904128":"code","6de966ff":"code","86857a5e":"code","7759425d":"code","892145ae":"code","a053086f":"code","eb845d49":"code","286a6566":"code","a50bd0e8":"code","4f7b1341":"code","f18eafb8":"code","51fcb5d8":"code","7eeffe9a":"code","45f8fe81":"code","4053a6ac":"code","afd37bff":"code","a8c6e128":"code","701266af":"code","721d9d65":"code","78ea18c9":"code","aceebc61":"code","c66dd0b7":"code","1a54061e":"code","b597f2ad":"code","072cfaf3":"code","82b87423":"code","1bf5fac3":"code","a42ff5fd":"code","51c44616":"markdown","9c5bb1da":"markdown","15fde2bb":"markdown","b63578bd":"markdown","5b3806e8":"markdown","a249865f":"markdown","2d9630d7":"markdown","1e9499b9":"markdown","d5ac5842":"markdown","c1cd843a":"markdown","c500ed81":"markdown","49bd207d":"markdown","4d4175df":"markdown","6212a9bd":"markdown","55346f97":"markdown","5a201f00":"markdown","a48bfbfc":"markdown","6ad3e466":"markdown","9d068c4d":"markdown","153e7d53":"markdown","216670f0":"markdown","08f00704":"markdown"},"source":{"c2601144":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os,gc\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.options.display.max_columns = 50","bd698d57":"%%time\ntrain = pd.read_csv('..\/input\/cat-in-the-dat-ii\/train.csv')\ntest  = pd.read_csv('..\/input\/cat-in-the-dat-ii\/test.csv')\nBIN_COL  = [s for s in train.columns if 'bin_' in s]\nNOM_COL  = [s for s in train.columns if 'nom_' in s]\nORD_COL  = [s for s in train.columns if 'ord_' in s]\nNOM_5_9  = ['nom_5','nom_6','nom_7','nom_8','nom_9']\nDATE_COL = ['day','month']","d738a1e2":"def plot_corr(annot=False):\n    plt.figure(figsize=(12,6))\n    plt.title(f'Correlation coefficient heatmap(train data)')\n    sns.heatmap(train.drop(columns='id').corr(), annot=annot, vmin=-1.0, cmap='Spectral')\n    plt.show()\n    plt.figure(figsize=(12, 6))\n    plt.title(f'Correlation coefficient heatmap(test data)')\n    sns.heatmap(test.drop(columns='id').corr(), annot=annot, vmin=-1.0, cmap='Spectral')\n    \ndef plot_line(col_name):\n    plt.figure(figsize=(12,3))\n    sns.lineplot(train[col_name], train.target)\n    plt.show()\n\ndef plot_traintest(col_name, h):\n#     col_name = f'ord_{i}'\n    fig, ax = plt.subplots(1, 3, figsize=(15, h))\n    _order = sorted(list(set(train[col_name].dropna().unique()) & \\\n                  set(test[col_name].dropna().unique())))\n    ax[0].set_title(f'train data {col_name}')\n    ax[1].set_title(f'test data {col_name}')\n    ax[2].set_title(f'train data {col_name} per target')\n    if train[col_name].nunique()>8:\n        sns.countplot(y=train[col_name], order=_order, ax=ax[0])\n        sns.countplot(y=test[col_name],  order=_order, ax=ax[1])\n    else:\n        tmp = train[col_name].value_counts(dropna=False)\n        ax[0].pie(tmp, labels= tmp.index, autopct='%1.1f%%',\n                 shadow=True, startangle=90)\n        tmp = test[col_name].value_counts(dropna=False)\n        ax[1].pie(tmp, labels= tmp.index, autopct='%1.1f%%',\n                  shadow=True, startangle=90)\n    \n    sns.countplot(y=train[col_name], order=_order, hue=train['target'], ax=ax[2])\n    plt.tight_layout()","a0dd88f6":"train.info()\ntest.info()","189368a1":"import missingno as msno\nmsno.heatmap(train.drop(columns=['id','target']))\nplt.show()\nmsno.heatmap(test.drop(columns=['id']))","fea3ea22":"feats = test.drop(columns=['id']).columns\nprint('1.null data size --')\ntmp_df = pd.concat([train[feats].isnull().sum(),\n                 test[feats].isnull().sum()],axis=1).rename(columns={0:'train',1:'test'})\nprint(tmp_df)\nprint()\nprint('2.null data rate -------')\ntmp_df['train'] = tmp_df['train']\/len(train)*100\ntmp_df['test']  = tmp_df['test']\/len(test)*100\nprint(tmp_df)\ndel tmp_df;gc.collect()","e9bb1e09":"%%time\ntrain['null_count'] = train.isnull().sum(axis=1)\ntest['null_count']  = test.isnull().sum(axis=1)\nprint('train null_count value_counts-----------')\nprint(train['null_count'].value_counts())\nprint('test null_count value_counts-----------')\nprint(test['null_count'].value_counts())\ntrain.null_count = np.clip(train.null_count,0,6) \n# train.null_count = np.clip(train.null_count,0,2) \n\nfor col in test.drop(columns=['id','null_count']).columns.tolist():\n    train[f'{col}_missing'] = train[col].isnull().astype(int)\n    test[f'{col}_missing']  = test[col].isnull().astype(int)    ","f74098f3":"sns.countplot(x='null_count', data=train,hue='target')","7423a904":"# feats = test.drop(columns=['id','null_count']).columns\n# for col1 in feats:\n#     for col2 in feats.drop(col1):\n#         if len(train[feats].dropna(subset=[col1])[col2].value_counts(dropna=False))<=3:\n#             print(f'{col1}, {col2}-----------------------------')\n#             print(train[feats].dropna(subset=[col1])[col2].value_counts(dropna=False).head(20))","a2d97038":"# feats = test.drop(columns=['id','null_count']).columns#.tolist()\n# for col1 in feats:\n#     for col2 in feats.drop(col1):\n#         if len(train.drop(columns=['id','null_count']).loc[\n#             train[col1].isnull(), col2].value_counts(dropna=False))<=3:\n#             print(f'{col1}, {col2}-----------------------------')\n#             print(train.drop(columns=['id','null_count']).loc[\n#                 train[col1].isnull(), col2].value_counts(dropna=False).head(2))\n#             print(test.drop(columns=['id','null_count']).loc[\n#                 train[col1].isnull(), col2].value_counts(dropna=False).head(2))","cabbed64":"plt.title(f'target==1 ratio {len(train[train.target==1]) \/ len(train)}' )\nsns.countplot(train[f'target'])","38622647":"display(train.head(5))\ndisplay(test.head(5))","7946053a":"plot_corr(annot=False)","307e7d0c":"gc.collect()","8b1f740e":"%time\ntmp_df = pd.concat([train.drop(columns=['target']).nunique(),\n                    test.nunique()],axis=1)\ntmp_df = pd.concat([tmp_df, pd.concat([train,test], axis=0).drop(columns=['target']).nunique()], axis=1)\ntmp_df = tmp_df.reset_index()\ntmp_df.columns = ['feature','train','test','all']\ntmp_df = tmp_df.loc[tmp_df.feature!='id'].reset_index(drop=True)\nprint(tmp_df)","2f0a0cae":"for col in test.drop(columns='id').columns:\n    if len(set(train[col].dropna().unique().tolist())^ set(test[col].dropna().unique().tolist()))>0:\n        print(col, \n              '(train only)', set(train[col].dropna().unique().tolist()) - set(test[col].dropna().unique().tolist()),    \n              '(test only)',  set(test[col].dropna().unique().tolist()) - set(train[col].dropna().unique().tolist())) \n\nprint(f'train only nom_5 count:', len(train[train['nom_5'].isin(['b3ad70fcb'])]))\nprint(f'train only nom_6 count:', len(train[train['nom_6'].isin(['f0732a795', 'ee6983c6d', '3a121fefb'])]))\nprint(f'test only nom_6 count:', len(test[test['nom_6'].isin(['a885aacec'])]))\nprint(f'train only nom_9 count:', len(train[train['nom_9'].isin(['3d19cd31d', '1065f10dd'])]))","5977e701":"%%time\nfor df in [train, test]:\n    df.bin_3.replace({'F':0, 'T':1}, inplace=True)\n    df.bin_4.replace({'N':0, 'Y':1}, inplace=True)","b659afeb":"for col in BIN_COL:\n    fig, ax = plt.subplots(1, 3, figsize=(15,6))\n#     sns.countplot(y=train[col], ax=ax[0])\n    tmp = train[col].value_counts(dropna=False)\n    ax[0].set_title(f'train data {col}')\n    ax[0].pie(tmp, labels= tmp.index, autopct='%1.1f%%',\n             shadow=True, startangle=90,\n             labeldistance=0.8)\n#     sns.countplot(y=test[col],  ax=ax[1])\n    tmp = test[col].value_counts(dropna=False)\n    ax[1].set_title(f'test data {col}')\n    ax[1].pie(tmp, labels= tmp.index, autopct='%1.1f%%',\n              shadow=True, startangle=90,\n              labeldistance=0.8,)\n    ax[2].set_title(f'train data {col} per target')\n    sns.countplot(train[col], hue=train['target'], ax=ax[2])\n\n    plt.show()","47e1d48c":"%%time\nord_1_map = {'Novice':1,'Contributor':2,'Expert':3,'Master':4,'Grandmaster':5}\nord_2_map = {'Freezing':1, 'Cold':2,'Warm':3,'Hot':4, 'Boiling Hot':5,'Lava Hot':6}\n\ntrain.loc[train['ord_1'].notnull(),'ord_1'] = train.loc[train['ord_1'].notnull(),'ord_1'].map(ord_1_map)\ntrain.loc[train['ord_2'].notnull(),'ord_2'] = train.loc[train['ord_2'].notnull(),'ord_2'].map(ord_2_map)\ntrain.loc[train['ord_3'].notnull(),'ord_3'] = train.loc[train['ord_3'].notnull(),'ord_3'].apply(lambda c: ord(c) - ord('a') + 1)\ntrain.loc[train['ord_4'].notnull(),'ord_4'] = train.loc[train['ord_4'].notnull(),'ord_4'].apply(lambda c: ord(c) - ord('A') + 1)\ntest.loc[test['ord_1'].notnull(),'ord_1']   = test.loc[test['ord_1'].notnull(),'ord_1'].map(ord_1_map)\ntest.loc[test['ord_2'].notnull(),'ord_2']   = test.loc[test['ord_2'].notnull(),'ord_2'].map(ord_2_map)\ntest.loc[test['ord_3'].notnull(),'ord_3']   = test.loc[test['ord_3'].notnull(),'ord_3'].apply(lambda c: ord(c) - ord('a') + 1)\ntest.loc[test['ord_4'].notnull(),'ord_4']   = test.loc[test['ord_4'].notnull(),'ord_4'].apply(lambda c: ord(c) - ord('A') + 1)\nfor i in range(1,5):\n    train[f'ord_{i}'] = train[f'ord_{i}'].astype(float)\n    test[f'ord_{i}']  = test[f'ord_{i}'].astype(float)","03e209d0":"for i, h in zip(range(5),[6,6,6,6,6]):\n    plot_traintest(f'ord_{i}',h)","deb22bc4":"# for i in range(5):\nfor i in range(6):\n    plot_line(f'ord_{i}')","eee1ebc4":"# %%time\n# for col in ['ord_5']:\n#     _map = pd.concat([train[col], test[col]]).value_counts().rank().to_dict()\n#     train[f'{col}_freq'] = train[col].map(_map)\n#     test[f'{col}_freq']  = test[col].map(_map)","1492e497":"%%time\nfor df in [train, test]:\n    df.loc[df.ord_5.notnull(), 'ord_5_1'] = df.loc[df.ord_5.notnull(), 'ord_5'].apply(lambda x: x[0])\n    df.loc[df.ord_5.notnull(), 'ord_5_2'] = df.loc[df.ord_5.notnull(), 'ord_5'].apply(lambda x: x[1])","fdedd688":"for i in range(1,3):\n    plot_line(f'ord_5_{i}')","bef19003":"%%time\nfor col in ['ord_5_1', 'ord_5_2']:\n    _map = pd.concat([train[col], test[col]]).value_counts().rank().to_dict()\n    train[f'{col}_freq'] = train[col].map(_map)\n    test[f'{col}_freq']  = test[col].map(_map)","3dcc71c3":"%%time\ntrain.loc[train['ord_5_1'].notnull(),'ord_5_1'] = train.loc[train['ord_5_1'].notnull(),'ord_5_1'].apply(lambda c: ord(c) - ord('a') + 33).astype(float)\ntest.loc[test['ord_5_1'].notnull(),'ord_5_1']   = test.loc[test['ord_5_1'].notnull(),'ord_5_1'].apply(lambda c: ord(c) - ord('a') + 33).astype(float)\ntrain.loc[train['ord_5_2'].notnull(),'ord_5_2'] = train.loc[train['ord_5_2'].notnull(),'ord_5_2'].apply(lambda c: ord(c) - ord('a') + 33).astype(float)\ntest.loc[test['ord_5_2'].notnull(),'ord_5_2']   = test.loc[test['ord_5_2'].notnull(),'ord_5_2'].apply(lambda c: ord(c) - ord('a') + 33).astype(float)","5b58ab78":"for i in range(1,3):\n    plot_line(f'ord_5_{i}')","43e8009d":"for i in range(1,3):\n    plot_line(f'ord_5_{i}_freq')","7f07c150":"for i, h in zip(range(5),[6,6,6,6,6]):\n    plot_traintest(f'nom_{i}',h)","fc778a2b":"%%time\nfor i in range(5):\n    _map = pd.concat([train[f'nom_{i}'], test[f'nom_{i}']]).value_counts().rank().to_dict()\n    train[f'nom_{i}_freq'] = train[f'nom_{i}'].map(_map)\n    test[f'nom_{i}_freq']  = test[f'nom_{i}'].map(_map)","fbf7e4e9":"for i in range(5):\n    plot_line(f'nom_{i}_freq')","d3e5998b":"%%time\n\nfor i in range(5,10):\n    col_name = f'nom_{i}'\n    print(f'{col_name} train data ---------------')\n    print(train[col_name].value_counts(dropna=False, normalize=False)[:20])\n    print(f'{col_name} test data ---------------')\n    print(test[col_name].value_counts(dropna=False, normalize=False)[:20])","19b1cd19":"%%time\nfor i in range(5, 10):\n    col_name = f'nom_{i}'\n    print(f'{col_name} train data ---------------')\n    print(train[col_name].value_counts(dropna=False, normalize=True)[:20])\n    print(f'{col_name} test data ---------------')\n    print(test[col_name].value_counts(dropna=False, normalize=True)[:20])    ","0222fc48":"%%time\nfor i in range(5, 10):\n    plot_line(f'nom_{i}')","2105c9b6":"# %%time\n# for i in range(5, 10):\n#     _map = pd.concat([train[f'nom_{i}'], test[f'nom_{i}']]).value_counts().to_dict()\n#     train[f'nom_{i}_freq'] = train[f'nom_{i}'].map(_map)\n#     test[f'nom_{i}_freq']  = test[f'nom_{i}'].map(_map)","f48fecdb":"# for i in range(5, 10):\n#     plot_line(f'nom_{i}_freq')","befb8114":"train.info()\ntest.info()","fb904128":"%%time\n\nfig, ax = plt.subplots(1, 2, figsize=(12,6))\nsns.lineplot(train.month, train.target, ax=ax[0])\nsns.lineplot(train.day,   train.target, ax=ax[1])","6de966ff":"train.day = train.day.replace({3:5,2:6,1:7})\ntest.day  = test.day.replace({3:5,2:6,1:7})","86857a5e":"sns.lineplot(train.day, train.target)","7759425d":"# %%time\n# train['sin_day'] = np.sin(train['day']*np.pi\/3.5).astype(float)\n# train['cos_day'] = np.cos(train['day']*np.pi\/3.5).astype(float)\n# test['sin_day']  = np.sin(test['day']*np.pi\/3.5).astype(float)\n# test['cos_day']  = np.cos(test['day']*np.pi\/3.5).astype(float)","892145ae":"display(train.day.value_counts(dropna=False))\ndisplay(test.day.value_counts(dropna=False))","a053086f":"# %%time\n# threthold = 0.9#0.8#0.87\n# for col in test.columns:\n#     if train[col].value_counts().tolist()[0] >len(train.dropna())*threthold:\n#         print('-'*20,'\\ntrain:', train[col].value_counts().head(1))\n#     if test[col].value_counts().tolist()[0] >len(test.dropna())*threthold:\n#         print('test:',  test[col].value_counts().head(1))","eb845d49":"%%time\n# train.loc[train['nom_5'].isin(['b3ad70fcb']),'nom_5'] = np.NaN\n# train.loc[train['nom_6'].isin(['f0732a795', 'ee6983c6d', '3a121fefb']),'nom_6'] = np.NaN\n# test.loc[test['nom_6'].isin(['a885aacec']),'nom_6'] = np.NaN\n# train.loc[train['nom_9'].isin(['3d19cd31d', '1065f10dd']),'nom_9'] = np.NaN\nfrom scipy import stats\n\nfor col in NOM_5_9:\n    train.loc[train[col].notnull(), col] = train.loc[train[col].notnull(), col].astype(str).apply(int, base=16)\n    test.loc[test[col].notnull(), col]   = test.loc[test[col].notnull(), col].astype(str).apply(int, base=16)\nfor col in NOM_5_9:\n    train[col] = train[col].astype(float)\n    test[col]  = test[col].astype(float)\n\n# exclude_cols = ['sin_day', 'cos_day'] + [s for s in train.columns if '_mean' in s]\nfeats = train.select_dtypes(float).columns.drop([]).tolist()#\nfor col in feats:\n    if col in ['bin_0']:\n        train[col].fillna(0, inplace=True)\n        test[col].fillna(0,  inplace=True)\n    else:\n        train[col].fillna(-1, inplace=True)\n        test[col].fillna(-1,  inplace=True)\n\n    \n    \nfor col in feats:    \n    train[col] = train[col].astype(int)\n    test[col]  = test[col].astype(int)","286a6566":"%%time\nfor col in NOM_COL+['ord_5']:\n    _map = train.groupby(col).mean()['target'].to_dict()\n    train[f'{col}_mean'] = train[col].map(_map)\n    test[f'{col}_mean']  = test[col].map(_map)","a50bd0e8":"# fig, ax = plt.subplots(1,2,figsize=(12,6))\n# sns.scatterplot(train.sin_day, train.cos_day, ax=ax[0])\n# sns.scatterplot(test.sin_day,  test.cos_day,  ax=ax[1])","4f7b1341":"plot_corr()","f18eafb8":"tmp_df = train.drop(columns='id').corr().sort_values(['target'], ascending=False)[1:]\nplt.figure(figsize=(12,10))\nplt.title('Correlation plot(per target)')\nsns.barplot(x=tmp_df['target'], y=tmp_df.index)","51fcb5d8":"train.info(null_counts=True)\ntest.info(null_counts=True)","7eeffe9a":"from sklearn.model_selection import train_test_split,KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfeats = train.select_dtypes(int).columns.drop([\n    'id','target',#'bin_3','day',\n    'nom_0_freq', 'nom_1_freq', 'nom_2_freq', 'nom_3_freq', 'nom_4_freq',\n]).tolist()+['nom_0_mean','nom_1_mean','nom_2_mean','nom_3_mean', 'nom_4_mean']#+['sin_day', 'cos_day',]#+[s for s in train.columns if '_mean' in s]#train.select_dtypes(float).columns.tolist()#train.drop(columns=['id','target']).columns#\n\nislgb, isxgb, isctb = False, False, True\n\nX = train[feats]\ny = train.target\nX_test = test[feats]\n(X_train,X_val, y_train, y_val) = train_test_split(X, y, stratify=y, random_state=42)\nprint(X_train.shape,X_val.shape, y_train.shape, y_val.shape)\nfeats","45f8fe81":"def create_categorical_feats(df, category_columns):\n    index_df = pd.DataFrame([df.columns, df.dtypes]).T\n\n    index_df = index_df.rename(columns={0:\"column_name\", 1:'dtype',})\n#     categorical_feats = index_df[index_df.column_name.isin(category_columns)].index.tolist()\n    print(categorical_feats)\n    return categorical_feats   ","4053a6ac":"import lightgbm as lgb\nimport xgboost  as xgb\nimport catboost as ctb\nfrom sklearn.metrics import confusion_matrix\n\ncategorical_feats = [\n#     'bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4', \n    'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9',\n#     'ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4',\n    'day', \n    'month',\n#     'nom_0_freq', 'nom_1_freq', 'nom_2_freq', 'nom_3_freq', 'nom_4_freq', \n#     'ord_5_1_freq', 'ord_5_2_freq'\n]\n\nlgb_params = {\n    'boosting_type':'gbdt', \n    'num_leaves':2**4-1,#2**5-1,\n    'learning_rate':0.05,#0.1, \n    'n_estimators':3000,#1000,#100, \n#     subsample_for_bin=200000, \n    'objective':'binary',#=None,\n    'metrics':'auc',\n    'feature_fraction':0.8,\n    'reg_alpha':0.9,#0.1, \n    'reg_lambda':0.5,#0.1, \n    'random_state':42,\n#     'verbosity':100,\n    'early_stopping_rounds':100\n}\n\nxgb_params = {\n    'learning_rate':0.1, \n    'n_estimators':3000,#100, \n#     subsample_for_bin=200000, \n    'objective':'binary:logistic',\n    'eval_metric':'auc',\n    'random_state':42, \n}\n\nctb_params = {\n    'task_type':'GPU',\n    'learning_rate':0.1, \n    'n_estimators':10000,#3000,#100, \n    'objective':'Logloss',\n    'eval_metric':'AUC',\n    'random_state':10372,#42, \n    'use_best_model':True,\n    'verbose':1000,\n    'early_stopping_rounds':100,\n    'l2_leaf_reg':0.9,\n#     'silent':False,\n#     'plot':True,\n    'cat_features':create_categorical_feats(X, categorical_feats)\n}","afd37bff":"if islgb:\n    model = lgb.LGBMClassifier(**lgb_params)\n    model.fit(X_train, y_train, \n              eval_set=[(X_train, y_train),(X_val, y_val)],\n              verbose=100)\n    oof_preds = model.predict_proba(\n        X_val, num_iteration=model.best_iteration_)[:,1]\n    print(roc_auc_score(y_val, oof_preds))\n    print(confusion_matrix(y_val, np.round(oof_preds).astype(np.int8)))  \n    \n    plt.figure(figsize=(12,10))\n    lgb.plot_importance(model)","a8c6e128":"%%time\n\nif isxgb:\n    model = xgb.XGBClassifier(**xgb_params)\n    model.fit(X_train, y_train, eval_set=[(X_train, y_train),\n                                          (X_val, y_val)], \n              verbose=10, early_stopping_rounds=100)\n    oof_preds = model.predict_proba(X_val)[:,1]\n    print(roc_auc_score(y_val, oof_preds))\n    print(confusion_matrix(y_val, np.round(oof_preds).astype(np.int8)))\n    \n    plt.figure(figsize=(12,10))\n    xgb.plot_importance(model)","701266af":"%%time\n\nif isctb:\n    model = ctb.CatBoostClassifier(**ctb_params)\n    model.fit(X_train, y_train, \n              eval_set=[#(X_train, y_train), \n                        (X_val, y_val)],#Multiple eval sets are not supported on GPU\n              plot=True)\n    oof_preds = model.predict_proba(X_val)[:,1]\n    print(roc_auc_score(y_val, oof_preds))\n    \n    feature_importance_df = pd.DataFrame(np.log1p(model.get_feature_importance()), model.feature_names_).reset_index() \n    feature_importance_df = feature_importance_df.rename(columns={'index':'feature',0:'importance'}).sort_values('importance',ascending=False)\n    plt.figure(figsize=(12,10))\n    display(sns.barplot(feature_importance_df['importance'], feature_importance_df['feature']))\n\n    print(confusion_matrix(y_val, np.round(oof_preds).astype(np.int8)))","721d9d65":"def display_importances(feature_importance_df_,height=50,title='catboost'):\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n        by=\"importance\", ascending=False).index\n\n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n\n#     plt.figure(figsize=(8, height))\n    plt.figure(figsize=(15, height))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title(f'{title} Features (avg over folds)')\n    plt.tick_params(labelcolor='Red')\n    plt.tight_layout()\n    plt.savefig('lgbm_importances.png')","78ea18c9":"def set_importance(model, feature_importance_df, fold_idx, feats, modeltype): \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = feats\n    if modeltype=='lgb':\n        fold_importance_df[\"importance\"] = np.log1p(model.feature_importance(\n            importance_type='gain',iteration=model.best_iteration)) \n    else:\n        fold_importance_df[\"importance\"] = np.log1p(model.feature_importances_)\n        \n    fold_importance_df[\"fold\"] = fold_idx + 1\n    feature_importance_df      = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    return feature_importance_df   ","aceebc61":"%%time\n\nkf = StratifiedKFold(n_splits=10,#5, \n                     shuffle=True, random_state=42)\n# kf = KFold(n_splits=5, shuffle=True,#False, \n#            random_state=42)\noof_preds = np.zeros(len(X)).astype(np.float32)\nsub_preds = np.zeros(len(X_test)).astype(np.float32)\nfeature_importance_df = pd.DataFrame()\nfor fold_, (train_idx, val_idx) in enumerate(kf.split(X,y=y)):\n#     print(\"train:\", train_idx, \"val:\", val_idx)\n    X_train = X.loc[train_idx] \n    y_train = y.loc[train_idx]\n    X_val, y_val = X.loc[val_idx], y.loc[val_idx]\n    if islgb:\n        model = lgb.LGBMClassifier(**lgb_params)\n        model.fit(X_train, y_train, \n                  eval_set=[(X_train, y_train),(X_val, y_val)], \n                  verbose=100)\n        oof_preds[val_idx] = model.predict_proba(\n            X_val, num_iteration=model.best_iteration_)[:,1]\n        sub_preds += model.predict_proba(\n            X_test, num_iteration=model.best_iteration_)[:,1] \/ kf.n_splits\n\n        plt.figure(figsize=(12,10))\n        lgb.plot_importance(model)\n        feature_importance_df = set_importance(model, feature_importance_df, fold_, feats, 'lgb')\n    \n    if isctb:\n        model = ctb.CatBoostClassifier(**ctb_params)\n        model.fit(X_train, y_train, \n                  eval_set=[#(X_train, y_train), \n                      (X_val, y_val)],\n                  plot=True)\n        oof_preds[val_idx] = model.predict_proba(X_val)[:,1]\n        sub_preds += model.predict_proba(X_test)[:,1] \/ kf.n_splits\n        \n        feature_importance_df = set_importance(\n            model, feature_importance_df, fold_, feats, 'ctb')\n\nplt.title(f'auc_score:{roc_auc_score(y, oof_preds)}')\nsns.distplot(oof_preds)\nsns.distplot(sub_preds)\nplt.legend(['train','test'])\nplt.show()        ","c66dd0b7":"plt.title(f'auc_score:{roc_auc_score(y, oof_preds)}')\nsns.distplot(oof_preds)\nsns.distplot(sub_preds)\nplt.legend(['train','test'])\nplt.show()        ","1a54061e":"display_importances(feature_importance_df, height=len(feats), title='catboost(training cv)')","b597f2ad":"submission = pd.read_csv('..\/input\/cat-in-the-dat-ii\/sample_submission.csv')\nsubmission['target'] = sub_preds","072cfaf3":"submission","82b87423":"submission.describe()","1bf5fac3":"%%time\nsubmission.to_csv('submission.csv', index=False)","a42ff5fd":"gc.collect()","51c44616":"# Easy training(lightgbm)","9c5bb1da":"# NaN Check\n\n","15fde2bb":"### ord_5","b63578bd":"target meaning distribution\n- month\n- day  \n  The distributions of 3 and 5, 2 and 6, 1 and 7 are similar.\n \n","5b3806e8":"# Imbalanced data\n\npending","a249865f":"# Easy training(catboost)","2d9630d7":"# Easy training(xgboost)","1e9499b9":"# Training(KFold)","d5ac5842":"# binary features","c1cd843a":"# NaN Cleaning\n\n\npending","c500ed81":"# unique value check\n\nChecked for each feature.  \nFew data exists only in training data \/ test data.","49bd207d":"# ordinal features\n- ord_0\n\n- ord_1\/ord_2  \n\n- ord_3\/ord_4\n\n- ord_5","4d4175df":"# Correlation","6212a9bd":"About 3% of data is NaN for each feature","55346f97":"target meaning distribution","5a201f00":"# Cyclical features\n\n\n","a48bfbfc":"# Categorical Feature Encoding Challenge II\n\n\nThere is little explanation in the kernel. sorry.","6ad3e466":"# simple Data Check","9d068c4d":"# nominal features\n### nom_0 - nom_4","153e7d53":"### nom_5 - nom_9\n\n- Features with high cardinality\n","216670f0":"# Correlation (Immediately before training)","08f00704":"# train"}}