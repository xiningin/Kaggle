{"cell_type":{"b75eaf88":"code","698dffef":"code","1135c190":"code","40cae755":"code","6339dabf":"code","5416af9e":"code","3498f8ff":"code","fa8cd59c":"code","640a4ce4":"code","87301e2f":"code","60ede7b8":"code","63ddd0fc":"code","be7ae3bd":"code","d8e3f656":"code","c73df64c":"code","40a69dee":"code","6a4552f8":"code","83d0bde0":"code","d7d89297":"code","e5bc878c":"code","30dc8b98":"code","853b7c9c":"code","6cfdd1f2":"code","024c1913":"code","7b72d1eb":"markdown","41ac1129":"markdown","2581a7b4":"markdown","bb0df464":"markdown","35634145":"markdown","7dc69fa5":"markdown","7c7a07a1":"markdown","650753a9":"markdown","8543009c":"markdown","0f26e41d":"markdown","25ff453a":"markdown","161a8aaa":"markdown","f2d43ba9":"markdown","69f159de":"markdown","1bb788e2":"markdown","2df607d5":"markdown","dd40f122":"markdown","fd1427b9":"markdown","2ad987a5":"markdown","35dda034":"markdown","3a0781fc":"markdown"},"source":{"b75eaf88":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nfrom hyperopt import fmin, hp, tpe\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","698dffef":"labeled_df = pd.read_csv('\/kaggle\/input\/hmboost\/train.csv', index_col=0)\ntest_df = pd.read_csv('\/kaggle\/input\/hmboost\/test.csv', index_col=0)","1135c190":"labeled_df.head()","40cae755":"X_train, X_val, y_train, y_val = train_test_split(labeled_df.drop('MEDV', axis=1), labeled_df['MEDV'], test_size=0.3, random_state=42)","6339dabf":"decision_tree_model = DecisionTreeRegressor(random_state=2323)\ndecision_tree_model.fit(X_train, y_train)","5416af9e":"pred_val = decision_tree_model.predict(X_val)","3498f8ff":"rmse = np.sqrt(mean_squared_error(pred_val, y_val))\nrmse","fa8cd59c":"pred_val_allmeans = [y_train.mean()]*len(y_val)","640a4ce4":"rmse_allmeans = np.sqrt(mean_squared_error(pred_val_allmeans, y_val))\nrmse_allmeans","87301e2f":"test_predictions = decision_tree_model.predict(test_df)\ntest_predictions","60ede7b8":"sample_sub = pd.read_csv('\/kaggle\/input\/hmboost\/sampleSubmission.csv')\nsample_sub.head()","63ddd0fc":"submission_df = pd.DataFrame({'target':test_predictions}).reset_index()\nsubmission_df","be7ae3bd":"submission_df.to_csv('dec_tree.csv', index=None)","d8e3f656":"dtr = DecisionTreeRegressor(random_state=77)\n\npipe = Pipeline(steps=[('dtr', dtr)])\n\n# Parameters of pipelines can be set using \u2018__\u2019 separated parameter names:\nparam_grid = {\n    'dtr__max_depth': [i+1 for i in range(13)],\n    'dtr__max_features': [i+1 for i in range(13)],\n}\nsearch = GridSearchCV(pipe, param_grid, n_jobs=-1, scoring='neg_root_mean_squared_error', cv=KFold(3, random_state=88))\nsearch.fit(X_train, y_train)\nprint(\"Best parameter (CV score=%0.3f):\" % -search.best_score_)\nprint(search.best_params_)","c73df64c":"search.best_params_","40a69dee":"grid_tree_model = DecisionTreeRegressor(max_depth=search.best_params_['dtr__max_depth'],\n                                        max_features=search.best_params_['dtr__max_features']\n                                       )\ngrid_tree_model.fit(X_train, y_train)\npred_val_grid = grid_tree_model.predict(X_val)\nnp.sqrt(mean_squared_error(pred_val_grid, y_val))","6a4552f8":"def evaluate(model, X_train, y_train):\n    X_fit, X_eval, y_fit, y_eval = train_test_split(X_train, y_train, test_size=0.3) \n    model.fit(X_fit, y_fit)\n    preds = model.predict(X_eval)\n    rmse = np.sqrt(mean_squared_error(preds, y_eval))\n    return rmse","83d0bde0":"def opt_fn(params):\n    model = DecisionTreeRegressor(max_depth=int(params['max_depth']),\n                                  max_features=int(params['max_features']),\n                                  random_state=42\n                                 )\n    rmse = evaluate(model, X_train, y_train)\n    return rmse","d7d89297":"search_space = {'max_depth': hp.quniform('max_depth', 1, 13, 1),\n                'max_features': hp.quniform('max_features', 1, 13, 1),\n               }","e5bc878c":"argmin = fmin(\n   fn=opt_fn,\n   space=search_space,\n   algo=tpe.suggest,\n   max_evals=100\n)","30dc8b98":"argmin","853b7c9c":"hopt_tree_model = DecisionTreeRegressor(max_depth=int(argmin['max_depth']),\n                                        max_features=int(argmin['max_features']),\n                                        random_state=23\n                                       )\nhopt_tree_model.fit(X_train, y_train)\npred_val_hopt = hopt_tree_model.predict(X_val)\nnp.sqrt(mean_squared_error(pred_val_hopt, y_val))","6cfdd1f2":"import xgboost\n\nxg = xgboost.XGBRegressor()\nxg.fit(X_train, y_train)\npred_val_xg = xg.predict(X_val)\n\n# If you want to use early stopping\n# Here you may want to split the training data again\n# instead of using X_val to avoid overfitting the the validation set\nxg.fit(X_train,\n       y_train,\n       early_stopping_rounds=50,\n       eval_metric=\"rmse\",\n       eval_set=[(X_val, y_val)])","024c1913":"import lightgbm as lgb\n\nparameters = {\n    'boosting': 'gbdt',\n    'metric': 'mse',\n    'verbose': 0\n}\n\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)\n\nlight = lgb.train(parameters,\n                  lgb_train)\n\npred_val_light = light.predict(X_val)\n\n# If you want to use early stopping\n# Here you may want to split the training data again\n# instead of using lgb_eval to avoid overfitting the the validation set\nlight_es = lgb.train(parameters,\n                     lgb_train,\n                     valid_sets=lgb_eval,\n                     early_stopping_rounds=5)","7b72d1eb":"Is this value good or bad? What do we get if we always guess the training set mean?","41ac1129":"And run the optimization. In this case it is a bit pointless to run the guided random search because we have so few hyperparameter values that we can just try them all, but usually (especially with continuous hyperparameters) we can't evaluate all combinations.","2581a7b4":"Load the training and test data. The former has labels, the latter doesn't. We will predict labels for it.","bb0df464":"Let's build a vanilla decision tree with default parameters.\n\nI use a random seed here (random_state=...) just to get reproducible results; you don't need to do this.","35634145":"<h2>Now it's your turn to make a better model!<\/h2>\n\nAs a base model, you can try:\n\n- RandomForestRegressor\n- XGBoost\n- LightGBM\n- Whatever you want!\n\nFor hyperparameter tuning:\n\n- sklearn's GridSearchCV\n- hyperopt\n- roll your own!\n\nFor evaluation:\n\n- training\/validation splits\n- k-fold cross-validation","7dc69fa5":"Let us use these parameters to train and evaluate a model.","7c7a07a1":"A simple way to recreate this is to make a Pandas data frame and insert the predictions.\n\nHere, the reset_index() call is a small \"trick\" to insert a new `index` column with consecutive numbering 0,1,...","650753a9":"**LightGBM**","8543009c":"Define the search space for hyperopt's random search.","0f26e41d":"<h2>Two flavors of hyperparameter optimization<\/h2>\n\nNow we try to find better hyperparameters for the decision tree model. There are a number of parameters: ccp_alpha, criterion, max_depth, max_features, max_leaf_nodes,                min_impurity_decrease, min_impurity_split min_samples_leaf, min_samples_split, min_weight_fraction_leaf. For now let's just focus on max_depth and max_features.","25ff453a":"Calculate the root mean square error.","161a8aaa":"<h2>Hints: how to run XGBoost and LightGBM<\/h2>","f2d43ba9":"Can we do better with hyperopt?\n\nHere will write a function that does its own data split and evaluates the fit. You could also do a k-fold cross-validation in here but that is left as an exercise :D","69f159de":"**Option 1: Scikit-learn grid search**\n\nThis will do internal cross-validation on the training data set to find the best parameters. Then we can evaluate on our separate validation data set to get an idea of how the model will perform on the hidden test set on which leaderboard scores are calculated.","1bb788e2":"Divide the labeled data into training and validation sets.","2df607d5":"Then we need to define an optimization function for hyperopt.","dd40f122":"**XGBoost**","fd1427b9":"Evaluate the model on the validation set.","2ad987a5":"Let's now evaluate our model on the test set and write out a file that we can submit to the leaderboard if we feel like it.","35dda034":"We need to put this in the format of the sample submission file:","3a0781fc":"And write out the file!"}}