{"cell_type":{"6a4a7b9c":"code","ae93b763":"code","2fd39f46":"code","179bb074":"code","9155c1df":"code","00d9d066":"code","dc0cdcff":"code","7ca2f411":"code","bf6ad1f0":"code","33d29c89":"code","cab42bae":"code","ef098820":"code","1da1cb54":"code","5368b355":"code","20d7bd7a":"code","25f11b0f":"code","f0951dac":"code","56c711b9":"code","534f91a4":"code","2d0fb050":"code","2096ebc3":"code","b4725b02":"code","a00f9ad2":"code","11400c43":"code","ad5acb67":"code","912d2fda":"code","9c44cca5":"code","07481d76":"code","c62313fb":"code","8a84bf51":"code","1d8f6a7b":"code","54a50f31":"code","2d836235":"code","da198eb0":"markdown","0f15b301":"markdown","358420ee":"markdown","8c1b4561":"markdown","02960d17":"markdown","84b27782":"markdown","64583215":"markdown","ca9dbba5":"markdown","b6099e84":"markdown","c3e6c7a1":"markdown","f54db7b0":"markdown","cb4c2d4e":"markdown","fed7d8ae":"markdown","dce8a0e5":"markdown","06286f3d":"markdown","55b4d555":"markdown","477d676a":"markdown","d9cb7ea0":"markdown","341b3205":"markdown","3d5072a0":"markdown","c88ea49b":"markdown","573792d0":"markdown","63ea4821":"markdown","47be9704":"markdown","5c395d44":"markdown","be748d79":"markdown","50c1e170":"markdown","a3827486":"markdown","0df44887":"markdown","8b15d7b0":"markdown"},"source":{"6a4a7b9c":"# ignore local warnings\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import cross_val_score\n\n# Model packages\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom keras.wrappers.scikit_learn import KerasClassifier","ae93b763":"import os\nos.chdir(\"\/kaggle\/input\/churn-modelling\")","2fd39f46":"df = pd.read_csv(\"Churn_Modelling.csv\")\ndf.head()","179bb074":"df.shape","9155c1df":"df.isnull().sum()","00d9d066":"df.info()","dc0cdcff":"X = df.iloc[:, 3:-1].values\ny = df.iloc[:, -1].values","7ca2f411":"# Gender\nle = LabelEncoder()\nX[:,2] = le.fit_transform(X[:,2])","bf6ad1f0":"# Geography\nct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [1])] , remainder = \"passthrough\")\nX = np.array(ct.fit_transform(X))","33d29c89":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)","cab42bae":"scaler = StandardScaler()\nX_train[:, [3,5,6,7,8,11]] = scaler.fit_transform(X_train[:, [3,5,6,7,8,11]])\nX_test[:, [3,5,6,7,8,11]] = scaler.transform(X_test[:, [3,5,6,7,8,11]])","ef098820":"classifier_1 = XGBClassifier()\nclassifier_1.fit(X_train, y_train)","1da1cb54":"y_pred_1 = classifier_1.predict(X_test)\nprint(confusion_matrix(y_test,y_pred_1))","5368b355":"acc_1 = cross_val_score(estimator = classifier_1, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(acc_1.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(acc_1.std()*100))","20d7bd7a":"classifier_2 = RandomForestClassifier(n_estimators = 100, random_state = 0)\nclassifier_2.fit(X_train, y_train)","25f11b0f":"y_pred_2 = classifier_2.predict(X_test)\nprint(confusion_matrix(y_test,y_pred_2))","f0951dac":"acc_2 = cross_val_score(estimator = classifier_2, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(acc_2.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(acc_2.std()*100))","56c711b9":"classifier_3 = GaussianNB()\nclassifier_3.fit(X_train, y_train)","534f91a4":"y_pred_3 = classifier_3.predict(X_test)\nprint(confusion_matrix(y_test,y_pred_3))","2d0fb050":"acc_3 = cross_val_score(estimator = classifier_3, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(acc_3.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(acc_3.std()*100))","2096ebc3":"classifier_4 = LogisticRegression(random_state = 0)\nclassifier_4.fit(X_train, y_train)","b4725b02":"y_pred_4 = classifier_4.predict(X_test)\nprint(confusion_matrix(y_test,y_pred_4))","a00f9ad2":"acc_4 = cross_val_score(estimator = classifier_4, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(acc_4.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(acc_4.std()*100))","11400c43":"classifier_5 = KNeighborsClassifier(n_neighbors = 10, metric= \"minkowski\", p=2)\nclassifier_5.fit(X_train, y_train)","ad5acb67":"y_pred_5 = classifier_5.predict(X_test)\nprint(confusion_matrix(y_test,y_pred_5))","912d2fda":"acc_5 = cross_val_score(estimator = classifier_5, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(acc_5.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(acc_5.std()*100))","9c44cca5":"ann = tf.keras.models.Sequential()","07481d76":"ann.add(tf.keras.layers.Dense(units=6, activation=\"relu\", input_dim=12)) #first layer\nann.add(tf.keras.layers.Dense(units=6, activation=\"relu\")) #Second layer \nann.add(tf.keras.layers.Dense(units=6, activation=\"relu\")) #Third layer\nann.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\")) #Output layer","c62313fb":"ann.compile(optimizer=\"adam\", loss= \"binary_crossentropy\", metrics= [\"accuracy\"])","8a84bf51":"type(X_train)","1d8f6a7b":"ann.fit(X_train, y_train, batch_size=32, epochs=100)","54a50f31":"y_pred_6 = ann.predict(X_test)\ny_pred_6 = (y_pred_6 > 0.5)\nprint(confusion_matrix(y_test, y_pred_6))","2d836235":"acc_6 = accuracy_score(y_test, y_pred_6)\nprint(\"Accuracy: {:.2f}%\".format(acc_6*100))","da198eb0":"#### K-fold Cross Validation","0f15b301":"#### Feature Scaling\n\nWe will need to scale some of the features (some numerical features). The reason we are doing this is because we do not want one feature to overweight the other because of their relatively bigger range number. We will use methods of standardization to scale some of the numerical values.\n\nHere's the formula for standardization:\n\n#### $ x_{standardized} = \\frac{x - \\mu(X)}{\\sigma(X)} $","358420ee":"#### Confusion Matrix","8c1b4561":"#### Confusion Matrix","02960d17":"### 3) Naive Bayes Classifier","84b27782":"#### K-fold Cross Validation","64583215":"We can see that there are no missing data in the dataset. We will proceed with exploring each of the feature and try to find the best approach to each of the features.\n\nWe won't need to include the first three columns in our model since \"CustomerId\", \"Surname\" and \"RowNumber\" do not  have any logical contribution to our prediction.","ca9dbba5":"#### Confusion Matrix","b6099e84":"### 6) Artifical Neural Network","c3e6c7a1":"### 4) Logistic Regression","f54db7b0":"# Churn Model Prediction","cb4c2d4e":"### 5) K-Nearest Neighbors","fed7d8ae":"#### K-fold Cross Validation","dce8a0e5":"#### Confusion Matrix","06286f3d":"### 2) Random Forest Classifier","55b4d555":"#### Initializing","477d676a":"#### Confusion Matrix","d9cb7ea0":"#### Confusion Matrix","341b3205":"#### K-fold Cross Validation","3d5072a0":"#### Categorical Variable Encoding\n\nWe will encode two categorical variables: \"Geography\" and \"Gender\".\nThe way to encode feature \"Geography\" is different from \"Gender\" as we only have two possible values for \"Gender\" (Female or Male). We can simply encode one of the to be \"1\" and the other one to be \"0\". But in \"Geography\" case, we have more than two possible values. If we encode this feature the same way as \"Gender\" there will be some numerical order in the value of this feature which would create some confusion in our model (we do not want to create a correlation between the numerical order in the feature and our target variable) ","c88ea49b":"#### Compile","573792d0":"#### Split data","63ea4821":"## Import the dataset","47be9704":"## Import the libraries needed","5c395d44":"#### K-fold Cross Validation","be748d79":"## Applying different classification models\n### 1) XGBoost Classifier","50c1e170":"## Exploratory Data Analysis (EDA)","a3827486":"#### Adding layers","0df44887":"## Data Pre-processing","8b15d7b0":"There are so many improvement that could be made such as tuning the hyperparameter, adding extra layers (for ANN), etc. All in all, the best classification algorithm that we get for now are Artifical Neural Network (ANN) and Random Forest Classifier, with accuracy score of approximately 86%."}}