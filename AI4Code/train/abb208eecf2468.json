{"cell_type":{"ce462ac1":"code","dc610551":"code","04ae2c22":"code","6b2b9e04":"code","6250d716":"code","590b98d7":"code","07e85514":"code","3a4f6789":"code","1110882e":"code","3b6ff0e7":"code","e7de16ef":"code","59e6660f":"code","1151c740":"code","3e8d6732":"code","8b55e1fc":"code","b719b149":"code","980ec8db":"code","afba35da":"code","8e9cf4e6":"code","466bcac6":"code","cf44b082":"code","dded30c1":"code","7f729762":"code","fe48a0eb":"code","bb21a388":"code","3b8dfe19":"code","f3ed7614":"code","97d9a650":"code","5acf2601":"code","adf80d5e":"code","afc6fd92":"code","5ba809ef":"code","52022a2d":"code","ec386972":"code","2aea9f1a":"code","cdc19568":"code","03851e1c":"code","8d596b02":"code","ad62aee8":"code","b89780be":"code","ff222ce2":"code","26e46a3c":"code","dad200da":"code","8743e84c":"code","fa3b42fb":"code","0c4decd4":"code","4e1cdd26":"code","11949f1d":"code","cb2d2841":"markdown","7f8bbbec":"markdown","424aef59":"markdown","50f710f5":"markdown","dd222ce6":"markdown","b97e9d82":"markdown","f8e2c5be":"markdown","5756dee1":"markdown","8d63c92d":"markdown","d2122b1b":"markdown","759db531":"markdown","ece40102":"markdown","8b097978":"markdown","7f4bc95b":"markdown","1ad322e6":"markdown","1deff767":"markdown","48b90fb0":"markdown","e83fcaac":"markdown","57725a96":"markdown","0ffda0f0":"markdown","fed85820":"markdown","1ffeb9fd":"markdown","a42f5e52":"markdown","81229eeb":"markdown","c6c6bc35":"markdown","bdb766ae":"markdown","b98141ae":"markdown","09999416":"markdown"},"source":{"ce462ac1":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\n\n# --- plotly ---\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\nimport seaborn as sns","dc610551":"%%time\ndatadir = Path('\/kaggle\/input\/google-quest-challenge')\n\n# Read in the data CSV files\ntrain = pd.read_csv(datadir\/'train.csv')\ntest = pd.read_csv(datadir\/'test.csv')\nsample_submission = pd.read_csv(datadir\/'sample_submission.csv')","04ae2c22":"# Lets check the size\n\nprint(\"Train\" , train.shape)\nprint(\"Test\" , test.shape)\nprint(\"Submission\" , sample_submission.shape)","6b2b9e04":"sample_submission.head()","6250d716":"sample_submission.columns","590b98d7":"feature_columns = [col for col in train.columns if col not in sample_submission.columns]\nprint(\"Feature columns are \" , feature_columns)","07e85514":"train[feature_columns].head()","3a4f6789":"train0 = train.iloc[0]\n\nprint('URL           : ', train0['url'])\nprint('question_title: ', train0['question_title'])\nprint('question_body : ', train0['question_body'])","1110882e":"print('answer: ', train0['answer'])","3b6ff0e7":"train[['url', 'question_user_name', 'question_user_page', 'answer_user_name', 'answer_user_page', 'url', 'category', 'host']]","e7de16ef":"# target label distribution\n\ntarget_cols = ['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written', 'answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']","59e6660f":"fig, axes = plt.subplots(6, 5, figsize=(18, 15))\naxes = axes.ravel()\nbins = np.linspace(0, 1, 20)\n\nfor i, col in enumerate(target_cols):\n    ax = axes[i]\n    sns.distplot(train[col], label=col, kde=False, bins=bins, ax=ax)\n    # ax.set_title(col)\n    ax.set_xlim([0, 1])\n    ax.set_ylim([0, 6079])\nplt.tight_layout()\nplt.show()\nplt.close()","1151c740":"# Null values\ntrain.isnull().sum()\n# There is no Null Values\/","3e8d6732":"train_category = train['category'].value_counts()\ntest_category = test['category'].value_counts()","8b55e1fc":"fig, axes = plt.subplots(1, 2, figsize=(12, 6))\ntrain_category.plot(kind='bar', ax=axes[0])\naxes[0].set_title('Train')\ntest_category.plot(kind='bar', ax=axes[1])\naxes[1].set_title('Test')\nprint('Train\/Test category distribution')","b719b149":"from wordcloud import WordCloud\n\ndef plot_wordcloud(text, ax, title=None):\n    wordcloud = WordCloud(max_font_size=None, background_color='white',\n                          width=1200, height=1000).generate(text_cat)\n    ax.imshow(wordcloud)\n    if title is not None:\n        ax.set_title(title)\n    ax.axis(\"off\")","980ec8db":"print('Training data Word Cloud')\n\nfig, axes = plt.subplots(1, 3, figsize=(16, 18))\n\ntext_cat = ' '.join(train['question_title'].values)\nplot_wordcloud(text_cat, axes[0], 'Question title')\n\ntext_cat = ' '.join(train['question_body'].values)\nplot_wordcloud(text_cat, axes[1], 'Question body')\n\ntext_cat = ' '.join(train['answer'].values)\nplot_wordcloud(text_cat, axes[2], 'Answer')\n\nplt.tight_layout()\nfig.show()","afba35da":"print('Test data Word Cloud')\n\nfig, axes = plt.subplots(1, 3, figsize=(16, 18))\n\ntext_cat = ' '.join(test['question_title'].values)\nplot_wordcloud(text_cat, axes[0], 'Question title')\n\ntext_cat = ' '.join(test['question_body'].values)\nplot_wordcloud(text_cat, axes[1], 'Question body')\n\ntext_cat = ' '.join(test['answer'].values)\nplot_wordcloud(text_cat, axes[2], 'Answer')\n\nplt.tight_layout()\nfig.show()","8e9cf4e6":"fig, ax = plt.subplots(figsize=(23, 23))\nsns.heatmap(train[target_cols].corr(), ax=ax  , annot=True)","466bcac6":"train_question_user = train['question_user_name'].unique()\ntest_question_user = test['question_user_name'].unique()\n\nprint('Number of unique question user in train: ', len(train_question_user))\nprint('Number of unique question user in test : ', len(test_question_user))\nprint('Number of unique question user in both train & test : ', len(set(train_question_user) & set(test_question_user)))","cf44b082":"train_answer_user = train['answer_user_name'].unique()\ntest_answer_user = test['answer_user_name'].unique()\n\nprint('Number of unique answer user in train: ', len(train_answer_user))\nprint('Number of unique answer user in test : ', len(test_answer_user))\nprint('Number of unique answer user in both train & test : ', len(set(train_answer_user) & set(test_answer_user)))","dded30c1":"print('Number of unique user in both question & answer in train  : ', len(set(train_answer_user) & set(train_question_user)))\nprint('Number of unique user in both question & answer in test  : ', len(set(test_answer_user) & set(test_question_user)))","7f729762":"def char_count(s):\n    return len(s)\n\ndef word_count(s):\n    return s.count(' ')","fe48a0eb":"train['question_title_n_chars'] = train['question_title'].apply(char_count)\ntrain['question_title_n_words'] = train['question_title'].apply(word_count)\ntrain['question_body_n_chars'] = train['question_body'].apply(char_count)\ntrain['question_body_n_words'] = train['question_body'].apply(word_count)\ntrain['answer_n_chars'] = train['answer'].apply(char_count)\ntrain['answer_n_words'] = train['answer'].apply(word_count)\n\ntest['question_title_n_chars'] = test['question_title'].apply(char_count)\ntest['question_title_n_words'] = test['question_title'].apply(word_count)\ntest['question_body_n_chars'] = test['question_body'].apply(char_count)\ntest['question_body_n_words'] = test['question_body'].apply(word_count)\ntest['answer_n_chars'] = test['answer'].apply(char_count)\ntest['answer_n_words'] = test['answer'].apply(word_count)","bb21a388":"# Number of chars and words in Question title\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\nsns.distplot(train['question_title_n_chars'], label='train', ax=axes[0])\nsns.distplot(test['question_title_n_chars'], label='test', ax=axes[0])\naxes[0].legend()\nsns.distplot(train['question_title_n_words'], label='train', ax=axes[1])\nsns.distplot(test['question_title_n_words'], label='test', ax=axes[1])\naxes[1].legend()","3b8dfe19":"# Number of chars and words in Question body\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\nsns.distplot(train['question_body_n_chars'], label='train', ax=axes[0])\nsns.distplot(test['question_body_n_chars'], label='test', ax=axes[0])\naxes[0].legend()\nsns.distplot(train['question_body_n_words'], label='train', ax=axes[1])\nsns.distplot(test['question_body_n_words'], label='test', ax=axes[1])\naxes[1].legend()","f3ed7614":"# Outlier has too long, let's cut these outlier for visualization.\ntrain['question_body_n_chars'].clip(0, 5000, inplace=True)\ntest['question_body_n_chars'].clip(0, 5000, inplace=True)\ntrain['question_body_n_words'].clip(0, 1000, inplace=True)\ntest['question_body_n_words'].clip(0, 1000, inplace=True)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\nsns.distplot(train['question_body_n_chars'], label='train', ax=axes[0])\nsns.distplot(test['question_body_n_chars'], label='test', ax=axes[0])\naxes[0].legend()\nsns.distplot(train['question_body_n_words'], label='train', ax=axes[1])\nsns.distplot(test['question_body_n_words'], label='test', ax=axes[1])\naxes[1].legend()","97d9a650":"train['answer_n_chars'].clip(0, 5000, inplace=True)\ntest['answer_n_chars'].clip(0, 5000, inplace=True)\ntrain['answer_n_words'].clip(0, 1000, inplace=True)\ntest['answer_n_words'].clip(0, 1000, inplace=True)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\nsns.distplot(train['answer_n_chars'], label='train', ax=axes[0])\nsns.distplot(test['answer_n_chars'], label='test', ax=axes[0])\naxes[0].legend()\nsns.distplot(train['answer_n_words'], label='train', ax=axes[1])\nsns.distplot(test['answer_n_words'], label='test', ax=axes[1])\naxes[1].legend()","5acf2601":"from scipy.spatial.distance import cdist\n\ndef calc_corr(df, x_cols, y_cols):\n    arr1 = df[x_cols].T.values\n    arr2 = df[y_cols].T.values\n    corr_df = pd.DataFrame(1 - cdist(arr2, arr1, metric='correlation'), index=y_cols, columns=x_cols)\n    return corr_df","adf80d5e":"number_feature_cols = ['question_title_n_chars', 'question_title_n_words', 'question_body_n_chars', 'question_body_n_words', 'answer_n_chars', 'answer_n_words']\n# train[number_feature_cols].corrwith(train[target_cols], axis=0)\n\ncorr_df = calc_corr(train, target_cols, number_feature_cols)","afc6fd92":"fig, ax = plt.subplots(figsize=(25, 5))\nsns.heatmap(corr_df, ax=ax, annot=True)","5ba809ef":"num_question = train['question_user_name'].value_counts()\nnum_answer = train['answer_user_name'].value_counts()","52022a2d":"train['num_answer_user'] = train['answer_user_name'].map(num_answer)\ntrain['num_question_user'] = train['question_user_name'].map(num_question)\ntest['num_answer_user'] = test['answer_user_name'].map(num_answer)\ntest['num_question_user'] = test['question_user_name'].map(num_question)","ec386972":"number_feature_cols = ['num_answer_user', 'num_question_user']\ncorr_df = calc_corr(train, target_cols, number_feature_cols)","2aea9f1a":"fig, ax = plt.subplots(figsize=(30, 2))\nsns.heatmap(corr_df, ax=ax ,  annot=True)","cdc19568":"# CODE TAKEN FROM https:\/\/github.com\/kpe\/bert-for-tf2\/\n# ALL CREDITS TO https:\/\/github.com\/kpe\n# CODE COPIED TO LOCAL FOLDER DUE TO INTERNET RESTRICTIONS\n# NORMALLY THIS CODE WOULD BE AVAILABLE VIA pip install bert-for-tf2\n\n# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Tokenization classes.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport re\nimport unicodedata\nimport six\nimport tensorflow as tf\n\n\ndef validate_case_matches_checkpoint(do_lower_case, init_checkpoint):\n    \"\"\"Checks whether the casing config is consistent with the checkpoint name.\"\"\"\n\n    # The casing has to be passed in by the user and there is no explicit check\n    # as to whether it matches the checkpoint. The casing information probably\n    # should have been stored in the bert_config.json file, but it's not, so\n    # we have to heuristically detect it to validate.\n\n    if not init_checkpoint:\n        return\n\n    m = re.match(\"^.*?([A-Za-z0-9_-]+)\/bert_model.ckpt\", init_checkpoint)\n    if m is None:\n        return\n\n    model_name = m.group(1)\n\n    lower_models = [\n        \"uncased_L-24_H-1024_A-16\", \"uncased_L-12_H-768_A-12\",\n        \"multilingual_L-12_H-768_A-12\", \"chinese_L-12_H-768_A-12\"\n    ]\n\n    cased_models = [\n        \"cased_L-12_H-768_A-12\", \"cased_L-24_H-1024_A-16\",\n        \"multi_cased_L-12_H-768_A-12\"\n    ]\n\n    is_bad_config = False\n    if model_name in lower_models and not do_lower_case:\n        is_bad_config = True\n        actual_flag = \"False\"\n        case_name = \"lowercased\"\n        opposite_flag = \"True\"\n\n    if model_name in cased_models and do_lower_case:\n        is_bad_config = True\n        actual_flag = \"True\"\n        case_name = \"cased\"\n        opposite_flag = \"False\"\n\n    if is_bad_config:\n        raise ValueError(\n            \"You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. \"\n            \"However, `%s` seems to be a %s model, so you \"\n            \"should pass in `--do_lower_case=%s` so that the fine-tuning matches \"\n            \"how the model was pre-training. If this error is wrong, please \"\n            \"just comment out this check.\" % (actual_flag, init_checkpoint,\n                                              model_name, case_name, opposite_flag))\n\n\ndef convert_to_unicode(text):\n    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n    if six.PY3:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, bytes):\n            return text.decode(\"utf-8\", \"ignore\")\n        else:\n            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n    elif six.PY2:\n        if isinstance(text, str):\n            return text.decode(\"utf-8\", \"ignore\")\n        elif isinstance(text, unicode):\n            return text\n        else:\n            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n    else:\n        raise ValueError(\"Not running on Python2 or Python 3?\")\n\n\ndef printable_text(text):\n    \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n\n    # These functions want `str` for both Python2 and Python3, but in one case\n    # it's a Unicode string and in the other it's a byte string.\n    if six.PY3:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, bytes):\n            return text.decode(\"utf-8\", \"ignore\")\n        else:\n            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n    elif six.PY2:\n        if isinstance(text, str):\n            return text\n        elif isinstance(text, unicode):\n            return text.encode(\"utf-8\")\n        else:\n            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n    else:\n        raise ValueError(\"Not running on Python2 or Python 3?\")\n\n\ndef load_vocab(vocab_file):\n    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n    vocab = collections.OrderedDict()\n    index = 0\n    with tf.io.gfile.GFile(vocab_file, \"r\") as reader:\n        while True:\n            token = convert_to_unicode(reader.readline())\n            if not token:\n                break\n            token = token.strip()\n            vocab[token] = index\n            index += 1\n    return vocab\n\n\ndef convert_by_vocab(vocab, items):\n    \"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"\n    output = []\n    for item in items:\n        output.append(vocab[item])\n    return output\n\n\ndef convert_tokens_to_ids(vocab, tokens):\n    return convert_by_vocab(vocab, tokens)\n\n\ndef convert_ids_to_tokens(inv_vocab, ids):\n    return convert_by_vocab(inv_vocab, ids)\n\n\ndef whitespace_tokenize(text):\n    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n\n\nclass FullTokenizer(object):\n    \"\"\"Runs end-to-end tokenziation.\"\"\"\n\n    def __init__(self, vocab_file, do_lower_case=True):\n        self.vocab = load_vocab(vocab_file)\n        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n\n    def tokenize(self, text):\n        split_tokens = []\n        for token in self.basic_tokenizer.tokenize(text):\n            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                split_tokens.append(sub_token)\n\n        return split_tokens\n\n    def convert_tokens_to_ids(self, tokens):\n        return convert_by_vocab(self.vocab, tokens)\n\n    def convert_ids_to_tokens(self, ids):\n        return convert_by_vocab(self.inv_vocab, ids)\n\n\nclass BasicTokenizer(object):\n    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n\n    def __init__(self, do_lower_case=True):\n        \"\"\"Constructs a BasicTokenizer.\n        Args:\n          do_lower_case: Whether to lower case the input.\n        \"\"\"\n        self.do_lower_case = do_lower_case\n\n    def tokenize(self, text):\n        \"\"\"Tokenizes a piece of text.\"\"\"\n        text = convert_to_unicode(text)\n        text = self._clean_text(text)\n\n        # This was added on November 1st, 2018 for the multilingual and Chinese\n        # models. This is also applied to the English models now, but it doesn't\n        # matter since the English models were not trained on any Chinese data\n        # and generally don't have any Chinese data in them (there are Chinese\n        # characters in the vocabulary because Wikipedia does have some Chinese\n        # words in the English Wikipedia.).\n        text = self._tokenize_chinese_chars(text)\n\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if self.do_lower_case:\n                token = token.lower()\n                token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token))\n\n        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n        return output_tokens\n\n    def _run_strip_accents(self, text):\n        \"\"\"Strips accents from a piece of text.\"\"\"\n        text = unicodedata.normalize(\"NFD\", text)\n        output = []\n        for char in text:\n            cat = unicodedata.category(char)\n            if cat == \"Mn\":\n                continue\n            output.append(char)\n        return \"\".join(output)\n\n    def _run_split_on_punc(self, text):\n        \"\"\"Splits punctuation on a piece of text.\"\"\"\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if _is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return [\"\".join(x) for x in output]\n\n    def _tokenize_chinese_chars(self, text):\n        \"\"\"Adds whitespace around any CJK character.\"\"\"\n        output = []\n        for char in text:\n            cp = ord(char)\n            if self._is_chinese_char(cp):\n                output.append(\" \")\n                output.append(char)\n                output.append(\" \")\n            else:\n                output.append(char)\n        return \"\".join(output)\n\n    def _is_chinese_char(self, cp):\n        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n        #   https:\/\/en.wikipedia.org\/wiki\/CJK_Unified_Ideographs_(Unicode_block)\n        #\n        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n        # despite its name. The modern Korean Hangul alphabet is a different block,\n        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n        # space-separated words, so they are not treated specially and handled\n        # like the all of the other languages.\n        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n            return True\n\n        return False\n\n    def _clean_text(self, text):\n        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp == 0 or cp == 0xfffd or _is_control(char):\n                continue\n            if _is_whitespace(char):\n                output.append(\" \")\n            else:\n                output.append(char)\n        return \"\".join(output)\n\n\nclass WordpieceTokenizer(object):\n    \"\"\"Runs WordPiece tokenziation.\"\"\"\n\n    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=200):\n        self.vocab = vocab\n        self.unk_token = unk_token\n        self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text):\n        \"\"\"Tokenizes a piece of text into its word pieces.\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n        For example:\n          input = \"unaffable\"\n          output = [\"un\", \"##aff\", \"##able\"]\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer.\n        Returns:\n          A list of wordpiece tokens.\n        \"\"\"\n\n        text = convert_to_unicode(text)\n\n        output_tokens = []\n        for token in whitespace_tokenize(text):\n            chars = list(token)\n            if len(chars) > self.max_input_chars_per_word:\n                output_tokens.append(self.unk_token)\n                continue\n\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                while start < end:\n                    substr = \"\".join(chars[start:end])\n                    if start > 0:\n                        substr = \"##\" + substr\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                if cur_substr is None:\n                    is_bad = True\n                    break\n                sub_tokens.append(cur_substr)\n                start = end\n\n            if is_bad:\n                output_tokens.append(self.unk_token)\n            else:\n                output_tokens.extend(sub_tokens)\n        return output_tokens\n\n\ndef _is_whitespace(char):\n    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n    # \\t, \\n, and \\r are technically contorl characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n        return True\n    cat = unicodedata.category(char)\n    if cat == \"Zs\":\n        return True\n    return False\n\n\ndef _is_control(char):\n    \"\"\"Checks whether `chars` is a control character.\"\"\"\n    # These are technically control characters but we count them as whitespace\n    # characters.\n    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n        return False\n    cat = unicodedata.category(char)\n    if cat in (\"Cc\", \"Cf\"):\n        return True\n    return False\n\n\ndef _is_punctuation(char):\n    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n    cp = ord(char)\n    # We treat all non-letter\/number ASCII as punctuation.\n    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(\"P\"):\n        return True\n    return False","03851e1c":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport tensorflow_hub as hub\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport gc\nimport os\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\n\nnp.set_printoptions(suppress=True)","8d596b02":"BERT_PATH = '..\/input\/bert-base-from-tfhub\/bert_en_uncased_L-12_H-768_A-12'\ntokenizer = FullTokenizer(BERT_PATH+'\/assets\/vocab.txt', True)","ad62aee8":"MAX_SEQUENCE_LENGTH = 512","b89780be":"def _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\ndef _trim_input(title, question, answer, max_sequence_length, \n                t_max_len=60, q_max_len=224, a_max_len=224):\n\n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+4) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)\/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)\/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\" \n                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n        \n        t = t[:t_new_len]\n        q = q[:q_new_len]\n        a = a[:a_new_len]\n    \n    return t, q, a\n\ndef _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n\n    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks(stoken, max_sequence_length)\n    input_segments = _get_segments(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays(df, columns, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        t, q, a = _trim_input(t, q, a, max_sequence_length)\n\n        ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)\n           ]\n\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","ff222ce2":"def compute_spearmanr(trues, preds):\n    rhos = []\n    for col_trues, col_pred in zip(trues.T, preds.T):\n        rhos.append(\n            spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n    return np.nanmean(rhos)\n\n\nclass CustomCallback(tf.keras.callbacks.Callback):\n    \n    def __init__(self, valid_data, test_data, batch_size=16, fold=None):\n\n        self.valid_inputs = valid_data[0]\n        self.valid_outputs = valid_data[1]\n        self.test_inputs = test_data\n        \n        self.batch_size = batch_size\n        self.fold = fold\n        \n    def on_train_begin(self, logs={}):\n        self.valid_predictions = []\n        self.test_predictions = []\n        \n    def on_epoch_end(self, epoch, logs={}):\n        self.valid_predictions.append(\n            self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n        \n        rho_val = compute_spearmanr(\n            self.valid_outputs, np.average(self.valid_predictions, axis=0))\n        \n        print(\"\\nvalidation rho: %.4f\" % rho_val)\n        \n        if self.fold is not None:\n            self.model.save_weights(f'bert-base-{fold}-{epoch}.h5py')\n        \n        self.test_predictions.append(\n            self.model.predict(self.test_inputs, batch_size=self.batch_size)\n        )\n\ndef bert_model():\n    \n    input_word_ids = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_word_ids')\n    input_masks = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_masks')\n    input_segments = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_segments')\n    \n    bert_layer = hub.KerasLayer(BERT_PATH, trainable=True)\n    \n    _, sequence_output = bert_layer([input_word_ids, input_masks, input_segments])\n    \n    x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    out = tf.keras.layers.Dense(30, activation=\"sigmoid\", name=\"dense_output\")(x)\n    \n    model = tf.keras.models.Model(\n        inputs=[input_word_ids, input_masks, input_segments], outputs=out)\n    model.summary()\n    return model    \n        \ndef train_and_predict(model, train_data, valid_data, test_data, \n                      learning_rate, epochs, batch_size, loss_function, fold):\n        \n    custom_callback = CustomCallback(\n        valid_data=(valid_data[0], valid_data[1]), \n        test_data=test_data,\n        batch_size=batch_size,\n        fold=None)\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(loss=loss_function, optimizer=optimizer)\n    model.fit(train_data[0], train_data[1], epochs=epochs, \n              batch_size=batch_size, callbacks=[custom_callback])\n    \n    return custom_callback","26e46a3c":"output_categories = list(train.columns[11:41])\ninput_categories = list(train.columns[[1,2,5]])\n#additional_features = list(train.columns[41:])","dad200da":"gkf = GroupKFold(n_splits=10).split(X=train.question_body, groups=train.question_body)\n\noutputs = compute_output_arrays(train, target_cols)\ninputs = compute_input_arays(train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\ntest_inputs = compute_input_arays(test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)","8743e84c":"histories = []\nfor fold, (train_idx, valid_idx) in enumerate(gkf):\n    \n    if fold <= 6:\n        K.clear_session()\n        model = bert_model()\n        print(f'\/kaggle\/input\/bertbase\/bert-base-{fold}-4.h5py')\n        model.load_weights(f'\/kaggle\/input\/bertbase\/bert-base-{fold}-4.h5py')\n        preds = model.predict(test_inputs,batch_size=8, verbose=1)\n        histories.append(preds)\n    \n    # will actually only do 3 folds (out of 5) to manage < 2h\n    if fold < 10 and fold >=7:\n        print(f'\/kaggle\/input\/bertbaseextra\/bert-base-{fold}-4.h5py')\n        model.load_weights(f'\/kaggle\/input\/bertbaseextra\/bert-base-{fold}-4.h5py')\n        preds = model.predict(test_inputs,batch_size=8, verbose=1)\n        histories.append(preds)","fa3b42fb":"len(histories)","0c4decd4":"test_predictions_google_qa = histories","4e1cdd26":"test_preds_google_qa = [test_predictions_google_qa[i] for i in range(len(test_predictions_google_qa))]\ntest_preds_google_qa = [np.average(test_preds_google_qa, axis=0) for i in range(len(test_preds_google_qa))]\ntest_preds_google_qa = np.mean(test_preds_google_qa, axis=0)\ntest_preds_google_qa.shape","11949f1d":"sample_submission.iloc[:, 1:] = test_preds_google_qa\n\nsample_submission.to_csv('submission.csv', index=False)","cb2d2841":"### Obtain inputs and targets, as well as the indices of the train\/validation splits","7f8bbbec":"### Number of chars and words in answer\n\nAnswer number chars\/words distribution is similar to question body.","424aef59":"## Lets find out feature columns","50f710f5":"# Target labels\n\nEach row has a *qa_id* and other 30 columns as targets ","dd222ce6":"## Create model\n\ncompute_spearmanr() is used to compute the competition metric for the validation set\n\nCustomCallback() is a class which inherits from tf.keras.callbacks.Callback and will compute and append validation score and validation\/test predictions respectively, after each epoch.\n\nbert_model() contains the actual architecture that will be used to finetune BERT to our dataset. It's simple, just taking the sequence_output of the bert_layer and pass it to an AveragePooling layer and finally to an output layer of 30 units (30 classes that we have to predict)\n\ntrain_and_predict() this function will be run to train and obtain predictions","b97e9d82":"## Feature columns\n\nLet's check feature columns one by one.","f8e2c5be":"### Let's focus on the first row of the data. You can access original page mentioned in the url column.\n\nhttps:\/\/photo.stackexchange.com\/questions\/9169\/what-am-i-losing-when-using-extension-tubes-instead-of-a-macro-lens\n\nOnly the question contains *\"title\" (question_title)*, and we have *question_body* and *answer* which is given by sentences.","5756dee1":"## Preprocessing functions","8d63c92d":"### Number of words","d2122b1b":"# Introduction to Data\n### Data Import","759db531":"### User check\n\nThe dataset contains question user and answer user information. This may be because user attribution is impotant, same user tend to answer same kind of question and same answer user tends to answer in similar quality.\n\nLet's check if how the user are distributed, and the user are duplicated in train\/test or not.","ece40102":"Although correlation scale is small and it might not be a \"true correlation\", I can see following pattern:\n\nnum_question_user and question_conversational is correlated: People who post question a lot tend to ask question in conversational form.","8b097978":"### Category Col\n\nThe dataset consists of 5 categories: \"Technology\", \"Stackoverflow\", \"Culture\", \"Science\", \"Life arts\".\n\n[](http:\/\/)Train\/Test distribution is almost same.","7f4bc95b":"## BERT Implementation ","1ad322e6":"It seems some of the labels are quite imbalanced. For example \"question_not_really_a_question\" is almost always 0, which means most of the question in the data is not a noisy data but an \"actual question\".","1deff767":"# EDA","48b90fb0":"### Correlation in target labels\n\nThere are following pairs **corelated**:\n\n1. \"question_type_instructions\" & \"answer_type_instructions\" = 0.77\n1. \"question_type_procedure\" & \"answer_type_procedure\" = 0.61\n1. \"question_type_reason_explanation\" & \"answer_type_reason_explanation\" = 0.59\n\nThis is reasonable that same evaluation on both question & answer are correlated.\n\nOn the other hand, Anticorrelation pattern can be found on following pairs:\n\n1. \"question_fact_seeking\" & \"question_opinion_seeking\" = -0.69\n1. \"answer_type_instruction\" & \"answer_type_reason_explanation\" = -0.48\n\nI think this is also reasonable that question that asks fact & opinion conflicts.\nAnd answer which shows instruction or reason explanation also conflicts.","e83fcaac":"### Each output has 27 question related lables and 9 as answer related lables\n\n**NOTE:** the labels are gi****ven in the continuous range from [0, 1]. NOT binary value.\n\nThis is not a binary prediction challenge. Target labels are aggregated from multiple raters, and can have continuous values in the range [0,1]. Therefore, predictions must also be in that range.","57725a96":"## Training, validation and testing\n\nLoops over the folds in gkf and trains each fold for 5 epochs --- with a learning rate of 1e-5 and batch_size of 8. A simple binary crossentropy is used as the objective-\/loss-function.","0ffda0f0":"Seems several users are in both train & test dataset.\n\nAlso, it seems many users ask question and answer.","fed85820":"Are these feature useful for predicting target values?\n\nLet's check correlation with target values.","1ffeb9fd":"### Word Cloud visualization\n\nLet's see what kind of word are used for question and answer. Also let's check the difference between train and test.","a42f5e52":"## Google QUEST Q&A Labeling\n\n### Improving automated understanding of complex question answer content\n\n> Computers are really good at answering questions with single, verifiable answers. But, humans are often still better at answering questions about opinions, recommendations, or personal experiences. ... In this competition, you\u2019re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering.\n\n![](https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/google-research\/human_computable_dimensions_1.png)\n\nThe competition is **Notebook-only competition**. Your Notebook will re-run automatically against an unseen test set.\n\nThis competition data is small, only made of 6079 rows of train dataset.","81229eeb":"# Number of question or answer by user","c6c6bc35":"It seems common word usage distribution is similar between train & test dataset!","bdb766ae":"**Each row contains a single question and a single answer to that question, along with additional features. The training data contains rows with some duplicated questions (but with different answers). The test data does not contain any duplicated questions.\n**\n\nWhen you access to the URL, you can understand that multiple answer to the single question is given in the page. But only one answer is sampled in the dataset. Also this answer may not be the most popular answer. We can find the answer of this data in the relatively bottom part of the homepage.\n\nOther columns are metadata, which shows **question user property, answer user property and category of question**.","b98141ae":"### We can see following relationship\n\n1. length of answer is correlated with \"answer_level_of_information\".\n1. length of question_title is correlated with \"question_body_critical\" and length of question body is anticorrelated with it.\n1. length of question_body is anticorrelated with \"question_well_written\"","09999416":"# Simple feature engineering\n\nNow, I will proceed simple feature engineering and check if it explains data well or not.\n\n1. Number of words in question title, body and answer.\n1. question_user's question count in train.\n1. answer_user's answer count in train."}}