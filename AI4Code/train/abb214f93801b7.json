{"cell_type":{"2bd0c65c":"code","648ae7a8":"code","b5f59251":"code","ac7cdb05":"code","4dd18174":"code","7a6c5a11":"code","697de05c":"code","8baa7270":"code","7d226c60":"code","aea33989":"code","4e182a63":"code","f5f3ee0f":"code","fb30cb18":"code","d6f4251b":"code","eb17d861":"code","a675c2fd":"code","9a655498":"code","b86f4bd5":"code","afbf04b4":"code","54ff9b97":"code","a1466d54":"code","93098c58":"code","68216fb4":"code","adfb88d8":"code","e27a6e27":"code","49b7003f":"code","26338153":"code","61789bc7":"code","d3659d82":"code","ea744295":"code","48e83fd2":"code","7219ba40":"markdown","0df47fd6":"markdown","34d0839d":"markdown","d1776ad1":"markdown","5bd1efd6":"markdown","6c2a035c":"markdown","1399b663":"markdown"},"source":{"2bd0c65c":"import pandas as pd\nimport numpy as np\nimport json\nimport tensorflow.keras.layers as L\nimport tensorflow as tf\nimport plotly.express as px\nfrom sklearn.preprocessing import quantile_transform,StandardScaler,MinMaxScaler\nfrom transformers import BertConfig,TFBertModel,BertModel","648ae7a8":"AUTO = tf.data.experimental.AUTOTUNE\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","b5f59251":"# This will tell us the columns we are predicting\npred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']","ac7cdb05":"config = BertConfig() ","4dd18174":"config.num_attention_heads","7a6c5a11":"def MCRMSE(y_true, y_pred):\n    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=1)\n\ndef gru_layer(hidden_dim, dropout):\n    return L.Bidirectional(L.GRU(hidden_dim, dropout=dropout, return_sequences=True))\n\ndef build_model(seq_len=107, pred_len=68, dropout=0.5, embed_dim=100, hidden_dim=128):\n    ids = L.Input(shape=(seq_len,3), dtype=tf.int32)\n    flat = L.Flatten()(ids)\n    config = BertConfig() \n    config.vocab_size = 7\n    config.num_hidden_layers = 3\n    config.num_attention_heads = 1\n    config.attention_probs_dropout_prob  = 0.5\n    config.hidden_size = 120\n    config.hidden_act= tf.sinh #tf.tanh\n    bert_model = TFBertModel(config=config)\n\n    bert_embeddings = bert_model(flat)[0]\n\n    hidden = L.AveragePooling1D(pool_size=2)(bert_embeddings)\n    # Since we are only making predictions on the first part of each sequence, we have\n    # to truncate it\n    truncated = hidden[:,:pred_len, :]\n    out = L.Dense(5, activation='linear')(truncated)\n\n    model = tf.keras.Model(inputs=ids, outputs=out)\n\n    model.compile(tf.keras.optimizers.Adam(), loss=MCRMSE)\n    \n    return model","697de05c":"vocab = {\n    'sequence': {x:i for i, x in enumerate(\"A C G U\".split())},\n    'structure': {x:i for i, x in enumerate(\"( . )\".split())},\n    'predicted_loop_type': {x:i for i, x in enumerate(\"B E H I M S X\".split())},\n}\ndef preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n    \n    def f(x):\n        return [vocab['sequence'][x] for x in x[0]],\\\n                [vocab['structure'][x] for x in x[1]],\\\n                [vocab['predicted_loop_type'][x] for x in x[2]],\n\n    return np.array(\n            df[cols]\n            .apply(f, axis=1)\n            .values\n            .tolist()\n        )","8baa7270":"train = pd.read_json('\/kaggle\/input\/stanford-covid-vaccine\/train.json', lines=True)\ntest = pd.read_json('\/kaggle\/input\/stanford-covid-vaccine\/test.json', lines=True)\nsample_df = pd.read_csv('\/kaggle\/input\/stanford-covid-vaccine\/sample_submission.csv')","7d226c60":"print(pd.Series(list(train['structure'][0])).value_counts())\nprint(pd.Series(list(train['sequence'][0])).value_counts())\nprint(pd.Series(list(train['predicted_loop_type'][0])).value_counts())","aea33989":"train.columns","4e182a63":"sorted(train['signal_to_noise'].apply(np.round).astype(int).unique())","f5f3ee0f":"np.bincount(train['signal_to_noise'].apply(np.round).astype(int))","fb30cb18":"sorted(train['SN_filter'].apply(np.round).astype(int).unique()) ","d6f4251b":"np.bincount(train['SN_filter'].apply(np.round).astype(int))","eb17d861":"train = train.query(\"signal_to_noise >= 4\")\ntrain_inputs = preprocess_inputs(train)\ntrain_labels = np.array(train[pred_cols].values.tolist()).transpose((0, 2, 1))","a675c2fd":"train_inputs.shape","9a655498":"for df in [train,test]:\n    df['Paired']=[sum([i=='(' or i==')' for i in j]) for j in df['structure']]\n    df['Unpaired']=[sum([i=='.' for i in j]) for j in df['structure']]\n    for col in ['E','S','H','I','G','A','U']:\n        if col in ['E','S','H','I']:\n            df[col]=[sum([i==col for i in j])\/len(j) for j in df['predicted_loop_type']]\n        else:\n            df[col]=[sum([i==col for i in j])\/len(j) for j in df['sequence']]\nfor a in [ 'G', 'A', 'C', 'U']:\n    train[a+'_position']=[np.sum([i for i in range(len(j)) if j[i]==a])\/len([i for i in range(len(j)) if j[i]==a]) for j in train['sequence']]\n    test[a+'_position']=[np.sum([i for i in range(len(j)) if j[i]==a])\/len([i for i in range(len(j)) if j[i]==a]) for j in test['sequence']]\nfor a in [ 'E', 'S', 'H',]:\n    train[a+'_position']=[np.sum([i for i in range(len(j)) if j[i]==a])\/len([i for i in range(len(j)) if j[i]==a]) for j in train['predicted_loop_type']]\n    test[a+'_position']=[np.sum([i for i in range(len(j)) if j[i]==a])\/len([i for i in range(len(j)) if j[i]==a]) for j in test['predicted_loop_type']]\nfor a in [ 'E', 'S', 'H',]:\n    train[a+'']=[np.sum([i for i in range(len(j)) if j[i]==a])\/len([i for i in range(len(j)) if j[i]==a]) for j in train['predicted_loop_type']]\n    test[a+'_position']=[np.sum([i for i in range(len(j)) if j[i]==a])\/len([i for i in range(len(j)) if j[i]==a]) for j in test['predicted_loop_type']]","b86f4bd5":"target_columns = ['reactivity', 'deg_Mg_pH10','deg_pH10', 'deg_Mg_50C', 'deg_50C']\ntarget_columns.extend(['SN_filter', 'signal_to_noise'])\ntarget_columns.extend(['deg_error_pH10', 'deg_error_Mg_50C', 'deg_error_50C', 'reactivity_error', 'deg_error_Mg_pH10'] )\ntrain.drop(target_columns,axis=1,inplace=True)","afbf04b4":"SC = MinMaxScaler(feature_range=(-1, 1))\ntrain_measurements = SC.fit_transform(pd.concat((train.select_dtypes('float64'),train.select_dtypes('int64')),axis=1))","54ff9b97":"train_measurements.shape","a1466d54":"np.min(train_measurements),np.max(train_measurements)\n# np.min(test_measurements),np.max(test_measurements)","93098c58":"pd.DataFrame(train_measurements).describe().T","68216fb4":"model = build_model()\nmodel.summary()","adfb88d8":"train_inputs.shape,train_labels.shape","e27a6e27":"public_df = test.query(\"seq_length == 107\").copy()\nprivate_df = test.query(\"seq_length == 130\").copy()\npublic_test_measurements  = SC.fit_transform(pd.concat((public_df.select_dtypes('float64'),public_df.select_dtypes('int64')),axis=1))\nprivate_test_measurements  = SC.fit_transform(pd.concat((private_df.select_dtypes('float64'),private_df.select_dtypes('int64')),axis=1))\npublic_inputs = preprocess_inputs(public_df)\nprivate_inputs = preprocess_inputs(private_df)","49b7003f":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=5,shuffle=True,random_state=42)","26338153":"# with tf.device('\/gpu'):\nwith strategy.scope():\n    model = build_model()\n    for fold,(idxT,idxV) in enumerate(kf.split(train_inputs)):\n        history = model.fit(\n            train_inputs[idxT,:,:], train_labels[idxT,:,:], \n            batch_size=64,\n            epochs=100,\n            validation_split=0.05,\n                callbacks=[\n            tf.keras.callbacks.ReduceLROnPlateau(),\n            tf.keras.callbacks.ModelCheckpoint('model'+str(fold)+'.h5',save_weights_only=True,save_best_only=True)\n        ]\n        )\n        # Caveat: The prediction format requires the output to be the same length as the input,\n        # although it's not the case for the training data.\n        model_short = build_model(seq_len=107, pred_len=107)\n        model_long = build_model(seq_len=130, pred_len=130)\n\n        model_short.load_weights('model'+str(fold)+'.h5')\n        model_long.load_weights('model'+str(fold)+'.h5')\n        \n        if fold == 0:\n            public_preds = model_short.predict([public_inputs])\/5\n            private_preds = model_long.predict([private_inputs])\/5\n        else:\n            public_preds += model_short.predict([public_inputs])\/5\n            private_preds += model_long.predict([private_inputs])\/5\n            \n        fig = px.line(\n        history.history, y=['loss', 'val_loss'], \n        labels={'index': 'epoch', 'value': 'Mean Squared Error'}, \n        title='Training History')\n        fig.show()","61789bc7":"#Bert's a bitch with weights\n# model.save_weights('model.h5')","d3659d82":"print(public_preds.shape, private_preds.shape)","ea744295":"preds_ls = []\n\nfor df, preds in [(public_df, public_preds), (private_df, private_preds)]:\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n\n        single_df = pd.DataFrame(single_pred, columns=pred_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n        preds_ls.append(single_df)\n\npreds_df = pd.concat(preds_ls)","48e83fd2":"submission = sample_df[['id_seqpos']].merge(preds_df, on=['id_seqpos'])\nsubmission.to_csv('submission.csv', index=False)","7219ba40":"Public and private sets have different sequence lengths, so we will preprocess them separately and load models of different tensor shapes.","0df47fd6":"For each sample, we take the predicted tensors of shape (107, 5) or (130, 5), and convert them to the long format (i.e. $629 \\times 107, 5$ or $3005 \\times 130, 5$):","34d0839d":"## Define helper functions and useful vars","d1776ad1":"## Post-processing and submit","5bd1efd6":"## Predict on test set","6c2a035c":"## Load and preprocess data","1399b663":"## Build and train model"}}