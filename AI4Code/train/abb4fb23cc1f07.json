{"cell_type":{"3ef3aa3a":"code","1285d244":"code","6cf6a758":"code","03bf4612":"code","555e79cd":"code","1dc1d71a":"code","25f27c10":"code","6f738c80":"code","f35f7045":"code","9511cd38":"code","da2708b3":"code","57ffde86":"code","1824e57c":"code","42681403":"code","c6c2b5a0":"code","424722e0":"code","1fbc1deb":"code","33439da8":"code","6372dc7e":"code","7c3ecef8":"code","92d203e2":"code","3cfe0f6b":"code","42c9896a":"code","8585876c":"code","fc8bda79":"code","7a61b26a":"code","63ff8221":"code","8c1b964e":"markdown","48cbac70":"markdown","e13f4eb4":"markdown","45f03183":"markdown","b65906d1":"markdown","ff3de27a":"markdown","ae826cdc":"markdown","894b815c":"markdown","5e0ef152":"markdown","e117cd62":"markdown","8f5dea13":"markdown","7a713001":"markdown","fe9f17d3":"markdown","f788f103":"markdown","05beb951":"markdown","0a96c531":"markdown","0026762b":"markdown","74e213d1":"markdown","a663bfb9":"markdown","18a36ed8":"markdown","7326b181":"markdown","9050dd70":"markdown","f44f97ec":"markdown","1f39c8cd":"markdown","fbdc0226":"markdown","a483361f":"markdown","ff414ff2":"markdown","9605705b":"markdown","116ecf71":"markdown","ffd875c1":"markdown","557604c1":"markdown","64ba2b1c":"markdown","6074400c":"markdown","1eed3362":"markdown","4a2e018b":"markdown"},"source":{"3ef3aa3a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb # import xgboost KEY PART\nimport os\nprint(os.listdir(\"..\/input\"))","1285d244":"seed = 888 # keep in constant to ouput same result","6cf6a758":"df = pd.read_csv(\"..\/input\/Admission_Predict_Ver1.1.csv\")\ndf.head()","03bf4612":"print(df.shape)","555e79cd":"print(df.info())","1dc1d71a":"print(\"Number of columns: \",len(df.columns))\nprint(df.columns)","25f27c10":"# dropping Serial No. as the index will be not needed here\ndf.drop(['Serial No.'], axis = 1, inplace = True)\n# rename columns to make things easier\ndf.rename(columns = {'LOR ':'LOR', 'Chance of Admit ':'Chance of Admit'}, inplace = True)\nprint(df.columns)","6f738c80":"df.describe()","f35f7045":"# plotting heatmap for easier visuallization of correlation\nfig,ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(df.corr(), ax=ax, annot=True)\nplt.show()","9511cd38":"df[df.columns[:-2]].hist(bins = 100,figsize = (12,10))\nplt.tight_layout()\nplt.show()","da2708b3":"# remove spaces in features: it is to prevent error later for plotting XGBoost tree\ndf.rename(columns = {'GRE Score':'GRE', 'TOEFL Score':'TOEFL', 'University Rating':'UniRating', 'LOR ':'LOR', 'Chance of Admit ':'Chance_of_Admit'}, inplace = True)\nprint(df.columns)\n\n# seperate data into features and output\nX, y = df.iloc[:, :-1], df.iloc[:, -1]","57ffde86":"# seperate data into train and validation data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = seed)\n# prepare model to train\nxg_reg = xgb.XGBRegressor(objective ='binary:logistic', colsample_bytree = 0.4, learning_rate = 0.1,\n                alpha = 0, n_estimators = 10)","1824e57c":"# fit & predict\nxg_reg.fit(X_train,y_train)\npreds = xg_reg.predict(X_test)","42681403":"# compute RMSE root mean square error\nrmse = np.sqrt(mean_squared_error(y_test, preds))\nprint(\"RMSE (test): %f\" % (rmse))\npreds_train = xg_reg.predict(X_train)\n# compute RMSE for trained data\nrmse = np.sqrt(mean_squared_error(y_train, preds_train))\nprint(\"RMSE (train): %f\" % (rmse))\n\n# import function for computing r2 score\nfrom sklearn.metrics import r2_score\n\n# compute r2 score\nprint(\"r_square score (test): \", r2_score(y_test, preds))\nprint(\"r_square score (train): \", r2_score(y_train, preds_train))","c6c2b5a0":"print(\"Some predictions vs real data:\")\nfor i in range(0,100,20):\n    print(preds[i], \"\\t\",y_test.iloc[i])","424722e0":"# convert dataset into optimized data structure 'DMatrix'\ndata_dmatrix = xgb.DMatrix(data = X, label = y)","1fbc1deb":"def gini(actual, pred, cmpcol = 0, sortcol = 1):\n    assert( len(actual) == len(pred) )\n    all = np.asarray(np.c_[ actual, pred, np.arange(len(actual)) ], dtype=np.float)\n    all = all[ np.lexsort((all[:,2], -1*all[:,1])) ]\n    totalLosses = all[:,0].sum()\n    giniSum = all[:,0].cumsum().sum() \/ totalLosses\n    \n    giniSum -= (len(actual) + 1) \/ 2.\n    return giniSum \/ len(actual)\n\ndef gini_normalized(a, p):\n    return gini(a, p) \/ gini(a, a)\n\ndef gini_xgb(preds, dtrain):\n    labels = dtrain.get_label()\n    gini_score = gini_normalized(labels, preds)\n    return [('gini', gini_score)]","33439da8":"params = {'objective':'binary:logistic','colsample_bytree': 0.4, 'subsample': 0.5, 'learning_rate': 0.005,\n                'max_depth': 6, 'alpha':0}\n\ncv_results = xgb.cv(dtrain = data_dmatrix, params = params, nfold = 10,\n                    num_boost_round = 3000, early_stopping_rounds = 10,\n                    metrics = [\"rmse\"], feval = gini_xgb, as_pandas=True, seed = seed)","6372dc7e":"cv_results.head()","7c3ecef8":"cv_results.tail()","92d203e2":"# prepare model to train\nxg_reg_v2 = xgb.XGBRegressor(objective ='binary:logistic', colsample_bytree = 0.4, subsample = 0.5, learning_rate = 0.005,\n                alpha = 0, n_estimators = 1087, eval_metric= [\"rmse\", \"auc\"])\n# fit & predict\nxg_reg_v2.fit(X_train,y_train)\n# predict results for later comparisons\npreds_v2 = xg_reg_v2.predict(X_test)","3cfe0f6b":"for i in range(4):\n    xgb.plot_tree(xg_reg_v2,num_trees=i, rankdir='LR')\n    plt.rcParams['figure.figsize'] = [30, 30]\nplt.show()","42c9896a":"xgb.plot_importance(xg_reg_v2)\nplt.rcParams['figure.figsize'] = [8, 8]\nplt.show()","8585876c":"# compute RMSE root mean square error\nrmse = np.sqrt(mean_squared_error(y_test, preds_v2))\nprint(\"RMSE(test): %f\" % (rmse))\npreds_train_v2 = xg_reg_v2.predict(X_train)\n# compute RMSE for trained data\nrmse = np.sqrt(mean_squared_error(y_train, preds_train_v2))\nprint(\"RMSE(train): %f\" % (rmse))\n\nfrom sklearn.metrics import r2_score\n\nprint(\"r_square score (test): \", r2_score(y_test, preds_v2))\nprint(\"r_square score (train): \", r2_score(y_train, preds_train_v2))","fc8bda79":"print(\"Some predictions vs real data:\")\nfor i in range(0,100,20):\n    print(preds_v2[i], \"\\t\",y_test.iloc[i])","7a61b26a":"blue = plt.scatter(np.arange(len(preds)\/\/2), preds[::2], color = \"blue\")\ngreen = plt.scatter(np.arange(len(y_test)\/\/2), y_test[::2], color = \"green\")\nred = plt.scatter(np.arange(len(preds_v2)\/\/2), preds_v2[::2], color = \"red\")\nplt.title(\"Comparison of XGBoost\")\nplt.xlabel(\"Index of Candidate\")\nplt.ylabel(\"Chance of Admit\")\nplt.legend((blue, red, green),('XGBv1', 'XGBv2', 'Actual'))\nplt.show()","63ff8221":"df[\"Chance of Admit\"].plot(kind = 'hist',bins = 300,figsize = (6,6))\nplt.title(\"Chance of Admit\")\nplt.xlabel(\"Chance of Admit\")\nplt.ylabel(\"Frequency\")\nplt.show()","8c1b964e":"## [K-fold Cross Validation](#K-fold-Cross-Validation)\nHere we will be trying to improve the model by adjusting some parameters and applying cross validation\n<br><br>\n*Note: Even though I have not noted here, I have tried adjusting some of the parameters myself. It is upto one to try explore more on how each parameters may outcome different result. Document can be found [here](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html#general-parameters)*","48cbac70":"### [Preparing CV Model](#Preparing-CV-Model)\n* ***nfold*** is set to ***10***. This is 10-fold cross validation model (k=5 or k=10 is somewhat arbitrary)\n* ***colsample_bytree*** is adjusted to ***0.4*** which is less than default value 1 to **prevent overfitting** (Mentioned earlier)\n* ***subsample*** ratio is set to ***0.5***: XGBoost would randomly sample half of the training data prior to growing trees and this will **prevent overfitting**.\n* ***max_depth*** is set to ***6*** (default value) determines maximum depth each tree is allowed to grow durig boosting rounds. \n    * Setting this to 0 indicates no limit to max_depth\n    * Icreasing this value will make the model more complex and more likely to overfit\n* ***learining_rate*** is set to ***0.005*** which is really low (tried some larger numbers but seems setting it at 0.005 performs better)\n    * Advantage: Higher chance of acheving more robust model and can prevent overshooting the point of convergence for the optimal solution\n    * Disadvantage: Slower convergence to optimal solution. Hence, it takes longer time\n* ***early_stopping rounds*** is set to 10 to prevent overfitting so that model will not need to train if the evaluation scores do not improve for a given number of rounds.\n* ***num_boost_round*** is set to ***3000***. This may seems really high but I set it high since I have set ***early_stopping_rounds*** and ***low learning_rate***\n* ***evaluation metrics*** used here are RMSE and Gini. Since RMSE evaluation funnction is in-built, we simply set ***metrics = \"rmse\"***. For Gini, we add function that we defined above by stating in feval: ***feval = gini_xgb***\n\nNote: If one wants to use AUC with RMSE insead of Gini, since AUC function is in-built, he\/she can simple state **metrics = [\"rmse\", \"auc\"]**. More options on XGBoost CV can be found [here](https:\/\/rdrr.io\/cran\/xgboost\/man\/xgb.cv.html).","e13f4eb4":"### [Shape of dataframe](#Shape-of-dataframe)","45f03183":"![](http:\/\/)","b65906d1":"## [Introduction](#Introduction)\n### [Purpose](#Purpose)\nUsing XGBoost to predict the chance of admit based on different features.\n### [Dataset](#Dataset)\nFeatures in the dataset:\n* GRE Scores (290 to 340)\n* TOEFL Scores (92 to 120)\n* University Rating (1 to 5)\n* Statement of Purpose (1 to 5)\n* Letter of Recommendation Strength (1 to 5)\n* Undergraduate CGPA (6.8 to 9.92)\n* Research Experience (0 or 1)\n* Chance of Admit (0.34 to 0.97)\n\nUsing ***Admission_Predict_Ver1.1.csv*** retrieved from https:\/\/www.kaggle.com\/mohansacharya\/graduate-admissions\n\n*Note: There may be some scaling & contrast problems in xgboost related images like plotting trees when displaying. It can be fixed by forking the kernel and re-run the plotting lines*","ff3de27a":"Quick look at the initial evaluation values","ae826cdc":"### [Histograms](#Histograms)\nBy looking at Some of these histograms, we can roughly see what candidates should have in order to **stand out** by observing **high frequency\/peaks** and **density** between points in the histogram.\n* For CGPA, some of points where we can stand out from others is around 8.65 and 9.15\n* For GRE Score, 312 and 325 seems good score to be stand out\n* For LOR, 4.0 seems good to stand out\n* For SOP, 4.0 seems good to stand out\n* For TOEFL, 105 and 110 seems good score to stand out\n* From UniRating, we can observe **high frequency** in rating **3**.  By looking at the shape of the histogram, we can observe that **lower\/higher** the university rating, **lesser people tend to apply**. From this, we can carefully assume that **lower the university rating**, **higher the admission chance** due to less competition. However, this is likely **not true** for the **higher university rating**.","894b815c":"## [Quick Look at the Dataset](#Quick-Look-at-the-Dataset)","5e0ef152":"### [Statiscal information of dataset](#Statiscal-information-of-dataset)","e117cd62":"### [Scatter Plot of Predictions](#Scatter-Plot-of-Predictions)\n* Scattered points are plotted every 2 steps for easier visualization\n* **Blue** indicates the predictions made by the **initially built model**\n* **Red** indicates the predictions made by the **improved model**\n* **Green** indicates the **actual outcomes**\n* It can be clearly seen that the improved model is performing much better than the initial model","8f5dea13":"Quick look at the improvements at the end of CV\n* At the end of the CV round, we can observe hugh improvements on both Gini and RMSE","7a713001":"## [Preparing For Dataset](#Preparing-For-Dataset)","fe9f17d3":"### [More detail look at the structure of the data](#More-detail-look-at-the-structure-of-the-data)\n<br>*Note: It is good that all the data is **non-null** so we don't have any null value to deal with.*","f788f103":"### [More Robust XGBoost](#More-Robust-XGBoost)\nPreparing improved model by setting parameters based on the outcomes of 10-fold cross validation\n\n*Note: Here I used auc instead of Gini. The model when I used Gini with RMSE output same result as when used AUC with RMSE\n<br> Didn't use ***xgb.train*** for convenient purpose for evaluating performance later. Hence, there may be slight difference in the outcome.*","05beb951":"### [Plotting Trees](#Plotting-Trees)\nHere are some of tress in more robus XGBoost.\n<br> Index of trees can be stated in ***num_trees***","0a96c531":"### [Quick look at the predictions](#Quicklook-at-the-predictions)\n* The model did not perform well on both RMSE and R-SQUARED\n    * RMSE is quite high given that our range varies from 0 to 1 \n    * R-SQUARED is low given that 1 is best possible score\n* By observing and comparing some predictions made by the model to the actaul outcome, we can see that the model is not so well predicting the outcome\n\n*Note: Do not worry so much about the low scores as we will be trying to improve our model*","0026762b":"### [Reading Dataset into dataframe](#Reading-Dataset-into-dataframe)","74e213d1":"### [Splitting data & Preparing XGBoost model](#Splitting-data-&-Preparing-XGBoost-model)\nSplit data into train and validation data 8:2 ratio randomly.\n<br>Prepare XGBoost model for training:\n* ***binary:logistic*** was used in this model as we want to predict **probablity** of admission. (binary:logistic is used aslogistic regression for binary classification, output probability)\n* ***colsample_bytree*** is adjusted to ***0.4*** which is less than default value 1 to **prevent overfitting**\n* ***learning_rate*** is not set so low for now and ***n_estimator*** is not set high for now as we will be **improving the model later on**\n\n*Note: There isn't really difference setting objective as 'reg:logistic' or 'binary:logistic' to train and test. Just that default evaluation metrics for 'reg:logistic' is RMSE (Root Mean Square Error)*","a663bfb9":"## [XGBoost](#XGBoost)","18a36ed8":"### [Names of features](#Names-of-features)","7326b181":"## [Conclusion](#Conclusion)\n* The improved XGBoost model shows that XGBoost is a method to be used in training and predicting the chance of admit in this context.\n* It is important to carry out some experiments (in this case, K-fold cross validation) to improve the model\n    * adjusting some parameters in XGBoost model can affect the performance of the model hugely \n    * Note: If the dataset is too huge, one may not want to use K-fold cross validation as it may take really long time\n* XGBoost can be useful in extracting the important features from the dataset \n* By looking more detail at the predictions made by the model, we can observe that having more data of candidates with lower chance of admit will likely increase the model's performance\n","9050dd70":"# OUTLINE\n## [INTRODUCTION](#Introduction)\n* ### [PURPOSE](#Purpose)\n* ### [DATASET](#Dataset)\n\n## [PREPARING FOR DATASET](#Preparing-For-Dataset)\n* ### [IMPORTING LIBRARIES](#Importing-libraries)\n* ### [READING DATASET INTO DATAFRAME](#Reading-dataset-into-dataframe)\n\n## [QUICK LOOK AT THE DATASET](#Quick-Look-at-the-Dataset)\n* ### [SHAPE](#Shape-of-dataframe)\n* ### [STRUCTURE](#More-detail-look-at-the-structure-of-the-data)\n* ### [FEATURES](#Names-of-features)\n* ### [STATISTICAL INFORMATION](#Statiscal-information-of-dataset)\n* ### [CORRELATION](#Correlation-between-all-features)\n* ### [HISTOGRAMS](#Histograms)\n\n## [XGBOOST](#XGBoost)\n* ### [PREPARATION OF TRAIN & VALIDATION DATA](#Preparation-of-Train-and-Validation-Data)\n* ### [SPLIT DATA & PREPARE MODEL](#Splitting-data-&-Preparing-XGBoost-model)\n* ### [FIT & PREDICT](#Fit & Predict)\n* ### [ANALYSE RMSE & R-SQAURED](#RMSE-&-R-SQAURED)\n* ### [SOME PREDICTIONS](#Quick-look-at-the-predictions)\n\n## [K-FOLD CROSS VALIDATION](#K-fold-Cross-Validation)\n* ### [DMATRIX](#Converting-into-Dmatrix)\n* ### [GINI SCORE FUNCTION](#Defining-Gini-Score-Function)\n* ### [CV MODEL](#Preparing-CV-Model)\n* ### [IMPROVED XGBOOST](#More-Robust-XGBoost)\n* ### [TREES](#Plotting-Trees)\n* ### [FEATURE RANKING](#Feature-Ranking)\n* ### [ANALYSE RMSE & R-SQAURED](#RMSE-&-R-SQAURED-IMPROVED)\n* ### [SCATTER PLOT](#Scatter-Plot-of-Predictions)\n* ### [FINAL LOOK AT THE PERFOMANCE](#Analysing-Performance)\n\n## [CONCLUSION](#Conclusion)","f44f97ec":"### [Preparation of Train and Validation Data](#Preparation-of-Train-and-Validation-Data)\nRemoving all the spaces in the name of the features as having spaces will result in error when plotting XGBoost tree.\n<br> Separate **features** and **outcomes** into ***X*** and ***y***","1f39c8cd":"### [Defining Gini Score Function](#Defining-Gini-Score-Function)\nDefining custom gini score function to be used as one of evaluation function during cross validation. \n<br> Gini metric, source from [anokas' kernel](http:\/\/https:\/\/www.kaggle.com\/anokas\/simple-xgboost-btb-0-27)\n<br><br>\n*Note: It is okay to just use AUC instead of Gini as the evaluation function. (Gini = 2 x AUC \u2013 1)*\n\n***Note: Gini above 60% is a good model***","fbdc0226":"### [Analysing Performance](#Analysing-Performance)\nIn the above graph, we can observe that the improved model tend to predict quite well most of times. \nHowever, it doesn't seem to predict well at range below around ***0.43***\nBy observing the histogram plotted below, we can see that this is likely due to **lack of data** on candidates who have **lower Chance of Admit** below around ***0.6*** to train the model.","a483361f":"### [Feature Ranking](#Feature-Ranking)\nThe graph shows feature importance score.\n<br> By observing importance score, we can try to remove less important\/relavant feature to try creating more robust model. In the example, it will be *Research*\n\n*Note: **Higher importance scores indicates usefulness** of the feature(s) during the construction of the boosted decision tree model based on how often each feature is used in making key decisions in the trees*","ff414ff2":"### [RMSE & R-SQAURED IMPROVED](#RMSE-&-R-SQAURED-IMPROVED)\n* There was hugh improvements on both scores.\n* From the scores we can see that test data is not overfitted.\n* Based on RMSE and R-SQAURED, the model seems to perform well on both. ","9605705b":"### [Correlation between all features](#Correlation-between-all-features)\n* The 3 **most important** features for admission: CGPA, GRE Score, TOEFL Score\n* The 3 **least important** features for admission: Research, LOR, and SOP\n\n*<br>Note: **Higher** correlation means likely more **important** determinant feature (preferably x > 0.8).\n<br>We will be looking at alternative ways to perhaps better rank these features.*","116ecf71":"### [Importing libraries](#Importing-libraries)","ffd875c1":"**Remove** the **Serial No**. as it denotes **index** which is **not needed** for trainings & **Remove spaces between names** of features","557604c1":"Quick look at the actual predictoins by the improved model\n* We can see that the predictions are closer to the actual outcome,","64ba2b1c":"### [Converting into Dmatrix](#Converting-into-Dmatrix)\nCoverting dataset into an optimized data structure '***Dmatrix***' that XGBoost supports","6074400c":"### [RMSE & R-SQAURED](#RMSE-&-R-SQAURED)\nNote about **RMSE**: \n* Statistical measure of how spread out these residuals are\n* A good model will have **very similar RMSE** values for training and test sets\n* If the RMSE for the test set is **much higher** than that of the training set, it is likely that the data is overfitted\n\nNote about **R-SQUARED**:\n* Statistical measure of how close the data are to the fitted regression line\n* A good model will have R-SQUARED score close to **1**\n* The score may be negative","1eed3362":"Setting seed **constant** so the training outcomes will be **same throughout** <br>\n<br>*Note: default value 0 can be used*","4a2e018b":"### [Fit & Predict](#Fit-&-Predict)\n*Note: Do not need normalization of train and validation data since we are using trees*\n<br> *If you do want to normalize, please normalize **after** you splite the data.*"}}