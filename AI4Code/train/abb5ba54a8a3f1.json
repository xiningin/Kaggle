{"cell_type":{"e651e9c5":"code","b62b3d24":"code","bfe1c72a":"code","32cba8cc":"code","952a0925":"code","76ed9aed":"code","fb7b2ae4":"code","5bee8cb7":"code","2e4ca729":"code","642a0a57":"code","03817c0a":"code","d041e5e7":"code","a2ff78d3":"code","aac975e7":"code","c512c154":"code","3a2c837f":"code","4fa41933":"code","b1983113":"code","30db6831":"code","b934bfcf":"code","80aaa7dc":"code","70c5e495":"code","92b7546a":"code","74512f30":"code","be3713ce":"code","bda1007f":"code","e88cd58c":"code","b649f2a9":"code","8f653dec":"code","1c53c9c8":"code","25db0b08":"code","b7e0eaae":"code","59cbf05e":"code","2e1de092":"code","4f58b007":"code","53407c3a":"code","2e633953":"code","b34b31cb":"code","27562bfe":"code","14671230":"code","274f702e":"code","46a742e7":"code","5078dbb5":"code","7f9990fb":"code","69bec789":"code","e7184bd7":"code","a246b642":"markdown","e6ab5682":"markdown","d5a56985":"markdown","b1004eb6":"markdown","e08abda6":"markdown","54effc1b":"markdown","338da965":"markdown","174ad1ae":"markdown","fc1fe040":"markdown","2d1964c4":"markdown","cfa25a59":"markdown","0e5930b8":"markdown","f82ed681":"markdown","895fcb64":"markdown","ffd64650":"markdown","b0a1fb65":"markdown","9b3b6031":"markdown"},"source":{"e651e9c5":"import os\nos.listdir(\"..\/input\")","b62b3d24":"!mkdir labelencoder\n!mkdir onehotencoder\n!mkdir inputs\n!mkdir standartScaler\n!mkdir test_data\n!mkdir specific\n!mkdir model\n!mkdir model\/teams","bfe1c72a":"#Importing Libraries\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.externals import joblib\nfrom sklearn.preprocessing import OneHotEncoder","32cba8cc":"def get_golden_feature_list(df,threshold=.5):\n    df_numeric = df.select_dtypes(include=[\"float64\",\"int64\"])\n    df_corr = df_numeric.corr()\n    df_corr = df_corr.iloc[2,:-1]\n    golden_features_list = df_corr[abs(df_corr) >= threshold].sort_values(ascending=False)\n    return golden_features_list\n","952a0925":"def get_worst_feature_list(df,threshold=.5):\n    df_numeric = df.select_dtypes(include=[\"float64\",\"int64\"])\n    df_corr = df_numeric.corr()\n    df_corr = df_corr.iloc[2,:-1]\n    golden_features_list = df_corr[abs(df_corr) <= threshold].sort_values(ascending=False)\n    return golden_features_list\n","76ed9aed":"# Clean Up Columns Names\ndef remove_whitespaces_in_df_columns(df):\n    df.columns = df.columns.str.replace(' ', '')\n    return df.columns","fb7b2ae4":"# Get Columns Names For All Data\ndef get_column_names():\n    \"\"\"\n    :return: Column names of Data.csv\n    \"\"\"\n    feature_names_of_players = [\"PLAYER_NAME\", \"MIN\", \"FGM\", \"FGA\", \"FG_PCT\", \"FG3M\", \"FG3A\", \"FG3_PCT\", \"FTM\", \"FTA\",\n                                \"FT_PCT\", \"OREB\", \"DREB\", \"REB\", \"AST\", \"STL\", \"BLK\", \"TO\", \"PF\", \"PTS\", \"PLUS_MINUS\"]\n\n    feature_names_of_matchups = [\"gameId\", \"teamAbbr\", \"opptAbbr\", \"rslt\", \"teamMin\", \"teamPTS\", \"teamPTS1\", \"teamPTS2\",\n                                 \"teamPTS3\", \"teamPTS4\", \"opptPTS\",\n                                 \"opptPTS1\", \"opptPTS2\", \"opptPTS3\", \"opptPTS4\",\n                                 \"teamFGM\", \"teamFGA\", \"teamFG\", \"team3PM\", \"team3PA\", \"team3PCT\", \"teamFTM\", \"teamFTA\",\n                                 \"teamFTC\", \"teamORB\", \"teamDRB\", \"teamREB\", \"teamAST\", \"teamSTL\"\n        , \"teamBLK\", \"teamTO\", \"teamPF\", \"team2P\", \"teamTS\", \"teamEFG\", \"teamPPS\", \"teamFIC\", \"teamFIC40\", \"teamOrtg\",\n                                 \"teamDrtg\", \"teamPlay\",\n                                 \"opptMin\", \"opptFGM\", \"opptFGA\", \"opptFG\", \"oppt3PM\", \"oppt3PA\", \"oppt3PCT\", \"opptFTM\",\n                                 \"opptFTA\", \"opptFTC\", \"opptORB\", \"opptDRB\", \"opptREB\", \"opptAST\", \"opptSTL\"\n        , \"opptBLK\", \"opptTO\", \"opptPF\", \"oppt2P\", \"opptTS\", \"opptEFG\", \"opptPPS\", \"opptFIC\", \"opptFIC40\", \"opptOrtg\",\n                                 \"opptDrtg\", \"opptPlay\", \"poss\", \"pace\"]\n\n    team_features = [\"gameId\", \"teamAbbr\", \"opptAbbr\", \"rslt\", \"teamMin\", \"teamPTS\", \"teamPTS1\", \"teamPTS2\", \"teamPTS3\",\n                     \"teamPTS4\", \"opptPTS\",\n                     \"opptPTS1\", \"opptPTS2\", \"opptPTS3\", \"opptPTS4\",\n                     \"teamFGM\", \"teamFGA\", \"teamFG\", \"team3PM\", \"team3PA\", \"team3PCT\", \"teamFTM\", \"teamFTA\", \"teamFTC\",\n                     \"teamORB\", \"teamDRB\", \"teamREB\", \"teamAST\", \"teamSTL\"\n        , \"teamBLK\", \"teamTO\", \"teamPF\", \"team2P\", \"teamTS\", \"teamEFG\", \"teamPPS\", \"teamFIC\", \"teamFIC40\", \"teamOrtg\",\n                     \"teamDrtg\", \"teamPlay\"]\n    team_features = team_features + feature_names_of_players * 11\n\n    oppt_features = [\"opptMin\", \"opptFGM\", \"opptFGA\", \"opptFG\", \"oppt3PM\", \"oppt3PA\", \"oppt3PCT\", \"opptFTM\", \"opptFTA\",\n                     \"opptFTC\", \"opptORB\", \"opptDRB\", \"opptREB\", \"opptAST\", \"opptSTL\"\n        , \"opptBLK\", \"opptTO\", \"opptPF\", \"oppt2P\", \"opptTS\", \"opptEFG\", \"opptPPS\", \"opptFIC\", \"opptFIC40\", \"opptOrtg\",\n                     \"opptDrtg\", \"opptPlay\"]\n\n    oppt_features = oppt_features + feature_names_of_players * 11\n\n    last_features = [\"poss\", \"LM_totalPoint\",\"LM_dayOffset\",\"pace\"]\n\n    feature_names_of_matchups = team_features + oppt_features + last_features\n    return feature_names_of_matchups\n","5bee8cb7":"# Getting unique team names \ndef get_unique_team_abbr(df):\n    \"\"\"\n\n    :param df: Data.csv\n    :return: Team Abbrs\n    \"\"\"\n    return np.unique(df.teamAbbr.values)\n","2e4ca729":"# Getting Unique Player List\ndef get_unique_player_list(df):\n    \"\"\"\n    :param df: Data.csv\n    :return: Player List\n    \"\"\"\n    players = []\n    for i in range(df.PLAYER_NAME.shape[0]):\n        for j in range(df.PLAYER_NAME.shape[1]):\n            players.append(df.PLAYER_NAME.iloc[i, j])\n\n    unique_players = np.unique(players)\n    return unique_players\n","642a0a57":"# Now,We write 3 methods for re-creating all player stats dataset.\n\n# First one is creating empty dataset to fill with real stats.\n\ndef create_empty_player_stats(player_list):\n    \"\"\"\n\n    :param player_list: Oyuncu Listesi\n    :return: Oyuncu say\u0131s\u0131 kadar row olan stats tablosu d\u00f6nd\u00fcr\u00fcr\n    \"\"\"\n    col_names_for_player_stats = [\"PLAYER_NAME\", \"MIN\", \"FGM\", \"FGA\", \"FG_PCT\", \"FG3M\", \"FG3A\", \"FG3_PCT\", \"FTM\", \"FTA\",\n                                  \"FT_PCT\", \"OREB\", \"DREB\", \"REB\", \"AST\", \"STL\", \"BLK\", \"TO\", \"PF\", \"PTS\", \"PLUS_MINUS\"]\n    index_for_player_stats = range(len(player_list))\n    player_stats_df = pd.DataFrame(columns=col_names_for_player_stats, index=index_for_player_stats)\n    return player_stats_df\n\n# Second one is splitting main data and getting player stats\n\ndef get_all_players_stats_df(df):\n    \"\"\"\n\n    :param df: Data.csv --> Dataframe\n    :return:B\u00fct\u00fcn oyuncular\u0131n t\u00fcm ma\u00e7lardaki verilerini d\u00f6nd\u00fcr\u00fcr\n    \"\"\"\n    team_player_first_column_index = df.columns.get_loc(\"teamPlay\") + 1\n    team_player_last_column_index = df.columns.get_loc(\"opptMin\")\n    oppt_player_first_column_index = df.columns.get_loc(\"opptPlay\") + 1\n    oppt_player_last_column_index = df.columns.get_loc(\"poss\")\n    team_player_stats = df.iloc[:, team_player_first_column_index:team_player_last_column_index]\n    oppt_player_stats = df.iloc[:, oppt_player_first_column_index:oppt_player_last_column_index]\n    player_stats_conc = pd.concat([team_player_stats, oppt_player_stats], axis=0)\n    player_stats_conc_splited = np.array_split(player_stats_conc, 11, axis=1)\n    full_df = pd.concat([player_stats_conc_splited[0],\n                         player_stats_conc_splited[1],\n                         player_stats_conc_splited[2],\n                         player_stats_conc_splited[3],\n                         player_stats_conc_splited[4],\n                         player_stats_conc_splited[5],\n                         player_stats_conc_splited[6],\n                         player_stats_conc_splited[7],\n                         player_stats_conc_splited[8],\n                         player_stats_conc_splited[9],\n                         player_stats_conc_splited[10]], axis=0)\n    return full_df\n\n# Last one is filling empty dataframe with these datas\n# Last x Match variable is how many match ago\n\ndef get_all_player_stats_last_x_match(all_player_stats_df,all_player_stats_empty_df,player_list,last_x_match=10):\n    \"\"\"\n\n    :param all_player_stats_df: Ma\u00e7lara g\u00f6re olan b\u00fct\u00fcn oyuncular\u0131n verileri\n    :param all_player_stats_empty_df: Player say\u0131s\u0131 kadar row olan bo\u015f veri tablosu\n    :param player_list: Playerlar\u0131n listesi\n    :param last_x_match: Son ka\u00e7 ma\u00e7 oldu\u011fu\n    :return: B\u00fct\u00fcn oyuncular\u0131n ortalama verileri\n    \"\"\"\n    for i in range(len(player_list)):\n        each = player_list[i]\n        player_stats_each = all_player_stats_df[all_player_stats_df.PLAYER_NAME == each][-1:-last_x_match:-1]\n        player_name = each\n        mean_of_stats_this_player = player_stats_each.drop(columns=[\"PLAYER_NAME\"], axis=1).mean()\n        all_player_stats_empty_df.at[i, \"PLAYER_NAME\"] = player_name\n        all_player_stats_empty_df.iloc[i, 1:] = mean_of_stats_this_player\n    try :\n        all_player_stats_empty_df.to_csv(\"inputs\/all_player_stats.csv\")\n        print(\"CSV Saved To Input Folder name all_player_stats.csv\")\n    except Exception as e:\n        print(e)\n    return all_player_stats_empty_df\n","03817c0a":"# If you want to get just one player by name you can use this method\n# All player stats df is dataframe which get_all_player_stats_last_x_match return\ndef get_player_stats_by_name(all_player_stats_df,player_name,last_x_match=10):\n    \"\"\"\n\n    :param all_player_stats_df: B\u00fct\u00fcn playerlar\u0131n ma\u00e7lara g\u00f6re verileri\n    :param player_name: Player ismi\n    :param last_x_match: Son ka\u00e7 ma\u00e7 oldu\u011fu\n    :return: Player\u0131n ortalama de\u011ferleri\n    \"\"\"\n\n    player_stats_each = all_player_stats_df[all_player_stats_df.PLAYER_NAME == player_name][-1:-last_x_match:-1]\n    mean_of_stats_this_player = player_stats_each.drop(columns=[\"PLAYER_NAME\"], axis=1).mean()\n    # all_player_stats_empty_df.at[i, \"PLAYER_NAME\"] = player_name\n    # all_player_stats_empty_df.iloc[i, 1:] = mean_of_stats_this_player\n\n    return player_name,mean_of_stats_this_player\n\n","d041e5e7":"# Now we create dataframe which includes every teams matches with each others.\n# df is main dataframe\ndef get_play_by_play_stats(df,last_x_match=10):\n    \"\"\"\n\n    :param df: Data.csv\n    :param last_x_match: Son ka\u00e7 ma\u00e7\n    :return: Tak\u0131mlar\u0131n di\u011fer tak\u0131mlarla olan ma\u00e7 istatistikleri\n    \"\"\"\n\n    \"\"\"preparing dropped columns\"\"\"\n\n    teamAbbr_unique = get_unique_team_abbr(df)\n    opptAbbr_unique = get_unique_team_abbr(df)\n    coll = df.drop(df.columns[42:272],axis=1)\n    coll = df.drop(df.columns[300:530],axis=1)\n    coll = coll.drop(\"gameId\",axis=1)\n    df = coll\n\n    play_by_play = pd.DataFrame(columns=df.columns, index=range(len(teamAbbr_unique) * len(teamAbbr_unique)))\n    i = 0\n    for each in teamAbbr_unique:\n        for k in opptAbbr_unique:\n            if each != k:\n                if df[(df.teamAbbr == each) & (df.opptAbbr == k)].empty == False:\n                    df_for_each = df[(df.teamAbbr == each) & (df.opptAbbr == k)][-1:-last_x_match:-1]\n\n                    play_by_play_count = df_for_each[\"rslt\"].value_counts()\n                    # print(each,k)\n                    # print(df[(df.teamAbbr == each) & (df.opptAbbr == k)])\n                    if play_by_play_count.shape[0] == 1:\n                        play_by_play_count = play_by_play_count[0] \/ (play_by_play_count[0]) * 100\n                    else:\n                        play_by_play_count = play_by_play_count[0] \/ (play_by_play_count[0] + play_by_play_count[1]) * 100\n                    play_by_play.iloc[i, 3:] = df[(df.teamAbbr == each) & (df.opptAbbr == k)].iloc[:, 3:].mean()\n                    play_by_play.loc[i, \"teamAbbr\"] = each\n                    play_by_play.loc[i, \"opptAbbr\"] = k\n                    play_by_play.at[i, \"rslt\"] = play_by_play_count\n\n                    i += 1\n                else:\n                    pass\n    try :\n        play_by_play.to_csv(\"inputs\/play_by_play.csv\")\n        print(\"CSV Saved To Input Folder name play_by_play.csv\")\n    except Exception as e:\n        print(e)\n    return play_by_play\n","a2ff78d3":"# If you want to use any match in your model you can create one row data with this method.\n\n# teamAbbr = home Team\n# opptAbbr = away Team\n# data_csv_name = Main data path\n# Player stats csv name = all_player_stats path\ndef get_test_data(teamAbbr,opptAbbr,homeplayers,awayplayers,data_csv_name = \"..\/input\/matches_w_player_stats.csv\",player_stats_csv_name = \"..\/input\/all_player_stats.csv\",last_x_match=10):\n    df = pd.read_csv(data_csv_name)\n    df.columns = remove_whitespaces_in_df_columns(df)  # clean column names\n    df.columns = get_column_names()  # get column names and assign\n    all_players_stats_df = pd.read_csv(player_stats_csv_name,index_col=0)\n    play_by_play = get_play_by_play_stats(df,last_x_match)\n    test_data = play_by_play[(play_by_play[\"teamAbbr\"] == teamAbbr) & (play_by_play[\"opptAbbr\"] == opptAbbr)]\n    home = []\n    away = []\n    feature_names_of_players = [\"MIN\", \"FGM\", \"FGA\", \"FG_PCT\", \"FG3M\", \"FG3A\", \"FG3_PCT\", \"FTM\", \"FTA\",\n                                \"FT_PCT\", \"OREB\", \"DREB\", \"REB\", \"AST\", \"STL\", \"BLK\", \"TO\", \"PF\", \"PTS\", \"PLUS_MINUS\"]\n    for each in homeplayers:\n        player_name , mean_of_player = get_player_stats_by_name(all_player_stats_df=all_players_stats_df,player_name=each,last_x_match=last_x_match)\n        mean_of_player = mean_of_player.tolist()\n        home.append(mean_of_player)\n    for each in awayplayers:\n        player_name , mean_of_player = get_player_stats_by_name(all_player_stats_df=all_players_stats_df,player_name=each,last_x_match=last_x_match)\n        mean_of_player = mean_of_player.tolist()\n        away.append(mean_of_player)\n    all_df = pd.DataFrame(columns=range(len(feature_names_of_players)*(len(homeplayers)+len(awayplayers))),index=range(1))\n    start_offset = 0\n    end_offset = len(feature_names_of_players)\n    for i in range(len(homeplayers)):\n        all_df.iloc[0,start_offset:end_offset] = home[i]\n        start_offset = end_offset\n        end_offset += 20\n    start_offset = len(homeplayers) * len(feature_names_of_players)\n    end_offset = start_offset + len(feature_names_of_players)\n    for i in range(len(awayplayers)):\n        all_df.iloc[0,start_offset:end_offset] = away[i]\n        start_offset = end_offset\n        end_offset += len(feature_names_of_players)\n    all_df.columns = feature_names_of_players * (len(homeplayers) + len(awayplayers))\n    test_data.reset_index(inplace=True)\n\n    conc =  pd.concat([test_data,all_df],axis=1)\n    conc.to_csv(\"test_data\/\"+teamAbbr+\"vs\"+opptAbbr+\"_test_data.csv\")\n    conc.fillna(value=0,inplace=True)\n    conc.drop(\"index\",axis=1,inplace=True)\n    return conc\n","aac975e7":"# Dropping un neccesary columns for training.\ndef drop_some_columns(df):\n    df = df.drop(columns=[\"PLAYER_NAME\"],axis=1)\n    df = df.drop(columns=[\"gameId\"],axis=1)\n    return df\n","c512c154":"# Label Encoding all data and saving label encoder.\ndef label_train_data(df):\n    le = LabelEncoder()\n    df[\"teamAbbr\"] = le.fit_transform(df[\"teamAbbr\"])\n    keys = le.classes_\n    values = le.transform(le.classes_)\n    dictionary1 = dict(zip(keys, values))\n    np.save('labelencoder\/team_abbr.npy', le.classes_)\n    df[\"opptAbbr\"] = le.fit_transform(df[\"opptAbbr\"])\n    keys = le.classes_\n    values = le.transform(le.classes_)\n    np.save('labelencoder\/oppt_abbr.npy', le.classes_)\n    dictionary2 = dict(zip(keys, values))\n    df[\"rslt\"] = le.fit_transform(df[\"rslt\"])\n    keys = le.classes_\n    values = le.transform(le.classes_)\n    np.save('labelencoder\/rslt.npy', le.classes_)\n    dictionary3 = dict(zip(keys, values))\n    return df,dictionary1,dictionary2,dictionary3\n","3a2c837f":"# Same for test data.Loading and Encoding.\ndef label_test_data(df):\n    encoder = LabelEncoder()\n    encoder.classes_ = np.load('labelencoder\/team_abbr.npy',allow_pickle=True)\n    df.teamAbbr = encoder.transform(df.teamAbbr)\n    encoder = LabelEncoder()\n    encoder.classes_ = np.load('labelencoder\/oppt_abbr.npy',allow_pickle=True)\n    df.opptAbbr = encoder.transform(df.opptAbbr)\n\n    return df\n","4fa41933":"# For cleaning Nan Values use this function.\ndef clean_nan_values(df):\n    df.dropna(inplace=True)\n    return df\n","b1983113":"# Scaling All Data and saving standart scaler.\n\ndef standart_scaler_all_data(df):\n    scaler_filename = \"standartScaler\/scaler.bin\"\n    ss = StandardScaler()\n    df.iloc[:, 3:] = ss.fit_transform(df.iloc[:, 3:])\n    joblib.dump(ss, scaler_filename,compress=True)\n    print(\"Saved Standart Scaler File to \",scaler_filename)\n    return df,ss\n","30db6831":"# Same for test data load and scale.\ndef standart_scaler_test_data(df):\n    scaler_filename = \"standartScaler\/scaler.bin\"\n    scaler = joblib.load(scaler_filename)\n    df.iloc[:, 3:] = scaler.transform(df.iloc[:, 3:])\n    return df\n","b934bfcf":"# One hot encoding required columns and save it.\ndef onehotencoder_all_data(df):\n    ohe = OneHotEncoder(categorical_features=[0, 1], n_values='auto',\n                        handle_unknown='ignore')\n\n    df = ohe.fit_transform(df).toarray()\n    df = pd.DataFrame(df)\n    onehotencoder_name = \"onehotencoder\/onehotencoder.bin\"\n    joblib.dump(ohe, onehotencoder_name, compress=True)\n    print(\"Saved ohe to \",onehotencoder_name)\n    return df\n","80aaa7dc":"#Same for test data load and encode.\ndef onehotencoder_test_data(df):\n    onehotencoder_name = \"onehotencoder\/onehotencoder.bin\"\n    ohe = joblib.load(onehotencoder_name)\n    df = ohe.transform(df).toarray()\n    return pd.DataFrame(df)\n","70c5e495":"#You must use this method because some teams is closed in NBA therefore we don't have to use them for training.\ndef old_to_new_team_abbrs(df):\n    currently_available_teams = {}\n    currently_available_teams[\"NJN\"] = \"BKN\" # old to new\n    currently_available_teams[\"NOH\"] = \"NOP\" # old to new\n\n    non_teams = ['EST', 'FLA', 'GNS', 'GUA', 'MAC']\n    for key, value in currently_available_teams.items():\n        df['teamAbbr'] = df['teamAbbr'].str.replace(key, value)\n        df[\"opptAbbr\"] = df[\"opptAbbr\"].str.replace(key, value)\n    for each in non_teams:\n        drop_index = df[(df.teamAbbr == each) | (df.opptAbbr == each)].index\n        df.drop(drop_index, inplace=True)\n    print(get_unique_team_abbr(df))\n    return df\n","92b7546a":"#This method is doing same job with re-creating all player stats\n#df is main dataframe\n#players is player list \ndef get_one_shot_player_stats(players,df,last_x_match):\n    empty_stats_df = create_empty_player_stats(players)  # create empty stats of teams table\n    all_players_stats_df = get_all_players_stats_df(df)  # assign all team stats to empty table\n    all_players_stats_last_x_match = get_all_player_stats_last_x_match(all_player_stats_df=all_players_stats_df, all_player_stats_empty_df=empty_stats_df,player_list= players,last_x_match=last_x_match)\n","74512f30":"#This one is not include parameter.\n#You can use by giving csv urls.\ndef get_available_player_stats():\n    csv_url=\"..\/input\/matches_w_player_stats.csv\"\n    df = pd.read_csv(csv_url,header=None)\n    df.columns = get_column_names()\n    play_by_play = get_play_by_play_stats(df,last_x_match=1000)\n    players = pd.read_csv(\"..\/input\/player_list.csv\",header=None)\n    players = players.values\n    players = players[:,0]\n    get_one_shot_player_stats(players,df)\n","be3713ce":"#Importing LIBS\n\nfrom preprocessing import *\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom xgboost import XGBClassifier,XGBRegressor,Booster\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.externals import joblib\nimport pickle\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import roc_auc_score,regression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import  GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor,BaggingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor,RadiusNeighborsRegressor\nfrom sklearn.ensemble import AdaBoostRegressor,ExtraTreesRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.svm import SVR,LinearSVR,NuSVR","bda1007f":"#Preprocessing for predicting match result\n\ndef preprocess(csv_url = \"..\/input\/matches_w_player_stats.csv\"):\n    df = pd.read_csv(csv_url)  # read data\n    df.columns = remove_whitespaces_in_df_columns(df)  # clean column names\n    df.columns = get_column_names()  # get column names and assign\n    df = old_to_new_team_abbrs(df)\n    # players = get_unique_player_list(df)  # get all player list\n    # team_abbrs = get_unique_team_abbr(df)  # get all team names\n    df = drop_some_columns(df)\n    df = clean_nan_values(df)\n    Y = np.array(df.rslt)\n    encoded,dict1,dict2,dict3 = label_train_data(df)\n    scalered ,ss= standart_scaler_all_data(encoded)\n    df = onehotencoder_all_data(scalered)\n    return df,Y\n","e88cd58c":"\ndef preprocess_home_team_point(csv_url =\"..\/input\/matches_w_player_stats.csv\"):\n    df = pd.read_csv(csv_url)  # read data\n    df.columns = remove_whitespaces_in_df_columns(df)  # clean column names\n    df.columns = get_column_names()  # get column names and assign\n    df = old_to_new_team_abbrs(df)\n    # players = get_unique_player_list(df)  # get all player list\n    # team_abbrs = get_unique_team_abbr(df)  # get all team names\n    df = drop_some_columns(df)\n    df = clean_nan_values(df)\n    Y = np.array(df.teamPTS)\n    encoded,dict1,dict2,dict3 = label_train_data(df)\n    scalered ,ss= standart_scaler_all_data(encoded)\n    df = onehotencoder_all_data(scalered)\n    return df,Y\ndef preprocess_away_team_point(csv_url =\"..\/input\/matches_w_player_stats.csv\"):\n    df = pd.read_csv(csv_url)  # read data\n    df.columns = remove_whitespaces_in_df_columns(df)  # clean column names\n    df.columns = get_column_names()  # get column names and assign\n    df = old_to_new_team_abbrs(df)\n    # players = get_unique_player_list(df)  # get all player list\n    # team_abbrs = get_unique_team_abbr(df)  # get all team names\n    df = drop_some_columns(df)\n    df = clean_nan_values(df)\n    Y = np.array(df.opptPTS)\n    encoded,dict1,dict2,dict3 = label_train_data(df)\n    scalered ,ss= standart_scaler_all_data(encoded)\n    df = onehotencoder_all_data(scalered)\n    return df,Y","b649f2a9":"\ndef preprocess_home_q1_point(csv_url):\n    df = pd.read_csv(csv_url)  # read data\n    df.columns = remove_whitespaces_in_df_columns(df)  # clean column names\n    df.columns = get_column_names()  # get column names and assign\n    df = old_to_new_team_abbrs(df)\n    # players = get_unique_player_list(df)  # get all player list\n    # team_abbrs = get_unique_team_abbr(df)  # get all team names\n    df = drop_some_columns(df)\n    df = clean_nan_values(df)\n    Y = np.array(df.teamPTS1)\n    encoded,dict1,dict2,dict3 = label_train_data(df)\n    scalered ,ss= standart_scaler_all_data(encoded)\n    df = onehotencoder_all_data(scalered)\n    return df,Y\ndef preprocess_home_q2_point(csv_url):\n    df = pd.read_csv(csv_url)  # read data\n    df.columns = remove_whitespaces_in_df_columns(df)  # clean column names\n    df.columns = get_column_names()  # get column names and assign\n    df = old_to_new_team_abbrs(df)\n    # players = get_unique_player_list(df)  # get all player list\n    # team_abbrs = get_unique_team_abbr(df)  # get all team names\n    df = drop_some_columns(df)\n    df = clean_nan_values(df)\n    Y = np.array(df.teamPTS2)\n    encoded,dict1,dict2,dict3 = label_train_data(df)\n    scalered ,ss= standart_scaler_all_data(encoded)\n    df = onehotencoder_all_data(scalered)\n    return df,Y\ndef preprocess_home_q3_point(csv_url):\n    df = pd.read_csv(csv_url)  # read data\n    df.columns = remove_whitespaces_in_df_columns(df)  # clean column names\n    df.columns = get_column_names()  # get column names and assign\n    df = old_to_new_team_abbrs(df)\n    # players = get_unique_player_list(df)  # get all player list\n    # team_abbrs = get_unique_team_abbr(df)  # get all team names\n    df = drop_some_columns(df)\n    df = clean_nan_values(df)\n    Y = np.array(df.teamPTS3)\n    encoded,dict1,dict2,dict3 = label_train_data(df)\n    scalered ,ss= standart_scaler_all_data(encoded)\n    df = onehotencoder_all_data(scalered)\n    return df,Y\ndef preprocess_home_q4_point(csv_url):\n    df = pd.read_csv(csv_url)  # read data\n    df.columns = remove_whitespaces_in_df_columns(df)  # clean column names\n    df.columns = get_column_names()  # get column names and assign\n    df = old_to_new_team_abbrs(df)\n    # players = get_unique_player_list(df)  # get all player list\n    # team_abbrs = get_unique_team_abbr(df)  # get all team names\n    df = drop_some_columns(df)\n    df = clean_nan_values(df)\n    Y = np.array(df.teamPTS4)\n    encoded,dict1,dict2,dict3 = label_train_data(df)\n    scalered ,ss= standart_scaler_all_data(encoded)\n    df = onehotencoder_all_data(scalered)\n    return df,Y","8f653dec":"\ndef preprocess_away_q1_point(csv_url):\n    df = pd.read_csv(csv_url)  # read data\n    df.columns = remove_whitespaces_in_df_columns(df)  # clean column names\n    df.columns = get_column_names()  # get column names and assign\n    df = old_to_new_team_abbrs(df)\n    # players = get_unique_player_list(df)  # get all player list\n    # team_abbrs = get_unique_team_abbr(df)  # get all team names\n    df = drop_some_columns(df)\n    df = clean_nan_values(df)\n    Y = np.array(df.opptPTS1)\n    encoded,dict1,dict2,dict3 = label_train_data(df)\n    scalered ,ss= standart_scaler_all_data(encoded)\n    df = onehotencoder_all_data(scalered)\n    return df,Y\ndef preprocess_away_q2_point(csv_url):\n    df = pd.read_csv(csv_url)  # read data\n    df.columns = remove_whitespaces_in_df_columns(df)  # clean column names\n    df.columns = get_column_names()  # get column names and assign\n    df = old_to_new_team_abbrs(df)\n    # players = get_unique_player_list(df)  # get all player list\n    # team_abbrs = get_unique_team_abbr(df)  # get all team names\n    df = drop_some_columns(df)\n    df = clean_nan_values(df)\n    Y = np.array(df.opptPTS2)\n    encoded,dict1,dict2,dict3 = label_train_data(df)\n    scalered ,ss= standart_scaler_all_data(encoded)\n    df = onehotencoder_all_data(scalered)\n    return df,Y\ndef preprocess_away_q3_point(csv_url):\n    df = pd.read_csv(csv_url)  # read data\n    df.columns = remove_whitespaces_in_df_columns(df)  # clean column names\n    df.columns = get_column_names()  # get column names and assign\n    df = old_to_new_team_abbrs(df)\n    # players = get_unique_player_list(df)  # get all player list\n    # team_abbrs = get_unique_team_abbr(df)  # get all team names\n    df = drop_some_columns(df)\n    df = clean_nan_values(df)\n    Y = np.array(df.opptPTS3)\n    encoded,dict1,dict2,dict3 = label_train_data(df)\n    scalered ,ss= standart_scaler_all_data(encoded)\n    df = onehotencoder_all_data(scalered)\n    return df,Y\ndef preprocess_away_q4_point(csv_url):\n    df = pd.read_csv(csv_url)  # read data\n    df.columns = remove_whitespaces_in_df_columns(df)  # clean column names\n    df.columns = get_column_names()  # get column names and assign\n    df = old_to_new_team_abbrs(df)\n    # players = get_unique_player_list(df)  # get all player list\n    # team_abbrs = get_unique_team_abbr(df)  # get all team names\n    df = drop_some_columns(df)\n    df = clean_nan_values(df)\n    Y = np.array(df.opptPTS4)\n    encoded,dict1,dict2,dict3 = label_train_data(df)\n    scalered ,ss= standart_scaler_all_data(encoded)\n    df = onehotencoder_all_data(scalered)\n    return df,Y","1c53c9c8":"# For this,We use seperate methods to split data because we have 11 model.\ndef split_data_by_result(df,result_index=60):\n    X = df.drop(columns=result_index)\n    Y = df[result_index]\n    print(Y)\n    return X,Y\ndef split_data_home_point(df, result_index=62):\n    X = df.drop(columns=result_index)\n    Y = df[result_index]\n    return X, Y\ndef split_data_away_point(df, result_index=67):\n    X = df.drop(columns=result_index)\n    Y = df[result_index]\n    return X, Y\ndef split_data_home_q1_point(df, result_index=63):\n    X = df.drop(columns=result_index)\n    Y = df[result_index]\n    return X, Y\ndef split_data_home_q2_point(df, result_index=64):\n    X = df.drop(columns=result_index)\n    Y = df[result_index]\n    return X, Y\ndef split_data_home_q3_point(df, result_index=65):\n    X = df.drop(columns=result_index)\n    Y = df[result_index]\n    return X, Y\ndef split_data_home_q4_point(df, result_index=66):\n    X = df.drop(columns=result_index)\n    Y = df[result_index]\n    return X, Y\ndef split_data_away_q1_point(df, result_index=68):\n    X = df.drop(columns=result_index)\n    Y = df[result_index]\n    return X, Y\ndef split_data_away_q2_point(df, result_index=69):\n    X = df.drop(columns=result_index)\n    Y = df[result_index]\n    return X, Y\ndef split_data_away_q3_point(df, result_index=70):\n    X = df.drop(columns=result_index)\n    Y = df[result_index]\n    return X, Y\ndef split_data_away_q4_point(df, result_index=71):\n    X = df.drop(columns=result_index)\n    Y = df[result_index]\n    return X, Y\n","25db0b08":"#Build classification model and save\ndef build_classifier_model(X_train,Y_train,X_test,Y_test):\n    # model = XGBClassifier(n_estimators=5000,nthread=4,seed=42,reg_lambda=0.95,reg_alpha=0.45,tree_method=\"gpu_hist\",max_depth=3,objective=\"binary:logistic\")\n    model = XGBClassifier(tree_method=\"gpu_hist\",nthread=4,n_estimators=1000)\n\n    model.fit(X_train, Y_train)\n    pickle.dump(model,open(\"model\/xgb_classifier.pkl\",\"wb\"))\n    print(model.score(X_test,Y_test))\n    print(model.classes_)\n\n    return model , model.score(X_test,Y_test)\n","b7e0eaae":"def build_class_model(csv_url=\"..\/input\/matches_w_player_stats.csv\"):\n    df,Y=preprocess(csv_url)\n\n    X,_ = split_data_by_result(df=df)\n    X_train,X_test,Y_train,Y_test = data_split(X,Y)\n    model_ , score__ = build_classifier_model(X_train,Y_train,X_test,Y_test)\n    return model_,X_train,X_test,Y_train,Y_test\n","59cbf05e":"def build_regressor_model(X_train, Y_train, X_test, Y_test, name):\n    # model = XGBRegressor(learning_rate=0.15,gamma=0,reg_lambda=0.01,max_delta_step=0, max_depth=3,n_estimators=10000,\n    #                      min_child_weight=1,nthread=4,tree_method=\"gpu_hist\")\n    model = XGBRegressor(tree_method=\"gpu_hist\",n_estimators=1000,nthread=4)\n    model.fit(X_train, Y_train)\n    pickle.dump(model, open(name, \"wb\"))\n    print(model.score(X_test,Y_test))\n    return model, model.score(X_test, Y_test)\n","2e1de092":"def build_home_point_predict_model(csv_url=\"..\/input\/matches_w_player_stats.csv\"):\n    df,Y=preprocess_home_team_point(csv_url)\n    X,_ = split_data_home_point(df=df)\n    X_train,X_test,Y_train,Y_test = data_split_regresyon(X,Y)\n    model_ , score__ = build_regressor_model(X_train=X_train,Y_train=Y_train,X_test=X_test,Y_test=Y_test,\n                                             name=\"model\/teams\/home_point_model.pkl\")\n    return model_,X_train,X_test,Y_train,Y_test\ndef build_away_point_predict_model(csv_url=\"..\/input\/matches_w_player_stats.csv\"):\n    df,Y=preprocess_away_team_point(csv_url)\n    X,_ = split_data_away_point(df=df)\n    X_train,X_test,Y_train,Y_test = data_split_regresyon(X,Y)\n    model_ , score__ = build_regressor_model(X_train=X_train,Y_train=Y_train,X_test=X_test,Y_test=Y_test,name=\"model\/teams\/away_point_model.pkl\")\n    return model_,X_train,X_test,Y_train,Y_test\n","4f58b007":"def build_home_q1_point_predict_model(csv_url=\"..\/input\/matches_w_player_stats.csv\"):\n    df,Y=preprocess_home_q1_point(csv_url)\n    X,_ = split_data_home_q1_point(df=df)\n    X_train,X_test,Y_train,Y_test = data_split_regresyon(X,Y)\n    model_ , score__ = build_regressor_model(X_train,Y_train,X_test,Y_test,name=\"model\/teams\/home_q1_point_model.pkl\")\n    return model_,X_train,X_test,Y_train,Y_test\ndef build_home_q2_point_predict_model(csv_url=\"..\/input\/fulldata\/full_data.csv\"):\n    df,Y=preprocess_home_q2_point(csv_url)\n    X,_ = split_data_home_q2_point(df=df)\n    X_train,X_test,Y_train,Y_test = data_split_regresyon(X,Y)\n    model_ , score__ = build_regressor_model(X_train,Y_train,X_test,Y_test,name=\"model\/teams\/home_q2_point_model.pkl\")\n    return model_,X_train,X_test,Y_train,Y_test\ndef build_home_q3_point_predict_model(csv_url=\"..\/input\/matches_w_player_stats.csv\"):\n    df,Y=preprocess_home_q3_point(csv_url)\n    X,_ = split_data_home_q3_point(df=df)\n    X_train,X_test,Y_train,Y_test = data_split_regresyon(X,Y)\n    model_ , score__ = build_regressor_model(X_train,Y_train,X_test,Y_test,name=\"model\/teams\/home_q3_point_model.pkl\")\n    return model_,X_train,X_test,Y_train,Y_test\ndef build_home_q4_point_predict_model(csv_url=\"..\/input\/matches_w_player_stats.csv\"):\n    df,Y=preprocess_home_q4_point(csv_url)\n    X,_ = split_data_home_q4_point(df=df)\n    X_train,X_test,Y_train,Y_test = data_split_regresyon(X,Y)\n    model_ , score__ = build_regressor_model(X_train,Y_train,X_test,Y_test,name=\"model\/teams\/home_q4_point_model.pkl\")\n    return model_,X_train,X_test,Y_train,Y_test","53407c3a":"def build_away_q1_point_predict_model(csv_url=\"..\/input\/matches_w_player_stats.csv\"):\n    df,Y=preprocess_away_q1_point(csv_url)\n    X,_ = split_data_away_q1_point(df=df)\n    X_train,X_test,Y_train,Y_test = data_split_regresyon(X,Y)\n    model_ , score__ = build_regressor_model(X_train,Y_train,X_test,Y_test,name=\"model\/teams\/away_q1_point_model.pkl\")\n    return model_,X_train,X_test,Y_train,Y_test\ndef build_away_q2_point_predict_model(csv_url=\"..\/input\/matches_w_player_stats.csv\"):\n    df,Y=preprocess_away_q2_point(csv_url)\n    X,_ = split_data_away_q2_point(df=df)\n    X_train,X_test,Y_train,Y_test = data_split_regresyon(X,Y)\n    model_ , score__ = build_regressor_model(X_train,Y_train,X_test,Y_test,name=\"model\/teams\/away_q2_point_model.pkl\")\n    return model_,X_train,X_test,Y_train,Y_test\ndef build_away_q3_point_predict_model(csv_url=\"..\/input\/matches_w_player_stats.csv\"):\n    df,Y=preprocess_away_q3_point(csv_url)\n    X,_ = split_data_away_q3_point(df=df)\n    X_train,X_test,Y_train,Y_test = data_split_regresyon(X,Y)\n    model_ , score__ = build_regressor_model(X_train,Y_train,X_test,Y_test,name=\"model\/teams\/away_q3_point_model.pkl\")\n    return model_,X_train,X_test,Y_train,Y_test\ndef build_away_q4_point_predict_model(csv_url=\"..\/input\/matches_w_player_stats.csv\"):\n    df,Y=preprocess_away_q4_point(csv_url)\n    X,_ = split_data_away_q4_point(df=df)\n    X_train,X_test,Y_train,Y_test = data_split_regresyon(X,Y)\n    model_ , score__ = build_regressor_model(X_train,Y_train,X_test,Y_test,name=\"model\/teams\/away_q4_point_model.pkl\")\n    return model_,X_train,X_test,Y_train,Y_test","2e633953":"def get_result(X, model):\n    return model.predict_proba(X)\ndef get_point_result(X, model):\n    return model.predict(X)\ndef get_points(X,model):\n    return model.predict(X) ,model.predict_proba(X)","b34b31cb":"def data_split(X,Y,test_size=0.1):\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=42)\n    Y_train = Y_train.reshape(-1, 1)\n    Y_test = Y_test.reshape(-1, 1)\n    print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n    return X_train,X_test,Y_train,Y_test\ndef data_split_regresyon(X,Y,test_size=0.1):\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=42)\n    Y_train = Y_train.reshape(-1, 1)\n    Y_test = Y_test.reshape(-1, 1)\n    print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n    return X_train,X_test,Y_train,Y_test","27562bfe":"def predict_match_result(data):\n    model = pickle.load(open(\"model\/xgb_classifier.pkl\",\"rb\"))\n    encoded= label_test_data(data)\n    scalered = standart_scaler_test_data(encoded)\n    df = onehotencoder_test_data(scalered)\n    print(df.head())\n    X, Y = split_data_by_result(df=df,result_index=60)\n    return get_result(X, model),int(Y[0])\n","14671230":"def predict_home_point(data):\n    model = pickle.load(open(\"model\/teams\/home_point_model.pkl\",\"rb\"))\n    Y = np.array(data.teamPTS)\n    encoded= label_test_data(data)\n    scalered = standart_scaler_test_data(encoded)\n    df = onehotencoder_test_data(scalered)\n    X, _ = split_data_home_point(df=df)\n    return get_point_result(X, model),int(Y[0])\ndef predict_away_point(data):\n    model = pickle.load(open(\"model\/teams\/away_point_model.pkl\",\"rb\"))\n    Y = np.array(data.opptPTS)\n    encoded= label_test_data(data)\n    scalered = standart_scaler_test_data(encoded)\n    df = onehotencoder_test_data(scalered)\n    X, _ = split_data_away_point(df=df)\n    return get_point_result(X, model),int(Y[0])","274f702e":"def predict_home_q1_point(data):\n    model = pickle.load(open(\"model\/teams\/home_q1_point_model.pkl\",\"rb\"))\n    Y = np.array(data.teamPTS1)\n    encoded= label_test_data(data)\n    scalered = standart_scaler_test_data(encoded)\n    df = onehotencoder_test_data(scalered)\n    X, _ = split_data_home_q1_point(df=df)\n    return get_point_result(X, model),int(Y[0])\ndef predict_home_q2_point(data):\n    model = pickle.load(open(\"model\/teams\/home_q2_point_model.pkl\",\"rb\"))\n    Y = np.array(data.teamPTS2)\n    encoded= label_test_data(data)\n    scalered = standart_scaler_test_data(encoded)\n    df = onehotencoder_test_data(scalered)\n    X, _ = split_data_home_q2_point(df=df)\n    return get_point_result(X, model),int(Y[0])\ndef predict_home_q3_point(data):\n    model = pickle.load(open(\"model\/teams\/home_q3_point_model.pkl\",\"rb\"))\n    Y = np.array(data.teamPTS3)\n    encoded= label_test_data(data)\n    scalered = standart_scaler_test_data(encoded)\n    df = onehotencoder_test_data(scalered)\n    X, _ = split_data_home_q3_point(df=df)\n    return get_point_result(X, model),int(Y[0])\ndef predict_home_q4_point(data):\n    model = pickle.load(open(\"model\/teams\/home_q4_point_model.pkl\",\"rb\"))\n    Y = np.array(data.teamPTS4)\n    encoded= label_test_data(data)\n    scalered = standart_scaler_test_data(encoded)\n    df = onehotencoder_test_data(scalered)\n    X, _ = split_data_home_q4_point(df=df)\n    return get_point_result(X, model),int(Y[0])\ndef predict_away_q1_point(data):\n    model = pickle.load(open(\"model\/teams\/away_q1_point_model.pkl\",\"rb\"))\n    Y = np.array(data.opptPTS1)\n    encoded= label_test_data(data)\n    scalered = standart_scaler_test_data(encoded)\n    df = onehotencoder_test_data(scalered)\n    X, _ = split_data_away_q1_point(df=df)\n    return get_point_result(X, model),int(Y[0])\ndef predict_away_q2_point(data):\n    model = pickle.load(open(\"model\/teams\/away_q2_point_model.pkl\",\"rb\"))\n    Y = np.array(data.opptPTS2)\n    encoded= label_test_data(data)\n    scalered = standart_scaler_test_data(encoded)\n    df = onehotencoder_test_data(scalered)\n    X, _ = split_data_away_q2_point(df=df)\n    return get_point_result(X, model),int(Y[0])\ndef predict_away_q3_point(data):\n    model = pickle.load(open(\"model\/teams\/away_q3_point_model.pkl\",\"rb\"))\n    Y = np.array(data.opptPTS3)\n    encoded= label_test_data(data)\n    scalered = standart_scaler_test_data(encoded)\n    df = onehotencoder_test_data(scalered)\n    X, _ = split_data_away_q3_point(df=df)\n    return get_point_result(X, model),int(Y[0])\ndef predict_away_q4_point(data):\n    model = pickle.load(open(\"model\/teams\/away_q4_point_model.pkl\",\"rb\"))\n    Y = np.array(data.opptPTS4)\n    encoded= label_test_data(data)\n    scalered = standart_scaler_test_data(encoded)\n    df = onehotencoder_test_data(scalered)\n    X, _ = split_data_away_q4_point(df=df)\n    return get_point_result(X, model),int(Y[0])","46a742e7":"def build_all_in_one(csv_url=\"..\/input\/matches_w_player_stats.csv\",test_csv=\"test_data\/GSWvsHOU_test_data.csv\"):\n\n\n    model_, X_train, X_test, Y_train, Y_test = build_class_model(csv_url=csv_url)\n\n    model_, X_train, X_test, Y_train, Y_test = build_home_q1_point_predict_model(csv_url=csv_url)\n    model_, X_train, X_test, Y_train, Y_test = build_home_q2_point_predict_model(csv_url=csv_url)\n    model_, X_train, X_test, Y_train, Y_test = build_home_q3_point_predict_model(csv_url=csv_url)\n    model_, X_train, X_test, Y_train, Y_test = build_home_q4_point_predict_model(csv_url=csv_url)\n    model_, X_train, X_test, Y_train, Y_test = build_away_q1_point_predict_model(csv_url=csv_url)\n    model_, X_train, X_test, Y_train, Y_test = build_away_q2_point_predict_model(csv_url=csv_url)\n    model_, X_train, X_test, Y_train, Y_test = build_away_q3_point_predict_model(csv_url=csv_url)\n    model_, X_train, X_test, Y_train, Y_test = build_away_q4_point_predict_model(csv_url=csv_url)\n    model_, X_train, X_test, Y_train, Y_test = build_home_point_predict_model(csv_url=csv_url)\n    model_, X_train, X_test, Y_train, Y_test = build_away_point_predict_model(csv_url=csv_url)\n\n","5078dbb5":" list_ = get_test_data(teamAbbr=\"LAC\",opptAbbr= \"POR\", homeplayers=[\"Caron Butler\", \"Blake Griffin\", \"DeAndre Jordan\", \"Randy Foye\",\n                         \"Chris Paul\", \"Mo Williams\", \"Brian Cook\", \"Ryan Gomes\", \"Trey Thompkins\", \"Chauncey Billups\",\n                         \"Eric Bledsoe\"],awayplayers=[\"Caron Butler\", \"Blake Griffin\", \"DeAndre Jordan\", \"Randy Foye\", \"Chris Paul\", \"Mo Williams\",\n                         \"Brian Cook\", \"Ryan Gomes\", \"Trey Thompkins\", \"Chauncey Billups\", \"Eric Bledsoe\"],\n                          last_x_match=10)","7f9990fb":"os.listdir(\"test_data\")","69bec789":"build_all_in_one(test_csv=\"test_data\/LACvsPOR_test_data.csv\")","e7184bd7":"df = pd.read_csv(\"test_data\/LACvsPOR_test_data.csv\", index_col=0)\ndf.drop(\"index\", axis=1, inplace=True)\ndf.fillna(0, inplace=True)\ndf_2 = df.copy()\ndf_3 = df.copy()\ndf_4 = df.copy()\ndf_5 = df.copy()\ndf_6 = df.copy()\ndf_7 = df.copy()\ndf_8 = df.copy()\ndf_9 = df.copy()\ndf_10 = df.copy()\ndf_11 = df.copy()\nprint(predict_match_result(df_11))\nprint(predict_home_q1_point(df_5))\nprint(predict_home_q2_point(df_6))\nprint(predict_home_q3_point(df_7))\nprint(predict_home_q4_point(df_8))\nprint(predict_away_q1_point(df))\nprint(predict_away_q2_point(df_2))\nprint(predict_away_q3_point(df_3))\nprint(predict_away_q4_point(df_4))\nprint(predict_home_point(df_9)) \nprint(predict_away_point(df_10))","a246b642":"# 12-You can use build_all_in_one to build all models one line code.","e6ab5682":"# 10-Train_Test_Split","d5a56985":"# 13-Test our models","b1004eb6":"This kernel not including EDA because this kernel is too long and our main goal is building prediction model.\nFor features,We just collected required features.","e08abda6":"##### *get_golden_feature_list() : getting muchcorrelated features with total points which greater equal than threshold*","54effc1b":"# 2-Building Model","338da965":"# 9-Getting Prediction with easier way","174ad1ae":"## 8-Building Models based XGBOOST","fc1fe040":"### Now we preprocessed our data.Time to label encoding , one hot encoding , scaling.","2d1964c4":"# 11-Easier Prediction Functions","cfa25a59":"# 1-Preprocessing\n    We do this job by seperating classes or files all code.\n    Then,We use all these methods for build our model.","0e5930b8":"# 6-Preprocessing for predicting away quarter's point\n","f82ed681":"##### *get_worst_feature_list() : getting much uncorrelated features with total points which lower equal than threshold*","895fcb64":"# 5-Preprocessing for predicting home quarter's point\n","ffd64650":"# 4-Preprocessing for predicting home and away point\n","b0a1fb65":"# 3-Calling Preprocess Methods for our data","9b3b6031":"# 7-Splitting data to Data and Label"}}