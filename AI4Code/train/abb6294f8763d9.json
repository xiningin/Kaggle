{"cell_type":{"55503032":"code","e6938975":"code","76537460":"code","bfc91124":"code","c0445da4":"code","b709fcce":"code","4c15c11b":"code","4a09064a":"code","1e6ce055":"code","76d60c48":"code","f2b4c24b":"code","73709687":"markdown","16e2c08d":"markdown","506b5e17":"markdown","f8dd7781":"markdown","905af1f9":"markdown","33217c0b":"markdown","caff8b92":"markdown","3664333c":"markdown"},"source":{"55503032":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e6938975":"import random\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfrom IPython.display import Image, display\nfrom tensorflow.keras.preprocessing.image import load_img\nimport PIL\nfrom PIL import ImageOps\n\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras import layers","76537460":"input_dir = \"..\/input\/pistachio-tree-canopies-for-semantic-segmentation\/Pistachio canopies\/flir_images\"\ntarget_dir = \"..\/input\/pistachio-tree-canopies-for-semantic-segmentation\/Pistachio canopies\/labels\"\nimg_size = (160, 160)\nnum_classes = 3\nbatch_size = 16\n\ninput_img_paths = sorted(\n    [\n        os.path.join(input_dir, fname)\n        for fname in os.listdir(input_dir)\n        if fname.endswith(\".jpg\")\n    ]\n)\ntarget_img_paths = sorted(\n    [\n        os.path.join(target_dir, fname)\n        for fname in os.listdir(target_dir)\n        if fname.endswith(\".png\") and not fname.startswith(\".\")\n    ]\n)\n\nprint(\"Number of samples:\", len(input_img_paths))","bfc91124":"#Display sample of Image Dataset\ni = 7\nfigure, ax = plt.subplots(nrows=1,ncols=2,figsize=(8,8))\nax.ravel()[0].imshow(mpimg.imread(input_img_paths[i]))\nax.ravel()[0].set_title(\"Original image\")\nax.ravel()[0].set_axis_off()\nax.ravel()[1].imshow(mpimg.imread(target_img_paths[i]))\nax.ravel()[1].set_title(\"Mask\")\nax.ravel()[1].set_axis_off()\n#ax.ravel()[2].imshow(PIL.ImageOps.autocontrast(load_img(target_img_paths[i])))\n#ax.ravel()[2].set_title(\"Contrast of mask\")\n#ax.ravel()[2].set_axis_off()\nplt.tight_layout()","c0445da4":"class PistachioDataset(keras.utils.Sequence):\n    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n\n    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.input_img_paths = input_img_paths\n        self.target_img_paths = target_img_paths\n\n    def __len__(self):\n        return len(self.target_img_paths) \/\/ self.batch_size\n\n    def __getitem__(self, idx):\n        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n        i = idx * self.batch_size\n        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n        for j, path in enumerate(batch_input_img_paths):\n            img = load_img(path, target_size=self.img_size)\n            x[j] = img\n        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n        for j, path in enumerate(batch_target_img_paths):\n            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n            y[j] = np.expand_dims(img, 2)\n            # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n            y[j] -= 1\n        return x, y","b709fcce":"#Build the U-Net Model Architecture\ndef get_model(img_size, num_classes):\n    inputs = keras.Input(shape=img_size + (3,))\n\n    ### [First half of the network: downsampling inputs] ###\n\n    # Entry block\n    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n\n    previous_block_activation = x  # Set aside residual\n\n    # Blocks 1, 2, 3 are identical apart from the feature depth.\n    for filters in [64, 128, 256]:\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n\n        # Project residual\n        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n            previous_block_activation\n        )\n        x = layers.add([x, residual])  # Add back residual\n        previous_block_activation = x  # Set aside next residual\n\n    ### [Second half of the network: upsampling inputs] ###\n\n    for filters in [256, 128, 64, 32]:\n        x = layers.Activation(\"relu\")(x)\n        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.Activation(\"relu\")(x)\n        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.UpSampling2D(2)(x)\n\n        # Project residual\n        residual = layers.UpSampling2D(2)(previous_block_activation)\n        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n        x = layers.add([x, residual])  # Add back residual\n        previous_block_activation = x  # Set aside next residual\n\n    # Add a per-pixel classification layer\n    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n\n    # Define the model\n    model = keras.Model(inputs, outputs)\n    return model\n\n\n# Free up RAM in case the model definition cells were run multiple times\nkeras.backend.clear_session()\n\n# Build model\nmodel = get_model(img_size, num_classes)\nmodel.summary()","4c15c11b":"#Split Dataset into a training and a validation set\n\nval_samples = 185 # 85%(de 1225 vide input 3) Training -- 15% (185 arredondado) Validation\nrandom.Random(305).shuffle(input_img_paths)#306 \u00e9 aprox.25% de 1225\nrandom.Random(305).shuffle(target_img_paths)#no original era 1822 de 7390\ntrain_input_img_paths = input_img_paths[:-val_samples]\ntrain_target_img_paths = target_img_paths[:-val_samples]\nval_input_img_paths = input_img_paths[-val_samples:]\nval_target_img_paths = target_img_paths[-val_samples:]\n\n# Instantiate data Sequences for each split\ntrain_gen = PistachioDataset(\n    batch_size, img_size, train_input_img_paths, train_target_img_paths\n)\nval_gen = PistachioDataset(batch_size, img_size, val_input_img_paths, val_target_img_paths)","4a09064a":"#Training\n\n# We use the \"sparse\" version of categorical_crossentropy\n# because our target data is integers.\nmodel.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"pets_segmentation.h5\", save_best_only=True)\n]\n\nepochs = 30\nmodelunet=model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)","1e6ce055":"# summarize history for accuracy\nplt.plot(modelunet.history['accuracy'])\nplt.plot(modelunet.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.grid(True)\nplt.show()\n# summarize history for loss\nplt.plot(modelunet.history['loss'])\nplt.plot(modelunet.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.grid(True)\nplt.show()","76d60c48":"#Display results for validation image\n\ndef display_mask(i):\n    \"\"\"Quick utility to display a model's prediction.\"\"\"\n    mask = np.argmax(val_preds[i], axis=-1)\n    mask = np.expand_dims(mask, axis=-1)\n    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n    return img","f2b4c24b":"# Display image #120\ni = 120\nfigure, ax = plt.subplots(nrows=1,ncols=3,figsize=(8,5))\nax.ravel()[0].imshow(mpimg.imread(val_input_img_paths[i]))\nax.ravel()[0].set_title(\"Original image\")\nax.ravel()[0].set_axis_off()\nax.ravel()[1].imshow(mpimg.imread(val_target_img_paths[i]))\nax.ravel()[1].set_title(\"Mask\")\nax.ravel()[1].set_axis_off()\nax.ravel()[2].imshow(display_mask(i))\nax.ravel()[2].set_title(\"Predicted mask \")\nax.ravel()[2].set_axis_off()\nplt.tight_layout()","73709687":"#Codes by Ammar Alhaj Ali https:\/\/www.kaggle.com\/ammarnassanalhajali\/image-segmentation-with-a-u-net-and-keras","16e2c08d":"#T\u00f4 quase l\u00e1. Ainda n\u00e3o foi desta vez. Continuo sem accuracy e model loss. ","506b5e17":"#Build the U-Net Model Architecture","f8dd7781":"#Unfortunately, the Dataset's Author DELETED the Dataset and ruined all the Pistachios Kaggle Notebook.","905af1f9":"#InvalidArgumentError:  Received a label value of 149 which is outside the valid range of 0, 3.","33217c0b":"#A machine vision based pistachio sorting using transferred mid-level image representation of Convolutional Neural Network\n\nAuthors:Mohammad Farazi; Mohammad Javad Abbas-Zadeh; Hadi Moradi - doi: 10.1109\/IranianMVIP.2017.8342335\n\nM. Farazi, M. J. Abbas-Zadeh and H. Moradi, \"A machine vision based pistachio sorting using transferred mid-level image representation of Convolutional Neural Network,\" 2017 10th Iranian Conference on Machine Vision and Image Processing (MVIP), Isfahan, Iran, 2017, pp. 145-148, doi: 10.1109\/IranianMVIP.2017.8342335.\n\n\n\nConvolutional neural networks have proved to be prominent in various fields of machine vision and image classification. Although it necessitates a large-scale dataset for promising performance, the mid-level representation of these networks can be exploited for specified tasks with smaller annotated image dataset. To this end, by evaluating the generality-specificity of the desired layer as a feature extractor layer, the parameters of Convolutional Neural Networks learned on massive-size dataset like ImageNet can be transferred to a new model. \n\nIn this study, the images of different sort of pistachios including trashes have been acquired to feed into a new model using a support vector classifier. The ultimate goal of the author's machine vision system is to separate the desired open-shell pistachios from other defected pistachios as well as trashes. For image segmentation, they used active contour method to detect objects and form both new images of each object type and their augmented images. Since their dataset is not large-scale compared to ImageNet classes, a feature reduction method is performed after the feature extractor layer of pre-trained Convolutional Neural Network. \n\nThe results show the better performance of the proposed approach in detection of desired-formed pistachio facing unseen test set of images compared to basic approaches.\n\nhttps:\/\/ieeexplore.ieee.org\/document\/8342335","caff8b92":"#The 1st is jpg then is png","3664333c":"#Labels of different class of objects in test image. Yellow(chip woods), Green (desired open-shell pistachios), Red \n\n#(undesired Pistachios, Defected Pistachios, quite open-shell Pistachios, etc..) \n\n\n![](https:\/\/d3i71xaburhd42.cloudfront.net\/2cbb5d7238a9e46fb074f864dbc6598b73d44430\/4-Figure2-1.png)https:\/\/www.semanticscholar.org\/paper\/A-machine-vision-based-pistachio-sorting-using-of-Farazi-abbaszadeh\/2cbb5d7238a9e46fb074f864dbc6598b73d44430\/figure\/1"}}