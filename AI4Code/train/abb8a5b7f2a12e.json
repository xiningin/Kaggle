{"cell_type":{"dce18823":"code","268b3f9c":"code","7ea0c1b3":"code","3f2731d8":"code","df1a0cdb":"code","efdce7b9":"code","f042fcb9":"code","831a920a":"code","544263ba":"code","60e09897":"code","e209d57f":"code","89cebc02":"code","dfe80bff":"code","ef6fbef5":"code","65bc0673":"code","b88a93ce":"code","2ce98c67":"code","78086ee9":"code","cf8cc6fe":"code","5224ca80":"code","03f0f905":"code","6db4e5c8":"code","ea27681c":"code","5a4899f9":"code","0c98da26":"code","e3afeec3":"code","0dd4ff1b":"code","791a0a9e":"code","5cf47780":"code","654bb878":"code","75972215":"code","ae9f636d":"code","7322aba1":"code","0974a797":"code","023314b1":"code","2ab68e11":"code","192b3be9":"code","1417500f":"code","10c062b7":"code","1f7def96":"code","54b61d12":"code","08bf1e13":"code","f742bee1":"code","d205fa44":"code","15ba991b":"code","7dc3b980":"code","10b63fc9":"code","6cc2e4c5":"markdown","9ee12890":"markdown","00651635":"markdown","835a3b29":"markdown","fe6e0a00":"markdown","9b446147":"markdown","208af0f5":"markdown","2ac5acea":"markdown","7768b59a":"markdown","a278a676":"markdown","ace9091a":"markdown","f3a30d9b":"markdown","ef4bb9e4":"markdown","5219df25":"markdown","e6afdb8c":"markdown","53dd0d91":"markdown","02c91891":"markdown","18ddee10":"markdown","1aa0325d":"markdown","89f04398":"markdown","76c24456":"markdown","59018ec9":"markdown","3ceeadd1":"markdown","6cfe61e9":"markdown","26262265":"markdown","ada54bb6":"markdown","3a8cb873":"markdown"},"source":{"dce18823":"#A place for the imports\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn import metrics\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n","268b3f9c":"abalone = pd.read_csv('..\/input\/abalone.csv')\nabalone.columns=['Sex','Length','Diameter','Height','Whole weight', 'Shucked weight','Viscera weight', \n                 'Shell weight','Rings']\nabalone.sample(5)","7ea0c1b3":"abalone.info()","3f2731d8":"abalone.describe()","df1a0cdb":"abalone[abalone.Height == 0]","efdce7b9":"abalone = abalone[abalone.Height > 0]\nabalone.describe()","f042fcb9":"abalone.hist(figsize=(20,10), grid = False, layout=(2,4), bins = 30);","831a920a":"nf = abalone.select_dtypes(include=[np.number]).columns\ncf = abalone.select_dtypes(include=[np.object]).columns","544263ba":"skew_list = stats.skew(abalone[nf])\nskew_list_df = pd.concat([pd.DataFrame(nf,columns=['Features']),pd.DataFrame(skew_list,columns=['Skewness'])],axis = 1)\nskew_list_df.sort_values(by='Skewness', ascending = False)","60e09897":"sns.set()\ncols = ['Length','Diameter','Height','Whole weight', 'Shucked weight','Viscera weight', 'Shell weight','Rings']\nsns.pairplot(abalone[cols], height = 2.5)\nplt.show();","e209d57f":"data = pd.concat([abalone['Rings'], abalone['Height']], axis = 1)\ndata.plot.scatter(x='Height', y='Rings', ylim=(0,30));\n","89cebc02":"abalone = abalone[abalone.Height < 0.4]\ndata = pd.concat([abalone['Rings'], abalone['Height']], axis = 1)\ndata.plot.scatter(x='Height', y='Rings', ylim=(0,30));","dfe80bff":"abalone.hist(column = 'Height', figsize=(20,10), grid=False, layout=(2,4), bins = 30);","ef6fbef5":"corrmat = abalone.corr()\ncols = corrmat.nlargest(8, 'Rings')['Rings'].index\ncm = np.corrcoef(abalone[nf].values.T)\nsns.set(font_scale=1.25)\nplt.figure(figsize=(15,15))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=nf.values, xticklabels=nf.values)\nplt.show();","65bc0673":"data = pd.concat([abalone['Rings'], abalone['Sex']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxenplot(x='Sex', y=\"Rings\", data=abalone)\nfig.axis(ymin=0, ymax=30);","b88a93ce":"abalone = pd.get_dummies(abalone)\nabalone.head()","2ce98c67":"X = abalone.drop(['Rings'], axis = 1)\ny = abalone['Rings']","78086ee9":"X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.3)","cf8cc6fe":"from sklearn.linear_model import LinearRegression \nparamLin = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\nLinearReg = GridSearchCV(LinearRegression(),paramLin, cv = 10)\nLinearReg.fit(X = X_train,y= y_train)\nLinearRegmodel = LinearReg.best_estimator_\nprint(LinearReg.best_score_, LinearReg.best_params_)","5224ca80":"LinearReg.score(X_train,y_train)","03f0f905":"LinearReg.score(X_test,y_test)","6db4e5c8":"predictions = LinearReg.predict(X_test)\nplt.scatter(y_test, predictions)\nplt.xlabel('True Values')\nplt.ylabel('Predictions')","ea27681c":"from sklearn.linear_model import Ridge\nparamsRidge = {'alpha':[0.01, 0.1, 1,10,100], 'solver' : ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}\n\nridgeReg = GridSearchCV(Ridge(),paramsRidge, cv = 10)\nridgeReg.fit(X = X_train,y= y_train)\nRmodel = ridgeReg.best_estimator_\nprint(ridgeReg.best_score_, ridgeReg.best_params_)","5a4899f9":"ridgeReg.score(X_train,y_train)","0c98da26":"ridgeReg.score(X_test,y_test)","e3afeec3":"predictions = ridgeReg.predict(X_test)\nplt.scatter(y_test, predictions)\nplt.xlabel('True Values')\nplt.ylabel('Predictions')","0dd4ff1b":"from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X)","791a0a9e":"kmeans = KMeans(n_clusters=3, random_state=0).fit(X_std)\ny_kmeans = kmeans.predict(X_std)","5cf47780":"plt.scatter(X_std[:, 0], X_std[:, 1], c=y_kmeans, s=50, cmap='viridis');\n\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);","654bb878":"corr_mat = np.corrcoef(X_std.T)","75972215":"eigenvalues, eigenvectors = np.linalg.eig(corr_mat)\nprint('\\nEigenvalues \\n%s' %eigenvalues)","ae9f636d":"#eigenvalue and eigenvector pairs\npairs = [(np.abs(eigenvalues[i]), eigenvectors[:,i]) for i in range(len(eigenvalues))]\npairs.sort(key = lambda x: x[0], reverse = True)","7322aba1":"sorted_eigenval = []\nfor i in pairs:\n    sorted_eigenval.append(i[0])\nprint(sorted_eigenval)","0974a797":"total = sum(eigenvalues)\nvariance_explained = [(i\/total)*100 for i in sorted_eigenval]","023314b1":"variance_explained","2ab68e11":"cum_variance_explained = np.cumsum(variance_explained)\ncum_variance_explained","192b3be9":"\n#Plot variance explained by the principal components\nwith plt.style.context('fivethirtyeight'):\n    plt.figure(figsize=(8, 6))\n    plt.bar(range(10), variance_explained, alpha=0.7, align='center',\n            label='individual explained variance')\n    plt.step(range(10), cum_variance_explained, where='mid',\n             label='cumulative explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout();","1417500f":"projection_mat = np.hstack((pairs[0][1].reshape(10,1),\n                           pairs[1][1].reshape(10,1),\n                           pairs[2][1].reshape(10,1)))","10c062b7":"X_new = X_std.dot(projection_mat)\nX_new.shape","1f7def96":"abalone.head(5)","54b61d12":"bins = [0,8,10,abalone['Rings'].max()]\ngroup_names = ['young','medium','old']\nabalone['Rings'] = pd.cut(abalone['Rings'],bins, labels = group_names)","08bf1e13":"dictionary = {'young':0, 'medium':1, 'old':2}\nabalone['Rings'] = abalone['Rings'].map(dictionary)","f742bee1":"abalone.head(10)","d205fa44":"X = abalone.drop(['Rings'], axis = 1)\ny = abalone['Rings']","15ba991b":"X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.2)","7dc3b980":"from sklearn.neighbors import KNeighborsClassifier\nparamsKn = {'n_neighbors':range(1,30)}\nKneighbours = GridSearchCV(KNeighborsClassifier(),paramsKn, cv=10)\n\nKneighbours.fit(X=X_train,y=y_train)\nKmodel = Kneighbours.best_estimator_\nprint(Kneighbours.best_score_, Kneighbours.best_params_)","10b63fc9":"from sklearn.svm import SVC\nparamsSvm = {'kernel':['linear', 'poly', 'rbf', 'sigmoid'],\n                  'C':[0.1,1,10],'gamma':[0.01,0.1,0.5,1,2]}\n\nSvm = GridSearchCV(SVC(),paramsSvm,cv=5)\n\nSvm.fit(X_train,y_train)\nmodel_svm = Svm.best_estimator_\nprint(Svm.best_score_,Svm.best_params_)","6cc2e4c5":"## Linear Regression Models","9ee12890":"Observations:\n    \n    - Many features are highly correlated\n        - length and diameter show linear correlation\n        - the length and weight features are quadratic correlated\n        - whole weight is linearly correlated with other weight features\n    - Number of Rings is positively corelated with almost all quadratic features\n    - Possible outliers in Height features\n    \nScatter plot analysis also shows that data mostly cover the values for Rings from 3 to little over 20, selecting only this data in the model may be taken under consideration to increase the accuracy.\n\nFirst I will take a closer look at the Height outliers and then I will investigate correlations between the features.","00651635":"Deleted data as suspected was the cause for the skewness of Height feature, now it is closer to a normal distribution.","835a3b29":"There are two records where Height is equal to 0, it is possible that it was hard to measure it or it was simply omitted. Nevertheless, this can be treated as a NULL value and since there are only two records like that it will be simplest to ignore them.","fe6e0a00":"\n\n\nThere are 8 numerical not-null features in the data. Feature Sex will need to be changed to dummy values in data preparation in order to use it in the model.\n\nLet's investigate further the data as there is a possibility that some of the values that are not null are set to 0 instead.\n\n\n\n","9b446147":"### SVM\n\nThe Support Vector Machine is a discriminative classifier formally defined by a separating hyperplane. The goal of the model is to output optimal hyperplane that will categorize the data into categories. There are many hyperplanes dividing data possible so the object is to find one that will maximize the distance from the line to the classes.","208af0f5":"## K-means","2ac5acea":"Now I will set the X and y labels","7768b59a":"As mentioned it seems that there are minimum values in Height that are 0","a278a676":"## Classification\n","ace9091a":"\nFrom the above plot we see that the first three principal components can explain over 98% of the variation of the feature variables. We may project the original features from the 10-dimensional space to a 3-dimensional space.","f3a30d9b":"As there is a high correlation between variables I will perform principal components analysis for dimensionality reduction.","ef4bb9e4":"As this is linear problem I decided to use two models: Linear Regression and Ridge.\n\n### Linear Regression\n\nLinear regression is a statistical model that examines the linear relationship between two  or more variables. Linear relationship means that when one (or more) independent variables increases (or decreases), the dependent variable increases (or decreases) also.\n\n","5219df25":"First I will transofrm Sex feature ","e6afdb8c":"Histograms show that the data may be skewed, so it will be reasonable to measure it. \n\nIt also shows that there are possible outliers in Height and that there might be a strong relationship between the Diameter and Lenght and between Shell weight, Shucked weight Viscera weight and Whole weight.","53dd0d91":"# Categorical Feature\n\n\nFinally, I will analyse the relation of Rings with the Sex feature","02c91891":"### Ridge Regression\n\n\nAs mentioned previously there is high correlation between the features in the data. That is why i chosed to use Ridge Regression. Ridge Regression is a technique used when the data suffers from multicollinearity.By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors. There is one value smaller than 0 which can be a topic of further investigatin. \n","18ddee10":"## Data exploration\n","1aa0325d":"### KNN\n\nThe k-Nearest-Neighbors  method of classification it is essentially classification by finding the most similar data points in the training data, and making an educated guess based on their classifications. This method is used in areas like recommendation systems, semantic searching, and anomaly detection.","89f04398":"    -Distribution between Male and Female is similar\n    -Most of the Rings both for Male and Female are between 8 and 19\n    -Infants have mostly from 5 to 10 Rings\nThe plot also shows that Rings majority lies between 3 to 22, as mentioned previously.","76c24456":"Skewness value points in which direction data is distorted in a statistical distribution, in Gaussian distribution the value for skewness is 0. In abalone data Height has highest skewness value followed by Rings.\n\nHigh skewness in Height feature may be an outcome of outliers. I will investigate it further using scatter plots.","59018ec9":"# Scatter plots","3ceeadd1":"The heat map shows that features are highly correlated and multicollinearity is possible.\n\n\n    -Whole weight is almost linearly correlated with all the features except Rings\n    -Length is linearly correlated with Diameter\n    -From all the features excluding Rings, Height is least correlated with other features\n    -Rings feature has the highest correlation with Shell Weight followed by Height, Length and Diameter\n    \nPossible solutions for a high level of collinearity in data:\n\n    - Use principal component analysis(PCA) to generate new features\n    - Select partial features for modelling","6cfe61e9":"# Correlation matrix","26262265":"Two values seem not to follow the trend, that is why I will treat them as outliers and delete from data.","ada54bb6":"Classification is the process of predicting the class of given data points. Classes are sometimes called as targets\/ labels or categories. Classification predictive modeling is the task of approximating a mapping function (f) from input variables (X) to discrete output variables (y).\n\nFor example, spam detection in email service providers can be identified as a classification problem. This is s binary classification since there are only 2 classes as spam and not spam. A classifier utilizes some training data to understand how given input variables relate to the class. In this case, known spam and non-spam emails have to be used as the training data. When the classifier is trained accurately, it can be used to detect an unknown email.\n    \nFor this task, I will use K-means clustering and Super Vector Machine.\n\n\nFirst I need to divide the Rings for that I will use the target value Age and divide it into young, medium and old.","3a8cb873":"## PCA"}}