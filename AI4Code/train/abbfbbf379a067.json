{"cell_type":{"fba78c4e":"code","05e92e9d":"code","d2a9c013":"markdown"},"source":{"fba78c4e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","05e92e9d":"import pandas as pd\nimport numpy as np\nimport pickle\nimport sys\nimport os\nimport io\nimport re\nfrom sys import path\nimport numpy as np\nimport pickle\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nimport matplotlib.pyplot as plt\nfrom string import punctuation, digits\nfrom IPython.core.display import display, HTML\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import RegexpTokenizer\n\n#Amazon Data\ninput_file = \"..\/input\/sentiment-labelled-sentences-data-set\/sentiment labelled sentences\/sentiment labelled sentences\/amazon_cells_labelled.txt\"\namazon = pd.read_csv(input_file,delimiter='\\t',header=None)\namazon.columns = ['Sentence','Class']\n#Yelp Data\ninput_file = \"..\/input\/sentiment-labelled-sentences-data-set\/sentiment labelled sentences\/sentiment labelled sentences\/yelp_labelled.txt\"\nyelp = pd.read_csv(input_file,delimiter='\\t',header=None)\nyelp.columns = ['Sentence','Class']\n#Imdb Data\ninput_file = \"..\/input\/sentiment-labelled-sentences-data-set\/sentiment labelled sentences\/sentiment labelled sentences\/imdb_labelled.txt\"\nimdb = pd.read_csv(input_file,delimiter='\\t',header=None)\nimdb.columns = ['Sentence','Class']\n#combine all data sets\ndata = pd.DataFrame()\ndata = pd.concat([amazon, yelp, imdb])\ndata['index'] = data.index\n\ndata\n\n#Total Count of Each Category\npd.set_option('display.width', 4000)\npd.set_option('display.max_rows', 1000)\ndistOfDetails = data.groupby(by='Class', as_index=False).agg({'index': pd.Series.nunique}).sort_values(by='index', ascending=False)\ndistOfDetails.columns =['Class', 'COUNT']\nprint(distOfDetails)\n#Distribution of All Categories\nplt.pie(distOfDetails['COUNT'],autopct='%1.0f%%',shadow=True, startangle=360)\nplt.show()\n\n#Text Preprocessing\ncolumns = ['index','Class', 'Sentence']\ndf_ = pd.DataFrame(columns=columns)\n#lower string\ndata['Sentence'] = data['Sentence'].str.lower()\n#remove email adress\ndata['Sentence'] = data['Sentence'].replace('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)\n#remove IP address\ndata['Sentence'] = data['Sentence'].replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', regex=True)\n#remove punctaitions and special chracters\ndata['Sentence'] = data['Sentence'].str.replace('[^\\w\\s]','')\n#remove numbers\ndata['Sentence'] = data['Sentence'].replace('\\d', '', regex=True)\n\n#remove stop words\nfor index, row in data.iterrows():\n    word_tokens = word_tokenize(row['Sentence'])\n    filtered_sentence = [w for w in word_tokens if not w in stopwords.words('english')]\n    df_ = df_.append({\"index\": row['index'], \"Class\":  row['Class'],\"Sentence\": \" \".join(filtered_sentence[0:])}, ignore_index=True)\ndata = df_\n\nX_train, X_test, y_train, y_test = train_test_split(data['Sentence'].values.astype('U'),data['Class'].values.astype('int32'), test_size=0.10, random_state=0)\nclasses  = data['Class'].unique()\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier\n\n#grid search result\nvectorizer = TfidfVectorizer(analyzer='word',ngram_range=(1,2), max_features=50000,max_df=0.5,use_idf=True, norm='l2') \ncounts = vectorizer.fit_transform(X_train)\nvocab = vectorizer.vocabulary_\nclassifier = SGDClassifier(alpha=1e-05,max_iter=50,penalty='elasticnet')\ntargets = y_train\nclassifier = classifier.fit(counts, targets)\nexample_counts = vectorizer.transform(X_test)\npredictions = classifier.predict(example_counts)\n\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import classification_report\n\n#Model Evaluation\nacc = accuracy_score(y_test, predictions, normalize=True)\nhit = precision_score(y_test, predictions, average=None,labels=classes)\ncapture = recall_score(y_test, predictions, average=None,labels=classes)\nprint('Model Accuracy:%.2f'%acc)\nprint(classification_report(y_test, predictions))\n","d2a9c013":"Data Set Name: Sentiment Labelled Sentences Data Set\n\nData Set Source:UCI Machine Learning Libarary\n\nData Set Info: This dataset was created with user reviews collected via 3 different websites (Amazon, Yelp, Imdb). These comments consist of restaurant, film and product reviews. Each record in the data set is labeled with two different emoticons. These are 1: Positive, 0: Negative.\n\nWe will create a sentiment analysis model using the data set we have given above.\n\nWe will build the Machine Learning model with the Python programming language using the sklearn and nltk library.\n\nNow we can go to the writing part of our code.\n"}}