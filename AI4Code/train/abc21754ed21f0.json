{"cell_type":{"635ad1a7":"code","c23fa8ff":"code","990b6a78":"code","0d153a2b":"code","a50ff25d":"code","9300b8dd":"code","7d2cec0a":"code","c01b0748":"code","bebf4fd4":"code","64f67743":"code","d4edbd26":"code","1e79ae96":"code","a5f3c481":"markdown","6b8bc2f3":"markdown","38e46229":"markdown","c645fa85":"markdown","7b61c38c":"markdown","cc3a7283":"markdown","2ef64e0f":"markdown","d4336c88":"markdown","fc159345":"markdown","c888ef79":"markdown","f9150633":"markdown","a6973248":"markdown","fb696319":"markdown","7d96bb0b":"markdown"},"source":{"635ad1a7":"import pandas as pd\npd.options.display.float_format = '{:.2f}'.format\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nplt.style.use('fivethirtyeight')\nimport warnings\nwarnings.filterwarnings('ignore')\nimport folium\nfrom folium import plugins\nfrom sklearn.cluster import DBSCAN\nprint('Bibliotecas preparadas!')","c23fa8ff":"files = {'customers'    : '\/kaggle\/input\/brazilian-ecommerce\/olist_customers_dataset.csv',\n         'geolocation'  : '\/kaggle\/input\/brazilian-ecommerce\/olist_geolocation_dataset.csv',\n         'items'        : '\/kaggle\/input\/brazilian-ecommerce\/olist_order_items_dataset.csv',\n         'payment'      : '\/kaggle\/input\/brazilian-ecommerce\/olist_order_payments_dataset.csv',\n         'orders'       : '\/kaggle\/input\/brazilian-ecommerce\/olist_orders_dataset.csv',\n         'products'     : '\/kaggle\/input\/brazilian-ecommerce\/olist_products_dataset.csv',\n         'sellers'      : '\/kaggle\/input\/brazilian-ecommerce\/olist_sellers_dataset.csv',\n         'review'       : '\/kaggle\/input\/brazilian-ecommerce\/olist_order_reviews_dataset.csv',\n         }\n\ndfs = {}\nfor key, value in files.items():\n    dfs[key] = pd.read_csv(value)","990b6a78":"# Cruzamento gradativo\ncustomers_location = dfs['customers'].merge(dfs['geolocation'], how='inner', left_on='customer_zip_code_prefix', right_on='geolocation_zip_code_prefix').drop_duplicates('customer_id', keep='first')\ncusloc_order = customers_location.merge(dfs['orders'], how='inner', on='customer_id')\ncuslocord_item = cusloc_order.merge(dfs['items'], how='inner', on='order_id')\ncuslocordite_prod = cuslocord_item.merge(dfs['products'], how='inner', on='product_id')\ncuslocordite_rev= cuslocordite_prod.merge(dfs['review'], how='left', on='order_id')\n\n# Selecionando as colunas de interesse\nfinal = cuslocordite_rev[['customer_id', 'customer_unique_id', 'customer_zip_code_prefix',\n       'customer_city', 'customer_state',\n       'geolocation_lat', 'geolocation_lng','order_id', 'order_status',\n       'order_purchase_timestamp', 'order_approved_at',\n       'order_delivered_carrier_date', 'order_delivered_customer_date',\n       'order_estimated_delivery_date', 'order_item_id', 'product_id',\n       'seller_id', 'shipping_limit_date', 'price', 'freight_value',\n       'product_category_name', 'product_photos_qty',\n       'review_id', 'review_score', 'review_comment_title',\n       'review_comment_message', 'review_creation_date',\n       'review_answer_timestamp']]\n\n# Convertendo para datetime\ndatas = ['order_purchase_timestamp'\n        ,'order_purchase_timestamp'\n        ,'order_delivered_carrier_date'\n        ,'order_delivered_customer_date'\n        ,'order_estimated_delivery_date'\n        ,'shipping_limit_date'\n        ,'review_creation_date'\n        ,'review_answer_timestamp' \n        ]\n\nfor data in datas:\n    final[data] = pd.to_datetime(final[data])\n\n# Criando coluna de tempo de entrega e Hora da compra\nfinal['delivery_time'] = (final['order_delivered_customer_date'].dt.date - final['order_purchase_timestamp'].dt.date).dt.days\n\n# Sele\u00e7\u00e3o do per\u00edodo de interesse\nfinal = final[(final['order_purchase_timestamp'].dt.year > 2016) \n              & \n              (final['order_purchase_timestamp'] < pd.to_datetime('20180901'))\n             ]\nfinal = final.reset_index(drop=True)\nfinal.info()","0d153a2b":"meses_compras = pd.DataFrame()\nmeses_compras['mes'] = final['order_purchase_timestamp'].dt.month\nmeses_compras['ano'] = final['order_purchase_timestamp'].dt.year\nmeses_compras['count'] = final['customer_id']\nmeses_compras = meses_compras.groupby(['ano','mes'])['count'].count().reset_index()\nmeses_compras['ano_mes'] = meses_compras['ano'].astype(str) + ', ' + meses_compras['mes'].astype(str)\n\nmeses_compras.plot(x='ano_mes', y='count', figsize=(25,8))#, color='#42A5F5', alpha=0.9, ci=None)\nplt.xlabel('Ano, M\u00eas', size=20)\nplt.ylabel('Qtd. de Pedidos', size=20)\n\nprint(r'Gr\u00e1fico I')\nplt.show()","a50ff25d":"estado_compras = final.groupby('customer_state', as_index=False)['price'].sum().sort_values(by='customer_state')\nestado_compras_med = final.groupby('customer_state', as_index=False)['price'].mean().sort_values(by='customer_state')\nfrete_medio = final.groupby('customer_state', as_index=False)['freight_value'].mean().sort_values(by='customer_state')\n\nprint('Tabela 1')\nestado_compras_med['price'].describe()","9300b8dd":"figure(num=None, figsize=(12, 8), dpi=80)\n\nplt.subplot(2, 1, 1)\nsns.barplot(x=estado_compras['customer_state'], y=estado_compras['price'], color='#42A5F5', alpha=0.9)\nplt.xlabel(None)\nplt.ylabel('Volume de Compras')\n\nplt.subplot(2, 1, 2)\nsns.lineplot(x=frete_medio['customer_state'], y=frete_medio['freight_value'], color='#28B463', alpha=0.9)\n#ylim(top=3)  # adjust the top leaving bottom unchanged\nplt.ylim(0,50)\nplt.xlabel('Estado')\nplt.ylabel('Frete M\u00e9dio')\n\nprint(r'Gr\u00e1fico II')\nplt.show()","7d2cec0a":"# Cria\u00e7\u00e3o da coluna com o valor referente \u00e0s Horas\nfinal['purchase_hour'] = final['order_purchase_timestamp'].dt.hour\n\nfigure(num=None, figsize=(10, 3), dpi=100)\nplt.hist(final['purchase_hour'], bins=24, facecolor='b', alpha=0.6)\nplt.xticks(ticks=np.arange(24))\nplt.xlabel('Hora')\nplt.ylabel('Qtd. de Pedidos')\n\nprint(r'Gr\u00e1fico III')\nplt.show()","c01b0748":"#Sele\u00e7\u00e3o de informa\u00e7\u00f5es agrupadas pelo consumidor\ncus_valor = final.groupby('customer_unique_id', as_index=False)['price'].sum() #price_x\ncus_qtd = final.groupby('customer_unique_id', as_index=False)['price'].count() #price_y\ncus_frete = final.groupby('customer_unique_id', as_index=False)['freight_value'].sum()\ncus_loc = final[['customer_unique_id', 'geolocation_lat', 'geolocation_lng', 'customer_state']].drop_duplicates('customer_unique_id')\ncus_review = final.groupby('customer_unique_id', as_index=False)['review_score'].mean()\n\n#Uni\u00e3o das informa\u00e7\u00f5es em um Dataframe\ncustomer = cus_valor.merge(cus_qtd, on='customer_unique_id')\ncustomer = customer.merge(cus_frete, on='customer_unique_id')\ncustomer = customer.merge(cus_loc, on='customer_unique_id')\ncustomer = customer.merge(cus_review, on='customer_unique_id')\ncustomer = customer.rename(columns={'price_x':'price', 'price_y':'count_items'})\n\nprint('M\u00e9dia do valor de compra: R$ ' + str(round(customer['price'].mean(),2)) + '\\nDesvio Padr\u00e3o: R$ ' + str(round(customer['price'].std(),2)))\ncustomer.sort_values(by='price', ascending=False).head(10)","bebf4fd4":"a = final[['customer_unique_id','review_score', 'review_comment_message']][\n    (final['customer_unique_id'] == '0a0a92112bd4c708ca5fde585afaa872')\n    |(final['customer_unique_id'] == '763c8b1c9c68a0229c42c9fc6f662b93')\n    |(final['customer_unique_id'] == '459bef486812aa25204be022145caa62')]\nprint('Coment\u00e1rios por parte dos consumidores que pontuar\u00e3o a avalia\u00e7\u00e3o com nota 1.00')\nprint('')\nprint('CONSUMIDOR 1: ' + a.groupby('customer_unique_id')['review_comment_message'].min()[0])\nprint('CONSUMIDOR 2: ' + a.groupby('customer_unique_id')['review_comment_message'].min()[1])\nprint('CONSUMIDOR 3: ' + a.groupby('customer_unique_id')['review_comment_message'].min()[2])","64f67743":"# Selecionando dados do Distrito Federal e uma amostra de 1000 consumidores\ncustomer_df = customer[customer['customer_state'] == 'DF']\ncustomer_df = customer_df.sample(1000, random_state=1223)\n\nClus_dataSet = customer_df[['geolocation_lat','geolocation_lng']]\n\ndb = DBSCAN(eps=0.015, min_samples=50).fit(Clus_dataSet)\nlabels = db.labels_\ncustomer_df[\"Clus_Db\"]=labels\n\n# A sample of clusters\nprint('Clusters formados:')\ncustomer_df[\"Clus_Db\"].value_counts()","d4edbd26":"# Visualiza\u00e7\u00e3o gr\u00e1fica\nmap_clusters = folium.Map(location=[-15.89, -47.9], zoom_start=11)\nrainbow = ['#CD5C5C','#7B68EE','#FF8C00','#8B4513','#008B8B','#FF69B4']\nprint(' Cluster -1: Ciano\\n','Cluster  0: Rosa\\n','Cluster  1: Vermelho\\n','Cluster  2: Azul\\n','Cluster  3: Laranja\\n','Cluster  4: Marrom')\n# add markers to the map\nmarkers_colors = []\nfor lat, lon, price, cluster in zip(customer_df['geolocation_lat'], customer_df['geolocation_lng'], customer_df['price'], customer_df['Clus_Db']):\n    label = folium.Popup('R$ ' + str(price) + ' \\(Cluster ' + str(cluster) + '\\)', parse_html=True, max_width=150,min_width=100)\n    folium.CircleMarker(\n        [lat, lon],\n        radius=3,\n        popup=label,\n        color=rainbow[cluster-1],\n        fill=True,\n        fill_color=rainbow[cluster-1],\n        fill_opacity=0.7).add_to(map_clusters)\nmap_clusters ","1e79ae96":"# Agrupamento por Regi\u00e3o encontrada na clusteriza\u00e7\u00e3o\ncustomer_df['Regi\u00e3o'] = customer_df['Clus_Db'].replace({-1:'Outras Sat\u00e9lites'\n                                                       ,0:'\u00c1guas Claras'\n                                                       ,1:'Asa Norte'\n                                                       ,2:'Asa Sul'\n                                                       ,3:'Guar\u00e1'\n                                                       ,4:'Cruzeiro'\n                                                       })\n\nprint('Valor m\u00e9dio de compras:')\ncustomer_df.groupby('Regi\u00e3o')['price'].mean().sort_values(ascending=False)","a5f3c481":"### Cruzamento entre os Datasets e Transforma\u00e7\u00e3o de Dados\nOs cruzamentos foram realizados de forma gradativa, para assegurar que n\u00e3o ocorresse duplica\u00e7\u00e3o de dados.\n\nTamb\u00e9m tomou-se a decis\u00e3o de remover dados do ano de 2016 e do m\u00eas setembro de 2018, o primeiro por conter apenas 375 registros e o segundo por conter apenas um registro.","6b8bc2f3":"# 2. Dados <a name=\"data\"><\/a>\nA base de dados pode ser acessada no Kaggle, em https:\/\/www.kaggle.com\/olistbr\/brazilian-ecommerce#olist_products_dataset.csv, contendo mais de 100 mil regitros de pedidos em 8 conjuntos de dados separados, sobre:\n\n* Consumidores\n* Vendedores\n* Produtos\n* Pedidos\n* Artigos dos pedidos\n* Avalia\u00e7\u00e3o de pedidos\n* Pagamentos\n* Geolocaliza\u00e7\u00e3o\n\nCada conjunto possui informa\u00e7\u00f5es exclusivas sobre um assunto em quest\u00e3o, podendo ser cruzados de diversas maneiras a fim de se obter informa\u00e7\u00f5es sobre o quesito escolhido. No caso deste trabalho, a abordagem ser\u00e1 realizada com foco nos **Consumidores**.","38e46229":"### 3.3 Quantidade de Compras por Hor\u00e1rio\nA fim de verificar as faixas de hor\u00e1rio em que mais ocorrem compras, optou-se por criar uma coluna com apenas a hora, sem os minutos, para que fosse poss\u00edvel visualizar os dados em um histograma, conforme demonstrado no **Gr\u00e1fico III**.\n\n\u00c9 poss\u00edvel constatar que o per\u00edodo de maior movimenta\u00e7\u00e3o de compras \u00e9 compreendido entre 10:00 e 22:00, representando, ent\u00e3o, 50% das tempo de um dia completo.","c645fa85":"# 3. An\u00e1lises Descritivas <a name=\"analises\"><\/a>\n### 3.1 Volume de compras por m\u00eas\nA primeira an\u00e1lise realizada foi para verificar a evolu\u00e7\u00e3o do volume de compras realizadas no decorrer do tempo e se existe alguma sazonalidade.\n\n\u00c9 poss\u00edvel observar pelo **Gr\u00e1fico I** que, embora n\u00e3o seja poss\u00edvel identificar sazonalidade, houve aumento expressivo do volume de compras no m\u00eas de novembro de 2017 (per\u00edodo em que ocorre o evento \"Black Friday\") e posteriormente, ap\u00f3s uma ligeira queda, o volume se manteve constante e em um n\u00edvel bastante elevado em rela\u00e7\u00e3o ao ano anterior. Sinais indicativos de que a Olist, fundada em 2015, est\u00e1 em ritmo de expans\u00e3o acelerado e consistente.","7b61c38c":"<h3><font size=\"3\">Leandro Alencar \u2013 1931133007<\/font><\/h3>","cc3a7283":"<img style=\"float: left;\" src=\"http:\/\/sindser.org.br\/s\/wp-content\/uploads\/2013\/09\/iesb1.jpg\"  width=\"300\" height=\"300\">\n\n## Instituto de Educa\u00e7\u00e3o Superior de Bras\u00edlia\n## P\u00f3s Gradua\u00e7\u00e3o em Ci\u00eancia de Dados\n## Softwares para uso em Ci\u00eancia de Dados I","2ef64e0f":"### Bibliotecas e depend\u00eancias do Python:\n* Pandas \u2013 Manipula\u00e7\u00e3o e an\u00e1lise de dados \n* NumPy \u2013 Manipula\u00e7\u00e3o de dados  \n* Seaborn - Visualiza\u00e7\u00e3o gr\u00e1fica    \n* Matplotlib \u2013 Visualiza\u00e7\u00e3o gr\u00e1fica     \n* Folium \u2013 Visualiza\u00e7\u00e3o em mapas \n* Scikit-learn - Aprendizado de m\u00e1quina","d4336c88":"----\n# Brazilian E-Commerce Public Dataset by Olist\n## Table of contents\n* [Introdu\u00e7\u00e3o](#introducao)\n* [Dados](#data)\n* [An\u00e1lises Descritivas](#analises)\n* [Conclus\u00e3o](#conclusao)\n","fc159345":"### 3.2 Volume de compras e valor do frete por Estado\nA segunda abordagem foi de verificar a movimenta\u00e7\u00e3o de compras por Estado e tamb\u00e9m o valor m\u00e9dio gasto em fretes, para visualmente analisar se existe alguma rela\u00e7\u00e3o entre as vari\u00e1veis.\n\nA respeito do valor m\u00e9dio pago pelos fretes, a varia\u00e7\u00e3o \u00e9 pequena com m\u00e9dia de R\\\\$ 145,18 e desvio padr\u00e3o de R$ 21,17, conforme **Tabela 1**. J\u00e1 em rela\u00e7\u00e3o \u00e0 observa\u00e7\u00e3o conjunta do volume de compras e do valor m\u00e9dio pago por estado, \u00e9 muito interessante observar no **Gr\u00e1fico II** que o pre\u00e7o m\u00e9dio do frete aparenta ser inversamente proporcional ao volume de compras.","c888ef79":"# 4. Conclus\u00e3o <a name=\"conclusao\"><\/a>\nEmbora a quantidade de registros possa ser considerada pequena, tendo em vista que tratam-se de compras na internet durante um per\u00edodo de 2 anos, os dados s\u00e3o muito interessante para realizar an\u00e1lises a fim de identificar comportamentos ou tentar entender o que possa impactar na satisfa\u00e7\u00e3o dos consumidores.\n\nN\u00e3o h\u00e1 ind\u00edcios suficientes para afirmar que existe sazonalidade no consumo, mas que a partir do per\u00edodo mais atual do conjunto de dados o comportamento \u00e9 de consumo elevado e constante. Tamb\u00e9m constatou-se que os estados que mais movimentam este mercado s\u00e3o os que possuem menor custo m\u00e9dio em rela\u00e7\u00e3o ao valor do frete, e que o per\u00edodo de maior movimenta\u00e7\u00e3o corresponde \u00e0 metade de um dia inteiro, ficando compreendido entre 10:00 e 22:00. \n\nAl\u00e9m disso, foi identificado que embora existam consumidores realizaram gastos muito acima da m\u00e9dia, alguns deles n\u00e3o tiveram uma boa experi\u00eancia com o servi\u00e7o de entrega e isso contribuiu para um baix\u00edssimo n\u00edvel satisfa\u00e7\u00e3o. Existe ainda espa\u00e7o dispon\u00edvel para uma an\u00e1lise mais aprofundada para identificar quais fatores possam estar relacionados \u00e0 satisfa\u00e7\u00e3o do consumidor utilizando Processamento de Linguagem Natural (Natural Language Processing, ou NLP, em ingl\u00eas).","f9150633":"### 3.4 Valor e quantidade de items por consumidor\nUma tabela foi criada para reunir dados referentes ao consumidor para, a partir deles, verificar alguma informa\u00e7\u00e3o que fosse interessante. Neste caso, definiu-se que iria an\u00e1lises seriam realizadas com foco no somat\u00f3rio de todas as compras realizadas.\n\nDentro do \"Top 10\" observado, um \u00fanico consumidor realizou compras no valor total de R\\\\$ 13440.00, enquanto a m\u00e9dia foi de R$ 143.00, com apenas 8 produtos comprados e um nota de avalia\u00e7\u00e3o m\u00e9dia de apenas 1.00. Fato que chamou aten\u00e7\u00e3o, ent\u00e3o foram selecionados mais 2 consumidores com nota igual a 1.00 para se verificar os coment\u00e1rios realizados na avalia\u00e7\u00e3o a fim de avaliar semelhan\u00e7as. Nesta pequena sele\u00e7\u00e3o para observa\u00e7\u00e3o, foi poss\u00edvel perceber que os 3 consumidores atribu\u00edram a nota m\u00ednima devido a problemas com a entrega do produto comprado.\n\nSeria muito interessante a utiliza\u00e7\u00e3o de Processamento de Linguagem Natural para uma futura abordagem dentro deste escopo.","a6973248":"### 3.5 Clusteriza\u00e7\u00e3o\nPor fim, foi realizada um an\u00e1lise de cluster utilizando as dist\u00e2ncias entre os consumidores, com base nas informa\u00e7\u00f5es de latitude e longitude. Para essa tarefa, o algoritimo DBSCAN foi escolhido: consiste em um m\u00e9todo que determina um cluster com base em uma quantidade m\u00ednima de pontos dentro de um raio especificado para cada ponto.\n\nFoi selecionada uma amostra aleat\u00f3ria de 1000 consumidores devido \u00e0 limita\u00e7\u00f5es da biblioteca folium em rela\u00e7\u00e3o ao n\u00famero m\u00e1ximo de pontos poss\u00edveis de serem desenhados no gr\u00e1fico.\n\nComo resultado, foi poss\u00edvel identificar, claramente, as regi\u00f5es de \u00c1guas Claras, Guar\u00e1, Cruzeiro, Asa Sul, Asa Norte e as demais cidades sat\u00e9lites como pertencentes a um mesmo cluster. Com isso tamb\u00e9m foi poss\u00edvel encontrar o valor m\u00e9dio de compras realizadas para cada um desses clusters, sendo a maior delas a regi\u00e3o do Guar\u00e1.\n","fb696319":"# 1. Introdu\u00e7\u00e3o <a name=\"introducao\"><\/a>\nNo Brasil, milhares de compras s\u00e3o efetuadas pela Internet todos os anos. Trata-se de um mercado em constante ritmo de crescimento, de modo que seria extremamente relevante tentar entender comportamentos do consumidor inerentes a este contexto, como meses em que ocorrem maior quantidade de compras, hor\u00e1rio em que s\u00e3o realizados mais neg\u00f3cios, tempo m\u00e9dio de entrega, n\u00edvel de satisfa\u00e7\u00e3o, etc.\n\nPara isso, foi selecionado um conjunto de dados disponibilizado pela Olist, a maior loja de departmento dos \"marketplaces\", com informa\u00e7\u00f5es de mais de 100 mil pedidos de 2016 a 2018.\n\nComo se tratam de dados reais, estes foram disponibilizados de forma a manter o anonimato de qualquer indiv\u00edduo, empresa ou parceiro.","7d96bb0b":"### Importa\u00e7\u00e3o dos Datasets\nFoi realizada a importa\u00e7\u00e3o de todos os datasets atrav\u00e9s do Pandas e posteriormente foram cruzados a fim de formar um \u00fanico Dataframe com informa\u00e7\u00f5es de: consumidores, geolocaliza\u00e7\u00e3o, pedidos, artigos, produtos, vendedores e avalia\u00e7\u00f5es."}}