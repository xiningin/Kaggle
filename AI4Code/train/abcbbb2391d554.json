{"cell_type":{"7525b573":"code","0dcd5ab9":"code","dc779703":"code","b22936e4":"code","e7467f31":"code","ce9f43b8":"code","3c935aa0":"code","5bdfdbbc":"code","9c5da714":"code","741cb7a4":"code","9d5e8136":"code","dc74dbb9":"code","ee0321a9":"code","3d3f44f8":"code","51d394fe":"code","dea69817":"code","7fd17eea":"code","0eeb0e39":"code","e719c956":"code","95ece903":"code","93a6dae7":"code","dda9bf5c":"code","e8f1d6cd":"code","af463159":"code","aed74ccc":"code","1cb8a018":"code","27b95c77":"code","25b4b633":"code","a79d749d":"code","ed3d9ead":"code","bd981dd3":"code","b062cae1":"code","82d65926":"code","4f4cf827":"code","d0202df3":"code","6a1462c6":"code","0716dc61":"code","3b61dbb2":"code","64abc36e":"code","c24f3fe5":"code","f0aa7eca":"code","72ce4519":"code","3e85b16a":"code","59deb08e":"code","c6be7484":"code","a1e7e475":"code","cfb6c06d":"code","ad835959":"code","0bcfe490":"code","430bc836":"code","ba34cca4":"markdown","bd4d0eb9":"markdown","e1eaf76b":"markdown","aab6a6f0":"markdown","3a8cda59":"markdown","5a3c201f":"markdown","46637461":"markdown","715045a0":"markdown","20c5d80f":"markdown","93f2fcbb":"markdown","966dfb88":"markdown","3a077452":"markdown","214db798":"markdown","9cc7cf2b":"markdown"},"source":{"7525b573":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0dcd5ab9":"!pip install feature-engine","dc779703":"!pip install pydotplus","b22936e4":"!pip install eli5","e7467f31":"!pip install shap","ce9f43b8":"import os\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pydotplus\n\n%matplotlib inline\nnp.random.seed(0)\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\n# Revise according to your data directory\nPATH = '\/kaggle\/input\/ibm-hr-data\/'\nFILE = 'IBM_HR_Data.csv'","3c935aa0":"df = pd.read_csv(filepath_or_buffer=os.path.join(PATH, FILE))\n\n# We are dropping the following columns since they are not features, just ID nos and so on. Plus, 'StandardHours' contains no variation\ndf.drop(labels=['EmployeeCount', 'EmployeeNumber', 'ApplicationID', 'Over18', 'StandardHours'], axis='columns', inplace=True)\n\n# Overview of dtypes and Missing Values\ndef observe_data(df):\n    '''\n    Presents exploratory data summary in a crisp manner; \n    with dtype, null values, total values and feature summary columns.\n    '''\n    df = df.copy()\n    properties = pd.Series()\n    for i in df.columns.tolist():\n        if pd.api.types.is_object_dtype(df[i]):\n            properties[i] = df[i].unique().tolist()\n        elif pd.api.types.is_numeric_dtype(df[i]):\n            properties[i] = round(df[i].describe(),2).tolist()\n        elif pd.api.types.is_datetime64_any_dtype(df[i]):\n            properties[i] = [df[i].min().strftime(format='%Y-%m-%d'), df[i].max().strftime(format='%Y-%m-%d')]\n        elif pd.api.types.is_categorical_dtype(df[i]):\n            properties[i] = list(df[i].unique())\n    observe = pd.concat([df.dtypes, df.isnull().sum(), df.notnull().sum(), properties], axis=1)\n    observe.columns = ['dtypes', 'Missing_Vals', 'Total_Vals', 'Properties']\n    return observe\n\nobserve_data(df)","5bdfdbbc":"# We will binarize 'NumCompaniesWorked'\ndef binarize(column, bins, num_categories=[1,2,3]):\n    x = pd.cut(x=column.tolist(), bins=bins, include_lowest=True)\n    x.categories = num_categories\n    tmp = pd.concat([column, pd.Series(x)], axis=1)\n    \n    column = x\n    return column","9c5da714":"# Categories are binarized into: 0-2 years: single; 3-5 years: few, 6-9 years: many\nbins = pd.IntervalIndex.from_tuples([(-1, 2), (2, 5), (5, 9)])\n\n# transforming 'NumCompaniesWorked' and a few more variables\ndf['NumCompaniesWorked'] = binarize(column=df['NumCompaniesWorked'], bins=bins).astype('O')\ndf['TrainingTimesLastYear'] = df['TrainingTimesLastYear'].astype('O')\ndf['WorkLifeBalance'] = df['WorkLifeBalance'].map({1:'Low',2:'Medium',3:'High',4:'Very High',5:'Very High'})\ndf['BusinessTravel'] = df['BusinessTravel'].map({'Travel_Rarely':2, 'Travel_Frequently':3, 'Non-Travel':1})","741cb7a4":"# List of all variables that are of 'O' type\ncategorical_features = df.select_dtypes(include=['object','category']).columns.tolist()\ncategorical_features.remove('Attrition')\n\n# A view of categorical features\nprint('\\033[1m' +'categorical features: ', '\\033[0m',categorical_features)\nprint('='*100)\n# List of all variables that are of 'float' or 'int' type\nnumerical_features = df.select_dtypes(include=np.number).columns.tolist()\nprint('\\033[1m' +'numerical features: ', '\\033[0m',numerical_features)\nprint('='*100)\n\n# This lists shall be expanded after categorical encoding with categorical_features \nordinal_features = ['Education', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', 'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel','BusinessTravel', 'NumCompaniesWorked']\nnominal_features = [i for i in categorical_features if i not in ordinal_features]\ncommon = list(set(ordinal_features).intersection(set(numerical_features)))\nactual_numerical = [i for i in numerical_features if i not in common]\nprint('\\033[1m' +'ordinal features: ', '\\033[0m',ordinal_features) #\nprint('='*100)\nprint('\\033[1m' +'nominal features: ', '\\033[0m',nominal_features) #\nprint('='*100)\nprint('\\033[1m' +'actual numerical: ', '\\033[0m',actual_numerical) #","9d5e8136":"# If we recall, all the variables with missing values were of numerical types,\n# Let's view them once again to ensure the actual_numerical dtypes captures all of them\nobserve_data(df[actual_numerical])","dc74dbb9":"# The missing values are found in 'Age', 'DailyRate', 'HourlyRate', 'MonthlyIncome' and 'MonthlyRate'; all are numerical type\ndf.loc[df.isna().any(axis=1),numerical_features]","ee0321a9":"# Scikit-learn libraries\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, StackingClassifier, VotingClassifier\nfrom sklearn.metrics import recall_score, classification_report, confusion_matrix, roc_curve\nfrom sklearn import tree\nfrom sklearn.linear_model import LogisticRegression\n\n# For oversampling through imbalance-learn\nfrom imblearn.pipeline import make_pipeline, Pipeline\nfrom imblearn.combine import SMOTETomek\n\n# For data processing through feature-engine\nfrom feature_engine.variable_transformers import YeoJohnsonTransformer\nfrom feature_engine.missing_data_imputers import MeanMedianImputer\nfrom feature_engine.categorical_encoders import WoERatioCategoricalEncoder\n\n# For visualizing trees\nfrom graphviz import Source\nfrom IPython.display import SVG, Image\n\n# Model Interpretation\nimport eli5\nimport shap","3d3f44f8":"shap.initjs()","51d394fe":"# Data preprocessing pipeline\n## Train-Test split\nX = df.drop(labels='Attrition', axis=1)\ny = df['Attrition'].map({'Voluntary Resignation':1, 'Current employee':0})\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n\n## Imputing numerical missing values from their respective 'mean'\/ 'median'\nimpute = MeanMedianImputer(imputation_method='median', variables=actual_numerical)\n\n## Transforming nominal categorical features based on their probability ratio (ordinal features are already transformed)\ntransform_nominal = WoERatioCategoricalEncoder(encoding_method='ratio', variables=nominal_features)\n\n## Pre-processing pipeline\npreprocessor = make_pipeline(impute, transform_nominal)\n\n## Transforming the variable\nX_train = preprocessor.fit_transform(X_train, y_train)\nX_test = preprocessor.transform(X_test)\n\n## Oversampling using SMOTE and creating hard boundaries using Tomac lines\nsmotetomec = SMOTETomek(random_state=0)\nX_sample, y_sample = smotetomec.fit_resample(X_train, y_train)","dea69817":"## Instantiating Random Forest classifier\nclassifier = RandomForestClassifier(max_depth=5, min_samples_leaf=100, class_weight={1:1.5}, random_state=0)\n\n## Parameters grid\nparameter_grid = {\n    'criterion': ['entropy', 'gini'], \n    'min_samples_leaf': [10, 50, 80], \n    'max_depth':[2,3,4,5]} #\n\n## Instantiating and fitting GridsearchCV to the train set\ngscvrf = GridSearchCV(estimator=classifier, param_grid=parameter_grid, cv=10, iid=False, scoring='f1_weighted', verbose=False)\ngscvrf.fit(X_sample, y_sample)\n\n## Predicting test outcomes\ny_pred = gscvrf.predict(X_test)\n\n## confusion matrix\ncf = pd.DataFrame(confusion_matrix(y_test, y_pred), index=['current_employee', 'resigned'], columns=['current_employee', 'resigned'])[::-1].T[::-1]","7fd17eea":"# Model Evaluation\nprint(pd.Series(gscvrf.best_params_))\nprint('='*40)\nprint('recall score: %.3f' % recall_score(y_test, y_pred))\nprint('='*40)\nprint(classification_report(y_test, y_pred))\nprint('='*40)\nprint(cf)","0eeb0e39":"# ROC Curve\ny_pred_train_prob = gscvrf.predict_proba(X_train)[:,1]\ny_pred_test__prob = gscvrf.predict_proba(X_test)[:,1]\n\nfp_rate_train, tp_rate_train, thresh1 = roc_curve(y_train, y_pred_train_prob)\nfp_rate_test, tp_rate_test, thresh2 = roc_curve(y_test, y_pred_test__prob)\n\nplt.figure(figsize=(8,8))\nplt.plot(fp_rate_train, tp_rate_train, label='train')\nplt.plot(fp_rate_test, tp_rate_test, label='test')\nplt.plot([0, 1], ls=\"--\")\nplt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC Curve', fontweight='semibold')\nplt.legend(loc='center left', bbox_to_anchor=(1.,.5), frameon=False)\nplt.grid()\nplt.show()","e719c956":"# Let's interpret mean weightage of each feature in the model, globally\neli5.show_weights(gscvrf.best_estimator_, feature_names=X_train.columns.tolist())","95ece903":"# We can also view mean weightage of a sample observation, i.e., local interpret\neli5.show_prediction(estimator=gscvrf.best_estimator_, doc=X_test.sample(), feature_names=X_train.columns.tolist(), show_feature_values=True)","93a6dae7":"observations = shap.sample(X_test)\nexplainer_rf = shap.TreeExplainer(gscvrf.best_estimator_)\n\nshap_vals_rf = explainer_rf.shap_values(observations)","dda9bf5c":"shap.summary_plot(shap_values=shap_vals_rf, features=X_test)","e8f1d6cd":"shap.force_plot(base_value=explainer_rf.expected_value[1], shap_values=shap_vals_rf[1], features=observations, feature_names=X_test.columns.tolist())","af463159":"# joblib.dump(value=gscvrf, filename=os.path.join(PATH, 'randomforest.pkl'))","aed74ccc":"## Instantiating Decision Tree as base classifier\nbase_classifier = tree.DecisionTreeClassifier(max_depth=5, min_impurity_decrease=0.001, class_weight={1:1.5})\n\n## Instantiating Adaptive Boosting as meta classifier\nmeta_classifier = AdaBoostClassifier(learning_rate=0.1, random_state=0, base_estimator=base_classifier)\n\n## Parameters grid\nparameter_grid = {\n    'n_estimators': [i for i in range(20,50,10)], \n    'learning_rate': [i for i in np.linspace(start=0.1, stop=0.25, num=5)]}\n\n## Instantiating and fitting GridsearchCV to the train set\ngscvab = GridSearchCV(estimator=meta_classifier, param_grid=parameter_grid, cv=10, iid=False, n_jobs=-1, scoring='f1_weighted', verbose=False)\ngscvab.fit(X_sample, y_sample)\n\n## Predicting test outcomes\ny_pred = gscvab.predict(X_test)\n\n## confusion matrix\ncf = pd.DataFrame(confusion_matrix(y_test, y_pred), index=['current_employee', 'resigned'], columns=['current_employee', 'resigned'])[::-1].T[::-1]","1cb8a018":"# Model Evaluation\nprint(pd.Series(gscvab.best_params_))\nprint('='*40)\nprint('recall score: %.3f' % recall_score(y_test, y_pred))\nprint('='*40)\nprint(classification_report(y_test, y_pred))\nprint('='*40)\nprint(cf)","27b95c77":"# ROC Curve\ny_pred_train_prob = gscvab.predict_proba(X_train)[:,1]\ny_pred_test__prob = gscvab.predict_proba(X_test)[:,1]\n\nfp_rate_train, tp_rate_train, thresh1 = roc_curve(y_train, y_pred_train_prob)\nfp_rate_test, tp_rate_test, thresh2 = roc_curve(y_test, y_pred_test__prob)\n\nplt.figure(figsize=(8,8))\nplt.plot(fp_rate_train, tp_rate_train, label='train')\nplt.plot(fp_rate_test, tp_rate_test, label='test')\nplt.plot([0, 1], ls=\"--\")\nplt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC Curve', fontweight='semibold')\nplt.legend(loc='center left', bbox_to_anchor=(1.,.5), frameon=False)\nplt.grid()\nplt.show()","25b4b633":"# Model Interpretation\npd.Series(gscvab.best_estimator_.feature_importances_, index=X_train.columns.tolist()).sort_values(ascending=False).plot(kind='bar', figsize=(10,6), title='Feature Importance');","a79d749d":"# joblib.dump(value=gscvab, filename=os.path.join(PATH, 'adaboost.pkl'))","ed3d9ead":"## Instantiating Random Forest classifier\nclassifier = GradientBoostingClassifier(n_estimators=30, min_samples_leaf=50, min_impurity_decrease=0.02, random_state=0, max_features='auto', n_iter_no_change=3)\n\n## Parameters grid\nparameter_grid = {\n    'n_estimators': [20, 30, 40],\n    'max_depth': [3,4,5], \n    'learning_rate': [i for i in np.linspace(start=0.1, stop=0.5, num=5)], \n    'loss': ['deviance', 'exponential']}\n\n## Instantiating and fitting GridsearchCV to the train set\ngscvgb = GridSearchCV(estimator=classifier, param_grid=parameter_grid, cv=10, scoring='f1_weighted', verbose=False)\ngscvgb.fit(X_sample, y_sample)\n\n## Predicting test outcomes\ny_pred = gscvgb.predict(X_test)\n\n## confusion matrix\ncf = pd.DataFrame(confusion_matrix(y_test, y_pred), index=['current_employee', 'resigned'], columns=['current_employee', 'resigned'])[::-1].T[::-1]","bd981dd3":"# Model Evaluation\nprint(pd.Series(gscvgb.best_params_))\nprint('='*40)\nprint('recall score: %.3f' % recall_score(y_test, y_pred))\nprint('='*40)\nprint(classification_report(y_test, y_pred))\nprint('='*40)\nprint(cf)","b062cae1":"# ROC Curve\ny_pred_train_prob = gscvgb.predict_proba(X_train)[:,1]\ny_pred_test__prob = gscvgb.predict_proba(X_test)[:,1]\n\nfp_rate_train, tp_rate_train, thresh1 = roc_curve(y_train, y_pred_train_prob)\nfp_rate_test, tp_rate_test, thresh2 = roc_curve(y_test, y_pred_test__prob)\n\nplt.figure(figsize=(8,8))\nplt.plot(fp_rate_train, tp_rate_train, label='train')\nplt.plot(fp_rate_test, tp_rate_test, label='test')\nplt.plot([0, 1], ls=\"--\")\nplt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC Curve', fontweight='semibold')\nplt.legend(loc='center left', bbox_to_anchor=(1.,.5), frameon=False)\nplt.grid()\nplt.show()","82d65926":"# Let's interpret mean weightage of each feature in the model, globally\neli5.show_weights(gscvgb.best_estimator_, feature_names=X_train.columns.tolist())","4f4cf827":"# We can also view mean weightage of a sample observation, i.e., local interpret\neli5.show_prediction(estimator=gscvgb.best_estimator_, doc=X_test.sample(), feature_names=X_train.columns.tolist(), show_feature_values=True)","d0202df3":"observations = shap.sample(X_test)\nexplainer_gb  = shap.TreeExplainer(gscvgb.best_estimator_)\n\nshap_vals_gb = explainer_gb.shap_values(observations)","6a1462c6":"shap.summary_plot(shap_values=shap_vals_rf, features=observations)","0716dc61":"shap.force_plot(base_value=explainer_gb.expected_value, shap_values=shap_vals_gb, features=observations, feature_names=X_test.columns.tolist())","3b61dbb2":"# joblib.dump(value=gscvgb, filename=os.path.join(PATH, 'gradientboost.pkl'))","64abc36e":"## Stacking Classifier Steps\n### Extracting the best estimator from each model into a list\nrandom_forest = gscvrf.best_estimator_\nadaptive_boost= gscvab.best_estimator_\ngradient_boost= gscvgb.best_estimator_\n\nclassifier_list = [('random_forest',random_forest), \n                   ('adaptive_boost',adaptive_boost), \n                   ('gradient_boost',gradient_boost)]\n\n# ### Declaring meta classifier\nm_classifier = LogisticRegression()\n\n# ### Instantiating Stacking Classifier\nstack = StackingClassifier(estimators=classifier_list, final_estimator=m_classifier)\nstack.fit(X_sample, y_sample)\n\n# Predicting test outcomes\ny_pred = stack.predict(X_test)\n\n# ## confusion matrix\ncf = pd.DataFrame(confusion_matrix(y_test, y_pred), index=['current_employee', 'resigned'], columns=['current_employee', 'resigned'])[::-1].T[::-1]","c24f3fe5":"# Model Evaluation\nprint(classification_report(y_test, y_pred))\nprint('='*80)\nprint(cf)","f0aa7eca":"# ROC Curve\ny_pred_train_prob = stack.predict_proba(X_train)[:,1]\ny_pred_test__prob = stack.predict_proba(X_test)[:,1]\n\nfp_rate_train, tp_rate_train, thresh1 = roc_curve(y_train, y_pred_train_prob)\nfp_rate_test, tp_rate_test, thresh2 = roc_curve(y_test, y_pred_test__prob)\n\nplt.figure(figsize=(8,8))\nplt.plot(fp_rate_train, tp_rate_train, label='train')\nplt.plot(fp_rate_test, tp_rate_test, label='test')\nplt.plot([0, 1], ls=\"--\")\nplt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC Curve', fontweight='semibold')\nplt.legend(loc='center left', bbox_to_anchor=(1.,.5), frameon=False)\nplt.grid()\nplt.show()","72ce4519":"# Global Interpretation of the Meta Model of Stacking Classifier\neli5.show_weights(stack.final_estimator_, feature_names=['Random_Forest', 'Adaptive_Boost', 'Gradient_Boost'])","3e85b16a":"# How this perticular observation is scored?\neli5.show_prediction(estimator=stack.final_estimator_, doc=stack.transform(X_test)[0], feature_names=['Random_Forest', 'Adaptive_Boost', 'Gradient_Boost'], show_feature_values=True)","59deb08e":"explainer = shap.LinearExplainer(model=stack.final_estimator_, data=stack.transform(X_test), nsamples=100)\n\nobservations = stack.transform(X_test.sample(1000, random_state=0))\nshap_values = explainer.shap_values(observations)","c6be7484":"shap.force_plot(base_value=explainer.expected_value, shap_values=shap_values, features=observations, feature_names=['Random_Forest', 'Adaptive_Boost', 'Gradient_Boost'])","a1e7e475":"shap.summary_plot(shap_values=shap_values, features=observations, feature_names=['Random_Forest', 'Adaptive_Boost', 'Gradient_Boost'])","cfb6c06d":"# Classifier list: this time we'll also add stacking\nclassifier_list = [('random_forest',random_forest), \n                   ('adaptive_boost',adaptive_boost), \n                   ('gradient_boost',gradient_boost), \n                   ('stacking', stack)]\n\n# Instantiating Stacking Classifier\nvote = VotingClassifier(estimators=classifier_list, voting='soft')\nvote.fit(X_sample, y_sample)\n\n## Predicting test outcomes\ny_pred = vote.predict(X_test)\n\n# ## confusion matrix\ncf = pd.DataFrame(confusion_matrix(y_test, y_pred), index=['current_employee', 'resigned'], columns=['current_employee', 'resigned'])[::-1].T[::-1]","ad835959":"# Model Evaluation\nprint(classification_report(y_test, y_pred))\nprint('='*40)\nprint(cf)","0bcfe490":"# ROC Curve\ny_pred_train_prob = vote.predict_proba(X_train)[:,1]\ny_pred_test__prob = vote.predict_proba(X_test)[:,1]\n\nfp_rate_train, tp_rate_train, thresh1 = roc_curve(y_train, y_pred_train_prob)\nfp_rate_test, tp_rate_test, thresh2 = roc_curve(y_test, y_pred_test__prob)\n\nplt.figure(figsize=(8,8))\nplt.plot(fp_rate_train, tp_rate_train, label='train')\nplt.plot(fp_rate_test, tp_rate_test, label='test')\nplt.plot([0, 1], ls=\"--\")\nplt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC Curve', fontweight='semibold')\nplt.legend(loc='center left', bbox_to_anchor=(1.,.5), frameon=False)\nplt.grid()\nplt.show()","430bc836":"compensation = pd.concat([X_train['DailyRate'], y_train], axis=1)\n\nsns.boxplot(data=compensation, y='DailyRate', x='Attrition');","ba34cca4":"The collective ```force_plot``` the same information as above, but breaks down for each observation. It is also possible to hold a perticular feature constant and view the interaction.","bd4d0eb9":"## Preprocessing pipeline, model fitting and model interpretation","e1eaf76b":"It turns out that Stacking classifier performed even better. What's happening? Looking at the weights, it turns out that adaptive boosting classifier holds higher weight than the gradient boosting classifier. Observing the summary plot above, it is aparent that few observations in gradient boosting hold very high SHAP value (thereby high impact on model performance) while most of observations hold low to medium SHAP values. On the other hand, adaptive boosting classifier has greater proportion of high SHAP values, thereby indicating consistance in model prediction.\n\n### Voting Classifier\n\nFinal verdict! We will now run our final ensemble classifier. This classifier will combine all above four classifier and apply 'soft' voting mechanism.","aab6a6f0":"The collective ```force_plot``` the same information as above, but breaks down for each observation. It is also possible to hold a perticular feature constant and view the interaction.","3a8cda59":"### Adaptive Boosting Classifier","5a3c201f":"## Conclusion\n\nFrom the above models, we observe that features like Age, overtime, compensation level (daily-rate, monthly-rate, hourly-rate, monthly-income), distance from home play major role in prediction, whereas job role, maritial status, gender, employee source etc play little importance in predicting the outcome.\n\nThings to consider: \n* Certain age groups are more likely to quite than others.\n* With increase in overtime, chances of an employee quitting also increases - that's obvious because already employees are clocking standard 80 work hours per week (feature that was dropped), anything beyond that might cause burnout.\n* Although, the distribution of compensation levels *viz*., HourlyRate, DailyRate, MonthlyRate between current employees and resigned is equal (as depicted in the following box plot), the interaction of these variables with other variables might explain the outcome better (Adaptive boosting classifier).","46637461":"Despite most features being of integer data type, most could be classified into ordinal or nominal data type.\n\nThe following is some observation\/ action items:\n* Features ```'NumCompaniesWorked'``` and ```'BusinessTravel'``` in ```categorical_features``` list are or ordinal type whereas rest are of nominal types. We will therefore segregate elements of ```categorical_features``` list into their respective sub-categories.\n* Feature ```'NumCompaniesWorked'```, contianing 10 labels, will be reclassified into three labels. This will facilitate in classification.\n* Finally, we shall classify variables into ```'actual numerical'``` (interval\/ measure scale), ```'nominal_variables'``` and ```'ordinal_variables'```.\n\n|actual numerical features|ordinal features|nominal features|\n|----|----|----|\n|'DistanceFromHome'|'Education'|'TrainingTimesLastYear'|\n|'PercentSalaryHike'|'EnvironmentSatisfaction'|'WorkLifeBalance'|\n|'YearsAtCompany'|'JobInvolvement'|'Employee Source'|\n|'YearsInCurrentRole'|'JobLevel'|'Department'|\n|'YearsSinceLastPromotion'|'JobSatisfaction'|'EducationField'|\n|'YearsWithCurrManager'|'PerformanceRating'|'Gender'|\n|'TotalWorkingYears'|'RelationshipSatisfaction'|'JobRole'|\n|'Age'|'StockOptionLevel|'MaritalStatus'|\n|'DailyRate'|'BusinessTravel'|'OverTime'|\n|'HourlyRate'|'NumCompaniesWorked'||\n|'MonthlyIncome'|||\n|'MonthlyRate'|||","715045a0":"# IBM HR Analytics Employee Attrition Detection\n\n## Business Problem\n\nThis exercise will deal with predicting which employees are likely to resign.\n\nAs one may already know, attrition of highly skilled talent in a high-tech sector could mean serious business implications for firms like IBM. As such, there is a greate business interest in understanding the drivers and predicting the probability of employees quitting.\n\nThe model would facilitate timely HR intervention which could possibly avert an employee from quitting.\n\n## Data\n\nThe data comprises of an employee survey which records following details of the  employee, including whether or not the employee resigned or is a current employee.\n\n|Feature|Description|\n|----|----|\n|Age|Age of employee in nUmerical value|\n|Attrition|Employee quitting (0=No, 1=Yes)|\n|Business Travel|(1=No Travel, 2=Travel Frequently, 3=Tavel Rarely)|\n|Daily Rate|Numerical Value - Salary Level|\n|Department|(1=HR, 2=R&D, 3=Sales)|\n|Distance From Home|Numerical Value - distance from work to home|\n|Education|No. of years in numerical value|\n|Education Field|(1=HR, 2=LIFE SCIENCES, 3=MARKETING, 4=MEDICAL SCIENCES, 5=OTHERS, 6= TEHCNICAL)|\n|Employee Count|Numerical value|\n|Employee Number|Employee ID in numerical value|\n|Environment Satisfaction|satisfaction level with the environment in numerical value|\n|Gender|(1=FEMALE, 2=MALE)|\n|Hourly Rate|Hourly salary in numerical value|\n|Job Involvement|Numerical value|\n|Job Level|Numerical value|\n|Job Role|(1=HC REP, 2=HR, 3=LAB TECHNICIAN, 4=MANAGER, 5= MANAGING DIRECTOR, 6= REASEARCH DIRECTOR, 7= RESEARCH SCIENTIST, 8=SALES EXECUTIEVE, 9= SALES REPRESENTATIVE)|\n|Job Satisfaction|Numerical value|\n|Marital Status|(1=divorced, 2=married, 3=single)|\n|Monthly Income|Numerical value|\n|Monthly Rate|Numerical value|\n|No. of Companies Worked|0-9 numerical value|\n|Overtime|(1=NO, 2=YES)|\n|Salary Hike %|Numerical Value|\n|Over 18|(1=YES, 2=NO)|\n|Performance Rating|Numerical value|\n|Relationship Satisfaction|Numerical value|\n|Standard Hours|Numerical value|\n|Stock Option Level|Numerical value|\n|Total Working Hours|Numerical value|\n|Training Times Last Year|Numerical value|\n|Work-Life Balance|Numerical value|\n|Years At Company|Numerical value|\n|Years Since Last Promotion|Numerical value|\n|Years with Current Manager|Numerical value|\n|Hiring Source|(seek, referral, recruit.net, linkedin, jora, indeed, glassdoor, adzuna, company website)|\n\n## Solution and Methodology\n\nThis problem is similar to predicting cancer, in the sense that cost of False Negative is far greater than cost of False Positive. Therefore the objective of the model would be to **maximize recall score**. In other words, the model should be highly sensitive.\n\nAs we will see later, the variables are not normally distributed. Further, although not verified, I feel it wouldn be prudent not to rule out possibility multicollinearity among the feature set. To circumvent the possibility of violating any of the assumptions of a linear model, it would be wise to go for a non-parametric model, say a tree based algorithm.\n\nTherefore, the plan of action will be:\n1. Fitting Base Classifiers\n    1. RandomForest\n    2. Adaboost\n    3. Gradient Boosting (or XGBoost)\n2. Stacking $\\rightarrow$ with all the above 'base' classifiers\n3. Voting $\\rightarrow$ with the base classifiers plus stacking classifier\n\nFor the all the models, the following pipeline shalll be trained:\n1. Missing value imputation with column median.\n2. Transforming categorical features to their Weight of Evidence: $\\ln{\\frac{\\text{Proportion of Positive Events}}{\\text{Proportion of Negative Events}}}$\n3. Transforming variables to normal distribution using Yeo-Johnson transformation (all the features in the dataset are of numerical dtype by now)\n4. Standardizing the dataset using sklearn ```StandardScaler()```.\n5. Adjusting for class imbalance using imbalance-learn combination algorithm, SMOTE-Tomac. As said before, the intent is to increase the sensitivity, by demarkating a clear decision boundary.\n6. Fitting the base algorithm\n\nThe respective hyperparameters of each algorithm in the above pipeline is in turn tuned using ```GridSearchCV()``` with 10-fold cross validation.\n\n### Model Interpretation\n\n<font size=1.5>*source*:\n1. [Importance of ML Interpretation](https:\/\/towardsdatascience.com\/human-interpretable-machine-learning-part-1-the-need-and-importance-of-model-interpretation-2ed758f5f476)\n2. [Model Interpretation Strategies](https:\/\/towardsdatascience.com\/explainable-artificial-intelligence-part-2-model-interpretation-strategies-75d4afa6b739)\n3. [Hands-On Machine Learning Interpretation](https:\/\/towardsdatascience.com\/explainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608)\n<\/font>\n\nThe models we will be deploying are complex, black-box models in that they are non-parametric and meta-classifiers of many simple tree classifiers (random forest, adaptive boosting or gradient boosting), with which it's not possible to guage theit inner functioning.\n\nTo have confidence on these models, often the decision makers may feel the need to understand what drives the model. Besides, it's no longer a luxury; regulations require models to be explainable. Therefore, recently, model explainability is the new \"frontier\" in ML.\n\n#### Criteria\n\nAlthough model interpretation is still an evolving field, most of the techniques revolve around the following three criteria:\n\n<u>intrinsic\/ post hoc<\/u>? Intrinsic implies the model itself being interpretable, _viz_., parametric model or a single decision tree; whereas post-hoc involves trying to interpret a pre-trained model.\n\n<u>Model specific\/ model-agnostic<\/u>? Certain model interpretation techniques apply to specific models _viz_., p-values and AIC scores pertaining to regression models. Whereas model-agnostic tools are relavant to performing post-hoc methods and can be applied to any machine learning model.\n\nBroadly speaking scope of interpretability can be either local or global. <u>Local<\/u> implies (Why did model make a specific decision?) being able to explain the conditional interaction between response variable and predictor variables w.r.t. single example. Whereas, <u>global<\/u> interpretation (How does model makes interpretations?) tries to explain the model based on complete dataset.\n\nIn our analysis, we will implement post-hoc, model-agnostic techniques having both local as well as global scope. \n\n#### Techniques\n##### Feature Importance\nDegree to which a predictive model relies on a perticular feature. Typically, it's the increase in model's prediction error after we permuted the feature's values.\n\nSince we will be using SHAP values (discussed below), computing feature importance separately gets redundunt. \n\n##### Partial Dependence Plots\n*Not implemented*\n\n##### Global Surrogate Models\n*Not implemented*\n\n##### Local-Interpretable Model-Agnostic Explainations (LIME)\n*Not implemented*\n\n##### Shapley Additive Explanations\n<font size=1.5>**Source**: [SHAP](https:\/\/christophm.github.io\/interpretable-ml-book\/shap.html)<\/font>\n\nSHAP (SHapley Additive exPlanations) by Lundberg and Lee (2016) is a method to explain individual predictions. Based on the game theoretically optimal Shapley Values, the goal of SHAP is to explain the prediction of an instance $x$ by computing contribution of each feature to the prediction.\n\nThe original Shapley values (from Game Theory) tell how fairly distribute the payout (prediction) among the players (features). In ML context, the Shapely values in SHAP is represented as an additive feature attribution method, a linear model.\n\n\\begin{align}\ng(z') &= \\phi_0+\\sum^M_{j=1}\\phi_j z'_j \\\\\n\\text{where}\\\\\ng &: \\text{explanation model} \\\\\nz' & \\in\\{0,1\\}^M\\;\\;\\text{is a coalition vector} \\\\\n\\phi_j & \\in \\mathbb{R};\\;\\text{is a feature attribution of feature }j \\\\\n\\end{align}","20c5d80f":"### Random Forest Classifier","93f2fcbb":"## Cleaning and processing raw data\n\nThe entire statstical modelling process begins with loading and cleaning the dataset to desired form.\n\nCertain variables _viz_., EmployeeCount, EmployeeNumber, ApplicationID are not features. Further variables of Over18 and StandardHours are 99.99% constant; features without variability are of no use for model. We shall remove all such variables.\n\nI also have defined a utility function to view each variable, its data type and its sample in tabular form...","966dfb88":"### Gradient Boosting Classifier","3a077452":"### Preprocessing Pipeline","214db798":"### Stacking Classifier\n\nIntuitively, the performance of random forest classifier is good, that of adaptive boosting is better and that of gradient boosting is the best. Does it imply we should simply go for gradient boosting algorithm? Let's find out...","9cc7cf2b":"The collective ```force_plot``` the same information as above, but breaks down for each observation. It is also possible to hold a perticular feature constant and view the interaction."}}