{"cell_type":{"8eeef392":"code","b5846df6":"code","0b0e5132":"code","07007745":"code","4dce92d4":"code","ec7ab36c":"code","e80d0d4e":"code","093ce7fe":"code","f5593166":"code","eee179e6":"code","8251c5ad":"code","1894355c":"code","60b652e9":"code","0eb2b787":"code","c88dd4f5":"code","722f62eb":"code","1e1f8721":"code","a04056f6":"markdown","e153849a":"markdown"},"source":{"8eeef392":"# importing libraries.\nimport os\nimport shutil\nimport datetime\nimport cv2 as cv\nimport numpy as np\nimport pandas as pd\nimport plotly.io as pio\nimport plotly.graph_objs as go\nimport tensorflow_addons as tfa\n\nfrom plotly import subplots\nfrom tqdm.notebook import tqdm\nfrom texttable import Texttable\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nfrom tensorflow.keras import optimizers\nfrom keras.preprocessing.image import ImageDataGenerator","b5846df6":"# setting renderer as colab.\npio.renderers.default = \"kaggle\"","0b0e5132":"# creating the directory structure.\nroot = \".\/Dataset\"\nif os.path.exists(root):\n    shutil.rmtree(root)\nos.mkdir(root)\nsub_folders = [\"Train\", \"Validation\", \"Test\"]\nfor sub_folder in sub_folders:\n    os.mkdir(os.path.join(root, sub_folder))","07007745":"# loading image metadata.\ndataset = pd.read_csv(\"..\/input\/fashion-product-images-dataset\/fashion-dataset\/styles.csv\", on_bad_lines = \"skip\")\ndataset.head()","4dce92d4":"# understanding class distribution.\ndataset[\"gender\"].value_counts()","ec7ab36c":"# dropping classes with less data and extracting only required features.\ndataset = dataset.loc[:, [\"id\", \"gender\"]]\ndataset.drop(dataset[dataset[\"gender\"] == \"Boys\"].index, inplace = True)\ndataset.drop(dataset[dataset[\"gender\"] == \"Girls\"].index, inplace = True)\ndataset.drop(dataset[dataset[\"gender\"] == \"Unisex\"].index, inplace = True)\ndataset.head()","e80d0d4e":"# visualizing new data.\ntrace0 = go.Pie(\n    labels = dataset[\"gender\"].unique(),\n    values = dataset[\"gender\"].value_counts(),\n    textinfo = \"label+percent\",\n    name = \"Class Category\",\n    hole = 0.6,\n    marker = dict(\n        colors = [\n            \"darkred\",\n            \"floralwhite\"\n        ]\n    )\n)\ngender = dataset[\"gender\"].value_counts()\ntrace1 = go.Bar(\n    x = gender.values,\n    y = gender.keys(),\n    orientation = 'h',\n    name = \"Class Counts\",\n    texttemplate = [\n            str(value) \\\n            for value in gender.values        \n    ],\n    textposition = \"outside\",\n    textfont_color = \"white\",\n    marker = dict(\n        color = gender.values,\n        colorscale = \"OrRd\"\n    )\n)\nspecs = [\n         [{\"type\": \"domain\"}],\n         [{\"type\": \"bar\"}]\n]\nfig_data = subplots.make_subplots(\n    specs = specs,\n    rows = 2, cols = 1,\n    row_heights = [0.8, 0.2], \n    vertical_spacing = 0.1\n)\nfig_data.add_trace(trace0, row = 1, col = 1)\nfig_data.add_trace(trace1, row = 2, col = 1)\nfig_data.update_layout(\n    title = \"Class Visualization\", \n    template = \"plotly_dark\",\n    showlegend = False\n)\nfig_data.show()","093ce7fe":"# making class folders.\ncategories = dataset[\"gender\"].unique()\nfor sub_folder in sub_folders:\n    location = os.path.join(root, sub_folder)\n    for category in categories:\n        os.mkdir(os.path.join(location, category))","f5593166":"# separating images into train, validation and test.\nbase = \"..\/input\/fashion-product-images-dataset\/fashion-dataset\/images\"\nimage_IDs = [int(image_name.split(\".\")[0]) for image_name in os.listdir(base)]\ntotal_images = len(dataset[\"id\"])\nfor n, ID in enumerate(tqdm(dataset[\"id\"], desc = \"Segregating images\")):\n    if ID in image_IDs:\n        image_name = str(ID) + \".jpg\"\n        source = os.path.join(base, image_name)\n        if n < 0.6 * total_images:\n            category = dataset.loc[dataset[\"id\"] == ID, \"gender\"]\n            destination = os.path.join(\".\/Dataset\/Train\", str(category.iloc[0]) + \"\/\" + image_name)\n        elif n < 0.8 * total_images:\n            category = dataset.loc[dataset[\"id\"] == ID, \"gender\"]\n            destination = os.path.join(\".\/Dataset\/Validation\", str(category.iloc[0]) + \"\/\" + image_name)\n        else:\n            category = dataset.loc[dataset[\"id\"] == ID, \"gender\"]\n            destination = os.path.join(\".\/Dataset\/Test\", str(category.iloc[0]) + \"\/\" + image_name)\n        shutil.copyfile(source, destination)","eee179e6":"# sanity check.\ntable = Texttable()\nrows = []\nrows.append([\"Category\"] + sub_folders)\nfor category in categories:\n    count = []\n    for sub_folder in sub_folders:\n        location = os.path.join(root, sub_folder)\n        count.append(len(os.listdir(os.path.join(location, category))))\n    rows.append([category] + count)\ntable.add_rows(rows)\nprint(table.draw())","8251c5ad":"# initializing image dimension.\nimage_width = 256\nimage_height = 256\nchannels = 3","1894355c":"# initializing a small covenet. (baseline)\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation = \"relu\", input_shape = (image_width, image_height, channels)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation = \"relu\"))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation = \"relu\"))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation = \"relu\"))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(512, activation = \"relu\"))\nmodel.add(layers.Dense(1, activation = \"sigmoid\"))","60b652e9":"# diplaying model summary.\nmodel.summary()","0eb2b787":"# compiling the mmodel\nmodel.compile(loss = \"binary_crossentropy\", optimizer = optimizers.RMSprop(learning_rate = 1e-4), metrics = [\"acc\"])","c88dd4f5":"# normalizing data\ntrain_dir = \".\/Dataset\/Train\"\nvalidation_dir = \".\/Dataset\/Validation\"\ntest_dir = \".\/Dataset\/Test\"\nbatch_size = 20\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255)\ntest_datagen = ImageDataGenerator(rescale = 1.\/255)\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size = (image_width, image_height),\n    batch_size = batch_size,\n    class_mode = \"binary\"\n)\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_dir,\n    target_size = (image_width, image_height),\n    batch_size = batch_size,\n    class_mode = \"binary\"\n)","722f62eb":"# training model.\ntqdm_callback = tfa.callbacks.TQDMProgressBar()\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch = count[0] \/\/ batch_size,\n    epochs = 30,\n    validation_data = validation_generator,\n    validation_steps = count[1] \/\/ batch_size,\n    callbacks = [tqdm_callback],\n    verbose = 0\n)","1e1f8721":"# visualizing results.\naccuracy = history.history[\"acc\"]\nval_accuracy = history.history[\"val_acc\"]\nloss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\nepochs = np.arange(1, len(accuracy) + 1)\ntrace0 = go.Scatter(x = epochs, y = accuracy, mode = \"markers\", marker = dict(color = \"floralwhite\"), name = \"Training Accuracy\", legendgroup = \"1\")\ntrace1 = go.Scatter(x = epochs, y = val_accuracy, marker = dict(color = \"orange\"), name = \"Validation Accuracy\", legendgroup = \"1\")\ntrace2 = go.Scatter(x = epochs, y = loss, mode = \"markers\", marker = dict(color = \"ghostwhite\"), name = \"Traning Loss\", legendgroup = \"2\")\ntrace3 = go.Scatter(x = epochs, y = val_loss, marker = dict(color = \"darkred\"), name = \"Validation Loss\", legendgroup = \"2\")\ntitles = [\"Training and Validation Accuracy\", \"Training and validation Loss\"]\nfig_results_1 = subplots.make_subplots(rows = 2, cols = 1, vertical_spacing = 0.1, subplot_titles = titles)\nfig_results_1.add_trace(trace0, row = 1, col = 1)\nfig_results_1.add_trace(trace1, row = 1, col = 1)\nfig_results_1.add_trace(trace2, row = 2, col = 1)\nfig_results_1.add_trace(trace3, row = 2, col = 1)\nfig_results_1.update_layout(title = \"Training Results\", template = \"plotly_dark\", height = 1500)\nfig_results_1.show()","a04056f6":"# Conlusion:\n    Clear signs of overfitting.\n    We can do better in part 2.","e153849a":"# Aim:\n    To train a CNN that can recognise women and men Fashion Wear."}}