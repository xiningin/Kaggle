{"cell_type":{"96929b9d":"code","530f4128":"code","000d5730":"code","80bb8aaf":"code","672c08e2":"code","e7fbdf4e":"code","85074088":"code","9965f341":"code","63b12eb8":"code","3ca8cc81":"code","63d65d1d":"code","2f3e50f3":"code","2863efa3":"code","e0500654":"code","59eb62cf":"code","6c9d9bd6":"code","53679118":"code","844a1374":"code","bcd86dad":"code","7084510d":"code","88dd8901":"code","39729f5b":"code","839e28a5":"code","4b1ebefb":"code","beb745a4":"code","c494bded":"code","351594d2":"code","79c842fc":"code","209d4639":"code","c1fbe940":"code","c8491c66":"code","9ae1a0c2":"code","c9b7c7cb":"code","e84f3ac2":"code","e4e780e8":"code","7f835728":"code","c529ea85":"code","97ac5bd4":"code","74d6b31a":"code","0c95c987":"code","a38527b1":"code","b8f313ec":"markdown","c537a862":"markdown","d61cf0c5":"markdown","01d0a306":"markdown"},"source":{"96929b9d":"import numpy as np \nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport numpy\nimport pandas\nimport seaborn as sns\nfrom keras.models import Sequential\nfrom keras.layers import Dense ,Dropout,BatchNormalization\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline","530f4128":"import os\nprint(os.defpath)\nos.chdir(\"\/kaggle\/input\/\")\nprint(os.getcwd())\nprint(os.listdir(\"..\/input\"))","000d5730":"df=pd.read_csv(\"Admission_Predict.csv\")\n\n#changing names because previous names are little bit confusing\ndf=df.rename(index=str, columns={\"GRE Score\": \"GRE\", \"TOEFL Score\": \"TOEFL\", \"Chance of Admit \": \"Admission_Chance\"})\ndf.head(10)","80bb8aaf":"df=df.drop(\"Serial No.\",axis=1)\n# Dropped Sr no. column\ndf.shape","672c08e2":"df.describe()","e7fbdf4e":"print(df.isna().sum(),\"\\n\",df.isnull().sum())","85074088":"admit=np.asarray(df[\"Admission_Chance\"])\nlen(np.unique(admit))\n# different values in the coloum [ unique chance to predict]","9965f341":"import pandas as pd\nimport matplotlib.pyplot as plt\ncorr = df.corr()\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(corr,cmap='magma', vmin=-1, vmax=1)\nfig.colorbar(cax)\nticks = np.arange(0,len(df.columns),1)\nax.set_xticks(ticks)\nplt.xticks(rotation=90)\nax.set_yticks(ticks)\nax.set_xticklabels(df.columns)\nax.set_yticklabels(df.columns)\nplt.show()","63b12eb8":"import seaborn as sns\ncorr = df.corr()\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)","3ca8cc81":"fig = plt.figure(figsize = (20, 25))\nj = 0\nfor i in df.columns:\n    plt.subplot(6, 4, j+1)\n    j += 1\n    sns.distplot(df[i][df['Admission_Chance']<0.72], color='r', label = 'Not Got Admission')\n    sns.distplot(df[i][df['Admission_Chance']>0.72], color='g', label = 'Got Admission')\n    plt.legend(loc='best')\nfig.suptitle('Admission Chance In University ')\nfig.tight_layout()\nfig.subplots_adjust(top=0.95)\nplt.show()","63d65d1d":"for column in df:\n    plt.figure()\n    sns.boxplot(x=df[column])","2f3e50f3":"for column_1st in df:\n    for coloum_2nd in df:\n        jet=plt.get_cmap('jet')\n        plt.figure(figsize=(15,5))\n        plt.scatter(df[column_1st], df[coloum_2nd], s=30, c=df['Admission_Chance'], vmin=0, vmax=1, cmap=jet)\n        plt.xlabel(column_1st,fontsize=40)\n        plt.ylabel(coloum_2nd,fontsize=40)\n        plt.colorbar()\n        plt.show()","2863efa3":"X=np.asarray(df.drop(\"Admission_Chance\",axis=1))\nY=np.asarray(df[\"Admission_Chance\"])\ny = Y","e0500654":"X_train, X_test, y_train, y_test = train_test_split(\n     X,Y, test_size=0.2, random_state=0)\nfrom sklearn.preprocessing import MinMaxScaler\nscaler =  MinMaxScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.fit_transform(X_test)","59eb62cf":"from sklearn.linear_model import LinearRegression\nregr = LinearRegression()\nlr_model = regr.fit(X_train, y_train)\nlr_pred = lr_model.predict(X_test)\nlr_pred","6c9d9bd6":"# define keras model\ndef baseline_model():\n    # create model\n    model = Sequential()\n    \n    \n    \n    model.add(Dense(16, input_dim=7, activation='relu'))\n    \n    \n    model.add(Dense(8, input_dim=7, activation='relu'))\n    \n    \n    model.add(Dense(1))\n    # Compile model\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model","53679118":"estimator = KerasRegressor(build_fn=baseline_model, epochs=20, batch_size=3, verbose=1)","844a1374":"history=estimator.fit(X_train,y_train)","bcd86dad":"# Plot training & validation loss values\ndef plotloss(args):\n  plt.plot(args)\n  plt.title('Model loss')\n  plt.ylabel('Loss')\n  plt.xlabel('Epoch')\n  plt.legend(['Train', 'Test'], loc='upper left')\n  plt.show()\nhist = history.history['loss']\nplotloss(hist)","7084510d":"from sklearn.metrics import accuracy_score\nprediction = estimator.predict(X_test)\nprint(prediction)","88dd8901":"# FN FOR CALCULATING ERRORS OF PREDICTION MODELS\ndef errors(y_test, prediction):\n  train_error =  np.abs(y_test - prediction)\n  mean_error = np.mean(train_error)\n  min_error = np.min(train_error)\n  max_error = np.max(train_error)\n  std_error = np.std(train_error)\n  print(\"std_error: \",std_error)\n  print(\"mean_error: \",mean_error)\n  print(\"min_error: \",min_error)\n  print(\"max_error: \",max_error)\n","39729f5b":"print(\"==== Error for keras model ===\")\nerrors(y_test, prediction)\nprint(\"==== Error for sklearn LR model ===\")\nerrors(y_test, lr_pred)  ","839e28a5":"#Visualising the Acutal and predicted Result\nplt.plot(y_test, color = 'green', label = 'Actual')\nplt.plot(prediction, color = 'blue', label = 'Predicted')\nplt.grid(alpha = 0.3)\nplt.xlabel('Number of Candidate')\nplt.ylabel('Score')\nplt.title('Actual vs Predicted')\nplt.legend()\nplt.show()","4b1ebefb":"from sklearn.metrics import r2_score\ndef getkerasscore():\n\n  a = r2_score(y_test,prediction)\n  \n  \n  train_prediction = estimator.predict(X_train)\n  \n  b = r2_score(y_train,train_prediction)\n\n  print(\"r_square score: \", a)\n  \n  print(\"r_square score (train dataset): \", b)\n  return(a,b)\ngetkerasscore()\n","beb745a4":"train_prediction = estimator.predict(X_train)\n\nprint(\"r_square score: \", r2_score(y_test,prediction))\nkeras_pred = train_prediction\nprint(\"r_square score (train dataset): \", r2_score(y_train,train_prediction))\n\na = r2_score(y_test,model_prediction)\n","c494bded":"# Save the weights\n\nprint(os.getcwd())\nos.chdir(\"\/kaggle\/working\/\")\nprint(os.getcwd())\n\n\ndef saveweights():\n  estimator.model.save_weights('model_weights.h5')\n  #estimator.model.save_weights(\"\")\n  # Saving the model architecture\n  with open('model_architecture.json', 'w') as f:\n        \n        f.write(estimator.model.to_json())\n        print(\"Model Wieghts saved successfully\")\nsaveweights()","351594d2":"def getscore(model_prediction, model_predictor):\n  from sklearn.metrics import r2_score\n  a = r2_score(y_test,model_prediction)\n  train_prediction = model_predictor(X_train)\n  b = r2_score(y_train,train_prediction)\n\n  print(\"r_square score: \", a)\n  \n  print(\"r_square score (train dataset): \", b)\n  return(a,b)\n  \ngetscore(lr_pred, lr_model.predict)","79c842fc":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import r2_score","209d4639":"# step-1: create a cross-validation scheme\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 100)\n\n# step-2: specify range of hyperparameters to tune\nhyper_params = [{'n_features_to_select': list(range(1, 9))}]\n\n\n# step-3: perform grid search\n# 3.1 specify model\nrfe = RFE(lr_model)             \n\n# 3.2 call GridSearchCV()\nmodel_cv = GridSearchCV(estimator = rfe, param_grid = hyper_params, scoring= 'r2',cv = folds, verbose = 1,return_train_score=True)      \n\n# fit the model\nmodel_cv.fit(X_train, y_train)                  ","c1fbe940":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","c8491c66":"plt.figure(figsize=(16,6))\n\nplt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_test_score\"])\nplt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_train_score\"])\nplt.xlabel('number of features')\nplt.ylabel('r-squared')\nplt.title(\"Optimal Number of Features\")\nplt.legend(['test score', 'train score'], loc='upper left')","9ae1a0c2":"# final sklearn LR model\nn_features_optimal = 8\n\nregr = LinearRegression()\nlr_model = regr.fit(X_train, y_train)\nlr_pred = lr_model.predict(X_test)\nlr_pred\n\nrfe = RFE(lr_model, n_features_to_select=n_features_optimal)             \nrfe = rfe.fit(X_train, y_train)\n\n#r2 = sklearn.metrics.r2_score(y_test, y_pred)\nr2 = r2_score(y_test, lr_pred)\nprint(r2)","c9b7c7cb":"r2_k = r2_score(y_test,prediction)\nr2_k","e84f3ac2":"#scores = cross_val_score(lr_model, X_train, y_train, scoring='r2', cv=10)\ndef scoresinfo(modelname, parts):\n  s = cross_val_score(modelname, X_train, y_train, scoring='r2', cv=parts)\n  print(\"Scores for {} parts CV:\" .format(parts), s)\n  print(\"Minimum Socore :\",s.min())\n  print(\"Max. score :\", s.max())\n  print(\"Mean score:\" , s.mean())\n \nscoresinfo(lr_model, 10)","e4e780e8":"#Visualising the Acutal and predicted Result\nplt.plot(y_test, color = 'green', label = 'Actual')\nplt.plot(lr_pred, color = 'yellow', label = 'Predicted')\nplt.grid(alpha = 0.3)\nplt.xlabel('Number of Candidate')\nplt.ylabel('Score')\nplt.title('After HPT Actual vs Predicted')\nplt.legend()\nplt.show()","7f835728":"print(\"=== R2 value for keras model ===\")\nr2k = list(getkerasscore())\n\nprint(\"=== R2 value for keras model ===\")\nr2sk = list(getscore(lr_pred, lr_model.predict))","c529ea85":"#r2_score(y_test,prediction)\nr2keras = r2_score(y_train,train_prediction)\nr2sklearn = r2_score(y_test, lr_pred)\nscoresdf = pd.DataFrame(columns= [\"Score\", \"Keras Model\", \"Skl Model\"])\nscoresdf[\"Score\"] = [\"r_square score\", \"r_square score (train dataset)\"]\nscoresdf[\"Keras Model\"] = r2k\nscoresdf[\"Skl Model\"] = r2sk\nscoresdf","97ac5bd4":"#Visualising the Acutal and predicted Result\nfig = plt.figure(figsize= [20,8])\nplt.plot(y_test, color = 'green', label = 'Actual')\nplt.plot(lr_pred, color = 'red', label = 'Predicted by SKl model')\nplt.plot(prediction, color = 'blue', label = 'Predicted by Keras Model')\n\nplt.grid(alpha = 0.3)\nplt.xlabel('Number of Candidate')\nplt.ylabel('Score')\nplt.title('Comparison of Actual vs Predicted by both models')\nplt.legend()\nplt.show()","74d6b31a":"df.head()","0c95c987":"my_submission = pd.DataFrame({'Actual_values': y_test, ' Keras_Prediction': prediction, \"LR_Model_Prediction\": lr_pred })\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","a38527b1":"subm = pd.read_csv(\"submission.csv\")\nsubm","b8f313ec":"# Prediction of chances of prediction using Nueral Network reggrssor and Linear Regression:","c537a862":"### **there are no outliers**","d61cf0c5":"**For Checking our Model that it is not General we are using Kfolds**","01d0a306":"# # hyper parameter tuning for linear regression model:"}}