{"cell_type":{"e4c0f1d9":"code","5cd3f194":"code","835d01e8":"code","66d701af":"code","3d412d13":"code","65d1a7d1":"code","0ed62289":"code","3ecaffcf":"code","39259935":"code","32bd79bb":"code","8dda9a63":"code","bc6171fa":"code","2db0e55e":"code","865c7fb0":"code","1a499b51":"code","840be2a8":"code","d294e0ec":"markdown","2236552a":"markdown","21474ec3":"markdown","b254017a":"markdown","847a171c":"markdown","c3efa0e0":"markdown","4cfea472":"markdown","4c75a2e2":"markdown","aadf8f6f":"markdown","60e3a5f1":"markdown","bfba1828":"markdown"},"source":{"e4c0f1d9":"import pandas as pd\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string \nimport nltk","5cd3f194":"data = pd.read_csv('..\/input\/large-random-tweets-from-pakistan\/Random Tweets from Pakistan- Cleaned- Anonymous.csv', encoding_errors='ignore')\ndata.head(10)","835d01e8":"data.shape","66d701af":"data.drop(['Unnamed: 0', 'created_at_tweet',\n       'retweet_count', 'favorite_count', 'reply_count',\n       'location'], axis = 1,inplace=True)","3d412d13":"data","65d1a7d1":"data.drop_duplicates(subset =\"full_text\",\n                     keep = False, inplace = True)","0ed62289":"data","3ecaffcf":"def cleantxt(text):\n    text = re.sub(r'@[A-Za-z0-9]+', '',text)\n    text = re.sub(r'#', '',text)\n    text = re.sub(r'RT[\\s]+', '',text)\n    text = re.sub(r'https?:\\\/\\\/\\S+', '',text)\n    \n    return text\n\ndata['full_text'] = data['full_text'].apply(cleantxt)\n\ndata.head()","39259935":"punctuation = '\"!\"#$%&''()*+,-.\/:;<=>?@[\\]^_`{|}~\"'\n\nbag_of_word = \"Hello!!! , he said -----and went.\"\n\nno_punc = \"\"\nfor char in bag_of_word:\n    if(char not in punctuation):\n        no_punc = no_punc + char","32bd79bb":"no_punc","8dda9a63":"import nltk\nnltk.download('punkt')\nnltk.download('wordnet')","bc6171fa":"nltk.word_tokenize(\"hi how are you\")","2db0e55e":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nprint(stop_words)","865c7fb0":"from nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\ninput_str = \"There are several types of stemming algorithms.\"\ninput_str=nltk.word_tokenize(input_str)\nfor word in input_str:\n    print(stemmer.stem(word))","1a499b51":"from nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\ninput_str = \"There are many languages in cities.\"\ninput_str=nltk.word_tokenize(input_str)\nfor word in input_str:\n    print(lemmatizer.lemmatize(word))","840be2a8":"lemmatizer = WordNetLemmatizer()\nfor index,row in data.iterrows():\n    filter_sentences = []\n    sentence = row['full_text']\n    sentence = re.sub(r'[^\\w\\s]', '',sentence)\n    words = nltk.word_tokenize(sentence)\n    words = [w for w in words if not w in stop_words]\n    for word in words:\n        filter_sentences.append(lemmatizer.lemmatize(word))\n    print(filter_sentences)","d294e0ec":"# Lemmatization","2236552a":"# Cleaning Text","21474ec3":"# Tokenization","b254017a":"# Stemming","847a171c":"# Remove Duplicate Records","c3efa0e0":"# Apply on Dataset","4cfea472":"# Drop Cloumns","4c75a2e2":"# Punctuation","aadf8f6f":"# Import Data","60e3a5f1":"# Import Libraries","bfba1828":"# Stopwords"}}