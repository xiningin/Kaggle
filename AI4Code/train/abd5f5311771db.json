{"cell_type":{"1882d54c":"code","3fceaad2":"code","f4356a0b":"code","2012f45d":"code","439a5ce1":"code","b0bddbb0":"code","96838a7f":"code","7a8fbe5c":"code","2e7976fc":"code","ed0626db":"code","89ad32d4":"code","b28050ac":"code","c16f0eb5":"code","9e731d77":"code","b30fce12":"code","5e7d4f82":"code","433be451":"code","ce7ec574":"code","432d27ca":"code","2377c431":"code","515ba846":"code","ed6f5cf1":"code","5bda4841":"code","b9d70a00":"code","cfdf114e":"code","9147c885":"code","ab868230":"code","d68349e8":"code","8ea1dd8a":"code","bbe45391":"code","3d1319a5":"code","ac9fd8eb":"code","bcca9a41":"code","a215952e":"code","984101f2":"code","4bb440d3":"code","b498c3a9":"code","90aa5e44":"code","4c86a078":"code","b4b2a855":"code","2ce68daa":"code","abfba692":"code","262281a1":"code","a78415b7":"code","fa79500f":"code","4fbe73eb":"code","3a76d056":"code","fa713590":"code","6dbe76bf":"code","1507d007":"code","591473f4":"code","2ae310da":"code","6df8ffbe":"code","51d3a8af":"code","f58bd660":"code","953bf0f9":"code","694ba37a":"code","2de5f415":"code","f539de6f":"code","eaf8a71b":"code","49e35e3e":"code","bccbcb13":"code","66feb8eb":"code","ae343ffb":"code","836a669b":"code","91f595ef":"markdown","40b33633":"markdown","a9cf6811":"markdown","26f90378":"markdown","204d6c42":"markdown","5b5934ed":"markdown","f6a49960":"markdown","76ca5012":"markdown","7367540c":"markdown","ea9205ee":"markdown","2f24658b":"markdown","dd322a9b":"markdown","eff8bd7c":"markdown","9247bfdd":"markdown","f837800a":"markdown","82c213a7":"markdown","83ba09dd":"markdown","5db48bae":"markdown","f2a389d1":"markdown","b8dff17d":"markdown","85868635":"markdown","d5e5252e":"markdown","97d83931":"markdown","58f92932":"markdown","dc7f9c14":"markdown","28fd373f":"markdown","27be8080":"markdown","e2715376":"markdown","e26e0082":"markdown","1ff9fc0d":"markdown","933f2b3a":"markdown","d95d8c41":"markdown","506a1a4d":"markdown","b13831e3":"markdown","196a1714":"markdown","8c963c18":"markdown","d3a354c0":"markdown","f94d3a6e":"markdown","3932c5dc":"markdown","bd457295":"markdown","72cf93ec":"markdown","f1e32fe0":"markdown","03c28f34":"markdown","460b7fde":"markdown","9171cedb":"markdown","454f3e17":"markdown","1777d40d":"markdown","003d838c":"markdown","0bb353f0":"markdown","4fe87125":"markdown","c43e99f8":"markdown","8ce48f96":"markdown","dbc66b92":"markdown","c582f1b4":"markdown","38b7128b":"markdown","566e8941":"markdown","906c34fc":"markdown"},"source":{"1882d54c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom IPython.display import display\npd.options.display.max_columns = None\npd.options.display.max_rows = None\npd.options.mode.chained_assignment = None\n\ndataset = pd.read_csv('..\/input\/telecom_churn_data.csv') # use this for kaggle\n#dataset = pd.read_csv('telecom_churn_data.csv')           # use this for running locally\n","3fceaad2":"print('Number of Rows: ' + str(len(dataset)))\nprint('Number of columns: ' + str(len(dataset.columns)))\ndataset.describe()","f4356a0b":"dataset = dataset.drop(['mobile_number','circle_id','loc_og_t2o_mou','std_og_t2o_mou','loc_ic_t2o_mou','std_og_t2c_mou_6',\n                       'std_og_t2c_mou_7','std_og_t2c_mou_8','std_og_t2c_mou_9'], axis=1)\n\nprint('Number of columns after removing above columns: ' + str(len(dataset.columns)))","2012f45d":"print(dataset.dtypes.sort_values())","439a5ce1":"date_column_names = ['last_date_of_month_6','last_date_of_month_7','last_date_of_month_8','last_date_of_month_9',\n        'date_of_last_rech_data_6','date_of_last_rech_data_7','date_of_last_rech_data_8','date_of_last_rech_data_9',\n        'date_of_last_rech_6','date_of_last_rech_7','date_of_last_rech_8','date_of_last_rech_9']\ndataset[date_column_names].describe()","b0bddbb0":"dataset['last_date_of_month_7']=dataset['last_date_of_month_7'].fillna((dataset['last_date_of_month_7'].mode()[0]))\ndataset['last_date_of_month_8']=dataset['last_date_of_month_8'].fillna((dataset['last_date_of_month_8'].mode()[0]))\ndataset['last_date_of_month_9']=dataset['last_date_of_month_9'].fillna((dataset['last_date_of_month_9'].mode()[0]))\n\n#Change data type \nfor c_name in date_column_names:\n    dataset[c_name]= pd.to_datetime(dataset[c_name],infer_datetime_format=True)","96838a7f":"#Method to extract new features\ndef add_change_kpis_from_6_7_to_8(df, feature_names):    \n    for f_name in feature_names:\n        df[f_name+'_6'] = df[f_name+'_6'].fillna(0) # Imputing missing values with 0\n        df[f_name+'_7'] = df[f_name+'_7'].fillna(0) # Imputing missing values with 0\n        df[f_name+'_8'] = df[f_name+'_8'].fillna(0)# Imputing missing values with 0\n        df['change_in_'+f_name] = (df[f_name+'_6'] + df[f_name+'_7'])\/2 - df[f_name+'_8']\n    return df    ","7a8fbe5c":"df_processed = add_change_kpis_from_6_7_to_8(dataset,['arpu','onnet_mou','offnet_mou','total_og_mou','total_ic_mou','total_rech_amt',\n                                                  'spl_ic_mou','spl_og_mou','total_rech_num','total_rech_data','count_rech_2g',\n                                                            'count_rech_3g','arpu_3g','arpu_2g','monthly_2g','sachet_2g','monthly_3g',\n                                                            'sachet_3g','av_rech_amt_data', 'roam_ic_mou','roam_og_mou','loc_og_t2t_mou',\n                                                           'loc_og_t2m_mou','loc_og_t2f_mou','loc_og_t2c_mou','loc_og_mou','std_og_t2t_mou',\n                                                           'std_og_t2m_mou','std_og_t2f_mou','std_og_mou','isd_og_mou','og_others',\n                                                           'loc_ic_t2t_mou','loc_ic_t2m_mou','loc_ic_t2f_mou','loc_ic_mou','std_ic_t2t_mou',\n                                                           'std_ic_t2m_mou','std_ic_t2f_mou','std_ic_t2o_mou','std_ic_mou','isd_ic_mou',\n                                                           'ic_others','night_pck_user','max_rech_amt','last_day_rch_amt','max_rech_data',\n                                                           'av_rech_amt_data'])\n\n#Handling special scenario where column names are not in standard format\ndf_processed['jul_vbc_3g'] = df_processed['jul_vbc_3g'].fillna(0)\ndf_processed['aug_vbc_3g'] = df_processed['aug_vbc_3g'].fillna(0)\ndf_processed['jun_vbc_3g'] = df_processed['jun_vbc_3g'].fillna(0)\ndf_processed['change_in_vbc_3g'] = (df_processed['jul_vbc_3g'] + df_processed['jun_vbc_3g']) \/2 - df_processed['aug_vbc_3g']\n","2e7976fc":"dataset['total_data_rech_6']= dataset['total_rech_data_6'] * dataset['av_rech_amt_data_6']\ndataset['total_data_rech_7']= dataset['total_rech_data_7'] * dataset['av_rech_amt_data_7']\ndataset['total_data_rech_8']= dataset['total_rech_data_8'] * dataset['av_rech_amt_data_8']\ndataset['total_data_rech_6'] = dataset['total_data_rech_6'].fillna(0) # Imputing missing values with 0\ndataset['total_data_rech_7'] = dataset['total_data_rech_7'].fillna(0) # Imputing missing values with 0\ndataset['total_data_rech_8'] = dataset['total_data_rech_8'].fillna(0) # Imputing missing values with 0\n\ndataset['amt_data_6'] = dataset['total_rech_amt_6'] + dataset['total_data_rech_6']\ndataset['amt_data_7'] = dataset['total_rech_amt_7'] + dataset['total_data_rech_7']\ndataset['amt_data_8'] = dataset['total_rech_amt_8'] + dataset['total_data_rech_8']","ed0626db":"def get_difference_in_days(date_co1, date_col2):\n    return (pd.to_datetime(date_co1)- pd.to_datetime(date_col2))\/np.timedelta64(1,'D')\n\n#Derived Features after operating on dates - average duration change in last recharge of month\njune_last_recharge_duration = get_difference_in_days(df_processed['last_date_of_month_6'], df_processed['date_of_last_rech_6'])\njuly_last_recharge_duration = get_difference_in_days(df_processed['last_date_of_month_7'], df_processed['date_of_last_rech_7'])\naug_last_recharge_duration = get_difference_in_days(df_processed['last_date_of_month_8'], df_processed['date_of_last_rech_8'])\ndf_processed['change_in_duration_of_last_recharge'] = (june_last_recharge_duration + july_last_recharge_duration) \/2 - aug_last_recharge_duration\ndf_processed['change_in_duration_of_last_recharge'] = df_processed['change_in_duration_of_last_recharge'].fillna(0)\n\n#Derived Features after operating on dates - average duration change in last data recharge of month\njune_last_data_recharge_duration = get_difference_in_days(df_processed['last_date_of_month_6'], df_processed['date_of_last_rech_data_6'])\njuly_last_data_recharge_duration = get_difference_in_days(df_processed['last_date_of_month_6'], df_processed['date_of_last_rech_data_7'])\naug_last_data_recharge_duration = get_difference_in_days(df_processed['last_date_of_month_6'], df_processed['date_of_last_rech_data_8'])\ndf_processed['change_in_duration_of_last_data_recharge'] = (june_last_data_recharge_duration + july_last_data_recharge_duration) \/2 - aug_last_data_recharge_duration\ndf_processed['change_in_duration_of_last_data_recharge'] = df_processed['change_in_duration_of_last_data_recharge'].fillna(0)","89ad32d4":"dataset['av_amt_data_6_7'] = (dataset['amt_data_6']  + dataset['amt_data_7'])\/2\ndataset['av_amt_data_6_7'].describe()","b28050ac":"#Find the threshold to filter the customers\ntop_30_threshold = dataset[['av_amt_data_6_7']].quantile(0.70)[0]\nprint('Top 30 - high value customer recharge threshold : ' + str(top_30_threshold))","c16f0eb5":"#Filter records \nhigh_value_df = dataset.loc[dataset['av_amt_data_6_7'] >= top_30_threshold,:]\nprint('Number of records after applying filter: ' + str(len(high_value_df)))    \n\n#Derived feature for amt_data\nhigh_value_df = add_change_kpis_from_6_7_to_8(high_value_df,['amt_data'])","9e731d77":"high_value_df['churn'] = np.nan\nhigh_value_df.loc[:, 'churn'] = (( high_value_df['total_ic_mou_9'] <=0 ) &  (high_value_df['total_og_mou_9'] <=0) & ( high_value_df['vol_2g_mb_9'] <=0 ) & ( high_value_df['vol_3g_mb_9'] <=0))\nhigh_value_df['churn']= high_value_df['churn'].map({False:0, True:1})\n\nhigh_value_df['churn'].value_counts().plot(kind='bar', title='Churn Customer- Bar Plot', grid =True)","b30fce12":"#display churn and non churn customer count\nprint(high_value_df.groupby('churn')['churn'].count())","5e7d4f82":"col_names_to_remove = [name for name in high_value_df.columns.values if name.endswith('_9')]\nprint('Number of columns before removing churned phase columns: ' + str(len(high_value_df.columns)))\n\ncol_names_to_remove1 = ['last_date_of_month_6', 'last_date_of_month_7', 'last_date_of_month_8','arpu_6','arpu_7','arpu_8',\n                      'onnet_mou_6','onnet_mou_7','onnet_mou_8','offnet_mou_6','offnet_mou_7', 'offnet_mou_8',\n                       'total_og_mou_6','total_og_mou_7','total_og_mou_8','total_ic_mou_6','total_ic_mou_7','total_ic_mou_8',\n                       'spl_ic_mou_6','spl_ic_mou_7','spl_ic_mou_8','spl_og_mou_6','spl_og_mou_7','spl_og_mou_8',\n                        'total_rech_num_6','total_rech_num_7', 'total_rech_num_8','total_rech_data_6', 'total_rech_data_7', 'total_rech_data_8',\n                        'count_rech_2g_6', 'count_rech_2g_7', 'count_rech_2g_8','count_rech_3g_6', 'count_rech_3g_7',\n                       'count_rech_3g_8','arpu_3g_6', 'arpu_3g_7', 'arpu_3g_8', 'arpu_2g_6', 'arpu_2g_7', 'arpu_2g_8',\n                       'monthly_2g_6', 'monthly_2g_7', 'monthly_2g_8', 'sachet_2g_6', 'sachet_2g_7', 'sachet_2g_8',\n                       'monthly_3g_6', 'monthly_3g_7', 'monthly_3g_8', 'sachet_3g_6', 'sachet_3g_7', 'sachet_3g_8', \n                       'roam_ic_mou_6', 'roam_ic_mou_7', 'roam_ic_mou_8', 'roam_og_mou_6', 'roam_og_mou_7', 'roam_og_mou_8',\n                       'loc_og_t2t_mou_6', 'loc_og_t2t_mou_7', 'loc_og_t2t_mou_8', 'loc_og_t2m_mou_6', 'loc_og_t2m_mou_7', 'loc_og_t2m_mou_8',\n                       'loc_og_t2f_mou_6', 'loc_og_t2f_mou_7', 'loc_og_t2f_mou_8', 'loc_og_t2c_mou_6', 'loc_og_t2c_mou_7', 'loc_og_t2c_mou_8',\n                        'loc_og_mou_6', 'loc_og_mou_7', 'loc_og_mou_8', 'std_og_t2t_mou_6', 'std_og_t2t_mou_7', 'std_og_t2t_mou_8',\n                        'std_og_t2m_mou_6', 'std_og_t2m_mou_7', 'std_og_t2m_mou_8', 'std_og_t2f_mou_6', 'std_og_t2f_mou_7',\n                       'std_og_t2f_mou_8', 'std_og_mou_6', 'std_og_mou_7', 'std_og_mou_8', 'isd_og_mou_6', 'isd_og_mou_7',\n                       'isd_og_mou_8',  'og_others_6', 'og_others_7', 'og_others_8', 'loc_ic_t2t_mou_6', 'loc_ic_t2t_mou_7',\n                       'loc_ic_t2t_mou_8', 'loc_ic_t2m_mou_6', 'loc_ic_t2m_mou_7', 'loc_ic_t2m_mou_8', 'loc_ic_t2f_mou_6',\n                       'loc_ic_t2f_mou_7', 'loc_ic_t2f_mou_8', 'loc_ic_mou_6', 'loc_ic_mou_7', 'loc_ic_mou_8',\n                       'std_ic_t2t_mou_6', 'std_ic_t2t_mou_7', 'std_ic_t2t_mou_8', 'std_ic_t2m_mou_6', 'std_ic_t2m_mou_7',\n                      'std_ic_t2m_mou_8', 'std_ic_t2f_mou_6', 'std_ic_t2f_mou_7', 'std_ic_t2f_mou_8', 'std_ic_t2o_mou_6',\n                       'std_ic_t2o_mou_7', 'std_ic_t2o_mou_8', 'std_ic_mou_6', 'std_ic_mou_7', 'std_ic_mou_8',\n                       'isd_ic_mou_6', 'isd_ic_mou_7', 'isd_ic_mou_8', 'ic_others_6', 'ic_others_7', 'ic_others_8',\n                      'date_of_last_rech_data_6', 'date_of_last_rech_data_7', 'date_of_last_rech_data_8', \n                       'date_of_last_rech_6', 'date_of_last_rech_7', 'date_of_last_rech_8','fb_user_6', 'fb_user_7', 'fb_user_8',\n                       'night_pck_user_6', 'night_pck_user_7', 'night_pck_user_8','aug_vbc_3g', 'jul_vbc_3g', 'jun_vbc_3g',\n                       'sep_vbc_3g', 'vol_2g_mb_6', 'vol_2g_mb_7', 'vol_2g_mb_8', 'vol_3g_mb_6', 'vol_3g_mb_7',\n                       'vol_3g_mb_8', 'total_rech_amt_6', 'total_rech_amt_7', 'total_rech_amt_8', 'max_rech_amt_6',\n                       'max_rech_amt_7', 'max_rech_amt_8', 'last_day_rch_amt_6', 'last_day_rch_amt_7', 'last_day_rch_amt_8',\n                       'max_rech_data_6', 'max_rech_data_7', 'max_rech_data_8', 'av_rech_amt_data_6', 'av_rech_amt_data_7',\n                       'av_rech_amt_data_8','av_amt_data_6_7','total_data_rech_6', 'total_data_rech_7','total_data_rech_8','amt_data_6', 'amt_data_7', 'amt_data_8' ]\n\n#Drop columns\nhigh_value_df.drop(col_names_to_remove, axis =1, inplace=True)\nhigh_value_df.drop(col_names_to_remove1, axis =1, inplace=True)\n\n\nprint('Number of columns after removing churned phase columns + columns used in derived features: ' + str(len(high_value_df.columns)))","433be451":"print('Number of columns: ' + str (len(high_value_df.columns)))\nprint('Final set of feature list:' )\nhigh_value_df.columns","ce7ec574":"#Churn Distribution\nax = (high_value_df['churn'].value_counts()*100.0 \/len(high_value_df)).plot.pie(autopct='%.1f%%', labels = ['No', 'Yes'],figsize =(5,5), fontsize = 12 )                                                                           \nax.set_ylabel('Churn',fontsize = 12)\nax.grid(True)\nax.set_title('Churn Distribution', fontsize = 12)","432d27ca":"churn_df = high_value_df.loc[high_value_df['churn']> 0 , :]\nnon_churn_df = high_value_df.loc[high_value_df['churn'] == 0 , :]\n\ndef draw_plot_of_churn_vs_non_chrun_for(churn_df, non_churn_df, column_name):\n    plt.figure(figsize=(20,8))\n    plt.subplot(1,2,1)\n    plt.grid(True)\n    plt.title('Churn customer - ' + column_name)\n    sns.distplot(churn_df[column_name])\n\n    plt.subplot(1,2,2)\n    plt.grid(True)\n    plt.title('Non-Churn customer - ' + column_name)\n    sns.distplot(non_churn_df[column_name])\n\ndraw_plot_of_churn_vs_non_chrun_for(churn_df, non_churn_df, 'change_in_arpu' )","2377c431":"draw_plot_of_churn_vs_non_chrun_for(churn_df, non_churn_df, 'change_in_onnet_mou' )","515ba846":"draw_plot_of_churn_vs_non_chrun_for(churn_df, non_churn_df, 'change_in_offnet_mou' )","ed6f5cf1":"draw_plot_of_churn_vs_non_chrun_for(churn_df, non_churn_df, 'change_in_total_rech_amt' )","5bda4841":"draw_plot_of_churn_vs_non_chrun_for(churn_df, non_churn_df, 'aon' )","b9d70a00":"draw_plot_of_churn_vs_non_chrun_for(churn_df, non_churn_df, 'change_in_vbc_3g' )","cfdf114e":"#Get Correlation of \"Churn\" with other variables:\nplt.figure(figsize=(20,10))\nhigh_value_df.corr()['churn'].sort_values(ascending = False).plot(kind='bar', grid=True)","9147c885":"from imblearn.over_sampling import SMOTE \nfrom sklearn.model_selection import train_test_split\n\nsm = SMOTE(sampling_strategy ='minority' ,random_state=1000, ratio=0.3)\n\n\nX = high_value_df.drop('churn', axis =1)\ny = high_value_df[['churn']]\n\n\n#Test train - Split\nx_train, x_val, y_train, y_val = train_test_split(X, y, test_size = .2, random_state=12)\n\n#resampling data using SMOTE to balance the classes - After executing this line, we will have 70% data for non churn customer and \n#30 % data for churned customer \nx_train_res, y_train_res = sm.fit_sample(x_train, y_train)\n\nx_train_df = pd.DataFrame(x_train_res, columns=x_train.columns)\ny_train_df = pd.DataFrame(y_train_res, columns= y_train.columns)\n","ab868230":"x_mean = x_train_df.mean()\nx_std = x_train_df.std()\nx_train_normalized_df=(x_train_df-x_mean)\/x_std\nx_test_normalized_df = (x_val --x_mean)\/x_std","d68349e8":"from sklearn.decomposition import PCA\npca = PCA(svd_solver='randomized', random_state=100)\nx_train_normalized_df = x_train_normalized_df.drop('change_in_std_ic_t2o_mou', axis=1) \nx_test_normalized_df= x_test_normalized_df.drop('change_in_std_ic_t2o_mou', axis=1) \npca.fit(x_train_normalized_df)","8ea1dd8a":"display(pca.components_)","bbe45391":"#Display pca components\ncolnames = list(x_train.columns)\npcs_df = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'Feature':x_train_normalized_df.columns})\npcs_df.head(10)","3d1319a5":"%matplotlib inline\nfig = plt.figure(figsize = (8,8))\nplt.scatter(pcs_df.PC1, pcs_df.PC2)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nfor i, txt in enumerate(pcs_df.Feature):\n    plt.annotate(txt, (pcs_df.PC1[i],pcs_df.PC2[i]))\nplt.tight_layout()\nplt.show()","ac9fd8eb":"# Scree plot to see number of required components\n%matplotlib inline\nfig = plt.figure(figsize = (12,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.grid(True)\nplt.ylabel('cumulative explained variance')\nplt.show()","bcca9a41":"from sklearn.decomposition import IncrementalPCA\npca_final = IncrementalPCA(n_components=30)","a215952e":"df_train_pca = pca_final.fit_transform(x_train_normalized_df)\ndf_train_pca.shape","984101f2":"#Correlation Matrix\ncorrmat = np.corrcoef(df_train_pca.transpose())\n\n%matplotlib inline\nplt.figure(figsize = (20,12))\nsns.heatmap(corrmat,annot = True)","4bb440d3":"x_test_normalized_df\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nlearner_pca = LogisticRegression( )\nmodel_pca = learner_pca.fit(df_train_pca, y_train_df)\n\ndf_test_pca = pca_final.fit_transform(x_test_normalized_df)","b498c3a9":"pred_probs_test = model_pca.predict_proba(df_test_pca)[:,1]\nprint(\"Overall AUC score : \" + str(metrics.roc_auc_score(y_val, pred_probs_test)))","90aa5e44":"from sklearn.metrics import confusion_matrix\ndef draw_confusion_matrix(cf_matrix):\n    df_cm = pd.DataFrame(cf_matrix)\n    sns.heatmap(df_cm, annot=True, fmt='g')  \n\ny_pred = model_pca.predict(df_test_pca)\ndraw_confusion_matrix(confusion_matrix(y_val, y_pred))","4c86a078":"learner_pca = LogisticRegression( class_weight = {0: 0.2, 1: 0.8} )\nmodel_pca = learner_pca.fit(df_train_pca, y_train_df)\n\ny_pred = model_pca.predict(df_test_pca)\nconfusion= confusion_matrix(y_val, y_pred)\ndraw_confusion_matrix (confusion)","b4b2a855":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives\n\nprint('Sensitivity of PCA w\/ logistic regression model ' + str( TP \/ float(TP+FN)))\nprint('Specificity of PCA w\/ logistic regression model ' + str (TN \/ float(TN+FP)))","2ce68daa":"# fit model on training data with default hyperparameters\nimport xgboost as xgb\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nmodel = XGBClassifier()\nmodel.fit(x_train_normalized_df, y_train_df)","abfba692":"# make predictions for test data\n# use predict_proba since we need probabilities to compute auc\ny_pred = model.predict_proba(x_test_normalized_df)\nprint(y_pred[:10])","262281a1":"# evaluate predictions\nprint(\"AUC: \" + str(metrics.roc_auc_score(y_val, y_pred[:, 1])))","a78415b7":"# hyperparameter tuning with XGBoost\n\n# creating a KFold object \nfolds = 3\n# specify range of hyperparameters\nparam_grid = {'learning_rate': [0.2, 0.6], 'subsample': [0.3, 0.6, 0.9]}         \n\n\n# specify model\nxgb_model = XGBClassifier(max_depth=2, n_estimators=200)\n\n# set up GridSearchCV()\nmodel_cv = GridSearchCV(estimator = xgb_model,   param_grid = param_grid,   scoring= 'roc_auc',   cv = folds, \n                        n_jobs=-1,    verbose = 1,   return_train_score=True)      ","fa79500f":"# fit the model\nmodel_cv.fit(x_train_normalized_df, y_train_df)  ","4fbe73eb":"# cv results\ncv_results = pd.DataFrame(model_cv.cv_results_)","3a76d056":"# convert parameters to int for plotting on x-axis\ncv_results['param_learning_rate'] = cv_results['param_learning_rate'].astype('float')","fa713590":"# # plotting\nplt.figure(figsize=(16,6))\n\nparam_grid = {'learning_rate': [0.2, 0.6], \n             'subsample': [0.3, 0.6, 0.9]} \n\n\nfor n, subsample in enumerate(param_grid['subsample']):\n    \n\n    # subplot 1\/n\n    plt.subplot(1,len(param_grid['subsample']), n+1)\n    df = cv_results[cv_results['param_subsample']==subsample]\n\n    plt.plot(df[\"param_learning_rate\"], df[\"mean_test_score\"])\n    plt.plot(df[\"param_learning_rate\"], df[\"mean_train_score\"])\n    plt.xlabel('learning_rate')\n    plt.ylabel('AUC')\n    plt.title(\"subsample={0}\".format(subsample))\n    plt.ylim([0.60, 1])\n    plt.legend(['test score', 'train score'], loc='upper left')\n    plt.xscale('log')","6dbe76bf":"# chosen hyperparameters\n# 'objective':'binary:logistic' outputs probability rather than label, which we need for auc\nparams = {'learning_rate': 0.2, 'max_depth': 2, 'n_estimators':200,  'subsample':0.6,  'n_jobs':-1,\n         'objective':'binary:logistic'}\n\n# fit model on training data\nmodel = XGBClassifier(params = params)\nmodel.fit(x_train_normalized_df, y_train_df)","1507d007":"# predict\ny_pred = model.predict_proba(x_test_normalized_df)","591473f4":"from sklearn.metrics import roc_auc_score\nauc = roc_auc_score(y_val, y_pred[:, 1])\nprint(\"AUC: \" + str(auc))\n\npredictions = model.predict(x_test_normalized_df)\nconfusion = confusion_matrix(y_val, predictions)\nTP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives\n\nprint('Sensitivity of XG Boost: ' + str (TP \/ float(TP+FN)))\nprint('Specificity of XG Boost: ' + str (TN \/ float(TN+FP)))","2ae310da":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(class_weight={1:0.7, 0:0.3})","6df8ffbe":"rfc.fit(x_train_normalized_df, y_train_df)","51d3a8af":"from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\npredictions = rfc.predict(x_test_normalized_df)\nprint(classification_report(y_val,predictions))\nprint('Accuracy score: ' + str(accuracy_score(y_val,predictions)))","f58bd660":"#Hyperparameter Tuning\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nn_folds = 5\nparameters = {'max_depth': range(2, 20, 2)}\nrf = RandomForestClassifier(class_weight={1: 0.7, 0: 0.3})\nrf = GridSearchCV(rf, parameters, cv=n_folds,  scoring=\"accuracy\")\nrf.fit(x_train_normalized_df, y_train_df)\n","953bf0f9":"# Plot - Hyperparameter grid search result\nscores = rf.cv_results_\nplt.figure()\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_train_score\"], \n         label=\"training accuracy\")\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"Accuracy\")\nplt.grid(True)\nplt.legend()\nplt.show()","694ba37a":"#Final Model\nrfc = RandomForestClassifier(bootstrap=True, class_weight={1: 0.7, 0: 0.3},  max_depth=14, n_estimators=20)\nrfc.fit(x_train_normalized_df, y_train_df)\n\npredictions = rfc.predict(x_test_normalized_df)\nprint('Accuracy score: ' + str(accuracy_score(y_val,predictions)))\n","2de5f415":"confusion = confusion_matrix(y_val, predictions)\nTP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives\n\nprint('Sensitivity of Random Forest: ' + str( TP \/ float(TP+FN)))\nprint('Specificity of Random Forest: ' + str (TN \/ float(TN+FP)))","f539de6f":"logreg = LogisticRegression()\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg, 15)   # select top-15 features\nrfe = rfe.fit(x_train_normalized_df, y_train_df)","eaf8a71b":"list(zip(x_train_normalized_df.columns, rfe.support_, rfe.ranking_))","49e35e3e":"col = x_train_normalized_df.columns[rfe.support_]\nprint('Top 15 features: ')\ncol","bccbcb13":"import statsmodels.api as sm\nX_train_sm = sm.add_constant(x_train_normalized_df[col])\nlogm2 = sm.GLM(y_train_df,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","66feb8eb":"x_test_normalized_df_sm = sm.add_constant(x_test_normalized_df[col])\n\n#consider all predicted values above 0.5 as churn\ny_train_pred = res.predict(x_test_normalized_df_sm).map(lambda x: 1 if x > 0.5 else 0)\nprint('Accuracy score: ' +str(accuracy_score(y_val,y_train_pred)))","ae343ffb":"confusion = confusion_matrix(y_val, y_train_pred)\nTP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives\n\nprint('Sensitivity of Logistic Regression: ' + str( TP \/ float(TP+FN)))\nprint('Specificity of Logistic Regression: ' + str (TN \/ float(TN+FP)))","836a669b":"#### Top Parameters\n> We can conclude that the top parameters impacting churn of customers are:\n1. aon\n2. change in recharge amount\n3. change in arpu\n4. change in duration of recharge\n5. change in total recharge num","91f595ef":"#### Creating features out of date column to understand the difference in pattern between jun+july and august","40b33633":"First we create default XGB Classifier based without any parameter tuning","a9cf6811":"> We can clearly see that variables having high correlation with churn variable.\nPositive Correlation : Change in ARPU, mou, data usage etc\nNegative Correlation : Change in aon, duration of last recharge etc","26f90378":"#### Change in volume based cost 3g of Churn vs Non-Churn","204d6c42":"### Model #1: PCA (+ Logistic Regression)","5b5934ed":"> Now we will build the model using x_train_normalized_df, y_train_df dataframe","f6a49960":"#### Creating features to observe difference in pattern between june, july and august","76ca5012":"### Model #2: XG Boost","7367540c":"# <p style=\"text-align: center;\"> Telecom Churn - Case Study <\/p>\nThe business objective is to predict the churn in the last (i.e. the ninth) month using the data (features) from the first three months. To do this task well, understanding the typical customer behaviour during churn will be helpful\n## Business Goals \n1. Retain high profitable customers \n2. Identify customers at high risk of churn\n3. Identify the main indicators of churn","ea9205ee":"#### ARPU of Churn vs Non-Churn","2f24658b":"#### Change in_offnet_mou of Churn vs Non-Churn","dd322a9b":"> All the non-numeric columns are date columns. Four columns are used to store last date of the month. We will process these columns later before building model.\n\n> We need to change data types to date, also lets fill the missing values for last date of month column","eff8bd7c":"### Model #3: Random Forest","9247bfdd":"> There does not look like much difference between churn vs non-churn for aon feature","f837800a":"## Step 2: Data Preparation","82c213a7":"## Step 3: Exploratory Data Analysis","83ba09dd":"> Looking at the graph, max_depth =14 seems good. ","5db48bae":"> Now, we have got decent score and class are being predicted correctly.","f2a389d1":"#### Correlation plot - Churn vs Other variables","b8dff17d":"> As we can see from the result that there are more churned customer which are being predicted as non churn.\nso based on business context we wil need to give more weightage to get more non churn customer predicted correctly.","85868635":"### Feature Engineering - Extracting New features","d5e5252e":"> Looking at the color codes of cell, we can see that there is no correlation at all in the data\n","97d83931":"### Import required libraries and load churn data set","58f92932":"> As we can see, approximately 30 compoents are enough to explain 90%+ of variance","dc7f9c14":"#### Final set of features after data preprocessing","28fd373f":"### Set 'churn' label based on phone calls and internet data usage","27be8080":"> Now lets move to data preparation step","e2715376":"> Same observations as of previous one ","e26e0082":"#### Change in aon of Churn vs Non-Churn","1ff9fc0d":"## Step 5: Model Building","933f2b3a":"| Model | Accuracy | AUC | Sensitivity | Specificity |\n|------|------|------|------|------|\n|PCA (+Logistic Regression)  | - | 0.7997 | 0.8357 | 0.6454 |\n|XG Boost | - | 0.8417 | 0.9655 | 0.3121 |\n|Random Forest | 0.7964 | - | 0.7688 | 0.7988 |\n|Logistic Regression with RFE | 0.9180 | - | 0.1460 | 0.9871 |","d95d8c41":"#### Normalize data","506a1a4d":"### Filter records to keep only high value customer\n> As we are only interested in customer who are paying significantly (top 30%) more than rest of the customer.","b13831e3":"### Removing columns having same value across rows","196a1714":"### Checking data columns having non numeric data types","8c963c18":"> As shown in the above Pie chart, churn vs non-churn data dist. is highly imbalanced so we will need to take appropriate action before building model","d3a354c0":"#### Create derived feature to filter high value customer data","f94d3a6e":"> Non- churned customers have wider distribution.","3932c5dc":"#### Churn disturbution ","bd457295":"> Observations based on above result-\n1. mobile_number - Not important as this is just unique customer mobile number.\n2. circle_id - All the rows having same values so not important and can be dropped.\n3. loc_og_t2o_mou,\tstd_og_t2o_mou, loc_ic_t2o_mou, std_og_t2c_mou_6, std_og_t2c_mou_7,\tstd_og_t2c_mou_8, std_og_t2c_mou_9 -> All the values are zero so they can be dropped.\n\nSo lets remove these columns based on observations above\n","72cf93ec":"> As shown in the above figure, we can see that first PC1 covers the duration of last recharge and second compoenent covers outgoing minute usage","f1e32fe0":"#### Best model|\n> Summarizing all the 4 models","03c28f34":"Based on business context and understanding given in the problem statement, following are the set of features we are going to extract from data:\n1. Change in 'arpu' from june and july to auguest\n2. Change in onnet_mou and offnet_mou \n3. Change in total outgoing calls\n4. Change in total incoming calls\n5. Change in special incoming and outgoing calls\n6. Change in number of recharges\n7. Change in total amount of recharges\n8. Change in number of data recharge and rest of the columns\n9. We will create few features using date columns\n","460b7fde":"> We can see same trend in the change in onnet mou - as of previous one","9171cedb":"## Step 4: Handling of Class Imbalance using SMOTE","454f3e17":"Removing all the attributes of 'churned' phase + all the attributes from which derived features have been created","1777d40d":"##### Logistic Regression using PCA components","003d838c":"> From the four models that were evaluated: \n- Logistic Regression with RFE has the highest accuracy (~92%), however it scores very low on Sensitivity (~15%) which is essential for this use case.\n- XG Boost has the next best accuracy (~84%), and it has highest sensitivity (~97%). Considering the low specificity (~31%), there is an imbalance.\n- PCA has good AUC (~80%) and high sensitivity (~84%) with a good degree of specificity (~65%)\n- Random Forest also has good AUC (~80%) with good balance of sensitivity (~77%) and specificity (~80%)\n","0bb353f0":"### Model #4: Logistic Regression with RFE\n##### For selecting top influencing features ","4fe87125":"#### Change_in_onnet_mou of Churn vs Non-Churn","c43e99f8":">  The results obtained by parameter tuning is good to compare to other models. Thus we consider XGBoost as more accurate and optimised","8ce48f96":"> After doing gridsearch of multiple parameter, we found above model which has good score with limited set of parameters","dbc66b92":"## Step 1: Data Pre-processing","c582f1b4":"#### Change in total recharge amnt of Churn vs Non-Churn","38b7128b":"## Step 6: Conclusion","566e8941":"> As we can see above, ARPU of the churned customers have wider distribution of change in ARPU between june\/july and august. Also it is rightly skewed.\n\n> It means that ARPU of august is significantly less compared to jun\/july for most of the churned customers","906c34fc":"> Now lets do hyper parameter tuning"}}