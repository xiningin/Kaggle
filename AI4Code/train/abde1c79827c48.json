{"cell_type":{"c89b1185":"code","98d73ca3":"code","e912b475":"code","27a44bc4":"code","f85f0ae6":"markdown","46be5177":"markdown","0c7e94f1":"markdown"},"source":{"c89b1185":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","98d73ca3":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()\ntrain_data.info()","e912b475":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()\ntest_data.info()","27a44bc4":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\ny = train_data[\"Survived\"]\n\n#features taken for consideration\nfeatures = [\"Pclass\",\"Sex\",\"SibSp\",\"Parch\", \"Embarked\",\"Fare\"] \n\n#pre-process\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nimp = IterativeImputer(max_iter=10, random_state=0)\n\n\n#getting the input for our model\nx = pd.get_dummies(train_data[features])\nx_test = pd.get_dummies(test_data[features])\n\nimp.fit(x)\nx_clean = imp.transform(x) \nx_test_clean = imp.transform(x_test)\n\n\n#defining our model\nmodel = RandomForestClassifier(n_estimators = 200, max_depth = 20\n                              , random_state = 2)\n\n#linking i\/p and expected o\/p r prediction for our model for training\nmodel.fit(x_clean ,y)\n\n##return a list of predictions for test data\npredictions = model.predict(x_test_clean)\n\n#create a o\/p dataframe\noutput = pd.DataFrame ({'PassengerId' : test_data.PassengerId , \n                       'Survived' : predictions })\n\n#save as a csv for submission\noutput.to_csv(\"my_submission.csv\",index=False)\n\n#return a list of predictions for training data (testing accuracy)\ny_predict = model.predict(x_clean)\nprint(accuracy_score(y , y_predict))\n","f85f0ae6":"### Loading the training dataset\n","46be5177":"### Loading Test dataset\nAs we can notice, the number of columns is one less from _train_data_ meaning the **\"Survived\"** column is removed","0c7e94f1":"### Implementing Random Forest Classifier\n"}}