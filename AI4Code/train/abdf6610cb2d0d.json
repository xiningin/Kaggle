{"cell_type":{"ab1f0ac0":"code","e7b61b24":"code","b6ab3a8d":"code","07189462":"code","0ec4d07e":"code","45f5fbd2":"code","159394b9":"code","27722fc0":"code","2f8602c7":"code","c5ecef00":"code","056e5f09":"code","cf2a6f6c":"code","cf53f272":"code","47617e5c":"code","2a9440ed":"code","8ef86fab":"code","b6b01ffa":"code","392408cf":"code","747f361c":"code","29afd7ed":"code","af67d42d":"code","bc822d9f":"code","3767faf4":"code","b785955b":"code","cf21a0b4":"code","b4ed82fc":"code","be5cc51a":"code","b997a818":"code","08338936":"code","874c2d68":"code","ad6d8542":"code","3a22ac56":"code","577632b6":"code","146a66d1":"code","02ce365c":"code","e9fa4723":"code","54d7c118":"code","d8f0c12e":"markdown","b1c5bcaf":"markdown","a5bb486d":"markdown","571347d8":"markdown","58a7d97c":"markdown","162ae76e":"markdown","5de34f50":"markdown","c168c12b":"markdown","e7b8c109":"markdown","48a032bf":"markdown","25197945":"markdown","217957c2":"markdown","f167d32f":"markdown","5b8234e9":"markdown","0c4d934e":"markdown","142469d0":"markdown","fbffc9c7":"markdown"},"source":{"ab1f0ac0":"from fastai.text.all import *\n# from utils import *\nfrom nlp_utils import *  # different name used in Kaggle platform\nfrom IPython.display import clear_output\n\npath = Path(\"..\/input\/nlp-fastai-dataset-chapter-8\")\npath.ls()","e7b61b24":"df = pd.read_csv(path\/\"questions_easy.csv\")\ndf[\"en\"] = df[\"en\"].apply(lambda x: x.lower())\ndf[\"fr\"] = df[\"fr\"].apply(lambda x: x.lower())\n\nsl = 72\n\ndls = DataBlock(\n    blocks=(TextBlock.from_df(\"fr\", seq_len=sl, tok=SpacyTokenizer(\"fr\")), \n            TextBlock.from_df(\"en\", seq_len=sl, tok=SpacyTokenizer(\"en\"))),\n    get_x=ColReader(\"text\"), get_y=ColReader(\"text\"),\n    splitter=RandomSplitter(0.1)\n).dataloaders(df, bs=64, num_workers=os.cpu_count(), seq_len=sl)\ndls.show_batch(max_n=2)","b6ab3a8d":"v = dls.vocab[0]\nstoi(v, \"xxpad\")","07189462":"def shift_tfm(b):\n    x, y = b\n    y = F.pad(y, (1, 0), value=1)\n    return [x, y[:, :-1]], y[:, 1:]\n\n# class ShiftTfm(Transform):\n#     def encodes(self, b): return shift_tfm(b)\n#     def decodes(self, b): return b\n\nclass ShiftTfm(Callback):\n    def before_batch(self): \n        self.learn.xb, _ = shift_tfm((self.x, self.y))\n\n\n# dls.add_tfms([ShiftTfm()], \"after_batch\")","0ec4d07e":"d = 30\ntorch.arange(0., d, 2.) \/ d","45f5fbd2":"class PositionalEncoding(Module):\n    \"\"\"\n    Encode the position with a sinusoid. \n    \"\"\"\n    def __init__(self, d): \n        self.register_buffer(\"freq\", 1 \/ (1e4 ** (torch.arange(0., d, 2.) \/ d)))\n\n    def forward(self, pos):\n        inp = torch.ger(pos, self.freq)\n        enc = torch.cat([inp.sin(), inp.cos()], dim=-1)\n        return enc","159394b9":"tst_enc = PositionalEncoding(20)\nres = tst_enc(torch.arange(0, 100).float())\nplt.figure(dpi=100)\nfor i in range(1, 5): plt.plot(res[:, i])","27722fc0":"res[:6, :6]","2f8602c7":"class TransformerEmbedding(Module):\n    \"Embedding + positional encoding + dropout\"\n    def __init__(self, vocab_sz, emb_sz, inp_p=0.):\n        super(TransformerEmbedding, self).__init__()\n        self.emb_sz = emb_sz\n        self.embed = embedding(vocab_sz, emb_sz)\n        self.pos_enc = PositionalEncoding(emb_sz)\n        self.drop = nn.Dropout(inp_p)\n        self._msq_emb_sz = math.sqrt(self.emb_sz)\n\n    def forward(self, inp):\n        pos = torch.arange(0, inp.size(1), device=inp.device).float()\n        return self.drop(self.embed(inp) * self._msq_emb_sz + self.pos_enc(pos))","c5ecef00":"def feed_forward(d_model, d_ff, ff_p=0., double_drop=True):\n    layers = [nn.Linear(d_model, d_ff), nn.ReLU()]\n    if double_drop: layers.append(nn.Dropout(ff_p))\n    return SequentialEx(*layers, nn.Linear(d_ff, d_model), nn.Dropout(ff_p),\n                MergeLayer(), nn.LayerNorm(d_model))","056e5f09":"class MultiHeadAttention(Module):\n    def __init__(self, n_heads, d_model, d_head=None, p=0., bias=True, scale=True):\n        super(MultiHeadAttention, self).__init__()\n        d_head = ifnone(d_head, d_model \/\/ n_heads)\n        self.n_heads, self.d_head, self.scale = n_heads, d_head, scale\n        self.q_wgt, self.k_wgt, self.v_wgt = [nn.Linear(\n            d_model, n_heads * d_head, bias=bias\n        ) for o in range(3)]\n\n        self.out = nn.Linear(n_heads * d_head, d_model, bias=bias)\n        self.drop_att, self.drop_res = nn.Dropout(p), nn.Dropout(p)\n        self.ln = nn.LayerNorm(d_model)\n\n    def forward(self, q, kv, mask=None):\n        return self.ln(q + self.drop_res(self.out(\n            self._apply_attention(q, kv, mask=mask))))\n        \n    def create_attn_mat(self, x, layer, bs):\n        return layer(x).view(bs, x.size(1), self.n_heads, self.d_head\n                        ).permute(0, 2, 1, 3)\n\n    def _apply_attention(self, q, kv, mask=None):\n        bs, seq_len = q.size(0), q.size(1)\n        wq, wk, wv = map(lambda o: self.create_attn_mat(*o, bs),\n                        zip( (q, kv, kv), (self.q_wgt, self.k_wgt, self.v_wgt) ))\n        \n        attn_score = wq @ wk.transpose(2, 3)\n        if self.scale: attn_score \/= math.sqrt(self.d_head)\n\n        if mask is not None:\n            attn_score = attn_score.float().masked_fill(mask, -float(\"inf\")) \\\n                                    .type_as(attn_score)\n\n        attn_prob = self.drop_att(F.softmax(attn_score, dim=-1))\n        attn_vec = attn_prob @ wv\n\n        return attn_vec.permute(0, 2, 1, 3).contiguous().view(bs, seq_len, -1)","cf2a6f6c":"def get_output_mask(inp, pad_idx=1):\n    return torch.triu(inp.new_ones((inp.size(1), inp.size(1))),\n    diagonal=1)[None, None].bool()\n\n\ntorch.triu(torch.ones(10, 10), diagonal=1).bool()","cf53f272":"class EncoderBlock(Module):\n    \"\"\"Encoder block of Transformer Model.\"\"\"\n    def __init__(self, n_heads, d_model, d_head, d_inner, p=0., bias=True,\n                scale=True, double_drop=True):\n        super(EncoderBlock, self).__init__()\n        self.mha = MultiHeadAttention(n_heads, d_model, d_head, p=p, bias=bias,\n                    scale=scale)\n        self.ff = feed_forward(d_model, d_inner, ff_p=p, double_drop=double_drop)\n\n    def forward(self, x, mask=None): return self.ff(self.mha(x, x, mask=mask))","47617e5c":"class DecoderBlock(Module):\n    \"\"\"Decoder Block of a Transformer\"\"\"\n    def __init__(self, n_heads, d_model, d_head, d_inner, p=0., bias=True,\n                scale=True, double_drop=True):\n        super(DecoderBlock, self).__init__()\n        self.mha1 = MultiHeadAttention(n_heads, d_model, d_head, p=p, bias=bias, scale=scale)\n        self.mha2 = MultiHeadAttention(n_heads, d_model, d_head, p=p, bias=bias, scale=scale)\n        self.ff = feed_forward(d_model, d_inner, ff_p=p, double_drop=double_drop)\n\n    def forward(self, x, enc, mask_out=None): \n        return self.ff(self.mha2(\n            self.mha1(x, x, mask_out), enc\n        ))","2a9440ed":"class Transformer(Module):\n    def __init__(self, inp_vsz, out_vsz, n_layers=6, n_heads=8, d_model=256,\n                d_head=32, d_inner=1024, p=0.1, bias=True, scale=True, \n                double_drop=True, pad_idx=1):\n        self.enc_emb = TransformerEmbedding(inp_vsz, d_model, p)\n        self.dec_emb = TransformerEmbedding(out_vsz, d_model, 0.)\n        self.n_layers = n_layers\n\n        args = (n_heads, d_model, d_head, d_inner, p, bias, scale, double_drop)\n\n        self.encoder = nn.ModuleList([EncoderBlock(*args) for _ in range(n_layers)])\n        self.decoder = nn.ModuleList([DecoderBlock(*args) for _ in range(n_layers)])\n        self.out = nn.Linear(d_model, out_vsz)\n        self.out.weight = self.dec_emb.embed.weight\n        self.pad_idx = pad_idx\n\n    def forward(self, inp, out):\n        mask_out = get_output_mask(out, self.pad_idx)\n        enc, out = self.enc_emb(inp), self.dec_emb(out)\n        for encd in self.encoder: enc = encd(enc)\n        for k, decd in enumerate(self.decoder): out = decd(out, enc, mask_out)\n            \n        ### The below code now raises NotImplementedError without any explanation.\n        ### One could only infer how to implement the code below. \n        ### However, the best would just be to use PyTorch's \n        ### nn.Transformer module instead of manual implementation nowadays. \n\n        # enc = compose(self.encoder)(enc)\n        # out = compose(self.decoder)(out, enc, mask_out)\n\n        return self.out(out)","8ef86fab":"class DecoderBlock(nn.Module):\n    \"Decoder block of a Transformer model.\"\n    #Can't use Sequential directly cause more than one input...\n    def __init__(self, n_heads, d_model, d_head, d_inner, p=0., bias=True, scale=True, double_drop=True):\n        super().__init__()\n        self.mha1 = MultiHeadAttention(n_heads, d_model, d_head, p=p, bias=bias, scale=scale)\n        self.mha2 = MultiHeadAttention(n_heads, d_model, d_head, p=p, bias=bias, scale=scale)\n        self.ff   = feed_forward(d_model, d_inner, ff_p=p, double_drop=double_drop)\n    \n    def forward(self, x, enc, mask_out=None): return self.ff(self.mha2(self.mha1(x, x, mask_out), enc))","b6b01ffa":"n_x_vocab, n_y_vocab = [len(a) for a in dls.vocab]\n\nmodel = Transformer(n_x_vocab, n_y_vocab, d_model=256)\nlearn = Learner(dls, model, metrics=[accuracy, CorpusBLEUMetric(n_y_vocab)],\n                loss_func=CrossEntropyLossFlat(), cbs=ShiftTfm()).to_fp16()\nlearn.lr_find()","392408cf":"learn.fit_one_cycle(8, 5e-4, div=5)","747f361c":"def get_preds(learn):\n    learn.model.eval()\n    inputs, targets, outputs = [], [], []\n\n    with torch.no_grad():\n        for xb, yb in progress_bar(learn.dls.valid):\n            xb, yb = shift_tfm((xb, yb))\n            out = learn.model(*xb)\n\n            for x, y, z in zip(xb[0], xb[1], out):\n                inputs.append(x.cpu())\n                targets.append(y.cpu())\n                outputs.append(z.argmax(1).cpu())\n\n    clear_output()\n    \n    return inputs, targets, outputs","29afd7ed":"class GetPreds:\n    def __init__(self, inputs, preds, targs):\n        self.inputs, self.preds, self.targs = inputs, preds, targs\n\n    def get_predictions(self, num, ignore_pad=False): \n        \"\"\":ignore_pad: Whether to ignore pad for predictions. Default: False\"\"\"\n        return (\n            itos(dls.vocab[0], self.inputs[num], join=True, ignore_pad=True),\n            itos(dls.vocab[1], self.targs[num], join=True, ignore_pad=True),\n            itos(dls.vocab[1], self.preds[num], join=True, ignore_pad=ignore_pad)\n        )","af67d42d":"# inputs, preds, targs = learn.get_preds(with_input=True)\ninputs, targs, preds = get_preds(learn)\np = GetPreds(inputs, preds, targs)","bc822d9f":"p.get_predictions(10, ignore_pad=True)","3767faf4":"p.get_predictions(700, ignore_pad=True)","b785955b":"p.get_predictions(701, ignore_pad=True)","cf21a0b4":"p.get_predictions(2500, ignore_pad=True)","b4ed82fc":"p.get_predictions(4002, ignore_pad=True)","be5cc51a":"try: del model, learn, inputs, preds, targs\nexcept Exception: pass\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()","b997a818":"model = Transformer(n_x_vocab, n_y_vocab, d_model=256)\nruntimeerror = True\nwhile runtimeerror:\n    try:\n        learn = Learner(dls, model, metrics=[accuracy, CorpusBLEUMetric(n_y_vocab)],\n                        loss_func=LabelSmoothingCrossEntropyFlat(axis=-1),\n                        cbs=ShiftTfm()).to_fp16()\n        learn.lr_find()\n        runtimeerror = False\n    except RuntimeError:\n        torch.cuda.empty_cache()\n        gc.collect()","08338936":"learn.fit_one_cycle(8, 5e-4, div=5)","874c2d68":"gc.collect()\ntorch.cuda.empty_cache()\n\ninputs, targs, preds = get_preds(learn)\np = GetPreds(inputs, preds, targs)","ad6d8542":"p.get_predictions(10, ignore_pad=True)","3a22ac56":"p.get_predictions(700, ignore_pad=True)","577632b6":"p.get_predictions(701, ignore_pad=True)","146a66d1":"# This looks problematic with input as English instead of French... \np.get_predictions(4001, ignore_pad=True)","02ce365c":"learn.model.eval()\n\nxb, yb = dls.one_batch()\nxb, yb = shift_tfm((xb, yb))  # as usual, we need this since its now in callback.\n\n# We make some changes as our batch shape isn't the same as the original code. \ninp1, out1 = xb[0], xb[1]\ninp2, out2 = inp1.clone(), out1.clone()\nout2[15] = 10","e9fa4723":"y1 = learn.model(inp1, out1)\ny2 = learn.model(inp2, out2)","54d7c118":"(y1[0, :15] - y2[0, :15]).abs().mean()","d8f0c12e":"## Test Leakage\nIf we change a token in targets at position n, it shouldn't impact the preds before that. ","b1c5bcaf":"### The Whole Model","a5bb486d":"#### Chapter 8: Attention and the Transformer\n*RNN is losing its luster with researchers.*\n\nRNNs can be a pain: parallelization can be tricky and they can be difficult to debug. Attention have achieving state of the art results on NLP. Convolutions may beat attention on some tasks, including English to German translation. Perhaps various strengths of RNNs, CNNs, and transformers\/attention combine the best of each. \n\n## NOTE: \nThis notebook easily gets out of memory error. In order to run without restarting jupyter notebook, choose one of the following: \n- Decreasing batch size until you can run both without CUDA OOM. \n- Deleting unneeded `learn`, etc. This is already done in this notebook (hopefully) to reduce memory wastage. However, this is not certain. Sometimes it takes time before python's internal program clear its cache, especially for GPU. Even `torch.cuda.empty_cache()` is not certain. It is unsure whether proper delete will make it certain. \n- Use a GPU with large Memory. Ideally, you need 18GB (predicted) to run properly. Better, get a GPU instance with 40GB or more, just to make sure it runs without human intervention. (You could rent one of the A6000 instances on [Datacrunch.io](https:\/\/datacrunch.io\/) for $1.10 per hour, and you can use jupyter notebook with fastai image already provided by DataCrunch, or [follow this guide to set it up yourself](https:\/\/wabinab.github.io\/2021\/06\/30\/Setting-up-Datacrunch.io-from-scratch.html) ).","571347d8":"It seems that we are unsure where does the **original** of the second value of `shift_tfm` should be assigned to. One doesn't know. However, if we check, then we know that `y[:, 1:]` is actually the original target. Hence, the target we did not modify, we just modify `self.learn.xb` so we can leave it without assigning it to anything. ","58a7d97c":"`div_factor` is now renamed as `div`. ","162ae76e":"### Feed Forward\nTwo linear layers with skip connection and LayerNorm. ","5de34f50":"### Label Smoothing\nHelped getting better BLEU\/accuracy, even if it made the loss worse. ","c168c12b":"### Embeddings\nThe input and output embeddings are traditional PyTorch embeddings (or pretrained vectors). Transformer model isn't recurrent, so it has no idea of relative positions of words. TO help with that, input embeddings a positional encoding used which is cosine of certain frequency. ","e7b8c109":"## Transformer Model\n### Shifting\nWe add transform to dataloader that shifts the targets right and adds a padding at the beginning. ","48a032bf":"## Masking\nThe attention layer uses a mask to avoid paying attention to certain timesteps. First thing is we don't really want network to pay attention to padding, so we're going to mask it. Second is model isn't recurrent, we need to mask (in output) all tokens we're not supposed to see yet (otherwise cheating). ","25197945":"Due to the prediction requires callback as well, we can't just use `learn.get_preds`. Instead, we have to write our own `get_predictions` now. ","217957c2":"### Multi-head attention","f167d32f":"It seems like too short a sequence length doesn't give good values here? (unsure) So we use the default sequence length. ","5b8234e9":"Due to how fastai v2 works differently, although we can modify the dls, however it doesn't give the expected output. In fact, `after_batch` in `Transform` only returns either `x` or `y` rather than a tuple of `(x, y)`, resulting in error. Therefore, what we could do is to instead do the transformations during Callback. ","0c4d934e":"Perhaps there are some problems with the data itself. ","142469d0":"### Encoder and Decoder Block\nRegroup layers in blocks. ","fbffc9c7":"### Training\nDue to the change of `compose` function, and change in PyTorch's `nn.ModuleList` function, the code above no longer works. We shall abandon here and see whom could solve this issue. As mentioned, new PyTorch's `nn.Transformer` does everything in one line of code, so in practice you don't need to write anything above anymore and just do that single line of code. \n\nOne tries with `nn.Transformer` and encounters 2 problems: \n- Unlike fastai, it doesn't allow you to use varying `seq_len`. Hence, it must pass the test `assert seq_len == d_model`. This isn't true in fastai dataloaders. \n- Our input is of type `torch.Long` (expected because it is the Numericalized text, which are all integers). However, `nn.Transformer` can only take in floats, and will raise \"adam backward not supported for `torch.Long`\" instead. \n\nSo, one doesn't know how to fix this. "}}