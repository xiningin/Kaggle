{"cell_type":{"cb38cd4c":"code","8f1b479e":"code","7b13b206":"code","a77900e9":"code","6692cdab":"code","64c466eb":"code","4a31986d":"code","7fc280cb":"code","4e5a9801":"code","2a1fed4f":"markdown","9d813b04":"markdown","5126b68b":"markdown","6dc1c15d":"markdown"},"source":{"cb38cd4c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8f1b479e":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport string","7b13b206":"data_train = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip')\nprint(data_train)\ndata_test = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip')\n\ntest_id = data_test['id']\ny_train = data_train.iloc[:,2:8]\ny_train","a77900e9":"StopWords = set(stopwords.words('english'))\n\ndef text_preprocess(text):\n    trans = str.maketrans('','',string.punctuation)\n    text = text.translate(trans)\n    text = ' '.join([word.lower() for word in text.split() if word.lower() not in StopWords])\n    return text\n\ndata_train['comment_text'] = data_train['comment_text'].apply(text_preprocess)\ndata_test['comment_text'] = data_test['comment_text'].apply(text_preprocess)\nX_train = data_train['comment_text']\nX_test = data_test['comment_text']\nprint(X_test.head())\nX_train.head()","6692cdab":"X_train = X_train.tolist()\nX_test = X_test.tolist()\n\ndef lemmatize(data):\n    lemmatizer = WordNetLemmatizer()\n    data_lemm = []\n    for text in data:\n        lem_text = ''\n        for word in text.split():\n            lem_word = lemmatizer.lemmatize(word)\n            lem_word = lemmatizer.lemmatize(lem_word, pos='v')\n            lem_text = lem_text + ' ' + lem_word\n        data_lemm.append(lem_text)\n    return data_lemm","64c466eb":"X_train_lemm = lemmatize(X_train)\nX_test_lemm = lemmatize(X_test)","4a31986d":"tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5, max_df=0.9)\nX_train_tfidf = tfidf.fit_transform(X_train_lemm)\nX_test_tfidf = tfidf.transform(X_test_lemm)","7fc280cb":"clf = OneVsRestClassifier(LogisticRegression(penalty='l2',C=1)).fit(X_train_tfidf, y_train)\nclf.predict(X_test_tfidf)\ny_pred = clf.predict_proba(X_test_tfidf)\nprint(y_pred)\ny_pred[:,0]","4e5a9801":"output_df = pd.DataFrame()\noutput_df['id'] = test_id\noutput_df['toxic'] = y_pred[:,0]\noutput_df['severe_toxic'] = y_pred[:,1]\noutput_df['obscene'] = y_pred[:,2]\noutput_df['threat'] = y_pred[:,3]\noutput_df['insult'] = y_pred[:,4]\noutput_df['identity_hate'] = y_pred[:,5]\nprint(output_df)\noutput_df.to_csv('Submission.csv', index=False)","2a1fed4f":"# I. Importing required libraries and data","9d813b04":"# II. Basic Text Preprocessing","5126b68b":"# IV. Training using Tfidf Vectorization","6dc1c15d":"# III. Lemmatization"}}