{"cell_type":{"419aa7ac":"code","0f4fe06a":"code","45f13f88":"code","5e9f9467":"code","75dac949":"code","582a418e":"code","cf154e63":"code","b4fe1b08":"code","dc793101":"markdown","2eb479ae":"markdown","a2f3082a":"markdown","b93efb22":"markdown","0ad6c8fa":"markdown","af238d99":"markdown"},"source":{"419aa7ac":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential,layers,Model","0f4fe06a":"def _make_divisible(ch, divisor=8, min_ch=None):\n    \"\"\"\n    This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by 8\n    \"\"\"\n    if min_ch is None:\n        min_ch = divisor\n    new_ch = max(min_ch, int(ch + divisor \/ 2) \/\/ divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_ch < 0.9 * ch:\n        new_ch += divisor\n    return new_ch\n\nclass ConvBNReLU(layers.Layer):\n    '''\u666e\u901a\u5377\u79ef\u5757Conv+BN+Relu'''\n    def __init__(self, out_channel, kernel_size=3, strides=1, **kwargs):\n        super(ConvBNReLU, self).__init__(**kwargs)\n        self.conv=layers.Conv2D(out_channel,kernel_size,strides,padding='SAME',use_bias=False, name='Conv2d')\n        self.bn  =layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name='BatchNorm')\n        self.relu=layers.ReLU(max_value=6.0)#Relu6\u6fc0\u6d3b\n    def call(self,x,training=False):\n        x=self.conv(x)\n        x=self.bn(x,training=training)\n        return self.relu(x)\n\nclass InvertedResidual(layers.Layer):\n    '''if use_shorcut>>>\u5012\u6b8b\u5dee\u7ed3\u6784\uff0c\u51481*1\u5347\u7ef4(channel)+3*3\u5377\u79ef+1*1\u964d\u7ef4\uff08channels)'''\n    def __init__(self, in_channel, out_channel, stride, expand_ratio, **kwargs):\n        super(InvertedResidual, self).__init__(**kwargs)\n        self.hidden_channel = in_channel * expand_ratio #1*1\u5347\u7ef4\u7684channel\u6570\n        #\u662f\u5426\u4f7f\u7528shortcut\u8fde\u63a5\uff1a\u8f93\u5165\u548c\u8f93\u51fachannel\u76f8\u540c\u4e14stride=1\u65f6\n        self.use_shortcut = stride == 1 and in_channel == out_channel\n\n        layer_list=[]\n        if expand_ratio != 1:\n            #\u5982\u679cexpand_ratio !=1, 1*1\u5347\u7ef4\n            layer_list.append(ConvBNReLU(self.hidden_channel,kernel_size=1,name='expand'))\n        #\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u5757 (depthwiseConv2D+BN+Relu)+(1*1Conv2D+bn+Linear_activation)\n        layer_list.extend([\n            layers.DepthwiseConv2D(kernel_size=3,strides=stride,padding='same',\n                                  use_bias=False, name='depthwise'),\n            layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name='depthwise\/BatchNorm'),\n            layers.ReLU(max_value=6.0),\n            ##########1*1Conv2D \u7ebf\u6027\u6fc0\u6d3b\n            layers.Conv2D(out_channel,kernel_size=1,strides=1,padding='SAME', \n                          use_bias=False, name='project'),\n            layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name='project\/BatchNorm')\n        ])\n        self.main_branch=Sequential(layer_list)\n    def call(self,x,training=False, **kwargs):\n        if self.use_shortcut:\n            #\u4f7f\u7528\u5012\u6b8b\u5dee\u7ed3\u6784\u5757\n            return x + self.main_branch(x,training=training)\n        else:\n            return self.main_branch(x,training=training)\n","45f13f88":"def MobileNetV2(im_height=224,im_width=224,num_classes=10,\n                alpha=1.0,#\u8c03\u8282\u5377\u79ef\u6838\u4e2a\u6570\u7684\u8d85\u53c2\u6570\uff0c\u9ed8\u8ba41\uff0c\u5177\u4f53\u4f5c\u7528\u5c1a\u4e0d\u660e\n                round_nearest=8,include_top=True):\n    input_channel = _make_divisible(32 * alpha, round_nearest)\n    last_channel = _make_divisible(1280 * alpha, round_nearest)\n    inverted_residual_setting = [\n        # t, c, n, s \u5012\u6b8b\u5dee\u7ed3\u6784\u914d\u7f6e\u8868\n        #t\u901a\u9053\u7684\u6269\u5f20\u7cfb\u6570\uff0cc\u4ee3\u8868channel\u6570\uff0cn\u4e3ablock\u91cd\u590d\u4e2a\u6570\uff0cs\u4e3astride\n        [1, 16, 1, 1],\n        [6, 24, 2, 2],\n        [6, 32, 3, 2],\n        [6, 64, 4, 2],\n        [6, 96, 3, 1],\n        [6, 160, 3, 2],\n        [6, 320, 1, 1],\n    ]\n    input_image=layers.Input(shape=(im_height, im_width, 3), dtype='float32')\n    \n    x = ConvBNReLU(input_channel, strides=2, name='Conv')(input_image)#kernel_size\u9ed8\u8ba4\u4e3a3\n    #\u6309\u7167config list\u4f9d\u6b21\u521b\u5efaInvertedResidual block\n    for t,c,n,s in inverted_residual_setting:\n        output_channel = _make_divisible(c * alpha, round_nearest)#\u8ba1\u7b97\u5404\u4e2a\u5b50\u5757\u8f93\u51fa\u901a\u9053\u6570\n        for i in range(n):\n            stride = s if i == 0 else 1#\u6bcf\u4e2a\u91cd\u590d\u7684\u5012\u6b8b\u5dee\u5757\uff0c\u53ea\u6709\u7b2c\u4e00\u4e2astride=s\uff0c\u5176\u4f59\u5747\u4e3a1\n            x=InvertedResidual(\n                in_channel=x.shape[-1],\n                out_channel=output_channel,\n                stride=stride,\n                expand_ratio=t)(x)\n    #\u5012\u6b8b\u5dee\u5757\u521b\u5efa\u5b8c\u6bd5\uff0c\u8f93\u51fa\u5c42\u521b\u5efa\n    x = ConvBNReLU(last_channel, kernel_size=1, name='Conv_1')(x)\n    if include_top is True:\n        # building classifier\n        x = layers.GlobalAveragePooling2D()(x)  # pool + flatten\n        x = layers.Dropout(0.2)(x)\n        output = layers.Dense(num_classes, name='Logits')(x)\n    else:\n        output = x\n    model = Model(inputs=input_image, outputs=output)\n    return model\nmy_model=MobileNetV2(im_height=128,im_width=128)\nmy_model.summary()","5e9f9467":"official_model=tf.keras.applications.MobileNetV2(\n    input_shape=(128,128,3), alpha=1.0,include_top=False, weights='imagenet',classifier_activation='softmax')\nfor layer in official_model.layers:\n    layer.trainable=False\n\noutput_part=Sequential([\n    layers.GlobalAveragePooling2D(),  # pool + flatten\n    layers.Dropout(0.2),\n    layers.Dense(10, name='Logits'),\n])\nfinal_outputs=output_part(official_model.output)\n  \nmodel=Model(inputs=official_model.input,outputs=final_outputs)\nmodel.summary()","75dac949":"(x_train,y_train),(x_test,y_test)=tf.keras.datasets.cifar10.load_data()\nx_train=x_train.astype('float32')\nx_test=x_test.astype('float32')\n\ndef process_img(img,label):\n    img=tf.image.resize(img,(128,128))\n    img\/=255.\n    return img,label\n\ntrain_set=tf.data.Dataset.from_tensor_slices((x_train[:20000],y_train[:20000])).map(process_img).shuffle(20000).batch(64)\ntest_set=tf.data.Dataset.from_tensor_slices((x_test[:5000],y_test[:5000])).map(process_img).batch(64)","582a418e":"model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\nhist=model.fit(train_set,epochs=15,validation_data=test_set)\n               #callbacks=[tf.keras.callbacks.TensorBoard('.\/logs')])","cf154e63":"plt.plot(hist.history['loss'],label='train loss')\nplt.plot(hist.history['val_loss'],label='validation loss')\nplt.legend()\nplt.title('loss curve')\nplt.show()\nplt.plot(hist.history['accuracy'],label='train accuracy')\nplt.plot(hist.history['val_accuracy'],label='validation accuracy')\nplt.legend()\nplt.title('accuracy curve')","b4fe1b08":"for x,y in test_set.take(1):\n    y_pred=model(x)\n    y_pred=tf.keras.layers.Softmax(axis=-1)(y_pred)\n    y_pred=tf.argmax(y_pred,axis=-1)\n    y_pred=tf.cast(y_pred,'uint8')\n\ndef pred_img(img,y_pred,y_true):\n    plt.figure(figsize=(20,20))\n    for i in range(x.shape[0]):\n        plt.subplot(8,8,i+1)\n        plt.imshow(img[i])\n        plt.title('pred label:{},\\nTrue label:{}'.format(y_pred[i],y_true[i]))\n        plt.axis('off')\npred_img(x,y_pred,y)    ","dc793101":"\u91c7\u7528\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u7684CNN\u7f51\u7edc","2eb479ae":"# keras\u5bfc\u5165\u7684\u5b98\u65b9\u5b9e\u73b0MobileNet\u6a21\u578b\uff08\u5e26\u9884\u8bad\u7ec3\u6743\u91cd\uff09","a2f3082a":"\u8bad\u7ec3\u7528\u5230\u7684\u6570\u636e\u96c6","b93efb22":"# \u642d\u5efaMobileNETV2","0ad6c8fa":"# \u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u7684\u5b9e\u73b0\u53ca\u7406\u89e3","af238d99":"model=Sequential([\n    #\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u53ef\u5206\u4e24\u6b65\u5b9e\u73b0\uff0cDepthwiseConv2D + 1*1Conv2D\n    layers.DepthwiseConv2D(3,1,padding='same',use_bias=False),#\u4ec5\u5b9e\u73b0\u7b2c\u4e00\u6b65\uff0cchannel\u5206\u79bb\uff0c\u5404\u81ea\u8fdb\u884c\u5377\u79ef\n    layers.Conv2D(16,1,1,padding='same',activation='relu'),#1*1\u666e\u901a\u5377\u79ef\uff0c\u5408\u5e76\u5404\u901a\u9053\u4fe1\u606f\n\n    #\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\uff0c\n    layers.SeparableConv2D(16,3,1,padding='same',activation='relu'),\n    \n    #\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u53c2\u6570\u91cf\u8fdc\u5c0f\u4e8e\u666e\u901a\u5377\u79eflayers.Conv2D\n    layers.Conv2D(16,3,1,padding='same',activation='relu')\n])\nx=tf.keras.Input(shape=(28,28,3))\nout=model(x)\nmodel.summary()"}}