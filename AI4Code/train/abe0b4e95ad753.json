{"cell_type":{"57de2da2":"code","49952122":"code","808ea021":"code","131096ca":"code","44e608b2":"code","e3065dca":"code","307373fc":"code","044e4051":"code","cf3d70ed":"code","cfb0396a":"code","b7f8caa2":"code","a7a843ac":"code","43b22b7c":"code","c647c6a8":"code","873aa78c":"code","8671610c":"code","92e62d66":"code","be5a896e":"code","1dda82aa":"code","9eed42de":"code","27d8e731":"code","bf80484a":"code","023ef96b":"code","85d9a0f9":"code","2bf7424a":"code","0dab7ca1":"code","3ebb3d0e":"code","0979e913":"code","fd34ba61":"code","cbcaf068":"code","b276541a":"code","5dff488a":"code","ccb8d362":"code","037d74db":"code","5220a5ec":"code","3d8b0cb7":"code","61699fe7":"code","ad5ad706":"code","5ce02d1c":"code","5d7f9268":"code","20af68d0":"code","26d06b99":"code","33d02a0b":"code","4025e95d":"code","c73488ca":"code","6f00629d":"code","9d89bd72":"code","46aa5c5e":"code","70b0de36":"code","e4404da3":"markdown","33d037e0":"markdown","c8efd56c":"markdown","0784beb5":"markdown","bb97678a":"markdown","28ca9686":"markdown","e65bb3f4":"markdown","1c5805e6":"markdown","b9c8c881":"markdown","cac1a945":"markdown","ee7a4e27":"markdown","ffc66558":"markdown","4b3746f2":"markdown","d0d02f3c":"markdown","c5e20d28":"markdown","f0a5e6b7":"markdown","75954238":"markdown","e90c9f23":"markdown","abfcc5b3":"markdown","297b0e6c":"markdown","a532515e":"markdown","ed151480":"markdown","d7df8b27":"markdown","6f2621f1":"markdown","cf685975":"markdown","f0e5da60":"markdown"},"source":{"57de2da2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Importing all the tools we need\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score,f1_score\nfrom sklearn.metrics import plot_roc_curve\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","49952122":"#Read data from csv\ndf = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")","808ea021":"#Get some rows\ndf.head()","131096ca":"#Check if everything is a number\ndf.info()","44e608b2":"# Are there any missing data?\ndf.isna().sum() ","e3065dca":"df[\"target\"].value_counts()","307373fc":"df[\"target\"].value_counts().plot(kind=\"bar\", color=[\"salmon\",\"lightblue\"]);","044e4051":" df.describe()","cf3d70ed":"# Create a plot of crosstab\npd.crosstab(df.target, df.sex).plot(kind=\"bar\",\n                                    figsize=(10,6),\n                                    color=[\"salmon\",\"lightblue\"])\nplt.title(\"Heart Disease Frequency for Sex\")\nplt.xlabel(\"0 = No Disease, 1 = Disease\")\nplt.legend([\"Female\",\"Male\"])\nplt.xticks(rotation=0);","cfb0396a":"# Create another figure\nplt.figure(figsize=(10, 6))\n\n# Scatter with positive example\nplt.scatter(df.age[df.target==1], df.thalach[df.target==1], color=\"salmon\")\n\n#Scatter with negative examples\nplt.scatter(df.age[df.target==0], df.thalach[df.target==0], color=\"lightblue\");\n\n# Add some helpful info\nplt.title(\"Heart Disease in function of Age and Max Heart Rate\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Max Hear Rate\")\nplt.legend([\"Disease\", \"No Disease\"]);","b7f8caa2":"# Check the distribution of the age column with histogram\ndf.age.plot.hist();","a7a843ac":"# Make a crosstab more visual\npd.crosstab(df.cp, df.target).plot(kind=\"bar\",\n                                  figsize=(10,6),\n                                  color=[\"salmon\",\"lightblue\"])\n\n#Add some communication\nplt.title(\"Heart Disease Frequency Per Chest Pain Type\")\nplt.xlabel(\"Chest Pain Type\")\nplt.ylabel(\"Amount\")\nplt.legend([\"Disease\",\"No Disease\"])\nplt.xticks(rotation=0);","43b22b7c":"#Create a correlation matrix\ncorr_matrix = df.corr()\nfig,ax=plt.subplots(figsize=(15,10))\nax = sns.heatmap(corr_matrix,\n                annot=True,\n                linewidths=0.5,\n                fmt=\".2f\",\n                cmap=\"YlGnBu\")","c647c6a8":"# Split data into X and y\nX = df.drop(\"target\",axis=1)\ny = df[\"target\"]","873aa78c":"#Split into train and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","8671610c":"# Put models in a dictionary\nmodels = {\"Logistic Regression\": LogisticRegression(),\n         \"KNN\": KNeighborsClassifier(),\n         \"Random Forest\": RandomForestClassifier()}\n\n# Create a function to fit and score models\n\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models: a dict of different Scikit-Learn machine learning models\n    X_train : training data (no labels)\n    X_test : testing data (no labels)\n    y_train : training labels\n    y_test : testing labels\n    \"\"\"\n    # Set random seed\n    np.random.seed(42)\n    \n    #Make a dictionary to keep model scores\n    model_scores={}\n    #Loop through models\n    for name, model in models.items():\n        #Fit the model to the data\n        model.fit(X_train,y_train)\n        #Evaluate the model and append its score to model_scores\n        model_scores[name] = model.score(X_test,y_test)\n    return model_scores","92e62d66":"model_scores = fit_and_score(models=models,\n                             X_train=X_train,\n                             X_test=X_test,\n                             y_train=y_train,\n                             y_test=y_test)\nmodel_scores","be5a896e":"model_compare = pd.DataFrame(model_scores, index=[\"accuracy\"])\nmodel_compare.T.plot.bar();","1dda82aa":"train_scores = []\ntest_scores = []\n\n# Create a list of different values for n_neighbors\nneighbors = range(1,40)\n\n#Setup KNN instance\nknn=KNeighborsClassifier()\n\n# Loop through different n_neighbors\nfor i in neighbors:\n    knn.set_params(n_neighbors=i)\n    \n    #Fit the algorithm\n    knn.fit(X_train,y_train)\n    \n    # Update the training scores list\n    train_scores.append(knn.score(X_train, y_train))\n    \n    #Update the test scores list\n    test_scores.append(knn.score(X_test, y_test ))","9eed42de":"plt.plot(neighbors, train_scores, label = \"train score\")\nplt.plot(neighbors, test_scores, label = \"test scores\")\nplt.xticks(np.arange(1,21,1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n\nprint(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")","27d8e731":"# Create a hyperparameter grid for Logistic regression\nlog_reg_grid = {\"C\": np.logspace(-4,4,20),\n               \"solver\": [\"liblinear\"]}\n\n# Create hyperparameter grid for RandomForestClassfier\nrf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n          \"max_depth\": [None, 3, 5, 10],\n          \"min_samples_split\": np.arange(2, 20, 2),\n          \"min_samples_leaf\": np.arange(1, 20, 2)}","bf80484a":"# Tune LogsticRegression\n\n# Setup random hyperparameter search for LogisticRegression\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=log_reg_grid, \n                                cv=5,\n                               n_iter=20,\n                               verbose = True)\n\n# Fit random hyperparametr search model for LogisticRegression\nrs_log_reg.fit(X_train,y_train)","023ef96b":"#best parameters are:\nrs_log_reg.best_params_","85d9a0f9":"#best accuracy score for Logistic regression\nrs_log_reg.score(X_test,y_test)","2bf7424a":"# Setup random hyperparameter search for RandomizedForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions=rf_grid,\n                           cv = 5,\n                           n_iter=20,\n                           verbose=True,\n                           n_jobs=-1)\n\n# Fit random Hyperparameter srach model for RandomForestClassifier()\nrs_rf.fit(X_train,y_train)","0dab7ca1":"#best parameters are:\nrs_rf.best_params_","3ebb3d0e":"# best accuracy score for RandomForestClassifier\nrs_rf.score(X_test,y_test)","0979e913":"# Different hyperparameters for our LogisticsRegression model\nlog_reg_grid = {\"C\": np.logspace(-4,4,30),\n               \"solver\": [\"liblinear\"]}\n\n#Setup grid hyperparameter search for LogisticRegression\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                         param_grid=log_reg_grid,\n                         cv=5,\n                         verbose=True)\n\n# Fit grid hyperparameter search model\ngs_log_reg.fit(X_train,y_train)","fd34ba61":"# Best parameters are:\ngs_log_reg.best_params_","cbcaf068":"# Evaluate the grid search LogisticRegression model\ngs_log_reg.score(X_test,y_test)","b276541a":"# Make predictions with tuned model\ny_preds = gs_log_reg.predict(X_test)","5dff488a":"# Plot ROC curve and calculate AUC metric\nplot_roc_curve(gs_log_reg, X_test,y_test)","ccb8d362":"sns.set(font_scale=1.5)\n\ndef plot_conf_mat(y_test, y_preds):\n    \"\"\"\n    Plots a nice looking confusion matrix using Seaborn's heatmap()\n    \"\"\"\n    fig, ax = plt.subplots (figsize=(3,3))\n    ax= sns.heatmap(confusion_matrix(y_test,y_preds),\n                   annot=True,\n                   cbar=False)\n    plt.ylabel(\"True label\")\n    plt.xlabel(\"Predicted label\")\n    \nplot_conf_mat(y_test,y_preds)","037d74db":"print(classification_report(y_test,y_preds))","5220a5ec":"# Check best params\ngs_log_reg.best_params_","3d8b0cb7":"#Making it dynamically, my best params is commented\nclf = LogisticRegression()\nclf.set_params(**gs_log_reg.best_params_)\n#clf = LogisticRegression(C = 0.38566204211634725,solver = \"liblinear\")\n","61699fe7":"cv_acc=cross_val_score(clf,\n                       X,y,\n                       cv=5,scoring=\"accuracy\"\n)\ncv_acc = np.mean(cv_acc)","ad5ad706":"cv_precision=cross_val_score(clf,\n                       X,y,\n                       cv=5,scoring=\"precision\"\n)\ncv_precision = np.mean(cv_precision)","5ce02d1c":"cv_recall=cross_val_score(clf,\n                       X,y,\n                       cv=5,scoring=\"recall\"\n)\ncv_recall = np.mean(cv_recall)","5d7f9268":"cv_f1=cross_val_score(clf,\n                       X,y,\n                       cv=5,scoring=\"f1\"\n)\ncv_f1 = np.mean(cv_f1)","20af68d0":"print(f\"Accuracy: {cv_acc*100:.2f}%   Overall, how often is the classifier correct?\")\nprint(f\"Recall (Sensitivity): {cv_recall*100:.2f}%    When it's actually yes, how often does it predict yes?\")\nprint(f\"Precision: {cv_precision*100:.2f}%    When it predicts yes, how often is it correct?\")\nprint(f\"F1 score: {cv_f1*100:.2f}%    good F1 score means that you have low false positives and low false negatives\")\n","26d06b99":"# Visualize cross-validated metrics\ncv_metrics = pd.DataFrame({\"Accuracy\":cv_acc,\n                          \"Precision\":cv_precision,\n                          \"Recall\": cv_recall,\n                          \"F1 score\": cv_f1},\n                         index=[0])\ncv_metrics.T.plot.bar(title=\"Cross-validated metrics\", legend=False);","33d02a0b":"clf.fit(X_train,y_train)","4025e95d":"df.head()","c73488ca":" clf.coef_","6f00629d":"# Match coef's of features to columns\nfeature_dict = dict (zip(df.columns, list(clf.coef_[0])))\nfeature_dict","9d89bd72":"# Visualize feature importance\nfeature_df = pd.DataFrame(feature_dict, index=[0])\nfeature_df.T.plot.bar(title=\"feature Importance\", legend=False);","46aa5c5e":"# Helper function for plotting feature importance\ndef plot_features(columns, importances, n=20):\n    df = (pd.DataFrame({\"features\": columns,\n                       \"feature_importances\": importances})\n          .sort_values(\"feature_importances\",ascending = False)\n          .reset_index(drop=True))\n    fig, ax =plt.subplots()\n    ax.barh(df[\"features\"][:n], df[\"feature_importances\"][:20])\n    ax.set_ylabel(\"Features\")\n    ax.set_xlabel(\"Feature importance\")\n    ax.invert_yaxis()","70b0de36":"# another visualization\nplot_features(X_train.columns, clf.coef_[0])","e4404da3":"2. Correlation coefficient and feature importance for cholesterol is non existent.\n\nThat's a surprising finding, every media is saying there is a correlation between cholesterol levels and heart diseases.","33d037e0":"Confusion matrix","c8efd56c":"## 1. Definition\nSee if you can find any other trends in heart data to predict certain cardiovascular events or find any clear indications of heart health.\n## 2. Data\nTaken from https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\n## 3. Features\n\nThis is where you'll get different information about each of the features in your data.\n\n**Create data dictionary**\n\n1. age - age in years\n2. sex - (1 = male; 0 = female)\n3. cp - chest pain type\n    * 0: Typical angina: chest pain related decrease blood supply to the heart\n    * 1: Atypical angina: chest pain not related to heart\n    * 2: Non-anginal pain: typically esophageal spasms (non heart related)\n    * 3: Asymptomatic: chest pain not showing signs of disease\n4. trestbps - resting blood pressure (in mm Hg on admission to the hospital) anything above 130-140 is typically cause for concern\n5. chol - serum cholestoral in mg\/dl\n    * serum = LDL + HDL + .2 * triglycerides\n    * above 200 is cause for concern\n6. fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n    * '>126' mg\/dL signals diabetes\n7. restecg - resting electrocardiographic results\n    * 0: Nothing to note\n    * 1: ST-T Wave abnormality\n        * can range from mild symptoms to severe problems\n        * signals non-normal heart beat\n    * 2: Possible or definite left ventricular hypertrophy\n        * Enlarged heart's main pumping chamber\n8. thalach - maximum heart rate achieved\n9. exang - exercise induced angina (1 = yes; 0 = no)\n10. oldpeak - ST depression induced by exercise relative to rest looks at stress of heart during excercise unhealthy heart will stress more\n11. slope - the slope of the peak exercise ST segment\n    * 0: Upsloping: better heart rate with excercise (uncommon)\n    * 1: Flatsloping: minimal change (typical healthy heart)\n    * 2: Downslopins: signs of unhealthy heart\n12. ca - number of major vessels (0-3) colored by flourosopy\n    * colored vessel means the doctor can see the blood passing through\n    * the more blood movement the better (no clots)\n13. thal - thalium stress result\n    * 1,3: normal\n    * 6: fixed defect: used to be defect but ok now\n    * 7: reversable defect: no proper blood movement when excercising\n14. target - have disease or not (1=yes, 0=no) (= the predicted attribute)","0784beb5":"# Tuning","bb97678a":"## Hyperparameter Tuning with GridSearchCV\n\n","28ca9686":"## Results (as a non expert in field):","e65bb3f4":"3. There is a positive correlation between cp (chest pain) and heart disease\n\ncp - chest pain type\n  * 0: Typical angina: chest pain related decrease blood supply to the heart\n  * 1: Atypical angina: chest pain not related to heart\n  * 2: Non-anginal pain: typically esophageal spasms (non heart related)\n  * 3: Asymptomatic: chest pain not showing signs of disease\n\nSo, basically, if there is no chest pain, it's more likely patient have a heart disease. Which can be a sign for dataset problem.","1c5805e6":"Cross validated recall","b9c8c881":"## Calculate evaluation metrics using cross-validation","cac1a945":"Classification report","ee7a4e27":"Cross validated precision","ffc66558":"We're going to try 3 different machine learning models:\n1. Logistic regression\n2. K-Nearest Neighbours Classifier\n3. Random Forest Classifier","4b3746f2":"# Data exploration","d0d02f3c":"1. Dataset has strange Heart Disease Frequency according to Sex distribution\n\nIf the patient is female, she has a higher chances of heart disease.\n\nIs it a dataset problem? Or women generally go to hospitals only with serious pains?","c5e20d28":"## Feature importance","f0a5e6b7":"Cross validated accuracy","75954238":"ROC curve and AUC metric","e90c9f23":"## Checking accuracy based on different parameters for KNN","abfcc5b3":"Tune RandomizedForestClassifier","297b0e6c":"Results: KNN is staying behind. We will check how accuracy changes with a little tweaking. If it's not increasing, KNN will be droppped for further tuning.","a532515e":"Cross validated f1-score","ed151480":"## Trying to get better parameters for LogisticRegression() and RandomForestClassfier() using RandomizedGridCV","d7df8b27":"# Evaluating our tuned machine learning classifier, beyond accuracy\n\nAfter acuring best parameters for LogisticRegression. We will evaluate following metrics:\n* ROC curve and AUC score\n* Confusion matrix\n* Classification report\n* Precision\n* Recall\n* F1-score","6f2621f1":"# Modelling","cf685975":"Results: Since our LogisticRegression model provides the best scores so far, we'll try and imporve them again using GfridSearchCV","f0e5da60":"Tune LogsticRegression"}}