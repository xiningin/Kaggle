{"cell_type":{"4d111cec":"code","80508eb4":"code","4cef0260":"code","22d3a22f":"code","cc4ed523":"code","55a059e3":"code","b65e8ad8":"code","a9cb30e4":"code","f948196d":"code","c892d266":"code","425abdbf":"code","9c5f792d":"code","99c3411d":"code","8305caf8":"code","f91812d6":"code","d1a23082":"code","d0a0db42":"code","a4d593a8":"code","d5af32aa":"markdown","e14151f4":"markdown","dad8a891":"markdown","0ac7b26f":"markdown","1aa82e37":"markdown","01a63d9f":"markdown","b9bb50f0":"markdown"},"source":{"4d111cec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","80508eb4":"data=pd.read_csv(\"\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")\ndata.rename(columns={\"pelvic_tilt numeric\":\"pelvic_tilt_numeric\"},inplace=True)\ndata.info()","4cef0260":"# \ndata[\"class\"]=[1 if each==\"Abnormal\" else 0 for each in data[\"class\"]]\nx_data=data.drop(columns=[\"class\"])\nx=(x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\ny=data[\"class\"].values\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\nbest_score_all=[] #Storing inside all method's best scores. ","22d3a22f":"# General view of data\ncolor_list = ['red' if i==1 else 'green' for i in data[\"class\"]]\npd.plotting.scatter_matrix(x, alpha=0.7,figsize=(20,20),c=color_list)\nplt.show()\n","cc4ed523":"N=data[data[\"class\"]==0]\nA=data[data[\"class\"]==1]\nfig = plt.figure(\"degree_spondylolisthesis & pelvic_incidence\")\nplt.scatter(N.pelvic_incidence,N.degree_spondylolisthesis,color=\"green\",alpha=0.5,label=\"Normal\")\nplt.scatter(A.pelvic_incidence,A.degree_spondylolisthesis,color=\"red\",alpha=0.5,label=\"Abnormal\")\nplt.show()","55a059e3":"#%% KNN-Trying simle test with pelvic_incidence and degree_spondylolisthesis\n# score:  0.8225806451612904\npi_ds=x.drop(columns=[\"pelvic_tilt_numeric\",\"lumbar_lordosis_angle\",\"sacral_slope\",\"pelvic_radius\"])\npi_ds_train,pi_ds_test,y_train3,y_test3=train_test_split(pi_ds,y,test_size=0.2,random_state=42)\nlist_of_knn=[]\nfor each in range(1,50):\n    knn=KNeighborsClassifier(n_neighbors=each)\n    knn.fit(pi_ds_train,y_train3)\n    list_of_knn.append(knn.score(pi_ds_test,y_test3))\nplt.scatter(range(1,50),list_of_knn)\nplt.show()","b65e8ad8":"# KNN=5 is best one    \nknn2=KNeighborsClassifier(n_neighbors=(list_of_knn.index(max(list_of_knn))+1))\nknn2.fit(pi_ds_train,y_train3)\nprint(\"score: \",knn2.score(pi_ds_test,y_test3))","a9cb30e4":"#%% KNN-General usage (Main Method)\n# score:  0.8064516129032258\nlist_of_KNN=[]\nfor each in range(1,50):\n    KNN=KNeighborsClassifier(n_neighbors=each)\n    KNN.fit(x_train,y_train)\n    list_of_KNN.append(KNN.score(x_test,y_test))\n  \nKNN=KNeighborsClassifier(n_neighbors=(list_of_KNN.index(max(list_of_KNN))+1))\nKNN.fit(x_train,y_train)\nprint(\"score: \",KNN.score(x_test,y_test))","f948196d":"#%% KNN- Testing all columns correlation between them and return best score\n# Better than main method\n# score of degree_spondylolisthesis and pelvic_radius:  0.8709677419354839\nlist_of_columns=[]\nlast_score_knn=[]\nfor col in x.columns:\n    list_of_columns.append(col)\n    \ndef kkn(x,y):\n    if x.columns[0]==x.columns[1]:\n        return\n    else:    \n        x_train_s,x_test_s,y_train_s,y_test_s=train_test_split(x,y,test_size=0.2,random_state=42)\n        list_of_knn_s=[]\n        for each in range(1,50):\n            knn_s=KNeighborsClassifier(n_neighbors=each)\n            knn_s.fit(x_train_s,y_train_s)\n            list_of_knn_s.append(knn_s.score(x_test_s,y_test_s))\n        if list_of_knn_s.index(max(list_of_knn_s))== 0:\n            return\n        else :    \n            knn_s2=KNeighborsClassifier(n_neighbors=(list_of_knn_s.index(max(list_of_knn_s))+1))\n            knn_s2.fit(x_train_s,y_train_s)\n            print(\"score of {} and {}: \".format(x.columns[0],x.columns[1]))\n            print(\"\",knn_s2.score(x_test_s,y_test_s))\n            print(\"\")\n            return knn_s2.score(x_test_s,y_test_s)\n    \nfor each in list_of_columns:\n    for beach in list_of_columns:\n        concat=pd.concat([x[each],x[beach]],axis=1)\n        last_score_knn.append(kkn(concat,y))\nlast_score_knn = [x for x in last_score_knn if str(x) != 'None']\nprint(\"Best Score: \",max(last_score_knn))\nbest_score_all.append(max(last_score_knn))","c892d266":"#%% These are give us best score from KNN algorithm and let's see\nfig = plt.figure(\"degree_spondylolisthesis & sacral_slope\")\nplt.scatter(N.degree_spondylolisthesis,N.sacral_slope,color=\"green\",alpha=0.5,label=\"Normal\")\nplt.scatter(A.degree_spondylolisthesis,A.sacral_slope,color=\"red\",alpha=0.5,label=\"Abnormal\")\nplt.show()","425abdbf":"#%% NAIVE BAYES-- I will try columns which has giv better score on KNN algorithm\n# score: 0.7903225806451613\nss_ds=x.drop(columns=[\"pelvic_tilt_numeric\",\"lumbar_lordosis_angle\",\"pelvic_incidence\",\"pelvic_radius\"])\nss_ds_train,ss_ds_test,y_train4,y_test4=train_test_split(ss_ds,y,test_size=0.2,random_state=42)\nnb=GaussianNB()\nnb.fit(ss_ds_train,y_train4)\nprint(\"score:\",nb.score(ss_ds_test,y_test4))\n","9c5f792d":"#%% NAIVE BAYES--General usage (Main Method)\n# score: 0.782258064516129\nnb2=GaussianNB()\nnb2.fit(x_train,y_train)\nprint(\"score:\",nb2.score(x_train,y_train))","99c3411d":"#%% I'm using here my own code for the get best score\n# And again it works better than main method\n# score of degree_spondylolisthesis and pelvic_radius: 0.8225806451612904\nlast_score_naive=[]\ndef naive(x,y):\n    if x.columns[0]==x.columns[1]:\n        return\n    else:    \n        x_train_s,x_test_s,y_train_s,y_test_s=train_test_split(x,y,test_size=0.2,random_state=42)\n        nb3=GaussianNB()\n        nb3.fit(x_train_s,y_train_s)\n        print(\"score of {} and {}: \".format(x.columns[0],x.columns[1]))\n        print(\"\",nb3.score(x_test_s,y_test_s))\n        print(\"\")\n        return nb3.score(x_test_s,y_test_s)\n\nfor each in list_of_columns:\n    for beach in list_of_columns:\n        concat=pd.concat([x[each],x[beach]],axis=1)\n        last_score_naive.append(naive(concat,y))\nlast_score_naive = [x for x in last_score_naive if str(x) != 'None']\nprint(\"Best Score: \",max(last_score_naive))\nbest_score_all.append(max(last_score_naive))","8305caf8":"#Logistic Regression model is the same score with main method\n# Score: 0.7741935483870968\nlast_score_logreg=[]\ndef logistic(x,y):\n    if x.columns[0]==x.columns[1]:\n        return\n    else:    \n        x_train_s,x_test_s,y_train_s,y_test_s=train_test_split(x,y,test_size=0.2,random_state=42)\n        lr=LogisticRegression()\n        lr.fit(x_train_s,y_train_s)\n        print(\"score of {} and {}: \".format(x.columns[0],x.columns[1]))\n        print(\"\",lr.score(x_test_s,y_test_s))\n        print(\"\")\n        return lr.score(x_test_s,y_test_s)\n\nfor each in list_of_columns:\n    for beach in list_of_columns:\n        concat=pd.concat([x[each],x[beach]],axis=1)\n        last_score_logreg.append(logistic(concat,y))\nlast_score_logreg = [x for x in last_score_logreg if str(x) != 'None']\nprint(\"Best Score: \",max(last_score_logreg))\nbest_score_all.append(max(last_score_logreg))","f91812d6":"# Logistic Regression-- General usage (Main method)\n# score: 0.7741935483870968\nlr2 = LogisticRegression()\nlr2.fit(x_train,y_train)\nprint(\"test accuracy {}\".format(lr2.score(x_test,y_test)))\n","d1a23082":"#%% I'm using here my own code for the get best score\n# And again it works better than main method\n# score of degree_spondylolisthesis and pelvic_radius: 0.8387096774193549\nlast_score_SVC=[]\ndef svc(x,y):\n    if x.columns[0]==x.columns[1]:\n        return\n    else:    \n        x_train_s,x_test_s,y_train_s,y_test_s=train_test_split(x,y,test_size=0.2,random_state=42)\n        svm=SVC(random_state=42)\n        svm.fit(x_train_s,y_train_s)\n        print(\"score of {} and {}: \".format(x.columns[0],x.columns[1]))\n        print(\"\",svm.score(x_test_s,y_test_s))\n        print(\"\")\n        return svm.score(x_test_s,y_test_s)\n\nfor each in list_of_columns:\n    for beach in list_of_columns:\n        concat=pd.concat([x[each],x[beach]],axis=1)\n        last_score_SVC.append(svc(concat,y))\nlast_score_SVC = [x for x in last_score_SVC if str(x) != 'None']\nprint(\"Best Score: \",max(last_score_SVC))\nbest_score_all.append(max(last_score_SVC))","d0a0db42":"#%% SVM--General usage (Main method)\n#score: 0.8064516129032258\nsvm2=SVC(random_state=42)\nsvm2.fit(x_train,y_train)\nprint(\"score: \",svm2.score(x_test,y_test)) ","a4d593a8":"# Visualizing all scores\nstr_models=[\"knn_model\",\"nb_model\",\"log_reg\",\"svm_tuned_model\"]\ndf=pd.DataFrame({\"Models\": str_models,\"Accuracy_Score\":best_score_all})\ndf.sort_values(by=\"Accuracy_Score\",ascending=False)\ndf.reset_index(drop=True)\n\nplt.figure(figsize=(10,10))\nsns.barplot(x=\"Accuracy_Score\",y=\"Models\",data=df,orient=\"h\")\nplt.xticks(rotation=90)\nplt.grid() ","d5af32aa":"Than want to see general usage of KNN algorithm. When i get score from general usage i see it's lower than which i found. That wan't correct and decided again for see all correlation between all columns.","e14151f4":"Here is my own algorithm. It's get from x.columns, all columns names and it provided the opportunity see everything. In knn function splitting for train and test variable. In for loop looking for best number for n_neigbours. Use it in KNeighborsClassifier(n_neighbors) got the score and append in the last_score_knn. End of the code we get inside of last_score_knn highest score and append it again best_score_all list. I used this algorithm for all methods for see my code or general usage is givin better score. And i see my code better for all except linear regression. General usage and my code return same score.","dad8a891":"Using scatter_matrix show us all columns correlation with each other.","0ac7b26f":"pelvic_incidence and degree_spondylolisthesis columns seems to me they have great correlation and i want to use it. Let's Look at that first. \n","1aa82e37":"Finally it's time to see which method good for this data. You can see the table below.","01a63d9f":"We read our csv from file and ready to start. As we see class columns is obejct. It has to change for the prepare in machine learning methods. We would set x_data and y variables. For not make mistake i just normalized x_data and it became x variable. They are ready to use.","b9bb50f0":"After visualization decided to use KNN algorithm. Did choose the correct columns that what i want to see. Dropped all columns except for pelvic_incidence and degree_spondylolisthesis. Splitted pi_ds for get train and test variables. In the for loop i searced best n_neigbour variable and used for train my data and test it for best score."}}