{"cell_type":{"353ced40":"code","0f43076d":"code","33507c78":"code","e4d21ccc":"code","8914496b":"code","18d5040f":"code","647dab50":"code","07c86418":"code","b8c9af90":"code","69177924":"code","52b6ddbf":"code","8602dc51":"code","ac351069":"code","24f8d29e":"code","791a1faf":"code","beb50c10":"code","df299dd0":"code","f10f8aa4":"code","bd0faf2c":"code","e32a8c5f":"code","7013e039":"code","52e3b156":"code","e191b3ad":"code","0ca6847a":"code","a48f4721":"code","5d292335":"code","f1dad9c4":"code","c296ded7":"code","5b47219f":"code","594f480f":"markdown","2b415d43":"markdown","4ea728dd":"markdown","0df7f919":"markdown","6cc51ec4":"markdown","3d84d29f":"markdown","8d5e6feb":"markdown","b2662e7b":"markdown","3c2732b6":"markdown"},"source":{"353ced40":"# !pip3 install git+https:\/\/github.com\/Kaggle\/kaggle-api.git --upgrade","0f43076d":"!pip3 install jovian --quiet","33507c78":"def setup_local(kaggle_creds='kaggle.json'):\n    import os\n    import json\n    with open(kaggle_creds, 'r') as f:\n        creds = json.loads(f.read())\n    os.environ['KAGGLE_USERNAME']=creds['username']\n    os.environ['KAGGLE_KEY']=creds['key']\n    os.system('kaggle datasets download -d so2-emissions-daily-summary-data')\n    os.system('unzip so2-emissions-daily-summary-data.zip')\n    return ''\n\n# provide a kaggle.json credentials and\n# uncomment the line if you run localy\n#\n# data_dir = setup_local()","e4d21ccc":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.dataset import random_split\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nimport jovian\nimport geopandas as gpd\nimport folium\nfrom folium.plugins import HeatMap\nfrom tqdm import tqdm\n#from category_encoders import Label\n# jovian.commit(project=project_name)\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (15, 5);","8914496b":"project_name = 'air-pollution-case-p2'\nauthor = 'Valentyna Fihurska'","18d5040f":"try:\n    data_dir\nexcept Exception:\n    data_dir = '..\/input\/so2-emissions-daily-summary-data\/'\npol_df = pd.read_csv(os.path.join(data_dir, 'daily_42401_2017\/daily_42401_2017.csv'))\np2018 = pd.read_csv(os.path.join(data_dir, 'daily_42401_2018.csv'))\np2019 = pd.read_csv(os.path.join(data_dir, 'daily_42401_2019.csv'))\npol_df = pol_df.loc[(pol_df['County Code']==101) & (pol_df['State Code']==42)]\np2018 = p2018.loc[(p2018['County Code']==101) & (p2018['State Code']==42)]\np2019 = p2019.loc[(p2019['County Code']==101) & (p2019['State Code']==42)]\npol_df = pd.concat([pol_df, p2018, p2019], axis=0)\ndel p2018, p2019","647dab50":"to_drop = ['State Code', 'County Code', 'Site Num', \n           'Parameter Code', 'Latitude', 'Longitude',\n          'Parameter Name', 'Sample Duration', \n           'Pollutant Standard']\ndf = pol_df.drop(to_drop, axis=1)\ndf.drop(['Datum', 'Units of Measure', 'Event Type', \n         'State Name', 'County Name', 'City Name',\n        'CBSA Name'], axis=1, inplace=True)\ndf.loc[:, 'Date Local'] = pd.to_datetime(df['Date Local'])\ndf.set_index('Date Local', inplace=True)\ndf = df.loc[(df['Address']==df['Address'].unique()[0]) & (\n    df['Method Name']==df['Method Name'].unique()[0])]\ndf.drop(['Address', 'Method Name', 'Method Code', 'Local Site Name', \n         'Date of Last Change', 'POC', 'Observation Count', \n         'Observation Percent', '1st Max Hour'], axis=1, inplace=True)\ndf","07c86418":"def save_n_commit(model, metrics=None):\n    \n    kw = ['arch', 'epochs', 'lr', \n          'scheduler', 'weight_decay', \n          'grad_clip', 'opt', 'val_loss',\n          'val_score', 'train_loss', \n          'train_time', 'loss_func']\n    \n    if metrics:\n        for k in kw:\n            try:\n                metrics[k]\n            except Exception:\n                metrics[k] = None\n    else:\n        metrics = {k: None for k in kw}\n    \n    weights_fname = '{}-{}ep-{}lr.pth'.format(metrics['arch'],\n                                              metrics['epochs'],\n                                              metrics['lr'])\n    torch.save(model.state_dict(), weights_fname)\n    jovian.reset()\n\n    jovian.log_hyperparams(arch=metrics['arch'], \n                           epochs=metrics['epochs'], \n                           lr=metrics['lr'], \n                           scheduler=metrics['scheduler'], \n                           weight_decay=metrics['weight_decay'], \n                           grad_clip=metrics['grad_clip'],\n                           opt=metrics['opt'],\n                           val_loss=metrics['val_loss'],\n                           val_score=metrics['val_score'],\n                           train_loss=metrics['train_loss'],\n                           train_time=metrics['train_time'],\n                          loss_func=metrics['loss_func'])\n    \n    jovian.commit(project=project_name, \n                  environment=None, \n                  outputs=[weights_fname])\n    return True","b8c9af90":"def get_sequences(df, seq_len):\n    xs = []\n    ys = []\n    for i in range(len(df)-seq_len-1):\n        x = df[i:(i+seq_len)].values\n        y = df.loc[df.index[i+seq_len]]#.values\n        xs.append(x)\n        ys.append(y)\n\n    return np.array(xs), np.array(ys)","69177924":"def plot_dist(df):\n    fig, ax = plt.subplots(1, len(df.columns))\n    for i, col in zip(range(len(df.columns)), \n                      df.columns):\n        sns.distplot(df[col], ax=ax[i], axlabel=col)\n\nplot_dist(df)","52b6ddbf":"def log_scale(df, invert=False):\n    for col in df.columns:\n        if invert:\n            df.loc[:, col] = np.exp(df[col]) #- 1\n            continue\n        df.loc[:, col] = np.log1p(df[col])\n    return df","8602dc51":"class PollutionDataset(Dataset):\n    \n    def __init__(self, frame, window=14):\n        super().__init__()\n        self.frame = frame\n        self.window = window\n        self.x, self.y = get_sequences(self.frame,\n                            seq_len=self.window)\n        \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, ind):\n        x = np.array(self.x[ind])\n        y = np.array(self.y[ind])\n        return torch.tensor(x), torch.tensor(y)\n    \n    def stationary_conversion(self):\n        self.frame = self.frame.diff()\n        print(self.frame)\n    \n    def scale_features(self, scaler='custom'):\n        if scaler == 'custom':\n            scalers = self.get_scalers()\n            for i in range(len(self.frame.columns)):\n                col = self.frame.columns[i]\n                self.frame.loc[:, col] = self.frame[col]\/scalers[i]\n            return\n        scalers = []\n        for col in self.frame.columns:\n            if scaler=='standart':\n                sc = StandardScaler()\n            if scaler=='minmax':\n                sc = MinMaxScaler()\n            sc.fit(self.frame[col].values.reshape(-1, 1))\n            self.frame.loc[:, col] = sc.transform(\n                self.frame[col].values.reshape(-1, 1))\n            scalers.append(sc)\n        self.scalers = scalers\n        \n    def get_scalers(self):\n        scalers = list()\n        scalers.append(self.frame[self.frame.columns[0]].values.max())\n        scalers.append(self.frame[self.frame.columns[1]].values.max())\n        scalers.append(self.frame[self.frame.columns[2]].values.max())\n        self.scalers = scalers\n        return self.scalers\n        ","ac351069":"train_size = int(0.85*len(df))\ntrain_set = PollutionDataset(df[:train_size])\nval_set = PollutionDataset(df[train_size:])","24f8d29e":"class BaseModel(nn.Module):\n    \n    def training_step(self, x, y, loss_f=F.mse_loss):\n        x, y = x.float(), y.float()\n        out = self(x)\n        #print(out)\n        loss = loss_f(out, y)      \n        return loss\n    \n    def validation_step(self, x, y, loss_f=F.mse_loss):\n        x, y = x.float(), y.float()\n        out = self(x)\n        loss = loss_f(out, y)\n        return {'val_loss': loss.detach() }\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()\n        return {'val_loss': epoch_loss.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], last_lr: {}, train_loss: {:.4f}, val_loss: {:.4f}\".format(\n            epoch, result['lrs'][-1], result['train_loss'], result['val_loss']))","791a1faf":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","beb50c10":"class StatefulModel(BaseModel):\n    \n    def __init__(self, n_features, n_hidden, seq_len=14, \n                 n_layers=2, out_features=1):\n        super().__init__()\n        self.n_features = n_features\n        self.n_hidden = n_hidden\n        self.seq_len = seq_len\n        self.n_layers = n_layers\n        self.init_hidden()\n        self.lstm1 = nn.LSTM(n_features, \n                            n_hidden, \n                            n_layers,\n                            dropout=0.5)\n        self.lstm2 = nn.LSTM(n_hidden, \n                            n_hidden, \n                            n_layers,\n                            dropout=0.5)\n        self.linear = nn.Linear(n_hidden, out_features)\n    \n    def init_hidden(self):\n        self.hidden = (torch.zeros(self.n_layers, self.seq_len, self.n_hidden),\n                       torch.zeros(self.n_layers, self.seq_len, self.n_hidden))\n    \n    def forward(self, x):\n        x = x.view(1, self.seq_len, -1).float()\n        #print(x.view(1, self.seq_len, -1))\n        lstm_out, self.hidden = self.lstm1(x, self.hidden)\n        lstm_out, self.hidden = self.lstm2(lstm_out, \n                                           self.hidden)\n        last_time_step = lstm_out.view(\n            self.seq_len, len(x), self.n_hidden)[-1]\n        #last_time_step = torch.flatten(last_time_step)\n        x = self.linear(last_time_step)\n        #print(x)\n        self.hidden = self.hidden[0].detach(), self.hidden[1].detach()\n        return torch.flatten(x)","df299dd0":"def get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef fit(model, train_set, val_set, subset, epochs=10, \n        lr=1e-4, loss_f=F.mse_loss, save=False):\n    \n    optimizer = Adam(model.parameters(), lr=lr, weight_decay=1e-06)\n    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, \n                                                 base_lr=lr, \n                                                 max_lr=1e-6, \n                                                 step_size_up=int(len(train_set)\/2),\n                                                 step_size_down=int(len(train_set)\/2),\n                                                 cycle_momentum=False)\n    model.train()\n    outputs = []\n    for epoch in range(epochs):\n        total_loss = []\n        lrs = []\n        for each in tqdm(train_set):\n            x, y = each\n            x = x.T[subset]\n            #x = x.view(1, 30, -1)\n            y = y[subset].view(1, )\n            #print(x, y)\n            model.zero_grad()\n            loss = model.training_step(x, y)\n            loss.backward()\n            optimizer.step()\n            scheduler.step()\n            lrs.append(get_lr(optimizer))\n            total_loss.append(loss.item())\n            optimizer.zero_grad()\n            \n        t_loss = np.array(total_loss).mean()\n        print('Epoch: {}, loss: {}, last lr: {}'.format(\n            epoch, t_loss, lrs[-1]))\n    if save:\n        save_n_commit(model, metrics={'arch': str(model),\n                                     'epochs': epochs,\n                                      'last_lr': lrs[-1],\n                                     'loss_func': loss_f.__name__,\n                                     'opt': str(optimizer),\n                                     'train_loss': t_loss})\n    return model","f10f8aa4":"model_am = StatefulModel(1, 512)\nmodel_mv = StatefulModel(1, 512)\nmodel_aqi = StatefulModel(1, 512)","bd0faf2c":"fit(model_am, train_set, val_set, 0, save=True)\nfit(model_mv, train_set, val_set, 1, save=True)\nfit(model_aqi, train_set, val_set, 2, save=True)","e32a8c5f":"def evaluate(model, val_set, subset, return_preds=True):\n    #x, y = val_set\n    diviance = []\n    model.eval()\n    if return_preds:\n        preds = []\n    for each in tqdm(val_set):\n        x, y = each\n        x = x.T[subset].float()\n        y = torch.tensor(y[subset]).view(1, )\n        out = model(x).detach().item()\n        if return_preds:\n            preds.append(out)\n        diviance.append(np.abs(out-y))\n    mean_div = np.array(diviance).mean()\n\n    if return_preds:\n        return mean_div, np.array(preds)\n    return mean_div","7013e039":"mean_div_am, preds_am = evaluate(\n    model_am, val_set, 0)\nmean_div_mv, preds_mv = evaluate(\n    model_mv, val_set, 1)\nmean_div_aqi, preds_aqi = evaluate(\n    model_aqi, val_set, 2)\npreds_am","52e3b156":"true_am = np.array([val_set[i][1][0] for i in range(len(val_set))])\ntrue_mv = np.array([val_set[i][1][1] for i in range(len(val_set))])\ntrue_aqi = np.array([val_set[i][1][2] for i in range(len(val_set))])","e191b3ad":"for i in range(1, len(preds_am)):\n    print('true:', true_am[i-1], '\\tpredicted:', preds_am[i])\n    if i > 10:\n        break","0ca6847a":"plt.plot(true_am, label='True parts per billion')\nplt.plot(preds_am, label='Predicted parts per billion')\nplt.legend()\nplt.xticks([]);","a48f4721":"for i in range(1, len(preds_mv)):\n    print('true:', true_mv[i-1], '\\tpredicted:', preds_mv[i])\n    if i > 10:\n        break","5d292335":"plt.plot(true_mv, label='True maximum parts per billion')\nplt.plot(preds_mv, label='Predicted maximum parts per billion')\nplt.legend()\nplt.xticks([]);","f1dad9c4":"for i in range(1, len(preds_am)):\n    print('true:', true_aqi[i-1], '\\tpredicted:', preds_aqi[i])\n    if i > 10:\n        break","c296ded7":"plt.plot(true_aqi, label='True AQI')\nplt.plot(preds_aqi, label='Predicted AQI')\nplt.legend()\nplt.xticks([]);","5b47219f":"import jovian\njovian.commit(project=project_name, environment=None)","594f480f":"I decided to build LSTM model and here what's new to it:\n* model has two LSTM model embedded inside\n* it captures a state, i.e. in the process of fitting\/evaluating it relies on both input and \"memory\" of what it saw before","2b415d43":"There are some difficulties I did not figure out how to resolve efficiently:\n* the data is not stationary and multiple differencing does not work to make it such\n* usual scaling technics also did not work as expected, more so it made models perform worse\n\nAs a result you will see model is not going to be a magic ball. It perform quite great to predict the dynamic complonent of the behivor of data points but it lacks a component of scale. For now I will leave it as it is but as I learn more about time series prediction methods I might improve it in the future. ","4ea728dd":"For now it concludes my project for <a href='https:\/\/jovian.ml\/forum\/c\/pytorch-zero-to-gans\/18'>Deep Learning with PyTorch: Zero to GANs<\/a> for now but not forever. The knowldge of PyTorch solidified for me know but by no means I know everything (luckily it has a great documentation). Thanks a lot <a href='https:\/\/www.freecodecamp.org\/'>FreeCodeCamp<\/a> and <a href='https:\/\/jovian.ml'>JovianML<\/a> for the opportunity to grow!","0df7f919":"In the <a href='https:\/\/jovian.ml\/erelin6613\/air-pollution-case'>first part<\/a> of this project we explored different approaches to model sequences of metrics regarding S02 emissions. After basic EDA and simple modeling we will be building a smarter model.","6cc51ec4":"Something we did not do before but worth to note. Our values in columns are not exactly normaly distributed. Although it might introduce some difficulties to our model, it should not be too strikingly bad.","3d84d29f":"Although convolutional network seem to give better results at first, after some experimenting I discovered it is not as good as recurrent stateful model. So here is what we will be bulding. But first we need to replicate the setup we have got before.","8d5e6feb":"The bottomline here is that we succesfully build a model which can predict the dynamics of the SO2 emissions. However, the time series prediction task is not an easy challange. Perphaps, our task would be even better modeled with such mathematical model as ARIMA but of course there is a lot of hand-work (or grid-search) tuning and hyperparameter search involved. The remaining issue we have in our task is scaling, i.e. we know when to expect big spikes in emissions but by how much question cannot be answered with high certainty.","b2662e7b":"### Development with what we have learned so far","3c2732b6":"#### Setup repication"}}