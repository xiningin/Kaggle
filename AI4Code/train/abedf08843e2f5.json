{"cell_type":{"af761824":"code","8b7a2482":"code","3500d318":"code","0abeccba":"code","ed753a30":"code","3972f48c":"code","5868a0c6":"code","872ae554":"code","cc52bee2":"code","306822fc":"code","2c8c035b":"code","298220a6":"code","5f088879":"code","735c7ebf":"code","19b297b9":"code","8ed33be7":"code","d4c7eb01":"code","7d6fff49":"code","b9d1346c":"code","fd62edf2":"code","57366300":"markdown","6d6b0494":"markdown","e134b490":"markdown"},"source":{"af761824":"# Standard batch import\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport nltk\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.classify import SklearnClassifier\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport time\nimport nltk.classify.util\nfrom nltk.classify import NaiveBayesClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier ","8b7a2482":"df=pd.read_csv('..\/input\/Hotel_Reviews.csv')","3500d318":"# Lets look at some stats shall we?\ndisplay(df.shape)\ndisplay(df.describe())\ndisplay(df.describe(include=['O']))","0abeccba":"# Keys handy for getting columns later on\ndf.keys()","ed753a30":"# Just for fun, create a database with only hotels info\nhotels=df[['Hotel_Name','Average_Score','Total_Number_of_Reviews',\\\n           'Hotel_Address', 'Additional_Number_of_Scoring','lat', 'lng']].drop_duplicates().reset_index()\nprint(hotels.shape)\nhotels.head(3)","3972f48c":"# Now lets get some new columns running\n# Hotel address ---> Country ('Netherlands', 'UK', 'France', 'Spain', 'Italy', 'Austria')\nhotels['Country']=hotels['Hotel_Address'].apply(lambda x: x.split()[-1]).replace('Kingdom','UK')\nhotels.Country.unique()","5868a0c6":"# I would be curious to see which country gives more reviews (UK most of the time)\n# So lets get the most common nationality of reviews for each hotel\nmost_national=df[['Hotel_Name','Hotel_Address','Reviewer_Nationality']].groupby(['Hotel_Name','Hotel_Address']).agg(lambda x:x.value_counts().index[0])","872ae554":"# In case we want to use the Tags, here I clean up the format \n# and create list from Tags ranked them by occurence\ntags_rank=pd.Series(re.findall(r'[\\']\\s([\\w\\s]+)\\s[\\']',''.join(df.Tags))).value_counts()","cc52bee2":"# For some reason days where not numbers but string so..\n# Correct day of the review into integer\ndf['days_since_review']=pd.to_numeric(df['days_since_review'].str.replace(r'[a-z]+', \"\"))","306822fc":"# Lets isolate the reviews that have some information into negative\/positive\n# For simplicity I start getting the badly scored and highly scored reviews \n# to feed later on to the classifier.\n# This was he\/she\/it will understand how a bad or good review looks like\n# DataFrames with Negative reviews and positive reviews given\nneg_rev=df[df.Negative_Review!='No Negative'].reset_index().drop('index',1)\npos_rev=df[df.Positive_Review!='No Positive'].reset_index().drop('index',1)\nneg_rev = neg_rev[neg_rev['Reviewer_Score']<5].reset_index().drop('index',1)\npos_rev = pos_rev[pos_rev['Reviewer_Score']>8].reset_index().drop('index',1)","2c8c035b":"# Takes review and gives back the clean list of words\n\n#TokTok faster than word_tokenize\nfrom nltk.tokenize import ToktokTokenizer\ntoktok = ToktokTokenizer()\n\n# Stopwords, numbers and punctuation to remove\nremove_punct_and_digits = dict([(ord(punct), ' ') for punct in string.punctuation + string.digits])\nstopWords = set(stopwords.words('english'))\n\n\ndef word_cleaner(data):\n    cleaned_word = data.lower().translate(remove_punct_and_digits)\n    words = word_tokenize(cleaned_word)\n    words = [toktok.tokenize(sent) for sent in sent_tokenize(cleaned_word)]\n    wordsFiltered = []\n    if not words:\n        pass\n    else:\n        for w in words[0]:\n            if w not in stopWords:\n                wordsFiltered.append(w)\n                end=time.time()\n    return wordsFiltered\n\n\n# Example\nwordsFiltered = word_cleaner(neg_rev.Negative_Review[1])\nprint(neg_rev.Negative_Review[1])\nprint(wordsFiltered)","298220a6":"# We take a small sample within our database to speed up Learning\n# with a decent machine and some time to spare we can easily skip this step\nneg_red=neg_rev[:50000].copy()\npos_red=pos_rev[:50000].copy()","5f088879":"# Create set related to positive and negative review\ndef word_feats(words):\n    return dict([(word, True) for word in words])\nneg_set=[(word_feats(word_feats(word_cleaner(neg_red.loc[i,'Negative_Review']))), 0) for i in range(len(neg_red))]\npos_set=[(word_feats(word_feats(word_cleaner(pos_red.loc[i,'Positive_Review']))), 1) for i in range(len(pos_red))]","735c7ebf":"# Finally some Machine is Learning!\n# Train the model and use CV to test accuracy\n\nnegcutoff = int(len(neg_set)*3\/4)\nposcutoff = int(len(pos_set)*3\/4)\n \ntrainfeats = neg_set[:negcutoff] + pos_set[:poscutoff]\ntestfeats = neg_set[negcutoff:] + pos_set[poscutoff:]\nprint(len(trainfeats), len(testfeats))\n \nclassifier = NaiveBayesClassifier.train(trainfeats)\nprint( 'accuracy:', nltk.classify.util.accuracy(classifier, testfeats))","19b297b9":"# Now we want to have a clear overview of the most hated\/loved words so...\n# Builds the dataframe of words with respective sentiment and score\n\ncpdist = classifier._feature_probdist\nword=[]\nscore=[]\nsentiment=[]\nfor (fname, fval) in classifier.most_informative_features(100):\n            def labelprob(l):\n                return cpdist[l, fname].prob(fval)\n\n            labels = sorted([l for l in classifier._labels\n                             if fval in cpdist[l, fname].samples()],\n                            key=labelprob)\n            if len(labels) == 1:\n                continue\n            l0 = labels[0]\n            l1 = labels[-1]\n            if cpdist[l0, fname].prob(fval) == 0:\n                ratio = 'INF'\n            else:\n                ratio = '%8.1f' % (cpdist[l1, fname].prob(fval) \/\n                                   cpdist[l0, fname].prob(fval))\n            sentiment.append(int(l1))\n            word.append(fname)\n            score.append(float(ratio))","8ed33be7":"# Divides scores into negative and positive\nword_scores=pd.DataFrame({'word':word,'sentiment':sentiment,'score':score})\nneg_word_scores=word_scores[word_scores.sentiment==0]\npos_word_scores=word_scores[word_scores.sentiment==1]\ndisplay(word_scores[word_scores['sentiment']==1].head())\ndisplay(word_scores[word_scores['sentiment']==0].head())\nneg_given=df[df.Negative_Review!='No Negative'].reset_index().drop('index',1)\npos_given=df[df.Positive_Review!='No Positive'].reset_index().drop('index',1)","d4c7eb01":"# I want to create two new columns, one that will give a positive and one a negative score\n# Sums positive and negative scores for a given review\ndef pos_sentiment_sum(review):\n    pos=0\n    asd=word_cleaner(review)\n    set_w=set(pos_word_scores.word)-set(['no','negative','positive'])\n    \n    for word in asd:\n        if word in set_w:\n            pos+=pos_word_scores[pos_word_scores['word']==word].score.iloc[0]\n    \n    return pos\n\ndef neg_sentiment_sum(review):\n    neg=0\n    asd=word_cleaner(review)\n    set_w=set(neg_word_scores.word)-set(['no','negative','positive'])\n    \n    for word in asd:\n        if word in set_w:\n            neg+=neg_word_scores[neg_word_scores['word']==word].score.iloc[0]\n    \n    return neg\n\n# TEST\nprint('negative score:',neg_sentiment_sum(df.Negative_Review[6584]),\\\n       'positive score:', pos_sentiment_sum(df.Positive_Review[6584]))","7d6fff49":"# VERY SLOW! AROUND 45min for full database\n# I comment it out so it does not need to run when I submit\n\n# This is the final step where we get the additional columns \n# Produce the pos and neg colums in database\n\n#=================\n#pos_col=[]\n#for i in range(len(df)):\n#    if df.Positive_Review[i]=='No Positive':\n#        pos_col.append(int(0))\n#    else:\n#        pos_col.append(pos_sentiment_sum(df.Positive_Review[i]))\n#df['pos_score']=pos_col\n#\n#neg_col=[]\n#for i in range(len(df)):\n#    if df.Negative_Review[i]=='No Negative':\n#        neg_col.append(int(0))\n#    else:\n#        neg_col.append(neg_sentiment_sum(df.Negative_Review[i]))\n#df['neg_score']=neg_col\n#=================","b9d1346c":"# The analysis can go on, but this is the last step for now\n# Reviews grouped by rate band\nscore_9=df[df.Reviewer_Score>9].copy()\nscore_4=df[df.Reviewer_Score<4].copy()\nscore_6=df[df.Reviewer_Score<7].copy()\nscore_7=df[(df.Reviewer_Score>7)&(df.Reviewer_Score<8)].copy()\nscore_8=df[(df.Reviewer_Score>8)&(df.Reviewer_Score<9)].copy()\nprint(score_6.shape)\nprint(score_7.shape)\nprint(score_8.shape)\nprint(score_9.shape)","fd62edf2":"# Some plots\n\nfrom matplotlib import rcParams\nrcParams.update({'figure.autolayout': True})\nplt.style.use('fivethirtyeight')\n\nplt.figure(figsize=(7,7))\nplt.hist(df['Reviewer_Score'],bins=20)\nplt.ylabel('Number_Reviewers',fontsize=16)\nplt.xlabel('Rating',fontsize=16)\nplt.title('Ratings accross Users',fontsize=16)\nplt.axvline(df['Reviewer_Score'].mean(), color='k', linestyle='dashed', linewidth=1)\nplt.savefig('Ratings_user.png')\n\nplt.figure(figsize=(7,7))\nplt.hist(df['Average_Score'],bins=20)\nplt.ylabel('Number Hotels',fontsize=16)\nplt.xlabel('Rating',fontsize=16)\nplt.title('Ratings accross Hotels',fontsize=16)\nplt.axvline(df['Average_Score'].mean(), color='k', linestyle='dashed', linewidth=1)\nplt.savefig('Ratings_hotel.png')\n\nplt.figure(figsize=(7,4))\nweek_bins=int(np.floor((max(df['days_since_review'])-min(df['days_since_review']))\/7))\nvals = plt.hist(df['days_since_review'],bins=week_bins);\nplt.ylabel('Number ratings',fontsize=16)\nplt.xlabel('Days passed',fontsize=16)\nplt.title('Ratings per week',fontsize=16)\nplt.axhline(vals[0].mean(), color='k', linestyle='dashed', linewidth=1)\nplt.savefig('Ratings_week.png')\n\nplt.figure(figsize=(7,4))\nplt.plot(vals[0]);\nplt.ylabel('Number ratings',fontsize=16)\nplt.xlabel('Weeks passed',fontsize=16)\nplt.title('Ratings per week',fontsize=16)\nplt.axhline(vals[0].mean(), color='k', linestyle='dashed', linewidth=1)\nplt.savefig('Ratings_week_plot.png')","57366300":"**Add columns, Clean columns and feel the data**\nI play a bit with the data to get confortable with it and get some new columns and lists that could be useful for better understanding of the dataset.","6d6b0494":"**Create some useful function to play with words**\nHere we define some handy functions, some choices have been made for making them a bit faster (like using Toktok) as we will have to run this over the whole database (515K! my laptop is old...)","e134b490":"**Sentiment Analysis**:\nHere I go through some data exploration, and manipulation to build up a database that contains some paramenters for Sentiment Analysis. Using a quick NaiveBayesClassifier I build a score for each word that can be used to rank results (indipendently form their review scores)\n\nThis is a WIP, so comments and suggestions are more than welcome!\n\nMany bits of this Kernel come from info and tutorials sparse around the internet, I will try to add the links to the major resources once I am done with the analysis, please bear with me!"}}