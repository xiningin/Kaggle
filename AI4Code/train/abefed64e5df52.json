{"cell_type":{"ded5b3b7":"code","46b1860d":"code","1f3d92da":"code","24686bfc":"code","adfa17b4":"code","6d033d41":"code","84ceee81":"code","8121f62f":"code","0d7dc0d3":"code","893b773a":"code","8011abac":"code","2a73ae1b":"code","a4ba32bf":"code","8954484c":"code","a3f8f198":"code","bed93081":"code","9667375f":"code","8658c16b":"code","a5620166":"code","37919cd4":"code","761caa84":"code","febdd99a":"code","b1203a4d":"code","94f37907":"code","58525814":"code","97bf86b3":"code","7f17301f":"code","3ee2e181":"code","107ab82e":"code","ce486af0":"code","f78e4c33":"code","3918013f":"code","d27eb25e":"code","12ec4635":"code","c339324d":"code","c9b360ea":"code","8d892086":"code","b1489ba4":"markdown","dfc2ea13":"markdown","343090c8":"markdown","1f4730e6":"markdown","ecace99f":"markdown","b6616795":"markdown","f33fd2a0":"markdown","0b6a2693":"markdown","2d16927e":"markdown","a19b7eda":"markdown","9ded31a7":"markdown","1cb628f3":"markdown","d4eeea4d":"markdown","b7048d57":"markdown","65b5c3ff":"markdown","28f1404c":"markdown"},"source":{"ded5b3b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","46b1860d":"#import library\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","1f3d92da":"df= pd.read_csv('\/kaggle\/input\/covid19-vaccine-tweets\/file.csv')\n\ndf.head()","24686bfc":"df = df[df['language']=='en']","adfa17b4":"data = df[['tweet']]\ndata.head()","6d033d41":"data= data.reset_index(drop=True)\ndata['process_tweet'] = data['tweet'].copy()\ndata.head()","84ceee81":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport transformers as ppb\n\nfrom tqdm import tqdm_notebook as tqdm\nimport random\nimport matplotlib.pyplot as plt\nimport warnings\n\n\n#import library\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder   ###########\nfrom sklearn.metrics import confusion_matrix , classification_report , accuracy_score\nfrom sklearn.manifold import TSNE ######\nfrom sklearn.feature_extraction.text import TfidfVectorizer #############\n\nfrom keras.preprocessing.text import Tokenizer         #######\nfrom keras.preprocessing.sequence import pad_sequences #######\nfrom keras.models import Sequential\nfrom keras.layers import Dropout , Conv1D ,MaxPool1D,Activation , Dense , Flatten , Embedding , LSTM ####\n\nfrom keras import utils\nfrom keras.callbacks import ReduceLROnPlateau , EarlyStopping\n\n\n#nltk\n\nimport nltk\nfrom nltk.corpus import stopwords ########\nfrom nltk.stem import SnowballStemmer ############\n\n\n#word to vec\nimport gensim\n\nimport re #####\nimport os\nfrom collections import Counter #######\nimport logging ###\nimport time\nimport pickle ######\nimport itertools ######\n\n\nfrom textblob import TextBlob # TextBlob - Python library for processing textual data\n\n\n\n","8121f62f":"def clean_data(txt):\n  txt = txt.lower()  # lowercase\n  txt = re.sub(r'@[A-Za-z0-9_]+' , '' , txt)   #remove mentions\n  txt = re.sub(r'#' , '' , txt) #remove hashtags\n  txt = re.sub(r'RT : ','' , txt) # remove retweets\n  txt = re.sub(r'https?:\\\/\\\/[A-Za-z0-9\\.\/\\\/]+' , '' , txt) #removes url\n  txt = re.sub('\\[.*?\\]' , '' , txt) #remove square brackets\n  txt = re.sub(r'[^\\w\\s]' , '' , txt) #remove puntuations\n  txt = re.sub('\\w*\\d\\w*' , '' ,txt) #removes words containig numbers\n  txt = re.sub('\\n' , '' ,txt) #remove new lines\n\n  return txt\n","0d7dc0d3":"data['process_tweet'] = data['process_tweet'].apply(clean_data)","893b773a":"stopwords = nltk.corpus.stopwords.words('english')\n\ndef remove_stopwords(txt):\n  rmv_stpwords = [i for i in txt.split() if i not in stopwords]\n  rmv_stpwords_join = ' '.join(rmv_stpwords)\n  return rmv_stpwords_join\n","8011abac":"data['process_tweet'] = data['process_tweet'].apply(remove_stopwords)\n\ndata.head()#after removing stopwords","2a73ae1b":"from nltk.stem import PorterStemmer\nst = PorterStemmer()","a4ba32bf":"def stemming(txt):\n  txt =[st.stem(word) for word in txt.split()]\n  txt_join = ' '.join(txt)\n  return txt_join","8954484c":"data['process_tweet'] = data['process_tweet'].apply(stemming)\ndata.head()","a3f8f198":"def get_text_polarity(txt):\n  return TextBlob(txt).sentiment.polarity","bed93081":"data['Polarity'] = data['process_tweet'].apply(get_text_polarity)","9667375f":"#Labeling\ndef get_text_analysis(i):\n  if (i<-0.5):\n    return 'Strongly Negative'\n  elif ((i<0 ) and (i >= -0.5)):\n    return 'Negative'\n  elif (i == 0):\n    return 'Neutral'\n  elif (i>0 and i<=0.5):\n    return 'Positive'\n  else:\n    return 'Strongly Positive'\n","8658c16b":"data['Sentiment'] = data['Polarity'].apply(get_text_analysis)","a5620166":"#for deep learning we need to only 2 colums(sentiment and process tweet)\ndata = data[['Sentiment','process_tweet' ]]\n\npossible_labels = data.Sentiment.unique()\n\nlabel_dict = {}\nfor index, possible_label in enumerate(possible_labels): \n#The enumerate() function takes a collection (e.g. a tuple) and returns it as an enumerate object.\n    label_dict[possible_label] = index\n\ndata['label'] = data.Sentiment.replace(label_dict)\n\n\nprint(label_dict)\n\ndata.head()","37919cd4":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,5))\nax = data['Sentiment'].value_counts(sort=False).plot(kind='barh')\nax.set_xlabel('Number of Samples in training Set')\nax.set_ylabel('Label')","761caa84":"unique_words = set(data['process_tweet'])\ncount = 0\nfor word in unique_words:\n  count += 1\nprint(count)","febdd99a":"#tokenization\ntokenizer = Tokenizer(num_words=163967, split=' ')\n#num_words: the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n\n\ntokenizer.fit_on_texts(data['process_tweet'].values)\n\nX = tokenizer.texts_to_sequences(data['process_tweet'].values)","b1203a4d":"X[:3] #before padding","94f37907":"#padding to make all text vector to same length\n\nX = pad_sequences(X)\n\nX[:3] #after padding","58525814":"#creating models\n\nmodel= Sequential()\nmodel.add(Embedding(163967, 256 , input_length=X.shape[1]))\nmodel.add(Dropout(0.3))\n\n\nmodel.add(LSTM(128 , return_sequences=True , dropout=0.3 , recurrent_dropout=0.3))\nmodel.add(LSTM(128,dropout=0.3 , recurrent_dropout=0.3))\n\n\nmodel.add(Dense(5 , activation='softmax'))\n\nmodel.summary()","97bf86b3":"model.compile(optimizer='adam' , loss = 'categorical_crossentropy' , metrics=['accuracy'])","7f17301f":"#one hot encoding\n\ny = pd.get_dummies(data['Sentiment']).values\n\n[print(data['Sentiment'][i] ,y[i]) for i in range(0,5)]","3ee2e181":"x_train , x_test , y_train , y_test = train_test_split(X , y , test_size = 0.2 ,random_state = 22)","107ab82e":"#trannig model\nbatch_size = 128\nepochs = 5","ce486af0":"history = model.fit(x_train , y_train ,\n          epochs = epochs,\n          batch_size = batch_size,\n          validation_split=0.1,\n          verbose=1)","f78e4c33":"%%time\nscore = model.evaluate(x_test, y_test , batch_size = batch_size)\nprint()\nprint(\"ACCURACY:\",score[1])\nprint(\"LOSS:\",score[0])","3918013f":"predictions = model.predict(x_test)\npred1 = np.argmax(predictions , axis=1)\npred1[:10]","d27eb25e":"y_test = np.argmax(y_test , axis=1)\n","12ec4635":"print(confusion_matrix(y_test , pred1))","c339324d":"print(classification_report(y_test , pred1))","c9b360ea":"accuracy_score(y_test , pred1)","8d892086":"epochs = [i for i in range(5)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\nfig.set_size_inches(30,10)\n\n\nax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\nax[0].plot(epochs , val_acc , 'ro-' , label = 'Validation Accuracy')\nax[0].set_title('Training & Validation Accuracy')\n\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Accuracy\")\n\n\nax[1].plot(epochs , train_loss , 'g-o' , label = 'Training Loss')\nax[1].plot(epochs , val_loss , 'r-o' , label = 'Validation Loss')\nax[1].set_title('Testing Accuracy & Loss')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Training & Validation Loss\")\nplt.show()","b1489ba4":"**Classification Report**","dfc2ea13":"**Accuracy**","343090c8":"**Import Data**","1f4730e6":"**Only Englsih language tweet**","ecace99f":"**RNN Model**","b6616795":"**Data split**","f33fd2a0":"**Remove Stopwords**","0b6a2693":"**Padding**","2d16927e":"**Labeling using the polarity score**","a19b7eda":"**Import library**","9ded31a7":"**Using TextBlob calculate the polarity**","1cb628f3":"**Total Unique words after pre processing**","d4eeea4d":"**Tokenization**","b7048d57":"**Data procesing**","65b5c3ff":"**Tranning the model**","28f1404c":"**PorterStemmer**"}}