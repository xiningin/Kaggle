{"cell_type":{"385be0a8":"code","bff3c843":"code","2d2308ea":"code","66d04010":"code","e6d583fe":"code","5d5666a4":"code","a51000e1":"code","50b9e98b":"code","d70536eb":"code","a94f9824":"markdown","de46d5f5":"markdown","676e6f54":"markdown","253e2e71":"markdown","ebeda580":"markdown","f9eba7d6":"markdown"},"source":{"385be0a8":"import lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\n\nimport riiideducation","bff3c843":"# You can only call make_env() once, so don't lose it!\nenv = riiideducation.make_env()","2d2308ea":"train_df = pd.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/train.csv', low_memory=False, nrows=10**5, \n                       dtype={'row_id': 'int64', 'timestamp': 'int64', 'user_id': 'int32', 'content_id': 'int16', 'content_type_id': 'int8',\n                              'task_container_id': 'int16', 'user_answer': 'int8', 'answered_correctly': 'int8', 'prior_question_elapsed_time': 'float32', \n                             'prior_question_had_explanation': 'boolean',\n                             }\n                      )\ntrain_df = train_df.query('answered_correctly != -1').reset_index(drop=True)\ntrain_df['prior_question_had_explanation'] = train_df['prior_question_had_explanation'].astype(float)","66d04010":"train_df.head()","e6d583fe":"y_train = train_df['answered_correctly']\nX_train = train_df.drop(['answered_correctly', 'user_answer'], axis=1)","5d5666a4":"models = []\noof_train = np.zeros((len(X_train),))\ncv = KFold(n_splits=5, shuffle=True, random_state=0)\n\ncategorical_features = ['user_id', 'content_type_id', 'task_container_id', 'prior_question_had_explanation']\n\nparams = {\n    'objective': 'binary',\n    'max_bin': 300,\n    'learning_rate': 0.05,\n    'num_leaves': 40\n}\n\nfor fold_id, (train_index, valid_index) in enumerate(cv.split(X_train)):\n    X_tr = X_train.loc[train_index, :]\n    X_val = X_train.loc[valid_index, :]\n    y_tr = y_train[train_index]\n    y_val = y_train[valid_index]\n\n    lgb_train = lgb.Dataset(X_tr, y_tr, categorical_feature=categorical_features)\n    lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train, categorical_feature=categorical_features)\n\n    model = lgb.train(\n        params, lgb_train,\n        valid_sets=[lgb_train, lgb_eval],\n        verbose_eval=10,\n        num_boost_round=1000,\n        early_stopping_rounds=10\n    )\n\n    oof_train[valid_index] = model.predict(X_val, num_iteration=model.best_iteration)\n    models.append(model)","a51000e1":"roc_auc_score(y_train, oof_train)","50b9e98b":"iter_test = env.iter_test()","d70536eb":"for (test_df, sample_prediction_df) in iter_test:\n    y_preds = []\n    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].astype(float)\n    X_test = test_df.drop(['prior_group_answers_correct', 'prior_group_responses'], axis=1)\n\n    for model in models:\n        y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n        y_preds.append(y_pred)\n\n    y_preds = sum(y_preds) \/ len(y_preds)\n    test_df['answered_correctly'] = y_preds\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","a94f9824":"Parameters\nControl Parameters\nmax_depth: It describes the maximum depth of tree. This parameter is used to handle model overfitting. Any time you feel that your model is overfitted, my first advice will be to lower max_depth.\nmin_data_in_leaf: It is the minimum number of the records a leaf may have. The default value is 20, optimum value. It is also used to deal over fitting\nfeature_fraction: Used when your boosting(discussed later) is random forest. 0.8 feature fraction means LightGBM will select 80% of parameters randomly in each iteration for building trees.\nbagging_fraction: specifies the fraction of data to be used for each iteration and is generally used to speed up the training and avoid overfitting.\nearly_stopping_round: This parameter can help you speed up your analysis. Model will stop training if one metric of one validation data doesn\u2019t improve in last early_stopping_round rounds. This will reduce excessive iterations.\nlambda: lambda specifies regularization. Typical value ranges from 0 to 1.\nmin_gain_to_split: This parameter will describe the minimum gain to make a split. It can used to control number of useful splits in tree.\nmax_cat_group: When the number of category is large, finding the split point on it is easily over-fitting. So LightGBM merges them into \u2018max_cat_group\u2019 groups, and finds the split points on the group boundaries, default:64\nCore Parameters\nTask: It specifies the task you want to perform on data. It may be either train or predict.\napplication: This is the most important parameter and specifies the application of your model, whether it is a regression problem or classification problem. LightGBM will by default consider model as a regression model.\nregression: for regression\nbinary: for binary classification\nmulticlass: for multiclass classification problem\nboosting: defines the type of algorithm you want to run, default=gdbt\ngbdt: traditional Gradient Boosting Decision Tree\nrf: random forest\ndart: Dropouts meet Multiple Additive Regression Trees\ngoss: Gradient-based One-Side Sampling\nnum_boost_round: Number of boosting iterations, typically 100+\nlearning_rate: This determines the impact of each tree on the final outcome. GBM works by starting with an initial estimate which is updated using the output of each tree. The learning parameter controls the magnitude of this change in the estimates. Typical values: 0.1, 0.001, 0.003\u2026\nnum_leaves: number of leaves in full tree, default: 31\ndevice: default: cpu, can also pass gpu\nMetric parameter\nmetric: again one of the important parameter as it specifies loss for model building. Below are few general losses for regression and classification.\nmae: mean absolute error\nmse: mean squared error\nbinary_logloss: loss for binary classification\nmulti_logloss: loss for multi classification\nIO parameter\nmax_bin: it denotes the maximum number of bin that feature value will bucket in.\ncategorical_feature: It denotes the index of categorical features. If categorical_features=0,1,2 then column 0, column 1 and column 2 are categorical variables.\nignore_column: same as categorical_features just instead of considering specific columns as categorical, it will completely ignore them.\nsave_binary: If you are really dealing with the memory size of your data file then specify this parameter as \u2018True\u2019. Specifying parameter true will save the dataset to binary file, this binary file will speed your data reading time for the next time.\nKnowing and using above parameters will definitely help you implement the model. Remember I said that implementation of LightGBM is easy but parameter tuning is difficult. So let\u2019s first start with implementation and then I will give idea about the parameter tuning.\nImplementation\nInstallating LGBM:\nInstalling LightGBM is a crucial task. I found this as the best resource which will guide you in LightGBM installation.\nI am using Anaconda and installing LightGBM on anaconda is a clinch. Just run the following command on your Anaconda command prompt and whoosh, LightGBM is on your PC.\nconda install -c conda-forge lightgbm\n","de46d5f5":"Why Light GBM is gaining extreme popularity?\nThe size of data is increasing day by day and it is becoming difficult for traditional data science algorithms to give faster results. Light GBM is prefixed as \u2018Light\u2019 because of its high speed. Light GBM can handle the large size of data and takes lower memory to run. Another reason of why Light GBM is popular is because it focuses on accuracy of results. LGBM also supports GPU learning and thus data scientists are widely using LGBM for data science application development.","676e6f54":"We briefly discussed the concept of Light GBM, now what about it\u2019s implementation?\nImplementation of Light GBM is easy, the only complicated thing is parameter tuning. Light GBM covers more than 100 parameters but don\u2019t worry, you don\u2019t need to learn all.\nIt is very important for an implementer to know atleast some basic parameters of Light GBM. If you carefully go through following parameters of LGBM, I bet you will find this powerful algorithm a piece of cake.","253e2e71":"Light GBM is a gradient boosting framework that uses tree based learning algorithm.\nLight GBM grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm.\nBelow diagrams explain the implementation of LightGBM and other boosting algorithms.\n![image.png](attachment:image.png)\n","ebeda580":"![image.png](attachment:image.png)\n","f9eba7d6":"Can we use Light GBM everywhere?\nNo, it is not advisable to use LGBM on small datasets. Light GBM is sensitive to overfitting and can easily overfit small data. Their is no threshold on the number of rows but my experience suggests me to use it only for data with 10,000+ rows."}}