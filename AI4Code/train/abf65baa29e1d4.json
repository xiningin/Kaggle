{"cell_type":{"4a0dd3f3":"code","ecc91d61":"code","3804a201":"code","eb47808e":"code","1d709e8c":"code","4a5218ae":"code","ad64198e":"code","fd190f00":"code","95196e2b":"code","1b7f98ed":"code","5f4e172a":"code","f0851948":"code","0dd8edc3":"code","5fc9d64f":"code","6f78bba2":"code","77ef5e81":"code","6b550b0f":"code","73daf3ef":"code","1fa335f1":"code","e438c6b3":"code","73176e3d":"markdown","9a214cf0":"markdown","ffc75bc0":"markdown","c34df590":"markdown","63d16768":"markdown","6d0c1c18":"markdown","ed72cfc0":"markdown","65385e1d":"markdown","d6f0b530":"markdown","e0ca5bc3":"markdown","ca766738":"markdown","1eb3a04b":"markdown","b3f721fc":"markdown","d6963afc":"markdown","d6a47135":"markdown","b6e0d104":"markdown"},"source":{"4a0dd3f3":"# 1. Enable Internet in the Kernel (Settings side pane)\n\n# 2. Curl cache may need purged if v0.1.6 cannot be found (uncomment if needed). \n# !curl -X PURGE https:\/\/pypi.org\/simple\/kaggle-environments\n\n# ConnectX environment was defined in v0.1.6\n!pip install kaggle-environments","ecc91d61":"import numpy as np\nimport gym\nimport random\nimport matplotlib.pyplot as plt\nfrom random import choice\nfrom tqdm.notebook import tqdm\nfrom kaggle_environments import evaluate, make, utils\nimport pickle","3804a201":"class ConnectX(gym.Env):\n    def __init__(self):\n        self.env = make(\"connectx\", debug=True)\n        self.pair = [None, 'negamax']\n        self.trainer = self.env.train(self.pair)\n        config = self.env.configuration\n        self.action_space = gym.spaces.Discrete(config.columns)\n        self.observation_space = gym.spaces.Discrete(config.columns * config.rows)\n        \n    def switch_trainer(self):\n        self.pair = self.pair[::-1]\n        self.trainer = self.env.train(self.pair)\n        \n    def step(self, action):\n        return self.trainer.step(action)\n    \n    def reset(self):\n        return self.trainer.reset()\n    \n    def render(self, **kwargs):\n        return self.env.render(**kwargs)\n    \nclass QTable:\n    def __init__(self, action_space):\n        self.table = dict()\n        self.action_space = action_space\n        \n    def add_item(self, state_key):\n        self.table[state_key] = list(np.zeros(self.action_space.n))\n        \n    def set_table(self, table):\n        self.table = table\n\n    def __call__(self, state):\n        board = state.board[:]\n        board.append(state.mark)\n        state_key = np.array(board).astype(str)\n        state_key = hex(int(''.join(state_key), 3))[2:]\n        if state_key not in self.table.keys():\n            self.add_item(state_key)\n        return self.table[state_key]","eb47808e":"env = ConnectX()","1d709e8c":"LEARNING_RATE = 0.3\nDISCOUNT_RATE = 0.7\nEPISODES = 10000\n\nALPHA_DECAY_STEP = 1000\nALPHA_DECAY_RATE = 0.9","4a5218ae":"q_table_1 = QTable(env.action_space)\nq_table_2 = QTable(env.action_space)","ad64198e":"dql_all_epochs = []\ndql_all_total_rewards = []\ndql_all_avg_rewards = []\ndql_all_qtable_rows = []","fd190f00":"def maxAction(Q1, Q2, state):\n    values = []\n    for j in range(env.action_space.n):\n        if state.board[j] == 0:\n            values.append(Q1(state)[j] + Q2(state)[j])\n        else:\n            values.append(-1e7)\n    action = np.argmax(values)\n    return int(action)","95196e2b":"for i in tqdm(range(EPISODES)):\n    state = env.reset()\n    epochs, total_rewards = 0, 0\n    done = False\n    \n    while not done:\n        action = choice([c for c in range(env.action_space.n) if state.board[c] == 0])\n            \n        next_state, reward, done, info = env.step(action)\n        \n        if done:\n            if reward == 1: # Won\n                reward = 20\n            elif reward == 0: # Lost\n                reward = -20\n            else: # Draw\n                reward = 10\n        else:\n            reward = -0.05\n        \n        rand = random.uniform(0, 1)\n        if rand <= 0.5:\n            new_value = maxAction(q_table_1, q_table_1, state)\n            q_table_1(state)[action] = q_table_1(state)[action] + LEARNING_RATE * (reward + DISCOUNT_RATE * q_table_2(next_state)[new_value] - q_table_1(state)[action])\n        elif rand > 0.5:\n            new_value = maxAction(q_table_2, q_table_2, state)\n            q_table_2(state)[action] = q_table_2(state)[action] + LEARNING_RATE * (reward + DISCOUNT_RATE * q_table_1(next_state)[new_value] - q_table_2(state)[action])\n    \n        state = next_state\n        epochs += 1\n        total_rewards += reward\n        \n    dql_all_epochs.append(epochs)\n    dql_all_total_rewards.append(total_rewards)\n    avg_rewards = np.mean(dql_all_total_rewards[max(0, i-100):(i+1)])\n    dql_all_avg_rewards.append(avg_rewards)\n    dql_all_qtable_rows.append(len(q_table_1.table))\n    if (i+1) % ALPHA_DECAY_STEP == 0:\n        LEARNING_RATE *= ALPHA_DECAY_RATE","1b7f98ed":"print(len(q_table_1.table))\nprint(len(q_table_2.table))","5f4e172a":"plt.plot(dql_all_avg_rewards)\nplt.xlabel('Episode')\nplt.ylabel('Avg rewards (100)')\nplt.show()","f0851948":"plt.plot(dql_all_qtable_rows)\nplt.xlabel('Episode')\nplt.ylabel('Explored states')\nplt.show()","0dd8edc3":"tmp_dict_q_table_1 = q_table_1.table.copy()\ntmp_dict_q_table_2 = q_table_2.table.copy()\n\ndict_q_table = dict()\nfor i in tmp_dict_q_table_1:\n    dict_q_table[i] = tmp_dict_q_table_1[i]\nfor i in tmp_dict_q_table_2:\n    if i in dict_q_table.keys():\n        for j in range(env.action_space.n):\n            dict_q_table[i][j] += tmp_dict_q_table_2[i][j]\n    else:\n        dict_q_table[i] = tmp_dict_q_table_2[i]\n        \nfor k in dict_q_table:\n    if np.count_nonzero(dict_q_table[k]) > 0:\n        dict_q_table[k] = int(np.argmax(dict_q_table[k]))\n    else:\n        dict_q_table[k] = -1","5fc9d64f":"import pickle\nimport zlib\nimport base64 as b64\n\ndef serializeAndCompress(value, verbose=True):\n    serializedValue = pickle.dumps(value)\n    if verbose:\n        print('Lenght of serialized object:', len(serializedValue))\n    c_data =  zlib.compress(serializedValue, 9)\n    if verbose:\n        print('Lenght of compressed and serialized object:', len(c_data))\n    return b64.b64encode(c_data)","6f78bba2":"serialized_q_table = serializeAndCompress(dict_q_table)","77ef5e81":"my_agent = '''def my_agent(observation, configuration):\n    from random import choice\n    import pickle\n    import zlib\n    import base64 as b64\n    \n    serialized_q_table = ''' \\\n    + str(serialized_q_table) \\\n    + '''\n\n    d_data_byte = b64.b64decode(serialized_q_table)\n    data_byte = zlib.decompress(d_data_byte)\n    q_table = pickle.loads(data_byte)\n\n    board = observation.board[:]\n    board.append(observation.mark)\n    state_key = list(map(str, board))\n    state_key = hex(int(''.join(state_key), 3))[2:]\n\n    if state_key not in q_table.keys():\n        return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n\n    action = q_table[state_key]\n    \n    if action == -1:\n        return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n\n    if observation.board[action] != 0:\n        return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n\n    return action\n    '''","6b550b0f":"with open('submission.py', 'w') as f:\n    f.write(my_agent)","73daf3ef":"from submission import my_agent","1fa335f1":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) \/ float(len(rewards))\n\n# Run multiple episodes to estimate its performance.\nprint(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))","e438c6b3":"env = make(\"connectx\", debug=True)\nenv.run([my_agent, my_agent])\nprint(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")","73176e3d":"<a class=\"anchor\" id=\"create_environment\"><\/a>\n# Create ConnectX environment","9a214cf0":"<a class=\"anchor\" id=\"reinforcement_learning\"><\/a>\n# Reinforcement learning\nReinforcement learning involves an agent, a set of states ***S***, and a set ***A*** of actions per state. By performing an action, the agent transitions from state to state. Executing an action in a specific state provides the agent with a reward (a numerical score).\n\nThe goal of the agent is to maximize its total reward. It does this by adding the maximum reward attainable from future states to the reward fro achieving its current state, effectively influencing the current action by the potention future reward.\n\nIn Q-Learning, before learning begins, a Q-table is initialized to a possibly arbitrary fixed value. ","ffc75bc0":"<a class=\"anchor\" id=\"implementation\"><\/a>\n# Implementation\n<a class=\"anchor\" id=\"useful_classes\"><\/a>\n# Define Useful Classes","c34df590":"<a class=\"anchor\" id=\"results\"><\/a>\n# Analyze results","63d16768":"<a class=\"anchor\" id=\"competition_connectx\"><\/a>\n# The Competition \"ConnectX\"\n[Kaggle](https:\/\/www.kaggle.com\/) annouce a beta-version of a brand-new type of Machine Learning competition called Simulations. In Simulation Compoetitions, you'll compete against a set of rules, rather than against an evaluation metric.\n\nInstead of submitting a CSV file, or a Kaggle Notebook, you will submit a Python .py file. You'll also notice that the leaderboard is not based on how accurate your model is but rather how well you've performed against other users.","6d0c1c18":"<a class=\"anchor\" id=\"create_agent\"><\/a>\n# Create the agent","ed72cfc0":"<a class=\"anchor\" id=\"evaluation_agent\"><\/a>\n# Evaluate your Agent","65385e1d":"<a class=\"anchor\" id=\"ToC\"><\/a>\n# Table of Contents\n* [The Competition \"ConnectX\"](#competion_connectx)\n* [Reinforcement learning](#reinforcement_learning)\n* [Q-Learning](#q_learning)\n* [Double Q-Learning](#double_q_learning)\n* [Implementation](#implementation)\n    * [Define useful classes](#useful_classes)\n    * [Create ConnectX environment](#create_environment)\n    * [Configure hyper-parameters](#hyper_parameters)\n    * [Train the agent](#train)\n    * [Analyze results](#results)\n    * [Create an agent](#create_agent)\n    * [Evaluate the agent](#evaluation_agent)\n* [Conclusion](#conclusion)\n* [Credits](#credits)","d6f0b530":"<a class=\"anchor\" id=\"double_q_learning\"><\/a>\n# Double Q-Learning\nIn this competition, Q-Learning is already use and presented. If I would add my contribution in this competion I choose to implement Double Q-Learning algorithm.\nIn a noisy environments Q-Learning can sometimes overestimate the action values, slowing the learning. A variant called Double Q-learning was proposed to correct this.\n\nTwo sperate value functions are trained in a mutually symmetric fashion using separate experiences, $Q^{A}$ and $Q^{B}$. The double Q-Learning update step is then as follow:\n\n![QA](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/4941acabf5144d1b3e9c271606011abdc0df444d)\n![QB](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/3e37476013126ddd4afdba69ef7b03767f4c4b75)\n\nNow the estimated value of the discounted future is evaluated using a diffrent policy, which solves the overestimation issue.","e0ca5bc3":"<a class=\"anchor\" id=\"hyper_parameters\"><\/a>\n# Define hyper-parameters","ca766738":"<a class=\"anchor\" id=\"conclusion\"><\/a>\n# Conclusion\nFor this competition, the double Q-Learning method is probably not the best because it would be necessary to be able to submit a final file larger than 1MB, and train the agent on more than 10000 episodes, usually double Q-Learning need more episodes to converge\n\nAccording to the leaderboard, Double Q-Learning can manage the Deep Q-Learning (DQN) algorithm. We could consider implementing a double Deep Q-Learning (Double DQN).","1eb3a04b":"<a class=\"anchor\" id=\"train\"><\/a>\n# Train the agent","b3f721fc":"<a class=\"anchor\" id=\"validate\"><\/a>\n# Validate submissions","d6963afc":"# Double Q-Learning Implementation\n> NOTE : This kernel uses [Hieu Phung's Kernel](https:\/\/www.kaggle.com\/phunghieu\/connectx-with-q-learning) as a base\n\n# Introduction\n[Connect Four](https:\/\/en.wikipedia.org\/wiki\/Connect_Four) is a game where two players alternate turns dropping colored discs into a vertical grid. Each player uses a different color (usually red or yellow), and the objective of the game is to be the first player to get four discs in a row.","d6a47135":"<a class=\"anchor\" id=\"q_learning\"><\/a>\n# Q-Learning\nBefore learning begins, ***Q*** is initialized to a possibly arbitrary fixed value.\n***Q*** is commonly called ***Q-table***, It is a table with the rewards for each states and actions.\n\n![Q-table](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/e\/e0\/Q-Learning_Matrix_Initialized_and_After_Training.png\/440px-Q-Learning_Matrix_Initialized_and_After_Training.png)\n\nFor a game like Tic-Tac-Toe it is possible to store all the values because the game has less than 6000 states. The game \"Connect 4\" is far more complex than Tic-Tac-Toe because it has more than $10^{14}$. Initialize ***Q-table*** is almost impossible in a kernel, the approach that dynamically adding newly discovered states into an object of QTable class.\n\nThe Q-learning algorithm follow a function that easy to understand.\n![Q-function](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/678cb558a9d59c33ef4810c9618baf34a9577686)","b6e0d104":"<a class=\"anchor\" id=\"credits\"><\/a>\n# Credits\n* [Adam - ConnectX Getting Started](https:\/\/www.kaggle.com\/ajeffries\/connectx-getting-started)\n* [Hieu Phung - ConnectX with Q-Learning](https:\/\/www.kaggle.com\/phunghieu\/connectx-with-q-learning)\n* [Sentdex - Reinforcement learning playlist video](https:\/\/www.youtube.com\/playlist?list=PLQVvvaa0QuDezJFIOU5wDdfy4e9vdnx-7)\n* [Rubik's code - Introduction to Double Q-Learning](https:\/\/rubikscode.net\/2020\/01\/13\/introduction-to-double-q-learning\/)\n* [Hado van Haselt - Double Q-Learning](https:\/\/papers.nips.cc\/paper\/3964-double-q-learning.pdf)\n* [Wikipedia - Q-Learning](https:\/\/en.wikipedia.org\/wiki\/Q-learning)\n"}}