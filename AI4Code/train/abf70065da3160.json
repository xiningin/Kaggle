{"cell_type":{"256f2ba7":"code","e7cd580a":"code","243dd580":"code","729643e3":"code","3ddfe3af":"code","7c0f1bd0":"code","2e937f00":"code","810d5edf":"code","e42285a5":"code","c8982cb2":"code","55f8fef7":"code","af632ec7":"code","08c533f5":"code","61554e59":"code","ed73f644":"code","3fee832e":"code","b5f7a396":"code","61d70982":"code","815af94f":"code","8873a8ab":"code","9d53b29b":"code","bc263349":"code","040195bc":"code","09b4c181":"code","da51064a":"code","81c6fa7f":"code","785b36e7":"code","d045f0e7":"code","179878a2":"code","21dd8454":"code","c08834cb":"code","1b2912c9":"markdown","0ecb5d6d":"markdown","a599e84f":"markdown","8d5c8cc7":"markdown","357e560a":"markdown","1b827103":"markdown","e9b5e2d4":"markdown","c41e1c1f":"markdown","96928bd3":"markdown","fef6b71c":"markdown","bd0760f4":"markdown","d5ad94b9":"markdown","d6a807f0":"markdown","8cfec7f2":"markdown","67a156f8":"markdown","551d4d82":"markdown","c4af91ae":"markdown","f1bad67d":"markdown","c4430961":"markdown","ba76c379":"markdown","ae9df43e":"markdown","720d9e47":"markdown","812ad346":"markdown","9c6b25f5":"markdown","8fc5cb56":"markdown","aea1fa3e":"markdown","b111032b":"markdown","2b8e12ce":"markdown","73c5294a":"markdown","1f4e04cd":"markdown","dc461c11":"markdown"},"source":{"256f2ba7":"#%% Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\nimport shap\nimport lightgbm as lgb\n%matplotlib inline\n\nfrom matplotlib import pyplot\nfrom pprint import pprint\nfrom IPython.display import display \nfrom sklearn import model_selection\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, roc_auc_score, precision_score\nfrom sklearn.model_selection import RandomizedSearchCV","e7cd580a":"#%% Read raw data \ndata_raw = pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')\n# Columns to be dropped\ndrop_columns = ['CLIENTNUM',\n                'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n                'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2']\n\n# Remove CLIENTNUM and the Naive_Bayes_Classifiers\ndata_raw = data_raw.drop(drop_columns,errors='ignore',axis=1)\n# Initial glance at data\ndisplay(data_raw.info(verbose = True,null_counts=False))\n# Check for missing or NaN data \nprint(\"No Missing Data in this Dataset\")\nprint(data_raw.isnull().sum())","243dd580":"#%% PlotMultiplePie \n# Input: df = Pandas dataframe, categorical_features = list of features , dropna = boolean variable to use NaN or not\n# Output: prints multiple px.pie() \n\ndef PlotMultiplePie(df,categorical_features = None,dropna = False):\n    # set a threshold of 30 unique variables, more than 50 can lead to ugly pie charts \n    threshold = 30\n    \n    # if user did not set categorical_features \n    if categorical_features == None: \n        categorical_features = df.select_dtypes(['object','category']).columns.to_list()\n        print(categorical_features)\n    \n    # loop through the list of categorical_features \n    for cat_feature in categorical_features: \n        num_unique = df[cat_feature].nunique(dropna = dropna)\n        num_missing = df[cat_feature].isna().sum()\n        # prints pie chart and info if unique values below threshold \n        if num_unique <= threshold:\n            print('Pie Chart for: ', cat_feature)\n            print('Number of Unique Values: ', num_unique)\n            print('Number of Missing Values: ', num_missing)\n            fig = px.pie(df[cat_feature].value_counts(dropna = dropna), values=cat_feature, \n                 names = df[cat_feature].value_counts(dropna = dropna).index,title = cat_feature,template='ggplot2')\n            fig.show()\n        else: \n            print('Pie Chart for ',cat_feature,' is unavailable due high number of Unique Values ')\n            print('Number of Unique Values: ', num_unique)\n            print('Number of Missing Values: ', num_missing)\n            print('\\n')","729643e3":"#%% Use PlotMultiplePie to see the distribution of the categorical variables \nPlotMultiplePie(data_raw)","3ddfe3af":"#%% Print the continous features in the dataset \ncontinous_features = data_raw.select_dtypes(['float64']).columns.to_list()\n\nfor cont_feature in continous_features: \n    plt.figure()\n    plt.title(cont_feature)\n    ax = sns.distplot(data_raw[cont_feature])","7c0f1bd0":"#%% Print the discrete features in the dataset \ndiscrete_features = data_raw.select_dtypes(['int64']).columns.to_list()\n\nfor disc_feature in discrete_features: \n    plt.figure()\n    plt.title(disc_feature)\n    ax = sns.distplot(data_raw[disc_feature],kde = False)","2e937f00":"#%% Clean dataset, and convert appropriate columns to appropriate dtypes()  \n\n# convert bool and object to category \ncat_types = ['bool','object','category']\ndata_clean = data_raw.copy()\ndata_clean[data_clean.select_dtypes(cat_types).columns] = data_clean.select_dtypes(cat_types).apply(lambda x: x.astype('category'))\n\n#     # Another method to convert bool and object to category \n# # Create a series called dtypes that has the dtype of every column in data_clean \n# dtypes = data_clean.dtypes\n# # Convert object dtypes to category dtypes \n# cat_dtypes = data_clean.dtypes[dtypes == 'object'].index.tolist()\n# data_clean[cat_dtypes] = data_clean[cat_dtypes].apply(lambda x: x.astype('category'))\n\n#%% Initial glance at cleaned data\ndata_clean.info()","810d5edf":"#%% Split data_clean into two datasets y - depedent variable, x - independent variables \n# Map Attrited Customer = 1 and Existing Customer = 0\ncodes = {'Existing Customer':0, 'Attrited Customer':1}\ndata_clean['Attrition_Flag'] = data_clean['Attrition_Flag'].map(codes)\n\ny = data_clean['Attrition_Flag']\nX = data_clean.drop('Attrition_Flag',errors='ignore',axis=1)","e42285a5":"#%% Label and One Hot Encoding on catagorical independent variables\n    # Label Encoding for ordinal variables (ex: rankings, scales, etc)\n    # One Hot Encoding for nominal variables (ex: color, gender, etc.)\n\n# print the categories for each catagorical columns \nfor col in X.select_dtypes('category').columns.to_list():\n    print(col + ': '+ str(X[col].cat.categories.to_list()))\n\n# Use One Hot Eoncoding on each catagorical independent variables \n# Because Income_Category has an \"Unknown\" value, unable to convert to an ordinal variable to use Label Encoding \n\n# https:\/\/stackoverflow.com\/questions\/37292872\/how-can-i-one-hot-encode-in-python \ndef encode_and_bind(original_dataframe, feature_to_encode):\n    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n    res = pd.concat([original_dataframe, dummies], axis=1)\n    res = res.drop([feature_to_encode], axis=1)\n    return(res) \n\nfeatures_to_encode = X.select_dtypes('category').columns.to_list()\nfor feature in features_to_encode:\n    X = encode_and_bind(X, feature)\n    \nX.info()","c8982cb2":"#%%  train-test stratified split using 80-20 \nX_train80, X_test20, y_train80, y_test20 = train_test_split(X, y, test_size=0.2, random_state = 0, shuffle= True,stratify = y)\n# By using a stratified split, the raito of 1 and 0s are consistent btwn the two splits \nprint(y_train80.value_counts())\nprint(y_test20.value_counts())","55f8fef7":"#%% Initial fit using RandomForestClassifier\nRFC = RandomForestClassifier(random_state = 0)\nRFC.fit(X_train80,y_train80)\nprint(\"Accuracy: %.2f%%\" % ((RFC.score(X_test20,y_test20))*100.0))","af632ec7":"#%% Confusion Matrix for RFC\n# https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\nnp.set_printoptions(precision=2)\n\n# Plot Both Confusion Matrix, without normalization and with normalization \ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp = plot_confusion_matrix(RFC, X_test20, y_test20,\n                                 display_labels=RFC.classes_,\n                                 cmap=plt.cm.Blues,\n                                 normalize=normalize)\n    disp.ax_.set_title(title)\n\nplt.show()","08c533f5":"#%% Calculate Precision, Recall, F1-Score, and Accurarcy \n# https:\/\/towardsdatascience.com\/accuracy-precision-recall-or-f1-331fb37c5cb9 \n\n# predictions using RFC model given X_test20 data\ny_pred20 = RFC.predict(X_test20)\n\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.precision_recall_fscore_support.html\n# precision, recall, f1-score\nprecision_recall_fscore_support(y_test20, y_pred20, average='binary',pos_label=1,beta = 1)\n\n# classification_report for Attrited Customer  \nprint(classification_report(y_test20,y_pred20))\nprint(\"Accuracy: %.2f%%\" % (accuracy_score(y_test20, y_pred20)*100.0))\nprint(\"Recall: %.2f%%\" % ((recall_score(y_test20,y_pred20))*100.0))","61554e59":"# %% Use RandomSearchCV to tune parameters for RFC \n\n# Takes a while to compute and uses a lot of CPU \n# https:\/\/stats.stackexchange.com\/questions\/186182\/a-way-to-maintain-classifiers-recall-while-improving-precision\n#model\n# https:\/\/towardsdatascience.com\/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n\nprint('Running RandomizedSearchCV')\n# default RFClassifier but set random_state = 0 to keep consistent results\nMOD = RandomForestClassifier(random_state=0) \n#Implement RandomSearchCV\n# Number of trees in random forest [100,150,...,500]\nn_estimators = [int(x) for x in np.arange(start = 100, stop = 501, step = 50)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.arange(start = 20, stop = 101, step = 20)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nscoreFunction = {\"recall\": \"recall\"}\n# run a RandomizedSearchCV with 3 folds and 25 iterations \nrandom_search = RandomizedSearchCV(MOD,\n                                   param_distributions = random_grid,\n                                   n_iter = 25,\n                                   scoring = scoreFunction,               \n                                   refit = \"recall\",\n                                   return_train_score = False,\n                                   random_state = 0,\n                                   verbose = 2,\n                                   cv = 3,\n                                   n_jobs = -1) \n\n#trains and optimizes the model\nrandom_search.fit(X_train80, y_train80)\n\nprint('Finished RandomizedSearchCV ')","ed73f644":"#%% Evaluate method to test model performance\ndef evaluate(model, test_features, test_labels):\n    predictions = model.predict(test_features)\n    print('Model Performance')\n    print(classification_report(test_labels,predictions))\n    print(\"Accuracy: %.2f%%\" % (accuracy_score(test_labels, predictions)*100.0))\n    print(\"Recall: %.2f%%\" % ((recall_score(test_labels,predictions))*100.0))","3fee832e":"#%% Evaluate the base model RFC vs. improved model RFC_search from RandomizedSearchCV\n\nprint(\"Improved Model from RandomizedSearchCV\")\nRFC_search = random_search.best_estimator_\nRFC_search.set_params(random_state=0)\npprint(RFC_search.get_params())\nevaluate(RFC_search,X_test20,y_test20)","b5f7a396":"#%% Confusion Matrix for RFC_search Improved Model \n# https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\nnp.set_printoptions(precision=2)\n\n# Plot Both Confusion Matrix, without normalization and with normalization \ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp = plot_confusion_matrix(RFC_search, X_test20, y_test20,\n                                 display_labels=RFC.classes_,\n                                 cmap=plt.cm.Blues,\n                                 normalize=normalize)\n    disp.ax_.set_title(title)\n\nplt.show()","61d70982":"#%% Feature Importance \n# https:\/\/stackoverflow.com\/questions\/41900387\/mapping-column-names-to-random-forest-feature-importances\n\nfeat_importances = pd.Series(RFC_search.feature_importances_, index=X_train80.columns)\nfeat_importances.nlargest(10).sort_values().plot(kind='barh')\nplt.title(\"Top 10 Important Features\")\nplt.show()","815af94f":"##% Feature Importance using shap package \n# Note: Takes several minutes to run \n\n# shap_values = shap.TreeExplainer(RFC_search).shap_values(X_train80)\n# shap.summary_plot(shap_values, X_train80, plot_type=\"bar\")","8873a8ab":"#%% MultiColumnLabelEncoder\n# Code snipet found on Stack Exchange \n# https:\/\/stackoverflow.com\/questions\/24458645\/label-encoding-across-multiple-columns-in-scikit-learn\n# from sklearn.preprocessing import LabelEncoder\n\nclass MultiColumnLabelEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                # convert float NaN --> string NaN\n                output[col] = output[col].fillna('NaN')\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)\n\n#%% Recreate X and y from data_raw\ny = data_raw['Attrition_Flag']\nX = data_raw.drop('Attrition_Flag',errors='ignore',axis=1)\n\n# store the catagorical features names as a list      \ncat_features = X.select_dtypes(['object']).columns.to_list()\n\n# use MultiColumnLabelEncoder to apply LabelEncoding on cat_features \n# uses NaN as a value , no imputation will be used for missing data\nX_Encoded = MultiColumnLabelEncoder(columns = cat_features).fit_transform(X)","9d53b29b":"#%% Before and After Label Encoding for X\ndisplay(X)\ndisplay(X_Encoded)","bc263349":"#%% Before and After Label Encoding for y\ny_Encoded = y.replace({'Existing Customer': 0, 'Attrited Customer': 1})\ndisplay(y)\ndisplay(y_Encoded)","040195bc":"#%%  train-test stratified split using a 80-20 split\n# drop enrollee_id for aug_train as it is a useless feature \ntrain_x, valid_x, train_y, valid_y = train_test_split(X_Encoded, y_Encoded, test_size=0.2, shuffle=True, stratify=y, random_state=0)\n\n# Create the LightGBM data containers\n# Make sure that cat_features are used\ntrain_data=lgb.Dataset(train_x,label=train_y, categorical_feature = cat_features)\nvalid_data=lgb.Dataset(valid_x,label=valid_y, categorical_feature = cat_features)\n\n#Select Hyper-Parameters\nparams = {'objective':'binary',\n          'metric' : 'auc',\n          'boosting_type' : 'gbdt',\n          'colsample_bytree' : 0.9234,\n          'num_leaves' : 13,\n          'max_depth' : -1,\n          'n_estimators' : 200,\n          'min_child_samples': 399, \n          'min_child_weight': 0.1,\n          'reg_alpha': 2,\n          'reg_lambda': 5,\n          'subsample': 0.855,\n          'verbose' : -1,\n          'num_threads' : 4\n}","09b4c181":"#%% Train model on selected parameters and number of iterations\nlgbm = lgb.train(params,\n                 train_data,\n                 2500,\n                 valid_sets=valid_data,\n                 early_stopping_rounds= 30,\n                 verbose_eval= 10\n                 )","da51064a":"#%% Overall AUC\ny_hat = lgbm.predict(X_Encoded)\nscore = roc_auc_score(y_Encoded, y_hat)\nprint(\"Overall AUC: {:.3f}\" .format(score))","81c6fa7f":"#%% ROC Curve for training\/validation data\n# https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py\ny_probas = lgbm.predict(valid_x) \nfrom sklearn.metrics import roc_curve, auc\nfpr, tpr, _ = roc_curve(valid_y, y_probas)\nroc_auc = auc(fpr, tpr)\n\nplt.figure()\nlw = 2\nplt.plot(fpr, tpr, color='darkorange',\n         lw=lw, label='ROC curve (area = %0.4f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic for training data')\nplt.legend(loc=\"lower right\")\nplt.show()","785b36e7":"# Great Function found on Kaggle for plotting a Confusion Matrix\n# https:\/\/www.kaggle.com\/grfiv4\/plot-a-confusion-matrix\ndef plot_confusion_matrix_kaggle(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n    \"\"\"\n    given a sklearn confusion matrix (cm), make a nice plot\n\n    Arguments\n    ---------\n    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n\n    target_names: given classification classes such as [0, 1, 2]\n                  the class names, for example: ['high', 'medium', 'low']\n\n    title:        the text to display at the top of the matrix\n\n    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n                  see http:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html\n                  plt.get_cmap('jet') or plt.cm.Blues\n\n    normalize:    If False, plot the raw numbers\n                  If True, plot the proportions\n\n    Usage\n    -----\n    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n                                                              # sklearn.metrics.confusion_matrix\n                          normalize    = True,                # show proportions\n                          target_names = y_labels_vals,       # list of names of the classes\n                          title        = best_estimator_name) # title of graph\n\n    Citiation\n    ---------\n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import itertools\n\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()\n\n# binarize an array based of a threshold \ndef binarizeArray(array,threshold = 0.5):\n    return [0 if num < threshold else 1 for num in array]","d045f0e7":"#%% Plot ROC curve and find best threshold to binarize the predictions from LGBM\n# https:\/\/machinelearningmastery.com\/threshold-moving-for-imbalanced-classification\/\npred_y = lgbm.predict(valid_x)\n# calculate roc curves\nfpr, tpr, thresholds = roc_curve(valid_y, pred_y)\n# calculate the g-mean for each threshold\ngmeans = np.sqrt(tpr * (1-fpr))\n# locate the index of the largest g-mean\nix = np.argmax(gmeans)\nprint('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n# plot the roc curve for the model\npyplot.figure(num=0, figsize=[6.4, 4.8])\npyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')\npyplot.plot(fpr, tpr, marker='.', label='Logistic')\npyplot.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\npyplot.legend()\n# show the plot\npyplot.show()","179878a2":"#%% Plot Confusion Matrix with best threshold\npref_y_bin = binarizeArray(pred_y,thresholds[ix])\ncm = confusion_matrix(valid_y,pref_y_bin)\nplot_confusion_matrix_kaggle(cm =cm, \n                      normalize    = False,\n                      target_names = ['Existing Customer', 'Attrited Customer'],\n                      title        = \"Confusion Matrix\")\nprint(classification_report(valid_y,pref_y_bin))\nprint(\"Accuracy: %.2f%%\" % (accuracy_score(valid_y, pref_y_bin)*100.0))\nprint(\"Recall: %.2f%%\" % ((recall_score(valid_y,pref_y_bin))*100.0))","21dd8454":"##% Feature Importance \n# https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_forest_importances.html\nlgb.plot_importance(lgbm,figsize=(15,10))","c08834cb":"##% Feature Importance using shap package \nlgbm.params['objective'] = 'binary'\nshap_values = shap.TreeExplainer(lgbm).shap_values(X_Encoded)\nshap.summary_plot(shap_values, X_Encoded)","1b2912c9":"<a id=\"Train-Test-Stratfied-Split\"><\/a>\n## Train-Test Stratfied Split","0ecb5d6d":"\n## Label Encoding ","a599e84f":"The most important features appear to be Total_Trans_Ct, Total_Trans_Amt, and Total_Revolving_Bal. It appears that numeric features compared to categorical features give more value in determining whether or not a customer churn or not. ","8d5c8cc7":"<a id=\"Prepare-Data-for-LightGBM\"><\/a>\n## Prepare Data for LightGBM","357e560a":"<a id=\"Data-Cleansing\"><\/a>\n# Data Cleansing","1b827103":"<a id=\"Notes\"><\/a>\n# Notes \n1. dataset is **imbalanced**\n2. dataset has **no missing values** \n3. dataset has **10127** customers and **19** features\n4. Seek for better **recall**, make sure you are targeting recall for Attrition_Flag = 1, many other notebooks report the wrong recall\n","e9b5e2d4":"Although **accurarcy** is 95.66%, **recall** which is 76.00% is a more important metric. To improve this metric, we use **RandomSearchCV** to hypertune the parameters. ","c41e1c1f":"<a id=\"Random-Forest-Model-Performance\"><\/a>\n## Random Forest Model Performance ","96928bd3":"<a id=\"Importing-Libraries\"><\/a>\n# Importing Libraries","fef6b71c":"Both Accurarcy and Recall has improved with **Accuracy: 96.10%** and **Recall: 78.15%**. Although this is only a slight improvement we can try other machine learning algorithims to obtain a better Recall while also preserving accuracy.","bd0760f4":"<a id=\"RandomSearchCV\"><\/a>\n## RandomSearchCV","d5ad94b9":"<a id=\"Train-LightGBM-Model\"><\/a>\n## Train LightGBM Model","d6a807f0":"<a id=\"Target\"><\/a>\n## Target \n\nAttrition_Flag: Internal event (customer activity) variable - if the account is closed then 1(Attrited Customer) else 0(Existing Customer)","8cfec7f2":"<a id=\"Conclusion\"><\/a>\n## Conclusion\n\nI wanted to update this notebook to include a table of contents, more visuals for the data, and implement LightGBM. My first try did not go well as Random Forest did not improve the model significantly. This time around by using LightGBM, we get a great model that has both high accuracy and recall. Please take your time and give a like and comment on the post. I would really appreciate it! \n\n- Joseph Chan 2021","67a156f8":"By using the best threshold we find a balance between accuracy and recall for Attrited Customers, we don't want to lower or increase the threshold as it might break the balance. If we lower the threshold, we increase recall, but the total accuracy is greatly hurt. On the other hand, if we increase the threshold, recall decreases which is something we don't want. \n\nWith LightGBM we created a model that has an Accuracy: 94.52% and Recall: 96.92%. Although accuracy has decreased a slight amount, Recall has improved tremendously. We now check for feature importance and see which features influence the models the most ","551d4d82":"<a id=\"Introduction\"><\/a>\n# Introduction \nThis notebook goes through various machine learning techniques such as Random Forest and LightGBM. This is a beginner-level notebook but I believe you will find it still useful to read and look over. Please leave comments on where I can improve and what you liked. Thank you!","c4af91ae":"<a id=\"Improved-RandomForestClassifier\"><\/a>\n## Improved RandomForestClassifier","f1bad67d":"<a id=\"Random-Forest-Confusion-Matrix\"><\/a>\n## Random Forest Confusion Matrix","c4430961":"<a id=\"Initial-Glance-at-Training-Data\"><\/a>\n## Initial Glance at Training Data","ba76c379":"## MultiColumnLabelEncoder","ae9df43e":"<a id=\"Feature-Importance-RF\"><\/a>\n## Feature Importance Random Forest","720d9e47":"<a id=\"Table-Of-Contents\"><\/a>\n# Table Of Contents\n* [Table Of Contents](#Table-Of-Contents)\n* [Introduction](#Introduction)\n* [Task Details](#Task-Details)\n* [Notes](#Notes)\n    - [Feature Description](#Feature-Description)\n    - [Target](#Target)\n* [Importing Libraries](#Importing-Libraries)\n* [Read in Data (BankChurners.csv)](#Read-in-Data)\n    - [Inital Glace at Training Data](#Initial-Glance-at-Training-Data)\n* [Data Visualization](#Data-Visualization)\n* [Data Cleansing](#Data-Cleansing)\n    - [Label Encoding](#Label-Encoding)\n    - [Train-Test Stratfied Split](#Train-Test-Stratfied-Split)\n* [RandomForestClassifier](#RandomForestClassifier)\n    - [Random Forest Confusion Matrix](#Random-Forest-Confusion-Matrix) \n    - [Random Forest Model Performance](#Random-Forest-Model-Performance)\n    - [RandomSearchCV](#RandomSearchCV)\n    - [Improved RandomForestClassifier](#Improved-RandomForestClassifier)\n    - [Feature Importance Random Forest](#Feature-Importance-RF)\n* [LightGBM Classifier ](#LightGBM-Classifier )\n    - [Prepare Data for LightGBM](#Prepare-Data-for-LightGBM)\n    - [Train LightGBM Model](#Train-LightGBM-Model)\n    - [LightGBM Model Performance](#LightGBM-Model-Performance)\n    - [Feature Importance LightGBM](#Feature-Importance-LightGBM)\n* [Conclusion](#Conclusion)","812ad346":"<a id=\"LightGBM-Model-Performance\"><\/a>\n## LightGBM Model Performance","9c6b25f5":"<a id=\"Feature-Importance-RF\"><\/a>\n# LightGBM Classifier \nRandom Forest is a great Machine Learning algorithim, but I wanted to try a different one called LightGBM that has become popular in the recent years and won many Kaggle competitions. ","8fc5cb56":"<a id=\"Task-Details\"><\/a>\n# Task Details\nOur top priority in this business problem is to identify customers who are getting churned. Even if we predict non-churning customers as churned, it won't harm our business. But predicting churning customers as Non-churning will do. So **recall** (TP\/TP+FN) need to be higher.\n\nTill now, I have managed to get a recall of 62%. Need better.","aea1fa3e":"<a id=\"Feature-Importance-LightGBM\"><\/a>\n## Feature Importance LightGBM","b111032b":"<a id=\"RandomForestClassifier\"><\/a>\n## RandomForestClassifier","2b8e12ce":"<a id=\"Data-Visualization \"><\/a>\n# Data Visualization ","73c5294a":"<a id=\"Read-in-Data\"><\/a>\n# Read in Data (BankChurners.csv)","1f4e04cd":"# <center>BankChurnersClassifier <\/center>\n<img src=\"data:image\/jpeg;base64,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\" width=\"300\" height=\"200\" align=\"center\"\/>","dc461c11":"<a id=\"Feature-Description\"><\/a>\n## Feature Description \n\nCLIENTNUM: Client number. Unique identifier for the customer holding the account\n\nCustomer_Age: Demographic variable - Customer's Age in Years\n\nGender: Demographic variable - M=Male, F=Female\n\nDependent_count: Demographic variable - Number of dependents\n\nEducation_Level: Demographic variable - Educational Qualification of the account holder (example: high school, college graduate, etc.)\n\nMarital_Status: Demographic variable - Married, Single, Divorced, Unknown\n\nIncome_Category: Demographic variable - Annual Income Category of the account holder (< $40K, $40K - 60K, $60K - $80K, $80K-$120K, > $120K, Unknown)\n\nCard_Category: Product Variable - Type of Card (Blue, Silver, Gold, Platinum)\n\nMonths_on_book: Period of relationship with bank\n\nTotal_Relationship_Count: Total no. of products held by the customer\n\nMonths_Inactive_12_mon: No. of months inactive in the last 12 months\n\nContacts_Count_12_mon: No. of Contacts in the last 12 months\n\nCredit_Limit: Credit Limit on the Credit Card\n\nTotal_Revolving_Bal: Total Revolving Balance on the Credit Card\n\nAvg_Open_To_Buy: Open to Buy Credit Line (Average of last 12 months)\n\nTotal_Amt_Chng_Q4_Q1: Change in Transaction Amount (Q4 over Q1)\n\nTotal_Trans_Amt: Total Transaction Amount (Last 12 months)\n\nTotal_Trans_Ct: Total Transaction Count (Last 12 months)\n\nTotal_Ct_Chng_Q4_Q1: Change in Transaction Count (Q4 over Q1)\n\nAvg_Utilization_Ratio: Average Card Utilization Ratio  "}}