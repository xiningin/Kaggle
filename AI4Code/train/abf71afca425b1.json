{"cell_type":{"1f14bb03":"code","0410ee8d":"code","e0988dd3":"code","040250c1":"code","8ec1286e":"code","85ae6fac":"code","985aa055":"code","a3833153":"code","3f707789":"code","4bd5d683":"code","3778ac19":"code","5bfa3a7a":"code","4591896c":"code","984c6110":"code","5d941cbb":"code","1929bce7":"code","a849bdb2":"code","874cefd3":"code","2c9cad17":"code","2daed489":"code","7ff9d2ad":"code","00da7f4d":"code","e0411afc":"code","81ef97cd":"code","c5ef20c7":"code","c7f5eafb":"code","2e92559a":"code","29786f67":"code","ccb722f9":"code","953cc8e7":"code","2c2f4388":"code","a635014d":"code","715dd278":"code","c1154265":"code","b69cc83b":"code","0911c4a0":"code","75ca9de0":"code","18110707":"code","ac1a83dc":"code","0274004e":"code","efced246":"code","15ad45bf":"code","95638642":"code","7ab452c9":"markdown","e7941d55":"markdown","e721d082":"markdown","c92b93a9":"markdown","d68dcf19":"markdown","f38caa44":"markdown","e0bb95b4":"markdown","485be7b3":"markdown","d2a7a73c":"markdown","c1fa0306":"markdown","e4d9c4c5":"markdown","bebb565d":"markdown","166bbd79":"markdown","041fa8d1":"markdown","1f277a7b":"markdown","a5dc81fe":"markdown","1fc65294":"markdown","5c6b4244":"markdown","2de36917":"markdown","4276ea44":"markdown","24d56144":"markdown","d69c3d5d":"markdown","28207d77":"markdown","c2c4c1d0":"markdown"},"source":{"1f14bb03":"# matplotlib\u3067\u65e5\u672c\u8a9e\u3092\u6271\u3048\u308b\u3088\u3046\u306b\n!pip install japanize_matplotlib -Uq\n\n# RainCloud Plot(\u6563\u5e03\u56f3\uff0bBoxPlot\uff0bViolin\u3092\u4e00\u3064\u3067\u8868\u793a)\n!pip install ptitprince -Uq\n\n# \u30d9\u30f3\u56f3\u3092\u4f5c\u6210\n!pip install matplotlib-venn -Uq","0410ee8d":"import warnings\nwarnings.simplefilter('ignore')\n\nimport os\nimport gc\ngc.enable()\nimport sys\nimport glob\nimport math\nimport time\nimport random\nimport string\nimport psutil\nimport pathlib\nfrom pathlib import Path\nfrom contextlib import contextmanager\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', 200)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport japanize_matplotlib\nfrom ptitprince import RainCloud\nfrom matplotlib_venn import venn2\n\nfrom tqdm.auto import tqdm as tqdmp\nfrom tqdm.autonotebook import tqdm as tqdm\ntqdmp.pandas()\n\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\nimport lightgbm as lgb\n\n## Model\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import datasets, models, transforms","e0988dd3":"# \u5b9f\u9a13\u3067\u4f7f\u3046\u30d1\u30e9\u30e1\u30fc\u30bf\u306fConfig\u3067\u7ba1\u7406\u3057\u3066\u3044\u307e\u3059\u3002\n# \u3053\u306e\u5b9f\u9a13\u4f55\u3084\u3063\u305f\u304b\u306a\u3068\u5f8c\u3067\u632f\u308a\u8fd4\u308a\u3084\u3059\u3044\u3088\u3046\u306b\u3001\u306a\u308b\u3079\u304fConfig\u3060\u3051\u898b\u308c\u3070\u308f\u304b\u308b\u3088\u3046\u306b\u3057\u3066\u3044\u307e\u3059\n\nclass CFG:\n    \n    def __init__(self):\n        \n        self.seed=42\n        self.n_fold = 6\n        self.trn_fold = [0,1,2,3,4,5]\n        self.environment='Kaggle'\n        self.project='Shiggle_2nd'\n        self.exp_name = '001_transformer_Baseline'\n        self.debug=True\n        \n        # model\n        self.epoch = 200\n        self.lr = 2e-4\n        self.weight_decay = 5e-5\n        \nCONFIG = CFG()","040250c1":"## \u518d\u73fe\u6027\u78ba\u4fdd\u306e\u305f\u3081\u306eSeed\u56fa\u5b9a\ndef seed_everything(seed:int==42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(CONFIG.seed)","8ec1286e":"## \u51e6\u7406\u306b\u304b\u304b\u3063\u305f\u6642\u9593\u3068\u4f7f\u7528\u3057\u305f\u30e1\u30e2\u30ea\u3092\u8a08\u6e2c\n@contextmanager\ndef timer(name:str, slack:bool=False):\n    t0 = time.time()\n    p = psutil.Process(os.getpid())\n    m0 = p.memory_info()[0] \/ 2. ** 30\n    print(f'<< {name} >> Start')\n    yield\n    \n    m1 = p.memory_info()[0] \/ 2. ** 30\n    delta = m1 - m0\n    sign = '+' if delta >= 0 else '-'\n    delta = math.fabs(delta)\n    \n    print(f\"<< {name} >> {m1:.1f}GB({sign}{delta:.1f}GB):{time.time() - t0:.1f}sec\", file=sys.stderr)","85ae6fac":"## Directory\u8a2d\u5b9a\nINPUT_DIR = Path('..\/input\/shigglecup-2nd')\nMODEL_DIR = Path('.\/')\nOUTPUT_DIR = Path('.\/')\nprint(f\"INPUT_DIR is {INPUT_DIR}\\nMODEL_DIR is {MODEL_DIR}\\nOUTPUT_DIR is {OUTPUT_DIR}\")","985aa055":"## Data Check\nfor dirnames, _, filenames in os.walk(INPUT_DIR):\n    for filename in filenames:\n        print(f'{dirnames}\/{filename}')","a3833153":"with timer('Data Load'):\n    pokemon_df = pd.read_csv(INPUT_DIR \/ 'pokemon.csv')\n    team_id_df = pd.read_csv(INPUT_DIR \/ 'team_id.csv')\n    type_df = pd.read_csv(INPUT_DIR \/ 'typetable.csv')\n    \n    train_df = pd.read_csv(INPUT_DIR \/ 'train.csv')\n    test_df = pd.read_csv(INPUT_DIR \/ 'test.csv')\n    \n    sub_df = pd.read_csv(INPUT_DIR \/ 'sample_submission.csv')\n    \n    \n    print(f'pokemon_df: {pokemon_df.shape} | team_id_df: {team_id_df.shape} | type_df: {type_df.shape}')\n    print(f'train_df: {train_df.shape} | test_df: {test_df.shape} | sub_df: {sub_df.shape}')","3f707789":"type_df = type_df.set_index('atck')","4bd5d683":"# type_df\u304b\u3089\u3001\u5404type\u306e\u653b\u6483\u671f\u5f85\u5024\u3001\u9632\u5fa1\u671f\u5f85\u5024\u3092\u6c42\u3081\u308b\nwith timer(\"\u653b\u6483\u671f\u5f85\u5024&\u9632\u5fa1\u671f\u5f85\u5024\"):\n    atack_mean_df = pd.DataFrame(\n        type_df.mean(axis=1),\n        index=type_df.index,\n        columns=[\"type_atck_expected\"]\n    )\n\n    defence_mean_df = pd.DataFrame(\n        type_df.mean(axis=0),\n        index=type_df.columns,\n        columns=['type_def_expected']\n    )","3778ac19":"with timer(\"type feature merge\"):\n    pokemon_df = pd.merge(pokemon_df, atack_mean_df,\n                          left_on=\"Type_1\",\n                          right_index=True,\n                          how=\"left\")\n    \n    pokemon_df = pd.merge(pokemon_df, defence_mean_df,\n                          left_on=\"Type_1\",\n                          right_index=True,\n                          how=\"left\")","5bfa3a7a":"def FE_pokemon_df(df:pd.DataFrame) -> pd.DataFrame:\n    \n    # \u5408\u8a08\u7a2e\u65cf\u5024\n    stats_cols = [\"HP\", \"Attack\", \"Defense\", \"Sp_Atk\", \"Sp_Def\", \"Speed\"]\n    df[\"total_stats\"] = df[stats_cols].sum(axis=1)\n    \n    # \u4f1d\u8aac\u304b\u3069\u3046\u304b\u3092int\u306b\n    df[\"Legendary\"] = df[\"Legendary\"].astype(int)\n    \n    # type_1, type_2\u3092Category_Enconding\n    labels, uniques = pd.concat([df['Type_1'], df['Type_2']], axis=0).factorize()\n    df['Type_1'] = labels[:len(df)]\n    df['Type_2'] = labels[len(df):]\n    \n    return df","4591896c":"with timer(\"FE_pokemon_df\"):\n    pokemon_df = FE_pokemon_df(pokemon_df)","984c6110":"pokemon_df.head()","5d941cbb":"def team_pokemon_merge(team_df:pd.DataFrame, pokemon_df:pd.DataFrame) -> pd.DataFrame:\n    \n    '''\n    team_id_df\u306e\u30dd\u30b1\u30e2\u30f3\u306b\u5404\u30dd\u30b1\u30e2\u30f3\u306e\u60c5\u5831\u3092\u30de\u30fc\u30b8\n    '''\n    \n    team_pokemon_id = [f\"pokemon_id_{i+1}\" for i in range(6)]\n    tq = tqdm(team_pokemon_id, total=len(team_pokemon_id))\n    \n    for i, pokemon_id in enumerate(tq):\n        \n        team_df = pd.merge(\n            left=team_df,\n            right=pokemon_df.drop(\"Name\", axis=1), # Name\u306f\u4f7f\u308f\u306a\u3044\n            left_on=pokemon_id,\n            right_on=\"pokemon_id\",\n            how=\"left\",\n            suffixes=[\"\", f\"_{pokemon_id}\"]\n        )\n        \n    return team_df","1929bce7":"with timer(\"team_pokemon_merge\"):\n    team_df = team_pokemon_merge(team_id_df, pokemon_df)\n    display(team_df.head())","a849bdb2":"stats_columns = [col for col in team_df.columns.tolist() if \"total_stats\" in col]\nprint(f\"len(stats_columns): {len(stats_columns)}\")\n\ndef calc_max_stats(row):\n    return row[stats_columns].max()\n\ndef calc_min_stats(row):\n    return row[stats_columns].min()\n\ndef calc_mean_stats(row):\n    return row[stats_columns].mean()\n\nwith timer(\"\u30c1\u30fc\u30e0\u5185\u306e\u7a2e\u65cf\u5024\/\u6700\u5927\/\u6700\u5c0f\/\u5e73\u5747\"):  \n    team_df[\"max_stats\"] = team_df.progress_apply(calc_max_stats, axis=1)\n    team_df[\"min_stats\"] = team_df.progress_apply(calc_min_stats, axis=1)\n    team_df[\"mean_stats\"] = team_df.progress_apply(calc_mean_stats, axis=1)","874cefd3":"plt.figure(figsize=(25, 6))\nsns.barplot(data=team_df, x=\"team_id\", y=\"mean_stats\")\nplt.grid()\nplt.show()","2c9cad17":"## Type\u306e\u30e6\u30cb\u30fc\u30af\u6570\ntype_columns = [col for col in team_df.columns.tolist() if \"Type_\" in col]\nprint(f\"len(type_columns): {len(type_columns)}\")\n\ndef calc_type_unique(row):\n    return row[type_columns].nunique()\n\nwith timer(\"\u5404\u30c1\u30fc\u30e0\u306e\u30bf\u30a4\u30d7\u6570\"):\n    team_df[\"type_unique\"] = team_df.apply(calc_type_unique, axis=1)","2daed489":"plt.figure(figsize=(25, 6))\nsns.barplot(data=team_df, x=\"team_id\", y=\"type_unique\")\nplt.grid()\nplt.show()","7ff9d2ad":"def combat_team_merge(df:pd.DataFrame, team_df:pd.DataFrame) -> pd.DataFrame:\n    \n    out_df = pd.merge(\n        left=df,\n        right=team_df,\n        left_on=\"first\",\n        right_on=\"team_id\",\n        how=\"left\"\n    )\n    \n    out_df = pd.merge(\n        left=out_df,\n        right=team_df,\n        left_on=\"second\",\n        right_on=\"team_id\",\n        how=\"left\",\n        suffixes=[\"_team_first\", \"_team_second\"]\n    )\n    \n    return out_df","00da7f4d":"with timer(\"team_df -> train\/test\u3078\u30de\u30fc\u30b8\"):\n    train_df = combat_team_merge(train_df, team_df)\n    test_df = combat_team_merge(test_df, team_df)","e0411afc":"## \u7279\u5fb4\u91cf\u306e\u30ab\u30e9\u30e0\u3068Target\u306e\u30ab\u30e9\u30e0\u3092\u5206\u3051\u308b\ntarget_col = [\"target\"]\nfeature_cols = [col for col in train_df.columns.tolist() if col not in target_col]\n\nprint(f\"\u7279\u5fb4\u91cf: {len(feature_cols)}\")\nprint(f\"\u76ee\u7684\u5909\u6570: {len(target_col)}\")\nprint(f\"train\u306e\u30ab\u30e9\u30e0\u6570: {train_df.shape[1]}\")","81ef97cd":"# \u6b63\u898f\u5316\n# train_df[feature_cols] = (train_df[feature_cols] - train_df[feature_cols].min(axis=0)) \/ (train_df[feature_cols].max(axis=0) - train_df[feature_cols].min(axis=0))\n# test_df[feature_cols] = (test_df[feature_cols] - test_df[feature_cols].min(axis=0)) \/ (test_df[feature_cols].max(axis=0) - test_df[feature_cols].min(axis=0))\ntrain_df[feature_cols] = (train_df[feature_cols] - train_df[feature_cols].mean(axis=0)) \/ train_df[feature_cols].std(axis=0)\ntest_df[feature_cols] = (test_df[feature_cols] - test_df[feature_cols].mean(axis=0)) \/ test_df[feature_cols].std(axis=0)","c5ef20c7":"# \u6b20\u640d\u5024\nprint(train_df[feature_cols].isnull().sum()[train_df[feature_cols].isnull().sum()!=0])\nprint(test_df[feature_cols].isnull().sum()[test_df[feature_cols].isnull().sum()!=0])","c7f5eafb":"train_df['Legendary_team_first'].unique()","2e92559a":"test_df['Legendary_team_first'] = -0.17666221\ntest_df['Legendary_pokemon_id_2_team_first'] = -0.17666221\ntest_df['Legendary_pokemon_id_6_team_first'] = -0.17666221","29786f67":"# \u6b20\u640d\u5024\nprint(test_df[feature_cols].isnull().sum()[test_df[feature_cols].isnull().sum()!=0])","ccb722f9":"class ShiggleDataset:\n\n    def __init__(self, df, first_list, feature_list, target_col, seq_len=85, is_train=True):\n\n        self.df = df\n        self.first_list = first_list\n        self.feature_list = feature_list\n        self.target = target_col\n        self.seq_len = seq_len\n        self.is_train = is_train\n\n    def __len__(self):\n        return len(self.first_list)\n\n    def __getitem__(self, idx:int):\n\n        first_id = self.first_list[idx]\n        sample = self.df[self.df['first']==first_id]\n        sample_len = len(sample)\n        \n        x = np.zeros((self.seq_len, len(self.feature_list))) # [\u5bfe\u6226\u6570, feature_list]\n        x[:sample_len, :] = sample[self.feature_list].values\n        x = torch.tensor(x, dtype=torch.float)\n\n        mask = np.zeros(self.seq_len, dtype=bool)\n        mask[:sample_len] = True\n\n        if self.is_train:\n            target = np.zeros((self.seq_len, 1)) #(\u5bfe\u6226\u6570, target)\n            target[:sample_len] = sample[self.target].values\n            target = torch.tensor(target, dtype=torch.float)\n\n            return x, mask, target\n        \n        else:\n            return x, mask","953cc8e7":"# \u78ba\u8a8d\ntrain_dataset = ShiggleDataset(\n        df=train_df.loc[:],\n        first_list = train_df.loc[:]['first'].unique(),\n        feature_list=feature_cols,\n        target_col=target_col,\n        seq_len=86,\n        is_train=True\n    )\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    batch_size=128,\n    pin_memory=True,\n    shuffle=False,\n    num_workers=1\n)\n\nfor data in train_loader:\n    break","2c2f4388":"print(f'1\u30d0\u30c3\u30c1\u306eFeature Size: {data[0].size()}')\nprint(f'1\u30d0\u30c3\u30c1\u306eMask Size: {data[1].size()}')\nprint(f'1\u30d0\u30c3\u30c1\u306eTarget Size: {data[2].size()}')\nprint(f'1\u30d0\u30c3\u30c1\u306efirst: {train_df[\"first\"].unique()[:128]}')\n\nprint(f'1\u30d0\u30c3\u30c1\u306eFeature: {data[0][0]}')\nprint(f'1\u30d0\u30c3\u30c1\u306eMask: {data[1][0]}')\nprint(f'1\u30d0\u30c3\u30c1\u306eTarget: {data[2][0]}')","a635014d":"class TransformerBlock(nn.Module):\n\n    def __init__(self, embed_dim, num_heads, rate=0.1):\n\n        super(TransformerBlock, self).__init__()\n\n        self.att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n        self.ffn = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim),\n            nn.GELU(),\n            nn.Linear(embed_dim, embed_dim)\n        )\n\n        self.layernorm1 = nn.LayerNorm(embed_dim)\n        self.layernorm2 = nn.LayerNorm(embed_dim)\n        self.dropout1 = nn.Dropout(p=0.2)\n        self.dropout2 = nn.Dropout(p=0.2)\n\n    def forward(self, q, v, k, mask):\n\n        attn_outputs, _ = self.att(q, v, k, key_padding_mask=mask)\n        attn_outputs = self.dropout1(attn_outputs.permute(1,0,2))\n\n        out1 = self.layernorm1(q.permute(1,0,2) + attn_outputs)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output)\n\n        return self.layernorm2(out1+ffn_output)","715dd278":"class ShiggleModel(nn.Module):\n\n    def __init__(\n        self,\n        num_features=192,\n        n_hidden=512,\n        n_head=8,\n        num_classes=1,\n        head_n_hidden=256\n    ):\n\n        super(ShiggleModel, self).__init__()\n        self.num_features = num_features\n        self.n_hidden = n_hidden\n        self.n_head = n_head\n        self.num_classes = num_classes\n        self.head_n_hidden = head_n_hidden\n\n        # \"EMBEDDING LAYER\"\n        self.seq_linear = nn.Sequential(\n            nn.Linear(self.num_features, self.n_hidden),\n            nn.LayerNorm(self.n_hidden),\n            nn.ReLU(),\n        )\n\n        self.transformer_block_1= TransformerBlock(embed_dim=self.n_hidden, num_heads=self.n_head)\n        self.transformer_block_2= TransformerBlock(embed_dim=self.n_hidden, num_heads=self.n_head)\n        self.transformer_block_3= TransformerBlock(embed_dim=self.n_hidden, num_heads=self.n_head)\n        self.transformer_block_4= TransformerBlock(embed_dim=self.n_hidden, num_heads=self.n_head)\n        self.transformer_block_5= TransformerBlock(embed_dim=self.n_hidden, num_heads=self.n_head)\n        self.transformer_block_6= TransformerBlock(embed_dim=self.n_hidden, num_heads=self.n_head)\n        self.transformer_block_7= TransformerBlock(embed_dim=self.n_hidden, num_heads=self.n_head)\n        self.transformer_block_8= TransformerBlock(embed_dim=self.n_hidden, num_heads=self.n_head)\n\n        self.head = nn.Sequential(\n            nn.Linear(self.n_hidden, self.head_n_hidden),\n            nn.LayerNorm(head_n_hidden),\n            nn.ReLU(),\n            nn.Dropout(p=0.2),\n            nn.Linear(self.head_n_hidden, self.num_classes),\n        )\n\n    def forward(self, x, mask):\n        x = self.seq_linear(x) # Batch_size, sequence, feature\n        x = x.permute(1,0,2)# sequence, Batch_size, feature\n\n        x_old = x\n        x = self.transformer_block_1(x, x, x, mask)\n        x = 0.8*x.permute(1,0,2) + 0.2*x_old # SKIP CONNECTION\n\n        x_old = x\n        x = self.transformer_block_2(x, x, x, mask)\n        x = 0.8*x.permute(1,0,2) + 0.2*x_old # SKIP CONNECTION\n\n        x_old = x\n        x = self.transformer_block_3(x, x, x, mask)\n        x = 0.8*x.permute(1,0,2) + 0.2*x_old # SKIP CONNECTION\n\n        x_old = x\n        x = self.transformer_block_4(x, x, x, mask)\n        x = 0.8*x.permute(1,0,2) + 0.2*x_old # SKIP CONNECTION\n\n        x_old = x\n        x = self.transformer_block_5(x, x, x, mask)\n        x = 0.8*x.permute(1,0,2) + 0.2*x_old # SKIP CONNECTION\n\n        x_old = x\n        x = self.transformer_block_6(x, x, x, mask)\n        x = 0.8*x.permute(1,0,2) + 0.2*x_old # SKIP CONNECTION\n\n        x_old = x\n        x = self.transformer_block_7(x, x, x, mask)\n        x = 0.8*x.permute(1,0,2) + 0.2*x_old # SKIP CONNECTION\n\n        x_old = x\n        x = self.transformer_block_8(x, x, x, mask)\n        x = 0.8*x.permute(1,0,2) + 0.2*x_old # SKIP CONNECTION\n\n        x = x.permute(1,0,2) # Batch_size, sequence, feature\n        x = self.head(x)\n\n        return x","c1154265":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","b69cc83b":"def train_func(model, optimizer, scheduler, loss_fn, dataloader, device, epoch):\n    \n    losses = AverageMeter()\n\n    model.train()\n#     tq = tqdm(dataloader, total=len(dataloader))\n\n    for step, data in enumerate(dataloader):\n\n        s_t = time.time()\n        optimizer.zero_grad()\n        \n        x = data[0].to(device)\n        mask = data[1].to(device)\n        target = data[2].to(device)\n        bsz = target.shape[0] # Batch Size\n        preds = model(x, mask)\n            \n        output = torch.masked_select(preds, mask.unsqueeze(-1)).view(-1, 1)\n        target = torch.masked_select(target, mask.unsqueeze(-1)).view(-1, 1)\n        \n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n\n        scheduler.step()\n        losses.update(loss.item(), bsz)\n\n        e_t = time.time() - s_t\n\n        if CONFIG.debug==False:\n            wandb.log({'epoch': epoch+1, 'fold': fold+1, \"step_loss\": loss})\n            wandb.log({'epoch': epoch+1, 'fold': fold+1, \"Learning Rate\": optimizer.param_groups[0][\"lr\"]})\n            \n#         tq.set_description(f\"Train Epoch: {epoch+1:2d} | Loss{losses.avg:.5f}\")\n\n#     tq.close()\n    _ = gc.collect()\n    \n    return losses.avg","0911c4a0":"def valid_func(model, loss_fn, dataloader, device, epoch):\n    \n    losses = AverageMeter()\n    model.eval()\n#     tq = tqdm(dataloader, total=len(dataloader))\n    valid_preds = []\n\n    for step, data in enumerate(dataloader):\n        \n        s_t = time.time()\n        x = data[0].to(device)\n        mask = data[1].to(device)\n        target = data[2].to(device)       \n        bsz = target.shape[0] # Batch Size\n        preds = model(x, mask)\n        output = torch.masked_select(preds, mask.unsqueeze(-1)).view(-1, 1)\n        target = torch.masked_select(target, mask.unsqueeze(-1)).view(-1, 1)\n        \n        loss = loss_fn(output, target)\n        e_t = time.time()- s_t\n\n        valid_preds.append(output.sigmoid().detach().cpu().numpy())\n        losses.update(loss.item(), bsz)\n#         tq.set_description(f\"Valid Epoch: {epoch+1:2d} | Loss{losses.avg:.5f}\")\n    \n#     tq.close()\n    _ = gc.collect()\n    valid_preds = np.concatenate(valid_preds)\n    \n    return losses.avg, valid_preds","75ca9de0":"import torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","18110707":"oof = np.zeros((len(train_df), 1))\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nCV = GroupKFold(n_splits=CONFIG.n_fold)\n\nfor fold, (tr, te) in enumerate(CV.split(train_df, train_df[target_col], groups=train_df[\"first\"])):\n\n    print(f'######################### Fold: {fold+1} #############################')\n    train_dataset = ShiggleDataset(\n        df=train_df.loc[tr],\n        first_list = train_df.loc[tr]['first'].unique(),\n        feature_list=feature_cols,\n        target_col=target_col,\n        seq_len=86,\n        is_train=True\n    )\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=128,\n        pin_memory=True,\n        shuffle=False,\n        num_workers=1\n    )\n\n    valid_dataset = ShiggleDataset(\n        df=train_df.loc[te],\n        first_list = train_df.loc[te]['first'].unique(),\n        feature_list=feature_cols,\n        target_col=target_col,\n        seq_len=86,\n        is_train=True\n    )\n\n    valid_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=128,\n        pin_memory=True,\n        shuffle=False,\n        num_workers=1\n    )\n\n\n    model = ShiggleModel(\n        num_features=len(feature_cols),\n        n_hidden=512,\n        n_head=8,\n        num_classes=1,\n        head_n_hidden=256\n    )\n    model.to(device)\n\n    base_optimizer = torch.optim.AdamW\n    optimizer = torch.optim.AdamW(model.parameters(),\n                                  lr=CONFIG.lr,\n                                  weight_decay=CONFIG.weight_decay)\n\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer,\n        T_max=CONFIG.epoch * len(train_loader),\n        eta_min=0,\n        last_epoch=-1\n    )\n\n    loss_tr = SmoothBCEwLogits(smoothing=0.005)\n    loss_fn = nn.BCEWithLogitsLoss()\n    best_score = 0\n\n    for epoch in range(CONFIG.epoch):\n\n            start_time = time.time()\n            train_loss = train_func(\n                model, \n                optimizer, \n                scheduler,\n                loss_fn,\n                train_loader, \n                device,\n                epoch\n            )\n            valid_loss, valid_preds = valid_func(\n                model, \n                loss_fn,\n                valid_loader, \n                device,\n                epoch\n            )\n            valid_score = roc_auc_score(train_df.loc[te, target_col], valid_preds)\n        \n            if CONFIG.debug==False:\n                wandb.log({'epoch': epoch+1, 'fold': fold+1, \"train_loss\": train_loss})\n                wandb.log({'epoch': epoch+1, 'fold': fold+1, \"valid_loss\": valid_loss})\n                wandb.log({'epoch': epoch+1, 'fold': fold+1, \"valid_score\": valid_score})\n            \n            end_time = time.time()\n            \n            if valid_score > best_score:\n                best_score = valid_score\n                oof[te] = valid_preds\n\n                MODEL_PATH = f\"{MODEL_DIR}\/{CONFIG.exp_name}_fold{fold+1}.pth\"\n                torch.save(model.state_dict(), MODEL_PATH)\n\n                print(f\"|\u2605| FOLD: {fold+1} | EPOCH:{epoch+1:02d} | train_loss:{train_loss:.6f} | valid_score:{valid_score:.6f} | Best Score:{best_score:.5f} | time:{end_time-start_time:.1f}s \")\n\n            else:\n                if epoch+1%10==0:\n                    print(f\"| | FOLD: {fold+1} | EPOCH:{epoch+1:02d} | train_loss:{train_loss:.6f} | valid_score:{valid_score:.6f} | Best Score:{best_score:.5f} | time:{end_time-start_time:.1f}s \")\n\ntotal_auc = roc_auc_score(train_df[target_col], oof)\nprint(\"\u2605\"*50)\nprint(f\"total auc: {total_auc:.5f}\")","ac1a83dc":"# OOF\u3067ROC Curve\u3092\u78ba\u8a8d\nfpr, tpr, thres = roc_curve(train_df[target_col], oof)\nplt.figure(figsize = (5, 5)) #\u5358\u4e00\u30b0\u30e9\u30d5\u306e\u5834\u5408\u306e\u30b5\u30a4\u30ba\u6bd4\u306e\u4e0e\u3048\u65b9\nplt.plot(fpr, fpr, linestyle='dashed')\nplt.plot(fpr, tpr, marker='o')\nplt.xlabel('False Positive Rete', fontsize = 13)\nplt.ylabel('True Positive Rete', fontsize = 13)\nplt.grid()\nplt.show()","0274004e":"def inference(dataloader, model, states, device):\n\n    model.to(device)\n    probs = []\n    start = end = time.time()\n\n    for step, data in enumerate(dataloader):\n    \n        x = data[0].to(device)\n        mask = data[1].to(device)\n        avg_preds = []\n\n        for state in states:\n            model.load_state_dict(state)\n            model.eval()\n            with torch.no_grad():\n                preds = model(x, mask)\n            output = torch.masked_select(preds, mask.unsqueeze(-1)).view(-1, 1)\n            avg_preds.append(output.sigmoid().detach().cpu().numpy())\n        avg_preds = np.mean(avg_preds, axis=0)\n        probs.append(avg_preds)\n\n    probs = np.concatenate(probs)\n    _ = gc.collect()\n\n    return probs","efced246":"# ====================================================\n# inference\n# ====================================================\nmodel = ShiggleModel(\n    num_features=len(feature_cols),\n    n_hidden=512,\n    n_head=8,\n    num_classes=1,\n    head_n_hidden=256\n)\n# states = [torch.load(MODEL_DIR \/ f'{CONFIG.exp_name}_fold{fold+1}.pth') for fold in CONFIG.trn_fold]\nstates = [torch.load(MODEL_DIR \/ f'{CONFIG.exp_name}_fold{fold+1}.pth') for fold in [0]]\n# test_dataset = ShiggleDataset(\n#     df=test_df.loc[:],\n#     first_list = test_df.loc[:]['first'].unique(),\n#     feature_list=feature_cols,\n#     target_col=target_col,\n#     seq_len=86,\n#     is_train=False\n# )\ntest_dataset = ShiggleDataset(\n    df=test_df,\n    first_list = test_df['first'].unique(),\n    feature_list=feature_cols,\n    target_col=target_col,\n    seq_len=86,\n    is_train=False\n)\ntest_loader = torch.utils.data.DataLoader(test_dataset, \n                                          batch_size=128, \n                                          shuffle=False, \n                                          num_workers=1, \n                                          pin_memory=True)\npred = inference(test_loader, model, states, device)","15ad45bf":"# oof, pred\u306e\u4e88\u6e2c\u5206\u5e03\u306e\u78ba\u8a8d\nplt.figure(figsize=(16, 5))\nsns.distplot(oof, label='oof')\nsns.distplot(pred, label='pred')\nplt.legend()\nplt.grid()\nplt.show()","95638642":"sub_df['target'] = pred\nsub_df.to_csv(f'.\/{CONFIG.exp_name}_CV{total_auc:.6f}_submision.csv', index=False) # submission\u30d5\u30a1\u30a4\u30eb\u540d\u306b\u5b9f\u9a13\u540d\u3084CV\u30b9\u30b3\u30a2\u3092\u5165\u308c\u3066\u304a\u304f\u3068\u3001\u5f8c\u3067Sub\u3092\u9078\u3076\u3068\u304d\u306b\u4fbf\u5229\u3067\u3059","7ab452c9":"## pokemon_df -> team_df\u3078\u30de\u30fc\u30b8","e7941d55":"# Settings","e721d082":"![\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2021-11-12 8.47.09.png](attachment:5471ca3c-75b4-4c8f-8853-5e95c8c22baf.png)","c92b93a9":"- 1epoch\u3054\u3068\u306e\u5b66\u7fd2\u3092\u5b9a\u7fa9\u3057\u307e\u3059","d68dcf19":"- transformer\u306e\u6982\u8981\u306a\u3069\u306b\u3064\u3044\u3066\u306f\u79c1\u306f\u3044\u3064\u3082\u4ee5\u4e0b\u30b5\u30a4\u30c8\u306b\u304a\u4e16\u8a71\u306b\u306a\u3063\u3066\u307e\u3059\u306e\u3067\u3001\u305d\u3061\u3089\u3092\u53c2\u8003\u306b\u3057\u3066\u307f\u3066\u304f\u3060\u3055\u3044\n- https:\/\/qiita.com\/birdwatcher\/items\/b3e4428f63f708db37b7","f38caa44":"# Make Submission\n- submission\u3092\u3059\u308b\u524d\u306b\u3001\u4e88\u6e2c\u3068\u5b9f\u969b\u306e\u5024\u3068\u306e\u78ba\u8a8d\u3084\u3001Train\u3068Test\u306e\u4e88\u6e2c\u5206\u5e03\u306e\u78ba\u8a8d\u3092\u884c\u306a\u3046\u3053\u3068\u3067\u7121\u99c4\u306aSubmission\u3092\u3057\u306a\u3044\u3088\u3046\u6c17\u3092\u3064\u3051\u307e\u3059","e0bb95b4":"- \u5404\u30c1\u30fc\u30e0\u306e\u4e2d\u3067\u3082\u7279\u5fb4\u3092\u4f5c\u3063\u3066\u307f\u307e\u3057\u3087\u3046\n  - \u4f8b\u3048\u3070\u3001\u5404\u30c1\u30fc\u30e0\u306e\u4e2d\u3067\u6700\u3082\u7a2e\u65cf\u5024\u304c\u9ad8\u3044\u3082\u306e\u3084\u7a2e\u65cf\u5024\u304c\u4f4e\u3044\u3082\u306e\u3001\u307e\u305f\u306f\u305d\u306e\u5e73\u5747\n  - type\u306e\u30e6\u30cb\u30fc\u30af\u6570\u3000\u3092\u3053\u3053\u3067\u306f\u8a66\u3057\u3066\u307f\u305f\u3044\u3068\u601d\u3044\u307e\u3059","485be7b3":"![\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2021-11-12 7.59.32.png](attachment:e24416ae-74ee-4f63-8446-f60c5049d296.png)","d2a7a73c":"![\u30b9\u30af\u30ea\u30fc\u30f3\u30b7\u30e7\u30c3\u30c8 2021-11-12 8.47.26.png](attachment:b8d486e6-36a2-4270-8844-59f7850fa8ee.png)","c1fa0306":"<img src=\"https:\/\/pbs.twimg.com\/media\/FBleFqjVEAEZa0t?format=jpg&name=large\" width=400%>\n","e4d9c4c5":"- \u306a\u3093\u304b\u30c0\u30e1\u305d\u3046w","bebb565d":"# Data Load","166bbd79":"<div class = 'alert alert-block alert-info'\n     style = 'background-color:#292952;\n              color:#e64a31;\n              border-width:5px;\n              border-color:#ffd518;\n              font-family:Comic Sans MS'>\n    <p style = 'font-size:24px'>Transformer Baseline<\/p>\n    <a href = \"#Settings\"\n       style = \"color:#e64a31;\n                font-size:14px\">1.Settings<\/a><br>\n    <a href = \"#Data-Load\"\n       style = \"color:#e64a31;\n                font-size:14px\">2.Data Load<\/a><br>\n    <a href = \"#Preprocess\"\n       style = \"color:#e64a31;\n                font-size:14px\">3.EDA<\/a><br>\n    <a href = \"#Preprocess\"\n       style = \"color:#e64a31;\n                font-size:14px\">4.Preprocess<\/a><br>\n    <a href = \"#Training\"\n       style = \"color:#e64a31;\n                font-size:14px\">5.Training<\/a><br>\n<\/div>   \n<p style = 'font-size:24px'>\n    \u30db\u30b9\u30c8\u304b\u3089\u8a17\u3055\u308c\u305f\u306e\u3067TransformerBaseline\u3092\u4f5c\u6210\u3057\u307e\u3057\u305f\n<\/p>\n    <li style = \"color:#000000;\n                font-size:14px\">\n    \u30db\u30b9\u30c8\u306eBaseline\u30b3\u30fc\u30c9\u3092\u30d9\u30fc\u30b9\u306b\u30e2\u30c7\u30eb\u90e8\u5206\u3092\u5909\u66f4\u3057\u3066\u3044\u307e\u3059\n    <\/li>\n    <li style = \"color:#000000;\n                font-size:14px\">\n    \u4fee\u6b63\u30dd\u30a4\u30f3\u30c8\u3042\u308a\u307e\u3057\u305f\u3089\u30b3\u30e1\u30f3\u30c8\u304a\u9858\u3044\u3057\u307e\u3059\n    <\/li>","041fa8d1":"# Model","1f277a7b":"- \u4f5c\u3063\u305f\u7279\u5fb4\u91cf\u3092\u898b\u3066\u307f\u307e\u3057\u3087\u3046","a5dc81fe":"## pokemon_df","1fc65294":"- \u5168\u4f53\u30d5\u30ed\u30fc\u306b\u3064\u3044\u3066\u7c21\u5358\u306b\u8aac\u660e\u3057\u307e\u3059\n- \u307e\u305a(batch_size, 85, 192)\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092embedding layer\u3067512\u6b21\u5143\u306b\u5897\u3084\u3057\u307e\u3059\n- \u6b21\u306btransformerblock\u3067attention\u306e\u8a08\u7b97\u3068FFN\u306b\u3088\u308b\u7279\u5fb4\u91cf\u5909\u63db\u3001\u305d\u308c\u3089\u306e\u8db3\u3057\u7b97\u306a\u3069\u3092\u3057\u3066\u3044\u304d\u307e\u3059\n- \u3053\u308c\u3092\u4f55\u56de\u304b\u7e70\u308a\u8fd4\u3057\u307e\u3059\u3002\u4eca\u56de\u306f8\u56de\u3057\u307e\u3059\u3002\n- \u6700\u5f8c\u306b(batch_size, 85, 1)\u306b\u3057\u3066\u5404\u5bfe\u6226\u3054\u3068\u306etarget\u3092\u7b97\u51fa\u3057\u307e\u3059","5c6b4244":"# Inference","2de36917":"# Dataset","4276ea44":"# Training","24d56144":"## type_df","d69c3d5d":"### team_df -> train\/test\u3078\u30de\u30fc\u30b8","28207d77":"# Helper Function","c2c4c1d0":"# Preprocess"}}