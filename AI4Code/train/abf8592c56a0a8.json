{"cell_type":{"270b3524":"code","a6b888af":"code","815bc965":"code","4fb4e211":"code","8fa769e8":"code","52711963":"code","f195ae6b":"code","476fcdc2":"code","6de71062":"code","17442517":"code","8fc32279":"code","f4ea60ba":"code","0c20af16":"code","03bfca56":"code","db4f5b2a":"code","fe368f16":"code","e79fb401":"code","2de4c06b":"code","59a156a4":"code","2ee78bdd":"code","12983202":"code","d74d0429":"code","31822394":"code","030a6423":"code","6e116f8c":"code","f3ff8146":"code","d6f796c9":"code","ade12d07":"markdown","90f5c932":"markdown","acc5b48f":"markdown","ae010c96":"markdown","63fe2393":"markdown","1849b132":"markdown","c55757f4":"markdown","9a5dedf2":"markdown","d8eea101":"markdown","8c6f79d9":"markdown","b1a34ade":"markdown","bd4eff25":"markdown","df12ae1e":"markdown","d2ff6dac":"markdown","d391e30d":"markdown","44fdefa9":"markdown","bd76de03":"markdown","a8ef32d6":"markdown","d1dcc06b":"markdown","aa65e930":"markdown","03cbc315":"markdown","e1146b87":"markdown"},"source":{"270b3524":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","a6b888af":"# importing libraries\n# preprocesing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# classification models\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport xgboost\n\n# metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\n\n# model tuning\nfrom sklearn.model_selection import RandomizedSearchCV","815bc965":"# reading data here, let's go\ntrain_data=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data=pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","4fb4e211":"train_data.head()","8fa769e8":"train_data.info()","52711963":"test_data.isnull().sum()","f195ae6b":"test_data['Fare']=test_data['Fare'].fillna(0)","476fcdc2":"# let's try and find null values\ntrain_data.isnull().sum()\/len(train_data)*100","6de71062":"# unique Cabin values\ntrain_data['Cabin'].unique()","17442517":"# to begin with, let's take a look at the distribution\ntrain_data['Age'].plot(kind='hist', bins=20, alpha=0.5)","8fc32279":"# median age\nprint(\"Average age:\", round(train_data['Age'].mean()))","f4ea60ba":"train_data['Embarked'].value_counts()","0c20af16":"train_data.groupby('Embarked')['Survived'].sum()","03bfca56":"train_data.info()","db4f5b2a":"# SibSp\ntrain_data.groupby('SibSp')['Survived'].sum()","fe368f16":"# Parch\ntrain_data.groupby('Parch')['Survived'].sum()","e79fb401":"def clean_data(df):\n    # drop columns\n    df=df.drop(columns=['Name', 'SibSp', 'Parch', 'PassengerId', 'Cabin', 'Ticket'])\n    \n    # impute Age\n    df['Age']=df['Age'].fillna(value=round(df['Age'].mean()))\n    \n    # remove null rows from Embarked\n    df=df[df['Embarked'].notna()]\n    \n    # let's label encode the categorical columns\n    df=pd.get_dummies(df, columns=['Sex', 'Embarked', 'Pclass'])\n    \n    return df","2de4c06b":"train_df=clean_data(train_data)\ntest_df=clean_data(test_data)","59a156a4":"ss=StandardScaler().fit(train_df[['Age', 'Fare']])\ntrain_df[['Age', 'Fare']]=ss.transform(train_df[['Age', 'Fare']])\ntest_df[['Age', 'Fare']]=ss.transform(test_df[['Age', 'Fare']])","2ee78bdd":"feature_selector=ExtraTreesClassifier()\nfeature_selector.fit(train_df.drop(columns=['Survived']), train_df['Survived'])\n\npd.DataFrame({'feature':train_df.drop(columns=['Survived']).columns, \n             'importance':feature_selector.feature_importances_}).sort_values(by='importance', ascending=False)","12983202":"train_df.drop(columns=['Embarked_S', 'Embarked_C', 'Embarked_Q'], inplace=True)\ntest_df.drop(columns=['Embarked_S', 'Embarked_C', 'Embarked_Q'], inplace=True)","d74d0429":"# model testing function\nX=train_df.drop(columns=['Survived'])\ny=train_df['Survived']\n\nlr=LogisticRegression()\ndt=DecisionTreeClassifier()\nsvc=SVC()\nknn=KNeighborsClassifier()\nrf=RandomForestClassifier()\ngb=GradientBoostingClassifier()\nxb=xgboost.XGBClassifier()\nmodels=[('Linear Regression', lr), ('Decision Tree', dt), \n        ('SVC', svc), ('KNN', knn), \n        ('Random Forest', rf), ('Gradient Boosting', gb), \n        ('Xgboost',xb)]\n\ndef model_testing(X, y, models):\n    model_names=[]\n    accuracy=[]\n    x_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=0)\n    for name, classifier in models:\n        model_names.append(name)\n        classifier.fit(x_train, y_train)\n        preds=classifier.predict(x_test)\n        accuracy.append(accuracy_score(y_test, preds))\n        \n    model_score_df=pd.DataFrame(data={'model':model_names, \n                                      'accuracy':accuracy})\n    return model_score_df","31822394":"scoring_df=model_testing(X, y, models)\nscoring_df.sort_values(by=['accuracy'], ascending=False)","030a6423":"rf=RandomForestClassifier()\nx_train, x_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=0)\nrf.fit(x_train, y_train)\nprint(\"AUC:\", round(roc_auc_score(y_test, rf.predict(x_test)),2))","6e116f8c":"rf_grid = {'n_estimators': [100, 200, 300, 400, 500],\n           'max_features': ['auto', 'sqrt'],\n           'max_depth': [7, 8, 9],\n           'min_samples_split': [2, 5, 8],\n           'min_samples_leaf': [1, 2, 4]}\n\nrf_tuned=RandomizedSearchCV(rf, param_distributions=rf_grid, random_state=0, cv=10, n_jobs=5, verbose=1)\nrf_tuned.fit(x_train, y_train)\n\nprint(\"Base model AUC: \", roc_auc_score(y_test, rf.predict(x_test)))\nprint(\"Tuned model AUC:\", roc_auc_score(y_test, rf_tuned.predict(x_test)))","f3ff8146":"rf_tuned.best_params_","d6f796c9":"output_predictions=rf_tuned.predict(test_df)\noutput=pd.DataFrame({'PassengerId':test_data.PassengerId, 'Survived':output_predictions})\noutput.to_csv(\"submission.csv\", index=False)\nprint(\"Submission successfully saved\")","ade12d07":"Clearly this is imbalanced and does not have any implication regarding survivability.","90f5c932":"Now this doesn't look too complicated, but let's see if there's any implications regarding survivability.","acc5b48f":"Not bad at all, this means that Random Forest is the highest performing model for the job.\n\nNow let's try tuning the model further to see if the performance improves.","ae010c96":"This is the same as SibSp, this columns is going to be dropped right away. Also, Name and Ticket won't add any value here either.\n\nAlright! We can now clean the data.","63fe2393":"there are 3 columns with null values:\n\nCabin\nAge\nEmbarked\nNow we could drop these columns outright, but let's see if we could impute values first.\n\nLet's begin with Cabin.","1849b132":"We can drop impute that one row with a missing fare as 0.","c55757f4":"# Cleaning Data","9a5dedf2":"I think we can use this model for predictions.","d8eea101":"****Let's take a look at the data.","8c6f79d9":"May We know what the data looks like, let's see what's happening with the data at a deeper level.","b1a34ade":"Let's drop off all the Embarked features\n\nAll done! I have a bunch of models to test and it will be cumbersome writing a lot of code to test each model, so I will be writing a function to fit these models and test them.","bd4eff25":"# Feature Selection","df12ae1e":"Data's cleaned, let's get to modelling.","d2ff6dac":"# Step 3. Submission","d391e30d":"Now the average age can be used to fill missing values based on the assumption that most people on the ship were around that age.\n\nAnd finally we come to embarked.","44fdefa9":"Hang on! We need to scale the continuous features first. I will use standard scaler for this, it's clean and simple.","bd76de03":"Clearly, Random Forest seems to be performing very well here. Not surprised because tree based models perform very well when it comes to classification problems.\n\nEven though Accuracy is a good metric, let's take a look at AUC and see if it's actually performing well.","a8ef32d6":"Very messy and none of these values seem to have any implications straight-away so we can drop this column.\n\nNow let's look at Age","d1dcc06b":"# Step 2. Model Tuning","aa65e930":"# Step 0. Data Cleaning and Pre-processing","03cbc315":"Huh, looks like a minor improvement. Let's see what parameters worked out.","e1146b87":"So it looks like it does have some implication regarding survivability, so we could drop off the missing rows instead of removing this column outright.\n\nNow that we're done with the null values, we can look at the other columns by survivability to see if we can drop them off"}}