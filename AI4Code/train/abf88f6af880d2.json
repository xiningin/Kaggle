{"cell_type":{"5f2402f9":"code","be69a62a":"code","2cd3369f":"code","b3b645c2":"code","e3f0c8d6":"code","8283ca33":"code","646dba72":"code","220b8292":"code","169f9d2b":"code","3986a407":"code","6c4d21d7":"code","4fa13ca5":"code","0dc200bb":"code","0d69325e":"code","c1b38b54":"code","c9008105":"code","ba3c7697":"code","1f747700":"code","a71a73ce":"code","c48a567d":"markdown","e80259a0":"markdown","336b2313":"markdown","dcc27747":"markdown","ad462ecb":"markdown","7828d97d":"markdown","0488c4cd":"markdown","99542bfc":"markdown","0333a251":"markdown","e1028ba8":"markdown","3783e7e2":"markdown"},"source":{"5f2402f9":"import numpy as np\nimport pandas as pd\nfrom glob import glob\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.style as style\nstyle.use('fivethirtyeight')\nfrom matplotlib.ticker import FuncFormatter\nfrom nltk.corpus import stopwords\nfrom tqdm.notebook import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\nimport spacy","be69a62a":"train = pd.read_csv('..\/input\/feedback-prize-2021\/train.csv')\ntrain[['discourse_id', 'discourse_start', 'discourse_end']] = train[['discourse_id', 'discourse_start', 'discourse_end']].astype(int)\n\nsample_submission = pd.read_csv('..\/input\/feedback-prize-2021\/sample_submission.csv')\n\n#The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell\ntrain_txt = glob('..\/input\/feedback-prize-2021\/train\/*.txt') \ntest_txt = glob('..\/input\/feedback-prize-2021\/test\/*.txt')","2cd3369f":"!cat ..\/input\/feedback-prize-2021\/train\/423A1CA112E2.txt","b3b645c2":"train.query('id == \"423A1CA112E2\"')","e3f0c8d6":"#add columns\ntrain[\"discourse_len\"] = train[\"discourse_text\"].apply(lambda x: len(x.split()))\ntrain[\"pred_len\"] = train[\"predictionstring\"].apply(lambda x: len(x.split()))\n\n\ncols_to_display = ['discourse_id', 'discourse_text', 'discourse_type','predictionstring', 'discourse_len', 'pred_len']\ntrain[cols_to_display].head()","8283ca33":"fig = plt.figure(figsize=(12,8))\n\nax1 = fig.add_subplot(211)\nax1 = train.groupby('discourse_type')['discourse_len'].mean().sort_values().plot(kind=\"barh\")\nax1.set_title(\"Average number of words versus Discourse Type\", fontsize=14, fontweight = 'bold')\nax1.set_xlabel(\"Average number of words\", fontsize = 10)\nax1.set_ylabel(\"\")\n\nax2 = fig.add_subplot(212)\nax2 = train.groupby('discourse_type')['discourse_type'].count().sort_values().plot(kind=\"barh\")\nax2.get_xaxis().set_major_formatter(FuncFormatter(lambda x, p: format(int(x), ','))) #add thousands separator\nax2.set_title(\"Frequency of Discourse Type in all essays\", fontsize=14, fontweight = 'bold')\nax2.set_xlabel(\"Frequency\", fontsize = 10)\nax2.set_ylabel(\"\")\n\nplt.tight_layout(pad=2)\nplt.show()","646dba72":"fig = plt.figure(figsize=(12,8))\nav_per_essay = train['discourse_type_num'].value_counts(ascending = True).rename_axis('discourse_type_num').reset_index(name='count')\nav_per_essay['perc'] = round((av_per_essay['count'] \/ train.id.nunique()),3)\nav_per_essay = av_per_essay.set_index('discourse_type_num')\nax = av_per_essay.query('perc > 0.03')['perc'].plot(kind=\"barh\")\nax.set_title(\"discourse_type_num: Percent present in essays\", fontsize=20, fontweight = 'bold')\nax.bar_label(ax.containers[0], label_type=\"edge\")\nax.set_xlabel(\"Percent\")\nax.set_ylabel(\"\")\nplt.show()","220b8292":"train['discourse_nr'] = 1\ncounter = 1\n\nfor i in tqdm(range(1, len(train))):\n    if train.loc[i, 'id'] == train.loc[i-1, 'id']:\n        counter += 1\n        train.loc[i, 'discourse_nr'] = counter\n    else:\n        counter = 1\n        train.loc[i, 'discourse_nr'] = counter\n        \nav_position = train.groupby('discourse_type')['discourse_nr'].mean().sort_values()\nav_position","169f9d2b":"# this code chunk is copied from Rob Mulla\nlen_dict = {}\nword_dict = {}\nfor t in tqdm(train_txt):\n    with open(t, \"r\") as txt_file:\n        myid = t.split(\"\/\")[-1].replace(\".txt\", \"\")\n        data = txt_file.read()\n        mylen = len(data.strip())\n        myword = len(data.split())\n        len_dict[myid] = mylen\n        word_dict[myid] = myword\ntrain[\"essay_len\"] = train[\"id\"].map(len_dict)\ntrain[\"essay_words\"] = train[\"id\"].map(word_dict)","3986a407":"#initialize column\ntrain['gap_length'] = np.nan\n\n#set the first one\ntrain.loc[0, 'gap_length'] = 7 #discourse start - 1 (previous end is always -1)\n\n#loop over rest\nfor i in tqdm(range(1, len(train))):\n    #gap if difference is not 1 within an essay\n    if ((train.loc[i, \"id\"] == train.loc[i-1, \"id\"])\\\n        and (train.loc[i, \"discourse_start\"] - train.loc[i-1, \"discourse_end\"] > 1)):\n        train.loc[i, 'gap_length'] = train.loc[i, \"discourse_start\"] - train.loc[i-1, \"discourse_end\"] - 2\n        #minus 2 as the previous end is always -1 and the previous start always +1\n    #gap if the first discourse of an new essay does not start at 0\n    elif ((train.loc[i, \"id\"] != train.loc[i-1, \"id\"])\\\n        and (train.loc[i, \"discourse_start\"] != 0)):\n        train.loc[i, 'gap_length'] = train.loc[i, \"discourse_start\"] -1\n\n\n #is there any text after the last discourse of an essay?\nlast_ones = train.drop_duplicates(subset=\"id\", keep='last')\nlast_ones['gap_end_length'] = np.where((last_ones.discourse_end < last_ones.essay_len),\\\n                                       (last_ones.essay_len - last_ones.discourse_end),\\\n                                       np.nan)\n\ncols_to_merge = ['id', 'discourse_id', 'gap_end_length']\ntrain = train.merge(last_ones[cols_to_merge], on = [\"id\", \"discourse_id\"], how = \"left\")","6c4d21d7":"#display an example\ncols_to_display = ['id', 'discourse_start', 'discourse_end', 'discourse_type', 'essay_len', 'gap_length', 'gap_end_length']\ntrain[cols_to_display].query('id == \"AFEC37C2D43F\"')","4fa13ca5":"#how many pieces of tekst are not used as discourses?\nprint(f\"Besides the {len(train)} discourse texts, there are {len(train.query('gap_length.notna()', engine='python'))+ len(train.query('gap_end_length.notna()', engine='python'))} pieces of text not classified.\")","0dc200bb":"all_gaps = (train.gap_length[~train.gap_length.isna()]).append((train.gap_end_length[~train.gap_end_length.isna()]), ignore_index= True)\n#filter outliers\nall_gaps = all_gaps[all_gaps<300]\nfig = plt.figure(figsize=(12,6))\nall_gaps.plot.hist(bins=100)\nplt.title(\"Histogram of gap length (gaps up to 300 characters only)\")\nplt.xticks(rotation=0)\nplt.xlabel(\"Length of gaps in characters\")\nplt.show()","0d69325e":"def add_gap_rows(essay):\n    cols_to_keep = ['discourse_start', 'discourse_end', 'discourse_type', 'gap_length', 'gap_end_length']\n    df_essay = train.query('id == @essay')[cols_to_keep].reset_index(drop = True)\n\n    #index new row\n    insert_row = len(df_essay)\n   \n    for i in range(1, len(df_essay)):          \n        if df_essay.loc[i,\"gap_length\"] >0:\n            if i == 0:\n                start = 0 #as there is no i-1 for first row\n                end = df_essay.loc[0, 'discourse_start'] -1\n                disc_type = \"Nothing\"\n                gap_end = np.nan\n                gap = np.nan\n                df_essay.loc[insert_row] = [start, end, disc_type, gap, gap_end]\n                insert_row += 1\n            else:\n                start = df_essay.loc[i-1, \"discourse_end\"] + 1\n                end = df_essay.loc[i, 'discourse_start'] -1\n                disc_type = \"Nothing\"\n                gap_end = np.nan\n                gap = np.nan\n                df_essay.loc[insert_row] = [start, end, disc_type, gap, gap_end]\n                insert_row += 1\n\n    df_essay = df_essay.sort_values(by = \"discourse_start\").reset_index(drop=True)\n\n    #add gap at end\n    if df_essay.loc[(len(df_essay)-1),'gap_end_length'] > 0:\n        start = df_essay.loc[(len(df_essay)-1), \"discourse_end\"] + 1\n        end = start + df_essay.loc[(len(df_essay)-1), 'gap_end_length']\n        disc_type = \"Nothing\"\n        gap_end = np.nan\n        gap = np.nan\n        df_essay.loc[insert_row] = [start, end, disc_type, gap, gap_end]\n        \n    return(df_essay)","c1b38b54":"add_gap_rows(\"129497C3E0FC\")","c9008105":"def print_colored_essay(essay):\n    df_essay = add_gap_rows(essay)\n    #code from https:\/\/www.kaggle.com\/odins0n\/feedback-prize-eda, but adjusted to df_essay\n    essay_file = \"..\/input\/feedback-prize-2021\/train\/\" + essay + \".txt\"\n\n    ents = []\n    for i, row in df_essay.iterrows():\n        ents.append({\n                        'start': int(row['discourse_start']), \n                         'end': int(row['discourse_end']), \n                         'label': row['discourse_type']\n                    })\n\n    with open(essay_file, 'r') as file: data = file.read()\n\n    doc2 = {\n        \"text\": data,\n        \"ents\": ents,\n    }\n\n    colors = {'Lead': '#EE11D0','Position': '#AB4DE1','Claim': '#1EDE71','Evidence': '#33FAFA','Counterclaim': '#4253C1','Concluding Statement': 'yellow','Rebuttal': 'red'}\n    options = {\"ents\": df_essay.discourse_type.unique().tolist(), \"colors\": colors}\n    spacy.displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True);","ba3c7697":"print_colored_essay(\"7330313ED3F0\")\n#print_colored_essay(\"423A1CA112E2\")","1f747700":"train['discourse_text'] = train['discourse_text'].str.lower()\n\n#get stopwords from nltk library\nstop_english = stopwords.words(\"english\")\nother_words_to_take_out = ['school', 'students', 'people', 'would', 'could', 'many']\nstop_english.extend(other_words_to_take_out)\n\n#put series of Top-10 words in dict for all discourse types\ncounts_dict = {}\nfor dt in train['discourse_type'].unique():\n    df = train.query('discourse_type == @dt')\n    text = df.discourse_text.apply(lambda x: x.split()).tolist()\n    text = [item for elem in text for item in elem]\n    df1 = pd.Series(text).value_counts().to_frame().reset_index()\n    df1.columns = ['Word', 'Frequency']\n    df1 = df1[~df1.Word.isin(stop_english)].head(10)\n    df1 = df1.set_index(\"Word\").sort_values(by = \"Frequency\", ascending = True) #to series\n    counts_dict[dt] = df1\n","a71a73ce":"plt.figure(figsize=(15, 12))\nplt.subplots_adjust(hspace=0.5)\n\nkeys = list(counts_dict.keys())\n\nfor n, key in enumerate(keys):\n    ax = plt.subplot(4, 2, n + 1)\n    ax.set_title(f\"Most used words in {key}\")\n    counts_dict[keys[n]].plot(ax=ax, kind = 'barh')\n    plt.ylabel(\"\")","c48a567d":"\u30d5\u30a3\u30fc\u30eb\u30c9discourse_type_num\u304c\u3042\u308a\u307e\u3059\u3002 Evidence1\u3001Position1\u3001Claim1\u306f\u307b\u3068\u3093\u3069\u306e\u5834\u5408\u30a8\u30c3\u30bb\u30a4\u306b\u542b\u307e\u308c\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002 \u307b\u3068\u3093\u3069\u306e\u5b66\u751f\u306f\u307e\u305f\u3001\u5c11\u306a\u304f\u3068\u30821\u3064\u306e\u7d50\u8ad6\u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\u3092\u6301\u3063\u3066\u3044\u307e\u3057\u305f\u3002 ","e80259a0":"# \u30a8\u30c3\u30bb\u30a4\u306e\u5206\u985e\u3054\u3068\u8272\u5857\u308a\n\n\u30a8\u30c3\u30bb\u30a4\u306b\u542b\u307e\u308c\u308b\u8ac7\u8a71\u3092\u5206\u985e\u3054\u3068\u306b\u8272\u5857\u308a\u3057\u307e\u3059\u3002\u7121\u5206\u985e\u306e\u7b87\u6240\u3082\u3042\u308b\u3053\u3068\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002","336b2313":"# \u6ce8\u91c8\uff08discourse_text\u3068\u3057\u3066\u4f7f\u7528\u3055\u308c\u3066\u3044\u306a\u3044\u30c6\u30ad\u30b9\u30c8\uff09\u9593\u306e\u30ae\u30e3\u30c3\u30d7\u3092\u8abf\u67fb\u3059\u308b\n\ntrain\u306e\u4e2d\u3067\u6700\u5f8c\u306ediscourse_end\u3092\u53d6\u308b\u3060\u3051\u3067\u306f\u3001\u6700\u5f8c\u306e\u30c6\u30ad\u30b9\u30c8\u304c\u8ac7\u8a71\u3068\u3057\u3066\u4f7f\u7528\u3055\u308c\u3066\u3044\u306a\u3044\u53ef\u80fd\u6027\u304c\u3042\u308b\u305f\u3081\u3001\u5b8c\u5168\u306b\u6b63\u3057\u3044\u308f\u3051\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002","dcc27747":"# \u30b3\u30f3\u30da\u306e\u7d39\u4ecb\n\n\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u306f\u30016\u5e74\u751f\u304b\u308912\u5e74\u751f\u306e\u7c73\u56fd\u306e\u5b66\u751f\u304c\u66f8\u3044\u305f\u8ad6\u4e89\u306e\u30a8\u30c3\u30bb\u30a4\u304c\u542b\u307e\u308c\u3066\u3044\u307e\u3059\u3002\u30a8\u30c3\u30bb\u30a4\u306f\u3001\u8ad6\u4e89\u7684\u306a\u57f7\u7b46\u3067\u4e00\u822c\u7684\u306b\u898b\u3089\u308c\u308b\u8981\u7d20\u306b\u3064\u3044\u3066\u5c02\u9580\u5bb6\u306e\u8a55\u4fa1\u8005\u306b\u3088\u3063\u3066\u6ce8\u91c8\u304c\u4ed8\u3051\u3089\u308c\u307e\u3057\u305f\u3002\n\n\u3053\u308c\u306f\u30b3\u30fc\u30c9\u306e\u7af6\u4e89\u3067\u3042\u308a\u3001\u76ee\u306b\u898b\u3048\u306a\u3044\u30c6\u30b9\u30c8\u30bb\u30c3\u30c8\u306b\u5bfe\u3057\u3066\u5b9f\u884c\u3055\u308c\u308b\u30b3\u30fc\u30c9\u3092\u63d0\u51fa\u3059\u308b\u3053\u3068\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u898b\u3048\u306a\u3044\u30c6\u30b9\u30c8\u30bb\u30c3\u30c8\u306f\u7d041\u4e07\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\u3067\u3059\u3002\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3092\u30c6\u30b9\u30c8\u3059\u308b\u305f\u3081\u306e\u5c0f\u3055\u306a\u516c\u958b\u30c6\u30b9\u30c8\u30b5\u30f3\u30d7\u30eb\u304c\u63d0\u4f9b\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\n\u3042\u306a\u305f\u306e\u4ed5\u4e8b\u306f\u4eba\u9593\u306e\u6ce8\u91c8\u3092\u4e88\u6e2c\u3059\u308b\u3053\u3068\u3067\u3059\u3002\u307e\u305a\u3001\u5404\u30a8\u30c3\u30bb\u30a4\u3092\u500b\u5225\u306e\u4fee\u8f9e\u7684\u8981\u7d20\u3068\u8b70\u8ad6\u7684\u8981\u7d20\uff08\u3064\u307e\u308a\u3001\u8ac7\u8a71\u8981\u7d20\uff09\u306b\u5206\u5272\u3057\u3066\u304b\u3089\u3001\u5404\u8981\u7d20\u3092\u6b21\u306e\u3044\u305a\u308c\u304b\u306b\u5206\u985e\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n\n- Lead \u30ea\u30fc\u30c9-\u7d71\u8a08\u3001\u5f15\u7528\u3001\u8aac\u660e\u3001\u307e\u305f\u306f\u8aad\u8005\u306e\u6ce8\u610f\u3092\u5f15\u304d\u3001\u8ad6\u6587\u306b\u5411\u3051\u308b\u305d\u306e\u4ed6\u306e\u30c7\u30d0\u30a4\u30b9\u3067\u59cb\u307e\u308b\u7d39\u4ecb\n- Position -\u7acb\u5834-\u4e3b\u306a\u8cea\u554f\u306b\u5bfe\u3059\u308b\u610f\u898b\u307e\u305f\u306f\u7d50\u8ad6\n- Claim -\u4e3b\u5f35-\u7acb\u5834\u3092\u652f\u6301\u3059\u308b\u4e3b\u5f35\n- Counterclaim -\u53cd\u8a34-\u5225\u306e\u4e3b\u5f35\u306b\u53cd\u8ad6\u3059\u308b\u3001\u307e\u305f\u306f\u305d\u306e\u7acb\u5834\u306b\u53cd\u5bfe\u306e\u7406\u7531\u3092\u4e0e\u3048\u308b\u4e3b\u5f35\n- Rebuttal -\u53cd\u8a34-\u53cd\u8a34\u306b\u53cd\u8ad6\u3059\u308b\u4e3b\u5f35\n- Evidence -\u8a3c\u62e0-\u4e3b\u5f35\u3001\u53cd\u8a34\u3001\u307e\u305f\u306f\u53cd\u8ad6\u3092\u88cf\u4ed8\u3051\u308b\u30a2\u30a4\u30c7\u30a2\u307e\u305f\u306f\u4f8b\u3002\n- Concluding Statement -\u7d50\u8ad6\u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8-\u30af\u30ec\u30fc\u30e0\u3092\u8a00\u3044\u63db\u3048\u308b\u7d50\u8ad6\u30b9\u30c6\u30fc\u30c8\u30e1\u30f3\u30c8\n\n\u307e\u305a\u30011\u3064\u306e\u30a8\u30c3\u30bb\u30a4\u306e\u5168\u6587\u3092\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002","ad462ecb":"However, I am also interested in the relative positions of discourse types with the essays. Therefore, I am adding this number in the loop below. I think the main takeaway is that the Lead (if it's there!) is almost always the first discourse in an essay.\n\n**More on this in the next version.**","7828d97d":"# Feedback Prize - Evaluating Student Writing\n\n\u3053\u306e\u30b3\u30f3\u30da\u306e\u8aac\u660e\u306f\u5c11\u3057\u5206\u304b\u308a\u3065\u3089\u3044\u3088\u3046\u306b\u601d\u3048\u307e\u3059\u3002\n\u51ac\u4f11\u307f\u306b\u53d6\u308a\u7d44\u307f\u305f\u3044\u65e5\u672c\u4eba\u5411\u3051\u306b\u3001\u81ea\u7fd2\u3082\u304b\u306d\u3066\u89e3\u8aac\u3057\u3066\u307f\u307e\u3057\u305f\u3002\n\n\u53c2\u8003\u306b\u306a\u3063\u305f\u65b9\u306f\u3001\u305c\u3072\u6295\u7968\u3092\u304a\u9858\u3044\u3057\u307e\u3059\u3002\n<br><br>\nI referred to the following great notebook.<br>\n<a href=\"https:\/\/www.kaggle.com\/erikbruin\/nlp-on-student-writing-eda\">NLP on Student Writing: EDA<\/a><br>\n<a href=\"https:\/\/www.kaggle.com\/odins0n\/feedback-prize-eda\">\ud83d\udd25\ud83d\udcca Feedback Prize - EDA \ud83d\udcca\ud83d\udd25<\/a>","0488c4cd":"\u4ee5\u4e0b\u306b\u3001\u5916\u308c\u5024\u3092\u53d6\u308a\u9664\u3044\u305f\u3059\u3079\u3066\u306e\u30ae\u30e3\u30c3\u30d7\u306e\u9577\u3055\u306e\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3092\u793a\u3057\u307e\u3059\uff08\u3059\u3079\u3066\u306e\u30ae\u30e3\u30c3\u30d7\u304c300\u6587\u5b57\u3088\u308a\u9577\u3044\uff09\u3002","99542bfc":"train\u30c7\u30fc\u30bf\u306f\u3001\u3053\u306e\u30a8\u30c3\u30bb\u30a4\u304b\u3089\u62bd\u51fa\u3055\u308c\u305f\u6b21\u306e\u4eba\u9593\u306e\u6ce8\u91c8\u3092\u63d0\u4f9b\u3057\u307e\u3059\u3002","0333a251":"# discourse_type\u3054\u3068\u306e\u9577\u3055\u3068\u983b\u5ea6\u304a\u3088\u3073\u76f8\u5bfe\u4f4d\u7f6e\n\n\u8ac7\u8a71\u306e\u9577\u3055\u3068\u30af\u30e9\u30b9\uff08discourse_type\uff09\u306e\u9593\u306b\u76f8\u95a2\u95a2\u4fc2\u306f\u3042\u308a\u307e\u3059\u304b\uff1f\u306f\u3044\u3042\u308a\u307e\u3059\u3002\u8a3c\u62e0\u306f\u3001\u5e73\u5747\u3057\u3066\u6700\u9577\u306e\u5272\u5f15\u30bf\u30a4\u30d7\u3067\u3059\u3002\u767a\u751f\u983b\u5ea6\u3092\u898b\u308b\u3068\u3001\u53cd\u8a34\u3068\u53cd\u8ad6\u306f\u6bd4\u8f03\u7684\u307e\u308c\u3067\u3042\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059","e1028ba8":"\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30bb\u30c3\u30c8\u306f\u3001.txt\u30d5\u30a1\u30a4\u30eb\u306e\u30d5\u30a9\u30eb\u30c0\u30fc\u5185\u306e\u500b\u3005\u306e\u30a8\u30c3\u30bb\u30a4\u3068\u3001\u3053\u308c\u3089\u306e\u30a8\u30c3\u30bb\u30a4\u306e\u6ce8\u91c8\u4ed8\u304d\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u542b\u3080.csv\u30d5\u30a1\u30a4\u30eb\u3067\u69cb\u6210\u3055\u308c\u307e\u3059\u3002 \u30a8\u30c3\u30bb\u30a4\u306e\u4e00\u90e8\u306b\u306f\u6ce8\u91c8\u304c\u4ed8\u3051\u3089\u308c\u3066\u3044\u306a\u3044\u3053\u3068\u306b\u6ce8\u610f\u3059\u308b\u3053\u3068\u304c\u91cd\u8981\u3067\u3059\uff08\u3064\u307e\u308a\u3001\u4e0a\u8a18\u306e\u5206\u985e\u306e1\u3064\u306b\u5f53\u3066\u306f\u307e\u308a\u307e\u305b\u3093\uff09\u3002\n\ntrain.csv-\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30bb\u30c3\u30c8\u5185\u306e\u3059\u3079\u3066\u306e\u30a8\u30c3\u30bb\u30a4\u306e\u6ce8\u91c8\u4ed8\u304d\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u542b\u3080.csv\u30d5\u30a1\u30a4\u30eb\n- id-\u30a8\u30c3\u30bb\u30a4\u5fdc\u7b54\u306eID\u30b3\u30fc\u30c9\n- discourse_id-\u8ac7\u8a71\u8981\u7d20\u306eID\u30b3\u30fc\u30c9\n- discourse_start-\u30a8\u30c3\u30bb\u30a4\u306e\u5fdc\u7b54\u3067\u8ac7\u8a71\u8981\u7d20\u304c\u59cb\u307e\u308b\u6587\u5b57\u306e\u4f4d\u7f6e\n- discourse_end-\u8ac7\u8a71\u8981\u7d20\u304c\u30a8\u30c3\u30bb\u30a4\u5fdc\u7b54\u3067\u7d42\u4e86\u3059\u308b\u6587\u5b57\u4f4d\u7f6e\n- discourse_text-\u8ac7\u8a71\u8981\u7d20\u306e\u30c6\u30ad\u30b9\u30c8\n- discourse_type-\u8ac7\u8a71\u8981\u7d20\u306e\u5206\u985e\n- discourse_type_num-\u8ac7\u8a71\u8981\u7d20\u306e\u5217\u6319\u3055\u308c\u305f\u30af\u30e9\u30b9\u30e9\u30d9\u30eb\n- predictionstring-\u4e88\u6e2c\u306b\u5fc5\u8981\u306a\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30b5\u30f3\u30d7\u30eb\u306e\u5358\u8a9e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\n\n\u3053\u3053\u3067\u306e\u6b63\u89e3\u306f\u3001\u8ac7\u8a71\u30bf\u30a4\u30d7\u3068\u4e88\u6e2c\u6587\u5b57\u5217\u306e\u7d44\u307f\u5408\u308f\u305b\u3067\u3059\u3002\u4e88\u6e2c\u6587\u5b57\u5217\u306f\u30a8\u30c3\u30bb\u30a4\u306e\u5358\u8a9e\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306b\u5bfe\u5fdc\u3057\u3001\u3053\u306e\u4e00\u9023\u306e\u5358\u8a9e\u306e\u4e88\u6e2c\u3055\u308c\u308b\u8ac7\u8a71\u30bf\u30a4\u30d7\u306f\u6b63\u3057\u3044\u306f\u305a\u3067\u3059\u3002\u6b63\u3057\u3044\u8ac7\u8a71\u30bf\u30a4\u30d7\u304c\u4e88\u6e2c\u3055\u308c\u3066\u3044\u308b\u304c\u3001Ground Truth\u3067\u6307\u5b9a\u3055\u308c\u3066\u3044\u308b\u3088\u308a\u3082\u9577\u3044\u307e\u305f\u306f\u77ed\u3044\u5358\u8a9e\u306e\u30b7\u30fc\u30b1\u30f3\u30b9\u3067\u3042\u308b\u5834\u5408\u3001\u90e8\u5206\u7684\u306b\u4e00\u81f4\u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\n\n\u3054\u89a7\u306e\u3068\u304a\u308a\u3001\u30a8\u30c3\u30bb\u30a4\u306e\u3059\u3079\u3066\u306e\u30c6\u30ad\u30b9\u30c8\u304c\u8ac7\u8a71\u306e\u4e00\u90e8\u3067\u3042\u308b\u3068\u306f\u9650\u308a\u307e\u305b\u3093\u3002\u3053\u306e\u5834\u5408\u3001\u30bf\u30a4\u30c8\u30eb\u306f\u8ac7\u8a71\u306e\u4e00\u90e8\u3067\u306f\u3042\u308a\u307e\u305b\u3093\u3002","3783e7e2":"# \u5206\u985e\u3054\u3068\u306b\u6700\u3082\u3088\u304f\u4f7f\u7528\u3055\u308c\u308b\u5358\u8a9e\u00b6\n\u3053\u3053\u3067\u3001\u5404discourse_type\uff08\u30b9\u30c8\u30c3\u30d7\u30ef\u30fc\u30c9\u3092\u9664\u304f\uff09\u3067\u6700\u3082\u3088\u304f\u4f7f\u7528\u3055\u308c\u3066\u3044\u308b\u5358\u8a9e\u3092\u8abf\u3079\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002 \u307e\u305f\u3001\u5404discourse_type\u306e\u56f3\u306e\u3044\u305f\u308b\u3068\u3053\u308d\u306b\u3042\u308b\u4f59\u5206\u306a\u5358\u8a9e\u3092\u3044\u304f\u3064\u304b\u53d6\u308a\u51fa\u3057\u307e\u3057\u305f\u3002"}}