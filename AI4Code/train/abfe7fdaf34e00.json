{"cell_type":{"21e06855":"code","8e643ec8":"code","124c2025":"code","a9705cae":"code","1b8e42cf":"code","df61e3a3":"code","6499a54c":"code","9fe91c2a":"code","60cc3df4":"code","b356df23":"code","88b466ff":"code","f3383ae6":"code","821da106":"markdown","5cd91d7b":"markdown","ffc3bf35":"markdown","e4ac30f4":"markdown","069b99cd":"markdown","63a8c96b":"markdown","696836a5":"markdown","9845d5d7":"markdown","2080fea3":"markdown","f8301558":"markdown","8f059515":"markdown","45c0031b":"markdown","ad1d8f68":"markdown","f1124e86":"markdown","5ab20ccc":"markdown","0901ce06":"markdown","aaee62d0":"markdown","7489cf3d":"markdown","f8bcf43a":"markdown","e3469fa6":"markdown","2a5016a4":"markdown","5e9a22d9":"markdown","3d3e8dec":"markdown","a4524e64":"markdown","1be557d7":"markdown","f838d41a":"markdown"},"source":{"21e06855":"#importing libraries\nfrom sklearn.datasets import load_boston\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n#Loading the dataset\nx = load_boston()\ndf = pd.DataFrame(x.data, columns = x.feature_names)\ndf[\"MEDV\"] = x.target\nX = df.drop(\"MEDV\",1)   #Feature Matrix\ny = df[\"MEDV\"]          #Target Variable\ndf.head()","8e643ec8":"#Using Pearson Correlation\nplt.figure(figsize=(12,10))\ncor = df.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","124c2025":"#Correlation with output variable\ncor_target = abs(cor[\"MEDV\"])\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.5]\nrelevant_features","a9705cae":"print(df[[\"LSTAT\",\"PTRATIO\"]].corr())\nprint(df[[\"RM\",\"LSTAT\"]].corr())","1b8e42cf":"#Adding constant column of ones, mandatory for sm.OLS model\nX_1 = sm.add_constant(X)\n#Fitting sm.OLS model\nmodel = sm.OLS(y,X_1).fit()\nmodel.pvalues","df61e3a3":"#Backward Elimination\ncols = list(X.columns)\npmax = 1\nwhile (len(cols)>0):\n    p= []\n    X_1 = X[cols]\n    X_1 = sm.add_constant(X_1)\n    model = sm.OLS(y,X_1).fit()\n    p = pd.Series(model.pvalues.values[1:],index = cols)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features_BE = cols\nprint(selected_features_BE)","6499a54c":"model = LinearRegression()\n#Initializing RFE model\nrfe = RFE(model, 7)\n#Transforming data using RFE\nX_rfe = rfe.fit_transform(X,y)  \n#Fitting the data to model\nmodel.fit(X_rfe,y)\nprint(rfe.support_)\nprint(rfe.ranking_)","9fe91c2a":"#no of features\nnof_list=np.arange(1,13)            \nhigh_score=0\n#Variable to store the optimum features\nnof=0           \nscore_list =[]\nfor n in range(len(nof_list)):\n    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0)\n    model = LinearRegression()\n    rfe = RFE(model,nof_list[n])\n    X_train_rfe = rfe.fit_transform(X_train,y_train)\n    X_test_rfe = rfe.transform(X_test)\n    model.fit(X_train_rfe,y_train)\n    score = model.score(X_test_rfe,y_test)\n    score_list.append(score)\n    if(score>high_score):\n        high_score = score\n        nof = nof_list[n]\nprint(\"Optimum number of features: %d\" %nof)\nprint(\"Score with %d features: %f\"%(nof, high_score))","60cc3df4":"cols = list(X.columns)\nmodel = LinearRegression()\n#Initializing RFE model\nrfe = RFE(model, 10)             \n#Transforming data using RFE\nX_rfe = rfe.fit_transform(X,y)  \n#Fitting the data to model\nmodel.fit(X_rfe,y)              \ntemp = pd.Series(rfe.support_,index = cols)\nselected_features_rfe = temp[temp==True].index\nprint(selected_features_rfe)","b356df23":"reg = LassoCV()\nreg.fit(X, y)\nprint(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\nprint(\"Best score using built-in LassoCV: %f\" %reg.score(X,y))\ncoef = pd.Series(reg.coef_, index = X.columns)","88b466ff":"print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","f3383ae6":"imp_coef = coef.sort_values()\nimport matplotlib\nmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Feature importance using Lasso Model\")","821da106":"There are different wrapper methods such as Backward Elimination, Forward Selection, Bidirectional Elimination and RFE.","5cd91d7b":"We do that by using loop starting with 1 feature and going up to 13. We then take the one for which the accuracy is highest.","ffc3bf35":"we took LinearRegression model with 7 features and RFE gave feature ranking as above, but the selection of number \u20187\u2019 was random. Now we need to find the optimum number of features, for which the accuracy is the highest.","e4ac30f4":"As we can see, only the features RM, PTRATIO and LSTAT are highly correlated with the output variable MEDV. Hence we will drop all other features apart from these. However this is not the end of the process.","069b99cd":"This model is used for performing linear regression.","63a8c96b":"#### 3) Embedded Method","696836a5":"We will be using the built-in Boston dataset which can be loaded through sklearn. We will be selecting features using the above listed methods for the regression problem of predicting the \u201cMEDV\u201d column. In the following code snippet, we will import all the required libraries and load the dataset.","9845d5d7":"we will do feature selection using Lasso regularization. If the feature is irrelevant, lasso penalizes it\u2019s coefficient and make it 0. Hence the features with coefficient = 0 are removed and the rest are taken.","2080fea3":"## Linear Regression: Feature selection","f8301558":"the optimum number of features is 10. We now feed 10 as number of features to RFE and get the final set of features given by RFE method","8f059515":"Feature selection can be done in multiple ways but there are broadly 3 categories of it:\n1. Filter Method \n2. Wrapper Method \n3. Embedded Method","45c0031b":"### 1) Filter Method","ad1d8f68":"Embedded methods are iterative in a sense that takes care of each iteration of the model training process and carefully extract those features which contribute the most to the training for a particular iteration. Regularization methods are the most commonly used embedded methods which penalize a feature given a coefficient threshold.","f1124e86":"#### i) Backward Elimination","5ab20ccc":"Lasso model has taken all the features except NOX, CHAS and INDUS.","0901ce06":"The correlation coefficient has values between -1 to 1\n\u2014 A value closer to 0 implies weaker correlation (exact 0 implying no correlation)\n\u2014 A value closer to 1 implies stronger positive correlation\n\u2014 A value closer to -1 implies stronger negative correlation","aaee62d0":"the variable \u2018AGE\u2019 has highest pvalue of 0.9582293 which is greater than 0.05. Hence we will remove this feature and build the model once again. This is an iterative process and can be performed at once with the help of loop.","7489cf3d":"#### ii) Recursive feature Elimination","f8bcf43a":"from the above code, it is seen that the variables RM and LSTAT are highly correlated with each other (-0.613808). Hence we would keep only one variable and drop the other. We will keep LSTAT since its correlation with MEDV is higher than that of RM.\nAfter dropping RM, we are left with two feature, LSTAT and PTRATIO. These are the final features given by Pearson correlation.","e3469fa6":"Feature selection is one of the first and important steps while performing any machine learning task. A feature in case of a dataset simply means a column. When we get any dataset, not necessarily every column (feature) is going to have an impact on the output variable. If we add these irrelevant features in the model, it will just make the model worst (Garbage In Garbage Out). This gives rise to the need of doing feature selection.","2a5016a4":"Here we will first plot the Pearson correlation heatmap and see the correlation of independent variables with the output variable MEDV. We will only select features which has correlation of above 0.5 (taking absolute value) with the output variable.","5e9a22d9":"### 2) Wrapper Method","3d3e8dec":"As the name suggest, we feed all the possible features to the model at first. We check the performance of the model and then iteratively remove the worst performing features one by one till the overall performance of the model comes in acceptable range.","a4524e64":"The Recursive Feature Elimination (RFE) method works by recursively removing attributes and building a model on those attributes that remain. It uses accuracy metric to rank the feature according to their importance. The RFE method takes the model to be used and the number of required features as input. It then gives the ranking of all the variables, 1 being most important. It also gives its support, True being relevant feature and False being irrelevant feature.","1be557d7":"This approach is implemented below, which would give the final set of variables which are CRIM, ZN, CHAS, NOX, RM, DIS, RAD, TAX, PTRATIO, B and LSTAT","f838d41a":"The performance metric used here to evaluate feature performance is pvalue. If the pvalue is above 0.05 then we remove the feature, else we keep it.\nWe will first run one iteration here just to get an idea of the concept and then we will run the same code in a loop, which will give the final set of features. Here we are using OLS model which stands for \u201cOrdinary Least Squares\u201d."}}