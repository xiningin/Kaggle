{"cell_type":{"21971785":"code","d347781a":"code","2aecb927":"code","bd1c51cd":"code","dda06df4":"code","03cbe80f":"code","6ca5235d":"code","d23cc5f3":"code","dbbdf5ed":"code","c07e3345":"code","84801863":"code","edeb14b3":"code","084e9832":"code","6401a425":"code","26cb8f49":"code","4659f152":"code","7a0c2a77":"code","26fdf2ad":"code","8395c60e":"code","818eb5f2":"code","9af007c9":"code","9ae33a10":"code","cdc6a0f4":"code","1ca0105d":"code","834e20dd":"code","0bad29c9":"code","7f39981e":"code","19f9fa1f":"code","cb959dbf":"code","11b29574":"code","6752a0b8":"code","4baee14e":"code","b4595ec2":"code","733f96df":"code","dd595b17":"markdown","815ff874":"markdown","edf305aa":"markdown","d1f92408":"markdown","3d64f311":"markdown","927340c0":"markdown","51e507e7":"markdown","3620e4dd":"markdown","e73781df":"markdown","b4fd6dc1":"markdown","c4f5a4e6":"markdown","c5fa1eff":"markdown","67e12d2e":"markdown","06cd9af5":"markdown","22db37cf":"markdown","babf6dc9":"markdown","b21115ff":"markdown","90cbb0f3":"markdown","4021cea9":"markdown","bc78bb62":"markdown","92eaead9":"markdown","fc276c9f":"markdown","6b7643bf":"markdown","a461b0cc":"markdown","4fd7456c":"markdown","495e9f42":"markdown","68a39774":"markdown"},"source":{"21971785":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport matplotlib.style as style\nimport seaborn as sns\nfrom itertools import product, combinations\nimport gc\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\nrand_state = 719","d347781a":"data_path = '\/kaggle\/input\/learn-together\/'\ndef reload(x):\n    return pd.read_csv(data_path + x, index_col = 'Id')\n\ntrain = reload('train.csv')\nn_train = len(train)\ntest = reload('test.csv')\nn_test = len(test)\n\nall_data = train.iloc[:,train.columns != 'Cover_Type'].append(test)\nall_data['train'] = [1]*n_train + [0]*n_test","2aecb927":"print('Shape of train and test sets: {0}, {1}'.format(train.shape, test.shape))\nprint('Number of NaN values in train: {}'.format(train.isna().sum().sum()))\nprint('Number of NaN values in test: {}'.format(test.isna().sum().sum()))","bd1c51cd":"def data_count(df):\n    sns.set_style('whitegrid')\n    plt.figure(figsize = (16,8))\n    count = df.nunique().sort_values(ascending=False)\n    f = sns.barplot(x=count.index, y=count)\n    f.set_xticklabels(labels = count.index, rotation=90)\n    for i,v in enumerate(count):\n        plt.text(x=i-0.2, y=v+len(str(v))*35, s=v, rotation=90)\n    plt.ylabel('Count')\n    plt.title('Unique data count')\n    plt.show()\n    \ndata_count(train.iloc[:,0:-1]) # except target column","dda06df4":"numerical = ['Elevation', 'Horizontal_Distance_To_Hydrology',\n             'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n             'Horizontal_Distance_To_Fire_Points',\n             'Aspect', 'Slope', \n             'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\n\ncategorical = ['Soil_Type{}'.format(i) for i in range(1,41)] + ['Wilderness_Area{}'.format(i) for i in range(1,5)]\nprint('Train data numerical features:')\ntrain[numerical].describe().T","03cbe80f":"print('Test data numerical features:')\ntest[numerical].describe().T","6ca5235d":"print('Correlation matrix of continuous features in the training set:')\ncorr = train[numerical + ['Cover_Type']].corr()\nplt.figure(figsize = (7,7))\nsns.heatmap(corr, annot = True, vmin = -1, vmax = 1, annot_kws = {\"fontsize\":8})\nplt.show()","d23cc5f3":"def distplot(df, columns, colors=['red', 'green', 'blue', 'c', 'purple'], bins_num = None, hist = True, kde = False): \n    # df is either dataframe or list of ('name_df',df)\n    # col is either string or list\n    sns.set_style('whitegrid')\n#### CONVERT INPUT DATA'S TYPE\n    if type(df) != list: \n        df = [('df',df)] \n    if type(columns) == str: \n        columns = [columns]\n    l_col = len(columns)\n    l_df = len(df)\n###### CALCULATE ROWS AND COLS OF GRAPHS\n    c = min([l_col, 3]) # cols\n    r = l_col\/\/3 + sum([l_col%3!=0]) # rows\n    fig = plt.figure(figsize=(c*7, r*6))\n    \n    for index in range(l_col):\n        column = columns[index]\n####### CALCULATE BINS OF HIST\n        if bins_num == None: \n            combined_data = np.hstack(tuple([df[x][1][column] for x in range(l_df)])) \n            n_bins = min(50,len(np.unique(combined_data))) # number of bins: <= 50\n            bins = np.histogram(combined_data, bins=n_bins)[1] # get \"edge\" of each bin\n        bins = next(b for b in [bins_num, bins] if b is not None)\n####### ADD SUBPLOT AND PLOT\n        ax = fig.add_subplot(r,c,index+1) \n        for i in range(l_df):\n            sns.distplot(df[i][1][column], bins=bins, hist = hist, kde=kde, color=colors[i], \n                         label=df[i][0], norm_hist=True, hist_kws={'alpha':0.4})\n        plt.xlabel(column)\n        if (l_df>1) & ((index+1) % c == 0): # legend at the graph on the right\n            ax.legend()\n    plt.tight_layout()\n    plt.show() ","dbbdf5ed":"distplot([('train',train), ('test',test)], columns = numerical)","c07e3345":"# count % of samples that are zero\ncols_w0 = []\nfor col in numerical:\n    if min(train[col]) <= 0:\n        cols_w0.append(col)\ninitial_values = [0]*len(cols_w0)\nzero_counts = pd.DataFrame(index = cols_w0)\nfor df, col in product(['train','test', 'all_data'], cols_w0):\n    zero_counts.loc[col, '{}_0_count'.format(df)] = eval('len({0}[{0}.{1} == 0])'.format(df,col))\n    zero_counts.loc[col, '{}_0_portion'.format(df)] = eval('sum({0}.{1}==0)\/len({0}.{1})'.format(df, col))\n    zero_counts.loc[col, '1\/{}_nunique'.format(df)] = round(eval('{0}.{1}.nunique()'.format(df, col)) ** (-1), 6)\n    \nzero_counts","84801863":"questionable_0 = ['Hillshade_9am', 'Hillshade_3pm'] # Hillshade_3pm visualization looks weird\ndistplot(all_data, questionable_0)","edeb14b3":"zero_counts.loc[questionable_0,:]","084e9832":"for col in questionable_0:\n    all_data_0 = all_data[all_data[col] == 0].copy()\n    all_data_non0 = all_data[all_data[col] != 0].copy()\n    corr = all_data_non0.corr()[col]\n    corr = np.abs(corr[corr.index != col]).sort_values(ascending = False)\n    \n    sns.set_style('whitegrid')\n    plt.figure(figsize = (17,4))\n    fig = sns.barplot(x=corr.index, y=corr)\n    fig.set_xticklabels(labels = corr.index, rotation=90)\n#     for i,v in enumerate(corr):\n#         plt.text(x=i-0.2, y=v+len(str(v))*35, s=v, rotation=90)\n    plt.ylabel('Correlation')\n    plt.title(col)\n    plt.show()","6401a425":"corr_cols = {'Hillshade_9am': ['Hillshade_3pm', 'Aspect', 'Slope', 'Soil_Type10', 'Wilderness_Area1',\n                   'Wilderness_Area4', 'Vertical_Distance_To_Hydrology'],\n             'Hillshade_3pm': ['Hillshade_9am', 'Hillshade_Noon', 'Slope', 'Aspect']\n            }","26cb8f49":"rfr = RandomForestRegressor(n_estimators = 100, random_state = rand_state, verbose = 0, n_jobs = -1)\n\n# for col in questionable_0: \n#     print('='*20)\n#     scores = cross_val_score(rfr,\n#                              all_data_non0[corr_cols[col]], \n#                              all_data_non0[col],\n#                              n_jobs = -1)\n#     print(col + ': {0:.4} (+\/- {1:.4}) ## [{2}]'.format(scores.mean(), scores.std()*2, ', '.join(map(str, np.round(scores,4)))))\n\n# ===========OUTPUT=====================\n# ====================\n# Hillshade_9am: 1.0 (+\/- 0.00056) ## [0.9995, 0.9993, 0.9988]\n# ====================\n# Hillshade_3pm: 1.0 (+\/- 0.0029) ## [0.9981, 0.9971, 0.9947]\n\n## NEAR PERFECT SCORES FOR ALL => no need further feature engineering for questionable_0 predictions","4659f152":"summary = pd.DataFrame(index = train.describe().index)\n\nfor col in questionable_0:\n    print('='*20)\n    print(col, end='')\n    all_data_0 = all_data[all_data[col] == 0].copy()\n    all_data_non0 = all_data[all_data[col] != 0].copy()\n    rfr.fit(all_data_non0[corr_cols[col]], all_data_non0[col])\n    pred = rfr.predict(all_data_0[corr_cols[col]])\n    pred_col = 'predicted_{}'.format(col)\n    \n    all_data[pred_col] = all_data[col].copy()\n    all_data.loc[all_data_0.index, pred_col] = pred\n    summary[pred_col] = all_data[all_data[col] == 0][[pred_col]].describe()\n    print(': finished!')\n\nsummary","7a0c2a77":"for col in questionable_0:\n    all_data['predicted_{}'.format(col)] = all_data['predicted_{}'.format(col)].apply(int)","26fdf2ad":"def scatterplot(df, x, y, title='', size=None, color = [None, None, None], cm=None, alpha=0.5):\n    style.use('fivethirtyeight')\n    plt.figure(figsize = (7,5))\n    if color[0] != None: # color must be list [color_column, vmin, vmax] with [vmin, vmax] = range of color bar\n        color_column = df[color[0]]\n        if color[1] == None:\n            color[1] = df[color[0]].min()\n        if color[2] == None:\n            color[2] = df[color[0]].max()\n        if cm == None:\n            cm = plt.cm.get_cmap('twilight')\n    else:\n        color_column = color[0]\n    if size != None:\n        scaler = StandardScaler()\n        scaler.fit(train[[size]])\n        size = scaler.transform(train[[size]])*200\n    plot = plt.scatter(df[x], df[y], s=size, c=color_column, alpha = alpha, \n                       cmap = cm, vmin=color[1], vmax=color[2])\n    if color[0] != None:\n        bar = plt.colorbar(plot)\n        bar.set_label(color[0])\n    plt.xlabel(x)\n    plt.ylabel(y)\n    plt.title(title, fontsize = 18)\n    plt.show()","8395c60e":"for col in ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']:\n    scatterplot(df=train, x='Aspect', y='Slope', color=[col,0,254], cm=plt.cm.get_cmap('plasma'), alpha = 0.5)","818eb5f2":"train['Hillshade_Mean'] = train[['Hillshade_9am',\n                              'Hillshade_Noon',\n                              'Hillshade_3pm']].apply(np.mean, axis = 1)\nscatterplot(df=train, x='Aspect', y='Slope', \n                color=['Hillshade_Mean',0,254], cm=plt.cm.get_cmap('plasma'), alpha = 0.5)","9af007c9":"train['SlopeSin_Elevation'] = np.sin(np.radians(train.Slope)) * train.Elevation\nfor col in ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']:\n    scatterplot(df=train, x='Aspect', y='SlopeSin_Elevation', color=[col,0,254], cm=plt.cm.get_cmap('plasma'), alpha = 0.5)","9ae33a10":"def aspect_slope(df):\n    df['AspectSin'] = np.sin(np.radians(df.Aspect))\n    df['AspectCos'] = np.cos(np.radians(df.Aspect))\n    df['AspectSin_Slope'] = df.AspectSin * df.Slope\n    df['AspectCos_Slope'] = df.AspectCos * df.Slope\n    df['AspectSin_Slope_Abs'] = np.abs(df.AspectSin_Slope)\n    df['AspectCos_Slope_Abs'] = np.abs(df.AspectCos_Slope)\n    df['Hillshade_Mean'] = df[['Hillshade_9am',\n                              'Hillshade_Noon',\n                              'Hillshade_3pm']].apply(np.mean, axis = 1)\n    return df\n\ntrain = aspect_slope(train)","cdc6a0f4":"for col in ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Hillshade_Mean']:\n    scatterplot(df=train, x='AspectSin_Slope', y=col, alpha = 0.5)","1ca0105d":"for col in ['Aspect', 'Slope','AspectSin_Slope', 'AspectCos_Slope']:\n    sns.set(style='whitegrid')\n    f = plt.figure(figsize=(8,6))\n    ax = sns.boxplot(x='Cover_Type', y=col, data=train)\n    plt.show()","834e20dd":"def distances(df):\n    horizontal = ['Horizontal_Distance_To_Fire_Points', \n                  'Horizontal_Distance_To_Roadways',\n                  'Horizontal_Distance_To_Hydrology']\n    \n    df['Euclidean_to_Hydrology'] = np.sqrt(df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2)\n    df['EuclidHydro_Slope'] = df.Euclidean_to_Hydrology * df.Slope\n    df['Elevation_VDH_sum'] = df.Elevation + df.Vertical_Distance_To_Hydrology\n    df['Elevation_VDH_diff'] = df.Elevation - df.Vertical_Distance_To_Hydrology\n    df['Elevation_2'] = df.Elevation**2\n    df['Elevation_3'] = df.Elevation**3\n    df['Elevation_log1p'] = np.log1p(df.Elevation) # credit: https:\/\/www.kaggle.com\/evimarp\/top-6-roosevelt-national-forest-competition\/notebook\n    \n    for col1, col2 in combinations(zip(horizontal, ['HDFP', 'HDR', 'HDH']), 2):\n        df['{0}_{1}_diff'.format(col1[1], col2[1])] = df[col1[0]] - df[col2[0]]\n        df['{0}_{1}_sum'.format(col1[1], col2[1])] = df[col1[0]] + df[col2[0]]\n    \n    df['Horizontal_sum'] = df[horizontal].sum(axis = 1)\n    return df\n\ntrain = distances(train)","0bad29c9":"distance_cols = ['Elevation', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n            'Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points']\n\nfor col in ['Elevation', 'Elevation_2', 'Elevation_3']:\n    f = plt.figure(figsize=(8,6))\n    ax = sns.boxplot(x='Cover_Type', y=col, data=train)\n    plt.show()","7f39981e":"def OHE_to_cat(df, colname, data_range): # data_range = [min_index, max_index+1]\n    df[colname] = sum([i * df[colname + '{}'.format(i)] for i in range(data_range[0], data_range[1])])\n    return df\n\ntrain = OHE_to_cat(train, 'Wilderness_Area',  [1,5])\ntest = OHE_to_cat(test, 'Wilderness_Area', [1,5])\n","19f9fa1f":"counts = train.groupby('Cover_Type')['Wilderness_Area'].value_counts().sort_index().unstack(level=1).fillna(0)\n\nplt.figure(figsize=(6,8))\nsns.heatmap(counts\/100, annot=True)","cb959dbf":"soils = [\n    [7, 15, 8, 14, 16, 17,\n     19, 20, 21, 23], #unknow and complex \n    [3, 4, 5, 10, 11, 13],   # rubbly\n    [6, 12],    # stony\n    [2, 9, 18, 26],      # very stony\n    [1, 24, 25, 27, 28, 29, 30,\n     31, 32, 33, 34, 36, 37, 38, \n     39, 40, 22, 35], # extremely stony and bouldery\n]\nsoil_dict = {}\nfor index, soil_group in enumerate(soils):\n    for soil in soil_group:\n        soil_dict[soil] = index\n\ndef rocky(df):\n    df['Rocky'] = sum(i * df['Soil_Type' + str(i)] for i in range(1,41))\n    df['Rocky'] = df['Rocky'].map(soil_dict)\n    return df","11b29574":"# Data setup\nall_data = aspect_slope(all_data)\nall_data = distances(all_data)\nall_data = OHE_to_cat(all_data, 'Wilderness_Area', [1,5])\nall_data = OHE_to_cat(all_data, 'Soil_Type', [1,41])\nall_data = rocky(all_data)\nall_data.drop(['Soil_Type7', 'Soil_Type15', 'train'] + questionable_0, axis = 1, inplace = True)","6752a0b8":"X_train = all_data.iloc[:n_train,:].copy()\ny_train = train.Cover_Type.copy()\nX_test = all_data.iloc[n_train:, :].copy()\n\ndef mem_reduce(df):\n    # credit: https:\/\/www.kaggle.com\/arateris\/2-layer-k-fold-learning-forest-cover\n    start_mem = df.memory_usage().sum() \/ 1024.0**2\n    for col in df.columns:\n        if df[col].dtype=='float64': \n            df[col] = df[col].astype('float32')\n        if df[col].dtype=='int64': \n            if df[col].max()<1: df[col] = df[col].astype(bool)\n            elif df[col].max()<128: df[col] = df[col].astype('int8')\n            elif df[col].max()<32768: df[col] = df[col].astype('int16')\n            else: df[col] = df[col].astype('int32')\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Reduce from {0:.3f} MB to {1:.3f} MB (decrease by {2:.2f}%)'.format(start_mem, end_mem, \n                                                                               (start_mem - end_mem)\/start_mem*100))\n    return df\n\nX_train = mem_reduce(X_train)\nprint('='*10)\nX_test=mem_reduce(X_test)\ngc.collect()","4baee14e":"rfc = RandomForestClassifier(n_estimators = 719,\n                             max_depth = 464,\n                             max_features = 0.3,\n                             min_samples_split = 2,\n                             min_samples_leaf = 1,\n                             bootstrap = False,\n                             verbose = 0,\n                             random_state = rand_state,\n                             n_jobs = -1)\n# scores = cross_val_score(rfc\n#                         , X_train\n#                         , y_train\n#                         , scoring = 'accuracy'\n#                         , cv = 5\n#                         , n_jobs = -1\n#                         )\n\n# print('scores: {0:.4} (+\/- {1:.4}) ## [{2}]'.format(scores.mean(), \n#                                                     scores.std()*2, ', '.join(map(str, np.round(scores,4)))))\n# # scores: 0.8097 (+\/- 0.06924) ## [0.788, 0.7894, 0.7867, 0.8069, 0.8773]","b4595ec2":"rfc.fit(X_train, y_train)\n# predict = rfc.predict(X_test)\n\n# output = pd.DataFrame({'Id': test.index,\n#                       'Cover_Type': predict})\n# output.to_csv('Submission.csv', index=False)","733f96df":"importance_threshold = 0.003\n# PLOT FEATURES' IMPORTANCES\nimportances = pd.DataFrame({'Features': X_train.columns, \n                                'Importances': rfc.feature_importances_})\nmpl.rcParams.update(mpl.rcParamsDefault)\nfig = plt.figure(figsize=(8,14))\nsns.barplot(x='Importances', y='Features', data=importances.sort_values(by=['Importances']\n                                                                       , axis = 'index'\n                                                                       , ascending = False)\n           , orient=\"h\")\nplt.xticks(rotation='vertical')\nplt.show()\n\n# EXTRACT IMPORTANT FEATURES\nimportant_cols = importances[importances.Importances >= importance_threshold].Features\nprint('Important features:')\nprint([col for col in important_cols])","dd595b17":"## 1. Model Setup\nThe model chosen is `RandomForestClassfier`. Credits go to @jakelj for hyper-parameters tuning in __[this notebook](https:\/\/www.kaggle.com\/jakelj\/basic-ensemble-model)__.","815ff874":"## 2. Feature Importance\nDropping features with importance values under an arbitrary `importance_threshold`. In this case, `importance_threshold` is set to 0.003.","edf305aa":"# I. Package and Data Loading","d1f92408":"## 5. Rockiness\nSimilar to `Elevation` and `Distance`, `Soil_Type` cohort has been incorporated in many previous kernel as a way to reduce number of categorical features and condense information. You can view @evimarp 's [notebook](https:\/\/www.kaggle.com\/evimarp\/top-6-roosevelt-national-forest-competition#Feautures-importances) or @kwabenantim 's [work](https:\/\/www.kaggle.com\/kwabenantim\/forest-cover-stacking-multiple-classifiers) for more information.","3d64f311":"Indeed, the spike at 0 of *Hillshade_3pm* looks a little weird. Let's see which other features also contain 0s.","927340c0":"### 1.1. Features with 0s\nInitially I considered 0 values in both train and test dataset. My initial thought was that if data entry errors were really to happen it would affect both of the datasets. However, I was also worried that this would cause data leakage if I use information in BOTH train and test sets to impute their data. Therefore, I decided to look at 0s in the train set only and then use data in both train and test to impute. My hope is that this would reduce the chance of data leakage, but this could still be technically wrong, so please let me know what you think in the comment section.","51e507e7":"The graphs above show that on average hillshade\/sunlight is generally highest around 150-270 aspect, and lowest between 325-120 aspect. However, these differences only exist at high-slope points. This implies that `AspectSin_Slope` (as suggested by @evimarp in this [notebook](https:\/\/www.kaggle.com\/evimarp\/top-6-roosevelt-national-forest-competition\/comments)) is an appropriate choice of feature.\n<br><br>\nCan shadow\/light be affected by `Elevation` as well?","3620e4dd":"### 1.2. Impute \"questionable\" 0 values\nWhat features are the most correlated with `Hillshade_9am` and `Hillshade_3pm`?","e73781df":"## 4. Categorical Features\nThis section is just to convert multiple OHE categorical features into one. Due to the limited time, I couldn't explore further on this. Any comment or suggestion on this section is hugely appreciated!","b4fd6dc1":"Let's once again look at the distribution of the \"questionable 0\" features.","c4f5a4e6":"# III. Feature Importances","c5fa1eff":"From the correlation graphs above, the following features have been chosen to impute the questionable-0 features:\n- *Hillshade_9am*: *Hillshade_3pm*, *Aspect*, *Slope*, *Soil_Type10*, *Wilderness_Area1*, *Wilderness_Area4*, *Vertical_Distance_To_Hydrology*\n- *Hillshade_3pm*: *Hillshade_9am*, *Hillshade_Noon*, *Slope*, *Aspect*","67e12d2e":"This kernel is part 1 of my work, listed below, in this competition. Any comments or suggestions you may have are greatly appreciated!\n1. [EAD and Feature Engineering](https:\/\/www.kaggle.com\/hoangnguyen719\/1-eda-and-feature-engineering\/notebook) (preliminary data exploration and features reduction)\n2. [ExtraTreesClassifier tuning](https:\/\/www.kaggle.com\/hoangnguyen719\/extratree-tuning) (hyper-parameter tuning for ExtraTree model)\n3. [AdaboostClassifier tuning](https:\/\/www.kaggle.com\/hoangnguyen719\/adaboost-tuning) (hyper-parameter tuning for AdaBoostClassifier model)\n4. [LGBMClassifier tuning](https:\/\/www.kaggle.com\/hoangnguyen719\/lightgbm-tuning) (hyper-parameter tuning for LightGBM Classifier model)\n5. [KNearestClassifier tuning](https:\/\/www.kaggle.com\/hoangnguyen719\/knn-tuning) (hyper-parameter tuning for KNearestNeighbor Classifier model)\n6. [StackingCVClassifier (use_probas tuning)](https:\/\/www.kaggle.com\/hoangnguyen719\/stacking-use-probas-tuning) (stacking multiple classifier using StackingCV)\n7. [Mis-Classified Inspection](https:\/\/www.kaggle.com\/hoangnguyen719\/mis-classified-inspection) (examining mis-classified instances)","06cd9af5":"# II. Exploratory Data Analysis\nFirst, let's take a look at the dataset as a whole.","22db37cf":"It can be seen roughly that there are two groups of correlated features here. The first group, being around *Elevation* and *Distance* features, has a rather bright color signifying the possible positive correlation between the height and the distance from major points (roadways, fire points, water) of the lands. <br>\nThe second group around *Hillshade*, *Aspect* and *Slope* with a more mixed combination of colors implies that shadown, aspect and slope of the land might be related but in a more complicated way.","babf6dc9":"## 3. Distance & Elevation\nAs pointed out in other notebooks ([by @evimarp](https:\/\/www.kaggle.com\/evimarp\/top-6-roosevelt-national-forest-competition#Feautures-importances) for example), `Elevation` plays an important role in predicting `Cover_Type`. The section below is to incorporate different features from `Elevation` and `Distances` that have been compiled by @evimarp. Huge thanks to him\/her for this!","b21115ff":"The imputation results above are near perfect, so there's no need for further tuning.","90cbb0f3":"## 2. Shadow\nIn this [notebook](https:\/\/www.kaggle.com\/evimarp\/top-6-roosevelt-national-forest-competition#Slope-and-Aspect-analysis), @evimarp gives a very interesting EDA on the relationship between `Slope`, `Aspect` and `Hillshade`. My section below is mostly replication of his\/her notebook with some additional further search on other relationships of these features.<br><br>\nFirst, let's see how they can be related.","4021cea9":"## 1. Zeroes\nCredit goes to @arateris' and his [Stacked Classifiers for Forest Cover Notebook](https:\/\/www.kaggle.com\/arateris\/stacked-classifiers-for-forest-cover), which has pointed out that the 0 values in *Hillshade_3pm* seems a little odd and could have been data entry errors. He then proceeds and uses `RandomForestRegressor` to impute the \"wrong\" 0s.<br><br>\nIn this section I expand the search onto other features that also contain 0s. First, let's look at their distribution.","bc78bb62":"There doesn't seem to be a clearer relationship between `Elevation` and these features.\n<br><br>\nLet's incorporate the findings above into our dataset and see what we have.","92eaead9":"Next, let's use these features to impute the questionable 0 values. The model chosen for imputation is `RandomForestRegressor`, as suggested by @arateris 's [notebook](https:\/\/www.kaggle.com\/arateris\/stacked-classifiers-for-forest-cover).","fc276c9f":"Here, I look at the prevalence of 0s in each feature, including their count (`train_0_count`, `test_0_count` and `all_data_0_count`) and frequency (`train_0_portion`, `test_0_portion` and `all_data_0_portion`). I then compare the 0s' frequency with the overall frequency of all values (`1\/train_nunique`, `1\/test_nunique` and `1\/all_data_unique`). In addition to the distribution graphs above, I used this comparison to decide if the 0s are \"odd\/rare enough\" to be considered possibly errors.<br><br>\nBased on the graphs and the comparison, the only feature that has \"small enough\" frequency consistently in both train and test sets is `Hillshade_9am`. Therefore:\n- Normal zero values: *Aspect*, *Horizontal_Distance_To_Hydrology*, *Vertical_Distance_To_Hydrology*, *Horizontal_Distance_To_Fire_Points*, *Horizontal_Distance_To_Roadways*, *Hillshade_Noon*, *Slope*\n- Questionable zero values: *Hillshade_9am*, *Hillshade_3pm* (due to the weird distribution at 0 as shown below)\n<br>\n<br>\n**NOTE**: As mentioned above, I'm looking only at 0s in the train set to avoid data leakage. Had I also considered the test set then the *Hillshade_Noon* and *Slope* would have also been included in `Questionable 0`.","6b7643bf":"*Note*: Many of my EDA parts have been greatly inspired by previous kernels in the competition and I have been trying to give credits to the owners as much as I can. However, because (1) many kernels appear to have the same ideas (even codes), which makes it hard to trace back where the ideas originated from, and (2) I carelessly forgot to note down all the sources (this is totally my bad), sometimes the credit may not be given where it's due. I apologize beforehand, and please let me know in the comment section if you have any question or suggestions. Thank you!\n<br><br>\n**Outline of this notebook**<br>\nI. [Package and Data Loading](#I.-Package-and-Data-Loading) <br>\nII. [Exploratory Data Analysis](#II.-Exploratory-Data-Analysis) <br>\n..... 1. [Zeros](#1.-Zeroes) <br>\n..... 2. [Shadow](#2.-Shadow) <br>\n..... 3. [Distance & Elevation](#3.-Distance-&-Elevation) <br>\n..... 4. [Categorical Features](#4.-Categorical-Features) <br>\n..... 5. [Rockiness](#5.-Rockiness) <br>\nIII. [Feature Importance](#III.-Feature-Importances) <br>\n..... 1. [Model Setup](#1.-Model-Setup) <br>\n..... 2. [Feature Importance](#2.-Feature-Importance)","a461b0cc":"There is still something that causes high variation for low-aspect points. Does it mean flatter points (slope closer to 0) often have less variation in sunlight no matter aspect? That seems correct right? <br><br>\n(`AspectSin_Slope^2`, (`AspectSin_Slope`)^2, `Hillshade_9am_sqrt` and `Hillshade_Mean_sqrt` were also tried but they were not really meaningful in the **Feature Importance** part below and had been removed)<br><br>\nLet's see if the newly created features can better separate our `Cover_Type`.","4fd7456c":"The `AspectSin_Slope` and `AspectCos_Slope` does not seem to make much difference...","495e9f42":"We should drop *Soil_Type7* and *Soil_Type15* which take only 0 values.\n<br><br>\nIn the datasets, there are two types of categorical features (*Soil_Type* and *Wilderness_Area*) and 10 continuous\/numerical features. Let's separate them out for out EA and look at the continuous features first.","68a39774":"Low-slope points seem to have less variance in terms of hillshade.<br>\nDoes N and S have more constant hillshade than E and W?"}}