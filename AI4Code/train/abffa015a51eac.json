{"cell_type":{"b5d0a504":"code","c85195d4":"code","8029a1b1":"code","db4dd4b3":"code","55c182a6":"code","2df54c29":"code","553ff000":"code","9e17880c":"code","17951436":"code","bd8908e9":"code","58c68955":"code","5015ff81":"code","d2fe6d10":"code","f77c5c90":"code","009adab2":"code","dfcbf3bc":"markdown","4b7aba10":"markdown","4fce5842":"markdown","e0112522":"markdown","fcd02d23":"markdown","ab9edf03":"markdown","dfaedeb3":"markdown","4113bb51":"markdown","595a7887":"markdown","3c0ce489":"markdown","d6db7ce4":"markdown","55ab9867":"markdown","af3e60e0":"markdown","29fd911d":"markdown","b83192b9":"markdown","0aeea6fa":"markdown","aae7b288":"markdown","2257e0aa":"markdown","e7cf931e":"markdown","ffe66760":"markdown","8bd07a9e":"markdown","68cb14cf":"markdown","fd9ef5f9":"markdown","d6c76189":"markdown","07295eed":"markdown","9f6d1d97":"markdown","32ab615b":"markdown"},"source":{"b5d0a504":"!pip install pyreadr==v0.3.3\n\nimport pyreadr\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport scipy.stats\nimport sklearn.decomposition\nimport sklearn.preprocessing\nimport lightgbm as lgb","c85195d4":"print(np.__version__)\nprint(pd.__version__)\nprint(matplotlib.__version__)\nprint(scipy.__version__)\nprint(sklearn.__version__)\nprint(lgb.__version__)","8029a1b1":"class ModelPCA ():\n    \n    def spe (self, X, X_pred): \n        return np.sum((X-X_pred)**2, axis=1)\n   \n    def train(self, df_train, plot = False):\n        \n        self.mu_train = df_train.mean(axis=0)\n        self.std_train = df_train.std(axis=0)\n\n        self.m = sklearn.decomposition.PCA(n_components = 0.9)\n\n        X_train = sklearn.preprocessing.scale(df_train)\n        X_train_pred = self.m.inverse_transform(self.m.fit_transform(X_train))\n        \n        if plot:\n            fig, ax = plt.subplots()\n            xaxis = np.arange(len(self.m.explained_variance_ratio_))\n            ax.bar(xaxis, self.m.explained_variance_ratio_)\n            ax.plot(xaxis, np.cumsum(self.m.explained_variance_ratio_));\n            ax.set_title('PCA - Explained variance');\n        \n        return self.spe(X_train, X_train_pred)\n            \n    def test(self, df_test, plot = False):\n        \n        X_test = np.array((df_test-self.mu_train)\/self.std_train)\n        X_test_pred = self.m.inverse_transform(self.m.transform(X_test))\n\n        return self.spe(X_test, X_test_pred)","db4dd4b3":"# as proposed by Sergei Averkiev (kaggle.com\/averkij\/anomaly-detection-methods)\n\nclass ModelEnsembleRegressors():\n    \n    def __init__ (self, regressor, **kwargs):\n        self.regressor = regressor\n        self.kwargs = kwargs\n\n    def spe(self, y, y_pred): \n        return (y-y_pred)**2\n        \n    def train(self, df_train, plot = False):\n\n        self.mu_train = df_train.mean(axis=0)\n        self.std_train = df_train.std(axis=0)\n        \n        self.models = {}\n        spe_df = pd.DataFrame()\n        \n        cols_to_predict = df_train.columns\n        \n        i = 0\n        \n        self.to_predict_test = []\n        \n        if plot:\n            n_rows = int(np.ceil(len(df_train.columns)\/4))\n            fig, ax = plt.subplots(n_rows,4,figsize=(30,4*n_rows))\n        \n        for col in df_train.columns:\n            \n            #print('training model for', col)\n            model = self.regressor(**self.kwargs)\n            tr_x = sklearn.preprocessing.scale(df_train.drop([col],axis=1))\n            tr_y = sklearn.preprocessing.scale(df_train[col])\n        \n            model.fit(X=tr_x, y=tr_y)\n            \n            tr_y_pred = model.predict(X=tr_x)\n            \n            self.models[col] = model\n                                    \n            spe_df[col] = self.spe(tr_y, tr_y_pred)\n            \n            #if np.mean(spe_df[col])<0.5:\n            #    self.to_predict_test.append(col)\n            \n            self.to_predict_test.append(col)\n            \n            if plot:\n                ax.ravel()[i].plot(tr_y)\n                ax.ravel()[i].plot(tr_y_pred, alpha=0.5)\n                ax.ravel()[i].set_title(f'{col} - SPE = {np.mean(spe_df[col]):.2f}')\n                \n            i+=1\n                        \n        return np.mean(spe_df, axis=1)\n    \n    def test(self, df_test, plot = False):\n\n        spe_df = pd.DataFrame()\n        \n        i = 0\n\n        if plot:\n            n_rows = int(np.ceil(len(df_train.columns)\/4))\n            fig, ax = plt.subplots(n_rows,4,figsize=(30,4*n_rows))\n        \n        for col in self.to_predict_test:\n            ts_x = np.array((df_test.drop([col],axis=1)-self.mu_train.drop([col]))\/self.std_train.drop([col]))\n            ts_y = np.array((df_test[col]-self.mu_train[col])\/self.std_train[col])\n            ts_y_pred = self.models[col].predict(ts_x)\n            spe_df[col] = self.spe(ts_y, ts_y_pred)\n            \n            if plot:\n                ax.ravel()[i].plot(ts_y)\n                ax.ravel()[i].plot(ts_y_pred, alpha=0.5)\n                ax.ravel()[i].set_title(f'{col}\/SPE = {np.mean(spe_df[col]):.2f}')\n            \n            i+=1\n                \n        return np.mean(spe_df, axis=1)","55c182a6":"def apply_lag (df, lag = 1):\n       \n    from statsmodels.tsa.tsatools import lagmat\n    array_lagged = lagmat(df, maxlag=lag,\n                          trim=\"forward\", original='in')[lag:,:]  \n    new_columns = []\n    for l in range(lag):\n        new_columns.append(df.columns+'_lag'+str(l+1))\n    columns_lagged = df.columns.append(new_columns)\n    index_lagged = df.index[lag:]\n    df_lagged = pd.DataFrame(array_lagged, index=index_lagged,\n                             columns=columns_lagged)\n       \n    return df_lagged  ","2df54c29":"def filter_noise_ma (df, WS = 100,reduction = False):\n\n    import copy\n    \n    new_df = copy.deepcopy(df)\n\n    for column in df:\n        new_df[column] = new_df[column].rolling(WS).mean()\n\n    if reduction:\n        return new_df.drop(df.index[:WS])[::WS]\n    else:\n        return new_df.drop(df.index[:WS])","553ff000":"train_normal_path = '\/kaggle\/input\/tennessee-eastman-process-simulation-dataset\/TEP_FaultFree_Training.RData'\ntrain_faulty_path = '\/kaggle\/input\/tennessee-eastman-process-simulation-dataset\/TEP_Faulty_Training.RData'\n\ntest_normal_path = '\/kaggle\/input\/tennessee-eastman-process-simulation-dataset\/TEP_FaultFree_Testing.RData'\ntest_faulty_path = '\/kaggle\/input\/tennessee-eastman-process-simulation-dataset\/TEP_Faulty_Testing.RData'\n\ntrain_normal_complete = pyreadr.read_r(train_normal_path)['fault_free_training']\n#train_faulty_complete = pyreadr.read_r(train_fault_path)['faulty_training']\n\n#test_normal_complete = pyreadr.read_r(test_normal_path)['fault_free_testing']\ntest_faulty_complete = pyreadr.read_r(test_faulty_path)['faulty_testing']","9e17880c":"df_train = train_normal_complete[train_normal_complete.simulationRun==1].iloc[:,3:]\n\nfig, ax = plt.subplots(13,4,figsize=(30,50))\n\nfor i in range(df_train.shape[1]):\n    df_train.iloc[:,i].plot(ax=ax.ravel()[i]) \n    ax.ravel()[i].legend();","17951436":"test_f1_run1 = test_faulty_complete[(test_faulty_complete.faultNumber==7) & (test_faulty_complete.simulationRun==1)].iloc[:,3:]\n\nfig, ax = plt.subplots(13,4,figsize=(30,50))\n\nfor i in range(test_f1_run1.shape[1]):\n    test_f1_run1.iloc[:,i].plot(ax=ax.ravel()[i]) \n    ax.ravel()[i].legend();","bd8908e9":"from scipy.cluster import hierarchy as hc\ncorr = np.round(scipy.stats.spearmanr(df_train).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(16,10))\ndendrogram = hc.dendrogram(z, labels=df_train.columns, orientation='left', leaf_font_size=8)\nplt.show()","58c68955":"df_train = train_normal_complete[train_normal_complete.simulationRun==1].iloc[:,3:]\ndf_validation = train_normal_complete[(train_normal_complete.simulationRun>1)&(train_normal_complete.simulationRun<5)].iloc[:,3:]\ndf_test = train_normal_complete[(train_normal_complete.simulationRun>5)&(train_normal_complete.simulationRun<10)].iloc[:,3:]\n\nreg = lgb.LGBMRegressor\n\nmodels = {'pca': ModelPCA(), 'lgb': ModelEnsembleRegressors(reg)}\ndetection_limits = {}\n\nfig, ax = plt.subplots(len(models),3, figsize=(20,4*len(models)))\n\ni = 0\n\nfor name, model in models.items():\n    \n    spe_train = model.train(df_train, plot = True)\n    spe_validation = model.test(df_validation, plot = True)\n    spe_test = model.test(df_test)\n    \n    detection_limits[name] = np.percentile(spe_validation, 99.99)\n\n    ax[i,0].plot(spe_train);\n    ax[i,1].plot(spe_validation);\n    ax[i,2].plot(spe_test);\n\n    ax[i,0].set_title(f'Training - {name}')\n    ax[i,1].set_title(f'Validation - {name}')\n    ax[i,2].set_title(f'Test - {name}')\n\n    ax[i,1].axhline(detection_limits[name], ls = '--')\n    ax[i,2].axhline(detection_limits[name], ls = '--')\n                  \n    i+=1\n    \nfig.suptitle('SPE');","5015ff81":"fig, ax = plt.subplots(5,4, figsize = (20, 20))\n\nfdr_df = pd.DataFrame(index=[f'IDV({i})' for i in range(1,21)])\n\nfor name, model in models.items():\n\n    spe_test = []\n    fdr = []\n\n    for i in range(20):\n\n        df_test = test_faulty_complete[(test_faulty_complete.faultNumber==i+1) & (test_faulty_complete.simulationRun==1)].iloc[:,3:]\n\n        spe_test.append(model.test(df_test))\n\n        fault_occurrence = len(spe_test[i])\/\/6\n\n        fdr.append(np.mean(spe_test[i][fault_occurrence:]>detection_limits[name]))\n\n        color = next(ax.ravel()[i]._get_lines.prop_cycler)['color']\n        ax.ravel()[i].plot(np.arange(len(spe_test[i])), spe_test[i], color = color)\n        ax.ravel()[i].axhline(detection_limits[name], ls='--', color = color)\n        ax.ravel()[i].axvline(fault_occurrence)\n        ax.ravel()[i].set_title(f'IDV({i+1})')\n        ax.ravel()[i].set_yscale('log')\n        \n    fdr_df[name] = fdr\n    \nprint(100*fdr_df.mean())    \n100*fdr_df.T","d2fe6d10":"fig, ax = plt.subplots(5,4, figsize = (20, 20))\n\ndf_train = train_normal_complete[train_normal_complete.simulationRun==1].iloc[:,3:]\ndf_validation = train_normal_complete[(train_normal_complete.simulationRun>1)&(train_normal_complete.simulationRun<5)].iloc[:,3:]\n\nmodels = {'pca_lag1': ModelPCA(), 'pca_lag2': ModelPCA(), 'pca_lag3': ModelPCA()}\nlags = {'pca_lag1': 1, 'pca_lag2': 2, 'pca_lag3': 3}\n\ndetection_limits = {}\n\nfdr_df = pd.DataFrame(index=[f'IDV({i})' for i in range(1,21)])\n\nfor name, model in models.items():\n\n    spe_test = []\n    fdr = []\n    \n    _ = model.train(apply_lag(df_train,lags[name]))\n    spe_validation = model.test(apply_lag(df_validation,lags[name]))\n    detection_limits[name] = np.percentile(spe_validation, 99.99)\n\n    for i in range(20):\n        \n        df_test = apply_lag(test_faulty_complete[(test_faulty_complete.faultNumber==i+1) & \n                                                 (test_faulty_complete.simulationRun==1)].iloc[:,3:], lags[name])\n\n        spe_test.append(model.test(df_test))\n\n        fault_occurrence = len(spe_test[i])\/\/6\n\n        fdr.append(np.mean(spe_test[i][fault_occurrence:]>detection_limits[name]))\n\n        color = next(ax.ravel()[i]._get_lines.prop_cycler)['color']\n        ax.ravel()[i].plot(np.arange(len(spe_test[i])), spe_test[i], color = color, label=name)\n        ax.ravel()[i].axhline(detection_limits[name], ls='--', color = color)\n        ax.ravel()[i].axvline(fault_occurrence)\n        ax.ravel()[i].set_title(f'IDV({i+1})')\n        ax.ravel()[i].set_yscale('log')\n        ax.ravel()[i].legend()\n        \n    fdr_df[name] = fdr\n    \nprint(100*fdr_df.mean())\n100*fdr_df.T","f77c5c90":"fig, ax = plt.subplots(5,4, figsize = (20, 20))\n\ndf_train = train_normal_complete[train_normal_complete.simulationRun==1].iloc[:,3:]\ndf_validation = train_normal_complete[(train_normal_complete.simulationRun>1)&(train_normal_complete.simulationRun<5)].iloc[:,3:]\n\nmodels = {'WS_10': ModelPCA(), 'WS_50': ModelPCA(), 'WS_100': ModelPCA()}\nWS = {'WS_10': 10, 'WS_50': 50, 'WS_100': 100}\n\ndetection_limits = {}\n\nfdr_df = pd.DataFrame(index=[f'IDV({i})' for i in range(1,21)])\n\nfor name, model in models.items():\n\n    spe_test = []\n    fdr = []\n    \n    _ = model.train(filter_noise_ma(df_train,WS[name]))\n    spe_validation = model.test(filter_noise_ma(df_validation, WS[name]))\n    detection_limits[name] = np.percentile(spe_validation, 99.99)\n\n    for i in range(20):\n        \n        df_test = filter_noise_ma(test_faulty_complete[(test_faulty_complete.faultNumber==i+1) & \n                                                 (test_faulty_complete.simulationRun==1)].iloc[:,3:], WS[name])\n\n        spe_test.append(model.test(df_test))\n\n        fault_occurrence = df_test.index[0]-WS[name]+160\n\n        fdr.append(np.mean(spe_test[i][160-WS[name]:]>detection_limits[name]))\n\n        color = next(ax.ravel()[i]._get_lines.prop_cycler)['color']\n        ax.ravel()[i].plot(df_test.index, spe_test[i], color = color, label=name)\n        ax.ravel()[i].axhline(detection_limits[name], ls='--', color = color)\n        ax.ravel()[i].axvline(fault_occurrence)#, color = color)\n        ax.ravel()[i].set_title(f'IDV({i+1})')\n        ax.ravel()[i].set_yscale('log')\n        ax.ravel()[i].legend()\n        \n    fdr_df[name] = fdr\n    \nprint(100*fdr_df.mean())\n100*fdr_df.T","009adab2":"fig, ax = plt.subplots(5,4, figsize = (20, 20))\n\ndf_train = train_normal_complete[train_normal_complete.simulationRun==1].iloc[:,3:]\ndf_validation = train_normal_complete[(train_normal_complete.simulationRun>1)&(train_normal_complete.simulationRun<5)].iloc[:,3:]\n\nreg = lgb.LGBMRegressor\n\nmodels = {'WS_10': ModelEnsembleRegressors(reg), 'WS_50': ModelEnsembleRegressors(reg), 'WS_100': ModelEnsembleRegressors(reg)}\nWS = {'WS_10': 10, 'WS_50': 50, 'WS_100': 100}\n\ndetection_limits = {}\n\nfdr_df = pd.DataFrame(index=[f'IDV({i})' for i in range(1,21)])\n\nfor name, model in models.items():\n\n    spe_test = []\n    fdr = []\n    \n    _ = model.train(filter_noise_ma(df_train,WS[name]))\n    spe_validation = model.test(filter_noise_ma(df_validation, WS[name]))\n    detection_limits[name] = np.percentile(spe_validation, 99.99)\n\n    for i in range(20):\n        \n        df_test = filter_noise_ma(test_faulty_complete[(test_faulty_complete.faultNumber==i+1) & \n                                                 (test_faulty_complete.simulationRun==1)].iloc[:,3:], WS[name])\n\n        spe_test.append(model.test(df_test))\n\n        fault_occurrence = df_test.index[0]-WS[name]+160\n\n        fdr.append(np.mean(spe_test[i][160-WS[name]:]>detection_limits[name]))\n\n        color = next(ax.ravel()[i]._get_lines.prop_cycler)['color']\n        ax.ravel()[i].plot(df_test.index, spe_test[i], color = color, label=name)\n        ax.ravel()[i].axhline(detection_limits[name], ls='--', color = color)\n        ax.ravel()[i].axvline(fault_occurrence)#, color = color)\n        ax.ravel()[i].set_title(f'IDV({i+1})')\n        ax.ravel()[i].set_yscale('log')\n        ax.ravel()[i].legend()\n        \n    fdr_df[name] = fdr\n    \nprint(100*fdr_df.mean())\n100*fdr_df.T","dfcbf3bc":"## Visualizing Spearman Correlations\n\nFeatures that have strong correlations with others will be easier to model with LGB.","4b7aba10":"# Importing and visualizing data","4fce5842":"<img src=\"https:\/\/raw.githubusercontent.com\/gmxavier\/TEP-meets-LSTM\/master\/tep_flowsheet.png\" width=\"900\" height=\"900\"\/>","e0112522":"## Visualizing a faulty test set","fcd02d23":"It is noticed that, in the global measure, the PCA model is not affected, although there is a significant improvement specifically in relation to faults 5 and 20.\n\nRegarding the LGB model, there is an improvement in the overall performance that makes it reach fault detection rates similar to those of the PCA.","ab9edf03":"# Lag variables\n\nLag variables are columns added to the data set that include past points of the variables. With this transformation, each row contains the past of the variables. The representation can help to model dynamics, that is, when the past of the variables is important to understand the present.","dfaedeb3":"### Manipulated Variables\n\nVariable | Description\n-------- | -----------\n`XMV(1)`  | D Feed Flow (stream 2)            (Corrected Order)\n`XMV(2)`  | E Feed Flow (stream 3)            (Corrected Order)\n`XMV(3)`  | A Feed Flow (stream 1)            (Corrected Order)\n`XMV(4)`  | A and C Feed Flow (stream 4)\n`XMV(5)`  | Compressor Recycle Valve\n`XMV(6)`  | Purge Valve (stream 9)\n`XMV(7)`  | Separator Pot Liquid Flow (stream 10)\n`XMV(8)`  | Stripper Liquid Product Flow (stream 11)\n`XMV(9)`  | Stripper Steam Valve\n`XMV(10)` | Reactor Cooling Water Flow\n`XMV(11)` | Condenser Cooling Water Flow\n`XMV(12)` | Agitator Speed","4113bb51":"### Manipulated Variables\n\nVariable | Description\n-------- | -----------\n`XMV(1)`  | D Feed Flow (stream 2)            (Corrected Order)\n`XMV(2)`  | E Feed Flow (stream 3)            (Corrected Order)\n`XMV(3)`  | A Feed Flow (stream 1)            (Corrected Order)\n`XMV(4)`  | A and C Feed Flow (stream 4)\n`XMV(5)`  | Compressor Recycle Valve\n`XMV(6)`  | Purge Valve (stream 9)\n`XMV(7)`  | Separator Pot Liquid Flow (stream 10)\n`XMV(8)`  | Stripper Liquid Product Flow (stream 11)\n`XMV(9)`  | Stripper Steam Valve\n`XMV(10)` | Reactor Cooling Water Flow\n`XMV(11)` | Condenser Cooling Water Flow\n`XMV(12)` | Agitator Speed","595a7887":"# Principal Component Analysis\n\nThe Principal Component Analysis (PCA) technique applies a linear transformation to generate an orthogonal set of linear combinations of the original variables, whose directions are called principal components, with the aim of selecting a subset of these variables that appropriately summarizes the data variability.\n\nWhen PCA is applied to detect faults, the principal components are identified using the training data; the principal components found during the training are also expected to explain the variability of the test data. If this is not the case, the process is considered to be faulty.","3c0ce489":"# Testing (faulty sets)\n\nHere we have a comparison between PCA and LGB. We will print the overall fault detection rate and the results for each fault (tabularly and graphically).","d6db7ce4":"# Light Gradient Boosting\n\nLight Gradient Boosting is a tree-based algorithm that groups simple decision tree models using the boosting strategy, in which trees are trained sequentially and each one tries to correct the error of the previous tree.\n\nHere we will use a strategy proposed by [Sergei Averkiev](https:\/\/kaggle.com\/averkij\/anomaly-detection-methods), in which each feature is predicted using the remaining features as predictors. The fault is detected when the mean squared error of all predictions exceeds a certain limit.","55ab9867":"# Adding lag variables\n\nHere, this strategy will only be used with PCA (in this case, the technique is called Dynamical PCA, or DPCA).","af3e60e0":"# Process description","29fd911d":"## Visualizing training set","b83192b9":"### LGB","0aeea6fa":"### Sampled Process Measurements\n\n- Reactor Feed Analysis (Stream 6)\n  > - Sampling Frequency = 0.1 hr\n  > - Dead Time = 0.1 hr\n  > - Mole %\n  \nVariable | Description\n-------- | -----------\n`XMEAS(23)` | Component A\n`XMEAS(24)` | Component B\n`XMEAS(25)` | Component C\n`XMEAS(26)` | Component D\n`XMEAS(27)` | Component E\n`XMEAS(28)` | Component F\n\n- Purge Gas Analysis (Stream 9)\n  > - Sampling Frequency = 0.1 hr\n  > - Dead Time = 0.1 hr\n  > - Mole %\n\nVariable | Description\n-------- | -----------\n`XMEAS(29)` | Component A\n`XMEAS(30)` | Component B\n`XMEAS(31)` | Component C\n`XMEAS(32)` | Component D\n`XMEAS(33)` | Component E\n`XMEAS(34)` | Component F\n`XMEAS(35)` | Component G\n`XMEAS(36)` | Component H\n\n- Product Analysis (Stream 11)\n  > - Sampling Frequency = 0.25 hr\n  > - Dead Time = 0.25 hr\n  > - Mole %\n\nVariable | Description\n-------- | -----------\n`XMEAS(37)` | Component D\n`XMEAS(38)` | Component E\n`XMEAS(39)` | Component F\n`XMEAS(40)` | Component G\n`XMEAS(41)` | Component H","aae7b288":"### Process Disturbances\n\nVariable | Description\n-------- | -----------\n`IDV(1)`  | A\/C Feed Ratio, B Composition Constant (Stream 4)          Step\n`IDV(2)`  | B Composition, A\/C Ratio Constant (Stream 4)               Step\n`IDV(3)`  | D Feed Temperature (Stream 2)                              Step\n`IDV(4)`  | Reactor Cooling Water Inlet Temperature                    Step\n`IDV(5)`  | Condenser Cooling Water Inlet Temperature                  Step\n`IDV(6)`  | A Feed Loss (Stream 1)                                     Step\n`IDV(7)`  | C Header Pressure Loss - Reduced Availability (Stream 4)   Step\n`IDV(8)`  | A, B, C Feed Composition (Stream 4)            Random Variation\n`IDV(9)`  | D Feed Temperature (Stream 2)                  Random Variation\n`IDV(10)` | C Feed Temperature (Stream 4)                  Random Variation\n`IDV(11)` | Reactor Cooling Water Inlet Temperature        Random Variation\n`IDV(12)` | Condenser Cooling Water Inlet Temperature      Random Variation\n`IDV(13)` | Reaction Kinetics                                    Slow Drift\n`IDV(14)` | Reactor Cooling Water Valve                            Sticking\n`IDV(15)` | Condenser Cooling Water Valve                          Sticking\n`IDV(16)` | Unknown\n`IDV(17)` | Unknown\n`IDV(18)` | Unknown\n`IDV(19)` | Unknown\n`IDV(20)` | Unknown","2257e0aa":"In this notebook, we will apply the PCA (Principal Component Analysis) and LGB (Light Gradient Boosting) techniques to the fault detection problem in the Tennessee Eastman Process benchmark. We will also evaluate the effect on the results of adding lag variables and filtering the measurement noise.\n\n","e7cf931e":"# Conclusions\n\n* PCA and LGB can detect faults in Tennesse Eastman Processes.\n* Some faults are easier to detect than others.\n* In general, PCA is superior to LGB.\n* Some faults are easier to detect when pre-processing techniques are applied.\n* LGB benefits from noise filtering, achieving performances comparable to vanilla PCA.","ffe66760":"# Noise filtering\n\nThe moving average filter is one of the simplest ways to decrease noise (unwanted variability) in a data set. It works by replacing individual points with averages calculated in data windows. \n\nThere are several strategies for applying moving average filters. Here, we will replace each point with the average of a window of size $W$ which contains the point in question and $W-1$ previous points.","8bd07a9e":"The PCA model is superior.","68cb14cf":"### Continuous Process Measurements\n\nVariable | Description | unit\n-------- | ----------- | ----\n`XMEAS(1)`  | A Feed  (stream 1)                  | kscmh\n`XMEAS(2)`  | D Feed  (stream 2)                  | kg\/hr\n`XMEAS(3)`  | E Feed  (stream 3)                  | kg\/hr\n`XMEAS(4)`  | A and C Feed  (stream 4)            | kscmh\n`XMEAS(5)`  | Recycle Flow  (stream 8)            | kscmh\n`XMEAS(6)`  | Reactor Feed Rate  (stream 6)       | kscmh\n`XMEAS(7)`  | Reactor Pressure                    | kPa gauge\n`XMEAS(8)`  | Reactor Level                       | %\n`XMEAS(9)`  | Reactor Temperature                 | Deg C\n`XMEAS(10)` | Purge Rate (stream 9)               | kscmh\n`XMEAS(11)` | Product Sep Temp                    | Deg C\n`XMEAS(12)` | Product Sep Level                   | %\n`XMEAS(13)` | Prod Sep Pressure                   | kPa gauge\n`XMEAS(14)` | Prod Sep Underflow (stream 10)      | m3\/hr\n`XMEAS(15)` | Stripper Level                      | %\n`XMEAS(16)` | Stripper Pressure                   | kPa gauge\n`XMEAS(17)` | Stripper Underflow (stream 11)      | m3\/hr\n`XMEAS(18)` | Stripper Temperature                | Deg C\n`XMEAS(19)` | Stripper Steam Flow                 | kg\/hr\n`XMEAS(20)` | Compressor Work                     | kW\n`XMEAS(21)` | Reactor Cooling Water Outlet Temp   | Deg C\n`XMEAS(22)` | Separator Cooling Water Outlet Temp | Deg C","fd9ef5f9":"### PCA","d6c76189":"Although the overall performance does not increase, some cases benefit from the technique, especially the faults 11 and 14.","07295eed":"<img src=\"https:\/\/miro.medium.com\/max\/700\/0*OC6s0YDZcQxACUAJ.jpg\" width=\"600\" height=\"600\"\/>","9f6d1d97":"# Training\n\nIn the following pictures we have:\n\n* the SPE charts for the two models;\n* the explained variance of PCA;\n* the LGB predictions of each variable.\n\nValidation and testing were calculated using normal data.","32ab615b":"## Filtering noise\n\nThree window sizes will be evaluated: 10, 50 and 100. "}}