{"cell_type":{"a8f5e16f":"code","4c35bb3c":"code","1ba6c297":"code","bc7375f4":"code","b7576d5b":"code","2cd6bbbb":"code","558b7164":"code","8cbf4af3":"code","d2075666":"code","4884388c":"code","6c7d6d49":"code","9e7f9298":"code","4a17ca1a":"code","674c48a3":"code","ff8eadca":"code","07749846":"code","1b1d5c87":"code","3624db9c":"code","3a162bbd":"code","6489074f":"code","8dfb8d0c":"code","8da70cc1":"code","1a1b21cd":"code","f813a6eb":"code","debc5a50":"code","3ac4c710":"code","f15baa16":"code","6c3867d3":"code","95329eac":"code","a5658895":"code","8901220a":"code","554556e3":"code","6bff19f8":"code","1f710b07":"code","28aaad70":"code","0ec4a717":"code","1fc62a53":"code","3eb65f3b":"code","a1dcb105":"markdown","98435240":"markdown","3f64985e":"markdown","44f1a4a7":"markdown","be59d3df":"markdown","0f3196de":"markdown","1857ead1":"markdown","304722ea":"markdown","6e0a7001":"markdown","2f946745":"markdown","eccdd551":"markdown","3f947b75":"markdown","fd2141d8":"markdown","1691e198":"markdown","eb0ddb56":"markdown","ad21f79e":"markdown","c11e5744":"markdown","aa0eb644":"markdown"},"source":{"a8f5e16f":"import numpy as np\nimport pandas as pd\nimport seaborn as sea\nimport random\nimport os\nimport glob\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import classification_report\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Activation\nfrom tensorflow.keras.layers import GlobalAveragePooling2D\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout\nfrom tensorflow.keras.layers import BatchNormalization, SpatialDropout2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.metrics import precision_recall_curve, roc_curve, accuracy_score, confusion_matrix, precision_score, recall_score\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec","4c35bb3c":"# Read csv file containing training datadata\ndf = pd.read_csv(\"metadata.csv\")\n# Print first 5 rows\n#print(f'There are {train_df.shape[0]} rows and {train_df.shape[1]} columns in this data frame')\ndf.head()","1ba6c297":"df['finding'].value_counts()","bc7375f4":"from sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ndf['finding'] = labelencoder.fit_transform(df.finding)","b7576d5b":"df['finding'].value_counts()","2cd6bbbb":"viral=df[df['finding']==1]['patientid'].unique() #viral pneumonia\nlen(viral)","558b7164":"normal=df[df['finding']==0]['patientid'].unique() #normal \nnormal","8cbf4af3":"import cv2\nimport glob\n\nimdir = 'images\/'\next = ['png', 'jpg', 'jpeg']    # image formats \n\nfiles = []\n[files.extend(glob.glob(imdir + '*.' + e)) for e in ext]\nfiles.sort(key=lambda x: int(x.split()[1].split('.')[0][1:-1] ))\n\n# images = [cv2.imread(file) for file in files]\n","d2075666":"len(files)","4884388c":"from PIL import Image #nomal images \nnormal_train_image=[]\nfor i in normal[0:len(normal)]:\n    for x in files:\n        if(x.split()[1].split('.')[0][1:-1]==str(i)):\n            normal_train_image.append(x.replace('\\\\','\/'))\n#             Image.open(x.replace('\\\\','\/')).save(f'train\/normal\/{x[7:]}')","6c7d6d49":"len(normal_train_image\n)","9e7f9298":"viral_train_image=[]  #viral images \nfor i in viral:\n    for x in files:\n        if(x.split()[1].split('.')[0][1:-1]==str(i)):\n            viral_train_image.append(x.replace('\\\\','\/'))\n#             Image.open(x.replace('\\\\','\/')).save(f'train\/viral\/{x[7:]}')","4a17ca1a":"len(viral_train_image)","674c48a3":"train_list = []\nfor x in normal_train_image:\n    train_list.append([x, 0])\n    \nfor x in viral_train_image:\n    train_list.append([x, 1])","ff8eadca":"import random as rn\nrn.shuffle(train_list)","07749846":"train_df = pd.DataFrame(train_list, columns=['image', 'label'])\n","1b1d5c87":"train_df","3624db9c":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test= train_test_split(train_df['image'],train_df['label'],test_size=0.2,random_state=0)\n","3a162bbd":"len(X_train)","6489074f":"for i,v in enumerate(X_train.index):\n    \n    for x in files:\n        if(Y_train[v]==0):\n            if(x.replace('\\\\','\/')==X_train[v]):\n                c+=1\n                Image.open(x.replace('\\\\','\/')).save(f'train\/normal\/{x[7:]}')","8dfb8d0c":"for i,v in enumerate(X_train.index):\n    \n    for x in files:\n        if(Y_train[v]==1):\n            if(x.replace('\\\\','\/')==X_train[v]):\n                c+=1\n                Image.open(x.replace('\\\\','\/')).save(f'train\/viral\/{x[7:]}')\n    ","8da70cc1":"for i,v in enumerate(X_test.index):\n    \n    for x in files:\n        if(Y_test[v]==0):\n            if(x.replace('\\\\','\/')==X_test[v]):\n                c+=1\n                Image.open(x.replace('\\\\','\/')).save(f'test\/normal\/{x[7:]}')","1a1b21cd":"for i,v in enumerate(X_test.index):\n    \n    for x in files:\n        if(Y_test[v]==1):\n            if(x.replace('\\\\','\/')==X_test[v]):\n                c+=1\n                Image.open(x.replace('\\\\','\/')).save(f'test\/viral\/{x[7:]}')","f813a6eb":"labels = ['viral', 'normal']\nimg_size = 200\ndef get_training_data(data_dir):\n    data = [] \n    for label in labels: \n        path = os.path.join(data_dir, label)\n        class_num = labels.index(label)\n        for img in os.listdir(path):\n            try:\n                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n                resized_arr = cv2.resize(img_arr, (img_size, img_size))\n                data.append([resized_arr, class_num])\n            except Exception as e:\n                print(e)\n    return np.array(data)","debc5a50":"train = get_training_data('train')\ntest = get_training_data('test')","3ac4c710":"viral = 0 \nnormal = 0 \n\nfor i, j in train:\n    if j == 0:\n        viral+=1\n    else:\n        normal+=1\n        \nprint('viral:', viral)\nprint('Normal:', normal)","f15baa16":"X = []\ny = []\n\nfor feature, label in train:\n    X.append(feature)\n    y.append(label)\n\nfor feature, label in test:\n    X.append(feature)\n    y.append(label)\n    \n\n# resize data for deep learning \nX = np.array(X).reshape(-1, img_size, img_size, 1)\ny = np.array(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=32)","6c3867d3":"X_train = X_train \/ 255\nX_test = X_test \/ 255","95329eac":"# good for balancing out disproportions in the dataset \ndatagen = ImageDataGenerator(\n        featurewise_center=False, \n        samplewise_center=False,  \n        featurewise_std_normalization=False,  \n        samplewise_std_normalization=False,  \n        zca_whitening=False,  \n        rotation_range=90, \n        zoom_range = 0.1, \n        width_shift_range=0.1,  \n        height_shift_range=0.1,  \n        horizontal_flip=True,  \n        vertical_flip=True)  \n\ndatagen.fit(X_train)","a5658895":"import tensorflow_addons as tfa\nmodel = Sequential()\n\nmodel.add(Conv2D(256, (3, 3), input_shape=X_train.shape[1:], padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\nmodel.add(BatchNormalization(axis=1))\n\nmodel.add(Conv2D(64, (3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\nmodel.add(BatchNormalization(axis=1))\n\nmodel.add(Conv2D(16, (3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\nmodel.add(BatchNormalization(axis=1))\n\nmodel.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\n\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\nearly_stop = EarlyStopping(patience=3, monitor='val_loss', restore_best_weights=True)\nadam = Adam(learning_rate=0.0001)\nmodel.compile(loss='binary_crossentropy',optimizer=adam,metrics=['acc',tf.keras.metrics.AUC()])\n","8901220a":"model.summary()","554556e3":"history = model.fit(datagen.flow(X_train, y_train, batch_size=10), callbacks=[early_stop], validation_data=(X_val, y_val), epochs=5)","6bff19f8":"model.evaluate(X_test, y_test)","1f710b07":"pred = model.predict(X_train)\nprecisions, recalls, thresholds = precision_recall_curve(y_train, pred)\nfpr, tpr, thresholds2 = roc_curve(y_train, pred)","28aaad70":"import tensorflow_addons as tfa\nmodel = Sequential()\n\nmodel.add(Conv2D(256, (3, 3), input_shape=X_train.shape[1:], padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\nmodel.add(BatchNormalization(axis=1))\n\nmodel.add(Conv2D(64, (3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\nmodel.add(BatchNormalization(axis=1))\n\nmodel.add(Conv2D(16, (3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\nmodel.add(BatchNormalization(axis=1))\n\nmodel.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\n\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\nearly_stop = EarlyStopping(patience=3, monitor='val_loss', restore_best_weights=True)\nadam = Adam(learning_rate=0.0001)\nmodel.compile(loss=tfa.losses.WeightedKappaLoss(num_classes=1),optimizer=adam,metrics=['acc',tf.keras.metrics.AUC()])","0ec4a717":"model.evaluate(X_test, y_test)","1fc62a53":"def plot_precision_recall(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], 'b--')\n    plt.plot(thresholds, recalls[:-1], 'g-')\n    plt.title('Precision vs. Recall')\n    plt.xlabel('Thresholds')\n    plt.legend(['Precision', 'Recall'], loc='best')\n    plt.show()\n\ndef plot_roc(fpr, tpr):\n    plt.plot(fpr, tpr)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.title('FPR (False Positive rate) vs TPR (True Positive Rate)')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate (Recall)')\n    plt.show()\n    \nplot_precision_recall(precisions, recalls, thresholds)\nplot_roc(fpr, tpr)","3eb65f3b":"plt.figure(figsize=(16, 9))\nplt.plot(history.epoch, history.history['acc'])\nplt.title('Model Accuracy')\nplt.legend(['train'], loc='upper left')\nplt.show()\n\nplt.figure(figsize=(16, 9))\nplt.plot(history.epoch, history.history['loss'])\nplt.title('Model Loss')\nplt.legend(['train'], loc='upper left')\nplt.show()\n\nplt.figure(figsize=(16, 9))\nplt.plot(history.epoch, history.history['val_acc'])\nplt.title('Model Validation Accuracy')\nplt.legend(['train'], loc='upper left')\nplt.show()\n\nplt.figure(figsize=(16, 9))\nplt.plot(history.epoch, history.history['val_loss'])\nplt.title('Model Validation Loss')\nplt.legend(['train'], loc='upper left')\nplt.show()\n","a1dcb105":"### Splitting into test, train and validation test","98435240":"### Spliting into test and train set","3f64985e":"## Data Preprocessing","44f1a4a7":"### Importing images with labels","be59d3df":"### Appending label to images","0f3196de":"### Plotting","1857ead1":"### Segragating images into normal and viral","304722ea":"## Prediction","6e0a7001":"### Converting list into dataframe","2f946745":"### Shuffling list","eccdd551":"### Importing images ","3f947b75":"### Importing csv","fd2141d8":"#### Kappa loss","1691e198":"### Scaling images","eb0ddb56":"### Encoding Finding column\n","ad21f79e":"### Saving normal and viral images in test and train folder separately","c11e5744":"#### binary crossentropy","aa0eb644":"## Training CNN"}}