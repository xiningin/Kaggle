{"cell_type":{"12f79e2d":"code","0ef07ea4":"code","194db385":"code","92d06ccf":"code","b8d49c40":"code","5311c56b":"code","c4ae637e":"code","56686a86":"code","c84e00d9":"code","bf81e7f4":"code","1bafc338":"code","1d149dde":"code","ce5aa69c":"code","c36c3708":"code","7004663a":"code","cac9b14d":"code","3b66d558":"code","32f9fb13":"code","4c44e199":"code","5dcfa2de":"code","aebbae69":"code","e9a34e2b":"code","3409f41f":"code","3c16c9e3":"code","05e8288d":"code","e4f798f5":"code","b9a140bc":"code","6bcf94c7":"code","d00eda38":"code","8fb49e65":"code","910b41ae":"code","6fc77472":"code","8f58d9f1":"code","c17e6253":"code","1cc8f3d1":"code","eade0081":"code","17a3f308":"code","3f758504":"code","5ca679e1":"code","12b77878":"code","8179e731":"code","84b44356":"code","dbbabb1f":"code","6a571847":"code","e40f9417":"code","a3c33cec":"code","e6c02354":"code","2c0be22b":"markdown","0f8ee7c3":"markdown","bc712b33":"markdown","c476c76c":"markdown","aaf8b1ea":"markdown","2a3e57da":"markdown","2753d0d9":"markdown","e8508d29":"markdown","c5994c27":"markdown","9721ef07":"markdown","781cdb58":"markdown","a29803d3":"markdown","5dff3b0d":"markdown","0083f59a":"markdown","d8288027":"markdown","ae00a180":"markdown","1bb35a09":"markdown","ad277db6":"markdown","428cc0a3":"markdown","8276da45":"markdown","5ab42872":"markdown","3f29950b":"markdown","3b5d1c84":"markdown","f60bda6d":"markdown","c5451a75":"markdown","a00ea627":"markdown","75db27d5":"markdown","2b83e9a8":"markdown","617d9db4":"markdown","974db556":"markdown","fffa51e9":"markdown","2d06fcfc":"markdown","8a517047":"markdown","880035fe":"markdown"},"source":{"12f79e2d":"#call libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pylab\nimport math\nimport seaborn as sns\n\n# Set default matplot figure size\npylab.rcParams['figure.figsize'] = (6.5, 5.0)\n\n#Turn off pandas warning for changing variables & future warnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\npd.options.mode.chained_assignment = None","0ef07ea4":"#set random seed\nnp.random.seed(123)","194db385":"#Import dataset and look at what information we are dealing with\ntitanic = pd.read_csv(\"..\/input\/train.csv\", header = 0)\ntitanic.head(20)\n","92d06ccf":"#column with names all passangers\nnames = titanic[\"Name\"]\n\n#Check whether there are duplicates in the name list\nduplicates = names[names.duplicated()]\nprint(duplicates)","b8d49c40":"titanic = titanic.drop(columns = \"PassengerId\")\ntitanic = titanic.drop(columns = \"Ticket\")\ntitanic = titanic.drop(columns = \"Name\")\ntitanic.head()","5311c56b":"titanic.columns = (['Survived', 'Class', 'Sex', 'Age', \n                    'n_Siblings_Spouse', 'n_Parents_Chidren', \n                    'Fare_Price', 'Cabin_ID', 'Embarked'])","c4ae637e":"print(\"Summary statistics of the numerical columns:\")\nprint()\nprint(titanic.describe())\nprint()\nprint(\"Missing values per column:\")\nprint()\nprint(titanic.isna().sum())","56686a86":"# Turn sex, class, survived and embarked in categorical variable\ntitanic[\"Sex\"] = titanic.Sex.astype('category')\ntitanic[\"Class\"] = titanic.Class.astype('category')\ntitanic[\"Survived\"] = titanic.Survived.astype('category')\ntitanic[\"Embarked\"] = titanic.Embarked.astype('category')\n\n#Rename Embarked cities\ntitanic[\"Embarked\"] = titanic.Embarked.cat.rename_categories({\"S\" :\"Southampton\",\n                                                              \"C\" : \"Cherbourg\",\n                                                              \"Q\" : \"Queenstown\"})\n#Plot barplots for the independent categorical variables \nsns.countplot(titanic.Sex).set_title('Distribution of sexes')\nplt.show()\nsns.countplot(titanic.Class).set_title('Distribution of classes')\nplt.show()\nsns.countplot(titanic.Embarked).set_title('Distribution of where people embarked')\nplt.show()","c84e00d9":"#Plot the distribution of the ticket price\nsns.distplot(titanic.Fare_Price, bins  = 50).set_title('Distribution of ticket prices')\nplt.show()\n\n#Plot the distribution of ages\n#Since there are Na's in the age distribution, we specify we only want to see the distribution of the available age data\nsns.distplot(titanic.Age[-titanic.Age.isnull()], bins = 40).set_title('Distribution of ages')\nplt.show()","bf81e7f4":"babies = titanic[titanic.Age < 2]\nbabies","1bafc338":"#specify the ages that should be turned into 0 and change types to integer\ntitanic.Age[titanic.Age < 1] = 0\ntitanic.Age[-titanic.Age.isnull()] = titanic.Age[-titanic.Age.isnull()].astype('int')","1d149dde":"#show observations where fare price > 100,-\ntitanic[titanic.Fare_Price > 100]","ce5aa69c":"#Create the dataframe for ticket prices that were above 100,-\nthe_wealthy = titanic[titanic.Fare_Price > 100]\n\n#calculate the percentage that was either female and the percentage that embarked in Cherbourg within this dataframe. \nprint(\"Of the people that had a ticket price of more than 100,\",\n      (len(the_wealthy[the_wealthy.Sex == 'female'])) \/ (len(the_wealthy))*100, \n      \"% was female.\")\nprint()\nprint(\"Of the people that had a ticket price of more than 100,\",\n      (len(the_wealthy[the_wealthy.Embarked == 'Cherbourg'])) \/ (len(the_wealthy))*100,\n     \"% embarked in Cherbourg.\")","c36c3708":"NaN_ages = titanic[-(titanic.Age > -2)]\n\n#Plot barplots for the independent categorical variables \nsns.countplot(NaN_ages.Sex).set_title('Distribution of sexes ageless')\nplt.show()\nsns.countplot(NaN_ages.Class).set_title('Distribution of classes ageless')\nplt.show()\nsns.countplot(NaN_ages.Embarked).set_title('Distribution of where people embarked ageless')\nplt.show()\nsns.countplot(NaN_ages.Survived).set_title('Distribution of whether people survived ageless')\nplt.show()","7004663a":"#full dataset children\/parent distribution\nsns.countplot(titanic.n_Parents_Chidren).set_title('Children\/parents amongst titanic passengers')\nplt.show()\n#ageless children\/parent distribution\nsns.countplot(NaN_ages.n_Parents_Chidren).set_title('Children\/parents amongst ageless titanic passengers ')\nplt.show()\n\n#full dataset sibling\/spouse distribution\nsns.countplot(titanic.n_Siblings_Spouse).set_title('Siblings\/spouse amongst titanic passengers')\nplt.show()\n#ageless sibling\/spouse distribution\nsns.countplot(NaN_ages.n_Siblings_Spouse).set_title('Siblings\/spouse amongst ageless titanic passengers')\nplt.show()","cac9b14d":"#distinguish survival by sex\nsns.catplot('Sex', data = titanic, hue = 'Survived', kind='count', aspect=1.5)\nplt.show()","3b66d558":"#distinguish survival by class\nsns.catplot('Class', data = titanic, hue = 'Survived', kind='count', aspect=1.5)\nplt.show()","32f9fb13":"#distinguish survival by where people were embarked\nsns.catplot('Embarked', data = titanic, hue = 'Survived', kind='count', aspect=1.5)\nplt.show()","4c44e199":"#make a categorical variable for the ages\ntitanic.loc[(titanic.Age < 15), \"AgeCat\"] = \"Kids\"\ntitanic.loc[(titanic.Age >= 15) & (titanic.Age <= 30), \"AgeCat\"] = \"Adolescents\"\ntitanic.loc[(titanic.Age >= 31) & (titanic.Age <= 60), \"AgeCat\"] = \"Adults\"\ntitanic.loc[(titanic.Age >= 61), \"AgeCat\"] = \"Elderly\"","5dcfa2de":"#distinguish survival by agecategories\n\nsns.catplot('AgeCat', data = titanic, hue = 'Survived', kind='count', aspect=1.5)\nplt.show()","aebbae69":"#create dataframe for only the kid category\nsave_the_kids = titanic[titanic.AgeCat == \"Kids\"]\n#plot the kids' sex and survival\nsns.catplot('Sex', data = save_the_kids, hue = 'Survived', kind='count', aspect=1.5)\nplt.show()\n#plot the kids' ticket class and survival\nsns.catplot('Class', data = save_the_kids, hue = 'Survived', kind='count', aspect=1.5)\nplt.show()\n#plot the how many parents were with the children and their survival\nsns.catplot('n_Parents_Chidren', data = save_the_kids, hue = 'Survived', kind='count', aspect=1.5)\nplt.show()\n","e9a34e2b":"#convert all categories into numerical variables so they can be proberly used to model with\ntitanic.Sex = pd.CategoricalIndex(titanic.Sex)\ntitanic.Class = pd.CategoricalIndex(titanic.Class)\ntitanic.Embarked = pd.CategoricalIndex(titanic.Embarked)\n\n\ntitanic['Sex'] = titanic.Sex.cat.codes\ntitanic['Class'] = titanic.Class.cat.codes\ntitanic['Embarked'] = titanic.Embarked.cat.codes\n\ntitanic = titanic.drop([\"Cabin_ID\", \"AgeCat\"], axis = 1)","3409f41f":"titanic2 = titanic.dropna()","3c16c9e3":"#Split the data in a train and testset\ntitanic_dep = titanic2.Survived\ntitanic_indep = titanic2.drop(['Survived'], axis=1)\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(titanic_indep, titanic_dep, test_size=0.3)\n\n#Tree packages for checking the feature importance\nfrom sklearn.ensemble import RandomForestClassifier \nrf = RandomForestClassifier() \n\n# Build a forest and compute the feature importances\n## Fit the model on your training data.\nrf.fit(X_train, y_train) \n## And score it on your testing data.\nrf.score(X_test, y_test)\n","05e8288d":"feature_importances = pd.DataFrame(rf.feature_importances_,\n                                   index = X_train.columns,\n                                    columns=['importance']).sort_values('importance',\n                                                                        ascending=False)\nprint(feature_importances)","e4f798f5":"#oerform logistic regression and KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\n# Do a Kfold cross validation on the training data for k = 3\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nCVscores = cross_val_score(knn, X_train, y_train, cv = 10, scoring = \"accuracy\")\nprint(\"knn score:\", CVscores.mean())\n\n# Do a Kfold cross validation on the training data for a logistic regression\n\nlogregression = LogisticRegression(solver='liblinear')\nCVscores = cross_val_score(logregression, X_train, y_train, cv = 10, scoring = \"accuracy\")\nprint(\"logistic regression score:\", CVscores.mean())","b9a140bc":"#Import dataset and look at what information we are dealing with\ntitanic = pd.read_csv(\"..\/input\/train.csv\", header = 0)","6bcf94c7":"#Create dummy variable for married by looping over whether passangers names' contain Mr. or Mrs.\ntitanic[\"Mr.\"] = 0\nfor i in range(0,len(titanic[\"Name\"])):\n    if \"Mr.\" in titanic.loc[i][\"Name\"]:\n        titanic.at[i, \"Mr.\"] = 1\n\ntitanic[\"Mrs.\"] = 0\nfor i in range(0,len(titanic[\"Name\"])):\n    if \"Mrs.\" in titanic.loc[i][\"Name\"]:\n        titanic.at[i, \"Mrs.\"] = 1\n        \ntitanic[\"Miss.\"] = 0\nfor i in range(0,len(titanic[\"Name\"])):\n    if \"Miss.\" in titanic.loc[i][\"Name\"]:\n        titanic.at[i, \"Miss.\"] = 1        \n\n                \ntitanic[\"Master.\"] = 0\nfor i in range(0,len(titanic[\"Name\"])):\n    if \"Master.\" in titanic.loc[i][\"Name\"]:\n        titanic.at[i, \"Master.\"] = 1\n        \ntitanic[\"Other_Title\"] = 1 - (titanic[\"Master.\"] + titanic[\"Miss.\"] + titanic[\"Mrs.\"] + titanic[\"Mr.\"])\n","d00eda38":"#Sort by ticket number and see if we can find any interesting patterns\ntitanic.sort_values(by = \"Ticket\")","8fb49e65":"#Display the passangers that didn't pay for their tickets\ntitanic.loc[(titanic.Fare == 0)]","910b41ae":"#generate a zero ticket fare variable\ntitanic[\"Zero_ticket_fare\"] = 0\nfor i in range(0,len(titanic[\"Fare\"])):\n    if titanic.loc[i][\"Fare\"] == 0:\n        titanic.at[i, \"Zero_ticket_fare\"] = 1\n\n\n#locate and change ticket nr.\ntitanic.loc[(titanic.Ticket == \"LINE\"), \"Ticket\"] = str(370160)","6fc77472":"#Create a seperate dataframe with count data for how often each ticket occurs\nticket_counts = titanic['Ticket'].value_counts()\nticket_counts = pd.Series.to_frame(ticket_counts)\nticket_counts[\"Ticket_nr\"] = ticket_counts.index\nticket_counts.index = range(0,len(ticket_counts))\nticket_counts.columns = [\"Ticket_group_size\", \"Ticket\"]\n\n#Add this column to the full dataframe \ntitanic = pd.merge(titanic, ticket_counts, how='outer', on='Ticket')","8f58d9f1":"#Now calculate the actual ticket value\ntitanic[\"Price_per_person\"] = (titanic[\"Fare\"] \/ titanic[\"Ticket_group_size\"])","c17e6253":"#generate dummies for the class variable\nclass_dummies = pd.get_dummies(titanic.Pclass)\nclass_dummies.columns = [\"First_class\", \"Second_class\", \"Third_class\"]\ntitanic = pd.concat([titanic, class_dummies], axis=1, sort=False)","1cc8f3d1":"#generate dummies for where the ship embarked\nembarked_dummies = pd.get_dummies(titanic.Embarked)\nembarked_dummies.columns = [\"Southampton\", \"Cherbourg\", \"Queenstown\"]\ntitanic = pd.concat([titanic, embarked_dummies], axis=1, sort=False)","eade0081":"#Drop columns that wont be used in the analysis\ntitanic = titanic.drop(columns = \"PassengerId\")\ntitanic = titanic.drop(columns = \"Ticket\")\ntitanic = titanic.drop(columns = \"Name\")\ntitanic = titanic.drop(columns = \"Cabin\")\ntitanic = titanic.drop(columns = \"Fare\")\ntitanic = titanic.drop(columns = \"Pclass\")\ntitanic = titanic.drop(columns = \"Embarked\")","17a3f308":"#Seperate columns\ntitanic_indep = titanic.drop(columns = \"Survived\")\ntitanic_dep = titanic[\"Survived\"]","3f758504":"#Change the type of all dummies to categories \ntitanic_indep[\"Sex\"] = titanic_indep.Sex.astype('category')\ntitanic_indep[\"First_class\"] = titanic_indep.First_class.astype('category')\ntitanic_indep[\"Second_class\"] = titanic_indep.Second_class.astype('category')\ntitanic_indep[\"Third_class\"] = titanic_indep.Third_class.astype('category')\ntitanic_indep[\"Southampton\"] = titanic_indep.Southampton.astype('category')\ntitanic_indep[\"Cherbourg\"] = titanic_indep.Cherbourg.astype('category')\ntitanic_indep[\"Queenstown\"] = titanic_indep.Queenstown.astype('category')\n\n#convert all categories into numerical variables so they can be properly used to model with\ntitanic_indep.Sex = pd.CategoricalIndex(titanic_indep.Sex)\ntitanic_indep.First_class = pd.CategoricalIndex(titanic_indep.First_class)\ntitanic_indep.Second_class = pd.CategoricalIndex(titanic_indep.Second_class)\ntitanic_indep.Third_class = pd.CategoricalIndex(titanic_indep.Third_class)\ntitanic_indep.Southampton = pd.CategoricalIndex(titanic_indep.Southampton)\ntitanic_indep.Cherbourg = pd.CategoricalIndex(titanic_indep.Cherbourg)\n\n#Turn sex variable into a dummy\ntitanic_indep['Sex'] = titanic_indep.Sex.cat.codes","5ca679e1":"# Take the age column seperate and split them in the section with Na's and without Na's\nagecolumn = titanic_indep[\"Age\"]\nAges_noNA = agecolumn[agecolumn > -1]\nAges_yesNa = agecolumn[agecolumn.isna()]\n\n# Normalize the section without Na's and add both sections back together\nAges_noNA = (Ages_noNA - Ages_noNA.mean()) \/ (Ages_noNA.max() - Ages_noNA.min())\nagecolumn = Ages_noNA.append(Ages_yesNa, ignore_index=False)","12b77878":"# Take all columns except age\nrestcolumns = titanic_indep.loc[:, titanic_indep.columns != \"Age\"]\nrestcolumns = restcolumns.apply(pd.to_numeric)\n\n# Apply normalization to each column of this dataframe\nfor i in (range(0, len(list(restcolumns)))):\n    restcolumns.iloc[:,[i]] = ((restcolumns.iloc[:,[i]] - restcolumns.iloc[:,[i]].mean()) \/ \n                               (restcolumns.iloc[:,[i]].max() - restcolumns.iloc[:,[i]].min()))","8179e731":"#Add the normalized age columns to the dataframe\nrestcolumns[\"Age\"] = agecolumn\ntitanic_indep = restcolumns","84b44356":"from fancyimpute import KNN\n\n#We use the train dataframe from Titanic dataset\n#fancy impute removes column names, so let's save them.\ntitanic_cols = list(titanic_indep)\n\n# Use Knn to fill in each value and add the column names back to the dataframe\ntitanic_indep = pd.DataFrame(KNN(k = 9).fit_transform(titanic_indep))\ntitanic_indep.columns = titanic_cols\ntitanic_indep[\"Age\"] = round(titanic_indep[\"Age\"])","dbbabb1f":"from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(titanic_indep, titanic_dep, test_size=0.3)\n\n#Tree packages for checking the feature importance\nfrom sklearn.ensemble import RandomForestClassifier \nrf = RandomForestClassifier() \n\n# Build a forest and compute the feature importances\n## Fit the model on your training data.\nrf.fit(X_train, y_train) \n\nrf_predictions = rf.predict(X_test)\n## And score it on your testing data.\nrf.score(X_test, y_test)","6a571847":"#perform logistic regression and KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\n# Do a Kfold cross validation on the training data for k = 3\n\nknn = KNeighborsClassifier(n_neighbors = 7)\nCVscores = cross_val_score(knn, X_train, y_train, cv = 10, scoring = \"accuracy\")\nprint(\"knn score:\", CVscores.mean())\n\nknn.fit(X_train, y_train)\nknn_predictions = knn.predict(X_test)\n\n# Do a Kfold cross validation on the training data for a logistic regression\n\nlogregression = LogisticRegression(solver='liblinear')\nCVscores = cross_val_score(logregression, X_train, y_train, cv = 10, scoring = \"accuracy\")\nprint(\"logistic regression score:\", CVscores.mean())\n\n        \nlogregression.fit(X_train, y_train)        \nlogreg_predictions = logregression.predict(X_test)\n","e40f9417":"#Print the accuracy scores for each model.\nfrom sklearn.metrics import accuracy_score\n\nprint(\"Random forest fit score\",accuracy_score(rf_predictions, y_test))\nprint(\"Knn fit score\", accuracy_score(knn_predictions, y_test))\nprint(\"Logistic regression fit score\", accuracy_score(logreg_predictions, y_test))","a3c33cec":"#Let's see what the accuracy of the combined score would be:\ncombined_predictions = (rf_predictions + knn_predictions + logreg_predictions)\/3\ncombined_predictions[combined_predictions < 0.5] = 0\ncombined_predictions[combined_predictions > 0.5] = 1\nprint(\"Combined predictions fit score\", accuracy_score(combined_predictions, y_test))","e6c02354":"Log_regression_model = logregression.fit(titanic_indep, titanic_dep)\nimport pickle\n\n# save the model to disk\nfilename = 'Logistic_reg_model.sav'\npickle.dump(Log_regression_model, open(filename, 'wb'))","2c0be22b":"### What's in the ticket?!\n\nAlso, there might be some interesting information observable in people's ticket number as well. Lets see what kind of information we can extract from people's ticket numbers.","0f8ee7c3":"### Normalizing the independent variables\nSince KNN will be used to impede missing values and eventually as an algorithm to predict survivability, it is necessary to normalize the data so the variables values comparable to the model. However, the missing values in the age column make it not possible to normalize the complete dataframe at once. Therefore, I'll do the normalization column by columns, where I first take care of the ages. ","bc712b33":"#### Wealthy French women\nWhen looking at the table presented above, interestingly enough it appears that there are many women that have bought the more expensive tickets, even though the passengerlist is predominately filled with men (65%). This means that the lower tickts were relatively sold even more to men than to women. Perhaps poor men were more attracted in the 1910's to migrate and find their luck in the USA than the women in their social class. <br \/>Also interesting to note is that many of the observations seem to have embarked in Cherbourg, whereas in general most people embarked in Southhampton (only 19% of the total passengers embarked in Cherbourg). Must be a fancy place. \n\nI'll estimate the specific percentages to see whether these observations are actually true.","c476c76c":"### Dep & Indep\nSeperate the columns for the dependent variable (survived) and the independent variables, so that the independent variable columns can be further preprocessed (normalization etc.)","aaf8b1ea":"### Let's take a random guess\nCurrently our model is not performing very well. Knn is performing just as well as a random guess. What does this mean? That its time for some:\n## FEATURE ENGINEERING","2a3e57da":"#### Just some fun stuff \nLet's see (because we can) whether the average of the 3 predictions give a more accurate estimation of the survivability. By adding each predicion together and dividing it by 3. ","2753d0d9":"Since we want to achieve a much higher accuracy (100000% correct please), the dataset requires some hard mangling and engineering. I'll give a step, by step explanation of the things that first come to mind, and apply them. Afterwards i'll test the model again to see whether my predictions have led to an improvement.","e8508d29":"#### Forever Young\nAlright so I took the age categories very broad but it was mainly to get some idea of what's going on. These categories therefore probably give an incomplete image. Anyhow, the kids were the luckiest ones. More Survived than died, which is nice. The elderly seem to have experienced the worst survival ratio. about 1\/4th survived. Ice Cold water and the ability to run towards the lifeboats might have played a role here. Adolescents and adults both were not very lucky, but someone has to take the blow amiright?","c5994c27":"### Column Names\nNext let's change some names of the columns. This will make future tables easier for people that have not read about what the variables exactly mean. ","9721ef07":"#### The ladies live\nWell this plot already shows that sex was of a huge huge influence when it comes to surviving. About 1\/6th of the men appear to have survived compared to 3\/4th of the women. Well done ladies!","781cdb58":"### Lets make a quick model!\n\nAnother way that can help to learn more about the data is to create a quick first model without any specific feature engineering. Let's also run a random forest to see what variables currently have to most predictive power. For simplicty lets start by dropping rows where the age is unknown and getting rid of cabinID.","a29803d3":"### What's in the name!?\nOn the current passengers list, everyone has a title. Common titles are Mr. (for men), Mrs. (for married women), Miss. (for unmarried women), Master. (for young boys). Some passangers did not go by either of these the titles These were people with special titles. Lets use this informtion by making a variables for each specific title.","5dff3b0d":"#### Okay \nSo that quick hypothesis appeared to be correct. Distributions seem to shift depending on specific customer segments. \n#### The missing ages\nThe 177 missing observations regarding the age of passengers is currently still unsolved. I think i will impute these values, but let's first see whether there is a specific pattern observable for the customers where the age is missing. ","0083f59a":"### Create the models\nAllright so here the moddeling finally starts. For now i'm not going to get into the actual parameter engineering, but I might do this in the near future. I'll start with generating a Random forest model, knn model and Logistic regression model using a seperate training set, and see how acurate they are on the test set. ","d8288027":"# Predicting Titanic Surviors\n\nFor more information regarding this dataset, visit: \nhttps:\/\/www.kaggle.com\/c\/titanic\/data.\n\nThis particular script will contain an initial exploration of the data, followed by some necessary preprocessing, \nfeature enineering and selection, and finally the models and results.\n\n## Some general information regarding the Titanic\nThe Titanic is the world famous ship that didn't even manage to complete 1 trip. On the 10th of April 1912 it left Southhampton, England, to first pick up passengers in Cheroux, France and Queenstown, Ireland and make its way to New York. Unfortunately, in the night of the 14th\/15th of April, it was steered into an iceberg. This wrecked the ship. Since there were not enough lifeboats, people were stuck on the sinking ship. As a result, the majority of the passengers drowned or froze to death in the icy waters south of Newfoundland, Canada.\n\n\n##### SAD\nNow that's out of the way, let's start by importing the libraries used in the exploratory part:","ae00a180":"### About the plots\n\nAlright so: \n- There were almost twice as many men as there were females on the ship. \n- Majority was in third class, 1st and second class have almost the same amount of passengers\n- Majority of the people embarked in Southampton, followed by Queenstown and Cherbourg\n\nNow lets see the distribution of ages and ticket prices, followed by how the different categories compare depending on their survival rate!","1bb35a09":"### Summary statistics and NaN's\nNow that's out of the way let's look at what the available columns exactly entail. \n\nIt was observable that there are multiple missing vaues in a few columns. Let's see which columns have these missing values and ","ad277db6":"### Traveling with the bunch\nAnother thing that was observable when looking at the ticket numbers is that people were traveling on the same ticket number and that the costs that accompanied the ticket are always the same amount. There are 2 interesting things that can be obtained from this knowledge: 1 is with how many persons the passengers were traveling by counting the amount of duplicates per ticket), and the price per person for their trip (by dividing the total fare price by the passengers on 1 ticket). \n\nThe following lines of code will provide these 2 variables.","428cc0a3":"### What can we make up of this?\n\nOkay so lets list a few interesting facts that are distinghuisable from these previous statistics:\n\n- There are a total of 891 observations\n- +- 38% survived (342 passengers)\n- There were 3 classes\n- There were many youngsters\/ adolescents on the boat half of the people were below the age of 28)\n- The youngest person had an age of 0.42. This is either a very weird measurement, or there is an error and he should actually be 42 years old.\n- Most people were not traveling with their siblings\/spouse\/parents\/children.\n- There is one(?) observation that had 8 siblings on board. Jesus that's a big family.\n- There were fare prices of 0,-. That's one cheap ride.\n\n\n- For 177 observations the age was missing. Thats +- 20% of the data. \n- 687 observations have no Cabin_ID. Maybe they did not have a cabin?\n- For 2 observations it is unknown where they had embarked.\n\nThis information will come in handy in a later stage when deciding which features will be used to model with and\/or when features are being engineered. Let's first make sure some of the numerical columns become categories and take a look at how the data is distributed!","8276da45":"### Impute missing ages\nSince there are many ages missing, I'll impute them using a machine learning technique instead of using the mean. This means that the ages will be imputed based on the other variables","5ab42872":"#### No major differences\nPeople seem to be slightly more on themselves compared to the full dataset. The difference does not appear to be striking though.\n\n### Dead or alive???\nLet's have a closer look at the thing we are interested in the most when researching this dataset: What did the people that survived the disaster have relatively a lot in common. Did they all embark in the same city? Were they in the same age group? Did the men leave the women to die while rowing out of the danger zone quickly? The following few plots and statistics should provide a more in-depth ","3f29950b":"#### Babies\nIt seems that the children that were not born yet were given floats depending on how close it was to their birthday(?) Im annoyed by the fact that this causes all the ages to be catagorized as 'floats' so i'm gonna change these values into 0's (since that is their actual age.","3b5d1c84":"#### Feature imporances.\n\nSo the sex, age and the fare price are considered the most important features in the current state of the data. Class a bit less, but class and fare price are ofcourse highly intertwined.","f60bda6d":"#### Regarding the Ageless...\nAllright so it appears that the people where no age was noted have a similiar distribution of males and females when compared to the full dataset. They overwhelmingly were in third class, and by the looks of it, almost everyone that embarked in Queenstown misses lacks their respective age. The amount of survivors amongst these ageless people appears to be similarly distributed when compared with the full dataset.\n\nlet's extend this analysis to see whether spouse\/children might have caused the data to go missing....","c5451a75":"### Prices and age\n\nMost ticket prices seem fairly low. There are a few exeptions of very high prices. Later on i'll check the average price per class.\n\nThe age seems fairly normally distributed, although it is skewed to the right and there are many values of zero's. Are these babies or is there something weird with the data regarding the ages of the passengers? I'll have a closer look at what these children's values entail.\n","a00ea627":"#### Sacre Bleu!\nThere are some differences distinguishable between where people embarked and whether they surived or not. The French had a bigger chance of survival than the English had. Again, this seems more than a correlation than as a causation, considering that the French were outnumbering the others in the first-class suites. ","75db27d5":"#### Show some class\nWhich class your ticket the people were residing in was also of major influence on whether they survived. This ofcourse can be related to sex as well, since there were a lot more men in third class than female. Anyhow, Third class citizens died by the numbers, where the mahjority of the first classers survived.  ","2b83e9a8":"#### Cheap but fatal\nA few things can be noted here. First: most of the people that had a zero ticket fare died. For a lot of them, their age is missing. They are all men. Looking up their names, it seems that these passengers were either working for the titanics main company (White Star Lines) or were working for partners of the company. Therefore most of them probably helped people get off the boat before going themselves, which has led to their death. This is noteworthy, hence i'll create a variable that notes this.\n\nA few of the passangers in this list has the ticket number \"LINE\". I looked for information on the people traveling on these tickets and found that they were travelling together and were rebooked from a different ship. Their actual ticket number number is 370160, so lets change that.","617d9db4":"### What? No Munny?\nAlso when scrolling through the tickets and looking at their respective fares, I saw a few people that had a ticket fare of 0. Let's have a closer look at these people to see if there is a pattern observable for these uncommon ticket prices.","974db556":"## Logistic regression wins\nIn the end, the logistic regression model was estimated on average to be the most accurate. Therefore I'll make a model using the full dataset (so without splitting it to test and train). Then, I'll use this model on the competition testset to check the results. **EXCITING**  ","fffa51e9":"### Nice!\nFirst I'll check if there are any duplicates by finding duplicates in the names column.\n\nI already see some columns that i'm pretty sure of that removing them do not cause any problems, namely PassengerID and Name. It seems extremely unlikely that someone's name or their \"ID\" has anything to do with surviving a boat crash. Someone's ticket ID also seems useless since everyone's ID is likely to be unique, hence has no predicting power. <br \/>For now I'll leave Cabin ID in the dataset since its numbers might entail information whether someones cabin was in proximity of a lifeboat \n\nDropping stuff makes the data cleaner, so i'll start with removing these columns before going further with the analysis.","2d06fcfc":"#### The rich kids\nNow lets have a closer look at the more wealthy people, the ones that purchased a more expensive boat ticket, and see what kind of people these were.","8a517047":"#### All about the kids\nWhen taking a look at the passengers up untill the age of 14, we see that even for the young ones wthe women were more lucky than the man. Also, sad to see is that class mattered even for the children. Kids in third class had lower survivability rate than in dhe higher classes (only 1 didn't survive in all of class 1 and 2)\n\nAlso remarkable is that children that traveled with 2 parents had a much lower survival rate than children that traveled with 1 parent or no parents. Could be a coincidence, or there might be an interesting cause to this.","880035fe":"### Dummies for the cats\n\nTo use some of the categorical values in our model, we will have to dummify them. in the following frames I will create dummies for passenger's class and their location of embarkment  "}}