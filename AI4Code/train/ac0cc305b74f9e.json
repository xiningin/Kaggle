{"cell_type":{"85fd7a2c":"code","92db3f34":"code","9cb3880b":"code","69a54366":"code","c756d88e":"code","acb5dee7":"code","a6a1ccec":"code","168097ae":"code","d1b9cde5":"code","6152cd47":"code","6a3e38ef":"code","6a69385c":"code","bff498e6":"code","4f9dc6b0":"code","126c3b55":"code","b845215f":"code","5a321f54":"code","d830d0a9":"code","4ee57726":"code","c58cfea8":"code","b356e414":"code","da00c351":"code","dc02e377":"code","b937cd11":"code","1963962a":"code","0435c38e":"code","c106e956":"code","ad6fa296":"code","d777bed3":"code","daacff38":"code","40196426":"code","91ae66e7":"code","49e01747":"code","9ff6dcaf":"code","ae65fe3f":"code","cdbd3b50":"code","17c4efc1":"code","ca063ae3":"code","f2531c0b":"code","4683bcf0":"code","57958842":"markdown","c7fbb29a":"markdown","777d4662":"markdown","64275f24":"markdown","62a7dceb":"markdown","2987f28c":"markdown","c45000eb":"markdown","020dc88b":"markdown","27b00411":"markdown","8ffdc8ef":"markdown","43f1d791":"markdown","423790f4":"markdown","70fd5f93":"markdown","1408cc3c":"markdown","baf6f5f6":"markdown","24a81186":"markdown","af35ccfc":"markdown","aaf3b9a1":"markdown","a8781264":"markdown","e30975a9":"markdown","643627b4":"markdown","21ec73d6":"markdown","8a3ff8f2":"markdown","8e5fb4b7":"markdown","7494ea01":"markdown"},"source":{"85fd7a2c":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style='darkgrid')\nimport plotly.graph_objs as go\nimport plotly.offline as py\n\n# For Data Preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# For Model Building\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\n\n# For Model Evaluation\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_roc_curve, auc\nfrom sklearn.model_selection import learning_curve\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","92db3f34":"# Import the dataset as CSV file\ndf = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')","9cb3880b":"# Preview the dataset\ndf.head()","69a54366":"# Rename the origical columns\ndf.columns = ['Age', 'Sex', 'Chest_pain_type', 'Resting_bp', \n              'Cholesterol', 'Fasting_bs', 'Resting_ecg', \n              'Max_heart_rate', 'Exercise_induced_angina', \n              'ST_depression', 'ST_slope', 'Num_major_vessels',\n              'Thallium_test', 'Condition']","c756d88e":"# Check the renamed columns\ndf.head()","acb5dee7":"df.isnull().sum()","a6a1ccec":"df.info()","168097ae":"def get_train_test_split(data):\n    \"\"\"\n    Split into train and test set:\n    - X = independent variables.\n    - y = dependent variable.\n    - Setup train_size, 80%, and test_size, 20%, of the dataset.\n    \"\"\"\n    X = data.drop(['Condition'], axis=1)\n    y = data.Condition\n    X_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                        test_size=0.2,\n                                                        random_state=42)\n    print(f'Shape of X_train {X_train.shape}')\n    print(f'Shape of X_test {X_test.shape}')\n    print(f'Shape of y_train {y_train.shape}')\n    print(f'Shape of y_test {y_test.shape}')\n    return X_train, X_test, y_train, y_test    \n \n    \nX_train, X_test, y_train, y_test = get_train_test_split(df);    ","d1b9cde5":"# Normalize the dataset\ndef get_normalization(X):\n    scaler = MinMaxScaler()\n    X_normalized = scaler.fit_transform(X)\n    return X_normalized\n\n\nX_train = get_normalization(X_train);\nX_test = get_normalization(X_test);","6152cd47":"# Build and fit Logistic Regression model\nlogreg = LogisticRegression()\nlogreg = logreg.fit(X_train, y_train)","6a3e38ef":"def get_model_accuracy(model, X_test, y_test):\n    \"\"\"\n    Return the mean accuracy of model on X_test and y_test\n    \"\"\"\n    model_acc = model.score(X_test, y_test)\n    return model_acc","6a69385c":"# Accuracy in Logistic Regression model\nlogreg_acc = get_model_accuracy(logreg, X_test, y_test)\nprint(f'Logistic Regression Accuracy: {logreg_acc:.4}')\nprint()\n\n# Predict class for X_test\ny_pred_logreg = logreg.predict(X_test)\n\n# Classification Report of logistic regression model\nprint(classification_report(y_pred_logreg, y_test))","bff498e6":"def get_best_parameters_GridSearchCV(model, params, X_train, y_train):\n    clf = GridSearchCV(model, params, cv=5)\n    clf.fit(X_train, y_train)\n    \n    best_params = clf.best_params_\n    print(f'Best Parameters in {model}: {best_params}')\n    print()\n    \n    best_estimator = clf.best_estimator_\n    return best_estimator\n  ","4f9dc6b0":"params_logreg = {'penalty': ['l1', 'l2', 'elasticnet'],\n                 'C': np.logspace(-4, 4, 20),\n                 'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}","126c3b55":"# Build and fit parameter tuned Logistic Regression model\nlogreg_gscv = get_best_parameters_GridSearchCV(logreg, params_logreg, X_train, y_train) \nlogreg_gscv = logreg_gscv.fit(X_train, y_train)\n\n# Accuracy test score for logreg_cv\nlogreg_gscv_acc = get_model_accuracy(logreg_gscv, X_test, y_test)\nprint(f'Logistic Regression Model with GridSearchCV Accuracy: {logreg_gscv_acc:.4}')\nprint()\n\n# Make prediction on test dataset\ny_pred_logreg_gscv = logreg_gscv.predict(X_test)\n\n# Classification Report of logreg_cv\nprint(classification_report(y_pred_logreg_gscv, y_test))","b845215f":"# Build and fit Random Forest model\nrf_model = RandomForestClassifier()\nrf_model = rf_model.fit(X_train, y_train)\n\n# Accuracy score for Random Forest\nrf_acc = get_model_accuracy(rf_model, X_test, y_test)\nprint(f'RandomForestClassifier Accuracy: {rf_acc:.4}')\nprint()\n\n# Predict class for X_test\ny_pred_rf = rf_model.predict(X_test)\n\n# Classification Report of Random Forest model\nprint(classification_report(y_pred_rf, y_test))","5a321f54":"params_rf = {'max_depth': [2, 3, 4, 5],\n               'max_features': ['auto', 'sqrt', 'log2'],\n               'n_estimators':[0, 10, 50],\n               'random_state': [0, 10, 42]}","d830d0a9":"# Build and fit Random Forest model with the best hyperparameters\nrf_gscv = get_best_parameters_GridSearchCV(rf_model, params_rf, X_train, y_train)\nrf_gscv = rf_gscv.fit(X_train, y_train)\n\n# Accuracy score for rf_gscv\nrf_gscv_acc = get_model_accuracy(rf_gscv, X_test, y_test)\nprint(f'Random Forest with GridSearchCV Accuracy: {rf_gscv_acc:.4}')\nprint()\n\n# Make prediction on test dataset\ny_pred_rf_gscv = rf_gscv.predict(X_test)\n\n# Classification Report of grid_rf_model\nprint(classification_report(y_pred_rf_gscv, y_test))","4ee57726":"# Build and fit Support Vestor Classification model\nsvm_model = SVC()\nsvm_model = svm_model.fit(X_train, y_train)\n\n# Accuracy score for svm_model\nsvm_acc = get_model_accuracy(svm_model, X_test, y_test)\nprint(f'SVM Accuracy: {svm_acc:.4}')\nprint()\n\n# Predict class for X_test\ny_pred_svm = svm_model.predict(X_test)\n\n# Classification Report of svm_model\nprint(classification_report(y_pred_svm, y_test))","c58cfea8":"params_svm = {'C': [0.1,1, 10, 100], \n              'gamma': [1, 0.1, 0.01, 0.001, 'scale', 'auto'],\n              'kernel': ['linear', 'poly', 'sigmoid']}","b356e414":"# Build and fit SVM model with the best parameters\nsvm_gscv = get_best_parameters_GridSearchCV(svm_model, params_svm, X_train, y_train)\nsvm_gscv = svm_gscv.fit(X_train, y_train)\n\n# Accuracy test score for svm_gscv\nsvm_gscv_acc = get_model_accuracy(svm_gscv, X_test, y_test)\nprint(f'SVM with GridSerchCV Accuracy: {svm_gscv_acc:.4}')\nprint()\n\n# Predict class for X_test\ny_pred_svm_gscv = svm_gscv.predict(X_test)\n\n# Classification Report of svm_gscv\nprint(classification_report(y_pred_svm_gscv, y_test))","da00c351":"# Build and fit XGBoost model\nxgb_model = XGBClassifier(eval_metric='error')\nxgb_model = xgb_model.fit(X_train, y_train)\n\n# Accuracy score for xgb_model\nxgb_acc = get_model_accuracy(xgb_model, X_test, y_test)\nprint(f'XGBoost Classifier Accuracy: {xgb_acc:.4}')\nprint()\n\n# Predict class for X_test\ny_pred_xgb = xgb_model.predict(X_test)\n\n# Classification Report of xgb_model\nprint(classification_report(y_pred_xgb, y_test))","dc02e377":"params_xgb = {'learning_rate': [4, 5, 6], \n              'max_depth': [4, 5, 6],\n              'min_child_weight': [4, 5, 6],\n              'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1],\n              'eval_metric': ['error']}\n\n","b937cd11":"# Build and fit XGBoost Classifier model with the best parameters\nxgb_gscv = get_best_parameters_GridSearchCV(xgb_model, params_xgb, X_train, y_train)\nxgb_gscv = xgb_gscv.fit(X_train, y_train)\n\n# Accuracy score for xgb_gscv\nxgb_gscv_acc = get_model_accuracy(xgb_gscv, X_test, y_test)\nprint(f'XGBoost Classifier with GridSerchCV Accuracy: {xgb_gscv_acc:.4}')\nprint()\n\n# Predict class for X_test\ny_pred_xgb_gscv = xgb_gscv.predict(X_test)\n\n# Classification Report of xgb_cv\nprint(classification_report(y_pred_xgb_gscv, y_test))","1963962a":"def plot_confusion_matrix(y_test, y_predict, color, title):\n    \"\"\"\n    Confusion matrix to evaluate the accuracy of Models\n    \"\"\"\n    cm = confusion_matrix(y_test, y_predict)\n    sns.heatmap(cm, annot=True, fmt='d', cmap=color)\n    plt.title(title)\n    plt.xlabel('Predicted', fontsize=15)\n    plt.ylabel('Actual', fontsize=15)\n    plt.tight_layout()\n    plt.show()\n    ","0435c38e":"# Logistic Regression model\ntitle = 'Confusion Matrix of Logistic Regression'\nplot_confusion_matrix(y_test, y_pred_logreg, 'Blues', title);\n\n# Logistic Regression with GridSearchCV\ntitle = 'Confution Matrix of Logistic Regression with GridSearchCV'\nplot_confusion_matrix(y_test, y_pred_logreg_gscv, 'Greens', title)","c106e956":"# Random Forest model\ntitle = 'Confution Matrix of Random Forest'\nplot_confusion_matrix(y_test, y_pred_rf, 'Blues', title);\n\n# Random Forest with hyperparameter tuning\ntitle = 'Confution Matrix of Random Forest with GridSearchCV'\nplot_confusion_matrix(y_test, y_pred_rf_gscv, 'Greens', title)","ad6fa296":"# SVM model\ntitle = 'Confution Matrix of SVM Model'\nplot_confusion_matrix(y_test, y_pred_svm, 'Blues', title);\n\n# SVM with parameter tuning\ntitle = 'Confution Matrix of SVM Model with GridSearchCV'\nplot_confusion_matrix(y_test, y_pred_svm_gscv, 'Greens', title)","d777bed3":"# XGBoost Classifier model\ntitle = 'Confution Matrix of XGBoost Classifier'\nplot_confusion_matrix(y_test, y_pred_xgb, 'Blues', title);\n\n# XGBoost Classifier with hyperparameter tuning\ntitle = 'Confution Matrix of XGBoost Model with GridSearchCV'\nplot_confusion_matrix(y_test, y_pred_xgb_gscv, 'Greens', title)","daacff38":"def plot_roc_auc(model1, model2, X_test, y_test, title1, title2):\n    \"\"\"\n    Show both ROC and AUC of the model and its parameter tuning model\n    \"\"\"\n    plot_roc_curve(model1, X_test, y_test)\n    plt.title(title1, fontsize=13)\n    \n    plot_roc_curve(model2, X_test, y_test)\n    plt.title(title2, fontsize=13)\n    plt.show()\n    ","40196426":"# ROC and AUC for Logistic Regression models\ntitle1 = 'ROC and AUC of Logistic Regression'\ntitle2 = 'ROC and AUC of Logistic Regression with GridSearchCV'\nplot_roc_auc(logreg, logreg_gscv, X_test, y_test, title1, title2);","91ae66e7":"# ROC and AUC for Random Forest models\ntitle1 = 'ROC and AUC of Random Forest'\ntitle2 = 'ROC and AUC of Random Forest with GridSearchCV'\nplot_roc_auc(rf_model, rf_gscv, X_test, y_test, title1, title2);","49e01747":"# ROC and AUC for SVM models\ntitle1 = 'ROC and AUC of SVM'\ntitle2 = 'ROC and AUC of SVM with GridSearchCV'\nplot_roc_auc(svm_model, svm_gscv, X_test, y_test, title1, title2);","9ff6dcaf":"# ROC and AUC for XGBClassifier    \ntitle1 = 'ROC and AUC of XGBClassifier'\ntitle2 = 'ROC and AUC of XGBClassifier with GridSearchCV'\nplot_roc_auc(xgb_model, xgb_gscv, X_test, y_test, title1, title2);","ae65fe3f":"def plot_learning_curve(model, title, X, y, ylim=None, cv=5, n_jobs=4, \n                        train_sizes=np.linspace(0.1, 1.0, 10)):\n    \"\"\"\n    Draw the training and GridSearchCV testing learning curves\n    \"\"\"\n    plt.figure(figsize=(10, 7))\n    plt.title(title, fontsize=20)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel('Number of training samples', fontsize=15)\n    plt.ylabel('Score', fontsize=15)\n    plt.tick_params(labelsize=14)\n    \n    # Get training and test scores along with train_sizes\n    train_sizes, train_scores, test_scores = learning_curve(model, X, y, \n                                                            cv=cv, n_jobs=n_jobs,\n                                                            train_sizes=train_sizes)\n    \n    # Calculate mean and standard deviation of training and test data\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid(color='gray',linestyle='-')\n    \n    # Plot the learning curves\n    plt.fill_between(train_sizes, \n                     train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std,\n                     alpha=0.1, color='r')\n    plt.fill_between(train_sizes,\n                     test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std,\n                     alpha=0.1, color='g')\n   \n    plt.plot(train_sizes, train_scores_mean, 'o-', color='r',\n             label='Training score')\n    plt.plot(train_sizes, test_scores_mean, 'o-', color='g',\n             label='Cross-validation score')\n    plt.legend(loc='best')\n    plt.tight_layout()\n    plt.show()\n    return plt\n","cdbd3b50":"# Learning curve for Logistic Regression Models\ntitle = 'Learning Curves (Logistic Regression)'\nplot_learning_curve(logreg, title, X_train, y_train, ylim=None, \n                    cv=5, n_jobs=4, train_sizes=np.linspace(0.1, 1.0, 10));\n\ntitle = 'Learning Curves (Logistic Regression with GridSearchCV)'\nplot_learning_curve(logreg_gscv, title, X_train, y_train, ylim=None, \n                    cv=5, n_jobs=4, train_sizes=np.linspace(0.1, 1.0, 10));","17c4efc1":"# Learning curve for Random Forest models\ntitle = 'Learning Curves (Random Forest Model)'\nplot_learning_curve(rf_model, title, X_train, y_train, ylim=None, \n                    cv=5, n_jobs=4, train_sizes=np.linspace(0.1, 1.0, 10));\n\ntitle = 'Learning Curves (Random Forest Model with GridSearchCV)'\nplot_learning_curve(rf_gscv, title, X_train, y_train, ylim=None, \n                    cv=5, n_jobs=4, train_sizes=np.linspace(0.1, 1.0, 10));","ca063ae3":"# Learning curve for SVM models\ntitle = 'Learning Curves (SVM Model)'\nplot_learning_curve(svm_model, title, X_train, y_train, ylim=None, \n                    cv=5, n_jobs=4, train_sizes=np.linspace(0.1, 1.0, 10));\n\ntitle = 'Learning Curves (SVM with GridSearchCV)'\nplot_learning_curve(svm_gscv, title, X_train, y_train, ylim=None, \n                    cv=5, n_jobs=4, train_sizes=np.linspace(0.1, 1.0, 10));","f2531c0b":"# Learning Curve for XGBoost Classifier Model\ntitle = 'Learning Curves (XGBoost Classifier Model)'\nplot_learning_curve(xgb_model, title, X_train, y_train, ylim=None, \n                    cv=5, n_jobs=4, train_sizes=np.linspace(0.1, 1.0, 10));\n\ntitle = 'Learning Curves (XGBoost Classifier with GridSearchCV)'\nplot_learning_curve(xgb_gscv, title, X_train, y_train, ylim=None, \n                    cv=5, n_jobs=4, train_sizes=np.linspace(0.1, 1.0, 10));","4683bcf0":"# Get test and model tuning test score for each model\n# Make lits of test score for each model and a model name list\ntest_list = [logreg_acc, rf_acc, svm_acc, xgb_acc]\ntuning_test_list = [logreg_gscv_acc, rf_gscv_acc, svm_gscv_acc, xgb_gscv_acc]\nmodel_names = ['LogisticRegression', 'RandomForest', 'SVM', 'XGBClassifier']\n\n# Make dataframes of each test score\ntest_list = pd.DataFrame(test_list)\ntuning_test_list = pd.DataFrame(tuning_test_list)\nmodel_names = pd.DataFrame(model_names)\n\n# Name each column\ntest_list.columns = ['Default']\ntuning_test_list.columns = ['with GridSearchCV']\nmodel_names.columns = ['model']\n\n# Concatenate each column\nover_all_score = pd.concat([model_names, test_list, tuning_test_list], axis=1)\nprint(over_all_score)\n\n    \n# Plot bar chart of each test score results\nfig = go.Figure(data=[go.Bar(x=over_all_score['model'], y=over_all_score['Default'], \n                             name='Test Results', texttemplate='%{y:20,.4f}', \n                             textposition='outside', marker={'line': {'width': 5}}),\n                      go.Bar(x=over_all_score['model'], y=over_all_score['with GridSearchCV'], \n                             name='Model Tuning Test Results', texttemplate='%{y:20,.4f}', \n                             textposition='outside', marker={'line': {'width': 5}})])\nfig.update_layout(title_text='Test and Model Tuning Test Results for each Model',\n                  title_x=0.5, title_font=dict(size=20))\nfig.update_xaxes(title_text='Models', title_font={'size': 15})\nfig.update_yaxes(title_text='Score', title_font={'size': 15})\nfig.update_traces(marker={'line': {'color': 'Black', 'width': 1}})\nfig.show()\n","57958842":"## 1. Import Liraries","c7fbb29a":"## 6. Model Evaluation\n\n### 6-1. Confusion Matrix\nConfusion matrix is a measurement technique for machine learning classification. \nIt shows:\n* **TP (True Positive)**: Predicted values correctly predicted as actual positive.\n* **FP (False Positive)**: Predicted values incorrectly predicted an actual positive. \n(i.e., Negative values predicted as positive)\n* **FN (False Negative)**: Positive values predicted as negative.\n* **TN (True Negative)**: Predicted values correctly predicted as an actual negative.","777d4662":"## 4. Data Preprocessing","64275f24":"### 4-2. Normalization","62a7dceb":"* SVM with GridSearchCV model showed the best performance.","2987f28c":"## 6-3. Learning Curve\nLearning curve shows the training score and the validation of a model for varying numbers of training samples.","c45000eb":"## Random Forest Model with GridSearchCV  ","020dc88b":"The features following are used for the prediction of heart disease presence:  \n\n1. **<font color=\"MediumBlue\">Age<\/font>**: age in years  \n2. **<font color=\"MediumBlue\">Sex<\/font>**:  \n    * 0 = Female  \n    * 1 = Male  \n3. **<font color=\"MediumBlue\">Chest_pain_type<\/font>**: Chest pain types  \n    * 0 = Typical angina  \n    * 1 = Atypical angina  \n    * 2 = Non-angina pain  \n    * 3 = Asymptomatic  \n4. **<font color=\"MediumBlue\">Resting_bp<\/font>**: Resting blood pressure (mmHg)  \n    * High blood pressure = 140 mmHg or above  \n5. **<font color=\"MediumBlue\">Cholesterol<\/font>**: Serum cholestoral (mg\/dL)  \n    * Certain elements in the blood, including low-density lipoprotein (LDL), high-density lipoprotein (HDL) and triglycerides  \n    * Calculation = HDL + LDL + 0.2 * triglycerides  \n    * Higher than 200 mg\/dL is concerned  \n6. **<font color=\"MediumBlue\">Fasting_bs<\/font>**: Fasting blood sugar (> 120 mg\/dL)  \n    * 0 = False  \n    * 1 = True  \n    * Higher than 126 mg\/dL points to diabetes  \n7. **<font color=\"MediumBlue\">Resting_ecg<\/font>** : Resting electrocardiographic results  \n    * 0 = Normal  \n    * 1 = Abnormal ST-T wave  \n    * 2 = Showing probable or definite left ventricular hypertrophy  \n8. **<font color=\"MediumBlue\">Max_heart_rate<\/font>**: Maximum heart rate achieved (bpm)  \n9. **<font color=\"MediumBlue\">Exercise_induced_angina<\/font>**: Exercise induced angina  \n    * 0 = Negative  \n    * 1 = Positive  \n10. **<font color=\"MediumBlue\">ST_depression<\/font>**: ST depression induced by exercise relative to rest  \n11. **<font color=\"MediumBlue\">ST_slope<\/font>**: The slope of the peak exercise ST segment  \n    * 0 = Upsloping  \n    * 1 = Horizontal  \n    * 2 = Downsloping  \n12. **<font color=\"MediumBlue\">Num_major_vessels<\/font>**: Number of major vessels (0-3) colored by fluoroscopy  \n    * The colored blood vessels are passing through  \n    * There is a clot if the blood vessel is not colored  \n13. **<font color=\"MediumBlue\">Thallium_test<\/font>**: Thallium scintigraphy  \n    * 3 = Normal  \n    * 6 = Fixed defect  \n    * 7 = Reversable defect  \n14. **<font color=\"MediumBlue\">Condition<\/font>**: Heart Disease  \n    * 0 = Benign  \n    * 1 = Malignant  ","27b00411":"### 3-3. Check out Missing Values","8ffdc8ef":"## 5-4. XGBoost Classifier","43f1d791":"## 5. Build Models  \n\n### 5- 1. Logistic Regression  ","423790f4":"# Thanks for reading.\nIf you have any advice, please leave your comment.\nI appriciate your comment, upvote, and advice!","70fd5f93":"# Heart Disease Classification - Model Comparison  \nHeart disease (or Cardiovascular disease) is a general term for body conditions affecting the heart or blood vessels. It is one of top 10 global causes of deaths in the world and an estimated 17.9 million people died from cardiovascular disease each year, representing 31% of all global deaths ([see more](https:\/\/www.who.int\/health-topics\/cardiovascular-diseases\/#tab=tab_1)).  \n\nMachine learning has been transforming the practice of madicine. It helps doctors more accurately diagnose patients, make predictions about the presence of disease, and suggest recommendation of better madical treatments. In this project, we will classify the presence of heart disease using the dataset, [Heart Disease UCI](https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci) from Kaggle. We have already explored the dataset in [Exploratory Data Analysis (EDA)](https:\/\/www.kaggle.com\/microvision\/heart-disease-exploratory-data-analysis) and we will do same steps (from import libraries to data preparation) with [Exploratory Data Analysis (EDA)](https:\/\/www.kaggle.com\/microvision\/heart-disease-exploratory-data-analysis).  \n\nWe will start by preparing the medical data for the prediction. Then, we will apply four models:  \n1. **<font color=\"MediumBlue\">Logistic Regression<\/font>**,  \n2. **<font color=\"MediumBlue\">Random Forest<\/font>**,  \n3. **<font color=\"MediumBlue\">SVM<\/font>**, and  \n4. **<font color=\"MediumBlue\">XGBoost Classifier<\/font>**  \n\nTo improve these models, we will implement parameter tuning for each model. We then compare the performance of our algorithm.  ","1408cc3c":"### 5-2. Random Forest","baf6f5f6":"## Logistic Regression with GridSearchCV","24a81186":"## 5-3. Support Vector Machine (SVM)","af35ccfc":"## 6-2. ROC and AUC\n**Receiver Operating Characteristic (ROC) Curve** shows the accuracy of a model classification. \n* It plots true positive (TP) rate to false positive (FP) rate\n* represents the intuitive trade-off between sensitivity (=TP) and specificity (=FP).\n* The upper left corner of the ROC means the point where 100% of sensitivity and 100% of specificity, a perfect test. So, the closer the ROC curve is to the top left corner, the better the test model is.\n\nOn the other hand, **AUC (Area Under Curve)** represents measurement of separability for the estimator. \n* The classification threshold is 0.5. \n* The higher the AUC is, the better the model is at predicting appropriately. ","aaf3b9a1":"## SVM Model with GridSearchCV","a8781264":"### 4-1. Train-test Split","e30975a9":"We will process the same data preparation in [Heart Disease - Exploratory Data Analysis](https:\/\/www.kaggle.com\/microvision\/heart-disease-exploratory-data-analysis).  \n\n### Table of Contents  \n1. [Import Libraries](#1.-Import-Libraries)  \n2. [Load Dataset](#2.-Load-Dataset)  \n3. [Data Preparation](#3.-Data-Preparation)  \n   3-1. [Rename Columns](#3-1.-Rename-Columns)  \n   3-2. [Data Description](#3-2.-Data-Description)  \n   3-3. [Check out Missing Values](#3-3.-Check-out-Missing-Values)  \n   \n4. [Data Preprocessing](#4.-Data-Preprocessing)  \n   4-1. [Train-test Split](#4-1.-Train---test-Split)  \n   4-2. [Normalization](#4-2.-Normalization)  \n   \n5. [Build Models](#5.-Build-Models)  \n  5-1. [Logistic Regression](#5-1.-Logistic-Regression)  \n   * Logistic Regression with GridSearchCV  \n   \n  5-2. [Random Forest](#5-2.-Random-Forest)\n   * Random Forest with GridSearchCV  \n   \n  5-3. [Support Vector Machine (SVM)](#5-3.-Support-Vector-Machine-(SVM))\n   * Support Vector Machine (SVM) with GridSearchCV  \n   \n  5-4. [XGBoost Classifier](#5-4.-XGBoost-Classifier)\n   * XGBoost Classifier with GridSearchCV  \n   \n6. [Model Evaluation](#6.-Model-Evaluation)  \n   6-1. [Confusion Matrix](#6-1.-Confusion-Matrix)  \n   6-2. [ROC and AUC](#6-2.-ROC-and-AUC)  \n   6-3. [Learning Curve](#6-3.-Learning-Curve)  \n   \n7. [Model Comparison](#7.-Model-Comparison)  ","643627b4":"## 2. Load Dataset","21ec73d6":"## XGBoost Classifier with GridSearchCV","8a3ff8f2":"### 3-2. Data Description","8e5fb4b7":"## 3. Data Preparation\n\n### 3-1. Rename Columns","7494ea01":"# 7. Model Comparison"}}