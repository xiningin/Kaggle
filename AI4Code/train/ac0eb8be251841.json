{"cell_type":{"12f231ba":"code","f57c512f":"code","f3ad8ced":"code","9702e7b9":"code","0c80af3e":"code","ceb19a9c":"code","85b799a4":"code","49cf316a":"code","18bcf6be":"code","a0387fe7":"code","25d76216":"code","0a022a5c":"code","1539c222":"code","b5b19994":"code","305e974f":"code","60bc2098":"code","582e4884":"code","f90dacaa":"code","adbff2c6":"code","9b6bbc1f":"code","28c06aaa":"code","b760ef8f":"code","dc78dc72":"code","5e2a91c0":"code","2c26ec05":"code","d07db648":"code","b9054420":"code","d8490460":"code","fe890435":"code","5f215be8":"code","a0db7c27":"code","a3f47b1f":"code","18d6a2a0":"code","b3341c93":"code","05a0f856":"code","d2e6628b":"code","7020f37f":"code","64028770":"code","a6d8b806":"code","b6d73165":"code","cf58dde2":"code","78d6f32c":"code","2696af86":"code","dc2e435a":"markdown","d87e1c01":"markdown","437c45a3":"markdown","a3aab823":"markdown","b95843d9":"markdown","3987093e":"markdown","e0b9942a":"markdown","fadb2c6c":"markdown","f13444bc":"markdown","a934e174":"markdown","9fc38cb9":"markdown","48c323a4":"markdown","69b725e7":"markdown","c257a357":"markdown","5540b490":"markdown","162be7e2":"markdown","5157b13f":"markdown","2db6cb5a":"markdown","dc04cff9":"markdown","8a9d0ff7":"markdown","92dedbdb":"markdown","639374e4":"markdown","5b3f4ea6":"markdown","a11423f7":"markdown"},"source":{"12f231ba":"!pip install --upgrade seaborn\n!pip install ensemble_boxes","f57c512f":"import numpy as np, pandas as pd\nfrom glob import glob\nimport shutil, os\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GroupKFold\nfrom tqdm.notebook import tqdm\nimport seaborn as sns","f3ad8ced":"dim = 512 #512, 256, 'original'\nfold = 4","9702e7b9":"train_df = pd.read_csv(f'..\/input\/vinbigdata-{dim}-image-dataset\/vinbigdata\/train.csv')\ntrain_df.head()","0c80af3e":"train_df['image_path'] = f'\/kaggle\/input\/vinbigdata-{dim}-image-dataset\/vinbigdata\/train\/'+train_df.image_id+('.png' if dim!='original' else '.jpg')\ntrain_df.head()","ceb19a9c":"# \u5c06no finding\u53bb\u6389\ntrain_df = train_df[train_df.class_id!=14].reset_index(drop = True)","85b799a4":"df = pd.read_csv(\"..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train.csv\")","49cf316a":"df.head()","18bcf6be":"train_df.head()","a0387fe7":"import numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nfrom ensemble_boxes import *\n\n# ===============================\n# Default WBF config (you can change these)\niou_thr = 0.5\nskip_box_thr = 0.0001\nsigma = 0.1\n# ===============================\n\n# Loading the train DF\ndf = pd.read_csv(\"..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train.csv\")\ndf = train_df\ndf.fillna(0, inplace=True)\ndf.loc[df[\"class_id\"] == 14, ['x_max', 'y_max']] = 1.0\n\nresults = []\nimage_ids = df[\"image_id\"].unique()\n\nfor image_id in tqdm(image_ids, total=len(image_ids)):\n\n    # All annotations for the current image.\n    data = df[df[\"image_id\"] == image_id]\n    data = data.reset_index(drop=True)\n    annotations = {}\n    weights = []\n    \n    \n    width=data.iloc[0].width\n    height=data.iloc[0].height\n    image_path=data.iloc[0].image_path\n    class_name=data.iloc[0].class_name\n\n    # WBF expects the coordinates in 0-1 range.\n    max_value = data.iloc[:, 4:8].values.max()\n    data.loc[:, [\"x_min\", \"y_min\", \"x_max\", \"y_max\"]] = data.iloc[:, 4:8] \/ max_value\n\n    # Loop through all of the annotations\n    for idx, row in data.iterrows():\n\n        rad_id = row[\"rad_id\"]\n\n        if rad_id not in annotations:\n            annotations[rad_id] = {\n                \"boxes_list\": [],\n                \"scores_list\": [],\n                \"labels_list\": [],\n            }\n\n            # We consider all of the radiologists as equal.\n            weights.append(1.0)\n\n        annotations[rad_id][\"boxes_list\"].append([row[\"x_min\"], row[\"y_min\"], row[\"x_max\"], row[\"y_max\"]])\n        annotations[rad_id][\"scores_list\"].append(1.0)\n        annotations[rad_id][\"labels_list\"].append(row[\"class_id\"])\n\n    boxes_list = []\n    scores_list = []\n    labels_list = []\n\n    for annotator in annotations.keys():\n        boxes_list.append(annotations[annotator][\"boxes_list\"])\n        scores_list.append(annotations[annotator][\"scores_list\"])\n        labels_list.append(annotations[annotator][\"labels_list\"])\n\n    # Calculate WBF\n    boxes, scores, labels = weighted_boxes_fusion(\n        boxes_list,\n        scores_list,\n        labels_list,\n        weights=weights,\n        iou_thr=iou_thr,\n        skip_box_thr=skip_box_thr\n    )\n\n    for idx, box in enumerate(boxes):\n        results.append({\n            \"image_id\": image_id,\n            \"class_id\": int(labels[idx]),\n            \"rad_id\": \"wbf\",\n            \"x_min\": box[0] * max_value,\n            \"y_min\": box[1] * max_value,\n            \"x_max\": box[2] * max_value,\n            \"y_max\": box[3] * max_value,\n            \"width\": width,\n            \"height\": height,\n            \"image_path\": image_path,\n            \"class_name\": class_name\n        })\n\nresults = pd.DataFrame(results)","25d76216":"results.shape","0a022a5c":"train_df=results","1539c222":"df = pd.read_csv(\"..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train.csv\")","b5b19994":"# \u505a\u5f52\u4e00\u5316\uff0c\u628a\u5de6\u4e0a\u53f3\u4e0b\u56db\u4e2a\u70b9\u7684\u5750\u6807\uff0c\u8ba1\u7b97\u6210\u4e2d\u5fc3\u70b9\u5750\u6807\u548cw\/h\ntrain_df['x_min'] = train_df.apply(lambda row: (row.x_min)\/row.width, axis =1)\ntrain_df['y_min'] = train_df.apply(lambda row: (row.y_min)\/row.height, axis =1)\n\ntrain_df['x_max'] = train_df.apply(lambda row: (row.x_max)\/row.width, axis =1)\ntrain_df['y_max'] = train_df.apply(lambda row: (row.y_max)\/row.height, axis =1)\n\ntrain_df['x_mid'] = train_df.apply(lambda row: (row.x_max+row.x_min)\/2, axis =1)\ntrain_df['y_mid'] = train_df.apply(lambda row: (row.y_max+row.y_min)\/2, axis =1)\n\ntrain_df['w'] = train_df.apply(lambda row: (row.x_max-row.x_min), axis =1)\ntrain_df['h'] = train_df.apply(lambda row: (row.y_max-row.y_min), axis =1)\n\ntrain_df['area'] = train_df['w']*train_df['h']\ntrain_df.head()","305e974f":"# \u7c7b\u540d\u6539\u6210str\nclass_ids, class_names = list(zip(*set(zip(train_df.class_id, train_df.class_name))))\nclasses = list(np.array(class_names)[np.argsort(class_ids)])\nclasses = list(map(lambda x: str(x), classes))\nclasses","60bc2098":"# \u53d6\u7279\u5f81\u6570\u636e\nfeatures = ['x_min', 'y_min', 'x_max', 'y_max', 'x_mid', 'y_mid', 'w', 'h', 'area']\nX = train_df[features]\ny = train_df['class_id']\nX.shape, y.shape","582e4884":"# %%time\n# # \u901a\u8fc7t-SNE\u770b\u6837\u672c\u9ad8\u7ef4\u5206\u5e03\n\n# from sklearn.manifold import TSNE\n\n# tsne = TSNE(n_components = 2, perplexity = 40, random_state=1, n_iter=5000)\n# data_X = X\n# data_y = y.loc[data_X.index]\n# embs = tsne.fit_transform(data_X)\n# # Add to dataframe for convenience\n# plot_x = embs[:, 0]\n# plot_y = embs[:, 1]","f90dacaa":"# import matplotlib.pyplot as plt\n# plt.figure(figsize = (15, 15))\n# plt.axis('off')\n# scatter = plt.scatter(plot_x, plot_y, marker = 'o',s = 50, c=data_y.tolist(), alpha= 0.5,cmap='viridis')\n# plt.legend(handles=scatter.legend_elements()[0], labels=classes)","adbff2c6":"from scipy.stats import gaussian_kde\n\n\nx_val = train_df.x_mid.values\ny_val = train_df.y_mid.values\n\n# Calculate the point density\nxy = np.vstack([x_val,y_val])\nz = gaussian_kde(xy)(xy)\n\nfig, ax = plt.subplots(figsize = (10, 10))\nax.axis('off')\nax.scatter(x_val, y_val, c=z, s=100, cmap='viridis')\n# ax.set_xlabel('x_mid')\n# ax.set_ylabel('y_mid')\nplt.show()","9b6bbc1f":"x_val = train_df.w.values\ny_val = train_df.h.values\n\n# Calculate the point density\nxy = np.vstack([x_val,y_val])\nz = gaussian_kde(xy)(xy)\n\nfig, ax = plt.subplots(figsize = (10, 10))\nax.axis('off')\nax.scatter(x_val, y_val, c=z, s=100, cmap='viridis')\n# ax.set_xlabel('bbox_width')\n# ax.set_ylabel('bbox_height')\nplt.show()","28c06aaa":"x_val = train_df.width.values\ny_val = train_df.height.values\n\n# Calculate the point density\nxy = np.vstack([x_val,y_val])\nz = gaussian_kde(xy)(xy)\n\nfig, ax = plt.subplots(figsize = (10, 10))\nax.axis('off')\nax.scatter(x_val, y_val, c=z, s=100, cmap='viridis')\n# ax.set_xlabel('image_width')\n# ax.set_ylabel('image_height')\nplt.show()","b760ef8f":"# K\u6298\u9a8c\u8bc1\ngkf  = GroupKFold(n_splits = 5)\ntrain_df['fold'] = -1\nfor fold, (train_idx, val_idx) in enumerate(gkf.split(train_df, groups = train_df.image_id.tolist())):\n    train_df.loc[val_idx, 'fold'] = fold\ntrain_df.head()","dc78dc72":"train_df.shape","5e2a91c0":"# \u62c6\u5206\u51fa\u4e00\u4e2a\u8bad\u7ec3\u548c\u4e00\u4e2a\u9a8c\u8bc1\ntrain_files = []\nval_files   = []\nval_files += list(train_df[train_df.fold==fold].image_path.unique())\ntrain_files += list(train_df[train_df.fold!=fold].image_path.unique())\nlen(train_files), len(val_files)","2c26ec05":"# \u628a\u6807\u7b7e\u548c\u56fe\u7247\u5206\u522b\u5904\u7406\u597d\uff0c\u6309\u7167Yolov5\u7684\u8981\u6c42\u653e\u5230\u5bf9\u5e94\u7684\u6587\u4ef6\u5939\u91cc\u53bb\nos.makedirs('\/kaggle\/working\/vinbigdata\/labels\/train', exist_ok = True)\nos.makedirs('\/kaggle\/working\/vinbigdata\/labels\/val', exist_ok = True)\nos.makedirs('\/kaggle\/working\/vinbigdata\/images\/train', exist_ok = True)\nos.makedirs('\/kaggle\/working\/vinbigdata\/images\/val', exist_ok = True)\nlabel_dir = '\/kaggle\/input\/vinbigdata-yolo-labels-dataset\/labels'\nfor file in tqdm(train_files):\n    shutil.copy(file, '\/kaggle\/working\/vinbigdata\/images\/train')\n    filename = file.split('\/')[-1].split('.')[0]\n    shutil.copy(os.path.join(label_dir, filename+'.txt'), '\/kaggle\/working\/vinbigdata\/labels\/train')\n    \nfor file in tqdm(val_files):\n    shutil.copy(file, '\/kaggle\/working\/vinbigdata\/images\/val')\n    filename = file.split('\/')[-1].split('.')[0]\n    shutil.copy(os.path.join(label_dir, filename+'.txt'), '\/kaggle\/working\/vinbigdata\/labels\/val')","d07db648":"train_df","b9054420":"# \u83b7\u53d6\u7c7b\u540d\nclass_ids, class_names = list(zip(*set(zip(train_df.class_id, train_df.class_name))))\nclasses = list(np.array(class_names)[np.argsort(class_ids)])\nclasses = list(map(lambda x: str(x), classes))\nclasses","d8490460":"classes=list(set(classes))","fe890435":"# \u505ayolov5\u7684\u914d\u7f6e\u6587\u4ef6\nfrom os import listdir\nfrom os.path import isfile, join\nimport yaml\n\ncwd = '\/kaggle\/working\/'\n\nwith open(join( cwd , 'train.txt'), 'w') as f:\n    for path in glob('\/kaggle\/working\/vinbigdata\/images\/train\/*'):\n        f.write(path+'\\n')\n            \nwith open(join( cwd , 'val.txt'), 'w') as f:\n    for path in glob('\/kaggle\/working\/vinbigdata\/images\/val\/*'):\n        f.write(path+'\\n')\n\ndata = dict(\n    train =  join( cwd , 'train.txt') ,\n    val   =  join( cwd , 'val.txt' ),\n    nc    = 14,\n    names = classes\n    )\n\nwith open(join( cwd , 'vinbigdata.yaml'), 'w') as outfile:\n    yaml.dump(data, outfile, default_flow_style=False)\n\nf = open(join( cwd , 'vinbigdata.yaml'), 'r')\nprint('\\nyaml:')\nprint(f.read())","5f215be8":"# https:\/\/www.kaggle.com\/ultralytics\/yolov5\n# !git clone https:\/\/github.com\/ultralytics\/yolov5  # clone repo\n# %cd yolov5\nshutil.copytree('\/kaggle\/input\/yolov5-official-v31-dataset\/yolov5', '\/kaggle\/working\/yolov5')\nos.chdir('\/kaggle\/working\/yolov5')\n# %pip install -qr requirements.txt # install dependencies\n\nimport torch\nfrom IPython.display import Image, clear_output  # to display images\n\nclear_output()\nprint('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))","a0db7c27":"# \u6d4b\u8bd5\u5b98\u65b9\u7684\u6743\u91cd\u6587\u4ef6\n!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data\/images\/\nImage(filename='runs\/detect\/exp\/zidane.jpg', width=600)","a3f47b1f":"# !WANDB_MODE=\"dryrun\" python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --nosave --cache \n!WANDB_MODE=\"dryrun\" python train.py --img 640 --batch 16 --epochs 30 --data \/kaggle\/working\/vinbigdata.yaml --weights yolov5x.pt --cache","18d6a2a0":"# xywh\u7684\u5206\u5e03\nplt.figure(figsize = (20,20))\nplt.axis('off')\nplt.imshow(plt.imread('runs\/train\/exp\/labels_correlogram.jpg'));","b3341c93":"# \u7c7b\u7684\u5206\u5e03\uff0c\u5404\u7c7b\u6846\u7684\u5206\u5e03\nplt.figure(figsize = (20,20))\nplt.axis('off')\nplt.imshow(plt.imread('runs\/train\/exp\/labels.jpg'));","05a0f856":"# \u753b\u51fa\u5404\u4e2a\u6279\u6b21\u7684\u6807\u8bb0\u56fe\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (15, 15))\nplt.imshow(plt.imread('runs\/train\/exp\/train_batch0.jpg'))\n\nplt.figure(figsize = (15, 15))\nplt.imshow(plt.imread('runs\/train\/exp\/train_batch1.jpg'))\n\nplt.figure(figsize = (15, 15))\nplt.imshow(plt.imread('runs\/train\/exp\/train_batch2.jpg'))","d2e6628b":"# \u770b\u6807\u8bb0\u548c\u9884\u6d4b\u7684\u5dee\u522b\nfig, ax = plt.subplots(3, 2, figsize = (2*5,3*5), constrained_layout = True)\nfor row in range(3):\n    ax[row][0].imshow(plt.imread(f'runs\/train\/exp\/test_batch{row}_labels.jpg'))\n    ax[row][0].set_xticks([])\n    ax[row][0].set_yticks([])\n    ax[row][0].set_title(f'runs\/train\/exp\/test_batch{row}_labels.jpg', fontsize = 12)\n    \n    ax[row][1].imshow(plt.imread(f'runs\/train\/exp\/test_batch{row}_pred.jpg'))\n    ax[row][1].set_xticks([])\n    ax[row][1].set_yticks([])\n    ax[row][1].set_title(f'runs\/train\/exp\/test_batch{row}_pred.jpg', fontsize = 12)","7020f37f":"# \u770b\u635f\u5931\u4e0b\u964d\u901f\u5ea6\nplt.figure(figsize=(30,15))\nplt.axis('off')\nplt.imshow(plt.imread('runs\/train\/exp\/results.png'));","64028770":"# \u6df7\u6dc6\u77e9\u9635\nplt.figure(figsize=(30,15))\nplt.axis('off')\nplt.imshow(plt.imread('runs\/train\/exp\/confusion_matrix.png'));","a6d8b806":"# \u7528\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u63a8\u7406\n!python detect.py --weights 'runs\/train\/exp\/weights\/best.pt'\\\n--img 640\\\n--conf 0.15\\\n--iou 0.5\\\n--source \/kaggle\/working\/vinbigdata\/images\/val\\\n--exist-ok","b6d73165":"# \u63a8\u65ad\u6837\u672c\uff0c\u5e76\u505a\u6807\u8bb0\u56fe\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport numpy as np\nimport random\nimport cv2\nfrom glob import glob\nfrom tqdm import tqdm\n\nfiles = glob('runs\/detect\/exp\/*')\nfor _ in range(3):\n    row = 4\n    col = 4\n    grid_files = random.sample(files, row*col)\n    images     = []\n    for image_path in tqdm(grid_files):\n        img          = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n        images.append(img)\n\n    fig = plt.figure(figsize=(col*5, row*5))\n    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n                     nrows_ncols=(col, row),  # creates 2x2 grid of axes\n                     axes_pad=0.05,  # pad between axes in inch.\n                     )\n\n    for ax, im in zip(grid, images):\n        # Iterating over the grid returns the Axes.\n        ax.imshow(im)\n        ax.set_xticks([])\n        ax.set_yticks([])\n    plt.show()","cf58dde2":"# \u5220\u9664\u4e00\u4e9b\u6587\u4ef6\u5939\uff0c\u9632\u6b62\u518d\u6b21\u8fd0\u884c\u65f6\u62a5\u9519\nshutil.rmtree('\/kaggle\/working\/vinbigdata')\nshutil.rmtree('runs\/detect')\nfor file in (glob('runs\/train\/exp\/**\/*.png', recursive = True)+glob('runs\/train\/exp\/**\/*.jpg', recursive = True)):\n    os.remove(file)","78d6f32c":"import numpy as np, pandas as pd\nfrom glob import glob\nimport shutil, os\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GroupKFold\nfrom tqdm.notebook import tqdm\nimport seaborn as sns","2696af86":"dim = 512 #1024, 256, 'original'\ntest_dir = f'\/kaggle\/input\/vinbigdata-{dim}-image-dataset\/vinbigdata\/test'\nweights_dir = '\/kaggle\/input\/vinbigdata-cxr-ad-yolov5-14-class-train\/yolov5\/runs\/train\/exp\/weights\/best.pt'","dc2e435a":"# Copying Files","d87e1c01":"# Pre-Processing","437c45a3":"# Class Distribution","a3aab823":"# Inference Plot","b95843d9":"# Confusion Matrix","3987093e":"# YOLOv5 Stuff","e0b9942a":"# Version\n* `v13`: Fold4\n* `v12`: Fold3\n* `v10`: Fold2\n* `v09`: Fold1\n* `v03`: Fold0","fadb2c6c":"# Inference","f13444bc":"# [YOLOv5](https:\/\/github.com\/ultralytics\/yolov5)\n![](https:\/\/user-images.githubusercontent.com\/26833433\/98699617-a1595a00-2377-11eb-8145-fc674eb9b1a7.jpg)\n![](https:\/\/user-images.githubusercontent.com\/26833433\/90187293-6773ba00-dd6e-11ea-8f90-cd94afc0427f.png)","a934e174":"## bbox_w Vs bbox_h","9fc38cb9":"# Selecting Models\nIn this notebok I'm using `v5s`. To select your prefered model just replace `--cfg models\/yolov5s.yaml --weights yolov5s.pt` with the following command:\n* `v5s` : `--cfg models\/yolov5s.yaml --weights yolov5s.pt`\n* `v5m` : `--cfg models\/yolov5m.yaml --weights yolov5m.pt`\n* `v5l` : `--cfg models\/yolov5l.yaml --weights yolov5l.pt`\n* `v5x` : `--cfg models\/yolov5x.yaml --weights yolov5x.pt`","48c323a4":"# (Loss, Map) Vs Epoch","69b725e7":"# Only 14 Class","c257a357":"# Train","5540b490":"# t-SNE Visualization","162be7e2":"# \u51c6\u5907\u6570\u636e","5157b13f":"## x_mid Vs y_mid","2db6cb5a":"# Image Aspect Ratio\nbox\u7684\u5bbd\u9ad8","dc04cff9":"## Pretrained Checkpoints:\n\n| Model | AP<sup>val<\/sup> | AP<sup>test<\/sup> | AP<sub>50<\/sub> | Speed<sub>GPU<\/sub> | FPS<sub>GPU<\/sub> || params | FLOPS |\n|---------- |------ |------ |------ | -------- | ------| ------ |------  |  :------: |\n| [YOLOv5s](https:\/\/github.com\/ultralytics\/yolov5\/releases\/tag\/v3.0)    | 37.0     | 37.0     | 56.2     | **2.4ms** | **416** || 7.5M   | 13.2B\n| [YOLOv5m](https:\/\/github.com\/ultralytics\/yolov5\/releases\/tag\/v3.0)    | 44.3     | 44.3     | 63.2     | 3.4ms     | 294     || 21.8M  | 39.4B\n| [YOLOv5l](https:\/\/github.com\/ultralytics\/yolov5\/releases\/tag\/v3.0)    | 47.7     | 47.7     | 66.5     | 4.4ms     | 227     || 47.8M  | 88.1B\n| [YOLOv5x](https:\/\/github.com\/ultralytics\/yolov5\/releases\/tag\/v3.0)    | **49.2** | **49.2** | **67.7** | 6.9ms     | 145     || 89.0M  | 166.4B\n| | | | | | || |\n| [YOLOv5x](https:\/\/github.com\/ultralytics\/yolov5\/releases\/tag\/v3.0) + TTA|**50.8**| **50.8** | **68.9** | 25.5ms    | 39      || 89.0M  | 354.3B\n| | | | | | || |\n| [YOLOv3-SPP](https:\/\/github.com\/ultralytics\/yolov5\/releases\/tag\/v3.0) | 45.6     | 45.5     | 65.2     | 4.5ms     | 222     || 63.0M  | 118.0B","8a9d0ff7":"# BBox Location","92dedbdb":"# Batch Image","639374e4":"# Split","5b3f4ea6":"# GT Vs Pred","a11423f7":"# Get Class Name"}}