{"cell_type":{"6618d910":"code","47934ab4":"code","92cd69c9":"code","37bd93eb":"code","d03fa753":"code","b01f928d":"code","372b7030":"code","9fc2e4ea":"code","3d4c8678":"code","b7e81a38":"code","f1206020":"code","d6e271b0":"code","481c3872":"code","8d979b30":"code","97cbb179":"code","fc212ff7":"code","6d835b6c":"markdown","bf67e78e":"markdown","d0b11157":"markdown","dc7abac4":"markdown","4f7f4922":"markdown","e9704a0e":"markdown","b38c0054":"markdown","08d779ec":"markdown","d460165d":"markdown","551979c7":"markdown","f196b4f1":"markdown"},"source":{"6618d910":"\nimport numpy as np \nimport pandas as pd \nimport os\nimport math\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\n \nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M","47934ab4":"test_df = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\ntrain_df = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_target_df = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\nsub = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\n\ntarget_cols = train_target_df.columns[1:]","92cd69c9":"train_df.head()","37bd93eb":"sub.head()","d03fa753":"SEED = 9876\nEPOCHS = 30\nBATCH_SIZE = 128\nFOLDS = 5\nREPEATS = 2\nLR = 0.0005\nN_TARGETS = len(target_cols)","b01f928d":"def seed_everything(seed):\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)","372b7030":"def multi_log_loss(y_true, y_pred):\n    losses = []\n    for col in y_true.columns:\n        losses.append(log_loss(y_true.loc[:, col], y_pred.loc[:, col]))\n    return np.mean(losses)","9fc2e4ea":"def preprocess_df(data):\n    data['cp_type'] = (data['cp_type'] == 'trt_cp').astype(int)\n    data['cp_dose'] = (data['cp_dose'] == 'D2').astype(int)\n    return data","3d4c8678":"x_train = preprocess_df(train_df.drop(columns=\"sig_id\"))\nx_test =preprocess_df(test_df.drop(columns=\"sig_id\"))\ny_train = train_target_df.drop(columns=\"sig_id\")\nN_FEATURES = x_train.shape[1]","b7e81a38":"def create_model():\n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(N_FEATURES),\n    tf.keras.layers.GaussianNoise(NOISE_STD_VECTOR),\n    tf.keras.layers.BatchNormalization(),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(3072, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.4),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(3072, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.4),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(N_TARGETS, activation=\"sigmoid\"))\n    ])\n    model.compile(optimizer=tfa.optimizers.Lookahead(tf.optimizers.Adam(lr=LR), sync_period=10), loss='binary_crossentropy', metrics=[\"accuracy\"])\n    return model","f1206020":"def build_train(resume_models = None, repeat_number = 0, folds = 5, skip_folds = 0):\n    \n    models = []\n    oof_preds = y_train.copy()\n    \n\n    kfold = KFold(folds, shuffle = True)\n    for fold, (train_ind, val_ind) in enumerate(kfold.split(x_train)):\n        print('-'*50)\n        print(f'Training fold {fold + 1}')\n        \n        cb_lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor = 0.4, patience = 3, verbose = 0, min_delta = 0.0001, mode = 'auto')\n        checkpoint_path = f'repeat:{repeat_number}_Fold:{fold}.hdf5'\n        cb_checkpt = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True, save_weights_only = True, mode = 'min')\n\n        model = create_model()\n        model.fit(x_train.values[train_ind],\n              y_train.values[train_ind],\n              validation_data=(x_train.values[val_ind], y_train.values[val_ind]),\n              callbacks = [cb_lr_schedule, cb_checkpt],\n              epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=0\n             )\n        model.load_weights(checkpoint_path)\n        oof_preds.loc[val_ind, :] = model.predict(x_train.values[val_ind])\n        models.append(model)\n\n    return models, oof_preds","d6e271b0":"noise_factors = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]","481c3872":"# get the standard deviation of the numeric features\nfeat_stds = x_train[x_train.columns[3:]].std(axis=0).values\nlosses_ind = []\nlosses_mean = []\nlosses_mean_adj = []\n\ntest_pred_all = []\n\n\n\n# seed everything\nseed_everything(SEED)\n\nfor j, fac in enumerate(noise_factors):\n    print('\\n')\n    print('#'*50)\n    print(f\"Training with Setting {j+1}, Noise Factor: {fac}\")\n    # Do not add noise in the first 3 columns\n    NOISE_STD_VECTOR = [0, 0, 0]\n    # noise std is a function of feture std\n    NOISE_STD_VECTOR.extend(feat_stds * fac)\n\n    oof_preds = []\n    models = []\n    \n    for i in range(REPEATS):\n        print(f\"Starting Repeat {i+1}\")\n        m, oof = build_train(repeat_number = i, folds=FOLDS)\n        models = models + m\n        oof_preds.append(oof)\n\n        \n    print(f\"Finished training.\")\n    \n    mean_oof_preds = y_train.copy()\n    mean_oof_preds.loc[:, target_cols] = 0\n    rep_losses = []\n    print(f\"OOF Results for Setting {j+1}, Noise Factor: {fac}\")\n    for i, p in enumerate(oof_preds):\n        loss = multi_log_loss(y_train, p)\n        print(f\"Repeat {i + 1} OOF Log Loss: {loss}\")\n        rep_losses.append(loss)\n        mean_oof_preds.loc[:, target_cols] += p[target_cols]\n    losses_ind.append(rep_losses)\n    \n    mean_oof_preds.loc[:, target_cols] \/= len(oof_preds)\n    loss_mean = multi_log_loss(y_train, mean_oof_preds)\n    print(f\"Mean OOF Log Loss: {multi_log_loss(y_train, mean_oof_preds)}\")\n    losses_mean.append(loss_mean)\n    mean_oof_preds.loc[x_train['cp_type'] == 0, target_cols] = 0\n    loss_mean_adj = multi_log_loss(y_train, mean_oof_preds)\n    print(f\"Mean OOF Log Loss (ctl adjusted): {loss_mean_adj}\\n\")\n    losses_mean_adj.append(loss_mean_adj)\n    \n    print(f\"Making test predicitons:\")\n    test_preds = sub.copy()\n    test_preds[target_cols] = 0\n    for model in models:\n        test_preds.loc[:,target_cols] += model.predict(x_test)\n    test_preds.loc[:,target_cols] \/= len(models)\n    test_preds.loc[x_test['cp_type'] == 0, target_cols] = 0\n    test_pred_all.append(test_preds)\n    \n    print(\"Freeing Memory.\")\n    for m in models:\n        del m\n    K.clear_session()\n    \n    print('#'*50)\n","8d979b30":"pd.DataFrame({'Noise Factor': noise_factors, 'Loss (each repeat)': losses_ind, 'Losses (mean pred.)': losses_mean, 'Losses (mean pred. adj.)': losses_mean_adj})","97cbb179":"plt.figure(figsize=(12,8))\nlow = min(losses_mean_adj)\nhigh = max(losses_mean_adj)\nplt.ylim([round(low-2*(high-low),5), round(high+0.5*(high-low), 5)])\nplt.bar(['{:.2f}'.format(x) for x in noise_factors], losses_mean_adj)\nplt.ylabel(f'Log Loss (adjusted {FOLDS} by {REPEATS} mean)')\nplt.xlabel('Noise Factor')\nplt.title('Gaussian Noise Factor vs Mean OOF Prediction Log Loss')","fc212ff7":"test_preds = sub.copy()\ntest_preds[target_cols] = 0\nfor p in test_pred_all:\n    test_preds.loc[:,target_cols] += p[target_cols]\ntest_preds.loc[:,target_cols] \/= len(test_pred_all)\ntest_preds.loc[x_test['cp_type'] == 0, target_cols] = 0\ntest_preds.to_csv(f'submission.csv', index=False)","6d835b6c":"OOF prediction loss results indicate best accuracy at approximately 40% of feature standard deviation. This approach definitely has some potential value. Future tests will need to assert that the OOF prediction improvements translate to LB.","bf67e78e":"## Define our Experiment Values","d0b11157":"### Read Data","dc7abac4":"### Main CV and Model Training Function","4f7f4922":"### Basic Setup and Helpers","e9704a0e":"### Import Libraries","b38c0054":"### Encode Categoricals to Binary","08d779ec":"## Results Table and Graph","d460165d":"### Make Submission Using All Models from All Noise Settings","551979c7":"<h1><center>Gaussian Noise Augmentation Experiment<\/center><\/h1>\n\nOriginally based on this excellent work by Stanley Zheng:\nhttps:\/\/www.kaggle.com\/stanleyjzheng\/baseline-nn-with-k-folds\n\n\nThis experiment builds on a previously shared public notebook (public LB 0.01901) adds Gaussian noise to each non-categorical feature input with standard deviation proportionate to the feature standard deviation using the Keras GaussianNoise layer. Noise with 0% (control), 10%, 20%, 30%, 40%, 50%, 60% and 70% of the original feature st dev will be implemented and evaluated using 2 repeats of 5 fold CV. To assess the viability of this noise augmentation, OOF log loss will be compared for each intensity. \n\nInitial experiments indicated small improvements on CV OOF prediction loss but mixed results on public LB.","f196b4f1":"### Define Model Architecture"}}