{"cell_type":{"da7f1a30":"code","2f57fa82":"code","5ad7d9f3":"code","cf53e632":"code","3a27273b":"code","07c892c2":"code","e863fe2b":"code","de89f173":"code","edc3b3f8":"code","73d41844":"code","c3b29433":"code","d5807521":"code","627fc731":"code","56991a5a":"code","a60eb6cf":"code","d5e85d08":"code","fdb34cc7":"code","1374afd3":"code","b80eaed3":"code","212d9591":"code","4863f3a6":"code","e4aba31c":"code","17918821":"code","e2b1d081":"code","bdf6c43a":"code","89aa6386":"code","df3af486":"code","a49260ac":"code","8d3e65de":"markdown","04d56cdb":"markdown","b154c85b":"markdown","7783842c":"markdown","59f4613a":"markdown","f922708d":"markdown","93bad29f":"markdown","30091f26":"markdown","addd1b23":"markdown","70fd1fc7":"markdown","a20a6edf":"markdown","bf0d8c24":"markdown","69aedd29":"markdown"},"source":{"da7f1a30":"import os\nimport gc\ngc.enable()\nimport time\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn import impute\nfrom sklearn import metrics\nfrom sklearn import ensemble\nfrom sklearn import linear_model\nfrom sklearn import decomposition\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\n\nwarnings.filterwarnings('ignore')\n\nSEED = 42\nnp.random.seed(SEED)\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\n\nsns.set_style(\"darkgrid\")\nmpl.rcParams['figure.dpi'] = 600\n%matplotlib inline","2f57fa82":"train_df = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv')\n\nprint('Quick view of training data: ')\ntrain_df.head()","5ad7d9f3":"TARGET = 'target'\nFEATURES = [col for col in train_df.columns if col not in ['id', TARGET]]\nprint(f'Training data:\\n\\t Number of rows: {train_df.shape[0]}, Number of columns: {train_df.shape[1]}')\nprint(f'Testing data:\\n\\t Number of rows: {test_df.shape[0]}, Number of columns: {test_df.shape[1]}')","cf53e632":"print('Basic statistics of training data:')\ntrain_df[FEATURES+[TARGET]].describe()","3a27273b":"print('Basic statistics of testing data:')\ntest_df[FEATURES].describe()","07c892c2":"print(f'Number of missing values in training data: {train_df.isna().sum().sum()}')\nprint(f'Number of missing values in testing data: {test_df.isna().sum().sum()}')","e863fe2b":"df = pd.concat([train_df[FEATURES], test_df[FEATURES]], axis=0)\n\ncat_features = [col for col in FEATURES if df[col].nunique() < 25]\ncont_features = [col for col in FEATURES if df[col].nunique() >= 25]\n\ndel df\nprint(f'Total number of features: {len(FEATURES)}')\nprint(f'Number of categorical features: {len(cat_features)}')\nprint(f'Number of continuos features: {len(cont_features)}')\n\nplt.pie([len(cat_features), len(cont_features)], \n        labels=['Categorical', 'Continuos'],\n        colors=['#76D7C4', '#F5B7B1'],\n        textprops={'fontsize': 13},\n        autopct='%1.1f%%')\nplt.show()","de89f173":"print(\"Feature distribution of continous features: \")\nncols = 5\nnrows = int(len(cont_features) \/ ncols + (len(FEATURES) % ncols > 0))\n\nfig, axes = plt.subplots(nrows, ncols, figsize=(18, 150), facecolor='#EAEAF2')\n\nfor r in range(nrows):\n    for c in range(ncols):\n        col = cont_features[r*ncols+c]\n        sns.kdeplot(x=train_df[col], ax=axes[r, c], color='#58D68D', label='Train data')\n        sns.kdeplot(x=test_df[col], ax=axes[r, c], color='#DE3163', label='Test data')\n        axes[r, c].set_ylabel('')\n        axes[r, c].set_xlabel(col, fontsize=8, fontweight='bold')\n        axes[r, c].tick_params(labelsize=5, width=0.5)\n        axes[r, c].xaxis.offsetText.set_fontsize(4)\n        axes[r, c].yaxis.offsetText.set_fontsize(4)\nplt.show()","edc3b3f8":"print(\"Feature distribution of categorical features: \")\nncols = 5\nnrows = int(len(cat_features) \/ ncols + (len(FEATURES) % ncols > 0))\n\nfig, axes = plt.subplots(nrows, ncols, figsize=(18, 45), facecolor='#EAEAF2')\n\nfor r in range(nrows):\n    for c in range(ncols):\n        col = cat_features[r*ncols+c]\n        sns.histplot(x=train_df[col], ax=axes[r, c], color='#58D68D', label='Train data')\n        sns.histplot(x=test_df[col], ax=axes[r, c], color='#DE3163', label='Test data')\n        axes[r, c].set_ylabel('')\n        axes[r, c].set_xlabel(col, fontsize=8, fontweight='bold')\n        axes[r, c].tick_params(labelsize=5, width=0.5)\n        axes[r, c].xaxis.offsetText.set_fontsize(4)\n        axes[r, c].yaxis.offsetText.set_fontsize(4)\nplt.show()","73d41844":"print(\"Target Distribution: \")\n\ntarget_df = pd.DataFrame(train_df[TARGET].value_counts()).reset_index()\ntarget_df.columns = [TARGET, 'count']\n\nfig, ax = plt.subplots(1, 1, figsize=(25, 8), facecolor='#EAEAF2')\nsns.barplot(y=TARGET, x='count', data=target_df, palette=['#58D68D', '#DE3163'], ax=ax, orient='h')\nax.set_xlabel('Count', fontsize=16)\nax.set_ylabel('Target', fontsize=16)\nplt.show()","c3b29433":"train_df[\"mean\"] = train_df[FEATURES].mean(axis=1)\ntrain_df[\"std\"] = train_df[FEATURES].std(axis=1)\ntrain_df[\"min\"] = train_df[FEATURES].min(axis=1)\ntrain_df[\"max\"] = train_df[FEATURES].max(axis=1)\n\ntest_df[\"mean\"] = test_df[FEATURES].mean(axis=1)\ntest_df[\"std\"] = test_df[FEATURES].std(axis=1)\ntest_df[\"min\"] = test_df[FEATURES].min(axis=1)\ntest_df[\"max\"] = test_df[FEATURES].max(axis=1)\n\nFEATURES.extend(['mean', 'max', 'min', 'max'])","d5807521":"def format_time(seconds):\n    \"\"\"\n    Formates time in human readable form\n\n    Args:\n        seconds: seconds passed in a process\n    Return:\n        formatted string in form of MM:SS or HH:MM:SS\n    \"\"\"\n    h = int(seconds \/\/ 3600)\n    m = int((seconds % 3600) \/\/ 60)\n    s = int(seconds % 60)\n\n    result = ''\n\n    if h > 0:\n        if h < 10:\n            h = '0' + str(h)\n        else:\n            h = str(h)\n        h += ' Hr'\n        result += h\n        result += ' '\n    \n    if m > 0:\n        if m < 10:\n            m = '0' + str(m)\n        else:\n            m = str(m)\n        m += ' min'\n        result += m\n        result += ' '\n\n    if s < 10:\n        s = '0' + str(s)\n    else:\n        s = str(s)\n    s += ' sec'\n    result += s\n    \n    return result\n\nINT8_MIN    = np.iinfo(np.int8).min\nINT8_MAX    = np.iinfo(np.int8).max\nINT16_MIN   = np.iinfo(np.int16).min\nINT16_MAX   = np.iinfo(np.int16).max\nINT32_MIN   = np.iinfo(np.int32).min\nINT32_MAX   = np.iinfo(np.int32).max\n\nFLOAT16_MIN = np.finfo(np.float16).min\nFLOAT16_MAX = np.finfo(np.float16).max\nFLOAT32_MIN = np.finfo(np.float32).min\nFLOAT32_MAX = np.finfo(np.float32).max\n\n\ndef memory_usage(data, detail=1):\n    if detail:\n        display(data.memory_usage())\n    memory = data.memory_usage().sum() \/ (1024*1024)\n    print(\"Memory usage : {0:.2f}MB\".format(memory))\n    return memory\n\n\ndef compress_dataset(data):\n    memory_before_compress = memory_usage(data, 0)\n    print()\n    length_interval      = 50\n    length_float_decimal = 4\n\n    print('='*length_interval)\n    for col in data.columns:\n        col_dtype = data[col][:100].dtype\n\n        if col_dtype != 'object':\n            print(\"Name: {0:24s} Type: {1}\".format(col, col_dtype))\n            col_series = data[col]\n            col_min = col_series.min()\n            col_max = col_series.max()\n\n            if col_dtype == 'float64':\n                print(\" variable min: {0:15s} max: {1:15s}\".format(str(np.round(col_min, length_float_decimal)), str(np.round(col_max, length_float_decimal))))\n                if (col_min > FLOAT16_MIN) and (col_max < FLOAT16_MAX):\n                    data[col] = data[col].astype(np.float16)\n                    print(\"  float16 min: {0:15s} max: {1:15s}\".format(str(FLOAT16_MIN), str(FLOAT16_MAX)))\n                    print(\"compress float64 --> float16\")\n                elif (col_min > FLOAT32_MIN) and (col_max < FLOAT32_MAX):\n                    data[col] = data[col].astype(np.float32)\n                    print(\"  float32 min: {0:15s} max: {1:15s}\".format(str(FLOAT32_MIN), str(FLOAT32_MAX)))\n                    print(\"compress float64 --> float32\")\n                else:\n                    pass\n                memory_after_compress = memory_usage(data, 0)\n                print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) \/ memory_before_compress))\n                print('='*length_interval)\n\n            if col_dtype == 'int64':\n                print(\" variable min: {0:15s} max: {1:15s}\".format(str(col_min), str(col_max)))\n                type_flag = 64\n                if (col_min > INT8_MIN\/2) and (col_max < INT8_MAX\/2):\n                    type_flag = 8\n                    data[col] = data[col].astype(np.int8)\n                    print(\"     int8 min: {0:15s} max: {1:15s}\".format(str(INT8_MIN), str(INT8_MAX)))\n                elif (col_min > INT16_MIN) and (col_max < INT16_MAX):\n                    type_flag = 16\n                    data[col] = data[col].astype(np.int16)\n                    print(\"    int16 min: {0:15s} max: {1:15s}\".format(str(INT16_MIN), str(INT16_MAX)))\n                elif (col_min > INT32_MIN) and (col_max < INT32_MAX):\n                    type_flag = 32\n                    data[col] = data[col].astype(np.int32)\n                    print(\"    int32 min: {0:15s} max: {1:15s}\".format(str(INT32_MIN), str(INT32_MAX)))\n                    type_flag = 1\n                else:\n                    pass\n                memory_after_compress = memory_usage(data, 0)\n                print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) \/ memory_before_compress))\n                if type_flag == 32:\n                    print(\"compress (int64) ==> (int32)\")\n                elif type_flag == 16:\n                    print(\"compress (int64) ==> (int16)\")\n                else:\n                    print(\"compress (int64) ==> (int8)\")\n                print('='*length_interval)\n\n    print()\n    memory_after_compress = memory_usage(data, 0)\n    print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) \/ memory_before_compress))\n    \n    return data\n","627fc731":"train_df = compress_dataset(train_df)\ntest_df = compress_dataset(test_df)","56991a5a":"scaler = preprocessing.StandardScaler()\nfor col in FEATURES:\n    train_df[col] = scaler.fit_transform(train_df[col].to_numpy().reshape(-1,1))\n    test_df[col] = scaler.transform(test_df[col].to_numpy().reshape(-1,1))\n    \nX = train_df[FEATURES].to_numpy().astype(np.float32)\nY = train_df[TARGET].to_numpy().astype(np.float32)\nX_test = test_df[FEATURES].to_numpy().astype(np.float32)\n\ndel train_df, test_df\ngc.collect()","a60eb6cf":"xgb_params = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'max_depth': 6,\n    'n_estimators': 9500,\n    'learning_rate': 0.007279718158350149,\n    'subsample': 0.7,\n    'colsample_bytree': 0.2,\n    'colsample_bylevel': 0.6000000000000001,\n    'min_child_weight': 56.41980735551558,\n    'reg_lambda': 75.56651890088857,\n    'reg_alpha': 0.11766857055687065,\n    'gamma': 0.6407823221122686,\n    'tree_method': 'gpu_hist',\n    'gpu_id': 0,\n    'predictor': 'gpu_predictor',\n}\n\nlgb_params = {\n    'objective' : 'binary',\n    'metric' : 'auc',\n    'num_leaves' : 7,\n    'learning_rate' : 0.08,\n    'device' : 'gpu',\n    'feature_pre_filter': False, \n    'reg_alpha': 9.314037635261775, \n    'reg_lambda': 0.10613573572440353,\n    'num_leaves': 7,\n    'colsample_bytree': 0.4, \n    'subsample': 0.8391963650875751, \n    'subsample_freq': 5, \n    'min_child_samples': 100,\n    'num_iterations': 10000,\n    'n_estimators': 20000\n}\n\ncatb_params = {    \n    \"objective\": \"CrossEntropy\",\n    \"eval_metric\" : \"AUC\",\n    \"task_type\": \"GPU\",\n    \"grow_policy\": \"SymmetricTree\",\n    \"learning_rate\": 0.08,\n    \"n_estimators\":  10_000,\n    \"random_strength\" : 1.0,\n    \"max_bin\": 128,\n    \"l2_leaf_reg\": 0.002550319996478972,\n    \"max_depth\": 4,\n    \"min_data_in_leaf\": 193,\n    'verbose': 0\n}","d5e85d08":"from xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nmodels = [\n    [XGBClassifier, xgb_params, 'xgb'],\n    [LGBMClassifier, lgb_params, 'lgbm'],\n    [CatBoostClassifier, catb_params, 'catb'], \n]","fdb34cc7":"from collections import defaultdict\n\noof_df = defaultdict(lambda : [])\ntest_df = defaultdict(lambda : np.zeros((X_test.shape[0])))\n\nSEEDS = [42, 2021]\nN_FOLDS = 5\nstart = time.time()\n\nskfolds = model_selection.StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\nfor fold, (t, v) in enumerate(skfolds.split(X, Y)):\n    x_train, x_val = X[t], X[v]\n    y_train, y_val = Y[t], Y[v]\n    \n    oof_df[TARGET].extend(y_val)\n    print('-'*38, f\"\\n{'-'*15} FOLD-{fold} {'-'*15}\")\n    print('-'*38)\n\n    for i in range(len(SEEDS)):\n        for class_name, class_params, name in models:\n            tic = time.time()\n            if name in ['qda', 'gnb']:\n                if i > 0:\n                    continue\n            else:\n                class_params['random_state'] = SEEDS[i]\n\n            clf = class_name(**class_params)\n            clf = clf.fit(x_train, y_train)\n            preds = clf.predict_proba(x_val)[:, 1].tolist()\n            oof_df[f'{name}_{SEEDS[i]}'].extend(preds)\n            test_df[f'{name}_{SEEDS[i]}'] += (clf.predict_proba(X_test)[:, 1] \/ N_FOLDS)\n\n            score = metrics.roc_auc_score(y_val, preds)\n            print(f\"MODEL: {name}\\tSEED: {SEEDS[i]}\\tSCORE: {score}\\tTIME: {format_time(time.time()-tic)}\")\n        \n            del clf\n            gc.collect()\n    del x_train, x_val, y_train, y_val\n    gc.collect()\n\nprint('='*38)\nfor k, v in oof_df.items():\n    if k != TARGET:\n        score = metrics.roc_auc_score(oof_df[TARGET], v)\n        print(f'Overall ROC AUC of {k}: {score}')\n        \noof_df = pd.DataFrame(oof_df)\ntest_df = pd.DataFrame(test_df)\n\nprint()\nprint(f'TOTAL TIME: {format_time(time.time() - start)}')","1374afd3":"oof_df.head()","b80eaed3":"oof_df.to_csv('oof_df.csv', index=False)\ntest_df.to_csv('test_df.csv', index=False)","212d9591":"FEATURES = [col for col in oof_df.columns if col not in [TARGET]]\nX = oof_df[FEATURES].to_numpy()\nY = oof_df[TARGET].to_numpy()\nX_test = test_df[FEATURES].to_numpy()","4863f3a6":"from xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegression,LinearRegression\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nmodels = [\n    [CatBoostClassifier, catb_params, 'catb'],\n    [QuadraticDiscriminantAnalysis, {}, 'qda'],\n    [GaussianNB, {}, 'gnb'],\n    [LogisticRegression, {}, 'log'],\n    [LinearRegression, {}, 'reg']\n]","e4aba31c":"from collections import defaultdict\n\noof_df = defaultdict(lambda : [])\ntest_df = defaultdict(lambda : np.zeros((X_test.shape[0])))\n\nN_FOLDS = 5\nstart = time.time()\n\nskfolds = model_selection.StratifiedKFold(n_splits=N_FOLDS, shuffle=False, random_state=SEED)\n\nfor fold, (t, v) in enumerate(skfolds.split(X, Y)):\n    x_train, x_val = X[t], X[v]\n    y_train, y_val = Y[t], Y[v]\n    \n    oof_df[TARGET].extend(y_val)\n    print('-'*38, f\"\\n{'-'*15} FOLD-{fold} {'-'*15}\")\n    print('-'*38)\n\n    for class_name, class_params, name in models:\n        tic = time.time()\n\n        clf = class_name(**class_params)\n        clf = clf.fit(x_train, y_train)\n        if name != 'reg':\n            preds = clf.predict_proba(x_val)[:, 1].tolist()\n            test_df[f'{name}'] += (clf.predict_proba(X_test)[:, 1] \/ N_FOLDS)\n        else:\n            preds = clf.predict(x_val).tolist()\n            test_df[f'{name}'] += (clf.predict(X_test) \/ N_FOLDS)   \n        \n        oof_df[f'{name}'].extend(preds)\n        score = metrics.roc_auc_score(y_val, preds)\n        print(f\"MODEL: {name}\\tSCORE: {score}\\tTIME: {format_time(time.time()-tic)}\")\n\n        del clf\n        gc.collect()\n        \n    del x_train, x_val, y_train, y_val\n    gc.collect()\n\nprint('='*38)\nfor k, v in oof_df.items():\n    if k != TARGET:\n        score = metrics.roc_auc_score(oof_df[TARGET], v)\n        print(f'Overall ROC AUC of {k}: {score}')\n        \noof_df = pd.DataFrame(oof_df)\ntest_df = pd.DataFrame(test_df)\n\nprint()\nprint(f'TOTAL TIME: {format_time(time.time() - start)}')","17918821":"oof_df.to_csv('oof_df_l2.csv', index=False)\ntest_df.to_csv('test_df_l2.csv', index=False)","e2b1d081":"oof_l1 = pd.read_csv('oof_df.csv')\ntest_l1 = pd.read_csv('test_df.csv')\n\nfor col in oof_l1:\n    if col != TARGET:\n        oof_df[col] = oof_l1[col]\n        test_df[col] = test_l1[col]\n        \noof_df.head()","bdf6c43a":"FEATURES = [col for col in oof_df.columns if col not in [TARGET]]\nX = oof_df[FEATURES].to_numpy()\nY = oof_df[TARGET].to_numpy()\nX_test = test_df[FEATURES].to_numpy()","89aa6386":"from collections import defaultdict\n\ntest_preds = np.zeros((test_df.shape[0]))\noof_y = []\noof_preds = []\n\nN_FOLDS = 5\nstart = time.time()\n\nskfolds = model_selection.StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\nfor fold, (t, v) in enumerate(skfolds.split(X, Y)):\n    x_train, x_val = X[t], X[v]\n    y_train, y_val = Y[t], Y[v]\n    \n    print(f\"\\n{'-'*15} FOLD-{fold} {'-'*15}\")\n    tic = time.time()\n\n    clf = LinearRegression()\n    clf = clf.fit(x_train, y_train)\n    preds = clf.predict(x_val).tolist()\n    test_preds += (clf.predict(X_test) \/ N_FOLDS)\n    oof_y.extend(y_val.tolist())\n    oof_preds.extend(preds)    \n\n    score = metrics.roc_auc_score(y_val, preds)\n    \n    print(f\"MODEL: log\\tSCORE: {score}\\tTIME: {format_time(time.time()-tic)}\")\n    \n    del x_train, x_val, y_train, y_val, clf\n    gc.collect()\n\nscore = metrics.roc_auc_score(oof_y, oof_preds)\nprint(f'Overall ROC AUC of reg: {score}')\n\nprint()\nprint(f'TOTAL TIME: {format_time(time.time() - start)}')","df3af486":"submission = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv')\nsubmission[TARGET] = test_preds\n\nsubmission.head()","a49260ac":"submission.to_csv('submission.csv', index=False)","8d3e65de":"## Submission","04d56cdb":"## Imports","b154c85b":"### Level - 3","7783842c":"## EDA","59f4613a":"## Data","f922708d":"## Introduction\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the biological response of molecules given various chemical properties. Although the features are anonymized, they have properties relating to real-world features. \n\nSubmissions are evaluated on **area under the ROC Curve** between the predicted probability and target.","93bad29f":"### Level - 1","30091f26":"## Utils","addd1b23":"### Level - 2","70fd1fc7":"## Modeling","a20a6edf":"## Feature Engineering","bf0d8c24":"## Data preprocessing","69aedd29":"Looks like all the categorical features are binary."}}