{"cell_type":{"ce8e3f47":"code","e4df0c7a":"code","0b4ed2fe":"code","688b5f8c":"code","8394f911":"code","36bd9c23":"code","7ed3a1e1":"code","6e61b749":"code","2d320fe0":"code","670f6eea":"code","17b8a01e":"code","dc04eee0":"code","afc9177c":"code","bc34fcb9":"code","4b9785bf":"code","3812a154":"code","2760524d":"code","e86aff22":"code","02b48018":"code","f7fb8901":"code","371dbef8":"code","01ef3074":"code","2d777f0a":"code","ddf6879f":"code","fcd96fb7":"code","c115b7d2":"code","cd7780e2":"code","d9fd2063":"code","18d9abbd":"code","0f73296e":"code","0c419c63":"code","c6296c9d":"code","5c930ca4":"code","e9507d4c":"code","cb3e7249":"code","1c6c63ba":"code","848eda97":"code","97ade924":"code","50126e51":"code","930ab18c":"code","b08cfa0e":"code","cd181547":"code","07b11ba2":"code","6c12b45c":"code","41f351ea":"code","abd1e663":"code","42e2e351":"code","8c086d6a":"code","975fc2f6":"code","558fc0d9":"code","ea29c57a":"code","0534168f":"code","1f0178bd":"code","819b867b":"code","1928abd6":"code","6e2a8b51":"code","615484eb":"code","c51969cb":"code","054098e9":"code","19026dbe":"code","5952d87f":"code","94bbd866":"code","0b62183d":"code","11dae7f1":"code","8e109f21":"code","18de78d7":"code","e661cf37":"code","dcd90e80":"code","18046837":"code","8953cf71":"code","b5465d48":"code","a08570be":"code","2e455136":"code","0b43ccc4":"code","e1728b83":"code","55a8b845":"code","11459d22":"code","7b6cfae9":"code","48d7c605":"code","3c7de20a":"code","b93c54b6":"code","32fe7433":"code","02d239d2":"code","18d518ea":"code","c48d2d02":"code","ba0ffa17":"code","d1b17852":"code","2d3df4ee":"code","e2afd243":"code","155113b9":"code","ec39d3b6":"code","46b1c649":"code","8aeae1e4":"code","87e966f1":"code","ca56479d":"code","ba7ee090":"code","4857b139":"code","8493da4f":"code","e30da84b":"code","ed3a0c0d":"code","1d92ff3f":"code","0ae69d08":"code","9a3aef6f":"code","df141e6a":"code","39f0f9fc":"code","1d8e5265":"code","f6215cb2":"code","a7503fdd":"code","b162e31e":"code","b980068a":"code","e3eafcf9":"code","0c5a5a7a":"code","017bcd57":"code","51a1daac":"code","c2b9526f":"code","210dedf9":"code","92d4158d":"code","e013daa5":"code","0a7b842f":"code","9970f2df":"code","30d5436b":"code","a1889cec":"code","a3befac6":"code","bfec59cf":"code","3b953f8f":"code","02bed9de":"code","bda77023":"code","4892d392":"code","0a8c37d6":"code","0bdbf0b7":"code","d5e51a4c":"code","4da519dc":"code","cce4f181":"code","12244b5a":"code","04026d57":"code","76a519b6":"code","8614334e":"code","6ec97b5f":"code","b2971f21":"code","6a9702b7":"code","dc5817fd":"code","d7ea5e0d":"code","2ef03b86":"code","0eb889e8":"code","67ca1387":"code","5a572f0a":"markdown","ba4a0b9e":"markdown","40ba6818":"markdown","c2ec4f43":"markdown","6ae257ab":"markdown","117b4ab1":"markdown","94ca07c6":"markdown","745737b6":"markdown","e9d2d69f":"markdown","13eabf6e":"markdown","3cc6093b":"markdown","600af2cd":"markdown","b033c0f3":"markdown","5c0cfbe6":"markdown","1fa88728":"markdown","7ac4792f":"markdown","4dcf7c41":"markdown","b7a900d3":"markdown","bcf378a9":"markdown","df56b468":"markdown","68284a21":"markdown","90f718b6":"markdown","cda00fce":"markdown","51312f7c":"markdown","6ae0f24d":"markdown","4b2e1419":"markdown","21deef83":"markdown","f10929c7":"markdown","81b29b9f":"markdown","a555ea06":"markdown","e720e9a8":"markdown","106b370b":"markdown","294fbeac":"markdown","27f8772f":"markdown","9b6e04bf":"markdown","bb1d3ed3":"markdown","20f493bb":"markdown","3064c230":"markdown","8c67f2a3":"markdown","6818f101":"markdown","170f9b70":"markdown"},"source":{"ce8e3f47":"%matplotlib inline\n#pip install folium\nimport seaborn as sns\nimport statsmodels.formula.api as smf\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nfrom sklearn.model_selection  import train_test_split\nimport numpy as np\nfrom scipy.stats import norm # for scientific Computing\nfrom scipy import stats, integrate\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import fetch_mldata\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd","e4df0c7a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","0b4ed2fe":"# Any results you write to the current directory are saved as output.\n%time KSI_CLEAN=pd.read_csv('\/kaggle\/input\/killed-or-seriously-injured-ksi-toronto-clean\/KSI_CLEAN.csv')","688b5f8c":"#missing values\nKSI_CLEAN = KSI_CLEAN.replace(' ', np.nan, regex=False)\nprint(KSI_CLEAN.isna().sum()\/len(KSI_CLEAN)*100)","8394f911":"fig, ax = plt.subplots(figsize=(15,7))\nsns.heatmap(KSI_CLEAN.isnull(), yticklabels=False,cmap='viridis')","36bd9c23":"KSI_CLEAN.shape","7ed3a1e1":"## Dropping columns where missing values were greater than 80%\nKSI_CLEAN = KSI_CLEAN.drop([\"PEDTYPE\", \"PEDACT\", \"PEDCOND\", \"CYCLISTYPE\", \"CYCACT\", \"CYCCOND\", \"OFFSET\"], axis=1)\nKSI_CLEAN.shape","6e61b749":"KSI_CLEAN.ACCLASS.unique()","2d320fe0":"KSI_CLEAN['ACCLASS'] = np.where(KSI_CLEAN['ACCLASS'] == 'Property Damage Only', 'Non-Fatal', KSI_CLEAN['ACCLASS'])\nKSI_CLEAN['ACCLASS'] = np.where(KSI_CLEAN['ACCLASS'] == 'Non-Fatal Injury', 'Non-Fatal', KSI_CLEAN['ACCLASS'])\nKSI_CLEAN.ACCLASS.unique()","670f6eea":"# Verifying columns with object data type\nprint(KSI_CLEAN.select_dtypes([\"object\"]).columns)","17b8a01e":"##changing all object data types to category \nobjdtype_cols = KSI_CLEAN.select_dtypes([\"object\"]).columns\nKSI_CLEAN[objdtype_cols] = KSI_CLEAN[objdtype_cols].astype('category')","dc04eee0":"KSI_CLEAN.info()","afc9177c":"#Number of Unique accidents by Year\nNum_accident = KSI_CLEAN.groupby('YEAR')['ACCNUM'].nunique()\nplt.figure(figsize=(12,6))\nNum_accident.plot(kind='bar', legend = True)\nplt.show()\n  ","bc34fcb9":"#Number of Unique accidents by Year\nNum_accident = KSI_CLEAN.groupby('MONTH')['ACCNUM'].nunique()\nplt.figure(figsize=(12,6))\nNum_accident.plot(kind='bar', legend = True)\nplt.show()\n#Total number of incidnets have reduced slightly over the years.  ","4b9785bf":"import statsmodels.formula.api as smf\nfrom sklearn import metrics\nfrom sklearn import neighbors\nfrom sklearn.metrics import mean_squared_error \nfrom sklearn import tree, metrics\nfrom scipy.stats import norm \nfrom scipy import stats, integrate\nimport matplotlib.pyplot as plt\nfrom IPython.display import HTML\nimport folium\nfrom folium.plugins import HeatMap\nfrom math import sqrt","3812a154":"#Creating a Heatmap of where Fatality happened (Injury == Fatal)\nKSI_Fatal = KSI_CLEAN[KSI_CLEAN['INJURY'] == 'Fatal']\nKSI_Fatal = KSI_Fatal[['LATITUDE', 'LONGITUDE', 'FATAL']]\nlat_Toronto = KSI_CLEAN.describe().at['mean','LATITUDE']\nlng_Toronto = KSI_CLEAN.describe().at['mean','LONGITUDE']\n#Fatal_map = folium.Map(location = [lat_Toronto, lng_Toronto], zoom_start=5)\n\nToronto_location = [43.6532, -79.3832]\n\nFatal_map = folium.Map(Toronto_location, zoom_start=5)\nHeatMap(KSI_Fatal.values, min_opacity =0.3).add_to(Fatal_map)\nFatal_map","2760524d":"#Categorizing Fatal vs. non-Fatal Incident (non-unique i.e: one accident is counted depending upon involved parties)\n\nsns.catplot(x='YEAR', kind='count', data=KSI_CLEAN,  hue='ACCLASS')","e86aff22":"#Categorizing Fatal vs. non-Fatal Incident (non-unique i.e: one accident is counted depending upon involved parties)\n\nsns.catplot(x='YEAR', kind='count', data=KSI_CLEAN,  hue='FATAL')","02b48018":"#Lets look at Fatality over years (# of people died)\nFatality = KSI_CLEAN[KSI_CLEAN['INJURY'] =='Fatal']\nFatality = Fatality.groupby(KSI_CLEAN['YEAR']).count()\nplt.figure(figsize=(12,6))\nFatality['INJURY'].plot(kind='bar', legend = True,color='r')\n\nplt.show()\n#Fatility was highest in 2016 ","f7fb8901":"\n#Lets look at Fatality over years (# of people died)\nFatal = KSI_CLEAN[KSI_CLEAN['FATAL'] ==1]\nFatal = Fatal.groupby(KSI_CLEAN['YEAR']).count()\nACCFatal=KSI_CLEAN[KSI_CLEAN['ACCLASS'] =='Fatal']\nACCFatal = ACCFatal.groupby(KSI_CLEAN['YEAR']).count()\n# multiple line plot\n# multiple line plot\nplt.plot( 'YEAR', 'FATAL', data=Fatal, marker='o', markerfacecolor='blue', markersize=12, color='skyblue', linewidth=4)\nplt.plot( 'YEAR', 'ACCLASS', data=ACCFatal, marker='', color='olive', linewidth=2)\n\nplt.legend()","371dbef8":"#Looking at area where accident happens\nRegion_KSI_CLEAN = KSI_CLEAN['District'].value_counts()\nplt.figure(figsize=(12,6))\nRegion_KSI_CLEAN.plot(kind='bar', legend = True)\nplt.show()","01ef3074":"## Driving condition VS accident #\n## creating a pivot table for accidents causing by 'SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL'  for EDA.\nKSI_pivot_cause = KSI_CLEAN.pivot_table(index='YEAR', \n                           values = ['SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL'],\n                           aggfunc=np.sum,\n                           margins = True,\n                           margins_name = 'Total Under Category')\nfig, ax1 = plt.subplots(figsize=(8,8))\nKSI_pivot_cause.iloc[11].plot(kind='pie', ax=ax1, autopct='%3.1f%%',fontsize=10)\nax1.set_ylabel('')\nax1.set_xlabel('Driving condition VS Accidents in Ontario in last 10 years(%age)',fontsize=20)","2d777f0a":"## creating a pivot table for accidents causing by 'SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL'  in 10 years\nKSI_pivot_cause.drop('Total Under Category', axis=0, inplace=True)\nfig, ax1 = plt.subplots(figsize=(12,5))\nKSI_pivot_cause.plot(kind='bar', ax=ax1, xticks=KSI_pivot_cause.index)\nplt.legend(bbox_to_anchor=(1.01, 1), loc='upper left')","ddf6879f":"#Causes for Fatal in line graph\nCause_Fatal = KSI_CLEAN.pivot_table(index='YEAR', margins=False ,values=['ALCOHOL', 'AG_DRIV', 'SPEEDING','REDLIGHT','DISABILITY'],aggfunc=np.sum)\nCause_Fatal.plot(figsize=(10,8), title=\"Causes for accidents\", grid=True)\nplt.ylabel('Accidents')","fcd96fb7":"## vechile type VS accident #\n## creating a pivot table for accidents causing by 'AUTOMOBILE', 'MOTORCYCLE', 'TRUCK', 'TRSN_CITY_VEH', 'EMERG_VEH'   in 10 years\nKSI_pivot_Types = KSI_CLEAN.pivot_table(index='YEAR', \n                           values = [ 'AUTOMOBILE', 'MOTORCYCLE', 'TRUCK', 'TRSN_CITY_VEH', 'EMERG_VEH' ],\n                           aggfunc=np.sum,\n                           margins = True,\n                           margins_name = 'Total Under Category')\nfig, ax1 = plt.subplots(figsize=(8,8))\nKSI_pivot_Types.iloc[11].plot(kind='pie', ax=ax1, autopct='%3.1f%%',fontsize=10)\nax1.set_ylabel('')\nax1.set_xlabel('Vechile type VS Accidents in Ontario in last 10 years(%age)',fontsize=20)","c115b7d2":"KSI_pivot_Types.drop('Total Under Category', axis=0, inplace=True)\nfig, ax1 = plt.subplots(figsize=(12,5))\nKSI_pivot_Types.plot(kind='bar', ax=ax1, xticks=KSI_pivot_cause.index)\nplt.legend(bbox_to_anchor=(1.01, 1), loc='upper left')","cd7780e2":"#Type of vehicles involved\nVeh_involved = KSI_CLEAN.pivot_table(index='YEAR',values=['AUTOMOBILE', 'CYCLIST', 'EMERG_VEH', 'MOTORCYCLE', 'TRUCK'],aggfunc=np.sum)\nVeh_involved.plot(figsize=(10,8), title=\"Type of Vehicle Involved\", grid=True)\nplt.ylabel('Vehicles')","d9fd2063":"## Victims VS accident #\n## creating a pivot table for Victims by 'CYCLIST','PEDESTRIAN','PASSENGER' \nKSI_pivot_CPP = KSI_CLEAN.pivot_table(index='YEAR', \n                           values = [ 'CYCLIST','PEDESTRIAN','PASSENGER' ],\n                           aggfunc=np.sum,\n                           margins = True,\n                           margins_name = 'Total Under Category')\nfig, ax1 = plt.subplots(figsize=(8,8))\nKSI_pivot_CPP.iloc[11].plot(kind='pie', ax=ax1, autopct='%3.1f%%',fontsize=10)\nax1.set_ylabel('')\nax1.set_xlabel('Victims VS Accidents in Ontario in last 10 years(%age)',fontsize=20)","18d9abbd":"## Fatal and Disability VS accident #\n## creating a pivot table for 'FATAL','DISABILITY' against accidents #\nKSI_pivot_DF = KSI_CLEAN.pivot_table(index='YEAR', \n                           values = [ 'FATAL','DISABILITY' ],\n                           aggfunc=np.sum,\n                           margins = True,\n                           margins_name = 'Total Under Category')\nfig, ax1 = plt.subplots(figsize=(8,8))\nKSI_pivot_DF.iloc[11].plot(kind='pie', ax=ax1, autopct='%3.1f%%',fontsize=10)\nax1.set_ylabel('')\nax1.set_xlabel('Total Accidents in Ontario in last 10 years(%age)',fontsize=20)","0f73296e":"## creating a pivot table for 'FATAL','DISABILITY' against accidents # in 10 years\nKSI_pivot_DF.drop('Total Under Category', axis=0, inplace=True)\nfig, ax1 = plt.subplots(figsize=(12,5))\nKSI_pivot_DF.plot(kind='bar', ax=ax1, xticks=KSI_pivot_cause.index)\nplt.legend(bbox_to_anchor=(1.01, 1), loc='upper left')","0c419c63":"data = KSI_CLEAN.groupby(by=['YEAR', 'MONTH'],as_index=False).sum()\ndata = data.pivot('MONTH','YEAR','FATAL')\ndata","c6296c9d":"KSI_CLEAN.shape","5c930ca4":"KSI_CLEAN.columns","e9507d4c":"KSI_CLEAN.dtypes","cb3e7249":"KSI_CLEAN_data=KSI_CLEAN[['ACCNUM', 'YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTES', 'WEEKDAY',\n       'LATITUDE', 'LONGITUDE',  'Hood_ID',\n        'District',  \n         'VISIBILITY', 'LIGHT', 'RDSFCOND', \n        'PEDESTRIAN', 'CYCLIST',\n       'AUTOMOBILE', 'MOTORCYCLE', 'TRUCK', 'TRSN_CITY_VEH', 'EMERG_VEH',\n       'PASSENGER', 'SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL', 'DISABILITY']]","1c6c63ba":"KSI_CLEAN_data.dtypes","848eda97":"KSI_CLEAN_data['LATITUDE']=KSI_CLEAN_data['LATITUDE'].astype('int')\nKSI_CLEAN_data['LONGITUDE']=KSI_CLEAN_data['LATITUDE'].astype('int')","97ade924":"print(\"Percentage of missing values in the KSI_CLEAN_data dataset\")\nKSI_CLEAN_data.isna().sum()\/len(KSI_CLEAN_data)*100","50126e51":"KSI_CLEAN_data['District'].unique()","930ab18c":"KSI_CLEAN_data['VISIBILITY'].unique()","b08cfa0e":"KSI_CLEAN_data['LIGHT'].unique()","cd181547":"KSI_CLEAN_data['RDSFCOND'].unique()","07b11ba2":"\nKSI_CLEAN_data = pd.get_dummies(KSI_CLEAN_data, columns=['VISIBILITY','RDSFCOND','LIGHT','District'])","6c12b45c":"KSI_CLEAN_data.shape","41f351ea":"\nKSI_CLEAN_target=KSI_CLEAN[[ 'FATAL']]\nKSI_CLEAN_data.dtypes","abd1e663":"drop_colmns = ['ACCNUM', 'YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTES', 'WEEKDAY',\n       'LATITUDE', 'LONGITUDE', 'Ward_Name', 'Ward_ID', 'Hood_ID',\n       'Division',  'STREET1', 'STREET2', 'ROAD_CLASS',\n       'LOCCOORD', 'ACCLOC', 'TRAFFCTL', 'VISIBILITY', 'LIGHT', 'RDSFCOND',\n       'ACCLASS', 'IMPACTYPE', 'INVTYPE', 'INVAGE', 'INJURY', 'FATAL_NO',\n       'INITDIR', 'VEHTYPE', 'MANOEUVER', 'DRIVACT', 'DRIVCOND',  'PEDESTRIAN',\n       'CYCLIST', 'MOTORCYCLE', 'TRUCK', 'TRSN_CITY_VEH',\n       'EMERG_VEH', 'PASSENGER', 'AUTOMOBILE']\ndf1 = KSI_CLEAN.drop(columns=drop_colmns)","42e2e351":"df1_g2=df1.groupby(['Hood_Name','SPEEDING']).size().to_frame('count').reset_index()\ndf1speed = df1_g2.pivot(index='Hood_Name',columns='SPEEDING',values='count')\ndf1_g2=df1.groupby(['Hood_Name','AG_DRIV']).size().to_frame('count').reset_index()\ndf1agdriv = df1_g2.pivot(index='Hood_Name',columns='AG_DRIV',values='count')\ndf1_g2=df1.groupby(['Hood_Name','REDLIGHT']).size().to_frame('count').reset_index()\ndf1red = df1_g2.pivot(index='Hood_Name',columns='REDLIGHT',values='count')\ndf1_g2=df1.groupby(['Hood_Name','ALCOHOL']).size().to_frame('count').reset_index()\ndf1alco = df1_g2.pivot(index='Hood_Name',columns='ALCOHOL',values='count')\ndf1_g2=df1.groupby(['Hood_Name','DISABILITY']).size().to_frame('count').reset_index()\ndf1disb = df1_g2.pivot(index='Hood_Name',columns='DISABILITY',values='count')\ndf1speed = df1speed.drop(df1speed.columns[0], axis=1)\ndf1speed[2] = df1agdriv.drop(df1agdriv.columns[0], axis=1)\ndf1speed[3] = df1red.drop(df1red.columns[0], axis=1)\ndf1speed[4] = df1alco.drop(df1alco.columns[0], axis=1)\ndf1speed[5] = df1disb.drop(df1alco.columns[0], axis=1)\ndf1speed.columns.names = ['Cause'] \ndf1 = df1speed\ndf1 = df1.dropna()\ndf1.columns = ['SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL','DISABILITY']","8c086d6a":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport matplotlib","975fc2f6":"scaler = StandardScaler()\nSum_of_squared_distances = []\nstd_scale = scaler.fit(df1)\ndf_transformed = std_scale.transform(df1)\npca = PCA(n_components=3)\npca = pca.fit(df_transformed)\nX = pca.transform(df_transformed)\nK = range(1,15)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(df_transformed)\n    Sum_of_squared_distances.append(km.inertia_)","558fc0d9":"plt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Deciding Optimal k')\nplt.show()","ea29c57a":"# import KMeans\nfrom sklearn.cluster import KMeans","0534168f":"KSI_CLEAN_data_cluster=KSI_CLEAN[['ACCNUM', 'YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTES', 'WEEKDAY',\n        'Hood_ID',\n        'District',  \n         'VISIBILITY', 'LIGHT', 'RDSFCOND', \n        'PEDESTRIAN', 'CYCLIST',\n       'AUTOMOBILE', 'MOTORCYCLE', 'TRUCK', 'TRSN_CITY_VEH', 'EMERG_VEH',\n       'PASSENGER', 'SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL', 'DISABILITY','FATAL']]","1f0178bd":"\n\nKSI_CLEAN_data_cluster= pd.get_dummies(KSI_CLEAN_data_cluster, columns=['VISIBILITY','RDSFCOND','LIGHT','District'])","819b867b":"KSI_CLEAN_data_cluster.shape","1928abd6":"# create kmeans object\nkmeans = KMeans(n_clusters=4)\n# fit kmeans object to data\nkmeans.fit(KSI_CLEAN_data_cluster)\n# print location of clusters learned by kmeans object\ncluster_center=pd.DataFrame(kmeans.cluster_centers_,columns=KSI_CLEAN_data_cluster.columns)\nprint(cluster_center)\n# save new clusters for chart\ny_km = kmeans.fit_predict(KSI_CLEAN_data_cluster)","6e2a8b51":"cluster_center.describe()","615484eb":"drop_colmns = ['ACCNUM', 'YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTES', 'WEEKDAY',\n       'LATITUDE', 'LONGITUDE', 'Ward_Name', 'Ward_ID', 'Hood_ID',\n       'Division', 'Hood_Name', 'STREET1', 'STREET2', 'ROAD_CLASS',\n       'LOCCOORD', 'ACCLOC', 'TRAFFCTL', 'VISIBILITY', 'LIGHT', 'RDSFCOND',\n       'ACCLASS', 'IMPACTYPE', 'INVTYPE', 'INVAGE', 'INJURY', 'FATAL_NO',\n       'INITDIR', 'VEHTYPE', 'MANOEUVER', 'DRIVACT', 'DRIVCOND',  'PEDESTRIAN',\n       'CYCLIST', 'MOTORCYCLE', 'TRUCK', 'TRSN_CITY_VEH',\n       'EMERG_VEH', 'PASSENGER', 'AUTOMOBILE']\ndk_dropped = KSI_CLEAN.drop(columns=drop_colmns)\ndk = dk_dropped[dk_dropped['FATAL']==1]\ndk.columns","c51969cb":"dk_g2=dk.groupby(['District','SPEEDING']).size().to_frame('count').reset_index()\ndkspeed = dk_g2.pivot(index='District',columns='SPEEDING',values='count')\ndk_g2=dk.groupby(['District','AG_DRIV']).size().to_frame('count').reset_index()\ndkagdriv = dk_g2.pivot(index='District',columns='AG_DRIV',values='count')\ndk_g2=dk.groupby(['District','REDLIGHT']).size().to_frame('count').reset_index()\ndfred = dk_g2.pivot(index='District',columns='REDLIGHT',values='count')\ndk_g2=dk.groupby(['District','ALCOHOL']).size().to_frame('count').reset_index()\ndkalco = dk_g2.pivot(index='District',columns='ALCOHOL',values='count')\ndk_g2=dk.groupby(['District','DISABILITY']).size().to_frame('count').reset_index()\ndkdisb = dk_g2.pivot(index='District',columns='DISABILITY',values='count')\ndknew = dkspeed\ndknew.columns.names = ['Cause'] \ndknew = dknew.dropna()\ndknew.columns = ['SPEEDING', 'AG_DRIV']\ndknew.nlargest(10,'SPEEDING')\n","054098e9":"dk_g2=dk.groupby(['District','SPEEDING']).size().to_frame('count').reset_index()\ndk_g2","19026dbe":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport matplotlib\n\nscaler = StandardScaler()\nSum_of_squared_distances = []\nstd_scale = scaler.fit(dknew)\ndk_transformed = std_scale.transform(dknew)\nfrom sklearn.cluster import KMeans\nkmeansk = KMeans(n_clusters=2,random_state=3425)\ncolors = ['green','blue']\n\n\nplt.figure(figsize=(15, 5)) \n\nax = plt.subplot(121)\nkc =kmeansk.fit(dk_transformed)\nlabel = pd.DataFrame(kc.labels_)\ndk_result =pd.DataFrame(dk_transformed)\n# label = label.sort_values(by=0)\ndk_result['label']=label\nprint(dk_result.columns)\nscatterd = plt.scatter(dk_result[0],dk_result[1],\n                     c=list(label.iloc[:,0]), cmap=matplotlib.colors.ListedColormap(colors),s=50)\nplt.title('K-Means Clustering Speeding and Aggresive Driving VS District')\nplt.xlabel('Speeding')\nplt.ylabel('Aggresive Driving')\nplt.colorbar(scatterd)","5952d87f":"print('We Conclude that the Highest number of accidents causing fatalitiesby speeding and aggresive driving in Toronto District from 2007-2017, based on Kmeans, occured in')\nneighborhoodsk = dknew.index\nneighborhoodsk = np.array(neighborhoodsk)\nprint(neighborhoodsk[np.where(label[0]==1)])\n#kdsafe = neighborhoodsk[np.where(label[0]==1)]\n#kdaccident = neighborhoodsk[np.where(label[0]==0)]","94bbd866":"ClusterLabelk=pd.DataFrame(kmeansk.labels_) \nClusterLabelk['label']=dknew.index\nClusterLabelk","0b62183d":"drop_colmns = ['ACCNUM', 'YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTES', 'WEEKDAY',\n       'LATITUDE', 'LONGITUDE', 'Ward_Name', 'Ward_ID', 'Hood_ID',\n       'Division',  'STREET1', 'STREET2', 'ROAD_CLASS',\n       'LOCCOORD', 'ACCLOC', 'TRAFFCTL', 'VISIBILITY', 'LIGHT', 'RDSFCOND',\n       'ACCLASS', 'IMPACTYPE', 'INVTYPE', 'INVAGE', 'INJURY', 'FATAL_NO',\n       'INITDIR', 'VEHTYPE', 'MANOEUVER', 'DRIVACT', 'DRIVCOND',  'PEDESTRIAN',\n       'CYCLIST', 'MOTORCYCLE', 'TRUCK', 'TRSN_CITY_VEH',\n       'EMERG_VEH', 'PASSENGER', 'AUTOMOBILE']\ndf_dropped = KSI_CLEAN.drop(columns=drop_colmns)\ndf = df_dropped[df_dropped['FATAL']==1]\ndf.columns","11dae7f1":"df_g2=df.groupby(['Hood_Name','SPEEDING']).size().to_frame('count').reset_index()\ndfspeed = df_g2.pivot(index='Hood_Name',columns='SPEEDING',values='count')\ndf_g2=df.groupby(['Hood_Name','AG_DRIV']).size().to_frame('count').reset_index()\ndfagdriv = df_g2.pivot(index='Hood_Name',columns='AG_DRIV',values='count')\ndf_g2=df.groupby(['Hood_Name','REDLIGHT']).size().to_frame('count').reset_index()\ndfred = df_g2.pivot(index='Hood_Name',columns='REDLIGHT',values='count')\ndf_g2=df.groupby(['Hood_Name','ALCOHOL']).size().to_frame('count').reset_index()\ndfalco = df_g2.pivot(index='Hood_Name',columns='ALCOHOL',values='count')\ndf_g2=df.groupby(['Hood_Name','DISABILITY']).size().to_frame('count').reset_index()\ndfdisb = df_g2.pivot(index='Hood_Name',columns='DISABILITY',values='count')\ndfnew = dfspeed\ndfnew[1] = dfagdriv","8e109f21":"dfnew.columns.names = ['Cause'] \ndfnew = dfnew.dropna()\ndfnew.columns = ['SPEEDING', 'AG_DRIV']\ndfnew.nlargest(10,'SPEEDING')","18de78d7":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nimport matplotlib\n\nscaler = StandardScaler()\nSum_of_squared_distances = []\nstd_scale = scaler.fit(dfnew)\ndf_transformed = std_scale.transform(dfnew)","e661cf37":"from sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=4,random_state=3425)\ncolors =  ['green','blue','red','black']\n\n\nplt.figure(figsize=(15, 5)) \n\nax = plt.subplot(121)\nkc =kmeans.fit(df_transformed)\nlabel = pd.DataFrame(kc.labels_)\ndf_result =pd.DataFrame(df_transformed)\n# label = label.sort_values(by=0)\ndf_result['label']=label\nprint(df_result.columns)\nscatter = plt.scatter(df_result[0],df_result[1],\n                     c=list(label.iloc[:,0]), cmap=matplotlib.colors.ListedColormap(colors),s=50)\nplt.title('K-Means Clustering Speeding and Aggresive Driving VS Hood_Name')\nplt.xlabel('Speeding')\nplt.ylabel('Aggresive Driving')\nplt.colorbar(scatter)\n","dcd90e80":"ClusterLabelh=pd.DataFrame(kmeans.labels_) \nClusterLabelh['label']=dfnew.index\nClusterLabelh.head(5)","18046837":"print('We Conclude that the Highest number of accidents causing fatalities in Toronto Neighborhoods from 2007-2017, based on Kmeans, occured in')\nneighborhoods = dfnew.index\nneighborhoods = np.array(neighborhoods)\nprint(neighborhoods[np.where(label[0]==2)])\n#ksafe = neighborhoods[np.where(label[0]==1)]\n#kaccident = neighborhoods[np.where(label[0]==0)]","8953cf71":"dfnew.nlargest(3,'SPEEDING')","b5465d48":"print('We Conclude that the Highest number of accidents causing fatalities in Toronto Neighborhoods from 2007-2017, based on Kmeans, occured in')\nneighborhoods = dfnew.index\nneighborhoods = np.array(neighborhoods)\nprint(neighborhoods[np.where(label[0]==0)])\n#ksafe = neighborhoods[np.where(label[0]==1)]\n#kaccident = neighborhoods[np.where(label[0]==0)]","a08570be":"dfnew.nsmallest(10,'SPEEDING')","2e455136":"dfnew.nsmallest(10,'SPEEDING').index","0b43ccc4":"KSI_CLEAN.columns","e1728b83":"KSI_CLEAN_data_clusterHieCluster=KSI_CLEAN[['FATAL','YEAR', 'MONTH', 'DAY','Hood_Name',\n                                            'SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL','VISIBILITY', 'LIGHT', 'RDSFCOND']]\n\nKSI_CLEAN_data_clusterHieCluster = pd.get_dummies(KSI_CLEAN_data_clusterHieCluster, columns=['VISIBILITY','RDSFCOND','LIGHT','Hood_Name'])","55a8b845":"# import hierarchical clustering libraries\nimport scipy.cluster.hierarchy as sch\nfrom sklearn.cluster import AgglomerativeClustering","11459d22":"# create dendrogram\ndendrogram = sch.dendrogram(sch.linkage(KSI_CLEAN_data_clusterHieCluster, method='ward'))\n# create clusters\nhc = AgglomerativeClustering(n_clusters=4, affinity = 'euclidean', linkage = 'ward')\n# save clusters for chart\ny_hcm = hc.fit_predict(KSI_CLEAN_data_clusterHieCluster )","7b6cfae9":"import statsmodels.api as sm\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n#Adding constant column of ones, mandatory for sm.OLS model\nX_1 = sm.add_constant(KSI_CLEAN_data)\n#Fitting sm.OLS model\nmodel = sm.OLS(KSI_CLEAN_target,X_1).fit()\nmodel.pvalues\nmodel.pvalues>0.05","48d7c605":"KSI_CLEAN_data.columns","3c7de20a":"##KSI_CLEAN = KSI_CLEAN.drop([\"PEDTYPE\", \"PEDACT\", \"PEDCOND\", \"CYCLISTYPE\", \"CYCACT\", \"CYCCOND\", \"OFFSET\"], axis=1)\nX_new0= KSI_CLEAN_data.drop([\"MONTH\",\"DAY\",\"VISIBILITY_Clear\",\"VISIBILITY_Drifting Snow\",\"VISIBILITY_Fog, Mist, Smoke, Dust\",\"VISIBILITY_Freezing Rain\",\"VISIBILITY_Other\",\"VISIBILITY_Rain\",\"VISIBILITY_Snow\",\"VISIBILITY_Strong wind\", \"EMERG_VEH\",\"Hood_ID\",\"AUTOMOBILE\",\"CYCLIST\"],axis=1)\nX_new0.columns","b93c54b6":"X_new0.shape","32fe7433":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nX_new = SelectKBest(chi2, k=2).fit_transform(KSI_CLEAN_data, KSI_CLEAN_target)\nKSI_CLEAN_data.shape","02d239d2":"X_new.shape","18d518ea":"X_new","c48d2d02":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\nclf = ExtraTreesClassifier(n_estimators=50)\nclf = clf.fit(KSI_CLEAN_data, KSI_CLEAN_target)\n \nmodelETC = SelectFromModel(clf, prefit=True)\nX_new1 = modelETC.transform(KSI_CLEAN_data)\nX_new1.shape   \n","ba0ffa17":"maskETC = modelETC.get_support(indices=False)    # this will return boolean mask for the columns\nX_new1 = KSI_CLEAN_data.loc[:, maskETC]                      # the sliced dataframe, keeping selected columns\nfeatured_col_namesETC =X_new1.columns  # columns name index\nfeatured_col_namesETC","d1b17852":"from sklearn.svm import LinearSVC\n\nfrom sklearn.feature_selection import SelectFromModel\n\nlsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(KSI_CLEAN_data, KSI_CLEAN_target)\nmodelSVC = SelectFromModel(lsvc, prefit=True)\nX_new2 = modelSVC.transform(KSI_CLEAN_data)\nX_new2.shape","2d3df4ee":"maskSVC = modelSVC.get_support(indices=False)    # this will return boolean mask for the columns\nX_new2 = KSI_CLEAN_data.loc[:, maskSVC]                      # the sliced dataframe, keeping selected columns\nfeatured_col_namesSVC =X_new2.columns  # columns name index\nfeatured_col_namesSVC","e2afd243":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor\nimport sklearn.ensemble as ske\nimport matplotlib.pyplot as plt\nregr_depth2 = DecisionTreeRegressor(max_depth=2)\nregr_depth5 = DecisionTreeRegressor(max_depth=5)","155113b9":"# test_size: what proportion of original data is used for test set\nXa_train, Xa_test, y_train,y_test = train_test_split(\n    KSI_CLEAN_data, KSI_CLEAN_target,test_size=1\/7.0, random_state=1)\nregr_depth2.fit(Xa_train,y_train)\n\nscorea = regr_depth2.score(Xa_train,y_train)\nprint(scorea)","ec39d3b6":"logisticRegrb = LogisticRegression(solver = 'lbfgs')\nlogisticRegrb.fit(Xa_train,y_train)\n\nscoreb = logisticRegrb.score(Xa_train,y_train)\nprint(scoreb)","46b1c649":"RFRc = ske.RandomForestRegressor()\nRFRc.fit(Xa_train,y_train)\n\nscorec = RFRc.score(Xa_train,y_train)\nprint(scorec)","8aeae1e4":"#'ACCNUM', 'YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTES', 'WEEKDAY','Hood_ID', 'PASSENGER','SPEEDING'\nfeat_importancesc = pd.Series(RFRc.feature_importances_, index=Xa_train.columns)\nfeat_importancesc.nlargest(10).plot(kind='barh')","87e966f1":"Xa_train.shape","ca56479d":"Xa_train.columns","ba7ee090":"KSI_CLEAN_hoodname=KSI_CLEAN[['Hood_Name', 'YEAR', 'MONTH',\n        'VISIBILITY','RDSFCOND','LIGHT',\n        'SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL', 'DISABILITY']]\n\nKSI_CLEAN_hoodname = pd.get_dummies(KSI_CLEAN_hoodname, columns=['VISIBILITY','RDSFCOND','LIGHT','Hood_Name'])\nKSI_CLEAN_target=KSI_CLEAN[['FATAL']]\n\nXh_train, Xh_test, y_train,y_test = train_test_split(\n    KSI_CLEAN_hoodname, KSI_CLEAN_target,test_size=1\/7.0, random_state=1)\n\n\nRFRh = ske.RandomForestRegressor()\nRFRh.fit(Xh_train,y_train)\n\nscorehn = RFRh.score(Xh_train,y_train)\nprint(scorehn)","4857b139":"KSI_CLEAN_hoodname.shape","8493da4f":"feat_importanceshn = pd.Series(RFRh.feature_importances_, index=Xh_train.columns)\nfeat_importanceshn.nlargest(20).plot(kind='barh')","e30da84b":"KSI_CLEAN_hoodname.columns","ed3a0c0d":"KSI_CLEAN_hoodnameSouthdale=KSI_CLEAN[['Hood_Name','YEAR', 'MONTH','VISIBILITY','RDSFCOND','LIGHT','SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL','DISABILITY']]\nKSI_CLEAN_hoodnameSouthdale = pd.get_dummies(KSI_CLEAN_hoodnameSouthdale, columns=['Hood_Name'])\nKSI_CLEAN_hoodnameSouthdale_columnsname = KSI_CLEAN_hoodnameSouthdale.columns.tolist()\n","1d92ff3f":"KSI_CLEAN_hoodnameSouthdale=KSI_CLEAN_hoodnameSouthdale[['Hood_Name_South Parkdale (85)','YEAR', 'MONTH','VISIBILITY','RDSFCOND','LIGHT','SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL','DISABILITY']]\nKSI_CLEAN_hoodnameSouthdale.columns","0ae69d08":"\n\nKSI_CLEAN_hoodnameSouthdale = pd.get_dummies(KSI_CLEAN_hoodnameSouthdale, columns=['VISIBILITY','RDSFCOND','LIGHT'])\nKSI_CLEAN_target=KSI_CLEAN[['FATAL']]\nXhsd_train, Xhsd_test, y_train,y_test = train_test_split(\n    KSI_CLEAN_hoodnameSouthdale, KSI_CLEAN_target,test_size=1\/7.0, random_state=1)\n\n\nRFRhsd = ske.RandomForestRegressor()\nRFRhsd.fit(Xhsd_train,y_train)\n\nscorehnsd = RFRhsd.score(Xhsd_train,y_train)\nprint(scorehnsd)","9a3aef6f":"KSI_CLEAN_hoodnameSouthdale.shape","df141e6a":"feat_importanceshn = pd.Series(RFRhsd.feature_importances_, index=Xhsd_train.columns)\nfeat_importanceshn.nlargest(20).plot(kind='barh')","39f0f9fc":"feat_importanceshn.nlargest(10).index","1d8e5265":"KSI_CLEAN_data.shape","f6215cb2":"\n# test_size: what proportion of original data is used for test set\nXpca_train, Xpca_test, ypca_train,ypca_test = train_test_split(\n    KSI_CLEAN_data, KSI_CLEAN_target,test_size=1\/7.0, random_state=1)\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# Fit on training set only.\nscaler.fit(Xpca_train)\n\n# Apply transform to both the training set and the test set.\nXpca_train = scaler.transform(Xpca_train)\nXpca_test = scaler.transform(Xpca_test)","a7503fdd":"from sklearn.decomposition import PCA","b162e31e":"\npca = PCA(.95)","b980068a":"\npca.fit(Xpca_train)","e3eafcf9":"\npca.n_components_","0c5a5a7a":"\nXpca_train = pca.transform(Xpca_train)\nXpca_test = pca.transform(Xpca_test)","017bcd57":"Xpca_train.shape","51a1daac":"# all parameters not specified are set to their defaults\n# default solver is incredibly slow thats why we change it\n# solver = 'lbfgs'\nlogisticRegr = LogisticRegression(solver = 'lbfgs')\nlogisticRegr.fit(Xpca_train,ypca_train)\nlogisticRegr.predict(Xpca_test[0].reshape(1,-1))\n","c2b9526f":"logisticRegr.predict(Xpca_test[0:10])","210dedf9":"\npca.explained_variance_ratio_","92d4158d":"pca.explained_variance_ratio_.sum()","e013daa5":"score_pcalg = logisticRegr.score(Xpca_train,ypca_train)\nprint(score_pcalg)","0a7b842f":"for i in range(28,38):\n    print(logisticRegr.predict(Xpca_train[i].reshape(1,-1)))","9970f2df":"\nregr_depth2.fit(Xpca_train,ypca_train)\nregr_depth5.fit(Xpca_train,ypca_train)\ny_1 = regr_depth2.predict(Xpca_train)\ny_2 = regr_depth5.predict(Xpca_train)\nscore_pcaDT=regr_depth2.score(Xpca_train,ypca_train)","30d5436b":"print('Accuracy %d', regr_depth2.score(Xpca_train,ypca_train))","a1889cec":"\nprint('Accuracy %d', regr_depth5.score(Xpca_train,ypca_train))","a3befac6":"RFR = ske.RandomForestRegressor()\nRFR.fit(Xpca_train,ypca_train)\nscore_pcaRF=RFR.score(Xpca_train,ypca_train)\nRFR.score(Xpca_train,ypca_train)","bfec59cf":"# test_size: what proportion of original data is used for test set\nX_new0_train, X_new0_test, y_new0_train,y_new0_test = train_test_split(\n    X_new0, KSI_CLEAN_target,test_size=1\/7.0, random_state=0)\nregr_depth2.fit(X_new0_train,y_new0_train)\n\nscore0DT = regr_depth2.score(X_new0_train,y_new0_train)\nprint(score0DT)","3b953f8f":"logisticRegrb = LogisticRegression(solver = 'lbfgs')\nlogisticRegrb.fit(X_new0_train,y_new0_train)\n\nscore0LR = logisticRegrb.score(X_new0_train,y_new0_train)\nprint(score0LR)","02bed9de":"RFR = ske.RandomForestRegressor()\nRFR.fit(X_new0_train,y_new0_train)\n\nscore0RF = RFR.score(X_new0_train,y_new0_train)\nprint(score0RF)","bda77023":"# test_size: what proportion of original data is used for test set\nX_new1_train, X_new1_test, y_new1_train,y_new1_test = train_test_split(\n    X_new1, KSI_CLEAN_target,test_size=1\/7.0, random_state=0)\nregr_depth2.fit(X_new1_train,y_new1_train)\n\nscore1DT = regr_depth2.score(X_new1_train,y_new1_train)\nprint(score1DT)","4892d392":"logisticRegrb = LogisticRegression(solver = 'lbfgs')\nlogisticRegrb.fit(X_new1_train,y_new1_train)\n\nscore1LR = logisticRegrb.score(X_new1_train,y_new1_train)\nprint(score1LR)","0a8c37d6":"RFR = ske.RandomForestRegressor()\nRFR.fit(X_new1_train,y_new1_train)\n\nscore1RF = RFR.score(X_new1_train,y_new1_train)\nprint(score1RF)","0bdbf0b7":"X_new2_train, X_new2_test, y_new2_train,y_new2_test = train_test_split(\n    X_new2, KSI_CLEAN_target,test_size=2\/7.0, random_state=0)\nregr_depth2.fit(X_new2_train,y_new2_train)\n\nscore2DT = regr_depth2.score(X_new2_train,y_new2_train)\nprint(score2DT)\n","d5e51a4c":"\nlogisticRegrb = LogisticRegression(solver = 'lbfgs')\nlogisticRegrb.fit(X_new2_train,y_new2_train)\n\nscore2LR = logisticRegrb.score(X_new2_train,y_new2_train)\nprint(score2LR)","4da519dc":"RFR = ske.RandomForestRegressor()\nRFR.fit(X_new2_train,y_new2_train)\n\nscore2RF = RFR.score(X_new2_train,y_new2_train)\nprint(score2RF)","cce4f181":"X_new_train, X_new_test, y_new_train,y_new_test = train_test_split(\n    X_new, KSI_CLEAN_target,test_size=3\/7.0, random_state=0)\nregr_depth2.fit(X_new_train,y_new_train)\n\nscore3DT = regr_depth2.score(X_new_train,y_new_train)\nprint(score3DT)\n","12244b5a":"\nlogisticRegrb = LogisticRegression(solver = 'lbfgs')\nlogisticRegrb.fit(X_new_train,y_new_train)\n\nscore3LR = logisticRegrb.score(X_new_train,y_new_train)\nprint(score3LR)\n","04026d57":"RFR = ske.RandomForestRegressor()\nRFR.fit(X_new_train,y_new_train)\n\nscore3RF = RFR.score(X_new_train,y_new_train)\nprint(score3RF)\n","76a519b6":"## data = {'Name':['Tom', 'nick', 'krish', 'jack'],Age':[20, 21, 19, 18]}\nModel_Summary={'Raw':[54,scorea,scoreb,scorec],'PCA':[40,score_pcaDT,score_pcalg,score_pcaRF],\n               'CSK':[2,score3DT,score3LR,score3RF],'ExtraTrees':[12,score1DT,score1LR,score1RF],\n               'LinearSVC':[24,score2DT,score2LR,score2RF],\n               'Pvalue0.05':[40,score0DT,score0LR,score0RF],\n               'RawHood':[173,'N\/A','N\/A',0.945],\n               'RawHSouth':[34,'N\/A','N\/A',0.538]\n              }\nModel_Summary=pd.DataFrame(Model_Summary)\nModel_Summary.rename(index={0:'NumbersOfColumns',1:'DecisionTree',2:'Logistic',3:'Randomforest'}, inplace=True)\nModel_Summary=Model_Summary.round(3)\nModel_Summary","8614334e":"KSI_CLEAN_hoodnamemap2=KSI_CLEAN[['Hood_Name', 'YEAR', 'MONTH','DAY',\n        'VISIBILITY','RDSFCOND','LIGHT',\n        'SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL', 'DISABILITY','FATAL']]\nKSI_CLEAN_hoodnamemap2['Hood_Name'] = KSI_CLEAN_hoodnamemap2['Hood_Name'].map({'Bendale (127)' :'Cluster4',\n         \"L'Amoreaux (117)\":'Cluster4',  'Clairlea-Birchmount (120)' :'Cluster4','Clanton Park (33)' :'Cluster4',\n        'Malvern (132)':'Cluster4','Don Valley Village (47)' :'Cluster4', 'Kensington-Chinatown (78)' :'Cluster4',\n'Moss Park (73)':'Cluster4','Newtonbrook West (36)' :'Cluster4',  'Rosedale-Moore Park (98)' :'Cluster4',\n\"Tam O'Shanter-Sullivan (118)\":'Cluster4','Trinity-Bellwoods (81)' :'Cluster4',  'Waterfront Communities-The Island (77)' :'Cluster4',                                                                            \n'Woburn (137)':'Cluster4','Yonge-St.Clair (97)' :'Cluster4',\n                                                                             \n'South Parkdale (85)':'Cluster3','West Humber-Clairville (1)' :'Cluster3',  'Wexford\/Maryvale (119)'  :'Cluster3',  \n                                                                             \n'Agincourt North (129)':'Cluster2','Newtonbrook West (36)' :'Cluster2',  'Rosedale-Moore Park (98)' :'Cluster2',                                                                            \n 'Bay Street Corridor (76)':'Cluster2','Agincourt South-Malvern West (128)' :'Cluster2',  'Banbury-Don Mills (42)':'Cluster2',                                                                           \n'Bayview Village (52)' :'Cluster2','Bedford Park-Nortown (39)' :'Cluster2',  'Birchcliffe-Cliffside (122)' :'Cluster2', \n'Cabbagetown-South St.James Town (71)':'Cluster2','Caledonia-Fairbank (109)' :'Cluster2',   'Cliffcrest (123)' :'Cluster2',\n                                                                             \n'Corso Italia-Davenport (92)':'Cluster2','Danforth East York (59)' :'Cluster2',  'Dorset Park (126)' :'Cluster2',\n 'Dovercourt-Wallace Emerson-Junction (93)':'Cluster2', 'Downsview-Roding-CFB (26)':'Cluster2',  'Eglinton East (138)' :'Cluster2',\n                                                                             \n'Flemingdon Park (44)':'Cluster2', 'High Park-Swansea (87)':'Cluster2',   'Humber Heights-Westmount (8)':'Cluster2',\n'Humber Summit (21)':'Cluster2',  'Humbermede (22)':'Cluster2',  'Ionview (125)':'Cluster2',                                                                           \n'Islington-City Centre West (14)' :'Cluster2', 'Kennedy Park (124)':'Cluster2',   'Lawrence Park South (103)' :'Cluster2',\n 'Lawrence Park South (103)':'Cluster2',                                                                          \n 'Newtonbrook East (50)':'Cluster2',  'Little Portugal (84)':'Cluster2',   'The Beaches (63)':'Cluster2',                                                                               \n 'Rexdale-Kipling (4)':'Cluster2',    'Milliken (130)':'Cluster2',                                                                               \n 'Roncesvalles (86)':'Cluster2','Rockcliffe-Smythe (111)' :'Cluster2',                                                                                \n 'Rouge (131)':'Cluster2', 'Scarborough Village (139)':'Cluster2',   'South Riverdale (70)':'Cluster2',  \n 'Steeles (116)':'Cluster2',   'Willowdale East (51)':'Cluster2', 'Westminster-Branson (35)' :'Cluster2',                                                                               \n'York University Heights (27)':'Cluster2',                                                                              \n\n                                                                             \n'Alderwood (20)' :'Cluster1', 'Annex (95)':'Cluster1',  'Bathurst Manor (34)'  :'Cluster1',                                                                              \n'Black Creek (24)' :'Cluster1', 'Broadview North (57)':'Cluster1',   'Brookhaven-Amesbury (30)':'Cluster1',                                                                                \n'Casa Loma (96)' :'Cluster1', 'Centennial Scarborough (133)':'Cluster1',   'Church-Yonge Corridor (75)':'Cluster1',                                                                              \n 'Dufferin Grove (83)':'Cluster1', 'Edenbridge-Humber Valley (9)':'Cluster1','Elms-Old Rexdale (5)':'Cluster1',                                                                             \n 'Englemount-Lawrence (32)':'Cluster1',  'Forest Hill South (101)':'Cluster1',   'Glenfield-Jane Heights (25)' :'Cluster1', \n'Greenwood-Coxwell (65)':'Cluster1', 'Henry Farm (53)':'Cluster1',   'High Park North (88)':'Cluster1',                                                                              \n'Highland Creek (134)' :'Cluster1', 'Hillcrest Village (48)':'Cluster1',  'Humewood-Cedarvale (106)' :'Cluster1',                                                                              \n 'Junction Area (90)':'Cluster1', 'Keelesdale-Eglinton West (110)':'Cluster1',                                                                               \n 'Lansing-Westgate (38)' :'Cluster1', 'Leaside-Bennington (56)' :'Cluster1',   'Lawrence Park North (105)':'Cluster1',                                                                             \n 'Long Branch (19)':'Cluster1', 'Maple Leaf (29)':'Cluster1',  'Markland Wood (12)' :'Cluster1',                                                                             \n 'Mimico (includes Humber Bay Shores) (17)':'Cluster1', 'Morningside (135)' :'Cluster1',  'Mount Dennis (115)':'Cluster1',                                                                             \n'Mount Olive-Silverstone-Jamestown (2)':'Cluster1',  'New Toronto (18)' :'Cluster1',   'North Riverdale (68)':'Cluster1', \n'North St.James Town (74)':'Cluster1', 'Oakwood Village (107)':'Cluster1',   'Old East York (58)':'Cluster1',                                                                              \n \"O'Connor-Parkview (54)\":'Cluster1', 'Pleasant View (46)' :'Cluster1',  'Parkwoods-Donalda (45)' :'Cluster1',                                                                             \n 'Regent Park (72)':'Cluster1', 'Rustic (28)' :'Cluster1',   'St.Andrew-Windfields (40)':'Cluster1',                                                                             \n  'Thistletown-Beaumond Heights (3)' :'Cluster1', 'University (79)':'Cluster1',  'Victoria Village (43)' :'Cluster1',                                                                             \n 'Victoria Village (43)':'Cluster1', 'West Hill (136)' :'Cluster1',   'Weston (113)' :'Cluster1', \n 'Weston-Pellam Park (91)':'Cluster1',  'Willowdale West (37)':'Cluster1',   'Willowridge-Martingrove-Richview (7)' :'Cluster1', \n  'Yonge-Eglinton (100)' :'Cluster1'     })\n\nKSI_CLEAN_hoodnamemap1=KSI_CLEAN_hoodnamemap2[['Hood_Name', 'YEAR', 'MONTH',\n        'VISIBILITY','RDSFCOND','LIGHT',\n        'SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL', 'DISABILITY']]\nKSI_CLEAN_hoodnamemap1['Hood_Name'].nunique()                                                                            \n                                                        ","6ec97b5f":"KSI_CLEAN_hoodnamemap= pd.get_dummies(KSI_CLEAN_hoodnamemap1, columns=['VISIBILITY','RDSFCOND','LIGHT','Hood_Name'])\nKSI_CLEAN_target=KSI_CLEAN[['FATAL']]\n\nXhm_train, Xhm_test, y_train,y_test = train_test_split(\n    KSI_CLEAN_hoodnamemap, KSI_CLEAN_target,test_size=1\/7.0, random_state=1)\n\n\nRFRhm = ske.RandomForestRegressor()\nRFRhm.fit(Xhm_train,y_train)\n\nscorehm = RFRhm.score(Xhm_train,y_train)\nprint(scorehm)","b2971f21":"KSI_CLEAN_hoodnamemap.columns","6a9702b7":"feat_importanceshnmap = pd.Series(RFRhm.feature_importances_, index=Xhm_train.columns)\nfeat_importanceshnmap.nlargest(17).plot(kind='barh')\n","dc5817fd":"KSI_CLEAN_hoodnamemap3=KSI_CLEAN_hoodnamemap2[['Hood_Name', 'VISIBILITY', 'RDSFCOND', 'LIGHT',\n       'SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL', 'FATAL','YEAR', 'MONTH','DAY']]\nKSI_CLEAN_hoodnamemap3= pd.get_dummies(KSI_CLEAN_hoodnamemap3, columns=['VISIBILITY','RDSFCOND','LIGHT'])\nKSI_CLEAN_hoodnamemap3.columns","d7ea5e0d":"analysis=['FATAL','SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL', \n        'YEAR', 'MONTH','VISIBILITY_Freezing Rain',\n        'VISIBILITY_Rain', 'VISIBILITY_Snow',\n        'RDSFCOND_Wet', 'LIGHT_Dark',\n        'LIGHT_Dawn', \n       'LIGHT_Daylight',  'LIGHT_Dusk',\n       ]\n","2ef03b86":"KSI_CLEAN_hoodnamemap3.dtypes","0eb889e8":"clusteranalysis_hoodnamemap=KSI_CLEAN_hoodnamemap3.groupby('Hood_Name').mean()\nclusteranalysis_hoodnamemap[analysis]","67ca1387":"clusteranalysis_hoodnamemap1=KSI_CLEAN_hoodnamemap3[KSI_CLEAN_hoodnamemap3['FATAL']==1]\n\nclusteranalysis_hoodnamemap1=clusteranalysis_hoodnamemap1.groupby('Hood_Name').mean()\nclusteranalysis_hoodnamemap1[analysis]","5a572f0a":"plt.figure(figsize=(10, 7))\nplt.scatter(KSI_CLEAN_data_clusterHieCluster[:,0], KSI_CLEAN_data_clusterHieCluster[:,1], c=cluster.labels_, cmap='rainbow')","ba4a0b9e":"##### 3.3.3 Feature selection using ExtraTreesClassifier 12 columns  X_new1\n\nIn ExtraTreeClassifier, 12 columns such as 'ACCNUM', 'YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTES', 'WEEKDAY',\n       'Hood_ID', 'PEDESTRIAN', 'TRUCK', 'PASSENGER', 'AG_DRIV' were selected for modeling","40ba6818":"##### 3.2.4 Cluster analysis using hierarchical clustering libraries","c2ec4f43":"From groupby clause and feature importance, cluster 3 presented highest numbers in fatal injury and its dark light, red light and freezing rain was slightly higher than other clusters. Besides most of the fatal injury in cluster 3 seems to happening earilier than 2011 and focused more on months before July. \n\nBut when further analysis was carried out to only selecting 'FATAL' = 1, that is to say in those fatal injury what is going to change in those numbers. From the next groupby table, it seems that aggressive driving and speeding contribute a lot to accidents lead to fatal injury in cluster 1 though the actual number or accident rate is lower than other region. But dark light in cluster 3 presented higher units among others and it is true that cluster 3 has the most fatal injury reported.\n\nTo finish this section, cluster 3 was found to be more fatal injury and it was largely related to dark light condition in that region. Cluster 1 had the lowest numbers of fatal injury reported but its aggressive driving and speeding tend to be higher compared to other regions or clusters. So the relative safer zone was cluster 4 and then cluster 2 region.","6ae257ab":"##### 3.3.1 Feature selection using P value\n\nFrom P value, MONTH,DAY,Visbility, EMERG_VEH,HoodID,AUTOMOBILE,CYCLIST are over 0.05 that can be dropped for modelling. 40 columns were selected('ACCNUM', 'YEAR', 'HOUR', 'MINUTES', 'WEEKDAY', 'LATITUDE', 'LONGITUDE',\n       'PEDESTRIAN', 'MOTORCYCLE', 'TRUCK', 'TRSN_CITY_VEH', 'PASSENGER',\n       'SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL', 'DISABILITY',\n       'RDSFCOND_Dry', 'RDSFCOND_Ice', 'RDSFCOND_Loose Sand or Gravel',\n       'RDSFCOND_Loose Snow', 'RDSFCOND_Other', 'RDSFCOND_Packed Snow',\n       'RDSFCOND_Slush', 'RDSFCOND_Spilled liquid', 'RDSFCOND_Wet',\n       'LIGHT_Dark', 'LIGHT_Dark, artificial', 'LIGHT_Dawn',\n       'LIGHT_Dawn, artificial', 'LIGHT_Daylight',\n       'LIGHT_Daylight, artificial', 'LIGHT_Dusk', 'LIGHT_Dusk, artificial',\n       'LIGHT_Other', 'District_Etobicoke York', 'District_No District',\n       'District_North York', 'District_Scarborough',\n       'District_Toronto East York').","117b4ab1":"#### 2.5 Pivot table and pie chart for Summarison\n\nAutomobiles have been pretty consistent reason of accident over the years involving aggressive driving and pedestrians \n\n##### The columns in ksi_pivot can be classified into following categories:\n    - Driving condition for accidents (AG_DRIV, ALCOHOL, DISABILITY, REDLIGHT, SPEEDING)\n    - Type of vehicles involved (AUTOMOBILE, CYCLIST, EMERG_VEH, MOTORCYCLE, TRNS_CITY_VEH, TRUCK)\n\nFrom KSI_pivot_cause, AG_DRIV (Aggressive and Distracted Driving) are the major cause of accidents (62.9%), speeding accounts for 21.4, redlight is 10.4% and alcohol is 5.3 %.\n\nFrom KSI_pivot_Types, automobile is reponsible for 45.6 % accident while truck, motocycle,Transit or City Vehicle are all second to it.\n\nFrom KSI_pivot_CPP, passengers and PEDESTRIAN are major victims (each 40~ %) and cyclist (12 %) are second to them.\n\nFrom KSI_pivot_DF, Accidents caused 80~ % fatality.\n","94ca07c6":"##### 3.2.2 Cluster analysis using Kmeans in general\n\nIn this section, Kmean was applied to all other column besides those mentioned above. But using district instead of hoodname.","745737b6":"#### 2.4 Region vs District --- Toronto East York has the most accident numbers.","e9d2d69f":"In this modeling, several graph was used to explore relations inside the dataset. Column 'FATAL' was chozen as target output as it did not has missing value and in numeric format.In region-wise, Toronto east region was reported to be the most accidents ending up with fatal injury as showed in 2.4 section. Automobiles have been pretty consistent reason of accident over the years involving aggressive driving and pedestrians. AG_DRIV (Aggressive and Distracted Driving) are the major cause of accidents (62.9%), speeding accounts for 21.4%, redlight is 10.4% and alcohol is 5.3 % based on 2.5 section. \n\nThen feature selection and cluster analysis was employed to reduce dimension and optimized for best modeling score. From feature selection, column like accnum, year, month, hoodID or name, weekday, minute and hour has great impact to modeling score and relation to fatal injury. But accnum, minute and hour did not has a lot business sense in real situation and in this modeling, thus was not selecting for final modeling. From elbow cluster method, 4 clusters was used to map the hoodname for final modeling to present which region result in more fatal injury and what is the cause behind it. In general, the results are not very significant that can demonstrate big reveral of the cause. But relative safer zone or dangerous area can be identified. Dark light, red light and freezing rain was found at slightly higher in the dangerous area that causing 0.05 unit more fatal injury than other areas. Those dangerous area are South parkdale, West humber-clairville and Wexford\/Maryvale. Hood name in Cluster 4 and 2 was found to be relative safer zone in fatal injury.\n\nRegarding modeling-wise, pca was the best among all modeling for dimension reducing and modeling score but its drawback was obvious that it can't tell right away difference and that indicating real facotrs. It has to be done with PCA ratios in order to find out contribution for each features. Among all modeling and feature selection, 'Year' and 'Month' column greatly impact th modeling score. Column accident number, though not presenting real indication of fatal injury, plays a important role in affecting scores. So those year and month column were kept for modeling. Only selecting hoodname and other condition related column yield modeling score under or around 0.5 which will greatly impact our prediction for fatal injury. Selecting the final modeling with score 0.74 was the compromise between those factors. Finding a balance between real business model and theoretical prediction modeling was very interesting and sometimes complicated. \n\nIn general, aggressive driving and speeding contribute greatly to fatal injury. But in dangerous neighborhood, dark light was found be to causing issues compared to other regions. \n\nFor future work, if more numeric data was reported based on different condition, that will help us to understand the real business model. Ages for driver invloed in fatal injury was missed in the dataset that may replace the impact of accunum column and improve the modeling score. \n\nGreat thanks!\n\n\n\n\n\n\n\n\n\n","13eabf6e":"Looking at cluster that labeled as 2 and its relative pivot table for speeding and aggressive fatal numbers,  'Elms-Old Rexdale (5)', 'Kingsway South (15)','Broadview North (57)', 'Church-Yonge Corridor (75)','Englemount-Lawrence (32)', 'Highland Creek (134)','Hillcrest Village (48)', 'Lansing-Westgate (38)','O'Connor-Parkview (54)', 'Oakwood Village (107)' are the least accidents reported among all regions and are in cluster 0.","3cc6093b":"plt.figure(figsize=(12,6))\nsns.heatmap(data)\nplt.show()","600af2cd":"#### 1.3 Data cleaning by changing data type","b033c0f3":"District         category\n\nTRAFFCTL         category\nVISIBILITY       category\nLIGHT            category\nRDSFCOND         category\nACCLASS          category","5c0cfbe6":"From the data above, accidents happened more from June to October","1fa88728":"#### 3.3.9 Final modeling\n\nBased on 3.1 cluster analysis, fousing on hood name would be more interesting and 4 clusters were identified by K mean. The next question is how to employ this into our modeling? So here hood name was mapped by 4 clusters based on K mean clusters and would be applied to the modeling with factors that affecting the condition that causing fatal injury. \n\n'YEAR' and 'MONTH' column would be kept as they contributed greatly to the score of modeling. without them, the score would drop to 0.5. Then columns that affect greatly to score but do not have any practical sense to the business would be dropped, such as 'HOUR', 'MINUTES', 'WEEKDAY', 'PEDESTRIAN', 'AUTOMOBILE', 'TRUCK','TRSN_CITY_VEH', 'PASSENGER'.'Hood_ID' would be replaced by Hood name that would be mapped in 4 clusters(regions).","7ac4792f":"From 2007 to 2017, the numbers of non-fatal accident declined while those occurance of fatal accident kept unchanged.","4dcf7c41":"##### 3.2.3 Cluster analysis using Kmeans checking neighborhood using raw data with selected columns\n\nHere 'SPEEDING' and 'AG_DRIV' were selected as for investigation of Neighborhood\n\n1. It is Concluded that the Highest number of accidents causing fatalities by speeding and aggressive driving in Toronto District from 2007-2017, based on Kmeans, was occurred more in['Toronto East York']\n\n\n2. when hoodname was chozen as input,  'South Parkdale (85)' 'West Humber-Clairville (1)' and 'Wexford\/Maryvale (119)' rank top 3 out of all hoods as highest numbers of accidents that related to speeding and aggressive driving based on cluster that labeled as 2 and its relative pivot table for speeding and aggressive fatal numbers.\n\nFrom cluster that labeled as 0 and its relative pivot table for speeding and aggressive fatal numbers,  'Elms-Old Rexdale (5)', 'Kingsway South (15)','Broadview North (57)', 'Church-Yonge Corridor (75)','Englemount-Lawrence (32)', 'Highland Creek (134)','Hillcrest Village (48)', 'Lansing-Westgate (38)','O'Connor-Parkview (54)', 'Oakwood Village (107)' are the least accidents reported among all regions.\n","b7a900d3":"#### 1.2 Data cleaning by dropping columns with large amount of missing value from heat map","bcf378a9":"##### 3.3.8 Modeling score comparison\n\n\n","df56b468":"##### 3.3.6 Raw data modeling selecting Hoodname instead of district for checking whcih hoodname has problems. 173 columns ","68284a21":"plt.figure(figsize=(10, 7))\nplt.scatter(KSI_CLEAN_data_clusterHieCluster[:,0], KSI_CLEAN_data_clusterHieCluster[:,1], c=cluster.labels_, cmap='rainbow')","90f718b6":"#### 3.2 Cluster analysis\n\n##### 3.2.1 Elbow Method to Determine Number of Clusters\n\nTo begin with Cluster analysis, Elbow method was explored to identify optimized cluster number using 'SPEEDING', 'AG_DRIV', 'REDLIGHT', 'ALCOHOL','DISABILITY' as input and aggregate those numbers as output to further investigate and identify which neibourhood has highest numbers of accident related to input condition.\n\nElbow method revealed the optimal cluster to be 4 clusters.\n\n","cda00fce":"Lets try to understand that what causes of accidents resulted in Fatal incidents (involving 1 or more deaths).\nTo do the first lets get the unique values of fatal incidents in a seperate df and then do the analysis. ","51312f7c":"#### 3.3.5 Raw data modeling (no pca and feature selection)  54 columns\n\n10 columns 'ACCNUM', 'YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTES', 'WEEKDAY','Hood_ID', 'PASSENGER','SPEEDING' were selected as top 10 feature importance.\n\nPerformance\n\nregr_depth2.score 0.028\n\nlogisticRegrb.score 0.864\n\nRFRc.score 0.971\n\n","6ae0f24d":"Looking at cluster that labeled as 2 and its relative pivot table for speeding and aggressive fatal numbers,  'South Parkdale (85)' 'West Humber-Clairville (1)' and 'Wexford\/Maryvale (119)' rank top 3 out of all hoods.","4b2e1419":"Here is the list of dropping columns: 'ACCLOC', 'Ward_Name', 'Ward_ID', 'Hood_Name', 'Hood_ID',\n       'Division', 'STREET1', 'STREET2', 'ROAD_CLASS', 'LOCCOOR','TRAFFCTL',\n       'ACCLOC',  'RDSFCOND', 'ACCLASS','IMPACTYPE', 'INVTYPE', 'INVAGE', 'INJURY', 'FATAL_NO', 'INITDIR',\n       'VEHTYPE', 'MANOEUVER', 'DRIVACT', 'DRIVCOND'\n       'ACCLASS',\n\nReason:overlapping or missing value or not related\n       ","21deef83":"### 4.0 Conclusion and future work","f10929c7":"### 3.0 Data Modeling\n\n#### 3.1 Data preparation for modeling by checking null value, get dummies\n","81b29b9f":"#### 2.3 ACCLASS VS Fatal VS injury --- looking for target column (FATAL)\n\nACCLASS is columns that classified into 3 catergory while fatal only show whether is fatal. \n\n1.In general column ACCLASS and FATAL both show same pattern. \n\n2.Injury miss a lot of information thus not very meaningful to be the target column\n\nAs Fatal column was already in int type and quite similar to ACCLASS, it will be used as output in data modeling later.","a555ea06":"### 1.Data Preparation\n\n#### 1.1 Data cleaning by replacing blank value to NA","e720e9a8":"#### 2.2 Heat map of accidents","106b370b":"##### 3.3.2 Feature selection using chi2 and SelectKBest\n\nIn this model, only 2 columns was selected, namely, FATAL as output and ACCNUM as input. ","294fbeac":"Raw data modeling selecting one Hoodname instead of district for checking only hoodname southdale has problems. 32 columns \n\n'LIGHT_Daylight', 'AG_DRIV', 'LIGHT_Dark, artificial', 'LIGHT_Dark', 'ALCOHOL', 'RDSFCOND_Dry', 'REDLIGHT', 'RDSFCOND_Wet' are condition that need to be improved in South dale area when feature importance was used. Accidnets tend to be more on the dawn time and aggressive driving was the next contribution to fatality in South Parkdale region","27f8772f":"##### 3.3.4 Feature selection using LinearSVC X_new2\n\nIn LinearSVC model, 24 columns including 'YEAR', 'MONTH', 'DAY', 'HOUR', 'MINUTES', 'WEEKDAY', 'LATITUDE',\n       'LONGITUDE', 'Hood_ID', 'PEDESTRIAN', 'AUTOMOBILE', 'TRUCK',\n       'TRSN_CITY_VEH', 'PASSENGER', 'SPEEDING', 'AG_DRIV', 'VISIBILITY_Clear',\n       'RDSFCOND_Wet', 'LIGHT_Dark', 'LIGHT_Daylight',\n       'District_Etobicoke York', 'District_North York',\n       'District_Scarborough', 'District_Toronto East York' were selected for modeling","9b6e04bf":"Measuring Model Performance was evaluated using model scrore. that is accuracy (fraction of correct predictions): correct predictions \/ total number of data points\n\nBasically, how the model performs on new data (test set)\n\nPerformance\n\nLogistic   0.734\n\ndecision treee 2  0.035\n\nRandom tree forest   0.97\n","bb1d3ed3":"Total number of incidnets have reduced slightly over the years.","20f493bb":"Columns meaning\n\nACCNUMAccident Number\n\nYEAR Year of Accident\n\nMONTH Month of Accident\n\nDAY Day of Accident\n\nHOUR Hour of Accident (24hrs)\n\nMINUTES Minute of Accident\n\nWEEKDAY Weekday of Accident (0 is Monday)\n\nLATITUDE Latitude\n\nLONGITUDE Longitude\n\nWard_NameCity Ward\n\nWard_IDCity Ward ID\n\nHood_NameNeighbourhood Name\n\nHood_IDNeighbourhood ID\n\nDivisionPolice Division\n\nDistrictCity District\n\nSTREET1Street of Accident\n\nSTREET2Street of Accident\n\nOFFSETDistance and direction of the accident\n\nROAD_CLASSRoad Classification\n\nLOCCOORDLocation Coordinate\n\nACCLOCAccident Location\n\nTRAFFCTL Traffic Control Type\n\nVISIBILITY Environment Condition\n\nLIGHT Light Condition\n\nRDSFCOND Road Surface Condition \n\nACCLASS Classification of Accident\n\nIMPACTYPE Initial Impact Type\n\nINVTYPE Involvement Type\n\nINVAGE Age of Involved Party\n\nINJURY Severity of Injury\n\nFATAL_NO Sequential Number\n\nINITDIR Initial Direction of Travel\n\nVEHTYPE Type of Vehicle\n\nMANOEUVER Vehicle Manouever\n\nDRIVACT Apparent Driver Action\n\nDRIVCOND Driver Condition\n\nPEDTYPE Pedestrian Crash Type\n\nPEDACT Pedestrian Action\n\nPEDCOND Condition of Pedestrian\n\nCYCLISTYPE Cyclist Crash Type\n\nCYCACT Cyclist Action\n\nCYCCOND Cyclist Condition\n\nPEDESTRIAN Pedestrian Involved In Collision\n\nCYCLIST Cyclists Involved in Collision\n\nAUTOMOBILE Driver Involved in Collision\n\nMOTORCYCLE Motorcyclist Involved in Collision\n\nTRUCK Truck Driver Involved in Collision\n\nTRSN_CITY_VEH Transit or City Vehicle Involved in Collision\n\nEMERG_VEH Emergency Vehicle Involved in Collision\n\nPASSENGER Passenger Involved in Collision\n\nSPEEDINGSpeeding Related Collision\n\nAG_DRIVAggressive and Distracted Driving Collision\n\nREDLIGHT Red Light Related Collision\n\nALCOHOL Alcohol Related Collision\n\nDISABILITY Medical or Physical Disability Related Collision\n\nFATAL Fatal Injury in Collision\n","3064c230":"### 2.0 Data relationship exploration\n\n#### 2.1 Accident numbers against month and year","8c67f2a3":"# Killed or Seriously Injured (KSI) Toronto Clean\nShanhua Huang\n\n### Abstract:\n\nFeature selection and kmean clusters were applied to help identify key contribution to fatal injury and neighbourhood in order to provide improvement and suggestions on how to improve.    \n\n### Introduction:\n\nThis data set is a cleaned version of the Killed or Seriously Injured (KSI) traffic accident reports from the City of Toronto Police Open Data portal. Inside is information for all traffic accidents reported between 2007 and 2017. There is data on the time, location, and the type of incident with various attributes about the traffic conditions at the time of the incident.\n    \n    \n### Background:\n\nToronto may not have the most friendly weather conditions and despite the best technologies, it is difficult to eliminate accidents caused by acts of nature. However, with greater safety precautions, right kind of education and higher prosecution rates, it would help increase road safety. Nowadays, driving safety has been a hot topic in the past few years as people from all walks of life travel to work by means of cars. Driving did indeed reduce time for transportation but its safety was a great concern. It came with a high speed that may not stop at the right timing to save life or accidents. Our interest would be to help Toronto district and neighbourhood to investigate cause of fatal injury and provide suggestions on how to reduce such fatal moments. \n\n\n### Objective\n\n1.Identifying area which is leading to more fatal injury\n\n2.Digging out cause for fatal injury and suggestion to specific regoin\n\n\n### Outline\n1.Data Cleaning\n\n1.1 Data cleaning by replacing blank with NA\n\n1.1 Data cleaning by replacing blank with NA\n\n1.2 Data cleaning by rdropping columns with large amounts of missing value\n\n1.3 Data cleaning by changing data type\n\n2.Data Exploration\n\n2.1 Accident numbers against month and year\n\n2.2 Heat map of accidents\n\n2.3 ACCLASS VS Fatal VS injury --- looking for target column (FATAL)\n\n2.4 Region vs District --- Toronto East York has the most accident numbers.\n\n2.5 Pivot table and pie chart for summary\n\n3.Data Modeling\n\n3.1 Data preparation and get dummies\n\n3.2 Cluster analysis\n\n3.2.1 Elbow Method to Determine Number of Clusters\n\n3.2.2 Cluster analysis using Kmean in general\n\n3.2.3 Cluster analysis using Kmean checking neighborhood using raw data with selected columns\n\n3.2.4 Cluster analysis using hierarchical clustering libraries\n\n3.3 Feature selection\n\n3.3.1 Feature selection using P value\n\n3.3.2 Feature selection using chi2 and SelectKBest\n\n3.3.3 Feature selection using ExtraTreesClassifier 12 columns  X_new1\n\n3.3.4 Feature selection using LinearSVC X_new2\n\n3.3.5 Raw data modeling (no pca and feature selection)  54 columns\n\n3.3.6 Raw data modeling selecting Hoodname instead of district for checking only hoodname southdale has problems. 32 columns \n\n3.3.7 Data model using PCA transform 40 columns from vectors  Xpca_train\n\n3.3.8 Modeling score comparison\n\n3.3.9 Final modeling\n   \n4.0 Conclusion and future work","6818f101":"#### 3.3 Feature selection","170f9b70":"##### 3.3.7 Data model using PCA transform 40 columns from vectors  Xpca_train\n\nIn PCA modeling, applying vector theory, 40 vectors from 54 columns were generated for modeling"}}