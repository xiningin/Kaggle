{"cell_type":{"408bf77a":"code","1443739e":"code","bce5191c":"code","91382405":"code","933d92d9":"code","ee9dd019":"markdown","ad355896":"markdown"},"source":{"408bf77a":"%%writefile submission.py\n\nfrom typing import Tuple, Optional\n\nimport numpy as np\nimport pandas as pd\n\n\nclass MyConfig:\n    P_NOISY_DECISION = 0.1\n\nassert isinstance(MyConfig.P_NOISY_DECISION, float)\nassert 0.00 <= MyConfig.P_NOISY_DECISION <= 1.00\n\nclass HandSigns:\n    ROCK: int = 0\n    PAPER: int = 1\n    SCISSORS: int = 2\n    ALL: Tuple[int] = (ROCK, PAPER, SCISSORS)\n\n    @classmethod\n    def predetor_of(cls, sign: int) -> int:\n        \"\"\"Return predetor of given sing.\n        \"\"\"\n        assert(sign in cls.ALL)\n        return (sign + 1) % 3\n    \n    @classmethod\n    def win(cls, my_action: int, opponent_action: int) -> bool:\n        return my_action == cls.predetor_of(opponent_action)\n\n\nclass History:\n\n    def __init__(self):\n        \n        self.total = [[0, 0, 0], [0, 0, 0]]\n        self.log = []\n        self.last_agent_action = None\n        self.last_opponent_action = None\n        self.note = ''\n\n\n    def memorize(self,\n                 agent_action: Optional[int] = None,\n                 last_opponent_action: Optional[int] = None,\n                 note: Optional[str] = None) -> None:\n        \"\"\"Save agent's action done on this step as the last action.\n        \"\"\"\n        if agent_action is not None:\n            self.last_agent_action = agent_action\n        \n        if last_opponent_action is not None:\n            self.last_opponent_action = last_opponent_action\n        \n        if note is not None:\n            self.note = note\n        return None\n\n\n    def update(self) -> None:\n        \"\"\"Update game history by step.\n        \n        When it is an first step, save oppenent's last action only.\n        If not, new action log is created by last action of agent and opponent.\n        \"\"\"\n\n        # self.last_opponent_action = last_opponent_action\n        on_first_step = self.last_opponent_action is None\n\n        if not on_first_step:  # not first step\n            # create new log\n            self.log.append((self.last_agent_action,\n                             self.last_opponent_action,\n                             HandSigns.win(self.last_agent_action, self.last_opponent_action),\n                             self.note))\n            self.total[0][self.last_agent_action] += 1\n            self.total[1][self.last_opponent_action] += 1\n        return None\n\n\n#     def write_log(filepath: str = 'history.csv') -> None:\n#         df = pd.DataFrame(history.log, columns=['agent', 'opponent', 'win', 'note'])\n#         df.to_csv(filepath)\n#         return None\n\n\n    def most_frequent_actions(self, idx: int = 1) -> list:\n        '''Return action taken most frequently by player.\n        \n        Parameters\n        ----------\n        idx: int, default = 1.\n            0 is my agent, 1 is opponent.\n        '''\n        assert(idx in (0, 1))  # 0: agent, 1: opponent\n        total = self.total[idx]\n        max_freq = max(total)\n        most_frequent_actions = []\n        for sign in HandSigns.ALL:\n            n_freq = total[sign]\n            if n_freq == max_freq:\n                most_frequent_actions.append(sign)\n        return most_frequent_actions\n\n    \nhistory = History()\ndef agent(observation, configuration):        \n    global history\n    last_opponent_action = None if observation.step < 1 else observation.lastOpponentAction\n    history.memorize(last_opponent_action=last_opponent_action)\n    history.update()\n\n    my_action = HandSigns.ROCK\n    note = ''\n    if np.random.choice([True, False], p=[MyConfig.P_NOISY_DECISION, 1 - MyConfig.P_NOISY_DECISION]):\n        my_action = np.random.randint(3)\n        note = 'Noise'\n    else:\n        # Decide my action with assuming that opponent will take an action \n        # that opponent have taken most freqently.\n        # If all hand-signs have same frequency, my action is chosen randomly.\n        opponent_most_frequent_actions = history.most_frequent_actions(1)\n        if len(opponent_most_frequent_actions) == 3:\n            # All hand-signs have same frequency.\n            my_action = np.random.randint(3)\n            note = 'Random choice'\n        elif len(opponent_most_frequent_actions) == 2:\n            # 2 hand-sings have same frequency.\n            if opponent_most_frequent_actions == {HandSigns.ROCK, HandSigns.PAPER}:\n                my_action = HandSigns.PAPER\n            elif opponent_most_frequent_actions == {HandSigns.ROCK, HandSigns.SCISSORS}:\n                my_action = HandSigns.ROCK\n            else:\n                my_action = HandSigns.SCISSORS\n            note = \"Counter against most frequent action\"\n        else:\n            most_frequent_action = opponent_most_frequent_actions[0]\n            my_action = HandSigns.predetor_of(most_frequent_action)\n            note = \"Counter against most frequent action\"\n\n    history.memorize(agent_action=my_action, note=note)\n    return my_action","1443739e":"from kaggle_environments.envs.rps.agents import *\nfrom kaggle_environments import make\n\ndef randomAgent(obs, config):\n    return random.randint(0, 2)\n\nenv = make('rps', debug = True)\n\n# Pass in your agent to evaluate\ndef evaluate(agent):\n    \n    pool = [\n        rock,\n        paper,\n        scissors,\n        copy_opponent,\n        reactionary,\n        counter_reactionary,\n        statistical,\n        randomAgent,\n    ]\n\n\n    print()\n    overall = 0\n\n    for opponent in pool:\n\n        env.reset()\n\n        env.run([agent, opponent])\n        json = env.toJSON()\n        rewards = json['rewards']\n\n        opponentName = str(opponent).split()[1].capitalize()\n\n        padding = ''; length = len(str(rewards))\n        if length < 15: padding = ' ' * (15 - length)\n\n        print(f'Rewards: {rewards}{padding}  Agent vs {opponentName}')\n        overall += rewards[0]\n    \n    overall \/= len(pool)\n    print(f'Overall Score: {overall}')\n    \n    return overall","bce5191c":"score = evaluate('\/kaggle\/working\/submission.py')\nif score <= 0:\n    print('\\nU MIGHT WANNA FIX SOME THINGS BUDDY')\nelse:\n    print('\\nITS GREAT! NOW CHUCK IT INTO THE LEADERBOARD')","91382405":"env.reset()\nenv.run(['submission.py', reactionary])\nenv.render(mode=\"ipython\", width=500, height=500)","933d92d9":"env.reset()\nenv.run(['submission.py', statistical])\nenv.render(mode=\"ipython\", width=500, height=500)","ee9dd019":"Creadit:  \nhttps:\/\/www.kaggle.com\/taahakhan\/rps-agent-pool-evaluation","ad355896":"# Evaluation"}}