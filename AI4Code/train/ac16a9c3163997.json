{"cell_type":{"3603735e":"code","7bd024c9":"code","6df0cd26":"code","7305ead2":"code","b607bb01":"code","768ee027":"code","ad055180":"code","ef90fed8":"code","f3a533be":"code","4a060703":"code","cb62a237":"code","37f054c9":"code","be5bd9d3":"code","e50d3dab":"code","6827965b":"code","1af73a67":"code","3f69e885":"code","9b9dcd8d":"code","5fd5dbd3":"code","9bcb4dcf":"code","5a96ffbc":"code","4a29f403":"code","2fda7910":"code","ece07865":"code","dd391185":"code","1e9ff7f0":"code","ecd116c1":"code","ed341c7c":"code","2de9a8d5":"code","1a784f36":"code","9c98d943":"code","c4295d30":"code","c013c862":"code","02166dd9":"code","46b20271":"code","7f550b7b":"code","cb3ce4f1":"code","d66facc8":"code","c9195beb":"code","e1676d10":"code","578819da":"code","e6d1ebe9":"code","ee8d7ca9":"code","bb5cee7b":"code","2dd537f7":"code","30d099fa":"code","9fec9893":"code","a2b8ea90":"code","9fc8ca5b":"code","8348e653":"code","d923c290":"code","8fdca89e":"code","d347ca99":"code","c013b48b":"code","ae098d91":"code","50259379":"code","39791295":"code","9d2ce714":"code","ed4dc823":"code","305a6577":"code","d41c7895":"code","59009e61":"code","fdcc9dc4":"code","c29e0582":"code","ef8d76f1":"code","7a90c0da":"code","aaf346dc":"code","7dee8164":"code","f21ed0d0":"code","6db4ce15":"code","cbf94745":"code","b9d2ed6b":"code","df8f99a8":"code","632d90f3":"code","84c6ffc4":"code","f1e59c94":"code","765b8a26":"code","8bd72cf9":"code","d5c2ad28":"code","95258a6e":"code","c0efc765":"code","be5e693c":"code","0f7fd360":"code","eef2e8f2":"code","eb39a5bd":"code","2a2ceea5":"code","5ac8377b":"code","7a4457be":"code","d980a895":"code","5d1debf2":"code","f453f176":"code","c9f7a43b":"code","1f8738cf":"code","e3a5499d":"code","15f92d20":"code","0298ffc6":"code","c5ff29c6":"code","838f5112":"code","3bc45026":"code","8f69baea":"code","3fa43871":"markdown","cecd49fa":"markdown","6817e9e0":"markdown","769a09ea":"markdown","ddd7b2d9":"markdown","14d08b1e":"markdown","dfcd7bab":"markdown","bddb8fcf":"markdown","1a2ef81d":"markdown","3f19780b":"markdown","20b9ad81":"markdown","13a74d9f":"markdown","749aa33b":"markdown","04a41895":"markdown","1af5d15c":"markdown","43286d8e":"markdown","adee4f74":"markdown","0258041a":"markdown","3df50ec0":"markdown","2f508ec2":"markdown","42f12aff":"markdown","75ecc3db":"markdown","b0f4e72e":"markdown","53e295e7":"markdown","d8ea9903":"markdown","41897cc4":"markdown","77c393b2":"markdown","2c8d66e1":"markdown","4526d00a":"markdown","a6b49ee1":"markdown","a7becd23":"markdown","77f95e03":"markdown","152b2b1b":"markdown","611db55f":"markdown","8be2f50f":"markdown","86e95712":"markdown","d697c3b0":"markdown","89dbb114":"markdown","e54045cf":"markdown","99c82c64":"markdown","04201674":"markdown","aa4a5c89":"markdown","d2dbac87":"markdown","87deb274":"markdown","42abc6fa":"markdown","6b3bc270":"markdown","ec793be6":"markdown","b4f495c5":"markdown","5b818981":"markdown","ee0140ee":"markdown","7ebada42":"markdown","7320b7a7":"markdown","5afb3782":"markdown","a3f5f1f7":"markdown","79994d37":"markdown","ee3ecab7":"markdown","a8725f87":"markdown","577eddf4":"markdown","a3d952da":"markdown","07e51889":"markdown","fd1a8038":"markdown","f4484b14":"markdown","66fa09ca":"markdown","5112e9eb":"markdown","e18642e9":"markdown","d86c6dfe":"markdown","e427c829":"markdown","b33cbced":"markdown","acdac87d":"markdown","8296ec7b":"markdown","a74a9977":"markdown","9a2e298b":"markdown","6028c050":"markdown"},"source":{"3603735e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy as sp\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Standard plotly imports\n#import plotly.plotly as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\n#import cufflinks\n#import cufflinks as cf\nimport plotly.figure_factory as ff\n\n# Using plotly + cufflinks in offline mode\ninit_notebook_mode(connected=True)\n#cufflinks.go_offline(connected=True)\n\n# Preprocessing, modelling and evaluating\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n\n## Hyperopt modules\nfrom hyperopt import fmin, hp, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\nfrom functools import partial\n\n\nimport os\nimport gc\nprint(os.listdir(\"..\/input\"))","7bd024c9":"pd.options.display.float_format = '{:,.3f}'.format","6df0cd26":"df_id = pd.read_csv(\"..\/input\/train_identity.csv\")\ndf_trans = pd.read_csv(\"..\/input\/train_transaction.csv\")","7305ead2":"df_trans.info()","b607bb01":"df_trans.iloc[:,0:50].info()","768ee027":"df_id.info()","ad055180":"df_id.iloc[:,0:10].describe()","ef90fed8":"df_trans.head()","f3a533be":"df_trans.info()","4a060703":"label = ['isFraud']\ntrans_id = ['TransactionID']\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n\nnumeric_col = [x for x in df_trans.select_dtypes(include=numerics).columns\n              if x not in label + trans_id ]\ncat_col = [x for x in df_trans.columns \n           if x not in numeric_col + label + trans_id]","cb62a237":"numeric_col","37f054c9":"len(numeric_col)","be5bd9d3":"cat_col","e50d3dab":"len(cat_col)","6827965b":"def resumetable(df):\n    '''\n    resumetable for numeric features\n    '''\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['Max'] = df.max().values\n    summary['Min'] = df.min().values\n    summary['Mean'] = df.mean().values\n    summary['Var'] = df.std().values\n    summary['Q1'] = df.quantile(0.25).values\n    summary['Q3'] = df.quantile(0.75).values    \n    \n    \n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n#     summary['Sample 1'] = df.sample(1).values\n#     summary['Sample 2'] = df.sample(1).values\n#     summary['Sample 3'] = df.sample(1).values\n    \n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n    summary.sort_values(by=['Name', 'dtypes'])\n    return summary\n\n\n## Function to reduce the DF size\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ndef CalcOutliers(df_num): \n\n    # calculating mean and std of the array\n    data_mean, data_std = np.mean(df_num), np.std(df_num)\n\n    # seting the cut line to both higher and lower values\n    # You can change this value\n    cut = data_std * 3\n\n    #Calculating the higher and lower cut values\n    lower, upper = data_mean - cut, data_mean + cut\n\n    # creating an array of lower, higher and total outlier values \n    outliers_lower = [x for x in df_num if x < lower]\n    outliers_higher = [x for x in df_num if x > upper]\n    outliers_total = [x for x in df_num if x < lower or x > upper]\n\n    # array without outlier values\n    outliers_removed = [x for x in df_num if x > lower and x < upper]\n    \n    print('Identified lowest outliers: %d' % len(outliers_lower)) # printing total number of values in lower cut of outliers\n    print('Identified upper outliers: %d' % len(outliers_higher)) # printing total number of values in higher cut of outliers\n    print('Total outlier observations: %d' % len(outliers_total)) # printing total number of values outliers of both sides\n    print('Non-outlier observations: %d' % len(outliers_removed)) # printing total number of non outlier values\n    print(\"Total percentual of Outliers: \", round((len(outliers_total) \/ len(outliers_removed) )*100, 4)) # Percentual of outliers in points\n    \n    return","1af73a67":"## REducing memory\ndf_trans = reduce_mem_usage(df_trans)\ndf_id = reduce_mem_usage(df_id)","3f69e885":"df_trans.iloc[:,:20].max()","9b9dcd8d":"resumetable(df_trans[numeric_col].iloc[:,0:20])","5fd5dbd3":"# This function assits to expand Exploratory Data Analysis (EDA) is an open-ended \n#process where we calculate statistics and make figures to find trends, anomalies,\n#patterns, or relationships within the data. The goal of EDA is to learn what our\n#data can tell us. It generally starts out with a high level overview, \n#then narrows in to specific areas as we find intriguing areas of the data. \n\ndef des_stat_analyze(df_input, get_sample=False):\n    # check number of rows, cols\n    no_rows = df_input.shape[0]\n    no_cols = df_input.shape[1]\n    print(\"No. observations:\", no_rows )\n    print(\"No. features:\", no_cols )\n  \n    # checking type of features\n    name = []\n    cols_type = []\n    for n,t in df_input.dtypes.iteritems():\n        name.append(n)\n        cols_type.append(t)\n\n    # checking distinction (unique values) of features\n    ls_unique = []\n    for cname in df_input.columns:\n        try:\n            nunique = df_input[cname].nunique()\n            pct_unique = nunique*100.0\/ no_rows\n            ls_unique.append(\"{} ({:0.2f}%)\".format(nunique, pct_unique))\n        except:\n            ls_unique.append(\"{} ({:0.2f}%)\".format(0,0))\n            continue\n\n    # checking missing values of features\n    ls_miss = []\n    for cname in df_input.columns:\n        try:\n            nmiss = df_input[cname].isnull().sum()\n            pct_miss = nmiss*100.0\/ no_rows\n            ls_miss.append(\"{} ({:0.2f}%)\".format(nmiss, pct_miss))\n        except:\n            ls_miss.append(\"{} ({:0.2f}%)\".format(0,0))\n            continue \n      \n    # checking zeros\n    ls_zeros = []\n    for cname in df_input.columns:\n        try:\n            nzeros = (df_input[cname] == 0).sum()\n            pct_zeros = nzeros * 100.0\/ no_rows\n            ls_zeros.append(\"{} ({:0.2f}%)\".fornat(nzeros, pct_zeros))\n        except:\n            ls_zeros.append(\"{} ({:0.2f}%)\".format(0,0))\n            continue\n      \n    # checking negative values\n    ls_neg = []\n    for cname in df_input.columns:\n        try:\n            nneg = (df_input[cname].astype(\"float\")<0).sum()\n            pct_neg =nneg * 100.0 \/ no_rows\n            ls_neg.append(\"{} ({:0.2f}%)\".format(nneg, pct_neg))\n        except:\n            ls_neg.append(\"{} ({:0.2f}%)\".format(0,0))\n            continue\n      \n    # extracting the output\n    data = {\n      \"name\": name,\n      \"col_type\": cols_type,\n      \"n_unique\": ls_unique,\n      \"n_miss\": ls_miss,\n      \"n_zeros\":ls_zeros,\n      \"n_neg\":ls_neg      \n    }\n  \n    # statistical info\n    df_stats = df_input.describe().transpose()\n    ex_stats = pd.concat([df_input.median(),df_input.kurtosis(),df_input.skew()], axis = 1)\n    ex_stats = ex_stats.rename(columns={0:\"median\",1:\"kurtosis\",2:\"skew\"})\n    df_stats = df_stats.merge(ex_stats, left_index=True, right_index=True)\n    #ls_stats = []\n    for stat in df_stats.columns:\n        data[stat] = []\n        for cname in df_input.columns:\n            try:\n                data[stat].append(df_stats.loc[cname, stat])\n            except:\n                data[stat].append(\"NaN\")\n        \n    # take samples\n    if get_sample:\n        df_sample = df_input.sample(frac = .5).head().transpose()\n        df_sample.columns = [\"sample_{}\".format(i) for i in range(5)]\n\n    \n    # repair the output\n    col_ordered = [\"name\",\"col_type\",\"count\",\"n_unique\",\"n_miss\",\"n_zeros\",\"n_neg\",\n                \"25%\",\"50%\",\"75%\",\"max\",\"min\",\"mean\",\"median\",\"std\",\"kurtosis\",\"skew\"]\n    df_data = pd.DataFrame(data, columns = col_ordered).set_index(\"name\")\n    if get_sample:\n        df_data = pd.concat([df_data, df_sample], axis = 1)\n    df_data = df_data.reset_index().sort_values(by=['col_type'])  \n    # entropy\n    for col in name:\n        #print(col)\n        df_data.loc[df_data['name'] == col, 'Entropy'] = round(stats.entropy(df_input[col].value_counts(normalize=True), base=2),2) \n    \n\n    return df_data","9bcb4dcf":"df_trans[numeric_col].iloc[:,0:20].isnull().sum()","5a96ffbc":"des_stat_analyze(df_trans[numeric_col].iloc[:,0:50])","4a29f403":"df_trans.isFraud.value_counts()","2fda7910":"df_trans.TransactionDT.describe()","ece07865":"col_name = 'card1'\n\ntotal_amt = df_trans.groupby(['isFraud']).agg({\n    col_name: ['mean','sum','size']\n}).reset_index()\ntotal_amt.columns = ['_'.join(col) for col in total_amt.columns]\n\ntotal_amt['per_amount'] = total_amt[col_name+'_sum']\/ total_amt[col_name+'_sum'].sum()\ntotal_amt['per_size'] = total_amt[col_name+'_size']\/ total_amt[col_name+'_size'].sum()\ntotal_amt","dd391185":"df_trans[cat_col].nunique()","1e9ff7f0":"col_name = 'TransactionAmt'\n\ntotal_amt = df_trans.groupby(['isFraud','ProductCD']).agg({\n    col_name: ['mean','sum','size']\n}).reset_index()\ntotal_amt.columns = ['_'.join(col) for col in total_amt.columns]\n\ntotal_amt['per_amount'] = total_amt[col_name+'_sum']\/ total_amt[col_name+'_sum'].sum()\ntotal_amt['per_size'] = total_amt[col_name+'_size']\/ total_amt[col_name+'_size'].sum()\ntotal_amt","ecd116c1":"total_amt = df_trans.groupby(['isFraud']).agg({\n    'TransactionAmt': ['mean','sum','size']\n}).reset_index()\ntotal_amt.columns = ['_'.join(col) for col in total_amt.columns]\n\ntotal_amt['per_amount'] = total_amt.TransactionAmt_sum\/ total_amt.TransactionAmt_sum.sum()\ntotal_amt['per_size'] = total_amt.TransactionAmt_size\/ total_amt.TransactionAmt_size.sum()\ntotal_amt","ed341c7c":"df_trans['TransactionAmt'] = df_trans['TransactionAmt'].astype(float)\ntotal = len(df_trans)\ntotal_amt = df_trans.groupby(['isFraud'])['TransactionAmt'].sum().sum()\nplt.figure(figsize=(16,6))\n\nplt.subplot(121)\ng = sns.countplot(x='isFraud', data=df_trans, )\ng.set_title(\"Fraud Transactions Distribution \\n# 0: No Fraud | 1: Fraud #\", fontsize=22)\ng.set_xlabel(\"Is fraud?\", fontsize=18)\ng.set_ylabel('Count', fontsize=18)\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\", fontsize=15) \n\nperc_amt = (df_trans.groupby(['isFraud'])['TransactionAmt'].sum())\nperc_amt = perc_amt.reset_index()\nplt.subplot(122)\ng1 = sns.barplot(x='isFraud', y='TransactionAmt',  dodge=True, data=perc_amt)\ng1.set_title(\"% Total Amount in Transaction Amt \\n# 0: No Fraud | 1: Fraud #\", fontsize=22)\ng1.set_xlabel(\"Is fraud?\", fontsize=18)\ng1.set_ylabel('Total Transaction Amount Scalar', fontsize=18)\nfor p in g1.patches:\n    height = p.get_height()\n    g1.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total_amt * 100),\n            ha=\"center\", fontsize=15) \n    \nplt.show()","2de9a8d5":"df_trans['TransactionAmt'].hist()","1a784f36":"df_trans['TransactionAmt'] = df_trans['TransactionAmt'].astype(float)\nprint(\"Transaction Amounts Quantiles:\")\nprint(df_trans['TransactionAmt'].quantile([.01, .025, .1, .25, .5, .75, .9, .975, .99]))","9c98d943":"df_trans.loc[df_trans.isFraud == 0,'TransactionAmt'].hist()","c4295d30":"df_trans.loc[df_trans.isFraud == 1,'TransactionAmt'].hist()","c013c862":"df_trans['TransactionAmt'] = df_trans['TransactionAmt'].astype(float)\nprint(\"Transaction Amounts Quantiles:\")\nprint(df_trans.loc[df_trans.isFraud == 1,'TransactionAmt'].quantile([.01, .025, .1, .25, .5, .75, .9, .975, .99]))\nprint(df_trans.loc[df_trans.isFraud == 0,'TransactionAmt'].quantile([.01, .025, .1, .25, .5, .75, .9, .975, .99]))","02166dd9":"df_trans['TransactionAmt'].describe()","46b20271":"bins = [0, 30, 60, 120, 500, 800, 1000]\nlabels = ['1', '2', '3', '4', '5', '6']\ndf_trans['TransactionAmt_bins'] = pd.cut(df_trans['TransactionAmt'], bins, labels=labels)","7f550b7b":"df_trans['TransactionAmt_bins'].value_counts()","cb3ce4f1":"df_trans.groupby(['TransactionAmt_bins']).agg({\n  'isFraud' : ['mean','sum','size']  \n    \n})","d66facc8":"df_trans['TransactionAmt'] = df_trans['TransactionAmt'].astype(float)\nprint(\"Transaction Amounts Quantiles:\")\nprint(df_trans['TransactionAmt'].quantile([.01, .025, .1, .25, .5, .75, .9, .975, .99]))","c9195beb":"plt.figure(figsize=(16,12))\nplt.suptitle('Transaction Values Distribution', fontsize=22)\nplt.subplot(221)\ng = sns.distplot(df_trans[df_trans['TransactionAmt'] <= 1000]['TransactionAmt'])\ng.set_title(\"Transaction Amount Distribuition <= 1000\", fontsize=18)\ng.set_xlabel(\"\")\ng.set_ylabel(\"Probability\", fontsize=15)\n\nplt.subplot(222)\ng1 = sns.distplot(np.log(df_trans['TransactionAmt']))\ng1.set_title(\"Transaction Amount (Log) Distribuition\", fontsize=18)\ng1.set_xlabel(\"\")\ng1.set_ylabel(\"Probability\", fontsize=15)\n\nplt.figure(figsize=(16,12))\n\n\nplt.subplot(212)\ng4 = plt.scatter(range(df_trans[df_trans['isFraud'] == 0].shape[0]),\n                 np.sort(df_trans[df_trans['isFraud'] == 0]['TransactionAmt'].values), \n                 label='NoFraud', alpha=.2)\ng4 = plt.scatter(range(df_trans[df_trans['isFraud'] == 1].shape[0]),\n                 np.sort(df_trans[df_trans['isFraud'] == 1]['TransactionAmt'].values), \n                 label='Fraud', alpha=.2)\ng4= plt.title(\"ECDF \\nFRAUD and NO FRAUD Transaction Amount Distribution\", fontsize=18)\ng4 = plt.xlabel(\"Index\")\ng4 = plt.ylabel(\"Amount Distribution\", fontsize=15)\ng4 = plt.legend()\n\nplt.figure(figsize=(16,12))\n\nplt.subplot(321)\ng = plt.scatter(range(df_trans[df_trans['isFraud'] == 1].shape[0]), \n                 np.sort(df_trans[df_trans['isFraud'] == 1]['TransactionAmt'].values), \n                label='isFraud', alpha=.4)\nplt.title(\"FRAUD - Transaction Amount ECDF\", fontsize=18)\nplt.xlabel(\"Index\")\nplt.ylabel(\"Amount Distribution\", fontsize=12)\n\nplt.subplot(322)\ng1 = plt.scatter(range(df_trans[df_trans['isFraud'] == 0].shape[0]),\n                 np.sort(df_trans[df_trans['isFraud'] == 0]['TransactionAmt'].values), \n                 label='NoFraud', alpha=.2)\ng1= plt.title(\"NO FRAUD - Transaction Amount ECDF\", fontsize=18)\ng1 = plt.xlabel(\"Index\")\ng1 = plt.ylabel(\"Amount Distribution\", fontsize=15)\n\nplt.suptitle('Individual ECDF Distribution', fontsize=22)\n\nplt.show()","e1676d10":"print(pd.concat([df_trans[df_trans['isFraud'] == 1]['TransactionAmt']\\\n                 .quantile([.01, .1, .25, .5, .75, .9, .99])\\\n                 .reset_index(), \n                 df_trans[df_trans['isFraud'] == 0]['TransactionAmt']\\\n                 .quantile([.01, .1, .25, .5, .75, .9, .99])\\\n                 .reset_index()],\n                axis=1, keys=['Fraud', \"No Fraud\"]))","578819da":"CalcOutliers(df_trans['TransactionAmt'])","e6d1ebe9":"tmp = pd.crosstab(df_trans['ProductCD'], df_trans['isFraud'], normalize='index') * 100\ntmp = tmp.reset_index()\ntmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\nplt.figure(figsize=(14,10))\nplt.suptitle('ProductCD Distributions', fontsize=22)\n\nplt.subplot(221)\ng = sns.countplot(x='ProductCD', data=df_trans)\n# plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\n\ng.set_title(\"ProductCD Distribution\", fontsize=19)\ng.set_xlabel(\"ProductCD Name\", fontsize=17)\ng.set_ylabel(\"Count\", fontsize=17)\ng.set_ylim(0,500000)\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\", fontsize=14) \n\nplt.subplot(222)\ng1 = sns.countplot(x='ProductCD', hue='isFraud', data=df_trans)\nplt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\ngt = g1.twinx()\ngt = sns.pointplot(x='ProductCD', y='Fraud', data=tmp, color='black', order=['W', 'H',\"C\", \"S\", \"R\"], legend=False)\ngt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n\ng1.set_title(\"Product CD by Target(isFraud)\", fontsize=19)\ng1.set_xlabel(\"ProductCD Name\", fontsize=17)\ng1.set_ylabel(\"Count\", fontsize=17)\n\nplt.subplot(212)\ng3 = sns.boxenplot(x='ProductCD', y='TransactionAmt', hue='isFraud', \n              data=df_trans[df_trans['TransactionAmt'] <= 2000] )\ng3.set_title(\"Transaction Amount Distribuition by ProductCD and Target\", fontsize=20)\ng3.set_xlabel(\"ProductCD Name\", fontsize=17)\ng3.set_ylabel(\"Transaction Values\", fontsize=17)\n\nplt.subplots_adjust(hspace = 0.6, top = 0.85)\n\nplt.show()","ee8d7ca9":"## Knowning the Card Features\n#resumetable(df_trans[['card1', 'card2', 'card3','card4', 'card5', 'card6']])\ndes_stat_analyze(df_trans[['card1', 'card2', 'card3','card4', 'card5', 'card6']])","bb5cee7b":"print(\"Card Features Quantiles: \")\nprint(df_trans[['card1', 'card2', 'card3', 'card5']].quantile([0.01, .025, .1, .25, .5, .75, .975, .99]))","2dd537f7":"\ndf_trans.loc[df_trans.card3.isin(df_trans.card3.value_counts()[df_trans.card3.value_counts() < 200].index), 'card3'] = \"Others\"\ndf_trans.loc[df_trans.card5.isin(df_trans.card5.value_counts()[df_trans.card5.value_counts() < 300].index), 'card5'] = \"Others\"","30d099fa":"tmp = pd.crosstab(df_trans['card3'], df_trans['isFraud'], normalize='index') * 100\ntmp = tmp.reset_index()\ntmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\ntmp2 = pd.crosstab(df_trans['card5'], df_trans['isFraud'], normalize='index') * 100\ntmp2 = tmp2.reset_index()\ntmp2.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\nplt.figure(figsize=(14,22))\n\nplt.subplot(411)\ng = sns.distplot(df_trans[df_trans['isFraud'] == 1]['card1'], label='Fraud')\ng = sns.distplot(df_trans[df_trans['isFraud'] == 0]['card1'], label='NoFraud')\ng.legend()\ng.set_title(\"Card 1 Values Distribution by Target\", fontsize=20)\ng.set_xlabel(\"Card 1 Values\", fontsize=18)\ng.set_ylabel(\"Probability\", fontsize=18)\n\nplt.subplot(412)\ng1 = sns.distplot(df_trans[df_trans['isFraud'] == 1]['card2'].dropna(), label='Fraud')\ng1 = sns.distplot(df_trans[df_trans['isFraud'] == 0]['card2'].dropna(), label='NoFraud')\ng1.legend()\ng1.set_title(\"Card 2 Values Distribution by Target\", fontsize=20)\ng1.set_xlabel(\"Card 2 Values\", fontsize=18)\ng1.set_ylabel(\"Probability\", fontsize=18)\n\nplt.subplot(413)\ng2 = sns.countplot(x='card3', data=df_trans, order=list(tmp.card3.values))\ng22 = g2.twinx()\ngg2 = sns.pointplot(x='card3', y='Fraud', data=tmp, \n                    color='black', order=list(tmp.card3.values))\ngg2.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\ng2.set_title(\"Card 3 Values Distribution and % of Transaction Frauds\", fontsize=20)\ng2.set_xlabel(\"Card 3 Values\", fontsize=18)\ng2.set_ylabel(\"Count\", fontsize=18)\nfor p in g2.patches:\n    height = p.get_height()\n    g2.text(p.get_x()+p.get_width()\/2.,\n            height + 25,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\") \n\nplt.subplot(414)\ng3 = sns.countplot(x='card5', data=df_trans, order=list(tmp2.card5.values))\ng3t = g3.twinx()\ng3t = sns.pointplot(x='card5', y='Fraud', data=tmp2, \n                    color='black', order=list(tmp2.card5.values))\ng3t.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\ng3.set_title(\"Card 5 Values Distribution and % of Transaction Frauds\", fontsize=20)\ng3.set_xticklabels(g3.get_xticklabels(),rotation=90)\ng3.set_xlabel(\"Card 5 Values\", fontsize=18)\ng3.set_ylabel(\"Count\", fontsize=18)\nfor p in g3.patches:\n    height = p.get_height()\n    g3.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\",fontsize=11) \n    \nplt.subplots_adjust(hspace = 0.6, top = 0.85)\n\n\nplt.show()","9fec9893":"tmp = pd.crosstab(df_trans['card4'], df_trans['isFraud'], normalize='index') * 100\ntmp = tmp.reset_index()\ntmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\nplt.figure(figsize=(14,10))\nplt.suptitle('Card 4 Distributions', fontsize=22)\n\nplt.subplot(221)\ng = sns.countplot(x='card4', data=df_trans)\n# plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\ng.set_title(\"Card4 Distribution\", fontsize=19)\ng.set_ylim(0,420000)\ng.set_xlabel(\"Card4 Category Names\", fontsize=17)\ng.set_ylabel(\"Count\", fontsize=17)\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\",fontsize=14) \n\n\nplt.subplot(222)\ng1 = sns.countplot(x='card4', hue='isFraud', data=df_trans)\nplt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\ngt = g1.twinx()\ngt = sns.pointplot(x='card4', y='Fraud', data=tmp, \n                   color='black', legend=False, \n                   order=['discover', 'mastercard', 'visa', 'american express'])\ngt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\ng1.set_title(\"Card4 by Target(isFraud)\", fontsize=19)\ng1.set_xlabel(\"Card4 Category Names\", fontsize=17)\ng1.set_ylabel(\"Count\", fontsize=17)\n\nplt.subplot(212)\ng3 = sns.boxenplot(x='card4', y='TransactionAmt', hue='isFraud', \n              data=df_trans[df_trans['TransactionAmt'] <= 2000] )\ng3.set_title(\"Card 4 Distribuition by ProductCD and Target\", fontsize=20)\ng3.set_xlabel(\"Card4 Category Names\", fontsize=17)\ng3.set_ylabel(\"Transaction Values\", fontsize=17)\n\nplt.subplots_adjust(hspace = 0.6, top = 0.85)\n\nplt.show()","a2b8ea90":"tmp = pd.crosstab(df_trans['card6'], df_trans['isFraud'], normalize='index') * 100\ntmp = tmp.reset_index()\ntmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\nplt.figure(figsize=(14,10))\nplt.suptitle('Card 6 Distributions', fontsize=22)\n\nplt.subplot(221)\ng = sns.countplot(x='card6', data=df_trans, order=list(tmp.card6.values))\n# plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\ng.set_title(\"Card6 Distribution\", fontsize=19)\ng.set_ylim(0,480000)\ng.set_xlabel(\"Card6 Category Names\", fontsize=17)\ng.set_ylabel(\"Count\", fontsize=17)\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\",fontsize=14) \n\nplt.subplot(222)\ng1 = sns.countplot(x='card6', hue='isFraud', data=df_trans, order=list(tmp.card6.values))\nplt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\ngt = g1.twinx()\ngt = sns.pointplot(x='card6', y='Fraud', data=tmp, order=list(tmp.card6.values),\n                   color='black', legend=False, )\ngt.set_ylim(0,20)\ngt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\ng1.set_title(\"Card6 by Target(isFraud)\", fontsize=19)\ng1.set_xlabel(\"Card6 Category Names\", fontsize=17)\ng1.set_ylabel(\"Count\", fontsize=17)\n\nplt.subplot(212)\ng3 = sns.boxenplot(x='card6', y='TransactionAmt', hue='isFraud', order=list(tmp.card6.values),\n              data=df_trans[df_trans['TransactionAmt'] <= 2000] )\ng3.set_title(\"Card 6 Distribuition by ProductCD and Target\", fontsize=20)\ng3.set_xlabel(\"Card6 Category Names\", fontsize=17)\ng3.set_ylabel(\"Transaction Values\", fontsize=17)\n\nplt.subplots_adjust(hspace = 0.6, top = 0.85)\n\nplt.show()","9fc8ca5b":"for col in ['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']:\n    df_trans[col] = df_trans[col].fillna(\"Miss\")\n    \ndef ploting_dist_ratio(df, col, lim=2000):\n    tmp = pd.crosstab(df[col], df['isFraud'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\n    plt.figure(figsize=(20,5))\n    plt.suptitle(f'{col} Distributions ', fontsize=22)\n\n    plt.subplot(121)\n    g = sns.countplot(x=col, data=df, order=list(tmp[col].values))\n    # plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\n    g.set_title(f\"{col} Distribution\\nCound and %Fraud by each category\", fontsize=18)\n    g.set_ylim(0,400000)\n    gt = g.twinx()\n    gt = sns.pointplot(x=col, y='Fraud', data=tmp, order=list(tmp[col].values),\n                       color='black', legend=False, )\n    gt.set_ylim(0,20)\n    gt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n    g.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g.set_ylabel(\"Count\", fontsize=17)\n    for p in gt.patches:\n        height = p.get_height()\n        gt.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total*100),\n                ha=\"center\",fontsize=14) \n        \n    perc_amt = (df_trans.groupby(['isFraud',col])['TransactionAmt'].sum() \/ total_amt * 100).unstack('isFraud')\n    perc_amt = perc_amt.reset_index()\n    perc_amt.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\n    plt.subplot(122)\n    g1 = sns.boxplot(x=col, y='TransactionAmt', hue='isFraud', \n                     data=df[df['TransactionAmt'] <= lim], order=list(tmp[col].values))\n    g1t = g1.twinx()\n    g1t = sns.pointplot(x=col, y='Fraud', data=perc_amt, order=list(tmp[col].values),\n                       color='black', legend=False, )\n    g1t.set_ylim(0,5)\n    g1t.set_ylabel(\"%Fraud Total Amount\", fontsize=16)\n    g1.set_title(f\"{col} by Transactions dist\", fontsize=18)\n    g1.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g1.set_ylabel(\"Transaction Amount(U$)\", fontsize=16)\n        \n    plt.subplots_adjust(hspace=.4, wspace = 0.35, top = 0.80)\n    \n    plt.show()\n","8348e653":"for col in ['M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9']:\n    ploting_dist_ratio(df_trans, col, lim=2500)","d923c290":"print(\"Card Features Quantiles: \")\nprint(df_trans[['addr1', 'addr2']].quantile([0.01, .025, .1, .25, .5, .75, .90,.975, .99]))","8fdca89e":"df_trans.loc[df_trans.addr1.isin(df_trans.addr1.value_counts()[df_trans.addr1.value_counts() <= 5000 ].index), 'addr1'] = \"Others\"\ndf_trans.loc[df_trans.addr2.isin(df_trans.addr2.value_counts()[df_trans.addr2.value_counts() <= 50 ].index), 'addr2'] = \"Others\"","d347ca99":" def ploting_cnt_amt(df, col, lim=2000):\n    tmp = pd.crosstab(df[col], df['isFraud'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n    \n    plt.figure(figsize=(16,14))    \n    plt.suptitle(f'{col} Distributions ', fontsize=24)\n    \n    plt.subplot(211)\n    g = sns.countplot( x=col,  data=df, order=list(tmp[col].values))\n    gt = g.twinx()\n    gt = sns.pointplot(x=col, y='Fraud', data=tmp, order=list(tmp[col].values),\n                       color='black', legend=False, )\n    gt.set_ylim(0,tmp['Fraud'].max()*1.1)\n    gt.set_ylabel(\"%Fraud Transactions\", fontsize=16)\n    g.set_title(f\"Most Frequent {col} values and % Fraud Transactions\", fontsize=20)\n    g.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g.set_ylabel(\"Count\", fontsize=17)\n    g.set_xticklabels(g.get_xticklabels(),rotation=45)\n    sizes = []\n    for p in g.patches:\n        height = p.get_height()\n        sizes.append(height)\n        g.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total*100),\n                ha=\"center\",fontsize=12) \n        \n    g.set_ylim(0,max(sizes)*1.15)\n    \n    #########################################################################\n    perc_amt = (df.groupby(['isFraud',col])['TransactionAmt'].sum() \\\n                \/ df.groupby([col])['TransactionAmt'].sum() * 100).unstack('isFraud')\n    perc_amt = perc_amt.reset_index()\n    perc_amt.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n    amt = df.groupby([col])['TransactionAmt'].sum().reset_index()\n    perc_amt = perc_amt.fillna(0)\n    plt.subplot(212)\n    g1 = sns.barplot(x=col, y='TransactionAmt', \n                       data=amt, \n                       order=list(tmp[col].values))\n    g1t = g1.twinx()\n    g1t = sns.pointplot(x=col, y='Fraud', data=perc_amt, \n                        order=list(tmp[col].values),\n                       color='black', legend=False, )\n    g1t.set_ylim(0,perc_amt['Fraud'].max()*1.1)\n    g1t.set_ylabel(\"%Fraud Total Amount\", fontsize=16)\n    g.set_xticklabels(g.get_xticklabels(),rotation=45)\n    g1.set_title(f\"{col} by Transactions Total + %of total and %Fraud Transactions\", fontsize=20)\n    g1.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g1.set_ylabel(\"Transaction Total Amount(U$)\", fontsize=16)\n    g1.set_xticklabels(g.get_xticklabels(),rotation=45)    \n    \n    for p in g1.patches:\n        height = p.get_height()\n        g1.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total_amt*100),\n                ha=\"center\",fontsize=12) \n        \n    plt.subplots_adjust(hspace=.4, top = 0.9)\n    plt.show()\n    \nploting_cnt_amt(df_trans, 'addr1')","c013b48b":"ploting_cnt_amt(df_trans, 'addr2')","ae098d91":"df_trans.loc[df_trans['P_emaildomain'].isin(['gmail.com', 'gmail']),'P_emaildomain'] = 'Google'\n\ndf_trans.loc[df_trans['P_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk',\n                                         'yahoo.co.jp', 'yahoo.de', 'yahoo.fr',\n                                         'yahoo.es']), 'P_emaildomain'] = 'Yahoo Mail'\ndf_trans.loc[df_trans['P_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', \n                                         'hotmail.es','hotmail.co.uk', 'hotmail.de',\n                                         'outlook.es', 'live.com', 'live.fr',\n                                         'hotmail.fr']), 'P_emaildomain'] = 'Microsoft'\ndf_trans.loc[df_trans.P_emaildomain.isin(df_trans.P_emaildomain\\\n                                         .value_counts()[df_trans.P_emaildomain.value_counts() <= 500 ]\\\n                                         .index), 'P_emaildomain'] = \"Others\"\ndf_trans.P_emaildomain.fillna(\"NoInf\", inplace=True)","50259379":"ploting_cnt_amt(df_trans, 'P_emaildomain')","39791295":"df_trans.loc[df_trans['R_emaildomain'].isin(['gmail.com', 'gmail']),'R_emaildomain'] = 'Google'\n\ndf_trans.loc[df_trans['R_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk',\n                                             'yahoo.co.jp', 'yahoo.de', 'yahoo.fr',\n                                             'yahoo.es']), 'R_emaildomain'] = 'Yahoo Mail'\ndf_trans.loc[df_trans['R_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', \n                                             'hotmail.es','hotmail.co.uk', 'hotmail.de',\n                                             'outlook.es', 'live.com', 'live.fr',\n                                             'hotmail.fr']), 'R_emaildomain'] = 'Microsoft'\ndf_trans.loc[df_trans.R_emaildomain.isin(df_trans.R_emaildomain\\\n                                         .value_counts()[df_trans.R_emaildomain.value_counts() <= 300 ]\\\n                                         .index), 'R_emaildomain'] = \"Others\"\ndf_trans.R_emaildomain.fillna(\"NoInf\", inplace=True)","9d2ce714":"ploting_cnt_amt(df_trans, 'R_emaildomain')","ed4dc823":"resumetable(df_trans[['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8',\n                      'C9', 'C10', 'C11', 'C12', 'C13', 'C14']])","305a6577":"df_trans[['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8',\n                      'C9', 'C10', 'C11', 'C12', 'C13', 'C14']].describe()","d41c7895":"df_trans.loc[df_trans.C1.isin(df_trans.C1\\\n                              .value_counts()[df_trans.C1.value_counts() <= 400 ]\\\n                              .index), 'C1'] = \"Others\"","59009e61":"ploting_cnt_amt(df_trans, 'C1')","fdcc9dc4":"df_trans.loc[df_trans.C2.isin(df_trans.C2\\\n                              .value_counts()[df_trans.C2.value_counts() <= 350 ]\\\n                              .index), 'C2'] = \"Others\"","c29e0582":"ploting_cnt_amt(df_trans, 'C2')","ef8d76f1":"# https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100400#latest-579480\nimport datetime\n\nSTART_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\ndf_trans[\"Date\"] = df_trans['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n\ndf_trans['_Weekdays'] = df_trans['Date'].dt.dayofweek\ndf_trans['_Hours'] = df_trans['Date'].dt.hour\ndf_trans['_Days'] = df_trans['Date'].dt.day","7a90c0da":"ploting_cnt_amt(df_trans, '_Days')","aaf346dc":"ploting_cnt_amt(df_trans, '_Weekdays')","7dee8164":"ploting_cnt_amt(df_trans, '_Hours')","f21ed0d0":"# Calling the function to transform the date column in datetime pandas object\n\n#seting some static color options\ncolor_op = ['#5527A0', '#BB93D7', '#834CF7', '#6C941E', '#93EAEA', '#7425FF', '#F2098A', '#7E87AC', \n            '#EBE36F', '#7FD394', '#49C35D', '#3058EE', '#44FDCF', '#A38F85', '#C4CEE0', '#B63A05', \n            '#4856BF', '#F0DB1B', '#9FDBD9', '#B123AC']\n\n\ndates_temp = df_trans.groupby(df_trans.Date.dt.date)['TransactionAmt'].count().reset_index()\n# renaming the columns to apropriate names\n\n# creating the first trace with the necessary parameters\ntrace = go.Scatter(x=dates_temp['Date'], y=dates_temp.TransactionAmt,\n                    opacity = 0.8, line = dict(color = color_op[7]), name= 'Total Transactions')\n\n# Below we will get the total amount sold\ndates_temp_sum = df_trans.groupby(df_trans.Date.dt.date)['TransactionAmt'].sum().reset_index()\n\n# using the new dates_temp_sum we will create the second trace\ntrace1 = go.Scatter(x=dates_temp_sum.Date, line = dict(color = color_op[1]), name=\"Total Amount\",\n                        y=dates_temp_sum['TransactionAmt'], opacity = 0.8, yaxis='y2')\n\n#creating the layout the will allow us to give an title and \n# give us some interesting options to handle with the outputs of graphs\nlayout = dict(\n    title= \"Total Transactions and Fraud Informations by Date\",\n    xaxis=dict(\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1, label='1m', step='month', stepmode='backward'),\n                dict(count=3, label='3m', step='month', stepmode='backward'),\n                dict(count=6, label='6m', step='month', stepmode='backward'),\n                dict(step='all')\n            ])\n        ),\n        rangeslider=dict(visible = True),\n        type='date' ),\n    yaxis=dict(title='Total Transactions'),\n    yaxis2=dict(overlaying='y',\n                anchor='x', side='right',\n                zeroline=False, showgrid=False,\n                title='Total Transaction Amount')\n)\n\n# creating figure with the both traces and layout\nfig = dict(data= [trace, trace1,], layout=layout)\n\n#rendering the graphs\niplot(fig) #it's an equivalent to plt.show()","6db4ce15":"# Calling the function to transform the date column in datetime pandas object\n\n#seting some static color options\ncolor_op = ['#5527A0', '#BB93D7', '#834CF7', '#6C941E', '#93EAEA', '#7425FF', '#F2098A', '#7E87AC', \n            '#EBE36F', '#7FD394', '#49C35D', '#3058EE', '#44FDCF', '#A38F85', '#C4CEE0', '#B63A05', \n            '#4856BF', '#F0DB1B', '#9FDBD9', '#B123AC']\n\ntmp_amt = df_trans.groupby([df_trans.Date.dt.date, 'isFraud'])['TransactionAmt'].sum().reset_index()\ntmp_trans = df_trans.groupby([df_trans.Date.dt.date, 'isFraud'])['TransactionAmt'].count().reset_index()\n\ntmp_trans_fraud = tmp_trans[tmp_trans['isFraud'] == 1]\ntmp_amt_fraud = tmp_amt[tmp_amt['isFraud'] == 1]\n\ndates_temp = df_trans.groupby(df_trans.Date.dt.date)['TransactionAmt'].count().reset_index()\n# renaming the columns to apropriate names\n\n# creating the first trace with the necessary parameters\ntrace = go.Scatter(x=tmp_trans_fraud['Date'], y=tmp_trans_fraud.TransactionAmt,\n                    opacity = 0.8, line = dict(color = color_op[1]), name= 'Fraud Transactions')\n\n# using the new dates_temp_sum we will create the second trace\ntrace1 = go.Scatter(x=tmp_amt_fraud.Date, line = dict(color = color_op[7]), name=\"Fraud Amount\",\n                    y=tmp_amt_fraud['TransactionAmt'], opacity = 0.8, yaxis='y2')\n\n#creating the layout the will allow us to give an title and \n# give us some interesting options to handle with the outputs of graphs\nlayout = dict(\n    title= \"FRAUD TRANSACTIONS - Total Transactions and Fraud Informations by Date\",\n    xaxis=dict(\n        rangeselector=dict(\n            buttons=list([\n                dict(count=1, label='1m', step='month', stepmode='backward'),\n                dict(count=3, label='3m', step='month', stepmode='backward'),\n                dict(count=6, label='6m', step='month', stepmode='backward'),\n                dict(step='all')\n            ])\n        ),\n        rangeslider=dict(visible = True),\n        type='date' ),\n    yaxis=dict(title='Total Transactions'),\n    yaxis2=dict(overlaying='y',\n                anchor='x', side='right',\n                zeroline=False, showgrid=False,\n                title='Total Transaction Amount')\n)\n\n# creating figure with the both traces and layout\nfig = dict(data= [trace, trace1], layout=layout)\n\n#rendering the graphs\niplot(fig) #it's an equivalent to plt.show()","cbf94745":"df_id[['id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18',\n       'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25',\n       'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32',\n       'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38']].describe(include='all')","b9d2ed6b":"df_train = df_trans.merge(df_id, how='left', left_index=True, right_index=True)","df8f99a8":"def cat_feat_ploting(df, col):\n    tmp = pd.crosstab(df[col], df['isFraud'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n\n    plt.figure(figsize=(14,10))\n    plt.suptitle(f'{col} Distributions', fontsize=22)\n\n    plt.subplot(221)\n    g = sns.countplot(x=col, data=df, order=tmp[col].values)\n    # plt.legend(title='Fraud', loc='upper center', labels=['No', 'Yes'])\n\n    g.set_title(f\"{col} Distribution\", fontsize=19)\n    g.set_xlabel(f\"{col} Name\", fontsize=17)\n    g.set_ylabel(\"Count\", fontsize=17)\n    # g.set_ylim(0,500000)\n    for p in g.patches:\n        height = p.get_height()\n        g.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total*100),\n                ha=\"center\", fontsize=14) \n\n    plt.subplot(222)\n    g1 = sns.countplot(x=col, hue='isFraud', data=df, order=tmp[col].values)\n    plt.legend(title='Fraud', loc='best', labels=['No', 'Yes'])\n    gt = g1.twinx()\n    gt = sns.pointplot(x=col, y='Fraud', data=tmp, color='black', order=tmp[col].values, legend=False)\n    gt.set_ylabel(\"% of Fraud Transactions\", fontsize=16)\n\n    g1.set_title(f\"{col} by Target(isFraud)\", fontsize=19)\n    g1.set_xlabel(f\"{col} Name\", fontsize=17)\n    g1.set_ylabel(\"Count\", fontsize=17)\n\n    plt.subplot(212)\n    g3 = sns.boxenplot(x=col, y='TransactionAmt', hue='isFraud', \n                       data=df[df['TransactionAmt'] <= 2000], order=tmp[col].values )\n    g3.set_title(\"Transaction Amount Distribuition by ProductCD and Target\", fontsize=20)\n    g3.set_xlabel(\"ProductCD Name\", fontsize=17)\n    g3.set_ylabel(\"Transaction Values\", fontsize=17)\n\n    plt.subplots_adjust(hspace = 0.4, top = 0.85)\n\n    plt.show()","632d90f3":"for col in ['id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29']:\n    df_train[col] = df_train[col].fillna('NaN')\n    cat_feat_ploting(df_train, col)","84c6ffc4":"df_train.loc[df_train['id_30'].str.contains('Windows', na=False), 'id_30'] = 'Windows'\ndf_train.loc[df_train['id_30'].str.contains('iOS', na=False), 'id_30'] = 'iOS'\ndf_train.loc[df_train['id_30'].str.contains('Mac OS', na=False), 'id_30'] = 'Mac'\ndf_train.loc[df_train['id_30'].str.contains('Android', na=False), 'id_30'] = 'Android'\ndf_train['id_30'].fillna(\"NAN\", inplace=True)","f1e59c94":"ploting_cnt_amt(df_train, 'id_30')","765b8a26":"df_train.loc[df_train['id_31'].str.contains('chrome', na=False), 'id_31'] = 'Chrome'\ndf_train.loc[df_train['id_31'].str.contains('firefox', na=False), 'id_31'] = 'Firefox'\ndf_train.loc[df_train['id_31'].str.contains('safari', na=False), 'id_31'] = 'Safari'\ndf_train.loc[df_train['id_31'].str.contains('edge', na=False), 'id_31'] = 'Edge'\ndf_train.loc[df_train['id_31'].str.contains('ie', na=False), 'id_31'] = 'IE'\ndf_train.loc[df_train['id_31'].str.contains('samsung', na=False), 'id_31'] = 'Samsung'\ndf_train.loc[df_train['id_31'].str.contains('opera', na=False), 'id_31'] = 'Opera'\ndf_train['id_31'].fillna(\"NAN\", inplace=True)\ndf_train.loc[df_train.id_31.isin(df_train.id_31.value_counts()[df_train.id_31.value_counts() < 200].index), 'id_31'] = \"Others\"","8bd72cf9":"ploting_cnt_amt(df_train, 'id_31')","d5c2ad28":"df_trans = pd.read_csv('..\/input\/train_transaction.csv')\ndf_test_trans = pd.read_csv('..\/input\/test_transaction.csv')\n\ndf_id = pd.read_csv('..\/input\/train_identity.csv')\ndf_test_id = pd.read_csv('..\/input\/test_identity.csv')\n\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='TransactionID')\n\ndf_train = df_trans.merge(df_id, how='left', left_index=True, right_index=True, on='TransactionID')\ndf_test = df_test_trans.merge(df_test_id, how='left', left_index=True, right_index=True, on='TransactionID')\n\nprint(df_train.shape)\nprint(df_test.shape)\n\n# y_train = df_train['isFraud'].copy()\ndel df_trans, df_id, df_test_trans, df_test_id\n","95258a6e":"df_train = reduce_mem_usage(df_train)\ndf_test = reduce_mem_usage(df_test)","c0efc765":"\nemails = {'gmail': 'google', 'att.net': 'att', 'twc.com': 'spectrum', \n          'scranton.edu': 'other', 'optonline.net': 'other', 'hotmail.co.uk': 'microsoft',\n          'comcast.net': 'other', 'yahoo.com.mx': 'yahoo', 'yahoo.fr': 'yahoo',\n          'yahoo.es': 'yahoo', 'charter.net': 'spectrum', 'live.com': 'microsoft', \n          'aim.com': 'aol', 'hotmail.de': 'microsoft', 'centurylink.net': 'centurylink',\n          'gmail.com': 'google', 'me.com': 'apple', 'earthlink.net': 'other', 'gmx.de': 'other',\n          'web.de': 'other', 'cfl.rr.com': 'other', 'hotmail.com': 'microsoft', \n          'protonmail.com': 'other', 'hotmail.fr': 'microsoft', 'windstream.net': 'other', \n          'outlook.es': 'microsoft', 'yahoo.co.jp': 'yahoo', 'yahoo.de': 'yahoo',\n          'servicios-ta.com': 'other', 'netzero.net': 'other', 'suddenlink.net': 'other',\n          'roadrunner.com': 'other', 'sc.rr.com': 'other', 'live.fr': 'microsoft',\n          'verizon.net': 'yahoo', 'msn.com': 'microsoft', 'q.com': 'centurylink', \n          'prodigy.net.mx': 'att', 'frontier.com': 'yahoo', 'anonymous.com': 'other', \n          'rocketmail.com': 'yahoo', 'sbcglobal.net': 'att', 'frontiernet.net': 'yahoo', \n          'ymail.com': 'yahoo', 'outlook.com': 'microsoft', 'mail.com': 'other', \n          'bellsouth.net': 'other', 'embarqmail.com': 'centurylink', 'cableone.net': 'other', \n          'hotmail.es': 'microsoft', 'mac.com': 'apple', 'yahoo.co.uk': 'yahoo', 'netzero.com': 'other', \n          'yahoo.com': 'yahoo', 'live.com.mx': 'microsoft', 'ptd.net': 'other', 'cox.net': 'other',\n          'aol.com': 'aol', 'juno.com': 'other', 'icloud.com': 'apple'}\n\nus_emails = ['gmail', 'net', 'edu']\n\n# https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/100499#latest-579654\nfor c in ['P_emaildomain', 'R_emaildomain']:\n    df_train[c + '_bin'] = df_train[c].map(emails)\n    df_test[c + '_bin'] = df_test[c].map(emails)\n    \n    df_train[c + '_suffix'] = df_train[c].map(lambda x: str(x).split('.')[-1])\n    df_test[c + '_suffix'] = df_test[c].map(lambda x: str(x).split('.')[-1])\n    \n    df_train[c + '_suffix'] = df_train[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')\n    df_test[c + '_suffix'] = df_test[c + '_suffix'].map(lambda x: x if str(x) not in us_emails else 'us')","be5e693c":"# Label Encoding\nfor f in df_train.drop('isFraud', axis=1).columns:\n    if df_train[f].dtype=='object' or df_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df_train[f].values) + list(df_test[f].values))\n        df_train[f] = lbl.transform(list(df_train[f].values))\n        df_test[f] = lbl.transform(list(df_test[f].values))   ","0f7fd360":"df_train['Trans_min_mean'] = df_train['TransactionAmt'] - df_train['TransactionAmt'].mean()\ndf_train['Trans_min_std'] = df_train['Trans_min_mean'] \/ df_train['TransactionAmt'].std()\ndf_test['Trans_min_mean'] = df_test['TransactionAmt'] - df_test['TransactionAmt'].mean()\ndf_test['Trans_min_std'] = df_test['Trans_min_mean'] \/ df_test['TransactionAmt'].std()","eef2e8f2":"df_train['TransactionAmt_to_mean_card1'] = df_train['TransactionAmt'] \/ df_train.groupby(['card1'])['TransactionAmt'].transform('mean')\ndf_train['TransactionAmt_to_mean_card4'] = df_train['TransactionAmt'] \/ df_train.groupby(['card4'])['TransactionAmt'].transform('mean')\ndf_train['TransactionAmt_to_std_card1'] = df_train['TransactionAmt'] \/ df_train.groupby(['card1'])['TransactionAmt'].transform('std')\ndf_train['TransactionAmt_to_std_card4'] = df_train['TransactionAmt'] \/ df_train.groupby(['card4'])['TransactionAmt'].transform('std')\n\ndf_test['TransactionAmt_to_mean_card1'] = df_test['TransactionAmt'] \/ df_test.groupby(['card1'])['TransactionAmt'].transform('mean')\ndf_test['TransactionAmt_to_mean_card4'] = df_test['TransactionAmt'] \/ df_test.groupby(['card4'])['TransactionAmt'].transform('mean')\ndf_test['TransactionAmt_to_std_card1'] = df_test['TransactionAmt'] \/ df_test.groupby(['card1'])['TransactionAmt'].transform('std')\ndf_test['TransactionAmt_to_std_card4'] = df_test['TransactionAmt'] \/ df_test.groupby(['card4'])['TransactionAmt'].transform('std')","eb39a5bd":"df_train['TransactionAmt'] = np.log(df_train['TransactionAmt'])\ndf_test['TransactionAmt'] = np.log(df_test['TransactionAmt'])","2a2ceea5":"df_test['isFraud'] = 'test'\ndf = pd.concat([df_train, df_test], axis=0, sort=False )\ndf = df.reset_index()\ndf = df.drop('index', axis=1)","5ac8377b":"def PCA_change(df, cols, n_components, prefix='PCA_', rand_seed=4):\n    pca = PCA(n_components=n_components, random_state=rand_seed)\n\n    principalComponents = pca.fit_transform(df[cols])\n\n    principalDf = pd.DataFrame(principalComponents)\n\n    df.drop(cols, axis=1, inplace=True)\n\n    principalDf.rename(columns=lambda x: str(prefix)+str(x), inplace=True)\n\n    df = pd.concat([df, principalDf], axis=1)\n    \n    return df","7a4457be":"mas_v = df_train.columns[55:394]","d980a895":"from sklearn.preprocessing import minmax_scale\nfrom sklearn.decomposition import PCA\n# from sklearn.cluster import KMeans\n\nfor col in mas_v:\n    df[col] = df[col].fillna((df[col].min() - 2))\n    df[col] = (minmax_scale(df[col], feature_range=(0,1)))\n\n    \ndf = PCA_change(df, mas_v, prefix='PCA_V_', n_components=30)","5d1debf2":"df = reduce_mem_usage(df)","f453f176":"df_train, df_test = df[df['isFraud'] != 'test'], df[df['isFraud'] == 'test'].drop('isFraud', axis=1)","c9f7a43b":"df_train.shape","1f8738cf":"X_train = df_train.sort_values('TransactionDT').drop(['isFraud', \n                                                      'TransactionDT', \n                                                      #'Card_ID'\n                                                     ],\n                                                     axis=1)\ny_train = df_train.sort_values('TransactionDT')['isFraud'].astype(bool)\n\nX_test = df_test.sort_values('TransactionDT').drop(['TransactionDT',\n                                                    #'Card_ID'\n                                                   ], \n                                                   axis=1)\ndel df_train\ndf_test = df_test[[\"TransactionDT\"]]","e3a5499d":"from sklearn.model_selection import KFold,TimeSeriesSplit\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import plot_importance\nfrom sklearn.metrics import make_scorer\n\nimport time\ndef objective(params):\n    time1 = time.time()\n    params = {\n        'max_depth': int(params['max_depth']),\n        'gamma': \"{:.3f}\".format(params['gamma']),\n        'subsample': \"{:.2f}\".format(params['subsample']),\n        'reg_alpha': \"{:.3f}\".format(params['reg_alpha']),\n        'reg_lambda': \"{:.3f}\".format(params['reg_lambda']),\n        'learning_rate': \"{:.3f}\".format(params['learning_rate']),\n        'num_leaves': '{:.3f}'.format(params['num_leaves']),\n        'colsample_bytree': '{:.3f}'.format(params['colsample_bytree']),\n        'min_child_samples': '{:.3f}'.format(params['min_child_samples']),\n        'feature_fraction': '{:.3f}'.format(params['feature_fraction']),\n        'bagging_fraction': '{:.3f}'.format(params['bagging_fraction'])\n    }\n\n    print(\"\\n############## New Run ################\")\n    print(f\"params = {params}\")\n    FOLDS = 7\n    count=1\n    skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n\n    tss = TimeSeriesSplit(n_splits=FOLDS)\n    y_preds = np.zeros(sample_submission.shape[0])\n    y_oof = np.zeros(X_train.shape[0])\n    score_mean = 0\n    for tr_idx, val_idx in tss.split(X_train, y_train):\n        clf = xgb.XGBClassifier(\n            n_estimators=600, random_state=4, verbose=True, \n            tree_method='gpu_hist', \n            **params\n        )\n\n        X_tr, X_vl = X_train.iloc[tr_idx, :], X_train.iloc[val_idx, :]\n        y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n        \n        clf.fit(X_tr, y_tr)\n        #y_pred_train = clf.predict_proba(X_vl)[:,1]\n        #print(y_pred_train)\n        score = make_scorer(roc_auc_score, needs_proba=True)(clf, X_vl, y_vl)\n        # plt.show()\n        score_mean += score\n        print(f'{count} CV - score: {round(score, 4)}')\n        count += 1\n    time2 = time.time() - time1\n    print(f\"Total Time Run: {round(time2 \/ 60,2)}\")\n    gc.collect()\n    print(f'Mean ROC_AUC: {score_mean \/ FOLDS}')\n    del X_tr, X_vl, y_tr, y_vl, clf, score\n    return -(score_mean \/ FOLDS)\n\n\nspace = {\n    # The maximum depth of a tree, same as GBM.\n    # Used to control over-fitting as higher depth will allow model \n    # to learn relations very specific to a particular sample.\n    # Should be tuned using CV.\n    # Typical values: 3-10\n    'max_depth': hp.quniform('max_depth', 7, 23, 1),\n    \n    # reg_alpha: L1 regularization term. L1 regularization encourages sparsity \n    # (meaning pulling weights to 0). It can be more useful when the objective\n    # is logistic regression since you might need help with feature selection.\n    'reg_alpha':  hp.uniform('reg_alpha', 0.01, 0.4),\n    \n    # reg_lambda: L2 regularization term. L2 encourages smaller weights, this\n    # approach can be more useful in tree-models where zeroing \n    # features might not make much sense.\n    'reg_lambda': hp.uniform('reg_lambda', 0.01, .4),\n    \n    # eta: Analogous to learning rate in GBM\n    # Makes the model more robust by shrinking the weights on each step\n    # Typical final values to be used: 0.01-0.2\n    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n    \n    # colsample_bytree: Similar to max_features in GBM. Denotes the \n    # fraction of columns to be randomly samples for each tree.\n    # Typical values: 0.5-1\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.3, .9),\n    \n    # A node is split only when the resulting split gives a positive\n    # reduction in the loss function. Gamma specifies the \n    # minimum loss reduction required to make a split.\n    # Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n    'gamma': hp.uniform('gamma', 0.01, .7),\n    \n    # more increases accuracy, but may lead to overfitting.\n    # num_leaves: the number of leaf nodes to use. Having a large number \n    # of leaves will improve accuracy, but will also lead to overfitting.\n    'num_leaves': hp.choice('num_leaves', list(range(20, 250, 10))),\n    \n    # specifies the minimum samples per leaf node.\n    # the minimum number of samples (data) to group into a leaf. \n    # The parameter can greatly assist with overfitting: larger sample\n    # sizes per leaf will reduce overfitting (but may lead to under-fitting).\n    'min_child_samples': hp.choice('min_child_samples', list(range(100, 250, 10))),\n    \n    # subsample: represents a fraction of the rows (observations) to be \n    # considered when building each subtree. Tianqi Chen and Carlos Guestrin\n    # in their paper A Scalable Tree Boosting System recommend \n    'subsample': hp.choice('subsample', [0.2, 0.4, 0.5, 0.6, 0.7, .8, .9]),\n    \n    # randomly select a fraction of the features.\n    # feature_fraction: controls the subsampling of features used\n    # for training (as opposed to subsampling the actual training data in \n    # the case of bagging). Smaller fractions reduce overfitting.\n    'feature_fraction': hp.uniform('feature_fraction', 0.4, .8),\n    \n    # randomly bag or subsample training data.\n    'bagging_fraction': hp.uniform('bagging_fraction', 0.4, .9)\n    \n    # bagging_fraction and bagging_freq: enables bagging (subsampling) \n    # of the training data. Both values need to be set for bagging to be used.\n    # The frequency controls how often (iteration) bagging is used. Smaller\n    # fractions and frequencies reduce overfitting.\n}\n","15f92d20":"# # Set algoritm parameters\n# best = fmin(fn=objective,\n#             space=space,\n#             algo=tpe.suggest,\n#             max_evals=27)\n\n# # Print best parameters\n# best_params = space_eval(space, best)","0298ffc6":"# print(\"BEST PARAMS: \", best_params)\n\n# best_params['max_depth'] = int(best_params['max_depth'])","c5ff29c6":"best_params = {'bagging_fraction': 0.8993155305338455, 'colsample_bytree': 0.7463058454739352, \n               'feature_fraction': 0.7989765808988153, 'gamma': 0.6665437467229817, 'learning_rate': 0.013887824598276186, \n               'max_depth': 16.0, 'min_child_samples': 170, 'num_leaves': 220, \n               'reg_alpha': 0.39871702770778467, 'reg_lambda': 0.24309304355829786, 'subsample': 0.7}","838f5112":"clf = xgb.XGBClassifier(\n    n_estimators=100,\n    **best_params,\n    #tree_method='gpu_hist'\n)\n\nclf.fit(X_train, y_train)\n\ny_preds = clf.predict_proba(X_test)[:,1] ","3bc45026":"feature_important = clf.get_booster().get_score(importance_type=\"weight\")\nkeys = list(feature_important.keys())\nvalues = list(feature_important.values())\n\ndata = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)\n\n# Top 10 features\ndata.head(20)","8f69baea":"\nsample_submission['isFraud'] = y_preds\nsample_submission.to_csv('XGB_hypopt_model.csv')","3fa43871":"## Ploting WeekDays Distributions","cecd49fa":"# Addr1 and Addr2","6817e9e0":"# Numericals Feature Card Quantiles","769a09ea":"To start simple, I will start using as base the kernels below: <br>\nhttps:\/\/www.kaggle.com\/artkulak\/ieee-fraud-simple-baseline-0-9383-lb - (@artkulak - Art) <br>\nhttps:\/\/www.kaggle.com\/artgor\/eda-and-models - (@artgor - Andrew Lukyanenko)\n","ddd7b2d9":"# Best parameters","14d08b1e":"W, C and R are the most frequent values. <br>\nWe can note that in W, H and R the distribution of Fraud values are slightly higher than the Non-Fraud Transactions","dfcd7bab":"# Concating dfs to get PCA of V features","bddb8fcf":"## Sources:\nhttps:\/\/www.kaggle.com\/kabure\/extensive-eda-and-modeling-xgb-hyperopt\nhttps:\/\/www.kaggle.com\/kabure\/almost-complete-feature-engineering-ieee-data\nhttps:\/\/www.kaggle.com\/kabure\/baseline-fraud-detection-eda-interactive-views?scriptVersionId=17308287\n\n\nhttps:\/\/www.kaggle.com\/artgor\/eda-and-models","1a2ef81d":"We can see that 97% of our data are in Mastercard(32%) and Visa(65%);  <br>\nwe have a highest value in discover(~8%) against ~3.5% of Mastercard and Visa and 2.87% in American Express","3f19780b":"I will set all functions in the cell bellow.","20b9ad81":"# Importing train datasets","13a74d9f":"# Mapping emails","749aa33b":"We can note interesting patterns on Addr1.","04a41895":"We have 3.5% of Fraud transactions in our dataset. <br>I think that it would be interesting to see if the amount percentual is higher or lower than 3.5% of total. I will see it later. <br>\nWe have the same % when considering the Total Transactions Amount by Fraud and No Fraud. <br>\nLet's explore the Transaction amount further below.","1af5d15c":"## Ploting P-Email Domain","43286d8e":"## C1 Distribution Plot","adee4f74":"# Exploring M1-M9 Features ","0258041a":"## Importing necessary libraries","3df50ec0":"## Addr1 Distributions","2f508ec2":"## Predicting X test","42f12aff":"# Modelling \n\n","75ecc3db":"We can see a very similar distribution in both email domain features. <br>\nIt's interesting that we have high values in google and icloud frauds","b0f4e72e":"# Encoding categorical features","53e295e7":"# Trainning and Predicting with best Parameters","d8ea9903":"# Target Distribution","41897cc4":"# Card 4 - Categorical","77c393b2":"# Features [id_12 to id_38]\n- categorical features in training identity dataset","2c8d66e1":"# P emaildomain Distributions\n- I will group all e-mail domains by the respective enterprises.\n- Also, I will set as \"Others\" all values with less than 500 entries.","4526d00a":"# Knowing the data","a6b49ee1":"Card2-Card6 has some missing values. We will need to due with it later.","a7becd23":"Before Ploting the Transaction Amount, let's see the quantiles of Transaction Amount","77f95e03":"Very cool!!! This graphs give us many interesting intuition about the M features.<br>\nOnly in M4 the Missing values haven't the highest % of Fraud.\n\n","152b2b1b":"## R-Email Domain plot distribution\n- I will group all e-mail domains by the respective enterprises.\n- I will set as \"Others\" all values with less than 300 entries.","611db55f":"# Transaction Amount Outliers\n- It's considering outlier values that are highest than 3 times the std from the mean","8be2f50f":"# reducing memory usage","86e95712":"We can see that Card 1 and Card 2 has a large distribution of values, so maybe it will be better to get the log of these columns","d697c3b0":"# Card Features\n- Based on Competition Description, card features are categoricals.\n- Lets understand the distribution of values\n- What's the different in transactions and % of Fraud for each values in these features\n- Card features has 6 columns, and 4 of them seems to be numericals, so lets see the quantiles and distributions","89dbb114":"# Visualizing Card 1, Card 2 and Card 3 Distributions\n- As the Card 1 and 2 are numericals, I will plot the distribution of them\n- in Card 3, as we have many values with low frequencies, I decided to set value to \"Others\" \n- Also, in Card 3 I set the % of Fraud ratio in yaxis2","e54045cf":"# Seting X and y","99c82c64":"# C1-C14 features\n- Let's understand what this features are.\n- What's the distributions? ","04201674":"## Top Days with highest Total Transaction Amount","aa4a5c89":"# TimeDelta Feature\n- Let's see if the frauds have some specific hour that has highest % of frauds ","d2dbac87":"# Card 6 - Categorical","87deb274":"## Id 30","42abc6fa":"To see the output of the Resume Table, click to see the output ","6b3bc270":"# Seeing the Quantiles of Fraud and No Fraud Transactions","ec793be6":"Cool and Very Meaningful information. <br>\nIn Card3 we can see that 100 and 106 are the most common values in the column. <br>\nWe have 4.95% of Frauds in 100 and 1.52% in 106; The values with highest Fraud Transactions are 185, 119 and 119; <br>\n\nIn card5 the most frequent values are 226, 224, 166 that represents 73% of data. Also is posible to see high % of frauds in 137, 147, 141 that has few entries for values.","b4f495c5":"Top 3 values are 1, 2 and 3 and is the same on Total Amounts. We see the same pattern on fraud ratios","5b818981":"All data is on Credit and Debit. We can see a high percentual of Frauds in Credit than Debit transactions. <br>\nThe Distribution of Transaction Amount don't shows clear differences.","ee0140ee":"If we consider only values between >= 0 to 800 we will avoid the outliers and has more confidence in our distribution. <br>\nWe have 10k rows with outliers that represents 1.74% of total rows.","7ebada42":"## Transactions and Total Amount by each day","7320b7a7":"Nice! Now, we can see clearly the distribution of ","5afb3782":"## Addr2 Distributions","a3f5f1f7":"# Running the optimizer","79994d37":"Almost all entries in Addr2 are in the same value. <br>\nInterestingly in the value 65 , the percent of frauds are almost 60% <br>\nAltought the value 87 has 88% of total entries, it has 96% of Total Transaction Amounts","ee3ecab7":"## Ploting columns with few unique values","a8725f87":"# Defining the HyperOpt function with parameters space and model","577eddf4":"## Ploting Hours Distributions","a3d952da":"# Now, let's known the Product Feature\n- Distribution Products\n- Distribution of Frauds by Product\n- Has Difference between Transaction Amounts in Products? ","07e51889":"# Ploting Transaction Amount Values Distribution","fd1a8038":"# Competition Objective is to detect fraud in transactions; \n\n## Data\n\n\nIn this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target ```isFraud```.\n\nThe data is broken into two files **identity** and **transaction**, which are joined by ```TransactionID```. \n\n> Note: Not all transactions have corresponding identity information.\n\n**Categorical Features - Transaction**\n\n- ProductCD\n- emaildomain\n- card1 - card6\n- addr1, addr2\n- P_emaildomain\n- R_emaildomain\n- M1 - M9\n\n**Categorical Features - Identity**\n\n- DeviceType\n- DeviceInfo\n- id_12 - id_38\n\n**The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).**\n\n# Questions\nI will start exploring based on Categorical Features and Transaction Amounts.\nThe aim is answer some questions like:\n- What type of data we have on our data?\n- How many cols, rows, missing values we have?\n- Whats the target distribution?\n- What's the Transactions values distribution of fraud and no fraud transactions?\n- We have predominant fraudulent products? \n- What features or target shows some interesting patterns? \n- And a lot of more questions that will raise trought the exploration. \n\n\n## I hope you enjoy my kernel and if it be useful for you, <b>upvote<\/b> the kernel","f4484b14":"## Seting y_pred to csv","66fa09ca":"## FRAUD TRANSACTIONS BY DATE\n- Visualizing only Fraud Transactions by Date","5112e9eb":"# Seting train and test back","e18642e9":"# Transaction Amount Quantiles","d86c6dfe":"## M distributions:  Count, %Fraud and Transaction Amount distribution","e427c829":"# Top 20 Feature importance","b33cbced":"## Id 31","acdac87d":"I will set all values in Addr1 that has less than 5000 entries to \"Others\"<br>\nIn Addr2 I will set as \"Others\" all values with less than 50 entries","8296ec7b":"## Converting to Total Days, Weekdays and Hours\nIn discussions tab I read an excellent solution to Timedelta column, I will set the link below; <br>\nWe will use the first date as 2017-12-01 and use the delta time to compute datetime features\n","a74a9977":"We don't have the reference of date but we can see that two days has lower transactions, that we can infer it is weekend days","9a2e298b":"# Some feature engineering","6028c050":"# Getting PCA "}}