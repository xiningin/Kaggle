{"cell_type":{"35ccd74c":"code","45ff8ca1":"code","43698cb6":"code","11ee7d00":"code","d3cb772f":"code","4ef21afd":"code","0bba77eb":"code","ffa55953":"code","96fec8e3":"code","f69a7b36":"code","393f6fbf":"code","b352a109":"code","b2d02a98":"code","2f7ac3ff":"code","c4ecf852":"code","411be412":"code","56bd791e":"code","0444cd9c":"code","0891fa6a":"code","37808667":"code","7b1c2f80":"code","8dd11f7b":"code","fb592f33":"markdown","3105bba8":"markdown","9926e8ac":"markdown","aba1407b":"markdown","ac85783a":"markdown","3712e60a":"markdown","b630abd7":"markdown","873fa885":"markdown","5bfcd7e9":"markdown","d200170c":"markdown","7e118453":"markdown","b530846a":"markdown","64abda28":"markdown","5fc6bff8":"markdown","6978b514":"markdown","a5f6f351":"markdown","e6b6d4ba":"markdown","aeae1a34":"markdown"},"source":{"35ccd74c":"# Basic libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom mlxtend.preprocessing import minmax_scaling\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Libraries for bootstrap\nfrom sklearn.utils import resample\n\n# Libraries for estimation\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Pre-defined random state        \nRandState = 100","45ff8ca1":"df_path = '\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv'\ndf = pd.read_csv(df_path)\nNoRecords = df.shape[0]","43698cb6":"#df.head()\ndf.describe()","11ee7d00":"C = (df.dtypes == 'object')\nCategoricalVariables = list(C[C].index)\n\nprint(CategoricalVariables)\n\nInteger = (df.dtypes == 'int64') \nFloat   = (df.dtypes == 'float64') \nNumericVariables = list(Integer[Integer].index) + list(Float[Float].index)\n\nprint(NumericVariables)","d3cb772f":"Missing_Percentage = (df.isnull().sum()).sum()\/np.product(df.shape)*100\n\nprint(\"The number of missing entries: \" + str(round(Missing_Percentage,10)) + \" %\")","4ef21afd":"AllVars = list(df.columns)  \nContVars = ['age','ejection_fraction','creatinine_phosphokinase','platelets','serum_creatinine','serum_sodium','time']\nBinVars = [x for x in AllVars if x not in ContVars]\ndf.index.name = 'Id'\n\nsc_df = df\nsc_df[ContVars] = minmax_scaling(df, columns=ContVars)","0bba77eb":"boot = resample(sc_df, replace=True, n_samples=round(NoRecords*0.80), random_state=RandState) # 240 ~= 299 * 80%\n# boot.describe()\n\nboot_indices_list = list(boot.index)  \nboot_indices = pd.DataFrame(boot.index)  \n\n# out of bag observations\noob = sc_df[~sc_df.index.isin(boot_indices_list)]\nprint('The number of records in bootstrap sample is: ' + format(boot.shape[0]) + '. The number of records in out-of-bag sample is: ' + format(oob.shape[0]))","ffa55953":"boot_indices_agg = boot_indices.groupby('Id').Id.count()\n\n(unique, counts) = np.unique(boot_indices_agg, return_counts=True)\n\nboot_indices_agg = pd.DataFrame(np.asarray((unique, counts)).T, columns = ['Value','Frequency'])\n\nboot_indices_agg.plot(kind='bar',x='Value',y='Frequency')","96fec8e3":"def BootstrapRatio(Data, F): # Data for our data set, F for the size of sample\n    BootSet = resample(Data, replace=True, n_samples=round(NoRecords*F))\n    DeadBoot = BootSet.loc[BootSet.DEATH_EVENT == 1].shape[0] \n    DeadBootRatio = DeadBoot\/BootSet.shape[0]\n    OobSet = Data[~Data.index.isin(list(BootSet.index))]\n    DeadOob = OobSet.loc[OobSet.DEATH_EVENT == 1].shape[0]\n    DeadOobRatio = DeadOob\/OobSet.shape[0]\n    \n    return DeadBootRatio, DeadOobRatio","f69a7b36":"B = 101 # How many runs - 1, so 100 runs\n\nB_results = pd.DataFrame(index=range(1,B), columns=['DeathsBoot','DeathsOob'])\n\nfor i in range(1,B):\n    B_results.DeathsBoot[i],B_results.DeathsOob[i] = BootstrapRatio(sc_df,0.8)\n    \nB_results","393f6fbf":"print('The diff is: '+format(round(B_results.DeathsBoot.mean() - B_results.DeathsOob.mean(),3)))","b352a109":"B_1 = 501 # How many runs - 1, so 500 runs\n\nB_1_results = pd.DataFrame(index=range(1,B_1), columns=['DeathsBoot','DeathsOob','DeathsBootAvg','DeathsOobAvg'])\n\nfor i in range(1,B_1):\n    B_1_results.DeathsBoot[i],B_1_results.DeathsOob[i] = BootstrapRatio(sc_df,0.8)\n    B_1_results.DeathsBootAvg[i]=B_1_results.DeathsBoot.mean()\n    B_1_results.DeathsOobAvg[i]=B_1_results.DeathsOob.mean()\n    \nB_1_results","b2d02a98":"sns.distplot(a=B_1_results['DeathsBoot'], hist=False, rug=True, label=\"Deaths Boot\")\nsns.distplot(a=B_1_results['DeathsOob'], hist=False, rug=True, label=\"Deaths Oob\")\nplt.legend();","2f7ac3ff":"# sns.regplot(data = B_1_results.reset_index(), x = 'index', y = 'DeathsBoot', fit_reg=False, label=\"Deaths Boot\")\n# sns.regplot(data = B_1_results.reset_index(), x = 'index', y = 'DeathsOob', fit_reg=False, label=\"Deaths Oob\")\n\nsns.regplot(data = B_1_results.reset_index(), x = 'index', y = 'DeathsBootAvg', fit_reg=False, label=\"Deaths Boot Avg\")\nsns.regplot(data = B_1_results.reset_index(), x = 'index', y = 'DeathsOobAvg', fit_reg=False, label=\"Deaths Oob Avg\")","c4ecf852":"RF = DecisionTreeClassifier()\n\ndef BootstrapRandomForest(Data, F, Target): # Data for our data set, F for the size of sample\n    BootSet = resample(Data, replace=True, n_samples=round(NoRecords*F))\n    OobSet = Data[~Data.index.isin(list(BootSet.index))]\n    RF.fit(BootSet.drop(columns=[Target]), BootSet[Target])\n    predictions = RF.predict(OobSet.drop(columns=[Target]))\n    score = accuracy_score(OobSet[Target], predictions)\n    \n    return score","411be412":"B_2 = 1001 # How many runs - 1, so 1000 runs\n\nB_2_results = pd.DataFrame(index=range(1,B_2), columns=['ScoreRF','ScoreXgb'])\n\nfor i in range(1,B_2):\n    B_2_results.ScoreRF[i] = BootstrapRandomForest(sc_df,0.8,'DEATH_EVENT')\n    \nB_2_results","56bd791e":"sns.distplot(B_2_results['ScoreRF'], color=\"g\").set_title(\"Accuracy scores for random forest (bootstrapping method)\", color=\"g\")","0444cd9c":"alpha = 0.95\np = ((1.0-alpha)\/2.0) * 100\nlower = max(0.0, np.percentile(B_2_results['ScoreRF'], p))\np = (alpha+((1.0-alpha)\/2.0)) * 100\nupper = min(1.0, np.percentile(B_2_results['ScoreRF'], p))\nprint('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))","0891fa6a":"Xgb =XGBClassifier( booster='gbtree',\n             importance_type='gain', learning_rate=0.01,\n             max_depth=4, min_child_weight=1.5, n_estimators=500, objective='binary:logistic')\n\ndef BootstrapXgb(Data, F, Target): # Data for our data set, F for the size of sample\n    BootSet = resample(Data, replace=True, n_samples=round(NoRecords*F))\n    OobSet = Data[~Data.index.isin(list(BootSet.index))]\n    Xgb.fit(BootSet.drop(columns=[Target]), BootSet[Target])\n    predictions = Xgb.predict(OobSet.drop(columns=[Target]))\n    score = accuracy_score(OobSet[Target], predictions)\n    \n    return score\n\nfor i in range(1,B_2):\n    B_2_results.ScoreXgb[i] = BootstrapXgb(sc_df,0.8,'DEATH_EVENT')\n    \nB_2_results","37808667":"sns.distplot(B_2_results['ScoreXgb'], color=\"g\").set_title(\"Accuracy scores for extreme boosting (bootstrapping method)\", color=\"g\")","7b1c2f80":"alpha = 0.95\np = ((1.0-alpha)\/2.0) * 100\nlower = max(0.0, np.percentile(B_2_results['ScoreXgb'], p))\np = (alpha+((1.0-alpha)\/2.0)) * 100\nupper = min(1.0, np.percentile(B_2_results['ScoreXgb'], p))\nprint('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))","8dd11f7b":"sns.distplot(a=B_2_results['ScoreRF'], hist=False, rug=True, label=\"Accuracy Random Forest\")\nsns.distplot(a=B_2_results['ScoreXgb'], hist=False, rug=True, label=\"Accuracy Extreme boosting\")\nplt.legend();","fb592f33":"# Bootstrap","3105bba8":"Now, we will apply bootstrap to our data. We implement our data set as first argument, we allow for replacement and use pre-defined random state. The argument *number of samples* is not trivial. How to choose? Similarly to any train\/test set in other topics. Hence, as our number of records is 299, we will chose let's say 80% of them. Will it be really 80%? No, because of resampling, in other words some records may appear twice or more and some records will not appear there for sure.","9926e8ac":"Following the histogram, the confidence intervals below. How to understand it? There is 95% likelihood that the confidence interval ~70% and ~83% cover the true skill of the model.","aba1407b":"We fitted 1000 random forests and calculated accuracy scores for them. The histogram for our scores is achievable below.","ac85783a":"Let's check it for extreme boosting as well.","3712e60a":"For this random seed, we have even 130 out-of-bag records, what means that in total 130 records did not appear in our bootstrap sample. Let's investigate how many records appear more than once.","b630abd7":"We have 100 records, and each of them corresponds to separate sample withdrawn from basic data set. Our evaluation for this simple statistic is the difference between mean in-sample and out-of-sample ratios.\n\nBut wait, let's make it more interesting: **we will check how the increasing number of sample influences these general means.**","873fa885":"The main added value of this notebook will be use of **bootstrap** step by step. \n\nWhat is bootstrap and why can it be useful in this case? Bootstrap is a powerful, but simple method which let us to make assumptions about **whole population only by use of small data set**. In this case, we have only 299 records what definitely belongs to the group of small data sets.\n\nDue to the fact that bootstrap is a central topic of this notebook, cleaning and estimation will be only briefly described.\n\nSome theory:\n\n> From beloved [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Bootstrapping_(statistics)): Bootstrapping is any test or metric that uses random sampling with replacement, and falls under the broader class of resampling methods. Bootstrapping assigns measures of accuracy (bias, variance, confidence intervals, prediction error, etc.) to sample estimates. This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods.\n\nIn other words, we will take some records from the sample, use it for estimation and return to sample set (so-called sampling with replacement). We use then the same data set many times to build assumptions over it. In my opinion this method is not extremely intuitive at the beginning since it looks like if we pretended that our sample is bigger than it really is by using stuff many times. And indeed the beuty of bootstrap comes from its simplicity. Please look at the picture below. We use given data set to produce statistic or fit a model. On the basis of them we can produce one general statistic.","5bfcd7e9":"# Intro to bootstrapping","d200170c":"# Estimation\nWe will use what was learnt from bootstrap analysis to find parameters and the confidence intervals. First, we change a bit the function for bootstrap and implement random forest in it.","7e118453":"**The above graph presents how extreme boosting outperforms random forest and is more informative than alone statistic regarding the model comparison. In that way bootstrap allows for broader methods' comparison.**","b530846a":"Some values appear even 4 times. This explains why oob set can be so big. Alright, we managed to make bootstrapping once. However, the base of this method is repetittion of the procedure many times. Let's do it then, for this it will be much easier to make a function, our function will calculate the number of dead patients is the train & test samples. But as these numbers do not tell us a lot (in their nominal form). We will define the ratio: dead patients\/ all patients in sample","64abda28":"Alright, the data set is:\n* only numeric, all variables are of format integer.float\n* very clean, no corrections needed at this stage\nLet's jump to bootstrap","5fc6bff8":"# Data cleaning","6978b514":"First, we apply scaling to continous features.","a5f6f351":"Three stages present in the notebook:\n* First, we check what data should be cleaned\n* Second, we apply bootstrapping\n* Last, estimation is done by use of random forest and extreme boosting\n\nLet's start","e6b6d4ba":"![bootstrap.png](attachment:bootstrap.png)","aeae1a34":"Between 0 and 30 long-term average is very unstable. Further around 50th run it crosses and remains stable. Basically after 300th average, there is no relevant difference. This simple process showed us how in general bootstrap can be useful to estimate any statistic like mean or variance. However, it was not very useful regarding the prediction of heart failure. For this task, we have to change approach a bit. As our target is binary, we will use logistic regression in our random forest model. "}}