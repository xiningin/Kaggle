{"cell_type":{"be95e814":"code","16d8ebaf":"code","21161662":"code","903e8600":"code","efe8de8a":"code","5863dfc1":"code","949ff3a7":"code","55c59232":"code","9ab0185c":"code","962aca4c":"code","4b589fcf":"code","a6664a11":"markdown","c99cc201":"markdown","0dbb261b":"markdown","48a6f255":"markdown","8daa39ae":"markdown","ca1ab5fe":"markdown","942fad23":"markdown","7a22fd5f":"markdown","9825933b":"markdown","49669e2f":"markdown","8815f7ed":"markdown","a7eb2bb0":"markdown","08ba16ea":"markdown"},"source":{"be95e814":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","16d8ebaf":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\n\nX_full = pd.read_csv(\"..\/input\/train.csv\",index_col=0)\nX_full_test = pd.read_csv(\"..\/input\/test.csv\",index_col=0)\n\nX_full.shape","21161662":"X_full.head()","903e8600":"y = X_full.Survived\nfeatures = ['Pclass','Sex','Age','SibSp','Parch'] #I had included Cabin as well. Removed it later.\nX = X_full[features].copy()\nX_test = X_full_test[features].copy()\n\nX_train, X_valid, y_train, y_valid = train_test_split(X,y,train_size=0.8,test_size=0.2,random_state=0)\n\n#cols_with_missing = [col for col in X_train.columns if X_train[col].isnull().any()]\n#print(cols_with_missing)\n#Only two columns. But how many values are missing?\n\n# --- No need for the above as the below line of code did the task really well!\n\n#X.shape --> (891,6)\nX.isnull().sum()\n","efe8de8a":"#dropping 'Cabin'\n#X_train = X_train.drop(\"Cabin\")\n#X_valid = X_train.drop('Cabin',axis=1)\n\n#Error: ['Cabin'] not found in axis ???  --- I'm just gonna remove it from the features itself for now.\n\n#Imputing\n\nX_train.fillna(X_train.mean(),inplace=True) \n#X_train.isnull().any() #dtype changed to bool...\nX_valid.fillna(X_valid.mean(),inplace=True)\nX_test.fillna(X_valid.mean(),inplace=True)  #Must not forget this!\n\nX_train.head()\n","5863dfc1":"from sklearn.preprocessing import LabelEncoder\n\n# Apply label encoder\nlabel_encoder = LabelEncoder()\nX_train['Sex'] = label_encoder.fit_transform(X_train['Sex'])\nX_valid['Sex'] = label_encoder.transform(X_valid['Sex'])\nX_test['Sex'] = label_encoder.fit_transform(X_test['Sex'])\n\nX_train.head() # Female: 0, Male: 1","949ff3a7":"from xgboost import XGBClassifier\n\nmy_model = XGBClassifier(n_estimators=1000, learning_rate=0.05)\nmy_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)","55c59232":"from sklearn.metrics import accuracy_score\n\npredictions = my_model.predict(X_valid)\nprint(\"Accuracy Score: \" + str(accuracy_score(predictions, y_valid)))","9ab0185c":"preds = my_model.predict(X_valid)\npreds_test = my_model.predict(X_test)\n","962aca4c":"features = ['Pclass','Sex','Age','SibSp','Embarked']\nX = X_full[features].copy()\nX_test = X_full_test[features].copy()\n\n# Filling missing values\nX.fillna(X.mean(),inplace=True) # So apparently this works only for columns with numbers, not for categorical variables.\nX_test.fillna(X_test.mean(),inplace=True)\n\nX['Embarked'] = X['Embarked'].fillna('S')\nX_test['Embarked'] = X_test['Embarked'].fillna('S')\n\n#Label Encoding for categorical variables\nX['Sex'] = label_encoder.fit_transform(X['Sex'])\nX_test['Sex'] = label_encoder.transform(X_test['Sex'])\nX['Embarked'] = label_encoder.fit_transform(X['Embarked'])\nX_test['Embarked'] = label_encoder.transform(X_test['Embarked'])\n\n#X.isnull().sum()\n#X_test.isnull().any()\n#X_test.head()\n\nX_train, X_valid, y_train, y_valid = train_test_split(X,y,train_size=0.8,test_size=0.2,random_state=0)\n\nmy_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)\npredictions = my_model.predict(X_valid)\nprint(\"Accuracy Score: \" + str(accuracy_score(predictions, y_valid)))\n\npreds = my_model.predict(X_valid)\npreds_test = my_model.predict(X_test)\n","4b589fcf":"output = pd.DataFrame({'PassengerId': X_test.index,\n                       'Survived': preds_test})\noutput.to_csv('submission.csv', index=False)","a6664a11":"0.81! \n\nI'll make predictions now. ","c99cc201":"# Attempt 2","0dbb261b":"So there are 891 rows.\n\nAround 19.9% of 'Age' values are missing. So I'll just Impute it.\n\nWhereas, 77% of 'Cabin' values are missing. I'll drop it. Anyway 'Cabin' doesn't seem like a strong feature for my model.","48a6f255":"Accuracy score increased by 0.01 (That's it!).\n\nRemoving 'Parch' has no effect on the accuracy score, so I'll remove it.\n\nRemoving 'Sex' reduced the score by 0.1!\n\nRemoving 'Fare' got the score up by 0.01. Cool.\n\nSo overall, you could say, my accuracy score improved by almost 0.04 (0.810 to 0.838).","8daa39ae":"That's it with data processing.\n\nNow I'll use XGBoost to create the model.","ca1ab5fe":"\"You advanced 3,578 places on the leaderboard!\n\nYour submission scored 0.79425, which is an improvement of your previous score of 0.77511. Great job!\"\n\n\n\nYAYYYY!! :D","942fad23":"Let's check the Accuracy Score.","7a22fd5f":"Finally done! I had made a mistake of using 'XGBRegressor' and calculating 'mean absolute score'. This was giving my 'Survived' values in terms of decimals.\n\nSince this is a classification problem, 'XGBClassifier' should be used and 'accuracy score' can be calculated instead. (Pointed out by @lucabasa - Thanks!)\n\n","9825933b":"I'll try including 'Fare' and 'Embarked'. Let's see what happens.","49669e2f":"This is my first ever Kernel. I will use this dataset to practice my ML skills. Hopefully they'll improve overtime!\n(And I'll make lots of edits too!)\n\n# What is the problem?\n\nA huge dataset is given. I have to analyze it and make predictions on which passengers were likely to survive. (Not sure how to do that...). I'll use the ML courses on Kaggle as a guide for now.","8815f7ed":"'PassengerId' is the index, 'Survived' is definitely our target. Now I have to select features.\nI don't think 'Name', 'Ticket', 'Fare' and 'Embarked' are required. How would they help in determining whether that passenger survived or not? I think the others can be selected as features. \n\n'Cabin' has NaN values. Let's check for null values in our features.\n","a7eb2bb0":"So it's shape, not shape(). Also, shape and head() when used in the same block don't give the output.\n\nSo! It has 891 rows and 11 columns (excluding index 'PassengerId'). ","08ba16ea":"Okay that was really tiring! So I've removed 'Cabin', and I've imputed 'Age'. I tried using the SimpleImputer but it threw some error, so I just did it manually.\n\nNow let's deal with our Categorical variable: 'Sex'\n"}}