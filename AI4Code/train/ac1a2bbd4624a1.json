{"cell_type":{"38f21feb":"code","7999f55a":"code","e6a238fd":"code","823b82fa":"code","a96c50d9":"code","3c3edc34":"code","1464366e":"code","58d88a78":"code","350410db":"code","7c346897":"code","b109ac87":"code","6045028a":"code","4118c451":"code","20a7ae85":"code","6d9f76a5":"code","c2a53e65":"code","aa10fc05":"code","e4cf519b":"code","65012341":"code","de231d47":"code","0e622873":"code","ca69124c":"code","6a13c63e":"code","9d6bad68":"code","0bf5466a":"code","baa52691":"code","cff67075":"code","010edfd9":"code","1b977216":"code","f6ccd440":"code","c43d3a38":"code","6c7faeda":"code","b69fb375":"code","fa577465":"code","00bf2dcb":"code","45d83de5":"code","83e33600":"code","84d9b248":"code","9e374f37":"code","c22d0c6e":"code","14674b63":"code","8605bf16":"code","ed779694":"code","ec183898":"code","13cb2f36":"code","e48e68b1":"code","f9c17350":"markdown","4a4adaa1":"markdown","efe51132":"markdown","884507e5":"markdown","8b3cbc6a":"markdown"},"source":{"38f21feb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7999f55a":"nba = pd.read_csv(\"\/kaggle\/input\/unsupervised-ml\/nba_2013.csv\")\nnba.head()","e6a238fd":"# make simple data then I can use large data for clustering\nnba[\"pos\"].unique()","823b82fa":"point_gaurd = nba[nba.pos == \"PG\"].copy()\npoint_gaurd.head()","a96c50d9":"# Create a two new column name point per garud and atr\npoint_gaurd[\"ppg\"] = point_gaurd[\"pts\"]\/point_gaurd[\"g\"]","3c3edc34":"point_gaurd = point_gaurd[point_gaurd[\"tov\"] != 0].copy()","1464366e":"point_gaurd[\"atr\"] = point_gaurd[\"ast\"]\/point_gaurd[\"tov\"]","58d88a78":"point_gaurd.head()","350410db":"point_gaurd[[\"ppg\",\"atr\"]].head(5)","7c346897":"point_gaurd.index\npoint_gaurd.shape[0]","b109ac87":"num_cluster = 5\nnum_cluster1 = 2\nnp.random.seed(1)\nrandom_initial_value = np.random.choice(point_gaurd.index, size = num_cluster)\nrandom_initial_value","6045028a":"centroids = point_gaurd.loc[random_initial_value]\ncentroids","4118c451":"# {0:[12.535211,1.670455], 1 :[16.671429,1.785425]}\n\ndef centroids_to_dic(centroids):\n    dictionary = {}\n    counter = 0\n    for index, row in centroids.iterrows():\n        coordinates = [row[\"ppg\"], row[\"atr\"]]\n        dictionary[counter] = coordinates\n        counter += 1\n    return dictionary","20a7ae85":"centroids_dic = centroids_to_dic(centroids)\ncentroids_dic","6d9f76a5":"point_gaurd.iloc[0][[\"ppg\", \"atr\"]]","c2a53e65":"euclidean_distance = np.sqrt(np.array([[12.53-13.0986]])**2 +np.array([[1.67- 2.504]])**2 )\neuclidean_distance","aa10fc05":"def calculate_distance(q,p):\n    distance = 0\n    for i in range(len(q)):\n        distance += (q[i] - p[i])**2\n    return np.sqrt(distance)","e4cf519b":"q = [12.53, 1.67]\np = [13.0986,2.504]\nprint(calculate_distance(q,p))","65012341":"row1_distances = []\nfor q1 in centroids_dic.values():\n#     print(q1)\n    distance = calculate_distance(q1,p)\n    row1_distances.append(distance)","de231d47":"row1_distances","0e622873":"minimum = min(row1_distances)\n\nrow1_distances.index(minimum)","ca69124c":"def assign_cluster(row):\n    row_distances = []\n    p = [row[\"ppg\"], row[\"atr\"]]\n    for q in centroids_dic.values():\n        distance = calculate_distance(q,p)\n        row_distances.append(distance)\n    minimum = min(row_distances)\n    cluster = row_distances.index(minimum)\n    return cluster","6a13c63e":"assign_cluster(point_gaurd.iloc[0])","9d6bad68":"assign_cluster(point_gaurd.iloc[81])","0bf5466a":"point_gaurd[\"cluster\"] = point_gaurd.apply(lambda row :assign_cluster(row),axis = 1)","baa52691":"point_gaurd[\"cluster\"].value_counts()","cff67075":"def visualize_cluster(df, num_cluster):\n    colors = [\"b\",\"g\", \"r\", \"y\", \"k\"]\n    for i in range(num_cluster):\n        cluster = df[df[\"cluster\"] == i]\n        plt.scatter(cluster[\"ppg\"], cluster[\"atr\"], c = colors[i])\n    plt.show()","010edfd9":"visualize_cluster(point_gaurd, num_cluster)","1b977216":"cluster_0 = point_gaurd[point_gaurd[\"cluster\"] == 0]\nppg = cluster_0[\"ppg\"].mean()\natr = cluster_0[\"atr\"].mean()\n\ncen = {0: [ppg, atr]}\ncen","f6ccd440":"def recalculate_cent(df):\n    dictionary = {}\n    for i in range(num_cluster):\n        cluster = point_gaurd[point_gaurd[\"cluster\"] == i]\n        ppg = cluster[\"ppg\"].mean()\n        atr = cluster[\"atr\"].mean()\n        dictionary[i] = [ppg,atr]\n    return dictionary","c43d3a38":"centroids_dic = recalculate_cent(point_gaurd)\ncentroids_dic","6c7faeda":"point_gaurd[\"cluster\"] = point_gaurd.apply(lambda row :assign_cluster(row),axis = 1)","b69fb375":"visualize_cluster(point_gaurd, num_cluster)","fa577465":"kmeans =  KMeans(n_clusters=num_cluster, random_state = 1)\nkmeans.fit(point_gaurd[[\"ppg\", \"atr\"]])\nkmeans.labels_","00bf2dcb":"point_gaurd[\"cluster\"] = kmeans.labels_","45d83de5":"visualize_cluster(point_gaurd, num_cluster)","83e33600":"point_gaurd.head(10)","84d9b248":"votes = pd.read_csv(\"\/kaggle\/input\/unsupervised-ml\/114_congress.csv\")","9e374f37":"votes.head()","c22d0c6e":"votes[\"party\"].value_counts()","14674b63":"kmean = KMeans(n_clusters= 2, random_state = 1)\nkmean.fit(votes.iloc[:,3:])","8605bf16":"labels = kmean.labels_\nlabels","ed779694":"votes[\"group\"] = labels","ec183898":"votes[[\"party\", \"group\"]]","13cb2f36":"pd.crosstab(votes[\"group\"], votes[\"party\"])","e48e68b1":"boolean = (votes[\"party\"] == \"D\") & (votes[\"group\"] == 0)\nvotes[boolean]","f9c17350":"# Using Sklearn library  ","4a4adaa1":"# Using another dataset (Senator votes)","efe51132":"Firstly I can work on without Machine Learnig library then I can use KMeans library for Clustering","884507e5":"## If you fined helpfull then Please Upvote my notebook .","8b3cbc6a":"# In This notebook we are learn Basic starting ML Models of clustering."}}