{"cell_type":{"e066fa11":"code","53ac1bf8":"code","6511c056":"code","13ca3774":"code","66cbe629":"code","f34becff":"code","0b35d1c1":"code","a0360c73":"code","a7b5b142":"code","2927c8a0":"code","a9161299":"code","ab51e83a":"code","af92b2a6":"code","ebc7efd1":"code","93e411c1":"code","a38461a4":"code","a2cc8fc2":"code","404aec2b":"code","b60e0170":"code","73cedb7f":"code","1ce89d6a":"code","fd4a6e03":"code","8d4c46f6":"code","64601189":"code","12f2c1e2":"code","278c915b":"code","41ee102b":"code","e4c8133d":"code","8d068943":"code","e28e9ac7":"code","b407f1f8":"code","5ffe0f27":"code","47bdde6c":"code","e0daf4af":"code","c7e14158":"code","57a8db49":"markdown","057a98fc":"markdown","dab5e0a8":"markdown","020eac41":"markdown","cbde0be8":"markdown"},"source":{"e066fa11":"import numpy as np?\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split","53ac1bf8":"raw_train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nraw_test_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nraw_train_data.head()","6511c056":"# assessing category of columns\nraw_train_data.dtypes\nnumerical_categories = raw_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\nobject_categories = raw_data.select_dtypes(include=['object']).columns.tolist()\nprint(\"numerical categories:\", numerical_categories)\nprint(\"object categories:\", object_categories)","13ca3774":"raw_train_data.describe(include='all')","66cbe629":"# combine test and train data after dropping survived from train data\ndrop_columns = ['PassengerId', 'Name', 'Cabin', 'Ticket']\nraw_train_data_drop = raw_train_data.drop(['Survived'], axis=1)\nraw_total_data = pd.concat(objs=[raw_train_data_drop, raw_test_data], axis=0).reset_index(drop=True)\ndropped_data = raw_total_data.drop(drop_columns, axis=1)\ntargets = raw_train_data['Survived']\ndropped_data","f34becff":"for category in numerical_categories:\n    if category in drop_columns or category in ['Survived']:\n        continue\n    print(dropped_data[category].value_counts())","0b35d1c1":"# classifying \ndropped_data['Parch'] = np.where(dropped_data['Parch']>1,1,0)\nreason_columns = pd.get_dummies(dropped_data['Pclass'], drop_first=True)\ndropped_data = dropped_data.drop(['Pclass'], axis=1)\ndropped_data['Sex'] = dropped_data['Sex'].map({'male':0, 'female':1})\ndropped_data['Embarked'] = dropped_data['Embarked'].map({'S':0, 'C':1, 'Q':2})\ndropped_data.dropna(subset=['Embarked'], inplace=True)\ndropped_data['Age'].fillna(dropped_data['Age'].mean(), inplace=True)\npreprocessed_data = pd.concat([dropped_data, reason_columns], axis=1)\npreprocessed_data.head()","a0360c73":"def scale_unscaled(unscaled_data):\n    unscaled_inputs = unscaled_data.iloc[:,4]\n    mean_fare = unscaled_inputs.mean()\n    std_fare = unscaled_inputs.std()\n    fare_scaled = unscaled_inputs - mean_fare\n    fare_scaled = fare_scaled\/std_fare\n    unscaled_data = unscaled_data.drop(['Fare'], axis=1)\n    data_scaled = pd.concat([unscaled_data, fare_scaled], axis=1)\n    return data_scaled\n\ndata_scaled = scale_unscaled(preprocessed_data)\ndata_scaled = data_scaled['Fare'].fillna(data_scaled['Fare'].median())\ndata_scaled.isnull().sum()","a7b5b142":"print(targets.shape)\ntrain_len = raw_train_data.shape[0]\ntrain = data_scaled[:train_len]\ntest = data_scaled[train_len:]\nx_train, x_val, y_train, y_val = train_test_split(train, targets, train_size=0.9, random_state=20)\nreg = LogisticRegression()\nreg.fit(x_train, y_train)\nreg.score(x_train, y_train)","2927c8a0":"train_len = raw_train_data.shape[0]\ntrain = data_scaled[:train_len]\ntest = data_scaled[train_len:]\nSurvived = reg.predict(data_scaled)\nPassengerId = raw_test_data['PassengerId']","a9161299":"print(len(Survived), len(PassengerId))\nPassengerId = PassengerId.drop(i)","ab51e83a":"len(PassengerId)","af92b2a6":"submission = pd.DataFrame(PassengerId, columns=['PassengerId'])\nsubmission['Survived'] = Survived\nsubmission","ebc7efd1":"submission.to_csv('my_new_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","93e411c1":"copy_data = raw_data.copy()\ncopy_data = copy_data.drop(['PassengerId', 'Name', 'Cabin', 'Ticket'], axis=1)\ncopy_data.head()\ncopy_data.isnull().sum()","a38461a4":"copy_data['Sex'] = copy_data['Sex'].map({'male':0, 'female':1})\n","a2cc8fc2":"copy_data","404aec2b":"copy_data['Embarked'] = copy_data['Embarked'].map({'S':0, 'C':1, 'Q':2})","b60e0170":"copy_data.dropna(subset=['Embarked'], inplace=True)\ncopy_data['Embarked'].isnull().sum()","73cedb7f":"copy_data['Age'].fillna(copy_data['Age'].mean(), inplace=True)\ncopy_data['Age'].isnull().sum()","1ce89d6a":"copy_data['Parch'].value_counts()\ncopy_data['Parch'] = np.where(copy_data['Parch']>1,1,0)\ncopy_data","fd4a6e03":"reason_columns = pd.get_dummies(copy_data['Pclass'], drop_first=True)\nreason_columns\ncopy_data = copy_data.drop(['Pclass'], axis=1)\ncopy_data = pd.concat([copy_data, reason_columns], axis=1)\ncopy_data","8d4c46f6":"unscaled_inputs = copy_data.iloc[:,5]\nunscaled_inputs.head()","64601189":"mean_fare = unscaled_inputs.mean()\nstd_fare = unscaled_inputs.std()\nfare_scaled = unscaled_inputs - mean_fare\nfare_scaled = fare_scaled\/std_fare\nfare_scaled","12f2c1e2":"copy_data = copy_data.drop(['Fare'], axis=1)\ndata_pre = pd.concat([copy_data, fare_scaled], axis=1)\n","278c915b":"data_pre = data_pre.drop(['Survived'], axis=1)\ndata_pre","41ee102b":"targets = copy_data.iloc[:,0]\ntargets","e4c8133d":"from sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(data_pre, targets, train_size=0.9, random_state=20)","8d068943":"print(x_train.shape, y_train.shape)","e28e9ac7":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nreg = LogisticRegression()\nreg.fit(x_train, y_train)\n\nreg.score(x_train, y_train)","b407f1f8":"print(reg.intercept_)\nprint(reg.coef_)\nfeature_name = data_pre.columns.values\nsummary_table = pd.DataFrame(columns=['feature_name'], data=feature_name)\nsummary_table['coefficients'] = reg.coef_[0]\nsummary_table.head()","5ffe0f27":"summary_table.index = summary_table.index+1\nsummary_table.loc[0] = ['Intercept', reg.intercept_[0]]\nsummary_table = summary_table.sort_index()\nsummary_table","47bdde6c":"summary_table['Odds_ratio'] = np.exp(summary_table.coefficients)\nsummary_table.sort_values('Odds_ratio', ascending=False)","e0daf4af":"reg.score(x_val, y_val)","c7e14158":"raw_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nraw_test.head()","57a8db49":"## 4. Feature Scaling","057a98fc":"## 1. Introduction\n\nThis is my first attempt at any competition on Kaggle. Through this exercise, I would like to explore feature analysis and develop a mindset towards machine learning and feature engineering. \n\nThis will follow approach which follows a structured approach starting with data preprocessing followed by feature analysis and model building and prediction.  ","dab5e0a8":"### **Akshat Mandloi**\n#### 22\/07\/2020\n\n* **1 Introduction**\n* **2 Load raw data**\n    * 2.1 Load Raw Data\n* **3 Pre Processing Data**\n* **4 Feature Scaling**\n    * 4.1 Scaling Inputs\n* **5 Modeling**\n    * 5.1 Simple Logistic Model\n    * 5.2 Predicting and Submitting Results\n    ","020eac41":"## 2. Importing Raw Data","cbde0be8":"## 3 Pre Processing Data"}}