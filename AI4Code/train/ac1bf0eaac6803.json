{"cell_type":{"6178aa96":"code","57add1df":"code","0a42a91b":"code","9c0b3bac":"code","0318a894":"code","375423e9":"code","2e73de61":"code","8238522d":"code","b7b98137":"code","06526ad9":"code","9d24f5a0":"code","0bd6efcb":"code","21de929b":"code","98656284":"code","b321974e":"code","e1471cc5":"code","21930436":"code","855d100c":"code","1d63d467":"code","5afb4c12":"code","d57ce0cf":"code","3603599a":"code","96cf579c":"code","c7a64ef8":"code","60e9841f":"code","dc38f32c":"code","33d5cdeb":"code","5260be4a":"code","519ad05b":"code","b9b7e463":"code","c3124a2e":"code","09e53673":"code","a94344db":"code","a5adbff2":"code","7aadfb7c":"code","ef60c2bb":"code","5dbf534a":"code","6225b3ad":"code","ff90241e":"code","2bf0e016":"code","f6dce897":"code","d963ee0b":"code","2c915597":"code","1a23fd76":"code","fc239e5b":"code","2060000c":"code","1020a8d1":"code","7caaf521":"code","26f7a713":"code","f8c902ac":"code","1413c538":"code","2c8e3d78":"code","efa7cd6e":"code","9ba42f8e":"code","90fc1ff3":"code","3a13101a":"code","e5d59f1b":"markdown","fa608f26":"markdown","3db590b5":"markdown","f606e3ca":"markdown","154b1129":"markdown","fbf726df":"markdown","668ed2ca":"markdown","2067709e":"markdown","e54cc22d":"markdown","b93caf8d":"markdown","831121d3":"markdown","b82dd627":"markdown","738a78f0":"markdown","1f8b0170":"markdown","2976783a":"markdown","852c198f":"markdown","b11fb23e":"markdown","d85a3003":"markdown","0fa5e589":"markdown","e395d4e4":"markdown","b06fb4a3":"markdown","4c7d579d":"markdown","f49d24f5":"markdown","bef841ef":"markdown","73e09443":"markdown","e4e23f3c":"markdown","03cad513":"markdown","8b8d9639":"markdown","67767700":"markdown","56e8fe0d":"markdown","57e62763":"markdown","383ef55c":"markdown","e8c25b7e":"markdown","2bb0c4e2":"markdown","e755bece":"markdown","52dea5ad":"markdown","4d114004":"markdown"},"source":{"6178aa96":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom imblearn.combine import SMOTETomek\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.decomposition import PCA\n\n# display all columns of the dataframe\npd.options.display.max_columns = None\n\n# display all rows of the dataframe\npd.options.display.max_rows = None\n\n# use below code to convert the 'exponential' values to float\nnp.set_printoptions(suppress=True)","57add1df":"df = pd.read_csv('..\/input\/arketing-campaign\/marketing_campaign.csv', sep=';')\ndf.head()","0a42a91b":"# Dropping ID Column beacause we dont id column for predictions\ndf.drop('ID', axis=1, inplace = True)","9c0b3bac":"# Shape of Dataset\nprint('Data contains', df.shape[0], 'rows and', df.shape[1], 'columns')","0318a894":"# Dataset information about value count and variable data type\ndf.info()","375423e9":"# Numerical Data Description\ndf.describe().T","2e73de61":"# Categorical Data Description\ndf.describe(include='O').T","8238522d":"# Check for null values in the dataset\ndf.isnull().sum()","b7b98137":"def fill_na(frame):\n    for i in frame.columns:\n        if(((frame[i].isnull().sum() \/ len(frame))*100) <= 30) & (frame[i].dtype == 'int64'):\n            frame[i] = frame[i].fillna(frame[i].median())\n            \n        elif(((frame[i].isnull().sum() \/ len(frame))*100) <= 30) & (frame[i].dtype == 'O'):\n            frame[i] = frame[i].fillna(frame[i].mode()[0])\n            \n        elif(((frame[i].isnull().sum() \/ len(frame))*100) <= 30) & (frame[i].dtype == 'float64'):\n            frame[i] = frame[i].fillna(frame[i].median())\n            \nfill_na(df)","06526ad9":"def detect_outliers(frame):\n    for i in frame.columns:\n        if(frame[i].dtype == 'int64'):\n            sns.boxplot(frame[i])\n            plt.show()\n            \n        elif(frame[i].dtype == 'float64'):\n            sns.boxplot(frame[i])\n            plt.show()\n            \ndetect_outliers(df)","9d24f5a0":"def univariant(frame):\n    for i in frame.columns:\n        if(frame[i].dtype == 'int64'):\n            print(i)\n            sns.distplot(x=frame[i])\n            plt.show()\n                \n        elif(frame[i].dtype == 'float64'):\n            print(i)\n            sns.distplot(x=frame[i])\n            plt.show()\n            \nunivariant(df)","0bd6efcb":"# Plot Response variable seperately because our target variable(Class) is int and we have to treat it like object this time\nsns.countplot(df['Response'])\nplt.show()","21de929b":"sns.pairplot(df)","98656284":"# Check correlation between variables\nplt.figure(figsize=(30,25))\nsns.heatmap(df.corr(), annot=True)","b321974e":"# Converting dt_Customer into datetime64 data type\ndf['Dt_Customer'] = df['Dt_Customer'].astype('datetime64')","e1471cc5":"# Creating two new columns Date_customer and Month_customer from Dt_Customer column\ndf['Date_Customer'] = df['Dt_Customer'].dt.day\ndf['Month_Customer'] = df['Dt_Customer'].dt.month\ndf['Year_Customer'] = df['Dt_Customer'].dt.year","21930436":"# Now we can drop Dt_Customer column\ndf.drop('Dt_Customer', axis=1, inplace=True)","855d100c":"def encode(dataframe):\n    lec = LabelEncoder()\n    for j in dataframe.columns:\n        if(dataframe[j].dtype == 'object'):\n            dataframe[j] = lec.fit_transform(dataframe[j])\n            \nencode(df)","1d63d467":"x = df.drop('Response', axis=1)\ny = df['Response']\n\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=1)","5afb4c12":"lr = LogisticRegression(max_iter=10000)\nlr.fit(X_train, Y_train)","d57ce0cf":"lr_pred = lr.predict(X_test)\nprint(classification_report(Y_test, lr_pred))","3603599a":"xgb = XGBClassifier()\nxgb.fit(X_train, Y_train)","96cf579c":"xgb_pred = xgb.predict(X_test)\nprint(classification_report(Y_test, xgb_pred))","c7a64ef8":"rf = RandomForestClassifier()\nrf.fit(X_train, Y_train)","60e9841f":"rf_pred = rf.predict(X_test)\nprint(classification_report(Y_test, rf_pred))","dc38f32c":"gb = GradientBoostingClassifier()\ngb.fit(X_train, Y_train)","33d5cdeb":"gb_pred = gb.predict(X_test)\nprint(classification_report(Y_test, gb_pred))","5260be4a":"accuracy_score(Y_test, gb_pred)","519ad05b":"estimators = [('xgb', XGBClassifier()),\n             ('rf', RandomForestClassifier()),\n             ('gb', GradientBoostingClassifier())]\nstack = StackingClassifier(estimators=estimators)\nstack.fit(X_train, Y_train)","b9b7e463":"stack_pred = stack.predict(X_test)\nprint(classification_report(Y_test, stack_pred))","c3124a2e":"smote = SMOTETomek()\nx_train, y_train = smote.fit_resample(X_train, Y_train)","09e53673":"slr = LogisticRegression(max_iter=10000)\nslr.fit(x_train, y_train)","a94344db":"slr_pred = slr.predict(X_test)\nprint(classification_report(Y_test, slr_pred))","a5adbff2":"sxgb = XGBClassifier()\nsxgb.fit(x_train, y_train)","7aadfb7c":"sxgb_pred = sxgb.predict(X_test)\nprint(classification_report(Y_test, sxgb_pred))","ef60c2bb":"srf = RandomForestClassifier()\nsrf.fit(x_train, y_train)","5dbf534a":"srf_pred = srf.predict(X_test)\nprint(classification_report(Y_test, srf_pred))","6225b3ad":"sgb = GradientBoostingClassifier()\nsgb.fit(x_train, y_train)","ff90241e":"sgb_pred = sgb.predict(X_test)\nprint(classification_report(Y_test, sgb_pred))","2bf0e016":"sstack = StackingClassifier(estimators=estimators)\nsstack.fit(x_train, y_train)","f6dce897":"sstack_pred = sstack.predict(X_test)\nprint(classification_report(Y_test, sstack_pred))","d963ee0b":"th = np.sort(gb.feature_importances_)\nl = []\nfor g in th:\n    select = SelectFromModel(gb, threshold = g, prefit = True)\n    x_Train = select.transform(X_train)\n    model = GradientBoostingClassifier()\n    model.fit(x_Train, Y_train)\n    x_Test = select.transform(X_test)\n    y_pred = model.predict(x_Test)\n    accuracy = accuracy_score(Y_test, y_pred)\n    print('Threshold:', g, 'Model Score:', accuracy)","2c915597":"imp = pd.DataFrame(rf.feature_importances_)\nimp.index = X_train.columns\nimp[imp[0] < 0.017037885998921535]","1a23fd76":"X_train = X_train.drop(['Z_CostContact', 'Z_Revenue'], axis=1)\nX_test = X_test.drop(['Z_CostContact', 'Z_Revenue'], axis=1)","fc239e5b":"fgb = GradientBoostingClassifier()\nfgb.fit(X_train, Y_train)","2060000c":"fgb_pred = fgb.predict(X_test)\nprint(classification_report(Y_test, fgb_pred))","1020a8d1":"accuracy_score(Y_test, fgb_pred)","7caaf521":"# First i check how many components we want\n# For this first i am initializing the pca\npca = PCA()\n# Fitting the training set in pca\npca.fit(X_train)","26f7a713":"# Now check number of components\npca.explained_variance_ratio_","f8c902ac":"# Creating pca with n_components = 15\nPca = PCA(n_components=15)\n# Fitting the training data\nX_Train = Pca.fit_transform(X_train)\nX_Test = Pca.fit_transform(X_test)","1413c538":"# Building models after applying pca\npgb = GradientBoostingClassifier()\npgb.fit(X_Train, Y_train)","2c8e3d78":"pgb_pred = pgb.predict(X_Test)\nprint(classification_report(Y_test, pgb_pred))","efa7cd6e":"grid = {\n    'learning_rate' : [0.2, 0.3, 0.4, 0.5],\n    'n_estimators' : [300, 500, 700, 900],\n    'min_samples_split' : [3, 4, 5, 6],\n    'max_depth' : [2, 3, 4, 5],\n    'loss' : ['deviance', 'exponential']\n}\nrandom_cv = RandomizedSearchCV(estimator=gb,\n                              param_distributions=grid,\n                              n_iter=20,\n                              n_jobs=-1,\n                              cv=5,\n                              verbose=7,\n                              random_state=10,\n                              scoring='accuracy')\nrandom_cv.fit(X_train, Y_train)","9ba42f8e":"random_cv.best_estimator_","90fc1ff3":"hgb = GradientBoostingClassifier(learning_rate=0.5, loss='exponential', max_depth=2,\n                           min_samples_split=4, n_estimators=300)\nhgb.fit(X_train, Y_train)","3a13101a":"hgb_pred = hgb.predict(X_test)\nprint(classification_report(Y_test, hgb_pred))","e5d59f1b":"# Lets Build Models","fa608f26":"# Check for null values","3db590b5":"# Hyper Parameter Tuning","f606e3ca":"# 2. XGBoost Classifier","154b1129":"# Import Required Libraries","fbf726df":"Our target variable(Response) is not balanced ","668ed2ca":"# 2. XGBoost Classifier","2067709e":"# 4. Gradient Boosting Classifier","e54cc22d":"# 2. Univariant Analysis","b93caf8d":"# Data Information","831121d3":"# Split data into train and test","b82dd627":"After feature selection i am getting same accuracy. So i further build model using base line","738a78f0":"Models after balancing the target variable gives good results. But if i them compare with base line models then base line model of Gradient Boosting Classifier give highest accuracy. So i further build my model with Gradient Boosting Classifier base line.","1f8b0170":"# 3. Random Forest Classifier","2976783a":"# Balancing the target variable","852c198f":"# Dimentionality Reduction","b11fb23e":"# Feature Selection","d85a3003":"# Building model after feature selection","0fa5e589":"# 1. Logistic Regression","e395d4e4":"# 5. Stacking Classifier","b06fb4a3":"Only income column contains null values","4c7d579d":"As shown above our 99.97% data covers in 1 principal component","f49d24f5":"From all my base line models Gradient Boosting Classifier gives best results","bef841ef":"# 3. Multivariant Analysis","73e09443":"# Encode Categorical Variables","e4e23f3c":"# Load Data","03cad513":"# 1. Logistic Regression","8b8d9639":"# My Best Model\nMy Best model is Gradient Boosting Classifier after Hyper Parameter tuning","67767700":"# Base Line Models","56e8fe0d":"# 3. Random Forest Classifier","57e62763":"# EDA","383ef55c":"# Filling Null Values","e8c25b7e":"# 4. Gradient Boosting Classifier","2bb0c4e2":"Building models again using new training sets ","e755bece":"# 4. Correlation","52dea5ad":"Table of content\n1. Import Required Libraries\n2. Load Data\n3. Data Information\n4. Check for null values\n5. EDA\n    1. Check Outliers\n    2. Univariant Analysis\n    3. Bivariant Analysis\n    4. Multivariant Analysis\n    5. Correlation\n6. Encode Categorical Variables\n7. Spliting Data\n8. Base Line Models\n    1. Logistic Regression\n    2. XGBoost Classifier\n    3. Random Forest Classifier\n    4. Gradient Boosting Classifier\n    5. Stacking Classifier\n9. Balancing target variable\n10. Feature Selection\n11. Dimentionality Reduction\n12. Hyper Parameter Tuning\n13. Best Model","4d114004":"# 1. Check Outliers"}}