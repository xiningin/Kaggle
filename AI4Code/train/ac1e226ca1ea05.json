{"cell_type":{"946f549d":"code","221a842c":"code","2ec590ac":"code","c2050299":"code","929eea7e":"code","2093a771":"code","024a3570":"code","425b7dd5":"code","6fdf3a3e":"code","0345e833":"code","a5cf065c":"code","59fa60c0":"code","52bc6580":"code","5209c3df":"code","f97f6fbe":"code","08f84b9c":"code","71df39d2":"code","5d5a0e9e":"code","e514c50a":"code","40e72ae1":"code","61bf3639":"markdown","335ee33e":"markdown","c9ded0e9":"markdown","1479dbeb":"markdown","c898c205":"markdown","5479ea2a":"markdown","b4aae7a0":"markdown","88bd6275":"markdown","49cb618f":"markdown","bd5267e1":"markdown","9c27d06c":"markdown"},"source":{"946f549d":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n#the test set\n\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n#the train set\n\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\n\n\n\n\n","221a842c":"train.head(10)","2ec590ac":"train.describe()","c2050299":"train.info()","929eea7e":"def clean(train):\n    train = train.drop([\"Ticket\", \"Name\", \"Cabin\"])\n    \ntrain.describe()","2093a771":"sex_pivot = train.pivot_table(index=\"Sex\",values=\"Survived\")\nsex_pivot","024a3570":"pclass_pivot = train.pivot_table(index=\"Pclass\",values=\"Survived\")\n# pclass_pivot\npclass_pivot.plot.bar()\nplt.show()\n","425b7dd5":"train['Age'].describe()","6fdf3a3e":"survived = train[train[\"Survived\"] == 1]\ndied = train[train[\"Survived\"] == 0]\nsurvived[\"Age\"].plot.hist(alpha=0.5,color='blue',bins=50)\ndied[\"Age\"].plot.hist(alpha=0.5,color='red',bins=50)\nplt.legend(['Survived','Died'])\nplt.show()","0345e833":"def label_age(df,cut_points,label_names):\n    df[\"Age\"] = df[\"Age\"].fillna(-1)\n    df[\"Age groups\"] = pd.cut(df[\"Age\"],cut_points,labels=label_names)\n    return df\n\ncuts = [-1,0,18,150]\nlabel_names = [\"Missing\",\"Child\",\"Adult\"]\n\ntrain = label_age(train,cuts,label_names)\ntest = label_age(test,cuts,label_names)\n\nage_pivot = train.pivot_table(index=\"Age groups\",values=\"Survived\")\nage_pivot.plot.bar()\nplt.show()","a5cf065c":"column_name = \"Pclass\"\ndummies = pd.get_dummies(train[column_name],prefix='Pclass')\n#pd.get_dummies(train.Embarked, prefix='Embarked')\n\ndummies.head()","59fa60c0":"def create_dummies(train,column_name):\n    dummies = pd.get_dummies(train[column_name],prefix=column_name)\n    train = pd.concat([train,dummies],axis=1) #concat concatenates coloumns or rows on top of one another. In order to concat columns we use axis=1\n    return train\n","52bc6580":"#train = create_dummies(train,\"Sex\")\ntrain = create_dummies(train,\"Pclass\")\ntrain = create_dummies(train,\"Age groups\")\n\n#This can also be done use .map() to achieve the same thing.\n\ntrain['Sex_male'] = train.Sex.map({'female':0, 'male':1})\ntest['Sex_male'] = test.Sex.map({'female':0, 'male':1})\ntrain['Sex_female'] = train.Sex.map({'female':1, 'male':0})\ntest['Sex_female'] = test.Sex.map({'female':1, 'male':0})\n\n\n#train['Sex_male']\n#train['Sex_female']","5209c3df":"train.head()","f97f6fbe":"test = create_dummies(test,\"Pclass\")\ntest = create_dummies(test,\"Age groups\")\n#test = create_dummies(test,\"Sex\")\n\ntest.head()","08f84b9c":"columns = ['Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male',\n       'Age groups_Missing','Age groups_Child','Age groups_Adult']\n","71df39d2":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(train[columns], train['Survived']) #we use fit() to train our data set to that it can make predictions.","5d5a0e9e":"from sklearn.model_selection import train_test_split\ntest_prediction = test\n\nX = train[columns] #containing the abovementioned columns.\ny = train['Survived']\n\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2,random_state=50)\n","e514c50a":"\nlr.fit(X, y)\n\n\npredictions = lr.predict(test_prediction[columns])\n\ntest_prediction_ids = test_prediction[\"PassengerId\"]\n\nsubmission_df = {\"PassengerId\": test_prediction_ids, \"Survived\": predictions}\n\nsubmission = pd.DataFrame(submission_df)\n\n\nsubmission.to_csv('titanic_submission.csv', index=False)","40e72ae1":"print(submission)","61bf3639":"**TATANIC KAGGLE COMPETITION** with a 77% accuracy score.\n\n\nWe now want to explore the data by looking at it, identifying what types the dataset is made out of.\n\n\n\n- PassengerId is a coumn added by Kaggle to identify the rows of the dataset. \n- Survived describes if the passenger survived or not. 0 being no and 1 being yes.\n- Pclass - The class of the ticket the passenger purchased. 1 = 1st class, 2 = 2nd class, 3 = 3rd class\n- Name describes the name of the passenger\n- Sex describes the gender of the passenger\n- Age describes the age of the passenger\n- SibSp describes the number of siblings\/spouses\n- Parch describes the number of parents\/children \n- Ticket describes the ticket number of the passenger\n- Fare describes the cost of the passengers ticket.\n- Cabin describes the cabin number of the passengers. Some of the numbers are unknown = NaN\n- Embarked describes where the passenger was embarked with the Titanic (C=Cherbourg, Q=Queenstown, S=Southampton\n\n\n\n\n\n\n","335ee33e":"There also seems to be a correlation between which class the passengers travelled with. Looking at a numerical data-value such as age, we can gather some more information on the age coloumn using describe()","c9ded0e9":"As we can see, we're still left with the columns that give us the necessary data such as Age and Pclass. Since every column won't be investigated, removing unnecessary columns will help us get a better viewpoint of the data","1479dbeb":"We use Logistic regression (lg) as a classification algorithm that can be used to predict the probability of a categorical dependent variable. Therefore, by using predict() the model does exactly that, and tries to predict the values based on probability. We have chosen to use logistic regression due to it being simple to use as a classification technique. \n\nThe columns variable now has all the relevant information so that we may use the fit() function, in order for us to use the predict() function.","c898c205":"Let us now import train_test_split\n\nthe train_test_split() function can take two parameters: X and y. These parameters have all the data that we want to train and test on. Further, the function returns four objects back: test_X, test_y, train_X and train_y.","5479ea2a":"We have now converted the age data into different groups:\n\n- The missing label which shows if a value is missing in the age coloumn and has the value -1. \n- The Child label which puts a passenger into the child category if their age is between 0-18\n- The Adult label which puts a passenger into the adult category if their age is between 18-100. \n\nWe know that no passengers have an age above 100, but just in-case we put the age at 150 to ensure reproduciability in our code, so that it may apply to similar sets of data.\n\nAs can be seen on the barchart, we have some missing values as already mentioned. We can also notice, that children have a larger success rate than adults. We used the fillna() function to fill out the missing variables in the age column.\n\n\nFor our next step, we turn our attention towards the Pclass column.\n\nWe will now create dummy variables by using the get_dummies call which converts series into dummy codes. In other words, it turns categorical variables to dummy\/indicator variables. get_dummes sets up a coloumn for each of the possible values, and inputs a 1 into a column if the passenger travelled with that specific class. **the same could be done for the embarked column.** We use the dummy variables so that we may include a categorical feature (such as Pclass or Embarked) into our machine learning model. Machine learning models are not capable of understanding categorical data, so we need to make sure that the model is going to work. We specifically use dummy variables on Pclass since it is a variable that has no quantifiable relationship. This is because the relation between each class is not the same as the relation between the numbers , 2, and 3 - we want to remove this relationship by creating dummy columns for each specific value in the Pclass column.","b4aae7a0":"As can be seen, the abovementioned data shows a bunch of different categorical and numerical attributes regarding passengers on board the Titanic. These attributes mostly consist of different data types such as integers and floats.\n\nAs we will try to showcase, some of the variables such as gender have a large impact on the survival rates of passengers, while other variable such as Embarked might not be as impactful. In this particular investigation, we will mainly focus on three different attributes:\n\n- The age of passengers\n- The gender of passengers\n- The wealth of passengers\n\nThe age of passengers is interesting - did children have a higher chance of survival than men? We know that historically this must be true. \nThe gender of passengers follows the same thought - historically, we know that women and children were proiritized so they should have a higher survival rate than men. \nThe wealth of passengers (e.g. what class they travelled with) is also interesting - did passengers travelling with first class having a higher survival ratio than passengers on the third class?\nLastly, the title of passengers is interesting, as some passengers are doctors, countesses, and more.\n\n**Getting an overview of the data**\n","88bd6275":"The red bars indicate the death rates, and the blue bars the survival rates. It can be hard to see if there is an actual correlation between the age and the survival rates of passengers, but we can see that the blue lines out range the red lines at certain points.\n\nSince age is a continous variable we want to convert it to a categorical one. At this point, we have certain age ranges as previously shown and we wish to gather certain ages into certain groups. To do this, we use the pandas.cut(). We also want to make sure that we include both the training and test set, so that we do not make changes in only one of them, as this would have an impact on our predictions. \n\nLet us now put a label on the different age groups.","49cb618f":"With that being said, some of the data such as name, ticket, passengerid and cabin will not be investigated. This decision was made due to time constraints, although it would have been interesting to see if passengers with certain titles such as: \"the countess\" as well as other high-ranking titles, would increase the passengers survival rate. However, let us drop those coloumns and see what we're left with.\n    ","bd5267e1":"Just by looking at the pure survival rates between males and females it should be clear, that predicting that females would survive rather than males, would already give us a good standpoint for predicting which passengers would or would not survive.\n\nLet us now have a look at the survival rates in the different travelling classes.","9c27d06c":"We gain some different data from using describe() such as the mean of the age of passengers on the Titanic, the standard deviation, the minimum value and the maximum value. We can also observe, that only 714 values are contained in the Age column, meaning that some of the passengers have no age listed in the coloumn. This means that we later on either have to fill in those missing values, or remove the column entirely in order to properly work with the dataset.\n\nTo gain a more visualized understanding of the correleation between age and survival rates, let us create a histogram that compares the two values."}}