{"cell_type":{"57305120":"code","9c012a96":"code","162f8a88":"code","460e28a2":"code","b1993933":"code","40cf508a":"markdown","a474a793":"markdown","dc802ab2":"markdown","aea2a898":"markdown"},"source":{"57305120":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# load data\npath = '\/kaggle\/input\/'\n# mnist or gauss\nm_x = np.loadtxt(fname='%s_x'%(path+'mnist'), delimiter=' ')\nm_y = np.loadtxt(fname='%s_y'%(path+'mnist'), delimiter=' ')\ng_x = np.loadtxt(fname='%s_x'%(path+'gauss'), delimiter=' ')\ng_y = np.loadtxt(fname='%s_y'%(path+'gauss'), delimiter=' ')\n\n# show data\n## mnist\ndata = np.reshape(np.array(m_x[0], dtype=int), [28, 28])\nprint(data)\n\n## gauss\nplt.scatter(g_x[:, 0], g_x[:, 1], c=g_y)\nplt.show()","9c012a96":"# split data\nthredhold = int(len(m_x) * 0.8)\ntrain_data = {'x': m_x[:thredhold], 'y': m_y[:thredhold]}\ntest_data = {'x': m_x[thredhold:], 'y': m_y[thredhold:]}","162f8a88":"class KNN():\n    def __init__(self, k, label_num):\n        self.k = k\n        self.label_num = label_num\n\n    def fit(self, train_data):\n        self.train_data = train_data\n        # from scipy import spatial\n        # self.kdtree = spatial.KDTree(data=self.train_data['x'])#\n\n    def predict(self, test_x):\n        predicted_test_labels = np.zeros(shape=[len(test_x)], dtype=int)\n        for x, i in zip(test_x, np.arange(len(test_x))):\n            predicted_test_labels[i] = self.get_label(x)\n        return predicted_test_labels\n\n    def get_label(self, x):\n        knn_indexes = self.get_knn_indexes(x)\n        label_statistic = np.zeros(shape=[self.label_num])\n        for index in knn_indexes:\n            label = int(self.train_data['y'][index])\n            label_statistic[label] += 1\n        return np.argmax(label_statistic)\n\n    def get_knn_indexes(self, x):\n        # return self.kdtree.query(x, k=self.k)[1]\n        dis = list(map(lambda a: self.distance(a, x), self.train_data['x']))\n        knn_pairs = sorted(zip(dis, np.arange(len(dis))), key=lambda x: x[0])[:self.k]\n        knn_indexes = [p[1] for p in knn_pairs]\n        return knn_indexes\n\n    def distance(self, a, b):\n        return np.sqrt(np.sum(np.square(a-b)))","460e28a2":"for k in range(1, 10):\n    # fit knn\n    knn = KNN(k, label_num=10)\n    knn.fit(train_data)\n    predicted_labels = knn.predict(test_data['x'])\n\n    # evaluate\n    accuracy = np.mean(np.equal(predicted_labels, test_data['y']))\n    print('k: %2d, accuracy: %.3f' % (k, accuracy))","b1993933":"from sklearn.neighbors import KNeighborsClassifier\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = g_x\ny = g_y\n\nstep = 0.02\nx_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\ny_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, step), np.arange(y_min, y_max, step))\ngrid_data = np.c_[xx.ravel(), yy.ravel()]\n\nknn1 = KNeighborsClassifier(n_neighbors=10)\nknn1.fit(x, y)\nz1 = knn1.predict(grid_data)\n\nknn2 = KNeighborsClassifier(n_neighbors=1)\nknn2.fit(x, y)\nz2 = knn2.predict(grid_data)\n\ncmap_light = ListedColormap(['#FF9999', '#AAFFAA'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00'])\nfig = plt.figure(figsize=(16,6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\nax1.pcolormesh(xx, yy, z1.reshape(xx.shape), cmap=cmap_light)\nax1.scatter(x[:, 0], x[:, 1], c=y, cmap=cmap_bold)\nax2.pcolormesh(xx, yy, z2.reshape(xx.shape), cmap=cmap_light)\nax2.scatter(x[:, 0], x[:, 1], c=y, cmap=cmap_bold)\nplt.show()","40cf508a":"## KNN in scikit-learn","a474a793":"## Implementation","dc802ab2":"## KNN\nFor regression problem, the k-nearest neighbor prediction is defined as:\n\\begin{aligned}\n    \\hat Y(x) = \\sum_{x_i\\in N_k(x)}\\omega_i y_i,\n\\end{aligned}\nwhere $N_k(x)$ is the neighborhood of x defined by the k closest points $x_i$ in the training samples, and $\\sum_{x_i\\in N_k(x)}\\omega_i = 1$.\n\nLet $\\omega_i = \\frac{1}{k}$, we have:\n\\begin{aligned}\n    \\hat Y(x) = \\frac{1}{k}\\sum_{x_i\\in N_k(x)}y_i.\n\\end{aligned}\n\nFor classification problem, we can utilize the voting mechanism to conduct prediction, which can be formulated as:\n$$\nG_j(x) = \\sum_{x_i\\in N_k(x)}I(y_i=j),\n$$\n\n$$\n\\hat Y(x) = \\mathop{\\arg\\max}_j G_j(x),\n$$\nwhere $I()$ is an indicator function which returns 1 if the statement is true and returns 0 otherwise.","aea2a898":"## Datasets"}}