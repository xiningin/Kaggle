{"cell_type":{"8a5efa66":"code","17082ff1":"code","ec9fbde9":"code","fdf84ae2":"code","e33c413c":"code","be7a4656":"code","a724e8d7":"code","e3482a5b":"code","144e1533":"code","9037bf11":"code","9326db60":"code","ce1f993e":"code","5829b6e7":"code","d44ebe24":"code","edb1e1b5":"code","c9dd466d":"code","b84d8a0b":"code","8394d222":"code","4825f085":"code","bca4fa90":"code","6ce29c95":"code","a904748b":"code","dd27f050":"code","f64a3ab8":"code","864815a6":"code","248143c4":"code","9dd39098":"code","8ccb28b3":"code","b62116d3":"markdown","ef20d291":"markdown","7ce31340":"markdown","0e35557a":"markdown","242639f0":"markdown","860dc659":"markdown","09a64adf":"markdown","cb246421":"markdown","f60e2946":"markdown","9e59d588":"markdown","444910a6":"markdown","b07be257":"markdown","ab24eb19":"markdown","67fda640":"markdown","7652437c":"markdown","7550dc5a":"markdown","231cc5c0":"markdown","5d630b0b":"markdown","b5b26ff6":"markdown","868cf60f":"markdown","b35b38d3":"markdown","c5899acc":"markdown","d04bca2d":"markdown","a95b512c":"markdown"},"source":{"8a5efa66":"# General Libraries\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# LinAlg and Dataframes\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Pre-processing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Models\nfrom sklearn import tree\nfrom sklearn.naive_bayes import GaussianNB\n\n# Scores\nfrom sklearn.metrics import accuracy_score","17082ff1":"titanic_dataset = pd.read_csv(\"..\/input\/train.csv\")\ntitanic_dataset.head()","ec9fbde9":"titanic_dataset.info()","fdf84ae2":"titanic_dataset[\"Name\"].value_counts()","e33c413c":"titanic_dataset[\"Sex\"].value_counts()","be7a4656":"titanic_dataset[\"Ticket\"].value_counts()","a724e8d7":"titanic_dataset[\"Cabin\"].value_counts()","e3482a5b":"titanic_dataset[\"Embarked\"].value_counts()","144e1533":"titanic_dataset.hist(bins=100,figsize=(20,15))\nplt.show()","9037bf11":"corr_matrix = titanic_dataset.corr()\ncorr_matrix[\"Survived\"].sort_values(ascending=False)","9326db60":"\"\"\"\nDropping the attributes that seems to have absolutely no correlation with Survival of Passenger\n\"\"\"\ntitanic_dataset = titanic_dataset.drop(\"Name\",axis=1)\ntitanic_dataset = titanic_dataset.drop(\"Cabin\",axis=1)\ntitanic_dataset = titanic_dataset.drop(\"Ticket\",axis=1)\ntitanic_dataset = titanic_dataset.drop(\"PassengerId\",axis=1)\ntitanic_dataset.head()","ce1f993e":"mean_age = titanic_dataset[\"Age\"].mean()\ntitanic_dataset[\"Age\"].fillna(mean_age,inplace=True)","5829b6e7":"titanic_dataset.info()","d44ebe24":"titanic_dataset = titanic_dataset.dropna(subset=[\"Embarked\"])\ntitanic_dataset.info()","edb1e1b5":"titanic_categorical = titanic_dataset[[\"Sex\",\"Embarked\"]]\ntitanic_categorical.head(10)","c9dd466d":"titanic_categorical.info()","b84d8a0b":"titanic_dataset['Sex_Encoded'] = LabelEncoder().fit_transform(titanic_dataset['Sex'])\ntitanic_dataset[['Sex', 'Sex_Encoded']]","8394d222":"titanic_dataset['Embarked_Encoded'] = LabelEncoder().fit_transform(titanic_dataset['Embarked'])\ntitanic_dataset[['Embarked', 'Embarked_Encoded']]","4825f085":"titanic_dataset.info()","bca4fa90":"titanic_dataset = titanic_dataset.drop(\"Sex\",axis=1)\ntitanic_dataset = titanic_dataset.drop(\"Embarked\",axis=1)\ntitanic_dataset.info()","6ce29c95":"X_attr = [\"Pclass\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Sex_Encoded\",\"Embarked_Encoded\"]\nX = titanic_dataset[X_attr]\ny_attr = [\"Survived\"]\ny = titanic_dataset[y_attr]","a904748b":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42)","dd27f050":"DT_clf = tree.DecisionTreeClassifier()\nDT_clf.fit(X_train,y_train)\nDT_preds = DT_clf.predict(test_set)","f64a3ab8":"x = [1,2,3,4,5,6,7,8,9,10]\nscores = []\nfor max_depth in range(1,11):\n    DT_clf_tuned = tree.DecisionTreeClassifier(max_depth=max_depth)\n    DT_clf_tuned.fit(X_train,y_train)\n    DT_clf_tuned_preds = DT_clf_tuned.predict(X_valid)\n    scores.append(accuracy_score(y_valid,DT_clf_tuned_preds))\nplt.plot(x,scores)\nplt.show()\nprint(scores)","864815a6":"test_set = pd.read_csv(\"..\/input\/test.csv\")\n\"\"\"\nSimilar pre-processing with test data as well\n1. Drop useless Features\n2. Convert Object\/Strings to Numerical Type\n3. Fill NAs for Numerical Attributes\n\"\"\"\n# Step 1\ntest_set = test_set.drop(\"Name\",axis=1)\ntest_set = test_set.drop(\"Cabin\",axis=1)\ntest_set = test_set.drop(\"Ticket\",axis=1)\ntest_set = test_set.drop(\"PassengerId\",axis=1)\n# Step 2\ntest_set['Sex_Encoded'] = test_set['Sex'].map( {'male':1, 'female':0} )\ntest_set['Embarked_Encoded'] = test_set['Embarked'].map( {'C':0, 'Q':1, 'S':2})\ntest_set = test_set.drop('Sex',axis=1)\ntest_set = test_set.drop('Embarked',axis=1)\n# Step 3\nmean_test_age = test_set[\"Age\"].mean()\nmean_fare = test_set[\"Fare\"].mean()\ntest_set[\"Age\"].fillna(mean_test_age,inplace=True)\ntest_set[\"Fare\"].fillna(mean_fare,inplace=True)","248143c4":"DT_final = tree.DecisionTreeClassifier(max_depth=3)\nDT_final.fit(X_train,y_train)\nfinal_preds = DT_final.predict(test_set)","9dd39098":"x = [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\nscores = []\nfor min_samples_split in range(2,21):\n    DT_clf_tuned = tree.DecisionTreeClassifier(max_depth=3,min_samples_split=min_samples_split)\n    DT_clf_tuned.fit(X_train,y_train)\n    DT_clf_tuned_preds = DT_clf_tuned.predict(X_valid)\n    scores.append(accuracy_score(y_valid,DT_clf_tuned_preds))\nplt.plot(x,scores)\nplt.show()\nprint(scores)","8ccb28b3":"GNB_clf = GaussianNB()\nGNB_clf.fit(X_train,y_train)\nGNB_preds = GNB_clf.predict(test_set)","b62116d3":"78.71% is pretty good, but let us try to tune the parameters to get better accuracy","ef20d291":"Similar case as Ticket.","7ce31340":"Booo!! For the attribute name, it looks like there are no repetitions so we might not find any patterns. So we can declare that Name is a useless feature.","0e35557a":"**value_counts()** is used to get the individual number of counts for each possible value in the column. We use it to see if the values an attribute takes is repetitive or not.","242639f0":"# Conclusion: DT(max_depth=3), probably the most optimal DT","860dc659":"Yayy! 1 down!!","09a64adf":"Oops! looks like there is some repetition but almost practically useless","cb246421":"So underwhelming to see only 3 lines of actual building of model. I'm sorry, but that is just the way real life is...","f60e2946":"Now that we have got some idea about the data and removed useless attributes, let's take care of missing values. This happened with Age and Embarked. And Age is the only numerical attribute, so we first fill NA with mean.","9e59d588":"Now we can drop the categorical values, with this our preprocessing is completed.","444910a6":"### Parameter 2: Min Samples Split\n","b07be257":"No significant change in the validation set accuracy, so we leave this at default","ab24eb19":"## Attempt 1:  Decision Tree","67fda640":"From the above graph it is clear that when **max_depth=3**, the Decision Tree performs the best. We need to this (plotting graph and finding ideal max_depth) with **validation set** (a part of training dataset). \n\n> Generally, when the **max depth** is ~ **no.features\/2**, the model will have maximum accuracy.","7652437c":"To get some more insight into the data, we'll start with simple histograms\n> Note: Histograms make sense only when the data is numerical","7550dc5a":"We still have 2 null objects for Embarked, it is better to drop those 2 datapoints because there is no point in filling that NA with some other values for Categorial data types","231cc5c0":"### Parameter 1: Maximum Depth of Decision Tree\nIf we allow the DT to grow till the maximum depth then we are overfitting to the training data, so we try to find the fine balance between depth and overfitting.","5d630b0b":"Yayy! We found another","b5b26ff6":"Woohoo!!! The preprocessing is done, now we have to build a classifier, majority of our attributes are categorical in nature so I'm thinking of using a Decision Tree Classifier.","868cf60f":"As we can observe there are 891 data points. But there are few problems:\n\n1. Only 714 non-null Age ==> 177 Null Age\n2. Only 204 non-null Cabin ==> 687 Null Cabin\n3. Only 889 non-null Embarked ==> 2 Null Embarked\n4. Name, Sex, Ticket, Cabin, Embarked are object types, but we loaded it from CSV so these are strings\/text attribute\n\nTo remove the problem #4 we try to see if these are repetitive, to find if there are any categories","b35b38d3":"The test accuracy for this is 0.7799 ~ 0.78","c5899acc":"Even after all this cleaning, we still have a little bit more to do. Convert Sex, Embarked objects to numerical types","d04bca2d":"## Attempt 2: Naive Bayes","a95b512c":"So,\n* **Categories Exist**: Sex,Embarked\n* **Do not Exist**: Name\n* **Partial but almost useless**: Ticket, Cabin"}}