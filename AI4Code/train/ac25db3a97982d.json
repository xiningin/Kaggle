{"cell_type":{"096ada5c":"code","b1cf4ac3":"code","26f98b7d":"code","15af06ea":"code","9c9fe2e8":"code","6e6c5428":"code","65607b94":"code","f1e59d80":"code","d4b0065b":"code","7f71c994":"code","dc0576d1":"code","be37b6bc":"code","30e88991":"code","0c2a0338":"code","52958f20":"code","d7628ad0":"code","40674556":"code","dbe694d8":"code","2ca45538":"code","3ea8774d":"code","c5b8dbb9":"code","3dc77cec":"code","96595136":"code","3045e79d":"code","2b85dceb":"code","0a498afe":"code","d3a45265":"code","73a83af2":"code","be0e0461":"code","9f911794":"markdown","555c4ba3":"markdown","112e3210":"markdown","1ff5025d":"markdown","693f8d7f":"markdown","12e4a8a5":"markdown","8c875b77":"markdown"},"source":{"096ada5c":"#B\u00e0i t\u1eadp ng\u00e0y 2 - Linear Regression\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b1cf4ac3":"\n\nimport matplotlib.pyplot as plt\nimport math\n\n","26f98b7d":"\n\nftPath = \"\/kaggle\/input\/housingprice\/ex1data2.txt\"\nlines = [] # Living area & number of bedroom\nwith open(ftPath) as f:\n    for line in f:\n        a = [float(i) for i in line.strip().split(',')]\n        lines.append(a)\ndata = np.array(lines)\n\n","15af06ea":"X = data[:,[0, 1]]\ny=data[:,[2]]\na = X[:, 0]","9c9fe2e8":"m = len(a)\nA = 1\/(2*m) * (a.T.dot(a))\nB = -1\/(2*m) * (a.T.dot(y))\nC = 1\/(2*m) * (y.T.dot(y))\nprint(A)\nprint(B)\nprint(C)\n\n# 2310419.212765957 -382104564.09574467 65591548106.45744","6e6c5428":"plt.figure(figsize=(10, 5))\ntheta = np.linspace(-230, 400, 100)\nJ = A*theta**2 + B*theta + C\n\n# Gives a new shape to an array without changing its data.\nplt.plot(theta, J.reshape(-1, 1), color='r')\n\nplt.xlabel(\"Theta\")\nplt.ylabel(\"Cost function\")\nplt.grid(True)\nplt.show()","65607b94":"# TEST CODE\n# aaa = np.arange(12).reshape((6, 2))\n# print(aaa)\n# print(np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])])","f1e59d80":"# Solve the problem for theta0, theta1\nfrom sklearn.linear_model import LinearRegression \nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\nprint(lin_reg.intercept_)\nprint(lin_reg.coef_)","d4b0065b":"from sklearn.linear_model import LinearRegression \nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\nprint(lin_reg.intercept_, lin_reg.coef_)","7f71c994":"m = len(X)\n# Translates slice objects to concatenation along the second axis.\n# add slice 1..1 to X\nXb = np.c_[np.ones((m, 1)), X]\n","dc0576d1":"theta_best = np.array([[89597.9], [139.21067402], [-8738.01911233]])\nerror = 1\/m * math.sqrt((Xb.dot(theta_best)-y).T.dot(Xb.dot(theta_best)-y))\nprint(\"MSE:\", error)\nprint(theta_best)","be37b6bc":"c = len(Xb[0])\n# Return a sample\n# (or samples) from the \u201cstandard normal\u201d distribution.\n\ntheta = np.random.randn(c,1)\nprint(c)\nprint(theta)\neta = 2e-7","30e88991":"# Each iteration\ntmp1 = Xb.dot(theta) - y\ngradients = (2\/m) * Xb.T.dot(Xb.dot(theta) - y)\ntheta = theta - eta * gradients\nerr = 1\/m * math.sqrt((Xb.dot(theta)-y).T.dot(Xb.dot(theta)-y))\nprint(\"MSE:\", err) \nprint(theta)","0c2a0338":"# TODO: Add early stopping criteria if err doesn't decrease more than epsilon value\n# TODO: Check the number of iteration to converge\ni = 1\nepsilon1=0.0005\ne0=err+1+epsilon1\n   \nwhile (e0-err>epsilon1):\n    e0=err\n    tmp1 = Xb.dot(theta) - y\n    gradients = (2\/m) * Xb.T.dot(Xb.dot(theta) - y)\n    theta = theta - eta * gradients\n    err = 1\/m * math.sqrt((Xb.dot(theta)-y).T.dot(Xb.dot(theta)-y))\n    i = i + 1\nprint(i)\nprint(\"err:   \",err)","52958f20":"plt.figure(figsize=(10, 5))\nplt.title(\"Batch gradient descent\", fontsize=16)\n\n# plt.axis([140, 190, 45, 75])\nplt.xlabel(\"Living area\")\nplt.ylabel(\"Price\")\nxLine = np.array([700, 4500])\ntheta = np.array([[90000], [120], [-8000]])\nyBatchGradient = theta[0,0] + theta[1,0]*xLine\nplt.plot(xLine, yBatchGradient, marker=\"^\", color=\"blue\")\n\neta = 2e-7\ngradients = (2\/m) * Xb.T.dot(Xb.dot(theta) - y)\ntheta = theta - eta * gradients\nerr = 1\/m * math.sqrt((Xb.dot(theta)-y).T.dot(Xb.dot(theta)-y))\n\n# TODO: Add early stopping criteria if err doesn't decrease more than epsilon value\n# TODO: Check the number of iteration to converge\ni = 1\nepsilon2=0.0006\ne0=err+1+epsilon2\naErr=np.array([])\nwhile (e0-err>epsilon2):\n    e0=err\n    gradients = (2\/m) * Xb.T.dot(Xb.dot(theta) - y)\n    theta = theta - eta * gradients\n    err = 1\/m * math.sqrt((Xb.dot(theta)-y).T.dot(Xb.dot(theta)-y))\n    aErr=np.append(aErr,err)\n    i = i + 1\n    yBatchGradient = theta[0,0] + theta[1,0]*xLine\n    plt.plot(xLine, yBatchGradient, marker=\".\", color=\"r\")\n    \nplt.plot(xLine, yBatchGradient, marker=\"^\", color=\"black\")\nplt.plot(a, y, \"bo\")\n\nplt.grid(True)\nplt.show()","d7628ad0":"plt.figure(figsize=(10, 5))\nplt.title(\"Batch gradient descent\", fontsize=16)\nplt.xlabel(\"i\")\nplt.ylabel(\"error\")\nplt.plot(np.array(range(aErr.shape[0])), aErr, \"bo\")\nplt.grid(True)\nplt.show()","40674556":"from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 unused import\nfig = plt.figure(figsize=(12, 7))\nax = fig.add_subplot(111, projection='3d')\nplt.title(\"Batch gradient descent\", fontsize=16)\n\ntheta = np.array([[90000], [120], [-8000]])\nax.scatter(theta[0],theta[1],theta[2],marker=\"^\",color='blue') \neta = 2e-7\ngradients = (2\/m) * Xb.T.dot(Xb.dot(theta) - y)\ntheta = theta - eta * gradients\nerr = 1\/m * math.sqrt((Xb.dot(theta)-y).T.dot(Xb.dot(theta)-y))\nprint(\"MSE:\", err)\nprint(theta)\n\ni = 1\nepsilon2=0.0006\ne0=err+1+epsilon2\nwhile (e0-err>epsilon2):\n    e0=err\n    gradients = (2\/m) * Xb.T.dot(Xb.dot(theta) - y)\n    theta = theta - eta * gradients\n    err = 1\/m * math.sqrt((Xb.dot(theta)-y).T.dot(Xb.dot(theta)-y))\n    i = i + 1\n    ax.scatter(theta[0],theta[1],theta[2],marker=\".\",color='r')\nax.scatter(theta[0],theta[1],theta[2],marker=\"^\",color='black') \nax.set_xlabel('\u03b8_zero')\nax.set_ylabel('\u03b8_one')\nax.set_zlabel('\u03b8_two')\nplt.show()","dbe694d8":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(X)\nscaleX = scaler.transform(X)\nscaleXb = np.c_[np.ones((m, 1)), scaleX]\n","2ca45538":"c = len(scaleXb[0])\ntheta = np.random.randn(c,1)\nprint(theta)\neta = 0.05","3ea8774d":"for i in range(1000):\n    tmp1 = scaleXb.dot(theta) - y\n    gradients = (2\/m) * scaleXb.T.dot(scaleXb.dot(theta) - y)\n    theta = theta - eta * gradients\n    err = 1\/m * math.sqrt((scaleXb.dot(theta)-y).T.dot(scaleXb.dot(theta)-y))\n    if (i>995):\n        print(i)\n        print(theta)\n        print(\"MSE:\", err)\n        print(\"-\"*10)","c5b8dbb9":"# TODO Question: How to convert theta from scaled value back to normal value? Derive the equation.\n\nmax_=np.array([1,np.amax(data[:,0]),np.amax(data[:,1])])\nmin_=np.array([1,np.amin(data[:,0]),np.amin(data[:,1])])\ndelta=max_-min_\nprint(delta)\nrevert_theta=np.zeros((3,1))\nrevert_theta[1,0]=theta[1,0]\/delta[1]\nrevert_theta[2,0]=theta[2,0]\/delta[2]\nrevert_theta[0,0]=theta[0,0]-revert_theta[1,0]*min_[1]-revert_theta[2,0]*min_[2]\n\nprint(theta)\nprint(\"-\"*20)\nprint(revert_theta)\n\n","3dc77cec":"plt.figure(figsize=(10, 5))\nplt.title(\"MinMaxScaler\", fontsize=16)\n\nplt.plot(a, y, \"bo\")\nplt.xlabel(\"Living area\")\nplt.ylabel(\"Price\")\nxLine = np.array([700, 4500])\nyLine = 50000 + 60*xLine\n\n#my new\nyRevert = revert_theta[0,0] + revert_theta[1,0]*xLine\nplt.plot(xLine, yRevert, marker=\"^\", color=\"r\")\nplt.grid(True)\nplt.show()\n","96595136":"fig = plt.figure(figsize=(12, 7))\nax = fig.add_subplot(111, projection='3d')\nplt.title('Stochastic Gradient Descent', fontsize=16)\n\nnEpochs = 20\nt0, t1 = 2, 10000000 # learning schedule hyperparameters\ndef learningSchedule(t): \n    return t0\/(t+t1)\n\nc = len(Xb[0])\ntheta = np.random.randn(c,1)*20 # random initialization\nprint(theta)\nax.scatter(theta[0],theta[1],theta[2],marker=\"o\",color='blue') \nfor epoch in range(nEpochs):\n    values=np.array([])\n    for i in range(m):\n        k=np.random.randint(m)\n        while k in values:\n            k=np.random.randint(m)\n        values=np.append(values,[k])\n        gradients=1\/m*(Xb[k].dot(theta) - y[k])*Xb[k].reshape(3,1)\n        eta = learningSchedule(epoch*m + i)\n        theta = theta - eta *gradients \n        err = 1\/m * math.sqrt((Xb.dot(theta)-y).T.dot(Xb.dot(theta)-y))\n        ax.scatter(theta[0],theta[1],theta[2],marker=\".\",color='r')\nax.scatter(theta[0],theta[1],theta[2],marker=\"o\",color='black') \nax.set_xlabel('\u03b8_zero')\nax.set_ylabel('\u03b8_one')\nax.set_zlabel('\u03b8_two')\nplt.show()","3045e79d":"print(theta)","2b85dceb":"plt.figure(figsize=(10, 5))\n# plt.axis([140, 190, 45, 75])\nplt.xlabel(\"Living area\")\nplt.ylabel(\"Price\")\nxLine = np.array([700, 4500])\nplt.title('Stochastic Gradient Descent', fontsize=16)\n\nnEpochs = 20\nt0, t1 = 2, 10000000 # learning schedule hyperparameters\ndef learningSchedule(t): \n    return t0\/(t+t1)\n\nc = len(Xb[0])\ntheta = np.random.randn(c,1)*20 # random initialization\nprint(theta)\n\nfor epoch in range(nEpochs):\n    values=np.array([])\n    for i in range(m):\n        k=np.random.randint(m)\n        while k in values:\n            k=np.random.randint(m)\n        values=np.append(values,[k])\n        gradients=1\/m*(Xb[k].dot(theta) - y[k])*Xb[k].reshape(3,1)\n        eta = learningSchedule(epoch*m + i)\n        theta = theta - eta *gradients \n        err = 1\/m * math.sqrt((Xb.dot(theta)-y).T.dot(Xb.dot(theta)-y))\n        Stoch2D= theta[0] + theta[1]*xLine\n        plt.plot(xLine, theta[0] + theta[1]*xLine, marker=\".\", color=\"cyan\",alpha=(nEpochs-epoch)\/nEpochs)\n        if (i==0 and epoch==0):\n            plt.plot(xLine, theta[0] + theta[1]*xLine, marker=\".\", color=\"blue\",alpha=(nEpochs-epoch)\/nEpochs)\nplt.plot(xLine, theta[0] + theta[1]*xLine, marker=\"^\", color=\"black\")    \nplt.plot(a, y, \"bo\")\nplt.grid(True)\nplt.show()\n\n# colors = [cm(2.*i\/15) for i in range(20)]\n# colored = [colors[k] for k in df['dest_cluster']]\n\n# ax.scatter(df.dropoff_longitude,df.dropoff_latitude,color=colored,s=0.0002,alpha=1)\n","0a498afe":"fig = plt.figure(figsize=(12, 7))\nax1 = fig.add_subplot(111, projection='3d')\nplt.title('Stochastic Gradient Descent', fontsize=16)\nax1.scatter(Xb[:,1],Xb[:,2],y.reshape(m),marker=\"o\",color='r')\nax1.scatter(Xb[:,1],Xb[:,2],Xb.dot(theta),marker=\"*\",color='black')\n\n#my new\nax1.set_xlabel('X1_area')\nax1.set_ylabel('X2_room_number')\nax1.set_zlabel('Y_price')\nplt.show()","d3a45265":"fig = plt.figure(figsize=(12, 7))\nax1 = fig.add_subplot(111, projection='3d')\nplt.title('Mini-batch Gradient Descent', fontsize=16)\n\n\nnEpochs = 100\nbatchSize = 10\nt0, t1 = 2, 10000000 # learning schedule hyperparameters\n\ndef learningSchedule(t):\n    return t0\/(t+t1)\nc = len(Xb[0])\ntheta = np.random.randn(c,1) # random initialization\nprint(\"theta random:\",theta)\nax1.scatter(theta[0],theta[1],theta[2],marker=\"*\",color='blue')\n\nfor epoch in range(nEpochs):\n    values=np.array([])\n    while m-values.shape[0]>=batchSize:\n        Xbatch=np.zeros((0,3))\n        ybatch=np.zeros((0,1))\n        for i in range(batchSize):\n            k=np.random.randint(m)\n            while k in values:\n                k=np.random.randint(m)\n            values=np.append(values,[k])\n            Xbatch=np.append(Xbatch,Xb[k].reshape(1,3),axis=0)\n            ybatch=np.append(ybatch,y[k].reshape(1,1),axis=0)\n        gradients=1\/batchSize*Xbatch.T.dot(Xbatch.dot(theta)-ybatch)\n        eta = learningSchedule(epoch*batchSize)\n        theta = theta - eta * gradients\n        err = 1\/batchSize * math.sqrt((Xbatch.dot(theta)-ybatch).T.dot(Xbatch.dot(theta)-ybatch))\n        ax1.scatter(theta[0],theta[1],theta[2],marker=\".\",color='r',alpha=(nEpochs-epoch)\/nEpochs)\nax1.scatter(theta[0],theta[1],theta[2],marker=\"o\",color='black')        \nprint(\"theta final:\",theta)\nax1.set_xlabel('\u03b8\u03b8\u03b8\u03b8\u03b8_khong')\nax1.set_ylabel('\u03b8\u03b8\u03b8\u03b8\u03b8_mot')\nax1.set_zlabel('\u03b8\u03b8\u03b8\u03b8\u03b8_hai')\nplt.show()","73a83af2":"plt.figure(figsize=(10, 5))\n# plt.axis([140, 190, 45, 75])\nplt.xlabel(\"Living area\")\nplt.ylabel(\"Price\")\nplt.title('Mini-batch Gradient Descent', fontsize=16)\n\nxLine = np.array([700, 4500])\n\n\nnEpochs = 20\nbatchSize = 10\nt0, t1 = 2, 10000000 # learning schedule hyperparameters\n\ndef learningSchedule(t):\n    return t0\/(t+t1)\nc = len(Xb[0])\ntheta = np.array([[100],[1],[10]]) # random initialization\nprint(\"theta random:\",theta)\nplt.plot(xLine, theta[0] + theta[1]*xLine, marker=\"o\", color=\"blue\")    \nfor epoch in range(nEpochs):\n    values=np.array([])\n    while m-values.shape[0]>=batchSize:\n        Xbatch=np.zeros((0,3))\n        ybatch=np.zeros((0,1))\n        for i in range(batchSize):\n            k=np.random.randint(m)\n            while k in values:\n                k=np.random.randint(m)\n            values=np.append(values,[k])\n            Xbatch=np.append(Xbatch,Xb[k].reshape(1,3),axis=0)\n            ybatch=np.append(ybatch,y[k].reshape(1,1),axis=0)\n        gradients=1\/batchSize*Xbatch.T.dot(Xbatch.dot(theta)-ybatch)\n        eta = learningSchedule(epoch*batchSize)\n        theta = theta - eta * gradients\n        err = 1\/batchSize * math.sqrt((Xbatch.dot(theta)-ybatch).T.dot(Xbatch.dot(theta)-ybatch))\n        ax1.scatter(theta[0],theta[1],theta[2],marker=\".\",color='r')        \n        Stoch= theta[0] + theta[1]*xLine\n        plt.plot(xLine, theta[0] + theta[1]*xLine, marker=\".\", color=\"r\",alpha=epoch*0.05)\nplt.plot(xLine, theta[0] + theta[1]*xLine, marker=\"o\", color=\"black\")    \nplt.plot(a, y, \"bo\")\nprint(\"theta final:\",theta)\nplt.grid(True)\nplt.show()","be0e0461":"fig = plt.figure(figsize=(12, 7))\nax1 = fig.add_subplot(111, projection='3d')\nplt.title('Mini-batch Gradient Descent', fontsize=16)\nax1.scatter(Xb[:,1],Xb[:,2],y.reshape(m),marker=\"o\",color='r')\nax1.scatter(Xb[:,1],Xb[:,2],Xb.dot(theta),marker=\"^\",color='blue')\n\n#my new\nax1.set_xlabel('X1_area')\nax1.set_ylabel('X2_room_number')\nax1.set_zlabel('Y_price')\nplt.show()\n","9f911794":"## Batch gradient descent","555c4ba3":"### With dummy init","112e3210":"### With chosen init","1ff5025d":"## Mini-batch Gradient Descent","693f8d7f":"## Using sklearn built-in function","12e4a8a5":"### With scale data","8c875b77":"![](http:\/\/)> ## Stochastic Gradient Descent"}}