{"cell_type":{"ea979ea8":"code","28d6c8d7":"code","e63d92c3":"code","55b916b7":"code","5b2d919d":"code","f119566c":"code","18fd0787":"code","46ab64b4":"markdown","bf5502a1":"markdown","00df68a7":"markdown","62639aa5":"markdown","e626fdf5":"markdown","8e6bb624":"markdown","6b8fc6fb":"markdown","627e9783":"markdown"},"source":{"ea979ea8":"######## Base\nimport numpy as np \nimport pandas as pd \n\npd.set_option('display.max_columns', None)\n\n######### Warning ##############\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\n########## Sklearn #############\n# Pre-processing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n# Metrics\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.metrics import roc_auc_score, roc_curve\n# Models\nfrom sklearn.linear_model import LogisticRegression     # Logistic Regression\nfrom sklearn.naive_bayes import GaussianNB              # Naive Bayes\nfrom sklearn.neighbors import KNeighborsClassifier      # KNN \nfrom sklearn.svm import SVC                             # SVC \nfrom sklearn import tree                                # CART - S\u0131n\u0131fland\u0131rma ve Regresyon A\u011fa\u00e7lar\u0131\nfrom sklearn.tree import DecisionTreeClassifier         # CART - S\u0131n\u0131fland\u0131rma ve Regresyon A\u011fa\u00e7lar\u0131\nfrom sklearn.ensemble import BaggingClassifier          # Bagging\nfrom sklearn.ensemble import VotingClassifier           # Voting \nfrom sklearn.ensemble import RandomForestClassifier     # Random Forest\nfrom sklearn.ensemble import AdaBoostClassifier         # Ada Boost\nfrom sklearn.ensemble import GradientBoostingClassifier # GBM - Gradient Boosting Machine\nfrom xgboost import XGBClassifier                       # XGBoost | !pip install xgboost\nfrom lightgbm import LGBMClassifier                     # LightGBM | !conda install -c conda-forge lightgbm\nfrom catboost import CatBoostClassifier                 # CatBoost | !pip install catboost\n!pip install --upgrade nboost                           # NGBoost\n!pip install --upgrade git+https:\/\/github.com\/stanfordmlgroup\/ngboost.git\nfrom ngboost import NGBClassifier\nfrom ngboost.distns import k_categorical, Bernoulli","28d6c8d7":"############ IMPORT\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntr = train.copy()\nts = test.copy()\n\n############ MISSING VALUE IMPUTATION\ntrain[\"Age\"] = np.where(train.Age.isnull(), train.Age.mean(), train.Age)\ntest[\"Age\"] = np.where(test.Age.isnull(), test.Age.mean(), test.Age)\ntest[\"Fare\"] = np.where(test.Fare.isnull(), test.Fare.mean(), test.Fare)\n\n############ DROP VARIABLES\ntrain.drop([\"PassengerId\", \"Name\"], axis = 1, inplace = True)\ntest.drop([\"PassengerId\", \"Name\"], axis = 1, inplace = True)\n\n############ LABEL ENCODER\ncat = train.select_dtypes(include=[\"object\"]).columns\nfor col in train[cat].columns:\n        \n    train[col] = train[col].astype(str)\n    test[col] = test[col].astype(str)\n        \n    le = LabelEncoder()\n    le.fit(list(train[col])+list(test[col]))\n    train[col] = le.transform(train[col])\n    test[col]  = le.transform(test[col])\n    \n############ TRAIN-TEST SPLIT FOR TRAIN DATA    \nX_train, X_test, y_train, y_test = train_test_split(train.drop(\"Survived\", \n                                                               axis = 1),\n                                                    train.Survived, \n                                                    test_size = 0.20,\n                                                    random_state = 41)","e63d92c3":"########## ALL MODELS\n# Logistic Regression\nlog = LogisticRegression(solver = \"liblinear\")\nlog.fit(X_train, y_train)\ny_pred_log = log.predict(X_test)\n\n# Naive Bayes\nnb = GaussianNB()\nnb.fit(X_train, y_train)\ny_pred_nb = nb.predict(X_test)\n\n# KNN\nknn = KNeighborsClassifier() # k (n_neighbors) say\u0131s\u0131 \u00f6n tan\u0131ml\u0131 de\u011feri 5'tir.\nknn.fit(X_train, y_train)\ny_pred_knn = knn.predict(X_test)\n\n# SVM - Linear\nsvc = SVC(kernel = \"linear\", probability=True) \nsvc.fit(X_train, y_train)\ny_pred_svc = svc.predict(X_test)\n\n# SVM - RBF\nsvc_rbf = SVC(kernel = \"rbf\",probability=True) \nsvc_rbf.fit(X_train, y_train)\ny_pred_svc_rbf = svc_rbf.predict(X_test)\n\n# CART\ncart = DecisionTreeClassifier()\ncart.fit(X_train, y_train)\ny_pred_cart = cart.predict(X_test)\n\n# BAGGING\nbag = BaggingClassifier()\nbag.fit(X_train, y_train)\ny_pred_bag = bag.predict(X_test)\n\n# VOTING\nclf1 = LogisticRegression(solver = \"liblinear\")\nclf2 = RandomForestClassifier()\nclf3 = GaussianNB()\nclf4 = KNeighborsClassifier()\nvote = VotingClassifier(\n    estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\nvote.fit(X_train, y_train)\ny_pred_vote = vote.predict(X_test)\n\n# RANDOM FOREST\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\n\n# ADABOOST\nada = AdaBoostClassifier()\nada.fit(X_train, y_train)\ny_pred_ada = ada.predict(X_test)\n\n# GBM\ngbm = GradientBoostingClassifier()\ngbm.fit(X_train, y_train)\ny_pred_gbm = gbm.predict(X_test)\n\n# XGBOOST\nxgb = XGBClassifier()\nxgb.fit(X_train, y_train)\ny_pred_xgb = xgb.predict(X_test)\n\n# LGBM\nlgb = LGBMClassifier()\nlgb.fit(X_train, y_train)\ny_pred_lgb = lgb.predict(X_test)\n\n# CATBOOST\ncat = CatBoostClassifier()\ncat.fit(train.drop(\"Survived\",axis = 1), train.Survived, verbose = 0)\ny_pred_cat = cat.predict(X_test) \n\n# NGBOOST\nngb_cat = NGBClassifier(Dist=k_categorical(2), verbose=False)\nngb_cat.fit(X_train, y_train)\ny_pred_ngb = ngb_cat.predict(X_test)","55b916b7":"########## RESULTS\nmodels = [\"Logistic Regression\", \"Naive Bayes\", \"KNN\", \"Linear SVM\", \"RBF SVM\", \"CART\", \"Bagging\", \"Voting\", \n            \"Random Forest\", \"AdaBoost\", \"GBM\", \"XGBoost\", \"LightGBM\", \"CatBoost\", \"NGBoost\"]\ntest_acc = [\n    accuracy_score(y_test, y_pred_log),\n    accuracy_score(y_test, y_pred_nb),\n    accuracy_score(y_test, y_pred_knn),\n    accuracy_score(y_test, y_pred_svc),\n    accuracy_score(y_test, y_pred_svc_rbf),\n    accuracy_score(y_test, y_pred_cart),\n    accuracy_score(y_test, y_pred_bag),\n    accuracy_score(y_test, y_pred_vote),\n    accuracy_score(y_test, y_pred_rf),\n    accuracy_score(y_test, y_pred_ada),\n    accuracy_score(y_test, y_pred_gbm),\n    accuracy_score(y_test, y_pred_xgb),\n    accuracy_score(y_test, y_pred_lgb),\n    accuracy_score(y_test, y_pred_cat),\n    accuracy_score(y_test, y_pred_ngb)\n]\n\ntrain_acc = [\n    \n    accuracy_score(y_train, log.predict(X_train)),\n    accuracy_score(y_train, nb.predict(X_train)),\n    accuracy_score(y_train, knn.predict(X_train)),\n    accuracy_score(y_train, svc.predict(X_train)),\n    accuracy_score(y_train, svc_rbf.predict(X_train)),\n    accuracy_score(y_train, cart.predict(X_train)),\n    accuracy_score(y_train, bag.predict(X_train)),\n    accuracy_score(y_train, vote.predict(X_train)),\n    accuracy_score(y_train, rf.predict(X_train)),\n    accuracy_score(y_train, ada.predict(X_train)),\n    accuracy_score(y_train, gbm.predict(X_train)),\n    accuracy_score(y_train, xgb.predict(X_train)),\n    accuracy_score(y_train, lgb.predict(X_train)),\n    accuracy_score(y_train, cat.predict(X_train)),\n    accuracy_score(y_train, ngb_cat.predict(X_train))\n]\n\npd.DataFrame({\n    \n    \"Model\":models,\n    \"Train Accuracy\": train_acc,\n    \"Test Accuracy\": test_acc\n    \n})","5b2d919d":"log = LogisticRegression(solver = \"liblinear\")\nlog.fit(train.drop(\"Survived\", axis = 1), train.Survived)\nypred = log.predict_proba(test)[:,1]\n# Best threshold 0.6\nypred = [1 if i > 0.6 else 0 for i in ypred]\nts[\"Survived\"] = ypred\nsubmissionlog = ts[[\"PassengerId\", \"Survived\"]]\nsubmissionlog.to_csv(\"submissionlog.csv\",columns = [\"PassengerId\", \"Survived\"] , index = None)","f119566c":"########### NAIVE BAYES\nnb = GaussianNB()\nmodel = nb.fit(train.drop(\"Survived\",axis = 1), train.Survived)\ny_predprob = model.predict_proba(test)[:,1]\nypred = [1 if i > 0.5 else 0 for i in y_predprob]\n\nts[\"Survived\"] = ypred\nsubmissionnaive = ts[[\"PassengerId\", \"Survived\"]]\nsubmissionnaive.to_csv(\"submissionnaive.csv\",columns = [\"PassengerId\", \"Survived\"] , index = None)\n\n########### KNN\nknn = KNeighborsClassifier()\nmodel = knn.fit(train.drop(\"Survived\",axis = 1), train.Survived)\ny_predprob = model.predict_proba(test)[:,1]\nypred = [1 if i > 0.5 else 0 for i in y_predprob]\n\nts[\"Survived\"] = ypred\nsubmissionknn = ts[[\"PassengerId\", \"Survived\"]]\nsubmissionknn.to_csv(\"submissionknn.csv\",columns = [\"PassengerId\", \"Survived\"] , index = None)\n\n########### LINEAR SVM\nsvc = SVC(kernel = \"linear\", probability=True) \nmodel = svc.fit(train.drop(\"Survived\",axis = 1), train.Survived)\ny_predprob = model.predict_proba(test)[:,1]\nypred = [1 if i > 0.6 else 0 for i in y_predprob]\nts[\"Survived\"] = ypred\nsubmissionsvc = ts[[\"PassengerId\", \"Survived\"]]\nsubmissionsvc.to_csv(\"submissionsvc.csv\",columns = [\"PassengerId\", \"Survived\"] , index = None)\n\n########### RBF SVM\nsvc = SVC(kernel = \"rbf\", probability=True) \nmodel = svc.fit(train.drop(\"Survived\",axis = 1), train.Survived)\ny_predprob = model.predict_proba(test)[:,1]\nypred = [1 if i > 0.6 else 0 for i in y_predprob]\n\nts[\"Survived\"] = ypred\nsubmissionsvc = ts[[\"PassengerId\", \"Survived\"]]\nsubmissionsvc.to_csv(\"submissionsvc.csv\",columns = [\"PassengerId\", \"Survived\"] , index = None)\n\n########### CART\ncart = DecisionTreeClassifier()\nmodel = cart.fit(train.drop(\"Survived\",axis = 1), train.Survived)\ny_predprob = model.predict_proba(test)[:,1]\nypred = [1 if i > 0.5 else 0 for i in y_predprob]\n\nts[\"Survived\"] = ypred\nsubmissionsvc = ts[[\"PassengerId\", \"Survived\"]]\nsubmissionsvc.to_csv(\"submissioncart.csv\",columns = [\"PassengerId\", \"Survived\"] , index = None)\n\n########### BAGGING\nbag = BaggingClassifier()\nmodel = bag.fit(train.drop(\"Survived\",axis = 1), train.Survived)\ny_predprob = model.predict_proba(test)[:,1]\nypred = [1 if i > 0.5 else 0 for i in y_predprob]\n\nts[\"Survived\"] = ypred\nsubmissionsvc = ts[[\"PassengerId\", \"Survived\"]]\nsubmissionsvc.to_csv(\"submissionbagging.csv\",columns = [\"PassengerId\", \"Survived\"] , index = None)\n\n########### VOTING\nclf1 = LogisticRegression(solver = \"liblinear\")\nclf2 = RandomForestClassifier()\nclf3 = GaussianNB()\nclf4 = KNeighborsClassifier()\n\nvote = VotingClassifier(\n    estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard'\n)\nmodel = vote.fit(train.drop(\"Survived\",axis = 1), train.Survived)\n#y_predprob = model.predict_proba(test)[:,1]\n#ypred = [1 if i > 0.5 else 0 for i in y_predprob]\nypred = model.predict(test)\n\nts[\"Survived\"] = ypred\nsubmissionsvc = ts[[\"PassengerId\", \"Survived\"]]\nsubmissionsvc.to_csv(\"submissionvoting.csv\",columns = [\"PassengerId\", \"Survived\"] , index = None)\n\n########### RANDOM FOREST\nrf = RandomForestClassifier()\nmodel = rf.fit(train.drop(\"Survived\",axis = 1), train.Survived)\ny_predprob = model.predict_proba(test)[:,1]\nypred = [1 if i > 0.4 else 0 for i in y_predprob]\n\nts[\"Survived\"] = ypred\nsubmissionsvc = ts[[\"PassengerId\", \"Survived\"]]\nsubmissionsvc.to_csv(\"submissionrf.csv\",columns = [\"PassengerId\", \"Survived\"] , index = None)\n\n########### ADABOOST\nada = AdaBoostClassifier()\nmodel = ada.fit(train.drop(\"Survived\",axis = 1), train.Survived)\ny_predprob = model.predict_proba(test)[:,1]\nypred = [1 if i > 0.5 else 0 for i in y_predprob]\n\nts[\"Survived\"] = ypred\nsubmissionsvc = ts[[\"PassengerId\", \"Survived\"]]\nsubmissionsvc.to_csv(\"submissionada.csv\",columns = [\"PassengerId\", \"Survived\"] , index = None)\n\n########### GBM\ngbm = GradientBoostingClassifier()\nmodel = gbm.fit(train.drop(\"Survived\",axis = 1), train.Survived)\ny_predprob = model.predict_proba(test)[:,1]\nypred = [1 if i > 0.5 else 0 for i in y_predprob]\n\nts[\"Survived\"] = ypred\nsubmissionsvc = ts[[\"PassengerId\", \"Survived\"]]\nsubmissionsvc.to_csv(\"submissiongbm.csv\",columns = [\"PassengerId\", \"Survived\"] , index = None)\n\n########### XGBOOST\nxgb = XGBClassifier()\nmodel = xgb.fit(train.drop(\"Survived\",axis = 1), train.Survived)\ny_predprob = model.predict_proba(test)[:,1]\nypred = [1 if i > 0.5 else 0 for i in y_predprob]\n\nts[\"Survived\"] = ypred\nsubmissionsvc = ts[[\"PassengerId\", \"Survived\"]]\nsubmissionsvc.to_csv(\"submissionxgb.csv\",columns = [\"PassengerId\", \"Survived\"] , index = None)\n\n########### LGBM\nlgb = LGBMClassifier()\nmodel = lgb.fit(train.drop(\"Survived\",axis = 1), train.Survived)\ny_predprob = model.predict_proba(test)[:,1]\nypred = [1 if i > 0.6 else 0 for i in y_predprob]\n\nts[\"Survived\"] = ypred\nsubmissionsvc = ts[[\"PassengerId\", \"Survived\"]]\nsubmissionsvc.to_csv(\"submissionlgb.csv\",columns = [\"PassengerId\", \"Survived\"] , index = None)\n\n########### CATBOOST\ncat = CatBoostClassifier()\nmodel = cat.fit(train.drop(\"Survived\",axis = 1), train.Survived, verbose = 0)\ny_predprob = model.predict_proba(test)[:,1]\nypred = [1 if i > 0.5 else 0 for i in y_predprob]\n\nts[\"Survived\"] = ypred\nsubmissionsvc = ts[[\"PassengerId\", \"Survived\"]]\nsubmissionsvc.to_csv(\"submissioncat.csv\",columns = [\"PassengerId\", \"Survived\"] , index = None)\n\n########### NGBOOST\nngb_cat = NGBClassifier(Dist=k_categorical(2), verbose=False) # tell ngboost that there are 3 possible outcomes\nngb_cat.fit(train.drop(\"Survived\",axis = 1), train.Survived)\nts[\"Survived\"] = ngb_cat.predict(test)\nsubmissionngb = ts[[\"PassengerId\", \"Survived\"]]\nsubmissionngb.to_csv(\"submissionngb.csv\",columns = [\"PassengerId\", \"Survived\"] , index = None)","18fd0787":"models = [\"Logistic Regression\", \"Naive Bayes\", \"KNN\", \"Linear SVM\", \"RBF SVM\", \"CART\", \"Bagging\", \"Voting\", \"Random Forest\", \"AdaBoost\", \"GBM\", \"XGBoost\", \"LightGBM\", \"CatBoost\", \"NGBoost\"]\nkaggle_scores = [0.78468, 0.73205, 0.63636, 0.77511, 0.66985, 0.73205, 0.80382, 0.76076, 0.74641, 0.74162,0.76555, 0.77990 ,0.77033 ,0.76076, 0.76555]\n\npd.DataFrame({\"Model\": models, \"Kaggle Score\": kaggle_scores}).sort_values(\"Kaggle Score\", ascending=False).reset_index(drop = True)\n\n","46ab64b4":"<a id=\"section4\"><\/a>\n# 4.FIRST MODEL: LOGISTIC REGRESSION & KAGGLE SCORE","bf5502a1":"<a id=\"section3\"><\/a>\n# 3.NO FREE LUNCH THEOREM: ALL MODELS\n\n\nLogistic Regression, Naive Bayes, KNN, Linear SVM, RBF SVM, CART, Bagging, Voting, Random Forest, AdaBoost, GBM, XGBoost, LightGBM, CatBoost, NGBoost.","00df68a7":"<a id=\"section2\"><\/a>\n# 2.DATA MANIPULATION","62639aa5":"<center><img\nsrc=\"https:\/\/www.veribilimiokulu.com\/wp-content\/uploads\/2020\/04\/bagging-1024x166.jpg\" style=\"width:100%;height:100%;\">\n<\/center>","e626fdf5":"<a id=\"section5\"><\/a>\n# 5.OTHER MODELS & MODEL RESULTS","8e6bb624":"<a id=\"section1\"><\/a>\n# 1.PACKAGES","6b8fc6fb":"<center><img\nsrc=\"https:\/\/www.veribilimiokulu.com\/wp-content\/uploads\/2020\/02\/submissionlog-1024x522.jpg\" style=\"width:100%;height:100%;\">\n<\/center>\n\n<center><img\nsrc=\"https:\/\/www.veribilimiokulu.com\/wp-content\/uploads\/2020\/02\/submissionlog2-1024x104.jpg\" style=\"width:100%;height:100%;\">\n<\/center>\n\n","627e9783":"<center><img\nsrc=\"https:\/\/www.veribilimiokulu.com\/wp-content\/uploads\/2020\/02\/0-1024x228.jpg\" style=\"width:100%;height:100%;\">\n<\/center>\n\n<br>\n\n# Table of Contents\n1. [**PACKAGES**](#section1)\n1. [**DATA MANIPULATION**](#section2)\n1. [**NO FREE LUNCH THEOREM: ALL MODELS**](#section3)\n1. [**FIRST MODEL: LOGISTIC REGRESSION & KAGGLE SCORE**](#section4)\n1. [**OTHER MODELS & MODEL RESULTS**](#section5)"}}