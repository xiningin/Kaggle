{"cell_type":{"74988eac":"code","799ead5c":"code","cdb8aa62":"code","112ed38e":"code","46bae369":"code","dd9de751":"code","f918b5c2":"code","7cee67f3":"code","9846e8f0":"code","9a663fff":"code","cf73c8fa":"code","f009568d":"code","4b824378":"code","ab70ea84":"code","1e453546":"code","c5a46e20":"code","d9b77d57":"code","d3a92570":"code","6f168a89":"code","16404d24":"code","7f7ac43d":"code","de81bd7c":"code","26e0ff47":"code","cde78fdd":"code","72128794":"code","99a0d310":"code","727d95fd":"code","d57a3dd3":"code","0bbec6da":"code","5ac8d18b":"code","d3ff2d11":"code","319a52a9":"code","1eb9f462":"code","8996066e":"code","6c7c4b05":"code","ac3086aa":"code","47d1b6a9":"markdown","2479e464":"markdown","ff9ce4f7":"markdown","437c8123":"markdown","d70049d7":"markdown","34ad12e2":"markdown","10c4385c":"markdown","f1ba3b23":"markdown","7e08242f":"markdown","38f93471":"markdown","7e70dbd7":"markdown","0d04bde7":"markdown","fe9af4d2":"markdown","900d13e0":"markdown"},"source":{"74988eac":"nrows = None","799ead5c":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import f1_score\nfrom collections import Counter\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\n\n%matplotlib inline\n\nimport os\nos.listdir(\"..\/input\/ykc-cup-2nd\/\")","cdb8aa62":"%%time\ntrain = pd.read_csv(\"..\/input\/ykc-cup-2nd\/train.csv\", nrows = nrows)\ntest = pd.read_csv(\"..\/input\/ykc-cup-2nd\/test.csv\", nrows = nrows)\nsub = pd.read_csv(\"..\/input\/ykc-cup-2nd\/sample_submission.csv\")","112ed38e":"train.head()","46bae369":"test.head()","dd9de751":"sub.head()","f918b5c2":"df = pd.concat([train, test])\ndf = df.reset_index(drop = True)\ndf.shape","7cee67f3":"def clear(x, punct, rep = \"\"):\n    for p in punct:\n        x = x.replace(p, rep)\n    return x\npunct = [\"\u00f1\",\"\\\\\",\"\\\"\",\"\u00e8\",\"!\",\"\u2122\",\"\u00ae\",\"(\",\")\",\"'s\"]\nsplit_word = [\",\",\"&\",\"\/\",\"-\",\"\u2011\",\"+\"]\n\n## \u6b63\u898f\u5316\ndf[\"product_name\"] = df[\"product_name\"].apply(lambda words : clear(clear(words.lower(), punct, \"\"), split_word, \" \").split(\" \"))","9846e8f0":"%%time\nn_word = 2000\n## \u3088\u304f\u51fa\u3066\u304f\u308b\u5358\u8a9e\u3060\u3051\u4f7f\u7528\nc = Counter(np.hstack(df[\"product_name\"]))\nprint(len(list(c.keys())))\nuse_words = [wc[0] for wc in c.most_common(n_word)]\n\n## \u305d\u306e\u5358\u8a9e\u304c\u3042\u308b\u304b\u3069\u3046\u304b\u306ebinary\nbinary_cols = []\nfor word in tqdm(use_words):\n    df[f\"is_{word}\"] = df[\"product_name\"].apply(lambda word_list : word in word_list)\n    binary_cols.append(f\"is_{word}\")\ndf.shape","9a663fff":"%%time\nfrom sklearn.decomposition import LatentDirichletAllocation\nn_lda = 100\nlda = LatentDirichletAllocation(n_lda, random_state = 42)\nlda = lda.fit_transform(df[binary_cols])\nlda_cols = [f\"lda{k}\" for k in range(n_lda)]\nlda_df = pd.DataFrame(lda, columns = lda_cols)\ndf = pd.concat([df, lda_df], axis = 1)\ndf.shape","cf73c8fa":"%%time\nfrom sklearn.decomposition import NMF\nn_nmf = 300\nnmf = NMF(n_nmf, random_state = 42)\nnmf = nmf.fit_transform(df[binary_cols])\nnmf_cols = [f\"nmf{k}\" for k in range(n_nmf)]\nnmf_df = pd.DataFrame(nmf, columns = nmf_cols)\ndf = pd.concat([df, nmf_df], axis = 1)\ndf.shape","f009568d":"# %%time\n# from sklearn.decomposition import PCA\n# n_pca = 300\n# pca = PCA(n_pca, random_state = 42)\n# pca = pca.fit_transform(df[binary_cols])\n# pca_cols = [f\"pca{k}\" for k in range(n_pca)]\n# pca_df = pd.DataFrame(pca, columns = pca_cols)\n# df = pd.concat([df, pca_df], axis = 1)\n# df.shape","4b824378":"def get_vec( x):\n    vs = []\n    for xx in x:\n        try:\n            vs.append(model.wv[xx])\n        except:\n            flg = False\n            for i in range(1, len(xx)):\n                try:\n                    v1 = model.wv[xx[:i]]\n                    v2 = model.wv[xx[i:]]\n    #                     print(xx[:i], xx[i:])\n                    vs.append(v1)\n                    vs.append(v2)\n                    flg = True\n                    break\n                except:\n                    pass\n            if flg == False:\n                for i in range(1, len(xx)):\n                    try:\n                        v1 = model.wv[xx[:i]]\n    #                         print(xx[:i])\n                        vs.append(v1)\n                        break\n                    except:\n                        pass\n                for i in range(1, len(xx)):\n                    try:\n                        v2 = model.wv[xx[i:]]\n    #                         print(xx[i:])\n                        vs.append(v2)\n                        break\n                    except:\n                        pass\n    vs = np.array(vs)\n    if len(vs) == 0:\n        vs = np.zeros([1, model.vector_size])\n    return vs","ab70ea84":"%%time\nimport gensim\nmodel = pd.read_pickle(\"..\/input\/ykc-cup-2nd-save-fasttext\/fasttext_gensim_model.pkl\")\n\ndef to_vec(x):\n#     print(vs.shape)\n    vs = get_vec(x)\n    v = np.sum(vs, axis = 0)\n    v = v \/ (np.sqrt(np.sum(v ** 2)) + 1e-16)\n    v_std = np.std(vs, axis = 0)\n    v_max = np.max(vs, axis = 0)\n    v_min = np.min(vs, axis = 0)\n    vs = np.hstack([v, v_std, v_max, v_min])\n#     print(vs.shape)\n    return vs\n\nvecs = df[\"product_name\"].apply(lambda x : to_vec(x))\nvecs = np.vstack(vecs)\nfasttext_pre_cols = \\\n    [f\"fasttext_pre_vec{k}\" for k in range(model.vector_size)] + \\\n    [f\"fasttext_pre_std{k}\" for k in range(model.vector_size)] + \\\n    [f\"fasttext_pre_max{k}\" for k in range(model.vector_size)] + \\\n    [f\"fasttext_pre_min{k}\" for k in range(model.vector_size)]\nvec_df = pd.DataFrame(vecs, columns=fasttext_pre_cols)\ndf = pd.concat([df, vec_df], axis = 1)\ndf.shape","1e453546":"%%time\n## use word\nwords = c.most_common(10000)\nword_vecs = []\nwords_with_vec = []\nfor w in words:\n    w = w[0]\n    try:\n        word_vecs.append(model.wv[w])\n        words_with_vec.append(w)\n    except:\n        pass\nword_vecs = np.array(word_vecs)\n\n## gmm\nfrom sklearn.mixture import GaussianMixture\nnum_clusters = 10\nclf =  GaussianMixture(n_components=num_clusters,\n                covariance_type=\"tied\", init_params='kmeans', max_iter=50, verbose=1)\nclf.fit(word_vecs)\nprobs = clf.predict_proba(word_vecs)\n\n## weighted\nword_vec_with_prob_list = []\nfor i in range(num_clusters):\n    dic = {w : word_vecs[k] * probs[k, i] for k, w in enumerate(words_with_vec)}\n    word_vec_with_prob_list.append(dic)\n\ndef to_vec_scdv(x):\n    vs = []\n    for xx in x:\n        try:\n            v = np.hstack([word_vec_with_prob_list[i][xx] for i in range(num_clusters)])\n#             print(v.shape)\n            vs.append(v)\n        except:\n            pass\n    vs = np.array(vs)\n    if len(vs) == 0:\n        vs = np.zeros([1, model.vector_size * num_clusters])\n#     print(vs.shape)\n    v = np.sum(vs, axis = 0)\n    v = v \/ (np.sqrt(np.sum(v ** 2)) + 1e-16)\n#     v_std = np.std(vs, axis = 0)\n#     v_max = np.max(vs, axis = 0)\n#     v_min = np.min(vs, axis = 0)\n#     vs = np.hstack([v, v_std, v_max, v_min])\n    return v\n\nvecs = df[\"product_name\"].apply(lambda x : to_vec_scdv(x))\nvecs = np.vstack(vecs)\n\nfasttext_scdv_cols = []\nfor i in range(num_clusters):\n    fasttext_scdv_cols += [f\"fasttext_scdv_vec{i}_{k}\" for k in range(model.vector_size)]\nvec_df = pd.DataFrame(vecs, columns=fasttext_scdv_cols)\ndf = pd.concat([df, vec_df], axis = 1)\ndf.shape","c5a46e20":"target = \"department_id\" ## \u4e88\u6e2c\u5bfe\u8c61\ntrain = df[~df[target].isna()]\ntest = df[df[target].isna()]\ntrain.shape, test.shape","d9b77d57":"import lightgbm as lgb\ndef pandas_to_numpy(x):\n    if \"pandas\" in str(type(x)):\n        return x.values\n    else:\n        return x\n    \ndef trainLGBM(x_train, y_train, x_val, y_val, params, w_train = None, w_val = None, verbose = 10):\n    x_train = pandas_to_numpy(x_train)\n    x_val = pandas_to_numpy(x_val)\n    y_train = pandas_to_numpy(y_train)\n    y_val = pandas_to_numpy(y_val)\n    dat_train = lgb.Dataset(x_train, y_train, weight=w_train)\n    dat_val = lgb.Dataset(x_val, y_val, weight=w_val)\n    evals_result = {}\n    gbm = lgb.train(params, \n                    dat_train, \n                    num_boost_round = params[\"num_boost_round\"],\n                    early_stopping_rounds=params[\"early_stopping_rounds\"],\n                    valid_sets=[dat_train, dat_val],\n                    verbose_eval = verbose, evals_result = evals_result)\n    return gbm, evals_result\n","d3a92570":"def run_cv(features, params, train, test, n_split):\n    ## cross validation\u3092\u884c\u3044\uff0c\u5404fold\u3067\u8a13\u7df4\u3057\u305f\u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u306e\u5e73\u5747\u5024\u3092submit\u3059\u308b\n    preds_test = []\n    scores = []\n    oof = np.zeros([len(train), 21])\n    feature_importance = pd.DataFrame(features, columns=[\"name\"])\n    feature_importance[\"fi\"] = 0\n    kfold = StratifiedKFold(n_splits=n_split, shuffle = True, random_state=42)\n    for i_fold, (train_idx, valid_idx) in enumerate(kfold.split(train, train[target])):\n        print(f\"--------fold {i_fold}-------\")\n\n        ## train data\n        x_tr = train.loc[train_idx, features]\n        y_tr = train.loc[train_idx, target]\n\n        ## valid data\n        x_va = train.loc[valid_idx, features]\n        y_va = train.loc[valid_idx, target]\n\n        ## train LGBM model\n\n        model, evals_result = trainLGBM(x_tr, y_tr, x_va, y_va, params)\n    #     model = LGBMClassifier()\n    #     model.fit(x_tr, y_tr, )\n\n        ## save feature importance\n        feature_importance[\"fi\"] += model.feature_importance(\"gain\")\n\n        ## evaluate on valid\n        pred_val = model.predict(x_va)\n        oof[valid_idx] = pred_val\n\n        score = {\n            \"logloss\"  : log_loss(y_va, pred_val),\n            \"f1_micro\" : f1_score(y_va, np.argmax(pred_val, axis = 1), average = \"micro\")}\n        print(score)\n        scores.append(score)\n\n        ## pred on test\n        pred_test = model.predict(test[features])\n        preds_test.append(pred_test)\n    return scores, oof, preds_test, feature_importance","6f168a89":"params = {'lambda_l1': 3,\n 'lambda_l2': 3,\n \"min_split_gain\" : 0.5,\n 'feature_fraction': 0.3,\n 'bagging_fraction': 0.5,\n 'bagging_freq': 0,\n 'min_child_samples': 20,\n 'num_boost_round' : 2000,\n \"learning_rate\" : 1e-1,\n 'early_stopping_rounds' : 50,\n 'objective': 'multiclass',\n \"num_class\" : 21}\n\nfeatures = [\"order_rate\", \"order_dow_mode\", \"order_hour_of_day_mode\"] ## \u4e88\u6e2c\u306b\u4f7f\u7528\u3059\u308b\u7279\u5fb4\u91cf\u306e\u540d\u524d\nfeatures += fasttext_pre_cols + lda_cols + nmf_cols + fasttext_scdv_cols\nn_split = 3 ## cross validation\u306efold\u6570","16404d24":"%%time\nscores, oof, preds_test, feature_importance = run_cv(features, params, train, test, n_split)","7f7ac43d":"score_df = pd.DataFrame(scores)\nprint(score_df.mean())\nscore_df","de81bd7c":"plt.figure(figsize = (8, 12))\nsns.barplot(data = feature_importance.sort_values(by = \"fi\", ascending = False).head(50), x = \"fi\", y = \"name\")","26e0ff47":"plt.figure(figsize = (8, 12))\nsns.barplot(data = feature_importance.sort_values(by = \"fi\", ascending = False).tail(50), x = \"fi\", y = \"name\")","cde78fdd":"print(len(features))\nq = np.quantile(feature_importance[\"fi\"], 0.2)\nprint(q)\nfeatures = list(feature_importance[feature_importance[\"fi\"] > q][\"name\"])\nprint(len(features))","72128794":"%%time\nscores, oof, preds_test, feature_importance = run_cv(features, params, train, test, n_split)","99a0d310":"score_df = pd.DataFrame(scores)\nprint(score_df.mean())\nscore_df","727d95fd":"print(len(features))\nq = np.quantile(feature_importance[\"fi\"], 0.2)\nprint(q)\nfeatures = list(feature_importance[feature_importance[\"fi\"] > q][\"name\"])\nprint(len(features))","d57a3dd3":"%%time\nscores, oof, preds_test, feature_importance = run_cv(features, params, train, test, n_split)","0bbec6da":"score_df = pd.DataFrame(scores)\nprint(score_df.mean())\nscore_df","5ac8d18b":"print(len(features))\nq = np.quantile(feature_importance[\"fi\"], 0.2)\nprint(q)\nfeatures = list(feature_importance[feature_importance[\"fi\"] > q][\"name\"])\nprint(len(features))","d3ff2d11":"params = {'lambda_l1': 3,\n 'lambda_l2': 3,\n \"min_split_gain\" : 0.5,\n 'feature_fraction': 0.1,\n 'bagging_fraction': 1.0,\n 'bagging_freq': 0,\n 'min_child_samples': 20,\n 'num_boost_round' : 5000,\n \"learning_rate\" : 1e-2,\n 'early_stopping_rounds' : 50,\n 'objective': 'multiclass',\n \"num_class\" : 21}\n\nn_split = 5 ## cross validation\u306efold\u6570","319a52a9":"%%time\nscores, oof, preds_test, feature_importance = run_cv(features, params, train, test, n_split)","1eb9f462":"score_df = pd.DataFrame(scores)\nprint(score_df.mean())\nscore_df","8996066e":"pred_test_mean = np.array(preds_test).mean(axis = 0)\npred_test_final = np.argmax(pred_test_mean, axis = 1)","6c7c4b05":"sub[\"department_id\"] = pred_test_final\nsub.to_csv(\"submission.csv\", index = False)","ac3086aa":"pd.to_pickle(\n    {\"train\" : train, \"test\" : test, \"oof\" : oof, \"pred_test\" : pred_test_mean, \"sub\" : sub, \"features\" : features,\n    \"feature_importance\" : feature_importance, \"params\" : params}, \"data.pkl\")","47d1b6a9":"## last","2479e464":"## 2","ff9ce4f7":"## feature selection","437c8123":"## PCA","d70049d7":"## submission","34ad12e2":"## LDA","10c4385c":"## fasttext pretrain","f1ba3b23":"## train","7e08242f":"## 1","38f93471":"## SCDV","7e70dbd7":"\n## feature engineering","0d04bde7":"## NMF","fe9af4d2":"## binary","900d13e0":"## read data "}}