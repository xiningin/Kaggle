{"cell_type":{"3878015a":"code","978e40bf":"code","17908fd7":"code","b8e39727":"code","8e4fd72c":"code","b59d5fd0":"code","5760431b":"code","5fd26585":"code","9a65ea00":"code","e987c1f0":"code","452bface":"code","efd08116":"code","c006f3b7":"code","e7ba7f3f":"code","3947a263":"code","7eb24676":"code","bb026f66":"code","b694d11e":"code","6653b1fb":"code","bfab9457":"code","31e5c731":"code","1331c335":"code","f3befac8":"code","2579b0bb":"code","dbd817ae":"code","729a4173":"code","e63b3ccc":"code","6ce69f6d":"code","c5699b96":"code","4d90fbfd":"code","b2591a75":"code","d459cb1d":"code","3c52f7c2":"code","082edea2":"code","c91bf2a6":"code","135c83a8":"code","a580116f":"code","6c89849b":"code","38fa8e21":"code","cf76697e":"code","cf5f12f7":"code","fdeca138":"code","5a90e6e6":"code","2abb32fc":"code","88ed25ec":"code","50c000f5":"code","617bde44":"code","a8f25d78":"code","8876572e":"code","771fb0a3":"code","1d847b71":"code","21e386f4":"code","92552cc4":"code","119d08e5":"code","43d82332":"code","a666e3e2":"code","c743bb14":"code","d15359ff":"code","d9c7ba63":"code","d1020b5e":"code","a682475b":"code","47dac030":"code","ff87b129":"code","5912ace0":"code","36778c30":"code","30e9ddcf":"code","6e4c2621":"code","0baa1351":"code","c027de14":"code","952f4914":"code","dbbce6b9":"code","bb2e0a94":"code","dfa76198":"code","b6eddc28":"code","b52c6120":"code","613d3f07":"code","0511e175":"code","c1c352ba":"code","5bca27a7":"code","a56626fa":"code","abce4f7c":"code","14ada6bd":"code","8b79878e":"code","982d1014":"code","dc4fd563":"code","61836c06":"code","da3b8dc4":"code","327499f8":"code","88e34d46":"code","93cb40ba":"code","38dd11e2":"code","017c5fd0":"code","b95b0701":"code","82e3dc8a":"code","091df94e":"code","691d7b2f":"code","6580054c":"code","a37d83f1":"code","831bc187":"code","17ca621f":"code","43fc67f6":"code","e549a812":"code","5d2724b9":"code","93b9a8b2":"code","3f890c10":"code","f5cf380c":"code","ab9e1d4a":"code","b4e18667":"code","4e9d0f16":"code","2bab7a78":"code","b7333326":"code","0d393d3e":"code","a3734c6f":"code","8543e903":"code","698887a3":"code","5fcc01ca":"code","ce120bd3":"code","52c5ae7b":"code","6804ad8d":"code","98a5d308":"markdown","37f2a433":"markdown","8369047e":"markdown","ce992125":"markdown","805369d2":"markdown","80289926":"markdown","8cf0bc0b":"markdown","b1f42c2f":"markdown","5d945546":"markdown","1a0b616e":"markdown","b80e86bb":"markdown","0db6920b":"markdown","5216fb43":"markdown","548cf84a":"markdown","230b2b35":"markdown","ad74d8f3":"markdown","621160e9":"markdown","28e991ea":"markdown","1f98fb25":"markdown","6a38cbc3":"markdown","ea46728a":"markdown","3931138f":"markdown","2c0ec7ca":"markdown","b7d8d8ad":"markdown","67813679":"markdown","40182e87":"markdown","eccd59b7":"markdown","dd9b3728":"markdown","64b1ea96":"markdown","d762e1ce":"markdown","64b2ebfd":"markdown","51e8e0d0":"markdown","39c6174b":"markdown","e1821b98":"markdown","1d1c169f":"markdown","d19a08f0":"markdown","bb3683e7":"markdown","548da677":"markdown","f688c62f":"markdown","b5930adb":"markdown","5ccbd7b7":"markdown","c0be1731":"markdown","e3278f3c":"markdown","aa53a960":"markdown","b44a9a3a":"markdown","46c21f82":"markdown","b36d9c25":"markdown","88d8f049":"markdown","4cb4cca0":"markdown","3429698c":"markdown","d5e418a8":"markdown","739d8b61":"markdown","9ccba8cc":"markdown","e378cf31":"markdown","c3fc8fb6":"markdown","2f4bd4ed":"markdown","0e095817":"markdown","84ee517c":"markdown","03b8599c":"markdown","03413a50":"markdown","8debd0af":"markdown","f0cb5ff9":"markdown","a0ae33e2":"markdown","78f0ea63":"markdown","884d1a13":"markdown","b69a1d74":"markdown","b8d920e9":"markdown","88f631b7":"markdown","b5615843":"markdown","096abbfa":"markdown","7558a2a0":"markdown","8cb30dac":"markdown","6442ea07":"markdown","96d455dd":"markdown","68d29af7":"markdown","7ac77f19":"markdown","3b0e7daa":"markdown","09f3f218":"markdown","cd349fe7":"markdown"},"source":{"3878015a":"import os\nimport re\nimport gc \nfrom tqdm import tqdm\nfrom datetime import date     #calculating age\nfrom datetime import datetime #converting string to date\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import GridSearchCV , train_test_split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import train_test_split,KFold,cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score , f1_score , make_scorer\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder , LabelEncoder ,normalize\nfrom sklearn.feature_selection import SelectKBest,f_classif,chi2\nfrom sklearn.metrics.classification import accuracy_score, log_loss\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nimport matplotlib.pyplot as plt\nfrom imblearn.over_sampling import SMOTE\nfrom matplotlib.pyplot import figure\nprint(\"DONE\")","978e40bf":"dd1=pd.read_csv(\"..\/input\/lt-vehicle-loan-default-prediction\/train.csv\")","17908fd7":"print(\"Total Size of the Provided Data: \",dd1.shape)","b8e39727":"print(\"Provided Features: \",dd1.shape[1])","8e4fd72c":"#Lets looks at data description\ninfo = pd.read_csv(\"..\/input\/lt-vehicle-loan-default-prediction\/data_dictionary.csv\")\ninfo","b59d5fd0":"# CHANGING TO UPPER CASE\ndd1.columns=dd1.columns.str.upper()\n# REPLACING \".\" with \"_\" in column names\ndd1.columns=dd1.columns.str.replace(\".\",\"_\")","5760431b":"dd1.head()","5fd26585":"dd1.head()","9a65ea00":"dd1.columns","e987c1f0":"dd1['LOAN_DEFAULT'].value_counts()","452bface":"#Graph\nmy_pal = {0: 'lightblue', 1: 'red'}\n\nplt.figure(figsize = (12, 6))\nax = sns.countplot(x = 'LOAN_DEFAULT', data = dd1, palette = my_pal)\nplt.title('Class Distribution')\nplt.show()","efd08116":"dd1.describe()","c006f3b7":"dd1.info()","e7ba7f3f":"dd1.isnull().sum()","3947a263":"dd1['EMPLOYMENT_TYPE'].unique()","7eb24676":"## TOTAL NULL VALUES\ndd1['EMPLOYMENT_TYPE'].isna().sum()","bb026f66":"dd1['EMPLOYMENT_TYPE'].value_counts()","b694d11e":"dd1['EMPLOYMENT_TYPE'] = dd1['EMPLOYMENT_TYPE'].fillna(method = 'bfill')","6653b1fb":"dd1['EMPLOYMENT_TYPE'].unique()","bfab9457":"dd1['EMPLOYMENT_TYPE'].isna().sum()","31e5c731":"dd1['EMPLOYMENT_TYPE'].value_counts()","1331c335":"dd1.nunique()","f3befac8":"dd1['PERFORM_CNS_SCORE_DESCRIPTION'].nunique()","2579b0bb":"dd1['PERFORM_CNS_SCORE_DESCRIPTION'].unique()","dbd817ae":"dd1['PERFORM_CNS_SCORE_DESCRIPTION'].value_counts()","729a4173":"dd1 = dd1.replace({'PERFORM_CNS_SCORE_DESCRIPTION':{'C-Very Low Risk':'Low', 'A-Very Low Risk':'Low',\n                                                       'B-Very Low Risk':'Low', 'D-Very Low Risk':'Low',\n                                                       'F-Low Risk':'Low', 'E-Low Risk':'Low', 'G-Low Risk':'Low',\n                                                       'H-Medium Risk': 'Medium', 'I-Medium Risk': 'Medium',\n                                                       'J-High Risk':'High', 'K-High Risk':'High','L-Very High Risk':'Very_High',\n                                                       'M-Very High Risk':'Very_High','Not Scored: More than 50 active Accounts found':'Not_Scored',\n                                                       'Not Scored: Only a Guarantor':'Not_Scored','Not Scored: Not Enough Info available on the customer':'Not_Scored',\n                                                        'Not Scored: No Activity seen on the customer (Inactive)':'Not_Scored','Not Scored: No Updates available in last 36 months':'Not_Scored',\n                                                       'Not Scored: Sufficient History Not Available':'Not_Scored', 'No Bureau History Available':'Not_Scored'\n                                                       }})","e63b3ccc":"dd1[\"PERFORM_CNS_SCORE_DESCRIPTION\"].unique()","6ce69f6d":"dd1['PERFORM_CNS_SCORE_DESCRIPTION'].value_counts()","c5699b96":"for i in dd1.columns:\n    print('Distinct_values for the column:',i)\n    print('No.of unique items:',dd1[i].nunique())\n    print(dd1[i].unique())\n    print('-'*30)\n    print('')","4d90fbfd":"dd1.info()","b2591a75":"# Before Convertion\ndd1['AVERAGE_ACCT_AGE']","d459cb1d":"dd1[['AVERAGE_ACCT_Yr','AVERAGE_ACCT_Month']] = dd1['AVERAGE_ACCT_AGE'].str.split(\"yrs\",expand=True)\ndd1[['AVERAGE_ACCT_Month','AVERAGE_ACCT_Month1']] = dd1['AVERAGE_ACCT_Month'].str.split(\"mon\",expand=True)\ndd1[\"AVERAGE_ACCT_AGE\"]= dd1[\"AVERAGE_ACCT_Yr\"].astype(str).astype(int)+((dd1[\"AVERAGE_ACCT_Month\"].astype(str).astype(int))\/12)\ndd1= dd1.drop(columns= [\"AVERAGE_ACCT_Yr\",\"AVERAGE_ACCT_Month\",'AVERAGE_ACCT_Month1'])","3c52f7c2":"dd1[['CREDIT_HISTORY_LENGTH_Yr','CREDIT_HISTORY_LENGTH_Month']] = dd1['CREDIT_HISTORY_LENGTH'].str.split(\"yrs\",expand=True)\ndd1[['CREDIT_HISTORY_LENGTH_Month','CREDIT_HISTORY_LENGTH_Month1']] = dd1['CREDIT_HISTORY_LENGTH_Month'].str.split(\"mon\",expand=True)\ndd1[\"CREDIT_HISTORY_LENGTH\"]= dd1[\"CREDIT_HISTORY_LENGTH_Yr\"].astype(str).astype(int)+((dd1[\"CREDIT_HISTORY_LENGTH_Month\"].astype(str).astype(int))\/12)\ndd1= dd1.drop(columns= [\"CREDIT_HISTORY_LENGTH_Yr\",\"CREDIT_HISTORY_LENGTH_Month\",'CREDIT_HISTORY_LENGTH_Month1'])\n","082edea2":"## Aftering Converting\ndd1['AVERAGE_ACCT_AGE']","c91bf2a6":"now = pd.Timestamp('now')\ndd1['DATE_OF_BIRTH'] = pd.to_datetime(dd1['DATE_OF_BIRTH'], format='%d-%m-%y')\ndd1['DATE_OF_BIRTH'] = dd1['DATE_OF_BIRTH'].where(dd1['DATE_OF_BIRTH'] < now, dd1['DATE_OF_BIRTH'] -  np.timedelta64(100, 'Y'))\ndd1['AGE'] = (now - dd1['DATE_OF_BIRTH']).astype('<m8[Y]')","135c83a8":"now = pd.Timestamp('now')\ndd1['DISBURSALDATE'] = pd.to_datetime(dd1['DISBURSALDATE'], format='%d-%m-%y')\ndd1['DISBURSALDATE'] = dd1['DISBURSALDATE'].where(dd1['DISBURSALDATE'] < now, dd1['DISBURSALDATE'] -  np.timedelta64(100, 'Y'))\ndd1['LOAN_AGE'] = (now - dd1['DISBURSALDATE']).astype('<m8[Y]')","a580116f":"sns.distplot(dd1['AGE'], color = 'blue')\nplt.title('Distribution of Age')","6c89849b":"dd1['DATE_OF_BIRTH'].dtypes","38fa8e21":"dd1.info()","cf76697e":"df=dd1.copy()","cf5f12f7":"y=df['LOAN_DEFAULT']\nX=df.drop(\"LOAN_DEFAULT\",axis=1)","fdeca138":"# from sklearn.model_selection import train_test_split,KFold,cross_val_score\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)","5a90e6e6":"print(\"Size of X\",X.shape)\nprint(\"Size of y\",y.shape)\nprint(\"Size of X_train\",X_train.shape)\nprint(\"Size of y_train\",y_train.shape)","2abb32fc":"y_train_plot=pd.DataFrame(y_train,columns=['LOAN_DEFAULT'])\ny_test_plot=pd.DataFrame(y_test,columns=['LOAN_DEFAULT'])\n\ndefaulters_train=y_train_plot['LOAN_DEFAULT'].sum()\nnon_defaulters_train=len(y_train_plot)-y_train_plot['LOAN_DEFAULT'].sum()\ntotal_train=len(y_train_plot)\n\ndefaulters_test=y_test_plot['LOAN_DEFAULT'].sum()\nnon_defaulters_test=len(y_test_plot)-y_test_plot['LOAN_DEFAULT'].sum()\ntotal_test=len(y_test_plot)","88ed25ec":"print(\"\\n\")\nprint(\"X_TRAIN INFO: Total:\",total_train)\nprint(\"DEFAULTERS:\",defaulters_train,\"->\",round(defaulters_train\/total_train,2),\"Percent\")\nprint(\"Non-DEFAULTERS:\",non_defaulters_train,\"->\",round(non_defaulters_train\/total_train,2),\"%\")\nprint(\"\\n\")\nprint(\"X_TEST INFO: Total:\",total_test)\nprint(\"DEFAULTERS:\",defaulters_test,\"->\",round(defaulters_test\/total_test,2),\"Percent\")\nprint(\"Non-DEFAULTERS:\",non_defaulters_test,\"->\",round(non_defaulters_test\/total_test,2),\"%\")","50c000f5":"#Graph\nmy_pal = {0: 'lightblue', 1: 'red'}\n\nplt.figure(figsize = (6, 3))\nax = sns.countplot(x = 'LOAN_DEFAULT', data = y_train_plot, palette = my_pal)\nplt.title('X_Train Class Distribution')\nplt.show()","617bde44":"#Graph\nmy_pal = {0: 'lightblue', 1: 'red'}\n\nplt.figure(figsize = (6, 3))\nax = sns.countplot(x = 'LOAN_DEFAULT', data = y_test_plot, palette = my_pal)\nplt.title('X_Test Class Distribution')\nplt.show()","a8f25d78":"columnsToDelete = ['UNIQUEID','MOBILENO_AVL_FLAG','CURRENT_PINCODE_ID',\n                   'EMPLOYEE_CODE_ID','STATE_ID','BRANCH_ID','MANUFACTURER_ID',\n                   'SUPPLIER_ID','DATE_OF_BIRTH','DISBURSALDATE','NO_OF_INQUIRIES']\n","8876572e":"## BEFORE DELETING THE COLUMNS\nprint(\"Size AFTER Deleting the Features\",len(X_train.columns))\n\n## DROPING THE COLUMNS FROM THE DATA FRAME\nX_train=X_train.drop(X_train[columnsToDelete],axis=1)\n\n## AFTER DROPPING THE COLUMNS\nprint(\"Size AFTER Deleting the Features\",len(X_train.columns))","771fb0a3":"X_train.columns","1d847b71":"numericalTypes=['DISBURSED_AMOUNT', 'ASSET_COST', 'PRI_NO_OF_ACCTS', 'PRI_ACTIVE_ACCTS', 'LTV',\n           'PRI_OVERDUE_ACCTS', 'PRI_CURRENT_BALANCE', 'PRI_SANCTIONED_AMOUNT', \n           'PRI_DISBURSED_AMOUNT', 'SEC_NO_OF_ACCTS', 'SEC_ACTIVE_ACCTS', 'SEC_OVERDUE_ACCTS', \n           'SEC_CURRENT_BALANCE', 'SEC_SANCTIONED_AMOUNT', 'SEC_DISBURSED_AMOUNT', \n           'PRIMARY_INSTAL_AMT', 'SEC_INSTAL_AMT', 'NEW_ACCTS_IN_LAST_SIX_MONTHS', 'PERFORM_CNS_SCORE',\n           'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', 'LOAN_AGE','AGE','AVERAGE_ACCT_AGE','CREDIT_HISTORY_LENGTH']\n\ncategoricalTypes=[ 'AADHAR_FLAG', 'PAN_FLAG', 'VOTERID_FLAG',\n             'DRIVING_FLAG', 'PASSPORT_FLAG','EMPLOYMENT_TYPE','PERFORM_CNS_SCORE_DESCRIPTION']","21e386f4":"len(numericalTypes)","92552cc4":"len(categoricalTypes)","119d08e5":"## Creating New dataframe for Numerical and Categorical \nX_train_numerical=X_train[numericalTypes].copy()\nX_test_numerical=X_test[numericalTypes].copy()","43d82332":"n = SelectKBest(score_func=f_classif,k=16)\n# numcols=n.fit(dd11[numericalTypes],dd11['LOAN_DEFAULT'])\nnumcols=n.fit(X_train_numerical,y_train)\nplt.figure(figsize=(7,7))\nsns.barplot(x=numcols.scores_,y=numericalTypes)\nplt.title('Best Numerical Features')\nplt.show()","a666e3e2":"## Creating dictionaries to store the feature names and its importance value\ntopNumFeatures={}\n\n## https:\/\/machinelearningmastery.com\/feature-selection-with-categorical-data\/ \nfor i in range(len(n.scores_)):\n    topNumFeatures[numericalTypes[i]]=n.scores_[i]\n\n# SORT THE DICTIONARY AS PER THE IMPORTANT SCORES\ntopNumFeatures = sorted(topNumFeatures.items(), key=lambda x: x[1],reverse=True) \nprint(\"-----------------------TOP FEATURES SORTED AS PER THE HIGH IMPORTANCE----------------\")\ntopNumFeatures","c743bb14":"X_train_numerical.loc[:,'No_of_Accounts'] = X_train_numerical['PRI_NO_OF_ACCTS'] + X_train_numerical['SEC_NO_OF_ACCTS']\nX_train_numerical.loc[:,'PRI_Inactive_accounts'] = X_train_numerical['PRI_NO_OF_ACCTS'] - X_train_numerical['PRI_ACTIVE_ACCTS']\nX_train_numerical.loc[:,'SEC_Inactive_accounts'] = X_train_numerical['SEC_NO_OF_ACCTS'] - X_train_numerical['SEC_ACTIVE_ACCTS']\nX_train_numerical.loc[:,'Total_Inactive_accounts'] = X_train_numerical['PRI_Inactive_accounts'] + X_train_numerical['SEC_Inactive_accounts']\nX_train_numerical.loc[:,'Total_Overdue_Accounts'] = X_train_numerical['PRI_OVERDUE_ACCTS'] + X_train_numerical['SEC_OVERDUE_ACCTS']\nX_train_numerical.loc[:,'Total_Current_Balance'] = X_train_numerical['PRI_CURRENT_BALANCE'] + X_train_numerical['SEC_CURRENT_BALANCE']\nX_train_numerical.loc[:,'Total_Sanctioned_Amount'] = X_train_numerical['PRI_SANCTIONED_AMOUNT'] + X_train_numerical['SEC_SANCTIONED_AMOUNT']\nX_train_numerical.loc[:,'Total_Disbursed_Amount'] = X_train_numerical['PRI_DISBURSED_AMOUNT'] + X_train_numerical['SEC_DISBURSED_AMOUNT']\nX_train_numerical.loc[:,'Total_Installment'] = X_train_numerical['PRIMARY_INSTAL_AMT'] + X_train_numerical['SEC_INSTAL_AMT']\n\n\n\nX_test_numerical.loc[:,'No_of_Accounts'] = X_test_numerical['PRI_NO_OF_ACCTS'] + X_test_numerical['SEC_NO_OF_ACCTS']\nX_test_numerical.loc[:,'PRI_Inactive_accounts'] = X_test_numerical['PRI_NO_OF_ACCTS'] - X_test_numerical['PRI_ACTIVE_ACCTS']\nX_test_numerical.loc[:,'SEC_Inactive_accounts'] = X_test_numerical['SEC_NO_OF_ACCTS'] - X_test_numerical['SEC_ACTIVE_ACCTS']\nX_test_numerical.loc[:,'Total_Inactive_accounts'] = X_test_numerical['PRI_Inactive_accounts'] + X_test_numerical['SEC_Inactive_accounts']\nX_test_numerical.loc[:,'Total_Overdue_Accounts'] = X_test_numerical['PRI_OVERDUE_ACCTS'] + X_test_numerical['SEC_OVERDUE_ACCTS']\nX_test_numerical.loc[:,'Total_Current_Balance'] = X_test_numerical['PRI_CURRENT_BALANCE'] + X_test_numerical['SEC_CURRENT_BALANCE']\nX_test_numerical.loc[:,'Total_Sanctioned_Amount'] = X_test_numerical['PRI_SANCTIONED_AMOUNT'] + X_test_numerical['SEC_SANCTIONED_AMOUNT']\nX_test_numerical.loc[:,'Total_Disbursed_Amount'] = X_test_numerical['PRI_DISBURSED_AMOUNT'] + X_test_numerical['SEC_DISBURSED_AMOUNT']\nX_test_numerical.loc[:,'Total_Installment'] = X_test_numerical['PRIMARY_INSTAL_AMT'] + X_test_numerical['SEC_INSTAL_AMT']","d15359ff":"X_test_numerical.columns","d9c7ba63":"X_train_numerical.columns","d1020b5e":"print(\"Shape of X_train_numerical: \",X_train_numerical.shape)\nprint(\"Shape of X_test_numerical: \",X_test_numerical.shape)","a682475b":"X_train_numerical=X_train_numerical.drop(['PRI_NO_OF_ACCTS','SEC_NO_OF_ACCTS',\n\t\t\t'PRI_ACTIVE_ACCTS','SEC_ACTIVE_ACCTS',\n\t\t\t'PRI_CURRENT_BALANCE','SEC_CURRENT_BALANCE',\n\t\t\t'PRI_Inactive_accounts','SEC_Inactive_accounts',\n            'PRI_SANCTIONED_AMOUNT','SEC_SANCTIONED_AMOUNT',\n            'PRI_DISBURSED_AMOUNT','SEC_DISBURSED_AMOUNT',\n            'PRI_OVERDUE_ACCTS','SEC_OVERDUE_ACCTS',\n            'PRIMARY_INSTAL_AMT','SEC_INSTAL_AMT'],axis=1)\n\nX_test_numerical=X_test_numerical.drop(['PRI_NO_OF_ACCTS','SEC_NO_OF_ACCTS',\n\t\t\t'PRI_ACTIVE_ACCTS','SEC_ACTIVE_ACCTS',\n\t\t\t'PRI_CURRENT_BALANCE','SEC_CURRENT_BALANCE',\n\t\t\t'PRI_Inactive_accounts','SEC_Inactive_accounts',\n            'PRI_SANCTIONED_AMOUNT','SEC_SANCTIONED_AMOUNT',\n            'PRI_DISBURSED_AMOUNT','SEC_DISBURSED_AMOUNT',\n            'PRI_OVERDUE_ACCTS','SEC_OVERDUE_ACCTS',\n            'PRIMARY_INSTAL_AMT','SEC_INSTAL_AMT'],axis=1)     ","47dac030":"X_test_numerical.columns","ff87b129":"X_train_numerical.columns","5912ace0":"print(\"After Droping: Shape of X_train_numerical: \",X_train_numerical.shape)\nprint(\"After Droping: Shape of X_test_numerical: \",X_test_numerical.shape)","36778c30":"numericalTypesUpdated=['DISBURSED_AMOUNT', 'ASSET_COST', 'LTV', 'NEW_ACCTS_IN_LAST_SIX_MONTHS', \n           'PERFORM_CNS_SCORE','DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', 'LOAN_AGE','AGE',\n           'AVERAGE_ACCT_AGE','CREDIT_HISTORY_LENGTH',\n           'No_of_Accounts', 'Total_Inactive_accounts','Total_Overdue_Accounts', \n           'Total_Current_Balance','Total_Sanctioned_Amount', 'Total_Disbursed_Amount','Total_Installment']","30e9ddcf":"## Before Standardization\nX_train.head()","6e4c2621":"scaler = StandardScaler()\nscaler.fit(X_train_numerical)\nX_train_numerical_std = scaler.transform(X_train_numerical)\nX_test_numerical_std = scaler.transform(X_test_numerical)","0baa1351":"## Type of Returned Data\ntype(X_train_numerical_std)","c027de14":"X_train_numerical_std","952f4914":"X_train_numerical_std=pd.DataFrame(X_train_numerical_std,columns=['DISBURSED_AMOUNT', 'ASSET_COST', 'LTV', 'NEW_ACCTS_IN_LAST_SIX_MONTHS',\n       'PERFORM_CNS_SCORE', 'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', 'LOAN_AGE',\n       'AGE', 'AVERAGE_ACCT_AGE', 'CREDIT_HISTORY_LENGTH', 'No_of_Accounts',\n       'Total_Inactive_accounts', 'Total_Overdue_Accounts',\n       'Total_Current_Balance', 'Total_Sanctioned_Amount',\n       'Total_Disbursed_Amount', 'Total_Installment'])\n\nX_test_numerical_std=pd.DataFrame(X_test_numerical_std,columns=['DISBURSED_AMOUNT', 'ASSET_COST', 'LTV', 'NEW_ACCTS_IN_LAST_SIX_MONTHS',\n       'PERFORM_CNS_SCORE', 'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', 'LOAN_AGE',\n       'AGE', 'AVERAGE_ACCT_AGE', 'CREDIT_HISTORY_LENGTH', 'No_of_Accounts',\n       'Total_Inactive_accounts', 'Total_Overdue_Accounts',\n       'Total_Current_Balance', 'Total_Sanctioned_Amount',\n       'Total_Disbursed_Amount', 'Total_Installment'])","dbbce6b9":"# Checking the values after converting\nX_train_numerical_std","bb2e0a94":"print(\"Shape of Standardized X_train: \",X_train_numerical_std.shape)\nprint(\"Shape of Standardized X_test: \",X_test_numerical_std.shape)","dfa76198":"nn = SelectKBest(score_func=f_classif,k='all')\nnumcols=nn.fit(X_train_numerical_std,y_train)\nplt.figure(figsize=(7,7))\nsns.barplot(x=numcols.scores_,y=X_train_numerical_std.columns)\nplt.title('Best Numerical Features')\nplt.show()","b6eddc28":"## Creating dictionaries to store the feature names and its importance value\ntopNumFeatures={}\n\n## https:\/\/machinelearningmastery.com\/feature-selection-with-categorical-data\/ \nfor i in range(len(nn.scores_)):\n#     print('Feature %s: %f' % (numerical[i], n.scores_[i]))\n    topNumFeatures[numericalTypesUpdated[i]]=nn.scores_[i]\n\n# SORT THE DICTIONARY AS PER THE IMPORTANT SCORES\ntopNumFeatures = sorted(topNumFeatures.items(), key=lambda x: x[1],reverse=True) \nprint(\"-----------------------TOP FEATURES SORTED AS PER THE HIGH IMPORTANCE----------------\")\ntopNumFeatures","b52c6120":"X_train_categorical=X_train[categoricalTypes].copy()\nX_test_categorical=X_test[categoricalTypes].copy()","613d3f07":"X_train_categorical.columns","0511e175":"onehot_encoder = OneHotEncoder(sparse=False)\nX_train_categorical_encoded = onehot_encoder.fit(X_train_categorical)\nX_train_categorical_encoded = onehot_encoder.transform(X_train_categorical) ## NOT EXECUTED\nX_test_categorical_encoded = onehot_encoder.transform(X_test_categorical)","c1c352ba":"# Checking the Encoded Data\nX_train_categorical_encoded","5bca27a7":"print(\"Shape of X_train after One Hot Encoding: \",X_train_categorical_encoded.shape)","a56626fa":"type(X_train_categorical_encoded)","abce4f7c":"## Obtaining Feature Names from the Classifier\nonehot_encoder.get_feature_names(['AADHAR_FLAG', 'PAN_FLAG', 'VOTERID_FLAG', 'DRIVING_FLAG',\n       'PASSPORT_FLAG', 'EMPLOYMENT_TYPE', 'PERFORM_CNS_SCORE_DESCRIPTION'])","14ada6bd":"## Adding the Obtained feature names into LIST\nencodedCatColumnNames=['AADHAR_FLAG_0', 'AADHAR_FLAG_1', 'PAN_FLAG_0', 'PAN_FLAG_1',\n       'VOTERID_FLAG_0', 'VOTERID_FLAG_1', 'DRIVING_FLAG_0',\n       'DRIVING_FLAG_1', 'PASSPORT_FLAG_0', 'PASSPORT_FLAG_1',\n       'EMPLOYMENT_TYPE_Salaried', 'EMPLOYMENT_TYPE_Self employed',\n       'PERFORM_CNS_SCORE_DESCRIPTION_High',\n       'PERFORM_CNS_SCORE_DESCRIPTION_Low',\n       'PERFORM_CNS_SCORE_DESCRIPTION_Medium',\n       'PERFORM_CNS_SCORE_DESCRIPTION_Not_Scored',\n       'PERFORM_CNS_SCORE_DESCRIPTION_Very_High']","8b79878e":"X_train_categorical_encoded=pd.DataFrame(X_train_categorical_encoded,columns=['AADHAR_FLAG_0', 'AADHAR_FLAG_1', 'PAN_FLAG_0', 'PAN_FLAG_1',\n       'VOTERID_FLAG_0', 'VOTERID_FLAG_1', 'DRIVING_FLAG_0',\n       'DRIVING_FLAG_1', 'PASSPORT_FLAG_0', 'PASSPORT_FLAG_1',\n       'EMPLOYMENT_TYPE_Salaried', 'EMPLOYMENT_TYPE_Self employed',\n       'PERFORM_CNS_SCORE_DESCRIPTION_High',\n       'PERFORM_CNS_SCORE_DESCRIPTION_Low',\n       'PERFORM_CNS_SCORE_DESCRIPTION_Medium',\n       'PERFORM_CNS_SCORE_DESCRIPTION_Not_Scored',\n       'PERFORM_CNS_SCORE_DESCRIPTION_Very_High'])\n\nX_test_categorical_encoded=pd.DataFrame(X_test_categorical_encoded,columns=['AADHAR_FLAG_0', 'AADHAR_FLAG_1', 'PAN_FLAG_0', 'PAN_FLAG_1',\n       'VOTERID_FLAG_0', 'VOTERID_FLAG_1', 'DRIVING_FLAG_0',\n       'DRIVING_FLAG_1', 'PASSPORT_FLAG_0', 'PASSPORT_FLAG_1',\n       'EMPLOYMENT_TYPE_Salaried', 'EMPLOYMENT_TYPE_Self employed',\n       'PERFORM_CNS_SCORE_DESCRIPTION_High',\n       'PERFORM_CNS_SCORE_DESCRIPTION_Low',\n       'PERFORM_CNS_SCORE_DESCRIPTION_Medium',\n       'PERFORM_CNS_SCORE_DESCRIPTION_Not_Scored',\n       'PERFORM_CNS_SCORE_DESCRIPTION_Very_High'])","982d1014":"print(\"Shape of Encoded X_train Categorical: \",X_train_categorical_encoded.shape)\nprint(\"Shape of Encoded X_test Categorical: \",X_test_categorical_encoded.shape)","dc4fd563":"X_test_categorical_encoded","61836c06":"c = SelectKBest(score_func=chi2)\nnumcols=c.fit(X_train_categorical_encoded,y_train)\nplt.figure(figsize=(7,7))\nsns.barplot(x=numcols.scores_,y=encodedCatColumnNames)\nplt.title('Best Categorical Features')\nplt.show()","da3b8dc4":"## Creating dictionaries to store the feature names and its importance value\ntopCatFeatures={}\n\n## https:\/\/machinelearningmastery.com\/feature-selection-with-categorical-data\/ \nfor i in range(len(c.scores_)):\n#     print('Feature %s: %f' % (numerical[i], n.scores_[i]))\n    topCatFeatures[encodedCatColumnNames[i]]=c.scores_[i]\n\n# SORT THE DICTIONARY AS PER THE IMPORTANT SCORES\ntopCatFeatures = sorted(topCatFeatures.items(), key=lambda x: x[1],reverse=True) \nprint(\"-----------------------TOP FEATURES SORTED AS PER THE HIGH IMPORTANCE----------------\")\ntopCatFeatures","327499f8":"X_train_merged = pd.concat([X_train_numerical_std,X_train_categorical_encoded], axis=1)\nX_test_merged = pd.concat([X_test_numerical_std,X_test_categorical_encoded], axis=1)","88e34d46":"X_train_numerical_std","93cb40ba":"print(\"Shape of X_train Merged: \",X_train_merged.shape)\nprint(\"Shape of X_test Merged: \",X_test_merged.shape)","38dd11e2":"print(\"Shape of y_train : \",y_train.shape)\nprint(\"Shape of y_test : \",y_test.shape)","017c5fd0":"X_train_merged.columns","b95b0701":"y_train_corr=pd.DataFrame(y_train)\ny_test_corr=pd.DataFrame(y_test)","82e3dc8a":"y_train_corr.shape","091df94e":"y_train_corr=pd.DataFrame(y_train_corr,columns=['LOAN_DEFAULT'])\ny_test_corr=pd.DataFrame(y_test_corr,columns=['LOAN_DEFAULT'])","691d7b2f":"X_train_corr=X_train_merged.copy()","6580054c":"X_train_corr['LOAN_DEFAULT']=y_train.values","a37d83f1":"corr_mat = X_train_corr.corr()\n\nfig2=plt.figure()\nsns.set(rc={'figure.figsize':(30,15)})\nk = 34\ncols = corr_mat.nlargest(k, 'LOAN_DEFAULT')['LOAN_DEFAULT'].index\ncm = np.corrcoef(X_train_corr[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.title('Correlation Matrix')\nplt.show()","831bc187":"# This function plots the confusion matrices given y_i, y_i_hat.\ndef plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    A =(((C.T)\/(C.sum(axis=1))).T)\n    B =(C\/C.sum(axis=0))\n    plt.figure(figsize=(20,4))\n    \n    labels = [1,2]\n    # representing A in heatmap format\n    cmap=sns.light_palette(\"blue\")\n    plt.subplot(1, 3, 1)\n    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Confusion matrix\")\n    \n    plt.subplot(1, 3, 2)\n    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Precision matrix\")\n    \n    plt.subplot(1, 3, 3)\n    # representing B in heatmap format\n    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Recall matrix\")\n    \n    plt.show()","17ca621f":"test_len=len(y_test)","43fc67f6":"predicted_y = np.zeros((test_len,2))\nfor i in range(test_len):\n    rand_probs = np.random.rand(1,2)\n    predicted_y[i] = ((rand_probs\/sum(sum(rand_probs)))[0])\nprint(\"Log loss on Test Data using Random Model\",log_loss(y_test, predicted_y, eps=1e-15))\n\npredicted_y =np.argmax(predicted_y, axis=1)\nplot_confusion_matrix(y_test, predicted_y)","e549a812":"print(\"Accuracy On Random Model: \", 50.14)","5d2724b9":"alpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n\nlog_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42,class_weight=\"balanced\")\n    clf.fit(X_train_merged, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_train_merged, y_train)\n    predict_y = sig_clf.predict_proba(X_test_merged)\n    log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n    \n    ","93b9a8b2":"fig, ax = plt.subplots()\nax.plot(alpha, log_error_array,c='g')\nfor i, txt in enumerate(np.round(log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.rcParams[\"figure.figsize\"] = [10,7]\nplt.show()","3f890c10":"best_alpha = np.argmin(log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(X_train_merged, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_train_merged, y_train)\n\npredict_y = sig_clf.predict_proba(X_train_merged)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_test_merged)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\npredicted_y =np.argmax(predict_y,axis=1)\nprint(\"Total number of data points :\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","f5cf380c":"print(\"Accuracy On Random Model: \", 78.27)","ab9e1d4a":"#Importing Machine Learning Model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n    \n#Bagging Algo\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\n\n#statistical Tools\nfrom sklearn.metrics import roc_auc_score,accuracy_score,precision_score,recall_score,f1_score\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc","b4e18667":"lr = LogisticRegression(C=5.0,class_weight=\"balanced\")\nknn = KNeighborsClassifier(weights='distance', algorithm='auto', n_neighbors=15)\nrfc = RandomForestClassifier(n_estimators=300,criterion='gini',class_weight=\"balanced\")\ndtc = DecisionTreeClassifier(class_weight=\"balanced\")","4e9d0f16":"accuracy = {}\nroc_r = {}\n\ndef train_model(model):\n    # Checking accuracy\n    model = model.fit(X_train_merged, y_train)\n    pred = model.predict(X_test_merged)\n    acc = accuracy_score(y_test, pred)*100\n    accuracy[model] = acc\n    print('accuracy_score',acc)\n    print('precision_score',precision_score(y_test, pred)*100)\n    print('recall_score',recall_score(y_test, pred)*100)\n    print('f1_score',f1_score(y_test, pred)*100)\n    roc_score = roc_auc_score(y_test, pred)*100\n    roc_r[model] = roc_score\n    print('roc_auc_score',roc_score)\n    # confusion matrix\n    print('confusion_matrix')\n    plot_confusion_matrix(y_test,pred)\n#     print(pd.DataFrame(confusion_matrix(y_test, pred)))\n    fpr, tpr, threshold = roc_curve(y_test, pred)\n    roc_auc = auc(fpr, tpr)*100\n\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.rcParams[\"figure.figsize\"] = [7,7]\n#     plt.figure(figsize=(2,3))\n#     plt.figure(figsize=(3,4))\n    plt.show()","2bab7a78":"train_model(lr)","b7333326":"train_model(dtc)","0d393d3e":"a=15\na","a3734c6f":"train_model(knn)","8543e903":"train_model(rfc)","698887a3":"xgb = XGBClassifier(scale_pos_weight=3)","5fcc01ca":"train_model(xgb)","ce120bd3":"xgb = XGBClassifier(scale_pos_weight=2)","52c5ae7b":"train_model(xgb)","6804ad8d":"from prettytable import PrettyTable \n\n# Specify the Column Names while initializing the Table \nmyTable = PrettyTable([\"Classifier\", \"Accuracy\", \"AUC\"]) \n\n# Add rows \nmyTable.add_row([\"Random Model\", \"50.14\", \"50\"]) \nmyTable.add_row([\"Logistic Regression->SGD\", \"78\", \"-\"]) \nmyTable.add_row([\"Logistic Regression->SKLearn\", \"57\", \"59\"]) \nmyTable.add_row([\"Decision Tree\", \"67\", \"52\"]) \nmyTable.add_row([\"KNN\", \"77\", \"51\"]) \nmyTable.add_row([\"Random Forest\", \"77\", \"51\"]) \nmyTable.add_row([\"XGBoost v1\", \"65\", \"59\"]) \nmyTable.add_row([\"XGBoost v2\", \"77\", \"53\"]) \n\nprint(myTable)\n","98a5d308":"## Utility Functions","37f2a433":"## -> 1.2. Renaming Columns for better readability.","8369047e":"# 2. EXPLORATORY DATA ANALYSIS","ce992125":"### **OBSERVATION:**\n- ### We can see from the PDF that the Mean Age lies in the range of 20 - 30.","805369d2":"## **OBSERVATION:** \n- ### 1. XGBoostv2 with tuned parameter has high accuracy of 77 but the confusion matrix shows that the performance in classifying defaulters is dropped compared to its above version.\n- ### 2.  AUC is also reduced which is not recommended.","80289926":"## -> 3.6. CONVERTING THE DATE OF BIRTH TO AGE AND DISBURSALDATE TO LOAN_AGE","8cf0bc0b":"# 8. ENCODING THE CATEGORICAL VARIABLE USING ONE HOT ENCODER","b1f42c2f":"# IMPORTING MODELS ","5d945546":"## We'll use SelectKBest library to narrow down choices of features. This will make use of Annova test.","1a0b616e":"### Dropping the useless features.","b80e86bb":"## -> 3.5. Converting CREDIT HISTORY LEN and AVG ACCOUNT AGE into Numeric Format","0db6920b":"## **-> 12.7. BOOSTING: XGBOOST with Hyperparameter Tuning (scale_pos_weight) Parameter.**","5216fb43":"# Importing Libraries ","548cf84a":"## **-> 12.4. DECISION TREE**","230b2b35":"# 12. MODEL IMPLEMENTATION","ad74d8f3":"### Now we update our Numerical Feature List with the newly added Features","621160e9":"### **OBSERVATION:** \n- ### 1. AVERAGE_ACCT_AGE, CREDIT_HISTORY_LENGTH are object, but they should be int.\n- ### 2. DATE_OF_BIRTH & DISBURSAL_DATE should be datetime type.","28e991ea":"# 1. READING DATASET","1f98fb25":"# 3. DATA PREPROCESSING","6a38cbc3":"## -> 2.1. Checking whether the data is balanced or not\u00b6","ea46728a":"## -> 3.3. Converting PERFORM_CNS_SCORE_DESCRIPTION into Multiple Categories","3931138f":"## **-> 12.6. BAGGING: RANDOM FOREST CLASSIFIER**","2c0ec7ca":"### **OBSERVATION:** \n- ### The graph shows that Secondary Account informations are insignificant. But banks can't afford to drop Secondary Account informations.\n### **ACTION:**\n- ### We will combine the Primary and Secondary Account informations.","b7d8d8ad":"# 14. CONCLUSION\n### As per our experimentation with the EDA, Data Preprocessing, Cleaning and Modelling, we can conclude that the **XGBOOST model version 1**  gives us a balanced result of Prediction for Defaulters and also has a stable AUC value.","67813679":"## **-> 12.2. Logistic Regression with hyperparameter tuning.**","40182e87":"### **ACTION:**\n- ### Now we can safely delete DATE_OF_BIRTH and DISBURSALDATE, therefore will be deleted later","eccd59b7":"### **OBSERVATION:** The random model has as LogLoss of 0.88, therefore any other model must have a log loss and better accuracy than 50.12 ","dd9b3728":"## ->-> 12.7.1. XGBoost with scale_pos_weight=3","64b1ea96":"## -> 3.1. Missing value check","d762e1ce":"## -> 3.4. Understanding all the Distinct Values of our features","64b2ebfd":"Financial institutions incur significant losses due to the default of vehicle loans. This has led to the tightening up of vehicle loan underwriting and increased vehicle loan rejection rates. The need for a better credit risk scoring model is also raised by these institutions. This warrants a study to estimate the determinants of vehicle loan default. A financial institution has hired you to accurately predict the probability of loanee\/borrower defaulting on a vehicle loan in the first EMI (Equated Monthly Instalments) on the due date. Following Information regarding the loan and loanee are provided in the datasets:\n\nLoanee Information (Demographic data like age, Identity proof etc.)\n\nLoan Information (Disbursal details, loan to value ratio etc.)\n\nBureau data & history (Bureau score, number of active accounts, the status of other loans, credit history etc.)\n\nDoing so will ensure that clients capable of repayment are not rejected and important determinants can be identified which can be further used for minimising the default rates.","51e8e0d0":"### Our newly converted categories","39c6174b":"# C. MACHINE LEARNING OBJECTIVE\n - ## 1. To build a high accuracy model for the Binary Classification of Defaulter or Non Defaulter.\n - ## 2. To have good AUC Value.","e1821b98":"## -> 2.2. Descriptive Stats","1d1c169f":"# B.BUSINESS OBJECTIVE\n- ## To accurately predict the loanee\/borrower defaulting or non defaulting on a vehicle loan in the first EMI (Equated Monthly Instalments) on the due date.","d19a08f0":"### **OBSERVATION:** EMPLOYMENT_TYPE column has 7661 NaN Values which needs to be handled","bb3683e7":"# 11. CORELATION MATRIX","548da677":"## Updated total no. of columns ond the Data Type.dd1.columns","f688c62f":"## -> 2.3. Structure of the data","b5930adb":"# Creating Objects of the MODELS \n- ## 1. Logistic Regression\n- ## 2. KNN\n- ## 3. Decision\n- ## 4. Random Forest\n- ## 5. XGBoost","5ccbd7b7":"## **OBSERVATION:** \n- ### 1. KNN accuracy is very also high at 77 but the confusion matrix shows the poor performance in classifying defaulters than compared to the Logistic Regression which has lower accuracy.\n- ### 2.  AUC is same as Random Model.","c0be1731":"## Now dropping the unwanted columns as we have already merged them","e3278f3c":"## -> 5.1. Identifying Useless Features","aa53a960":"## -> 5.2. Listing the Numerical and Categorical Type Features.","b44a9a3a":"# 5. FEATURE SELECTION","46c21f82":"## We'll use SelectKBest library to narrow down choices of features. This will make use of CHI2 test.","b36d9c25":"1. UNIQUEID = It is provided to every customer so its Unique and will always be different.<\/br>\n1. MOBILENO_AVL_FLAG = Whether person provided Mobile No. Doesn't tell us if loan will default.<\/br>\n1. CURRENT_PINCODE_ID = It is Customers address we don't need that for Prediction.<\/br>\n1. EMPLOYEE_CODE_ID = Employee ID is not required as it doesn't related with Loan_defualt.<\/br>\n1. NO_OF_INQUIRIES = No. of Inquiries to loan doesn't help us to determine wheather loan will default or not.<\/br>\n1. STATE_ID = It is where loan is availed and doesn't add much to prediction to loan default.<\/br>\n1. BRANCH_ID = Branch ID isn't relevent to Data Processing.<\/br>\n1. MANUFACTURER_ID = Manufacturer ID doesn't add much too data.<\/br>\n1. SUPPLIER_ID = Supplier ID doesn't add much too data.<\/br>\n1. DATE_OF_BIRTH= As we have calculate the AGE we dont need the Date of Birth.<\/br>\n1. DISBURSALDATE= As we have calculate the LOAN_AGE we dont need the Disbursal Date.<\/br>","88d8f049":"### Converting the ndarray to Pandas DataFrame with Column Names","4cb4cca0":"### Combining the Primary and Secondry information ","3429698c":"# -------------INDEX OF THE SOLUTION---------------\n# A. DATA INFORMATION\n# B. BUSINESS OBJECTIVE\n# C. MACHINE LEARNING OBJECTIVE\n# 1. READING DATASET\n- ### -> 1.1. Data Description\n- ### -> 1.2. Renaming Columns for better readability.\n\n# 2. EXPLORATORY DATA ANALYSIS\n- ### -> 2.1. Checking whether the data is balanced or not\u00b6\n- ### -> 2.1. Descriptive Stats\n- ### -> 2.1. Structure of the data\n\n# 3. DATA PREPROCESSING\n- ### -> 3.1. Missing\/NaN value check\n- ### -> 3.2. NAN Value Imputation with Backward Fill (bfill) method.\n- ### -> 3.3. Converting PERFORM_CNS_SCORE_DESCRIPTION into Multiple Categories\n- ### -> 3.4. Understanding all the Distinct Values of our features\n- ### -> 3.5. Converting CREDIT HISTORY LEN and AVG ACCOUNT AGE into Numeric Format\n- ### -> 3.6. CONVERTING THE DATE OF BIRTH TO AGE AND DISBURSALDATE TO LOAN_AGE\n\n# 4. SPLITTING THE DATASET INTO TRAIN-TEST (70-30)\n\n# 5. FEATURE SELECTION\n- ### -> 5.1. Identifying Useless Features and dropping them.\n- ### -> 5.2. Listing the Numerical and Categorical Type Features.\n\n# 6. SELECTING THE TOP NUMERICAL FEATURES using SelectKBest (Annona)\n\n# 7. STANDARDIZING THE TRAIN AND TEST DATA\n\n# 8. ENCODING THE CATEGORICAL VARIABLE USING ONE HOT ENCODER\n\n# 9. SELECTING THE TOP CATEGORICAL FEATURES\n\n# 10. MERGING THE NUMERICAL AND CATEGORICAL PROCESSSED FEATURES\n# 11. CORELATION MATRIX\n\n# 12. MODEL IMPLEMENTATION\n- ### -> 12.1. RANDOM MODEL as Benchmark.\n- ### -> 12.2. Logistic Regression with hyperparameter tuning.\n- ### -> 12.Creating Objects of the MODELS\n- ### -> 12.DATASET Balancing using Classifier Parameter\n- ### -> 12.3. Logistic Regression\n- ### -> 12.4. DECISION TREE\n- ### -> 12.5. KNN\n- ### -> 12.6. BAGGING: RANDOM FOREST CLASSIFIER\n- ### -> 12.7. BOOSTING: XGBOOST with Hyperparameter Tuning (scale_pos_weight) Parameter.\n- ### ->-> 12.7.1. XGBoost with scale_pos_weight=3\n- ### ->-> 12.7.2. XGBoost with scale_pos_weight=2\n\n# 13. COMPARISION\n\n# 14. CONCLUSION","d5e418a8":"# NOTE: Data is balanced with the help of the Classifiers inbuilt \"class_weight\" parameter.\n### For optimal performance, it is always recommened to use the inbuilt data balancing parameters of classifiers to balance the data than any other techniques of Upsamling like Sampling with Repeatation or SMOTE: Synthetic Minority Over-sampling Technique. Hence we are using the Classifier parameter.","739d8b61":"# 4. SPLITTING THE DATASET INTO TRAIN-TEST (70-30)","9ccba8cc":"### We have missing values only in the variable \"EMPLOYMENT_TYPE\" which we will impute it using the backward fill method","e378cf31":"# 6. SELECTING THE TOP NUMERICAL FEATURES","c3fc8fb6":"# A. DATA INFORMATION","2f4bd4ed":"### **OBSERVATION:**\n- ### 1. From the above information we can say that the variable MOBILENO_AVL_FLAG has only 1 class hence we can drop that variable as is not going to provide any unique information to our model.\n- ### 1. PERFORM_CNS_SCORE_DESCRIPTION seems important, need to be handled as it related to the CNS SCORE.","0e095817":"# ->-> 12.7.2. XGBoost with scale_pos_weight=2","84ee517c":"## **OBSERVATION:** \n- ### 1. Random Forest accuracy is very high at 77 but the confusion matrix shows is poor performance in classifying defaulters than compared to the Logistic Regression which has lower accuracy.\n- ### 2.  AUC is same as Random Model.","03b8599c":"# -----------------------------------------------------------------------------------------------------------------","03413a50":"## **OBSERVATION:** \n- ### The SGD model with Log loss has a very accuracy of 78.27 because of Imbalanced Data, however as we can see from the Confusion Matrix, that the model is performing very poor in classifying the Defaulters","8debd0af":"### **OBSERVATION:**\n- #### Best value of Alpha is: 0.001","f0cb5ff9":"# 13. COMPARISION","a0ae33e2":"### **OBSERVATION:**\n1. We need to change the date and time related variables (DATE_OF_BIRTH, DISBURSALDATE,AVERAGE_ACCT_AGE).<\/br>\n1. EMPLOYMENT_TYPE has less data, we need to analyse it further.<\/br>\n1. The beauruea data history is given under the variable PERFORM_CNS.SCORE.DESCRIPTION with 20 distinct classes we need to figure out a way to encode it such that its acceptable to our model\n\n","78f0ea63":"### **OBSERVATION:**\n- ### 1. CREDIT HISTORY LEN and AVG ACCOUNT AGE needs to converted into valid numeric format.","884d1a13":"# 7. STANDARDIZING THE TRAIN AND TEST DATA","b69a1d74":"### Converting ndarray to Pandas Data Frame","b8d920e9":"# 9. SELECTING THE TOP CATEGORICAL FEATURES","88f631b7":"# 10. MERGING THE NUMERICAL AND CATEGORICAL PROCESSSED FEATURES","b5615843":"## **OBSERVATION:** \n- ### 1. XGBoost_v1 accuracy is 65. The confusion matrix shows high performance in classifying defaulters than compared to other models excluding the Logistic Regression. \n- ### 2.  AUC is also high than all other models.","096abbfa":"## > 1.1. Data Description","7558a2a0":"## **-> 12.1. RANDOM MODEL as Benchmark.**","8cb30dac":"### **OBSERVATION:** Data is higly imbalanced\n### **ACTION:** Data need to made BALANCED either using SMOTE or using Classfier's class_weight='balanced' technique.","6442ea07":"## Reconfirming the K BEST FEATURES after standardization ","96d455dd":"## **-> 12.5. KNN**","68d29af7":"## **OBSERVATION:** \n- ### 1. The SKLearns Logistic Regression model has an accuracy of 57%, however as we can see from the Confusion Matrix, that the model is performing better as compared to the SGD Variant in classifying the Defaulters.\n- ### 2. The AUC is roughly at 60 than compared to the Random Model's 50.","7ac77f19":"## **OBSERVATION:** \n- ### 1. Decision Tree is performing poor in terms of Accuracy and AUC Scores.\n- ### 2. The Precision and Recall Matrix show the improved performance in classifying the Defaulters.","3b0e7daa":"### **OBSERVATION:**\n- ## The final no. of features that we will be working are 34. Check the below output for the list of final features.","09f3f218":"## **-> 12.3. Logistic Regression**","cd349fe7":"## -> 3.2. NaN Value Imputation with Backward Fill (bfill) method."}}