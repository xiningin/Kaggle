{"cell_type":{"a7ed5cbd":"code","5ab806d6":"code","c43cfa09":"code","e36900ab":"code","e2bc907d":"code","5b6f339f":"code","66188c14":"code","b2a05854":"code","a8b23c90":"code","ff5111d9":"code","e0841600":"code","e62dd387":"code","87e7abff":"code","30728151":"code","b72131d1":"markdown","f8124de3":"markdown","bd2a7b51":"markdown","2e37cdea":"markdown","242a364d":"markdown","0f25baed":"markdown","8a0ad113":"markdown","099ae5d5":"markdown","c24973eb":"markdown","0629fda1":"markdown","76069fdc":"markdown","2a4b15af":"markdown"},"source":{"a7ed5cbd":"'''--- Libraries ---'''\n\n# Generic Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os, gc, warnings, datetime, pickle, re\nwarnings.filterwarnings(\"ignore\")\n\n\n# Plotting\nimport plotly.express as px\nimport plotly.graph_objects as go\n#from pyLDAvis import sklearn as sklda\nimport pyLDAvis.gensim \nimport pyLDAvis.sklearn\n\n#Gensim Library for Text Processing\nimport gensim.parsing.preprocessing as gsp\nfrom gensim import utils, corpora, models\n\n# SK Learn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA\n\n# Transformer\nfrom transformers import TFAutoModelWithLMHead, AutoTokenizer, pipeline\n\n# Spacy\nimport spacy\nfrom spacy.matcher import Matcher\nnlp = spacy.load('en_core_web_sm',disable=['ner','textcat'])","5ab806d6":"'''--- Data ---'''\n\n# Load\nurl = '..\/input\/speeches-modi\/PM_Modi_speeches.csv'\ndata = pd.read_csv(url, header='infer')\n\n# Convert date column datatype to datetime\ndata['date'] = pd.to_datetime(data['date'], errors='ignore')\n\n# Extract Year\ndata['year'] = pd.DatetimeIndex(data['date']).year\n\n# Selecting only english speeches delivered in 2020\ndf = data[(data['year'] == 2020) & (data['lang'] =='en')]\n\n# Dropping Unwanted Columns\ndf.drop(['lang','year', 'url'], axis=1, inplace=True)\n\n# Total Speeches\nprint(\"Total Speeches Made in 2020: \", df.shape[0])\n\n# Inspect\ndf.head()","c43cfa09":"# Visualize\nfig = px.line(df, x='date', y='words', title=\"Speeches made by P.M Mr Modi in 2020\")\nfig.show()","e36900ab":"'''Selecting Records''' \nword_count = 20000\ndf_20k = df[df['words'] >= word_count]\ndf_20k.reset_index(drop=True, inplace=True)\n\n\n\n'''Text Cleaning Utility Function'''\n\nprocesses = [\n               gsp.strip_tags, \n               gsp.strip_punctuation,\n               gsp.strip_multiple_whitespaces,\n               gsp.strip_numeric,\n               gsp.remove_stopwords, \n               gsp.strip_short #, \n               #gsp.stem_text,\n               #utils.tokenize\n            ]\n\n# Utility Function\ndef proc_txt(txt):\n    text = txt.lower()\n    text = utils.to_unicode(text)\n    for p in processes:\n        text = p(text)\n    return text\n\n# Applying the function to text column\ndf_20k['cleaned_txt'] = df_20k['text'].apply(lambda x: proc_txt(x))\n\n\n# Inspect\ndf_20k.head()","e2bc907d":"# Initialise the count vectorizer with the English stop words\ncount_vectorizer = CountVectorizer(stop_words='english')\n\n# Fit and transform the processed titles\ncount_data = count_vectorizer.fit_transform(df_20k['cleaned_txt'])\n\n\n# Parameters\nnum_topics = 5    # can be changed\nnum_words = 20    # can be changed\n\n\n# Utility Function\ndef topics (model, vectors,num_top_wrds):\n    words = count_vectorizer.get_feature_names()\n    \n    print(f\"*** {num_topics} TOPICS DISPLAYED WITH {num_top_wrds} WORDS ***\\n\")\n    \n    for topic_idx, topic in enumerate(model.components_):\n        print(\"Topic Index: %d\" %topic_idx)\n        print(\" \".join([words[i]\n                        for i in topic.argsort()[:-num_top_wrds - 1:-1]]), \"\\n\")\n\n# LDA Model\nlda = LDA(n_components=num_topics, n_jobs=-1, random_state=42, verbose=0)\nlda.fit(count_data)\n              \n# Topics Detected by LDA Model\ntopics(lda, count_vectorizer, num_words)","5b6f339f":"# Visualize Topics\nLDAvis_data_filepath = os.path.join('.\/ldavis_prepared_'+str(num_topics))\nLDAvis_prepared = pyLDAvis.sklearn.prepare(lda, count_data, count_vectorizer)\npyLDAvis.display(LDAvis_prepared)","66188c14":"'''Text Cleaning Utility Function'''\n\nprocesses = [\n               gsp.strip_tags, \n               gsp.strip_punctuation,\n               gsp.strip_multiple_whitespaces,\n               gsp.strip_numeric,\n               gsp.remove_stopwords, \n               gsp.strip_short, \n               #gsp.stem_text,\n               utils.tokenize\n            ]\n\n# Utility Function\ndef proc_txt(txt):\n    text = txt.lower()\n    text = utils.to_unicode(text)\n    for p in processes:\n        text = p(text)\n    return list(text)\n\n# Applying the function to text column\ndf_20k['cleaned_txt'] = df_20k['text'].apply(lambda x: proc_txt(x))","b2a05854":"# Dictionary & Corpus\ndictionary = corpora.Dictionary(df_20k['cleaned_txt'])\ncorpus = [dictionary.doc2bow(txt) for txt in df_20k['cleaned_txt']]\n\n# Saving the dictionary\npickle.dump(corpus, open('corpus.pkl', 'wb'))\ndictionary.save('dictionary.gensim')","a8b23c90":"# Gensim LDA Model\nlda_model = models.ldamodel.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=20)\nlda_model.save(\"ldamodel.gensim\")\n\n# Display Topics\nprint(f\"*** {num_topics} TOPICS DISPLAYED WITH {num_words} WORDS ***\\n\")\ntopics = lda_model.print_topics(num_words=num_words)\nfor topic in topics:\n    print(topic,\"\\n\")\n    \n# Save Dictionary\ndictionary = corpora.Dictionary.load('dictionary.gensim')\ncorpus = pickle.load(open('corpus.pkl', 'rb'))\nlda = models.ldamodel.LdaModel.load('ldamodel.gensim')\n\n# Visualize Topics\nlda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\npyLDAvis.display(lda_display)","ff5111d9":"# Garbage Collection\ngc.collect()","e0841600":"'''Select Text'''\narticle = df_20k['text'].iloc[1]\n\n\n'''Text Cleaning Utility Function'''\n\nprocesses = [\n               gsp.strip_tags, \n               gsp.strip_punctuation,\n               gsp.strip_multiple_whitespaces,\n               gsp.strip_numeric,\n               gsp.remove_stopwords, \n               gsp.strip_short\n            ]\n\n# Utility Function\ndef proc_txt(txt):\n    text = txt.lower()\n    text = utils.to_unicode(text)\n    for p in processes:\n        text = p(text)\n    return text\n\n# Cleaning the article\narticle = proc_txt(article)","e62dd387":"# Instantiate Model\nmodel = TFAutoModelWithLMHead.from_pretrained(\"bert-large-cased\")\ntokenizer = AutoTokenizer.from_pretrained(\"bert-large-cased\")","87e7abff":"# Define Input\ninput = tokenizer.encode(article, return_tensors=\"tf\", max_length=256)\noutput = model.generate(input, max_length=150, min_length=40, length_penalty=2.0, num_beams=4,early_stopping=True)\nprint(\"Summarized Text: \\n\", tokenizer.decode(output[0], skip_special_tokens=True))","30728151":"# Garbage Collection\ngc.collect()","b72131d1":"For Visualizing the topics we will use **pyLDAvis** which is designed to help users interpret the topics in a topic model that has been fit to a corpus of text data. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization.\n\n**Saliency**: a measure of how much the term tells you about the topic.\n\n**Relevance**: a weighted average of the probability of the word given the topic and the word given the topic normalized by the probability of the topic.\nThe size of the bubble measures the importance of the topics, relative to the data.\n\nFirst, we got the most salient terms, means terms mostly tell us about what\u2019s going on relative to the topics. We can also look at individual topic.","f8124de3":"### Observations:\n\n* The most salient terms used by P.M  Mr. Modi are INDIA, COUNTRY, FRIENDS, PEOPLE, CORONA, VILLAGE.\n* The term CORONA was used in 3 out of 5 topics\n\n\n**Note**: When there are >=10 topics, we will see clustering of certain topics, this indicates the similarity between topics.","bd2a7b51":"### Observations:\n\n* The term CORONA was used in all the 5 topics.\n* Terms likes VILLAGE, FARMERS, PANCHAYAT also makes it into the top 30 salient terms.\n* We can observe that topic#1 & topic#3 are clustered which means there are similarities\n* Health & Well being related terms such as DOCTOR, FAMILY, YOGA, AYUSHMAN were used frequently\n* Epidemic related terms such as CORONA, QUARANTINE, LOCKDOWN, VIRUS were used frequently","2e37cdea":"### Modelling using SKLearn Library\n\nWe will now transform the textual data in a format that will serve as an input for training the LDA model. Firstly, we will convert the documents into a simple vector representation (Bag of Words BOW). Then, we shall convert a list of text into lists of vectors, all with length equal to the vocabulary.","242a364d":"# Summarization\n\nSummarization is a task of producing a concise and fluent summary while preserving key information and overall meaning. There are two types of summarization, abstractive and extractive summarization\n\n* Abstractive Summarization (Input document > understand context > semantics > create own summary)\n* Extractive Summarization (Input document > sentences similarity > weight sentences > select sentences with higher rank)\n\nFor the scope of this notebook, we're going to focus on the **Extractive Summarization** technique and the speech made by P.M Mr Modi on **15 Aug 2020**.","0f25baed":"# Topic Modelling\n\nTopic Modelling is a process of identifying the topic that is being discussed in a document. Latent Dirichlet Allocation (LDA) is a widely used topic modelling technique. We will apply LDA to convert set of speeches to a set of topics. Speech topic modelling is an unsupervised machine learning method that helps us discover hidden semantic structures in a speech, that allows us to learn topic representations of the speech.\n\n**Trivia:** An average person takes around 1hr to speak 20,000 words. \n\nBased on this little trivia, for **Topic Modelling**, we will be focussing on the **english speeches with >= 20,000 words made by P.M Mr Modi in 2020**.\n\n\n\n### Selecting Records & Text Cleaning","8a0ad113":"### Data\n\nIn this notebook, we'll be focussing on the english speeches made by P.M Mr Modi in 2020. ","099ae5d5":"### Using Pre-Trained Transformer Model","c24973eb":"# Setup","0629fda1":"# Analysis of speeches given by P.M Mr. Modi\n\nIn this notebook, we're going to analyse the speeches given my P.M Mr Modi. We will be performing Topic Modelling & Text Summarization. As always, I have tried to keep the notebook well commented & organized for easy reading. I hope you find this notebook helpful.\n","76069fdc":"### Modelling using Gensim\n\nFirstly, will create a dictionary from the data, then convert to bag-of-words corpus and save the dictionary and corpus for future use.","2a4b15af":"## I hope that was enlighting and helpful. Please do consider to UPVOTE :)"}}