{"cell_type":{"283ef09e":"code","f2834a4e":"code","359cb02d":"code","0e028fee":"code","40bfedae":"code","f6570f42":"code","4b337a7f":"code","7ab2d589":"code","1b5d378f":"code","36fcf2dc":"code","48b5de12":"code","2245fcfc":"code","0249ff86":"code","eb580fa1":"code","151065c3":"code","656fe14f":"code","9d3dccac":"code","f328ceb7":"code","825dd8fd":"code","1047593f":"code","f0096926":"code","3f24d61c":"code","ac48f1f2":"markdown","29d6473f":"markdown","a1c45ab4":"markdown","d9909efa":"markdown","c90dba14":"markdown","a8749fe4":"markdown","a34eb4ba":"markdown","6a4cb9e4":"markdown","fc628405":"markdown"},"source":{"283ef09e":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py > \/dev\/null\n!python pytorch-xla-env-setup.py --version 20200420 --apt-packages libomp5 libopenblas-dev > \/dev\/null\n!pip install transformers==2.5.1 > \/dev\/null\n!pip install pandarallel > \/dev\/null\n!pip install catalyst==20.4.2 > \/dev\/null","f2834a4e":"import numpy as np\nimport pandas as pd\n\nimport os\nos.environ['XLA_USE_BF16'] = \"1\"\n\nfrom glob import glob\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.autograd import Variable\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nimport sklearn\n\nimport time\nimport random\nfrom datetime import datetime\nfrom tqdm import tqdm\ntqdm.pandas()\n\nfrom transformers import BertModel, BertTokenizer\nfrom transformers import RobertaModel, RobertaTokenizer\nfrom transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\nfrom catalyst.data.sampler import DistributedSamplerWrapper, BalanceClassSampler\n\nimport gc\nimport re\n\n# !pip install nltk > \/dev\/null\nimport nltk\nnltk.download('punkt')\n\nfrom nltk import sent_tokenize\n\nfrom pandarallel import pandarallel\n\npandarallel.initialize(nb_workers=4, progress_bar=False)","359cb02d":"SEED = 42\n\nMAX_LENGTH = 64\nBACKBONE_PATH = 'roberta-base'\nROOT_PATH = f'..'\n# ROOT_PATH = f'\/content\/drive\/My Drive\/jigsaw2020-kaggle-public-baseline' # for colab\n\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)","0e028fee":"from nltk import sent_tokenize\nfrom random import shuffle\nimport random\nimport albumentations\nfrom albumentations.core.transforms_interface import DualTransform, BasicTransform\n\n\ndef get_sentences(text):\n    sentences = sent_tokenize(text)\n    return ' '.join(sentences)\n\ndef clean_text(text):\n    text = str(text)\n    text = re.sub(r'[0-9\"]', '', text)\n    text = re.sub(r'#[\\S]+\\b', '', text)\n    text = re.sub(r'@[\\S]+\\b', '', text)\n    text = re.sub(r'https?\\S+', '', text)\n    text = re.sub(r'\\s+', ' ', text)\n    text = get_sentences(text)\n    return text.strip()\n\n\nclass NLPTransform(BasicTransform):\n    \"\"\" Transform for nlp task.\"\"\"\n\n    @property\n    def targets(self):\n        return {\"data\": self.apply}\n    \n    def update_params(self, params, **kwargs):\n        if hasattr(self, \"interpolation\"):\n            params[\"interpolation\"] = self.interpolation\n        if hasattr(self, \"fill_value\"):\n            params[\"fill_value\"] = self.fill_value\n        return params\n\n    def get_sentences(self, text):\n        return sent_tokenize(text)\n\nclass ShuffleSentencesTransform(NLPTransform):\n    \"\"\" Do shuffle by sentence \"\"\"\n    def __init__(self, always_apply=False, p=0.5):\n        super(ShuffleSentencesTransform, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text = data\n        sentences = self.get_sentences(text)\n        random.shuffle(sentences)\n        return ' '.join(sentences)\n\nclass ExcludeNumbersTransform(NLPTransform):\n    \"\"\" exclude any numbers \"\"\"\n    def __init__(self, always_apply=False, p=0.5):\n        super(ExcludeNumbersTransform, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text = data\n        text = re.sub(r'[0-9]', '', text)\n        text = re.sub(r'\\s+', ' ', text)\n        return text\n\nclass ExcludeHashtagsTransform(NLPTransform):\n    \"\"\" Exclude any hashtags with # \"\"\"\n    def __init__(self, always_apply=False, p=0.5):\n        super(ExcludeHashtagsTransform, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text = data\n        text = re.sub(r'#[\\S]+\\b', '', text)\n        text = re.sub(r'\\s+', ' ', text)\n        return text\n\nclass ExcludeUsersMentionedTransform(NLPTransform):\n    \"\"\" Exclude @users \"\"\"\n    def __init__(self, always_apply=False, p=0.5):\n        super(ExcludeUsersMentionedTransform, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text = data\n        text = re.sub(r'@[\\S]+\\b', '', text)\n        text = re.sub(r'\\s+', ' ', text)\n        return text\n\nclass ExcludeUrlsTransform(NLPTransform):\n    \"\"\" Exclude urls \"\"\"\n    def __init__(self, always_apply=False, p=0.5):\n        super(ExcludeUrlsTransform, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text = data\n        text = re.sub(r'https?\\S+', '', text)\n        text = re.sub(r'\\s+', ' ', text)\n        return text","40bfedae":"def get_train_transforms():\n    return albumentations.Compose([\n        ExcludeUsersMentionedTransform(p=0.95),\n        ExcludeUrlsTransform(p=0.95),\n        ExcludeNumbersTransform(p=0.95),\n        ExcludeHashtagsTransform(p=0.95),\n    ], p=1.0)\n\n\ntrain_transforms = get_train_transforms()\ntokenizer = RobertaTokenizer.from_pretrained(BACKBONE_PATH)\nshuffle_transforms = ShuffleSentencesTransform(always_apply=True)","f6570f42":"def onehot(size, target):\n    vec = torch.zeros(size, dtype=torch.float32)\n    vec[target] = 1.\n    return vec\n\nclass DatasetRetriever(Dataset):\n\n    def __init__(self, labels_or_ids, texts, use_train_transforms=False, test=False):\n        self.test = test\n        self.labels_or_ids = labels_or_ids\n        self.texts = texts\n        self.use_train_transforms = use_train_transforms\n        \n    def get_tokens(self, text):\n        encoded = tokenizer.encode_plus(\n            text, \n            add_special_tokens=True, \n            max_length=MAX_LENGTH, \n            pad_to_max_length=True\n        )\n        return encoded['input_ids'], encoded['attention_mask']\n\n    def __len__(self):\n        return self.texts.shape[0]\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        if self.test is False:\n            label = self.labels_or_ids[idx]\n            target = onehot(2, label)\n\n        if self.use_train_transforms:\n            text = train_transforms(data=(text))['data']\n            tokens, attention_mask = self.get_tokens(str(text))\n            token_length = sum(attention_mask)\n            if token_length > 0.8*MAX_LENGTH:\n                text = shuffle_transforms(data=(text))['data']\n            else:\n                tokens, attention_mask = torch.tensor(tokens), torch.tensor(attention_mask)\n                return target, tokens, attention_mask\n\n        tokens, attention_mask = self.get_tokens(str(text))\n        tokens, attention_mask = torch.tensor(tokens), torch.tensor(attention_mask)\n\n        if self.test is False:\n            return target, tokens, attention_mask\n        return self.labels_or_ids[idx], tokens, attention_mask\n\n    def get_labels(self):\n        return list(self.labels_or_ids.astype(str))","4b337a7f":"from sklearn.model_selection import train_test_split\n\ndf = pd.read_csv(f'{ROOT_PATH}\/input\/nlp-getting-started\/train.csv')\nX = df['text'].values\ny = df['target'].values\n\n# split for train and valid\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n\ndf_train = pd.DataFrame(\n    {'target': y_train,\n     'text': X_train})\n\ndf_val = pd.DataFrame(\n    {'target': y_val,\n     'text': X_val})\n\ndel df\ngc.collect();","7ab2d589":"%%time\n\ntrain_dataset = DatasetRetriever(\n    labels_or_ids=df_train['target'].values, \n    texts=df_train['text'].values, \n    use_train_transforms=True,\n)\n\ndel df_train\ngc.collect();\n\nfor targets, tokens, attention_masks in train_dataset:\n    break\n    \nprint(targets)\nprint(tokens.shape)\nprint(attention_masks.shape)","1b5d378f":"np.unique(train_dataset.get_labels())","36fcf2dc":"validation_tune_dataset = DatasetRetriever(\n    labels_or_ids=df_val['target'].values, \n    texts=df_val['text'].values, \n    use_train_transforms=True,\n)\n\ndf_val['text'] = df_val.apply(lambda x: clean_text(x['text']), axis=1)\n\nvalidation_dataset = DatasetRetriever(\n    labels_or_ids=df_val['target'].values, \n    texts=df_val['text'].values, \n    use_train_transforms=False,\n)\n\ndel df_val\ngc.collect();\n\nfor targets, tokens, attention_masks in validation_dataset:\n    break\n\nprint(targets)\nprint(tokens.shape)\nprint(attention_masks.shape)","48b5de12":"df_test = pd.read_csv(f'{ROOT_PATH}\/input\/nlp-getting-started\/test.csv', index_col='id')\ndf_test['text'] = df_test.parallel_apply(lambda x: clean_text(x['text']), axis=1)\n\ntest_dataset = DatasetRetriever(\n    labels_or_ids=df_test.index.values, \n    texts=df_test['text'].values, \n    use_train_transforms=False,\n    test=True\n)\n\ndel df_test\ngc.collect();\n\nfor ids, tokens, attention_masks in test_dataset:\n    break\n\nprint(ids)\nprint(tokens.shape)\nprint(attention_masks.shape)","2245fcfc":"class RocAucMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.y_true = np.array([0,1])\n        self.y_pred = np.array([0.5,0.5])\n        self.score = 0\n\n    def update(self, y_true, y_pred):\n        y_true = y_true.cpu().numpy().argmax(axis=1)\n        y_pred = nn.functional.softmax(y_pred, dim=1).data.cpu().numpy()[:,1]\n        self.y_true = np.hstack((self.y_true, y_true))\n        self.y_pred = np.hstack((self.y_pred, y_pred))\n        self.score = sklearn.metrics.roc_auc_score(self.y_true, self.y_pred, labels=np.array([0, 1]))\n    \n    @property\n    def avg(self):\n        return self.score\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","0249ff86":"class LabelSmoothing(nn.Module):\n    def __init__(self, smoothing = 0.1):\n        super(LabelSmoothing, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n\n    def forward(self, x, target):\n        if self.training:\n            x = x.float()\n            target = target.float()\n            logprobs = torch.nn.functional.log_softmax(x, dim = -1)\n\n            nll_loss = -logprobs * target\n            nll_loss = nll_loss.sum(-1)\n    \n            smooth_loss = -logprobs.mean(dim=-1)\n\n            loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n\n            return loss.mean()\n        else:\n            return torch.nn.functional.cross_entropy(x, target)","eb580fa1":"import warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\nfrom catalyst.data.sampler import DistributedSamplerWrapper, BalanceClassSampler\n\nclass TPUFitter:\n    \n    def __init__(self, model, device, config):\n        if not os.path.exists('node_submissions'):\n            os.makedirs('node_submissions')\n\n        self.config = config\n        self.epoch = 0\n        self.log_path = 'log.txt'\n\n        self.model = model\n        self.device = device\n\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n        optimizer_grouped_parameters = [\n            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n        ]\n\n        self.optimizer = AdamW(optimizer_grouped_parameters, lr=config.lr*xm.xrt_world_size())\n        self.scheduler = config.SchedulerClass(self.optimizer, **config.scheduler_params)\n\n        self.criterion = config.criterion\n        xm.master_print(f'Fitter prepared. Device is {self.device}')\n\n    def fit(self, train_loader, validation_loader):\n        for e in range(self.config.n_epochs):\n            if self.config.verbose:\n                lr = self.optimizer.param_groups[0]['lr']\n                timestamp = datetime.utcnow().isoformat()\n                self.log(f'\\n{timestamp}\\nLR: {lr}')\n\n            t = time.time()\n            para_loader = pl.ParallelLoader(train_loader, [self.device])\n            losses, final_scores = self.train_one_epoch(para_loader.per_device_loader(self.device))\n            \n            self.log(f'[RESULT]: Train. Epoch: {self.epoch}, loss: {losses.avg:.5f}, final_score: {final_scores.avg:.5f}, time: {(time.time() - t):.5f}')\n\n            t = time.time()\n            para_loader = pl.ParallelLoader(validation_loader, [self.device])\n            losses, final_scores = self.validation(para_loader.per_device_loader(self.device))\n\n            self.log(f'[RESULT]: Validation. Epoch: {self.epoch}, loss: {losses.avg:.5f}, final_score: {final_scores.avg:.5f}, time: {(time.time() - t):.5f}')\n\n            if self.config.validation_scheduler:\n                self.scheduler.step(metrics=final_scores.avg)\n\n            self.epoch += 1\n    \n    def run_tuning_and_inference(self, test_loader, validation_tune_loader):\n        for e in range(2):\n            self.optimizer.param_groups[0]['lr'] = self.config.lr*xm.xrt_world_size()\n            para_loader = pl.ParallelLoader(validation_tune_loader, [self.device])\n            losses, final_scores = self.train_one_epoch(para_loader.per_device_loader(self.device))\n            para_loader = pl.ParallelLoader(test_loader, [self.device])\n            self.run_inference(para_loader.per_device_loader(self.device))\n\n    def validation(self, val_loader):\n        self.model.eval()\n        losses = AverageMeter()\n        final_scores = RocAucMeter()\n\n        t = time.time()\n        for step, (targets, inputs, attention_masks) in enumerate(val_loader):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    xm.master_print(\n                        f'Valid Step {step}, loss: ' + \\\n                        f'{losses.avg:.5f}, final_score: {final_scores.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}'\n                    )\n            with torch.no_grad():\n                inputs = inputs.to(self.device, dtype=torch.long) \n                attention_masks = attention_masks.to(self.device, dtype=torch.long) \n                targets = targets.to(self.device, dtype=torch.float) \n\n                outputs = self.model(inputs, attention_masks)\n                loss = self.criterion(outputs, targets)\n                \n                batch_size = inputs.size(0)\n\n                final_scores.update(targets, outputs)\n                losses.update(loss.detach().item(), batch_size)\n                \n        return losses, final_scores\n         \n    def train_one_epoch(self, train_loader):\n        self.model.train()\n\n        losses = AverageMeter()\n        final_scores = RocAucMeter()\n        t = time.time()\n        for step, (targets, inputs, attention_masks) in enumerate(train_loader):   \n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    self.log(\n                        f'Train Step {step}, loss: ' + \\\n                        f'{losses.avg:.5f}, final_score: {final_scores.avg:.5f}, ' + \\\n                        f'time: {(time.time() - t):.5f}'\n                    )\n\n            inputs = inputs.to(self.device, dtype=torch.long)\n            attention_masks = attention_masks.to(self.device, dtype=torch.long)\n            targets = targets.to(self.device, dtype=torch.float)\n\n            self.optimizer.zero_grad()\n\n            outputs = self.model(inputs, attention_masks)\n            loss = self.criterion(outputs, targets)\n\n            batch_size = inputs.size(0)\n            \n            final_scores.update(targets, outputs)\n            \n            losses.update(loss.detach().item(), batch_size)\n\n            loss.backward()\n            xm.optimizer_step(self.optimizer)\n\n            if self.config.step_scheduler:\n                self.scheduler.step()\n        \n        self.model.eval()\n        self.save('last-checkpoint.bin')\n        return losses, final_scores\n\n    def run_inference(self, test_loader):\n        self.model.eval()\n        result = {'id': [], 'target': []}\n        t = time.time()\n        for step, (ids, inputs, attention_masks) in enumerate(test_loader):\n            if self.config.verbose:\n                if step % self.config.verbose_step == 0:\n                    xm.master_print(f'Prediction Step {step}, time: {(time.time() - t):.5f}')\n\n            with torch.no_grad():\n                inputs = inputs.to(self.device, dtype=torch.long) \n                attention_masks = attention_masks.to(self.device, dtype=torch.long)\n                outputs = self.model(inputs, attention_masks)\n                toxics = nn.functional.softmax(outputs, dim=1).data.cpu().numpy()[:,1]\n\n            result['id'].extend(ids.cpu().numpy())\n            result['target'].extend(toxics)\n\n        result = pd.DataFrame(result)\n        node_count = len(glob('node_submissions\/*.csv'))\n        result.to_csv(f'node_submissions\/submission_{node_count}_{datetime.utcnow().microsecond}_{random.random()}.csv', index=False)\n\n    def save(self, path):        \n        xm.save(self.model.state_dict(), path)\n\n    def log(self, message):\n        if self.config.verbose:\n            xm.master_print(message)\n        with open(self.log_path, 'a+') as logger:\n            xm.master_print(f'{message}', logger)","151065c3":"class NNModel(nn.Module):\n\n    def __init__(self):\n        super(NNModel, self).__init__()\n        self.backbone = RobertaModel.from_pretrained(BACKBONE_PATH)\n        self.dropout = nn.Dropout(0.3)\n        self.linear = nn.Linear(\n            in_features=self.backbone.pooler.dense.out_features*2,\n            out_features=2,\n        )\n\n    def forward(self, input_ids, attention_masks):\n        bs, seq_length = input_ids.shape\n        seq_x, _ = self.backbone(input_ids=input_ids, attention_mask=attention_masks)\n        apool = torch.mean(seq_x, 1)\n        mpool, _ = torch.max(seq_x, 1)\n        x = torch.cat((apool, mpool), 1)\n        x = self.dropout(x)\n        return self.linear(x)","656fe14f":"net = NNModel()","9d3dccac":"class TrainGlobalConfig:\n    \"\"\" Global Config for this notebook \"\"\"\n    num_workers = 0  \n    batch_size = 16  \n    n_epochs = 3  \n    lr = 0.5 * 1e-5\n    fold_number = 0  \n\n    # -------------------\n    verbose = True \n    verbose_step = 5 \n    # -------------------\n\n    # --------------------\n    step_scheduler = False \n    validation_scheduler = True  \n    SchedulerClass = torch.optim.lr_scheduler.ReduceLROnPlateau\n    scheduler_params = dict(\n        mode='max',\n        factor=0.7,\n        patience=0,\n        verbose=False, \n        threshold=0.0001,\n        threshold_mode='abs',\n        cooldown=0, \n        min_lr=1e-8,\n        eps=1e-08\n    )\n    # --------------------\n\n    # -------------------\n    criterion = LabelSmoothing()\n    # -------------------","f328ceb7":"def _mp_fn(rank, flags):\n    device = xm.xla_device()\n    net.to(device)\n\n    train_sampler = DistributedSamplerWrapper(\n        sampler=BalanceClassSampler(labels=train_dataset.get_labels(), mode=\"downsampling\"),\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=True\n    )\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TrainGlobalConfig.batch_size,\n        sampler=train_sampler,\n        pin_memory=False,\n        drop_last=True,\n        num_workers=TrainGlobalConfig.num_workers,\n    )\n    validation_sampler = torch.utils.data.distributed.DistributedSampler(\n        validation_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=False\n    )\n    validation_loader = torch.utils.data.DataLoader(\n        validation_dataset,\n        batch_size=TrainGlobalConfig.batch_size,\n        sampler=validation_sampler,\n        pin_memory=False,\n        drop_last=False,\n        num_workers=TrainGlobalConfig.num_workers\n    )\n    validation_tune_sampler = torch.utils.data.distributed.DistributedSampler(\n        validation_tune_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=True\n    )\n    validation_tune_loader = torch.utils.data.DataLoader(\n        validation_tune_dataset,\n        batch_size=TrainGlobalConfig.batch_size,\n        sampler=validation_tune_sampler,\n        pin_memory=False,\n        drop_last=False,\n        num_workers=TrainGlobalConfig.num_workers\n    )\n    test_sampler = torch.utils.data.distributed.DistributedSampler(\n        test_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=False\n    )\n    test_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=TrainGlobalConfig.batch_size,\n        sampler=test_sampler,\n        pin_memory=False,\n        drop_last=False,\n        num_workers=TrainGlobalConfig.num_workers\n    )\n\n    if rank == 0:\n        time.sleep(1)\n    \n    fitter = TPUFitter(model=net, device=device, config=TrainGlobalConfig)\n    fitter.fit(train_loader, validation_loader)\n    fitter.run_tuning_and_inference(test_loader, validation_tune_loader)","825dd8fd":"FLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","1047593f":"submission = pd.concat([pd.read_csv(path) for path in glob('node_submissions\/*.csv')]).groupby('id').mean()\nsubmission['target'].hist(bins=100)","f0096926":"submission['target'] = submission['target'].apply(lambda x: round(x))","3f24d61c":"submission.to_csv('submission.csv')","ac48f1f2":"# Label Smoothing","29d6473f":"# Preprocessing\nUsed this for [NLP Albumenations](https:\/\/www.kaggle.com\/shonenkov\/nlp-albumentations). Please upvote if you find this useful.","a1c45ab4":"# All Important Imports","d9909efa":"# XLA Setup","c90dba14":"# TPU Fitter","a8749fe4":"# Training","a34eb4ba":"# Data Loaders","6a4cb9e4":"# Model","fc628405":"# Config"}}