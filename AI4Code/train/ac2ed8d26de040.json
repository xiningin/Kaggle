{"cell_type":{"97843c74":"code","06e24840":"code","b31b66ae":"code","d0b32f07":"code","8dc4dd17":"code","874fffea":"code","87c7c382":"code","ad332dd8":"code","f0c90ee0":"code","b97de611":"code","37917b55":"code","6acbf3e5":"code","85451d96":"code","5c548f42":"code","c74d7f44":"code","8c8e5bec":"code","f0503869":"code","7838ab64":"code","61f3e341":"code","7f6214f9":"code","0ca577cb":"code","9827a732":"code","f9da3d01":"markdown","d30f2d4e":"markdown","8cd68f2b":"markdown","c53a088f":"markdown","b3ce9b1f":"markdown"},"source":{"97843c74":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom keras import models \nfrom keras import layers\n\nfrom keras import regularizers\n\nfrom sklearn.metrics import accuracy_score\n\nimport os\nprint(os.listdir(\"..\/input\/\"))","06e24840":"#import train and test CSV files\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n#import real historical data.\nhistory=pd.read_csv(\"..\/input\/test-dataset\/TitanicHistory.csv\")","b31b66ae":"#this is real data on history\nhistory['Survived']=history['Survived'].map({\"No\":0,\"Yes\":1})\nhistory[['Name','Survived']].head()","d0b32f07":"#combine historical data with test\ntestWithHistory=pd.merge(test,history[['Name','Survived']],on='Name', how = 'left')\ntestWithHistory.head()","8dc4dd17":"train['Mark']='train'\ntestWithHistory['Mark']='test'\n#combine train and test for convenience, making them easy for feature engineer\ndataInput=pd.concat([train,testWithHistory])\ndataPrepare=dataInput.copy(deep=True)","874fffea":"#let's see the null situation.\nprint(pd.isnull(test).sum())\nprint(pd.isnull(train).sum())\nprint(pd.isnull(dataPrepare).sum())","87c7c382":"dataPrepare['Age'][dataPrepare['Age']<1]=1\n\ndataPrepare['Age']=dataPrepare['Age'].fillna(0)\ndataPrepare['Age']=dataPrepare['Age'].astype('float32')\n\ndataPrepare['Age']=dataPrepare['Age']\ndataPrepare.head()","ad332dd8":"dataPrepare['Sex']=dataPrepare['Sex'].map({\"male\":1,\"female\":2})\ndataPrepare['Sex'].value_counts()","f0c90ee0":"dataPrepare['Embarked']=dataPrepare['Embarked'].map({\"S\":1,\"C\":2,\"Q\":3})\ndataPrepare['Embarked']=dataPrepare['Embarked'].fillna(0)","b97de611":"#exact title from name\ndataPrepare['Title'] = dataPrepare.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ndataPrepare['Title']=dataPrepare['Title'].map({\"Mr\":1,\"Miss\":2,\"Mrs\":3,\"Master\":4})\ndataPrepare['Title']=dataPrepare['Title'].fillna(0)\ndataPrepare['Title']=dataPrepare['Title'].astype('int8')\ndataPrepare['Title'].value_counts()","37917b55":"dataPrepare.head()","6acbf3e5":"selectColumns=['PassengerId', 'Embarked', 'Title', 'Age', 'Fare', 'Mark','Parch',\n        'Pclass', 'Sex', 'SibSp', 'Survived']\ndataPrepare[selectColumns].head()\ndataPrepare['Fare']=dataPrepare['Fare'].fillna(0)\nprint(pd.isnull(dataPrepare[selectColumns]).sum())","85451d96":"dataPrepare[selectColumns].head()","5c548f42":"#set train and validation set.\n#as previous mentioned, we use historical data as Validation set.\n#let's what will give us.\ntrain_input=dataPrepare[dataPrepare['Mark']=='train']\nvalidation_input=dataPrepare[dataPrepare['Mark']=='test']\nvalidation_input=validation_input[validation_input['Survived'].notnull()]\nfeatureColumns=[ 'Age','Title','Embarked', 'Fare','Parch','Pclass', 'Sex', 'SibSp']\n\nx=train_input[featureColumns].values\ny=train_input['Survived'].values\n\nx_val=validation_input[featureColumns].values\ny_val=validation_input['Survived'].values\n","c74d7f44":"#keras deep net\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(10, activation='relu', input_shape=(len(featureColumns),))) \nmodel.add(layers.Dropout(0.1))\nmodel.add(layers.Dense(10, activation='relu')) \nmodel.add(layers.Dropout(0.1))\nmodel.add(layers.Dense(10, activation='relu')) \nmodel.add(layers.Dropout(0.1))\n\nmodel.add(layers.Dense(10, activation='relu')) \nmodel.add(layers.Dropout(0.1))\nmodel.add(layers.Dense(10, activation='relu')) \nmodel.add(layers.Dropout(0.1))\nmodel.add(layers.Dense(1, activation='sigmoid',))\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n","8c8e5bec":"history = model.fit(x, y, epochs=100,\nbatch_size=32, validation_data=(x_val, y_val),verbose=0)","f0503869":"history_dict = history.history\nimport matplotlib.pyplot as plt\nhistory_dict = history.history \nloss_values = history_dict['loss'] \nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, 'bo', label='Training loss') \nplt.plot(epochs, val_loss_values, 'b', label='Validation loss') \nplt.title('Training and validation loss') \nplt.xlabel('Epochs') \nplt.ylabel('Loss') \nplt.legend()\nplt.show()","7838ab64":"plt.clf()\nacc = history_dict['acc'] \nval_acc = history_dict['val_acc']\nplt.plot(epochs, acc, 'bo', label='Training acc') \nplt.plot(epochs, val_acc, 'b', label='Validation acc') \nplt.title('Training and validation accuracy') \nplt.xlabel('Epochs') \nplt.ylabel('Accuracy') \nplt.legend()\nplt.show()","61f3e341":"y_pred = model.predict(x_val)\nacc_randomforest = round(accuracy_score(np.where(y_pred<0.5,0,1), y_val) * 100, 2)\nprint(acc_randomforest)","7f6214f9":"# Random Forest as reference\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x, y)\ny_pred = randomforest.predict(x_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)","0ca577cb":"# Gradient Boosting Classifier as reference\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x, y)\ny_pred = gbk.predict(x_val)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk)","9827a732":"#submit upload and you will see that equal the accuracy of this notebook. \nids = test['PassengerId']\npredits=dataPrepare[dataPrepare['Mark']=='test']\npredits=predits[featureColumns].values\npredictions = model.predict(predits)\npredictions=np.where(predictions<0.5,0,1)\npredictions=pd.Series(predictions.reshape(418))\n\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput['Survived']=output['Survived'].astype('int8')\noutput.to_csv('submission.csv', index=False)","f9da3d01":"Now we move on model part.\nsome nominal feature should be convert use dummy() or something like that,  but may be deep net could understand if I give it more layers.\nso Let's try","d30f2d4e":"Let's change some char to number.","8cd68f2b":"Age feature will fill null with 0 because deep net could recognize 0 if we leave enough space.\nEnough space means we make move everyting close to 0 equal 1 , \nthen let deep net see the difference:\n0 as null \n1 still means age\n","c53a088f":"I am going to imput real historical data as Validation set, so we could get real accuracy without submit.\nThat maybe will give us some new insight, at least that save a lot of time.","b3ce9b1f":"We don't split validation set from train.\nWe use history data as validation set.\nmaybe this try will give us something different."}}