{"cell_type":{"ed965ef3":"code","fceebd8f":"code","0c666f8f":"code","ca0e4c21":"code","888f2283":"code","488262e1":"code","8ad30e93":"code","8fdd764b":"code","ea4b1cd9":"code","e1a985cf":"code","640a65da":"code","236e9bcd":"code","2acd06a7":"code","41889610":"code","29181e4f":"code","9d92c297":"code","2caab872":"markdown","16df7c64":"markdown","e3514ce9":"markdown","e70e7436":"markdown","fe205f5d":"markdown","0a2f5288":"markdown","2f8d4df6":"markdown","4e16d533":"markdown"},"source":{"ed965ef3":"reset -fs","fceebd8f":"import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import QuantileTransformer\nimport lightgbm as lgb\nimport optuna\nimport warnings\nfrom tqdm import tqdm\nimport time\nfrom sklearn.decomposition import TruncatedSVD, PCA\nwarnings.filterwarnings('ignore')","0c666f8f":"train = pd.read_csv(\"..\/input\/lish-moa\/train_features.csv\")\ntest = pd.read_csv(\"..\/input\/lish-moa\/test_features.csv\")\ntrain_targets_scored = pd.read_csv(\"..\/input\/lish-moa\/train_targets_scored.csv\")\ntrain_targets_nonscored = pd.read_csv(\"..\/input\/lish-moa\/train_targets_nonscored.csv\")\ntrain_drug = pd.read_csv('..\/input\/lish-moa\/train_drug.csv')\nsub = pd.read_csv(\"..\/input\/lish-moa\/sample_submission.csv\")","ca0e4c21":"train.cp_time = train.cp_time.astype(str)\ntest.cp_time = test.cp_time.astype(str)\ngs = train.columns[train.columns.str.contains('g-')]\ncs = train.columns[train.columns.str.contains('c-')]","888f2283":"def feature_engineering(train, test):\n    # scaling\n    scaler = preprocessing.StandardScaler()\n    train_n = scaler.fit_transform(train.iloc[:, 4:])\n    test_n = scaler.transform(test.iloc[:, 4:])\n    # pca\n    pca = PCA(n_components=117, random_state=42)\n    train_pca = pca.fit_transform(train_n)\n    test_pca = pca.transform(test_n)\n    for i in range(117):\n        train[f'pca{i+1}'] = train_pca[:, i]\n        test[f'pca{i+1}'] = test_pca[:, i]\n        \n    # rank gauss\n    for col in train.iloc[:, 4:].columns:\n        transformer = QuantileTransformer(n_quantiles=500,random_state=42, output_distribution=\"normal\")\n        vec_len = len(train[col].values)\n        vec_len_test = len(test[col].values)\n        raw_vec = train[col].values.reshape(vec_len, 1)\n        transformer.fit(raw_vec)\n        train[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n        test[col] = transformer.transform(test[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]\n        \n    # categorical variable concat\n    train['combined_cat0'] = train.cp_type + train.cp_time \n    test['combined_cat0'] = test.cp_type + test.cp_time\n    train['combined_cat1'] = train.cp_type + train.cp_dose \n    test['combined_cat1'] = test.cp_type + test.cp_dose\n    train['combined_cat2'] = train.cp_time + train.cp_dose \n    test['combined_cat2'] = test.cp_time + test.cp_dose\n    train['combined_cat3'] = train.cp_type + train.cp_dose + train.cp_time\n    test['combined_cat3'] = test.cp_type + test.cp_dose + test.cp_time\n    \n    # other features\n    train['mean_g'] = train[gs].mean(axis=1) # mean of all genes\n    test['mean_g'] = test[gs].mean(axis=1)\n    train['mean_c'] = train[cs].mean(axis=1)\n    test['mean_c'] = train[cs].mean(axis=1)\n    train['median_g'] = train[gs].median(axis=1)\n    test['median_g'] = train[gs].median(axis=1)\n    train['median_c'] = train[cs].median(axis=1)\n    test['median_c'] = train[cs].median(axis=1)\n    train['max_g'] = train[gs].max(axis=1)\n    test['max_g'] = train[gs].max(axis=1)\n    train['max_c'] = train[cs].max(axis=1)\n    test['max_c'] = train[cs].max(axis=1)\n    train['min_g'] = train[gs].min(axis=1)\n    test['min_g'] = train[gs].min(axis=1)\n    train['min_c'] = train[cs].min(axis=1)\n    test['min_c'] = train[cs].min(axis=1)\n    train['q25_g'] = train[gs].quantile(0.25, axis=1)\n    test['q25_g'] = train[gs].quantile(0.25, axis=1)\n    train['q25_c'] = train[cs].quantile(0.25, axis=1)\n    test['q25_c'] = train[cs].quantile(0.25, axis=1)\n    train['q75_g'] = train[gs].quantile(0.75, axis=1)\n    test['q75_g'] = train[gs].quantile(0.75, axis=1)\n    train['q75_c'] = train[cs].quantile(0.75, axis=1)\n    test['q75_c'] = train[cs].quantile(0.75, axis=1)\n    return train, test\n\ndef label_encoding(train: pd.DataFrame, test: pd.DataFrame, encode_cols):\n    n_train = len(train)\n    train = pd.concat([train, test], sort=False).reset_index(drop=True)\n    for f in encode_cols:\n        try:\n            lbl = preprocessing.LabelEncoder()\n            train[f] = lbl.fit_transform(list(train[f].values))\n        except:\n            print(f)\n    test = train[n_train:].reset_index(drop=True)\n    train = train[:n_train]\n    # drop id\n    train.drop(['sig_id'], axis=1, inplace=True)\n    test.drop(['sig_id'], axis=1, inplace=True)\n    return train, test","488262e1":"train, test = feature_engineering(train, test)\ntrain, test = label_encoding(train, test, ['cp_type', 'cp_dose', 'combined_cat0', \n                                           'combined_cat1','combined_cat2', 'combined_cat3',\n                                           'cp_time'])","8ad30e93":"def run_lgbm(target_col):\n    \n    X_train = train\n    y_train = train_targets_scored[target_col]\n    X_test = test\n    \n    combined = train.copy()\n    combined[target_col] = train.combined_cat3.astype(str) + y_train.astype(str) + train_drug.drug_id.astype(str).values\n    \n    y_preds = []\n    models = []\n    oof_train = np.zeros((len(X_train),))\n    \n    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    \n    for fold_id, (train_index, valid_index) in enumerate(kf.split(X=combined, y=combined[target_col].values)):\n        print(f'-----------running fold {fold_id} for {target_col}-----------')\n        print('*'*50)\n        X_tr = X_train.loc[train_index, :]\n        X_val = X_train.loc[valid_index, :]\n        y_tr = y_train[train_index]\n        y_val = y_train[valid_index]\n        lgb_train = lgb.Dataset(X_tr,\n                                y_tr,)\n\n        lgb_eval = lgb.Dataset(X_val,\n                               y_val,\n                               reference=lgb_train)\n\n        model = lgb.train(params,\n                          lgb_train,\n                          valid_sets=[lgb_train, lgb_eval],\n                          verbose_eval=-1,\n                          num_boost_round=5000, # 5000\n                          early_stopping_rounds=100\n                         )\n\n\n        oof_train[valid_index] = model.predict(X_val,\n                                               num_iteration=model.best_iteration)\n        y_pred = model.predict(X_test,\n                               num_iteration=model.best_iteration)\n\n        y_preds.append(y_pred)\n        models.append(model)\n\n    return oof_train, sum(y_preds) \/ len(y_preds)","8fdd764b":"## things to change here\nparams = {\n    'num_leaves': 19,\n    'max_depth': 2,\n    \"lambda_l1\": 1,\n    \"lambda_l2\": 1,\n    'objective': 'binary',\n    'metric': \"binary_logloss\",\n    'learning_rate': 0.01,\n    'verbosity': -1,\n    \"feature_fraction\": 0.6\n}\n\noof = train_targets_scored.copy()","ea4b1cd9":"# pick top features that have more postive samples\ntop_k = 75","e1a985cf":"easy_tar = train_targets_scored.iloc[:, 1:].sum(axis=0).sort_values(ascending=False)[:top_k].index.values\nhard_tar = train_targets_scored.iloc[:, 1:].sum(axis=0).sort_values(ascending=False)[top_k:].index.values\nassert len(easy_tar) + len(hard_tar) == 206","640a65da":"print(f'Training {top_k} easy targets.....')\nstart_time = time.time()\nfor target_col in tqdm(easy_tar):\n    if target_col != \"sig_id\":\n        _oof, _preds = run_lgbm(target_col)\n        oof[target_col] = _oof\n        sub[target_col] = _preds\nend_time = time.time()\nprint('*' * 100)\nprint(f'Total time for training Easy Targets: {(end_time- start_time)\/60:.1f} minutes. Kaggle CPU sucks.')","236e9bcd":"#update train and test for stage 2, append the oofs as features\ntrain = pd.concat([train, oof[easy_tar]], axis=1)\ntest = pd.concat([test, sub[easy_tar]], axis=1)","2acd06a7":"print(f'Training {206-top_k} hard targets.....')\nstart_time = time.time()\nfor target_col in tqdm(hard_tar):\n    if target_col != \"sig_id\":\n        _oof, _preds = run_lgbm(target_col)\n        oof[target_col] = _oof\n        sub[target_col] = _preds\nend_time = time.time()\nprint('*' * 100)\nprint(f'Total time for training hard targets: {(end_time- start_time)\/60:.1f} minutes')","41889610":"scores = []\nscore_for_each_tar = {}\nfor target_col in train_targets_scored.columns:\n    if target_col != \"sig_id\":\n        score_for_each_tar[target_col] = log_loss(train_targets_scored[target_col], oof[target_col])\n        scores.append(log_loss(train_targets_scored[target_col], oof[target_col]))\nprint('Score without PP:')\nprint(np.mean(scores))","29181e4f":"oof.iloc[train.query('cp_type==0').index] = 0\nscores_pp = []\nfor target_col in train_targets_scored.columns:\n    if target_col != \"sig_id\":\n        scores_pp.append(log_loss(train_targets_scored[target_col], oof[target_col]))\nprint('Score when setting vehicle equal to 0:')\nprint(np.mean(scores_pp))","9d92c297":"print('Writing to submission')\nsub.to_csv(f'submission.csv', index=False)","2caab872":"I took the name **self-stacking** from [@Yirun Zhang](https:\/\/www.kaggle.com\/gogo827jz) as the idea is similar but not exactly the same.\n\nI also took some of the code from https:\/\/www.kaggle.com\/sishihara\/moa-lgbm-benchmark.\n\n## Main Idea:\n* Train targets that contain lots of postive samples(=1) first\n* Save the oof and add the oof predictions as features for training targets with less postive samples\n\n## Results:\n* With a slight twist of this code you can get a CV around `0.015` and LB around `0.019` with LightGBM","16df7c64":"## Submission","e3514ce9":"## Stage 1: Training 'Easy' Target, more positive samples","e70e7436":"## LGB Setup","fe205f5d":"## OOF Check","0a2f5288":"## Stage 2: Training 'hard' targets, less postive samples","2f8d4df6":"## Data Prep","4e16d533":"## Imports"}}