{"cell_type":{"1e806868":"code","5b15ad09":"code","a5ede66f":"code","0e962b5b":"code","7c1742be":"code","c68aaea0":"code","38136062":"code","b5871f28":"code","9e593ebe":"code","67a71805":"code","87e3538d":"code","3d1e8b7d":"code","3363542e":"code","8e9c5591":"code","604ff559":"code","6bcc079e":"code","4e3ebc38":"markdown","f9b855f9":"markdown","751ed9d7":"markdown","cb99db32":"markdown","7f54a8a0":"markdown","6501ebda":"markdown","cbb42d13":"markdown","fd64b8d2":"markdown","22306fbe":"markdown","2625f9ef":"markdown","835e620e":"markdown","2adb9c69":"markdown"},"source":{"1e806868":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5b15ad09":"#setting up coding environment with necessary imports...\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mlxtend.frequent_patterns import apriori, association_rules \n\n#reading the dataset in the dataframe\n\ndf = pd.read_csv('..\/input\/the-bread-basket\/bread basket.csv')\n\n#checking the top few data  \ndf.head(10)","a5ede66f":"# getting the statistical summary of the data\ndf.describe()","0e962b5b":"#understanding the data and its data types\ndf.info()","7c1742be":"#get no. of missing data points per column if any\n\nmissing_values_count = df.isnull().sum()\n\n#checking the missing values in all columns if any\nmissing_values_count[0:4]","c68aaea0":"df.groupby(['Transaction'])\ndf\n#df.head()","38136062":"df.Item.unique()","b5871f28":"#removing trailing and preceding whitespaces in item column   \ndf['Item']=df['Item'].str.strip()","9e593ebe":"grouped_df = df.groupby(['Transaction','Item'])['Item'].count().reset_index(name='Count')\n\nbasket_df = grouped_df.pivot_table(index='Transaction', columns='Item', values='Count', aggfunc='sum').fillna(0)","67a71805":"# Defining the hot encoding function to make the data suitable for the concerned libraries \ndef hot_encode(x): \n    if(x<= 0): \n        return 0\n    if(x>= 1): \n        return 1","87e3538d":"# Encoding the datasets \nbasket_encoded = basket_df.applymap(hot_encode) \nbasket_df = basket_encoded \n\nbasket_df","3d1e8b7d":"# Building the model with min support = 0.01 (1%)\nfrq_items = apriori(basket_df, min_support = 0.01, use_colnames = True)\nfrq_items","3363542e":"# Collecting the inferred rules in a dataframe \nrules = association_rules(frq_items, metric =\"confidence\", min_threshold = 0.25) \nrules = rules.sort_values(['confidence', 'lift'], ascending =[False, False]) \nrules.reset_index()","8e9c5591":"# SUPPORT Vs CONFIDENCE\n\nplt.scatter(rules['support'], rules['confidence'], alpha=0.5)\nplt.xlabel('support')\nplt.ylabel('confidence')\nplt.title('Support vs Confidence')\nplt.show()","604ff559":"# SUPPORT Vs LIFT\n\n\nplt.scatter(rules['support'], rules['lift'], alpha=0.5)\nplt.xlabel('support')\nplt.ylabel('lift')\nplt.title('Support vs Lift')\nplt.show()","6bcc079e":"# LIFT Vs CONFIDENCE\n\n\nfit = np.polyfit(rules['lift'], rules['confidence'], 1)\nfit_fn = np.poly1d(fit)\nplt.plot(rules['lift'], rules['confidence'], 'yo', rules['lift'], \n fit_fn(rules['lift']))","4e3ebc38":"In above table, each column is a unique item. If item present in transaction then value 1 is shown else 0. This transformed data is now useful to apply mining.","f9b855f9":"Above results shows the frequent Itemsets fulfilling necessary conditions of mining algorithm.\nNow the block below shows the Association Rule Mining with necessary parameters set in the model. Output is collected in the variable *rules*","751ed9d7":"# **Step 2 : Understanding the Data**\n\nAs we have fetched data, now its time to understand the data.\n* We will check the data summary using **df.describe()**\n* Also we will check the datatype of each column by **df.info()**","cb99db32":"Hello!!\n\nHere We will perform some data mining useful for market basket analysis. The methods we will use are **Apriori** and **Association Rule Mining**. More over we are using the-bread-basket dataset provided. We will perform the data mining task in step-by-step traditional method. Let's do it.\n","7f54a8a0":"# **Step 3 : Checking the Data for Missing Values**\n\nHere we are checking the data for any missing values in any columns. **missing_value_count** list shows the same. The result shows no missing values in any columns.\n","6501ebda":"So, Here this conclude the apriori and association rule mining task. To explore more you can try with various values of algorithm parameters and check the difference in the results.\n\nShare your valuable feedbacks.\n\nThanks in advance.","cbb42d13":"As some items may have more than one occurances in a transaction, we are transforming data by applying the hot encoding. After applying this data will be suitable for concerned libraries","fd64b8d2":"# **Step 4 : Cleaning the Data**\n\nNow cleaning the data in columns. First we check the distinct items and cleaning any preceding and trailing whitespaces if any.","22306fbe":"# **Step 1 : Reading the Data**\n\nFirstly we would fetch the data from the bread basket.csv file and read it in the dataframe df using the read_csv() method.\n","2625f9ef":"# **Step 4 : Data Selection & Transformation**\n\nAfter cleaning the data, we will perform the data selection. Here we will prepare the data as per the need of Frequent Itemset and Association mining process.\n\nHere data is firstly grouped by transaction to collect various item occurances in each transaction.\n\nAfter that we are creating a pivot table with transaction as index and items as columns to map the items with transcations. During this we are placing 0 for items which are not in perticular transcation.","835e620e":"# **Step 5 : Data Mining**\n\nHere we are performing data mining by applying the apriori and association_rules algorithm imported from **mlxtend.frequent_patterns** library. \n\n> We have set some parameters of the algorithm as follows\n\nFor Apriori : **Min support = 0.01**\n\nFor Association Rules : **Min. confidence = 0.25**\n\n","2adb9c69":"# **Step 6 : Model Evaluation Plots**\n\nAs we have performed Association Rule Mining for Bread Basket Analysis, we would like to check the relations between various algorithm parameters. Following are the plots showing the same."}}