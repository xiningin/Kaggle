{"cell_type":{"322c4d9f":"code","ca62fd2d":"code","03401bcb":"code","f6968ba0":"code","85898640":"code","2b6fa5d4":"code","0408e413":"code","bfd857b3":"code","6ba9f13a":"code","3532e395":"code","70a11b67":"code","48eae854":"code","ba242495":"code","15cc463a":"code","a45d6744":"code","4870a066":"code","976ea257":"code","f232f8a8":"code","c6cd2f1c":"code","0821fdaa":"markdown","10deb45c":"markdown"},"source":{"322c4d9f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ca62fd2d":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport os\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom collections import defaultdict\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","03401bcb":"df = pd.read_csv('..\/input\/fsu-stats-comp-2021\/train.csv')\ntest =  pd.read_csv('..\/input\/fsu-stats-comp-2021\/test.csv')","f6968ba0":"import datetime\nCLdateint = np.zeros(len(df.claim_date),dtype=np.int32)\nfor i, date in enumerate(df.claim_date):\n    mmm, ddd,  yyy = map(int, date.split('\/'))\n    yyy =  2000 + yyy\n    temp1 = datetime.datetime(yyy,mmm,ddd)\n    temp2 = datetime.datetime(2015,1,1)\n    CLdateint[i] = (temp1 - temp2).days\ndf['day'] = CLdateint","85898640":"CLdateint = np.zeros(len(test.claim_date),dtype=np.int32)\nfor i, date in enumerate(test.claim_date):\n    mmm, ddd,  yyy = map(int, date.split('\/'))\n    yyy =  2000 + yyy\n    temp1 = datetime.datetime(yyy,mmm,ddd)\n    temp2 = datetime.datetime(2015,1,1)\n    CLdateint[i] = (temp1 - temp2).days\ntest['day'] = CLdateint","2b6fa5d4":"gender_code = preprocessing.LabelEncoder()\ngender_code.fit(df.gender)\ndf.gender = gender_code.transform(df.gender)\ntest.gender = gender_code.transform(test.gender)","0408e413":"living_status_code = preprocessing.LabelEncoder()\nliving_status_code.fit(df.living_status)\ndf.living_status = living_status_code.transform(df.living_status)\ntest.living_status = living_status_code.transform(test.living_status)","bfd857b3":"accident_site_code = preprocessing.LabelEncoder()\naccident_site_code.fit(df.accident_site)\ndf.accident_site = accident_site_code.transform(df.accident_site)\ntest.accident_site = accident_site_code.transform(test.accident_site)","6ba9f13a":"df['zip'] = df['zip_code'] \/\/ 1000\ntest['zip'] = test['zip_code'] \/\/ 1000","3532e395":"df['marital_status'].fillna(0,inplace=True)\ndf['witness_present_ind'].fillna(0,inplace=True)\ndf['claim_est_payout'].fillna(np.mean(df['claim_est_payout']),inplace=True)\ntest['marital_status'].fillna(0,inplace=True)\ntest['witness_present_ind'].fillna(0,inplace=True)\ntest['claim_est_payout'].fillna(np.mean(df['claim_est_payout']),inplace=True)","70a11b67":"feature = [\n    'age_of_driver', \n    'gender',\n    'marital_status',\n    'past_num_of_claims',\n    'high_education_ind',\n    'witness_present_ind',\n    'address_change_ind',\n    'age_of_vehicle',\n    'day',\n    'zip',\n    'living_status',\n    'safty_rating',\n    'accident_site',\n    'liab_prct',\n    'claim_est_payout',\n]","48eae854":"categorical_columns = [\n    'gender',\n    'marital_status',\n    'high_education_ind',\n    'witness_present_ind',\n    'address_change_ind',\n    'zip',\n    'living_status',\n    'accident_site',\n                      ]","ba242495":"data_types_dict_train = {\n    'age_of_driver':'int16',\n    'gender':'int8',\n    'marital_status':'int8',\n    'past_num_of_claims':'int8', \n    'high_education_ind':'int8',\n    'witness_present_ind':'int8',\n    'address_change_ind':'int8',  \n    'age_of_vehicle':'float64',\n    'day':'int16',  \n    'zip':'int8', \n    'living_status':'int8',  \n    'safty_rating':'int8',  \n    'accident_site':'int8',  \n    'liab_prct':'int8',  \n    'claim_est_payout':'float64',\n    'fraud':'int8'\n}","15cc463a":"data_types_dict_test = {\n    'Id':'int16',\n    'age_of_driver':'int16',\n    'gender':'int8',\n    'marital_status':'int8',\n    'past_num_of_claims':'int8', \n    'high_education_ind':'int8',\n    'witness_present_ind':'int8',\n    'address_change_ind':'int8',  \n    'age_of_vehicle':'float64',\n    'day':'int16',  \n    'zip':'int8', \n    'living_status':'int8',  \n    'safty_rating':'int8',  \n    'accident_site':'int8',  \n    'liab_prct':'int8',  \n    'claim_est_payout':'float64',\n}","a45d6744":"df = df[feature+['fraud']].astype(data_types_dict_train)\ntest = test[feature+['Id']].astype(data_types_dict_test)","4870a066":"from sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom random import randrange","976ea257":"aa = []\nn_model = 500\npredictions = np.zeros([n_model ,len(test)])\npredictions_train = np.zeros([n_model ,len(df)])\nfor i in range(n_model):\n    r = randrange(10000)\n    params = {\n    'num_leaves': 3,\n    'max_bin':100,\n    'min_child_weight': 100,\n    'feature_fraction': 0.3,\n    'bagging_fraction': 0.6,\n    'min_data_in_leaf': 500,\n    'objective': 'binary',\n    'max_depth': -1,\n    'learning_rate': 0.1,\n    \"boosting_type\": \"gbdt\",\n    \"metric\": 'auc',\n    \"feature_fraction_seed\":r+2,\n    \"verbosity\": -1,\n    'reg_alpha': 0.5,\n    'reg_lambda':0,\n    'random_state': r+3,\n    'class_weight': None,\n    }\n    X_train, X_test, y_train, y_test = train_test_split(df[feature], df.fraud, test_size=0.2,random_state = r)\n    tr_data = lgb.Dataset(X_train[feature], label=y_train)\n    va_data = lgb.Dataset(X_test[feature], label=y_test)\n    evals_result = {}\n    model = lgb.train(\n            params, \n            tr_data,\n            num_boost_round=10000,\n            valid_sets=[tr_data, va_data],\n            early_stopping_rounds=50,\n            feature_name= feature,\n            evals_result = None,\n            categorical_feature=categorical_columns,\n            verbose_eval=5)\n    aa.append(model.best_score['valid_1']['auc'])\n    predictions[i,:] = model.predict(test[feature])","f232f8a8":"np.mean(aa)","c6cd2f1c":"predictions_test = np.mean(predictions,0)\nmy_submission = pd.DataFrame({\"Id\":test['Id'],\"fraud\":predictions_test})\nmy_submission.to_csv('.\/my_prediction.csv',index=False)","0821fdaa":"* Use forward selection method to select useful featrues. First fit a model with all variable using LightGBM, and get the importance of each features. Then first fit a null model and add one more feature into the null model according to the importance at each step.(I set a threshold 0.05, if the feature can increase the auc more than 0.05, then add this feature into our model)\n* Include 15 features in our final model.\n* Construct 'day' feature, which represent the 'time' feature, the value is how many days from 2015.01.0.1.\n* 'zip' feature is constructed by deviding zipcode by 1000 and use it as a categorical feature.\n* Use LightGBM with very small leaves number (3) and large min_child_weight(100), min_data_in_leaf(500).\n* Generate 500 LightGBM models (use different random number) and calculate the mean prediction for each observation in test dataset.\n* The final prediction auc for test dataset should be around 0.752 (private score).","10deb45c":"**The biggest difficulty for this dataset is overfitting. It contains too many noise and useless features and I use some techiques to avoid overfitting. The machine learning methods I used is LightGBM and Blending techniques. The code of my solution is very easy to understand.**"}}