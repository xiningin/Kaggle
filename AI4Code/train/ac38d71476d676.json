{"cell_type":{"02f64c32":"code","7dd54014":"code","94e0a1f2":"code","6e93306e":"code","a8c866b9":"code","b334213a":"code","b1fb7d0f":"code","50361ebb":"code","8cc2af1c":"code","aabfe37b":"code","f1e9b021":"code","c8af8e65":"code","0e4c742e":"code","405382f3":"code","70e142f9":"code","9383bd6e":"code","3d581894":"code","96454426":"code","fe612e63":"code","bbf7caad":"markdown","eb89c4f9":"markdown","996d519f":"markdown","8de21c92":"markdown","583450fe":"markdown","182cbe32":"markdown","efe3ec3d":"markdown","e37752b1":"markdown"},"source":{"02f64c32":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7dd54014":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import cross_val_score","94e0a1f2":"test = pd.read_csv('\/kaggle\/input\/udacity-mlcharity-competition\/test_census.csv', index_col = 0)\ntrain = pd.read_csv('\/kaggle\/input\/udacity-mlcharity-competition\/census.csv')\n\ntest['test'] = 1\ntrain['test'] = 0\n\ntrain['income'] = train.income.replace({'<=50K':0, '>50K':1})\n\n\nall_data = pd.concat([test,train])\n\nall_data.head()","6e93306e":"all_data.describe()","a8c866b9":"all_data.info()","b334213a":"# select categorical variables\ncat_vars = [x for x in all_data.columns if all_data[x].dtype == 'object']\nprint('Categorical Variables')\nprint(cat_vars)","b1fb7d0f":"# select numeric variables\nnum_vars = [x for x in all_data.columns if x not in cat_vars]\nnum_vars.remove('test')\nnum_vars.remove('income')\nnum_vars","50361ebb":"all_data[cat_vars].nunique()","8cc2af1c":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfor i in cat_vars:\n    sns.barplot(all_data[i].value_counts().index,all_data[i].value_counts()).set_title(i)\n    plt.show()","aabfe37b":"for i in num_vars:\n    plt.hist(all_data[i])\n    plt.title(i)\n    plt.show()","f1e9b021":"# Missing Numeric Data\nfor x in num_vars:\n    if all_data[x].isna().sum() > 0:\n        print(x,all_data[x].isna().sum())\n\n# All of these can be imputed with the median\nimp_num = SimpleImputer(missing_values=np.nan, strategy='median')\nall_data[num_vars] = imp_num.fit_transform(all_data[num_vars])\n\nprint()\n\n# Check if any are left\nfor x in num_vars:\n    if all_data[x].isna().sum() > 0:\n        print(x,all_data[x].isna().sum())","c8af8e65":"# Categorical Data\nfor x in cat_vars:\n    if all_data[x].isna().sum() > 0:\n        print(x,all_data[x].isna().sum())\n        \n# Impute mode for categorical variables\nimp_cat = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\nall_data[cat_vars] = imp_cat.fit_transform(all_data[cat_vars])\n\n# One hot encoding\n# 1. drop native-country\ncat_vars.remove('native-country')\none_hot = pd.get_dummies(all_data, columns = cat_vars)\n\none_hot","0e4c742e":"all_data.info()","405382f3":"X = [x for x in one_hot.columns if x not in cat_vars]\nX","70e142f9":"final_df = one_hot[X]\nfinal_df.drop('native-country', inplace = True, axis = 1)\ntrain_set = final_df[final_df.test == 0]\ntrain_set.drop('test', inplace=True, axis=1)\ntest_set = final_df[final_df.test == 1]\ntest_set.drop('test', inplace=True, axis=1)\n\ncols = [x for x in train_set.columns]\ncols.remove('income')\nX = cols\n\nX = train_set[X]\ny = train_set.income\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()\n","9383bd6e":"# Specifying x_train\nX_train = X\n\n# Specifying x_test\ncols = [x for x in test_set.columns]\ncols.remove('income')\nx_test = test_set[cols]\n\n# Specifying y_train\ny_train = train_set.income\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nmodel = LinearDiscriminantAnalysis()","3d581894":"# Fitting the model \nmodel.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = model.predict(x_test)","96454426":"output = pd.DataFrame({'id': test_set.index,\n                       'income': preds})\noutput.to_csv('submission_donations_discAn.csv', index=False)","fe612e63":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()\n\n# Fitting the model \nmodel.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = model.predict(x_test)\n\noutput = pd.DataFrame({'id': test_set.index,\n                       'income': preds})\noutput.to_csv('submission_donations_rf.csv', index=False)","bbf7caad":"# Feature Selection","eb89c4f9":"# Cleaning the data","996d519f":"## Random forest model","8de21c92":"# Model","583450fe":"0.75098\/1","182cbe32":"0.95519\/1","efe3ec3d":"## Trying a linear Discriminant Analysis Model","e37752b1":"perhaps in the future I will narrow down my features"}}