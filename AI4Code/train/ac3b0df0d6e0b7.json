{"cell_type":{"236efa8c":"code","9cc98425":"code","9d25427f":"code","79a5d039":"code","a835c5bb":"code","7a85b022":"code","b340a3e2":"code","aba2af40":"code","b311be23":"code","a303ce72":"code","99054ea1":"code","bfbc1997":"code","fb511f7d":"code","5cd8cef2":"code","f01ab73e":"code","520b65a9":"code","1aae8a00":"code","a0527d37":"code","850ee9cf":"code","bba93284":"code","3c45c034":"code","4184947e":"code","742e887f":"code","702e6077":"code","c698d375":"code","b725adc5":"code","1abd92f4":"code","fed45cc5":"code","43a7c167":"code","7d2579ca":"code","7be76f0f":"code","3b6044b0":"code","2d1a8d02":"code","24e1b082":"code","dc060319":"code","a644b7c3":"code","f2bdac18":"code","23255997":"code","35872bd9":"code","8c38c409":"code","c8c10e56":"code","e50af21d":"code","979715ff":"code","a4693cb2":"code","f30d3242":"code","76573314":"code","6a2e3a45":"code","ba66e6f3":"code","c8455b55":"markdown","5d8c7aee":"markdown","1a6c4585":"markdown","27f30c74":"markdown"},"source":{"236efa8c":"\n#Importing the essential libraries\n#Beautiful Soup is a Python library for pulling data out of HTML and XML files\n#The Natural Language Toolkit\n\nimport requests\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer \nfrom bs4 import BeautifulSoup\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport random\nfrom wordcloud import WordCloud\nfrom html.parser import HTMLParser\n\nimport bs4 as bs\nimport urllib.request\nimport re","9cc98425":"#we are using request package to make a GET request for the website, which means we're getting data from it.\n#It is a \"Times Of India\" article on the economic impact of COVID-19\n\nr=requests.get('https:\/\/timesofindia.indiatimes.com\/blogs\/transforming-india\/episode-11-economic-impact-of-coronavirus\/')","9d25427f":"\n#Setting the correct text encoding of the HTML page\nr.encoding = 'utf-8'","79a5d039":"#Extracting the HTML from the request object\nhtml = r.text","a835c5bb":"# Printing the first 500 characters in html\nprint(html[:500])","7a85b022":"\n# Creating a BeautifulSoup object from the HTML\nsoup = BeautifulSoup(html)\n\n# Getting the text out of the soup\ntext = soup.get_text()","b340a3e2":"#total length\nlen(text)\n","aba2af40":"#We are taking this range, as this contains the actual news article.\ntext=text[48700:70000]","b311be23":"text[10000:12000]","a303ce72":"#The text of the novel contains a lot of unwanten stuff, we need to remove them\n#We will start by tokenizing the text, that is, remove everything that isn't a word (whitespace, punctuation, etc.) \n#Then then split the text into a list of words","99054ea1":"#CLEANING THE PUNCTUATION","bfbc1997":"len(text)","fb511f7d":"import string\nstring.punctuation","5cd8cef2":"text_nopunct=''\n\ntext_nopunct= \"\".join([char for char in text if char not in string.punctuation])","f01ab73e":"len(text_nopunct)","520b65a9":"#We see a large part has been cleaned","1aae8a00":"#Creating the tokenizer\ntokenizer = nltk.tokenize.RegexpTokenizer('\\w+')","a0527d37":"#Tokenizing the text\ntokens = tokenizer.tokenize(text_nopunct)","850ee9cf":"len(tokens)","bba93284":"print(tokens[0:20])","3c45c034":"#now we shall make everything lowercase for uniformity\n#to hold the new lower case words\n\nwords = []\n\n# Looping through the tokens and make them lower case\nfor word in tokens:\n    words.append(word.lower())","4184947e":"print(words[0:500])","742e887f":"#Stop words are generally the most common words in a language.\n#English stop words from nltk.\n\nstopwords = nltk.corpus.stopwords.words('english')","702e6077":"words_new = []\n\n#Now we need to remove the stop words from the words variable\n#Appending to words_new all words that are in words but not in sw\n\nfor word in words:\n    if word not in stopwords:\n        words_new.append(word)","c698d375":"len(words_new)","b725adc5":"from nltk.stem import WordNetLemmatizer \n  \nwn = WordNetLemmatizer() ","1abd92f4":"lem_words=[]\n\nfor word in words_new:\n    word=wn.lemmatize(word)\n    lem_words.append(word)\n    \n","fed45cc5":"len(lem_words)","43a7c167":"len(words_new)","7d2579ca":"same=0\ndiff=0\n\nfor i in range(0,1832):\n    if(lem_words[i]==words_new[i]):\n        same=same+1\n    elif(lem_words[i]!=words_new[i]):\n        diff=diff+1","7be76f0f":"print('Number of words Lemmatized=', diff)\nprint('Number of words not Lemmatized=', same)","3b6044b0":"#This shows that lemmatization helps to process the data efficiently.\n#It is used in- \n# 1. Comprehensive retrieval systems like search engines.\n# 2. Compact indexing","2d1a8d02":"#The frequency distribution of the words\nfreq_dist = nltk.FreqDist(lem_words)","24e1b082":"#Frequency Distribution Plot\nplt.subplots(figsize=(20,12))\nfreq_dist.plot(50)","dc060319":"#converting into string\n\nres=' '.join([i for i in lem_words if not i.isdigit()])","a644b7c3":"plt.subplots(figsize=(16,10))\nwordcloud = WordCloud(\n                          background_color='black',\n                          max_words=100,\n                          width=1400,\n                          height=1200\n                         ).generate(res)\n\n\nplt.imshow(wordcloud)\nplt.title('Economic News (100 words)')\nplt.axis('off')\nplt.show()","f2bdac18":"plt.subplots(figsize=(16,10))\nwordcloud = WordCloud(\n                          background_color='black',\n                          max_words=200,\n                          width=1400,\n                          height=1200\n                         ).generate(res)\n\n\nplt.imshow(wordcloud)\nplt.title('Economic News (200 words)')\nplt.axis('off')\nplt.show()","23255997":"# Removing Square Brackets and Extra Spaces\nclean_text = re.sub(r'\\[[0-9]*\\]', ' ', text)\nclean_text = re.sub(r'\\s+', ' ', clean_text)","35872bd9":"clean_text[0:500]","8c38c409":"#We need to tokenize the article into sentences\n#Sentence tokenization\n\nsentence_list = nltk.sent_tokenize(clean_text)","c8c10e56":"sentence_list","e50af21d":"#Weighted Frequency of Occurrence\n\nstopwords = nltk.corpus.stopwords.words('english')\n\nword_frequencies = {}\nfor word in nltk.word_tokenize(clean_text):\n    if word not in stopwords:\n        if word not in word_frequencies.keys():\n            word_frequencies[word] = 1\n        else:\n            word_frequencies[word] += 1","979715ff":"type(word_frequencies)","a4693cb2":"maximum_frequncy = max(word_frequencies.values())\n\nfor word in word_frequencies.keys():\n    word_frequencies[word] = (word_frequencies[word]\/maximum_frequncy)","f30d3242":"sentence_scores = {}\nfor sent in sentence_list:\n    for word in nltk.word_tokenize(sent.lower()):\n        if word in word_frequencies.keys():\n            if len(sent.split(' ')) < 30:\n                if sent not in sentence_scores.keys():\n                    sentence_scores[sent] = word_frequencies[word]\n                else:\n                    sentence_scores[sent] += word_frequencies[word]","76573314":"sentence_scores","6a2e3a45":"import heapq\nsummary_sentences = heapq.nlargest(20, sentence_scores, key=sentence_scores.get)\n\nsummary = ' '.join(summary_sentences)\nprint(summary)","ba66e6f3":"#Thank You","c8455b55":"Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word.\nLemmatization is preferred over Stemming because lemmatization does morphological analysis of the words.","5d8c7aee":"# Lemmatization","1a6c4585":"Covid-19 has spread all the over the world. Economic situations of countries are not well. In an attempt to understand the current situation, we are analysing a Times of India economic article.\n\nArticle link- https:\/\/timesofindia.indiatimes.com\/blogs\/transforming-india\/episode-11-economic-impact-of-coronavirus\/\n","27f30c74":"# Text Summarzation"}}