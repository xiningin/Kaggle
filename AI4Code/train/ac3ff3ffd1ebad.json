{"cell_type":{"f9fe4445":"code","927caffc":"code","74bfb0a6":"code","b03273aa":"code","c406fc0b":"code","2d394ea1":"code","522efcb7":"code","f15f088a":"code","284cb405":"code","a291e7b4":"code","83b88d3f":"code","f6205186":"code","00605570":"code","3b9f02c2":"code","9c8a1bee":"code","97260e31":"code","61ad1109":"code","8a1b92a1":"code","b2b95121":"code","b5d647eb":"code","82a607e7":"code","10baf565":"code","ee0981aa":"code","bef538e7":"code","a3587cc1":"code","ba7921ea":"code","5523b2d5":"code","01baf2d3":"code","cf95e299":"code","d87a2d35":"code","d7a43f66":"code","3d8406dd":"code","c3e5a40f":"code","1d888400":"code","5ea56bc1":"code","f41fe324":"code","61de3914":"code","ab4f1a85":"code","b088cec3":"code","b9786866":"code","860d5575":"code","068104d4":"code","b716bed7":"code","5402e342":"code","a3716069":"code","faf38083":"code","f218d331":"code","1f0241ba":"code","0b32e9e1":"code","e71013b0":"code","5b8002ee":"code","d33e9c0f":"code","b62c88c6":"code","7f8735ea":"code","d16995e6":"code","e3fe1b5d":"code","9a27bc87":"code","5398cb30":"code","6edf554a":"code","d4bfdda9":"code","b2655f10":"code","36e817ff":"code","f83d813e":"code","297eb5a1":"code","a74e70d1":"code","b1c9b28c":"code","4edcbb2e":"code","57b9e55a":"code","dc122de3":"code","de7a2b68":"code","48f94d05":"code","b91e5a78":"code","27f4ca49":"code","b7215a81":"code","743cd4a9":"code","c53b5cfb":"code","4c8e2bdc":"code","e95e4836":"code","7a28f260":"code","afbb9d99":"code","4c08852e":"code","802f276e":"code","4436d07e":"code","3533851f":"code","9d78b3ed":"code","869a929f":"code","9b82471c":"code","df31a7cc":"code","05e6de94":"code","26fb378f":"code","ea362829":"code","4dc288f1":"code","ccb1a2c4":"code","4f3417bd":"code","e13d0419":"code","9aa579e4":"code","12d8bf11":"code","9a8c86b8":"code","270148ab":"code","eb5eebd4":"code","90dfc57e":"code","a269efbe":"code","50a61131":"code","20b4dc31":"code","aef598bc":"code","45a9e644":"code","3785d11f":"code","d9e45588":"code","8bdb16af":"code","1bb56d0e":"code","9e1e58b8":"code","ba887733":"code","3cb8cf3f":"code","27d05a09":"code","7c149e89":"code","11d215dc":"code","48c73f3c":"code","459c20c2":"code","1063fbec":"code","bdf866b8":"code","7914a70b":"code","e740a9ad":"code","4978ccc8":"code","23fa9d64":"code","4bffc778":"code","d1a1e3bb":"markdown","118b1b90":"markdown","d7be35ec":"markdown","ab1699dd":"markdown","f66c9f99":"markdown","2755d2c0":"markdown","1e3d42df":"markdown","0d4774f3":"markdown","495fe309":"markdown","a00cd036":"markdown","e1f16dc6":"markdown","fc0af6fd":"markdown","e3468cbd":"markdown","93cdc54d":"markdown"},"source":{"f9fe4445":"#from google.colab import drive\n#drive.mount('\/content\/drive')","927caffc":"#pwd","74bfb0a6":"#cd \/content\/drive\/My Drive\/Module 5 Capstone\/","b03273aa":"#ls","c406fc0b":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport itertools\nimport tensorflow as tf\nimport tensorflow_hub as hub","2d394ea1":"from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\nfrom tensorflow.keras.constraints import max_norm\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras import datasets, layers, models, Sequential\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (Input, Dense, Flatten, \n                                     Conv1D, MaxPooling1D, Dropout, \n                                     GlobalMaxPooling1D, Activation, \n                                     Embedding, Bidirectional, LSTM, \n                                     BatchNormalization)","522efcb7":"from collections import Counter\nfrom sklearn.compose import ColumnTransformer,make_column_transformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import (classification_report,accuracy_score,\n                             precision_score,recall_score,f1_score, \n                             confusion_matrix, precision_recall_curve)\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import (LabelEncoder,\n                                   OneHotEncoder,\n                                   LabelBinarizer)","f15f088a":"#%tensorflow_version 2.x\n#import tensorflow as tf\n#device_name = tf.test.gpu_device_name()\n#if device_name != '\/device:GPU:0':\n#  raise SystemError('GPU device not found')\n#print('Found GPU at: {}'.format(device_name))","284cb405":"data = pd.read_csv('pdb_data_no_dups.csv')\nseq = pd.read_csv('pdb_data_seq.csv')","a291e7b4":"data.head(1)","83b88d3f":"seq.head(1)","f6205186":"# select only protein for macromolecule type\ndata = data[data.macromoleculeType == 'Protein']","00605570":"# delete columns macromoleculeType, residueCount, structureId\ndata = data.drop(['macromoleculeType',\n                  'pdbxDetails',\n                  'structureId'], axis=1)","3b9f02c2":"#data['experimentalTechnique'].value_counts()","9c8a1bee":"counts = data.experimentalTechnique.value_counts()\nhello = np.asarray(counts[(counts > 96)].index)","97260e31":"data = data[data.experimentalTechnique.isin(hello)]\n#data.head()","61ad1109":"#data['crystallizationMethod'].value_counts()[:20]","8a1b92a1":"counts = data.crystallizationMethod.value_counts()\nhello = np.asarray(counts[(counts > 100)].index)","b2b95121":"data = data[data.crystallizationMethod.isin(hello)]\n#data.head()","b5d647eb":"data[\"crystallizationMethod\"].replace({\"VAPOR DIFFUSION, HANGING DROP\" : \"VAPOR DIFFUSION\", \n                                       \"VAPOR DIFFUSION, SITTING DROP\" : \"VAPOR DIFFUSION\", \n                                       \"VAPOR DIFFUSION\" : \"VAPOR DIFFUSION\", \n                                       \"EVAPORATION\" : \"EVAPORATION\", \n                                       \"MICROBATCH\" : \"MICROBATCH\", \n                                       \"hanging drop\" : \"VAPOR DIFFUSION\", \n                                       \"LIPIDIC CUBIC PHASE\" : \"LIPIDIC CUBIC PHASE\", \n                                       \"SMALL TUBES\" : \"Miscellaneous\", \n                                       \"MICRODIALYSIS\" : \"MICRODIALYSIS\", \n                                       \"BATCH MODE\" : \"MICROBATCH\", \n                                       \"LIQUID DIFFUSION\" : \"Miscellaneous\", \n                                       \"VAPOR DIFFUSION,SITTING DROP,NANODROP\" : \"VAPOR DIFFUSION\", \n                                       \"batch\" : \"MICROBATCH\", \n                                       \"microbatch under oil\" : \"MICROBATCH\", \n                                       \"VAPOR DIFFUSION, SITTING DROP, NANODROP\" : \"VAPOR DIFFUSION\"}, \n                                      inplace=True)","82a607e7":"data['crystallizationMethod'].value_counts()","10baf565":"#data['classification'].value_counts()[:20]","ee0981aa":"#data['classification'].value_counts()[:20]\ncounts = data.classification.value_counts()\nhello = np.asarray(counts[(counts > 829)].index)","bef538e7":"data = data[data.classification.isin(hello)]","a3587cc1":"missing_data = []\n\nfor col in data.columns:\n    if data[col].isnull().sum() > 0:\n        missing_data.append(col)\n    \nmissing_data","ba7921ea":"# replace null values in numeric vars with mean: \n# resolution, crystallizationMethod, crystallizationTempK,\n# densityMatthews, densityPercentSol, phValue, publicationYear\n\ndef impute_mean(df, col):\n    mean_val=df[col].mean()\n    df[col].fillna(mean_val,inplace=True)","5523b2d5":"impute_mean(data, 'resolution')\nimpute_mean(data, 'crystallizationTempK')\nimpute_mean(data, 'densityMatthews')\nimpute_mean(data, 'densityPercentSol')\nimpute_mean(data, 'phValue')\nimpute_mean(data, 'publicationYear')","01baf2d3":"data.describe().T","cf95e299":"# count plots of crystallizationMethod vs. classification\nplt.style.use('ggplot')\nplt.figure(figsize=(10,8))\nplt.xlabel('crystallization method', fontsize=12)\nplt.ylabel('count', fontsize=12)\nplt.title(\"Countplot of Crystallization Method Used on Each Protein Class\",fontsize=16, fontweight='bold')\nplt.xticks(rotation=90)\ng = sns.countplot(x=data['crystallizationMethod'], data=data, hue='classification')\ng.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=12)","d87a2d35":"# crystallization method and experimental technique do not matter\n# only feed numerical values to model\ndata = data.drop(['crystallizationMethod',\n                  'experimentalTechnique'], \n                  axis=1)","d7a43f66":"data.info()","3d8406dd":"# boxplots\nplt.style.use('ggplot')\nax = plt.subplots(figsize=(12,8))\nsns.boxplot(y=data['residueCount'], x=data['classification'], linewidth=1)\nplt.title('Residue Count by Protein Class', fontsize=14, fontweight='bold')\nplt.xlabel('Protein Class', fontsize=12, fontweight='bold')\nplt.ylabel('Residue Count', fontsize=12, fontweight='bold')\nplt.xticks(rotation=90)","c3e5a40f":"# boxplots\nplt.style.use('ggplot')\nax = plt.subplots(figsize=(12,8))\nsns.boxplot(y=data['resolution'], x=data['classification'], linewidth=1)\n#plt.ylim(0.5,4)\nplt.title('Resolution by Protein Class', fontsize=14, fontweight='bold')\nplt.xlabel('Protein Class', fontsize=12, fontweight='bold')\nplt.ylabel('Resolution', fontsize=12, fontweight='bold')\nplt.xticks(rotation=90)","1d888400":"# structureMolecularWeight\n# boxplots\nplt.style.use('ggplot')\nax = plt.subplots(figsize=(12,8))\nsns.boxplot(y=data['structureMolecularWeight'], x=data['classification'], linewidth=1)\n#plt.ylim(0,300000)\nplt.title('Structure Molecular Weight by Protein Class', fontsize=14, fontweight='bold')\nplt.xlabel('Protein Class', fontsize=12, fontweight='bold')\nplt.ylabel('Structure Molecular Weight', fontsize=12, fontweight='bold')\nplt.xticks(rotation=90)","5ea56bc1":"#crystallizationTempK\nplt.style.use('ggplot')\nax = plt.subplots(figsize=(12,8))\nsns.boxplot(y=data['crystallizationTempK'], x=data['classification'], linewidth=1)\n#plt.ylim(280,300)\nplt.title('Crystallization Temperature by Protein Class', fontsize=14, fontweight='bold')\nplt.xlabel('Protein Class', fontsize=12, fontweight='bold')\nplt.ylabel('Temperature (K)', fontsize=12, fontweight='bold')\nplt.xticks(rotation=90)","f41fe324":"#densityMatthews\nplt.style.use('ggplot')\nax = plt.subplots(figsize=(12,8))\nsns.boxplot(y=data['densityMatthews'], x=data['classification'], linewidth=1)\n#plt.ylim(1.5,4)\nplt.title('densityMatthews by Protein Class', fontsize=14, fontweight='bold')\nplt.xlabel('Protein Class', fontsize=12, fontweight='bold')\nplt.ylabel('densityMatthews', fontsize=12, fontweight='bold')\nplt.xticks(rotation=90)","61de3914":"#densityPercentSol\nplt.style.use('ggplot')\nax = plt.subplots(figsize=(12,8))\nsns.boxplot(y=data['densityPercentSol'], x=data['classification'], linewidth=1)\n#plt.ylim(23, 70)\nplt.title('densityPercentSol by Protein Class', fontsize=14, fontweight='bold')\nplt.xlabel('Protein Class', fontsize=12, fontweight='bold')\nplt.ylabel('densityPercentSol', fontsize=12, fontweight='bold')\nplt.xticks(rotation=90)","ab4f1a85":"#phValue\nplt.style.use('ggplot')\nax = plt.subplots(figsize=(12,8))\nsns.boxplot(y=data['phValue'], x=data['classification'], linewidth=1)\nplt.ylim(0,14)\nplt.title('phValue by Protein Class', fontsize=14, fontweight='bold')\nplt.xlabel('Protein Class', fontsize=12, fontweight='bold')\nplt.ylabel('phValue', fontsize=12, fontweight='bold')\nplt.xticks(rotation=90)","b088cec3":"#publicationYear\nplt.style.use('ggplot')\nax = plt.subplots(figsize=(12,8))\nsns.boxplot(y=data['publicationYear'], x=data['classification'], linewidth=1)\n#plt.ylim(1990, 2020)\nplt.title('publicationYear by Protein Class', fontsize=14, fontweight='bold')\nplt.xlabel('Protein Class', fontsize=12, fontweight='bold')\nplt.ylabel('publicationYear', fontsize=12, fontweight='bold')\nplt.xticks(rotation=90)","b9786866":"sns.pairplot(data[['resolution', \n                   'crystallizationTempK', \n                   'residueCount', \n                   'classification']],\n             hue='classification', \n             height=4, \n             diag_kind='hist')","860d5575":"sns.pairplot(data[['structureMolecularWeight', \n                   'densityMatthews', \n                   'densityPercentSol', \n                   'classification']],\n             hue='classification', \n             height=4, \n             diag_kind='hist')","068104d4":"data = pd.read_csv('pdb_data_no_dups.csv')\nseq = pd.read_csv('pdb_data_seq.csv')","b716bed7":"df_intermediate = seq.merge(data, on='structureId', how='left')\ndf_intermediate.head()","5402e342":"df_intermediate.drop_duplicates(subset=['structureId'], keep='first', inplace=True)\ndf_intermediate.head()","a3716069":"# select only protein for macromolecule type\ndf_intermediate = df_intermediate[df_intermediate.macromoleculeType_x == 'Protein']","faf38083":"df_intermediate.info()","f218d331":"# delete columns macromoleculeType, residueCount, structureId\ndf_intermediate = df_intermediate.drop(['macromoleculeType_x',\n                                        'macromoleculeType_y',\n                                        'chainId',\n                                        'residueCount_x',\n                                        'residueCount_y',\n                                        'resolution',\n                                        'structureMolecularWeight',\n                                        'crystallizationMethod',\n                                        'crystallizationTempK',\n                                        'densityMatthews',\n                                        'densityPercentSol',\n                                        'phValue',\n                                        'publicationYear',\n                                        'pdbxDetails',\n                                        'structureId',\n                                        'experimentalTechnique'], axis=1)","1f0241ba":"df_intermediate.info()","0b32e9e1":"def impute_mode(df, col):\n    mode_category=df[col].mode()[0]\n    df[col].fillna(mode_category,inplace=True)","e71013b0":"impute_mode(df_intermediate, 'sequence')","5b8002ee":"impute_mode(df_intermediate, 'classification')","d33e9c0f":"data = df_intermediate","b62c88c6":"data.head()","7f8735ea":"data['seq_char_count']= data['sequence'].apply(lambda x: len(x))","d16995e6":"def plot_seq_count(df):\n    sns.distplot(df['seq_char_count'].values, color='b')\n    plt.xlabel('Sequence Length')\n    plt.ylabel('Frequency')\n    plt.title('Sequence Character Count')\n    plt.grid(True)","e3fe1b5d":"plt.style.use('ggplot')\nplot_seq_count(data)\nplt.xlim(-100,1000)\nplt.show()","9a27bc87":"def get_code_freq(df):\n    \n    df = df.apply(lambda x: \" \".join(x))\n    \n    codes = []\n    \n    for i in df: # concatination of all codes\n        codes.extend(i)\n\n    codes_dict= Counter(codes)\n    codes_dict.pop(' ') # removing white space\n    \n    df = pd.DataFrame({'Amino Acid': list(codes_dict.keys()), 'Frequency': list(codes_dict.values())})\n    return df.sort_values('Frequency', ascending=False).reset_index()[['Amino Acid', 'Frequency']]","5398cb30":"code_freq = get_code_freq(data['sequence'])\ncode_freq","6edf554a":"def plot_code_freq(df):\n    plt.title('Amino Acid Frequency')\n    sns.barplot(x='Amino Acid', y='Frequency', data=df)","d4bfdda9":"plot_code_freq(code_freq)\nplt.show()","b2655f10":"sequences = data.sequence.values\nlengths = [len(s) for s in sequences]","36e817ff":"# mean length of a sequence - will use this as a max input length for models\nnp.mean(lengths)","f83d813e":"data['classification'].value_counts()[:20]","297eb5a1":"# only keep top 20 classifications\ncounts = data.classification.value_counts()\nhello = np.asarray(counts[(counts > 1273)].index)","a74e70d1":"data = data[data.classification.isin(hello)]","b1c9b28c":"# count plot of classification\nplt.style.use('ggplot')\nplt.figure(figsize=(10,8))\nplt.xlabel('classification', fontsize=12)\nplt.ylabel('count', fontsize=12)\nplt.title(\"Countplot of Each Protein Class (Top 20)\",fontsize=16, fontweight='bold')\nplt.xticks(rotation=90)\ng = sns.countplot(x=data['classification'], data=data, hue='classification')\ng.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize=12)","4edcbb2e":"X = data['sequence']\ny = data['classification']","57b9e55a":"# encode classification labels\nlb = LabelBinarizer()\ny_encoded = lb.fit_transform(y)","dc122de3":"# encode sequences using tokenizer\n# create and fit tokenizer\ntokenizer = Tokenizer(char_level=True)\ntokenizer.fit_on_texts(X)\n# represent input data as word rank number sequences\nX = tokenizer.texts_to_sequences(X)\nX = sequence.pad_sequences(X, maxlen=285)","de7a2b68":"# number of distinct characters stored in variable max_id\n# this will be part of the input shape (reps each AA)\nmax_id = len(tokenizer.word_index)\nmax_id","48f94d05":"# total number of characters \ndataset_size = tokenizer.document_count\ndataset_size","b91e5a78":"# split training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2)","27f4ca49":"# confirm shape\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","b7215a81":"def plot_model_results(model):\n    \n    accuracy = model.history.history['accuracy']\n    loss = model.history.history['loss']\n    \n    val_accuracy = model.history.history['val_accuracy']\n    val_loss = model.history.history['val_loss']\n    \n    epochs = range(1, len(accuracy) + 1)\n\n    plt.plot(epochs, accuracy, label='Training Accuracy')\n    plt.plot(epochs, val_accuracy, label='Validation Accuracy')\n    plt.title('Model Accuracy')\n    \n    plt.ylabel('% accuracy')\n    plt.xlabel('Epoch')\n    #plt.ylim(0.1, 1.0)\n    plt.legend()\n    plt.figure()\n\n    plt.plot(epochs, loss, label='Training Loss')\n    plt.plot(epochs, val_loss, label='Validation Loss')\n    plt.title('Model Loss')\n    \n    plt.ylabel('% loss')\n    plt.xlabel('Epoch')\n    #plt.ylim(0.1, 1.0)\n    plt.legend()\n    plt.show()","743cd4a9":"# create the model\nmodel_0 = Sequential()\n\n# embedding layer takes 3 args: 26 (25 AAs + 1), 20 (output vector shape = classes), 285 (max length of sequence)\nmodel_0.add(Embedding(26, 20, input_length=285))\n\n# This layer creates a convolution kernel that is convolved with the \n# layer input over a single spatial (or temporal) dimension to \n# produce a tensor of outputs.\nmodel_0.add(Conv1D(filters=128, kernel_size=12, padding='same', activation='relu'))\nmodel_0.add(MaxPooling1D(pool_size=2))\n\nmodel_0.add(Conv1D(filters=64, kernel_size=6, padding='same', activation='relu'))\nmodel_0.add(MaxPooling1D(pool_size=2))\n\nmodel_0.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\nmodel_0.add(MaxPooling1D(pool_size=2))\n\nmodel_0.add(Flatten())\nmodel_0.add(Dense(1120, activation='relu'))\nmodel_0.add(Dense(20, activation='softmax'))\n\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', \n                                                  mode= 'max', \n                                                  restore_best_weights=True, \n                                                  patience=2)\n\nopt = tf.keras.optimizers.Adam(learning_rate= 0.001)\n\nmodel_0.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n\nmodel_0.summary()","c53b5cfb":"model_0.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, callbacks=early_stopping)","4c8e2bdc":"plot_model_results(model_0)","e95e4836":"train_pred = model_0.predict(X_train)\ntest_pred = model_0.predict(X_test)\nprint(\"train accuracy = \" + str(accuracy_score(np.argmax(y_train, axis=1), np.argmax(train_pred, axis=1))))\nprint(\"test accuracy = \" + str(accuracy_score(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1))))","7a28f260":"# Compute confusion matrix\n#cm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1))\n\n# Plot normalized confusion matrix\n#cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n#np.set_printoptions(precision=2)\n#plt.figure(figsize=(7,7))\n#plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n#plt.title('Confusion matrix')\n#plt.colorbar()\n#tick_marks = np.arange(len(lb.classes_))\n#plt.xticks(tick_marks, lb.classes_, rotation=90)\n#plt.yticks(tick_marks, lb.classes_)\n#plt.ylabel('True label')\n#plt.xlabel('Predicted label')\n#plt.show()","afbb9d99":"#print(classification_report(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1), target_names=lb.classes_))","4c08852e":"model_1 = Sequential()\nmodel_1.add(Embedding(26, 20, input_length=285))\n\nmodel_1.add(LSTM(25, return_sequences=True))\n\nmodel_1.add(Flatten())\nmodel_1.add(Dense(7125, activation='sigmoid'))\nmodel_1.add(Dense(20, activation='softmax'))\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', \n                                                  mode='max', \n                                                  restore_best_weights=True, \n                                                  patience=2)\n\nopt = tf.keras.optimizers.Adam(learning_rate= 0.001)\n\nmodel_1.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n\nmodel_1.summary()","802f276e":"model_1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, callbacks=early_stopping)","4436d07e":"plot_model_results(model_1)","3533851f":"train_pred = model_1.predict(X_train)\ntest_pred = model_1.predict(X_test)\nprint(\"train accuracy = \" + str(accuracy_score(np.argmax(y_train, axis=1), np.argmax(train_pred, axis=1))))\nprint(\"test accuracy = \" + str(accuracy_score(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1))))","9d78b3ed":"# Compute confusion matrix\ncm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1))\n\n# Plot normalized confusion matrix\ncm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\nnp.set_printoptions(precision=2)\nplt.figure(figsize=(7,7))\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion matrix')\nplt.colorbar()\ntick_marks = np.arange(len(lb.classes_))\nplt.xticks(tick_marks, lb.classes_, rotation=90)\nplt.yticks(tick_marks, lb.classes_)\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","869a929f":"print(classification_report(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1), target_names=lb.classes_))","9b82471c":"model_2 = Sequential()\nmodel_2.add(Embedding(26, 20, input_length=285))\n\nmodel_2.add(Bidirectional(LSTM(10, return_sequences=True)))\n\nmodel_2.add(Flatten())\nmodel_2.add(Dense(5700, activation='sigmoid'))\nmodel_2.add(Dense(20, activation='softmax'))\n\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', \n                                                  mode='max', \n                                                  restore_best_weights=True, \n                                                  patience=2)\n\nopt = tf.keras.optimizers.Adam(learning_rate= 0.001)\n\nmodel_2.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n\nmodel_2.summary()","df31a7cc":"model_2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, callbacks=early_stopping)","05e6de94":"plot_model_results(model_2)","26fb378f":"train_pred = model_2.predict(X_train)\ntest_pred = model_2.predict(X_test)\nprint(\"train accuracy = \" + str(accuracy_score(np.argmax(y_train, axis=1), np.argmax(train_pred, axis=1))))\nprint(\"test accuracy = \" + str(accuracy_score(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1))))","ea362829":"# Compute confusion matrix\ncm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1))\n\n# Plot normalized confusion matrix\ncm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\nnp.set_printoptions(precision=2)\nplt.figure(figsize=(7,7))\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion matrix')\nplt.colorbar()\ntick_marks = np.arange(len(lb.classes_))\nplt.xticks(tick_marks, lb.classes_, rotation=90)\nplt.yticks(tick_marks, lb.classes_)\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","4dc288f1":"print(classification_report(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1), target_names=lb.classes_))","ccb1a2c4":"model_3 = Sequential()\nmodel_3.add(Embedding(26, 20, input_length=285))\n\nmodel_3.add(LSTM(25,\n                 kernel_regularizer=l2(0.01),\n                 recurrent_regularizer=l2(0.01),\n                 bias_regularizer=l2(0.01),\n                 return_sequences=True))\n\nmodel_3.add(Flatten())\nmodel_3.add(Dense(7125, activation='sigmoid'))\nmodel_3.add(Dense(20, activation='softmax'))\nmodel_3.add(Dropout(0.5))\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', \n                                                  mode='max', \n                                                  restore_best_weights=True, \n                                                  patience=2)\n\nopt = tf.keras.optimizers.Adam(learning_rate= 0.001)\n\nmodel_3.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n\nmodel_3.summary()","4f3417bd":"model_3.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=15, callbacks=early_stopping)","e13d0419":"plot_model_results(model_3)","9aa579e4":"train_pred = model_3.predict(X_train)\ntest_pred = model_3.predict(X_test)\nprint(\"train accuracy = \" + str(accuracy_score(np.argmax(y_train, axis=1), np.argmax(train_pred, axis=1))))\nprint(\"test accuracy = \" + str(accuracy_score(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1))))","12d8bf11":"# Compute confusion matrix\ncm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1))\n\n# Plot normalized confusion matrix\ncm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\nnp.set_printoptions(precision=2)\nplt.figure(figsize=(7,7))\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion matrix')\nplt.colorbar()\ntick_marks = np.arange(len(lb.classes_))\nplt.xticks(tick_marks, lb.classes_, rotation=90)\nplt.yticks(tick_marks, lb.classes_)\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","9a8c86b8":"print(classification_report(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1), target_names=lb.classes_))","270148ab":"model_4 = Sequential()\nmodel_4.add(Embedding(26, 20, input_length=285))\n\n# can alter this as well (bigger = overfit)\nmodel_4.add(LSTM(25, return_sequences=True))\n\nmodel_4.add(Flatten())\n# can play with this layer\nmodel_4.add(Dense(7125, activation='sigmoid'))\nmodel_4.add(Dense(20, activation='softmax'))\n\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', \n                                                  mode='max', \n                                                  restore_best_weights=True, \n                                                  patience=5)\n\nopt = tf.keras.optimizers.Adam(learning_rate= 0.001)\n\nmodel_4.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n\nmodel_4.summary()","eb5eebd4":"model_4.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, callbacks=early_stopping)","90dfc57e":"plot_model_results(model_4)","a269efbe":"train_pred = model_4.predict(X_train)\ntest_pred = model_4.predict(X_test)\nprint(\"train accuracy = \" + str(accuracy_score(np.argmax(y_train, axis=1), np.argmax(train_pred, axis=1))))\nprint(\"test accuracy = \" + str(accuracy_score(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1))))","50a61131":"# Compute confusion matrix\ncm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1))\n\n# Plot normalized confusion matrix\ncm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\nnp.set_printoptions(precision=2)\nplt.figure(figsize=(7,7))\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion matrix')\nplt.colorbar()\ntick_marks = np.arange(len(lb.classes_))\nplt.xticks(tick_marks, lb.classes_, rotation=90)\nplt.yticks(tick_marks, lb.classes_)\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","20b4dc31":"print(classification_report(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1), target_names=lb.classes_))","aef598bc":"model_5 = Sequential()\nmodel_5.add(Embedding(26, 20, input_length=285))\n\nmodel_5.add(LSTM(30, return_sequences=True))\n\nmodel_5.add(Flatten())\nmodel_5.add(Dense(8550, activation='sigmoid'))\nmodel_5.add(Dense(20, activation='softmax'))\nmodel_5.add(Dropout(0.5))\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', \n                                                  mode='max', \n                                                  restore_best_weights=True, \n                                                  patience=5)\n\nopt = tf.keras.optimizers.Adam(learning_rate= 0.001)\n\nmodel_5.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n\nmodel_5.summary()","45a9e644":"model_5.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, callbacks=early_stopping)","3785d11f":"plot_model_results(model_5)","d9e45588":"train_pred = model_5.predict(X_train)\ntest_pred = model_5.predict(X_test)\nprint(\"train accuracy = \" + str(accuracy_score(np.argmax(y_train, axis=1), np.argmax(train_pred, axis=1))))\nprint(\"test accuracy = \" + str(accuracy_score(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1))))","8bdb16af":"# Compute confusion matrix\ncm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1))\n\n# Plot normalized confusion matrix\ncm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\nnp.set_printoptions(precision=2)\nplt.figure(figsize=(7,7))\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion matrix')\nplt.colorbar()\ntick_marks = np.arange(len(lb.classes_))\nplt.xticks(tick_marks, lb.classes_, rotation=90)\nplt.yticks(tick_marks, lb.classes_)\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","1bb56d0e":"print(classification_report(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1), target_names=lb.classes_))","9e1e58b8":"data","ba887733":"data['classification'].value_counts()[:10]","3cb8cf3f":"# only keep top 10 classifications\ncounts = data.classification.value_counts()\nhello = np.asarray(counts[(counts > 2530)].index)","27d05a09":"data_top10 = data[data.classification.isin(hello)]","7c149e89":"X = data_top10['sequence']\ny = data_top10['classification']","11d215dc":"# encode classification labels\nlb = LabelBinarizer()\ny_encoded = lb.fit_transform(y)","48c73f3c":"# encode sequences using tokenizer\n# create and fit tokenizer\ntokenizer = Tokenizer(char_level=True)\ntokenizer.fit_on_texts(X)\n# represent input data as word rank number sequences\nX = tokenizer.texts_to_sequences(X)\nX = sequence.pad_sequences(X, maxlen=285)","459c20c2":"# number of distinct characters stored in variable max_id\n# this will be part of the input shape (reps each AA)\nmax_id = len(tokenizer.word_index)\nmax_id","1063fbec":"# split training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2)\n\n# confirm shape\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","bdf866b8":"model_6 = Sequential()\nmodel_6.add(Embedding(26, 10, input_length=285))\n\nmodel_6.add(LSTM(15, return_sequences=True))\n\nmodel_6.add(Dropout(0.5))\nmodel_6.add(Flatten())\nmodel_6.add(Dense(4275, activation='sigmoid'))\nmodel_6.add(Dense(10, activation='softmax'))\n\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', \n                                                  mode='max', \n                                                  restore_best_weights=True, \n                                                  patience=5)\n\nopt = tf.keras.optimizers.Adam(learning_rate= 0.001)\n\nmodel_6.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n\nmodel_6.summary()","7914a70b":"model_6.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, callbacks=early_stopping)","e740a9ad":"plot_model_results(model_6)","4978ccc8":"train_pred = model_6.predict(X_train)\ntest_pred = model_6.predict(X_test)\nprint(\"train accuracy = \" + str(accuracy_score(np.argmax(y_train, axis=1), np.argmax(train_pred, axis=1))))\nprint(\"test accuracy = \" + str(accuracy_score(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1))))","23fa9d64":"# Compute confusion matrix\ncm = confusion_matrix(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1))\n\n# Plot normalized confusion matrix\ncm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\nnp.set_printoptions(precision=2)\nplt.figure(figsize=(7,7))\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('Confusion matrix')\nplt.colorbar()\ntick_marks = np.arange(len(lb.classes_))\nplt.xticks(tick_marks, lb.classes_, rotation=90)\nplt.yticks(tick_marks, lb.classes_)\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","4bffc778":"print(classification_report(np.argmax(y_test, axis=1), np.argmax(test_pred, axis=1), target_names=lb.classes_))","d1a1e3bb":"## Conclusions\n- Using only the top 10 classes definitely helps the model focus on the most frequently classified proteins\n- Also adjusting the parameters for the LSTM and Dense nodes helped increase accuracy and decrease loss\n- Loss is still pretty high but can explore more regularizers to help mitigate this\n- Sequences are highly conserved in the classification of protein function\n- RNNs are the best models for protein classification based on amino acid sequences, specifically adding the LSTM layer\n  - This solves the problem of having short term memory (as with RNN) so we incorporate a cell (LSTM) that help regulate the flow of information\n  - LSTMs are good at processing long strings of sequences\n\n## Future Steps\n- Adjust max_length of sequences to input (try a few ranges to test)\n- Adjust dense layer prior to final dense layer; increase\/decrease input into final node\n- Fine tune some of the LSTM parameters but be careful about overfitting! \n- Incorporate the data on physical properties and feed into an ML model\n- Build a pipeline to take in this data and include the sequence information\n","118b1b90":"`model_1` \n- Performed very well considering we're trying to classify into 20 labels\n- The following protein types\/classes were predicted with high precision:\n                                       precision    recall  f1-score   support\n                        IMMUNE SYSTEM       0.88      0.87      0.87       815\n                               LIGASE       0.83      0.71      0.77       386\n                                LYASE       0.88      0.78      0.82       824\n                       OXIDOREDUCTASE       0.88      0.88      0.88      2461\n                SUGAR BINDING PROTEIN       0.83      0.81      0.82       258\n                        VIRAL PROTEIN       0.80      0.68      0.74       431\n                SUGAR BINDING PROTEIN       0.83      0.80      0.82       296\n\n`model_2`\n- Adding bidirectional LSTM layer; however, since order of the sequence matters this might be a problem if the fragments are not all equal length (`max_length` = 285 AA's long)\n- Source: https:\/\/keras.io\/api\/layers\/recurrent_layers\/bidirectional\/","d7be35ec":"## Protein Classification Problem\n\n### Background Research\n- Proteins are macromolecules responsible for all biological processes in living cells\n- They are made up of amino acid chains, making up a larger sequence of these molecules\n    - Further, each amino acid type is determined by the underlying DNA sequence in a gene\n- The sequence of amino acids influence how the proteins fold, which dictates the function of the protein\n- Protein function is a vast area of research in biotechnology, and understanding this further is critical for developing therapeutics and precision diagnostics \n\n<img src= \"https:\/\/www.ebi.ac.uk\/training\/online\/courses\/protein-classification-intro-ebi-resources\/wp-content\/uploads\/sites\/96\/2020\/07\/figure1.png\" width=400>\n\n<a href=\"https:\/\/www.ebi.ac.uk\/training\/online\/courses\/protein-classification-intro-ebi-resources\/protein-classification\/\" target=\"_blank\">image source<\/a>\n\n### Purpose\n- Proteins can be classified by their physical properties; but these classifications tend to be very general\n- Amino acids each have unique physical and chemical properties, so when we have this kind of data, we can make broad generalizations of the protein function overall\n- More specific classifications can be made, however, by analyzing the amino acid sequence itself\n- The order of each amino acid within the longer chain is crucial for protein function, and other arrangements of these building blocks results in a totally different protein with different function\n\n*** \n- Some examples of how these amino acid building blocks influence protein function:\n    - Active sites on proteins contain amino acids involved in catalytic activity\n        - Example: Lipase catalyses the formation and hydrolysis of fats --> has two amino acid residues (a histidine followed by a glycine) that are essential for its catalytic activity\n    - Binding sites contain amino acids that are directly involved in binding molecules or ions\n        - Example: Iron-binding site of haemoglobin\n    - Post-translational modification (PTM) sites contain residues known to be chemically modified (phosphorylated, palmitoylated, acetylated, etc) after the process of protein translation\n    - Repeats are typically short amino acid sequences that are repeated within a protein\n***\n\n### Dataset\n- We have 2 .csv files of data: \n    - `pdb_data_no_dups.csv` provides physical properties of the protein\n        - properties (numeric): residue count, pH, crystallization temperature, resolution, molecular weight, and density\n            - residue count: number of amino acids in the sequence\n            - pH: scale determining acidity or basicity of a solution (e.g. acidic solutions have a pH < 7.0 and basic solutions have pH > 7.0)\n            - crystallization temperature: temperature at which the protein crystallizes (precipitates out of solution during crystallization process)\n            - resolution: measure of the quality of the data that has been collected on the crystal containing the protein\n            - molecular weight: molecular mass of a given molecule (expressed in kilo Daltons, kDa)\n            - density: \n        - target label is provided here, as \"classification\" of the protein type (e.g. ligase, transferase, oxidoreductase, etc.)\n    - `pdb_data_seq.csv` provides the amino acid sequence for each protein\n    #### target label:\n    - `classification`\n\n    #### categorical vars: \n    - `experimentalTechnique`\n    - `crystallizationMethod`\n    - `pdbxDetails`\n    - `publicationYear`\n    - `sequence`\n\n    #### numerical vars:\n    - `residueCount`\n    - `resolution`\n    - `structureMolecularWeight`\n    - `crystallizationTempK`\n    - `densityMatthews`\n    - `densityPercentSol`\n    - `phValue`\n\n\n### Problem Statement\n- Classify the protein type\/function using information provided in the dataset\n- Evaluate performance of models\n\n### Sources\n1. https:\/\/www.ebi.ac.uk\/training\/online\/courses\/protein-classification-intro-ebi-resources\/protein-classification\/\n2. https:\/\/pdb101.rcsb.org\/learn\/guide-to-understanding-pdb-data\/introduction","ab1699dd":"### Exploratory Data Analysis ","f66c9f99":"### Last thing to try!\n- Pare down classes to top 10, run `model_5` again (`model_6`) and let's see if we can improve precision for more classes","2755d2c0":"`model_1` \n- Add LSTM layer after setting input\n- Proceed to adding hidden layers and use same optimization parameters as baseline model\n- I predict this model will significantly outperform the CNN model from the baseline\n- This is because the sequences of AA characters matter in a particular order; each cluster of specific AAs will encode a particular function for a protein\n\n- LSTM models mitigate the issue with traditional RNN models in that they are able to store information for longer amounts of time \n\n**RNN**:\n\n<img src= \"https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-SimpleRNN.png\" width=400>\n\n\n\n**LSTM**:\n\n<img src= \"https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-chain.png\" width=400>\n\n\n- Sources: https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/","1e3d42df":"### EDA Interpretations\n\n- Plots show that these variables likely do not influence protein classification\n- The variables analyzed here specify mostly physical and chemical properties of the proteins\n- We could do some regression analyses on the numerical variables but considering how these initial plots look, I'd hypothesize these parameters would not deliver a strong classification model\n- We know that AA sequences influence protein function and all machine learning models in this notebook will focus on this","0d4774f3":"### Machine Learning Models\n#### *Using sequence analysis to predict protein classification*\n<img src= \"https:\/\/openclipart.org\/image\/800px\/95191\" width=400>\n\n- The image above shows the codon sequences that make up each amino acid\n- Ordered sequences of these AAs define a specific function of the resulting encoded protein\n- Use tokenizer to encode sequence characters\n\n#### Convolutional Neural Networks (CNN)\n- `model_0`\n  - Sequential base, building 1-dimensional convolutional and pooling layers\n  - Semi-qualitative model, treating the sequence input as some sort of image or pattern\n    - One caveat is that these sequences don't all start with the same starting codon, therefore they have similar segments but are not necessarily aligned neatly\n    - If they were aligned and we had all equal fragments of sequences, this approach might be successful\n  - This baseline is just being used to see what our results look like with a CNN model although this is not the model I hypothesize will be iterated on for fine tuning\n\n#### Recurring Neural Networks (RNN)\n- RNN is a type of artificial neural network which uses sequential data or time series data\n- Commonly used for ordinal or temporal problems, such as language translation, natural language processing (NLP), speech recognition, and image captioning\n- RNN take information from prior inputs to influence the current input and output\n- Output of RNNs depend on the prior elements within the sequence\n- Future events would also be helpful in determining the output of a given sequence, use **bidirectional RNNs** to account for these events in their predictions","495fe309":"`model_3`\n- Addition of regularizers did not help model performance\n\n`model_4`\n- Re-run `model_1` because it was the most successful\n- LSTM layer with same parameters\n- Run 50 epochs\n- Increase `patience` to `5` in `early_stopping` callback\n- Should see the `accuracy` and `loss` curves approach asymptotes, approximately to 95% and 15%, respectively","a00cd036":"<a href=\"https:\/\/colab.research.google.com\/github\/seetarajpara\/Module5_Capstone_ProteinClassificationProject\/blob\/main\/SRajpara_Module5_Capstone_ProteinClassificationProject.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","e1f16dc6":"`model_2`\n- Achieved slightly lower accuracy on test set with this bidirectional LSTM model\n- This actually follows the initial thought that the unidirectional sequence of AAs in the chain matter more, and doing a reverse read on the sequence would not be beneficial considering the sequences are of differing lengths (max=285)\n\n`model_3`\n- Revisiting the initial LSTM model from `model_1` \n- Added layer weight regularizers\n  - Apply penalties on layer parameters or layer activity during optimization\n  - These penalties are summed into the loss function that the network optimizes\n  - `kernel_regularizer`: Regularizer to apply a penalty on the layer's kernel\n  - `bias_regularizer`: Regularizer to apply a penalty on the layer's bias\n  - `activity_regularizer`: Regularizer to apply a penalty on the layer's output\n\nSource:\n- https:\/\/keras.io\/api\/layers\/regularizers\/","fc0af6fd":"`model_4` works well and we're achieving high precision on some of the most ubiquitous classes:\n\n                                        precision    recall  f1-score   support\n                            HYDROLASE       0.73      0.84      0.79      4046\n                        IMMUNE SYSTEM       0.82      0.87      0.84       762\n                            ISOMERASE       0.85      0.72      0.78       514\n                               LIGASE       0.89      0.70      0.78       422\n                                LYASE       0.87      0.79      0.83       852\n                       OXIDOREDUCTASE       0.88      0.89      0.89      2473\n                SUGAR BINDING PROTEIN       0.83      0.71      0.77       263\n                          TRANSFERASE       0.76      0.83      0.79      3053\n                    TRANSPORT PROTEIN       0.82      0.64      0.72       639\n\n### `model_5`\n- Test LSTM parameters = increasing too much could lead to overfitting! \n- Adjust first dense layer to match the input from previous flattening layer","e3468cbd":"### Sequence Analysis","93cdc54d":"`model_0` as a CNN model, I predicted this would not work well but I wanted to see if there was any patterns this model could recognize semi-qualitatively"}}