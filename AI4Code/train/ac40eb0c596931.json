{"cell_type":{"e99f95fd":"code","fac4833c":"code","56fb306e":"code","34edfcf7":"code","7af274b0":"code","386e53b6":"code","809585a5":"code","60addcc0":"code","fa6496b0":"code","3eb01f69":"code","63b5aa17":"code","80f01977":"code","805c4638":"code","156e376c":"code","7105bbda":"code","b66e7baa":"code","40b83d70":"code","923fffb2":"code","f3f5aba2":"code","82a8f251":"code","4cc61484":"code","19d669c6":"code","74d81e68":"code","1d082fd4":"code","a259043f":"code","58453d1b":"code","9ec7b15c":"code","906f751f":"code","0a98774b":"code","b2dda82e":"code","e1590ca1":"code","255ae565":"code","60f03811":"code","dc354619":"code","e2e6777d":"code","d680cc66":"code","d2380daa":"code","10e5c8f8":"code","0293bc87":"code","8b90f008":"code","0f10365f":"code","8b922f22":"code","192e099a":"code","78505498":"code","2f01e9bc":"code","8f92b23f":"code","b4e38527":"code","f6ef29b9":"code","f35c9d54":"code","95423f29":"code","a7d64417":"code","2c469dc8":"code","6dbad8f7":"code","bcb96f8c":"code","f331b2fc":"code","66bd36c7":"code","26a881af":"code","1b7f9023":"code","0d5a22ca":"code","1deb8066":"code","18e69c48":"code","4c459ddf":"code","c44a3960":"code","880d58d4":"code","54b88b87":"code","726b797f":"code","0265e48d":"code","0929bed3":"code","a393c85f":"code","3263f165":"code","1dc41cc3":"code","d905ce82":"code","b3a266f6":"code","58919bbe":"code","2ead4af2":"code","eb15a049":"code","cec00634":"code","0f66166f":"code","f0ca6f01":"code","8b8e5969":"code","af56caa1":"code","b0401660":"code","063265ca":"code","6c917197":"code","26dd70b3":"code","c7d397b1":"code","16550471":"code","562d9931":"code","7e1fc7c2":"code","91cffa6b":"code","7199c115":"code","c7482f4e":"code","e81d2f5b":"code","5991beb5":"code","40141e88":"code","8ad77887":"code","1029ec33":"code","22c218d3":"code","1c8c9c29":"code","4de67851":"code","3fdaa895":"code","01acd705":"code","e36ba4e8":"code","36a1660d":"code","7bc59ec0":"code","15a49f18":"code","93d7337a":"code","192e0537":"code","82831509":"code","ffac3b74":"code","702b52f1":"code","01654aca":"code","7859b365":"code","a54d6791":"code","e0428221":"code","d010d7af":"code","16b31b77":"code","9d6da60d":"code","7d6a51bd":"code","57e15f89":"markdown","e209068c":"markdown","c17ba324":"markdown","40da79ff":"markdown","c423d138":"markdown","c8aca27a":"markdown","3f1e3b29":"markdown","d92b9362":"markdown","472c6417":"markdown","650dc9e3":"markdown","dd1e4d46":"markdown","c29cd3ec":"markdown","24fc23b6":"markdown","4175395e":"markdown","4fd513ea":"markdown","af633673":"markdown","230f51c7":"markdown","349dd02f":"markdown","ef17288d":"markdown","d0413258":"markdown","cbaf9efa":"markdown","56118222":"markdown","eca2da1e":"markdown","ecd0a70f":"markdown","a0060163":"markdown","c020aa85":"markdown","26c0de42":"markdown","d93b7726":"markdown","829b29c3":"markdown","e51716d4":"markdown","8ef350e1":"markdown","d6c10a08":"markdown","f98012ab":"markdown","1a6b9097":"markdown","a806b94f":"markdown","34157832":"markdown","06519089":"markdown","1bbf1f94":"markdown","7fbf5282":"markdown","b61bad5d":"markdown","23fe0d23":"markdown","e1437b3e":"markdown","39c2f7b4":"markdown","9a1441e9":"markdown","ecfa53e3":"markdown","a7ebcdda":"markdown","c8be0ed2":"markdown","8a7f34ad":"markdown","f1cc3941":"markdown","91fac1b2":"markdown","40c9fe7a":"markdown","3cdeb878":"markdown","bfd87399":"markdown","86c9bcd8":"markdown","98417562":"markdown","f6046f67":"markdown","8115fed1":"markdown","8a1fe681":"markdown","5ed4a51f":"markdown","0dd3ccba":"markdown","c7cbea1c":"markdown","2de117e5":"markdown","f104a960":"markdown","a267250b":"markdown","b01abd57":"markdown","38bace97":"markdown","4c33b23e":"markdown","8903c98d":"markdown","10f24e16":"markdown"},"source":{"e99f95fd":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","fac4833c":"import pandas as pd\nprint(\"Pandas version:\", pd.__version__)\n\nimport numpy as np\nprint(\"Numpy version:\", np.__version__)\n\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import AutoMinorLocator\n\nfrom scipy import stats\nfrom math import floor\n\n# For outlier detection\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.impute import SimpleImputer","56fb306e":"def null_ratio(df, null_column):\n  columns = list(df.columns)\n  columns.remove(null_column)\n  null_rows = df[df[null_column].isnull()][columns]\n  denominator = len(columns) * len(null_rows)\n  numerator = denominator - null_rows.isnull().sum().sum()\n  return numerator\/denominator","34edfcf7":"def overlapping_histogram(data1, data2,column, label1=None, label2=None,  bins=100):\n  plt.figure(figsize=(8,6))\n  plt.hist(data1[column], alpha=0.5, label=label1, bins=bins)\n  plt.hist(data2[column], alpha=0.5, label=label2, bins=bins)\n  plt.xlabel(\"Data\", size=14)\n  plt.ylabel(\"Count\", size=14)\n  plt.title(column)\n  plt.legend(loc='upper right')","7af274b0":"def plot_by_id(column):\n  np.random.seed(42)\n  fig, ax  = plt.fig, axs = plt.subplots(figsize=(15,5))\n  ax.scatter(train_data['Id'], train_data[column] )\n  ax.scatter(train_data['Id'], train_data[column].isnull(), c=train_data[column].isnull() );\n  ax.axhline(y=train_data[column].median(), color = 'r', linestyle = '-')\n  ax.set_title(column)\n  ax.text(x=-0.25, y=train_data[column].median(), s=train_data[column].median(), weight='bold');\n","386e53b6":"def show_heatmap(data,figsize=(12,8) , highest_only=False , thresold=0.7, annot=False):\n  correlation_matrix = data.corr()\n  high_corr = correlation_matrix[np.abs(correlation_matrix )>= thresold]\n\n  plt.figure(figsize=figsize)\n\n  if highest_only:\n    sns.heatmap(high_corr, annot=annot,cmap=\"YlGnBu\",  linecolor='black', linewidths=0.2)\n  else:\n    sns.heatmap(correlation_matrix, annot=annot)","809585a5":"def display_skew(data, columns, title='Skewness', figsize=(8,4)):\n  data_sub = data[columns]\n  plt.figure(figsize=figsize)\n  plt.bar(data_sub.skew().index, data_sub.skew().values)\n  plt.grid(axis='y')\n  plt.axhline(y = 1, color = 'r', linestyle = '-')\n  plt.axhline(y = -1, color = 'r', linestyle = '-', label='-1')\n\n  # if the size of the width is less than number of columns, make xticks vertical\n  if len(columns) > figsize[0]:\n    plt.xticks(rotation='vertical')\n\n  plt.title(title)","60addcc0":"train_data = pd.read_csv('\/kaggle\/input\/prudential-life-insurance-assessment\/train.csv.zip')\ntest_data = pd.read_csv('\/kaggle\/input\/prudential-life-insurance-assessment\/test.csv.zip')\nsubmission = pd.read_csv('\/kaggle\/input\/prudential-life-insurance-assessment\/sample_submission.csv.zip')\n\ntrain_data.shape, test_data.shape","fa6496b0":"drop_columns=set()\nskewed_columns=set()\noutliers=set()","3eb01f69":"basic_cols = (train_data.columns[~train_data.columns.str.startswith('Medical') & ~train_data.columns.str.startswith('Product')]).tolist()\nproduct_col = (train_data.columns[train_data.columns.str.startswith('Product')]).tolist()\nmedical_hist_col = (train_data.columns[train_data.columns.str.startswith('Medical_History')]).tolist()\nmedical_kw_col = (train_data.columns[train_data.columns.str.startswith('Medical_Keyword')]).tolist()","63b5aa17":"basic_train = train_data[basic_cols]\nbasic_train.info()","80f01977":"show_heatmap(basic_train, highest_only=True, annot=True, figsize=(18,7))","805c4638":"drop_columns.add('BMI')\ndrop_columns","156e376c":"employment_info_cont = ['Employment_Info_1', 'Employment_Info_4', 'Employment_Info_6']\nbasic_train[employment_info_cont].hist(figsize=(12,6))\nplt.suptitle(\"Employee Info Continous values Histogram\");","7105bbda":"display_skew(train_data,employment_info_cont, 'Employment Skewness')","b66e7baa":"skewed_columns.update({'Employment_Info_1', 'Employment_Info_4'})\nskewed_columns","40b83d70":"fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\nfor idx, col in enumerate(employment_info_cont):\n  sns.lineplot(ax=ax[idx],y=\"Response\", x=col, data=train_data)\nplt.suptitle('Employment Info Continous values to Response');","923fffb2":"employment_info_cat = ['Employment_Info_2', 'Employment_Info_3', 'Employment_Info_5']\nbasic_train[employment_info_cat].hist(figsize=(12,6))\nplt.suptitle(\"Employee Info Categorical values Histogram\");\n# sns.catplot(x=\"Employment_Info_2\", col=\"Response\", data=train_data, kind=\"count\", col_wrap=3);","f3f5aba2":"sns.catplot(data=train_data[employment_info_cat])\nplt.suptitle('Employment Info Category');","82a8f251":"fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15,5))\nfor idx, col in enumerate(employment_info_cat):\n  p = sns.scatterplot(ax=ax[idx],x=\"Response\", y=col, hue=col,data=train_data)\n  p.legend_.remove()\nplt.suptitle('Employment Info Categorical values to Response');","4cc61484":"insured_info_col = (train_data.columns[train_data.columns.str.startswith('InsuredInfo')]).tolist()","19d669c6":"fig, ax = plt.subplots(3, 3, figsize=(21, 12))\nfor col, an_axes in zip(insured_info_col, ax.flatten()):\n    sns.countplot(data=train_data, x=col, ax=an_axes)\nplt.delaxes(ax[2][1])\nplt.delaxes(ax[2][2])\nplt.suptitle('Insured Info Count Plot');","74d81e68":"display_skew(train_data,insured_info_col, figsize=(6,4), title='Insured Info Skewness')","1d082fd4":"fig, ax = plt.subplots(3, 3, figsize=(21, 12))\nfor col, an_axes in zip(insured_info_col, ax.flatten()):\n    sns.boxplot(data=train_data, x=col, y='Response', ax=an_axes)\n\nplt.delaxes(ax[2][1])\nplt.delaxes(ax[2][2])\nplt.suptitle('Insured Info vs Response');","a259043f":"# Check basic columns has correlation with non basic columns\nbasic_corr = train_data.corr()[basic_cols]\nset(basic_corr[basic_corr[np.abs(basic_corr) > 0.7 ].notnull().sum(axis=1) > 1].index) - set(basic_cols)","58453d1b":"insured_hist_col = (train_data.columns[train_data.columns.str.startswith('Insurance_History')]).tolist()\ninsured_hist_col","9ec7b15c":"fig,ax = plt.subplots(ncols=3, figsize=(16,4))\nsns.scatterplot(ax=ax[0],data=train_data, x='Insurance_History_5', y='Response')\nsns.histplot(ax=ax[1],data=train_data, x='Insurance_History_5', bins=3)\nsns.boxplot(ax=ax[2],data=train_data, x='Insurance_History_5')\nplt.suptitle('Insurace_History_5');","906f751f":"outliers.add('Insurance_History_5')","0a98774b":"# Categorical columns of Insured History\ninsured_history_cat = ['Insurance_History_1',\n 'Insurance_History_2',\n 'Insurance_History_3',\n 'Insurance_History_4',\n 'Insurance_History_7',\n 'Insurance_History_8',\n 'Insurance_History_9']\n\n# display the box plot for categorical insured histry\nfig, ax = plt.subplots(2, 4, figsize=(15, 8))\nfor col, an_axes in zip(insured_history_cat, ax.flatten()):\n    sns.boxplot(data=train_data, x=col, y='Response', ax=an_axes)\nplt.delaxes(ax[1][3])\nplt.suptitle('Insured History Categorical vs Response');","b2dda82e":"sns.countplot(data=train_data, x='Family_Hist_1', hue='Response');","e1590ca1":"sns.violinplot(data=train_data, x='Family_Hist_1', y='Response');","255ae565":"family_hist_cont = ['Family_Hist_2', 'Family_Hist_3', 'Family_Hist_4', 'Family_Hist_5']","60f03811":"train_data[family_hist_cont].hist(figsize=(8,6));","dc354619":"sns.boxplot(data=train_data[family_hist_cont]);","e2e6777d":"outliers.update({'Family_Hist_2', 'Family_Hist_3', 'Family_Hist_4', 'Family_Hist_5'})\noutliers","d680cc66":"# display the box plot for famil history continous data\nfig, ax = plt.subplots(2, 2, figsize=(24, 8))\nfor col, an_axes in zip(family_hist_cont, ax.flatten()):\n    sns.lineplot(data=train_data, x=col, y='Response',ax=an_axes)\n    an_axes.grid()\n\nplt.suptitle('Family History continous vs Response')\nplt.show()","d2380daa":"sns.histplot(data=train_data, x='Product_Info_4');","10e5c8f8":"ax = sns.lineplot(data=train_data, x='Product_Info_4', y='Response')\nax.grid(which='minor')\nax.minorticks_on()\nplt.show()","0293bc87":"product_info_cat =['Product_Info_1', 'Product_Info_2', 'Product_Info_3', 'Product_Info_5', 'Product_Info_6', 'Product_Info_7']","8b90f008":"fig, ax = plt.subplots(3, 2 , figsize=(15, 12))\nfor col, an_axes in zip(product_info_cat, ax.flatten()):\n    sns.countplot(data=train_data, x=col, hue='Response', ax=an_axes)\n    an_axes.tick_params(axis='x', rotation=90)\n# plt.xticks(rotation='vertical')\nplt.show();","0f10365f":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train_data['Product_Info_2'])\ntrain_data['Product_Info_2_code'] = le.transform(train_data['Product_Info_2'])\ntest_data['Product_Info_2_code'] = le.transform(test_data['Product_Info_2'])\n\n\n# train_data.drop('Product_Info_2', axis=1, inplace=True)\ndrop_columns.add('Product_Info_2')\nproduct_info_cat.remove('Product_Info_2')\nproduct_info_cat.append('Product_Info_2_code')","8b922f22":"product_info_cat","192e099a":"fig, ax = plt.subplots(3, 2 , figsize=(15, 12))\nfor col, an_axes in zip(product_info_cat, ax.flatten()):\n    sns.boxplot(data=train_data, x=col, y='Response', ax=an_axes)\n    an_axes.tick_params(axis='x', rotation=90)\nplt.show();","78505498":"outliers.update({'Product_Info_3', 'Product_Info_5', 'Product_Info_2_code'})\noutliers","2f01e9bc":"medical_hist_disc = ['Medical_History_1', 'Medical_History_10', 'Medical_History_15', 'Medical_History_24', 'Medical_History_32']","8f92b23f":"train_data[medical_hist_disc].hist(figsize=(15,8));","b4e38527":"fg = sns.catplot(data=train_data[medical_hist_disc])\nfg.axes[0][0].tick_params(axis='x', rotation=90)\nplt.show();","f6ef29b9":"fig, ax = plt.subplots(3, 2 , figsize=(15, 12))\nfor col, an_axes in zip(medical_hist_disc, ax.flatten()):\n    sns.boxplot(data=train_data, y=col, x='Response', ax=an_axes, showmeans=True)\n    an_axes.tick_params(axis='x', rotation=90)\nplt.delaxes(ax[2][1])\nplt.show();","f35c9d54":"outliers.update({'Medical_History_1', 'Medical_History_10', 'Medical_History_15', 'Medical_History_24', 'Medical_History_32'})","95423f29":"fig, ax = plt.subplots(3, 2 , figsize=(15, 12))\nfor col, an_axes in zip(medical_hist_disc, ax.flatten()):\n    sns.pointplot(data=train_data, y=col, x='Response', ax=an_axes)\nplt.delaxes(ax[2][1])\nplt.show();","a7d64417":"train_missing = train_data.isnull().sum()[train_data.isnull().sum(axis=0) > 0].sort_values()\nfig, ax = plt.subplots(figsize=(15,5))\nax.barh(train_missing.index, train_missing)\n  \nfor i, v in enumerate(train_missing):\n    ax.text(v + 3, i, str(v))\nplt.title('Missing Values')\nplt.show()","2c469dc8":"test_missing = test_data.isnull().sum()[test_data.isnull().sum(axis=0) > 0].sort_values()\nfig, ax = plt.subplots(figsize=(15,5))\nax.barh(test_missing.index, test_missing)\n  \nfor i, v in enumerate(test_missing):\n    ax.text(v + 3, i, str(v))\nplt.title('Missing Values')\nplt.show()","6dbad8f7":"train_data.info()","bcb96f8c":"train_data.describe()","f331b2fc":"mean_column=set()\nzero_column=set()\niterative_column=set()\nknn_column=set()","66bd36c7":"overlapping_histogram(train_data, test_data, 'Employment_Info_1','train', 'test')","26a881af":"mean_column.add('Employment_Info_1')\nmean_column","1b7f9023":"overlapping_histogram(train_data, test_data, 'Employment_Info_4', 'train', 'test')","0d5a22ca":"correlation_matrix = train_data.corr()\ncorrelation_matrix[np.abs(correlation_matrix['Employment_Info_4']) > 0.30]['Employment_Info_4']","1deb8066":"np.random.seed(42)\nfig, ax  = plt.fig, axs = plt.subplots(figsize=(15,5))\nax.scatter(train_data['Id'], train_data['Employment_Info_4'] )\nax.scatter(train_data['Id'], train_data['Employment_Info_4'].isnull(), c=train_data['Employment_Info_4'].isnull() );","18e69c48":"null_ratio(train_data, 'Employment_Info_4')","4c459ddf":"zero_column.add('Employment_Info_4')\nzero_column","c44a3960":"overlapping_histogram(train_data, test_data, 'Medical_History_1', 'train', 'test')","880d58d4":"correlation_matrix[np.abs(correlation_matrix['Medical_History_1']) > 0.30]['Medical_History_1']","54b88b87":"np.random.seed(42)\nfig, ax  = plt.fig, axs = plt.subplots(figsize=(15,5))\nax.scatter(train_data['Id'], train_data['Medical_History_1'] )\nax.scatter(train_data['Id'], train_data['Medical_History_1'].isnull(), c=train_data['Medical_History_1'].isnull() );\nax.axhline(y=train_data['Medical_History_1'].median(), color = 'r', linestyle = '-')\nax.text(x=-0.25, y=train_data['Medical_History_1'].median(), s=train_data['Medical_History_1'].median(), c='w',weight='bold');\n","726b797f":"null_ratio(train_data, 'Medical_History_1', )","0265e48d":"knn_column.add('Medical_History_1')","0929bed3":"overlapping_histogram(train_data, test_data, 'Employment_Info_6', 'train', 'test')","a393c85f":"correlation_matrix[np.abs(correlation_matrix['Employment_Info_6']) > 0.30]['Employment_Info_6']","3263f165":"plot_by_id('Employment_Info_6')","1dc41cc3":"iterative_column.add('Employment_Info_6')","d905ce82":"overlapping_histogram(train_data, test_data, 'Family_Hist_4', 'train', 'test')","b3a266f6":"plot_by_id('Family_Hist_4')","58919bbe":"correlation_matrix[np.abs(correlation_matrix['Family_Hist_4']) > 0.30]['Family_Hist_4'].sort_values()","2ead4af2":"iterative_column.add('Family_Hist_4')\niterative_column","eb15a049":"overlapping_histogram(train_data, test_data, 'Insurance_History_5', 'train', 'test')","cec00634":"plot_by_id('Insurance_History_5')","0f66166f":"correlation_matrix[np.abs(correlation_matrix['Insurance_History_5']) > 0.30]['Insurance_History_5'].sort_values()","f0ca6f01":"zero_column.add('Insurance_History_5')\nzero_column","8b8e5969":"overlapping_histogram(train_data, test_data, 'Family_Hist_2', 'train', 'test')","af56caa1":"plot_by_id('Family_Hist_2')","b0401660":"correlation_matrix[np.abs(correlation_matrix['Family_Hist_2']) > 0.30]['Family_Hist_2'].sort_values()","063265ca":"iterative_column.add('Family_Hist_2')\niterative_column","6c917197":"overlapping_histogram(train_data, test_data,  'Family_Hist_3', 'train', 'test')","26dd70b3":"plot_by_id('Family_Hist_3')","c7d397b1":"correlation_matrix[np.abs(correlation_matrix['Family_Hist_3']) > 0.30]['Family_Hist_3'].sort_values()","16550471":"iterative_column.add('Family_Hist_3')\niterative_column","562d9931":"overlapping_histogram(train_data, test_data,  'Family_Hist_5', 'train', 'test')\nplot_by_id('Family_Hist_5')\ncorrelation_matrix[np.abs(correlation_matrix['Family_Hist_5']) > 0.30]['Family_Hist_5'].sort_values()\n","7e1fc7c2":"iterative_column.add('Family_Hist_5')\niterative_column","91cffa6b":"print(\"Age 0\\t:\",(train_data['Ins_Age'] == 0 ).sum())\nprint(\"Ht 0\\t:\",(train_data['Ht'] == 0 ).sum())\nprint(\"Wt 0\\t:\",(train_data['Wt'] == 0 ).sum())\nprint(\"BMI 0\\t:\",(train_data['BMI'] == 0 ).sum())","7199c115":"large_missing_columns = ['Medical_History_10', 'Medical_History_15', 'Medical_History_24', 'Medical_History_32']\n\nfor a_missing_col in large_missing_columns:\n  print(a_missing_col,\":\")\n  print(correlation_matrix[(np.abs(correlation_matrix[a_missing_col]) > 0.30) & (np.abs(correlation_matrix[a_missing_col]) < 1)][a_missing_col].sort_values())\n  print(\"\")","c7482f4e":"# How many row have all those four columns as null\nmissing_rows = np.full(train_data.shape[0], True, dtype=bool)\nfor a_missing_col in large_missing_columns:\n  missing_rows = train_data[a_missing_col].isnull() & missing_rows\n\nmissing_rows.sum()","e81d2f5b":"fig, ax  = plt.fig, axs = plt.subplots(figsize=(15,5))\nfor a_missing_col in large_missing_columns:\n  ax.scatter(train_data['Id'], train_data[a_missing_col], label=a_missing_col)\nplt.legend(loc='upper left');\nplt.show()","5991beb5":"drop_columns.update(large_missing_columns)\ndrop_columns","40141e88":"# Additionaly drop the medical keyword column as they are dummy\nkeyword_col = train_data.columns[train_data.columns.str.startswith('Medical_Keyword')].tolist()\ndrop_columns.update(keyword_col)\ndrop_columns.add('Id')\nprint(drop_columns)","8ad77887":"print(\"Drop Columns:\", drop_columns)","1029ec33":"train_data.drop(drop_columns, axis=1, inplace=True)\nX = train_data.drop('Response', axis=1)\ny = train_data['Response']\nX.head(5)","22c218d3":"X_train, X_val , y_train, y_val = train_test_split(X,y,random_state=42)\nX_train.shape, X_val.shape, y_train.shape, y_val.shape","1c8c9c29":"X_test = test_data.drop(drop_columns, axis=1)","4de67851":"# # Dummy imputation just to enable the feature selection\ntmp_X_train = make_pipeline(SimpleImputer(strategy='mean'), StandardScaler()).fit_transform(X_train)\ntmp_X_train_df = pd.DataFrame(tmp_X_train, columns=X_train.columns)","3fdaa895":"from sklearn.feature_selection import SelectKBest, f_classif\nk_best = SelectKBest(score_func=f_classif, k='all')\nk_best.fit(tmp_X_train_df, y_train)\n\n# transform train data\ntmp_X_train = k_best.transform(tmp_X_train_df)\ntmp_X_train_df = pd.DataFrame(tmp_X_train_df, columns=X_train.columns)","01acd705":"# An thresold score selected for experiment\nselected_thresold = 70\nprint(\"=>\", selected_thresold,( k_best.scores_ >= selected_thresold ).sum())\n\n# See how other thresold looks\nprint(\"=> 250\",( k_best.scores_ >= 250 ).sum())\nprint(\"=> 75\",( k_best.scores_ >= 70 ).sum())\nprint(\"=> 10\",( k_best.scores_ >= 10 ).sum())\nprint(\"< 10\",( k_best.scores_ > 10 ).sum())","e36ba4e8":"pd.DataFrame(k_best.scores_.reshape(1,73), columns=tmp_X_train_df.columns)","36a1660d":"# plot the scores\nplt.figure(figsize=(14,6))\nplt.bar(tmp_X_train_df.columns, k_best.scores_)\nplt.axhline(y=selected_thresold, color = 'r', linestyle = '-.')\nplt.axhline(y=250, color = 'b', linestyle = '-.')\nplt.axhline(y=10, color = 'violet', linestyle = '-.')\n\nplt.xticks(rotation='vertical')\nplt.title('F Score using Annova')\nplt.show()","7bc59ec0":"selected_features = set(list(tmp_X_train_df.columns[k_best.scores_ >= selected_thresold]))\nprint(f\"Number of selected features:{len(selected_features)}\")\nprint(selected_features)","15a49f18":"X_train_selected = X_train[selected_features]\nX_val_selected = X_val[selected_features]\nX_test_selected = X_test[selected_features]","93d7337a":"outliers = outliers.intersection(selected_features)\noutliers","192e0537":"df_for_outlier = X_train[~X_train['Family_Hist_4'].isna()].copy()\nisf=IsolationForest()\nisf.fit(df_for_outlier[['Family_Hist_4']])\n\ndf_for_outlier['scores']  =isf.decision_function(df_for_outlier[['Family_Hist_4']])\ndf_for_outlier['anomaly'] =isf.predict(df_for_outlier[['Family_Hist_4']])","82831509":"fig, ax = plt.subplots(ncols=2, figsize=(12,6))\nsns.countplot(data=df_for_outlier, x='anomaly', ax=ax[0]);\nsns.scatterplot(data=df_for_outlier, x='Family_Hist_4', y='scores', hue=\"anomaly\", ax=ax[1]);","ffac3b74":"# Create an outlier transformer for pipeline\nclass OutlierTransformer(object):\n  \"\"\"\n  This transformer will add a Anomaly columns\n  Return a Pandas Data frame\n  \"\"\"\n  def __init__(self, transformer, outlier_cols, df_columns):\n    # The outlier predictor, example isolation forest\n    self.transformer = transformer\n    # The columns which has outliers, this is list\n    self.outlier_cols = outlier_cols\n    # All the columns in the incoming dataframe\n    self.df_columns = df_columns\n\n  def fit(self,X,y=None):\n    return self\n\n  def transform(self,X,y=None):\n    X_ = X.copy()\n    \"\"\"\n    Previous steps in pipeline outputs numpy array rather than dataframe\n    Here we recreate the dataframe\n    \"\"\"\n    X_df = pd.DataFrame(X_, columns=self.df_columns)\n    # Add the anomaly column\n    X_df['Anomaly'] = self.transformer.predict(X_df[self.outlier_cols])\n    return X_df","702b52f1":"# Use only selected columns\nzero_column = zero_column.intersection(selected_features)\nmean_column = mean_column.intersection(selected_features)\niterative_column = iterative_column.intersection(selected_features)\nknn_column = knn_column.intersection(selected_features)\n\nprint(\"Zero Constant Imputation:\", zero_column )\nprint(\"Mean Imputation:\", mean_column)\nprint(\"Iterative Imputation:\", iterative_column)\nprint(\"KNN Imputation:\", knn_column)","01654aca":"# select columns with no missing values\nnon_missing_col = selected_features\nnon_missing_col = non_missing_col - zero_column - mean_column - iterative_column - knn_column - drop_columns\nprint(\"Non Missing Columns: Size {} : {} \".format(len(non_missing_col), non_missing_col) )","7859b365":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer","a54d6791":"%%time\nprocessed_columns = list(X_train_selected.columns)\nprocessed_columns.append('Missing_Ind')\n\niter_transformer = IterativeImputer(random_state=42,  add_indicator=True)\npreprocessor = Pipeline(steps=[\n                           ('normalizer', StandardScaler()),\n                           ('imputer', iter_transformer),\n                           ('outlier_transformer', OutlierTransformer(isf, ['Family_Hist_4'], processed_columns))\n])\n\n\nX_train_processed = preprocessor.fit_transform(X_train_selected)\nX_val_processed = preprocessor.transform(X_val_selected)\nX_test_processed = preprocessor.transform(X_test_selected)","e0428221":"X_train_processed.shape, X_val_processed.shape, X_test_processed.shape","d010d7af":"%%time\n\nimport xgboost \nmodel = xgboost.XGBClassifier(random_state=42)\nmodel.fit(X_train_processed, y_train)\nprint(model.score(X_val_processed, y_val))","16b31b77":"y_pred = model.predict(X_test_processed)","9d6da60d":"sample_df = pd.DataFrame(zip(test_data['Id'], y_pred), columns=['Id', 'Response'])","7d6a51bd":"sample_df.to_csv(\"sample_submission.csv\", index=False)\n%ls -lrt","57e15f89":"# Split data into training and validation","e209068c":"# Preprocessing","c17ba324":"This proves that none of the basic colums are correlated with non basic columns","40da79ff":"* Employment info 6: seems to be less correlated with response\n* Employment info 1: higher value gives less chance of getting response between 3-5\n* Employment info 4: values between 4-8 does not result in response 1-3\n","c423d138":"### Insurance_History","c8aca27a":"Number of missing values for Employment_Info_1 is low, hence it is unlikely change the distribution if we fill it with mean.","3f1e3b29":"Conclusion\n* All of them have skewed single tail\n* Insured History 2 & 3 value 2 directly indicates 8 & 1 respectively","d92b9362":"Again this is a candidate for iterative imputer","472c6417":"# Meet and Greet the data\n<br>[Kaggle URL](https:\/\/www.kaggle.com\/c\/prudential-life-insurance-assessment\/data)","650dc9e3":"## Rest of missing values\n* 'Medical_History_10'\n* 'Medical_History_15'\n* 'Medical_History_24'\n* 'Medical_History_32'\n\nThese columns have almost more than 50% columns missing, lets see how much correlation they have with other columns","dd1e4d46":"\n*   There are outliers\n*   Not much Correlation with others\n*   Median is very close to zero\n\n","c29cd3ec":"## Family_Hist_5 - Continous","24fc23b6":"Interestingly there is no Insurance_History_6\n<br>Only Insurance_History_5 is continous, lets look at it","4175395e":"# Imports","4fd513ea":"Again consider this for iterative imputation, though this was a ideal candidate for KNN imputer but let see how its perform with iterative","af633673":"\n*   Lower Family History 2 near to zero have more chance of response 1-5\n*   Response is unstable at after value 0.8\n*   As we have seen earlier there are outliers for Family History 2, 3 & 6\n\n","230f51c7":"* High number of missing values\n* Similar distribution as other family history\n* Looks like there are outliers\n\nThough median imputation would have been fine considerting the large missing values lets go for iterative approach","349dd02f":"very similar to Family_Hist_3","ef17288d":"# XG Boost","d0413258":"There are outliers which needs taken care of","cbaf9efa":"Count is enogh low to be ignored","56118222":"* Value near to zero don't have response more than 5\n* Values more than 8.15 also doesn't have response less than 6\n","eca2da1e":"# Kick-off","ecd0a70f":"## Breaking data for convenience\nThere are too many columns. Lets divided them into logical segments","a0060163":"Employment_Info_1, Employment_Info_4, Employment_Info_6 are continous variable and looks to be highly skewed. Lets check the skewness level","c020aa85":"* These discrete categories si not suitable for encoding, however we have to normalize them as the upper range is high.\n* This kind of data very prone to outliers lets check the boxplot","26c0de42":"### Medical History","d93b7726":"* Employee Info 4 does not have much correlation with other columns\n* Missing values are spread accross the dataset\n* The rows with null Employee_Info_4 have 95% completeness score for other columns, hence there is no point of dropping this rows\n* Data is very much left skewed, hence it is not a good candidate for mean imputation as it may change the distribution of the data.\n* One of the possible way to imput with 0 and add missing indicator column\n","829b29c3":"Data are too much concentrated near zero. This is definatley candidate for outlier and skewness removal","e51716d4":"Most of the insured info are skewed","8ef350e1":"### Family History","d6c10a08":"# Exploratory Data Analysis","f98012ab":"Employment info 2 have large number of categories. Lets see employment info relation with response","1a6b9097":"## Non Zero columns\nWe don't expect Ins_Age, Ht, Wt, BMI to have zeros, lets find out how many","a806b94f":"Only Product_Info_4 is continous","34157832":"## Employment_Info_4   - Continous    \nMissing count: 6779","06519089":"As shown above specially for Medical History 1 outlier removal is important.","1bbf1f94":"* Employment Info 2 values between 20-25 does not favour response 2,3, 4 and 5.\n* Other two variable does not have much impact on the response","7fbf5282":"This is a temporary imputation just to enable feature selection. We will do the actual missing value treatment later.\n* Since oneway annova deals with mean, imputation with mean is not going to change the mean, hence outcome will not be affected by imputation. ","b61bad5d":"Family history 1 also have very skewed data. Spcially we have very lower percentage of data of Family History 1 for response 3 & 4. This may be because of data collection restriction for response 3 & 4 . Prudential definitely need to look into collecting Family History 1 for response 3 & 4.","23fe0d23":"Lets drop these columns because\n* 90% rows have all these 4 columns empty\n* These four columns have good correleation with other columns, hence we will get some information still keeping even after we drop them\n ","e1437b3e":"# Utility Functions","39c2f7b4":"### Product info","9a1441e9":"Discrete columns","ecfa53e3":"# Feature Selection - One way annova","a7ebcdda":"## Insurance_History_5 - Continous","c8be0ed2":"* Again the date are very skewed\n* The bar for response 8 is taller in every chart","8a7f34ad":"## Family_Hist_2 - Continous","f1cc3941":"## Family Hist 4 -  Continous","91fac1b2":"#### Employment Info","40c9fe7a":"Only Family History 1 is categorical","3cdeb878":"Wow, first time we have a columns which resembles normal distribution","bfd87399":"## Employment_Info_6 - Continous","86c9bcd8":"Insured Info 2,3 & 7 have similar distribution also have some relation with response. Lower category value tends to give higher response.","98417562":"There are outliers in Insurance_History_5 , lets try to remove them and see","f6046f67":"## Correlation","8115fed1":"All insured info seems very skewed, lets check their skewness","8a1fe681":"#### Insured Info","5ed4a51f":"Too many columns, lets them separate out by two parts\n\n* Basic Info - All columns except product, medical history & medical keywords\n* Product Info\n* Medical History\n* Medical keywords","0dd3ccba":"* Again there is low correlation with other columns\n* Data is skewed hence mean imputation is not advisable\n* Missing count is large hence can't imput with 0\n* Consider this as candiate for iterative imputer","c7cbea1c":"Columns with missing values - Test Data","2de117e5":"## Family_Hist_3 - Continous","f104a960":"#### BMI\nAs expected BMI is highly correlated with Weight. But surpisingly height is not. That means BMI may be not rightly calculated. One reason can be for different person BMI is calculated using different unit of height. \nConsidering above it is worth droping BMI from features.","a267250b":"### Employment_Info_1 - Continuous","b01abd57":"## Medical_History_1 - Discrete\nMissing counts: 8889","38bace97":"# Missing Values","4c33b23e":"# Outlier detection","8903c98d":"Well, that make most of the columns out of scope","10f24e16":"Looks like chosing number of features to select to that score makes sense because\n* The selected columns covers all the groups... \n<br>age, hight, weight, employment info, insured info, insured history, family history, medical history and product info\n* At least 2 columns selected from each group"}}