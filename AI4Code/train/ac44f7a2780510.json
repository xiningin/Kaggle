{"cell_type":{"8333fa2b":"code","ee8752d5":"code","279e39d3":"code","f49b2021":"code","3e322f0c":"code","8dff2613":"code","bcfc50ac":"code","912fc41b":"code","3897ccf1":"code","6d8c88fb":"code","765ff63c":"code","7eff7113":"code","83da7d23":"code","40807468":"code","a08a3ed6":"code","dd2d05d8":"code","9f765409":"code","24abc103":"code","34520939":"code","ef56ceda":"code","b3d41eeb":"code","6fc9f4f1":"code","971fe50b":"code","a08febdc":"code","63aeb714":"code","caa8d4cc":"code","c1725c64":"code","f0182d2a":"code","adc49fd9":"code","dff9db11":"code","0fee8995":"code","2d452181":"code","44c58019":"code","4e6b83fd":"code","517db73c":"code","53895c9b":"code","b27e1689":"code","3a159a9a":"code","e57aad54":"code","eff010bc":"code","7a6cacb6":"code","b38f7831":"code","96a290d2":"code","9864a86f":"code","3bb47350":"code","99835d12":"code","0a7359be":"code","bf558c8c":"code","b0cb45f1":"code","951ffb68":"code","ebc9b9e6":"code","7d1bc919":"code","4f106818":"code","dba9a2d0":"code","d62b8212":"code","373a445b":"code","ce47685b":"code","b8e3a5ef":"code","17ad0c84":"code","a769a6f4":"code","ae97ed3d":"code","a99bea1d":"code","ca59f6ee":"markdown","efb00447":"markdown","c7419741":"markdown"},"source":{"8333fa2b":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport feather\nimport os\nimport gc\nimport multiprocessing\nfrom tqdm import tqdm\nfrom numba import jit\nfrom keras import Model, Sequential\nfrom keras.layers import Dense, Flatten, BatchNormalization, Dropout, Activation\nfrom keras.layers import Conv1D, SeparableConv1D, MaxPooling1D, GlobalAveragePooling1D\nfrom keras.layers import Input,Concatenate,Reshape,CuDNNLSTM,CuDNNGRU,GlobalMaxPooling1D\nfrom keras.layers import PReLU, LeakyReLU\nfrom keras.optimizers import adam, rmsprop\nfrom keras.regularizers import l1,l2, l1_l2\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import load_model\nfrom scipy.stats import *\nfrom sklearn.metrics import mean_absolute_error\n\n#from scipy import rfft\nfrom numpy.fft import *\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom numpy.random import seed\nseed(1337)\nfrom tensorflow import set_random_seed\nset_random_seed(1337)","ee8752d5":"print(os.listdir('..\/input\/LANL-Earthquake-Prediction\/'))","279e39d3":"%%time\ntrain = feather.read_dataframe('..\/input\/lanl-ft\/train.ft')\n# zero center \ntrain['acoustic_data'] = train['acoustic_data'].values - 4","f49b2021":"# to plot training\/validation history object\ndef plt_dynamic(x, vy, ty, ax, colors=['b'], title=''):\n    ax.plot(x, vy, 'b', label='Validation Loss')\n    ax.plot(x, ty, 'r', label='Train Loss')\n    plt.legend()\n    plt.grid()\n    plt.title(title)\n    fig.canvas.draw()\n    plt.show()","3e322f0c":"# get the earthquake indices. this is where the experiement resets\ndiff = np.diff(train['time_to_failure'].values)\nend = np.nonzero(diff>0)[0]\nstart = end + 1\nstart = np.insert(start, 0, 0)\ndel diff\ngc.collect()\nstart","8dff2613":"def gen_batches_front(col, interval=150_000):\n    high = []\n    low = []\n    splits =[]\n    high_ttf = list(range(9))\n    \n    for i, beg in enumerate(start):\n        counter = 0\n        if beg != 621_985_673:\n            last = start[i+1]\n        else:\n            last = len(train)\n        last = (last-beg)\/\/150_000 * 150_000 + beg\n        \n        for x in range(beg, last, interval):\n            if col == 'acoustic_data':\n                if i in high_ttf:\n                    high.append(train[col].iloc[x:150_000+x].values)\n                else:\n                    low.append(train[col].iloc[x:150_000+x].values)\n            else:\n                if i in high_ttf:\n                    high.append(train[col].iloc[x:150_000+x].values[-1])\n                else:\n                    low.append(train[col].iloc[x:150_000+x].values[-1])\n                    \n        # oversample the end points\n        sample = 15000\n        seg = 150000\n        for z, y in enumerate(range(0, sample*11, sample)):\n            if col == 'acoustic_data':\n                if i in high_ttf:\n                    high.append(train[col].iloc[last-seg*(z+1):last-seg*z].values)\n                else:\n                    low.append(train[col].iloc[last-seg*(z+1):last-seg*z].values)\n            else:\n                if i in high_ttf:\n                    high.append(train[col].iloc[last-seg*(z+1):last-seg*z].values[-1])\n                else:\n                    low.append(train[col].iloc[last-seg*(z+1):last-seg*z].values[-1])\n    return np.asarray(high), np.asarray(low)","bcfc50ac":"# can modify the interval to a factor of 150000 for increased sampling\ndef preprocess_front():\n    xtrain, xtest = gen_batches_front('acoustic_data', interval=150000)\n    ytrain, ytest = gen_batches_front('time_to_failure', interval=150000)\n    xtrain = xtrain.reshape(-1, 150000, 1)\n    xtest = xtest.reshape(-1, 150000, 1)\n    print(xtrain.shape)\n    print(xtest.shape)\n    print(ytrain.shape)\n    print(ytest.shape)\n    return xtrain, xtest, ytrain, ytest","912fc41b":"xtrain, xtest, ytrain, ytest = preprocess_front()","3897ccf1":"gc.collect()\ndel train","6d8c88fb":"checkpoint1 = ModelCheckpoint('best1.hdf5', verbose=0, save_best_only=True, mode='min')","765ff63c":"%%time\nepochs=20\nmdl = Sequential()\nmdl.add(SeparableConv1D(32, 8, activation='relu', input_shape=(xtrain.shape[1],1)))\nmdl.add(CuDNNGRU(32, return_sequences=True))\nmdl.add(GlobalAveragePooling1D())\nmdl.add(Dense(32, activation='relu'))\nmdl.add(Dense(1))\nmdl.compile(loss='mae', optimizer=adam(lr=0.001))\nmdl.summary()","7eff7113":"#visualize network architecture\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\nSVG(model_to_dot(mdl).create(prog='dot', format='svg'))","83da7d23":"history = mdl.fit(xtrain, ytrain, epochs=epochs, batch_size=64, verbose=2, \n                  validation_data=[xtest,ytest], callbacks=[checkpoint1])\nfig, ax = plt.subplots(1,1)\nvy = history.history['val_loss']\nty = history.history['loss']\nax.set_xlabel('Epoch')\nx = list(range(1,epochs+1))\nax.set_ylabel('Mean Absolute Error')\nplt_dynamic(x,vy,ty,ax, title='history')","40807468":"x_train = np.concatenate([xtrain, xtest], axis=0)\ny_train = np.concatenate([ytrain, ytest], axis=0)\noof_pred = np.zeros(len(x_train))\nbest_oof = np.zeros(len(x_train))","a08a3ed6":"%%time\ntrain_pred = mdl.predict(xtrain)\nvalid_pred = mdl.predict(xtest)\noof_pred[xtrain.shape[0]:] += valid_pred[:,0]\nprint(f'Train MAE: {mean_absolute_error(ytrain, train_pred):.4f}')\nprint(f'Valid MAE: {mean_absolute_error(ytest, valid_pred):.4f}')","dd2d05d8":"plt.title('predicted train')\nsns.distplot(train_pred)","9f765409":"plt.title('predicted test')\nsns.distplot(valid_pred)","24abc103":"#compute absolute error\ntrain_error = np.abs(np.subtract(train_pred.reshape(1,-1)[0], ytrain))\nvalid_error = np.abs(np.subtract(valid_pred.reshape(1,-1)[0], ytest))","34520939":"#plot the train error distribution\nplt.title('train error')\nplt.xlabel('absolute error')\nsns.distplot(train_error)","ef56ceda":"#plot the validation error distribution\nplt.title('validation error')\nplt.xlabel('absolute error')\nsns.distplot(valid_error)","b3d41eeb":"print(f'max ytrain: {np.max(ytrain):.4f}')\nprint(f'max ytest: {np.max(ytest):.4f}')\nprint(f'max p_train: {np.max(train_pred):.4f}')\nprint(f'max p_test: {np.max(valid_pred):.4f}')","6fc9f4f1":"#plot scatter\nplt.figure(figsize=(9,9))\nplt.scatter(ytrain, train_pred)\nplt.xlabel('ytrue')\nplt.ylabel('ypred')\nplt.show()","971fe50b":"#plot scatter\nplt.figure(figsize=(9,9))\nplt.scatter(ytest, valid_pred)\nplt.title('oof predictions')\nplt.xlabel('ytrue')\nplt.ylabel('ypred')\nplt.show()","a08febdc":"%%time\nbest1 = load_model('best1.hdf5')\ntrain_pred = best1.predict(xtrain)\nvalid_pred = best1.predict(xtest)\nbest_oof[xtrain.shape[0]:] += valid_pred[:,0]\nprint(f'Train MAE: {mean_absolute_error(ytrain, train_pred):.4f}')\nprint(f'Valid MAE: {mean_absolute_error(ytest, valid_pred):.4f}')","63aeb714":"plt.title('predicted train')\nsns.distplot(train_pred)","caa8d4cc":"plt.title('predicted test')\nsns.distplot(valid_pred)","c1725c64":"#compute absolute error\ntrain_error = np.abs(np.subtract(train_pred.reshape(1,-1)[0], ytrain))\nvalid_error = np.abs(np.subtract(valid_pred.reshape(1,-1)[0], ytest))","f0182d2a":"#plot the train error distribution\nplt.title('train error')\nplt.xlabel('absolute error')\nsns.distplot(train_error)","adc49fd9":"#plot the validation error distribution\nplt.title('validation error')\nplt.xlabel('absolute error')\nsns.distplot(valid_error)","dff9db11":"print(f'max ytrain: {np.max(ytrain):.4f}')\nprint(f'max ytest: {np.max(ytest):.4f}')\nprint(f'max p_train: {np.max(train_pred):.4f}')\nprint(f'max p_test: {np.max(valid_pred):.4f}')","0fee8995":"del train_pred, valid_pred, train_error, valid_error\ngc.collect()","2d452181":"checkpoint2 = ModelCheckpoint('best2.hdf5', verbose=0, save_best_only=True, mode='min')","44c58019":"%%time\nepochs=20\nmdl2 = Sequential()\nmdl2.add(SeparableConv1D(32, 8, activation='relu', input_shape=(xtrain.shape[1],1)))\nmdl2.add(CuDNNGRU(32, return_sequences=True))\nmdl2.add(GlobalAveragePooling1D())\nmdl2.add(Dense(32, activation='relu'))\nmdl2.add(Dense(1))\nmdl2.compile(loss='mae', optimizer=adam(lr=0.001))\nhistory = mdl2.fit(xtest, ytest, epochs=epochs, batch_size=64, verbose=2, \n                  validation_data=[xtrain, ytrain], callbacks=[checkpoint2])\nfig, ax = plt.subplots(1,1)\nvy = history.history['val_loss'][4:]\nty = history.history['loss'][4:]\nax.set_xlabel('Epoch')\nx = list(range(5,epochs+1))\nax.set_ylabel('Mean Absolute Error')\nplt_dynamic(x,vy,ty,ax, title='history')","4e6b83fd":"%%time\ntrain_pred = mdl2.predict(xtrain)\nvalid_pred = mdl2.predict(xtest)\noof_pred[:xtrain.shape[0]] += train_pred[:,0]\nprint(f'Train MAE: {mean_absolute_error(ytrain, train_pred):.4f}')\nprint(f'Valid MAE: {mean_absolute_error(ytest, valid_pred):.4f}')","517db73c":"#plot the train distibution\nplt.title('predicted train')\nsns.distplot(train_pred)","53895c9b":"#plot the test distribution\nplt.title('predicted test')\nsns.distplot(valid_pred)","b27e1689":"#compute absolute error\ntrain_error = np.abs(np.subtract(train_pred.reshape(1,-1)[0], ytrain))\nvalid_error = np.abs(np.subtract(valid_pred.reshape(1,-1)[0], ytest))","3a159a9a":"#plot the train error distribution\nplt.title('train error')\nplt.xlabel('absolute error')\nsns.distplot(train_error)","e57aad54":"#plot the test error distribution\nplt.title('validation error')\nplt.xlabel('absolute error')\nsns.distplot(valid_error)","eff010bc":"print(f'max ytrain: {np.max(ytrain):.4f}')\nprint(f'max ytest: {np.max(ytest):.4f}')\nprint(f'max p_train: {np.max(train_pred):.4f}')\nprint(f'max p_test: {np.max(valid_pred):.4f}')","7a6cacb6":"%%time\nbest2 = load_model('best2.hdf5')\ntrain_pred = best2.predict(xtrain)\nvalid_pred = best2.predict(xtest)\nbest_oof[:xtrain.shape[0]] += train_pred[:,0]\nprint(f'Train MAE: {mean_absolute_error(ytrain, train_pred):.4f}')\nprint(f'Valid MAE: {mean_absolute_error(ytest, valid_pred):.4f}')","b38f7831":"plt.title('predicted train')\nsns.distplot(train_pred)","96a290d2":"plt.title('predicted test')\nsns.distplot(valid_pred)","9864a86f":"#plot scatter - visualize actual vs prediction\nplt.figure(figsize=(9,9))\nplt.scatter(ytrain, train_pred)\nplt.title('oof predictions')\nplt.xlabel('ytrue')\nplt.ylabel('ypred')\nplt.show()","3bb47350":"#plot scatter - visualize actual vs prediction\nplt.figure(figsize=(9,9))\nplt.scatter(ytest, valid_pred)\nplt.xlabel('ytrue')\nplt.ylabel('ypred')\nplt.show()","99835d12":"#compute absolute error\ntrain_error = np.abs(np.subtract(train_pred.reshape(1,-1)[0], ytrain))\nvalid_error = np.abs(np.subtract(valid_pred.reshape(1,-1)[0], ytest))","0a7359be":"#plot the train error distribution\nplt.title('train error')\nplt.xlabel('absolute error')\nsns.distplot(train_error)","bf558c8c":"#plot the validation error distribution\nplt.title('validation error')\nplt.xlabel('absolute error')\nsns.distplot(valid_error)","b0cb45f1":"print(f'max ytrain: {np.max(ytrain):.4f}')\nprint(f'max ytest: {np.max(ytest):.4f}')\nprint(f'max p_train: {np.max(train_pred):.4f}')\nprint(f'max p_test: {np.max(valid_pred):.4f}')","951ffb68":"del train_pred, valid_pred, train_error, valid_error\ngc.collect()","ebc9b9e6":"print('overtrain mae: {:.4f}'.format(mean_absolute_error(y_train, oof_pred)))\nprint('best mae: {:.4f}'.format(mean_absolute_error(y_train, best_oof)))","7d1bc919":"plt.title('oof_pred')\nsns.distplot(oof_pred)\nplt.show()","4f106818":"plt.title('best_oof')\nsns.distplot(best_oof)\nplt.show()","dba9a2d0":"plt.figure(figsize=(9,9))\nplt.scatter(y_train, oof_pred)\nplt.title('oof_pred')\nplt.xlabel('ytrue')\nplt.ylabel('ypred')\nplt.show()","d62b8212":"plt.figure(figsize=(9,9))\nplt.scatter(y_train, best_oof)\nplt.title('best_oof')\nplt.xlabel('ytrue')\nplt.ylabel('ypred')\nplt.show()","373a445b":"%%time\nsub = pd.read_csv('..\/input\/LANL-Earthquake-Prediction\/sample_submission.csv', \n                  dtype={'seg_id': 'category', 'time_to_failure':np.float32})\n\ntest_data = []\nfor fname in sub['seg_id'].values:\n    test_data.append(pd.read_csv('..\/input\/LANL-Earthquake-Prediction\/test\/'+fname+'.csv', \n                                 dtype={'acoustic_data':np.int16})['acoustic_data'].values)\n# zero center & reshape\ntest_data = np.asarray(test_data) - 4\ntest_data = test_data.reshape(-1, 150_000, 1)","ce47685b":"#%time \npred1 = best1.predict(test_data)\nsub['time_to_failure'] = pred1\nsub.to_csv('submission1.csv', index=False)\nsub.head()","b8e3a5ef":"plt.title('first half')\nsns.distplot(sub['time_to_failure'].values)","17ad0c84":"%time pred2 = best2.predict(test_data)\nsub['time_to_failure'] = pred2\nsub.to_csv('submission2.csv', index=False)\nsub.head()","a769a6f4":"plt.title('second half')\nsns.distplot(sub['time_to_failure'].values)","ae97ed3d":"# blended first + second half\nfirst = pd.read_csv('submission1.csv')\nsecond = pd.read_csv('submission2.csv')\nblend = first.copy()\nblend['time_to_failure'] = (blend['time_to_failure'] + second['time_to_failure'])\/2\nblend.to_csv('frontback.csv', index=False)\nblend.head()","a99bea1d":"plt.title('blended')\nsns.distplot(blend['time_to_failure'].values)","ca59f6ee":"**Basic Information**\n\nIn this notebook, I build a Neural Network architecture to predict TTF.\n\nThe goal of this competition is to use seismic signals to predict the timing of laboratory earthquakes. The data comes from a well-known experimental set-up used to study earthquake physics. The acoustic_data input signal is used to predict the time remaining before the next laboratory earthquake (time_to_failure).\n\nThe training data is a single, continuous segment of experimental data. The test data consists of a folder containing many small segments. The data within each test file is continuous, but the test files do not represent a continuous segment of the experiment; thus, the predictions cannot be assumed to follow the same regular pattern seen in the training file.\n\nFor each seg_id in the test folder, you should predict a single time_to_failure corresponding to the time between the last row of the segment and the next laboratory earthquake.\n\n* train length: 629,145,480\n* Max time_to_failure = 16.1074\n* Min time_to_failure = 9.5503965e-05\n* test length: 2624 * 150,000 = 393,600,000\n<br>\n","efb00447":"# Submission File\n","c7419741":"# OOF CV Results"}}