{"cell_type":{"9222b5c9":"code","e0ec3bed":"code","112a7591":"code","1a359fd2":"code","66bdb6b8":"code","ffe365e4":"code","36a59c14":"code","117b1b2a":"code","42609a2a":"code","ec5174fe":"code","4d8654d9":"code","4d5ada35":"code","c0926878":"code","1f549db1":"code","0f16efa6":"markdown","442ae77e":"markdown","51137ed5":"markdown","52fd084f":"markdown","30788118":"markdown"},"source":{"9222b5c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nfrom numpy import array\nimport torch\nimport gc\nimport torch.nn as nn\nfrom tqdm import tqdm_notebook as tqdm\nfrom torch.utils.data import Dataset,DataLoader","e0ec3bed":"solar_power = pd.read_csv('\/kaggle\/input\/solarpanelspower\/PV_Elec_Gas2.csv').rename(columns={'Unnamed: 0':'timestamp'}).set_index('timestamp')","112a7591":"train_set = solar_power[:'2018-10-31']\nvalid_set = solar_power['2018-11-01':'2019-11-18']\nprint('Proportion of train_set : {:.2f}%'.format(len(train_set)\/len(solar_power)))\nprint('Proportion of valid_set : {:.2f}%'.format(len(valid_set)\/len(solar_power)))","1a359fd2":"def split_sequence(sequence, n_steps):\n    x, y = list(), list()\n    for i in range(len(sequence)):\n        \n        end_ix = i + n_steps\n        \n        if end_ix > len(sequence)-1:\n            break\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n        x.append(seq_x)\n        y.append(seq_y)\n    return array(x), array(y)\n\nraw_seq = [10,20,30,40,50,60,70,80,90]\nn_steps = 3\ntrain_x,train_y = split_sequence(train_set.Elec_kW.values,n_steps)\nvalid_x,valid_y = split_sequence(valid_set.Elec_kW.values,n_steps)","66bdb6b8":"class ElecDataset(Dataset):\n    def __init__(self,feature,target):\n        self.feature = feature\n        self.target = target\n    \n    def __len__(self):\n        return len(self.feature)\n    \n    def __getitem__(self,idx):\n        item = self.feature[idx]\n        label = self.target[idx]\n        \n        return item,label","ffe365e4":"class CNN_ForecastNet(nn.Module):\n    def __init__(self):\n        super(CNN_ForecastNet,self).__init__()\n        self.conv1d = nn.Conv1d(3,64,kernel_size=1)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc1 = nn.Linear(64*2,50)\n        self.fc2 = nn.Linear(50,1)\n        \n    def forward(self,x):\n        x = self.conv1d(x)\n        x = self.relu(x)\n        x = x.view(-1)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        \n        return x","36a59c14":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = CNN_ForecastNet().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\ncriterion = nn.MSELoss()","117b1b2a":"train = ElecDataset(train_x.reshape(train_x.shape[0],train_x.shape[1],1),train_y)\nvalid = ElecDataset(valid_x.reshape(valid_x.shape[0],valid_x.shape[1],1),valid_y)\ntrain_loader = torch.utils.data.DataLoader(train,batch_size=2,shuffle=False)\nvalid_loader = torch.utils.data.DataLoader(train,batch_size=2,shuffle=False)","42609a2a":"train_losses = []\nvalid_losses = []\ndef Train():\n    \n    running_loss = .0\n    \n    model.train()\n    \n    for idx, (inputs,labels) in enumerate(train_loader):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        preds = model(inputs.float())\n        loss = criterion(preds,labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss\n        \n    train_loss = running_loss\/len(train_loader)\n    train_losses.append(train_loss.detach().numpy())\n    \n    print(f'train_loss {train_loss}')\n    \ndef Valid():\n    running_loss = .0\n    \n    model.eval()\n    \n    with torch.no_grad():\n        for idx, (inputs, labels) in enumerate(valid_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            preds = model(inputs.float())\n            loss = criterion(preds,labels)\n            running_loss += loss\n            \n        valid_loss = running_loss\/len(valid_loader)\n        valid_losses.append(valid_loss.detach().numpy())\n        print(f'valid_loss {valid_loss}')","ec5174fe":"epochs = 200\nfor epoch in range(epochs):\n    print('epochs {}\/{}'.format(epoch+1,epochs))\n    Train()\n    Valid()\n    gc.collect()","4d8654d9":"import matplotlib.pyplot as plt\nplt.plot(train_losses,label='train_loss')\nplt.plot(valid_losses,label='valid_loss')\nplt.title('MSE Loss')\nplt.ylim(0, 100)\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)","4d5ada35":"target_x , target_y = split_sequence(train_set.Elec_kW.values,n_steps)\ninputs = target_x.reshape(target_x.shape[0],target_x.shape[1],1)","c0926878":"model.eval()\nprediction = []\nbatch_size = 2\niterations =  int(inputs.shape[0]\/2)\n\nfor i in range(iterations):\n    preds = model(torch.tensor(inputs[batch_size*i:batch_size*(i+1)]).float())\n    prediction.append(preds.detach().numpy())","1f549db1":"fig, ax = plt.subplots(1, 2,figsize=(11,4))\nax[0].set_title('predicted one')\nax[0].plot(prediction)\nax[1].set_title('real one')\nax[1].plot(target_y)\nplt.show()","0f16efa6":"#### Train & Valid split(Almost 8.5:1.5)","442ae77e":"Reference : https:\/\/machinelearningmastery.com\/how-to-develop-convolutional-neural-network-models-for-time-series-forecasting\/","51137ed5":"#### Do I have to think autocorrelation when setting batch_size? \nMaybe Batch_size is important, and It is related with autocorrelation I guess.","52fd084f":"### Build CNN Forecast Model","30788118":"#### Prediction Result"}}