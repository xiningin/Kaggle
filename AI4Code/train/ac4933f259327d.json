{"cell_type":{"59471a4d":"code","3866564e":"code","70d901ee":"code","ab42642d":"code","14b525ad":"code","c7fa2cf3":"code","133c61b3":"code","1631ca09":"code","380720c2":"code","eeb6cf77":"code","574a95a8":"code","4059675d":"code","8cc6fb3e":"code","19b683b4":"code","2a2edbce":"code","5e78e8c9":"code","9647b51f":"code","3131f001":"code","f6ca32df":"code","8e91fafb":"code","e0fafd61":"code","0d955914":"code","d5224b1e":"code","e850ef38":"code","23d90180":"code","322b67f1":"code","2aabcfbb":"code","ca417b99":"code","ecab89ef":"code","ba6e02cf":"code","09fa7e61":"code","9f66e4b6":"code","00e2a2f9":"markdown","dbe1ae9b":"markdown","e5c1fd5a":"markdown","8424e42e":"markdown","b04bce60":"markdown","48bded2f":"markdown","e0979093":"markdown","0f2dcdc0":"markdown","64c133f3":"markdown","2cc3de67":"markdown","ffbda5e6":"markdown","8f958832":"markdown","c2804216":"markdown","8630ded5":"markdown","268c0751":"markdown"},"source":{"59471a4d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3866564e":"#import packages\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#packages for encoding categorical values\n\n\nfrom sklearn.preprocessing import LabelEncoder\n\n# packages for imputing missing values\nimport missingno as msno\n\n# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\n\n\n","70d901ee":"# importing the datasets\n\ntrain = pd.read_csv('..\/input\/titanic\/train.csv') \ntest= pd.read_csv('..\/input\/titanic\/test.csv')\ntarget = train[\"Survived\"].values\nfull_df = pd.concat([train, test])\n\n","ab42642d":"#check head of Training dataset\nfull_df.info()\n","14b525ad":"#categorical and numerical values \ncategory = full_df.select_dtypes(include=['object']).columns.tolist()\nnumerical = full_df.select_dtypes(exclude=['object']).columns.tolist()\n\n\nprint(f'Categorical columns: {category}\\n')\nprint(f'columns with numerical values: {numerical}')","c7fa2cf3":"# It looks cool,but I can only see columns with numerical values\nfull_df.describe()","133c61b3":"# let's check few rows of dataset\nfull_df.head()\n","1631ca09":"# look at the correlatios beween survived and Pclass\n\ng= sns.barplot(x='Pclass', y='Survived', data=full_df)","380720c2":"# Check wich sex had more change of survival\nsns.barplot(x='Sex', y='Survived', data=full_df)","eeb6cf77":"# Age distributions on survival \ng = sns.FacetGrid(full_df, col = 'Survived')\ng.map(sns.distplot, 'Age')","574a95a8":"# names of categorical columns: ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']\n\n# name colomns do not contain categorical values, so I'm going to use the Formal Titles to create new catergorical values for names.\nname = full_df[\"Name\"]\nfull_df[\"Title\"] = [i.split(\".\")[0].split(\",\")[-1].strip() for i in name]\nfull_df.Title.value_counts()\n","4059675d":"# Use most counted titles ['Mr', 'Mrs', 'Miss', 'Master',] and the titles with less values are replaced with \"other\".\nfull_df[\"Title\"] = full_df[\"Title\"].replace([\"Lady\",\"the Countess\",\"Capt\",\"Col\",\"Don\",\"Dr\",\"Major\",\"Rev\",\"Sir\",\"Jonkheer\",\"Dona\",'Mme', 'Ms', 'Mlle'],\"other\")\nfull_df.Title.unique()\n","8cc6fb3e":"#Encoding Embarked column. This columsn has 2 missing valeus,so I must be sure that the missing values are not encoded. \n\nLABEL_COL = [\"Cabin\",\"Sex\",\"Embarked\", \"Title\"]\n\ndef label(df):\n    le = LabelEncoder()\n    for col in LABEL_COL:\n        # Not NaN index\n        idx = ~df[col].isna()\n        df.loc[idx, col] \\\n            = le.fit(df.loc[idx, col]).transform(df.loc[idx, col])\n    return df\n\n# encoding full_df\n\nfull_df = label(full_df)\n\nfull_df.head()\n#encode Testset \n\n","19b683b4":"# look at the missing values column \nmissing_values = full_df[full_df.columns[full_df.isnull().any()]].isnull().sum() \nmissing_values_percent = full_df[full_df.columns[full_df.isnull().any()]].isnull().sum() * 100 \/ train.shape[0]\nprint(f'missing_values: \\n\\n {missing_values}')\nprint('------------------------------------------')\nprint(f'missing_values percentages: \\n\\n {missing_values_percent}')\n\n","2a2edbce":"# using missingno find out the correlation between the missing values\nsorted_age= full_df.sort_values('Age')\nmsno.matrix(sorted_age)","5e78e8c9":"# filling the missing Embarked column. There are only two values are missing, so I'm going to fill these with most frequet value of Embarked column\n\ndef fill_frequent(df, column):\n    result = df[column].fillna(df[column].mode().values[0], inplace = True)\n    return (f'numbers of missing values in {column} column is: {result}')\n\n#fill the full_df \nfill_frequent(full_df, 'Embarked')\n#fill the Full_df\nfill_frequent(full_df, 'Fare')\n","9647b51f":"#fill Age and using Interplation\n\nfull_df['Age'] = full_df['Age'].interpolate()\n\n# create Age categories columsn \n\nbins = [0,14,25,50,80]\nlabels=[0,1,2,3]\nfull_df['Age_bins'] = pd.cut(full_df['Age'], bins=bins, labels=labels, include_lowest=True)\n       \n#fill Cabin column with mos common value\nfull_df['Cabin'] =full_df['Cabin'].fillna(full_df['Cabin'].mode()[0])\n","3131f001":"#drop Name and Ticket columns \n\nfull_df = full_df.drop(['Name', 'Ticket', 'Age'], axis= 1)\n","f6ca32df":"# Split full_df into train en test set\n\ntrain = full_df[0:891].copy()\ntest = full_df[891:].copy()\n\nX_train = train.drop([\"Survived\",\"PassengerId\"], axis=1)\nY_train = train[\"Survived\"]\nX_test  = test.drop([\"Survived\",\"PassengerId\"], axis=1).copy()","8e91fafb":"# scaling the datasets\nfrom sklearn.preprocessing import MinMaxScaler\n\nmin_max_scaler = MinMaxScaler()\nX_train= min_max_scaler.fit_transform(X_train)\nX_test= min_max_scaler.fit_transform(X_test)\n\n","e0fafd61":"# Stochastic Gradient Descent (SGD):\n\nsgd = linear_model.SGDClassifier(max_iter=5, tol=None)\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\n\nsgd.score(X_train, Y_train)\n\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)","0d955914":"#Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)","d5224b1e":"# Logistic Regression:\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\nY_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)","e850ef38":"# K Nearest Neighbor\nknn = KNeighborsClassifier(n_neighbors = 3) \nknn.fit(X_train, Y_train)  \nY_pred = knn.predict(X_test)  \nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)","23d90180":"#Gaussian Naive Bayes\n\ngaussian = GaussianNB() \ngaussian.fit(X_train, Y_train)  \nY_pred = gaussian.predict(X_test)  \nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)","322b67f1":"#Linear Support Vector Machine:\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\n\nY_pred = linear_svc.predict(X_test)\n\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)","2aabcfbb":"#Decision Tree\ndecision_tree = DecisionTreeClassifier() \ndecision_tree.fit(X_train, Y_train)  \nY_pred = decision_tree.predict(X_test)  \nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)","ca417b99":"#Perceptron\nperceptron = Perceptron(max_iter=5)\nperceptron.fit(X_train, Y_train)\n\nY_pred = perceptron.predict(X_test)\n\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)","ecab89ef":"# compare models\nresults = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', \n              'Decision Tree'],\n    'Score': [acc_linear_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_decision_tree]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head(9)","ba6e02cf":"# K_Fold Cross validation\n\nfrom sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(rf, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","09fa7e61":"#final submission \n\nsubmission = pd.DataFrame({'PassengerId':test['PassengerId'],'Survived':Y_prediction}) # Y_prediction is defined in #Random Forest\n\n#Visualize the first 5 rows\nsubmission.head()","9f66e4b6":"filename = 'Titanic_Predictions.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","00e2a2f9":"# Age and Surived \n\n","dbe1ae9b":"Passengers from Pclass-1 had a higher change of survival. ","e5c1fd5a":"# Survived and Sex ","8424e42e":"# Filling the Embaked and Fare column","b04bce60":"# Missing values imputation \n","48bded2f":"# Building ML Models\n\nSource of ML codes:\nhttps:\/\/towardsdatascience.com\/predicting-the-survival-of-titanic-passengers-30870ccc7e8\n","e0979093":"There is no direcht correlation between the columns with missing values. This can be seen on Age column. As you can see in code, I sorted the Age column, so the white empty space is represent the missing values in Age column. If there any relation between Age, Cabin,and Embarked columns, you can see that other columns look like Age column.  ","0f2dcdc0":"# Exploring Dataset\n","64c133f3":"The missing values on Survived columns is from testset, so it does not need to be imputed. ","2cc3de67":"# Dealing with categorical values ","ffbda5e6":"More female passengers survived. ","8f958832":"Someone between 18 and 40-year-old had a higher chance of survival. ","c2804216":"# Correlation between features and target value\n","8630ded5":"Random Forest has highest score. ","268c0751":"#  Pclass and Survived "}}