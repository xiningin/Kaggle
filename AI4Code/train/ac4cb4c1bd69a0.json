{"cell_type":{"1af99891":"code","27871e64":"code","ded89dc4":"code","190be72a":"code","e86caf4f":"code","b21ed948":"code","009091e8":"code","5f50ffca":"code","1594b37d":"code","7321569f":"code","64fc08ef":"code","fa2e7263":"code","a3daaba0":"code","8c3333bb":"code","f54ec456":"code","5bdf9f92":"code","f2ad8f86":"code","b4596733":"code","fb2cb127":"code","43a75c73":"code","ebe39bbd":"code","8507bcb9":"code","6af28a57":"code","e9b927dc":"code","55235a23":"code","efbbbd14":"code","f5d83700":"code","14eef5ce":"code","61de0cb6":"code","0ed36f9f":"code","d2cbc5ee":"code","d85656c3":"code","c2e02c10":"code","e9dfe2fa":"code","84cd2b06":"code","117738f0":"code","4144c270":"code","c07d2946":"code","d1218d2f":"code","b27ec3fd":"code","19984410":"code","48f3d2e1":"code","81da8fd5":"code","06af9601":"code","68b96a92":"code","ad6af257":"code","5d8a118f":"code","45ace9dd":"code","16706d26":"code","16a3f089":"code","8dabd386":"code","c8a6323f":"code","747d3a3e":"code","e96d5295":"code","862597ce":"code","35845a5e":"code","8e541a94":"code","7d5789fe":"code","e0f0a167":"code","deba6c00":"code","3c6424b4":"code","c30aece9":"code","8b0aef5d":"code","b7678259":"code","a7321e76":"code","18823991":"code","397b2cf1":"code","c235e01d":"code","a32d5a0a":"code","cfe6213f":"code","8555cc40":"code","68719076":"code","a88a0428":"code","c056e6db":"code","36cdf09f":"code","f637877c":"code","75848a3a":"code","b816fec5":"code","f08ca5df":"code","bc069cc7":"code","1846535f":"code","8ed985fb":"code","8e526be9":"code","97b28c90":"code","44c4962b":"code","8ee7d93b":"code","a3f0accb":"code","525e89ac":"code","842757c3":"code","000ef331":"code","e952fc43":"code","56299248":"code","5ce48d5f":"code","d7377226":"code","d746940f":"code","276ff0b2":"code","fd936ac1":"code","34e4400a":"code","a37f4409":"code","1ed75b49":"code","f45e0ed3":"code","682e194b":"code","31524f0a":"code","238d74aa":"code","d1ebb3f5":"code","6c08b367":"code","0afc366f":"code","9ffb4233":"code","bbc8cb70":"code","436d1a6e":"code","04efaf62":"code","dc81d1c5":"code","710274e3":"code","03c559f4":"code","94382954":"code","b3bd5ad2":"markdown","62709f15":"markdown","73df8a59":"markdown","83f519f9":"markdown","1e4e05bd":"markdown","66789803":"markdown","c6d2eb81":"markdown","949954bf":"markdown","fbcb982b":"markdown","0a80130c":"markdown","068ec1fd":"markdown","2ab828a8":"markdown","29898f40":"markdown","557a0934":"markdown","e23d8532":"markdown","51adc48a":"markdown","b0291385":"markdown","c870db5a":"markdown","5c117327":"markdown","ad0a55c5":"markdown","55eca6a5":"markdown","3a4efc7a":"markdown","1ca869ff":"markdown","59db9a96":"markdown","71db793f":"markdown","4784d4b0":"markdown","663552d8":"markdown","30515e64":"markdown","46f32b71":"markdown","032846ed":"markdown","331740f4":"markdown","a7e3c5cb":"markdown","c7dea6ba":"markdown","781069a2":"markdown","15b185ad":"markdown","34b083f6":"markdown","40122808":"markdown","86bf02f9":"markdown","62ffd15c":"markdown","113efcf5":"markdown","672cb619":"markdown","b7bc9405":"markdown","7a45208e":"markdown","75b4b939":"markdown","1a9e8737":"markdown","89a01338":"markdown","7198af25":"markdown","477902d6":"markdown","a2aba409":"markdown","7abcb298":"markdown","c3888cac":"markdown","25a11b8e":"markdown","1932779d":"markdown","04bb8be5":"markdown","e58d0c48":"markdown","5658ade0":"markdown","0c58e27a":"markdown","62f94978":"markdown","5635297e":"markdown","5da12727":"markdown","3e673534":"markdown","257bb718":"markdown","4a240bfb":"markdown","1c329090":"markdown","9b502d5f":"markdown","fb8a4b31":"markdown","bbabe606":"markdown","cee8250c":"markdown","19b607cb":"markdown","4f96749e":"markdown","d59f3316":"markdown","88dfc352":"markdown","3b3aa58c":"markdown","09222fcf":"markdown","6989ac12":"markdown","e32c3f50":"markdown","1e0da9db":"markdown"},"source":{"1af99891":"# Standard Tools\nfrom scipy import stats\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport os\n\n# Visualisation Tools\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n# Modelling Tools\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_validate, KFold, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nimport statsmodels as sm\n\n# Misc\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 100)\nplt.style.use('fivethirtyeight')\n\n# Setting the random state\nnp.random.seed(8888)\nSEED = 8888\n\n# # File names \n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","27871e64":"# Loading our data\ndf = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\n\nprint(f'The dataset has {df.shape[0]} rows and {df.shape[1]} columns.')","ded89dc4":"# Getting a feel for how our data looks like\ndf.head()","190be72a":"# Dropping column Id as we will use the dataframe indexing\ndf = df.drop('Id', axis=1)","e86caf4f":"# Quick scan for missing values and wrong datatypes\ndf.info()","b21ed948":"df.describe()","009091e8":"# Helper function to help check for missing data\ndef variable_missing_percentage(df, save_results=False):\n    '''\n    Function that shows variables that have missing values and the percentage of total observations that are missing.\n    \n    Arguments:\n        df : Pandas DataFrame\n        save_results : bool, default is False\n            Set as True to save the Series with the missing percentages.\n    \n    Returns:\n        percentage_missing : Pandas Series\n            Series with variables and their respective missing percentages.\n    '''\n    percentage_missing = df.isnull().mean().sort_values(ascending=False) * 100\n    percentage_missing = percentage_missing.loc[percentage_missing > 0].round(2)\n    missing_variables = len(percentage_missing)\n    \n    if len(percentage_missing) > 0:\n        print(f'There are a total of {missing_variables} variables with missing values. Percentage of total missing:')\n        print()\n        print(percentage_missing)\n    \n    else:\n        print('The dataframe has no missing values in any column.')\n    \n    if save_results:\n        return percentage_missing","5f50ffca":"variable_missing_percentage(df)","1594b37d":"def drop_missing_variables(df, threshold, verbose=True):\n    '''Function that removes variables that have missing percentages above a threshold.\n    \n    Arguments:\n        df : Pandas DataFrame\n        threshold : float\n            Threshold missing percentage value in decimals.\n        verbose : bool, default is True\n            Prints the variables that were removed.\n            \n    Returns:\n        df : Pandas DataFrame with variables removed\n    '''\n    shape_prior = df.shape\n    vars_to_remove = df.columns[df.isnull().mean() > threshold].to_list()\n    df = df.drop(vars_to_remove, axis=1)\n    shape_post = df.shape\n    \n    print(f'The original DataFrame had {shape_prior[1]} variables.')\n    print(f'The returned DataFrame has {shape_post[1]} variables.')\n    \n    if verbose:\n        print()\n        print('The following variables were removed:')\n        print(vars_to_remove)\n        \n    return df","7321569f":"df = drop_missing_variables(df, 0.8)","64fc08ef":"# Dropping features that are related to the ones we just removed\ndf = df.drop(['MiscVal', 'PoolArea'], axis=1)","fa2e7263":"df.FireplaceQu = df.FireplaceQu.fillna('NoFirePlace')\n\nbasement_variables = ['BsmtFinType2', 'BsmtExposure', 'BsmtFinType1', 'BsmtCond', 'BsmtQual']\ndf[basement_variables] = df[basement_variables].fillna('NoBasement')\n\ngarage_variables = ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']\ndf[garage_variables] = df[garage_variables].fillna('NoGarage')","a3daaba0":"# Checking which variables are still missing\nvariable_missing_percentage(df)","8c3333bb":"df.YearBuilt.corr(df.GarageYrBlt).round(2)","f54ec456":"df = df.drop('GarageYrBlt', axis=1)","5bdf9f92":"df = df.dropna(how='any', subset=['MasVnrType', 'MasVnrArea', 'Electrical'])","f2ad8f86":"# Installing missingpy package, remember to turn internet on in the settings! \n!pip install missingpy\n\nfrom missingpy import KNNImputer\nknn_imputer = KNNImputer(n_neighbors=5, weights='distance', metric='masked_euclidean')\n\ndf.LotFrontage = knn_imputer.fit_transform(np.array(df.LotFrontage).reshape(-1,1))","b4596733":"# Making sure we have tackled all missing variables\nvariable_missing_percentage(df)","fb2cb127":"# Changing numeric variables to categorical\ndf = df.replace({\n    'MSSubClass' : {20 : \"SC20\", 30 : \"SC30\", 40 : \"SC40\", 45 : \"SC45\", 50 : \"SC50\", 60 : \"SC60\",\n                    70 : \"SC70\", 75 : \"SC75\", 80 : \"SC80\", 85 : \"SC85\", 90 : \"SC90\", 120 : \"SC120\", \n                    150 : \"SC150\", 160 : \"SC160\", 180 : \"SC180\", 190 : \"SC190\"},\n    'MoSold' : {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", 6 : \"Jun\",\n                7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"}\n})","43a75c73":"# Converting categorical variables to an interval scale as they are ordinal in nature.\ndf = df.replace({\n    'ExterQual' : {'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n    'ExterCond' : {'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n    'BsmtQual' : {'NoBasement' : 0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n    'BsmtCond' : {'NoBasement' : 0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n    'BsmtExposure' : {'NoBasement' : 0, 'No' : 1, 'Mn' : 2, 'Av' : 3, 'Gd' : 4},\n    'BsmtFinType1' : {'NoBasement' : 0, 'Unf' : 1, 'LwQ' : 2, 'Rec' : 3, 'BLQ' : 4, 'ALQ' : 5, 'GLQ' : 6},\n    'BsmtFinType2' : {'NoBasement' : 0, 'Unf' : 1, 'LwQ' : 2, 'Rec' : 3, 'BLQ' : 4, 'ALQ' : 5, 'GLQ' : 6},\n    'HeatingQC' : {'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n    'KitchenQual' : {'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n    'FireplaceQu' : {'NoFirePlace' : 0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n    'GarageFinish' : {'NoGarage' : 0, 'Unf' : 1, 'RFn' : 2, 'Fin' : 3},\n    'GarageQual' : {'NoGarage' : 0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n    'GarageCond' : {'NoGarage' : 0, 'Po' : 1, 'Fa' : 2, 'TA' : 3, 'Gd' : 4, 'Ex' : 5},\n})\n\n# Creating a list of our ordinal variables\nordinal_vars = [\n    'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n    'BsmtFinType1', 'BsmtFinType2','HeatingQC', 'KitchenQual',\n    'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond'\n]","ebe39bbd":"# Checking our datatypes once again\ndf.info()","8507bcb9":"# Changing features to their correct data types\ndf.BsmtCond = df.BsmtCond.astype('int64')\ndf.BsmtFinType2 = df.BsmtFinType2.astype('int64')\ndf.FireplaceQu = df.FireplaceQu.astype('int64')","6af28a57":"def change_variables_to_categorical(df, vars_to_change=[]):\n    '''Function that changes all non-numeric variables to categorical datatype.\n    \n    Arguments:\n        df : Pandas DataFrame\n        vars_to_change : list, default is an empty list\n            If a non-empty list is passed, only the variables in the list are converted to \n            categorical datatype.\n    \n    Returns:\n        df : Pandas DataFrame with categorical datatypes converted.\n    '''\n    categorical_variables = df.select_dtypes(exclude='number').columns.to_list()\n    \n    if len(vars_to_change) > 0:\n        categorical_variables = vars_to_change\n    \n    for var in categorical_variables:\n        df[var] = df[var].astype('category')\n        \n    return df","e9b927dc":"def numerical_categorical_split(df):\n    '''Function that creates a list for numerical and categorical variables respectively.\n    '''\n    numerical_var_list = df.select_dtypes(include='number').columns.to_list()\n    categorical_var_list = df.select_dtypes(exclude='number').columns.to_list()\n    \n    return numerical_var_list, categorical_var_list","55235a23":"# Changing datatypes from 'objects' to 'category' --> More memory efficient\ndf = change_variables_to_categorical(df)","efbbbd14":"# Creating lists of numerical and categorical features\nnumerical_vars, categorical_vars = numerical_categorical_split(df)\n\n# Splitting 2 dataframes, one for numeric variables and another for categorical\nnumerical_df = df[numerical_vars]\ncategorical_df = df[categorical_vars]","f5d83700":"# Checking if both have same number of observations\nprint(numerical_df.shape, categorical_df.shape)","14eef5ce":"any(numerical_df.SalePrice <= 0)","61de0cb6":"print(f'Skewness of SalePrice : {round(stats.skew(df.SalePrice),2)}')\n\nfig = plt.figure(figsize=(7,4))\nax = sns.distplot(numerical_df.SalePrice, fit=stats.norm)\nax.set_title('Distribution of SalePrice', size=18, y=1.05)\nplt.show();","0ed36f9f":"numerical_df['LogSalePrice'] = np.log(numerical_df.SalePrice)\n\nprint(f'Skewness of LogSalePrice : {round(stats.skew(numerical_df.LogSalePrice),2)}')\n\nfig = plt.figure(figsize=(7,4))\nax = sns.distplot(numerical_df.LogSalePrice, fit=stats.norm)\nax.set_title('Distribution of LogSalePrice', size=18, y=1.05)\nplt.show();","d2cbc5ee":"numerical_df = numerical_df.drop('SalePrice', axis=1)","d85656c3":"def check_variable_skew(df, threshold=1, verbose=True):\n    '''Function that checks each variable in the dataframe for their skewness.\n    \n    Arguments:\n        df : Pandas DataFrame\n        threshold : int, default = 1\n            The threshold that we allow for skewness within the variable.\n        verbose : bool, default = True\n            Prints out highly skewed variables and their values.\n        \n    Returns:\n        highly_skewed_vars_list : list\n    '''\n    skewness = df.apply(lambda x : np.abs(stats.skew(x)))\n    skewed_vars = skewness.loc[skewness >= threshold].sort_values(ascending=False).round(2)\n    \n    if len(skewed_vars) == 0:\n        print('There are no variables that are highly skewed.')\n        return []\n    \n    skewed_vars_list = skewed_vars.index.to_list()\n    \n    print(f'The following {len(skewed_vars_list)} variables are highly skewed:')\n    print()\n    for var in skewed_vars_list:\n        print(var, '\\t', skewed_vars.loc[var])\n      \n    return skewed_vars_list","c2e02c10":"def skewness_subplots(df, skewed_vars_list, n_cols=4, fig_size=(18,12)):\n    '''Function that plots the distribution of each variable within a grid.\n    \n    Arguments:\n        df : Pandas DataFrame\n        skewed_vars_list : list\n            List of variables to plot histograms for.\n        n_cols : int, default = 4\n            Number of columns for the grid\n    '''\n    num_vars = len(skewed_vars_list)\n    n_rows = int(np.ceil(num_vars \/ n_cols))\n    df_skewed_vars = df[skewed_vars_list]\n    \n    fig = plt.figure(figsize=fig_size)\n    plt.suptitle('Distributions for Highly Skewed Variables', y=1.03, size=18)\n\n    for i, col in enumerate(skewed_vars_list):\n        skew = np.round(stats.skew(df[col]), 2)\n        ax = fig.add_subplot(n_rows, n_cols, i+1)\n        sns.distplot(df[col], ax=ax, kde=False, bins=50)\n        ax.set_title(f'Skew : {skew}', size=16)\n    \n    plt.tight_layout()    \n    plt.show();","e9dfe2fa":"# Creating a list of variables that are highly skewed\nhighly_skewed_vars = check_variable_skew(numerical_df)","84cd2b06":"skewness_subplots(numerical_df, highly_skewed_vars, fig_size=(18,18))","117738f0":"def most_frequent_value_proportion(df, threshold=0.8, verbose=True):\n    '''Function that returns series with variables and their most frequent values respectively.\n    \n    Arguments:\n        df : Pandas DataFrame\n        threshold : float\n            Threshold for the maximum allowed proportion of a single value\/class. \n            \n    Returns:\n        most_frequent_series : Pandas Series\n            Variables as index and values as proportions for their most common value.\n    '''\n    most_frequent_pct = []\n    for col in df.columns:\n        most_frequent = df[col].value_counts(normalize=True).sort_values(ascending=False).iloc[0]\n        most_frequent_pct.append(np.round(most_frequent,2))\n    \n    most_frequent_series = pd.Series(most_frequent_pct, index=df.columns)\n    most_frequent_series = most_frequent_series.loc[most_frequent_series >= threshold]\n    most_frequent_series = most_frequent_series.sort_values(ascending=False)\n    \n    if verbose:\n        print(f'The following {len(most_frequent_series)} variables have a high concentration (>{threshold*100}%) of their values in one value only.')\n        print()\n        print(most_frequent_series)\n    \n    return most_frequent_series","4144c270":"narrow_dist_vars = most_frequent_value_proportion(numerical_df, threshold=0.8)","c07d2946":"# Dropping narrowly distributed variables\nnumerical_df = numerical_df.drop(narrow_dist_vars.index.to_list(), axis=1)","d1218d2f":"# List of positvely skewed variables\npos_skewed_vars = list(set(highly_skewed_vars) - set(narrow_dist_vars.index.to_list()) - set(ordinal_vars))","b27ec3fd":"def make_log_variables(df, variables_list, drop=False):\n    '''Function to make new columns of the logarithmic transformation of a list of variables.\n    Arguments:\n        df : Pandas DataFrame\n        variables_list : list\n            List of variables to log-transform.\n        drop : bool, default = False\n            Pass as true to drop the original variables.\n    Returns:\n        df : Pandas DataFrame with new variables.\n        log_var_list : list\n            List of the log-transformed variable names.\n    '''\n    # Checking for negative values for each variable\n    any_neg_value = np.sum((df[variables_list] < 0).all(axis=0))\n    if any_neg_value:\n        raise ValueError('There are one or more columns with negative values and cannot be log-transformed.')\n    \n    log_var_list = []\n    \n    for var in variables_list:\n        log_var_name = 'Log' + var\n        df[log_var_name] = np.log1p(df[var])\n        log_var_list.append(log_var_name)\n    \n    if drop:\n        df = df.drop(variables_list, axis=1)\n    \n    return df, log_var_list","19984410":"# Creating log-transformations for our highly skewed variables and saving the new variables in a list\nnumerical_df, log_var_list = make_log_variables(numerical_df, pos_skewed_vars, drop=False)","48f3d2e1":"print(f'Prior to log-transformation, there were {len(pos_skewed_vars)} variables that were highly positively skewed.')","81da8fd5":"highly_skewed_vars = check_variable_skew(numerical_df[log_var_list])","06af9601":"# Dropping original variables\npos_skewed_vars.remove('TotalBsmtSF')\nnumerical_df = numerical_df.drop(pos_skewed_vars, axis=1)","68b96a92":"fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\nax1.set_title(f'Skew : {stats.skew(numerical_df.TotalBsmtSF):.2f}', y=1.03)\nsns.distplot(numerical_df.TotalBsmtSF, ax=ax1, bins=50)\n\nax2.set_title(f'Skew : {stats.skew(numerical_df.LogTotalBsmtSF):.2f}', y=1.03)\nsns.distplot(numerical_df.LogTotalBsmtSF, ax=ax2, bins=50)\n\nfig.tight_layout()\nplt.show();","ad6af257":"fig = sns.scatterplot(numerical_df.TotalBsmtSF, numerical_df.LogSalePrice)","5d8a118f":"# Removing the observation from both our numerical and caregorical data frames\noutliers = numerical_df.loc[numerical_df.TotalBsmtSF > 5000].index.to_list()","45ace9dd":"# Dropping the outlier from both numerical and categorical data frames\nnumerical_df = numerical_df.drop(outliers, axis=0)\ncategorical_df = categorical_df.drop(outliers, axis=0)\nprint(numerical_df.shape, categorical_df.shape)","16706d26":"fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\nax1.set_title(f'Skew : {stats.skew(numerical_df.TotalBsmtSF):.2f}', y=1.03)\nsns.distplot(numerical_df.TotalBsmtSF, ax=ax1, bins=50)\n\nax2.set_title(f'Skew : {stats.skew(numerical_df.LogTotalBsmtSF):.2f}', y=1.03)\nsns.distplot(numerical_df.LogTotalBsmtSF, ax=ax2, bins=50)\n\nfig.tight_layout()\nplt.show();","16a3f089":"numerical_df = numerical_df.drop('LogTotalBsmtSF', axis=1)","8dabd386":"# Rearranging our dataframe for easier interpretation of heatmap\nlog_sale_price = numerical_df.LogSalePrice\nnumerical_df = numerical_df.drop('LogSalePrice', axis=1)\nnumerical_df['LogSalePrice'] = log_sale_price","c8a6323f":"sns.set(style=\"white\")\n\n# Compute the correlation matrix\ncorr = numerical_df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(20, 16))\nplt.title('Correlation Heatmap', size=20)\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nheatmap = sns.heatmap(corr, mask=mask, cmap=cmap, fmt='.2f', vmin=-1, vmax=1.0, center=0, square=True,\n                      linewidths=.5, cbar_kws={\"shrink\": .7}, annot=True, annot_kws={\"size\": 8})\n\nbottom, top = ax.get_ylim()\nheatmap.set_ylim(bottom + 0.5, top - 0.5)","747d3a3e":"numerical_df = numerical_df.drop(['OverallCond', 'YrSold', 'GarageCars'], axis=1)","e96d5295":"corr_matrix_unstacked = corr.unstack().sort_values(ascending=False).drop_duplicates()\ncorrelated_pairs = corr_matrix_unstacked.loc[corr_matrix_unstacked >= 0.75].index.to_list()\ncorrelated_pairs","862597ce":"def scatter_subplots(df, target, hue=None, n_cols=4, fig_size=(12,12)):\n    '''Function that plots the scatterplots of each variable against the target variable within a grid.\n    \n    Arguments:\n        df : Pandas DataFrame with target variable included\n        target : str\n            Target feature name\n        hue : str, default = None\n            Column in the data frame that should be used for colour encoding\n        n_cols : int, default = 4\n            Number of columns for the grid\n    '''\n    independent_vars_list = list(df.columns)\n    independent_vars_list.remove(target)\n    num_vars = len(independent_vars_list)\n    n_rows = int(np.ceil(num_vars \/ n_cols))\n    \n    plt.style.use('fivethirtyeight')\n    fig = plt.figure(figsize=fig_size)\n    plt.suptitle(f'Scatterplots of Independent Variables against {target}', y=1.02, size=18)\n\n    for i, col in enumerate(independent_vars_list):\n        ax = fig.add_subplot(n_rows, n_cols, i+1)\n        sns.scatterplot(x=col, y=target, hue=hue, data=df, ax=ax)\n    \n    plt.tight_layout()\n    plt.show();","35845a5e":"scatter_subplots(numerical_df, 'LogSalePrice', fig_size=(16,24))","8e541a94":"# Adding the target variable to the categorical dataframe\ncategorical_df['LogSalePrice'] = numerical_df.LogSalePrice\nprint(categorical_df.shape, numerical_df.shape)","7d5789fe":"def annotate_plot(ax, dec_places=1, annot_size=14):\n    '''Function that annotates plots with their value labels.\n    Arguments:\n        ax : Plot Axis.\n        dec_places : int\n            Number of decimal places for annotations.\n        annot_size : int\n            Font size of annotations.\n    '''\n    for p in ax.patches:\n        ax.annotate(\n            format(p.get_height(), '.{}f'.format(dec_places)),\n            (p.get_x() + p.get_width() \/ 2., p.get_height(),),\n            ha='center', va='center',\n            xytext=(0,10), textcoords='offset points', size=annot_size\n        )","e0f0a167":"def var_categories_countplots(df, n_cols=3, orientation='v', x_rotation=45, y_rotation=0, palette='pastel', fig_size=(18,12)):\n    '''Function that plots the class distribution for categorical variables.\n    \n    Arguments:\n        df : Pandas DataFrame\n        n_cols : int, default = 3\n            Number of columns for the subplot grid.\n        orientation : str, default = 'v'\n            Plot orientation, with 'v' for vertical and 'h' for horizontal.\n        x_rotation : int, default = 45\n            Rotation of the x-axis labels.\n        palette : str, default = 'pastel'\n            Seaborn color palette for plotting.\n    '''\n    categorical_vars = df.select_dtypes(exclude='number').columns.to_list()\n    num_vars = len(categorical_vars)\n    n_rows = int(np.ceil(num_vars \/ n_cols))\n    \n    fig = plt.figure(figsize=fig_size)\n    plt.suptitle('Class Distributions for Categorical Variables', y=1.01, size=24)\n    \n    for i, col in enumerate(categorical_vars):\n        ax = fig.add_subplot(n_rows, n_cols, i+1)\n        sns.countplot(x=df[col], ax=ax, orient=orientation, palette=palette)\n        ax.set_ylabel('Frequency')\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=x_rotation)\n        \n        annotate_plot(ax, dec_places=0, annot_size=12) # Annotating plot with count labels\n    \n    plt.tight_layout()\n    plt.show();","deba6c00":"# Finding variables that have more than 80% of their values in one category\nhighly_imbalanced_vars = most_frequent_value_proportion(categorical_df, threshold=0.8, verbose=False)\nhighly_imbalanced_vars_list = highly_imbalanced_vars.index.to_list()\nprint(f'The following {len(highly_imbalanced_vars)} variables have more than 80% of their data concentrated in only one class:')\nprint()\nprint(highly_imbalanced_vars)\n\n# Note: Function was defined earlier in univariate analysis of numerical variables","3c6424b4":"var_categories_countplots(categorical_df[highly_imbalanced_vars_list], fig_size=(16,16))","c30aece9":"categorical_df = categorical_df.drop(highly_imbalanced_vars_list, axis=1)","8b0aef5d":"var_categories_countplots(categorical_df, fig_size=(20,16))","b7678259":"categorical_df.MSZoning = categorical_df.MSZoning.apply(lambda x :\n                                                        x if x == 'RL'\n                                                        else x if x == 'RM'\n                                                        else 'Others')\n\ncategorical_df.LotShape = categorical_df.LotShape.apply(lambda x :\n                                                        x if x =='Reg'\n                                                        else 'Irregular')\n\ncategorical_df.LotConfig = categorical_df.LotConfig.apply(lambda x :\n                                                          x if x == 'Inside'\n                                                          else x if x == 'CulDSac'\n                                                          else x if x == 'Corner'\n                                                          else 'FR')\n\ncategorical_df.RoofStyle = categorical_df.RoofStyle.apply(lambda x :\n                                                          x if x =='Gable'\n                                                          else x if x == 'Hip'\n                                                          else 'Others')\n\ncategorical_df.MasVnrType = categorical_df.MasVnrType.apply(lambda x :\n                                                            x if x == 'None'\n                                                            else x if x == 'Stone'\n                                                            else 'Brk')\n\ncategorical_df.Foundation = categorical_df.Foundation.apply(lambda x :\n                                                            x if x =='BrkTil'\n                                                            else x if x == 'CBlock'\n                                                            else x if x == 'PConc'\n                                                            else 'Others')\n\ncategorical_df.GarageType = categorical_df.GarageType.apply(lambda x : \n                                                            x if x == 'Attchd'\n                                                            else x if x == 'BuiltIn'\n                                                            else x if x == 'Detchd'\n                                                            else x if x == 'NoGarage'\n                                                            else 'Others')","a7321e76":"def var_categories_boxplots(df, target, hue=None, n_cols=3, orientation='v', x_rotation=45, y_rotation=0, palette='pastel', fig_size=(18,12)):\n    '''Function that plots the class distribution for categorical variables against target variable.\n    \n    Arguments:\n        df : Pandas DataFrame\n        target : str\n            Target variable name.\n        hue : str, default = None\n            Column in the data frame that should be used for colour encoding.\n        n_cols : int, default = 3\n            Number of columns for the subplot grid.\n        orientation : str, default = 'v'\n            Plot orientation, with 'v' for vertical and 'h' for horizontal.\n        x_rotation : int, default = 45\n            Rotation of the x-axis labels.\n        palette : str, default = 'pastel'\n            Seaborn color palette for plotting.\n    '''\n    categorical_vars = df.select_dtypes(exclude='number').columns.to_list()\n    num_vars = len(categorical_vars)\n    n_rows = int(np.ceil(num_vars \/ n_cols))\n    \n    fig = plt.figure(figsize=fig_size)\n    plt.suptitle('Categorical Variables vs Target', y=1.01, size=24)\n    \n    for i, col in enumerate(categorical_vars):\n        ax = fig.add_subplot(n_rows, n_cols, i+1)\n        sns.boxplot(x=df[col], y=df[target], ax=ax, hue=hue, orient=orientation, palette=palette)\n        ax.set_xticklabels(ax.get_xticklabels(), rotation=x_rotation)\n    \n    plt.tight_layout()\n    plt.show();","18823991":"var_categories_boxplots(categorical_df, 'LogSalePrice', n_cols=3, fig_size=(20,30))","397b2cf1":"df = pd.concat([categorical_df.drop('LogSalePrice', axis=1), numerical_df], axis=1).reset_index(drop=True)\n\n# One Hot Encoding\ndf = pd.get_dummies(df)\n\ny = df.LogSalePrice\nX = df.drop('LogSalePrice', axis=1)\nprint(X.shape, y.shape)\nX.head()","c235e01d":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)","a32d5a0a":"kfold = KFold(n_splits=5, shuffle=True, random_state=SEED)","cfe6213f":"def rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\n# Creating a sklearn scorer instance\nrmse_scorer = {'RMSE' : make_scorer(rmse, greater_is_better=False, needs_proba=False, needs_threshold=False)}","8555cc40":"# Kwargs for cross_validate method\ncv_kwargs = {\n    'scoring' : rmse_scorer,\n    'cv' : kfold,\n    'n_jobs' : -1,\n    'return_train_score' : True,\n    'verbose' : False,\n    'return_estimator' : True\n}","68719076":"def save_cv_results(cv_results, scoring_name='score', verbose=True):\n    '''Function to save the training and testing results from cross validation into a dataframe.\n    \n    Arguments:\n        cv_results : dict\n            Dictionary of results from scikit-learn's cross_validate method.\n        scoring_name : str, default = 'score'\n            Name of scorer used in the cross_validate method. If no custom scorer was passed, default should be 'score'.\n            In the cv_results dictionary, there should be keys 'train_score' and 'test_score'\n            If custom scorer was passed as the scoring method, the cv_results dictionary should have 'train_scoring_name'.\n        verbose : bool, default = True\n            Prints the mean training and testing scores and fitting times. \n            \n    Returns:\n        results_df : Pandas DataFrame with training and test scores.\n    \n    '''\n    train_key = 'train_' + scoring_name\n    test_key = 'test_' + scoring_name\n    \n    # Sklearn scorer flips the sign to negative so we need to flip it back\n    train_scores = [-result for result in cv_results[train_key]]\n    test_scores = [-result for result in cv_results[test_key]]\n    \n    indices = ['Fold 1', 'Fold 2', 'Fold 3', 'Fold 4', 'Fold 5']\n    \n    results_df = pd.DataFrame({'TrainScores' : train_scores, 'TestScores' : test_scores}, index=indices)\n    \n    if verbose:\n        avg_train_score = np.mean(train_scores)\n        avg_test_score = np.mean(test_scores)\n        avg_training_time = np.mean(cv_results['fit_time'])\n        avg_predict_time = np.mean(cv_results['score_time'])\n        \n        title = 'Cross Validation Results Summary'\n        print(title)\n        print('=' * len(title))\n        print(f'Avg Training {scoring_name}', '\\t', '{:.6f}'.format(avg_train_score))\n        print(f'Avg Testing {scoring_name}', '\\t', '{:.6f}'.format(avg_test_score))\n        print()\n        print('Avg Fitting Time', '\\t', '{:.4f}s'.format(avg_training_time))\n        print('Avg Scoring Time', '\\t', '{:.4f}s'.format(avg_predict_time))\n    \n    return results_df","a88a0428":"def training_vs_testing_plot(results, fig_size=(5,5), title_fs=18, legend_fs=12):\n    '''Function that plots the training and testing scores obtained from cross validation.'''\n    \n    fig = plt.figure(figsize=fig_size)\n    plt.style.use('fivethirtyeight')\n    plt.title('Cross Validation : Training and Testing Scores', y=1.03, x=0.6, size=title_fs)\n    plt.plot(results.TrainScores, color='b', label='Training')\n    plt.plot(results.TestScores, color='r', label='Testing')\n    plt.legend(loc='center left', bbox_to_anchor=(1.02,0.5), ncol=1, fontsize=legend_fs)\n    plt.show();","c056e6db":"def get_best_estimator(cv_results, scoring_name='score'):\n    ''' Function that returns the best estimator found during cross valiation.\n    Arguments:\n        cv_results : dict\n            Results from Sklearn's cross_validate method.\n        scoring_name : str, default = 'score'\n            Custom scoring name if a custom scorer was passed during cross validation.\n            Default 'score' should be used when using sci-kit learn's  scoring metrics. \n    \n    Returns:\n        best_estimator : Sklearn estimator object\n            Best estimator found during cross validation.\n    '''\n    test_key = 'test_' + scoring_name\n    \n    # Sklearn flips the sign during scoring so we need to flip it back\n    scores = [-result for result in cv_results[test_key]]\n    max_score_index = scores.index(max(scores))\n    best_estimator = cv_results['estimator'][max_score_index]\n    \n    return best_estimator","36cdf09f":"linreg = LinearRegression(fit_intercept=True, normalize=False, n_jobs=-1)\n\nlinreg_cv_results = cross_validate(linreg, X_train, y_train, **cv_kwargs)\nlinreg_cv_results","f637877c":"# Saving cross validation results to a dataframe\nlinreg_cv_scores = save_cv_results(linreg_cv_results, scoring_name='RMSE')\n\n# Plotting training vs testing RMSE scores\ntraining_vs_testing_plot(linreg_cv_scores)","75848a3a":"# Saving best estimator from cross validation\nbest_linreg = get_best_estimator(linreg_cv_results, scoring_name='RMSE')","b816fec5":"def holdout_set_evaluation(model, X_train, y_train, X_test, y_test, model_name, scoring_name):\n    '''Function that evaluates the performance on the holdout dataset.\n    \n    Arguments:\n        model : sklearn estimator object\n        model_name : str\n            String to be passed as the index for the dataframe.\n        scoring_name : str\n            Evluation metric used as column header.\n    \n    Returns:\n        rmse_score : Pandas DataFrame\n            Column is the scoring_name, index is the model_name and value is the model performance on the holdout dataset.\n    '''    \n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    \n    rmse_score = rmse(y_test, y_pred) # calls rmse function\n    rmse_score = pd.DataFrame({scoring_name : [rmse_score]}, index=[model_name])\n    \n    return rmse_score.round(4)","f08ca5df":"# Saving our result to a dataframe\nlinreg_result = holdout_set_evaluation(best_linreg, X_train, y_train, X_test, y_test, model_name='Linear', scoring_name='RMSE')\nlinreg_result","bc069cc7":"# Creating a new dataframe to store model results\nmodel_results = linreg_result.copy(deep=True)","1846535f":"print('Highly Correlated Pairs of Variables:')\nprint(correlated_pairs)","8ed985fb":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\nvif_X = add_constant(X)\nvifs = pd.Series([\n    variance_inflation_factor(vif_X.values, i) for i in range(vif_X.shape[1])],\n    index=vif_X.columns\n)\n\n# Dropping inf values due to dummy variables\nvifs = vifs.loc[vifs != np.inf].sort_values(ascending=False)\nvifs.loc[vifs > 5]","8e526be9":"# Range of alphas to iterate over\nalphas_vector = np.arange(1,200)","97b28c90":"def alpha_tuning_results(alphas_vector, X_train, y_train, cv_kwargs, model_name='Ridge', scoring_name='score', x_axis_log_scale=False, fig_size=(5,5)):\n    ''' Function to obtain the average training and testing RMSE across different alpha values.\n    \n    Arguments:\n        alphas_vector : array\n            array of alpha values to iterate through and fit the model\n        cv_kwargs : dict\n            kwargs for the cross_validate method\n        model_name : str, default = 'Ridge'\n            Type of model to fit training data. Any other str input will fit a Lasso model.\n        scoring_name : str, default = 'score'\n            Scoring used in sklearn's cross_validate method. Use default value if no custom scorer was used.\n            Else, enter the name of the scorer used when making scorer.\n        x_axis_log_scale : bool, default = False\n            Set the X-axis to log scale. Useful when tuning Lasso Alpha.\n        \n    Returns:\n        results_df : Pandas DataFrame with average training and testing RMSE per alpha. \n            \n    '''\n    results_df = pd.DataFrame(columns=['Avg_Train_RMSE', 'Avg_Test_RMSE'])\n\n    for alpha in alphas_vector:\n        if model_name == 'Ridge':\n            model = Pipeline(steps=[\n                ('Standardise',  StandardScaler()),\n                ('Ridge', Ridge(alpha=alpha, fit_intercept=True, random_state=SEED))\n            ])\n        else:\n            model = Pipeline(steps=[\n                ('Standardise', StandardScaler()),\n                ('Lasso', Lasso(alpha=alpha, fit_intercept=True, random_state=SEED))\n            ])\n        \n        cv_results = cross_validate(model, X_train, y_train, **cv_kwargs)\n        train_key = 'train_' + scoring_name\n        test_key = 'test_' + scoring_name\n        # Sklearn scorer flips the sign to negative so we need to flip it back\n        train_scores = [-result for result in cv_results[train_key]]\n        test_scores = [-result for result in cv_results[test_key]]\n        avg_train_rmse = np.mean(train_scores)\n        avg_test_rmse = np.mean(test_scores)\n        \n        results_df.loc[alpha] = [avg_train_rmse, avg_test_rmse]\n    \n    # Visualising the results\n    fig = plt.figure(figsize=fig_size)\n    plt.style.use('fivethirtyeight')\n    plt.title(f'Training and Testing {scoring_name} for Different Alpha Values', y=1.03, x=0.6, size=16)\n    plt.ylabel(f'{scoring_name}', size=14)\n    plt.xlabel('Alpha', size=14)\n    \n    if x_axis_log_scale:\n        plt.xscale('log')\n    \n    plt.plot(results_df.Avg_Train_RMSE, color='b', label='Training')\n    plt.plot(results_df.Avg_Test_RMSE, color='r', label='Testing')\n    plt.legend(loc='center left', bbox_to_anchor=(1.02,0.5), ncol=1, prop={'size': 14})\n    plt.plot();\n    \n    return results_df","44c4962b":"# Function to perform cross validation for each alpha value and plot the average RMSE obtained for each alpha value\nridge_results_df = alpha_tuning_results(np.arange(1,200), X_train, y_train, cv_kwargs, model_name='Ridge', scoring_name='RMSE')","8ee7d93b":"# Getting the alpha which has the lowest testing RMSE\noptimal_ridge_alpha = ridge_results_df.Avg_Test_RMSE.idxmin()\nprint(f'The optimal alpha from cross validation : {optimal_ridge_alpha}')","a3f0accb":"# Creating model with optimal alpha\nridge = Pipeline(steps=[\n    ('Standardise',  StandardScaler()),\n    ('Ridge', Ridge(alpha=optimal_ridge_alpha, fit_intercept=True, random_state=SEED))\n])\n\n# Cross validation results\nridge_cv_results = cross_validate(ridge, X_train, y_train, **cv_kwargs)\n\n# Saving scores from cv results\nridge_cv_scores = save_cv_results(ridge_cv_results, scoring_name='RMSE')\n\n# Plotting training vs testing RMSE scores\ntraining_vs_testing_plot(ridge_cv_scores)","525e89ac":"# Saving best estimator from cross validation\nbest_ridge = get_best_estimator(ridge_cv_results, scoring_name='RMSE')\n\n# Evaluating resutls \nridge_result = holdout_set_evaluation(best_ridge, X_train, y_train, X_test, y_test, model_name='Ridge', scoring_name='RMSE')","842757c3":"model_results = model_results.append(ridge_result)\nmodel_results","000ef331":"# Using a GridSearch to find the optimal alpha for the ridge regression \nridge_pipe = Pipeline(steps=[\n    ('Standardise', StandardScaler()),\n    ('Ridge', Ridge(fit_intercept=True, random_state=SEED))\n])\n\nridge_params = {'Ridge__alpha' : np.arange(1,200)}\n\nridge_gscv = GridSearchCV(ridge_pipe, ridge_params, scoring=rmse_scorer['RMSE'], n_jobs=-1, cv=kfold, return_train_score=True)\nridge_gscv.fit(X_train, y_train)","e952fc43":"# Best alpha \nridge_gscv.best_params_","56299248":"# Range of alphas to iterate over\nalphas_vector = np.logspace(-6,0,7)\n\n# Function to perform cross validation for each alpha value and plot the average RMSE obtained for each alpha value\nlasso_results_df = alpha_tuning_results(alphas_vector, X_train, y_train, cv_kwargs, model_name='Lasso', scoring_name='RMSE', x_axis_log_scale=True)","5ce48d5f":"lasso_results_df.Avg_Test_RMSE.idxmin()","d7377226":"alphas_vector = np.linspace(0.0001, 0.01, 100)\n\n# Function to perform cross validation for each alpha value and plot the average RMSE obtained for each alpha value\nlasso_results_df = alpha_tuning_results(alphas_vector, X_train, y_train, cv_kwargs, model_name='Lasso', scoring_name='RMSE', x_axis_log_scale=True)","d746940f":"# Getting the alpha which has the lowest testing RMSE\noptimal_lasso_alpha = lasso_results_df.Avg_Test_RMSE.idxmin()\noptimal_lasso_alpha","276ff0b2":"lasso_pipe = Pipeline(steps=[\n    ('Standardise', StandardScaler()),\n    ('Lasso', Lasso(alpha=optimal_lasso_alpha, fit_intercept=True, random_state=SEED))\n])\n\nlasso_cv_results = cross_validate(lasso_pipe, X_train, y_train, **cv_kwargs)\n\n# Saving cross validation results \nlasso_cv_scores = save_cv_results(lasso_cv_results, scoring_name='RMSE')\n\n# Plotting training vs testing RMSE scores\ntraining_vs_testing_plot(lasso_cv_scores)","fd936ac1":"# Saving best model from cross validation\nbest_lasso = get_best_estimator(lasso_cv_results, scoring_name='RMSE')\n\n# Evaluating Lasso performance\nlasso_result = holdout_set_evaluation(best_lasso, X_train, y_train, X_test, y_test, model_name='Lasso', scoring_name='RMSE')","34e4400a":"model_results = model_results.append(lasso_result)\nmodel_results","a37f4409":"best_lasso.fit(X_train, y_train)\n\n# We need to access the 'named_steps' to get our Lasso estimator as we are using a Pipeline\nlasso_model = best_lasso.named_steps['Lasso']\nlasso_coefs = pd.Series(lasso_model.coef_, index=X.columns)\nzero_coefs = lasso_coefs.loc[lasso_coefs == 0].index.to_list()\n\nprint(f'Original training data frame had {X.shape[1]} variables.')\nprint(f'Lasso selection removed {len(zero_coefs)} variables.')\nprint(f'Number of variables with non-zero coefficients : {X.shape[1] - len(zero_coefs)}')","1ed75b49":"# Creating new training and testing datasets after removing variables\nsmall_df = df.drop(zero_coefs, axis=1)\n\nXsmall = small_df.drop('LogSalePrice', axis=1)\nXsmall_train, Xsmall_test, y_train, y_test = train_test_split(Xsmall, y, test_size=0.2, random_state=SEED)\nprint(Xsmall_train.shape, Xsmall_test.shape)","f45e0ed3":"linreg_small = LinearRegression(fit_intercept=True, n_jobs=-1)\n\n# Cross validation \nlinreg_small_cv_results = cross_validate(linreg_small, Xsmall_train, y_train, **cv_kwargs)\n\n# Saving cross validation results\nlinreg_small_cv_scores = save_cv_results(linreg_small_cv_results, scoring_name='RMSE')\n\n# Plotting training vs testing RMSE scores\ntraining_vs_testing_plot(linreg_small_cv_scores)","682e194b":"# Saving best estimator from cross validation\nbest_linreg_small = get_best_estimator(linreg_small_cv_results, scoring_name='RMSE')\n\n# Saving our result to a dataframe so it is easier to append the performances of other models\nlinreg_small_result = holdout_set_evaluation(best_linreg_small, Xsmall_train, y_train, Xsmall_test, y_test, model_name='Linear_SmallDf', scoring_name='RMSE')\n\n# Evaluating results\nmodel_results = model_results.append(linreg_small_result)","31524f0a":"model_results","238d74aa":"# Baseline Untuned Random Forest Model\nbaseline_rforest = RandomForestRegressor(random_state=SEED)\nbaseline_rf_cv_results = cross_validate(baseline_rforest, Xsmall_train, y_train, **cv_kwargs)\n\n# Saving cross validation results\nbaseline_rf_cv_scores = save_cv_results(baseline_rf_cv_results, scoring_name='RMSE')\n\n# Plotting training vs testing RMSE scores\ntraining_vs_testing_plot(baseline_rf_cv_scores)","d1ebb3f5":"# Saving best estimator from cross validation\nbest_baseline_rf = get_best_estimator(baseline_rf_cv_results, scoring_name='RMSE')\n\n# Saving our result to a dataframe so it is easier to append the performances of other models\nbaseline_rf_result = holdout_set_evaluation(best_baseline_rf, Xsmall_train, y_train, Xsmall_test, y_test, model_name='BaselineRF', scoring_name='RMSE')","6c08b367":"model_results = model_results.append(baseline_rf_result)\nmodel_results","0afc366f":"def rforest_tuning_scores(model, X_train, y_train, parameter, param_range, scorer, cv, flip_scores=True):\n    \n    gridsearch = GridSearchCV(model, param_grid={parameter : param_range}, scoring=scorer,\n                              cv=cv, n_jobs=-1, return_train_score=True, verbose=False)\n    \n    gridsearch.fit(X_train, y_train)\n    cv_results = gridsearch.cv_results_\n    \n    if flip_scores:\n        train_scores = [-result for result in cv_results['mean_train_score']]\n        test_scores = [-result for result in cv_results['mean_test_score']]\n    else:\n        train_scores = [cv_results['mean_train_score']]\n        test_scores = [cv_results['mean_test_score']]\n        \n    results_df = pd.DataFrame({'TrainScores' : train_scores, 'TestScores' : test_scores}, index=param_range)\n    \n    return results_df","9ffb4233":"param_grid = {\n    'max_features' : ['auto', 'sqrt', 'log2'],\n    'max_depth' : [1, 2, 5, 10, 20, 30, 50, None],\n    'min_samples_split' : np.arange(2, 30, step=2),\n    'min_samples_leaf' : np.arange(1, 20, step=1)\n}","bbc8cb70":"n_cols = 2\nn_vars = len(param_grid)\nn_rows = int(np.ceil(n_vars \/ n_cols))\nindex = 0\n\nfig = plt.figure(figsize=(12,6))\nplt.suptitle('Training and Test Scores for Different Hyper Parameters', y=1.03, size=20)\n\nfor parameter, param_range in dict.items(param_grid):\n    results_df = rforest_tuning_scores(baseline_rforest, Xsmall_train, y_train, parameter=parameter, param_range=param_range,\n                                       scorer=rmse_scorer['RMSE'], cv=kfold, flip_scores=True)\n\n    ax = fig.add_subplot(n_rows, n_cols, index+1)\n    plt.plot(results_df.TrainScores, color='b', label='Training')\n    plt.plot(results_df.TestScores, color='r', label='Testing')\n    \n    plt.xlabel(parameter, size=14)\n    plt.legend(loc='center left', bbox_to_anchor=(1.02,0.5), ncol=1, prop={'size': 14})\n    \n    index += 1\n\nplt.tight_layout()\nplt.show();","436d1a6e":"# New parameter tuning grid\nrforest_tuning_grid = {\n    'max_depth' : np.arange(8,13 , step=1),\n    'max_features' : ['auto', 'sqrt', 'log2'],\n    'min_samples_split' : np.arange(2, 6, step=1),\n    'min_samples_leaf' : np.arange(1, 6, step=1),\n}","04efaf62":"# Dictionary where we will store the optimal kwargs for the tuned random forest model\nrforest_kwargs = {\n    'n_estimators' : 100,\n    'criterion' : 'mse',\n    'max_features' : 'sqrt',\n    'bootstrap' : True,\n    'n_jobs' : -1,\n    'random_state' : SEED, \n}","dc81d1c5":"np.random.seed(8888)\ngs_rforest = GridSearchCV(estimator=RandomForestRegressor(**rforest_kwargs), param_grid=rforest_tuning_grid,\n                          scoring=rmse_scorer['RMSE'], n_jobs=-1, cv=kfold, refit=True)\n\ngs_rforest.fit(Xsmall_train, y_train)","710274e3":"# Saving the best estimator from the gird search\ntuned_rforest = gs_rforest.best_estimator_\n\n# Updating our random forest kwargs with the optimal hyper parameter values\nrforest_kwargs.update(gs_rforest.best_params_)\nrforest_kwargs","03c559f4":"# Evaluating tuned random forest results\ntuned_rf_result = holdout_set_evaluation(tuned_rforest, Xsmall_train, y_train, Xsmall_test, y_test, model_name='TunedRF', scoring_name='RMSE')","94382954":"model_results = model_results.append(tuned_rf_result)\nmodel_results","b3bd5ad2":"### 2.2 Handling Missing Data <a id='missing_data'><\/a>\n\n__Missing data__ is common in most data sets. While there are algorithms that can handle missing data (i.e. xgboost), not all of them can.\n\nAs we will be comparing different models, it is good practice to handle these missing values.\n\nThere are 2 options when dealing with missing data:\n1. __Imputation__\n2. __Dropping Column \/ Observations__\n\nFor imputation, the method varies depending on the type of variable:\n* __Numerical__ - Imputation with Mean \/ Median \/ K-Nearest Neighbours\n* __Categorical__ - Imputation with Mode \/ K-Nearest Neighbours \/ Create New Category\n\nNote: For __*time series*__ data, we can impute numerical variables using __Linear Interpolation \/ Back-filling \/ Forward-filling__.\n\nWith regards to dropping variables, i do not know of any 'rules' but the __rules of thumb__ that i usually employ are:\n* Drop variables with more than 80-90% missing data\n* If fraction of missing values is negligible\/small compared to number of observations, drop rows that are missing data.\n\nIf anyone has any suggestions\/advice on when to drop data, please do let me know! (:","62709f15":"## Notebook Overview\n\nThe goal of this notebook is to provide a __comprehensive walkthrough__ of the different stages of solving a machine learning regression problem. \n\nThroughout this notebook, i detail my thought process as i work through the problem and provide insights wherever i can. Please note that this is a pretty lengthy notebook, and i have created a table of contents so it is easier to revisit the notebook.\n\nI hope that this notebook serves as a reference for newer Kagglers on how to approach a Regression Problem. For experienced Kagglers that come across this notebook, i welcome any feedback\/criticism of my methodologies and ways to improve. \n\n*If you enjoyed reading this notebook, an upvote would be appreciated! Thank you and have a great day! (:*","73df8a59":"Let's check the count distributions for the remaining categorical variables to check if there any more problematic variables.","83f519f9":"### 3.1 Target Variable : SalePrice <a id='target'><\/a>\n\nFirst, we look at the distribution of the target variable and check for any negative values.","1e4e05bd":"Next, we can scan the data types of our variables and check back with our data and see if they are correct. \n\nDetermining whether they are correct or not may require some domain knowledge but most of the time can be identified with a bit of common sense. (: \n\ni.e Prices are numerical but they are classified as 'object', which is definitely a mistake.\n\nNote: Expand the cell below to view the data types.","66789803":"From the table above, we can see that by removing the features with 0 coefficients from the Lasso regression, we were able to improve the model performance of the Linear model! \n\nThis shows that Lasso Regression can be useful for feature selection prior to the modelling process. \n\nWe will proceed to ensemble modelling using the smaller dataset.","c6d2eb81":"## 4.4 Lasso Regression - L1 Regularisation <a id='lasso'><\/a>\n\nLasso Regression is another form of regularisation that we can use to help our model generalise better to unseen data. Similar to Ridge Regression, it incorporates a penalty term (lambda \/ alpha) to the cost function. \n\nWhile Ridge Regression regularises the model by shrinking all coefficients towards 0 but never reaching 0, L1 regularisation by Lasso is able to shrink some non-informative variables' coefficients all the way to 0. \n\nThis property of Lasso Regression is useful as it helps us with feature selection. By setting their coefficients to 0, the Lasso model is essentially removing them from the model.\n\nThis [article][1] does a good comparison between Ridge and Lasso regression.\n\n[1]: https:\/\/towardsdatascience.com\/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b","949954bf":"Phew! We finally finished our preprocessing and exploratory data analysis for both our numerical and categorical data!\n\nWe can finally move on to the fun part, modelling our data!","fbcb982b":"After looking into the description.txt file, we find that the cause of most of the missing variables are due to the property not having the associated feature. Therefore, we will be dropping the variables that have high percentage of missing data ( > 80%) as they would not generalise well to new data.","0a80130c":"For Lasso Regression, what i usually like to do is see how the alpha changes in orders of 10.\n\nSimilar to Ridge Regression, the regularisation strength decreases as we approach 0, and in the case where alpha = 0, it is the same as Linear Regression.","068ec1fd":"# 2. Preprocessing the Data <a id='preprocessing'><\/a>\n\nAfter we have defined the problem statement and know how to evaluate our model performance, we need to preprocess the data before the modelling stage.\n\nThere are often mistakes or issues with our data that we need to address before we can actually fit our model using them. \n\nCommon things to look out for in our data:\n* Typos\n* Outliers\n* Missing Values\n* Incorrect Data Types\n* Whitespace in Column Headers","2ab828a8":"After removing the outlier, the original variable is much more normally distributed, therefore we will remove the log-transformed variable from out dataset. ","29898f40":"From the diagram above, we observe that the class imbalance within these variables are quite extreme, therefore, we will be dropping these variables from the analysis. This would help alleviate the problem of 'The Curse of Dimensionality' as we would have less dummy variables when we encode our data for modelling later on.","557a0934":"From the table above, we see that the tuned Random Forest Model has outperformed the untuned version, albeit only slightly. This is because the tuned parameters were very similar to the default settings of the untuned model. \n\nHowever, we see that even after tuning, the Random Forest Regression does not outperform even the simple Linear Regression model. This shows tthat when modelling a datset, there is *__'No Free Lunch'__*! \n\nMore advanced\/complex models will not outperform simple models all the time!","e23d8532":"Next, let's check the distribution and skewness of the target.","51adc48a":"Comparing the results of the linear models, Lasso performed the best.\n\nThe poorer performance of the Linear and Ridge models could be due to multicollinearity. Ridge is able to outperform the Linear model as it is able to reduce the impacts of multicollinearity, by shrinking the coefficients of correlated pairs. \n\nIn the case of Lasso, it handles multicollinearity by dropping the less important variable from the correlated pair of variables. ","b0291385":"Now that we have tuned the hyper parameters of the Random Forest Regression model, we can evaluate its performance against the initial untuned version as well as the linear models.","c870db5a":"First, let's look at the __last row__ in the heatmap, which shows the __correlation between each individual variable and the target__. We observe that most variables are postively correlated to our target, with the overall quality ('OverallQual') and logarithmic transformation of the ground living area ('LogGrLivArea') being particularly strong predictors. This makes sense as larger houses are expected to cost more.\n\nAs the variables 'OverallCond' and 'YrSold' have almost no relationship with the target, we will drop these variables. \n\nHowever, there may potentially be __multicollinearity__ present in our data, with some pairs of independent variables having high correlation with each other. In terms of multicollinearity between independent variables, there is no hard cutoff to remove variables. But as a general rule of thumb, attention should be placed on variable pairs that have correlations around 0.8 or higher. Highly correlated variable pairs:\n1. LogBsmtFinSF1 and BsmtFinType1 \n2. 1stFlrSF and TotalBsmtSF\n3. TotRmsAbrGrd and LogGrLivArea\n4. FirePlaceQu and FirePlaces\n5. GarageCars and GarageArea\n\nWe will drop GarageCars as it is clearly correlated with the size of the garage (GarageArea).\n\nFor the rest of the correlated variable pairs, we will just keep them in mind for now, and proceed with modelling with the variables included. Later on, we can use regularisation which helps deal with multicollinearity. ","5c117327":"__*Alternatively*__, we could have used a cross validated __GridSearch__ to find the optimal alpha as well, and the steps are shown below. \n\nUltimately, we will end up with the same alpha but its just a personal preference that i like using cross_validate and making my own functions.","ad0a55c5":"### 4.2 Linear Regression Model - Baseline <a id='linear'><\/a>\n\nThe linear regression model is a good baseline model for comparison. There are no hyper parameters to tune and there is no need to transform the data before fitting the model.","55eca6a5":"## 4.5 Linear Regression with Smaller Dataset <a id='linear_small'><\/a>","3a4efc7a":"# Upcoming Sections\n\nIf you are reading this, thanks for making it this far! I hope this notebook has been helpful! (:\n\nI plan to update this notebook over the next few days covering the sections:\n* Ridge Regression post feature engineering\n* Gradient Boosting \n* Model Comparisons and Conclusion\n\nLooking forward to any feedback \/ comments on ways to improve! If you have any questions, i'll try to answer to the best of abilities. (:\n\nThanks and have a great day!","1ca869ff":"### 3.2.2 Numeric Variables - Bivariate Analysis <a id='num_var_bivariate'><\/a>\n\nFor bivariate analysis, we are interested in the following:\n1. Relationship between each independent variable and target (LogSalePrice)\n2. Relationship between the independent variables - Checking for multicollinearity\n\nWe can obtain a quantitative understanding of the relationship by calculating the pearson's coefficient between the variables. \n\nWe can visualise the relationships using scatterplots and a correlation heatmap.\n\nNote : For a clearer understanding of why detecting multicollinearity is important, i found this [article][1] very helpful.\n\n*Future Work : Calculate correlations between binary - continuous variables using [Point Biserial Correlation][2].*\n\n*Future Work : Calculate correlations between categorical - continuous variables using Spearman's Rho.*\n\n\n[1]: https:\/\/statisticsbyjim.com\/regression\/multicollinearity-in-regression-analysis\/\n[2]: https:\/\/www.statisticssolutions.com\/point-biserial-correlation\/","59db9a96":"It appears that we have a bit of positive skew, we will try taking the logarithmic transformation to see if it alleviates this issue.","71db793f":"From the results above, we observe that there is definitely Multicollinearity in our variables, and more than the number of variables we identified previously.\n\nGiven that there is Multicollinearity present, does that mean our model is unusable?\n\nActually, [not really][1]! The primary concern of multicollinearity is that the estimates of the regression model become unstable and the standard errors of the coefficients become inflated.\n\nThis problem only really affects us when we are trying to __use regression for causal inference__ on the effects of our independent variables on our target. However in terms of __predictive performance__, multicollinearity does not adversely affect the results. \n\nFurthermore, the next two models (Ridge and Lasso) that we will be working with have regularisation paramters that help reduce the negative effect of multicollinearity.\n\nFor Reference:\n* [Discussion on Multicollinearity in Machine Learning.][2]\n* [Times where Multicollinearity is not that big of an issue.][3]\n\n[1]: https:\/\/www.lexjansen.com\/wuss\/2018\/131_Final_Paper_PDF.pdf\n[2]: https:\/\/stats.stackexchange.com\/questions\/168622\/why-is-multicollinearity-not-checked-in-modern-statistics-machine-learning\n[3]: https:\/\/statisticalhorizons.com\/multicollinearity","4784d4b0":"Looking at the variable distributions, we see that we still have some variables have classes that have very few observations. Therefore, we may want to lump similar classes together or under one category as 'Others'.\n\nFor variables that are not so obvious how to encode or if we are unsure how to encode them, we will just leave them as is.","663552d8":"### 3.3.2 Categorical Variables - Bivariate Analysis\n\nNow that we have finished processing the categorical variables individually, we can explore the relationship between their individual levels and the target variable. ","30515e64":"Since we have some variables that are highly skewed, we would like to visualise them to better understand how to handle them. The figure below is a grid of the distributions of these variables.","46f32b71":"### 4.3 Ridge Regression - L2 Regularization <a id='ridge'><\/a>\n\nRidge Regression is a regularisation method to reduce the variance of the model in exchange for a tolerable increase in the bias of our model.  \n\nThe penalty term (lambda \/ alpha) regularizes the coefficients such that if the coefficients take large values the optimization function is penalized. So, ridge regression __shrinks the coefficients__ and it helps to reduce the model complexity and multi-collinearity.\n\nFor more information on how Ridge Regression works, i found [this article][1] very helpful.\n\nReferences : [Rules of thumb for applying Ridge Regression][2]\n\n[1]: https:\/\/towardsdatascience.com\/ridge-regression-for-better-usage-2f19b3a202db\n[2]: https:\/\/stats.stackexchange.com\/questions\/169664\/what-are-the-assumptions-of-ridge-regression-and-how-to-test-them","032846ed":"For the following categorical variables, we will be replacing the missing values as a new category that indicates the property does not have that feature.","331740f4":"From our initial analysis, we observe that we have variables with missing data and some with potentially wrong datatypes (ordinal variables). We will address these issues below.","a7e3c5cb":"We observe that a large number of our variables were deemed as insignificant by the Lasso selection.\n\nWe will save the remaining variables as a new dataset and check if the lower dimensions help to alleviate the curse of dimensionality and improve our results for the earlier models. ","c7dea6ba":"We can also check for skewness or potential outliers in the numerical variables by using the .describe() method, which provides summary statistics for our numerical variables.","781069a2":"Now that we have created new features by log-transforming the positively skewed variable, let's see if there are any improvements. ","15b185ad":"Note: As we pass a custom scoring *__loss__* function as the scorer into __scikit-learn's cross_validate method__, the scores returned will be __negative__. In the functions above (hidden), we have made the necessary adjustments to return positive results.","34b083f6":"From the figure above, we observe that the actually the original distribution is less skewed than the log-transformed variable. The skew present in the original distribution is likely to be caused by outliers.\n\nUsing a scatteplot against the target variable, we can check for outliers.","40122808":"From the figure, we observe that there are some variables such as '3SsnPorch' and 'LowQualFinSF' that have very __narrow distributions__ (most of its values are concentrated with very few\/one values). \n\nAn option is to binary encode these variables into a categorical variable of having the most common value versus other values. However, as these variables have values that are mostly concentrated in one value, there is not much variation and hence not much information within the variable. Therefore, we will be dropping such variables with more than 80% of their values being concentrated in a single value. \n\nAs some of the variables are postively skewed with no negative values, therefore, we will apply a logarithmic transformation and check if it helps alleviate the skewness.\n\nFor the other oridinal variables that are skewed we will just leave them as is. ","86bf02f9":"Now that we have our baseline result, we can use this as a benchmark to evaluate the performance of other algorithms.","62ffd15c":"### 2.3 Correcting Data Types <a id='data_types'><\/a>\n\nIn our initial analysis, we noted some variables being stored as numerical when they should be categorical, as well as some categorical variables that can be stored as ordinal variables due to the inate hierarchy in the variable. Therefore, this section seeks to address these problems.","113efcf5":"As mentioned previously, Lasso helps to do feature selection by setting variable coefficients to 0. Let's see which variables were removed from the Lasso Regression.","672cb619":"### 3.3.1 Categorical Variables Analysis <a id='cat_var_univariate'><\/a>\n\nSimilar to numerical variables univariate analysis, we are interested in the distribution of categorical variables across their individual classes. We also do not want categorical variables that are majority concentrated within one class. ","b7bc9405":"The untuned Random Forest Regression has worse performance than the linear models. Let's see how much the model performance increases as we tuned the hyper parameters.\n\nSo how do we go about tuning the random forest? One method is to create a range of values for each hyper parameter and shove it all into a GridSearchCV, and it will output the best combination of hyper parameters. However, this is computationally expensive and may not be feasible for larger datasets.\n\nPersonally, i like to tune each parameter separately and visualise how it affects the model performance. After i have a good idea how each hyper parameter affects the model performance, i will fit a smaller and more targeted parameter grid into the GridSearch.\n\nThe hyper parameters that i mainly care about are the regularisation parameters:\n1. min_samples_split - number of samples to have in a node to split\n2. min_samples_leaf - number of samples to have in each leaf node\n3. max_leaf_nodes - maximum number of leaf nodes\n4. max_depth - maximum depth of the tree can grow\n5. max_features - percentage of total features included to train each tree\n\nNote: For further reading, you can check out these [Bayesian Optimisation Methods][1] for hyper parameter tuning as well.\n\n[1]: https:\/\/roamanalytics.com\/2016\/09\/15\/optimizing-the-hyperparameter-of-which-hyperparameter-optimizer-to-use\/","7a45208e":"Now that we are done with preprocessing our data, we can move on to exploratory data analysis of our data!","75b4b939":"### 2.1 Loading the Data and Initial Analysis <a id='initial_analysis'><\/a>\n\nFirstly, we import our data and get an overall feel of what it contains.\n\nWe usually want to take a quick scan at the first few rows of our data and look our for any potential mistakes mentioned previously.\n\nFurthermore, i highly recommend looking through any __README__ or __DESCRIPTION__ files that come with the dataset. Often, they provide information on how categorical variables were encoded and may also provide explaination for encoding observations as missing.","1a9e8737":"Now that we have finished our univariate analysis and cleaning of our numerical data, we can proceed to bivariate analysis! ","89a01338":"For the variables that have low missing percentages (<1%), we will just drop the missing observations.","7198af25":"As per the submission requirements, we will be using the Root Mean Squared Error as our evaluation metric.","477902d6":"# 4. Modelling <a id='modelling'><\/a>\n\nIn this section we will be trying different machine learning algorithms and comparing their performances. For most of these algorithms, there will be some hyper-parameter tuning involved as well.\n\nModels that we will be comparing:\n1. Linear Models - Linear \/ Lasso \/ Ridge Regression\n2. Ensemble Bagging Model - Random Forest\n3. Ensemble Boosting Model - Gradient Boosting Regression","a2aba409":"The distribution of LogSalePrice is much more normally distributed, therefore, we will be using the log-transform as our new target variable. \n\nFor our bivariate analysis in later sections, we will be comparing the independent variables against the LogSalePrice.\n\nNote: For submission later, we need to take the __exponent of our predicted results__ to get the predicted SalePrice.\n\nNote: [Effect of transforming the targets in regression model.][1]\n\n[1]: https:\/\/scikit-learn.org\/stable\/auto_examples\/compose\/plot_transformed_target.html#sphx-glr-auto-examples-compose-plot-transformed-target-py","7abcb298":"### 4.1 Preparing Our Data <a id='preparing_data'><\/a>\n\nInitially, we split our data to analyse the different data types, therefore we need to concat the separate data frames back together.\n\nFurthermore, most algorithms in Scikit-Learn do not accept categorical variables, therefore we need to one hot encode our variables as well.","c3888cac":"The goal of this competition is to __predict__ housing prices. Given that our target varible (prices) are continuous in nature, we are trying to solve a __regression problem__.\n\nGiven that we are focused on obtaining the __most accurate predictions__, we just need to select the model that performs the best, given an evaluation metric. \n\nFor this competition, the evaluation metric chosen is the __Root Mean Squared Error (RMSE)__, and we will be trying to find the model that gives us the __lowest RMSE__.\n\nThe models that we will be considering in this notebook include:\n\n* __Linear Models__ : Linear Regression \/ Ridge Regression \/ Lasso Regression\n\n* __Ensemble Models__ : Random Forest \/ Gradient Boosting \n\nWithin linear models, we can compare the effects of different types of __Regularisation__ on the performance. As for the ensemble methods, we can compare performances between __Bootstrap Aggregation__ (Random Forest) and __Boosting__. Given that the dataset is not very large, we did not choose to implement deep learning models.\n\nNote : The correct evaluation metric to use varies from problem to problem. This [article][1] compares the differences between the two most common metrics for regression problems, the __Mean Absolute Error (MAE)__ and the __RMSE__.\n\n[1]: https:\/\/medium.com\/human-in-a-machine-world\/mae-and-rmse-which-metric-is-better-e60ac3bde13d","25a11b8e":"### 3.2.1 Numerical Variables - Univariate Analysis <a id='num_var_univariate'><\/a>\n\nTypically if the number of variables are small, i like to plot the individual distributions of the variables to get a better understanding of our data.\n\nHowever, in reality, the number of features within datasets are often too numerous and it takes too much time to look at each variable individually.\n\nTherefore, in these situations, it is more efficient to analyse the variables that are likely to cause problems in our modelling later on. One of the main problems is the variables having large positive or negative skew.\n\nUsually, variables are considered skewed if they have skew of magnitude > 0.5, and highly skewed when the magnitude > 1. \n\nHence, in this section, we will first identify which variables are very skewed and use visualisations to guide us on what approaches to handle these issues.\n\nReference : [Rule of thumb for identifying skewed variables.][1]\n\n[1]: https:\/\/stats.stackexchange.com\/questions\/245835\/range-of-values-of-skewness-and-kurtosis-for-normal-distribution","1932779d":"The correlation heatmap has provided us a rough idea of the relationships between the variables. Now, we will plot the scatter plots of each variable against the target variable.","04bb8be5":"We can see that the optimal alpha for Lasso Regression is likely to be somewhere between 0.0001 and 0.01.\n\nTherefore, let's iterate through these values and plot their respective RMSEs.","e58d0c48":"The observation that is greater than 6000 is clearly an outlier, therefore we will drop the observation and check back on our variable's skew after.","5658ade0":"We hypothesize that the Garage Year Built variable is highly correlated to the variable Year House Built.","0c58e27a":"From the scatterplots, it doesn't seem like we have any obvious outliers. Now that we have a good understanding of our numerical variables, we can move on to looking at our categorical variables.","62f94978":"# Walkthrough of Preprocessing || EDA || Modelling Results\n\n### Author : [Joshua Yeo][1]\n\n### Updated : 1 May 2020\n\nGreetings! Thank you for taking the time to view this notebook. If you use any parts of this notebook, it would be greatly appreciated if you would give credit by linking back to this notebook! (: \n\n[1]: https:\/\/github.com\/Joshuayeo95\n\n------","5635297e":"From the diagram, we observe that the model trained on the 5th fold had the lowest training error, but had the highest testing error. By visualising the training and test scores, we are able to tell which models are overfitting.\n\nThe best model from our cross validation is the one trained on the second fold. Despite having the highest training error, it had the lowest testing error, which indicates that it is better able to generalise and predict unseen data.\n\nTherefore, we will save the model in fold 2 and evaluate it once again on the holdout testing data (20% from the initial split) for comparison with other models.","5da12727":"For the variable 'LotFrontage', we chose to use a K-Nearest Neighbours imputation of the missing value. The rationale is that houses come in different sizes which would definitely influence the Lot Frontage of the property. Therefore, we believe that similar sized houses would have similar Lot Frontage, therefore, being a better imputation than just taking the median.","3e673534":"Let's visualise these variables and get a better understanding of their frequency distributions. Problematic variables would be those that have many categories but are sparsely distributed. For binary variables, the imbalanced class issue might not be as bad, as long as the imbalance is not too great. ","257bb718":"Nice! The Ridge Regression outperformed the baseline Linear Regression model!\n\nThis shows the benefits of regularisation and how it helps the model generalise to newer predictions.","4a240bfb":"For Cross Validation, we will be using the [cross_validate](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html) method from scikit-learn.\n\nI prefer using this method over [cross_val_score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_val_score.html) as it provides more information and i can check if the model is overfitting by comparing the training and validation scores.","1c329090":"Previously, we identified the possibilility of multicollinearity in our model due to having highly correlated pairs of variables. \n\nWe can check for multicollinearity by looking at the [Variance Inflation Values (VIFs)][1] of the variable coefficients from our Linear model. As a rule of thumb, VIFs greater than 5 are signs of potential multicollinearity.  \n\n[1]: https:\/\/online.stat.psu.edu\/stat462\/node\/180\/","9b502d5f":"From the figure above, we observe that as the max_leaf_nodes exceeds 60, as there is little improvement in testing data despite improving training performance. \n\nIn similar fashion, a depth greater than 10 does not seem to improve the model's performance.\n\nAs for min_samples_split and min_samples_leaf, it appears that smaller values offer better performance. \n\nAs for the max features to include, their performances are similar but taking the square roots seems to have marginally better performance.\n\nThis helps us shrink the grid search space as we are able to narrow our search to smaller range of values per parameter.","fb8a4b31":"It looks like the log transformation has corrected the skews for all but one variable. Therefore, we will __keep the log transformed variables for the successful transformation and drop the original variables__.\n\nFor the variable TotalBsmtSF that is still positively skewed, we will take a closer look at both its original and transfromed distributions.","bbabe606":"As expected, the two variables are highly correlated and we will be dropping the variable 'GarageYrBlt' as most of its information is captured in the variable 'YearBuilt'.","cee8250c":"# 1. Understanding the Problem <a id='problem'><\/a>","19b607cb":"Now, we need to split our data into training (80%) and testing (20%) data.\n\nFor hyper-parameter tunining, we will be using a 5-fold cross validation on the training dataset.","4f96749e":"## Table of Contents\n\n#### [1. Understanding the Problem](#problem)\n\n\n#### [2. Initial Data Preprocessing](#preprocessing)\n* [Getting a Feel of our Data](#initial_analysis)\n* [Handling Missing Data](#missing_data)\n* [Correcting Data Types](#data_types)\n    \n    \n#### [3. Exploratory Data Analysis and Further Processing](#eda)\n* [Target Variable Distribution](#target)\n* [Numerical Variables - Univariate Analysis](#num_var_univariate)\n* [Numerical Variables - Bivariate Analysis](#num_var_bivariate)\n* [Categorical Variables - Univariate Analysis](#cat_var_univariate)\n* [Categorical Variables - Bivariate Analysis](#cat_var_bivariate)\n\n\n#### [4. Modelling](#modelling)\n* [Preparing our Data](#preparing_data)\n* [Linear Models - Linear Regression](#linear)\n* [Linear Models - Ridge Regression](#ridge)\n* [Linear Models - Lasso Regression](#lasso)\n* [Linear Regression after Feature Selection](#linear_small)\n* [Ensemble - Random Forest Regression](#rforest)\n* Ensemble - Gradient Boosting Regression (work in progress)\n* Potential Feature Engineering (work in progress)\n\n#### 5. Evaluating Model Performances (work in progress)","d59f3316":"### Importing Libraries (expand to view)","88dfc352":"## 4.6 Random Forest Regression <a id='rforest'><\/a>\n\nHaving seen the performance of the linear models, let's compare their performance against other non-parametric ensemble methods.\n\nThe first ensemble model we will try is the Random Forest, which uses Bootstrap Aggregating (bagging). Unlike linear models, Random Forests have many hyper parameters to tune and we will compare the performance a baseline versus a fully tuned model.\n\nIf you are new to Random Forest Regression or decision trees in general, i would highly recommend checking out the YouTube channel, [StatQuest][1].\n\nNote : I took inspiration for some of the functions from this [kernel][2].\n\n[1]: https:\/\/www.youtube.com\/watch?v=J4Wdy0Wc_xQ&list=PLblh5JKOoLUIcdlgu78MnlATeyx4cEVeR&index=11&t=0s\n[2]: https:\/\/www.kaggle.com\/hadend\/tuning-random-forest-parameters","3b3aa58c":"Now that we have obtained the optimum alpha, we can fit a new Lasso model and see how it performs during cross validation.","09222fcf":"# 3. Exploratory Data Analysis <a id='eda'><\/a>\n\nIn this section, we will be taking a closer look at our data and using visualisations to better understand our data. As the type of analysis differs for numerical and categorical data, we will analyse them separately in their own sections.\n\nFor numerical variables, __univariate analysis__ will consist of checking the __variable's distribution__ (focusing on skewness) and performing __transformations__ if necessary. We will then analyse how each variable relates to our target variable, SalesPrice. \n\nFor __categorical variables__, we will look into the __variable's distribution across different categories__, before seeing the effect that each category has on our target variable. \n\n*Note: I have previously defined some helper functions that i use across various projects and have collapsed the code for cleaner presentation. Feel free to expand them for more clarity on what they are doing.* ","6989ac12":"Now that we have sucessfully dealt with the missing values, we need to check that they have correct data types.","e32c3f50":"The parameter grid below is a dictionary of the hyper paramters and an array of values to iterate through. I focus my attention on these variables as they help to regularise the model. \n\n* max_features - Helps to decorrelate the trees as a subset of features are selected when building each individual tree. \n* The others control how deep each tree is allowed to grow and reduces overfitting.\n\nNote : The parameters in max_features are the options availble in scikit-learn, with 'auto' being all features. These values for max_features are those that have been  empirically proven to give the best results.","1e0da9db":"From the figure above, as alpha increases, the regularisation strength increases, which shrinks the coefficients to a larger extent. This bias of shrinking the coefficients towards 0 causes the model to [underfit][1] to the training data, which can be observed by the increasing training error. \n\nHowever, this allows our model to generalise better to unseen data, as seen from the decrease in testing error as alpha increases. Although there will come a point where the marginal increase in bias out weighs the marginal decrease in variance, and performance starts to deteriorate due to the model underfitting to unseen data as well.\n\nFrom the cross validation, we found that the optimal alpha is 128, and we can proceed to testing the model on the holdout dataset.\n\n[1]: https:\/\/stats.stackexchange.com\/questions\/351990\/why-dont-we-want-to-choose-a-big-lambda-in-ridge-regression"}}