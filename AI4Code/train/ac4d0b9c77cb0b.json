{"cell_type":{"623d35f0":"code","cc499374":"code","1926c3c6":"code","5f9574ca":"code","d65b38f1":"code","665de173":"code","f874d590":"code","e0243977":"code","06f06dae":"code","c9379441":"code","46637cd0":"code","ece2b335":"code","95750332":"code","b03fd465":"code","91413510":"code","a305983c":"code","43c19370":"code","0b8f20b1":"code","6356df00":"code","4609b788":"code","5b1159fe":"code","7da45b0b":"code","3c64c61c":"code","c40508fd":"code","94855e01":"code","40feb02d":"code","3c2225af":"code","c9d3a961":"code","f853cbcd":"code","59a72bd3":"code","ff1733bc":"code","c26dc1e4":"code","73dd3166":"code","d5b59ffb":"code","e21604ed":"code","9d8df7c2":"code","59ace619":"code","df069ce3":"code","96ee95fc":"code","d045b106":"code","161ca37a":"code","bd25591c":"code","ce86cf5a":"code","65416179":"code","72d601ed":"markdown","914ea4bc":"markdown","06ab8983":"markdown","150a07ac":"markdown","527ec213":"markdown","5cf821ca":"markdown","d5860eed":"markdown","8c9c1e17":"markdown","1f4b0267":"markdown","1998aa53":"markdown","50a72aa3":"markdown"},"source":{"623d35f0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cc499374":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport zipfile","1926c3c6":"df1 = pd.read_csv('..\/input\/data-clinical\/data.csv')\ndf1.shape","5f9574ca":"df = df1[['text','mort_icu']]\ndf1 = None\ndf['text'] = df['text'].astype(str)\n\ndf = df.dropna() \n#df = df.apply(np.random.permutation, axis=1)\ndf = df.rename(columns={'mort_icu': 'label'}) \nimport sklearn.utils\ndf = sklearn.utils.shuffle(df)","d65b38f1":"import nltk\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\n\nimport pandas as pd\nimport random, time\nfrom babel.dates import format_date, format_datetime, format_time\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, accuracy_score\n\n\nimport torch\nfrom torch import Tensor\nfrom torch import nn, optim\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nimport torch.nn.functional as F\n!pip install transformers \nimport transformers, os\nfrom transformers import BertModel, AutoModel, AdamW, get_linear_schedule_with_warmup, BertTokenizer, BertForSequenceClassification","665de173":"# Check device \n# Get the GPU device name if available.\nif torch.cuda.is_available():    \n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available. {}'.format(torch.cuda.device_count()))\n    print('We will use the GPU: {}'.format(torch.cuda.get_device_name(0)))\n\n# If we dont have GPU but a CPU, training will take place on CPU instead\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")\n    \ntorch.cuda.empty_cache()\n    \n# Set the seed value all over the place to make this reproducible.\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","f874d590":"df.describe()","e0243977":"df.info()","06f06dae":"# get length of all the text in the dataframe\nseq_len_premise = [len(i.split()) for i in df['text']]\n\npd.Series(seq_len_premise).hist(bins = 25)","c9379441":"# Plot the count of fake and true news \nsns.countplot(df['label'])","46637cd0":"#  Preprocess train dataset\n# remove special characters from text column\ndf.text = df.text.str.replace('[#,@,&]', '')\n# Remove digits\ndf.text = df.text.str.replace('\\d*','')\n#Remove www\ndf.text = df.text.str.replace('w{3}','')\n# remove urls\ndf.text = df.text.str.replace(\"http\\S+\", \"\")\n# remove multiple spaces with single space\ndf.text = df.text.str.replace('\\s+', ' ')\n#remove all single characters\ndf.text = df.text.str.replace(r'\\s+[a-zA-Z]\\s+', '')\n\n# Remove english stopwords\n#f['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))","ece2b335":"# Split test and train data using 25% of the dataset for validation purposes\nx_train, x_test, y_train, y_test = train_test_split(df['text'], \n                                                      df['label'], test_size=0.25, shuffle=True, random_state=42)","95750332":"# Obtain a 10% test set from train set\nX_train_Transformer, X_val_Transformer, y_train_Transformer, y_val_Transformer = train_test_split(\n                                                    x_train, y_train, test_size=0.20, random_state=42)","b03fd465":"model_name = 'emilyalsentzer\/Bio_ClinicalBERT'\n\n#tokenizer = transformers.BertTokenizer.from_pretrained('emilyalsentzer\/Bio_ClinicalBERT', do_lower_case=True)\n#bert_model = transformers.BertModel.from_pretrained('emilyalsentzer\/Bio_ClinicalBERT')\n\nSEQ_LEN = 128\nbatch_size = 16\nepochs = 2\nlearning_rate = 1e-3 # Controls how large a step is taken when updating model weights during training.\nsteps_per_epoch = 20\nnum_workers = 2","91413510":"def get_split(text1):\n    '''Get split of the text with 200 char lenght'''\n    l_total = []\n    l_parcial = []\n    if len(text1.split())\/\/150 >0:\n        n = len(text1.split())\/\/150\n    else: \n        n = 1\n    for w in range(n):\n        if w == 0:\n            l_parcial = text1.split()[:200]\n            l_total.append(\" \".join(l_parcial))\n        else:\n            l_parcial = text1.split()[w*150:w*150 + 200]\n            l_total.append(\" \".join(l_parcial))\n    return str(l_total)\n\n# Splits train and validation sets to be feed to the transformer which only accepts 512 tokens maximum\nsplit_train_text = [get_split(t) for t in X_train_Transformer]\nsplit_valid_text = [get_split(t) for t in X_val_Transformer]\nsplit_test_text = [get_split(t) for t in x_test]","a305983c":"#split_valid_text","43c19370":"# Load the RoBERTa tokenizer and tokenize the data\nprint('Loading BERT tokenizer...')\ntokenizer = transformers.BertTokenizer.from_pretrained('emilyalsentzer\/Bio_ClinicalBERT', do_lower_case=True)\n\n#tokenizer = transformers.BertTokenizer.from_pretrained('emilyalsentzer\/Bio_ClinicalBERT', do_lower_case=True)\n#bert_model = transformers.BertModel.from_pretrained('emilyalsentzer\/Bio_ClinicalBERT')","0b8f20b1":"trencoding = tokenizer.batch_encode_plus(\n  list(split_train_text),\n  max_length=SEQ_LEN,\n  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n  return_token_type_ids=True,\n  truncation=True,\n  padding='longest',\n  return_attention_mask=True,\n)\n\nvalencoding = tokenizer.batch_encode_plus(\n  list(split_valid_text),\n  max_length=SEQ_LEN,\n  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n  return_token_type_ids=True,\n  truncation=True,\n  padding='longest',\n  return_attention_mask=True,\n)\n\n\ntestencoding = tokenizer.batch_encode_plus(\n  list(split_test_text),\n  max_length=SEQ_LEN,\n  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n  return_token_type_ids=True,\n  truncation=True,\n  padding='longest',\n  return_attention_mask=True,\n)\n","6356df00":"tokenizer.special_tokens_map","4609b788":"trencoding.keys()","5b1159fe":"#compute the class weights\nclass_wts = compute_class_weight('balanced', np.unique(df['label'].values.tolist()), \n                                 df['label'])\n\n#print(class_wts)\n\n# convert class weights to tensor\nweights= torch.tensor(class_wts,dtype=torch.float)\nweights = weights.to(device)\n\n# loss function\n#cross_entropy  = nn.NLLLoss(weight=weights) \ncross_entropy  = nn.CrossEntropyLoss(weight=weights)","7da45b0b":"def loadData(prep_df, batch_size, num_workers, sampler):\n    \n    return  DataLoader(\n            prep_df,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            sampler=sampler,\n            pin_memory=True\n        )\n\n## convert lists to tensors\ntrain_seq = torch.tensor(trencoding['input_ids'])\ntrain_mask = torch.tensor(trencoding['attention_mask'])\ntrain_token_ids = torch.tensor(trencoding['token_type_ids'])\ntrain_y = torch.tensor(y_train_Transformer.tolist())\n\nval_seq = torch.tensor(valencoding['input_ids'])\nval_mask = torch.tensor(valencoding['attention_mask'])\nval_token_ids = torch.tensor(valencoding['token_type_ids'])\nval_y = torch.tensor(y_val_Transformer.tolist())\n\ntest_seq = torch.tensor(testencoding['input_ids'])\ntest_mask = torch.tensor(testencoding['attention_mask'])\ntest_token_ids = torch.tensor(testencoding['token_type_ids'])\ntest_y = torch.tensor(y_test.tolist())\n\n# wrap tensors\ntrain_data = TensorDataset(train_seq, train_mask, train_token_ids, train_y)\n# sampler for sampling the data during training\ntrain_sampler = RandomSampler(train_data)\n# Train Data Loader\ntraindata = loadData(train_data, batch_size, num_workers, train_sampler)\n\n# wrap tensors\nval_data = TensorDataset(val_seq, val_mask, val_token_ids, val_y)\n# sampler for sampling the data during training\nval_sampler = SequentialSampler(val_data)\n# Val Data Loader\nvaldata = loadData(val_data, batch_size, num_workers, val_sampler)\n\n# wrap tensors\ntest_data = TensorDataset(test_seq, test_mask, test_token_ids, test_y)\n# sampler for sampling the data during training\ntest_sampler = SequentialSampler(test_data)\n# Val Data Loader\ntestdata = loadData(test_data, batch_size, num_workers, test_sampler)\n\n\nprint('Number of data in the train set', len(traindata))\nprint('Number of data in the validation set', len(valdata))\nprint('Number of data in the test set', len(testdata))","3c64c61c":"batch_size","c40508fd":"class BERT_Arch(nn.Module):\n    \n    def __init__(self, n_classes, freeze_bert=False):\n        \n        super(BERT_Arch,self).__init__()\n        # Instantiating BERT model object\n        self.bert = BertModel.from_pretrained(model_name, return_dict=False)\n        \n        # Freeze bert layers\n        if freeze_bert:\n            for p in self.bert.parameters():\n                p.requires_grad = False\n                \n        self.bert_drop_1 = nn.Dropout(0.3)\n        self.fc = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size) # (768, 64)\n        self.bn = nn.BatchNorm1d(768) # (768)\n        self.bert_drop_2 = nn.Dropout(0.25)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes) # (768,2)\n\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        _, output = self.bert(\n            input_ids = input_ids,\n            attention_mask = attention_mask,\n            token_type_ids = token_type_ids\n        )\n        output = self.bert_drop_1(output)\n        output = self.fc(output)\n        output = self.bn(output)\n        output = self.bert_drop_2(output)\n        output = self.out(output)        \n        return output","94855e01":"class_names = np.unique(df['label'])\nprint('Downloading the BERT custom model...')\nmodel = BERT_Arch(len(class_names))\nmodel.to(device) # Model to GPU.\n\n#optimizer parameters\nparam_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [{'params': [p for n, p in param_optimizer \n                                    if not any(nd in n for nd in no_decay)],'weight_decay':0.001},\n                        {'params': [p for n, p in param_optimizer \n                                    if any(nd in n for nd in no_decay)],'weight_decay':0.0}]\n\nprint('Preparing the optimizer...')\n#optimizer \noptimizer = AdamW(optimizer_parameters, lr=learning_rate)\nsteps = steps_per_epoch\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps = 0,\n    num_training_steps = steps\n)","40feb02d":"# function to train the bert model\ndef trainBERT():\n  \n    print('Training...')\n    model.train()\n    total_loss, total_accuracy = 0, 0\n\n    # empty list to save model predictions\n    total_preds=[]\n\n    # iterate over batches\n    for step, batch in enumerate(traindata):\n    \n        # progress update after every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(traindata)))\n\n        if torch.cuda.is_available():\n            # push the batch to gpu\n            batch = [r.to(device) for r in batch]\n\n        sent_id, mask, token_type_ids, labels = batch\n        # clear previously calculated gradients \n        model.zero_grad()        \n        # get model predictions for the current batch\n        preds = model(sent_id, mask, token_type_ids)\n        # compute the loss between actual and predicted values\n        loss = cross_entropy(preds, labels)\n        # add on to the total loss\n        total_loss = total_loss + loss.item()\n        # backward pass to calculate the gradients\n        loss.backward()\n        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        # update parameters\n        optimizer.step()\n        # model predictions are stored on GPU. So, push it to CPU\n        preds=preds.detach().cpu().numpy()\n        # append the model predictions\n        total_preds.append(preds)\n        \n        torch.cuda.empty_cache()\n\n    # compute the training loss of the epoch\n    avg_loss = total_loss \/ len(traindata)\n\n    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_preds  = np.concatenate(total_preds, axis=0)\n\n    #returns the loss and predictions\n    return avg_loss, total_preds","3c2225af":"# function for evaluating the model\ndef evaluate():\n  \n    print(\"\\nEvaluating...\")\n    t0 = time.time()\n    \n    model.eval() # deactivate dropout layers\n    total_loss, total_accuracy = 0, 0\n    \n    # empty list to save the model predictions\n    total_preds = []\n\n    # iterate over batches\n    for step, batch in enumerate(valdata):\n        # Progress update every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(valdata)))\n\n        if torch.cuda.is_available():\n            # push the batch to gpu\n            batch = [t.to(device) for t in batch]\n\n        sent_id, mask, token_type_ids, labels = batch\n\n        # deactivate autograd\n        with torch.no_grad(): # Dont store any previous computations, thus freeing GPU space\n\n            # model predictions\n            preds = model(sent_id, mask, token_type_ids)\n            # compute the validation loss between actual and predicted values\n            loss = cross_entropy(preds, labels)\n            total_loss = total_loss + loss.item()\n            preds = preds.detach().cpu().numpy()\n            total_preds.append(preds)\n\n        torch.cuda.empty_cache()\n    # compute the validation loss of the epoch\n    avg_loss = total_loss \/ len(valdata) \n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_preds  = np.concatenate(total_preds, axis=0)\n\n    return avg_loss, total_preds","c9d3a961":"# set initial loss to infinite\nbest_valid_loss = float('inf')\n\n# Empty lists to store training and validation loss of each epoch\ntrain_losses=[]\nvalid_losses=[]\n\n# for each epoch perform training and evaluation\nfor epoch in range(epochs):\n     \n    print('\\n Epoch {:} \/ {:}'.format(epoch + 1, epochs))\n    \n    #train model\n    train_loss, _ = trainBERT()\n    \n    #evaluate model\n    valid_loss, _ = evaluate()\n    \n    print('Evaluation done for epoch {}'.format(epoch + 1))\n    #save the best model\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        print('Saving model...')\n        torch.save(model.state_dict(), 'bert_weights.pt') # Save model weight's (you can also save it in .bin format)\n    \n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    \n    print(f'\\nTraining Loss: {train_loss:.3f}')\n    print(f'Validation Loss: {valid_loss:.3f}')","f853cbcd":"print('\\nTest Set...')\n\ntest_preds = []\n\nprint('Total batches:', len(testdata))\n\nfor fold_index in range(0, 3):\n    \n    print('\\nFold Model', fold_index)\n    \n    # Load the fold model\n    path_model = 'bert_weights.pt'\n    model.load_state_dict(torch.load(path_model))\n\n    # Send the model to the GPU\n    model.to(device)\n\n    stacked_val_labels = []\n    \n    # Put the model in evaluation mode.\n    model.eval()\n\n    # Turn off the gradient calculations.\n    # This tells the model not to compute or store gradients.\n    # This step saves memory and speeds up validation.\n    torch.set_grad_enabled(False)\n\n\n    # Reset the total loss for this epoch.\n    total_val_loss = 0\n\n    for j, test_batch in enumerate(testdata):\n\n        inference_status = 'Batch ' + str(j + 1)\n\n        print(inference_status, end='\\r')\n\n        b_input_ids = test_batch[0].to(device)\n        b_input_mask = test_batch[1].to(device)\n        b_token_type_ids = test_batch[2].to(device)\n        b_test_y = test_batch[3].to(device)\n\n\n        outputs = model(b_input_ids, \n                        attention_mask=b_input_mask,\n                        token_type_ids=b_token_type_ids)\n\n        # Get the preds\n        preds = outputs[0]\n\n        # Move preds to the CPU\n        val_preds = preds.detach().cpu().numpy()\n        \n        #true_labels.append(b_test_y.to('cpu').numpy().flatten())\n        \n        # Stack the predictions.\n        if j == 0:  # first batch\n            stacked_val_preds = val_preds\n            \n        else:\n            stacked_val_preds = np.vstack((stacked_val_preds, val_preds))\n            \n    test_preds.append(stacked_val_preds)\n    \n            \nprint('\\nPrediction complete.')","59a72bd3":"print(len(test_preds))\nprint(test_preds[:5])","ff1733bc":"# Sum the predictions of all fold models\nfor i, item in enumerate(test_preds):\n    if i == 0:\n        preds = item\n    else:\n        # Sum the matrices\n        preds = item + preds\n\n# Average the predictions\navg_preds = preds\/(len(test_preds))\n\n#print(preds)\n#print()\n#print(avg_preds)\n\n# Take the argmax. \n# This returns the column index of the max value in each row.\ntest_predictions = np.argmax(avg_preds, axis=1)\n\n# Take a look of the output\nprint(type(test_predictions))\nprint(len(test_predictions))\nprint()\nprint(test_predictions)","c26dc1e4":"true_y = []\nfor j, test_batch in enumerate(testdata):\n    true_y.append(int(test_batch[3][0].numpy().flatten()))\nprint(true_y)","73dd3166":"# Accuracy and classification report \ntarget_names = ['true_y', 'predicted_y']\n\ndata = {'true_y': true_y,\n       'predicted_y': test_predictions}\n\ndf_pred_BERT = pd.DataFrame(data, columns=['true_y','predicted_y'])\n\nconfusion_matrix = pd.crosstab(df_pred_BERT['true_y'], df_pred_BERT['predicted_y'], rownames=['True'], colnames=['Predicted'])\n\nsns.heatmap(confusion_matrix, annot=True)\nplt.show()","d5b59ffb":"print('Accuracy of BERT model', accuracy_score(true_y, test_predictions))","e21604ed":"print(classification_report(true_y, test_predictions, target_names=target_names))","9d8df7c2":"# Create a Pipeline with the TfidfVectorizer and LogisticRegression model\nLR_pipeline = Pipeline(steps = [('tf', TfidfVectorizer()), \n                                ('lgrg', LogisticRegression())]) # initialize TfidfVectorizer and LogisticRegression\n\n\n# Create Parameter Grid\npgrid_lgrg = {\n 'tf__max_features' : [1000, 2000, 3000],\n 'tf__ngram_range' : [(1,1),(1,2)],\n 'tf__use_idf' : [True, False],\n 'lgrg__penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n 'lgrg__class_weight' : ['balanced', None]\n}\n\n# Apply GridSearch to Pipeline to find the best parameters\ngs_lgrg = GridSearchCV(LR_pipeline, pgrid_lgrg, cv=2, n_jobs=-1, verbose=2)","59ace619":"gs_lgrg.fit(x_train, y_train) # Train LR model","df069ce3":"gs_lgrg.best_params_","96ee95fc":"print('Score of train set', gs_lgrg.score(x_train, y_train))\nprint('Score of test set',gs_lgrg.score(x_test, y_test))","d045b106":"LR_pred = gs_lgrg.predict(x_test) # Predict on validation data\n\ndata = {'true_y': y_test,\n       'predicted_y': LR_pred}\ndf_pred = pd.DataFrame(data, columns=['true_y','predicted_y'])\nconfusion_matrix = pd.crosstab(df_pred['true_y'], df_pred['predicted_y'], rownames=['True'], colnames=['Predicted'])\n\nsns.heatmap(confusion_matrix, annot=True)\nplt.show()","161ca37a":"print('Accuracy of LR model', accuracy_score(y_test, LR_pred))","bd25591c":"print(classification_report(y_test, LR_pred, target_names=target_names))","ce86cf5a":"import numpy as np\nfrom sklearn import metrics\nfpr, tpr, thresholds = metrics.roc_curve(y_test, LR_pred,  pos_label =1)\nmetrics.auc(fpr, tpr)","65416179":"from sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_test,gs_lgrg.decision_function(x_test))","72d601ed":"## Term frequencies","914ea4bc":"# Try transformer model","06ab8983":"## Find Class Weights","150a07ac":"## Wordclouds","527ec213":"# Try Logistic regression","5cf821ca":"# Get to know the data","d5860eed":"## Train the BERT model","8c9c1e17":"# Load BERT model","1f4b0267":"As we can see, we are obtaining a 0.80 acc by just training a Logistic Regression model. Sometimes the simplest solution is the best choice to solve a certain task if it can save us computation time. In any case, the results are very similar.","1998aa53":"# Cleaning the data","50a72aa3":"In this section I am going to train a Logistic Regression model by using a Pipeline containing the TfidfVectorizer and LogisticRegression. Also, I am going to apply a GridSearchCV to the Pipeline to find the best parameters for the model. This is going to find the optimal parameters, however, it's a bit time consuming."}}