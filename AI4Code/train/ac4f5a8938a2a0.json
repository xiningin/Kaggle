{"cell_type":{"edfa7634":"code","782520b7":"code","8c916e8d":"code","302a5d16":"code","280eaf8f":"code","93018432":"code","1b4ad2a1":"code","192dffb2":"code","77022fe7":"code","71e9017b":"code","9c48a18e":"code","8400d0f7":"code","eefa1fc6":"code","60b9ad10":"code","2a0b255e":"code","f16d96ef":"code","418f53d5":"code","a36a8940":"code","ae90fe0d":"code","92fee453":"code","837bcfff":"code","83a8e992":"code","784a4060":"code","a7094fd9":"code","2736be58":"code","4d9069ec":"code","f95fbfe5":"code","9b65758d":"code","ae67d398":"code","d448e06c":"code","2fe0366f":"code","87d44c35":"code","d98b8fdd":"code","26791b37":"code","ff3738df":"code","eb9aa99c":"code","4b91fdba":"code","a9c563a9":"code","569c3320":"code","ac639c1b":"code","081eecd8":"code","9ef6353a":"code","172f9053":"code","29d54524":"code","51c51661":"code","ff5e35db":"code","11ee20f1":"code","1a22d908":"code","c505f5bd":"code","a8689c5a":"code","7b8cb0c1":"code","963851b4":"code","c6bb9029":"code","04488f08":"markdown","6fca7c55":"markdown","41af355d":"markdown","76c20741":"markdown","ad5a5766":"markdown","f01ff10f":"markdown","915c8fe1":"markdown","2a4ec66f":"markdown","b1707ebf":"markdown","b59730a3":"markdown","32e47fab":"markdown","452562b5":"markdown","5f5aae3d":"markdown","9fa53b61":"markdown","6cda872e":"markdown","7a70853d":"markdown","9b9c9a0c":"markdown","0295ce86":"markdown","5ee3bf9c":"markdown","17a967f0":"markdown","53b0b8de":"markdown","40373b8c":"markdown","6a6fcc5c":"markdown","9c72069e":"markdown","d4abdb9c":"markdown","8d35b9a3":"markdown","55a2268d":"markdown","eb5b2892":"markdown","25933144":"markdown","6bfecb11":"markdown"},"source":{"edfa7634":"import pandas as pd\nfrom nltk.tag import UnigramTagger\nfrom nltk.corpus import treebank\nimport nltk\nimport re\nimport matplotlib.pyplot as plt\n\nreplacement_patterns = [\n    (r'won\\'t', 'will not'),\n    (r'can\\'t', 'cannot'),\n    (r'i\\'m', 'i am'),\n    (r'ain\\'t', 'is not'),\n    (r'(\\w+)\\'ll', '\\g<1> will'),\n    (r'(\\w+)n\\'t', '\\g<1> not'),\n    (r'(\\w+)\\'ve', '\\g<1> have'),\n    (r'(\\w+)\\'s', '\\g<1> is'),\n    (r'(\\w+)\\'re', '\\g<1> are'),\n    (r'(\\w+)\\'d', '\\g<1> would'),\n]\n\nclass RegexpReplacer(object):\n    def __init__(self, patterns=replacement_patterns): \n        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n    def replace(self, text):\n        s = text\n        for (pattern, repl) in self.patterns:\n            s = re.sub(pattern, repl, s) \n        return s\n\nreplacer=RegexpReplacer()\nreplacer.replace(\"Don't hesistate to ask questions\")\n\nfrom sklearn.model_selection import train_test_split\n\nimport math\nimport random\nfrom collections import defaultdict\nfrom pprint import pprint\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nimport re\nimport string\nfrom sklearn.metrics import mean_squared_error\nimport nltk\n\n# Prevent future\/deprecation warnings from showing in output\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\nimport numpy as np\nimport pandas as pd\n\n\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import TreebankWordTokenizer\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import sentiwordnet as swn\nfrom bs4 import BeautifulSoup             \nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom nltk.corpus import stopwords \nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n\n#Visualization packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import mean_squared_error\nimport random\n","782520b7":"import nltk\nnltk.download('treebank')\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","8c916e8d":"#file_id='1DiCkP6qwCxIPK2TuK47US_QTVEminv5T'\n#link='https:\/\/drive.google.com\/uc?export=download&id={FILE_ID}'\n#csv_url=link.format(FILE_ID=file_id)\n\n#original_dataset = pd.read_csv(csv_url, sep=';', index_col='Unnamed: 0')\n\ncolumn_names = ['reviews.rating','reviews.text']\noriginal_dataset = pd.read_csv('http:\/\/christophe-rodrigues.fr\/eval_reviews.csv', usecols=column_names, sep=\";\")\n\n","302a5d16":"original_dataset.head()","280eaf8f":"#Dimension of the dataset\noriginal_dataset.shape","93018432":"original_dataset.describe()","1b4ad2a1":"original_dataset.isna().sum()","192dffb2":"original_dataset = original_dataset[original_dataset['reviews.text']!='MoreMore']","77022fe7":"original_dataset.shape","71e9017b":"original_dataset['reviews.rating'].value_counts()","9c48a18e":"original_dataset['reviews.rating'].value_counts().plot.bar(color='blue')","8400d0f7":" \ndef preprocess_text(test):\n\n    #Convert the text to lowercase\n    test = test.lower()\n\n    #Removing Numbers\n    test=re.sub(r'\\d+','',test)\n\n\n    \n    #Removing white spaces\n    test=test.strip()\n    \n    #Replacer replace\n    text_replaced = replacer.replace(test)\n    \n\n    \n    #Tokenize\n    tokenizer = nltk.data.load('tokenizers\/punkt\/english.pickle')\n    sentences = tokenizer.tokenize(text_replaced)\n\n    #Tokenize words\n    from nltk.tokenize import RegexpTokenizer\n    tokenizer=RegexpTokenizer(\"[\\w]+\")\n\n    for i in range(len(sentences)):\n        sentences[i] = tokenizer.tokenize(sentences[i])\n\n    #Remove stop words\n\n    from nltk.corpus import stopwords\n    stops=set(stopwords.words('english'))\n\n    for i in range(len(sentences)):\n        sentences[i] = [word for word in sentences[i] if word not in stops]\n\n    #Lemmatize\n\n    from nltk.stem import WordNetLemmatizer\n    lemmatizer_output=WordNetLemmatizer()\n\n    for i in range(len(sentences)):\n        for j in range(len(sentences[i])):\n            sentences[i][j] = lemmatizer_output.lemmatize(sentences[i][j])\n\n\n    #Join the words back into a sentence.\n    a=[' '.join(s) for s in sentences]\n    b=['. '.join(a)]\n\n    return b \n","eefa1fc6":"review_clean = [preprocess_text(doc) for doc in original_dataset['reviews.text']]\nsentences = [' '.join(r) for r in review_clean]\n","60b9ad10":"original_dataset['text_cleaned']=sentences\noriginal_dataset.head()","2a0b255e":"dataset = original_dataset.copy()","f16d96ef":"dataset[dataset['reviews.rating'] != 3]\ndataset['labels'] = np.where(dataset['reviews.rating'] > 2, 1, 0)","418f53d5":"dataset.head()","a36a8940":"dataset['labels'].value_counts()","ae90fe0d":"dataset['labels'].value_counts().plot.bar(color='green')","92fee453":"from sklearn.model_selection import train_test_split\n\nX = dataset.text_cleaned\ny = dataset.labels\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","837bcfff":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(max_features=15000, binary=True)\n\nX_train_vect = vectorizer.fit_transform(X_train)","83a8e992":"#Utilisation de smote pour les dataset d\u00e9s\u00e9quilibr\u00e9s\nfrom imblearn.over_sampling import SMOTE\n\nsm = SMOTE()\n\nX_train_res, y_train_res = sm.fit_sample(X_train_vect, y_train)","784a4060":"X_test_vect = vectorizer.transform(X_test)","a7094fd9":"from sklearn.naive_bayes import MultinomialNB\n\nnb = MultinomialNB()\n\nnb.fit(X_train_res, y_train_res)\n\ny_pred = nb.predict(X_test_vect)","2736be58":"print(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test, y_pred) * 100))","4d9069ec":"print(nb.predict(vectorizer.transform(['this hotel was amazing'])))","f95fbfe5":"print(nb.predict(vectorizer.transform(['This hotel was a fucking joke, have you ever seen a housekipper that doesn\\'t clean room? '])))","9b65758d":"original_dataset.head()","ae67d398":"x1 = original_dataset['text_cleaned']\ny1 = original_dataset['reviews.rating']","d448e06c":"vect = TfidfVectorizer(ngram_range = (1,2))\nx_vect1 = vect.fit_transform(x1)","2fe0366f":"x_train_c, x_test_c, y_train_c, y_test_c = train_test_split(x_vect1, y1, test_size=0.15, random_state = 10, shuffle=True)","87d44c35":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nlin_svc_mod = LinearSVC(C=0.13, class_weight=None, dual=True, fit_intercept=True,\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n     multi_class='ovr', penalty='l2', random_state=0, tol=1e-05, verbose=0)\nlin_svc_mod.fit(x_train_c, y_train_c)\npred = lin_svc_mod.predict(x_test_c)\nprint(\"Linear SVC:\",accuracy_score(y_test_c, pred))\nprint(\"MSE: \",mean_squared_error(y_test_c,pred))\n","d98b8fdd":"print(lin_svc_mod.predict(vect.transform(['this hotel was horrible'])))\n","26791b37":"print(\"Score suppos\u00e9 : 4\")\nprint(\"Score predit : \")\nprint(lin_svc_mod.predict(vect.transform(['loved \tstayed warwick overnight getway enjoy christmas shopping \twarwick exceeded expectations \tstaff wonderful extrememly friendly room clean service lounge wonderful \tcame contact hotel friendly \twomen bathroom lever lounge well.. think haunted totally creepy vibe lights anywho \treally enjoyed stay going couple days \t '])))\n","ff3738df":"from sklearn.ensemble import RandomForestClassifier\nrmfr = RandomForestClassifier()\nrmfr.fit(x_train_c, y_train_c)\npredrmfr = rmfr.predict(x_test_c)\nprint(\"Score:\",round(accuracy_score(y_test_c,predrmfr)*100,2))\nprint(\"MSE: \",mean_squared_error(y_test_c,predrmfr))\n","eb9aa99c":"\nparameters = {\n    \"n_estimators\":[5,10,50,100,250],\n    \"max_depth\":[2,4,8,16,32,64]\n    \n}\nfrom sklearn.model_selection import GridSearchCV\ncv = GridSearchCV(rmfr,parameters,cv=5)\ncv.fit(x_train_c, y_train_c)","4b91fdba":"def display(results):\n    print(f'Best parameters are: {results.best_params_}')\n    print(\"\\n\")\n    mean_score = results.cv_results_['mean_test_score']\n    std_score = results.cv_results_['std_test_score']\n    params = results.cv_results_['params']\n    for mean,std,params in zip(mean_score,std_score,params):\n        print(f'{round(mean,3)} + or -{round(std,3)} for the {params}')","a9c563a9":"display(cv)\n","569c3320":"from sklearn.ensemble import RandomForestClassifier\nrmfrclass = RandomForestClassifier(max_depth = 64,n_estimators = 10 )\nrmfrclass.fit(x_train_c, y_train_c)\npredrmfrclass = rmfrclass.predict(x_test_c)\nprint(\"Score:\",round(accuracy_score(y_test_c,predrmfrclass)*100,2))\nprint(\"MSE: \",mean_squared_error(y_test_c,predrmfrclass))","ac639c1b":"\nfrom sklearn.svm import SVC\nsvm = SVC(random_state=101)\nsvm.fit(x_train_c,y_train_c)\npredsvm = svm.predict(x_test_c)\nprint(\"Score:\",round(accuracy_score(y_test_c,predsvm)*100,2))\nprint(\"MSE: \",mean_squared_error(y_test_c,predsvm))","081eecd8":"import numpy as np\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()\nreg.fit(x_train_c,y_train_c)\nprint(\"Score: \", reg.score(x_test_c, y_test_c))\npred_lin_reg = reg.predict(x_test_c)\nprint(\"MSE: \",mean_squared_error(y_test_c,pred_lin_reg))","9ef6353a":"#Suppos\u00e9 2\nprint(reg.predict(vect.transform([' \t1st time seattle delayed anniversary trip wanted stay nicer hotels room reminded holiday inn level hotel \tplain room extra pillows \tbathroom ordinary corian sink ordinary bathroom \troom higher floor looking freeway loud \treason earplugs sleep cd \tasked switch rooms told probably stay way stay 2 nights staying hotel different area town \tluggage room decided eat \tstopped concierge asked good place walk rudely told just walk area \tnot sure concierge doorman just sitting desk expected help \tdecided night hotel come day earlier happily said \tused club points crowne rooms maybe lousy experience opted leave pay room luxury hotel hotel 1000'])))","172f9053":"#from sklearn import datasets,linear_model\n#from sklearn.model_selection import GridSearchCV\n#parameters = {'kernel':('linear', 'rbf')}\n#svc=linear_model.ARDRegression(n_iter=300,tol=0.001)\n#clf = GridSearchCV(svc, parameters, cv=5)\n#clf","29d54524":"import keras\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom keras import Sequential\nfrom keras.layers import Embedding\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Lambda\nfrom keras.layers import LSTM\nimport keras.backend as K\n\nimport nltk\nnltk.download('treebank')\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","51c51661":"def create_dataset(num_words, max_text_len, data):\n  res = []\n  for i in tqdm(range(len(data))):\n    res.append([preprocess_text(data.iloc[i][\"reviews.text\"]), data.iloc[i][\"reviews.rating\"]])\n  inp, targ = zip(*res)\n  print(\"Tokenizing the data ...\")\n  tokenizer = keras.preprocessing.text.Tokenizer(num_words = num_words)\n  tokenizer.fit_on_texts([i[0] for i in inp])\n  tensor = [ tokenizer.texts_to_sequences(i)[0] for i in inp]\n  tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding='post',   value=0, maxlen=max_text_len)\n  print(\"Splitting the data into train\/val datasets (0.2) ...\")\n  input_tensor_train, input_tensor_test, target_tensor_train, target_tensor_test = train_test_split(tensor, targ, test_size=0.1)\n\n  return (input_tensor_train, np.array(target_tensor_train)-1), (input_tensor_test, target_tensor_test),  tokenizer","ff5e35db":"vocab_size = 5000\nembed_size = 300\nmax_text_len = 200\nlearning_rate = 0.001\nbatch_size = 128\nn_epochs = 4","11ee20f1":"train_set, test_set, dictionary = create_dataset(vocab_size, max_text_len, original_dataset)","1a22d908":"model = Sequential()\nmodel.add(Embedding(vocab_size, embed_size, input_shape=(max_text_len,), mask_zero=True))\nmodel.add(LSTM(512, return_sequences=True))\nmodel.add(LSTM(512, return_sequences=True))\nmodel.add(Lambda(lambda x : K.mean(x, axis=1)))\nmodel.add(Dense(256, activation=\"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(100, activation=\"relu\"))\nmodel.add(Dropout(0.2))\n#model.add(Dense(50, activation=\"relu\"))\nmodel.add(Dense(5, activation=\"softmax\"))","c505f5bd":"import tensorflow as tf","a8689c5a":"# Variable-length int sequences.\nquery_input = tf.keras.Input(shape=(max_text_len,), dtype='int32')\n\n# Embedding lookup.\ntoken_embedding = tf.keras.layers.Embedding(vocab_size, embed_size)\n# Query embeddings of shape [batch_size, Tq, dimension].\nquery_embeddings = token_embedding(query_input)\n# Value embeddings of shape [batch_size, Tv, dimension].\nvalue_embeddings = token_embedding(query_input)\n\n# CNN layer.\ncnn_layer = tf.keras.layers.Conv1D(\n    filters=100,\n    kernel_size=4,\n    # Use 'same' padding so outputs have the same shape as inputs.\n    padding='same')\n# Query encoding of shape [batch_size, Tq, filters].\nquery_seq_encoding = cnn_layer(query_embeddings)\n# Value encoding of shape [batch_size, Tv, filters].\nvalue_seq_encoding = cnn_layer(value_embeddings)\n\n# Query-value attention of shape [batch_size, Tq, filters].\nquery_value_attention_seq = tf.keras.layers.AdditiveAttention()(\n    [query_seq_encoding, value_seq_encoding])\n\n# Reduce over the sequence axis to produce encodings of shape\n# [batch_size, filters].\nquery_encoding = tf.keras.layers.GlobalAveragePooling1D()(\n    query_seq_encoding)\nquery_value_attention = tf.keras.layers.GlobalAveragePooling1D()(\n    query_value_attention_seq)\n\nfcn1 = tf.keras.layers.Dense(256)(query_value_attention)\ndropout1 = tf.keras.layers.Dropout(0.5)(fcn1)\nfcn2 = tf.keras.layers.Dense(100)(dropout1)\ndropout2 = tf.keras.layers.Dropout(0.2)(fcn2)\n\noutput = tf.keras.layers.Dense(5, activation=\"softmax\")(dropout2)\n\nmodel = tf.keras.Model(query_input, output)","7b8cb0c1":"model.summary()","963851b4":"optimizer = tf.keras.optimizers.Adam()","c6bb9029":"model.compile(loss=\"sparse_categorical_crossentropy\", optimizer = optimizer, metrics=[\"accuracy\"])","04488f08":"We can see that the distribution is unbalanced and  there is a very large part of 5 unlike the note 1 and 2. The opinions are therefore positive in the majority\n","6fca7c55":"* We are going to split the dataset into training and test *","41af355d":"Faisons quelques tests sur differentes phrases","76c20741":"### Linear SVM pour une classification multiclasse","ad5a5766":"###### We will therefore use a classification model to predict class 0 or 1","f01ff10f":"Let's look at the missing values","915c8fe1":"## Importing Libraries","2a4ec66f":"### We apply the following principle to be able to have a binary model\n- If the score is less than 3 then the score becomes 0\n- Otherwise the note becomes 1","b1707ebf":"#### Support Vector Machine","b59730a3":"## Model","32e47fab":"Creating a column with cleaned reviews","452562b5":"# Improve the model with Deep Learning","5f5aae3d":"##### Let's delete all MoreMore","9fa53b61":"We will create a vectorizer to split the text into unigram and bigrams","6cda872e":"#### The linear SVC has the best precision but the linear regression gives us the smallest MSE","7a70853d":"#### Let's apply this function on reviews","9b9c9a0c":"#### Utilisation du modele Naive bayes","0295ce86":"## Resume des differents modele de machine learning","5ee3bf9c":"Let's take a sentence from a review that does not appear in the dataset and that was also rated on TipAdvisor","17a967f0":"### Preprocessing function for reviews. We will go through the various stages in order to clean the reviews in the best way.\n- Convert the text to lowercase\n- Removing Numbers\n- Removing white spaces\n- Replacer replace\n- Tokenize into sentences\n- Tokenize into words\n- Remove stop words\n- Lemmatize\n\n\n","53b0b8de":"We are approaching the desired class, in fact our model has an accuracy of 60%.","40373b8c":"### We will make a copy of the dataset in order to try a binary approach\n","6a6fcc5c":"Let's do a grid search to find out which are the best parameters","9c72069e":"## Data Preprocessing","d4abdb9c":"## Importing dataset","8d35b9a3":"### Let's look at the distribution of the different grades","55a2268d":"# **TripAdvisor Hotel Reviews**","eb5b2892":"## Regression lineaire","25933144":"However we want to predict the grades and not a binary class.\n\nAfter testing a binary model we will make it more complex by going with the real notes and not the binary labels","6bfecb11":"## Hyperparams"}}