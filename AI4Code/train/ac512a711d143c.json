{"cell_type":{"f75ddd2b":"code","4e43ec57":"code","2bf7ccc1":"code","9b93ea63":"code","345ced6a":"code","85231be7":"code","56f9ebea":"code","e842fe6c":"code","a5408fce":"code","f07d7f1d":"code","c34961d0":"code","15ce041c":"code","9c0822e4":"code","3d6cac70":"code","df666506":"code","afc1f300":"code","f9220b28":"code","45ad9bee":"code","57ab1f86":"code","44c070cc":"code","543c9c14":"code","49f1946d":"code","ea24c67c":"code","640ffa05":"code","5adc9ed2":"code","c60079f1":"code","18cfbf5b":"code","c43b53e3":"code","24789aae":"code","1ff8c17b":"code","f999d2de":"code","b19f2b3c":"code","6a415c7b":"code","de31cf48":"code","1e730aac":"code","1f4a48f9":"code","eeffc91b":"code","327cb7e9":"code","c12b2f11":"code","3f395c81":"code","ff04af03":"code","6e61fa7e":"code","7dd8731f":"code","0a3788c9":"code","90b6e00f":"code","328c571d":"code","172fe9f4":"code","7851f29d":"code","c6f50245":"code","d8900272":"code","f3d10f4c":"code","917767f6":"code","7c7ff799":"code","b906eab5":"code","633a274b":"code","e8825159":"code","3f047ca3":"code","a57b5724":"code","10af7240":"markdown"},"source":{"f75ddd2b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4e43ec57":"# Reading the train dataset\ntrn = pd.read_csv('..\/input\/mbdata\/train.csv')\n  \n#Reading the test dataset                  \ntst = pd.read_csv('..\/input\/mbdata\/test.csv')                  ","2bf7ccc1":"trn.head()","9b93ea63":"tst.head()","345ced6a":"# Insights:\n#----------\n# There are 378 cols in train data set, X columns are named as X1, X2 etc.. and target is the Y column\n# There are 377 cols in test data set , all columns except the Y column.\n\n\n# Since there are lot of columns , we can use the below aggregate function to know about the datatypes of columns\n\ndtype_df = trn.dtypes.reset_index()\ndtype_df.columns = [\"feature name\",\"dtypes\"]\ndtype_df.groupby(\"dtypes\").agg(\"count\").reset_index()","85231be7":"dtype_df = tst.dtypes.reset_index()\ndtype_df.columns = [\"feature name\",\"dtypes\"]\ndtype_df.groupby(\"dtypes\").agg(\"count\").reset_index()","56f9ebea":"# Print the column if it has any null values\nfor i in trn.columns:\n    if trn[i].isnull().sum() > 0 :\n        print(trn[i].isnull().sum())","e842fe6c":"# No features has null values in train data","a5408fce":"# Print the column if it has any null values\nfor i in tst.columns:\n    if tst[i].isnull().sum() > 0 :\n        print(tst[i].isnull().sum())","f07d7f1d":"# No features has null values in test data","c34961d0":"# Checking the unique values for each column in train data\nfor i in trn.columns:\n    print(i,'**',trn[i].unique())","15ce041c":"# Checking the unique values for each column in test data\nfor i in tst.columns:\n    print(i,'**',tst[i].unique())","9c0822e4":"# Insights:\n#----------\n# From the above, we can see most of the featues are binary from X10 onwards \n# Many are constants like X11,X93 etc has only values as zeroes, we will confirm this by finding the variance below\n# After confirming the variance as zero, We can drop such cols as they will not contribute to the model\n# We can also see fetures with zero variance in train are not similar with test data - so here we have to remove the same features from test data as well","3d6cac70":"# To identify features with 0 variance , Since we could not find variance of categorical features, we do this after label encoding","df666506":"# Encoding Categorical features in train data - X0\n# print(trn['X0'].unique())\nfrom sklearn.preprocessing import LabelEncoder\n# enc = LabelEncoder()\n# trn['X0'] = enc.fit_transform(trn['X0'])","afc1f300":"# Applying same label encoder to test data X0 column\n# tst['X0'] = enc.transform(tst['X0'])","f9220b28":"# With the above error, we can understand that some of values in X0 column in test set are not presennt in train set \n# and they were not encoded, hence we get error while applying on test\n\n# Now we will identify the uncommon values\nset(trn['X0'].unique()) - (set(tst['X0'].unique()))","45ad9bee":"set(tst['X0'].unique()) - (set(trn['X0'].unique()))","57ab1f86":"# So we get the uncommon values in both train & test data sets. Now we will apply label encoder using the unique values\nenc = LabelEncoder()\nenc.fit_transform(['k','az','t','al','o','w', 'j', 'h', 's', 'n', 'ay', 'f', 'x', 'y', 'aj', 'ak', 'am',\n 'z', 'q', 'at', 'ap', 'v', 'af', 'a', 'e', 'ai', 'd', 'aq', 'c', 'aa', 'ba', 'as', 'i',\n 'r', 'b', 'ax', 'bc', 'u', 'ad', 'au', 'm', 'l', 'aw', 'ao', 'ac', 'g', 'ab', 'ae', 'ag', 'an', 'av', 'bb', 'p'])","44c070cc":"trn.head()","543c9c14":"# Now tranform the train data using the encoder\ntrn['X0'] = enc.transform(trn['X0'])","49f1946d":"trn.head()","ea24c67c":"tst.head()","640ffa05":"# Now tranform the test data using the encoder\ntst['X0'] = enc.transform(tst['X0'])","5adc9ed2":"tst.head()","c60079f1":"## From the above we can see the value 'az' in both train & test data has been decoded as 24. Now this holds good.\n# This can be repeated for the columns X1-X6,X8 as well","18cfbf5b":"# Now we will identify the uncommon values for X1 column\nprint(set(trn['X1'].unique()) - (set(tst['X1'].unique())))\nprint(set(tst['X1'].unique()) - (set(trn['X1'].unique())))","c43b53e3":"# From the above we can see there are no uncommon values between train & test data for column X1. So we can directly apply using column\nenc = LabelEncoder()\ntrn['X1'] = enc.fit_transform(trn['X1'])\ntst['X1'] = enc.transform(tst['X1'])","24789aae":"# Now we will identify the uncommon values for X1 column\nprint(set(trn['X2'].unique()) - (set(tst['X2'].unique())))\nprint(set(tst['X2'].unique()) - (set(trn['X2'].unique())))","1ff8c17b":"# So we have some uncommon values similar to X0 column\nprint(trn['X2'].unique())\nprint(tst['X2'].unique())","f999d2de":"# Now we define the encoder and fit the values\nencX2 = LabelEncoder()\nencX2.fit_transform(['at', 'av', 'n', 'e', 'as', 'aq', 'r', 'ai', 'ak', 'm', 'a', 'k','ae', 's', 'f', 'd', 'ag', \n                     'ay', 'ac', 'ap', 'g', 'i', 'aw', 'y', 'b', 'ao', 'al', 'h', 'x', 'au', 't', 'an', 'z', 'ah', \n                     'p', 'am', 'j', 'q', 'af', 'l', 'aa', 'c', 'o', 'ar','ad', 'ax', 'u', 'ab', 'w', 'aj'])","b19f2b3c":"# Now tranform the train & test data using the encoder\ntrn['X2'] = encX2.transform(trn['X2'])\ntst['X2'] = encX2.transform(tst['X2'])","6a415c7b":"# Now we will identify the uncommon values for X3 column\nprint(set(trn['X3'].unique()) - (set(tst['X3'].unique())))\nprint(set(tst['X3'].unique()) - (set(trn['X3'].unique())))","de31cf48":"# From the above we can see there are no uncommon values between train & test data for column X3. So we can directly apply using column\nencX3 = LabelEncoder()\ntrn['X3'] = encX3.fit_transform(trn['X3'])\ntst['X3'] = encX3.transform(tst['X3'])","1e730aac":"# Now we will identify the uncommon values for X4 column\nprint(set(trn['X4'].unique()) - (set(tst['X4'].unique())))\nprint(set(tst['X4'].unique()) - (set(trn['X4'].unique())))","1f4a48f9":"# From the above we can see there are no uncommon values between train & test data for column X4. So we can directly apply using column\nencX4 = LabelEncoder()\ntrn['X4'] = encX4.fit_transform(trn['X4'])\ntst['X4'] = encX4.transform(tst['X4'])","eeffc91b":"# Now we will identify the uncommon values for X5 column\nprint(set(trn['X5'].unique()) - (set(tst['X5'].unique())))\nprint(set(tst['X5'].unique()) - (set(trn['X5'].unique())))","327cb7e9":"# So we have some uncommon values similar to X5 column\ntrn['X5'].unique()","c12b2f11":"# Now we define the encoder and fit the values of column X5\nencX5 = LabelEncoder()\nencX5.fit_transform(['u', 'y', 'x', 'h', 'g', 'f', 'j', 'i', 'd', 'c', 'af', 'ag', 'ab',\n       'ac', 'ad', 'ae', 'ah', 'l', 'k', 'n', 'm', 'p', 'q', 's', 'r',\n       'v', 'w', 'o', 'aa','z', 'a', 'b', 't'])","3f395c81":"# Now tranform the train & test data using the encoder\ntrn['X5'] = encX5.transform(trn['X5'])\ntst['X5'] = encX5.transform(tst['X5'])","ff04af03":"# Now we will identify the uncommon values for X6 column\nprint(set(trn['X6'].unique()) - (set(tst['X6'].unique())))\nprint(set(tst['X6'].unique()) - (set(trn['X6'].unique())))","6e61fa7e":"# From the above we can see there are no uncommon values between train & test data for column X6. \n# So we can directly apply using column\nencX6 = LabelEncoder()\ntrn['X6'] = encX6.fit_transform(trn['X6'])\ntst['X6'] = encX6.transform(tst['X6'])","7dd8731f":"# Now we will identify the uncommon values for X8 column\nprint(set(trn['X8'].unique()) - (set(tst['X8'].unique())))\nprint(set(tst['X8'].unique()) - (set(trn['X8'].unique())))","0a3788c9":"# From the above we can see there are no uncommon values between train & test data for column X8. \n# So we can directly apply using column\nencX8 = LabelEncoder()\ntrn['X8'] = encX8.fit_transform(trn['X8'])\ntst['X8'] = encX8.transform(tst['X8'])","90b6e00f":"# Identify features with 0 variance , Since we could not find variance of categorical features, we do this after label encoding\ntemp = []\nfor i in trn.columns:\n    if trn[i].var()==0:\n        temp.append(i)\n        \nprint('No. of features in train data has zero variance:',len(temp))\nprint('List here:',temp)","328c571d":"# Dropping cols with Zero variance\n\ntrn.drop(['X11', 'X93', 'X107', 'X233', 'X235', 'X268', 'X289', 'X290', 'X293', 'X297', 'X330', 'X347'], axis=1)","172fe9f4":"# Since features with 0 variance are removed from train data, the same should be removed from test data\n\ntst.drop(['X11', 'X93', 'X107', 'X233', 'X235', 'X268', 'X289', 'X290', 'X293', 'X297', 'X330', 'X347'], axis=1)","7851f29d":"# Checking whether all features in train are numerical datatype to go before PCA\ndtype_df = trn.dtypes.reset_index()\ndtype_df.columns = [\"feature name\",\"dtypes\"]\ndtype_df.groupby(\"dtypes\").agg(\"count\").reset_index()","c6f50245":"# Checking whether all features in test are numerical datatype to go before PCA\ndtype_df = tst.dtypes.reset_index()\ndtype_df.columns = [\"feature name\",\"dtypes\"]\ndtype_df.groupby(\"dtypes\").agg(\"count\").reset_index()","d8900272":"# Dropping ID from both train & test as it will not be used by model\n\ntrnPca=trn.drop(['ID','y'],axis=1)\ntstPca=tst.drop(['ID'],axis=1)","f3d10f4c":"# Perform dimensionality reduction using PCA\nfrom sklearn.decomposition import PCA\nn_comp = 12\npca = PCA(n_components=n_comp, random_state=420)\ntrnPca= pca.fit_transform(trnPca)\ntstPca = pca.transform(tstPca)","917767f6":"trnPca.shape","7c7ff799":"tstPca.shape","b906eab5":"# Now the total 377 X columns are reduced into 12 columns after applying PCA","633a274b":"# ML Modeling with XGboost\nimport xgboost as xgb\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\n# Defining train & test for model input\ntrain_X = trnPca\ntrain_y = trn['y']\n\n# Splitting\nx_train, x_valid, y_train, y_valid = train_test_split(train_X, train_y, test_size=0.2, random_state=420)\n\n# Defining feature set\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\nd_test = xgb.DMatrix(tstPca)\nxgb_params = {\n 'n_trees': 500, \n 'eta': 0.0050,\n 'max_depth': 3,\n 'subsample': 0.95,\n 'objective': 'reg:linear',\n 'eval_metric': 'rmse',\n 'base_score': np.mean(train_y), # base prediction = mean(target)\n 'silent': 1\n}\n\n# Creating a function for the predicting score\ndef xgb_r2_score(preds, dtrain):\n labels = dtrain.get_label()\n return 'r2', r2_score(labels, preds)\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\nmdl = xgb.train(xgb_params, d_train, 1050 , watchlist, early_stopping_rounds=50, feval=xgb_r2_score, maximize=True, verbose_eval=10)","e8825159":"# Predicting on test set\np_test = mdl.predict(d_test)\np_test","3f047ca3":"Predicted_Data = pd.DataFrame()\nPredicted_Data['y'] = p_test\nPredicted_Data.head()","a57b5724":"# With the above model, we have the below metrics:\n# train-rmse:9.05751\ttrain-r2:0.49805\n# valid-rmse:9.00293\tvalid-r2:0.45923","10af7240":"**Mercedes-Benz Greener Manufacturing.**\n\n**DESCRIPTION**\n\nReduce the time a Mercedes-Benz spends on the test bench.\n\nProblem Statement Scenario:\nSince the first automobile, the Benz Patent Motor Car in 1886, Mercedes-Benz has stood for important automotive innovations. These include the passenger safety cell with a crumple zone, the airbag, and intelligent assistance systems. Mercedes-Benz applies for nearly 2000 patents per year, making the brand the European leader among premium carmakers. Mercedes-Benz is the leader in the premium car industry. With a huge selection of features and options, customers can choose the customized Mercedes-Benz of their dreams.\n\nTo ensure the safety and reliability of every unique car configuration before they hit the road, the company\u2019s engineers have developed a robust testing system. As one of the world\u2019s biggest manufacturers of premium cars, safety and efficiency are paramount on Mercedes-Benz\u2019s production lines. However, optimizing the speed of their testing system for many possible feature combinations is complex and time-consuming without a powerful algorithmic approach.\n\nYou are required to reduce the time that cars spend on the test bench. Others will work with a dataset representing different permutations of features in a Mercedes-Benz car to predict the time it takes to pass testing. Optimal algorithms will contribute to faster testing, resulting in lower carbon dioxide emissions without reducing Mercedes-Benz\u2019s standards."}}