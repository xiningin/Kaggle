{"cell_type":{"ace0f17c":"code","b9b32a86":"code","6743533c":"code","6fcabfb8":"code","7bcf7325":"code","0d33e2e9":"code","611274c3":"code","027b7ae4":"code","f018c61a":"code","aa85dd34":"markdown","f84d4c3d":"markdown","8ed185a3":"markdown","cb732c21":"markdown","7e859ffa":"markdown","bb8b0ff2":"markdown","56eac057":"markdown","d885e203":"markdown","990716b9":"markdown"},"source":{"ace0f17c":"\nimport numpy as np\n\nfrom keras.datasets import reuters\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.preprocessing.text import Tokenizer\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","b9b32a86":"(XTrain, YTrain),(XTest, YTest) = reuters.load_data(num_words=None, test_split=0.3)\n\nprint('XTrain class = ',type(XTrain))\nprint('YTrain class = ',type(YTrain))\nprint('XTest shape = ',type(XTest))\nprint('YTest shape = ',type(YTest))\n\nprint('XTrain shape = ',XTrain.shape)\nprint('XTest shape = ',XTest.shape)\nprint('YTrain shape = ',YTrain.shape)\nprint('YTest shape = ',YTest.shape)\n","6743533c":"print('YTrain values = ',np.unique(YTrain))\nprint('YTest values = ',np.unique(YTest))\n\nunique, counts = np.unique(YTrain, return_counts=True)\nprint('YTrain distribution = ',dict(zip(unique, counts)))\nunique, counts = np.unique(YTest, return_counts=True)\nprint('YTrain distribution = ',dict(zip(unique, counts)))\n","6fcabfb8":"plt.figure(1)\nplt.subplot(121)\nplt.hist(YTrain, bins='auto')\nplt.xlabel(\"Classes\")\nplt.ylabel(\"Number of occurrences\")\nplt.title(\"YTrain data\")\n\nplt.subplot(122)\nplt.hist(YTest, bins='auto')\nplt.xlabel(\"Classes\")\nplt.ylabel(\"Number of occurrences\")\nplt.title(\"YTest data\")\nplt.show()\n\nprint(XTrain[1])\nlen(XTrain[1])\n","7bcf7325":"#The dataset_reuters_word_index() function returns a list where the names are words and the values are integer\nWordIndex = reuters.get_word_index(path=\"reuters_word_index.json\")\n\nprint(len(WordIndex))\n\nIndexToWord = {}\nfor key, value in WordIndex.items():\n    IndexToWord[value] = key\n\nprint(' '.join([IndexToWord[x] for x in XTrain[1]]))\nprint(YTrain[1])\n\nMaxWords = 10000\n\n# Tokenization of words.\nTok = Tokenizer(num_words=MaxWords)\nXTrain = Tok.sequences_to_matrix(XTrain, mode='binary')\nXTest = Tok.sequences_to_matrix(XTest, mode='binary')\n\n# Preprocessing of labels\nNumClasses = max(YTrain) + 1\nYTrain = to_categorical(YTrain, NumClasses)\nYTest = to_categorical(YTest, NumClasses)\n\nprint(XTrain[1])\nprint(len(XTrain[1]))","0d33e2e9":"model = Sequential()\nmodel.add(Dense(512, input_shape=(MaxWords,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(NumClasses))\nmodel.add(Activation('softmax'))\nmodel.summary()\n\nmodel.compile(loss='categorical_crossentropy', \n              optimizer='adam', \n              metrics=['accuracy'])\n","611274c3":"history = model.fit(XTrain, YTrain, \n                    validation_data=(XTest, YTest), \n                    epochs=10, \n                    batch_size=64)","027b7ae4":"Scores = model.evaluate(XTest, YTest, verbose=1)\nprint('Test loss:', Scores[0])\nprint('Test accuracy:', Scores[1])","f018c61a":"def plotmodelhistory(history): \n    fig, axs = plt.subplots(1,2,figsize=(15,5)) \n    # summarize history for accuracy\n    axs[0].plot(history.history['accuracy']) \n    axs[0].plot(history.history['val_accuracy']) \n    axs[0].set_title('Model Accuracy')\n    axs[0].set_ylabel('Accuracy') \n    axs[0].set_xlabel('Epoch')\n    axs[0].legend(['train', 'validate'], loc='upper left')\n    # summarize history for loss\n    axs[1].plot(history.history['loss']) \n    axs[1].plot(history.history['val_loss']) \n    axs[1].set_title('Model Loss')\n    axs[1].set_ylabel('Loss') \n    axs[1].set_xlabel('Epoch')\n    axs[1].legend(['train', 'validate'], loc='upper left')\n    plt.show()\n\n# list all data in history\nprint(history.history.keys())\n\nplotmodelhistory(history)","aa85dd34":"## 1. About the dataset.\nThis is a dataset of 11,228 newswires from Reuters, labeled over 46 topics. This was originally generated by parsing and preprocessing the classic Reuters-21578 dataset, but the preprocessing code is no longer packaged with Keras.\n\n\nEach newswire is encoded as a list of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n\nAs a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.","f84d4c3d":"## 2. Data preprocessing and visualization.","8ed185a3":"## 4. Train the model.","cb732c21":"## 3. Build the Classifier.","7e859ffa":"## 5. Evaluate the model.\n### Scoring the model.","bb8b0ff2":"**Hope that you find this notebook helpful. More to come.**\n\n**If it's useful for you, Please upvote this to keep me motivate for doing better.**\n\n**Thanks for comment and suggestions.**\n","56eac057":"# A Simple MLP for Reuters News Classifier in Keras for NLP\n\nNatural language processing (NLP) is the process of automatic processing of information written or spoken in a natural language using an electronic calculator. This is made particularly difficult and complex due to the intrinsic characteristics of the ambiguity of human language. When it's necessary to make the machine learn methods of interaction with the environment typical of man, the question isn't so much that of storing data, but that of letting the machine learn how this data can be translated simultaneously to create a concept. Natural language interacts with the environment generating predictive knowledge.\n\nIn this Notebook, We will use Simple MLP with Keras layers to build a model to classify Reuter's newswire.\n\n### Table of interest:\n1. About the dataset.\n2. Data preprocessing and visualization.\n3. Build the Classifier.\n4. Train the model.\n5. Evaluate the model.","d885e203":"### Tokenization and preprocessing of labels.","990716b9":"### Plot history of training and validations cuvres."}}