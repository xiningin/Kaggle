{"cell_type":{"85ce7894":"code","609c7bd2":"code","5a682267":"code","371024f4":"code","888519e2":"code","63a96319":"code","f34054cf":"code","dee09e42":"code","5a96ee1c":"code","5251e791":"code","d0627c46":"code","877f22a8":"code","dc2b17c6":"code","afc24075":"markdown","c3f156cd":"markdown","d9b395b5":"markdown"},"source":{"85ce7894":"import os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix, accuracy_score\nimport matplotlib.pyplot as plt","609c7bd2":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5a682267":"df = pd.read_csv('\/kaggle\/input\/tictactoe-endgame-dataset-uci\/tic-tac-toe-endgame.csv',',')\ndf","371024f4":"df['V1'],v1 = pd.factorize(df['V1'], sort=True)\ndf['V2'],v2 = pd.factorize(df['V2'], sort=True)\ndf['V3'],v3 = pd.factorize(df['V3'], sort=True)\ndf['V4'],v4 = pd.factorize(df['V4'], sort=True)\ndf['V5'],v5 = pd.factorize(df['V5'], sort=True)\ndf['V6'],v6 = pd.factorize(df['V6'], sort=True)\ndf['V7'],v7 = pd.factorize(df['V7'], sort=True)\ndf['V8'],v8 = pd.factorize(df['V8'], sort=True)\ndf['V9'],v9 = pd.factorize(df['V9'], sort=True)\ndf['V10'],v10 = pd.factorize(df['V10'], sort=True)\nprint(v1, v2, v3, v4, v5, v6, v7, v8, v9, v10)\nprint(df)","888519e2":"class_names = [v10[0], v10[1]]\nclass_names","63a96319":"x = df.drop('V10',axis=1)\ny = df['V10']\n\n# Vamos separar (split) nossos dados em conj. de dados para treinamento e testes..\nx_train, x_test, y_train, y_test = train_test_split(x, y)\n[x_train.shape, x_test.shape, y_train.shape, y_test.shape]","f34054cf":"def classify_mean_accuracy(max_n, max_c):\n    \n    max_range = range(1, max_n)\n    cases = range(0, max_c)\n    funcs = ['relu', 'logistic', 'tanh']\n    \n    best_func = 'relu'\n    best_g_sum = 0\n    best_g = 0\n    best_g_n = 0\n    \n    for func in funcs:\n        \n        best_list = []\n        best_sum = 0\n        best_c = 0\n        best_c_n = 0\n        \n        for case in cases:\n            \n            best = 0\n            best_n = 0\n            accuracy_list = []\n            \n            for n in max_range:\n                mlp = MLPClassifier(solver='lbfgs', activation=func, hidden_layer_sizes=(n))\n                mlp.fit(x_train, y_train)\n                y_pred = mlp.predict(x_test)\n                score = accuracy_score(y_test, y_pred)\n                accuracy_list.append(score)\n                \n                if score > best:\n                    best = score\n                    best_n = n\n            \n            accuracy_sum = sum(accuracy_list)\n            \n            if accuracy_sum > best_sum:\n                best_sum = accuracy_sum\n                best_list = accuracy_list\n                best_c = best\n                best_c_n = best_n\n                \n        if best_sum > best_g_sum:\n            best_g_sum = best_sum\n            best_g = best_c\n            best_g_n = best_c_n\n            best_func = func\n                \n        plt.plot(max_range, best_list, label=func)\n    \n    plt.title('Activation function: relu x logistic x tanh')\n    plt.xlabel('Number of neurons')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='best')\n    plt.show()\n    \n    return (best_g, best_g_n, best_func)\n\ndef classify_point_accuracy(max_n, max_c):\n      \n    max_range = range(1, max_n)\n    cases = range(0, max_c)\n    funcs = ['relu', 'logistic', 'tanh']\n    colors = ['ro', 'go', 'bo']\n    \n    best = 0\n    best_n = 0\n    best_mlp = None\n    c = 0\n    \n    for func in funcs:\n        \n        color = colors[c]\n        c += 1\n        \n        for case in cases:\n            \n            accuracy_list = []\n            \n            for n in max_range:\n                mlp = MLPClassifier(solver='lbfgs', activation=func, hidden_layer_sizes=(n))\n                mlp.fit(x_train, y_train)\n                y_pred = mlp.predict(x_test)\n                score = accuracy_score(y_test, y_pred)\n                accuracy_list.append(score)\n                \n                if score > best:\n                    best = score\n                    best_n = n\n                    best_mlp = mlp\n            \n            plt.plot(max_range, accuracy_list, color, label='{}{}'.format(func, case))\n    \n    plt.title('Activation function: relu x logistic x tanh')\n    plt.xlabel('Number of neurons')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='best')\n    plt.show()\n    \n    return (best, best_n, best_mlp)","dee09e42":"best_mean_results = classify_mean_accuracy(10, 10) # usually the best amount of neurons is found between the input and output size. Only one hidden layer is required for most feedback neural networks\nbest_mean_results","5a96ee1c":"best_point_results = classify_point_accuracy(10, 10) # usually the best amount of neurons is found between the input and output size. Only one hidden layer is required for most feedback neural networks\nbest_point_results","5251e791":"# First try out the best punctual accuracy mlp found. It usually brings results of at least 85%\nmlp = best_point_results[2]","d0627c46":"# use the model to make predictions with the test data\ny_pred = mlp.predict(x_test)\n# how did our model perform?\ncount_misclassified = (y_test != y_pred).sum()\nprint('Misclassified samples: {}\/{}'.format(count_misclassified, len(y_test)))\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\nprint(confusion_matrix(y_test, y_pred))","877f22a8":"# Second try out the best mean accuracy data into a new mlp and check results. They are usually at least 75%\nmlp = MLPClassifier(solver='lbfgs', activation=best_mean_results[2], hidden_layer_sizes=best_mean_results[1])\nmlp.fit(x_train,y_train)","dc2b17c6":"# use the model to make predictions with the test data\ny_pred = mlp.predict(x_test)\n# how did our model perform?\ncount_misclassified = (y_test != y_pred).sum()\nprint('Misclassified samples: {}\/{}'.format(count_misclassified, len(y_test)))\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy: {:.2f}'.format(accuracy))\nprint(confusion_matrix(y_test, y_pred))","afc24075":"## Setup","c3f156cd":"## Testing\n","d9b395b5":"# Tic Tac Toe Decision Tree"}}