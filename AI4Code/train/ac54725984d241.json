{"cell_type":{"c40dff26":"code","49f4f182":"code","725b3b2d":"code","14baccea":"code","0b918ff8":"code","d2b55a70":"code","3f8e8344":"code","8a051ba7":"code","f814e3cf":"code","c8f8be28":"code","5562684c":"code","94264937":"code","9e6601fb":"code","449bda25":"code","1cb1afcc":"code","cf38ecd3":"code","f8a66cf7":"code","35cbff84":"code","52c9a4ea":"code","da12b6db":"code","64f1df11":"code","ca1ac62a":"code","946aec48":"code","1075c9c5":"code","a0534bd6":"code","80f15713":"code","d0416db8":"code","8c6026da":"code","7e106d27":"code","e8630f3b":"code","9d82a32b":"code","443130a1":"code","d2bc8ebc":"code","ccf22349":"code","8785267b":"code","b9807a54":"code","904d0e1a":"code","c91d4a13":"code","6864a442":"code","cb5f9ea8":"code","7b9c5bd8":"code","af4c0f0f":"code","77d67ce1":"code","520305df":"code","3f7a9e6f":"code","0a209bcf":"code","d79bbaa5":"code","7935be2d":"code","defd7419":"code","9d8990df":"code","e8796057":"code","4e126107":"code","5cd839f7":"code","313671a4":"code","e365975d":"code","b33f8eef":"code","01778afd":"code","11c205b9":"code","0ebce25c":"code","2e1c8627":"code","375eea7a":"code","b7b6fa9e":"code","7186a3b2":"code","ef5e5024":"code","897c0c40":"code","38368c47":"code","f36688ea":"code","0caddc31":"markdown","df07f38a":"markdown","51b3ddd3":"markdown","a9c47720":"markdown","a943c283":"markdown","b9b6a0bb":"markdown","916e61ee":"markdown","06b3e7d0":"markdown","984df5e4":"markdown","15a313f0":"markdown","9a7a93e2":"markdown","2b10ef7b":"markdown","cd417bc7":"markdown","45a430ef":"markdown","f334590a":"markdown","029b29f9":"markdown","17dbea19":"markdown","211d29a7":"markdown","80030c08":"markdown","38be317d":"markdown","4b43a8cc":"markdown","a731c17c":"markdown"},"source":{"c40dff26":"import pandas as pd\nimport numpy as np\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set(style = \"white\",color_codes=True)\nsns.set(font_scale=1.5)\nfrom IPython.display import display\npd.options.display.max_columns = None\n\nfrom sklearn.model_selection import GridSearchCV #to fine tune Hyperparamters using Grid search\nfrom sklearn.model_selection import RandomizedSearchCV# to seelect the best combination(advance ver of Grid Search)\n\n# importing some ML Algorithms \nfrom sklearn.linear_model import LinearRegression # y=mx+c\nfrom sklearn.tree import DecisionTreeRegressor # Entropy(impurities),Gain. \nfrom sklearn.ensemble import RandomForestRegressor # Average of Many DT's\n\n# Testing Libraries - Scipy Stats Models\nfrom scipy.stats import shapiro # Normality Test 1\nfrom scipy.stats import normaltest # Normality Test 2\nfrom scipy.stats import anderson # Normality Test 3\nfrom statsmodels.graphics.gofplots import qqplot # plotting the Distribution of Y with a Line of dot on a 45 degree Line.\n\n# Model Varification\/Validation Libraries\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import ShuffleSplit\n\n\n# Matrices and Reporting Libraries\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.metrics import make_scorer\nfrom statsmodels.tools.eval_measures import rmse\nfrom sklearn.model_selection import learning_curve","49f4f182":"ds = pd.read_csv(\"..\/input\/incometrain\/income-train.csv\")\ndt = pd.read_csv(\"..\/input\/incometest\/income-test.csv\")","725b3b2d":"ds.head(),dt.head()","14baccea":"ds.shape, dt.shape","0b918ff8":"ds.dtypes.unique()","d2b55a70":"ds.select_dtypes(include='float').head()","3f8e8344":"ds.select_dtypes(include='int64').head()","8a051ba7":"ds.select_dtypes(include='O').head()","f814e3cf":"for a in [ds,dt]:\n    a['dependency'] = a['dependency'].replace({'yes':1,'no':2}).astype(np.float64)\n    a['edjefe'] = a[\"edjefe\"].replace({'yes':1,\"no\":2}).astype(np.float64)\n    a[\"edjefa\"]  = a[\"edjefa\"].replace({'yes':1,\"no\":2}).astype(np.float64)","c8f8be28":"ds.select_dtypes(include='O').head()","5562684c":"f_null = ds.select_dtypes(\"float\").isnull().sum()\nf_null[f_null > 0]","94264937":"i_null = ds.select_dtypes(\"int64\").isnull().sum()\ni_null[i_null>0]","9e6601fb":"o_null = ds.select_dtypes(\"O\").isnull().sum()\no_null[o_null>0]","449bda25":"ds.columns , dt.columns","1cb1afcc":"for a,b in zip(ds.columns,dt.columns):\n    if a == b:\n        print(a)","cf38ecd3":"ds[ds['v2a1'].isnull()][[\"v2a1\",'tipovivi1','tipovivi2','tipovivi3','tipovivi4','tipovivi5']]","f8a66cf7":"cleaned_Data = ds[ds['v2a1'].isnull()][['tipovivi1','tipovivi2','tipovivi3','tipovivi4','tipovivi5']].sum()\ncleaned_Data","35cbff84":"# plotting the above analysis. \nplt.style.use(\"dark_background\")\n\ncleaned_Data.plot.bar(figsize =(13,5),color ='red',edgecolor = 'k',linewidth = 3)\n\nplt.title('Monthly Rent Missing Data', size = 18);\nplt.xticks([0, 1, 2, 3, 4],\n           ['own and fully paid house', 'own-paying installments', 'Rented', 'Precarious', 'Other(assigned,borrowed)'],\n           rotation =50)","52c9a4ea":"for null in [ds,dt]:\n    null[\"v2a1\"].fillna(value=0,inplace=True)","da12b6db":"ds[\"v2a1\"].isnull().sum()","64f1df11":"ds.head()","ca1ac62a":"dt['parentesco1'].head()","946aec48":"# Heads of household\nHOD = ds.loc[ds['parentesco1'] == 1].copy()\nHOD.groupby('v18q')['v18q1'].apply(lambda x: x.isnull().sum())","1075c9c5":"# plotting the same\nplt.figure(figsize = (8, 6))\nds['v18q1'].value_counts().sort_index().plot.bar(color = 'blue',edgecolor = 'k',linewidth = 2)                              \nplt.xlabel('v18q1')\nplt.ylabel('Count')\nplt.title('v18q1 Counts')","a0534bd6":"for x in [ds, dt]:\n    x['v18q1'].fillna(value=0, inplace=True)\n\nds[['v18q1']].isnull().sum()","80f15713":"# Lets look at the data with not null values first.\nds[ds['rez_esc'].notnull()]['age'].describe()","d0416db8":"ds.loc[ds['rez_esc'].isnull()]['age'].describe()","8c6026da":"ds.loc[(ds['rez_esc'].isnull() & \n                     ((ds['age'] > 7) & (ds['age'] < 17)))]['age'].describe()\n#There is one value that has Null for the 'behind in school' column with age between 7 and 17","7e106d27":"ds[(ds['age'] ==10) & ds['rez_esc'].isnull()].head()\nds[(ds['Id'] =='ID_f012e4242')].head()\n#there is only one member in household for the member with age 10 and who is 'behind in school'. This explains why the member is \n#behind in school.","e8630f3b":"#from above we see that  the 'behind in school' column has null values \n# Lets use the above to fix the data\nfor x in [ds, dt]:\n    x['rez_esc'].fillna(value=0, inplace=True)\nds[['rez_esc']].isnull().sum()","9d82a32b":"data = ds[ds['meaneduc'].isnull()].head()\n\ncolumns=['edjefe','edjefa','instlevel1','instlevel2']\ndata[columns][data[columns]['instlevel1']>0].describe()","443130a1":"#from the above, we find that meaneduc is null when no level of education is 0\n#Lets fix the data\nfor x in [ds, dt]:\n    x['meaneduc'].fillna(value=0, inplace=True)\nds[['meaneduc']].isnull().sum()","d2bc8ebc":"data = ds[ds['SQBmeaned'].isnull()].head()\n\ncolumns=['edjefe','edjefa','instlevel1','instlevel2']\ndata[columns][data[columns]['instlevel1']>0].describe()","ccf22349":"#from the above, we find that SQBmeaned is null when no level of education is 0\n#Lets fix the data\nfor x in [ds, dt]:\n    x['SQBmeaned'].fillna(value=0, inplace=True)\nds[['SQBmeaned']].isnull().sum()","8785267b":"#Lets look at the overall data\nnull_counts = ds.isnull().sum()\nnull_counts[null_counts > 0].sort_values(ascending=False)","b9807a54":"# Groupby the household and figure out the number of unique values\nall_equal = ds.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n\n# Households where targets are not all equal\nnot_equal = all_equal[all_equal != True]\nprint('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))","904d0e1a":"#Lets check one household\nds[ds['idhogar'] == not_equal.index[0]][['idhogar', 'parentesco1', 'Target']]","c91d4a13":"#Lets use Target value of the parent record (head of the household) and update rest. But before that lets check\n# if all families has a head. \n\nhouseholds_head = ds.groupby('idhogar')['parentesco1'].sum()\n\n# Find households without a head\nhouseholds_no_head = ds.loc[ds['idhogar'].isin(households_head[households_head == 0].index), :]\n\nprint('There are {} households without a head.'.format(households_no_head['idhogar'].nunique()))","6864a442":"# Find households without a head and where Target value are different\nhouseholds_no_head_equal = households_no_head.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\nprint('{} Households with no head have different Target value.'.format(sum(households_no_head_equal == False)))","cb5f9ea8":"#Lets fix the data\n#Set poverty level of the members and the head of the house within a family.\n# Iterate through each household\nfor household in not_equal.index:\n    # Find the correct label (for the head of household)\n    true_target = int(ds[(ds['idhogar'] == household) & (ds['parentesco1'] == 1.0)]['Target'])\n    \n    # Set the correct label for all members in the household\n    ds.loc[ds['idhogar'] == household, 'Target'] = true_target\n    \n    \n# Groupby the household and figure out the number of unique values\nall_equal = ds.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n\n# Households where targets are not all equal\nnot_equal = all_equal[all_equal != True]\nprint('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))","7b9c5bd8":"# 1 = extreme poverty 2 = moderate poverty 3 = vulnerable households 4 = non vulnerable households \ntarget_counts = HOD['Target'].value_counts().sort_index()\ntarget_counts","af4c0f0f":"target_counts.plot.bar(figsize = (8, 6),linewidth = 2,edgecolor = 'k',title=\"Target vs Total_Count\")","77d67ce1":"#Lets remove them\nprint(ds.shape)\ncols=['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', \n        'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']\n\n\nfor df in [ds, dt]:\n    df.drop(columns = cols,inplace=True)\n\nprint(ds.shape)","520305df":"id_ = ['Id', 'idhogar', 'Target']\n\nind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n            'instlevel9', 'mobilephone']\n\nind_ordered = ['rez_esc', 'escolari', 'age']\n\nhh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n           'pisonatur', 'pisonotiene', 'pisomadera',\n           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2']\n\nhh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin',\n              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']\n\nhh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding']","3f7a9e6f":"#Check for redundant household variables\nheads = ds.loc[ds['parentesco1'] == 1, :]\nheads = heads[id_ + hh_bool + hh_cont + hh_ordered]\nheads.shape","0a209bcf":"# Create correlation matrix\ncorr_matrix = heads.corr()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nto_drop","d79bbaa5":"corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9]","7935be2d":"\nsns.heatmap(corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.7, corr_matrix['tamhog'].abs() > 0.9],\n            annot=True, cmap = plt.cm.Accent_r, fmt='.3f')\n\nsns.set(style=\"whitegrid\",font_scale=0.7)","defd7419":"cols=['tamhog', 'hogar_total', 'r4t3']\nfor x in [ds, dt]:\n    x.drop(columns = cols,inplace=True)\n\nds.shape","9d8990df":"#Check for redundant Individual variables\nind = ds[id_ + ind_bool + ind_ordered]\nind.shape","e8796057":"# Create correlation matrix\ncorr_matrix = ind.corr()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nto_drop","4e126107":"# This is simply the opposite of male! We can remove the male flag.\nfor x in [ds, dt]:\n    x.drop(columns = 'male',inplace=True)\n\nds.shape","5cd839f7":"#lets check area1 and area2 also\n# area1, =1 zona urbana \n# area2, =2 zona rural \n#area2 redundant because we have a column indicating if the house is in a urban zone\n\nfor x in [ds, dt]:\n    x.drop(columns = 'area2',inplace=True)\n\nds.shape","313671a4":"#Finally lets delete 'Id', 'idhogar'\ncols=['Id','idhogar']\nfor x in [ds, dt]:\n    x.drop(columns = cols,inplace=True)\n\nds.shape","e365975d":"x_features=ds.iloc[:,0:-1] # feature without target\ny_features=ds.iloc[:,-1] # only target\nprint(x_features.shape)\nprint(y_features.shape)","b33f8eef":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,confusion_matrix,f1_score,classification_report\n\nx_train,x_test,y_train,y_test=train_test_split(x_features,y_features,test_size=0.2,random_state=1)\nrmclassifier = RandomForestClassifier()","01778afd":"rmclassifier.fit(x_train,y_train)","11c205b9":"y_predict = rmclassifier.predict(x_test)","0ebce25c":"print(accuracy_score(y_test,y_predict))\nprint(confusion_matrix(y_test,y_predict))\nprint(classification_report(y_test,y_predict))","2e1c8627":"y_predict_testdata = rmclassifier.predict(dt)","375eea7a":"y_predict_testdata","b7b6fa9e":"from sklearn.model_selection import KFold,cross_val_score","7186a3b2":"seed=7\nkfold=KFold(n_splits=5,random_state=seed,shuffle=True)\n\nrmclassifier=RandomForestClassifier(random_state=10,n_jobs = -1)\nprint(cross_val_score(rmclassifier,x_features,y_features,cv=kfold,scoring='accuracy'))\nresults=cross_val_score(rmclassifier,x_features,y_features,cv=kfold,scoring='accuracy')\nprint(results.mean()*100)","ef5e5024":"num_trees= 100\n\nrmclassifier=RandomForestClassifier(n_estimators=100, random_state=10,n_jobs = -1)\nprint(cross_val_score(rmclassifier,x_features,y_features,cv=kfold,scoring='accuracy'))\nresults=cross_val_score(rmclassifier,x_features,y_features,cv=kfold,scoring='accuracy')\nprint(results.mean()*100)","897c0c40":"rmclassifier.fit(x_features,y_features)\nlabels = list(x_features)\nfeature_importances = pd.DataFrame({'feature': labels, 'importance': rmclassifier.feature_importances_})\nfeature_importances=feature_importances[feature_importances.importance>0.015]\nfeature_importances.head()","38368c47":"y_predict_testdata = rmclassifier.predict(dt)\ny_predict_testdata","f36688ea":"feature_importances.sort_values(by=['importance'], ascending=True, inplace=True)\nfeature_importances['positive'] = feature_importances['importance'] > 0\nfeature_importances.set_index('feature',inplace=True)\nfeature_importances.head()\n\nfeature_importances.importance.plot(kind='barh', figsize=(11, 6),color = feature_importances.positive.map({True: 'blue', False: 'red'}))\nplt.xlabel('Importance')","0caddc31":"Dealing With null values for v2a1=6860 which is related to the columns 'tipovivi1','tipovivi2','tipovivi3','tipovivi4','tipovivi5'\n\ntipovivi1, =1 own and fully paid house\ntipovivi2, \"=1 own, paying in installments\"\ntipovivi3, =1 rented\ntipovivi4, =1 precarious\ntipovivi5, \"=1 other(assigned, borrowed)\"\nlets pull Nulls values related to these columns with respect to \"v2a1\"","df07f38a":"Lets look at the dataset and plot head of household and Target","51b3ddd3":"From the above figure, meaneduc,dependency,overcrowding has significant influence on the model.","a9c47720":"x_features, y_features: The first parameter is the dataset you're selecting to use. train_size: This parameter sets the size of the training dataset. There are three options: None, which is the default, Int, which requires the exact number of samples, and float, which ranges from 0.1 to 1.0. test_size: This parameter specifies the size of the testing dataset. The default state suits the training size. It will be set to 0.25 if the training size is set to default. random_state: The default mode performs a random split using np.random. Alternatively, you can add an integer using an exact number.","a943c283":"Lets look at SQBmeaned (total nulls: 5) : square of the mean years of education of adults (>=18) in the household 142 why the null values, Lets look at few rows with nulls in SQBmeaned Columns related to average years of education for adults (18+) edjefe, years of education of male head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0 edjefa, years of education of female head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0 instlevel1, =1 no level of education instlevel2, =1 incomplete primary","b9b6a0bb":"There are several variables here having to do with the size of the house: r4t3, Total persons in the household tamhog, size of the household tamviv, number of persons living in the household hhsize, household size hogar_total, # of total individuals in the household These variables are all highly correlated with one another.","916e61ee":"So, We have only one column that can be related to v18q1, which is v18q= owns a tablet which inturn is related to only the Head of the house.","06b3e7d0":"Lets Convert the Object type columns into Integer type.","984df5e4":" ### We can Notice there are null values for few columns of Float d-type","15a313f0":"extreme poverty is the smallest count in the train dataset. The dataset is biased.\n\nLets look at the Squared Variables \u2018SQBescolari\u2019 \u2018SQBage\u2019 \u2018SQBhogar_total\u2019 \u2018SQBedjefe\u2019 \u2018SQBhogar_nin\u2019 \u2018SQBovercrowding\u2019 \u2018SQBdependency\u2019 \u2018SQBmeaned\u2019 \u2018agesq\u2019","9a7a93e2":"Lets fill the Null values","2b10ef7b":"So from this we can understand that, when ever \"own and fully paid house\" tipovivi1=1 , there is no neccessity to pay Monthly rent v2a1. And hence, 5911 Rows in v2a1 are mentioned as Null since tipovivi1=1 for these Rows. Similarly, tipovivi4=163 \"precarious\" and tipovivi5=786 \"other(assigned, borrowed)\" are not paying the rent due to their respective reasons.","cd417bc7":"Predict the accuracy using random forest classifier.","45a430ef":"Replacing the null values v2a1 6860\n\n","f334590a":"Looking at the above data it makes sense that when owns a tablet column is 0, there will be no number of tablets household owns. Lets add 0 for all the null values.","029b29f9":"### Exploratory Visualisation","17dbea19":"### Lets it=dentify the null Values.","211d29a7":"Lets look at rez_esc (total nulls: 7928) : Years behind in school why the null values, Lets look at few rows with nulls in rez_esc Columns related to Years behind in school Age in years","80030c08":"Lets look at meaneduc (total nulls: 5) : average years of education for adults (18+) why the null values, Lets look at few rows with nulls in meaneduc Columns related to average years of education for adults (18+) edjefe, years of education of male head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0 edjefa, years of education of female head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0 instlevel1, =1 no level of education instlevel2, =1 incomplete primary","38be317d":"Lets Deal with \"number of tablets household owns\" v18q1= 7342","4b43a8cc":"###we can see that there is one column difference between training and testing data. Lets find out the column and check if that is a Target column.\n","a731c17c":"From the above , we see that when min age is 7 and max age is 17 for Years, then the 'behind in school' column has a value. Lets confirm"}}