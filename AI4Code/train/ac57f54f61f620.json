{"cell_type":{"ec25eeae":"code","029dcbac":"code","b9d0bbce":"code","4f79fa4b":"code","0c599382":"code","61397bbf":"code","3da1199d":"code","733a545f":"code","dea2b8b2":"code","39560c84":"code","dde43fd4":"code","30ddd5cb":"code","a8a7ea9d":"code","bfffbc5a":"code","18bb95f7":"code","acca10d2":"code","e03172ee":"code","25702c2f":"code","f26a33ce":"code","d592a523":"code","d9ef1fd5":"code","ee3e26b0":"code","38bc793d":"code","83ee2e09":"code","eddcc206":"code","fac382c4":"code","ce5cf7d4":"code","9346704c":"code","9024d32a":"code","d89a2546":"code","46914cdc":"code","0d42879d":"code","54e2c58a":"code","0f874c70":"code","d0fc311b":"code","ac3a20dd":"code","42c2e483":"code","9a366d73":"code","bd3682c7":"code","8b51d172":"code","6c032861":"code","f0cd69a9":"code","5419196a":"code","92cf37ba":"code","10aad3ab":"code","610cb06d":"code","d701ef39":"code","b5dc1711":"code","af28d8f9":"code","16227b0a":"code","cd01dde1":"code","1b0b5c4b":"code","be158ee1":"code","2a02097c":"code","8435c578":"code","f0b98030":"code","3c5571f3":"code","f13abd51":"code","d830566f":"code","7112381b":"code","f08be63f":"code","7c089786":"code","58a8f120":"code","7b9f01c7":"code","12599358":"code","dcc91533":"code","7857ca8c":"code","8044529b":"code","db83fecf":"code","2c592f5d":"code","f72aa21b":"code","b02ed8c9":"code","86e9b7c5":"code","fa6fd59f":"code","6a4c1905":"code","5a32c9c2":"code","2b379eb2":"code","dcd36942":"code","63282596":"code","d7fd40dc":"code","91487a6a":"code","e0f0cc40":"code","0f4c1ddb":"code","68917aea":"code","64e42c57":"code","7a54a7de":"code","b8877430":"code","386c2d10":"code","31b51624":"code","3e3bfb32":"code","7b7ab84b":"code","054a9bf7":"code","674de905":"code","ed9394ec":"code","9bf9e82e":"code","4b192388":"code","f686e1c0":"code","e02dfc22":"code","543a4be5":"code","8b136ee7":"code","9e9db262":"code","9ce6b6ff":"code","98d25347":"code","1ccb9195":"code","a2192516":"code","e99a7ae1":"code","8e1f21c0":"code","1b1b636a":"code","b7679055":"code","29d08e82":"code","93b21082":"code","ce4c5ece":"code","2d40f707":"code","daf1de96":"code","23854b3e":"code","a3650256":"code","26609995":"code","53c7c054":"code","70e695ac":"code","7c2613e7":"code","7f4dce9a":"code","027f4c3a":"code","a044accd":"code","c0be44de":"code","c4678783":"code","617c2f5c":"code","a4a54fdf":"code","9f9cff5c":"code","1b1c5be4":"code","1a7e9a7f":"code","fcd9c40c":"code","1f372567":"code","6845e181":"code","a87f61fa":"code","93e35fd2":"code","bfeb1832":"code","89b8114d":"code","db980101":"code","7ef5c777":"code","427f5851":"code","dee64d02":"code","a4edb96e":"code","c982cdc3":"code","60d66c17":"code","82b530ca":"code","f20eb92a":"code","cd474658":"code","ccf5bb21":"code","082e639c":"code","bd15ab62":"code","cd118a5a":"code","26ace150":"code","d33698ed":"code","7bfea42a":"code","a5c5b537":"code","c39fe439":"code","85dc69d9":"code","65575825":"code","3562f56a":"code","f1e56d16":"code","44a0039f":"code","c970f02e":"code","ea088ac0":"code","0d82b57f":"code","6bf8ca30":"code","0e822dd5":"code","d8f5004e":"code","3ca7ec35":"code","4cf3b3f9":"code","1af50dc9":"code","39a0450b":"code","3c8bc7a5":"code","cfe139ef":"code","6ef8bff4":"code","b766f76b":"code","f1cb0461":"code","922713a2":"code","aecb5481":"code","1edee6e1":"code","895b0ff6":"code","2121ccf3":"code","779955d5":"code","a3585b90":"code","f4a1d701":"code","c2823de7":"code","fb32639b":"code","1f6dd9e3":"code","a42db50e":"code","89a39d38":"code","7eee93cd":"code","892a00bb":"code","fb5d8a22":"code","e7929c4b":"code","a8081343":"code","2ea57325":"code","d8b0ca23":"code","06294ebb":"code","9d2c19ec":"code","bed17c27":"code","5f2f14b7":"code","35a390ec":"code","327f79e0":"code","aefda817":"code","01018365":"code","0607c3e3":"code","406d2777":"code","c39748f2":"code","02fd7016":"code","e12bf177":"code","b38e5144":"code","12c42313":"code","40d2f18f":"code","8bd8391f":"code","5c63bff8":"code","b5d8cc96":"code","fb5e5a7d":"code","4744eaf7":"code","2e3f91a0":"code","be383dc4":"code","11823351":"code","bd557a98":"code","ea9ed2be":"code","8ca387e0":"code","932cf9e8":"code","59dd31e6":"code","ccb8b757":"code","ed85152e":"code","c5f2f384":"code","c8f33ccb":"code","3f1f38d7":"code","11e49253":"code","98364744":"code","f60cb13b":"code","7efcc69d":"code","3ca6c72f":"code","7ac681d9":"code","d3def62f":"code","093835c3":"code","78b99c3f":"code","fa051d14":"code","cd11f329":"code","3b920165":"code","e178c2f8":"code","7b801fcb":"code","c08933e9":"code","3ffae73e":"code","9c9b387a":"code","3d65f614":"code","a78fd30b":"code","d1f4e094":"code","0160100a":"code","ae0e5696":"code","f0a917b6":"code","2dfc3cfd":"code","4097b832":"code","5c2fbade":"code","71434364":"code","11f395df":"code","63520ea0":"code","43a3e62c":"code","38ffe187":"code","e19469ed":"code","27c98bd0":"code","5522ea68":"code","e1ebb6f9":"code","1bb9c845":"code","d355977a":"code","77a8efba":"code","85b757e6":"code","ffd0261c":"code","ab39ace4":"code","57232e53":"code","429733b5":"code","ba411a5f":"code","420fd267":"code","a0edd60f":"code","18403a61":"code","b391ff56":"code","f0d90c09":"code","2b42021f":"code","9a376850":"code","97d71613":"code","22249c2e":"code","35cbee43":"code","112fca78":"code","3a3c7b2c":"code","8e7f79da":"code","265fd3a2":"code","660b6fe6":"code","145f7986":"code","a62d553c":"code","ccfccb18":"code","10eba669":"code","af655887":"code","61865cfd":"code","1153095b":"code","d596cfd3":"code","9669db6c":"code","99db6edd":"code","e33fad36":"code","684343b5":"code","41e3ac12":"code","f919c230":"code","42d827a8":"code","a5723198":"code","f1c71540":"code","457ca24a":"code","77886162":"code","baabd174":"code","16a80288":"code","f05f31b4":"code","dfb8a9f0":"code","8afa4bf4":"code","f78a998d":"code","a868d675":"code","5bc2889c":"code","c1a2ecf9":"code","fe11d16a":"code","5b9d96bf":"code","ff17c091":"code","2c0ac0b4":"code","c79e05b7":"code","5c81eb5a":"code","696658ab":"code","7eb24087":"code","f1e540e2":"code","acf9c2d2":"code","491b6aec":"code","b071cbf6":"code","6868843e":"code","39d96ca5":"code","a704f1ec":"code","9de299e8":"code","d1433e4b":"code","996f2c48":"code","1f04fd58":"code","fe437986":"code","42c4404e":"code","2c9698e2":"code","f45d6f17":"code","7fd04fa3":"code","8cd46d21":"code","09649238":"code","00aca9c1":"code","61fb53ee":"code","f2e9fb66":"code","670d202e":"code","c6eb67e6":"code","97b51359":"code","4c09e09c":"code","83558ed7":"code","442f17cf":"code","e9fe7a68":"code","a7bbe941":"code","0accc224":"code","4c13e2e9":"code","2176f085":"code","da4bf7a9":"code","ef08ab08":"code","26a9f80f":"code","c434ffa2":"code","b9521a2b":"code","3698e9ad":"code","5f7f3701":"code","c41a0319":"code","a42e1da1":"code","e1d58d54":"code","01792457":"code","3712e3d5":"code","a801e379":"code","a9f14a78":"code","26ab5800":"code","64d93095":"markdown","6638e9b5":"markdown","c38833d0":"markdown","963309d4":"markdown","e5760515":"markdown","8f5732c7":"markdown","498932ff":"markdown","6c97c21f":"markdown","f0d6b3ac":"markdown","1c04200f":"markdown","4ef0ea21":"markdown","0318dd0d":"markdown","7e5dda20":"markdown","78461966":"markdown","0806ac40":"markdown","d58aae61":"markdown","074fabfc":"markdown","a1b189e0":"markdown","e27e6177":"markdown","43c26c5f":"markdown","c170c496":"markdown","672a6b3a":"markdown","5099dfdd":"markdown","68b026a1":"markdown","85d16440":"markdown","08e38251":"markdown","5d1744e5":"markdown","017a48dc":"markdown","f94e4143":"markdown","7996e35c":"markdown","484d45b5":"markdown","c3bcc580":"markdown","08b6f8bf":"markdown","89d775f6":"markdown","383fe3a5":"markdown","7fa54311":"markdown","b0d0840c":"markdown","98743597":"markdown","d44ed211":"markdown","5b79c434":"markdown","38df48fa":"markdown","ec03bb89":"markdown","dd76cc28":"markdown","686eea3e":"markdown","06e18c89":"markdown","04539108":"markdown","17c031d5":"markdown","0fc4c3b8":"markdown","91ce899c":"markdown","b574fba5":"markdown","b31fa0e6":"markdown","34c435ee":"markdown","429d0eb5":"markdown","9021e689":"markdown"},"source":{"ec25eeae":"import pandas as pd \nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","029dcbac":"loan_data=pd.read_csv(\"..\/input\/loan-data\/3.1 loan_data_2007_2014.csv\")\nloan_data.head()","b9d0bbce":"pd.options.display.max_columns = None\n#pd.options.display.max_rows = None\n# Sets the pandas dataframe options to display all columns\/ rows.","4f79fa4b":"loan_data = loan_data.copy()","0c599382":"del loan_data['Unnamed: 0']","61397bbf":"loan_data.info()\n# Displays column names, complete (non-missing) cases per column, and datatype per column.","3da1199d":"loan_data.isna().sum()","733a545f":"loan_data.columns.values\n# Displays all column names.","dea2b8b2":"loan_data['emp_length'].unique()","39560c84":"loan_data['emp_length_int'] = loan_data['emp_length'].str.replace('\\+ years', '')\nloan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace('< 1 year', str(0))\nloan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace('n\/a',  str(0))\nloan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace(' years', '')\nloan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace(' year', '')","dde43fd4":"type(loan_data['emp_length_int'][0])","30ddd5cb":"# covert to int\nloan_data['emp_length_int'] = pd.to_numeric(loan_data['emp_length_int'])\ntype(loan_data['emp_length_int'][0])","a8a7ea9d":"loan_data['earliest_cr_line']","bfffbc5a":"loan_data['earliest_cr_line_date'] = pd.to_datetime(loan_data['earliest_cr_line'], format = '%b-%y')\ntype(loan_data['earliest_cr_line_date'][0])","18bb95f7":" loan_data['earliest_cr_line_date']","acca10d2":"round(pd.to_numeric((pd.to_datetime('2017-12-01') - loan_data['earliest_cr_line_date'])))","e03172ee":"round(pd.to_numeric((pd.to_datetime('2017-12-01') - loan_data['earliest_cr_line_date']) \/ np.timedelta64(1, 'M')))","25702c2f":"np.timedelta64(1, 'M')","f26a33ce":"pd.to_datetime('2017-12-01') - loan_data['earliest_cr_line_date']\nloan_data['mths_since_earliest_cr_line'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - loan_data['earliest_cr_line_date']) \/ np.timedelta64(1, 'M')))\nloan_data['mths_since_earliest_cr_line'].describe()","d592a523":"loan_data.loc[: , ['earliest_cr_line', 'earliest_cr_line_date', 'mths_since_earliest_cr_line']][loan_data['mths_since_earliest_cr_line'] < 0]","d9ef1fd5":"loan_data['mths_since_earliest_cr_line'][loan_data['mths_since_earliest_cr_line'] < 0] = loan_data['mths_since_earliest_cr_line'].max()","ee3e26b0":"min(loan_data['mths_since_earliest_cr_line'])","38bc793d":"loan_data['term'].head()","83ee2e09":"loan_data['term'].describe()\n# Shows some descriptive statisics for the values of a column.\n","eddcc206":"type(loan_data['term'][25])","fac382c4":"# Checks the datatype of a single element of a column.\nloan_data['term_int'] = pd.to_numeric(loan_data['term'].str.replace(' months', ''))\ntype(loan_data['term_int'][25])","ce5cf7d4":"loan_data['issue_d']","9346704c":"# Assume we are now in December 2017\n# covert to date\nloan_data['issue_d_date'] = pd.to_datetime(loan_data['issue_d'], format = '%b-%y')\n# Extracts the date and the time from a string variable that is in a given format.\nloan_data['mths_since_issue_d'] = round(pd.to_numeric((pd.to_datetime('2017-12-01') - loan_data['issue_d_date']) \/ np.timedelta64(1, 'M')))\n# We calculate the difference between two dates in months, turn it to numeric datatype and round it.\n# We save the result in a new variable.\nloan_data['mths_since_issue_d'].describe()\n# Shows some descriptive statisics for the values of a column.","9024d32a":"loan_data.info()\n# Displays column names, complete (non-missing) cases per column, and datatype per column.","d89a2546":"loan_data.isnull().sum()","46914cdc":"loan_data[['grade', 'sub_grade', 'home_ownership', 'verification_status','loan_status', 'purpose', 'addr_state', 'initial_list_status']]","0d42879d":"loan_data_dummies = [pd.get_dummies(loan_data['grade'], prefix = 'grade', prefix_sep = ':'),\n                     pd.get_dummies(loan_data['sub_grade'], prefix = 'sub_grade', prefix_sep = ':'),\n                     pd.get_dummies(loan_data['home_ownership'], prefix = 'home_ownership', prefix_sep = ':'),\n                     pd.get_dummies(loan_data['verification_status'], prefix = 'verification_status', prefix_sep = ':'),\n                     pd.get_dummies(loan_data['loan_status'], prefix = 'loan_status', prefix_sep = ':'),\n                     pd.get_dummies(loan_data['purpose'], prefix = 'purpose', prefix_sep = ':'),\n                     pd.get_dummies(loan_data['addr_state'], prefix = 'addr_state', prefix_sep = ':'),\n                     pd.get_dummies(loan_data['initial_list_status'], prefix = 'initial_list_status', prefix_sep = ':')]\n# We create dummy variables from all 8 original independent variables, and save them into a list.\n# Note that we are using a particular naming convention for all variables: original variable name, colon, category name.\nloan_data_dummies","54e2c58a":"loan_data_dummies = pd.concat(loan_data_dummies, axis = 1)\n# We concatenate the dummy variables and this turns them into a dataframe.","0f874c70":"type(loan_data_dummies)\n# Returns the type of the variable.","d0fc311b":"loan_data = pd.concat([loan_data, loan_data_dummies], axis = 1)\n# Concatenates two dataframes.\n# Here we concatenate the dataframe with original data with the dataframe with dummy variables, along the columns. ","ac3a20dd":"loan_data.columns.values\n# Displays all column names.","42c2e483":"loan_data","9a366d73":"pd.options.display.max_rows = None\n# Sets the pandas dataframe options to display all columns\/ rows.\nloan_data.isnull().sum()","bd3682c7":"pd.options.display.max_rows = 100\n# Sets the pandas dataframe options to display 100 columns\/ rows.\n# 'Total revolving high credit\/ credit limit', so it makes sense that the missing values are equal to funded_amnt.\nloan_data['total_rev_hi_lim'].fillna(loan_data['funded_amnt'], inplace=True)\n# We fill the missing values with the values of another variable.\nloan_data['total_rev_hi_lim'].isnull().sum()","8b51d172":"loan_data['annual_inc'].fillna(loan_data['annual_inc'].mean(), inplace=True)\n# We fill the missing values with the mean value of the non-missing values.","6c032861":"loan_data['mths_since_earliest_cr_line'].fillna(0, inplace=True)\nloan_data['acc_now_delinq'].fillna(0, inplace=True)\nloan_data['total_acc'].fillna(0, inplace=True)\nloan_data['pub_rec'].fillna(0, inplace=True)\nloan_data['open_acc'].fillna(0, inplace=True)\nloan_data['inq_last_6mths'].fillna(0, inplace=True)\nloan_data['delinq_2yrs'].fillna(0, inplace=True)\nloan_data['emp_length_int'].fillna(0, inplace=True)\n# We fill the missing values with zeroes.\nloan_data.isna().sum()","f0cd69a9":"loan_data['loan_status'].unique()\n# Displays unique values of a column.","5419196a":"loan_data['loan_status'].value_counts()\n# Calculates the number of observations for each unique value of a variable.","92cf37ba":"loan_data['loan_status'].count()","10aad3ab":"loan_data['loan_status'].value_counts() \/ loan_data['loan_status'].count()\n# We divide the number of observations for each unique value of a variable by the total number of observations.\n# Thus, we get the proportion of observations for each unique value of a variable.","610cb06d":"# Good\/ Bad Definition\nloan_data['good_bad'] = np.where(loan_data['loan_status'].isin(['Charged Off', 'Default',\n                                                       'Does not meet the credit policy. Status:Charged Off',\n                                                       'Late (31-120 days)']), 0, 1)\n# We create a new variable that has the value of '0' if a condition is met, and the value of '1' if it is not met.\nloan_data['good_bad']","d701ef39":"loan_data_inputs_train, loan_data_inputs_test, loan_data_targets_train, loan_data_targets_test = train_test_split(loan_data.drop('good_bad', axis = 1), loan_data['good_bad'])\n# We split two dataframes with inputs and targets, each into a train and test dataframe, and store them in variables.\n# Takes a set of inputs and a set of targets as arguments. Splits the inputs and the targets into four dataframes:\n# Inputs - Train, Inputs - Test, Targets - Train, Targets - Test.","b5dc1711":"print('loan_data_inputs_train.shape:',loan_data_inputs_train.shape)\n# Displays the size of the dataframe.\nprint('loan_data_targets_train.shape:',loan_data_targets_train.shape)\nprint('loan_data_inputs_test.shape:', loan_data_inputs_test.shape)\n# Displays the size of the dataframe.\nprint('loan_data_targets_test.shape:',loan_data_targets_test.shape)","af28d8f9":"loan_data_inputs_train, loan_data_inputs_test, loan_data_targets_train, loan_data_targets_test = train_test_split(loan_data.drop('good_bad', axis = 1), loan_data['good_bad'], test_size = 0.2, random_state = 42)\n# We split two dataframes with inputs and targets, each into a train and test dataframe, and store them in variables.\n# This time we set the size of the test dataset to be 20%.\n# Respectively, the size of the train dataset becomes 80%.\n# We also set a specific random state.\n# This would allow us to perform the exact same split multimple times.\n# This means, to assign the exact same observations to the train and test datasets.","16227b0a":"print('loan_data_inputs_train.shape:',loan_data_inputs_train.shape)\n# Displays the size of the dataframe.\nprint('loan_data_targets_train.shape:',loan_data_targets_train.shape)\nprint('loan_data_inputs_test.shape:', loan_data_inputs_test.shape)\n# Displays the size of the dataframe.\nprint('loan_data_targets_test.shape:',loan_data_targets_test.shape)","cd01dde1":"#####\n#df_inputs_prepr = loan_data_inputs_train\n#df_targets_prepr = loan_data_targets_train\n#####\ndf_inputs_prepr = loan_data_inputs_test\ndf_targets_prepr = loan_data_targets_test","1b0b5c4b":"df_inputs_prepr['grade'].unique()\n# Displays unique values of a column.","be158ee1":"df1 = pd.concat([df_inputs_prepr['grade'], df_targets_prepr], axis = 1)\n# Concatenates two dataframes along the columns.\ndf1.head()","2a02097c":"df1.columns.values[1]","8435c578":"df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].count()\n# Groups the data according to a criterion contained in one column.\n# Does not turn the names of the values of the criterion as indexes.\n# Aggregates the data in another column, using a selected function.\n# In this specific case, we group by the column with index 0 and we aggregate the values of the column with index 1.\n# More specifically, we count them.\n# In other words, we count the values in the column with index 1 for each value of the column with index 0.","f0b98030":"df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].mean()\n# Groups the data according to a criterion contained in one column.\n# Does not turn the names of the values of the criterion as indexes.\n# Aggregates the data in another column, using a selected function.\n# Here we calculate the mean of the values in the column with index 1 for each value of the column with index 0.","3c5571f3":"df1 = pd.concat([df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].count(),\n                df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].mean()], axis = 1)\n# Concatenates two dataframes along the columns.\ndf1","f13abd51":"df1 = df1.iloc[:, [0, 1, 3]]\n# Selects only columns with specific indexes.\ndf1","d830566f":"df1.columns = [df1.columns.values[0], 'n_obs', 'prop_good']\n# Changes the names of the columns of a dataframe.\ndf1","7112381b":"df1['prop_n_obs'] = df1['n_obs'] \/ df1['n_obs'].sum()\n# We divide the values of one column by he values of another column and save the result in a new variable.\ndf1","f08be63f":"df1['n_good'] = df1['prop_good'] * df1['n_obs']\n# We multiply the values of one column by he values of another column and save the result in a new variable.\ndf1['n_bad'] = (1 - df1['prop_good']) * df1['n_obs']\ndf1","7c089786":"df1['prop_n_good'] = df1['n_good'] \/ df1['n_good'].sum()\ndf1['prop_n_bad'] = df1['n_bad'] \/ df1['n_bad'].sum()\ndf1","58a8f120":"df1['WoE'] = np.log(df1['prop_n_good'] \/ df1['prop_n_bad'])\n# We take the natural logarithm of a variable and save the result in a nex variable.\ndf1","7b9f01c7":"df1 = df1.sort_values(['WoE'])\n# Sorts a dataframe by the values of a given column.\ndf1 = df1.reset_index(drop = True)\n# We reset the index of a dataframe and overwrite it.\ndf1","12599358":"df1['diff_prop_good'] = df1['prop_good'].diff().abs()\n# We take the difference between two subsequent values of a column. Then, we take the absolute value of the result.\ndf1['diff_WoE'] = df1['WoE'].diff().abs()\n# We take the difference between two subsequent values of a column. Then, we take the absolute value of the result.\ndf1","dcc91533":"df1['IV'] = (df1['prop_n_good'] - df1['prop_n_bad']) * df1['WoE']\ndf1['IV'] = df1['IV'].sum()\n# We sum all values of a given column.\ndf1","7857ca8c":"# WoE function for discrete unordered variables\ndef woe_discrete(df, discrete_variabe_name, good_bad_variable_df):\n    df = pd.concat([df[discrete_variabe_name], good_bad_variable_df], axis = 1)\n    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),\n                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)\n    df = df.iloc[:, [0, 1, 3]]\n    df.columns = [df.columns.values[0], 'n_obs', 'prop_good']\n    df['prop_n_obs'] = df['n_obs'] \/ df['n_obs'].sum()\n    df['n_good'] = df['prop_good'] * df['n_obs']\n    df['n_bad'] = (1 - df['prop_good']) * df['n_obs']\n    df['prop_n_good'] = df['n_good'] \/ df['n_good'].sum()\n    df['prop_n_bad'] = df['n_bad'] \/ df['n_bad'].sum()\n    df['WoE'] = np.log(df['prop_n_good'] \/ df['prop_n_bad'])\n    df = df.sort_values(['WoE'])\n    df = df.reset_index(drop = True)\n    df['diff_prop_good'] = df['prop_good'].diff().abs()\n    df['diff_WoE'] = df['WoE'].diff().abs()\n    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']\n    df['IV'] = df['IV'].sum()\n    return df\n# Here we combine all of the operations above in a function.\n# The function takes 3 arguments: a dataframe, a string, and a dataframe. The function returns a dataframe as a result.","8044529b":"# 'grade'\ndf_temp = woe_discrete(df_inputs_prepr, 'grade', df_targets_prepr)\n# We execute the function we defined with the necessary arguments: a dataframe, a string, and a dataframe.\n# We store the result in a dataframe.\ndf_temp","db83fecf":"# Below we define a function that takes 2 arguments: a dataframe and a number.\n# The number parameter has a default value of 0.\n# This means that if we call the function and omit the number parameter, it will be executed with it having a value of 0.\n# The function displays a graph.\ndef plot_by_woe(df_WoE, rotation_of_x_axis_labels = 0):\n    x = np.array(df_WoE.iloc[:, 0].apply(str))\n    # Turns the values of the column with index 0 to strings, makes an array from these strings, and passes it to variable x.\n    y = df_WoE['WoE']\n    # Selects a column with label 'WoE' and passes it to variable y.\n    plt.figure(figsize=(18, 6))\n    # Sets the graph size to width 18 x height 6.\n    plt.plot(x, y, marker = 'o', linestyle = '--', color = 'k')\n    # Plots the datapoints with coordiantes variable x on the x-axis and variable y on the y-axis.\n    # Sets the marker for each datapoint to a circle, the style line between the points to dashed, and the color to black.\n    plt.xlabel(df_WoE.columns[0])\n    # Names the x-axis with the name of the column with index 0.\n    plt.ylabel('Weight of Evidence')\n    # Names the y-axis 'Weight of Evidence'.\n    plt.title(str('Weight of Evidence by ' + df_WoE.columns[0]))\n    # Names the grapth 'Weight of Evidence by ' the name of the column with index 0.\n    plt.xticks(rotation = rotation_of_x_axis_labels)\n    # Rotates the labels of the x-axis a predefined number of degrees.","2c592f5d":"plot_by_woe(df_temp)\n# We execute the function we defined with the necessary arguments: a dataframe.\n# We omit the number argument, which means the function will use its default value, 0.","f72aa21b":"# WoE function for ordered discrete and continuous variables\ndef woe_ordered_continuous(df, discrete_variabe_name, good_bad_variable_df):\n    df = pd.concat([df[discrete_variabe_name], good_bad_variable_df], axis = 1)\n    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),\n                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)\n    df = df.iloc[:, [0, 1, 3]]\n    df.columns = [df.columns.values[0], 'n_obs', 'prop_good']\n    df['prop_n_obs'] = df['n_obs'] \/ df['n_obs'].sum()\n    df['n_good'] = df['prop_good'] * df['n_obs']\n    df['n_bad'] = (1 - df['prop_good']) * df['n_obs']\n    df['prop_n_good'] = df['n_good'] \/ df['n_good'].sum()\n    df['prop_n_bad'] = df['n_bad'] \/ df['n_bad'].sum()\n    df['WoE'] = np.log(df['prop_n_good'] \/ df['prop_n_bad'])\n    #df = df.sort_values(['WoE'])\n    #df = df.reset_index(drop = True)\n    df['diff_prop_good'] = df['prop_good'].diff().abs()\n    df['diff_WoE'] = df['WoE'].diff().abs()\n    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']\n    df['IV'] = df['IV'].sum()\n    return df\n# Here we define a function similar to the one above, ...\n# ... with one slight difference: we order the results by the values of a different column.\n# The function takes 3 arguments: a dataframe, a string, and a dataframe. The function returns a dataframe as a result.","b02ed8c9":"# term\ndf_inputs_prepr['term_int'].unique()\n# There are only two unique values, 36 and 60.","86e9b7c5":"df_temp = woe_ordered_continuous(df_inputs_prepr, 'term_int', df_targets_prepr)\n# We calculate weight of evidence.\ndf_temp","fa6fd59f":"plot_by_woe(df_temp)\n# We plot the weight of evidence values.","6a4c1905":"# Leave as is.\n# '60' will be the reference category.\ndf_inputs_prepr['term:36'] = np.where((df_inputs_prepr['term_int'] == 36), 1, 0)\ndf_inputs_prepr['term:60'] = np.where((df_inputs_prepr['term_int'] == 60), 1, 0)\n# emp_length_int\ndf_inputs_prepr['emp_length_int'].unique()\n# Has only 11 levels: from 0 to 10. Hence, we turn it into a factor with 11 levels.","5a32c9c2":"# We create the following categories: '0', '1', '2 - 4', '5 - 6', '7 - 9', '10'\n# '0' will be the reference category\ndf_inputs_prepr['emp_length:0'] = np.where(df_inputs_prepr['emp_length_int'].isin([0]), 1, 0)\ndf_inputs_prepr['emp_length:1'] = np.where(df_inputs_prepr['emp_length_int'].isin([1]), 1, 0)\ndf_inputs_prepr['emp_length:2-4'] = np.where(df_inputs_prepr['emp_length_int'].isin(range(2, 5)), 1, 0)\ndf_inputs_prepr['emp_length:5-6'] = np.where(df_inputs_prepr['emp_length_int'].isin(range(5, 7)), 1, 0)\ndf_inputs_prepr['emp_length:7-9'] = np.where(df_inputs_prepr['emp_length_int'].isin(range(7, 10)), 1, 0)\ndf_inputs_prepr['emp_length:10'] = np.where(df_inputs_prepr['emp_length_int'].isin([10]), 1, 0)","2b379eb2":"df_inputs_prepr['mths_since_issue_d'].unique()","dcd36942":"df_inputs_prepr['mths_since_issue_d']","63282596":"df_inputs_prepr['mths_since_issue_d_factor'] = pd.cut(df_inputs_prepr['mths_since_issue_d'], 50)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\ndf_inputs_prepr['mths_since_issue_d_factor']","d7fd40dc":"df_inputs_prepr['mths_since_issue_d_factor'].iloc[-1]","91487a6a":"# mths_since_issue_d\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'mths_since_issue_d_factor', df_targets_prepr)\n# We calculate weight of evidence.\ndf_temp","e0f0cc40":"plot_by_woe(df_temp)\nplt.xticks(rotation=90)\n# We plot the weight of evidence values.\n# We have to rotate the labels because we cannot read them otherwise.","0f4c1ddb":"plot_by_woe(df_temp, 90)\n# We plot the weight of evidence values, rotating the labels 90 degrees.","68917aea":"plot_by_woe(df_temp.iloc[3: , : ], 90)\n# We plot the weight of evidence values.","64e42c57":"# int_rate\ndf_inputs_prepr['int_rate_factor'] = pd.cut(df_inputs_prepr['int_rate'], 50)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'int_rate_factor', df_targets_prepr)\n# We calculate weight of evidence.\ndf_temp","7a54a7de":"plot_by_woe(df_temp, 90)\n# We plot the weight of evidence values.","b8877430":"df_inputs_prepr['int_rate:<9.548'] = np.where((df_inputs_prepr['int_rate'] <= 9.548), 1, 0)\ndf_inputs_prepr['int_rate:9.548-12.025'] = np.where((df_inputs_prepr['int_rate'] > 9.548) & (df_inputs_prepr['int_rate'] <= 12.025), 1, 0)\ndf_inputs_prepr['int_rate:12.025-15.74'] = np.where((df_inputs_prepr['int_rate'] > 12.025) & (df_inputs_prepr['int_rate'] <= 15.74), 1, 0)\ndf_inputs_prepr['int_rate:15.74-20.281'] = np.where((df_inputs_prepr['int_rate'] > 15.74) & (df_inputs_prepr['int_rate'] <= 20.281), 1, 0)\ndf_inputs_prepr['int_rate:>20.281'] = np.where((df_inputs_prepr['int_rate'] > 20.281), 1, 0)\n# funded_amnt\ndf_inputs_prepr['funded_amnt_factor'] = pd.cut(df_inputs_prepr['funded_amnt'], 50)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'funded_amnt_factor', df_targets_prepr)\n# We calculate weight of evidence.\ndf_temp","386c2d10":"plot_by_woe(df_temp, 90)\n# We plot the weight of evidence values.","31b51624":"# mths_since_earliest_cr_line\ndf_inputs_prepr['mths_since_earliest_cr_line_factor'] = pd.cut(df_inputs_prepr['mths_since_earliest_cr_line'], 50)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'mths_since_earliest_cr_line_factor', df_targets_prepr)\n# We calculate weight of evidence.\ndf_temp","3e3bfb32":"plot_by_woe(df_temp, 90)\n# We plot the weight of evidence values.","7b7ab84b":"plot_by_woe(df_temp.iloc[6: , : ], 90)\n# We plot the weight of evidence values.","054a9bf7":"# We create the following categories:\n# < 140, # 141 - 164, # 165 - 247, # 248 - 270, # 271 - 352, # > 352\ndf_inputs_prepr['mths_since_earliest_cr_line:<140'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(140)), 1, 0)\ndf_inputs_prepr['mths_since_earliest_cr_line:141-164'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(140, 165)), 1, 0)\ndf_inputs_prepr['mths_since_earliest_cr_line:165-247'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(165, 248)), 1, 0)\ndf_inputs_prepr['mths_since_earliest_cr_line:248-270'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(248, 271)), 1, 0)\ndf_inputs_prepr['mths_since_earliest_cr_line:271-352'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(271, 353)), 1, 0)\ndf_inputs_prepr['mths_since_earliest_cr_line:>352'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(353, int(df_inputs_prepr['mths_since_earliest_cr_line'].max()))), 1, 0)\n# delinq_2yrs\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'delinq_2yrs', df_targets_prepr)\n# We calculate weight of evidence.\ndf_temp","674de905":"plot_by_woe(df_temp)\n# We plot the weight of evidence values.","ed9394ec":"# Categories: 0, 1-3, >=4\ndf_inputs_prepr['delinq_2yrs:0'] = np.where((df_inputs_prepr['delinq_2yrs'] == 0), 1, 0)\ndf_inputs_prepr['delinq_2yrs:1-3'] = np.where((df_inputs_prepr['delinq_2yrs'] >= 1) & (df_inputs_prepr['delinq_2yrs'] <= 3), 1, 0)\ndf_inputs_prepr['delinq_2yrs:>=4'] = np.where((df_inputs_prepr['delinq_2yrs'] >= 9), 1, 0)\n# inq_last_6mths\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'inq_last_6mths', df_targets_prepr)\n# We calculate weight of evidence.\ndf_temp","9bf9e82e":"plot_by_woe(df_temp)\n# We plot the weight of evidence values.","4b192388":"# Categories: 0, 1 - 2, 3 - 6, > 6\ndf_inputs_prepr['inq_last_6mths:0'] = np.where((df_inputs_prepr['inq_last_6mths'] == 0), 1, 0)\ndf_inputs_prepr['inq_last_6mths:1-2'] = np.where((df_inputs_prepr['inq_last_6mths'] >= 1) & (df_inputs_prepr['inq_last_6mths'] <= 2), 1, 0)\ndf_inputs_prepr['inq_last_6mths:3-6'] = np.where((df_inputs_prepr['inq_last_6mths'] >= 3) & (df_inputs_prepr['inq_last_6mths'] <= 6), 1, 0)\ndf_inputs_prepr['inq_last_6mths:>6'] = np.where((df_inputs_prepr['inq_last_6mths'] > 6), 1, 0)\n# open_acc\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'open_acc', df_targets_prepr)\n# We calculate weight of evidence.\ndf_temp.head()","f686e1c0":"plot_by_woe(df_temp, 90)\n# We plot the weight of evidence values.","e02dfc22":"plot_by_woe(df_temp.iloc[ : 40, :], 90)\n# We plot the weight of evidence values.","543a4be5":"# Categories: '0', '1-3', '4-12', '13-17', '18-22', '23-25', '26-30', '>30'\ndf_inputs_prepr['open_acc:0'] = np.where((df_inputs_prepr['open_acc'] == 0), 1, 0)\ndf_inputs_prepr['open_acc:1-3'] = np.where((df_inputs_prepr['open_acc'] >= 1) & (df_inputs_prepr['open_acc'] <= 3), 1, 0)\ndf_inputs_prepr['open_acc:4-12'] = np.where((df_inputs_prepr['open_acc'] >= 4) & (df_inputs_prepr['open_acc'] <= 12), 1, 0)\ndf_inputs_prepr['open_acc:13-17'] = np.where((df_inputs_prepr['open_acc'] >= 13) & (df_inputs_prepr['open_acc'] <= 17), 1, 0)\ndf_inputs_prepr['open_acc:18-22'] = np.where((df_inputs_prepr['open_acc'] >= 18) & (df_inputs_prepr['open_acc'] <= 22), 1, 0)\ndf_inputs_prepr['open_acc:23-25'] = np.where((df_inputs_prepr['open_acc'] >= 23) & (df_inputs_prepr['open_acc'] <= 25), 1, 0)\ndf_inputs_prepr['open_acc:26-30'] = np.where((df_inputs_prepr['open_acc'] >= 26) & (df_inputs_prepr['open_acc'] <= 30), 1, 0)\ndf_inputs_prepr['open_acc:>=31'] = np.where((df_inputs_prepr['open_acc'] >= 31), 1, 0)\n# pub_rec\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'pub_rec', df_targets_prepr)\n# We calculate weight of evidence.\ndf_temp","8b136ee7":"plot_by_woe(df_temp, 90)\n# We plot the weight of evidence values.","9e9db262":"# Categories '0-2', '3-4', '>=5'\ndf_inputs_prepr['pub_rec:0-2'] = np.where((df_inputs_prepr['pub_rec'] >= 0) & (df_inputs_prepr['pub_rec'] <= 2), 1, 0)\ndf_inputs_prepr['pub_rec:3-4'] = np.where((df_inputs_prepr['pub_rec'] >= 3) & (df_inputs_prepr['pub_rec'] <= 4), 1, 0)\ndf_inputs_prepr['pub_rec:>=5'] = np.where((df_inputs_prepr['pub_rec'] >= 5), 1, 0)\n# total_acc\ndf_inputs_prepr['total_acc_factor'] = pd.cut(df_inputs_prepr['total_acc'], 50)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'total_acc_factor', df_targets_prepr)\n# We calculate weight of evidence.\ndf_temp.head()","9ce6b6ff":"plot_by_woe(df_temp, 90)\n# We plot the weight of evidence values.","98d25347":"# Categories: '<=27', '28-51', '>51'\ndf_inputs_prepr['total_acc:<=27'] = np.where((df_inputs_prepr['total_acc'] <= 27), 1, 0)\ndf_inputs_prepr['total_acc:28-51'] = np.where((df_inputs_prepr['total_acc'] >= 28) & (df_inputs_prepr['total_acc'] <= 51), 1, 0)\ndf_inputs_prepr['total_acc:>=52'] = np.where((df_inputs_prepr['total_acc'] >= 52), 1, 0)\n# acc_now_delinq\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'acc_now_delinq', df_targets_prepr)\n# We calculate weight of evidence.\ndf_temp","1ccb9195":"plot_by_woe(df_temp)\n# We plot the weight of evidence values.","a2192516":"# Categories: '0', '>=1'\ndf_inputs_prepr['acc_now_delinq:0'] = np.where((df_inputs_prepr['acc_now_delinq'] == 0), 1, 0)\ndf_inputs_prepr['acc_now_delinq:>=1'] = np.where((df_inputs_prepr['acc_now_delinq'] >= 1), 1, 0)\n# total_rev_hi_lim\ndf_inputs_prepr['total_rev_hi_lim_factor'] = pd.cut(df_inputs_prepr['total_rev_hi_lim'], 2000)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 2000 categories by its values.\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'total_rev_hi_lim_factor', df_targets_prepr)\n# We calculate weight of evidence.\ndf_temp.head()","e99a7ae1":"plot_by_woe(df_temp.iloc[: 50, : ], 90)\n# We plot the weight of evidence values.","8e1f21c0":"# Categories\n# '<=5K', '5K-10K', '10K-20K', '20K-30K', '30K-40K', '40K-55K', '55K-95K', '>95K'\ndf_inputs_prepr['total_rev_hi_lim:<=5K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] <= 5000), 1, 0)\ndf_inputs_prepr['total_rev_hi_lim:5K-10K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 5000) & (df_inputs_prepr['total_rev_hi_lim'] <= 10000), 1, 0)\ndf_inputs_prepr['total_rev_hi_lim:10K-20K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 10000) & (df_inputs_prepr['total_rev_hi_lim'] <= 20000), 1, 0)\ndf_inputs_prepr['total_rev_hi_lim:20K-30K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 20000) & (df_inputs_prepr['total_rev_hi_lim'] <= 30000), 1, 0)\ndf_inputs_prepr['total_rev_hi_lim:30K-40K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 30000) & (df_inputs_prepr['total_rev_hi_lim'] <= 40000), 1, 0)\ndf_inputs_prepr['total_rev_hi_lim:40K-55K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 40000) & (df_inputs_prepr['total_rev_hi_lim'] <= 55000), 1, 0)\ndf_inputs_prepr['total_rev_hi_lim:55K-95K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 55000) & (df_inputs_prepr['total_rev_hi_lim'] <= 95000), 1, 0)\ndf_inputs_prepr['total_rev_hi_lim:>95K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 95000), 1, 0)\n# installment\ndf_inputs_prepr['installment_factor'] = pd.cut(df_inputs_prepr['installment'], 50)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'installment_factor', df_targets_prepr)\n# We calculate weight of evidence.\ndf_temp.head()","1b1b636a":"plot_by_woe(df_temp, 90)\n# We plot the weight of evidence values.","b7679055":"# annual_inc\ndf_inputs_prepr['annual_inc_factor'] = pd.cut(df_inputs_prepr['annual_inc'], 50)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'annual_inc_factor', df_targets_prepr)\n# We calculate weight of evidence.\ndf_temp.head()","29d08e82":"df_inputs_prepr['annual_inc_factor'] = pd.cut(df_inputs_prepr['annual_inc'], 100)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 100 categories by its values.\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'annual_inc_factor', df_targets_prepr)\n# We calculate weight of evidence.\ndf_temp.head()","93b21082":"# Initial examination shows that there are too few individuals with large income and too many with small income.\n# Hence, we are going to have one category for more than 150K, and we are going to apply our approach to determine\n# the categories of everyone with 140k or less.\ndf_inputs_prepr_temp = df_inputs_prepr.loc[df_inputs_prepr['annual_inc'] <= 140000, : ]\n#loan_data_temp = loan_data_temp.reset_index(drop = True)\n#df_inputs_prepr_temp","ce4c5ece":"df_inputs_prepr_temp[\"annual_inc_factor\"] = pd.cut(df_inputs_prepr_temp['annual_inc'], 50)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\ndf_temp = woe_ordered_continuous(df_inputs_prepr_temp, 'annual_inc_factor', df_targets_prepr[df_inputs_prepr_temp.index])\n# We calculate weight of evidence.\ndf_temp.head()","2d40f707":"plot_by_woe(df_temp, 90)\n# We plot the weight of evidence values.","daf1de96":"# WoE is monotonically decreasing with income, so we split income in 10 equal categories, each with width of 15k.\ndf_inputs_prepr['annual_inc:<20K'] = np.where((df_inputs_prepr['annual_inc'] <= 20000), 1, 0)\ndf_inputs_prepr['annual_inc:20K-30K'] = np.where((df_inputs_prepr['annual_inc'] > 20000) & (df_inputs_prepr['annual_inc'] <= 30000), 1, 0)\ndf_inputs_prepr['annual_inc:30K-40K'] = np.where((df_inputs_prepr['annual_inc'] > 30000) & (df_inputs_prepr['annual_inc'] <= 40000), 1, 0)\ndf_inputs_prepr['annual_inc:40K-50K'] = np.where((df_inputs_prepr['annual_inc'] > 40000) & (df_inputs_prepr['annual_inc'] <= 50000), 1, 0)\ndf_inputs_prepr['annual_inc:50K-60K'] = np.where((df_inputs_prepr['annual_inc'] > 50000) & (df_inputs_prepr['annual_inc'] <= 60000), 1, 0)\ndf_inputs_prepr['annual_inc:60K-70K'] = np.where((df_inputs_prepr['annual_inc'] > 60000) & (df_inputs_prepr['annual_inc'] <= 70000), 1, 0)\ndf_inputs_prepr['annual_inc:70K-80K'] = np.where((df_inputs_prepr['annual_inc'] > 70000) & (df_inputs_prepr['annual_inc'] <= 80000), 1, 0)\ndf_inputs_prepr['annual_inc:80K-90K'] = np.where((df_inputs_prepr['annual_inc'] > 80000) & (df_inputs_prepr['annual_inc'] <= 90000), 1, 0)\ndf_inputs_prepr['annual_inc:90K-100K'] = np.where((df_inputs_prepr['annual_inc'] > 90000) & (df_inputs_prepr['annual_inc'] <= 100000), 1, 0)\ndf_inputs_prepr['annual_inc:100K-120K'] = np.where((df_inputs_prepr['annual_inc'] > 100000) & (df_inputs_prepr['annual_inc'] <= 120000), 1, 0)\ndf_inputs_prepr['annual_inc:120K-140K'] = np.where((df_inputs_prepr['annual_inc'] > 120000) & (df_inputs_prepr['annual_inc'] <= 140000), 1, 0)\ndf_inputs_prepr['annual_inc:>140K'] = np.where((df_inputs_prepr['annual_inc'] > 140000), 1, 0)\n# mths_since_last_delinq\n# We have to create one category for missing values and do fine and coarse classing for the rest.\ndf_inputs_prepr_temp = df_inputs_prepr[pd.notnull(df_inputs_prepr['mths_since_last_delinq'])]\ndf_inputs_prepr_temp['mths_since_last_delinq_factor'] = pd.cut(df_inputs_prepr_temp['mths_since_last_delinq'], 50)\ndf_temp = woe_ordered_continuous(df_inputs_prepr_temp, 'mths_since_last_delinq_factor', df_targets_prepr[df_inputs_prepr_temp.index])\n# We calculate weight of evidence.\ndf_temp.head()","23854b3e":"plot_by_woe(df_temp, 90)\n# We plot the weight of evidence values.","a3650256":"# Categories: Missing, 0-3, 4-30, 31-56, >=57\ndf_inputs_prepr['mths_since_last_delinq:Missing'] = np.where((df_inputs_prepr['mths_since_last_delinq'].isnull()), 1, 0)\ndf_inputs_prepr['mths_since_last_delinq:0-3'] = np.where((df_inputs_prepr['mths_since_last_delinq'] >= 0) & (df_inputs_prepr['mths_since_last_delinq'] <= 3), 1, 0)\ndf_inputs_prepr['mths_since_last_delinq:4-30'] = np.where((df_inputs_prepr['mths_since_last_delinq'] >= 4) & (df_inputs_prepr['mths_since_last_delinq'] <= 30), 1, 0)\ndf_inputs_prepr['mths_since_last_delinq:31-56'] = np.where((df_inputs_prepr['mths_since_last_delinq'] >= 31) & (df_inputs_prepr['mths_since_last_delinq'] <= 56), 1, 0)\ndf_inputs_prepr['mths_since_last_delinq:>=57'] = np.where((df_inputs_prepr['mths_since_last_delinq'] >= 57), 1, 0)","26609995":"# dti\ndf_inputs_prepr['dti_factor'] = pd.cut(df_inputs_prepr['dti'], 100)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 100 categories by its values.\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'dti_factor', df_targets_prepr)\n# We calculate weight of evidence.\ndf_temp.head()","53c7c054":"plot_by_woe(df_temp, 90)\n# We plot the weight of evidence values.","70e695ac":"# Similarly to income, initial examination shows that most values are lower than 200.\n# Hence, we are going to have one category for more than 35, and we are going to apply our approach to determine\n# the categories of everyone with 150k or less.\ndf_inputs_prepr_temp = df_inputs_prepr.loc[df_inputs_prepr['dti'] <= 35, : ]\ndf_inputs_prepr_temp['dti_factor'] = pd.cut(df_inputs_prepr_temp['dti'], 50)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\ndf_temp = woe_ordered_continuous(df_inputs_prepr_temp, 'dti_factor', df_targets_prepr[df_inputs_prepr_temp.index])\n# We calculate weight of evidence.\ndf_temp","7c2613e7":"plot_by_woe(df_temp, 90)\n# We plot the weight of evidence values.","7f4dce9a":"# Categories:\ndf_inputs_prepr['dti:<=1.4'] = np.where((df_inputs_prepr['dti'] <= 1.4), 1, 0)\ndf_inputs_prepr['dti:1.4-3.5'] = np.where((df_inputs_prepr['dti'] > 1.4) & (df_inputs_prepr['dti'] <= 3.5), 1, 0)\ndf_inputs_prepr['dti:3.5-7.7'] = np.where((df_inputs_prepr['dti'] > 3.5) & (df_inputs_prepr['dti'] <= 7.7), 1, 0)\ndf_inputs_prepr['dti:7.7-10.5'] = np.where((df_inputs_prepr['dti'] > 7.7) & (df_inputs_prepr['dti'] <= 10.5), 1, 0)\ndf_inputs_prepr['dti:10.5-16.1'] = np.where((df_inputs_prepr['dti'] > 10.5) & (df_inputs_prepr['dti'] <= 16.1), 1, 0)\ndf_inputs_prepr['dti:16.1-20.3'] = np.where((df_inputs_prepr['dti'] > 16.1) & (df_inputs_prepr['dti'] <= 20.3), 1, 0)\ndf_inputs_prepr['dti:20.3-21.7'] = np.where((df_inputs_prepr['dti'] > 20.3) & (df_inputs_prepr['dti'] <= 21.7), 1, 0)\ndf_inputs_prepr['dti:21.7-22.4'] = np.where((df_inputs_prepr['dti'] > 21.7) & (df_inputs_prepr['dti'] <= 22.4), 1, 0)\ndf_inputs_prepr['dti:22.4-35'] = np.where((df_inputs_prepr['dti'] > 22.4) & (df_inputs_prepr['dti'] <= 35), 1, 0)\ndf_inputs_prepr['dti:>35'] = np.where((df_inputs_prepr['dti'] > 35), 1, 0)\n# mths_since_last_record\n# We have to create one category for missing values and do fine and coarse classing for the rest.\ndf_inputs_prepr_temp = df_inputs_prepr[pd.notnull(df_inputs_prepr['mths_since_last_record'])]\n#sum(loan_data_temp['mths_since_last_record'].isnull())\ndf_inputs_prepr_temp['mths_since_last_record_factor'] = pd.cut(df_inputs_prepr_temp['mths_since_last_record'], 50)\n# Here we do fine-classing: using the 'cut' method, we split the variable into 50 categories by its values.\ndf_temp = woe_ordered_continuous(df_inputs_prepr_temp, 'mths_since_last_record_factor', df_targets_prepr[df_inputs_prepr_temp.index])\n# We calculate weight of evidence.\ndf_temp.head()","027f4c3a":"plot_by_woe(df_temp, 90)\n# We plot the weight of evidence values.","a044accd":"# Categories: 'Missing', '0-2', '3-20', '21-31', '32-80', '81-86', '>86'\ndf_inputs_prepr['mths_since_last_record:Missing'] = np.where((df_inputs_prepr['mths_since_last_record'].isnull()), 1, 0)\ndf_inputs_prepr['mths_since_last_record:0-2'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 0) & (df_inputs_prepr['mths_since_last_record'] <= 2), 1, 0)\ndf_inputs_prepr['mths_since_last_record:3-20'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 3) & (df_inputs_prepr['mths_since_last_record'] <= 20), 1, 0)\ndf_inputs_prepr['mths_since_last_record:21-31'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 21) & (df_inputs_prepr['mths_since_last_record'] <= 31), 1, 0)\ndf_inputs_prepr['mths_since_last_record:32-80'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 32) & (df_inputs_prepr['mths_since_last_record'] <= 80), 1, 0)\ndf_inputs_prepr['mths_since_last_record:81-86'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 81) & (df_inputs_prepr['mths_since_last_record'] <= 86), 1, 0)\ndf_inputs_prepr['mths_since_last_record:>86'] = np.where((df_inputs_prepr['mths_since_last_record'] > 86), 1, 0)","c0be44de":"#####\n#loan_data_inputs_train = df_inputs_prepr\n#####\nloan_data_inputs_test = df_inputs_prepr","c4678783":"loan_data_inputs_train.to_csv('loan_data_inputs_train.csv')\nloan_data_targets_train.to_csv('loan_data_targets_train.csv')\nloan_data_inputs_test.to_csv('loan_data_inputs_test.csv')\nloan_data_targets_test.to_csv('loan_data_targets_test.csv')","617c2f5c":"# Import Train and Test Data.\nloan_data_inputs_train = pd.read_csv('loan_data_inputs_train.csv', index_col = 0)\nloan_data_targets_train = pd.read_csv('loan_data_targets_train.csv', index_col = 0, header = None)\nloan_data_inputs_test = pd.read_csv('loan_data_inputs_test.csv', index_col = 0)\nloan_data_targets_test = pd.read_csv('loan_data_targets_test.csv', index_col = 0, header = None)","a4a54fdf":"# Here we import the new data.\nloan_data_backup = pd.read_csv('..\/input\/loan-2015\/loan_data_2015.csv')\nloan_data = loan_data_backup.copy()","9f9cff5c":"pd.options.display.max_columns = None\n#pd.options.display.max_rows = None\n# Sets the pandas dataframe options to display all columns\/ rows.","1b1c5be4":"loan_data.head()","1a7e9a7f":"loan_data.info()","fcd9c40c":"loan_data['emp_length'].unique()","1f372567":"loan_data['emp_length_int'] = loan_data['emp_length'].str.replace('\\+ years', '')\nloan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace('< 1 year', str(0))\nloan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace('n\/a',  str(0))\nloan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace(' years', '')\nloan_data['emp_length_int'] = loan_data['emp_length_int'].str.replace(' year', '')","6845e181":"type(loan_data['emp_length_int'][0])","a87f61fa":"loan_data['emp_length_int'] = pd.to_numeric(loan_data['emp_length_int'])\ntype(loan_data['emp_length_int'][0])","93e35fd2":"loan_data['earliest_cr_line']","bfeb1832":"loan_data['earliest_cr_line_date'] = pd.to_datetime(loan_data['earliest_cr_line'], format = '%b-%y')\ntype(loan_data['earliest_cr_line_date'][0])","89b8114d":"# Assume we are now in December 2017\nloan_data['mths_since_earliest_cr_line'] = round(pd.to_numeric((pd.to_datetime('2018-12-01') - loan_data['earliest_cr_line_date']) \/ np.timedelta64(1, 'M')))\nloan_data['mths_since_earliest_cr_line'].describe()\n# Dates from 1969 and before are not being converted well, i.e., they have become 2069 and similar, and negative differences are being calculated.","db980101":"# There are 2303 such values.\nloan_data.loc[: , ['earliest_cr_line', 'earliest_cr_line_date', 'mths_since_earliest_cr_line']][loan_data['mths_since_earliest_cr_line'] < 0]","7ef5c777":"# We set all these negative differences to the maximum.\nloan_data['mths_since_earliest_cr_line'][loan_data['mths_since_earliest_cr_line'] < 0] = loan_data['mths_since_earliest_cr_line'].max()","427f5851":"min(loan_data['mths_since_earliest_cr_line'])","dee64d02":"loan_data['term']","a4edb96e":"loan_data['term'].describe()","c982cdc3":"loan_data['term_int'] = loan_data['term'].str.replace(' months', '')\nloan_data['term_int']","60d66c17":"type(loan_data['term_int'][25])","82b530ca":"loan_data['term_int'] = pd.to_numeric(loan_data['term'].str.replace(' months', ''))\nloan_data['term_int']","f20eb92a":"type(loan_data['term_int'][0])","cd474658":"loan_data['issue_d']","ccf5bb21":"# Assume we are now in December 2017\nloan_data['issue_d_date'] = pd.to_datetime(loan_data['issue_d'], format = '%b-%y')\nloan_data['mths_since_issue_d'] = round(pd.to_numeric((pd.to_datetime('2018-12-01') - loan_data['issue_d_date']) \/ np.timedelta64(1, 'M')))\nloan_data['mths_since_issue_d'].describe()","082e639c":"loan_data.info()\n# loan_data['grade_factor'] = loan_data['grade'].astype('category')","bd15ab62":"#grade\n#sub_grade\n#home_ownership\n#verification_status\n#loan_status\n#purpose\n#addr_state\n#initial_list_status","cd118a5a":"loan_data_dummies = [pd.get_dummies(loan_data['grade'], prefix = 'grade', prefix_sep = ':'),\n                     pd.get_dummies(loan_data['sub_grade'], prefix = 'sub_grade', prefix_sep = ':'),\n                     pd.get_dummies(loan_data['home_ownership'], prefix = 'home_ownership', prefix_sep = ':'),\n                     pd.get_dummies(loan_data['verification_status'], prefix = 'verification_status', prefix_sep = ':'),\n                     pd.get_dummies(loan_data['loan_status'], prefix = 'loan_status', prefix_sep = ':'),\n                     pd.get_dummies(loan_data['purpose'], prefix = 'purpose', prefix_sep = ':'),\n                     pd.get_dummies(loan_data['addr_state'], prefix = 'addr_state', prefix_sep = ':'),\n                     pd.get_dummies(loan_data['initial_list_status'], prefix = 'initial_list_status', prefix_sep = ':')]\nloan_data_dummies = pd.concat(loan_data_dummies, axis = 1)","26ace150":"type(loan_data_dummies)","d33698ed":"loan_data_dummies.shape","7bfea42a":"loan_data_dummies.info()","a5c5b537":"loan_data = pd.concat([loan_data, loan_data_dummies], axis = 1)","c39fe439":"loan_data.columns.values","85dc69d9":"loan_data.isnull()","65575825":"pd.options.display.max_rows = None\nloan_data.isnull().sum()","3562f56a":"pd.options.display.max_rows = 100","f1e56d16":"# loan_data$total_rev_hi_lim - There are 70276 missing values here.\n# 'Total revolving high credit\/credit limit', so it makes sense that the missing values are equal to funded_amnt.\n\n# loan_data$acc_now_delinq\n# loan_data$total_acc\n# loan_data$pub_rec\n# loan_data$open_acc\n# loan_data$inq_last_6mths\n# loan_data$delinq_2yrs\n# loan_data$mths_since_earliest_cr_line\n# - There are 29 missing values in all of these columns. They are likely the same observations.\n# An eyeballing examination of the dataset confirms that.\n# All of these are with loan_status 'Does not meet the credit policy. Status:Fully Paid'.\n# We impute these values.\n\n# loan_data$annual_inc\n# - There are 4 missing values in all of these columns.\n\n# loan_data$mths_since_last_record\n# loan_data$mths_since_last_delinq","44a0039f":"# 'Total revolving high credit\/credit limit', so it makes sense that the missing values are equal to funded_amnt.\nloan_data['total_rev_hi_lim'].fillna(loan_data['funded_amnt'], inplace=True)","c970f02e":"loan_data['mths_since_earliest_cr_line'].fillna(0, inplace=True)","ea088ac0":"loan_data['acc_now_delinq'].fillna(0, inplace=True)\nloan_data['total_acc'].fillna(0, inplace=True)\nloan_data['pub_rec'].fillna(0, inplace=True)\nloan_data['open_acc'].fillna(0, inplace=True)\nloan_data['inq_last_6mths'].fillna(0, inplace=True)\nloan_data['delinq_2yrs'].fillna(0, inplace=True)\nloan_data['emp_length_int'].fillna(0, inplace=True)","0d82b57f":"loan_data['annual_inc'].fillna(loan_data['annual_inc'].mean(), inplace=True)","6bf8ca30":"loan_data['loan_status'].unique()","0e822dd5":"loan_data['loan_status'].value_counts()","d8f5004e":"loan_data['loan_status'].value_counts() \/ loan_data['loan_status'].count()","3ca7ec35":"# Good\/ Bad Definition\nloan_data['good_bad'] = np.where(loan_data['loan_status'].isin(['Charged Off', 'Default',\n                                                       'Does not meet the credit policy. Status:Charged Off',\n                                                       'Late (31-120 days)']), 0, 1)","4cf3b3f9":"#loan_data['good_bad'].sum()\/loan_data['loan_status'].count()","1af50dc9":"loan_data['good_bad']","39a0450b":"# loan_data_inputs_train, loan_data_inputs_test, loan_data_targets_train, loan_data_targets_test\n# Here we don't split data into training and test\n#train_test_split(loan_data.drop('good_bad', axis = 1), loan_data['good_bad'])\n#loan_data_inputs_train, loan_data_inputs_test, loan_data_targets_train, loan_data_targets_test = train_test_split(loan_data.drop('good_bad', axis = 1), loan_data['good_bad'])\n#loan_data_inputs_train.shape\n#loan_data_targets_train.shape\n#loan_data_inputs_test.shape\n#loan_data_targets_test.shape\n#loan_data_inputs_train, loan_data_inputs_test, loan_data_targets_train, loan_data_targets_test = train_test_split(loan_data.drop('good_bad', axis = 1), loan_data['good_bad'], test_size = 0.2, random_state = 42)\n#loan_data_inputs_train.shape\n#loan_data_targets_train.shape\n#loan_data_inputs_test.shape\n#loan_data_targets_test.shape","3c8bc7a5":"loan_data.drop('good_bad', axis = 1)\nloan_data['good_bad']","cfe139ef":"#####\ndf_inputs_prepr = loan_data.drop('good_bad', axis = 1)\ndf_targets_prepr = loan_data['good_bad']\n#####\n#df_inputs_prepr = loan_data_inputs_test\n##df_targets_prepr = loan_data_targets_test\ndf_inputs_prepr['grade'].unique()","6ef8bff4":" df1 = pd.concat([df_inputs_prepr['grade'], df_targets_prepr], axis = 1)\ndf1.head()","b766f76b":"df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].count()","f1cb0461":"df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].mean()","922713a2":"df1 = pd.concat([df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].count(),\n                df1.groupby(df1.columns.values[0], as_index = False)[df1.columns.values[1]].mean()], axis = 1)\ndf1","aecb5481":"df1 = df1.iloc[:, [0, 1, 3]]\ndf1","1edee6e1":"df1.columns = [df1.columns.values[0], 'n_obs', 'prop_good']\ndf1","895b0ff6":"df1['prop_n_obs'] = df1['n_obs'] \/ df1['n_obs'].sum()\ndf1","2121ccf3":"df1['n_good'] = df1['prop_good'] * df1['n_obs']\ndf1['n_bad'] = (1 - df1['prop_good']) * df1['n_obs']\ndf1","779955d5":"df1['prop_n_good'] = df1['n_good'] \/ df1['n_good'].sum()\ndf1['prop_n_bad'] = df1['n_bad'] \/ df1['n_bad'].sum()\ndf1","a3585b90":"df1['WoE'] = np.log(df1['prop_n_good'] \/ df1['prop_n_bad'])\ndf1","f4a1d701":"df1 = df1.sort_values(['WoE'])\ndf1 = df1.reset_index(drop = True)\ndf1","c2823de7":"df1['IV'] = (df1['prop_n_good'] - df1['prop_n_bad']) * df1['WoE']\ndf1['IV'] = df1['IV'].sum()\ndf1","fb32639b":"# WoE function for discrete unordered variables\ndef woe_discrete(df, discrete_variabe_name, good_bad_variable_df):\n    df = pd.concat([df[discrete_variabe_name], good_bad_variable_df], axis = 1)\n    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),\n                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)\n    df = df.iloc[:, [0, 1, 3]]\n    df.columns = [df.columns.values[0], 'n_obs', 'prop_good']\n    df['prop_n_obs'] = df['n_obs'] \/ df['n_obs'].sum()\n    df['n_good'] = df['prop_good'] * df['n_obs']\n    df['n_bad'] = (1 - df['prop_good']) * df['n_obs']\n    df['prop_n_good'] = df['n_good'] \/ df['n_good'].sum()\n    df['prop_n_bad'] = df['n_bad'] \/ df['n_bad'].sum()\n    df['WoE'] = np.log(df['prop_n_good'] \/ df['prop_n_bad'])\n    df = df.sort_values(['WoE'])\n    df = df.reset_index(drop = True)\n    df['diff_prop_good'] = df['prop_good'].diff().abs()\n    df['diff_WoE'] = df['WoE'].diff().abs()\n    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']\n    df['IV'] = df['IV'].sum()\n    return df","1f6dd9e3":"# 'grade', 'home_ownership', 'verification_status',\n# 'purpose', 'addr_state', 'initial_list_status'","a42db50e":"# 'grade'\ndf_temp = woe_discrete(df_inputs_prepr, 'grade', df_targets_prepr)\ndf_temp","89a39d38":"def plot_by_woe(df_WoE, rotation_of_x_axis_labels = 0):\n    #x = df_WoE.iloc[:, 0]\n    x = np.array(df_WoE.iloc[:, 0].apply(str))\n    y = df_WoE['WoE']\n    plt.figure(figsize=(18, 6))\n    plt.plot(x, y, marker = 'o', linestyle = '--', color = 'k')\n    plt.xlabel(df_WoE.columns[0])\n    plt.ylabel('Weight of Evidence')\n    plt.title(str('Weight of Evidence by ' + df_WoE.columns[0]))\n    plt.xticks(rotation = rotation_of_x_axis_labels)","7eee93cd":"plot_by_woe(df_temp)\n# Leave as is.\n# 'G' will be the reference category.","892a00bb":"# 'home_ownership'\ndf_temp = woe_discrete(df_inputs_prepr, 'home_ownership', df_targets_prepr)\ndf_temp","fb5d8a22":"plot_by_woe(df_temp)","e7929c4b":"# There are many categories with very few observations and many categories with very different \"good\" %.\n# Therefore, we create a new discrete variable where we combine some of the categories.\n# 'OTHERS' and 'NONE' are riskiest but are very few. 'RENT' is the next riskiest.\n# 'ANY' are least risky but are too few. Conceptually, they belong to the same category. Also, their inclusion would not change anything.\n# We combine them in one category, 'RENT_OTHER_NONE_ANY'.\n# We end up with 3 categories: 'RENT_OTHER_NONE_ANY', 'OWN', 'MORTGAGE'.\n#df_inputs_prepr['home_ownership:RENT_OTHER_NONE_ANY'] = sum([df_inputs_prepr['home_ownership:RENT'], df_inputs_prepr['home_ownership:OTHER', df_inputs_prepr['home_ownership:NONE'],df_inputs_prepr['home_ownership:ANY']])\n# 'RENT_OTHER_NONE_ANY' will be the reference category.\n\n# Alternatively:\n#loan_data.loc['home_ownership' in ['RENT', 'OTHER', 'NONE', 'ANY'], 'home_ownership:RENT_OTHER_NONE_ANY'] = 1\n#loan_data.loc['home_ownership' not in ['RENT', 'OTHER', 'NONE', 'ANY'], 'home_ownership:RENT_OTHER_NONE_ANY'] = 0\n#loan_data.loc['loan_status' not in ['OWN'], 'home_ownership:OWN'] = 1\n#loan_data.loc['loan_status' not in ['OWN'], 'home_ownership:OWN'] = 0\n#loan_data.loc['loan_status' not in ['MORTGAGE'], 'home_ownership:MORTGAGE'] = 1\n#loan_data.loc['loan_status' not in ['MORTGAGE'], 'home_ownership:MORTGAGE'] = 0","a8081343":"loan_data['home_ownership'].unique()","2ea57325":"df_inputs_prepr['home_ownership:RENT_OTHER_NONE_ANY'] = sum([df_inputs_prepr['home_ownership:RENT'], df_inputs_prepr['home_ownership:ANY']])\n# 'addr_state'\ndf_inputs_prepr['addr_state'].unique()","d8b0ca23":"#df_inputs_prepr['addr_state:ND'] = 0","06294ebb":"if ['addr_state:ND'] in df_inputs_prepr.columns.values:\n    pass\nelse:\n    df_inputs_prepr['addr_state:ND'] = 0","9d2c19ec":"if ['addr_state:ID'] in df_inputs_prepr.columns.values:\n    pass\nelse:\n    df_inputs_prepr['addr_state:ID'] = 0","bed17c27":"if ['addr_state:IA'] in df_inputs_prepr.columns.values:\n    pass\nelse:\n    df_inputs_prepr['addr_state:IA'] = 0","5f2f14b7":"df_temp = woe_discrete(df_inputs_prepr, 'addr_state', df_targets_prepr)\ndf_temp.head()","35a390ec":"plot_by_woe(df_temp,90)","327f79e0":"plot_by_woe(df_temp.iloc[2: -2, : ])","aefda817":"plot_by_woe(df_temp.iloc[6: -6, : ])","01018365":"df_inputs_prepr.columns.values","0607c3e3":"# We create the following categories:\n# 'ND' 'NE' 'IA' NV' 'FL' 'HI' 'AL'\n# 'NM' 'VA'\n# 'NY'\n# 'OK' 'TN' 'MO' 'LA' 'MD' 'NC'\n# 'CA'\n# 'UT' 'KY' 'AZ' 'NJ'\n# 'AR' 'MI' 'PA' 'OH' 'MN'\n# 'RI' 'MA' 'DE' 'SD' 'IN'\n# 'GA' 'WA' 'OR'\n# 'WI' 'MT'\n# 'TX'\n# 'IL' 'CT'\n# 'KS' 'SC' 'CO' 'VT' 'AK' 'MS'\n# 'WV' 'NH' 'WY' 'DC' 'ME' 'ID'\n\n# 'IA_NV_HI_ID_AL_FL' will be the reference category.\n\ndf_inputs_prepr['addr_state:ND_NE_IA_NV_FL_HI_AL'] = sum([df_inputs_prepr['addr_state:ND'], df_inputs_prepr['addr_state:NE'],\n                                              df_inputs_prepr['addr_state:IA'], df_inputs_prepr['addr_state:NV'],\n                                              df_inputs_prepr['addr_state:FL'], df_inputs_prepr['addr_state:HI'],\n                                                          df_inputs_prepr['addr_state:AL']])\n\ndf_inputs_prepr['addr_state:NM_VA'] = sum([df_inputs_prepr['addr_state:NM'], df_inputs_prepr['addr_state:VA']])\n\ndf_inputs_prepr['addr_state:OK_TN_MO_LA_MD_NC'] = sum([df_inputs_prepr['addr_state:OK'], df_inputs_prepr['addr_state:TN'],\n                                              df_inputs_prepr['addr_state:MO'], df_inputs_prepr['addr_state:LA'],\n                                              df_inputs_prepr['addr_state:MD'], df_inputs_prepr['addr_state:NC']])\n\ndf_inputs_prepr['addr_state:UT_KY_AZ_NJ'] = sum([df_inputs_prepr['addr_state:UT'], df_inputs_prepr['addr_state:KY'],\n                                              df_inputs_prepr['addr_state:AZ'], df_inputs_prepr['addr_state:NJ']])\n\ndf_inputs_prepr['addr_state:AR_MI_PA_OH_MN'] = sum([df_inputs_prepr['addr_state:AR'], df_inputs_prepr['addr_state:MI'],\n                                              df_inputs_prepr['addr_state:PA'], df_inputs_prepr['addr_state:OH'],\n                                              df_inputs_prepr['addr_state:MN']])\n\ndf_inputs_prepr['addr_state:RI_MA_DE_SD_IN'] = sum([df_inputs_prepr['addr_state:RI'], df_inputs_prepr['addr_state:MA'],\n                                              df_inputs_prepr['addr_state:DE'], df_inputs_prepr['addr_state:SD'],\n                                              df_inputs_prepr['addr_state:IN']])\n\ndf_inputs_prepr['addr_state:GA_WA_OR'] = sum([df_inputs_prepr['addr_state:GA'], df_inputs_prepr['addr_state:WA'],\n                                              df_inputs_prepr['addr_state:OR']])\n\ndf_inputs_prepr['addr_state:WI_MT'] = sum([df_inputs_prepr['addr_state:WI'], df_inputs_prepr['addr_state:MT']])\n\ndf_inputs_prepr['addr_state:IL_CT'] = sum([df_inputs_prepr['addr_state:IL'], df_inputs_prepr['addr_state:CT']])\n\ndf_inputs_prepr['addr_state:KS_SC_CO_VT_AK_MS'] = sum([df_inputs_prepr['addr_state:KS'], df_inputs_prepr['addr_state:SC'],\n                                              df_inputs_prepr['addr_state:CO'], df_inputs_prepr['addr_state:VT'],\n                                              df_inputs_prepr['addr_state:AK'], df_inputs_prepr['addr_state:MS']])\n\ndf_inputs_prepr['addr_state:WV_NH_WY_DC_ME_ID'] = sum([df_inputs_prepr['addr_state:WV'], df_inputs_prepr['addr_state:NH'],\n                                              df_inputs_prepr['addr_state:WY'], df_inputs_prepr['addr_state:DC'],\n                                              df_inputs_prepr['addr_state:ME'], df_inputs_prepr['addr_state:ID']])","406d2777":"# 'verification_status'\ndf_temp = woe_discrete(df_inputs_prepr, 'verification_status', df_targets_prepr)\ndf_temp","c39748f2":"plot_by_woe(df_temp)","02fd7016":"# Leave as is.\n# 'Verified' will be the reference category.","e12bf177":"# 'purpose'\ndf_temp = woe_discrete(df_inputs_prepr, 'purpose', df_targets_prepr)\ndf_temp","b38e5144":"#plt.figure(figsize=(15, 5))\n#sns.pointplot(x = 'purpose', y = 'WoE', data = df_temp, figsize = (5, 15))\nplot_by_woe(df_temp, 90)","12c42313":"# We combine 'educational', 'small_business', 'wedding', 'renewable_energy', 'moving', 'house' in one category: 'educ__sm_b__wedd__ren_en__mov__house'.\n# We combine 'other', 'medical', 'vacation' in one category: 'oth__med__vacation'.\n# We combine 'major_purchase', 'car', 'home_improvement' in one category: 'major_purch__car__home_impr'.\n# We leave 'debt_consolidtion' in a separate category.\n# We leave 'credit_card' in a separate category.\n# 'educ__sm_b__wedd__ren_en__mov__house' will be the reference category.\ndf_inputs_prepr['purpose:educ__sm_b__wedd__ren_en__mov__house'] = sum([df_inputs_prepr['purpose:educational'], df_inputs_prepr['purpose:small_business'],\n                                                                 df_inputs_prepr['purpose:wedding'], df_inputs_prepr['purpose:renewable_energy'],\n                                                                 df_inputs_prepr['purpose:moving'], df_inputs_prepr['purpose:house']])\ndf_inputs_prepr['purpose:oth__med__vacation'] = sum([df_inputs_prepr['purpose:other'], df_inputs_prepr['purpose:medical'],\n                                             df_inputs_prepr['purpose:vacation']])\ndf_inputs_prepr['purpose:major_purch__car__home_impr'] = sum([df_inputs_prepr['purpose:major_purchase'], df_inputs_prepr['purpose:car'],\n                                                        df_inputs_prepr['purpose:home_improvement']])\n# 'initial_list_status'\ndf_temp = woe_discrete(df_inputs_prepr, 'initial_list_status', df_targets_prepr)\ndf_temp","40d2f18f":"plot_by_woe(df_temp)","8bd8391f":"# Leave as is.\n# 'f' will be the reference category.","5c63bff8":"# WoE function for ordered discrete and continuous variables\ndef woe_ordered_continuous(df, discrete_variabe_name, good_bad_variable_df):\n    df = pd.concat([df[discrete_variabe_name], good_bad_variable_df], axis = 1)\n    df = pd.concat([df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].count(),\n                    df.groupby(df.columns.values[0], as_index = False)[df.columns.values[1]].mean()], axis = 1)\n    df = df.iloc[:, [0, 1, 3]]\n    df.columns = [df.columns.values[0], 'n_obs', 'prop_good']\n    df['prop_n_obs'] = df['n_obs'] \/ df['n_obs'].sum()\n    df['n_good'] = df['prop_good'] * df['n_obs']\n    df['n_bad'] = (1 - df['prop_good']) * df['n_obs']\n    df['prop_n_good'] = df['n_good'] \/ df['n_good'].sum()\n    df['prop_n_bad'] = df['n_bad'] \/ df['n_bad'].sum()\n    df['WoE'] = np.log(df['prop_n_good'] \/ df['prop_n_bad'])\n    #df = df.sort_values(['WoE'])\n    #df = df.reset_index(drop = True)\n    df['diff_prop_good'] = df['prop_good'].diff().abs()\n    df['diff_WoE'] = df['WoE'].diff().abs()\n    df['IV'] = (df['prop_n_good'] - df['prop_n_bad']) * df['WoE']\n    df['IV'] = df['IV'].sum()\n    return df","b5d8cc96":"# term\ndf_inputs_prepr['term_int'].unique()\n# There are only two unique values, 36 and 60.","fb5e5a7d":"df_temp = woe_ordered_continuous(df_inputs_prepr, 'term_int', df_targets_prepr)\ndf_temp","4744eaf7":"plot_by_woe(df_temp)","2e3f91a0":"# Leave as is.\n# '60' will be the reference category.\ndf_inputs_prepr['term:36'] = np.where((df_inputs_prepr['term_int'] == 36), 1, 0)\ndf_inputs_prepr['term:60'] = np.where((df_inputs_prepr['term_int'] == 60), 1, 0)\n# emp_length_int\ndf_inputs_prepr['emp_length_int'].unique()\n# Has only 11 levels: from 0 to 10. Hence, we turn it into a factor with 11 levels.","be383dc4":"df_temp = woe_ordered_continuous(df_inputs_prepr, 'emp_length_int', df_targets_prepr)\ndf_temp","11823351":"plot_by_woe(df_temp)","bd557a98":"# We create the following categories: '0', '1', '2 - 4', '5 - 6', '7 - 9', '10'\n# '0' will be the reference category\ndf_inputs_prepr['emp_length:0'] = np.where(df_inputs_prepr['emp_length_int'].isin([0]), 1, 0)\ndf_inputs_prepr['emp_length:1'] = np.where(df_inputs_prepr['emp_length_int'].isin([1]), 1, 0)\ndf_inputs_prepr['emp_length:2-4'] = np.where(df_inputs_prepr['emp_length_int'].isin(range(2, 5)), 1, 0)\ndf_inputs_prepr['emp_length:5-6'] = np.where(df_inputs_prepr['emp_length_int'].isin(range(5, 7)), 1, 0)\ndf_inputs_prepr['emp_length:7-9'] = np.where(df_inputs_prepr['emp_length_int'].isin(range(7, 10)), 1, 0)\ndf_inputs_prepr['emp_length:10'] = np.where(df_inputs_prepr['emp_length_int'].isin([10]), 1, 0)\ndf_inputs_prepr['mths_since_issue_d'].unique()","ea9ed2be":"df_inputs_prepr['mths_since_issue_d_factor'] = pd.cut(df_inputs_prepr['mths_since_issue_d'], 50)","8ca387e0":"df_inputs_prepr['mths_since_issue_d_factor']","932cf9e8":"# mths_since_issue_d\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'mths_since_issue_d_factor', df_targets_prepr)\ndf_temp.head()","59dd31e6":"# !!!!!!!!!\n#df_temp['mths_since_issue_d_factor'] = np.array(df_temp.mths_since_issue_d_factor.apply(str))\n#df_temp['mths_since_issue_d_factor'] = list(df_temp.mths_since_issue_d_factor.apply(str))\n#df_temp['mths_since_issue_d_factor'] = tuple(df_temp.mths_since_issue_d_factor.apply(str))\nplot_by_woe(df_temp,90)","ccb8b757":"plot_by_woe(df_temp.iloc[3: , : ], 90)","ed85152e":"# We create the following categories:\n# < 38, 38 - 39, 40 - 41, 42 - 48, 49 - 52, 53 - 64, 65 - 84, > 84.\ndf_inputs_prepr['mths_since_issue_d:<38'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(38)), 1, 0)\ndf_inputs_prepr['mths_since_issue_d:38-39'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(38, 40)), 1, 0)\ndf_inputs_prepr['mths_since_issue_d:40-41'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(40, 42)), 1, 0)\ndf_inputs_prepr['mths_since_issue_d:42-48'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(42, 49)), 1, 0)\ndf_inputs_prepr['mths_since_issue_d:49-52'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(49, 53)), 1, 0)\ndf_inputs_prepr['mths_since_issue_d:53-64'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(53, 65)), 1, 0)\ndf_inputs_prepr['mths_since_issue_d:65-84'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(65, 85)), 1, 0)\ndf_inputs_prepr['mths_since_issue_d:>84'] = np.where(df_inputs_prepr['mths_since_issue_d'].isin(range(85, int(df_inputs_prepr['mths_since_issue_d'].max()))), 1, 0)\n# int_rate\ndf_inputs_prepr['int_rate_factor'] = pd.cut(df_inputs_prepr['int_rate'], 50)\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'int_rate_factor', df_targets_prepr)\ndf_temp","c5f2f384":"plot_by_woe(df_temp, 90)","c8f33ccb":"# '< 9.548', '9.548 - 12.025', '12.025 - 15.74', '15.74 - 20.281', '> 20.281'","3f1f38d7":"#loan_data.loc[loan_data['int_rate'] < 5.8, 'int_rate:<5.8'] = 1\n#(loan_data['int_rate'] > 5.8) & (loan_data['int_rate'] <= 8.64)\n#loan_data['int_rate:<5.8'] = np.where(loan_data['int_rate'] < 5.8, 1, 0)\n#loan_data[(loan_data['int_rate'] > 5.8) & (loan_data['int_rate'] <= 8.64)]\n#loan_data['int_rate'][(np.where((loan_data['int_rate'] > 5.8) & (loan_data['int_rate'] <= 8.64)))]\n#loan_data.loc[(loan_data['int_rate'] > 5.8) & (loan_data['int_rate'] <= 8.64), 'int_rate:<5.8'] = 1\ndf_inputs_prepr['int_rate:<9.548'] = np.where((df_inputs_prepr['int_rate'] <= 9.548), 1, 0)\ndf_inputs_prepr['int_rate:9.548-12.025'] = np.where((df_inputs_prepr['int_rate'] > 9.548) & (df_inputs_prepr['int_rate'] <= 12.025), 1, 0)\ndf_inputs_prepr['int_rate:12.025-15.74'] = np.where((df_inputs_prepr['int_rate'] > 12.025) & (df_inputs_prepr['int_rate'] <= 15.74), 1, 0)\ndf_inputs_prepr['int_rate:15.74-20.281'] = np.where((df_inputs_prepr['int_rate'] > 15.74) & (df_inputs_prepr['int_rate'] <= 20.281), 1, 0)\ndf_inputs_prepr['int_rate:>20.281'] = np.where((df_inputs_prepr['int_rate'] > 20.281), 1, 0)","11e49253":"# mths_since_earliest_cr_line\ndf_inputs_prepr['mths_since_earliest_cr_line_factor'] = pd.cut(df_inputs_prepr['mths_since_earliest_cr_line'], 50)\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'mths_since_earliest_cr_line_factor', df_targets_prepr)\ndf_temp.head()","98364744":"plot_by_woe(df_temp, 90)","f60cb13b":"plot_by_woe(df_temp.iloc[6: , : ], 90)","7efcc69d":"# We create the following categories:\n# < 140, # 141 - 164, # 165 - 247, # 248 - 270, # 271 - 352, # > 352\ndf_inputs_prepr['mths_since_earliest_cr_line:<140'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(140)), 1, 0)\ndf_inputs_prepr['mths_since_earliest_cr_line:141-164'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(140, 165)), 1, 0)\ndf_inputs_prepr['mths_since_earliest_cr_line:165-247'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(165, 248)), 1, 0)\ndf_inputs_prepr['mths_since_earliest_cr_line:248-270'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(248, 271)), 1, 0)\ndf_inputs_prepr['mths_since_earliest_cr_line:271-352'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(271, 353)), 1, 0)\ndf_inputs_prepr['mths_since_earliest_cr_line:>352'] = np.where(df_inputs_prepr['mths_since_earliest_cr_line'].isin(range(353, int(df_inputs_prepr['mths_since_earliest_cr_line'].max()))), 1, 0)","3ca6c72f":"# REFERENCE CATEGORY!!!","7ac681d9":"# delinq_2yrs\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'delinq_2yrs', df_targets_prepr)\ndf_temp","d3def62f":"plot_by_woe(df_temp)","093835c3":"# Categories: 0, 1-3, >=4\ndf_inputs_prepr['delinq_2yrs:0'] = np.where((df_inputs_prepr['delinq_2yrs'] == 0), 1, 0)\ndf_inputs_prepr['delinq_2yrs:1-3'] = np.where((df_inputs_prepr['delinq_2yrs'] >= 1) & (df_inputs_prepr['delinq_2yrs'] <= 3), 1, 0)\ndf_inputs_prepr['delinq_2yrs:>=4'] = np.where((df_inputs_prepr['delinq_2yrs'] >= 9), 1, 0)","78b99c3f":"# inq_last_6mths\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'inq_last_6mths', df_targets_prepr)\ndf_temp","fa051d14":"plot_by_woe(df_temp)","cd11f329":"# Categories: 0, 1 - 2, 3 - 6, > 6\ndf_inputs_prepr['inq_last_6mths:0'] = np.where((df_inputs_prepr['inq_last_6mths'] == 0), 1, 0)\ndf_inputs_prepr['inq_last_6mths:1-2'] = np.where((df_inputs_prepr['inq_last_6mths'] >= 1) & (df_inputs_prepr['inq_last_6mths'] <= 2), 1, 0)\ndf_inputs_prepr['inq_last_6mths:3-6'] = np.where((df_inputs_prepr['inq_last_6mths'] >= 3) & (df_inputs_prepr['inq_last_6mths'] <= 6), 1, 0)\ndf_inputs_prepr['inq_last_6mths:>6'] = np.where((df_inputs_prepr['inq_last_6mths'] > 6), 1, 0)\n# open_acc\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'open_acc', df_targets_prepr)\ndf_temp","3b920165":"plot_by_woe(df_temp, 90)","e178c2f8":"plot_by_woe(df_temp.iloc[ : 40, :], 90)","7b801fcb":"# Categories: '0', '1-3', '4-12', '13-17', '18-22', '23-25', '26-30', '>30'\ndf_inputs_prepr['open_acc:0'] = np.where((df_inputs_prepr['open_acc'] == 0), 1, 0)\ndf_inputs_prepr['open_acc:1-3'] = np.where((df_inputs_prepr['open_acc'] >= 1) & (df_inputs_prepr['open_acc'] <= 3), 1, 0)\ndf_inputs_prepr['open_acc:4-12'] = np.where((df_inputs_prepr['open_acc'] >= 4) & (df_inputs_prepr['open_acc'] <= 12), 1, 0)\ndf_inputs_prepr['open_acc:13-17'] = np.where((df_inputs_prepr['open_acc'] >= 13) & (df_inputs_prepr['open_acc'] <= 17), 1, 0)\ndf_inputs_prepr['open_acc:18-22'] = np.where((df_inputs_prepr['open_acc'] >= 18) & (df_inputs_prepr['open_acc'] <= 22), 1, 0)\ndf_inputs_prepr['open_acc:23-25'] = np.where((df_inputs_prepr['open_acc'] >= 23) & (df_inputs_prepr['open_acc'] <= 25), 1, 0)\ndf_inputs_prepr['open_acc:26-30'] = np.where((df_inputs_prepr['open_acc'] >= 26) & (df_inputs_prepr['open_acc'] <= 30), 1, 0)\ndf_inputs_prepr['open_acc:>=31'] = np.where((df_inputs_prepr['open_acc'] >= 31), 1, 0)\n# pub_rec\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'pub_rec', df_targets_prepr)\ndf_temp","c08933e9":"plot_by_woe(df_temp, 90)","3ffae73e":"# Categories '0-2', '3-4', '>=5'\ndf_inputs_prepr['pub_rec:0-2'] = np.where((df_inputs_prepr['pub_rec'] >= 0) & (df_inputs_prepr['pub_rec'] <= 2), 1, 0)\ndf_inputs_prepr['pub_rec:3-4'] = np.where((df_inputs_prepr['pub_rec'] >= 3) & (df_inputs_prepr['pub_rec'] <= 4), 1, 0)\ndf_inputs_prepr['pub_rec:>=5'] = np.where((df_inputs_prepr['pub_rec'] >= 5), 1, 0)\n# total_acc\ndf_inputs_prepr['total_acc_factor'] = pd.cut(df_inputs_prepr['total_acc'], 50)\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'total_acc_factor', df_targets_prepr)\ndf_temp.head()","9c9b387a":"plot_by_woe(df_temp, 90)","3d65f614":"# Categories: '<=27', '28-51', '>51'\ndf_inputs_prepr['total_acc:<=27'] = np.where((df_inputs_prepr['total_acc'] <= 27), 1, 0)\ndf_inputs_prepr['total_acc:28-51'] = np.where((df_inputs_prepr['total_acc'] >= 28) & (df_inputs_prepr['total_acc'] <= 51), 1, 0)\ndf_inputs_prepr['total_acc:>=52'] = np.where((df_inputs_prepr['total_acc'] >= 52), 1, 0)\n# acc_now_delinq\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'acc_now_delinq', df_targets_prepr)\ndf_temp","a78fd30b":"plot_by_woe(df_temp)","d1f4e094":"# Categories: '0', '>=1'\ndf_inputs_prepr['acc_now_delinq:0'] = np.where((df_inputs_prepr['acc_now_delinq'] == 0), 1, 0)\ndf_inputs_prepr['acc_now_delinq:>=1'] = np.where((df_inputs_prepr['acc_now_delinq'] >= 1), 1, 0)\n# total_rev_hi_lim\ndf_inputs_prepr['total_rev_hi_lim_factor'] = pd.cut(df_inputs_prepr['total_rev_hi_lim'], 2000)\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'total_rev_hi_lim_factor', df_targets_prepr)\ndf_temp","0160100a":"plot_by_woe(df_temp.iloc[: 50, : ], 90)","ae0e5696":"# Categories\n# '<=5K', '5K-10K', '10K-20K', '20K-30K', '30K-40K', '40K-55K', '55K-95K', '>95K'\ndf_inputs_prepr['total_rev_hi_lim:<=5K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] <= 5000), 1, 0)\ndf_inputs_prepr['total_rev_hi_lim:5K-10K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 5000) & (df_inputs_prepr['total_rev_hi_lim'] <= 10000), 1, 0)\ndf_inputs_prepr['total_rev_hi_lim:10K-20K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 10000) & (df_inputs_prepr['total_rev_hi_lim'] <= 20000), 1, 0)\ndf_inputs_prepr['total_rev_hi_lim:20K-30K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 20000) & (df_inputs_prepr['total_rev_hi_lim'] <= 30000), 1, 0)\ndf_inputs_prepr['total_rev_hi_lim:30K-40K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 30000) & (df_inputs_prepr['total_rev_hi_lim'] <= 40000), 1, 0)\ndf_inputs_prepr['total_rev_hi_lim:40K-55K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 40000) & (df_inputs_prepr['total_rev_hi_lim'] <= 55000), 1, 0)\ndf_inputs_prepr['total_rev_hi_lim:55K-95K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 55000) & (df_inputs_prepr['total_rev_hi_lim'] <= 95000), 1, 0)\ndf_inputs_prepr['total_rev_hi_lim:>95K'] = np.where((df_inputs_prepr['total_rev_hi_lim'] > 95000), 1, 0)","f0a917b6":"# annual_inc\ndf_inputs_prepr['annual_inc_factor'] = pd.cut(df_inputs_prepr['annual_inc'], 50)\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'annual_inc_factor', df_targets_prepr)\ndf_temp","2dfc3cfd":"df_inputs_prepr['annual_inc_factor'] = pd.cut(df_inputs_prepr['annual_inc'], 100)\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'annual_inc_factor', df_targets_prepr)\ndf_temp","4097b832":"# Initial examination shows that there are too few individuals with large income and too many with small income.\n# Hence, we are going to have one category for more than 150K, and we are going to apply our approach to determine\n# the categories of everyone with 140k or less.\ndf_inputs_prepr_temp = df_inputs_prepr.loc[df_inputs_prepr['annual_inc'] <= 140000, : ]\n#loan_data_temp = loan_data_temp.reset_index(drop = True)\n#df_inputs_prepr_temp\n#pd.options.mode.chained_assignment = None \ndf_inputs_prepr_temp[\"annual_inc_factor\"] = pd.cut(df_inputs_prepr_temp['annual_inc'], 50)\ndf_temp = woe_ordered_continuous(df_inputs_prepr_temp, 'annual_inc_factor', df_targets_prepr[df_inputs_prepr_temp.index])\ndf_temp.head()","5c2fbade":"plot_by_woe(df_temp, 90)","71434364":"# WoE is monotonically decreasing with income, so we split income in 10 equal categories, each with width of 15k.\ndf_inputs_prepr['annual_inc:<20K'] = np.where((df_inputs_prepr['annual_inc'] <= 20000), 1, 0)\ndf_inputs_prepr['annual_inc:20K-30K'] = np.where((df_inputs_prepr['annual_inc'] > 20000) & (df_inputs_prepr['annual_inc'] <= 30000), 1, 0)\ndf_inputs_prepr['annual_inc:30K-40K'] = np.where((df_inputs_prepr['annual_inc'] > 30000) & (df_inputs_prepr['annual_inc'] <= 40000), 1, 0)\ndf_inputs_prepr['annual_inc:40K-50K'] = np.where((df_inputs_prepr['annual_inc'] > 40000) & (df_inputs_prepr['annual_inc'] <= 50000), 1, 0)\ndf_inputs_prepr['annual_inc:50K-60K'] = np.where((df_inputs_prepr['annual_inc'] > 50000) & (df_inputs_prepr['annual_inc'] <= 60000), 1, 0)\ndf_inputs_prepr['annual_inc:60K-70K'] = np.where((df_inputs_prepr['annual_inc'] > 60000) & (df_inputs_prepr['annual_inc'] <= 70000), 1, 0)\ndf_inputs_prepr['annual_inc:70K-80K'] = np.where((df_inputs_prepr['annual_inc'] > 70000) & (df_inputs_prepr['annual_inc'] <= 80000), 1, 0)\ndf_inputs_prepr['annual_inc:80K-90K'] = np.where((df_inputs_prepr['annual_inc'] > 80000) & (df_inputs_prepr['annual_inc'] <= 90000), 1, 0)\ndf_inputs_prepr['annual_inc:90K-100K'] = np.where((df_inputs_prepr['annual_inc'] > 90000) & (df_inputs_prepr['annual_inc'] <= 100000), 1, 0)\ndf_inputs_prepr['annual_inc:100K-120K'] = np.where((df_inputs_prepr['annual_inc'] > 100000) & (df_inputs_prepr['annual_inc'] <= 120000), 1, 0)\ndf_inputs_prepr['annual_inc:120K-140K'] = np.where((df_inputs_prepr['annual_inc'] > 120000) & (df_inputs_prepr['annual_inc'] <= 140000), 1, 0)\ndf_inputs_prepr['annual_inc:>140K'] = np.where((df_inputs_prepr['annual_inc'] > 140000), 1, 0)\n# dti\ndf_inputs_prepr['dti_factor'] = pd.cut(df_inputs_prepr['dti'], 100)\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'dti_factor', df_targets_prepr)\ndf_temp","11f395df":"plot_by_woe(df_temp, 90)","63520ea0":"# Similarly to income, initial examination shows that most values are lower than 200.\n# Hence, we are going to have one category for more than 35, and we are going to apply our approach to determine\n# the categories of everyone with 150k or less.\ndf_inputs_prepr_temp['dti_factor'] = pd.cut(df_inputs_prepr_temp['dti'], 50)\ndf_temp = woe_ordered_continuous(df_inputs_prepr_temp, 'dti_factor', df_targets_prepr[df_inputs_prepr_temp.index])\ndf_temp","43a3e62c":"plot_by_woe(df_temp, 90)","38ffe187":"# Categories:\ndf_inputs_prepr['dti:<=1.4'] = np.where((df_inputs_prepr['dti'] <= 1.4), 1, 0)\ndf_inputs_prepr['dti:1.4-3.5'] = np.where((df_inputs_prepr['dti'] > 1.4) & (df_inputs_prepr['dti'] <= 3.5), 1, 0)\ndf_inputs_prepr['dti:3.5-7.7'] = np.where((df_inputs_prepr['dti'] > 3.5) & (df_inputs_prepr['dti'] <= 7.7), 1, 0)\ndf_inputs_prepr['dti:7.7-10.5'] = np.where((df_inputs_prepr['dti'] > 7.7) & (df_inputs_prepr['dti'] <= 10.5), 1, 0)\ndf_inputs_prepr['dti:10.5-16.1'] = np.where((df_inputs_prepr['dti'] > 10.5) & (df_inputs_prepr['dti'] <= 16.1), 1, 0)\ndf_inputs_prepr['dti:16.1-20.3'] = np.where((df_inputs_prepr['dti'] > 16.1) & (df_inputs_prepr['dti'] <= 20.3), 1, 0)\ndf_inputs_prepr['dti:20.3-21.7'] = np.where((df_inputs_prepr['dti'] > 20.3) & (df_inputs_prepr['dti'] <= 21.7), 1, 0)\ndf_inputs_prepr['dti:21.7-22.4'] = np.where((df_inputs_prepr['dti'] > 21.7) & (df_inputs_prepr['dti'] <= 22.4), 1, 0)\ndf_inputs_prepr['dti:22.4-35'] = np.where((df_inputs_prepr['dti'] > 22.4) & (df_inputs_prepr['dti'] <= 35), 1, 0)\ndf_inputs_prepr['dti:>35'] = np.where((df_inputs_prepr['dti'] > 35), 1, 0)\n# mths_since_last_delinq\n# We have to create one category for missing values and do fine and coarse classing for the rest.\n#loan_data_temp = loan_data[np.isfinite(loan_data['mths_since_last_delinq'])]\ndf_inputs_prepr_temp = df_inputs_prepr[pd.notnull(df_inputs_prepr['mths_since_last_delinq'])]\n#sum(loan_data_temp['mths_since_last_delinq'].isnull())\ndf_inputs_prepr_temp['mths_since_last_delinq_factor'] = pd.cut(df_inputs_prepr_temp['mths_since_last_delinq'], 50)\ndf_temp = woe_ordered_continuous(df_inputs_prepr_temp, 'mths_since_last_delinq_factor', df_targets_prepr[df_inputs_prepr_temp.index])\ndf_temp.head()","e19469ed":"plot_by_woe(df_temp, 90)","27c98bd0":"# Categories: Missing, 0-3, 4-30, 31-56, >=57\ndf_inputs_prepr['mths_since_last_delinq:Missing'] = np.where((df_inputs_prepr['mths_since_last_delinq'].isnull()), 1, 0)\ndf_inputs_prepr['mths_since_last_delinq:0-3'] = np.where((df_inputs_prepr['mths_since_last_delinq'] >= 0) & (df_inputs_prepr['mths_since_last_delinq'] <= 3), 1, 0)\ndf_inputs_prepr['mths_since_last_delinq:4-30'] = np.where((df_inputs_prepr['mths_since_last_delinq'] >= 4) & (df_inputs_prepr['mths_since_last_delinq'] <= 30), 1, 0)\ndf_inputs_prepr['mths_since_last_delinq:31-56'] = np.where((df_inputs_prepr['mths_since_last_delinq'] >= 31) & (df_inputs_prepr['mths_since_last_delinq'] <= 56), 1, 0)\ndf_inputs_prepr['mths_since_last_delinq:>=57'] = np.where((df_inputs_prepr['mths_since_last_delinq'] >= 57), 1, 0)\n# mths_since_last_record\n# We have to create one category for missing values and do fine and coarse classing for the rest.\ndf_inputs_prepr_temp = df_inputs_prepr[pd.notnull(df_inputs_prepr['mths_since_last_record'])]\n#sum(loan_data_temp['mths_since_last_record'].isnull())\ndf_inputs_prepr_temp['mths_since_last_record_factor'] = pd.cut(df_inputs_prepr_temp['mths_since_last_record'], 50)\ndf_temp = woe_ordered_continuous(df_inputs_prepr_temp, 'mths_since_last_record_factor', df_targets_prepr[df_inputs_prepr_temp.index])\ndf_temp.head()","5522ea68":"plot_by_woe(df_temp, 90)","e1ebb6f9":"# Categories: 'Missing', '0-2', '3-20', '21-31', '32-80', '81-86', '>86'\ndf_inputs_prepr['mths_since_last_record:Missing'] = np.where((df_inputs_prepr['mths_since_last_record'].isnull()), 1, 0)\ndf_inputs_prepr['mths_since_last_record:0-2'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 0) & (df_inputs_prepr['mths_since_last_record'] <= 2), 1, 0)\ndf_inputs_prepr['mths_since_last_record:3-20'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 3) & (df_inputs_prepr['mths_since_last_record'] <= 20), 1, 0)\ndf_inputs_prepr['mths_since_last_record:21-31'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 21) & (df_inputs_prepr['mths_since_last_record'] <= 31), 1, 0)\ndf_inputs_prepr['mths_since_last_record:32-80'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 32) & (df_inputs_prepr['mths_since_last_record'] <= 80), 1, 0)\ndf_inputs_prepr['mths_since_last_record:81-86'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 81) & (df_inputs_prepr['mths_since_last_record'] <= 86), 1, 0)\ndf_inputs_prepr['mths_since_last_record:>=86'] = np.where((df_inputs_prepr['mths_since_last_record'] >= 86), 1, 0)\ndf_inputs_prepr['mths_since_last_delinq:Missing'].sum()","1bb9c845":"# display inputs_train, inputs_test","d355977a":"# funded_amnt\ndf_inputs_prepr['funded_amnt_factor'] = pd.cut(df_inputs_prepr['funded_amnt'], 50)\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'funded_amnt_factor', df_targets_prepr)\ndf_temp.head()","77a8efba":"plot_by_woe(df_temp, 90)","85b757e6":"# WON'T USE because there is no clear trend, even if segments of the whole range are considered.\n# installment\ndf_inputs_prepr['installment_factor'] = pd.cut(df_inputs_prepr['installment'], 50)\ndf_temp = woe_ordered_continuous(df_inputs_prepr, 'installment_factor', df_targets_prepr)\ndf_temp.head()","ffd0261c":"plot_by_woe(df_temp, 90)","ab39ace4":"# WON'T USE because there is no clear trend, even if segments of the whole range are considered.","57232e53":"#####\n#loan_data_inputs_train = df_inputs_prepr\n#####\n#loan_data_inputs_test = df_inputs_prepr\n######\nloan_data_inputs_2015 = df_inputs_prepr\nloan_data_targets_2015 = df_targets_prepr\n#loan_data_inputs_train.columns.values\n#loan_data_inputs_test.columns.values\n#loan_data_inputs_train.shape\n#loan_data_targets_train.shape\n#loan_data_inputs_test.shape\n#loan_data_targets_test.shape\nloan_data_inputs_2015.columns.values","429733b5":"loan_data_inputs_2015.shape","ba411a5f":"loan_data_targets_2015.shape","420fd267":"#loan_data_inputs_train.to_csv('loan_data_inputs_train.csv')\n#loan_data_targets_train.to_csv('loan_data_targets_train.csv')\n#loan_data_inputs_test.to_csv('loan_data_inputs_test.csv')\n#loan_data_targets_test.to_csv('loan_data_targets_test.csv')","a0edd60f":"loan_data_inputs_2015.to_csv('loan_data_inputs_2015.csv')\nloan_data_targets_2015.to_csv('loan_data_targets_2015.csv')","18403a61":"inputs_train_with_ref_cat = pd.read_csv('loan_data_inputs_2015.csv', index_col = 0)\n# We import the dataset with old data, i.e. \"expected\" data.","b391ff56":"# From the dataframe with new, \"actual\" data, we keep only the relevant columns.\ninputs_2015_with_ref_cat = loan_data_inputs_2015.loc[: , ['grade:A',\n'grade:B',\n'grade:C',\n'grade:D',\n'grade:E',\n'grade:F',\n'grade:G',\n'home_ownership:RENT_OTHER_NONE_ANY',\n'home_ownership:OWN',\n'home_ownership:MORTGAGE',\n'addr_state:ND_NE_IA_NV_FL_HI_AL',\n'addr_state:NM_VA',\n'addr_state:NY',\n'addr_state:OK_TN_MO_LA_MD_NC',\n'addr_state:CA',\n'addr_state:UT_KY_AZ_NJ',\n'addr_state:AR_MI_PA_OH_MN',\n'addr_state:RI_MA_DE_SD_IN',\n'addr_state:GA_WA_OR',\n'addr_state:WI_MT',\n'addr_state:TX',\n'addr_state:IL_CT',\n'addr_state:KS_SC_CO_VT_AK_MS',\n'addr_state:WV_NH_WY_DC_ME_ID',\n'verification_status:Not Verified',\n'verification_status:Source Verified',\n'verification_status:Verified',\n'purpose:educ__sm_b__wedd__ren_en__mov__house',\n'purpose:credit_card',\n'purpose:debt_consolidation',\n'purpose:oth__med__vacation',\n'purpose:major_purch__car__home_impr',\n'initial_list_status:f',\n'initial_list_status:w',\n'term:36',\n'term:60',\n'emp_length:0',\n'emp_length:1',\n'emp_length:2-4',\n'emp_length:5-6',\n'emp_length:7-9',\n'emp_length:10',\n'mths_since_issue_d:<38',\n'mths_since_issue_d:38-39',\n'mths_since_issue_d:40-41',\n'mths_since_issue_d:42-48',\n'mths_since_issue_d:49-52',\n'mths_since_issue_d:53-64',\n'mths_since_issue_d:65-84',\n'mths_since_issue_d:>84',\n'int_rate:<9.548',\n'int_rate:9.548-12.025',\n'int_rate:12.025-15.74',\n'int_rate:15.74-20.281',\n'int_rate:>20.281',\n'mths_since_earliest_cr_line:<140',\n'mths_since_earliest_cr_line:141-164',\n'mths_since_earliest_cr_line:165-247',\n'mths_since_earliest_cr_line:248-270',\n'mths_since_earliest_cr_line:271-352',\n'mths_since_earliest_cr_line:>352',\n'inq_last_6mths:0',\n'inq_last_6mths:1-2',\n'inq_last_6mths:3-6',\n'inq_last_6mths:>6',\n'acc_now_delinq:0',\n'acc_now_delinq:>=1',\n'annual_inc:<20K',\n'annual_inc:20K-30K',\n'annual_inc:30K-40K',\n'annual_inc:40K-50K',\n'annual_inc:50K-60K',\n'annual_inc:60K-70K',\n'annual_inc:70K-80K',\n'annual_inc:80K-90K',\n'annual_inc:90K-100K',\n'annual_inc:100K-120K',\n'annual_inc:120K-140K',\n'annual_inc:>140K',\n'dti:<=1.4',\n'dti:1.4-3.5',\n'dti:3.5-7.7',\n'dti:7.7-10.5',\n'dti:10.5-16.1',\n'dti:16.1-20.3',\n'dti:20.3-21.7',\n'dti:21.7-22.4',\n'dti:22.4-35',\n'dti:>35',\n'mths_since_last_delinq:Missing',\n'mths_since_last_delinq:0-3',\n'mths_since_last_delinq:4-30',\n'mths_since_last_delinq:31-56',\n'mths_since_last_delinq:>=57',\n'mths_since_last_record:Missing',\n'mths_since_last_record:0-2',\n'mths_since_last_record:3-20',\n'mths_since_last_record:21-31',\n'mths_since_last_record:32-80',\n'mths_since_last_record:81-86',\n'mths_since_last_record:>=86',\n]]\ninputs_train_with_ref_cat.shape","f0d90c09":"inputs_2015_with_ref_cat.shape","2b42021f":"df_scorecard = pd.read_csv('df_scorecard.csv', index_col = 0)\n# We import the scorecard.\ndf_scorecard","9a376850":"inputs_train_with_ref_cat_w_intercept = inputs_train_with_ref_cat\ninputs_train_with_ref_cat_w_intercept.insert(0, 'Intercept', 1)\n# We insert a column in the dataframe, with an index of 0, that is, in the beginning of the dataframe.\n# The name of that column is 'Intercept', and its values are 1s.\ninputs_train_with_ref_cat_w_intercept = inputs_train_with_ref_cat_w_intercept[df_scorecard['Feature name'].values]\n# Here, from the 'inputs_train_with_ref_cat_w_intercept' dataframe, we keep only the columns with column names,\n# exactly equal to the row values of the 'Feature name' column from the 'df_scorecard' dataframe.\ninputs_train_with_ref_cat_w_intercept.head()","97d71613":"inputs_2015_with_ref_cat_w_intercept = inputs_2015_with_ref_cat\ninputs_2015_with_ref_cat_w_intercept.insert(0, 'Intercept', 1)\n# We insert a column in the dataframe, with an index of 0, that is, in the beginning of the dataframe.\n# The name of that column is 'Intercept', and its values are 1s.\ninputs_2015_with_ref_cat_w_intercept = inputs_2015_with_ref_cat_w_intercept[df_scorecard['Feature name'].values]\n# Here, from the 'inputs_train_with_ref_cat_w_intercept' dataframe, we keep only the columns with column names,\n# exactly equal to the row values of the 'Feature name' column from the 'df_scorecard' dataframe.\ninputs_2015_with_ref_cat_w_intercept.head()","22249c2e":"scorecard_scores = df_scorecard['Score - Final']\nscorecard_scores = scorecard_scores.values.reshape(102, 1)","35cbee43":"# Import data.\nloan_data_preprocessed_backup = pd.read_csv('loan_data_2007_2014_preprocessed.csv')","112fca78":"loan_data_preprocessed = loan_data_preprocessed_backup.copy()\nloan_data_preprocessed.columns.values\n# Displays all column names.","3a3c7b2c":"loan_data_preprocessed.head()","8e7f79da":"loan_data_defaults = loan_data_preprocessed[loan_data_preprocessed['loan_status'].isin(['Charged Off','Does not meet the credit policy. Status:Charged Off'])]\n# Here we take only the accounts that were charged-off (written-off).\nloan_data_defaults.shape","265fd3a2":"pd.options.display.max_rows = None\n# Sets the pandas dataframe options to display all columns\/ rows.\nloan_data_defaults.isnull().sum()","660b6fe6":"loan_data_defaults['mths_since_last_delinq'].fillna(0, inplace = True)\n# We fill the missing values with zeroes.\n#loan_data_defaults['mths_since_last_delinq'].fillna(loan_data_defaults['mths_since_last_delinq'].max() + 12, inplace=True)","145f7986":"loan_data_defaults['mths_since_last_record'].fillna(0, inplace=True)\n# We fill the missing values with zeroes.","a62d553c":"loan_data_defaults['recovery_rate'] = loan_data_defaults['recoveries'] \/ loan_data_defaults['funded_amnt']\n# We calculate the dependent variable for the LGD model: recovery rate.\n# It is the ratio of recoveries and funded amount.\nloan_data_defaults['recovery_rate'].describe()\n# Shows some descriptive statisics for the values of a column.","ccfccb18":"loan_data_defaults['recovery_rate'] = np.where(loan_data_defaults['recovery_rate'] > 1, 1, loan_data_defaults['recovery_rate'])\nloan_data_defaults['recovery_rate'] = np.where(loan_data_defaults['recovery_rate'] < 0, 0, loan_data_defaults['recovery_rate'])\n# We set recovery rates that are greater than 1 to 1 and recovery rates that are less than 0 to 0.\nloan_data_defaults['recovery_rate'].describe()\n# Shows some descriptive statisics for the values of a column.","10eba669":"loan_data_defaults['CCF'] = (loan_data_defaults['funded_amnt'] - loan_data_defaults['total_rec_prncp']) \/ loan_data_defaults['funded_amnt']\n# We calculate the dependent variable for the EAD model: credit conversion factor.\n# It is the ratio of the difference of the amount used at the moment of default to the total funded amount.\nloan_data_defaults['CCF'].describe()\n# Shows some descriptive statisics for the values of a column.","af655887":"loan_data_defaults.to_csv('loan_data_defaults.csv')\n# We save the data to a CSV file.","61865cfd":"plt.hist(loan_data_defaults['recovery_rate'], bins = 100)\n# We plot a histogram of a variable with 100 bins.","1153095b":"plt.hist(loan_data_defaults['recovery_rate'], bins = 50)\n# We plot a histogram of a variable with 50 bins.","d596cfd3":"plt.hist(loan_data_defaults['CCF'], bins = 100)\n# We plot a histogram of a variable with 100 bins.","9669db6c":"loan_data_defaults['recovery_rate_0_1'] = np.where(loan_data_defaults['recovery_rate'] == 0, 0, 1)\n# We create a new variable which is 0 if recovery rate is 0 and 1 otherwise.\nloan_data_defaults['recovery_rate_0_1']","99db6edd":"# LGD model stage 1 datasets: recovery rate 0 or greater than 0.\nlgd_inputs_stage_1_train, lgd_inputs_stage_1_test, lgd_targets_stage_1_train, lgd_targets_stage_1_test = train_test_split(loan_data_defaults.drop(['good_bad', 'recovery_rate','recovery_rate_0_1', 'CCF'], axis = 1), loan_data_defaults['recovery_rate_0_1'], test_size = 0.2, random_state = 42)\n# Takes a set of inputs and a set of targets as arguments. Splits the inputs and the targets into four dataframes:\n# Inputs - Train, Inputs - Test, Targets - Train, Targets - Test.","e33fad36":"features_all = ['grade:A',\n'grade:B',\n'grade:C',\n'grade:D',\n'grade:E',\n'grade:F',\n'grade:G',\n'home_ownership:MORTGAGE',\n'home_ownership:NONE',\n'home_ownership:OTHER',\n'home_ownership:OWN',\n'home_ownership:RENT',\n'verification_status:Not Verified',\n'verification_status:Source Verified',\n'verification_status:Verified',\n'purpose:car',\n'purpose:credit_card',\n'purpose:debt_consolidation',\n'purpose:educational',\n'purpose:home_improvement',\n'purpose:house',\n'purpose:major_purchase',\n'purpose:medical',\n'purpose:moving',\n'purpose:other',\n'purpose:renewable_energy',\n'purpose:small_business',\n'purpose:vacation',\n'purpose:wedding',\n'initial_list_status:f',\n'initial_list_status:w',\n'term_int',\n'emp_length_int',\n'mths_since_issue_d',\n'mths_since_earliest_cr_line',\n'funded_amnt',\n'int_rate',\n'installment',\n'annual_inc',\n'dti',\n'delinq_2yrs',\n'inq_last_6mths',\n'mths_since_last_delinq',\n'mths_since_last_record',\n'open_acc',\n'pub_rec',\n'total_acc',\n'acc_now_delinq',\n'total_rev_hi_lim']\n# List of all independent variables for the models.\nfeatures_reference_cat = ['grade:G',\n'home_ownership:RENT',\n'verification_status:Verified',\n'purpose:credit_card',\n'initial_list_status:f']\n# List of the dummy variable reference categories. \nlgd_inputs_stage_1_train = lgd_inputs_stage_1_train[features_all]\n# Here we keep only the variables we need for the model.\nlgd_inputs_stage_1_train = lgd_inputs_stage_1_train.drop(features_reference_cat, axis = 1)\n# Here we remove the dummy variable reference categories.\nlgd_inputs_stage_1_train.isnull().sum()\n# Check for missing values. We check whether the value of each row for each column is missing or not,\n# then sum accross columns.","684343b5":"# P values for sklearn logistic regression.\n\n# Class to display p-values for logistic regression in sklearn.\n\nfrom sklearn import linear_model\nimport scipy.stats as stat\n\nclass LogisticRegression_with_p_values:\n    \n    def __init__(self,*args,**kwargs):#,**kwargs):\n        self.model = linear_model.LogisticRegression(*args,**kwargs)#,**args)\n\n    def fit(self,X,y):\n        self.model.fit(X,y)\n        \n        #### Get p-values for the fitted model ####\n        denom = (2.0 * (1.0 + np.cosh(self.model.decision_function(X))))\n        denom = np.tile(denom,(X.shape[1],1)).T\n        F_ij = np.dot((X \/ denom).T,X) ## Fisher Information Matrix\n        Cramer_Rao = np.linalg.inv(F_ij) ## Inverse Information Matrix\n        sigma_estimates = np.sqrt(np.diagonal(Cramer_Rao))\n        z_scores = self.model.coef_[0] \/ sigma_estimates # z-score for eaach model coefficient\n        p_values = [stat.norm.sf(abs(x)) * 2 for x in z_scores] ### two tailed test for p-values\n        \n        self.coef_ = self.model.coef_\n        self.intercept_ = self.model.intercept_\n        #self.z_scores = z_scores\n        self.p_values = p_values\n        #self.sigma_estimates = sigma_estimates\n        #self.F_ij = F_ij","41e3ac12":"reg_lgd_st_1 = LogisticRegression_with_p_values()\n# We create an instance of an object from the 'LogisticRegression' class.\nreg_lgd_st_1.fit(lgd_inputs_stage_1_train, lgd_targets_stage_1_train)\n# Estimates the coefficients of the object from the 'LogisticRegression' class\n# with inputs (independent variables) contained in the first dataframe\n# and targets (dependent variables) contained in the second dataframe.","f919c230":"feature_name = lgd_inputs_stage_1_train.columns.values\n# Stores the names of the columns of a dataframe in a variable.","42d827a8":"summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n# Creates a dataframe with a column titled 'Feature name' and row values contained in the 'feature_name' variable.\nsummary_table['Coefficients'] = np.transpose(reg_lgd_st_1.coef_)\n# Creates a new column in the dataframe, called 'Coefficients',\n# with row values the transposed coefficients from the 'LogisticRegression' object.\nsummary_table.index = summary_table.index + 1\n# Increases the index of every row of the dataframe with 1.\nsummary_table.loc[0] = ['Intercept', reg_lgd_st_1.intercept_[0]]\n# Assigns values of the row with index 0 of the dataframe.\nsummary_table = summary_table.sort_index()\n# Sorts the dataframe by index.\np_values = reg_lgd_st_1.p_values\n# We take the result of the newly added method 'p_values' and store it in a variable 'p_values'.\np_values = np.append(np.nan,np.array(p_values))\n# We add the value 'NaN' in the beginning of the variable with p-values.\nsummary_table['p_values'] = p_values\n# In the 'summary_table' dataframe, we add a new column, called 'p_values', containing the values from the 'p_values' variable.\nsummary_table","a5723198":"summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\nsummary_table['Coefficients'] = np.transpose(reg_lgd_st_1.coef_)\nsummary_table.index = summary_table.index + 1\nsummary_table.loc[0] = ['Intercept', reg_lgd_st_1.intercept_[0]]\nsummary_table = summary_table.sort_index()\np_values = reg_lgd_st_1.p_values\np_values = np.append(np.nan,np.array(p_values))\nsummary_table['p_values'] = p_values\nsummary_table","f1c71540":"lgd_inputs_stage_1_test = lgd_inputs_stage_1_test[features_all]\n# Here we keep only the variables we need for the model.\nlgd_inputs_stage_1_test = lgd_inputs_stage_1_test.drop(features_reference_cat, axis = 1)\n# Here we remove the dummy variable reference categories.\ny_hat_test_lgd_stage_1 = reg_lgd_st_1.model.predict(lgd_inputs_stage_1_test)\n# Calculates the predicted values for the dependent variable (targets)\n# based on the values of the independent variables (inputs) supplied as an argument.\ny_hat_test_lgd_stage_1","457ca24a":"y_hat_test_proba_lgd_stage_1 = reg_lgd_st_1.model.predict_proba(lgd_inputs_stage_1_test)\n# Calculates the predicted probability values for the dependent variable (targets)\n# based on the values of the independent variables (inputs) supplied as an argument.\ny_hat_test_proba_lgd_stage_1\n# This is an array of arrays of predicted class probabilities for all classes.\n# In this case, the first value of every sub-array is the probability for the observation to belong to the first class, i.e. 0,\n# and the second value is the probability for the observation to belong to the first class, i.e. 1.","77886162":"y_hat_test_proba_lgd_stage_1 = y_hat_test_proba_lgd_stage_1[: ][: , 1]\n# Here we take all the arrays in the array, and from each array, we take all rows, and only the element with index 1,\n# that is, the second element.\n# In other words, we take only the probabilities for being 1.\ny_hat_test_proba_lgd_stage_1","baabd174":"lgd_targets_stage_1_test_temp = lgd_targets_stage_1_test\nlgd_targets_stage_1_test_temp.reset_index(drop = True, inplace = True)\n# We reset the index of a dataframe.\ndf_actual_predicted_probs = pd.concat([lgd_targets_stage_1_test_temp, pd.DataFrame(y_hat_test_proba_lgd_stage_1)], axis = 1)\n# Concatenates two dataframes.\ndf_actual_predicted_probs.columns = ['lgd_targets_stage_1_test', 'y_hat_test_proba_lgd_stage_1']","16a80288":"df_actual_predicted_probs.index = lgd_inputs_stage_1_test.index\n# Makes the index of one dataframe equal to the index of another dataframe.","f05f31b4":"df_actual_predicted_probs.head()","dfb8a9f0":"tr = 0.5\n# We create a new column with an indicator,\n# where every observation that has predicted probability greater than the threshold has a value of 1,\n# and every observation that has predicted probability lower than the threshold has a value of 0.\ndf_actual_predicted_probs['y_hat_test_lgd_stage_1'] = np.where(df_actual_predicted_probs['y_hat_test_proba_lgd_stage_1'] > tr, 1, 0)\npd.crosstab(df_actual_predicted_probs['lgd_targets_stage_1_test'], df_actual_predicted_probs['y_hat_test_lgd_stage_1'], rownames = ['Actual'], colnames = ['Predicted'])\n# Creates a cross-table where the actual values are displayed by rows and the predicted values by columns.\n# This table is known as a Confusion Matrix.\npd.crosstab(df_actual_predicted_probs['lgd_targets_stage_1_test'], df_actual_predicted_probs['y_hat_test_lgd_stage_1'], rownames = ['Actual'], colnames = ['Predicted']) \/ df_actual_predicted_probs.shape[0]\n# Here we divide each value of the table by the total number of observations,\n# thus getting percentages, or, rates.\n(pd.crosstab(df_actual_predicted_probs['lgd_targets_stage_1_test'], df_actual_predicted_probs['y_hat_test_lgd_stage_1'], rownames = ['Actual'], colnames = ['Predicted']) \/ df_actual_predicted_probs.shape[0]).iloc[0, 0] + (pd.crosstab(df_actual_predicted_probs['lgd_targets_stage_1_test'], df_actual_predicted_probs['y_hat_test_lgd_stage_1'], rownames = ['Actual'], colnames = ['Predicted']) \/ df_actual_predicted_probs.shape[0]).iloc[1, 1]\n# Here we calculate Accuracy of the model, which is the sum of the diagonal rates.","8afa4bf4":"from sklearn.metrics import roc_curve, roc_auc_score","f78a998d":"fpr, tpr, thresholds = roc_curve(df_actual_predicted_probs['lgd_targets_stage_1_test'], df_actual_predicted_probs['y_hat_test_proba_lgd_stage_1'])\n# Returns the Receiver Operating Characteristic (ROC) Curve from a set of actual values and their predicted probabilities.\n# As a result, we get three arrays: the false positive rates, the true positive rates, and the thresholds.\n# we store each of the three arrays in a separate variable.","a868d675":"plt.plot(fpr, tpr)\n# We plot the false positive rate along the x-axis and the true positive rate along the y-axis,\n# thus plotting the ROC curve.\nplt.plot(fpr, fpr, linestyle = '--', color = 'k')\n# We plot a seconary diagonal line, with dashed line style and black color.\nplt.xlabel('False positive rate')\n# We name the x-axis \"False positive rate\".\nplt.ylabel('True positive rate')\n# We name the x-axis \"True positive rate\".\nplt.title('ROC curve')\n# We name the graph \"ROC curve\".","5bc2889c":"AUROC = roc_auc_score(df_actual_predicted_probs['lgd_targets_stage_1_test'], df_actual_predicted_probs['y_hat_test_proba_lgd_stage_1'])\n# Calculates the Area Under the Receiver Operating Characteristic Curve (AUROC)\n# from a set of actual values and their predicted probabilities.\nAUROC","c1a2ecf9":"import pickle","fe11d16a":"pickle.dump(reg_lgd_st_1, open('lgd_model_stage_1.sav', 'wb'))\n# Here we export our model to a 'SAV' file with file name 'lgd_model_stage_1.sav'.","5b9d96bf":"lgd_stage_2_data = loan_data_defaults[loan_data_defaults['recovery_rate_0_1'] == 1]\n# Here we take only rows where the original recovery rate variable is greater than one,\n# i.e. where the indicator variable we created is equal to 1.\n# LGD model stage 2 datasets: how much more than 0 is the recovery rate\nlgd_inputs_stage_2_train, lgd_inputs_stage_2_test, lgd_targets_stage_2_train, lgd_targets_stage_2_test = train_test_split(lgd_stage_2_data.drop(['good_bad', 'recovery_rate','recovery_rate_0_1', 'CCF'], axis = 1), lgd_stage_2_data['recovery_rate'], test_size = 0.2, random_state = 42)\n# Takes a set of inputs and a set of targets as arguments. Splits the inputs and the targets into four dataframes:\n# Inputs - Train, Inputs - Test, Targets - Train, Targets - Test.","ff17c091":"from sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score","2c0ac0b4":"# Since the p-values are obtained through certain statistics, we need the 'stat' module from scipy.stats\nimport scipy.stats as stat\n\n# Since we are using an object oriented language such as Python, we can simply define our own \n# LinearRegression class (the same one from sklearn)\n# By typing the code below we will ovewrite a part of the class with one that includes p-values\n# Here's the full source code of the ORIGINAL class: https:\/\/github.com\/scikit-learn\/scikit-learn\/blob\/7b136e9\/sklearn\/linear_model\/base.py#L362\n\n\nclass LinearRegression(linear_model.LinearRegression):\n    \"\"\"\n    LinearRegression class after sklearn's, but calculate t-statistics\n    and p-values for model coefficients (betas).\n    Additional attributes available after .fit()\n    are `t` and `p` which are of the shape (y.shape[1], X.shape[1])\n    which is (n_features, n_coefs)\n    This class sets the intercept to 0 by default, since usually we include it\n    in X.\n    \"\"\"\n    \n    # nothing changes in __init__\n    def __init__(self, fit_intercept=True, normalize=False, copy_X=True,\n                 n_jobs=1):\n        self.fit_intercept = fit_intercept\n        self.normalize = normalize\n        self.copy_X = copy_X\n        self.n_jobs = n_jobs\n\n    \n    def fit(self, X, y, n_jobs=1):\n        self = super(LinearRegression, self).fit(X, y, n_jobs)\n        \n        # Calculate SSE (sum of squared errors)\n        # and SE (standard error)\n        sse = np.sum((self.predict(X) - y) ** 2, axis=0) \/ float(X.shape[0] - X.shape[1])\n        se = np.array([np.sqrt(np.diagonal(sse * np.linalg.inv(np.dot(X.T, X))))])\n\n        # compute the t-statistic for each feature\n        self.t = self.coef_ \/ se\n        # find the p-value for each feature\n        self.p = np.squeeze(2 * (1 - stat.t.cdf(np.abs(self.t), y.shape[0] - X.shape[1])))\n        return self","c79e05b7":"import scipy.stats as stat\n\nclass LinearRegression(linear_model.LinearRegression):\n    def __init__(self, fit_intercept=True, normalize=False, copy_X=True,\n                 n_jobs=1):\n        self.fit_intercept = fit_intercept\n        self.normalize = normalize\n        self.copy_X = copy_X\n        self.n_jobs = n_jobs\n    def fit(self, X, y, n_jobs=1):\n        self = super(LinearRegression, self).fit(X, y, n_jobs)\n        sse = np.sum((self.predict(X) - y) ** 2, axis=0) \/ float(X.shape[0] - X.shape[1])\n        se = np.array([np.sqrt(np.diagonal(sse * np.linalg.inv(np.dot(X.T, X))))])\n        self.t = self.coef_ \/ se\n        self.p = np.squeeze(2 * (1 - stat.t.cdf(np.abs(self.t), y.shape[0] - X.shape[1])))\n        return self","5c81eb5a":"lgd_inputs_stage_2_train = lgd_inputs_stage_2_train[features_all]\n# Here we keep only the variables we need for the model.\nlgd_inputs_stage_2_train = lgd_inputs_stage_2_train.drop(features_reference_cat, axis = 1)\n# Here we remove the dummy variable reference categories.","696658ab":"reg_lgd_st_2 = LinearRegression()\n# We create an instance of an object from the 'LogisticRegression' class.\nreg_lgd_st_2.fit(lgd_inputs_stage_2_train, lgd_targets_stage_2_train)\n# Estimates the coefficients of the object from the 'LogisticRegression' class\n# with inputs (independent variables) contained in the first dataframe\n# and targets (dependent variables) contained in the second dataframe.","7eb24087":"feature_name = lgd_inputs_stage_2_train.columns.values\n# Stores the names of the columns of a dataframe in a variable.","f1e540e2":"summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n# Creates a dataframe with a column titled 'Feature name' and row values contained in the 'feature_name' variable.\nsummary_table['Coefficients'] = np.transpose(reg_lgd_st_2.coef_)\n# Creates a new column in the dataframe, called 'Coefficients',\n# with row values the transposed coefficients from the 'LogisticRegression' object.\nsummary_table.index = summary_table.index + 1\n# Increases the index of every row of the dataframe with 1.\nsummary_table.loc[0] = ['Intercept', reg_lgd_st_2.intercept_]\n# Assigns values of the row with index 0 of the dataframe.\nsummary_table = summary_table.sort_index()\n# Sorts the dataframe by index.\np_values = reg_lgd_st_2.p\n# We take the result of the newly added method 'p_values' and store it in a variable 'p_values'.\np_values = np.append(np.nan,np.array(p_values))\n# We add the value 'NaN' in the beginning of the variable with p-values.\nsummary_table['p_values'] = p_values.round(3)\n# In the 'summary_table' dataframe, we add a new column, called 'p_values', containing the values from the 'p_values' variable.\nsummary_table","acf9c2d2":"summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\nsummary_table['Coefficients'] = np.transpose(reg_lgd_st_2.coef_)\nsummary_table.index = summary_table.index + 1\nsummary_table.loc[0] = ['Intercept', reg_lgd_st_2.intercept_]\nsummary_table = summary_table.sort_index()\np_values = reg_lgd_st_2.p\np_values = np.append(np.nan,np.array(p_values))\nsummary_table['p_values'] = p_values.round(3)\nsummary_table","491b6aec":"lgd_inputs_stage_2_test = lgd_inputs_stage_2_test[features_all]\n# Here we keep only the variables we need for the model.\nlgd_inputs_stage_2_test = lgd_inputs_stage_2_test.drop(features_reference_cat, axis = 1)\n# Here we remove the dummy variable reference categories.\nlgd_inputs_stage_2_test.columns.values\n# Calculates the predicted values for the dependent variable (targets)\n# based on the values of the independent variables (inputs) supplied as an argument.","b071cbf6":"y_hat_test_lgd_stage_2 = reg_lgd_st_2.predict(lgd_inputs_stage_2_test)\n# Calculates the predicted values for the dependent variable (targets)\n# based on the values of the independent variables (inputs) supplied as an argument.\nlgd_targets_stage_2_test_temp = lgd_targets_stage_2_test\nlgd_targets_stage_2_test_temp = lgd_targets_stage_2_test_temp.reset_index(drop = True)\n# We reset the index of a dataframe.\npd.concat([lgd_targets_stage_2_test_temp, pd.DataFrame(y_hat_test_lgd_stage_2)], axis = 1).corr()\n# We calculate the correlation between actual and predicted values.","6868843e":"sns.distplot(lgd_targets_stage_2_test - y_hat_test_lgd_stage_2)\n# We plot the distribution of the residuals.","39d96ca5":"pickle.dump(reg_lgd_st_2, open('lgd_model_stage_2.sav', 'wb'))\n# Here we export our model to a 'SAV' file with file name 'lgd_model_stage_1.sav'.","a704f1ec":"y_hat_test_lgd_stage_2_all = reg_lgd_st_2.predict(lgd_inputs_stage_1_test)\ny_hat_test_lgd_stage_2_all","9de299e8":"y_hat_test_lgd = y_hat_test_lgd_stage_1 * y_hat_test_lgd_stage_2_all\n# Here we combine the predictions of the models from the two stages.","d1433e4b":"pd.DataFrame(y_hat_test_lgd).describe()\n# Shows some descriptive statisics for the values of a column.","996f2c48":"y_hat_test_lgd = np.where(y_hat_test_lgd < 0, 0, y_hat_test_lgd)\ny_hat_test_lgd = np.where(y_hat_test_lgd > 1, 1, y_hat_test_lgd)\n# We set predicted values that are greater than 1 to 1 and predicted values that are less than 0 to 0.","1f04fd58":"pd.DataFrame(y_hat_test_lgd).describe()\n# Shows some descriptive statisics for the values of a column.","fe437986":"# EAD model datasets\nead_inputs_train, ead_inputs_test, ead_targets_train, ead_targets_test = train_test_split(loan_data_defaults.drop(['good_bad', 'recovery_rate','recovery_rate_0_1', 'CCF'], axis = 1), loan_data_defaults['CCF'], test_size = 0.2, random_state = 42)\n# Takes a set of inputs and a set of targets as arguments. Splits the inputs and the targets into four dataframes:\n# Inputs - Train, Inputs - Test, Targets - Train, Targets - Test.\nead_inputs_train.columns.values","42c4404e":"ead_inputs_train = ead_inputs_train[features_all]\n# Here we keep only the variables we need for the model.\nead_inputs_train = ead_inputs_train.drop(features_reference_cat, axis = 1)\n# Here we remove the dummy variable reference categories.","2c9698e2":"reg_ead = LinearRegression()\n# We create an instance of an object from the 'LogisticRegression' class.\nreg_ead.fit(ead_inputs_train, ead_targets_train)\n# Estimates the coefficients of the object from the 'LogisticRegression' class\n# with inputs (independent variables) contained in the first dataframe\n# and targets (dependent variables) contained in the second dataframe.","f45d6f17":"summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\n# Creates a dataframe with a column titled 'Feature name' and row values contained in the 'feature_name' variable.\nsummary_table['Coefficients'] = np.transpose(reg_ead.coef_)\n# Creates a new column in the dataframe, called 'Coefficients',\n# with row values the transposed coefficients from the 'LogisticRegression' object.\nsummary_table.index = summary_table.index + 1\n# Increases the index of every row of the dataframe with 1.\nsummary_table.loc[0] = ['Intercept', reg_ead.intercept_]\n# Assigns values of the row with index 0 of the dataframe.\nsummary_table = summary_table.sort_index()\n# Sorts the dataframe by index.\np_values = reg_lgd_st_2.p\n# We take the result of the newly added method 'p_values' and store it in a variable 'p_values'.\np_values = np.append(np.nan,np.array(p_values))\n# We add the value 'NaN' in the beginning of the variable with p-values.\nsummary_table['p_values'] = p_values\n# In the 'summary_table' dataframe, we add a new column, called 'p_values', containing the values from the 'p_values' variable.\nsummary_table","7fd04fa3":"summary_table = pd.DataFrame(columns = ['Feature name'], data = feature_name)\nsummary_table['Coefficients'] = np.transpose(reg_ead.coef_)\nsummary_table.index = summary_table.index + 1\nsummary_table.loc[0] = ['Intercept', reg_ead.intercept_]\nsummary_table = summary_table.sort_index()\np_values = reg_lgd_st_2.p\np_values = np.append(np.nan,np.array(p_values))\nsummary_table['p_values'] = p_values\nsummary_table","8cd46d21":"ead_inputs_test = ead_inputs_test[features_all]\n# Here we keep only the variables we need for the model.\nead_inputs_test = ead_inputs_test.drop(features_reference_cat, axis = 1)\n# Here we remove the dummy variable reference categories.\nead_inputs_test.columns.values","09649238":"y_hat_test_ead = reg_ead.predict(ead_inputs_test)\n# Calculates the predicted values for the dependent variable (targets)\n# based on the values of the independent variables (inputs) supplied as an argument.\nead_targets_test_temp = ead_targets_test\nead_targets_test_temp = ead_targets_test_temp.reset_index(drop = True)\n# We reset the index of a dataframe.\npd.concat([ead_targets_test_temp, pd.DataFrame(y_hat_test_ead)], axis = 1).corr()\n# We calculate the correlation between actual and predicted values.","00aca9c1":"sns.distplot(ead_targets_test - y_hat_test_ead)\n# We plot the distribution of the residuals.","61fb53ee":"pd.DataFrame(y_hat_test_ead).describe()\n# Shows some descriptive statisics for the values of a column.","f2e9fb66":"y_hat_test_ead = np.where(y_hat_test_ead < 0, 0, y_hat_test_ead)\ny_hat_test_ead = np.where(y_hat_test_ead > 1, 1, y_hat_test_ead)\n# We set predicted values that are greater than 1 to 1 and predicted values that are less than 0 to 0.\npd.DataFrame(y_hat_test_ead).describe()\n# Shows some descriptive statisics for the values of a column.","670d202e":"loan_data_preprocessed.head()","c6eb67e6":"loan_data_preprocessed['mths_since_last_delinq'].fillna(0, inplace = True)\n# We fill the missing values with zeroes.","97b51359":"loan_data_preprocessed['mths_since_last_record'].fillna(0, inplace = True)\n# We fill the missing values with zeroes.","4c09e09c":"loan_data_preprocessed_lgd_ead = loan_data_preprocessed[features_all]\n# Here we keep only the variables we need for the model.\nloan_data_preprocessed_lgd_ead = loan_data_preprocessed_lgd_ead.drop(features_reference_cat, axis = 1)\n# Here we remove the dummy variable reference categories.\nloan_data_preprocessed['recovery_rate_st_1'] = reg_lgd_st_1.model.predict(loan_data_preprocessed_lgd_ead)\n# We apply the stage 1 LGD model and calculate predicted values.\nloan_data_preprocessed['recovery_rate_st_2'] = reg_lgd_st_2.predict(loan_data_preprocessed_lgd_ead)\n# We apply the stage 2 LGD model and calculate predicted values.\nloan_data_preprocessed['recovery_rate'] = loan_data_preprocessed['recovery_rate_st_1'] * loan_data_preprocessed['recovery_rate_st_2']\n# We combine the predicted values from the stage 1 predicted model and the stage 2 predicted model\n# to calculate the final estimated recovery rate.\n","83558ed7":"loan_data_preprocessed['recovery_rate'] = np.where(loan_data_preprocessed['recovery_rate'] < 0, 0, loan_data_preprocessed['recovery_rate'])\nloan_data_preprocessed['recovery_rate'] = np.where(loan_data_preprocessed['recovery_rate'] > 1, 1, loan_data_preprocessed['recovery_rate'])\n# We set estimated recovery rates that are greater than 1 to 1 and  estimated recovery rates that are less than 0 to 0.","442f17cf":"loan_data_preprocessed['LGD'] = 1 - loan_data_preprocessed['recovery_rate']\n# We calculate estimated LGD. Estimated LGD equals 1 - estimated recovery rate.","e9fe7a68":"loan_data_preprocessed['LGD'].describe()\n# Shows some descriptive statisics for the values of a column.","a7bbe941":"loan_data_preprocessed['CCF'] = reg_ead.predict(loan_data_preprocessed_lgd_ead)\n# We apply the EAD model to calculate estimated credit conversion factor.\nloan_data_preprocessed['CCF'] = np.where(loan_data_preprocessed['CCF'] < 0, 0, loan_data_preprocessed['CCF'])\nloan_data_preprocessed['CCF'] = np.where(loan_data_preprocessed['CCF'] > 1, 1, loan_data_preprocessed['CCF'])\n# We set estimated CCF that are greater than 1 to 1 and  estimated CCF that are less than 0 to 0.\nloan_data_preprocessed['EAD'] = loan_data_preprocessed['CCF'] * loan_data_preprocessed_lgd_ead['funded_amnt']\n# We calculate estimated EAD. Estimated EAD equals estimated CCF multiplied by funded amount.\nloan_data_preprocessed['EAD'].describe()\n# Shows some descriptive statisics for the values of a column.","0accc224":"loan_data_preprocessed.head()","4c13e2e9":"loan_data_inputs_train = pd.read_csv('loan_data_inputs_train.csv')\n# We import data to apply the PD model.\nloan_data_inputs_test = pd.read_csv('loan_data_inputs_test.csv')\n# We import data to apply the PD model.\nloan_data_inputs_pd = pd.concat([loan_data_inputs_train, loan_data_inputs_test], axis = 0)\n# We concatenate the two dataframes along the rows.\nloan_data_inputs_pd.shape","2176f085":"loan_data_inputs_pd.head()","da4bf7a9":"loan_data_inputs_pd = loan_data_inputs_pd.set_index('Unnamed: 0')\n# We set the index of the dataframe to the values of a specific column. ","ef08ab08":"loan_data_inputs_pd.head()","26a9f80f":"features_all_pd = ['grade:A',\n'grade:B',\n'grade:C',\n'grade:D',\n'grade:E',\n'grade:F',\n'grade:G',\n'home_ownership:RENT_OTHER_NONE_ANY',\n'home_ownership:OWN',\n'home_ownership:MORTGAGE',\n'addr_state:ND_NE_IA_NV_FL_HI_AL',\n'addr_state:NM_VA',\n'addr_state:NY',\n'addr_state:OK_TN_MO_LA_MD_NC',\n'addr_state:CA',\n'addr_state:UT_KY_AZ_NJ',\n'addr_state:AR_MI_PA_OH_MN',\n'addr_state:RI_MA_DE_SD_IN',\n'addr_state:GA_WA_OR',\n'addr_state:WI_MT',\n'addr_state:TX',\n'addr_state:IL_CT',\n'addr_state:KS_SC_CO_VT_AK_MS',\n'addr_state:WV_NH_WY_DC_ME_ID',\n'verification_status:Not Verified',\n'verification_status:Source Verified',\n'verification_status:Verified',\n'purpose:educ__sm_b__wedd__ren_en__mov__house',\n'purpose:credit_card',\n'purpose:debt_consolidation',\n'purpose:oth__med__vacation',\n'purpose:major_purch__car__home_impr',\n'initial_list_status:f',\n'initial_list_status:w',\n'term:36',\n'term:60',\n'emp_length:0',\n'emp_length:1',\n'emp_length:2-4',\n'emp_length:5-6',\n'emp_length:7-9',\n'emp_length:10',\n'mths_since_issue_d:<38',\n'mths_since_issue_d:38-39',\n'mths_since_issue_d:40-41',\n'mths_since_issue_d:42-48',\n'mths_since_issue_d:49-52',\n'mths_since_issue_d:53-64',\n'mths_since_issue_d:65-84',\n'mths_since_issue_d:>84',\n'int_rate:<9.548',\n'int_rate:9.548-12.025',\n'int_rate:12.025-15.74',\n'int_rate:15.74-20.281',\n'int_rate:>20.281',\n'mths_since_earliest_cr_line:<140',\n'mths_since_earliest_cr_line:141-164',\n'mths_since_earliest_cr_line:165-247',\n'mths_since_earliest_cr_line:248-270',\n'mths_since_earliest_cr_line:271-352',\n'mths_since_earliest_cr_line:>352',\n'inq_last_6mths:0',\n'inq_last_6mths:1-2',\n'inq_last_6mths:3-6',\n'inq_last_6mths:>6',\n'acc_now_delinq:0',\n'acc_now_delinq:>=1',\n'annual_inc:<20K',\n'annual_inc:20K-30K',\n'annual_inc:30K-40K',\n'annual_inc:40K-50K',\n'annual_inc:50K-60K',\n'annual_inc:60K-70K',\n'annual_inc:70K-80K',\n'annual_inc:80K-90K',\n'annual_inc:90K-100K',\n'annual_inc:100K-120K',\n'annual_inc:120K-140K',\n'annual_inc:>140K',\n'dti:<=1.4',\n'dti:1.4-3.5',\n'dti:3.5-7.7',\n'dti:7.7-10.5',\n'dti:10.5-16.1',\n'dti:16.1-20.3',\n'dti:20.3-21.7',\n'dti:21.7-22.4',\n'dti:22.4-35',\n'dti:>35',\n'mths_since_last_delinq:Missing',\n'mths_since_last_delinq:0-3',\n'mths_since_last_delinq:4-30',\n'mths_since_last_delinq:31-56',\n'mths_since_last_delinq:>=57',\n'mths_since_last_record:Missing',\n'mths_since_last_record:0-2',\n'mths_since_last_record:3-20',\n'mths_since_last_record:21-31',\n'mths_since_last_record:32-80',\n'mths_since_last_record:81-86',\n'mths_since_last_record:>=86']\nref_categories_pd = ['grade:G',\n'home_ownership:RENT_OTHER_NONE_ANY',\n'addr_state:ND_NE_IA_NV_FL_HI_AL',\n'verification_status:Verified',\n'purpose:educ__sm_b__wedd__ren_en__mov__house',\n'initial_list_status:f',\n'term:60',\n'emp_length:0',\n'mths_since_issue_d:>84',\n'int_rate:>20.281',\n'mths_since_earliest_cr_line:<140',\n'inq_last_6mths:>6',\n'acc_now_delinq:0',\n'annual_inc:<20K',\n'dti:>35',\n'mths_since_last_delinq:0-3',\n'mths_since_last_record:0-2']","c434ffa2":"loan_data_inputs_pd_temp = loan_data_inputs_pd[features_all_pd]\n# Here we keep only the variables we need for the model.\nloan_data_inputs_pd_temp = loan_data_inputs_pd_temp.drop(ref_categories_pd, axis = 1)\n# Here we remove the dummy variable reference categories.\nloan_data_inputs_pd_temp.shape","b9521a2b":"reg_pd = pickle.load(open('pd_model.sav', 'rb'))\n# We import the PD model, stored in the 'pd_model.sav' file.","3698e9ad":"reg_pd.model.predict_proba(loan_data_inputs_pd_temp)[: ][: , 0]\n# We apply the PD model to caclulate estimated default probabilities.\nloan_data_inputs_pd['PD'] = reg_pd.model.predict_proba(loan_data_inputs_pd_temp)[: ][: , 0]\n# We apply the PD model to caclulate estimated default probabilities.\nloan_data_inputs_pd['PD'].head()","5f7f3701":"loan_data_inputs_pd['PD'].describe()\n# Shows some descriptive statisics for the values of a column.","c41a0319":"loan_data_preprocessed_new = pd.concat([loan_data_preprocessed, loan_data_inputs_pd], axis = 1)\n# We concatenate the dataframes where we calculated LGD and EAD and the dataframe where we calculated PD along the columns.\nloan_data_preprocessed_new.shape","a42e1da1":"loan_data_preprocessed_new.head()","e1d58d54":"loan_data_preprocessed_new['EL'] = loan_data_preprocessed_new['PD'] * loan_data_preprocessed_new['LGD'] * loan_data_preprocessed_new['EAD']\n# We calculate Expected Loss. EL = PD * LGD * EAD.\nloan_data_preprocessed_new['EL'].describe()\n# Shows some descriptive statisics for the values of a column.","01792457":"loan_data_preprocessed_new[['funded_amnt', 'PD', 'LGD', 'EAD', 'EL']].head()","3712e3d5":"loan_data_preprocessed_new['funded_amnt'].describe()","a801e379":"loan_data_preprocessed_new['EL'].sum()\n# Total Expected Loss for all loans.","a9f14a78":"loan_data_preprocessed_new['funded_amnt'].sum()\n# Total funded amount for all loans.","26ab5800":"loan_data_preprocessed_new['EL'].sum() \/ loan_data_preprocessed_new['funded_amnt'].sum()\n# Total Expected Loss as a proportion of total funded amount for all loans.\n####\n####\n####\n# THE END.","64d93095":"### Term","6638e9b5":"## Estimation and Interpretation","c38833d0":"## >>> The code up to here, from the other line starting with '>>>' is copied from the Data Preparation notebook, with minor adjustments.\n","963309d4":"# Explore Dependent Variables","e5760515":"#### Splitting Data","8f5732c7":"### Preprocessing the Test Dataset","498932ff":"### Time since the loan was funded","6c97c21f":"# Explore Data","f0d6b3ac":"## Estimating the \u0410ccuracy of the \u041codel","1c04200f":"# Population Stability Index: Preprocessing","4ef0ea21":"### General Preprocessing","0318dd0d":"## Data preparation: preprocessing discrete variables","7e5dda20":"# Independent Variables","78461966":"## Preparing the Inputs","0806ac40":"# Import Data","d58aae61":"### Data Preparation : An Example","074fabfc":"## Combining Stage 1 and Stage 2","a1b189e0":"# Import Data","e27e6177":"### Data Preparation: Continuous Variables, Part 1 and 2: Homework","43c26c5f":"### Dependent Variable. Good\/ Bad (Default) Definition. Default and Non-default Accounts.","c170c496":"# Preprocessing few discrete variables","672a6b3a":"## PD model: Data Preparation: Continuous Variables, Part 1","5099dfdd":"## Estimating the Model","68b026a1":"### Preprocessing Continuous Variables: Creating Dummy Variables, Part 1\n","85d16440":"## EAD Model","08e38251":"## Check and clean missing data","5d1744e5":"### PD model: Data preparation: Good\/ Bad (DV for the PD model)","017a48dc":"## Expected Loss","f94e4143":"### Preprocessing Continuous Variables: Creating Dummy Variables, Part 3: Homework","7996e35c":"## PD model: Data Preparation: Continuous Variables, Part 2","484d45b5":"# Splitting Data","c3bcc580":"## Model Validation","08b6f8bf":"## Data preparation: check for missing values and clean","89d775f6":"# PD Model","383fe3a5":"## PD model: Data Preparation: Discrete Variables","7fa54311":"## Saving the Model","b0d0840c":"## Preprocessing the test dataset","98743597":"# Dependent Variables","d44ed211":"### Preprocessing few continuous variables","5b79c434":"## PD model: Data Preparation: Continuous Variables, Part 1: Homework","38df48fa":"### Preprocessing Continuous Variables: Creating Dummy Variables, Part 2","ec03bb89":"###  The code from here to the other line starting with '>>>' is copied from the Data Preparation notebook, with minor adjustments. We have to perform the exact same data preprocessing, fine-classing, and coarse classing on the new data, in order to be able to calculate statistics for the exact same variables to the ones we used for training and testing the PD model.\n### Preprocessing few continuous variables","dd76cc28":"# LGD Model","686eea3e":"# Explore Data","06e18c89":"### Preprocessing Continuous Variables: Automating Calculations and Visualizing Results","04539108":"## Stage 2 \u2013 Linear Regression Evaluation","17c031d5":"## Data Preparation","0fc4c3b8":"## Stage 2 \u2013 Linear Regression","91ce899c":"## PD model: Data Preparation: Splitting Data","b574fba5":"# General Preprocessing","b31fa0e6":"#### We are going to preprocess the following discrete variables: grade, sub_grade, home_ownership, verification_status, loan_status, purpose, addr_state, initial_list_status. Most likely, we are not going to use sub_grade, as it overlaps with grade.","34c435ee":"### Earliest credit line","429d0eb5":"### Preprocessing Continuous Variables: Creating Dummy Variables, Part 3","9021e689":"## Testing the Model"}}