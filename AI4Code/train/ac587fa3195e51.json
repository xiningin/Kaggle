{"cell_type":{"4368a8b2":"code","9632d03c":"code","4acbb9df":"code","f603e247":"code","10a015d9":"code","137e1448":"code","5dcfce67":"code","cde7a649":"code","14f3a817":"code","dd40f50d":"code","02924f95":"code","b7310168":"code","d109de2b":"code","96f7662f":"code","d4ff35a9":"code","51bafe0c":"code","4a67a5de":"markdown","b8e54ce4":"markdown","c0ecdd57":"markdown","2d1035dd":"markdown","0f305200":"markdown","cf89ff21":"markdown","74631c4a":"markdown","40a61797":"markdown","7a0bfff9":"markdown","314b20c4":"markdown"},"source":{"4368a8b2":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n!pip uninstall -q typing --yes","9632d03c":"!pip install pytorch-lightning\n!pip install timm","4acbb9df":"import os\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport random\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nimport torch\n# from torchvision import models\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport albumentations as albu\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom sklearn.model_selection import StratifiedKFold\n# from efficientnet_pytorch import EfficientNet\nimport torch_xla.core.xla_model as xm\nimport torch_xla\nimport timm","f603e247":"import random\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything()","10a015d9":"TRAIN_CSV = \"..\/input\/cassava-leaf-disease-classification\/train.csv\"\nTRAIN_IMAGE_FOLDER = '..\/input\/cassava-leaf-disease-classification\/train_images'\nCLASSES = 5","137e1448":"FOLDS = 8\nBATCH_SIZE = 8\nLR = 0.01\nEPOCHS=2\nLOSS_FUNCTION = nn.BCEWithLogitsLoss()\nIMG_SIZE = 128\nEARLY_STOPPING = True\nMODEL_ARCH = 'resnet50'","5dcfce67":"class CassavaDataset(Dataset):\n    def __init__(self, train, train_mode=True, transforms=None):\n        self.train = train\n        self.transforms = transforms\n        self.train_mode = train_mode\n    \n    def __len__(self):\n        return self.train.shape[0]\n    \n    def __getitem__(self, index):\n        image_path = os.path.join(TRAIN_IMAGE_FOLDER, self.train.iloc[index].image_id)\n        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if (self.transforms):\n            image = self.transforms(image=image)[\"image\"]\n        \n        if not(self.train_mode):\n            return {\"x\":image}\n        \n        return {\n            \"x\": image,\n            \"y\": torch.tensor(self.train.iloc[index, self.train.columns.str.startswith('label')], dtype=torch.float64)\n        }","cde7a649":"def get_augmentations():\n    \n    mean = (0.485, 0.456, 0.406)\n    std = (0.229, 0.224, 0.225)    \n    \n    train_augmentations = albu.Compose([\n        albu.RandomResizedCrop(IMG_SIZE, IMG_SIZE, p=1.0),\n        albu.Transpose(p=0.5),\n        albu.HorizontalFlip(p=0.5),\n        albu.VerticalFlip(0.5),\n        albu.Normalize(mean, std, max_pixel_value=255, always_apply=True),        \n        ToTensorV2(p=1.0)\n    ], p=1.0)\n    \n    valid_augmentations = albu.Compose([\n        albu.Normalize(mean, std, max_pixel_value=255, always_apply=True),        \n        ToTensorV2(p=1.0)\n    ], p=1.0)   \n    \n    return train_augmentations, valid_augmentations\n\ntrain_augs, val_augs = get_augmentations()","14f3a817":"# # These are the available model architectures in timm\n# from pprint import pprint\n# model_names = timm.list_models(pretrained=True)\n# pprint(model_names)","dd40f50d":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = timm.create_model(MODEL_ARCH, pretrained=True)\n#         self.model = base_model\n\n#         # Efficientnets\n#         n_features = self.model.classifier.in_features\n#         self.model.classifier = nn.Linear(n_features, CLASSES)\n        \n        # Resnets\n        n_features = self.model.fc.in_features\n        self.model.fc = nn.Linear(n_features, CLASSES)\n        \n    def forward(self, x):\n        x = self.model(x)\n        return x","02924f95":"traincsv = pd.read_csv(TRAIN_CSV)\ntraincsv['kfold'] = -1\ntraincsv = traincsv.sample(frac=1).reset_index(drop=True)\nstratifier = StratifiedKFold(n_splits=FOLDS)\n\nfor fold, (train_index, val_index) in enumerate(stratifier.split(X=traincsv.image_id.values, y=traincsv.label.values)):\n    traincsv.loc[val_index, \"kfold\"] = fold\n\ntraincsv.to_csv(\"train_folds.csv\", index=False)","b7310168":"class CassavaDataModule(pl.LightningDataModule):\n    def __init__(self, fold):\n        super().__init__()\n        self.train_aug, self.valid_aug = get_augmentations()\n        self.fold = fold\n        self.batch_size = BATCH_SIZE\n    \n    def setup(self, stage=None):\n        folds = pd.read_csv('.\/train_folds.csv')\n        folds = pd.get_dummies(folds, columns=['label'])\n        train_fold = folds.loc[folds[\"kfold\"] != self.fold]\n        val_fold = folds.loc[folds[\"kfold\"] == self.fold]\n        \n        self.train_ds = CassavaDataset(train_fold, transforms=train_augs)\n        self.val_ds = CassavaDataset(val_fold, transforms=val_augs)\n        \n    def train_dataloader(self):\n        return DataLoader(self.train_ds, self.batch_size, num_workers=4, shuffle=True)\n        \n    def val_dataloader(self):\n        return DataLoader(self.val_ds, self.batch_size, num_workers=4, shuffle=False)        \n        ","d109de2b":"early_stopping = EarlyStopping('val_accuracy', patience=3, mode='max')\n\ncallbacks=[]\n\nif EARLY_STOPPING == True:\n    callbacks.append(early_stopping)","96f7662f":"class CassavaPLModule(pl.LightningModule):\n    def __init__(self, hparams, model):\n        super(CassavaPLModule, self).__init__()\n        self.hparams = hparams\n        self.model = model\n        self.criterion = LOSS_FUNCTION\n        self.accuracy = pl.metrics.Accuracy()\n        \n    def forward(self, x):\n        return self.model(x)\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=self.hparams.lr)\n        scheduler = {\n            'scheduler': \n                torch.optim.lr_scheduler.ReduceLROnPlateau(\n                    optimizer, patience=3,\n                    threshold=0.001,\n                    mode='min', verbose=True\n                ),\n            'interval': 'epoch',\n            'monitor' : 'val_loss'\n        }\n        return [optimizer], [scheduler]\n        \n    def training_step(self, batch, batch_index):\n        # One batch at a time\n        features = batch['x']\n        targets = batch['y']\n        out = self(features)\n        loss = self.criterion(out, targets)\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)      \n        metric_acc = self.accuracy(out, targets)\n        self.log(\"train_accuracy\", metric_acc, on_step=True, on_epoch=True, prog_bar=True,logger=True)\n        \n    def validation_step(self, batch, batch_index):\n        # One batch at a time\n        features = batch['x']\n        targets = batch['y']\n        out = self(features)\n        loss = self.criterion(out, targets)\n        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True) \n        metric_acc = self.accuracy(out, targets)\n        self.log(\"val_accuracy\", metric_acc, on_step=True, on_epoch=True, prog_bar=True, logger=True)        ","d4ff35a9":"def train(fold):   \n    checkpoint_callback = ModelCheckpoint(\n        dirpath='checkpoints\/',\n        filename='model_{fold}-{val_loss:.2f}',\n        monitor='val_loss', verbose=True,\n        save_last=False, save_top_k=1, save_weights_only=False,\n        mode='min', period=1, prefix=''\n    )        \n    \n    tpu_core = fold + 1\n    \n    trainer = pl.Trainer(\n#                         gpus=-1 if torch.cuda.is_available() else None, \n                        tpu_cores=[tpu_core],\n#                         precision=16 if torch.cuda.is_available() else 32,\n                        precision=16,\n#                         plugins='ddp_sharded',\n                        max_epochs=EPOCHS,\n                        checkpoint_callback=checkpoint_callback,\n                        callbacks=callbacks)\n    model = Model()\n    pl_dm = CassavaDataModule(fold=fold)\n    pl_module = CassavaPLModule(hparams={'lr':LR, 'batch_size':BATCH_SIZE}, model=model)\n    \n    trainer.use_native_amp = False\n    trainer.fit(pl_module, pl_dm)\n    \n    print(checkpoint_callback.best_model_path, checkpoint_callback.best_model_score)\n    ","51bafe0c":"import joblib as jl\nparallel = jl.Parallel(n_jobs=FOLDS, backend='threading', batch_size=1)\nparallel(jl.delayed(train)(i) for i in range(FOLDS))","4a67a5de":"### Training","b8e54ce4":"### Hyper parameters","c0ecdd57":"### K-Fold CV","2d1035dd":"### Dataset","0f305200":"Here is an attempt to do KFold, Parallel training. However training happens only on one core at a time. ","cf89ff21":"### PL Module","74631c4a":"### PL Data module","40a61797":"### Transforms","7a0bfff9":"### Callbacks","314b20c4":"### NN Model"}}