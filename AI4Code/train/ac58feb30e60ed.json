{"cell_type":{"7e28ce95":"code","067e2270":"code","ea0a84d2":"code","33f1351a":"code","56a0d9a3":"code","27369fc8":"code","9b9655a2":"code","6c85e07d":"code","ba5f7436":"code","aa62ab4a":"code","60924f75":"code","1f6d00cb":"code","e9f57539":"code","80e26a43":"code","2f8f1335":"code","4ad9c5a2":"code","51f9a422":"code","9d6050cd":"code","cd4c7b5e":"code","62cfa422":"code","a8d6349e":"code","7734113b":"code","e7308ead":"code","05c95bcc":"code","4e692984":"code","af5380e7":"code","1c53c47d":"code","060ebc2f":"code","4e438e03":"code","f364a421":"code","0b064e10":"code","a97b6d69":"code","760bc2d9":"code","133c1893":"code","034cf1de":"code","86f4ca37":"code","421990a2":"code","e11d9bae":"code","2f8ebbed":"code","83cb8812":"code","de7ac316":"code","83e4cb51":"code","81c165f2":"code","55c9f02c":"code","6a8274ad":"code","8e460720":"code","aa3693b9":"code","aa72126e":"code","6dadce7b":"code","8bb9129d":"code","d49e7b66":"code","bdf248c8":"code","17dae622":"code","6f5000fe":"code","74c3d4b2":"code","7201509b":"code","dd29f572":"code","864baf6a":"code","92bcceb1":"code","a66d9386":"code","1bafda65":"code","01147e34":"code","75af953b":"code","547cfdc4":"code","e678260e":"code","b2674ca0":"code","05684e94":"code","398d5a6b":"code","3b04988c":"code","56f1fed4":"code","299e1d6b":"code","f3bc3ab9":"code","0bfe4951":"code","80714e76":"code","5ee7e0b5":"code","503d730c":"code","9990879c":"code","5d39d6b9":"code","68e2124b":"code","1eae75fc":"code","f6c9765a":"code","c26a1112":"code","606afa9c":"code","7c42c0ae":"code","274c4832":"code","cf3ab6f1":"code","87ca2cee":"code","c0d4c1b1":"code","f38619e6":"code","ed127b48":"code","adcd6e29":"code","26b72261":"code","6fd8917e":"code","5935c3ab":"code","65c09871":"code","cef35500":"code","7b0f4231":"code","1c1e1e5c":"code","123e2db0":"code","1873f46a":"code","75272da2":"code","973c2198":"code","09d41a52":"code","7e95ae5b":"code","53feeec2":"code","c8d335df":"code","070bcc45":"code","cb42876b":"code","98e52fc2":"code","41502733":"code","8308c8a5":"code","452768dc":"code","76fbfff0":"code","a9912a03":"code","df44c392":"code","dfed29c3":"code","042c3ab9":"code","af87e3a8":"code","14e285b5":"code","afb49b7c":"code","50086081":"code","5320ef94":"code","78b4945d":"code","5e2f17bd":"code","15cb6938":"code","92be2172":"code","dc3c56e0":"markdown","005d6a1f":"markdown","143b6d81":"markdown","57004576":"markdown","2629c881":"markdown","0beda3b1":"markdown","b7d3e394":"markdown","099516e8":"markdown","62d7cdcf":"markdown","2dcecd7e":"markdown","fa5103b7":"markdown","cfc9cd98":"markdown","5fea1fbe":"markdown","6315f4c4":"markdown","0d652d0b":"markdown","ea33dfd4":"markdown","e55ac695":"markdown","0baed7e0":"markdown","325fef84":"markdown","09d34b10":"markdown","cbbb59e8":"markdown","45c0c6a9":"markdown","ec03fa73":"markdown","e2caacb7":"markdown","07f92948":"markdown","9cfb0dbe":"markdown","74f0f2bf":"markdown","aca17a84":"markdown","d434c176":"markdown","85d54964":"markdown","71abe041":"markdown","5768df67":"markdown","42181f86":"markdown","09772b07":"markdown","dcd88e93":"markdown","de06f5aa":"markdown","cc006833":"markdown","ac83365d":"markdown","4d1bf931":"markdown","a62bd550":"markdown","a02b56b3":"markdown","480f6bc0":"markdown","0cd611ad":"markdown","afe946a2":"markdown","47121253":"markdown","bce59178":"markdown","753cfdf6":"markdown","bcb74d66":"markdown","cce0f413":"markdown","19281635":"markdown","35ed8b5d":"markdown","932ffcf3":"markdown","4a030de9":"markdown","afac1e1c":"markdown","e474f3ca":"markdown","366c7484":"markdown","15ca6206":"markdown","1c4f924e":"markdown","54675694":"markdown","f60bf5d9":"markdown","272e3970":"markdown","e6eafaac":"markdown"},"source":{"7e28ce95":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import confusion_matrix\nimport sklearn.metrics as metrics\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder\nfrom scipy.stats import chi2_contingency, boxcox\nimport scipy.stats as stats\nimport sklearn.preprocessing as preprocessing\n","067e2270":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","ea0a84d2":"train.head()","33f1351a":"train.shape","56a0d9a3":"test.shape","27369fc8":"train.info()","9b9655a2":"train.isna().sum()","6c85e07d":"test.isna().sum()","ba5f7436":"train.loc[train.Embarked.isna()]","aa62ab4a":"train.loc[train.Embarked.isna(),'Embarked'] = 'S'","60924f75":"train.loc[train.Cabin == 'B28']","1f6d00cb":"train_Cabin = train.loc[train.Cabin.notna()].copy()","e9f57539":"train_Cabin['cabinName'] = train_Cabin.Cabin.apply(lambda x: str(x)[0])","80e26a43":"train_Cabin.head()","2f8f1335":"stat, p, dof, expected = chi2_contingency(pd.crosstab(train_Cabin['Survived'],train_Cabin['cabinName']))","4ad9c5a2":"alpha = 0.05\nif(p<alpha):\n    print('p = ',p)\n    print('Dependent (reject H0)')\nelse:\n    print('p = ',p)\n    print('Independent (fail to reject H0)')","51f9a422":"stat, p, dof, expected = chi2_contingency(pd.crosstab(train_Cabin['Pclass'],train_Cabin['cabinName']))","9d6050cd":"alpha = 0.05\nif(p<alpha):\n    print('p = ',p)\n    print('Dependent (reject H0)')\nelse:\n    print('p = ',p)\n    print('Independent (fail to reject H0)')","cd4c7b5e":"train.drop(columns='Cabin', inplace = True)\ntest.drop(columns='Cabin', inplace = True)","62cfa422":"train.head()","a8d6349e":"train['Titles'] = train.Name.apply(lambda x: x.split(', ')[1].split('. ')[0])","7734113b":"train.Titles.value_counts()","e7308ead":"test['Titles'] = test.Name.apply(lambda x: x.split(', ')[1].split('. ')[0])","05c95bcc":"test.Titles.value_counts()","4e692984":"new_titles = {\n    'Mr':'Mr',\n    'Miss':'Miss',\n    'Mrs':'Mrs',\n    'Master':'Master',\n    'Dr':'Officer',\n    'Dona':'Royalty',\n    'Rev':'Officer',\n    'Mlle':'Miss',\n    'Major':'Officer',\n    'Col':'Officer',\n    'Sir':'Royalty',\n    'Don':'Royalty',\n    'Capt':'Officer',\n    'the Countess':'Royalty',\n    'Jonkheer':'Officer',\n    'Mme':'Mrs',\n    'Lady':'Royalty',\n    'Ms':'Miss'\n}","af5380e7":"train['Titles'] = train.Titles.map(new_titles)\ntest['Titles'] = test.Titles.map(new_titles)","1c53c47d":"train.Titles.value_counts()","060ebc2f":"test.Titles.value_counts()","4e438e03":"train['Age'] = train.groupby(['Titles','Pclass'])['Age'].transform(lambda grp: grp.fillna(np.mean(grp)))","f364a421":"train['Age'].isna().sum()","0b064e10":"test['Age'] = test.groupby(['Titles','Pclass'])['Age'].transform(lambda grp: grp.fillna(np.mean(grp)))","a97b6d69":"test['Age'].isna().sum()","760bc2d9":"test.isna().sum()","133c1893":"test.loc[test.Fare.isna()]","034cf1de":"test['Fare'] = test.groupby(['Pclass'])['Fare'].transform(lambda grp: grp.fillna(np.mean(grp)))","86f4ca37":"test.isna().sum()","421990a2":"train.isna().sum()","e11d9bae":"train.head()","2f8ebbed":"similarTicket = train.Ticket.value_counts()","83cb8812":"train['ticketMate'] = train.Ticket.apply(lambda x: 1 if similarTicket[x] > 1 else 0)","de7ac316":"stat, p, dof, expected = chi2_contingency(pd.crosstab(train['Survived'],train['ticketMate']))\nalpha = 0.05\nif(p<alpha):\n    print('p = ',p)\n    print('Dependent (reject H0)')\nelse:\n    print('p = ',p)\n    print('Independent (fail to reject H0)')","83e4cb51":"stat, p, dof, expected = chi2_contingency(pd.crosstab(train['Pclass'],train['ticketMate']))\nalpha = 0.05\nif(p<alpha):\n    print('p = ',p)\n    print('Dependent (reject H0)')\nelse:\n    print('p = ',p)\n    print('Independent (fail to reject H0)')","81c165f2":"train.drop(columns=['Ticket','ticketMate'], inplace = True)","55c9f02c":"test.drop(columns=['Ticket'], inplace = True)","6a8274ad":"num_cols = train.describe().columns","8e460720":"cat_cols = train.columns.drop(num_cols)","aa3693b9":"num_cols","aa72126e":"cat_cols","6dadce7b":"num_cols = num_cols.drop(['PassengerId','Survived'])","8bb9129d":"for col in num_cols:\n    sns.histplot(data = train, x = col, kde = True)\n    plt.show()","d49e7b66":"for col in num_cols:\n    if ((col!= 'Age') & (col != 'Fare')):\n        train[col] = train[col].astype(object)","bdf248c8":"train.info()","17dae622":"for col in num_cols:\n    if ((col!= 'Age') & (col != 'Fare')):\n        train[col] = train[col].astype(object)","6f5000fe":"test.info()","74c3d4b2":"num_cols = train.describe().columns\ncat_cols = train.columns.drop(num_cols)\nnum_cols = num_cols.drop(['PassengerId','Survived'])","7201509b":"num_cols","dd29f572":"cat_cols","864baf6a":"cat_cols = cat_cols.drop('Name')","92bcceb1":"for col in cat_cols:\n    sns.countplot(data=train, x = col)\n    plt.show()","a66d9386":"sns.countplot(data = train, x='Survived')","1bafda65":"for col in num_cols:\n    sns.boxplot(data = train, x='Survived', y = col)\n    plt.show()","01147e34":"for col in cat_cols:\n    sns.countplot(data=train, x = col, hue = 'Survived')\n    plt.show()","75af953b":"train['Family'] = (train['SibSp'].astype(int)+train['Parch'].astype(int)).astype(str)","547cfdc4":"def chi2(col1, col2):\n    stat, p, dof, expected = chi2_contingency(pd.crosstab(train[col1],train[col2]))\n    alpha = 0.05\n    if(p<alpha):\n        print('p = ',p)\n        print('Dependent (reject H0)')\n    else:\n        print('p = ',p)\n        print('Independent (fail to reject H0)')","e678260e":"print('Chi-Square test for SibSp')\nchi2('SibSp','Survived')\nprint('*'*30)\nprint('Chi-Square test for Parch')\nchi2('Parch','Survived')\nprint('*'*30)\nprint('Chi-Square test for family')\nchi2('Family','Survived')\nprint('*'*30)","b2674ca0":"test['Family'] = (test['SibSp'].astype(int)+test['Parch'].astype(int)).astype(str)","05684e94":"train.drop(columns=['SibSp','Parch'], inplace= True)\ntest.drop(columns=['SibSp','Parch'], inplace= True)","398d5a6b":"chi2('Sex','Titles')","3b04988c":"train.drop(columns='Sex', inplace= True)\ntest.drop(columns='Sex', inplace = True)","56f1fed4":"stats.probplot(train['Fare'], dist='norm', plot=plt)[-1]","299e1d6b":"train['Fare'], fitted_lambda = boxcox(train['Fare']+1)","f3bc3ab9":"stats.probplot(train['Fare'], dist='norm', plot=plt)[-1]","0bfe4951":"test['Fare'], fitted_lambda = boxcox(test['Fare']+1)","80714e76":"stats.probplot(train['Age'], dist='norm', plot=plt)[-1]","5ee7e0b5":"stats.probplot(boxcox(train['Age'])[0], dist='norm', plot=plt)[-1]","503d730c":"trainReg = train.copy()\ntestReg = test.copy()\ndf = trainReg.append(testReg)\ndf.head()","9990879c":"df.drop(columns=['PassengerId','Name'], inplace=True)","5d39d6b9":"df = pd.get_dummies(df)","68e2124b":"df.shape","1eae75fc":"trainReg_dummies = df.iloc[:len(trainReg)]\ntestReg_dummies = df.iloc[len(trainReg):]","f6c9765a":"X_train = trainReg_dummies.drop(columns='Survived')\ny_train = trainReg_dummies['Survived'].astype(int)\nX_test = testReg_dummies.drop(columns='Survived')","c26a1112":"scaler = StandardScaler()","606afa9c":"X_train_scaled = scaler.fit_transform(X_train)","7c42c0ae":"scaler = StandardScaler()\nX_test_scaled = scaler.fit_transform(X_test)","274c4832":"X_test_scaled.shape","cf3ab6f1":"X_train_scaled.shape","87ca2cee":"logreg = LogisticRegression()\nlogreg.fit(X_train_scaled,y_train)\ny_pred = logreg.predict(X_test_scaled)","c0d4c1b1":"submission = pd.DataFrame()","f38619e6":"submission['PassengerId'] = np.arange(len(trainReg)+1, len(df)+1)","ed127b48":"submission['Survived'] = y_pred","adcd6e29":"submission.shape","26b72261":"submission.to_csv('18J.csv')","6fd8917e":"submission.head()","5935c3ab":"def confusion_matrix_scorer(logreg, X,y):\n    y_pred = logreg.predict(X)\n    cm = confusion_matrix(y, y_pred)\n    return {'tn': cm[0, 0], 'fp': cm[0, 1],\n            'fn': cm[1, 0], 'tp': cm[1, 1]}","65c09871":"logreg.coef_","cef35500":"score = cross_validate(logreg, X_train_scaled, y_train,scoring=['accuracy','precision','recall'],cv=5)","7b0f4231":"score","1c1e1e5c":"score = cross_validate(logreg, X_train_scaled, y_train,scoring=confusion_matrix_scorer,cv=5)","123e2db0":"score","1873f46a":"df = trainReg.append(testReg)","75272da2":"df.info()","973c2198":"def enc(df):\n    lb = LabelEncoder()\n    return (lb.fit_transform(df))","09d41a52":"df.drop(columns=['PassengerId','Name'], inplace = True)","7e95ae5b":"lb = LabelEncoder()","53feeec2":"df['Pclass'] = lb.fit_transform(df['Pclass'])","c8d335df":"df['Pclass'].value_counts()","070bcc45":"df['Embarked'] = lb.fit_transform(df['Embarked'])","cb42876b":"df['Embarked'].value_counts()","98e52fc2":"df['Titles'] = lb.fit_transform(df['Titles'])","41502733":"df['Titles'].value_counts()","8308c8a5":"df['Family'] = lb.fit_transform(df['Family'])","452768dc":"df['Family'].value_counts()","76fbfff0":"trainReg_dummies = df.iloc[:len(trainReg)]\ntestReg_dummies = df.iloc[len(trainReg):]\nX_train = trainReg_dummies.drop(columns='Survived')\ny_train = trainReg_dummies['Survived'].astype(int)\nX_test = testReg_dummies.drop(columns='Survived')\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nscaler = StandardScaler()\nX_test_scaled = scaler.fit_transform(X_test)","a9912a03":"X_train_scaled.shape","df44c392":"X_test_scaled.shape","dfed29c3":"logreg = LogisticRegression()\nscore = cross_validate(logreg, X_train_scaled, y_train,scoring=['accuracy','precision','recall'],cv=5)\nscore","042c3ab9":"df = trainReg.append(testReg)\ndf.drop(columns=['PassengerId','Name'], inplace=True)\ndf = pd.get_dummies(df)\ntrainReg_dummies = df.iloc[:len(trainReg)]\ntestReg_dummies = df.iloc[len(trainReg):]\nX_train = trainReg_dummies.drop(columns='Survived')\ny_train = trainReg_dummies['Survived'].astype(int)\nX_test = testReg_dummies.drop(columns='Survived')\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nscaler = StandardScaler()\nX_test_scaled = scaler.fit_transform(X_test)","af87e3a8":"param_grid = [\n    {\n    'penalty' : ['l1'],\n    'C' : np.logspace(-4,4,20),\n    'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n    'max_iter' : [100,500,1000,5000,10000],\n}\n]","14e285b5":"logreg = LogisticRegression()\nclf = GridSearchCV(logreg, param_grid=param_grid, cv=5, verbose=True, n_jobs=-1)","afb49b7c":"best_clf = clf.fit(X_train_scaled,y_train)","50086081":"best_clf.best_estimator_","5320ef94":"best_clf.score(X_train_scaled, y_train)","78b4945d":"y_pred = best_clf.predict(X_test_scaled)","5e2f17bd":"submission = pd.DataFrame()","15cb6938":"submission['PassengerId'] = np.arange(len(trainReg)+1, len(df)+1)\nsubmission['Survived'] = y_pred\nsubmission.shape","92be2172":"submission.to_csv('18J2.csv',index = False)","dc3c56e0":"<h3> Scaling the data using Standard Scaler","005d6a1f":"<p> Moreover we can proved that Cabin column and Pclass column are correlated. Since, the Cabin's in the titanic were given according to the Pclass. <\/p>","143b6d81":"<p> If more than one person has the same ticket we will assign 1 to ticketMate column of the passenger or else 0<\/p>","57004576":"<h4> Test Dataset <\/h4>","2629c881":"<p> We can see that there is a range of titles, our next step would be to bin these tiles into <b>Mr,Mrs,Miss,Master,Royalty and Officer<\/b><\/p>\n","0beda3b1":"<h2> Data Preprocessing <\/h2>","b7d3e394":"<h4>After applying boxcox transformation<\/h4>","099516e8":"<p> The only missing value in test dataset is in Fare column. Let's check the entry<\/p>","62d7cdcf":"<h3> Embarked Column <\/h3>\n<p>We will work on Embarked column which has two missing values<\/p>\n<p>Let's check the record who has missing values in Embarked column<\/p>","2dcecd7e":"<p> We have two numerical columns namely <b>Fare and Age<\/b> which need to be tranformed into a normal distribution<\/p>","fa5103b7":"<p> We can impute the Fare value according to the mean of Pclass <\/p> ","cfc9cd98":"<h3> Update the list of Categorical and Numerical columns to fit the defintion <\/h3>","5fea1fbe":"<h3> Checking correlation of Sex column and Titles column<\/h3>\n<p> If they are highly correlated we will have to drop the sex column <\/p>","6315f4c4":"<h2> Result of Encoders <\/h2>","0d652d0b":"<h4> Dealing with train and test dataset missing values<\/h4>\n<p> In train dataset, there are 177 missing values in Age columns, 687 missing values in Cabin column and 2 missing values in Embarked column<\/p>\n<p> In test dataset there are 86 missing values in Age column, 327 missing values in Cabin column and 1 missing value in Fare Column","ea33dfd4":"<h3> Fitting the Train Dataset to Logistic Regression model without hypertuning te parameters<\/h3>","e55ac695":"<h2> Histograms of Numerical Columns <\/h2>","0baed7e0":"<p> We will be dropping the Cabin column<\/p>","325fef84":"<h2> Outliers in numerical columns <\/h2>\n<p> It can be seen that Age and Fare column are <b>not normally distributed<\/b> and from the histograms and kde plot it can be seen that they are <b>right skewed<\/b>. We will handle this in our next step which is Data Preprocessing<\/p>","09d34b10":"<p> It is clear that One-Hot Encoding gives better results than Label Encoding. This is because the categorical variable cannot be considered to be ordinal variables. The realtionship between the elements is not linear. <\/p>","cbbb59e8":"Setting alpha to 0.05","45c0c6a9":"Let's set our alpha to 0.05, to check the statistical significance","ec03fa73":"<p> We will remove columns like <b>PassengerId and Name<\/b> Since they will not be used<\/p>","e2caacb7":"<p>We can see that boxcox transformation on Age column does not bring about much change. There is a visible trade-off between the higher values and lower values before and after the transformation. Hence we will not apply boxcox tranformation on the Age column<\/p>","07f92948":"<h4> Dependence on Pclass column <\/h4>","9cfb0dbe":"<h4> Before applying the boxcox plot <\/h4>","74f0f2bf":"<p> Now let's check the statistical significance of the new column with Survived column and Pclass column<\/p>","aca17a84":"<h3>Train dataset<\/h3>","d434c176":"<h3> Green Signal on creating the family column <\/h3>\n<p> We can see that family column has <i>greater statistical significance<\/i> than both of the columns combined. Hence we will be <b>droping SibSp and Parch<\/b> and <b>adding Family column<\/b><\/p>","85d54964":"<p> Before that let's visualize categorical columns by using Survived as hue. <p>","71abe041":"<h2> Filtering numerical columns <\/h2>\n<p> We can see that only <b>Age<\/b> and <b>Fare<\/b> fit the definition of numerical columns. We cant prove that other ordinal variables have a linear relationship. Hence we will conver columns other than Age and Fare into categorical columns<\/p>\n","5768df67":"<h3> Dealing with Ticket column <\/h3>\n<p> Ticket column has ticket numbers and the only possible information that we can get from it is the people who has the same ticket number may have had high chance of survival together. It can be shown that the first two digits of Ticket number are related to Pclass. Before moving ahead we need to check the correlation of the column we are going to create with other columns(Pclass).<\/p>","42181f86":"<h3> Age column <\/h3>","09772b07":"<h1> Data Visualization <\/h1>","dcd88e93":"<p> Both of them have the same Ticket number, that proves that they have embarked from the same city<\/p>\n<p> Mrs Stone boarded the Titanic in <i>Southampton<\/i> on 10 April 1912 and was travelling in first class with her maid Amelie Icard. She occupied cabin B-28. You can read about her \n    <a href='https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/martha-evelyn-stone.html'>here<\/a>","de06f5aa":"<h4> Dependence on Survived column<\/h4>","cc006833":"<h2> Feature Engineering <\/h2>\n<p> There is one possibility of feature engineering that we are going to explore. We will be combining SibSp and Parch column to create family column. We will then test the statistical significance of the new column and compare it to the original columns.<\/p>\n<p> One issue which we have not resolved is the high correlation of Sex and Titles column. Since Titles contains the information already in Sex column, we need to drop sec column.<\/p>","ac83365d":"<h2> Using Label Encoder <\/h2>","4d1bf931":"<h3>Check For missing values<\/h3>","a62bd550":"<p> We will use Label Encoder here and check our results<\/p>","a02b56b3":"<h2> Categorical columns visualization with Survived<\/h2>","480f6bc0":"<h1>Titanic Survival Prediction<\/h1>\n<h3>Description<\/h3>\n<p>Titanic Dataset on Kaggle comes in two parts, train and test dataset. There are eleven predictor features including 'PassengerId'. The target feature is 'Survived', which can take two values 0 and 1. Each row is a person who boarded the Titanic. Given the predictor variables we have to find if the passenger in the test dataset survived. Our plan would be to fit a <b>Classification algorithm<\/b> on the train dataset and predict the target variable in test dataset.<\/p>\n","0cd611ad":"<h2> Boxplot of Numerical columns with Survived column <\/h2>","afe946a2":"<h4>After applying box plot<\/h4>","47121253":"<p> Should do the same for test dataset <\/p>","bce59178":"<h1> Using GridSearchCV to optimize the Hyperparameters <\/h1>","753cfdf6":"<p> We will do the data visualization of Categorical and Numerical columns separately<\/p>\n<p> So lets make the list of the name of numerical columns and categorical columns<\/p>","bcb74d66":"<h2> Create dummy variables using One-Hot Encoding <\/h2>","cce0f413":"<h4> Creating Title column <\/h4>","19281635":"<h3> Fare column <\/h3>","35ed8b5d":"<h2> Countplot of Catogorical columns <\/h2>","932ffcf3":"<h2> Preparing the data learning algorithm<\/h2>","4a030de9":"<p> Let's use a groupby function to impute missing values in the Age column <\/p>","afac1e1c":"<h3>Age Column<\/h3>\n<p> Now we will work on Age column which has 177 missing values out of 891 values in the train dataset<\/p>\n<p> One possible way to impute the age column according to impute the mean of age for each age groups and profession and Pclass<\/p>\n<p> To segregate the data according to age groups and profession we will the need the title given to the passengers.<\/p>\n<p> This information is available in the Name column where each passenger has a title after their surname<\/p>\n<p> The steps would to make a new column Title and impute the missing values based on Title column and Pclass<\/p>","e474f3ca":"<h3> Creating Family column <\/h3>","366c7484":"<p> One possible way to impute the missing values in Cabin column would be by assignment according to the Pclass, but that would lead to high correlation between Pclass and \n    Cabin column. Hence it is safe to say that cabin column has no statistical significance.<\/p>","15ca6206":"<p> All the missing values have been dealt with and also some of the columns which had a lot of missing values and did not show any statistical significance have been removed <\/p>","1c4f924e":"<h4> Before applying boxcox tranformation<\/h4>","54675694":"<p> The probability plot tells us how the given distribution related to the normal distribution on the given quantiles<\/p>","f60bf5d9":"<p> As we can see, both the columns ar highly correlated. Hence we will be dropping Sex column from the train and test dataset.<\/p>","272e3970":"<p> It can be seen that although Ticket mate column is statistically significant, it is highly correlated to the Pclass colomn. Hence we will not use it <\/p>","e6eafaac":"<h3>Cabin Column<\/h3>\n<p> We will work on Cabin column which has 687 missing values out of 891 values<p>\n<p> Since there are a large number of missing values and no possible way to impute the values through feature engineering. Let's check if the <i>Cabin<\/i> column is <b>dependent column or independent column<\/b> with the <i>Survived<\/i> column. We will use <b>Chi-Squared Test<\/b> to check the statistical significance.<\/p>\n<p> There are two components to the Cabin column. It contain the alphabet which denotes the cabin section and the number denotes the position of the room<\/p>\n<p> We will make a new dataset which contain only those records which have non-null cabin values. Then we will extract the letters from the cabin column and make a new column Cabin Name<\/p>"}}