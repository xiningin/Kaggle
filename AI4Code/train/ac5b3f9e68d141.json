{"cell_type":{"c660996c":"code","015145c4":"code","40e41549":"code","6e98ae19":"code","8c35dc23":"code","c09d9e4f":"code","dae421c5":"code","3b254759":"code","60fca90a":"code","3d1bad55":"code","a0496030":"code","017e0e7f":"code","ee87df16":"code","e86844a1":"code","5950acae":"code","6ff152d0":"code","513956d1":"code","10b1970e":"code","61bcb2f7":"code","bcd60299":"code","e716a202":"markdown","1ca2937a":"markdown","c1bddf97":"markdown","44327910":"markdown","48968bc9":"markdown","c062bc09":"markdown","e479871b":"markdown","0685b7ea":"markdown","9f601502":"markdown","c2fb4fd9":"markdown","e839f824":"markdown"},"source":{"c660996c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import *\nfrom copy import deepcopy","015145c4":"train = pd.read_csv(\"..\/input\/cat-in-the-dat-ii\/train.csv\")\ntest = pd.read_csv(\"..\/input\/cat-in-the-dat-ii\/test.csv\")","40e41549":"print(\"Training set size: (%d, %d)\" %(train.shape[0], train.shape[1]))\nprint(\"Test set size: (%d, %d)\" %(test.shape[0], test.shape[1]))","6e98ae19":"# Visualisation options\npd.set_option('display.max_columns', None)\ntrain.head()","8c35dc23":"summary = pd.DataFrame()\nsummary['Name'] = train.columns\nsummary['Type'] = train.dtypes.values\nsummary['Missing'] = train.isna().sum().values    \nsummary['Uniques'] = train.nunique().values\nprint(summary.to_string(index=False))","c09d9e4f":"full_summary = pd.DataFrame(columns=[\"Train categories\", \"Test categories\", \"Not in test\", \"Not in train\"],\n                            index=train.drop(columns=['id','target']).columns)\nfor i, column in enumerate(train.drop(columns=['id','target'])):\n    full_summary.iloc[i, 0] = len(train[column].value_counts())\n    full_summary.iloc[i, 1] = len(test[column].value_counts())\n    train_categories = set(train[column].value_counts().keys())\n    test_categories = set(test[column].value_counts().keys())\n    full_summary.iloc[i, 2] = len(train_categories - test_categories)\n    full_summary.iloc[i, 3] = len(test_categories - train_categories)\n\nprint(full_summary)","dae421c5":"# Get target labels and test id\ntrain_labels = train.target\ntest_id = test.id\n\n# ID column and target not necessary\ntrain = train.drop(columns=['id', 'target'])\ntest = test.drop(columns=['id'])","3b254759":"fullset = pd.concat([train, test])\nfor column in fullset.columns:\n    fullset[column] = fullset[column].fillna(\"NULL\").astype(str)","60fca90a":"ohe_encoder = preprocessing.OneHotEncoder(dtype=np.int8).fit(fullset)\nfullset = ohe_encoder.transform(fullset)","3d1bad55":"train = fullset[:train.shape[0]]\ntest = fullset[train.shape[0]:]","a0496030":"var_filter = feature_selection.VarianceThreshold(threshold=0).fit(train)\ntrain = var_filter.transform(train)\ntest = var_filter.transform(test)","017e0e7f":"# Univariate features filter\nunivariate_selection = feature_selection.SelectKBest(k='all').fit(train, train_labels)","ee87df16":"plt.plot(np.cumsum(sorted(univariate_selection.scores_ \/ sum(univariate_selection.scores_), reverse=True)))\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"% of k-score obtained\")\nplt.title(\"Number of features (sorted by k-score) vs % of k-score explained\")\nplt.show()","e86844a1":"kscore_filter = feature_selection.SelectKBest(k=1900).fit(train, train_labels)\ntrain = kscore_filter.transform(train)\ntest = kscore_filter.transform(test)","5950acae":"splitter = model_selection.StratifiedShuffleSplit(n_splits=5, test_size=0.05)\nclassifier = linear_model.LogisticRegression(max_iter=1e+5)\ntuning_grid = {\"C\": (0.01, 0.1)}","6ff152d0":"grid_searcher = model_selection.GridSearchCV(estimator=classifier, param_grid=tuning_grid,\n                                             scoring='roc_auc', cv=splitter, return_train_score=True)\nmodel = grid_searcher.fit(train, train_labels)","513956d1":"print(\"Best hyperparameter: %s\" %model.best_params_)","10b1970e":"print(\"Best score: %.4f\" %model.best_score_)","61bcb2f7":"model = linear_model.LogisticRegression(max_iter=1e+6, C=0.1).fit(train, train_labels)\npredictions = model.predict_proba(test)[:, 1]","bcd60299":"submission = pd.DataFrame({'id': test_id, 'target': predictions})\nsubmission.to_csv('submission.csv', index=False)","e716a202":"So, dataset is pretty huge and contains 25 features, target and id included, same the test set but it doesn't contains the target column for obvious reasons. Even though each feature is fictional it is possible to understand its type by looking at the name which doesn't explain the *meaning* but the kind of feature (binary, categorical, etc.). There are five binary features, ten nominal (half low and half high cardinality), six ordinal (three low, two *medium* and one high cardinality), and two representing day and month so ordinal but cyclic. In addition there are many missing values which will have to handled. Unfortunately, the number of categories for high cardinality column is not the same between the training set and the test set, so special care will be needed.\n\nAll in all, it is quite a difficult dataset to handle. In fact, there will be many degrees of freedom to manipulate the dataset for classification: missing values handling, categorical encoding, other preprocessing steps and classification algorithms.","1ca2937a":"### Test set encoding\nWe need to pay special care in encoding the test set. I cannot, in fact, just use the fitted encoder because unfortunately there are some categories which are unique only for one of the two. In real life scenarios it's something to try to avoid because new categories may produce errors since the model is trained without them, even more if one-hot encoding is used which would make the category a whole new feature. In this case I'm going to fit the encoders on the whole dataset, and then split it again, with a note that such situations must be handed carefully in real life scenarios. In particular, I'm going to use a variance filter to remove constant column in the training set that are categories displayed only in the test set.","c1bddf97":"## Preprocessing\nThere are several preprocessing possibilities that I tried out but here, I will show the ones providing the best cv score. Firslt, let's drop the id and keep the labels to work with just the features.","44327910":"## Final model\nAfter finding the best missing values technique, encoding and features, I finally found out also that a simple logistic regression perform better than a more complex method like extreme gradient boosting. Final step will be then to apply the same transformations to the test set, fit the model and submit the results.\n\nIn the future I might be going to try neural network to further improve the model.","48968bc9":"# Cat in the data\n## Predict the outcome of categorical features","c062bc09":"## Features engineering\nAs seen above one additional category with one hot encoding seems to be the best method but, in all likely, not every feature is important for classification and most could be eliminated to improve generalisation abilities. I will perform an univariate features filter by studying the k-score in descending order and removing the less significant ones.","e479871b":"### Missing values filling\nFirstly missing values must be filled out. The first trivial method is to just use the mode of each column but this is limiting since it does not preserve distributions. To overcome this problem it could be possible to random pick the replacement from the relative feature distribution but it might end up to always wrong picks and, in fact, it's not a very used technique. As a last attempt, which is the one yielding the best score I will just replace missing values with a a valid different category <i>\"NULL\"<\/i>.","0685b7ea":"### Categorical encoding\nThere are many ways to encode categorical features but two are particular common, namely label econding and one hot encoding. The first one assigns an integer to each category, taking into account the order in case of ordinal features. Problem with this method is that in case of features with different number of categories, labels scale differences will be very large and further preprocessing will be needed to bring them to the same range. One hot encoding columns, on the other hand, are filled by just one or zero avoiding the scale problem, but the size of the matrix will greatly increase. Furthermore cyclical features are suitable for cyclical encoding that preserve proximity between apparently *far* labels that are not (like the first and the last ones). \n\nAll in all, the encoding performing the best, at least with logistic regression as baseline estimator, is one hot encoding with <i>null<\/i> filling.","9f601502":"## Introduction\nThis fictional dataset was created ad hoc in order to offer a challenging dataset to make practice with the common machine learning task of encoding and predict from categorical features. The dataset contains only categorical features, and includes:\n\n* binary features\n* low- and high-cardinality nominal features\n* low- and high-cardinality ordinal features\n* (potentially) cyclical features\n\nFirstly, let's import needed libraries and data.","c2fb4fd9":"## Hyperparameters tuning\nAs final step I need to tune the regularisation parameter (actually <i>C<\/i> is the inverse of such parameter) and then just use this as final model.","e839f824":"## Data inspection\nBefore getting into any kind of encoding or classification it is appropriate to inspect the data to get insights about the dataset. I am going start by getting dataset size, features name and distributions."}}