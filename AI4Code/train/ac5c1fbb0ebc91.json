{"cell_type":{"ed6a1d42":"code","0b412735":"code","0974bb42":"code","5ceeb4d7":"code","e98af597":"code","9fc41ca0":"code","6dba0ef3":"code","ecc57bc4":"code","0f97cf55":"code","b0da7a79":"markdown"},"source":{"ed6a1d42":"#!pip install tensorflow==2.5.0","0b412735":"import tensorflow as tf\nprint(tf.__version__)","0974bb42":"import numpy as np\n","5ceeb4d7":"import csv\ntime_step =[]\ntemps = []\ntemporanea=[]\nwith open('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv') as csvfile:\n# YOUR CODE HERE. READ TEMPERATURES INTO TEMPS\n# HAVE TIME STEPS BE A SIMPLE ARRAY OF 1, 2, 3, 4 etc\n  reader = csv.reader(csvfile, delimiter=',')\n  next(reader)\n  i=0\n \n  for row in reader:\n    \n    temps.append([int(row[0]),int(row[1]),int(row[2]),int(row[3]),int(row[4]),int(row[5]),int(row[6]),int(row[7]),int(row[8]),float(row[9]),int(row[10]),int(row[11]),int(row[12])])\n    #print(row[:13])\n    time_step.append(int(row[13]))\n\nseries = np.array(temps)\n#print(series)\ntime = np.array(time_step)\n\nprint(len(time))\n","e98af597":"split_time = 44\ntime_train = time[split_time:]\nx_train = series[split_time:]\ntime_valid = time[:split_time-20]\nx_valid = series[:split_time-20]\nx_test=series[24:split_time]\ntime_test=time[24:split_time]\n#print(time_valid)\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, time_train))\nvalid_dataset = tf.data.Dataset.from_tensor_slices((x_valid, time_valid))\ntest_dataset =tf.data.Dataset.from_tensor_slices((x_test, time_test))\nBATCH_SIZE = 64\nSHUFFLE_BUFFER_SIZE = 100\n\ntrain_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\nvalid_dataset = valid_dataset.batch(BATCH_SIZE)\ntest_dataset = test_dataset.batch(BATCH_SIZE)\n\n","9fc41ca0":"learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience = 3, verbose=1,factor=0.8, min_lr=0.0000001)","6dba0ef3":"tf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n#train_set = windowed_dataset(x_train, window_size=128, batch_size=128, shuffle_buffer=shuffle_buffer_size)\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.BatchNormalization(),\n  tf.keras.layers.Dense(512, activation=\"relu\",kernel_regularizer=tf.keras.regularizers.L2(),bias_regularizer=tf.keras.regularizers.L2()),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(256, activation=\"relu\",kernel_regularizer=tf.keras.regularizers.L2(),bias_regularizer=tf.keras.regularizers.L2()),\n  tf.keras.layers.Dense(128, activation=\"relu\"),\n  tf.keras.layers.Dropout(0.1),\n  tf.keras.layers.Dense(64, activation=\"relu\"),\n  tf.keras.layers.Dropout(0.05),\n\n  tf.keras.layers.Dense(8, activation=\"relu\",kernel_regularizer=tf.keras.regularizers.L2(),bias_regularizer=tf.keras.regularizers.L2()),\n  \n  tf.keras.layers.Dense(1,activation=\"sigmoid\"),\n])\n\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\nmodel.compile(optimizer = optimizer , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n","ecc57bc4":"history = model.fit(train_dataset, epochs=10,validation_data=valid_dataset,callbacks=[learning_rate_reduction])\nwhile (model.evaluate(test_dataset)[1]<0.88):                                 \n    model.fit(train_dataset, epochs=1,validation_data=valid_dataset,callbacks=[learning_rate_reduction])\n","0f97cf55":"\nprint(\"evaluation  model  - \" , model.evaluate(test_dataset)[1])\n\n","b0da7a79":"**Simple DNN model using TensorFlow**"}}