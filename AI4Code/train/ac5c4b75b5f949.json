{"cell_type":{"c83dea7b":"code","e76cbde6":"code","99c13047":"code","56ce9349":"code","5c95d684":"code","163f3de0":"code","0af5fe0d":"code","1d38c7d1":"code","e32cac9a":"code","4837edaf":"code","e4b63abe":"code","e97ef69b":"code","1a7622e6":"code","ab4e919e":"code","e273bfe4":"markdown","e56675e2":"markdown","4ec46d54":"markdown","2ef7a6a2":"markdown","cb9dc63f":"markdown","a7edbe21":"markdown","e6ef99a5":"markdown","8dab1420":"markdown","64cad1cc":"markdown","9b7ad8c9":"markdown","a1d4ce83":"markdown","c4301163":"markdown","b8700806":"markdown","d0f6c254":"markdown","39a7b2b7":"markdown"},"source":{"c83dea7b":"import pandas as pd\nfrom PIL import Image\nimport os\n\nimport matplotlib.pyplot as plt","e76cbde6":"from fastai.vision.all import * # This line imports all of the fastai.vision library","99c13047":"# Let's store the train & test paths which we will use later for accessing the images.\ntrain_path='..\/input\/asl-alphabet\/asl_alphabet_train\/asl_alphabet_train\/'\ntest_path='..\/input\/asl-alphabet\/asl_alphabet_test\/asl_alphabet_test\/'","56ce9349":"# Let's look into an sign language symbol\nimage_path='..\/input\/asl-alphabet\/asl_alphabet_train\/asl_alphabet_train\/A\/A1013.jpg'\nim = Image.open(image_path)\nwidth, height = im.size\nprint(height,width)\nim","5c95d684":"db=DataBlock(blocks=(ImageBlock,CategoryBlock),\n            get_items=get_image_files,\n            get_y=parent_label,\n            splitter=RandomSplitter(valid_pct=0.2))","163f3de0":"f=get_image_files(train_path)\nf[0]","0af5fe0d":"f[0].parent.name","1d38c7d1":"dls=db.dataloaders(train_path,bs=16)","e32cac9a":"dls.show_batch() # Let's have a look into a batch of data","4837edaf":"learn=cnn_learner(dls,resnet18,metrics=[accuracy],loss_func=CrossEntropyLossFlat()) # passing the dataloader, model, metrics and loss function.","e4b63abe":"learn.fine_tune(5)","e97ef69b":"learn.show_results()","1a7622e6":"test_images=get_image_files(test_path) # getting path of test images\ntest_dls=learn.dls.test_dl(test_images) # Creating the test dataloader\npred,_=learn.get_preds(dl=test_dls) # returns the probability prediction for each class\npreds=pred.argmax(dim=-1).numpy() #returns the class with maximum probability","ab4e919e":"test_df=pd.DataFrame(os.listdir(test_path)).rename(columns={0:'Image Name'}) # Loading the path of test data\n\ntest_df['Actual']=test_df['Image Name'].str.split('_').str[0]\n\ntest_df['Prediction']=preds\n\nvocab=dict(zip(np.arange(29),dls.vocab))\ntest_df['Prediction']=test_df['Prediction'].replace(vocab)\ntest_df['Image Path']=test_path+test_df['Image Name']\n\n# Display picture of the dataset with their labels\nfig, axes = plt.subplots(nrows=4, ncols=7, figsize=(20, 12),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(plt.imread(test_df['Image Path'].iloc[i]))\n    ax.set_title(f\"True: {test_df.Actual.iloc[i]}\\nPredicted: {test_df.Prediction.iloc[i]}\", fontsize = 15)\nplt.tight_layout()\nplt.show()","e273bfe4":"### show_batch(): \n\nThe below function show a batch of images along with it's class label","e56675e2":"Now the below function tells fastai what kind of dataset we have and how it is structured. Datablocks is a high level API to quickly get your data in a `DataLoaders`.\n<br>Let me describe you each of the components we have used of DataBlock API:","4ec46d54":"## Using trained model to predict on unseen test data.","2ef7a6a2":"### Load required libraries","cb9dc63f":"<b>Hurray!!!! We have trained our model and got pretty good prediction on test images. However this is a very basic step for solving the real world translation problem leveraging the power of fastai library.<\/b>\n\n","a7edbe21":"# Let's train the Model\n\nTo achieve this task we will use famous CNN architecture, <b>ResNet<\/b>.","e6ef99a5":"# American Sign Language \n\n<b>Sign language<\/b> is commonly used as the main form of communication for people who are \"Deaf\" or \"hard of hearing\" expressed by movement of hands and face..\nThere are over 135 different sign languages all around the world including American Sign Language (ASL), Australian Sign Language (Auslan) and British Sign Language (BSL).\nAmerican sign language is used by many deaf inviduals in America and and in many parts of Canada.\n\n<div style=\"width:100%;text-align: center;\"> <img src=\"https:\/\/www.lingvano.com\/asl\/wp-content\/uploads\/sites\/3\/asl_content\/content-pages\/blog\/asl-1024x523.jpg\" alt='\"ASL\" spelled out in American Sign Language fingerspelling' style=\"width: 600px;\"\/><\/div>","8dab1420":"### Splitter: Use to split the dataset into train & validation\n\nHere we are using Random Splitter which randomly splits the data into train and validation set. Here we are keeping 80% of data in training set and 20% in validation set.\n\n### blocks=(ImageBlock,CategoryBlock): \nThe block tells us the kind of input date (Image, Text etc.) and type of output (e.g. Multiclass or Multilabel.)\n\n## What is DataLoaders ?\n\n- To make our data ready for training a model, we need to create a DataLoaders object in fastai.\n\n- Once we have our datablock ready, we can create dataloader by passing the training path and batch size.","64cad1cc":"Below images show the prediction of our model on a batch.","9b7ad8c9":"### get_image_files: this function is used to grab all the file locations of our images. \nTo check how the files are named and where they are located, let's look at the first one:","a1d4ce83":"## Now let's see the prediction on 29 test images.","c4301163":"Here we are loading the pretrained ResNet model using Fastai's <b>cnn_learner<\/b> function. Along with model name you can pass various parameters such as different metrics, loss function etc.\n\nNow to fit the model we will use fastai's fine_tune class.","b8700806":"# Problem Statement & Data Description\n\nConversation with a person having hearing disability is always a major challenge. Sign language is a very powerful tool invented for individuals with hearing and speech disability to communicate with the world smoothly and with less complexity. However, there are few flaws to the sign language system as there doesn't exist universal sign language. The sign gestures often get mixed and confused for someone who is a beginner or knows it in a different language. \n\nA.I. and computer vision system can be leveraged to interpret the sign language. Building such AI systems will be extermely useful for people (e.g. doctors\/family\/friends etc.) to communicate with others having hearing\/speaking disablities.  \n\nThe task here is to recognize the alphabet corresponding to the sign language symbol.\nIn this Kernel, we will classify the sign language symbols using state of the art ResNet pretrained model. After training the model, the alphabet corresponding to the sign language symbol will be predicted.\n\nHere we are using American sign language dataset consisting of, sign language images of 26 alphabets letters (A-Z), along with three word signs DELETE, SPACE, NOTHING  separated in 29 folders werein each folder contains 3000 images for each sign which represent the various classes.\n\nIn this kernel we will try to solve the above mentioned task using a famous deep learning library, <b>Fastai<\/b>.","d0f6c254":"The above import gives us all of the functions and classes we will need to create a wide variety of computer vision models.","39a7b2b7":"### parent_label: function to get a classlabel\n\nAs we know that the folder names represents the class labels for the images inside it. To get these folder name we directly have a function in fastai called parent_label, which basically fetches the name of the last folder from image path.\n"}}