{"cell_type":{"0d25026a":"code","72b295fc":"code","b50db2dc":"code","8565e9d4":"code","3b574b29":"code","b73e2acc":"code","9988f714":"code","ae6739c3":"code","5699c6a1":"code","3cb9ac9e":"code","0468e08f":"code","9f52c1be":"code","7a39dddb":"code","036d26ec":"code","6f7b2408":"code","13d541ca":"code","892bf1bb":"code","36d2f074":"code","59035da8":"code","82f28dc3":"code","8af7ee30":"code","1b412284":"code","ef71477e":"code","2e1dee01":"code","23c2930a":"code","26bd40f6":"code","df4ab3d7":"code","c72d2202":"code","ae1160c8":"code","312ac334":"code","adf603f5":"code","0f6f7172":"code","7b50cc27":"markdown","5afa5963":"markdown","e261363f":"markdown","8095bca5":"markdown","74b45f4f":"markdown","cedd54b1":"markdown","899ba214":"markdown","4751ed9c":"markdown","ca7bddf0":"markdown","391f7917":"markdown","54b87fd6":"markdown","7854fac9":"markdown","68ea466d":"markdown","b23b76c9":"markdown","5ab4391b":"markdown","171eea88":"markdown","c697d7fa":"markdown","9790fad1":"markdown","35bbb935":"markdown"},"source":{"0d25026a":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import max_error\nfrom sklearn.metrics import mean_squared_log_error\n\n","72b295fc":"housing = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\nhousing.head()","b50db2dc":"housing.info()","8565e9d4":"housing.isna().sum().loc[lambda x : x>0].sort_values(ascending=False)","3b574b29":"housing.drop(['Alley', 'PoolQC', 'Fence', 'MiscFeature'], axis=1, inplace=True)","b73e2acc":"plt.subplots(figsize=(12, 9))\nsns.heatmap(housing.corr(), vmin=-1, vmax=1, cmap='RdGy')\nplt.show()","9988f714":"housing.drop(['Id', 'BsmtFinSF2', 'BsmtHalfBath', '3SsnPorch', 'MiscVal'], axis=1, inplace=True)","ae6739c3":"from sklearn.model_selection import train_test_split\nX = housing.drop(columns=['SalePrice'])\ny = housing['SalePrice']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8, random_state=8)\nX_train.head()","5699c6a1":"num_col = [ col for col in X_train.columns if X_train[col].dtypes != 'object']","3cb9ac9e":"cat_col = [ col for col in X_train.columns if X_train[col].dtypes == 'object']","0468e08f":"# Categorical pipeline - filling the missing values in categorical columns using SimpleImputer and OneHotEncoder. \ncat_pipeline = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('one_hot_enc', OneHotEncoder(drop='first'))\n])\n\n# Numerical pipeline - filling the missing values in numerical columns using SimpleImputer and scaling with MinMaxScaler to preserve the shape of the original distribution. \nnum_pipeline = Pipeline(steps=[\n    ('num_impute', SimpleImputer(strategy='mean')),\n    ('scale', MinMaxScaler())\n])\n\nfull_processor = ColumnTransformer(transformers=[\n    ('number', num_pipeline, num_col), \n    ('category', cat_pipeline, cat_col)\n])","9f52c1be":"lin_model_pipeline = Pipeline(steps=[\n    ('processor', full_processor), \n    ('model', LinearRegression())\n])\n\nlm = lin_model_pipeline.fit(X_train, y_train)","7a39dddb":"lin_model_pipeline.named_steps.model.get_params()","036d26ec":"def plot_predictions(y_true, y_pred):\n    print(\n        f\"\"\"\n        MSE: {mean_squared_error(y_true, y_pred)}\n        RMSE: {mean_squared_error(y_true, y_pred)**0.5}\n        MAE: {mean_absolute_error(y_true, y_pred)}\n        R_SQR: {r2_score(y_true, y_pred)}\n        EXV: {explained_variance_score(y_true, y_pred)}\n        ME: {max_error(y_true, y_pred)}\n        RMSLE: {mean_squared_log_error(y_true, y_pred)**0.5}\n        \"\"\"\n    )\n    max_preds = min([max(y_pred.tolist()), max(y_true.tolist())])\n    min_preds = max([min(y_pred.tolist()), min(y_true.tolist())])\n    print(max_preds, min_preds)\n    # plot\n    plt.figure(figsize=(8,8))\n    sns.scatterplot(x=y_pred, y=y_true)\n    sns.lineplot(x=[min_preds,max_preds], y=[min_preds, max_preds], color='red')\n    plt.ylabel('Reference')\n    plt.xlabel('Predictions')\n    plt.show()\n    \n    errors = y_pred - y_true\n    plt.subplots(figsize=(10, 6))\n    sns.histplot(errors)\n    plt.vlines(x = 0, ymin = 0, ymax = 140, color = 'red')\n    plt.show()\n    \n    p_df = (\n        pd.DataFrame({'y_true':y_true, 'y_pred':y_pred})\n        .assign(error = lambda x: x['y_pred'] - x['y_true'])\n        .sort_values(by = 'y_true')\n        )\n    \n    plt.subplots(figsize = (10, 6))\n    sns.scatterplot(data=p_df, x = 'y_true', y = 'error')\n    plt.hlines(y = 0, xmin = 0, xmax = 700000, color = 'red')\n    plt.show()\n    \nplot_predictions(y_train, lin_model_pipeline.predict(X_train))","6f7b2408":"lm_test = lin_model_pipeline.fit(X_test, y_test)\n\nresults = pd.DataFrame({\n    'prediction': lin_model_pipeline.predict(X_test), \n    'true_value': y_test\n})\n\nresults.head()","13d541ca":"plot_predictions(y_test, lin_model_pipeline.predict(X_test))","892bf1bb":"coef_lr = pd.DataFrame(lin_model_pipeline['model'].coef_)\ncoef_lr","36d2f074":"intercept_lr = lin_model_pipeline['model'].intercept_\nintercept_lr","59035da8":"rf_pipeline = Pipeline(steps=[\n    ('processor', full_processor), \n    ('model', RandomForestRegressor())\n])\n\nrf = rf_pipeline.fit(X_train, y_train)","82f28dc3":"plot_predictions(y_train, rf_pipeline.predict(X_train))","8af7ee30":"rf_test = rf_pipeline.fit(X_test, y_test)\n\nresults = pd.DataFrame({\n    'prediction': rf_pipeline.predict(X_test), \n    'true_value': y_test\n})\n\nresults.head()","1b412284":"plot_predictions(y_test, rf_pipeline.predict(X_test))","ef71477e":"sns.histplot(y_train)\nplt.title('Price distribution')\nplt.show()","2e1dee01":"sns.histplot(np.log(y_train))\nplt.title('Price distribution')\nplt.show()","23c2930a":"lin_model_pipeline.fit(X_train, np.log(y_train))\n\nplot_predictions(y_train, np.exp(lin_model_pipeline.predict(X_train)))","26bd40f6":"rf_pipeline.fit(X_train, np.log(y_train))\nplot_predictions(y_train, np.exp(rf_pipeline.predict(X_train)))","df4ab3d7":"rf_pipeline.fit(X, np.log(y))","c72d2202":"house_test = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","ae1160c8":"final_submission = house_test.drop(house_test.columns.difference(cat_col + num_col), axis=1)","312ac334":"SalePricePreds = np.exp(rf_pipeline.predict(final_submission))","adf603f5":"submission_file = pd.DataFrame({\n    'Id': house_test['Id'],\n    'SalePrice': SalePricePreds\n}).to_csv('submission.csv', index=None)","0f6f7172":"house_submission = pd.read_csv(\"submission.csv\")\nhouse_submission","7b50cc27":"The columns which has least corelation with SalePrice are dropped. ","5afa5963":"* **Mean Absolute Error (MAE):** absolute value of the difference between the predicted value and the true value. Tells us how big of an error we can expect from the forecast on average.\n* **Mean Squared Error (MSE):** average squared difference between the predicted values and the true value.\n* **Root Mean Squared Error (RMSE):** estimator measuring the quality of the fit of the model. Small RMSE means predicted value to be close to true values.\n* **R\u00b2 score (R_SQR):** proportion of the variance for a dependent variable that's explained by an independent variable. Range between 0 and 1.\n* **Explained variance score:** computes the explained variance regression score. The best possible score is 1.0, lower values are worse.\n* **Max error:** computes the maximum residual error, a metric that captures the worst case error between the predicted value and the true value.\n* **Root Mean Squared Logarithmic Error (RMSLE):** computes a risk metric corresponding to the predicted value of the squared logarithmic (quadratic) error or loss.","e261363f":"# Performance Metrics","8095bca5":"# Supervised Machine Learning: Regression","74b45f4f":"# Random Forest Regressor","cedd54b1":"# Random Forest Regression","899ba214":"Linear regression is a quiet and simple statistical regression method used for predictive analysis and shows the relationship between the continuous variables. Linear regression shows the linear relationship between the independent variable (X-axis) and the dependent variable (Y-axis). The red line is referred to as the best fit straight line. Based on the given data points, we try to plot a line that models the points the best.","4751ed9c":"One hot encoding is one method of converting data to prepare it for an algorithm and get a better prediction. With one-hot, we convert each categorical value into a new categorical column and assign a binary value of 1 or 0 to those columns. Each integer value is represented as a binary vector. All the values are zero, and the index is marked with a 1.","ca7bddf0":"# Logarithmic transformation","391f7917":"# Splitting the data set","54b87fd6":"Dataset is splited into train and test datasets. Train data is used to build the model.The model is trained on the training data. Usually it is between 70% and 85% of the data. Test data is used to evaluate the model.It is the remaining 15% to 30%. Scikit-learn makes this process easy.\n\nTrain_test_split returns four values. The first two is the breakup of the features of the data. The features variable is traditionally called x, so they are called x_train and x_test. Labels or targets are usually named y.","7854fac9":"# Results on Test Set","68ea466d":"# Loading test data","b23b76c9":"# Linear Regression","5ab4391b":"The SimpleImputer class provides basic strategies for imputing missing values. Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located. Below first of all numeric columns are selected then missing values in numeric columns are replaced by using median value of the columns.","171eea88":"The SimpleImputer class also supports categorical data represented as string values or pandas categoricals when using the 'most_frequent' or 'constant' strategy: Below 'most_frequent' strategy is used.","c697d7fa":"Log transformation is a data transformation method in which it replaces each variable x with a log(x). The log transformation reduces or removes the skewness of our original data.","9790fad1":"# Results on Test Set","35bbb935":"Random Forest is a Supervised learning algorithm that is based on the ensemble learning method and many Decision Trees. Ensemble learning method is a technique that combines predictions from multiple machine learning algorithms to make a more accurate prediction than a single model.\nRandom Forest is a Bagging technique, so all calculations are run in parallel and there is no interaction between the Decision Trees when building them. RF can be used to solve both Classification and Regression tasks."}}