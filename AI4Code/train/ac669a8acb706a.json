{"cell_type":{"61e3b78f":"code","ff734a4f":"code","e69a271b":"code","46f1a1d6":"code","bdfa5d1c":"code","6351075c":"code","b1f56ce2":"code","4add7fac":"code","36673b51":"code","394eae5d":"code","d87edfae":"code","30a034aa":"code","ab77721e":"code","e12b76ce":"code","f24a2d6a":"code","cac7a297":"code","9c0806d8":"code","c691352f":"code","8db78be3":"code","955cd40c":"code","5c8d060c":"code","a4f8784b":"code","ec886d31":"code","9628b88b":"code","ab3f8dc1":"code","87bd3239":"code","18f5e3bb":"code","5dfb8fec":"code","bff47064":"code","b9454c1d":"code","35de94d8":"code","c6a376f1":"code","d080d4a5":"markdown","e3404948":"markdown","29bae97b":"markdown","d4232026":"markdown","eea4e2eb":"markdown","47fe3e31":"markdown","523f0fc9":"markdown","ee78d08f":"markdown","7b4df0c1":"markdown"},"source":{"61e3b78f":"#import libraries\n\nimport sys  # to access system parameter\nimport numpy as np # linear algebra\nimport pandas as pd  # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib\nimport scipy as sp\n\nimport IPython\nfrom IPython import display\n\nimport sklearn\n\nimport random\nimport time\n\nimport warnings\nwarnings.filterwarnings('ignore')\nprint('-'*25)\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n","ff734a4f":"#import data modelling libraries\n\nfrom sklearn import svm, linear_model, tree, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\n\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","e69a271b":"import pandas as pd\ndata_raw = pd.read_csv('..\/input\/train.csv')\ndata_val  = pd.read_csv('..\/input\/test.csv')\n\ndata1 = data_raw.copy(deep = True)\ndata_cleaner = [data1, data_val]\nprint (data_raw.info())\ndata_raw.sample(10)","46f1a1d6":"#clean data\n\nprint('Train columns with null values:\\n', data1.isnull().sum())\nprint(\"-\"*10)\nprint('Test\/Validation columns with null values:\\n', data_val.isnull().sum())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')","bdfa5d1c":"#complete data\n\nfor dataset in data_cleaner:    \n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \ndrop_column = ['PassengerId','Cabin', 'Ticket']    \ndata1.drop(drop_column, axis=1, inplace = True)\n\nprint(data1.isnull().sum())\nprint(\"-\"*10)\nprint(data_val.isnull().sum())\n","6351075c":"# create\nfor dataset in data_cleaner:  \n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0\n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n    \n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n    \nstat_min = 10\ntitle_names = (data1['Title'].value_counts() < stat_min) \ndata1['Title'] = data1['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\nprint(data1['Title'].value_counts())\nprint(\"-\"*10)\n\ndata1.info()\ndata_val.info()\ndata1.sample(10)\n","b1f56ce2":"# convert\nfrom sklearn.preprocessing import LabelEncoder\n#code categorical data\nlabel = LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n    \nTarget = ['Survived']\n\ndata1_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone']\ndata1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare'] \ndata1_xy =  Target + data1_x\nprint('Original X Y: ', data1_xy, '\\n')\n\ndata1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\ndata1_xy_bin = Target + data1_x_bin\nprint('Bin X Y: ', data1_xy_bin, '\\n')\n\ndata1_dummy = pd.get_dummies(data1[data1_x])\ndata1_x_dummy = data1_dummy.columns.tolist()\ndata1_xy_dummy = Target + data1_x_dummy\nprint('Dummy X Y: ', data1_xy_dummy, '\\n')","4add7fac":"print('Train columns with null values: \\n', data1.isnull().sum())\nprint(\"-\"*10)\nprint (data1.info())\nprint(\"-\"*10)\n\nprint('Test\/Validation columns with null values: \\n', data_val.isnull().sum())\nprint(\"-\"*10)\nprint (data_val.info())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')","36673b51":"import numpy as np\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], data1[Target], random_state = 0)\n\n\nprint(\"Data1 Shape: {}\".format(data1.shape))\nprint(\"Train1 Shape: {}\".format(train1_x.shape))\nprint(\"Test1 Shape: {}\".format(test1_x.shape))\ntrain1_x_bin.head()","394eae5d":"for x in data1_x:\n    if data1[x].dtype != 'float64' :\n        print('Survival Correlation by:', x)\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n        print('-'*10, '\\n')\n        \n\n#using crosstabs: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.crosstab.html\nprint(pd.crosstab(data1['Title'],data1[Target[0]]))","d87edfae":"import matplotlib.pyplot as plt\nplt.figure(figsize=[16,12])\n\nplt.subplot(231)\nplt.boxplot(x=data1['Fare'], showmeans = True, meanline = True)\nplt.title('Fare Boxplot')\nplt.ylabel('Fare ($)')\n\nplt.subplot(232)\nplt.boxplot(data1['Age'], showmeans = True, meanline = True)\nplt.title('Age Boxplot')\nplt.ylabel('Age (Years)')\n\nplt.subplot(233)\nplt.boxplot(data1['FamilySize'], showmeans = True, meanline = True)\nplt.title('Family Size Boxplot')\nplt.ylabel('Family Size (#)')\n\nplt.subplot(234)\nplt.hist(x = [data1[data1['Survived']==1]['Fare'], data1[data1['Survived']==0]['Fare']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Fare Histogram by Survival')\nplt.xlabel('Fare ($)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\nplt.subplot(235)\nplt.hist(x = [data1[data1['Survived']==1]['Age'], data1[data1['Survived']==0]['Age']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Age Histogram by Survival')\nplt.xlabel('Age (Years)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\nplt.subplot(236)\nplt.hist(x = [data1[data1['Survived']==1]['FamilySize'], data1[data1['Survived']==0]['FamilySize']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Family Size Histogram by Survival')\nplt.xlabel('Family Size (#)')\nplt.ylabel('# of Passengers')\nplt.legend()","30a034aa":"#graph individual features by survival\nimport seaborn as sns\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\n\nsns.barplot(x = 'Embarked', y = 'Survived', data=data1, ax = saxis[0,0])\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data1, ax = saxis[0,1])\nsns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data1, ax = saxis[0,2])\n\nsns.pointplot(x = 'FareBin', y = 'Survived',  data=data1, ax = saxis[1,0])\nsns.pointplot(x = 'AgeBin', y = 'Survived',  data=data1, ax = saxis[1,1])\nsns.pointplot(x = 'FamilySize', y = 'Survived', data=data1, ax = saxis[1,2])","ab77721e":"#Pclass\n#we know class mattered in survival, now let's compare class and a 2nd feature\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\n\nsns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data1, ax = axis1)\naxis1.set_title('Pclass vs Fare Survival Comparison')\n\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data1, split = True, ax = axis2)\naxis2.set_title('Pclass vs Age Survival Comparison')\n\nsns.boxplot(x = 'Pclass', y ='FamilySize', hue = 'Survived', data = data1, ax = axis3)\naxis3.set_title('Pclass vs Family Size Survival Comparison')","e12b76ce":"# sex\n#we know sex mattered in survival, now let's compare sex and a 2nd feature\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data1, ax = qaxis[0])\naxis1.set_title('Sex vs Embarked Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data1, ax  = qaxis[1])\naxis1.set_title('Sex vs Pclass Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone', data=data1, ax  = qaxis[2])\naxis1.set_title('Sex vs IsAlone Survival Comparison')","f24a2d6a":"# othercomparisons\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\n\n#how does family size factor with sex & survival compare\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\n\n#how does class factor with sex & survival compare\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis2)","cac7a297":"e = sns.FacetGrid(data1, col = 'Embarked')\ne.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\ne.add_legend()\n\n#plot distributions of age of passengers who survived or did not survive\na = sns.FacetGrid( data1, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , data1['Age'].max()))\na.add_legend()","9c0806d8":"#histogram comparison of sex, class, and age by survival\nh = sns.FacetGrid(data1, row = 'Sex', col = 'Pclass', hue = 'Survived')\nh.map(plt.hist, 'Age', alpha = .75)\nh.add_legend()","c691352f":"#pair plots of entire dataset\npp = sns.pairplot(data1, hue = 'Survived', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\npp.set(xticklabels=[])","8db78be3":"#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data1)","955cd40c":"\n\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n    XGBClassifier()    \n    ]\n\n\n\n#split dataset in cross-validation with this splitter class: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n#note: this is an alternative to train_test_split\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60\/30 split intentionally leaving out 10%\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict = data1[Target]\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n    cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target], cv  = cv_split)\n\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    ######AAAAAAA#MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n\n    alg.fit(data1[data1_x_bin], data1[Target])\n    MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])\n    \n    row_index+=1\n\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare","5c8d060c":"#barplot using https:\/\/seaborn.pydata.org\/generated\/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#prettify using pyplot: https:\/\/matplotlib.org\/api\/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","a4f8784b":"for index, row in data1.iterrows(): \n    #random number generator: https:\/\/docs.python.org\/2\/library\/random.html\n    if random.random() > .5:     # Random float x, 0.0 <= x < 1.0    \n        data1.set_value(index, 'Random_Predict', 1) #predict survived\/1\n    else: \n        data1.set_value(index, 'Random_Predict', 0) #predict died\/0\n    \n\n#score random guess of survival. Use shortcut 1 = Right Guess and 0 = Wrong Guess\n#the mean of the column will then equal the accuracy\ndata1['Random_Score'] = 0 #assume prediction wrong\ndata1.loc[(data1['Survived'] == data1['Random_Predict']), 'Random_Score'] = 1 #set to 1 for correct prediction\nprint('Coin Flip Model Accuracy: {:.2f}%'.format(data1['Random_Score'].mean()*100))\n\n#we can also use scikit's accuracy_score function to save us a few lines of code\n#http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score\nprint('Coin Flip Model Accuracy w\/SciKit: {:.2f}%'.format(metrics.accuracy_score(data1['Survived'], data1['Random_Predict'])*100))\n\n\npivot_female = data1[data1.Sex=='female'].groupby(['Sex','Pclass', 'Embarked','FareBin'])['Survived'].mean()\nprint('Survival Decision Tree w\/Female Node: \\n',pivot_female)\n\npivot_male = data1[data1.Sex=='male'].groupby(['Sex','Title'])['Survived'].mean()\nprint('\\n\\nSurvival Decision Tree w\/Male Node: \\n',pivot_male)","ec886d31":"def mytree(df):\n    \n    #initialize table to store predictions\n    Model = pd.DataFrame(data = {'Predict':[]})\n    male_title = ['Master'] #survived titles\n\n    for index, row in df.iterrows():\n\n        #Question 1: Were you on the Titanic; majority died\n        Model.loc[index, 'Predict'] = 0\n\n        #Question 2: Are you female; majority survived\n        if (df.loc[index, 'Sex'] == 'female'):\n                  Model.loc[index, 'Predict'] = 1\n\n        if ((df.loc[index, 'Sex'] == 'female') & \n            (df.loc[index, 'Pclass'] == 3) & \n            (df.loc[index, 'Embarked'] == 'S')  &\n            (df.loc[index, 'Fare'] > 8)\n\n           ):\n                  Model.loc[index, 'Predict'] = 0\n\n        if ((df.loc[index, 'Sex'] == 'male') &\n            (df.loc[index, 'Title'] in male_title)\n            ):\n            Model.loc[index, 'Predict'] = 1\n        \n        \n    return Model\n#model data\nTree_Predict = mytree(data1)\nprint('Decision Tree Model Accuracy\/Precision Score: {:.2f}%\\n'.format(metrics.accuracy_score(data1['Survived'], Tree_Predict)*100))\n\nprint(metrics.classification_report(data1['Survived'], Tree_Predict))","9628b88b":"#Plot Accuracy Summary\n#Credit: http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\ncnf_matrix = metrics.confusion_matrix(data1['Survived'], Tree_Predict)\nnp.set_printoptions(precision=2)\n\nclass_names = ['Dead', 'Survived']\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names,\n                      title='Confusion matrix, without normalization')\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, \n                      title='Normalized confusion matrix')","ab3f8dc1":"dtree = tree.DecisionTreeClassifier(random_state = 0)\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\ndtree.fit(data1[data1_x_bin], data1[Target])\n\nprint('BEFORE DT Parameters: ', dtree.get_params())\n#print(\"BEFORE DT Training w\/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT Test w\/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT Test w\/bin score 3*std: +\/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n#print(\"BEFORE DT Test w\/bin set score min: {:.2f}\". format(base_results['test_score'].min()*100))\nprint('-'*10)\n\n\n#tune hyper-parameters: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\nparam_grid = {'criterion': ['gini', 'entropy'],'max_depth': [2,4,6,8,10,None],'random_state': [0]}\n\n#choose best model with grid_search: #http:\/\/scikit-learn.org\/stable\/modules\/grid_search.html#grid-search\n#http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_grid_search_digits.html\ntune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\ntune_model.fit(data1[data1_x_bin], data1[Target])\n\n#print(tune_model.cv_results_.keys())\n#print(tune_model.cv_results_['params'])\nprint('AFTER DT Parameters: ', tune_model.best_params_)\n#print(tune_model.cv_results_['mean_train_score'])\n#print(\"AFTER DT Training w\/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n#print(tune_model.cv_results_['mean_test_score'])\nprint(\"AFTER DT Test w\/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint(\"AFTER DT Test w\/bin score 3*std: +\/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\nprint('-'*10)\n","87bd3239":"print('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \nprint('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n\n#############print(\"BEFORE DT RFE Training w\/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT RFE Test w\/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT RFE Test w\/bin score 3*std: +\/- {:.2f}\". format(base_results['test_score'].std()*100*3))\nprint('-'*10)\n\n\n\n#feature selection\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_split)\ndtree_rfe.fit(data1[data1_x_bin], data1[Target])\n\n#transform x&y to reduced features and fit new model\n#alternative: can use pipeline to reduce fit and transform steps: http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html\nX_rfe = data1[data1_x_bin].columns.values[dtree_rfe.get_support()]\nrfe_results = model_selection.cross_validate(dtree, data1[X_rfe], data1[Target], cv  = cv_split)\n\n#print(dtree_rfe.grid_scores_)\nprint('AFTER DT RFE Training Shape New: ', data1[X_rfe].shape) \nprint('AFTER DT RFE Training Columns New: ', X_rfe)\n\n#######print(\"AFTER DT RFE Training w\/bin score mean: {:.2f}\". format(rfe_results['train_score'].mean()*100)) \nprint(\"AFTER DT RFE Test w\/bin score mean: {:.2f}\". format(rfe_results['test_score'].mean()*100))\nprint(\"AFTER DT RFE Test w\/bin score 3*std: +\/- {:.2f}\". format(rfe_results['test_score'].std()*100*3))\nprint('-'*10)\n\n\n#tune rfe model\nrfe_tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\nrfe_tune_model.fit(data1[X_rfe], data1[Target])\n\n#print(rfe_tune_model.cv_results_.keys())\n#print(rfe_tune_model.cv_results_['params'])\nprint('AFTER DT RFE Tuned Parameters: ', rfe_tune_model.best_params_)\n#print(rfe_tune_model.cv_results_['mean_train_score'])\n########print(\"AFTER DT RFE Tuned Training w\/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \nprint(\"AFTER DT RFE Tuned Test w\/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint(\"AFTER DT RFE Tuned Test w\/bin score 3*std: +\/- {:.2f}\". format(rfe_tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\nprint('-'*10)","18f5e3bb":"import graphviz \ndot_data = tree.export_graphviz(dtree, out_file=None, \n                                feature_names = data1_x_bin, class_names = True,\n                                filled = True, rounded = True)\ngraph = graphviz.Source(dot_data) \ngraph","5dfb8fec":"correlation_heatmap(MLA_predict)","bff47064":"\nvote_est = [\n    #Ensemble Methods: http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassifier()),\n\n    #Gaussian Processes: http:\/\/scikit-learn.org\/stable\/modules\/gaussian_process.html#gaussian-process-classification-gpc\n    ('gpc', gaussian_process.GaussianProcessClassifier()),\n    \n    #GLM: http:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#logistic-regression\n    ('lr', linear_model.LogisticRegressionCV()),\n    \n    #Navies Bayes: http:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html\n    ('bnb', naive_bayes.BernoulliNB()),\n    ('gnb', naive_bayes.GaussianNB()),\n    \n    #Nearest Neighbor: http:\/\/scikit-learn.org\/stable\/modules\/neighbors.html\n    ('knn', neighbors.KNeighborsClassifier()),\n    \n    #SVM: http:\/\/scikit-learn.org\/stable\/modules\/svm.html\n    ('svc', svm.SVC(probability=True)),\n    \n    #xgboost: http:\/\/xgboost.readthedocs.io\/en\/latest\/model.html\n   ('xgb', XGBClassifier())\n\n]\n\n\n#Hard Vote or majority rules\nvote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\nvote_hard_cv = model_selection.cross_validate(vote_hard, data1[data1_x_bin], data1[Target], cv  = cv_split)\nvote_hard.fit(data1[data1_x_bin], data1[Target])\n\n########print(\"Hard Voting Training w\/bin score mean: {:.2f}\". format(vote_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting Test w\/bin score mean: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting Test w\/bin score 3*std: +\/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\nprint('-'*10)\n\n\n#Soft Vote or weighted probabilities\nvote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\nvote_soft_cv = model_selection.cross_validate(vote_soft, data1[data1_x_bin], data1[Target], cv  = cv_split)\nvote_soft.fit(data1[data1_x_bin], data1[Target])\n\n######print(\"Soft Voting Training w\/bin score mean: {:.2f}\". format(vote_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting Test w\/bin score mean: {:.2f}\". format(vote_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting Test w\/bin score 3*std: +\/- {:.2f}\". format(vote_soft_cv['test_score'].std()*100*3))\nprint('-'*10)","b9454c1d":"grid_n_estimator = [10, 50, 100, 300]\ngrid_ratio = [.1, .25, .5, .75, 1.0]\ngrid_learn = [.01, .03, .05, .1, .25]\ngrid_max_depth = [2, 4, 6, 8, 10, None]\ngrid_min_samples = [5, 10, .03, .05, .10]\ngrid_criterion = ['gini', 'entropy']\ngrid_bool = [True, False]\ngrid_seed = [0]\n\n\ngrid_param = [\n            [{\n            #AdaBoostClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostClassifier.html\n            'n_estimators': grid_n_estimator, #default=50\n            'learning_rate': grid_learn, #default=1\n            #'algorithm': ['SAMME', 'SAMME.R'], #default=\u2019SAMME.R\n            'random_state': grid_seed\n            }],\n       \n    \n            [{\n            #BaggingClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'max_samples': grid_ratio, #default=1.0\n            'random_state': grid_seed\n             }],\n\n    \n            [{\n            #ExtraTreesClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'criterion': grid_criterion, #default=\u201dgini\u201d\n            'max_depth': grid_max_depth, #default=None\n            'random_state': grid_seed\n             }],\n\n\n            [{\n            #GradientBoostingClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n            #'loss': ['deviance', 'exponential'], #default=\u2019deviance\u2019\n            'learning_rate': [.05], #default=0.1 -- 12\/31\/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n            'n_estimators': [300], #default=100 -- 12\/31\/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n            #'criterion': ['friedman_mse', 'mse', 'mae'], #default=\u201dfriedman_mse\u201d\n            'max_depth': grid_max_depth, #default=3   \n            'random_state': grid_seed\n             }],\n\n    \n            [{\n            #RandomForestClassifier - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n            'n_estimators': grid_n_estimator, #default=10\n            'criterion': grid_criterion, #default=\u201dgini\u201d\n            'max_depth': grid_max_depth, #default=None\n            'oob_score': [True], #default=False -- 12\/31\/17 set to reduce runtime -- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds.\n            'random_state': grid_seed\n             }],\n    \n            [{    \n            #GaussianProcessClassifier\n            'max_iter_predict': grid_n_estimator, #default: 100\n            'random_state': grid_seed\n            }],\n        \n    \n            [{\n            #LogisticRegressionCV - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n            'fit_intercept': grid_bool, #default: True\n            #'penalty': ['l1','l2'],\n            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs\n            'random_state': grid_seed\n             }],\n            [{\n            'alpha': grid_ratio, #default: 1.0\n             }],\n    \n            #GaussianNB - \n            [{}],\n    \n            [{\n            'n_neighbors': [1,2,3,4,5,6,7], #default: 5\n            'weights': ['uniform', 'distance'], #default = \u2018uniform\u2019\n            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n            }],\n            \n    \n            [{\n            'C': [1,2,3,4,5], #default=1.0\n            'gamma': grid_ratio, #edfault: auto\n            'decision_function_shape': ['ovo', 'ovr'], #default:ovr\n            'probability': [True],\n            'random_state': grid_seed\n             }],\n\n    \n            [{\n            'learning_rate': grid_learn, #default: .3\n            'max_depth': [1,2,4,6,8,10], #default 2\n            'n_estimators': grid_n_estimator, \n            'seed': grid_seed  \n             }]   \n        ]\nstart_total = time.perf_counter() #https:\/\/docs.python.org\/3\/library\/time.html#time.perf_counter\nfor clf, param in zip (vote_est, grid_param): #https:\/\/docs.python.org\/3\/library\/functions.html#zip\n\n    start = time.perf_counter()        \n    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n    best_search.fit(data1[data1_x_bin], data1[Target])\n    run = time.perf_counter() - start\n\n    best_param = best_search.best_params_\n    print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n    clf[1].set_params(**best_param) \n\n\nrun_total = time.perf_counter() - start_total\nprint('Total optimization time was {:.2f} minutes.'.format(run_total\/60))\n\nprint('-'*10)","35de94d8":"grid_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\ngrid_hard_cv = model_selection.cross_validate(grid_hard, data1[data1_x_bin], data1[Target], cv  = cv_split)\ngrid_hard.fit(data1[data1_x_bin], data1[Target])\n\n#######print(\"Hard Voting w\/Tuned Hyperparameters Training w\/bin score mean: {:.2f}\". format(grid_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting w\/Tuned Hyperparameters Test w\/bin score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting w\/Tuned Hyperparameters Test w\/bin score 3*std: +\/- {:.2f}\". format(grid_hard_cv['test_score'].std()*100*3))\nprint('-'*10)\n\n#Soft Vote or weighted probabilities w\/Tuned Hyperparameters\ngrid_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\ngrid_soft_cv = model_selection.cross_validate(grid_soft, data1[data1_x_bin], data1[Target], cv  = cv_split)\ngrid_soft.fit(data1[data1_x_bin], data1[Target])\n\n#########print(\"Soft Voting w\/Tuned Hyperparameters Training w\/bin score mean: {:.2f}\". format(grid_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting w\/Tuned Hyperparameters Test w\/bin score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting w\/Tuned Hyperparameters Test w\/bin score 3*std: +\/- {:.2f}\". format(grid_soft_cv['test_score'].std()*100*3))\nprint('-'*10)","c6a376f1":"#prepare data for modeling\nprint(data_val.info())\nprint(\"-\"*10)\n#data_val.sample(10)\n\ndata_val['Survived'] = mytree(data_val).astype(int)\ndata_val['Survived'] = grid_hard.predict(data_val[data1_x_bin])\n\nsubmit = data_val[['PassengerId','Survived']]\nsubmit.to_csv(\"..\/working\/submit.csv\", index=False)\n\nprint('Validation Data Distribution: \\n', data_val['Survived'].value_counts(normalize = True))\nsubmit.sample(10)","d080d4a5":"****Split training and testing data****","e3404948":"Model data","29bae97b":"Correlation","d4232026":"****Perform Exploratory Analysis with Statistics****","eea4e2eb":"TUNE WITH FEATURE SELECTION","47fe3e31":"Graph plotting","523f0fc9":"****double check clean data****","ee78d08f":"EVALUATE MODEL PERFORMANCE","7b4df0c1":"MODEL PERFORMANCE WITH CROSS VALIDATION  ::::::: TUNE MODEL WITH HYPER PARAMETER"}}