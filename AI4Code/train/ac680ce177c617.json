{"cell_type":{"e764a487":"code","ed4289dc":"code","0a451638":"code","f118c48b":"code","6c87b80d":"code","b2f422eb":"code","ef312420":"code","1155e709":"code","e776b101":"code","793120a6":"code","a6fabe2f":"code","1029529f":"code","feaa1bfe":"code","ef8d17e0":"code","90bda036":"code","e6effb47":"code","5c4cc77e":"code","431d3384":"code","4af57aa0":"code","e0756971":"code","509d8b1d":"code","6c2173ee":"code","279704e0":"code","b88831b0":"code","0f6fb1cd":"code","ba9ccec9":"code","40618a5c":"code","6a99ca63":"code","304d6c6b":"code","1f828ab4":"code","30895199":"code","447521b5":"code","c678da64":"markdown","651b2f67":"markdown","eff16860":"markdown","4793b0fe":"markdown","08755154":"markdown","e3dcbd33":"markdown","df9f25f0":"markdown","89b33848":"markdown","713ee529":"markdown","25e423e6":"markdown","e4d3748c":"markdown","3530f8b4":"markdown","6b14723f":"markdown","5341149b":"markdown","7ba1cc74":"markdown","2effedb9":"markdown","5056381e":"markdown","bd996f1d":"markdown","894605ee":"markdown","cc3ab7bd":"markdown"},"source":{"e764a487":"data_url = 'https:\/\/raw.githubusercontent.com\/pkmklong\/Breast-Cancer-Wisconsin-Diagnostic-DataSet\/master\/data.csv'","ed4289dc":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set()\nimport matplotlib.pyplot as plt","0a451638":"df = pd.read_csv(data_url)\ndf.head()","f118c48b":"df.columns","6c87b80d":"sns.countplot(df['diagnosis'])\nplt.show()","b2f422eb":"df.drop(['Unnamed: 32'], axis = 1, inplace = True)","ef312420":"df.head()","1155e709":"df.drop(['id'], axis = 1, inplace = True)","e776b101":"df.head()","793120a6":"df.columns","a6fabe2f":"X = df.iloc[:, 1:].values\ny = df['diagnosis'].values","1029529f":"X.shape","feaa1bfe":"y.shape","ef8d17e0":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state= 0)","90bda036":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","e6effb47":"from sklearn.decomposition import PCA\n\npca = PCA(n_components = 1)\n\nX_train = pca.fit_transform(X_train)\nX_test = pca.transform(X_test)","5c4cc77e":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(max_depth = 2, random_state = 0)\nclf.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = clf.predict(X_test)","431d3384":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint('Accuracy -> '+ str(accuracy_score(y_test, y_pred)))","4af57aa0":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state= 0)","e0756971":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","509d8b1d":"from sklearn.manifold import TSNE","6c2173ee":"tsne = TSNE(n_components = 2, random_state = 0)","279704e0":"tsne_obj = tsne.fit_transform(X_train)","b88831b0":"tsne_df = pd.DataFrame({'X' : tsne_obj[:,0],\n                       'Y' : tsne_obj[:,1],\n                        'classification' : y_train\n                       })","0f6fb1cd":"tsne_df.head()","ba9ccec9":"tsne_df['classification'].value_counts()","40618a5c":"plt.figure(figsize = (10,10))\nsns.scatterplot(x = 'X', y = 'Y', data = tsne_df)\nplt.show()","6a99ca63":"plt.figure(figsize = (10,10))\nsns.scatterplot(x = \"X\", y = 'Y', hue = 'classification', legend = 'full', data = tsne_df)\nplt.show()","304d6c6b":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state= 0)\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","1f828ab4":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n\nlda = LDA(n_components = 1)\nX_train = lda.fit_transform(X_train, y_train)\nX_test = lda.transform(X_test)","30895199":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(max_depth = 2, random_state = 0)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)","447521b5":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\nprint('Accuracy -> ' + str(accuracy_score(y_test, y_pred)))","c678da64":"PCA has no concern with the class labels. It summarizes the feature set without considering the output. PCA tries to find the directions of the maximum variance in the dataset. In a high cardinality feature set, there are possibilities of duplicate features which would add redundancy to the dataset, increase the computation cost and add unneccessary model complexity. **The role of PCA is to find such highly correlated or duplicate features and to come up with a new feature set where there is minimum correlation between the features or in other words feature set with maximum variance between the features.**","651b2f67":"# What's the difference from PCA?\nLDA tries to reduce the dimensionality by taking into consideration the information that discriminates the output classes. LDA tries to find the decision boundary around each cluster of class. <br><br>\nIt projects the data points to new dimension in a way that the clusters are as seperate from each other as possible and individual elements within a class are as close to the centroid as possible. <br><br>\n**In other words, the inter-class seperability is increased in LDA. Intra-class seperability is reduced.** <br>\nThe new dimensions are the linear discriminants of the feature set.","eff16860":"#### Evaluation","4793b0fe":"### Conclusion\n","08755154":"## t-SNE (t-Distributed Stochastic Neighbor)","e3dcbd33":"We have obtained the plot of the data points but are unable to segregate. Let's introduce hue","df9f25f0":"### Setting up Dataset for the notebook","89b33848":"# Let's Begin -> PCA, t-SNE & LDA","713ee529":"We are ready to dive in to perform dimensionality reduction as our dataset is in the format we wanted.","25e423e6":"## LDA (Linear Discriminant Analysis)","e4d3748c":"### Feature Scaling","3530f8b4":"### Performing LDA (Linear Discriminant Analysis)","6b14723f":"In this notebook, I'll walk through the dimensionality reduction techniques namely PCA, t-SNE and LDA. <br>\nThe dataset used is **Breast Cancer Winconsin Dataset**. Python Library Used -> **Sklearn**. <br>\n## If you liked my work, please support my efforts my giving an UPVOTE & please add your valuable comments in the comment section.","5341149b":"## PCA (Principal Component Analysis)","7ba1cc74":"**NOTE->** Make sure you don't reuse the X_train and X_test as they were transformed. We'll resume from X and y.","2effedb9":"# Welcome! Let's Discuss Dimensionality Reduction Techniques using Python & Sklearn","5056381e":"# If you liked the Notebook, please support my work by giving an UPVOTE. And please add your valuable comments in the comment section.","bd996f1d":"#### Training and Making Predictions","894605ee":"#### Let's perform classification using RandomForestClassifier","cc3ab7bd":"#### Performance Evaluation"}}