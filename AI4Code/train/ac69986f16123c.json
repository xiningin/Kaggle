{"cell_type":{"07f5be73":"code","c3f5b3ee":"code","6e83f173":"code","6b435a08":"code","dc3140e3":"code","b11bbd9b":"code","54804563":"code","b856d546":"code","28a3ae37":"code","41bff4dd":"code","b8547e40":"code","2cd14aa6":"code","b703e0b8":"code","15313ef6":"code","f439598a":"code","61f11d79":"code","46b71627":"code","ad37fe0f":"code","8f532e57":"code","d6c97b97":"code","0d2efbe1":"code","5ec6b9b3":"code","c9c29786":"code","f704ab7a":"markdown","7e10b291":"markdown","648387c3":"markdown","5d008e16":"markdown","2a7b27f8":"markdown","0ff27747":"markdown","dea0b04a":"markdown","877d8a15":"markdown","794b0846":"markdown"},"source":{"07f5be73":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split,cross_val_score,StratifiedKFold\nfrom sklearn.metrics import confusion_matrix,classification_report,precision_score, recall_score, f1_score, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom bayes_opt import BayesianOptimization","c3f5b3ee":"df = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')","6e83f173":"df.head()","6b435a08":"df['target'].value_counts()","dc3140e3":"df.isna().sum()","b11bbd9b":"X = df.iloc[:,:-1]\ny = df.target\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,stratify=y,random_state=42)\n","54804563":"rf = RandomForestClassifier()","b856d546":"rf.fit(X_train,y_train)","28a3ae37":"preds = rf.predict(X_test)","41bff4dd":"print(classification_report(preds,y_test))","b8547e40":"print(\"Precision:{}\".format(precision_score(preds,y_test)))\nprint(\"Recall:{}\".format(recall_score(preds,y_test)))\nprint(\"F1 Score:{}\".format((f1_score(preds,y_test))))","2cd14aa6":"rf.get_params()","b703e0b8":"def stratified_kfold_score(clf,X,y,n_fold):\n    X,y = X.values,y.values\n    strat_kfold = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=1)\n    accuracy_list = []\n\n    for train_index, test_index in strat_kfold.split(X, y):\n        x_train_fold, x_test_fold = X[train_index], X[test_index]\n        y_train_fold, y_test_fold = y[train_index], y[test_index]\n        clf.fit(x_train_fold, y_train_fold)\n        preds = clf.predict(x_test_fold)\n        accuracy_test = accuracy_score(preds,y_test_fold)\n        accuracy_list.append(accuracy_test)\n\n    return np.array(accuracy_list).mean()","15313ef6":"def bo_params_rf(max_samples,n_estimators,max_features):\n    \n    params = {\n        'max_samples': max_samples,\n        'max_features':max_features,\n        'n_estimators':int(n_estimators)\n    }\n    clf = RandomForestClassifier(max_samples=params['max_samples'],max_features=params['max_features'],n_estimators=params['n_estimators'])\n    score = stratified_kfold_score(clf,X_train, y_train,5)\n    return score","f439598a":"rf_bo = BayesianOptimization(bo_params_rf, {\n                                              'max_samples':(0.5,1),\n                                                'max_features':(0.5,1),\n                                              'n_estimators':(100,200)\n                                             })","61f11d79":"results = rf_bo.maximize(n_iter=200, init_points=20,acq='ei')","46b71627":"params = rf_bo.max['params']\nparams['n_estimators']= int(params['n_estimators'])\nprint(params)","ad37fe0f":"rf_v1 = RandomForestClassifier(max_samples=params['max_samples'],max_features=params['max_features'],n_estimators=params['n_estimators'])","8f532e57":"rf_v1.fit(X_train,y_train)","d6c97b97":"preds = rf_v1.predict(X_test)","0d2efbe1":"print(classification_report(preds,y_test))","5ec6b9b3":"print(\"Precision:{}\".format(precision_score(preds,y_test)))\nprint(\"Recall:{}\".format(recall_score(preds,y_test)))\nprint(\"F1 Score:{}\".format((f1_score(preds,y_test))))","c9c29786":"cv = StratifiedKFold(n_splits=5, random_state=123, shuffle=True)\nscores = []\n    \nfor (train, test), i in zip(cv.split(X, y), range(5)):\n    rf_v1.fit(X.iloc[train], y.iloc[train])\n    preds = rf_v1.predict(X.iloc[test])    \n    accuracy = accuracy_score(preds,y.iloc[test])\n    scores.append(accuracy)\n\ndf_val= pd.DataFrame(scores, columns=['Accuracy Test'])\nprint(\"KFold validation mean accuracy on test set : {}\".format(df_val['Accuracy Test'].mean()))","f704ab7a":"# Model","7e10b291":"# Train-test split","648387c3":"# WIP\n\n1) A note on mathematical formulation of various acquistion functions <br>","5d008e16":"# Dataset\n\nRef: https:\/\/archive.ics.uci.edu\/ml\/datasets\/heart+Disease","2a7b27f8":"The hyperparameters and their default values for randomforest classifier are:","0ff27747":"# KFold validation","dea0b04a":"# Problem Statement\n\nTo predict the presence of heart disease in a patient, with the personal and medical informations related to the patient.","877d8a15":"Scikit-learn has a very good documentation about each hyperparameter.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\n\n\nFor understanding how bayesian optimization works,we will finetune for 'max_samples','n_estimators' and 'max_features' hyperparameters.","794b0846":"# Hyperparamete tuning\n\n\nReference: https:\/\/proceedings.neurips.cc\/paper\/2012\/file\/05311655a15b75fab86956663e1819cd-Paper.pdf\n\nMachine learning models have parameters and hyper parameters. Parameters are learnt from the data and hyperparameters are pre-defined for a model. For example, in linear\/logistic regression, the coefficients and biases are the parameters that is being optimised while training, whereas learning rate is one hyperparameter that is set to a constant before training the model. The process of finding the best hyperparameters from a range of values is called hyperparameter tuning.\n\n## Bayesian optimization for hyperparameter tuning\n\n\nBayesian optimization (BO) is an automated procedure for hyperparameter tuning. BO uses gaussian process to model mean and variance of the target function.\n\n### Gaussian process\n\nGuassian process is a prior distribution on functions, and in the case of hyperparameter tuning it will be of the form:\n\n$\n\\begin{align}\nf:\\chi \\rightarrow {\\mathbb{R}} \n\\end{align}\n$ <br>\n<br>\nWhere $\\chi$ is the space of hyperparameter set we are tuning, and $f$ is the target function decided based on the criteria on which the hyperparameters needs to be tuned. For a gaussian process, $p(f|\\chi)$ follows a normal distribution. In our problem, we define mean accuracy from five fold cross validation as the target function. \n\n### Acquisition function\n\nIt is assumed that the function $f(x)$ is drawn from a gaussain process prior and our observation is of the form ${x_i,y_i}$, where <br>\n\n$ y_i \\thicksim N(f(x_i), \u03bd) $ , $\u03bd$ is the variance of noise introduced into the function observations.\n\nThis prior and the data induce a posterior over functions,the acquisition function, $a:\\chi \\rightarrow {\\mathbb{R^{+}}} \n$, determines what points in $\\chi$ to be evaluated next.\n\nThere are different choices for acquisition functions, mainly <br>\n\n1) Upper Confidence Bounds method <br>\n2) Expected improvement method <br>\n3) Probability Of Improvement method <br>\n\n\nAcquisiton function follows a greedy approach to find the optimum hyperparameters in each step, and with a fixed step of iteration, we are  expected to get the suitable hyperparameters for our model.\n\n\n\n\n"}}