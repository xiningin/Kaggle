{"cell_type":{"1b66f804":"code","e433807f":"code","8b108889":"code","a54ebfb4":"code","d5456cab":"code","47d9462f":"code","ce561b70":"code","e921db65":"code","f096e62c":"code","605a0fc8":"code","bfb9bb1c":"code","2d250c5f":"code","ae85d1d3":"code","ba5d67a5":"code","47ecb619":"code","794b57e5":"code","c89e22a0":"markdown","5a7afde1":"markdown","39a0a457":"markdown","31b3da9b":"markdown"},"source":{"1b66f804":"import cv2\nfrom matplotlib import pyplot as plt\nimport pandas as pd \n","e433807f":"train_metadata = pd.read_csv(\"..\/input\/tensorflow-great-barrier-reef\/train.csv\")\ntrain_metadata[train_metadata.annotations!=\"[]\"]","8b108889":"from ast import literal_eval\n\ndef load_image(video_id, image_id):\n    path = f\"..\/input\/tensorflow-great-barrier-reef\/train_images\/video_{video_id}\/{image_id}.jpg\"\n    img = cv2.imread(path)\n    return img\n\ndef plot_image(img):\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n    plt.imshow(img)\n    return plt.show()\n\ndef parse_annotations(annotations):\n    return literal_eval(annotations)\n\ndef load_image_with_annotations(video_id, image_id, annotations):\n    img = load_image(video_id, image_id)\n    for ret in parse_annotations(annotations):\n        cv2.rectangle(img,\n                      (ret['x'], ret['y']),\n                      (ret['x'] + ret['width'], ret['y'] + ret['height']),\n                      (0,0,255),\n                      2)\n    return img\n\nplot_image(load_image(0, 16))\nimg = load_image_with_annotations(\n                            train_metadata.video_id.iloc[16],\n                            train_metadata.video_frame.iloc[16],\n                            train_metadata.annotations.iloc[16],\n                            )\nfigsize = (16,8)\nplt.figure(figsize=figsize)\nplot_image(img)\n","a54ebfb4":"from IPython.display import clear_output\n\nplt.figure(figsize=(16,8))\nfor i in train_metadata[train_metadata.annotations!=\"[]\"].index:\n    img = load_image_with_annotations(\n                            train_metadata.video_id.iloc[i],\n                            train_metadata.video_frame.iloc[i],\n                            train_metadata.annotations.iloc[i],\n                            )\n    plot_image(img)\n    clear_output(wait=True)","d5456cab":"shapes = []\n\nfor i in train_metadata.index:\n    img = load_image(train_metadata.video_id.iloc[i], train_metadata.video_frame.iloc[i])\n    shapes.append(img.shape)\n    \nprint(set(shapes))","47d9462f":"import torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n# load a model pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)","ce561b70":"model","e921db65":"# replace the classifier with a new one, that has\n# num_classes which is user-defined\nnum_classes = 2  # 1 class (starfish) + background\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)","f096e62c":"from torch.utils.data import Dataset\nfrom PIL import Image\nimport numpy as np\nimport torch\nimport os\n\nclass StarfishDataset(torch.utils.data.Dataset):\n    def __init__(self, transforms=None):\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.imgs = list(sorted(os.listdir(\"..\/input\/tensorflow-great-barrier-reef\/train_images\/video_0\")))\n        #self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n\n    def __getitem__(self, idx):\n        # load images and masks\n        img_path = os.path.join(\"..\/input\/tensorflow-great-barrier-reef\/train_images\/video_0\", self.imgs[idx])\n        annotation = parse_annotations(train_metadata[\"annotations\"].iloc[idx])\n        print(annotation)\n        img = Image.open(img_path).convert(\"RGB\")\n        #img = torch.as_tensor(img, dtype=torch.float32)\n        # get bounding box coordinates for each mask\n        num_objs = len(annotation)\n        boxes = []\n        for annotations in annotation:\n            boxes.append((annotations[\"x\"], annotations[\"y\"], annotations[\"x\"]+annotations[\"width\"], annotations[\"y\"]+annotations[\"height\"]))\n\n        # convert everything into a torch.Tensor\n        if len(annotation) == 0:\n            boxes = [(1, 1, 2, 2)]\n            labels = torch.zeros((1,), dtype=torch.int64)\n        else:\n            labels = torch.ones((num_objs,), dtype=torch.int64)\n        \n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        boxes = boxes.squeeze()\n        labels = labels.squeeze()\n        # there is only one class\n\n        image_id = torch.tensor([idx])\n        #area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        #target[\"masks\"] = masks\n        target[\"image_id\"] = image_id\n        #target[\"area\"] = area\n\n        if self.transforms is not None:\n            img = self.transforms(img)\n        print(img)\n        print(target)\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)\n    \n","605a0fc8":"import torchvision.transforms as T\n\ndef get_transform(train):\n    transforms = []\n    transforms.append(T.ToTensor())\n#     if train:\n#         transforms.append(T.RandomHorizontalFlip(0.5))\n    return T.Compose(transforms)","bfb9bb1c":"dataset = StarfishDataset(transforms=get_transform(train=True))\ndata_loader = torch.utils.data.DataLoader(\n dataset, batch_size=1, shuffle=True, num_workers=0)\n\n# For Training\nimages,targets = next(iter(data_loader))\nimages = list(image for image in images)\n\ntargets = [{k: v for k, v in targets.items()}]\noutput = model(images,targets)   # Returns losses and detections\n# For inference\nmodel.eval()\nx = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\npredictions = model(x)           # Returns predictions","2d250c5f":"predictions","ae85d1d3":"!ls ..\/input\/tensorflow-great-barrier-reef\/train_images\/vide","ba5d67a5":"train_with_annotations = train_metadata[train_metadata.annotations!=\"[]\"]","47ecb619":"\n\ndata = []\nall_labels = []\nall_bboxes = []\nimage_id = []\n# loop over the rows\nfor idx, row in train_with_annotations.iterrows():\n    bboxes = []\n    labels = []\n\n    image = load_image(row[\"video_id\"], row[\"video_frame\"])\n    (h, w) = image.shape[:2]\n    # scale the bounding box coordinates relative to the spatial\n    # dimensions of the input image\n#     startX = float(startX) \/ w\n#     startY = float(startY) \/ h\n#     endX = float(endX) \/ w\n#     endY = float(endY) \/ h\n    # load the image and preprocess it\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = cv2.resize(image, (224, 224))\n    # update our list of data, class labels, bounding boxes, and\n    # image paths\n    data.append(image)\n    annotations = parse_annotations(row[\"annotations\"]) \n\n    for annotation in annotations:\n        labels.append(1)\n        bboxes.append((annotations[\"x\"], annotations[\"y\"], annotations[\"x\"]+annotations[\"width\"], annotations[\"y\"]+annotations[\"height\"]))\n    \n    image_id.append(row[\"image_id\"])\n    all_bboxes.append(bboxes)\n    all_labels.append(labels)\n    imagePaths.append(imagePath)\n    break","794b57e5":"data = np.array(data, dtype=\"float32\")\nlabels = np.array(labels)\nbboxes = np.array(bboxes, dtype=\"float32\")\nimagePaths = np.array(imagePaths)\n\n(trainImages, testImages) = torch.tensor(trainImages), torch.tensor(testImages)\n\n\n# create data loaders\ntrainDS = CustomTensorDataset((trainImages, trainLabels, trainBBoxes))\nbatch_size = 8\n\n","c89e22a0":"# Section 3: Submit Test predictions","5a7afde1":"metadata","39a0a457":"# Section 1: Exploratory Analysis\n","31b3da9b":"# Section 2: Simple model"}}