{"cell_type":{"be26b427":"code","73bc03ad":"code","619868af":"code","10427033":"code","68d8a195":"code","93e1a961":"code","7120f1e0":"code","56c7295e":"code","1a164b6a":"code","5f81479d":"code","d0e546be":"code","8bde848f":"code","005b21c8":"code","95ec7eb4":"code","72cee640":"code","5e240252":"code","452c6e8e":"code","8a7b907a":"code","894765a7":"code","f392c9bb":"code","40ec0784":"code","5c559f82":"code","5453487f":"code","4fad6d4d":"code","b4413f2c":"code","9e5fd9f9":"code","89688646":"code","0784ff49":"code","3916d821":"code","2a999974":"code","29b6181d":"code","9f767fb9":"code","73e65499":"markdown","9e7c84fb":"markdown","14dc2836":"markdown","617b6e72":"markdown","31c526db":"markdown","cd5656c6":"markdown","c697bfec":"markdown","a2dd4fa7":"markdown","f9712d1d":"markdown","a5ae3b1d":"markdown","e566e25c":"markdown","d1844b0e":"markdown","cb896273":"markdown","8c028bf9":"markdown","094c487a":"markdown","e3e2cc66":"markdown","b43c522e":"markdown","b9a8bc5f":"markdown","fd632fa0":"markdown","54ab7919":"markdown","b9de3ee4":"markdown","462dde06":"markdown","4d8dccb5":"markdown"},"source":{"be26b427":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\n\n#impore re for regular expression\nimport re\n\n# import punctuations\nfrom string import punctuation\n\n#importing stopwords\nfrom nltk.corpus import stopwords\n\n#import the tokenizer\nfrom nltk.tokenize import RegexpTokenizer\n\n#importing the Stemmer\nfrom nltk.stem import PorterStemmer,SnowballStemmer\n\n#importing the Lemmatizer\nfrom nltk.stem import WordNetLemmatizer\n\n# import wordcloud for text visualization\nfrom wordcloud import WordCloud","73bc03ad":"import chardet\nwith open(\"..\/input\/sms-spam-collection-dataset\/spam.csv\" , \"rb\") as f:\n    enc = chardet.detect(f.read(1000000))\n    \nprint(enc)","619868af":"data = pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\" , encoding=\"Windows-1252\")\ndata.head()","10427033":"print(f\"Shape of Data: {data.shape}\")","68d8a195":"data.isna().sum()","93e1a961":"# P.S: The display keyword only works in the jupyter notebook enviroment and would raise an error in an actual python file\ndisplay(data[data[\"Unnamed: 2\"].notnull()].head())\ndisplay(data[data[\"Unnamed: 3\"].notnull()].head())\ndisplay(data[data[\"Unnamed: 4\"].notnull()].head())","7120f1e0":"#for every line where feature \"Unnamed: 2\" is not_null, we add Feature \"Unnamed: 2\" to \"Text\" feature\ndata[\"v2\"][data[\"Unnamed: 2\"].notna()] = data[\"v2\"][data[\"Unnamed: 2\"].notna()] + data[\"Unnamed: 2\"][data[\"Unnamed: 2\"].notna()]\n#for every line where feature \"Unnamed: 3\" is not_null, we add Feature \"Unnamed: 3\" to \"Text\" feature\ndata[\"v2\"][data[\"Unnamed: 3\"].notna()] = data[\"v2\"][data[\"Unnamed: 3\"].notna()] + data[\"Unnamed: 3\"][data[\"Unnamed: 3\"].notna()]\n#for every line where feature \"Unnamed: 4\" is not_null, we add Feature \"Unnamed: 4\" to \"Text\" feature\ndata[\"v2\"][data[\"Unnamed: 4\"].notna()] = data[\"v2\"][data[\"Unnamed: 4\"].notna()] + data[\"Unnamed: 4\"][data[\"Unnamed: 4\"].notna()]\n#drop feature \"A\",\"B\", and \"C\"\ndata.drop([\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\"] , axis=1, inplace=True)","56c7295e":"data.columns = [\"label\",\"text\"]#rename columns\ndata.head()","1a164b6a":"data[\"label\"].value_counts().plot(kind=\"bar\")\nplt.show()","5f81479d":"fig , ax = plt.subplots(nrows=6 , ncols=1,figsize=(10,50))\ndef word_length_count(text):\n    return len(text.split())\ndata[\"word_length\"] = data[\"text\"].apply(word_length_count)\nsns.kdeplot(data[\"word_length\"][data[\"label\"] == \"ham\"] , shade=True , label=\"ham\" , ax = ax[0])\nsns.kdeplot(data[\"word_length\"][data[\"label\"] == \"spam\"] , shade=True , label=\"spam\" , ax=ax[0])\nax[0].set_title(\"Distribution of Words Count\", fontsize=14)\nax[0].set_xlabel(\"Number of Words in Text\" , fontsize=14)\nax[0].set_ylabel(\"Frequency Rate\" , fontsize=14)\n\ndef char_length_count(text):\n    return len(text)\ndata[\"characters_count\"] = data[\"text\"].apply(char_length_count)\nsns.kdeplot(data[\"characters_count\"][data[\"label\"] == \"ham\"] , shade=True , label=\"ham\" , ax = ax[1])\nsns.kdeplot(data[\"characters_count\"][data[\"label\"] == \"spam\"] , shade=True , label=\"spam\" , ax=ax[1])\nax[1].set_title(\"Distribution of Characters Count\", fontsize=14)\nax[1].set_xlabel(\"Number of Characters in Text\" , fontsize=14)\nax[1].set_ylabel(\"Frequency Rate\" , fontsize=14)\n\ndef punct_count(text):\n    return len([char for char in text if char in punctuation])\ndata[\"punctuation_count\"] = data[\"text\"].apply(punct_count)\nsns.kdeplot(data[\"punctuation_count\"][data[\"label\"] == \"ham\"] , shade=True , label=\"ham\" , ax = ax[2])\nsns.kdeplot(data[\"punctuation_count\"][data[\"label\"] == \"spam\"] , shade=True , label=\"spam\" ,ax = ax[2])\nax[2].set_title(\"Distribution of Punctuations Count\", fontsize=14)\nax[2].set_xlabel(\"Number of Punctuations in Text\" , fontsize=14)\nax[2].set_ylabel(\"Frequency Rate\" , fontsize=14)\n\ndef numbers_count(text):\n    return len([char for char in text if char.isnumeric()])\ndata[\"numbers_count\"] = data[\"text\"].apply(numbers_count)\nsns.kdeplot(data[\"numbers_count\"][data[\"label\"] == \"ham\"] , shade=True , label=\"ham\" , ax = ax[3])\nsns.kdeplot(data[\"numbers_count\"][data[\"label\"] == \"spam\"] , shade=True , label=\"spam\",ax = ax[3])\nax[3].set_title(\"Distribution of Numerical Characters Count\", fontsize=14)\nax[3].set_xlabel(\"Number of Numerical String in Text\" , fontsize=14)\nax[3].set_ylabel(\"Frequency Rate\" , fontsize=14)\n\ndef stopwords_counts(text):\n    return len([word for word in text.split() if word in stopwords.words(\"english\")])\ndata[\"stopwords_count\"] = data[\"text\"].apply(stopwords_counts)\nsns.kdeplot(data[\"stopwords_count\"][data[\"label\"] == \"ham\"] , shade=True , label=\"ham\" , ax = ax[4])\nsns.kdeplot(data[\"stopwords_count\"][data[\"label\"] == \"spam\"] , shade=True , label=\"spam\" , ax = ax[4])\nax[4].set_title(\"Distribution of Stopwords Count\", fontsize=14)\nax[4].set_xlabel(\"Number of Stopwords in Text\" , fontsize=14)\nax[4].set_ylabel(\"Frequency Rate\" , fontsize=14)\n\ndef non_stopwords_counts(text):\n    return len([word for word in text.split() if word not in stopwords.words(\"english\")])\ndata[\"non_stopwords_count\"] = data[\"text\"].apply(non_stopwords_counts)\nsns.kdeplot(data[\"non_stopwords_count\"][data[\"label\"] == \"ham\"] , shade=True , label=\"ham\" , ax = ax[5])\nsns.kdeplot(data[\"non_stopwords_count\"][data[\"label\"] == \"spam\"] ,  shade=True , label=\"spam\" , ax = ax[5])\nax[5].set_title(\"Distibution of Non-Stopwords Count\", fontsize=14)\nax[5].set_xlabel(\"Number of Non-Stopwords in Text\" , fontsize=14)\nax[5].set_ylabel(\"Frequency Rate\" , fontsize=14)\nplt.show()","d0e546be":"for _ in data[\"text\"]:\n    print(_)","8bde848f":"#change text to lowercase\ndata[\"text\"] = data[\"text\"].str.lower()","005b21c8":"display(data[data[\"text\"].str.contains(\"http\")].head())\ndisplay(data[data[\"text\"].str.contains(\"www\")].head())","95ec7eb4":"# write a function to remove web links\ndef remove_html(text):\n    text = re.sub(r\"http:\/\/.+\\.com\" , \"\" , text)\n    text = re.sub(r\"http:\/\/\\S+\" , \"\" , text)\n    text = re.sub(r\"http:\/\/[A-Za-z0-9.\/?= *&:-]+\" , \"\" , text)\n    text = re.sub(r\"http.+\\.com\" , \"\" , text)\n    text = re.sub(r\"http\/\/[A-Za-z0-9.\/?= *&:-]+\" , \"\" ,text)\n    text = re.sub(r\"www[.A-Za-z0-9\/+-]+\" , \"\" , text)\n    return text\ndata[\"text\"] = data[\"text\"].apply(remove_html)","72cee640":"#Remove numbers and words which are joined with numbers such that the number removal makes them useless\ndef remove_numbers(text):\n    text = re.sub(r\"[\u00e5\u00a3A-Za-z0-9]*\\d+[A-Za-z0-9]*\" , \"\" , text)\n    return text\ndata[\"text\"] = data[\"text\"].apply(remove_numbers)","5e240252":"# to view some datapoints with charcters such as \"\u00e5\"\ndata[data[\"text\"].str.contains(\"\u00e5\")].sample(5)","452c6e8e":"def remove_fancy_text(word):\n    if word.isascii() == False:\n        return \"\"\n    else:\n        return word\ndata[\"text\"] = data[\"text\"].apply(lambda x: \" \".join([remove_fancy_text(word) for word in x.split()]))","8a7b907a":"for _ in data[\"text\"]:\n    print(_)","894765a7":"data[data[\"text\"].str.contains(\"@\")].head()","f392c9bb":"def remove_mails(text):\n    text = re.sub(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+(\\.[a-zA-Z]{2,5})\" , \"\" , text)\n    return text\ndata[\"text\"] = data[\"text\"].apply(remove_mails)","40ec0784":"# view amils with \"@\" in them \ndata[data[\"text\"].str.contains(\"@\")]","5c559f82":"tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S=+')\ndata[\"text\"] = data[\"text\"].apply(lambda x : tokenizer.tokenize(x))","5453487f":"def remove_stopwords(text):\n    return [word for word in text if word not in stopwords.words(\"english\")]\ndata[\"text\"] = data[\"text\"].apply(remove_stopwords)","4fad6d4d":"def remove_puncts(text):\n    return [word for word in text if word not in punctuation]\ndata[\"text\"] = data[\"text\"].apply(remove_puncts)","b4413f2c":"data[\"text_length\"] = data[\"text\"].apply(lambda x: len(x))","9e5fd9f9":"data.head()","89688646":"data[data[\"text_length\"] > data[\"non_stopwords_count\"]]","0784ff49":"data[\"text\"] = data[\"text\"].apply(lambda x: [word for word in x if len(word) >= 2])","3916d821":"lemma = WordNetLemmatizer()\ndata[\"text\"] = data[\"text\"].apply(lambda x: \" \".join([lemma.lemmatize(word) for word in x]))","2a999974":"# stemmer = SnowballStemmer(language=\"english\")\n# data[\"text\"].apply(lambda x: \" \".join([stemmer.stem(word) for word in x]))","29b6181d":"spam_corpus = \"\"\nfor text in data[data[\"label\"] == \"spam\"][\"text\"]:\n    spam_corpus += \" \" + text\nham_corpus = \"\"\nfor text in data[data[\"label\"] == \"ham\"][\"text\"]:\n    ham_corpus += \" \" + text","9f767fb9":"spam_wordcloud = WordCloud(background_color=\"white\").generate(spam_corpus)\nham_wordcloud = WordCloud(background_color=\"white\").generate(ham_corpus)\nfig,ax= plt.subplots(nrows=1,ncols=2,figsize=(20,100))\nax[0].imshow(spam_wordcloud)\nax[0].set_title(\"Spam Text\" , fontsize=20)\nax[0].axis(False)\nax[0].grid(False)\nplt.savefig(\"Spam Text\")\nax[1].imshow(ham_wordcloud)\nax[1].set_title(\"Ham Text\" , fontsize=20)\nax[1].axis(False)\nax[1].grid(False)\nplt.show()","73e65499":"### Stemming","9e7c84fb":"### Stopwords Removal","14dc2836":"### Remove standalone text","617b6e72":"#### Let's take a look at the distribution in our Dataset","31c526db":"<div align=\"center\"><font size=6 color=\"red\">Spam <\/font><font size=6 color=\"green\"> & Ham<\/font><font size=6 color=\"black\"> Detection<\/font><\/div>","cd5656c6":"### Data Preprocessing","c697bfec":"After cleaning, we still have some text with \"@\" in them, we'll deal with this in punctattion removel","a2dd4fa7":"Let's take a look at the colums with lot's of missing values","f9712d1d":"removing standalone words like \"k\",\"a\",\"m\" and the likes","a5ae3b1d":"Text in the \"v2\" column and the other colums are all of the same text, this could have been caused by commas(,) in the text since we're reading as csv(comma seperated values), we can just add all the columns back together to make a single text","e566e25c":"\"text_length\" column should be less than or equal to \"non_stopwords_count\" and also similiar to it and if not so it should should only be greater than \"non_stopwords_counts\" by 1","d1844b0e":"There is a very HUGE imbalance in our dataset","cb896273":"#### Let's check is our text containg web links","8c028bf9":"### Remove Punctautions","094c487a":"Not intrested in stemming but the cell below will do stemming if uncommented","e3e2cc66":"### Lemmatization","b43c522e":"**OBSERVATIONS:**\n1. Spam Text has alot of numbers in them while Ham Text has comaparably low numbers in them, this is very logical\n2. Spam Text has an average of more words than Ham Text, this is also very logical since scammers with try to convince you and convey extra amount of information to intice unsuspecting people","b9a8bc5f":"If we look well, we'll see there are some datapoints with \"https\" | \"http\" and some with \"www\", we'll have to deal with this","fd632fa0":"#### Let's get rid of email addresses","54ab7919":"### Exploratory Data Analysis","b9de3ee4":"### Tokenization","462dde06":"The problem of missing value has been solved with that","4d8dccb5":"There are alot of instances with text like \"\u00e5\u00a3\". For example:\n\"winner!! as a valued network customer you have been selected to receivea \u00e5\u00a3900 prize reward! to claim call. claim code. valid hours only\"\n\n**Let's deal with that**"}}