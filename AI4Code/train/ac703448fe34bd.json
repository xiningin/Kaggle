{"cell_type":{"c2231a5c":"code","f87dede5":"code","c1434358":"code","907e57d2":"code","6e27cdec":"code","ee41485a":"code","37d55b39":"code","5ca9bcc0":"code","de353df4":"code","f0e68e88":"code","b975004f":"code","ab07fc66":"code","145f357a":"code","82960ab3":"code","0b34ce53":"code","609aac02":"code","3ce08f65":"code","dc8e67d5":"code","431858bb":"code","313553c1":"code","f5779622":"code","5393079e":"code","f2f58900":"code","4eb5a4fc":"code","1a310ee0":"code","d59f2c42":"code","398a2598":"code","c7375516":"code","707a15ba":"code","4e91de11":"code","25122e4d":"code","0a5c1aab":"code","e0100b1f":"code","b3c85c6f":"code","00d3f660":"code","a1dac811":"code","29181e72":"code","b4ff7229":"code","2232bea5":"code","aed56bd0":"code","934a90eb":"markdown","352d7ff8":"markdown","4be85bcd":"markdown","981feed2":"markdown","5f8c2db1":"markdown","7ef9787d":"markdown","2f6b64cf":"markdown","ad3b8ed7":"markdown","b698999e":"markdown","e98fd95a":"markdown","e303f19b":"markdown","4e0c66de":"markdown","e598c868":"markdown","cfae2491":"markdown","b816b616":"markdown","31010cdc":"markdown","9b144dd8":"markdown","cfc1a615":"markdown","f7fd39a5":"markdown","1b235258":"markdown","55f9726e":"markdown","1f427a7a":"markdown","a6a15042":"markdown","48c7ad6d":"markdown"},"source":{"c2231a5c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')","f87dede5":"test= pd.read_csv('..\/input\/song-popularity-prediction\/test.csv')\nsub = pd.read_csv('..\/input\/song-popularity-prediction\/sample_submission.csv')\nprint(\"Sample Submission Head(2)\")\ndisplay(sub.head(2))\nprint(\"Test Dataset Head(2)\")\ntest.head(2)","c1434358":"df= pd.read_csv('..\/input\/song-popularity-prediction\/train.csv')\ndf","907e57d2":"### Over all view of data \n#print('Data distribution of training set')\ndf.hist(figsize=(18,10), edgecolor='black',linewidth=2);","6e27cdec":"#print('Data distribution of test set')\ntest.hist(figsize=(18,10), edgecolor='black',linewidth=2);","ee41485a":"## Looking for correlation among columns\nplt.figure(figsize=(12,8))\nsns.heatmap(df.corr(),annot= True, cmap='Blues', fmt= \".1f\", linewidths=0.01);","37d55b39":"df.info()","5ca9bcc0":"print(\"unique elements in train set\")\ndisplay(df.nunique())\nprint(\"unique elements in test set\")\ntest.nunique()","de353df4":"## There are some missing values lets check them out\nplt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\n(df.isnull().mean()*100).plot(kind='bar')\nplt.title('Missing Data in train set', size=20)\n\nplt.subplot(1,2,2)\n(test.isnull().mean()*100).plot(kind='bar')\nplt.title('Missing Data in test set', size=20)\nplt.tight_layout();","f0e68e88":"print(\"Missing Data in Train set: \", (df.isnull().sum()).sum(), 'which is :',round(100*(df.isnull().sum()).sum()\/df.size,2) ,'Percentage')\nprint(\"Missing Data in Test set: \", (test.isnull().sum()).sum(), 'which is :',round(100*(test.isnull().sum()).sum()\/test.size,2) ,'Percentage')","b975004f":"color= ['grey', 'black']\nplt.figure(figsize=(12,7))\nsns.heatmap(df.isnull(), cmap= color, cbar=False)\nplt.title('Missing Data on Train Set Heatmap', fontsize=20)\nplt.xticks(fontsize=15);","ab07fc66":"## There are some missing values lets check them out\n# colour = sns.color_palette(\"crest\",2)\ncolour =['Black','Blue'] ## Creating a color palette for graph\n\nmissing_data = pd.DataFrame({\"Training\":df.isnull().mean()*100, \"Test\" : df.isnull().mean()*100}).drop('id')\nmissing_data.plot(kind='bar', stacked=True, color = colour,figsize=(12,5))\nplt.grid('on', linestyle='--', alpha=0.5)\nplt.title(\"Missing data in Dataset\", fontsize=20)\nplt.show();","145f357a":"df.describe().T.sort_values(by='std' , ascending = False)\\\n                     .style.background_gradient(cmap='Blues')\\\n                     .bar(subset=[\"max\"], color='#BB0000')\\\n                     .bar(subset=[\"mean\",], color='#5999ff')\\\n                     .bar(subset=[\"min\",], color='green')\\\n                     .bar(subset=['50%',], color= 'blue')","82960ab3":"test.describe().T.sort_values(by='std' , ascending = False)\\\n                     .style.background_gradient(cmap='Blues')\\\n                     .bar(subset=[\"max\"], color='#BB0000')\\\n                     .bar(subset=[\"mean\",], color='#5999ff')\\\n                     .bar(subset=[\"min\",], color='green')\\\n                     .bar(subset=['50%',], color= 'blue')\n","0b34ce53":"### Check for Duplicates\nprint(\"Duplicate entery in train dataset\",df.duplicated().sum())\nprint(\"Duplicate entery in test dataset\",test.duplicated().sum())","609aac02":"j=1\nplt.figure(figsize=(16,10))\nfor i in df:\n    if i != 'id':\n        plt.subplot(7,2,j)\n        sns.boxplot(df[i], palette='Blues')\n        plt.suptitle('Boxplot of train dataset', size=20)\n        j=j+1\n        plt.tight_layout()","3ce08f65":"j=1\nplt.figure(figsize=(16,10))\nfor i in test:\n    if i != 'id':\n        plt.subplot(7,2,j)\n        plt.suptitle('Boxplot of test dataset', size=20)\n        sns.boxplot(df[i], palette='Blues')\n        j=j+1\n        plt.tight_layout()","dc8e67d5":"TARGET = 'song_popularity'\nFEATURES = [col for col in df.columns if col not in ['id', TARGET]]\nRANDOM_STATE = 7","431858bb":"df_new = pd.concat([df[FEATURES], test[FEATURES]], axis=0)\n\ncat_features = [col for col in FEATURES if df_new[col].nunique() < 15]\ncont_features = [col for col in FEATURES if df_new[col].nunique() >= 15]\n\ndel df_new  ### Delete this new df to save memory\nprint(f'Total number of features: {len(FEATURES)}')\nprint(f'\\033[92mNumber of categorical features: {len(cat_features)}')\nprint(f'\\033[96mNumber of continuos features: {len(cont_features)}')\n\nplt.pie([len(cat_features), len(cont_features)], \n        labels=['Categorical', 'Continuos'],\n        colors=['#BFAE5A', '#f0cd51'],\n        textprops={'fontsize': 13},\n        autopct='%1.1f%%')\nplt.show()","313553c1":"ncols = 5\nnrows = int(len(cont_features) \/ ncols + (len(FEATURES) % ncols > 0))-1\n\nfig, axes = plt.subplots(nrows, ncols, figsize=(18, 10), facecolor='#EAEAF2')\n\nfor r in range(nrows):\n    for c in range(ncols):\n        col = cont_features[r*ncols+c]\n        sns.kdeplot(x= df[col], ax=axes[r, c], color='#69b1ff', label='Train data' , fill =True)\n        sns.kdeplot(x=test[col], ax=axes[r, c], color='black', label='Test data')\n        axes[r,c].legend()\n        axes[r, c].set_ylabel('')\n        axes[r, c].set_xlabel(col, fontsize=8)\n        axes[r, c].tick_params(labelsize=5, width=0.5)\n        axes[r, c].xaxis.offsetText.set_fontsize(4)\n        axes[r, c].yaxis.offsetText.set_fontsize(4)\nplt.show()","f5779622":"### Here we will check for relationship between cont features and popularity\nj=1\nplt.figure(figsize=(18,29))\nfor i in cont_features:\n    plt.subplot(5,2,j)\n    #sns.kdeplot(x= df[i], hue=df['song_popularity'] , fill =True)\n    sns.kdeplot(x= df[i], hue=df['song_popularity'] , palette=\"viridis\",multiple=\"stack\")\n    plt.title(i, fontsize=15)\n    j=j+1\nplt.suptitle(\"Reationship between Continuos feature and Song Popularity(TARGET)\", fontsize=20)\nplt.tight_layout(rect=[0,0,1,0.99])  ### After some trial and error :: recttuple (left, bottom, right, top), default: (0, 0, 1, 1)\nplt.plot();","5393079e":"if len(cat_features) == 0 :\n    print(\"No Categorical features\")\nelse:\n    ncols = 3\n    nrows = 1\n\n    fig, axes = plt.subplots(nrows, ncols, figsize=(18, 5))\n    for r in range(nrows):\n        for c in range(ncols):\n            col = cat_features[c]\n            sns.countplot(df[col],ax = axes[c] ,palette = \"viridis\", label='Train data')\n            sns.countplot(test[col],ax = axes[c] ,palette = \"magma\", label='Train data')\n            axes[c].legend()\n            axes[c].set_ylabel('')\n            axes[c].set_xlabel(col, fontsize=20)\n            axes[c].tick_params(labelsize=10, width=0.5)\n            axes[c].xaxis.offsetText.set_fontsize(4)\n            axes[c].yaxis.offsetText.set_fontsize(4)\n            plt.suptitle(\"Categorical Features\",fontsize=15)\n    plt.show()","f2f58900":"from sklearn import preprocessing\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler  ### To Standardise the data \nfrom sklearn.pipeline import Pipeline     ### To create pipeline\nfrom sklearn.impute import SimpleImputer ### To handle missing data\n\n### Will Create a 6 Model Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier","4eb5a4fc":"x= df[FEATURES]\ny= df[TARGET]","1a310ee0":"### Spliting data  \nx_train, x_test, y_train, y_test = train_test_split( x, y, test_size=0.33, random_state=7)","d59f2c42":"from sklearn.preprocessing import StandardScaler,OrdinalEncoder,LabelEncoder,RobustScaler","398a2598":"from sklearn.svm import LinearSVC, SVC\nfrom sklearn.neural_network import MLPClassifier","c7375516":"### Creating Pipeline\npipeline_lr=Pipeline([('imputer', SimpleImputer(strategy='mean')),\n                     ('scalar1',StandardScaler()),\n                     ('pca1',PCA(n_components=2)),\n                     ('lr_classifier',LogisticRegression(random_state=0))])\n\npipeline_dt=Pipeline([('imputer', SimpleImputer(strategy='mean')),('dt_classifier',DecisionTreeClassifier())])\n\npipeline_randomforest=Pipeline([('imputer', SimpleImputer(strategy='mean')),('rf_classifier',RandomForestClassifier())])\n\npipeline_xgb=Pipeline([('imputer', SimpleImputer(strategy='mean')),('scalar1',StandardScaler()),('xgb_classifier',XGBClassifier())])\n\npipeline_cat=Pipeline([('imputer', SimpleImputer(strategy='mean')),('scalar1',StandardScaler()),('cat_classifier',CatBoostClassifier())])\n\npipeline_lgbm =Pipeline([('imputer', SimpleImputer(strategy='mean')),('lgbm_classifier',LGBMClassifier())])\n\n### \npipeline_lgbm_median =Pipeline([('imputer', SimpleImputer(strategy='median')),('lgbm_classifier_median',LGBMClassifier())])\n\npipeline_lgbm_robust =Pipeline([('imputer', SimpleImputer(strategy='mean')),('scalar1',RobustScaler()),('lgbm_RobustScaler',LGBMClassifier())])\n\npipeline_svc =Pipeline([('imputer', SimpleImputer(strategy='mean')),('Support Vector Machine (Linear Kernel)',SVC(probability=True))])\n\npipeline_NN =Pipeline([('imputer', SimpleImputer(strategy='mean')),('scalar1',StandardScaler()),('Neural Network (MLPClassifier)',MLPClassifier())])\n\npipeline_NN_r =Pipeline([('imputer', SimpleImputer(strategy='mean')),('scalar1',RobustScaler()),('MLPClassifier Robust',MLPClassifier())])\n\n## Lets make the list of pipelines\npipelines = [pipeline_lr, pipeline_dt, pipeline_randomforest,pipeline_xgb, pipeline_cat, pipeline_lgbm,pipeline_lgbm_median, \n             pipeline_lgbm_robust, pipeline_svc,pipeline_NN, pipeline_NN_r ]\n\nbest_accuracy=0.0\nbest_classifier=0\nbest_pipeline=\"\"\n\n# Dictionary of pipelines and classifier types for ease of reference\npipe_dict = {0: 'Logistic Regression', 1: 'Decision Tree', 2: 'RandomForest' , 3: 'XGBoost', 4: 'CatBoost', 5: 'LightGBM' ,6: 'LGBM Median', 7: 'LGBM Robust',\n            8:'SVC', 9:'Neural Network', 10:'pipeline_NN_r'}\n\n# Fit the pipelines\nfor pipe in pipelines:\n    pipe.fit(x_train, y_train)","707a15ba":"for i,model in enumerate(pipelines):\n    print(\"{} Test Accuracy: {}\".format(pipe_dict[i],model.score(x_test,y_test)))","4e91de11":"for i,model in enumerate(pipelines):\n    roc= roc_auc_score(y_test, model.predict_proba(x_test)[:, 1])\n    print(\"{} Test Accuracy: {} ROC AUC{}\".format(pipe_dict[i],model.score(x_test,y_test), roc))\n","25122e4d":"for i,model in enumerate(pipelines):\n    if model.score(x_test,y_test)>best_accuracy:\n        best_accuracy=model.score(x_test,y_test)\n        best_pipeline=model\n        best_classifier=i\nprint('Classifier with best accuracy:{}'.format(pipe_dict[best_classifier]))\n\n######  ROC AUC  ###########\nk=0\nfor i,model in enumerate(pipelines):\n    roc= roc_auc_score(y_test, model.predict_proba(x_test)[:, 1])\n    if roc>k:\n        k=roc\n        best_pipeline=model\n        best_classifier=i\nprint('Classifier with best ROC AUC accuracy:{}'.format(pipe_dict[best_classifier]))","0a5c1aab":"## creating sub file\ntest_fin= test[FEATURES]\n\nlr_pred= pipelines[0].predict(test_fin)\ndt_pred= pipelines[1].predict(test_fin)\nrf_pred= pipelines[2].predict(test_fin)\nxgb_pred= pipelines[3].predict(test_fin)\ncat_pred= pipelines[4].predict(test_fin)\nlgbm_pred= pipelines[5].predict(test_fin)\n\nlr_submission = sub.copy()\nlr_submission['song_popularity'] = lr_pred\nlr_submission.to_csv(\"lr-subs.csv\",index=False)\n\n\ndt_submission = sub.copy()\ndt_submission['song_popularity'] = dt_pred\ndt_submission.to_csv(\"dt-subs.csv\",index=False)\n\n\nrf_submission = sub.copy()\nrf_submission['song_popularity'] = rf_pred\nrf_submission.to_csv(\"rf-subs.csv\",index=False)\n\nxgb_submission = sub.copy()\nxgb_submission['song_popularity'] = xgb_pred\nxgb_submission.to_csv(\"xgb-subs.csv\",index=False)\n\n\ncat_submission = sub.copy()\ncat_submission['song_popularity'] = cat_pred\ncat_submission.to_csv(\"cat-subs.csv\",index=False)\n\nlgbm_submission = sub.copy()\nlgbm_submission['song_popularity'] = lgbm_pred\nlgbm_submission.to_csv(\"lgbm-subs.csv\",index=False)\n","e0100b1f":"x_train_main= x_train.copy()\nx_test_main = x_test.copy()\n### Handling missing data with median\n\nimp = SimpleImputer(missing_values=np.nan, strategy='median')\nimp.fit(x_train_main)\nmy_array = imp.transform(x_train_main)\ntest_array = imp.transform(x_test_main)\nx_train_df = pd.DataFrame(my_array, columns=x_train.columns)\nx_test_df = pd.DataFrame(test_array,columns=x_test.columns)\n#X_train_full.describe()","b3c85c6f":"# Random Model\nfrom imblearn.ensemble import BalancedBaggingClassifier\n\nrm_forest = RandomForestClassifier(max_depth= 10, max_leaf_nodes=90, n_estimators=120)\nclassifier = BalancedBaggingClassifier(base_estimator=rm_forest,\n                                       n_estimators = 15,\n                                sampling_strategy='all',\n                                replacement=False,\n                                random_state=42)\nclassifier.fit(x_train_df, y_train)\nroc_auc_score(y_test, classifier.predict_proba(x_test_df)[:, 1])","00d3f660":"Stop\n\nparams_lgb = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    'subsample': 0.95312,\n    'learning_rate': 0.001635,\n    \"max_depth\": 3,\n    \"feature_fraction\": 0.2256038826485174,\n    \"bagging_fraction\": 0.7705303688019942,\n    \"min_child_samples\": 290,\n    \"reg_alpha\": 14.68267919457715,\n    \"reg_lambda\": 66.156,\n    \"max_bin\": 772,\n    \"min_data_per_group\": 177,\n    \"bagging_freq\": 1,\n    \"cat_smooth\": 96,\n    \"cat_l2\": 17,\n    \"verbosity\": -1,\n    'random_state':42,\n    'n_estimators':5000,\n    'colsample_bytree':0.1107\n    }","a1dac811":"# Light\nimport lightgbm as lgb\n\n#lgb parameters\nparams_lgb = {\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    'subsample': 0.95312,\n    'learning_rate': 0.001635,\n    \"max_depth\": 3,\n    \"feature_fraction\": 0.2256038826485174,\n    \"bagging_fraction\": 0.7705303688019942,\n    \"min_child_samples\": 290,\n    \"reg_alpha\": 14.68267919457715,\n    \"reg_lambda\": 66.156,\n    \"max_bin\": 772,\n    \"min_data_per_group\": 177,\n    \"bagging_freq\": 1,\n    \"verbosity\": -1,\n    'random_state':42,\n    'n_estimators':5000,\n    'colsample_bytree':0.1107\n    }\n    \n\nlgbm = lgb.LGBMClassifier(params=params_lgb,\n                      early_stopping_rounds=300,\n                      verbose_eval=False)\n\n\n\nlgbm.fit(x_train_df, y_train)\nroc_auc_score(y_test, classifier.predict_proba(x_test_df)[:, 1])","29181e72":"clf = lgb.LGBMClassifier()\nclf.fit(x_train_df, y_train)\nroc_auc_score(y_test, clf.predict_proba(x_test_df)[:, 1])","b4ff7229":"# Cat\n","2232bea5":"# XGB\n","aed56bd0":"### Stack","934a90eb":"<li style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:40px;color: #c1531f  \"> Pipelines:<\/li>\n\n<p style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:20px;color: #c1531f  \"> Pipelines are a simple way to keep your data preprocessing and modeling code organized. Specifically, a pipeline bundles preprocessing and modeling steps so you can use the whole bundle as if it were a single step.<\/p>\n\n><div class=\"alert alert-info\" role=\"alert\">\n><ul style=\"font-family:cursive;font-size:20px;color:  #2025bd\">\n><li ><b>Cleaner Code:<\/b> Accounting for data at each step of preprocessing can get messy. With a pipeline, you won't need to manually keep track of your training and validation data at each step.<\/li>\n><li ><b>Fewer Bugs:<\/b> There are fewer opportunities to misapply a step or forget a preprocessing step.<\/li>\n><li><b>Easier to Productionize:<\/b> It can be surprisingly hard to transition a model from a prototype to something deployable at scale. We won't go into the many related concerns here, but pipelines can help.<\/li>\n><li > <b>More Options for Model Validation:<\/b> You will see an example in the next tutorial, which covers cross-validation.<\/li>\n><\/ul>\n><\/div>","352d7ff8":"<li style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:40px;color:  #c1531f   \">About the dataset<\/li>\n","4be85bcd":"<li style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:20px;color: #c1531f  \"> Data distribution of train set<\/li>","981feed2":"# Lets Investigate data further with Descriptive Statistics","5f8c2db1":"<li style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:40px;color: #c1531f  \"> Checking Correlations<\/li>\n\n><div class=\"alert alert-info\" role=\"alert\">\n><ul style=\"font-family:cursive;font-size:20px;color:  #2025bd\">\n><li >Since we have only numerical (Float) type variables we can directly check correlation and obtain some more insights.<\/li>\n><\/ul>\n><\/div>","7ef9787d":"<li style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:60px;color: #c1531f; text-align: center; \"> Thank you !!<\/li>","2f6b64cf":"#### Note: Why I will be not perfroming Feature Scaling.\n* Transforming Data For Skewness : We need to correct skewness before model creation as it impact algo for both Reg and Classification\n* Some columns have very large range and STD in these case feature scaling might be benificial. But I won't be performing these as I will be using Tree based models mostly.\n* **Normalization:** Scale between 0 to 1. AKA `Min Max Scaler` [**Formula: Xnormal = (X - Xmin)\/(Xmax - Xmin)**]\n        from sklearn.preprocessing import MinMaxScaler\n* **Standardization:** This help us scale down the feature based on standard normal distribution (i.e. in this case **mean =0 and std=1**). Library name `StandardScaler` [**Formula z= (x-\u03bc)\/\u03c3**]\n        from sklearn.preprocessing import StandardScaler\n        Sample code\n        from sklearn.preprocessing import StandardScaler\n        scaling = StandardScaler()\n        scaled_numpy = scaling.fit_transform(x_train) ## This will return a numpy array\n        Scaled_df= pd.DataFrame(scaled_numpy, columns = ['Column_A','Column_B','Column_C']) ### columns accept list so we can pass FEATURES\n        \n### When to use `Normalization` and `Standardization`?\n* In ML or Dl which involves `Euclidean Distance` or `Gradient Descent` or any other distance we need to use feature scaling.\n* As this will make the computation faster as **range will reduced** so weather its GD finding **global minima** on parabolic curve or \n* ML models like KNN, K-Means Clustering, Linearn Regression(we also consider GD), Logistic Regression.\n* Normalization Mostly in Deep Learning Becasue we need to scale down the value bet 0 to 1. For Example in case of CNN (0 to 255 normalized to 0 to 1, now we can't use standardization as it will change the pixels of image)\n* Standardization Mostly for ML.","ad3b8ed7":"## Stacking with CAT, LGBM, NN Multiclassifier","b698999e":"<li style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:40px;color: #c1531f  \"> Data Visualization:<\/li>","e98fd95a":"<p style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:20px;color: #c1531f  \"> Classifier with best accuracy :Logistic Regression, followed by CatBoosting and LightGBM (all Without Hyperparameter Tuning)<\/p>\n","e303f19b":"`Self Note:` I tried to standardized different models in pipeline but it didn't create much impact. In next notebook try different transform like log, sqrt etc and target on skewness.","4e0c66de":"<li style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:30px;color: #c1531f  \"> Data insight from these hist plot:<\/li>\n\n><div class=\"alert alert-info\" role=\"alert\">\n><ul style=\"font-family:cursive;font-size:20px;color:  #2025bd\">\n><li ><b>audio_mode<\/b> and <b>song_popularity<\/b> are Binary.<\/li>\n><li >No Data Distribution is Gaussian\/Normal Distribution<\/li>\n><li >There are some Categorical Column like <b>key & time_signature<\/b><\/li>\n><li ><b>All Data are skewed to some extend<\/b>\n    ><li > To remove Skewness we can try following things<\/b>\n    <ul style=\"font-size:15px;\">\n        <li><b>Log Transform<\/b>It can be easily done via Numpy, just by calling the log() function on the desired column.<mark>np.log(df['energy'])<\/mark><\/li>\n      <li><b>Square Root Transform<\/b>The square root sometimes works great and sometimes isn\u2019t the best suitable option. In this case, I still expect the transformed distribution to look somewhat exponential, but just due to taking a square root the range of the variable will be smaller.We can apply a square root transformation via Numpy, by calling the sqrt()<mark>np.sqrt(df['energy'])<\/mark><\/li>\n        <li><b>Box-Cox Transform<\/b>To use it, your data must be positive \u2014 so that can be a bummer sometimes.we can import it from the Scipy library, but the check for the skew we need to convert the resulting Numpy array to a Pandas Series:<\/li>\n    <\/ul>\n><\/div>","e598c868":"# Model + pipeline Creation","cfae2491":"We need to perfrom transformation on **instrumentainess** as feature like this are `probably not good enough for its own`\n> **liveness,loudness & speechness** also heavily skewed","b816b616":"<li style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:30px;color: #c1531f  \"> Conclusion on Missing Data:<\/li>\n\n><div class=\"alert alert-info\" role=\"alert\">\n><ul style=\"font-family:cursive;font-size:20px;color:  #2025bd\">\n><li >Missing Data in Train set: 32187 which is : <b>5.36 Percentage of Trainset<\/b>.<\/li>\n><li >Missing Data in Test set: 7962 which is : <b>5.69 Percentage of Test set<\/b>.<\/li>\n><li>Not all features have missing values, though. We find them only in the first 8 columns (not counting the id). Consequently, those columns have around 10% missing values.<\/li>\n><li > <b>5-6% is quite a big chunk we need to inspect this and try to find out why data is missing.<\/b>\n    <ul>\n      <li>This could have be easy if we know how data was collected.<\/li>\n      <li>We can impute these data by\n        <ul>\n      <li>filling missing data with default value<\/li>\n      <li>or by filling data with mean<\/li>\n    <\/ul><\/li>\n    <\/ul> <\/li>\n><\/ul>\n><\/div>","31010cdc":"<li style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:50px;color:  #1456a6   \">Spotify Song Popularity Prediction<\/li>\n\n![141286728_7ed37a4c1e_o.jpg](attachment:57458722-e455-4691-82f4-879f593ae2ed.jpg)","9b144dd8":"<li style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:40px;color: #c1531f  \"> Data Preparation for Model:<\/li>","cfc1a615":"<li style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:25px;color: #c1531f  \"> Missing Data:<\/li>\n\n<p style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:20px;color: #c1531f  \"> The train set has 32187 missing values, and the test set has 7962.<\/p>","f7fd39a5":"<li style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:30px;color: #c1531f  \"> Insight from Descriptive Statistics:<\/li>\n\n><div class=\"alert alert-info\" role=\"alert\">\n><ul style=\"font-family:cursive;font-size:20px;color:  #2025bd\">\n><li >Tempo loudness, key and song Duration are having very high STD meaning that most of the numbers are far away from mean value. Thus implementing values are spread out over a wide range.<\/li>\n><li >Where as other columns values are close to mean.<\/li>\n><li >This Dataset have no Duplicate values (or rows).<\/li>\n><li ><b>We can Normalize or Standardize this dataset for better analysis<\/b><\/li>\n    <\/ul>\n><\/div>","1b235258":"[Reference](https:\/\/www.kaggle.com\/c\/song-popularity-prediction\/discussion\/301616)  \nSource - Spotify: \"In Spotify's API is something called Valence, that describes the musical positiveness conveyed by a track. Tracks with high valence sound more positive (happy, cheerful, euphoric), while tracks with low valence sound more negative (sad, depressed, angry).\"\n\nFrom very good article explaining Spotify API What Makes a Song Likeable?- https:\/\/towardsdatascience.com\/what-makes-a-song-likeable-dbfdb7abe404 we can read that:\n\n**Spotify Audio Features**\nFor every track on their platform, Spotify provides data for thirteen Audio Features.The Spotify Web API developer guide defines them as follows:\n\n**Danceability:** Describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity.\n\n**Valence:** Describes the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n\n**Energy:** Represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale.\n\n**Tempo:** The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece, and derives directly from the average beat duration.\n\n**Loudness:** The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks.\n\n**Speechiness:** This detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value.\nInstrumentalness: Predicts whether a track contains no vocals. \u201cOoh\u201d and \u201caah\u201d sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly \u201cvocal\u201d.\n\n**Liveness:** Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live.\n\n**Acousticness:** A confidence measure from 0.0 to 1.0 of whether the track is acoustic.\n\n**Key:** The estimated overall key of the track. Integers map to pitches using standard Pitch Class notation . E.g. 0 = C, 1 = C\u266f\/D\u266d, 2 = D, and so on.\n\n**Mode:** Indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.\n\n**Duration:** The duration of the track in milliseconds.\n\n**Time Signature:** An estimated overall time signature of a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure).\n","55f9726e":"<li style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:20px;color: #c1531f  \"> Data distribution of test set<\/li>","1f427a7a":"<li style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:40px;color: #c1531f  \">Outlier Detection<\/li>","a6a15042":"<li style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:40px;color: #c1531f  \"> Observations:<\/li>\n\n><div class=\"alert alert-info\" role=\"alert\">\n><ul style=\"font-family:cursive;font-size:20px;color:  #2025bd\">\n><li >There are many Outliers and must be treated before finalizing the model.<\/li>\n><li >Some ways could be to use Flooring, Capping methods etc.<\/li>\n><li style=\"font-size:25px;\" >Or we can use Algo which are not sensitive to outliers as there are many outliers<\/li>\n><\/ul>\n><ul style=\"font-family:cursive;font-size:20px;color:  #2025bd\">Which Machine LEarning Models Are Sensitive To Outliers?.\n><li >Naivye Bayes Classifier------------------- Not Sensitive To Outliers<\/li>\n><li >SVM--------------------------------------- Not Sensitive To Outliers.<\/li>\n><li >Decision Tree Regressor or Classifier---- Not Sensitive To Outliers<\/li>\n><li >Ensemble(RF,XGboost,GB)------------------ Not Sensitive To Outliers<\/li>\n><li >Naivye Bayes Classifier------------------- Not Sensitive To Outliers<\/li>\n><li >KNN---------------------------------------- Not Sensitive To Outliers<\/li>\n><li >Linear Regression-------------------------- Sensitive To Outliers<\/li>\n><li >Logistic Regression------------------------ Sensitive To Outliers<\/li>\n><li >Kmeans------------------------------------- Sensitive To Outliers<\/li>\n><li >Hierarichal-------------------------------- Sensitive To Outliers<\/li>\n><li >PCA---------------------------------------- Sensitive To Outliers<\/li>\n><li >Neural Networks---------------------------- Sensitive To Outliers<\/li>\n><\/ul>\n><\/div>","48c7ad6d":"<li style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:40px;color: #c1531f  \"> Stack Models:<\/li>\n\n<p style=\"font-family:'Goudy Old Style';font-weight: bold;font-size:20px;color: #c1531f  \"> For Stacking we take the training data and run in through multiple models and all these models are typically known as <b>Base Learners or Base Models<\/b>. We generate final prediction from these models combined<\/p>\n\n![stacking.jpg](attachment:acec900a-e514-483b-9e4e-90c0f9ac586c.jpg)\n"}}