{"cell_type":{"6807dbea":"code","1e915c5f":"code","f858f1e4":"code","4cc5013f":"code","8d2804bc":"code","766c9f24":"code","81e06f2f":"code","60729bc0":"code","57148bef":"code","36a3b14a":"code","3a9644e4":"code","aca5a7f2":"code","152c8b2e":"code","6f75ec23":"code","9b4bfec1":"code","92c384f5":"code","45125a2e":"code","0d5ed356":"code","e995b58b":"code","c970d3d0":"code","25b78101":"code","97953043":"code","6c2e559b":"code","d2b14c8f":"code","e68d8058":"code","883589c4":"code","d3b8f8fd":"code","ba6010ab":"code","5512976f":"code","2e5d3b05":"code","386fbb38":"code","8f272c6c":"code","371b981d":"code","a0a4d580":"code","48056f66":"code","1b22e6a4":"code","81239001":"code","7f9b08f8":"code","58e1ab2e":"code","3273cc00":"code","330937e1":"code","e6c0318e":"code","e01ee081":"code","6eb41e67":"code","62069c95":"code","ef6a490d":"code","39aa772b":"code","4812a0a2":"code","0e61eadb":"code","09ab8247":"code","69fa3a60":"code","66160b99":"code","1a513d4c":"code","b292dd7c":"code","f2e9b272":"code","d37fc8bf":"code","7832dd92":"code","546a8181":"code","b400f0a0":"code","e4accfee":"code","d87fe2bf":"code","ef4c6a3f":"code","054993dd":"markdown","20f82502":"markdown","006d4055":"markdown","f1c85360":"markdown","0cd3d978":"markdown","169a6386":"markdown","664e7088":"markdown","42ca6781":"markdown","b5e6280d":"markdown","5b531302":"markdown","9b3ef91f":"markdown","67baa2e4":"markdown","4fac28da":"markdown","1eeed9e9":"markdown","9f40d866":"markdown","5d6605ad":"markdown","2cd743de":"markdown","d3aba1db":"markdown","5df1b1c4":"markdown","f8b20994":"markdown","0e0df234":"markdown","2e6d707f":"markdown"},"source":{"6807dbea":"import csv\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport time","1e915c5f":"df=pd.read_csv('..\/input\/customer-churn\/Customer Churn.csv')\ndf","f858f1e4":"df.info()","4cc5013f":"#rename columns\ndf=df.rename(columns={\"Call  Failure\": \"call_failure\", \"Complains\": \"complains\", \"Subscription  Length\": \"subs_len\", \"Charge  Amount\": \"charge_amount\",\n                   \"Seconds of Use\": \"total_sec_calls\", \"Frequency of use\": \"total_num_calls\", \"Frequency of SMS\": \"total_num_sms\", \"Distinct Called Numbers\": \"distinct_call_nums\",\n                   \"Age Group\": \"age_group\", \"Tariff Plan\": \"tariff_plan\", \"Status\": \"status\", \"Age\": \"age\", \"Customer Value\": \"customer_value\"})","8d2804bc":"df","766c9f24":"#see how many unique values for each col\ndf.nunique()","81e06f2f":"#there is no Nan values\ndf.isnull().sum()","60729bc0":"#see target class is imbalanced\n\nsns.set_style(\"dark\")\nsns.set(rc={'figure.figsize':(4,4)})\nsns.countplot(x=\"Churn\", data=df, palette=sns.color_palette(\"Paired\", 7), saturation=10)","57148bef":"sns.set_style(\"dark\")\nsns.countplot(x=\"age_group\", data=df, palette=sns.color_palette(\"husl\", 8), saturation=10)","36a3b14a":"sns.set_style(\"dark\")\nsns.countplot(x=\"charge_amount\", data=df, palette=sns.color_palette(\"husl\", 8), saturation=10)","3a9644e4":"sns.set_style(\"dark\")\nsns.countplot(x=\"age_group\", data=df, palette=sns.color_palette(\"husl\", 8), saturation=10, hue=\"Churn\")","aca5a7f2":"sns.set(rc={\"font.style\":\"normal\",\n            \"text.color\":\"black\",\n            \"xtick.color\":\"black\",\n            \"ytick.color\":\"black\",\n            \"axes.labelcolor\":\"black\",\n            \"axes.grid\":False,\n            'axes.labelsize':30,\n            'figure.figsize':(12.0, 6),\n            'xtick.labelsize':25,\n            'ytick.labelsize':20})\n\nsns.set(style=\"white\",font_scale=1)\n\n\nsns.set_style(\"dark\")\nsns.countplot(x=\"charge_amount\", data=df, palette=sns.color_palette(\"husl\", 8), \n              saturation=10, edgecolor=(0,0,0), linewidth=2)","152c8b2e":"# library\nimport matplotlib.pyplot as plt\nfrom palettable.colorbrewer.qualitative import Pastel1_7\n\n# create data\nnames=list(df[\"age\"].unique())\nsizes=[df[\"age\"].value_counts()[unique_class]*100\/len(df[\"age\"]) for unique_class in names]\ncolors = Pastel1_7.hex_colors\nexplode = (0, 0, 0, 0, 0)  # explode a slice if required\n\nplt.pie(sizes, explode=explode, labels=names, colors=colors,\n        autopct='%1.1f%%', shadow=True)\n        \n#draw a circle at the center of pie to make it look like a donut\ncentre_circle = plt.Circle((0,0), 0.50, color='black', fc='white',linewidth=0.80)\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n\n\n# Set aspect ratio to be equal so that pie is drawn as a circle.\nplt.axis('equal')\nplt.show()","6f75ec23":"cat_feature_col=[\"complains\", \"charge_amount\", \"tariff_plan\", \"status\", \"age\",\"Churn\"]\nfor i in cat_feature_col:\n    print(f\"{i} : {df[i].unique()}\")\n    print(df[i].value_counts())\n    print(\"-------------------------------------------\")","9b4bfec1":"#heatmap for correlation coefficient\n\n# calculate correlation\ndf_corr = df.corr()\n\n# correlation matrix\nsns.set(font_scale=0.8)\nplt.figure(figsize=(16,12))\nsns.heatmap(df_corr, annot=True, fmt=\".4f\",vmin=-1, vmax=1, linewidths=.5, cmap = sns.diverging_palette(145, 300, s=60, as_cmap=True))\n\n#plt.yticks(rotation=0)\nplt.show()","92c384f5":"#feature importance using corr\ndf.drop('Churn', axis=1).corrwith(df.Churn).plot(kind='barh', figsize=(8, 6), color='skyblue', title=\"Churn vs all features\")","45125a2e":"!pip install ppscore","0d5ed356":"import seaborn as sns\nimport ppscore as pps\n\nmatrix_df = pps.matrix(df).pivot(columns='x', index='y',  values='ppscore')\n\nsns.heatmap(matrix_df, annot=True)","e995b58b":"#for cat data distribution\nimport matplotlib\n\nplt.figure(figsize=(32, 32))\nmatplotlib.rc('axes', titlesize=24)#cols size\n\ncat_feature_col=[\"complains\", \"charge_amount\", \"age_group\", \"tariff_plan\", \"status\", \"age\"]\nfor i, column in enumerate(cat_feature_col, 1):\n    plt.subplot(4, 4, i)\n    df[df[\"Churn\"] == 0][column].hist(bins=20, color='pink', label='churn = 0(non-churn)', alpha=1)\n    df[df[\"Churn\"] == 1][column].hist(bins=20, color='tomato', label='churn = 1(churn)', alpha=1)\n    plt.legend(fontsize='medium')\n    plt.title(column)","c970d3d0":"#since age_group and age is highly correlated, we decide to del age_group\ndf=df.drop(columns=[\"age_group\", \"FN\", \"FP\"])","25b78101":"df","97953043":"#for cont data scatterplot matrix\ncont_feature_col=[\"call_failure\", \"subs_len\", \"total_sec_calls\", \"total_num_calls\", \"total_num_sms\", \"distinct_call_nums\",\"customer_value\"]\n\nsns.set(style=\"ticks\")\n\nsns.pairplot(df[cont_feature_col + ['Churn']], hue='Churn', palette=\"husl\", corner=True)","6c2e559b":"#outlier analysis using box-plot(continuos data can have outliers)\n\nsns.set(style=\"whitegrid\",font_scale=1)\nplt.figure(figsize=(10,8))\nsns.boxplot(data=df[cont_feature_col])\nplt.xticks(rotation=80)\nplt.title(\"Box plot \")\nplt.show()","d2b14c8f":"df.describe()","e68d8058":"# find the IQR\nq1 = df[cont_feature_col].quantile(.25)\nq3 = df[cont_feature_col].quantile(.75)\nIQR = q3-q1\n\noutliers_df = np.logical_or((df[cont_feature_col] < (q1 - 1.5 * IQR)), (df[cont_feature_col] > (q3 + 1.5 * IQR))) \n\noutlier_list=[]\ntotal_outlier=[]\nfor col in list(outliers_df.columns):\n    try:\n        total_outlier.append(outliers_df[col].value_counts()[True])\n        outlier_list.append((outliers_df[col].value_counts()[True] \/ outliers_df[col].value_counts().sum()) * 100)\n    except:\n        outlier_list.append(0)\n        total_outlier.append(0)\n        \noutlier_list\n\noutlier_df=pd.DataFrame(zip(list(outliers_df.columns), total_outlier, outlier_list), columns=['name of the column', 'total', 'outlier(%)'])\n\n#see totally how many outliers in cont features\noutlier_df.set_index('name of the column', inplace=True)\n#del outlier_df.index.name\noutlier_df","883589c4":"outliers_df","d3b8f8fd":"df_cont=df[cont_feature_col]\nout_nan_df=df_cont[~outliers_df]\nout_nan_df","ba6010ab":"for col in cont_feature_col:\n  #qq=out_nan_df.dropna()\n  col_mean=df[col].mean() #calculate mean for each col\n  out_nan_df[col]=out_nan_df[col].fillna(col_mean) #first convert outliers to Nan values then fill Nan's with col mean\n  #df[cont_feature_col]=df_cont","5512976f":"out_nan_df","2e5d3b05":"deneme=df.drop(columns=[\"call_failure\", \"subs_len\", \"total_sec_calls\", \"total_num_calls\", \"total_num_sms\", \"distinct_call_nums\", \"customer_value\"])","386fbb38":"#concat cat_df and clear out of outliers cont_df\n\ndf=pd.concat([out_nan_df, deneme], axis=1)","8f272c6c":"df","371b981d":"#import sklearn methods\nfrom sklearn.metrics import accuracy_score, roc_curve, confusion_matrix, classification_report, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, CategoricalNB\nfrom sklearn.svm import LinearSVC,SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import learning_curve\nimport sys \nimport os","a0a4d580":"# split df to X and Y\ny = df.loc[:, 'Churn'].values\nX = df.drop('Churn', axis=1)\n\n# split data into 80-20 for training set \/ test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y)\n\n# cross-validation with 5 splits\ncv = StratifiedShuffleSplit(n_splits=5, random_state = 88)\n\n#hold-out\nhold_out=StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state = 88)","48056f66":"X","1b22e6a4":"y","81239001":"print(\"X_train size is\", len(X_train))\nprint(\"y_train size is\", len(y_train))\nprint(\"--------------------\")\nprint(\"X_test size is\", len(X_test))\nprint(\"y_test size is\", len(y_test))","7f9b08f8":"#normalization(make all values bet. 0-1)\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train_normalized_arr=scaler.transform(X_train)\nX_train_normalized_df=pd.DataFrame(X_train_normalized_arr, columns=list(X.columns))\n\nX_test_normalized_arr=scaler.transform(X_test)\nX_test_normalized_df=pd.DataFrame(X_test_normalized_arr, columns=list(X.columns))","58e1ab2e":"X_train_normalized_df","3273cc00":"X_test_normalized_df","330937e1":"print(\"X_train_normalized_df size is\", len(X_train_normalized_df))\nprint(\"----------------------------------\")\nprint(\"X_test_normalized_df size is\", len(X_test_normalized_df))","e6c0318e":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators = 500, max_depth=5)\nrf.fit(X_train_normalized_df, y_train)\nrf_y_pred = rf.predict(X_test_normalized_df)\n\npd.Series(rf.feature_importances_, index = X_train_normalized_df.columns).nlargest(15).plot(kind = 'pie',\n                                                                               figsize = (8, 8),\n                                                                              title = 'Feature importance from RandomForest', colormap='magma')","e01ee081":"# display test scores and return result string and indexes of false samples\ndef display_test_scores(test, pred):\n    str_out = \"\\n\"\n    str_out += (\"#####  TEST SCORES  #####\\n--------------------\")\n    str_out += (\"\\n\")\n\n    #print accuracy\n    accuracy = accuracy_score(test, pred)\n    str_out += (\"ACCURACY: {:.4f}\\n\".format(accuracy))\n    str_out += (\"\\n\")\n\n    #print AUC score\n    auc = roc_auc_score(test, pred)\n    str_out += (\"AUC: {:.4f}\\n\".format(auc))\n    str_out += (\"\\n\")\n\n    #print confusion matrix\n    str_out += (\"CONFUSION MATRIX:\\n--------------------\\n\")\n    conf_mat = confusion_matrix(test, pred)\n    str_out += (\"{}\".format(conf_mat))\n    str_out += (\"\\n\")\n    str_out += (\"\\n--------------------\\n\")\n\n    #print classification report\n    str_out += (\"{}\".format(classification_report(test, pred)))\n    \n    false_indexes = np.where(test != pred)\n    return str_out, false_indexes","6eb41e67":"# decision tree with \"gini\"\ndt_1 = DecisionTreeClassifier(random_state = 0, criterion=\"gini\")\n\n# parameters \nparameters = {\n                \"splitter\": [\"best\",\"random\"],\n                \"class_weight\": [None, \"balanced\"],\n                \"max_depth\": [9, 11, 13, 15, 17, None]\n                }\n\nstart_time=time.time()##\n\n# grid search for parameters\ngrid_1 = GridSearchCV(estimator=dt_1, param_grid=parameters, cv=cv, n_jobs=-1)\ngrid_1.fit(X_train_normalized_df, y_train)\n\n# print best scores\nprint(\"The best parameters are %s with a score of %0.4f\"\n      % (grid_1.best_params_, grid_1.best_score_))\n\n################################################\n# detailed dataframe of gridsearch\n\n#detailed_grid_results = pd.DataFrame(grid.cv_results_)\n#detailed_grid_results\n\n\n################################################\n\n# prediction results\ny_pred = grid_1.predict(X_test_normalized_df)\n\nend_time=time.time()##\nprint(\"\\nRun time for train&test cv (DT-gini): \",end_time-start_time)\n\n# print accuracy metrics\nresults, false = display_test_scores(y_test, y_pred)\nprint(results)","62069c95":"start_time=time.time()##\n\n# grid search for parameters for hold_out\ngrid_1_h = GridSearchCV(estimator=dt_1, param_grid=parameters, cv=hold_out, n_jobs=-1)\ngrid_1_h.fit(X_train_normalized_df, y_train)\n\n# print best scores\nprint(\"The best parameters are %s with a score of %0.4f\"\n      % (grid_1_h.best_params_, grid_1_h.best_score_))\n\n#########################################\n\n# prediction results\ny_pred = grid_1_h.predict(X_test_normalized_df)\n\nend_time=time.time()##\nprint(\"\\nRun time for train&test hold_out (DT-gini): \",end_time-start_time)\n\n# print accuracy metrics\nresults, false = display_test_scores(y_test, y_pred)\nprint(results)\n","ef6a490d":"# decision tree with \"entropy\" gain_ratio\ndt_2 = DecisionTreeClassifier(random_state = 0, criterion=\"entropy\")\n\n# parameters \nparameters = {\n                \"splitter\": [\"best\",\"random\"],\n                \"class_weight\": [None, \"balanced\"],\n                \"max_depth\": [11, 13, 15, 17, 19, 21, None]\n                }\n\nstart_time=time.time()##\n\n# grid search for parameters\ngrid_2 = GridSearchCV(estimator=dt_2, param_grid=parameters, cv=cv, n_jobs=-1)\ngrid_2.fit(X_train_normalized_df, y_train)\n\n# print best scores\nprint(\"The best parameters are %s with a score of %0.4f\"\n      % (grid_2.best_params_, grid_2.best_score_))\n\n########################################\n\n# prediction results\ny_pred = grid_2.predict(X_test_normalized_df)\n\nend_time=time.time()##\nprint(\"\\nRun time for train&test cv (DT-gain ratio): \",end_time-start_time)\n\n# print accuracy metrics\nresults, false = display_test_scores(y_test, y_pred)\nprint(results)","39aa772b":"start_time=time.time()##\n\n# grid search for parameters for hold_out\ngrid_2_h = GridSearchCV(estimator=dt_2, param_grid=parameters, cv=hold_out, n_jobs=-1)\ngrid_2_h.fit(X_train_normalized_df, y_train)\n\n# print best scores\nprint(\"The best parameters are %s with a score of %0.4f\"\n      % (grid_2_h.best_params_, grid_2_h.best_score_))\n\n#####################################\n\n# prediction results\ny_pred = grid_2_h.predict(X_test_normalized_df)\n\nend_time=time.time()##\nprint(\"\\nRun time for train&test hold_out (DT-gain ratio): \",end_time-start_time)\n\n# print accuracy metrics\nresults, false = display_test_scores(y_test, y_pred)\nprint(results)","4812a0a2":"# Naive-Bayes with different approaches\nnb_list = [GaussianNB(), MultinomialNB(), ComplementNB()]\n\nfor nb in nb_list:\n    print(\"*********\", str(nb), \"**********\")\n    # parameters \n    parameters = {}\n\n    start_time=time.time()##\n    # grid search for parameters\n    grid_3 = GridSearchCV(estimator=nb, param_grid=parameters, cv=cv, n_jobs=-1)\n    grid_3.fit(X_train_normalized_df, y_train)\n\n    # print best scores\n    print(\"The best parameters are %s with a score of %0.4f\\n\"\n          % (grid_3.best_params_, grid_3.best_score_))\n\n    # prediction results\n    y_ord_pred = grid_3.predict(X_test_normalized_df)\n    \n    end_time=time.time()##\n    print(\"\\nRun time for train&test cv{}: \".format(str(nb)), end_time-start_time)\n\n    # print accuracy metrics\n    results, false = display_test_scores(y_test, y_pred)\n    print(\"\\n>>>>>>>>><<<<<<<<<<>>>>>>>>>>><<<<<<<<<<<<<>>>>>>>>><<<<<<<<\\n\")\n    print(results)\n    ","0e61eadb":"for nb in nb_list:\n    print(\"*********\", str(nb), \"**********\")\n    # parameters \n    parameters = {}\n\n    start_time=time.time()##\n    # grid search for parameters\n    grid_3_h = GridSearchCV(estimator=nb, param_grid=parameters, cv=hold_out, n_jobs=-1)\n    grid_3_h.fit(X_train_normalized_df, y_train)\n\n    # print best scores\n    print(\"The best parameters are %s with a score of %0.4f\\n\"\n          % (grid_3_h.best_params_, grid_3_h.best_score_))\n\n    # prediction results\n    y_ord_pred = grid_3_h.predict(X_test_normalized_df)\n    \n    end_time=time.time()##\n    print(\"\\nRun time for train&test hold_out{}: \".format(str(nb)), end_time-start_time)\n\n    # print accuracy metrics\n    results, false = display_test_scores(y_test, y_pred)\n    print(\"\\n>>>>>>>>><<<<<<<<<<>>>>>>>>>>><<<<<<<<<<<<<>>>>>>>>><<<<<<<<\\n\")\n    print(results)\n    ","09ab8247":"# NN with 1 layer\nann_1 = MLPClassifier(tol=1e-5, random_state=0, solver='adam', activation='tanh', max_iter=1000, batch_size=256)\n\nparameters = {\n                'hidden_layer_sizes': [(10,),(50,),(100,)],\n                'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10]\n            }\n\n\nstart_time=time.time()##\n# grid search for parameters\ngrid_4 = GridSearchCV(estimator=ann_1, param_grid=parameters, cv=cv, n_jobs=-1)\ngrid_4.fit(X_train_normalized_df, y_train)\n\n# print best scores\nprint(\"The best parameters are %s with a score of %0.4f\\n\"\n      % (grid_4.best_params_, grid_4.best_score_))\n\n###################################\n\n# prediction results\ny_pred = grid_4.predict(X_test_normalized_df)\n\nend_time=time.time()##\nprint(\"\\nRun time for train&test cv NN-1 layer: \", end_time-start_time)\n\n# print accuracy metrics\nresults, false = display_test_scores(y_test, y_pred)\nprint(results)","69fa3a60":"start_time=time.time()##\n\n# grid search for parameters for hold_out\ngrid_4_h = GridSearchCV(estimator=ann_1, param_grid=parameters, cv=hold_out, n_jobs=-1)\ngrid_4_h.fit(X_train_normalized_df, y_train)\n\n# print best scores\nprint(\"The best parameters are %s with a score of %0.4f\"\n      % (grid_4_h.best_params_, grid_4_h.best_score_))\n\n####################################\n\n# prediction results\ny_pred = grid_4_h.predict(X_test_normalized_df)\n\nend_time=time.time()##\nprint(\"\\nRun time for train&test hold_out NN-1 layer: \", end_time-start_time)\n\n# print accuracy metrics\nresults, false = display_test_scores(y_test, y_pred)\nprint(results)\n","66160b99":"# NN with 2 layers\nnn_2 = MLPClassifier(tol=1e-5, random_state=0, solver='adam', activation='tanh', max_iter=1000, batch_size=256)\n\n\nparameters = {\n                'hidden_layer_sizes': [(10, 10),(50, 50),(100, 100)],\n                'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10]\n            }\n\nstart_time=time.time()##\n# grid search for parameters\ngrid_5 = GridSearchCV(estimator=nn_2, param_grid=parameters, cv=cv, n_jobs=-1)\ngrid_5.fit(X_train_normalized_df, y_train)\n\n# print best scores\nprint(\"The best parameters are %s with a score of %0.4f\\n\"\n      % (grid_5.best_params_, grid_5.best_score_))\n\n############################\n\n# prediction results\ny_pred = grid_5.predict(X_test_normalized_df)\n\nend_time=time.time()##\nprint(\"\\nRun time for train&test cv NN-2 layer: \", end_time-start_time)\n\n# print accuracy metrics\nresults, false = display_test_scores(y_test, y_pred)\nprint(results)","1a513d4c":"start_time=time.time()##\n\n# grid search for parameters for hold_out\ngrid_5_h = GridSearchCV(estimator=nn_2, param_grid=parameters, cv=hold_out, n_jobs=-1)\ngrid_5_h.fit(X_train_normalized_df, y_train)\n\n# print best scores\nprint(\"The best parameters are %s with a score of %0.4f\"\n      % (grid_5_h.best_params_, grid_5_h.best_score_))\n\n####################################\n\n# prediction results\ny_pred = grid_5_h.predict(X_test_normalized_df)\n\nend_time=time.time()##\nprint(\"\\nRun time for train&test hold_out NN-2 layer: \", end_time-start_time)\n\n# print accuracy metrics\nresults, false = display_test_scores(y_test, y_pred)\nprint(results)\n","b292dd7c":"# SVM classifier\nsvm = SVC(tol=1e-5)\n\n# parameters \nparameters = {\n                'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n                'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n                'max_iter': [100, 300, 800, 1000, 1200],\n                'class_weight': [None, 'balanced']\n            }\n\nstart_time=time.time()##\n\n# grid search for parameters\ngrid_6 = GridSearchCV(estimator=svm, param_grid=parameters, cv=cv, n_jobs=-1)\ngrid_6.fit(X_train_normalized_df, y_train)\n\n# print best scores\nprint(\"The best parameters are %s with a score of %0.4f\\n\"\n      % (grid_6.best_params_, grid_6.best_score_))\n\n# prediction results\ny_pred = grid_6.predict(X_test_normalized_df)\n\n\nend_time=time.time()##\nprint(\"\\nRun time for train&test cv SVM : \", end_time-start_time)\n\n# print accuracy metrics\nresults, false = display_test_scores(y_test, y_pred)\nprint(results)","f2e9b272":"start_time=time.time()##\n\n# grid search for parameters\ngrid_6_h = GridSearchCV(estimator=svm, param_grid=parameters, cv=hold_out, n_jobs=-1)\ngrid_6_h.fit(X_train_normalized_df, y_train)\n\n# print best scores\nprint(\"The best parameters are %s with a score of %0.4f\\n\"\n      % (grid_6_h.best_params_, grid_6_h.best_score_))\n\n# prediction results\ny_pred = grid_6_h.predict(X_test_normalized_df)\n\nend_time=time.time()##\nprint(\"\\nRun time for train&test hold_out SVM : \", end_time-start_time)\n\n# print accuracy metrics\nresults, false = display_test_scores(y_test, y_pred)\nprint(results)","d37fc8bf":"start_time=time.time()##\n\n#generate subsamples by indices\nindexes=X_train.index.values\nrep = np.array([np.random.choice(indexes, len(indexes), replace = True) for _ in range(6)])\n\n#rep_x_train has 6 dfs \nrep_x_train=[df.iloc[arr,:-1] for arr in rep]\nrep_y_train=[df.iloc[arr,-1] for arr in rep]\n\n#dfs are created by bootstrapping\nrep_x_train[0].duplicated()\nrep_y_train[0].duplicated()","7832dd92":"indexes","546a8181":"rep","b400f0a0":"preds=[]\n\n#dt_1 --> cv \nmodel_1 = grid_1.best_estimator_\nmodel_1.fit(rep_x_train[0], rep_y_train[0])\n\npred_1=model_1.predict(X_test)\npreds.append(pred_1)\n\n#####################################\n\n#dt_2 --> cv \nmodel_2 = grid_2.best_estimator_\nmodel_2.fit(rep_x_train[1], rep_y_train[1])\n\npred_2=model_2.predict(X_test)\npreds.append(pred_2)\n\n#####################################\n\n#nb --> cv\nmodel_3 = grid_3.best_estimator_\nmodel_3.fit(rep_x_train[2], rep_y_train[2])\n\npred_3=model_3.predict(X_test)\npreds.append(pred_3)\n\n###################################\n\n#ann_1 --> cv\nmodel_4 = grid_4.best_estimator_\nmodel_4.fit(rep_x_train[3], rep_y_train[3])\n\npred_4=model_4.predict(X_test)\npreds.append(pred_4)\n\n###################################\n\n#ann_2 --> cv\nmodel_5 = grid_5.best_estimator_\nmodel_5.fit(rep_x_train[4], rep_y_train[4])\n\npred_5=model_5.predict(X_test)\npreds.append(pred_5)\n\n\n###################################\n\n#svm --> hold\nmodel_6 = grid_6_h.best_estimator_\nmodel_6.fit(rep_x_train[5], rep_y_train[5])\n\npred_6=model_6.predict(X_test)\npreds.append(pred_6)\n","e4accfee":"arr_preds=np.array(preds)\narr_preds_mean=arr_preds.mean(axis=0)\n\nend_time=time.time()##\nprint(\"\\nRun time for train&test bagging: \", end_time-start_time)\n\n#see it has same len with X_test\nprint(len(arr_preds_mean))\n\n# print accuracy metrics\nresults, false = display_test_scores(y_test, arr_preds_mean.round())\nprint(results)","d87fe2bf":"def boosting_step(grid, initial_weight, X_train, y_train):\n    \n    # Initialize the weights of each sample with wi = 1\/N and \n    #create a dataframe in which the evaluation is computed\n    df_eval = pd.DataFrame(y_train, columns=[\"target\"])\n    df_eval['weights'] = initial_weight \n\n\n    model = grid.best_estimator_\n    model.fit(X_train, y_train, sample_weight=np.array(df_eval['weights']))\n    pred=model.predict(X_train)\n    score = model.score(X_train,y_train)\n\n    #add values to the df_eval\n    df_eval['predictions'] = pred\n    df_eval['evaluation'] = np.where(df_eval['predictions'] == df_eval['target'], 1, 0)\n    df_eval['misclassified'] = np.where(df_eval['predictions'] != df_eval['target'], 1, 0)\n\n    #cal the misclassification rate and accuracy\n    accuracy = sum(df_eval['evaluation']) \/ len(df_eval['evaluation'])\n    misclassification = sum(df_eval['misclassified']) \/ len(df_eval['misclassified'])\n\n\n    #cal the error\n    err = np.sum(df_eval['weights'] * df_eval['misclassified']) \/ np.sum(df_eval['weights'])\n\n\n    #cal the alpha values\n    alpha = np.log((1-err) \/ err)\n \n\n    # Update the weights wi --> These updated weights are used in the sample_weight parameter\n    # for the training of the next decision stump. \n    df_eval['weights'] *= np.exp(alpha * df_eval['misclassified'])\n\n    prediction = alpha * df_eval[\"predictions\"]\n\n    return prediction, df_eval['weights']","ef4c6a3f":"#Set the initial weights w = 1\/N\nstart_time=time.time()##\n\npred_1, w_1=boosting_step(grid_1, 1\/len(y_train), X_train, y_train)\n\npred_2, w_2=boosting_step(grid_2, w_1, X_train, y_train)\n\npred_3, w_3=boosting_step(grid_3, w_2, X_train, y_train)\n\n#alpha_4, w_4=boosting_step(grid_4, w_3, X_train, y_train)\n\n#alpha_5, w_5=boosting_step(grid_5, w_4, X_train, y_train)\n\npred_6, w_6=boosting_step(grid_6_h, w_3, X_train, y_train)\n\npred_final=(pred_1+pred_2+pred_3+pred_6) \/ 4\n\npred_final=np.where(pred_final >=0.5, 1, 0)\n\n\nend_time=time.time()##\nprint(\"\\nRun time for train&test boosting: \", end_time-start_time)\n\n# print accuracy metrics\nresults, false = display_test_scores(y_train, pred_final)\nprint(results)","054993dd":"*### hold_out NN-1*","20f82502":"# **Classifier #1: Decision Tree**","006d4055":"# EDA","f1c85360":"*### hold_out NB*","0cd3d978":"*### hold_out DT2*","169a6386":"# **Classifier #3: Naive Bayes**","664e7088":"# Classifier #8: Boosting","42ca6781":"Here, our display test scores method","b5e6280d":"*### hold_out NN-2*","5b531302":"*### hold_out DT1*","9b3ef91f":"*### hold_out SVM*","67baa2e4":"# CLASSIFICATION","4fac28da":"# Normalization","1eeed9e9":"# feature importances","9f40d866":"# Classifier #7: Bagging","5d6605ad":"- https:\/\/www.analyticsvidhya.com\/blog\/2018\/06\/comprehensive-guide-for-ensemble-models\/\n- https:\/\/www.python-course.eu\/Boosting.php#Boosting-Pseudocode","2cd743de":"In this notebook:\n\n- EDA for Iranian Churn Dataset\n- Grid Search for hyperparameter tuning with cross-val&hold-out methods\n- Decision Tree\n- Naive Bayes\n- SVM\n- Neural Networks\n- Bagging ensemble method\n- Boosting ensemble method","d3aba1db":"# Churn Prediction\u260e\ufe0f\ud83d\udcca\ud83d\udd16\ud83d\udcc9","5df1b1c4":"# Classifier #5: ANN - 2 layer","f8b20994":"# **Classifier #2: Decision Tree**","0e0df234":"# Classifier #4: ANN - 1 layer","2e6d707f":"# **Classifier #6: SVM**"}}