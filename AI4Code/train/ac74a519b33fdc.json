{"cell_type":{"007a15e9":"code","abf7d9e9":"code","94bda3f4":"code","7a633123":"code","d143060e":"code","d4fe7b43":"code","ab982269":"code","9de128f0":"code","d336b118":"code","a22907b2":"code","edb15caf":"code","272179a5":"code","4f028c29":"code","59a6886d":"code","407d24a2":"code","53d5ce91":"code","aa7d5708":"code","a3b38985":"code","bd267f46":"code","5b56ebf2":"code","0336ff78":"code","b7f61ca8":"code","0ab190e5":"code","167788ac":"code","6f5ec118":"code","633594ba":"code","388c1ba2":"code","54f4fc60":"code","1eff9375":"code","96dbc837":"code","0158f4bf":"code","6ea4503d":"code","4664ab2e":"code","193dfd10":"code","d10ba7ce":"code","e5444c5a":"code","c550566a":"code","b9317794":"code","1c9e43ed":"code","3a297c96":"code","fa60b64d":"markdown","e6233f31":"markdown","991be5ae":"markdown","3ca744bf":"markdown","0c6f6487":"markdown","7bd044c3":"markdown","64f6cef4":"markdown","2ed1e8e9":"markdown","9faab5c4":"markdown","6c7ef434":"markdown","a7000111":"markdown","d61ef975":"markdown","6ae4071d":"markdown","839c524b":"markdown","d8e9964f":"markdown","cbf298d0":"markdown","6c33fa27":"markdown","11ce7ac2":"markdown","004f8527":"markdown","23e578b3":"markdown","727c2c7e":"markdown"},"source":{"007a15e9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","abf7d9e9":"train_df = pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/train.csv')\ntest_df = pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/test.csv')\n\ntrain_df.columns","94bda3f4":"train_df.head(10)","7a633123":"train_df.describe()","d143060e":"train_df.info()","d4fe7b43":"train_df.isnull().sum()","ab982269":"# libraries for Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt","9de128f0":"pd.crosstab(train_df.Age,train_df.Gender).plot(kind=\"bar\",figsize=(30,8))\nplt.title('Age Frequency for Genders')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()","d336b118":"plt.subplot\ng = sns.FacetGrid(train_df, col = \"Response\", height = 6)\ng.map(sns.distplot, \"Age\", bins = 50)\nplt.show()","a22907b2":"def bar_plot(variable):\n    \"\"\"\n        input: variable ex: \"Vehicle_Age\"\n        output: bar plot & value count\n    \"\"\"\n    # get feature\n    var = train_df[variable]\n    \n    # count number of categorical variable(value\/sample)\n    varValue = var.value_counts()   \n    \n    #visualize\n    plt.figure(figsize =(6,6))\n    labels = varValue.index\n    colors = ['#2C4447','#F3EC86','#679B75','red','green','brown']\n    plt.pie(varValue, labels=labels, colors=colors, autopct='%1.1f%%')\n    plt.ylabel(\"Rate\")\n    plt.title(variable)\n    plt.show()\n    \n    #print(\"{}: \\n {}\".format(variable,varValue))\n    ","edb15caf":"category1 = [\"Gender\",\"Vehicle_Age\",\"Vehicle_Damage\"]\nfor c in category1:\n    bar_plot(c)","272179a5":"train_df.head(10)","4f028c29":"test_df.head()","59a6886d":"train_df = pd.get_dummies(train_df, columns = [\"Driving_License\"])\ntest_df = pd.get_dummies(test_df, columns = [\"Driving_License\"])\ntrain_df.head()","407d24a2":"plt.figure(figsize=(30,8))\nsns.countplot(x=\"Region_Code\", data = train_df)\nplt.xticks(rotation = 60)\nplt.show()","53d5ce91":"train_df = pd.get_dummies(train_df, columns = [\"Region_Code\"], prefix = \"RC\")\ntest_df = pd.get_dummies(test_df, columns = [\"Region_Code\"], prefix = \"RC\")\ntrain_df.head()","aa7d5708":"plt.figure(figsize=(30,8))\nsns.countplot(x=\"Policy_Sales_Channel\", data = train_df)\nplt.xticks(rotation = 60)\nplt.show()","a3b38985":"train_df.Policy_Sales_Channel.value_counts().head(10)","bd267f46":"train_df[\"Policy_Sales_Channel\"] = [i if i == 152.0 or i == 26.0 or i == 124.0 or i == 160.0 or i == 156.0 or i==122.0 or i == 157.0 or i == 154.0 else 200 for i in train_df.Policy_Sales_Channel]\ntest_df[\"Policy_Sales_Channel\"] = [i if i == 152.0 or i == 26.0 or i == 124.0 or i == 160.0 or i == 156.0 or i==122.0 or i == 157.0 or i == 154.0 else 200 for i in test_df.Policy_Sales_Channel]\ntrain_df.Policy_Sales_Channel.value_counts().head(10)","5b56ebf2":"plt.figure(figsize=(30,8))\nsns.countplot(x=\"Policy_Sales_Channel\", data = train_df)\nplt.xticks(rotation = 60)\nplt.show()","0336ff78":"train_df = pd.get_dummies(train_df, columns = [\"Policy_Sales_Channel\"], prefix = \"SC\")\ntest_df = pd.get_dummies(test_df, columns = [\"Policy_Sales_Channel\"], prefix = \"SC\")\ntrain_df.head()","b7f61ca8":"train_df = pd.get_dummies(train_df, columns = [\"Vehicle_Damage\"], prefix = \"VD\")\ntest_df = pd.get_dummies(test_df, columns = [\"Vehicle_Damage\"], prefix = \"VD\")\ntrain_df.head()","0ab190e5":"train_df = pd.get_dummies(train_df, columns = [\"Vehicle_Age\"], prefix = \"VA\")\ntest_df = pd.get_dummies(test_df, columns = [\"Vehicle_Age\"], prefix = \"VA\")\ntrain_df.head()","167788ac":"train_df = pd.get_dummies(train_df, columns = [\"Gender\"], prefix = \"G\")\ntest_df = pd.get_dummies(test_df, columns = [\"Gender\"], prefix = \"G\")\ntrain_df.head()","6f5ec118":"train_df.info()","633594ba":"test_df.info()","388c1ba2":"train_df.drop(labels = [\"id\"], axis = 1, inplace = True)","54f4fc60":"train_df.columns","1eff9375":"y = train_df.Response.values\nx_data = train_df.drop([\"Response\"],axis=1)\n\n# normalization \nx = ( x_data - np.min(x_data) ) \/ ( np.max(x_data) - np.min(x_data) ).values","96dbc837":"# %% split data\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_val, y_train, y_val = train_test_split(x,y,test_size = 0.1, random_state = 42) # validation data = 0.1 data","0158f4bf":"print(\"X_train\",len(x_train))\nprint(\"x_val\",len(x_val))\nprint(\"y_train\",len(y_train))\nprint(\"y_val\",len(y_val))\n\nprint(\"test\",len(test_df))","6ea4503d":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train,y_train)\nacc_log_train = round(logreg.score(x_train, y_train)*100,2)\nacc_log_val = round(logreg.score(x_val, y_val)*100,2)\nprint(\"Training Accuracy: % {}\".format(acc_log_train))\nprint(\"Testing Accuracy: % {}\".format(acc_log_val))","4664ab2e":"# import models\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import GaussianNB","193dfd10":"# %% split data\n\nx_train1, x_ss, y_train1, y_ss = train_test_split(x_train,y_train,test_size = 0.01, random_state = 42) # subset data = 0.01 training data","d10ba7ce":"random_state = 42\nclassifier = [DecisionTreeClassifier(random_state = random_state),\n             SVC(random_state = random_state),\n             RandomForestClassifier(random_state = random_state),\n             LogisticRegression(random_state = random_state),\n             KNeighborsClassifier(),\n             GaussianNB()]\n\ndt_param_grid = {\"min_samples_split\" : range(10,100,20),\n                \"max_depth\": range(1,20,4)}\n\nsvc_param_grid = {\"kernel\" : [\"rbf\"],\n                 \"gamma\": [ 0.01, 0.1, 1],\n                 \"C\": [1,10,50,100,500]}\n\nrf_param_grid = {\"max_features\": [1,3,10],\n                \"min_samples_split\":[2,3,10],\n                \"min_samples_leaf\":[1,3,10],\n                \"bootstrap\":[False],\n                \"n_estimators\":[100,300],\n                \"criterion\":[\"gini\"]}\n\nlogreg_param_grid = {\"C\":np.logspace(-3,3,7),\n                    \"penalty\": [\"l1\",\"l2\"]}\n\nknn_param_grid = {\"n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\"]}\n\nnaive_param_grid = {}\n\nclassifier_param = [dt_param_grid,\n                   svc_param_grid,\n                   rf_param_grid,\n                   logreg_param_grid,\n                   knn_param_grid,\n                   naive_param_grid]","e5444c5a":"cv_result = []\n\nbest_estimators = []\n\nfor i in range(len(classifier)):\n    clf = GridSearchCV(classifier[i], param_grid=classifier_param[i], cv = StratifiedKFold(n_splits = 10), scoring = \"accuracy\", n_jobs = -1,verbose = 1)\n    clf.fit(x_ss,y_ss)     \n    cv_result.append(clf.best_score_) # save best scores\n    best_estimators.append(clf.best_estimator_) # save best estimators\n    print(cv_result[i])","c550566a":"cv_results = pd.DataFrame({\"Cross Validation Means\":cv_result, \"ML Models\":[\"DecisionTreeClassifier\", \"SVM\",\"RandomForestClassifier\",\n             \"LogisticRegression\",\n             \"KNeighborsClassifier\",\"GaussianNB\"]})\n\ng = sns.barplot(\"Cross Validation Means\", \"ML Models\", data = cv_results)\ng.set_xlabel(\"Mean Accuracy\")\ng.set_title(\"Cross Validation Scores\")\nplt.show()","b9317794":"votingC = VotingClassifier(estimators = [(\"dt\",best_estimators[0]),\n                                        (\"rfc\",best_estimators[2]),\n                                        (\"lr\",best_estimators[3])],\n                                        voting = \"soft\", n_jobs = -1)\n\nvotingC = votingC.fit(x_train, y_train) \nprint(accuracy_score(votingC.predict(x_val),y_val))","1c9e43ed":"test_df_id = test_df.id\ntest_df.drop(labels = [\"id\"], axis = 1, inplace = True)","3a297c96":"test_df_response = pd.Series(votingC.predict(test_df), name = \"Response\").astype(int)\n\nresults = pd.concat([test_df_id, test_df_response],axis = 1)\n\nresults.to_csv(\"cross_sell_prediction.csv\", index = False)","fa60b64d":"There are 52 different Region Codes. I think we don't change it.","e6233f31":"<a id = '11'><\/a><br>\n# Comparison of Accuracy","991be5ae":"- **id:** Unique ID for the customer \n- **Gender:** Gender of the customer\n- **Age:** Age of the customer\n- **Driving_License:** 0 = Customer does not have DL, 1 = Customer already has DL\n- **Region_Code:** Unique code for the region of the customer\n- **Previously_Insured:** 1 = Customer already has Vehicle Insurance, 0 = Customer doesn't have Vehicle Insurance\n- **Vehicle_Age:** Age of the Vehicle\n- **Vehicle_Damage:** 1 = Customer got his\/her vehicle damaged in the past. 0 = Customer didn't get his\/her vehicle damaged in the past.\n- **Annual_Premium:** : The amount customer needs to pay as premium in the year\n- **PolicySalesChannel:** Anonymised Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc.\n- **Vintage:** : Number of Days, Customer has been associated with the company\n- **Response:** : 1 = Customer is interested, 0 = Customer is not interested","3ca744bf":"<a id = '6'><\/a><br>\n# Modeling - Supervised Learning\n\nIn this section, we'll use some of the widely used supervised learning classification algorithms. But first we'll prepare data.","0c6f6487":"I will choose 3 ML algortihms.  ","7bd044c3":"<a id = '4'><\/a><br>\n# Feature Engineering","64f6cef4":"Besides, we don't have missing data.","2ed1e8e9":"<a id = '13'><\/a><br>\n## Conclusion\nPlease let me know if you have any suggestions or ideas on how to improve the model and results. Thanks for reading.","9faab5c4":"<a id = '3'><\/a><br>\n# Take a Look Data","6c7ef434":"<a id = '8'><\/a><br>\n## Train Test Split","a7000111":"\n<a id = '9'><\/a><br>\n# Simple Logistic Regression","d61ef975":"<a id = '12'><\/a><br>\n# Prediction and Submission\nActually, we've achieved almost same score with Logistic Regression. Although I will use last model to make prediction. ","6ae4071d":"Now, we have less category for Policy Sales Channel but this might work too.","839c524b":"<a id = '7'><\/a><br>\n## Normalization ","d8e9964f":"<a id = '10'><\/a><br>\n# Hyperparameter Tuning - Grid Search - Cross Validation\n\nWe will compare 6 ml classifier and evaluate mean accuracy of each of them by stratified cross validation. Therefore, we will create subset training data.\n\n* Decision Tree\n* SVM\n* Random Forest\n* KNN\n* Logistic Regression\n* Naive Bayes Classification","cbf298d0":"# Introduction\n\nAn insurance company that has provided Health Insurance to its customers want to predict which customers may be interest in Vehicle Insurance provided by the company. Besides we have some informations about these customers and our goal is that will try to predict potential customers with using data.\n\nIn this kernel, we'll use some of the widely used supervised learning algorithms.\n\n<font color = 'blue'>\nContent:\n    \n1. [Load and Check Data](#1)\n1. [Variable Description](#2)\n1. [Take a Look Data](#3)\n1. [Feature Engineering](#4)\n    * [Drop ID](#5)\n1. [Modeling - Supervised Learning](#6)\n    * [Normalization](#7)\n    * [Train Test Split](#8)\n    * [Simple Logistic Regression](#9)\n    * [Hyperparameter Tuning - Grid Search - Cross Validation](#10)\n    * [Comparison of Accuracy](#11)\n1. [Prediction and Submission](#12)\n1. [Conclusion](#13) ","6c33fa27":"There is a big difference among the Sales Channels here. So we will keep top 8 Sales Channels that sold  most insurances.","11ce7ac2":"<a id = '2'><\/a><br>\n# Variable Description","004f8527":"<a id = '1'><\/a><br>\n# Load and Check Data","23e578b3":"At the same time, we will do same process for test data.","727c2c7e":"<a id = '5'><\/a><br>\n## Drop ID"}}