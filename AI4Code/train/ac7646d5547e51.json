{"cell_type":{"101aefea":"code","afe886f4":"code","bddf80cf":"code","ab5f6df9":"code","97137178":"code","6454437f":"code","a1b98b2d":"code","19e2cb40":"code","932cb225":"code","8bedf789":"code","b6373a17":"code","e0f72b2f":"code","de3534e1":"code","86469806":"code","3ff23da5":"code","0751f584":"code","de1e72a3":"code","01d31660":"code","431943ed":"code","7a627cfc":"code","b6d4568c":"code","5546cfc9":"code","f4f49761":"code","8c5773cf":"code","3407c857":"code","0350675c":"code","7d523d50":"code","34c149ae":"code","2cbf76ce":"code","2052dc02":"code","80568d1a":"code","1eb0e84b":"code","3b766152":"code","21d527a5":"code","031cb9e6":"code","2692edb3":"code","cbd9620a":"code","44eb2aa2":"code","7c79b56c":"code","6d5394e7":"code","3b9bcde7":"code","15df7d78":"code","37303f55":"code","1e811b0c":"code","f9ce5c9d":"code","6050356a":"code","30fe8c62":"code","d35c3430":"code","e6465e51":"code","64a8b820":"code","8f2257ee":"code","e4b5be5d":"code","5557a52a":"code","2da42fec":"code","b29df906":"code","83829a82":"code","49c4f9dc":"code","c40fe5f8":"code","a1b7a30a":"code","82fdbd2b":"code","105820ff":"code","ac7fa678":"code","a2b9d094":"code","799f1d89":"code","f19315bb":"code","1895b913":"code","fdf6668e":"code","4015e3c2":"code","63386f2b":"code","ee2b27fe":"code","9cc313b1":"code","294c5e99":"code","1e5e5d58":"code","b25c24b9":"code","07f66816":"code","f675e5b2":"code","2fed1520":"code","57a76e16":"code","1f7dfd01":"code","7ff608c7":"code","b351de3c":"code","f1a72abc":"code","790beb25":"code","d4979dec":"code","2c2e3e66":"code","fa40301c":"code","49e5bc0c":"code","bdd7d833":"code","379b9214":"code","167dceb3":"code","b9cc2c1c":"code","5452b150":"code","bf64018a":"code","ed3f9e7f":"code","94f7b3c2":"code","1522a6a3":"code","f4b017ae":"code","74b0142a":"code","067acc2d":"code","6de49b2b":"code","dc9d2955":"code","0fc721c3":"code","ec319a77":"code","4068f0d9":"code","3a0264f0":"code","42fa0cdc":"code","74d0dab7":"code","d8dee95a":"code","431e796e":"code","e1eb51c9":"code","81ffcbba":"code","9d27fcd2":"code","90503171":"code","e1314578":"code","55a66973":"code","a4416cbb":"code","132c5fa3":"code","54c088b3":"code","2fd62c28":"code","5ff91d3c":"code","65ce6a58":"code","cacf6bd5":"code","b73bc356":"code","aefa68b4":"code","b869d973":"code","5e34c730":"code","266d21f3":"code","2389c7d3":"code","bdb205fd":"code","63bfcb70":"code","016932c6":"code","c97ef5e5":"code","00460adb":"code","c9ee7d1f":"code","88220bfb":"code","f95c17f8":"code","cca77d11":"code","cf273aa1":"code","6fe74d9e":"code","d3b9b7f6":"code","f17af6fa":"code","3bface38":"code","ac09c45e":"code","cb779766":"code","e13bdb10":"code","3c769a6e":"code","73d9347b":"code","534d7db0":"code","93349295":"code","ed3c08ef":"code","b5cf3ef1":"code","73639f9d":"code","87218b14":"code","a60b9afb":"code","8a830d00":"code","d3e6634e":"code","6eec7095":"code","84b25bf4":"code","2835e74b":"code","5920f209":"code","764f9b55":"code","209b20f3":"code","1da8b228":"code","57ff537a":"code","3d10bce1":"code","3dcf4111":"code","b6030e90":"code","6a3d219a":"code","7c1b1dcb":"code","3ff3f9fe":"code","d359ee0c":"code","f2f10c14":"code","36d4d947":"code","7f13211d":"code","9d6238dc":"code","a3f14315":"code","1a1df793":"code","36bd39ea":"code","13128f18":"code","17631a04":"code","8ef1cd89":"markdown","6d50fb29":"markdown","55bb5d1e":"markdown","6b432e16":"markdown","f697bfb5":"markdown","b495024f":"markdown","f0a68769":"markdown","8dcd3d32":"markdown","7f5f99d2":"markdown","3e268dea":"markdown","5bec3b31":"markdown","48c31e6a":"markdown","d2c934da":"markdown","42fa5776":"markdown","70f36a50":"markdown","4c1a349f":"markdown","e583d595":"markdown","ccb93389":"markdown","670e144a":"markdown","9860598e":"markdown","73c5bddf":"markdown","5362b0cc":"markdown","04d1687d":"markdown","27e1ca4f":"markdown","961a26e8":"markdown","298f9365":"markdown","e8d9972f":"markdown","f21bceb0":"markdown","6ffe5928":"markdown","5291c053":"markdown","48e1eaa6":"markdown","904711ee":"markdown","25b5e0d8":"markdown","beca1e96":"markdown","a6d9c6da":"markdown","3493c961":"markdown","4747845b":"markdown","af0d9fda":"markdown","f54f3b8b":"markdown","c614c3b1":"markdown","5108849c":"markdown","2b2a545c":"markdown","e069da63":"markdown","e1f6e9e3":"markdown","30ae2ce0":"markdown","ebca3198":"markdown","736cfd1f":"markdown","32a1b58d":"markdown","1d50024e":"markdown","22f50c7a":"markdown","03d2f7d9":"markdown","36cc5c56":"markdown","122e6ac4":"markdown","0f9ea8d7":"markdown","dc57afab":"markdown","ff86f743":"markdown","8886a61f":"markdown","a0039558":"markdown","43b110c1":"markdown","9e121a69":"markdown","f9c89db9":"markdown","e1a28575":"markdown","48f266cf":"markdown","8309826c":"markdown","134cebf1":"markdown","94d7cca0":"markdown","c94dd7c9":"markdown","77ae9513":"markdown","cc75ee41":"markdown","b3564545":"markdown","de667958":"markdown","3af68762":"markdown","800cea2f":"markdown","dfbd7692":"markdown","a510d4b4":"markdown","852db66c":"markdown","d6266f51":"markdown","ef6f9229":"markdown","60de0528":"markdown","e95ce43d":"markdown","2d121d14":"markdown","913c4043":"markdown","368d2c46":"markdown","b23c3811":"markdown","73ca77e1":"markdown","fd66211e":"markdown","40a1081d":"markdown","d9226605":"markdown","2e35f504":"markdown","24c88439":"markdown","bdc50708":"markdown","2f4a24cb":"markdown","5f2e409f":"markdown","fdeba34c":"markdown","1d1f0026":"markdown","1dad4a66":"markdown","375e910b":"markdown","3a2fe099":"markdown","f535771e":"markdown","f41593c8":"markdown","fa4bc65c":"markdown","c58bf51f":"markdown","43e873d1":"markdown","5c4cf9c7":"markdown","f0b0dad9":"markdown","acec0372":"markdown","4f2512e1":"markdown","ecf76b70":"markdown","3ec5c6ce":"markdown","3620c5a9":"markdown","3ea39f75":"markdown","32e5f8f4":"markdown","81c4d8fc":"markdown","a32ac5e0":"markdown","db61fe9b":"markdown","88705ad7":"markdown","ed0b00df":"markdown","794546fd":"markdown","3940dc25":"markdown","681b5d4f":"markdown","09721d8b":"markdown","a5b17a4f":"markdown","a08078bf":"markdown","ad98b359":"markdown","6f216cf9":"markdown","941a7177":"markdown","887b00c1":"markdown","41cb7b10":"markdown","642c3aff":"markdown","9248f540":"markdown","55db3f75":"markdown","900d2f1b":"markdown","0b774809":"markdown","f059d1d7":"markdown","215cb741":"markdown","db09b80b":"markdown","465e82d3":"markdown","947579ba":"markdown","b0b856a9":"markdown","3b415c95":"markdown","91986863":"markdown","941ccfef":"markdown","afc764a1":"markdown","c5e22bb0":"markdown","8f03dbd8":"markdown","bf3d1eda":"markdown","dc408833":"markdown","4e314302":"markdown","1c1612c9":"markdown","684451d6":"markdown","738fa4a7":"markdown","df29eb3b":"markdown","d35de9bd":"markdown","6764f3e3":"markdown","09ccfef8":"markdown","739f3b7f":"markdown","5de47ce4":"markdown","9461b0ea":"markdown","ee142a85":"markdown","846aa2ab":"markdown","30623d76":"markdown","7c7985d2":"markdown","613d2c31":"markdown","eca8c61d":"markdown","a3149438":"markdown"},"source":{"101aefea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","afe886f4":"import matplotlib.pyplot as plt\nimport seaborn as sns","bddf80cf":"plt.figure(figsize=(12,4))\nplt.imshow(plt.imread(\"..\/input\/linearmodel\/linear.PNG\"))\n#Here is the formation of Linear Regression:","ab5f6df9":"plt.figure(figsize=(12,4))\nplt.imshow(plt.imread(\"..\/input\/meanerror\/mean_error.PNG\"))","97137178":"plt.figure(figsize=(12,4))\nplt.imshow(plt.imread(\"..\/input\/normalequation\/normalequation.PNG\"))\n#Here is the formation of Normal Equation for Linear Regression:","6454437f":"print(np.random.rand(10)) #This will create randomly distributed 10 numbers\nprint(\"****************************************\")\nprint(np.random.rand(10,1)) #This will create randomly distributed 10 numbers with 10 rows and 1 column\nprint(np.random.rand(10,2)) #This will create randomly distributed 20 numbers with 10 rows and 2 columns","a1b98b2d":"#The function that we used to generate the data is y = 4 + 3x + Gaussian noise.\nsns.set_style(\"darkgrid\")\nplt.figure(figsize=(12,8))\nX = 2*np.random.rand(100,1) # we create our X features\ny = 4 + 3*X + np.random.rand(100,1) # we make y according to the formulation of linera regression\nplt.scatter(x= X, y=y,c=\"red\")","19e2cb40":"np.c_[np.array([1,2,3]),np.array([4,5,6])]","932cb225":"X_0 = np.c_[np.ones((100, 1)), X] # here we add x0 = 1 to each instance\nX_0","8bedf789":"plt.figure(figsize=(12,4))\nplt.imshow(plt.imread(\"..\/input\/normalequation\/normalequation.PNG\"))\n#Here is the formation of Normal Equation for Linear Regression we will implement","b6373a17":"y_teta = np.dot(np.dot(np.linalg.inv(np.dot(X_0.T,X_0)), X_0.T) , y)\ny_teta","e0f72b2f":"X_new = np.array([[0], [2]])\nX_new","de3534e1":"X_new_0 = np.c_[np.ones((2, 1)), X_new]\nX_new_0","86469806":"X_new = np.array([[0], [2]])\nX_new_0 = np.c_[np.ones((2, 1)), X_new]\nprediction = np.dot(X_new_0,y_teta)\nprediction","3ff23da5":"plt.plot(X_new, prediction, \"r-\")\nplt.plot(X, y, \"b.\")\nplt.axis([0, 2, 0, 15])\nplt.show()","0751f584":"from sklearn.linear_model import LinearRegression #Here we import the Linear regression algorithm\nlinear_model = LinearRegression() # here we create an instance of the algorithm\nlinear_model.fit(X,y) #Here we make the algorithm fit the X values and y values \nprint(linear_model.intercept_) #This is the bias or intercept of the algorithm\nprint(linear_model.coef_)   #This is weights or coefficient of the algorithm has found after fitting the data\n","de1e72a3":"print(y_teta) #This is the intercept and coefficient of our own model\n#as you can see they are similar because they design the algorith in the same way as we do here.","01d31660":"print(linear_model.predict(X_new)) #This is the prediction of Linear Regression model of the scikit-learn library\nprint(prediction) #This is the prediction of Linear Regression model of our own model\n#Both of them predict the same values because they have the same intercept and coefficient","431943ed":"plt.figure(figsize=(12,10))\nplt.imshow(plt.imread(\"..\/input\/gradientdescent\/gradient descent.jpeg\"))","7a627cfc":"plt.figure(figsize=(12,10))\nplt.imshow(plt.imread(\"..\/input\/gradient2\/gradient2.png\"))","b6d4568c":"plt.figure(figsize=(12,10))\nplt.imshow(plt.imread(\"..\/input\/challange\/challenge.png\"))","5546cfc9":"plt.figure(figsize=(12,10))\nplt.imshow(plt.imread(\"..\/input\/partial\/partialderivative.PNG\"))","f4f49761":"plt.figure(figsize=(12,10))\nplt.imshow(plt.imread(\"..\/input\/partial2\/partial2.PNG\"))","8c5773cf":"plt.figure(figsize=(12,10))\nplt.imshow(plt.imread(\"..\/input\/nextstep\/nextstep.PNG\"))","3407c857":"#quick implementation of this algorithm:\nlearning_rate= 0.1\nnumber_of_iterations = 1000\nm = 100\ntheta = np.random.randn(2,1) # random initialization\nprint(theta)\nprint(\"**********************\")\nprint(X_0)","0350675c":"for iteration in range(number_of_iterations):\n    gradients = 2\/m * np.dot(X_0.T,(np.dot(X_0,theta) - y))\n    theta = theta - learning_rate * gradients\ntheta","7d523d50":"plt.figure(figsize=(12,4))\nplt.imshow(plt.imread(\"..\/input\/normalequation\/normalequation.PNG\"))\n#Here is the formation of Normal Equation for Linear Regression we will implement","34c149ae":"y_teta = np.dot(np.dot(np.linalg.inv(np.dot(X_0.T,X_0)), X_0.T) , y)\ny_teta # The result of normal equation is similar to the result of the gradient descent","2cbf76ce":"for iteration in range(number_of_iterations):\n    gradients = 2\/m * np.dot(X_0.T,(np.dot(X_0,theta) - y))\n    theta = theta - learning_rate * gradients","2052dc02":"plt.figure(figsize=(12,10))\nplt.imshow(plt.imread(\"..\/input\/stochastic\/stochastic.png\"))","80568d1a":"from sklearn.linear_model import SGDRegressor\nsgd= SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\nsgd.fit(X, y.ravel()) #y.ravel() Return a flattened array.\nprint(sgd.coef_)\nprint(sgd.intercept_)\n#The values are very close to the values of Linear Regression","1eb0e84b":"print(sgd.predict(X_new))\nprint(linear_model.predict(X_new))\n#As we cans see below, the predictions are also very close","3b766152":"np.random.rand(100,1)[:5]","21d527a5":"X = 8* np.random.rand(100,1) +2 # we create our features\nX[:5]","031cb9e6":"\ny = 3*X**4-3*X**3+0.5* X**2+ 2*X-100 + np.random.rand(100,1) # we create our target as polynomial version of X features\ny[:5]","2692edb3":"sns.set_style(\"darkgrid\")\nplt.figure(figsize=(12,8))\nplt.scatter(x= X, y=y, c=\"green\")","cbd9620a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","44eb2aa2":"from sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree=4, include_bias=False)\nX_poly=poly_reg.fit_transform(X)\n#X_poly_test = poly_reg.transform(X_test)\nprint(X_train[:2])\nprint(\"*************************\")\nprint(X_poly[:2])\nprint(\"***********TEST****************\")\n#print(X_test[:2])\nprint(\"*************************\")\n#print(X_poly_test[:2])","7c79b56c":"print(X_poly.shape)\nprint(X.shape)\n","6d5394e7":"from sklearn.linear_model import LinearRegression\nlinear = LinearRegression()\nlinear.fit(X_poly, y)\n","3b9bcde7":"print(linear.intercept_)\nprint(linear.coef_)","15df7d78":"predictions= linear.predict(X_poly)","37303f55":"sns.set_style(\"darkgrid\")\nplt.figure(figsize=(12,8))\n#plt.scatter(x= X, y=y, c=\"green\")\nplt.scatter(x=X,y=predictions, c=\"red\")","1e811b0c":"linear2 = LinearRegression()\nlinear2.fit(X,y)\nnormal_linear_predictions = linear2.predict(X)","f9ce5c9d":"print(y[:3])\nprint(\"********PREDICTIONS*********\")\nprint(predictions[:3])\n#The predictions of the polynomial linear regression model is very close\nprint(\"**********NORMAL LINEAR REGRESSION***************\")\nprint(normal_linear_predictions[:3])\n#However, predictions of the normal linear regression is fra from the actual values\n","6050356a":"plt.figure(figsize=(12,10))\nsns.regplot(x=normal_linear_predictions, y=y)\n#This is regression line of linear regression\n","30fe8c62":"plt.figure(figsize=(12,10))\nsns.regplot(x=predictions, y=y, color=\"red\")\n#This is predictions of polynomial regression","d35c3430":"from sklearn.metrics import mean_squared_error\ndef plot_learning_curves(model, X, y):\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n    train_errors, val_errors = [], []\n    for m in range(1, len(X_train)):\n        model.fit(X_train[:m], y_train[:m])\n        y_train_predict = model.predict(X_train[:m])\n        y_val_predict = model.predict(X_val)\n        train_errors.append(mean_squared_error(y_train[:m],\n    y_train_predict))\n        val_errors.append(mean_squared_error(y_val, y_val_predict))\n    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")","e6465e51":"lin_reg = LinearRegression()\nplot_learning_curves(lin_reg, X, y)","64a8b820":"from sklearn.pipeline import Pipeline\npolynomial_regression = Pipeline([\n(\"poly_features\", PolynomialFeatures(degree=10,\ninclude_bias=False)),\n(\"lin_reg\", LinearRegression()),\n])\nplot_learning_curves(polynomial_regression, X, y)\n","8f2257ee":"plt.figure(figsize=(10,4))\nplt.imshow(plt.imread(\"..\/input\/ridgeregression\/ridgereg.PNG\"))","e4b5be5d":"from sklearn.linear_model import Ridge\nridge= Ridge(alpha=1.0,fit_intercept= True, normalize= True, solver=\"cholesky\")\nridge.fit(X,y)\nridge_predictions = ridge.predict(X)\nplt.figure(figsize=(12,8))\nsns.set_style(\"darkgrid\")\nplt.scatter(x=ridge_predictions,y=y,c=\"red\")","5557a52a":"sns.set_style(\"darkgrid\")\nplt.figure(figsize=(12,8))\nplt.scatter(x= X, y=y, c=\"green\")","2da42fec":"print(ridge.predict([[2.5]]))\n\nprint(linear2.predict([[2.5]]))\n                     ","b29df906":"from sklearn.linear_model import Lasso\nlasso= Lasso(alpha=0.1)\nlasso.fit(X,y)\nlasso_predictions = lasso.predict(X)","83829a82":"plt.figure(figsize=(12,8))\nsns.set_style(\"darkgrid\")\nplt.scatter(x=lasso_predictions,y=y,c=\"red\")","49c4f9dc":"print(ridge.predict([[2.5]])) #This is ridge regression\nprint(lasso.predict([[2.5]])) #This is lasso regression\nprint(linear2.predict([[2.5]])) #This polynomial regression model\n#The predictions of Polynomial regression and Lasso regression are very close to each other.","c40fe5f8":"plt.figure(figsize=(10,10))\nplt.imshow(plt.imread(\"..\/input\/elastic\/elastic.PNG\"))\n#Elastic Net cost function","a1b7a30a":"from sklearn.linear_model import ElasticNet\nelastic= ElasticNet(alpha=0.1,l1_ratio=0.5)\nelastic.fit(X,y)\nelastic_predictions = elastic.predict(X)\nplt.scatter(x=elastic_predictions, y=y)","82fdbd2b":"print(ridge.predict([[2.5]])) #This is ridge regression\nprint(lasso.predict([[2.5]])) #This is lasso regression\nprint(linear2.predict([[2.5]])) #This polynomial regression model\nprint(elastic.predict([[2.5]])) #This is the prediction of Elasticnet","105820ff":"\nplt.figure(figsize=(10,10))\nplt.imshow(plt.imread(\"..\/input\/logistic\/logistic.PNG\"))","ac7fa678":"\nplt.figure(figsize=(10,10))\nplt.imshow(plt.imread(\"..\/input\/logistic2\/logistic2.PNG\"))","a2b9d094":"#This is the cost function of logistic regression(Logloss function)\nplt.figure(figsize=(10,10))\nplt.imshow(plt.imread(\"..\/input\/logloss\/loglos.PNG\"))","799f1d89":"plt.figure(figsize=(10,10))\nplt.imshow(plt.imread(\"..\/input\/softmax2\/softmax.PNG\"))","f19315bb":"plt.figure(figsize=(10,10))\nplt.imshow(plt.imread(\"..\/input\/softmax3\/saofmax3.PNG\"))","1895b913":"plt.figure(figsize=(10,10))\nplt.imshow(plt.imread(\"..\/input\/crossentropy\/crossnetropy.PNG\"))","fdf6668e":"plt.figure(figsize=(15,15))\nplt.imshow(plt.imread(\"..\/input\/support\/support.PNG\"))","4015e3c2":"plt.figure(figsize=(15,15))\nplt.imshow(plt.imread(\"..\/input\/outliers\/outliers.PNG\"))","63386f2b":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import train_test_split","ee2b27fe":"df= pd.read_csv(\"..\/input\/iris-flower-dataset\/IRIS.csv\")\ndf.head()","9cc313b1":"X=df.drop(\"species\",axis=1)\ny=df[\"species\"]\nX.head()","294c5e99":"y.head()","1e5e5d58":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.05, random_state=42)","b25c24b9":"pipe = Pipeline([('scaler', StandardScaler()), ('svc', LinearSVC(C=1, loss=\"hinge\"))])\npipe.fit(X_train,y_train)\n#When creating the SVC model, we would write SVC(kernel=\"linear\", C=1).","07f66816":"pipe.score(X_test,y_test)","f675e5b2":"predictions = pipe.predict(X_test)","2fed1520":"pd.DataFrame([predictions,y_test]) # It seem we have %100 accuracy","57a76e16":"from sklearn.metrics import accuracy_score, classification_report,confusion_matrix\nprint(accuracy_score(y_test, predictions))\nprint(classification_report(y_test, predictions))\nprint(confusion_matrix(y_test, predictions))","1f7dfd01":"from sklearn.datasets import make_moons\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nX, y = make_moons(n_samples=100, noise=0.15)\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1, random_state=2)\npolynomial_svm = Pipeline([\n(\"poly_features\", PolynomialFeatures(degree=3)),\n(\"scaler\", StandardScaler()),\n(\"svm_clf\", LinearSVC(C=10, loss=\"hinge\"))\n])\npolynomial_svm.fit(X_train, y_train)","7ff608c7":"from sklearn.preprocessing import StandardScaler\nss= StandardScaler()\nX_train= ss.fit_transform(X_train)\nX_test= ss.transform(X_test)","b351de3c":"from matplotlib.colors import ListedColormap\nX_set, y_set = ss.inverse_transform(X_train), y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, polynomial_svm.predict(ss.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\nplt.title('SVM with Polynomial Features (Training set)')\n#plt.xlabel('Age')\n#plt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()\n","f1a72abc":"from sklearn.svm import SVC\npoly_kernel_svm_clf = Pipeline([\n(\"scaler\", StandardScaler()),\n(\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))#This code trains an SVM classifier using a third-degree polynomial kernel.\n])\npoly_kernel_svm_clf.fit(X_train, y_train)","790beb25":"from matplotlib.colors import ListedColormap\nX_set, y_set = ss.inverse_transform(X_train), y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 0.25),\n                     np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 0.25))\nplt.contourf(X1, X2, poly_kernel_svm_clf.predict(ss.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red', 'green')))\nplt.xlim(X1.min(), X1.max())\nplt.ylim(X2.min(), X2.max())\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('purple', 'blue'))(i), label = j)\nplt.title('SVM with Innner Polynomial Features (Training set)')\n#plt.xlabel('Age')\n#plt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","d4979dec":"rbf_kernel_svm = Pipeline([(\"scaler\", StandardScaler()),\n                          (\"rbf\",SVC(kernel=\"rbf\",gamma=5,C=0.001))])\nX, y = make_moons(n_samples=100, noise=0.15)\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1, random_state=2)\nrbf_kernel_svm.fit(X_train, y_train)","2c2e3e66":"predictions= rbf_kernel_svm.predict(X_test)","fa40301c":"pd.DataFrame([predictions, y_test])","49e5bc0c":"plt.figure(figsize=(15,15))\nplt.imshow(plt.imread(\"..\/input\/rbfsvm\/rbfsvm.PNG\"))","bdd7d833":"plt.figure(figsize=(15,15))\nplt.imshow(plt.imread(\"..\/input\/svmreg\/svmreg.PNG\"))","379b9214":"from sklearn.svm import SVR\nsvm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\nsvm_poly_reg.fit(X, y)","167dceb3":"plt.figure(figsize=(15,15))\nplt.imshow(plt.imread(\"..\/input\/decision\/decision.PNG\"))","b9cc2c1c":"df=pd.read_csv(\"..\/input\/iris-flower-dataset\/IRIS.csv\")\ndf.head()","5452b150":"X = df.drop(\"species\",axis=1)\ny = df[\"species\"]","bf64018a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1,random_state=2)\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz, export_text,plot_tree\ndtree = DecisionTreeClassifier(max_depth=2)","ed3f9e7f":"dtree.fit(X_train, y_train)\npredictions = dtree.predict(X_test)","94f7b3c2":"plot_tree(dtree,\nfeature_names=df.columns[:-1],\nclass_names=df.columns[-1],\nrounded=True,\nfilled=True)","1522a6a3":"dtree2 = DecisionTreeClassifier(max_depth=3)\ndtree2.fit(X_train, y_train)\npredictions2 = dtree2.predict(X_test)","f4b017ae":"plot_tree(dtree2,\nfeature_names=df.columns[:-1],\nclass_names=df.columns[-1],\nrounded=True,\nfilled=True)","74b0142a":"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nprint(accuracy_score(y_test,predictions))\nprint(accuracy_score(y_test,predictions2))\n#As we can see more depth from 2 to 3 gives usaccuracy from %93 to %100","067acc2d":"print(classification_report(y_test,predictions))\nprint(\"*****************************************************************************\")\nprint(classification_report(y_test,predictions2))","6de49b2b":"print(confusion_matrix(y_test,predictions))\nprint(\"*****************************************************************************\")\nprint(confusion_matrix(y_test,predictions2))","dc9d2955":"plt.figure(figsize=(12,8))\nplt.imshow(plt.imread(\"..\/input\/depths\/depths.PNG\"))","0fc721c3":"print(dtree.predict_proba([[4.6, 3.4,1.4,0.3]]))\nprint(dtree2.predict_proba([[4.6, 3.4,1.4,0.3]]))","ec319a77":"print(dtree.predict([[4.6, 3.4,1.4,0.3]]))\nprint(dtree2.predict([[4.6, 3.4,1.4,0.3]]))\n","4068f0d9":"y_test.head(1) #I used the first properties of the testt set and both of them predicted Itish-setosa correctly","3a0264f0":"plt.figure(figsize=(12,8))\nplt.imshow(plt.imread(\"..\/input\/decision2\/decision.PNG\"))","42fa0cdc":"plt.figure(figsize=(15,10))\nplt.imshow(plt.imread(\"..\/input\/coins-img\/coins.PNG\"))","74d0dab7":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntrain.head()","d8dee95a":"def age_mean(col):\n    Age=col[0]\n    Pclass=col[1]\n    if pd.isnull(Age):\n        \n        if Pclass==1:\n            return 38\n        elif Pclass ==2:\n            return 29\n        else:\n            return 25\n    else:\n        return Age","431e796e":"train[\"Age\"]=train[[\"Age\",\"Pclass\"]].apply(age_mean,axis=1)\ntrain[\"Age\"].head()","e1eb51c9":"test[\"Age\"]=test[[\"Age\",\"Pclass\"]].apply(age_mean,axis=1)\ntest[\"Age\"]","81ffcbba":"train.drop(\"Cabin\",axis=1,inplace=True)\ntest.drop(\"Cabin\",axis=1,inplace=True)\ntrain[\"Embarked\"].fillna(value=\"S\",inplace=True)\ntest[\"Fare\"].fillna(value=test[\"Fare\"].mean(),inplace=True)\ntrain.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)\ntest.drop([\"PassengerId\",\"Name\",\"Ticket\"],axis=1,inplace=True)","9d27fcd2":"train.isnull().sum()","90503171":"test.isnull().sum()","e1314578":"train=pd.get_dummies(data=train,columns=[\"Sex\",\"Embarked\"],drop_first=True)\ntrain.head()","55a66973":"test=pd.get_dummies(data=test,columns=[\"Sex\",\"Embarked\"],drop_first=True)\ntest.head()","a4416cbb":"X=train.drop(\"Survived\",axis=1)\ny= train[\"Survived\"]\nprint(X.head())\nprint(y.head())","132c5fa3":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.05, random_state=2)\n","54c088b3":"from sklearn.ensemble import VotingClassifier, RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\nforest = RandomForestClassifier()\nsvm_classifier = SVC(probability=True,kernel='linear')\nlogistic = LogisticRegression()\n\nvoting_clfr= VotingClassifier(estimators=[(\"logistic\",logistic),(\"forest\",forest),(\"svm\",svm_classifier)],voting=\"soft\")\nvoting_clfr.fit(X_train,y_train)\n","2fd62c28":"from sklearn.metrics import accuracy_score\nfor classifier in (forest,svm_classifier,logistic, voting_clfr):\n    classifier.fit(X_train, y_train)\n    predictions = classifier.predict(X_test)\n    print(classifier.__class__.__name__, accuracy_score(y_test, predictions))","5ff91d3c":"plt.figure(figsize=(15,10))\nplt.imshow(plt.imread(\"..\/input\/ensemble\/ensemble.PNG\"))","65ce6a58":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier","cacf6bd5":"#First we try pasting\npasting_clf = BaggingClassifier(\nDecisionTreeClassifier(), n_estimators=500,\nmax_samples=100, bootstrap=False, n_jobs=-1)\npasting_clf.fit(X_train, y_train)\ny_pred = pasting_clf.predict(X_test)","b73bc356":"print(accuracy_score(y_test, y_pred))","aefa68b4":"#Lets try Bagging:\nbagging_clf= BaggingClassifier(DecisionTreeClassifier(),n_estimators=500,max_samples=100,bootstrap=True,n_jobs=-1)\nbagging_clf.fit(X_train,y_train)\ny_pred2 = bagging_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred2))","b869d973":"dtree= DecisionTreeClassifier()\ndtree.fit(X_train,y_train)\ndtree_preds= dtree.predict(X_test)\nprint(accuracy_score(y_test, dtree_preds))\n#As we can see it both bagging and pasting methods performs better than the single decisiion tree algorithm","5e34c730":"bagging_clf2= BaggingClassifier(DecisionTreeClassifier(),n_estimators=500,max_samples=100,bootstrap=True,n_jobs=-1,oob_score=True)\nbagging_clf2.fit(X_train,y_train)\ny_pred3 = bagging_clf2.predict(X_test)\nprint(accuracy_score(y_test, y_pred3))","266d21f3":"bagging_clf2.oob_score_","2389c7d3":"#Lets try BAgging with SVM\nbagging_clf3= BaggingClassifier(SVC(kernel='linear', probability=True),n_estimators=500,max_samples=100,bootstrap=True,n_jobs=-1)\nbagging_clf3.fit(X_train,y_train)\ny_pred4 = bagging_clf3.predict(X_test)\nprint(accuracy_score(y_test, y_pred4))\n#We have better results","bdb205fd":"#Lets try PAsting with SVM\nbagging_clf4= BaggingClassifier(SVC(kernel='linear', probability=True),n_estimators=500,max_samples=100,bootstrap=False,n_jobs=-1)\nbagging_clf4.fit(X_train,y_train)\ny_pred5 = bagging_clf4.predict(X_test)\nprint(accuracy_score(y_test, y_pred5))","63bfcb70":"forest_clf = RandomForestClassifier()\nforest_clf.fit(X_train, y_train)\ny_pred6 = forest_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred6))","016932c6":"forest_clf.feature_importances_","c97ef5e5":"for feature, score in zip(X, forest_clf.feature_importances_):\n    print(feature, score)","00460adb":"from sklearn.ensemble import ExtraTreesClassifier\nextra_clf = ExtraTreesClassifier(n_estimators=500,criterion=\"entropy\", max_samples=50,max_leaf_nodes=16, n_jobs=-1)\nextra_clf.fit(X_train, y_train)\ny_pred7 = extra_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred7))","c9ee7d1f":"from sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier(\nDecisionTreeClassifier(max_depth=2), n_estimators=500,\nalgorithm=\"SAMME.R\", learning_rate=1)\nada.fit(X_train, y_train)\ny_pred8 = ada.predict(X_test)\nprint(accuracy_score(y_test, y_pred8))\n# we get %80 accuracy","88220bfb":"from sklearn.ensemble import GradientBoostingClassifier\ngbrt = GradientBoostingClassifier(max_depth=2, n_estimators=3, learning_rate=3)\ngbrt.fit(X_train, y_train)\ny_pred10 = gbrt.predict(X_test)\nprint(accuracy_score(y_test, y_pred10))","f95c17f8":"import xgboost\nfrom sklearn.metrics import log_loss\nxgb = xgboost.XGBClassifier()\nxgb.fit(X_train, y_train)\ny_pred11 = gbrt.predict(X_test)\nprint(accuracy_score(y_test, y_pred11))\nprint(log_loss(y_test, y_pred11))","cca77d11":"xgb2 = xgboost.XGBClassifier()\nxgb2.fit(X_train, y_train,\neval_set=[(X_test, y_test)], early_stopping_rounds=2)\ny_pred = xgb2.predict(X_test)\nprint(accuracy_score(y_test, y_pred))\nprint(log_loss(y_test, y_pred))","cf273aa1":"plt.figure(figsize=(15,10))\nplt.imshow(plt.imread(\"..\/input\/threedimens\/threedimesion.PNG\"))","6fe74d9e":"plt.figure(figsize=(15,10))\nplt.imshow(plt.imread(\"..\/input\/twodimens\/twodimesional.PNG\"))","d3b9b7f6":"plt.figure(figsize=(15,10))\nplt.imshow(plt.imread(\"..\/input\/swissroll\/swissroll.PNG\"))","f17af6fa":"plt.figure(figsize=(15,10))\nplt.imshow(plt.imread(\"..\/input\/swissroll2\/swissroll2.PNG\"))","3bface38":"plt.figure(figsize=(15,10))\nplt.imshow(plt.imread(\"..\/input\/principal\/principal.PNG\"))","ac09c45e":"#Lets find principal components of Titanic Dataset:\nX_centered = X - X.mean(axis=0)\nU, s, Vt = np.linalg.svd(X_centered)\nc1 = Vt.T[:, 0]\nc2 = Vt.T[:, 1]\nc1","cb779766":"c2","e13bdb10":"W2 = Vt.T[:, :2]\nX2D = X_centered.dot(W2)\nX2D","3c769a6e":"from sklearn.decomposition import PCA\npca = PCA(n_components = 2)\nX2D_2 = pca.fit_transform(X)\nX2D_2 # we get the same results as we did previously by our own code above.","73d9347b":"pca.components_.T[:, 0]","534d7db0":"pca.explained_variance_ratio_","93349295":"pca = PCA()\npca.fit(X)\ncumsum = np.cumsum(pca.explained_variance_ratio_)\nd = np.argmax(cumsum >= 0.95) + 1\nd # we find the ideal values for n dimensions","ed3c08ef":"pca = PCA(n_components=0.95)\nX_reduced = pca.fit_transform(X)\nX_reduced # we get the same results","b5cf3ef1":"pca = PCA(n_components = 2)\nX_reduced = pca.fit_transform(X)\nprint(X_reduced.shape)\nX_recovered = pca.inverse_transform(X_reduced)\nprint(X_recovered.shape)\nprint(X.shape)","73639f9d":"#Lets compare X recovered with original X","87218b14":"pd.DataFrame(X_recovered)","a60b9afb":"X #There are some differenced between recovered X and original X, but it is not so high.","8a830d00":"rnd_pca = PCA(n_components=2, svd_solver=\"randomized\")\nX_randomized_reduced = rnd_pca.fit_transform(X)\nX_randomized_reduced","d3e6634e":"X_reduced","6eec7095":"rnd_pca = PCA(n_components=2, svd_solver=\"full\")\nX_full_reduced = rnd_pca.fit_transform(X)\nX_full_reduced","84b25bf4":"from sklearn.decomposition import KernelPCA\nrbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\nX_reduced = rbf_pca.fit_transform(X)\nX_reduced","2835e74b":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nclf = Pipeline([\n(\"kpca\", KernelPCA(n_components=2)),\n(\"log_reg\", LogisticRegression())\n])\nparam_grid = [{\n\"kpca__gamma\": np.linspace(0.03, 0.05, 10),\n\"kpca__kernel\": [\"rbf\", \"sigmoid\"]\n}]\ngrid_search = GridSearchCV(clf, param_grid, cv=3)\ngrid_search.fit(X, y)","5920f209":"print(grid_search.best_params_)","764f9b55":"rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.03222222222222222,fit_inverse_transform=True)\nX_reduced = rbf_pca.fit_transform(X)\nX_preimage = rbf_pca.inverse_transform(X_reduced)\npd.DataFrame(X_preimage) \n# we see that we have almost the same values as we in the original data","209b20f3":"X","1da8b228":"from sklearn.manifold import LocallyLinearEmbedding\nlle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\nX_reduced_lle = lle.fit_transform(X)\nX_reduced_lle","57ff537a":"X_reduced","3d10bce1":"df=pd.read_csv(\"https:\/\/cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud\/IBMDeveloperSkillsNetwork-ML0101EN-SkillsNetwork\/labs\/Module%204\/data\/Cust_Segmentation.csv\")\ndf.head()","3dcf4111":"df.drop([\"Customer Id\", \"Address\"],axis=1,inplace=True)\ndf.head()","b6030e90":"df.isnull().sum()","6a3d219a":"df[\"Defaulted\"].value_counts()","7c1b1dcb":"df[\"Defaulted\"].fillna(0,inplace=True)\ndf.isnull().sum()","3ff3f9fe":"from sklearn.cluster import KMeans","d359ee0c":"from yellowbrick.cluster import KElbowVisualizer\nmodel = KMeans()\nvisualizer = KElbowVisualizer(model, k=(1,12))\n\nvisualizer.fit(df)        # Fit the data to the visualizer\nvisualizer.show()        # Finalize and render the figure\n#It seems the best value for k clusters =3 ","f2f10c14":"from sklearn.metrics import silhouette_score\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(df)\nlabel= kmeans.predict(df)\nsilhouette_score(df,label )","36d4d947":"scores=list()\nfor n_clusters in range(2,12):\n    clusterer = KMeans(n_clusters=n_clusters)\n    clusterer.fit(df)\n    preds = clusterer.predict(df)\n    score = silhouette_score(df, preds)\n    scores.append(score)\n    print(\"For n_clusters = {}, silhouette score is {})\".format(n_clusters, score))","7f13211d":"plt.scatter(list(range(2,12)), scores)","9d6238dc":"img = plt.imread(\"..\/input\/mixima\/miximage.jpg\")\nimg.shape","a3f14315":"plt.imshow(img)","1a1df793":"X = img.reshape((-1, 3))\nX.shape","36bd39ea":"from sklearn.cluster import KMeans\nX = img.reshape(-1, 3)\nkmeans = KMeans(n_clusters=3).fit(X)\nsegmented_img = kmeans.cluster_centers_[kmeans.labels_]\nsegmented_img.shape","13128f18":"segmented_img = segmented_img.reshape(img.shape)\nsegmented_img.shape","17631a04":"plt.imshow(segmented_img)","8ef1cd89":"<font color=\"red\">\nSelecting a Kernel and Tuning Hyperparameters:","6d50fb29":"<font color=\"purple\">\nA forest of such extremely random trees is called an Extremely Randomized Trees\nensemble (or Extra-Trees for short). Once again, this technique trades more bias for a\nlower variance. It also makes Extra-Trees much faster to train than regular Random\nForests, because finding the best possible threshold for each feature at every node is one\nof the most time-consuming tasks of growing a tree.\nYou can create an Extra-Trees classifier using Scikit-Learn\u2019s ExtraTreesClassifier\nclass. Its API is identical to the RandomForestClassifier class.","55bb5d1e":"<font color=\"red\">\n7.2. Regression","6b432e16":"## 4. Logistic Regression:","f697bfb5":"<font color=\"blue\">\nElastic Net is a middle ground between Ridge Regression and Lasso\nRegression. The regularization term is a simple mix of both Ridge and\nLasso\u2019s regularization terms, and you can control the mix ratio r. When r =\n0, Elastic Net is equivalent to Ridge Regression, and when r = 1, it is\nequivalent to Lasso Regression","b495024f":"<font color=\"green\">\nHowever, projection is not always the best approach to dimensionality\nreduction. In many cases the subspace may twist and turn, such as in the\nfamous Swiss roll toy dataset represented in the figures below:","f0a68769":"<font color=\"purple\">\nAfter fitting the PCA transformer to the dataset, its components_ attribute\nholds the transpose of W (e.g., the unit vector that defines the first\nprincipal component is equal to pca.components_.T[:, 0]).","8dcd3d32":"<font color=\"red\">\nRecontructing to Original Space:","7f5f99d2":"<font color=\"blue\">\nNow we can make predictions using \u02c6\u03b8:","3e268dea":"Notice that this formula involves calculations over the full training set X, at each\nGradient Descent step! This is why the algorithm is called Batch Gradient Descent: it\nuses the whole batch of training data at every step (actually, Full Gradient Descent\nwould probably be a better name). As a result it is terribly slow on very large training\nsets (but we will see much faster Gradient Descent algorithms shortly).","5bec3b31":"<font color=\"blue\">\n    \nA Support Vector Machine (SVM) is a powerful and versatile Machine Learning model,\ncapable of performing linear or nonlinear classification, regression, and even outlier\ndetection. It is one of the most popular models in Machine Learning, and anyone interested\nin Machine Learning should have it in their toolbox. SVMs are particularly well suited for\nclassification of complex small- or medium-sized datasets.\n    \nYou can\nthink of an SVM classifier as fitting the widest possible street (represented by the parallel\ndashed lines) between the classes.\n    \nNotice that adding more training instances \u201coff the street\u201d will not affect the decision\nboundary at all: it is fully determined (or \u201csupported\u201d) by the instances located on the edge\nof the street. These instances are called the support vectors (they are circled in the figure below:","48c31e6a":"<font color=\"red\">\n3.2.Lasso Regression:","d2c934da":"<font color=\"blue\">\nBoosting (originally called hypothesis boosting) refers to any Ensemble method that can\ncombine several weak learners into a strong learner. The general idea of most boosting\nmethods is to train predictors sequentially, each trying to correct its predecessor. \n\n    \nThere are many boosting methods available, but by far the most popular are AdaBoost (short\nfor Adaptive Boosting) and Gradient Boosting","42fa5776":"<font color=\"green\">\nGradient Descent is a generic optimization algorithm capable of finding\noptimal solutions to a wide range of problems. The general idea of GradientDescent is to tweak parameters iteratively in order to minimize a cost\nfunction.\n    \nSuppose you are lost in the mountains in a dense fog, and you can only feel\nthe slope of the ground below your feet. A good strategy to get to the bottom\nof the valley quickly is to go downhill in the direction of the steepest slope.\nThis is exactly what Gradient Descent does: it measures the local gradient of\nthe error function with regard to the parameter vector \u03b8, and it goes in the\ndirection of descending gradient. Once the gradient is zero, you have reached\na minimum.\n\nConcretely, you start by filling \u03b8 with random values (this is called random\ninitialization). Then you improve it gradually, taking one baby step at a time,\neach step attempting to decrease the cost function (e.g., the MSE), until the\nalgorithm converges to a minimum","70f36a50":"<font color=\"green\">\nIn Machine Learning, vectors are often represented as column vectors, which are 2D\narrays with a single column. If \u03b8 and x are column vectors, then the prediction is\n\u02c6y = \u03b8\u22bax, where \u03b8\u22ba is the transpose of \u03b8 (a row vector instead of a column vector) and\n\u03b8\u22bax is the matrix multiplication of \u03b8\u22ba and x. It is of course the same prediction, except\nthat it is now represented as a single-cell matrix rather than a scalar value.","4c1a349f":"<font color=\"green\">\nAnother approach to get a diverse set of classifiers is to use the same training algorithm for every\npredictor and train them on different random subsets of the training set. When sampling\nis performed with replacement, this method is called bagging (short for bootstrap\naggregating ). When sampling is performed without replacement, it is called pasting.\n    \nIn other words, both bagging and pasting allow training instances to be sampled several\ntimes across multiple predictors, but only bagging allows training instances to be\nsampled several times for the same predictor.\n    \n    \nOnce all predictors are trained, the ensemble can make a prediction for a new instance\nby simply aggregating the predictions of all predictors. The aggregation function is\ntypically the statistical mode (i.e., the most frequent prediction, just like a hard voting\nclassifier) for classification, or the average for regression. Each individual predictor has\na higher bias than if it were trained on the original training set, but aggregation reduces\nboth bias and variance.Generally, the net result is that the ensemble has a similar bias\nbut a lower variance than a single predictor trained on the original training set.\n    \n    \npredictors can all be trained in parallel, via different CPU\ncores or even different servers. Similarly, predictions can be made in parallel. This is\none of the reasons bagging and pasting are such popular methods: they scale very well.","e583d595":"<font color=\"green\">\nIt is based on a simple idea: instead of using trivial functions\n(such as hard voting) to aggregate the predictions of all predictors in an ensemble, why\ndon\u2019t we train a model to perform this aggregation?","ccb93389":"<font color=\"green\">\nIt is worth noting that an optimized implementation of Gradient Boosting is available in\nthe popular Python library XGBoost, which stands for Extreme Gradient Boosting.","670e144a":"<font color=\"red\">\n7.1. Classification","9860598e":"A Decision Tree can also estimate the probability that an instance belongs to a particular\nclass k. First it traverses the tree to find the leaf node for this instance, and then it returns the\nratio of training instances of class k in this node. For example, suppose you have found a\nflower whose petals are 5 cm long and 1.5 cm wide.","73c5bddf":"<font color=\"blue\">\nYou may be wondering how to perform this reconstruction. One solution is\nto train a supervised regression model, with the projected instances as the\ntraining set and the original instances as the targets. Scikit-Learn will do\nthis automatically if you set fit_inverse_transform=True, as shown in\nthe following code:","5362b0cc":"<font color=\"red\">\n8.4.3.XGBoost:","04d1687d":"After dimensionality reduction, the training set takes up much less space.It is also possible to decompress the reduced dataset back to the original\ndimensions by applying the inverse transformation of the PCA projection.\n\n\nThe following code compresses the MNIST dataset down to the original dimensions\ndimensions, then uses the inverse_transform() method to decompress it\nback to 784 dimensions:","27e1ca4f":"<font color=\"green\">\nJust like AdaBoost,\nGradient Boosting works by sequentially adding predictors to an ensemble, each one\ncorrecting its predecessor. However, instead of tweaking the instance weights at every\niteration like AdaBoost does, this method tries to fit the new predictor to the residual\nerrors made by the previous predictor.\n    \nMuch like the RandomForestRegressor class, it\nhas hyperparameters to control the growth of Decision Trees (e.g., max_depth,\nmin_samples_leaf), as well as hyperparameters to control the ensemble training, such\nas the number of trees (n_estimators).","961a26e8":"Out-of-Bag Evaluation:","298f9365":"<font color=\"green\">\nNow let\u2019s compute \u02c6\u03b8 using the Normal Equation. \n\nWe will use the inv() function from NumPy\u2019s linear algebra module (np.linalg) to compute the\ninverse of a matrix, \n\nand the dot() method for matrix multiplication:","e8d9972f":"<font color=\"red\">\n1.1.3. Mini-batch Gradient Descent:","f21bceb0":"<font color=\"purple\">\nOnce you have identified all the principal components, you can reduce the\ndimensionality of the dataset down to d dimensions by projecting it onto\nthe hyperplane defined by the first d principal components. Selecting this\nhyperplane ensures that the projection will preserve as much variance as\npossible.\n    \nThe following Python code projects the training set onto the plane defined\nby the first two principal components:","6ffe5928":"The image is represented as a 3D array. The first dimension\u2019s size is the\nheight; the second is the width; and the third is the number of color\nchannels, in this case red, green, and blue (RGB). In other words, for each\npixel there is a 3D vector containing the intensities of red, green, and blue,\neach between 0.0 and 1.0","5291c053":"<font color=\"green\">\n\nLets do the same operation via scikit-learn, not by our own way","48e1eaa6":"<font color=\"purple\">\nSo how can you find the principal components of a training set? Luckily,\nthere is a standard matrix factorization technique called Singular Value\nDecomposition (SVD) that can decompose the training set matrix X into\nthe matrix multiplication of three matrices U \u03a3 V , where V contains the\nunit vectors that define all the principal components that we are looking\nfor.\n    \n    \nThe following Python code uses NumPy\u2019s svd() function to obtain all the\nprincipal components of the training set, then extracts the two unit vectors\nthat define the first two PCs:","904711ee":"<font color=\"red\">\n9. Dimensionality Reduction :","25b5e0d8":"<font color=\"purple\">\nInstead of arbitrarily choosing the number of dimensions to reduce down\nto, it is simpler to choose the number of dimensions that add up to a\nsufficiently large portion of the variance (e.g., 95%).\n    \n    \n    \nThe following code performs PCA without reducing dimensionality, then\ncomputes the minimum number of dimensions required to preserve 95%\nof the training set\u2019s variance:","beca1e96":"1. Elbow Method:","a6d9c6da":"<font color=\"green\">\nSuppose you pose a complex question to thousands of random people, then aggregate\ntheir answers. In many cases you will find that this aggregated answer is better than an\nexpert\u2019s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the\npredictions of a group of predictors (such as classifiers or regressors), you will often get\nbetter predictions than with the best individual predictor. A group of predictors is called\nan ensemble; thus, this technique is called Ensemble Learning, and an Ensemble\nLearning algorithm is called an Ensemble method.\n    \n    \nAs an example of an Ensemble method, you can train a group of Decision Tree\nclassifiers, each on a different random subset of the training set. To make predictions,\nyou obtain the predictions of all the individual trees, then predict the class that gets the\nmost votes . Such an ensemble of Decision Trees is\ncalled a Random Forest, and despite its simplicity, this is one of the most powerful\nMachine Learning algorithms available today.\n    \nHow is possible? this ensemble classifier often achieves a higher accuracy than any single the\nbest classifier in the ensemble.","3493c961":"<font color=\"red\">\nTo train a Linear Regression model means to find the value of \u03b8 that minimizes the error, or the difference the values of model's predictions and actual values.\n\nIn practice,the aim is to minimize the mean squared error (MSE).","4747845b":"<font color=\"blue\">\nLinear Regression model prediction (vectorized form):\n\n    y = h\u03b8(x) = \u03b8 \u22c5 X\n\n\u03b8 is the model\u2019s parameter vector, containing the bias term \u03b8 and\nthe feature weights \u03b8 to \u03b8 .\n    \nx is the instance\u2019s feature vector, containing x to x , with x always\nequal to 1.\n    \n\u03b8 \u00b7 x is the dot product of the vectors \u03b8 and x, which is of course\nequal to \u03b80x0 + \u03b81x1 + \u03b82x2 +\u22ef+ \u03b8nxn.\n    \nh is the hypothesis function, using the model parameters \u03b8.","af0d9fda":"<font color=\"red\">\n6.2.2. Non-Linear Implementing Support Vector Machines:","f54f3b8b":"<font color=\"red\">\n10.1. KMeans:","c614c3b1":"<font color=\"green\">\n    \nGini Impurity or Entropy?\n    \nBy default, the Gini impurity measure is used, but you can select the entropy impurity\nmeasure instead by setting the criterion hyperparameter to \"entropy\". The concept of\nentropy originated in thermodynamics as a measure of molecular disorder: entropy\napproaches zero when molecules are still and well ordered. Entropy later spread to a wide\nvariety of domains, including Shannon\u2019s information theory, where it measures the average\ninformation content of a message: entropy is zero when all messages are identical. In\nMachine Learning, entropy is frequently used as an impurity measure: a set\u2019s entropy is zero\nwhen it contains instances of only one class.\n    \nThe truth is, most of the time it does not make\na big difference: they lead to similar trees. Gini impurity is slightly faster to compute, so it\nis a good default. However, when they differ, Gini impurity tends to isolate the most\nfrequent class in its own branch of the tree, while entropy tends to produce slightly more\nbalanced trees.","5108849c":"<font color=\"orange\">\nLasso Regression is another regularized version of Linear\nRegression: just like Ridge Regression, it adds a regularization term to the\ncost function, but it uses the \u2113 norm of the weight vector instead of half the\nsquare of the \u2113 norm\n    \nAn important characteristic of Lasso Regression is that it tends to eliminate\nthe weights of the least important features (i.e., set them to zero).\n\nIn other words, Lasso Regression\nautomatically performs feature selection and outputs a sparse model (i.e.,\nwith few nonzero feature weights).","2b2a545c":"<font color=\"red\">\n8.4.Boosting:","e069da63":"<font color=\"red\">\nIt is important to scale the data (e.g., using a StandardScaler) before performing\nRidge Regression, as it is sensitive to the scale of the input features. This is true of most\nregularized models.","e1f6e9e3":"In this equation:\ny(i)k is the target probability that the i instance belongs to class k.\nIn general, it is either equal to 1 or 0, depending on whether the\ninstance belongs to the class or not.","30ae2ce0":"<font color=\"red\">\n8.3. Random Forests:","ebca3198":"np.c_   = Stack 1-D arrays as columns into a 2-D array","736cfd1f":"<font color=\"green\">\nThe following analogy can help shed some light on this mystery.\nSuppose you have a slightly biased coin that has a 51% chance of coming up heads and\n49% chance of coming up tails. If you toss it 1,000 times, you will generally get more\nor less 510 heads and 490 tails, and hence a majority of heads. If you do the math, you\nwill find that the probability of obtaining a majority of heads after 1,000 tosses is close\nto 75%. The more you toss the coin, the higher the probability (e.g., with 10,000 tosses,\nthe probability climbs over 97%). This is due to the law of large numbers: as you keep\ntossing the coin, the ratio of heads gets closer and closer to the probability of heads\n(51%).\n    \nSimilarly, suppose you build an ensemble containing 1,000 classifiers that are\nindividually correct only 51% of the time.","32a1b58d":"<font color=\"red\">\n8.4.1. AdaBoost:","1d50024e":"<font color=\"blue\">\nJust like the polynomial features method, the similarity features method can be useful with\nany Machine Learning algorithm, but it may be computationally expensive to compute all\nthe additional features, especially on large training sets. Once again the kernel trick does\nits SVM magic, making it possible to obtain a similar result as if you had added many\nsimilarity features.\n    \nDoing that creates many\ndimensions and thus increases the chances that the transformed training set will be linearly\nseparable.","22f50c7a":"<font color=\"green\">\nAlthough linear SVM classifiers are efficient and work surprisingly well in many cases,\nmany datasets are not even close to being linearly separable.\n    \nAdding polynomial features is simple to implement and can work great with all sorts of\nMachine Learning algorithms (not just SVMs). That said, at a low polynomial degree, this\nmethod cannot deal with very complex datasets, and with a high polynomial degree it\ncreates a huge number of features, making the model too slow.","03d2f7d9":"<font color=\"blue\">\nThere are many other dimensionality reduction techniques, several of\nwhich are available in Scikit-Learn. Here are some of the most popular\nones:\n    \n    Random Projections\n    \n    \nAs its name suggests, projects the data to a lower-dimensional space\nusing a random linear projection. This may sound crazy, but it turns\nout that such a random projection is actually very likely to preserve\ndistances well, as was demonstrated mathematically by William B.\nJohnson and Joram Lindenstrauss in a famous lemma. The quality of\nthe dimensionality reduction depends on the number of instances and\nthe target dimensionality, but surprisingly not on the initial\ndimensionality. Check out the documentation for the\nsklearn.random_projection package for more details.\n    \n    \n    \n    Multidimensional Scaling (MDS)\n    \n    \nReduces dimensionality while trying to preserve the distances between\nthe instances.\n    \n    \n    Isomap\nCreates a graph by connecting each instance to its nearest neighbors,\nthen reduces dimensionality while trying to preserve the geodesic\ndistances between the instances.\n    \n    \n    t-Distributed Stochastic Neighbor Embedding (t-SNE)\nReduces dimensionality while trying to keep similar instances close\nand dissimilar instances apart. It is mostly used for visualization, in\nparticular to visualize clusters of instances in high-dimensional space\n(e.g., to visualize the MNIST images in 2D).\n    \n    \n    Linear Discriminant Analysis (LDA)\nIs a classification algorithm, but during training it learns the most\ndiscriminative axes between the classes, and these axes can then be\nused to define a hyperplane onto which to project the data. The benefit of this approach is that the projection will keep classes as far apart as possible, so LDA is a good technique to reduce dimensionality before\nrunning another classification algorithm such as an SVM classifier.","36cc5c56":"<font color=\"blue\">\nClustering is used in a wide variety of applications, including these:\n    1.For customer segmentation:\nYou can cluster your customers based on their purchases and their\nactivity on your website. This is useful to understand who your\ncustomers are and what they need, so you can adapt your products and\nmarketing campaigns to each segment. For example, customer\nsegmentation can be useful in recommender systems to suggest content\nthat other users in the same cluster enjoyed.\n    2.For data analysis:\nWhen you analyze a new dataset, it can be helpful to run a clustering\nalgorithm, and then analyze each cluster separately.\n    3.For anomaly detection (also called outlier detection):\nAny instance that has a low affinity to all the clusters is likely to be an\nanomaly. For example, if you have clustered the users of your website\nbased on their behavior, you can detect users with unusual behavior,\nsuch as an unusual number of requests per second. Anomaly detection\nis particularly useful in detecting defects in manufacturing, or for\nfraud detection.","122e6ac4":"Ridge(alpha=1.0, *, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver='auto', random_state=None)\n |  \n |  Linear least squares with l2 regularization.\n |  \n |  Minimizes the objective function::\n |  \n |  ||y - Xw||^2_2 + alpha * ||w||^2_2\n |  \n |  This model solves a regression model where the loss function is\n |  the linear least squares function and regularization is given by\n |  the l2-norm. Also known as Ridge Regression or Tikhonov regularization.\n \n Parameters\n \n |  ----------\n alpha : {float, ndarray of shape (n_targets,)}, default=1.0\n |      Regularization strength; must be a positive float\n \n fit_intercept : bool, default=True\n |      Whether to fit the intercept for this model. If set\n |      to false, no intercept will be used in calculations\n \n normalize : bool, default=False\n |      This parameter is ignored when ``fit_intercept`` is set to False.\n |      If True, the regressors X will be normalized before regression by\n |      subtracting the mean and dividing by the l2-norm.\n \n solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'},         default='auto'\n |      Solver to use in the computational routines:\n |  \n |      - 'auto' chooses the solver automatically based on the type of data.\n |  \n |      - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n |        coefficients. More stable for singular matrices than 'cholesky'.\n |  \n |      - 'cholesky' uses the standard scipy.linalg.solve function to\n |        obtain a closed-form solution.\n |  \n |      - 'sparse_cg' uses the conjugate gradient solver as found in\n |        scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n |        more appropriate than 'cholesky' for large-scale data\n |        (possibility to set `tol` and `max_iter`).\n |  \n |      - 'lsqr' uses the dedicated regularized least-squares routine\n |        scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n |        procedure.\n |  \n |      - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n |        its improved, unbiased version named SAGA.\n \n ","0f9ea8d7":"<font color=\"blue\">\nThe objective is to have a model that\nestimates a high probability for the target class (and consequently a low\nprobability for the other classes). Minimizing the cost function  called the cross entropy, should lead to this objective because\nit penalizes the model when it estimates a low probability for a target class.\n\nCross entropy is frequently used to measure how well a set of estimated\nclass probabilities matches the target classes","dc57afab":"<font color=\"blue\">\nThe Normal Equation:\n\nTo find the value of \u03b8 that minimizes the cost function, Normal Equation is used as follows:\n    In this equation:\n\n    \u02c6\u03b8 is the value of \u03b8 that minimizes the cost function.\n    \n     y is the vector of target values containing y to y .\n    \n    X is the features vector.\n","ff86f743":"<font color=\"green\">\nNot all cost functions look like nice, regular bowls. There may be\nholes, ridges, plateaus, and all sorts of irregular terrains, making\nconvergence to the minimum difficult.  If the random initialization starts the\nalgorithm on the left, then it will converge to a local minimum, which is not\nas good as the global minimum. If it starts on the right, then it will take a\nvery long time to cross the plateau. And if you stop too early, you will never\nreach the global minimum.","8886a61f":"<font color=\"green\">","a0039558":"<font color=\"red\">\n1.1. Using Gradient Descent:","43b110c1":"<font color=\"green\">\nXGBoost also offers several nice features, such as automatically taking care of early\nstopping:","9e121a69":"<font color=\"purple\">\nA Random Forest is an ensemble of Decision Trees, generally\ntrained via the bagging method (or sometimes pasting), typically with max_samples set\nto the size of the training set. Instead of building a BaggingClassifier and passing it a\nDecisionTreeClassifier, you can instead use the RandomForestClassifier class,\nwhich is more convenient and optimized for Decision Trees.","f9c89db9":"<font color=\"red\">\n6.1.Soft Margin Classification for Support Vector Machines Algorithm:","e1a28575":"<font color=\"red\">\n9.2. PCA:","48f266cf":"Projection:","8309826c":"<font color=\"green\">\nThere are 3 types of gradient descent\n    \n    1. Batch Gradient Descent:\n    \n    2. Mini Batch Gradient Descent:\n    \n    3. Stochastic Gradient Descent","134cebf1":"<font color=\"green\">\nRidge Regression (also called Tikhonov regularization) is a regularized\nversion of Linear Regression: a regularization term equal \nadded to the cost function. This forces the learning algorithm to not only fit\nthe data but also keep the model weights as small as possible. Note that the\nregularization term should only be added to the cost function during training.\nOnce the model is trained, you want to use the unregularized performance\nmeasure to evaluate the model\u2019s performance.","94d7cca0":"Just like for classification tasks, Decision Trees are prone to overfitting when dealing with\nregression tasks. Without any regularization (i.e., using the default hyperparameters),  These predictions are obviously overfitting the\ntraining set very badly. Just setting min_samples_leaf=10 results in a much more\nreasonable model.","c94dd7c9":"<font color=\"blue\">\nBy default, fit_inverse_transform=False and KernelPCA has no\ninverse_transform() method. This method only gets created when you set\nfit_inverse_transform=True.","77ae9513":"<font color=\"purple\">\nJust like a Linear Regression model,\na Logistic Regression model computes a weighted sum of the input features\n(plus a bias term), but instead of outputting the result directly like the Linear\nRegression model does, it outputs the logistic of this result","cc75ee41":"<font color=\"red\">\n1.1.1 Batch Gradient Descent:","b3564545":"<font color=\"purple\">\nA RandomForestClassifier has all the hyperparameters of a\nDecisionTreeClassifier (to control how trees are grown), plus all the\nhyperparameters of a BaggingClassifier to control the ensemble itself.\n    \n    \nThe Random Forest algorithm introduces extra randomness when growing trees; instead\nof searching for the very best feature when splitting a node (see Chapter 6), it searches\nfor the best feature among a random subset of features. The algorithm results in greater\ntree diversity, which (again) trades a higher bias for a lower variance, generally yielding\nan overall better model.","de667958":"<font color=\"red\">\nUsing Scikit-Learn:","3af68762":"<font color=\"red\">\nSo when should you use plain Linear Regression (i.e., without any\nregularization), Ridge, Lasso, or Elastic Net? It is almost always preferable\nto have at least a little bit of regularization, so generally you should avoid\nplain Linear Regression. Ridge is a good default, but if you suspect that only\na few features are useful, you should prefer Lasso or Elastic Net because\nthey tend to reduce the useless features\u2019 weights down to zero, as we have\ndiscussed. In general, Elastic Net is preferred over Lasso because Lasso may\nbehave erratically when the number of features is greater than the number of\ntraining instances or when several features are strongly correlated.","800cea2f":"ElasticNet(alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, normalize=False, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n |  \n |  Linear regression with combined L1 and L2 priors as regularizer.\n \n alpha : float, default=1.0\n |      Constant that multiplies the penalty terms.\n \n l1_ratio : float, default=0.5\n |      The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For\n |      ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it\n |      is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a\n |      combination of L1 and L2.\n |  \n |  fit_intercept : bool, default=True\n |      Whether the intercept should be estimated or not. If ``False``, the\n |      data is assumed to be already centered.\n \n selection : {'cyclic', 'random'}, default='cyclic'\n |      If set to 'random', a random coefficient is updated every iteration\n |      rather than looping over features sequentially by default. This\n |      (setting to 'random') often leads to significantly faster convergence\n |      especially when tol is higher than 1e-4.","dfbd7692":"Higher Silhouette Score is better than lower.","a510d4b4":"## 2. Polynomial Regression:","852db66c":"By default, svd_solver is actually set to \"auto\": Scikit-Learn\nautomatically uses the randomized PCA algorithm if m or n is greater than\n500 and d is less than 80% of m or n, or else it uses the full SVD approach.\nIf you want to force Scikit-Learn to use full SVD, you can set the\nsvd_solver hyperparameter to \"full\".","d6266f51":"Scikit-Learn computes this score automatically for each feature after training, then it\nscales the results so that the sum of all importances is equal to 1. You can access the\nresult using the feature_importances_ variable. It seems that Age and Fare has higher importance","ef6f9229":"<font color=\"purple\">\nAnother useful piece of information is the explained variance ratio of\neach principal component, available via the explained_variance_ratio_\nvariable. The ratio indicates the proportion of the dataset\u2019s variance that\nlies along each principal component.\n\n\nThis output tells you that 93.4% of the dataset\u2019s variance lies along the\nfirst PC, and 06.5% lies along the second PC. This leaves less than 1%\nfor the third PC, so it is reasonable to assume that the third PC probably\ncarries little information.","60de0528":"<font color=\"red\">\n9.1. Main Approaches for Dimensionality\nReduction","e95ce43d":"<font color=\"red\">\n9.2.2.Kernel PCA:","2d121d14":" PolynomialFeatures(degree=2, *, interaction_only=False, include_bias=True, order='C')\n |  \n |  Generate polynomial and interaction features.\n |  \n |  Generate a new feature matrix consisting of all polynomial combinations\n |  of the features with degree less than or equal to the specified degree.\n |  For example, if an input sample is two dimensional and of the form\n |  [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n |  \n |  Parameters\n |  ----------\n |  degree : int, default=2\n |      The degree of the polynomial features.\n |  \n |  interaction_only : bool, default=False\n |      If true, only interaction features are produced: features that are\n |      products of at most ``degree`` *distinct* input features (so not\n |      ``x[1] ** 2``, ``x[0] * x[2] ** 3``, etc.).\n |  \n |  include_bias : bool, default=True\n |      If True (default), then include a bias column, the feature in which\n |      all polynomial powers are zero (i.e. a column of ones - acts as an\n |      intercept term in a linear model).\n |  \n |  order : {'C', 'F'}, default='C'\n |      Order of output array in the dense case. 'F' order is faster to\n |      compute, but may slow down subsequent estimators.","913c4043":"<font color=\"blue\">\nThe figure above shows the iris dataset with just one additional outlier: on the left, it is\nimpossible to find a hard margin.To avoid these issues, we use a more flexible model. The objective is to find a good balance\nbetween keeping the street as large as possible and limiting the margin violations (i.e.,\ninstances that end up in the middle of the street or even on the wrong side). This is called\nsoft margin classification.","368d2c46":"<font color=\"red\">\n6.2.3.SVM Regression:\n","b23c3811":"<font color=\"purple\">\nFirst it identifies the hyperplane that\nlies closest to the data, and then it projects the data onto it.\n    \nBefore you can project the training set onto a lower-dimensional\nhyperplane, you first need to choose the right hyperplane. For example, a\nsimple 2D dataset is represented on the left in Figure 8-7, along with three\ndifferent axes (i.e., 1D hyperplanes). On the right is the result of the\nprojection of the dataset onto each of these axes. As you can see, the\nprojection onto the solid line preserves the maximum variance, while the\nprojection onto the dotted line preserves very little variance and the\nprojection onto the dashed line preserves an intermediate amount of\nvariance.\n    \n    \n    \nIt seems reasonable to select the axis that preserves the maximum amount\nof variance, as it will most likely lose less information than the other\nprojections. Another way to justify this choice is that it is the axis that\nminimizes the mean squared distance between the original dataset and its\nprojection onto that axis. This is the rather simple idea behind PCA","73ca77e1":"<font color=\"red\">\n9.2.3.LLE:","fd66211e":"<font color=\"blue\">\nThe kernel trick, a mathematical technique that\nimplicitly maps instances into a very high-dimensional space (called the\nfeature space), enabling nonlinear classification and regression with\nSupport Vector Machines. Recall that a linear decision boundary in the\nhigh-dimensional feature space corresponds to a complex nonlinear\ndecision boundary in the original space.\nIt turns out that the same trick can be applied to PCA, making it possible\nto perform complex nonlinear projections for dimensionality reduction.\nThis is called Kernel PCA (kPCA). It is often good at preserving clusters\nof instances after projection, or sometimes even unrolling datasets that lie\nclose to a twisted manifold.\n    \n    \n    \nThe following code uses Scikit-Learn\u2019s KernelPCA class to perform kPCA\nwith an RBF kernel :","40a1081d":"## 6. Support Vector Machines:","d9226605":"<font color=\"blue\">\nThe other plots show models\ntrained with different values of hyperparameters gamma (\u03b3) and C. Increasing gamma makes\nthe bell-shaped curve narrower (see the righthand plots in Figure 5-8). As a result, each\ninstance\u2019s range of influence is smaller: the decision boundary ends up being more\nirregular, wiggling around individual instances. Conversely, a small gamma value makes the\nbell-shaped curve wider: instances have a larger range of influence, and the decision\nboundary ends up smoother. So \u03b3 acts like a regularization hyperparameter: if your model\nis overfitting, you should reduce it; if it is underfitting, you should increase it (similar to\nthe C hyperparameter).","2e35f504":"<font color=\"green\">\nHere is a more troublesome difference: if you pick two points randomly in\na unit square, the distance between these two points will be, on average,\nroughly 0.52. If you pick two random points in a unit 3D cube, the average\ndistance will be roughly 0.66. But what about two points picked randomly\nin a 1,000,000-dimensional hypercube? The average distance, believe it or\nnot, will be about 408.25 (roughly \u221a1, 000, 000\/6)! This is\ncounterintuitive: how can two points be so far apart when they both lie\nwithin the same unit hypercube? Well, there\u2019s just plenty of space in high\ndimensions. As a result, high-dimensional datasets are at risk of being\nvery sparse: most training instances are likely to be far away from each\nother. This also means that a new instance will likely be far away from any\ntraining instance, making predictions much less reliable than in lower\ndimensions, since they will be based on much larger extrapolations. In\nshort, the more dimensions the training set has, the greater the risk of\noverfitting it.","24c88439":"<font color=\"green\">\nIn this depiction of Gradient Descent, the model parameters are initialized randomly\nand get tweaked repeatedly to minimize the cost function; the learning step size is proportional to\nthe slope of the cost function, so the steps gradually get smaller as the parameters approach the\nminimum","bdc50708":"<font color=\"orange\">\nThere are two main\ndifferences with Lasso and Ridge. First, the gradients get smaller as the parameters\napproach the global optimum, so Gradient Descent naturally slows down,\nwhich helps convergence (as there is no bouncing around). Second, the\noptimal parameters (represented by the red square) get closer and closer to\nthe origin when you increase \u03b1, but they never get eliminated entirely.\n    \nTo avoid Gradient Descent from bouncing around the optimum at the end when using\nLasso, you need to gradually reduce the learning rate during training (it will still bounce\naround the optimum, but the steps will get smaller and smaller, so it will converge).","2f4a24cb":"<font color=\"green\">\nScikit-Learn offers a simple API for both bagging and pasting with the\nBaggingClassifier class (or BaggingRegressor for regression).\n\n\nThe following code\ntrains an ensemble of 500 Decision Tree classifiers: each is trained on 100 training\ninstances randomly sampled from the training set with replacement (this is an example\nof bagging, but if you want to use pasting instead, just set bootstrap=False). The\nn_jobs parameter tells Scikit-Learn the number of CPU cores to use for training and\npredictions (\u20131 tells Scikit-Learn to use all available cores):","5f2e409f":"<font color=\"green\">\n    \nWith bagging, some instances may be sampled several times for any given predictor,\nwhile others may not be sampled at all. By default a BaggingClassifier samples m\ntraining instances with replacement (bootstrap=True), where m is the size of the\ntraining set. This means that only about 63% of the training instances are sampled on\naverage for each predictor. The remaining 37% of the training instances that are not\nsampled are called out-of-bag (oob) instances. Note that they are not the same 37% for\nall predictors.\n    \nIn Scikit-Learn, you can set oob_score=True when creating a BaggingClassifier to\nrequest an automatic oob evaluation after training. The following code demonstrates\nthis. The resulting evaluation score is available through the oob_score_ variable:","fdeba34c":"<font color=\"blue\">\nOne way for a new predictor to correct its predecessor is to pay a bit more attention to\nthe training instances that the predecessor underfitted. This results in new predictors\nfocusing more and more on the hard cases. This is the technique used by AdaBoost.\n    \nFor example, when training an AdaBoost classifier, the algorithm first trains a base\nclassifier (such as a Decision Tree) and uses it to make predictions on the training set.\nThe algorithm then increases the relative weight of misclassified training instances.\nThen it trains a second classifier, using the updated weights, and again makes\npredictions on the training set, updates the instance weights, and so on.\n    \nOnce all predictors are trained, the ensemble makes predictions very much like bagging\nor pasting, except that predictors have different weights depending on their overall\naccuracy on the weighted training set.\n    \nFinally, a new predictor is trained using the updated weights, and the whole process is\nrepeated (the new predictor\u2019s weight is computed, the instance weights are updated,\nthen another predictor is trained, and so on). The algorithm stops when the desired\nnumber of predictors is reached, or when a perfect predictor is found.\nTo make predictions, AdaBoost simply computes the predictions of all the predictors\nand weighs them using the predictor weights \u03b1 . The predicted class is the one that\nreceives the majority of weighted votes.","1d1f0026":"<font color=\"blue\">\n    \nWhen creating an SVM model using Scikit-Learn, we can specify a number of\nhyperparameters. C is one of those hyperparameters. If we set it to a low value, then we end\nup with the model on the left of Figure below. With a high value, we get the model on the\nright. Margin violations are bad. It\u2019s usually better to have few of them. However, in this\ncase the model on the left has a lot of margin violations but will probably generalize better.\n    \nIf your SVM model is overfitting, you can try regularizing it by reducing C.","1dad4a66":"<font color=\"blue\">\nFinding Best Cluster Number that Fits Best for the Data:","375e910b":"Clearly, a straight line will never fit this data properly. So let\u2019s use Scikit-\nLearn\u2019s PolynomialFeatures class to transform our training data, adding\nthe quadratic (fourth-degree polynomial) of each feature in the training set as a\nnew feature.","3a2fe099":"<font color=\"green\">\nWhen using Gradient Descent, it requires that all features have a similar scale\n(e.g., using Scikit-Learn\u2019s StandardScaler class), or else it will take much longer to\nconverge.\n\nFortunately, the MSE cost function for a Linear Regression model happens to\nbe a convex function, which means that if you pick any two points on the\ncurve, the line segment joining them never crosses the curve. This implies\nthat there are no local minima, just one global minimum.","f535771e":"The best kernel and hyperparameters are then available through the\nbest_params_ variable:","f41593c8":"<font color=\"purple\">\nThe linear SVM classifier model predicts the class of a new instance x by simply\ncomputing the decision function w x + b = w x + \u22ef + w x + b. If the result is positive,\nthe predicted class \u0177 is the positive class (1), and otherwise it is the negative class (0);","fa4bc65c":"In this equation:\n\nK is the number of classes.\n\ns(x) is a vector containing the scores of each class for the instance x.\n\n\u03c3(s(x)) is the estimated probability that the instance x belongs to\n\nclass k, given the scores of each class for that instance.","c58bf51f":"To perform Linear Regression using Stochastic Gradient Descent with Scikit-Learn, you\ncan use the SGDRegressor class, which defaults to optimizing the squared\nerror cost function.","43e873d1":"We get \u03b8 = 4.51 and \u03b8 = 2.97 and we know that the original values are \u03b8 = 4 and \u03b8 =3. Close enough, but the noise made it impossible to recover the exact parameters of the original function.","5c4cf9c7":"<font color=\"red\">\n9.2.1. Randomized PCA:","f0b0dad9":"<font color=\"green\">\nFortunately, when using SVMs you can apply an almost miraculous mathematical\ntechnique called the kernel trick (explained in a moment). The kernel trick makes it\npossible to get the same result as if you had added many polynomial features, even with\nvery high-degree polynomials, without actually having to add them. So there is no\ncombinatorial explosion of the number of features because you don\u2019t actually add any\nfeatures. This trick is implemented by the SVC class.","acec0372":"<font color=\"green\">\nBaggingClassifier(base_estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)\n |  \n |  A Bagging classifier.\n |  \n |  A Bagging classifier is an ensemble meta-estimator that fits base\n |  classifiers each on random subsets of the original dataset and then\n |  aggregate their individual predictions (either by voting or by averaging)\n |  to form a final prediction.","4f2512e1":"<font color=\"green\">\nThe main problem with Batch Gradient Descent is the fact that it uses the\nwhole training set to compute the gradients at every step, which makes it\nvery slow when the training set is large. At the opposite extreme, Stochastic\nGradient Descent picks a random instance in the training set at every step\nand computes the gradients based only on that single instance. Obviously,\nworking on a single instance at a time makes the algorithm much faster\nbecause it has very little data to manipulate at every iteration. It also makes\nit possible to train on huge training sets, since only one instance needs to be\nin memory at each iteration\n  \ndue to its stochastic (i.e., random) nature, this algorithm\nis much less regular than Batch Gradient Descent: instead of gently\ndecreasing until it reaches the minimum, the cost function will bounce up\nand down, decreasing only on average. Over time it will end up very close to\nthe minimum, but once it gets there it will continue to bounce around, never\nsettling down. So once the algorithm stops, the final\nparameter values are good, but not optimal.","ecf76b70":"<font color=\"purple\">\nInstead of computing the gradients\nbased on the full training set (as in Batch GD) or based on just one instance\n(as in Stochastic GD), Mini-batch GD computes the gradients on small\nrandom sets of instances called mini-batches. The main advantage of Minibatch\nGD over Stochastic GD is that you can get a performance boost from\nhardware optimization of matrix operations.\n    \nAs a result, Minibatch\nGD will end up walking around a bit closer to the minimum than\nStochastic GD\u2014but it may be harder for it to escape from local minima (in\nthe case of problems that suffer from local minima, unlike Linear\nRegression).","3ec5c6ce":"<font color=\"purple\">\nScikit-Learn\u2019s PCA class uses SVD decomposition to implement PCA.The following code applies PCA to\nreduce the dimensionality of the dataset down to two dimensions (note\nthat it automatically takes care of centering the data):","3620c5a9":"## 8. Ensemble Learning and Random Forests:","3ea39f75":"<font color=\"red\">\n9.2.4.Other Dimensionality Reduction Techniques:","32e5f8f4":"<font color=\"purple\">\nWe can use a linear model to fit nonlinear data. A simple way to do this is to add\npowers of each feature as new features, then train a linear model on this\nextended set of features. This technique is called Polynomial Regression.","81c4d8fc":"<font color=\"red\">\n8.4.2.Gradient Boosting:","a32ac5e0":"<font color=\"red\">\nChoosing the Right Number of Dimensions:","db61fe9b":"<font color=\"purple\">\nImage segmentation is the task of partitioning an image into multiple\nsegments. In semantic segmentation, all pixels that are part of the same\nobject type get assigned to the same segment. For example, in a selfdriving\ncar\u2019s vision system, all pixels that are part of a pedestrian\u2019s image\nmight be assigned to the \u201cpedestrian\u201d segment (there would be one\nsegment containing all the pedestrians).In some applications, this may be\nsufficient. For example, if you want to analyze satellite images to measure\nhow much total forest area there is in a region, color segmentation may be\njust fine.","88705ad7":"<font color=\"red\">\n6.2.3.Decision Function and Predictions:","ed0b00df":"1. Adding Polynomial Features:","794546fd":"## 10. Clustering Algorithms:","3940dc25":"<font color=\"red\">\n3.3.Elastic Net:","681b5d4f":"<font color=\"red\">\n1.1.2. Stochastic Gradient Descent:","09721d8b":"<font color=\"blue\">\nWith so many kernels to choose from, how can you decide which one to use? As a rule of thumb, you\nshould always try the linear kernel first (remember that LinearSVC is much faster than\nSVC(kernel=\"linear\")), especially if the training set is very large or if it has plenty of features. If the\ntraining set is not too large, you should also try the Gaussian RBF kernel; it works well in most cases.\nThen if you have spare time and computing power, you can experiment with a few other kernels, using\ncross-validation and grid search. You\u2019d want to experiment like that especially if there are kernels\nspecialized for your training set\u2019s data structure.","a5b17a4f":"<font color=\"purple\">\nTo use SVMs\nfor regression instead of classification, the trick is to reverse the objective: instead of\ntrying to fit the largest possible street between two classes while limiting margin\nviolations, SVM Regression tries to fit as many instances as possible on the street while\nlimiting margin violations (i.e., instances off the street). The width of the street is\ncontrolled by a hyperparameter, \u03f5. Figure below shows two linear SVM Regression models\ntrained on some random linear data, one with a large margin (\u03f5 = 1.5) and the other with a\nsmall margin (\u03f5 = 0.5).","a08078bf":"Lets compare the errors of both model","ad98b359":"<font color=\"purple\">\nWhen the cost function is very irregular, this can actually\nhelp the algorithm jump out of local minima, so Stochastic Gradient Descent\nhas a better chance of finding the global minimum than Batch Gradient\nDescent does.","6f216cf9":"<font color=\"green\">\nRegularization Hyperparameters:\n    \nTo avoid overfitting the training data, you need to restrict the Decision Tree\u2019s freedom\nduring training. As you know by now, this is called regularization. The regularization\nhyperparameters depend on the algorithm used, but generally you can at least restrict the\nmaximum depth of the Decision Tree. In Scikit-Learn, this is controlled by the max_depth\nhyperparameter (the default value is None, which means unlimited). Reducing max_depth\nwill regularize the model and thus reduce the risk of overfitting.\n    \nThe DecisionTreeClassifier class has a few other parameters that similarly restrict the\nshape of the Decision Tree: min_samples_split (the minimum number of samples a node\nmust have before it can be split), min_samples_leaf (the minimum number of samples a\nleaf node must have), min_weight_fraction_leaf (same as min_samples_leaf but\nexpressed as a fraction of the total number of weighted instances), max_leaf_nodes (the\nmaximum number of leaf nodes), and max_features (the maximum number of features that\nare evaluated for splitting at each node). Increasing min_* hyperparameters or reducing\nmax_* hyperparameters will regularize the model.","941a7177":"<font color=\"orange\">\nLocally Linear Embedding (LLE) is another powerful nonlinear\ndimensionality reduction (NLDR) technique. It is a Manifold Learning\ntechnique that does not rely on projections, like the previous algorithms\ndo. In a nutshell, LLE works by first measuring how each training instance\nlinearly relates to its closest neighbors (c.n.), and then looking for a lowdimensional\nrepresentation of the training set where these local\nrelationships are best preserved (more details shortly). This approach\nmakes it particularly good at unrolling twisted manifolds, especially when\nthere is not too much noise.\nThe following code uses Scikit-Learn\u2019s LocallyLinearEmbedding class to\nunroll the Swiss roll:","887b00c1":"2. Adding Similarity Features via Gaussian RBF Kernel:","41cb7b10":"<font color=\"red\">\n3.1.Ridge Regression","642c3aff":"<font color=\"blue\">\nThe Logistic Regression model can be generalized to support multiple\nclasses directly, without having to train and combine multiple binary\nclassifiers.This is called Softmax Regression, or\nMultinomial Logistic Regression.\n    \nThe idea is simple: when given an instance x, the Softmax Regression model\nfirst computes a score s(x) for each class k, then estimates the probability of\neach class by applying the softmax function","9248f540":"<font color=\"blue\">\nNote that each class has its own dedicated parameter vector \u03b8 . All these\nvectors are typically stored as rows in a parameter matrix \u0398.\n\nOnce you have computed the score of every class for the instance x, you can\nestimate the probability \u02c6p that the instance belongs to class k by running the\nscores through the softmax function (Equation 4-20). The function computes\nthe exponential of every score, then normalizes them (dividing by the sum of\nall the exponentials).\n    \nJust like the Logistic Regression classifier, the Softmax Regression\nclassifier predicts the class with the highest estimated probability (which is\nsimply the class with the highest score.","55db3f75":"2.Silhouette Score\n\nThe Silhouette Coefficient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. The Silhouette Coefficient for a sample is (b - a) \/ max(a, b). To clarify, b is the distance between a sample and the nearest cluster that the sample is not a part of. We can compute the mean Silhouette Coefficient over all samples and use this as a metric to judge the number of clusters.","900d2f1b":"<font color=\"red\">\n8.1. Voting Classifiers","0b774809":"<font color=\"green\">\nSimply projecting onto a plane (e.g., by dropping x ) would squash\ndifferent layers of the Swiss roll together on the left side of the figure below,however, What you really want is to unroll the Swiss roll to obtain the 2D dataset on the right side of as shown on the figure below:","f059d1d7":"<font color=\"blue\">\n\u0177 is the predicted value.\n    \nn is the number of features.\n    \nx is the i feature value.\n\nThe bias term \u03b8 and the\nfeature weights \u03b8 , \u03b8 , \u22ef, \u03b8 .","215cb741":"<font color=\"red\">\n10.2.Using Clustering for Image Segmentation:","db09b80b":"<font color=\"red\">\nInversing Back to the Original Dimensions:","465e82d3":"<font color=\"red\">\n8.5. Stacking :\n","947579ba":"## 7. Decision Trees:","b0b856a9":"<font color=\"blue\">\nLinear model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the bias term (also called the intercept term)\n       ","3b415c95":"<font color=\"green\">\nYou start at the root node (depth 0, at the top): this node\nasks whether the flower\u2019s petal length is smaller than 2.45 cm. If it is, then you move down\nto the root\u2019s left child node (depth 1, left). In this case, it is a leaf node (i.e., it does not have\nany child nodes), so it does not ask any questions: simply look at the predicted class for that\nnode, and the Decision Tree predicts that your flower is an Iris setosa (class=setosa).\n\nNow suppose you find another flower, and this time the petal length is greater than 2.45 cm.\nYou must move down to the root\u2019s right child node (depth 1, right), which is not a leaf node,\nso the node asks another question: is the petal width smaller than 1.75 cm? If it is, then your\nflower is most likely an Iris versicolor (depth 2, left). If not, it is likely an Iris virginica\n(depth 2, right). It\u2019s really that simple.\n    \nFinally, a node\u2019s\ngini attribute measures its impurity: a node is \u201cpure\u201d (gini=0) if all training instances it\napplies to belong to the same class.\n    \nThe thick vertical line represents\nthe decision boundary of the root node (depth 0): petal length = 2.45 cm. Since the lefthand\narea is pure (only Iris setosa), it cannot be split any further. However, the righthand area is\nimpure, so the depth-1 right node splits it at petal width = 1.75 cm (represented by the\ndashed line). Since max_depth was set to 2, the Decision Tree stops right there. If you set\nmax_depth to 3, then the two depth-2 nodes would each add another decision boundary\n(represented by the dotted lines).","91986863":"You could then set n_components=d and run PCA again. But there is a\nmuch better option: instead of specifying the number of principal\ncomponents you want to preserve, you can set n_components to be a float\nbetween 0.0 and 1.0, indicating the ratio of variance you wish to preserve:","941ccfef":"<font color=\"purple\">\nThe algorithm works mostly the same way as earlier, except that instead of trying to\nsplit the training set in a way that minimizes impurity, it now tries to split the training set in\na way that minimizes the MSE.","afc764a1":"Lasso(alpha=1.0, *, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n |  \n |  Linear Model trained with L1 prior as regularizer ","c5e22bb0":"<font color=\"green\">\nIf you set the svd_solver hyperparameter to \"randomized\", Scikit-Learn\nuses a stochastic algorithm called Randomized PCA that quickly finds an\napproximation of the first d principal components.","8f03dbd8":"## 5. Softmax Regression:","bf3d1eda":"If your AdaBoost ensemble is overfitting the training set, you can try reducing the number of\nestimators or more strongly regularizing the base estimator.","dc408833":"## 1. Linear Regression:","4e314302":"<font color=\"green\">\nIf all classifiers are able to estimate class probabilities (i.e., they all have a\npredict_proba() method), then you can tell Scikit-Learn to predict the class with the\nhighest class probability, averaged over all the individual classifiers. This is called soft\nvoting. It often achieves higher performance than hard voting because it gives more\nweight to highly confident votes. All you need to do is replace voting=\"hard\" with\nvoting=\"soft\" and ensure that all classifiers can estimate class probabilities. This is\nnot the case for the SVC class by default, so you need to set its probability\nhyperparameter to True (this will make the SVC class use cross-validation to estimate\nclass probabilities, slowing down training, and it will add a predict_proba() method).","1c1612c9":"<font color=\"blue\">\nAs kPCA is an unsupervised learning algorithm, there is no obvious\nperformance measure to help you select the best kernel and\nhyperparameter values. That said, dimensionality reduction is often a\npreparation step for a supervised learning task (e.g., classification), so you\ncan use grid search to select the kernel and hyperparameters that lead to\nthe best performance on that task. The following code creates a two-step\npipeline, first reducing dimensionality to two dimensions using kPCA,\nthen applying Logistic Regression for classification. Then it uses\nGridSearchCV to find the best kernel and gamma value for kPCA in order\nto get the best classification accuracy at the end of the pipeline:","684451d6":"<font color=\"red\">\n8.2. Bagging and Pasting:","738fa4a7":"<font color=\"purple\">\nWhen there are multiple features, Polynomial Regression is capable\nof finding relationships between features (which is something a plain Linear\nRegression model cannot do). This is made possible by the fact that\nPolynomialFeatures also adds all combinations of features up to the given\ndegree. For example, if there were two features a and b,\nPolynomialFeatures with degree=3 would not only add the features a , a ,\nb , and b , but also the combinations ab, a b, and ab .","df29eb3b":"## 3. Regularized Linear Models:","d35de9bd":"<font color=\"green\">\nTo implement Gradient Descent, you need to compute the gradient of the\ncost function with regard to each model parameter \u03b8 . In other words, you\nneed to calculate how much the cost function will change if you change \u03b8\njust a little bit. This is called a partial derivative. It is like asking \u201cWhat is\nthe slope of the mountain under my feet if I face east?\u201d and then asking the\nsame question facing north (and so on for all other dimensions, if you can\nimagine a universe with more than three dimensions).","6764f3e3":"<font color=\"purple\">\nThe logistic\u2014noted \u03c3(\u00b7)\u2014is a sigmoid function (i.e., S-shaped) that outputs a\nnumber between 0 and 1.\n\nOnce the Logistic Regression model has estimated the probability \u02c6p = h (x)\nthat an instance x belongs to the positive class, it can make its prediction \u0177\neasily.\n    \nNotice that \u03c3(t) < 0.5 when t < 0, and \u03c3(t) \u2265 0.5 when t \u2265 0, so a Logistic\nRegression model predicts 1 if x \u03b8 is positive and 0 if it is negative.","09ccfef8":"<font color=\"green\">\nBootstrapping introduces a bit more diversity in the subsets that each predictor is\ntrained on, so bagging ends up with a slightly higher bias than pasting; but the extra\ndiversity also means that the predictors end up being less correlated, so the ensemble\u2019s\nvariance is reduced. Overall, bagging often results in better models, which explains why\nit is generally preferred.","739f3b7f":"<font color=\"green\">\nThe hyperparameter \u03b1 controls how much you want to regularize the model.\nIf \u03b1 = 0, then Ridge Regression is just Linear Regression. If \u03b1 is very large,\nthen all weights end up very close to zero and the result is a flat line going\nthrough the data\u2019s mean.\n\nBelow is the formulation how cost function is calculated in Ridge Regression","5de47ce4":"<font color=\"green\">\nAn important parameter in Gradient Descent is the size of the steps,\ndetermined by the learning rate hyperparameter. If the learning rate is too small, then the algorithm will have to go through many iterations to\nconverge, which will take a long time.\n\nOn the other hand, if the learning rate is too high, you might jump across the\nvalley and end up on the other side, possibly even higher up than you were\nbefore. This might make the algorithm diverge, with larger and larger\nvalues, failing to find a good solution","9461b0ea":"<font color=\"purple\">\nThe dashed lines represent the points where the decision function is equal to 1 or \u20131: they\nare parallel and at equal distance to the decision boundary, and they form a margin around\nit. Training a linear SVM classifier means finding the values of w and b that make this\nmargin as wide as possible while avoiding margin violations (hard margin) or limiting\nthem (soft margin).","ee142a85":"<font color=\"red\">\n6.2. Implementing Support Vector Machines:","846aa2ab":"Once you have the gradient vector, which points uphill, just go in the\nopposite direction to go downhill. This means subtracting \u2207 MSE(\u03b8) from \u03b8.\nThis is where the learning rate \u03b7 comes into play: multiply the gradient\nvector by \u03b7 to determine the size of the downhill step","30623d76":"<font color=\"green\">\nA good way to reduce overfitting is to\nregularize the model (i.e., to constrain it): the fewer degrees of freedom it\nhas, the harder it will be for it to overfit the data.\n\nRidge Regression, Lasso Regression, and Elastic Net, which implement three different ways to\nconstrain the weights.","7c7985d2":"<font color=\"green\">\nIn most real-world problems, training instances are not spread out\nuniformly across all dimensions. Many features are almost constant, while\nothers are highly correlated (as discussed earlier for MNIST). As a result,\nall training instances lie within (or close to) a much lower-dimensional\nsubspace of the high-dimensional space.\n\nNotice that all training instances lie close to a plane: this is a lowerdimensional\n(2D) subspace of the high-dimensional (3D) space. If we\nproject every training instance perpendicularly onto this subspace (as\nrepresented by the short lines connecting the instances to the plane), we get the new 2D dataset shown in the figure below. Ta-da! We have just reduced\nthe dataset\u2019s dimensionality from 3D to 2D. Note that the axes correspond\nto new features z and z (the coordinates of the projections on the plane).","613d2c31":"With Stochastic Gradient Descent, each training step is much faster but also much more\nstochastic than when using Batch Gradient Descent","eca8c61d":"X_poly now contains the original feature of X plus the quadratic of this feature\nNow you can fit a LinearRegression model to this extended training data","a3149438":"<font color=\"red\">\n6.2.1. Linear Implementing Support Vector Machines:"}}