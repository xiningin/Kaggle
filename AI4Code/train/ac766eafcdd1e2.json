{"cell_type":{"a78f3cec":"code","c8ce6bc7":"code","60c186b5":"code","161cde59":"code","a66ecabb":"code","1a5f3ce9":"code","0f0c26cc":"code","ae67e04d":"code","cff199fb":"code","03fa79d8":"code","23e9da63":"code","509b4de5":"code","921dde2d":"code","ef4e74e3":"code","32878db7":"code","f7f10099":"code","3f07d727":"code","72fffaa4":"code","aff89401":"code","be49c56a":"code","8244e177":"code","7aeb2e33":"code","5b673556":"code","7d946f47":"code","5f1eb441":"code","10d3387e":"code","7c4c501a":"code","23f4c46c":"code","336ef677":"code","1c08cc43":"code","a45b9406":"code","7c3085ee":"code","62371186":"code","0ec51eae":"code","2a2119ff":"code","36fbe769":"code","edbd7923":"code","d02fd76a":"code","8976ba99":"code","4aedbe9b":"code","77eb9e00":"code","32972253":"code","3ad07a00":"code","e56f7f28":"code","ceb67be1":"code","4d4f4f05":"code","d61bf735":"code","56953be2":"code","dd2d9fe9":"code","c8915da3":"code","a2694bf7":"code","ee62221d":"code","03401851":"code","94855521":"code","7401fab7":"code","c61b851c":"code","ee0f62a4":"code","917db147":"code","f2d33162":"code","00309cfb":"code","80622899":"markdown","3f10a04e":"markdown","796ac098":"markdown","7df9e09c":"markdown","3b31b644":"markdown","8ac22f8a":"markdown","578e740b":"markdown","a4ecd24b":"markdown","f1d5853e":"markdown","7574d05b":"markdown","bf0154ee":"markdown","498b876b":"markdown","f9f88042":"markdown","bb6cd6b6":"markdown","73759ad9":"markdown","d0f540f6":"markdown","98c7e8ed":"markdown","4ea3f079":"markdown","981752f9":"markdown","ff90ef7a":"markdown","e7efd918":"markdown","d02dd51e":"markdown","4bf86a0d":"markdown","c114895b":"markdown","03b04df3":"markdown","102862f2":"markdown","e816ac70":"markdown","fe7fecd2":"markdown","f830c415":"markdown","ca1fbd80":"markdown","c447c8b2":"markdown","bdb8b619":"markdown","635113a9":"markdown","6c4dbf60":"markdown","5525d5ec":"markdown","21b44f72":"markdown","777239d7":"markdown","95851502":"markdown","d9f470b0":"markdown","03c9bd24":"markdown","85cdbe94":"markdown","a2a0758a":"markdown","56bc4e1c":"markdown"},"source":{"a78f3cec":"# upgrade\n!python --version\n!python -m pip install --upgrade pip\n!pip install seaborn --upgrade\n!pip install rfpimp","c8ce6bc7":"# data processing\nimport numpy as np\nimport pandas as pd\n# ML libs\n# classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.ensemble import (\n    RandomForestClassifier, GradientBoostingClassifier\n)\nimport xgboost\nfrom xgboost import XGBClassifier\n# evaluation\nimport rfpimp # https:\/\/github.com\/parrt\/random-forest-importances\nfrom sklearn.model_selection import cross_val_score, learning_curve, RandomizedSearchCV\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import confusion_matrix, classification_report\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nprint('seaborn version:', sns.__version__)\nprint('xgboost version:', xgboost.__version__)","60c186b5":"# helper functions\n!python -m pip install wikitablescrape\n\n# impute age from external sources\ndef impute_missing_ages_from_external(df):\n    \"\"\"\n    External Sources Imputation\n        There are several external sources where this information is available and it is worth\n    exploiting it to improve model performance. To impute the missing ages we will leverage\n    the Titanic Passengers list https:\/\/en.wikipedia.org\/wiki\/Passengers_of_the_Titanic that\n    contains the age of ~1300 passengers. The reasons why we chose this source ar two-fold:\n      - it is open-source and reliable because it checked by a large community, and\n      - it is free unlike https:\/\/www.encyclopedia-titanica.org.\n    \"\"\"\n    import string\n    from difflib import SequenceMatcher\n    def norm_string(s):\n        \"\"\"Takes a string and normalize it (i.e. lowercase, stripped puctuation).\"\"\"\n        return s.translate(str.maketrans('', '', string.punctuation)).lower()\n    # we isolate passenger with unknown\n    na_age_mask = df['Age'].isna()\n    print('passengers without age:', na_age_mask.sum())\n    if na_age_mask.sum() == 0:\n        print('age feature have been fixed. Nothing to do!')\n        return\n    # wikitablescrape scrapes all tables from a given urlall_df\n    !wikitablescrape --url=\"https:\/\/en.wikipedia.org\/wiki\/Passengers_of_the_Titanic#First_class_2\" --output-folder='\/kaggle\/temp\/'\n    # prepare Wikipedia tables (divided by Pclass)\n    first_class = pd.read_csv('\/kaggle\/temp\/table_1_passenger_list_first_class.csv')\n    first_class['Pclass'] = 1\n    second_class = pd.read_csv('\/kaggle\/temp\/table_2_passenger_list_second_class.csv')\n    second_class['Pclass'] = 2\n    third_class = pd.read_csv('\/kaggle\/temp\/table_3_passenger_list_third_class.csv')\n    third_class['Pclass'] = 3\n    passengers_wiki = pd.concat([first_class, second_class, third_class], axis=0)\n    passengers_wiki['Embarked'] = passengers_wiki['Boarded'].str[0]\n    # remove unnecessaty features\n    passengers_wiki.drop(columns=['Hometown', 'Destination', 'Home country', 'Boarded'])\n    print('passengers_wiki shape:', passengers_wiki.shape, '\\npassengers_wiki features:', passengers_wiki.columns.tolist())\n    print('ensure numeric type for Age feature (non numeric are converted into NaN)')\n    passengers_wiki['Age'] = pd.to_numeric(passengers_wiki['Age'], errors='coerce')\n    # prepare the dataframes to be compared sorted by the feature to be matched (i.e. Name)\n    matching_keys = ['Name', 'Pclass', 'Embarked', 'Age']\n    ages_to_update = df.loc[na_age_mask, matching_keys].sort_values('Name')\n    ref_list = passengers_wiki.loc[:, matching_keys].sort_values('Name')\n    # create name similarity report\n    # we will match passengers without age or with estimated age\n    # and find the best match from the wikipedia table\n    report = dict()\n    print('creating name similarity report using Wiki table https:\/\/en.wikipedia.org\/wiki\/Passengers_of_the_Titanic')\n    for pid, row in ages_to_update.iterrows():\n        filtered = ref_list.loc[(ref_list['Pclass'] == row['Pclass']) & (ref_list['Embarked'] == row['Embarked'])]\n        sim_score = filtered['Name'].apply(lambda x: SequenceMatcher(None, norm_string(x), norm_string(row['Name'])).ratio())\n        match = filtered.loc[sim_score.idxmax()]\n        base = row.to_dict()\n        base['SimScore'] = sim_score.max()\n        base['MatchedName'] = match['Name']\n        base['Age'] = match['Age']\n        report[pid] = base\n    # we analysis the report to set a threshold below which to exclude bad matching\n    # this is a manual investigation of the matched name against the name in the training data\n    # we assume that any data point with a name similarity score above 80% is a correct match\n    df_report = pd.DataFrame(report).T.sort_values('SimScore', ascending=False)\n    df_report.index.name = 'PassengerId'\n    # check if there are missing values in the scraped table\n    passengers_names_with_unknown_age = df_report.loc[df_report['Age'].isna(), 'Name']\n    print('the age of these passengers is not present in the scraped data:', passengers_names_with_unknown_age.to_dict())\n    print('Kassem, Mr. Fared age (19) was found on https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/fared-kassem-houssein.html')\n    df_report.loc[525, 'Age'] = 19\n    group_features = ['Survived', 'Pclass', 'Sex', 'SibSp', 'Parch']\n    # use the full dataframe to compute group median\n    age_estim = df.set_index(group_features).sort_index(level=group_features).loc[(0.0, 3, 'male', 0, 0), 'Age'].median()\n    print('the ages of passengers Kraeff, Mr. Theodor (https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/theodor-kraeff.html) and Gheorgheff, Mr. Stanio (https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/stanio-gheorgheff.html) remain unknown')\n    print('we will estimate it as median age of third class, male, passengers, travelling alone, who did not survive:', age_estim)\n    df_report.loc[df_report['Age'].isna(), 'Age'] = age_estim\n    # after analysing the report, we are confident to say that a similarity score > 75% returns consistently correct matches\n    print('% of data points with similarity score > 75%:', (df_report['SimScore'] > .75).sum() \/ len(df_report) * 100)\n    # any record with a similarity score below this threshold should be manually verified\n    # matched below the threshold are manually checked and the information recorded in a dictionary (format={PassengerId: Age})\n    # we use the https:\/\/www.encyclopedia-titanica.org\/ website to find\/confirm the age of the matches to be\n    # verified (some passengers were corrently matched but their ages slightly differ with those of the Wikipedia)\n    manually_verified = {\n        49: 3,    # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/youssef-joseph-samaan.html\n        1062: 27, # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/simon-kutscher-lithman.html\n        710: 4,   # https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/halim-gonios-william-george-moubarek.html\n        977: 25,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/betros-khalil.html\n        925: 36,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/eliza-johnston.html\n        177: 5,   # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/henry-lefebvre.html\n        523: 35,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/sarkis-lahoud-ishaq-mowad.html\n        565: 62,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/marian-meanwell.html\n        1080: 10, # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/elizabeth-ada-sage.html\n        230: 12,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/mathilde-lefebvre.html\n        850: 46,  # https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/nella-goldenberg.html\n        1001: 19, # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/george-swane.html\n        1257: 44, # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/annie-elizabeth-sage.html\n        1146: 28, # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/wenzel-linhart.html\n        634: 29,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/william-henry-marsh-parr.html\n        1141: 20, # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/zahie-maria-khalil.html\n        508: 37,  # https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/george-brereton.html\n        486: 8,   # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/jeannie-lefebvre.html\n        1025: 30, # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/charles-rad-thomas.html\n        1024: 40, # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/frances-marie-lefebvre.html\n        927: 19,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/vasilios-katavelos.html\n        1231: 10, # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/betros-seman.html\n        445: 29,  # https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/bernt-johannes-johannesen.html\n        967: 33,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/edwin-herbert-keeping.html\n        410: 3,   # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/ida-lefebvre.html\n        975: 23,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/dmitri-marinko.html\n        1166: 20, # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/saade-nassr-rizq.html\n        844: 30,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/panagiotis-lymperopoulus.html\n        271: 28,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/alexander-cairns.html\n        860: 30,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/raihed-razi.html\n        599: 18,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/boulos-hanna.html\n        129: 2,   # https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/anna-mary-joseph-peter-joseph.html\n        141: 40,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/sultana-boulos.html\n        169: 48,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/john-d-baumann.html\n        1178: 46, # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/charles-franklin.html\n        528: 48,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/john-farthing.html\n        919: 19,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/daher-shedid.html\n        603: 37,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/charles-harrington.html\n        816: 39,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/richard-fry.html\n        307: 42,  # https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/margaret-fleming.html\n        534: 23,  # https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/catherine-peter-joseph.html\n        1000: 37, # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/aaron-willer.html\n        278: 21,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/frank-parkes.html\n        1083: 43, # https:\/\/www.encyclopedia-titanica.org\/titanic-suall_dfrvivor\/abraham-salomon.html\n        733: 42,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/robert-knight.html\n        123: 29,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/nicholas-nasser.html\n        414: 21,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/alfred-fleming-cunningham.html\n        355: 23,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/yousif-ahmed-wazli.html\n        532: 17,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/toufik-nakhli.html\n        675: 18,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/ennis-hastings-watson.html\n        644: 32,  # https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/choong-foo.html\n        482: 38,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/anthony-wood-frost.html\n        1309: 4,  # https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/michael-peter_shafiq_joseph.html\n        182: 39,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/rene-pernot.html\n        1117: 25, # https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/omine-moubarek.html\n        558: 45,  # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/victor-robbins.html\n        1158: 43, # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/roderick-chisholm.html\n        914: 48,  # https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/antoinette-flegenheim.html\n        27: 26,   # https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/emir-farres-chehab-shihab.html\n    }\n    print('# data points manually verified:', len(manually_verified))\n    # update report with manually verified data points\n    print('update report with manually verified data points')\n    # update missing age (passengers_manually_verified contains updates for train and test sets)\n    # update only data poSurvivedints in report\n    report_index = df_report.index.tolist()\n    for pid, age_update in manually_verified.items():\n        if pid not in report_index:\n            #print(pid, 'not in report, skipping..')\n            continue\n        df_report.loc[pid, 'Age'] = age_update\n    # intersect only matching data points that we need to update\n    imputed = df_report.loc[ages_to_update.index, 'Age'].astype(float)\n    imputed.index.name = 'PassengerId'\n    imputed.sort_index(inplace=True)\n    # ensure data points consistency\n    assert len(ages_to_update) == len(imputed), f'ages_to_update={len(ages_to_update)} != inputed_ages={len(imputed)}'\n    print('Age imputed?', imputed.isna().sum() == 0)\n    fig = go.Figure()\n    fig.add_trace(go.Histogram(x=df['Age'], name='original'))\n    fig.add_trace(go.Histogram(x=imputed, name='missing'))\n    fig.update_layout(barmode='stack', title='Age distribution with inputed missing values').show()\n    return imputed","161cde59":"# load the training and test data\ntrain_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', index_col='PassengerId')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv', index_col='PassengerId')\ntrain_df.head()","a66ecabb":"# training\/test data\ntrain_df.info()\nprint('-'*50)\ntest_df.info()","1a5f3ce9":"# How representative is the training set?\ntotal_onboard = 885 + 1317\nsurvived = 705\nestimated_survival_rate = survived \/ total_onboard\nsamples, _ = train_df.shape\nprint('training size (% total passengers):', samples \/ total_onboard)\ntraining_survival_rate = train_df['Survived'].astype(bool).sum() \/ samples\nprint('\\n1. survival rate variation:', abs(estimated_survival_rate - training_survival_rate))\nprint('\\n2. Passengers by Socio Econimic Status (SES)')\nprint(train_df['Pclass'].value_counts(normalize=True).round(2))\nprint('\\n3. Passengers by Sex')\nprint(train_df['Sex'].value_counts(normalize=True).round(2))\nprint('\\n4. Passengers by Port of Embarkation')\nprint(train_df['Embarked'].value_counts(normalize=True).round(2))\nprint('\\n5. Women & Children Survival Rate')\nprint(train_df.loc[(train_df['Age'] < 18) | (train_df['Sex'] == 'female'), 'Survived'].mean())\nprint('\\n6. SES Survival Rate')\nprint(train_df.groupby('Pclass')['Survived'].mean())","0f0c26cc":"# divide the features by data type\nnumerical = ['Age', 'Fare', 'Parch', 'SibSp']\ncategorical = ['Survived', 'Pclass', 'Sex', 'Embarked', 'Ticket', 'Cabin', 'Name']","ae67e04d":"# 5-number summary of the quantitative features\ntrain_df.describe()","cff199fb":"# shapes of quantitative features\ntrain_df.hist(column=numerical, figsize=(18,4), layout=(1,4));","03fa79d8":"# usually pandas infer 'Object' for string data (this means that Survived and Pclass are treated as numerical)\ntrain_df.describe(include=['O'])","23e9da63":"fig, axes = plt.subplots(1, 2, figsize=(10,4))\nsns.countplot(data=train_df, x='Sex', ax=axes[0])\nsns.countplot(data=train_df, x='Embarked', ax=axes[1]);","509b4de5":"# how is the avg survival rate of the numerical features?\ntrain_df.pivot_table(index='Survived', values=numerical)","921dde2d":"# what is the survival rate by number of siblings\/spouse?\ng = sns.catplot(data=train_df, x='SibSp', y='Survived', kind='bar', height=4, aspect=1.5)","ef4e74e3":"# what is the survival rate by number of parents\/children?\ng = sns.catplot(data=train_df, x='Parch', y='Survived', kind='bar', height=4, aspect=1.5)","32878db7":"# what is the survival rate by SES?\ng = sns.catplot(data=train_df, x='Pclass', y='Survived', kind='bar', height=4, aspect=1.5)","f7f10099":"# what is the survival rate by gender?\ng = sns.catplot(data=train_df, x='Sex', y='Survived', kind='bar', height=4, aspect=1.5)","3f07d727":"# what is the survival rate by port of embarkation?\ng = sns.catplot(data=train_df, x='Embarked', y='Survived', kind='bar', height=4, aspect=1.5)","72fffaa4":"# why passengers embarked at Cherbourg have higher survival rate?\n# hypotesis 1: # Average passenger\\'s Fare per port of embarcation by Pclass\ng = sns.heatmap(train_df.pivot_table(index='Embarked', columns='Pclass', values='Fare', aggfunc='mean'), annot=True, cmap='RdBu_r')\ng = g.set(title='Average passenger\\'s fare per port of embarcation by Pclass')","aff89401":"# why passengers embarked at Cherbourg have higher survival rate?\n# hypotesis 2: Average passenger\\'s Age per port of embarcation by Pclass\ng = sns.heatmap(train_df.pivot_table(index='Embarked', columns='Pclass', values='Age', aggfunc='mean'), annot=True, cmap='RdBu_r')\ng = g.set(title='Average passenger\\'s age per port of embarcation by Pclass')","be49c56a":"# how do they correlate with each others?\ng = sns.heatmap(train_df.corr(), annot=True, fmt='.2f')","8244e177":"# what is the age grouped by target?\ng = sns.displot(data=train_df, x='Age', hue='Sex', col='Survived', kind='kde', height=4)","7aeb2e33":"print('Training data', len(train_df))\nprint(train_df.isna().sum())\nprint('\\nTest data', len(test_df))\nprint(test_df.isna().sum())","5b673556":"# how age values are distributes with regards to other features?\nfig, ax = plt.subplots(1, 2, figsize=(16,4))\nax[0].set_title('Age distribution by Pclass and Sex on training set')\nax[1].set_title('Age distribution by Pclass and Sex on test set')\ng_train = sns.barplot(data=train_df[~train_df['Age'].isna()], x='Pclass', y='Age', hue='Sex', ax=ax[0])\ng_test = sns.barplot(data=test_df[~test_df['Age'].isna()], x='Pclass', y='Age', hue='Sex', ax=ax[1])","7d946f47":"# how cabin values are distributes with regards to other features?\nfig, [ax_train, ax_test] = plt.subplots(1, 2, figsize=(15, 3))\nax_train.set_title('Number of cabins in training data')\nax_test.set_title('Number of cabins in test data')\ntrain_df[~train_df['Cabin'].isna()].pivot_table(index='Sex', columns='Pclass', values='Name', aggfunc='count').plot.barh(stacked=True, ax=ax_train)\ntest_df[~test_df['Cabin'].isna()].pivot_table(index=['Sex'], columns='Pclass', values='Name', aggfunc='count').plot.barh(stacked=True, ax=ax_test);","5f1eb441":"g = sns.boxplot(data=train_df, x='Fare').set(title='Outliers in Fare')","10d3387e":"# free lunch\ntrain_df[train_df['Fare'] == 0]","7c4c501a":"# suites\ntrain_df[train_df['Fare'] > 500]","23f4c46c":"g = sns.boxplot(data=train_df, x='Age').set(title='Outliers in Age')","336ef677":"train_df[train_df['Age'] > 65]","1c08cc43":"g = sns.boxplot(data=train_df, x='SibSp').set(title='Outliers in #Siblings\/Spouses')","a45b9406":"train_df[train_df['SibSp'] == 8]","7c3085ee":"g = sns.boxplot(data=train_df, x='Parch').set(title='Outliers in #Parents\/Children')","62371186":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.preprocessing import StandardScaler\n\n\ndef wrangle_titanic(train, test, target=None, to_drop=None, scaler=None, random_state=42):\n    \"\"\"\n    wrangle_titanic (or munge) train and test sets for prototyping phase.\n    Return X_train, X_test\n    \"\"\"\n    y_train = None\n    train_tmp = train.copy(deep=True)\n    if target is not None:\n        if target not in train_tmp.columns:\n            raise ValueError('target not found in train set')\n        y_train = train_tmp.pop(target)\n    # remove unnecessary features and one-hot encode categorical features\n    X_train = train_tmp.drop(columns=to_drop).pipe(pd.get_dummies, drop_first=True, dtype=bool)\n    X_test = test.drop(columns=to_drop).pipe(pd.get_dummies, drop_first=True, dtype=bool)\n    # select features to impute\/scale\n    num_cols = X_train.select_dtypes(exclude='bool').columns\n    # impute numerical\n    num_imputer = IterativeImputer(random_state=random_state)\n    # fit only on training data to avoid leaking info into the test set\n    num_imputer.fit(X_train[num_cols])\n    X_train.loc[:, num_cols] = num_imputer.transform(X_train[num_cols])\n    X_test.loc[:, num_cols] = num_imputer.transform(X_test[num_cols])\n    if scaler is not None:\n        # fit only on training data to avoid leaking info into the test set\n        scaler.fit(X_train[num_cols])\n        X_train.loc[:, num_cols] = scaler.transform(X_train[num_cols])\n        X_test.loc[:, num_cols] = scaler.transform(X_test[num_cols])\n    # verify missing values\n    print('missing values in training:', X_train.isna().any().any())\n    print('missing values in testing:', X_test.isna().any().any())\n    print('training.shape:', X_train.shape)\n    print('testing.shape:', X_test.shape)\n    return X_train, X_test, y_train","0ec51eae":"# prepare data for training phase\nX_train, X_test, y_train = wrangle_titanic(\n    train_df, test_df, target='Survived', to_drop=['Name', 'Ticket', 'Cabin'],\n    scaler=StandardScaler(), random_state=42\n)\nX_train.head()","2a2119ff":"# is there a relation between the lack of Cabin and the survival rate?\ncabin_missing_train = train_df['Cabin'].isna()\ncabin_missing_test = test_df['Cabin'].isna()\nfig, ax = plt.subplots(figsize=(6,4))\ng = sns.barplot(x=cabin_missing_train, y=y_train, ax=ax).set(xlabel='No Cabin', title='Survival by (lack of) Cabin')","36fbe769":"# is there a relation between the passenger deck and the survival rate?\ndeck_train = train_df['Cabin'].fillna('X').str[0] # deck is the first letter in Cabin\ndeck_test = test_df['Cabin'].fillna('X').str[0]\n# why all passengers on T deck died?\nprint('# passengers on T deck:', len(deck_train[deck_train=='T']))\n# why survival error is hgh for passengers on G deck?\nprint('# passengers on G deck:', len(deck_train[deck_train=='G']))\nfig, ax = plt.subplots(figsize=(6,4))\ng = sns.barplot(x=deck_train, y=y_train, order=['A','B','C','D','E','F','G','T','X'], ax=ax).set(xlabel='Deck', title='Survival by Deck')","edbd7923":"# is there a relation between the passenger title and the survival rate?\ntitle_train = train_df['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\ntitle_test = test_df['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\nfig, ax = plt.subplots(1, 2, figsize=(18,4))\ng = sns.barplot(x=title_train, y=y_train, ax=ax[0]).set(xlabel='Title', title='Survival rate by passenger\\'s Title in training set')\n# group rare titles\ntitle_map = {'Ms': 'Mrs', 'Mme': 'Mrs', 'Mlle': 'Miss'}\ntitle_map.update({t: 'Rare' for t in ['Dr', 'Rev', 'Sir', 'Don', 'Jonkheer', 'Lady', 'the Countess', 'Col', 'Major', 'Capt', 'Dona']})\ntitle_train = title_train.apply(lambda x: title_map.get(x, x))\ntitle_test = title_test.apply(lambda x: title_map.get(x, x))\ng = sns.barplot(x=title_train, y=y_train, ax=ax[1]).set(xlabel='Title', title='Survival rate by passenger\\'s Title (grouped) in training set')","d02fd76a":"# is there a relation between the lenght of the name and the survival rate?\nname_size_train = train_df['Name'].apply(lambda x: len(x.split(' ')))\nname_size_test = test_df['Name'].apply(lambda x: len(x.split(' ')))\nfig, ax = plt.subplots(1, 2, figsize=(18,4))\ng = sns.barplot(x=name_size_train, y=y_train, ax=ax[0]).set(xlabel='name_size_train', title='Survival rate by passenger\\'s Name Parts count')\ng = sns.barplot(x=name_size_train >= 6, y=y_train, ax=ax[1]).set(xlabel='name_size_train', title='Survival rate by passenger\\'s Name Parts count')\n# passengers with longer name have higher chance of survival\nname_size_train = name_size_train >= 6\nname_size_test = name_size_test >= 6","8976ba99":"# normalize the new features (-3 to remove the first redundant level of the one-hot encoding)\ndeck_train.nunique() + title_train.nunique() + name_size_train.nunique() - 3","4aedbe9b":"# correlation\ng = sns.heatmap(\n    X_train.assign(Target=y_train).corr(), annot=True, vmin=-1, vmax=1, cmap='RdBu_r', fmt='.2f'\n)","77eb9e00":"# distribution\ng = sns.pairplot(\n    data=X_train.assign(Target=y_train), vars=['Age','Fare','Pclass','Sex_male'], hue='Target', kind='reg'\n)","32972253":"# learning helpers functions\n\n# Source https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_learning_curve.html\ndef plot_learning_curves(model, X, y, cv=5, scoring='roc_auc', train_sizes=np.linspace(0.1, 1.0, 5), n_jobs=-1, ylim=None, ax=None, figsize=(10,4)):\n    # compute\n    train_sizes_abs, train_scores, test_scores = learning_curve(\n        model, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=scoring,\n    )\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    # plot\n    if not ax:\n        _, ax = plt.subplots(figsize=figsize)\n    ax.set_title(f'Learning Curves {model}')\n    if ylim is not None:\n        ax.set_ylim(*ylim)\n    ax.set_xlabel('# Training Examples')\n    ax.set_ylabel(f'{scoring.title()} Score (CV={cv})')\n    ax.fill_between(train_sizes_abs, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.2,\n                         color=\"r\")\n    ax.fill_between(train_sizes_abs, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.2,\n                         color=\"g\")\n    ax.plot(train_sizes_abs, train_scores_mean, 'o-', color=\"r\", label='Training score')\n    ax.plot(train_sizes_abs, test_scores_mean, 'o-', color=\"g\", label='Cross-validation score')\n    ax.legend(loc='lower right')\n    ax.grid(True)\n    return ax\n\n\ndef evaluate(model, X, y, cv=5, scoring='roc_auc', verbose=1, n_jobs=-1):\n    g = plot_learning_curves(model, X, y, cv=cv, scoring=scoring) # how the model learns on a growing number of training examples?\n    cv_score = cross_val_score(model, X, y, cv=cv, scoring=scoring) # how well the model generalize on new training examples?\n    model_id = model.__class__.__name__\n    print(f'{model_id} {scoring}: {cv_score.mean():.4f} +\/- {cv_score.std():.4f}')\n    return dict({\n        scoring: cv_score.mean(), 'error': cv_score.std(), 'id': model_id, 'model': model, 'params': model.get_params()\n    })","3ad07a00":"# ensure reproducibility\nrandom_state = 42\n# cross-validation kolds\ncv = 5\n# evaluation metric\nscoring = 'roc_auc'\n# preselection\nmodels = [\n    LogisticRegression(random_state=random_state), # baseline score to beat\n    SVC(random_state=random_state),\n    RandomForestClassifier(random_state=random_state),\n    GradientBoostingClassifier(random_state=random_state),\n    # See https:\/\/xgboost.readthedocs.io\/en\/release_0.72\/get_started.html\n    XGBClassifier(random_state=random_state, eval_metric='auc') # enforce roc_auc scoring\n]\n# create hypotheses collector and store the baseline hypotesis (i.e. model with best scoring)\nhypotheses = []\nfor model in models:\n    hypotheses.append(evaluate(model, X_train, y_train, cv=cv, scoring=scoring))\n# visualize CV report\nhypotheses_df = pd.DataFrame(hypotheses).sort_values(scoring)\nfig, ax = plt.subplots(figsize=(8, 4))\nids = hypotheses_df['id']\nax.barh(ids, hypotheses_df[scoring], xerr=hypotheses_df['error'], align='center', alpha=0.5, ecolor='black', capsize=10)\nax.set_xlabel(scoring.title())\nax.set_xlim(.7, .95)\nax.set_yticks(ids)\nax.set_yticklabels(ids)\nax.set_title('Cross Validation Report')\nax.xaxis.grid(True)\nplt.tight_layout();","e56f7f28":"hypotheses_df","ceb67be1":"# Under VS Overfitting Analysis\n# GradientBoostingClassifier performs well, it slightly overfits the data\n#   cross-validation score is lower than the training score (\\~10%)\n#   noticeable variability (error oscillations) in the cross-validation score\n# Solutions\n#   more data can help the model to generalize better\n#   we can tweak the regularization parameters\n\n# how do we fix an overfitting model?\n# From the GradientBoostingClassifier Scikit-learn documentation, we can use 'learning_rate',\n# 'subsample', and 'max_features' hyperparamters to increase\/decrease the effect of regularization.\n# For more details, see https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_gradient_boosting_regularization.html\n# Note that subsampling (subsample < 1.0) without shrinkage (learning_rate < 1.0) usually does poorly.\n\n# NEW HYPOTHESIS: shrinking the Learning Rate parameter helps to reduce model overfitting\n# Note that Learning Rate of GradientBoostingClassifier is not the same parameter of XGBClassifier\nparams = hypotheses_df.loc[hypotheses_df[scoring].idxmax(), 'params']\n# update the regularization hyperparameter learning rate\nregularized_learning_rate = {'learning_rate': .01}\nparams.update(regularized_learning_rate)\n# re-evaluate with the new learning rate\nlr_h = evaluate(GradientBoostingClassifier(**params), X_train, y_train, cv=cv, scoring=scoring)\n# cache hypotesis evaluation\nhypotheses_df = hypotheses_df.append(lr_h, ignore_index=True)\n# Much better already!\n# We mitigate overfitting by shrinking the learning rate, therefore we reject the null hypotesis (reducing learning rate has no effect).\n# However, our model still overfit given the large variability on the validation score.\n# We will implement RandormSearchCV to quickly evaluate multiple hyperparmeters at once\n# and try to reduce overfitting while increasing the ROC AUC score.","4d4f4f05":"# XGBClassifier randomized search distribution\nxgb_params = {\n    'eta': [.3, .1, .01], # (.3) step size shrinkage used in update to prevents overfitting\n    'booster': ['gbtree', 'dart'],\n    'gamma': np.linspace(.01, 1), # (0) the larger gamma is, the more conservative the algorithm will be.\n    'subsample': np.linspace(0.1, 1), # if 0.5 XGBoost would randomly sample 1\/2 of the training prior to growing trees\n    'max_depth': [6, 5, 7], # (6) low values of max_depth keep the model simple\n    'alpha': np.linspace(.01, 1), # (0) L1 regularization, increasing this value will make model more conservative\n    'lambda': np.linspace(.1, 1), # (1) L2 regularization, increasing this value will make model more conservative\n    'eval_metric': ['auc'],\n    'random_state': [random_state],\n}\n# enable randomized search with hp argument\nxgb_search = RandomizedSearchCV(\n    XGBClassifier(), xgb_params, cv=cv, scoring=scoring, random_state=random_state, verbose=1, n_jobs=-1\n).fit(X_train, y_train)\n# evaluate xgboost hypotesis\nxgb_search_h = evaluate(XGBClassifier(**xgb_search.best_params_), X_train, y_train, cv=cv, scoring=scoring)\n# cache hypotesis evaluation\nhypotheses_df = hypotheses_df.append(xgb_search_h, ignore_index=True)\n# In general, GradientBoostingClassifier achieve better cross-validation score performance and\n# less score variability (more consistent scores).\n# Next we start with the features selection since both models seem to overfit the data\n# i.e. variability + gap between training and cross-validation scores","d61bf735":"# GradientBoostingClassifier randomized search distribution\ngb_params = {\n    'n_estimators': [50, 100, 250, 500],\n    'learning_rate': np.logspace(-3, -1, 50),\n    'subsample': np.logspace(-2, 1, 50),\n    'max_features': [.1, .25, .5, .75, None],\n    'max_depth': [3, 4, 5], # low values of max_depth keep the model simple\n    'random_state': [random_state],\n}\n# randomized search model\ngb_search = RandomizedSearchCV(\n    GradientBoostingClassifier(), gb_params, cv=cv, scoring=scoring, random_state=random_state, verbose=1, n_jobs=-1\n).fit(X_train, y_train)\ngb_search_h = evaluate(GradientBoostingClassifier(**gb_search.best_params_), X_train, y_train, cv=cv, scoring=scoring)\n# cache hypotesis evaluation\nhypotheses_df = hypotheses_df.append(gb_search_h, ignore_index=True)\n# We produced a ~3% improvement (from the baseline score of 84.9%) on the cross-validation score with the right hyperparameters.\n# The model seems to generalize worse than the previous hypothesis and it could benefit from more data.\n# However, the gap between the training and cross-validation scores is still in the range of ~10%","56953be2":"# NEW HYPOTHESIS: model performance improves if we merge and one hot encode categorical features enginireed previously\n# concatenate with original data into unique dataframe\nX_train1 = pd.concat([X_train, pd.get_dummies(\n    title_train, dtype=bool, drop_first=True, prefix='title'\n).assign(cabin_missing=cabin_missing_train, long_name=name_size_train)], axis=1)\n# same for test df\nX_test1 = pd.concat([X_test, pd.get_dummies(\n    title_test, dtype=bool, drop_first=True, prefix='title'\n).assign(cabin_missing=cabin_missing_test, long_name=name_size_test)], axis=1)\n\n# Rerun randomized search model to refit the model to the new data\ngb_search = RandomizedSearchCV(\n    GradientBoostingClassifier(), gb_params, cv=cv, scoring=scoring, random_state=random_state, verbose=1, n_jobs=-1\n).fit(X_train1, y_train)\n# Re-evaluate model performance\nh1 = evaluate(GradientBoostingClassifier(**gb_search.best_params_), X_train1, y_train, cv=cv, scoring=scoring)\n# cache hypotesis evaluation\nhypotheses_df = hypotheses_df.append(h1, ignore_index=True)","dd2d9fe9":"# features importance\nI = pd.DataFrame()\nI['Feature'] = X_train1.columns\nI['Importance'] = GradientBoostingClassifier(**gb_search.best_params_).fit(X_train1, y_train).feature_importances_\nI = I.sort_values('Importance', ascending=False).set_index('Feature')\nrfpimp.plot_importances(I, width=5, color='#FDDB7D',title=\"Feature importance via average gini\/variance drop (sklearn)\")","c8915da3":"# The scikit-learn Random Forest feature importances strategy is mean decrease in impurity (or gini importance)\n# mechanism, which is unreliable. To get reliable results, use permutation importance, provided in the rfpimp package.\n# Given training observation independent variables in X_train (a dataframe), compute the feature importance\n# using each var as a dependent variable using a RandomForestRegressor or RandomForestClassifier.\n# We retrain a random forest for each var as target using the others as independent vars.\n# Only numeric columns are considered.\n# For more info, see https:\/\/github.com\/parrt\/random-forest-importances\nrfpimp.plot_dependence_heatmap(rfpimp.feature_dependence_matrix(X_train1), figsize=(7,6))\n# The dependence heatmap can be read as follows:\n# The features on the X axis predict the features on the Y axis, the higher the score, the higher the correlation.\n# Pclass can be (100%) predicted from Fare so we should remove Pclass and still obtain equivalent results","a2694bf7":"# NEW HYPOTHESIS: model performance improves if we remove uniformative and correlated predictors\n# we should expect similar cv score even with fewer features\nX_train2 = X_train1.drop(columns='Pclass')\nX_test2 = X_test1.drop(columns='Pclass')\nprint(X_train2.shape[1], 'predictors:', X_train2.columns.values)\nh2 = evaluate(GradientBoostingClassifier(**gb_search.best_params_), X_train2, y_train)\n# cache hypotesis evaluation\nhypotheses_df = hypotheses_df.append(h2, ignore_index=True)\n# ROC AUC score and variability remain stable but we obtained a simpler model that is less likely to overfit\n# therefore we reject the null hypotesis","ee62221d":"# How is the features dependence matrix now?\nrfpimp.plot_dependence_heatmap(rfpimp.feature_dependence_matrix(X_train2), figsize=(7,6))","03401851":"# NEW HYPOTHESIS: combining SibSp and Parch features does not help reducing model complexity hence overfitting\n# Engineer family_size as the sum of SibSp + Parch so we wrangle_titanic again the data\n# 1 is the passenger themselves\n# Remember to drop Pclass\nX_train3, X_test3, _ = wrangle_titanic(\n    train_df.assign(family_size=(1 + train_df['SibSp'] + train_df['Parch'])),\n    test_df.assign(family_size=(1 + test_df['SibSp'] + test_df['Parch'])),\n    target='Survived', to_drop=['Pclass', 'SibSp', 'Parch', 'Name', 'Ticket', 'Cabin'], scaler=StandardScaler(), random_state=42\n)\n\nX_train3 = pd.concat([X_train3, pd.get_dummies(\n    title_train, dtype=bool, drop_first=True, prefix='title'\n).assign(cabin_missing=cabin_missing_train, long_name=name_size_train)], axis=1)\n# same for test df\nX_test3 = pd.concat([X_test3, pd.get_dummies(\n    title_test, dtype=bool, drop_first=True, prefix='title'\n).assign(cabin_missing=cabin_missing_test, long_name=name_size_test)], axis=1)\n\nprint(X_train3.shape[1], 'predictors:', X_train3.columns.values)\nh3 = evaluate(GradientBoostingClassifier(**gb_search.best_params_), X_train3, y_train)\n# cache hypotesis evaluation\nhypotheses_df = hypotheses_df.append(h3, ignore_index=True)\n# ROC AUC score increases (~1%) and so the variability error (even if in lower entity)\n# We obtained a simpler modelt by combining two features therefore we reject the null hypotesis","94855521":"# How is the features dependence matrix now?\nrfpimp.plot_dependence_heatmap(rfpimp.feature_dependence_matrix(X_train3), figsize=(7,6))","7401fab7":"# features importance\nI = pd.DataFrame()\nI['Feature'] = X_train3.columns\nI['Importance'] = GradientBoostingClassifier(**gb_search.best_params_).fit(X_train3, y_train).feature_importances_\nI = I.sort_values('Importance', ascending=False).set_index('Feature')\nrfpimp.plot_importances(I, width=5, color='#FDDB7D', title=\"Feature importance via average gini\/variance drop (sklearn)\")","c61b851c":"# X_extra = temp?\n# Instead of dropping Pclass, we multiply Fare by Family_size to get family_fare estimate\nX_train4, X_test4, _ = wrangle_titanic(\n    train_df.assign(family_fare=train_df['Fare'] * (1 + train_df['SibSp'] + train_df['Parch'])),\n    test_df.assign(family_fare=test_df['Fare'] * (1 + test_df['SibSp'] + test_df['Parch'])),\n    target='Survived', to_drop=['Pclass', 'SibSp', 'Parch', 'Name', 'Ticket', 'Cabin'], scaler=StandardScaler(), random_state=42\n)\n\nX_train4 = pd.concat([X_train4, pd.get_dummies(\n    title_train, dtype=bool, drop_first=True, prefix='title'\n).assign(cabin_missing=cabin_missing_train, long_name=name_size_train)], axis=1)\n# same for test df\nX_test4 = pd.concat([X_test4, pd.get_dummies(\n    title_test, dtype=bool, drop_first=True, prefix='title'\n).assign(cabin_missing=cabin_missing_test, long_name=name_size_test)], axis=1)\n\nprint(X_train4.shape[1], 'predictors:', X_train4.columns.values)\nh4 = evaluate(GradientBoostingClassifier(**gb_search.best_params_), X_train4, y_train)\n# cache hypotesis evaluation\nhypotheses_df = hypotheses_df.append(h4, ignore_index=True)","ee0f62a4":"# How is the features dependence matrix now?\nrfpimp.plot_dependence_heatmap(rfpimp.feature_dependence_matrix(X_train4), figsize=(7,6))","917db147":"# features importance\nI = pd.DataFrame()\nI['Feature'] = X_train4.columns\nI['Importance'] = GradientBoostingClassifier(**gb_search.best_params_).fit(X_train4, y_train).feature_importances_\nI = I.sort_values('Importance', ascending=False).set_index('Feature')\nrfpimp.plot_importances(I, width=5, color='#FDDB7D', title=\"Feature importance via average gini\/variance drop (sklearn)\")","f2d33162":"# hypoteses evaluation\nhypotheses_df.sort_values('roc_auc', ascending=False)\n# hypotesis 8 is the best in terms of accuracy (roc_auc) and consistency (error) so we select that for submission","00309cfb":"# select the best model wrt to roc_auc and error\nparams = hypotheses_df.loc[7, 'params'] # obtained with the original data\n# prepare submission\nmodel = GradientBoostingClassifier(**params).fit(X_train, y_train)\nmy_submission = pd.DataFrame({'PassengerId': X_test.index, 'Survived': model.predict(X_test)})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('my_submission.csv', index=False)","80622899":"* on average, french passengers paid higher fares and they reserved better spots on board\n* note that we use the mean to account for outliers","3f10a04e":"* `Title` is a great engineered feature as it contains information over passenger's genger and age\n* `Mr` referes to adult male passengers who had the lowest survival rate because of the `women and children first` protocol\n* `Mrs` and younger passengers (`Miss`, and `Master`) show a clear difference in the chance of survival\n    * note also the striking difference between `master` (young male) and `miss` (young un married female) titles","796ac098":"* on average, passengers embarked at Cherbourg are younger","7df9e09c":"* survival rate increase if passenger travelled in `small families`\n* survival rate of passengers with more relatives are unreliable given the large variations","3b31b644":"* as we saw, the average age of first-class passenger is higher in comparison to the other 2 SES groups\n* we will be able to catch this peculiarity by binning age into age groups\n\n**Extras**\n\n*Crosby, Capt. Edward Gifford*, the captain of the Titanic died with the ship","8ac22f8a":"* not surprisingly female passengers have the highest survival rate","578e740b":"* we can see two main groups:\n    * short names (< 6) with low survival rate (`>0.4`)\n    * long names (> 6) with high survival rate (`<0.7`)\n* note that longer names become more rare and the error increases","a4ecd24b":"## Model Selection\nNext, we evaluate the performance of the selected models to find the best candidate to classify survival of passengers.\n\nWe assume:\n* `LogisticRegression` is our baseline model because it is one of the simplest models for binary classification\n* all models are tested with default parameters (besided XGBoost scoring metric)\n* `ROC AUC` or Area Under the Curve score is our evaluation metric\n* `ROC AUC` score is computed as the average of cross-validation scores obtained on the validation set","f1d5853e":"* passengers without deck (`X`) had very little chance of survival\n* large error for `G` deck due to small number of data points available\n* only 1 passenger on `T` deck cause 0 survival rate\n    * we merge `T` with the deck mode (`X`) to reduce the number of levels\n* only 4 passengers on `G` deck cause the large error","7574d05b":"# Conclusions & Submission\nIn summary, we investigate and discover many insights from the provided datasets. For example a strong contributor to predict the survival of passengers besides `Sex` and `Age` was `Fare`. We so how multicollinearity confuses the model and does not help in performance improvements (e.g. between `Fare` and `Pclass`). We also saw how a feature in a very bad shape (`Cabin`) can be transformed into valuable knowledge to train a machine learning model. In the end, we reviewed how sensitive models are during the model selection phase when we played with the effect of engineered features, features selection, and hyperparameters tweaking.\n\nMany other optimizations can be applied to further improve the score, such as try out different models, new hyperparameters (e.g. polynomial features, gamma) and engineer new features*. Also, it is interesting to dig deeper and find out the prediction mistakes that the model makes and play around with precision\/recall trade-off.\n\n**\\*Bonus**<br>\nI have created a function to impute the missing age of passengers of both training and test set. See the helper function cell after the imports statements. We could try to improve the age related features and the score of the model.","bf0154ee":"* `Cabin` is prominently available for first-class passengers which hints us that is Missing Not At Random (MNAR)\n* create a category out of the missing values to retain knowledge about this feature","498b876b":"# Exploratory Data Analysis\n1. Data Inspection\n2. Data Representativeness\n3. Univariate Analysis\n4. Bi-variate Analysis\n5. Missing Values\n6. Outliers","f9f88042":"## Outliers","bb6cd6b6":"`Name`, `Ticket`, and `Cabin` features need to be cleaned and normalized","73759ad9":"* `Age` has a light bimodal distribution for **male survivors** highlighting 2 groups and 1 tail: children\/adults and old passengers\n    * breaking point between dead and survived is \\~13y (children\/adult) and \\~50y (adult\/old)\n* young survivors are mostly male while older survivors (20-40y) are mostly female (mothers accompanying their offspring?)\n\n**Notes**\n`Age` has \\~20% missing values that do not appear in the graph.","d0f540f6":"* for some reasons, passengers embarked at Cherbourg have higher survival rate","98c7e8ed":"* `Survived` is our target feature so we are dealing with a binary classification problem\n* missing values in both datasets:\n    * `Age` and `Cabin` have many missing values and we need to find a good way to fix it without losing information and\/or adding bias\n    * `Fare` and `Embarked` have just few missing values that we can impute using descriptive metrics","4ea3f079":"## Missing Values","981752f9":"**Extras**\n\n**Fare** \/ ticket [PC 17755](https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/thomas-cardeza.html) is the ticket of the parlour suite that cost 512 pounds. All wealthiest passengers survived.<br>\n15 male passengers of different SES, all embarked at Southampton, did not pay. 4\\* of them (`Ticket`= \"LINE\") were employees of the [American Line](https:\/\/en.wikipedia.org\/wiki\/American_Line) which was part of the [IMM](https:\/\/en.wikipedia.org\/wiki\/International_Mercantile_Marine_Company) trust together with the [White Star Line](https:\/\/en.wikipedia.org\/wiki\/White_Star_Line). Due to a [British coal strike](https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/william-henry-tornquist.html), they had to travel back to New York on board of the Titanic. Of the 4, only *Tornquist, Mr. William Henry* survived the sinking.\n\n\\*According to [Encyclopedia Titanica](https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/william-henry-tornquist.html), 6 employees affected by the strike were on board of the Titanic: *William Cahoone Johnson Jr., August (Alfred) Johnson, Lionel Leonard (Andrew Shannon), Alfred Carver and Thomas Storey*. The last 2 are not present in the data.","ff90ef7a":"* `Age` appears normally distributed\n* we need to transform `SibSp`, `Parch`, and `Fare` to obtain a more normal distribution\n* `Survived` and `Pclass` should be converted to Nominal (boolean) and Ordinal, respectively","e7efd918":"**ROC AUC SCORE TO BEAT: 84.9%** (Logistic Regression)\n* our baseline model `Logistic Regression` has the worst performance but it has the smaller cv score variability\n* `GradientBoosting` has the highest `ROC AUC` score\n    * it outperforms also `XGBoost`\n* `RandomForest` seems to overfit the data\n    * large gap between training and cv score seen in the learning curves and cross-validation score variability\n* `Logistic Regression` and `SVC` learning is more consistent\n    * smallest gap between training and cv score seen in the learning curves and cross-validation score variability\n\nIn conclusion, we select `GradientBoosting` as the best fit for the next phase given that our target is to maximize the accuracy of model predictions and we already obtained a solid ~2% score improvement","d02dd51e":"# Kaggle Titanic Challenge in Python\n\nEDA and Machine Learning techniques applied with Python on one of the most famous datasets in the Data Science community.\n\n\n# Table of Contents\n* Introduction\n* Exploratory Data Analysis\n* Iterative Learning Phase\n* Submition & Conclusions\n* Credits\n\n\n# Introduction\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this [challenge](https:\/\/www.kaggle.com\/c\/titanic\/overview), we need to build and train a predictive model that answers the question: \u201cwhat sorts of passengers on the RMS Titanic were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n\n## Overview\n\n*Table 1: Data Overview*\n\n|Variable|Definition|Key|Notes|\n|---|---|---|:---|\n|`PassengerId`|Passenger Identifier||It is required by Kaggle to match predicted passangers at submission|\n|`Survival`|Survival|0=No, 1=Yes||\n|`Pclass`|Ticket class|1 = 1st, 2 = 2nd, 3 = 3rd|A proxy for Socio Economic Status (SES):<ul><li>1st = Upper<\/li><li>2nd = Middle<\/li><li>3rd = Lower<\/li><\/ul>|\n|`Sex`|Gender|||\n|`Age`|Age in years||Age is fractional if less than 1.<br>If the age is estimated, it is a float (xx.5)|\n|`Sibsp`|# of siblings \/ spouses aboard the Titanic||The dataset defines family relations in this way:<ul><li>Sibling = brother, sister, stepbrother, stepsister<\/li><li>Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)<\/li><\/ul>|\n|`Parch`|# of parents \/ children aboard the Titanic||The dataset defines family relations in this way:<ul><li>Parent = mother, father<\/li><li>Child = daughter, son, stepdaughter, stepson<\/li><\/ul>Some children travelled only with a nanny, therefore parch=0 for them.|\n|`Ticket`|Ticket number|||\n|`Fare`|Passenger fare||Fare unit is \u00a3 (see [statisticalconsultants](https:\/\/www.statisticalconsultants.co.nz\/blog\/titanic-fare-data.html))|\n|`Cabin`|Cabin number|||\n|`Embarked`|Port of Embarkation|C=Cherbourg, Q=Queenstown, S=Southampton|||\n\n<br><br>\n\n## Facts\nThe following facts are gathered from the [Titanic Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Titanic) page.\n\nFrom [Introduction](https:\/\/en.wikipedia.org\/wiki\/Titanic)\n> RMS Titanic was a British passenger liner operated by the White Star Line that sank in the North Atlantic Ocean on 15 April 1912, after striking an iceberg during her maiden voyage from Southampton to New York City. Of the estimated 2,224 assengers and crew aboard, more than 1,500 died*.\n\n> **A disproportionate number of men were left aboard because of a \"women and children first\" protocol for loading lifeboats.** At 2:20 am, she broke apart and foundered with well over one thousand people still aboard. Just under two hours after Titanic sank, the Cunard liner RMS Carpathia arrived and brought aboard an **estimated 705 survivors.** \n\nFrom [Crew](https:\/\/en.wikipedia.org\/wiki\/Titanic#Crew) Section\n> Titanic had around 885 crew members on board for her maiden voyage.\n\nFrom [Passengers](https:\/\/en.wikipedia.org\/wiki\/Titanic#Passengers) Section\n> Titanic's passengers numbered approximately **1,317** people: 324 (25%) in First Class, 284 (21%) in Second Class, and 709 (54%) in Third Class. Of these, 869 (66%) were male and 447 (34%) female. There were 107 children aboard, the largest number of whom were in Third Class. **The ship was considerably under capacity on her maiden voyage, as she could accommodate 2,453 passengers \u2014 833 First Class, 614 Second Class, and 1,006 Third Class.**\n\n> The exact number of people aboard is not known, as not all of those who had booked tickets made it to the ship; about 50 people cancelled for various reasons, and not all of those who boarded stayed aboard for the entire journey.\n\nFrom [Collecting Passengers](https:\/\/en.wikipedia.org\/wiki\/Titanic#Collecting_passengers) Section\n> The large number of Third Class passengers meant they were the first to board, with First and Second Class passengers following up to an hour before departure. In all, 920 passengers boarded Titanic at Southampton \u2013 179 First Class, 247 Second Class, and 494 Third Class. \n\n> Four hours after Titanic left Southampton, she arrived at Cherbourg. There, 274 additional passengers were taken aboard \u2013 142 First Class, 30 Second Class, and 102 Third Class. Twenty-four passengers left aboard the tenders to be conveyed to shore, having booked only a cross-Channel passage.\n\n> At 11:30 a.m. on Thursday 11 April, Titanic arrived at Cork Harbour on the south coast of Ireland. In all, 123 passengers boarded Titanic at Queenstown \u2013 three First Class, seven Second Class and 113 Third Class. In addition to the 24 cross-Channel passengers who had disembarked at Cherbourg, another seven passengers had booked an overnight passage from Southampton to Queenstown.\n\n**\\* Although, I have seen several Kaggle notebook reporting 2224 estimated passengers, I feel that the correct estimated number is 2202 (i.e. 885 [Crew](https:\/\/en.wikipedia.org\/wiki\/Titanic#Crew) and 1317 [Passengers](https:\/\/en.wikipedia.org\/wiki\/Titanic#Passengers)). The same estimated number of passengers is also reported in [Encyclopedia Titanica](https:\/\/www.encyclopedia-titanica.org\/titanic-passenger-lists\/). In addition to that, I found several inconsistencies with regards to the number of passengers throughout the Wiki page. For instance, the crew and passenger numbers reported in the Crew and Passengers sections, respectively do not add up to the estimated number of passengers (2224) reported in the Introduction.**\n\n## Representativeness\nGiven the available facts, we verify if our training data is a good representation of reality. We assume **1317** to be the most precise estimated number of passengers - crew members excluded - as reported by Wikipedia and Encyclopedia Titanica. The following points will be verified:\n1. the estimated survival rate is \\~32% (i.e. 705 survivors by 1317 passengers + 885 crew members)\n2. the passengers per class distribution are 25% 1st class, 21% 2nd class, and 54% 3rd class\n3. the passengers per sex distribution are 66% male, and 34% female\n4. the passengers per port of embarkation are 920 (\\~70%) at Southampton (-24 who left at Cherbourg), 274 (\\~20%) at Cherbourg (-7 who left at Queenstown), 123 (\\~10%) at Queenstown\n5. women and children group has higher survival rate due to the `women and children first` protocol followed when loading lifeboats\n6. first-class passengers have higher survival rate as they were closer to the boat deck where the lifeboats were located","4bf86a0d":"## Features Engineering & Selection\nAlthough fitting and evaluating models is routine, achieving good performance for a predictive modeling problem is highly dependent upon how the data is prepared. Features Engineering & Selection are important steps \n\nWe use variations of *feature_importances_* (i.e. tree-based property of `GradientBoostingClassifier` model) and cross-validation scores as reference to evaluate different *hypoteses*. We refer to *feature_importances_* score as **contribution**.\n\n    For many problems, additional variation in the response can be explained by the effect of two or more predictors working in conjunction with each other. [\u2026] More formally, two or more predictors are said to interact if their combined effect is different (less or greater) than what we would expect if we were to add the impact of each of their effects when considered alone.\n\n\u2014 Page 157, \u201c[Feature Engineering and Selection](https:\/\/amzn.to\/2VZgn4j)\u201d 2019.","c114895b":"* 4 features have missing values in both training and test sets\n* `Fare` and `Embarked` have just 1, and 2 missing values respectively and can be imputed with the mode value of their distribution\n* `Age` and `Cabin` have many missing values and imputing them might introduce bias\n\n**Extras**\n\n**Cabin** \/ According to [encyclopedia-titanica.org](https:\/\/www.encyclopedia-titanica.org\/cave-list.html) only the first class passenger list was recovered after the disaster on the body of a first class stewards. This makes the data to be Missing Not At Random. We will consider missing values in `Cabin` as another category level.<br>\n\n**Embarked** \/ We imputed the missing value as the feature mode (`S`). We confirm that the imputation is correct via Encyclopedia Titanica for the 2 passengers, [Icard, Miss. Amelie](https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/amelia-icard.html), and [Stone, Mrs. George Nelson (Martha Evelyn)](https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/martha-evelyn-stone.html).","03b04df3":"The training dataset is a cross-sectional sample made of 891 observations (~40% of the total number of passengers). It is representative of the facts we gathered above","102862f2":"## Features Engineering","e816ac70":"## Data Preparation\n\nSo far, we investigated the relations between features and the survival rate. Gender, age, and Socio Economic Status (SES) of passengers are very important features in predicting survival rate of the Titanic passengers.\n\nPrior to start the learning phase, we will prepare the datasets for the training phase. We treat missing values as well as outliers, and we transform the features for model fitting.\n\n> Refactorized to improve consistency and simplify Notebook. The changes allow us to easily implement hypoteses iteratively during model selection. The following changes have been implemented:\n    * removed features binning\n    * removed missing indication features\n    * removed by-feature transformations\n    * introduced IterativeImputers for missing data\n    * introduced normalization with Scikit-learn StandardScaler\n    * removed first level of categorical features during one-hot encoding conversion\n    * created a function that return train, test, and target data ready for training","fe7fecd2":"* same as above","f830c415":"* survival rate is higher for wealthier passengers","ca1fbd80":"## Bi-variate Analysis\nBi-variate Analysis finds out the relationship between two variables. Here, we look for association and disassociation between variables at a pre-defined significance level. We can perform bi-variate analysis for any combination of categorical and continuous variables. The combination can be: Categorical & Categorical, Categorical & Continuous and Continuous & Continuous. Different methods are used to tackle these combinations during analysis process.","c447c8b2":"* `Age` is influenced more by `Pclass` than by `Sex`\n* they should be taken into account when we impute missing ages to limit the guessing of a chosen model\n* distribution of missing ages is similar across training and test datasets","bdb8b619":"## Data Representativeness","635113a9":"# Iterative Learning Phase\nIn this phase we find the best model that accurately predicts what passengers in the test dataset either survived or not.\nWe will try different approaches to find a model that makes accurate predictions. *Accuracy score* is the metric to be optimized.\n\nFirst we select the most accurate model from a pool of different models and we use the highest accuracy score as the baseline from which we want to improve. The cheat sheet below helps us choosing the right models for our *classification* task:\n![Scikit Learn Algorithm Cheat Sheet](http:\/\/scikit-learn.org\/stable\/_static\/ml_map.png)\n\nThen, we use cross-validation and learning curves to determine whether the model is underfitting or overfitting the data. In case of underfitting, we select a more complex model by creating new features and\/or decreasing regularization (e.g. `C` in Scikit Learn). Whereas in case of overfitting, we select a simpler model by decreasing the number of features and\/or increasing regularization.","6c4dbf60":"The *average Titanic passenger* had \\~38% chance of survival, was about 29-30 years old, travelled alone (`SibSp=0.5`, `Parch=0.4`) in third class and paid about 32\u00a3. Now, this does not take into account the substantial differences with regards to the SES (`Pclass`) and the gender (`Sex`). We will analyse those in more details later in the notebook.\n\n> Note that `Survived` is actually nominal (bool). It is interpreted as numerical returning the survival rate %","5525d5ec":"* we engineered 13 extra features for a total of 21 features including the 8 features we prepared for training in the previous section\n* most likely some of the features are not informative and can introduce noise during training; ANOVA will come in handy with features selection","21b44f72":"* `Sex` and `Embarked` have fewer unique values (2-3) which make them candidates for exploration\n* `Name`, `Ticket` and `Cabin` features need further processing and they will be handled below","777239d7":"# Credits\n1. [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Passengers_of_the_Titanic)\n2. [Encyclopedia Titanica](https:\/\/www.encyclopedia-titanica.org\/)\n3. [Dummies](https:\/\/www.dummies.com\/education\/history\/titanic-facts-the-layout-of-the-ship\/)\n4. Roche (2016), **wikitablescrape**, [Source Code](https:\/\/github.com\/rocheio\/wiki-table-scrape)\n5. Zheng (2015), Evaluating Machine Learning Models, O\u2019Reilly Media\n6. Kuhn & Johnson (2019), Feature Engineering and Selection: A Practical Approach for Predictive Models, Chapman & Hall\/CRC\n7. Parr & Turgutlu (2018), **rfpimp**, [Source Code](https:\/\/github.com\/parrt\/random-forest-importances)","95851502":"* unexpectedly, `Age` shows no correlation with the target\n    * increases with `Fare`, but decreases with the number of relatives `SibSp` and `Parch`\n* `Pclass` is negatively correlated with `Fare` which highlighs the differences in SES\n* `Pclass` and `Fare` correlate with the target feature\n    * higher values of `Fare` and `Pclass` (lower SES) have higher survival rate\n* as expected, `Parch` and `SibSp` correlate since an additional sibling is equivalent to an additional child; it might lead to multicollinearity and a model cannot differentiate the effect of the two","d9f470b0":"* our target is correlated with passenger's `Pclass`, `Fare`, and `Sex_male`\n* `Fare` is negatively correlated with `Pclass`, low Fare == high `Pclass`\n* no correlation exists between `Age` and `Fare`\n* `Age` distribution shows a smaller and a larger group of young survivors\n\n> Highly correlated features do NOT add value and can impact the interpretation of coefficients of regression (or features importance in tree models)","03c9bd24":"* outliers can introduce noise during training and should be removed\/normalized\n* we will create binned versions of these features to:\n    * mitigate the noise from outliers\n    * create new parameters that explain different aspect of the feature that a model can learn","85cdbe94":"**Extras**\n\n**SibSp\/Parch** \/ The [Sages](https:\/\/www.encyclopedia-titanica.org\/titanic-victim\/john-george-sage.html) was the largest family on the Titanic. Their are split across train and test sets. Sadly, the whole family died.","a2a0758a":"## Data Inspection","56bc4e1c":"## Univariate Analysis\nWe explore features in isolation. The methods to perform uni-variate analysis will depend on whether the variable type is categorical or continuous. The features types are distributed as follow:\n\n*Table 2: Features Overview*\n\n|Feature|Type|Scale|Missing Values|Notes|\n|-------|----|-----|--------------|-----|\n|`Age`|Numerical|Continuous|Y||\n|`Fare`|Numerical|Ratio|Y|Missing values in test only|\n|`SibSp`|Numerical|Discrete|N||\n|`Parch`|Numerical|Discrete|N||\n|`Pclass`|Categorical|Ordinal|N||\n|`Name`|Categorical|Nominal|N||\n|`Sex`|Categorical|Nominal|N||\n|`Ticket`|Categorical|Nominal|N|Mixed type (string + integer)|\n|`Cabin`|Categorical|Nominal|Y|Mixed type (string + integer)|\n|`Embarked`|Categorical|Nominal|Y|Missing values in  train only|\n|`Survived`|Categorical|Nominal|N|Target|"}}