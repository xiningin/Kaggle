{"cell_type":{"17484f72":"code","4d054069":"code","213b6c6b":"code","dbaaafdc":"code","8266bc14":"code","0b87d876":"code","272a473c":"code","42dd88cb":"code","a3196d2a":"code","f0ad341b":"code","bc56b501":"code","8505ccfc":"code","4ebb9b0d":"code","724361ff":"code","82dd4c2c":"code","bd44d401":"code","49ba439c":"code","893b26e7":"code","f89d89e1":"code","6bcd0366":"code","e2d8ae96":"code","8c22f8ca":"code","50911ddd":"code","73f907fa":"code","295b61c1":"code","eeb48836":"code","6a3f2d97":"code","c39262fc":"code","73aaccfc":"code","8411588e":"code","0e4795a1":"code","9747b709":"code","7ec7ef38":"code","008cea80":"code","635be104":"code","d78f18c4":"code","58ed7c59":"code","2c22f927":"code","1170393c":"code","96de3acd":"code","4b1f13e7":"code","48b9db45":"code","6c466779":"code","3cb628b9":"code","b0484876":"code","a658672f":"code","6e9dcb68":"code","9a024177":"code","694c8480":"code","34463244":"code","dc763e29":"code","d91b1bf1":"code","f493fe58":"code","6f3368dd":"code","3ba94ac7":"code","9cab4124":"code","398bb1dd":"code","ccab2987":"code","5d0a1984":"code","fe100b52":"code","558dde9e":"code","c7cfea6a":"code","4085cc22":"code","a20290fe":"code","dfdfae75":"code","42154bfb":"code","25251dc8":"code","de7ee550":"code","6e7cbb57":"code","a4ebec02":"code","33334f52":"code","61e858f9":"code","6fcad529":"markdown","a4db9466":"markdown","5b005898":"markdown","68007119":"markdown","d190f1ca":"markdown","b6df5a07":"markdown","48ee731f":"markdown","6781fa3a":"markdown","40657844":"markdown","dc210b94":"markdown","cde0e9e1":"markdown","e1562955":"markdown"},"source":{"17484f72":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","4d054069":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","213b6c6b":"train_path = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\"\ntest_path = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\"\nsample_path = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\"\ndata_desc = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/data_description.txt\"","dbaaafdc":"train_data = pd.read_csv(train_path)\ntest_data = pd.read_csv(test_path)\nsample_data = pd.read_csv(sample_path)","8266bc14":"train_data.shape, test_data.shape","0b87d876":"train_data.head()","272a473c":"data_types = train_data.dtypes\ndata_types.groupby(data_types).count()","42dd88cb":"# both values should be same, it means each value in column \"Id\" is unique\nprint(train_data.shape[0]==train_data['Id'].nunique())\nprint(test_data.shape[0]==test_data['Id'].nunique())","a3196d2a":"# dropping ID column as it has unique value for each record\ntrain_data.drop(\"Id\", inplace=True, axis=1)\ntest_data.drop(\"Id\", inplace=True, axis=1)","f0ad341b":"train_data.shape, test_data.shape","bc56b501":"train_data.isnull().sum().sort_values(ascending=False) \/ len(train_data) * 100","8505ccfc":"test_data.isnull().sum().sort_values(ascending=False) \/ len(test_data) * 100","4ebb9b0d":"# dropping the columns with more than 50% null values\nfor col in train_data.columns:\n    if train_data[col].isnull().sum()\/train_data.shape[0]>0.5:\n        train_data.drop(col, axis=1, inplace=True)\n        test_data.drop(col, axis=1, inplace=True)","724361ff":"train_data.shape, test_data.shape","82dd4c2c":"# Count of null values in respective column of training data\ntrain_null_cols = train_data.isnull().sum().sort_values(ascending=False)\ntrain_null_cols = train_null_cols[train_null_cols>0]\ntrain_null_cols","bd44d401":"# Count of null values in respective column of test data\ntest_null_cols = test_data.isnull().sum().sort_values(ascending=False)\ntest_null_cols = test_null_cols[test_null_cols>0]\ntest_null_cols","49ba439c":"len(skewed_data.index)","893b26e7":"sns.pairplot(train_data[skewed_data.index], size=2.5)","f89d89e1":"# applying transformation on each columns specified above to normalize them\nfor skewed_col in skewed_data.index:\n    if len(train_data[skewed_col].value_counts().index)>20:\n        \n        train_data[skewed_col] = np.log(train_data[skewed_col])\n        train_data[skewed_col].fillna(train_data[skewed_col].mean(), inplace=True)\n        \n        test_data[skewed_col] = np.log(test_data[skewed_col])\n        test_data[skewed_col].fillna(train_data[skewed_col].mean(), inplace=True)","6bcd0366":"for null_col in test_data.columns:\n    if train_data[null_col].dtype == \"object\":\n        train_data[null_col].fillna(train_data[null_col].mode()[0], inplace=True)\n        test_data[null_col].fillna(train_data[null_col].mode()[0], inplace=True)\n    if train_data[null_col].dtype != \"object\":\n        train_data[null_col].fillna(train_data[null_col].mean(), inplace=True)\n        test_data[null_col].fillna(train_data[null_col].mean(), inplace=True)","e2d8ae96":"sns.heatmap(train_data.isnull())","8c22f8ca":"sns.heatmap(test_data.isnull(), cmap=\"YlGnBu\")","50911ddd":"# if correlation is greater than 50 than value will be imputed. Otherwise, columns would be dropped.\ncorr = train_data[:len(train_data)].corr()[\"SalePrice\"].sort_values(ascending=False)","73f907fa":"corr","295b61c1":"# dropping columns with less than 50% correlation\ncorr = corr[abs(corr)>0.5]\nnum_cols = corr.index.tolist()","eeb48836":"len(num_cols), len(numeric_cols)","6a3f2d97":"cols = categ_cols.copy()\ncols.extend(num_cols)","c39262fc":"len(cols) == len(categ_cols)+len(num_cols)","73aaccfc":"len(cols)","8411588e":"train_data = train_data[cols]\ntest_data = test_data[cols[:cols.index(\"SalePrice\")] + cols[cols.index(\"SalePrice\")+1:]]","0e4795a1":"plt.figure(figsize=(10, 5))\nsns.heatmap(train_data.corr())","9747b709":"train_data.shape, test_data.shape","7ec7ef38":"train_data[\"SalePrice\"].describe()","008cea80":"# Sale Price is right skewed as less number of people buy expensive houses\nsns.distplot(train_data[\"SalePrice\"])","635be104":"train_data[\"Transformed_Price\"] = np.log(train_data[\"SalePrice\"])","d78f18c4":"fig, ax = plt.subplots(1, 2, figsize=(10, 5))\nsns.distplot(train_data[\"SalePrice\"], ax=ax[0])\nsns.distplot(train_data[\"Transformed_Price\"], ax=ax[1])","58ed7c59":"df_ = train_data.copy()","2c22f927":"# concatenating both data frames so that encoding could be done smoothly\ndf = pd.concat([train_data, test_data], axis=0)","1170393c":"df.shape","96de3acd":"for categ_col in categ_cols:\n    df = pd.concat([df, pd.get_dummies(df[categ_col], drop_first=True)], axis=1)\n    df.drop(categ_col, axis=1, inplace=True)","4b1f13e7":"df.shape","48b9db45":"# considering only unique columns\ndf = df.loc[:, ~df.columns.duplicated()]","6c466779":"df.shape","3cb628b9":"df.head()","b0484876":"# concatinating Sale Price (Label) to train data\ntrain_data = df.iloc[:len(train_data), :]","a658672f":"train_data.shape","6e9dcb68":"train_data.head()","9a024177":"# reassigning updated testing features\ntest_data = df.iloc[len(train_data):, :]\ntest_data.drop(\"SalePrice\", inplace=True, axis=1)\ntest_data.drop(\"Transformed_Price\", inplace=True, axis=1)","694c8480":"test_data.shape","34463244":"from sklearn.metrics import mean_squared_error as mse\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression","dc763e29":"# initializing k-fold object\nk_fold = KFold(n_splits=5, shuffle=True, random_state=42)","d91b1bf1":"# any values should not be null\nnp.any(np.isnan(train_data))","f493fe58":"# all values should be finite\nnp.all(np.isfinite(train_data))","6f3368dd":"train_data[train_data==-np.inf]=0\ntest_data[test_data==-np.inf]=0","3ba94ac7":"X_train = train_data.drop([\"SalePrice\", \"Transformed_Price\"], axis=1).values\ny_train = train_data[\"Transformed_Price\"]","9cab4124":"X_test = test_data.values","398bb1dd":"np.all(np.isfinite(X_train)), np.all(np.isfinite(X_test))","ccab2987":"X_train","5d0a1984":"y_train","fe100b52":"# define loss function\ndef rmse(score):\n    rmse = np.sqrt(score)\n    return rmse","558dde9e":"lr_scores = []\nlr_model = LinearRegression()\nfor index, data in enumerate(k_fold.split(X_train, y_train)):\n    print(\"Fold-{0}: Training dataset-{1} & Validation dataset-{2}\".format(index, len(data[0]), len(data[1])))\n    lr_model.fit(X_train[data[0]], y_train[data[0]])\n    y_pred = lr_model.predict(X_train[data[1]])\n    lr_scores.append(mse(y_train[data[1]], y_pred))","c7cfea6a":"lr_rmse = rmse(np.mean(lr_scores))\nlr_rmse","4085cc22":"xgb_scores = []\nxgb_reg = xgboost.XGBRegressor()\nfor index, data in enumerate(k_fold.split(X_train, y_train)):\n    print(\"Fold-{0}: Training dataset-{1} & Validation dataset-{2}\".format(index, len(data[0]), len(data[1])))\n    xgb_reg.fit(X_train[data[0]], y_train[data[0]])\n    y_pred = xgb_reg.predict(X_train[data[1]])\n    xgb_scores.append(mse(y_train[data[1]], y_pred))","a20290fe":"xgb_rmse = rmse(np.mean(xgb_scores))\nxgb_rmse","dfdfae75":"xgb_rmse > lr_rmse","42154bfb":"# as linear regression is better then xgoost, that's why making final prediction with LR\nlr_pred = lr_model.predict(X_test)","25251dc8":"y_pred = np.exp(lr_pred)","de7ee550":"y_pred.shape","6e7cbb57":"sample_data.columns","a4ebec02":"sample_data.shape","33334f52":"sample_data[\"SalePrice\"] = y_pred\nsample_data","61e858f9":"sample_data.to_csv(\"sample_submission.csv\", index=False)","6fcad529":"# 1. Importing Libraries","a4db9466":"* Output variable is positively skewewd\n* Will apply *log* transformation to normalize it","5b005898":"* Above results shown that both data set have null values\n* Therefore, each numerical column will be imputed by mean\/ median depends on data skewness\n* Categorical features will be imputed by the mode","68007119":"# 4. Exploring Target Variable - Sale Price","d190f1ca":"# 3. Handling Missing Values","b6df5a07":"# 5. Handing Categorical Features","48ee731f":"## Linear Regression:","6781fa3a":"# 2. Reading Files","40657844":"## XGBoost:","dc210b94":"* PoolQC, MiscFeature, Alley & Fence have more than 50% Null values\n* All above mentioned features will be dropped","cde0e9e1":"* Kfold Cross Validation technique will be applied to get rid of underfitting\/ overfitting\n* Model validation & tuning could be done easily using Kfold Cross Validation","e1562955":"# 5. Model Training"}}