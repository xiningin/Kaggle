{"cell_type":{"db0d3ccc":"code","69ddfad8":"code","062d4f60":"code","8c726e7c":"code","f16ad680":"code","836ff38d":"code","6fa3f12a":"code","2484a880":"code","bcd95bcc":"code","2091ec36":"code","aa0086de":"code","9b8d2af8":"code","373d67bd":"code","e5b0cbe1":"code","9e884fbe":"code","161bc9a3":"code","123de783":"code","680f34ca":"code","a4d46619":"code","100f8761":"code","70f6fd68":"markdown","fed57d2d":"markdown","c61876f2":"markdown","07e88f9a":"markdown","5ce294c9":"markdown","a3465fd7":"markdown","aad6a009":"markdown","2ece9125":"markdown","279f9bd2":"markdown","e2e928ca":"markdown","27d421e1":"markdown","7e6ff759":"markdown","beba197b":"markdown","7b5be8ae":"markdown","3fc20f6a":"markdown","e10dc0c6":"markdown","33aa5284":"markdown","ac281290":"markdown","6eb96eff":"markdown","c5bf7be6":"markdown","f479dbed":"markdown","d1ef5110":"markdown","fbe514e1":"markdown","f939b633":"markdown","98c7d604":"markdown","f4ecd0ce":"markdown"},"source":{"db0d3ccc":"#\nimport numpy as np \nimport pandas as pd \nfrom collections import Counter\n\n#import imblearn\nfrom imblearn.over_sampling import SMOTE\n\n####.   # visus\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight');\n\n####.   #sklearn\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.metrics import classification_report\nfrom sklearn import set_config; set_config(display='diagram')\n\nimport warnings\nwarnings.filterwarnings('ignore')","69ddfad8":"data = pd.read_csv(\"\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\")\ndata.head()","062d4f60":"data.shape","8c726e7c":"target = pd.DataFrame(data[\"DEATH_EVENT\"].value_counts()).reset_index().rename(columns={\"index\": \"status\", \"DEATH_EVENT\": \"count\"})\ntarget[\"status\"] = target[\"status\"].apply(lambda x: \"alive\" if x==0 else \"death\")\ntarget","f16ad680":"X = data.drop([\"DEATH_EVENT\"], axis=1)\ny = data[\"DEATH_EVENT\"]","836ff38d":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify = y)","6fa3f12a":"fig = plt.figure(figsize=(18,7))\ngs = fig.add_gridspec(1,1)\nax1 = fig.add_subplot(gs[0, 0])\nplt.bar(x=list(y_train.value_counts().index), height=list(y_train.value_counts()), tick_label=list(y_train.value_counts().index));\nax1.set_title(\"Train set DEATH_EVENT repartition\")\nplt.show()","2484a880":"cm = sns.light_palette(\"#00688B\", as_cmap=True)\ndata.corr().style.background_gradient(cmap=cm)","bcd95bcc":"over = SMOTE(sampling_strategy=0.9)\n#\nX_resampled, y_resampled = over.fit_resample(X_train, y_train)\n#\nX_resampled = pd.DataFrame(X_resampled, columns=X_train.columns)","2091ec36":"fig = plt.figure(figsize=(18,5))\ngs = fig.add_gridspec(1,2)\nax1 = fig.add_subplot(gs[0, 0])\nplt.bar(x=list(y_train.value_counts().index), height=list(y_train.value_counts()), tick_label=list(y_train.value_counts().index));\nax1.set_title(\"Before resampling\")\nax2 = fig.add_subplot(gs[0, 1:3])\nplt.bar(x=list([0, 1]), height=list(pd.Series(y_resampled).value_counts()), tick_label=[0, 1]);\nax2.set_title(\"After resampling\")\nplt.show()","aa0086de":"counter = Counter(y_resampled)\nprint(counter)","9b8d2af8":"X_resampled.head()","373d67bd":"model = RandomForestClassifier()","e5b0cbe1":"y_pred = model.fit(X_resampled, y_resampled).predict(X_test)","9e884fbe":"fig = plt.figure(figsize=(18,6))\ngs = fig.add_gridspec(1,2)\nax1 = fig.add_subplot(gs[0, 0])\nax1.grid(False)\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(np.array(y_test), y_pred))\ndisp.plot(cmap=\"Blues\", ax=ax1);","161bc9a3":"target_names = ['DEATH_EVENT = 0', 'DEATH_EVENT = 1']\nprint(classification_report(y_test, y_pred,target_names=target_names))","123de783":"%%time\ngrid = {'n_estimators': list(range(1, 100)),\n        'criterion': [\"gini\", \"entropy\"],\n        'max_depth': list(range(1, 600)),\n        'min_samples_split': [int(i) for i in range(2, 100)],\n        'min_samples_leaf': list(range(1, 20))        \n       }\n\nsearch = RandomizedSearchCV(model,\n                            grid,\n                            n_iter=500,\n                            scoring='f1',\n                            cv=3, \n                            n_jobs=-1,\n                            verbose=0)\n\nsearch.fit(X_resampled, y_resampled);\n#\npd.DataFrame({\"params\": list(search.best_params_.keys())+[\"F1 score\"], \n              \"values\": list(search.best_params_.values())+[str(round(search.best_score_*100, 1))+\"%\"]}).set_index(\"params\")","680f34ca":"y_pred = search.best_estimator_.predict(X_test)","a4d46619":"fig = plt.figure(figsize=(18,6))\ngs = fig.add_gridspec(1,2)\nax1 = fig.add_subplot(gs[0, 0])\nax1.grid(False)\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(np.array(y_test), y_pred))\ndisp.plot(cmap=\"Blues\", ax=ax1);","100f8761":"print(classification_report(y_test, y_pred))","70f6fd68":"<div style=\"font-weight:700\">The data set is a bit unbalanced and is very small.<\/div><br>\n\u279c 1\/3 of the given caracteristics results in DEATH_EVENT <br>\n\u279c 2\/3 don't","fed57d2d":"<b>Let's now put 25% of the dataset aside with the same repartition as the full dataset by setting the parameter<\/b> `stratify` <b>to<\/b> `y` <b>in the<\/b> `train_test_split`. <b>It will be used as test set later.<\/b>","c61876f2":"<b>Let's fit the model on<\/b> `X_resampled` <b>and<\/b> `y_resampled` <b>dataset, and use it to predict on unseen dataset<\/b> `X_test`:","07e88f9a":"<b>Let's look at the new repartition of classes after oversampling the minority class:<\/b>","5ce294c9":"<b>Now we have a more balanced dataset that will be used for training model:<\/b>","a3465fd7":"<b>We are going to use a Random Forest Classifier. So we don't need to scale the data first.<\/b>","aad6a009":"<b>We have a baseline score of 88% for F1 score which is quite good.<\/b> <br>In this particular case, we want to detect as much as possible when `DEATH_EVENT` could occur. If we detect a `DEATH_EVENT` that will finally not be one, it is not a big deal. But if we consider someone not at risk and he finally dies, this would be a problem. <br><b> So in this case, we want to minimize as much as possible the false negative. Thus, we want to score on RECALL metric or may be F1 score as well<\/b>. Here the recall score is good too: 96%","2ece9125":"<b>Let's use the <\/b> `best_estimator_` <b>from the grid search to predict on unseen dataset<\/b> `X_test`:","279f9bd2":"<b>First we separate the target from the features:<\/b>","e2e928ca":"<b>Now let's grid search to try to increase the performance of the model if possible:<\/b>","27d421e1":"# 2. Loading data","7e6ff759":"## 4.2. Model tuning","beba197b":"<b>Let's keep all features.<\/b>","7b5be8ae":"<div style=\"display: block; height: 500px; overflow:hidden; text-align:center\">\n     <img src=\"https:\/\/imgur.com\/6q9JgJE.jpg\" style=\"top: 0px;border-radius: 20px; \">\n<\/div>","3fc20f6a":"## 4.1. Baseline model","e10dc0c6":"# 3. Data preparation","33aa5284":"<b>We have a best score of 86.1% for F1 score.<\/b>","ac281290":"<b>We have the following repartition on the train set:<\/b>","6eb96eff":"<b>With the correlation matrix we can see that the features are overall not much correlated with each other.<\/b><br>\n<b>We can also see that the most correlated features to <\/b>`DEATH_EVENT` <b>are<\/b> `time`, `serum_creatinine`, `ejection_fraction` <b>and<\/b> `age`.<br>","c5bf7be6":"<b>Model instantiation:<\/b>","f479dbed":"<b>Baseline Model evaluation:<\/b>","d1ef5110":"<b>We got the same score after model tuning, the baseline scores were already quite high.<\/b>","fbe514e1":"<b>Tuned Model evaluation:<\/b>","f939b633":"# 1. Imports","98c7d604":"<b>We are going to oversample the minority class - 1 or <\/b>`DEATH_EVENT` <b>= YES - with <\/b>`SMOTE` <b>so we have a little bit more class 1 so that the dataset is a little bit more balanced:<\/b>","f4ecd0ce":"# 4. Model"}}