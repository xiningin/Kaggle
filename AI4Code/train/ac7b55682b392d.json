{"cell_type":{"bc6f1144":"code","8ee0e852":"code","1a824841":"code","f0071162":"code","206c113a":"code","ddbb5907":"code","145827ee":"code","0041cc2d":"code","51ab070f":"code","ffaf9cf3":"code","338a2d77":"code","39fe0c87":"code","56e68087":"code","31291b83":"code","0406e480":"code","98b60bf5":"code","58c9e9a0":"code","608bcc89":"code","533f2402":"markdown","68cbbfe7":"markdown","3b9c1a1f":"markdown","1053f0b6":"markdown","d9fdf990":"markdown","a5c3ad83":"markdown","58900a68":"markdown","3a54797b":"markdown","8b81eb63":"markdown"},"source":{"bc6f1144":"!pip install imageio-ffmpeg","8ee0e852":"import os\nimport cv2\nimport imageio\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nfrom tqdm.auto import tqdm","1a824841":"## https:\/\/www.kaggle.com\/go5kuramubon\/merge-label-and-tracking-data\n\n# Read in data files\nBASE_DIR = '..\/input\/nfl-health-and-safety-helmet-assignment'\n\n# Labels and sample submission\nlabels = pd.read_csv(f'{BASE_DIR}\/train_labels.csv')\nss = pd.read_csv(f'{BASE_DIR}\/sample_submission.csv')\n\n# Player tracking data\ntr_tracking = pd.read_csv(f'{BASE_DIR}\/train_player_tracking.csv')\nte_tracking = pd.read_csv(f'{BASE_DIR}\/test_player_tracking.csv')\n\n# Baseline helmet detection labels\ntr_helmets = pd.read_csv(f'{BASE_DIR}\/train_baseline_helmets.csv')\nte_helmets = pd.read_csv(f'{BASE_DIR}\/test_baseline_helmets.csv')\n\n# Extra image labels\nimg_labels = pd.read_csv(f'{BASE_DIR}\/image_labels.csv')","f0071162":"##https:\/\/www.kaggle.com\/robikscube\/nfl-helmet-assignment-getting-started-guide\n\ndef add_track_features(tracks, fps=59.94, snap_frame=10):\n    \"\"\"\n    Add column features helpful for syncing with video data.\n    \"\"\"\n    tracks = tracks.copy()\n    tracks[\"game_play\"] = (\n        tracks[\"gameKey\"].astype(\"str\")\n        + \"_\"\n        + tracks[\"playID\"].astype(\"str\").str.zfill(6)\n    )\n    tracks[\"time\"] = pd.to_datetime(tracks[\"time\"])\n    snap_dict = (\n        tracks.query('event == \"ball_snap\"')\n        .groupby(\"game_play\")[\"time\"]\n        .first()\n        .to_dict()\n    )\n    tracks[\"snap\"] = tracks[\"game_play\"].map(snap_dict)\n    tracks[\"isSnap\"] = tracks[\"snap\"] == tracks[\"time\"]\n    tracks[\"team\"] = tracks[\"player\"].str[0].replace(\"H\", \"Home\").replace(\"V\", \"Away\")\n    tracks[\"snap_offset\"] = (tracks[\"time\"] - tracks[\"snap\"]).astype(\n        \"timedelta64[ms]\"\n    ) \/ 1_000\n    # Estimated video frame\n    tracks[\"est_frame\"] = (\n        ((tracks[\"snap_offset\"] * fps) + snap_frame).round().astype(\"int\")\n    )\n    return tracks\n\n\ntr_tracking = add_track_features(tr_tracking)\nte_tracking = add_track_features(te_tracking)\n","206c113a":"def merge_label_and_tracking(tracking_df, label_df):\n\n    tracking_with_game_index = tracking_df.set_index([\"gameKey\", \"playID\", \"player\"])\n\n    df_list = []\n\n    for key, _label_df in tqdm(label_df.groupby([\"gameKey\", \"playID\", \"view\", \"label\"])):\n        # skip because there are sideline player\n        if key[3] == \"H00\" or key[3] == \"V00\":\n            continue\n\n        tracking_data = tracking_with_game_index.loc[(key[0], key[1], key[3])]\n        _label_df = _label_df.sort_values(\"frame\")\n\n        # merge with frame and est_frame\n        merged_df = pd.merge_asof(\n            _label_df,\n            tracking_data,\n            left_on=\"frame\",\n            right_on=\"est_frame\",\n            direction='nearest',\n        )\n        df_list.append(merged_df)\n\n    all_merged_df = pd.concat(df_list)\n    all_merged_df = all_merged_df.sort_values([\"video_frame\", \"label\"], ignore_index=True)\n    \n    return all_merged_df","ddbb5907":"merged_df = merge_label_and_tracking(tr_tracking, labels)","145827ee":"unique_gameKeys = merged_df.gameKey.unique()\ncheck_frame = 1\nhomography_df = merged_df[(merged_df.gameKey == unique_gameKeys[0]) & (merged_df.frame == check_frame) & (merged_df.view =='Sideline')].copy()\nhomography_df.head()","0041cc2d":"trakcing_coordinate = np.float32(list(zip(homography_df['x'],53.33-homography_df['y']))).reshape(-1,1,2)\nlabel_coordinate =  np.float32(list(zip(homography_df['left']+homography_df['width']\/2,homography_df['top']-homography_df['height']\/2))).reshape(-1,1,2)","51ab070f":"H, mask = cv2.findHomography(label_coordinate, trakcing_coordinate)\ntransformed_coordinate =  cv2.perspectiveTransform(label_coordinate, H)","ffaf9cf3":"print(H)","338a2d77":"plt.figure(figsize=(12,10))\n\nplt.scatter(transformed_coordinate[:, :, 0],transformed_coordinate[:, :, 1])\n\nplt.scatter(homography_df['x'], 53.33-homography_df['y'])\n\nplt.legend(['Transformed coordinate from Sideline helmet box','Ground truth tracking data'])","39fe0c87":"video_name = homography_df.video.unique()\nvideo_path = f\"{BASE_DIR}\/train\/{video_name[0]}\"\n\nvid = imageio.get_reader(video_path, 'ffmpeg')\nimg = vid.get_data(check_frame - 1)\nplt.figure(figsize=(12, 10))\nplt.imshow(img)","56e68087":"line_numbers = [[110, 600],  ## Home Sideline 20\n                [550, 630],  ## Home Sideline 30\n                [990, 680],  ## Home Sideline 40\n                [1150, 200], ## Victory Sideline 40\n                [770, 200]]  ## Victory Sideline 30\nfor line_number in line_numbers:\n    img = cv2.circle(img, (line_number[0],line_number[1]), radius=2, color=(0, 255, 255), thickness=10)\n\nplt.figure(figsize=(12, 10))\nplt.imshow(img)    ","31291b83":"projection_numbers = [[30, 53.3-10], ## Home Sideline 20\n                      [40, 53.3-10], ## Home Sideline 30\n                      [50, 53.3-10], ## Home Sideline 40\n                      [50, 10],      ## Victory Sideline 40\n                      [40, 10]]      ## Victory Sideline 30","0406e480":"H, mask = cv2.findHomography(np.float32(line_numbers).reshape(5, 2), np.float32(projection_numbers).reshape(5, 2))","98b60bf5":"print(H)","58c9e9a0":"transformed_coordinate =  cv2.perspectiveTransform(label_coordinate, H)","608bcc89":"plt.figure(figsize=(12,10))\n\nplt.scatter(transformed_coordinate[:, :, 0],transformed_coordinate[:, :, 1])\n\nplt.scatter(homography_df['x'], 53.33-homography_df['y'])\n\nplt.legend(['Transformed coordinate from Sideline helmet box','Ground truth tracking data'])","533f2402":"## ","68cbbfe7":"In this notebook I will share one idea to merging traking data with sideline helmet label information.\n\nMain approch is that if we can find specific 4 pair points(cx, cy) which is matching with `tracking images` & `side or endline images`, we can find homography `H` for Perspective Transformation.\n\nin this notebook I'll use field line numbers to find homography `H` between `tracking images` and `sideline images`\n\nReference: \n- https:\/\/www.kaggle.com\/robikscube\/nfl-helmet-assignment-getting-started-guide\n- https:\/\/www.kaggle.com\/c\/nfl-health-and-safety-helmet-assignment\/discussion\/264361#1467283\n- https:\/\/www.kaggle.com\/coldfir3\/camera-tracking-matching-with-gradient-descent\n- https:\/\/www.kaggle.com\/go5kuramubon\/merge-label-and-tracking-data","3b9c1a1f":"# Prepare","1053f0b6":"## Next to \n- Matching label using homography information\n- Build filed number detection model ? \n- Merging with MOT models like deepsort, FairMOT","d9fdf990":"But important thing is that we can't match each keypoints exactly becaues we don't have a enough information to find homography.\n\nIf we can match specific pair points with tracking images, side & endzone images, we might find good homography.\n\nI'll use filed line numbers to match both images","a5c3ad83":"## Finding filed line number points in sideline","58900a68":"## Finding filed line number points in tracking data\n![images](https:\/\/drive.google.com\/uc?export=view&id=1IdUQHo9G673ifp-mIrwiG_ep0q88H13N)\n\nIf we treat tracking `x`, `y`  as pair points in this images, we can guess that Finding filed line number points.","3a54797b":"## Video to Frame","8b81eb63":"## PerspectiveTransform\n\nIf we know matched Keypoints in the images, we can find homography `H` using `cv2.findHomography`. \n\nbelow code show how we can transform sideline helmet boxes to tracking data scale."}}