{"cell_type":{"6725e8c8":"code","37cbb392":"code","d4a1ec60":"code","0e02ecca":"code","5ce2e0d1":"code","cdffbeae":"code","a1dd7d83":"code","71660546":"code","4f1ffbd6":"code","ee9e89ca":"code","0336a99b":"code","9ac9b7b1":"code","11fced5d":"code","c68d65c1":"code","905f8723":"code","8b27c6e6":"code","7cc7ae10":"code","18ec8eb6":"code","04cc0d64":"code","f401da04":"code","9c18be2d":"code","b3126f05":"code","65550ba0":"code","2476b6d0":"code","579cb050":"code","35be17e4":"code","366dfe25":"code","a65d781b":"code","301ac96b":"code","751b26bc":"code","ddca9c23":"code","8c3a0a4b":"code","43266b7c":"code","ad15e29a":"code","5628a9d5":"code","07ba96cf":"code","68e6dd05":"code","3ce1aea5":"code","698c7eea":"code","6ba28f4c":"code","11a4d836":"code","e4c0b79a":"code","398f186e":"code","bb02deb5":"code","82ee42d9":"code","12fd15f1":"code","177e3a55":"code","6a10ed31":"code","9209db4d":"code","c24f5c3b":"code","b2938707":"code","c5d5ed0e":"code","9b670baa":"code","e2dae899":"code","1c482cd1":"code","3fad9b84":"code","598b432a":"code","695ec415":"code","08e49382":"code","24b7a182":"code","785605e2":"code","cafc66fa":"code","40720805":"code","242b69c0":"code","6934880e":"code","3b02bbca":"code","5f96d6a4":"code","88188719":"code","a323e762":"code","c949d3e8":"markdown","172b028e":"markdown","a74d829e":"markdown","978c1b4e":"markdown","426865dd":"markdown","2f23f9f8":"markdown","4f7882c5":"markdown","13c057f1":"markdown","9f240fea":"markdown","005df052":"markdown","2e41918f":"markdown","d31be720":"markdown","174de3a2":"markdown","89f62c0d":"markdown","038d8051":"markdown","c17c9e74":"markdown","b15c42e3":"markdown","f7e55837":"markdown","f902163b":"markdown","cffd3622":"markdown","186e27b6":"markdown","5f855fdf":"markdown","bd5dc389":"markdown"},"source":{"6725e8c8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","37cbb392":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV\nfrom sklearn.metrics import accuracy_score, roc_auc_score\n\nfrom xgboost import XGBClassifier","d4a1ec60":"import warnings\nwarnings.filterwarnings('ignore')","0e02ecca":"df_train =  pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv')","5ce2e0d1":"df_train.head()","cdffbeae":"df_train.info()","a1dd7d83":"print('Dataset shape: ', df_train.shape )","71660546":"df_train.describe()","4f1ffbd6":"df_train.isna().sum()","ee9e89ca":"miss_perc = df_train.isnull().sum()\/df_train.shape[0] * 100\nmiss_perc = miss_perc.sort_values(ascending=False)\nplt.rc('font', family='serif', size=16)\nplt.figure(figsize=(15,35))\nplt.title('Percentage of missing values on each variable')\nplt.xlabel('Percentage (%)')\nplt.ylabel('Feature')\nplt.barh(miss_perc.index, miss_perc.round(2), alpha=0.5)\n\n# for column in df_train.columns :\n#   print(column + ' ' + '%.2f' % miss_perc[column] + '%')","0336a99b":"# plt.figure(figsize=(6,4))\n# plt.hist(df_train['claim'], bins=2, color='#3498db', histtype='bar', edgecolor='white') \nsns.countplot(df_train['claim'])\nplt.title('Distribution of classes in target variable (claim) \\n')\nplt.xlabel('Claim')\nplt.ylabel('Count')","9ac9b7b1":"x = df_train.copy().drop('id', axis=1)\nx.dropna(inplace=True)\ny = x.pop('claim')","11fced5d":"discrete_features = x.dtypes == int","c68d65c1":"def make_mi_scores(x, y, discrete_features):\n    mi_scores = mutual_info_classif(x, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=x.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores","905f8723":"def plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")","8b27c6e6":"mi_scores = make_mi_scores(x, y, discrete_features)\nmi_scores[::3]  # show a few features with their MI scores","7cc7ae10":"plt.figure(dpi=100, figsize=(15, 35))\nplot_mi_scores(mi_scores)","18ec8eb6":"print('There are ', (mi_scores == 0).value_counts()[True], ' features with 0 score out of 118 features.')","04cc0d64":"def evaluate_metrics(model, x, y):\n    y_pred = model.predict(x)\n    acc = accuracy_score(y, y_pred)\n    y_pred_prob = model.predict_proba(x)[:, 1]\n    auc_roc = roc_auc_score(y, y_pred_prob)\n    return {'accuracy' : acc, 'auc_roc_curve' : auc_roc}","f401da04":"df_subset = df_train.sample(frac=.20, random_state=42)\nx = df_subset.drop(['id', 'claim'], axis=1)\ny = df_subset['claim']\n\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\nprint(\"Using 80% of data for training and 20% for testing the baseline models\")\nprint('x_train', x_train.shape, 'y_train', y_train.shape)\nprint('x_valid', x_valid.shape, 'y_valid', y_valid.shape)\n\n# Impute and Scale the values\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nx_train = imputer.fit_transform(x_train)\nx_valid = imputer.transform(x_valid)\n\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_valid = scaler.transform(x_valid)","9c18be2d":"# Logisct Classifier\nmodel = LogisticRegression(random_state=0)\nmodel.fit(x_train, y_train)\nmodel.score(x_valid, y_valid)\nresults = evaluate_metrics(model, x_valid, y_valid)\n# y_pred_prob = model.predict_proba(x_valid)[:, 1]\n# auc_roc = roc_auc_score(y_pred_prob, y_valid)\nprint(results)","b3126f05":"y_pred = model.predict(x_valid)\naccuracy_score(y_valid, y_pred)","65550ba0":"# XGBoost Classifier\nmodel = XGBClassifier(random_state=0, tree_method='gpu_hist', predictor='gpu_predictor', verbosity=0)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_valid)\n# accuracy_score(y_valid, y_pred)\nresults = evaluate_metrics(model, x_valid, y_valid)\nprint(results)","2476b6d0":"# XGBoost Classifier - 10 estimators\nmodel = XGBClassifier(n_estimators=10, random_state=0, tree_method='gpu_hist', predictor='gpu_predictor', verbosity=0)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_valid)\nresults = evaluate_metrics(model, x_valid, y_valid)\nprint(results)","579cb050":"df_subset = df_train.sample(frac=.20, random_state=42)\ndf_subset = df_subset.drop(mi_scores[mi_scores == 0].index, axis=1)\n\nx = df_subset.drop(['id', 'claim'], axis=1)\ny = df_subset['claim']\n\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\nprint(\"Using 80% of data for training and 20% for testing the baseline models\")\nprint('x_train', x_train.shape, 'y_train', y_train.shape)\nprint('x_valid', x_valid.shape, 'y_valid', y_valid.shape)\n\n# Impute and Scale the values\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nx_train = imputer.fit_transform(x_train)\nx_valid = imputer.transform(x_valid)\n\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_valid = scaler.transform(x_valid)","35be17e4":"# Logisct Classifier\nmodel = LogisticRegression(random_state=0)\nmodel.fit(x_train, y_train)\nresults = evaluate_metrics(model, x_valid, y_valid)\nprint(results)","366dfe25":"# XGBoost Classifier\nmodel = XGBClassifier(random_state=0, tree_method='gpu_hist', predictor='gpu_predictor', verbosity=0)\nmodel.fit(x_train, y_train)\nresults = evaluate_metrics(model, x_valid, y_valid)\nprint(results)\n\n# y_pred = model.predict(x_valid)\n# predictions = [round(value) for value in y_pred]\n# accuracy_score(y_valid, predictions)","a65d781b":"# XGBoost Classifier - 10 estimators\nmodel = XGBClassifier(n_estimators=10, random_state=0, tree_method='gpu_hist', predictor='gpu_predictor', verbosity=0)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_valid)\nresults = evaluate_metrics(model, x_valid, y_valid)\nprint(results)","301ac96b":"df_subset = df_train.sample(frac=.20, random_state=42)\ndf_subset = df_subset.dropna()\n\nx = df_subset.drop(['id', 'claim'], axis=1)\ny = df_subset['claim']\n\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\nprint(\"Using 80% of data for training and 20% for testing the baseline models\")\nprint('x_train', x_train.shape, 'y_train', y_train.shape)\nprint('x_valid', x_valid.shape, 'y_valid', y_valid.shape)\n\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_valid = scaler.transform(x_valid)","751b26bc":"# Logisct Classifier\nmodel = LogisticRegression(random_state=0)\nmodel.fit(x_train, y_train)\nresults = evaluate_metrics(model, x_valid, y_valid)\nprint(results)","ddca9c23":"# XGBoost Classifier\nmodel = XGBClassifier(random_state=0, tree_method='gpu_hist', predictor='gpu_predictor', verbosity=0)\nmodel.fit(x_train, y_train)\nresults = evaluate_metrics(model, x_valid, y_valid)\nprint(results)","8c3a0a4b":"# XGBoost Classifier - 10 estimators\nmodel = XGBClassifier(n_estimators=10, random_state=0, tree_method='gpu_hist', predictor='gpu_predictor', verbosity=0)\nmodel.fit(x_train, y_train)\nresults = evaluate_metrics(model, x_valid, y_valid)\nprint(results)","43266b7c":"df_subset = df_train.sample(frac=.20, random_state=42)\ndf_subset = df_subset.drop(mi_scores[mi_scores == 0].index, axis=1)\ndf_subset = df_subset.dropna()\n\nx = df_subset.drop(['id', 'claim'], axis=1)\ny = df_subset['claim']\n\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\nprint(\"Using 80% of data for training and 20% for testing the baseline models\")\nprint('x_train', x_train.shape, 'y_train', y_train.shape)\nprint('x_valid', x_valid.shape, 'y_valid', y_valid.shape)\n\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_valid = scaler.transform(x_valid)","ad15e29a":"# Logistic Classifier\nmodel = LogisticRegression(random_state=0)\nmodel.fit(x_train, y_train)\nresults = evaluate_metrics(model, x_valid, y_valid)\nprint(results)","5628a9d5":"# XGBoost Classifier\nmodel = XGBClassifier(random_state=0, tree_method='gpu_hist', predictor='gpu_predictor', verbosity=0)\nmodel.fit(x_train, y_train)\nresults = evaluate_metrics(model, x_valid, y_valid)\nprint(results)","07ba96cf":"# XGBoost Classifier\nmodel = XGBClassifier(n_estimators=10, random_state=0, tree_method='gpu_hist', predictor='gpu_predictor', verbosity=0)\nmodel.fit(x_train, y_train)\nresults = evaluate_metrics(model, x_valid, y_valid)\nprint(results)","68e6dd05":"df_subset = df_train.sample(frac=.20, random_state=42)\nx = df_subset.drop(['id', 'claim'], axis=1)\nx[\"nan_count\"] = x.isnull().sum(axis=1)\ny = df_subset['claim']\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\nprint(\"Using 80% of data for training and 20% for testing the baseline models\")\nprint('x_train', x_train.shape, 'y_train', y_train.shape)\nprint('x_valid', x_valid.shape, 'y_valid', y_valid.shape)\n\n# Impute and Scale the values\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nx_train = imputer.fit_transform(x_train)\nx_valid = imputer.transform(x_valid)\n\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_valid = scaler.transform(x_valid)","3ce1aea5":"# Logistic Classifier\nmodel = LogisticRegression(random_state=0)\nmodel.fit(x_train, y_train)\nresults = evaluate_metrics(model, x_valid, y_valid)\nprint(results)","698c7eea":"# XGBoost Classifier\nmodel = XGBClassifier(random_state=0, tree_method='gpu_hist', predictor='gpu_predictor', verbosity=0)\nmodel.fit(x_train, y_train)\nresults = evaluate_metrics(model, x_valid, y_valid)\nprint(results)","6ba28f4c":"# XGBoost Classifier\nmodel = XGBClassifier(n_estimators=10, random_state=0, tree_method='gpu_hist', predictor='gpu_predictor', verbosity=0)\nmodel.fit(x_train, y_train)\nresults = evaluate_metrics(model, x_valid, y_valid)\nprint(results)","11a4d836":"df_subset = df_train.sample(frac=.20, random_state=42)\ndf_subset = df_subset.drop(mi_scores[mi_scores == 0].index, axis=1)\n\nx = df_subset.drop(['id', 'claim'], axis=1)\nx[\"nan_count\"] = x.isnull().sum(axis=1)\ny = df_subset['claim']\n\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\nprint(\"Using 80% of data for training and 20% for testing the baseline models\")\nprint('x_train', x_train.shape, 'y_train', y_train.shape)\nprint('x_valid', x_valid.shape, 'y_valid', y_valid.shape)\n\n# Impute and Scale the values\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nx_train = imputer.fit_transform(x_train)\nx_valid = imputer.transform(x_valid)\n\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_valid = scaler.transform(x_valid)","e4c0b79a":"# Logistic Classifier\nmodel = LogisticRegression(random_state=0)\nmodel.fit(x_train, y_train)\nresults = evaluate_metrics(model, x_valid, y_valid)\nprint(results)","398f186e":"# XGBoost Classifier\nmodel = XGBClassifier(random_state=0, tree_method='gpu_hist', predictor='gpu_predictor', verbosity=0)\nmodel.fit(x_train, y_train)\nresults = evaluate_metrics(model, x_valid, y_valid)\nprint(results)","bb02deb5":"# XGBoost Classifier\nmodel = XGBClassifier(n_estimators=10, random_state=0, tree_method='gpu_hist', predictor='gpu_predictor', verbosity=0)\nmodel.fit(x_train, y_train)\nresults = evaluate_metrics(model, x_valid, y_valid)\nprint(results)","82ee42d9":"from tqdm.notebook import tqdm","12fd15f1":"df_subset = df_train.sample(frac=.20, random_state=42)\nx = df_subset.drop(['id', 'claim'], axis=1)\nx[\"nan_count\"] = x.isnull().sum(axis=1)\ny = df_subset['claim']\n\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\nprint(\"Using 80% of data for training and 20% for testing the baseline models\")\nprint('x_train', x_train.shape, 'y_train', y_train.shape)\nprint('x_valid', x_valid.shape, 'y_valid', y_valid.shape)\n\n# Impute and Scale the values\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nx_train = imputer.fit_transform(x_train)\nx_valid = imputer.transform(x_valid)\n\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_valid = scaler.transform(x_valid)","177e3a55":"import warnings\nwarnings.filterwarnings('ignore')","6a10ed31":"def get_models_n_estimators():\n    models = dict()\n    trees = [10, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000]\n    for n in trees:\n        models[str(n)] = XGBClassifier(n_estimators=n, tree_method='gpu_hist', predictor='gpu_predictor', verbosity=0)\n    return models","9209db4d":"models = get_models_n_estimators()\nresults, names = list(), list()\ni = 0\n\nfor name, model in tqdm(models.items()):\n    model.fit(x_train, y_train, verbose=True)\n    scores = evaluate_metrics(model, x_valid, y_valid)\n    results.append(scores)\n    names.append(name)\n    print(name, 'accuracy: %.3f auc_roc: %.3f' % (results[i]['accuracy'], results[i]['auc_roc_curve']))\n    i += 1","c24f5c3b":"def get_models_n_depths():\n    models = dict()\n    for i in range(1,20):\n        models[str(i)] = XGBClassifier(max_depth=i, tree_method='gpu_hist', predictor='gpu_predictor', verbosity=0)\n    return models","b2938707":"models = get_models_n_depths()\nresults, names = list(), list()\ni = 0\n\nfor name, model in tqdm(models.items()):\n    model.fit(x_train, y_train, verbose=True)\n    scores = evaluate_metrics(model, x_valid, y_valid)\n    results.append(scores)\n    names.append(name)\n    print(name, 'accuracy: %.3f auc_roc: %.3f' % (results[i]['accuracy'], results[i]['auc_roc_curve']))\n    i += 1","c5d5ed0e":"def get_models_subsamples():\n    models = dict()\n    for i in np.arange(0.1, 1.1, 0.1):\n        key = '%.1f' % i\n        models[key] = XGBClassifier(subsample=i,  tree_method='gpu_hist', predictor='gpu_predictor', verbosity=0)\n    return models","9b670baa":"models = get_models_subsamples()\nresults, names = list(), list()\ni = 0\n\nfor name, model in tqdm(models.items()):\n    model.fit(x_train, y_train, verbose=True)\n    scores = evaluate_metrics(model, x_valid, y_valid)\n    results.append(scores)\n    names.append(name)\n    print(name, 'accuracy: %.3f auc_roc: %.3f' % (results[i]['accuracy'], results[i]['auc_roc_curve']))\n    i += 1","e2dae899":"def get_models_lr():\n    models = dict()\n    rates = [0.0001, 0.001, 0.003, 0.005, 0.01, 0.03, 0.05, 0.1, 0.12, 0.13, 0.3, 0.5, 1.0]\n    for r in rates:\n        key = '%.4f' % r\n        models[key] = XGBClassifier(eta=r, tree_method='gpu_hist', predictor='gpu_predictor', verbosity=0)\n    return models","1c482cd1":"models = get_models_lr()\nresults, names = list(), list()\ni = 0\n\nfor name, model in tqdm(models.items()):\n    model.fit(x_train, y_train, verbose=True)\n    scores = evaluate_metrics(model, x_valid, y_valid)\n    results.append(scores)\n    names.append(name)\n    print(name, 'accuracy: %.3f auc_roc: %.3f' % (results[i]['accuracy'], results[i]['auc_roc_curve']))\n    i += 1","3fad9b84":"def get_models_nfeatures():\n    models = dict()\n    for i in np.arange(0.1, 1.1, 0.1):\n        key = '%.1f' % i\n        models[key] = XGBClassifier(colsample_bytree=i, tree_method='gpu_hist', predictor='gpu_predictor', verbosity=0)\n    return models","598b432a":"models = get_models_nfeatures()\nresults, names = list(), list()\ni = 0\n\nfor name, model in tqdm(models.items()):\n    model.fit(x_train, y_train, verbose=True)\n    scores = evaluate_metrics(model, x_valid, y_valid)\n    results.append(scores)\n    names.append(name)\n    print(name, 'accuracy: %.3f auc_roc: %.3f' % (results[i]['accuracy'], results[i]['auc_roc_curve']))\n    i += 1","695ec415":"df_subset = df_train.sample(frac=.20, random_state=42)\nx_train = df_subset.drop(['id', 'claim'], axis=1)\ny_train = df_subset['claim']\nx_train[\"nan_count\"] = x_train.isnull().sum(axis=1)\n\nprint(\"Using 80% of data for training and 20% for testing the baseline models\")\nprint('x_train', x_train.shape, 'y_train', y_train.shape)\nprint('x_valid', x_valid.shape, 'y_valid', y_valid.shape)\n\n# Impute and Scale the values\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nx_train = imputer.fit_transform(x_train)\n\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)","08e49382":"params = {'n_estimators' : [10, 50, 100, 150, 200],\n          'max_depth' : [1,2,3],\n          'subsample' : [0.8, 0.9, 1.0],\n          'eta' : [0.12, 0.13],\n          'colsample_bytree' : [0.1, 0.2]\n         }","24b7a182":"metrics = ['roc_auc']\ngrid_cv = GridSearchCV(XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor', verbosity=0), param_grid=params, scoring=metrics, verbose=1, refit='roc_auc', return_train_score=False, n_jobs=-1, cv=3)","785605e2":"result = grid_cv.fit(x_train, y_train)","cafc66fa":"grid_cv.best_params_","40720805":"print(grid_cv.best_score_)\nprint(grid_cv.best_params_)","242b69c0":"x_train = df_train.drop(['id', 'claim'], axis=1)\nx_train[\"nan_count\"] = x_train.isnull().sum(axis=1)\ny_train = df_train['claim']\n\nprint(\"Using all data for training and submiting\")\nprint('x_train', x_train.shape, 'y_train', y_train.shape)\nprint('x_valid', x_valid.shape, 'y_valid', y_valid.shape)\n\n# Impute and Scale the values\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nx_train = imputer.fit_transform(x_train)\n\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)","6934880e":"# model = XGBClassifier(n_estimators=250, eta=0.13, max_depth=5, subsample=0.9, colsample_bytree=0.1, tree_method='gpu_hist', predictor='gpu_predictor')\nmodel = XGBClassifier(**grid_cv.best_params_, tree_method='gpu_hist', predictor='gpu_predictor')\nmodel.fit(x_train, y_train)","3b02bbca":"df_test =  pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv')\nx_test = df_test.drop(['id'], axis=1)\nx_test[\"nan_count\"] = x_test.isnull().sum(axis=1)\n\nprint(\"Using test data for predict and submiting\")\nprint('x_test', x_test.shape)\n\n# Impute and Scale the values\nx_test = imputer.transform(x_test)\nx_test = scaler.transform(x_test)","5f96d6a4":"claim = model.predict(x_test)\nids = df_test['id'].values\nsubmission = pd.DataFrame({'id' : ids, 'claim' : claim})","88188719":"submission.head()","a323e762":"submission.to_csv('submission.csv', index=False)","c949d3e8":"# Baseline\n\nLets use to baseline models to choose the methods that will allow the model to achieve a good performance.\n\n1 - Logistic Regression\n\n2 - XGBoost\n\nAll the tests will be based on a fraction of 20% of all available data","172b028e":"# Mutual Information","a74d829e":"# Strategy 4\n\n<ul>\n    <li><h3>Drop rows with nans<\/h3><\/li>\n    <li><h3>Use features whose mutual information is bigger than 0<\/li>\n<\/ul>","978c1b4e":"Lets do some statistics on missing values","426865dd":"# Strategy 1\n\n<ul>\n    <li><h3>Impute values with mean<\/h3><\/li>\n    <li><h3>Use all features<\/h3><\/li>\n<\/ul>","2f23f9f8":"# Strategy 6\n\n<ul>\n    <li><h3>Use strategy 2<\/h3><\/li>\n    <li><h3>Create a sintetic feature. Counting the number of nan values.<\/ul>","4f7882c5":"# 2 - Testing different max_depth ","13c057f1":"# Feature Selection\n\nSince we have a lot of features, it will be too much work to analyse one of them at once alone. Thus, lets use some method to help analysing those features that are most important to our analyses.","9f240fea":"# Strategy 2\n\n<ul>\n    <li><h3>Impute values with mean<\/h3><\/li>\n    <li><h3>Use features whose mutual information is bigger than 0<\/h3><\/li>\n<\/ul>","005df052":"# 4 - Testing different learning rates","2e41918f":"# Strategy 3\n\n<ul>\n    <li><h3>Drop rows with nans<\/h3><\/li>\n    <li><h3>Use all features<\/h3><\/li>\n<\/ul>","d31be720":"# 3 - Testing different subsamples","174de3a2":"# Strategy 5\n\n<ul>\n    <li><h3>Use strategy 1<\/h3><\/li>\n    <li><h3>Create a sintetic feature. Counting the number of nan values. This was as tip from the discussion board<\/ul>","89f62c0d":"# Extreme Gradient Boosting (XGBoost) - Testing different configurations","038d8051":"As can be seen, all featuares have almost the same quantity of missing values. The maximum percentage of missing values does not surpass the 1.6% of the overall data available. Given the quiantity of data available, during the modeling, we will stick with the strategy of drop the rows with missing values, however we will also impute with mean value as a baseline.","c17c9e74":"As we can see, they are equally distributed, thus we won't need to do any kind of special treatment to deal with imbalance.","b15c42e3":"# Fit model on entire dataset for submission\n\nLets fit the model on full dataset and submit it","f7e55837":"Now lets verify how is the target variable distributed","f902163b":"Lets get some basic information about the dataset.\n\n1 - Look at some samples with head function;\n\n2 - Check how much rows does it has and also the data type of each of them;\n\n3 - Look at some central tendency metrics.","cffd3622":"# 5 - Testing different number of features","186e27b6":"# Exploratory Data Analyses - EDA\n\nAt first, lets do some EDA to get more acquainted with the dataset","5f855fdf":"# 1 - Testing different number of estimators ","bd5dc389":"# Grid Search \n\nLets do a Grid search on some of the best parameters obtained above"}}