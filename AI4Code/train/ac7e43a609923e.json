{"cell_type":{"87af93f4":"code","60dff49f":"code","6ff66987":"code","ddb54e48":"code","1d2713ed":"code","13b2a9b0":"code","18774df6":"code","a3cb9b20":"code","6cb6bab6":"code","78c970c0":"code","1a1f3311":"code","4dfc2123":"code","e0667ca5":"code","07826ac9":"code","a0eb2829":"code","b62475d0":"code","0a922d43":"code","11f6042f":"code","c923129f":"code","0801f01f":"code","ce0b616a":"code","f4bd5215":"code","370800fb":"code","2e76f16e":"code","91b9d9e0":"code","703f95dc":"markdown","c550305b":"markdown","0b1a4713":"markdown","20dfcc5b":"markdown","adf0b622":"markdown","7acb969f":"markdown","b1f4071b":"markdown","109223a1":"markdown","f4b48410":"markdown","f42b16f0":"markdown","013b19ee":"markdown","7cbcfa68":"markdown","38f69984":"markdown","7b790c5b":"markdown","07f45a4d":"markdown","d551f7da":"markdown"},"source":{"87af93f4":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom time import time\nimport matplotlib.pyplot as plt\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","60dff49f":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\n# keeping copy of original datasets \n# 1. Train data  -  Has all the features along with target variable. \n# 2. Test data - Similar to training data , but it does not have target variable. \ntrain_original = train_df.copy()\ntest_original = test_df.copy()\n\ntrain_df.head()\n\n","6ff66987":"train_df['Cabin'].value_counts().head()","ddb54e48":"test_df['Cabin'].value_counts().head()","1d2713ed":"# ckecking structure of training dataset\n\ntrain_df.columns","13b2a9b0":"# checking structure of testing dataset \n\ntest_df.columns","18774df6":"train_df['Survived'].value_counts()","a3cb9b20":"# we will now normalize this variable \ntrain_df['Survived'].value_counts(normalize = True)\n","6cb6bab6":"plt.figure(1)\nplt.subplot(221)\ntrain_df['Sex'].value_counts(normalize=True).plot.bar(figsize=(20,10), title= 'Gender')\n\nplt.subplot(222)\ntrain_df['Embarked'].value_counts(normalize=True).plot.bar(title= 'Embarked')\n\nplt.subplot(223)\ntrain_df['Pclass'].value_counts(normalize=True).plot.bar(title= 'Passenger_Class')\n\nplt.subplot(224)\ntrain_df['SibSp'].value_counts(normalize=True).plot.bar(figsize=(24,6), title= 'Sibling_Spouse')\n\n\n\nplt.show()","78c970c0":"Gender=pd.crosstab(train_df['Sex'],train_df['Survived'])\nGender.div(Gender.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4))","1a1f3311":"p_class=pd.crosstab(train_df['Pclass'],train_df['Survived'])\nDependents=pd.crosstab(train_df['SibSp'],train_df['Survived'])\nEmbarked=pd.crosstab(train_df['Embarked'],train_df['Survived'])\n\n\np_class.div(p_class.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4))\nplt.show()\n\nDependents.div(Dependents.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\nplt.show()\n\nEmbarked.div(Embarked.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True, figsize=(4,4))\nplt.show()\n\n","4dfc2123":"#Now let us look at the correlation between the variables by heatmap \nmatrix = train_df.corr()\nf, ax = plt.subplots(figsize=(9, 6))\nsns.heatmap(matrix, vmax=.8, square=True, cmap=\"BuPu\");","e0667ca5":"# printing information of all the columns in training and test datasets.\ntrain_df.info()\nprint(\"-----------------Test Info------------------\")\ntest_df.info()","07826ac9":"# to see categorical data\ntrain_df.describe(include=['O'])","a0eb2829":"\ndef delete_features(df):\n    return df.drop(['PassengerId','Ticket','Cabin'], axis=1)\n\ndef fill_value(df):\n    df.Embarked = df.Embarked.fillna(\"S\")\n    #df['Age'] = df.Age.fillna(df.Age.median())\n    df['Age'] = df.groupby(['Sex'],sort=False)['Age'].apply(lambda x: x.fillna(x.median()))\n    df['Fare'] = df.groupby(['Pclass','Embarked'],sort=False)['Fare'].apply(lambda x : x.fillna(x.median()))\n    return df\n\ndef fill_cabin(df):\n    df.Cabin = df['Cabin'].fillna(\"N\")\n    '''Keep only the 1st character where Cabin is alphanumerical.'''\n    df.Cabin = df['Cabin'].apply(lambda c : c[0])\n    return df\n\ndef extract_name_from_title(df):\n    df['Name'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n    return df\n    \n    \ndef family_members(df):\n    # to find if passenger has any family member or not in the ship.\n    df['Family'] = df['SibSp'] + df['Parch'] + 1\n   \n    return df\n\ndef family_size_bin(df):\n    \"\"\" Creating buckets as per the family size - Individual, Small , Medium and large family\"\"\"\n    df.loc[ df['Family'] == 1, 'Family'] = \"Individual\"\n    #df.loc[(df['Family'] > int(1)) & (df['Family'] <= int(2)), 'Family'] = \"Small\"\n    #df.loc[(df['Family'] > 2) & (df['Family'] <= 5), 'Family'] = \"Medium\"\n    #df.loc[(df['Family'] > 5) , 'Family'] =\"Large\"\n    df['Family'].replace(to_replace = [2,3,4], value = 'small', inplace = True)\n    df['Family'].replace(to_replace = [5,6], value = 'medium', inplace = True)\n    df['Family'].replace(to_replace = [7,8,9, 10,11], value = 'large', inplace = True)\n    return df\n    \n    \ndef transform_feature(df):\n    df = delete_features(df)\n    df = fill_value(df)\n   # df = fill_cabin(df)\n    df = extract_name_from_title(df)\n    df = family_members(df)\n    df = family_size_bin(df)\n    return df\n\ntrain_df = transform_feature(train_df)\ntest_df = transform_feature(test_df)\n\ntrain_df.head()","b62475d0":"display(train_df['Name'].value_counts())","0a922d43":"\"\"\" Now we will bin the titles and also try to place  the same titles together \"\"\"\n\ndef replace_title(df):\n    df['Name'] = df['Name'].replace(['Lady','Countess','Capt', 'Col','Don', 'Major','Rev','Sir','Jonkheer','Dona'], 'Special')\n    df['Name'] = df[\"Name\"].replace(['Mlle','Ms','Miss'],'Miss')\n    df['Name'] = df['Name'].replace(['Mrs','Mme'],'Mrs')\n    return df\n\ntrain_df = replace_title(train_df)\ntest_df = replace_title(test_df)\ntrain_df.head()\n    ","11f6042f":"display(train_df['Name'].value_counts())","c923129f":"\nfrom sklearn.preprocessing import MinMaxScaler\n\ndef encoder(df):\n    scaler = MinMaxScaler()\n    numerical = ['Age', 'Fare', 'SibSp','Parch']\n    features_transform = pd.DataFrame(data= df)\n    features_transform[numerical] = scaler.fit_transform(df[numerical])\n    display(features_transform.head(n = 5))\n    return df\n\ntrain_df = encoder(train_df)\ntest_df = encoder(test_df)\n","0801f01f":"def convert_numerical(df):\n    #categorical = df.select_dtypes(exclude=[\"number\"])\n           \n        \n    #df = pd.get_dummies(df, columns=['Sex'], drop_first=True)\n    #df = pd.get_dummies(df, columns=['Embarked'], drop_first=False)\n    #df = pd.get_dummies(df, columns=['Name'], drop_first=False)\n    #df = pd.get_dummies(df, columns=['Family'], drop_first=False)\n    \n    df = pd.get_dummies(df)\n    \n    encoded = list(df.columns)\n    print(\"{} total features after one-hot encoding.\".format(len(encoded)))\n    print(encoded)\n    return df\n    \n\ntrain_df_final = convert_numerical(train_df)\ntest_df_final = convert_numerical(test_df)\n\n\n#print(test_df_final.Cabin_N)","ce0b616a":"# splitting the data into training and testing data set \nfrom sklearn.model_selection import train_test_split\nytest  = train_df_final['Survived']\nxtrain = train_df_final.drop(['Survived'], axis = 1)\n\n\nX_train, X_test, y_train, y_test = train_test_split(xtrain, ytest, test_size=.25, random_state=1)","f4bd5215":"from sklearn.ensemble import RandomForestClassifier\n#from sklearn.linear_model import LogiscticRegression\nfrom sklearn.metrics import make_scorer, accuracy_score,fbeta_score\nfrom sklearn.grid_search import GridSearchCV\n\n\nclf = RandomForestClassifier(random_state = 1)\n\n#creating parameters to fit into algortihm \nparameters = {'n_estimators' : [10, 20, 30,50, 100] , 'max_features' : [0.6, 0.2, 0.3], 'min_samples_leaf' :[1,2,3], \n              'min_samples_split':[2,3,4,6]}\n\n#parameters = {'penalty':['l1', 'l2'],'C': np.logspace(0, 4, 10)}\n\n# calculating accuracy score\nacc_scorer = make_scorer(accuracy_score)\n\n# Running grid search \ngrid_obj = GridSearchCV(clf, parameters,  scoring=acc_scorer, cv = 5)\n\n# Fit the grid search object to the training data and find the optimal parameters\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nbest_clf = grid_obj.best_estimator_\n\n# Fit the best parameter to the data. \nbest_clf.fit(X_train, y_train)\n\n#making predictions \nbest_predictions = best_clf.predict(X_test)\n\n#printing fbeta score and accuracy score of the optimized model . \nprint(\"\\nOptimized Model\\n------\")\nprint(\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\nprint(\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5)))\n","370800fb":"xtrain.head()","2e76f16e":"test_df_final.head()","91b9d9e0":"pred_test = best_clf.predict(test_df_final)\n\nsubmission = pd.read_csv('..\/input\/gender_submission.csv')\nsubmission['Survived']=pred_test\nsubmission['PassengerId']=test_original['PassengerId']\n\n#submission = pd.DataFrame({\n#\"PassengerId\": test[\"PassengerId\"],\n#        \"Survived\": y_pred_rf_tunned})\"\"\"\"\"\"\n#submission.to_csv('submission_rf.csv', index = False)\"''\"\n#converting to csv\n\npd.DataFrame(submission, columns=['PassengerId','Survived']).to_csv('randomforest.csv', index = False)\nprint(submission.head())","703f95dc":"A passenger had greater chance of survival if he belongs to class = 1 and has embarked on 'C' port. ","c550305b":"From the above graph it is observed that female survivals were more as compared to the male survivors\n\nNow let us Visualize the remaining variables","0b1a4713":"**3. Scaling Features:**\nWe should perform scaling on numerical features. Normalization ensures that each feature is treated equally while applyingalgorithms.\n\nWe will be performing min max scaler in numerical features - Age, Fare, SibSp","20dfcc5b":"**1. Problem Statement - **\nThis is a binary classification problem.  In the problem, we need to predict of the passenger will survive or not. \n**\n2. Hypothesis Generation - **\n\nIt involves finding\/thinking of the features which might affect the outcome\/prediction. \n\nHere are some of the factors which I think might affect the survical rate - \n1. Gender : Gender plays an important role in the prediction. \n2. Fare : It might be a factor worth considering. As the passengers who paid higher fare might have a better chance for survival. \n3. PClass - This will also play a role in survival rate. \n4. Age - Small children and senior citizens will have less chance of survival. So it will definitely affect the target variable. \n","adf0b622":"**1. Reading training and testing files.**","7acb969f":"\nWe will drop the target variable form the training data set and will store in in another variable. \n","b1f4071b":"**Correlation **\nWe see that most correlated variables are - (SibSp - Parch ) and  (Survived - FAre)","109223a1":"**4. One-hot encoding**\nThere are several categorical features in the dataset. As the algorithms mainly work on numerical data, we will convert features - sex to numerical values.","f4b48410":"**Filling Missing Values *\n\nWe will need to fill the misisng values . SO we will do the following - \n1. For categorical values - filling with the most occured values(mode)\n2. For numerical values - filling with mean\/median","f42b16f0":"**Implementing algorithm**\nWe will be using RandomForestClassifier. ","013b19ee":"**Predicting the test data**","7cbcfa68":"**Categorical Variable : **\n\n1. Categorical Variables  -  Sex , Cabin, Embarked, Ticket,PClass  ","38f69984":"# Steps followed for problem solving - \n1. Loading the data\n2. Exploratory Data Analysis (EDA)\n3. Missing value treatment\n4. Scaling the features\n5. One-hot encoding the features\n6. Building the model\n\n","7b790c5b":"**Target Variable **\nWe will analyze the target variable. As it is a class variable with two outupts 1 - Survived and 0 - Not survived. We will plot a bar chart for this variable \n","07f45a4d":"**Inference from the plots  - **\n1. 60% passengers in the data set are Male. \n2. 70% passengers have embarked from 'S' port. \n3. More than 50% passengers belong to the PClass = 3\n4. More than 60% of passengers came without their spouse or children. \n","d551f7da":"**Target Variable and it's relation with the categorical variables**"}}