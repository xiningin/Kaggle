{"cell_type":{"202ee8f3":"code","48ec8916":"code","bbb96bea":"code","249c48fc":"code","9f7f52ee":"code","192890e2":"code","72a37010":"code","bd1daf74":"code","6da960b5":"code","a744f23a":"code","630a9fda":"code","0f3ebf43":"code","81e7c186":"code","11c2e4e3":"code","d8b95877":"code","80284eb2":"code","28778ff9":"code","6f9a9e19":"code","59aada75":"code","b354fba5":"code","89b0826a":"code","663e796d":"code","b978d4d4":"code","43c38813":"code","bdde5d2d":"code","55333a15":"code","be02885a":"code","dc0a441c":"code","7afb855d":"code","8c9ffcd5":"code","fbc596a7":"code","de906a2d":"code","c4ebbd91":"code","de51baf3":"code","a6da2503":"code","c59b8ec7":"code","235fbf77":"code","d6b578a4":"code","ec4a42fb":"code","a8fb8236":"code","d9d3b688":"code","123b2d37":"code","1b60542d":"code","36f283cd":"code","d3b555d4":"code","ab6e49c3":"code","ec2cad77":"code","97f6e169":"code","30c74b05":"code","be5c282c":"code","2bcc2c4c":"code","f502013b":"code","b9217aae":"code","1b3fdb44":"code","5e841c0c":"code","73ce4419":"code","febcf9f5":"code","8def1440":"code","4a6de50d":"code","bcc08879":"code","7a96b6ba":"code","b7432965":"code","f6420398":"code","dea0c138":"code","92e6e44a":"code","e6b4678b":"code","d88d206e":"code","b7e0a4c2":"code","60f90498":"code","97fba050":"code","79e0fb64":"code","f95bb8dc":"code","ade563df":"code","3861dec9":"code","73b13e0b":"code","6bb11a91":"code","36f670fa":"code","650b0045":"code","0d955c5c":"code","7e94f9da":"code","12aa0396":"code","0a0e3f33":"code","971a2bed":"code","7af3f5a3":"markdown","b6e92602":"markdown","c369be67":"markdown","428f35d1":"markdown","25096bb4":"markdown","a9ceeeb9":"markdown","fbe45266":"markdown","da0737ca":"markdown","a319e4b8":"markdown","0012c354":"markdown","11c91dcd":"markdown","e85dd51a":"markdown","f01d69dc":"markdown","f8b68ddf":"markdown","638d1e93":"markdown","8c94e785":"markdown","0ca5fcfc":"markdown","829e906b":"markdown","1d95e186":"markdown","a786527d":"markdown","27b61e4e":"markdown","f8fd92f7":"markdown","b091dc9b":"markdown","3f0429fe":"markdown","149bd303":"markdown","d1b9aa20":"markdown","c053572f":"markdown","738257d4":"markdown","e53325b6":"markdown","35aa622c":"markdown","43ddce64":"markdown","9cbdf68f":"markdown","41f6c239":"markdown","2291f5d6":"markdown","68e18418":"markdown","c1305e9d":"markdown","f046020a":"markdown","cd0590db":"markdown","17e666b4":"markdown","0a9ca6a2":"markdown","8f4b4ae6":"markdown","72d25a66":"markdown","5f7488e8":"markdown","07d6e331":"markdown","ac773027":"markdown","93624d92":"markdown","c74f1380":"markdown","fe8ff106":"markdown","205b1f8d":"markdown","9981adb6":"markdown","a88d2395":"markdown","1b8ee3b4":"markdown","1fe61033":"markdown","abcb1012":"markdown","a60a0216":"markdown","832212cc":"markdown","32afc71d":"markdown","f709ea79":"markdown","aeeca6ce":"markdown","81d10529":"markdown","f4798edb":"markdown","df77f2df":"markdown","2bde2d88":"markdown","568052e6":"markdown","f13eca4d":"markdown"},"source":{"202ee8f3":"# Data manipulation\nimport numpy as np \nimport pandas as pd \n\n# Data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno\n\n# Preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Machine learning\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.initializers import Constant\nfrom keras.layers import Dense, Dropout\nfrom keras import backend as K\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.svm import SVC\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor","48ec8916":"train_df=pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df=pd.read_csv('..\/input\/titanic\/test.csv')\ntrain_df.head()","bbb96bea":"train_df.shape","249c48fc":"train_df.drop('PassengerId', axis=1, inplace=True)  \ntest_id = test_df.pop('PassengerId')     #needed for the submission\ntrain_df.head()","9f7f52ee":"train_df.nunique()","192890e2":"train_df.drop(['Name','Ticket'], axis=1, inplace=True)\ntest_df.drop(['Name','Ticket'], axis=1, inplace=True)\ntrain_df","72a37010":"fig, ax = plt.subplots(1, 2, figsize=(20, 5))\nsns.histplot(data=train_df, x='Age', ax=ax[0])\nsns.histplot(data=train_df, x='Fare', ax=ax[1])","bd1daf74":"Survivedproportions = train_df.Survived.value_counts()\/len(train_df)*100.0 #panda Series\nSurvivedproportions","6da960b5":"Survivedproportions.index","a744f23a":"Pclassproportions = train_df.Pclass.value_counts()\/len(train_df)*100.0 \nSexproportions = train_df.Sex.value_counts()\/len(train_df)*100.0 \nSibSpproportions = train_df.SibSp.value_counts()\/len(train_df)*100.0 \nParchproportions = train_df.Parch.value_counts()\/len(train_df)*100.0 \nEmbarkedproportions = train_df.Embarked.value_counts()\/len(train_df)*100.0 \n\nfig, ax = plt.subplots(2,3,figsize=(20,10))\n\nax[0][0].pie(Survivedproportions, labels=Survivedproportions.index, autopct='%1.1f%%', startangle=90)\nax[0][0].set_title('Survival rate \\n 0: Didnt Survive \\n 1: Survived')\n                   \nax[0][1].pie(Pclassproportions, labels=Pclassproportions.index, autopct='%1.1f%%', startangle=90)\nax[0][1].set_title('Ticket class')\n                   \nax[0][2].pie(Sexproportions, labels=Sexproportions.index, autopct='%1.1f%%', startangle=90)\nax[0][2].set_title('Sex')\n\nax[1][0].pie(SibSpproportions, labels=SibSpproportions.index, autopct='%1.1f%%', startangle=90)\nax[1][0].set_title('Siblings \/ Spouses')\n                   \nax[1][1].pie(Parchproportions, labels=Parchproportions.index, autopct='%1.1f%%', startangle=90)\nax[1][1].set_title('Parents \/ children ')\n                   \nax[1][2].pie(Embarkedproportions, labels=Embarkedproportions.index, autopct='%1.1f%%', startangle=90)\nax[1][2].set_title('Port of Embarkation')\n\nplt.show()","630a9fda":"fig, ax = plt.subplots(2, 3, figsize=(15,10))\nsns.barplot(x='Sex', y='Survived',ci = None, data=train_df, ax=ax[0][0])\nsns.barplot(x='Embarked', y='Survived', ci = None, data=train_df, ax=ax[0][1], palette = 'hls')\nsns.barplot(x='Pclass', y='Survived', ci = None, data=train_df, ax=ax[0][2], palette = 'Set2')\nsns.barplot(x='SibSp', y='Survived', ci = None, data=train_df, ax=ax[1][0], palette = 'husl')\nsns.barplot(x='Parch', y='Survived',  ci = None, data=train_df, ax=ax[1][1], palette = 'Paired')","0f3ebf43":"plt.figure(figsize=(10,5))\nsns.barplot(x = 'Sex', y = 'Pclass', ci = None, data = train_df,  palette = 'hls')\nplt.show()","81e7c186":"bins = [0, 10, 20, 30, 40, 50, 60, 70, 100]\nlabels = ['<11','11-20','21-30','31-40','41-50','51-60','61-70','>70']\n#Age_bins=train_df['Age'].fillna(value=train_df['Age'])\nAge_bins = pd.cut(train_df['Age'], bins=bins, labels=labels)","11c2e4e3":"Age_bins","d8b95877":"Age_bins_df=train_df[{'Age','Survived'}].copy()\nAge_bins_df['Age']=Age_bins\nAge_bins_df.rename({'Age': 'Age_bins'}, axis=1, inplace=True)\nAge_bins_df","80284eb2":"sns.barplot(x='Age_bins', y='Survived',  ci = None, data=Age_bins_df)","28778ff9":"missingno.matrix(train_df, figsize = (10,5))","6f9a9e19":"train_df.isnull().any()","59aada75":"column_with_nan = train_df.columns[train_df.isnull().any()]\ncolumn_with_nan","b354fba5":" train_df.isnull().sum()","89b0826a":"test_df.isnull().sum()","663e796d":"columns_to_drop=[]\nfor column in column_with_nan:\n    if train_df[column].isnull().sum()\/len(train_df)*100.0 > 50:\n        columns_to_drop.append(column)\n\nfor column in columns_to_drop:\n    train_df.drop(column, axis=1, inplace=True)\n    test_df.drop(column, axis=1, inplace=True)\n    \ntrain_df.head()","b978d4d4":"train_df.isnull().any(axis=1)","43c38813":"train_df[888:889]","bdde5d2d":"train_df.index[train_df.isnull().any(axis=1)]","55333a15":"plt.figure(figsize = (20,10))\n\ndf_correlation = train_df.copy().dropna()\ndf_correlation['Sex']=df_correlation['Sex'].astype('category').cat.codes\ndf_correlation['Embarked']=df_correlation['Embarked'].astype('category').cat.codes\ncorr=df_correlation.corr()\nsns.heatmap(corr,center=0)\nplt.show()\n\n\ncorr_values=corr['Survived'].sort_values(ascending=False)\ncorr_values=abs(corr_values).sort_values(ascending=False)\nprint(\"Correlation of features with target in ascending order\")\nprint(abs(corr_values).sort_values(ascending=False))","be02885a":"columns = np.full((corr.shape[0],), True, dtype=bool)\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if abs(corr.iloc[i,j]) >= 0.9:\n            if columns[j]:\n                columns[j] = False\nselected_columns = train_df.columns[columns]\ntrain_df = train_df[selected_columns]","dc0a441c":"selected_columns","7afb855d":"scaler = MinMaxScaler()\n\ncolumn_Embarked= train_df['Embarked']   # needed for later\n\ntrain_df[['Age','Fare','SibSp','Parch']] = scaler.fit_transform(train_df[['Age','Fare','SibSp','Parch']]) #we fit the scaler only on the training data\ntest_df[['Age','Fare','SibSp','Parch']] = scaler.transform(test_df[['Age','Fare','SibSp','Parch']])  \n    \ndef transform_data(df):\n    df = pd.get_dummies(df, columns=['Sex'], drop_first=True)\n    df = pd.get_dummies(df, columns=['Pclass','Embarked'])\n    \n    df.rename(columns = {'Age':'Age_norm'}, inplace = True)\n    df.rename(columns = {'Fare':'Fare_norm'}, inplace = True)\n    df.rename(columns = {'SibSp':'SibSp_norm'}, inplace = True)\n    df.rename(columns = {'Parch':'Parch_norm'}, inplace = True)\n    \n    return df\n    \ntrain_df=transform_data(train_df)\ntest_df=transform_data(test_df)","8c9ffcd5":"train_df","fbc596a7":"test_df","de906a2d":"train_df.isnull().sum()","c4ebbd91":"test_df.isnull().sum()","de51baf3":"NaNindexs_Fare_testdf = test_df[test_df.Fare_norm.isnull()].index.tolist()\n# np.where(test_df.Fare_norm.isnull())[0] also works (we get an array instead)\nprint(NaNindexs_Fare_testdf)\ntest_df.iloc[NaNindexs_Fare_testdf]","a6da2503":"test_df['Fare_norm'] = test_df['Fare_norm'] .fillna(test_df.Fare_norm.mean())\ntest_df.iloc[NaNindexs_Fare_testdf]","c59b8ec7":"all_columns_but_age = [c for c in train_df.columns if c != 'Age_norm']\nall_columns_but_age_and_survived = [c for c in test_df.columns if c!= 'Age_norm']  #no survived column on test_df -> can't use it to predict age\n\nX_noage=train_df.loc[train_df.Age_norm.notnull(), all_columns_but_age].values  #training features from train_df for age prediction #.values: to get an array\n\nX_noage_nosurvived = train_df.loc[train_df.Age_norm.notnull(), all_columns_but_age_and_survived].values #same but without survived column\nX_noage_nosurvived_testdf = test_df.loc[test_df.Age_norm.notnull(), all_columns_but_age_and_survived].values #training features from test_df\nX_noage_nosurvived_concat = np.concatenate((X_noage_nosurvived, X_noage_nosurvived_testdf)) #concatenation -> more training samples (only useful for test_df)\n\nY_age = train_df.loc[train_df.Age_norm.notnull(), 'Age_norm'].values  #training age labels from train_df\n\nY_age_testdf = test_df.loc[test_df['Age_norm'].notnull(), 'Age_norm'].values #training age labels from test_df\nY_age_concat = np.concatenate((Y_age, Y_age_testdf))  #concatenation -> more training samples\n\nX_pred_age = train_df.loc[train_df.Age_norm.isnull(), all_columns_but_age].values #features to predict the nan values of age after training\nX_pred_age_testdf = test_df.loc[test_df.Age_norm.isnull(), all_columns_but_age_and_survived].values #same but for test_df","235fbf77":"print(train_df.shape)\nprint(X_noage.shape)\nprint(Y_age.shape)\nprint(X_pred_age.shape)\nprint('\\n')\nprint(test_df.shape)\nprint(X_noage_nosurvived_testdf.shape)\nprint(Y_age_testdf.shape)\nprint(X_pred_age_testdf.shape)\nprint('\\n')\nprint(X_noage_nosurvived_concat.shape)\nprint(Y_age_concat.shape)","d6b578a4":"kfold =  KFold(n_splits=5, shuffle=True, random_state=26)\n\nlist_models = []\nnames = []\nlist_models.append(('Linear Regression', LinearRegression()))\nlist_models.append(('KNN', KNeighborsRegressor(n_neighbors=5)))\nlist_models.append(('Decision Tree', DecisionTreeRegressor(random_state=0)))\nlist_models.append(('SVM', SVR()))\nlist_models.append(('Dummy regressor', DummyRegressor(strategy='mean')))\n\nresults = []\n\nfor name, model in list_models:\n    scores = - cross_val_score(model, X_noage, Y_age, cv=kfold, scoring='neg_mean_squared_error')   # the - signe is to compensate the negative MSE\n    print(\"%s MSE:\" %name , scores)\n    results.append([name, np.mean(scores), np.std(scores)])\n\ndataframe_results = pd.DataFrame(results, columns = ['Regression model', 'mean_MSE', 'std'])\n\ndataframe_results.sort_values(['mean_MSE', 'std'])  ","ec4a42fb":"results = []\n\nfor name, model in list_models:\n    scores = - cross_val_score(model, X_noage_nosurvived_concat, Y_age_concat, cv=kfold, scoring='neg_mean_squared_error')   \n    print(\"%s MSE:\" %name , scores)\n    results.append([name, np.mean(scores), np.std(scores)])\n\ndataframe_results = pd.DataFrame(results, columns = ['Regression model', 'mean_MSE', 'std'])\n\ndataframe_results.sort_values(['mean_MSE', 'std'])","a8fb8236":"alphas = []\nresults = []\nfor i in np.arange(0.5,5.5,0.5):\n    alphas.append(i)\n    \nfor alpha in alphas:\n    ridge = Ridge(alpha=alpha)\n    scores = - cross_val_score(ridge, X_noage_nosurvived_concat, Y_age_concat, cv=kfold, scoring='neg_mean_squared_error')\n    results.append([alpha, np.mean(scores), np.std(scores)])\n\nscores = - cross_val_score(LinearRegression(), X_noage_nosurvived_concat, Y_age_concat, cv=kfold, scoring='neg_mean_squared_error')\nresults.append([\"0 : Linear Regression\", np.mean(scores), np.std(scores)])\n    \ndataframe_results = pd.DataFrame(results, columns = ['alpha', 'mean_MSE', 'std'])\n\ndataframe_results.sort_values(['mean_MSE', 'std'])   ","d9d3b688":"regr = SVR()\nregr.fit(X_noage, Y_age)\npred_age_NaN=regr.predict(X_pred_age)\nprint(pred_age_NaN)","123b2d37":"regr = Ridge(alpha=1.5)\nregr.fit(X_noage_nosurvived_concat, Y_age_concat)\npred_age_NaN_test_df=regr.predict(X_pred_age_testdf)\nprint(pred_age_NaN_test_df)","1b60542d":"train_df.loc[train_df.Age_norm.isnull(), 'Age_norm'] = pred_age_NaN\ntest_df.loc[test_df.Age_norm.isnull(), 'Age_norm'] = pred_age_NaN_test_df\n\nprint(train_df.Age_norm.isnull().any())\nprint(test_df.Age_norm.isnull().any())","36f283cd":"train_df.drop(['Embarked_C', 'Embarked_Q', 'Embarked_S'], axis=1, inplace=True)\ntrain_df['Embarked']=column_Embarked\ntrain_df","d3b555d4":"train_df.Embarked, mapping_index = pd.Series(train_df.Embarked).factorize()\ntrain_df['Embarked'].replace(to_replace=-1, value=np.nan, inplace=True)\n\nprint(mapping_index)\ntrain_df","ab6e49c3":"all_columns_but_embarked = [c for c in train_df.columns if c != 'Embarked']\n\nX_noembarked = train_df.loc[train_df.Embarked.notnull(), all_columns_but_embarked].values #training features for Embarked prediction\n\nY_embarked = train_df.loc[train_df.Embarked.notnull(), 'Embarked'].values  #training Embarked labels\n\nX_pred_embarked = train_df.loc[train_df.Embarked.isnull(), all_columns_but_embarked].values  #features to predict the nan values of Embarked after training","ec2cad77":"print(train_df.shape)\nprint(X_noembarked.shape)\nprint(Y_embarked.shape)\nprint(X_pred_embarked.shape)","97f6e169":"kfold =  KFold(n_splits=5, shuffle=True, random_state=26)\n\nlist_models = []\nnames = []\nlist_models.append(('Logistic regression', LogisticRegression()))\nlist_models.append(('KNN', KNeighborsClassifier(n_neighbors=5)))\nlist_models.append(('Decision Tree',DecisionTreeClassifier(random_state=0)))\nlist_models.append(('SVM', SVC()))\nlist_models.append(('Dummy classifier', DummyClassifier(strategy=\"most_frequent\")))\n\nresults = []\n\nfor name, model in list_models:\n    scores = cross_val_score(model, X_noembarked, Y_embarked, cv=kfold, scoring='accuracy')  \n    print(\"%s accuracy:\" %name , scores)\n    results.append([name, np.mean(scores), np.std(scores)])\n\ndataframe_results = pd.DataFrame(results, columns = ['Classification model', 'mean_accuracy', 'std'])\n\ndataframe_results.sort_values(['mean_accuracy', 'std'], ascending=[False, True])  ","30c74b05":"clf = DecisionTreeClassifier(random_state=0)\nclf.fit(X_noembarked, Y_embarked)\nplt.figure(figsize=(15,7.5))\ntree.plot_tree(clf, filled=True, rounded=True, class_names=mapping_index, feature_names=all_columns_but_embarked)\nplt.show()","be5c282c":"n_nodes = clf.tree_.node_count\nprint(n_nodes)","2bcc2c4c":"clf = DecisionTreeClassifier(random_state=0)\npath = clf.cost_complexity_pruning_path(X_noembarked, Y_embarked) #determine values for alpha\nccp_alphas =  path.ccp_alphas #extract different values for alpha","f502013b":"results = []\n\nfor ccp_alpha in ccp_alphas:\n    clf_dt = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    scores = cross_val_score(clf_dt, X_noembarked, Y_embarked, cv=kfold, scoring='accuracy')\n    results.append([ccp_alpha, np.mean(scores), np.std(scores)])\n    \ndataframe_results = pd.DataFrame(results, columns = ['alpha', 'mean_accuracy', 'std'])\n\ndataframe_results.sort_values(['mean_accuracy', 'std'], ascending=[False, True])   ","b9217aae":"dataframe_results.plot(x='alpha',\n                  y='mean_accuracy',\n                  yerr='std',\n                  marker='o',\n                  linestyle='--',figsize=(20, 4))","1b3fdb44":"accuracy_max = dataframe_results.mean_accuracy.max()\nbest_alpha = dataframe_results.loc[dataframe_results.mean_accuracy==accuracy_max, 'alpha']\nbest_alpha = float(best_alpha)\nprint(best_alpha)","5e841c0c":"clf = DecisionTreeClassifier(random_state=0, ccp_alpha=best_alpha)\nclf.fit(X_noembarked, Y_embarked)\nplt.figure(figsize=(15,7.5))\ntree.plot_tree(clf, filled=True, rounded=True, class_names=mapping_index, feature_names=all_columns_but_embarked)\nplt.show()","73ce4419":"n_nodes = clf.tree_.node_count\nprint(n_nodes)","febcf9f5":"pred_Embarked_NaN=clf.predict(X_pred_embarked)\nprint(pred_Embarked_NaN)","8def1440":"train_df.loc[train_df.Embarked.isnull(), 'Embarked'] = pred_Embarked_NaN\ntrain_df['Embarked'].replace(to_replace=[0,1,2], value=mapping_index, inplace=True)\ntrain_df.Embarked.isnull().any()","4a6de50d":"train_df = pd.get_dummies(train_df, columns=['Embarked'])\ntrain_df","bcc08879":"test_df.loc[NaNindexs_Fare_testdf, 'Fare_norm'] = np.nan #sinon test iloc","7a96b6ba":"test_df.loc[NaNindexs_Fare_testdf]","b7432965":"all_columns_but_fare_and_survived = [c for c in test_df.columns if c!= 'Fare_norm']  \n\nX_nofare_nosurvived = train_df.loc[train_df.Fare_norm.notnull(), all_columns_but_fare_and_survived].values #training features from train_df\nX_nofare_nosurvived_testdf = test_df.loc[test_df.Fare_norm.notnull(), all_columns_but_fare_and_survived].values #training features from test_df\nX_nofare_nosurvived_concat = np.concatenate((X_nofare_nosurvived, X_nofare_nosurvived_testdf)) #concatenation -> more training samples  \n\nY_fare = train_df.loc[train_df.Fare_norm.notnull(), 'Fare_norm'].values  #training age labels from train_df  \n\nY_fare_testdf = test_df.loc[test_df.Fare_norm.notnull(), 'Fare_norm'].values #training age labels from test_df\nY_fare_concat = np.concatenate((Y_fare, Y_fare_testdf))  #concatenation -> more training samples\n\nX_pred_fare_testdf = test_df.loc[test_df.Fare_norm.isnull(), all_columns_but_fare_and_survived].values #features to predict the nan value of fare after training","f6420398":"print(test_df.shape)\nprint(X_nofare_nosurvived_testdf.shape)\nprint(Y_fare_testdf.shape)\nprint(X_pred_fare_testdf.shape)\nprint('\\n')\nprint(X_nofare_nosurvived_concat.shape)\nprint(Y_fare_concat.shape)","dea0c138":"kfold =  KFold(n_splits=5, shuffle=True, random_state=26)\n\nlist_models = []\nnames = []\nlist_models.append(('Linear Regression', LinearRegression()))\nlist_models.append(('KNN', KNeighborsRegressor(n_neighbors=5)))\nlist_models.append(('Decision Tree', DecisionTreeRegressor(random_state=0)))\nlist_models.append(('SVM', SVR()))\nlist_models.append(('Dummy regressor', DummyRegressor(strategy='mean')))\n\nresults = []\n\nfor name, model in list_models:\n    scores = - cross_val_score(model, X_nofare_nosurvived_concat, Y_fare_concat, cv=kfold, scoring='neg_mean_squared_error')   \n    print(\"%s MSE:\" %name , scores)\n    results.append([name, np.mean(scores), np.std(scores)])\n    \ndataframe_results = pd.DataFrame(results, columns = ['Regression model', 'mean_MSE', 'std'])\n\ndataframe_results.sort_values(['mean_MSE', 'std']) ","92e6e44a":"list_n_neighbors=[]\nresults = []\n\nfor i in range(1,21):\n    list_n_neighbors.append(i)\n    \nfor n_neighbors in list_n_neighbors:\n    neigh = KNeighborsRegressor(n_neighbors=n_neighbors)\n    scores = - cross_val_score(neigh, X_nofare_nosurvived_concat, Y_fare_concat, cv=kfold, scoring='neg_mean_squared_error')\n    results.append([n_neighbors, np.mean(scores), np.std(scores)])\n\n    \ndataframe_results = pd.DataFrame(results, columns = ['n_neighbors', 'mean_MSE', 'std'])\n\ndataframe_results.sort_values(['mean_MSE', 'std'])  ","e6b4678b":"dataframe_results.plot(x='n_neighbors',\n                  y='mean_MSE',\n                  yerr='std',\n                  marker='o',\n                  linestyle='--')","d88d206e":"neigh = KNeighborsRegressor(n_neighbors=10)\nneigh.fit(X_nofare_nosurvived_concat, Y_fare_concat)\npred_fare_NaN_testdf=neigh.predict(X_pred_fare_testdf)\nprint(pred_fare_NaN_testdf)","b7e0a4c2":"test_df.loc[test_df.Fare_norm.isnull(), 'Fare_norm'] = pred_fare_NaN_testdf\nprint(test_df.Fare_norm.isnull().any())\ntest_df.loc[NaNindexs_Fare_testdf]","60f90498":"Y=train_df.pop('Survived').values   #training labels","97fba050":"test_df.columns==train_df.columns","79e0fb64":"X=train_df.values      #training features\nX_test=test_df.values  #test features","f95bb8dc":"print('Training features shape: ',X.shape)\nprint('Training labels shape: ', Y.shape)\nprint('Test features shape: ',X_test.shape)","ade563df":"kfold =  KFold(n_splits=5, shuffle=True, random_state=26)\n\nlist_models = []\nnames = []\nlist_models.append(('Logistic regression', LogisticRegression()))\nlist_models.append(('KNN', KNeighborsClassifier(n_neighbors=5)))\nlist_models.append(('Decision Tree',DecisionTreeClassifier(random_state=0)))\nlist_models.append(('Random Forest',RandomForestClassifier(random_state=0)))\nlist_models.append(('SVM', SVC()))\n\nresults = []\n\nfor name, model in list_models:\n    scores = cross_val_score(model, X, Y, cv=kfold, scoring='accuracy')  \n    print(\"%s accuracy:\" %name , scores)\n    results.append([name, np.mean(scores), np.std(scores)])","3861dec9":"from sklearn.metrics import accuracy_score\nscores = []\nfor train_index, val_index in kfold.split(X):    \n    X_train, X_val = X[train_index], X[val_index]\n    Y_train, Y_val = Y[train_index], Y[val_index]\n    \n    model = Sequential([\n    Dense(128, activation='relu', input_shape=(X[0].shape)),    #input shape: (11,)\n    Dropout(0.5),\n    Dense(64, activation='relu'),\n    Dropout(0.5),\n    Dense(1, activation='sigmoid')\n    ])\n\n    model.compile(optimizer='rmsprop',\n                  loss=keras.losses.binary_crossentropy,\n                  metrics=['accuracy']) \n    \n    history=model.fit(\n    X_train,\n    Y_train,\n    verbose=0, #to hide the training process\n    epochs=20,\n    batch_size=32,\n    validation_data=(X_val, Y_val))\n    \n    Y_pred = model.predict(X_val)\n    Y_pred = [int(i>0.5) for i in model.predict(X_val)] #transforming predictions into binary values\n    \n    scores.append(accuracy_score(Y_val, Y_pred))    \n    \nprint(\"Neural Network accuracy:\", scores)\nresults.append([\"Neural Network\", np.mean(scores), np.std(scores)])","73b13e0b":"dataframe_results = pd.DataFrame(results, columns = ['Classification model', 'mean_accuracy', 'std'])\n\ndataframe_results.sort_values(['mean_accuracy', 'std'], ascending=[False, True])  ","6bb11a91":"clf = DecisionTreeClassifier(random_state=0)\npath = clf.cost_complexity_pruning_path(X, Y) #determine values for alpha\nccp_alphas =  path.ccp_alphas #extract different values for alpha\n\nresults = []\n\nfor ccp_alpha in ccp_alphas:\n    clf_RF = RandomForestClassifier(random_state=0, ccp_alpha=ccp_alpha)\n    scores = cross_val_score(clf_RF, X, Y, cv=kfold, scoring='accuracy')\n    results.append([ccp_alpha, np.mean(scores), np.std(scores)])\n    \ndataframe_results = pd.DataFrame(results, columns = ['alpha', 'mean_accuracy', 'std'])\n\ndataframe_results.sort_values(['mean_accuracy', 'std'], ascending=[False, True])  ","36f670fa":"accuracy_max = dataframe_results.mean_accuracy.max()\nbest_alpha = dataframe_results.loc[dataframe_results.mean_accuracy==accuracy_max, 'alpha']\nbest_alpha = float(best_alpha)\nprint(best_alpha)","650b0045":"def plot_cm(labels, predictions, p=0.5): \n\n    cm = confusion_matrix(labels, predictions > p)\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    annot[0,0] = 'TN\\n' \n    annot[0,1] = 'FP\\n' \n    annot[1,0] = 'FN\\n' \n    annot[1,1] = 'TP\\n' \n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            annot[i, j] = annot[i,j] + '%.1f%%\\n%d' % (p, c) \n    plt.figure(figsize=(5,5))\n    sns.heatmap(cm, annot=annot, fmt=\"\",cmap=\"YlGnBu\")\n    plt.title('Confusion matrix')\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    \n    \nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=0)\n\nRFmodel = RandomForestClassifier(random_state=0, ccp_alpha=best_alpha)\nRFmodel.fit(X_train,Y_train)\nval_prediction=RFmodel.predict(X_val)\nplot_cm(Y_val, val_prediction)","0d955c5c":"fea_imp = pd.DataFrame({'imp': RFmodel.feature_importances_, 'col': train_df.columns})\nfea_imp = fea_imp.sort_values(['imp'], ascending=True)\nfea_imp.plot(kind='barh', x='col', y='imp', figsize=(20, 10))","7e94f9da":"RFmodel = RandomForestClassifier(random_state=0, ccp_alpha=best_alpha)\nRFmodel.fit(X,Y)\nY_test = RFmodel.predict(X_test)","12aa0396":"Y_test","0a0e3f33":"submission_dataframe = pd.DataFrame({\"PassengerId\" : test_id, \"Survived\" : Y_test})\nsubmission_dataframe.to_csv(\"submission.csv\", index = False)","971a2bed":"submission_dataframe[0:20]","7af3f5a3":"This row has indeed a missing value.","b6e92602":"We replaced missing Fare values with the mean in the Age prediction part, so we'll set them back to the NaN value.","c369be67":"We may observe the correlation between features,  which could lead use to drop some features.\nIndeed if two features have high correlation then they are highly linearly dependent and thus have almost the same effect on the target feature (Survived here). In that case we would drop one of those features.","428f35d1":"For example let's check the 888th row:","25096bb4":"We can improve the linear regression model by reducing overfitting. To do that we apply **L2 regularization**: our new model is called **Ridge regression**. We perform cross validation to determine the best hyperparameter alpha. ","a9ceeeb9":"We perform one-hot encoding on categorical data while we normalize numerical data (Age, Fare, SibSp and Parch) with a MinMax scaler. Normalization is not needed for all machine learning algorithms but for some it is really needed, for example the algorithms that fit a model using a weighted sum of input variables  such as linear regression, logistic regression, and artificial neural networks. Since we want to compare many machine learning models it is appropriate to normalize the data.","fbe45266":"We can now display the results of our different models:","da0737ca":"<font size=\"+1\" ><b> Visualising the training dataset <\/b><\/font><br> <a id=\"3\"><\/a>\n","a319e4b8":"We map those values into integers:","0012c354":"Here linear regression seems to be the best performing model.","11c91dcd":"Let's first deal with the missing Age values since they are the most numerous. Since we use all other features to predict the missing values for Age, we cant have missing values on others features (only Fare in the test dataset in that case), so we'll fill temporarily them with a simple method. Here we'll replace them with the mean.","e85dd51a":"Random forest is a good candidate for our final model. Generally random forests don't overfit and it's not advised to prune the trees or to use stopping criterias, but pruning actually gave me better results so why not use it:","f01d69dc":"We're going to fill missing values for both train and test dataset, using machine learning models to predict the missing values. ","f8b68ddf":"We can also plot the importance of each features, i.e. how much each feature contributed to the model:","638d1e93":"<font size=\"+1\" ><b> Encoding <\/b><\/font><br> <a id=\"3\"><\/a>","8c94e785":"We can see that some features have some missing values, mainly the Cabin feature.","0ca5fcfc":"KNN is performing the best for this task, let's fine tune the hyperparameter n_neighbors:","829e906b":"<font size=\"+1\" ><b> Checking for missing values <\/b><\/font><br> <a id=\"3\"><\/a>","1d95e186":"<font size=\"+3\" color=\"blue\"><b> Exploratory Data Analysis and Feature Selection <\/b><\/font><br> <a id=\"3\"><\/a>\n","a786527d":"We can see that our tree is huge with 327 nodes and therefore it's probably overfitting the training data. To avoid that we could tune the hyperparameters of the tree like min_samples_leaf and max_depth but there is another option to control the size of the tree called **cost complexity pruning**. The pruining technique is parametized by the cost complexity parameter cc_alpha (the bigger the parameter, the smaller the tree), and we'll perform cross validation to find the best cost complexity parameter.","27b61e4e":"We fill the missing values with the predicted values and we re-encode the Embarked feature like it was before.","f8fd92f7":"Support Vector Machine seems to be the best performing model.","b091dc9b":"1046 =714 + 332","3f0429fe":"To predict the missing Age values of the test dataset we'll concatenate both training and test dataset to get more samples thus have better prediction. We do not concatenate to predict missing Age values of the training dataset because there are no Survived feature on the test dataset (obviously) and after experimentation it is not worth to lose this feature just to get more samples.","149bd303":"Now we can predict and fill the missing value:","d1b9aa20":"alpha = 1.5 appears to be the best choice.","c053572f":"In fact we only have 2 missing Embarked value in the training dataset and  1 missing Fare value in the test dataset so we could just drop the rows containing them without losing much, or even fill them with simple methods (like most frequent for categorical data and mean for numerical data). But we'll still try to predict them as best as we can.","738257d4":"As expected all features have been kept.","e53325b6":" Name and Ticket have too many unique values, we could reduce those but for the sake of simplicity we'll drop both.","35aa622c":"We can't use the cross_val_score method to evaluate a Neural network model so we have to do it ourselves:","43ddce64":"<font size=\"+3\" color=\"blue\"><b> Building our model to predict Survived <\/b><\/font><br> <a id=\"3\"><\/a>","9cbdf68f":"Now we perform cross validation to find the best model predicting missing Age values of the test dataset by training on both training and test dataset:","41f6c239":"Among the passengers, way more people didn't survive therefore the dataset is pretty imbalanced.","2291f5d6":"Like for Age we perform crossvalidation to compare different classification models:","68e18418":"We drop PassengerId since it doesn't bring any information (we need to save it for the test dataframe for submission).","c1305e9d":"Observation: Children seems to have higher chances of survival while elderly people have the worst survival chances (this makes sense, kids should be saved in priority while elderly people don't have the strength to survive the disaster).","f046020a":"We can now display the pruned tree using the optimal ccp_alpha:","cd0590db":"Women have on average a smaller ticket class than men butt the difference isn't that big to justify the huge difference in survival rate, thus ticket class does have an impact on survival chances. Perhaps the cabins of 1st class tickets are closer to emergency exits. ","17e666b4":"We pruned our tree from 327 nodes to 121 nodes and our pruned tree has a better accuracy (from 0.819 to 831). We can now use it to predict our missing values.","0a9ca6a2":"Now that we selected our 2 models  we can train them and use them to predict the missing age values for both training and test dataset:","8f4b4ae6":"Note: we do have missing values in Embarked feature in training dataset, but we performed one hot encoding so NaN values got coded into 0 0 0","72d25a66":"Finally we can fill the missing values with the previously predicted values :","5f7488e8":"As expected the sexe and the age are the most important features. But overall all the features we have kept have a decent importance.","07d6e331":"<font size=\"+1\" ><b> Selecting and tuning our model <\/b><\/font><br> <a id=\"3\"><\/a>","ac773027":"Now that we selected our features, we have to transform and encode them so they can be used with our machine learning models.","93624d92":"<font size=\"+0\" color=\"purple\"><b> Predicting missing Embarked values for the training dataset <\/b><\/font><br> <a id=\"3\"><\/a>","c74f1380":"Finally we can train our RandomForest on the full training dataset and predict for the submission:","fe8ff106":"<font size=\"+1\" ><b> Training our selected model and predicting for submission  <\/b><\/font><br> <a id=\"3\"><\/a>","205b1f8d":"Once again we'll perform cross validation to compare different machine learning models:","9981adb6":"We're going to drop features with more than 50% missing values:","a88d2395":"<font size=\"+1\" ><b> Filling missing values <\/b><\/font><br> <a id=\"3\"><\/a>","1b8ee3b4":"<font size=\"+1\" ><b> Correlation between features <\/b><\/font><br> <a id=\"3\"><\/a>","1fe61033":"n_neighbors= 10 appears to be the best value.","abcb1012":"<font size=\"+0\" color=\"purple\"><b> Predicting missing Fare values for the test dataset <\/b><\/font><br> <a id=\"3\"><\/a>","a60a0216":"Our model is doing pretty great especially to predict the negatives (the model is less acurate for the positive, this was expected because we have more negative training samples).","832212cc":"<font size=\"+3\" color=\"blue\"><b> Feature engineering <\/b><\/font><br> <a id=\"3\"><\/a>","32afc71d":"Let's start with the training dataset. We are testing different regression models using cross validation and we evaluate them with the MSE metric. One of the models is DummyRegressor(strategy=\"mean\") which simply always predicts the mean of the training set (X_train here).","f709ea79":"Observations: People are more likely to survive depending on certain \ncharacteristics. \nFor example women are more likely to survive than men, or people who got a 1st class ticket (the smaller the ticket class the better). \nBut each caracteristics can also be correlated between each other, for instance maybe most of people who got a 1st class ticket are female therefore they are more likely to survive.\n\nWe can check the average ticket class for both women and men :","aeeca6ce":"Like in the Age prediction part, we'll concatenate both training and test dataset to get more training samples.","81d10529":"Decision Tree is the best performing model by quite a margin.","f4798edb":"The following cell compares correlations between features and if two features have  a correlation >=0.9 (or <=-0.9) one of them will be dropped. In our case all features should be kept since they dont seem to be highly correlated according to the heatmap.","df77f2df":"Our Embarked values were one-hot encoded so we'll transform them back to a one-dimensional array since we want to predict them.","2bde2d88":"We sucessfully dropped the feature Cabin.\n\nWe can also check which rows have missing values:","568052e6":"<font size=\"+0\" color=\"purple\"><b> Predicting missing Age values for both training and test dataset <\/b><\/font><br> <a id=\"3\"><\/a>","f13eca4d":"Let's draw a confusion matrix to see the performance of our model:"}}