{"cell_type":{"9b52ccc3":"code","91289504":"code","bf4cb120":"code","6566a7fd":"code","aca81bba":"code","2273fc4e":"code","c5ae02ed":"code","16ef2554":"code","7e85cbcb":"code","edfa7d77":"code","cd418567":"code","083a2fa8":"code","3f85ec3c":"code","8ccdbe43":"code","6a4a8baa":"code","da1bc2a6":"code","2b04e7b2":"code","260303b8":"code","695a2d09":"code","7071778d":"code","35839c46":"code","d19cd85e":"code","c579fac3":"code","f5b39f95":"code","1dd1db66":"code","f6738683":"code","9fedf653":"code","7c8c71ab":"code","a8acbedc":"code","e94de99e":"code","000fad8f":"code","96d0c68f":"code","c22b4437":"code","24b2ede0":"code","5b05f107":"code","7afc0b61":"code","aa97769b":"code","3ee838f0":"code","45f67e69":"code","e21ed019":"code","0c583727":"code","2875d6d1":"code","bbec2eb2":"code","6b503330":"code","695ddc1c":"code","7ca88972":"code","8c2f7aea":"code","074027c6":"code","cee1250c":"code","4eef9b77":"code","01d4a24f":"code","276db9a5":"code","e1dbe89f":"code","f1066891":"code","74950f3a":"code","c0806326":"code","01c225dc":"code","bd49b81b":"code","445e89b0":"code","a538a127":"code","2963e3a9":"code","960aa8a6":"code","9f1a56b4":"code","7f27be00":"code","7ee37cc7":"code","71794e38":"code","3bcfa5e1":"code","99b84679":"code","18efcf02":"code","2bf50dfb":"code","93a8820d":"code","eab547cf":"code","09e0f520":"code","69b9c0a7":"code","017b3a15":"code","24770ef3":"code","a9b0e2b2":"code","cc1360ab":"code","c4929827":"markdown","b8bd179b":"markdown","74a085f8":"markdown","685cea3e":"markdown","f29b3a59":"markdown","7de1d138":"markdown","c2f3bcbd":"markdown","86082545":"markdown","81aad258":"markdown","6a779643":"markdown","949c8f82":"markdown","276a81e0":"markdown","3ffda9bc":"markdown","6a541941":"markdown","40eecfcc":"markdown","3ca2f9e9":"markdown","615f8700":"markdown","ddf502a9":"markdown","538c4d8b":"markdown","3ac65d89":"markdown","b6049f58":"markdown","de1a49dc":"markdown","b5856585":"markdown","c57d900d":"markdown","1d98cbe7":"markdown","490caf51":"markdown","e7761970":"markdown","1783ffd1":"markdown","b5265a37":"markdown","dc27706b":"markdown","6e6a2807":"markdown","ae11ff01":"markdown","c97ce9a2":"markdown","67fd250e":"markdown","2310fd98":"markdown","825f3a33":"markdown","9c1879e6":"markdown"},"source":{"9b52ccc3":"# \u958b\u59cb\u6642\u9593\nfrom datetime import datetime, timedelta, timezone\nJST = timezone(timedelta(hours=+9), 'JST')\nprint(datetime.now(JST))","91289504":"!pip install kaggle","bf4cb120":"!pip install datarobot","6566a7fd":"# \u57fa\u672c\nimport datetime as dt\nimport random\nimport glob\nimport cv2\nimport os\nfrom os import path\nimport gc\nimport sys\nimport json\n\n# \u30c7\u30fc\u30bf\u52a0\u5de5\nimport pandas as pd\nfrom pandas import DataFrame, Series\nimport numpy as np\nimport scipy as sp\nimport scipy.sparse as sps\n\n# \u30c7\u30fc\u30bf\u53ef\u8996\u5316\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom matplotlib import ticker\nplt.style.use('ggplot')\n%matplotlib inline\nimport seaborn as sns\n\n# \u691c\u5b9a\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom tqdm import tqdm_notebook as tqdm\n# \u8a55\u4fa1\nfrom sklearn.metrics import roc_auc_score, r2_score\n\n# \u524d\u51e6\u7406\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom category_encoders import OrdinalEncoder, OneHotEncoder, TargetEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import QuantileTransformer   # \u6570\u5024\u5217\u306eRankGauss\u51e6\u7406\u7528\n\n# \u30e2\u30c7\u30ea\u30f3\u30b0\nfrom sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nimport xgboost as xgb\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import BatchNormalization,Activation,Dropout,Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.layers import Flatten, Conv2D, MaxPooling2D, concatenate\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback, ReduceLROnPlateau\n\n# DataRobot SDK\nimport datarobot as dr\nfrom datarobot.enums import AUTOPILOT_MODE\nfrom datarobot.enums import DIFFERENCING_METHOD\nfrom datarobot.enums import TIME_UNITS\n\n# \u8868\u793a\u91cf\npd.set_option('display.max_columns', 200)\npd.set_option('display.max_rows', 100)\n\n# \u4e71\u6570\u30b7\u30fc\u30c9\u56fa\u5b9a\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_everything(2020)","aca81bba":"def capping(series, min_threshold, max_threshold):\n    series_filtered = series.copy()\n    index_outlier_up = [series_filtered  >= max_threshold]\n    index_outlier_low = [series_filtered <= min_threshold]\n    series_filtered.iloc[index_outlier_up] = max_threshold\n    series_filtered.iloc[index_outlier_low] = min_threshold\n    return series_filtered","2273fc4e":"# \u30b3\u30d4\u30da\u3067\u4f7f\u3048\u308b\u3002Kaggle\u3067\u306e\u5b9f\u9a13\u3092\u52b9\u7387\u5316\u3059\u308b\u5c0f\u6280\u307e\u3068\u3081 - \u5929\u8272\u30b0\u30e9\u30d5\u30a3\u30c6\u30a3\n#   https:\/\/amalog.hateblo.jp\/entry\/kaggle-snippets#DataFrame%E3%81%AE%E3%83%A1%E3%83%A2%E3%83%AA%E3%82%92%E7%AF%80%E7%B4%84%E3%81%99%E3%82%8B\n\n# pandas\u306eDataFrame\u306fint\u306a\u3089np.int64\u306b\u3001float\u306a\u3089np.float64\u304c\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u4f7f\u308f\u308c\u307e\u3059\u3002 \n# \u3057\u304b\u3057\u3001\u3042\u308b\u7a0b\u5ea6\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u304c\u5927\u304d\u304f\u306a\u3063\u3066\u304f\u308b\u3068\u3001DataFrame\u304c\u30e1\u30e2\u30ea\u3092\u5727\u8feb\u3057\u3066\u5b66\u7fd2\u3092\u601d\u3046\u3088\u3046\u306b\u9032\u3081\u308b\u3053\u3068\u304c\u3067\u304d\u306a\u304b\u3063\u305f\u308a\u3057\u307e\u3059\u3002\n# \u305d\u3053\u3067\u3001\u5404\u5217\u306e\u5024\u306e\u7bc4\u56f2\u3092\u53c2\u7167\u3057\u3001\u9069\u5207\u306a\u578b\u306b\u5909\u63db\u3057\u307e\u3059\u3002\n\nimport logging\n\ndef reduce_mem_usage(df, logger=None, level=logging.DEBUG):\n    print_ = print if logger is None else lambda msg: logger.log(level, msg)\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print_('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type != 'object' and col_type != 'datetime64[ns]':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)  # feather-format cannot accept float16\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print_('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print_('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    return df","c5ae02ed":"# \u30c7\u30fc\u30bf\u30b5\u30a4\u30ba(small, mid, full)\nflg_datasize = 'full'\n\n# CV\u3059\u308b\u304b\uff1f \u2192 \u610f\u5473\u306a\u3057\n#flg_cv = True\n\n# \u7279\u5fb4\u91cf\u3092\u7d5e\u308b\u304b\uff1f\nflg_cut_features = False\n\n# \u624b\u52d5\u3067\u7279\u5fb4\u91cf\u3092\u6307\u5b9a\u3059\u308b\uff1f\nfeature_list = []\n#feature_list = ['sub_grade', 'grade', 'dti', 'loan_amnt\/installment', 'revol_bal-tot_cur_bal', 'zip_code', 'open_acc', 'issue_d-earliest_cr_line', 'installment', 'tot_cur_bal', 'revol_util', 'mths_since_last_record', 'annual_inc\/installment', 'home_ownership', 'loan_amnt-revol_bal', 'annual_inc-tot_cur_bal', 'inq_last_6mths', 'annual_inc\/Real State Growth %_mean', 'mths_since_last_major_derog', 'revol_bal', 'StateName', 'annual_inc', 'loan_amnt-tot_cur_bal', 'loan_amnt', 'earliest_cr_line', 'annual_inc-installment', 'StateCode', 'installment\/loan_amnt', 'total_acc', 'addr_state', 'installment-tot_cur_bal', 'mths_since_last_delinq', 'revol_bal-tot_coll_amt', 'installment-loan_amnt', 'annual_inc\/State & Local Spending_mean', 'emp_length', 'title', 'delinq_2yrs', 'annual_inc\/Population (million)_mean', 'tot_coll_amt', 'Latitude', 'Real State Growth %_mean', 'annual_inc\/Gross State Product_mean', 'purpose', 'deputy', 'emp_length_isna', 'ceo', 'emp_title_isna', 'recruiter', 'distribution']\n\n# Top\u3044\u304f\u3064\u307e\u3067\u306e\u7279\u91cf\u306b\u7d5e\u308b\u304b\u3092\u6307\u5b9a\u3059\u308b\nif len(feature_list) > 0:\n    feature_imp_cnt = len(feature_list)\nelse:\n    feature_imp_cnt = 100\n\n# \u30c6\u30ad\u30b9\u30c8\u578b\u3092 TFIDF \u304b\u3051\u308b\u3068\u304d\u306e\u6700\u5927\u30ab\u30e9\u30e0\u6570 \nTFIDF_max_features = 2000\n\n# plot\u306a\u3069infomation\u3092\u8868\u793a\u3059\u308b\u304b\nflg_info = False","16ef2554":"# \u4f7f\u7528\u30e1\u30e2\u30ea\u3092\u524a\u6e1b\u3059\u308b\u305f\u3081\u306b\u5404\u5217\u306e\u578b\u3092\u660e\u793a\u7684\u306b\u6307\u5b9a \u2192 issue_d \u3092 2015\u5e74\u306b\u9650\u5b9a\u3057\u305f\u306e\u3067\u4f7f\u308f\u306a\u304f\u306a\u3063\u305f\n# https:\/\/qiita.com\/nannoki\/items\/2a8934de31ad2258439d\n# dtypes = {\n#     'ID': 'int32',\n#     'loan_amnt': 'int32',\n#     'installment': 'float32',\n#     'grade': 'object',\n#     'sub_grade': 'object',\n#     'emp_title': 'object',\n#     'emp_length': 'object',\n#     'home_ownership': 'object',\n#     'annual_inc': 'float32',\n#     'issue_d': 'object',\n#     'purpose': 'object',\n#     'title': 'object',\n#     'zip_code': 'object',\n#     'addr_state': 'object',\n#     'dti': 'float32',\n#     'delinq_2yrs': 'float32',\n#     'earliest_cr_line': 'object',\n#     'inq_last_6mths': 'float32',\n#     'mths_since_last_delinq': 'float32',\n#     'mths_since_last_record': 'float32',\n#     'open_acc': 'float32',\n#     'pub_rec': 'float32',\n#     'revol_bal': 'int32',\n#     'revol_util': 'float32',\n#     'total_acc': 'float32',\n#     'initial_list_status': 'object',\n#     'collections_12_mths_ex_med': 'float32',\n#     'mths_since_last_major_derog': 'float32',\n#     'application_type': 'object',\n#     'acc_now_delinq': 'float32',\n#     'tot_coll_amt': 'float32',\n#     'tot_cur_bal': 'float32',\n#     'loan_condition': 'int8'\n# }\n\n# dtypes = {\n#     'ID': 'int32',\n#     'loan_amnt': 'int32',\n#     'installment': 'float64',\n#     'grade': 'object',\n#     'sub_grade': 'object',\n#     'emp_title': 'object',\n#     'emp_length': 'object',\n#     'home_ownership': 'object',\n#     'annual_inc': 'float64',\n#     'issue_d': 'object',\n#     'purpose': 'object',\n#     'title': 'object',\n#     'zip_code': 'object',\n#     'addr_state': 'object',\n#     'dti': 'float64',\n#     'delinq_2yrs': 'float64',\n#     'earliest_cr_line': 'object',\n#     'inq_last_6mths': 'float64',\n#     'mths_since_last_delinq': 'float64',\n#     'mths_since_last_record': 'float64',\n#     'open_acc': 'float64',\n#     'pub_rec': 'float64',\n#     'revol_bal': 'int32',\n#     'revol_util': 'float64',\n#     'total_acc': 'float64',\n#     'initial_list_status': 'object',\n#     'collections_12_mths_ex_med': 'float64',\n#     'mths_since_last_major_derog': 'float64',\n#     'application_type': 'object',\n#     'acc_now_delinq': 'float64',\n#     'tot_coll_amt': 'float64',\n#     'tot_cur_bal': 'float64',\n#     'loan_condition': 'int8'\n# }\n","7e85cbcb":"\nif flg_datasize == 'small':\n    df_train = pd.read_csv('..\/input\/homework-for-students3\/train.csv', index_col=0, parse_dates=['issue_d', 'earliest_cr_line'], skiprows=lambda x: x%98!=0)\n    df_test = pd.read_csv('..\/input\/homework-for-students3\/test.csv', index_col=0, parse_dates=['issue_d', 'earliest_cr_line'], skiprows=lambda x: x%98!=0)\n    submission = pd.read_csv('..\/input\/homework-for-students3\/sample_submission.csv', index_col=0, skiprows=lambda x: x%98!=0)\nelif flg_datasize == 'mid':\n    df_train = pd.read_csv('..\/input\/homework-for-students3\/train.csv', index_col=0, parse_dates=['issue_d', 'earliest_cr_line'], skiprows=lambda x: x%20!=0)\n    df_test = pd.read_csv('..\/input\/homework-for-students3\/test.csv', index_col=0, parse_dates=['issue_d', 'earliest_cr_line'], skiprows=lambda x: x%20!=0)\n    submission = pd.read_csv('..\/input\/homework-for-students3\/sample_submission.csv', index_col=0, skiprows=lambda x: x%20!=0)\nelif flg_datasize == 'full':\n    df_train = pd.read_csv('..\/input\/homework-for-students3\/train.csv', index_col=0, parse_dates=['issue_d', 'earliest_cr_line'])\n    df_test = pd.read_csv('..\/input\/homework-for-students3\/test.csv', index_col=0, parse_dates=['issue_d', 'earliest_cr_line'])\n    submission = pd.read_csv('..\/input\/homework-for-students3\/sample_submission.csv', index_col=0)\n\n    \nprint(df_train.shape)","edfa7d77":"# 2015\u5e74\u306e\u30c7\u30fc\u30bf\u3060\u3051\u6b8b\u3059\n\ndf_train = df_train[df_train['issue_d'] >= dt.datetime(2015,1,1)]\ndf_train['issue_d']","cd418567":"# \u4f7f\u7528\u30e1\u30e2\u30ea\u3092\u524a\u6e1b\u3059\u308b\u305f\u3081\u306b\u5404\u5217\u306e\u578b\u3092\u660e\u793a\u7684\u306b\u6307\u5b9a\n# https:\/\/qiita.com\/nannoki\/items\/2a8934de31ad2258439d\n# dtypes_df_US_GDP_by_State = {\n#     'State': 'object',\n#     'State & Local Spending': 'float32',\n#     'Gross State Product': 'float32',\n#     'Real State Growth %': 'float32',\n#     'Population (million)': 'float32',\n#     'year': 'int16'\n# }\n\n# \u30a2\u30e1\u30ea\u30ab\u306e\u5404\u5dde\u306eGDP\ndf_US_GDP_by_State = pd.read_csv('..\/input\/homework-for-students3\/US_GDP_by_State.csv')\n\n# \u30a2\u30e1\u30ea\u30ab\u306e\u90f5\u4fbf\u756a\u53f7\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\uff08\u4f7f\u3063\u3066\u306a\u3044\uff09\n#df_free_zipcode_database = pd.read_csv('..\/input\/homework-for-students3\/free-zipcode-database.csv', index_col=0)\n\n# df_statelatlong = {\n#     'State': 'object',\n#     'Latitude': 'float32',\n#     'Longitude': 'float32',\n#     'City': 'object'\n# }\n# \u30a2\u30e1\u30ea\u30ab\u306e\u5404\u5dde\u306e\u7def\u5ea6\u7d4c\u5ea6\ndf_statelatlong = pd.read_csv('..\/input\/homework-for-students3\/statelatlong.csv')","083a2fa8":"# df_US_GDP_by_State \u3092 'State' \u3067\u96c6\u8a08\ndf_US_GDP_by_State = df_US_GDP_by_State.groupby('State').mean()[['State & Local Spending', 'Gross State Product', 'Real State Growth %', 'Population (million)']]\n\n# MultiIndex\u306b\u306a\u3063\u3066\u3057\u307e\u3063\u3066\u3044\u3066\u4f7f\u3044\u3065\u3089\u3044\u306e\u3067\u89e3\u9664\ncol_names = []\nfor c1 in ['State & Local Spending', 'Gross State Product', 'Real State Growth %', 'Population (million)']:\n#    for c2 in ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']:\n    for c2 in ['mean']:\n        col_names.append(c1 + '_' + c2)\n\ndf_US_GDP_by_State.columns = col_names\ndf_US_GDP_by_State.reset_index(inplace=True)\n\n# 'State' \u5217\u304c 'All states combined'\u306e\u884c\u3092\u524a\u9664\ndf_US_GDP_by_State = df_US_GDP_by_State[df_US_GDP_by_State['State'] != 'All states combined']\n\nif flg_info:\n    df_US_GDP_by_State","3f85ec3c":"# \u5404DataFrame\u306e\u5217\u540d\u3092\u4fee\u6b63\ndf_US_GDP_by_State.rename(columns={'State': 'StateName'}, inplace=True)\ndf_statelatlong.rename(columns={'State': 'StateCode', 'City': 'StateName'}, inplace=True)\n\n#print('#### df_US_GDP_by_State')\n#print(df_US_GDP_by_State.head(1))\n#print('#### df_statelatlong')\n#print(df_statelatlong.head(1))","8ccdbe43":"# \u7d50\u5408\ndf_train = pd.merge(left=df_train, right=pd.merge(df_US_GDP_by_State, df_statelatlong, on='StateName'), how='left', left_on='addr_state', right_on='StateCode')\ndf_test = pd.merge(left=df_test, right=pd.merge(df_US_GDP_by_State, df_statelatlong, on='StateName'), how='left', left_on='addr_state', right_on='StateCode')\n\n# \u306a\u3093\u3068\u306a\u304fcsv\u306b\u51fa\u529b\ndf_train.to_csv('train_with_geo-datas.csv')\n\nif flg_info:\n    df_train","6a4a8baa":"## \u5168\u6570\u5024\u5217\u306ehist\u3092\u898b\u3066\u307f\u308b\nif flg_info:\n    num_cols = df_train.select_dtypes(include='number').columns.values.tolist()\n    num_cols.remove('loan_condition')\n    print(num_cols)\n    \n    for f in num_cols:\n        plt.figure(figsize=[5,5])\n        df_train[f].hist(density=True, alpha=0.5, bins=20, color=\"blue\")\n        df_test[f].hist(density=True, alpha=0.5, bins=20, color=\"red\")\n        plt.xlabel(f)\n        plt.ylabel('density')\n        plt.show()\n        # print(df_train[f].describe(), df_test[f].describe())","da1bc2a6":"# 'dti' \u306e 999\u306e\u884c\u3092\u78ba\u8a8d\u3059\u308b\n#df_train.query('dti > 100')","2b04e7b2":"# pub_rec \u306e 20\u4ee5\u4e0a\n#df_train.query('pub_rec > 20')","260303b8":"# revol_util \u306e 150\u4ee5\u4e0a\n#df_train.query('revol_util > 150')","695a2d09":"# acc_now_delinq \u306e 6 \u4ee5\u4e0a\n#df_train.query('acc_now_delinq > 6')","7071778d":"# tot_coll_amt \u306e 200000 \u4ee5\u4e0a\n#df_train.query('tot_coll_amt > 200000')","35839c46":"# x\u3068y\u306b\u5206\u5272\nY_train = df_train.loan_condition\nX_train = df_train.drop(['loan_condition'], axis=1)\nX_test = df_test\n\n# \u7279\u5fb4\u91cf\u30a8\u30f3\u30b8\u30cb\u30a2\u30ea\u30f3\u30b0\u524d\u306e\u3001\u5143\u306e\u30ab\u30e9\u30e0\u540d\u4e00\u89a7\u3092\u53d6\u3063\u3066\u304a\u304f\uff08\u6b20\u640d\u30d5\u30e9\u30b0\u3092\u7acb\u3066\u308b\u3068\u3053\u308d\u3067\u4f7f\u3046\uff09\nbase_columns = X_train.columns\n","d19cd85e":"# \u540c\u3058\u30e6\u30fc\u30b6\u304c\u4f55\u56de\u3082\u501f\u308a\u3066\u304d\u3066\u306a\u3044\u304b\u7591\u3063\u3066\u307f\u308b\n# \u91cd\u8907\u30e6\u30fc\u30b6\u304c\u3044\u306a\u3044\u304b\u30c1\u30a7\u30c3\u30af\ncols_dup_user_check = ['grade', 'sub_grade', 'earliest_cr_line', 'emp_title', 'emp_length', 'zip_code', 'addr_state', 'home_ownership']\ndf_train[df_train.duplicated(cols_dup_user_check, keep=False)].sort_values(by=cols_dup_user_check, ascending=True)","c579fac3":"X_train['user_id'] = \\\n    X_train['grade'].fillna('NaN') + \\\n    X_train['sub_grade'].fillna('NaN') + \\\n    X_train['earliest_cr_line'].fillna('NaN').apply(lambda x: x.strftime('%Y\/%m\/%d')) + \\\n    X_train['emp_title'].fillna('NaN') + \\\n    X_train['emp_length'].fillna('NaN') + \\\n    X_train['zip_code'].fillna('NaN') + \\\n    X_train['addr_state'].fillna('NaN') + \\\n    X_train['home_ownership'].fillna('NaN')\n\nX_test['user_id'] = \\\n    X_test['grade'].fillna('NaN') + \\\n    X_test['sub_grade'].fillna('NaN') + \\\n    X_test['earliest_cr_line'].fillna('NaN').apply(lambda x: x.strftime('%Y\/%m\/%d')) + \\\n    X_test['emp_title'].fillna('NaN') + \\\n    X_test['emp_length'].fillna('NaN') + \\\n    X_test['zip_code'].fillna('NaN') + \\\n    X_test['addr_state'].fillna('NaN') + \\\n    X_test['home_ownership'].fillna('NaN')\n\nif flg_info:\n    X_train","f5b39f95":"# user_id\u5217\u306e\u9867\u5ba2ID\u3092\u5358\u4f4d\u3068\u3057\u3066\u5206\u5272\u3059\u308b\u3053\u3068\u306b\u3059\u308b\nuser_id = X_train['user_id']\nunique_user_ids = user_id.unique()\n\n# \u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u306efold\u3092\u5b9a\u7fa9\n#skf = StratifiedKFold(n_splits=5, random_state=71, shuffle=True)\nkf = KFold(n_splits=5, random_state=71, shuffle=True)\n\n# \u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u7528\u306euser_id\u5217\u3092\u524a\u9664\nX_train.drop(['user_id'], axis=1, inplace=True)\nX_test.drop(['user_id'], axis=1, inplace=True)\n\nuser_id","1dd1db66":"# \u30e1\u30e2\u30ea\u304c\u3082\u3063\u305f\u3044\u306a\u3044\u306e\u3067\u4f7f\u308f\u306a\u3044 df \u3092\u524a\u9664\ndel df_train, df_test, df_US_GDP_by_State, df_statelatlong\ngc.collect()","f6738683":"# \u5143\u306e\u30ab\u30e9\u30e0\u306e\u3046\u3061\u3001\u6b20\u640d\u5024\u3092\u542b\u3080\u5217\u306b\u5bfe\u3057\u3066\u6b20\u640d\u30d5\u30e9\u30b0\u5217\u3092\u8ffd\u52a0\u3059\u308b\nfor col in base_columns:\n    if X_train[col].isnull().any():\n        X_train[col + '_isna'] = X_train[col].isnull().replace({True : 1, False : 0})\n        X_test[col + '_isna'] = X_test[col].isnull().replace({True : 1, False : 0})\n\nX_train[['emp_title', 'emp_title_isna']]","9fedf653":"# 'issue_d' - 'earliest_cr_line' \u9593\u306e\u65e5\u6570\u3092\u65b0\u305f\u306a\u30ab\u30e9\u30e0\u3068\u3057\u3066\u8ffd\u52a0\nX_train['issue_d-earliest_cr_line'] = (X_train['issue_d'] - X_train['earliest_cr_line']).apply(lambda x: x.days)\nX_test['issue_d-earliest_cr_line'] = (X_test['issue_d'] - X_test['earliest_cr_line']).apply(lambda x: x.days)\n\nX_train['issue_d-earliest_cr_line']","7c8c71ab":"# \u5217\u3092\u524a\u9664\nX_train.drop(['issue_d'], axis=1, inplace=True)\nX_test.drop(['issue_d'], axis=1, inplace=True)\n\n# \u73fe\u5728\u306fTarget Encoding\u3057\u3066\u3057\u307e\u3063\u3066\u3044\u308b\n# X_train.drop(['earliest_cr_line'], axis=1, inplace=True)\n# X_test.drop(['earliest_cr_line'], axis=1, inplace=True)","a8acbedc":"colormap = plt.cm.RdBu\nplt.figure(figsize=(30,30))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(X_train.corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","e94de99e":"# array(['10+ years', '2 years', '1 year', '3 years', nan, '7 years',\n#       '5 years', '6 years', '8 years', '< 1 year', '4 years', '9 years'],\n#      dtype=object)\n\ndict = {np.nan:0, '< 1 year':0.5, '1 year':1, '2 years':2, '3 years':3, '4 years':4, '5 years':5, '6 years':6, '7 years':7, '8 years':8, '9 years':9, '10+ years':10}\nX_train['emp_length'] = X_train['emp_length'].map(dict)\nX_test['emp_length'] = X_test['emp_length'].map(dict)\n\nX_train[X_train.emp_length < 0.5]","000fad8f":"# \u91d1\u984d\u7cfb\u306e\u5217\u3092\u4f7f\u3063\u3066\u7279\u5fb4\u91cf\u751f\u6210\n#num_cols = ['annual_inc', 'installment', 'loan_amnt', 'revol_bal', 'tot_coll_amt', 'tot_cur_bal']\n\n# \u5168\u6570\u5024\u5217\u306b\u5bfe\u3057\u3066\u7db2\u7f85\u7684\u306b\u7279\u5fb4\u91cf\u751f\u6210\nnum_cols_for_auto_create = X_train.select_dtypes(include='number').columns.values.tolist()\n\nprint(num_cols_for_auto_create)\n\nfor i in range(0, len(num_cols_for_auto_create)-1):\n    for j in (i+1, len(num_cols_for_auto_create)-1):\n        c1 = num_cols_for_auto_create[i]\n        c2 = num_cols_for_auto_create[j]\n        X_train[c1 + 'X' + c2] = X_train[c1] * X_train[c2]\n        X_test[c1 + 'X' + c2]  = X_test[c1]  * X_test[c2]\n\n        # c2\u306b0\u304c\u542b\u307e\u308c\u3066\u306a\u3044\u306a\u3089\u5272\u308a\u7b97\u30ab\u30e9\u30e0\u3082\u8ffd\u52a0\n        if (X_train[c2] == 0).sum() == 0 & (X_test[c2] == 0).sum() == 0:\n            X_train[c1 + '\/' + c2] = X_train[c1] \/ X_train[c2]\n            X_test[c1 + '\/' + c2]  = X_test[c1]  \/ X_test[c2]\n\n# \u7121\u9650\u5927\u3092Nan\u306b\u5909\u63db\uff08\u5f8c\u3067\u6b20\u640d\u5024\u3068\u3057\u3066\u88dc\u5b8c\u3055\u308c\u308b\uff09\nX_train=X_train.replace([np.inf, -np.inf], np.nan)\nX_test=X_test.replace([np.inf, -np.inf], np.nan)\n\nprint(X_train.columns)","96d0c68f":"# \u30ed\u30fc\u30f3\u652f\u6255\u3044\u56de\u6570\nX_train['loan_amnt\/installment'] = X_train['loan_amnt'] \/ X_train['installment']\nX_test['loan_amnt\/installment']  = X_test['loan_amnt']  \/ X_test['installment']\n\nX_train[['installment', 'loan_amnt', 'loan_amnt\/installment']]","c22b4437":"# \u5e74\u53ce\u3068\u5dde\u306e\u5404\u5024\u3068\u306e\u5272\u5408\nX_train['annual_inc\/State & Local Spending_mean'] = X_train['annual_inc'] \/ X_train['State & Local Spending_mean']\nX_train['annual_inc\/Gross State Product_mean']    = X_train['annual_inc'] \/ X_train['Gross State Product_mean']\nX_train['annual_inc\/Real State Growth %_mean']    = X_train['annual_inc'] \/ X_train['Real State Growth %_mean']\nX_train['annual_inc\/Population (million)_mean']   = X_train['annual_inc'] \/ X_train['Population (million)_mean']\n\nX_test['annual_inc\/State & Local Spending_mean'] = X_test['annual_inc'] \/ X_test['State & Local Spending_mean']\nX_test['annual_inc\/Gross State Product_mean']    = X_test['annual_inc'] \/ X_test['Gross State Product_mean']\nX_test['annual_inc\/Real State Growth %_mean']    = X_test['annual_inc'] \/ X_test['Real State Growth %_mean']\nX_test['annual_inc\/Population (million)_mean']   = X_test['annual_inc'] \/ X_test['Population (million)_mean']","24b2ede0":"# \u5404\u5217\u306e\u5909\u63db\u65b9\u91dd\n# loan_amnt                   Standard\n# installment                 Standard\n# annual_inc                  log\n# dti                         log\n# delinq_2yrs                 log\n# inq_last_6mths              log\n# mths_since_last_delinq      log\n# mths_since_last_record      Standard\n# open_acc                    log\n# pub_rec                     log\n# revol_bal                   log\n# revol_util                  log\n# total_acc                   log\n# collections_12_mths_ex_med  log\n# mths_since_last_major_derog log\n# acc_now_delinq              log\n# tot_coll_amt                log\n# tot_cur_bal                 log\n# State & Local Spending_mean log\n# Gross State Product_mean    log\n# Real State Growth %_mean    Standard\n# Population (million)_mean   log\n# Latitude                    Standard\n# Longitude                   Standard\n# issue_d-earliest_cr_line    Standard","5b05f107":"# # StandardScaler\u3067\u6a19\u6e96\u5316\n# num_cols_Standard = ['loan_amnt', 'installment', 'mths_since_last_record', 'Real State Growth %_mean', 'Latitude', 'Longitude', 'issue_d-earliest_cr_line']\n# \n# from sklearn.preprocessing import StandardScaler\n# scaler = StandardScaler()\n# \n# for c in num_cols_Standard:\n#     # \u5b66\u7fd2\u30c7\u30fc\u30bf\u306b\u57fa\u3065\u3044\u3066\u8907\u6570\u30ab\u30e9\u30e0\u306e\u6a19\u6e96\u5316\u3092\u5b9a\u7fa9\n#     scaler.fit(X_train.iloc[:, X_train.columns == c])\n#     # \u5909\u63db\u5f8c\u30c7\u30fc\u30bf\u306e\u5217\u3092\u4f5c\u6210\n#     X_train[c + '_STD'] = scaler.transform(X_train.iloc[:, X_train.columns == c])\n#     X_test[c + '_STD'] = scaler.transform(X_test.iloc[:, X_test.columns == c])\n# \n#     plt.figure(figsize=[10,5])\n#     plt.subplot(1,2,1)\n#     X_train[c].hist(density=True, alpha=0.5, bins=20, color=\"blue\")\n#     X_test[c].hist(density=True, alpha=0.5, bins=20, color=\"red\")\n#     plt.xlabel(c)\n#     plt.ylabel('density')\n#     plt.subplot(1,2,2)\n#     X_train[c + '_STD'].hist(density=True, alpha=0.5, bins=20, color=\"blue\")\n#     X_test[c + '_STD'].hist(density=True, alpha=0.5, bins=20, color=\"red\")\n#     plt.xlabel(c + '_STD')\n#     plt.ylabel('density')\n#     plt.show()\n# \n#     # \u5909\u63db\u5f8c\u30c7\u30fc\u30bf\u3067\u7f6e\u63db\u3057\u3066\u4e0d\u8981\u306a\u5217\u3092\u524a\u9664\n#     X_train[c] = X_train[c + '_STD']\n#     X_test[c] = X_test[c + '_STD']\n#     X_train.drop([c + '_STD'], axis=1, inplace=True)\n#     X_test.drop([c + '_STD'], axis=1, inplace=True)","7afc0b61":"# # \u5bfe\u6570\u5909\u63db\n# num_cols_Log = ['annual_inc', 'dti', 'delinq_2yrs', 'inq_last_6mths', 'mths_since_last_delinq', 'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc', 'collections_12_mths_ex_med', 'mths_since_last_major_derog', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', 'State & Local Spending_mean', 'Gross State Product_mean', 'Population (million)_mean']\n# \n# for c in num_cols_Log:\n#     X_train[c + '_LOG'] = X_train[c].apply(np.log1p)\n#     X_test[c + '_LOG'] = X_test[c].apply(np.log1p)\n#     \n#     plt.figure(figsize=[10,5])\n#     plt.subplot(1,2,1)\n#     X_train[c].hist(density=True, alpha=0.5, bins=20, color=\"blue\")\n#     X_test[c].hist(density=True, alpha=0.5, bins=20, color=\"red\")\n#     plt.xlabel(c)\n#     plt.ylabel('density')\n#     plt.subplot(1,2,2)\n#     X_train[c + '_LOG'].hist(density=True, alpha=0.5, bins=20, color=\"blue\")\n#     X_test[c + '_LOG'].hist(density=True, alpha=0.5, bins=20, color=\"red\")\n#     plt.xlabel(c + '_LOG')\n#     plt.ylabel('density')\n#     plt.show()\n# \n#     # \u5909\u63db\u5f8c\u30c7\u30fc\u30bf\u3067\u7f6e\u63db\u3057\u3066\u4e0d\u8981\u306a\u5217\u3092\u524a\u9664\n#     X_train[c] = X_train[c + '_LOG']\n#     X_test[c] = X_test[c + '_LOG']\n#     X_train.drop([c + '_LOG'], axis=1, inplace=True)\n#     X_test.drop([c + '_LOG'], axis=1, inplace=True)","aa97769b":"num_cols = X_train.select_dtypes(include='number').columns.values.tolist()\n\nfrom sklearn.preprocessing import QuantileTransformer\n\n# \u5b66\u7fd2\u30c7\u30fc\u30bf\u306b\u57fa\u3065\u3044\u3066\u8907\u6570\u5217\u306eRankGauss\u306b\u3088\u308b\u5909\u63db\u3092\u5b9a\u7fa9\nscaler = QuantileTransformer(n_quantiles=100, random_state=71, output_distribution='normal')\n\nfor c in num_cols:\n    # \u5b66\u7fd2\u30c7\u30fc\u30bf\u306b\u57fa\u3065\u3044\u3066RankGauss\u306b\u3088\u308b\u5909\u63db\u3092\u5b9a\u7fa9\n    scaler.fit(X_train.iloc[:, X_train.columns == c])\n    # \u5909\u63db\u5f8c\u30c7\u30fc\u30bf\u306e\u5217\u3092\u4f5c\u6210\n    X_train[c + '_RANKG'] = scaler.transform(X_train.iloc[:, X_train.columns == c])\n    X_test[c + '_RANKG'] = scaler.transform(X_test.iloc[:, X_test.columns == c])\n    \n    if flg_info:\n        plt.figure(figsize=[10,5])\n        plt.subplot(1,2,1)\n        X_train[c].hist(density=True, alpha=0.5, bins=20, color=\"blue\")\n        X_test[c].hist(density=True, alpha=0.5, bins=20, color=\"red\")\n        plt.xlabel(c)\n        plt.ylabel('density')\n        plt.subplot(1,2,2)\n        X_train[c + '_RANKG'].hist(density=True, alpha=0.5, bins=20, color=\"blue\")\n        X_test[c + '_RANKG'].hist(density=True, alpha=0.5, bins=20, color=\"red\")\n        plt.xlabel(c + '_RANKG')\n        plt.ylabel('density')\n        plt.show()\n\n    # \u5909\u63db\u5f8c\u30c7\u30fc\u30bf\u3067\u7f6e\u63db\u3057\u3066\u4e0d\u8981\u306a\u5217\u3092\u524a\u9664\n    X_train[c] = X_train[c + '_RANKG']\n    X_test[c] = X_test[c + '_RANKG']\n    X_train.drop([c + '_RANKG'], axis=1, inplace=True)\n    X_test.drop([c + '_RANKG'], axis=1, inplace=True)","3ee838f0":"# \u5404\u5217\u3054\u3068\u306e\u6226\u7565          \u30e6\u30cb\u30fc\u30af\u6570\n#  column\u540d             train     test\n#  grade                    7        7      \u8f9e\u66f8\u9806\u3067Label Encoding\n#  sub_grade               35       35      \u8f9e\u66f8\u9806\u3067Label Encoding \n#  emp_title            23506    10451      \u30c6\u30ad\u30b9\u30c8 \n#  emp_length              11       11      \u6570\u5024\u578b\u306b\u5909\u63db\uff08< 1 year = 0.5, n\/a = 0.0, 10+ years = 10)  \u2192 \u6e08\u307f\n#  home_ownership           5        4      Train \u3067 Target Encoding\n#  issue_d                103       12      \u65e5\u4ed8 \n#  purpose                 14       13      Train \u3067 Target Encoding\n#  title                 4832       12      purpose\u3067\u4ee3\u66ff\u3048\u3067\u304d\u308b\u306e\u3067\u6368\u3066\u308b\n#  zip_code               857      839      Train \u3067 Target Encoding\n#  addr_state              50       50      Train \u3067 Target Encoding\n#  earliest_cr_line       586      567      \u65e5\u4ed8 \n#  initial_list_status      2        2      One-Hot Encoding\n#  application_type         2        2      Train \u3067 Target Encoding\n#\n# \u3053\u308c\u4ee5\u5916\u306eobject\u578b\u3082\u3059\u3079\u3066 Target Encoding","45f67e69":"# \u5217\u3092\u524a\u9664\nX_train.drop(['title'], axis=1, inplace=True)\nX_test.drop(['title'], axis=1, inplace=True)\n","e21ed019":"# grade \u3068 sub_grade \u3092 Label Encoding\ncats_label = ['grade', 'sub_grade']\n\nfor c in cats_label:\n    le = LabelEncoder()\n    le.fit(X_train[c])\n    X_train[c] = le.transform(X_train[c])\n    X_test[c] = le.transform(X_test[c])\n\nX_train[cats_label]","0c583727":"# initial_list_status \u3092 One-hot Encoding\ncats_onehot = ['initial_list_status']\n\nX_train = pd.get_dummies(X_train, columns=cats_onehot)\nX_test = pd.get_dummies(X_test, columns=cats_onehot)\n\nX_train","2875d6d1":"cats_target = []\n# Target Encoding\n#cats_target = ['home_ownership', 'purpose', 'zip_code', 'addr_state', 'application_type', 'title']\n#cats_target = ['home_ownership', 'purpose', 'zip_code', 'addr_state', 'application_type', 'title', 'issue_d', 'earliest_cr_line']\n\ncats_target = X_train.select_dtypes(include=[object, 'datetime']).columns.values.tolist()\n# for col in X_train.columns:\n#     if X_train[col].dtype == 'object':\n#         cats_target.append(col)\n\n# 'emp_title' \u306f \u30c6\u30ad\u30b9\u30c8\u578b\u306a\u306e\u3067\u9664\u5916\ncats_target.remove('emp_title')\nprint(cats_target)\n\n# \u5909\u6570\u3092\u30eb\u30fc\u30d7\u3057\u3066Target Encoding\nfor c in cats_target:\n    # c\u5217\u3068\u76ee\u7684\u5909\u6570\u306e\u307f\u306e DataFrame \u3092\u4f5c\u6210\n    data_tmp = pd.DataFrame({c: X_train[c], 'target': Y_train})\n    \n    # ======= test \u306b\u5bfe\u3059\u308b Target Encoding ================\n    # train\u5168\u4f53\u3067\u30ab\u30c6\u30b4\u30ea\u5024\u3054\u3068\u306e\u76ee\u7684\u5909\u6570\u306e\u5e73\u5747\u5024\u3092\u96c6\u8a08\n    target_mean = data_tmp.groupby(c)['target'].mean()\n    # test\u306e\u5404\u30ab\u30c6\u30b4\u30ea\u5024\u3092\u5e73\u5747\u5024\u3067\u7f6e\u63db\n    X_test[c] = X_test[c].map(target_mean)\n    \n    # ======= train \u306b\u5bfe\u3059\u308b Target Encoding ================\n    # train \u306e\u5909\u63db\u5f8c\u306e\u5024\u3092\u683c\u7d0d\u3059\u308b\u914d\u5217\u3092\u6e96\u5099\n    tmp = np.repeat(np.nan, X_train.shape[0])\n    \n    # KFold\u30af\u30e9\u30b9\u3092\u7528\u3044\u3066\u3001\u9867\u5ba2ID\u5358\u4f4d\u3067\u5206\u5272\u3059\u308b\n    for tr_group_idx, va_group_idx in kf.split(unique_user_ids):\n        # \u9867\u5ba2ID\u3092train\/valid\uff08\u5b66\u7fd2\u306b\u4f7f\u3046\u30c7\u30fc\u30bf\u3001\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\uff09\u306b\u5206\u5272\u3059\u308b\n        tr_groups = unique_user_ids[tr_group_idx]  # Training\n        va_groups = unique_user_ids[va_group_idx]  # Validation\n        \n        # \u5404\u9867\u5ba2\u306e\u30ec\u30b3\u30fc\u30c9\u306e\u9867\u5ba2ID\u304ctrain\/valid\u306e\u3069\u3061\u3089\u306b\u5c5e\u3057\u3066\u3044\u308b\u304b\u306b\u3088\u3063\u3066\u5206\u5272\u3059\u308b\n        is_tr = user_id.isin(tr_groups)\n        is_va = user_id.isin(va_groups)\n\n        # out-of-fold\u3067\u30ab\u30c6\u30b4\u30ea\u5024\u3054\u3068\u306e\u76ee\u7684\u5909\u6570\u306e\u5e73\u5747\u5024\u3092\u96c6\u8a08\n        target_mean = data_tmp[is_tr].groupby(c)['target'].mean()\n        # \u7f6e\u63db\u5f8c\u306e\u5024\u3092\u4e00\u6642\u914d\u5217\u306b\u683c\u7d0d\u3057\u3066\u304a\u304f\n        tmp[is_va] = X_train[c][is_va].map(target_mean)\n    \n    # train\u306e\u5404\u30ab\u30c6\u30b4\u30ea\u5024\u3092\u5e73\u5747\u5024\u3067\u7f6e\u63db\n    X_train[c] = tmp\n    \nX_train[cats_target]","bbec2eb2":"# \u4e2d\u592e\u5024\u3067\u57cb\u3081\u308b\nfor col in X_train.columns:\n    if X_train[col].dtype != 'object':\n        median = X_train[col].median()\n        X_train[col].fillna(median, inplace=True)\n        X_test[col].fillna(median, inplace=True)\n\nX_train","6b503330":"%%time\n# X_train \u3068 X_test DataFrame\u306e\u578b\u3092\u6700\u9069\u5316\nX_train = reduce_mem_usage(X_train)\nX_test = reduce_mem_usage(X_test)","695ddc1c":"# Python \u30e1\u30e2\u30ea8GB\u3067kaggle\u3092\u95d8\u3046\u3068\u304d\u306b\u77e5\u3063\u3066\u304a\u304d\u305f\u3044\u3053\u3068 - Qiita\n#   https:\/\/qiita.com\/hmdhmd\/items\/2efb620abda7b20c6711\n\n#\u30e1\u30e2\u30ea\u3092\u98df\u3063\u3066\u3044\u308b\u5909\u6570\u3092\u5927\u304d\u3044\u307b\u3046\u304b\u3089\u8868\u793a\n\nprint(pd.DataFrame([[val for val in dir()], [sys.getsizeof(eval(val))\/1024\/1024 for val in dir()]],\n                   index=['name','size_MB']).T.sort_values('size_MB', ascending=False).reset_index(drop=True).query('size_MB > 100')\n     )","7ca88972":"# \u30c6\u30ad\u30b9\u30c8\u3068\u305d\u308c\u4ee5\u5916\u306b\u5206\u5272\nTXT_train = X_train.emp_title.copy()\nTXT_test = X_test.emp_title.copy()\n\nX_train.drop(['emp_title'], axis=1, inplace=True)\nX_test.drop(['emp_title'], axis=1, inplace=True)\n","8c2f7aea":"# \u5c0f\u6587\u5b57\u306b\u305d\u308d\u3048\u308b\nTXT_train = TXT_train.str.lower()\nTXT_test = TXT_test.str.lower()\n\n# \u30c6\u30ad\u30b9\u30c8\u3092TFIDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntdidf = TfidfVectorizer(min_df=3, norm='l2', ngram_range=(1, 3), stop_words='english', max_features=TFIDF_max_features)\n\ntdidf.fit(TXT_train.fillna('#'))\nTXT_train = tdidf.transform(TXT_train.fillna('#'))\nTXT_test = tdidf.transform(TXT_test.fillna('#'))\nprint('Vocabulary size: {}'.format(len(tdidf.vocabulary_)))","074027c6":"# TFIDF\u306b\u304b\u3051\u305f\u30c6\u30ad\u30b9\u30c8\u3092hstack\n# X_train = sp.sparse.hstack([X_train.values, TXT_train])\n# X_test = sp.sparse.hstack([X_test.values, TXT_test])\n","cee1250c":"# TFIDF\u306b\u304b\u3051\u305f sparse matrix \u3092\u3001index\u3092\u5143\u306b\u623b\u3057\u3064\u3064 DataFrame \u306b\u5909\u63db\nTXT_train_ = pd.DataFrame(TXT_train.toarray(), columns=tdidf.get_feature_names(), dtype=np.int8)\nTXT_train_.index = X_train.index\nTXT_test_ = pd.DataFrame(TXT_test.toarray(), columns=tdidf.get_feature_names(), dtype=np.int8)\nTXT_test_.index = X_test.index\n\nif flg_info:\n    print(TXT_train_.dtypes)\n    print(TXT_train_.info())\n\n# \u4e8b\u524d\u306b\u7279\u5fb4\u91cf\u30ea\u30b9\u30c8\u304c\u6307\u5b9a\u3055\u308c\u3066\u3044\u305f\u5834\u5408\u306f\n# \u4f7f\u308f\u306a\u3044\u30ab\u30e9\u30e0\u3092\u4e8b\u524d\u306b\u5208\u3063\u3066\u6d88\u8cbb\u30e1\u30e2\u30ea\u3092\u524a\u6e1b\u3059\u308b\n\nif len(feature_list) > 0:\n    print('##### before ##########################')\n    print(X_train.info())\n\n    for col in X_train.columns:\n        if col not in feature_list:\n            X_train.drop([col], axis=1, inplace=True)\n            X_test.drop([col], axis=1, inplace=True)\n    \n    print('##### after ##########################')\n    print(X_train.info())\n    \n    print('##### before ##########################')\n    print(TXT_train_.info())\n    \n    # \u4f7f\u308f\u306a\u3044\u30ab\u30e9\u30e0\u3092\u4e8b\u524d\u306b\u5208\u3063\u3066\u6d88\u8cbb\u30e1\u30e2\u30ea\u3092\u524a\u6e1b\u3059\u308b\n    for col in TXT_train_.columns:\n        if col not in feature_list:\n            TXT_train_.drop([col], axis=1, inplace=True)\n            TXT_test_.drop([col], axis=1, inplace=True)\n\n    print('##### after ##########################')\n    print(TXT_train_.info())\n\n# \u5143\u306eDataFrame\u306e\u53f3\u6a2a\u306b\u7d50\u5408\u3057\u306a\u304a\u3059\nX_train = pd.concat([X_train, TXT_train_], axis=1)\nX_test = pd.concat([X_test, TXT_test_], axis=1)\n\ndel TXT_train_, TXT_test_, TXT_train, TXT_test\ngc.collect()\n\nif flg_info:\n    print(X_train.dtypes)\n    print(X_train.info())\n\nX_train","4eef9b77":"# 'grade' \u3084 'title' \u306a\u3069 \u300c\u3082\u3068\u3082\u3068\u5408\u3063\u305fColumn\u540d\u300d \u3068 \u300cTFIDF\u3067\u751f\u6210\u3055\u308c\u305fColumn\u540d\u300d\u304c\u305f\u307e\u305f\u307e\u91cd\u8907\u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308b\u3002\n# \u305d\u306e\u6642\u3001\u300cTFIDF\u3067\u751f\u6210\u3055\u308c\u305fColumn\u300d\u306e\u307b\u3046\uff08\u53f3\u306b\u5f8c\u304b\u3089\u9023\u7d50\u3055\u308c\u305f\u307b\u3046\uff09\u3092\u524a\u9664\u3059\u308b\n# \u53c2\u8003 : https:\/\/qiita.com\/mynkit\/items\/6e8ddfa1c73363f6117e\n\nif len(X_train.columns) != X_train.columns.nunique():\n    print(X_train.columns[X_train.columns.duplicated(keep=False)])\n    X_train = X_train.loc[:,~X_train.columns.duplicated()]\n    X_test = X_test.loc[:,~X_test.columns.duplicated()]\n","01d4a24f":"#pd.concat([X_train, Y_train], axis=1).to_csv('train_after_feature_engineering.csv')\n#X_test.to_csv('test_after_feature_engineering.csv')","276db9a5":"# \u30e2\u30c7\u30ea\u30f3\u30b0\u306b\u4f7f\u3046\u30e9\u30a4\u30d6\u30e9\u30ea\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb\n\nfrom sklearn.model_selection import GridSearchCV","e1dbe89f":"# \u4e8b\u524d\u306b\u7279\u5fb4\u91cf\u30ea\u30b9\u30c8\u304c\u6307\u5b9a\u3055\u308c\u3066\u3044\u305f\u5834\u5408\u306f\nif len(feature_list) > 0:\n    # \u305d\u306e\u7279\u5fb4\u91cf\u30ea\u30b9\u30c8\u306b\u7d5e\u308b\n    X_train = X_train[feature_list]\n    X_test = X_test[feature_list]\n","f1066891":"# from hyperopt import fmin, tpe, hp, rand, Trials\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.metrics import roc_auc_score\n# \n# from lightgbm import LGBMClassifier","74950f3a":"# def objective(space):\n#     scores = []\n# \n#     # KFold\u30af\u30e9\u30b9\u3092\u7528\u3044\u3066\u3001\u9867\u5ba2ID\u5358\u4f4d\u3067\u5206\u5272\u3059\u308b\uff08\u30b0\u30eb\u30fc\u30d7\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\uff09\n#     for i, (tr_group_idx, va_group_idx) in enumerate(kf.split(unique_user_ids)):\n#         # \u9867\u5ba2ID\u3092train\/valid\uff08\u5b66\u7fd2\u306b\u4f7f\u3046\u30c7\u30fc\u30bf\u3001\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\uff09\u306b\u5206\u5272\u3059\u308b\n#         tr_groups = unique_user_ids[tr_group_idx]  # Training\n#         va_groups = unique_user_ids[va_group_idx]  # Validation\n# \n#         # \u5404\u9867\u5ba2\u306e\u30ec\u30b3\u30fc\u30c9\u306e\u9867\u5ba2ID\u304ctrain\/valid\u306e\u3069\u3061\u3089\u306b\u5c5e\u3057\u3066\u3044\u308b\u304b\u306b\u3088\u3063\u3066\u5206\u5272\u3059\u308b\n#         is_tr = user_id.isin(tr_groups)\n#         is_va = user_id.isin(va_groups)\n#         X_train_, Y_train_ = X_train[is_tr], Y_train[is_tr]  # Training\n#         X_valid_, Y_valid_ = X_train[is_va], Y_train[is_va]  # Validation\n# \n#         # \u30e2\u30c7\u30ea\u30f3\u30b0\n#         clf = LGBMClassifier(n_estimators=9999, **space)\n#         clf.fit(X_train_, Y_train_, early_stopping_rounds=20, eval_metric='auc', eval_set=[(X_valid_, Y_valid_)], verbose=False)\n# \n#         # \u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u3092\u4e88\u6e2c\u3057\u3066AUC\u3092\u8a08\u7b97\u3057\u51fa\u529b\n#         Y_pred = clf.predict_proba(X_valid_)[:,1]\n#         score = roc_auc_score(Y_valid_, Y_pred)\n#         scores.append(score)\n# \n#     scores = np.array(scores)\n#     print(scores.mean())\n#     \n#     return -scores.mean()","c0806326":"# # \u30d1\u30e9\u30e1\u30fc\u30bf\u63a2\u7d22\u7bc4\u56f2\u3092\u5b9a\u7fa9\n# space ={\n#         'max_depth': hp.choice('max_depth', np.arange(10, 30, dtype=int)),\n#         'subsample': hp.uniform ('subsample', 0.8, 1),\n#         'learning_rate' : hp.quniform('learning_rate', 0.025, 0.5, 0.025),\n#         'colsample_bytree' : hp.quniform('colsample_bytree', 0.5, 1, 0.05)\n#     }\n# \n# trials = Trials()\n# \n# best = fmin(fn=objective,\n#               space=space, \n#               algo=tpe.suggest,\n#               max_evals=20, \n#               trials=trials, \n#               rstate=np.random.RandomState(71) \n#              )\n# \n# LGBMClassifier(n_estimators=9999, **best)","01c225dc":"%%time\n# TensorFlow\/keras\u306e\u6700\u65b0version\u3067\u758e\u884c\u5217\u5468\u308a\u3067\u30a8\u30e9\u30fc\u304c\u51fa\u308b\u306e\u3067\u30d0\u30fc\u30b8\u30e7\u30f3\u6307\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002\n# \u73fe\u72b6\u3067\u306f\u672a\u89e3\u6c7a\u306e\u3088\u3046\u3067\u3001PyTorch\u4f7f\u3063\u305f\u307b\u3046\u304c\u3044\u3044\u304b\u3082\u3002\n!pip uninstall tensorflow -y\n!pip install tensorflow==1.11.0\n\n!pip uninstall keras -y\n!pip install keras==2.2.4","bd49b81b":"from keras.layers import Input, Dense ,Dropout, BatchNormalization\nfrom keras.optimizers import Adam, SGD\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping\n\n# \u30b7\u30f3\u30d7\u30eb\u306aMLP\n\ndef create_model(input_dim):\n    inp = Input(shape=(input_dim,), sparse=True) # \u758e\u884c\u5217\u3092\u5165\u308c\u308b\n#    inp = Input(shape=(input_dim,), sparse=False)\n#    inp = Input(shape=(input_dim,))\n    x = Dense(194, activation='relu')(inp)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    x = Dense(64, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    x = Dense(64, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.5)(x)\n    outp = Dense(1, activation='sigmoid')(x)\n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n    \n    return model\n","445e89b0":"%%time\n# CV\u3057\u3066\u30b9\u30b3\u30a2\u3092\u898b\u3066\u307f\u308b\u3002\n#df_scores = pd.DataFrame(index=[], columns=['Light GBM', 'Neural Network', 'k-Nearest Neighbors', 'Random Forest', 'CV_mean'])\n#df_scores = pd.DataFrame(index=[], columns=['Light GBM', 'Neural Network', 'Random Forest', 'CV_mean'])\n#df_scores = pd.DataFrame(index=[], columns=['Light GBM', 'Neural Network', 'CV_mean'])\n#df_scores = pd.DataFrame(index=[], columns=['Light GBM', 'Neural Network', 'GradientBoost', 'XGBoost', 'CV_mean'])\ndf_scores = pd.DataFrame(index=[], columns=['Light GBM', 'Neural Network', 'GradientBoost', 'CV_mean'])\nbest_iterations = []\nbest_epochs = []\nbest_knc_n_neighbors = []\n#df_feature_imp = pd.DataFrame()\n\nY_pred = pd.DataFrame()\n\n#plt.figure(figsize=(25.0, 20.0))\n\n# KFold\u30af\u30e9\u30b9\u3092\u7528\u3044\u3066\u3001\u9867\u5ba2ID\u5358\u4f4d\u3067\u5206\u5272\u3059\u308b\uff08\u30b0\u30eb\u30fc\u30d7\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\uff09\nfor i, (tr_group_idx, va_group_idx) in enumerate(kf.split(unique_user_ids)):\n    # \u9867\u5ba2ID\u3092train\/valid\uff08\u5b66\u7fd2\u306b\u4f7f\u3046\u30c7\u30fc\u30bf\u3001\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\uff09\u306b\u5206\u5272\u3059\u308b\n    tr_groups = unique_user_ids[tr_group_idx]  # Training\n    va_groups = unique_user_ids[va_group_idx]  # Validation\n\n    # \u5404\u9867\u5ba2\u306e\u30ec\u30b3\u30fc\u30c9\u306e\u9867\u5ba2ID\u304ctrain\/valid\u306e\u3069\u3061\u3089\u306b\u5c5e\u3057\u3066\u3044\u308b\u304b\u306b\u3088\u3063\u3066\u5206\u5272\u3059\u308b\n    is_tr = user_id.isin(tr_groups)\n    is_va = user_id.isin(va_groups)\n#    X_train_, Y_train_ = X_train[is_tr], Y_train[is_tr]  # Training\n#    X_valid_, Y_valid_ = X_train[is_va], Y_train[is_va]  # Validation\n    X_train_, Y_train_ = sps.csr_matrix(X_train[is_tr], dtype=np.float32), Y_train[is_tr]  # Training\n    X_valid_, Y_valid_ = sps.csr_matrix(X_train[is_va], dtype=np.float32), Y_train[is_va]  # Validation\n\n    # === \u30e2\u30c7\u30ea\u30f3\u30b0 =============================\n    # ##### Light GBM ###############\n    clf = LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.9,\n                            importance_type='split', learning_rate=0.05, max_depth=-1,\n                            min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n                            n_estimators=9999, n_jobs=-1, num_leaves=15, objective=None,\n                            random_state=71, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n                            subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n    clf.fit(X_train_, Y_train_, early_stopping_rounds=20, eval_metric='auc', eval_set=[(X_valid_, Y_valid_)], verbose=False)\n    best_iterations.append(clf.best_iteration_)\n    print('CV#' + str(i) + ' : Light GBM - Best Iterations = ' + str(clf.best_iteration_))\n\n    # ##### Neural Network #########\n    model = create_model(X_train_.shape[1])\n    es = EarlyStopping(monitor='val_loss', patience=0, verbose=0)\n    history = model.fit(X_train_, Y_train_, batch_size=32, epochs=999, validation_data=(X_valid_, Y_valid_), callbacks=[es], verbose=0)\n    best_epochs.append(len(history.history['val_loss']))\n    print('CV#' + str(i) + ' : Neural Network - Best epochs = ' + str(len(history.history['val_loss'])))\n    \n    # ##### GradientBoost ##########\n    gbc = GradientBoostingClassifier()\n    gbc.fit(X_train_, Y_train_)\n    print('CV#' + str(i) + ' : GradientBoost')\n\n    # ##### XGBoost ##########\n#    xgb_params = {\n#        'objective': 'binary:logistic',\n#        'eval_metric': 'auc',\n#    }\n#    bst = xgb.train(xgb_params,\n#                    xgb.DMatrix(X_train[is_tr], label=Y_train_),\n#                    num_boost_round=100,  # \u5b66\u7fd2\u30e9\u30a6\u30f3\u30c9\u6570\u306f\u9069\u5f53\n#                    )\n#    print('CV#' + str(i) + ' : XGBoost')\n    \n#     # ##### k-Nearest Neighbors ####\n#     # n_neighbors\u3092\u63a2\u7d22\u3059\u308b\n#     k_scores = pd.Series()\n#     for k in range(51, 101, 5):\n#         print(str(k) + ', ', end=\"\")\n#         knc = KNeighborsClassifier(n_neighbors=k)\n#         knc.fit(X_train_, Y_train_)\n#         k_scores[str(k)] = roc_auc_score(Y_valid_, knc.predict_proba(X_valid_)[:,1])\n# #    #print(k_scores)\n# #    #print(k_scores.idxmax())\n#     # \u63a2\u7d22\u3057\u305fn_neighbors\u3067\u518d\u5ea6\u4f5c\u6210\n#     knc = KNeighborsClassifier(n_neighbors=int(k_scores.idxmax()))\n# #    knc = KNeighborsClassifier(n_neighbors=100)\n#     knc.fit(X_train_, Y_train_)\n#     best_knc_n_neighbors.append(int(k_scores.idxmax()))\n# #    best_knc_n_neighbors.append(100)\n#     print('\\nCV#' + str(i) + ' : k-Nearest Neighbors - Best n_neighbors = ' + k_scores.idxmax())\n# #    print('CV#' + str(i) + ' : k-Nearest Neighbors - Best n_neighbors = ' + str(100))\n    \n#     # ##### Random Forest ##########\n#     rfc = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n#             max_depth=5, max_features=3, max_leaf_nodes=None,\n#             min_impurity_decrease=0.0, min_impurity_split=None,\n#             min_samples_leaf=1, min_samples_split=2,\n#             min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,\n#             oob_score=True, random_state=71, verbose=0, warm_start=False)\n#     rfc.fit(X_train_, Y_train_)\n#     print('CV#' + str(i) + ' : RandomForest')\n\n    # \u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u3092\u4e88\u6e2c\u3057\u3066AUC\u3092\u8a08\u7b97\u3057\u51fa\u529b\n    Y_pred_ = pd.DataFrame()\n    Y_pred_['Light GBM'] = clf.predict_proba(X_valid_)[:,1]\n    Y_pred_['Neural Network'] = model.predict(X_valid_)\n    Y_pred_['GradientBoost'] = gbc.predict_proba(X_valid_)[:,1]\n#    Y_pred_['XGBoost'] = bst.predict(xgb.DMatrix(X_train[is_va]))\n#    Y_pred_['k-Nearest Neighbors'] = knc.predict_proba(X_valid_)[:,1]\n#    Y_pred_['Random Forest'] = rfc.predict_proba(X_valid_)[:,1]\n    df_scores.loc['CV#' + str(i)] = [\n        roc_auc_score(Y_valid_, Y_pred_['Light GBM']), \n        roc_auc_score(Y_valid_, Y_pred_['Neural Network']), \n        roc_auc_score(Y_valid_, Y_pred_['GradientBoost']), \n#        roc_auc_score(Y_valid_, Y_pred_['XGBoost']), \n#        roc_auc_score(Y_valid_, Y_pred_['k-Nearest Neighbors']), \n#        roc_auc_score(Y_valid_, Y_pred_['Random Forest']), \n        roc_auc_score(Y_valid_, Y_pred_.mean(axis=1))\n    ]\n    #print(df_scores)\n    \n    # CV Averaging \u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\n    Y_pred['Light GBM CV#' + str(i)] = clf.predict_proba(X_test, num_iteration=clf.best_iteration_)[:,1]\n    Y_pred['Neural Network CV#' + str(i)] = model.predict(sps.csr_matrix(X_test))\n    Y_pred['GradientBoost CV#' + str(i)] = gbc.predict_proba(sps.csr_matrix(X_test))[:,1]\n#    Y_pred['XGBoost CV#' + str(i)] = bst.predict(xgb.DMatrix(X_test))\n#    Y_pred['k-Nearest Neighbors CV#' + str(i)] = knc.predict_proba(X_test)[:,1]\n#    Y_pred['Random Forest CV#' + str(i)] = rfc.predict_proba(X_test)[:,1]\n\n    # feature importance\u3092\u4fdd\u5b58\n    #print(\"feature_importances_ counts : \" + str(len(clf.feature_importances_)))\n    #print(\"X_train_.columns     counts : \" + str(len(X_train_.columns)))\n#    feat_imp = pd.Series(clf.feature_importances_, index=X_train.columns, name=\"CV#\" + str(i)).sort_values(ascending=False)\n    #print(\"feat_imp index       counts : \" + str(feat_imp.count()))\n    #print(\"feat_imp idx unique  counts : \" + str(feat_imp.index.nunique()))\n    #print(feat_imp.shape)\n#    df_feature_imp =  pd.concat([df_feature_imp, feat_imp.to_frame()], axis=1, sort=True)\n\n    # feature importance Top \u3092\u63cf\u753b\n#    plt.subplot(1,5,i+1)\n#    sns.set_palette(\"husl\")\n#    sns.barplot(feat_imp.head(feature_imp_cnt).values, feat_imp.head(feature_imp_cnt).index)\n#    plt.title('Top' + str(feature_imp_cnt) + ' Feature Importances in CV#' + str(i))\n#    plt.xlabel('Feature Importance Score')\n#    #plt.yticks(rotation=60)\n    \n    del X_train_, Y_train_, X_valid_, Y_valid_, Y_pred_\n    gc.collect()\n\ndf_scores.loc['algo_mean'] = df_scores.mean(axis=0)\nprint(df_scores)\nprint('===================================')\nprint(df_scores['CV_mean'].values)\n#print(np.mean(df_scores['CV_mean']))\nif df_scores.at['algo_mean', 'CV_mean'] < 0.69783:\n    print('Random_Baseline : \u4e0d\u53ef')\nelif 0.69783 <= df_scores.at['algo_mean', 'CV_mean'] < 0.69889:\n    print('Simple_Baseline : \u53ef')\nelif 0.69889 <= df_scores.at['algo_mean', 'CV_mean'] < 0.70169:\n    print('Median_Baseline : \u826f')\nelif 0.70169 <= df_scores.at['algo_mean', 'CV_mean'] < 0.70524:\n    print('GradeA_Baseline : \u512a')\nelif 0.70524 <= df_scores.at['algo_mean', 'CV_mean']:\n    print('\u904e\u53bb\u6700\u9ad8\u3092\u66f4\u65b0 : \u512a')\n\n# feature importance \u306e\u9ad8\u3044\u9806\u306b Top \"feature_imp_cnt\"\u500b \u3092\u51fa\u529b\n#df_feature_imp['mean'] = df_feature_imp.mean(axis=1)\n#df_feature_imp['std'] = df_feature_imp.std(axis=1)\n#feature_list = df_feature_imp.sort_values(by='mean', ascending=False).head(feature_imp_cnt).index\n#print(feature_list.to_list())\n\n# feature importance \u3092\u8868\u793a\n#plt.show()\n#print(df_feature_imp[['mean', 'std']].sort_values(by='mean', ascending=False).head(feature_imp_cnt))","a538a127":"if flg_cut_features:\n    # CV\u3067\u4e0a\u4f4d\u3060\u3063\u305f\u7279\u5fb4\u91cf\u3001\u3082\u3057\u304f\u306f\u4e8b\u524d\u306b\u6307\u5b9a\u3055\u308c\u305f\u7279\u5fb4\u91cf\u306b\u7d5e\u308b\n    X_train = X_train[feature_list]\n    X_test = X_test[feature_list]\n\n    X_train","2963e3a9":"%%time\n# Sparse Matrix\u306b\u5909\u63db\nX_train_columns = X_train.columns\nX_train = sps.csr_matrix(X_train, dtype=np.float32)\nX_test = sps.csr_matrix(X_test, dtype=np.float32)\n\ngc.collect()","960aa8a6":"#\u30e1\u30e2\u30ea\u3092\u98df\u3063\u3066\u3044\u308b\u5909\u6570\u3092\u5927\u304d\u3044\u307b\u3046\u304b\u3089\u8868\u793a\nprint(pd.DataFrame([[val for val in dir()], [sys.getsizeof(eval(val))\/1024\/1024 for val in dir()]],\n                   index=['name','size_MB']).T.sort_values('size_MB', ascending=False).reset_index(drop=True).query('size_MB > 100')\n     )","9f1a56b4":"%%time\n# Light GBM\n#clf = LGBMClassifier(**best)\nclf = LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.9,\n                                importance_type='split', learning_rate=0.05, max_depth=-1,\n                                min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n                                n_estimators=9999, n_jobs=-1, num_leaves=15, objective=None,\n                                random_state=71, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n                                subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\nclf.fit(X_train, Y_train, eval_metric='auc', verbose=False)","7f27be00":"%%time\n# Neural Network\nmodel = create_model(X_train.shape[1])\nmodel.fit(X_train, Y_train, batch_size=32, epochs=np.max(best_epochs), verbose=0)\n#model.fit(X_train, Y_train, batch_size=32, epochs=100, verbose=0)","7ee37cc7":"%%time\n# GradientBoost\ngbc = GradientBoostingClassifier()\ngbc.fit(X_train, Y_train)","71794e38":"#%%time\n#xgb_params = {\n#    'objective': 'binary:logistic',\n#    'eval_metric': 'auc',\n#}\n#bst = xgb.train(xgb_params,\n#                xgb.DMatrix(X_train, label=Y_train),\n#                num_boost_round=100,  # \u5b66\u7fd2\u30e9\u30a6\u30f3\u30c9\u6570\u306f\u9069\u5f53\n#                )","3bcfa5e1":"# %%time\n# # k-Nearest Neighbors\n# #knc = KNeighborsClassifier(n_neighbors=int(np.mean(best_knc_n_neighbors)))\n# knc = KNeighborsClassifier(n_neighbors=80)\n# knc.fit(X_train, Y_train)","99b84679":"# %%time\n# # Random Forest\n# rfc = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n#         max_depth=5, max_features=3, max_leaf_nodes=None,\n#         min_impurity_decrease=0.0, min_impurity_split=None,\n#         min_samples_leaf=1, min_samples_split=2,\n#         min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,\n#         oob_score=True, random_state=71, verbose=0, warm_start=False)\n# rfc.fit(X_train, Y_train)","18efcf02":"%%time\n# \u5168\u5b66\u7fd2\u3057\u305f\u30e2\u30c7\u30eb\u3067test\u306b\u5bfe\u3057\u3066\u4e88\u6e2c\u3059\u308b\nY_pred['Light GBM'] = clf.predict_proba(X_test, num_iteration=np.max(best_iterations))[:,1]\n#Y_pred['Light GBM'] = clf.predict_proba(X_test, num_iteration=100)[:,1]\n#Y_pred['Light GBM'] = clf.predict_proba(X_test)[:,1]","2bf50dfb":"%%time\nY_pred['Neural Network'] = model.predict(X_test)","93a8820d":"%%time\nY_pred['GradientBoost'] = gbc.predict_proba(X_test)[:,1]","eab547cf":"#%%time\n#Y_pred['XGBoost'] = bst.predict(xgb.DMatrix(X_test))","09e0f520":"# %%time\n# Y_pred['k-Nearest Neighbors'] = knc.predict_proba(X_test)[:,1]","69b9c0a7":"# %%time\n# Y_pred['Random Forest'] = rfc.predict_proba(X_test)[:,1]","017b3a15":"Y_pred","24770ef3":"submission.loan_condition = Y_pred.mean(axis=1).values\nsubmission.to_csv('submission.csv')\nsubmission","a9b0e2b2":"# # feature importance\u3092\u51fa\u529b\u3059\u308b\n# feat_imp = pd.Series(clf.feature_importances_, X_train_columns).sort_values(ascending=False)\n# \n# plt.figure(figsize=(20.0, 20.0))\n# sns.set_palette(\"husl\")\n# sns.barplot(feat_imp.head(feature_imp_cnt).values, feat_imp.head(feature_imp_cnt).index)\n# plt.title('Top' + str(feature_imp_cnt) + ' Feature Importances')\n# plt.xlabel('Feature Importance Score')\n# #plt.yticks(rotation=20)\n# plt.show()","cc1360ab":"# feat_imp.head(feature_imp_cnt)","c4929827":"# \u95a2\u6570:reduce_mem_usage:DataFrame\u3092\u5727\u7e2e\u3057\u3066\u30e1\u30e2\u30ea\u524a\u6e1b","b8bd179b":"# NN Model\u306e\u5b9a\u7fa9","74a085f8":"# ==== \u30ab\u30c6\u30b4\u30ea ====","685cea3e":"# \u3059\u3079\u3066\u306e\u6570\u5024\u5217\u3092RankGauss","f29b3a59":"# \u30c7\u30fc\u30bf\u78ba\u8a8d","7de1d138":"# \u95a2\u6570:capping:\u7bc4\u56f2\u3092\u6700\u5927\u30fb\u6700\u5c0f\u307e\u3067\u306bCap\u3059\u308b","c2f3bcbd":"\u30e6\u30fc\u30b6ID\u3092\u30d9\u30fc\u30b9\u306b\u3057\u305f\u30b0\u30eb\u30fc\u30d7\u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30f3\u3092\u63a1\u7528\u3059\u308b","86082545":"# \u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f","81aad258":"# \u305d\u306e\u4ed6\u3092Target Encoding","6a779643":"# \u7db2\u7f85\u7684\u306b\u7279\u5fb4\u91cf\u751f\u6210","949c8f82":"# \u5168\u30c7\u30fc\u30bf\u3067\u5b66\u7fd2","276a81e0":"# \u958b\u59cb\u6642\u9593","3ffda9bc":"# \u30d1\u30fc\u30c6\u30a3\u30b7\u30e7\u30cb\u30f3\u30b0","6a541941":"# emp_length \u3092 map\u3067\u81ea\u529b\u3067\u6570\u5024\u306b\u5909\u63db","40eecfcc":"# \u7279\u5fb4\u91cf\u540c\u58eb\u306e\u76f8\u95a2\uff08Pearson Correlation Heatmap\uff09","3ca2f9e9":"# \u6b20\u640d\u30d5\u30e9\u30b0\u3092\u7acb\u3066\u308b","615f8700":"# hyperopt\u3067\u30d1\u30e9\u30e1\u30fc\u30bf\u30c1\u30e5\u30fc\u30cb\u30f3\u30b0\uff08\u6642\u9593\u304c\u304b\u304b\u308b\u306e\u3067\u3084\u3089\u306a\u3044\uff09","ddf502a9":"# \u6559\u5e2b\u30c7\u30fc\u30bf\u3092\u51fa\u529b\u3057\u3066\u304a\u304f","538c4d8b":"# ==== \u30e2\u30c7\u30eb\u4f5c\u6210 =====","3ac65d89":"# \u5909\u6570","b6049f58":"# ==== \u6570\u5024\u7279\u5fb4\u91cf\u751f\u6210 ====","de1a49dc":"# \u30a2\u30e1\u30ea\u30ab\u306e\u5730\u7406\u60c5\u5831\u3092\u8aad\u307f\u8fbc\u3093\u3067\u7d50\u5408\nhttps:\/\/www.kaggle.com\/yusukehasegawa\/join-geographic-data?scriptVersionId=30590581","b5856585":"# \u4e88\u6e2c\u7d50\u679c\u3092\u51fa\u529b\uff08\u5e73\u5747\u5024\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\uff09","c57d900d":"# CV + CV Averaging","1d98cbe7":"# initial_list_status \u3092 One-hot Encoding","490caf51":"# ==== \u6570\u5024\u306e\u6a19\u6e96\u5316 ====","e7761970":"# \u81ea\u5206\u3067\u8003\u3048\u3066\u4f5c\u3063\u305f\u7279\u5fb4\u91cf","1783ffd1":"# \u76ee\u7684\u5909\u6570\u3068\u8aac\u660e\u5909\u6570\u306b\u5206\u5272","b5265a37":"# ==== \u6b20\u640d\u5024\u3092\u57cb\u3081\u308b ====","dc27706b":"# ==== \u30e1\u30e2\u30ea\u7bc0\u7d04 =====","6e6a2807":"# grade \u3068 sub_grade \u3092 Label Encoding","ae11ff01":"# \u65e5\u4ed8\u578b\u306e\u51e6\u7406","c97ce9a2":"# feature importance\u3092\u51fa\u529b\n\uff08\u3044\u3064\u304b Permutation Importance \u306b\u5909\u3048\u305f\u3044\uff09","67fd250e":"# \u5bfe\u6570\u5909\u63db\uff08\u3057\u306a\u3044\uff09","2310fd98":"# \u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u30a4\u30f3\u30dd\u30fc\u30c8","825f3a33":"# Standard Scaler\uff08\u3057\u306a\u3044\uff09","9c1879e6":"# ==== \u30c6\u30ad\u30b9\u30c8 ====\n\uff08Sparse Matrix\u3092\u4f7f\u3046\u5834\u5408\u306f\u3001\u3053\u3053\u306b\u6765\u308b\u307e\u3067\u306b\u3001\u4ed6\u306e\u3059\u3079\u3066\u306e\u7279\u5fb4\u91cf\u3092\u6570\u5024\u5316\u3057\u3066\u304a\u304b\u306a\u3044\u3068\u30a8\u30e9\u30fc\u304c\u8d77\u304d\u308b\u3088\uff09"}}