{"cell_type":{"a0c623ee":"code","43150adc":"code","bca8b783":"code","2017a22a":"code","2afc297c":"code","d9a8e046":"code","9ee05382":"code","09730271":"code","b7a5db70":"code","753858ff":"code","d229b6cb":"code","cc0d5cc5":"markdown","f0d2ff0c":"markdown","5caa2e70":"markdown","3a12c90e":"markdown","6b613190":"markdown","4303c296":"markdown","5e1f4c7d":"markdown","c95663e8":"markdown","f67cbdb2":"markdown"},"source":{"a0c623ee":"# Install the datatable package.\n!pip install datatable","43150adc":"import pandas as pd\nimport datatable as dt\nimport gc\nimport numpy as np","bca8b783":"%%time\ntrain_df = dt.fread(\"\/kaggle\/input\/riiid-test-answer-prediction\/train.csv\").to_pandas()","2017a22a":"train_df.info()","2afc297c":"dtype={\n    'row_id': np.int64, 'timestamp': np.int64, 'user_id': np.int32, 'content_id': np.int16, 'content_type_id': np.int8,\n    'task_container_id': np.int16, 'user_answer': np.int8, 'answered_correctly': np.int8, 'prior_question_elapsed_time': np.float32, \n    'prior_question_had_explanation': np.bool,\n}","d9a8e046":"for col in dtype.keys():\n    train_df[col] = train_df[col].astype(dtype[col])\ntrain_df.info()","9ee05382":"dt.Frame(train_df).to_jay(\"train_df.jay\")","09730271":"del train_df","b7a5db70":"gc.collect()","753858ff":"%%time\ntrain_df = dt.fread(\"train_df.jay\").to_pandas()","d229b6cb":"train_df.info()","cc0d5cc5":"Now, we've gained approximately 1.6 GB of extra RAM to be used in something really useful.","f0d2ff0c":"That's all folks!\n\nThis is the first of a series of short notebook that I'm planning to make. The goal is to build the critical phases of an end-to-end project, step by step. So, stay tuned for the other kernels.\n\nHave you found something useful? Please, give a an upvote!","5caa2e70":"We can see thath the entire dataset was loaded in hoghly 44 seconds, which is nice given the dataset size.\n\nNow, let's check the dataframe's information.","3a12c90e":"# Efficient dataframe loading with Datatable\n\nFirst of all, lets start by quoting what [Sohier Dane](https:\/\/www.kaggle.com\/sohier) said about the training dataset in the competition's starter [notebook](https:\/\/www.kaggle.com\/sohier\/competition-api-detailed-introduction).\n\n> It's larger than will fit in memory with default settings, so we'll specify more efficient datatypes and only load a subset of the data for now.\n\nAfter that, an instruction is given on how to efficiently load the dataset using specific data types. However, if you try to load the entire dataset that way using pandas, your RAM memmory limit will be likely reached.\n\nInspired by [Vopani](https:\/\/www.kaggle.com\/rohanrao)'s excelent [notebook](https:\/\/www.kaggle.com\/rohanrao\/riiid-with-blazing-fast-rid), we'll see how to load heavy .csv data using the [**Python datatable**](https:\/\/datatable.readthedocs.io\/en\/latest\/index.html) package.","6b613190":"As we can see, **datatable** has automatically infered some columns types, in contrast with the rather conservative Pandas's data loading.\n\nThe entire dataset fits nicely in 4.6 GB without effort. But, as stated in the starter notebook, the data types can be further tweaked in order to improve memmory consumption.","4303c296":"## Loading with datatable\n\nLoading .csv data and converting it to a Pandas Dataframe with Datatable is straightforward.","5e1f4c7d":"Cool! The entire dataset was loaded in amzing 4.84 seconds (~ 4 times faster). As a bonus, our data types were preserved.","c95663e8":"## Saving in binary format\n\nAs an additional step, well check the benefits of saving the processed data into a binary format.\n\nDatatable uses .jay format, which makes reading our dataset a breeze.","f67cbdb2":"Now, we'll load our data back."}}