{"cell_type":{"cfb679cb":"code","e5275ad5":"code","6b21a9d0":"code","3e23d526":"code","de3640f6":"code","397ed03c":"code","95838256":"code","d56813e2":"code","a9bd4a63":"code","e1f9383b":"code","0d6afcbb":"code","fb57e33b":"code","e1cdd501":"code","86f6ebba":"code","8352bd1f":"code","1f0baa26":"code","705d2597":"code","28aab499":"code","6a011c68":"code","80cd71eb":"code","151d459a":"code","c4b80608":"code","c83d2bab":"code","93bae0f2":"code","5b8d837f":"code","cbad85f8":"markdown","adff5b24":"markdown","64e2ea04":"markdown","d8b4e3a2":"markdown","7c8887ee":"markdown","884f1214":"markdown","fb1de0ba":"markdown","b987487a":"markdown","c9ab2327":"markdown","62bfa1b7":"markdown","438f0bc6":"markdown","1a6af18a":"markdown","930945a3":"markdown","ccd715b6":"markdown","15858184":"markdown","da47d11f":"markdown","9c197f96":"markdown","e7749721":"markdown","9fea0f4f":"markdown"},"source":{"cfb679cb":"#Importing packages\nimport numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","e5275ad5":"#Load dataset\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest  = pd.read_csv(\"..\/input\/test.csv\")","6b21a9d0":"#Dataset shape\nprint('Train %s\\nTest %s' % (train.shape, test.shape))","3e23d526":"#Feature to predict\nft_pred = list(set(train.columns) - set(test.columns))\nft_pred","de3640f6":"train[ft_pred].describe()","397ed03c":"#Plot GrLivArea vs SalePrice\nplt.scatter(train['GrLivArea'], train['SalePrice'], color='blue', alpha=0.5)\nplt.title(\"LotArea vs SalePrice\")\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()","95838256":"#save and drop id\ntrain_id = train[\"Id\"]\ntrain.drop(columns='Id',inplace=True)\n\ntest_id = test[\"Id\"]\ntest.drop(columns='Id',inplace=True)\n\n#select object columns\nobj_col = train.columns[train.dtypes == 'object'].values\n\n#select non object columns\nnum_col = train.columns[train.dtypes != 'object'].values\nnum_col_test = test.columns[test.dtypes != 'object'].values\n\n#replace null value in obj columns with None\ntrain[obj_col] = train[obj_col].fillna('None')\ntest[obj_col] = test[obj_col].fillna('None')\n\n#replace null value in numeric columns with 0\ntrain[num_col] = train[num_col].fillna(0)\ntest[num_col_test] = test[num_col_test].fillna(0)\n\ntrain_001 = train\ntest_001 = test","d56813e2":"import category_encoders as ce\n\n#Ordinal features\nordinal_features = [\"ExterQual\",\"ExterCond\",\"BsmtQual\",\"BsmtCond\",\"BsmtExposure\", \"BsmtFinType1\",\"BsmtFinType2\",\n                    \"HeatingQC\",\"Electrical\",\"KitchenQual\", \"FireplaceQu\",\"GarageQual\",\"GarageCond\",\"PoolQC\"]\n\n#Split X,y\ntrain_002_X = train_001.drop(ft_pred, axis=1)\ntrain_002_y = train_001[ft_pred]\n\nce_one_hot = ce.OrdinalEncoder(cols = ordinal_features)\n\ntrain_003 = pd.concat([ce_one_hot.fit_transform(train_002_X), train_002_y], axis=1, sort=False)\ntest_003  = ce_one_hot.transform(test_001)\n","a9bd4a63":"#Nominal features\nnominal_features = [x for x in obj_col if x not in ordinal_features]\n\n#Transfer object to int\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\n\n#for loop nominal feature column\nfor i in train_003[nominal_features].columns:\n    #fit and transform each column and assign to itself\n    train_003[i] = labelencoder.fit_transform(train_003[i])\n    \n#for loop nominal feature column\nfor i in test_003[nominal_features].columns:\n    #fit and transform each column and assign to itself\n    test_003[i] = labelencoder.fit_transform(test_003[i])\n    \n#Get dummy variable for nominal features\ntrain_005 = pd.get_dummies(train_003,columns=nominal_features,drop_first=True)\ntest_005 = pd.get_dummies(test_003,columns=nominal_features,drop_first=True)","e1f9383b":"#Only for test set\n#Check if any null values\nprint(train_005.isnull().any().sum())\nprint(test_005.isnull().any().sum())\n\n#Get missing columns in the training test\nmissing_cols = set(train_005.drop(columns=\"SalePrice\").columns) - set(test_005.columns)\n\n#Add a missing column in test set with default value equal to 0\nfor cols in missing_cols:\n    test_005[cols] = 0\n    \n#Ensure the order of column in the test set is in the same order than in train set\ntest_005 = test_005[train_005.drop(columns=\"SalePrice\").columns]","0d6afcbb":"\n\n#TotalBath\ntrain_005['TotalBath'] = (train_005['FullBath'] + train_005['HalfBath'] + train_005['BsmtFullBath'] + train_005['BsmtHalfBath'])\ntest_005['TotalBath']  = (test_005['FullBath']  + test_005['HalfBath']  + test_005['BsmtFullBath']  + test_005['BsmtHalfBath'])\n\n#TotalPorch\ntrain_005['TotalPorch'] = (train_005['OpenPorchSF'] + train_005['3SsnPorch'] + train_005['EnclosedPorch'] + train_005['ScreenPorch'] + train_005['WoodDeckSF'])\ntest_005['TotalPorch']  = (test_005['OpenPorchSF']  + test_005['3SsnPorch']  + test_005['EnclosedPorch']  + test_005['ScreenPorch']    + test_005['WoodDeckSF'])\n\n#Modeling happen during the sale year\ntrain_005[\"RecentRemodel\"] = (train_005[\"YearRemodAdd\"] == train_005[\"YrSold\"]) * 1\ntest_005[\"RecentRemodel\"]  = (test_005[\"YearRemodAdd\"]  == test_005[\"YrSold\"]) * 1\n\n#House sold in the year it was built\ntrain_005[\"NewHouse\"] = (train_005[\"YearBuilt\"] == train_005[\"YrSold\"]) * 1\ntest_005[\"NewHouse\"]  = (test_005[\"YearBuilt\"]  == test_005[\"YrSold\"]) * 1\n\n#YrBltAndRemod\ntrain_005[\"YrBltAndRemod\"] = train_005[\"YearBuilt\"] + train_005[\"YearRemodAdd\"]\ntest_005[\"YrBltAndRemod\"]  = test_005[\"YearBuilt\"]  + test_005[\"YearRemodAdd\"]\n\n#Total_sqr_footage\ntrain_005[\"Total_sqr_footage\"] = train_005[\"BsmtFinSF1\"] + train_005[\"BsmtFinSF2\"] + train_005[\"1stFlrSF\"] + train_005[\"2ndFlrSF\"]\ntest_005[\"Total_sqr_footage\"]  = test_005[\"BsmtFinSF1\"]  + test_005[\"BsmtFinSF2\"]  + test_005[\"1stFlrSF\"]  + test_005[\"2ndFlrSF\"]\n\n#HasPool\ntrain_005['HasPool'] = train_005['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ntest_005['HasPool']  = test_005['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n\n#HasFireplaces\ntrain_005['HasFirePlace'] = train_005['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\ntest_005['HasFirePlace']  = test_005['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\n#Has2ndFloor\ntrain_005['Has2ndFloor'] = train_005['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ntest_005['Has2ndFloor']  = test_005['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n\n#HasGarage\ntrain_005['HasGarage'] = train_005['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ntest_005['HasGarage']  = test_005['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n\n#HasBsmnt\ntrain_005['HasBsmnt'] = train_005['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ntest_005['HasBsmnt']  = test_005['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)","fb57e33b":"#Importing packages\nfrom sklearn.model_selection import train_test_split\n\nX = train_005.drop(columns=\"SalePrice\")\ny = train_005[\"SalePrice\"]\n\n#Particiona o data set originalmente Train em Train(Treino) e Val(valida\u00e7\u00e3o)\nX_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.25,random_state=0)","e1cdd501":"X_train.shape, X_val.shape","86f6ebba":"from sklearn.ensemble import IsolationForest\nfrom sklearn.covariance import EllipticEnvelope\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\n\n#Isolation Forest\n\n# identify outliers in the training dataset\n#iso = IsolationForest(contamination=0.01)\n#yhat = iso.fit_predict(X_train)\n\n#Minimum Covariance Determinant\n\n# identify outliers in the training dataset\nee = EllipticEnvelope(contamination=0.01)\nyhat = ee.fit_predict(X_train)\n\n#Local Outlier Factor\n\n# identify outliers in the training dataset\n#lof = LocalOutlierFactor()\n#yhat = lof.fit_predict(X_train)\n\n#One-Class SVM\n\n# identify outliers in the training dataset\n#ee = OneClassSVM(nu=0.01)\n#yhat = ee.fit_predict(X_train)\n\n# select all rows that are not outliers\nmask = yhat != -1\nX_train_001, y_train_001 = X_train[mask], y_train[mask]\n\n# select all rows that are outliers\nmasko = yhat == -1\nX_train_o, y_train_o = X_train[masko], y_train[masko]\n\n# summarize the shape of the updated training dataset\nprint(X_train_001.shape, y_train_001.shape)","8352bd1f":"#Plot GrLivArea vs SalePrice\nplt.scatter(X_train_001['GrLivArea'], y_train_001, color='blue', alpha=0.5)\nplt.scatter(X_train_o['GrLivArea'], y_train_o, color='red', alpha=0.5, label='outlier')\nplt.legend(loc=\"upper left\")\nplt.title(\"GrLivArea vs SalePrice\")\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.show()","1f0baa26":"#Importing Packages\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom xgboost import XGBRFRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\n#from sklearn.preprocessing import Imputer#","705d2597":"#XGBoost hyper-parameter tuning\ndef hyperParameterTuning(X_train_001, y_train_001):\n    param_tuning = {\n        'objective': ['reg:squarederror'],\n        'colsample_bytree': [0.2, 0.5, 1],\n        'subsample': [0.7, 1],\n        'learning_rate': [0.05, 0.1, 0.3],\n        'max_depth': [3, 6, 8],\n        'min_child_weight': [0, 1, 10],\n        'n_estimators' : [700, 1000, 5000]\n    }\n\n    xgb_model = XGBRegressor(tree_method='gpu_hist')\n\n    gsearch = GridSearchCV(estimator = xgb_model,\n                           param_grid = param_tuning,                        \n                           #scoring = 'neg_mean_absolute_error', #MAE\n                           #scoring = 'neg_mean_squared_error',  #MSE\n                           cv = 3,\n                           n_jobs = -1,\n                           verbose = 10)\n\n    gsearch.fit(X_train_001,y_train_001)\n\n    return gsearch.best_params_","28aab499":"#Run only in the first run of the kernel.\n#hyperParameterTuning(X_train, y_train)","6a011c68":"XGBReg_def = XGBRegressor(objective = 'reg:squarederror', \n                        tree_method='gpu_hist')\n\nXGBReg_t01 =  XGBRegressor(objective = 'reg:squarederror', \n                        colsample_bytree = 0.7, \n                        learning_rate = 0.01, \n                        max_depth = 10, \n                        min_child_weight = 5, \n                        n_estimators = 500, \n                        subsample = 0.5,\n                        seed=27,\n                        tree_method='gpu_hist')\n\nXGBReg_t02 =  XGBRegressor(learning_rate=0.01,\n                           n_estimators=3000,\n                           max_depth=5, \n                           min_child_weight=0,\n                           gamma=0, \n                           subsample=0.7,                                \n                           colsample_bytree=0.7,                                     \n                           objective='reg:squarederror',                                \n                           scale_pos_weight=1, \n                           seed=27,                                     \n                           reg_alpha=0.00006,\n                           tree_method='gpu_hist')","80cd71eb":"xgb_model_t01 = XGBReg_t01\nxgb_model_t02 = XGBReg_t02\n\n%time xgb_model_t01.fit(X_train_001, y_train_001, early_stopping_rounds=10, eval_set=[(X_val, y_val)], verbose=False)\n%time xgb_model_t02.fit(X_train_001, y_train_001, early_stopping_rounds=10, eval_set=[(X_val, y_val)], verbose=False)\n\ny_pred_xgb_t01 = xgb_model_t01.predict(X_val)\ny_pred_xgb_t02 = xgb_model_t02.predict(X_val)\n\nmae_xgb_t01 = mean_absolute_error(y_val, y_pred_xgb_t01)\nmae_xgb_t02 = mean_absolute_error(y_val, y_pred_xgb_t02)\n\nprint(\"MAE t01: \", mae_xgb_t01)\nprint(\"MAE t02: \", mae_xgb_t02)","151d459a":"y_pred = 0.5*y_pred_xgb_t01 + 0.5*y_pred_xgb_t02\n\nmae_xgb = mean_absolute_error(y_val, y_pred)","c4b80608":"print(mae_xgb)","c83d2bab":"#Plot Real vs Predict\nplt.scatter(X_val['GrLivArea'], y_val,          color='blue', label='Real',    alpha=0.5)\nplt.scatter(X_val['GrLivArea'], y_pred,  color='red' , label='Predict', alpha=0.5)\nplt.title(\"Real vs Predict\")\nplt.legend(loc='best')\nplt.show()","93bae0f2":"#Feature importance \nfor model in [xgb_model_t01, xgb_model_t02]:\n    xgb.plot_importance(model, max_num_features=20)\n    plt.title(\"xgboost.plot_importance(model)\")\n    plt.show()","5b8d837f":"X_test = test_005\n\n# Use the model to make predictions\ny_pred_test_t01 = xgb_model_t01.predict(X_test)\ny_pred_test_t02 = xgb_model_t02.predict(X_test)\n\ny_pred_test = 0.4*y_pred_test_t01 + 0.6*y_pred_test_t02\n\nsubmission = pd.DataFrame({'Id':test_id,'SalePrice':y_pred_test})\n\n# Save results\nsubmission.to_csv(\"submission.csv\",index=False)","cbad85f8":"# 7. Plot Results","adff5b24":"### Best Fit","64e2ea04":"# 8. Predic Test & Submission","d8b4e3a2":"# 4. Split dataframe","7c8887ee":"## 5.2 XGBoost","884f1214":"## 2.2 Encoding ordinal features","fb1de0ba":"## 2.1 Fill NaN values","b987487a":"### Parameters","c9ab2327":"# 6. Fit Models","62bfa1b7":"# XGBoost & Hyperparameter tuning\n\n\n* [1. Loading and Inspecting Data](#2.-Loading-and-Inspecting-Data)\n* [2. Data preprocessing](#3.-Data-preprocessing)\n* [2.1 Fill NaN values](#3.1-Fill-NaN-values)\n* [2.2 Encoding ordinal features](#3.2-Encoding-ordinal-features)\n* [2.3 Encode nominal features](#3.3-Encode-nominal-features)\n* [3. Feature Engineering](#4.-Feature-Engineering)\n* [4. Normalize](#5.-Normalize)\n* [5. Fit Models](#6.-Fit-Models)\n* [5.1 Base line model](#6.1-Base-line-model)\n* [5.2 XGBoost](#6.2-XGBoost)\n    * [Parameters](#Parameters)\n    * [Tuning the hyper-parameters](#Tuning-the-hyper-parameters)\n    * [Best Fit](#Best-Fit)\n* [6. Compare Models](#7.-Compare-Models)\n* [7. Plot Results](#8.-Plot-Results)\n* [8. Predic Test & Submission](#9.-Predic-Test-&-Submission)\n\n\n\n\n\n<br>Reference:<\/br>\n<br>https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingRegressor.html\n<br>https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\n<br>https:\/\/scikit-learn.org\/stable\/modules\/grid_search.html#multimetric-grid-search\n<br>https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter","438f0bc6":"### Best Params\n{'colsample_bytree': 0.7,\n 'learning_rate': 0.01,\n 'max_depth': 10,\n 'min_child_weight': 5,\n 'n_estimators': 500,\n 'subsample': 0.5}\n <br>\n {'colsample_bytree': 1,\n 'learning_rate': 0.05,\n 'max_depth': 8,\n 'min_child_weight': 0,\n 'n_estimators': 700,\n 'objective': 'reg:squarederror',\n 'subsample': 0.7}\n <br>\n {'colsample_bytree':0.01, 'n_estimators':3460,\n                                     'max_depth':3, 'min_child_weight':0,\n                                     'gamma':0, 'subsample':0.7,\n                                     'colsample_bytree':0.7,\n                                     'objective':'reg:linear', 'nthread':-1,\n                                     'scale_pos_weight':1, 'seed':27,\n                                     'reg_alpha':0.00006)","1a6af18a":"# 2. Data preprocessing\n\n<ul>\n    <li>First I'll replace the numeric missing values (NaN's) with 0 and non numeric with none.\n    <li>Create Dummy variables for the categorical features.\n    <li>transform the skewed numeric features by taking log(feature + 1) - this will make the features more normal.\n<\/ul>","930945a3":"# 5. Outlier Detection\n\nPerhaps the most important hyperparameter in the model is the \u201ccontamination\u201d argument, which is used to help estimate the number of outliers in the dataset. This is a value between 0.0 and 0.5 and by default is set to 0.1.","ccd715b6":"# 3. Feature Engineering","15858184":"# 1. Loading and Inspecting Data","da47d11f":"<b>Default parameters<\/b>\n<br>max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='reg:squarederror', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, importance_type='gain'","9c197f96":"## 2.3 Encode nominal features","e7749721":"**GridSearchCV params:**\n* **estimator:** estimator object\n* **param_grid :** dict or list of dictionaries\n* **scoring:** A single string or a callable to evaluate the predictions on the test set. If None, the estimator\u2019s score method is used.\n    * https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n* **n_jobs:** Number of jobs to run in parallel. None means. -1 means using all processors.\n* **cv:** cross-validation, None, to use the default 3-fold cross validation. Integer, to specify the number of folds in a (Stratified)KFold.","9fea0f4f":"# 7. Join models"}}