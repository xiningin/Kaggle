{"cell_type":{"4e377b24":"code","2384612d":"code","16d41fa6":"code","2cf96217":"code","2fc25f83":"code","036d58cc":"code","92763e19":"code","65ca6830":"code","88b59ed7":"code","9e1db8d2":"code","8eb8a43f":"code","befe68b5":"code","4eb24210":"code","a22ab9fe":"code","52af1fa9":"code","c1c499b7":"code","0d53640d":"code","87e4f3e9":"code","76d2e38f":"code","d48c307c":"code","40907ec5":"code","39dcb5c9":"code","6fca6de1":"markdown","ad6a3c21":"markdown","2424087e":"markdown","872d5657":"markdown","dcf71414":"markdown","3b2ea1a8":"markdown","667a911e":"markdown","aa3ec07d":"markdown","c3344c29":"markdown","eeffcd8d":"markdown","fe7a63c5":"markdown","d66f6797":"markdown","f7c4545b":"markdown","872c0390":"markdown","845a8cf0":"markdown","3846c84b":"markdown","43fe4c5b":"markdown","02da8bde":"markdown","ad6d9670":"markdown","4a06e582":"markdown","86119ab6":"markdown","9c2077cc":"markdown","49b4868e":"markdown","34649fdb":"markdown","97b2564b":"markdown","9e8568c4":"markdown","b7106a17":"markdown","f58eff83":"markdown","c0a7d452":"markdown","f28647ea":"markdown","e52b3df9":"markdown","97bf90a8":"markdown","fc692320":"markdown","42f9edfa":"markdown","27b98105":"markdown","3313db73":"markdown","753ed7a9":"markdown","c18affad":"markdown","eddd90e9":"markdown","62880756":"markdown","62415fec":"markdown","8fc5aeea":"markdown","a60786a6":"markdown","ba00cd66":"markdown","564cc844":"markdown","243a5f6c":"markdown","0e4a77ab":"markdown","5f0d9ad6":"markdown","74f6de36":"markdown","2895d4e9":"markdown","562a0b37":"markdown","557bb72c":"markdown","6db2f15c":"markdown","b669cee6":"markdown","fac8a70b":"markdown","7565eb4d":"markdown","d955c868":"markdown","c3445964":"markdown","d2a2468b":"markdown","7ccb3f07":"markdown","26aac217":"markdown","88e29dc3":"markdown","65af87d5":"markdown","3924e8c8":"markdown","5a11b1cf":"markdown","af3bdd3c":"markdown","83c2cb0f":"markdown","54145459":"markdown","fbf4c84b":"markdown","8cf36267":"markdown","a55ac56f":"markdown","74969c94":"markdown","4d93e3c1":"markdown","00a973a8":"markdown","7a0d35ac":"markdown","6bfff146":"markdown","d43f6cf2":"markdown","9a42e29a":"markdown","1fb316bd":"markdown","1e60cdba":"markdown","45d3de03":"markdown","c07ba910":"markdown","b77d36fd":"markdown","f0b92af6":"markdown","e33fa990":"markdown","dee624e3":"markdown","73da1d4a":"markdown","95712fff":"markdown","fb6e8024":"markdown","1cb9f789":"markdown","a61ce764":"markdown","95b435be":"markdown","bfd0c1dc":"markdown","5100afbf":"markdown","ebaa02b7":"markdown","e3c2e075":"markdown","7e0f54e1":"markdown","05ff1d6b":"markdown","b97dd29a":"markdown","f9c394c9":"markdown","f16c6d07":"markdown","ae5a9c9a":"markdown","d389a7d3":"markdown","bdaa41c3":"markdown","cedf6d8b":"markdown","8221b445":"markdown","9965c142":"markdown","7333a3ac":"markdown","42c2d610":"markdown","c1f73fea":"markdown","8a8ad000":"markdown","29e48353":"markdown","782ce033":"markdown","7f50ca0f":"markdown","2251837a":"markdown","3f10e554":"markdown","c89bea96":"markdown","09055db3":"markdown","cf5fdc43":"markdown","244e4e7b":"markdown","990ad11e":"markdown","10da3348":"markdown","a79d4690":"markdown","3c2ba370":"markdown","d16723b3":"markdown","e9cd9941":"markdown","d821deb2":"markdown","fef55d85":"markdown","5aee4e23":"markdown","06fd3463":"markdown","29d90a20":"markdown","f13cfd75":"markdown","123a5a82":"markdown","c07f3832":"markdown","e372cdd7":"markdown","ea7d2e74":"markdown","a66cb2f4":"markdown","ad84ef2d":"markdown","c7e8e22d":"markdown","16255eb3":"markdown","f1544074":"markdown","9e40e1bd":"markdown","11c62571":"markdown","f428aa47":"markdown"},"source":{"4e377b24":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimg = cv2.imread('..\/input\/cmm536-week-4\/logo.png', 0)\nplt.imshow(img,'gray')","2384612d":"print(\"Shape of original image: \", img.shape)\nimg_vector = img.flatten()\nprint(\"Flattened image: \", img_vector)\nprint(\"Shape of flattened image. \", img_vector.shape)","16d41fa6":"# creating a variable to store the dataset\nrepo = img_vector.copy()\n# Importing and showing a new image\nimg2 = cv2.imread('..\/input\/cmm536-week-4\/logotrans.png', 0)\nplt.imshow(img2,'gray')\nprint(img2.shape)","2cf96217":"# Flattening the second image\nimg_vector2 = img2.flatten()\n# stacking the second vector created into our repository\nrepo = np.vstack((repo,img_vector2))\n# Printing the repo\nprint('Image repo: ',repo)\nprint('Shape of image repo: ', repo.shape)","2fc25f83":"# Importing and showing a third image\nimg3 = cv2.imread('..\/input\/cmm536-week-4\/logoaltered.png', 0)\nplt.imshow(img3,'gray')\nprint(img3.shape)","036d58cc":"# Resizing the image\nimg3 = cv2.resize(img3, (75, 75))\nprint('New dimensions of the image: ', img3.shape)\n# Flattening the third image\nimg_vector3 = img3.flatten()\n# appending the third vector created\nrepo = np.vstack((repo,img_vector3))\n# Printing the array\nprint(repo, repo.shape)","92763e19":"img = cv2.imread('..\/input\/cmm536-week-4\/symbol.jpg')\nimggray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\nplt.imshow(imggray,'gray')","65ca6830":"# Convert the grayscale image into float32 \n# Harris Corners works better on this numeric data type\nimggray = np.float32(imggray)\n# Run the algorithm with some predefined parameters\ndst = cv2.cornerHarris(imggray,2,3,0.04)\n# Show the result (as an array and as an image)\nprint(dst)\nplt.imshow(dst)","88b59ed7":"# The resulting image is dilated to better mark the corners\ndst = cv2.dilate(dst,None)\nplt.imshow(dst)","9e1db8d2":"# Threshold for an optimal value to find the corners\nimg[dst>0.01*dst.max()]=[0,0,255]\n# Show the final image\nplt.imshow(img)","8eb8a43f":"# The image is thresholded for the optimal value\n_, dst = cv2.threshold(dst,0.01*dst.max(),255,0)\n# Show the final image\nplt.imshow(dst)","befe68b5":"# find centroids and stats\n# Centroids: (x,y) location of the corners\n# Stats: 5 numbers that indicate the position of the centroid, width, height and area\ndst = np.uint8(dst)\n_, _, stats, centroids = cv2.connectedComponentsWithStats(dst)\nprint('Total number of corners found:', len(centroids))\nprint('Centroids found: ')\nprint(centroids)\nprint('Stats for each centroid found: ')\nprint(stats)","4eb24210":"# Load the image in colour again to draw over it\nimg = cv2.imread('..\/input\/cmm536-week-4\/symbol.jpg')\n# Draw circles wherever a corner (i.e. centroud) has been found\ncentroids = np.uint8(centroids)\nfor centroid in centroids:\n    img = cv2.circle(img,tuple(centroid),int(img.shape[0]\/40),(255,0,0), -1)\n# Show the image\nplt.imshow(img)","a22ab9fe":"## Creating a feature vector from the Harris Corners features extracted\nprint('Feature vector: ', stats.flatten())\nprint('Size of feature vector: ', stats.flatten().shape)","52af1fa9":"## Obtaining the HOG gradients of an image\nfrom skimage import feature\nclass HOG:\n    def __init__(self, orientations = 9, pixelsPerCell = (8, 8),\n        cellsPerBlock = (2, 2), transform = False):\n        self.orienations = orientations\n        self.pixelsPerCell = pixelsPerCell\n        self.cellsPerBlock = cellsPerBlock\n        self.transform = transform\n    def describe(self, image):\n        hist = feature.hog(image, orientations = self.orienations,\n            pixels_per_cell = self.pixelsPerCell,\n            cells_per_block = self.cellsPerBlock,\n            transform_sqrt = self.transform)\n        return hist\nimg = cv2.imread('..\/input\/cmm536-week-4\/usain.jpg',0)\nhog = HOG(orientations = 9, pixelsPerCell = (8, 8), cellsPerBlock = (2, 2), transform = True)\nhist = hog.describe(img)\nprint('HOG Features for the image:')\nprint(hist)\nprint('Size of the HOG Features for the image:', hist.shape)","c1c499b7":"import cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Read image\noriginal_image = cv2.imread('..\/input\/cmm536-week-4\/nomask.jpg')\n# show converted because cv uses BGR, not RGB\nplt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))","0d53640d":"# Convert color image to grayscale for Viola-Jones\ngrayscale_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\n# We use \"gray\" to specify to plt.imshow that the image is grayscale\nplt.imshow(grayscale_image, 'gray')","87e4f3e9":"# Load the classifier and create a cascade object for face detection\nface_cascade = cv2.CascadeClassifier('..\/input\/cmm536-week-4\/haarcascade_frontalface_alt.xml')\n# Detect faces\ndetected_faces = face_cascade.detectMultiScale(grayscale_image)\nprint(detected_faces)","76d2e38f":"# Put rectangles in the images\nfor (column, row, width, height) in detected_faces:\n    cv2.rectangle(original_image,(column, row),\n        (column + width, row + height),\n        (0, 255, 0),2)","d48c307c":"plt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))","40907ec5":"original_image = cv2.imread('..\/input\/cmm536-week-4\/facemask.jpg')\ngrayscale_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\nface_cascade = cv2.CascadeClassifier('..\/input\/cmm536-week-4\/haarcascade_frontalface_alt.xml')\ndetected_faces = face_cascade.detectMultiScale(grayscale_image)\nprint(detected_faces)\nfor (column, row, width, height) in detected_faces:\n    cv2.rectangle(original_image,(column, row),\n        (column + width, row + height),\n        (0, 255, 0),2)\nplt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))","39dcb5c9":"original_image = cv2.imread('..\/input\/cmm536-week-4\/facemask2.jpg')\ngrayscale_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2GRAY)\nface_cascade = cv2.CascadeClassifier('..\/input\/cmm536-week-4\/haarcascade_frontalface_alt.xml')\ndetected_faces = face_cascade.detectMultiScale(grayscale_image)\nprint(detected_faces)\nfor (column, row, width, height) in detected_faces:\n    cv2.rectangle(original_image,(column, row),\n        (column + width, row + height),\n        (0, 255, 0),2)\nplt.imshow(cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB))","6fca6de1":"When we create our feature repository, it will be uneven as some images may have more features (i.e. columns) than others","ad6a3c21":"A simple classifier would right away deduct that, if there are eyes and nose, then there must be a face!","2424087e":"##### Assumptions of Semi-supervised Learning","872d5657":"#### Semi-supervised Learning","dcf71414":"K-means clustering","3b2ea1a8":"`Cluster`: Data naturally forms discrete clusters, most common to share label","667a911e":"* Artificial Neural Network (ANN)","aa3ec07d":"Can we project the same logic to the computer?","c3344c29":"Then, we download the **frontalface** .xml model from [Github](https:\/\/github.com\/opencv\/opencv\/tree\/master\/data\/haarcascades), load it here and use it to process the image using the `detectMultiScale` function","eeffcd8d":"#### Supervised Learning","fe7a63c5":"* E and F are corners of the building, and can be easily found based on their **patch**","d66f6797":"* Why this size?","f7c4545b":"Finally, we show the original image with the rectangle","872c0390":"* The \"correct\" output will be deduced from the `training` data, therefore the model requires a reliable base","845a8cf0":"Then, we convert the image to grayscale so that we can apply the Viola-Jones method","3846c84b":"It is also why faces in statues and paintings get recognised!","43fe4c5b":"Taking a simpler example you can see that **corners** are usually the most intuitive **structural features**","02da8bde":"This is the algorithm most commonly used in commercial cameras (it is fast and easy to use)","ad6d9670":"First, we need to import an image","4a06e582":"#### Unsupervised Learning","86119ab6":"Data is usually split into `training`, `testing` and `validation` data","9c2077cc":"**HOW MANY MORE FEATURE EXTRACTORS EXIST??**","49b4868e":"They also have more hyperparameters, so you need to consider a larger validation set","34649fdb":"Problem: Loss of data","97b2564b":"# Week 4 - Feature Extraction and Learning","9e8568c4":"How about now?\n![Fig. 13. Facemask example 2](https:\/\/www.dropbox.com\/s\/594o736z80aauxm\/facemask2.jpg?raw=1)","b7106a17":"* Done in the context of **classification** (when mapping input to output label) or **regression** (when mapping input to continuous output)","f58eff83":"Notice that we get four numbers! These are the coordinates where the face is found.","c0a7d452":"Finds inherent patterns of data","f28647ea":"* Then the image is divided in patches and the gradients are calculated","e52b3df9":"### Papers\n\n* C. Harris and M. Stephens, \u201cA Combined Corner and Edge Detector,\u201d Alvey Vision Conference, 1988.\n* N. Dalal and B. Triggs, \u201cHistograms of oriented gradients for human detection,\u201d Computer Vision and Pattern Recognition, vol. I, pp. 886\u2013893, 2005.\n* D. G. Lowe, \u201cObject recognition from local scale-invariant features,\u201d International Conference on Computer Vision (ICCV), vol. 2, no. 8, pp. 1150\u20131157, 1999.\n* H. Bay, T. Tuytelaars, and L. Van Gool, \u201cSURF: Speeded up robust features,\u201d Lecture Notes in Artificial Intelligence, vol. 3951, pp. 404\u2013417, 2006.","97bf90a8":"##### Examples of Supervised Learning Algorithms","fc692320":"* A and B are flat surfaces and difficult to find as an exact match","42f9edfa":"Sometimes is useful to **binarise** images prior to importing them to:\n    - Reduce the values for the features\n    - Increase quality\/standardise samples","27b98105":"![Fig. 6. Different Feature Extractors](https:\/\/www.dropbox.com\/s\/nze09qcgocx5mv1\/FEs.jpg?raw=1)","3313db73":"**Testing:** Provide an unbiased evaluation of a final model","753ed7a9":"The computer will look for specific **patterns** which are \n    - unique\n    - easy to track\n    - easy to compare","c18affad":"**Solution 2:** We could also use Harris Corners just as the **detector** and then use another algorithm as the **feature extractor**","eddd90e9":"This representation is typically not **rotation invariant** nor **structurally** representative of the images","62880756":"Problem: Incongruent!","62415fec":"* We have found an approximate location of the corners, but how useful is that for classification purposes?","8fc5aeea":"![Fig. 4. Simple example of feature detection](https:\/\/www.dropbox.com\/s\/w3wxoqkd4mfzii5\/feature_simple.jpg?raw=1)","a60786a6":"### Harris Corners","ba00cd66":"#### The problem with Harris Corners in Machine Learning","564cc844":"Label Propagation","243a5f6c":"This algorithm looks for changes on intensity from a pixel with respect to it's neighbours","0e4a77ab":"`Manifold`: Data lies in a lower dimensional space than the input space","5f0d9ad6":"**Solution:**Limit Harris Corners to find best $n$ features and only use those to build our data repository","74f6de36":"Notice that in this case, we obtain a fixed-size feature vector provided that the input has the same size","2895d4e9":"Before testing all of these methods, it is important to split the data into sets","562a0b37":"* Naive Bayes Classifier","557bb72c":"If a new image with a different size has to be added to `repo`, then we can use the `resize` function in `OpenCV`","6db2f15c":"Think about any data science problem that you have faced so far","b669cee6":"Which are the **best** features to do so!","fac8a70b":"However, if we intend to use this repository for **classification** purposes, then **all images should be of the same size**!","7565eb4d":"* Aims to learn a function that, given a sample of data and desired outputs, approximates a function that maps inputs to output","d955c868":"Can we project the same logic to data science as a whole?","c3445964":"The most basic features in an image","d2a2468b":"## LAB 4: \"FEATURE\" BASED DATA CLASSIFICATION","7ccb3f07":"The technique counts occurrences of gradient orientation in localised portions of an image","26aac217":"`OpenCV` comes with a ready to use Haar cascade!","88e29dc3":"We can do a method to \"draw\" green rectangles over the faces as follows:","65af87d5":"As humans, we use those features to recognise other individuals","3924e8c8":"Machine learning: An automatic function that maps $x \\rightarrow y$ based on the input data","5a11b1cf":"* Linear\/logistic Regression","af3bdd3c":"## Features for Face Recognition","83c2cb0f":"Depends on two things: **total number of samples** and **type of model trained**","54145459":"* C and D are simpler (edges of the building), however their exact location is difficult to find","fbf4c84b":"It uses something called [Haar-like features](https:\/\/realpython.com\/traditional-face-detection-python\/) to detect edges (1 and 2), lines (3) and diagonals (4)\n![Fig. 7. Haar-like features](https:\/\/www.dropbox.com\/s\/hijnqhik9jll6l3\/haar.png?raw=1)","8cf36267":"Most common tasks: **clustering** and **exploratory data analysis**","a55ac56f":"## Machine Learning","74969c94":"If we work with a repository of images, it is most likely that each image obtains a different amount of corners","4d93e3c1":"#### How to Split your Data (recommendations)","00a973a8":"Learning with both unlabelled and labeled data points","7a0d35ac":"### Histogram of Oriented Gradients (HOG)","6bfff146":"In simple terms: It converts a $64 \\times 128 \\times 3$ image into a **feature vector** of length $3780$","d43f6cf2":"* Suppose we have a repository of thousands of images to classify:\n    * Could we train the classifier simply by entering the numerical values of the $(x,y)$ coordinates?","9a42e29a":"Used when large amounts of data are costly to label","1fb316bd":"### Pixels as features","1e60cdba":"## Feature detection and extraction algorithms","45d3de03":"**MANY, MANY, MAAAAAAAAANY OTHERS!!!**","c07ba910":"`Continuity`: Data points that are \"close\" have a common label","b77d36fd":"Aims to label unlabelled data points using knowledge learned from a small number of labelled data points","f0b92af6":"It turns out that when overlapping these features in a human face, it can help us detect areas of interest","e33fa990":"* Support Vector Machine (SVM)","dee624e3":"Still, they are widely used in machine learning! **Why?!**","73da1d4a":"For example, *mask* (2) can help us identify the area with the eyes\n![Fig. 8. Eye detection](https:\/\/www.dropbox.com\/s\/u70jfji3h9svq0k\/haareyes.png?raw=1)","95712fff":"Now we need to `flatten` the image so that it is represented as a vector","fb6e8024":"Now using the image or the centroids, we will create a **vector of features** that describes the image","1cb9f789":"Like atoms for matter!","a61ce764":"* Understand the basic notions before applying algorithms to classify different types of problems","95b435be":"Conversely, *mask* (3) can help us find the nose\n![Fig. 9. Nose detection](https:\/\/www.dropbox.com\/s\/z70sno7xfckidat\/haarnose.png?raw=1)","bfd0c1dc":"![Fig. 5. HOG example](https:\/\/www.dropbox.com\/s\/aonf8kxg5fy36cg\/HOG.jpg?raw=1)","5100afbf":"##### Examples of Semi-supervised Learning Algorithms","ebaa02b7":"### Websites\n\n* https:\/\/opencv-python-tutroals.readthedocs.io\/en\/latest\/py_tutorials\/py_feature2d\/py_features_meaning\/py_features_meaning.html#exercises\n* https:\/\/opencv-python-tutroals.readthedocs.io\/en\/latest\/py_tutorials\/py_feature2d\/py_features_harris\/py_features_harris.html\n* https:\/\/stackoverflow.com\/questions\/35854197\/how-to-use-opencvs-connected-components-with-stats-in-python\/py_features_harris.html\n* https:\/\/en.wikipedia.org\/wiki\/Histogram_of_oriented_gradients\n* https:\/\/www.learnopencv.com\/histogram-of-oriented-gradients\/\n* https:\/\/towardsdatascience.com\/understanding-the-different-types-of-machine-learning-models-9c47350bb68a\n* https:\/\/towardsdatascience.com\/train-validation-and-test-sets-72cb40cba9e7\n* https:\/\/en.wikipedia.org\/wiki\/Viola%E2%80%93Jones_object_detection_framework\n* https:\/\/realpython.com\/traditional-face-detection-python\/\n* https:\/\/github.com\/opencv\/opencv\/tree\/master\/data\/haarcascades","e3c2e075":"**Make sure that all labels are represented in each set!**","7e0f54e1":"Then why wouldn't a machine use them as well?!","05ff1d6b":"## Features","b97dd29a":"If a $75 \\times 75$ image yielded $5'625$ features (pixels), imagine with larger images!","f9c394c9":"You have to analyse a series of data records\/entries, each possessing a fixed amount of attributes to describe them ","f16c6d07":"Do you think this works with a facemask?\n![Fig. 12. Facemask example 1](https:\/\/www.dropbox.com\/s\/hu4idfcm07xk528\/facemask.jpg?raw=1)","ae5a9c9a":"This results on the algorithm being mostly capable of finding the corners of shapes","d389a7d3":"##### Examples of Unsupervised Learning Algorithms","bdaa41c3":"#### Issues with using pixels as features ","cedf6d8b":"Notice that we can append more images to create a larger repository","8221b445":"Transductive SVM","9965c142":"## Resources for the Lecture","7333a3ac":"As more images get imported, we can create an **image repository** by appending new images into a `numpy` array","42c2d610":"![Fig. 3. Find the features](https:\/\/www.dropbox.com\/s\/nvizljjmuh204ul\/feature_building.jpg?raw=1)","c1f73fea":"Some models (e.g. ANN) need substantial training data","8a8ad000":"Autoencoders","29e48353":"Does not have (or need) any labelled outputs, so its goal is to infer the natural structure present within a set of data points","782ce033":"Approaches like this are much better suited for machine learning compared to classical feature extractors such as Harris Corners). **WHY?**","7f50ca0f":"### Types of Machine Learning","2251837a":"* Understand how images are converted into a set of features used for machine learning purposes","3f10e554":"![Fig. 2. Jigsaw](https:\/\/www.dropbox.com\/s\/659ta4spioz8pea\/jigsaw.jpg?raw=1)","c89bea96":"![Fig. 1. An example of data](https:\/\/www.dropbox.com\/s\/rpwcnj7fzygsfdr\/data.jpg?raw=1)","09055db3":"The human face innately has features!\n    - nose\n    - eyes\n    - etc","cf5fdc43":"Moreover, not all pixels contain valuable information","244e4e7b":"**Training:** The samples used to fit the model","990ad11e":"![Fig. 15. Typical Data Split](https:\/\/www.dropbox.com\/s\/oze1q3wj7d71pa1\/traintestvalid.jpg?raw=1)","10da3348":"In this case, they use a **cascading classifier** (you will sometimes find this in literature as Haar cascade)\n![Fig. 10. Basics of a cascade classifier](https:\/\/www.dropbox.com\/s\/z98smtkkndyk0yq\/cascade.png?raw=1)","a79d4690":"**Validation:** Provide an unbiased evaluation of the model while tuning hyperparameter, as skill of this validation is incorporated in the model","3c2ba370":"* The method calculates the $x$ and $y$ gradients of the image, as well as the magnitude","d16723b3":"### Splitting Data for Classification","e9cd9941":"### Pattern Recognition","d821deb2":"* We most likely need to extract some more useful **features**","fef55d85":"## Aims of the Session","5aee4e23":"This can get more complex (but also more robust) as more classifiers are used\n![Fig. 11. Full cascade classifier](https:\/\/www.dropbox.com\/s\/ndz2mvq3xp09jol\/haarcascade.png?raw=1)","06fd3463":"* This will help us understand the performance of the model prior to its deployment","29d90a20":"### Features in images","f13cfd75":"An image is converted into a **vector** where each column represents a feature (pixel intensity)","123a5a82":"![Fig. 14. Fundamentals of Machine Learning](https:\/\/www.dropbox.com\/s\/shrjabuib4m7nf5\/ml.jpg?raw=1)","c07f3832":"It would take me a whole module to explain all the details of this method, the only thing you need to know is that it is an **end-to-end one**, this means that authors not only proposed the feature extraction, but also the classifier!","e372cdd7":"Still, classical feature extractors are widely used since these are capable to represent **structure**","ea7d2e74":"* Decision Tree\/Random Forest","a66cb2f4":"There are other good features, such as blobs, changes of intensity, etc.","ad84ef2d":"Principal Component Analysis (PCA)","c7e8e22d":"One of the first and most famous frameworks for face detection was presented in 2001 by Paul Viola and Michael Jones, often referred to as the [Viola-Jones](https:\/\/en.wikipedia.org\/wiki\/Viola%E2%80%93Jones_object_detection_framework) object detection framework","16255eb3":"How do you assemble a jigsaw puzzle?","f1544074":"#### Feature detection $\\neq$ feature extraction","9e40e1bd":"Now that we understand how to produce features, we can input them into a **machine learning model**","11c62571":"Features are the most important component of data science","f428aa47":"Take a look at this image and find the features:"}}