{"cell_type":{"9943c56e":"code","f440dbc4":"code","abae6314":"code","f5f9ccbc":"code","dfd4a9c6":"code","776d3b9a":"code","145a1e7c":"code","5fae0d98":"code","a1b828bb":"code","22342fec":"code","1c2e0abb":"code","5585c5fe":"code","6a6b57fc":"code","01929b7a":"code","96c079f1":"code","f070445e":"code","f86a697d":"code","8293d7d5":"code","584fb54e":"code","732b511b":"code","aaa45161":"code","b9a94711":"code","c9a8f61f":"code","c3729b94":"code","58ea2fc4":"code","050c156b":"markdown"},"source":{"9943c56e":"!pip install transformers==3.0.2","f440dbc4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport gc\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","abae6314":"import tensorflow as tf\nimport transformers\n\nfrom sklearn.model_selection import KFold","f5f9ccbc":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)","dfd4a9c6":"transformers.__version__","776d3b9a":"from transformers import AutoTokenizer, TFAutoModel","145a1e7c":"train_df = pd.read_csv(\"\/kaggle\/input\/contradictory-my-dear-watson\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/contradictory-my-dear-watson\/test.csv\")","5fae0d98":"epochs = 3\nmaxlen = 50\n\nmodel_name = \"jplu\/tf-xlm-roberta-large\"\n\nbatch_size = 16 * strategy.num_replicas_in_sync","a1b828bb":"tokenizer = AutoTokenizer.from_pretrained(model_name)","22342fec":"train_df.premise.values","1c2e0abb":"list(train_df.premise.values[:10]), list(train_df.hypothesis.values[:10])","5585c5fe":"%%time\ntrain_encode = tokenizer(list(train_df.premise.values), list(train_df.hypothesis.values), \n                      max_length=maxlen, return_tensors=\"np\", padding=True, \n                      return_token_type_ids=True, return_attention_mask=True)","6a6b57fc":"def get_model(maxlen=50):\n    \n    #base_model = TFDistilBertModel.from_pretrained(\"distilbert-base-multilingual-cased\")\n    \n    base_model = TFAutoModel.from_pretrained(model_name)\n    \n    input_ids = tf.keras.layers.Input(shape =(maxlen, ), dtype=tf.int32, name=\"input_ids\")\n    input_type = tf.keras.layers.Input(shape =(maxlen, ), dtype=tf.int32, name=\"token_type_ids\")\n    input_mask = tf.keras.layers.Input(shape =(maxlen, ), dtype=tf.int32, name=\"attention_mask\")\n    \n    \n    embedding = base_model([input_ids, input_mask, input_type])[0]\n    #embedding = base_model([input_ids, input_mask])[0]\n    \n    print(embedding.shape)\n    \n    output = tf.keras.layers.Dense(3, activation=\"softmax\")(embedding[:, 0, :])\n    \n    model = tf.keras.models.Model(inputs=[input_ids, input_mask, input_type], outputs = output)\n    \n    model.compile(tf.keras.optimizers.Adam(1e-5), \"sparse_categorical_crossentropy\", [\"accuracy\"])\n    \n    return model","01929b7a":"with strategy.scope():\n    cls_model = get_model(maxlen)\n    cls_model.summary()","96c079f1":"%%time\nps = cls_model([train_encode['input_ids'][:10], train_encode['attention_mask'][:10], train_encode['token_type_ids'][:10]])","f070445e":"fold = KFold(n_splits=3, shuffle=True, random_state=108)","f86a697d":"%%time\nhists = []\nmodels = []\nfor i, (train_idx, val_idx) in enumerate(fold.split(np.arange(train_df.label.shape[0]))):\n    print(f\"----FOLD: {i+1}----\\n\",train_idx, val_idx)\n    \n    \n    x_train = [train_encode['input_ids'][train_idx], \n               train_encode['attention_mask'][train_idx], \n               train_encode['token_type_ids'][train_idx]]\n    \n    y_train = train_df.label.values[train_idx]\n    \n    x_val = [train_encode['input_ids'][val_idx],\n             train_encode['attention_mask'][val_idx],\n             train_encode['token_type_ids'][val_idx]]\n    y_val = train_df.label.values[val_idx]\n    \n    \n    hist=cls_model.fit(x_train, y_train,\n                       epochs=epochs, \n                       batch_size = batch_size,\n                       validation_data = (x_val, y_val),\n                      )\n    hists.append(hist)\n    #models.append(cls_model)\n    \n    gc.collect()","8293d7d5":"gc.collect()","584fb54e":"%%time\ntest_encode = tokenizer(list(test_df.premise.values), list(test_df.hypothesis.values), \n                      max_length=maxlen, return_tensors=\"tf\", padding=True, \n                      return_token_type_ids=True, return_attention_mask=True)","732b511b":"\"\"\"\npreds = []\nfor model in models:\n    ps = model.predict([test_encode['input_ids'], test_encode['attention_mask'], test_encode['token_type_ids']],\n                      verbose=1, batch_size=batch_size)\n    preds.append(ps)\n\"\"\"","aaa45161":"#ps = np.mean(np.stack(preds), 0)","b9a94711":"ps = cls_model.predict([test_encode['input_ids'], test_encode['attention_mask'], test_encode['token_type_ids']],\n                      verbose=1, batch_size=batch_size)","c9a8f61f":"submission = test_df.id.copy().to_frame()\nsubmission['prediction'] = np.argmax(ps, 1)","c3729b94":"submission.head()","58ea2fc4":"submission.to_csv(\"submission.csv\", index = False)","050c156b":"[HF Tokeinzers](https:\/\/huggingface.co\/transformers\/main_classes\/tokenizer.html#transformers.PreTrainedTokenizerFast.__call__)"}}