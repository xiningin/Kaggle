{"cell_type":{"3ae316e6":"code","3a11c917":"code","ab036e0a":"code","5339efa5":"code","f5eedb20":"code","ad721dde":"code","21b15821":"code","190aaeca":"code","d316383f":"code","ef2ccb5c":"code","ca9777f4":"code","5e617ddc":"code","5a5fadcd":"code","331572a6":"code","5265581b":"code","df0c35f1":"code","ff6d14ad":"code","2f81f05c":"code","d0749e07":"code","450110d6":"code","c3b16dd2":"code","0c94eff6":"code","4310cacf":"code","a37b968f":"code","52bd2807":"code","2e7ec045":"code","a7071b41":"code","c60e01d5":"code","4c03c73f":"code","7740d46b":"code","df363690":"code","3ab69938":"code","253ae66e":"code","3f676bde":"code","29992533":"code","e3be46e4":"code","0b0f4518":"code","a59e897d":"markdown","11406f19":"markdown","f6148870":"markdown","8b46d934":"markdown","6fe36f68":"markdown","53bd7bd5":"markdown","afc431df":"markdown","bd600817":"markdown","602c1f3b":"markdown","16548106":"markdown","0dae04d6":"markdown"},"source":{"3ae316e6":"import os\nimport matplotlib.pyplot as plt\nimport numpy as np \nimport pandas as pd \nimport time\nfrom ml_metrics import apk as apk, mapk as mapk\nfrom itertools import product\nfrom collections import OrderedDict\nimport gc\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom collections import Counter\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nplt.style.use('fivethirtyeight')","3a11c917":"# custom AP@K metrics\ndef ap_k(arr1, arr2, k):\n\n    mean_k=[]\n    sim = list(map(lambda x: x[0] == x[1] , zip(arr1,arr2)))\n    \n    for i in range(1,k+1):\n        time_arr = sim[:i]\n        tr = time_arr.count(True)\n        fp = time_arr.count(False)\n        mean_k.append(tr\/(tr+fp))\n        \n    return np.mean(mean_k)","ab036e0a":"# custom MAP@K metrics\ndef map_k(actual, predicted, k):\n    return np.mean([ap_k(a,p,k) for a,p in zip(actual, predicted)])","5339efa5":"# functions used to compare our predicts with our test set\n# custom MAP@K and ml_metrics' MAP@K used\n# NaN values filled from top_5 most popular shows\ndef test_mapk(df_X, df_y, top_5, check_sets=True):\n\n    test_set = df_y.copy()\n    test_set = test_set.join(df_X, on='user_id', how='left', rsuffix='_pred')\n    fillna_with_top(test_set, 'tv_show_id_pred', top_5)\n    \n    if check_sets:\n        assert (test_set['tv_show_id'].apply(lambda x: len(set(x)) != len(x)).mean()) == 0\n        assert (test_set['tv_show_id'].isna().sum()) == 0\n        assert ((test_set['tv_show_id'].apply(len) != 5).mean()) == 0\n        assert (test_set['tv_show_id_pred'].isna().sum()) == 0\n        assert (test_set['tv_show_id_pred'].apply(lambda x: len(set(x)) != len(x)).mean()) == 0\n        assert ((test_set['tv_show_id_pred'].apply(len) != 5).mean()) == 0\n\n    print(\"MAPK:\", mapk(test_set['tv_show_id'], test_set['tv_show_id_pred'], 5))\n    print(\"MAP_K:\", map_k(test_set['tv_show_id'], test_set['tv_show_id_pred'], 5))","f5eedb20":"# most popular shows by user\ndef top_user_show(df):        \n    return df.groupby('user_id')['tv_show_id'].value_counts().reset_index(name='count').groupby(\n        'user_id').head(5).groupby(\n        'user_id')['tv_show_id'].apply(list).to_frame()","ad721dde":"# most popular shows by user with sorted shows by their overall popularity\ndef top_user_show_sorted(df):\n    \n    res = df.copy()\n    res = res.join(res.groupby(\n        ['user_id', 'tv_show_id']).size().to_frame(name='u_s_cnt'), \n                   on=['user_id', 'tv_show_id'], how='left') \n    res = res.join(res.groupby(\n        'tv_show_id').size().to_frame(name='global_cnt'),\n                   on='tv_show_id', how='left')\n    show_cnt = res.groupby(\n        ['user_id', 'tv_show_id'])[['u_s_cnt', 'global_cnt']].mean().reset_index().sort_values(\n        ['u_s_cnt', 'global_cnt'], ascending=False).groupby('user_id').head(5).groupby(\n        'user_id')['tv_show_id'].apply(list).to_frame()\n    \n    return show_cnt   ","21b15821":"# function that combines views of one show in one time period\n# for example, when the user switched the channel and then returned\ndef full_intercept(df):\n    df_full = df.groupby(['start_time_tv','channel_id','user_id'])['interception_time'].sum().reset_index()\n    df_full.columns = ['start_time_tv', 'channel_id', 'user_id', 'full_interception_time']\n    big_df = pd.merge(df_full, df, \n                      on=['start_time_tv', 'channel_id', 'user_id'])\n    big_df_no_dub = big_df.drop_duplicates(\n        subset=['start_time_tv', 'channel_id', \n                'user_id','full_interception_time']).drop(\n        'interception_time', axis = 1)\n    \n    return big_df_no_dub","190aaeca":"# calculate top shows overall\ndef top_shows(df, group_col, N):\n    return df.groupby(group_col).size().sort_values(\n        ascending=False).head(N).index.to_list()","d316383f":"def fillna_with_top(df, column, top):\n    df[column] = df[column].apply(lambda x: top if x is np.nan else x)","ef2ccb5c":"# fill missing shows for each user\n# OrderedDict used to save original order of shows for user\ndef fill_to_5(df, column, top):\n    df[column] = df[column].apply(lambda x: x if len(x) == 5 else \n                                  list(OrderedDict.fromkeys((x + top)))[:5])","ca9777f4":"# similarity matrix from users' ratings\ndef create_similarity_matrix(df):\n    pvt_table = pd.pivot_table(df, \n                               values='full_interception_time', \n                               columns=['tv_show_id'],\n                               index=['user_id'],\n                               aggfunc='count').fillna(0)\n    pvt_table = pvt_table.apply(lambda x: (x - x.min()) \/ (x.max() - x.min()), axis=0)\n    cos_sim = cosine_similarity(pvt_table)\n    np.fill_diagonal(cos_sim, 0)\n    cos_sim = pd.DataFrame(cos_sim, \n                           index = pvt_table.index, \n                           columns = pvt_table.index)\n\n    return pvt_table, cos_sim","5e617ddc":"# find most popular shows among similar users for user_id\ndef similar_shows(cos_sim_matrix, pvt_table, user_id, n_shows):\n    similar_user = cos_sim_matrix[user_id].nlargest(10).index\n    similar_user_ratings = pvt_table.loc[similar_user].stack().reset_index(name='rating')\n    similar_user_top = similar_user_ratings.sort_values('rating', ascending=False).groupby(\n        'user_id').head(10)\n    similar_shows = similar_user_top.groupby('tv_show_id')['rating'].sum().nlargest(n_shows)\n    \n    return similar_shows.index.to_list()","5a5fadcd":"# filling missing shows for each user with similar users' shows\n# OrderedDict used to save original order of shows for user\ndef fill_with_recommend(df, column, cos_sim_matrix, pvt_table):\n    df[column] = df.apply(lambda x: x[column] if len(x[column]) == 5 else \n                          list(OrderedDict.fromkeys((\n                              x[column] + \n                              similar_shows(cos_sim_matrix, pvt_table, x['user_id'], 10))))[:5], axis=1)","331572a6":"FULL_DATA_DIR = '\/kaggle\/input\/sweet-data'\nDATA_DIR = '\/kaggle\/input\/sweettv-tv-program-recommender'\n\nbig_data_df = pd.read_csv(os.path.join(FULL_DATA_DIR, 'big_data.csv'))\narh_1 = pd.read_csv(os.path.join(DATA_DIR, 'export_arh_11-20-final.csv'))\narh_2 = pd.read_csv(os.path.join(DATA_DIR, 'export_arh_21-30-final.csv'))\ntest_arh = pd.read_csv(os.path.join(DATA_DIR, 'export_arh_31-42-final.csv'))\nsubmission = pd.read_csv(os.path.join(DATA_DIR, 'submission.csv'))\n\narh = pd.concat([arh_1, arh_2], axis=0)\ntrain_arh = arh[arh['tv_show_id'] != 0].reset_index(drop=True).copy()\ntest_arh = test_arh[test_arh['tv_show_id'] != 0].reset_index(drop=True).copy()\n\nuniq_train = np.setdiff1d(train_arh['tv_show_id'].unique(), test_arh['tv_show_id'].unique())\nbig_data_df = big_data_df[~big_data_df['tv_show_id'].isin(uniq_train)]\ntrain_arh = train_arh[~train_arh['tv_show_id'].isin(uniq_train)]\n\ndel arh, arh_1, arh_2","5265581b":"big_data_df.columns = ['channel_id', 'start_time_tv', 'tv_show_id', 'stop_time_tv', 'trunk_day',\n                       'user_id', 'vsetv_id', 'start_time_user', 'stop_time_user', 'interception_time']\nbig_data_df.drop(columns=['trunk_day', 'vsetv_id'], inplace=True)\n\ndt_format = \"%Y-%m-%d %H:%M:%S\"\nbig_data_df['start_time_tv'] = pd.to_datetime(big_data_df['start_time_tv'], format=dt_format)\nbig_data_df['stop_time_tv'] = pd.to_datetime(big_data_df['stop_time_tv'], format=dt_format)\nbig_data_df['start_time_user'] = pd.to_datetime(big_data_df['start_time_user'], format=dt_format)\nbig_data_df['stop_time_user'] = pd.to_datetime(big_data_df['stop_time_user'], format=dt_format)\ntrain_arh['start_time'] = pd.to_datetime(train_arh['start_time'], format='%d.%m.%Y %H:%M:%S')\ntest_arh['start_time'] = pd.to_datetime(test_arh['start_time'], format='%d.%m.%Y %H:%M:%S')\n\nbig_data_df['duration'] = (big_data_df['stop_time_tv'] - big_data_df['start_time_tv']).dt.seconds\nbig_data_df['view_pct'] = (big_data_df['interception_time'] \/ big_data_df['duration'])\nbig_data_df['view_pct'].clip(0, 1, inplace=True)\n\ntrain_df = full_intercept(big_data_df)\n\nshow_cat = train_arh[['tv_show_id', 'tv_show_category']].drop_duplicates().set_index('tv_show_id')\ntrain_df = train_df.join(show_cat, on='tv_show_id', how='left')","df0c35f1":"train_df.groupby([pd.Grouper(key='start_time_user', \n                             freq='D')])['duration'].sum().plot(figsize=(10, 6))\nplt.title(\"Durations by days\")\nplt.show()","ff6d14ad":"print(train_arh['tv_show_category'].unique())\nprint(test_arh['tv_show_category'].unique())\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\n\n(train_arh.groupby('tv_show_category')['tv_show_id'].count() \/ \n train_arh.shape[0]).plot(kind='barh', ax=ax[0])\nax[0].set_title(\"Train categories\")\n(test_arh.groupby('tv_show_category')['tv_show_id'].count() \/ \n test_arh.shape[0]).plot(kind='barh', ax=ax[1])\nax[1].set_title(\"Test categories\")\n\nplt.tight_layout()\nplt.show()","2f81f05c":"print(\"Percent of shows with more than one channel\", \n      (train_arh.groupby('tv_show_id')['channel_id'].nunique() > 1).mean())\n(train_arh.groupby('tv_show_id')['channel_id'].nunique()).hist()\nplt.title(\"Shows broadcast\u0443\u0432 on different channels\")\nplt.show()","d0749e07":"final_df_X = train_df[train_df['start_time_user'].dt.month != 7].copy()\nfinal_df_y = train_df[train_df['start_time_user'].dt.month == 7].copy()\n\ntest_set = top_user_show(final_df_y[final_df_y['view_pct'] >= 0.8])\ntest_set = test_set[test_set['tv_show_id'].apply(len) == 5]\n\nprint(\"Duplicates in shows -\", test_set['tv_show_id'].apply(lambda x: len(set(x)) != len(x)).mean())\nprint(\"NaNs in shows -\", test_set['tv_show_id'].isna().sum())\nprint(\"Missing shows -\", (test_set['tv_show_id'].apply(len) != 5).mean())","450110d6":"# how many times each user watched TV\ndef prepare_time_df(df):\n    agg_df = df.groupby(['day_of_week', 'hour', 'user_id'])['interception_time'].count().reset_index()\n    \n    agg_df['week_hour'] = agg_df['day_of_week'].astype(str) + '_' + agg_df['hour'].astype(str)\n    \n    agg_df_piv = pd.pivot_table(agg_df[['user_id', 'interception_time', 'week_hour']], \n                   values='interception_time', \n                   columns=['user_id'],\n                   index=['week_hour']).fillna(0)\n    \n    scaler = MinMaxScaler()\n    \n    df_agg_piv_sc = pd.DataFrame(scaler.fit_transform(agg_df_piv), \n                             index = agg_df_piv.index, \n                             columns = agg_df_piv.columns)\n    \n    return df_agg_piv_sc","c3b16dd2":"# rating of the show calculated by user matrix\ndef prepare_show_df(df):\n    agg_df = df.groupby(['tv_show_id', 'user_id'])['interception_time'].count().reset_index()\n    agg_df_piv = pd.pivot_table(agg_df, \n                       values='interception_time', \n                       columns=['user_id'],\n                       index=['tv_show_id']).fillna(0)\n    scaler = MinMaxScaler()\n\n    df_agg_piv_sc = pd.DataFrame(scaler.fit_transform(agg_df_piv), \n                                 index = agg_df_piv.index, \n                                 columns = agg_df_piv.columns)\n    return df_agg_piv_sc","0c94eff6":"df = pd.read_csv('\/kaggle\/input\/sweet-data\/crop_data_1.csv')\n\ndf.start_time_tv = pd.to_datetime(df.start_time_tv)\ndf.stop_time_tv = pd.to_datetime(df.stop_time_tv)\n\ndf['day_of_week'] = df.start_time_tv.dt.dayofweek\ndf['hour'] = df.start_time_tv.dt.hour\n\ndf.head()","4310cacf":"show_df = prepare_show_df(df)\nshow_df.head()","a37b968f":"time_df = prepare_time_df(df)\ntime_df.head()","52bd2807":"# calculating shows' ratings for the user based on screen time and show's age\nfinal_df_X['view_rating'] = final_df_X['view_pct'].apply(lambda x: 5 if x==1\n                                                         else 2 + 9*x**2)\nT = 60\ncont = 0.4\nt_ref = (final_df_X['start_time_tv']).max()\nfinal_df_X['view_rating'] = (final_df_X['view_rating'] * \n                             np.power(cont, (t_ref - final_df_X['start_time_tv']).dt.days \/ T))","2e7ec045":"# using different datasets to test our methods\nfinal_df_X_08 = final_df_X[(final_df_X['view_pct'] >= 0.8)]\nfinal_df_X_08_cat = final_df_X_08[final_df_X_08['tv_show_category'].isin(['\u0420\u0430\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f', '\u0418\u043d\u0444\u043e', \n                                                                          '\u0421\u0435\u0440\u0438\u0430\u043b\u044b', '\u0414\u0435\u0442\u044f\u043c', \n                                                                          '\u041f\u043e\u0437\u043d\u0430\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0435', \n                                                                          '\u0414\u0440\u0443\u0433\u043e\u0435'])]\n\nfinal_df_X_08_cat_dur = final_df_X_08_cat[(final_df_X_08_cat['duration'] < 10000)]","a7071b41":"show_cnt_X_08_cat = top_user_show_sorted(final_df_X_08_cat)\nshow_cnt_X_08_cat_dur = top_user_show(final_df_X_08_cat_dur)\nshow_cnt_new = top_user_show_sorted(final_df_X_08_cat_dur)","c60e01d5":"# top shows used for filling NaNs and other missing data\ntop_5_shows = top_shows(final_df_X_08_cat, 'tv_show_id', 5)\ntop_10_shows = top_shows(final_df_X_08_cat, 'tv_show_id', 10)","4c03c73f":"X = {'show_cnt_new': show_cnt_new,\n     'final_df_X_08_cat_cnt': show_cnt_X_08_cat}\n\n\nfor name, ds_X in X.items():\n    print(name)\n    print(\"Duplicates in shows -\", ds_X['tv_show_id'].apply(lambda x: len(set(x)) != len(x)).mean())\n    print(\"NaNs in shows -\", ds_X['tv_show_id'].isna().sum())\n    print(\"Missing shows -\", (ds_X['tv_show_id'].apply(len) != 5).mean())\n    print()\n    \n    \nfor name, ds_X in X.items():\n    fillna_with_top(ds_X, 'tv_show_id', top_5_shows)\n    fill_to_5(ds_X, 'tv_show_id', top_10_shows)","7740d46b":"for name, ds_X in X.items():\n    print(name)\n    test_mapk(ds_X, test_set, top_5_shows)\n    print()","df363690":"train_df['view_rating'] = train_df['view_pct'].apply(lambda x: 5 if x==1\n                                                     else 2 + 2*x**2)\nT = 60\ncont = 0.8\nt_ref = (train_df['start_time_tv']).max()\ntrain_df['view_rating'] = (train_df['view_rating'] * \n                           np.power(cont, (t_ref - train_df['start_time_tv']).dt.days \/ T))","3ab69938":"# Shows with high screen time and duration less than 10000\n# Category \"\u0424\u0438\u043b\u044c\u043c\u044b\" dropped\nhigh_frequency_df = train_df[(train_df['view_pct'] >= 0.8) &\n                             (train_df['duration'] < 10000)]\n\nhigh_frequency_df_cat = high_frequency_df[high_frequency_df['tv_show_category'].isin(\n    ['\u0420\u0430\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u044f', '\u0418\u043d\u0444\u043e', '\u0421\u0435\u0440\u0438\u0430\u043b\u044b', '\u0414\u0435\u0442\u044f\u043c', '\u0414\u0440\u0443\u0433\u043e\u0435', '\u041f\u043e\u0437\u043d\u0430\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0435'])]","253ae66e":"top_5_shows = top_shows(high_frequency_df_cat, 'tv_show_id', 5)\ntop_10_shows = top_shows(high_frequency_df_cat,'tv_show_id', 10)\npvt_table, cos_sim_matrix = create_similarity_matrix(high_frequency_df_cat)","3f676bde":"def counts(df):\n    count_by_user_show = df.groupby(['user_id', 'tv_show_id'])['view_rating'].size().reset_index()\n    rating_by_user_show = df.groupby(['user_id', 'tv_show_id'])['view_rating'].mean().reset_index()\n\n    return count_by_user_show, rating_by_user_show","29992533":"count_by_user_show, rating_by_user_show = counts(high_frequency_df_cat)\ntest_count = test_arh.groupby(['tv_show_id']).size().reset_index(name='test_count')\ntrain_count = train_arh.groupby(['tv_show_id']).size().reset_index(name='train_count')\n\nrating_by_user_show['count'] = count_by_user_show['view_rating']\nrating_by_user_show = pd.merge(rating_by_user_show, train_count, \n                               on='tv_show_id', how='left')\nrating_by_user_show = pd.merge(rating_by_user_show, test_count, \n                               on='tv_show_id', how='left')\nrating_by_user_show['train_total'] = (rating_by_user_show['count'] * \n                                      rating_by_user_show['view_rating'] * \n                                      np.log10(rating_by_user_show['train_count']))\n\nrating_by_user_show = rating_by_user_show.join(rating_by_user_show.groupby(\n    'tv_show_id').size().to_frame(name='global_cnt'),\n                                               on='tv_show_id', how='left')\n\nshow_user_rating = rating_by_user_show.sort_values(['train_total', 'global_cnt'], \n                                                   ascending=False).groupby(\n    'user_id').head(5).groupby('user_id')['tv_show_id'].apply(list).to_frame()","e3be46e4":"show_cnt_user = top_user_show_sorted(high_frequency_df_cat)\n\nmy_submission = submission[['user_id']].join(show_user_rating, on='user_id', how='left')\nprint(\"NaNs in shows before fill -\", my_submission['tv_show_id'].isna().sum())\nfillna_with_top(my_submission, 'tv_show_id', top_5_shows)\nprint(\"Duplicates in shows -\", my_submission['tv_show_id'].apply(lambda x: len(set(x)) != len(x)).mean())\nprint(\"Missing shows before fill -\", (my_submission['tv_show_id'].apply(len) != 5).mean())\nfill_with_recommend(my_submission, 'tv_show_id', cos_sim_matrix, pvt_table)\nassert (my_submission['tv_show_id'].apply(len) != 5).mean() == 0","0b0f4518":"my_submission['tv_show_id'] = my_submission['tv_show_id'].apply(lambda x: ' '.join([str(i) for i in x]))\nmy_submission.to_csv('\/kaggle\/working\/my_submission.csv', index=False)\nmy_submission","a59e897d":"## Data reading and cleaning","11406f19":"## Split data by train\/test","f6148870":"Calculation of the most popular shows for the user with respect to ratings and train\/test show ratio","8b46d934":"## Functions for validation","6fe36f68":"## Functions for calculating statistics","53bd7bd5":"## \u0421alculation of view rating","afc431df":"Top shows and cosine similarity matrix to fill missing shows for the user","bd600817":"# Preprocessing","602c1f3b":"Functions for filling missing shows for user with most popular shows ","16548106":"## Final solution","0dae04d6":"# Testing"}}