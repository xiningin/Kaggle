{"cell_type":{"c442d552":"code","60efff37":"code","e84e21a1":"code","29ed60fe":"code","a6c852ed":"code","95e08b27":"code","25624c31":"code","59b096f3":"code","a1b7995e":"code","004a5d10":"code","d0dbd8f8":"code","d3d1b55c":"code","d0c085b0":"code","4dce5d92":"code","5c892f84":"code","223dfc3f":"code","dc9cd7af":"code","156f480b":"code","606626e9":"code","dde03635":"code","8de861cf":"code","69f17189":"code","d8ec6dd9":"code","fc212302":"code","3aa338ea":"code","d97448f4":"code","f76d19ea":"code","47379cd5":"code","746572b2":"markdown","f7027373":"markdown","535e7f1d":"markdown","39060429":"markdown","b5876d3c":"markdown","fd1c9ea0":"markdown","a6a34544":"markdown","48d620ad":"markdown","32edb45c":"markdown","3e5e729a":"markdown","98b7f4d1":"markdown","64f279df":"markdown","de514aa4":"markdown","0d08369a":"markdown","122da96b":"markdown","48f1a9de":"markdown","be06db92":"markdown","6e1be781":"markdown","15882c42":"markdown","10922fa4":"markdown","08772e4c":"markdown","1d3447bc":"markdown","cd7c59be":"markdown","b757521e":"markdown","59812245":"markdown","5d427f38":"markdown","22f807c8":"markdown"},"source":{"c442d552":"import pandas as pd\nimport numpy as np","60efff37":"df_train=pd.read_csv('..\/input\/g-research-crypto-forecasting\/train.csv')\ndf_train.head()","e84e21a1":"df_train.describe()","29ed60fe":"len(df_train)","a6c852ed":"df_train['Asset_ID'].unique()","95e08b27":"df=df_train.query('Asset_ID == 0')\ndf.head()","25624c31":"len(df)","59b096f3":"import matplotlib.pylab as plt","a1b7995e":"plt.figure(figsize=(25,15))\nplt.rcParams['font.size']=15\nplt.plot(df.loc[:1000,'timestamp'],df.loc[:1000,'Open'])\nplt.xticks(rotation=90)\n#plt.title(df.loc[0,columns[0]])","004a5d10":"plt.figure(figsize=(25,15))\nplt.rcParams['font.size']=15\nplt.plot(df.loc[:1000,'timestamp'],df.loc[:1000,'Target'])\nplt.xticks(rotation=90)\n#plt.title(df.loc[0,columns[0]])","d0dbd8f8":"len(df.query(\"Target=='NaN'\"))","d3d1b55c":"plt.figure(figsize=(25,15))\nplt.rcParams['font.size']=15\nplt.plot(df.loc[:1000,'timestamp'],df.loc[:1000,'Count'])\nplt.xticks(rotation=90)\n#plt.title(df.loc[0,columns[0]])","d0c085b0":"sup_train=pd.read_csv('..\/input\/g-research-crypto-forecasting\/supplemental_train.csv')\nsup_train.head()","4dce5d92":"asset_details=pd.read_csv('..\/input\/g-research-crypto-forecasting\/asset_details.csv')\nasset_details","5c892f84":"df_test=pd.read_csv('..\/input\/g-research-crypto-forecasting\/example_test.csv')\ndf_test.head()","223dfc3f":"len(df_test)","dc9cd7af":"sumple_submission=pd.read_csv('..\/input\/g-research-crypto-forecasting\/example_sample_submission.csv')\nsumple_submission","156f480b":"import datatable as dt\n\nimport pandas as pd\nimport numpy as np\nimport random\nimport time\nimport os\nimport gc\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nimport lightgbm as lgb\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.simplefilter('ignore')\nimport gresearch_crypto","606626e9":"N_SPLITS = 5\nN_ESTIMATORS = 20000\nEARLY_STOPPING_ROUNDS = 200\nVERBOSE = 1000\nSEED = 42","dde03635":"lgb_params = {\n#    'objective': 'regression',\n    'n_estimators': N_ESTIMATORS,\n    'random_state': SEED,\n    'learning_rate': 8e-3,\n    'subsample': 0.6,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.4,\n    'reg_alpha': 10.0,\n    'reg_lambda': 1e-1,\n    'min_child_weight': 256,\n    'min_child_samples': 20,\n    'categorical_feature': 0,\n}","8de861cf":"TARGET='Target'\nfeatures = [column for column in df_train.drop('Target',axis=1)]\nfeatures","69f17189":"df_train=df_train.dropna().astype('float32')","d8ec6dd9":"df_train.isnull().sum()","fc212302":"def upper_shadow(df):\n    return df['High'] - np.maximum(df['Close'], df['Open'])\n\ndef lower_shadow(df):\n    return np.minimum(df['Close'], df['Open']) - df['Low']\n\ndef get_features(df):\n    df_feat = df[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']].copy()\n    df_feat['Upper_Shadow'] = upper_shadow(df_feat)\n    df_feat['Lower_Shadow'] = lower_shadow(df_feat)\n    return df_feat\n\ndef get_Xy_and_model_for_asset(df_train, asset_id):\n    df = df_train[df_train[\"Asset_ID\"] == asset_id]\n    \n    df_proc = get_features(df)\n    df_proc['y'] = df['Target']\n    df_proc = df_proc.dropna(how=\"any\")\n    \n    X = df_proc.drop(\"y\", axis=1)\n    y = df_proc[\"y\"]\n    \n    model = lgb.LGBMRegressor(n_estimators=40000)\n    model.fit(X, y)\n    return X, y, model","3aa338ea":"Xs = {}\nys = {}\nmodels = {}\n\nfor asset_id, asset_name in zip(asset_details['Asset_ID'], asset_details['Asset_Name']):\n    print(f\"Training model for {asset_name:<16} (ID={asset_id:<2})\")\n    X, y, model = get_Xy_and_model_for_asset(df_train, asset_id)    \n    Xs[asset_id], ys[asset_id], models[asset_id] = X, y, model","d97448f4":"x = get_features(df_train.iloc[1])\ny_pred = models[0].predict([x])\ny_pred[0]","f76d19ea":"#import pdb; pdb.set_trace()","47379cd5":"env = gresearch_crypto.make_env()\niter_test = env.iter_test()\n\nfor i, (df_test, df_pred) in enumerate(iter_test):\n    for j , row in df_test.iterrows():\n        \n        model = models[row['Asset_ID']]\n        x_test = get_features(row)\n        y_pred = model.predict([x_test])[0]\n        \n        df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = y_pred\n        \n        \n        if i == 0 and j == 0:\n            display(x_test)\n\n    if i == 0:\n        display(df_pred)\n\n    env.predict(df_pred)","746572b2":"**Marisa:Let's take a look at supplemental_train.csv.**","f7027373":"**\u9b54\u7406\u6c99:Finally, let's take a look at sumple_submission.**","535e7f1d":"**Marisa:Upnushi seems to refer to Aditya Sharma 01's code.**  \n[https:\/\/www.kaggle.com\/adityasharma01\/g-research-starter-lgbm-pipeline-lb-0-5799](https:\/\/www.kaggle.com\/adityasharma01\/g-research-starter-lgbm-pipeline-lb-0-5799)","39060429":"**Reimu:Oh? Can't you count? Why?**\n\n![http:\/\/3.bp.blogspot.com\/-42ddByrXRig\/VZ-W1-dVteI\/AAAAAAAAvTU\/SOdvlTbjoz0\/s300\/woman_question.png](http:\/\/3.bp.blogspot.com\/-42ddByrXRig\/VZ-W1-dVteI\/AAAAAAAAvTU\/SOdvlTbjoz0\/s300\/woman_question.png)","b5876d3c":"example_test.csv - An example of the data that will be delivered by the time series API.","fd1c9ea0":"**Reimu:Next, let's take a look at Test.**","a6a34544":"**Reimu:What do group_num and row_id mean?**","48d620ad":"**Reimu:Sometimes the Target is NaN. Why?  \nMarisa:How much data does Target have Nan?**","32edb45c":"train.csv - The training set\n\n* timestamp - A timestamp for the minute covered by the row.\n* Asset_ID - An ID code for the cryptoasset.\n* Count - The number of trades that took place this minute.\n* Open - The USD price at the beginning of the minute.\n* High - The highest USD price during the minute.\n* Low - The lowest USD price during the minute.\n* Close - The USD price at the end of the minute.\n* Volume - The number of cryptoasset units traded during the minute.\n* VWAP - The volume weighted average price for the minute.\n* Target - 15 minute residualized returns. See the 'Prediction and Evaluation' section of this notebook for details of how the target is calculated.","3e5e729a":"**Reimu:Next, let's look at the train data.**","98b7f4d1":"lunana\n<br>\nlast update 2021.11.11\n<br>\nyukkurisiteittene","64f279df":"This forecasting competition aims to predict returns in the near future for prices $P^a$, for each asset $a$. For each row in the dataset, we include the target for prediction, `Target`. `Target` is derived from log returns ($R^a$) over 15 minutes.\n\n$$R^a(t) = log (P^a(t+16)\\ \/\\ P^a(t+1))$$\n\nCrypto asset returns are highly correlated, following to a large extend the overall crypto market. As we want to test your ability to predict returns for individual assets, we perform a linear residualization, removing the market signal from individual asset returns when creating the target. In more detail, if $M(t)$ is the weighted average market returns, the target is:\n\n$$M(t) = \\frac{\\sum_a w^a R^a(t)}{\\sum_a w^a}  \\\\\n\\beta^a = \\frac{\\langle M \\cdot R^a \\rangle}{\\langle M^2 \\rangle} \\\\\n\\text{Target}^a(t) = R^a(t) - \\beta^a M(t)$$\n\nwhere the bracket $\\langle .\\rangle$ represent the rolling average over time (3750 minute windows), and same asset weights $w^a$ used for the evaluation metric.\n\nSome rows have null values for targets due to missing values in future prices. Rows with nulls in the test set ground truth are ignored for scoring purposes.\n\nIn the competition, your predictions will be evaluated on a weighted version of the Pearson correlation coefficient, with weights given by the `Weight` column in the Asset Details file.\n\nIn this tutorial, we will simplify things and use correlation (without weights) for evaluation, and consider only two assets, BTC and ETH.","de514aa4":"**Marisa:Let's draw a graph of time and USD transition.**","0d08369a":"**Reimu:I wonder if the timestamp is one minute?**","122da96b":"gresearch_crypto - An unoptimized version of the time series API files for offline work. You may need Python 3.7 and a Linux environment to run it without errors.","48f1a9de":"**Reimu:Today is a G-reserch competition.  \nMarisa:Let's take a look at the outline first.**","be06db92":"Over $40 billion worth of cryptocurrencies are traded every day. They are among the most popular assets for speculation and investment, yet have proven wildly volatile. Fast-fluctuating prices have made millionaires of a lucky few, and delivered crushing losses to others. Could some of these price movements have been predicted in advance?\n\nIn this competition, you'll use your machine learning expertise to forecast short term returns in 14 popular cryptocurrencies. We have amassed a dataset of millions of rows of high-frequency market data dating back to 2018 which you can use to build your model. Once the submission deadline has passed, your final score will be calculated over the following 3 months using live crypto data as it is collected.\n\nThe simultaneous activity of thousands of traders ensures that most signals will be transitory, persistent alpha will be exceptionally difficult to find, and the danger of overfitting will be considerable. In addition, since 2018, interest in the cryptomarket has exploded, so the volatility and correlation structure in our data are likely to be highly non-stationary. The successful contestant will pay careful attention to these considerations, and in the process gain valuable insight into the art and science of financial forecasting.\n\nG-Research is Europe\u2019s leading quantitative finance research firm. We have long explored the extent of market prediction possibilities, making use of machine learning, big data, and some of the most advanced technology available. Specializing in data science and AI education for workforces, Cambridge Spark is partnering with G-Research for this competition. Watch our introduction to the competition below:\n\n","6e1be781":"asset_details.csv - Provides the real name and of the cryptoasset for each Asset_ID and the weight each cryptoasset receives in the metric.","15882c42":"[![](https:\/\/img.youtube.com\/vi\/FnV0thLS1Fs\/0.jpg)](https:\/\/www.youtube.com\/watch?v=FnV0thLS1Fs)","10922fa4":"# parameta","08772e4c":"example_sample_submission.csv - An example of the data that will be delivered by the time series API. The data is just copied from train.csv.","1d3447bc":"**Reimu:It's almost the same as train.**","cd7c59be":"**Marisa:Let's graph the relationship between timestamp and Count.**","b757521e":"**Reimu:It's a file I've never seen. I don't know how to open.**","59812245":"supplemental_train.csv - After the submission period is over this file's data will be replaced with cryptoasset prices from the submission period. In the Evaluation phase, the train, train supplement, and test set will be contiguous in time, apart from any missing data. The current copy, which is just filled approximately the right amount of data from train.csv is provided as a placeholder.","5d427f38":"**Reimu:Many data!\n<br>Marisa:14 types of AssedId?**","22f807c8":"**Reimu:What 'weight'?**"}}