{"cell_type":{"bb0520ac":"code","2431a1fa":"code","bd8fb91f":"code","168c8025":"code","c14e9028":"code","13e3afae":"code","198d30f3":"code","ed5563fd":"code","456d24cc":"code","d5296dc9":"code","0fba844f":"code","f68d5c85":"code","75ca69ec":"code","3ce70dfc":"code","f253e1f0":"code","d61c7ccf":"code","30cfd323":"code","93d489db":"code","a80676fd":"code","e0dbb109":"code","2ca08efa":"code","ce57a68a":"code","07f7f125":"code","97132b0d":"code","52b2740e":"code","606a2fcd":"code","3fa14b05":"code","2759cd16":"code","c5d37923":"code","0b627d77":"code","4d5b3579":"code","d99cafa9":"code","204b106e":"code","766d85bd":"code","c62c31a0":"code","a42bbcfd":"code","628450f5":"code","cb708e71":"code","cf34b764":"code","49638e54":"code","627f7646":"code","51e75a63":"code","2824837b":"code","ecdbebd7":"markdown","271e1d8a":"markdown","1416e0a7":"markdown","6c453a30":"markdown","dacb7dbc":"markdown","84f64380":"markdown","dcf5141b":"markdown","b2dd11e3":"markdown","203823e0":"markdown","7a9d5caa":"markdown","534e117c":"markdown","c6b22e92":"markdown","01dbda2b":"markdown","1e653bdf":"markdown","8705d34a":"markdown","dbb398d1":"markdown","5f3a9fa6":"markdown","19008ea7":"markdown","e82b0171":"markdown","c0b5cc33":"markdown","e6b2cc18":"markdown","c30f020f":"markdown","0f2c475e":"markdown","4712f253":"markdown"},"source":{"bb0520ac":"import os\nimport warnings\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import display\nfrom pandas.api.types import CategoricalDtype\nfrom datetime import datetime\n\nfrom scipy.stats import skew, norm  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom xgboost import XGBRegressor\n\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_squared_error\n\nfrom mlxtend.regressor import StackingCVRegressor\n\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n\n# Set Matplotlib defaults\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n\n# Mute warnings\nwarnings.filterwarnings('ignore')\n","2431a1fa":"def load_data():\n    # Read data\n    data_dir = Path(\"..\/input\/house-prices-advanced-regression-techniques\/\")\n    df_train = pd.read_csv(data_dir \/ \"train.csv\", index_col=\"Id\")\n    df_test = pd.read_csv(data_dir \/ \"test.csv\", index_col=\"Id\")\n    # Merge the splits so we can process them together\n    df = pd.concat([df_train, df_test])\n    # Preprocessing\n    df = clean(df)\n    df = encode(df)\n    df = impute(df)\n    # Reform splits\n    df_train = df.loc[df_train.index, :]\n    df_test = df.loc[df_test.index, :]\n    return df_train, df_test\n","bd8fb91f":"data_dir = Path(\"..\/input\/house-prices-advanced-regression-techniques\/\")\ndf = pd.read_csv(data_dir \/ \"train.csv\", index_col=\"Id\")\n\ndf.Exterior2nd.unique()","168c8025":"def clean(df):\n    df[\"Exterior2nd\"] = df[\"Exterior2nd\"].replace({\"Brk Cmn\": \"BrkComm\"})\n    # Some values of GarageYrBlt are corrupt, so we'll replace them\n    # with the year the house was built\n    df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"].where(df.GarageYrBlt <= 2010, df.YearBuilt)\n    # Names beginning with numbers are awkward to work with\n    df.rename(columns={\n        \"1stFlrSF\": \"FirstFlrSF\",\n        \"2ndFlrSF\": \"SecondFlrSF\",\n        \"3SsnPorch\": \"Threeseasonporch\",\n    }, inplace=True,\n    )\n    return df\n","c14e9028":"\n# The numeric features are already encoded correctly (`float` for\n# continuous, `int` for discrete), but the categoricals we'll need to\n# do ourselves. Note in particular, that the `MSSubClass` feature is\n# read as an `int` type, but is actually a (nominative) categorical.\n\n# The nominative (unordered) categorical features\nfeatures_nom = [\"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\", \"LandContour\", \"LotConfig\", \n                \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\", \n                \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \n                \"Foundation\", \"Heating\", \"CentralAir\", \"GarageType\", \"MiscFeature\", \n                \"SaleType\", \"SaleCondition\"]\n\n\n# The ordinal (ordered) categorical features \n\n# Pandas calls the categories \"levels\"\nfive_levels = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\nten_levels = list(range(10))\nprint(ten_levels)\n\nordered_levels = {\n    \"OverallQual\": ten_levels,\n    \"OverallCond\": ten_levels,\n    \"ExterQual\": five_levels,\n    \"ExterCond\": five_levels,\n    \"BsmtQual\": five_levels,\n    \"BsmtCond\": five_levels,\n    \"HeatingQC\": five_levels,\n    \"KitchenQual\": five_levels,\n    \"FireplaceQu\": five_levels,\n    \"GarageQual\": five_levels,\n    \"GarageCond\": five_levels,\n    \"PoolQC\": five_levels,\n    \"LotShape\": [\"Reg\", \"IR1\", \"IR2\", \"IR3\"],\n    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n    \"BsmtExposure\": [\"No\", \"Mn\", \"Av\", \"Gd\"],\n    \"BsmtFinType1\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"BsmtFinType2\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"Functional\": [\"Sal\", \"Sev\", \"Maj1\", \"Maj2\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n    \"GarageFinish\": [\"Unf\", \"RFn\", \"Fin\"],\n    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n    \"Utilities\": [\"NoSeWa\", \"NoSewr\", \"AllPub\"],\n    \"CentralAir\": [\"N\", \"Y\"],\n    \"Electrical\": [\"Mix\", \"FuseP\", \"FuseF\", \"FuseA\", \"SBrkr\"],\n    \"Fence\": [\"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"],\n}\n\n# Add a None level for missing values\nordered_levels = {key: [\"None\"] + value for key, value in\n                  ordered_levels.items()}\n\n\ndef encode(df):\n    # Nominal categories\n    for name in features_nom:\n        df[name] = df[name].astype(\"category\")\n        # Add a None category for missing values\n        if \"None\" not in df[name].cat.categories:\n            df[name].cat.add_categories(\"None\", inplace=True)\n    # Ordinal categories\n    for name, levels in ordered_levels.items():\n        df[name] = df[name].astype(CategoricalDtype(levels,\n                                                    ordered=True))\n    return df\n","13e3afae":"def impute(df):\n    for name in df.select_dtypes(\"number\"):\n        df[name] = df[name].fillna(0)\n    for name in df.select_dtypes(\"category\"):\n        df[name] = df[name].fillna(\"None\")\n    return df\n","198d30f3":"df_train, df_test = load_data()","ed5563fd":"# Peek at the values\ndisplay(df_train)\ndisplay(df_test)\n\n# Display information about dtypes and missing values\ndisplay(df_train.info())\ndisplay(df_test.info())","456d24cc":"# Getting the main parameters of the Normal Ditribution ()\n(mu, sigma) = norm.fit(df['SalePrice'])\n\nplt.figure(figsize = (12,6))\nsns.distplot(df['SalePrice'], kde = True, hist=True, fit = norm)\nplt.title('SalePrice distribution vs Normal Distribution', fontsize = 13)\nplt.xlabel(\"House's sale Price in $\", fontsize = 12)\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.show()","d5296dc9":"# Skew and kurt\nfrom scipy import stats\n\nshap_t,shap_p = stats.shapiro(df['SalePrice'])\n\nprint(\"Skewness: %f\" % abs(df['SalePrice']).skew())\nprint(\"Kurtosis: %f\" % abs(df['SalePrice']).kurt())\nprint(\"Shapiro_Test: %f\" % shap_t)\nprint(\"Shapiro_Test: %f\" % shap_p)","0fba844f":"# Correlation Matrix\n\nf, ax = plt.subplots(figsize=(30, 25))\nmat = df.corr('pearson')\nmask = np.triu(np.ones_like(mat, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(mat, mask=mask, cmap=cmap, vmax=1, center=0, annot = True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","f68d5c85":"figure, ax = plt.subplots(1,3, figsize = (20,8))\nsns.stripplot(data=df, x = 'OverallQual', y='SalePrice', ax = ax[0])\nsns.violinplot(data=df, x = 'OverallQual', y='SalePrice', ax = ax[1])\nsns.boxplot(data=df, x = 'OverallQual', y='SalePrice', ax = ax[2])\nplt.show()","75ca69ec":"figure, ax = plt.subplots(1,3, figsize = (20,8))\nsns.stripplot(data=df, x = 'TotRmsAbvGrd', y='SalePrice', ax = ax[0])\nsns.violinplot(data=df, x = 'TotRmsAbvGrd', y='SalePrice', ax = ax[1])\nsns.boxplot(data=df, x = 'TotRmsAbvGrd', y='SalePrice', ax = ax[2])\nplt.show()","3ce70dfc":"Pearson_GrLiv = 0.71\nplt.figure(figsize = (12,6))\nsns.regplot(data=df, x = 'GrLivArea', y='SalePrice', scatter_kws={'alpha':0.2})\nplt.title('GrLivArea vs SalePrice', fontsize = 12)\nplt.legend(['$Pearson=$ {:.2f}'.format(Pearson_GrLiv)], loc = 'best')\nplt.show()","f253e1f0":"Pearson_TBSF = 0.63\nplt.figure(figsize = (12,6))\nsns.regplot(data=df, x = 'TotalBsmtSF', y='SalePrice', scatter_kws={'alpha':0.2})\nplt.title('TotalBsmtSF vs SalePrice', fontsize = 12)\nplt.legend(['$Pearson=$ {:.2f}'.format(Pearson_TBSF)], loc = 'best')\nplt.show()","d61c7ccf":"Pearson_YrBlt = 0.56\nplt.figure(figsize = (12,6))\nsns.regplot(data=df, x = 'YearBuilt', y='SalePrice', scatter_kws={'alpha':0.2})\nplt.title('YearBuilt vs SalePrice', fontsize = 12)\nplt.legend(['$Pearson=$ {:.2f}'.format(Pearson_YrBlt)], loc = 'best')\nplt.show()","30cfd323":"\ndef score_dataset(X, y, model=XGBRegressor()):\n    # Label encoding for categoricals\n    #\n    # Label encoding is good for XGBoost and RandomForest, but one-hot\n    # would be better for models like Lasso or Ridge. The `cat.codes`\n    # attribute holds the category levels.\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    # Metric for Housing competition is RMSLE (Root Mean Squared Log Error)\n    log_y = np.log(y)\n    score = cross_val_score(\n        model, X, log_y, cv=5, scoring=\"neg_mean_squared_error\",\n    )\n    score = -1 * score.mean()\n    score = np.sqrt(score)\n    return score\n","93d489db":"X = df_train.copy()\ny = X.pop(\"SalePrice\")\n\nbaseline_score = score_dataset(X, y)\nprint(f\"Baseline score: {baseline_score:.5f} RMSLE\")","a80676fd":"\ndef make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    print(width, ticks)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")","e0dbb109":"X = df_train.copy()\ny = X.pop(\"SalePrice\")\n\nmi_scores = make_mi_scores(X, y)\nprint(mi_scores)\nplt.figure(dpi=100, figsize=(8, 12))\nplot_mi_scores(mi_scores)","2ca08efa":"X.shape","ce57a68a":"def drop_uninformative(df, mi_scores):\n    return df.loc[:, mi_scores > 0.0]\n","07f7f125":"X = df_train.copy()\ny = X.pop(\"SalePrice\")\nX = drop_uninformative(X, mi_scores)\n\nscore_dataset(X, y)","97132b0d":"X.shape","52b2740e":"def label_encode(df):\n    X = df.copy()\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    return X\n","606a2fcd":"\ndef mathematical_transforms(df):\n    X = pd.DataFrame()  # dataframe to hold new features\n    \n    X[\"LivLotRatio\"] = df.GrLivArea \/ df.LotArea\n    X[\"Spaciousness\"] = (df.FirstFlrSF + df.SecondFlrSF) \/ df.TotRmsAbvGrd\n    X['YrBltAndRemod']=df['YearBuilt']+df['YearRemodAdd']\n    X['TotalSF']=df['TotalBsmtSF'] + df['FirstFlrSF'] + df['SecondFlrSF']\n    X['Total_sqr_footage'] = (df['BsmtFinSF1'] + df['BsmtFinSF2'] +\n                                 df['FirstFlrSF'] + df['SecondFlrSF'])\n    X['Total_Bathrooms'] = (df['FullBath'] + (0.5 * df['HalfBath']) +\n                               df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath']))\n    X['Total_porch_sf'] = (df['OpenPorchSF'] + df['Threeseasonporch'] +\n                              df['EnclosedPorch'] + df['ScreenPorch'] +\n                              df['WoodDeckSF'])\n    X['haspool'] = df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n    X['has2ndfloor'] = df['SecondFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    X['hasgarage'] = df['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    X['hasbsmt'] = df['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    X['hasfireplace'] = df['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n    # This feature ended up not helping performance\n    # X[\"TotalOutsideSF\"] = \\\n    #     df.WoodDeckSF + df.OpenPorchSF + df.EnclosedPorch + \\\n    #     df.Threeseasonporch + df.ScreenPorch\n    return X\n\n\ndef interactions(df):\n    X = pd.get_dummies(df.BldgType, prefix=\"Bldg\")\n    X = X.mul(df.GrLivArea, axis=0)\n    return X\n\n\ndef counts(df):\n    X = pd.DataFrame()\n    X[\"PorchTypes\"] = df[[\n        \"WoodDeckSF\",\n        \"OpenPorchSF\",\n        \"EnclosedPorch\",\n        \"Threeseasonporch\",\n        \"ScreenPorch\",\n    ]].gt(0.0).sum(axis=1)\n    return X\n\n\ndef break_down(df):\n    X = pd.DataFrame()\n    X[\"MSClass\"] = df.MSSubClass.str.split(\"_\", n=1, expand=True)[0]\n    return X\n\n\ndef group_transforms(df):\n    X = pd.DataFrame()\n    X[\"MedNhbdArea\"] = df.groupby(\"Neighborhood\")[\"GrLivArea\"].transform(\"median\")\n    return X\n","3fa14b05":"\ncluster_features = [\n    \"LotArea\",\n    \"TotalBsmtSF\",\n    \"FirstFlrSF\",\n    \"SecondFlrSF\",\n    \"GrLivArea\",\n]\n\n\ndef cluster_labels(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)\n    X_new = pd.DataFrame()\n    X_new[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n    return X_new\n\n\ndef cluster_distance(df, features, n_clusters=20):\n    X = df.copy()\n    X_scaled = X.loc[:, features]\n    X_scaled = (X_scaled - X_scaled.mean(axis=0)) \/ X_scaled.std(axis=0)\n    kmeans = KMeans(n_clusters=20, n_init=50, random_state=0)\n    X_cd = kmeans.fit_transform(X_scaled)\n    # Label features and join to dataset\n    X_cd = pd.DataFrame(\n        X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])]\n    )\n    return X_cd\n","2759cd16":"\ndef apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) \/ X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings\n\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n","c5d37923":"\ndef pca_inspired(df):\n    X = pd.DataFrame()\n    X[\"Feature1\"] = df.GrLivArea + df.TotalBsmtSF\n    X[\"Feature2\"] = df.YearRemodAdd * df.TotalBsmtSF\n    return X\n\n\ndef pca_components(df, features):\n    X = df.loc[:, features]\n    _, X_pca, _ = apply_pca(X)\n    return X_pca\n\n\npca_features = [\n    \"GarageArea\",\n    \"YearRemodAdd\",\n    \"TotalBsmtSF\",\n    \"GrLivArea\",\n]","0b627d77":"def corrplot(df, method=\"pearson\", annot=True, **kwargs):\n    sns.clustermap(\n        df.corr(method),\n        vmin=-1.0,\n        vmax=1.0,\n        cmap=\"icefire\",\n        method=\"complete\",\n        annot=annot,\n        **kwargs,\n    )\n\n\ncorrplot(df_train, annot=None)","4d5b3579":"def indicate_outliers(df):\n    X_new = pd.DataFrame()\n    X_new[\"Outlier\"] = (df.Neighborhood == \"Edwards\") & (df.SaleCondition == \"Partial\")\n    return X_new\n","d99cafa9":"\nclass CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=5)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) \/ len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n","204b106e":"def create_features(df, df_test=None):\n    X = df.copy()\n    y = X.pop(\"SalePrice\")\n    mi_scores = make_mi_scores(X, y)\n\n    # Combine splits if test data is given\n    #\n    # If we're creating features for test set predictions, we should\n    # use all the data we have available. After creating our features,\n    # we'll recreate the splits.\n    if df_test is not None:\n        X_test = df_test.copy()\n        X_test.pop(\"SalePrice\")\n        X = pd.concat([X, X_test])\n\n    # Lesson 2 - Mutual Information\n    X = drop_uninformative(X, mi_scores)\n\n    numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    numerics2 = []\n    for i in X.columns:\n        if X[i].dtype in numeric_dtypes:\n            numerics2.append(i)\n    \n    skew_features = X[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\n    high_skew = skew_features[skew_features > 0.5]\n    skew_index = high_skew.index\n    \n    for i in skew_index:\n        X[i] = boxcox1p(X[i], boxcox_normmax(X[i] + 1))\n    \n    \n    # Lesson 3 - Transformations\n    X = X.join(mathematical_transforms(X))\n    X = X.join(interactions(X))\n    X = X.join(counts(X))\n    # X = X.join(break_down(X))\n    X = X.join(group_transforms(X))\n\n    # Lesson 4 - Clustering - does not help the score\n    #X = X.join(cluster_labels(X, cluster_features, n_clusters=20))\n    #X = X.join(cluster_distance(X, cluster_features, n_clusters=20))\n\n    # Lesson 5 - PCA\n    X = X.join(pca_inspired(X))\n    # X = X.join(pca_components(X, pca_features))\n    # X = X.join(indicate_outliers(X))\n\n    X = label_encode(X)\n    \n    #X_final = pd.get_dummies(X).reset_index(drop=True)\n    #print(X_final.shape)\n\n    # Reform splits\n    if df_test is not None:\n        X_test = X.loc[df_test.index, :]\n        X.drop(df_test.index, inplace=True)\n\n    # Lesson 6 - Target Encoder\n    encoder = CrossFoldEncoder(MEstimateEncoder, m=1)\n    X = X.join(encoder.fit_transform(X, y, cols=[\"MSSubClass\"]))\n    if df_test is not None:\n        X_test = X_test.join(encoder.transform(X_test))\n\n    if df_test is not None:\n        return X, X_test\n    else:\n        return X\n\n\ndf_train, df_test = load_data()\nX_train = create_features(df_train)\ny_train = df_train.loc[:, \"SalePrice\"]\n\nscore_dataset(X_train, y_train)","766d85bd":"X_train.shape","c62c31a0":"X, X_test = create_features(df_train, df_test)\ny = np.log1p(df_train.loc[:, \"SalePrice\"])\ny","a42bbcfd":"X","628450f5":"print('START ML', datetime.now(), )\n\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\n\n# rmsle\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n\n# build our model scoring function\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y,\n                                    scoring=\"neg_mean_squared_error\",\n                                    cv=kfolds))\n    return (rmse)","cb708e71":"# setup models    \nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n\nridge = make_pipeline(RobustScaler(),\n                      RidgeCV(alphas=alphas_alt, cv=kfolds,))\n\nlasso = make_pipeline(RobustScaler(),\n                      LassoCV(max_iter=1e7, alphas=alphas2,\n                              random_state=42, cv=kfolds))\n\nelasticnet = make_pipeline(RobustScaler(),\n                           ElasticNetCV(max_iter=1e7, alphas=e_alphas,\n                                        cv=kfolds, random_state=42, l1_ratio=e_l1ratio))\n                                        \nsvr = make_pipeline(RobustScaler(),\n                      SVR(C= 20, epsilon= 0.008, gamma=0.0003,))\n\n\ngbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =42)\n                                   \n\nlightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       #min_data_in_leaf=2,\n                                       #min_sum_hessian_in_leaf=11\n                                       )\n                                       \n# changed objective from reg:linear to reg:squarederror\nxgboost = XGBRegressor(learning_rate=0.01, n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:squarederror', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006, random_state=42)\n\n# stack\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet,\n                                            gbr, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","cf34b764":"print('TEST score on CV')\n\nscore = cv_rmse(ridge)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(lasso)\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(elasticnet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(svr)\nprint(\"SVR score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(lightgbm)\nprint(\"Lightgbm score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(gbr)\nprint(\"GradientBoosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(xgboost)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )","49638e54":"print('START Fit')\nprint(datetime.now(), 'StackingCVRegressor')\nstack_gen_model = stack_gen.fit(np.array(X), np.array(y))\nprint(datetime.now(), 'elasticnet')\nelastic_model_full_data = elasticnet.fit(X, y)\nprint(datetime.now(), 'lasso')\nlasso_model_full_data = lasso.fit(X, y)\nprint(datetime.now(), 'ridge')\nridge_model_full_data = ridge.fit(X, y)\nprint(datetime.now(), 'svr')\nsvr_model_full_data = svr.fit(X, y)\nprint(datetime.now(), 'GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y)\nprint(datetime.now(), 'xgboost')\nxgb_model_full_data = xgboost.fit(X, y)\nprint(datetime.now(), 'lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)","627f7646":"def blend_models_predict(X):\n    return ((0.1 * elastic_model_full_data.predict(X)) + \\\n            (0.1 * lasso_model_full_data.predict(X)) + \\\n            (0.1 * ridge_model_full_data.predict(X)) + \\\n            (0.1 * svr_model_full_data.predict(X)) + \\\n            (0.1 * gbr_model_full_data.predict(X)) + \\\n            (0.15 * xgb_model_full_data.predict(X)) + \\\n            (0.1 * lgb_model_full_data.predict(X)) + \\\n            (0.25 * stack_gen_model.predict(np.array(X))))\n            \nprint('RMSLE score on train data:')\nprint(rmsle(y, blend_models_predict(X)))\n","51e75a63":"print('Predict submission', datetime.now(),)\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(blend_models_predict(X_test)))\nsubmission.head()","2824837b":"submission.to_csv(\"House_price_submission_v18.csv\", index=False)\nprint('Save submission', datetime.now(),)","ecdbebd7":"## Target Encoding ##\n\n","271e1d8a":"### Clean Data ###\n\nSome of the categorical features in this dataset have what are apparently typos in their categories:","1416e0a7":"We can reuse this scoring function anytime we want to try out a new feature set. We'll run it now on the processed data with no additional features and get a baseline score:","6c453a30":"In literature, acceptable values for skewness are between -0.5 and 0.5 while -2 and 2 for Kurtosis. ","dacb7dbc":"### Encode the Statistical Data Type ###","84f64380":"Groups of highly correlated features often yield interesting loadings.\n\n### PCA Application - Indicate Outliers ###\n\n","dcf5141b":"## Load Data and EDA ##\n\nAnd now we can call the data loader and get the processed data splits:","b2dd11e3":"## Principal Component Analysis ##\n\n","203823e0":"## Establish Baseline ##\n\nFinally, let's establish a baseline score to judge our feature engineering against.\n","7a9d5caa":"A label encoding is okay for any kind of categorical feature when you're using a tree-ensemble like XGBoost, even for unordered categories. If you wanted to try a linear regression model (also popular in this competition), you would instead want to use a one-hot encoding, especially for the features with unordered categories.\n\n## Create Features with Pandas ##\n\n","534e117c":"Later, we'll add the `drop_uninformative` function to our feature-creation pipeline.\n\n# Step 3 - Create Features #\n\nNow we'll start developing our feature set.\n\nTo make our feature engineering workflow more modular, we'll define a function that will take a prepared dataframe and pass it through a pipeline of transformations to get the final feature set. It will look something like this:\n\n```\ndef create_features(df):\n    X = df.copy()\n    y = X.pop(\"SalePrice\")\n    X = X.join(create_features_1(X))\n    X = X.join(create_features_2(X))\n    X = X.join(create_features_3(X))\n    # ...\n    return X\n```\n\nLet's go ahead and define one transformation now, a [label encoding](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables) for the categorical features:","c6b22e92":"Let's look at our feature scores again:","01dbda2b":"### Handle Missing Values ###","1e653bdf":"Use it like:\n\n```\nencoder = CrossFoldEncoder(MEstimateEncoder, m=1)\nX_encoded = encoder.fit_transform(X, y, cols=[\"MSSubClass\"]))\n```\n\n\n\n## Create Final Feature Set ##\n","8705d34a":"# Step 4a -  Blended Model #","dbb398d1":"---\n\n\n\n\n*Have questions or comments? Visit the [Learn Discussion forum](https:\/\/www.kaggle.com\/learn-forum\/221677) to chat with other Learners.*","5f3a9fa6":"Comparing these to `data_description.txt` shows us what needs cleaning. We'll take care of a couple of issues here, but you might want to evaluate this data further.","19008ea7":"Removing them does lead to a modest performance gain:","e82b0171":"This baseline score helps us to know whether some set of features we've assembled has actually led to any improvement or not.\n\n# Step 2 - Feature Utility Scores #\n\n ","c0b5cc33":"The correlation matrix is the best way to see all the numerical correlation between features. Let's see which are the feature that correlate most with our target variable.\n\n","e6b2cc18":"# Introduction #\n\nThis is a beginner's notebook based on tutorial for feature engineering for house price and it has incorporated some of important ideas from the following published notebooks:\n\nReference: Blending of 6 Models (Top 4%), Sandeep Kumar\n\nReference: Beginners_Prediction_Top3%, Marto93\n","c30f020f":"Uncomment and run this cell if you'd like to see what they contain. Notice that `df_test` is\nmissing values for `SalePrice`. (`NA`s were willed with 0's in the imputation step.)","0f2c475e":"## Data Preprocessing ##\n\n- **Load** the data from CSV files\n- **Clean** the data to fix any errors or inconsistencies\n- **Encode** the statistical data type (numeric, categorical)\n- **Impute** any missing values","4712f253":"## k-Means Clustering ##\n\n"}}