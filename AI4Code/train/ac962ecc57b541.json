{"cell_type":{"85d8dc13":"code","1d46976b":"code","da23d6d7":"code","8166c3eb":"code","ddec8fc9":"code","148e95dc":"code","859192b4":"code","ef9131bb":"code","536e967f":"code","0ccbe678":"code","8794aa6e":"code","1de97186":"code","b42e2407":"code","7bd00479":"code","cd5db9b8":"code","41668d3d":"code","57c4cdbe":"code","e2e7aca6":"code","1482cc0b":"code","7accf622":"code","209aab07":"code","f372e446":"code","ecced004":"code","2900d40d":"code","b1ecf785":"code","6b393751":"code","108aa5f3":"code","90755eba":"code","ceda5f46":"code","9de4b4e6":"code","9a90ee9a":"code","75b94f24":"code","a40e6869":"code","55c6f7e6":"markdown","ea4ea0d3":"markdown","131e7eb2":"markdown","44301def":"markdown","5479b0e6":"markdown","ef5168ee":"markdown","db7613d0":"markdown","181669a4":"markdown","03f6e418":"markdown","19f164fe":"markdown","b411c2fe":"markdown","14b0b83d":"markdown","31c9bbbd":"markdown","c6059b3b":"markdown","49fe3721":"markdown","72f8648c":"markdown","cca0cb1d":"markdown","d2ef3888":"markdown","ef4be9b7":"markdown","0897cd14":"markdown","fb8655a0":"markdown","b238f2cf":"markdown","ffbcb4d9":"markdown","921450c8":"markdown","0f49ba70":"markdown","b1825f91":"markdown","3f636f9d":"markdown","f8c12665":"markdown","6ea38f95":"markdown","55d98c02":"markdown","68c7cab2":"markdown","e5136aba":"markdown","4cd6a328":"markdown"},"source":{"85d8dc13":"# read & manipulate data\nimport pandas as pd \nimport numpy as np\nimport tensorflow as tf\n\n# visualisations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style='whitegrid', context='notebook')\n%matplotlib notebook\n\n# misc\nimport random as rn\n\n# load the dataset\ndf = pd.read_csv('..\/input\/creditcard.csv')\n\n# manual parameters\nRANDOM_SEED = 42\nTRAINING_SAMPLE = 200000\nVALIDATE_SIZE = 0.2\n\n# setting random seeds for libraries to ensure reproducibility\nnp.random.seed(RANDOM_SEED)\nrn.seed(RANDOM_SEED)\ntf.set_random_seed(RANDOM_SEED)","1d46976b":"# let's quickly convert the columns to lower case and rename the Class column \n# so as to not cause syntax errors\ndf.columns = map(str.lower, df.columns)\ndf.rename(columns={'class': 'label'}, inplace=True)\n\n# print first 5 rows to get an initial impression of the data we're dealing with\ndf.head()","da23d6d7":"# add a negligible amount to avoid taking the log of 0\ndf['log10_amount'] = np.log10(df.amount + 0.00001)","8166c3eb":"# keep the label field at the back\ndf = df[\n    [col for col in df if col not in ['label', 'log10_amount']] + \n    ['log10_amount', 'label']\n]","ddec8fc9":"# manual parameter \nRATIO_TO_FRAUD = 15\n\n# dropping redundant columns\ndf = df.drop(['time', 'amount'], axis=1)\n\n# splitting by class\nfraud = df[df.label == 1]\nclean = df[df.label == 0]\n\n# undersample clean transactions\nclean_undersampled = clean.sample(\n    int(len(fraud) * RATIO_TO_FRAUD),\n    random_state=RANDOM_SEED\n)\n\n# concatenate with fraud transactions into a single dataframe\nvisualisation_initial = pd.concat([fraud, clean_undersampled])\ncolumn_names = list(visualisation_initial.drop('label', axis=1).columns)\n\n# isolate features from labels \nfeatures, labels = visualisation_initial.drop('label', axis=1).values, \\\n                   visualisation_initial.label.values","148e95dc":"print(f\"\"\"The non-fraud dataset has been undersampled from {len(clean):,} to {len(clean_undersampled):,}.\nThis represents a ratio of {RATIO_TO_FRAUD}:1 to fraud.\"\"\")","859192b4":"from sklearn.manifold import TSNE\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef tsne_scatter(features, labels, dimensions=2, save_as='graph.png'):\n    if dimensions not in (2, 3):\n        raise ValueError('tsne_scatter can only plot in 2d or 3d (What are you? An alien that can visualise >3d?). Make sure the \"dimensions\" argument is in (2, 3)')\n\n    # t-SNE dimensionality reduction\n    features_embedded = TSNE(n_components=dimensions, random_state=RANDOM_SEED).fit_transform(features)\n    \n    # initialising the plot\n    fig, ax = plt.subplots(figsize=(8,8))\n    \n    # counting dimensions\n    if dimensions == 3: ax = fig.add_subplot(111, projection='3d')\n\n    # plotting data\n    ax.scatter(\n        *zip(*features_embedded[np.where(labels==1)]),\n        marker='o',\n        color='r',\n        s=2,\n        alpha=0.7,\n        label='Fraud'\n    )\n    ax.scatter(\n        *zip(*features_embedded[np.where(labels==0)]),\n        marker='o',\n        color='g',\n        s=2,\n        alpha=0.3,\n        label='Clean'\n    )\n\n    # storing it to be displayed later\n    plt.legend(loc='best')\n    plt.savefig(save_as);\n    plt.show;","ef9131bb":"tsne_scatter(features, labels, dimensions=2, save_as='tsne_initial_2d.png')","536e967f":"print(f\"\"\"Shape of the datasets:\n    clean (rows, cols) = {clean.shape}\n    fraud (rows, cols) = {fraud.shape}\"\"\")","0ccbe678":"# shuffle our training set\nclean = clean.sample(frac=1).reset_index(drop=True)\n\n# training set: exlusively non-fraud transactions\nX_train = clean.iloc[:TRAINING_SAMPLE].drop('label', axis=1)\n\n# testing  set: the remaining non-fraud + all the fraud \nX_test = clean.iloc[TRAINING_SAMPLE:].append(fraud).sample(frac=1)","8794aa6e":"print(f\"\"\"Our testing set is composed as follows:\n\n{X_test.label.value_counts()}\"\"\")","1de97186":"from sklearn.model_selection import train_test_split\n\n# train \/\/ validate - no labels since they're all clean anyway\nX_train, X_validate = train_test_split(X_train, \n                                       test_size=VALIDATE_SIZE, \n                                       random_state=RANDOM_SEED)\n\n# manually splitting the labels from the test df\nX_test, y_test = X_test.drop('label', axis=1).values, X_test.label.values","b42e2407":"print(f\"\"\"Shape of the datasets:\n    training (rows, cols) = {X_train.shape}\n    validate (rows, cols) = {X_validate.shape}\n    holdout  (rows, cols) = {X_test.shape}\"\"\")","7bd00479":"from sklearn.preprocessing import Normalizer, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\n# configure our pipeline\npipeline = Pipeline([('normalizer', Normalizer()),\n                     ('scaler', MinMaxScaler())])","cd5db9b8":"# get normalization parameters by fitting to the training data\npipeline.fit(X_train);","41668d3d":"# transform the training and validation data with these parameters\nX_train_transformed = pipeline.transform(X_train)\nX_validate_transformed = pipeline.transform(X_validate)","57c4cdbe":"g = sns.PairGrid(X_train.iloc[:,:3].sample(600, random_state=RANDOM_SEED))\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle('Before:')\ng.map_diag(sns.kdeplot)\ng.map_offdiag(sns.kdeplot);","e2e7aca6":"g = sns.PairGrid(pd.DataFrame(X_train_transformed, columns=column_names).iloc[:,:3].sample(600, random_state=RANDOM_SEED))\nplt.subplots_adjust(top=0.9)\ng.fig.suptitle('After:')\ng.map_diag(sns.kdeplot)\ng.map_offdiag(sns.kdeplot);","1482cc0b":"# Load the extension and start TensorBoard\n%load_ext tensorboard.notebook\n%tensorboard --logdir logs","7accf622":"# data dimensions \/\/ hyperparameters \ninput_dim = X_train_transformed.shape[1]\nBATCH_SIZE = 256\nEPOCHS = 100\n\n# https:\/\/keras.io\/layers\/core\/\nautoencoder = tf.keras.models.Sequential([\n    \n    # deconstruct \/ encode\n    tf.keras.layers.Dense(input_dim, activation='elu', input_shape=(input_dim, )), \n    tf.keras.layers.Dense(16, activation='elu'),\n    tf.keras.layers.Dense(8, activation='elu'),\n    tf.keras.layers.Dense(4, activation='elu'),\n    tf.keras.layers.Dense(2, activation='elu'),\n    \n    # reconstruction \/ decode\n    tf.keras.layers.Dense(4, activation='elu'),\n    tf.keras.layers.Dense(8, activation='elu'),\n    tf.keras.layers.Dense(16, activation='elu'),\n    tf.keras.layers.Dense(input_dim, activation='elu')\n    \n])\n\n# https:\/\/keras.io\/api\/models\/model_training_apis\/\nautoencoder.compile(optimizer=\"adam\", \n                    loss=\"mse\",\n                    metrics=[\"acc\"])\n\n# print an overview of our model\nautoencoder.summary();","209aab07":"from datetime import datetime\n\n# current date and time\nyyyymmddHHMM = datetime.now().strftime('%Y%m%d%H%M')\n\n# new folder for a new run\nlog_subdir = f'{yyyymmddHHMM}_batch{BATCH_SIZE}_layers{len(autoencoder.layers)}'\n\n# define our early stopping\nearly_stop = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    min_delta=0.0001,\n    patience=10,\n    verbose=1, \n    mode='min',\n    restore_best_weights=True\n)\n\nsave_model = tf.keras.callbacks.ModelCheckpoint(\n    filepath='autoencoder_best_weights.hdf5',\n    save_best_only=True,\n    monitor='val_loss',\n    verbose=0,\n    mode='min'\n)\n\ntensorboard = tf.keras.callbacks.TensorBoard(\n    f'logs\/{log_subdir}',\n    batch_size=BATCH_SIZE,\n    update_freq='batch'\n)\n\n# callbacks argument only takes a list\ncb = [early_stop, save_model, tensorboard]","f372e446":"history = autoencoder.fit(\n    X_train_transformed, X_train_transformed,\n    shuffle=True,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    callbacks=cb,\n    validation_data=(X_validate_transformed, X_validate_transformed)\n);","ecced004":"# transform the test set with the pipeline fitted to the training set\nX_test_transformed = pipeline.transform(X_test)\n\n# pass the transformed test set through the autoencoder to get the reconstructed result\nreconstructions = autoencoder.predict(X_test_transformed)","2900d40d":"# calculating the mean squared error reconstruction loss per row in the numpy array\nmse = np.mean(np.power(X_test_transformed - reconstructions, 2), axis=1)","b1ecf785":"clean = mse[y_test==0]\nfraud = mse[y_test==1]\n\nfig, ax = plt.subplots(figsize=(6,6))\n\nax.hist(clean, bins=50, density=True, label=\"clean\", alpha=.6, color=\"green\")\nax.hist(fraud, bins=50, density=True, label=\"fraud\", alpha=.6, color=\"red\")\n\nplt.title(\"(Normalized) Distribution of the Reconstruction Loss\")\nplt.legend()\nplt.show()","6b393751":"THRESHOLD = 3\n\ndef mad_score(points):\n    \"\"\"https:\/\/www.itl.nist.gov\/div898\/handbook\/eda\/section3\/eda35h.htm \"\"\"\n    m = np.median(points)\n    ad = np.abs(points - m)\n    mad = np.median(ad)\n    \n    return 0.6745 * ad \/ mad\n\nz_scores = mad_score(mse)\noutliers = z_scores > THRESHOLD","108aa5f3":"print(f\"Detected {np.sum(outliers):,} outliers in a total of {np.size(z_scores):,} transactions [{np.sum(outliers)\/np.size(z_scores):.2%}].\")","90755eba":"from sklearn.metrics import (confusion_matrix, \n                             precision_recall_curve)\n\n# get (mis)classification\ncm = confusion_matrix(y_test, outliers)\n\n# true\/false positives\/negatives\n(tn, fp, \n fn, tp) = cm.flatten()","ceda5f46":"print(f\"\"\"The classifications using the MAD method with threshold={THRESHOLD} are as follows:\n{cm}\n\n% of transactions labeled as fraud that were correct (precision): {tp}\/({fp}+{tp}) = {tp\/(fp+tp):.2%}\n% of fraudulent transactions were caught succesfully (recall):    {tp}\/({fn}+{tp}) = {tp\/(fn+tp):.2%}\"\"\")","9de4b4e6":"clean = z_scores[y_test==0]\nfraud = z_scores[y_test==1]\n\nfig, ax = plt.subplots(figsize=(6,6))\n\nax.hist(clean, bins=50, density=True, label=\"clean\", alpha=.6, color=\"green\")\nax.hist(fraud, bins=50, density=True, label=\"fraud\", alpha=.6, color=\"red\")\n\nplt.title(\"Distribution of the modified z-scores\")\nplt.legend()\nplt.show()","9a90ee9a":"encoder = tf.keras.models.Sequential(autoencoder.layers[:5])\nencoder.summary()","75b94f24":"# taking all the fraud, undersampling clean\nfraud = X_test_transformed[y_test==1]\nclean = X_test_transformed[y_test==0][:len(fraud) * RATIO_TO_FRAUD, ]\n\n# combining arrays & building labels\nfeatures = np.append(fraud, clean, axis=0)\nlabels = np.append(np.ones(len(fraud)),\n                   np.zeros(len(clean)))\n\n# getting latent space representation\nlatent_representation = encoder.predict(features)\n\nprint(f'Clean transactions downsampled from {len(X_test_transformed[y_test==0]):,} to {len(clean):,}.')\nprint('Shape of latent representation:', latent_representation.shape)","a40e6869":"X = latent_representation[:,0]\ny = latent_representation[:,1]\n\n# plotting\nplt.subplots(figsize=(8, 8))\nplt.scatter(X[labels==0], y[labels==0], s=1, c='g', alpha=0.3, label='Clean')\nplt.scatter(X[labels==1], y[labels==1], s=2, c='r', alpha=0.7, label='Fraud')\n\n# labeling\nplt.legend(loc='best')\nplt.title('Latent Space Representation')\n\n# saving & displaying\nplt.savefig('latent_representation_2d');\nplt.show()","55c6f7e6":"<a id=\"1\"><\/a> <br>\n# Unsupervised Learning with Auto-Encoders\n\nIf you are interested in an introduction to auto-encoders, head over to [Julien Despois' article](https:\/\/hackernoon.com\/latent-space-visualization-deep-learning-bits-2-bd09a46920df).\nIf a more technical breakdown is what you are looking for, check out [Lilian Weng's blog post](https:\/\/lilianweng.github.io\/lil-log\/2018\/08\/12\/from-autoencoder-to-beta-vae.html) from which the below image is sourced.\nIt illustrates the functioning of an auto-encoder for MNIST images, but the concept is the same.\n![image.png](attachment:image.png)","ea4ea0d3":"## Supervised\nWe know the labels, so we can verify our results.\n\n### Classification Matrix on MAD outliers\nA closer look:","131e7eb2":"<a id=\"10\"><\/a> <br>\n# Conclusion\nWe could already tell from our misclassifications that the network was not able to generalize perfectly. However, we must not forget that **our model was trained never having seen a single fraud case!** In that regard, its performance is decent. It illustrates the power of **autoencoders as anomaly detection tools**.\n\nTo improve its performance, perhaps we need to:\n* improve the model architecture\n* diversify the training data more, with a broader sample of clean transactions\n* augment the data with different, additional features - the data itself might not be good enough to distinguish between classes perfectly (i.e. fraudsters are disguising themselves well enough to always go undetected using these data points, no matter the algorithm).","44301def":"Although there is no perfectly distinct cluster, **most of the fradulent transactions appear to be neatly grouped together**.\nThis is in line with the hope\/idea that both **classes would occupy distinct areas in latent space**, due to the **encoder's weights not being calibrated to cope with fraudulent transactions**.\n![image.png](attachment:image.png)","5479b0e6":"<a id=\"7\"><\/a> <br>\n# Reconstructions\n\nWe **apply the transformation pipeline to our test set**. <br>\nThen, we **pass the data through the trained autoencoder**.","ef5168ee":"<a id=\"3\"><\/a> <br>\n# Visualising clusters with t-SNE\n*t-Distributed Stochastic Neighbor Embedding (t-SNE)*\n\nFrom the [sklearn documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.manifold.TSNE.html): <br>\n> t-SNE [1] is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.\n\nIn plain English, most certainly oversimplifying matters: **t-SNE is a dimensionality reduction technique used for visualisations** of complex datasets.\nIt **maps clusters in high-dimensional data** to **a two- or three dimensional plane** so we can get an idea of how easy it will be to **discriminate between classes**.\nIt does this by trying to keep the distance between data points in lower dimensions proportional to the probability that these data points are neighbours in the higher dimensions.\n\nA more elaborate [introduction](https:\/\/www.datacamp.com\/community\/tutorials\/introduction-t-sne) is available on DataCamp.","db7613d0":"## Architecture of our model\nKeras has become the standard high-level API within Tensorflow. No surprise, it's awesome.\nCheck out their [blog post on the topic of autoencoders](https:\/\/blog.keras.io\/building-autoencoders-in-keras.html).","181669a4":"We can tell the data is slightly more **uniform and proportionally distributed**. <br>\nThe ranges were also shrunk to fit **between 0 and 1**.","03f6e418":"### Asymmetric error cost\nIn the real world, we can expect **different costs associated with reporting a false positive versus reporting a false negative**. Missing a fraud case is likely to be much more costly than wrongly flagging a transaction as one. In [another kernel](https:\/\/www.kaggle.com\/robinteuwens\/fraud-detection-as-a-cost-optimization-problem\/comments), I discuss an approach to determining these costs for this dataset in depth. \n\n### Recall & Precision\nGenerally speaking, you will have to prioritise what you find more important. This dilemma is commonly called the **\"recall vs precision\" trade-off**.\nIf you want to increase recall, **adjust the MAD's Z-Score threshold** downwards, if you want recover precision, increase it.","19f164fe":"<a id=\"6\"><\/a> <br>\n# Training the auto-encoder\n","b411c2fe":"## Building our pipeline","14b0b83d":"## Visualising the Latent Space","31c9bbbd":"## Renaming columns","c6059b3b":"<a id=\"8\"><\/a> <br>\n# Setting a threshold for classification\n\n## Unsupervised\nNormally, in an unsupervised solution, this is where the story would end. We would **set a threshold that limits the amount of false positives** to a manageable degree, **and captures the most anomalous data points**.\n\n### Percentiles \nWe could set this threshold by taking the top x% of the dataset and considering it anomalous.\n\n### MAD\nWe could also use a **modified Z-score using the Median Absolute Deviation to define outliers** on our reconstruction data. Here is a [good blog post on the topic](https:\/\/medium.com\/james-blogs\/outliers-make-us-go-mad-univariate-outlier-detection-b3a72f1ea8c7) by Jo\u00e3o Rodrigues, illustrating why this algorithm is more robust and scalable than the percentiles method.","49fe3721":"# Table of Contents\n1. [Unsupervised Learning with Auto-Encoders](#1)\n1. [Preprocessing](#2)\n1. [Visualising clusters with t-SNE](#3)\n1. [Train\/Validate\/Test split](#4)\n1. [Normalising & Standardising](#5)\n1. [Training the auto-encoder](#6)\n1. [Reconstructions](#7)\n1. [Setting a threshold for classification](#8)\n1. [Latent Space ](#9)\n1. [Conclusion](#10)","72f8648c":"## TensorBoard\nAs documented in [this kernel by Aurelio Agundez](https:\/\/www.kaggle.com\/aagundez\/using-tensorboard-in-kaggle-kernels), TensorBoard requires a running kernel, so its output will only be available in an editor session.\nFork this notebook if you wish to interact with it.","cca0cb1d":"## Undersampling the non-fraud\nTo keep the computation time low, let's feed t-SNE only a small subsample (undersampling the clean transactions).","d2ef3888":"The idea is quite straightforward:\n1. Due to the **bottleneck architecture** of the neural network, it is forced to learn a **condensed representation** from which to reproduce the original input.\n2. We feed it **only normal transactions**, which it will learn to reproduce with high fidelity.\n3. As a consequence, if a **fraud transaction is sufficiently distinct** from normal transactions, the auto-encoder will have trouble reproducing it with its learned weights, and the subsequent **reconstruction loss will be high**.\n4. Anything above a specific loss (treshold) will be **flagged as anomalous** and thus labeled as fraud.\n\n<a id=\"2\"><\/a> <br>\n# Preprocessing\n\n## Import Libraries & set Random Seeds","ef4be9b7":"## Training","0897cd14":"## Fitting the pipeline","fb8655a0":"<a id=\"4\"><\/a> <br>\n# Train\/Validate\/Test split\nOur auto-encoder will **only train on transactions that were normal**. \nWhat's left over will be combined with the fraud set to form our test sample.\n\nWe will be doing something akin to the below:\n![image.png](attachment:image.png)\n \n\n1. Training: only non-fraud\n    * Split into:\n        1. Actual training of our autoencoder\n        2. Validation of the neural network's ability to generalize\n2. Testing : mix of fraud and non-fraud\n    * Treated like new data\n    * Attempt to locate outliers\n        1. Compute reconstruction loss\n        2. Apply threshold","b238f2cf":"Some clusters are apparent, but a minority of fraud transactions remains sneaky, sneaky.","ffbcb4d9":"## Summary","921450c8":"<a id=\"5\"><\/a> <br>\n# Normalising & Standardising \n\n## Why\nIn an [excellent article by Jeremy Jordan](https:\/\/www.jeremyjordan.me\/batch-normalization\/), it is explained why making sure your data is normally distributed can **help stochastic gradient descent converge** more effectively.\nIn a nutshell:\n![image.png](attachment:image.png)\n\n\n## When\nAt what point in the data processing do we apply standardisation\/normalisation? <br>\nAn [excellent answer was provided on StackOverflow](https:\/\/stackoverflow.com\/questions\/49444262\/normalize-data-before-or-after-split-of-training-and-testing-data).\n\n> Don't forget that **testing data points represent real-world data**. Feature normalization (or data standardization) of the explanatory (or predictor) variables is a technique used to center and normalise the data by subtracting the mean and dividing by the variance. **If you take the mean and variance of the whole dataset you'll be introducing future information into the training explanatory variables** (i.e. the mean and variance).\n> \n> Therefore, you should **perform feature normalisation over the training data**. Then **perform normalisation on testing **instances as well, but this time **using the mean and variance of training** explanatory variables. In this way, we can test and evaluate whether our model can generalize well to new, unseen data points.\n>\n> <span style=\"font-size:10px\">[Answer by [Giorgos Myrianthous](https:\/\/stackoverflow.com\/users\/7131757\/giorgos-myrianthous)]<\/span>","0f49ba70":"## Calculated field: log10(amount)\n\nTurn the amount feature into a normally distributed log equivalent.","b1825f91":"Very promising! Although some transactions seem to fool the autoencoder, the fraudulent transactions clearly have a distinguishing element in their data that sets them apart from clean ones. ","3f636f9d":"<a id=\"9\"><\/a> <br>\n# Latent Space \nIt is always interesting to look at the **compressed representation** our neural network devised.\n\n## Encoder\nLet's build the encoder that gets us to the bottleneck. We take the layers from our autoencoder.","f8c12665":"## Callbacks\n\n* Continue as long as the model is reducing the training loss.\n* Save only the weights for the model with the lowest validation loss, though.\n* Get graphical insights with Tensorboard.","6ea38f95":"## t-SNE output","55d98c02":"## Before & After","68c7cab2":"## Applying transformations with acquired parameters","e5136aba":"**Calculate the reconstruction loss** for every transaction and draw a sample.","4cd6a328":"## Undersampling\nConsistent with the previous t-sne visualisation, let's undersample the clean transactions."}}