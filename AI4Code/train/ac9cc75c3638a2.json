{"cell_type":{"deb4ac27":"code","99f03ad1":"code","5baf370f":"code","eb641152":"code","0c82536b":"code","cc46a9e4":"code","11638d1e":"code","5527fd12":"code","ea560a9c":"code","d4962f19":"code","6854018d":"code","77ca7bdb":"code","e835b62d":"code","88bba1e8":"code","a2609291":"code","0556c9ac":"code","af08b7ce":"code","b4b74494":"code","f783af2c":"code","9dbbcb84":"code","db8fe538":"code","362b799c":"code","af83a4e3":"code","b7378e1f":"code","a6c02565":"code","d3985fad":"code","def69e6f":"code","745f6bf7":"code","afb39858":"code","3b91ebcc":"code","fda106ca":"code","64bfc31f":"code","6faf9af6":"code","426efe73":"code","39e500bc":"code","c0861344":"code","530688aa":"code","ee8f4989":"code","ef1267a7":"code","26f9bb08":"code","c09dacf6":"code","df25f5e5":"code","8ec310a1":"code","ca01ae99":"code","b79af2fc":"code","4b0490a2":"code","78de2031":"code","da0fa3b5":"code","286524e4":"code","10b347c1":"code","809e4372":"code","908a9746":"code","25f987a8":"code","10f7edec":"code","f8cd70f5":"code","371ba3c2":"code","f1fecb73":"markdown","5bccdf8b":"markdown","358b946a":"markdown","ba3e5c45":"markdown","2f460a7f":"markdown","01575ff0":"markdown","bac89c0d":"markdown","80fbc5e3":"markdown","31d1974a":"markdown","855c37a6":"markdown","b49c8a68":"markdown","76debc6d":"markdown","067f83dd":"markdown","eed87b14":"markdown","e9441432":"markdown","b1960d43":"markdown","8f742918":"markdown","c2479167":"markdown","7d7c5865":"markdown","14a0a5ec":"markdown","5101b38b":"markdown","dc2a0f33":"markdown","2338a0f0":"markdown","1b4ee243":"markdown","0c1d9a52":"markdown","b053def3":"markdown","ba95efdb":"markdown","e9427fab":"markdown","319a4b6d":"markdown","56df387c":"markdown","bddc6bd7":"markdown","4e5db319":"markdown","b9aa882b":"markdown","00e8749f":"markdown","f3e725aa":"markdown","c99a9ebe":"markdown","ef1cb81a":"markdown","f28228c0":"markdown"},"source":{"deb4ac27":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","99f03ad1":"import matplotlib.pyplot as plt\nimport seaborn as sns\n#import dependancies\n%matplotlib inline\nsns.set_style('whitegrid')\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n","5baf370f":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n","eb641152":"#view the test data\ntrain.head()","0c82536b":"#view the test data\ntest.head()","cc46a9e4":"train.describe()","11638d1e":"train.info()","5527fd12":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","ea560a9c":"\nsns.heatmap(test.isnull(), yticklabels=False, cbar=False, cmap='viridis')","d4962f19":"train.corr()","6854018d":"#by using Pclass we can remove NaN value of age column","77ca7bdb":"def impute_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age):\n        if Pclass == 1:\n            return 37   #near to mean of Age\n        elif Pclass == 2:\n            return 29   #near to mean of Age\n        else:\n            return 24   #near to mean of Age\n    else:\n        return Age","e835b62d":"#impute the new function with old column\ntrain['Age'] = train[['Age','Pclass']].apply(impute_age,axis=1)","88bba1e8":"#impte the new function with old column\ntest['Age'] = test[['Age','Pclass']].apply(impute_age,axis=1)","a2609291":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","0556c9ac":"sns.heatmap(test.isnull(),yticklabels=False,cbar=False,cmap='viridis')","af08b7ce":"train.drop('Cabin',axis=1,inplace=True)#drop cabin from train","b4b74494":"test.drop('Cabin',axis=1,inplace=True)#drop cabin from test column","f783af2c":"train.dropna(inplace=True)#drop any NaN value from train","9dbbcb84":"test.dropna(inplace=True) #drop any NaN value from test","db8fe538":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","362b799c":"sns.heatmap(test.isnull(),yticklabels=False,cbar=False,cmap='viridis')","af83a4e3":"#get descriptive statistics on \"object \"datatype\ntrain.describe(include=['object'])","b7378e1f":"#get descriptive statistick on \"number\" datatype\ntrain.describe(include=['number'])","a6c02565":"train.Survived.value_counts(normalize=True)","d3985fad":"fig,axes = plt.subplots(2,4,figsize=(16,10))\nsns.countplot('Survived',data=train,ax=axes[0,0])\nsns.countplot('Pclass',data=train,ax=axes[0,1])\nsns.countplot('Sex',data=train,ax=axes[0,2])\nsns.countplot('SibSp',data=train,ax=axes[0,3])\nsns.countplot('Parch',data=train,ax=axes[1,0])\nsns.countplot('Embarked',data=train,ax=axes[1,1])\nsns.distplot(train['Fare'],kde=True,ax=axes[1,2])\nsns.distplot(train['Age'],kde=True,ax=axes[1,3])","def69e6f":"sns.jointplot(x=\"Age\",y=\"Fare\",data=train)","745f6bf7":"sex = pd.get_dummies(train['Sex'],drop_first=True)\nembark =pd.get_dummies(train['Embarked'],drop_first=True)","afb39858":"#concat dummies to train\ntrain = pd.concat([train, sex,embark],axis=1)","3b91ebcc":"train.head()","fda106ca":"#same process applying for test ","64bfc31f":"sex = pd.get_dummies(test['Sex'],drop_first=True)\nembark = pd.get_dummies(test['Embarked'],drop_first=True)","6faf9af6":"test = pd.concat([test,sex,embark],axis=1)","426efe73":"#drop column that dont need for model\ntrain.drop(['PassengerId','Sex','Embarked','Name','Ticket'],axis=1,inplace=True)\n#same process for test \ntest.drop(['Sex','Embarked','Name','Ticket'],axis=1,inplace=True)\n","39e500bc":"train.head(2)","c0861344":"test.head()","530688aa":"#x = train.drop(['Survived'],axis=1)\nx = train[['Pclass','Age','SibSp','Parch','Fare','male','Q','S']]  #predictors\ny = train['Survived'] #target","ee8f4989":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n","ef1267a7":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n","26f9bb08":"logmodel = LogisticRegression()","c09dacf6":"logmodel.fit(x_train,y_train)","df25f5e5":"print(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","8ec310a1":"logmodel_pred = logmodel.predict(x_test)","ca01ae99":"acc_logmodel = round(accuracy_score(logmodel_pred, y_test) * 100, 2)\nprint(acc_logmodel)\n","b79af2fc":"from sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\nsvc_pred = svc.predict(x_test)\nacc_svc = round(accuracy_score(svc_pred, y_test) * 100, 2)\nprint(acc_svc)","4b0490a2":"from sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train,y_train)\nlinear_svc_pred = linear_svc.predict(x_test)\nacc_linear_svc = round(accuracy_score(linear_svc_pred,y_test) * 100,2)\nprint(acc_linear_svc)","78de2031":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\n\ngaussian = GaussianNB()\ngaussian.fit(x_train,y_train)\ngaussian_pred = gaussian.predict(x_test)\nacc_gaussian = round(accuracy_score(gaussian_pred,y_test) * 100,2)\nprint(acc_gaussian)","da0fa3b5":"from sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_train,y_train)\nperception_pred = perceptron.predict(x_test)\nacc_perceptron = round(accuracy_score(perception_pred,y_test) * 100,2)\nprint(acc_perceptron)","286524e4":"from sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train,y_train)\ndecisiontree_pred = decisiontree.predict(x_test)\nacc_decisiontree = round(accuracy_score(decisiontree_pred,y_test) * 100,2)\nprint(acc_decisiontree)","10b347c1":"from sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train,y_train)\nrandomforest_pred = randomforest.predict(x_test)\nacc_randomforest = round(accuracy_score(randomforest_pred,y_test) *100,2)\nprint(acc_randomforest)\n","809e4372":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_train,y_train)\nknn_pred = knn.predict(x_test)\nacc_knn = round(accuracy_score(knn_pred,y_test) *100,2)\nprint(acc_knn)","908a9746":"from sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(x_train,y_train)\ny_pred = sgd.predict(x_test)\nacc_sgd = round(accuracy_score(y_pred,y_test) * 100,2)\nprint(acc_sgd)","25f987a8":"\nfrom sklearn.ensemble import GradientBoostingClassifier\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train,y_train)\ny_pred = gbk.predict(x_test)\nacc_gbk = round(accuracy_score(y_pred,y_test) *100,2)\nprint(acc_gbk)\n","10f7edec":"models = pd.DataFrame({'Model' :['Support Vector Machin','KNN','Logistic Regression','Random Forest','Naive Bayes','Perceptron','Linear SVC','Decision Tree','Stochastic Gradient Descent','Gradient Boosting Classifier'],\n                       'Score' :[acc_svc,acc_knn,acc_logmodel,acc_randomforest,acc_gaussian,acc_perceptron,acc_linear_svc,acc_decisiontree,acc_sgd,acc_gbk]})\nmodels.sort_values(by = 'Score',ascending=True)","f8cd70f5":"#set ids as PassengerId and predict survival \nids = test['PassengerId']\npredictions = logmodel.predict(test.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\nsubmission = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\nsubmission.to_csv('submission.csv', index=False)\n","371ba3c2":"submission","f1fecb73":"# 1) loading in the data","5bccdf8b":" # Decision Tree","358b946a":"#only 38% passenger are survived in disaster the majority of 61% passenger did not survived in disaster","ba3e5c45":"its important to visualize missing value early so you know where the major holes are in your dataset.","2f460a7f":"there is also clear relationship between pclass and the survival by referring to first plot.\npassenger of pclass1 had a better survival rate of approx 60% whereas passenger of pclass3 had the worst survival rate approx 22%.","01575ff0":"# 5) carryout univariate and multivariate analysis using graphical and non graphical(some number represent data)\n\n   ","bac89c0d":"# Random Forest","80fbc5e3":"**now our task is to clean the NaN value of age column**","31d1974a":"# 2) Data Description","855c37a6":"# stochastic Gradient Descent\n","b49c8a68":"# Logistic Regression","76debc6d":"There is also a marginal relationship between the fare and survival rate.","067f83dd":"# 8)Testing Different Models ","eed87b14":"# KNN or K-Nearest Neighbors","e9441432":"# Linear SVC\n","b1960d43":"ok we can clearly see some missing values here,especially in the cabin column.\n","8f742918":"**survival** 0 :no,1 :yes.\n**pclass**(tickit class): 1st 2nd and 3rd\n**sex**:sex\n**age**: age in year\n**sibsp**: number of sibling in titanic ship\n**parch**: number of parent children in titanic\n**ticket** :ticket number\n**fare**: passenger fare\n**cabin** :cabin number\n**embarked**: port of embarkation, c=Cherbough ,Q = **Qestown**,s= Southampton","c2479167":"# Perceptron","7d7c5865":"there are row which are missing a value or have NaN instead of something like the rest of the column","14a0a5ec":"where are the holes in our data","5101b38b":"**sex** column has 2 value '**male'&'female**'.\nby using **embark** column convert sex column into different **dummies**.","dc2a0f33":"# 7) preprocessing and prepare data for statistical modeling","2338a0f0":"# Gaussian Naive Bayes","1b4ee243":"# 6) joint plots(continues bs continuous)\n","0c1d9a52":"****Our Train and Test database does not have any NaN Value, now we are ready to perform machin learning algorithm.****","b053def3":"now its time to choose the model prediction submission.\ni decided to  Graddient Boosting Classifier is best for testing data.","ba95efdb":"we use 10 models for finding prediction, lets compare accuracy of ech of these model\n","e9427fab":"we can clearly see that male survival rate is around 20% where as female survival rate is abaut 75%  which suggest that gender has strong relationship with survival rate.","319a4b6d":"# 8)create submission File","56df387c":"Cabin column have more NaN value,to filled value of all NaN value, it is difficult task.\nto better is drop the Cabin column.","bddc6bd7":"urknowing this information will help with your EDA and figuring out what kind data cleaning and preprocessing is needed.","4e5db319":"# 4: run descriptive statistics of object and numerical datatypes,and finally transform datatype accordingly","b9aa882b":"# Support Vector Machin","00e8749f":"# Sources:","f3e725aa":"**now we clearly see that Age column NaN value filled with different value.**","c99a9ebe":"#  Gradient Boosting Classifier\n","ef1cb81a":"* https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n* https:\/\/www.kaggle.com\/jeffd23\/scikit-learn-ml-from-start-to-finish?scriptVersionId=320209\n* https:\/\/www.kaggle.com\/nadintamer\/titanic-survival-predictions-beginner\/comments","f28228c0":"# 3) Cleaning The Dataset"}}