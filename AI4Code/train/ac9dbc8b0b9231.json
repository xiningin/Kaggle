{"cell_type":{"a23cfe7c":"code","8e85f080":"code","45536972":"code","d3e304a0":"code","b1007e70":"code","89b67098":"code","aab4126c":"code","b2578c4d":"code","05d44b65":"code","2fbf89da":"code","1af0b448":"code","1d894c2c":"code","9661e866":"code","b3853a5f":"code","4c567f39":"code","68ac9e6b":"code","c1b40c60":"code","0c19fee6":"code","cf5727b5":"code","cd91826f":"code","44f9032b":"code","ac3a5a26":"code","991aafbd":"code","d014723a":"code","b48eb326":"code","99b14b71":"code","c98bfdb3":"code","93b92db2":"code","ad64ec18":"code","085fefac":"code","150d20ed":"code","160a6a10":"code","5c41c0d8":"code","aa8e349f":"code","b4aa6032":"code","b089c436":"code","b8148655":"code","61570d95":"code","15c6981b":"code","789712f1":"code","0f7075d2":"markdown","16577b1c":"markdown","4aa731e6":"markdown","8bbe6859":"markdown","606400fc":"markdown","77ea9ab6":"markdown","834dfa1a":"markdown","a9efa4c8":"markdown","d6c4fecb":"markdown","7753f0e8":"markdown","826828da":"markdown","002a0b82":"markdown","a3194a7f":"markdown","42ec0903":"markdown","678feb02":"markdown","9c140ea0":"markdown","a9b3da3b":"markdown","c837e109":"markdown","f32a3582":"markdown","8a599fe1":"markdown","ff8b31ac":"markdown","02386756":"markdown","d5e24b41":"markdown","c7adf7d4":"markdown","1628c4aa":"markdown","d804522a":"markdown"},"source":{"a23cfe7c":"# Importing required packages\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as func\nfrom torch.optim import Adam\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nimport seaborn as sns","8e85f080":"# Loading CIFAR10\n\ntorch.manual_seed(1102)\nnp.random.seed(1102)\n\ntransform = transforms.ToTensor()\n\ntrainset = torchvision.datasets.CIFAR10(root='.\/data', train=True,\n                                        download=True, transform=transform)\n\ntestset = torchvision.datasets.CIFAR10(root='.\/data', train=False,\n                                       download=True, transform=transform)\n\n\nclasses = ('plane', 'automobile', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')","45536972":"# Displaying 1 random image from the train set and its histogram, and 1 random image from the test set and its histogram for each class in the CIFAR-10 dataset\n\ndef imshow(img, class_nb, column):\n    # column = 0 for training set\n    # column = 2 for test set\n    npimg = img.numpy()\n    axs[class_nb,column].imshow(np.transpose(npimg, (1, 2, 0)))\n    axs[class_nb,column].set_xticks([])\n    axs[class_nb,column].set_yticks([])\n    \n    channels = ('Red', 'Green', 'Blue')\n    for c in range(len(channels)):\n        histogram, bin_edges = np.histogram(img[c,:,:], bins = 256)\n        axs[class_nb,column+1].plot(bin_edges[0:-1], histogram, c=channels[c], label='Channel {0}'.format(channels[c]))\n    axs[class_nb,column+1].set_xlabel('Intensity')\n    axs[class_nb,column+1].set_ylabel('Abundance')\n    axs[class_nb,column+1].legend()\n\nnum_classes = len(classes)\n\nfig, axs = plt.subplots(num_classes, 4, figsize=(32,32))\n\ncolumn_titles = [\"Training images\", \"Histogram of training images\", \"Test images\", \"Histogram of test images\"]\nfor ax, column in zip(axs[0], column_titles):\n    ax.set_title(column)\n    \nfor ax, class_nb in zip(axs[:,0], classes):\n    ax.set_ylabel(class_nb, size='large')\n\nfig.tight_layout()\n\nfor class_nb in range(num_classes):\n    trainlabel = torch.tensor(trainset.targets)\n    testlabel = torch.tensor(testset.targets)\n    trainindexes = torch.nonzero(trainlabel == class_nb)\n    trainsampler = torch.utils.data.sampler.SubsetRandomSampler(trainindexes)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=1,\n                                         num_workers=0, sampler = trainsampler)\n    traindataiter = iter(trainloader)\n    trainimage, trainlabel = traindataiter.next()\n    imshow(torchvision.utils.make_grid(trainimage),class_nb,0)\n    \n    testindexes = torch.nonzero(testlabel == class_nb)\n    testsampler = torch.utils.data.sampler.SubsetRandomSampler(testindexes)\n    testloader = torch.utils.data.DataLoader(testset, batch_size=1,\n                                         num_workers=0, sampler = testsampler)\n    testdataiter = iter(testloader)\n    testimage, testlabel = testdataiter.next()\n    imshow(torchvision.utils.make_grid(testimage), class_nb, 2)","d3e304a0":"# Splitting training set into new train and validation sets\n\ntest_size = len(testset)\noriginal_train_size = len(trainset)\n\nnew_trainset, valset = torch.utils.data.random_split(trainset, [original_train_size-test_size, test_size])\n\ntrain_size = len(new_trainset)\nval_size = len(valset)\n\n# Creating DataLoader for each set with a batch size of 32\n\ntrain_loader = torch.utils.data.DataLoader(new_trainset, batch_size=32,\n                                          shuffle=True, num_workers=2)\n\n\nval_loader = torch.utils.data.DataLoader(valset, batch_size=32,\n                                          shuffle=False, num_workers=2)\n\ntest_loader = torch.utils.data.DataLoader(testset, batch_size=32,\n                                          shuffle=False, num_workers=2)","b1007e70":"# Displaying the number of samples for each class in each set\n\ntraintargets = [trainset.targets[i] for i in new_trainset.indices] \nvaltargets = [trainset.targets[i] for i in valset.indices] \n\n(_, train_count) = np.unique(traintargets, return_counts=True)\n(_, val_count) = np.unique(valtargets, return_counts=True)\n(_, test_count) = np.unique(testset.targets, return_counts=True)\n\nplt.figure(figsize=(12,8))\nplt.title('CIFAR-10 data distribution')\nplt.xticks(range(10), labels=classes)\nplt.xlabel('Classes')\nplt.ylabel('Number of samples')\nplt.bar(range(10), train_count, label='Train')\nplt.bar(range(10), val_count, label='Validation', bottom=train_count)\nplt.bar(range(10), test_count, label='Test')\nplt.legend()\nplt.show()","89b67098":"# Implementing LeNet\n\nclass LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)\n        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n        self.fc1 = nn.Linear(16*5*5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = func.relu(self.conv1(x))\n        x = func.max_pool2d(x, 2)\n        x = func.relu(self.conv2(x))\n        x = func.max_pool2d(x, 2)\n        x = x.view(x.size(0), -1)\n        x = func.relu(self.fc1(x))\n        x = func.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n    \nmodel = LeNet()\n\nprint(model)","aab4126c":"# Defining optimizer as Adam with a learning rate of 0.0001 and loss function as Mean Squared Error\n\noptimizer = Adam(model.parameters(), lr = 0.0001)\ncriterion = nn.MSELoss()","b2578c4d":"# Sending the model to GPU if available\n\ncuda = torch.cuda.is_available()\nprint(\"GPU available:\", cuda)\n\nif cuda:\n    model.cuda()","05d44b65":"# Creating folder to save the epochs\n!mkdir saved_models","2fbf89da":"# Training LeNet with training set and evaluating on validation dataset\n\nEPOCHS = 200\n\ntrain_epoch_loss = []\nvalidation_epoch_loss = []\n\ntrain_epoch_accuracy = []\nvalidation_epoch_accuracy = []\n\nfor epoch in range(EPOCHS):\n    train_loss = []\n    validation_loss = []\n    \n    train_accuracy = []\n    val_accuracy = []\n\n    for batch_index, (train_image, train_label) in enumerate(train_loader):\n        # If GPU is available, move the data to the GPU for faster computation.\n        \n        train_image = train_image.view(32,3,32,32).type(torch.FloatTensor)\n        train_label_one_hot = func.one_hot(train_label,num_classes).type(torch.FloatTensor)\n        \n        if cuda:\n            #######################################################\n            ####################### Train #########################\n            #######################################################\n            # Set the model to train mode so that the parameters can be updated.\n            model.train()\n\n            train_label_predicted = model(train_image.cuda())\n\n            # compute the loss\n            loss = criterion(train_label_predicted, train_label_one_hot.cuda())\n            train_loss.append(loss.cpu().data.item())\n\n            # reset the gradient \n            optimizer.zero_grad()\n            # backpropagate the loss\n            loss.backward()\n            # update the parameters\n            optimizer.step()\n            \n        \n        # If GPU is not available.\n        else:\n            #######################################################\n            ####################### Train #########################\n            #######################################################\n            # Set the model to train mode so that the parameters can be updated.\n            model.train()\n\n            train_label_predicted = model(train_image)\n\n            # compute the loss\n            loss = criterion(train_label_predicted, train_label_one_hot)\n            train_loss.append(loss.cpu().data.item())\n\n            # reset the gradient\n            optimizer.zero_grad()\n            # backpropagate the loss\n            loss.backward()\n            # update the parameters\n            optimizer.step()\n            \n        model.eval()\n        # compute the accuracy\n        train_label_predicted_all = []\n        train_label_predicted_probability, train_label_predicted_index = torch.max(train_label_predicted.data, 1)\n        for current_prediction in train_label_predicted_index:\n            train_label_predicted_all.append(current_prediction.detach().cpu().numpy().item())\n\n        accuracy = accuracy_score(train_label, train_label_predicted_all)\n        train_accuracy.append(accuracy)\n\n            \n    for batch_index_val, (val_image, val_label) in enumerate(val_loader):\n        val_label_one_hot = func.one_hot(val_label,num_classes).type(torch.FloatTensor)\n       \n        if cuda:\n            #######################################################\n            ###################### Validation #####################\n            #######################################################\n            # Set the model to evaluation mode so that parameters are fixed.\n            model.eval ()\n            validation_label_predicted = model(val_image.cuda())\n\n            loss = criterion(validation_label_predicted, val_label_one_hot.cuda())\n            validation_loss.append(loss.cpu().data.item())\n        \n            \n        else:\n            model.eval ()\n            validation_label_predicted = model(val_image)\n            loss = criterion(validation_label_predicted, val_label_one_hot)\n            validation_loss.append(loss.cpu().data.item())    \n            \n        # compute the accuracy\n        val_label_predicted_all = []\n        val_label_predicted_probability, val_label_predicted_index = torch.max(validation_label_predicted.data, 1)\n        for current_prediction in val_label_predicted_index:\n            val_label_predicted_all.append(current_prediction.detach().cpu().numpy().item())\n\n        accuracy = accuracy_score(val_label, val_label_predicted_all)\n        val_accuracy.append(accuracy)\n\n    train_epoch_loss.append(np.mean(train_loss))\n    validation_epoch_loss.append(np.mean(validation_loss))\n    train_epoch_accuracy.append(np.mean(train_accuracy))\n    validation_epoch_accuracy.append(np.mean(val_accuracy))\n    # save models\n    torch.save(model.state_dict(), '.\/saved_models\/checkpoint_epoch_%s.pth' % (epoch))\n\n    print(\"Epoch: {} | train_loss: {} | train_accuracy: {} | validation_loss: {} | validation_accuracy: {} \".format(epoch, train_epoch_loss[-1], train_epoch_accuracy[-1], validation_epoch_loss[-1], validation_epoch_accuracy[-1]))\n    \n        \n# Saving losses and accuracies for current model\nlenet_loss = train_epoch_loss\nlenet_acc = train_epoch_accuracy\nlenet_val_loss = validation_epoch_loss\nlenet_val_acc = validation_epoch_accuracy","1af0b448":"# Displaying the learning curve\nplt.figure(figsize = (12, 8))\nplt.title('LeNet: Learning curve')\nplt.plot(lenet_loss, '-o', label = 'training loss', markersize = 3)\nplt.plot(lenet_val_loss, '-o', label = 'validation loss', markersize = 3)\nplt.plot(lenet_acc, '-o', label = 'training accuracy', markersize = 3)\nplt.plot(lenet_val_acc, '-o', label = 'validation accuracy', markersize = 3)\nplt.legend(loc = 'upper right');\nplt.xlabel('Epochs')","1d894c2c":"# Selecting the epoch corresponding to the lowest validation loss\n\nbest_epoch = np.argmin(lenet_val_loss)\nprint('best epoch: ', best_epoch)","9661e866":"# Loading the best model (corresponding to the parameters of the best epoch)\n\nstate_dict = torch.load('.\/saved_models\/checkpoint_epoch_%s.pth' % (best_epoch))\nprint(state_dict.keys())\nmodel.load_state_dict(state_dict)","b3853a5f":"# Testing the best model on the test set\n\ndef predict(model, input_data):\n    model.eval()\n    label_predicted_all = []\n\n    label_predicted_one_hot = model(input_data)\n    label_predicted_probability, label_predicted_index = torch.max(label_predicted_one_hot.data, 1)\n\n    for current_prediction in label_predicted_index:\n        label_predicted_all.append(current_prediction.detach().cpu().numpy().item())\n\n    return label_predicted_all\n\ntest_label_predicted = []\n\nfor batch_index, (test_image, test_label) in enumerate(test_loader):    \n\n    if cuda:\n        test_label_predicted += predict(model, test_image.cuda())\n    else:\n        test_label_predicted += predict(model, test_image)\n\naccuracy = accuracy_score(testset.targets, test_label_predicted)\n\nprint(\"Accuracy:\", accuracy * 100, \"%\")\n    ","4c567f39":"# Displaying the confusion matrix\n\nCM = confusion_matrix(testset.targets, test_label_predicted)\n\nplt.figure(figsize = (12,10))\nsns.heatmap(CM, annot = True, annot_kws = {\"size\": num_classes})\nplt.ylim([0, 10]);\nplt.ylabel('True labels');\nplt.xlabel('predicted labels');","68ac9e6b":"# Displaying the classification report\nprint(classification_report(testset.targets, test_label_predicted, labels=[i for i in range(num_classes)]))","c1b40c60":"# Displaying five random samples from each class along with its true label and predicted label\n\nfig, axs = plt.subplots(num_classes, 5, figsize=(32,32))\nplt.suptitle('Random 5 samples of CIFAR-10 test dataset and their estimated labels\\nActual label -> Estimated label for each class')\n\nfor class_nb in range(num_classes):\n    testlabel = torch.tensor(testset.targets)    \n    testindexes = torch.nonzero(testlabel == class_nb).squeeze()\n    fiveindexes = np.random.choice(testindexes, 5)\n    for i in range(len(fiveindexes)):\n        index = fiveindexes[i]\n        axs[class_nb,i].set_title('%d -> %d' % (testset.targets[index], test_label_predicted[index]))\n        img = testset[index][0]\n        npimg = img.numpy()\n        axs[class_nb,i].imshow(np.transpose(npimg, (1, 2, 0)))\n        axs[class_nb,i].axis('off')","0c19fee6":"# Defining a new instance of LeNet with a learning rate of 0.005 for Adam optimizer and the Cross-Entropy loss function\nmodel_CE = LeNet()\noptimizer = Adam(model_CE.parameters(), lr = 0.005)\ncriterion = nn.CrossEntropyLoss()","cf5727b5":"# Sending the model to GPU if available\n\ncuda = torch.cuda.is_available()\nprint(\"GPU available:\", cuda)\n\nif cuda:\n    model_CE.cuda()","cd91826f":"# Creating folder to save the epochs\n!mkdir saved_models_CE","44f9032b":"# Training LeNet with training set and evaluating on validation dataset\n\nEPOCHS = 200\n\ntrain_epoch_loss = []\nvalidation_epoch_loss = []\n\ntrain_epoch_accuracy = []\nvalidation_epoch_accuracy = []\n\nfor epoch in range(EPOCHS):\n    train_loss = []\n    validation_loss = []\n    \n    train_accuracy = []\n    val_accuracy = []\n\n    for batch_index, (train_image, train_label) in enumerate(train_loader):\n        # If GPU is available, move the data to the GPU for faster computation.\n        \n        train_image = train_image.view(32,3,32,32).type(torch.FloatTensor)\n        \n        if cuda:\n            #######################################################\n            ####################### Train #########################\n            #######################################################\n            # Set the model to train mode so that the parameters can be updated.\n            model_CE.train()\n\n            train_label_predicted = model_CE(train_image.cuda())\n\n            # compute the loss\n            loss = criterion(train_label_predicted, train_label.cuda())\n            train_loss.append(loss.cpu().data.item())\n\n            # reset the gradient \n            optimizer.zero_grad()\n            # backpropagate the loss\n            loss.backward()\n            # update the parameters\n            optimizer.step()\n            \n        \n        # If GPU is not available.\n        else:\n            #######################################################\n            ####################### Train #########################\n            #######################################################\n            # Set the model to train mode so that the parameters can be updated.\n            model_CE.train()\n\n            train_label_predicted = model_CE(train_image)\n\n            # compute the loss\n            loss = criterion(train_label_predicted, train_label)\n            train_loss.append(loss.cpu().data.item())\n\n            # reset the gradient\n            optimizer.zero_grad()\n            # backpropagate the loss\n            loss.backward()\n            # update the parameters\n            optimizer.step()\n            \n        model_CE.eval()\n        # compute the accuracy\n        train_label_predicted_all = []\n        train_label_predicted_probability, train_label_predicted_index = torch.max(train_label_predicted.data, 1)\n        for current_prediction in train_label_predicted_index:\n            train_label_predicted_all.append(current_prediction.detach().cpu().numpy().item())\n\n        accuracy = accuracy_score(train_label, train_label_predicted_all)\n        train_accuracy.append(accuracy)\n\n            \n    for batch_index_val, (val_image, val_label) in enumerate(val_loader):\n        \n        if cuda:\n            #######################################################\n            ###################### Validation #####################\n            #######################################################\n            # Set the model to evaluation mode so that parameters are fixed.\n            model_CE.eval ()\n            validation_label_predicted = model_CE(val_image.cuda())\n\n            loss = criterion(validation_label_predicted, val_label.cuda())\n            validation_loss.append(loss.cpu().data.item())\n            \n            \n        else:\n            model_CE.eval ()\n            validation_label_predicted = model_CE(val_image)\n            loss = criterion(validation_label_predicted, val_label)\n            validation_loss.append(loss.cpu().data.item())\n            \n         # compute the accuracy\n        val_label_predicted_all = []\n        val_label_predicted_probability, val_label_predicted_index = torch.max(validation_label_predicted.data, 1)\n        for current_prediction in val_label_predicted_index:\n            val_label_predicted_all.append(current_prediction.detach().cpu().numpy().item())\n\n        accuracy = accuracy_score(val_label, val_label_predicted_all)\n        val_accuracy.append(accuracy)\n\n    train_epoch_loss.append(np.mean(train_loss))\n    validation_epoch_loss.append(np.mean(validation_loss))\n    train_epoch_accuracy.append(np.mean(train_accuracy))\n    validation_epoch_accuracy.append(np.mean(val_accuracy))\n    # save models\n    torch.save(model_CE.state_dict(), '.\/saved_models_CE\/checkpoint_epoch_%s.pth' % (epoch))\n\n    print(\"Epoch: {} | train_loss: {} | train_accuracy: {} | validation_loss: {} | validation_accuracy: {} \".format(epoch, train_epoch_loss[-1], train_epoch_accuracy[-1], validation_epoch_loss[-1], validation_epoch_accuracy[-1]))\n    \n    # Saving losses and accuracies for current model\n    lenetCE_loss = train_epoch_loss\n    lenetCE_acc = train_epoch_accuracy\n    lenetCE_val_loss = validation_epoch_loss\n    lenetCE_val_acc = validation_epoch_accuracy","ac3a5a26":"# Displaying the learning curve\nplt.figure(figsize = (12, 8))\nplt.title('LeNet: Learning curve')\nplt.plot(lenetCE_loss, '-o', label = 'training loss', markersize = 3)\nplt.plot(lenetCE_val_loss, '-o', label = 'validation loss', markersize = 3)\nplt.plot(lenetCE_acc, '-o', label = 'training accuracy', markersize = 3)\nplt.plot(lenetCE_val_acc, '-o', label = 'validation accuracy', markersize = 3)\nplt.legend(loc = 'upper right');\nplt.xlabel('Epochs')","991aafbd":"# Selecting the epoch corresponding to the lowest validation loss\n\nbest_epoch = np.argmin(lenetCE_val_loss)\nprint('best epoch: ', best_epoch)","d014723a":"# Loading the best model (corresponding to the parameters of the best epoch)\n\nstate_dict = torch.load('.\/saved_models_CE\/checkpoint_epoch_%s.pth' % (best_epoch))\nprint(state_dict.keys())\nmodel_CE.load_state_dict(state_dict)","b48eb326":"# Testing the best model on the test set\n\ntest_label_predicted = []\n\nfor batch_index, (test_image, test_label) in enumerate(test_loader):    \n\n    if cuda:\n        test_label_predicted += predict(model_CE, test_image.cuda())\n    else:\n        test_label_predicted += predict(model_CE, test_image)\n\naccuracy = accuracy_score(testset.targets, test_label_predicted)\n\nprint(\"Accuracy:\", accuracy * 100, \"%\")","99b14b71":"# Displaying the confusion matrix\n\nCM = confusion_matrix(testset.targets, test_label_predicted)\n\nplt.figure(figsize = (12,10))\nsns.heatmap(CM, annot = True, annot_kws = {\"size\": num_classes})\nplt.ylim([0, 10]);\nplt.ylabel('True labels');\nplt.xlabel('predicted labels');","c98bfdb3":"# Displaying the classification report\n\nprint(classification_report(testset.targets, test_label_predicted, labels=[i for i in range(num_classes)]))","93b92db2":"# Displaying five random samples from each class along with its true label and predicted label\n\nfig, axs = plt.subplots(num_classes, 5, figsize=(32,32))\nplt.suptitle('Random 5 samples of CIFAR-10 test dataset and their estimated labels\\nActual label -> Estimated label for each class')\n\nfor class_nb in range(num_classes):\n    testlabel = torch.tensor(testset.targets)    \n    testindexes = torch.nonzero(testlabel == class_nb).squeeze()\n    fiveindexes = np.random.choice(testindexes, 5)\n    for i in range(len(fiveindexes)):\n        index = fiveindexes[i]\n        axs[class_nb,i].set_title('%d -> %d' % (testset.targets[index], test_label_predicted[index]))\n        img = testset[index][0]\n        npimg = img.numpy()\n        axs[class_nb,i].imshow(np.transpose(npimg, (1, 2, 0)))\n        axs[class_nb,i].axis('off')","ad64ec18":"# Implementing VGGNet16\n\nclass AlexNet(nn.Module):\n    def __init__(self):\n        super(AlexNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 96, kernel_size=5)\n        self.conv2 = nn.Conv2d(96, 256, kernel_size=5, padding=2)\n        self.conv3 = nn.Conv2d(256, 384, kernel_size=3, padding=1)\n        self.conv4 = nn.Conv2d(384, 384, kernel_size=3, padding=1)\n        self.conv5 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n        self.drop1 = nn.Dropout()\n        self.fc1 = nn.Linear(256*3*3, 4096)\n        self.drop2 = nn.Dropout()\n        self.fc2 = nn.Linear(4096, 4096)\n        self.fc3 = nn.Linear(4096, 10)\n\n    def forward(self, x):\n        x = func.relu(self.conv1(x))\n        x = func.max_pool2d(x, 2)\n        x = func.relu(self.conv2(x))\n        x = func.max_pool2d(x,2)\n        x = func.relu(self.conv3(x))\n        x = func.relu(self.conv4(x))\n        x = func.relu(self.conv5(x))\n        x = func.max_pool2d(x, 3, stride = 2)\n        x = x.view(x.size(0), -1)\n        x = self.drop1(x)\n        x = func.relu(self.fc1(x))\n        x = self.drop2(x)\n        x = func.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n    \nmodelAlexNet = AlexNet()\n\nprint(modelAlexNet)","085fefac":"# Sending model to GPU if available\n\ncuda = torch.cuda.is_available()\nprint(\"GPU available:\", cuda)\n\nif cuda:\n    modelAlexNet.cuda()","150d20ed":"# Defining learning rate of 0.001 for Adam optimizer and the Cross-Entropy loss function\n\noptimizer = Adam(modelAlexNet.parameters(), lr = 0.001)\ncriterion = nn.CrossEntropyLoss()","160a6a10":"# Creating folder to save the epochs\n\n!mkdir saved_models_AlexNet","5c41c0d8":"# Training AlexNet with training set and evaluating on validation dataset\n\nEPOCHS = 200\n\ntrain_epoch_loss = []\nvalidation_epoch_loss = []\n\ntrain_epoch_accuracy = []\nvalidation_epoch_accuracy = []\n\nfor epoch in range(EPOCHS):\n    train_loss = []\n    validation_loss = []\n    \n    train_accuracy = []\n    val_accuracy = []\n\n    for batch_index, (train_image, train_label) in enumerate(train_loader):\n        # If GPU is available, move the data to the GPU for faster computation.\n        \n        train_image = train_image.view(32,3,32,32).type(torch.FloatTensor)\n        \n        if cuda:\n            #######################################################\n            ####################### Train #########################\n            #######################################################\n            # Set the model to train mode so that the parameters can be updated.\n            modelAlexNet.train()\n\n            train_label_predicted = modelAlexNet(train_image.cuda())\n\n            # compute the loss\n            loss = criterion(train_label_predicted, train_label.cuda())\n            train_loss.append(loss.cpu().data.item())\n\n            # reset the gradient \n            optimizer.zero_grad()\n            # backpropagate the loss\n            loss.backward()\n            # update the parameters\n            optimizer.step()\n\n        # If GPU is not available.\n        else:\n            #######################################################\n            ####################### Train #########################\n            #######################################################\n            # Set the model to train mode so that the parameters can be updated.\n            modelAlexNet.train()\n\n            train_label_predicted = modelAlexNet(train_image)\n\n            # compute the loss\n            loss = criterion(train_label_predicted, train_label)\n            train_loss.append(loss.cpu().data.item())\n\n            # reset the gradient\n            optimizer.zero_grad()\n            # backpropagate the loss\n            loss.backward()\n            # update the parameters\n            optimizer.step()\n            \n        modelAlexNet.eval()\n        # compute the accuracy\n        train_label_predicted_all = []\n        train_label_predicted_probability, train_label_predicted_index = torch.max(train_label_predicted.data, 1)\n        for current_prediction in train_label_predicted_index:\n            train_label_predicted_all.append(current_prediction.detach().cpu().numpy().item())\n\n        accuracy = accuracy_score(train_label, train_label_predicted_all)\n        train_accuracy.append(accuracy)\n\n            \n    for batch_index_val, (val_image, val_label) in enumerate(val_loader):\n        \n        if cuda:\n            #######################################################\n            ###################### Validation #####################\n            #######################################################\n            # Set the model to evaluation mode so that parameters are fixed.\n            modelAlexNet.eval ()\n            validation_label_predicted = modelAlexNet(val_image.cuda())\n\n            loss = criterion(validation_label_predicted, val_label.cuda())\n            validation_loss.append(loss.cpu().data.item())\n\n            \n        else:\n            modelAlexNet.eval ()\n            validation_label_predicted = modelAlexNet(val_image)\n            loss = criterion(validation_label_predicted, val_label)\n            validation_loss.append(loss.cpu().data.item())\n            \n         # compute the accuracy\n        val_label_predicted_all = []\n        val_label_predicted_probability, val_label_predicted_index = torch.max(validation_label_predicted.data, 1)\n        for current_prediction in val_label_predicted_index:\n            val_label_predicted_all.append(current_prediction.detach().cpu().numpy().item())\n\n        accuracy = accuracy_score(val_label, val_label_predicted_all)\n        val_accuracy.append(accuracy)\n\n    train_epoch_loss.append(np.mean(train_loss))\n    validation_epoch_loss.append(np.mean(validation_loss))\n    train_epoch_accuracy.append(np.mean(train_accuracy))\n    validation_epoch_accuracy.append(np.mean(val_accuracy))\n    # save models\n    torch.save(modelAlexNet.state_dict(), '.\/saved_models_AlexNet\/checkpoint_epoch_%s.pth' % (epoch))\n\n    print(\"Epoch: {} | train_loss: {} | train_accuracy: {} | validation_loss: {} | validation_accuracy: {} \".format(epoch, train_epoch_loss[-1], train_epoch_accuracy[-1], validation_epoch_loss[-1], validation_epoch_accuracy[-1]))\n    \n    # Saving losses and accuracies for current model\n    alexnet_loss = train_epoch_loss\n    alexnet_acc = train_epoch_accuracy\n    alexnet_val_loss = validation_epoch_loss\n    alexnet_val_acc = validation_epoch_accuracy","aa8e349f":"# Displaying the learning curve\nplt.figure(figsize = (12, 8))\nplt.title('AlexNet: Learning curve')\nplt.plot(alexnet_loss, '-o', label = 'training loss', markersize = 3)\nplt.plot(alexnet_val_loss, '-o', label = 'validation loss', markersize = 3)\nplt.plot(alexnet_acc, '-o', label = 'training accuracy', markersize = 3)\nplt.plot(alexnet_val_acc, '-o', label = 'validation accuracy', markersize = 3)\nplt.legend(loc = 'upper right');\nplt.xlabel('Epochs')","b4aa6032":"# Selecting the epoch corresponding to the lowest validation loss\n\nbest_epoch = np.argmin(alexnet_val_loss)\nprint('best epoch: ', best_epoch)","b089c436":"# Loading the best model (corresponding to the parameters of the best epoch)\n\nstate_dict = torch.load('.\/saved_models_AlexNet\/checkpoint_epoch_%s.pth' % (best_epoch))\nprint(state_dict.keys())\nmodelAlexNet.load_state_dict(state_dict)","b8148655":"# Testing the best model on the test set\n\ntest_label_predicted = []\n\nfor batch_index, (test_image, test_label) in enumerate(test_loader):    \n\n    if cuda:\n        test_label_predicted += predict(modelAlexNet, test_image.cuda())\n    else:\n        test_label_predicted += predict(modelAlexNet, test_image)\n\naccuracy = accuracy_score(testset.targets, test_label_predicted)\n\nprint(\"Accuracy:\", accuracy * 100, \"%\")","61570d95":"# Displaying the confusion matrix\n\nCM = confusion_matrix(testset.targets, test_label_predicted)\n\nplt.figure(figsize = (12,10))\nsns.heatmap(CM, annot = True, annot_kws = {\"size\": num_classes})\nplt.ylim([0, 10]);\nplt.ylabel('True labels');\nplt.xlabel('predicted labels');","15c6981b":"# Displaying the classification report\n\nprint(classification_report(testset.targets, test_label_predicted, labels=[i for i in range(num_classes)]))","789712f1":"# Displaying five random samples from each class along with its true label and predicted label\n\nfig, axs = plt.subplots(num_classes, 5, figsize=(32,32))\nplt.suptitle('Random 5 samples of CIFAR-10 test dataset and their estimated labels\\nActual label -> Estimated label for each class')\n\nfor class_nb in range(num_classes):\n    testlabel = torch.tensor(testset.targets)    \n    testindexes = torch.nonzero(testlabel == class_nb).squeeze()\n    fiveindexes = np.random.choice(testindexes, 5)\n    for i in range(len(fiveindexes)):\n        index = fiveindexes[i]\n        axs[class_nb,i].set_title('%d -> %d' % (testset.targets[index], test_label_predicted[index]))\n        img = testset[index][0]\n        npimg = img.numpy()\n        axs[class_nb,i].imshow(np.transpose(npimg, (1, 2, 0)))\n        axs[class_nb,i].axis('off')","0f7075d2":"Adam: The Adam optimizer not only updates the model's parameters but it also updates the learning rate in the following manner before updating the model's parameters. It computes the exponential moving average of the gradient and that of the squared gradient, as well as the corresponding decay rates along each parameter. It uses those resulting values to update the learning rate before computing the new parameters.  \n\nMSE: The Mean Squared Error loss function takes the mean of the squared of the errors made by a classifier over all samples. The error is computed by taking the difference between the predicted value (probabilities for each class) and the true value for each sample (1 for its true class and 0 for other classes). ","16577b1c":"### 10. (5 pts.) Display the learning curve and illustrate the best epoch. Explain your criteria for the best epoch.\nThe learning curve shows the model's loss and accuracy at the end of each epoch for all epochs (200 epochs). The criteria for the best epoch can be the minimum loss or maximum accuracy or other criteria.","4aa731e6":"### 1. (5 pts.) According to the [CIFAR10](http:\/\/www.cs.toronto.edu\/~kriz\/cifar.html) dataset descriptions and other online resources, please identify the quantities below:\n\na) Total Number of samples \\\nb) Number of classes \\\nc) Image size \\\nd) Write class names and their corresponding label index (e.g., 0. cats, 1. dogs, ...) \\\ne) Intensity range","8bbe6859":"### 2. (0 pts.) Import the required packages in the following code block. \nYou can always come back here and import another package; please keep them all in the following block to make your code well-organized.","606400fc":"I chose the epoch with the lowest validation loss because it can better generalize to other data samples and has a better chance of performing well on the test set.","77ea9ab6":"### 7. (10 pts.) According to the LeNet architecture below, create a fully-connected model. Also, identify the architeture's hyper-parameters, activation functions, and tensor shapes.\nArchitecture hyper-parameter includes the number of layers, number of kernels in each layer, size of the kernels, stride, zero-padding size. Just by looking at the architecture itself, you should be able to identify the hyper-parameters. Keep in mind that you identified the $W, H$, and $N$ (Which refers to the number of classes) in the first question.\nFor more help, look at this [implementation](https:\/\/github.com\/icpm\/pytorch-cifar10\/blob\/master\/models\/LeNet.py).\n![LeNet Architecture](https:\/\/raw.githubusercontent.com\/soroush361\/DeepLearningInBME\/main\/Ass1_Arch1.png)","834dfa1a":"a) Total Number of samples: 60000 \\\nb) Number of classes: 10 \\\nc) Image size: 32x32 pixels in 3 channels \\\nd) Write class names and their corresponding label index: 0. airplanes, 1. automobiles, 2. birds, 3. cats, 4. deers, 5. dogs, 6. frogs, 7, horses, 8. ships, 9. trucks \\\ne) Intensity range: 0-255","a9efa4c8":"### 13. (20 pts.) Repeat the training, validation, and testing with the [Cross-Entropy](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) loss function and initial learning rate of $0.005$. Explain how the model's performance changed.\nEssentially you need to copy all the codes above and just change the loss function and edit the learning rate. Obviously, you don't need to re-import the dataset and the libraries. However, you need to create a new instance of the architecture. Otherwise, the weights would be the same as the last epoch (or the best epoch) in the last part. To avoid overwriting your previously trained model, change the save directories in the training loop.","d6c4fecb":"## Assignment 2\n### Due date:<font color='red'> 11:59 pm, Febraury 25, 2021<\/font>","7753f0e8":"The **model hyperparameters** are the following:\n* the number of layers: 8 layers (5 convolutional layers, 2 fully connected layers and 1 output layer also fully connected) + 3 pooling layers and 2 dropout layers\n* number of kernels in each layer: Conv1 --> 96, Conv2 --> 256, Conv3 --> 384, Conv4 --> 384, Conv5 --> 256, Pooling layers --> 1\n* size of the kernels: 5x5 for Conv1 and Conv2, 3x3 for Conv3, Conv4 and Conv5, 2x2 for pooling layers 1 and 2, 3x3 for last pooling layer\n* stride: 1 for convolutional layers, 2 for pooling layers \n* zero-padding size: 0 for Conv1, 2 for Conv2, 1 for Conv 3,4,5, 0 for pooling layers\n\nThe **activation functions** are the ReLU (Rectified Linear Unit) function for the convolutional layers as well as the first two fully connected layers. The output layer has a linear activation function but the Cross-entropy loss function will apply Softmax to the output layer as an activation function.\n\n\n**Tensor shapes**\n* Conv 1: (input) 3x32x32 --> (output) 96x28x28\n* MaxPool 1: (input) 96x28x28 --> (output) 96x14x14\n* Conv 2: (input) 96x14x14 --> (output) 256x14x14\n* MaxPool 2: (input) 256x14x14 --> (output) 256x7x7\n* Conv 3: (input) 256x7x7 --> (output) 384x7x7\n* Conv 4: (input) 384x7x7 --> (output) 384x7x7\n* Conv 5: (input) 384x7x7 --> (output) 256x7x7\n* MaxPool 3: (input) 256x7x7 --> (output) 256x3x3\n* FC 1: (input) 256$*$3$*$3 = 1x2304 --> (output) 1x4096\n* FC 2: (input) 1x4096 --> (output) 1x4096\n* FC 2: (input) 1x4096 --> (output) 1x10","826828da":"### 5. (5 pts.) Split up the train set into new train and validation sets so that the number of samples in the validation set is equal to the number of samples in the test set. Then create a [```DataLoader```](https:\/\/pytorch.org\/docs\/stable\/data.html#torch.utils.data.DataLoader) for each set, including the test set, with a batch size of 32.\nThe [Pytorch CIFAR10](https:\/\/pytorch.org\/tutorials\/beginner\/blitz\/cifar10_tutorial.html) tutorial will also help you here.\nMake sure none of the samples in the validation set exists in the new train set.","002a0b82":"## Introduction\nIn this assignment, you will implement, train, and test [LeNet](https:\/\/en.wikipedia.org\/wiki\/LeNet) model to classify [CIFAR10](http:\/\/www.cs.toronto.edu\/~kriz\/cifar.html) dataset.\n\n## Instructions\nDepending on each question, there are empty blocks you need to fill. The code blocks are only for Python codes and comments and currently have a comment like ```# [Your code here]```. The markdown blocks are only for plain or laTex text that you may use for answering descriptive questions. Currently, the markdown blocks have a comment like <font color='red'>[your answer here]<\/font>. Please remove the comments before filling the blocks. You can always add more blocks if you need to, or it just makes your answers well-organized.\n\nAlthough you may use other available online resources (such as GitHub, Kaggle Notebooks), it is highly recommended to try your best to do it yourself. If you are using an online code or paper, make sure you cite their work properly to avoid plagiarism. Using other students' works is absolutely forbidden and considered cheating.\n\nWrite comments for your codes in the big picture mode. One comment line at the top of each code block is necessary to explain what you did in that block. Don't comment on every detail.\n\nName your variables properly that represent the data they are holding, such as ``` test_set = ..., learning_rate = ...``` not ```a = ..., b = ...```.\n\nImplementing and reporting results using other architectures than LeNet will grant you an extra 20% on grade.\n\nIn this [Kaggle Notebook](https:\/\/www.kaggle.com\/roblexnana\/cifar10-with-cnn-for-beginer), you may find useful information about how your outputs must look. Just remember, they are using a different architecture, and they are using TensorFlow for implementations.\n\n\n## How to submit:\nAfter you have completed the assignment: \n1. Save a version (You can use 'Quick Save' to avoid re-running the whole notebook)\n2. Make the saved version public (Share -> Public)\n3. Copy the 'Public URL'\n4. Download your completed notebook as a '.ipynb' file (File -> Download)\n5. Upload the 'Public URL' and the '.ipynb' files on the [CourseWorks](https:\/\/courseworks2.columbia.edu\/).","a3194a7f":"### 12. (5 pts.) Display five random samples of each class titled with the true label and the predicted label. Comment on your model's performance.\nSamples must be pulled from the test set.","42ec0903":"# Deep Learning in Biomedical Engineering","678feb02":"The **model hyperparameters** are the following:\n* the number of layers: 7 layers (2 convolutional layers, 2 pooling layers, 2 fully connected layers and 1 output layer also fully connected)\n* number of kernels in each layer: Conv1 --> 6, Pool1 --> 1, Conv2 --> 16, Pool2 --> 1\n* size of the kernels: 5x5 for convolutional layer, 2x2 for pooling layers\n* stride: 1 for convolutional layers, 2 for pooling layers \n* zero-padding size: 0 for all kernels\n\nThe **activation functions** are the ReLU (Rectified Linear Unit) function for the convolutional layers as well as the first two fully connected layers. The output layer has a linear activation function.\n\n\n**Tensor shapes**\n* Conv 1: (input) 3x32x32 --> (output) 6x28x28\n* MaxPool 1: (input) 6x28x28 --> (output) 6x14x14\n* Conv 2: (input) 6x14x14 --> (output) 16x10x10\n* MaxPool 1: (input) 16x10x10 --> (output) 16x5x5\n* FC 1: (input) 16$*$5$*$5 = 1x400 --> (output) 1x120\n* FC 2: (input) 1x120 --> (output) 1x84\n* FC 2: (input) 1x84 --> (output) 1x10","9c140ea0":"### 8. (5 pts.) Create an instance of [ADAM optimizer](https:\/\/pytorch.org\/docs\/stable\/optim.html#torch.optim.Adam) with an initial learning rate of $0.0001$ and an instance of [Mean Squared Error (MSE)](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.MSELoss.html#torch.nn.MSELoss) loss function. Briefly explain the ADAM optimizer algorithm and the MSE loss function.\nFor ADAM optimizer, keep other arguments as default. \n\nFor your information, here is the mathematics behind the ADAM optimizer: \\\nFor each parameter $w^j$\n$$\nv_t = \\beta_1v_{t-1}-(1-\\beta_1)g_t \\\\\ns_t = \\beta_2s_{t-1}-(1-\\beta_2)g_t^2 \\\\\n\\Delta w^j = -\\eta\\frac{v_t}{\\sqrt{s_t+\\epsilon}}g_t \\\\\nw^j_{t+1} = w^j_t+\\Delta w^j\n$$\nWhere $\\eta$ is the initial learning rate, $g_t$ is the gradient at time $t$ along $w^j$, $v_t$ is the exponential average of gradients along $w^j$, $s_t$ is the exponential average of squares of gradients along $w^j$, $\\beta_1, \\beta_2$ are the hyper-parameters, and $\\epsilon$ is a small number to avoid dividing by zero.\n\nThe MSE loss function is:\n$$\nL(y,\\hat{y}) = \\frac{1}{N}\\sum_{i=1}^N{(y_i-\\hat{y}_i)^2}\n$$\nWhere $y$ is the true value, $\\hat{y}$ is the predicted value, $N$ is the number of classes. Keep in mind that $y$ is a one-hot vector like $y=\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ \\vdots \\end{bmatrix}$ (This example of $y$ indicates that the sample belongs to class ID 2, remember it is zero-indexed) and $\\hat{y}=\\begin{bmatrix} 0.1 \\\\ 0.03 \\\\ 0.8 \\\\ \\vdots \\end{bmatrix}$ shows the probability of belonging to each class for the same sample and predicted by the model.\n\nFor other optimization algorithms and loss functions, check the links below:\n\n[Optimizers list](https:\/\/pytorch.org\/docs\/stable\/optim.html#algorithms) \n\n[Loss function list](https:\/\/pytorch.org\/docs\/stable\/nn.html#loss-functions)","a9b3da3b":"The model stoped improving around the 50th epoch since the training loss kept decreasing but the validation loss stopped decreasing. The best epoch's parameters were used to predict the classes of the test set. This led to an accuracy of 62.31%. It performed the best for the frog, automobile, horse and ship classes while it made the most wrong predictions for the bird, cat, deer and dog classes. In particular, the model mislabeled dogs as cats more often and vice versa (from the confusion matrix). From the sample images, we can see for example that the model had trouble classifying birds that were partially shown (not full body).","c837e109":"### 11. (10 pts.) Load the model's weights at the best epoch and test your model performance on the test set. Display the confusion matrix and classification report.","f32a3582":"### 3. (5 pts.) Load train and test sets using Pytorch datasets functions.\nMake sure the intensity range is between $[0, 1]$ and images are stored as 'tensor' type. \nYou may use transformers while downloading the dataset to do the job. Look at this tutorial: [Pytorch CIFAR10](https:\/\/pytorch.org\/tutorials\/beginner\/blitz\/cifar10_tutorial.html))","8a599fe1":"### 4. (10 pts.) Using the 'matplotlib' library, make a figure with $N\\times4$ grid cells where $N$ is the number of classes. Display one random sample of each class pulled from the train set in its corresponding row that depends on its class index and the first column and its histogram in the second column. Repeat this for the third and fourth columns but pull images from the test set.","ff8b31ac":"## Hortense Le - hal2142","02386756":"### 6. (5 pts.) Display the number of samples for each class in the train, validation, and test sets as a stacked bar plot similar to the '[FirstTutorial](https:\/\/www.kaggle.com\/soroush361\/deeplearninginbme-firsttutorial)'.","d5e24b41":"AlexNet model performed better than LeNet models. However, after converging all losses and accuracies curves started to fluctuate a lot (around epoch 50) and starting at around epoch 100, it started to perform very poorly on the training set (and the validation set). It stopped improving and dropped from a training accuracy of around 50% to 10%. It remained at that level for the remaining epochs. The best epoch (14) before that happened gave a final test accuracy of 68.66%. Since the model has a more complicated architecture, it took much longer to train and used more memory space (about an hour more and didn't have enough space for the remaining 30 epochs) but it was able to better correlate the data to find common class features. It performed the best for the automobile, airplane and ship classes and the worst for the dog class. As the first LeNet model, dogs were the most often misclassified as cats. From the samples displayed, we can notice that the model performed relatively well for all classes making the most misclassifications in the dog class (predicted 2 cats and one airplane instead) while trucks seems to be most mistaken as airplanes.","c7adf7d4":"### Extra. Implementation of modified [AlexNet](http:\/\/www.cs.toronto.edu\/~kriz\/imagenet_classification_with_deep_convolutional.pdf) (Krizhevsky, 2012).  Training, validation, and testing with the [Cross-Entropy](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) loss function and initial learning rate of $0.001$. \nI modified the architecture a little bit (kernel size, strides) so that the model takes as input our CIFAR-10 images of size 32x32. The first convolutional layer's kernel has a size of 5 instead of 11, with no stride nor padding.The pooling layers have a kernel size of 2 except the last one where I kept the original.\n\nThe original architecture is the following:\n![AlexNet-architecture-Includes-5-convolutional-layers-and-3-fullyconnected-layers.png](attachment:AlexNet-architecture-Includes-5-convolutional-layers-and-3-fullyconnected-layers.png)\n","1628c4aa":"### 9. (15 pts.) Train the model for 200 epochs using the train set and validate your model's performance at the end of each epoch using the validation sets.\nTo increase the training speed, use the GPU accelerator.\nLook at the '[FirstTutorial](https:\/\/www.kaggle.com\/soroush361\/deeplearninginbme-firsttutorial)' for more help.\nDo not forget to save the model at the end of each epoch.","d804522a":"With the new learning rate and the Cross-Entropy loss, the model performed much worse with a final test accuracy of 46.52%, which is much lower than the original model. The model converged very fast and provided the best epoch at epoch 13. It then stopped improving with later epochs and it failed to reduce the training loss. Towards the last epochs, the training loss also suddenly increased and stayed at that high value. The model only reached a maximum training accuracy of about 55%, as opposed to the previous model which had an increasing training accuracy up to about 88%. This modified model performed the best for the frog and ship classes. It also gave more wrong predictions for the bird and cat classes. Birds were mostly mislabeled as dogs, cats as dogs too (and vice versa), and deers as horses. From the samples displayed, airplanes were also often misclassified as ships. The learning rate might have been too high so that the model failed to recognize the right direction of the gradient towards the minimum of the training loss (steps are too big). This modified model made a lot more misclassification errors for all classes than the original model."}}