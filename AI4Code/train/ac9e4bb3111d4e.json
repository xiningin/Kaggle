{"cell_type":{"59694479":"code","1f42fb3e":"code","ed7dcbc2":"code","7de4da51":"code","e2126a49":"code","4ca379b0":"code","1b03db93":"code","1a488611":"code","46cf5636":"code","907e7d9b":"markdown","e40fa600":"markdown","9edbf6de":"markdown","41b40be0":"markdown","fc5702b6":"markdown","a807cee2":"markdown","241b0671":"markdown","ce622c8f":"markdown","bc12323e":"markdown"},"source":{"59694479":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport re\n\npapers = pd.read_csv('..\/input\/papers.csv')\npapers.head()","1f42fb3e":"papers = papers.drop([\"id\", \"event_type\", \"pdf_name\"], axis = 1)\n\npapers['title_processed'] = papers['title'].map(lambda x: re.sub('[,\\.!?]', '', x))\npapers['title_processed'] = papers['title_processed'].map(str.lower)","ed7dcbc2":"data = [title.split() for title in papers['title_processed']]\n\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\n\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndata = remove_stopwords(data)\nid2word = corpora.Dictionary(data)\ncorpus = [id2word.doc2bow(text) for text in data]","7de4da51":"from gensim.models import CoherenceModel\n\ndef compute_coherence_values(dictionary, data, corpus, limit, start, step):\n    coherence_values = []\n    model_list = []\n    for num_topics in range(start, limit, step):\n        model = lda_model = gensim.models.ldamodel.LdaModel(corpus = corpus,\n                                           id2word = dictionary,\n                                           num_topics = num_topics,\n                                           random_state = 10)\n        model_list.append(model)\n        coherencemodel = CoherenceModel(model = model,\n                                        texts = data, \n                                        dictionary = dictionary,\n                                        coherence = 'c_v')\n        coherence_values.append(coherencemodel.get_coherence())\n\n    return model_list, coherence_values","e2126a49":"start = 10; limit = 30; step = 2\n\nmodel_list, coherence_values = compute_coherence_values(dictionary = id2word, \n                                                        data = data, corpus = corpus, \n                                                        start = start, limit = limit,\n                                                        step = step)","4ca379b0":"x = range(start, limit, step)\nplt.figure(figsize = (13, 4))\nplt.plot(x, coherence_values, color = 'indigo')\nplt.xlabel(\"Num Topics\", fontsize = 13)\nplt.ylabel(\"Coherence score\", fontsize = 13)\nplt.xticks(x)\nplt.show()","1b03db93":"for m, cv in zip(x, coherence_values):\n    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 3))","1a488611":"optimal_model = model_list[5]\nmodel_topics = optimal_model.show_topics(20, formatted = False)\nfor i in range(20):\n    print((model_topics[i][0] + 1), (list(model_topics[i][1][j][0] for j in range(3))))","46cf5636":"import warnings\nwarnings.simplefilter(\"ignore\", FutureWarning)\n\nfrom sklearn.feature_extraction.text import CountVectorizer\ncount_vectorizer = CountVectorizer(stop_words = 'english')\ncount_data = count_vectorizer.fit_transform(papers['title_processed'])\n\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA\nlda = LDA(n_components = 20, random_state = 10)\nmodel = lda.fit(count_data)\n\nimport pyLDAvis\nimport pyLDAvis.sklearn\npyLDAvis.enable_notebook()\nvis = pyLDAvis.sklearn.prepare(model, count_data, count_vectorizer, mds='tsne')\nsaved = pyLDAvis.save_html(vis, fileobj = \"vis.html\")","907e7d9b":"I will now plot the coherence values, and choose the optimal model based on the figure.","e40fa600":"If the coherence score seems to keep increasing, it makes sense to pick the model that gave the highest CV before flattening out. This is exactly the case here.\n\nSo for further steps I will choose the model with 20 topics itself and print out the three most dominant words in each topic.","9edbf6de":"Since the last time around we got the ideal number of topics as 20, I will search between 10 and 30 for the ideal number of topics and run the function above. This can take a while to run as the function trains multiple LDA models, however it is not as slow as the GridSearch before.","41b40be0":"# 1. Introduction\n\nIn my last notebook on [Natural Language Processing (NLP)](https:\/\/www.kaggle.com\/gargimaheshwari\/nlp-machine-learning-topics-part-a) I used <code>sklearn<\/code>'s GridSearch to find the best LDA model for the number of macine learning topics in the NIPS papers. We saw that using <code>search_params<\/code> over <code>[20, 25, 30, 35]<\/code> number of topics and <code>[10, 20, 50 ]<\/code> maximum iterations, the best model turns out to be the one with 20 topics and 50 iterations, with a perplexity of approximately 2453.\n\nHere, I would like to take that analysis further.\n\nAn important point to note is that perplexity alone might not be the best measure to evaluate topic models because it doesn\u2019t consider the context and semantic associations between words. These can be captured using *topic coherence measure*. I will explore this measure here.\n\nFirst, I'll load the necessary libraries and the data.","fc5702b6":"As before, I will process the database to remove unnecessary fields, and then process the text using the <code>re<\/code> library.","a807cee2":"# 2. Pre-processing \n\nTo further process the data and to ready it for LDA to consume, it needs to be tokenized. However, unlike last time, I will not use <code>sklearn<\/code>'s resources, but instead I will use NLTK and Gensim. This is because our further analysis of topic coherence uses Gensim's resources, and we need to maintain continuity in the API.","241b0671":"![](https:\/\/i.imgur.com\/H87fvR3.jpg)","ce622c8f":"# 3. LDA Topic Coherence measure\n\nAn approach to finding the optimal number of topics is to build many LDA models with different values of number of topics (k) and pick the one that gives the highest coherence value. Choosing a \u2018k\u2019 that marks the end of a rapid growth of topic coherence usually offers meaningful and interpretable topics. Picking an even higher value can sometimes provide more granular sub-topics. If you see the same keywords being repeated in multiple topics, it\u2019s probably a sign that the \u2018k\u2019 is too large.\n\nThe function below trains multiple LDA models and provides the models and their corresponding coherence scores.","bc12323e":"Finally, as a last step, I will use python's <code>sklearn<\/code> to create an LDA model with 20 topics, and use the LDA visualizer to view the topics-keywords distribution. A good topic model will have non-overlapping, fairly big sized blobs for each topic. This seems to be the case here. So, we are good.\n\nThe visalization is also available in the output tab on the left, under <code>vis.html<\/code>."}}