{"cell_type":{"d353f747":"code","dc776848":"code","5489978d":"code","e1fd9126":"code","cd429bcb":"code","d9080350":"code","5f0c40c1":"code","e229f42e":"code","15a8f840":"code","e4bbec59":"code","de664da9":"code","bf41a4e7":"code","8c274c23":"code","a0ce7a56":"code","09a5195d":"code","cdb5ef61":"code","2dd6e3c7":"code","0169a594":"code","ed849dcd":"code","f810bdbb":"code","e80c810b":"code","7b85b3ab":"code","697d3b8c":"code","ceab963e":"code","04867ff6":"code","d9672c14":"code","a28aa569":"code","4327e228":"code","acad17d9":"code","a41be84d":"code","465d1266":"code","284b188f":"code","ed130a96":"code","4b566b57":"code","1a6223e6":"code","2539eebc":"code","0095a2ec":"code","f084d86f":"code","1d0f363c":"code","75f77b23":"code","63fa0904":"code","36d9d5a6":"code","11229326":"code","3f0e3817":"code","986921d0":"code","7e6c664d":"code","07ae8803":"code","9e476a63":"code","cc93a5a6":"code","27f90670":"code","9d929f38":"code","6826530a":"code","befb7741":"code","62afad7f":"code","bed7ea8f":"code","657a9897":"code","4f6ca9ce":"code","3d46e141":"code","642acd53":"code","7fb518bf":"code","7b472ec9":"code","1e0a159c":"code","f3641ef9":"code","98bcefab":"markdown","d8a572c6":"markdown","bacaa1ca":"markdown","a44f2a12":"markdown","0cf24a73":"markdown","c4bb4bee":"markdown","004b76f4":"markdown","fbcaa1f3":"markdown","80758d6e":"markdown","59380e17":"markdown"},"source":{"d353f747":"import pandas as pd\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain = pd.read_csv('..\/input\/mobile-price-classification\/train.csv')\ntest = pd.read_csv('..\/input\/mobile-price-classification\/test.csv')","dc776848":"train.head(3)","5489978d":"test.head(3)","e1fd9126":"train.columns = ['Power Battery', 'Bluetooth', 'Clock Speed','Dual SIM', \n                 'Front Camera', '4G','Int. Memory','Thickness','Weight',\n                 'Core Pros.','PC','Height','Width','RAM','SC H', 'SC W','Talk Time',\n                 '3G','Touch Screen','Wifi','Price_Range']\n\ntest.columns = ['ID','Power Battery', 'Bluetooth', 'Clock Speed','Dual SIM', \n                 'Front Camera', '4G','Int. Memory','Thickness','Weight',\n                 'Core Pros.','PC','Height','Width','RAM','SC H', 'SC W','Talk Time',\n                 '3G','Touch Screen','Wifi']","cd429bcb":"print(f'Count of unique item for each columns in train data:\\n{train.nunique().sort_values(ascending=False)}')\nprint('-'*20)\nprint(f'Count of unique item for each columns in test data:\\n{test.nunique().sort_values(ascending=False)}')","d9080350":"print(f'Shape of train data:\\n{train.shape}')\nprint('-'*20)\nprint(f'Shape of test data:\\n{test.shape}')","5f0c40c1":"print(f'Info of train data:\\n{train.info()}')\nprint('-'*50)\nprint(f'Info of test data:\\n{test.info()}')","e229f42e":"train.describe().round(decimals=0)","15a8f840":"test.describe().round(decimals=0)","e4bbec59":"print(f'Unique item in target feature:\\n{train.Price_Range.unique()}')","de664da9":"all_data = pd.concat([train,test])","bf41a4e7":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(nrows=4, ncols=2, figsize=(15, 12))\nf.suptitle('Countplot Graph', fontsize=16)\n\nsns.countplot(x='Core Pros.', data = all_data, palette='YlOrBr_r', ax=ax[0,0])\nax[0,0].set_ylabel(ylabel='Core Pros.', fontsize=14)\n\nsns.countplot(x='Wifi', data = all_data, palette='YlOrBr_r', ax=ax[0,1])\nax[0,1].set_ylabel(ylabel='Wifi', fontsize=14)\n\nsns.countplot(x='3G', data = all_data, palette='YlOrBr_r', ax=ax[1,0])\nax[1,0].set_ylabel(ylabel='3G', fontsize=14)\n\nsns.countplot(x='4G', data = all_data, palette='YlOrBr_r', ax=ax[1,1])\nax[1,1].set_ylabel(ylabel='4G', fontsize=14)\n\nsns.countplot(x='Dual SIM', data = all_data, palette='YlOrBr_r', ax=ax[2,0])\nax[2,0].set_ylabel(ylabel='Dual SIM', fontsize=14)\n\nsns.countplot(x='Touch Screen', data = all_data, palette='YlOrBr_r', ax=ax[2,1])\nax[2,1].set_ylabel(ylabel='Touch Screen', fontsize=14)\n\nsns.countplot(x='Bluetooth', data = all_data, palette='YlOrBr_r', ax=ax[3,0])\nax[3,0].set_ylabel(ylabel='Bluetooth', fontsize=14)\n\nf.delaxes(ax[3, 1])\nplt.tight_layout()\nplt.show()","8c274c23":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(nrows=7, ncols=2, figsize=(10, 10))\nf.suptitle('Histplot Graph', fontsize=16)\n\nsns.distplot(all_data['RAM'], ax=ax[0,0])\nax[0,0].set_ylabel(ylabel='RAM', fontsize=14)\n\nsns.distplot(all_data['Height'], ax=ax[0,1])\nax[0,1].set_ylabel(ylabel='Height', fontsize=14)\n\nsns.distplot(all_data['Height'], ax=ax[1,0])\nax[1,0].set_ylabel(ylabel='Height', fontsize=14)\n\nsns.distplot(all_data['Power Battery'], ax=ax[1,1])\nax[1,1].set_ylabel(ylabel='Power Battery', fontsize=14)\n\nsns.distplot(all_data['Weight'], ax=ax[2,0])\nax[2,0].set_ylabel(ylabel='Weight', fontsize=14)\n\nsns.distplot(all_data['Int. Memory'], ax=ax[2,1])\nax[2,1].set_ylabel(ylabel='Int. Memory', fontsize=14)\n\nsns.distplot(all_data['Clock Speed'], ax=ax[3,0])\nax[3,0].set_ylabel(ylabel='Clock Speed', fontsize=14)\n\nsns.distplot(all_data['PC'], ax=ax[3,1])\nax[3,1].set_ylabel(ylabel='PC', fontsize=14)\n\nsns.distplot(all_data['Front Camera'], ax=ax[4,0])\nax[4,0].set_ylabel(ylabel='Front Camera', fontsize=14)\n\nsns.distplot(all_data['SC W'], ax=ax[4,1])\nax[4,1].set_ylabel(ylabel='SC W', fontsize=14)\n\nsns.distplot(all_data['Talk Time'], ax=ax[5,0])\nax[5,0].set_ylabel(ylabel='Talk Time', fontsize=14)\n\nsns.distplot(all_data['SC H'], ax=ax[5,1])\nax[5,1].set_ylabel(ylabel='SC H', fontsize=14)\n\nsns.distplot(all_data['Thickness'], ax=ax[6,0])\nax[6,0].set_ylabel(ylabel='Thickness', fontsize=14)\n\n\nf.delaxes(ax[6, 1])\nplt.tight_layout()\nplt.show()","a0ce7a56":"print(f'Count of unique item for each columns in train data:\\n{train.nunique().sort_values(ascending=False)}')\nprint('-'*20)\nprint(f'Count of unique item for each columns in test data:\\n{test.nunique().sort_values(ascending=False)}')","09a5195d":"feature_skew_kurt = ['RAM','Width','Power Battery','Height','Weight','Int. Memory',\n                     'Clock Speed','PC','Front Camera','Talk Time','SC W', 'SC H',\n                     'Thickness', 'Core Pros.']\n\nprint(f'Skewness:\\n{all_data[feature_skew_kurt].skew().sort_values(ascending=False)}')\nprint('-'*30)\nprint(f'Kurtosis:\\n{all_data[feature_skew_kurt].kurt().sort_values(ascending=False)}')","cdb5ef61":"power_battery = all_data[[\"Power Battery\", \"Price_Range\"]].groupby(['Price_Range'], as_index=False).mean().sort_values(by='Price_Range', ascending=False)\npower_battery","2dd6e3c7":"ram = all_data[[\"RAM\", \"Price_Range\"]].groupby(['Price_Range'], as_index=False).mean().sort_values(by='Price_Range', ascending=False)\nram","0169a594":"height = all_data[[\"Height\", \"Price_Range\"]].groupby(['Price_Range'], as_index=False).mean().sort_values(by='Price_Range', ascending=False)\nheight","ed849dcd":"width = all_data[[\"Width\", \"Price_Range\"]].groupby(['Price_Range'], as_index=False).mean().sort_values(by='Price_Range', ascending=False)\nwidth","f810bdbb":"weight = all_data[[\"Weight\", \"Price_Range\"]].groupby(['Price_Range'], as_index=False).mean().sort_values(by='Price_Range', ascending=False)\nweight","e80c810b":"int_memory= all_data[[\"Int. Memory\", \"Price_Range\"]].groupby(['Price_Range'], as_index=False).mean().sort_values(by='Price_Range', ascending=False)\nint_memory","7b85b3ab":"clock_speed = all_data[[\"Clock Speed\", \"Price_Range\"]].groupby(['Price_Range'], as_index=False).mean().sort_values(by='Price_Range', ascending=False)\nclock_speed","697d3b8c":"pc = all_data[[\"PC\", \"Price_Range\"]].groupby(['Price_Range'], as_index=False).mean().sort_values(by='Price_Range', ascending=False)\npc","ceab963e":"front_camera = all_data[[\"Front Camera\", \"Price_Range\"]].groupby(['Price_Range'], as_index=False).mean().sort_values(by='Price_Range', ascending=False)\nfront_camera","04867ff6":"sc_w = all_data[[\"SC W\", \"Price_Range\"]].groupby(['Price_Range'], as_index=False).mean().sort_values(by='Price_Range', ascending=False)\nsc_w","d9672c14":"talk_time = all_data[[\"Talk Time\", \"Price_Range\"]].groupby(['Price_Range'], as_index=False).mean().sort_values(by='Price_Range', ascending=False)\ntalk_time","a28aa569":"sc_h = all_data[[\"SC H\", \"Price_Range\"]].groupby(['Price_Range'], as_index=False).mean().sort_values(by='Price_Range', ascending=False)\nsc_h","4327e228":"thickness = all_data[[\"Thickness\", \"Price_Range\"]].groupby(['Price_Range'], as_index=False).mean().sort_values(by='Price_Range', ascending=False)\nthickness","acad17d9":"wifi = all_data[[\"Wifi\", \"Price_Range\"]].groupby(['Price_Range'], as_index=False).apply(pd.DataFrame.mode).reset_index(drop=True).sort_values(by='Price_Range', ascending=False)\nwifi = wifi[['Price_Range','Wifi']]\nwifi","a41be84d":"three_g = all_data[[\"3G\", \"Price_Range\"]].groupby(['Price_Range'], as_index=False).apply(pd.DataFrame.mode).reset_index(drop=True).sort_values(by='Price_Range', ascending=False)\nthree_g = three_g[['Price_Range','3G']]\nthree_g","465d1266":"four_g = all_data[[\"4G\", \"Price_Range\"]].groupby(['Price_Range'], as_index=False).apply(pd.DataFrame.mode).reset_index(drop=True).sort_values(by='Price_Range', ascending=False)\nfour_g = four_g[['Price_Range','4G']]\nfour_g","284b188f":"dual_sim = all_data[[\"Dual SIM\", \"Price_Range\"]].groupby(['Price_Range'], as_index=False).apply(pd.DataFrame.mode).reset_index(drop=True).sort_values(by='Price_Range', ascending=False)\ndual_sim = dual_sim[['Price_Range','Dual SIM']]\ndual_sim","ed130a96":"touch_screen = all_data[[\"Touch Screen\", \"Price_Range\"]].groupby(['Price_Range'], as_index=False).apply(pd.DataFrame.mode).reset_index(drop=True).sort_values(by='Price_Range', ascending=False)\ntouch_screen = touch_screen[['Price_Range','Touch Screen']]\ntouch_screen","4b566b57":"bluetooth = all_data[[\"Bluetooth\", \"Price_Range\"]].groupby(['Price_Range'], as_index=False).apply(pd.DataFrame.mode).reset_index(drop=True).sort_values(by='Price_Range', ascending=False)\nbluetooth = bluetooth[['Price_Range','Bluetooth']]\nbluetooth","1a6223e6":"print(f'Missing values in train data:\\n{train.isnull().sum().sort_values(ascending=False)}')\nprint('-'*30)\nprint(f'Missing values in test data:\\n{test.isnull().sum().sort_values(ascending=False)}')","2539eebc":"all_data.corr().style.background_gradient(cmap='coolwarm')","0095a2ec":"print(f'Correlation to target feature:\\n{all_data.corr().Price_Range.sort_values(ascending=False)}')","f084d86f":"train.loc[:,'Price_Range'].value_counts()","1d0f363c":"evaluation = pd.DataFrame({'Model': [],\n                           'Details':[],\n                           'Accuracy':[],\n                           'Precision':[],\n                           'Recall':[],\n                           'F1':[],\n                           'CVS':[]})","75f77b23":"'''DECISION TREE CLASSIFIER'''\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\n\ntrain_data,test_data = train_test_split(train,train_size = 0.8,random_state=3)\n\nfeatures = train.columns.values.tolist()[0:20]\n\nstandardScalerX = StandardScaler()\nstandardScalerX.fit_transform(train_data[features])\nstandardScalerX.fit_transform(test_data[features])\n\ndtc1 = DecisionTreeClassifier(criterion= 'gini', min_samples_split=4, min_samples_leaf = 3, max_features = 'auto')\nestimator1 = dtc1.fit(train_data[features], train_data['Price_Range'])\npredict1 = dtc1.predict(test_data[features])                                                                                \nacc1 = (accuracy_score(test_data['Price_Range'], predict1)*100)                                \ncvs1 = (cross_val_score(dtc1, train_data[features], train_data['Price_Range'], cv=5).mean())*100\nrecall1 = recall_score(test_data['Price_Range'], predict1, average='weighted')*100\nprecision1 = precision_score(test_data['Price_Range'], predict1, average='weighted')*100\nf11 = f1_score(test_data['Price_Range'], predict1, average='weighted')*100                                                                                                                                                                                                                           \n                                                                                                                                                                                                                                               \nr = evaluation.shape[0]\nevaluation.loc[r] = ['Decision Tree','All Feature',acc1,precision1,recall1,f11,cvs1]\nevaluation.sort_values(by = 'Accuracy', ascending=False)\nevaluation","63fa0904":"'''RANDOM FOREST CLASSIFIER'''\nfrom sklearn.ensemble import RandomForestClassifier\n\nrfc1 = RandomForestClassifier(min_samples_leaf = 3, min_samples_split=4, n_estimators = 100)\nestimator2 = rfc1.fit(train_data[features], train_data['Price_Range'])\npredict2 = rfc1.predict(test_data[features])                                                                                \nacc2 = (accuracy_score(test_data['Price_Range'], predict2)*100)\ncvs2 = (cross_val_score(rfc1, train_data[features], train_data['Price_Range'], cv=5).mean())*100\nrecall2 = recall_score(test_data['Price_Range'], predict2, average='weighted')*100\nprecision2 = precision_score(test_data['Price_Range'], predict2, average='weighted')*100\nf12 = f1_score(test_data['Price_Range'], predict2, average='weighted')*100                                                                                                                                                                                                                           \n                                                                                                                                                                                                                                               \nr = evaluation.shape[0]\nevaluation.loc[r] = ['Random Forest','All Feature',acc2,precision2,recall2,f12,cvs2]\nevaluation.sort_values(by = 'Accuracy', ascending=False)\nevaluation","36d9d5a6":"'''EXTRA TREE CLASSIFIER'''\nfrom sklearn.ensemble import ExtraTreesClassifier\n\netc1 = ExtraTreesClassifier(min_samples_leaf = 7, min_samples_split=2, n_estimators = 100)\nestimator3 = etc1.fit(train_data[features], train_data['Price_Range'])\npredict3 = etc1.predict(test_data[features])                                                                                \nacc3 = (accuracy_score(test_data['Price_Range'], predict3)*100)\ncvs3 = (cross_val_score(etc1, train_data[features], train_data['Price_Range'], cv=5).mean())*100\nrecall3 = recall_score(test_data['Price_Range'], predict3, average='weighted')*100\nprecision3 = precision_score(test_data['Price_Range'], predict3, average='weighted')*100\nf13 = f1_score(test_data['Price_Range'], predict3, average='weighted')*100                                                                                                                                                                                                                           \n                                                                                                                                                                                                                                               \nr = evaluation.shape[0]\nevaluation.loc[r] = ['Extra Tree','All Feature',acc3,precision3,recall3,f13,cvs3]\nevaluation.sort_values(by = 'Accuracy', ascending=False)\nevaluation","11229326":"'''XGB CLASSIFIER'''\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\nxgb1 = XGBClassifier(criterion = 'giny', learning_rate = 0.01, max_depth = 5, n_estimators = 100, objective ='binary:logistic', subsample = 1.0)\nestimator4 = xgb1.fit(train_data[features], train_data['Price_Range'])\npredict4 = xgb1.predict(test_data[features])                                                                                \nacc4 = (accuracy_score(test_data['Price_Range'], predict4)*100)\ncvs4 = (cross_val_score(etc1, train_data[features], train_data['Price_Range'], cv=5).mean())*100\nrecall4 = recall_score(test_data['Price_Range'], predict4, average='weighted')*100\nprecision4 = precision_score(test_data['Price_Range'], predict4, average='weighted')*100\nf14 = f1_score(test_data['Price_Range'], predict4, average='weighted')*100                                                                                                                                                                                                                           \n                                                                                                                                                                                                                                               \nr = evaluation.shape[0]\nevaluation.loc[r] = ['XGB','All Feature',acc4,precision4,recall4,f14,cvs4]\nevaluation.sort_values(by = 'Accuracy', ascending=False)\nevaluation","3f0e3817":"'''STACKING CLASSIFIER'''\nfrom mlxtend.classifier import StackingCVClassifier\n\nstc1 = StackingCVClassifier(classifiers=[dtc1, rfc1, etc1, xgb1], meta_classifier=rfc1, random_state=1)\nestimator5 = stc1.fit(train_data[features], train_data['Price_Range'])\npredict5 = stc1.predict(test_data[features])                                                                                \nacc5 = (accuracy_score(test_data['Price_Range'], predict5)*100)\ncvs5 = (cross_val_score(stc1, train_data[features], train_data['Price_Range'], cv=5).mean())*100\nrecall5 = recall_score(test_data['Price_Range'], predict5, average='weighted')*100\nprecision5 = precision_score(test_data['Price_Range'], predict5, average='weighted')*100\nf15 = f1_score(test_data['Price_Range'], predict5, average='weighted')*100                                                                                                                                                                                                                           \n                                                                                                                                                                                                                                               \nr = evaluation.shape[0]\nevaluation.loc[r] = ['Stacking','All Feature',acc5,precision5,recall5,f15,cvs5]\nevaluation.sort_values(by = 'Accuracy', ascending=False)\nevaluation","986921d0":"print(f'Shape of final train data:\\n{train_data.shape}')\nprint(f'Shape of final test data:\\n{test_data.shape}')","7e6c664d":"import numpy as np\ndef plot_feature_importance(importance,names,model_type):\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n\n    plt.figure(figsize=(10,8))\n\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n\n    plt.title(model_type + 'FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')","07ae8803":"plot_feature_importance(dtc1.feature_importances_,features,'DECISION TREE ')","9e476a63":"plot_feature_importance(rfc1.feature_importances_,features,'RANDOM FOREST ')","cc93a5a6":"plot_feature_importance(etc1.feature_importances_,features,'EXTRA TREE ')","27f90670":"plot_feature_importance(xgb1.feature_importances_,features,'XGB ')","9d929f38":"'''DECISION TREE SELECT FEATURE'''\n\nfeatures = ['RAM', 'Power Battery', 'Height', 'Width']\n\ndtc2 = DecisionTreeClassifier(criterion= 'gini', min_samples_split=4, min_samples_leaf = 3, max_features = 'auto')\nestimator6 = dtc2.fit(train_data[features], train_data['Price_Range'])\npredict6 = dtc2.predict(test_data[features])                                                                                \nacc6 = (accuracy_score(test_data['Price_Range'], predict6)*100)                                \ncvs6 = (cross_val_score(dtc2, train_data[features], train_data['Price_Range'], cv=5).mean())*100\nrecall6 = recall_score(test_data['Price_Range'], predict6, average='weighted')*100\nprecision6 = precision_score(test_data['Price_Range'], predict6, average='weighted')*100\nf16 = f1_score(test_data['Price_Range'], predict6, average='weighted')*100                                                                                                                                                                                                                           \n                                                                                                                                                                                                                                               \nr = evaluation.shape[0]\nevaluation.loc[r] = ['Decision Tree','Select Feature',acc6,precision6,recall6,f16,cvs6]\nevaluation.sort_values(by = 'Accuracy', ascending=False)\nevaluation","6826530a":"'''RANDOM FOREST SELECT FEATURE'''\n\nrfc2 = RandomForestClassifier(min_samples_leaf = 3, min_samples_split=4, n_estimators = 100)\nestimator7 = rfc2.fit(train_data[features], train_data['Price_Range'])\npredict7 = rfc2.predict(test_data[features])                                                                                \nacc7 = (accuracy_score(test_data['Price_Range'], predict7)*100)                                \ncvs7 = (cross_val_score(rfc2, train_data[features], train_data['Price_Range'], cv=5).mean())*100\nrecall7 = recall_score(test_data['Price_Range'], predict7, average='weighted')*100\nprecision7 = precision_score(test_data['Price_Range'], predict7, average='weighted')*100\nf17 = f1_score(test_data['Price_Range'], predict7, average='weighted')*100                                                                                                                                                                                                                           \n                                                                                                                                                                                                                                               \nr = evaluation.shape[0]\nevaluation.loc[r] = ['Random Forest','Select Feature',acc7,precision7,recall7,f17,cvs7]\nevaluation.sort_values(by = 'Accuracy', ascending=False)\nevaluation","befb7741":"'''EXTRA TREE SELECT FEATURE'''\n\netc2 = ExtraTreesClassifier(min_samples_leaf = 7, min_samples_split=2, n_estimators = 100)\nestimator8 = etc2.fit(train_data[features], train_data['Price_Range'])\npredict8 = etc2.predict(test_data[features])                                                                                \nacc8 = (accuracy_score(test_data['Price_Range'], predict8)*100)                                \ncvs8 = (cross_val_score(etc2, train_data[features], train_data['Price_Range'], cv=5).mean())*100\nrecall8 = recall_score(test_data['Price_Range'], predict8, average='weighted')*100\nprecision8 = precision_score(test_data['Price_Range'], predict8, average='weighted')*100\nf18 = f1_score(test_data['Price_Range'], predict8, average='weighted')*100                                                                                                                                                                                                                           \n                                                                                                                                                                                                                                               \nr = evaluation.shape[0]\nevaluation.loc[r] = ['Extra Tree','Select Feature',acc8,precision8,recall8,f18,cvs8]\nevaluation.sort_values(by = 'Accuracy', ascending=False)\nevaluation","62afad7f":"'''XGB SELECT FEATURE'''\n\nxgb2 = XGBClassifier(criterion = 'giny', learning_rate = 0.01, max_depth = 5, n_estimators = 100, objective ='binary:logistic', subsample = 1.0)\nestimator9 = xgb2.fit(train_data[features], train_data['Price_Range'])\npredict9 = xgb2.predict(test_data[features])                                                                                \nacc9 = (accuracy_score(test_data['Price_Range'], predict9)*100)                                \ncvs9 = (cross_val_score(xgb2, train_data[features], train_data['Price_Range'], cv=5).mean())*100\nrecall9 = recall_score(test_data['Price_Range'], predict9, average='weighted')*100\nprecision9 = precision_score(test_data['Price_Range'], predict9, average='weighted')*100\nf19 = f1_score(test_data['Price_Range'], predict9, average='weighted')*100                                                                                                                                                                                                                           \n                                                                                                                                                                                                                                               \nr = evaluation.shape[0]\nevaluation.loc[r] = ['XGB','Select Feature',acc9,precision9,recall9,f19,cvs9]\nevaluation.sort_values(by = 'Accuracy', ascending=False)\nevaluation","bed7ea8f":"'''STACKING SELECT FEATURE'''\n\nstc2 = StackingCVClassifier(classifiers=[dtc2, rfc2, etc2, xgb2], meta_classifier=rfc2, random_state=1)\nestimator10 = stc2.fit(train_data[features], train_data['Price_Range'])\npredict10 = stc2.predict(test_data[features])                                                                                \nacc10 = (accuracy_score(test_data['Price_Range'], predict10)*100)\ncvs10 = (cross_val_score(stc2, train_data[features], train_data['Price_Range'], cv=5).mean())*100\nrecall10 = recall_score(test_data['Price_Range'], predict10, average='weighted')*100\nprecision10 = precision_score(test_data['Price_Range'], predict10, average='weighted')*100\nf110 = f1_score(test_data['Price_Range'], predict10, average='weighted')*100                                                                                                                                                                                                                           \n                                                                                                                                                                                                                                               \nr = evaluation.shape[0]\nevaluation.loc[r] = ['Stacking','Select Feature',acc10,precision10,recall10,f110,cvs10]\nevaluation.sort_values(by = 'Accuracy', ascending=False)\nevaluation","657a9897":"'''Learning Curve'''\n\ndef plot_learning_curve(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n    if ylim is not None:\n        plt.ylim(*ylim)\n        \n    # First Estimator\n    train_sizes, train_scores, test_scores = learning_curve(estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\", label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\", label=\"Cross-validation score\")\n    ax1.set_title(\"DTC Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    \n    # Second Estimator \n    train_sizes, train_scores, test_scores = learning_curve(estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"#ff9124\")\n    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\", label=\"Training score\")\n    ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\", label=\"Cross-validation score\")\n    ax2.set_title(\"RFC Learning Curve\", fontsize=14)\n    ax2.set_xlabel('Training size (m)')\n    ax2.set_ylabel('Score')\n    ax2.grid(True)\n    ax2.legend(loc=\"best\")\n    \n    # Third Estimator\n    train_sizes, train_scores, test_scores = learning_curve(estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax3.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"#ff9124\")\n    ax3.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\", label=\"Training score\")\n    ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\", label=\"Cross-validation score\")\n    ax3.set_title(\"ETC Learning Curve\", fontsize=14)\n    ax3.set_xlabel('Training size (m)')\n    ax3.set_ylabel('Score')\n    ax3.grid(True)\n    ax3.legend(loc=\"best\")\n    \n    # Fourth Estimator\n    train_sizes, train_scores, test_scores = learning_curve(estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax4.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"#ff9124\")\n    ax4.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\", label=\"Training score\")\n    ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\", label=\"Cross-validation score\")\n    ax4.set_title(\"XGB Learning Curve\", fontsize=14)\n    ax4.set_xlabel('Training size (m)')\n    ax4.set_ylabel('Score')\n    ax4.grid(True)\n    ax4.legend(loc=\"best\")\n    \n    return plt","4f6ca9ce":"from sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(dtc2, rfc2, etc2, xgb2, train_data[features], train_data['Price_Range'], (0.67, 1.01), cv=cv, n_jobs=4)","3d46e141":"'''Confusion Matrix'''\nfrom sklearn.metrics import confusion_matrix\n\nDTC_matrix = confusion_matrix(test_data['Price_Range'], predict1)\nRF_matrix = confusion_matrix(test_data['Price_Range'], predict2)\nETC_matrix = confusion_matrix(test_data['Price_Range'], predict3)\nXGB_matrix = confusion_matrix(test_data['Price_Range'], predict4) \nSTACKING_matrix = confusion_matrix(test_data['Price_Range'], predict5) \n\nf, ax = plt.subplots(nrows=3, ncols=2, figsize=(15, 25))\nsns.heatmap(DTC_matrix,annot=True, fmt=\"d\", cbar=False, cmap=\"Pastel2\",  ax = ax[0,0])\nax[0,0].set_title(\"DTC All Feature\", weight='bold')\nax[0,0].set_xlabel('Predicted Labels')\nax[0,0].set_ylabel('Actual Labels')\n\nsns.heatmap(RF_matrix,annot=True, fmt=\"d\" ,cbar=False, cmap=\"tab20\", ax = ax[0,1])\nax[0,1].set_title(\"RFC All Feature\", weight='bold')\nax[0,1].set_xlabel('Predicted Labels')\nax[0,1].set_ylabel('Actual Labels')\n\nsns.heatmap(ETC_matrix,annot=True, fmt=\"d\", cbar=False, cmap=\"Paired\", ax = ax[1,0])\nax[1,0].set_title(\"ETC All Feature\", weight='bold')\nax[1,0].set_xlabel('Predicted Labels')\nax[1,0].set_ylabel('Actual Labels')\n\nsns.heatmap(XGB_matrix,annot=True, fmt=\"d\", cbar=False, cmap=\"Pastel1\", ax = ax[1,1])\nax[1,1].set_title(\"XGB All Feature\", weight='bold')\nax[1,1].set_xlabel('Predicted Labels')\nax[1,1].set_ylabel('Actual Labels')\n\nsns.heatmap(STACKING_matrix,annot=True, fmt=\"d\", cbar=False, cmap=\"Pastel1\", ax = ax[2,0])\nax[2,0].set_title(\"Stacking All Feature\", weight='bold')\nax[2,0].set_xlabel('Predicted Labels')\nax[2,0].set_ylabel('Actual Labels')\n\nf.delaxes(ax[2, 1])\nplt.tight_layout()\nplt.show()","642acd53":"'''Confusion Matrix'''\n\nDTC_matrix = confusion_matrix(test_data['Price_Range'], predict6)\nRF_matrix = confusion_matrix(test_data['Price_Range'], predict7)\nETC_matrix = confusion_matrix(test_data['Price_Range'], predict8)\nXGB_matrix = confusion_matrix(test_data['Price_Range'], predict9) \nSTACKING_matrix = confusion_matrix(test_data['Price_Range'], predict10) \n\nf, ax = plt.subplots(nrows=3, ncols=2, figsize=(15, 25))\nsns.heatmap(DTC_matrix,annot=True, fmt=\"d\", cbar=False, cmap=\"Pastel2\",  ax = ax[0,0])\nax[0,0].set_title(\"DTC Select Feature\", weight='bold')\nax[0,0].set_xlabel('Predicted Labels')\nax[0,0].set_ylabel('Actual Labels')\n\nsns.heatmap(RF_matrix,annot=True, fmt=\"d\" ,cbar=False, cmap=\"tab20\", ax = ax[0,1])\nax[0,1].set_title(\"RFC Select Feature\", weight='bold')\nax[0,1].set_xlabel('Predicted Labels')\nax[0,1].set_ylabel('Actual Labels')\n\nsns.heatmap(ETC_matrix,annot=True, fmt=\"d\", cbar=False, cmap=\"Paired\", ax = ax[1,0])\nax[1,0].set_title(\"ETC Select Feature\", weight='bold')\nax[1,0].set_xlabel('Predicted Labels')\nax[1,0].set_ylabel('Actual Labels')\n\nsns.heatmap(XGB_matrix,annot=True, fmt=\"d\", cbar=False, cmap=\"Pastel1\", ax = ax[1,1])\nax[1,1].set_title(\"XGB Select Feature\", weight='bold')\nax[1,1].set_xlabel('Predicted Labels')\nax[1,1].set_ylabel('Actual Labels')\n\nsns.heatmap(STACKING_matrix,annot=True, fmt=\"d\", cbar=False, cmap=\"Pastel1\", ax = ax[2,0])\nax[2,0].set_title(\"Stacking Select Feature\", weight='bold')\nax[2,0].set_xlabel('Predicted Labels')\nax[2,0].set_ylabel('Actual Labels')\n\nf.delaxes(ax[2, 1])\nplt.tight_layout()\nplt.show()","7fb518bf":"test['Predict'] = stc2.predict(test[features])","7b472ec9":"test[['ID', 'Predict']].head(3)","1e0a159c":"test.loc[:,'Predict'].value_counts().sort_values(ascending=False)","f3641ef9":"sns.countplot(x='Predict', data = test, palette='YlOrBr_r')","98bcefab":"- As we can see:\n    1. Decision Tree: Accuracy, Precision, Recall, F1, and Cross Validation Score (CVS) increased about 11%\n    2. Random Forest: Rougfly increased about 3.25%\n    3. Extra Tree: Increased about 4.25%\n    4. XGB: Increased 0.75%\n    5. Stacking: Increased 4.25%\n- The best model that we will use to predict test data is Extra Tree with feature selection","d8a572c6":"- I cancating train and test data to make visualization more easy\n- For the countplot graph, we can see that most of the feature have balanced unique item count, except 3G feature\n- Little explanation about skewness adn kurtosis:\n    1. Skewness is a measure of symmetry, or more precisely, the lack of symmetry. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point. (www.itl.nist.gov)\n    2. Kurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. (www.itl.nist.gov)\n    3. Hair et al. (2010) and Bryne (2010) argued that data considered to be normal if Skewness is between \u20102 to +2 and Kurtosis is between \u20107 to +7.\n- I only make histogram plot for feature that have many unique item, for feature that consist binary item a prefer to use countplot\n- For skewness value range between -0.08 - 1, and for kurtosis between -1.34 - 0.3\n- Based on explanation befero, our data must have normal distribution, and we dont neet to apply normalization\n- I try to groupby each column with our target feature (Price Range) and sort them by mean and mode. This for knowing how item in certain feature vary in our target feature\n- Again, for feature that have binary columns i use mode, and remains using mean\n- The result is some of the feature like Power Battery, RAM, Weight etc have strong correlation to our target feature. Highest Price Range seems to have high values.\n- For the mode, we can't clearly see the correlation, but still use it for model vallidation later","bacaa1ca":"- We already make 5 different model using all feature that given\n- I prefer to use train data to split again into train and test data. Because test data that given doesn't contain target feature. So, we can't check the key matrix (Accuracy, Precision, Recall, F1, and Cross Validation Score) for evalualuation\n- The average accuracy model is 85%\n- The highest accuracy model is Gradient Boost Model (XGB)\n- We create feature importance graph for each model, and show that RAM, Power Battery, Height and Width is the most important feature\n- The feature which show highest importance is feature that have strong correlation that we calculated before\n- We will update the feature in the model and we will see different accuracy given","a44f2a12":"Feature defenition:\n1. battery_power: Total energy a battery can store in one time measured in mAh\n2. blue: Has bluetooth or not\n3. clock_speed: speed at which microprocessor executes instructions\n4. dual_sim: Has dual sim support or not\n5. fc: Front Camera mega pixels\n6. four_g: Has 4G or not\n7. int_memory: Internal Memory in Gigabytes\n8. m_dep: Mobile Depth in cm\n9. mobile_wt: Weight of mobile phone\n10. n_cores: Number of cores of processor\n11. px_height: Height in pixel\n12. px_width: Width in pixel\n13. ram: Random Acces Memory in megabytes\n14. talk_time: Max time phone standby for calling (in hours)\n15. three_g: 3G fiture (in Boolean, 1 for exist 0 for not-exist)\n16. touch_screen: Touch screen fiture\n17. wifi: Wifi fiture\n18. price_range: Range of mobile price consist 0 1 2 3\n\nAnd some feature\/columns that have no description, but i still keep it for knowing its correlation","0cf24a73":"Problem:\n- Estimate price of mobiles using mobile phones sales data from various companies\n- Correlation between mobile phone features and selling price\n- Determine the price range (not an actual price)","c4bb4bee":"- Types of learning curves:\n\n    1. Bad Learning Curve: High Bias\n        - When training and testing errors converge and are high\n        - Poor fit\n        - Poor generalization\n    2. Bad Learning Curve: High Variance\n        - When there is a large gap between the errors\n        - Require data to improve\n        - Can simplify the model with fewer or less complex features\n    3. Ideal Learning Curve\n        - Model that generalizes to new data\n        - Testing and training learning curves converge at similar values\n        - Smaller the gap, the better our model generalizes \n    (www.ritchieng.com)\n    \n![](https:\/\/lh4.googleusercontent.com\/OC2DpGenL7UAswdG5EPTgGM1XA2ULiw_P7I31F-peWgBGgnF_zzlZift-RhIqMC3zRiO13xc6xCijOTCERlbGKqLLaSswOxUAMeXnOy1ZqZGF9qxvsb_oDRDejpGlvp9diXa2VM)\n\n![](https:\/\/lh3.googleusercontent.com\/grZboodXthKKYjZvJS5b5LkDovRR8Rwsxv3GxArkVOLYEYBR0jcS6XAVMtrGluytcdsurHwc9fO72KUE4MrLbAUC0C22rfL9INOMqsmtY85Y64Kn-miC6sRmc7aaSB9RiLjiL5I)\n                                                    (www.mygreatlearning.com)\n\n- From our learning curve, we can see that, Extra Tree Classifier have good condition (Not Overfit or Underfit)\n- From the Confussion Matriks we can see count of True Positive, False Positive, True Negative and False Negative. It will use to evaluate our model to knowing Accuracy, Precision, Recall and F1 score (we did it before)\n- In the confusion matrix, a true positive exists where observation is positive with a positive prediction. A false positive exists where observation is negative, with a positive prediction. A true negative exists where observation is negative with negative prediction, and a false negative indicates a positive observation with a negative prediction. (www.techopedia.com)","004b76f4":"- I'm changing name columns to better format\n- For unique item in each columns we can see that RAM, Height, Width and Powe Battery have many unique item. And several columns displayed as boolean\/binary, such as Wifi, 3G, 4G etc.\n- The shape of our data (train and test) have same number of columns. Train data have 2000 rows, and test 1000 rows. \n- For the data type, all columns is numerical format (int and float)\n- As the task given, our target feature should be Price Range, and all other features we can keep for modelling\n- Target feature consist 4 unique item, such as 1, 2, 3 and 0. The smallest number indicates that lowest price.","fbcaa1f3":"- Train and test data have no missing values\n- The most hights correlation to target feature is: RAM, Power Battery, Width, and Height\n- Train data have balanced item, we dont need to resample\n- For this problem we will use Classification Model such as:\n\n    1. Decision Tree Classifier (DTC)\n    2. Random Forest Classifier (RFC)\n    3. Extra Tree Classifier (ETC)\n    4. XGB Classifier (XGB)\n    5. Stacking","80758d6e":"Hello \ud83d\ude4c, welcome to my notebook. In this notebook we will try to learn Multiclass Classification using 5 Models. Also develop feature selection to increased accuracy.\nFeel free if you have any question or suggestion! Thank you!\n\n![](https:\/\/i.pcmag.com\/imagery\/roundups\/07ml3nh3QrzTLZ9UycfQQB2-36.fit_lim.size_1050x.jpg)\n(https:\/\/sea.pcmag.com)","59380e17":"Conclusion:\n- We already estimate the price range in test data in column 'Predict'\n    1. Class 0: 262\n    2. Class 1: 225\n    3. Class 2: 260\n    4. Class 3: 253\n- Feature with highest correlation is RAM, Power Battery, Width, Height, Int. Memory\n- Feature which show no correlation is Clock Speed, Weight, and Touch Screen\n\n- Dont' Forget to Upvote! Thank you!:)"}}