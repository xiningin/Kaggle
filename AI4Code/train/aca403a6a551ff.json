{"cell_type":{"f266fc43":"code","96b9a310":"code","5087beef":"code","04d5e507":"code","0a072105":"code","a8a70251":"code","fd7466f5":"code","c7e8b780":"code","2ba6591a":"code","c4b31b61":"code","4a92fc1a":"code","c00e5a58":"code","5e89ac39":"code","4c851f08":"code","bcb37d2a":"code","73fc77b0":"code","26a3b4c3":"code","df561f98":"code","931ba181":"code","22e548c4":"code","38883eca":"code","ccf14624":"code","e1858f71":"code","90420276":"code","d9c089c0":"code","403d5dc3":"code","84e61308":"code","8c59c4b2":"code","cc86f7ce":"code","7b5d6876":"code","c8f0b9bb":"code","1ae03283":"code","e4ea5acc":"code","b9c59e1f":"code","0ab46318":"code","7fd73866":"code","4c2367a7":"code","7787aadf":"code","fac5127e":"code","4431c8cd":"code","3bba789e":"code","e93b91a1":"code","b0582b91":"code","294504ad":"code","993d6f54":"code","b52374b4":"code","42b12072":"code","19bcbdd6":"code","aa5445e0":"code","e71a6300":"code","56c3858e":"code","675be2f3":"code","c4612d9d":"code","4f1d99fe":"code","c32a5607":"code","80000642":"code","2b89c5b9":"code","aa72b97b":"markdown","b3c3d20e":"markdown","109d96af":"markdown","548ebccb":"markdown"},"source":{"f266fc43":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","96b9a310":"import matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nfrom tqdm import tqdm","5087beef":"df = pd.read_csv('..\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\nprice_df = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/sell_prices.csv\")\ncal_df = pd.read_csv(\"..\/input\/m5-forecasting-accuracy\/calendar.csv\")","04d5e507":"cal_df[\"d\"]=cal_df[\"d\"].apply(lambda x: int(x.split(\"_\")[1]))\nprice_df[\"id\"] = price_df[\"item_id\"] + \"_\" + price_df[\"store_id\"] + \"_validation\"","0a072105":"df.head()","a8a70251":"for day in tqdm(range(1886, 1914)):\n    wk_id = list(cal_df[cal_df[\"d\"]==day][\"wm_yr_wk\"])[0]\n    wk_price_df = price_df[price_df[\"wm_yr_wk\"]==wk_id]\n    df = df.merge(wk_price_df[[\"sell_price\", \"id\"]], on=[\"id\"], how='inner')\n    df[\"unit_sales_\" + str(day)] = df[\"sell_price\"] * df[\"d_\" + str(day)]\n    df.drop(columns=[\"sell_price\"], inplace=True)","fd7466f5":"df[\"dollar_sales\"] = df[[c for c in df.columns if c.find(\"unit_sales\")==0]].sum(axis=1)","c7e8b780":"df.drop(columns=[c for c in df.columns if c.find(\"unit_sales\")==0], inplace=True)","2ba6591a":"df[\"weight\"] = df[\"dollar_sales\"] \/ df[\"dollar_sales\"].sum()","c4b31b61":"df.drop(columns=[\"dollar_sales\"], inplace=True)","4a92fc1a":"df[\"weight\"] \/= 12","c00e5a58":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))","5e89ac39":"df.dtypes","4c851f08":"print('Largest sales value is', df[[c for c in df.columns if c.find('d_')==0]].max().max(), \n      '\\nLargest int16 is', np.iinfo(np.int16).max)","bcb37d2a":"for d in range(1914, 1942):\n    df[\"d_\" + str(d)] = np.nan","73fc77b0":"temp = df.drop(columns = [c for c in df.columns if c.find('d_')==0 and int(c.split('_')[1]) < 1100])","26a3b4c3":"df_melted = temp.melt(id_vars=[n for n in temp.columns if n.find(\"id\")!=-1],\n       value_vars=[n for n in temp.columns if n.find(\"d_\")==0],\n       var_name = 'day', value_name = 'sales')\ndel temp","df561f98":"df[['id', 'd_1100']].head(1)","931ba181":"df_melted.head()","22e548c4":"df_melted[\"day\"]=df_melted[\"day\"].apply(lambda x: int(x.split(\"_\")[1]))","38883eca":"df_melted=df_melted.merge(cal_df.drop(columns=[\"date\", \"wm_yr_wk\", \n                                            \"weekday\"]), left_on=[\"day\"], right_on=[\"d\"]).drop(columns=[\"d\"])","ccf14624":"df_melted['event_name_1'].value_counts(dropna=False).head()","e1858f71":"df_melted['event_name_1'].astype('category').cat.codes.astype(\"int8\").value_counts().head()","90420276":"df_melted[\"event_name_1\"]=df_melted[\"event_name_1\"].astype('category').cat.codes.astype(\"int8\")\ndf_melted[\"event_name_2\"]=df_melted[\"event_name_2\"].astype('category').cat.codes.astype(\"int8\")\ndf_melted[\"event_type_1\"]=df_melted[\"event_type_1\"].astype('category').cat.codes.astype(\"int8\")\ndf_melted[\"event_type_2\"]=df_melted[\"event_type_2\"].astype('category').cat.codes.astype(\"int8\")","d9c089c0":"useful_ids = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\nid_encodings = [id_col + '_encoding' for id_col in useful_ids]\n\nfor id_col in useful_ids:\n    if id_col == 'item_id':\n        df_melted[id_col + '_encoding'] = df_melted[id_col].astype('category').cat.codes.astype(\"int16\")\n    else: \n        df_melted[id_col + '_encoding'] = df_melted[id_col].astype('category').cat.codes.astype(\"int8\")","403d5dc3":"df_melted.drop(columns=['year'] + useful_ids, inplace=True)","84e61308":"reduce_mem_usage(df_melted)","8c59c4b2":"df_melted['test'] = df_melted[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(1).fillna(-1).astype(np.int16)\nprint(list(df_melted[df_melted['day']==1101]['test']) == list(df_melted[df_melted['day']==1100]['sales']))","cc86f7ce":"df_melted.drop(columns=['test'], inplace=True)","7b5d6876":"# create lags, lags starts from 28 days ago to 77 days ago, spaced by 7 days\nfor lag in tqdm([28, 35, 42, 49, 56, 63, 70, 77]):\n    df_melted[\"lag_\" + str(lag)] = df_melted[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag).fillna(-1).astype(np.int16)","c8f0b9bb":"df_melted=df_melted[df_melted['lag_77']!=-1]","1ae03283":"assert list(df_melted[df_melted['day']==1528]['lag_28']) == list(df_melted[df_melted['day']==1500]['sales'])","e4ea5acc":"df_melted.head()","b9c59e1f":"price_df.head()","0ab46318":"df_melted=df_melted.merge(cal_df[['d', 'wm_yr_wk']], left_on=['day'], right_on=['d']).drop(columns=['d'])","7fd73866":"df_melted=df_melted.merge(price_df[['id', 'sell_price', 'wm_yr_wk']], on=['id', 'wm_yr_wk'], how='inner')","4c2367a7":"del price_df\ndel cal_df","7787aadf":"import lightgbm as lgb","fac5127e":"best_params = {\n            \"objective\" : \"poisson\",\n            \"metric\" :\"rmse\",\n            \"force_row_wise\" : True,\n            \"learning_rate\" : 0.05,\n    #         \"sub_feature\" : 0.8,\n            \"sub_row\" : 0.75,\n            \"bagging_freq\" : 1,\n            \"lambda_l2\" : 0.1,\n    #         \"nthread\" : 4\n            \"metric\": [\"rmse\"],\n        'verbosity': 1,\n        'num_iterations' : 2048,\n        'num_leaves': 64,\n        \"min_data_in_leaf\": 50,\n    }","4431c8cd":"del wk_price_df","3bba789e":"X_train = df_melted[df_melted[\"day\"] < 1886].drop(columns=[\"sales\"])\nX_val = df_melted[df_melted[\"day\"].between(1886, 1913)].drop(columns=[\"sales\"])\nX_test = df_melted[df_melted[\"day\"] > 1913].drop(columns=[\"sales\"])\n\ny_train = df_melted[df_melted[\"day\"] < 1886][\"sales\"]\ny_val = df_melted[df_melted[\"day\"].between(1886, 1913)][\"sales\"]","e93b91a1":"%who DataFrame","b0582b91":"del df_melted","294504ad":"X_train.columns","993d6f54":"%%time\n\nnp.random.seed(777)\n\nfake_valid_inds = np.random.choice(X_train.index.values, 2_000_000, replace = False)\ntrain_inds = np.setdiff1d(X_train.index.values, fake_valid_inds)\ntrain_data = lgb.Dataset(X_train.drop(columns=['id']).loc[train_inds] , label = y_train.loc[train_inds], \n                         categorical_feature=id_encodings, free_raw_data=False)\nfake_valid_data = lgb.Dataset(X_train.drop(columns=['id']).loc[fake_valid_inds], label = y_train.loc[fake_valid_inds],\n                              categorical_feature=id_encodings,\n                 free_raw_data=False)# This is a random sample, we're not gonna apply any time series train-test-split tricks here!","b52374b4":"%%time\n\nm_lgb = lgb.train(best_params, train_data, valid_sets = [fake_valid_data], verbose_eval=1) ","42b12072":"for d in range(1886, 1914):\n    df['F_' + str(d)] = m_lgb.predict(X_val[X_val['day']==d].drop(columns=['id']))","19bcbdd6":"df.head()","aa5445e0":"h = 28\nn = 1885\ndef rmsse(ground_truth, forecast, train_series, axis=1):\n    # assuming input are numpy array or matrices\n    assert axis == 0 or axis == 1\n    assert type(ground_truth) == np.ndarray and type(forecast) == np.ndarray and type(train_series) == np.ndarray\n    \n    if axis == 1:\n        # using axis == 1 we must guarantee these are matrices and not arrays\n        assert ground_truth.shape[1] > 1 and forecast.shape[1] > 1 and train_series.shape[1] > 1\n    \n    numerator = ((ground_truth - forecast)**2).sum(axis=axis)\n    if axis == 1:\n        denominator = 1\/(n-1) * ((train_series[:, 1:] - train_series[:, :-1]) ** 2).sum(axis=axis)\n    else:\n        denominator = 1\/(n-1) * ((train_series[1:] - train_series[:-1]) ** 2).sum(axis=axis)\n    if (numerator < 0).any():\n        print('nu')\n    elif (denominator < 0).any():\n        print(denominator[denominator < 0])\n    return (1\/h * numerator\/denominator) ** 0.5","e71a6300":"level_groupings = {2: [\"state_id\"], 3: [\"store_id\"], 4: [\"cat_id\"], 5: [\"dept_id\"], \n              6: [\"state_id\", \"cat_id\"], 7: [\"state_id\", \"dept_id\"], 8: [\"store_id\", \"cat_id\"], 9: [\"store_id\", \"dept_id\"],\n              10: [\"item_id\"], 11: [\"item_id\", \"state_id\"]}","56c3858e":"#remake agg_df\nnew_agg_df = pd.DataFrame(df[[c for c in df.columns if c.find(\"d_\") == 0 or c.find(\"F_\") == 0]].sum()).transpose()\nnew_agg_df[\"level\"] = 1\nnew_agg_df[\"weight\"] = 1\/12\ncolumn_order = new_agg_df.columns\n\nfor level in level_groupings:\n    temp_df = df.groupby(by=level_groupings[level]).sum().reset_index()\n    temp_df[\"level\"] = level\n    new_agg_df = new_agg_df.append(temp_df[column_order])\ndel temp_df\n\nagg_df = new_agg_df\n\ntrain_series_cols = [c for c in df.columns if c.find(\"d_\") == 0 and int(c.split('_')[1]) < 1886]\nground_truth_cols = [c for c in df.columns if c.find(\"d_\") == 0 and int(c.split('_')[1]) in range(1886, 1914)]\nforecast_cols = [c for c in df.columns if c.find(\"F_\") == 0]\n\ndf[\"rmsse\"] = rmsse(np.array(df[ground_truth_cols]), \n        np.array(df[forecast_cols]), np.array(df[train_series_cols]))\nagg_df[\"rmsse\"] = rmsse(np.array(agg_df[ground_truth_cols]), \n        np.array(agg_df[forecast_cols]), np.array(agg_df[train_series_cols]))\n\ndf[\"wrmsse\"] = df[\"weight\"] * df[\"rmsse\"]\nagg_df[\"wrmsse\"] = agg_df[\"weight\"] * agg_df[\"rmsse\"]\n\nprint(df[\"wrmsse\"].sum() + agg_df[\"wrmsse\"].sum())","675be2f3":"lgb.plot_importance(m_lgb)","c4612d9d":"submit_df = df[[\"id\"]]\nfor i in range(1, 29):\n    submit_df[\"F\" + str(i)] = m_lgb.predict(X_test[X_test['day']==i+1913].drop(columns=['id']))","4f1d99fe":"submit_df.head()","c32a5607":"submit_df2 = submit_df.copy()\nsubmit_df2[\"id\"] = submit_df2[\"id\"].apply(lambda x: x.replace('validation',\n                                                              'evaluation'))","80000642":"submit_df = submit_df.append(submit_df2).reset_index(drop=True)","2b89c5b9":"submit_df.to_csv(\"submission.csv\", index=False)","aa72b97b":"## 2. Transform time series","b3c3d20e":"###### Make submission file","109d96af":"## 1. Calculate weight for the level 12 series","548ebccb":"## 0. Import libraries and read in data"}}