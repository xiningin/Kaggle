{"cell_type":{"993cb7a4":"code","a624cb71":"code","63f11d15":"code","68fd9fc8":"code","3cbca06f":"code","04134c85":"code","ff4dc062":"code","9e654fb9":"code","d43f9dc5":"code","0efe61e3":"code","21babbbd":"code","5b9eb53a":"code","b594ebff":"code","110ca19e":"code","01b6c1ff":"code","7c78fd7b":"code","e666f752":"code","16acf6b8":"code","a9e2c8c9":"code","609a3f43":"code","949d746a":"code","fbe74d64":"code","5eb0292f":"code","fb01d48a":"code","c263bd59":"code","b445c3a4":"markdown","5f0717c4":"markdown","54967c1c":"markdown","05c3ec64":"markdown","619ed74b":"markdown","9a3b29fd":"markdown","7dfc858c":"markdown","970f73cf":"markdown","de596c47":"markdown","7cea2225":"markdown","c4ea7722":"markdown","6fd1cedd":"markdown"},"source":{"993cb7a4":"import cv2, pandas as pd, matplotlib.pyplot as plt\ntrain = pd.read_csv('..\/input\/siim-isic-melanoma-classification\/train.csv')\nprint('Examples WITH Melanoma')\nimgs = train.loc[train.target==1].sample(5).image_name.values\nplt.figure(figsize=(20,8))\nfor i,k in enumerate(imgs):\n    img = cv2.imread('..\/input\/jpeg-melanoma-128x128\/train\/%s.jpg'%k)\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n    plt.subplot(2,5,i+1); plt.axis('off')\n    plt.imshow(img)\nplt.show()\nprint('Examples WITHOUT Melanoma')\nimgs = train.loc[train.target==0].sample(5).image_name.values\nplt.figure(figsize=(20,8))\nfor i,k in enumerate(imgs):\n    img = cv2.imread('..\/input\/jpeg-melanoma-128x128\/train\/%s.jpg'%k)\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n    plt.subplot(2,5,i+1); plt.axis('off')\n    plt.imshow(img)\nplt.show()","a624cb71":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt # visualization\n\n# Directory for dataset\ntrain_csv_dir = \"\/kaggle\/input\/siim-isic-melanoma-classification\/train.csv\"\ntrain_jpeg_dir = \"\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/\"\n\n# Read datasets\ntrain_features = pd.read_csv(train_csv_dir)\ntrain_features.head()","63f11d15":"# Let's see the data types of the columns and if there are any null values in columns\nprint(train_features.info())","68fd9fc8":"print('Null values in anatom_site_general_challenge column: '+ str(train_features['anatom_site_general_challenge'].isna().sum()))\nprint('Null values in age column: '+ str(train_features['age_approx'].isna().sum()))\nprint('Null values in sex: '+ str(train_features['sex'].isna().sum()))","3cbca06f":"print('Unique location classes:' + str(train_features['anatom_site_general_challenge'].unique()))\nprint('Unique sex classes:' + str(train_features['sex'].unique()))","04134c85":"# Let's see the age histogram of malignant and benign cases\n\nbins = np.linspace(0, 100, 100)\nplt.hist((train_features['age_approx'][train_features['target']==0]), bins, color = 'mediumaquamarine', label='benign')\nplt.legend(loc='upper right')\nplt.show()\nplt.savefig('hist_age_benign',dpi=300);\n\nplt.hist((train_features['age_approx'][train_features['target']==1]), bins, color = 'indianred', label='malignant')\nplt.legend(loc='upper right')\nplt.show()\nplt.savefig('hist_age_malignant',dpi=300);","ff4dc062":"labels = 'malignant', 'benign'\nexplode = (0.1, 0)  # only explode 'malignant'\n\nfig1, ax1 = plt.subplots()\nax1.pie([len(train_features[train_features['target']==1]), len(train_features[train_features['target']==0])], explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90, colors=['indianred','mediumaquamarine'] )\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.show()\nplt.savefig('pie_chart');","9e654fb9":"!pip install -q efficientnet >> \/dev\/null\nimport pandas as pd, numpy as np\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow as tf, re, math\nimport tensorflow.keras.backend as K\nimport efficientnet.tfkeras as efn\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, classification_report, f1_score, roc_curve\nimport matplotlib.pyplot as plt","d43f9dc5":"DEVICE = \"TPU\" #or \"GPU\"\n\n# USE DIFFERENT SEED FOR DIFFERENT STRATIFIED KFOLD\nSEED = 7\n\n# NUMBER OF FOLDS. USE 3, 5, OR 15 \nFOLDS = 5\n\n# WHICH IMAGE SIZES TO LOAD EACH FOLD\n# CHOOSE 128, 192, 256, 384, 512, 768 \nIMG_SIZES = [384,384,384,384,384]\n\n# INCLUDE OLD COMP DATA? YES=1 NO=0\nINC2019 = [1,1,1,1,1]\nINC2018 = [1,1,1,1,1]\n\n# BATCH SIZE AND EPOCHS\nBATCH_SIZES = [64]*FOLDS\nEPOCHS = [20]*FOLDS\n\n# WHICH EFFICIENTNET B? TO USE\nEFF_NETS = [5,5,5,5,5]\n\n# WEIGHTS FOR FOLD MODELS WHEN PREDICTING TEST\nWGTS = [1\/FOLDS]*FOLDS\n\n# TEST TIME AUGMENTATION STEPS\nTTA = 11","0efe61e3":"if DEVICE == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        print(\"Could not connect to TPU\")\n        tpu = None\n\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except _:\n            print(\"failed to initialize TPU\")\n    else:\n        DEVICE = \"GPU\"\n\nif DEVICE != \"TPU\":\n    print(\"Using default strategy for CPU and single GPU\")\n    strategy = tf.distribute.get_strategy()\n\nif DEVICE == \"GPU\":\n    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n    \n\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","21babbbd":"GCS_PATH = [None]*FOLDS; GCS_PATH2 = [None]*FOLDS\nfor i,k in enumerate(IMG_SIZES):\n    GCS_PATH[i] = KaggleDatasets().get_gcs_path('melanoma-%ix%i'%(k,k))\n    GCS_PATH2[i] = KaggleDatasets().get_gcs_path('isic2019-%ix%i'%(k,k))\nfiles_train = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[0] + '\/train*.tfrec')))\nfiles_test  = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[0] + '\/test*.tfrec')))","5b9eb53a":"ROT_ = 180.0\nSHR_ = 2.0\nHZOOM_ = 8.0\nWZOOM_ = 8.0\nHSHIFT_ = 8.0\nWSHIFT_ = 8.0","b594ebff":"def get_mat(rotation, shear, height_zoom, width_zoom, height_shift, width_shift):\n    # returns 3x3 transformmatrix which transforms indicies\n        \n    # CONVERT DEGREES TO RADIANS\n    rotation = math.pi * rotation \/ 180.\n    shear    = math.pi * shear    \/ 180.\n\n    def get_3x3_mat(lst):\n        return tf.reshape(tf.concat([lst],axis=0), [3,3])\n    \n    # ROTATION MATRIX\n    c1   = tf.math.cos(rotation)\n    s1   = tf.math.sin(rotation)\n    one  = tf.constant([1],dtype='float32')\n    zero = tf.constant([0],dtype='float32')\n    \n    rotation_matrix = get_3x3_mat([c1,   s1,   zero, \n                                   -s1,  c1,   zero, \n                                   zero, zero, one])    \n    # SHEAR MATRIX\n    c2 = tf.math.cos(shear)\n    s2 = tf.math.sin(shear)    \n    \n    shear_matrix = get_3x3_mat([one,  s2,   zero, \n                                zero, c2,   zero, \n                                zero, zero, one])        \n    # ZOOM MATRIX\n    zoom_matrix = get_3x3_mat([one\/height_zoom, zero,           zero, \n                               zero,            one\/width_zoom, zero, \n                               zero,            zero,           one])    \n    # SHIFT MATRIX\n    shift_matrix = get_3x3_mat([one,  zero, height_shift, \n                                zero, one,  width_shift, \n                                zero, zero, one])\n    \n    return K.dot(K.dot(rotation_matrix, shear_matrix), \n                 K.dot(zoom_matrix,     shift_matrix))\n\n\ndef transform(image, DIM=384):    \n    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n    # output - image randomly rotated, sheared, zoomed, and shifted\n    XDIM = DIM%2 #fix for size 331\n    \n    rot = ROT_ * tf.random.normal([1], dtype='float32')\n    shr = SHR_ * tf.random.normal([1], dtype='float32') \n    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') \/ HZOOM_\n    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') \/ WZOOM_\n    h_shift = HSHIFT_ * tf.random.normal([1], dtype='float32') \n    w_shift = WSHIFT_ * tf.random.normal([1], dtype='float32') \n\n    # GET TRANSFORMATION MATRIX\n    m = get_mat(rot,shr,h_zoom,w_zoom,h_shift,w_shift) \n\n    # LIST DESTINATION PIXEL INDICES\n    x   = tf.repeat(tf.range(DIM\/\/2, -DIM\/\/2,-1), DIM)\n    y   = tf.tile(tf.range(-DIM\/\/2, DIM\/\/2), [DIM])\n    z   = tf.ones([DIM*DIM], dtype='int32')\n    idx = tf.stack( [x,y,z] )\n    \n    # ROTATE DESTINATION PIXELS ONTO ORIGIN PIXELS\n    idx2 = K.dot(m, tf.cast(idx, dtype='float32'))\n    idx2 = K.cast(idx2, dtype='int32')\n    idx2 = K.clip(idx2, -DIM\/\/2+XDIM+1, DIM\/\/2)\n    \n    # FIND ORIGIN PIXEL VALUES           \n    idx3 = tf.stack([DIM\/\/2-idx2[0,], DIM\/\/2-1+idx2[1,]])\n    d    = tf.gather_nd(image, tf.transpose(idx3))\n        \n    return tf.reshape(d,[DIM, DIM,3])","110ca19e":"def read_labeled_tfrecord(example):\n    tfrec_format = {\n        'image'                        : tf.io.FixedLenFeature([], tf.string),\n        'image_name'                   : tf.io.FixedLenFeature([], tf.string),\n        'patient_id'                   : tf.io.FixedLenFeature([], tf.int64),\n        'sex'                          : tf.io.FixedLenFeature([], tf.int64),\n        'age_approx'                   : tf.io.FixedLenFeature([], tf.int64),\n        'anatom_site_general_challenge': tf.io.FixedLenFeature([], tf.int64),\n        'diagnosis'                    : tf.io.FixedLenFeature([], tf.int64),\n        'target'                       : tf.io.FixedLenFeature([], tf.int64)\n    }           \n    example = tf.io.parse_single_example(example, tfrec_format)\n    one_hot_site = tf.reshape(tf.one_hot(example['anatom_site_general_challenge'], 6), [1,6])\n    age_min = 0\n    age_max = 100\n    age = tf.reshape(tf.cast((example['age_approx']-age_min)\/(age_max-age_min), tf.float32), [1,1])\n    sex = tf.reshape(tf.cast(example['sex'], tf.float32), [1,1])\n    features = tf.reshape(tf.concat([sex, age, one_hot_site], axis=1), [8])\n                                          \n    return example['image'], features, example['target']\n\ndef read_unlabeled_tfrecord(example, return_image_name):\n    tfrec_format = {\n        'image'                        : tf.io.FixedLenFeature([], tf.string),\n        'image_name'                   : tf.io.FixedLenFeature([], tf.string),\n        'sex'                          : tf.io.FixedLenFeature([], tf.int64),\n        'age_approx'                   : tf.io.FixedLenFeature([], tf.int64),\n        'anatom_site_general_challenge': tf.io.FixedLenFeature([], tf.int64),\n    }\n    example = tf.io.parse_single_example(example, tfrec_format)\n    one_hot_site = tf.reshape(tf.one_hot(example['anatom_site_general_challenge'], 6), [1,6])\n    age_min = 0\n    age_max = 100\n    age = tf.reshape(tf.cast((example['age_approx']-age_min)\/(age_max-age_min), tf.float32), [1,1])\n    sex = tf.reshape(tf.cast(example['sex'], tf.float32), [1,1])\n    features = tf.reshape(tf.concat([sex, age, one_hot_site], axis=1), [8])\n                                          \n    return example['image'], features, example['image_name'] if return_image_name else 0\n\n \ndef prepare_image(img, augment=True, dim=384):    \n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.cast(img, tf.float32) \/ 255.0\n    \n    if augment:\n        img = transform(img,DIM=dim)\n        img = tf.image.random_flip_left_right(img)\n        #img = tf.image.random_hue(img, 0.01)\n        img = tf.image.random_saturation(img, 0.7, 1.3)\n        img = tf.image.random_contrast(img, 0.8, 1.2)\n        img = tf.image.random_brightness(img, 0.1)\n                      \n    img = tf.reshape(img, [dim,dim, 3])\n            \n    return img\n\n    \n\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n         for filename in filenames]\n    return np.sum(n)","01b6c1ff":"def get_dataset(files, augment = False, shuffle = False, repeat = False, \n                labeled=True, return_image_names=True, batch_size=16, dim=384):\n    \n    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n    ds = ds.cache()\n    \n    \n    if repeat:\n        ds = ds.repeat()\n    \n    if shuffle: \n        ds = ds.shuffle(1024*8)\n        opt = tf.data.Options()\n        opt.experimental_deterministic = False\n        ds = ds.with_options(opt)\n        \n    if labeled: \n        ds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n    else:\n        ds = ds.map(lambda example: read_unlabeled_tfrecord(example, return_image_names), \n                    num_parallel_calls=AUTO)      \n    \n    ds = ds.map(lambda img, features, imgname_or_label: ((prepare_image(img, augment=augment, dim=dim), features), imgname_or_label), \n                    num_parallel_calls=AUTO)\n    \n    \n    ds = ds.batch(batch_size * REPLICAS)\n    ds = ds.prefetch(AUTO)\n    return ds","7c78fd7b":"EFNS = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3, \n        efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6,  efn.EfficientNetB7]\n\nnoisy_student = [\"..\/input\/efficientnet-noisy-student-keras-applications\/efficientnetb0_notop.h5\",\n                 \"..\/input\/efficientnet-noisy-student-keras-applications\/efficientnetb1_notop.h5\",\n                 \"..\/input\/efficientnet-noisy-student-keras-applications\/efficientnetb2_notop.h5\",\n                 \"..\/input\/efficientnet-noisy-student-keras-applications\/efficientnetb3_notop.h5\",\n                 \"..\/input\/efficientnet-noisy-student-keras-applications\/efficientnetb4_notop.h5\",\n                 \"..\/input\/efficientnet-noisy-student-keras-applications\/efficientnetb5_notop.h5\",\n                 \"..\/input\/efficientnet-noisy-student-keras-applications\/efficientnetb6_notop.h5\",\n                 \"..\/input\/efficientnet-noisy-student-keras-applications\/efficientnetb7_notop.h5\"]\n                 \ndef build_model(dim=384, ef=5):\n    input_image = tf.keras.layers.Input(shape=(dim,dim,3))\n    input_features = tf.keras.layers.Input(shape=(8))\n    \n    base = EFNS[ef](input_shape=(dim,dim,3),include_top=False)\n    base.load_weights(noisy_student[ef], by_name=True)\n    base.trainable = False\n    \n    x = base(input_image, training=False)\n    x = tf.keras.layers.Dropout(0.3)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dense(units = 128, activation=\"relu\")(x)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    \n    # Train the feature map with a shallow dense layer\n    x2 = tf.keras.layers.Dense(units = 16, activation=\"relu\")(input_features)\n    x2 = tf.keras.layers.Dropout(0.3)(x2)\n\n    # concatenate outputs of two branches\n    x = tf.keras.layers.concatenate([x, x2])\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dense(units = 128, activation=\"relu\")(x) \n    x = tf.keras.layers.Dropout(0.5)(x)\n\n    # make predictions\n    x = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n    model = tf.keras.Model(inputs = [input_image, input_features],outputs=x)\n    opt = tf.keras.optimizers.Adam(learning_rate=0.00005)\n    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05) \n    model.compile(optimizer=opt,loss=loss,metrics=['AUC'])\n    return model","e666f752":"plot_model = build_model(dim=384, ef=5)\nplot_model.summary()\ntf.keras.utils.plot_model(plot_model, to_file=\"plot_model.png\", show_shapes=True)","16acf6b8":"def get_lr_callback(batch_size=8):\n    lr_start   = 0.000005\n    lr_max     = 0.00000125 * REPLICAS * batch_size\n    lr_min     = 0.000001\n    lr_ramp_ep = 5\n    lr_sus_ep  = 0\n    lr_decay   = 0.8\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        return lr\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n    return lr_callback","a9e2c8c9":"# USE VERBOSE=0 for silent, VERBOSE=1 for interactive, VERBOSE=2 for commit\nVERBOSE = 1\nDISPLAY_PLOT = True\n\nskf = KFold(n_splits=FOLDS,shuffle=True,random_state=SEED)\noof_pred = []; oof_tar = []; oof_val = []; oof_names = []; oof_folds = [] \npreds = np.zeros((count_data_items(files_test),1))\n\nfor fold,(idxT,idxV) in enumerate(skf.split(np.arange(15))):\n    \n    # DISPLAY FOLD INFO\n    if DEVICE=='TPU':\n        if tpu: tf.tpu.experimental.initialize_tpu_system(tpu)\n    print('#'*25); print('#### FOLD',fold+1)\n    print('#### Image Size %i with EfficientNet B%i and batch_size %i'%\n          (IMG_SIZES[fold],EFF_NETS[fold],BATCH_SIZES[fold]*REPLICAS))\n    \n    # CREATE TRAIN AND VALIDATION SUBSETS\n    files_train = tf.io.gfile.glob([GCS_PATH[fold] + '\/train%.2i*.tfrec'%x for x in idxT])\n    if INC2019[fold]:\n        files_train += tf.io.gfile.glob([GCS_PATH2[fold] + '\/train%.2i*.tfrec'%x for x in idxT*2+1])\n        print('#### Using 2019 external data')\n    if INC2018[fold]:\n        files_train += tf.io.gfile.glob([GCS_PATH2[fold] + '\/train%.2i*.tfrec'%x for x in idxT*2])\n        print('#### Using 2018+2017 external data')\n    np.random.shuffle(files_train); print('#'*25)\n    files_valid = tf.io.gfile.glob([GCS_PATH[fold] + '\/train%.2i*.tfrec'%x for x in idxV])\n    files_test = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[fold] + '\/test*.tfrec')))\n    \n    # BUILD MODEL\n    K.clear_session()\n    with strategy.scope():\n        model = build_model(dim=IMG_SIZES[fold],ef=EFF_NETS[fold])\n        \n    # SAVE BEST MODEL EACH FOLD\n    sv = tf.keras.callbacks.ModelCheckpoint(\n        'fold-%i.h5'%fold, monitor='val_auc', verbose=0, save_best_only=True,\n        save_weights_only=False, mode='max', save_freq='epoch')\n   \n    # TRAIN\n    print('Training...')\n    history = model.fit(\n        get_dataset(files_train, augment=True, shuffle=True, repeat=True,\n                dim=IMG_SIZES[fold], batch_size = BATCH_SIZES[fold]), \n        epochs=EPOCHS[fold], callbacks = [sv,get_lr_callback(BATCH_SIZES[fold])],\n        steps_per_epoch=count_data_items(files_train)\/BATCH_SIZES[fold]\/\/REPLICAS,\n        validation_data=get_dataset(files_valid,augment=False,shuffle=False,\n                repeat=False,dim=IMG_SIZES[fold]), class_weight = {0:1,1:8},\n        verbose=VERBOSE\n    )\n    \n    # PLOT TRAINING\n    if DISPLAY_PLOT:\n        plt.figure(figsize=(15,5))\n        plt.plot(np.arange(EPOCHS[fold]),history.history['auc'],'-o',label='Train AUC',color='#ff7f0e')\n        plt.plot(np.arange(EPOCHS[fold]),history.history['val_auc'],'-o',label='Val AUC',color='#1f77b4')\n        x = np.argmax( history.history['val_auc'] ); y = np.max( history.history['val_auc'] )\n        xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n        plt.scatter(x,y,s=200,color='#1f77b4'); plt.text(x-0.03*xdist,y-0.13*ydist,'max auc\\n%.2f'%y,size=14)\n        plt.ylabel('AUC',size=14); plt.xlabel('Epoch',size=14)\n        plt.legend(loc=2)\n        plt2 = plt.gca().twinx()\n        plt2.plot(np.arange(EPOCHS[fold]),history.history['loss'],'-o',label='Train Loss',color='#2ca02c')\n        plt2.plot(np.arange(EPOCHS[fold]),history.history['val_loss'],'-o',label='Val Loss',color='#d62728')\n        x = np.argmin( history.history['val_loss'] ); y = np.min( history.history['val_loss'] )\n        ydist = plt.ylim()[1] - plt.ylim()[0]\n        plt.scatter(x,y,s=200,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'min loss\\n%.4f'%y,size=14)\n        plt.ylabel('Loss',size=14)\n        plt.title('FOLD %i - Image Size %i, EfficientNet B%i, inc2019=%i, inc2018=%i'%\n                (fold+1,IMG_SIZES[fold],EFF_NETS[fold],INC2019[fold],INC2018[fold]),size=18)\n        plt.legend(loc=3)\n        plt.show()\n    \n    # FINE TUNING TRAIN\n    def unfreeze_model(model):\n        # We unfreeze the layers while leaving BatchNorm layers frozen\n        for layer in model.layers[:]:\n            if not isinstance(layer, tf.keras.layers.BatchNormalization):\n                layer.trainable = True\n                \n    print('Loading best model...')            \n    model.load_weights('fold-%i.h5'%fold)        \n    unfreeze_model(model)\n    opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\n    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05) \n    model.compile(\n        optimizer=opt, loss=loss, metrics=[\"AUC\"])\n    \n    print('Fine tuning training...')\n    history = model.fit(\n        get_dataset(files_train, augment=True, shuffle=True, repeat=True,\n                dim=IMG_SIZES[fold], batch_size = BATCH_SIZES[fold]), \n        epochs=EPOCHS[fold], callbacks = [sv,get_lr_callback(BATCH_SIZES[fold])], \n        steps_per_epoch=count_data_items(files_train)\/BATCH_SIZES[fold]\/\/REPLICAS,\n        validation_data=get_dataset(files_valid,augment=False,shuffle=False,\n                repeat=False,dim=IMG_SIZES[fold]), class_weight = {0:1,1:8},\n        verbose=VERBOSE)\n    \n    # PLOT FINE TUNING TRAINING\n    if DISPLAY_PLOT:\n        plt.figure(figsize=(15,5))\n        plt.plot(np.arange(EPOCHS[fold]),history.history['auc'],'-o',label='Train AUC',color='#ff7f0e')\n        plt.plot(np.arange(EPOCHS[fold]),history.history['val_auc'],'-o',label='Val AUC',color='#1f77b4')\n        x = np.argmax( history.history['val_auc'] ); y = np.max( history.history['val_auc'] )\n        xdist = plt.xlim()[1] - plt.xlim()[0]; ydist = plt.ylim()[1] - plt.ylim()[0]\n        plt.scatter(x,y,s=200,color='#1f77b4'); plt.text(x-0.03*xdist,y-0.13*ydist,'max auc\\n%.2f'%y,size=14)\n        plt.ylabel('AUC',size=14); plt.xlabel('Epoch',size=14)\n        plt.legend(loc=2)\n        plt2 = plt.gca().twinx()\n        plt2.plot(np.arange(EPOCHS[fold]),history.history['loss'],'-o',label='Train Loss',color='#2ca02c')\n        plt2.plot(np.arange(EPOCHS[fold]),history.history['val_loss'],'-o',label='Val Loss',color='#d62728')\n        x = np.argmin( history.history['val_loss'] ); y = np.min( history.history['val_loss'] )\n        ydist = plt.ylim()[1] - plt.ylim()[0]\n        plt.scatter(x,y,s=200,color='#d62728'); plt.text(x-0.03*xdist,y+0.05*ydist,'min loss\\n%.4f'%y,size=14)\n        plt.ylabel('Loss',size=14)\n        plt.title('FINE TUNING FOLD %i - Image Size %i, EfficientNet B%i, inc2019=%i, inc2018=%i'%\n                (fold+1,IMG_SIZES[fold],EFF_NETS[fold],INC2019[fold],INC2018[fold]),size=18)\n        plt.legend(loc=3)\n        plt.show()\n    \n    print('Loading best model...')\n    model.load_weights('fold-%i.h5'%fold)\n    \n    # PREDICT OOF USING TTA\n    print('Predicting OOF with TTA...')\n    ds_valid = get_dataset(files_valid,labeled=False,return_image_names=False,augment=True,\n            repeat=True,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4)\n    ct_valid = count_data_items(files_valid); STEPS = TTA * ct_valid\/BATCH_SIZES[fold]\/4\/REPLICAS\n    pred = model.predict(ds_valid,steps=STEPS,verbose=VERBOSE)[:TTA*ct_valid,] \n    oof_pred.append( np.mean(pred.reshape((ct_valid,TTA),order='F'),axis=1) )                 \n    #oof_pred.append(model.predict(get_dataset(files_valid,dim=IMG_SIZES[fold]),verbose=1))\n    \n    # GET OOF TARGETS AND NAMES\n    ds_valid = get_dataset(files_valid, augment=False, repeat=False, dim=IMG_SIZES[fold],\n            labeled=True, return_image_names=True)\n    oof_tar.append( np.array([target.numpy() for img, target in iter(ds_valid.unbatch())]) )\n    oof_folds.append( np.ones_like(oof_tar[-1],dtype='int8')*fold )\n    ds = get_dataset(files_valid, augment=False, repeat=False, dim=IMG_SIZES[fold],\n                labeled=False, return_image_names=True)\n    oof_names.append( np.array([img_name.numpy().decode(\"utf-8\") for img, img_name in iter(ds.unbatch())]))\n    \n    # PREDICT TEST USING TTA\n    print('Predicting Test with TTA...')\n    ds_test = get_dataset(files_test,labeled=False,return_image_names=False,augment=True,\n            repeat=True,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4)\n    ct_test = count_data_items(files_test); STEPS = TTA * ct_test\/BATCH_SIZES[fold]\/4\/REPLICAS\n    pred = model.predict(ds_test,steps=STEPS,verbose=VERBOSE)[:TTA*ct_test,] \n    preds[:,0] += np.mean(pred.reshape((ct_test,TTA),order='F'),axis=1) * WGTS[fold]\n    \n    # REPORT RESULTS\n    y_pred = [1 if x >= 0.5 else 0 for x in oof_pred[-1]]\n    cm = confusion_matrix(oof_tar[-1], y_pred)\n    print('with TTA')\n    classification_report(oof_tar[-1], y_pred)\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = ['benign', 'malignant']).plot()\n    disp.plot()\n    plt.show()\n    \n    auc = roc_auc_score(oof_tar[-1],oof_pred[-1])\n    oof_val.append(np.max( history.history['val_auc'] ))\n    print('#### FOLD %i OOF AUC without TTA = %.3f, with TTA = %.3f'%(fold+1,oof_val[-1],auc))\n    \n","609a3f43":"# COMPUTE OVERALL OOF AUC\noof = np.concatenate(oof_pred); true = np.concatenate(oof_tar);\nnames = np.concatenate(oof_names); folds = np.concatenate(oof_folds)\nauc = roc_auc_score(true,oof)\nprint('Overall OOF AUC with TTA = %.3f'%auc)\n\n# PLOT CLASSIFICATION REPORT AND CONFUSION MATRIX\ny_pred = [1 if x >= 0.5 else 0 for x in oof]\nprint('Classification report with TTA')\nprint(classification_report(true, y_pred))\n        \ncm = confusion_matrix(true, y_pred)\nprint('Confusion matrix with TTA')\nprint(ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = ['benign', 'malignant']).plot())","949d746a":"# SAVE OOF TO DISK\ndf_oof = pd.DataFrame(dict(\n    image_name = names, target=true, pred = oof, fold=folds))\ndf_oof.to_csv('oof.csv',index=False)\ndf_oof.head()","fbe74d64":"oof = pd.read_csv('..\/input\/triple-stratified-kfold-with-tfrecords\/oof.csv')\noof.head()","5eb0292f":" fpr, tpr, thresh = roc_curve(oof['target'], oof['pred'], pos_label=1)\n    \n# plotting    \nplt.plot(fpr, tpr, linestyle='solid',color='mediumaquamarine', label='benign vs malignant')\nplt.title('ROC curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive rate')\nplt.legend(loc='best')\nplt.savefig('Binary_ROC',dpi=300);","fb01d48a":"ds = get_dataset(files_test, augment=False, repeat=False, dim=IMG_SIZES[fold],\n                 labeled=False, return_image_names=True)\n\nimage_names = np.array([img_name.numpy().decode(\"utf-8\") \n                        for img, img_name in iter(ds.unbatch())])","c263bd59":"submission = pd.DataFrame(dict(image_name=image_names, target=preds[:,0]))\nsubmission = submission.sort_values('image_name') \nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","b445c3a4":"## Train Model\nOur model will be trained for the number of FOLDS and EPOCHS choosen in the configuration above. Each fold the model with highest validation AUG will be saved and used to predict OOF and test.  ","5f0717c4":"Not only the images but also the metadata was extracted from the data records.","54967c1c":"# Triple Stratified KFold CV with TFRecords\n## Initialize Environment","05c3ec64":"# Step 1: Preprocess\nPreprocess has already been done and saved to TFRecords thanks to the starter notebook. Let's get the files.","619ed74b":"## After this summary, let's see how I obtained these results\n\nBelow are the examples of skin images with and without melanoma.\nIt doesn't seem possible to identify the images without a medical profession.","9a3b29fd":"# Step 4: Train Schedule\nThis is a common train schedule for transfer learning. The learning rate starts near zero, then increases to a maximum, then decays over time. ","7dfc858c":"## Parameters\n \n* DEVICE - is GPU or TPU\n* SEED - a different seed produces a different triple stratified kfold split.\n* FOLDS - number of folds. Best set to 3, 5, or 15 but can be any number between 2 and 15\n* IMG_SIZES - is a Python list of length FOLDS. These are the image sizes to use each fold\n* INC2019 - This includes the new half of the 2019 competition data. The second half of the 2019 data is the comp data from 2018 plus 2017\n* INC2018 - This includes the second half of the 2019 competition data which is the comp data from 2018 plus 2017\n* BATCH_SIZES - is a list of length FOLDS. These are batch sizes for each fold. For maximum speed, it is best to use the largest batch size your GPU or TPU allows.\n* EPOCHS - is a list of length FOLDS. These are maximum epochs. Note that each fold, the best epoch model is saved and used. So if epochs is too large, it won't matter.\n* EFF_NETS - is a list of length FOLDS. These are the EfficientNets to use each fold. The number refers to the B. So a number of `0` refers to EfficientNetB0, and `1` refers to EfficientNetB1, etc.\n* WGTS - this should be `1\/FOLDS` for each fold. This is the weight when ensembling the folds to predict the test set. If you want a weird ensemble, you can use different weights.\n* TTA - test time augmentation. Each test image is randomly augmented and predicted TTA times and the average prediction is used. TTA is also applied to OOF(out of fold) during validation.","970f73cf":"# Detecting melanoma with transfer learning\nIn this Kaggle competition, the aim is to identify melanoma, the most lethal of skin cancers, from skin lesion images as well as patient-level contextual information. The full description of this assignment can be found [here][1]. \n\nOnly a small percentage of suspicious lesions turn out to be actually malignant (less than 2%), the rest is benign. This leads to a dramatically imbalanced dataset, making image classification very challenging.\n\nFor this task I decided to use transfer learning, which consists of taking features that are learned on one problem and leveraging them on a new, similar problem. Deep learning networks are resource hungry and computationally expensive when there are millions of parameters. To overcome this issue, transfer learning allows rapid progress and improved performance. You should use a pre-trained model for a similar task with yours. For instance if your task is visual, you can use a pre-trained model from Google which was built on ImageNet data. Then this transferred knowledge from the pre-trained model will be used while modelling your own task. To do this, you simply freeze the weights of the pre-trained model, (so it actually means that you apply the weights of the pre-trained model as a function to your own data, there are less trainable parameters so it is computationally cheap) and then add few new trainable layers which will be fed with the data of your task.\n\nSince the task is related to images, I chose one of the pre-trained source models from Keras applications. EfficientNet is a family of CNN\u2019s built by Google. This state-of-the-art model provides very good accuracy as well as a reduced number of parameters (increased efficiency) compared to other models. After doing some experiments, I decided on the EfficientNet B5 model because of the excellent tradeoff between result quality and acceptable time cost. I tried both ImageNet and noisy_student as weights and obtained better results with noisy_student, which is a semi-supervised learning approach.\n\nSelecting proper metrics is another important part of imbalanced classification problems. In the dataset, 98.2% of the examples are labeled as benign. Suppose that an algorithm predicts all examples as benign. The accuracy will be the same with the ratio of the negative examples in the dataset: 98.2%. Despite the high accuracy, still, this algorithm is useless. It doesn\u2019t provide any valuable insight. That\u2019s why we can\u2019t use accuracy as a monitoring value for this project.\n\nThe metric I chose is Area Under Curve (AUC), which approximates the area under the ROC curve. This metric creates four local variables (true_positives, true_negatives, false_positives, and false_negatives), that are used to compute two values: true positive rate (sensitivity) and false-positive rate (1-specificity). An excellent model has an AUC near 1, which means it has a good measure of separability. When AUC is 0.5, it means the model has no class separation capacity whatsoever. An AUC near 0 means it is inverting the result, in other words: it is predicting 0\u2019s as 1\u2019s and 1\u2019s as 0\u2019s.\n\nMy models\u2019 architecture is based on a combination of an EfficientNet model for transfer learning and a simple model for the metadata. After applying a dense layer and dropout on metadata, I concatenated these two model outputs and applied an additional dense layer, batch normalization, and dropout layers before classification. In the transfer learning model, I also used fine-tuning. After epochs with frozen layers, I unfroze the layers (excluding the batch normalization layers, since this may reduce accuracy). Then I continued with fine-tuning training for another 20 epochs with all layers trainable. Besides dropout and batch normalization layers (to avoid overfitting), I also used the ModelCheckpoint callback and saved the weights of the epoch with the maximum validation AUC.\n\nAt first, I used the Graphical Processing Unit (GPU) for my experiments. Due to RAM limitations, I built a generator that creates small batches and passes them into the fit_generator of Keras. To overcome both the imbalance problem and the GPU limitations, I additionally used undersampling. This involves randomly removing samples from the majority class, creating a more balanced dataset and also decreasing the amount of data (decreasing RAM usage). However, both speed and the results weren\u2019t satisfying. Although I used transfer learning to speed up the neural network, image pre-processing and augmenting still needed lots of computational power. You can find the notebook [here][2].\n\nA major breakthrough came when I started experimented with the Tensor Processing Unit (TPU). I used [this][3] notebook by Chris Deotte as a starting point, as I didn\u2019t have prior experience with this approach. TPU allowed me to iterate much faster, so I could experiment with different data augmentations, model architectures, loss optimizers, and learning schedules.\n\nUsing class weights turned out to be a powerful approach for an imbalanced dataset. In this method, more weight is given to the minority class in the cost function of the algorithm during training. It provides a higher penalty to the minority class, and as a result, the algorithm can focus on reducing the errors for the minority class.\n\nTest Time Augmentation (TTA) also made a significant improvement. This method is not related with training but prediction. It is based on randomly augmenting each test image during prediction for as many times as preferred and finally using the average prediction.\n\nFinally, I achieved 93% AUC with the competition test set. But what does this result mean? Let\u2019s look at the results by ROC curve, classification report, and confusion matrix.\n\nConfusion matrix is a commonly used visual to compare true labels and predicted labels. It gives information about not only the success of the predictions via true positives and negatives, but also about the weak points via false positives and negatives. Looking at the confusion matrix, you can see that there are 30711 true negatives (TN), 343 true positives (TP), 1400 false positives (FP) and 238 false negatives (FN). This means that 343 of the 581 malignant cases and 30711 of the 32111 benign cases were detected successfully.\n\nSuppose that you tested a photo of your mole and the result is positive (malignant). Statistically speaking, it most likely is actually benign. The probability of the mole to be malignant following a positive result is only 20% (TP\/TP+FP = 343\/(343+1400) = 0,2).\n\nA negative result (benign), has a much higher accuracy. The probability of a negative result being actually benign is 99% (TN\/TN+FN = 30711 \/ (30711+238) = 0,99). Compared to the ratio of benign samples in the dataset (98%), the probability of malignancy has been halved (from 2% to 1%).\n\n\n[1]: https:\/\/www.kaggle.com\/c\/siim-isic-melanoma-classification\n[2]: https:\/\/www.kaggle.com\/goncaahin\/b5-model-siim-isic-melanoma\n[3]: https:\/\/www.kaggle.com\/cdeotte\/triple-stratified-kfold-with-tfrecords\n","de596c47":"## Calculate OOF AUC\nThe OOF (out of fold) predictions are saved to disk. ","7cea2225":"# Submit To Kaggle","c4ea7722":"# Step 2: Data Augmentation\nRotation, sheer, zoom and shift augmentation are used, which were first shown in this notebook [here][1] and successfully used in Melanoma comp by AgentAuers [here][2]. This notebook also uses horizontal flip, hue, saturation, contrast, brightness augmentation similar to last years winner and also similar to AgentAuers' notebook.\n\n[1]: https:\/\/www.kaggle.com\/cdeotte\/rotation-augmentation-gpu-tpu-0-96\n[2]: https:\/\/www.kaggle.com\/agentauers\/incredible-tpus-finetune-effnetb0-b6-at-once\n","6fd1cedd":"# Step 3: Build Model\nModel architecture based on a combination of an EfficientNet model used for transfer learning and a simple model based on the metadata. After applying a dense layer and dropout on metadata, I concatenate these two branchs at the end of the model and apply a last dense layer, batch normalization and dropout layers before classification. I also used fine tuning process in transfer learning. After epochs with frozen layers, I unfreezes the layers excluding the batch normalization layers as advised in Keras tutorial since it may reduce accuracy. Then I continued training for another 20 epochs."}}