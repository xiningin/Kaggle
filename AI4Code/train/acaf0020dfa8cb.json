{"cell_type":{"57af757a":"code","00baa597":"code","2541053b":"code","5311fcb8":"code","e9ba3205":"code","926eb460":"code","a17a8bbb":"code","d1529af9":"code","5f59dc14":"code","13c8cdbe":"code","f6628f9d":"code","b6918c92":"code","67f97025":"code","8189bdfe":"code","1ee884ff":"code","eefda4b5":"code","e1c5bf03":"code","f8adc6af":"code","618237c5":"code","051dbd06":"code","233bf6a2":"code","f973e5c3":"markdown","1823db5d":"markdown","5859fada":"markdown","839a3173":"markdown","4dfaddd5":"markdown","66c8cc6d":"markdown","eda3cbaa":"markdown","37d336b0":"markdown","55655d3f":"markdown","d64d7afe":"markdown","33731156":"markdown","fa7709e7":"markdown","0c389a64":"markdown","afbb47eb":"markdown","33130d1a":"markdown"},"source":{"57af757a":"import re\nimport numpy as np\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\nimport torch\n# !pip install -q transformers\nfrom transformers import set_seed\nfrom transformers import pipeline, AutoTokenizer\nfrom transformers import GPT2LMHeadModel, AutoTokenizer, AutoModelForCausalLM","00baa597":"# one of the most powerful open-source language models\nMODEL_NAME = 'EleutherAI\/gpt-neo-1.3B'\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nlm = AutoModelForCausalLM.from_pretrained(MODEL_NAME, pad_token_id=tokenizer.eos_token_id).eval()","2541053b":"softmax = torch.nn.functional.softmax\n\ndef create_inputs(text):\n    return tokenizer.encode(text, return_tensors='pt')\n\ndef model_pred(inputs):\n    with torch.no_grad():\n        outs = lm(inputs).logits\n        next_token_probs = outs[0][-1]\n    return next_token_probs\n\ndef plot_socres(next_token_probs, title):\n    indeces = torch.argsort(next_token_probs, descending=True)[:10]\n    dic = [tokenizer.decode(i) for i in indeces]\n    logits = next_token_probs[indeces]\n    scores = softmax(logits, dim=0)\n    plt.title(title)\n    plt.bar(dic , scores);","5311fcb8":"max_len = 20\nprefix = 'In order to have a fit body'\n\npredictions = []\n# generate until max_len\nfor i in range(max_len):\n    # convert words to tokens\n    inputs= create_inputs(prefix)\n    next_token_probs = model_pred(inputs)\n    # pick the highest probability\n    id =  torch.argmax(next_token_probs).item()\n    # convert to token and add new token to text\n    prefix += tokenizer.decode(id)\n    # keep track of scores for next token\n    predictions.append(next_token_probs)\n\nprint(prefix, '\\n')\nplot_socres(predictions[0], title='probs at first next world')","e9ba3205":"## using \ud83e\udd17 transformer\nmax_len = 20\nprefix = 'In order to have a fit body'\n\n# convert words to tokens\ninputs = create_inputs(prefix)\n# generate text until the output length (which includes the context length) reaches 20\nouts = lm.generate(inputs, max_length=max_len)[0]\n\nprint(tokenizer.decode(outs))","926eb460":"import numpy as np\n\ndef search(model, src_input, k=1, sequence_max_len=25):\n    # (log(1), initialize_of_zeros)\n    k_beam = [(0, [0]*(sequence_max_len+1))]\n    # l : point on target sentence to predict\n    for l in range(sequence_max_len):\n        all_k_beams = []\n        for prob, sent_predict in k_beam:\n            predicted = model.predict([np.array([src_input]), np.array([sent_predict])])[0]\n            # top k!\n            possible_k = predicted[l].argsort()[-k:][::-1]\n            # add to all possible candidates for k-beams\n            all_k_beams += [\n                (\n                    sum(np.log(predicted[i][sent_predict[i+1]]) for i in range(l)) + np.log(predicted[l][next_wid]),\n                    list(sent_predict[:l+1])+[next_wid]+[0]*(sequence_max_len-l-1)\n                )\n                for next_wid in possible_k\n            ]\n        # top k\n        k_beam = sorted(all_k_beams)[-k:]\n\n    return k_beam","a17a8bbb":"set_seed(0)\nmax_len = 30\nprefix = 'In order to have a fit body'\nbeam_size = 5\n\n# convert words to tokens\ninputs = create_inputs(prefix)\n# generate text until the output length (which includes the context length) reaches 20\nouts = lm.generate(inputs, max_length=max_len, num_beams=beam_size, early_stopping=True, num_return_sequences=2)\n\nfor out in outs:\n    print(tokenizer.decode(out))","d1529af9":"## using \ud83e\udd17 transformer\nset_seed(0)\nmax_len = 30\nprefix = 'In order to have a fit body'\nbeam_size = 5\n\n# convert words to tokens\ninputs = create_inputs(prefix)\n# generate text until the output length (which includes the context length) reaches 20\nouts = lm.generate(inputs, max_length=max_len, num_beams=beam_size, early_stopping=True, \n                   num_return_sequences=2, no_repeat_ngram_size=2)\n\nfor out in outs:\n    print(tokenizer.decode(out)) ","5f59dc14":"set_seed(0)\nmax_len = 30\nprefix = 'In order to have a fit body'\n\npredictions = []\n# generate until max_len\nfor i in range(max_len):\n    # convert words to tokens\n    inputs = create_inputs(prefix)\n    next_token_probs = model_pred(inputs)\n    # convert logits to scores\n    scores = softmax(next_token_probs, dim=0)\n    # sample from scores\n    id = torch.multinomial(scores, num_samples=1).item()\n    # convert to token and add new token to text\n    prefix += tokenizer.decode(id)\n    # keep track of scores for next token\n    predictions.append(scores[id].item())\n\nprint(prefix, '\\n')\n# plt.hist(predictions, title='distribution of selected token');","13c8cdbe":"## using \ud83e\udd17 transformer\nset_seed(0)\nmax_len = 30\nprefix = 'In order to have a fit body'\n\n# convert words to tokens\ninputs = create_inputs(prefix)\n# generate text until the output length (which includes the context length) reaches 20\nouts = lm.generate(inputs, do_sample=True, max_length=max_len, top_k=0)[0]\n\nprint(tokenizer.decode(outs))","f6628f9d":"set_seed(0)\nmax_len = 20\nprefix = 'In order to have a fit body'\ntemp = .5 # divercity contoroler\n\npredictions = []\n# generate until max_len\nfor i in range(max_len):\n    # convert words to tokens\n    inputs = create_inputs(prefix)\n    next_token_probs = model_pred(inputs)\n    # apply temp before softmax (sharper logits)\n    next_token_probs \/= temp\n    # convert logits to scores\n    scores = softmax(next_token_probs, dim=0)\n    # sample from scores\n    id = torch.multinomial(scores, num_samples=1).item()\n    # convert to token and add new token to text\n    prefix += tokenizer.decode(id)\n    # keep track of scores for next token\n    predictions.append(scores[id].item())\n\nprint(prefix)\n# plt.hist(predictions);","b6918c92":"## using \ud83e\udd17 transformer\nset_seed(0)\nmax_len = 20\nprefix = 'In order to have a fit body'\ntemp = .5\n\n# convert words to tokens\ninputs = create_inputs(prefix)\n# generate text until the output length (which includes the context length) reaches 20\nouts = lm.generate(inputs, do_sample=True, max_length=max_len, top_k=0, temperature=temp)[0]\n\nprint(tokenizer.decode(outs))","67f97025":"def top_k_sampler(logits, k=10):\n    # Remove all tokens with a probability less than the last token of the top-k\n    indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n    logits[indices_to_remove] = -100 # scores = 0 for other tokens\n    # apply softmax\n    scores = softmax(logits, dim=-1)\n    # sample from scores\n    next_token = torch.multinomial(scores, 1)\n    return next_token","8189bdfe":"## manual implementation\nset_seed(0)\nmax_len = 20\nprefix = 'In order to have a fit body'\ntop_k = 10\n\npredictions = []\n# generate until max_len\nfor i in range(max_len):\n    # convert words to tokens\n    inputs = create_inputs(prefix)\n    next_token_probs = model_pred(inputs)\n    # top k sampling \n    next_token = top_k_sampler(logits=next_token_probs, k=top_k)\n    prefix += tokenizer.decode(next_token)\n\nprint(prefix)","1ee884ff":"## using \ud83e\udd17 transformer\nset_seed(0)\nmax_len = 30\nprefix = 'In order to have a fit body'\ntop_k = 10\n\n# convert words to tokens\ninputs = create_inputs(prefix)\n# generate text until the output length (which includes the context length) reaches 20\nouts = lm.generate(inputs, max_length=max_len, top_k=top_k, do_sample=True)\n\nfor out in outs:\n    print(tokenizer.decode(out)) ","eefda4b5":"def top_p_sampler(logits, p=10):\n    # Remove all tokens with a probability less than the last token of the top-k\n    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n    cumulative_probs = torch.cumsum(softmax(sorted_logits, dim=-1), dim=-1)\n\n    # Remove tokens with cumulative probability above the threshold\n    sorted_indices_to_remove = cumulative_probs > p\n    # Shift the indices to the right to keep also the first token above the threshold\n    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n    sorted_indices_to_remove[..., 0] = 0\n\n    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n    logits[indices_to_remove] = -100\n    # apply softmax\n    scores = softmax(logits, dim=-1)\n    # sample from scores\n    next_token = torch.multinomial(scores, 1)\n    return next_token","e1c5bf03":"set_seed(0)\nmax_len = 20\nprefix = 'In order to have a fit body'\ntop_p = .5\n\npredictions = []\n# generate until max_len\nfor i in range(max_len):\n    # convert words to tokens\n    inputs = create_inputs(prefix)\n    next_token_probs = model_pred(inputs)\n    # top k sampling \n    next_token = top_p_sampler(logits=next_token_probs, p=top_p)\n    prefix += tokenizer.decode(next_token)\n\nprint(prefix)","f8adc6af":"## using \ud83e\udd17 transformer\nset_seed(0)\nmax_len = 30\nprefix = 'In order to have a fit body'\ntop_p = .5 # top 90%\n\n# convert words to tokens\ninputs = create_inputs(prefix)\n# generate text until the output length (which includes the context length) reaches 20\nouts = lm.generate(inputs, max_length=max_len, top_p=top_p, top_k=0, do_sample=True)\n\nfor out in outs:\n    print(tokenizer.decode(out)) ","618237c5":"set_seed(0)\nmax_len = 20\nprefix = 'In order to have a fit body'\ntemp = .9\nnum_samples = 50\n\n# convert words to tokens\ninputs = create_inputs(prefix)\n# generate text until the output length (which includes the context length) reaches 20\nouts = lm.generate(inputs, do_sample=True, max_length=max_len, top_k=0, num_return_sequences=num_samples, temperature=temp)","051dbd06":"def rouge1_similarity(system, reference):\n    \"\"\"Returns the ROUGE-1 score between two token lists\n    Returns:\n        float: overlap between the two token lists\n    \"\"\"    \n    sys_counter = Counter(system)\n    ref_counter = Counter(reference)\n    overlap = 0\n    for token in sys_counter:\n        overlap += min(sys_counter.get(token, 0), ref_counter.get(token, 0))\n\n    precision = overlap\/ len(system)\n    recall = overlap\/ len(reference)\n    if precision + recall != 0:\n        rouge1_score = 2*(precision*recall)\/(precision+recall)\n    else:\n        rouge1_score = 0 \n    return rouge1_score","233bf6a2":"scores = []\nfor i, candidate in enumerate(outs):\n    candidate_score = 0\n    for j, sample in enumerate(outs):\n        candidate_score += rouge1_similarity(candidate, sample)\n    scores.append(candidate_score\/j)\n\n\ngolden = tokenizer.decode(outs[np.argmax(scores)])\nre.sub('\\u200c', ' ', golden)","f973e5c3":"# Greedy Search\nGreedy search simply selects the token with the highest probability as its next word. It keeps going until you produce <END OF TEXT> token (or reach some max length)\nHowever, The generated words following the context are reasonable, but the model quickly starts repeating itself! \n\nThis is a very common problem in language generation in general and seems to be even more so in greedy and beam search\n\n- The major drawback of greedy search though is that the lack of backtracking\n- 2nd is the best word at time step t genereted w\/o seeing all sentene\n\n\nin conditional LM like translation or parapharaze generation after a mistake the entire thing is not proper and in open ended LM it repets itself\n\n\n![](https:\/\/s4.uupload.ir\/files\/gready_wmdv.png)\n\nUsecases: Translation, Summarization. (don't use it)","1823db5d":"# Conclusion\ntop-p and top-K sampling seem to produce more fluent text than traditional greedy - and beam search on open-ended language generation. Recently, there has been more evidence though that the apparent flaws of greedy and beam search - mainly generating repetitive word sequences - are caused by the model (especially the way the model is trained), rather than the decoding method, cf. Welleck et al. (2019). Also, as demonstrated in Welleck et al. (2020), it looks as top-K and top-p sampling also suffer from generating repetitive word sequences.\n\nin open ended task mistake doesn't matter\n\nsampling base aproaches like top k,p are not useful for machine translation\n\nIn [Welleck et al](https:\/\/arxiv.org\/pdf\/1908.04319.pdf). (2019), the authors show that according to human evaluations, beam search can generate more fluent text than Top-p sampling, when adapting the model's training objective.\n\n","5859fada":"In transformers, we set do_sample=True and deactivate Top-K sampling (more on this later) via top_k=0","839a3173":"# Top P Sampling (nucleus)\n- remove the problem of Top K (sahrp and flat dist)\n\n\nNucleus sampling is similar to Top-K sampling. Instead of focusing on Top-K words, nucleus sampling focuses on the smallest possible sets of Top-V words such that the sum of their probability is \u2265 p\n\n\nInstead of sampling only from the most likely K words, in Top-p sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. The probability mass is then redistributed among this set of words. This way, the size of the set of words (a.k.a the number of words in the set) can dynamically increase and decrease according to the next word's probability distribution\n\n\n\nWhile in theory, Top-p seems more elegant than Top-K, both methods work well in practice. Top-p can also be used in combination with Top-K, which can avoid very low ranked words while allowing for some dynamic selection.\n\nThe intuition is that when the model is very certain on some tokens, the set of potential candidate tokens is small otherwise, there will be more potential candidate tokens.\n\n\n![](https:\/\/s4.uupload.ir\/files\/top_p_sampling_kp4i.png)","4dfaddd5":"# Minimum Bayes Risk (MBR):\n- generate several random samples (with random-sampling + temp, top-k, top-p)\n- compare similirity (usually with ROUGE) \n- select the hight similaity smaple (the most likely)","66c8cc6d":"## TLDR:\n\n- Beam Search: Translation and Summarization.\n- Top K, Top P and Top KP Sampling: Creative writing.","eda3cbaa":"# All Types of Decoding (How to generate text)\n with manual implementation and \ud83e\udd17 transformers","37d336b0":"## Summary\n- Greedy: Select the best probable token at a time\n- Beam Search: Select the best probable response\n- Random Sampling: Random based on probability\n- Top-K Sampling: Select top probable K tokens\n- Nucleus Sampling: Dynamically choose the number of K (sort of)\n\nCommonly, top choices by researchers are beam search, top-K sampling (with temperature), and nucleus sampling.","55655d3f":"In open-ended generation, beam search is not the best option\n- has low divercity\n- with very higher beam size the output is very genreic and don't care about context","d64d7afe":"# Random Sampling (include randomness)\n- there is no right answer that we are looking for\n\nThe next option we have is random sampling. Much like before we have our potential outputs and their probability distribution. with this approach with solve **repet** problem\n\nThis solves our problem of getting stuck in a repeating loop of the same words because we add randomness to our predictions.\n\nHowever, this introduces a different problem \u2014 we will often find that this approach can be too random and lacks coherence\n\n\nSo on one side, we have greedy search which is too strict for generating text \u2014 on the other we have random sampling which produces wonderful gibberish.\nWe need to find something that does a little of both.\n\n\nAlternatively, we can look into stochastic approaches to avoid the response being generic. We can utilize the probability of each token from the softmax function to generate the next token.\n\n\nOK. There are less weird n-grams and the output is a bit more coherent now! While applying temperature can make a distribution less random\n\n\nThe models often generate incoherent gibberish, cf. Ari Holtzman et al. (2019).\n\n\n![](https:\/\/s4.uupload.ir\/files\/sampling_6r9y.png)","33731156":"where we can use:\n\nBeam search works very well in tasks where the length of the desired generation is more or less predictable as in machine translation or summarization - see Murray et al. (2018) and Yang et al. (2018). But this is not the case for open-ended generation where the desired output length can vary greatly, e.g. dialog and story generation.\n\nWe have seen that beam search heavily suffers from repetitive generation. This is especially hard to control with n-gram- or other penalties in story generation since finding a good trade-off between forced \"no-repetition\" and repeating cycles of identical n-grams requires a lot of finetuning.","fa7709e7":"# Beam Search\nAt each timestep, it generates all possible tokens in the vocabulary list; then, it will choose top B candidates that have the most probability. Those B candidates will move to the next time step, and the process repeats. In the end, there will only be B candidates\n- normalized with sequence len\n  - sum of the log probeblities to prevent numerical underflow in long sequences\n- beam size from usually 5 to 10 in controled task like NMT but bigger in story generation\n- it's solve the problem of sub-optimal decoding (just until step t)\n\nBeam search provides a tradeoff between accuracy versus computational cost via its flexible choice of the beam size.\n\nBeam search reduces the risk of missing hidden high probability word sequences by keeping the most likely num_beams of hypotheses at each time step and eventually choosing the hypothesis that has the overall highest probability. Let's illustrate with num_beams=2\n\nBeam search will always find an output sequence with higher probability than greedy search, but is not guaranteed to find the most likely output.\n\nWhile the result is arguably more fluent, the output still includes repetitions of the same word sequences.\n\nA simple remedy is to introduce n-grams (a.k.a word sequences of n words) penalties as introduced by Paulus et al. (2017) and Klein et al. (2017). The most common n-grams penalty makes sure that no n-gram appears twice by manually setting the probability of next words that could create an\n\n\nUsecases: High-quality translation and summarization.\n\n![](https:\/\/s4.uupload.ir\/files\/beam_czu1.png)\n\n\n- Can we use top P instead of beam size? for example 90%\n","0c389a64":"### Temperature: \nuse temperature to decrease the sensitivity to low probability candidates (randomness controler)\n\nA trick is to make the distribution $P(w\u2223w 1:t\u22121)$ sharper (increasing the likelihood of high probability words and decreasing the likelihood of low probability words) by lowering the so-called temperature of the softmax.\n\ntemperature to 0, temperature scaled sampling becomes equal to greedy decoding\n\ntemperature to 1 is like random sampling\n\nSoftmax temperature is another way to control diversity, it\u2019s a technique that can be applied alongside any decoding algorithm.","afbb47eb":"# Top K Sampling\n- on each step $t$, randomly sample from top k most probable words\n- like pure but truncate the probability distribution\n  - $k=1$ is greedy search, \n  - $k= len(V)$ is pure sampling\n  - increase k to get more <b>Risky<\/b> output\n- it's more efficient than beam search\n  - because there is no multiple hypotheses to track :)  \n- it's good for open ended task like (poetry, stories)\n\nSoftmax Temperature (like disitllation)\n  - by increase $t$ you get more uniform dist $P_t$\n\nGPT2 adopted this sampling scheme, which was one of the reasons for its success in story generation.\n\n\nOne concern though with Top-K sampling is that it does not dynamically adapt the number of words that are filtered from the next word probability distribution $P(w\u2223w 1:t\u22121)$. This can be problematic as some words might be sampled from a very sharp distribution (distribution on the right in the graph above), whereas others from a much more flat distribution (distribution on the left in the graph above).\n\n![](https:\/\/s4.uupload.ir\/files\/1_ixvvlan_ll3mdldefjj5qa_mma0.png)","33130d1a":"## Reference:  \n- [How to generate text: using different decoding methods for language generation with Transformers](https:\/\/huggingface.co\/blog\/how-to-generate)"}}