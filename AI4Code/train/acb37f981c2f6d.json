{"cell_type":{"c5a4c8d9":"code","5db26d69":"code","746fc891":"code","9135a6d8":"code","8c14c7ca":"code","81d437d9":"code","fd91b64a":"markdown","3ff922d9":"markdown","c6f27847":"markdown","02bec83d":"markdown","4c706935":"markdown","007fffd6":"markdown","ad94f061":"markdown","37c7fc22":"markdown","b34bb032":"markdown","05ed23b0":"markdown"},"source":{"c5a4c8d9":"import pandas as pd\ndata = pd.read_csv(\"..\/input\/advertising.csv\")\ndata.head()\n\n","5db26d69":"def predict_sales(radio, weight, bias):\n    return weight*radio + bias","746fc891":"def cost_function(radio, sales, weight, bias):\n    companies = len(radio)\n    total_error = 0.0\n    for i in range(companies):\n        total_error += (sales[i] - (weight*radio[i] + bias))**2\n    return total_error \/ companies","9135a6d8":"def update_weights(radio, sales, weight, bias, learning_rate):\n    weight_deriv = 0\n    bias_deriv = 0\n    companies = len(radio)\n\n    for i in range(companies):\n        # Calculate partial derivatives\n        # -2x(y - (mx + b))\n        weight_deriv += -2*radio[i] * (sales[i] - (weight*radio[i] + bias))\n\n        # -2(y - (mx + b))\n        bias_deriv += -2*(sales[i] - (weight*radio[i] + bias))\n\n    # We subtract because the derivatives point in direction of steepest ascent\n    weight -= (weight_deriv \/ companies) * learning_rate\n    bias -= (bias_deriv \/ companies) * learning_rate\n\n    return weight, bias","8c14c7ca":"def train(radio, sales, weight, bias, learning_rate, iters):\n    cost_history = []\n\n    for i in range(iters):\n        weight,bias = update_weights(radio, sales, weight, bias, learning_rate)\n\n        #Calculate cost for auditing purposes\n        cost = cost_function(radio, sales, weight, bias)\n        cost_history.append(cost)\n\n        # Log Progress\n        if i % 10 == 0:\n            print(\"iter={:d}    weight={:.2f}    bias={:.4f}    cost={:.2}\".format(i, weight, bias, cost))\n\n    return weight, bias, cost_history","81d437d9":"radio = data['Radio'].values\nsales = data['Sales'].values\nweight = 0\nbias = 0\nlr = 0.01\niters = 100\ntrain(radio,sales,weight,bias,lr,iters)","fd91b64a":"**Multivariable regression**\n\nA more complex, multi-variable linear equation might look like this, where w represents the coefficients, or weights, our model will try to learn.\n\n                        f(x,y,z)=w1x+w2y+w3z\nThe variables x,y,z represent the attributes, or distinct pieces of information, we have about each observation.For sales predictions, these attributes might include a company\u2019s advertising spend on radio, TV, and newspapers.\n                           \n                        Sales=w1Radio+w2TV+w3News ","3ff922d9":"**The Cost Function**\nThe prediction function is nice, but for our purposes we don\u2019t really need it. What we need is a cost function so we can start optimizing our weights.\n\nLet\u2019s use MSE (L2) as our cost function. MSE measures the average squared difference between an observation\u2019s actual and predicted values. The output is a single number representing the cost, or score, associated with our current set of weights. Our goal is to minimize MSE to improve the accuracy of our model.\n\n**Math**\n\n\nGiven our simple linear equation y=mx+b, we can calculate MSE as:\n\n                    MSE=1N\u2211i=1n(yi\u2212(mxi+b))2","c6f27847":"**Making Prediction**\nOur prediction function outputs an estimate of sales given a company\u2019s radio advertising spend and our current values for Weight and Bias.\n\n                            Sales=Weight\u22c5Radio+Bias\n\n**Weight**\n\nthe coefficient for the Radio independent variable. In machine learning we call coefficients weights.\n\n**Radio**\n\nthe independent variable. In machine learning we call these variables features.\n\n**Bias**\n\nthe intercept where our line intercepts the y-axis. In machine learning we can call intercepts bias. Bias offsets all predictions that we make.\n\nOur algorithm will try to learn the correct values for Weight and Bias. By the end of our training, our equation will approximate the line of best fit.\n\n![pic1.jpg](https:\/\/ml-cheatsheet.readthedocs.io\/en\/latest\/_images\/linear_regression_line_intro.png)\n\n","02bec83d":"**Model Evaluation** \n\n![pic2](https:\/\/ml-cheatsheet.readthedocs.io\/en\/latest\/_images\/linear_regression_line_1.png)\n\n![pic3](https:\/\/ml-cheatsheet.readthedocs.io\/en\/latest\/_images\/linear_regression_line_2.png)\n\n![pic4](https:\/\/ml-cheatsheet.readthedocs.io\/en\/latest\/_images\/linear_regression_line_3.png)\n\n![pic5](https:\/\/ml-cheatsheet.readthedocs.io\/en\/latest\/_images\/linear_regression_line_4.png)\n\n**Cost History**\n\n![pic6](https:\/\/ml-cheatsheet.readthedocs.io\/en\/latest\/_images\/linear_regression_training_cost.png)\n","4c706935":"**Gradient descent**\n\nTo minimize MSE we use Gradient Descent to calculate the gradient of our cost function. \n\n**Math**\n\nThere are two parameters (coefficients) in our cost function we can control: weight m and bias b. Since we need to consider the impact each one has on the final prediction, we use partial derivatives. To find the partial derivatives, we use the Chain rule. We need the chain rule because (y\u2212(mx+b))2 is really 2 nested functions: the inner function y\u2212(mx+b) and the outer function x2.\n\n**Returning to our cost function:**\n\n                        f(m,b)=1N\u2211i=1n(yi\u2212(mxi+b))2\n\nWe can calculate the gradient of this cost function as:\n\n![Capture.PNG](attachment:Capture.PNG)\n  \n**Code**\n\nTo solve for the gradient, we iterate through our data points using our new weight and bias values and take the average of the partial derivatives. The resulting gradient tells us the slope of our cost function at our current position (i.e. weight and bias) and the direction we should update to reduce our cost function (we move in the direction opposite the gradient). The size of our update is controlled by the learning rate.","007fffd6":"Hello Everyone!!!\nIn this notebook i will explain working of linear regression from scratch and it's implmentation without using any ml library.","ad94f061":"**Introduction**\n\nLinear Regression is a supervised machine learning algorithm where the predicted output is continuous and has a constant slope. It\u2019s used to predict values within a continuous range, (e.g. sales, price) rather than trying to classify them into categories (e.g. cat, dog). There are two main types:","37c7fc22":"**Simple Regression**\n\nLet\u2019s say we are given a dataset with the following columns (features): how much a company spends on Radio advertising each year and its annual Sales in terms of units sold. We are trying to develop an equation that will let us to predict units sold based on how much a company spends on radio advertising. The rows (observations) represent companies.","b34bb032":"**Training**\n\nTraining a model is the process of iteratively improving your prediction equation by looping through the dataset multiple times, each time updating the weight and bias values in the direction indicated by the slope of the cost function (gradient). Training is complete when we reach an acceptable error threshold, or when subsequent training iterations fail to reduce our cost.\n\nBefore training we need to initialize our weights (set default values), set our hyperparameters (learning rate and number of iterations), and prepare to log our progress over each iteration.\n\n","05ed23b0":"**Simple regression**\nSimple linear regression uses traditional slope-intercept form, where m and b are the variables our algorithm will try to \u201clearn\u201d to produce the most accurate predictions. x represents our input data and y represents our prediction.\n\ny=mx+b"}}