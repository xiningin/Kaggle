{"cell_type":{"67aa0dd0":"code","1446d0e3":"code","61eb5f70":"code","a2c9882e":"code","d02a0767":"code","128db1c6":"code","61343f87":"code","d300e76f":"code","351cdea8":"code","d406508e":"code","aa7c0ea8":"code","2482dbed":"code","5826d757":"code","490aba23":"code","3ebabe7e":"code","63864ab5":"code","b124b9f6":"code","341ff5c2":"code","053956ed":"code","5bc3539f":"code","995fb42a":"code","07d4adae":"code","e36b5305":"markdown","a641366d":"markdown","f0fca2e8":"markdown","624be951":"markdown","dc4bf6ac":"markdown","2a0963cf":"markdown","ce420d14":"markdown","d4c68f52":"markdown","6d59fa4c":"markdown","cea9df0a":"markdown","90fb476b":"markdown","4e6cc603":"markdown","18e1a420":"markdown"},"source":{"67aa0dd0":"import pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"..\/input\/water-potability\/water_potability.csv\")\ndf.head()","1446d0e3":"#shape of the data\ndf.shape","61eb5f70":"#Check for missing values\ndf.isnull().sum()","a2c9882e":"#Dropping missing values\n#because water quality is a sensitive data, we cannot tamper with the data by imputing mean, median, mode\ndf= df.dropna()\n","d02a0767":"df.Potability.value_counts()","128db1c6":"#Plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndf.Potability.value_counts().plot(kind ='pie')","61343f87":"zero  = df[df['Potability']==0]   #zero values in Potability column\none = df[df['Potability']==1]  # one values in Potability column\nfrom sklearn.utils import resample\n#minority class that  is 1, we need to upsample\/increase that class so that there is no bias\n#n_samples = 1998 means we want 1998 sample of class 1, since there are 1998 samples of class 0\ndf_minority_upsampled = resample(one, replace = True, n_samples = 1200) \n#concatenate\ndf = pd.concat([zero, df_minority_upsampled])\n\nfrom sklearn.utils import shuffle\ndf = shuffle(df) # shuffling so that there is particular sequence","d300e76f":"df.Potability.value_counts().plot(kind ='pie')","351cdea8":"#understanding correlation\nplt.figure(figsize = (15,9))\nsns.heatmap(df.corr(), annot = True)","d406508e":"sns.scatterplot(x=df[\"ph\"], y=df[\"Hardness\"], hue=df.Potability,\ndata=df)\n","aa7c0ea8":"sns.scatterplot(x=df[\"ph\"], y=df[\"Chloramines\"], hue=df.Potability,\ndata=df)","2482dbed":"df.corr().abs()['Potability'].sort_values(ascending = False)","5826d757":"X = df.drop(['Potability'], axis = 1)\ny = df['Potability']","490aba23":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nfeatures= X.columns\nX[features] = sc.fit_transform(X[features])","3ebabe7e":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom xgboost import XGBClassifier\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV","63864ab5":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.1)","b124b9f6":"#Hyperparameter tuning ;)\n\nlr = LogisticRegression(random_state=42)\n\nknn = KNeighborsClassifier()\n\ndt = DecisionTreeClassifier()\n\nrf = RandomForestClassifier()\n\nada = AdaBoostClassifier()\n\nxgb =XGBClassifier(eval_metric = 'logloss', use_label_encoder=False)\n\n\npara_knn = {'n_neighbors':np.arange(1, 50)}  #parameters of knn\ngrid_knn = GridSearchCV(knn, param_grid=para_knn, cv=5) #search knn for 5 fold cross validation\n\n#parameters for decision tree\npara_dt = {'criterion':['gini','entropy'],'max_depth':np.arange(1, 50), 'min_samples_leaf':[1,2,4,5,10,20,30,40,80,100]}\ngrid_dt = GridSearchCV(dt, param_grid=para_dt, cv=5) #grid search decision tree for 5 fold cv\n#\"gini\" for the Gini impurity and \u201centropy\u201d for the information gain.\n#min_samples_leaf: The minimum number of samples required to be at a leaf node, have the effect of smoothing the model\n\n#parameters for random forest\n#n_estimators: The number of trees in the forest.\nparams_rf = {'n_estimators':[100,200, 350, 500], 'min_samples_leaf':[2, 10, 30]}\ngrid_rf = GridSearchCV(rf, param_grid=params_rf, cv=5)\n\n#parameters fpr AdaBoost\nparams_ada = {'n_estimators': [50,100,250,400,500,600], 'learning_rate': [0.2,0.5,0.8,1]}\ngrid_ada =  GridSearchCV(ada, param_grid=params_ada, cv=5)\n\n#XGBoost\n#parameters for xgboost\nparams_xgb = {'n_estimators': [50,100,250,400,600,800,1000], 'learning_rate': [0.2,0.5,0.8,1]}\nrs_xgb =  RandomizedSearchCV(xgb, param_distributions=params_xgb, cv=5)\n","341ff5c2":"grid_knn.fit(X_train, y_train)\ngrid_dt.fit(X_train, y_train)\ngrid_rf.fit(X_train, y_train)\ngrid_ada.fit(X_train, y_train)\nrs_xgb.fit(X_train, y_train)\n\nprint(\"Best parameters for KNN:\", grid_knn.best_params_)\nprint(\"Best parameters for Decision Tree:\", grid_dt.best_params_)\nprint(\"Best parameters for Random Forest:\", grid_rf.best_params_)\nprint(\"Best parameters for AdaBoost:\", grid_ada.best_params_)\nprint(\"Best parameters for XGBoost:\", rs_xgb.best_params_)","053956ed":"lr = LogisticRegression(random_state=42)\ndt = DecisionTreeClassifier(criterion='entropy', max_depth=42, min_samples_leaf=1, random_state=42)\nknn = KNeighborsClassifier(n_neighbors=1)\nrf = RandomForestClassifier(n_estimators=100, min_samples_leaf=2, random_state=42)\nada = AdaBoostClassifier(n_estimators= 600, learning_rate= 1 )\nxgb = XGBClassifier(n_estimators= 250, learning_rate= 0.8)\n\n#let's also apply bagging and boosting\nbagging = BaggingClassifier(DecisionTreeClassifier(criterion='entropy', max_depth=46, min_samples_leaf=2, random_state=42),\n                           n_estimators = 100, random_state = 42)\nbagging.fit(X_train, y_train)\n","5bc3539f":"classifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn),\n               ('Decision Tree', dt), ('Random Forest', rf), ('AdaBoost', ada),\n              ('Bagging Classifier', bagging), ('XGBoost', xgb)]\n","995fb42a":"from sklearn.metrics import accuracy_score\n\nfor classifier_name, classifier in classifiers:\n \n    # Fit clf to the training set\n    classifier.fit(X_train, y_train)    \n   \n    # Predict y_pred\n    y_pred = classifier.predict(X_test)\n    accuracy = accuracy_score(y_test,y_pred)\n    \n\n   \n    # Evaluate clf's accuracy on the test set\n    print('{:s} : {:.2f}'.format(classifier_name, accuracy))","07d4adae":"from sklearn.metrics import classification_report\n\ny_pred_rf= rf.predict(X_test)\nprint(classification_report(y_test, y_pred_rf))\n","e36b5305":"highest correlation with Potability is solids with 5.24% only","a641366d":"**Access to safe drinking-water is essential to health, a basic human right and a component of effective policy for health protection. This is important as a health and development issue at a national, regional and local level. In some regions, it has been shown that investments in water supply and sanitation can yield a net economic benefit, since the reductions in adverse health effects and health care costs outweigh the costs of undertaking the interventions.**\n\n![Drink water](http:\/\/nextdayinspect.com\/wp-content\/uploads\/2020\/10\/Water-Infographic-Human-Body-e1603490328306.jpg)","f0fca2e8":"Now it's perfect!","624be951":"Thus it is an imbalanced dataset, since 0 is much more 1 (1998>1278)\nSo we need to balance the data so that there is no biasedness.","dc4bf6ac":"#### Applying bagging and boosting","2a0963cf":"## Hyper-parameter Tuning ;)","ce420d14":"## Do upvote if you like it or fork it. This motivates us to produce more notebooks for the community.","d4c68f52":"The precision of class 0 is 89% and that of class 1 is 90%\nIt means the model predicts 89% of class 0 and 90% of class 1 correctly","6d59fa4c":"There are 3276 rows and 10 columns","cea9df0a":"There is no particular pattern!","90fb476b":"Random Forest has performed better.","4e6cc603":"Accuracy is 89%","18e1a420":"Wohohhhoooooo! We got the best parameters."}}