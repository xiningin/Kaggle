{"cell_type":{"9bbc2b83":"code","787dcce8":"code","2c410fb1":"code","55fea18c":"code","36f85495":"code","7f3602fd":"code","64ad3407":"code","c27caaec":"code","ae8021bf":"code","5ff0768b":"code","5855a5cc":"code","7dea225c":"code","ab30c227":"code","c164707f":"code","57d0a322":"code","4b4d47ba":"code","423cefae":"code","ef598c4d":"code","f2ba4803":"code","7f405f06":"code","42c77747":"code","7886a856":"code","67395f48":"code","dbf86072":"code","0da73218":"code","f56acd87":"code","55a3ccfe":"code","8768b1dd":"code","1def47a5":"code","91231afc":"code","f4484025":"code","55eaea2b":"code","ecc42401":"code","459b19cb":"code","571f29c0":"code","e690b5f8":"code","24ea7e56":"code","55d641e9":"code","42499abc":"code","7ddc3d4b":"code","a3856110":"code","566ac7b7":"code","c304829e":"code","14f53456":"code","700ac915":"code","ec090576":"code","f0a746f9":"code","55a11c30":"code","b00dac72":"code","c9d4b46c":"code","87a690bc":"code","af91b2b1":"code","9ca1d164":"markdown","aca1a69a":"markdown","a0c4a8da":"markdown","15a49519":"markdown","eeb1f200":"markdown"},"source":{"9bbc2b83":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","787dcce8":"from xgboost import XGBClassifier\nimport xgboost as xgb\nfrom deepctr.models.wdl import WDL\nfrom deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names\nfrom sklearn.metrics import roc_curve,log_loss\nfrom sklearn.preprocessing import OneHotEncoder,LabelEncoder, MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV","2c410fb1":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split","55fea18c":"import time\nfrom sklearn.model_selection import KFold","36f85495":"continuous_variable = [f\"I{i}\" for i in range(1, 14)]\ndiscrete_variable = [f\"C{i}\" for i in range(1, 27)]\ntrain_df_cols = [\"Label\"] + continuous_variable + discrete_variable\ntest_df_cols = continuous_variable + discrete_variable","7f3602fd":"train_df = pd.read_csv(\"..\/input\/criteo-dataset\/dac\/train.txt\", sep='\\t', names=train_df_cols, nrows=1000000)","64ad3407":"train_df.head()","c27caaec":"test_df = pd.read_csv(\"..\/input\/criteo-dataset\/dac\/test.txt\", sep='\\t', names=test_df_cols)","ae8021bf":"print(f\"train_df has {train_df.shape[0]} rows and {train_df.shape[1]} columns.\")\nprint(f\"test_df has {test_df.shape[0]} rows and {test_df.shape[1]} columns.\")","5ff0768b":"for col in train_df.columns:\n    if train_df[col].dtypes == \"object\":\n        train_df[col], uniques = pd.factorize(train_df[col])\n    train_df[col].fillna(train_df[col].mean(), inplace=True)","5855a5cc":"train_df.head()","7dea225c":"train_df.info()","ab30c227":"count_label = pd.value_counts(train_df[\"Label\"], sort=True)\ncount_label.plot(kind=\"bar\")    # \u6761\u5f62\u56fe\nplt.title(\"Label Statistics\")\nplt.xlabel(\"Label\")\nplt.ylabel(\"Frequency\")","c164707f":"#\u8d1f\u6837\u672c:\u6b63\u6837\u672c = 2.92:1\uff08scale_pos_weight=2.92\uff09\npd.value_counts(train_df[\"Label\"], sort=True)","57d0a322":"# \u4e0b\u91c7\u6837\nnumber_of_click = len(train_df[train_df.Label == 1])    # \u7edf\u8ba1\u70b9\u51fb\u7684\u6570\u636e\u91cf\nclick_indices = np.array(train_df[train_df.Label == 1].index)    # \u70b9\u51fb\u7684\u6570\u636e\u7d22\u5f15\nno_click_indices = np.array(train_df[train_df.Label == 0].index)   # \u6b63\u5e38\u6570\u636e\u7d22\u5f15\n\n# \u5728\u6b63\u5e38\u6570\u636e\u4e2d\u968f\u673a\u9009\u62e9\u4e0e\u88ab\u8bc8\u9a97\u6570\u636e\u91cf\u76f8\u7b49\u7684\u6b63\u5e38\u6570\u636e\u7684\u7d22\u5f15\nrandom_no_click_indices = np.array(np.random.choice(no_click_indices, number_of_click, replace=False))\n\n# \u5c06\u6240\u6709\u70b9\u51fb\u7684\u6570\u636e\u7d22\u5f15\u548c\u672a\u70b9\u51fb\u7684\u7b49\u91cf\u6570\u636e\u7d22\u5f15\u5408\u5e76\nunder_sample_indices = np.concatenate([click_indices, random_no_click_indices])\n# \u4ece\u539f\u59cb\u6570\u636e\u4e2d\u53d6\u51fa\u4e0b\u91c7\u6837\u6570\u636e\u96c6\nunder_sample_train_df = train_df.iloc[under_sample_indices, :]\n\nX_under_sample = under_sample_train_df.iloc[:, under_sample_train_df.columns != \"Label\"]\nY_under_sample = under_sample_train_df.iloc[:, under_sample_train_df.columns == \"Label\"]","4b4d47ba":"# \u770b\u4e00\u4e0b\u4e0b\u91c7\u6837\u6570\u636e\u96c6\u7684\u957f\u5ea6\nprint(\"Total number of under_sample_train_df =\", len(under_sample_train_df))\nprint(\"Total number of no_click =\", len(under_sample_train_df[train_df.Label == 0]))\nprint(\"Total number of click =\", len(under_sample_train_df[train_df.Label == 1]))","423cefae":"#\u4e0b\u91c7\u6837\u524d\n#train_x, train_y = train_df.iloc[:, train_df.columns != \"Label\"], train_df.iloc[:, train_df.columns == \"Label\"]","ef598c4d":"#\u4e0b\u91c7\u6837\u524d\n#train_y = np.array(train_y).ravel()","f2ba4803":"#\u4e0b\u91c7\u6837\u540e\n#Y_under_sample = np.array(Y_under_sample).ravel()","7f405f06":"X_under_sample.head()","42c77747":"Y_under_sample","7886a856":"X_under_sample_train,X_under_sample_test,y_under_sample_train,y_under_sample_test = train_test_split(X_under_sample,Y_under_sample,test_size=0.3)","67395f48":"print(f\" X_under_sample_train:{X_under_sample_train.shape}\\n X_under_sample_test:{X_under_sample_test.shape}\\n y_under_sample_train:{y_under_sample_train.shape}\\n y_under_sample_test:{y_under_sample_test.shape}\")\ny_under_sample_train.shape\nX_under_sample_train","dbf86072":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score","0da73218":"def eval_model(y_pred, y_true):\n    print(f\"accuracy_score = {accuracy_score(y_true, y_pred)}\")\n    print(f\"precision_score = {precision_score(y_true, y_pred)}\")\n    print(f\"recall_score = {recall_score(y_true, y_pred)}\")\n    print(f\"f1_score = {f1_score(y_true, y_pred)}\")\n    print(f\"auc = {roc_auc_score(y_true, y_pred)}\")","f56acd87":"def kfold_lr_scores(x_train_data, y_train_data):\n    start_time = time.time()\n    fold = KFold(3, shuffle=True)       # 3\u6298\u4ea4\u53c9\u9a8c\u8bc1\n    c_param_range = [10, 100, 1000]      # \u60e9\u7f5a\u529b\u5ea6\uff0c\u6b63\u5219\u5316\u60e9\u7f5a\u9879\u7684\u7cfb\u6570  \n    \n    # \u505a\u53ef\u89c6\u5316\u5c55\u793a\n    results_table = pd.DataFrame(index=range(len(c_param_range), 2), columns=[\"C_parameter\", \"Mean recall scores\"])\n    results_table[\"C_parameter\"] = c_param_range\n    \n    # \u4e0d\u786e\u5b9a\u54ea\u4e00\u4e2a\u6b63\u5219\u5316\u60e9\u7f5a\u9879\u7684\u7cfb\u6570\u66f4\u597d\uff0c\u56e0\u6b64\u91c7\u7528\u5faa\u73af\u786e\u8ba4\n    index = 0\n    for c_param in c_param_range:\n        print('--------------------------------------------------------------------------------')\n        print(\"If C parameter =\", c_param, end=\"\\n\\n\")\n        \n        # \u505a\u4ea4\u53c9\u9a8c\u8bc1\n        recall_accs = []\n        lr = LogisticRegression(C=c_param, penalty='l1', solver='liblinear', max_iter=10000)\n        for iteration, indices in enumerate(fold.split(x_train_data)):\n            # \u62df\u5408\u8bad\u7ec3\u6570\u636e\n            lr.fit(x_train_data.iloc[indices[0], :], y_train_data.iloc[indices[0], :].values.ravel())\n            # \u4f7f\u7528\u9a8c\u8bc1\u96c6\u5f97\u51fa\u9884\u6d4b\u6570\u636e\n            y_predicted_undersample = lr.predict(x_train_data.iloc[indices[1], :])\n            # \u8ba1\u7b97recall\n            recall_acc = recall_score(y_train_data.iloc[indices[1], :], y_predicted_undersample)\n            recall_accs.append(recall_acc)\n            print('\\tIteration ', iteration, ': recall score = ', recall_acc)\n        \n        index += 1\n        \n        # \u8ba1\u7b97recall\u7684\u5e73\u5747\u503c\n        results_table.loc[index, \"Mean recall scores\"] = np.mean(recall_accs)\n        print('Mean recall score = ', results_table.loc[index, \"Mean recall scores\"], end=\"\\n\\n\")\n        print('--------------------------------------------------------------------------------')\n    \n    best_c_param = results_table.loc[results_table['Mean recall scores'].astype(float).idxmax()]['C_parameter']\n    print('Best C parameter = ', best_c_param, \"\\t duration: \", time.time() - start_time)\n    return lr, best_c_param\n","55a3ccfe":"# \u5bf9\u4e0b\u91c7\u6837\u6570\u636e\u8fdb\u884c\u6d4b\u8bd5\n#lr2, param2 = kfold_lr_scores(X_under_sample, Y_under_sample)\nlr = LogisticRegression(solver=\"liblinear\")","8768b1dd":"#lr = LogisticRegression(C=param2, penalty='l1', solver='liblinear', max_iter=10000)\nlr.fit(X_under_sample, Y_under_sample)","1def47a5":"#lr2.score(X_under_sample_test, y_under_sample_test)\nlr.score(X_under_sample_test, y_under_sample_test)","91231afc":"y_predict_lr = lr.predict(X_under_sample_test)\neval_model(y_predict_lr, y_under_sample_test)","f4484025":"# \u4e0b\u91c7\u6837\u540e\u505a\u4ea4\u53c9\u9a8c\u8bc1\ndef kfold_scores(x_train_data, y_train_data):\n    start_time = time.time()\n    fold = KFold(3, shuffle=True)  # 3\u6298\u4ea4\u53c9\u9a8c\u8bc1\n    #c_param_range = [0, 0.5, 1]      # \u60e9\u7f5a\u529b\u5ea6\uff0c\u6b63\u5219\u5316\u60e9\u7f5a\u9879\u7684\u7cfb\u6570\n\n    # \u505a\u53ef\u89c6\u5316\u5c55\u793a\n    results_table = pd.DataFrame(index=range(2),\n                                 columns=[\"C_parameter\", \"Mean recall scores\", \"Mean auc socres\"])\n\n    index = 0\n    print('--------------------------------------------------------------------------------')\n\n    # \u505a\u4ea4\u53c9\u9a8c\u8bc1\n    recall_accs = []\n    auc_scores = []\n    # \u53ef\u4ee5\u52a0\u5165scale_pos_weight=2.92\u53c2\u6570\uff0c\u5982\u679c\u4f7f\u7528\u4e0b\u91c7\u6837\u5c31\u4e0d\u52a0\u5165scale_pos_weight\u4ee5\u514d\u4e92\u76f8\u5f71\u54cd\n    xgb_model = XGBClassifier(objective=\"binary:logistic\", n_jobs=-1, n_estimators=1000, max_depth=16,\n                              eval_metric=\"auc\",\n                              colsample_bytree=0.8, subsample=0.8, learning_rate=0.2, min_child_weight=6)\n    for iteration, indices in enumerate(fold.split(x_train_data)):\n        # \u62df\u5408\u8bad\u7ec3\u6570\u636e\n        # lr.fit(x_train_data.iloc[indices[0], :], y_train_data.iloc[indices[0], :].values.ravel())\n        xgb_model.fit(x_train_data.iloc[indices[0], :], y_train_data.iloc[indices[0], :].values.ravel(),\n                      eval_metric=\"logloss\",\n                      eval_set=[(x_train_data.iloc[indices[0], :], y_train_data.iloc[indices[0], :]),\n                                (x_train_data.iloc[indices[1], :], y_train_data.iloc[indices[1], :])], verbose=True,\n                      early_stopping_rounds=10)\n        # \u4f7f\u7528\u9a8c\u8bc1\u96c6\u5f97\u51fa\u9884\u6d4b\u6570\u636e\n        # \u6700\u9002\u5408\u7684\u8fed\u4ee3\u6b21\u6570\uff0c\u7136\u540e\u9884\u6d4b\u7684\u65f6\u5019\u5c31\u4f7f\u7528stop\u4e4b\u524d\u8bad\u7ec3\u7684\u6811\u6765\u9884\u6d4b\u3002\n        print(f\"\u6700\u4f73\u8fed\u4ee3\u6b21\u6570\u4e3a\uff1a{xgb_model.best_iteration}\")\n        limit = xgb_model.best_iteration\n        # y_predicted_undersample = lr.predict(x_train_data.iloc[indices[1], :])\n        y_predicted_undersample = xgb_model.predict(x_train_data.iloc[indices[1], :], ntree_limit=limit)\n        # \u8ba1\u7b97recall\n        recall_acc = recall_score(y_train_data.iloc[indices[1], :], y_predicted_undersample)\n        recall_accs.append(recall_acc)\n        auc_score = roc_auc_score(y_train_data.iloc[indices[1], :], y_predicted_undersample)\n        auc_scores.append(auc_score)\n        print('\\tIteration ', iteration, ': recall score = ', recall_acc)\n        print('\\tIteration ', iteration, ': auc score = ', auc_score)\n\n    index += 1\n\n    # \u8ba1\u7b97recall\u7684\u5e73\u5747\u503c\n    results_table.loc[index, \"Mean recall scores\"] = np.mean(recall_accs)\n    results_table.loc[index, \"Mean auc scores\"] = np.mean(auc_scores)\n    print('Mean recall score = ', results_table.loc[index, \"Mean recall scores\"], end=\"\\n\\n\")\n    print('Mean auc score = ', results_table.loc[index, \"Mean auc scores\"], end=\"\\n\\n\")\n    print('--------------------------------------------------------------------------------')\n    return xgb_model","55eaea2b":"# \u5bf9\u4e0b\u91c7\u6837\u6570\u636e\u8fdb\u884c\u6d4b\u8bd5\nxgb = kfold_scores(X_under_sample, Y_under_sample)","ecc42401":"#\u53ef\u4ee5\u52a0\u5165scale_pos_weight=2.92\u53c2\u6570\uff0c\u5982\u679c\u4f7f\u7528\u4e0b\u91c7\u6837\u5c31\u4e0d\u52a0\u5165scale_pos_weight\u4ee5\u514d\u4e92\u76f8\u5f71\u54cd\n#xgb_model = XGBClassifier(objective=\"binary:logistic\", n_jobs=-1, n_estimators=1000, max_depth=16, eval_metric=\"auc\",\n#                      colsample_bytree=0.8, subsample=0.8, learning_rate=0.2, min_child_weight=6)","459b19cb":"#xgb_model.fit(X_train, Y_train, eval_metric=\"logloss\", eval_set=[(X_train, Y_train), (X_test, y_test)], verbose=True, early_stopping_rounds=10)","571f29c0":"#\u6700\u9002\u5408\u7684\u8fed\u4ee3\u6b21\u6570\uff0c\u7136\u540e\u9884\u6d4b\u7684\u65f6\u5019\u5c31\u4f7f\u7528stop\u4e4b\u524d\u8bad\u7ec3\u7684\u6811\u6765\u9884\u6d4b\u3002\n#print (f\"\u6700\u4f73\u8fed\u4ee3\u6b21\u6570\u4e3a\uff1a{xgb_model.best_iteration}\")\n#limit = xgb_model.best_iteration","e690b5f8":"#roc_auc_score(Y_train, xgb_model.predict(X_train))","24ea7e56":"y_predict_xgb = xgb.predict(X_under_sample_test,ntree_limit=xgb.best_iteration)\neval_model(y_predict_xgb, y_under_sample_test)","55d641e9":"#\u53ef\u4ee5\u52a0\u5165scale_pos_weight=2.92\u53c2\u6570\uff0c\u5982\u679c\u4f7f\u7528\u4e0b\u91c7\u6837\u5c31\u4e0d\u52a0\u5165scale_pos_weight\u4ee5\u514d\u4e92\u76f8\u5f71\u54cd\nxgb_model = XGBClassifier(objective=\"binary:logistic\", n_jobs=-1, n_estimators=1000, max_depth=16, eval_metric=\"auc\",\n                      colsample_bytree=0.8, subsample=0.8, learning_rate=0.2, min_child_weight=6)","42499abc":"xgb_enc = OneHotEncoder()\nxgb_lm = LogisticRegression(solver='lbfgs', max_iter=1000)\nX_train_xgb, X_train_lr, y_train_xgb, y_train_lr = train_test_split(X_under_sample_train, y_under_sample_train, test_size=0.25)","7ddc3d4b":"xgb_model.fit(X_train_xgb, y_train_xgb, eval_metric=\"logloss\", eval_set=[(X_train_xgb, y_train_xgb), (X_under_sample_test, y_under_sample_test)], verbose=True, early_stopping_rounds=10)","a3856110":"print(X_train_xgb.shape)\nprint(X_train_lr.shape)\nprint(xgb_model.apply(X_train_xgb).shape)\nprint(xgb_model.apply(X_train_lr).shape)","566ac7b7":"xgb_enc.fit(xgb_model.apply(X_train_xgb))\nxgb_lm.fit(xgb_enc.transform(xgb_model.apply(X_train_lr)), y_train_lr)\n\ny_pred_xgb_lm = xgb_lm.predict_proba(\n    xgb_enc.transform(xgb_model.apply(X_under_sample_test)))[:, 1]\nfpr_xgb_lm, tpr_xgb_lm, _ = roc_curve(y_under_sample_test, y_pred_xgb_lm)\nprint(\"xgboost+LR\u7684AUC\u4e3a\uff1a\", roc_auc_score(y_under_sample_test, y_pred_xgb_lm))","c304829e":"roc_auc_score(y_under_sample_test, xgb_lm.predict(xgb_enc.transform(xgb_model.apply(X_under_sample_test))))","14f53456":"y_predict_xgb_lr = xgb_lm.predict(xgb_enc.transform(xgb_model.apply(X_under_sample_test)))\neval_model(y_predict_xgb_lr, y_under_sample_test)","700ac915":"# \u4e0b\u91c7\u6837\u540e\u505a\u4ea4\u53c9\u9a8c\u8bc1\ndef kfold_scores(X_under_sample_train, y_under_sample_train,X_under_sample_test,y_under_sample_test):\n    X_train_xgb, X_train_lr, y_train_xgb, y_train_lr = train_test_split(X_under_sample_train, y_under_sample_train, test_size=0.25)\n    #print(X_train_xgb.shape,X_train_lr.shape,y_train_xgb.shape,y_train_lr.shape)\n    xgb_enc = OneHotEncoder()\n    xgb_lm = LogisticRegression(solver='lbfgs', max_iter=1000)\n    start_time = time.time()\n    fold = KFold(3, shuffle=True)  # 3\u6298\u4ea4\u53c9\u9a8c\u8bc1\n\n    # \u505a\u53ef\u89c6\u5316\u5c55\u793a\n    results_table = pd.DataFrame(index=range(2),\n                                 columns=[\"C_parameter\", \"Mean recall scores\", \"Mean auc socres\"])\n\n    index = 0\n    print('--------------------------------------------------------------------------------')\n\n    # \u505a\u4ea4\u53c9\u9a8c\u8bc1\n    recall_accs = []\n    auc_scores = []\n    # \u53ef\u4ee5\u52a0\u5165scale_pos_weight=2.92\u53c2\u6570\uff0c\u5982\u679c\u4f7f\u7528\u4e0b\u91c7\u6837\u5c31\u4e0d\u52a0\u5165scale_pos_weight\u4ee5\u514d\u4e92\u76f8\u5f71\u54cd\n    xgb_model = XGBClassifier(objective=\"binary:logistic\", n_jobs=-1, n_estimators=1000, max_depth=16,\n                              eval_metric=\"auc\",\n                              colsample_bytree=0.8, subsample=0.8, learning_rate=0.2, min_child_weight=6)\n    \n    for iteration, indices in enumerate(fold.split(X_train_xgb)):\n        # \u62df\u5408\u8bad\u7ec3\u6570\u636e\n        xgb_model.fit(X_train_xgb.iloc[indices[0], :],  y_train_xgb.iloc[indices[0], :].values.ravel(),eval_metric=\"logloss\",\n                      eval_set=[(X_train_xgb.iloc[indices[0], :], y_train_xgb.iloc[indices[0], :]),\n                                (X_train_xgb.iloc[indices[1], :], y_train_xgb.iloc[indices[1], :])],\n                      verbose=True,early_stopping_rounds=10)\n        #\u5bf9xgb\u9884\u6d4b\u7ed3\u679c\u8fdb\u884cone-hot\u7f16\u7801\n        #xgb_enc.fit(xgb_model.predict(X_train_xgb.iloc[indices[0], :]))\n        \n        \n        #print(X_train_xgb.iloc[indices[1],:].shape)\n        \n        \n        xgb_lm.fit(xgb_enc.fit_transform(xgb_model.predict(X_train_xgb.iloc[indices[0], :]).reshape(-1,1)),y_train_xgb.iloc[indices[0], :])\n        # \u4f7f\u7528\u9a8c\u8bc1\u96c6\u5f97\u51fa\u9884\u6d4b\u6570\u636e\n        y_predicted_undersample_lr = xgb_lm.predict(xgb_enc.fit_transform(xgb_model.predict(X_under_sample_test).reshape(-1,1)))\n        # \u8ba1\u7b97recall\n        recall_acc = recall_score(y_under_sample_test, y_predicted_undersample_lr)\n        recall_accs.append(recall_acc)\n        auc_score = roc_auc_score(y_under_sample_test, y_predicted_undersample_lr)\n        auc_scores.append(auc_score)\n        print('\\tIteration ', iteration, ': recall score = ', recall_acc)\n        print('\\tIteration ', iteration, ': auc score = ', auc_score)\n\n    index += 1\n\n    # \u8ba1\u7b97recall\u7684\u5e73\u5747\u503c\n    results_table.loc[index, \"Mean recall scores\"] = np.mean(recall_accs)\n    results_table.loc[index, \"Mean auc scores\"] = np.mean(auc_scores)\n    print('Mean recall score = ', results_table.loc[index, \"Mean recall scores\"], end=\"\\n\\n\")\n    print('Mean auc score = ', results_table.loc[index, \"Mean auc scores\"], end=\"\\n\\n\")\n    print('--------------------------------------------------------------------------------')\n    return xgb_lm,xgb_enc,xgb_model","ec090576":"# \u5bf9\u4e0b\u91c7\u6837\u6570\u636e\u8fdb\u884c\u6d4b\u8bd5\nxgb_lr,xgb_enc,xgb_model = kfold_scores(X_under_sample_train, y_under_sample_train,X_under_sample_test,y_under_sample_test)","f0a746f9":"y_predict_xgb = xgb_lr.predict(xgb_enc.transform(xgb_model.predict(X_under_sample_test).reshape(-1,1)))\neval_model(y_predict_xgb, y_under_sample_test)","55a11c30":"#\u7528\u4e0b\u91c7\u6837\u6570\u636e\u8fdb\u884c\u6d4b\u8bd5\ndata = pd.concat([X_under_sample,Y_under_sample],axis=1)\ndata.head()","b00dac72":"sparse_features = ['C' + str(i) for i in range(1, 27)]\ndense_features = ['I' + str(i) for i in range(1, 14)]\n\ndata[sparse_features] = data[sparse_features].fillna('-1', )\ndata[dense_features] = data[dense_features].fillna(0, )\ntarget = ['Label']\n\nfor feat in sparse_features:\n    lbe = LabelEncoder()\n    data[feat] = lbe.fit_transform(data[feat])\n\nmms = MinMaxScaler(feature_range=(0, 1))\ndata[dense_features] = mms.fit_transform(data[dense_features])\n\nsparse_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].nunique(), embedding_dim=4)\n                          for i, feat in enumerate(sparse_features)]\n# \u6216\u8005hash,vocabulary_size\u901a\u5e38\u8981\u5927\u4e00\u4e9b\uff0c\u4ee5\u907f\u514dhash\u51b2\u7a81\u592a\u591a\n# sparse_feature_columns = [SparseFeat(feat, vocabulary_size=1e6,embedding_dim=4,use_hash=True)\n#                            for i,feat in enumerate(sparse_features)]#The dimension can be set according to data\ndense_feature_columns = [DenseFeat(feat, 1)\n                         for feat in dense_features]\n\ndnn_feature_columns = sparse_feature_columns + dense_feature_columns\nlinear_feature_columns = sparse_feature_columns + dense_feature_columns\nfeature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n\ntrain, test = train_test_split(data, test_size=0.2)\n\ntrain_model_input = {name: train[name].values for name in feature_names}\ntest_model_input = {name: test[name].values for name in feature_names}\n\nmodel = WDL(linear_feature_columns, dnn_feature_columns, task='binary')\nmodel.compile(\"adam\", \"binary_crossentropy\",\n              metrics=['binary_crossentropy'], )\n\nhistory = model.fit(train_model_input, train[target].values,\n                    batch_size=256, epochs=10, verbose=2, validation_split=0.2, )\npred_ans = model.predict(test_model_input, batch_size=256)\n","c9d4b46c":"pred_ans = model.predict(test_model_input, batch_size=256)\n#logloss(y_true, y_pred, eps=1e-15):\u5bf9\u6570\u635f\u5931\u51fd\u6570\u7684\u8f93\u5165\u53c2\u6570 y_pred \u4e3a\u5f53\u9884\u6d4b\u5b9e\u4f8b\u5c5e\u4e8e\u7c7b 1 \u65f6\u7684\u6982\u7387; \n#\u6545pred_ans\u5e94\u5f53\u4e3a\u5f53\u9884\u6d4b\u5b9e\u4f8b\u5c5e\u4e8e\u7c7b 1 \u65f6\u7684\u6982\u7387\nprint(\"test LogLoss\", round(log_loss(test[target].values, pred_ans), 4))\nprint(\"test AUC\", round(roc_auc_score(test[target].values, pred_ans), 4))","87a690bc":"pred_ans2 = []\nfor i in pred_ans:\n    if i>=0.5:\n        pred_ans2.append(1)\n    else:\n        pred_ans2.append(0)\n#print(pred_ans2)","af91b2b1":"eval_model(pred_ans2,test[target].values)","9ca1d164":"## \u8bc4\u4f30\u6307\u6807","aca1a69a":"## wide&deep","a0c4a8da":"## Scikit-Learn LogisticRegression","15a49519":"## XGB Classifier","eeb1f200":"## LR + XGB"}}