{"cell_type":{"f5d271cb":"code","1d02ac47":"code","c24da1c5":"code","00920171":"code","d2effc7c":"code","f2629aab":"code","5c4b270c":"code","8fbf77e9":"code","783fed97":"code","d8bf498a":"code","4990de9c":"code","d0095c73":"code","9444d0fe":"code","ddda3c1c":"code","ce02c5fe":"code","219d5c68":"code","075b2d0e":"code","ee0ad9a0":"markdown","436a6f5d":"markdown","30f3ac77":"markdown","fb9d7e2c":"markdown","8e7871d1":"markdown","2d563d43":"markdown","f22435a0":"markdown","c7e767d9":"markdown","30858d87":"markdown","134b16b5":"markdown","69a3db57":"markdown","65b16cf2":"markdown","1066b044":"markdown","9a6445e3":"markdown","67a2e108":"markdown"},"source":{"f5d271cb":"# Familiar imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom math import ceil # round numbers up\n%matplotlib inline\n# For ordinal encoding categorical variables, splitting data\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\n\n# For training random forest model\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)","1d02ac47":"# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\n# Preview the data\ntrain.head()","c24da1c5":"bins = np.arange(0, 12, 0.1)\nsns.displot(train.target, height = 5, aspect = 2, bins = bins);","00920171":"plt.figure(figsize= (20, 15))\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.transpose(np.tril(np.ones(train.corr().shape)))\nsns.heatmap(train.corr(), annot = True, center = 0, cmap = 'RdBu', mask = mask);","d2effc7c":"num_cols = [col for col in train.columns if 'cont' in col] \nnum_cols","f2629aab":"def plot(data, cols, features_type, nrows, ncols, bins='auto', target=None, figsize=None,\n         hspace=None, wspace=None, color = None):\n    '''plot all features vs target or the distribution of features'''\n    if figsize != None:\n        plt.figure(figsize = figsize)\n    for col, plot_num in zip(cols, list(range(1, len(cols)))):\n        plt.subplot(nrows, ncols, plot_num)\n        if hspace != None or wspace != None:\n            plt.subplots_adjust(hspace = hspace, wspace = wspace)\n            \n        if features_type == 'numerical':\n            if target != None:\n                plt.scatter(data[col], data[target])\n                plt.title(col)\n            else:\n                sns.histplot(data[col], bins=bins)\n                \n        if features_type == 'categorical':\n            if target != None:\n                sns.violinplot(data=data, y=col, x=target, color=color, inner='quartile');\n            else:\n                countplot_ratio(x = col, data = data, color = color)","5c4b270c":"n_cols = 4\nn_rows = ceil(len(num_cols)\/n_cols)\nbins = np.arange(0, 1.3, 0.02)\nplot(data=train, cols=num_cols, features_type='numerical', nrows=n_rows, ncols=n_cols, hspace=0.3, wspace=0.5, bins=bins,\n    figsize = (15, 15))","8fbf77e9":"n_cols = 4\nn_rows = ceil(len(num_cols)\/n_cols)\nplot(data=train, target='target', cols=num_cols, features_type='numerical', nrows=n_rows, ncols=n_cols, hspace=0.3,\n    figsize = (15, 15))","783fed97":"# List of categorical columns\nobject_cols = [col for col in train.columns if 'cat' in col]\nobject_cols","d8bf498a":"# function to plot the distribution of categorical variable \n# since the countplot function show the counts of observations in each categorical bin using bars.\ndef countplot_ratio(x = None, data = None, hue = None, ax = None, color = None):\n    # plot the variable\n    ax = sns.countplot(x, data = data, hue = hue, ax = ax, color = color)\n    # names of x labels\n    ax.set_xticklabels(ax.get_xticklabels())\n    # plot title\n    ax.set_title(x + \" Distribution\")\n    # total number of data which used to get the proportion\n    total = float(len(data))\n    # for loop to iterate on the patches\n    for patch in ax.patches:\n        # get the height of the patch which represents the number of observations.\n        height = patch.get_height()\n        # Put text on each patch with the proportion of the observations\n        ax.text(patch.get_x()+patch.get_width()\/2,height+4,'{:.2f}%'.format((height\/total)*100),weight = 'bold',\n                fontsize = 12,ha = 'center')","4990de9c":"n_cols = 2\nn_rows = ceil(len(object_cols)\/n_cols)\nbase_color = sns.color_palette(n_colors=2)[1]\nplot(data=train, cols=object_cols, features_type='categorical', nrows=n_rows, ncols=n_cols,\n     hspace=0.5, figsize = (15, 20), color=base_color)","d0095c73":"n_cols = 3\nn_rows = ceil(len(object_cols)\/n_cols)\nplot(data=train, target='target', cols=object_cols, features_type='categorical',\n     nrows=n_rows, ncols=n_cols, hspace=0.5, figsize = (15, 20), color=base_color)","9444d0fe":"# Separate target from features\ny = train['target']\nfeatures = train.drop(['target'], axis=1)\n\n# Preview features\nfeatures.head()","ddda3c1c":"# ordinal-encode categorical columns\nX = features.copy()\nX_test = test.copy()\nordinal_encoder = OrdinalEncoder()\nX[object_cols] = ordinal_encoder.fit_transform(features[object_cols])\nX_test[object_cols] = ordinal_encoder.transform(test[object_cols])\n\n# Preview the ordinal-encoded features\nX.head()","ce02c5fe":"fold_3_params = {'n_estimators':2500, 'max_depth' : 3,'learning_rate': 0.1,\n              'colsample_bytree':0.13,\n              'subsample':1, 'random_state':1, 'reg_alpha':25.9987, 'booster':'gbtree',\n              'min_child_weight':1.1}\n\nfold_6_params = {'n_estimators':2500, 'max_depth' : 3,'learning_rate': 0.13,\n              'colsample_bytree':0.11,\n              'subsample':0.99, 'random_state':1, 'reg_alpha':25.9987, 'booster':'gbtree',\n              'min_child_weight':1.1}\n\nfold_7_params = {'n_estimators':2500, 'max_depth' : 3,'learning_rate': 0.1,\n              'colsample_bytree':0.13,\n              'subsample':0.99, 'random_state':1, 'reg_alpha':25.9987, 'booster':'gbtree',\n              'min_child_weight':1.1}\n\nfold_8_params = {'n_estimators':2500, 'max_depth' : 3,'learning_rate': 0.1,\n              'colsample_bytree':0.13,\n              'subsample':0.99, 'random_state':1, 'reg_alpha':25.9987, 'booster':'gbtree',\n              'min_child_weight':1.1}\n# fold parameters\nn_splits = 10\nkfold  = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# To store 'out of fold' predictions, we create an array of zeros\noof_predictions = np.zeros((X.shape[0],))\npredictions = 0\ntotal_mean_RMSE = 0\n\n# Generating folds and making training and prediction for each of 10 folds\nfor fold_num, (train_ix, valid_ix) in enumerate(kfold.split(X)):\n    params = {'n_estimators':2500, 'max_depth' : 3,'learning_rate': 0.13,\n              'colsample_bytree':0.13,\n              'subsample':0.99, 'random_state':1, 'reg_alpha':25.9987, 'booster':'gbtree',\n              'min_child_weight':1.1}\n    \n    X_train, X_valid = X.iloc[train_ix], X.iloc[valid_ix]\n    y_train, y_valid = y.iloc[train_ix], y.iloc[valid_ix]\n    if fold_num == 3:\n        params = fold_3_params\n    if fold_num == 6:\n        params = fold_6_params\n    if fold_num == 7:\n        params = fold_7_params\n    if fold_num == 8:\n        params = fold_8_params\n    model = XGBRegressor(**params)\n    model.fit(X_train, y_train,\n              verbose=False,\n              # These parameters stop training before the model starts overfitting. \n              eval_set=[(X_train, y_train),(X_valid, y_valid)],\n              eval_metric=\"rmse\",\n              early_stopping_rounds=100,\n              )\n    # Getting mean test data predictions \n    predictions += model.predict(X_test) \/ n_splits\n    \n    # Now, we get validation data preds. Each fold model get predictions from unseen data.\n    # After all folds model get predictions, it will be filled with unseen data preds.\n    # We will use it to evaluate hyperparameters performance only.\n    oof_predictions[valid_ix] = model.predict(X_valid)\n    \n    # score for a fold model\n    fold_RMSE = np.sqrt(mean_squared_error(y_valid, oof_predictions[valid_ix]))\n    print(f\"Fold {fold_num} RMSE: {fold_RMSE}\")\n\n    # Getting total mean of scores of all fold models \n    total_mean_RMSE += fold_RMSE \/ n_splits\n    \nprint(f\"\\nOverall RMSE: {total_mean_RMSE}\")","219d5c68":"from sklearn.model_selection import GridSearchCV\ndef  Hyperparameter_tuning(params):\n        '''We use this function to get the best hyperparameters\n        Parameters\n        ----------\n        param: dict\n        A dictionary of hyperparameters names and lists of possible values of it\n        example:\n        param = { 'max_depth': [3,6,10],\n           'learning_rate': [0.01, 0.05, 0.1],\n           'n_estimators': [100, 500, 1000],\n           'colsample_bytree': [0.3, 0.7]}'''\n        params = params\n        model = XGBRegressor(tree_method = 'gpu_hist')\n        clf = GridSearchCV(estimator=model, \n                           param_grid=params,\n                           scoring='neg_mean_squared_error', \n                           verbose=2)\n        clf.fit(X, y)\n        print(\"Best parameters:\", clf.best_params_)\n        print(\"Lowest RMSE: \", (-clf.best_score_)**(1\/2.0))","075b2d0e":"output = pd.DataFrame()\noutput[\"id\"] = test.index\noutput[\"target\"] = predictions\n# Save the predictions to a CSV file\noutput.to_csv('submission.csv', index=False, header=output.columns)\noutput.head()","ee0ad9a0":"# Step 4: Train a model\n\nNow that the data is prepared, the next step is to train a model.  ","436a6f5d":"# Step 3: Prepare the data\n\nNext, we'll need to handle the categorical columns (`cat0`, `cat1`, ... `cat9`).  ","30f3ac77":"# Step 2: Load the data\n\nNext, we'll load the training and test data.  ","fb9d7e2c":"let's see the distribution of categorical variable vs target","8e7871d1":"let's see the correlation ","2d563d43":"### EDA","f22435a0":"Distributions of numerical features","c7e767d9":"let's see the distribution of the target variable","30858d87":"In the code cell above, we set `squared=False` to get the root mean squared error (RMSE) on the validation data.\n\n# Step 5: Submit to the competition\n\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file.","134b16b5":"let's see the distribution of continuous variable vs target","69a3db57":"The next code cell separates the target (which we assign to `y`) from the training features (which we assign to `features`).","65b16cf2":"Next, we break off a validation set from the training data.","1066b044":"Distributions of categorical features","9a6445e3":"# Step 1: Import helpful libraries","67a2e108":"We can see that target is weakly correlated with all features"}}