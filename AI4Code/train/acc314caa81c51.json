{"cell_type":{"b7793d13":"code","05043664":"code","97193694":"code","3c34f1b2":"code","fd012a0d":"code","6d56a9f9":"code","70496c9b":"code","b5854ba9":"code","e1fb478d":"code","910b995f":"code","ec678be9":"code","2e29b466":"code","b95922c7":"code","22e1964d":"code","7dc69497":"code","aa95157c":"code","a210050b":"code","9e83aba2":"code","83332bb7":"code","5532dbec":"code","aae4cde0":"code","43828da8":"code","006c6bf1":"code","42a21104":"code","1ee96a0f":"code","c38b17c1":"code","63cf84a8":"code","55f90667":"code","e5945766":"code","7f169b28":"code","437b8e1a":"code","7a1f7313":"code","dee9b227":"code","bdd98792":"code","3af265f1":"code","94a5e919":"code","09e639b4":"code","07f6b7d4":"code","1c8247c9":"code","4c322443":"code","44499fbc":"markdown","c1751430":"markdown"},"source":{"b7793d13":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","05043664":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nimport matplotlib.patches as patches\n\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\npd.set_option('max_columns', 150)\n\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nimport os\nimport random\nimport math\nimport psutil\nimport pickle\n\nfrom sklearn.model_selection import train_test_split,KFold\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb","97193694":"%%time\nroot = '..\/input\/ashrae-energy-prediction\/'\ntrain = pd.read_csv(root + 'train.csv', parse_dates=['timestamp'])\nweather_train = pd.read_csv(root + 'weather_train.csv', parse_dates=['timestamp'])\ntest = pd.read_csv(root + 'test.csv', parse_dates=['timestamp'], usecols=['building_id','meter','timestamp'])\nweather_test = pd.read_csv(root + 'weather_test.csv', parse_dates=['timestamp'])\nmetadata = pd.read_csv(root + 'building_metadata.csv')\nsample_submission = pd.read_csv(root + 'sample_submission.csv')","3c34f1b2":"print('Size of train data', train.shape)\nprint('Size of test data', train.shape)\nprint('Size of weather_train data', weather_train.shape)\nprint('Size of weather_test data', weather_test.shape)\nprint('Size of building_meta data', metadata.shape)","fd012a0d":"train['meter_reading'] = np.log1p(train['meter_reading'])","6d56a9f9":"# Dropping floor_count variable as it has 75% missing values\nmetadata.drop('floor_count',axis=1,inplace=True)","70496c9b":"metadata['primary_use'].unique()","b5854ba9":"metadata['primary_use'].replace({\"Healthcare\":\"Other\",\"Parking\":\"Other\",\"Warehouse\/storage\":\"Other\",\"Manufacturing\/industrial\":\"Other\",\n                                \"Retail\":\"Other\",\"Services\":\"Other\",\"Technology\/science\":\"Other\",\"Food sales and service\":\"Other\",\n                                \"Utility\":\"Other\",\"Religious worship\":\"Other\"},inplace=True)\n","e1fb478d":"metadata['square_feet'] = np.log1p(metadata['square_feet'])","910b995f":"np.mean(metadata['year_built'].astype(np.float32))","ec678be9":"# metadata['year_built'].fillna(1968, inplace=True)\nmetadata['year_built'].fillna(-999, inplace=True)\nmetadata['year_built'] = metadata['year_built'].astype('int16')\n# metadata['year_built'] = metadata['year_built'] - metadata['year_built'].min()","2e29b466":"metadata.isna().sum()\/len(metadata)","b95922c7":"weather_train.isna().sum()\/len(weather_train)","22e1964d":"weather_train[weather_train['cloud_coverage'].isnull()]","7dc69497":"weather_train.groupby(['site_id'])['cloud_coverage'].mean()","aa95157c":"#Fill null values with mean value from each site\ncols = ['air_temperature','cloud_coverage','dew_temperature','precip_depth_1_hr','sea_level_pressure','wind_direction','wind_speed']\nweather_train = weather_train.set_index(['site_id'])\nweather_test = weather_test.set_index(['site_id'])\nfor col in cols:\n    weather_train[col].fillna(weather_train.groupby(['site_id'])[col].mean(),inplace=True)\n    weather_test[col].fillna(weather_test[col].mean(),inplace=True)\n    \nweather_train = weather_train.reset_index()\nweather_test = weather_test.reset_index()","a210050b":"weather_train[weather_train['cloud_coverage'].isnull()]","9e83aba2":"#As there are still null values (for sites where data is totally missing)\ncols = ['air_temperature','cloud_coverage','dew_temperature','precip_depth_1_hr','sea_level_pressure','wind_direction','wind_speed']\nfor col in cols:\n    weather_train[col].fillna(-999, inplace=True)\n    weather_test[col].fillna(-999, inplace=True)","83332bb7":"## Function to reduce the DF size, from: https:\/\/www.kaggle.com\/caesarlupum\/ashrae-start-here-a-gentle-introduction\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","5532dbec":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)\nweather_train = reduce_mem_usage(weather_train)\nweather_test = reduce_mem_usage(weather_test)\nmetadata = reduce_mem_usage(metadata)","aae4cde0":"%%time\ntrain = pd.merge(train,metadata,on='building_id',how='left')\ntest  = pd.merge(test,metadata,on='building_id',how='left')\nprint (\"Training Data+Metadata Shape {}\".format(train.shape))\nprint (\"Testing Data+Metadata Shape {}\".format(test.shape))\ngc.collect()\ntrain = pd.merge(train,weather_train,on=['site_id','timestamp'],how='left')\ntest  = pd.merge(test,weather_test,on=['site_id','timestamp'],how='left')\nprint (\"Training Data+Metadata+Weather Shape {}\".format(train.shape))\nprint (\"Testing Data+Metadata+Weather Shape {}\".format(test.shape))\ngc.collect()","43828da8":"for df in [train, test]:\n    df['Month'] = df['timestamp'].dt.month.astype(\"uint8\")\n    df['DayOfMonth'] = df['timestamp'].dt.day.astype(\"uint8\")\n    df['DayOfWeek'] = df['timestamp'].dt.dayofweek.astype(\"uint8\")\n    df['Hour'] = df['timestamp'].dt.hour.astype(\"uint8\")","006c6bf1":"# Drop nonsense entries\n# As per the discussion in the following thread, https:\/\/www.kaggle.com\/c\/ashrae-energy-prediction\/discussion\/117083, there is some discrepancy in the meter_readings for different ste_id's and buildings. It makes sense to delete them\nidx_to_drop = list((train[(train['site_id'] == 0) & (train['timestamp'] < \"2016-05-21 00:00:00\")]).index)\nprint (len(idx_to_drop))\ntrain.drop(idx_to_drop,axis='rows',inplace=True)\n\n# dropping all the electricity meter readings that are 0, after considering them as anomalies.\nidx_to_drop = list(train[(train['meter'] == 0) & (train['meter_reading'] == 0)].index)\nprint(len(idx_to_drop))\ntrain.drop(idx_to_drop,axis='rows',inplace=True)","42a21104":"%%time\nnumber_unique_meter_per_building = train.groupby('building_id')['meter'].nunique()\ntrain['number_unique_meter_per_building'] = train['building_id'].map(number_unique_meter_per_building)\n\n\nmean_meter_reading_per_building = train.groupby('building_id')['meter_reading'].mean()\ntrain['mean_meter_reading_per_building'] = train['building_id'].map(mean_meter_reading_per_building)\nmedian_meter_reading_per_building = train.groupby('building_id')['meter_reading'].median()\ntrain['median_meter_reading_per_building'] = train['building_id'].map(median_meter_reading_per_building)\nstd_meter_reading_per_building = train.groupby('building_id')['meter_reading'].std()\ntrain['std_meter_reading_per_building'] = train['building_id'].map(std_meter_reading_per_building)\n\n\nmean_meter_reading_on_year_built = train.groupby('year_built')['meter_reading'].mean()\ntrain['mean_meter_reading_on_year_built'] = train['year_built'].map(mean_meter_reading_on_year_built)\nmedian_meter_reading_on_year_built = train.groupby('year_built')['meter_reading'].median()\ntrain['median_meter_reading_on_year_built'] = train['year_built'].map(median_meter_reading_on_year_built)\nstd_meter_reading_on_year_built = train.groupby('year_built')['meter_reading'].std()\ntrain['std_meter_reading_on_year_built'] = train['year_built'].map(std_meter_reading_on_year_built)\n\n\nmean_meter_reading_per_meter = train.groupby('meter')['meter_reading'].mean()\ntrain['mean_meter_reading_per_meter'] = train['meter'].map(mean_meter_reading_per_meter)\nmedian_meter_reading_per_meter = train.groupby('meter')['meter_reading'].median()\ntrain['median_meter_reading_per_meter'] = train['meter'].map(median_meter_reading_per_meter)\nstd_meter_reading_per_meter = train.groupby('meter')['meter_reading'].std()\ntrain['std_meter_reading_per_meter'] = train['meter'].map(std_meter_reading_per_meter)\n\n\nmean_meter_reading_per_primary_usage = train.groupby('primary_use')['meter_reading'].mean()\ntrain['mean_meter_reading_per_primary_usage'] = train['primary_use'].map(mean_meter_reading_per_primary_usage)\nmedian_meter_reading_per_primary_usage = train.groupby('primary_use')['meter_reading'].median()\ntrain['median_meter_reading_per_primary_usage'] = train['primary_use'].map(median_meter_reading_per_primary_usage)\nstd_meter_reading_per_primary_usage = train.groupby('primary_use')['meter_reading'].std()\ntrain['std_meter_reading_per_primary_usage'] = train['primary_use'].map(std_meter_reading_per_primary_usage)\n\n\nmean_meter_reading_per_site_id = train.groupby('site_id')['meter_reading'].mean()\ntrain['mean_meter_reading_per_site_id'] = train['site_id'].map(mean_meter_reading_per_site_id)\nmedian_meter_reading_per_site_id = train.groupby('site_id')['meter_reading'].median()\ntrain['median_meter_reading_per_site_id'] = train['site_id'].map(median_meter_reading_per_site_id)\nstd_meter_reading_per_site_id = train.groupby('site_id')['meter_reading'].std()\ntrain['std_meter_reading_per_site_id'] = train['site_id'].map(std_meter_reading_per_site_id)\n\n\ntest['number_unique_meter_per_building'] = test['building_id'].map(number_unique_meter_per_building)\n\ntest['mean_meter_reading_per_building'] = test['building_id'].map(mean_meter_reading_per_building)\ntest['median_meter_reading_per_building'] = test['building_id'].map(median_meter_reading_per_building)\ntest['std_meter_reading_per_building'] = test['building_id'].map(std_meter_reading_per_building)\n\ntest['mean_meter_reading_on_year_built'] = test['year_built'].map(mean_meter_reading_on_year_built)\ntest['median_meter_reading_on_year_built'] = test['year_built'].map(median_meter_reading_on_year_built)\ntest['std_meter_reading_on_year_built'] = test['year_built'].map(std_meter_reading_on_year_built)\n\ntest['mean_meter_reading_per_meter'] = test['meter'].map(mean_meter_reading_per_meter)\ntest['median_meter_reading_per_meter'] = test['meter'].map(median_meter_reading_per_meter)\ntest['std_meter_reading_per_meter'] = test['meter'].map(std_meter_reading_per_meter)\n\ntest['mean_meter_reading_per_primary_usage'] = test['primary_use'].map(mean_meter_reading_per_primary_usage)\ntest['median_meter_reading_per_primary_usage'] = test['primary_use'].map(median_meter_reading_per_primary_usage)\ntest['std_meter_reading_per_primary_usage'] = test['primary_use'].map(std_meter_reading_per_primary_usage)\n\ntest['mean_meter_reading_per_site_id'] = test['site_id'].map(mean_meter_reading_per_site_id)\ntest['median_meter_reading_per_site_id'] = test['site_id'].map(median_meter_reading_per_site_id)\ntest['std_meter_reading_per_site_id'] = test['site_id'].map(std_meter_reading_per_site_id)","1ee96a0f":"%%time\nfor df in [train, test]:\n    df['mean_meter_reading_per_building'] = df['mean_meter_reading_per_building'].astype(\"float16\")\n    df['median_meter_reading_per_building'] = df['mean_meter_reading_per_building'].astype(\"float16\")\n    df['std_meter_reading_per_building'] = df['std_meter_reading_per_building'].astype(\"float16\")\n    \n    df['mean_meter_reading_on_year_built'] = df['mean_meter_reading_on_year_built'].astype(\"float16\")\n    df['median_meter_reading_on_year_built'] = df['median_meter_reading_on_year_built'].astype(\"float16\")\n    df['std_meter_reading_on_year_built'] = df['std_meter_reading_on_year_built'].astype(\"float16\")\n    \n    df['mean_meter_reading_per_meter'] = df['mean_meter_reading_per_meter'].astype(\"float16\")\n    df['median_meter_reading_per_meter'] = df['median_meter_reading_per_meter'].astype(\"float16\")\n    df['std_meter_reading_per_meter'] = df['std_meter_reading_per_meter'].astype(\"float16\")\n    \n    df['mean_meter_reading_per_primary_usage'] = df['mean_meter_reading_per_primary_usage'].astype(\"float16\")\n    df['median_meter_reading_per_primary_usage'] = df['median_meter_reading_per_primary_usage'].astype(\"float16\")\n    df['std_meter_reading_per_primary_usage'] = df['std_meter_reading_per_primary_usage'].astype(\"float16\")\n    \n    df['mean_meter_reading_per_site_id'] = df['mean_meter_reading_per_site_id'].astype(\"float16\")\n    df['median_meter_reading_per_site_id'] = df['median_meter_reading_per_site_id'].astype(\"float16\")\n    df['std_meter_reading_per_site_id'] = df['std_meter_reading_per_site_id'].astype(\"float16\")\n    \n    df['number_unique_meter_per_building'] = df['number_unique_meter_per_building'].astype('uint8')\ngc.collect()","c38b17c1":"train.drop('timestamp',axis=1,inplace=True)\ntest.drop('timestamp',axis=1,inplace=True)","63cf84a8":"le = LabelEncoder()\ntrain['primary_use'] = le.fit_transform(train['primary_use']).astype(\"uint8\")\ntest['primary_use'] = le.fit_transform(test['primary_use']).astype(\"uint8\")","55f90667":"%%time\n# Let's check the correlation between the variables and eliminate the one's that have high correlation\n# Threshold for removing correlated variables\nthreshold = 0.9\n\n# Absolute value correlation matrix\ncorr_matrix = train.corr().abs()\n# Upper triangle of correlations\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Select columns with correlations above threshold\nto_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n\nprint('There are %d columns to remove.' % (len(to_drop)))\nprint (\"Following columns can be dropped {}\".format(to_drop))\n\ntrain.drop(to_drop,axis=1,inplace=True)\ntest.drop(to_drop,axis=1,inplace=True)","e5945766":"train.head()","7f169b28":"test.head()","437b8e1a":"y = train['meter_reading']\ntrain.drop('meter_reading',axis=1,inplace=True)\ncategorical_cols = ['building_id','Month','meter','Hour','primary_use','DayOfWeek','DayOfMonth']","7a1f7313":"x_train,x_test,y_train,y_test = train_test_split(train,y,test_size=0.2,random_state=42)\nprint (x_train.shape)\nprint (y_train.shape)\nprint (x_test.shape)\nprint (y_test.shape)","dee9b227":"test.shape","bdd98792":"lgb_train = lgb.Dataset(x_train, y_train, categorical_feature=categorical_cols)\nlgb_test = lgb.Dataset(x_test, y_test, categorical_feature=categorical_cols)\ndel x_train, x_test , y_train, y_test\n\nparams = {'feature_fraction': 0.75,\n          'bagging_fraction': 0.75,\n          'objective': 'regression',\n          'max_depth': -1,\n          'learning_rate': 0.15,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'rmse',\n          \"verbosity\": -1,\n          'reg_alpha': 0.5,\n          'reg_lambda': 0.5,\n          'random_state': 47\n         }\n\nreg = lgb.train(params, lgb_train, num_boost_round=3000, valid_sets=[lgb_train, lgb_test], early_stopping_rounds=100, verbose_eval = 100)","3af265f1":"del lgb_train,lgb_test\nser = pd.DataFrame(reg.feature_importance(),train.columns,columns=['Importance']).sort_values(by='Importance')\nser['Importance'].plot(kind='bar',figsize=(10,6))","94a5e919":"del train","09e639b4":"test.shape","07f6b7d4":"%%time\npredictions = []\nstep = 50000\nfor i in range(0, len(test), step):\n    predictions.extend(np.expm1(reg.predict(test.iloc[i: min(i+step, len(test)), :], num_iteration=reg.best_iteration)))","1c8247c9":"%%time\nSubmission = pd.DataFrame(test.index,columns=['row_id'])\nSubmission['meter_reading'] = predictions\nSubmission['meter_reading'].clip(lower=0,upper=None,inplace=True)\nSubmission.to_csv(\"lgbm.csv\",index=None)","4c322443":"#use onehotencoding instead of categorical_feature ? https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html","44499fbc":"What to ADD : ","c1751430":"References:\n* https:\/\/www.kaggle.com\/kulkarnivishwanath\/ashrae-great-energy-predictor-iii-eda-model\n* https:\/\/www.kaggle.com\/caesarlupum\/ashrae-start-here-a-gentle-introduction"}}