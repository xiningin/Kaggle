{"cell_type":{"7de33089":"code","4d6095a1":"code","1a6fc694":"code","bea52585":"code","c8768cc7":"code","1ce46848":"code","67d680c7":"code","2d2ca9c5":"code","75f6a027":"code","49c06975":"code","3061a149":"code","c5f27c5a":"code","27621bb6":"code","8da09b5a":"code","86e07ec2":"code","f293ecb2":"code","8eae3bc1":"code","18e6198a":"code","ae2c68e4":"code","c6ca74d4":"code","6a5a888c":"code","c4b0ba59":"code","fd5a5eec":"code","e753eb1e":"code","b41358d3":"code","bc5a8363":"code","c5d8b16f":"code","61d366ac":"markdown"},"source":{"7de33089":"import tensorflow as tf\ntf.enable_eager_execution()\n\nimport glob\nimport imageio\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport PIL\nimport time\nfrom scipy import misc\nfrom IPython import display","4d6095a1":"from keras import backend as K","1a6fc694":"config = tf.ConfigProto()\n\n# Don't pre-allocate memory; allocate as-needed\nconfig.gpu_options.allow_growth = True\n\n# Only allow a total of half the GPU memory to be allocated\nconfig.gpu_options.per_process_gpu_memory_fraction =1\n\n# Create a session with the above options specified.\nK.tensorflow_backend.set_session(tf.Session(config=config))","bea52585":"from tensorflow.python.client import device_lib\ndevice_lib.list_local_devices()","c8768cc7":"import tensorflow as tf\ntf.enable_eager_execution()\ntf.__version__","1ce46848":"#AUTOTUNE = tf.data.experimental.AUTOTUNE","67d680c7":"#tf.test.gpu_device_name()\ntf.Session(config=tf.ConfigProto(log_device_placement=True))\n","2d2ca9c5":"import pathlib\n#data_root = tf.keras.utils.get_file('shoes','full_images_dest', untar=True)\ndata_root = pathlib.Path('..\/full_images_dest')\nprint(data_root)","75f6a027":"import random\nall_image_paths = [str(item) for item in data_root.iterdir()]\nrandom.shuffle(all_image_paths)\n\nimage_count = len(all_image_paths)\n","49c06975":"from scipy import ndimage\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n\ntrain_files = []\n#y_train = []\ni=0\nfor _file in all_image_paths:\n    train_files.append(_file)\n    #label_in_file = _file.find(\"_\")\n    #y_train.append(int(_file[0:label_in_file]))\n    \nprint(\"Files in train_files: %d\" % len(train_files))\n\n\nchannels = 3\n#nb_classes = 1\n\ndataset = np.ndarray(shape=(len(train_files), 112, 112,channels),\n                     dtype=np.float32)\n\ni = 0\nfor _file in train_files:\n    #img = load_img( _file)  # this is a PIL image\n    #img.thumbnail((image_width, image_height))\n    # Convert to Numpy Array \n    img = PIL.Image.open(_file)\n    x = img.resize((112,112), PIL.Image.ANTIALIAS)\n    x = img_to_array(x)  \n    #x = x.reshape((x.shape[0],x.shape[1],3))\n    # Normalize\n    x = (x - 128.0) \/ 128.0\n    dataset[i] = x\n    i += 1\n    if i % 5000 == 0:\n        print(\"%d images to array\" % i)\nprint(\"All images to array!\")","3061a149":"BUFFER_SIZE = 50025\nBATCH_SIZE = 64","c5f27c5a":"train_dataset = tf.data.Dataset.from_tensor_slices(dataset).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","27621bb6":"def make_generator_model():\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Dense(7*7*512, use_bias=False, input_shape=(100,)))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU())\n      \n    model.add(tf.keras.layers.Reshape((7, 7, 512)))\n    assert model.output_shape == (None, 7, 7, 512) # Note: None is the batch size\n    \n    model.add(tf.keras.layers.Conv2DTranspose(256, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n    assert model.output_shape == (None, 7, 7, 256)  \n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU())\n\n    model.add(tf.keras.layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    assert model.output_shape == (None, 14, 14, 128)    \n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU())\n    \n    model.add(tf.keras.layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    assert model.output_shape == (None, 28, 28, 64)    \n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU())\n    \n    model.add(tf.keras.layers.Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    assert model.output_shape == (None, 56, 56, 32)    \n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.LeakyReLU())\n    \n\n    model.add(tf.keras.layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n    assert model.output_shape == (None, 112,112,3)\n  \n    return model","8da09b5a":"def make_discriminator_model():\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'))\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Dropout(0.3))\n    \n    model.add(tf.keras.layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Dropout(0.3))\n    \n    model.add(tf.keras.layers.Conv2D(512, (5, 5), strides=(2, 2), padding='same'))\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Dropout(0.3))\n    \n    model.add(tf.keras.layers.Conv2D(256, (5, 5), strides=(2, 2), padding='same'))\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Dropout(0.3))\n    \n    model.add(tf.keras.layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    model.add(tf.keras.layers.LeakyReLU())\n    model.add(tf.keras.layers.Dropout(0.3))\n       \n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(1))\n     \n    return model","86e07ec2":"generator = make_generator_model()\ndiscriminator = make_discriminator_model()","f293ecb2":"def generator_loss(generated_output):\n    return tf.reduce_mean(tf.losses.sigmoid_cross_entropy(tf.ones_like(generated_output), generated_output))","8eae3bc1":"train_vars = tf.trainable_variables()","18e6198a":"def discriminator_loss(real_output, generated_output):\n    # [1,1,...,1] with real output since it is true and we want our generated examples to look like it\n    real_loss = tf.reduce_mean(tf.losses.sigmoid_cross_entropy(multi_class_labels=tf.ones_like(real_output), logits=real_output))\n\n    # [0,0,...,0] with generated images since they are fake\n    generated_loss = tf.reduce_mean(tf.losses.sigmoid_cross_entropy(multi_class_labels=tf.zeros_like(generated_output), logits=generated_output))\n\n    total_loss = real_loss + generated_loss\n\n    return total_loss","ae2c68e4":"generator_optimizer = tf.train.AdamOptimizer(1e-4)\ndiscriminator_optimizer = tf.train.AdamOptimizer(1e-4)","c6ca74d4":"checkpoint_dir = '.\/training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)","6a5a888c":"EPOCHS = 1000\nnoise_dim = 100\nnum_examples_to_generate =16 \n\n# We'll re-use this random vector used to seed the generator so\n# it will be easier to see the improvement over time.\nrandom_vector_for_generation = tf.random_normal([num_examples_to_generate,\n                                                 noise_dim])\n","c4b0ba59":"def train_step(images):\n   # generating noise from a normal distribution\n    noise = tf.random_normal([BATCH_SIZE, noise_dim])\n\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        generated_images = generator(noise, training=True)\n\n        real_output = discriminator(images, training=True)\n        generated_output = discriminator(generated_images, training=True)\n\n        gen_loss = generator_loss(generated_output)\n        disc_loss = discriminator_loss(real_output, generated_output)\n\n        gradients_of_generator = gen_tape.gradient(gen_loss, generator.variables)\n        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.variables)\n\n        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.variables))\n        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.variables))","fd5a5eec":"def train(dataset, epochs):  \n    for epoch in range(epochs):\n        start = time.time()\n        for images in dataset:\n            train_step(images) \n        display.clear_output(wait=True)\n        generate_and_save_images(generator,\n                           epoch + 401,\n                           random_vector_for_generation)\n\n        if((epoch+1)%10==0):\n            checkpoint.save(file_prefix = checkpoint_prefix)\n\n        print ('Time taken for epoch {} is {} sec'.format(epoch + 401,\n                                                  time.time()-start))\n    # generating after the final epoch\n    display.clear_output(wait=True)\n    generate_and_save_images(generator,\n                       epochs,random_vector_for_generation)","e753eb1e":"def generate_and_save_images(model, epoch, test_input):\n  # make sure the training parameter is set to False because we\n  # don't want to train the batchnorm layer when doing inference.\n    predictions = model(test_input, training=False)\n    predictions=predictions.numpy()\n    #print(predictions[0].shape)\n    fig = plt.figure(figsize=(12,12))\n    for i in range(predictions.shape[0]):\n        plt.subplot(4, 4, i+1)\n        plt.imshow((predictions[i, :, :,:] * 127.5 + 127.5).astype(np.uint8))\n        plt.axis('off')\n    if((epoch)%10 ==0):\n        plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n    plt.show()","b41358d3":"checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))","bc5a8363":"%%time\ntrain(train_dataset, EPOCHS)","c5d8b16f":"generate_and_save_images(generator,50,random_vector_for_generation)","61d366ac":"This is my first kaggle kernal and my first trial .\nThis is a basic GAN implementation tool ..Dataset has not been attached due to its huge size. "}}