{"cell_type":{"1c8bbdc8":"code","01f844ac":"code","064a7e0e":"code","6d124f18":"code","81c44dfc":"code","74edb1cd":"code","308e365d":"code","f16986c3":"code","751746a5":"code","80491765":"code","a9f4ef4e":"code","284db7ec":"code","72dbfb65":"code","3a8b30c4":"code","572b820a":"code","4f5082dc":"code","1927a7d8":"code","e88b909e":"code","65b4c041":"code","94a7ca31":"code","fae1ccc2":"code","b188ee96":"code","83d812ad":"code","c1ea0020":"code","360cd23f":"code","afa39732":"markdown","87727039":"markdown","8b678612":"markdown","c88f7a38":"markdown","f066a38d":"markdown","e0515086":"markdown","55e6995b":"markdown","a93c3c97":"markdown","a049b4da":"markdown","07771795":"markdown","206e43a9":"markdown","3eabec67":"markdown","b2a8135a":"markdown"},"source":{"1c8bbdc8":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nimport torch.nn.functional as F\nfrom transformers import RobertaTokenizer, RobertaConfig,AdamW, RobertaForSequenceClassification,get_linear_schedule_with_warmup\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,classification_report\n# Import and evaluate each test batch using Matthew's correlation coefficient\nfrom sklearn.metrics import accuracy_score,matthews_corrcoef\n\nfrom tqdm import tqdm, trange,tnrange,tqdm_notebook\nimport random\nimport os\nimport io\n% matplotlib inline","01f844ac":"# identify and specify the GPU as the device, later in training loop we will load data into device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)\n\nSEED = 19\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == torch.device(\"cuda\"):\n    torch.cuda.manual_seed_all(SEED)","064a7e0e":"df_train = pd.read_csv(\"\/kaggle\/input\/twitter-sentiment-dataset\/Twitter_Data.csv\")","6d124f18":"df_train.isnull().sum()","81c44dfc":"df_train.head()","74edb1cd":"df_train['category'].unique()","308e365d":"df_train['category'].value_counts()","f16986c3":"df_train = df_train[~df_train['category'].isnull()]","751746a5":"df_train = df_train[~df_train['clean_text'].isnull()]","80491765":"from sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ndf_train['category_1'] = labelencoder.fit_transform(df_train['category'])","a9f4ef4e":"df_train[['category','category_1']].drop_duplicates(keep='first')","284db7ec":"df_train.rename(columns={'category_1':'label'},inplace=True)","72dbfb65":"## create label and sentence list\nsentences = df_train.clean_text.values\n\n#check distribution of data based on labels\nprint(\"Distribution of data based on labels: \",df_train.label.value_counts())\n\n# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n# In the original paper, the authors used a length of 512.\nMAX_LEN = 256\n\n## Import ROBERTA tokenizer, that is used to convert our text into tokens that corresponds to ROBERTA library\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base',do_lower_case=True)","3a8b30c4":"input_ids = [tokenizer.encode(sent, add_special_tokens=True,\n                              max_length=MAX_LEN,\n                              pad_to_max_length=True,truncation=True) for sent in sentences]","572b820a":"labels = df_train.label.values\n\nprint(\"Actual sentence before tokenization: \",sentences[2])\nprint(\"Encoded Input from dataset: \",input_ids[2])\n\n## Create attention mask\nattention_masks = []\n## Create a mask of 1 for all input tokens and 0 for all padding tokens\nattention_masks = [[float(i>0) for i in seq] for seq in input_ids]\nprint(attention_masks[2])","4f5082dc":"train_inputs,validation_inputs,train_labels,validation_labels = train_test_split(input_ids,labels,random_state=41,test_size=0.1)\ntrain_masks,validation_masks,_,_ = train_test_split(attention_masks,input_ids,random_state=41,test_size=0.1)","1927a7d8":"# convert all our data into torch tensors, required data type for our model\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)\n\n# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\nbatch_size = 32\n\n# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n# with an iterator the entire dataset does not need to be loaded into memory\ntrain_data = TensorDataset(train_inputs,train_masks,train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs,validation_masks,validation_labels)\nvalidation_sampler = RandomSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data,sampler=validation_sampler,batch_size=batch_size)","e88b909e":"train_data[0]","65b4c041":"# Load RobertaForSequenceClassification, the pretrained ROBERTA model with a single linear classification layer on top.num_lables=3 because we have 3 class \nmodel = RobertaForSequenceClassification.from_pretrained('roberta-base',num_labels=3).to(device)\n\n# Parameters:\nlr = 2e-5\nadam_epsilon = 1e-8\n\n# Number of training epochs (authors recommend between 2 and 4)\nepochs = 3\n\nnum_warmup_steps = 0\nnum_training_steps = len(train_dataloader)*epochs\n\n### In Transformers, optimizer and schedules are splitted and instantiated like this:\noptimizer = AdamW(model.parameters(), lr=lr,eps=adam_epsilon,correct_bias=False)  # To reproduce AdamW specific behavior set correct_bias=False\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler","94a7ca31":"## Store our loss and accuracy for plotting\ntrain_loss_set = []\nlearning_rate = []\n\n# Gradients gets accumulated by default\nmodel.zero_grad()\n\n# tnrange is a tqdm wrapper around the normal python range\nfor _ in tnrange(1,epochs+1,desc='Epoch'):\n  print(\"<\" + \"=\"*22 + F\" Epoch {_} \"+ \"=\"*22 + \">\")\n  # Calculate total loss for this epoch\n  batch_loss = 0\n\n  for step, batch in enumerate(train_dataloader):\n    # Set our model to training mode (as opposed to evaluation mode)\n    model.train()\n    \n    # Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n\n    # Forward pass\n    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n    loss = outputs[0]\n    \n    # Backward pass\n    loss.backward()\n    \n    # Clip the norm of the gradients to 1.0\n    # Gradient clipping is not in AdamW anymore\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n    \n    # Update parameters and take a step using the computed gradient\n    optimizer.step()\n    \n    # Update learning rate schedule\n    scheduler.step()\n\n    # Clear the previous accumulated gradients\n    optimizer.zero_grad()\n    \n    # Update tracking variables\n    batch_loss += loss.item()\n\n  # Calculate the average loss over the training data.\n  avg_train_loss = batch_loss \/ len(train_dataloader)\n\n  #store the current learning rate\n  for param_group in optimizer.param_groups:\n    print(\"\\n\\tCurrent Learning rate: \",param_group['lr'])\n    learning_rate.append(param_group['lr'])\n    \n  train_loss_set.append(avg_train_loss)\n  print(F'\\n\\tAverage Training loss: {avg_train_loss}')\n    \n  # Validation\n\n  # Put model in evaluation mode to evaluate loss on the validation set\n  model.eval()\n\n  # Tracking variables \n  eval_accuracy,eval_mcc_accuracy,nb_eval_steps = 0, 0, 0\n\n  # Evaluate data for one epoch\n  for batch in validation_dataloader:\n    # Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n    with torch.no_grad():\n      # Forward pass, calculate logit predictions\n      logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n    \n    # Move logits and labels to CPU\n    logits = logits[0].to('cpu').numpy()\n    label_ids = b_labels.to('cpu').numpy()\n\n    pred_flat = np.argmax(logits, axis=1).flatten()\n    labels_flat = label_ids.flatten()\n    \n    df_metrics=pd.DataFrame({'Epoch':epochs,'Actual_class':labels_flat,'Predicted_class':pred_flat})\n    \n    tmp_eval_accuracy = accuracy_score(labels_flat,pred_flat)\n    tmp_eval_mcc_accuracy = matthews_corrcoef(labels_flat, pred_flat)\n    \n    eval_accuracy += tmp_eval_accuracy\n    eval_mcc_accuracy += tmp_eval_mcc_accuracy\n    nb_eval_steps += 1\n\n  print(F'\\n\\tValidation Accuracy: {eval_accuracy\/nb_eval_steps}')\n  print(F'\\n\\tValidation MCC Accuracy: {eval_mcc_accuracy\/nb_eval_steps}')","fae1ccc2":"## emotion labels\nlabel2int = {\n  \"Negative\": 0,\n  \"Neutral\": 1,\n  \"Positive\": 2\n}","b188ee96":"print(classification_report(df_metrics['Actual_class'].values, df_metrics['Predicted_class'].values, target_names=label2int.keys(), digits=len(label2int)))","83d812ad":"from sklearn.metrics import confusion_matrix,classification_report\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    import itertools\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","c1ea0020":"confusion_matrix(df_metrics['Actual_class'].values, df_metrics['Predicted_class'].values)","360cd23f":"plot_confusion_matrix(cm=confusion_matrix(df_metrics['Actual_class'].values, df_metrics['Predicted_class'].values), \n                      classes=[0  ,1  ,2],\n                          normalize=True,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues)","afa39732":"# ROBERTA - Twitter Sentiment Classifier","87727039":"# **Data Preperation For Roberta Model**","8b678612":"# Kindly upvote if you like it","c88f7a38":"# **Target Distribution**","f066a38d":"# **Checking Null Value**","e0515086":"# **Removing Null value**","55e6995b":"# **Lets see how the training data looks like**","a93c3c97":"# **Identify and specify the GPU as the device, later in training loop we will load data into device**","a049b4da":"# **Train Loop**","07771795":"# **Target Encoding**","206e43a9":"# **Load RobertaForSequenceClassification, the pretrained ROBERTA model with a single linear classification layer on top**","3eabec67":"# **Conclusion**:\nHere we can see the model didn't perform well in case of negative tweet.","b2a8135a":"# **Read File**"}}