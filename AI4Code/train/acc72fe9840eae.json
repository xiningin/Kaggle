{"cell_type":{"629f7985":"code","1b46cdd5":"code","b3ccd180":"code","be469249":"code","489eab73":"code","58203986":"code","175705ef":"code","766f2e1f":"code","d13bdfa1":"code","9abe73f5":"code","ada63c78":"code","e33e3099":"code","f0cf3ebd":"code","98119544":"code","3dfc94e0":"code","9c198f61":"code","fdd329ec":"code","475c34c6":"code","f4f4910e":"markdown","8227c501":"markdown","10e4ec9e":"markdown","6ea1ef81":"markdown","5f268494":"markdown","9380985b":"markdown","c4b85d75":"markdown","544abbe3":"markdown","019f83d9":"markdown","a65e3612":"markdown","ff6ea197":"markdown","e57d3d80":"markdown"},"source":{"629f7985":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1b46cdd5":"import tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom sklearn.model_selection import train_test_split\nimport numpy as np","b3ccd180":"df = pd.read_csv('..\/input\/Sentiment.csv')","be469249":"df.head(5)","489eab73":"df.sentiment.value_counts()","58203986":"df.sentiment_confidence.plot(kind='hist')","175705ef":"df = df[df['sentiment_confidence'] > 0.5]\n\ndf_pos = df[df['sentiment'] == 'Positive'].sample(frac=1)\ndf_neg = df[df['sentiment'] == 'Negative'].sample(frac=1)\ndf_neu = df[df['sentiment'] == 'Neutral'].sample(frac=1)\n\nsample_size = min(len(df_pos), len(df_neg), len(df_neu))\n\ndf_ = pd.concat([df_pos.head(sample_size), df_neg.head(sample_size), df_neu.head(sample_size)])[['text', 'sentiment']]","766f2e1f":"del df\nsample_size","d13bdfa1":"import re\n\ndef clean_str(string):\n    \"\"\"\n    Tokenization\/string cleaning for datasets.\n    Original taken from https:\/\/github.com\/yoonkim\/CNN_sentence\/blob\/master\/process_data.py\n    \"\"\"\n    string = str(string)\n    # remove the retweet part - maybe this should just be removed\n    if string[:4] == 'RT @':\n        tmp = string.find(':')\n        string = string[tmp + 2:]\n    string = re.sub(r\"(https?:\\\/\\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\\/\\\/(?:www\\.|(?!www))[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]\\.[^\\s]{2,})\", \"url\", string)\n    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n    string = re.sub(r\"\\'s\", \" \\'s\", string)\n    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n    string = re.sub(r\"\\'re\", \" \\'re\", string)\n    string = re.sub(r\"\\'d\", \" \\'d\", string)\n    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n    string = re.sub(r\",\", \" , \", string)\n    string = re.sub(r\"!\", \" ! \", string)\n    string = re.sub(r\"\\(\", \" \\( \", string)\n    string = re.sub(r\"\\)\", \" \\) \", string)\n    string = re.sub(r\"\\?\", \" \\? \", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n    return string.strip().lower()","9abe73f5":"df_['clean'] = df_['text'].apply(clean_str)","ada63c78":"df_.head(3)","e33e3099":"text_embeddings  = hub.text_embedding_column(\n  \"clean\", \n  module_spec=\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/2\"\n)","f0cf3ebd":"X_train, X_test, y_train, y_test = train_test_split(df_['clean'], df_['sentiment'], test_size=0.3, random_state=42)","98119544":"multi_class_head  = tf.contrib.estimator.multi_class_head(\n    n_classes=3,\n    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,\n    label_vocabulary=['Positive', 'Neutral', 'Negative']\n)","3dfc94e0":"estimator  = tf.contrib.estimator.DNNEstimator(\n    head=multi_class_head,\n    hidden_units=[256, 128, 64],\n    feature_columns=[text_embeddings],\n    optimizer=tf.train.AdamOptimizer()\n)","9c198f61":"features = {\n  \"clean\": np.array(X_train)\n}\nlabels = np.array(y_train)\n\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\n    features, \n    labels, \n    shuffle=True, \n    batch_size=32, \n    num_epochs=20\n)","fdd329ec":"estimator.train(input_fn=train_input_fn)","475c34c6":"eval_input_fn  = tf.estimator.inputs.numpy_input_fn({\"clean\": np.array(X_test).astype(np.str)}, np.array(y_test), shuffle=False)\n\nestimator.evaluate(input_fn=eval_input_fn)","f4f4910e":"Then let's train the thing!","8227c501":"So this is what our final dataset looks like","10e4ec9e":"So in this notebook we're going to try and classify the sentiment of tweets using tf hub","6ea1ef81":"tf hub has this great set of text embeddings, I'm going to use the universal sentence encoder here - this step can take a while to run because it has to download the embeddings from tf hub (note if you're in a kaggle notebook you have to auth internet access)","5f268494":"Then we have to specify our dictionary for tf","9380985b":"Split the data into a training and testing set","c4b85d75":"I'm going to (pretty much) arbitrarily remove that lower intival chunk of data because the confidence level is so low\n\nThen take an even split of the 3 classifications","544abbe3":"The nice thing about this dataset is includes the sentiment classification confidence","019f83d9":"At this point we get to pick the architecture of our NN.\n\nAlong with passing our head, we also get to specify how many hidden units we have an the optimizer (amoung other things if you are so inclined)\n\nThe general rule of thumb for fully connected networks is you should go down in size as the layers progress - but play about, you could get better results trying something else","a65e3612":"As you can see from above, this data needs a bit of cleaning - I'm going to borrow some code and adapt it to clean the data","ff6ea197":"Then we need to create a `head`. This is what the estimator will train against.\n\nHere, our data is one of 3 classes: Positive, Negative or Neutral. If it could be a mix of more than one class we would use a `multi_label_head`, but since we can only be one class then we use the `muti_class_head`\n\nNote the `label_vocabulary` here - this tells the head that these are the classes used. If you don't specify this then you get all sorts of cryptic failures when you try to train the estimator","e57d3d80":"We can then evaluate how good this model is with `estimator.evaluate`"}}