{"cell_type":{"8605f80a":"code","bb2de7a4":"code","7e4de0d5":"code","9860a492":"code","97827857":"code","c708236c":"code","371aafd6":"code","503ef23d":"code","1ef5ff0c":"code","7d5423ec":"code","d0f2d471":"code","dc18770f":"code","9a90c715":"code","9b9a0031":"code","dc4ef112":"code","65f9d44d":"code","441a90a5":"code","029abc89":"code","b1c2eeb1":"code","2b243692":"code","3e894232":"code","30ff5e69":"code","8d22f149":"code","29c84c74":"code","4fe22da5":"code","b3ea7754":"code","5c76a86d":"code","78f193c4":"code","9b98ae4a":"code","c45e4b3d":"code","9aa8d65b":"code","b1fa822e":"code","4fe8c97e":"code","9d2531f5":"code","fd86b5cc":"code","92062aab":"code","401c0f17":"markdown","13133df3":"markdown","2a3ecd1b":"markdown","b90cdfa8":"markdown","cc5f1b92":"markdown","324670b9":"markdown","070ddf42":"markdown","4f623c62":"markdown","b0514e8e":"markdown","1e32eee7":"markdown","56783a7c":"markdown","da568895":"markdown","56e9b777":"markdown","36e3c9b8":"markdown","d54355db":"markdown","8a9bfc4f":"markdown","c05995f5":"markdown","2081c490":"markdown"},"source":{"8605f80a":"MAX_SAMPLE = None # set a small number for experimentation, set None for production.","bb2de7a4":"!pip install datasets --no-index --find-links=file:\/\/\/kaggle\/input\/coleridge-packages\/packages\/datasets\n!pip install ..\/input\/coleridge-packages\/seqeval-1.2.2-py3-none-any.whl\n!pip install ..\/input\/coleridge-packages\/tokenizers-0.10.1-cp37-cp37m-manylinux1_x86_64.whl\n!pip install ..\/input\/coleridge-packages\/transformers-4.5.0.dev0-py3-none-any.whl","7e4de0d5":"import os\nimport re\nimport json\nimport time\nimport datetime\nimport random\nimport glob\nimport importlib\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy.spatial import distance\n\nrandom.seed(123)\nnp.random.seed(456)","9860a492":"train_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train.csv'\ntrain = pd.read_csv(train_path)\ntrain = train[:MAX_SAMPLE]\n\npaper_train_folder = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\npapers = {}\nfor paper_id in train['Id'].unique():\n    with open(f'{paper_train_folder}\/{paper_id}.json', 'r') as f:\n        paper = json.load(f)\n        papers[paper_id] = paper\n        \nprint(\"Unique Paper Ids: \", str(len(train['Id'].unique())))","97827857":"sample_submission_path = '..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv'\nsample_submission = pd.read_csv(sample_submission_path)\n\npaper_test_folder = '..\/input\/coleridgeinitiative-show-us-the-data\/test'\nfor paper_id in sample_submission['Id']:\n    with open(f'{paper_test_folder}\/{paper_id}.json', 'r') as f:\n        paper = json.load(f)\n        papers[paper_id] = paper","c708236c":"train.head()","371aafd6":"#train.index[train['Id']=='c754dec7-c5a3-4337-9892-c02158475064'].tolist()[0]\npapers['c5d5cd2c-59de-4f29-bbb1-6a88c7b52f29']","503ef23d":"train.info()","1ef5ff0c":"# finding unique values in each column\n[print(f\"{col}:{len(train[col].unique())}\") for col in train.columns]","7d5423ec":"from wordcloud import WordCloud, STOPWORDS\nfrom nltk.probability import FreqDist\n\nwords = list(train['cleaned_label'].values)\nstopwords=['ourselves', 'hers','the','of','and','in', 'between', 'yourself', 'but', 'again','of', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\nsplit_words=[]\nfor word in words:\n    lo_w=[]\n    list_of_words=str(word).split()\n    for w in list_of_words:\n        if w not in stopwords:\n            lo_w.append(w)\n    split_words.append(lo_w)\nallwords = []\nfor wordlist in split_words:\n    allwords += wordlist","d0f2d471":"mostcommon = FreqDist(allwords).most_common(100)\nwordcloud = WordCloud(width=1600, height=800, background_color='white', stopwords=STOPWORDS).generate(str(mostcommon))\nfig = plt.figure(figsize=(30,10), facecolor='white')\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.title('Top 100 Most Common Words in cleaned_label', fontsize=50)\nplt.tight_layout(pad=0)\nplt.show()\n\nmostcommon_small = FreqDist(allwords).most_common(25)\nx, y = zip(*mostcommon_small)\nplt.figure(figsize=(50,30))\nplt.margins(0.02)\nplt.bar(x, y)\nplt.xlabel('Words', fontsize=50)\nplt.ylabel('Frequency of Words', fontsize=50)\nplt.yticks(fontsize=40)\nplt.xticks(rotation=60, fontsize=40)\nplt.tight_layout(pad=0)\nplt.title('Freq of 25 Most Common Words in cleaned_label', fontsize=60)\nplt.show()","dc18770f":"all_labels = set()\n\nfor label_1, label_2, label_3 in train[['dataset_title', 'dataset_label', 'cleaned_label']].itertuples(index=False):\n    all_labels.add(str(label_1).lower())\n    all_labels.add(str(label_2).lower())\n    all_labels.add(str(label_3).lower())\n    \nprint(f'No. different labels: {len(all_labels)}')","9a90c715":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n\ndef totally_clean_text(txt):\n    txt = clean_text(txt)\n    txt = re.sub(' +', ' ', txt)\n    return txt","9b9a0031":"literal_preds = []\n\nfor paper_id in sample_submission['Id']:\n    paper = papers[paper_id]\n    text_1 = '. '.join(section['text'] for section in paper).lower()\n    text_2 = totally_clean_text(text_1)\n    \n    labels = set()\n    for label in all_labels:\n        if label in text_1 or label in text_2:\n            labels.add(clean_text(label))\n    \n    literal_preds.append('|'.join(labels))\n","dc4ef112":"literal_preds[:5]","65f9d44d":"print(literal_preds[:5])\nprint(len(literal_preds))","441a90a5":"MAX_LENGTH = 64 # max no. words for each sentence.\nOVERLAP = 20 # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\n\nPREDICT_BATCH = 64000 \n\n#SciBERT\nPRETRAINED_PATH = '..\/input\/coleridgescibert\/output'\nTEST_INPUT_SAVE_PATH = '.\/input_data'\nTEST_NER_DATA_FILE = 'test_ner_input.json'\nTRAIN_PATH = '..\/input\/coleridgescibert\/train_ner.json'\nVAL_PATH = '..\/input\/coleridgescibert\/train_ner.json'\n\n#Normal BERT\n# PRETRAINED_PATH = '..\/input\/coleridge-bert-models\/output'\n# TEST_INPUT_SAVE_PATH = '.\/input_data'\n# TEST_NER_DATA_FILE = 'test_ner_input.json'\n# TRAIN_PATH = '..\/input\/coleridge-bert-models\/train_ner.json'\n# VAL_PATH = '..\/input\/coleridge-bert-models\/train_ner.json'\n\nPREDICTION_SAVE_PATH = '.\/pred'\nPREDICTION_FILE = 'test_predictions.txt'","029abc89":"train = train.groupby('Id').agg({\n    'pub_title': 'first',\n    'dataset_title': '|'.join,\n    'dataset_label': '|'.join,\n    'cleaned_label': '|'.join\n}).reset_index()\n\nprint(f'No. grouped training rows: {len(train)}')","b1c2eeb1":"#print(train.info)\n#print(train['d0fa7568-7d8e-4db9-870f-f9c6f668c17b'][:5])","2b243692":"def clean_training_text(txt):\n    \"\"\"\n    similar to the default clean_text function but without lowercasing.\n    \"\"\"\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt)).strip()\n\ndef shorten_sentences(sentences):\n    short_sentences = []\n    for sentence in sentences:\n        words = sentence.split()\n        if len(words) > MAX_LENGTH:\n            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n        else:\n            short_sentences.append(sentence)\n    return short_sentences","3e894232":"test_rows = [] # test data in NER format\npaper_length = [] # store the number of sentences each paper has\n\nfor paper_id in sample_submission['Id']:\n    # load paper\n    paper = papers[paper_id]\n    \n    # extract sentences\n    sentences = [clean_training_text(sentence) for section in paper \n                 for sentence in section['text'].split('.')\n                ]\n    sentences = shorten_sentences(sentences) # make sentences short\n    sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n    sentences = [sentence for sentence in sentences if any(word in sentence.lower() for word in ['data', 'study'])]\n        \n    # collect all sentences in json\n    for sentence in sentences:\n        sentence_words = sentence.split()\n        dummy_tags = ['O']*len(sentence_words)\n        test_rows.append({'tokens' : sentence_words, 'tags' : dummy_tags})\n    \n    # track which sentence belongs to which data point\n    paper_length.append(len(sentences))\n    \nprint(f'total number of sentences: {len(test_rows)}')","30ff5e69":"#print(test_rows[35])\nprint(paper_length)","8d22f149":"os.environ[\"MODEL_PATH\"] = f\"{PRETRAINED_PATH}\"\nos.environ[\"TRAIN_FILE\"] = f\"{TRAIN_PATH}\"\nos.environ[\"VALIDATION_FILE\"] = f\"{VAL_PATH}\"\nos.environ[\"TEST_FILE\"] = f\"{TEST_INPUT_SAVE_PATH}\/{TEST_NER_DATA_FILE}\"\nos.environ[\"OUTPUT_DIR\"] = f\"{PREDICTION_SAVE_PATH}\"","29c84c74":"# copy my_seqeval.py to the working directory because the input directory is non-writable\n!cp \/kaggle\/input\/coleridge-packages\/my_seqeval.py .\/\n\n# make necessart directories and files\nos.makedirs(TEST_INPUT_SAVE_PATH, exist_ok=True)","4fe22da5":"def bert_predict():\n    !python ..\/input\/kaggle-ner-utils\/kaggle_run_ner.py \\\n    --model_name_or_path \"$MODEL_PATH\" \\\n    --train_file \"$TRAIN_FILE\" \\\n    --validation_file \"$VALIDATION_FILE\" \\\n    --test_file \"$TEST_FILE\" \\\n    --output_dir \"$OUTPUT_DIR\" \\\n    --report_to 'none' \\\n    --seed 123 \\\n    --do_predict","b3ea7754":"bert_outputs = []\n\nfor batch_begin in range(0, len(test_rows), PREDICT_BATCH):\n    # write data rows to input file\n    with open(f'{TEST_INPUT_SAVE_PATH}\/{TEST_NER_DATA_FILE}', 'w') as f:\n        for row in test_rows[batch_begin:batch_begin+PREDICT_BATCH]:\n            json.dump(row, f)\n            f.write('\\n')\n    \n    # remove output dir\n    !rm -r \"$OUTPUT_DIR\"\n    \n    # do predict\n    bert_predict()\n    \n    # read predictions\n    with open(f'{PREDICTION_SAVE_PATH}\/{PREDICTION_FILE}') as f:\n        this_preds = f.read().split('\\n')[:-1]\n        bert_outputs += [pred.split() for pred in this_preds]","5c76a86d":"# get test sentences\ntest_sentences = [row['tokens'] for row in test_rows]\n#print(test_sentences)\n#del test_rows","78f193c4":"bert_dataset_labels = [] # store all dataset labels for each publication\n#print(len(test_sentences))\n#print(\"paper length: \", str(paper_length))\nfor length in paper_length:\n    #print(\"\\n \\n Starting new paper\")\n    labels = set()\n    #print(\"bert outputs \", str(bert_outputs[:length]))\n    for sentence, pred in zip(test_sentences[:length], bert_outputs[:length]):\n        curr_phrase = ''\n        for word, tag in zip(sentence, pred):\n            if tag == 'B': # start a new phrase\n                #print(\"Sentence is: \", str(sentence))\n                if curr_phrase:\n                    labels.add(curr_phrase)\n                    curr_phrase = ''\n                curr_phrase = word\n            elif tag == 'I' and curr_phrase: # continue the phrase\n                curr_phrase += ' ' + word\n            else: # end last phrase (if any)\n                if curr_phrase:\n                    labels.add(curr_phrase)\n                    curr_phrase = ''\n        # check if the label is the suffix of the sentence\n        if curr_phrase:\n            labels.add(curr_phrase)\n            curr_phrase = ''\n        \n    \n    # record dataset labels for this publication\n    #print(\"adding \", str(labels), \" to bert labels\")\n    bert_dataset_labels.append(labels)\n    \n    del test_sentences[:length], bert_outputs[:length]","9b98ae4a":"#bert_dataset_labels[:5]\n#print(len(test_sentences))\n#print(bert_outputs)\n#print(this_preds)","c45e4b3d":"def jaccard_similarity(s1, s2):\n    l1 = s1.split(\" \")\n    l2 = s2.split(\" \")    \n    intersection = len(list(set(l1).intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    sim = float(intersection) \/ union\n    #printf(\"string1: %s string2: %s, jac_sim: %d\", s1, s2, sim)\n    return sim\n\nfiltered_bert_labels = []\n\nfor labels in bert_dataset_labels:\n    filtered = []\n    \n    for label in sorted(labels, key=len):\n        label = clean_text(label)\n        if len(filtered) == 0 or all(jaccard_similarity(label, got_label) < 0.75 for got_label in filtered):\n        #if len(filtered) == 0 or all(distance.sokalmichener(label, got_label) < 0.75 for got_label in filtered):\n            filtered.append(label)\n    \n    filtered_bert_labels.append('|'.join(filtered))","9aa8d65b":"# print(filtered_bert_labels[:5])\n# print(literal_preds[:5])\nfiltered_bert_labels[:5]","b1fa822e":"# final_predictions = []\n# for literal_match, bert_pred in zip(literal_preds, filtered_bert_labels):\n#     if literal_match:\n#         final_predictions.append(literal_match)\n#     else:\n#         print(\"we used BERT\")\n#         final_predictions.append(bert_pred)","4fe8c97e":"final_predictions = []\nfor literal_match, bert_pred in zip(literal_preds, filtered_bert_labels):\n    #print(\"\\n literal match: \", str(literal_match))\n   # print(\"bert pred: \", str(bert_pred))\n    if bert_pred:\n     #   print(\"we used BERT\")\n        final_predictions.append(bert_pred)\n    else:\n        print(\"we used Literal Matching\")\n        final_predictions.append(literal_match)","9d2531f5":"sample_submission['PredictionString'] = final_predictions\nsample_submission.head()","fd86b5cc":"sample_submission.to_csv(f'submission.csv', index=False)","92062aab":"#print(final_predictions)","401c0f17":"### Data Visualisation","13133df3":"Group by publication, training labels should have the same form as expected output.","2a3ecd1b":"# Literal matching","b90cdfa8":"# Import","cc5f1b92":"This notebook gives a simple combination of literal matching and Named Entity Recognition using BERT (base model from huggingface).\n\nThe training phase of the BERT model was done in another kernel: Pytorch BERT for Named Entity Recognition.","324670b9":"# Bert prediction","070ddf42":"### Matching on test data","4f623c62":"scibert predictions:\n['alzheimer s disease neuroimaging initiative adni',\n 'common core of data|alzheimer s disease neuroimaging initiative adni|trends in international mathematics and science study',\n 'common core of data|alzheimer s disease neuroimaging initiative adni|trends in international mathematics and science study',\n 'common core of data|alzheimer s disease neuroimaging initiative adni|trends in international mathematics and science study']\n\nbert predictions:\n['alzheimer s disease neuroimaging initiative adni',\n 'alzheimer s disease neuroimaging initiative adni|trends in international mathematics and science study',\n 'alzheimer s disease neuroimaging initiative adni|trends in international mathematics and science study',\n 'alzheimer s disease neuroimaging initiative adni|trends in international mathematics and science study']","b0514e8e":"### Create a knowledge bank","1e32eee7":"### Restore Dataset labels from predictions","56783a7c":"# Load data","da568895":"### Transform data to NER format","56e9b777":"# Aggregate final predictions and write submission file","36e3c9b8":"### Paths and Hyperparameters","d54355db":"### Do predict and collect results","8a9bfc4f":"# Install packages","c05995f5":"### Filter based on Jaccard score and clean","2081c490":"# Data Exploration"}}