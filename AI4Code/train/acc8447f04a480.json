{"cell_type":{"60725ddb":"code","9d9f1d08":"code","e02dc6db":"code","3b9bcec0":"code","b92f7545":"code","8df60db9":"code","1fdb1b2e":"code","b53895f0":"code","bc47d023":"code","0e88f25f":"code","6b9f6914":"code","24452c40":"code","34fc5134":"code","b0d8104e":"code","a81c3f4f":"code","580ffacd":"code","6e15e45f":"code","cb044670":"code","f6137b44":"code","826ecd10":"code","7cb06135":"code","3b56a4ba":"code","b4332f26":"code","311d57fd":"code","397a77cd":"code","5c866db7":"code","a9f7b058":"code","7f71c7b6":"code","f82483e3":"code","676f91a5":"code","6a145df0":"code","1dc08474":"code","c00bc2b7":"code","b7abce13":"code","20357da9":"code","03dd0a08":"code","2a53dda7":"markdown","d1aacfb1":"markdown","93ecc77c":"markdown","cdda7881":"markdown","a6a852c2":"markdown","5d85db2e":"markdown","b9e447fd":"markdown","39f92f8c":"markdown","59ed8c9b":"markdown","e8da33fa":"markdown","6677a2ab":"markdown","657a263f":"markdown","a239da2e":"markdown","483cd506":"markdown","b198e8e6":"markdown","11165582":"markdown","b2e85495":"markdown","a53cdaab":"markdown","4c4faa91":"markdown","156286ce":"markdown","0a5f81a6":"markdown","9d028dcd":"markdown","3cddf88f":"markdown","36b5aa52":"markdown","fe8af48a":"markdown","c5ae9deb":"markdown"},"source":{"60725ddb":"# Importing libraries for data analysis and wrangling\nimport imageio\nfrom PIL import Image\nimport cv2\nimport numpy as np\nimport pandas as pd \nimport os\nimport subprocess\nfrom tqdm import tqdm\n\n# Importing libraries for data visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n%matplotlib inline\nplt.rcParams['figure.dpi'] = 150\nimport seaborn as sns\n\nfrom IPython.display import Video, display\n\n#Block those warnings from pandas about setting values on a slice\nimport warnings\nwarnings.filterwarnings('ignore')","9d9f1d08":"# Read in the image labels file\n\nimage_df = pd.read_csv(\"..\/input\/nfl-impact-detection\/image_labels.csv\")\nimage_df.head()","e02dc6db":"image_df.tail()","3b9bcec0":"# Get a summary on the data type\n\nimage_df.info()","b92f7545":"train_df = pd.read_csv(\"..\/input\/nfl-impact-detection\/train_labels.csv\")\ntrain_df.head()","8df60db9":"train_df.tail()","1fdb1b2e":"train_df.info()","b53895f0":"train_tr_df = pd.read_csv(\"..\/input\/nfl-impact-detection\/train_player_tracking.csv\")\ntest_tr_df = pd.read_csv(\"..\/input\/nfl-impact-detection\/test_player_tracking.csv\")","bc47d023":"train_tr_df.head()","0e88f25f":"train_tr_df.info()","6b9f6914":"test_tr_df.head()","24452c40":"test_tr_df.info()","34fc5134":"ss_df= pd.read_csv(\"..\/input\/nfl-impact-detection\/sample_submission.csv\")","b0d8104e":"ss_df.head()","a81c3f4f":"ss_df.info()","580ffacd":"# Set the name of our working image\nimg_name = image_df['image'][0]\nimg_name","6e15e45f":"# Define the path to our selected image\nimg_path = f\"\/kaggle\/input\/nfl-impact-detection\/images\/{img_name}\"","cb044670":"# Read in and plot the image\nimg = imageio.imread(img_path) \nplt.imshow(img)\nplt.show()","f6137b44":"# Function to add labels to an image\n\ndef add_img_boxes(image_name, image_labels):\n    # Set label colors for bounding boxes\n    HELMET_COLOR = (0, 0, 0)    # Black\n\n    boxes = image_df.loc[image_df['image'] == img_name]\n    for j, box in boxes.iterrows():\n        color = HELMET_COLOR \n\n        # Add a box around the helmet\n        # Note that cv2.rectangle requires us to specify the top left pixel and the bottom right pixel\n        cv2.rectangle(img, (box.left, box.top), (box.left + box.width, box.top + box.height), color, thickness=1)\n        \n    # Display the image with bounding boxes added\n    plt.imshow(img)\n    plt.show()","826ecd10":"add_img_boxes(img_name, image_df)","7cb06135":"# Number of unique videos\n\ntrain_df['video'].nunique()","3b56a4ba":"frame_count = train_df[['gameKey','playID','frame']] \\\n    .drop_duplicates()[['gameKey','playID']] \\\n    .value_counts()\n\nfig, ax = plt.subplots(figsize=(12, 5))\nsns.set_style(\"whitegrid\")\nsns.distplot(frame_count, bins=15)\nax.set_title('Distribution of frames per video file')\nplt.show()","b4332f26":"train_df['area'] = train_df['width'] * train_df['height']\nfig, ax = plt.subplots(figsize=(12, 5))\ncolorpal = sns.color_palette(\"husl\", 9)\nsns.distplot(train_df['area'].value_counts(),\n             bins=10,\n             color=colorpal[1])\nsns.set_style(\"whitegrid\")\nax.set_title('Distribution bounding box sizes')\nplt.show()","311d57fd":"train_df['label'].value_counts() \\\n    .sort_values() \\\n    .tail(25).plot(kind='barh',\n                   figsize=(15, 5),\n                   title='Top 25 Box Labels',\n                   color=colorpal[3])\nplt.show()","397a77cd":"train_df['impactType'].value_counts() \\\n    .plot(kind='bar',\n          title='Impact Type Count',\n          figsize=(12, 4),\n          color=colorpal[4])\n\nplt.show()","5c866db7":"for i, d in train_df.groupby('impactType'):\n    if len(d) < 10:\n        continue\n    d['frame'].plot(kind='kde', alpha=1, figsize=(12, 4), label=i,\n                    title='Impact Type by Frame')\n    plt.legend()","a9f7b058":"pct_impact_occurance = train_df[['video','impact']] \\\n    .fillna(0)['impact'].mean() * 100\nprint(f'Of all bounding boxes, {pct_impact_occurance:0.4f}% of them involve an impact event')","7f71c7b6":"train_df[['video','impact','frame']] \\\n    .fillna(0) \\\n    .groupby(['frame']).mean() \\\n    .plot(figsize=(12, 5), title='Occurance of impacts by frame in video.',\n         color=colorpal[6])\nplt.show()","f82483e3":"sns.pairplot(train_df[['frame','area',\n                        'left','width',\n                        'top','height',\n                        'impact']] \\\n                .sample(5000).fillna(0),\n             hue='impact')\nplt.show()","676f91a5":"sns.pairplot(train_df[['frame','area',\n                        'left', 'top',\n                        'impactType']].dropna() \\\n             .sample(1000), hue='impactType',\n            plot_kws={'alpha': 0.5})\nplt.show()","6a145df0":"train_df['confidence'].dropna() \\\n    .astype('int').value_counts() \\\n    .plot(kind='bar',\n          title='Confidence Type Label Count',\n          figsize=(12, 4),rot=0)\nplt.show()","1dc08474":"train_df['visibility'].dropna() \\\n    .astype('int').value_counts() \\\n    .plot(kind='bar',\n          title='Visibility Label Count',\n          figsize=(12, 4),rot=0)\nplt.show()","c00bc2b7":"# Define the video we'll process\nvideo_name = train_df['video'][0]\nvideo_name","b7abce13":"# Define the path and then display the video using \nvideo_path = f\"\/kaggle\/input\/nfl-impact-detection\/train\/{video_name}\"\ndisplay(Video(data=video_path, embed=True))","20357da9":"# Create a function to annotate the video at the provided path using labels from the provided dataframe, return the path of the video\ndef annotate_video(video_path: str, video_labels: pd.DataFrame) -> str:\n    VIDEO_CODEC = \"MP4V\"\n    HELMET_COLOR = (0, 0, 0)    # Black\n    IMPACT_COLOR = (0, 0, 255)  # Red\n    video_name = os.path.basename(video_path)\n    \n    vidcap = cv2.VideoCapture(video_path)\n    fps = vidcap.get(cv2.CAP_PROP_FPS)\n    width = int(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    output_path = \"labeled_\" + video_name\n    tmp_output_path = \"tmp_\" + output_path\n    output_video = cv2.VideoWriter(tmp_output_path, cv2.VideoWriter_fourcc(*VIDEO_CODEC), fps, (width, height))\n    frame = 0\n    while True:\n        it_worked, img = vidcap.read()\n        if not it_worked:\n            break\n        \n        # We need to add 1 to the frame count to match the label frame index that starts at 1\n        frame += 1\n        # Let's add a frame index to the video so we can track where we are\n        img_name = f\"{video_name}_frame{frame}\"\n        cv2.putText(img, img_name, (0, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, HELMET_COLOR, thickness=2)\n    \n        # Now, add the boxes\n        boxes = video_labels.query(\"video == @video_name and frame == @frame\")\n        for box in boxes.itertuples(index=False):\n            if box.impact == 1 and box.confidence > 1 and box.visibility > 0:    # Filter for definitive head impacts and turn labels red\n                color, thickness = IMPACT_COLOR, 2\n            else:\n                color, thickness = HELMET_COLOR, 1\n            # Add a box around the helmet\n            cv2.rectangle(img, (box.left, box.top), (box.left + box.width, box.top + box.height), color, thickness=thickness)\n            cv2.putText(img, box.label, (box.left, max(0, box.top - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, thickness=1)\n        output_video.write(img)\n    output_video.release()\n    \n    # Not all browsers support the codec, we will re-load the file at tmp_output_path and convert to a codec that is more broadly readable using ffmpeg\n    if os.path.exists(output_path):\n        os.remove(output_path)\n    subprocess.run([\"ffmpeg\", \"-i\", tmp_output_path, \"-crf\", \"18\", \"-preset\", \"veryfast\", \"-vcodec\", \"libx264\", output_path])\n    os.remove(tmp_output_path)\n    \n    return output_path","03dd0a08":"# Label the video and display it - this will take a bit\nlabeled_video = annotate_video(f\"\/kaggle\/input\/nfl-impact-detection\/train\/{video_name}\", train_df)\ndisplay(Video(data=labeled_video, embed=True))","2a53dda7":"As we can see from the graph upwards. The length of each play varies in between 300 to 500. But the longest frame is over 600 frames\n","d1aacfb1":"<a id=\"1\"><\/a>\n<h2 style='background:#266da8; border:0; color:white'><center>Overview<center><h2>","93ecc77c":"<a id=\"2\"><\/a>\n<h2 style='background:#266da8; border:0; color:white'><center>General Visualization<center><h2>","cdda7881":"Similarly we can look at the impact type by bounding box location and area.\n\n","a6a852c2":"## Confidence Label\n- 1 = Possible\n- 2 = Definitive \n- 3 = Definitive and Obvious","5d85db2e":"<h1><center>Confidence<center><h1>","b9e447fd":"### For the purposes of evaluation, definitive helmet impacts are defined as meeting three criteria:\n\n- **impact = 1**\n- **confidence > 1**\n- **visibility > 0**","39f92f8c":"### Pairplot of Bounding Box, Impact vs Non-Impact\nThese plots attempt to quickly identify if there is any commonality between the location of the bounding box and where impacts occur. It appears that the locations tend to be","59ed8c9b":"Now, we can see in the image above that `bounding boxes` have been added to every helmet.","e8da33fa":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:white; background:#266da8; border:0' role=\"tab\" aria-controls=\"home\"><center>Quick Navigation<\/center><\/h3>\n\n* [1. Overview](#1)    \n    \n* [2. General Visualization](#2)\n\n* [3. Impact Analysis](#3)\n\n* [4. Adding bounding box to video](#4)","6677a2ab":"<a id=\"4\"><\/a>\n<h2 style='background:#266da8; border:0; color:white'><center>Adding bounding box to video<center><h2>","657a263f":"### Data:\n- `image_labels.csv` - contains the bounding boxes corresponding to the images.\n- `train_labels.csv` - Helmet tracking and collision labels for the training set.\n- `sample_submission.csv` - A valid sample submission file.\n- `[train\/test]_player_tracking.csv` - Each player wears a sensor that allows us to precisely locate them on the field; that information is reported in these two files.\n\nFolders:\n\n- `\/train\/` contains the mp4 video files for the training plays. Each play has both an endzone and sideline view.\n- `\/test\/` contains the videos for the test set. In the public dataset you only see 2 videos but these are just examples and are actually already in the training set. When your model actually submitted it will run on 15 unseen videos. We are told that 20% of the test videos will product the public LB score, and 80% will produce the private score (3 plays public LB, 12 private).. so there may be some shakeup on the private leaderboard!\n- `\/images\/` contains the additional annotated images of player helmets","a239da2e":"### - Train Data:","483cd506":"![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/12125\/logos\/header.png)","b198e8e6":"<h1><center>Impact<center><h1>","11165582":"This notebook is highly inspired by [Getting Started Notebook](https:\/\/www.kaggle.com\/samhuddleston\/nfl-1st-and-future-getting-started)","b2e85495":"### - Submission Data:","a53cdaab":"### - Image Data:\nThe labeled image dataset consists of 9947 labeled images and a .csv file named image_labels.csv that contains the labeled bounding boxes for all images. This dataset is provided to support the development of helmet detection algorithms.","4c4faa91":"### Visability Label\nVisibility labels are:\n- 0 = Not Visible from View, \n- 1 = Minimum,\n- 2 = Visible,\n- 3 = Clearly Visible","156286ce":"**This part is inspired from this [notebook](https:\/\/www.kaggle.com\/samhuddleston\/nfl-1st-and-future-getting-started).**","0a5f81a6":"<h1><center>Visability<center><h1>","9d028dcd":"### - Train\/Test Tracking data:","3cddf88f":"## Exploratory Data Analysis over NFL 1st and Future - Impact Detection\nOverview of EDA for [NFL 1st and Future - Impact Detection](https:\/\/www.kaggle.com\/c\/nfl-impact-detection)\n","36b5aa52":"The impacts are labeled by types: Helmet, shoudler, body, etc. We can see the the majority of impact types are with other helmets, but shoulder and body impacts do occur. Our submission does not need to identify the impact type, but it may be helpful information when training models.","fe8af48a":"Writing a function for adding the bounding boxes from the label to the image.To draw the `bounding box`, we need to specify the top left pixel location and the bottom right pixel location of the image.","c5ae9deb":"<a id=\"3\"><\/a>\n<h1 style='background:#266da8; border:0; color:white'><center>Impact Analysis<center><h1>"}}