{"cell_type":{"83dea303":"code","8a0d9dbe":"code","5fd9a298":"code","95d40069":"code","8a39a567":"code","97660dcf":"code","e74c44d7":"code","f27672b8":"code","8467d4d1":"code","a775d198":"code","34db0f11":"code","fb8769a2":"code","26cb8a07":"code","fa657125":"code","cfe12e1b":"code","cca5995b":"code","f365497f":"code","53d1cd1d":"code","eafcb515":"code","bd96d941":"code","97d6f26e":"code","d172ede9":"code","64d59bb8":"code","45f39f0e":"code","1b9a2805":"code","5f1661d3":"markdown","0ae7a970":"markdown","cb48bddd":"markdown","bfc99d52":"markdown","9b31e0dc":"markdown"},"source":{"83dea303":"import os\nprint(os.listdir(\"..\/input\"))","8a0d9dbe":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nimport gc\nimport time\nfrom pandas.core.common import SettingWithCopyWarning\nimport warnings\nimport lightgbm as lgb\nfrom sklearn.model_selection import GroupKFold\n\n# I don't like SettingWithCopyWarnings ...\nwarnings.simplefilter('error', SettingWithCopyWarning)\ngc.enable()\n%matplotlib inline","5fd9a298":"train = pd.read_csv('..\/input\/preprocessing_train.csv', \n                    dtype={'date': str, 'fullVisitorId': str, 'sessionId':str}, nrows=None)\ntest = pd.read_csv('..\/input\/preprocessing_test.csv', \n                   dtype={'date': str, 'fullVisitorId': str, 'sessionId':str}, nrows=None)\ny_reg = pd.read_csv(\"..\/input\/y_reg.csv\")\ny_reg = y_reg['totals.transactionRevenue']\ntrain.shape, test.shape, y_reg.shape","95d40069":"def get_folds(df=None, n_splits=5):\n    \"\"\"Returns dataframe indices corresponding to Visitors Group KFold\"\"\"\n    # Get sorted unique visitors\n    unique_vis = np.array(sorted(df['fullVisitorId'].unique()))\n\n    # Get folds\n    folds = GroupKFold(n_splits=n_splits)\n    fold_ids = []\n    ids = np.arange(df.shape[0])\n    for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n        fold_ids.append(\n            [\n                ids[df['fullVisitorId'].isin(unique_vis[trn_vis])],\n                ids[df['fullVisitorId'].isin(unique_vis[val_vis])]\n            ]\n        )\n\n    return fold_ids","8a39a567":"excluded_features = [\n    'date', 'fullVisitorId', 'sessionId', 'totals.transactionRevenue', \n    'visitId', 'visitStartTime', 'vis_date', 'nb_sessions',\n    'geoNetwork.subContinent','trafficSource.campaign','device.browser',\n    'totals.bounces','contry_sess_date_hours','contry_sess_date_dom',\n    'trafficSource.keyword'\n]\n\ncategorical_features = [\n    _f for _f in train.columns\n    if (_f not in excluded_features) & (train[_f].dtype == 'object')\n]","97660dcf":"y_clf = (y_reg > 0).astype(np.uint8)","e74c44d7":"from sklearn.metrics import mean_squared_error, roc_auc_score, log_loss\n\nfolds = GroupKFold(n_splits=5)\n\ntrain_features = [_f for _f in train.columns if _f not in excluded_features]\noof_clf_preds = np.zeros(train.shape[0])\nsub_clf_preds = np.zeros(test.shape[0])\nfor fold_, (trn_, val_) in enumerate(folds.split(y_clf, y_clf, groups=train['fullVisitorId'])):\n    trn_x, trn_y = train[train_features].iloc[trn_], y_clf.iloc[trn_]\n    val_x, val_y = train[train_features].iloc[val_], y_clf.iloc[val_]\n    \n    clf = lgb.LGBMClassifier(\n        num_leaves=31,\n        learning_rate=0.03,\n        n_estimators=1000,\n        subsample=.9,\n        colsample_bytree=.9,\n        random_state=1\n    )\n    clf.fit(\n        trn_x, trn_y,\n        eval_set=[(val_x, val_y)],\n        early_stopping_rounds=50,\n        verbose=50\n    )\n    \n    oof_clf_preds[val_] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)[:, 1]\n    print(roc_auc_score(val_y, oof_clf_preds[val_]))\n    sub_clf_preds += clf.predict_proba(test[train_features], num_iteration=clf.best_iteration_)[:, 1] \/ folds.n_splits\n    \nroc_auc_score(y_clf, oof_clf_preds)","f27672b8":"train['non_zero_proba'] = oof_clf_preds\ntest['non_zero_proba'] = sub_clf_preds","8467d4d1":"del oof_clf_preds; del sub_clf_preds; del clf; del y_clf\ngc.collect()","a775d198":"from sklearn.model_selection import StratifiedKFold, KFold\nimport lightgbm as lgb\n\ny_categorized = pd.cut(y_reg, bins=range(0,25,3), include_lowest=True,right=False, labels=range(0,24,3)) \n\n#folds = get_folds(df=df_train, n_splits=5)\nFOLDs = KFold(n_splits=5, shuffle=True, random_state=7)\ntrain_features = [_f for _f in train.columns if _f not in excluded_features]\nprint(train_features)\n\nimportances = pd.DataFrame()\noof_reg_preds = np.zeros(train.shape[0])\nsub_reg_preds = np.zeros(test.shape[0])\nfor fold_, (trn_, val_) in enumerate(FOLDs.split(train, y_categorized)):\n    trn_x, trn_y = train[train_features].iloc[trn_], y_reg.iloc[trn_]\n    val_x, val_y = train[train_features].iloc[val_], y_reg.iloc[val_]\n    \n    reg = lgb.LGBMRegressor(\n        num_leaves=31,\n        learning_rate=0.03,\n        n_estimators=1000,\n        subsample=.9,\n        colsample_bytree=.9,\n        random_state=1\n    )\n    reg.fit(\n        trn_x, np.log1p(trn_y),\n        eval_set=[(val_x, np.log1p(val_y))],\n        early_stopping_rounds=50,\n        verbose=100,\n        eval_metric='rmse'\n    )\n    imp_df = pd.DataFrame()\n    imp_df['feature'] = train_features\n    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n    \n    imp_df['fold'] = fold_ + 1\n    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n    \n    oof_reg_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n    oof_reg_preds[oof_reg_preds < 0] = 0\n    _preds = reg.predict(test[train_features], num_iteration=reg.best_iteration_)\n    _preds[_preds < 0] = 0\n    sub_reg_preds += np.expm1(_preds) \/ FOLDs.n_splits #len(folds)\n    \nmean_squared_error(np.log1p(y_reg), oof_reg_preds) ** .5","34db0f11":"import warnings\nwarnings.simplefilter('ignore', FutureWarning)\n\nimportances['gain_log'] = np.log1p(importances['gain'])\nmean_gain = importances[['gain', 'feature']].groupby('feature').mean()\nimportances['mean_gain'] = importances['feature'].map(mean_gain['gain'])\n\nplt.figure(figsize=(8, 12))\nsns.barplot(x='gain_log', y='feature', data=importances.sort_values('mean_gain', ascending=False))","fb8769a2":"train['predictions'] = np.expm1(oof_reg_preds)\ntest['predictions'] = sub_reg_preds","26cb8a07":"del imp_df; del importances; del oof_reg_preds; del sub_reg_preds; del y_categorized\ngc.collect()","fa657125":"# Aggregate data at User level\ntrn_data = train[train_features + ['fullVisitorId']].groupby('fullVisitorId').mean()","cfe12e1b":"%%time\n# Create a list of predictions for each Visitor\ntrn_pred_list = train[['fullVisitorId', 'predictions']].groupby('fullVisitorId')\\\n    .apply(lambda df: list(df.predictions))\\\n    .apply(lambda x: {'pred_'+str(i): pred for i, pred in enumerate(x)})","cca5995b":"train['target'] = y_reg\ntrn_user_target = train[['fullVisitorId', 'target']].groupby('fullVisitorId').sum()","f365497f":"del train;\ngc.collect()","53d1cd1d":"# Create a DataFrame with VisitorId as index\n# trn_pred_list contains dict \n# so creating a dataframe from it will expand dict values into columns\ntrn_all_predictions = pd.DataFrame(list(trn_pred_list.values), index=trn_data.index)\ntrn_feats = trn_all_predictions.columns\ntrn_all_predictions['t_mean'] = np.log1p(trn_all_predictions[trn_feats].mean(axis=1))\ntrn_all_predictions['t_median'] = np.log1p(trn_all_predictions[trn_feats].median(axis=1))\ntrn_all_predictions['t_sum_log'] = np.log1p(trn_all_predictions[trn_feats]).sum(axis=1)\ntrn_all_predictions['t_sum_act'] = np.log1p(trn_all_predictions[trn_feats].fillna(0).sum(axis=1))\ntrn_all_predictions['t_nb_sess'] = trn_all_predictions[trn_feats].isnull().sum(axis=1)\nfull_data = pd.concat([trn_data, trn_all_predictions], axis=1)\ndel trn_data, trn_all_predictions\ngc.collect()\nfull_data.shape","eafcb515":"%%time\nsub_pred_list = test[['fullVisitorId', 'predictions']].groupby('fullVisitorId')\\\n    .apply(lambda df: list(df.predictions))\\\n    .apply(lambda x: {'pred_'+str(i): pred for i, pred in enumerate(x)})","bd96d941":"sub_data = test[train_features + ['fullVisitorId']].groupby('fullVisitorId').mean()\nsub_all_predictions = pd.DataFrame(list(sub_pred_list.values), index=sub_data.index)\nfor f in trn_feats:\n    if f not in sub_all_predictions.columns:\n        sub_all_predictions[f] = np.nan\nsub_all_predictions['t_mean'] = np.log1p(sub_all_predictions[trn_feats].mean(axis=1))\nsub_all_predictions['t_median'] = np.log1p(sub_all_predictions[trn_feats].median(axis=1))\nsub_all_predictions['t_sum_log'] = np.log1p(sub_all_predictions[trn_feats]).sum(axis=1)\nsub_all_predictions['t_sum_act'] = np.log1p(sub_all_predictions[trn_feats].fillna(0).sum(axis=1))\nsub_all_predictions['t_nb_sess'] = sub_all_predictions[trn_feats].isnull().sum(axis=1)\nsub_full_data = pd.concat([sub_data, sub_all_predictions], axis=1)\ndel sub_data, sub_all_predictions\ngc.collect()\nsub_full_data.shape","97d6f26e":"del test;\ngc.collect()","d172ede9":"xgb_params = {\n        'objective': 'reg:linear',\n        'booster': 'gbtree',\n        'learning_rate': 0.02,\n        'max_depth': 22,\n        'min_child_weight': 57,\n        'gamma' : 1.45,\n        'alpha': 0.0,\n        'lambda': 0.0,\n        'subsample': 0.67,\n        'colsample_bytree': 0.054,\n        'colsample_bylevel': 0.50,\n        'n_jobs': -1,\n        'random_state': 456\n    }","64d59bb8":"from xgboost import XGBRegressor\nfolds = get_folds(df=full_data[['totals.pageviews']].reset_index(), n_splits=5)\n\noof_preds = np.zeros(full_data.shape[0])\noof_preds1 = np.zeros(full_data.shape[0])\nboth_oof = np.zeros(full_data.shape[0])\nsub_preds = np.zeros(sub_full_data.shape[0])\nvis_importances = pd.DataFrame()\n\nfor fold_, (trn_, val_) in enumerate(folds):\n    print(\"-\"* 20 + \"Fold :\"+str(fold_) + \"-\"* 20)\n    trn_x, trn_y = full_data.iloc[trn_], trn_user_target['target'].iloc[trn_]\n    val_x, val_y = full_data.iloc[val_], trn_user_target['target'].iloc[val_]\n    xg = XGBRegressor(**xgb_params, n_estimators=1000)\n    reg = lgb.LGBMRegressor(\n        num_leaves=31,\n        learning_rate=0.03,\n        n_estimators=1000,\n        subsample=.9,\n        colsample_bytree=.9,\n        random_state=1\n    )\n    print(\"-\"* 20 + \"LightGBM Training\" + \"-\"* 20)\n    reg.fit(\n        trn_x, np.log1p(trn_y),\n        eval_set=[(trn_x, np.log1p(trn_y)), (val_x, np.log1p(val_y))],\n        eval_names=['TRAIN', 'VALID'],\n        early_stopping_rounds=50,\n        eval_metric='rmse',\n        verbose=100\n    )\n    print(\"-\"* 20 + \"Xgboost Training\" + \"-\"* 20)\n    xg.fit(\n        trn_x, np.log1p(trn_y),\n        eval_set=[(trn_x, np.log1p(trn_y)), (val_x, np.log1p(val_y))],\n        early_stopping_rounds=50,\n        eval_metric='rmse',\n        verbose=100\n    )\n    imp_df = pd.DataFrame()\n    imp_df['feature'] = trn_x.columns\n    imp_df['gain'] = reg.booster_.feature_importance(importance_type='gain')\n\n    imp_df['fold'] = fold_ + 1\n    vis_importances = pd.concat([vis_importances, imp_df], axis=0, sort=False)\n\n    oof_preds[val_] = reg.predict(val_x, num_iteration=reg.best_iteration_)\n    oof_preds1[val_] = xg.predict(val_x)\n\n    oof_preds[oof_preds < 0] = 0\n    oof_preds1[oof_preds1 < 0] = 0\n\n    both_oof[val_] = oof_preds[val_] * 0.6 + oof_preds1[val_] * 0.4\n\n    # Make sure features are in the same order\n    _preds = reg.predict(sub_full_data[full_data.columns], num_iteration=reg.best_iteration_)\n    _preds[_preds < 0] = 0\n\n    pre = xg.predict(sub_full_data[full_data.columns])\n    pre[pre<0]=0\n\n    sub_preds += (_preds \/ len(folds)) * 0.6 + (pre \/ len(folds)) * 0.4\n    gc.collect()\nprint(\"LGB  \", mean_squared_error(np.log1p(trn_user_target['target']), oof_preds) ** .5)\nprint(\"XGB  \", mean_squared_error(np.log1p(trn_user_target['target']), oof_preds1) ** .5)\nprint(\"Combine  \", mean_squared_error(np.log1p(trn_user_target['target']), both_oof) ** .5)","45f39f0e":"vis_importances['gain_log'] = np.log1p(vis_importances['gain'])\nmean_gain = vis_importances[['gain', 'feature']].groupby('feature').mean()\nvis_importances['mean_gain'] = vis_importances['feature'].map(mean_gain['gain'])\n\nplt.figure(figsize=(8, 25))\nsns.barplot(x='gain_log', y='feature', data=vis_importances.sort_values('mean_gain', ascending=False).iloc[:300])","1b9a2805":"sub_full_data['PredictedLogRevenue'] = sub_preds\nsub_full_data[['PredictedLogRevenue']].to_csv('xgboost_lightgbm.csv', index=True)","5f1661d3":"### Modeling #1 : Classify noe-zero Revenues","0ae7a970":"### Save Predictions","cb48bddd":"### Modeling #2 : Predict revenues at session level","bfc99d52":"### Modeling #3 : Create user level predictions","9b31e0dc":"### Define folding strategy"}}