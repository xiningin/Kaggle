{"cell_type":{"18a02f18":"code","f516a4e6":"markdown","3c6bea07":"markdown"},"source":{"18a02f18":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\nfrom xgboost import XGBClassifier\nimport matplotlib.pylab as pylab\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.ensemble import BaggingClassifier\n\ndef extractDeck(x):\n    if str(x) != \"nan\":\n        return str(x)[0]\n    else :\n        return\n\n#Import data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ncombine = train.drop([\"Survived\"], axis=1).append(test).drop([\"PassengerId\", \"Ticket\"], axis=1)\ntarget = train['Survived']\n\n#Feature preprocessing\ncombine[\"hasParents\"] = combine[\"Parch\"].apply(lambda x : (x>0)*1)\ncombine[\"hasSibs\"] = combine[\"SibSp\"].apply(lambda x : (x>0)*1)\ncombine[\"title\"] = combine['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ncombine[\"Deck\"] = combine['Cabin'].apply(extractDeck)\ncombine.drop([\"Parch\", \"SibSp\", \"Cabin\", \"Name\"], axis=1)\n\n#Turning categorical to integer\ncombine['Sex'] = combine['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n    \n#One hot encoding on Embarked\ncombine['Embarked'].fillna('S', inplace = True)\n#combine = pd.get_dummies(combine)#, columns = ['Embarked'])\n\n\n#Fill the blank\ncombine['Age'].fillna(combine['Age'].dropna().median(), inplace = True)\n\n#Turning age to ranges\ncombine.loc[(combine['Age'] <= 16), 'Age'] = 0 \ncombine.loc[(combine['Age'] > 16) & (combine['Age'] <= 32), 'Age'] = 1 \ncombine.loc[(combine['Age'] > 32) & (combine['Age'] <= 48), 'Age'] = 2 \ncombine.loc[(combine['Age'] > 48) & (combine['Age'] <= 64), 'Age'] = 3 \ncombine.loc[(combine['Age'] > 64), 'Age'] \n\n\n#Filling the blank\ncombine['Fare'].fillna(combine['Fare'].dropna().median(), inplace=True)\n\n#Turning fare to ranges\ncombine.loc[ combine['Fare'] <= 7.91, 'Fare'] = 0\ncombine.loc[(combine['Fare'] > 7.91) & (combine['Fare'] <= 14.454), 'Fare'] = 1\ncombine.loc[(combine['Fare'] > 14.454) & (combine['Fare'] <= 31), 'Fare']   = 2\ncombine.loc[ combine['Fare'] > 31, 'Fare'] = 3\ncombine['Fare'] = combine['Fare'].astype(int)\n\ncombine[\"Pclass\"]=combine[\"Pclass\"].astype(\"str\")\ncombine = pd.get_dummies(combine)\n\n#Defining learning vectors\nnb = train.shape[0]\nX = combine[:nb]\ny = target\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.1, train_size=0.9)\n\n#XGBoost model tuning\nmodel = XGBClassifier(booster='gbtree', silent=1, seed=0, base_score=0.5, subsample=0.75)\nparameters = {'n_estimators':[75], #50,100\n            'max_depth':[4],#1,10\n            'gamma':[4],#0,6\n            'max_delta_step':[1],#0,2\n            'min_child_weight':[1], #3,5 \n            'colsample_bytree':[0.55,0.6,0.65], #0.5,\n            'learning_rate': [0.001,0.01,0.1]\n            }\ntune_model =  GridSearchCV(model, parameters, cv=3, scoring='accuracy')\ntune_model.fit(X_train,y_train)\nprint('Best parameters :', tune_model.best_params_)\nprint('Results :', format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n\n\n#Learn on the whole data\ntune_model.fit(X, y)\nY_pred = tune_model.predict(combine[nb:])\n\n#Submit the prediction\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submission.csv', index=False)\n","f516a4e6":"# Basic modeling ","3c6bea07":"This notebook presents simple and quick way to implement a XGBoost classifier on the Titanic dataset. It doesn't come to data visualization and feature engineering. This is kind of a first approach."}}