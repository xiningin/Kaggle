{"cell_type":{"458b10e5":"code","30148d0b":"code","b4d22ddb":"code","661e72bf":"code","ece6a434":"code","125189b4":"code","dd37aaff":"code","f314926f":"code","58955547":"code","ccbce24d":"code","4a6fcd4c":"code","4740d8c5":"code","9b10948c":"code","adb64fb6":"code","18852066":"code","7bd2be31":"code","d7f05523":"code","63396476":"code","aad94339":"code","e8954fe5":"code","40db4f4d":"code","8eec80df":"code","a2ffa6ba":"code","e86219de":"code","0f831b08":"code","22c6c3ab":"code","c3955a83":"code","ede1402d":"code","1c36551e":"code","63ee6d33":"code","973c8d84":"code","e1535811":"code","92a4a932":"code","56a48e1e":"code","f1b8cc4e":"code","d5d9d76d":"code","152bdf55":"code","19d86d24":"code","31540f17":"code","67ba8c96":"code","74df33b4":"code","af591e47":"code","38ff9cae":"code","6b8f19c7":"code","a840a619":"code","2657895c":"code","171e7c4e":"code","ef7e5404":"markdown","47f31309":"markdown","d74977d7":"markdown","6f65bd62":"markdown","6f724ef7":"markdown","d4e3a747":"markdown","738aa2a2":"markdown","2a00d573":"markdown","9141142e":"markdown","3d3e5c99":"markdown","4d3447ca":"markdown","fde33927":"markdown","162178bf":"markdown","f932bfb0":"markdown","7bff57e3":"markdown","21d5faf6":"markdown","c409d86b":"markdown","6a7a2321":"markdown"},"source":{"458b10e5":"#import standard packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\n\n#supress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","30148d0b":"#import data\nconcrete_data = pd.read_csv('..\/input\/yeh-concret-data\/Concrete_Data_Yeh.csv')","b4d22ddb":"#look at formatting of entries\nconcrete_data.head()","661e72bf":"#look at null count and dtype\nconcrete_data.info()","ece6a434":"#look at distribution of data\nconcrete_data.describe()","125189b4":"#look at data distribution\nfor i in concrete_data.columns:\n    plt.hist(concrete_data[i])\n    plt.xticks()\n    plt.xlabel(i)\n    plt.ylabel('counts')\n    plt.show()","dd37aaff":"#heat map using Pearson's coefficient\nplt.figure(figsize=(16, 6))\nsns.heatmap(concrete_data.corr(), annot=True)\nplt.title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);","f314926f":"#create bins from compressive strength\nbins = pd.qcut(concrete_data['csMPa'], q=4)\n\n#add bins to concrete df\nconcrete_data['bins']=bins","58955547":"#look at how target is distributed among variables\nsns.pairplot(concrete_data.loc[:, (concrete_data.columns != 'csMPa')], hue='bins')\nplt.legend()\nplt.show()","ccbce24d":"#plot strongest linear correlation\nsns.lmplot(x='cement', y='csMPa',data=concrete_data)\nplt.show()","4a6fcd4c":"#drop bins from concrete data\nconcrete_data = concrete_data.drop('bins', axis=1)","4740d8c5":"#copy of variables and target\nX = concrete_data.copy()\ny = X.pop('csMPa')","9b10948c":"#make a copy of features matrix for mutual information analysis\nX_mi = X.copy()\n\n#label encoding for categorical variables\nfor colname in X_mi.select_dtypes(\"object\"):\n    X_mi[colname], _ = X_mi[colname].factorize()\n\n#all discrete features have int dtypes\ndiscrete_features = X_mi.dtypes == object","adb64fb6":"#some continuous variables also have int dtypes\ndiscrete_features[X_mi.columns] = False","18852066":"#use regression since the target variable is continuous\nfrom sklearn.feature_selection import mutual_info_regression\n\n#define a function to produce mutual information scores\ndef make_mi_scores(X_mi, y, discrete_features):\n    mi_scores = mutual_info_regression(X_mi, y, discrete_features=discrete_features)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X_mi.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n#compute mutual information scores\nmi_scores = make_mi_scores(X_mi, y, discrete_features)\nmi_scores","7bd2be31":"#define a function to plot mutual information scores\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n\n#plot the scores\nplt.figure(dpi=100, figsize=(8, 5))\nplot_mi_scores(mi_scores)","d7f05523":"#plot top MI score predictors against target\nfor i in ['water', 'age', 'cement']:\n    fig, ax = plt.subplots(figsize=(12,4))\n    sns.scatterplot(x=X_mi[i], y=y, ax=ax)\n    plt.show()","63396476":"#copy features matrix for principal component analysis\nX_for_PCA = X.copy()\n\n#standardize\nX_for_PCA_scaled = (X_for_PCA - X_for_PCA.mean(axis=0)) \/ X_for_PCA.std(axis=0)\n\nfrom sklearn.decomposition import PCA\n\n#create principal components\npca = PCA(len(X.columns))\nX_pca = pca.fit_transform(X_for_PCA_scaled)\n\n#convert to dataframe\ncomponent_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\nX_pca = pd.DataFrame(X_pca, columns=component_names)","aad94339":"#plot data using principal components\nsns.scatterplot(x=X_pca.loc[:,'PC1'],y=X_pca.loc[:,'PC2'], hue=bins)\nplt.show()","e8954fe5":"#determine loadings\nloadings = pd.DataFrame(\n    pca.components_.T,  # transpose the matrix of loadings\n    columns=component_names,  # so the columns are the principal components\n    index=X.columns,  # and the rows are the original features\n)\nloadings","40db4f4d":"#determine % explained variance and use % cumulative variance for elbow method to determine number of PCs\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n\nplot_variance(pca);","8eec80df":"#generate OLS regression results for all features\nimport statsmodels.api as sm\n\nX_sm = sm.add_constant(X)\nmodel = sm.OLS(y,X_sm)\nprint(model.fit().summary())","a2ffa6ba":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n  \n#initialize VIF dataframe\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = X.columns\n  \n#calculate VIF for each feature\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n                          for i in range(len(X.columns))]\n  \nprint(vif_data)","e86219de":"#print OLS summary for each feature\nfor i in X.columns:\n    X_sm = sm.add_constant(X[i])\n    model = sm.OLS(y,X_sm)\n    print(model.fit().summary())","0f831b08":"from statsmodels.formula.api import ols\n\n#fit multiple linear regression model\nmodel = ols('csMPa ~ cement + slag + flyash + water + superplasticizer + coarseaggregate + fineaggregate + age', data=concrete_data).fit()\n\n#create residual vs. predictor plot for 'assists'\nfor i in X.columns:\n    fig = plt.figure(figsize=(12,8))\n    fig = sm.graphics.plot_regress_exog(model, i, fig=fig)\n    fig.show()","22c6c3ab":"#feature engineering using knowledge that water:cement ratio is an important factor for concrete strength\nX['water_cement_ratio'] = X['water']\/X['cement']","c3955a83":"#plot water:cement ratio against compressive strength\nsns.scatterplot(x=X['water_cement_ratio'], y=y)\nplt.show()","ede1402d":"#generate OLS regression results with water : cement ratio\nX_sm = sm.add_constant(X)\nmodel = sm.OLS(y,X_sm)\nprint(model.fit().summary())","1c36551e":"#generate OLS summary with only water : cement ratio\nX_sm = sm.add_constant(X['water_cement_ratio'])\nmodel = sm.OLS(y,X_sm)\nprint(model.fit().summary())","63ee6d33":"#copy water : cement ratio into concrete data\nconcrete_data['water_cement_ratio'] = X['water_cement_ratio']\n\n#fit multiple linear regression model with water : cement ratio\nmodel = ols('csMPa ~ cement + slag + flyash + water + superplasticizer + coarseaggregate + fineaggregate + age + water_cement_ratio', data=concrete_data).fit()\n\n#plot regression plots for water : cement ratio\nfig = plt.figure(figsize=(12,8))\nfig = sm.graphics.plot_regress_exog(model, 'water_cement_ratio', fig=fig)\nfig.show()","973c8d84":"#import ML preprocessing packages\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler","e1535811":"#column names\nfeature_names = X.columns\n\n#train\/test split 75% training, 25% test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=1)\n\n#numerical pipeline\nscaler=MinMaxScaler()\n\n#apply scaler to numerical data\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","92a4a932":"#import ML packages\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom numpy import mean\nfrom numpy import std","56a48e1e":"#LinearRegression mean cross-validation\nlm = LinearRegression()\nlm.fit(X_train, y_train)\ncv = cross_val_score(lm,X_train,y_train,scoring='neg_mean_absolute_error',cv=5)\nprint('LinearRegression')\nprint(mean(cv), '+\/-', std(cv))","f1b8cc4e":"#RandomForestRegressor mean cross-validation\nrf = RandomForestRegressor(random_state = 1)\ncv = cross_val_score(rf,X_train,y_train,scoring='neg_mean_absolute_error',cv=5)\nprint('RandomForestRegressor')\nprint(mean(cv), '+\/-', std(cv))","d5d9d76d":"#GradientBoostingRegressor mean cross-validation\ngbr = GradientBoostingRegressor(random_state = 1)\ncv = cross_val_score(gbr,X_train,y_train,scoring='neg_mean_absolute_error',cv=5)\nprint('GradientBoostingRegressor')\nprint(mean(cv), '+\/-', std(cv))","152bdf55":"#XGBoost mean cross-validation\nxgb = XGBRegressor(random_state = 1)\ncv = cross_val_score(xgb,X_train,y_train,scoring='neg_mean_absolute_error',cv=5)\nprint('XGBoost')\nprint(mean(cv), '+\/-', std(cv))","19d86d24":"#ml algorithm tuner\nfrom sklearn.model_selection import GridSearchCV\n\n#performance reporting function\ndef clf_performance(regressor, model_name):\n    print(model_name)\n    print('Best Score: {} +\/- {}'.format(str(regressor.best_score_),str(regressor.cv_results_['std_test_score'][regressor.best_index_])))\n    print('Best Parameters: ' + str(regressor.best_params_))","31540f17":"#LinearRegression GridSearchCV\nlm = LinearRegression()\nparam_grid = {\n                'fit_intercept':[True,False],\n                'normalize':[True,False],\n                'copy_X':[True, False]\n}\nclf_lm = GridSearchCV(lm, param_grid = param_grid, cv = 5, scoring='neg_mean_absolute_error', n_jobs = -1)\nbest_clf_lm = clf_lm.fit(X_train,y_train)\nclf_performance(best_clf_lm,'LinearRegressor')","67ba8c96":"#RanddomForestRegressor GridSearchCV\nrf = RandomForestRegressor(random_state = 1)\nparam_grid = {\n                'n_estimators': np.arange(160,200,2) , \n                'bootstrap': [True,False],\n#                 'max_depth': [20,30,40],\n#                 'max_features': ['auto','sqrt','log2'],\n#                  'min_samples_leaf': [2],\n#                  'min_samples_split': [6,8,10]\n              }\nclf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, scoring='neg_mean_absolute_error', n_jobs = -1)\nbest_clf_rf = clf_rf.fit(X_train,y_train)\nclf_performance(best_clf_rf,'RandomForestRegressor')","74df33b4":"#GradientBoostingRegressor GridSearchCV\ngbr = GradientBoostingRegressor(random_state = 1)\nparam_grid = {\n                'n_estimators': [160], \n                'max_depth': [4],\n                'max_features': ['auto'],\n                'learning_rate': np.arange(.1,1,.1),\n                'alpha': [0.0001],\n                'min_samples_leaf': [2],\n                'min_samples_split': np.arange(2,6,1)\n              }\nclf_gbr = GridSearchCV(gbr, param_grid = param_grid, cv = 5, scoring='neg_mean_absolute_error', n_jobs = -1)\nbest_clf_gbr = clf_gbr.fit(X_train,y_train)\nclf_performance(best_clf_gbr,'GradientBoostingRegressor')","af591e47":"#XGBoost GridSearchCV\nxgb = XGBRegressor(random_state = 1)\nparam_grid = {\n#               'nthread':[4],\n#               'objective':['reg:linear'],\n#               'learning_rate': [0.3],\n              'max_depth': [4],\n#               'min_child_weight': [1],\n#               'subsample': [1],\n#               'colsample_bytree': np.arange(0.5,1,0.1),\n              'n_estimators': [500]\n              }\nclf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5, scoring='neg_mean_absolute_error', n_jobs = -1)\nbest_clf_xgb = clf_xgb.fit(X_train,y_train)\nclf_performance(best_clf_xgb,'XGBoost')","38ff9cae":"#import metrics packages\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score","6b8f19c7":"#GradientBoostingRegressor metrics\ngbr = GradientBoostingRegressor(alpha = 0.0001,\n                                learning_rate= 0.2,\n                                max_depth= 4,\n                                max_features='auto',\n                                min_samples_leaf= 2,\n                                min_samples_split= 2,\n                                n_estimators= 160,\n                                random_state = 1)\ngbr.fit(X_train,y_train)\ntpred_gbr=gbr.predict(X_test)\nprint('GradientBoostingRegressor')\nprint('MSE: {}'.format(mean_squared_error(y_test,tpred_gbr)))\nprint('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test,tpred_gbr))))\nprint('MAE: {}'.format(mean_absolute_error(y_test,tpred_gbr)))\nprint('R-squared: {}'.format(r2_score(y_test,tpred_gbr)))","a840a619":"#XGBoost metrics\nxgb = XGBRegressor(max_depth=4,\n                   n_estimators=500,\n                   random_state = 1)\nxgb.fit(X_train,y_train)\ntpred_xgb=xgb.predict(X_test)\nprint('XGBoost')\nprint('MSE: {}'.format(mean_squared_error(y_test,tpred_xgb)))\nprint('RMSE: {}'.format(np.sqrt(mean_squared_error(y_test,tpred_xgb))))\nprint('MAE: {}'.format(mean_absolute_error(y_test,tpred_xgb)))\nprint('R-squared: {}'.format(r2_score(y_test,tpred_xgb)))","2657895c":"#import packages for explaining feature importance\nimport eli5\nfrom eli5.sklearn import PermutationImportance","171e7c4e":"#permutation importance from xgboost\nperm = PermutationImportance(xgb).fit(X_test, y_test)\neli5.show_weights(perm, feature_names = list(feature_names), top=len(feature_names))","ef7e5404":"# 1. Import packages and Data","47f31309":"# 10. Assessing Models","d74977d7":"The VIF scores exceeding 5 to 10 indicate collinearity (where 1 is the minimum). The aggregates, water, and cement exhibit multicollinearity. Superplasticizer is within the 5 to 10 range, so if being conservative then superplasticizer also demonstrates multicollinearity. Therefore, we cannot say that coarse aggregate is or is not statistically significant, because the wideness of the confidence interval could be due to multicollinearity.\n\nTo assess association of each predictor, seperate OLS for each predictor is performed:","6f65bd62":"# 3. Mutual Information","6f724ef7":"# 9. Hyperparameter Tuning","d4e3a747":"# 6. Feature Engineering with OLS","738aa2a2":"# 8. ML Baselines","2a00d573":"# 2. Initial EDA and Distributions","9141142e":"# 4. Principal Component Analysis","3d3e5c99":"# Questions\n\n1. Is there a relationship between the predictors (age and ingredients) and the response variable (compressive strength)?\n\n**Given there is a relationship:**\n\n2. How strong is it?\n3. Which predictors contribute to compressive strength?\n4. How large is the effect of each predictor on compressive strength?\n5. How accurately can I predict compressive strength?\n6. Is the relationship linear?\n7. Is there synergy\/interaction among the predictors?","4d3447ca":"Looking at the p-value of the t-statistic: water : cement ratio has a strong association with compressive strength.","fde33927":"**Question 1: Is there a relationship between the predictors (age and ingredients) and the response variable (compressive strength)?**\n\nNull hypothesis: coefficients for each predictor is zero.\n\nF-statistic = 204.3 >> 1 (suggests at least one of the predictors is related to compressive strength)\n\nProb(F-statistic) = 6.29e-206 << 0.05 (probability that the null hypothesis is true)\n\nTherefore, there is a relationship between the predictors and the response variable.\n\n\n**Question 2: How strong is the relationship?**\n\nR-squared = 0.616 (61.6% of variance is explained by the model)\n\n\n**Question 3: Which predictors contribute to compressive strength?**\n\nLook at the p-values for each t-statistic for each predictor where p-values are the probability of t-statistic given the null hypothesis is true. A probability less than 1\/20 (0.05) is considered sufficient to reject the null hypothesis.\n\nAll but coarse and fine aggregates are less than 0.05. Therefore, the aggregates do not contribute to compressive strength in this model.\n\n\n**Question 4: How large is the effect of each predictor on compressive strength?**\n\nThe only predictor confidence interval to include zero is coarse aggregate. The rest are considered to be statistically significant. To test whether collinearity is the reason why the confidence interval is so wide for coarse aggregate, the VIF scores are calculated.\n\n\n\n\n\n","162178bf":"Looking at the p-value of the t-statistic: all variables have a strong association with compressive strength where fly ash has the weakest by 0.001.\n\n\n**Question 5: How accurately can this model predict compressive strength?**\n\nThe accuracy depends on what type of prediction:\n\n1. Individual response (Y = f(X) + ep), the prediction interval is used\n2. Average response (f(X)), the confidence interval is used\n\nPrediction intervals are wider than confidence intervals because the account for the uncertainity associated with the irreducible error (ep).\n\n\n**Question 6: Is the relationship linear?**\n\nNon-linearity can be determined from residual vs. predicted value plot for each variable (top right plots below). When linearity exists, there should be no clear pattern. The residual plot with the most non-linear form is for age where for ages 0 to 20, there are negative residuals then the residuals increase from 20 to 100 before decreasing again. Water and fine aggregate have slight non-linear patterns. Transformations of the predictors (e.g., sqrt(X), X^2) could accomodate the nonlinearities.","f932bfb0":"Water : cement ratio residuals exhibits a near linear relationship.","7bff57e3":"**Question 7: Is there synergy among the predictors?**\n\nTo answer this, an interaction term needs to be created that accomodates non-additive relationships and the R-squared value should increase with this inclusion. Below I have created an interaction term for water and cement (water : cement ratio) and ran an OLS analysis again. With this interaction term, the R-squared term increased from 0.616 to 0.618. Since adding variables increases R-squared automatically and an increase of 0.002 is not large, it's better to look at the adj. R-squared which penalizes additional predictors. This value went from 0.613 to 0.615 indicating synergy exists between these predictors. Similarly, AIC and BIC metrics penalize models with more predictors, but AIC decreased with the addition of water : cement ratio (from 7756 to 7750) and BIC remained the same (7800) further demonstrating that the addition of the predictor is justified. The best method for judgement of the inclusion of this variable is through estimating the test set results through cross-validation, but given the nonlinearity of compressive strength to the predictors, linear regression isn't worth wasting further time on as other non-linear models will produce better predictive results. For inference purposes, adj.-R-squared, BIC, and AIC suffice.\n\nOther interaction terms I tried are cement : fineaggregate, cement : coarseaggregate, cement : fineaggregate : coarseaggregate, and superplasticizer : cement. None of these increased adj. R-squared and their t-statistic p-value were > 0.05.","21d5faf6":"# 7. Preparing Data for ML","c409d86b":"# 11. Feature Importance","6a7a2321":"# 5. OLS Regression Analysis"}}