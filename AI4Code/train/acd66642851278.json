{"cell_type":{"6ff43a13":"code","c4038493":"code","ee60670f":"code","d6e0b8f1":"code","9468ec24":"code","73fa2fe5":"code","410a83cd":"code","ec43337a":"code","667a8b2f":"code","b26eda3e":"code","889e659b":"code","62f9ff99":"code","31ea5a20":"code","320331bd":"code","ada878f8":"code","ecebe1ef":"code","bbfcfd5c":"code","0de59f69":"code","11bce49c":"code","6068d6de":"code","36a4cb03":"code","86dd6f25":"code","240e2ad9":"code","b0eb9193":"code","473c41d5":"code","2eca0151":"code","974ed5ce":"code","c5662b27":"code","aabe4f0f":"code","ed7809e5":"code","e3228786":"code","f62731b9":"code","733bad69":"code","9906e44d":"code","ca90fa31":"code","253e6c06":"code","e07b63db":"code","8d10bb99":"code","37943539":"code","b21f48e5":"code","094212ea":"code","c201f8db":"code","4a5c6897":"code","ae79dac0":"code","c6061e92":"code","df3e3dfa":"code","70a6167b":"code","7db1270a":"code","073904e8":"code","8c881fa0":"code","394ddb92":"code","81b441c1":"code","c5b96320":"code","e9223f12":"code","2062ec8c":"code","dfc0b3a3":"code","68f88784":"code","38c42881":"markdown","b5ee345a":"markdown","64a5a61b":"markdown","f781ac9b":"markdown","98ce4f84":"markdown","a6e804c8":"markdown","70cebe61":"markdown","8a84551e":"markdown","78210ba4":"markdown","4cfb207f":"markdown","7769b4de":"markdown","6b9db44e":"markdown","f50ab346":"markdown","5582aafd":"markdown","f894ec56":"markdown","edb7cfa2":"markdown","4934f4f5":"markdown","97788a70":"markdown","0c7a939c":"markdown","7c665b86":"markdown","12b475e0":"markdown","8c468949":"markdown","97894a8d":"markdown","024597a5":"markdown","02c45b41":"markdown"},"source":{"6ff43a13":"from IPython.display import Image\nimport os\n!ls ..\/input\/","c4038493":"Image(\"\/kaggle\/input\/example\/example.png\")","ee60670f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d6e0b8f1":"import pandas as pd\nimport numpy as np\nimport spacy \nimport matplotlib.pyplot as plt\nfrom keras.layers import Dense\nfrom keras.utils import to_categorical\nfrom keras.models import Model, Input,Sequential\nfrom keras import layers \nimport nltk #Using for getting phonemes\nimport string #To handle special characters\nimport gensim #Library to do the doc2vec https:\/\/github.com\/RaRe-Technologies\/gensim\/blob\/develop\/docs\/notebooks\/doc2vec-lee.ipynb\nfrom sklearn.feature_extraction.text import CountVectorizer #Used to create bag of phonemes\nfrom sklearn.decomposition import PCA\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")","9468ec24":"#nltk.download('cmudict')\narpabet = nltk.corpus.cmudict.dict() #Instance of the object to be used to obtain the phonemes","73fa2fe5":"#!python -m spacy download en_core_web_sm  # Download the spacy model to use\nnlp = spacy.load('en_core_web_sm')","410a83cd":"#Opening the file \nwith open('\/kaggle\/input\/kanyewestverses\/kanye_verses.txt','r',encoding='ascii',errors='ignore') as txt:\n  data=txt.read()","ec43337a":"#Obtaining all the characters to compare them with the words that will get their phonemes and these (words) are not special characters\nall_punct=[punct for punct in string.punctuation] ","667a8b2f":"def gettingFeatures(nlp,sentence,arpabet,num_words):\n  \"\"\"Function to obtain the characteristics of the last words using Spacy and the dictionary for phonemes.\n\n  Parameters: \n              - nlp:Model downloaded from Spacy\n              - sentence:Sentence that we want to get its characteristics\n              - arpabet:Model downloaded from NLTK to obtain phonemes\n              - num_words:Number of last words from which you want to obtain their characteristics\n  Return: A tuple with the characteristics obtained.\n  \"\"\"\n  doc=nlp(sentence)\n  last_words,phonemes,tag,dep=([] for _ in range(4))\n  for token in reversed(list(doc)): #We revert to go from the last words to the first\n    if token.text not in all_punct: #We verify if the word does not belong to a special character\n      last_words.append(token.text.lower()) #Convert to lowercase because it will be a key in the phoneme dictionary\n      #Because arpabet is a dictionary (key: word, value: phonemes) it is possible that the word entered is not found in it,\n      #that's why we verify\n      try:\n        phon=arpabet[token.text.lower()][0]\n      except Exception as e:\n        phon='NA'\n      phonemes.append(phon)\n    if len(last_words) == num_words: #number of last words\n      break\n\n\n  #The variable phonemes is a list of lists, we will flatten this\n  flatten_phonemes=[phon for phons in phonemes for phon in phons]\n\n  return last_words,\" \".join(flatten_phonemes)\n\n  ","b26eda3e":"gettingFeatures(nlp,\"\"\"(Ball so hard) That shit cray, ain\u2019t it Jay? What she order, fish Filet?\"\"\",arpabet,4)","889e659b":"def creatingDf(data):\n  \"\"\"Function that will be used to create a dataFrame from the data read in the txt.\n\n  Function that receives txt data and runs it by dividing it into verses and later into sentences (rhymes)\n\u00a0\u00a0 it makes use of the gettingFeatures function to obtain the parameters and from this create a dataframe.\n\n  Parameters:\n              -data: Data previously read from txt.\n  \n  Return: A dataframe with the characteristics that were obtained by the gettingFeatures function\n  \"\"\"\n  \n  verses=list(filter(None,data.split('\\n\\n'))) #Split data in verses\n  \n  df=pd.DataFrame(columns=['Verses','Rhymes','VersesRhymes','Rhyme','Last_Words','Phonemes']) #Dataframe instance\n  \n  for index,verse in enumerate(verses): \n    \n    rhymes=verse.split('\\n') # Split rhymes in sentences\n    j=1\n    for i,rhyme in enumerate(rhymes):\n      if (i+1) % 2 == 0:\n        j-=1\n      #The special characters of the rhymes will not be removed because these will be the answers later.\n      \n      features=gettingFeatures(nlp,rhyme,arpabet,1) #Calling the function to obtain the characteristics of the sentences\n      df=df.append({'Verses':f'Verse {index+1}',\n                 'Rhymes': f'Rhyme {j}',\n                 'VersesRhymes': f'Verse {index+1} Rhyme {j}',\n                 'Rhyme' : rhyme,\n                 'Last_Words': features[0],\n                 'Phonemes': features[1]\n                }, ignore_index=True) \n        \n      j+=1\n  return df    ","62f9ff99":"rpt=creatingDf(data)","31ea5a20":"rpt[rpt.Verses == 'Verse 1'].head()","320331bd":"def usingGensim(doc,tag=None,test=False):\n  \"\"\"Function to tokenize and add a tag to the statement using Gensim.\n\n  Function for train and test data, makes use of the simple_preprocess function for tokenization and TaggedDocument to tagear them.\n\u00a0\u00a0 If it is the training data, it will tokenize and tagerate the sentence for this the parameter must be passed tag, if it is the data\n\u00a0\u00a0 test will then only be tokenized and the test parameter must be changed to True.\n  Parameters:\n              - doc: Sentence that you want to tokenize and tagear (training).\n              - tag: Tag that will be assigned to the sentence if on the training data.\n              - test: Flag to know if it is about the test or training data.\n  Return: A list with the tokens of the statement if it is for the test and a TaggedDocument object with the tokens and the tag if it is for test data.\n  \"\"\"\n  if test:\n    return gensim.utils.simple_preprocess(doc) \n  else:\n    return gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(doc), [tag])","ada878f8":"usingGensim('(Ball so hard) That shit cray, ain\u2019t it Jay? What she order, fish Filet?',tag=1)","ecebe1ef":"taggedDocuments=[]\nfor index in range(len(rpt.Rhyme)):\n  taggedDocuments.append(usingGensim(rpt.Rhyme[index],tag=index)) #The tag will be the index of each row to be able to locate it later","bbfcfd5c":"taggedDocuments[:5]","0de59f69":"modelDoc2Vec = gensim.models.doc2vec.Doc2Vec(vector_size=50, epochs=40)\nmodelDoc2Vec.build_vocab(taggedDocuments)\nmodelDoc2Vec.train(taggedDocuments, total_examples=modelDoc2Vec.corpus_count, epochs=modelDoc2Vec.epochs)","11bce49c":"cv=CountVectorizer()\nX=cv.fit_transform(rpt['Phonemes']).toarray()","6068d6de":"#Representation of phonemes in a sparse matrix\nX[:2]","36a4cb03":"pca=PCA(n_components=2)\nXpca=pca.fit_transform(X) # X reducido a 2 dimensiones","86dd6f25":"plt.scatter(Xpca[:,0],Xpca[:,1],s=10) ","240e2ad9":"pca.explained_variance_ratio_.sum() ","b0eb9193":"from sklearn.mixture import BayesianGaussianMixture\n\nbgm = BayesianGaussianMixture(n_components=5, n_init=10, random_state=42,covariance_type='tied') #100\nbgm.fit(Xpca)","473c41d5":"np.round(bgm.weights_, 2)","2eca0151":"y_pred=bgm.predict(Xpca)","974ed5ce":"dfPrueba=pd.DataFrame(Xpca)\ndfPrueba['y_pred']=y_pred","c5662b27":"import matplotlib.cm as cm\ngrups=len(set(y_pred))\nx = np.arange(grups)\nys = [i+x+(i*x)**2 for i in range(grups)]\ncolors = cm.rainbow(np.linspace(0, 1, len(ys)));\nfor i in range(grups):\n  plt.scatter(dfPrueba[0][dfPrueba.y_pred == i],dfPrueba[1][dfPrueba.y_pred == i],s=10,c=[colors[i]])","aabe4f0f":"#Incrementaremos la varianza explicada aumentando la cantidad de componentes\npca=PCA(n_components=15)\nXpca=pca.fit_transform(X) \npca.explained_variance_ratio_.sum() ","ed7809e5":"bgm = BayesianGaussianMixture(n_components=100, n_init=10, random_state=42,covariance_type='tied') \nbgm.fit(Xpca)","e3228786":"y_pred=bgm.predict(Xpca)\nrpt['y_pred_bgm']=y_pred\nrpt.head(10)","f62731b9":"df_X=pd.DataFrame(X)\ndf_X.head()","733bad69":"#Getting the target\ny=rpt.y_pred_bgm.copy()\ny.shape","9906e44d":"df_X.shape,y.shape","ca90fa31":"y_not_categorical=y\ny=to_categorical(y) ","253e6c06":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test =train_test_split(df_X,y,test_size=0.1,random_state=2019,shuffle=True)","e07b63db":"def build_model(len_row):\n\n  model=Sequential()\n  \n  model.add(layers.Dropout(0.1,input_shape=(len_row,)))\n\n  model.add(Dense(120,init='uniform',activation='tanh'))\n  \n  model.add(Dense(100,init='uniform',activation='softmax'))\n  \n  \n  \n  model.compile(optimizer=\"rmsprop\", loss='categorical_crossentropy', metrics=['accuracy'])\n  \n  return model","8d10bb99":"model = build_model(X_train.shape[1])\nmodel.summary()","37943539":"history = model.fit(X_train, y_train, batch_size=128, epochs=10, validation_data=(X_test, y_test)) ","b21f48e5":"def plot_history(title, train, test, y_title, x_title):\n  plt.title(title)\n  plt.plot(train)\n  plt.plot(test)\n  plt.ylabel(y_title)\n  plt.xlabel(x_title)\n  plt.legend(['Train', 'Test'])\n  plt.show()","094212ea":"plot_history(\"Accuracy Plot\", history.history['acc'], history.history['val_acc'], \"Accuracy\", \"Epocas\")","c201f8db":"plot_history(\"Loss Plot\", history.history['loss'], history.history['val_loss'], \"Loss\", \"Epocas\")","4a5c6897":"def makeResponse(input_sentence):\n  \"\"\"Function that responds to the message entered with a dataset rhyme.\n  \"\"\"\n  df_test=creatingDf(input_sentence)#Getting features\n  bag_of_word_input=cv.transform(df_test.Phonemes).toarray() #Representation of your phonemes through an arrangement\n  input_prediction=model.predict_classes(bag_of_word_input)  #Class prediction of the input phrase\n  #We obtain its indexes to filter our search in the doc2vec\n  index=rpt[rpt.y_pred_bgm == input_prediction[0]].index\n\n  #tokenize the sentence\n  tokens_gensim=usingGensim(input_sentence,test=True)\n  #getting its vector representation\n  input_vector = modelDoc2Vec.infer_vector(tokens_gensim)\n  #The similarities of the phrase entered with all the rhymes, gives us a list of tuples, the tuple is composed of the index and its similarity with the phrase\n  sims=modelDoc2Vec.docvecs.most_similar([input_vector], topn=len(taggedDocuments))\n  #We go through each tuple until we find the first 10 that most resemble each other and have the same class\n  index_most_simi=[]\n  for tupla in sims:\n    if tupla[0] in list(index): \n      index_most_simi.append(tupla)\n      if len(index_most_simi) == 15:\n        break\n  \n  rand_index=random.randrange(0, 15)\n\n  return rpt.Rhyme[rpt.index == index_most_simi[rand_index][0]].values[0]\n","ae79dac0":"print(makeResponse(\"How are you?\"))","c6061e92":"dataset = \"\"\"\n*greet\nHi\nHello\nHey there\n\n*affirm\nYes\nLove it\nYeah\nYep\n\n*negation\nNo\nNo, I don't \nNot at the moment\nNot really\n\n*stop\nstop\nquit\n\"\"\"","df3e3dfa":"def filter_intents_and_their_examples(dataset, intent_character = \"*\"):\n  filter_list = list(filter(None, dataset.split(\"\\n\")))\n  intents_examples = {}\n  intent = \"\"\n  for element in filter_list:\n    if element[0] == intent_character:\n      intent = element[1:]\n    else:\n      if intent not in intents_examples:\n        intents_examples[intent] = [element]\n      else:\n        intents_examples[intent].append(element)\n  return intents_examples\n\ndef transform_examples_in_vectors(intents_examples, nlp):\n  intents_vector_examples = {}\n  for key, values in intents_examples.items():\n    vector_examples = []\n    for example in values:\n      vector_examples.append(nlp(example).vector)\n    \n    intents_vector_examples[key] = vector_examples\n  return intents_vector_examples\n\ndef create_x_and_y(intents_vector_examples):\n  x = []\n  y = []\n  for label, vector_examples in intents_vector_examples.items():\n    x.extend(vector_examples)\n    y.extend([label] * len(vector_examples))\n  \n  return x, y","70a6167b":"#Getting a dic from intents with their sentences\nintents_examples = filter_intents_and_their_examples(dataset)\n\n#Transforming the sentences into vecs in the dic\nintents_vector_examples = transform_examples_in_vectors(intents_examples, nlp)\n\n#Getting intents' names\nintents = list(intents_vector_examples.keys())\n\nx, y = create_x_and_y(intents_vector_examples)","7db1270a":"intents_examples","073904e8":"from sklearn.tree import DecisionTreeClassifier\n\ndecision_tree_classifier = DecisionTreeClassifier()\ndecision_tree_classifier.fit(x, y)\n\n#Evaluacion del accuracy\naccuracy = decision_tree_classifier.score(x, y)\naccuracy","8c881fa0":"def predict_intent(sentence, nlp, model):\n  vector = nlp(sentence).vector\n  \n  intent = model.predict([vector])\n  return intent[0]","394ddb92":"dataset = \"\"\"\n*greet\nHey, what's up buddy ? Do you wanna rhyme ?\nHow is it goin' bud ? Do you wanna play with some rhymes? \nZup my nigga ? Would you like rap with me ?\n\n*affirm\nAlright, so just gimme a sentence and I'll continue with a rhyme .\nAwesome, show me what you got and I'll rhyme that.\nK, let's see what you got.\n\n*negation\nHahaha it seems there's a coward around here. So come back as soon as possible.\nAlright, go to get warm and come back soon.\nK, no worries. Hope see ya soon.\n\n*goodbye\nThat was exciting, I had fun dude. Take care man.\nYou did really good my homie, Hope see ya soon.\nThat was funny, I'll wait for your rhymes man.\n\"\"\"","81b441c1":"def filter_utterances_and_their_examples(dataset, utterance_character = \"*\"):\n  filter_list = list(filter(None, dataset.split(\"\\n\")))\n\n  utterances_examples = {}\n  utterance = \"\"\n  for element in filter_list:\n    if element[0] == utterance_character:\n      utterance = element[1:]\n    else:\n      if utterance not in utterances_examples:\n        utterances_examples[utterance] = [element]\n      else:\n        utterances_examples[utterance].append(element)\n  return utterances_examples","c5b96320":"#Getting a dictionary of utterances with their sentences\nutterances_examples = filter_utterances_and_their_examples(dataset)\nutterances_examples","e9223f12":"rules = {\n    \"greet\": \"greet\",\n    \"negation\": \"negation\",\n    \"affirm\": \"affirm\",\n    \"stop\": \"goodbye\"\n}","2062ec8c":"from random import choice\n\ndef generate_answer(utterance, utterances_examples):\n  answers = utterances_examples[utterance]\n  answer = choice(answers)\n  return answer","dfc0b3a3":"def return_answer(sentence, nlp, model, rules, utterances_examples):\n  intent = predict_intent(sentence, nlp, decision_tree_classifier)\n  \n  utterance = rules[intent]\n  \n  answer = generate_answer(utterance, utterances_examples)\n  return answer, intent","68f88784":"intent=None\nwhile True:\n  sentence = input()\n  answer, intent = return_answer(sentence, nlp, decision_tree_classifier, rules, utterances_examples)\n  print(\"bot says: {}\\n\".format(answer))\n  if intent == 'affirm':\n    while True: \n       sentence=input()\n       rhyme=makeResponse(sentence)\n       if sentence == 'stop':\n         intent='stop'\n         answer, intent = return_answer(sentence, nlp, decision_tree_classifier, rules, utterances_examples)\n         print(\"bot says: {}\\n\".format(answer))\n         break\n       print(\"bot says: {}\\n\".format(rhyme))\n\n  if intent == 'negation' or intent=='stop':\n    break","38c42881":"**This is for display purposes only.**","b5ee345a":"# **Model Results**","64a5a61b":"# **Modeling**","f781ac9b":"**Creation of the doc2vec from the Rhyme column of the dataframe**","98ce4f84":"# **Description**","a6e804c8":"**Chatbot that responds to the phrase entered with a txt phrase that rhymes with it. The last cell should not show an error if it is run locally, it is just that kaggle does not support continuous interaction (while and input). The central objective was to respond to a phrase entered with a phrase of the txt that rhymes, so the creation of intents and utterances is based on a basic approach using a decision tree that at first glance presents overfitting. The following image shows an example of what the last cell should show when it runs. I hope this notebook helps you, keep in mind that I am a newbie, so any comment of improvement will be welcome.**","70cebe61":"**We know that the rhyming classes cannot be only 5 so it will be divided into multiples of this and the components cannot be only 2 so we will apply a PCA to keep 15 components**","8a84551e":"**PCA will be applied to reduce the dimensions of the Bag of Phonemes to 2 in order to visualize how these points are and decide whether or not to cluster**","78210ba4":"## **Clustering with BayesianGaussianMixture**","4cfb207f":"**This bag of phoenemes will be the input of our classifier model**","7769b4de":"**Utterance associated with sentences**","6b9db44e":"# **Creating a DataFrame from txt**","f50ab346":"## **Dimensionality reduction**","5582aafd":"# **Bot construction**","f894ec56":"# **Predictions with the model**","edb7cfa2":"**Method that will make the intent prediction**","4934f4f5":"# **REDUCTION OF DIMENSIONALITY WITH PCA AND CLUSTERIZATION FOR RHYMES**","97788a70":"# **Importing libraries**","0c7a939c":"# **Getting data**","7c665b86":"**Intents associated with sentences**","12b475e0":"# **Creation of the bag of phonemes (based on the bag of words model)**","8c468949":"**Training the model that will predict the intents**","97894a8d":"**Defining the rules of intents with utterances**","024597a5":"# **Creation of the doc2vec**","02c45b41":"# **Data preprocessing**"}}