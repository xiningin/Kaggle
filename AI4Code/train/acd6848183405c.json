{"cell_type":{"ac5fb73c":"code","17d0642f":"code","2c0dedad":"code","abd15138":"code","2c2d8b3d":"code","304b6a43":"code","9c57cbf5":"code","03201f49":"code","153c0f18":"code","ada3a1a9":"code","3e0a6beb":"code","df847bd9":"code","0ae65c82":"code","6b41769d":"code","d839dcd1":"code","3287beef":"code","f961334c":"code","93d17242":"code","23b80419":"code","65d707a8":"code","556f0d50":"code","366fcc12":"code","eccd3e4d":"code","053efe39":"code","eec0b9f9":"code","51fbe77d":"code","77d59dcc":"code","f0b811f1":"code","2b3b4f27":"code","98a8e3cd":"code","966e90f4":"code","c2927b97":"code","5fdbc45c":"code","a60faf2e":"code","1e151a85":"code","1e3088ab":"code","e9f9677d":"code","209348c0":"code","c247e1a3":"code","7338e4bf":"code","9ba22a24":"code","a3e89e3b":"code","9c60f4a4":"code","ef4a382a":"code","08c4d0a9":"code","be1f215b":"code","c58ec5ec":"code","fe5451a3":"markdown","1654e7c7":"markdown","3598a25c":"markdown","0e7d9411":"markdown","59285827":"markdown","7247ed4f":"markdown","49e37076":"markdown","3cd9a5dc":"markdown","e4970eed":"markdown","52051ec5":"markdown","bde993fc":"markdown","dc88fd2f":"markdown","f660ee1b":"markdown","315d1c9b":"markdown","97e7ec65":"markdown","5eb6322b":"markdown","3d4056a4":"markdown","ae3edf45":"markdown","5a010661":"markdown","a694ff44":"markdown"},"source":{"ac5fb73c":"#!python -m pip install detectron2==0.4 -f \\\n#  https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu110\/torch1.7\/index.html\n","17d0642f":"!python -m pip install detectron2==0.4 -f \\\n  https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu102\/torch1.6\/index.html","2c0dedad":"%pip freeze | grep torch","abd15138":"!mkdir -p models logs configs ","2c2d8b3d":"!nvidia-smi","304b6a43":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom typing import List, Optional, Union\n\nimport torch\nimport torchvision\nimport albumentations as A\n\n\n\nimport detectron2\nfrom detectron2.data.transforms import Transform as T\nfrom detectron2.data.datasets import register_coco_instances\nfrom detectron2.data import transforms as T\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog, DatasetCatalog, DatasetMapper, build_detection_test_loader , build_detection_train_loader\nfrom detectron2.engine import DefaultTrainer\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\nfrom detectron2.data import detection_utils as utils\nfrom detectron2.config import configurable\nfrom detectron2.engine.hooks import EvalHook\nfrom detectron2.modeling import build_model\nfrom detectron2.checkpoint import DetectionCheckpointer\nfrom detectron2.utils.logger import setup_logger\nsetup_logger()\n\n\nfrom sklearn.model_selection import KFold,StratifiedKFold,StratifiedShuffleSplit,GroupKFold\nfrom sklearn.utils import check_random_state\nfrom collections import Counter, defaultdict\nimport matplotlib.pyplot as plt\nimport cv2\nimport json\nimport io\nimport os\nimport copy\nimport random\nfrom IPython.display import FileLink, FileLinks\nimport yaml\nfrom abc import ABC,ABCMeta, abstractmethod\nfrom yacs.config import CfgNode as CN","9c57cbf5":"DATASET_PATH = \"..\/input\/tacotrashdataset\"\nLOGS_PATH = \"logs\"\nMODELS_PATH = \"models\"\nCONFIG_PATH = \"configs\"","03201f49":"_C = CN()\n_C.general=CN()\n_C.general.seed = 42\n_C.general.n_folds = 5\n_C.general.tool = \"detectron2\"\n_C.general.experiment_id = \"26-04-2021\"\n_C.general.category = \"super_category\"\n_C.general.augmentations = True\n_C.general.TTA = False\n\n_C.preprocess=CN()\n_C.preprocess.height = 1500\n_C.preprocess.width = 1500\n_C.preprocess.longest_max_size = 1500\n_C.preprocess.smallest_max_size = 1000\n\n_C.model=CN()\n_C.model.base_lr = 0.001\n_C.model.num_classes = 29 #29 if super category 60 if normal category \n_C.model.model_name = \"faster_rcnn_R_101_FPN_3x\"\n_C.model.batchsize_per_image = 1024\n#_C.model.images_per_batch = 4\n_C.model.images_per_batch = 4\n_C.model.epochs = 9","153c0f18":"def get_cfg_defaults():\n    \"\"\"Get a yacs CfgNode object with default values for my_project.\"\"\"\n    # Return a clone so that the defaults will not be altered\n    # This is for the \"local variable\" use pattern\n    #return _C.clone()\n    return _C\n\ndef dump_cfg(config = get_cfg_defaults() , path = \"experiment.yaml\"):\n    \"\"\"Save a yacs CfgNode object in a yaml file in path.\"\"\"\n    stream = open(path, 'w')\n    stream.write(config.dump())\n    stream.close()\n\ndef inject_config(funct):\n    \"\"\"Inject a yacs CfgNode object in a function as first arg.\"\"\"\n    def function_wrapper(*args,**kwargs):\n        return funct(_C,*args,**kwargs)  \n    return function_wrapper\n\ndef dump_dict(config,path=\"config.yaml\"):\n        stream = open(path, 'w')\n        yaml.dump(config,stream)\n        stream.close()\n\nc=get_cfg_defaults()","ada3a1a9":"dump_cfg(path = os.path.join(LOGS_PATH , \"experiment.yaml\"))","3e0a6beb":"@inject_config\ndef seed_all(config):\n    \"\"\"\n    seed my experiments to be able to reproduce\n    \"\"\"\n    seed_value=config.general[\"seed\"]\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False","df847bd9":"annot=json.load(open(os.path.join(DATASET_PATH,\"data\/annotations.json\")))\nannot[\"annotations\"][308][\"id\"]=0\nannot[\"annotations\"][4039][\"id\"]=2197","0ae65c82":"annot_to_delete=[]\nfor idx,annotation in enumerate(annot[\"annotations\"]):\n    if (annotation[\"bbox\"][0]<0 or annotation[\"bbox\"][1]<0 or\n        annotation[\"bbox\"][2]<0 or annotation[\"bbox\"][3]<0):\n        annot_to_delete.append(idx)\nfor pos,idx in enumerate(annot_to_delete):\n    del annot[\"annotations\"][idx-pos]\n","6b41769d":"json.dump(annot,open(\"new_annotations.json\",\"w\"))","d839dcd1":"categories={ annotation[\"id\"] : annotation[\"name\"] for annotation in annot[\"categories\"]}\nsuper_categories={ annotation[\"id\"] : annotation[\"supercategory\"] for annotation in annot[\"categories\"]}","3287beef":"annot_df=pd.DataFrame(annot[\"annotations\"])\nimages_df=pd.DataFrame(annot[\"images\"])","f961334c":"images_df.describe()","93d17242":"annot_df[\"category\"]=annot_df[\"category_id\"].apply(lambda value : categories[value])\nannot_df[\"super_category\"]=annot_df[\"category_id\"].apply(lambda value : super_categories[value])\nsuper_category_to_index={value : key for key,value in enumerate(annot_df[\"super_category\"].unique())}\nannot_df[\"super_category_id\"]=annot_df[\"super_category\"].apply(lambda value : super_category_to_index[value])\nannot_df[\"normal_category_id\"]=annot_df[\"category_id\"]\nannot_df[\"normal_category\"]=annot_df[\"category\"]\nif c.general[\"category\"] != \"normal_category\":\n    annot_df[\"category_id\"]=annot_df[\"super_category_id\"]\n    annot_df[\"category\"]=annot_df[\"super_category\"]\n    annot_cat=annot_df.groupby(\"category_id\")[[\"category_id\",\"category\",\"super_category\"]].first()\n    annot_cat.columns=[\"id\",\"name\",\"supercategory\"]\n    annot[\"categories\"]=annot_cat.to_dict(\"records\")\n\n                       ","23b80419":"categories = {}\nfor id , name in zip(annot_df[\"category_id\"],annot_df[\"category\"]):\n    categories[id]=name","65d707a8":"categories","556f0d50":"annot_df.head()","366fcc12":"annot_df[\"category\"].value_counts().head(10).plot(kind=\"bar\",title=\"annotations category distribution\")","eccd3e4d":"annot_df[\"super_category\"].value_counts().head(10).plot(kind=\"bar\",title=\"annotations super category distribution\")","053efe39":"class RepeatedStratifiedGroupKFold():\n\n    def __init__(self, n_splits=5, n_repeats=1, random_state=None):\n        self.n_splits = n_splits\n        self.n_repeats = n_repeats\n        self.random_state = random_state\n        \n    def split(self, X, y=None, groups=None):\n        k = self.n_splits\n        def eval_y_counts_per_fold(y_counts, fold):\n            y_counts_per_fold[fold] += y_counts\n            std_per_label = []\n            for label in range(labels_num):\n                label_std = np.std(\n                    [y_counts_per_fold[i][label] \/ y_distr[label] for i in range(k)]\n                )\n                std_per_label.append(label_std)\n            y_counts_per_fold[fold] -= y_counts\n            return np.mean(std_per_label)\n            \n        rnd = check_random_state(self.random_state)\n        for repeat in range(self.n_repeats):\n            #print(np.max(y))\n            labels_num = np.max(y) + 1\n            y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n            y_distr = Counter()\n            for label, g in zip(y, groups):\n                y_counts_per_group[g][label] += 1\n                y_distr[label] += 1\n\n            y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n            groups_per_fold = defaultdict(set)\n        \n            groups_and_y_counts = list(y_counts_per_group.items())\n            rnd.shuffle(groups_and_y_counts)\n\n            for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n                best_fold = None\n                min_eval = None\n                for i in range(k):\n                    fold_eval = eval_y_counts_per_fold(y_counts, i)\n                    if min_eval is None or fold_eval < min_eval:\n                        min_eval = fold_eval\n                        best_fold = i\n                y_counts_per_fold[best_fold] += y_counts\n                groups_per_fold[best_fold].add(g)\n            \n            all_groups = set(groups)\n            for i in range(k):\n                train_groups = all_groups - groups_per_fold[i]\n                test_groups = groups_per_fold[i]\n\n                train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n                test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n\n                yield train_indices, test_indices\n","eec0b9f9":"@inject_config\ndef kfold_split(config,df):\n    seed_all()\n    df[\"folds\"]=-1\n    #kf = GroupKFold(n_splits=config.general[\"n_folds\"])\n    kf = RepeatedStratifiedGroupKFold(n_splits=config.general[\"n_folds\"], random_state=config.general[\"seed\"])\n    #for fold, (_, val_index) in enumerate(kf.split(df,groups=df[\"image_id\"])):\n    for fold, (_, val_index) in enumerate(kf.split(df,df.category_id, df.image_id)):\n            df.loc[val_index, \"folds\"] = fold\n    return df","51fbe77d":"annot_df=kfold_split(annot_df)","77d59dcc":"annot_df[annot_df[\"folds\"]==0][\"category\"].value_counts().head(10).plot(kind=\"bar\",title=\"Test annotations category distribution\")","f0b811f1":"annot_df[annot_df[\"folds\"]!=0][\"category\"].value_counts().head(10).plot(kind=\"bar\",title=\"Train annotations category distribution\")","2b3b4f27":"annot_df[annot_df[\"folds\"]==0][\"super_category\"].value_counts().head(10).plot(kind=\"bar\",title=\"Test annotations super category distribution\")","98a8e3cd":"annot_df[annot_df[\"folds\"]==0][\"super_category\"].value_counts().head(10).plot(kind=\"bar\",title=\"Train annotations super category distribution\")","966e90f4":"@inject_config\ndef register_dataset(config,fold):\n    train_dataset_name=f\"my_dataset_train_{fold}\"\n    test_dataset_name=f\"my_dataset_test_{fold}\"\n    train_dataset_file=f\"my_dataset_train_{fold}.json\"\n    test_dataset_file=f\"my_dataset_test_{fold}.json\"\n    \n    train_annot_df=annot_df[annot_df[\"folds\"]!=fold]\n    test_annot_df=annot_df[annot_df[\"folds\"]==fold]\n    train_annot_df=train_annot_df.drop([\"normal_category\",\"normal_category_id\"],axis=1)\n    test_annot_df=test_annot_df.drop([\"normal_category\",\"normal_category_id\"],axis=1)\n\n    train_images_df=images_df[images_df[\"id\"].apply(lambda i:True if i in list(train_annot_df[\"image_id\"].unique()) else False)]\n    test_images_df=images_df[images_df[\"id\"].apply(lambda i:True if i in list(test_annot_df[\"image_id\"].unique()) else False)]\n    \n    train_annot=annot.copy()\n    test_annot=annot.copy()\n    \n    train_annot[\"annotations\"]=train_annot_df.reset_index(drop=True).to_dict(\"records\")\n    train_annot[\"images\"]=train_images_df.reset_index(drop=True).to_dict(\"records\")\n    test_annot[\"annotations\"]=test_annot_df.reset_index(drop=True).to_dict(\"records\")\n    test_annot[\"images\"]=test_images_df.reset_index(drop=True).to_dict(\"records\")\n    \n    json.dump(train_annot,open(train_dataset_file,\"w\"))\n    json.dump(test_annot,open(test_dataset_file,\"w\"))\n    \n    if train_dataset_name in DatasetCatalog.list():\n        DatasetCatalog.remove(train_dataset_name)\n        MetadataCatalog.remove(train_dataset_name)\n    if test_dataset_name in DatasetCatalog.list():\n        DatasetCatalog.remove(test_dataset_name)\n        MetadataCatalog.remove(test_dataset_name)\n        \n    register_coco_instances(train_dataset_name, {}, train_dataset_file, os.path.join(DATASET_PATH,\"data\"))\n    register_coco_instances(test_dataset_name, {}, test_dataset_file, os.path.join(DATASET_PATH,\"data\"))\n","c2927b97":"@inject_config\ndef get_train_transforms(config):\n    return A.Compose(\n        [\n            A.OneOf([\n                A.HueSaturationValue(hue_shift_limit=0.1, sat_shift_limit= 0.1, \n                                     val_shift_limit=0.1, p=0.8),\n                A.RandomBrightnessContrast(brightness_limit=0.3, \n                                           contrast_limit=0.2, p=0.8),\n            ],p=0.7),\n            A.Rotate (limit=15, interpolation=1, border_mode=4, value=None, mask_value=None, p=0.8),\n            \n            \n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.RandomResizedCrop (config.preprocess.height, config.preprocess.width, scale=(0.8, 0.8), ratio=(0.75, 1.3333333333333333), interpolation=1, always_apply=False, p=0.1),\n            A.OneOf([\n            A.Resize(height=config.preprocess.height, width=config.preprocess.width, p=0.2),\n            A.LongestMaxSize(max_size=config.preprocess.longest_max_size, p=0.2),\n            A.SmallestMaxSize(max_size=config.preprocess.smallest_max_size, p=0.2),\n                \n            ], p=1),\n            A.CLAHE(clip_limit=[1,4],p=1),\n            \n        ], \n        p=1.0, \n        bbox_params=A.BboxParams(\n            format='coco',\n            min_area=0.5, \n            min_visibility=0.5,\n            label_fields=['category_id']\n        )\n    )\n\n@inject_config\ndef get_valid_transforms(config):\n    return A.Compose(\n        [\n            A.SmallestMaxSize(max_size=config.preprocess.smallest_max_size, p=1.0),\n            A.CLAHE(clip_limit=[3,3],p=1),   \n        ], \n        p=1.0, \n        bbox_params=A.BboxParams(\n            format='coco',\n            min_area=0.5, \n            min_visibility=0.5,\n            label_fields=['category_id']\n        )\n    )\n\ndef get_transforms(train=True):\n    if (train):\n        return get_train_transforms()\n    return get_valid_transforms()\nalbu_transformations=get_transforms(train=True)","5fdbc45c":"class PersonalMapper (detectron2.data.DatasetMapper):\n    \"\"\"\n    Define a detectron2 personal mapper in order to be able to use albumentation augmentations\n    \"\"\"\n    def __call__(self, dataset_dict):\n        \"\"\"\n        Args:\n            dataset_dict (dict): Metadata of one image, in Detectron2 Dataset format.\n\n        Returns:\n            dict: a format that builtin models in detectron2 accept\n        \"\"\"\n        dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n        # USER: Write your own image loading if it's not from a file\n        image = utils.read_image(dataset_dict[\"file_name\"], format=self.image_format)\n        #utils.check_image_size(dataset_dict, image)\n\n        \n        \n        ##### ADDED PART\n\n        #print(\"dataset dict : \",dataset_dict)\n\n        annos = [\n            obj for obj in dataset_dict[\"annotations\"]\n        ]\n        annos_bbox = [\n            obj[\"bbox\"] for obj in dataset_dict[\"annotations\"]\n        ]\n        annos_categroy_id = [\n            obj[\"category_id\"] for obj in dataset_dict.pop(\"annotations\")\n        ]\n        \n        if albu_transformations is not None:\n            transform_list=get_transforms(self.is_train)\n            image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            transform_result=transform_list(image=image,bboxes=annos_bbox,category_id=annos_categroy_id)\n            image=cv2.cvtColor(transform_result[\"image\"], cv2.COLOR_RGB2BGR)\n            annos=[annos[i] for i in range(len(transform_result[\"bboxes\"]))]\n            for i in range(len(annos)):\n                annos[i][\"bbox\"]=list(transform_result[\"bboxes\"][i])\n                annos[i][\"category_id\"]=transform_result[\"category_id\"][i]\n        \n        dataset_dict[\"annotations\"]=annos\n        \n        \n        ##### ADDED PART\n        \n        # USER: Remove if you don't do semantic\/panoptic segmentation.\n        if \"sem_seg_file_name\" in dataset_dict:\n            sem_seg_gt = utils.read_image(dataset_dict.pop(\"sem_seg_file_name\"), \"L\").squeeze(2)\n        else:\n            sem_seg_gt = None\n\n        aug_input = T.AugInput(image, sem_seg=sem_seg_gt)\n        transforms = self.augmentations(aug_input)\n        image, sem_seg_gt = aug_input.image, aug_input.sem_seg\n\n        image_shape = image.shape[:2]  # h, w\n        # Pytorch's dataloader is efficient on torch.Tensor due to shared-memory,\n        # but not efficient on large generic data structures due to the use of pickle & mp.Queue.\n        # Therefore it's important to use torch.Tensor.\n        dataset_dict[\"image\"] = torch.as_tensor(np.ascontiguousarray(image.transpose(2, 0, 1)))\n        if sem_seg_gt is not None:\n            dataset_dict[\"sem_seg\"] = torch.as_tensor(sem_seg_gt.astype(\"long\"))\n\n        # USER: Remove if you don't use pre-computed proposals.\n        # Most users would not need this feature.\n        if self.proposal_topk is not None:\n            utils.transform_proposals(\n                dataset_dict, image_shape, transforms, proposal_topk=self.proposal_topk\n            )\n\n        if not self.is_train:\n            # USER: Modify this if you want to keep them for some reason.\n            dataset_dict.pop(\"annotations\", None)\n            dataset_dict.pop(\"sem_seg_file_name\", None)\n            return dataset_dict\n\n        if \"annotations\" in dataset_dict:\n            # USER: Modify this if you want to keep them for some reason.\n            for anno in dataset_dict[\"annotations\"]:\n                if not self.use_instance_mask:\n                    anno.pop(\"segmentation\", None)\n                if not self.use_keypoint:\n                    anno.pop(\"keypoints\", None)\n\n            # USER: Implement additional transformations if you have other types of data\n            annos = [\n                utils.transform_instance_annotations(\n                    obj, transforms, image_shape, keypoint_hflip_indices=self.keypoint_hflip_indices\n                )\n                for obj in dataset_dict.pop(\"annotations\")\n                if obj.get(\"iscrowd\", 0) == 0\n            ]\n            instances = utils.annotations_to_instances(\n                annos, image_shape, mask_format=self.instance_mask_format\n            )\n\n            # After transforms such as cropping are applied, the bounding box may no longer\n            # tightly bound the object. As an example, imagine a triangle object\n            # [(0,0), (2,0), (0,2)] cropped by a box [(1,0),(2,2)] (XYXY format). The tight\n            # bounding box of the cropped triangle should be [(1,0),(2,1)], which is not equal to\n            # the intersection of original bounding box and the cropping box.\n            if self.recompute_boxes:\n                instances.gt_boxes = instances.gt_masks.get_bounding_boxes()\n            dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n        return dataset_dict","a60faf2e":"class PersonalTrainer (detectron2.engine.defaults.DefaultTrainer):\n    \"\"\"\n    Personal trainer based on detectron2 DefaultTrainer to add some hooks and change data loaders\n    \"\"\"\n    \n    def __init__(self, cfg , config=c):\n        super().__init__(cfg)\n        self.metric=0\n        self.checkpointer.save_dir=MODELS_PATH\n\n        \n    def build_hooks(self):\n        hooks = super().build_hooks()\n        def save_best_model():\n            \n            metric=self.test(self.cfg, self.model)[\"bbox\"][\"AP50\"]\n            if(metric>self.metric):\n                self.metric=metric\n                self.checkpointer.save(\"best_model\") # it will add .pth alone\n                \n        steps_per_epoch=annot_df.shape[0]\/\/c.model[\"images_per_batch\"]\n        model_checkpointer=EvalHook(steps_per_epoch, save_best_model)\n        hooks.insert(-1,model_checkpointer)\n        return hooks\n    \n    @classmethod\n    def build_train_loader(cls, cfg):\n        \n        #return build_detection_train_loader(cfg,mapper=DatasetMapper(cfg,is_train=True,))\n        return build_detection_train_loader(cfg,mapper=PersonalMapper(cfg,is_train=True,augmentations=[]))\n    @classmethod\n    def build_test_loader(cls, cfg, dataset_name):\n        \n        #return build_detection_test_loader( cfg,dataset_name,mapper=DatasetMapper(cfg,is_train=False,))\n        return build_detection_test_loader( cfg,dataset_name,mapper=PersonalMapper(cfg,is_train=False,augmentations=[]))\n\n    \n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name):\n        return COCOEvaluator(dataset_name, (\"bbox\",), False, output_dir=None)","1e151a85":"@inject_config\ndef get_config(config,fold=0):\n    \"\"\"\n    Detectron2 config\n    \"\"\"\n    steps_per_epoch=annot_df.shape[0]\/\/config.model[\"images_per_batch\"]\n    train_dataset_name=f\"my_dataset_train_{fold}\"\n    test_dataset_name=f\"my_dataset_test_{fold}\"\n    cfg = get_cfg()\n    cfg.MODEL.DEVICE='cuda' if torch.cuda.is_available() else 'cpu'\n    cfg.merge_from_file(model_zoo.get_config_file(f\"COCO-Detection\/{config.model['model_name']}.yaml\"))\n    cfg.DATASETS.TRAIN = (train_dataset_name,)\n    cfg.DATASETS.TEST = (test_dataset_name,)\n    cfg.DATALOADER.NUM_WORKERS = 4\n    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(f\"COCO-Detection\/{config.model['model_name']}.yaml\")  # Let training initialize from model zoo\n    cfg.SOLVER.IMS_PER_BATCH = config.model[\"images_per_batch\"]\n    cfg.SOLVER.BASE_LR = config.model[\"base_lr\"]  # pick a good LR\n    cfg.SOLVER.MAX_ITER = steps_per_epoch*config.model[\"epochs\"]  # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n    cfg.SOLVER.STEPS = (steps_per_epoch*8,)\n    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = config.model[\"batchsize_per_image\"]   # faster, and good enough for this toy dataset (default: 512)\n    cfg.MODEL.ROI_HEADS.NUM_CLASSES = config.model[\"num_classes\"]  # only has one class (ballon). (see https:\/\/detectron2.readthedocs.io\/tutorials\/datasets.html#update-the-config-for-new-datasets)\n    cfg.TEST.EVAL_PERIOD=0\n    cfg.OUTPUT_DIR = LOGS_PATH\n    cfg.OUTPUT_DIR_BEST = LOGS_PATH\n    cfg.SOLVER.AMP.ENABLED = True\n    #cfg.MODEL.WEIGHTS = \"..\/input\/trash-taco-heavy-augs\/models\/best_model.pth\"\n\n    cfg.SEED = config.general[\"seed\"]\n\n    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n    os.makedirs(cfg.OUTPUT_DIR_BEST, exist_ok=True)\n    return cfg\n","1e3088ab":"cfg=get_config()","e9f9677d":"def train(fold):\n    \"\"\"\n    train function that help train on the dataset and validate on a certain fold\n    \"\"\"\n    seed_all()\n    register_dataset(fold)\n    cfg=get_config(fold)\n    #trainer = DefaultTrainer(cfg)\n    trainer = PersonalTrainer(cfg) \n    trainer.resume_or_load(resume=False)\n    trainer.evaluator = COCOEvaluator(f\"my_dataset_test_{fold}\", (\"bbox\",), False, output_dir=None)\n    trainer.train()\n    ","209348c0":"train(0)","c247e1a3":"with open(os.path.join(CONFIG_PATH,\"detectron_config.yaml\"),\"w\") as f:\n    f.write(get_config().dump())","7338e4bf":"%ls logs","9ba22a24":"%ls models","a3e89e3b":"%ls configs","9c60f4a4":"metrics={}","ef4a382a":"cfg=get_config()\ncfg.MODEL.WEIGHTS = os.path.join(MODELS_PATH, \"best_model.pth\")  # path to the model we just trained\nmodel = build_model(cfg)\nm=DetectionCheckpointer(model).load(cfg.MODEL.WEIGHTS)","08c4d0a9":"evaluator = COCOEvaluator(\"my_dataset_test_0\", (\"bbox\",), False, output_dir=LOGS_PATH)\nval_loader = build_detection_test_loader(cfg, \"my_dataset_test_0\")\ntrain_metric=inference_on_dataset(model, val_loader, evaluator)\nmetrics[\"train_metric\"]=train_metric","be1f215b":"evaluator = COCOEvaluator(\"my_dataset_train_0\", (\"bbox\",), False, output_dir=LOGS_PATH)\nval_loader = build_detection_test_loader(cfg, \"my_dataset_train_0\")\nvalid_metric=inference_on_dataset(model, val_loader, evaluator)\nmetrics[\"valid_metric\"]=valid_metric","c58ec5ec":"dump_dict(metrics,os.path.join(LOGS_PATH,\"metrics.yaml\"))","fe5451a3":"## Categories and Super categories dict","1654e7c7":"## Preprocess","3598a25c":"# Config","0e7d9411":"# Imports and utils","59285827":"# Kfold","7247ed4f":"# ENV","49e37076":"# FIX annotation duplicated ids and negative bboxes\n\nrepeated annotations idx \n\n308 => 0\n\n4039  =>2197\n","3cd9a5dc":"## delete negative BBOX","e4970eed":"# Register Dataset","52051ec5":"# Prepare trainer","bde993fc":"# Build Trainer","dc88fd2f":"# Prepare Output","f660ee1b":"# Preprocess and augmentations ","315d1c9b":"# Personal mapper","97e7ec65":"# Test Dataset (without validation augmentation)","5eb6322b":"# SEED","3d4056a4":"### Choose between normal categories or super categories","ae3edf45":"## Visualization","5a010661":"# Prepare config params","a694ff44":"# Annotation Preprocess and Visualization"}}