{"cell_type":{"243db9dd":"code","c29efd7e":"code","45aa6e6a":"code","518744f5":"code","30ae66bc":"code","f1eeacb5":"code","0bede7a6":"code","9378b906":"code","4ba1d97c":"code","f3c437a3":"code","e11463b8":"code","a4b104a6":"code","76b0bda1":"code","38bf5f1c":"code","de59fa8c":"code","f83867f1":"code","2e9bd950":"code","d4d05f34":"code","071c4be2":"code","9cdb8c05":"code","ae52fc0d":"code","7d830594":"code","c86bb3bb":"code","150f20ee":"code","ff7f5cd0":"code","3a3c4413":"code","190129b2":"code","3a3bb5fb":"code","5ef4ad1f":"code","d55ecc1d":"code","fdcdb983":"code","9094dc63":"code","c2616da2":"code","b6f58cdd":"code","c25ce705":"code","933a3923":"code","83a8734a":"code","2b39a313":"code","4eb64c12":"code","2de78563":"code","e57a6ef5":"code","6bc3f100":"code","a2048083":"code","ae987b50":"code","febb4da5":"code","acf2abc3":"code","a97c4577":"code","5a980508":"code","4b1c104f":"code","1b63d085":"code","582ec2f0":"code","2f563093":"code","052eb5f6":"code","08fcf1b6":"code","861c3cd7":"code","2d948af7":"code","e11230cd":"code","0b06ee3d":"code","1a596abe":"code","58879255":"code","c0d72405":"code","45fcb6a1":"code","e32d183e":"code","a98dc05c":"code","6c234624":"code","7404f6eb":"code","b9811beb":"code","f18699cd":"code","7e1f45fa":"code","544c10d1":"code","785e4831":"code","96c4e84a":"code","acc423cb":"code","e33663fe":"code","9574e23c":"code","4f7ef064":"code","2850142a":"code","7e71aa86":"code","0d9a155c":"code","e61cf957":"code","7ead41e1":"code","3bd07eaf":"code","bbc2f8fb":"code","e262d6c4":"code","90a658ed":"code","35577a73":"code","e1050f55":"code","76f00b16":"code","8b335f85":"code","7f9e2a7e":"code","fb178175":"code","630d4854":"code","ec6cb44f":"code","618a32b4":"code","996d0f8a":"code","aba828b5":"code","cd898f02":"code","7b814f36":"code","68a29cb6":"code","6ad9811e":"code","7af4cce4":"code","c8f2430d":"code","35033d51":"code","a9f9785d":"code","835a1efe":"code","c427155e":"code","f1180540":"code","2b5211d9":"code","6a8d3e58":"code","505f00e5":"code","2e56140f":"code","8d9052fe":"code","723f8790":"markdown","4361e910":"markdown","04dc7ebd":"markdown","c4fd0686":"markdown","96e3dc1c":"markdown","e6702b39":"markdown","9f70c34c":"markdown","e04954a0":"markdown","5090a378":"markdown","f751211f":"markdown","ed6c14b8":"markdown","6851eebd":"markdown","ad8efae6":"markdown","3f0a5014":"markdown","c175a9b9":"markdown","ed53b096":"markdown","8c489202":"markdown","0454095d":"markdown","2e97ca89":"markdown","89299578":"markdown","e0ab76d4":"markdown","f9cffc51":"markdown","7fa1d586":"markdown","0587cc10":"markdown","5f97c4df":"markdown","34f9bce9":"markdown","d7b73f4c":"markdown","cb8abc08":"markdown","4eb9bebc":"markdown","7c46cd03":"markdown"},"source":{"243db9dd":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\nimport time\n#PATH PROCESS\nimport os\nimport os.path\nfrom pathlib import Path\nimport glob\n#IMAGE PROCESS\nfrom PIL import Image\nfrom keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport cv2\nfrom keras.applications.vgg16 import preprocess_input, decode_predictions\nimport imageio\nfrom IPython.display import Image\nimport matplotlib.image as mpimg\nfrom skimage.transform import resize\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nimport zipfile\nfrom io import BytesIO\nfrom nibabel import FileHolder\nfrom nibabel.analyze import AnalyzeImage\nimport PIL\nfrom IPython import display\nfrom skimage.morphology import convex_hull_image, erosion\nfrom skimage.morphology import square\nfrom skimage.feature import hessian_matrix, hessian_matrix_eigvals\nfrom skimage import data, io, filters\nimport skimage\n#SCALER & TRANSFORMATION\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras import regularizers\nfrom sklearn.preprocessing import LabelEncoder\n#ACCURACY CONTROL\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n#OPTIMIZER\nfrom keras.optimizers import RMSprop,Adam,Optimizer,Optimizer, SGD\n#MODEL LAYERS\nfrom tensorflow.keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n                        Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN,\\\nLSTM, GlobalAveragePooling2D, SeparableConv2D, ZeroPadding2D, Convolution2D, ZeroPadding2D,Reshape, Conv2DTranspose, LeakyReLU, ReLU\nfrom keras import models\nfrom keras import layers\nimport tensorflow as tf\nfrom keras.applications import VGG16,VGG19,inception_v3\nfrom keras import backend as K\nfrom keras.utils import plot_model\nfrom keras.datasets import mnist\nimport keras\n#IGNORING WARNINGS\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\",category=DeprecationWarning)\nfilterwarnings(\"ignore\", category=FutureWarning) \nfilterwarnings(\"ignore\", category=UserWarning)","c29efd7e":"ADL02od_PCT = \"..\/input\/largest-dataset-mapping-human-brain\/Brain_Data_Video\/ADL02od_pct.mp4\"\nADM10t = \"..\/input\/largest-dataset-mapping-human-brain\/Brain_Data_Video\/ADM10t.mp4\"\nAPL = \"..\/input\/largest-dataset-mapping-human-brain\/Brain_Data_Video\/APL.mp4\"\nAVL01lo_PCT = \"..\/input\/largest-dataset-mapping-human-brain\/Brain_Data_Video\/AVL01lo_pct.mp4\"\nCentral_Complex_One = \"..\/input\/largest-dataset-mapping-human-brain\/Brain_Data_Video\/Central_Complex.mp4\"\nCentral_Complex_Two = \"..\/input\/largest-dataset-mapping-human-brain\/Brain_Data_Video\/Central_Complex_Two.mp4\"\nEM = \"..\/input\/largest-dataset-mapping-human-brain\/Brain_Data_Video\/EM_Data.mp4\"\nExR3 = \"..\/input\/largest-dataset-mapping-human-brain\/Brain_Data_Video\/ExR3.mp4\"\nFull_Connected = \"..\/input\/largest-dataset-mapping-human-brain\/Brain_Data_Video\/Full_Connected.mp4\"\nHemibrain = \"..\/input\/largest-dataset-mapping-human-brain\/Brain_Data_Video\/Hemibrain.mp4\"\nMBON01 = \"..\/input\/largest-dataset-mapping-human-brain\/Brain_Data_Video\/MBON01.mp4\"\nOlfactory_LN = \"..\/input\/largest-dataset-mapping-human-brain\/Brain_Data_Video\/olfactory_LN.mp4\"\nOvilN = \"..\/input\/largest-dataset-mapping-human-brain\/Brain_Data_Video\/ovilN.mp4\"","45aa6e6a":"Neuron_List = [ADL02od_PCT,ADM10t,APL,AVL01lo_PCT,ExR3,MBON01,Olfactory_LN,OvilN]","518744f5":"Neuron_IMG = []\n\nfor neuron_video in Neuron_List:\n    \n    Video_Path = neuron_video\n    \n    Video_Caption = cv2.VideoCapture(Video_Path)\n    \n    while Video_Caption.isOpened():\n        \n        _,frame = Video_Caption.read()\n        \n        if _ != True:\n            break\n            \n        if Video_Caption.isOpened():\n            \n            Transformation_IMG = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n            Resized_IMG = cv2.resize(Transformation_IMG,(180,180))\n            Neuron_IMG.append(Resized_IMG)\n            \n    Video_Caption.release()","30ae66bc":"print(len(Neuron_IMG))","f1eeacb5":"print(np.shape(np.array(Neuron_IMG)))","0bede7a6":"EM_IMG = []\n\nVideo_Path = EM\n\nVideo_Caption = cv2.VideoCapture(Video_Path)\n\nwhile Video_Caption.isOpened():\n    \n    _,frame = Video_Caption.read()\n    \n    if _ != True:\n        break\n    if Video_Caption.isOpened():\n        Transformation_IMG = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n        Resized_IMG = cv2.resize(Transformation_IMG,(180,180))\n        EM_IMG.append(Resized_IMG)","9378b906":"print(len(EM_IMG))","4ba1d97c":"print(np.shape(np.array(EM_IMG)))","f3c437a3":"Central_Complex_List = [Central_Complex_One,Central_Complex_Two]","e11463b8":"Central_IMG = []\n\nfor central_video in Central_Complex_List:\n    \n    Video_Path = central_video\n    \n    Video_Caption = cv2.VideoCapture(Video_Path)\n    \n    while Video_Caption.isOpened():\n        \n        _,frame = Video_Caption.read()\n        \n        if _ != True:\n            break\n            \n        if Video_Caption.isOpened():\n            \n            Transformation_IMG = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n            Resized_IMG = cv2.resize(Transformation_IMG,(180,180))\n            Central_IMG.append(Resized_IMG)\n            \n    Video_Caption.release()","a4b104a6":"print(len(Central_IMG))","76b0bda1":"print(np.shape(np.array(Central_IMG)))","38bf5f1c":"Hemibrain_IMG = []\n\nVideo_Path = Hemibrain\n\nVideo_Caption = cv2.VideoCapture(Video_Path)\n\nwhile Video_Caption.isOpened():\n    \n    _,frame = Video_Caption.read()\n    \n    if _ != True:\n        break\n    if Video_Caption.isOpened():\n        Transformation_IMG = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n        Resized_IMG = cv2.resize(Transformation_IMG,(180,180))\n        Hemibrain_IMG.append(Resized_IMG)","de59fa8c":"print(len(Hemibrain_IMG))","f83867f1":"print(np.shape(np.array(Hemibrain_IMG)))","2e9bd950":"Connected_IMG = []\n\nVideo_Path = Full_Connected\n\nVideo_Caption = cv2.VideoCapture(Video_Path)\n\nwhile Video_Caption.isOpened():\n    \n    _,frame = Video_Caption.read()\n    \n    if _ != True:\n        break\n    if Video_Caption.isOpened():\n        Transformation_IMG = cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n        Resized_IMG = cv2.resize(Transformation_IMG,(180,180))\n        Connected_IMG.append(Resized_IMG)","d4d05f34":"print(len(Connected_IMG))","071c4be2":"print(np.shape(np.array(Connected_IMG)))","9cdb8c05":"def display_single(image):\n    \n    figure = plt.figure(figsize=(7,7))\n    \n    Single_Image = image\n    \n    plt.xlabel(Single_Image.shape)\n    plt.ylabel(Single_Image.size)\n    plt.imshow(Single_Image)","ae52fc0d":"def display_threshold(image):\n    \n    figure = plt.figure(figsize=(7,7))\n    \n    Single_Image = image\n    _,Threshold_Image = cv2.threshold(Single_Image,190,200,cv2.THRESH_BINARY)\n    \n    plt.xlabel(Threshold_Image.shape)\n    plt.ylabel(Threshold_Image.size)\n    plt.title(\"THRESHOLD\")\n    plt.imshow(Threshold_Image)","7d830594":"def reading_threshold(image):\n    \n    Single_Image = image\n    _,Threshold_Image = cv2.threshold(Single_Image,190,200,cv2.THRESH_BINARY)\n    \n    return Threshold_Image","c86bb3bb":"def display_adaptive_threshold(image):\n    \n    figure = plt.figure(figsize=(7,7))\n    \n    Single_Image = image[:,:,0]\n    Adaptive_Image = cv2.adaptiveThreshold(Single_Image,20,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,9,2)\n    \n    plt.xlabel(Adaptive_Image.shape)\n    plt.ylabel(Adaptive_Image.size)\n    plt.title(\"ADAPTIVE\")\n    plt.imshow(Adaptive_Image)","150f20ee":"def reading_adaptive(image):\n    \n    Single_Image = image[:,:,0]\n    Adaptive_Image = cv2.adaptiveThreshold(Single_Image,20,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,9,2)\n    \n    return Adaptive_Image","ff7f5cd0":"def display_canny(image):\n    \n    figure = plt.figure(figsize=(7,7))\n    \n    Single_Image = image\n    Canny_Image = cv2.Canny(Single_Image,10,100)\n    \n    plt.xlabel(Canny_Image.shape)\n    plt.ylabel(Canny_Image.size)\n    plt.title(\"CANNY\")\n    plt.imshow(Canny_Image)","3a3c4413":"def reading_canny(image):\n    \n    Single_Image = image\n    Canny_Image = cv2.Canny(Single_Image,10,100)\n    \n    return Canny_Image","190129b2":"def display_bitwise_and(image):\n    \n    figure = plt.figure(figsize=(7,7))\n    \n    Single_Image = image\n    Threshold_Image = reading_threshold(Single_Image)\n    \n    Mask_For_Image = cv2.inRange(Single_Image,Single_Image,Threshold_Image)\n    Bitwise_And_Image = cv2.bitwise_and(Single_Image,Single_Image,mask=Mask_For_Image)\n    \n    plt.xlabel(Bitwise_And_Image.shape)\n    plt.ylabel(Bitwise_And_Image.size)\n    plt.title(\"BITWISE AND\")\n    plt.imshow(Bitwise_And_Image)","3a3bb5fb":"def display_hessian(image,sigma,order):\n    \n    figure,axis = plt.subplots(1,2,figsize=(7,7))\n    \n    Single_Image = image\n    Gray_Image = cv2.cvtColor(Single_Image,cv2.COLOR_RGB2GRAY)\n    \n    Hessian_IMG = hessian_matrix(Gray_Image,sigma=sigma,order=order)\n    maxima_IMG,minima_IMG = hessian_matrix_eigvals(Hessian_IMG)\n    \n    axis[0].set_xlabel(maxima_IMG.shape)\n    axis[0].set_ylabel(maxima_IMG.size)\n    axis[0].set_title(\"MAX\")\n    axis[0].imshow(maxima_IMG,cmap=\"Greys_r\")\n    \n    axis[1].set_xlabel(minima_IMG.shape)\n    axis[1].set_ylabel(minima_IMG.size)\n    axis[1].set_title(\"MIN\")\n    axis[1].imshow(minima_IMG,cmap=\"Greys_r\")","5ef4ad1f":"def display_skeleton(image):\n    \n    figure,axis = plt.subplots(1,2,figsize=(7,7))\n    \n    Single_Image = image\n    Gray_Image = cv2.cvtColor(Single_Image,cv2.COLOR_RGB2GRAY)\n    Threshold_Image = reading_threshold(Gray_Image)\n    Array_Image = np.array(Gray_Image > Threshold_Image).astype(int)\n    \n    Skeleton_Image = skimage.morphology.skeletonize(Array_Image)\n    \n    axis[0].set_xlabel(Gray_Image.shape)\n    axis[0].set_ylabel(Gray_Image.size)\n    axis[0].set_title(\"GRAY\")\n    axis[0].imshow(Gray_Image)\n    \n    axis[1].set_xlabel(Skeleton_Image.shape)\n    axis[1].set_ylabel(Skeleton_Image.size)\n    axis[1].set_title(\"SKELETON\")\n    axis[1].imshow(Skeleton_Image)","d55ecc1d":"def reading_skeleton(image):\n    \n    Single_Image = image\n    Gray_Image = cv2.cvtColor(Single_Image,cv2.COLOR_RGB2GRAY)\n    Threshold_Image = reading_threshold(Gray_Image)\n    Array_Image = np.array(Gray_Image > Threshold_Image).astype(int)\n    \n    Skeleton_Image = skimage.morphology.skeletonize(Array_Image)\n    \n    return Skeleton_Image","fdcdb983":"def display_histogram(image):\n    \n    Single_Image = image\n    \n    colors = (\"red\", \"green\", \"blue\")\n    channel_dim = (0, 1, 2)\n\n    plt.xlim([0, 255])\n    plt.ylim([0, 150])\n\n    for channel_id, c in zip(channel_dim, colors):\n        histogram, bin_edges = np.histogram(\n            Single_Image[:, :, channel_id], bins=256, range=(0, 256)\n        )\n        plt.plot(bin_edges[0:-1], histogram, color=c)\n\n    plt.xlabel(\"Color value\")\n    plt.ylabel(\"Pixels\")","9094dc63":"plt.style.use(\"dark_background\")","c2616da2":"display_single(Neuron_IMG[0])","b6f58cdd":"display_single(EM_IMG[0])","c25ce705":"display_single(Central_IMG[0])","933a3923":"display_single(Hemibrain_IMG[0])","83a8734a":"display_single(Connected_IMG[0])","2b39a313":"figure,axis = plt.subplots(5,5,figsize=(10,10))\n\nfor indexing,operation in enumerate(axis.flat):\n    \n    Picking_Image = Neuron_IMG[indexing]\n    operation.set_xlabel(Picking_Image.shape)\n    operation.set_ylabel(Picking_Image.size)\n    operation.set_title(\"NEURON\")\n    operation.imshow(Picking_Image)\n    \nplt.tight_layout()\nplt.show()","4eb64c12":"display_threshold(Neuron_IMG[0])","2de78563":"display_threshold(Neuron_IMG[10])","e57a6ef5":"display_threshold(Neuron_IMG[200])","6bc3f100":"display_threshold(EM_IMG[1000])","a2048083":"display_threshold(Central_IMG[123])","ae987b50":"display_threshold(Connected_IMG[3])","febb4da5":"display_canny(Neuron_IMG[10])","acf2abc3":"display_canny(Neuron_IMG[128])","a97c4577":"display_canny(Neuron_IMG[233])","5a980508":"display_canny(Connected_IMG[3])","4b1c104f":"display_canny(Central_IMG[3])","1b63d085":"display_hessian(Central_IMG[3],0.15,\"rc\")","582ec2f0":"display_hessian(Neuron_IMG[3],0.15,\"rc\")","2f563093":"display_hessian(Neuron_IMG[300],0.15,\"rc\")","052eb5f6":"display_hessian(EM_IMG[300],0.15,\"rc\")","08fcf1b6":"display_hessian(EM_IMG[1378],0.15,\"rc\")","861c3cd7":"display_skeleton(Neuron_IMG[0])","2d948af7":"display_skeleton(Neuron_IMG[100])","e11230cd":"display_skeleton(Central_IMG[100])","0b06ee3d":"figure = plt.figure(figsize=(10,10))\n\nplt.imshow(reading_skeleton(Neuron_IMG[200]))\nplt.axis(\"off\")","1a596abe":"figure = plt.figure(figsize=(10,10))\n\nplt.imshow(reading_skeleton(Neuron_IMG[300]))\nplt.axis(\"off\")","58879255":"figure = plt.figure(figsize=(10,10))\n\nplt.imshow(reading_skeleton(Central_IMG[300]))\nplt.axis(\"off\")","c0d72405":"figure = plt.figure(figsize=(10,10))\n\nplt.imshow(reading_skeleton(EM_IMG[1300]))\nplt.axis(\"off\")","45fcb6a1":"figure,axis = plt.subplots(5,5,figsize=(10,10))\n\nfor indexing,operation in enumerate(axis.flat):\n    \n    Picking_Image = Neuron_IMG[indexing*10]\n    Skeleton_Image = reading_skeleton(Picking_Image)\n    \n    operation.set_xlabel(Skeleton_Image.shape)\n    operation.set_ylabel(Skeleton_Image.size)\n    operation.set_title(\"NEURON\")\n    operation.imshow(Skeleton_Image)\n    \nplt.tight_layout()\nplt.show()","e32d183e":"display_histogram(Neuron_IMG[300])","a98dc05c":"display_histogram(Neuron_IMG[3])","6c234624":"display_histogram(Neuron_IMG[123])","7404f6eb":"display_histogram(Neuron_IMG[323])","b9811beb":"display_histogram(Central_IMG[23])","f18699cd":"display_histogram(Central_IMG[2])","7e1f45fa":"iterations = 60\nvector_noise_shape = 180\ncount_example = 9\nbatch_size = 3\ncount_buffer_time = 60000","544c10d1":"seed = tf.random.normal([count_example,vector_noise_shape])","785e4831":"X_Train = np.array(Neuron_IMG)\n\nX_Train = X_Train.astype(\"float32\")\n\nX_Train = (X_Train - 127.5) \/ 127.5","96c4e84a":"print(\"NEURON SHAPE: \",X_Train.shape)","acc423cb":"Y_Train = np.array(Central_IMG)\n\nY_Train = Y_Train.astype(\"float32\")\n\nY_Train = (Y_Train - 127.5) \/ 127.5","e33663fe":"print(\"NEURON SHAPE: \",Y_Train.shape)","9574e23c":"plt.imshow(X_Train[0])","4f7ef064":"plt.imshow(X_Train[600])","2850142a":"plt.imshow(Y_Train[0])","7e71aa86":"plt.imshow(Y_Train[652])","0d9a155c":"Train_Data = tf.data.Dataset.from_tensor_slices(Y_Train).shuffle(count_buffer_time).batch(batch_size) # for connected central complex neurons","e61cf957":"Train_Data_Two = tf.data.Dataset.from_tensor_slices(X_Train).shuffle(count_buffer_time).batch(batch_size) # for specific neurons, if you want","7ead41e1":"print(Train_Data.element_spec)","3bd07eaf":"print(Train_Data_Two.element_spec)","bbc2f8fb":"def Generator_Model():\n    \n    \n    Model = Sequential()\n    #\n    Model.add(Dense(90*90*128,use_bias=False,input_shape=(180,)))\n    Model.add(BatchNormalization())\n    Model.add(LeakyReLU())\n    #\n    Model.add(Reshape((90,90,128)))\n    #\n    Model.add(Conv2DTranspose(128,(3,3),padding=\"same\",use_bias=False))\n    Model.add(BatchNormalization())\n    Model.add(LeakyReLU())\n    \n    Model.add(Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', use_bias=False))\n    Model.add(BatchNormalization())\n    Model.add(LeakyReLU())\n    #\n    Model.add(Conv2DTranspose(3,(3,3),padding=\"same\",use_bias=False,activation=\"tanh\"))\n    \n    \n    return Model","e262d6c4":"Generator = Generator_Model()","90a658ed":"print(Generator.summary())","35577a73":"def Discriminator_Model():\n    \n    model = Sequential()\n    \n    model.add(Conv2D(64,(3,3),padding=\"same\",input_shape=[180,180,3]))\n    model.add(Dropout(0.3))\n    model.add(LeakyReLU())\n    \n    \n    model.add(Conv2D(128,(3,3),padding=\"same\"))\n    model.add(Dropout(0.3))\n    model.add(LeakyReLU())\n    \n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n    \n    return model","e1050f55":"Discriminator = Discriminator_Model()","76f00b16":"print(Discriminator.summary())","8b335f85":"Generator_Optimizer = tf.keras.optimizers.RMSprop(lr=0.0004,clipvalue=1.0,decay=1e-8)\nDiscriminator_Optimizer = tf.keras.optimizers.RMSprop(lr=0.0004,clipvalue=1.0,decay=1e-8)","7f9e2a7e":"Loss_Function = tf.keras.losses.BinaryCrossentropy(from_logits=True)","fb178175":"def Discriminator_Loss(real_out,fake_out):\n    \n    real_loss = Loss_Function(tf.ones_like(real_out),real_out)\n    fake_loss = Loss_Function(tf.zeros_like(fake_out),fake_out)\n    total_loss = real_loss + fake_loss\n    \n    return total_loss","630d4854":"def Generator_Loss(fake_out):\n    \n    return Loss_Function(tf.ones_like(fake_out),fake_out)","ec6cb44f":"def display_and_save_images(model, epoch, test_input):\n    \n    predictions = model(test_input, training=False)\n    fig = plt.figure(figsize=(12, 12))\n    \n    for i in range(predictions.shape[0]):\n        plt.subplot(4, 4, i+1)\n        plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5)\n        plt.axis('off')\n\n    plt.savefig('output_image{:04d}.png'.format(epoch))\n    plt.show()","618a32b4":"def Train_Step(images):\n    \n    random_noise_vector = tf.random.normal([batch_size,vector_noise_shape])\n    \n    with tf.GradientTape() as Generator_Tape, tf.GradientTape() as Discriminator_Tape:\n        Generator_Fake_Images = Generator(random_noise_vector,training=False)\n        \n        real_out = Discriminator(images,training=True)\n        fake_out = Discriminator(Generator_Fake_Images,training=True)\n        \n        Generator_Loss_Out = Generator_Loss(fake_out)\n        Discriminator_Loss_Out = Discriminator_Loss(real_out,fake_out)\n        \n    Generator_Gradients = Generator_Tape.gradient(Generator_Loss_Out,Generator.trainable_variables)\n    Discriminator_Gradients = Discriminator_Tape.gradient(Discriminator_Loss_Out,Discriminator.trainable_variables)\n    \n    Generator_Optimizer.apply_gradients(zip(Generator_Gradients,Generator.trainable_variables))\n    Discriminator_Optimizer.apply_gradients(zip(Discriminator_Gradients,Discriminator.trainable_variables))","996d0f8a":"def Training(dataset,iterations):\n    \n    for epoch in range(iterations):\n        \n        start = time.time()\n        \n        for image_batch in dataset:\n            Train_Step(image_batch)\n            \n        display.clear_output(wait=True)\n        display_and_save_images(Generator,epoch+1,seed)\n    \n    display.clear_output(wait=True)\n    display_and_save_images(Generator,epoch,seed)","aba828b5":"Training(Train_Data,iterations)","cd898f02":"Predict_Generator_Noise = tf.random.normal(shape=[50,vector_noise_shape])","7b814f36":"Generator_Predict = Generator(Predict_Generator_Noise)","68a29cb6":"figure, axes = plt.subplots(nrows=3,ncols=3,figsize=(10,10))\n\nfor i,ax in enumerate(axes.flat):\n    Prediction_Output = Generator_Predict[i]\n    ax.imshow(Prediction_Output,cmap=\"gray\")\n    ax.set_xlabel(Generator_Predict[i].shape)\nplt.tight_layout()\nplt.show()","6ad9811e":"figure = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.imshow(Generator_Predict[25])\nplt.show()","7af4cce4":"figure = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.imshow(Generator_Predict[20])\nplt.show()","c8f2430d":"figure = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.imshow(Generator_Predict[5])\nplt.show()","35033d51":"figure = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.imshow(Generator_Predict[3])\nplt.show()","a9f9785d":"figure = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.imshow(Generator_Predict[44])\nplt.show()","835a1efe":"Training(Train_Data_Two,iterations)","c427155e":"SN_Predict_Generator_Noise = tf.random.normal(shape=[50,vector_noise_shape])","f1180540":"Generator_Predict_SN = Generator(SN_Predict_Generator_Noise)","2b5211d9":"figure, axes = plt.subplots(nrows=3,ncols=3,figsize=(10,10))\n\nfor i,ax in enumerate(axes.flat):\n    Prediction_Output = Generator_Predict_SN[i]\n    ax.imshow(Prediction_Output,cmap=\"gray\")\n    ax.set_xlabel(Generator_Predict_SN[i].shape)\nplt.tight_layout()\nplt.show()","6a8d3e58":"figure = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.imshow(Generator_Predict_SN[20])\nplt.show()","505f00e5":"figure = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.imshow(Generator_Predict_SN[2])\nplt.show()","2e56140f":"figure = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.imshow(Generator_Predict_SN[33])\nplt.show()","8d9052fe":"figure = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.imshow(Generator_Predict_SN[45])\nplt.show()","723f8790":"#### DISCRIMINATOR PROCESS","4361e910":"#### FUNCTION","04dc7ebd":"#### TRANSFORMATION SPECIFIC NEURON","c4fd0686":"#### TRAIN FOR SPECIFIC NEURON","96e3dc1c":"#### SKELETON","e6702b39":"#### HISTOGRAM","9f70c34c":"#### TRANSFORMATION HEMIBRAIN","e04954a0":"#### TRANSFORMATION EM","5090a378":"#### DATA TRANSFORMATION","f751211f":"# HISTORY\n\n#### Largest Dataset Mapping Human Brain\n* In collaboration with the Lichtman Laboratory at Harvard University, Google is releasing the \u201cH01\u201d dataset, a 1.4 petabyte rendering of a small sample of human brain tissue, along with a companion paper, \u201cA connectomic study of a petascale fragment of human cerebral cortex.\u201d The H01 sample was imaged at 4nm-resolution by serial section electron microscopy, reconstructed and annotated by automated computational techniques, and analyzed for preliminary insights into the structure of the human cortex. The dataset comprises imaging data that covers roughly one cubic millimeter of brain tissue, and includes tens of thousands of reconstructed neurons, millions of neuron fragments, 130 million annotated synapses, 104 proofread cells, and many additional subcellular annotations and structures \u2014 all easily accessible with the Neuroglancer browser interface.\n\n* This dataset contains videos of specific networks.\n* It is shared for the first time on Kaggle. It is suitable for Computer Vision and DCGAN structures.\n\n#### INCLUDE\n\n* Full Connected\n* Hemibrain Connection\n* EM\n* Central Complex Structures\n* Connects Regions: ADL02od PCT\n* Connects Regions: ADM10t\n* Connects Regions: APL\n* Connects Regions: AVL01lo PCT\n* Connects Regions: ExR3\n* Connects Regions: MBON01\n* Connects Regions: Olfactory LN\n* Connects Regions: Ovil N\n\n\n\nhttps:\/\/h01-release.storage.googleapis.com\/landing.html","ed6c14b8":"#### TRAINING","6851eebd":"#### VIDEO PATH","ad8efae6":"#### PREDICTION FOR COMPLEX NEURON","3f0a5014":"#### TRAINING STEP","c175a9b9":"#### OPTIMIZERS","ed53b096":"# DATA EXPORTATION AND TRANSFORMATION","8c489202":"#### TRANSFORMATION FULL CONNECTED","0454095d":"#### LOSS FUNCTION","2e97ca89":"# PACKAGES AND LIBRARIES","89299578":"# DC-GAN STEPS","e0ab76d4":"#### GENERATING AND SAVING IMAGE","f9cffc51":"#### HESSIAN","7fa1d586":"#### CANNY","0587cc10":"#### THRESHOLD","5f97c4df":"#### GENERATOR PROCESS","34f9bce9":"#### TRAIN FOR COMPLEX NEURON","d7b73f4c":"# VISION & ANALYSIS","cb8abc08":"#### TRANSFORMATION CENTRAL COMPLEX","4eb9bebc":"#### SIMPLE GENERAL","7c46cd03":"#### PARAMETERS"}}