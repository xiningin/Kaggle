{"cell_type":{"b509ea25":"code","5fdbc753":"code","58d7c16e":"code","9b497197":"code","9b16e05e":"code","1b6a3a75":"code","9fa45785":"code","098d627f":"code","93682331":"code","6941a86a":"code","18316d12":"code","bec63453":"code","cabd1042":"code","0a9575c0":"code","3f4003cd":"code","be433d0e":"code","fbc3e10d":"code","df1ebfd5":"code","7653310b":"code","6a5ffefd":"code","fc8a9d3f":"code","914048f1":"code","bc424f55":"code","0b4ee0b4":"code","4589331b":"code","9aafb1de":"code","1491aa5f":"code","0f344586":"code","ad5bdc03":"code","d7e500f0":"code","f3eaf461":"code","f45f9409":"code","e2da074f":"code","a764d5a4":"code","55df16d2":"code","45203f85":"markdown","d4f8f6c5":"markdown","6926242a":"markdown","b6c82aff":"markdown","3d55f5e2":"markdown","f2bcbcd3":"markdown","16d7897e":"markdown","87ba3a83":"markdown","766c9b6a":"markdown","85a6bd32":"markdown","5cd8eb93":"markdown","f4be6d9b":"markdown","2656b553":"markdown","236bf284":"markdown","af91c03e":"markdown","7e6b4e9f":"markdown","b243b229":"markdown"},"source":{"b509ea25":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5fdbc753":"import nltk\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport string\nimport re\nfrom sklearn.svm import SVC\nimport wordcloud\nimport string\nstring.punctuation\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \nfrom sklearn.metrics import classification_report, confusion_matrix","58d7c16e":"sms=pd.read_csv('\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv')\nsms.head()","9b497197":"sns.countplot(sms['v1'])","9b16e05e":"# Percentage of data points with ham label \nprint(sms[sms['v1']=='ham'].shape[0]*100\/sms.shape[0])","1b6a3a75":"sms.info()","9fa45785":"sms.describe()","098d627f":"sms.isnull().sum()","93682331":"sms=sms.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)\nsms.isnull().sum()","6941a86a":"sms=sms.rename({'v1':'label','v2':'text'},axis=1)\nsms.head()","18316d12":"sms['length']=sms['text'].apply(lambda x: len(x))\nsns.distplot(sms['length'], kde=True)","bec63453":"sns.boxplot(y='length', x='label', data=sms)","cabd1042":"dt=sms.groupby('label').mean()\n#sns.barplot(y='length', x='label', data=dt)\ndt","0a9575c0":"sms.groupby('label').describe()","3f4003cd":"sms.hist(column='length', by='label', bins=50,figsize=(20,8))","be433d0e":"ham=sms[sms['label']=='ham'].copy()\nspam=sms[sms['label']=='spam'].copy()","fbc3e10d":"def show_wordcloud(df):\n    text = ' '.join(df['text'].astype(str).tolist())\n    stopwords = set(wordcloud.STOPWORDS)\n    diag = wordcloud.WordCloud(stopwords=stopwords,background_color='lightblue',\n                    colormap='coolwarm', width=800, height=600).generate(text)\n    \n    \n    plt.figure(figsize=(10,7), frameon=True)\n    plt.imshow(diag)  \n    plt.show()\n\nshow_wordcloud(ham)","df1ebfd5":"show_wordcloud(spam)","7653310b":"ham.head()","6a5ffefd":"def word_count_plot(x):\n    a = [i for i in x if i not in string.punctuation]\n    a = \"\".join(a).split()\n    b = [word.lower() for word in a if word.lower() not in stopwords.words(\"english\")]  \n    return b","fc8a9d3f":"ham['text'] = ham['text'].apply(word_count_plot)\nwords_ham = ham['text'].tolist()\nspam['text'] = spam['text'].apply(word_count_plot)\nwords_spam = spam['text'].tolist()","914048f1":"ham.head()","bc424f55":"ham_words = []\nfor i in words_ham:\n    for item in i:\n        ham_words.append(item)\nspam_words = []\nfor i in words_spam:\n    for item in i:\n        spam_words.append(item)","0b4ee0b4":"from collections import Counter\nham_count  = Counter(ham_words)\ndf_ham  = pd.DataFrame(ham_count.most_common(10),  columns=['word', 'count'])\nspam_count = Counter(spam_words)\ndf_spam = pd.DataFrame(spam_count.most_common(10), columns=['word', 'count'])","4589331b":"plt.figure(figsize=(10,7))\nsns.barplot(x='word', y='count', data=df_ham)","9aafb1de":"plt.figure(figsize=(10,7))\nsns.barplot(x='word', y='count', data=df_spam)","1491aa5f":"ps=PorterStemmer()\ndef preprocess(x):\n    x = x.lower()\n    x = re.sub(r'[^0-9a-zA-Z]', ' ', x)\n    x = re.sub(r'\\s+', ' ', x)\n    l = [item for item in x if item not in string.punctuation]\n    processed=[ps.stem(i) for i in l if i not in stopwords.words('english')]\n    return processed\n","0f344586":"X=sms['text']\ny=sms['label']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)","ad5bdc03":"def model(model_name,X_train,y_train,X_test,y_test):\n    pipeline=Pipeline([\n    ('bow', CountVectorizer(analyzer=preprocess)),\n    ('tfidf', TfidfTransformer()),\n    ('model', model_name),\n    ])\n    pipeline.fit(X_train,y_train)\n\n    preds=pipeline.predict(X_test)\n\n    print (classification_report(y_test,preds))\n    print (confusion_matrix(y_test,preds))\n    print('Accuracy:', pipeline.score(X_test, y_test)*100)\n    print(\"Training Score:\",pipeline.score(X_train,y_train)*100)\n    from sklearn.metrics import accuracy_score\n    score = accuracy_score(y_test,preds)\n    return score\n    \n","d7e500f0":"mnb=model(MultinomialNB(),X_train,y_train,X_test,y_test)","f3eaf461":"rf=model(RandomForestClassifier(),X_train,y_train,X_test,y_test)","f45f9409":"from sklearn.ensemble import GradientBoostingClassifier\ngb=model(GradientBoostingClassifier(),X_train,y_train,X_test,y_test)","e2da074f":"xgb=model(XGBClassifier(),X_train,y_train,X_test,y_test)","a764d5a4":"svc=model(SVC(),X_train,y_train,X_test,y_test)","55df16d2":"models = pd.DataFrame({\n    'Model':['MNB','RF', 'GBoost', 'XGBoost', 'SVC'],\n    'Accuracy_score' :[mnb ,rf, gb, xgb, svc]\n})\nsns.barplot(x='Accuracy_score', y='Model', data=models)\nmodels.sort_values(by='Accuracy_score', ascending=False)","45203f85":"# Model Comparison ","d4f8f6c5":"# Data Preprocessing","6926242a":"# EDA","b6c82aff":"# SVC","3d55f5e2":"# Pipeline ","f2bcbcd3":"# Data Loading","16d7897e":"# MultinomialNB ","87ba3a83":"The dataset contains 13.4% spam and 86.6% ham.","766c9b6a":"# RF","85a6bd32":"Removing stopwords like 'where', 'why', 'how', 'all', 'any', 'both', 'each' and punctuation symbols is essential before text based predictions. These symbols have an impact on the prediction accuracy if not removed.\n\nStemming is the process of producing morphological variants of a root\/base word. A stemming algorithm reduces the words \u201cchocolates\u201d, \u201cchocolatey\u201d, \u201cchoco\u201d to the root word, \u201cchocolate\u201d and \u201cretrieval\u201d, \u201cretrieved\u201d, \u201cretrieves\u201d reduce to the stem \u201cretrieve\u201d. Porter stemmer helps us stem text data used.","5cd8eb93":"Bag of words (BOW) \/ Bag of n-grams \n\nBy using the bag-of-words technique, we convert a text into its equivalent vector of numbers. It involves 3 sub processes:\n1. Tokenization - tokenizing strings and giving an integer id for each possible token by using white-spaces and punctuation as token separators.\n2. Vectorization - counting the occurrences of tokens in each document.\n3. TF-IDF - normalizing and weighting with diminishing importance tokens that occur in the majority of documents.","f4be6d9b":"Top 10 ham words","2656b553":"Top 10 spam words","236bf284":"# Gradient Boosting","af91c03e":"Distribution based on length of words","7e6b4e9f":"# XGBoost","b243b229":"# Overview\n\nSpam messages are unimportant messages like advertisements for goods and services or business opportunities delivered to a big group of people without their permission. The project aims at classifying these messages as ham or spam for optimizing user effort. "}}