{"cell_type":{"53874cf7":"code","dfed5228":"code","5e1445fd":"code","ec7a41f1":"code","3679c378":"code","6b2fea1d":"code","df3da5a9":"code","1e70faed":"code","66b4d67d":"code","6128febd":"code","85d4caa6":"code","d7598aee":"code","9c234b4c":"code","8fbef40b":"code","f0138b11":"code","c7b9d23a":"code","c789e1db":"code","46b09f49":"code","6197bf13":"code","ce501a97":"code","a994cd93":"code","67981b3f":"code","eac4b831":"code","c41ed589":"code","8889639d":"code","7d8a695c":"code","1aa16650":"code","fe098d7f":"code","a32c054d":"code","c95f28e4":"code","fff20015":"code","269bbde4":"markdown","1b80142f":"markdown","c30d5335":"markdown","cc990834":"markdown","e6f78326":"markdown","f69060df":"markdown","041d15d3":"markdown","b8a14e5d":"markdown","fe82770a":"markdown","44dacc67":"markdown","aa168b9a":"markdown","51bc75f1":"markdown","275a55f8":"markdown","52133ef9":"markdown","9c265132":"markdown","3d48931b":"markdown","dcf3e3c6":"markdown","3d771b2e":"markdown","c97be6f6":"markdown","239eca66":"markdown","751c003c":"markdown","0265a9c7":"markdown","32021acf":"markdown"},"source":{"53874cf7":"import pandas as pd \nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n\nimport gc\nfrom datetime import datetime \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn import svm\n\n\n\npd.set_option('display.max_columns', 100)\n\n\nRFC_METRIC = 'gini'  #metric used for RandomForrestClassifier\nNUM_ESTIMATORS = 100 #number of estimators used for RandomForrestClassifier\nNO_JOBS = 4 #number of parallel jobs used for RandomForrestClassifier\n\n\n#TRAIN\/VALIDATION\/TEST SPLIT\n#VALIDATION\nVALID_SIZE = 0.20 # simple validation using train_test_split\nTEST_SIZE = 0.20 # test size using_train_test_split","dfed5228":"data_df=pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","5e1445fd":"print(\"Credit Card Fraud Detection- rows:\",data_df.shape[0],\"columns:\",data_df.shape[1])","ec7a41f1":"data_df.head()","3679c378":"data_df.describe()","6b2fea1d":"total = data_df.isnull().sum().sort_values(ascending = False)\npercent = (data_df.isnull().sum()\/data_df.isnull().count()*100).sort_values(ascending = False)\npd.concat([total, percent], axis=1, keys=['Total', 'Percent']).transpose()","df3da5a9":"#checking for data unbalance wrt the target value i.e., class\ntemp = data_df[\"Class\"].value_counts()\ndf = pd.DataFrame({'Class': temp.index,'values': temp.values})\n\ntrace = go.Bar(\n    x = df['Class'],y = df['values'],\n    name=\"Credit Card Fraud Class - data unbalance (Not fraud = 0, Fraud = 1)\",\n    marker=dict(color=\"Red\"),\n    text=df['values']\n)\ndata = [trace]\nlayout = dict(title = 'Credit Card Fraud Class - data unbalance (Not fraud = 0, Fraud = 1)',\n          xaxis = dict(title = 'Class', showticklabels=True), \n          yaxis = dict(title = 'Number of transactions'),\n          hovermode = 'closest',width=600\n         )\nfig = dict(data=data, layout=layout)\niplot(fig,filename='class')","1e70faed":"plt.figure(figsize = (14,14))\nplt.title('Credit Card Transactions features correlation plot (Pearson)')\ncorr = data_df.corr()\nsns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,cmap=\"Reds\")\nplt.show()","66b4d67d":"#defining predictor and target values. In this Case, there are no categorical features\ntarget = 'Class'\npredictors = ['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\\\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19',\\\n       'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28',\\\n       'Amount']\n","6128febd":"#splitting the data in train, test and validation set\nRANDOM_STATE = 2018\ntrain_df, test_df = train_test_split(data_df, test_size=TEST_SIZE, random_state=RANDOM_STATE, shuffle=True )\ntrain_df, valid_df = train_test_split(train_df, test_size=VALID_SIZE, random_state=RANDOM_STATE, shuffle=True )","85d4caa6":"X = data_df.drop(columns = ['Class'])\ny = data_df['Class']\nX.shape\ny.shape","d7598aee":"X = np.array(X)\ny = np.array(y)","9c234b4c":"y = y.reshape(-1,1)\ny.shape","8fbef40b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=45)\nprint(X_train.shape); print(X_test.shape)","f0138b11":"from sklearn.neural_network import MLPClassifier\n\nmlp = MLPClassifier(hidden_layer_sizes=(5,8,5), activation='relu', solver='adam', max_iter=1000)\nmlp.fit(X_train,y_train.ravel())\n\npredict_train = mlp.predict(X_train)\npredict_test = mlp.predict(X_test)","c7b9d23a":"\"\"\"Testing Accuracy\"\"\"\nprint(mlp.score(X_test,y_test))\n\n\"\"\"Training Accuracy\"\"\"\nprint(mlp.score(X_train,y_train))","c789e1db":"from sklearn.metrics import classification_report,confusion_matrix\nprint(confusion_matrix(y_train,predict_train))\nprint(classification_report(y_train,predict_train))","46b09f49":"print(confusion_matrix(y_test,predict_test))\nprint(classification_report(y_test,predict_test))","6197bf13":"#initializing the randomforestclassifier\nclf = RandomForestClassifier(n_jobs=NO_JOBS,random_state=RANDOM_STATE,criterion=RFC_METRIC,n_estimators=NUM_ESTIMATORS,\n                             verbose=False)","ce501a97":"#train the randomforestclassifier using the train_df dataframe and fit function\nclf.fit(train_df[predictors], train_df[target].values)","a994cd93":"#predicting the target values for the valid_df data using the predict function\npreds = clf.predict(valid_df[predictors])","67981b3f":"tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': clf.feature_importances_})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (7,4))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.show()   ","eac4b831":"#confusion matrix to show the results we obtained\ncm = pd.crosstab(valid_df[target].values, preds, rownames=['Actual'], colnames=['Predicted'])\nfig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\nsns.heatmap(cm, xticklabels=['Not Fraud', 'Fraud'],yticklabels=['Not Fraud', 'Fraud'],annot=True,ax=ax1,\n            linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\nplt.title('Confusion Matrix', fontsize=14)\nplt.show()","c41ed589":"roc_auc_score(valid_df[target].values, preds)","8889639d":"import xgboost as xgb","7d8a695c":"# Prepare the train and validitaion datasets\ndtrain = xgb.DMatrix(train_df[predictors], train_df[target].values)\ndvalid = xgb.DMatrix(valid_df[predictors], valid_df[target].values)\ndtest = xgb.DMatrix(test_df[predictors], test_df[target].values)\n\n#What to monitor \nwatchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n\n# Set xgboost parameters\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eta'] = 0.039\nparams['silent'] = True\nparams['max_depth'] = 2\nparams['subsample'] = 0.8\nparams['colsample_bytree'] = 0.9\nparams['eval_metric'] = 'auc'\nparams['random_state'] = RANDOM_STATE","1aa16650":"MAX_ROUNDS = 1000 #lgb iterations\nEARLY_STOP = 50 #lgb early stop \nOPT_ROUNDS = 1000  #To be adjusted based on best validation rounds\nVERBOSE_EVAL = 50 #Print out metric result","fe098d7f":"#train the model\nmodel = xgb.train(params, dtrain,  MAX_ROUNDS,  watchlist,  early_stopping_rounds=EARLY_STOP,  maximize=True, \n                verbose_eval=VERBOSE_EVAL)","a32c054d":"fig, (ax) = plt.subplots(ncols=1, figsize=(12,8))\nxgb.plot_importance(model, height=0.8, title=\"Features importance (XGBoost)\", ax=ax, color=\"green\") \nplt.show()","c95f28e4":"#predicting the target value for the test set using the trained model\npreds = model.predict(dtest)","fff20015":"roc_auc_score(test_df[target].values, preds)","269bbde4":"# Neural Network","1b80142f":"# Predictive Model","c30d5335":"*XGBOOST model has a higher accuracy rate than RANDOMFORESTCLASSIFIER for this dataset with a maximum score of 97%*","cc990834":"# Group 11 \n\n# \"\"\"Credit card Fraud detection \"\"\"<br>\nSanjana Singh &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RA1911003010287\t<br>\nAakash Jaiswal  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; RA1911003010289\t<br>\nBurugadda Vivek  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   RA1911003010316\t<br>\nKashyapi Singh   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  RA1911003010330\t<br>\nLakshmi Priyanka V    &nbsp;&nbsp;&nbsp;       RA1911003010335 <br>\nArsh Shah      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; RA1911003010286 <br>","e6f78326":"# Check the data","f69060df":"**Correlation**","041d15d3":"**Feature Importance**","b8a14e5d":"**Area Under the curve**","fe82770a":"*The ROC-AUC score obtained with RandomForrestClassifier is 0.85.*","44dacc67":"*from the above output we can confirm that there are NO MISSING DATA*","aa168b9a":"# XGBOOST MODEL","51bc75f1":"**Feature Importance**","275a55f8":"**Area Under The Curve**","52133ef9":"# Checking for data unbalance","9c265132":"*We initialize the DMatrix objects for training and validation, starting from the datasets. We also set some of the parameters used for the model tuning.*\n","3d48931b":"# Read the data","dcf3e3c6":"*looking at the time column we can confirm that the data contains 284807 transactions within a span of 172792 seconds i.e., 2 consecutive days*","3d771b2e":"# RANDOMFORESTCLASSIFIER","c97be6f6":"*There is no notable correlation between V1 and V28 but there are certain correlations between some of the features and time(inverse correlation with V3) and amount(linear correlation with V7 and V20, inverse correlation with V1 and V5*\n","239eca66":"*The best validation score obtained is 0.984*","751c003c":"# looking for missing data","0265a9c7":"*We will run a model using the train_df for training and valid_df for validation.\nWe will use the validation criteria GINI, with the formula GINI = 2 * (AUC) - 1, where AUC is the Receiver Operating Characteristic - Area Under Curve (ROC-AUC). Number of estimators is set to 100 and number of parallel jobs is set to 4.*","32021acf":"**Confusion Matrix**"}}