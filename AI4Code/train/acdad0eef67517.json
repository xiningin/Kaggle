{"cell_type":{"0b485aa6":"code","799b5e96":"code","20590013":"code","d58e6819":"code","31c6d7f2":"code","c27dc5fa":"code","a45f5ed3":"code","53f720be":"code","e4761325":"code","d220bf16":"code","95131179":"code","265859bc":"code","e0275a86":"code","6b36846e":"code","b139cb85":"code","c076ccaa":"code","02a76045":"code","9c71a61d":"code","22b06f7b":"code","8fd82f86":"code","5cbaa977":"code","0f7e503d":"code","5186a6ba":"code","6b23ce67":"code","e2b37465":"code","2e47752d":"code","4d4cc65f":"code","fc0ff716":"code","12e6a543":"code","60cddbfc":"code","3b288592":"code","a66dfc05":"code","84fe28a3":"code","a5a75737":"code","0136b089":"code","4797fb8c":"code","b00b3ae1":"code","60e5fcea":"code","bfc5f78f":"code","b83b2ac3":"code","c81bcd25":"code","b3153260":"code","12bfad61":"code","71fc7b1f":"code","a69069aa":"code","b9dc94b4":"code","2063ca8a":"markdown","8e95db3a":"markdown","67d396d2":"markdown","8ba10ea3":"markdown","4c7c68db":"markdown","fdeb86c9":"markdown","ac966075":"markdown","29a74946":"markdown","eeff628d":"markdown","649fcf00":"markdown","a1f806ab":"markdown","3a78a7d5":"markdown","3d71d688":"markdown","f06fe564":"markdown","c495b3b8":"markdown","dbe24a8d":"markdown","489d7d40":"markdown","b47f706c":"markdown","0753895c":"markdown","d595a76c":"markdown","f0ddb6f1":"markdown","79ee2ccf":"markdown","582ca281":"markdown","2a44fc2e":"markdown","342ffe71":"markdown","88d64a25":"markdown","5bfbda39":"markdown","857e057c":"markdown","ece23198":"markdown","ece5e465":"markdown","f2f1f9d3":"markdown","be9b40be":"markdown","53e5786d":"markdown","78344f2f":"markdown","278bad98":"markdown","e47ad84f":"markdown","57ee8804":"markdown","f6fff43f":"markdown","58bcc00b":"markdown","f0d5aabb":"markdown","af44c38f":"markdown","f68fcae1":"markdown","7556e4a2":"markdown","90031c03":"markdown","728148e5":"markdown","afdbd507":"markdown","82e3734e":"markdown","35d07245":"markdown","d70ced1d":"markdown","619c48b0":"markdown","723216c6":"markdown","a6ff3620":"markdown","4fd642b0":"markdown","3867934e":"markdown","3decfd37":"markdown","f1472983":"markdown"},"source":{"0b485aa6":"import pandas as pd\nimport numpy as np\nheart_data = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')\nheart_data.head()","799b5e96":"heart_data.shape","20590013":"heart_data.dtypes","d58e6819":"heart_data.isnull().sum()","31c6d7f2":"heart_data.rename(columns={'trestbps':'restbp','thalach':'maxhr','ca':'nmv'},inplace=True)","c27dc5fa":"import matplotlib.pyplot as plt\nimport seaborn as sns","a45f5ed3":"heart_data.describe()","53f720be":"sns.boxplot(x='target',y='age',data=heart_data)","e4761325":"heart_data['age']  = pd.cut(heart_data['age'],bins=[29,39,49,59,69,79],labels=[1,2,3,4,5],include_lowest=True)","d220bf16":"age = heart_data.groupby(['age','target'])['target'].count().unstack()\nage['per'] = round((age[1]\/(age[0]+age[1]))*100,2)\nage","95131179":"sex = heart_data.groupby(['sex','target'])['target'].count().unstack()\nsex['per'] = round((sex[1]\/(sex[0]+sex[1]))*100,2)\nsex","265859bc":"age_sex = heart_data.groupby(['age','sex','target'])['target'].count().unstack()\nage_sex['per'] = round((age_sex[1]\/(age_sex[0]+age_sex[1]))*100,2)\nage_sex","e0275a86":"cp = heart_data.groupby(['cp','target'])['target'].count().unstack()\ncp['per'] = round((cp[1]\/(cp[0]+cp[1]))*100,2)\ncp","6b36846e":"sns.boxplot(x='target',y='restbp',data=heart_data)","b139cb85":"sns.boxplot(x='target',y='chol',data=heart_data)","c076ccaa":"fbs = heart_data.groupby(['fbs','target'])['target'].count().unstack()\nfbs['per'] = round((fbs[1]\/(fbs[0]+fbs[1]))*100,2)\nfbs","02a76045":"restecg = heart_data.groupby(['restecg','target'])['target'].count().unstack()\nrestecg['per'] = round((restecg[1]\/(restecg[0]+restecg[1]))*100,2)\nrestecg","9c71a61d":"sns.boxplot(x='target',y='maxhr',data=heart_data)","22b06f7b":"ex = heart_data.groupby(['exang','target'])['target'].count().unstack()\nex['per'] = round((ex[1]\/(ex[0]+ex[1]))*100,2)\nex","8fd82f86":"sns.boxplot(x='target',y='oldpeak',data=heart_data)","5cbaa977":"sl = heart_data.groupby(['slope','target'])['target'].count().unstack()\nsl['per'] = round((sl[1]\/(sl[0]+sl[1]))*100,2)\nsl","0f7e503d":"nmv = heart_data.groupby(['nmv','target'])['target'].count().unstack()\nnmv['per'] = round((nmv[1]\/(nmv[0]+nmv[1]))*100,2)\nnmv","5186a6ba":"thal = heart_data.groupby(['thal','target'])['target'].count().unstack()\nthal['per'] = round((thal[1]\/(thal[0]+thal[1]))*100,2)\nthal","6b23ce67":"heart_data['age']=heart_data['age'].astype('int')\nheart_data['age_sex'] = heart_data['age']*heart_data['sex']","e2b37465":"c = heart_data.corr()\nplt.figure(figsize=(20,6))\nsns.heatmap(c,annot=True,fmt='f')","2e47752d":"from sklearn.model_selection import train_test_split as tts\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import f1_score as fs\nfrom statistics import mean\nfrom sklearn.preprocessing import MinMaxScaler as MMS\nheart_data['mms_restbp'] = MMS().fit_transform(heart_data[['restbp']])\nheart_data['mms_maxhr'] = MMS().fit_transform(heart_data[['maxhr']])\nx = heart_data[['age_sex','cp','mms_restbp','restecg','mms_maxhr','exang','oldpeak','slope','nmv','thal']]\ny = heart_data['target']\nxtrain,xtest,ytrain,ytest = tts(x,y,test_size=0.2,random_state=100)\nkf = KFold(n_splits=5,shuffle=True,random_state=100)","4d4cc65f":"from sklearn.neighbors import KNeighborsClassifier as KNC\nneighbors,scores_train,scores_test = [i for i in range(1,20)],[],[]\nfor n in neighbors:\n    score_train,score_test = [],[]\n    for train,test in kf.split(xtrain):\n        xtr,xtt = xtrain.iloc[train],xtrain.iloc[test]\n        ytr,ytt = ytrain.iloc[train],ytrain.iloc[test]\n        knc = KNC(n_neighbors=n,weights='distance') #Giving weight to the distance of neighbors\n        knc.fit(xtr,ytr)\n        yhat_train = knc.predict(xtr)\n        yhat_test = knc.predict(xtt)\n        score_train.append(round(fs(ytr,yhat_train),2))\n        score_test.append(round(fs(ytt,yhat_test),2))\n    scores_train.append(mean(score_train))\n    scores_test.append(mean(score_test))\nsns.lineplot(x=neighbors,y=scores_train,color='r')\nsns.lineplot(x=neighbors,y=scores_test,color='b')\nplt.legend(('Train','Test'))","fc0ff716":"from sklearn.tree import DecisionTreeClassifier as DTC\ndepths,scores_train,scores_test = [i for i in range(3,20)],[],[]\nfor d in depths:\n    score_train,score_test = [],[]\n    for train,test in kf.split(xtrain):\n        xtr,xtt = xtrain.iloc[train],xtrain.iloc[test]\n        ytr,ytt = ytrain.iloc[train],ytrain.iloc[test]\n        dtc = DTC(max_depth=d)\n        dtc.fit(xtr,ytr)\n        yhat_train = dtc.predict(xtr)\n        yhat_test = dtc.predict(xtt)\n        score_train.append(round(fs(ytr,yhat_train),2))\n        score_test.append(round(fs(ytt,yhat_test),2))\n    scores_train.append(mean(score_train))\n    scores_test.append(mean(score_test))\nsns.lineplot(x=depths,y=scores_train,color='r')\nsns.lineplot(x=depths,y=scores_test,color='b')","12e6a543":"from sklearn.linear_model import LogisticRegression as LR\nC,scores_train,scores_test = [0.001,0.005,0.01,0.05,0.1,0.5],[],[]\nfor c in C:\n    score_train,score_test = [],[]\n    for train,test in kf.split(xtrain):\n        xtr,xtt = xtrain.iloc[train],xtrain.iloc[test]\n        ytr,ytt = ytrain.iloc[train],ytrain.iloc[test]\n        lr = LR(C=c,max_iter=1000)  # C is Inverse of regularization strength\n        lr.fit(xtr,ytr)\n        yhat_train = lr.predict(xtr)\n        yhat_test = lr.predict(xtt)\n        score_train.append(round(fs(ytr,yhat_train),2))\n        score_test.append(round(fs(ytt,yhat_test),2))\n    scores_train.append(mean(score_train))\n    scores_test.append(mean(score_test))\nsns.lineplot(x=C,y=scores_train,color='r')\nsns.lineplot(x=C,y=scores_test,color='b')","60cddbfc":"from sklearn.naive_bayes import GaussianNB as GNB\nscore_train,score_test = [],[]\nfor train,test in kf.split(xtrain):\n    xtr,xtt = xtrain.iloc[train],xtrain.iloc[test]\n    ytr,ytt = ytrain.iloc[train],ytrain.iloc[test]\n    gnb = GNB()\n    gnb.fit(xtr,ytr)\n    yhat_train = gnb.predict(xtr)\n    yhat_test = gnb.predict(xtt)\n    score_train.append(round(fs(ytr,yhat_train),2))\n    score_test.append(round(fs(ytt,yhat_test),2))\nprint(mean(score_train),mean(score_test))","3b288592":"from sklearn.svm import SVC\nC,scores_train,scores_test = [0.001,0.005,0.01,0.05,0.1,0.5],[],[]\nfor c in C:\n    score_train,score_test = [],[]\n    for train,test in kf.split(xtrain):\n        xtr,xtt = xtrain.iloc[train],xtrain.iloc[test]\n        ytr,ytt = ytrain.iloc[train],ytrain.iloc[test]\n        svc = SVC(C=c,kernel='linear')\n        svc.fit(xtr,ytr)\n        yhat_train = svc.predict(xtr)\n        yhat_test = svc.predict(xtt)\n        score_train.append(round(fs(ytr,yhat_train),2))\n        score_test.append(round(fs(ytt,yhat_test),2))\n    scores_train.append(mean(score_train))\n    scores_test.append(mean(score_test))\nsns.lineplot(x=C,y=scores_train,color='r')\nsns.lineplot(x=C,y=scores_test,color='b')","a66dfc05":"from sklearn.ensemble import RandomForestClassifier as RFC\nestimators,scores_train,scores_test = [100,150,200,250,300,350,400,450,500],[],[]\nfor e in estimators:\n    score_train,score_test = [],[]\n    for train,test in kf.split(xtrain):\n        xtr,xtt = xtrain.iloc[train],xtrain.iloc[test]\n        ytr,ytt = ytrain.iloc[train],ytrain.iloc[test]\n        rfc = RFC(n_estimators=e,max_depth=6)\n        rfc.fit(xtr,ytr)\n        yhat_train = rfc.predict(xtr)\n        yhat_test = rfc.predict(xtt)\n        score_train.append(round(fs(ytr,yhat_train),2))\n        score_test.append(round(fs(ytt,yhat_test),2))\n    scores_train.append(mean(score_train))\n    scores_test.append(mean(score_test))\nsns.lineplot(x=estimators,y=scores_train,color='r')\nsns.lineplot(x=estimators,y=scores_test,color='b')","84fe28a3":"from sklearn.metrics import accuracy_score as acs\nfrom sklearn.metrics import confusion_matrix\nmodels,f1score,acscore = ['KNN','DTC','LR','GNB','SVM','RFC'],[],[]","a5a75737":"knn = KNC(n_neighbors=8,weights='distance')\nknn.fit(xtrain,ytrain)\nyhat_eval = knn.predict(xtest)\nf1 = round(fs(ytest,yhat_eval),2)\nac = round(acs(ytest,yhat_eval),2)\nf1score.append(f1)\nacscore.append(ac)\nprint(f1,ac)","0136b089":"cfm = confusion_matrix(ytest,yhat_eval)\nsns.heatmap(cfm,annot=True)\nplt.title('Confusion Matrix KNN')\nplt.xlabel('Predicted Value')\nplt.ylabel('True Label')","4797fb8c":"dtc = DTC(max_depth=6)\ndtc.fit(xtrain,ytrain)\nyhat_eval = dtc.predict(xtest)\nf1 = round(fs(ytest,yhat_eval),2)\nac = round(acs(ytest,yhat_eval),2)\nf1score.append(f1)\nacscore.append(ac)\nprint(f1,ac)","b00b3ae1":"cfm = confusion_matrix(ytest,yhat_eval)\nsns.heatmap(cfm,annot=True)\nplt.title('Confusion Matrix DTC')\nplt.xlabel('Predicted Value')\nplt.ylabel('True Label')","60e5fcea":"lr = LR(C=0.05)\nlr.fit(xtrain,ytrain)\nyhat_eval = lr.predict(xtest)\nf1 = round(fs(ytest,yhat_eval),2)\nac = round(acs(ytest,yhat_eval),2)\nf1score.append(f1)\nacscore.append(ac)\nprint(f1,ac)","bfc5f78f":"cfm = confusion_matrix(ytest,yhat_eval)\nsns.heatmap(cfm,annot=True)\nplt.title('Confusion Matrix LR')\nplt.xlabel('Predicted Value')\nplt.ylabel('True Label')","b83b2ac3":"gnb = GNB()\ngnb.fit(xtrain,ytrain)\nyhat_eval = gnb.predict(xtest)\nf1 = round(fs(ytest,yhat_eval),2)\nac = round(acs(ytest,yhat_eval),2)\nf1score.append(f1)\nacscore.append(ac)\nprint(f1,ac)","c81bcd25":"cfm = confusion_matrix(ytest,yhat_eval)\nsns.heatmap(cfm,annot=True)\nplt.title('Confusion Matrix GNB')\nplt.xlabel('Predicted Value')\nplt.ylabel('True Label')","b3153260":"svc = SVC(kernel='linear',C=0.1)\nsvc.fit(xtrain,ytrain)\nyhat_eval = svc.predict(xtest)\nf1 = round(fs(ytest,yhat_eval),2)\nac = round(acs(ytest,yhat_eval),2)\nf1score.append(f1)\nacscore.append(ac)\nprint(f1,ac)","12bfad61":"cfm = confusion_matrix(ytest,yhat_eval)\nsns.heatmap(cfm,annot=True)\nplt.title('Confusion Matrix SVC')\nplt.xlabel('Predicted Value')\nplt.ylabel('True Label')","71fc7b1f":"rfc = RFC(n_estimators=250,max_depth=6)\nrfc.fit(xtrain,ytrain)\nyhat_eval = rfc.predict(xtest)\nf1 = round(fs(ytest,yhat_eval),2)\nac = round(acs(ytest,yhat_eval),2)\nf1score.append(f1)\nacscore.append(ac)\nprint(f1,ac)","a69069aa":"cfm = confusion_matrix(ytest,yhat_eval)\nsns.heatmap(cfm,annot=True)\nplt.title('Confusion Matrix RFC')\nplt.xlabel('Predicted Value')\nplt.ylabel('True Label')","b9dc94b4":"Results = pd.DataFrame({'F1 Score':f1score,'Accuracy Score':acscore},index=models)\nResults.sort_values(by=['F1 Score','Accuracy Score'],ascending=False)","2063ca8a":"The ecg result of 1 has a higher chance of heart disease.","8e95db3a":"We need to find the optimum value of regularization factor for our model. Increasing the regularization strength penalizes \"large\" weight coefficients. Our goal is to prevent that our model picks up \"peculiarities,\" \"noise,\" or \"imagines a pattern where there is none.\"","67d396d2":"### Gaussian Naive Bayes","8ba10ea3":"Similar to the logistic regression model, we will try to find the optimum value for the regularization factor.","4c7c68db":"As expected irreversible defects have a higher chance of disease and reversible defects have a lower chance.\n\nLet's create a new variable using age and sex.","fdeb86c9":"# Evaluation","ac966075":"# Exploratory Data Analysis","29a74946":"### K Nearest Neighbors","eeff628d":"### Decision Tree","649fcf00":"The dataset is not very large.\n\nLet's make sure that the data types for the features are according to the description.","a1f806ab":"Let's check the correlation between features.","3a78a7d5":"This model creates various trees and finds the average of all the trees. We will need to find the optimum number of trees to be considered for this model. We will use the maximum depth which we found from the DTC model.","3d71d688":"Here we will select the features we will use for prediction. We will select features which have a decent correlation with our target variable. Also we will make use of MinMaxScaler to reduce the range of 'restbp' and 'maxhr' to avoid bias towards them. We make use of training set to find the optimum parameters for our models and make use of KFold to check the performance of our model. The metric which we will use for evaluation will be F1 score. Since we need to reduce the false negatives, F1 score is a better metric than accuracy score. The reason being F1 score includes both precision and recall.","f06fe564":"As expected higher heart rate results in higher chance of heart disease. ","c495b3b8":"We can observe 'cp' and 'maxhr' have a decent positive correlation with our target variables. Features such as 'exang','oldpeak','nmv','thal' and 'age_sex' have a negative correlation.","dbe24a8d":"The value of optimum neighbors is 8.","489d7d40":"### Decision Tree Classifier","b47f706c":"Now we will test our models on the test data and see the performance with the help of confusion matrix.","0753895c":"# Data Cleaning","d595a76c":"Both the labels have same ranges. As expected very high cholestrol would result in heart disease, this is visible from the outliers.","f0ddb6f1":"According to the data mid age patients have a higher chance of having heart problems.","79ee2ccf":"### Support Vector Machine","582ca281":"This a model where it assumes data to be normally distributed and applies bayes theorem assumptions. There are no parameters to be adjusted in this model.","2a44fc2e":"We need to find the optimum number of neighbors for our model. Hence we will compare the F1 scores obtained for various values for our neighbors.","342ffe71":"Therefore from the above table it is clear that Logistic Regression was the best model as it generated the highest F1 and Accuracy score.","88d64a25":"very low and very high vessels results in heart disease. But mid range vessels have a lower chance of disease.","5bfbda39":"We will analyse the metrics for various models and select the best one.","857e057c":"Nothing conclusive can be obtained from fasting blood sugar.","ece23198":"A flat slope results in less chance of heart disease and down slope has a higher chance of disease.","ece5e465":"As expected, if a person has chest pain, they have a higher chance of heart problems.","f2f1f9d3":"The optimum number of trees was found to be 250.","be9b40be":"# Model Fitting","53e5786d":"It is surprising to see that no angina results in higher chance of heart disease and angina results in less chance of heart disease.","78344f2f":"Very low depression results in higher chance of heart disease. A higher ST depression is seen in normal patients.","278bad98":"Both the target classes have the same median. The IQR of '0' label is larger than '1'. There are some outliers, such as restbp of 200. This blood pressure would mean abnormal heart behaviour. ","e47ad84f":"### Random Forest Classifier","57ee8804":"# Prediction Of Heart Disease\n### By: Rahul Kulkarni","f6fff43f":"The features such as 'restbp','chol' and 'maxhr' have large ranges compared to other features. This might cause bias towards these features during modelling. We will handle this later.\n\nLet's have a closer at the features and how they affect the target variable.","58bcc00b":"Depth value of 3 has the highest F1 score but I am certain that this value would underfit the data. Therefore I believe choosing a value of 6 would be better.","f0d5aabb":"### Gaussian Naive Bayes","af44c38f":"### Description of the dataset:\n1. **age**: age of the patient\n2. **sex**: 1 = male and 0 = female\n3. **cp(chest pain type)**: 1= typical anginaValue, 2= atypical anginaValue, 3= non-anginal painValue, 4= asymptomatic\n4. **trestps(resting blood pressure)**:  resting blood pressure (in mm Hg on admission to the hospital)\n5. **chol(serum cholestoral in mg\/dl)**\n1. **fbs(fasting blood sugar > 120 mg\/dl)**: 1 = true, 0 = false\n1. **restecg(resting electrocardiographic results)**: 0= normal, 1= having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV), 2= showing probable or definite left ventricular hypertrophy by Estes' criteria\n1. **thalach(maximum heart rate achieved)**\n1. **exang(exercise induced angina)**: 1 = yes, 0 = no\n1. **oldpeak**: ST depression induced by exercise relative to rest\n1. **slope(the slope of the peak exercise ST segment)**: 1= upsloping, 2= flat, 3= downsloping\n1. **ca(number of major vessels (0-3) colored by flourosopy)**\n1. **thal**: 3 = normal; 6 = fixed defect; 7 = reversable defect\n1. **target**: 0= < 50% diameter narrowing, 1= > 50% diameter narrowing","f68fcae1":"### Random Forest Classifier","7556e4a2":"### Logistic Regression","90031c03":"We can club age and sex for further analysis, as this has given us interesting information.","728148e5":"### Logistic Regression","afdbd507":"The box plot doesn't give us good information, so binning ages would be a better option.","82e3734e":"Females have a higher chance of having heart problems.\n\nLet's do further analysis by grouping data by age and sex.","35d07245":"The optimum regularization factor was found to be 0.05.","d70ced1d":"The optimum value is 0.1","619c48b0":"We need to find the optimum depth for our model.","723216c6":"### Scope and Problem Statement:\nIn this kernel we will try to predict the possibility of a person having heart diesease with the help of various factors such as age, gender,blood pressure etc. We will use various classification models for the purpose of prediction. We will got through various processes such as Data cleaning,EDA,Model fitting and Evaluation.\n\nThis prediction is very useful in the healthcare industry, as an accurate prediction can help the doctors in helping the patient beforehand.","a6ff3620":"There are no missing values.\n\nThere are certain column names that bother me so I will rename them.","4fd642b0":"### Support Vector Machine","3867934e":"Let's look at the statistical properties of the features.","3decfd37":"### K Nearest Neighbors","f1472983":"Let's check for missing values."}}