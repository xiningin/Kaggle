{"cell_type":{"e7f8f60c":"code","202519b4":"code","c7a81609":"code","1a5f4aa5":"code","7e4a3454":"code","bf8aa497":"code","d0d60d85":"code","ac358584":"code","c3f4afeb":"code","766d9031":"code","d18c606e":"markdown","5d3a05b6":"markdown","0bf2d26a":"markdown","936f39e0":"markdown","ebab4655":"markdown","a1df9276":"markdown","8fe3013f":"markdown"},"source":{"e7f8f60c":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import PolynomialFeatures","202519b4":"#create dummy data for training\nx_values = [i for i  in range(11)]\nx_train = np.array(x_values, dtype = np.float32)\nx_train = x_train.reshape(-1,1)\n\ny_values = [2*i+1 for i in x_values]\ny_train = np.array(y_values,dtype=np.float32)\ny_train = y_train.reshape(-1,1)","c7a81609":"import torch\nfrom torch.autograd import Variable","1a5f4aa5":"class LinearRegression(torch.nn.Module):\n    def __init__(self,inputsize,outputsize):\n        super(LinearRegression,self).__init__()\n        self.linear = torch.nn.Linear(inputsize, outputsize)\n        \n    def forward(self, x):\n        out = self.linear(x)\n        return(out)","7e4a3454":"inputdim = 1\noutputdim = 1\nlearningrate = 0.01\nepochs = 100","bf8aa497":"model = LinearRegression(inputdim, outputdim)\n#for GPU\nif(torch.cuda.is_available()):\n    model.cuda()","d0d60d85":"criterion = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(),lr = learningrate)","ac358584":"for epoch in range(epochs):\n    #Converting inputs and lables to Varibale\n    if(torch.cuda.is_available()):\n        inputs = Variable(torch.from_numpy(x_train).cuda())\n        labels = Variable(torch.from_numpy(y_train).cuda())\n    else:\n        inputs = Variable(torch.from_numpy(x_train))\n        labels = Variable(torch.from_numpy(y_train))\n    # Clear gradient buffers because we don't want any gradient from previous\n    #epoch to carry forward,dont want to cummulate gradients\n    optimizer.zero_grad()\n    \n    #get output from the model, given the inputs\n    outputs = model(inputs)\n    \n    #get loss for predicted output\n    loss = criterion(outputs, labels)\n    print(loss)\n    \n    #get gradients w.r.t to parameters\n    loss.backward()\n    \n    #update parameters\n    optimizer.step()\n    \n    print('epochs {}, loss {}'.format(epoch, loss.item()))","c3f4afeb":"with torch.no_grad():\n    if(torch.cuda.is_available()):\n        #no need for gradients in testing phase\n        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n    else:\n        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n    print(predicted)","766d9031":"plt.clf()\nplt.plot(x_train, y_train,'go',label='True data', alpha=0.5)\nplt.plot(x_train, predicted,'go',label='predicted', alpha=0.5,color='red')\nplt.legend(loc='best')\nplt.show()","d18c606e":"We defined a class for linear regression, that inherits torch.nn.Module which is the basic Neural Network module containing all the required functions. Our Linear Regression model only contains one simple linear function.","5d3a05b6":"An Overview of Python\u2019s super() Function\n\nIf you have experience with object-oriented languages, you may already be familiar with the functionality of super().\n\nIt gives you access to methods in a superclass from the subclass that inherits from it.\n\nsuper() alone returns a temporary object of the superclass that then allows you to call that superclass\u2019s methods.\n\nWhy would you want to do any of this? While the possibilities are limited by your imagination, a common use case is building classes that extend the functionality of previously built classes.\n\nCalling the previously built methods with super() saves you from needing to rewrite those methods in your subclass, and allows you to swap out superclasses with minimal code changes. ","0bf2d26a":"instantiate the model","936f39e0":"After that, we initialize the loss (Mean Squared Error) and optimization (Stochastic Gradient Descent) functions that we\u2019ll use in the training of this model.","ebab4655":"Training Model","a1df9276":"Testing Model","8fe3013f":"# Linear Regression with PyTorch\n\nLinear Regression is an approach that tries to find a linear relationship between a dependent variable and an independent variable by minimizing the distance.\n\nLet\u2019s consider a very basic linear equation i.e., y=2x+1. Here, \u2018x\u2019 is the independent variable and y is the dependent variable. We\u2019ll use this equation to create a dummy dataset which will be used to train this linear regression model. Following is the code for creating the dataset."}}