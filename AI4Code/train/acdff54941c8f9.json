{"cell_type":{"eddc33ea":"code","288bfce2":"code","92a900c5":"code","2a8741da":"code","7680c6ef":"code","0eeead53":"code","0b139be2":"code","b4ba16ca":"code","af909643":"code","bbd79162":"code","e8f57123":"code","6f0f0254":"code","b4cdbd4f":"code","97fe0322":"code","052020be":"code","55d4c428":"code","920d7f74":"code","2c28b937":"code","1a34309d":"code","2fc062f3":"code","b729899e":"code","d197451a":"code","725a72d8":"code","6f25fdf7":"markdown"},"source":{"eddc33ea":"#imports\nimport tensorflow as tf\nimport time\nfrom tqdm import tqdm\nfrom glob import glob\nimport cv2\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom os import listdir\nfrom os.path import isfile, join\n\nfrom random import sample\nimport random\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nimport cv2\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport PIL\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\n\n#imports for capsule neural network in particular\nimport gc\nimport os\nimport nltk\nimport tqdm\nimport numpy as np\nimport pandas as pd\nnltk.download(\"punkt\")\n\nfrom keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\nfrom keras.engine import Layer\nfrom keras.layers import Activation, Add, Bidirectional, Conv1D, Dense, Dropout, Embedding, Flatten\nfrom keras.layers import concatenate, GRU, Input, LSTM, MaxPooling1D\nfrom keras.layers import GlobalAveragePooling1D,  GlobalMaxPooling1D, SpatialDropout1D\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.preprocessing import text, sequence\nfrom sklearn.metrics import accuracy_score, roc_auc_score, log_loss\nfrom sklearn.model_selection import train_test_split\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nimport keras.backend as K\nimport numpy as np\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom datetime import datetime\n\n\n%load_ext tensorboard","288bfce2":"#configuration section\nimageSize = 160\nimageWidth = 28 #downscale the image to x by x pixels\nimageHeight = 28 #original dimensions are 640 x 480\nepochNum = 50 #epochs are the number of times we pass through the data\nbatchSize = 8 #number of samples we work through before updating the model\nmaxFiles = 1000 #number to load for each category\ncolorDepth = 1 #3 for RGB\nclasses = 10 #number of classifications we want to bother with\nlearningRate = 0.0001 #this is how fast our model learns, lower = more accurate but takes longer","92a900c5":"# Parameters Based on Paper\nepsilon = 1e-7\nm_plus = 0.9\nm_minus = 0.1\nlambda_ = 0.5\nalpha = 0.0005\nepochs = 30\nno_of_secondary_capsules = 10\n\noptimizer = tf.keras.optimizers.Adam()\nparams = {\n    \"no_of_conv_kernels\": 16,\n    \"no_of_primary_capsules\": 32,\n    \"no_of_secondary_capsules\": 10,\n    \"primary_capsule_vector\": 8,\n    \"secondary_capsule_vector\": 16,\n    \"r\":3,\n}\n\n\n# checkpoint_path = '.\/logs\/model\/capsule'\n\n# stamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\n# logdir = '.\/logs\/func\/%s' % stamp\n# writer = tf.summary.create_file_writer(logdir)\n\n# scalar_logdir = '.\/logs\/scalars\/%s' % stamp\n# file_writer = tf.summary.create_file_writer(scalar_logdir + \"\/metrics\")","2a8741da":"# Load the dataset from Kaggle\ndef get_cv2_image(path, img_size, color_type):\n    # Loading as Grayscale image\n    if color_type == 1:\n        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    # Loading as color image\n    elif color_type == 3:\n        img = cv2.imread(path, cv2.IMREAD_COLOR)\n    # Reduce size\n    img = cv2.resize(img[:500], img_size) \n    return img\n\ndef load_data(img_size , color_type):\n    start_time = time.time()\n    training_images = []\n    training_labels = []\n\n    # Loop over the training folder \n    for class_ in tqdm(range(classes)):\n        \n        print('Loading directory c{}'.format(class_))\n        files = glob(os.path.join('..\/input\/state-farm-distracted-driver-detection\/imgs\/train', 'c' + str(class_), '*.jpg'))\n        \n        for file in files:\n            img = get_cv2_image(file, img_size , color_type)\n            training_images.append(img)\n            training_labels.append(class_) \n    \n    print(\"Data Loaded in {} Min\".format((time.time() - start_time)\/60))\n    return training_images, training_labels \n\n\n\ntrain_X, train_y = load_data( (imageWidth,imageHeight) , colorDepth)","7680c6ef":"# train_X = train_X[2:]\n# train_y = train_y[2:]\ntrain_X = np.array(train_X).reshape(len(train_X),imageWidth,imageHeight,colorDepth)\ntrain_X = tf.cast(train_X, dtype=tf.float32)\n\ntrain_y = np.array(train_y)\ntrain_X = train_X \/ 255.0\n\n\ndataset = tf.data.Dataset.from_tensor_slices((train_X, train_y))\n\ndataset = dataset.shuffle(buffer_size=len(dataset), reshuffle_each_iteration=True)\n\n\n\n\ntrain_dataset = dataset.take(int(len(dataset)*0.85))\ntest_dataset = dataset.skip(int(len(dataset)*0.85))\n\ntraining_dataset_size = len(dataset)\ntrain_size = len(train_dataset)\ntest_size = len(test_dataset)\n\ndataset = dataset.batch(batch_size=32)\ntrain_dataset = train_dataset.batch(batch_size=32)\ntest_dataset = test_dataset.batch(batch_size=32)\n\n\n\ncount = 0\nfor a, b in train_dataset:\n    count += 1\n    print(b)\n    if(count > 10):\n        break\n\n\n\n\nprint((training_dataset_size))\nprint(train_size)\n\n\n","0eeead53":"class CapsuleNetwork(tf.keras.Model):\n    def __init__(self, no_of_conv_kernels, no_of_primary_capsules, primary_capsule_vector, no_of_secondary_capsules, secondary_capsule_vector, r):\n        super(CapsuleNetwork, self).__init__()\n        self.no_of_conv_kernels = no_of_conv_kernels\n        self.no_of_primary_capsules = no_of_primary_capsules\n        self.primary_capsule_vector = primary_capsule_vector\n        self.no_of_secondary_capsules = no_of_secondary_capsules\n        self.secondary_capsule_vector = secondary_capsule_vector\n        self.r = r\n        \n        \n        with tf.name_scope(\"Variables\") as scope:\n            self.convolution = tf.keras.layers.Conv2D(self.no_of_conv_kernels, [9,9], strides=[1,1], name='ConvolutionLayer', activation='relu')\n            self.primary_capsule = tf.keras.layers.Conv2D(self.no_of_primary_capsules * self.primary_capsule_vector, [9,9], strides=[2,2], name=\"PrimaryCapsule\")\n            self.w = tf.Variable(tf.random_normal_initializer()(shape=[1, 1152, self.no_of_secondary_capsules, self.secondary_capsule_vector, self.primary_capsule_vector]), dtype=tf.float32, name=\"PoseEstimation\", trainable=True)\n            self.dense_1 = tf.keras.layers.Dense(units = 256, activation='relu')\n            self.dense_2 = tf.keras.layers.Dense(units = 512, activation='relu')\n            self.dense_3 = tf.keras.layers.Dense(units = 784, activation='sigmoid', dtype='float32')\n        \n    def build(self, input_shape):\n        pass\n        \n    def squash(self, s):\n        with tf.name_scope(\"SquashFunction\") as scope:\n            s_norm = tf.norm(s, axis=-1, keepdims=True)\n            return tf.square(s_norm)\/(1 + tf.square(s_norm)) * s\/(s_norm + epsilon)\n    \n    @tf.function\n    def call(self, inputs):\n        input_x, y = inputs\n        # input_x.shape: (None, 28, 28, 1)\n        # y.shape: (None, 10)\n        \n        x = self.convolution(input_x) # x.shape: (None, 20, 20, 256)\n        x = self.primary_capsule(x) # x.shape: (None, 6, 6, 256)\n        \n        with tf.name_scope(\"CapsuleFormation\") as scope:\n            u = tf.reshape(x, (-1, self.no_of_primary_capsules * x.shape[1] * x.shape[2], 8)) # u.shape: (None, 1152, 8)\n            u = tf.expand_dims(u, axis=-2) # u.shape: (None, 1152, 1, 8)\n            u = tf.expand_dims(u, axis=-1) # u.shape: (None, 1152, 1, 8, 1)\n            u_hat = tf.matmul(self.w, u) # u_hat.shape: (None, 1152, 10, 16, 1)\n            u_hat = tf.squeeze(u_hat, [4]) # u_hat.shape: (None, 1152, 10, 16)\n\n        \n        with tf.name_scope(\"DynamicRouting\") as scope:\n            b = tf.zeros((input_x.shape[0], 1152, self.no_of_secondary_capsules, 1)) # b.shape: (None, 1152, 10, 1)\n            for i in range(self.r): # self.r = 3\n                c = tf.nn.softmax(b, axis=-2) # c.shape: (None, 1152, 10, 1)\n                s = tf.reduce_sum(tf.multiply(c, u_hat), axis=1, keepdims=True) # s.shape: (None, 1, 10, 16)\n                v = self.squash(s) # v.shape: (None, 1, 10, 16)\n                agreement = tf.squeeze(tf.matmul(tf.expand_dims(u_hat, axis=-1), tf.expand_dims(v, axis=-1), transpose_a=True), [4]) # agreement.shape: (None, 1152, 10, 1)\n                # Before matmul following intermediate shapes are present, they are not assigned to a variable but just for understanding the code.\n                # u_hat.shape (Intermediate shape) : (None, 1152, 10, 16, 1)\n                # v.shape (Intermediate shape): (None, 1, 10, 16, 1)\n                # Since the first parameter of matmul is to be transposed its shape becomes:(None, 1152, 10, 1, 16)\n                # Now matmul is performed in the last two dimensions, and others are broadcasted\n                # Before squeezing we have an intermediate shape of (None, 1152, 10, 1, 1)\n                b += agreement\n                \n        with tf.name_scope(\"Masking\") as scope:\n            y = tf.expand_dims(y, axis=-1) # y.shape: (None, 10, 1)\n            y = tf.expand_dims(y, axis=1) # y.shape: (None, 1, 10, 1)\n            mask = tf.cast(y, dtype=tf.float32) # mask.shape: (None, 1, 10, 1)\n            v_masked = tf.multiply(mask, v) # v_masked.shape: (None, 1, 10, 16)\n            \n        with tf.name_scope(\"Reconstruction\") as scope:\n            v_ = tf.reshape(v_masked, [-1, self.no_of_secondary_capsules * self.secondary_capsule_vector]) # v_.shape: (None, 160)\n            reconstructed_image = self.dense_1(v_) # reconstructed_image.shape: (None, 512)\n            reconstructed_image = self.dense_2(reconstructed_image) # reconstructed_image.shape: (None, 1024)\n            reconstructed_image = self.dense_3(reconstructed_image) # reconstructed_image.shape: (None, 784)\n        \n        return v, reconstructed_image\n\n    @tf.function\n    def predict_capsule_output(self, inputs):\n        x = self.convolution(inputs) # x.shape: (None, 20, 20, 256)\n        x = self.primary_capsule(x) # x.shape: (None, 6, 6, 256)\n        \n        with tf.name_scope(\"CapsuleFormation\") as scope:\n            u = tf.reshape(x, (-1, self.no_of_primary_capsules * x.shape[1] * x.shape[2], 8)) # u.shape: (None, 1152, 8)\n            u = tf.expand_dims(u, axis=-2) # u.shape: (None, 1152, 1, 8)\n            u = tf.expand_dims(u, axis=-1) # u.shape: (None, 1152, 1, 8, 1)\n            u_hat = tf.matmul(self.w, u) # u_hat.shape: (None, 1152, 10, 16, 1)\n            u_hat = tf.squeeze(u_hat, [4]) # u_hat.shape: (None, 1152, 10, 16)\n\n        \n        with tf.name_scope(\"DynamicRouting\") as scope:\n            b = tf.zeros((inputs.shape[0], 1152, self.no_of_secondary_capsules, 1)) # b.shape: (None, 1152, 10, 1)\n            for i in range(self.r): # self.r = 3\n                c = tf.nn.softmax(b, axis=-2) # c.shape: (None, 1152, 10, 1)\n                s = tf.reduce_sum(tf.multiply(c, u_hat), axis=1, keepdims=True) # s.shape: (None, 1, 10, 16)\n                v = self.squash(s) # v.shape: (None, 1, 10, 16)\n                agreement = tf.squeeze(tf.matmul(tf.expand_dims(u_hat, axis=-1), tf.expand_dims(v, axis=-1), transpose_a=True), [4]) # agreement.shape: (None, 1152, 10, 1)\n                # Before matmul following intermediate shapes are present, they are not assigned to a variable but just for understanding the code.\n                # u_hat.shape (Intermediate shape) : (None, 1152, 10, 16, 1)\n                # v.shape (Intermediate shape): (None, 1, 10, 16, 1)\n                # Since the first parameter of matmul is to be transposed its shape becomes:(None, 1152, 10, 1, 16)\n                # Now matmul is performed in the last two dimensions, and others are broadcasted\n                # Before squeezing we have an intermediate shape of (None, 1152, 10, 1, 1)\n                b += agreement\n        return v\n\n    @tf.function\n    def regenerate_image(self, inputs):\n        with tf.name_scope(\"Reconstruction\") as scope:\n            v_ = tf.reshape(inputs, [-1, self.no_of_secondary_capsules * self.secondary_capsule_vector]) # v_.shape: (None, 160)\n            reconstructed_image = self.dense_1(v_) # reconstructed_image.shape: (None, 512)\n            reconstructed_image = self.dense_2(reconstructed_image) # reconstructed_image.shape: (None, 1024)\n            reconstructed_image = self.dense_3(reconstructed_image) # reconstructed_image.shape: (None, 784)\n        return reconstructed_image","0b139be2":"tf.summary.trace_on(graph=True, profiler=True)\nkeras.backend.clear_session()\nmodel = CapsuleNetwork(**params)\n\n\ndef safe_norm(v, axis=-1, epsilon=1e-7):\n    v_ = tf.reduce_sum(tf.square(v), axis = axis, keepdims=True)\n    return tf.sqrt(v_ + epsilon)\n\ndef loss_function(v, reconstructed_image, y, y_image):\n    prediction = safe_norm(v)\n    prediction = tf.reshape(prediction, [-1, no_of_secondary_capsules])\n    \n    left_margin = tf.square(tf.maximum(0.0, m_plus - prediction))\n    right_margin = tf.square(tf.maximum(0.0, prediction - m_minus))\n    \n    l = tf.add(y * left_margin, lambda_ * (1.0 - y) * right_margin)\n    \n    margin_loss = tf.reduce_mean(tf.reduce_sum(l, axis=-1))\n    \n    y_image_flat = tf.reshape(y_image, [-1, 784])\n    reconstruction_loss = tf.reduce_mean(tf.square(y_image_flat - reconstructed_image))\n    \n    loss = tf.add(margin_loss, alpha * reconstruction_loss)\n    \n    return loss","b4ba16ca":"def train(x,y):\n    y_one_hot = tf.one_hot(y, depth=10)\n    with tf.GradientTape() as tape:\n        v, reconstructed_image = model([x, y_one_hot])\n        loss = loss_function(v, reconstructed_image, y_one_hot, x)\n    grad = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(grad, model.trainable_variables))\n    return loss\n\ndef get_loss(x,y):\n    y_one_hot = tf.one_hot(y, depth=10)\n    with tf.GradientTape() as tape:\n        v, reconstructed_image = model([x, y_one_hot])\n        loss = loss_function(v, reconstructed_image, y_one_hot, x)\n    return loss","af909643":"\n\nprint(train_X.shape)\nprint(train_y.shape)\n\n_ = train(train_X[:1],train_y[:1])\n# with writer.as_default():\n#     tf.summary.trace_export(name=\"my_func_trace\", step=0, profiler_outdir=logdir)\n\n# tf.summary.trace_off()\nmodel.save_weights('my_model_weights.h5')\nmodel.summary()","bbd79162":"def predict(model, x):\n    pred = safe_norm(model.predict_capsule_output(x))\n    pred = tf.squeeze(pred, [1])\n    return np.argmax(pred, axis=1)[:,0]\n\n# checkpoint = tf.train.Checkpoint(model=model)\n# %tensorboard --logdir .\/logs\n","e8f57123":"count = 10\ntrain_dataset = train_dataset.shuffle(64)\nfor x,y in train_dataset:\n    print(predict(model, x))\n    #print(x)\n    print(y)\n    count -= 1\n    if count < 0:\n        break","6f0f0254":"def reset_weights(model):\n    session = tensorflow.keras.backend.get_session()\n    for layer in model.layers: \n        if hasattr(layer, 'kernel_initializer'):\n            layer.kernel.initializer.run(session=session)\n\n    ","b4cdbd4f":"losses = []\naccuracy = []\ntest_acc = []\n\ntest_losses = []\n#keras.backend.clear_session()\n#model.load_weights('my_model_weights.h5')\nK.get_session().close()\nK.set_session(tf.Session())\nK.get_session().run(tf.global_variables_initializer())\nfor i in range(1, epochs+1, 1):\n\n    loss = 0\n    with tqdm(total=len(train_dataset)) as pbar:\n        \n        description = \"Epoch \" + str(i) + \"\/\" + str(epochs)\n        pbar.set_description_str(description)\n\n        for X_batch, y_batch in train_dataset:\n\n            loss += train(X_batch,y_batch)\n            pbar.update(1)\n\n        loss \/= len(train_dataset)\n        losses.append(loss.numpy())\n        \n        training_sum = 0\n\n        print_statement = \"Loss :\" + str(loss.numpy()) + \" Evaluating Accuracy ...\"\n        pbar.set_postfix_str(print_statement)\n\n        for X_batch, y_batch in train_dataset:\n            training_sum += sum(predict(model, X_batch)==y_batch.numpy())\n        accuracy.append(training_sum\/train_size)\n        \n        training_sum = 0\n        test_loss = 0\n        \n        for X_batch, y_batch in test_dataset:\n            training_sum += sum(predict(model, X_batch)==y_batch.numpy())\n            test_loss += get_loss(X_batch, y_batch)\n        \n        test_loss \/= len(test_dataset)\n        test_losses.append(test_loss)\n        test_acc.append(training_sum\/test_size)\n            \n\n#         with file_writer.as_default():\n#             tf.summary.scalar('Loss', data=loss.numpy(), step=i)\n#             tf.summary.scalar('Accuracy', data=accuracy[-1], step=i)\n#             tf.summary.scalar('Test Acc', data = test_acc[-1], step=i)\n        \n        print_statement = \"Loss :\" + str(loss.numpy()) + \" Accuracy :\" + str(accuracy[-1])  + \" Test Acc :\" + str(test_acc[-1])\n\n#         if i % 10 == 0:\n#             print_statement += ' Checkpoint Saved'\n#             checkpoint.save(checkpoint_path)\n        \n        pbar.set_postfix_str(print_statement)","97fe0322":"import matplotlib.pyplot as plt\nimport numpy as np\n\nplt.plot(losses, label = \"train\")\nplt.plot(test_losses, label = \"test\")\nplt.title('Loss over Time')\nplt.xlabel('Loss')\nplt.ylabel('Epoch')\nplt.legend()\nplt.show()","052020be":"import matplotlib.pyplot as plt\nimport numpy as np\n\nplt.plot(accuracy, label = \"train\")\nplt.plot(test_acc, label = \"test\")\nplt.title('Accuracy over Time')\nplt.xlabel('Accuracy')\nplt.ylabel('Epoch')\nplt.legend()\nplt.show()","55d4c428":"losses = []\naccuracy = []\nfor i in range(1, epochs+1, 1):\n\n    loss = 0\n    with tqdm(total=len(dataset)) as pbar:\n        \n        description = \"Epoch \" + str(i) + \"\/\" + str(epochs)\n        pbar.set_description_str(description)\n\n        for X_batch, y_batch in dataset:\n\n            loss += train(X_batch,y_batch)\n            pbar.update(1)\n\n        loss \/= len(dataset)\n        losses.append(loss.numpy())\n        \n        training_sum = 0\n\n        print_statement = \"Loss :\" + str(loss.numpy()) + \" Evaluating Accuracy ...\"\n        pbar.set_postfix_str(print_statement)\n\n        for X_batch, y_batch in dataset:\n            training_sum += sum(predict(model, X_batch)==y_batch.numpy())\n        accuracy.append(training_sum\/training_dataset_size)\n\n        with file_writer.as_default():\n            tf.summary.scalar('Loss', data=loss.numpy(), step=i)\n            tf.summary.scalar('Accuracy', data=accuracy[-1], step=i)\n        \n        print_statement = \"Loss :\" + str(loss.numpy()) + \" Accuracy :\" + str(accuracy[-1])\n\n#         if i % 10 == 0:\n#             print_statement += ' Checkpoint Saved'\n#             checkpoint.save(checkpoint_path)\n        \n        pbar.set_postfix_str(print_statement)","920d7f74":"# Load the dataset from Kaggle\ndef get_cv2_image(path, img_size, color_type):\n    # Loading as Grayscale image\n    if color_type == 1:\n        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    # Loading as color image\n    elif color_type == 3:\n        img = cv2.imread(path, cv2.IMREAD_COLOR)\n    # Reduce size\n    img = cv2.resize(img[:500], img_size) \n    return img\n\ndef load_data(img_size , color_type, training):\n    start_time = time.time()\n    training_images = []\n    training_labels = []\n\n    # Loop over the training folder \n    for class_ in tqdm(range(classes)):\n        \n        print('Loading directory c{}'.format(class_))\n        if(training):\n            files = glob(os.path.join('..\/input\/state-farm-distracted-driver-detection\/imgs\/train', 'c' + str(class_), '*.jpg'))\n        else:\n            files = glob(os.path.join('..\/input\/state-farm-distracted-driver-detection\/imgs\/test', 'c' + str(class_), '*.jpg'))\n        \n        for file in files:\n            img = get_cv2_image(file, img_size , color_type)\n            training_images.append(img)\n            training_labels.append(class_) \n    \n    print(\"Data Loaded in {} Min\".format((time.time() - start_time)\/60))\n    return training_images, training_labels \n\ntrain_X, train_y = load_data( (imageWidth,imageHeight) , colorDepth, True)\ntest_X, test_y = load_data( (imageWidth,imageHeight) , colorDepth, False)\n\ntrain_y = np_utils.to_categorical(train_y, classes)\ntest_y = np_utils.to_categorical(test_y, classes)\n","2c28b937":"#now we prep our data\nnpLabels = np_utils.to_categorical(labels, classes) #we're casting the list of ints as a list of class labels\ntrainingImages, validationImages, trainingLabels, validationLabels = train_test_split(images, npLabels, test_size=0.15,shuffle=True)\n\n#transform our images into array that can be fed into the machine learning model\ntrainingImages = np.array(trainingImages).reshape(len(trainingImages),imageWidth,imageHeight,colorDepth)\nvalidationImages = np.array(validationImages).reshape(len(validationImages),imageWidth,imageHeight,colorDepth)\n\nprint(type(trainingImages))","1a34309d":"resNet  = tf.keras.applications.resnet.ResNet50(include_top = False,weights = 'imagenet',input_shape = (imageWidth,imageHeight,colorDepth))\nresNet.summary()","2fc062f3":"#resnet50 has some random output, we need to add a final layer that fits the output of our problem\n#preppedOutput = resNet.output #set this layer to the output layer of initial model\n#preppedOutput = tf.keras.layers.Flatten()(preppedOutput) #this takes our many dimensional output, and converts it to a longer 1 dimensional output\n\n#add a dense fully connected output with a number of nodes equal to our classes\n#ourOutput =tf.keras.layers.Dense(classes,activation = tf.nn.softmax)(preppedOutput)\n#finalModel = tf.keras.models.Model(inputs=resNet.inputs, outputs=ourOutput)\n\n#let's use a simple bread and butter model model and compare the two!\nsimpleModel = Sequential([\n  layers.experimental.preprocessing.Rescaling(1.\/255, input_shape=(100, 100, colorDepth)),\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(32 * 8, [9,9], strides=[2,2], name=\"PrimaryCapsule\"),\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n  layers.Dropout(0.5),\n  layers.MaxPooling2D(),\n  layers.Flatten(),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(classes)\n])\n\n\n\n#compile the model with the learning rate specified above, and standard loss config from resnet example\n#finalModel.compile(optimizer=tf.keras.optimizers.Adam(learningRate),\n#              loss=tf.keras.losses.CategoricalCrossentropy(from_logits = False),\n#              metrics=['accuracy'])\n\nsimpleModel.compile(optimizer=tf.keras.optimizers.Adam(learningRate),\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits = False),\n              metrics=['accuracy'])\n\n#print model details\n#finalModel.summary()\nsimpleModel.summary()","b729899e":"model2 = Sequential()\nmodel2.add(Dense(500, input_dim=2, activation='relu'))\n\nmodel2.add(Dense(1, activation='sigmoid'))\nmodel2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel2.summary()","d197451a":"#train resnet model\ntrainingResNet = finalModel.fit(\n      x = train_X,y=train_y,#training data\n      validation_data=(test_X,test_y), #validation data\n      batch_size = batchSize,\n      epochs=epochNum,\n      verbose=1)","725a72d8":"#train our simple model\ntrainingSimple = simpleModel.fit(\n      x = train_X,y=train_y,#training data\n      validation_data=(test_X,test_y), #validation data\n      batch_size = batchSize,\n      epochs=epochNum,\n      verbose=1)","6f25fdf7":"OLD TRAINING, IGNORE PAST THIS POINT"}}