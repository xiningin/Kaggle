{"cell_type":{"7ef237a0":"code","4f4d9eb8":"code","9a2898c2":"code","d33d447a":"code","884e6981":"code","05aa3aa1":"code","11f0bac5":"code","167e1048":"code","9558e810":"code","b1bb285c":"code","4a2b77ba":"code","4dee140d":"code","5b559272":"code","336f2a5e":"code","7dfa87a0":"markdown","f1c38e73":"markdown","86970a46":"markdown","8e18abda":"markdown","1b0c7d5f":"markdown","44a3b046":"markdown","835f65f9":"markdown","7c25e01a":"markdown","c2c710de":"markdown","75a0a7a2":"markdown","771f595f":"markdown","c9a39ee9":"markdown"},"source":{"7ef237a0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4f4d9eb8":"#importing necessery libraries for future analysis of the dataset\n!pip install calmap\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport folium\nfrom folium.plugins import HeatMapWithTime, TimestampedGeoJson\nimport matplotlib.style as style \nstyle.use('fivethirtyeight')\nimport numpy as np; np.random.seed(sum(map(ord, 'calmap')))\nimport pandas as pd\nimport calmap","9a2898c2":"# Pandas to read Covid Tweets dataset\ntweets = pd.read_csv('..\/input\/covid19-tweets\/covid19_tweets.csv')","d33d447a":"# Examining the dataset from begining\ntweets.head()","884e6981":"# Checking the shape of dataset\ntweets.shape","05aa3aa1":"tweets.info()","11f0bac5":"# World City Dataset\n\ncities = pd.read_csv('..\/input\/world-cities-datasets\/worldcities.csv')","167e1048":"# Exploring city dataset\ncities.head()","9558e810":"## Duplicate Location in Tweets Dataset\n\ntweets[\"location\"] = tweets[\"user_location\"]\ntweets[\"country\"] = np.NaN","b1bb285c":"user_location = tweets['location'].fillna(value='').str.split(',')","4a2b77ba":"lat = cities['lat'].fillna(value = '').values.tolist()\nlng = cities['lng'].fillna(value = '').values.tolist()\ncountry = cities['country'].fillna(value = '').values.tolist()\n\n# Getting all alpha 3 codes into  a list\nworld_city_iso3 = []\nfor c in cities['iso3'].str.lower().str.strip().values.tolist():\n    if c not in world_city_iso3:\n        world_city_iso3.append(c)\n        \n# Getting all alpha 2 codes into  a list    \nworld_city_iso2 = []\nfor c in cities['iso2'].str.lower().str.strip().values.tolist():\n    if c not in world_city_iso2:\n        world_city_iso2.append(c)\n        \n# Getting all countries into  a list        \nworld_city_country = []\nfor c in cities['country'].str.lower().str.strip().values.tolist():\n    if c not in world_city_country:\n        world_city_country.append(c)\n\n# Getting all amdin names into  a list\nworld_states = []\nfor c in cities['admin_name'].str.lower().str.strip().tolist():\n    world_states.append(c)\n\n\n# Getting all cities into  a list\nworld_city = cities['city'].fillna(value = '').str.lower().str.strip().values.tolist()\n\n","4dee140d":"\nfor each_loc in range(len(user_location)):\n    ind = each_loc\n    each_loc = user_location[each_loc]\n    for each in each_loc:\n        each = each.lower().strip()\n        if each in world_city:\n            order = world_city.index(each)\n            tweets['country'][ind] = country[order]\n            continue\n        if each in world_states:\n            order= world_states.index(each)\n            tweets['country'][ind] = country[order]\n            continue\n        if each in world_city_country:\n            order = world_city_country.index(each)\n            tweets['country'][ind] = world_city_country[order]\n            continue\n        if each in world_city_iso2:\n            order = world_city_iso2.index(each)\n            tweets['country'][ind] = world_city_country[order]\n            continue\n        if each in world_city_iso3:\n            order = world_city_iso3.index(each)\n            tweets['country'][ind] = world_city_country[order]\n            continue\n","5b559272":"print('Total Number of valid Tweets Available: ',tweets['country'].isnull().sum())","336f2a5e":"tweet_per_country = tweets['country'].str.lower().dropna()\ntw = tweet_per_country.value_counts().rename_axis('Country').reset_index(name='Tweet Count')\nprint(tw)\nplt.rcParams['figure.figsize'] = (15,10)\nplt.title('Top 20 Countries with Most Tweets',fontsize=15)\nsns.set_palette(\"husl\")\nax = sns.barplot(y=tw['Country'].head(20),x=tw['Tweet Count'].head(20))","7dfa87a0":"# <center> COVID'19 Tweets Geographical Distribution \ud83c\udf0e<\/center>","f1c38e73":"## Dataset Description\n\nThese tweets are collected using Twitter API and a Python script. A query for this high-frequency hashtag (#covid19) is run on a daily basis for a certain time period, to collect a larger number of tweets samples.\n\n**Content**\nThe tweets have #covid19 hashtag. Collection started on 25\/7\/2020, with an initial 17k batch and will continue on a daily basis.\n\n\nThe collection script can be found here: https:\/\/github.com\/gabrielpreda\/covid-19-tweets","86970a46":"# Data Insights\nWe have around 166,656 Tweets in the dataset and each tweet will possibly contain 13 parameters. Lets try to explore how many of them are necessary for visualizing the geographical distribution. To visualize geographically we need to have either country codes or Location co-ordinates. But te dataset has a parameter '*user_location*' in which the locations are very vague to plot them.\n\n\nMain idea is to extract geographical cordinates of user. Here I'm using two other datasets to improve the location feature in dataset.\n* world-cities-datasets (https:\/\/www.kaggle.com\/viswanathanc\/world-cities-datasets)\n\nworld-cities-datasets is used to identify Alpha 2,Alpha 3,Country Name, City Name from user location field in Covid Tweet Dataset.","8e18abda":"# Feature Engineering","1b0c7d5f":"## Top 20 Countries with Most Tweets ","44a3b046":"## Importing Necesssary Libraries","835f65f9":"# Data Mapping among Datasets\n\n1. Each User Location may possibly contain combination of city name, country name with country codes. (Ex: Pewee Valley, KY)\n2. We need to split the user location from user location\n3. Now each location will be possibily containing list of (country, city, code)\n4. If we find city name or admin name match in world city database we will assign the location from world city database to tweets dataset for a respective row.","7c25e01a":"## Missing Data Handling in tweets dataset\n\nHere i dont want to remove the NaN values now. So i'm trying to replace it with a empty string.","c2c710de":"# Dataset Exploration","75a0a7a2":"# Data Analysis","771f595f":"<center><h5>Recovered more than 70% of Geo Cordinates from user tweeted address<\/h5><\/center>","c9a39ee9":"<center><img src='https:\/\/d25yuvogekh0nj.cloudfront.net\/2019\/08\/Twitter-Banner-Size-Guide-blog-banner-1250x500.png'><\/center>"}}