{"cell_type":{"88b04db8":"code","2a5b72b0":"code","db02344b":"code","94957cda":"code","b315789f":"code","676c1cef":"code","d4ee6756":"code","d60e3f8a":"code","31a5a6f5":"code","bfaa893d":"code","4b57cd33":"code","8dff7ce1":"code","4b7c381e":"code","2a093238":"code","73b4a281":"code","12013ced":"code","2c211e1d":"code","906c9fd7":"markdown","3e9a36de":"markdown","40030f3a":"markdown"},"source":{"88b04db8":"import json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau","2a5b72b0":"# Read the .json file into a pandas DataFrame\npath = \"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json\"\ndf = pd.read_json(path, lines=True)","db02344b":"# Show the first 5 elements from the DataFrame\ndf.head()","94957cda":"# Get insights about the dataset\nprint(df.info())\nprint(\"\\n--------------------------------\\n\")\nprint(df.describe())","b315789f":"# Check how many examples of each class\ndf.is_sarcastic.value_counts()","676c1cef":"# Split the sentences and the associated labels into two DataFrames\ndf_sentence = df[\"headline\"]\ndf_label = df[\"is_sarcastic\"]","d4ee6756":"# Split the sentences and labels into training and validation sets\ntrain_sentences, val_sentences, train_labels, val_labels = train_test_split(df_sentence.to_numpy(),\n                                                                            df_label.to_numpy(),\n                                                                            test_size=0.1, \n                                                                            random_state=42)\n\nlen(train_sentences), len(val_sentences), len(train_labels), len(val_labels)","d60e3f8a":"# Place the USE into an encoder_layer for later use in model\n# For faster training but less accuarcy you can use: \n# https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4\nuse_url = \"https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/5\"\nsentence_encoder_layer = hub.KerasLayer(use_url,\n                                        # shape of inputs coming to our model \n                                        input_shape=[],\n                                        # data type of inputs coming to the USE layer\n                                        dtype=tf.string,\n                                        # keep the pretrained weights (this is a feature \n                                        # extractor without fine-tuning)\n                                        trainable=False, \n                                        name=\"USE\")","31a5a6f5":"# Build model\nmodel = tf.keras.Sequential([\n  sentence_encoder_layer,\n  tf.keras.layers.Dense(64, activation=\"relu\"),\n  tf.keras.layers.Dense(1, activation=\"sigmoid\")\n], name=\"model_USE_large_v5\")\n\n# Compile model\nmodel.compile(loss=\"binary_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"accuracy\"])","bfaa893d":"# Check the models architecture\nmodel.summary()","4b57cd33":"from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n# Setup callback functions\n\nmodel_checkpoint = ModelCheckpoint(filepath=\".\/model_checkpoint\/checkpoint.ckpt\",\n                                   # set to False to save the entire model\n                                   save_weights_only=True,\n                                   # set to False to save every model every epoch\n                                   save_best_only=True,\n                                   save_freq=\"epoch\",\n                                   monitor=\"val_loss\",\n                                   verbose=1)\n\nearly_stopping = EarlyStopping(patience=5, \n                               monitor=\"val_loss\",\n                               verbose=1)\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', \n                              factor=0.2, \n                              patience=2, \n                              min_lr=0.0005,\n                              verbose=1)\n\ncallbacks=[model_checkpoint, \n           early_stopping, \n           reduce_lr]","8dff7ce1":"# Train the model\nEPOCHS = 50\n\nhistory_model = model.fit(train_sentences,\n                          train_labels,\n                          epochs=EPOCHS,\n                          validation_data=(val_sentences, val_labels),\n                          callbacks=callbacks)","4b7c381e":"# Reload the best model which was saved by the ModelCheckpoint callback \n# function and evaluate the model with the validation sentences\/labels\nmodel.load_weights('.\/model_checkpoint\/checkpoint.ckpt')\neval_model = model.evaluate(val_sentences, val_labels)","2a093238":"# Check the loss and accuracy plots\ndef plot_loss_curves(history):\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    accuracy = history.history['accuracy']\n    val_accuracy = history.history['val_accuracy']\n\n    epochs = range(len(history.history['loss']))\n\n    # Plot loss\n    plt.plot(epochs, loss, label='training_loss')\n    plt.plot(epochs, val_loss, label='val_loss')\n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.legend()\n\n    # Plot accuracy\n    plt.figure()\n    plt.plot(epochs, accuracy, label='training_accuracy')\n    plt.plot(epochs, val_accuracy, label='val_accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epochs')\n    plt.legend();\n    \nplot_loss_curves(history_model)","73b4a281":"# Make predictions with the validation sentences\nmodel_pred_probs = model.predict(val_sentences)\nmodel_pred_probs[:10]","12013ced":"# Convert prediction probabilities into labels\nmodel_preds = tf.squeeze(tf.round(model_pred_probs))\nmodel_preds[:10]","2c211e1d":"# Calculate the metrics Accuracy, Precision, Recall and F1-Score\ndef calculate_results(y_true, y_pred):\n    # Calculate model accuracy\n    model_accuracy = accuracy_score(y_true, y_pred) * 100\n    # Calculate model precision, recall and f1 score using \"weighted\" average\n    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n    model_results = {\"accuracy\": round(model_accuracy, 4),\n                  \"precision\": round(model_precision, 4),\n                  \"recall\": round(model_recall, 4),\n                  \"f1\": model_f1}\n    return model_results\n\ncalculate_results(val_labels,\n                  model_preds)","906c9fd7":"**NOTE FOR THE NEXT STEP!**\n\nIf you want to use the Universal-Sentence-Encoder (USE) from TensorFlow HUB, you have to *enable the the internet connection* in this kaggle notebook, else you get the following error:\n > **URLError: <urlopen error [Errno -3] Temporary failure in name resolution>**\n\n* To enable the internet on kaggle you have to verify your phone number in the *settings* section.\n\n* You can also see the following kaggle post:\nhttps:\/\/www.kaggle.com\/product-feedback\/63544","3e9a36de":"# TensorFlow Hub's Universal-Sentence-Encoder - a Step by Step guide","40030f3a":"This approach uses TensorFlow Hub's **Universal-Sentence-Encoder large v.5 (USE)**.\n\nThis notebook is a ***step by step guide*** which can be used very easily by yourself.\n\n*'The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks.'*\n\nYou can find more informations on the tfhub page:\n   >https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/5\n   \nThe USE reached the following metrics for the news-headlines-dataset-for-sarcasm-detection dataset:\n * **Accuracy**:   88.46\n * **Precision**:  88.46\n * **Recall**:     88.47\n * **F1-Score**:   88.46"}}