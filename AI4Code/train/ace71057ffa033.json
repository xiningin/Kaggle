{"cell_type":{"e6f80401":"code","18a90560":"code","8c191648":"code","235550c8":"code","915fc8ea":"code","f5fab4cc":"code","ef5cf55e":"code","2b57f98a":"code","cf417e38":"code","7744d10d":"code","e4b9d1bd":"code","a2517ad3":"code","0f5a793c":"code","4af48ba2":"code","502564b0":"code","4489ce50":"code","d819c803":"code","05e59814":"code","efd1ec9e":"code","0d565d60":"code","7027a97f":"code","8a365245":"code","4d0778e2":"code","a4267030":"code","c274bc6b":"code","042c7552":"code","05ac6be1":"code","81d04c5e":"markdown","59f8703f":"markdown","4880ffe4":"markdown","e87eeeab":"markdown","523ddafb":"markdown","3eafcb02":"markdown","7d4f26b2":"markdown"},"source":{"e6f80401":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","18a90560":"# Imports\nimport matplotlib \nimport sklearn\nimport keras\nfrom sklearn import preprocessing","8c191648":"# Grab the data\ndata_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndata_test  = pd.read_csv('..\/input\/titanic\/test.csv')","235550c8":"# Preview the raw training data\nprint (data_train.info())","915fc8ea":"# List containing training and testing data\nall_data = [data_train, data_test]","f5fab4cc":"# Address null values\nfor dataset in all_data:    \n    \n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)","ef5cf55e":"# Drop unimportant categories from datasets\ndrop_column = ['Cabin', 'Ticket', 'Name']\nfor dataset in all_data:\n    dataset.drop(drop_column, axis=1, inplace = True)","2b57f98a":"# Minimal feature engineering\nfor dataset in all_data:\n    \n    # Create family size category\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n    \n    # Split Fare into categories (qcut)\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 8)\n    \n    # Split Age into categories (cut)\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 10)","cf417e38":"# Drop more categories from datasets\ndrop_column = ['SibSp', 'Parch', 'Fare', 'Age']\nfor dataset in all_data:\n    dataset.drop(drop_column, axis=1, inplace = True)","7744d10d":"data_train.head()","e4b9d1bd":"data_test.head()","a2517ad3":"# Change Categorical Data to Numerical\nlabel = preprocessing.LabelEncoder()\nfor dataset in all_data:    \n    dataset['Sex'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked'] = label.fit_transform(dataset['Embarked'])\n    dataset['AgeBin'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin'] = label.fit_transform(dataset['FareBin'])","0f5a793c":"data_train.head()","4af48ba2":"# Seperate \"answers\" from training data & handle PassengerID\nY = data_train['Survived']\nX = data_train.drop('Survived', axis=1)\nX = X.drop('PassengerId', axis=1)","502564b0":"# Train \/ Test Split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)","4489ce50":"# MinMaxScaler\nscaler = preprocessing.MinMaxScaler()\nscaler.fit(X_train)\n\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","d819c803":"# Make directory for saving\n# os.mkdir(\"Models\") ","05e59814":"# Deep Nueral Net Attempt 1\nimport tensorflow as tf\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Dropout, Flatten\n\n# Initializing a random seed for replication purposes\nnp.random.seed(42)\ntf.random.set_seed(42)\n\n# Initiating an empty neural network\nmodel = Sequential()\n\n# Adding a dense layer with 42 neurons\nmodel.add(Dense(42, input_shape=(6,), activation='relu'))\n\n# Adding a dropout layer for regularization\nmodel.add(Dropout(0.2))\n\n# Adding a dense layer with 16 neurons\nmodel.add(Dense(16, activation='relu'))\n\n# Adding a dropout layer for regularization\nmodel.add(Dropout(0.2))\n\n# Adding a dense layer with 12 neurons\nmodel.add(Dense(12, activation='relu'))\n\n# Adding an output layer\nmodel.add(Dense(1, activation='sigmoid'))\n\n# Initiating optimizer\nopt = keras.optimizers.Adam(learning_rate=0.002)\n\n# Compiling our neural network\nmodel.compile(loss='binary_crossentropy',\n                  optimizer=opt,\n                  metrics=['accuracy'])\n\n#Keras Callback\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=\"\/Models\",\n    save_weights_only=False,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True)\n\n# Fitting our neural network\nhistory = model.fit(X_train,\n                        Y_train, \n                        batch_size=16,\n                        validation_data=(X_test, Y_test),\n                        epochs=40,\n                        callbacks=[model_checkpoint_callback])","efd1ec9e":"import matplotlib.pyplot as plt\n\ndef plotHistory(history):\n    print(\"Max. Validation Accuracy\",max(history.history[\"val_accuracy\"]))\n    pd.DataFrame(history.history).plot(figsize=(12,6))\n    plt.show()","0d565d60":"plotHistory(history)","7027a97f":"from xgboost import XGBClassifier\n\n# Fit XGBoost Model to training data\nxg_model = XGBClassifier(n_estimators = 200)\nxg_model.fit(X_train, Y_train)","8a365245":"# Check Accuracy\nfrom sklearn.metrics import accuracy_score\n\n# Make predictions for test data\nY_pred = xg_model.predict(X_test)\n\n# Evaluate predictions\nxg_accuracy = accuracy_score(Y_test, Y_pred)\nprint(\"Accuracy: %.2f%%\" % (xg_accuracy * 100.0))","4d0778e2":"from sklearn.ensemble import RandomForestClassifier\n\n# Fit XGBoost Model to training data\nrf_model = RandomForestClassifier()\nrf_model.fit(X_train, Y_train)\n\n# Make predictions for test data\nY_pred = rf_model.predict(X_test)\n\n# Evaluate predictions\nxg_accuracy = accuracy_score(Y_test, Y_pred)\nprint(\"Accuracy: %.2f%%\" % (xg_accuracy * 100.0))","a4267030":"# Loading \"best\" model\nbest_model = keras.models.load_model(\"\/Models\/\")","c274bc6b":"# Making predictions\ntesting_data = data_test\ntesting_data = testing_data.drop('PassengerId', axis=1)\ntesting_data = scaler.transform(testing_data)\npredictions = best_model.predict(testing_data)","042c7552":"# Convert predictions from Sigmoid outputs to 1s and 0s\nfor i, predicted in enumerate(predictions):\n    if predicted[0] > 0.5:\n        predicted[0] = 1\n    else:\n        predicted[0] = 0\n        \npredictions = predictions.astype(int)","05ac6be1":"# Creating output csv\noutput = pd.DataFrame({'PassengerId': data_test.PassengerId, 'Survived': predictions[:, 0]})\noutput.to_csv('final_nueral_net_submission.csv', index=False)","81d04c5e":"##### Titanic Passenger Survival Nueral Net Classifier\n- Connor Puhala, 2020\n- CURRENT BEST ON SUBMISSION -> 77.5%\n\nBACKGROUND\nUsually, for this dataset, traditional ML algorithms like Random Forrest, SVG, and XGBoost are chosen over Neural Networks. Is it possible to train a multi-layer neural network in a simple and straightforward way to perform as well as these classic algorithms? \n\nGOAL\nTo train a Neural Network to classify passengers into two categories (survived and not survived) based on a number of pieces of information about them. Seeking >= 77% success on test data.","59f8703f":"WOW! \n\nOver 80% accurate on validation data with extremely beautiful coupling between accuracy\/val_accuracy as well as loss\/val_loss! Our network did NOT fall apart in late epochs as I had seen in the first 20 or so architechtures. Lots of behind the scenes tweaking and here we are! Very proud!\n\nINSIGHTS\n1. 'More dense' layers feeding into 'less dense' layers outperform other designs. Maybe we could call this concept \"filtering\"?\n2. Complexity of network must match complexity of data. Seems to me like our inner nueral layer densitys (42, 16, 12) are small enough to learn from the data without memorizing. Maybe the idea is, how many connections can you foster without allowing the network to memorize the data? Too much dropout and the network loses the capacity to learn.","4880ffe4":"78%! Less than our Nueral Network (82%). Lets try a Random Forest...","e87eeeab":"Lets compare this to an XGBoost Model","523ddafb":"79%. Worse than our Nueral Network(82%) but better than the XGBoost Classifier(78%).","3eafcb02":"Noteable Insights:\n1. Cabin & Age contain large percentage of null values\n2. 'Object' data types must be converted to categorical","7d4f26b2":"We did it! Now time for the real test..."}}