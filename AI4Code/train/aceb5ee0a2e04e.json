{"cell_type":{"e0d62518":"code","d1bd4204":"code","deefc6da":"code","068ca950":"code","dcf8e615":"code","ec60e6d5":"code","dd6f3abd":"code","501060f6":"code","4cad88f8":"code","7ac24973":"code","9306a643":"code","c43fb6f0":"code","03ef2770":"code","c060d2e3":"code","ec093ad1":"code","b8cfd85e":"code","370ab98a":"code","d9241616":"code","54c30e12":"code","eb69442a":"code","f7cd6395":"code","3642f6da":"code","c36292d9":"code","774f3a35":"code","3de8cbfd":"code","85e29613":"code","d71cbc4a":"code","fae7b38e":"code","8469534f":"code","edc17c06":"code","e743a3ef":"code","91120f5b":"code","20323877":"code","b037dc2a":"code","1891afff":"code","5c53b9d5":"code","fd054336":"code","dc9c5d6d":"code","61f47d54":"code","1564f2f7":"code","084bd686":"code","afe446fe":"code","616a4c72":"code","a5baba83":"code","eb10498a":"code","fb7c91cc":"code","67b25f01":"code","53dfaa49":"code","cbb0c826":"markdown","ed8dc5c4":"markdown","af13d4f6":"markdown","567ee6cc":"markdown","29b9811c":"markdown","c063aaa8":"markdown","8ca43df5":"markdown","bb83ae35":"markdown","494c1d69":"markdown","7aa66e8a":"markdown","5313a274":"markdown","9cefb90e":"markdown","19048a6d":"markdown","fea1288a":"markdown","5763dbd6":"markdown","410b4025":"markdown","c8e7354a":"markdown","a105697d":"markdown","86aae635":"markdown","6102c0b3":"markdown","3aca5bef":"markdown","778e4b76":"markdown"},"source":{"e0d62518":"# imports\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import make_grid\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as T\nimport matplotlib.pyplot as plt\n%matplotlib inline","d1bd4204":"import os\n\nDATA_DIR = '..\/input\/celebrities-100k\/100k\/'\nprint(len(os.listdir(DATA_DIR+'\/100k')))","deefc6da":"print(os.listdir(DATA_DIR+'\/100k')[:10])","068ca950":"image_size = 64\nbatch_size = 128\nstats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5) # mean, std for normalize imagess","dcf8e615":"train_ds = ImageFolder(root=DATA_DIR, \n                       transform=T.Compose([T.Resize(image_size),\n                                            T.CenterCrop(image_size), # pick central square crop of it\n                                            T.ToTensor(),\n                                            T.Normalize(*stats)        # normalize => -1 to 1                               \n                                        ]))\n\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True) # use multiple cores\n","ec60e6d5":"def denorm(img_tensors):\n    \"Denormalize image tensor with specified mean and std\"\n    return img_tensors * stats[1][0] + stats[0][0]","dd6f3abd":"def show_images(images, nmax=64):\n  fig, ax = plt.subplots(figsize=(8,8))\n  ax.set_xticks([]); ax.set_yticks([])\n  ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n  \ndef show_batch(dl, nmax=64):\n  for images, _ in dl:\n    show_images(images, nmax)\n    break","501060f6":"show_batch(train_dl)","4cad88f8":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    \"\"\" 3 things:\n    1. Connected to Nvidia GPU\n    2. Cuda drivers\n    3. Pytorch suitable to GPU version\n    then torch.cuda.is_available is True\n    \"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n\ndef to_device(data, device):\n  \"\"\"Move tensor(s) to chosen device\"\"\"\n  if isinstance(data, (list,tuple)):\n      return [to_device(x, device) for x in data]\n  return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","7ac24973":"device = get_default_device()","9306a643":"train_dl = DeviceDataLoader(train_dl, device)","c43fb6f0":"discriminator = nn.Sequential(\n    # in: 3x 64 x 64\n    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(64),\n    nn.LeakyReLU(0.2, inplace=True),\n    # out: 64 x 32 x 32\n\n    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.LeakyReLU(0.2, inplace=True),\n    # out: 128 x 16 x 16\n\n    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(256),\n    nn.LeakyReLU(0.2, inplace=True),\n    # out: 256 x 8 x 8\n\n    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(512),\n    nn.LeakyReLU(0.2, inplace=True),\n    # out: 512 x 4 x 4\n\n    nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n    # out: 1 x 1 x 1\n\n    nn.Flatten(),\n    nn.Sigmoid()\n)","03ef2770":"discriminator = to_device(discriminator, device)","c060d2e3":"# create a tensor Batch_Size,C,H,W\nX = torch.rand(size=(1, 3, 64, 64), dtype=torch.float32, device=device) \nfor layer in discriminator:\n    X = layer(X)\n    print(layer.__class__.__name__,'output shape: \\t',X.shape)","ec093ad1":"latent_size = 128","b8cfd85e":"generator = nn.Sequential(\n    # in: latent_size x 1 x 1\n\n    nn.ConvTranspose2d(latent_size, 512, kernel_size=4, stride=1, padding=0, bias=False),\n    nn.BatchNorm2d(512),\n    nn.ReLU(True),\n    # out: 512 x 4 x 4\n\n    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(256),\n    nn.ReLU(True),\n    # out: 256 x 8 x 8\n\n    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.ReLU(True),\n    # out: 128 x 16 x 16\n\n    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(64),\n    nn.ReLU(True),\n    # out: 64 x 32 x 32\n\n    nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.Tanh()  # output is between -1 to 1\n    # out: 3 x 64 x 64\n)","370ab98a":"X = torch.randn(size=(1, 128, 1, 1))\nfor layer in generator:\n  X = layer(X)\n  print(layer.__class__.__name__,'output shape: \\t',X.shape)","d9241616":"xb = torch.randn(batch_size, latent_size, 1, 1) # random latent tensors\nfake_images = generator(xb)\nprint(fake_images.shape)\nshow_images(fake_images)\n","54c30e12":"generator = to_device(generator, device) # move generator to device","eb69442a":"def train_discriminator(real_images, opt_d):\n  # Clear discriminator gradients\n  opt_d.zero_grad()\n\n  # Pass real images through  discriminator\n  real_preds = discriminator(real_images)\n  real_targets = torch.ones(real_images.size(0), 1, device=device)\n  real_loss = F.binary_cross_entropy(real_preds, real_targets)\n  real_score = torch.mean(real_preds).item()\n\n  # Generate fake images\n  latent = torch.randn(batch_size, latent_size, 1, 1, device=device)\n  fake_images = generator(latent)\n\n  # Pass Fake images through discriminator\n  fake_targets = torch.zeros(fake_images.size(0), 1, device=device)\n  fake_preds = discriminator(fake_images)\n  fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)\n  fake_score = torch.mean(fake_preds).item()\n\n  # Update discriminator weights\n  loss = real_loss + fake_loss\n  loss.backward()\n  opt_d.step()\n  return loss.item(), real_score, fake_score\n\n","f7cd6395":"def train_generator(opt_g):\n  # Clear generator gradients\n  opt_g.zero_grad()\n\n  # Generate fake images\n  latent = torch.randn(batch_size, latent_size, 1,1, device=device)\n  fake_images = generator(latent)\n\n  # Try to fool the discriminator\n  preds = discriminator(fake_images)\n  targets = torch.ones(batch_size, 1, device=device)\n  loss = F.binary_cross_entropy(preds, targets)\n\n  # Update generator \n  loss.backward()\n  opt_g.step()\n\n  return loss.item()","3642f6da":"from torchvision.utils import save_image\n","c36292d9":"sample_dir = 'generated'\nos.makedirs(sample_dir, exist_ok=True)","774f3a35":"def save_samples(index, latent_tensors, show=True):\n  fake_images = generator(latent_tensors)\n  fake_fname = 'generated=images-{0:0=4d}.png'.format(index)\n  save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8)\n  print(\"Saving\", fake_fname)\n\n  if show:\n    fig, ax = plt.subplots(figsize=(8,8))\n    ax.set_xticks([]); ax.set_yticks([])\n    ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))","3de8cbfd":"fixed_latent = torch.randn(64, latent_size, 1, 1, device=device)\n","85e29613":"save_samples(0, fixed_latent)","d71cbc4a":"from tqdm.notebook import tqdm\nimport torch.nn.functional as F","fae7b38e":"def fit(epochs, lr, start_idx = 1):\n  torch.cuda.empty_cache()\n\n  # Losses & scores\n  losses_g = []\n  losses_d = []\n  real_scores = []\n  fake_scores = []\n\n  # Create optimizers\n  opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n  opt_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n\n  for epoch in range(epochs):\n    for real_images, _ in tqdm(train_dl):\n      # Train discriminator\n      loss_d, real_score, fake_score = train_discriminator(real_images, opt_d)\n      # Train generator\n      loss_g = train_generator(opt_g)\n\n    # Record losses & scores\n    losses_g.append(loss_g)\n    losses_d.append(loss_d)\n    real_scores.append(real_score)\n    fake_scores.append(fake_score)\n\n    # Log losses & scores (last batch)\n    print(\"Epoch [{}\/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(epoch+1, epochs, loss_g, loss_d, real_score, fake_score))\n    # Save generated images\n    save_samples(epoch+start_idx, fixed_latent, show=False)\n\n  return losses_g, losses_d, real_scores, fake_scores","8469534f":"# Hyperparameters\nlr = 0.00025\nepochs = 60","edc17c06":"history = fit(epochs, lr)","e743a3ef":"# Save the model checkpoints \ntorch.save(generator.state_dict(), 'G.pth')\ntorch.save(discriminator.state_dict(), 'D.pth')","91120f5b":"losses_g, losses_d, real_scores, fake_scores = history","20323877":"from IPython.display import Image\n","b037dc2a":"Image('.\/generated\/generated=images-0001.png')\n","1891afff":"Image('.\/generated\/generated=images-0005.png')\n","5c53b9d5":"Image('.\/generated\/generated=images-0010.png')\n","fd054336":"Image('.\/generated\/generated=images-0020.png')\n","dc9c5d6d":"Image('.\/generated\/generated=images-0025.png')\n","61f47d54":"Image('.\/generated\/generated=images-0030.png')","1564f2f7":"Image('.\/generated\/generated=images-0035.png')","084bd686":"Image('.\/generated\/generated=images-0040.png')","afe446fe":"Image('.\/generated\/generated=images-0045.png')","616a4c72":"Image('.\/generated\/generated=images-0050.png')","a5baba83":"Image('.\/generated\/generated=images-0055.png')","eb10498a":"Image('.\/generated\/generated=images-0060.png')","fb7c91cc":"import cv2\nimport os\n\nvid_fname = 'gans_training.avi'\n\nprint(\"Starting converting images to video.\")\nfiles = [os.path.join(sample_dir, f) for f in os.listdir(sample_dir) if 'generated' in f]\nfiles.sort()\n\nprint(files)\n\nfourcc = cv2.VideoWriter_fourcc(*'MPEG')\nout = cv2.VideoWriter(vid_fname,fourcc, 1.0, (640,480))\n[out.write(cv2.imread(fname)) for fname in files]\nout.release()\nprint(\"DONE!\")","67b25f01":"plt.plot(losses_d, '-')\nplt.plot(losses_g, '-')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend(['Discriminator', 'Generator'])\nplt.title('Losses');","53dfaa49":"plt.plot(real_scores, '-')\nplt.plot(fake_scores, '-')\nplt.xlabel('epoch')\nplt.ylabel('score')\nplt.legend(['Real', 'Fake'])\nplt.title('Scores');","cbb0c826":"Let's create a directory where we can save intermediate outputs from the generator to visually inspect the progress of the model. We'll also create a helper function to export the generated images.","ed8dc5c4":"Let's load this dataset using the ImageFolder class from torchvision. We will also resize and crop the images to 64x64 px, and normalize the pixel values with a mean & standard deviation of 0.5 for each channel. This will ensure that pixel values are in the range (-1, 1), which is more convenient for training the discriminator. We will also create a data loader to load the data in batches.","af13d4f6":"We'll use a fixed set of input vectors to the generator to see how the individual generated images evolve over time as we train the model. Let's save one set of images before we start training our model.","567ee6cc":"## Training Loop","29b9811c":"## Generator Training\n\n* We generate a batch of images using the generator, pass the into the discriminator.\n\n* We calculate the loss by setting the target labels to 1 i.e. real. We do this because the generator's objective is to \"fool\" the discriminator.\n\n* We use the loss to perform gradient descent i.e. change the weights of the generator, so it gets better at generating real-like images to \"fool\" the discriminator.","c063aaa8":"#### Lets see fake_images generated by generator before training. Just for curiosity.","8ca43df5":"We can visualize the training process by combining the sample images generated after each epoch into a video using OpenCV.\n\n","bb83ae35":"## Using a GPU","494c1d69":"Here are the steps involved in training the discriminator.\n\n* We expect the discriminator to output 1 if the image was picked from the real Anime Faces dataset, and 0 if it was generated using the generator network.\n\n* We first pass a batch of real images, and compute the loss, setting the target labels to 1.\n\n* Then we pass a batch of fake images (generated using the generator) pass them into the discriminator, and compute the loss, setting the target labels to 0.\n\n* Finally we add the two losses and use the overall loss to perform gradient descent to adjust the weights of the discriminator.\n\n\n\nIt's important to note that we don't change the weights of the generator model while training the discriminator (opt_d only affects the discriminator.parameters())","7aa66e8a":"# Celebrity Face Generation with GANs","5313a274":"Here's what it looks like:\n\n\n\nWe can also visualize how the loss changes over time. Visualizing losses is quite useful for debugging the training process. For GANs, we expect the generator's loss to reduce over time, without the discriminator's loss getting too high.","9cefb90e":"#### Lets print output shape after each layer of discrminator.","19048a6d":"## Training Discriminator","fea1288a":"#### Lets print output shape after each layer of generator.","5763dbd6":"Note that In Pytorch -> C x Hx W whereas Matplotlib reques H x W x C. So, permute(1, 2, 0) do this.","410b4025":"# Discriminator Network","c8e7354a":"# Generator Network","a105697d":"Let's create helper functions to denormalize the image tensors and display some sample images from a training batch.\n\n","86aae635":"#### Remember in PyTorch, we have to move data, model in GPU for computation. So, `to_device` do that work.","6102c0b3":"Here's how the generated images look, after the 1st, 5th and 10th epochs of training.\n\n","3aca5bef":"# Build GAN","778e4b76":"We are now ready to train the model. Try different learning rates to see if you can maintain the fine balance between the training the generator and the discriminator.\n\n"}}