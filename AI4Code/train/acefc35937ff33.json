{"cell_type":{"a88b3d53":"code","9c368e02":"code","e924555d":"code","4d2dba79":"code","6d74ea6c":"code","734f2351":"code","c9876b88":"code","96d6d659":"code","328d8d91":"code","9d9f9801":"code","9a48b4de":"code","019f90a2":"code","1149e063":"code","23eff169":"code","e16f115d":"code","aace699f":"code","1af982a6":"code","ba9be299":"code","b5e1cf80":"code","e5dc4908":"code","c78af9f0":"code","d3b7ad23":"code","6a5969e4":"code","83d9d27c":"code","0186e7f0":"code","17e5d77b":"code","66de3ba5":"code","c6d8b5d1":"code","70bda2c2":"code","36e5b862":"code","b600c67a":"code","6418f237":"code","2cab374c":"code","f424a35e":"code","e309052f":"code","182d2d92":"code","fc947ca4":"code","272628de":"code","68c275ef":"code","6a06ddff":"code","dc5b2579":"code","fad7063f":"code","f3a26dff":"code","732e7e65":"code","0b763f53":"code","ae1afccb":"code","041ac7b9":"code","8b279b2c":"code","178ed0a0":"code","1255be47":"code","da618843":"code","2823b05b":"markdown"},"source":{"a88b3d53":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9c368e02":"from pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, KFold\n\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRFRegressor, XGBRegressor\nfrom lightgbm import LGBMRegressor\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error\n\nfrom functools import partial\nimport optuna\noptuna.logging.set_verbosity(optuna.logging.ERROR)","e924555d":"def rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))","4d2dba79":"# Description of data - Lots of categorical columns\n!cat \/kaggle\/input\/house-prices-advanced-regression-techniques\/data_description.txt","6d74ea6c":"DATA_DIR = Path(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/\")\n\ntrain_df = pd.read_csv(DATA_DIR \/ \"train.csv\")\ntest_df = pd.read_csv(DATA_DIR \/ \"test.csv\")\nsub_df = pd.read_csv(DATA_DIR \/ \"sample_submission.csv\")","734f2351":"train_df.head()","c9876b88":"# Find Numerical and Categorical Columns\n# Choose only columns that have at least 1000 non-null values\ntrain_df.info()","96d6d659":"# Most of the homes have sale price between 100-200K\ntrain_df[\"SalePrice\"].hist(bins=50)","328d8d91":"# Converts LongTail to Normal Dist\n# So We should convert target to Log, even if the Metric is not using it\nnp.log(train_df[\"SalePrice\"]).hist(bins=50);","9d9f9801":"# Observe that only two houses have over 700K Sale Price\n# May be we should exclude them from dataset since there's no much we can gain from two data points\ntrain_df.plot(kind=\"scatter\", x=\"Id\", y=\"SalePrice\", alpha=0.25)","9a48b4de":"plt.figure(figsize=(12,10))\nsns.heatmap(train_df.corr(), cmap='Greys');","019f90a2":"train_df.corr()","1149e063":"corr_cols = train_df.corr()[\"SalePrice\"].nlargest(15).index\ncorr_cols","23eff169":"plt.figure(figsize=(10, 6))\nsns.heatmap(train_df.loc[:, corr_cols].corr(), annot=True, cmap=\"gray\")","e16f115d":"## Lets look at ones with highest Correlation with SalePrice\n# Overall Quality has big impact on SalePrice\ntrain_df.plot(kind=\"scatter\", x=\"OverallQual\", y=\"SalePrice\", alpha=0.25)","aace699f":"## Lets look at ones with highest Correlation with SalePrice\n# GrLivAera is also strongly correlated.  \n# There are only four datapoints over 4K sqFeet. Should we include them?\ntrain_df.plot(kind=\"scatter\", x=\"GrLivArea\", y=\"SalePrice\", alpha=0.25)","1af982a6":"train_df[train_df.GrLivArea > 4000]","ba9be299":"## Lets look at ones with highest Correlation with SalePrice\n# GrLivAera is also strongly correlated.  \n# There are only four datapoints over 4K sqFeet. Should we include them?\ntrain_df.plot(kind=\"scatter\", x=\"GarageArea\", y=\"SalePrice\", alpha=0.25)","b5e1cf80":"train_df[train_df.GarageArea > 1200]","e5dc4908":"# Exclude the two rows that are over 700k\ntrain_df = train_df[train_df[\"SalePrice\"] < 700000]","c78af9f0":"# Automatic way to select all Categorical and Numeric Columns\ndef get_features(train_df):\n    num_features, cat_features = [], []\n    for col in train_df.columns:\n        if col in [\"Id\", \"SalePrice\"]:\n            continue\n        dtype = train_df[col].dtype\n        ratio = pd.notna(train_df[col]).sum() \/ len(train_df[col])\n        if ratio < 0.5:\n            continue\n        if dtype == \"object\":\n            cat_features.append(col)\n        else:\n            num_features.append(col)\n    return num_features, cat_features","d3b7ad23":"num_features, cat_features = get_features(train_df)","6a5969e4":"cat_features","83d9d27c":"def get_preprocess_pipeline(train_df, sample_features=False):\n    # Get Numeric and Categorical Features\n    numeric_features, categorical_features = get_features(train_df)\n    target = \"SalePrice\"\n    if sample_features:\n        numeric_features = [\"LotArea\"]\n        categorical_features = [\"SaleType\", \"SaleCondition\"]\n    numeric_transformer = Pipeline(steps=[\n        ('imputer', KNNImputer(n_neighbors=5)),\n        ('scaler', StandardScaler())])\n\n    categorical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='constant', fill_value='N\/A')),\n        ('onehpt', OneHotEncoder(handle_unknown='ignore'))])\n\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numeric_transformer, numeric_features),\n            ('cat', categorical_transformer, categorical_features)])\n    return preprocessor, numeric_features + categorical_features, target","0186e7f0":"def train_LR(train_df, test_df, sample_features=False):\n    \"\"\"\n    Train a Linear Regression Model \n    \"\"\"\n    # Start with simple linear Model\n    preprocessor, features, target = get_preprocess_pipeline(train_df, sample_features=sample_features)\n    clf = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('classifier', LinearRegression())])\n    X_train = train_df[features]\n    y_train = np.log(train_df[target])\n    X_test = test_df[features]\n    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n    \n    clf.fit(X_train, y_train)\n    print(\"RMSE Log Error\", rmse(clf.predict(X_valid), y_valid))\n    # On Prediction, do exp to inverse the loge done during training\n    sub_df = pd.DataFrame({\n        \"Id\": test_df[\"Id\"],\n        \"SalePrice\": np.exp(clf.predict(X_test))\n    })\n    return sub_df","17e5d77b":"# Choose only a small sample of features.- 2-3 features to test the pipeline\nsub_df = train_LR(train_df, test_df, sample_features=True)\nsub_df.to_csv(\"submission_lr_sample.csv\", index=False)\n# Make a submission to Kaggle after downloading the submission file from right side (data -> output)","66de3ba5":"# Lets improve the model, by giving it more features\nsub_df = train_LR(train_df, test_df, sample_features=False)\nsub_df.to_csv(\"submission_lr.csv\", index=False)\n# Make a submission to Kaggle after downloading the submission file from right side (data -> output)","c6d8b5d1":"kfold = KFold(n_splits=10, shuffle=True, random_state=42)\nfor p1, p2 in kfold.split(range(20)):\n    print(p1, p2)","70bda2c2":"kfold = KFold(n_splits=7, shuffle=True, random_state=42)\nfor idxs in kfold.split(train_df):\n    print(idxs[0].shape, idxs[1].shape)","36e5b862":"idxs","b600c67a":"def hyperparam_finder_nocv(df, model_fn, trial):\n    \"\"\"\n    Hyperparameter Finder\n    \"\"\"\n    # Start with simple linear Model\n    model = model_fn(trial)\n    preprocessor, features, target = get_preprocess_pipeline(train_df, sample_features=False)\n    clf = Pipeline(steps=[('preprocessor', preprocessor),\n                          ('classifier', model)])\n    X_train = train_df[features]\n    y_train = np.log(train_df[target])\n    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n    \n    clf.fit(X_train, y_train)\n    return rmse(clf.predict(X_valid), y_valid)","6418f237":"def hyperparam_finder(df, model_fn, trial):\n    \"\"\"\n    Hyperparameter Finder\n    \"\"\"\n    # Start with simple linear Model\n    model = model_fn(trial)\n    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n    valid_errors = []\n    test_preds = []\n    valid_preds = []\n    for train_idxs, valid_idxs in kfold.split(df):\n        train_df = df.iloc[train_idxs]\n        valid_df = df.iloc[valid_idxs]\n\n        preprocessor, features, target = get_preprocess_pipeline(train_df, sample_features=False)\n        clf = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('classifier', model)])\n        X_train = train_df[features]\n        y_train = np.log(train_df[target])\n        X_valid = valid_df[features]\n        y_valid = np.log(valid_df[target])\n        X_test = test_df[features]\n        clf.fit(X_train, y_train)\n        y_valid_preds = clf.predict(X_valid)\n        valid_errors.append(rmse(y_valid_preds, y_valid))\n    # Return Valid Pred Score for HyperParam Tuning\n    return np.mean(valid_errors)","2cab374c":"# You would have seen an improvement on your Leaderboard by providing it better features\n# Now lets do KFold (5 Fold)\ndef train_kfold(df, test_df, ModelClass, **model_kwargs):\n    \"\"\"\n    Train a Regression Model with 5 Fold CV\n    \"\"\"\n    # Start with simple linear Model\n    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n    valid_errors = []\n    test_preds = []\n    valid_preds = []\n    for train_idxs, valid_idxs in kfold.split(df):\n        train_df = df.iloc[train_idxs]\n        valid_df = df.iloc[valid_idxs]\n\n        preprocessor, features, target = get_preprocess_pipeline(train_df, sample_features=False)\n        clf = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('classifier', ModelClass(**model_kwargs))])\n        X_train = train_df[features]\n        y_train = np.log(train_df[target])\n        X_valid = valid_df[features]\n        y_valid = np.log(valid_df[target])\n        X_test = test_df[features]\n        clf.fit(X_train, y_train)\n        y_valid_preds = clf.predict(X_valid)\n        valid_errors.append(rmse(y_valid_preds, y_valid))\n        test_preds.append(np.exp(clf.predict(X_test)))\n        valid_preds.append(pd.DataFrame({\n            \"Id\": valid_df[\"Id\"],\n            \"SalePrice\": np.exp(y_valid_preds)\n        }))\n\n    print(\"RMSE Log Error\", np.mean(valid_errors))\n    # On Prediction, do exp to inverse the loge done during training\n    sub_df = pd.DataFrame({\n        \"Id\": test_df[\"Id\"],\n        \"SalePrice\": np.mean(test_preds, axis=0)\n    })\n    # Return test prediction with CV and the Validation Prediction (For Stacking later)\n    return sub_df, pd.concat(valid_preds)","f424a35e":"# Lets improve the model by choosing XGBoost over Linear Classifier\nmodel1_sub_df, model1_valid_preds = train_kfold(train_df, test_df, LinearRegression)\nsub_df.to_csv(\"submission_lr_kfold.csv\", index=False)\n# Make a submission to Kaggle after downloading the submission file from right side (data -> output)\n# Score might have improved over the LR without kfold.","e309052f":"def train_XGB_kfold(df, test_df):\n    \"\"\"\n    Train a XGBoost Model with 5 Fold CV\n    \"\"\"\n    # Start with simple linear Model\n    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n    valid_errors = []\n    test_preds = []\n    valid_preds = []\n    for train_idxs, valid_idxs in kfold.split(df.index.values):\n        train_df = df.loc[train_idxs]\n        valid_df = df.loc[valid_idxs]\n\n        preprocessor, features, target = get_preprocess_pipeline(train_df, sample_features=False)\n        clf = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('classifier', XGBRegressor(n_jobs=-1, n_estimators=500, max_depth=20))])\n        X_train = train_df[features]\n        y_train = np.log(train_df[target])\n        X_valid = valid_df[features]\n        y_valid = np.log(valid_df[target])\n        X_test = test_df[features]\n        clf.fit(X_train, y_train)\n        y_valid_preds = clf.predict(X_valid)\n        valid_errors.append(rmse(y_valid_preds, y_valid))\n        test_preds.append(np.exp(clf.predict(X_test)))\n        valid_preds.append(pd.DataFrame({\n            \"Id\": valid_df[\"Id\"],\n            \"SalePrice\": np.exp(y_valid_preds)\n        }))\n\n    print(\"Mean Squared Log Error\", np.mean(valid_errors))\n    # On Prediction, do exp to inverse the loge done during training\n    sub_df = pd.DataFrame({\n        \"Id\": test_df[\"Id\"],\n        \"SalePrice\": np.mean(test_preds, axis=0)\n    })\n    # Return test prediction with CV and the Validation Prediction (For Stacking later)\n    return sub_df, pd.concat(valid_preds)\n\ndef train_RF_kfold(df, test_df):\n    \"\"\"\n    Train a RF Model with 5 Fold CV\n    \"\"\"\n    # Start with simple linear Model\n    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n    valid_errors = []\n    test_preds = []\n    valid_preds = []\n    for train_idxs, valid_idxs in kfold.split(df.index.values):\n        train_df = df.loc[train_idxs]\n        valid_df = df.loc[valid_idxs]\n\n        preprocessor, features, target = get_preprocess_pipeline(train_df, sample_features=False)\n        clf = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('classifier', RandomForestRegressor(n_jobs=-1, max_depth=20))])\n        X_train = train_df[features]\n        y_train = np.log(train_df[target])\n        X_valid = valid_df[features]\n        y_valid = np.log(valid_df[target])\n        X_test = test_df[features]\n        clf.fit(X_train, y_train)\n        y_valid_preds = clf.predict(X_valid)\n        valid_errors.append(mean_squared_error(y_valid_preds, y_valid))\n        test_preds.append(np.exp(clf.predict(X_test)))\n        valid_preds.append(pd.DataFrame({\n            \"Id\": valid_df[\"Id\"],\n            \"SalePrice\": np.exp(y_valid_preds)\n        }))\n\n    print(\"Mean Squared Log Error\", np.mean(valid_errors))\n    # On Prediction, do exp to inverse the loge done during training\n    sub_df = pd.DataFrame({\n        \"Id\": test_df[\"Id\"],\n        \"SalePrice\": np.mean(test_preds, axis=0)\n    })\n    # Return test prediction with CV and the Validation Prediction (For Stacking later)\n    return sub_df, pd.concat(valid_preds)","182d2d92":"Ridge()","fc947ca4":"def lasso_hparams_finder(trial):\n    alpha = trial.suggest_float(\"alpha\", 0, 1.0)\n    max_iter = trial.suggest_int(\"max_iter\", 500, 5000)\n    return Lasso(alpha=alpha, max_iter=max_iter)\n\ndef ridge_hparams_finder(trial):\n    alpha = trial.suggest_float(\"alpha\", 0, 1.0)\n    max_iter = trial.suggest_int(\"max_iter\", 500, 5000)\n    return Ridge(alpha=alpha, max_iter=max_iter)\n\ndef xgb_hparams_finder(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 5, 30)\n    n_estimators = trial.suggest_int(\"n_estimators\", 100, 300)\n    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 1)\n    tree_method = trial.suggest_categorical(\"tree_method\", [\"gpu_hist\"])\n    gamma = trial.suggest_float(\"gamma\", 0, 1)\n    eta = trial.suggest_float(\"eta\", 0, 1)\n    return XGBRegressor(\n        max_depth=max_depth, \n        n_estimators=n_estimators, \n        learning_rate=learning_rate, \n        tree_method=tree_method, \n        gamma=gamma, \n        eta=eta\n    )\n\ndef rf_hparams_finder(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 10, 50)\n    n_estimators = trial.suggest_int(\"n_estimators\", 100, 300)\n    return RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth)\n\ndef lightgbm_hparams_finder(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 5, 30)\n    n_estimators = trial.suggest_int(\"n_estimators\", 100, 300)\n    learning_rate = trial.suggest_float(\"learning_rate\", 0.001, 1)\n    reg_alpha = trial.suggest_float(\"reg_alpha\", 0., 1)\n    reg_lambda = trial.suggest_float(\"reg_lambda\", 0., 1)\n    return LGBMRegressor(max_depth=max_depth, \n        n_estimators=n_estimators, \n        learning_rate=learning_rate, reg_alpha=reg_alpha, reg_lambda=reg_lambda)\n\ndef svr_hparams_finder(trial):\n    kernel = trial.suggest_categorical(\"kernel\", \n                                          ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'])\n    degree = trial.suggest_int(\"degree\", 1, 4)\n    c = trial.suggest_float(\"c\", 0, 1)\n    max_iter = trial.suggest_float(\"max_iter\", 50, 500)\n    return SVR(kernel=kernel, degree=degree, C=c, max_iterm=max_iter)","272628de":"# Lets improve the model by choosing XGBoost over Linear Classifier\noptuna.create_study()\nstudy = optuna.create_study()\nstudy.optimize(partial(hyperparam_finder, train_df, lasso_hparams_finder), \n               n_trials=100, \n               show_progress_bar=True\n              )\nlasso_params = study.best_params  # E.g. {'x': 2.002108042}","68c275ef":"# Lets improve the model by choosing XGBoost over Linear Classifier\nmodel_options = {\n        \"ridge\": ridge_hparams_finder,\n        \"lasso\": lasso_hparams_finder,\n        \"xgb\": xgb_hparams_finder,\n        \"rf\": rf_hparams_finder,\n        \"ligtgbm\": lightgbm_hparams_finder,\n        \"svr\": svr_hparams_finder\n    }\nbest_hparams = []\nfor model_name, model_hparams_fn in model_options.items():\n    print(model_name)\n    optuna.create_study()\n    study = optuna.create_study()\n    study.optimize(partial(hyperparam_finder_nocv, train_df, model_hparams_fn), \n                   n_trials=20, \n                   show_progress_bar=True\n                  )\n\n    best_hparams.append({\n        model_name: study.best_params\n    })","6a06ddff":"# Lets improve the model by choosing XGBoost over Linear Classifier\noptuna.create_study()\nstudy = optuna.create_study()\nstudy.optimize(partial(hyperparam_finder, train_df, lasso_hparams_finder), \n               n_trials=100, \n               show_progress_bar=True\n              )\nlasso_params = study.best_params  # E.g. {'x': 2.002108042}","dc5b2579":"# Lets improve the model by choosing XGBoost over Linear Classifier\nmodel1_test_preds, model1_valid_preds = train_kfold(train_df, test_df, Ridge)\n\n# Lets improve the model by choosing XGBoost over Linear Classifier\nmodel2_test_preds, model2_valid_preds = train_kfold(train_df, test_df,  XGBRegressor, n_jobs=4, n_estimators=500, max_depth=20)\n\n# Lets improve the model by choosing RF over Linear Classifier\nmodel3_test_preds, model3_valid_preds = train_kfold(train_df, test_df, RandomForestRegressor,n_jobs=4, n_estimators=500, max_depth=20)\n\n# Lets improve the model by choosing SVR\nmodel4_test_preds, model4_valid_preds = train_kfold(train_df, test_df,  SVR)\n\n# Lets improve the model by choosing  LightGBM\nmodel5_test_preds, model5_valid_preds = train_kfold(train_df, test_df,  LGBMRegressor, n_jobs=4, n_estimators=500, max_depth=20)","fad7063f":"model1_valid_preds.rename(columns={\"SalePrice\": \"model1_preds\"}, inplace=True)\nmodel2_valid_preds.rename(columns={\"SalePrice\": \"model2_preds\"}, inplace=True)\nmodel3_valid_preds.rename(columns={\"SalePrice\": \"model3_preds\"}, inplace=True)\nmodel4_valid_preds.rename(columns={\"SalePrice\": \"model4_preds\"}, inplace=True)\nmodel5_valid_preds.rename(columns={\"SalePrice\": \"model5_preds\"}, inplace=True)","f3a26dff":"model1_test_preds.rename(columns={\"SalePrice\": \"model1_preds\"}, inplace=True)\nmodel2_test_preds.rename(columns={\"SalePrice\": \"model2_preds\"}, inplace=True)\nmodel3_test_preds.rename(columns={\"SalePrice\": \"model3_preds\"}, inplace=True)\nmodel4_test_preds.rename(columns={\"SalePrice\": \"model4_preds\"}, inplace=True)\nmodel5_test_preds.rename(columns={\"SalePrice\": \"model5_preds\"}, inplace=True)","732e7e65":"pd.merge(model1_test_preds, model2_test_preds, left_on=\"Id\", right_on=\"Id\")","0b763f53":"# Model Blending - Take the Average of the three Model Predictions\nsub_df = pd.merge(model1_test_preds, model2_test_preds, left_on=\"Id\", right_on=\"Id\")\nsub_df = pd.merge(sub_df, model3_test_preds, left_on=\"Id\", right_on=\"Id\")\nsub_df = pd.merge(sub_df, model4_test_preds, left_on=\"Id\", right_on=\"Id\")\nsub_df = pd.merge(sub_df, model5_test_preds, left_on=\"Id\", right_on=\"Id\")\nsub_df[\"SalePrice\"] = (sub_df[\"model1_preds\"] + sub_df[\"model2_preds\"] + sub_df[\"model3_preds\"] + sub_df[\"model4_preds\"] + sub_df[\"model5_preds\"])\/5\nsub_df[[\"Id\", \"SalePrice\"]].to_csv(\"submission_model_blend.csv\", index=False)","ae1afccb":"# You would have seen an improvement on your Leaderboard with KFold.\n# Now Lets do Model Stacking.\n# Choose three Models - LinearRegression, RandomForest and XGBoost and get Predictions\n# Average ALL three Predictions (With KFold) and make a Submission.  This will give you Model Blending\n# TODO","041ac7b9":"layer1_test_df = pd.merge(model1_test_preds, model2_test_preds, left_on=\"Id\", right_on=\"Id\")\nlayer1_test_df = pd.merge(layer1_test_df, model3_test_preds, left_on=\"Id\", right_on=\"Id\")\nlayer1_test_df = pd.merge(layer1_test_df, model4_test_preds, left_on=\"Id\", right_on=\"Id\")\nlayer1_test_df = pd.merge(layer1_test_df, model5_test_preds, left_on=\"Id\", right_on=\"Id\")\n\nlayer1_test_df.head()","8b279b2c":"layer1_train_df = pd.merge(model1_valid_preds, model2_valid_preds, left_on=\"Id\", right_on=\"Id\")\nlayer1_train_df = pd.merge(layer1_train_df, model3_valid_preds, left_on=\"Id\", right_on=\"Id\")\nlayer1_train_df = pd.merge(layer1_train_df, model4_valid_preds, left_on=\"Id\", right_on=\"Id\")\nlayer1_train_df = pd.merge(layer1_train_df, model5_valid_preds, left_on=\"Id\", right_on=\"Id\")\n\nlayer1_train_df = pd.merge(layer1_train_df, train_df[[\"Id\", \"SalePrice\"]], left_on=\"Id\", right_on=\"Id\")\nlayer1_train_df.head()","178ed0a0":"# Finally try Model Stacking (Hint: Using the Validation Predictions you obtained from the CV Models to train a new Model for Prediction)\n# You would have seen an improvement on your Leaderboard by providing it better features\n# Now lets do KFold (5 Fold)\ndef train_model_stacking_kfold(df, test_df):\n    \"\"\"\n    Train a Linear Regression Model with 5 Fold CV\n    \"\"\"\n    # Start with simple linear Model\n    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n    valid_errors = []\n    test_preds = []\n    features = [\"model1_preds\", \"model2_preds\", \"model3_preds\", \"model4_preds\", \"model5_preds\"]\n    df = df.copy()\n    test_df = test_df.copy()\n    for feat in features:\n        df[feat] = np.log(df[feat])\n        test_df[feat] = np.log(test_df[feat])\n    df[\"SalePrice\"] = np.log(df[\"SalePrice\"])\n    for train_idxs, valid_idxs in kfold.split(df):\n        train_df = df.loc[train_idxs]\n        valid_df = df.loc[valid_idxs]\n        X_train = train_df[features]\n        y_train = train_df[\"SalePrice\"]\n        X_valid = valid_df[features]\n        y_valid = valid_df[\"SalePrice\"]\n        X_test = test_df[features]\n        clf = LinearRegression()\n        clf.fit(X_train, y_train)\n        y_valid_preds = clf.predict(X_valid)\n        valid_errors.append(rmse(y_valid_preds, y_valid))\n        test_preds.append(np.exp(clf.predict(X_test)))\n\n    print(\"RMSE Log Error\", np.mean(valid_errors))\n    # On Prediction, do exp to inverse the loge done during training\n    sub_df = pd.DataFrame({\n        \"Id\": test_df[\"Id\"],\n        \"SalePrice\": np.mean(test_preds, axis=0)\n    })\n    # Return test prediction with CV\n    return sub_df","1255be47":"sub_df = train_model_stacking_kfold(layer1_train_df, layer1_test_df)\nsub_df[[\"Id\", \"SalePrice\"]].to_csv(\"submission_model_stack.csv\", index=False)","da618843":"import lightgbm","2823b05b":"## EDA"}}