{"cell_type":{"fd9f9fc4":"code","4f246faa":"code","afa09650":"code","942a887f":"code","ffd82e55":"code","a1ca91f4":"code","d4191be2":"code","eb14de68":"code","7f332fa5":"code","f1461938":"markdown","148057f4":"markdown","159773cf":"markdown","031fab0a":"markdown","c16e0403":"markdown"},"source":{"fd9f9fc4":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os, gc\n# import cudf\nimport pandas as pd\nimport numpy as np\n# import cupy as cp\nimport janestreet\nimport xgboost as xgb\nfrom hyperopt import hp, fmin, tpe, Trials\nfrom hyperopt.pyll.base import scope\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom joblib import dump, load\n\nimport tensorflow as tf\ntf.random.set_seed(42)\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as layers\nfrom tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping","4f246faa":"# print('Loading...')\n# train = cudf.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv')\ntrain = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv', nrows = 3)\nfeatures = [c for c in train.columns if 'feature' in c]\n\n# print('Filling...')\n# f_mean = train[features[1:]].mean()\n# train = train.query('weight > 0').reset_index(drop = True)\n# train[features[1:]] = train[features[1:]].fillna(f_mean)\n# train['action'] = (train['resp'] > 0).astype('int')\n\n# print('Converting...')\n# train = train.to_pandas()\n# f_mean = f_mean.values.get()\n# np.save('f_mean.npy', f_mean)\n\n# print('Finish.')","afa09650":"def create_mlp(num_columns, num_labels, hidden_units, dropout_rates, label_smoothing, learning_rate):\n    \n    inp = tf.keras.layers.Input(shape = (num_columns, ))\n    x = tf.keras.layers.BatchNormalization()(inp)\n    x = tf.keras.layers.Dropout(dropout_rates[0])(x)\n    for i in range(len(hidden_units)): \n        x = tf.keras.layers.Dense(hidden_units[i])(x)\n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Activation(tf.keras.activations.swish)(x)\n        x = tf.keras.layers.Dropout(dropout_rates[i+1])(x)    \n        \n    x = tf.keras.layers.Dense(num_labels)(x)\n    out = tf.keras.layers.Activation('sigmoid')(x)\n    \n    model = tf.keras.models.Model(inputs = inp, outputs = out)\n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n                  loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = label_smoothing), \n                  metrics = tf.keras.metrics.AUC(name = 'AUC'), \n                 )\n    \n    return model","942a887f":"batch_size = 4096\nhidden_units = [384, 896, 896, 394]\ndropout_rates = [0.10143786981358652, 0.19720339053599725, 0.2703017847244654, 0.23148340929571917, 0.2357768967777311]\nlabel_smoothing = 1e-2\nlearning_rate = 1e-3\n\n# oof = np.zeros(len(train['action']))\n# gkf = GroupKFold(n_splits = 5)\n# for fold, (tr, te) in enumerate(gkf.split(train['action'].values, train['action'].values, train['date'].values)):\n    \n#     X_tr, X_val = train.loc[tr, features].values, train.loc[te, features].values\n#     y_tr, y_val = train.loc[tr, 'action'].values, train.loc[te, 'action'].values\n    \n#     ckp_path = f'JSModel_{fold}.hdf5'\n#     model = create_mlp(X_tr.shape[1], 1, hidden_units, dropout_rates, label_smoothing, learning_rate)\n#     rlr = ReduceLROnPlateau(monitor = 'val_AUC', factor = 0.1, patience = 3, verbose = 0, \n#                             min_delta = 1e-4, mode = 'max')\n#     ckp = ModelCheckpoint(ckp_path, monitor = 'val_AUC', verbose = 0, \n#                           save_best_only = True, save_weights_only = True, mode = 'max')\n#     es = EarlyStopping(monitor = 'val_AUC', min_delta = 1e-4, patience = 7, mode = 'max', \n#                        baseline = None, restore_best_weights = True, verbose = 0)\n#     model.fit(X_tr, y_tr, validation_data = (X_val, y_val), epochs = 1000, \n#               batch_size = batch_size, callbacks = [rlr, ckp, es], verbose = 0)\n                \n#     oof[te] += model.predict(X_val, batch_size = batch_size * 4).ravel()\n#     score = roc_auc_score(y_val, oof[te])\n#     print(f'Fold {fold} ROC AUC:\\t', score)\n    \n#     # Finetune 3 epochs on validation set with small learning rate\n#     model = create_mlp(X_tr.shape[1], 1, hidden_units, dropout_rates, label_smoothing, learning_rate \/ 100)\n#     model.load_weights(ckp_path)\n#     model.fit(X_val, y_val, epochs = 3, batch_size = batch_size, verbose = 0)\n#     model.save_weights(ckp_path)\n    \n#     K.clear_session()\n#     del model\n#     rubbish = gc.collect()","ffd82e55":"# score_oof = roc_auc_score(train['action'].values, oof)\n# print(score_oof)","a1ca91f4":"num_models = 2\n\nmodels = []\nfor i in range(num_models):\n    clf = create_mlp(len(features), 1, hidden_units, dropout_rates, label_smoothing, learning_rate)\n    clf.load_weights(f'..\/input\/js-nn-models\/JSModel_{i}.hdf5')\n#     clf.load_weights(f'.\/JSModel_{i}.hdf5')\n    models.append(clf)","d4191be2":"f_mean = np.load('..\/input\/js-nn-models\/f_mean.npy')\n# f_mean = np.load('.\/f_mean.npy')","eb14de68":"env = janestreet.make_env()\nenv_iter = env.iter_test()","7f332fa5":"opt_th = 0.5\nfor (test_df, pred_df) in tqdm(env_iter):\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        if np.isnan(x_tt[:, 1:].sum()):\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n        pred = 0.\n        for clf in models:\n            pred += clf(x_tt, training = False).numpy().item() \/ num_models\n#         pred = models[0](x_tt, training = False).numpy().item()\n        pred_df.action = np.where(pred >= opt_th, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","f1461938":"# Training","148057f4":"# Jane Street: Neural Network Starter\n\nI try implementing a simple Tensorflow Keras neural network here. Train in Version 17.\n\n**Caution:** The GroupCV method applied in this notebook may cause time leakage problem. Please use [Purged Time-Series CV][1] instead.\n\n[1]: https:\/\/www.kaggle.com\/marketneutral\/purged-time-series-cv-xgboost-optuna","159773cf":"# Load Models","031fab0a":"# Submitting\n\nJust use two models to reduce running time.","c16e0403":"# Preprocessing"}}