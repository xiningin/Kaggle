{"cell_type":{"de61a61c":"code","f65f9eef":"code","6764bc4f":"code","658a9dee":"code","e31f5c23":"code","75a48880":"code","2675a27d":"code","8c48a09b":"code","06bf8075":"code","7f55b90f":"code","7491f91b":"code","bfdfbba3":"code","14c57aa9":"code","79a1f14f":"code","021c4521":"markdown","efd3bc0f":"markdown","14647102":"markdown","50fbaa7e":"markdown","55309ac1":"markdown","288b6666":"markdown","9c813016":"markdown","d830d7f9":"markdown","27243d1f":"markdown","10aad958":"markdown","9dd4594a":"markdown","eca37645":"markdown"},"source":{"de61a61c":"import pandas as pd\ndf= pd.read_csv(\"..\/input\/rintro-chapter8 (1).csv\")\ndf.head()","f65f9eef":"from sklearn import preprocessing\ndf_s = df.drop(columns = ['brand'])\ndf_s = pd.DataFrame(preprocessing.normalize(df_s), columns=df_s.columns)\ndf_s = df_s.join(df['brand'])\ndf_s.head()","6764bc4f":"import seaborn as sns\ncorr = df_s.corr()\n#sns.light_palette(\"purple\")\nsns.heatmap(corr, cmap=sns.color_palette(\"RdBu_r\", 7), center = 0, vmin = -1, vmax = 1)","658a9dee":"import numpy as np\nimport scipy\nimport scipy.cluster.hierarchy as sch\n\nX = df_s.corr().values\nd = sch.distance.pdist(X)   # vector of ('55' choose 2) pairwise distances\nL = sch.linkage(d, method='complete')\nind = sch.fcluster(L, 0.5*d.max(), 'distance')\ncolumns = [df_s.columns.tolist()[i] for i in list((np.argsort(ind)))]\ndf_s = df_s.reindex(columns, axis=1)\ndf_s.head()","e31f5c23":"corr = df_s.corr()\n#sns.light_palette(\"purple\")\nsns.heatmap(corr, cmap=sns.color_palette(\"RdBu_r\", 7), center = 0, vmin = -1, vmax = 1)","75a48880":"df_m = df.groupby(['brand']).mean()\ndf_m.head()","2675a27d":"ax = sns.heatmap(df_m,  cmap=\"YlOrRd\",linewidths=0.4)","8c48a09b":"ax = sns.clustermap(df_m,  cmap=\"YlOrRd\",linewidths=0.4)","06bf8075":"from sklearn.decomposition import PCA\npca = PCA(n_components=3)\ndf_s = df.drop(columns = ['brand'])\ndf_s = pd.DataFrame(preprocessing.normalize(df_s), columns=df_s.columns)\ndf_s = df_s.join(df['brand'])\nX = df_s.drop(columns = ['brand'])\nprincipalComponents = pca.fit_transform(X)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['PC1', 'PC2', 'PC3'])\nprincipalDf.head()","7f55b90f":"df_pca = principalDf.join(df_s['brand'])\ndf_pca.head()","7491f91b":"pca = PCA(n_components=9)\nprincipalComponents = pca.fit_transform(X)\npca.explained_variance_ratio_","bfdfbba3":"import matplotlib.pyplot as plt\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))","14c57aa9":"from yellowbrick.datasets import load_credit\nfrom yellowbrick.features.pca import PCADecomposition\nfrom sklearn import preprocessing \nlabel_encoder = preprocessing.LabelEncoder() \n\n\ndf_s = df.drop(columns = ['brand'])\ndf_s = pd.DataFrame(preprocessing.normalize(df_s), columns=df_s.columns)\ndf_s = df_s.join(df['brand'])\ndf_s['brand']= label_encoder.fit_transform(df_s['brand']) #Label encoding for the YellowBricks functions to work\n\nX = df_s.drop(columns = ['brand'])\ny = df_s['brand']\n\n\n\n\nvisualizer = PCADecomposition(scale=True, proj_features = True, color = None)\nvisualizer.fit_transform(X, y)\nvisualizer.show()","79a1f14f":"df_agg = df.groupby(['brand'],as_index = False).mean()\n\n\n\ndf_s = df_agg.drop(columns = ['brand'])\ndf_s = pd.DataFrame(preprocessing.normalize(df_s), columns=df_s.columns)\ndf_s = df_s.join(df_agg['brand'])\ndf_s['brand']= label_encoder.fit_transform(df_s['brand']) #Label encoding for the YellowBricks functions to work\n\nX = df_s.drop(columns = ['brand'])\ny = df_s['brand']\n\n\nvisualizer = PCADecomposition(scale=True, proj_features = True, color = None)\nvisualizer.fit_transform(X, y)\nvisualizer.show()\n\n","021c4521":"### Cumulative Variance Plot - Suggests 3 to 4 components is optimum","efd3bc0f":"## 4. PCA\n\n- What does all these strong correlations and similarities mean? \n- We can reduce the complexity of this dataset using PCA","14647102":"### Ordering it on the basis of clusters","50fbaa7e":"This visualization of the basic data appears to show three general clusters that comprise fun\/latest\/trendy, rebuy\/bargain\/value, and perform\/leader\/serious, respectively.","55309ac1":"## 3. Heatmaps","288b6666":"### Now a bi-plot using the aggregrates ","9c813016":"- A heatmap for the mean of each adjective by brand.\n- Brands f and g are similar\u2014with high ratings for rebuy and value but low ratings for latest and fun. \n- Other groups of similar brands are b\/c, i\/h\/d, and a\/j.","d830d7f9":"## 1. Usually we do need to rescale\n\n- But the predictors in this dataset are all of the same unit - so it is comparable across individuals and samples.  \n- Nevertheless, lets rescale it (centre to 0 by subtracting mean, and rescale using standard deviation)","27243d1f":"### Cluster maps are better as they order by similarity ","10aad958":"## 2. Observe correlations\n","9dd4594a":"## PCA Projections - BiPlot","eca37645":"### Consumer brand perception surveys data  \nSurvey question: On a scale of 1(least) to 10 (most) how [Perceptual Adjective] is [Brand Name]\n\ne.g. How trendy is Nike?\n\nHere we are looking at a simulated dataset of 10 brands on 9 perceptual adjectives"}}