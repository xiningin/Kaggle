{"cell_type":{"5226f83d":"code","f9fe0ae1":"code","4abc9f5a":"code","dc47e131":"code","3ac82021":"code","08406390":"code","c8d6ba93":"code","2fff8a0a":"code","e48dad14":"code","b1e37ad3":"code","ea5cdd32":"code","cd78ab4b":"code","62a3e53d":"code","5586296b":"code","53e794e0":"code","978022d2":"code","466407f0":"markdown","5edb042c":"markdown","1fe3cb6e":"markdown","42e83a23":"markdown","954ad5d7":"markdown","f0d1b929":"markdown","e2d16e01":"markdown","caff524a":"markdown","4a045bc6":"markdown","27448e59":"markdown","4f56b1bd":"markdown"},"source":{"5226f83d":"#  Libraries\nimport numpy as np \nimport pandas as pd \n# Data processing, metrics and modeling\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom datetime import datetime\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\nfrom sklearn import metrics\n# Lgbm\nimport lightgbm as lgb\nimport catboost\nfrom catboost import Pool\nimport xgboost as xgb\n\n# Suppr warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport itertools\nfrom scipy import interp\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib import rcParams\n\n\n#Timer\ndef timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('Time taken for Modeling: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))","f9fe0ae1":"data = pd.read_csv('..\/input\/creditcard.csv')","4abc9f5a":"display(data.head(),data.describe(),data.shape)","dc47e131":"f,ax=plt.subplots(15,2,figsize=(12,60))\n#f.delaxes(ax)\ncol = list(data)\ncol = [e for e in col if e not in ('Class')]\n\nfor i,feature in enumerate(col):\n    sns.distplot(data[data['Class']==1].dropna()[(feature)], ax=ax[i\/\/2,i%2], kde_kws={\"color\":\"black\"}, hist=False )\n    sns.distplot(data[data['Class']==0].dropna()[(feature)], ax=ax[i\/\/2,i%2], kde_kws={\"color\":\"black\"}, hist=False )\n\n    # Get the two lines from the ax[i\/\/2,i%2]es to generate shading\n    l1 = ax[i\/\/2,i%2].lines[0]\n    l2 = ax[i\/\/2,i%2].lines[1]\n\n    # Get the xy data from the lines so that we can shade\n    x1 = l1.get_xydata()[:,0]\n    y1 = l1.get_xydata()[:,1]\n    x2 = l2.get_xydata()[:,0]\n    y2 = l2.get_xydata()[:,1]\n    ax[i\/\/2,i%2].fill_between(x2,y2, color=\"deeppink\", alpha=0.6)\n    ax[i\/\/2,i%2].fill_between(x1,y1, color=\"darkturquoise\", alpha=0.6)\n\n    #grid\n    ax[i\/\/2,i%2].grid(b=True, which='major', color='grey', linewidth=0.3)\n    \n    ax[i\/\/2,i%2].set_title('{} by target'.format(feature), fontsize=18)\n    ax[i\/\/2,i%2].set_ylabel('count', fontsize=12)\n    ax[i\/\/2,i%2].set_xlabel('Modality', fontsize=12)\n\n    #sns.despine(ax[i\/\/2,i%2]=ax[i\/\/2,i%2], left=True)\n    ax[i\/\/2,i%2].set_ylabel(\"frequency\", fontsize=12)\n    ax[i\/\/2,i%2].set_xlabel(str(feature), fontsize=12)\n\nplt.tight_layout()\nplt.show()","3ac82021":"#reshape amount\ndata['nAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))\n#drop useless \ndata = data.drop(columns = ['Time','Amount'])","08406390":"for i in range(1, 29):\n    col = 'V%d'%i\n    data['rounded_4_'+str(col)] = round(data[col],4) ","c8d6ba93":"for i in range(1, 29):\n    col = 'rounded_4_V%d'%i\n    var = data.groupby(col).agg({col:'count'})\n    var.columns = ['%s_count'%col]\n    data = pd.merge(data,var,on=col,how='left')","2fff8a0a":"train_df = data\nfeatures = list(train_df)\nfeatures.remove('Class')\ntarget = 'Class'","e48dad14":"# Confusion matrix \ndef plot_confusion_matrix(cm, classes,\n                          normalize = False,\n                          title = 'Confusion matrix\"',\n                          cmap = plt.cm.Blues) :\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 0)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment = 'center',\n                 color = 'white' if cm[i, j] > thresh else 'black')\n \n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","b1e37ad3":"def gradient_boosting_model(params, folds, model='LGB'):\n    print(str(model)+' modeling...')\n    start_time = timer(None)\n    \n    plt.rcParams[\"axes.grid\"] = True\n\n    nfold = 5\n    skf = StratifiedKFold(n_splits=nfold, shuffle=True, random_state=42)\n\n    oof = np.zeros(len(train_df))\n    mean_fpr = np.linspace(0,1,100)\n    cms= []\n    tprs = []\n    aucs = []\n    y_real = []\n    y_proba = []\n    recalls = []\n    roc_aucs = []\n    f1_scores = []\n    accuracies = []\n    precisions = []\n    feature_importance_df = pd.DataFrame()\n\n    i = 1\n    for train_idx, valid_idx in skf.split(train_df, train_df.Class.values):\n        print(\"\\nfold {}\".format(i))\n        \n        if model == 'LGB':\n        \n            trn_data = lgb.Dataset(train_df.iloc[train_idx][features].values,\n                                   label=train_df.iloc[train_idx][target].values\n                                   )\n            val_data = lgb.Dataset(train_df.iloc[valid_idx][features].values,\n                                   label=train_df.iloc[valid_idx][target].values\n                                   )   \n\n            clf = lgb.train(param_lgb, trn_data, num_boost_round=10000,  valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 100)\n            oof[valid_idx] = clf.predict(train_df.iloc[valid_idx][features].values) \n            \n        if model == 'CB':\n            \n            trn_data = Pool(train_df.iloc[train_idx][features].values,\n                           label=train_df.iloc[train_idx][target].values\n                           )\n            val_data = Pool(train_df.iloc[valid_idx][features].values,\n                            label=train_df.iloc[valid_idx][target].values\n                            )   \n\n            clf = catboost.train(trn_data, param_cb, eval_set=val_data, verbose = 100)\n\n            oof[valid_idx]  = clf.predict(train_df.iloc[valid_idx][features].values)   \n            oof[valid_idx]  = np.exp(oof[valid_idx]) \/ (1 + np.exp(oof[valid_idx]))\n            \n        if model == 'XGB':\n\n            trn_data = xgb.DMatrix(train_df.iloc[train_idx][features], \n                                   label=train_df.iloc[train_idx][target].values)\n            val_data = xgb.DMatrix(train_df.iloc[valid_idx][features], \n                                   label=train_df.iloc[valid_idx][target].values)\n\n            watchlist = [(trn_data, 'train'), (val_data, 'valid')]\n\n            clf = xgb.train(params, dtrain = trn_data, evals=watchlist, early_stopping_rounds=100, maximize=True, verbose_eval=100)\n            oof[valid_idx] = clf.predict(val_data, ntree_limit=clf.best_ntree_limit)\n\n        # Scores \n        roc_aucs.append(roc_auc_score(train_df.iloc[valid_idx][target].values, oof[valid_idx]))\n        accuracies.append(accuracy_score(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n        recalls.append(recall_score(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n        precisions.append(precision_score(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n        f1_scores.append(f1_score(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n\n        # Roc curve by folds\n        f = plt.figure(1)\n        fpr, tpr, t = roc_curve(train_df.iloc[valid_idx][target].values, oof[valid_idx])\n        tprs.append(interp(mean_fpr, fpr, tpr))\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        plt.plot(fpr, tpr, lw=2, alpha=0.3, label='ROC fold %d (AUC = %0.4f)' % (i,roc_auc))\n\n        # Precion recall by folds\n        g = plt.figure(2)\n        precision, recall, _ = precision_recall_curve(train_df.iloc[valid_idx][target].values, oof[valid_idx])\n        y_real.append(train_df.iloc[valid_idx][target].values)\n        y_proba.append(oof[valid_idx])\n        plt.plot(recall, precision, lw=2, alpha=0.3, label='P|R fold %d' % (i))  \n\n        i= i+1\n        \n        # Confusion matrix by folds\n        cms.append(confusion_matrix(train_df.iloc[valid_idx][target].values, oof[valid_idx].round()))\n        \n        # Features imp\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"Feature\"] = features\n        if model == 'LGB':\n            fold_importance_df[\"importance\"] = clf.feature_importance()\n        if model == 'CB':\n            fold_importance_df[\"importance\"] = clf.get_feature_importance()\n        fold_importance_df[\"fold\"] = nfold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)    \n\n    # Metrics\n    print(\n            '\\nCV roc score        : {0:.4f}, std: {1:.4f}.'.format(np.mean(roc_aucs), np.std(roc_aucs)),\n            '\\nCV accuracy score   : {0:.4f}, std: {1:.4f}.'.format(np.mean(accuracies), np.std(accuracies)),\n            '\\nCV recall score     : {0:.4f}, std: {1:.4f}.'.format(np.mean(recalls), np.std(recalls)),\n            '\\nCV precision score  : {0:.4f}, std: {1:.4f}.'.format(np.mean(precisions), np.std(precisions)),\n            '\\nCV f1 score         : {0:.4f}, std: {1:.4f}.'.format(np.mean(f1_scores), np.std(f1_scores))\n    )\n    \n    # Roc plt\n    f = plt.figure(1)\n    plt.plot([0,1],[0,1],linestyle = '--',lw = 2,color = 'grey')\n    mean_tpr = np.mean(tprs, axis=0)\n    mean_auc = auc(mean_fpr, mean_tpr)\n    plt.plot(mean_fpr, mean_tpr, color='blue',\n             label=r'Mean ROC (AUC = %0.4f)' % (np.mean(roc_aucs)),lw=2, alpha=1)\n\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(str(model)+' ROC curve by folds')\n    plt.legend(loc=\"lower right\")\n    \n    # PR plt\n    g = plt.figure(2)\n    plt.plot([0,1],[1,0],linestyle = '--',lw = 2,color = 'grey')\n    y_real = np.concatenate(y_real)\n    y_proba = np.concatenate(y_proba)\n    precision, recall, _ = precision_recall_curve(y_real, y_proba)\n    plt.plot(recall, precision, color='blue',\n             label=r'Mean P|R')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title(str(model)+' P|R curve by folds')\n    plt.legend(loc=\"lower left\")\n\n    # Confusion maxtrix\n    plt.rcParams[\"axes.grid\"] = False\n    cm = np.average(cms, axis=0)\n    class_names = [0,1]\n    plt.figure()\n    plot_confusion_matrix(cm, \n                          classes=class_names, \n                          title= str(model).title()+' Confusion matrix [averaged\/folds]')\n    \n    # Feat imp plt\n    if model != 'XGB':\n        cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n            .groupby(\"Feature\")\n            .mean()\n            .sort_values(by=\"importance\", ascending=False)[:30].index)\n        best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\n        plt.figure(figsize=(10,10))\n        sns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False),\n                edgecolor=('white'), linewidth=2, palette=\"rocket\")\n        plt.title(str(model)+' Features importance (averaged\/folds)', fontsize=18)\n        plt.tight_layout()\n        \n    # Timer end    \n    timer(start_time)","ea5cdd32":"param_xgb = {\n            'n_jobs' : -1, \n            'n_estimators' : 200,\n            'seed' : 1337,\n            'random_state':1337,\n            'eval_metric':'auc'\n    }","cd78ab4b":"gradient_boosting_model(param_xgb, 5, 'XGB')","62a3e53d":"param_lgb = {\n            'bagging_fraction': 0.8082060379239122,\n            'colsample_bytree': 0.4236846658378094,\n            'feature_fraction': 0.1622850961512378,\n            'learning_rate': 0.24617571597038826,\n            'max_depth': 10,\n            'min_child_samples': 110.4846966877894,\n            'min_child_weight': 0.0077240770377460955,\n            'min_data_in_leaf': 16,\n            'num_leaves': 8,\n            'reg_alpha': 0.6051612648874549,\n            'reg_lambda': 97.89699721669824,\n            'subsample': 0.20955925262252026,\n            'objective': 'binary',\n            'save_binary': True,\n            'seed': 1337,\n            'feature_fraction_seed': 1337,\n            'bagging_seed': 1337,\n            'drop_seed': 1337,\n            'data_random_seed': 1337,\n            'boosting_type': 'gbdt',\n            'verbose': 1,\n            'is_unbalance': False,\n            'boost_from_average': True,\n            'metric':'auc'\n    }","5586296b":"gradient_boosting_model(param_lgb, 5, 'LGB')","53e794e0":"param_cb = {\n            'learning_rate': 0.09445645065743215,\n            'colsample_bylevel' : 0.24672360788274705,\n            'bagging_temperature': 0.39963209507789, \n            'l2_leaf_reg': int(22.165305913463673),\n            'depth': int(7.920859337748043), \n            'iterations' : 500,\n            'loss_function' : \"Logloss\",\n            'objective':'CrossEntropy',\n            'eval_metric' : \"AUC\",\n            'bootstrap_type' : 'Bayesian',\n            'random_seed':1337,\n            'early_stopping_rounds' : 100,\n            'use_best_model':True \n    }","978022d2":"gradient_boosting_model(param_cb, 5, 'CB')","466407f0":"# <a id='7'>7. CatBoost<\/a> ","5edb042c":"# <a id='2'>2. Feature engineering and preprocessing<\/a> ","1fe3cb6e":"- <a href='#1'>1. Libraries and Data<\/a>  \n- <a href='#2'>2. Quick EDA<\/a>  \n- <a href='#3'>3. Feature engineering and preprocessing<\/a> \n- <a href='#4'>4. Confusion Matrix function<\/a>\n- <a href='#5'>5. Gradient Boosting Model function<\/a>\n- <a href='#6'>6. XGBoost<\/a>\n- <a href='#7'>7. LightGBM<\/a>\n- <a href='#8'>8. CatBoost<\/a>","42e83a23":"# <a id='2'>2. Quick EDA<\/a> ","954ad5d7":"![](https:\/\/image.noelshack.com\/fichiers\/2019\/31\/2\/1564488066-cc.jpg)","f0d1b929":"# <a id='1'>1. Librairies and data<\/a> ","e2d16e01":"# <a id='3'>3. Confusion Matrix function<\/a> ","caff524a":"# <a id='4'>4. Gradient Boosting Model function<\/a> ","4a045bc6":"# <a id='5'>5. XGBoost<\/a> ","27448e59":"# <a id='6'>6. LightGBM<\/a> ","4f56b1bd":"----------\n**Fraud Detection - XGB | LGB | CB**\n=====================================\n\n* XGB : F1 = 0.8498 (+\/-0.019)\n* LGB : F1 = 0.8576 (+\/-0.018)\n* CB  : F1 = 0.8650 (+\/-0.019)\n\n***Vincent Lugat***\n\n*July 2019*\n\n----------"}}