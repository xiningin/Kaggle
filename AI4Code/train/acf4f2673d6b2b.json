{"cell_type":{"1526e24a":"code","64580200":"code","597522ea":"code","bb77cdb7":"code","381f94c2":"code","14fb0b10":"code","8d0dfb11":"code","70e12922":"code","68e30ad6":"code","8636e72e":"code","2f494e05":"code","b88b9a55":"code","e3db7f94":"code","8b4a309a":"code","fb4b9c88":"code","cb2a2cbc":"code","f9db2f7a":"code","c3781540":"code","a2c67116":"code","1813c051":"markdown","5398d815":"markdown","17aa54df":"markdown","cfa8684f":"markdown","23c146dc":"markdown","2e8486e4":"markdown","fdf16fa7":"markdown","3256efcd":"markdown","2381eff2":"markdown","7c17b567":"markdown","0891f541":"markdown","26ca9745":"markdown","2370985f":"markdown","8b1626f4":"markdown"},"source":{"1526e24a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n\nimport os\nimport torch\nimport pandas as pd\nfrom scipy import stats\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom collections import OrderedDict, namedtuple\nimport torch.nn as nn\nfrom torch.optim import lr_scheduler\nimport joblib\n\nimport logging\nimport transformers\nfrom transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\nimport sys\nfrom sklearn import metrics, model_selection\nimport torch.nn.functional as F\nimport re\nimport warnings\nfrom sklearn.model_selection import train_test_split\nwarnings.filterwarnings(\"ignore\")\n\nDATA_PATH='..\/input\/tweet-sentiment-extraction\/'\nBERT_PATH='..\/input\/bert-base-uncased\/'\n\n# Any results you write to the current directory are saved as output.","64580200":"train=pd.read_csv(DATA_PATH+'train.csv')\ntest=pd.read_csv(DATA_PATH+'test.csv')\nsubmission=pd.read_csv(DATA_PATH+'sample_submission.csv')\n\ntokenizer=transformers.BertTokenizer.from_pretrained(BERT_PATH+'vocab.txt')","597522ea":"def create_targets(df):\n    df['t_text'] = df['text'].apply(lambda x: tokenizer.tokenize(str(x)))\n    df['t_selected_text'] = df['selected_text'].apply(lambda x: tokenizer.tokenize(str(x)))\n    \n    def func(row):\n        x,y = row['t_text'],row['t_selected_text'][:]\n        for offset in range(len(x)):\n            d = dict(zip(x[offset:],y))\n            #when k = v that means we found the offset\n            check = [k==v for k,v in d.items()]\n            if all(check)== True:\n                break \n        return [0]*offset + [1]*len(y) + [0]* (len(x)-offset-len(y))\n    \n    df['targets'] = df.apply(func,axis=1)\n    return df\n\ntrain = create_targets(train)\n\nprint('MAX_SEQ_LENGTH_TEXT', max(train['t_text'].apply(len)))\nprint('MAX_TARGET_LENGTH',max(train['targets'].apply(len)))\nMAX_TARGET_LEN = MAX_SEQUENCE_LENGTH = 108","bb77cdb7":"## same way tokenize the test data also (for later use)\ntest['t_text'] = test['text'].apply(lambda x: tokenizer.tokenize(str(x)))\n\n## pad all the targets\ntrain['targets'] = train['targets'].apply(lambda x :x + [0] * (MAX_TARGET_LEN-len(x)))","381f94c2":"class BERTBaseUncased(nn.Module):\n    def __init__(self, bert_path):\n        super(BERTBaseUncased, self).__init__()\n        self.bert_path = bert_path\n        self.bert = transformers.BertModel.from_pretrained(self.bert_path)\n        self.bert_drop = nn.Dropout(0.3)\n        self.out = nn.Linear(768 * 2, MAX_SEQUENCE_LENGTH)\n\n    def forward(\n            self,\n            ids,\n            mask,\n            token_type_ids\n    ):\n        o1, o2 = self.bert(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids)\n        \n        apool = torch.mean(o1, 1)\n        mpool, _ = torch.max(o1, 1)\n        cat = torch.cat((apool, mpool), 1)\n\n        bo = self.bert_drop(cat)\n        p2 = self.out(bo)\n        p2 = F.sigmoid(p2)\n        \n        return p2","14fb0b10":"class BERTDatasetTraining:\n    def __init__(self, comment_text, tokenizer, max_length, targets=None,train=False):\n        self.comment_text = comment_text\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.targets = targets\n        self.train=train\n\n    def __len__(self):\n        return len(self.comment_text[0])\n\n    def __getitem__(self, idx):\n        input_ids       = self.comment_text[0][idx]\n        input_masks     = self.comment_text[1][idx]\n        input_segments  = self.comment_text[2][idx]\n        \n        if self.train: # targets\n            labels = self.targets[idx]\n            return input_ids, input_masks, input_segments, labels\n        \n        return input_ids, input_masks, input_segments","8d0dfb11":"def train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n    model.train()\n    tk0=tqdm(data_loader, total=len(data_loader))\n    \n    for bi, d in enumerate(tk0):\n        \n        ids,mask, token_type_ids, targets= d\n\n        ids = ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n\n        optimizer.zero_grad()\n        outputs = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        loss = loss_fn(outputs, targets)\n        if bi % 100 == 0:\n            print(f'Training -> training_data_{bi}, loss={loss}')\n\n        loss.backward()\n        optimizer.step()\n        \n        if scheduler is not None:\n            scheduler.step()","70e12922":"def eval_loop_fn(data_loader, model, device,batch_size,valshape):\n    model.eval()\n    \n    valid_preds = np.zeros((valshape, MAX_SEQUENCE_LENGTH))\n    original = np.zeros((valshape, MAX_SEQUENCE_LENGTH))\n    \n    for bi, d in enumerate(data_loader):\n        ids,mask, token_type_ids,targets= d\n\n        ids = ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n\n        outputs = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        valid_preds[bi*batch_size : (bi+1)*batch_size] = outputs.detach().cpu().numpy()\n        original[bi*batch_size : (bi+1)*batch_size]    = targets.detach().cpu().numpy()   \n\n    return valid_preds, original\n\n\ndef loss_fn(outputs, targets):\n    return nn.BCEWithLogitsLoss()(outputs, targets)","68e30ad6":"def _convert_to_bert_inputs(text, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    inputs = tokenizer.encode_plus(\n        text,\n        None,\n        add_special_tokens=True,\n        max_length=max_sequence_length,\n    )\n    ids = inputs[\"input_ids\"]\n    token_type_ids = inputs[\"token_type_ids\"]\n    mask = inputs[\"attention_mask\"]\n\n    padding_length = max_sequence_length - len(ids)\n\n    ids = ids + ([0] * padding_length)\n    mask = mask + ([0] * padding_length)\n    token_type_ids = token_type_ids + ([0] * padding_length)\n\n    return [ids,mask,token_type_ids]\n\n\ndef compute_input_arays(df, columns, tokenizer, max_sequence_length):\n    \n    input_ids, input_masks, input_segments = [], [], []\n    \n    for _, instance in tqdm(df.iterrows(),total=len(df)):\n        \n        t = str(instance.text)\n        \n        ids, masks, segments = _convert_to_bert_inputs(t,tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [\n        torch.from_numpy(np.asarray(input_ids, dtype=np.int32)).long(), \n        torch.from_numpy(np.asarray(input_masks, dtype=np.int32)).long(),\n        torch.from_numpy(np.asarray(input_segments, dtype=np.int32)).long(),\n    ]\n\ndef compute_output_arrays(df, col):\n    return np.asarray(df[col].values.tolist())","8636e72e":"def run():\n    \n    TRAIN_BATCH_SIZE = 64\n    EPOCHS = 5\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    tokenizer = transformers.BertTokenizer.from_pretrained(BERT_PATH)\n    \n    \n    train_df , valid_df = train_test_split(train,test_size=0.20, random_state=42,shuffle=True) ## Split Labels\n\n    inputs_train = compute_input_arays(train_df, 'text', tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH)\n    outputs_train = compute_output_arrays(train_df,'targets')\n\n    train_dataset = BERTDatasetTraining(\n        comment_text=inputs_train,\n        targets=outputs_train,\n        tokenizer=tokenizer,\n        max_length=MAX_SEQUENCE_LENGTH,\n        train=True\n    )\n\n    train_loader =  torch.utils.data.DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n\n    inputs_valid = compute_input_arays(valid_df, 'text', tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH)\n    outputs_valid = compute_output_arrays(valid_df,'targets')\n    \n    valid_dataset = BERTDatasetTraining(\n        comment_text=inputs_valid,\n        targets=outputs_valid,\n        tokenizer=tokenizer,\n        max_length=MAX_SEQUENCE_LENGTH,\n        train=True\n    )\n\n    \n    valid_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=64,\n        num_workers=4\n    )\n\n    model = BERTBaseUncased(bert_path=BERT_PATH).to(device)\n\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n\n    lr = 3e-5\n    num_train_steps = int(len(train_dataset) \/ TRAIN_BATCH_SIZE * EPOCHS)\n\n    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_train_steps\n    )\n\n    for epoch in range(EPOCHS):\n        print(f'Epoch==>{epoch}')\n        train_loop_fn(train_loader, model, optimizer, device, scheduler=scheduler)\n        o, t = eval_loop_fn(valid_loader, model, device, batch_size=64,valshape=valid_df.shape[0])\n        \n        torch.save(model.state_dict(), \"model.bin\")","2f494e05":"run()","b88b9a55":"device = \"cuda\"\nmodel = BERTBaseUncased(bert_path=BERT_PATH).to(device)\nmodel.load_state_dict(torch.load(\"..\/working\/model.bin\"))\nmodel.eval()","e3db7f94":"inputs_test = compute_input_arays(test, 'text', tokenizer, max_sequence_length=MAX_SEQUENCE_LENGTH)\n\ntest_dataset = BERTDatasetTraining(\n    comment_text=inputs_test,\n    tokenizer=tokenizer,\n    max_length=MAX_SEQUENCE_LENGTH\n)\n\n\ntest_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    batch_size=64,\n    num_workers=4\n)","8b4a309a":"with torch.no_grad():\n    test_preds = np.zeros((len(test_dataset), MAX_SEQUENCE_LENGTH))\n    \n    for bi, d in enumerate(test_loader):\n     \n        ids,mask, token_type_ids= d\n\n        ids = ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n\n        outputs = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        test_preds[bi*64 : (bi+1)*64] = outputs.detach().cpu().numpy()","fb4b9c88":"pred = np.where(test_preds>=0.3,1,0)\n\ntemp_output = []\nfor idx,p in enumerate(pred):\n    indexes = np.where(p>=0.3)\n    current_text = test['t_text'][idx]\n    if len(indexes[0])>0:\n        start = indexes[0][0]\n        end = indexes[0][-1]\n    else:\n        start = 0\n        end = len(current_text)\n    \n    temp_output.append(' '.join(current_text[start:end+1]))","cb2a2cbc":"test['temp_output'] = temp_output","f9db2f7a":"def correct_op(row):\n    placeholder = row['temp_output']\n    for original_token in row['text'].split():\n        token_str = ' '.join(tokenizer.tokenize(original_token))\n        placeholder = placeholder.replace(token_str,original_token,1)\n    return placeholder\n\ntest['temp_output2'] = test.apply(correct_op,axis=1)","c3781540":"## for Neutral tweets keep things same\ndef replacer(row):\n    if row['sentiment']=='neutral':\n        return row['text']\n    else:\n        return row['temp_output2']\n\ntest['temp_output2'] = test.apply(replacer,axis=1)","a2c67116":"submission['selected_text']=test['temp_output2'].values\nsubmission.to_csv('submission.csv',index=None)\nsubmission.head()","1813c051":"References:\n\n* https:\/\/www.kaggle.com\/abhishek\/bert-multi-lingual-tpu-training\n* https:\/\/www.kaggle.com\/gskdhiman\/bert-baseline-starter-kernel\/data","5398d815":"<font size=\"4\" color='blue'>Dataset class<\/font>:","17aa54df":"<font size=\"4\" color='blue'>Post-processing<\/font>:","cfa8684f":"<font size=\"4\" color='blue'>Inference mode<\/font>:","23c146dc":"<font size=\"4\" color='blue'>Pre-processing<\/font>:","2e8486e4":"<font size=\"4\" color='blue'>Training engine<\/font>:","fdf16fa7":"<font size=\"4\" color='blue'>Model <\/font>:","3256efcd":"<font size=\"4\" color='blue'>Submission<\/font>:","2381eff2":"<font size=\"4\" color='blue'>Training loop<\/font>:","7c17b567":"<font size=\"4\" color='blue'>Evaluation<\/font>:","0891f541":"### Let's get started","26ca9745":"<font size=\"4\" color='red'>Kindly upvote the kernel if you like my work! Thanks<\/font>:","2370985f":"<font size=\"4\" color='blue'>Helper functions<\/font>:","8b1626f4":"## About this notebook:\n\nThe goal of this competition is to extract the relevant phrases from the text which supports the sentiment label. Now there are many different ways to solve this problem. Few of the approaches are:\n\n* Rule based chunking\n* Graph algorithms\n* Token level classification\n* QA model\n\nHere I have tried out token level classification using bert. This approach is pretty basic to start with\n\nThis notebook offers structured skeleton based on @abhishek's torch training notebook.\n\nNote: <p>This kernel is recreated in pytorch based on reference kernel [here](https:\/\/www.kaggle.com\/gskdhiman\/bert-baseline-starter-kernel\/data)<\/p>"}}