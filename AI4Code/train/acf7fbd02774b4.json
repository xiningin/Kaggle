{"cell_type":{"f15436dd":"code","e7a54884":"code","89c88c31":"code","97514f78":"code","8020ec26":"code","d5f83af0":"code","251bd5be":"code","88f36a11":"code","2e2cc8cf":"code","31c54e3e":"code","54b5870b":"code","27d0c2d8":"code","3d6a6a0c":"code","c51fb295":"code","46a72248":"code","c01db78e":"code","d97f4790":"code","302382d0":"code","c3b28165":"code","d1d9c815":"code","2a14d815":"code","1858420b":"code","d0feb5bd":"code","4add76aa":"code","ef0f6da9":"code","bbb960a1":"code","8ff5d904":"code","1b0fa48c":"code","c9b72681":"code","da398359":"code","7e8cfa0d":"code","30595355":"code","3ef8094b":"code","97efb160":"code","1e589a0d":"code","6d946813":"code","7655a05b":"code","53919505":"code","44a87b79":"code","4fdb0463":"code","a8b90cdd":"code","55a111ad":"code","5c1a9684":"code","0f14b4d2":"code","94813128":"code","45736622":"code","e38de436":"code","bd4b214a":"code","fe40986a":"code","affa5d7c":"code","37beaca4":"code","95593359":"code","b91d3571":"code","a586a4d4":"code","5a57cd58":"code","597bcbae":"code","55bc9aa1":"code","71eb839c":"code","6a5ddde1":"code","ea39cbe1":"code","cfd04b25":"code","072d4bab":"code","b3d2195f":"code","b82fc358":"code","50468016":"code","78a7613e":"code","07045013":"code","32debfec":"code","37af966e":"code","32db4f54":"code","e6dc9bd4":"code","674967e4":"code","593bf16a":"code","70c7ab83":"code","7273d845":"code","5fbc6577":"code","01ae1c66":"code","1f024e0b":"code","156d0a25":"code","fa294652":"code","e9990688":"code","f3ad4fde":"code","f7853ccf":"code","c9f299d4":"code","83e0e9a0":"code","f547ac48":"code","65048499":"code","2958b1e2":"code","761b2798":"code","b75bf388":"code","9db12306":"code","7494900d":"code","d762168d":"code","c1bc1573":"code","118ead39":"code","0fc7fd7b":"code","7e7aab2e":"code","4bcb7aa5":"code","2de7bb84":"code","41cdf600":"code","a292d1a7":"code","245cc9c0":"code","a2994fd9":"code","4ea37923":"code","e5cb571a":"code","6f4cee90":"code","17a43ec1":"code","1f9614b6":"code","41020b85":"markdown","ebdd3c8c":"markdown","c57c5fcb":"markdown","c5de6ee8":"markdown","98830fc6":"markdown","c19b712b":"markdown","679fb1d0":"markdown","d9131e38":"markdown","aacdd31c":"markdown","9c120ee0":"markdown","e53b0e18":"markdown","47318535":"markdown","fc29ed6c":"markdown","df53dc88":"markdown","a7ecc669":"markdown","f2dce717":"markdown","ee6bddd5":"markdown","015ea133":"markdown","818fb1e4":"markdown","0d1183b2":"markdown","fadeaa32":"markdown","6975a358":"markdown","ab38f06d":"markdown","2ac165ee":"markdown","06a7f8dd":"markdown","52eb38fc":"markdown","2d1397d8":"markdown","53f0a3ea":"markdown","d43bde7f":"markdown","d5d28334":"markdown","72b62562":"markdown","8116f535":"markdown","d549bf62":"markdown","2bc17f79":"markdown","0e1522f3":"markdown","b5ee0426":"markdown","9d84515f":"markdown","b06cf5d3":"markdown","7bfdd451":"markdown","13077589":"markdown","638fdfbe":"markdown","7b04c968":"markdown","a4ad02c5":"markdown","70c908f8":"markdown","8678e93c":"markdown","b3467497":"markdown","2b2a8cf2":"markdown","da178c42":"markdown","5e3fa327":"markdown"},"source":{"f15436dd":"import pandas as pd\n\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","e7a54884":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport numpy as np","89c88c31":"test.head()","97514f78":"train.head()","8020ec26":"train.info()","d5f83af0":"test.info()","251bd5be":"sns.distplot(train['count']);","88f36a11":"train[\"count\"] = np.log1p(train[\"count\"])\nsns.distplot(train['count']);","2e2cc8cf":"ntrain = train.shape[0] # number of training\nntest = test.shape[0] #number of test\n","31c54e3e":"y_train = train[\"count\"].values ","54b5870b":"y_train","27d0c2d8":"#concat train\/test data\nall_data = pd.concat((train, test)).reset_index(drop=True)\n","3d6a6a0c":"all_data.info()","c51fb295":"all_data.shape","46a72248":"all_data.drop(['casual'], axis=1, inplace=True)\nall_data.drop(['registered'], axis=1, inplace=True)\nall_data.drop(['count'], axis=1, inplace=True)","c01db78e":"all_data.shape","d97f4790":"all_data.info()","302382d0":"#target = cout \/ \n#casual, registerd = \ub0a0\ub824\uc57c?","c3b28165":"import datetime\nfrom datetime import datetime","d1d9c815":"all_data.head()","2a14d815":"all_data['date']  = all_data.datetime.apply(lambda x: x.split()[0])\nall_data['hour'] = all_data.datetime.apply(lambda x: x.split()[1].split(':')[0])","1858420b":"all_data['weekday'] = all_data.date.apply(lambda dateString : datetime.strptime(dateString, '%Y-%m-%d').weekday())\nall_data['month'] = all_data.date.apply(lambda dateString : datetime.strptime(dateString, '%Y-%m-%d').month)","d0feb5bd":"all_data.head()","4add76aa":"all_data.drop(['datetime'], axis=1, inplace=True)","ef0f6da9":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","bbb960a1":"train[\"count\"] = y_train","8ff5d904":"from sklearn.preprocessing import LabelEncoder\nfrom scipy import stats\nfrom scipy.stats import norm, skew ","1b0fa48c":"#get the numeric values\nnumeric_features = all_data.dtypes[all_data.dtypes != \"object\"].index\nnumeric_features","c9b72681":"# Check the skew of all numerical features\nskewed_feats = all_data[numeric_features].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness","da398359":"skewness = skewness[abs(skewness)>0.7]\nskewness","7e8cfa0d":"#you can take log like this \n\n#skewness = skewness[abs(skewness)>0.7]\n#all_data[\"holiday\"] = np.log1p(all_data[\"holiday\"])\n#all_data[\"weather\"] = np.log1p(all_data[\"weather\"])\n#all_data[\"workingday\"] = np.log1p(all_data[\"workingday\"])\n","30595355":"#skewed_feats = all_data[numeric_features].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n#print(\"\\nSkew in numerical features: \\n\")\n#skewness = pd.DataFrame({'Skew' :skewed_feats})\n#skewness","3ef8094b":"corr = train.corr(method='pearson').drop(['count']).sort_values('count', ascending=False)['count']\ncorr ","97efb160":"import seaborn as sns","1e589a0d":"corrMat = train.corr()\nmask = np.array(corrMat)\nmask[np.tril_indices_from(mask)] = False\nfig, ax= plt.subplots(figsize=(20, 10))\nsns.heatmap(corrMat, mask=mask,vmax=1., square=True,annot=True)","6d946813":"from patsy import dmatrices\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","7655a05b":"train.columns","53919505":"train.head()","44a87b79":"features = \"atemp+holiday+humidity+season+temp+weather+windspeed+workingday+hour+weekday+month\"","4fdb0463":"# Break into left and right hand side; y and X\ny, X = dmatrices(\"count ~\" + features, data=train, return_type=\"dataframe\")\n\n# For each Xi, calculate VIF\nvif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n# Fit X to y\nresult = sm.OLS(y, X).fit()","a8b90cdd":"print(result.summary())","55a111ad":"# For each X, calculate VIF and save in dataframe\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif[\"features\"] = X.columns","5c1a9684":"vif.round(1)","0f14b4d2":"#\uc774\ub7f0\uc2dd\uc73c\ub85c drop\ud574\uc904\uc218 \uc788\ub2e4\n#housing.drop('households', axis=1, inplace=True)\n#housing.drop('latitude', axis=1, inplace=True)","94813128":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","45736622":"train[\"count\"]=y_train","e38de436":"group_season = train.groupby(['season'])['count'].sum().reset_index()\nax = sns.barplot(x = group_season['season'], y = group_season['count'])\nax.set(xlabel='season', ylabel='count')\nplt.show()","bd4b214a":"group_dow = train.groupby(['weekday'])['count'].sum().reset_index()\nax = sns.barplot(x = group_dow['weekday'], y = group_dow['count'])\nax.set(xlabel='weekday', ylabel='count')\nplt.show()","fe40986a":"group_mn = train.groupby(['month'])['count'].sum().reset_index()\nax = sns.barplot(x = group_mn['month'], y = group_mn['count'])\nax.set(xlabel='month', ylabel='count')\nplt.show()","affa5d7c":"group_hr = train.groupby(['hour'])['count'].sum().reset_index()\nax = sns.barplot(x = group_hr['hour'], y = group_hr['count'])\nax.set(xlabel='hour', ylabel='count')\nplt.show()","37beaca4":"plt.figure(figsize=(29,15))\ngroup_season = train.groupby(['temp'])['count'].sum().reset_index()\nax = sns.barplot(x = group_season['temp'], y = group_season['count'])\nax.set(xlabel='temp', ylabel='count')\nplt.show()","95593359":"plt.figure(figsize=(30,12))\ngroup_season = train.groupby(['atemp'])['count'].sum().reset_index()\nax = sns.barplot(x = group_season['atemp'], y = group_season['count'])\nax.set(xlabel='atemp', ylabel='count')\nplt.show()","b91d3571":"all_data[\"temp\"].value_counts()","a586a4d4":"train_test_data = [train, test] # train\uacfc test set \ud569\uce68","5a57cd58":"for dataset in train_test_data:\n    dataset.loc[ dataset['temp'] <= 5, 'temp'] = 0,\n    dataset.loc[(dataset['temp'] > 5) & (dataset['temp'] <= 10), 'temp'] = 1,\n    dataset.loc[(dataset['temp'] > 10) & (dataset['temp'] <= 15), 'temp'] = 2,\n    dataset.loc[(dataset['temp'] > 15) & (dataset['temp'] <= 20), 'temp'] = 3,\n    dataset.loc[(dataset['temp'] > 20) & (dataset['temp'] <= 25), 'temp'] = 4,\n    dataset.loc[(dataset['temp'] > 25) & (dataset['temp'] <= 30), 'temp'] = 5,        \n    dataset.loc[(dataset['temp'] > 30) & (dataset['temp'] <= 35), 'temp'] = 6, \n    dataset.loc[ dataset['temp'] > 35, 'temp'] = 7","597bcbae":"train.head()","55bc9aa1":"plt.figure(figsize=(12,7))\ngroup_season = train.groupby(['temp'])['count'].sum().reset_index()\nax = sns.barplot(x = group_season['temp'], y = group_season['count'])\nax.set(xlabel='temp', ylabel='count')\nplt.show()","71eb839c":"plt.figure(figsize=(29,15))\ngroup_season = train.groupby(['humidity'])['count'].sum().reset_index()\nax = sns.barplot(x = group_season['humidity'], y = group_season['count'])\nax.set(xlabel='humidity', ylabel='count')\nplt.show()","6a5ddde1":"for dataset in train_test_data:\n    dataset.loc[ dataset['humidity'] <= 10, 'humidity'] = 0,\n    dataset.loc[(dataset['humidity'] > 10) & (dataset['humidity'] <= 20), 'humidity'] = 1,\n    dataset.loc[(dataset['humidity'] > 20) & (dataset['humidity'] <= 30), 'humidity'] = 2,\n    dataset.loc[(dataset['humidity'] > 30) & (dataset['humidity'] <= 40), 'humidity'] = 3,\n    dataset.loc[(dataset['humidity'] > 40) & (dataset['humidity'] <= 50), 'humidity'] = 4,\n    dataset.loc[(dataset['humidity'] > 50) & (dataset['humidity'] <= 60), 'humidity'] = 5,        \n    dataset.loc[(dataset['humidity'] > 60) & (dataset['humidity'] <= 70), 'humidity'] = 6, \n    dataset.loc[(dataset['humidity'] > 70) & (dataset['humidity'] <= 80), 'humidity'] = 7, \n    dataset.loc[(dataset['humidity'] > 80) & (dataset['humidity'] <= 90), 'humidity'] = 8,  \n    dataset.loc[ dataset['humidity'] > 90, 'humidity'] = 9","ea39cbe1":" \nplt.figure(figsize=(12,7))\ngroup_season = train.groupby(['temp'])['count'].sum().reset_index()\nax = sns.barplot(x = group_season['temp'], y = group_season['count'])\nax.set(xlabel='temp', ylabel='count')\nplt.show()","cfd04b25":"all_data.info()","072d4bab":"train.head()","b3d2195f":"test.head()","b82fc358":"train.drop('atemp', axis=1, inplace=True)\ntrain.drop('date', axis=1, inplace=True)\ntest.drop('atemp', axis=1, inplace=True)\ntest.drop('date', axis=1, inplace=True)","50468016":"train.head()","78a7613e":"train.drop('count', axis=1, inplace=True)","07045013":"#concat train\/test data\nall_data = pd.concat((train, test)).reset_index(drop=True)","32debfec":"all_data.head()","37af966e":"all_data.info()","32db4f54":"#Changing into a categorical variable\nall_data['season'] = all_data['season'].astype(str)\nall_data['weather'] = all_data['weather'].astype(str)\nall_data['hour'] = all_data['hour'].astype(str)\nall_data['weekday'] = all_data['weekday'].astype(str)\nall_data['month'] = all_data['month'].astype(str)\nall_data['workingday'] = all_data['workingday'].astype(str)\nall_data['holiday'] = all_data['holiday'].astype(str)\n\n","e6dc9bd4":"all_data.info()","674967e4":"#from sklearn.preprocessing import LabelEncoder\n#cols = ('season', 'weather', 'hour', 'weekday', 'month', 'workingday','holiday')\n# process columns, apply LabelEncoder to categorical features\n#for c in cols:\n#    lbl = LabelEncoder() \n#    lbl.fit(list(all_data[c].values)) \n#    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \n#all_data.shape","593bf16a":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","70c7ab83":"all_data.head()","7273d845":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","5fbc6577":"train.head()","01ae1c66":"test.head()","1f024e0b":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC, LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import fbeta_score, make_scorer","156d0a25":"from sklearn import metrics\nimport warnings\npd.options.mode.chained_assignment = None\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","fa294652":"def rmsle(y, y_,convertExp=True):\n    if convertExp:\n        y = np.exp(y),\n        y_ = np.exp(y_)\n    log1 = np.nan_to_num(np.array([np.log(v + 1) for v in y]))\n    log2 = np.nan_to_num(np.array([np.log(v + 1) for v in y_]))\n    calc = (log1 - log2) ** 2\n    return np.sqrt(np.mean(calc))","e9990688":"lModel = LinearRegression()\n\n# Train the model\nlModel.fit(X = train,y = y_train)","f3ad4fde":"lModel.fit(train,y_train)\nlModel_train_pred = lModel.predict(train)\nlModel_pred = np.expm1(lModel.predict(test.values))\nprint (\"RMSLE Value For Linear Regression: \")\nprint(rmsle(y_train, lModel_train_pred))","f7853ccf":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=3))","c9f299d4":"lasso.fit(train,y_train)\nlasso_train_pred = lasso.predict(train)\nlasso_pred = np.expm1(lasso.predict(test.values))\nprint (\"RMSLE Value For Lasso Regression: \")\nprint(rmsle(y_train, lasso_train_pred))","83e0e9a0":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","f547ac48":"ENet.fit(train,y_train)\nENet_train_pred = ENet.predict(train)\nENet_pred = np.expm1(ENet.predict(test.values))\nprint (\"RMSLE Value For ENet Regression: \")\nprint(rmsle(y_train, ENet_train_pred))","65048499":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5) #kernel = 'rbf' , 'sigmoid' ","2958b1e2":"KRR.fit(train,y_train)\nKRR_train_pred = KRR.predict(train)\nKRR_pred = np.expm1(KRR.predict(test.values))\nprint (\"RMSLE Value For KRR Regression: \")\nprint(rmsle(y_train, KRR_train_pred))","761b2798":"GBoost = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n             learning_rate=0.05, loss='huber', max_depth=4,\n             max_features='sqrt', max_leaf_nodes=None,\n             min_impurity_decrease=0.0, min_impurity_split=None,\n             min_samples_leaf=15, min_samples_split=10,\n             min_weight_fraction_leaf=0.0, n_estimators=3000,\n             presort='auto', random_state=5, subsample=1.0, verbose=0,\n             warm_start=False)","b75bf388":"GBoost.fit(train,y_train)\nGBoost_train_pred = GBoost.predict(train)\nGBoost_pred = np.expm1(GBoost.predict(test.values))\nprint (\"RMSLE Value For GBoost Regression: \")\nprint(rmsle(y_train, GBoost_train_pred))","9db12306":"#You can do grid search like this \n\n#scorer = make_scorer(rmsle, greater_is_better=False)\n\n#params = {'n_estimators':[3000, 3200, 3500], 'learning_rate' :[0.1, 0.05]}\n\n\n#model_gb = GradientBoostingRegressor(n_estimators=3200, learning_rate=0.1,\n#                                   max_depth=4, max_features='sqrt',\n#                                   min_samples_leaf=15, min_samples_split=10, \n#                                   loss='huber', random_state =5)\n\n#grid_search = GridSearchCV(model_gb, params, cv=5, scoring=scorer)\n#grid_search.fit(train, y_train)\n","7494900d":"#grid_search.best_estimator_","d762168d":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","c1bc1573":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","118ead39":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","0fc7fd7b":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","7e7aab2e":"model_svr = SVR(C=1, cache_size=200, coef0=0, degree=3, epsilon=0.1, gamma='auto',\n  kernel='poly', max_iter=-1, shrinking=True, tol=0.001, verbose=False)\n\n#\u2018linear\u2019, \u2018poly\u2019, \u2018rbf\u2019, \u2018sigmoid\u2019, \u2018precomputed\u2019 ","4bcb7aa5":"model_svr.fit(train, y_train)\nsvr_train_pred = model_svr.predict(train)\nsvr_pred = np.expm1(model_svr.predict(test.values))\n#You can do grid search like this print(rmsle(y_train, svr_train_pred))","2de7bb84":"#You can do grid search like this \n\n#params = {'coef0':[0, 0.1, 0.5, 1], 'C' :[0.1, 0.5, 1], 'epsilon':[0.1, 0.3, 0.5]}\n\n\n#model_svr = SVR()\n#grid_search = GridSearchCV(model_svr, params, cv=3, scoring=scorer)\n#grid_search.fit(train, y_train)","41cdf600":"#grid_search.best_estimator_","a292d1a7":"regr = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n           max_features=30, max_leaf_nodes=None, min_impurity_decrease=0.0,\n           min_impurity_split=None, min_samples_leaf=1,\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\n           n_estimators=60, n_jobs=1, oob_score=False, random_state=None,\n           verbose=0, warm_start=False)","245cc9c0":"regr.fit(train, y_train)\nregr_train_pred = regr.predict(train)\nregr_pred = np.expm1(regr.predict(test.values))\nprint(rmsle(y_train, regr_train_pred))","a2994fd9":"#You can do grid search like this \n\n#param_grid = [\n#    {'n_estimators': [3, 10, 30, 60, 90], 'max_features': [10,20,30,40,50]},\n#    {'bootstrap': [True], 'n_estimators': [3, 10, 30, 60, 90], 'max_features': [10,20,30,40,50]},\n#]\n\n#forest_reg = RandomForestRegressor()\n#grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring=scorer)\n#grid_search.fit(train, y_train)","4ea37923":"#grid_search.best_estimator_","e5cb571a":"#ensemble = xgb_pred*0.25 + GBoost_pred*0.25 + regr_pred*0.5  \n#ensemble = xgb_pred*0.3 + GBoost_pred*0.3 + regr_pred*0.4  \nensemble = xgb_pred*0.4 + GBoost_pred*0.4 + regr_pred*0.2  # \uc774\uac8c\uc824\ub192\ub2e4  this one gives me a best score ","6f4cee90":"#test = pd.read_csv('..\/input\/test.csv')","17a43ec1":"#timeColumn = test['datetime']","1f9614b6":"#sub = pd.DataFrame()\n#sub['datetime'] = timeColumn\n#sub['count'] = ensemble\n#sub.to_csv('submission.csv',index=False)","41020b85":"<h5> decrease scope of \"temp\" with binning","ebdd3c8c":"<h5> LightGBM","c57c5fcb":"<h5>make Ensemble prediction ","c5de6ee8":"<h5> check relation of month \/ count ","98830fc6":"\uc989 \uae30\uc628 \uad00\ub828 feature\uc640 highly correlated<br>\n\ud558\uc9c0\ub9cc temp\uc640 atemp\ub07c\ub9ac\ub3c4 correaltion\uc774 \ub192\uae30 \ub54c\ubb38\uc5d0 \ub458 \uc911 \ud558\ub098\ub9cc \ub0a8\uaca8\uc57c? vif\uc640 p-value \ud655\uc778\ud574\ubcf4\uc790","c19b712b":"<h5> Gradient Boosting Regression \n","679fb1d0":"\nLabel Encoding some categorical variables that may contain information in their ordering set","d9131e38":"Select Xgboost, RF Regression, Gradient Boosting Regression","aacdd31c":"<h5> Linear Regression","9c120ee0":"<h5> check relation of season \/ count ","e53b0e18":"<h5> check relation of weekday \/ count ","47318535":"people won't use it if it's cold weather (Jan, Feb)","fc29ed6c":"<h3>check skewness ","df53dc88":"<h5> Kernel Ridge Regression \n","a7ecc669":"<h5>Binning for \"humidity\" features","f2dce717":"<h3>Prepare modeling","ee6bddd5":"<br>\n<h5>Let's see correlation matrix \n<br>","015ea133":"- temp - temperature in Celsius\n- atemp - \"feels like\" temperature in Celsius\n- humidity - relative humidity","818fb1e4":"\nTransforming some numerical variables to categorical\nUsually, categorical data has no order","0d1183b2":"- \ub2e4\uc911\uacf5\uc0b0\uc131 Multicollinearity \uccb4\ud06c --> \uc6d0\ub798 10\uc774 \ub118\uc73c\uba74 \ubb38\uc81c\uac00 \uc788\ub294\uac83\uc73c\ub85c \ubcf4\uace0 \ucc98\ub9ac\ud574\uc918\uc57c \ud55c\ub2e4\n- temp\uc640 atemp\uc758 vif\uac00 \ub192\ub2e4. \ub530\ub77c\uc11c \uc6b0\ub9ac\ub294 ~\ub97c \uc0ad\uc81c -- \uc774\ub530\uac00 \uad50\ucc28\uac80\uc99d\ud574\ubcf4\uba70 rmse \ud655\uc778 ","fadeaa32":"we don't have null values. so next, <h5>we have to check skewness for all data","6975a358":"<h5> XGBoost","ab38f06d":"Label Encoding some categorical variables that may contain information in their ordering set","2ac165ee":"<h5> Random Forest Regressor","06a7f8dd":"# 3. Modeling","52eb38fc":"count(target variable) has lot of correlation with \"humidity, temp, atemp\"","2d1397d8":"<h3> check null values","53f0a3ea":"seems like we don't have null values","d43bde7f":"<h3> check correlation","d5d28334":"save values for training models","72b62562":"demand of bicycle is increasing at the commute time (around 7-8am and 5-6pm)","8116f535":"summer and fall is the best season of demand ","d549bf62":"<h5> Elastic Net Regression","2bc17f79":"# 2. Feature Engineering","0e1522f3":"<h3> split datetime to date \/ hour \/ weekday \/ month","b5ee0426":"<h5> check relation of hour \/ count ","9d84515f":"RMSLE Value For GBoost Regression: <br>\n0.29379046001726833<br>\n<br>\nRMSLE Value For XGBoost Regression:<br>\n0.3104879867673587<br>\n<br>\nRMSLE Value For RF Regression:<br>\n0.17073150609606672\n","b06cf5d3":"<h3>check skewness of target value","7bfdd451":"<h5> check relation of temp,atemp \/ count \n","13077589":"<h5> Support Vector Regressor","638fdfbe":"# 1. Explore data set","7b04c968":"<h5> Lasso Regression","a4ad02c5":"<h3>p-value & vif","70c908f8":"drop \"casual, registered, count\" columns","8678e93c":"<h5>Submission","b3467497":"<h3> Getting insight from features<\/h3>\n\n- hour \/ count \n- weekday \/ count \n- season \/ count\n- temp,atemp \/ count<br> \n\n\uc989 \uc2dc\uac04\ub300\ubcc4 \/ \uacc4\uc808\ubcc4 \/ \uc694\uc77c\uc5d0 \ub530\ub77c\uc11c count\uac00 \uc5bc\ub9c8\ub098 \ub098\uc624\ub294\uc9c0 \ud655\uc778","2b2a8cf2":"<h5>Binning for \"temp\" features","da178c42":"# 4. Prediction","5e3fa327":"\uadf8\ub0e5 \uc0c1\ud0dc\uc5d0\uc11c \ube44\uad50\ud558\ub0d0(\uc9c0\uc218\ud654\ud574\uc11c \uc6d0\ub798\uc0c1\ud0dc\ub85c \ub3cc\ub9ac\uace0) \uc544\ub2c8\uba74 \ub458 \ub2e4 \ub85c\uadf8\ucde8\ud574\uc9c4 \uc0c1\ud0dc\uc5d0\uc11c \ube44\uad50\ud558\ub0d0 <br>\n\uc608\uce21 \uc9c4\ud589\ud560\ub550 \uc5b4\ucc28\ud53c ~_pred\ub85c \ud558\ub2c8\uae50 \uc6d0\ub798 \uc0c1\ud0dc\uac12\uc73c\ub85c \uc608\uce21(\uc9c0\uc218\ud654\ub41c\uac70)"}}