{"cell_type":{"1814425e":"code","72de1239":"code","4415e57a":"code","d19b26c2":"code","efa920ac":"code","7aef39a5":"code","1c71b137":"code","045d4296":"code","9f740225":"code","07529f16":"code","c635482d":"code","93e21aa1":"code","9ea9aa1f":"code","407465b4":"code","44c92623":"code","84d76a24":"code","d0ec6531":"code","cdcedea9":"code","12d50965":"code","614b00dd":"code","f393c013":"code","993726e3":"code","883407c9":"code","4f632afb":"code","e269dec2":"code","424cab55":"code","b7b63111":"code","a8926d4d":"code","d6d49eee":"code","1593324a":"code","7c0dae15":"code","8159879b":"code","efce9132":"code","c418b20f":"code","8dcfd745":"code","f07f9c36":"code","6592d2af":"code","07d96158":"code","d6d7779b":"code","84d8d57b":"code","64de9b51":"code","3252ae24":"code","dd298b30":"code","31eea2c2":"code","5a4721ff":"code","96a3afd6":"code","ddfd5408":"code","7cce0cc4":"code","217bbcd0":"code","faa560db":"code","5b901542":"code","63e8883f":"code","c00bce02":"markdown","7426b16e":"markdown","908bbd08":"markdown","d5de002c":"markdown","28264b94":"markdown","d86a44b6":"markdown","61612995":"markdown","b92be7d4":"markdown","3871d46d":"markdown","2c204ac0":"markdown","cbc2840f":"markdown","6ff7e2a5":"markdown","18a7e84b":"markdown","d1a4c9e7":"markdown","57ea8867":"markdown","51ce4502":"markdown","9c6bbe70":"markdown"},"source":{"1814425e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nplt.style.use(\"seaborn-whitegrid\")       \nimport pandas_profiling as pp \n\nimport seaborn as sns\n\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","72de1239":"df =  pd.read_excel(\"\/kaggle\/input\/steelcomproper\/40192_2013_16_MOESM1_ESM.xlsx\")","4415e57a":"df.head()","d19b26c2":"df.shape","efa920ac":"df.info()","7aef39a5":"df.describe().T","1c71b137":"profile_report = pp.ProfileReport(df)","045d4296":"profile_report","9f740225":"df_1 = df[[\"C\", \"Si\", \"Mn\", \"P\", \"S\", \"Ni\", \"Cu\", \"Cr\", \"Mo\"]]\ndf_2 = df[['NT', 'THT', 'THt', 'THQCr', 'CT', 'Ct', 'DT', 'Dt', 'QmT',\n       'TT', 'Tt', 'TCr']]","07529f16":"sns.set(font_scale = 1)\ndf_2.hist(figsize=(10, 10));\nplt.suptitle(\"Heat Treatment Conditions - Temperature, Time and Other Process(normalizing,tempering etc.)\",\n             x=0.5, y=1.05, ha='center', fontsize='xx-large');\nplt.tight_layout();\n\n","c635482d":"df_1.plot(kind=\"density\", layout=(6,5),subplots=True,sharex=False, sharey=False, figsize=(15,15));\nplt.suptitle(\"Chemical Composition Distribution\",\n             x=0.5, y=1.05, ha='center', fontsize='xx-large');\nplt.tight_layout() ","93e21aa1":"from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\ndf.drop(columns=['Sl. No.'], inplace = True)\nX = df.drop([\"Fatigue\"], axis = 1)\ny = df[\"Fatigue\"]","9ea9aa1f":"from sklearn.linear_model import LinearRegression\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state= 42)\nlm = LinearRegression()\nlr_model = lm.fit(X_train, y_train)","407465b4":"#R^2 Score with Cross Validation\n\ncross_val_score1=cross_val_score(lr_model, X_train, y_train, cv=10, scoring='r2').mean() #verified score value for train model\nprint('Verified R2 value for Training model: ' + str(cross_val_score1))\n\ncross_val_score2=cross_val_score(lr_model, X_test, y_test, cv=10, scoring='r2').mean() #verified score value for test model\nprint('Verified R2 value for Testing Model: ' + str(cross_val_score2))","44c92623":"#Rmse Score Value with Cross Validation \n\n\ntrain_crs_val = np.sqrt(-cross_val_score(lr_model, \n                X_train, \n                y_train, \n                cv = 10, \n                scoring = \"neg_mean_squared_error\")).mean()\n\ntest_crs_val = np.sqrt(-cross_val_score(lr_model, \n                X_test, \n                y_test, \n                cv = 10, \n                scoring = \"neg_mean_squared_error\")).mean()\n\nprint(\"train cross val rmse:{}\\ntest cross val rmse:{}\".format(train_crs_val,test_crs_val))\n","84d76a24":"ax1 = sns.distplot(lr_model.predict(X_test), hist=False, color=\"r\", label=\"Predict Values\")\nsns.distplot(y_test, hist=False, color=\"b\", label=\"Actual Values\" , ax=ax1);","d0ec6531":"d = {'R^2 Score ': [0.93], 'Rmse Score': [0.39]}\nlr_data = pd.DataFrame(data=d)\nlr_data","cdcedea9":"from sklearn.neighbors import KNeighborsRegressor\nknn_model = KNeighborsRegressor().fit(X_train, y_train)\nprint(\"R^2 values:{}\".format(knn_model.score(X_test, y_test)))\ny_pred = knn_model.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"rmse: {}\".format(rmse))\n","12d50965":"# Model Tuning\n\nk = {'n_neighbors': np.arange(1,30,1)} \n\nknn_model = KNeighborsRegressor() \n\ncv_model = GridSearchCV(knn_model, k, cv = 10) \n\ncv_model.fit(X_train, y_train)","614b00dd":"cv_model.best_params_[\"n_neighbors\"]","f393c013":"knn_model_tuned = KNeighborsRegressor(n_neighbors = cv_model.best_params_[\"n_neighbors\"])\nknn_model_tuned.fit(X_train, y_train)","993726e3":"print(\"R^2 values:{}\".format(knn_model_tuned.score(X_test, y_test)))\ny_pred = knn_model_tuned.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"rmse: {}\".format(rmse))","883407c9":"ax1 = sns.distplot(knn_model_tuned.predict(X_test), hist=False, color=\"r\", label=\"Predict Values\")\nsns.distplot(y_test, hist=False, color=\"b\", label=\"Actual Values\" , ax=ax1);","4f632afb":"d = {'R^2 Score ': [0.91], 'Rmse Score': [0.57]}\nknn_data = pd.DataFrame(data=d)\nknn_data","e269dec2":"from sklearn.ensemble import GradientBoostingRegressor\ngbm_model = GradientBoostingRegressor()\ngbm_model.fit(X_train, y_train)\ny_pred = gbm_model.predict(X_test)\n\nprint(\"R^2 values:{}\".format(gbm_model.score(X_test, y_test)))\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"rmse: {}\".format(rmse))","424cab55":"#model tuning\n\ngbm_params = {\n    'learning_rate': [0.001, 0.01, 0.1, 0.2],\n    'max_depth': [3, 5, 8,50,100],\n    'n_estimators': [200, 500, 1000, 2000],\n    'subsample': [1,0.5,0.75],\n}\n\ngbm = GradientBoostingRegressor()\ngbm_cv_model = GridSearchCV(gbm, gbm_params, cv = 10, n_jobs = -1, verbose = 2)\ngbm_cv_model.fit(X_train, y_train)","b7b63111":"gbm_cv_model.best_params_","a8926d4d":"gbm_tuned = GradientBoostingRegressor(learning_rate = 0.2,  \n                                      max_depth = 3, \n                                      n_estimators = 200, \n                                      subsample = 0.75)","d6d49eee":"gbm_tuned = gbm_tuned.fit(X_train,y_train)\n\ny_pred = gbm_tuned.predict(X_test)\n\nprint(\"R^2 values:{}\".format(gbm_tuned.score(X_test, y_test)))\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"rmse: {}\".format(rmse))","1593324a":"Importance = pd.DataFrame({\"Importance\": gbm_tuned.feature_importances_*100},\n                         index = X_train.columns)\n\nImportance.sort_values(by = \"Importance\", \n                       axis = 0, \n                       ascending = True).plot(kind =\"barh\", color = \"r\");\n\nplt.xlabel(\"Variable Significance Levels\");","7c0dae15":"ax1 = sns.distplot(gbm_tuned.predict(X_test), hist=False, color=\"r\", label=\"Predict Values\")\nsns.distplot(y_test, hist=False, color=\"b\", label=\"Actual Values\" , ax=ax1);","8159879b":"d = {'R^2 Score ': [0.98], 'Rmse Score': [0.23]}\ngbm_data = pd.DataFrame(data=d)\ngbm_data","efce9132":"from lightgbm import LGBMRegressor\nlgbm = LGBMRegressor()\nlgbm_model = lgbm.fit(X_train, y_train)\n\ny_pred = lgbm_model.predict(X_test, \n                            num_iteration = lgbm_model.best_iteration_)\n\nprint(\"R^2 values:{}\".format(lgbm_model.score(X_test, y_test)))\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"rmse: {}\".format(rmse))","c418b20f":"lgbm_grid = {\n    'colsample_bytree': [0.4, 0.5,0.6,0.9,1],\n    'learning_rate': [0.01, 0.1, 0.5,1],\n    'n_estimators': [20, 40, 100, 200, 500,1000],\n    'max_depth': [1,2,3,4,5,6,7,8] }\n\nlgbm = LGBMRegressor()\nlgbm_cv_model = GridSearchCV(lgbm, lgbm_grid, cv=10, n_jobs = -1, verbose = 2)\nlgbm_cv_model.fit(X_train, y_train)","8dcfd745":"lgbm_cv_model.best_params_","f07f9c36":"lgbm_tuned = LGBMRegressor(learning_rate = 0.1, \n                           max_depth = 3, \n                           n_estimators = 1000,\n                          colsample_bytree = 0.5)","6592d2af":"lgbm_tuned = lgbm_tuned.fit(X_train,y_train)\ny_pred = lgbm_tuned.predict(X_test)\nprint(\"R^2 values:{}\".format(lgbm_tuned.score(X_test, y_test)))\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"rmse: {}\".format(rmse))","07d96158":"Importance = pd.DataFrame({\"Importance\": lgbm_tuned.feature_importances_*100},\n                         index = X_train.columns)\n\nImportance.sort_values(by = \"Importance\", \n                       axis = 0, \n                       ascending = True).plot(kind =\"barh\", color = \"r\");\n\nplt.xlabel(\"Variable Significance Levels\");","d6d7779b":"ax1 = sns.distplot(lgbm_tuned.predict(X_test), hist=False, color=\"r\", label=\"Predict Values\")\nsns.distplot(y_test, hist=False, color=\"b\", label=\"Actual Values\" , ax=ax1);","84d8d57b":"d = {'R^2 Score ': [0.99], 'Rmse Score': [0.18]}\nlgbm_data = pd.DataFrame(data=d)\nlgbm_data","64de9b51":"from catboost import CatBoostRegressor\n\ncatb = CatBoostRegressor()\ncatb_model = catb.fit(X_train, y_train)\ny_pred = catb_model.predict(X_test)\nprint(\"R^2 values:{}\".format(catb_model.score(X_test, y_test)))\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"rmse: {}\".format(rmse))","3252ae24":"#Model Tuning\n\ncatb_grid = {\n    'iterations': [200,500,1000,2000],\n    'learning_rate': [0.01, 0.03, 0.05, 0.1],\n    'depth': [3,4,5,6,7,8] }\n\ncatb = CatBoostRegressor()\ncatb_cv_model = GridSearchCV(catb, catb_grid, cv=5, n_jobs = -1, verbose = 2)\n\ncatb_cv_model.fit(X_train, y_train)","dd298b30":"catb_cv_model.best_params_","31eea2c2":"catb_tuned = CatBoostRegressor(iterations = 1000, \n                               learning_rate = 0.05, \n                               depth = 4)","5a4721ff":"catb_tuned = catb_tuned.fit(X_train,y_train)\ny_pred = catb_tuned.predict(X_test)\nprint(\"R^2 values:{}\".format(catb_tuned.score(X_test, y_test)))\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nprint(\"rmse: {}\".format(rmse))","96a3afd6":"Importance = pd.DataFrame({\"Importance\": catb_tuned.feature_importances_*100},\n                         index = X_train.columns)\n\nImportance.sort_values(by = \"Importance\", \n                       axis = 0, \n                       ascending = True).plot(kind =\"barh\", color = \"r\");\n\nplt.xlabel(\"Variable Significance Levels\");","ddfd5408":"ax1 = sns.distplot(catb_tuned.predict(X_test), hist=False, color=\"r\", label=\"Predict Values\")\nsns.distplot(y_test, hist=False, color=\"b\", label=\"Actual Values\" , ax=ax1);","7cce0cc4":"d = {'R^2 Score ': [0.99], 'Rmse Score': [0.18]}\ncatb_data = pd.DataFrame(data=d)\ncatb_data","217bbcd0":"models = [\n    \n    lr_model,\n    knn_model_tuned,\n    gbm_tuned,\n    lgbm_tuned,\n    catb_tuned,    \n\n\n]\n\n\nfor model in models:\n    name = model.__class__.__name__\n    y_pred = model.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    print(\"-\"*28)\n    print(name + \":\" )\n    print(\"Rmse: {:.4%}\".format(rmse))","faa560db":"result = []\n\nresults = pd.DataFrame(columns= [\"Models\",\"Rmse\"])\n\nfor model in models:\n    name = model.__class__.__name__\n    y_pred = model.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))    \n    result = pd.DataFrame([[name, rmse]], columns= [\"Models\",\"Rmse\"])\n    results = results.append(result)\n    \n    \nsns.barplot(x= 'Rmse', y = 'Models', data=results, color=\"r\")\nplt.xlabel('Rmse Value ')\nplt.title('Rmse of models'); ","5b901542":"models = [\n    lr_model,\n    knn_model_tuned,\n    gbm_tuned,\n    lgbm_tuned,\n    catb_tuned,\n    \n]\n\n\nfor model in models:\n    name = model.__class__.__name__\n    r2 = model.score(X_test, y_test)\n    print(\"-\"*28)\n    print(name + \":\" )\n    print(\"R^2 Score: {:.4%}\".format(r2))","63e8883f":"result = []\n\nresults = pd.DataFrame(columns= [\"Models\",\"R^2 Score\"])\n\nfor model in models:\n    name = model.__class__.__name__\n    r2 = model.score(X_test, y_test)    \n    result = pd.DataFrame([[name, r2]], columns= [\"Models\",\"R^2 Score\"])\n    results = results.append(result)\n    \n    \nsns.barplot(x= 'R^2 Score', y = 'Models', data=results, color=\"r\")\nplt.xlabel('R^2 Score')\nplt.title('R^2 Score of models'); ","c00bce02":"# 1. Load and Check Data","7426b16e":"## 4.2. R^2 Score Comparison","908bbd08":"* Best result is CatBoost model. ","d5de002c":"## 3.3. GBM","28264b94":"## 3.4.Light GBM","d86a44b6":"* After model tuning, R^2 score is increase, rmse score is decrease.","61612995":"## 3.1. Linear Regression","b92be7d4":"# Introduction\n\nThe mechanical properties of a material affect how it behaves as it is loaded. The elastic modulus of the material affects how much it deflects under a load, and the strength of the material determines the stresses that it can withstand before it fails. The ductility of a material also plays a significant role in determining when a material will break as it is loaded beyond its elastic limit. Because every mechanical system is subjected to loads during operation, it is important to understand how the materials that make up those mechanical systems behave.[1] There are a lot of parameters to affect mechanical properties such as microstrucre, heat treatment, process method, composition etc. \n\nComposition and heat treatment are most important parameters. They give information about mechanical properties of materials in many engineering applications. Many tests are carried out to examine the mechanical effects of these properties. Sometimes these tests are disadvantageous in terms of time and money. Machine learning methods have proven to be successful in the prediction of a large number of material properties.[2] MI studies have established the unequivocal potential of this emerging discipline in accelerating discovery and design of new\/improved materials. However, there still does not exist a standardized set of protocols for exploring this approach in a systematic manner on many potential applications, and thus, establishing the composition-processing-structure-property relationships still remains an arduous task.[3] \n\n#### DataSet[3]\n\nFatigue Dataset for Steel from National Institute of Material Science (NIMS) MatNavi was used in this work, which is one of the largest databases in the world with details on composition, mill product (upstream) features and subsequent processing (heat treatment) parameters. The database comprises carbon and low-alloy steels, carburizing steels and spring steels. Fatigue life data, which pertain to rotating bending fatigue tests at room temperature conditions, was the target property for which we aimed to construct predictive models in the current study. The features in the dataset can be categorized into the following:[3]\n\n*  Chemical composition - %C, %Si, %Mn, %P, %S, %Ni, %Cr, %Cu, %Mo (all in wt. %)\n*  Upstream processing details - ingot size, reduction ratio, non-metallic inclusionsAgrawal et al. \n*  Heat treatment conditions - temperature, time and other process conditions for normalizing, through-hardening, carburizing-quenching and tempering processes\n*  Mechanical properties - YS, UTS, %EL, %RA, hardness, Charpy impact value (J\/cm2), fatigue strength.[3]\n\nThe data used in this work has 437 instances\/rows, **25 features\/columns (composition and processing parameters)**, and **1 target property (fatigue strength).** The 437 data instances include 371 carbon and low alloy steels, 48 carburizing steels, and 18 spring steels. This data pertains to various heats of each grade of steel and different processing conditions.[3] \n\nIf you are interested in this topic, you can review [this article.](https:\/\/www.researchgate.net\/publication\/264222603_Exploration_of_data_science_techniques_to_predict_fatigue_strength_of_steel_from_composition_and_processing_parameters) . From this study I used the dataset(Fatigue Dataset for Steel from NIMS) used in this article. Since ML applications are new in materials science, it is very difficult to find datasets for different materials. You can either prepare it yourself (with Handbook data) or buy it. \n\n\n\n#### References\n1. https:\/\/mechanicalc.com\/reference\/mechanical-properties-of-materials\n2. https:\/\/www.nature.com\/articles\/s41524-019-0221-0#Sec9\n3. https:\/\/www.researchgate.net\/publication\/264222603_Exploration_of_data_science_techniques_to_predict_fatigue_strength_of_steel_from_composition_and_processing_parameters","3871d46d":"* Linear Regression\n* KNN\n* GBM\n* LightGBM\n* CatBoost\n\nCrossValidation was applied to all models and best parameters were determined with model tuning.","2c204ac0":"## 4.1. RMSE Value Comparison","cbc2840f":"* The database comprises carbon and low-alloy steels, carburizing steels and spring steels. You can see Cr-Mo steels and Cr-Mo-Ni Steels. You can see  below alloy composition(weight (%)) in Cr-Mo Steels.\n\n<table class=\"wikitable\">\n<caption>Alloy composition by weight (%)<sup id=\"cite_ref-Catalog_2-0\" class=\"reference\"><a href=\"#cite_note-Catalog-2\">[2]<\/a><\/sup>\n<\/caption>\n<tbody><tr>\n<th><a href=\"\/wiki\/SAE_steel_grades\" title=\"SAE steel grades\">SAE grade<\/a>\n<\/th>\n<th><a href=\"\/wiki\/Chromium\" title=\"Chromium\">Cr<\/a>\n<\/th>\n<th><a href=\"\/wiki\/Molybdenum\" title=\"Molybdenum\">Mo<\/a>\n<\/th>\n<th><a href=\"\/wiki\/Carbon\" title=\"Carbon\">C<\/a><sup id=\"cite_ref-3\" class=\"reference\"><a href=\"#cite_note-3\">[a]<\/a><\/sup>\n<\/th>\n<th><a href=\"\/wiki\/Manganese\" title=\"Manganese\">Mn<\/a>\n<\/th>\n<th><a href=\"\/wiki\/Phosphorus\" title=\"Phosphorus\">P<\/a>, <abbr title=\"maximum\">max.<\/abbr>\n<\/th>\n<th><a href=\"\/wiki\/Sulphur\" class=\"mw-redirect\" title=\"Sulphur\">S<\/a>, <abbr title=\"maximum\">max.<\/abbr>\n<\/th>\n<th><a href=\"\/wiki\/Silicon\" title=\"Silicon\">Si<\/a>\n<\/th><\/tr>\n<tr>\n<td>4118<\/td>\n<td>0.40\u20130.60<\/td>\n<td>0.08\u20130.15<\/td>\n<td>0.18\u20130.23<\/td>\n<td>0.70\u20130.90<\/td>\n<td>0.035<\/td>\n<td>0.040<\/td>\n<td>0.15\u20130.35\n<\/td><\/tr>\n<tr>\n<td>4120<\/td>\n<td>0.40\u20130.60<\/td>\n<td>0.13\u20130.20<\/td>\n<td>0.18\u20130.23<\/td>\n<td>0.90\u20131.20<\/td>\n<td>0.035<\/td>\n<td>0.040<\/td>\n<td>0.15\u20130.35\n<\/td><\/tr>\n<tr>\n<td>4121<\/td>\n<td>0.45\u20130.65<\/td>\n<td>0.20\u20130.30<\/td>\n<td>0.18\u20130.23<\/td>\n<td>0.75\u20131.00<\/td>\n<td>0.035<\/td>\n<td>0.040<\/td>\n<td>0.15\u20130.35\n<\/td><\/tr>\n<tr>\n<td>4130<\/td>\n<td>0.80\u20131.10<\/td>\n<td>0.15\u20130.25<\/td>\n<td>0.28\u20130.33<\/td>\n<td>0.40\u20130.60<\/td>\n<td>0.035<\/td>\n<td>0.040<\/td>\n<td>0.15\u20130.35\n<\/td><\/tr>\n<tr>\n<td>4135<\/td>\n<td>0.80\u20131.10<\/td>\n<td>0.15\u20130.25<\/td>\n<td>0.33\u20130.38<\/td>\n<td>0.70\u20130.90<\/td>\n<td>0.035<\/td>\n<td>0.040<\/td>\n<td>0.15\u20130.35\n<\/td><\/tr>\n<tr>\n<td>4137<\/td>\n<td>0.80\u20131.10<\/td>\n<td>0.15\u20130.25<\/td>\n<td>0.35\u20130.40<\/td>\n<td>0.70\u20130.90<\/td>\n<td>0.035<\/td>\n<td>0.040<\/td>\n<td>0.15\u20130.35\n<\/td><\/tr>\n<tr>\n<td>4140<\/td>\n<td>0.80\u20131.10<\/td>\n<td>0.15\u20130.25<\/td>\n<td>0.38\u20130.43<\/td>\n<td>0.75\u20131.00<\/td>\n<td>0.035<\/td>\n<td>0.040<\/td>\n<td>0.15\u20130.35\n<\/td><\/tr>\n<tr>\n<td>4142<\/td>\n<td>0.80\u20131.10<\/td>\n<td>0.15\u20130.25<\/td>\n<td>0.40\u20130.45<\/td>\n<td>0.75\u20131.00<\/td>\n<td>0.035<\/td>\n<td>0.040<\/td>\n<td>0.15\u20130.35\n<\/td><\/tr>\n<tr>\n<td>4145<\/td>\n<td>0.80\u20131.10<\/td>\n<td>0.15\u20130.25<\/td>\n<td>0.43\u20130.48<\/td>\n<td>0.75\u20131.00<\/td>\n<td>0.035<\/td>\n<td>0.040<\/td>\n<td>0.15\u20130.35\n<\/td><\/tr>\n<tr>\n<td>4147<\/td>\n<td>0.80\u20131.10<\/td>\n<td>0.15\u20130.25<\/td>\n<td>0.45\u20130.50<\/td>\n<td>0.75\u20131.00<\/td>\n<td>0.035<\/td>\n<td>0.040<\/td>\n<td>0.15\u20130.35\n<\/td><\/tr>\n<tr>\n<td>4150<\/td>\n<td>0.80\u20131.10<\/td>\n<td>0.15\u20130.25<\/td>\n<td>0.48\u20130.53<\/td>\n<td>0.75\u20131.00<\/td>\n<td>0.035<\/td>\n<td>0.040<\/td>\n<td>0.15\u20130.35\n<\/td><\/tr>\n<tr>\n<td>4161<\/td>\n<td>0.70\u20130.90<\/td>\n<td>0.25\u20130.35<\/td>\n<td>0.56\u20130.64<\/td>\n<td>0.75\u20131.00<\/td>\n<td>0.035<\/td>\n<td>0.040<\/td>\n<td>0.15\u20130.35\n<\/td><\/tr>\n<tr>\n<td colspan=\"8\"><div class=\"reflist\" style=\"list-style-type: lower-alpha;\">\n<div class=\"mw-references-wrap\"><ol class=\"references\">\n<li id=\"cite_note-3\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-3\" aria-label=\"Jump up\" title=\"Jump up\">^<\/a><\/b><\/span> <span class=\"reference-text\">The carbon composition of the alloy is denoted by the last two digits of the SAE specification number, in hundredths of a percent.<\/span>\n<\/li>\n<\/ol><\/div><\/div>\n<\/td><\/tr><\/tbody><\/table>","6ff7e2a5":"## 3.5. CatBoost","18a7e84b":"## Variable Description\n\n* C % - Carbon\n* Si % - Silicon\n* Mn % - Manganese\n* P % - Phosphorus\n* S % - Sulphur\n* Ni % - Nickel\n* Cr % - Chromium\n* Cu % - Copper\n* Mo % - Molybdenum\n* NT - Normalizing Temperature\n* THT - Through Hardening Temperature\n* THt - Through Hardening Time\n* THQCr - Cooling Rate for Through Hardening\n* CT - Carburization Temperature\n* Ct - Carburization Time\n* DT - Diffusion Temperature\n* Dt - Diffusion time\n* QmT - Quenching Media Temperature (for Carburization)\n* TT - Tempering Temperature\n* Tt - Tempering Time\n* TCr - Cooling Rate for Tempering\n* RedRatio - Reduction Ratio (Ingot to Bar)\n* dA  - Area Proportion of Inclusions Deformed by Plastic Work\n* dB  -Area Proportion of Inclusions Occurring in Discontinuous Array\n* dC  -Area Proportion of Isolated Inclusions\n* Fatigue  - Rotating Bending Fatigue Strength (107 Cycles)\n\n\n","d1a4c9e7":"# 4.Model Comparison","57ea8867":"# 3. Modelling","51ce4502":"## 3.2. KNN","9c6bbe70":"# 2.Visualization"}}