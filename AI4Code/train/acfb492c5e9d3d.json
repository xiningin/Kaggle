{"cell_type":{"da8749f5":"code","3691da32":"code","786883ed":"code","9e684b82":"code","c6e557f6":"code","328b60f8":"code","56fc5670":"code","143d9ce3":"code","bf01043e":"code","86338db0":"code","6164fb94":"code","705d9052":"code","fe304515":"code","5f705dc5":"code","717d065e":"code","976d0029":"code","7a463d19":"code","b6993860":"code","1e7de78b":"code","6004dec6":"code","08c4d25e":"markdown","d0a4f11e":"markdown","81b8a25c":"markdown","7c8b0e8f":"markdown","291b88db":"markdown","2efa50d7":"markdown","da05a100":"markdown","485fcd10":"markdown","9bd96725":"markdown","294056a7":"markdown","5f5d68ee":"markdown","8529f27f":"markdown","a88dfb6b":"markdown","7fa22ed5":"markdown","c4a27e9a":"markdown","dbd068e2":"markdown"},"source":{"da8749f5":"import numpy as np\nimport pandas as pd\n\nseed = 66\nnp.random.seed(seed)\nimport cv2\nimport json\nimport glob\nimport os\nfrom tqdm import *\nfrom shutil  import copyfile, rmtree \n\n\nfrom matplotlib import pyplot as plt\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dropout, Flatten, Dense, SpatialDropout2D, Input\nfrom keras import applications\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import load_img\n\nfrom keras import backend as K\nprint (K.image_dim_ordering())\nprint (K.image_data_format())\nfrom tqdm import tqdm_notebook, tqdm","3691da32":"TRAIN_PATH = '..\/input\/humpback-whale-identification\/train\/'\nTEST_PATH = '..\/input\/humpback-whale-identification\/test\/'\nMASK_PATH = '..\/input\/whales-masks-500\/masks\/'","786883ed":"mask_files = os.listdir(MASK_PATH)\nmask_files = [m for m in mask_files if 'mask' in m]\nX = []\nM = []\nfor mask_file in mask_files:\n    img_file = mask_file.split('_')[0] + '.jpg'\n    img = cv2.imread(TRAIN_PATH + img_file)\n    mask = cv2.imread(MASK_PATH + mask_file, 0)\n    mask[mask>108]= 255\n    mask[mask<=108]= 0\n    X.append(img)\n    M.append(mask)\n  \nX = np.array(X)\nM = np.array(M)\n","9e684b82":"!pip install segmentation_models","c6e557f6":"!pip install albumentations","328b60f8":"from albumentations import (\n    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine,\n    IAASharpen, IAAEmboss, RandomContrast, RandomBrightness, Flip, OneOf, Compose, RandomGamma, Rotate,IAAAffine\n)","56fc5670":"aug_null = Compose([])\naug = Compose([ \n    Blur(p=0.5, blur_limit=2),\n    IAAAffine(p=0.5, shear=5),\n    HorizontalFlip(p=0.5),              \n    #VerticalFlip(p=0.5),              \n    Rotate(limit=5, p=0.3),\n    #CLAHE(p=0.3),\n    RandomContrast(p=0.2, limit=0.1),\n    RandomBrightness(p=0.2, limit=0.1),\n    #RandomGamma(p=0.2, gamma_limit=(90, 110))\n])\n","143d9ce3":"import keras\nfrom segmentation_models.backbones.classification_models.classification_models.resnet import preprocess_input\n\nclass DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, X, M, batch_size=32,\n                 dim=(299,299),  shuffle=True, \n                 preprocess_input=preprocess_input, \n                 aug=aug_null, min_mask=2 ):\n        'Initialization'\n        self.X = X\n        self.M = M\n        self.batch_size = batch_size\n        self.n_classes = 1\n        self.shuffle = shuffle\n        self.preprocess_input = preprocess_input\n        self.aug = aug\n        self.on_epoch_end()\n        self.dim = dim\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor((len(self.X) \/ self.batch_size) \/ 1) )\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        \n        end_index = min((index+1)*self.batch_size, len(self.indexes))\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n\n        # Generate data\n        X, Y = self.__data_generation(indexes)\n\n        return X, Y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.X))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        \n        batch_size = len(indexes)\n        \n        # Initialization\n        XX = np.empty((batch_size, self.dim[0], self.dim[1], 3), dtype='float32')\n        YY = np.empty((batch_size, self.dim[0], self.dim[1], 1), dtype='float32')\n\n        # Generate data\n        for i, ID in enumerate(indexes):\n            # Store sample\n            img = self.X[ID]\n            if img.shape[0] != self.dim[0]:\n                img = cv2.resize(img, self.dim, cv2.INTER_CUBIC)\n            mask = self.M[ID]\n            if mask.shape[0] != self.dim[0]:\n                mask = cv2.resize(mask, self.dim, cv2.INTER_AREA)\n            \n            # Store class\n            augmented = self.aug(image=img, mask=mask)\n            aug_img = augmented['image']\n            aug_mask = augmented['mask']\n            aug_mask = np.expand_dims(aug_mask, axis=-1)\n            aug_mask = aug_mask\/255\n            \n            assert (np.max(aug_mask)<= 1.0 and  np.min(aug_mask) >= 0)\n            aug_mask[aug_mask>0.5] = 1\n            aug_mask[aug_mask<0.5] = 0\n            \n            YY[i,] = aug_mask.astype('float32')\n            XX[i,] = aug_img.astype('float32')\n    \n       \n        XX = self.preprocess_input(XX)\n\n        return XX, YY","bf01043e":"from segmentation_models.backbones import get_preprocessing\npreprocess_input = get_preprocessing('resnet34')\n\ntraining_generator = DataGenerator(X[:400], M[:400], batch_size=16,  dim=(384, 384), aug=aug, \n                                   preprocess_input=preprocess_input)\nvalid_genarator = DataGenerator(X[400:], M[400:], batch_size=16, dim=(384, 384), aug=aug_null, \n                                preprocess_input=preprocess_input, shuffle=False)","86338db0":"x, y= training_generator[7]\nnp.max(x), x.shape, y.shape, np.max(y), np.unique(y)","6164fb94":"plt.imshow(y[9, ..., 0])\nplt.show()","705d9052":"import keras.backend as K\nfrom keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nfrom keras.optimizers import Adam\nfrom segmentation_models import Unet, FPN\n\nmodel = FPN(backbone_name='resnet34', encoder_weights='imagenet', activation='sigmoid', classes=1, dropout=0.5)\n","fe304515":"from keras.losses import binary_crossentropy\nimport keras.backend as K\nimport tensorflow as tf\n\n\ndef dice_coeff_L(y_true, y_pred):\n    smooth = 1.\n    y_pred_sig = tf.nn.sigmoid(y_pred)\n\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred_sig)\n    intersection = K.sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return score\n\ndef dice_coeff(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    score = (2. * intersection + smooth) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return score\n\n\ndef dice_loss(y_true, y_pred):\n    loss = 1 - dice_coeff(y_true, y_pred)\n    return loss\n\n\ndef bce_dice_loss(y_true, y_pred):\n    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n    return loss","5f705dc5":"from keras.callbacks import ReduceLROnPlateau\nfrom keras.optimizers import SGD\n\ntraining_generator = DataGenerator(X[:400], M[:400], batch_size=16,  dim=(384, 384), aug=aug, \n                                   preprocess_input=preprocess_input)\nvalid_genarator = DataGenerator(X[400:], M[400:], batch_size=16, dim=(384, 384), aug=aug_null, \n                                preprocess_input=preprocess_input, shuffle=False)\n\nmodel.compile(optimizer=Adam(lr=0.001),\n          loss=bce_dice_loss,\n          metrics=[dice_coeff])\n\nepochs = 40\n\nearly_stopping = EarlyStopping(patience=10, verbose=1, monitor='val_dice_coeff', mode='max')\nmodel_checkpoint = ModelCheckpoint(\"fpnresnet34_384_{epoch:02d}-{val_loss:.3f}-{val_dice_coeff:.3f}.hdf5\", \n                                   save_best_only=True, \n                                   save_weights_only=True, \n                                   monitor='val_dice_coeff', verbose=1, mode='max', period=2)\nreduce_lr = ReduceLROnPlateau(factor=0.5, patience=5, min_lr=0.000001, verbose=1, monitor='val_dice_coeff', mode='max')\n\n \nhistory = model.fit_generator( training_generator,\n                                     validation_data=valid_genarator,\n                                     epochs=epochs,\n                                     callbacks=[ reduce_lr, early_stopping, model_checkpoint], \n                                     verbose=1)","717d065e":"aug_null = Compose([])\n\n\nclass TestDataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, X, batch_size=32,\n                 dim=(299,299),  shuffle=True, \n                 preprocess_input=preprocess_input, \n                 aug=aug_null, min_mask=2 ):\n        'Initialization'\n        self.X = X\n        self.batch_size = batch_size\n        self.n_classes = 1\n        self.shuffle = shuffle\n        self.preprocess_input = preprocess_input\n        self.aug = aug\n        self.dim = dim\n        self.on_epoch_end()\n\n    def set_aug(self, aug):\n        self.aug = aug\n        self.on_epoch_end()\n      \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor((len(self.X) \/ self.batch_size) \/ 1) )\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        \n        end_index = min((index+1)*self.batch_size, len(self.indexes))\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n\n        # Generate data\n        xx = self.__data_generation(indexes)\n\n        return xx\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.X))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, indexes):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        \n        batch_size = len(indexes)\n        \n        # Initialization\n        XX = np.empty((batch_size, self.dim[0], self.dim[1], 3), dtype='float32')\n\n        # Generate data\n        for i, ID in enumerate(indexes):\n            # Store sample\n            img = self.X[ID]\n            if img.shape[0] != self.dim[0]:\n                img = cv2.resize(img, self.dim, cv2.INTER_CUBIC)\n            \n            \n            # Store class\n            augmented = self.aug(image=img)\n            aug_img = augmented['image']\n           \n            XX[i,] = aug_img.astype('float32')\n    \n       \n        XX = self.preprocess_input(XX)\n\n        return XX","976d0029":"null_aug = Compose([])\nvalid_genarator = DataGenerator(X[400:], M[400:], batch_size=16, aug=null_aug, preprocess_input=preprocess_input, dim=(384, 384), \n                               shuffle=False)\npreds = model.predict_generator(valid_genarator, verbose=1)\n\nflip_aug = Compose([HorizontalFlip(p=1.0) ])\nvalid_genarator = DataGenerator(X[400:], M[400:], batch_size=16, aug=flip_aug, preprocess_input=preprocess_input,  dim=(384, 384), shuffle=False)\npreds_hflip = model.predict_generator(valid_genarator, verbose=1)\n\nblur_aug = Compose([Blur(p=1.0)])\nvalid_genarator = DataGenerator(X[400:], M[400:], batch_size=16, aug=blur_aug, preprocess_input=preprocess_input,  dim=(384, 384), shuffle=False)\npreds_blur = model.predict_generator(valid_genarator, verbose=1)\n\nTARGET_VAL = []\nfor i in range(len(preds)):\n  pp = (preds[i] + np.fliplr(preds_hflip[i]) + preds_blur[i])\/3\n  TARGET_VAL.append(pp)\n\nTARGET_VAL = np.array(TARGET_VAL)  ","7a463d19":"SIZE = 384\nf, axarr = plt.subplots(6, 6)\nf.set_figwidth(20)\nf.set_figheight(15)\nkernel = np.ones((3,3),np.uint8)\n\nfor i in range(0, 36):\n    img =  cv2.resize(X[400+i], (384, 384))\n    mask = ((TARGET_VAL[i, ..., 0]) > 0.25).astype('uint8')\n    back = ((TARGET_VAL[i, ..., 0]) <= 0.25).astype('uint8')\n\n    img = np.stack([img[..., j] * mask + back*255 for j in range(3)], axis=-1)\n\n    contours,hierarchy = cv2.findContours(mask, 1, 2)\n  # Cycle through contours and add area to array\n    areas = []\n    for c in contours:\n        areas.append(cv2.contourArea(c))\n\n    # Sort array of areas by size\n    sorted_areas = sorted(zip(areas, contours), key=lambda x: x[0], reverse=True)\n    title = str(len(sorted_areas)) \n    \n    cnt = sorted_areas[0][1]\n    x1,y1,w,h = cv2.boundingRect(cnt)\n    x2 = x1 + w\n    y2 = y1 + h\n    \n    for j in range(1, len(sorted_areas)):\n        cnt = sorted_areas[j][1]\n        tx1,ty1,tw,th = cv2.boundingRect(cnt)\n        tx2 = tx1 + tw\n        ty2 = ty1 + th\n        x1 = min(x1, tx1)\n        y1 = min(y1, ty1)\n        x2 = max(x2, tx2)\n        y2 = max(y2, ty2)\n    \n    x = x1\n    y = y1\n    w = x2-x1\n    h = y2-y1\n\n\n    img_cropped = img[y:y+h, x:x+w]\n    axarr[int(i\/6), i%6].imshow(img_cropped, cmap='gray')\n    axarr[int(i\/6), i%6].axis('off')\nplt.show()","b6993860":"SIZE = 384\nf, axarr = plt.subplots(6, 6)\nf.set_figwidth(20)\nf.set_figheight(15)\nkernel = np.ones((3,3),np.uint8)\n\nfor i in range(0, 36):\n    img =  cv2.resize(X[400+i], (384, 384))\n    mask = ((TARGET_VAL[i, ..., 0]) > 0.25).astype('uint8')\n\n    img = np.stack([img[..., j] * mask  for j in range(3)], axis=-1)\n\n    contours,hierarchy = cv2.findContours(mask, 1, 2)\n  # Cycle through contours and add area to array\n    areas = []\n    for c in contours:\n        areas.append(cv2.contourArea(c))\n\n    # Sort array of areas by size\n    sorted_areas = sorted(zip(areas, contours), key=lambda x: x[0], reverse=True)\n    title = str(len(sorted_areas)) \n    \n    cnt = sorted_areas[0][1]\n    x1,y1,w,h = cv2.boundingRect(cnt)\n    x2 = x1 + w\n    y2 = y1 + h\n    \n    for j in range(1, len(sorted_areas)):\n        cnt = sorted_areas[j][1]\n        tx1,ty1,tw,th = cv2.boundingRect(cnt)\n        tx2 = tx1 + tw\n        ty2 = ty1 + th\n        x1 = min(x1, tx1)\n        y1 = min(y1, ty1)\n        x2 = max(x2, tx2)\n        y2 = max(y2, ty2)\n    \n    x = x1\n    y = y1\n    w = x2-x1\n    h = y2-y1\n\n\n    img_cropped = img[y:y+h, x:x+w]\n    axarr[int(i\/6), i%6].imshow(img_cropped, cmap='gray')\n    axarr[int(i\/6), i%6].axis('off')\nplt.show()","1e7de78b":"SIZE = 384\nf, axarr = plt.subplots(6, 6)\nf.set_figwidth(20)\nf.set_figheight(15)\nkernel = np.ones((3,3),np.uint8)\n\nfor i in range(0, 36):\n    img =  cv2.resize(X[400+i], (384, 384))\n    mask = ((TARGET_VAL[i, ..., 0]) > 0.25).astype('uint8')\n\n    img = np.stack([img[..., j]  for j in range(3)], axis=-1)\n\n    contours,hierarchy = cv2.findContours(mask, 1, 2)\n  # Cycle through contours and add area to array\n    areas = []\n    for c in contours:\n        areas.append(cv2.contourArea(c))\n\n    # Sort array of areas by size\n    sorted_areas = sorted(zip(areas, contours), key=lambda x: x[0], reverse=True)\n    title = str(len(sorted_areas)) \n    \n    cnt = sorted_areas[0][1]\n    x1,y1,w,h = cv2.boundingRect(cnt)\n    x2 = x1 + w\n    y2 = y1 + h\n    \n    for j in range(1, len(sorted_areas)):\n        cnt = sorted_areas[j][1]\n        tx1,ty1,tw,th = cv2.boundingRect(cnt)\n        tx2 = tx1 + tw\n        ty2 = ty1 + th\n        x1 = min(x1, tx1)\n        y1 = min(y1, ty1)\n        x2 = max(x2, tx2)\n        y2 = max(y2, ty2)\n    \n    x = x1\n    y = y1\n    w = x2-x1\n    h = y2-y1\n\n\n    img_cropped = img[y:y+h, x:x+w]\n    axarr[int(i\/6), i%6].imshow(img_cropped, cmap='gray')\n    axarr[int(i\/6), i%6].axis('off')\nplt.show()","6004dec6":"SIZE = 384\nf, axarr = plt.subplots(6, 6)\nf.set_figwidth(20)\nf.set_figheight(15)\nkernel = np.ones((3,3),np.uint8)\n\nfor i in range(0, 36):\n    img =  cv2.resize(X[400+i], (384, 384))\n    img =  cv2.cvtColor(img, cv2.cv2.COLOR_BGR2GRAY)\n    mask = ((TARGET_VAL[i, ..., 0]) > 0.25).astype('uint8')\n    back = ((TARGET_VAL[i, ..., 0]) <= 0.25).astype('uint8')\n    img = img * mask + back*255\n\n    contours,hierarchy = cv2.findContours(mask, 1, 2)\n  # Cycle through contours and add area to array\n    areas = []\n    for c in contours:\n        areas.append(cv2.contourArea(c))\n\n    # Sort array of areas by size\n    sorted_areas = sorted(zip(areas, contours), key=lambda x: x[0], reverse=True)\n    title = str(len(sorted_areas)) \n    \n    cnt = sorted_areas[0][1]\n    x1,y1,w,h = cv2.boundingRect(cnt)\n    x2 = x1 + w\n    y2 = y1 + h\n    \n    for j in range(1, len(sorted_areas)):\n        cnt = sorted_areas[j][1]\n        tx1,ty1,tw,th = cv2.boundingRect(cnt)\n        tx2 = tx1 + tw\n        ty2 = ty1 + th\n        x1 = min(x1, tx1)\n        y1 = min(y1, ty1)\n        x2 = max(x2, tx2)\n        y2 = max(y2, ty2)\n    \n    x = x1\n    y = y1\n    w = x2-x1\n    h = y2-y1\n\n\n    img_cropped = img[y:y+h, x:x+w]\n    axarr[int(i\/6), i%6].imshow(img_cropped, cmap='gray')\n    axarr[int(i\/6), i%6].axis('off')\nplt.show()","08c4d25e":"Now let's install: [segmentation_models](http:\/\/https:\/\/github.com\/qubvel\/segmentation_models)","d0a4f11e":"We define the default preprocessing for resnet architectures and create train and validation generators (`keras.utils.Sequence`) ","81b8a25c":"In this kernel we will train a segmention model using 450 fluke masks provided by [Dene](http:\/\/https:\/\/www.kaggle.com\/dene33). I was a little bit skeptical about segmenting flukes mainly because you loose the **sea** information. If there was  some correlation between the sea waves and the classes of whales then segmentation would rule this out. However, I gave it a try (don't tested it yet in my models). \n\nI will use models from the great repo [segmentation_models](http:\/\/https:\/\/github.com\/qubvel\/segmentation_models) (thank you [Pavel](https:\/\/www.kaggle.com\/pavel92) for the great work). The whole task  is a straightforward single class segmentation and hopefully 450 fluke  masks would be enought  to generalize to the whole train and test set.","7c8b0e8f":"Extending keras.utils.Sequence structure can be really handy. It provides multithreaded access and ensures each input image will be considered exactly one time per epoch.","291b88db":"The nice side effect from segmenting the whale flukes is that we can get tight bounding boxes as well. In the figure bellow we keep a tight bounding box and only pixels that correspond to the mask. There are several ways to move from here:\n### 1. Draw colored flukes on white background","2efa50d7":"The model is trained... Let us see some validation cases. Using the handy generator with `shuffle=False` we can easily perform a simple test time augmentattion.","da05a100":"This is not good software engineering, but  I was in a hurry so I copied\/modified `DataGenerator` to create the `TestDataGenerator` class that also extends `keras.utils.Sequence` but ignores masks. This will be used for inference only. ","485fcd10":"### 4. Draw grayscale flukes in white background","9bd96725":"I calculate the bouning box of the mask using `cv2.findContours`. In case we have multiple disconnected components in the same mask, the bounding box is calculated using the minimum `x` and `y` and the maximum `x` and `y` of all components, hence the loop. This is the meaning of:\n```\n    x1 = min(x1, tx1)\n    y1 = min(y1, ty1)\n    x2 = max(x2, tx2)\n    y2 = max(y2, ty2)\n   ```\n   within the loop.","294056a7":"### 3. Draw colored flukes and keep the sea (tight bounding box)","5f5d68ee":"Trivial segmentation stuff. Dice coefficient and binary cross entropy loss united.","8529f27f":"Another great package for fast image augmentation is  [albumentations](http:\/\/https:\/\/github.com\/albu\/albumentations).","a88dfb6b":"### 2. Draw colored flukes on black background","7fa22ed5":"Let us parse the 450  intial masks (courtesy of **Dene**)","c4a27e9a":"Let's train the model to see what we can get.","dbd068e2":"A small explanation about this part of code\n   ```\n   contours,hierarchy = cv2.findContours(mask, 1, 2)\n    areas = []\n    for c in contours:\n        areas.append(cv2.contourArea(c))\n\n    # Sort array of areas by size\n    sorted_areas = sorted(zip(areas, contours), key=lambda x: x[0], reverse=True)\n    title = str(len(sorted_areas)) \n    \n    cnt = sorted_areas[0][1]\n    x1,y1,w,h = cv2.boundingRect(cnt)\n    x2 = x1 + w\n    y2 = y1 + h\n    \n    for j in range(1, len(sorted_areas)):\n        cnt = sorted_areas[j][1]\n        tx1,ty1,tw,th = cv2.boundingRect(cnt)\n        tx2 = tx1 + tw\n        ty2 = ty1 + th\n        x1 = min(x1, tx1)\n        y1 = min(y1, ty1)\n        x2 = max(x2, tx2)\n        y2 = max(y2, ty2)\n        ```"}}