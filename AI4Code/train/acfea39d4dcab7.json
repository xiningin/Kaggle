{"cell_type":{"7a38fbcd":"code","4b656b90":"code","41af15e9":"code","82e2fed6":"code","f2ba54e1":"code","b516e3bf":"code","5f0d69e8":"code","e1adf25e":"code","28e99d56":"code","5c0336b5":"code","259361aa":"code","42dff893":"code","3706059b":"code","34b7f665":"code","fe4cd306":"markdown","05a5dbbd":"markdown","22e63798":"markdown","1ca3708d":"markdown","9f80578a":"markdown","82314131":"markdown","102eadad":"markdown","5927f84e":"markdown","1eb8a958":"markdown","c9efdba8":"markdown","139c56ed":"markdown","f4a8adac":"markdown","00c58a97":"markdown","8ab1b8d3":"markdown","6d5779a6":"markdown","97ee4a40":"markdown","7a9d0942":"markdown"},"source":{"7a38fbcd":"import numpy as np # linear algebra\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\ndata = pd.read_csv('..\/input\/voice.csv')","4b656b90":"data.head()","41af15e9":"data.label = [1 if each == \"male\" else 0 for each in data.label]\ndata.label.unique()","82e2fed6":"data.label.value_counts()","f2ba54e1":"y = data.label.values\nx_data = data.drop([\"label\"],axis=1)\n## And normalization\nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values","b516e3bf":"x.head()","5f0d69e8":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=42) # random state for stability\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T","e1adf25e":"def init_weights_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w, b\ndef sigmoid(z):\n    y_head = 1 \/ (1 + np.exp(-z))\n    return y_head","28e99d56":"def forward_backward_propagation(w,b,x_train,y_train):\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]    \n    \n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] \n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                 \n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return cost, gradients","5c0336b5":"def update_weights_bias(w,b,x_train,y_train,learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    for i in range(number_of_iterarion):\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        w = w - learning_rate * gradients[\"derivative_weight\"] \n        b = b - learning_rate * gradients[\"derivative_bias\"]   \n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","259361aa":"def predict(w,b,x_test):\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","42dff893":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, num_iterations):\n    dimension = x_train.shape[0]\n    w, b = init_weights_bias(dimension)\n\n    parameters,gradients,cost_list = update_weights_bias(w,b,x_train,y_train,learning_rate,num_iterations)\n\n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    return y_prediction_test #Estimates for Complex Matrix\n    \ny_predict = logistic_regression(x_train, y_train, x_test, y_test, learning_rate=1, num_iterations=300)","3706059b":"predict = []\nfor i in range(0,1):\n    for each in y_predict[i]:\n        predict.append(int(each))\n\nconf_matrix = confusion_matrix(y_test,predict)\nf,ax = plt.subplots(figsize=(7,7))\nsns.heatmap(conf_matrix,annot=True,linewidths=0.5,linecolor=\"white\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"predict\")\nplt.show()","34b7f665":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T)))","fe4cd306":"### After Forward and Backward propagation we got some weight and bias values, so we need to update them for fitting, for the best predictions\n- Update function is where we update bias and weight values","05a5dbbd":"## Initialize Weight and Bias:\n\n- This function is where we initialize weight and bias.\n- Our formula is z = (weight * feature) + b\n               y_head = sigmoid(z)\n- Sigmoid for scale the result -1 and 1\n                    ","22e63798":"### *See it is so easy with sklearn :)*","1ca3708d":"As you see above, 1 represents \"male\" and 0 represents \"female\".\n- Now we need to split data","9f80578a":"#### Why normalization is needed? \n - The answer is simple. \"Domination\". If one feature has low values and other feature has high values, high values will dominate low values while calculating. For example;\n  * In second index,  kurt feature value is 1024 and sp.ent 0.84. If we calculate both of them in one process kurt feature will dominate sp.ent. See?","82314131":"*Then, lets split our data to train and test","102eadad":"Lets check what we have","5927f84e":"## -You see  \"label\" column ? This column is our result column. Lets convert it to binary system for machine. For comparison we need to convert data to binary.","1eb8a958":"## After Logisctic regression our predictions 97% correct. It is good result. \n    - As you can see If iteration increase, cost value will decrease. Please remember this, I will mention it below.\n    - Now lets show with confusion matrix what is wrong?","c9efdba8":"*According to our confusion matrix the model predict 7  and 11 wrong sample. 7 represent it is 0 but model predict 1 instead of it, 11 also represent it is 1 but model predict 0 instead of it*","139c56ed":"# But dont worry, it is for your learning. Sklearn do all this process for you.  ","f4a8adac":"# What is Logistic Regression ? \n\nLogistic regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary).Like all regression analyses, the logistic regression is a predictive analysis.  Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.\n\n#### Logistic regression steps are :\n- In\u0131tialize Weight and Bias values,\n- Function Sigmoid,\n- Forward Propagation,  ( In loop with backward to find minimum cost value )\n- Backward Propagation,  ( In loop with forward to find minimum cost value )\n- Update Weight and Bias,\n- Then predict","00c58a97":"The forward_backward_propagation function can be explained by the logical regression algorithm\n\nNOTE: This algorithm is described in detail https:\/\/www.kaggle.com\/kanncaa1\/deep-learning-tutorial-for-beginners","8ab1b8d3":"*In the logic regression learning rate is important. If you pick it too high, you can skip minimum cost value, if you pick it too low, model will learn slowly and will take time to find minimum value.*\n","6d5779a6":"## Conclusion\n- We initialize weight and bias, these are our learning parameters.\n- z = (weight * feature) +b \n- y = sigmoid(z)\n- Update weight and bias for minimum cost value with forward and backward propagation\n- Predict and run all function\n- With 97% it is good score :)\n\n- So our purpose is update weight and bias, minimize cost value \/ so our model learn well and give us good score and we get minimum cost value.","97ee4a40":"*Iteration number is another important issue. If you pick high value it takes time to finish iterations but your model learn better, if you pick low value, it will finished faster but will learn less* ","7a9d0942":"## Thank you for your time, if you like it please uptove to motivate :) "}}