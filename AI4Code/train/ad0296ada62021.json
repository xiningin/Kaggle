{"cell_type":{"2ae8bc2a":"code","ac03a54b":"code","5da42446":"code","ac04ba10":"code","2f1799ae":"code","1a2db745":"code","367c42a7":"code","21766ca0":"code","e1be96e7":"code","957a6fa0":"code","93e7eae0":"code","4fb39456":"code","57504641":"code","b6e7ef62":"code","788dcbcf":"code","0c7674c2":"code","5aab8405":"code","984581c1":"code","f69c76c5":"code","673d6d46":"code","7ac1240c":"code","7dd07324":"code","53725b76":"code","7725ce64":"code","34f4f21b":"code","9d200492":"code","87dc4990":"code","5b577ce7":"code","3c482136":"code","17714229":"code","153b8929":"code","e1611f08":"code","556c2074":"code","d440b8d2":"code","8f938ea1":"code","05f7ebb7":"code","b345ca36":"code","5f44956d":"code","adbd54a5":"code","e025155f":"code","47787916":"code","43f3fa33":"code","acf1fef8":"code","8c212744":"code","3990a46a":"code","c092b0d7":"code","ba937b10":"code","9b86c101":"code","b16199e1":"code","ec5bf2cb":"code","5d39fbd7":"code","a2743385":"code","21b4d9f9":"code","7ba23425":"code","f880d467":"code","e563bd6e":"code","74623b26":"code","21576304":"code","14b57571":"code","cdeda2af":"code","0a5f6d2f":"code","d33b0ae3":"code","39135532":"code","effdfa0f":"code","70a10a14":"code","3d9df205":"code","f5c523ab":"code","22652612":"code","92647647":"code","df43fd25":"code","cd02adbb":"code","4c440e7c":"code","a84ad475":"code","294bd4bc":"markdown","58056149":"markdown","e23bd079":"markdown","5f1b7080":"markdown","44c79ee2":"markdown","ebe3ddc7":"markdown","236f04c8":"markdown","3e6e51bc":"markdown","9c6787ea":"markdown","e2ef07cd":"markdown","a539d86a":"markdown","6989dfd6":"markdown","ac86413d":"markdown","1bf7a9fe":"markdown","49577837":"markdown"},"source":{"2ae8bc2a":"import warnings\nwarnings.filterwarnings('ignore')","ac03a54b":"from nltk.corpus import stopwords\nfrom collections import Counter\nfrom nltk import *\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.stem import WordNetLemmatizer\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","5da42446":"import numpy as np\nimport random\nimport pandas as pd\nimport sys\nimport os\nimport time\nimport codecs\nimport collections\nimport numpy\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import LSTM\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.utils import np_utils\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport scipy\nfrom scipy import spatial\nfrom nltk.tokenize.toktok import ToktokTokenizer\nimport re\ntokenizer = ToktokTokenizer()","ac04ba10":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense, Input, LSTM, Embedding,Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D,Conv1D, SimpleRNN\nfrom keras.models import Model\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.models import Sequential\nfrom keras import initializers, regularizers, constraints,optimizers, layers\nfrom keras.layers import Dense, Input, Flatten, Dropout,BatchNormalization\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import SimpleRNN\nimport sklearn\nfrom sklearn.metrics import precision_recall_fscore_support as score","2f1799ae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1a2db745":"# Importing data and checking it out\ndf = pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv', encoding='ISO-8859-1')\ndf.head()","367c42a7":"df.shape","21766ca0":"# Checking null values\ndf.isnull().sum()","e1be96e7":"# Extracting required columns\ndf = df[['v1', 'v2']]\ndf.head()","957a6fa0":"# Renaming columns\ndf.rename(columns={'v1':'label', 'v2':'text'}, inplace=True)","93e7eae0":"df.head()","4fb39456":"df['text']","57504641":"# Removing stop words and converting it all to lowercase\nstop = stopwords.words('english')\ndf['text'] = df['text'].apply(lambda x: \" \".join(x for x in x.split() if x.lower() not in stop))\ndf['text'].head()","b6e7ef62":"# Converting to lowercase\ndf['text'] = df['text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ndf['text'].head()","788dcbcf":"# Removing symbols\ndf['text'] = df['text'].apply(lambda x:re.sub('[!@#$:).;,?&]', \"\", x.lower()))\ndf['text'].head()","0c7674c2":"df.isnull().sum()","5aab8405":"# We can give only two arguments if we're working with a dataframe\ntraining, testing = train_test_split(df, test_size=0.2)","984581c1":"print(training.shape)\nprint(testing.shape)","f69c76c5":"# Finding max sentence length - 300\nnp.max(df['text'].apply(lambda x: len(x)))","673d6d46":"# We will take the top 200000 frequently occuring words\nwords = 20000\ntokenizer = Tokenizer(num_words=words)","7ac1240c":"tokenizer.fit_on_texts(training.text)","7dd07324":"train_seq = tokenizer.texts_to_sequences(training.text)\ntest_seq = tokenizer.texts_to_sequences(testing.text)","53725b76":"import itertools","7725ce64":"# Dictionary for the words and the index\nword_index = tokenizer.word_index\nprint(dict(itertools.islice(word_index.items(), 50)))\nprint()\nprint('Found %s unique tokens '%len(word_index))","34f4f21b":"# Padding data for equal lengths, for our models\ntraining_data = pad_sequences(train_seq, maxlen=300)\ntesting_data = pad_sequences(test_seq, maxlen=300)","9d200492":"print(training_data.shape)\nprint(testing_data.shape)","87dc4990":"y_train = training['label']\ny_test = testing['label']","5b577ce7":"le = LabelEncoder()\nle.fit(y_train)\ny_train = le.transform(y_train)\ny_test = le.transform(y_test)\nprint(le.classes_)","3c482136":"y_train.shape","17714229":"y_train","153b8929":"y_test.shape","e1611f08":"# Converting the labels to categorical\n# To pass through our model\ny_train_cat = to_categorical(np.asarray(y_train))\ny_test_cat = to_categorical(np.asarray(y_test))\nprint('Shape of data tensor', training_data.shape)\nprint('Shape of label tensors (training)', y_train_cat.shape)\nprint('Shape of label tensors (testing)', y_test_cat.shape)","556c2074":"y_train_cat","d440b8d2":"# Defining our embedding dimension\nembeds = 100","8f938ea1":"print('Training CNN 1D model')\nmodel = Sequential()\n# 20000 was our maximum word number in the tokenizer\nmodel.add(Embedding(20000,\n embeds,\n input_length=300\n ))\nmodel.add(Dropout(0.5))\nmodel.add(Conv1D(128, 5, activation='relu'))\nmodel.add(MaxPooling1D(5))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\nmodel.add(Conv1D(128, 5, activation='relu'))\nmodel.add(MaxPooling1D(5))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',\n optimizer='rmsprop',\n metrics=['acc'])","05f7ebb7":"model.fit(training_data, y_train_cat, batch_size=64, epochs=5, validation_data = (testing_data, y_test_cat))","b345ca36":"predicted=model.predict(testing_data)\npredicted","5f44956d":"# Metrics\nprecision, recall, fscore, support = score(y_test_cat,predicted.round())\nprint('precision: {}'.format(precision))\nprint('recall: {}'.format(recall))\nprint('fscore: {}'.format(fscore))\nprint('support: {}'.format(support))\nprint(\"############################\")\nprint(sklearn.metrics.classification_report(y_test_cat,predicted.round()))","adbd54a5":"print('Training SIMPLERNN model.')\nmodel = Sequential()\nmodel.add(Embedding(20000,\n embeds,\n input_length=300\n ))\nmodel.add(SimpleRNN(2, input_shape=(None,1)))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss = 'binary_crossentropy',\noptimizer='adam',metrics = ['accuracy'])\nmodel.fit(training_data, y_train_cat,\n batch_size=16,\n epochs=5,\n validation_data=(testing_data, y_test_cat))","e025155f":"# probabilities\npredicted_Srnn=model.predict(testing_data)\npredicted_Srnn","47787916":"precision, recall, fscore, support = score(y_test_cat, predicted_Srnn.round())\nprint('precision: {}'.format(precision))\nprint('recall: {}'.format(recall))\nprint('fscore: {}'.format(fscore))\nprint('support: {}'.format(support))\nprint(\"############################\")\nprint(sklearn.metrics.classification_report(y_test_cat,predicted_Srnn.round()))","43f3fa33":"print('Training LSTM model.')\nmodel = Sequential()\nmodel.add(Embedding(20000,\n embeds,\n input_length=300\n ))\nmodel.add(LSTM(16, activation='relu',return_sequences=True))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dense(2,activation='softmax'))\n\nmodel.compile(loss = 'binary_crossentropy',\noptimizer='adam',metrics = ['accuracy'])\nmodel.fit(training_data, y_train_cat,\n batch_size=16,\n epochs=5,\n validation_data=(testing_data, y_test_cat))","acf1fef8":"predicted_lstm=model.predict(testing_data)\npredicted_lstm","8c212744":"precision, recall, fscore, support = score(y_test_cat, predicted_lstm.round())\nprint('precision: {}'.format(precision))\nprint('recall: {}'.format(recall))\nprint('fscore: {}'.format(fscore))\nprint('support: {}'.format(support))\nprint(\"############################\")\nprint(sklearn.metrics.classification_report(y_test_cat,predicted_lstm.round()))","3990a46a":"print('Training Bidirectional LSTM model.')\nmodel = Sequential()\nmodel.add(Embedding(20000,\n embeds,\n input_length=300\n ))\nmodel.add(Bidirectional(LSTM(16, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)))\nmodel.add(Conv1D(16, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\"))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(50, activation=\"relu\"))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(2,activation='softmax'))\nmodel.compile(loss = 'binary_crossentropy',\noptimizer='adam',metrics = ['accuracy'])\nmodel.fit(training_data, y_train_cat,\n batch_size=16,\n epochs=3,\n validation_data=(testing_data, y_test_cat))","c092b0d7":"predicted_blstm=model.predict(testing_data)\npredicted_blstm","ba937b10":"precision, recall, fscore, support = score(y_test_cat, predicted_blstm.round())\nprint('precision: {}'.format(precision))\nprint('recall: {}'.format(recall))\nprint('fscore: {}'.format(fscore))\nprint('support: {}'.format(support))\nprint(\"############################\")\nprint(sklearn.metrics.classification_report(y_test_cat,predicted_blstm.round()))","9b86c101":"df.head()","b16199e1":"df_listing = df.text.tolist()\ndf_listing[:10]","ec5bf2cb":"# Convert the given list to strings\nfrom collections import Iterable\n\ndef reduce_dims(items):\n    for x in items:\n        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n            for sub_x in reduce_dims(x):\n                yield sub_x\n        else:\n            yield x","5d39fbd7":"string_final = ''.join(df_listing)","a2743385":"string_final = string_final.replace('\\n', '')\nstring_final = string_final.lower()","21b4d9f9":"pattern = r'[^a-zA-z0-9\\s]'\nstring_final = re.sub(pattern, \"\", string_final)","7ba23425":"tokens = tokenizer.tokenize(string_final)\ntokens = [token.strip() for token in tokens]","f880d467":"total_words = Counter(tokens)\nlen(total_words)","e563bd6e":"total_words.most_common()[:10]","74623b26":"words = [x[0] for x in total_words.most_common()]\nwords[:10]","21576304":"sorted_words = list(sorted(words))\nsorted_words[:10]","14b57571":"word_ind = {x: i for i, x in enumerate(sorted_words)}","cdeda2af":"# Decide on a sentence length\nsentence_length = 25","0a5f6d2f":"# Prepare input to output pairs encoded as integers\n# input - sentence input \n# output - model output with index\ninp = []\nout = []\n# As we need 11 words (10 words for sentence, 1 for output)\n# We will set the for loop like this\nfor i in range(0, len(total_words) - sentence_length, 1):\n    x = tokens[i:i+sentence_length]\n    y = tokens[i+sentence_length]\n    # Creating a vector\n    inp.append([word_ind[char] for char in x])\n    out.append(word_ind[y])","d33b0ae3":"# Inverse dictionary\ninv_dict = dict(map(reversed, word_ind.items()))","39135532":"out[:1]","effdfa0f":"X = numpy.reshape(inp, (len(inp), sentence_length, 1))\n# to_categorical for one-hot encoding\nY = np_utils.to_categorical(out)","70a10a14":"Y","3d9df205":"model = Sequential()\nmodel.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(Y.shape[1], activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',optimizer='adam')","f5c523ab":"file_name_path=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\ncheckpoint = ModelCheckpoint(file_name_path, monitor='loss',\nverbose=1, save_best_only=True, mode='min')\ncallbacks = [checkpoint]","22652612":"model.fit(X, Y, epochs=5, batch_size=128, callbacks=callbacks) ","92647647":"# Generate random sequence\nrand_val = numpy.random.randint(0, len(inp))\nrand_val","df43fd25":"input_sentence = inp[rand_val]\ninput_sentence","cd02adbb":"X = numpy.reshape(input_sentence, (1, len(input_sentence), 1))","4c440e7c":"predict_word = model.predict(X, verbose=0)\nindex = numpy.argmax(predict_word)","a84ad475":"result = inv_dict[index]\nsent_in = [inv_dict[value] for value in input_sentence]\nprint(sent_in)\nprint (\"\\n\")\nprint(result)","294bd4bc":"<h2>Generating random input to predict next word<\/h2>","58056149":"<h2>Data preparation for modeling<\/h2>\n\nWe will be dividing all the data in our text column into sequences of words with fixed length of 10 words (we can modify this according to our requirements). We will be splititng the text based on word sequences, when we create the sequence, we can slide the window across the whole document one word at a time, allowing to learn from its predeceding one.","e23bd079":"Now that we have our input and output data in numerical format, we can proceed with one-hot encoding the target variables and training our model.","5f1b7080":"<h1>Introduction<\/h1>\n\nThis guide will cover the basics of deep learning for NLP tasks. We will first cover classification of data as spam\/not-spam using various deep learing frameworks like RNNs and LSTMs. We will then also cover how to predict the next word in a given word-sequence using RNNs.","44c79ee2":"**NOTE** - We have not split the training and testing data here, as we are not interested in the accuracy. Deep learning models require huge amounts of data and time to train, so we will be using all the data we have access to.","ebe3ddc7":"`pad_sequences` is used to ensure that all sequences in a list have the same length. By default this is done by padding 0 in the beginning of each sequence until each sequence has the same length as the longest sequence.\nSequences longer than num_timesteps are truncated so that they fit the desired length.\nThe position where padding or truncation happens is determined by the arguments padding and truncating, respectively. Pre-padding or removing values from the beginning of the sequence is the default.","236f04c8":"`fit_on_texts` - Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. `word_index[\"the\"] = 1; word_index[\"cat\"] = 2` it is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding. So lower integer means more frequent word (often the first few are stop words because they appear a lot). Through num_words, we are picking the most frequent words i.e. the ones with the lower integer values.\n\n`texts_to_sequences` Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved.","3e6e51bc":"<h2>Text classification using deep learning<\/h2>\n\nOur main objective here is to build a text classifier using neural networks. The basic NLP pipeline will be the same, followed by a new process of building deep learning models:","9c6787ea":"<h2>Bidirectional LSTM<\/h2>","e2ef07cd":"<h2>Model building<\/h2>\n\nWe will be using LSTMs. We are using a single layer with 256 memory units. The model will use a dropout of 0.2. Softmax activation function is used alongside the ADAM optimizer.","a539d86a":"<h2>Model building and predicting<\/h2>\n\nWe are building the models using different deep learning approaches\nlike CNN, RNN, LSTM, and Bidirectional LSTM and comparing the\nperformance of each model using different accuracy metrics.\nWe can now define our CNN model.\nHere we define a single hidden layer with 128 memory units. The\nnetwork uses a dropout with a probability of 0.5. The output layer is a\ndense layer using the softmax activation function to output a probability\nprediction.","6989dfd6":"<h2>LSTM model<\/h2>","ac86413d":"So, given the 25 input words, it's predicting the word \u201cu\u201d as the next\nword. Of course, its not making much sense, since it has been trained on\nmuch less data and epochs. Make sure you have great computation power\nand train on huge data with high number of epochs.\n\nThrough this, we were successful in creating a model that can predict the next word based on a given sequence. This can further be improved with much larger corpus of text and bigger networks.","1bf7a9fe":"<h2>Next word prediction<\/h2>\n\nMechanisms such as autofills can help us understand the potential sequence of words that can be filled in front of an incomplete sentence. This technique is leveraged in different formats, mostly for email writing.\n\nWe will build an LSTM model to learn sequences of data from our spam texts.","49577837":"<h2>RNN model<\/h2>"}}