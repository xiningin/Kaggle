{"cell_type":{"0bac554a":"code","4bc28704":"code","e4a49cd5":"code","edfa124b":"code","38e8631a":"code","b89373ff":"code","46e2aee2":"code","97031d2d":"code","170d1448":"code","0963b92f":"code","f068da65":"code","e13184df":"code","d221951a":"code","88d225a7":"code","99a0a946":"code","82023069":"code","7e3755da":"code","f5ea825a":"code","71a3daaf":"code","9cbd4f48":"code","4b00a86d":"code","eb66aecd":"code","e69d1dfd":"markdown","2035ef3e":"markdown","a88338d3":"markdown","7b27335c":"markdown","5fde9b1e":"markdown","cea5720a":"markdown","baafac05":"markdown","aa3028d5":"markdown","0715b50b":"markdown","e0747dbd":"markdown","7d8cf300":"markdown","0d45e67a":"markdown","750e169b":"markdown","15eacd2f":"markdown","6d44224d":"markdown","3ebd9d64":"markdown","820601b2":"markdown","574a1871":"markdown","5088a0e4":"markdown","aa57dbea":"markdown","1dea89c6":"markdown","4e27c9da":"markdown","27a680f5":"markdown","0899d22f":"markdown","e73958c9":"markdown","79f0f74a":"markdown","3112cca9":"markdown","c82675b8":"markdown"},"source":{"0bac554a":"import numpy as np\nimport pandas as pd\nimport warnings \nimport tensorflow as tf # import tensor flow \nimport numpy as np\nimport keras\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt","4bc28704":"from tensorflow.keras.datasets import reuters\n(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)","e4a49cd5":"len(train_data),len(test_data)","edfa124b":"print(train_data[10])","38e8631a":"word_index = reuters.get_word_index()\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\ndecoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in\n    train_data[0]])","b89373ff":"train_labels[10]","46e2aee2":"def vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1.\n    return results\nx_train = vectorize_sequences(train_data)#1\nx_test = vectorize_sequences(test_data)#2","97031d2d":"def to_one_hot(labels, dimension=46):\n    results = np.zeros((len(labels), dimension))\n    for i, label in enumerate(labels):\n        results[i, label] = 1.\n    return results\none_hot_train_labels = to_one_hot(train_labels)#1\none_hot_test_labels = to_one_hot(test_labels)#2","170d1448":"from tensorflow.keras.utils import to_categorical\none_hot_train_labels = to_categorical(train_labels)\none_hot_test_labels = to_categorical(test_labels)","0963b92f":"model = keras.Sequential([\n  layers.Dense(64, activation='relu'),\n  layers.Dense(64, activation='relu'),\n  layers.Dense(46, activation='softmax')\n])","f068da65":" model.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","e13184df":"x_val = x_train[:1000]\npartial_x_train = x_train[1000:]\ny_val = one_hot_train_labels[:1000]\npartial_y_train = one_hot_train_labels[1000:]","d221951a":"history = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=20,\n                    batch_size=512,\n                    validation_data=(x_val, y_val))","88d225a7":"loss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","99a0a946":"plt.clf()\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nplt.plot(epochs, acc, 'bo', label='Training accuracy')\nplt.plot(epochs, val_acc, 'r', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","82023069":"model = keras.Sequential([\n  layers.Dense(64, activation='relu'),\n  layers.Dense(64, activation='relu'),\n  layers.Dense(46, activation='softmax')\n])\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.fit(partial_x_train,\n          partial_y_train,\n          epochs=10,\n          batch_size=512,\n          validation_data=(x_val, y_val))\nresults = model.evaluate(x_test, one_hot_test_labels)","7e3755da":"results","f5ea825a":"import copy\ntest_labels_copy = copy.copy(test_labels)\nnp.random.shuffle(test_labels_copy)\nhits_array = np.array(test_labels) == np.array(test_labels_copy)\nfloat(np.sum(hits_array))\/len(test_labels)","71a3daaf":" predictions = model.predict(x_test)","9cbd4f48":"predictions[0].shape","4b00a86d":"np.sum(predictions[0])","eb66aecd":"np.argmax(predictions[0])","e69d1dfd":"#### Further explorations \nTry using larger or smaller layers: 32 units, 128 units, and so on.\nWe used two intermediate layers before the final softmax classification layer. Now try using a single intermediate layer, or three intermediate layers.\n\nPost the results in the comment box please............\n\nIf you like this notebook please share it further\n","2035ef3e":"#### Model Defination","a88338d3":"#### The coefficients in this vector sum to 1, as they form a probability distribution:","7b27335c":"#### let\u2019s train the model for 20 epochs.\n\n#### Training the model ","5fde9b1e":"1. Verctorize training data\n2. Vencotrize testing data","cea5720a":"#### Load Dataset ","baafac05":"### Generating prediction on the new data \n\nCalling the predict method on new samples returns a class probability distribution over all 46 topics for each sample. Let\u2019s generate topic predictions for all of the test data.","aa3028d5":"#### This approach reaches an accuracy of ~79%. With a balanced binary classification problem, the accuracy reached by a purely random classifier would be 50%. But in this case, we have 46 classes, and they may not be equally represented. What would be the accuracy of a random baseline? We could try quickly implementing one to check this empirically:","0715b50b":"#### Plotting the training and validation accuracy","e0747dbd":"### Building the model\n\nThis topic-classification problem looks similar to the previous movie-review classificationq: in both cases, we are trying to classify short snippets of text. But there is a new constraint here: the number of output classes has gone from 2 to 46. The dimensionality of the output space is much larger.\n\nIn a stack of Dense layers like that we have been using, each layer can only access information present in the output of the previous layer. If one layer drops some information relevant to the classification problem, this information can never be recovered by later layers: each layer can potentially become an information bottleneck. In the previous example, we used 16-dimensional intermediate layers, but a 16-dimensional space may be too limited to learn to separate 46 different classes: such small layers may act as information bottlenecks, permanently dropping relevant information.\nFor this reason we will use larger layers. Let\u2019s go with 64 units.","7d8cf300":"#### As we can see, a random classifier would score around 19% classification accuracy, so the results of our model seem pretty good in that light.","0d45e67a":"### Data Prep\n\n#### vectorize the input data ","750e169b":"#### The largest entry is the predicted class \u2014 the class with the highest probability:","15eacd2f":"#### Validation of the approach \n\nLet\u2019s set apart 1,000 samples in the training data to use as a validation set.","6d44224d":"The argument num_words=10000 restricts the data to the 10,000 most frequently occurring words found in the data.\nYou have 8,982 training examples and 2,246 test examples:","3ebd9d64":"### Classifying newswires: a multiclass classification example\nIn my previous notebook https:\/\/www.kaggle.com\/santosh1974\/imdb-reviews-using-keras-binary-classification, you saw how to classify vector inputs into two mutually exclusive classes using a densely-connected neural network. But what happens when you have more than two classes?\n\nIn this example, we will build a model to classify Reuters newswires into 46 mutually exclusive topics. Because we have many classes, this problem is an instance of *multiclass classification*; and because each data point should be classified into only one category, the problem is more specifically an instance of single-label, multiclass classification. If each data point could belong to multiple categories (in this case, topics), you\u2019d be facing a multilabel, multiclass classification problem.\n\n### Reuters dataset\nWe will work with the Reuters dataset, a set of short newswires and their topics, published by Reuters in 1986. It\u2019s a simple, widely used toy dataset for text classification. There are 46 different topics; some topics are more represented than others, but each topic has at least 10 examples in the training set.\nLike IMDB and MNIST, the Reuters dataset comes packaged as part of Keras. Let\u2019s begin......\n","820601b2":"1. Verctorize training labels\n2. Vencotrize testing labels","574a1871":"#### Retraining a model from scratch","5088a0e4":"The model begins to overfit after nine epochs. Let\u2019s train a new model from scratch for nine epochs and then evaluate it on the test set.","aa57dbea":"Note about this architecture:\n\n1. We end the model with a Dense layer of size 46. This means for each input sample, the network will output a 46-dimensional vector. Each entry in this vector (each dimension) will encode a different output class.\n\n2. The last layer uses a softmax activation. You saw this pattern in the MNIST example. It means the model will output a probability distribution over the 46 different output classes \u2014 for every input sample, the model will produce a 46-dimensional output vector, where output[i] is the probability that the sample belongs to class i. The 46 scores will sum to 1.\n\n3. The best loss function to use in this case is categorical_crossentropy. It measures the distance between two probability distributions: here, between the probability distribution output by the model and the true distribution of the labels. By minimizing the distance between these two distributions, you train the model to output something as close as possible to the true labels.\n","1dea89c6":"### Key Takeaways\n\n1. If we are trying to classify data points among N classes,  model should end with a Dense layer of size N.\n2. In a single-label, multiclass classification problem, our model should end with a softmax activation so that it will output a probability distribution over the N output classes.\n3. Categorical crossentropy is almost always the loss function you should use for such problems. It minimizes the distance between the probability distributions output by the model and the true distribution of the targets.\n4. There are two ways to handle labels in multiclass classification:\n    Encoding the labels via categorical encoding (also known as one-hot encoding) and using   \n    categorical_crossentropy as a loss function\n    Encoding the labels as integers and using the sparse_categorical_-crossentropy loss function\n5. If you need to classify data into a large number of categories, you should avoid creating information bottlenecks in your model due to interme","4e27c9da":"#### vectorize the label with the exact same code as in the previous example.","27a680f5":"#### Compile the model","0899d22f":"Each example is a list of integers (word indices):\n\n#### Here\u2019s how you can decode it back to words, in case you\u2019re curious.","e73958c9":"Note that there is a built-in way to do this in Keras:","79f0f74a":"#### Each entry in \u201cpredictions` is a vector of length 46:","3112cca9":"#### Plotting the training and validation loss","c82675b8":"\nNote that the indices are offset by 3 because 0, 1, and 2 are reserved indices for \u201cpadding,\u201d \u201cstart of sequence,\u201d and \u201cunknown.\u201d\nThe label associated with an example is an integer between 0 and 45 \u2014 a topic index:"}}