{"cell_type":{"b6e23c2e":"code","7625810b":"code","9df523fb":"code","c635b15f":"code","152fb5b9":"code","77a30642":"code","35d35bf7":"code","395e0e69":"code","89bfc060":"code","b1cc9351":"code","0db8f735":"code","fb0a7d21":"code","8cbe0ed8":"code","e0e7aff1":"code","ac5c5668":"code","1decd073":"code","0096a5e7":"code","905d7d0d":"code","bfd1ec15":"code","48a8a25a":"code","efb9af70":"code","5e4c9512":"code","d66228b2":"code","8b0cd1bf":"code","4ce8fe1d":"code","3683f7f0":"code","9e22ffdf":"code","fe4945a0":"code","b9fad9dd":"code","0e1d3166":"code","6456c0d9":"code","f53af4f6":"code","dfdd5077":"code","af3cb829":"code","d926b8f5":"code","d4fd939a":"code","e5f58fe6":"code","88c493f8":"code","a492ef1f":"code","636b7f1e":"code","6ef49493":"code","8f8caaa9":"code","2d6e537d":"code","3cde6ff1":"code","4875eff6":"code","88783d8c":"code","8cf9b43f":"code","7a7dc36b":"code","80a7b77d":"code","5cd4d9ae":"code","178b3094":"code","5a557ab3":"code","9278c049":"code","820ffa3c":"code","91a84329":"code","59d6be89":"code","cc2d0b79":"code","1cbf791e":"code","3b26c37f":"code","b99d684b":"code","00ffe511":"code","1c03a87f":"code","3f74b8b0":"code","6e48faf7":"code","9c50ac50":"code","a9fc1edd":"code","137965c4":"code","04d11cc6":"code","60e89db6":"code","74d4461d":"code","176df606":"code","5155dbe6":"code","f1ca31fe":"code","d319604a":"code","939a88c8":"code","73bce4d1":"code","0cfa1b48":"code","2a3c8073":"code","ea874292":"code","6d95e6a1":"code","465ecca4":"code","e46da180":"code","7b69ce0c":"code","e853d0f6":"code","de995c15":"code","fc82c0ba":"code","bea7c106":"code","7e7d6432":"code","627b0d28":"code","e450170c":"code","9dfa2b64":"code","69b2ed97":"code","331f3a7a":"code","a364a7de":"code","060f832f":"code","f4a58391":"code","ea4f8005":"code","4677821f":"code","116b7cc3":"code","c3a301ad":"code","e82b7399":"code","493da995":"code","3062cf7b":"code","a7643ccb":"code","22a391ef":"code","8359f0e1":"code","f7733d09":"code","75b20d38":"code","7c16056f":"code","edda77fa":"code","df6d12f7":"code","70cb1a78":"code","e8fd1bea":"code","d941fb90":"code","e025eb8a":"code","6789f27e":"code","410b5aa7":"code","392feeec":"code","207b86f7":"code","b06566b9":"code","dd0bb480":"code","c5054491":"code","3beb6e77":"code","8fe3ec22":"code","53d44039":"code","8d9bb4e7":"code","af3c475e":"code","dd49c021":"code","140ca72d":"code","8fdf1711":"code","37024dfc":"code","8f99b385":"code","218ce0e1":"code","4b307585":"code","5c8b9ea5":"code","b60fc67c":"code","a4239b80":"code","b0685fe3":"code","4ef2f929":"code","d6072f9b":"code","20ebdfa2":"code","82ab3bb4":"markdown","33a370f5":"markdown","43f66658":"markdown","3bb99b6d":"markdown","73347914":"markdown","54f4a575":"markdown","44298a1c":"markdown","10f63220":"markdown","393804f1":"markdown","e0a36cd0":"markdown","dc4528ca":"markdown","e48a3cca":"markdown","fbaecbad":"markdown","c143d1d1":"markdown","b9ef5fcc":"markdown","3551d02d":"markdown","c4c4c6c7":"markdown","5cb14cf6":"markdown","bb326562":"markdown","783c6be5":"markdown","f2089e2b":"markdown","0cf1de94":"markdown","18444a3c":"markdown","28e16aa8":"markdown","f30f298e":"markdown","8226905b":"markdown","2f08d02b":"markdown","823dd353":"markdown","7a889bd0":"markdown","6d85f6ba":"markdown","537111f5":"markdown","a70ad45c":"markdown","e33a8244":"markdown","48eddc1c":"markdown","9541a896":"markdown","16ac7575":"markdown","54f247db":"markdown","e3651057":"markdown","c6db1b86":"markdown","8a759695":"markdown","ccf982ef":"markdown","1e8fbd8d":"markdown","23e716a7":"markdown","338fea71":"markdown","8af35dcc":"markdown","6079dd8c":"markdown","dfeb494b":"markdown","245a5a95":"markdown","3b650912":"markdown","57ac0873":"markdown","96df90b4":"markdown","fe839a7c":"markdown","7cf6651f":"markdown","83f512b3":"markdown","70219e20":"markdown","5b945832":"markdown","589f00aa":"markdown","d53e0288":"markdown","c3062fa2":"markdown","77a15ada":"markdown","6e9400ac":"markdown","c6f14a27":"markdown","caa3f678":"markdown","685c72a1":"markdown","dd3e6e76":"markdown","653d6435":"markdown","ba36c05d":"markdown","88e89acd":"markdown","022f015c":"markdown","51f4b168":"markdown","ea425f3a":"markdown","02268fdc":"markdown","ddf7a8db":"markdown","4b37e9a5":"markdown","826729b2":"markdown","3e96546f":"markdown","5c0e8ac4":"markdown","06a47319":"markdown","855d99f7":"markdown","7994721d":"markdown","d98fd87a":"markdown","c85d00ff":"markdown","fe7cd296":"markdown","565b1ea6":"markdown","aa8bce93":"markdown","fd0faf4f":"markdown","15580363":"markdown","df3558f7":"markdown","66bba634":"markdown","813e5e34":"markdown","19c004b1":"markdown","5a6bd196":"markdown","89f875c9":"markdown","7bd8f87a":"markdown","fb68ce52":"markdown","99d85dea":"markdown","5b0aeaf1":"markdown","a1bb4d9d":"markdown","0f6afe01":"markdown","1880fc58":"markdown","67ae9d4a":"markdown","8f9f80f7":"markdown","7660f333":"markdown","37b4923e":"markdown","d900dc46":"markdown","14c57497":"markdown","672c8dfa":"markdown","6e7d4f70":"markdown","b72681bb":"markdown","56543a71":"markdown","5152e075":"markdown","018bb776":"markdown","0c047c7d":"markdown","10cda14b":"markdown","0d676136":"markdown","614e1677":"markdown","e05843a5":"markdown","da39185a":"markdown","c171c3e4":"markdown","7f43a10c":"markdown","896d27df":"markdown","76bfdf4f":"markdown","3f519a9e":"markdown","4f6c07ee":"markdown","19464a8b":"markdown","f7c8edd6":"markdown","a6aecd4b":"markdown","f949169b":"markdown","8e3434a8":"markdown","7edcd20f":"markdown","b16c741d":"markdown","141f64ec":"markdown","b5c34d40":"markdown","389c3920":"markdown","3a69e747":"markdown","7dc7f7a4":"markdown","c1e88ad6":"markdown","b5caf188":"markdown","2b680471":"markdown","cb253868":"markdown","5e27e24f":"markdown","828bd1f6":"markdown","d9823313":"markdown","08368901":"markdown","ccd79073":"markdown","748c7e20":"markdown","c6cd2171":"markdown","4ebcf4d4":"markdown","de378b9e":"markdown","b63cde74":"markdown","d242747f":"markdown","3ff481db":"markdown"},"source":{"b6e23c2e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7625810b":"# import 'Pandas' \nimport pandas as pd \n\n# import 'Numpy' \nimport numpy as np\n\n# import subpackage of Matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n# import 'Seaborn' \nimport seaborn as sns\n\n# to suppress warnings \nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n# display all columns of the dataframe\npd.options.display.max_columns = None\n\n# display all rows of the dataframe\npd.options.display.max_rows = None\n \n# to display the float values upto 6 decimal places     \npd.options.display.float_format = '{:.6f}'.format\n\n# import train-test split \nfrom sklearn.model_selection import train_test_split\n\n# import various functions from statsmodels\nimport statsmodels\nimport statsmodels.api as sm\n\n# import StandardScaler to perform scaling\nfrom sklearn.preprocessing import StandardScaler \n\n# import various functions from sklearn \nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score \n\n# import function to perform feature selection\nfrom sklearn.feature_selection import RFE","9df523fb":"# display all columns of the dataframe\npd.options.display.max_columns = None\n# display all rows of the dataframe\npd.options.display.max_rows = None\n# return an output value upto 6 decimals\npd.options.display.float_format = '{:.6f}'.format","c635b15f":"# load the csv file\n# store the data in 'df_train'\ndf_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n\n# display first five observations using head()\ndf_train.head()","152fb5b9":"# load the csv file\n# store the data in 'df_test'\ndf_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\n# display first five observations using head()\ndf_test.head()","77a30642":"# load the csv file\n# store the data in 'df_g'\ndf_g = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\n\n# display first five observations using head()\ndf_g.head()","35d35bf7":"df_train.info()","395e0e69":"df_train.shape","89bfc060":"df_train.dtypes","b1cc9351":"df_train['PassengerId']=df_train['PassengerId'].astype('object')\ndf_train['Survived']=df_train['Survived'].astype('object')\ndf_train['Pclass']=df_train['Pclass'].astype('object')","0db8f735":"df_train.dtypes","fb0a7d21":"missing_value = pd.DataFrame({\n    'Missing Value': df_train.isnull().sum(),\n    'Percentage': (df_train.isnull().sum() \/ len(df_train))*100\n})","8cbe0ed8":"missing_value.sort_values(by='Percentage', ascending=False)","e0e7aff1":"# set the figure size\nplt.figure(figsize=(15, 8))\n\n# plot heatmap to check null values\n# isnull(): returns 'True' for a missing value\n# cbar: specifies whether to draw a colorbar; draws the colorbar for 'True' \nsns.heatmap(df_train.isnull(), cbar=False)\n\n# display the plot\nplt.show()","ac5c5668":"df_train['Age'].plot(kind='box')","1decd073":"df_train.drop(['Cabin'],axis=1,inplace=True)","0096a5e7":"df_train['Age'].fillna(df_train['Age'].median(),inplace=True)\ndf_train['Embarked'].fillna(df_train['Embarked'].mode()[0],inplace=True)","905d7d0d":"missing_value = pd.DataFrame({\n    'Missing Value': df_train.isnull().sum(),\n    'Percentage': (df_train.isnull().sum() \/ len(df_train))*100\n})","bfd1ec15":"missing_value.sort_values(by='Percentage', ascending=False)","48a8a25a":"# set the figure size\nplt.figure(figsize=(15, 8))\n\n# plot heatmap to check null values\n# isnull(): returns 'True' for a missing value\n# cbar: specifies whether to draw a colorbar; draws the colorbar for 'True' \nsns.heatmap(df_train.isnull(), cbar=False)\n\n# display the plot\nplt.show()","efb9af70":"duplicate = df_train.duplicated().sum()\nprint('There are {} duplicated rows in the data'.format(duplicate))","5e4c9512":"df_train.shape","d66228b2":"df_train.head()","8b0cd1bf":"df_train.describe()","4ce8fe1d":"df_train.describe(include='object')","3683f7f0":"corr_matrix=df_train.corr()","9e22ffdf":"plt.figure(figsize=(11,9))\ndropSelf = np.zeros_like(corr_matrix)\ndropSelf[np.triu_indices_from(dropSelf)] = True\n\nsns.heatmap(corr_matrix, cmap=sns.diverging_palette(220, 10, as_cmap=True), annot=True, fmt=\".2f\", mask=dropSelf)\n\nsns.set(font_scale=1.5)","fe4945a0":"data_cat_features = df_train.select_dtypes(include='object')\ndata_cat_features.drop(['PassengerId','Name','Ticket'],axis=1,inplace=True)\n# plot the count distribution for each categorical variable \n# 'figsize' sets the figure size\n# plot a count plot for all the categorical variables\nfor variable in data_cat_features.columns:\n    \n    cat_count  = data_cat_features[variable].value_counts()\n    plt.figure(figsize=(10,5))\n    sns.barplot(y=cat_count.values,x=cat_count.index)\n    plt.xlabel('{}'.format(variable), fontsize=12)\n    plt.ylabel('Number of count', fontsize=12)\n    plt.show()\n\n# avoid overlapping of the plots using tight_layout()    \nplt.tight_layout()   \n\n# display the plot\nplt.show()","b9fad9dd":"plt.figure(figsize = (12, 7))\nsns.boxplot(x = 'Pclass', y = 'Age', data = df_train, palette = 'winter')","0e1d3166":"# consider only the target variable\ndf_target = df_train['Survived']\n\n# get counts of 0's and 1's in the 'Chance of Admit' variable\ndf_target.value_counts()\n\n# plot the countplot of the variable 'Chance of Admit'\nsns.countplot(x = df_target)\n\n# use below code to print the values in the graph\n# 'x' and 'y' gives position of the text\n# 's' is the text \nplt.text(x = -0.05, y = df_target.value_counts()[0] + 1, s = str(round((df_target.value_counts()[0])*100\/len(df_target),2)) + '%')\nplt.text(x = 0.95, y = df_target.value_counts()[1] +1, s = str(round((df_target.value_counts()[1])*100\/len(df_target),2)) + '%')\n\n# add plot and axes labels\n# set text size using 'fontsize'\nplt.yticks([0,100,200,300,400,500,600])\nplt.title('Count Plot for Target Variable (Survived)', fontsize = 15)\nplt.xlabel('Target Variable', fontsize = 15)\nplt.ylabel('Count', fontsize = 15)\n\n# to show the plot\nplt.show()","6456c0d9":"Ind_col=df_train.drop(['PassengerId','Survived','Ticket','Name'],axis=1)","f53af4f6":"Ind_col_c=Ind_col.select_dtypes(include='object')","dfdd5077":"Ind_col_n=Ind_col.select_dtypes(include=np.number)","af3cb829":"target_col=df_train['Survived']","d926b8f5":"for col in Ind_col_c.columns:\n    sns.barplot(data =Ind_col_c ,y = target_col, x= col)\n    plt.show()","d4fd939a":"for col in Ind_col_n.columns:\n    sns.boxplot(data =Ind_col_n ,y = col, x= target_col)\n    plt.show()","e5f58fe6":"sns_plot = sns.pairplot(df_train,corner=True,palette='husl',height=4.0)\nplt.show()","88c493f8":"df_train.drop(['Name','Ticket','PassengerId'],axis=1,inplace=True)\ndf_train.head()","a492ef1f":"df_num_features=df_train.select_dtypes(include=np.number)","636b7f1e":"Q1 = df_num_features.quantile(0.25)\nQ3 = df_num_features.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","6ef49493":"outlier = pd.DataFrame((df_num_features < (Q1 - 1.5 * IQR)) | (df_num_features > (Q3 + 1.5 * IQR)))","8f8caaa9":"for i in outlier.columns:\n    print('Total number of Outliers in column {} are {}'.format(i, (len(outlier[outlier[i] == True][i]))))","2d6e537d":"for column in enumerate(df_num_features):\n    plt.figure(figsize=(30,5))\n    sns.set_theme(style=\"darkgrid\")\n    sns.boxplot(x=column[1], data=  df_num_features)\n    plt.xlabel(column[1],fontsize=18)\n    plt.show()","3cde6ff1":"df_train =pd.concat([df_train,(pd.get_dummies(data = df_train[['Sex','Embarked']], drop_first = True))],axis=1)\ndf_train.drop(['Sex','Embarked'],axis=1,inplace=True)\ndf_train.head()","4875eff6":"df_train['Survived']=df_train['Survived'].astype('int64')\ndf_train['Pclass']=df_train['Pclass'].astype('int64')","88783d8c":"from sklearn.preprocessing import MinMaxScaler","8cf9b43f":"df_num=df_train[['Age','SibSp','Parch','Fare']]","7a7dc36b":"mms = MinMaxScaler()\nmmsfit = mms.fit(df_num)\ndfxz = pd.DataFrame(mms.fit_transform(df_num), columns=df_num.columns)","80a7b77d":"df_train[['Age','SibSp','Parch','Fare']]=dfxz[['Age','SibSp','Parch','Fare']]\ndf_train.head()","5cd4d9ae":"X=df_train.drop(['Survived'],axis=1)\ny=df_train['Survived']","178b3094":"# add a constant column to the dataframe\n# while using the 'Logit' method in the Statsmodels library, the method do not consider the intercept by default\n# we can add the intercept to the set of independent variables using 'add_constant()'\nX = sm.add_constant(X)\n\n# split data into train subset and test subset\n# set 'random_state' to generate the same dataset each time you run the code \n# 'test_size' returns the proportion of data to be included in the testing set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 10, test_size = 0.3)\n\n# check the dimensions of the train & test subset using 'shape'\n# print dimension of train set\nprint('X_train', X_train.shape)\nprint('y_train', y_train.shape)\n\n# print dimension of test set\nprint('X_test', X_test.shape)\nprint('y_test', y_test.shape)","5a557ab3":"# create an empty dataframe to store the scores for various algorithms\nscore_card1 = pd.DataFrame(columns=['Probability Cutoff', 'AUC Score', 'Precision Score', 'Recall Score',\n                                       'Accuracy Score', 'Kappa Score', 'f1-score'])\n\n# append the result table for all performance scores\n# performance measures considered for model comparision are 'AUC Score', 'Precision Score', 'Recall Score','Accuracy Score',\n# 'Kappa Score', and 'f1-score'\n# compile the required information in a user defined function \ndef update_score_card1(model, cutoff):\n    \n    # let 'y_pred_prob' be the predicted values of y\n    y_pred_prob = model.predict(X_test)\n\n    # convert probabilities to 0 and 1 using 'if_else'\n    y_pred = [ 0 if x < cutoff else 1 for x in y_pred_prob]\n    \n    # assign 'score_card' as global variable\n    global score_card1\n\n    # append the results to the dataframe 'score_card'\n    # 'ignore_index = True' do not consider the index labels\n    score_card1 = score_card1.append({'Probability Cutoff': cutoff,\n                                    'AUC Score' : metrics.roc_auc_score(y_test, y_pred),\n                                    'Precision Score': metrics.precision_score(y_test, y_pred),\n                                    'Recall Score': metrics.recall_score(y_test, y_pred),\n                                    'Accuracy Score': metrics.accuracy_score(y_test, y_pred),\n                                    'Kappa Score':metrics.cohen_kappa_score(y_test, y_pred),\n                                    'f1-score': metrics.f1_score(y_test, y_pred)}, \n                                    ignore_index = True)","9278c049":"logreg = sm.Logit(y_train, X_train).fit()\n\n# print the summary of the model\nprint(logreg.summary())","820ffa3c":"# 'aic' retuns the AIC value for the model\nprint('AIC:', logreg.aic)","91a84329":"# consider a list of values for cut-off\ncutoff = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n\n# use the for loop to compute performance measures for each value of the cut-off\n# call the update_score_card() to update the score card for each cut-off\n# pass the model and cut-off value to the function\nfor value in cutoff:\n    update_score_card1(logreg, value)","59d6be89":"# print the score card \nprint('Score Card for Logistic regression:')\n\n# sort the dataframe based on the probability cut-off values ascending order\n# 'reset_index' resets the index of the dataframe\n# 'drop = True' drops the previous index\nscore_card1 = score_card1.sort_values('Probability Cutoff').reset_index(drop = True)\n\n# color the cell in the columns 'AUC Score', 'Accuracy Score', 'Kappa Score', 'f1-score' having maximum values\n# 'style.highlight_max' assigns color to the maximum value\n# pass specified color to the parameter, 'color'\n# pass the data to limit the color assignment to the parameter, 'subset' \nscore_card1.style.highlight_max(color = 'lightblue', subset = ['AUC Score', 'Accuracy Score', 'Kappa Score', 'f1-score'])","cc2d0b79":"# let 'y_pred_prob' be the predicted values of y\ny_pred_prob = logreg.predict(X_train)\n\n# print the y_pred_prob\ny_pred_prob.head()","1cbf791e":"# convert probabilities to 0 and 1 using 'if_else'\ny_pred = [ 0 if x < 0.5 else 1 for x in y_pred_prob]\ny_pred[:5]","3b26c37f":"# let 'y_pred_prob' be the predicted values of y\ny_pred_prob1 = logreg.predict(X_test)\n\n# print the y_pred_prob\ny_pred_prob1.head()","b99d684b":"# convert probabilities to 0 and 1 using 'if_else'\ny_pred1 = [ 0 if x < 0.5 else 1 for x in y_pred_prob1]\ny_pred1[:10]","00ffe511":"# create a confusion matrix\n# pass the actual and predicted target values to the confusion_matrix()\ncm = confusion_matrix(y_test, y_pred1)\n\n# label the confusion matrix  \n# pass the matrix as 'data'\n# pass the required column names to the parameter, 'columns'\n# pass the required row names to the parameter, 'index'\nconf_matrix = pd.DataFrame(data = cm,columns = ['Predicted:0','Predicted:1'], index = ['Actual:0','Actual:1'])\n\n# plot a heatmap to visualize the confusion matrix\n# 'annot' prints the value of each grid \n# 'fmt = d' returns the integer value in each grid\n# 'cmap' assigns color to each grid\n# as we do not require different colors for each grid in the heatmap,\n# use 'ListedColormap' to assign the specified color to the grid\n# 'cbar = False' will not return the color bar to the right side of the heatmap\n# 'linewidths' assigns the width to the line that divides each grid\n# 'annot_kws = {'size':25})' assigns the font size of the annotated text \nsns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = ListedColormap(['lightskyblue']), cbar = False, \n            linewidths = 0.1, annot_kws = {'size':25})\n\n# set the font size of x-axis ticks using 'fontsize'\nplt.xticks(fontsize = 20)\n\n# set the font size of y-axis ticks using 'fontsize'\nplt.yticks(fontsize = 20)\n\n# display the plot\nplt.show()","1c03a87f":"print(classification_report(y_train, y_pred))","3f74b8b0":"print(classification_report(y_test, y_pred1))","6e48faf7":"# the roc_curve() returns the values for false positive rate, true positive rate and threshold\n# pass the actual target values and predicted probabilities to the function\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob1)\n\n# plot the ROC curve\nplt.plot(fpr, tpr)\n\n# set limits for x and y axes\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\n# plot the straight line showing worst prediction for the model\nplt.plot([0, 1], [0, 1],'r--')\n\n# add plot and axes labels\n# set text size using 'fontsize'\nplt.title('ROC curve for Admission Prediction Classifier (RFE Model)', fontsize = 15)\nplt.xlabel('False positive rate (1-Specificity)', fontsize = 15)\nplt.ylabel('True positive rate (Sensitivity)', fontsize = 15)\n\n# add the AUC score to the plot\n# 'x' and 'y' gives position of the text\n# 's' is the text \n# use round() to round-off the AUC score upto 4 digits\nplt.text(x = 0.02, y = 0.9, s = ('AUC Score:', round(metrics.roc_auc_score(y_test, y_pred_prob1),4)))\n                               \n# plot the grid\nplt.grid(True)","9c50ac50":"#defining a score card\nscore_card=pd.DataFrame(columns=['Model_Name','Accuracy(Train)','Accuracy(Test)','Diff_b\/w_train&test(Acc)','AUC_Score','Avg(Acc)'])","a9fc1edd":"# Predicting Cross Validation Score\ndef cross_valid_score(obj):\n    cv_lr = cross_val_score(estimator = obj , X = X_train, y = y_train, cv = 10,scoring='accuracy')\n    return cv_lr.min()","137965c4":"score_card=score_card.append({'Model_Name': 'Logistic Regression',\n                             'Accuracy(Train)': metrics.accuracy_score(y_train, y_pred),\n                             'Accuracy(Test)':metrics.accuracy_score(y_test, y_pred1),\n                             'Diff_b\/w_train&test(Acc)': abs(metrics.accuracy_score(y_train, y_pred)-metrics.accuracy_score(y_test, y_pred1)),\n                             'AUC_Score':metrics.roc_auc_score(y_test, y_pred_prob1),\n                             'Avg(Acc)':cross_valid_score(LogisticRegression())},ignore_index=True)\nscore_card","04d11cc6":"# create a generalized function to calculate the metrics values for train set\ndef get_train_report(model):\n    \n    # for training set:\n    # train_pred: prediction made by the model on the train dataset 'X_train'\n    # y_train: actual values of the target variable for the train dataset\n\n    # predict the output of the target variable from the train data \n    train_pred = model.predict(X_train)\n\n    # return the performace measures on train set\n    return(classification_report(y_train, train_pred))","60e89db6":"# create a generalized function to calculate the performance metrics values for test set\ndef get_test_report(model):\n    \n    # for test set:\n    # test_pred: prediction made by the model on the test dataset 'X_test'\n    # y_test: actual values of the target variable for the test dataset\n\n    # predict the output of the target variable from the test data \n    test_pred = model.predict(X_test)\n\n    # return the classification report for test data\n    return(classification_report(y_test, test_pred))","74d4461d":"# define a to plot a confusion matrix for the model\ndef plot_confusion_matrix(model):\n    \n    # predict the target values using df_test\n    y_pred = model.predict(X_test)\n    \n    # create a confusion matrix\n    # pass the actual and predicted target values to the confusion_matrix()\n    cm = confusion_matrix(y_test, y_pred)\n\n    # label the confusion matrix  \n    # pass the matrix as 'data'\n    # pass the required column names to the parameter, 'columns'\n    # pass the required row names to the parameter, 'index'\n    conf_matrix = pd.DataFrame(data = cm,columns = ['Predicted:0','Predicted:1'], index = ['Actual:0','Actual:1'])\n\n    # plot a heatmap to visualize the confusion matrix\n    # 'annot' prints the value of each grid \n    # 'fmt = d' returns the integer value in each grid\n    # 'cmap' assigns color to each grid\n    # as we do not require different colors for each grid in the heatmap,\n    # use 'ListedColormap' to assign the specified color to the grid\n    # 'cbar = False' will not return the color bar to the right side of the heatmap\n    # 'linewidths' assigns the width to the line that divides each grid\n    # 'annot_kws = {'size':25})' assigns the font size of the annotated text \n    sns.heatmap(conf_matrix, annot = True, fmt = 'd', cmap = ListedColormap(['lightskyblue']), cbar = False, \n                linewidths = 0.1, annot_kws = {'size':25})\n\n    # set the font size of x-axis ticks using 'fontsize'\n    plt.xticks(fontsize = 20)\n\n    # set the font size of y-axis ticks using 'fontsize'\n    plt.yticks(fontsize = 20)\n\n    # display the plot\n    plt.show()","176df606":"# define a function to plot the ROC curve and print the ROC-AUC score\ndef plot_roc(model):\n    \n    # predict the probability of target variable using X_test\n    # consider the probability of positive class by subsetting with '[:,1]'\n    y_pred_prob = model.predict_proba(X_test)[:,1]\n    \n    # the roc_curve() returns the values for false positive rate, true positive rate and threshold\n    # pass the actual target values and predicted probabilities to the function\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n    # plot the ROC curve\n    plt.plot(fpr, tpr)\n\n    # set limits for x and y axes\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n\n    # plot the straight line showing worst prediction for the model\n    plt.plot([0, 1], [0, 1],'r--')\n\n    # add plot and axes labels\n    # set text size using 'fontsize'\n    plt.title('ROC curve ', fontsize = 15)\n    plt.xlabel('False positive rate (1-Specificity)', fontsize = 15)\n    plt.ylabel('True positive rate (Sensitivity)', fontsize = 15)\n\n    # add the AUC score to the plot\n    # 'x' and 'y' gives position of the text\n    # 's' is the text \n    # use round() to round-off the AUC score upto 4 digits\n    plt.text(x = 0.02, y = 0.9, s = ('AUC Score:',round(roc_auc_score(y_test, y_pred_prob),4)))\n\n    # plot the grid\n    plt.grid(True)","5155dbe6":"def update_score_card(model_name,model):\n    global score_card\n    train_pred = model.predict(X_train)\n    test_pred=model.predict(X_test)\n    y_pred_prob = model.predict_proba(X_test)[:,1]\n    score_card=score_card.append({'Model_Name': model_name,\n                             'Accuracy(Train)': metrics.accuracy_score(y_train, train_pred),\n                             'Accuracy(Test)':metrics.accuracy_score(y_test, test_pred),\n                             'Diff_b\/w_train&test(Acc)': abs(metrics.accuracy_score(y_train, train_pred)-metrics.accuracy_score(y_test, test_pred)),\n                             'AUC_Score':metrics.roc_auc_score(y_test, y_pred_prob),\n                             'Avg(Acc)':cross_valid_score(model)},ignore_index=True)\n    return score_card","f1ca31fe":"# instantiate the 'BernoulliNB'\nbnb = BernoulliNB()\nX_train.drop('const',axis=1,inplace=True)\nX_test.drop('const',axis=1,inplace=True)\n# fit the model using fit() on train data\nbnb_model = bnb.fit(X_train, y_train)","d319604a":"plot_confusion_matrix(bnb_model)","939a88c8":"train_report = get_train_report(bnb_model)\n\n# print the performace measures\nprint(train_report)","73bce4d1":"test_report = get_test_report(bnb_model)\n\n# print the performace measures\nprint(test_report)","0cfa1b48":"# call the function to plot the ROC curve\n# pass the bernoulli naive bayes model to the function\nplot_roc(bnb_model)","2a3c8073":"update_score_card('Navie Bayes',bnb_model)","ea874292":"# create a dictionary with hyperparameters and its values\n# n_neighnors: number of neighbors to consider\n# usually, we consider the odd value of 'n_neighnors' to avoid the equal number of nearest points with more than one class\n# pass the different distance metrics to the parameter, 'metric'\ntuned_paramaters = {'n_neighbors': np.arange(1, 25, 2),\n                   'metric': ['hamming','euclidean','manhattan','Chebyshev']}\n \n# instantiate the 'KNeighborsClassifier' \nknn_classification = KNeighborsClassifier()\n\n# use GridSearchCV() to find the optimal value of the hyperparameters\n# estimator: pass the knn model\n# param_grid: pass the list 'tuned_parameters'\n# cv: number of folds in k-fold i.e. here cv = 5\n# scoring: pass the scoring parameter 'accuracy'\nknn_grid = GridSearchCV(estimator = knn_classification, \n                        param_grid = tuned_paramaters, \n                        cv = 5, \n                        scoring = 'accuracy')\n\n# fit the model on X_train and y_train using fit()\nknn_grid.fit(X_train, y_train)\n\n# get the best parameters\nprint('Best parameters for KNN Classifier: ', knn_grid.best_params_, '\\n')","6d95e6a1":"# instantiate the 'KNeighborsClassifier'\n# n_neighnors: number of neighbors to consider\n# default metric is manhattan, and with p=1 it is equivalent to the manhattan metric\nknn_classification = KNeighborsClassifier(n_neighbors =21,p=1)\n\n# fit the model using fit() on train data\nknn_model = knn_classification.fit(X_train, y_train)","465ecca4":"# consider an empty list to store error rate\nerror_rate = []\n\n# use for loop to build a knn model for each K\nfor i in np.arange(1,25,2):\n    \n    # setup a knn classifier with k neighbors\n    # use the 'euclidean' metric \n    knn = KNeighborsClassifier(i, metric = 'manhattan')\n   \n    # fit the model using 'cross_val_score'\n    # pass the knn model as 'estimator'\n    # use 5-fold cross validation\n    score = cross_val_score(knn, X_train, y_train, cv = 5)\n    \n    # calculate the mean score\n    score = score.mean()\n    \n    # compute error rate \n    error_rate.append(1 - score)\n\n# plot the error_rate for different values of K \nplt.plot(range(1,25,2), error_rate)\n\n# add plot and axes labels\n# set text size using 'fontsize'\nplt.title('Error Rate', fontsize = 15)\nplt.xlabel('K', fontsize = 15)\nplt.ylabel('Error Rate', fontsize = 15)\n\n# set the x-axis labels\nplt.xticks(np.arange(1, 25, step = 2))\n\n# plot a vertical line across the minimum error rate\nplt.axvline(x = 21, color = 'red')\n\n# display the plot\nplt.show()","e46da180":"# call the function to plot the confusion matrix\n# pass the knn model to the function\nplot_confusion_matrix(knn_model)","7b69ce0c":"train_report = get_train_report(knn_model)\n\n# print the performace measures\nprint(train_report)","e853d0f6":"# compute the performance measures on test data\n# call the function 'get_test_report'\n# pass the knn model to the function\ntest_report = get_test_report(knn_model)\n\n# print the performace measures\nprint(test_report)","de995c15":"# call the function to plot the ROC curve\n# pass the knn model to the function\nplot_roc(knn_model)","fc82c0ba":"update_score_card('KNeighbors Classifier',knn_model)","bea7c106":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree\nfrom sklearn.model_selection import GridSearchCV ","7e7d6432":"tuned_paramaters = [{'criterion': ['entropy', 'gini'], \n                     'max_depth': [2,4,6,8,10],\n                     'max_features': [\"sqrt\", \"log2\"],\n                     'min_samples_split': [2,4,6,8,10],\n                     'min_samples_leaf': [2,4,6,8,10],\n                     'max_leaf_nodes': [2,4,6,8,10]}]\n \n# instantiate the 'DecisionTreeClassifier' \n# pass the 'random_state' to obtain the same samples for each time you run the code\ndecision_tree_classification = DecisionTreeClassifier(random_state = 10)\n\n# use GridSearchCV() to find the optimal value of the hyperparameters\n# estimator: pass the decision tree classifier model\n# param_grid: pass the list 'tuned_parameters'\n# cv: number of folds in k-fold i.e. here cv = 5\ntree_grid = GridSearchCV(estimator = decision_tree_classification, \n                         param_grid = tuned_paramaters, \n                         cv = 5)\n\n# fit the model on X_train and y_train using fit()\ntree_grid_model = tree_grid.fit(X_train, y_train)\n\n# get the best parameters\nprint('Best parameters for decision tree classifier: ', tree_grid_model.best_params_, '\\n')","627b0d28":"dt_model = DecisionTreeClassifier(criterion = tree_grid_model.best_params_.get('criterion'),\n                                  max_depth = tree_grid_model.best_params_.get('max_depth'),\n                                  max_features = tree_grid_model.best_params_.get('max_features'),\n                                  max_leaf_nodes = tree_grid_model.best_params_.get('max_leaf_nodes'),\n                                  min_samples_leaf = tree_grid_model.best_params_.get('min_samples_leaf'),\n                                  min_samples_split = tree_grid_model.best_params_.get('min_samples_split'),\n                                  random_state = 10)\n\n# use fit() to fit the model on the train set\ndt_model = dt_model.fit(X_train, y_train)","e450170c":"from sklearn.tree import plot_tree\nplt.figure(figsize=(60,30))\nplot_tree(dt_model, filled=True);","9dfa2b64":"# call the function to plot the confusion matrix\n# pass the knn model to the function\nplot_confusion_matrix(dt_model)","69b2ed97":"train_report = get_train_report(dt_model)\n\n# print the performace measures\nprint(train_report)","331f3a7a":"test_report = get_test_report(dt_model)\n\n# print the performace measures\nprint(test_report)","a364a7de":"# call the function to plot the ROC curve\n# pass the decision model to the function\nplot_roc(dt_model)","060f832f":"update_score_card('Decision Tree Classifier',dt_model)","f4a58391":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import StackingClassifier\nfrom xgboost import XGBClassifier","ea4f8005":"tuning_parameters = {'learning_rate': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],'n_estimators' : [10,20,30,40,50]}\nada_model = AdaBoostClassifier()\n\n# use GridSearchCV() to find the optimal value of the hyperparameters\n# estimator: pass the XGBoost classifier model\n# param_grid: pass the list 'tuned_parameters'\n# cv: number of folds in k-fold i.e. here cv = 3\n# scoring: pass a measure to evaluate the model on test set\nada_grid = GridSearchCV(estimator = ada_model, param_grid = tuning_parameters, cv = 3, scoring = 'roc_auc')\n\n# fit the model on X_train and y_train using fit()\nada_grid.fit(X_train, y_train)\n\n# get the best parameters\nprint('Best parameters for XGBoost classifier: ', ada_grid.best_params_, '\\n')","4677821f":"# instantiate the 'AdaBoostClassifier'\n# n_estimators: number of estimators at which boosting is terminated\n# pass the 'random_state' to obtain the same results for each code implementation\nada_model = AdaBoostClassifier(n_estimators = ada_grid.best_params_.get('n_estimators'), random_state = 10,learning_rate= ada_grid.best_params_.get('learning_rate'))\n\n# fit the model using fit() on train data\nada_model.fit(X_train, y_train)","116b7cc3":"plot_confusion_matrix(ada_model)","c3a301ad":"train_report = get_train_report(ada_model)\n\n# print the performace measures\nprint(train_report)","e82b7399":"test_report = get_test_report(ada_model)\n\n# print the performance measures\nprint(test_report)","493da995":"plot_roc(ada_model)","3062cf7b":"update_score_card('Ada Boost Classifier',ada_model)","a7643ccb":"tuning_parameters = {'learning_rate': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],'n_estimators':[10, 30, 50, 70, 90],'max_depth': [10, 15, 20]}\ngboost_model = GradientBoostingClassifier()\ngb_grid = GridSearchCV(estimator = gboost_model, param_grid = tuning_parameters, cv = 3, scoring = 'roc_auc')\ngb_grid.fit(X_train, y_train)\nprint('Best parameters for GBoost classifier: ', gb_grid.best_params_, '\\n')","22a391ef":"gboost_model = GradientBoostingClassifier(n_estimators = gb_grid.best_params_.get('n_estimators'), max_depth = gb_grid.best_params_.get('max_depth'), random_state = 10,learning_rate=gb_grid.best_params_.get('learning_rate'))\ngboost_model.fit(X_train, y_train)","8359f0e1":"plot_confusion_matrix(gboost_model)","f7733d09":"train_report = get_train_report(gboost_model)\n\n# print the performace measures\nprint(train_report)","75b20d38":"test_report = get_test_report(gboost_model)\n\n# print the performance measures\nprint(test_report)","7c16056f":"plot_roc(gboost_model)","edda77fa":"update_score_card('Gradient Boosting Classifier',gboost_model)","df6d12f7":"tuning_parameters = {'learning_rate': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n                     'max_depth': [1,3,5,7,9],\n                     'gamma': [0, 1, 2, 3, 4]}\nxgb_model = XGBClassifier()\nxgb_grid = GridSearchCV(estimator = xgb_model, param_grid = tuning_parameters, cv = 3, scoring = 'roc_auc',verbose=0)\nxgb_grid.fit(X_train, y_train)\n\n# get the best parameters\nprint('Best parameters for XGBoost classifier: ', xgb_grid.best_params_, '\\n')","70cb1a78":"xgb_model = XGBClassifier(learning_rate= xgb_grid.best_params_.get('learning_rate'), gamma = xgb_grid.best_params_.get('gamma'),max_depth=xgb_grid.best_params_.get('max_depth'),verbosity=0)\n\n# fit the model using fit() on train data\nxgb_model.fit(X_train, y_train)","e8fd1bea":"plot_confusion_matrix(xgb_model)","d941fb90":"train_report = get_train_report(xgb_model)\n\n# print the performace measures\nprint(train_report)","e025eb8a":"test_report = get_test_report(xgb_model)\n\n# print the performance measures\nprint(test_report)","6789f27e":"plot_roc(xgb_model)","410b5aa7":"update_score_card('Extreme Gradient Boosting Classifier',xgb_model)","392feeec":"from sklearn.ensemble import StackingClassifier","207b86f7":"# consider the various algorithms as base learners\nbase_learners = [('KNN_model', KNeighborsClassifier(n_neighbors = 21, metric = 'manhattan')),\n                 ('xgb_model',XGBClassifier(learning_rate= xgb_grid.best_params_.get('learning_rate'), gamma = xgb_grid.best_params_.get('gamma'),max_depth=xgb_grid.best_params_.get('max_depth'),verbosity=0)),\n                 ('GradientBoosting_model', GradientBoostingClassifier(n_estimators = gb_grid.best_params_.get('n_estimators'), max_depth = gb_grid.best_params_.get('max_depth'), random_state = 10,learning_rate=gb_grid.best_params_.get('learning_rate')))]\n\n# initialize stacking classifier \n# pass the base learners to the parameter, 'estimators'\n# pass the Naive Bayes model as the 'final_estimator'\/ meta model\nstack_model = StackingClassifier(estimators = base_learners, final_estimator = AdaBoostClassifier(n_estimators = ada_grid.best_params_.get('n_estimators'), random_state = 10,learning_rate= ada_grid.best_params_.get('learning_rate')))\n\n# fit the model on train dataset\nstack_model.fit(X_train, y_train)","b06566b9":"# call the function to plot the confusion matrix\n# pass the stack model to the function\nplot_confusion_matrix(stack_model)","dd0bb480":"train_report = get_train_report(stack_model)\n\n# print the performace measures\nprint(train_report)","c5054491":"# compute the performance measures on test data\n# call the function 'get_test_report'\n# pass the XGBoost model to the function\ntest_report = get_test_report(stack_model)\n\n# print the performance measures\nprint(test_report)","3beb6e77":"# call the function to plot the ROC curve\n# pass the stack model to the function\nplot_roc(stack_model)","8fe3ec22":"update_score_card('Stacking model',stack_model)","53d44039":"score_card.sort_values('Diff_b\/w_train&test(Acc)')","8d9bb4e7":"df_test.head()","af3c475e":"df_test.shape","dd49c021":"df_test.isna().sum()","140ca72d":"df_test.duplicated().sum()","8fdf1711":"df_test.drop(['PassengerId','Name','Ticket','Cabin'],axis=1,inplace=True)","37024dfc":"df_test.fillna(df_test.median(),inplace=True)","8f99b385":"df_test.isna().sum()","218ce0e1":"df_test.dtypes","4b307585":"df_test =pd.concat([df_test,(pd.get_dummies(data = df_test[['Sex','Embarked']], drop_first = True))],axis=1)\ndf_test.drop(['Sex','Embarked'],axis=1,inplace=True)\ndf_test.head()","5c8b9ea5":"df_num=df_test[['Age','SibSp','Parch','Fare']]\nmms = MinMaxScaler()\nmmsfit = mms.fit(df_num)\ndfxz = pd.DataFrame(mms.fit_transform(df_num), columns=df_num.columns)","b60fc67c":"df_test[['Age','SibSp','Parch','Fare']]=dfxz[['Age','SibSp','Parch','Fare']]\ndf_test.head()","a4239b80":"act_test=df_g.drop('PassengerId',axis=1)","b0685fe3":"y_val=ada_model.predict(df_test)","4ef2f929":"print(classification_report(act_test,y_val))","d6072f9b":"submission=pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmission['PassengerId'] = df_g['PassengerId']\nsubmission['Survived'] = y_val","20ebdfa2":"submission.to_csv('submissions.csv', header=True, index=False)","82ab3bb4":"**Inferences:** Even though out of 891 passengers 577 were male but only few of them were survived.Almost every female passenger was survived.","33a370f5":"**Interpretation:** The red dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner).<br>\nFrom the above plot, we can see that our classifier (logistic regression with features obtained from RFE method) is away from the dotted line; with the AUC score **0.8543**.","43f66658":"#### Building an Adaboost model on a training dataset.","3bb99b6d":"**Visualising missing values using Heatmap**","73347914":"#### A function to plot the ROC curve.","54f4a575":"The features Cabin,Age and Embarked has some missing values.\n\n1)Cabin has more than 50% of the missing values so,we will drop the feature cabin.\n\n2)Since the Age have many outliers we will replace the null values using median.\n\n3)The feature Embarked is categorical variable we will replace the null values using mode.","44298a1c":"<a id=\"6.6\"><\/a>\n## 6.6 Gradient Boosting","10f63220":"### 4.2.1 Summary Statistics <a id='Summary_Statistics'><\/a>","393804f1":"We can use the AIC value to compare different models created on the same dataset.","e0a36cd0":"#### A generalized function to calculate the performance metrics for the train set.","dc4528ca":"#### Train Report","e48a3cca":"#### Update the score card","fbaecbad":"#### Building an Stacking model on a training dataset","c143d1d1":"#### Update the score card","b9ef5fcc":"### 4.1.2 Data Types <a id='Data_Types'><\/a>","3551d02d":"#### Creating a generalized function to create a dataframe containing the scores for the models.","c4c4c6c7":"# 7. Conclusion<a id=\"7\"><\/a>","5cb14cf6":"## 5.4 Spliting the data into train and test  <a id='5.4'><\/a>","bb326562":"The dataset  now contains **8 object columns, 2 int column and 2 float columns**","783c6be5":"**Train Report**","f2089e2b":"# 4. Exploratory Data Analysis <a id='data_preparation'><\/a>","0cf1de94":"#### Building an Gradient boost model on a training dataset","18444a3c":"**Inferences:** \n\n1)Out of 891 passengers the survived people majority belongs to 1st Pclass and in Sex most of them were female. \n\n2)Even though out of 891 passengers 577 were male but only few of them were survived.Almost every female passenger was survived.\n\n3)Most of the people embarked from `Cherbourg`.","28e16aa8":"**Visualizing outliers using Boxplots**","f30f298e":"## 5.1 Outliers <a id='out'><\/a>","8226905b":"## 4.2 Understanding the Dataset <a id='Data_Understanding'><\/a>","2f08d02b":"#### Test Report","823dd353":"#### Line plot to see the error rate for each value of K using euclidean distance as a metric of KNN model","7a889bd0":"#### Building a model using the tuned hyperparameters.","6d85f6ba":"**predictions on the test set**","537111f5":"**Interpretation:** \n\nFrom the above plot, we can see that our classifier (knn_model with n_neighbors = 21) is away from the dotted line; with the AUC score **0.8729**.","a70ad45c":"**Numeric Variables**","e33a8244":"**Visualising using Heatmap**","48eddc1c":"#### Train report","9541a896":"**Interpretation:** \n\nFrom the above plot, we can see that our classifier is away from the dotted line; with the AUC score **0.8677**.","16ac7575":"**ROC curve.**","54f247db":"### 4.1.6 Final Dataset <a id='final_dataset'><\/a>","e3651057":"#### ROC Curve","c6db1b86":"**Interpretation:** \n\nFrom the above plot, we can see that our classifier is away from the dotted line; with the AUC score **0.8588**.","8a759695":"## 5.3 Feature Scaling<a id='5.3'><\/a>","ccf982ef":"<table align=\"center\" width=100%>\n    <tr>\n        <td width=\"30%\">\n            <img src=\"https:\/\/th.bing.com\/th\/id\/R21d1efc9c0464f1353ca7896f1f2b844?rik=bhEx93udPu%2fTHw&riu=http%3a%2f%2fcdn.psfk.com%2fwp-content%2fuploads%2f2012%2f04%2ftitanic.jpg&ehk=hlnxFfft2WriMBmClCabcCxyfeEOfiOEPHnWx%2fc75DE%3d&risl=&pid=ImgRaw\">  \n        <\/td>\n        <td>\n            <div align=\"center\">\n                <font color=\"#21618C\" size=24px>\n                    <b>Titanic - Machine Learning from Disaster\n                    <\/b>\n                <\/font>\n            <\/div>\n        <\/td>\n    <\/tr>\n<\/table>","1e8fbd8d":"Now, There are **No missing values** present in the dataset","23e716a7":"# 5. Data Preprocessing <a id='data_pre'><\/a>","338fea71":"**Interpretation:** \n\nFrom the above plot, we can see that our classifier is away from the dotted line; with the AUC score **0.8824**.","8af35dcc":"#### Tune the Hyperparameters (GridSearchCV)","6079dd8c":"#### Update the score card","dfeb494b":"**Interpretation:** The above dataframe shows that, the model cut_off probability 0.5, returns the highest AUC score, f1-score, kappa score and accuracy.","245a5a95":"#### Score card","3b650912":"#### A function to update the score card","57ac0873":"**Interpretation:** \n\n Ada Boost classifer model is the best model to predict the survived passengers. \n \n This model has the least difference of accuracy between the train and test data.\n \n Hence we can say that the model is neither overfit nor underfit.","96df90b4":"**Calculate the AIC (Akaike Information Criterion) value.**\n\nIt is a relative measure of model evaluation. It gives a trade-off between model accuracy and model complexity.","fe839a7c":"#### Train report","7cf6651f":"#### Confusion matrix.","83f512b3":"#### ROC Curve","70219e20":"<img src= \"https:\/\/th.bing.com\/th\/id\/Rc13cc940cd4a34c4152268d74110fdb3?rik=yNkPpS%2fhtX%2bYuA&riu=http%3a%2f%2fmedia1.giphy.com%2fmedia%2fOJw4CDbtu0jde%2fgiphy.gif&ehk=455XviuGnp%2feDyM5UqfHwHAEUUL%2f5qRtp8ecyOtmd%2bY%3d&risl=&pid=ImgRaw\" style='width: 1000px;'>\n","5b945832":"#### ROC Curve","589f00aa":"## Data Dictionary","d53e0288":"#### Test report","c3062fa2":"## 4.1 Preparing the Dataset <a id='Data_Preparing'><\/a>","77a15ada":"**ROC curve.**","6e9400ac":"##  Missing Values Replacement","c6f14a27":"#### Train Report","caa3f678":"From the above classification reports,we can infer that there is a difference when compared to test and train reports.\nHence we conclude that the model is overfitted.","685c72a1":"**Decision tree with tuned hyperparameters.**","dd3e6e76":"The final dataset has **891 records and 11 features with no missing and duplicate values**","653d6435":"#### Test report","ba36c05d":"# 1. Import Libraries <a id='import_lib'><\/a>","88e89acd":"**Interpretation:** \n\nFrom the above plot, we can see that our classifier is away from the dotted line; with the AUC score **0.8551**.","022f015c":"#  [](http:\/\/)THANK YOU :)","51f4b168":"plt.figure(figsize = (12, 7))\nsns.boxplot(x = 'Sex', y = 'Survived', data = df_train, palette = 'winter')","ea425f3a":"#### Confusion matrix","02268fdc":"# 2. Set Options <a id='set_options'><\/a>","ddf7a8db":"**Interpretation:** The `Pseudo R-squ.` obtained from the above model summary is the value of `McFadden's R-squared`. This value can be obtained from the formula:\n\n<p style='text-indent:25em'> <strong> McFadden's R-squared = $ 1 - \\frac{Log-Likelihood}{LL-Null} $<\/strong> <\/p>\n\nWhere,<br>\nLog-Likelihood: It is the maximum value of the log-likelihood function<br>\nLL-Null: It is the maximum value of the log-likelihood function for the model containing only the intercept \n\nThe LLR p-value is less than 0.05, implies that the model is significant.","4b37e9a5":"**ROC curve.**","826729b2":"<a id=\"6.7\"> <\/a>\n## 6.7 Extreme Gradient Boosting (XGB)","3e96546f":"#### Train Report","5c0e8ac4":"#### Update the score card","06a47319":"<a id=\"6.4\"><\/a>\n## 6.4 Decision Tree for Classification","855d99f7":"#### Building a knn model on a training dataset using euclidean distance.","7994721d":"**Inferences:**\n1)Most of the survived passengers were females(550).\n\n2)Most of the passengers belonged to 3rd class.\n\n3)Out of 891 passengers most of them were male.\n\n4)Most of the people embarked from Southampton.","d98fd87a":"**Inference:** \n\nFrom the above classification reports,we can infer that there is a difference when compared to test and train reports.\nHence we conclude that the model is underfitted.\n","c85d00ff":"## Table of Contents\n\n1. **[Import Libraries](#import_lib)**\n2. **[Set Options](#set_options)**\n3. **[Read Data](#Read_Data)**\n4. **[Exploratory Data Analysis](#data_preparation)**\n    - 4.1 - [Preparing the Dataset](#Data_Preparing)\n        - 4.1.1 - [Data Dimension](#Data_Shape)\n        - 4.1.2 - [Data Types](#Data_Types)\n        - 4.1.3 - [Missing Values](#Missing_Values)\n        - 4.1.4 - [Duplicate Data](#duplicate)\n        - 4.1.5 - [Final Dataset](#final_dataset)\n    - 4.2 - [Understanding the Dataset](#Data_Understanding)\n        - 4.2.1 - [Summary Statistics](#Summary_Statistics)\n        - 4.2.2 - [Correlation](#correlation)\n        - 4.2.3 - [Analyze Categorical Variables](#analyze_cat_var)\n        - 4.2.4 - [Anaylze Target Variable](#analyze_tar_var)\n        - 4.2.5 - [Analyze Relationship Between Target and Independent Variables](#analyze_tar_ind_var)\n5. **[Data Pre-Processing](#data_pre)**\n    - 5.1 - [Outliers](#out)\n        - 5.1.1 - [Discovery of Outliers](#dis_out)\n        - 5.1.2 - [Removal of Outliers](#rem_out)\n        - 5.1.3 - [Rechecking of Correlation](#rec_cor)\n    - 5.2 - [Categorical Encoding](#cat_enc)\n    - 5.3 - [Feature Scaling](#5.3)\n    - 5.4 - [Spliting the data into train and test](#5.4)\n6. **[Model Building](#6)**\n    - 6.1 - [Logistic Regression (Fiull Model)](#6.1)\n    - 6.2 - [Naive Bayes Algorithm](#6.2)\n    - 6.3 - [K-Nearest Neighbors (KNN)](#6.3)\n    - 6.4 - [Decision Tree for classification](#6.4)\n    - 6.5 - [ADA Boosting](#6.5)\n    - 6.6 - [Gradient Boosting ](#6.6)\n    - 6.7 - [Extreme Gradient Boosting](#6.7)\n    - 6.8 - [Stacking Generalization](#6.8)\n7. **[Conclusion](#7)**\n8. **[Predicting for test data](#8)**","fe7cd296":"### 4.1.4 Duplicate Data <a id='duplicate'><\/a>","565b1ea6":"#### Tune the Hyperparameters (GridSearchCV)","aa8bce93":"#### Confusion matrix","fd0faf4f":"**Interpretation:** Even though the overall model is significant but there are some attributes which are insignificant.","15580363":"#### Confusion Matrix","df3558f7":"**1. PassengerId** is the unique id of the row and it doesn't have any effect on target\n\n**2. Survived** is the target variable we are trying to predict (0 or 1):\n    1 = Survived\n    0 = Not Survived\n    \n**3. Pclass (Passenger Class)** is the socio-economic status of the passenger and it is a categorical ordinal feature which      has 3 unique values (1, 2 or 3):\n        1 = Upper Class\n        2 = Middle Class\n        3 = Lower Class\n        \n**4. Name, Sex and Age** are self-explanatory\n\n**5.SibSp** is the total number of the passenger's siblings and spouse\n\n**6.Parch** is the total number of the passenger's parents and children\n\n**7.Ticket** is the ticket number of the passenger\n\n**8.Fare** is the passenger fare\n\n**9.Cabin** is the cabin number of the passenger\n\n**10.Embarked** is port of embarkation and it is a categorical feature which has 3 unique values (C, Q or S):\n       C = Cherbourg\n       Q = Queenstown\n       S = Southampton","66bba634":"#### ROC Curve","813e5e34":"In this dataset we have 891 records across 12 features","19c004b1":"**Interpretation:** \n\nFrom the above plot, we can see that our classifier is away from the dotted line; with the AUC score **0.8599**.","5a6bd196":"#### Building an Extreme Gradient boost model on a training dataset","89f875c9":"# 8. Predicting for test data<a id=\"8\"><\/a>","7bd8f87a":"#### A function to plot the confusion matrix.","fb68ce52":"#### Tune the Hyperparameters (GridSearchCV)","99d85dea":"### 4.2.3 Analyse Categorical Variables <a id='analyze_cat_var'><\/a>","5b0aeaf1":"#### Update the score card","a1bb4d9d":"### 4.2.2 Correlation <a id='correlation'><\/a>","0f6afe01":"**Inferences:**\n1)Most of the 1st class passengers were aged 50+\n\n2)Most of the 3rd class passengers were aged 20-35\n\n3)Most of the 2nd class passengers were aged 36-50","1880fc58":"**Test Report**","67ae9d4a":"#### Test Report","8f9f80f7":"From the above classification reports,we can infer that there is a difference when compared to test and train reports.\nHence we conclude that the model is overfitted.","7660f333":"#### Train report","37b4923e":"From the above classification reports,we can infer that there is no much difference when compared to test and train reports.","d900dc46":"Stacking is a machine learning technique that takes several classification or regression models and uses their predictions as the input for the meta-classifier (final classifier) or meta-regressor (final regressor).\n\nAs we are using the distance-based algorithm like KNN, we scale the data before applying the stacking technique.","14c57497":"<a id=\"logit\"><\/a>\n## 6.1 Logistic Regression (Full Model)","672c8dfa":"### 5.1.1 Discovery of Outliers<a id='dis_out'><\/a>","6e7d4f70":"#### Confusion matrix","b72681bb":"**Tabulate the performance measures for different cut-offs.**\n\nNow, let us consider a list of values as cut-off and calculate the different performance measures.","56543a71":"**Interpretation:** We can see that the optimal value of K (= 17) obtained from the GridSearchCV() results in a lowest error rate. ","5152e075":"**Interpretation:**\n\nBased on the above classification report we can conculde that our best model predicts the survived passengers with 94% accuracy.","018bb776":"\n#### Finding Hyperparameters using GridSearchCV (Decision Tree)","0c047c7d":"#### ROC Curve","10cda14b":"<a id=\"logit\"><\/a>\n# 6. Model Building","0d676136":"### 4.1.1 Data Dimensions <a id='Data_Shape'><\/a>","614e1677":"From the above classification reports,we can infer that there is a difference when compared to test and train reports.\nHence we conclude that the model is overfitted.","e05843a5":"**Interpretation:**\nFrom the above plot, we can see that our classifier (Multinomial NaiveBayes) is away from the dotted line; with the AUC score **0.8098**","da39185a":"## 5.2 Categorical Encoding<a id='cat_enc'><\/a>","c171c3e4":"<a id=\"6.5\"><\/a>\n## 6.5 AdaBoost","7f43a10c":"**Visual proof that there are no missing values**","896d27df":"#### Identify the Best Cut-off Value","76bfdf4f":"### 4.2.5 Analyse Relationship between Target and Independent Variables <a id='analyze_tar_ind_var'><\/a>","3f519a9e":"#### Confusion matrix.","4f6c07ee":"# 3. Read Data <a id='Read_Data'><\/a>","19464a8b":"<a id=\"6.8\"><\/a>\n# 6.8 Stack Generalization  ","f7c8edd6":"#### Test report","a6aecd4b":"<a id=\"6.2\"><\/a>\n## 6.2 Naive Bayes Algorithm ","f949169b":"### Pre-defined functions","8e3434a8":"<a id=\"6.3\"><\/a>\n## 6.3 K Nearest Neighbors (KNN)","7edcd20f":"**Identifying outliers using IQR**","b16c741d":"#### Confusion matrix.","141f64ec":"#### Update the score card","b5c34d40":"### 4.1.3 Missing Values <a id='Missing_Values'><\/a>","389c3920":"#### Test Report","3a69e747":"#### Optimal Value of hyper parameter (using GridSearchCV)","7dc7f7a4":"As per the data description the features `PassengerId`,`Survived`and `Pclass` needs to be converted into object datatype.","c1e88ad6":"#### Update the score card","b5caf188":"Since the target variable can take only two values either 0 or 1. We decide the cut-off of 0.5. i.e. if 'y_pred_prob' is less than 0.5, then consider it to be 0 else consider it to be 1.","2b680471":"**Predictions on the train set**","cb253868":"#### Building a naive bayes model on a training dataset.","5e27e24f":"From the above classification reports,we can infer that there is a difference when compared to test and train reports.\nHence we conclude that the model is underfitted.","828bd1f6":"#### Train Report","d9823313":"#### Confusion matrix","08368901":"**Categorical Variables**","ccd79073":"There are 0 duplicate rows in the dataset.","748c7e20":"#### Create a generalized function to calculate the performance metrics for the test set.","c6cd2171":"**Inferences:**\n\nThere are no highly correlated variables present in the dataset.","4ebcf4d4":"## TEAM ACCESS DENIED:\n#### 1) THATHA SAI NISCHITHA\n#### 2) LAVANYA A\n#### 3) AKSHAY S SUTAGATTI","de378b9e":"From the above classification reports,we can infer that there is a difference when compared to test and train reports.\nHence we conclude that the model is underfitted.","b63cde74":"#### Test Report","d242747f":"**Inferences:** From the data,The survived passengers are 61.62% and passengers who did not survive are 38.38% .The target column is balanced.","3ff481db":"### 4.2.4 Analyse Target Variable <a id='analyze_tar_var'><\/a>"}}