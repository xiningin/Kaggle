{"cell_type":{"3a172eb9":"code","2fd64bc1":"code","990a3bcb":"code","da3d9ad7":"code","5604662a":"code","421f8956":"code","42691d20":"code","e69271b4":"code","8f8bf97b":"code","4afbedd5":"code","0e98878d":"code","75af7a38":"code","843587d3":"code","254cb161":"code","8a45f559":"code","06b8786b":"code","3396df70":"code","9d07b627":"code","efb51ac4":"code","5e5de314":"code","310b5ddc":"code","6f147289":"code","f59daf78":"code","224374ce":"code","00398559":"code","0adffacb":"code","b4e7c678":"code","ac62cbda":"code","1d5057c2":"code","f3ec1045":"markdown","bad138c2":"markdown","1282942c":"markdown","f512dffc":"markdown","d268168e":"markdown","bc12f1df":"markdown","15fe74cc":"markdown","d0667e98":"markdown","d436e292":"markdown","a39eadfb":"markdown","53fdff06":"markdown","f5f96c45":"markdown","238ea42d":"markdown","0309025e":"markdown","6e9c108b":"markdown","aa0c2dc0":"markdown","31c3a701":"markdown","5d3d5a35":"markdown","a51599ae":"markdown","6d039aec":"markdown","cd32217d":"markdown","b49a4018":"markdown","1fd6661e":"markdown","3b4ee227":"markdown","0f2edc32":"markdown","1a731093":"markdown","e19d0b1a":"markdown"},"source":{"3a172eb9":"#Import the libraries\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow_addons as tfa","2fd64bc1":"#Load the datasets\nx_train = np.loadtxt(\"..\/input\/linear-regression\/linearX.csv\")\ny_train = np.loadtxt(\"..\/input\/linear-regression\/linearY.csv\")\n\n#Find the dimensions\nprint(f\"Training set dimensions:{x_train.shape} x {y_train.shape}\")\n\n#Expand the data dimensions and add a dummy axis of ones\nx_train = np.expand_dims(x_train, axis=0)\nx_train = np.vstack((np.ones(x_train.shape), x_train))\n\ny_train = np.expand_dims(y_train, axis=0)\n\n#Finalize input\/output dimensions\ninput_dims = x_train.shape[0]\noutput_dims = y_train.shape[0]\nn_samples = y_train.shape[1]\n\nprint(f\"\\nRevised training set dimensions:{x_train.shape} x {y_train.shape}\")\nprint(f\"Input\/output dimensions: ({input_dims} x m), ({output_dims} x m)\")","990a3bcb":"def compute_cost(X, Y, W):\n    #[h(X) = w.T X= w0x0 + w1x1 = w1x1 + w0, with x0 = 1]\n    #Here, X is an nx2 matrix and W is a 2x1 vector. h = XW is an nx1 vector\n    ### YOUR CODE GOES HERE ###\n    h =               #Hint: h = w0x0 + w1x1 = w1x1 + w0, because x0 = 1\n    ### END CODE ###\n    \n    #cost = mean-squared error\n    delta = h - Y              #Error\n    ### YOUR CODE GOES HERE ###\n    J = ? \/ 2   #Square\n    cost = np.mean(?)          #Mean\n    ### END CODE ###\n\n    #d\/dw_i(J) = d\/dw_i((h - y)^2 \/ 2) = (h - y) d\/dw_i(w0x0 + w1x1 - y) = (h - y)x_i = delta_i * x_i\n    ### YOUR CODE GOES HERE ###\n    #Hint: the gradients are a mean of all the delta_i * x_i terms. We also need to expand their dimension.\n    gradients = np.expand_dims(np.mean(?, axis=1), axis=0)\n    ### END CODE ###\n\n    return (cost, gradients)\n\ndef update_w(W, gradients, alpha):\n    ### YOUR CODE GOES HERE ###\n    return\n    ### END CODE ###\n\n\ndef linear_regression(X, Y, learning_rate=0.01):\n    #Initialize W with zeroes.\n    W = np.expand_dims(np.zeros(input_dims), axis=0)\n    \n    #Generate logs of parameters over iterations\n    log = {\"alpha\": learning_rate,\n           \"learning_steps\": 0,\n           \"epochs\": 0,\n           \"train_loss\": [],\n           \"val_loss\": [],\n           \"w\": []}\n    \n    epoch = 0\n    cost = -1\n    cost_delta = 999  #Initialize to any high random value.\n    \n    #Absolute value function\n    abs = lambda x: x if x >= 0 else -x\n    \n    while cost_delta > 0.0001:\n        epoch += 1\n        #Perform the learning step\n        ### YOUR CODE GOES HERE ###\n        _, gradients =           #Hint: compute the cost\n        W =                             #Hint: update the weights\n        ### END CODE ###\n        \n        #Record the cost\n        train_cost, _ = compute_cost(X, Y, W)\n        cost_delta = abs(train_cost - cost)\n        cost = train_cost\n\n        #Log the step\n        log[\"learning_steps\"] += 1\n        log[\"w\"].append(W)\n        log[\"train_loss\"].append(cost)\n        \n    log[\"epochs\"] = epoch\n    return W, log\n\ndef get_3d_meshgrid(W, margins):\n    m0, m1 = margins\n    W = W.flatten()\n    #Adjust margins as per requirement.\n    W0 = np.linspace(W[0] - m0, W[0] + m0, 100)\n    W1 = np.linspace(W[1] - m1, W[1] + m1, 100)\n    X, Y = np.meshgrid(W0, W1)\n    Z = np.zeros((W0.size,  W1.size))\n\n    for i, w0 in enumerate(W0):\n        for j, w1 in enumerate(W1):\n            w = np.expand_dims([w0, w1], axis=0)\n            Z[i, j], _ = compute_cost(x_train, y_train, w)\n    return(X, Y, Z)","da3d9ad7":"#Plot the 3D surface of cost v\/s W\ndef plot_annotated_cost_surface(W, logs, y_label, margins=(0.02, 0.036)):\n    X, Y, Z = get_3d_meshgrid(W, margins)\n    \n    fig = plt.figure(figsize=(12,8))\n    graph = plt.subplot(projection='3d')\n        \n    #Plot the 3D surface\n    graph.plot_surface(X, Y, Z, cmap=cm.winter)\n    graph.set_xlim(X.min(), X.max())\n    graph.set_ylim(Y.min(), Y.max())\n    graph.set_zlim3d(zmax=Z.max())\n    graph.set_title('Loss Surface & Learning Curve')\n    graph.set_xlabel('W0')\n    graph.set_ylabel('W1')\n    graph.set_zlabel('J(W)')\n            \n    #Annotate the surface with learning curves\n    for label, log in logs.items():\n        W_array = np.vstack(tuple(log[\"w\"]))\n        if (len(logs) == 1):\n            graph.plot(W_array[:, 0], W_array[:, 1], log[\"train_loss\"], \"ro-\", linewidth=3, label=f\"{y_label} = {label}\")\n        else:\n            graph.plot(W_array[:, 0], W_array[:, 1], log[\"train_loss\"], \"o-\", linewidth=3, label=f\"{y_label} = {label}\")\n    graph.legend()\n    plt.show()","5604662a":"#Plot the contour of cost v\/s W\ndef plot_annotated_cost_contour(W, logs, y_label, margins=(0.06, 0.036)):\n    X, Y, Z = get_3d_meshgrid(W, margins)\n    \n    fig = plt.figure(figsize=(12,8))\n    graph = plt.subplot()\n    \n    #Plot the 3D surface\n    graph.contourf(X, Y, Z, cmap=cm.winter)\n    graph.set_xlim(X.min(), X.max())\n    graph.set_ylim(Y.min(), Y.max())\n    graph.set_title('Loss Contour & Learning Curve')\n    graph.set_xlabel('W0')\n    graph.set_ylabel('W1')\n        \n    #Annotate the surface with learning curves\n    for label, log in logs.items():\n        W_array = np.vstack(tuple(log[\"w\"]))\n        if (len(logs) == 1):\n            graph.plot(W_array[:, 0], W_array[:, 1], \"ro-\", linewidth=3, label=f\"{y_label} = {label}\")\n        else:\n            graph.plot(W_array[:, 0], W_array[:, 1], \"o-\", linewidth=3, label=f\"{y_label} = {label}\")\n    graph.legend()\n    plt.show()","421f8956":"def print_performance_metrics(log):\n    print(f\"Linear regression results.\\nepochs: {log['epochs']}\\nlearning rate (alpha) = {log['alpha']}\")\n    print(f\"final cost = {log['train_loss'][-1]}\\nW = {W}\")\n    print(\"Convergence criterion: abs(cost_delta) <= 0.0001\")","42691d20":"W, log = linear_regression(x_train, y_train)\nprint_performance_metrics(log)","e69271b4":"logs = {log[\"alpha\"]: log}\nplot_annotated_cost_surface(W, logs, \"alpha\", margins=(0.0035, 0.03))","8f8bf97b":"plot_annotated_cost_contour(W, logs, \"alpha\")","4afbedd5":"logs = {}\ncost = 999\nprint(f\"l_rate\\tcost\\tepochs\\t\\tW\")\nfor learning_rate in [0.001, 0.025, 0.01, 0.05, 0.09, 0.1]:\n    w, log = linear_regression(x_train, y_train, learning_rate)\n    logs[learning_rate] = log\n    \n    #Store the best fit\n    if log[\"train_loss\"][-1] < cost:\n        cost = log[\"train_loss\"][-1]\n        W = w\n    \n    #Print the outcome\n    print(learning_rate, round(log[\"train_loss\"][-1], 4), log[\"epochs\"], w, sep=\"\\t\")\n\nplot_annotated_cost_contour(W, logs, \"alpha\", margins=(0.03, 0.04))\nplot_annotated_cost_surface(W, logs, \"alpha\", margins=(0.03, 0.04))","0e98878d":"W, log = linear_regression(x_train, y_train, 0.001)\nprint_performance_metrics(log)\nlogs = {log[\"alpha\"]: log}\nplot_annotated_cost_surface(W, logs, \"alpha\")\nplot_annotated_cost_contour(W, logs, \"alpha\")","75af7a38":"#Redefine the linear regression function with batches\ndef linear_regression(X, Y, learning_rate=0.01, batch_size=None):\n    if batch_size is None:        #Defaults to regular GD\n        batch_size = n_samples\n    \n    #Set the batches\n    num_batches = int(n_samples \/ batch_size)\n    ### YOUR CODE GOES HERE ###\n    X_batches = np.array_split(?, ?, axis=1)\n    Y_batches = np.array_split(?, ?, axis=1)\n    ### END CODE ###\n\n    #Initialize W with zeroes.\n    W = np.expand_dims(np.zeros(input_dims), axis=0)\n    \n    #Generate logs of parameters over iterations\n    log = {\"alpha\": learning_rate,\n           \"learning_steps\": 0,\n           \"epochs\": 0,\n           \"train_loss\": [],\n           \"val_loss\": [],\n           \"w\": []}\n    \n    epoch = 0\n    cost = -1\n    cost_delta = 999  #Initialize to any high random value.\n    \n    #Absolute value function\n    abs = lambda x: x if x >= 0 else -x\n    \n    while cost_delta > 0.0001:\n        epoch += 1\n        #Perform the learning step\n        for i in range(num_batches):\n            ### YOUR CODE GOES HERE ###\n            #Hint: call the compute_cost function for the current batch, and not the entire training set.\n            _, gradients = \n            W = \n            ### END CODE ###\n        \n            #Record the cost\n            train_cost, _ = compute_cost(X, Y, W)\n            cost_delta = abs(train_cost - cost)\n            cost = train_cost\n\n            #Log the step\n            log[\"learning_steps\"] += 1\n            log[\"w\"].append(W)\n            log[\"train_loss\"].append(cost)\n\n    log[\"epochs\"] = epoch\n    return W, log","843587d3":"#Test gradient descent with different batch sizes and same learning rate (0.025)\nlogs = {}\nloss = 999\nl_rate = 0.025\nprint(f\"b_size\\tcost\\tepochs\\t\\tW\")\nfor b_size in [n_samples, 32, 8, 1]:\n    ### YOUR CODE GOES HERE ###\n    w, log = linear_regression(x_train, y_train, learning_rate=?, batch_size=b_size)\n    ### END CODE ###\n    logs[b_size] = log\n    \n    #Find the train_loss for the current set of weights\n    train_loss, _ = compute_cost(x_train, y_train, w)\n    #Store the best fit\n    if train_loss < loss:\n        loss = train_loss\n        W = w\n    \n    #Print the final train_loss\n    print(b_size, round(train_loss, 4), log[\"epochs\"], w, sep=\"\\t\")\n\nplot_annotated_cost_contour(W, logs, \"b_size\")\nplot_annotated_cost_surface(W, logs, \"b_size\")","254cb161":"def pack_opt_params(grads, v, alpha, w, batch):\n    '''Generic function to pack parameters for an optimizer'''\n    params = {\n        \"grads\": grads,\n        \"v\": v,\n        \"alpha\": alpha,\n        \"w\": w,\n        \"batch\": batch\n    }\n    return params\n\ndef momentum_optimizer(B=0.9):\n    def optimizer(params):\n        #Unpack the required params\n        grads = params[\"grads\"]\n        v = params[\"v\"]\n        alpha = params[\"alpha\"]\n        #Return the optimized learning step\n        ### YOUR CODE GOES HERE ###\n        v_new = \n        ### END CODE ###\n        return v_new\n    return optimizer\n\ndef accelerated_grad_optimizer(B=0.9):\n    def optimizer(params):\n        #Unpack the required params\n        v = params[\"v\"]\n        alpha = params[\"alpha\"]\n        w = params[\"w\"]\n        x, y = params[\"batch\"]\n        #Return the optimized learning step\n        ### YOUR CODE GOES HERE ###\n        _, grads_ = compute_cost(x, y, ?)\n        v_new = \n        ### END CODE ###\n        return v_new\n    return optimizer\n\nclass adam:\n    def __init__(self, b1=0.9, b2=0.999, e=1e-5):\n        #Init the internal variables\n        self.m = 0\n        self.v = 0\n        self.B1 = b1\n        self.B2 = b2\n        self.B1_ = 1\n        self.B2_ = 1\n        self.e = e\n   \n    def adam_optimizer(self, params):\n        #Unpack the required params\n        grads = params[\"grads\"]\n        alpha = params[\"alpha\"]\n        \n        ### YOUR CODE GOES HERE ###\n        #Update the moments\n        self.m = \n        self.v = \n        \n        #Update Beta_t\n        self.B1_ *= \n        self.B2_ *= \n        \n        #Update the normalized moments\n        self.m_ = \n        self.v_ = \n        ### END CODE ###\n\n        #return the optimized step\n        return (alpha * self.m_ \/ (np.sqrt(self.v_) + self.e))\n","8a45f559":"#Redefine the update_w function to include optimization\ndef update_w(params, optimizer=None):\n    '''Receives optimizer and params as input and returns, new weights and step as output'''\n    if optimizer is None:          #Default optimizer returns the simple learning step.\n        optimizer = lambda params: params[\"alpha\"] * params[\"grads\"]\n    #Return the updated weights\n    W = params[\"w\"]\n    v = optimizer(params)\n    return(W - v, v)\n\n#Redefine linear regression to be compatible with the optimization step\ndef linear_regression(X, Y, learning_rate=0.01, batch_size=None, optimizer=None):\n    if batch_size is None:        #Defaults to regular GD\n        batch_size = n_samples\n    \n    #Set the batches\n    num_batches = int(n_samples \/ batch_size)\n    X_batches = np.array_split(X, num_batches, axis=1)\n    Y_batches = np.array_split(Y, num_batches, axis=1)\n    \n    #Initialize W with zeroes.\n    W = np.expand_dims(np.zeros(input_dims), axis=0)\n    \n    #Generate logs of parameters over iterations\n    log = {\"alpha\": learning_rate,\n           \"learning_steps\": 0,\n           \"epochs\": 0,\n           \"train_loss\": [],\n           \"val_loss\": [],\n           \"w\": []}\n    \n    epoch = 0\n    v = 0\n    cost = -1\n    cost_delta = 999  #Initialize to any high random value.\n    \n    #Absolute value function\n    abs = lambda x: x if x >= 0 else -x\n    \n    while cost_delta > 0.0001:\n        epoch += 1\n        #Perform the learning step\n        for i in range(num_batches):\n            ### YOUR CODE GOES HERE ###\n            _, gradients = \n            W, v = update_w(\n                pack_opt_params(grads=?, v=?, alpha=?, w=?, batch=(X_batches[i], Y_batches[i])),\n                optimizer=optimizer)\n            ### END CODE ###\n        \n            #Record the cost\n            train_cost, _ = compute_cost(X, Y, W)\n            cost_delta = abs(train_cost - cost)\n            cost = train_cost\n\n            #Log the step\n            log[\"learning_steps\"] += 1\n            log[\"w\"].append(W)\n            log[\"train_loss\"].append(cost)\n\n    log[\"epochs\"] = epoch\n    return W, log","06b8786b":"#Test gradient descent with different optimizers and same learning rate (0.025)\nlogs = {}\nloss = 999\nl_rate = 0.025\nb_size = 1\n\nadam_instance = adam()\n\noptimizers = {\n    \"No optimizer\": None,\n    \"momentum\": momentum_optimizer(),\n    \"acc_grad\": accelerated_grad_optimizer(),\n    \"adam optimizer\": adam_instance.adam_optimizer   #Notice: no parantheses because we are passing a function, not a wrapper.\n}\n\nprint(f\"optimizer\\tcost\\tepochs\\t\\tW\")\nfor label, optimizer in optimizers.items():\n    ### YOUR CODE GOES HERE ### \n    w, log = linear_regression(x_train, y_train, learning_rate=?, batch_size=?, optimizer=optimizer)\n    ### END CODE ###\n    logs[label] = log\n    \n    #Find the val_loss for the current set of weights\n    val_loss, _ = compute_cost(x_train, y_train, w)\n    #Store the best fit\n    if val_loss < loss:\n        loss = val_loss\n        W = w\n    \n    #Print the outcome with the val_loss (not the last train_loss)\n    print(label, round(val_loss, 4), log[\"epochs\"], w, sep=\"\\t\")\n\nplot_annotated_cost_contour(W, logs, \"optimizer\", margins=(0.3, 0.03))\nplot_annotated_cost_surface(W, logs, \"optimizer\", margins=(0.07, 0.036))","3396df70":"_, axes = plt.subplots(2,2, figsize=(12, 12))\nfor i, optimizer in enumerate(optimizers.keys()):\n    axes[int(i \/ 2), i % 2].plot(logs[optimizer][\"train_loss\"], label=optimizer)\n    axes[int(i \/ 2), i % 2].set_title(\"Training Loss\")\n    axes[int(i \/ 2), i % 2].set_ylabel(\"Loss\")\n    axes[int(i \/ 2), i % 2].set_xlabel(\"Learning Steps\")\n    axes[int(i \/ 2), i % 2].legend()","9d07b627":"#Load the datasets\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\n#Find the dimensions\nprint(f\"\\nTraining set dimensions:{x_train.shape} x {y_train.shape}\")\nprint(f\"Train-train split: {x_train.shape[0]} : {x_test.shape[0]}\")\n\n### YOUR CODE GOES HERE ###\n#Expand the data dimensions. Hint: expand the last axis (-1).\nx_train = \nx_test = \n### END CODE ###\n\n#Finalize input\/output dimensions\ninput_dims = x_train.shape[1:]\noutput_dims = 10 #Digits [0-9]\n\n#Create categorical classes (one-hot vectors) for each digit\ny_train = keras.utils.to_categorical(y_train, output_dims)\ny_test = keras.utils.to_categorical(y_test, output_dims)\n\nprint(f\"\\nRevised training set dimensions:{x_train.shape} x {y_train.shape}\")\nprint(f\"Input\/output dimensions: (m x {input_dims}), (m x {output_dims})\")","efb51ac4":"def generic_convnet(normalization_layer=None):\n    '''Genrates a generic model with a given normalization layer'''\n    ### YOUR CODE GOES HERE ###\n    model_layers = [\n        keras.Input(shape=?),\n        layers.Conv2D(filters=?, kernel_size=(?, ?)),\n        layers.Flatten(),\n        layers.Dense(units=?, activation=\"?\"),\n        layers.Dropout(?),\n        layers.Dense(units=?, activation=\"?\"),\n    ]\n    ### END CODE ###\n    if normalization_layer:\n        model_layers.insert(2, normalization_layer)\n        \n    return keras.Sequential(model_layers)","5e5de314":"def compile_models(models_dict, batch_size, epochs, loss, optimizer):\n    '''Compiles the models and plots the performance curves'''\n    logs = {}\n    for name, model in models_dict.items():\n        print(f\"Training {name}...\")\n        model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n        logs[name] = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n        print(f\"{name} trained sucessfully\\n\")\n    return logs\n\ndef plot_performance(logs):\n    fig, ax = plt.subplots(2, 2, figsize=(13, 16))\n    for model_name, log in logs.items():\n        ax[0, 0].plot(logs[model_name].history[\"loss\"], marker='x', label = f\"{model_name}\")\n        ax[0, 1].plot(logs[model_name].history[\"accuracy\"], marker='x', label = f\"{model_name}\")\n        ax[1, 0].plot(logs[model_name].history[\"val_loss\"], marker='x', label = f\"{model_name}\")\n        ax[1, 1].plot(logs[model_name].history[\"val_accuracy\"], marker='x', label = f\"{model_name}\")\n    \n    ax[0, 0].set_title(\"Training Loss\")\n    ax[0, 0].set_ylabel(\"Loss\")\n    ax[0, 0].set_xlabel(\"Epoch\")\n    ax[0, 0].legend()\n    \n    ax[0, 1].set_title(\"Training Accuracy\")\n    ax[0, 1].set_ylabel(\"Accuracy\")\n    ax[0, 1].set_xlabel(\"Epoch\")\n    ax[0, 1].legend()\n    \n    ax[1, 0].set_title(\"Test Loss\")\n    ax[1, 0].set_ylabel(\"Loss\")\n    ax[1, 0].set_xlabel(\"Epoch\")\n    ax[1, 0].legend()\n    \n    ax[1, 1].set_title(\"Test Accuracy\")\n    ax[1, 1].set_ylabel(\"Accuracy\")\n    ax[1, 1].set_xlabel(\"Epoch\")\n    ax[1, 1].legend()\n    \n    for i in range(1):\n        for j in range(1):\n            ax[i, j].set_xlabel(\"Epoch\")\n            ax[i, j].legend()","310b5ddc":"models = {\n    ### YOUR CODE GOES HERE ###\n    #Hint: use the utility function generic_convnet() as shown in the group_norm example.\n    \"no_norm\": generic_convnet(),\n    \"batch_norm\": ?,\n    \"layer_norm\": ?,\n    \"group_norm\": generic_convnet(tfa.layers.GroupNormalization(groups=?)),\n    \"inst_norm\": ?,\n    ### END CODE ###\n}\n\n### YOUR CODE GOES HERE ###\nperformance_logs = compile_models(models, batch_size=?, epochs=?, loss=\"?\", optimizer=\"?\")\n### END CODE ###","6f147289":"plot_performance(performance_logs)","f59daf78":"#Archive the previous results\narchive = [(models, performance_logs)]\n\n### YOUR CODE GOES HERE ###\n#Create new models\nmodels = {\n    \"batch_norm\": ?,\n    \"group_norm\": ?,\n    }\n### END CODE ###\n\nperformance_logs = {}\nfor batch_size in [8, 16, 64, 128]:\n    ### YOUR CODE GOES HERE ###\n    logs = compile_models(models, batch_size=batch_size, epochs=?, loss=\"?\", optimizer=\"?\")\n    ### END CODE ###\n    for model_name, log in logs.items():\n        performance_logs[model_name + f\"_bs{batch_size}\"] = log","224374ce":"plot_performance(performance_logs)","00398559":"#Set the model parameters\nvocab_size = 50000\n### YOUR CODE GOES HERE ###\nmax_len = ?\nembedding_size = ?\nGRU_units = ?\n### END CODE ###\n\n#Load the datasets\n(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=vocab_size)\n\n#Find the dimensions\nprint(f\"\\nTraining set dimensions:{x_train.shape} x {y_train.shape}\")\nprint(f\"Train-test split: {x_train.shape[0]} : {x_test.shape[0]}\")\n\n#Pad each input sequence to a length of 256 characters.\nx_train = pad_sequences(x_train, maxlen=max_len)\nx_test = pad_sequences(x_test, maxlen=max_len)\n\n### YOUR CODE GOES HERE ###\n#Expand the data dimensions. Hint: expand the last axis (-1)\nx_train = \nx_test = \ny_train = \ny_test = \n### END CODE ###\n\n#Finalize input\/output dimensions\ninput_dims = x_train.shape[1]\noutput_dims = 1 #Sentiment [0 or 1]\n\nprint(f\"\\nRevised training set dimensions:{x_train.shape} x {y_train.shape}\")\nprint(f\"Input\/output dimensions: (m x {input_dims}), (m x {output_dims})\")\nprint(f\"Voacabulary size: {vocab_size} (most frequent words)\")","0adffacb":"def generic_GRU_convnet(units, bidirectional=False):\n    '''Genrates a generic GRU-based model with or without bi-directional GRUs'''\n    \n    GRU_layer1 = layers.GRU(units, return_sequences=True)\n    GRU_layer2 = layers.GRU(units)\n    ### YOUR CODE GOES HERE ###\n    if bidirectional:\n        GRU_layer1 = layers.Bidirectional(?)\n        GRU_layer2 = layers.Bidirectional(?)\n    \n    model = keras.Sequential([\n        layers.Embedding(input_dim=?, output_dim=?, input_length=?),\n        ?,\n        ?,\n        layers.Dense(units=?, activation=\"?\"),\n    ])\n    ### END CODE ###    \n    return model","b4e7c678":"def compile_models(models_dict, batch_size, epochs, loss, optimizer):\n    '''Compiles the models and plots the performance curves'''\n    logs = {}\n    for name, model in models_dict.items():\n        print(f\"Training {name}...\")\n        model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n        logs[name] = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n        print(f\"{name} trained sucessfully\\n\")\n    return logs\n\ndef plot_performance(logs):\n    fig, ax = plt.subplots(2, 2, figsize=(13, 16))\n    for model_name, log in logs.items():\n        ax[0, 0].plot(logs[model_name].history[\"loss\"], marker='x', label = f\"{model_name}\")\n        ax[0, 1].plot(logs[model_name].history[\"accuracy\"], marker='x', label = f\"{model_name}\")\n        ax[1, 0].plot(logs[model_name].history[\"val_loss\"], marker='x', label = f\"{model_name}\")\n        ax[1, 1].plot(logs[model_name].history[\"val_accuracy\"], marker='x', label = f\"{model_name}\")\n    \n    ax[0, 0].set_title(\"Training Loss\")\n    ax[0, 0].set_ylabel(\"Loss\")\n    ax[0, 0].set_xlabel(\"Epoch\")\n    ax[0, 0].legend()\n    \n    ax[0, 1].set_title(\"Training Accuracy\")\n    ax[0, 1].set_ylabel(\"Accuracy\")\n    ax[0, 1].set_xlabel(\"Epoch\")\n    ax[0, 1].legend()\n        \n    ax[1, 0].set_title(\"Test Loss\")\n    ax[1, 0].set_ylabel(\"Loss\")\n    ax[1, 0].set_xlabel(\"Epoch\")\n    ax[1, 0].legend()\n        \n    ax[1, 1].set_title(\"Test Accuracy\")\n    ax[1, 1].set_ylabel(\"Accuracy\")\n    ax[1, 1].set_xlabel(\"Epoch\")\n    ax[1, 1].legend()","ac62cbda":"### YOUR CODE GOES HERE ###\n#Create new models\nmodels = {\n    \"unidir_GRU\": generic_GRU_convnet(?),\n    \"bidir_GRU\": generic_GRU_convnet(?, bidirectional=True)\n    }\nperformance_logs = compile_models(models, batch_size=?, epochs=?, loss=\"?\", optimizer=\"?\")\n### END CODE ###","1d5057c2":"plot_performance(performance_logs)","f3ec1045":"### Load the data","bad138c2":"### Load the data","1282942c":"### Impact of Bi-directional layer","f512dffc":"As we can see, running GD on smaller batches (**mini-batch GD**) instead of the entire dataset (**batch GD**) significantly improves the final results and converges onto a much lower cost. However, it is prone to meandering.  \n\nWe can observe that the learning curve is very chaotic and goes back and forth, taking too many non-optimal steps. With a batch size of 1 (**stochastic GD**), the oscillations are huge.  \n\nLet's see if we can optimize the learning step for the **SGD**.","d268168e":"# Q3: Recurrent Neural Network","bc12f1df":"### Define utility functions","15fe74cc":"## Natural Language Processing: Sentiment Classification using GRUs\n\nIn this problem, we shall design a classifier to analyze the sentiment of IMDB movie review dataset of `keras` API. A vocabulary size of about $50000$ most frequently used words would be sufficient.\n\n### Data preparation:\n* Pad each sentence to a maximum length of $256$. (Why? you need to fix the maximum length for the inputs to recurrent network.) \\[Hint: tensorflow.keras.preprocessing.sequence.pad\\_sequences. Use the default prefixed-padding-with-zeroes method.\\]\n* Expand the last dimension of the features as well as labels \\(X \\& Y\\) \\[Hint: np.expand\\_dims\\] for smooth vectorization.\n\n### Model Architecture:\nIn this section we describe the architecture to be used in this problem. As before, we use `keras` terminologies for describing the architecture.\n* Embed each word-index in a 128-dimensional vector. Input dimensions of the [Embedding Layer](https:\/\/keras.io\/api\/layers\/core_layers\/embedding\/) are the same as the `vocab_size`.\n*  [Gated Recurrent Unit (GRU)](https:\/\/keras.io\/api\/layers\/recurrent_layers\/gru\/) layer with 64 units. Set _return\\_sequence_ as `True` in order to pass the input sequence to the following GRU layer.\n*  Another Gated Recurrent Unit (GRU) layer with 64 units.\n*  Dense layer with 1 neuron and `sigmoid` activation.\n\n\n### Impact of Bidirectional wrapper\nDesign the models:\n*  First, design the model network as defined in the architecture.\n*  Next, use Bidirectional wrappers for the GRU layers.\n\nTrain each model with Binary Crossentropy, Adam optimizer and a batch size of $32$ for $10$ epochs. Use the test set for cross-validation. Plot and compare the loss curves and accuracy curves for each training epoch, for the training and validation sets. Use a GPU to shorten the processing duration.\n","d0667e98":"It can be observed that optimizers significantly dampen oscillations and also converge in lesser epochs, albeit at a slightly higher loss. An optimized model would yield significantly better results as compared to a normal model if both the models are trained for the same number of epochs.  \n\nLet's verify this by plotting the loss over learning steps for different optimizers.","d436e292":"### 1. Compare the normalization schemes","a39eadfb":"### 1. Batch Gradient Descent","53fdff06":"### 2. Compare the impact of batch size","f5f96c45":"# Q2: Convolutional Neural Network\n","238ea42d":"The bi-directional GRUs show little difference on the training set performance, but they show higher validation accuracy, in general.\n\nBoth models seem to overfit the data.","0309025e":"## Impact of Different Normalization Schemes on MNIST Classification\n\nIn the class, you have been shown a demo of MNIST classifier using Convolutional Neural Network. In this problem, we shall study the effect of different normalisation schemes, viz. **Batch Normalization**[$^{[2]}$](#scrollTo=Chz-9zCYTiDU&line=11&uniqifier=1),  **Instance Normalization**[$^{[4]}$](#scrollTo=Chz-9zCYTiDU&line=11&uniqifier=1), **Batch-Instance Normalization**[$^{[3]}$](#scrollTo=Chz-9zCYTiDU&line=11&uniqifier=1), **Layer Normalization**[$^{[1]}$](#scrollTo=Chz-9zCYTiDU&line=11&uniqifier=1), and **Group Normalization**[$^{[5]}$](#scrollTo=Chz-9zCYTiDU&line=11&uniqifier=1) in MNIST classification. We shall stick to the the standard training-test split of MNIST dataset as provided by `Keras`.\n\n### Backbone Architecture\nIn this section, we describe the architecture to be used in this problem. We use `Keras' terminologies to describe the backbone architecture.\n\n* Pre-process the data set:\n\n    * Expand the last dimension of the feature vectors (X) \\[Hint: np.expand\\_dims\\]\n    * Convert the integer labels (Y) into one-hot vectors [Hint: keras.utils.to\\_categorical]\n\n* Model design:  \n    * Conv2D layer with 16 filters, kernel size: $3\\times 3$.\n    * Normalization Layer\n    * Flatten\n    * Dense layer with $128$ neurons and `relu` activation.\n    * Dropout layer with $0.3$ dropout rate\n    * Dense layer with $10$ neurons and `softmax` activation.\n\n### Impact of Normalization\nDesign the following five models:\n* First design the backbone network without any normalization scheme i.e., by omitting the $2^{nd}$ layer.\n* Next, use [Batch Normalization](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/BatchNormalization) in the $2^{nd}$ layer.\n* Then utilize [Layer Normalization](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/LayerNormalization) in $2^{nd}$ layer.\n* Next employ [Group Normalization](https:\/\/www.tensorflow.org\/addons\/api_docs\/python\/tfa\/layers\/GroupNormalization) in the $2^{nd}$ layer. Use a group size of 16.\n* Finally use [Instance Normalization](https:\/\/www.tensorflow.org\/addons\/api_docs\/python\/tfa\/layers\/InstanceNormalization).\n\nTrain each model with Categorical Crossentropy, Adam optimizer and a batch size of $32$ for $10$ epochs. Use the test set for cross-validation. Plot and compare the loss curves and accuracy curves for the training and validation sets. Use a GPU to shorten the processing duration.\n\n### Impact of Batch Size:\n Wu et al[$^{[5]}$](#scrollTo=Chz-9zCYTiDU&line=11&uniqifier=1)  claim that one of the advantages of Group Normalization over Batch Normalization is its insensitivity to batch size. Retrain the Batch Normalization and Group Normalization variants of the model with Batch Size $8$, $16$, $64$ and $128$, keeping other modelling choices same. Compare the loss curves and accuracy curves for training and validation sets.\n\n","6e9c108b":"### Define utility functions","aa0c2dc0":"### Import data","31c3a701":"## Import libraries","5d3d5a35":"## Stochastic Gradient Descent\nWe shall implement gradient descent for a linear regression problem to predict density of an industrial solvent based on its acidity. We shall consider mean squared error as the loss function, which, for an entire batch, is given by:\n\\begin{equation}\n    J(\\theta) = \\frac{1}{2n}\\sum_{i=1}^{n}\\Big( y^{(i)} - h_\\theta(x^{(i)}) \\Big)^2\n\\end{equation}\n\nwhere:\n* $h_\\theta(x) = \\theta^Tx$.\n* $x^{(i)}$ denotes the $i^{th}$ example of the acidity of the solvent.\n* $y^{(i)}$ denotes the $i^{th}$ example of the density of the solvent.\n\n\nThe $i^{th}$ row of `linearX.csv` and `linearY.csv` contain $x^{(i)}$ and $y^{(i)}$ respectively. We will implement linear regression to minimize the MSE function and learn the relationship between $x^{(i)}$ and $y^{(i)}$.\n\nIn this problem, you shall:  \n1.  Initially implement **Batch Gradient Descent** for optimizing $J(\\theta)$.\n    * Choose an appropriate learning rate and the stopping criteria (as a function of the change in the value of $J(\\theta)$). You may initialize the parameters as $\\theta = \\vec{0}$ (the vector of all zeros). Report your learning rate, stopping criteria, number of epochs and the final set of parameters obtained.\n    * Plot a 3-dimensional mesh where $z$-axis denotes the loss function $J(\\theta)$ and the parameters ($\\theta$) are plotted in the $x-y$ plane. Annotate this plot using the set of parameters obtained at each iteration of the gradient descent to obtain the learning curve. \n    * Visualize the contours of the error function. Annotate the contours with the learning curve.\n    * Repeat the part above (i.e. draw the annotated loss surfaces and loss contours) for different learning rates \\[0.001, 0.025, 0.1, 0.5, 0.9, 2.5\\]. What do you observe? Comment.\n2. Implement **Mini-batch** and **Stochastic Gradient Descent**.\n    * Modify the previously defined linear regression model to work on batches.\n    * Choose an optimal learning rate based on the previous results.\n    * Use different batch sizes and compare the annotated loss surfaces and loss contours of **Batch** \\(`batch_size` = `n_examples`\\), **Mini-batch** \\(`batch_size` < `n_examples`\\)and **Stochastic Gradient Descent** \\(`batch_size` = $1$\\).\n\n3. Optimize the **SGD** steps.\n    * Implement the **Momentum**, **Nesterov's Accelerated Gradient** as well as the **Adam** \\(Adaptive Momentum Estimation\\) optimizers for Gradient Descent.\n    * Redefine the cost update method to include optimizers.\n    * Using an optimal learning rate and a `batch_size` of $1$ \\(SGD\\), compare the annotated loss surfaces and loss contours of the optimizers.\n\n","a51599ae":"We shall redefine the _update_w()_ function to include optimizations.","6d039aec":"### 3. SGD Optimization\n \nWe shall define some **generic GD optimizer functions**.\n\nWe use decorator functions to wrap pre-defined hyperparameters in a function which implements the optimizer step.  \n\nNotice how we require an instance of the adam optimizer in order to retain the moments in the memory.","cd32217d":"### Define utility functions","b49a4018":"The learning steps overlap so much that it is difficult to get a sense of the direction at each step.  \nPerhaps, we should employ a smaller learning step.","1fd6661e":"### 2. Batch, Mini-batch & Stochastic Gradient Descent","3b4ee227":"# Deep Learning Assignment\n\nInstructions:  \n* Use GPUs to accelerate the computations (it takes about 1 hour to run the assignment end-to-end).\n* The second and third problem require significant processing time. Make sure that there are no errors before you compile these models.\n* It is recommended to complete the coding cells from all the questions before compiling the models so that you do not have to idle around when the models are being compiled.\n* The assignment comes along with helper script for trivial tasks. Insert your code between the `### YOUR CODE GOES HERE ###` and `### END CODE ###` labels. In the case of pre-filled function-calls, replace '?' with appropriate values\/variables.\n","0f2edc32":"# Q1: Linear Regression","1a731093":"# References\n\n1. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. _Layer Normalization, 2016_.\n\n2. Sergey  Ioffe  and  Christian  Szegedy. _Batch  normalization: Accelerating deepnetwork training by reducing internal covariate shift, 32nd International Conference on Machine Learning, ICML 2015, 1:448\u2013456, 2015_.\n\n3. Hyeonseob  Nam  and  Hyo-Eun  Kim. _Batch-instance  normalization  for  adaptively style-invariant neural networks_.  In Samy Bengio, Hanna M. Wallach,Hugo Larochelle,  Kristen Grauman,  Nicol`o Cesa-Bianchi,  and Roman Garnett, editors,_Advances in Neural Information Processing Systems 31:  AnnualConference on Neural Information Processing Systems 2018, NeurIPS 2018,December 3-8, 2018, Montr \u0301eal, Canada, pages 2563\u20132572, 2018_.\n\n4. Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.  _Instance Normaliza-tion:  The Missing Ingredient for Fast Stylization (2016), 2016_.\n\n5. Yuxin  Wu  and  Kaiming  He. _Group  Normalization.International  Journal of Computer Vision, 128(3):742\u2013755, 2020_.","e19d0b1a":"For large learning rates, the cost explodes quickly, implying that the steps taken are too large for the model to converge.  \nFor smaller learning rates, the cost oscillates within a neighbourhood with dampening amplitude and may take forever to converge.  \nThe optimal choice of learning rate prevents exploding gradients and also converges quickly.  \n\nHere's an example of a model (alpha = 0.001) whose cost oscillates in a dampened manner but takes too long to converge onto an optimal value.  \nA model with alpha = 0.025 reaches a more optimal value sooner (as can be seen above).\n"}}