{"cell_type":{"61f5f5c3":"code","17f1d5f6":"code","e252f005":"code","02e769f0":"code","372b834a":"code","b7b19838":"code","f5a12077":"code","fba65378":"code","33db9553":"code","e33dc47f":"code","50866d27":"code","21b390d2":"code","dcb5f03f":"code","886440d7":"code","f444f02a":"code","edbfd409":"code","763f03a4":"code","5b1c8ebe":"code","69856adf":"code","67b5e522":"code","adb93b1f":"code","a9ea9cf0":"code","55557d1e":"code","9f7d0fc3":"code","41a7cc50":"code","1bd0e0b3":"code","d4b7a1ed":"code","8870fd94":"code","4d0fca3d":"code","3b31d2c2":"code","017774ef":"code","bee9a8af":"code","903ab500":"markdown","46516c33":"markdown","fd39423a":"markdown","6c02e306":"markdown","e459d12f":"markdown","88657858":"markdown","2fc5c793":"markdown","6f176ca3":"markdown","ad85b66e":"markdown"},"source":{"61f5f5c3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","17f1d5f6":"import re\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport numpy as np\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom imblearn.under_sampling import NearMiss\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import learning_curve\nfrom matplotlib import style\nfrom imblearn.combine import SMOTETomek\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import recall_score, confusion_matrix","e252f005":"train = pd.read_csv('\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/test.csv')\n# remove the unwanted columns from the train and test\ntest.drop(['id'], axis = 1, inplace = True)\ntrain.drop(['id'], axis = 1, inplace = True)\n","02e769f0":"train.replace('[^a-zA-Z#]', ' ', inplace = True, regex = True)\ntest.replace('[^a-zA-Z#]', ' ', inplace = True, regex = True)\n#  to lemmatize, we first have to tokenize it, meaning.. splitting the sentence to words\nlemmatizer = WordNetLemmatizer()\n# create a object for stopwords \nstop_words = stopwords.words('english')\n# print(len(stop_words))\n# add the word 'user' to the stopwords list\nstop_words.append('user')\n# print(len(stop_words))","372b834a":"train['clean_tweet'] = np.nan\ntest['clean_tweet'] = np.nan\ntrain.head()","b7b19838":"# tokenize, remove word with less than 3 chars, remove words from stopwords and lemmatize them\nfor i in range(len(train.index)):\n    words = nltk.word_tokenize(train.iloc[i, 1])\n    words = [word for word in words if len(word) > 3]\n    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stop_words)]\n    train.iloc[i, 2]  = ' '.join(words)\n    words = nltk.word_tokenize(train.iloc[i, 2])\nprint(train.head())\n# similar way the test data set needs to be handeled.\nfor i in range(len(test.index)):\n    words = nltk.word_tokenize(test.iloc[i,0])\n    words = [word for word in words if len(word) > 3]\n    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stop_words)]\n    test.iloc[i, 1]  = ' '.join(words)\n    words = nltk.word_tokenize(test.iloc[i, 1])\nprint(test.head())","f5a12077":"# wordclod for all the words in train\n\ntxt = \" \".join(text for text in train.clean_tweet)\n\nwordcloud = WordCloud(max_font_size = 100, max_words = 50, background_color = 'orange').generate(txt)\n\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis(\"off\")\nplt.show()","fba65378":"# wordclod for racist messages\nbad = \" \".join([text for text in train['clean_tweet'][train['label']== 1]])\n\nwordcloud = WordCloud(max_font_size = 100, max_words = 50, background_color = 'red').generate(bad)\n\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis(\"off\")\nplt.show()","33db9553":"# wordclod for non racist messages\nnon_racist = \" \".join([text for text in train['clean_tweet'][train['label']== 0]])\n\nwordcloud = WordCloud(max_font_size = 100, max_words = 50, background_color = 'green').generate(non_racist)\n\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis(\"off\")\nplt.show()","e33dc47f":"# function for extract hashtag words and see if they contribute in any way\ndef hashtag_extract(x):\n    hashtags = []\n    # Loop over the words in the tweet\n    for i in x:\n        ht = re.findall(r\"#(\\w+)\", i)\n        hashtags.append(ht)\n\n    return hashtags","50866d27":"ht_regular = hashtag_extract(train['tweet'][train['label']== 0])\nht_racist = hashtag_extract(train['tweet'][train['label']== 1])\n\nht_racist\n\n# unlisting the lists - \nHt_regular = sum(ht_regular,[])\nht_racist = sum(ht_racist,[])\n\n# print(ht_racist)\n","21b390d2":"a = nltk.FreqDist(Ht_regular)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n# selecting top 10 most frequent hashtags     \nd = d.nlargest(columns=\"Count\", n = 10) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()","dcb5f03f":"neg = nltk.FreqDist(ht_racist)\nneg_dataframe = pd.DataFrame({'hash': list(neg.keys()),\n                             'count' : list(neg.values())\n                             })\ntop10 = neg_dataframe.nlargest(10, 'count')\nplt.figure(figsize = (12, 5))\nsns.barplot(data = top10, x = 'hash', y = 'count')","886440d7":"# TFIDF transform\n\nvector = TfidfVectorizer(max_features=1000, stop_words='english', lowercase = False)\ntfidf = vector.fit_transform(train['clean_tweet'])\ntfidf_test = vector.transform(test['clean_tweet'])\n\nprint(tfidf_test.shape)","f444f02a":"clf = LogisticRegression()\ny = train['label']\n\nX_train, X_valid, y_train, y_valid = train_test_split(tfidf, y, train_size = 0.8, random_state = 42)\nprint('X Train Shape', X_train.shape)\nprint('y Train Shape', y_train.shape)\nprint('X Valid Shape', X_valid.shape)\nprint('y Train Shape', y_valid.shape)\n\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_valid)\n\nscore = f1_score(y_valid, prediction)\nprint('F1 - Score using imbalanced data',score)\n\nacc_score = accuracy_score(y_valid, prediction)\nprint('Accuracy Score using imbalanced data = ',acc_score)\n\n# predict_test = clf.predict(tfidf_test)\n# test['label'] = predict_test\n# test.to_csv('twitter_test.csv', index = False)","edbfd409":"train.head()\ntrain['label'].value_counts()\n# sns.distplot(train['label'])","763f03a4":"nm = NearMiss()\nX_nearmiss,y_nearmiss = NearMiss().fit_sample(tfidf, y)\n\n\nX_train_nm, X_valid_nm, y_train_nm, y_valid_nm = train_test_split(X_nearmiss, y_nearmiss)\nprint('X Train Shape', X_train_nm.shape)\nprint('y Train Shape', y_train_nm.shape)\nprint('X Valid Shape', X_valid_nm.shape)\nprint('y Valid Shape', y_valid_nm.shape)","5b1c8ebe":"clf.fit(X_train_nm, y_train_nm)\nprediction_nm = clf.predict(X_valid_nm)\nf1_score_nm = f1_score(y_valid_nm, prediction_nm)\nprint('F1 - Score using NearMiss = ',f1_score_nm)\n\nacc_score = accuracy_score(y_valid_nm, prediction_nm)\nprint('Accuracy Score using NearMiss = ',acc_score)","69856adf":"\nstyle.use('fivethirtyeight')\ndef plot_learning_curve(model, X, y):\n    train_size, train_scores, test_scores = learning_curve(model, X, y, train_sizes=np.linspace(0.01, 1, 50), cv=10,\n                                                       scoring='accuracy', n_jobs=3, verbose=1, random_state=42,\n                                                      shuffle=True)\n    train_scores_mean = np.mean(train_scores, axis = 1)\n    train_scores_std = np.std(train_scores, axis = 1)\n    test_scores_mean = np.mean(test_scores, axis = 1)\n    test_scores_std = np.std(test_scores, axis = 1)\n    plt.figure(figsize = (8, 4))\n    plt.plot(train_size, train_scores_mean, color = 'red', label = 'Training Score')\n    plt.fill_between(train_size, train_scores_mean - train_scores_std, train_scores_mean+train_scores_std, color = '#DDDDDD')\n    plt.fill_between(train_size, test_scores_mean - test_scores_std, test_scores_mean+test_scores_std, color = '#DDDDDD')\n    plt.plot(train_size, test_scores_mean, color = 'green', label = 'CV Score')\n    plt.title('Learning Curve ')\n    plt.xlabel('CV Train Size')\n    plt.ylabel('Accuracy')\n    plt.legend(loc = 'best')\n    plt.show()","67b5e522":"plot_learning_curve(clf, X_nearmiss,y_nearmiss)","adb93b1f":"# from imblearn.over_sampling import SMOTE\n\n\nsm = SMOTETomek(random_state=42)\nX_sm, y_sm = sm.fit_resample(tfidf, y)\n\nX_train_sm, X_valid_sm, y_train_sm, y_valid_sm = train_test_split(X_sm, y_sm)\nprint('X Train Shape', X_train_sm.shape)\nprint('y Train Shape', y_train_sm.shape)\nprint('X Valid Shape', X_valid_sm.shape)\nprint('y Valid Shape', y_valid_sm.shape)\n\nclf.fit(X_train_sm,y_train_sm)\nprediction_sm = clf.predict(X_valid_sm)\n\nf1_score_sm = f1_score(y_valid_sm, prediction_sm)\nprint('F1 - Score using NearMiss = ',f1_score_sm)\n\nacc_score_sm = accuracy_score(y_valid_sm, prediction_sm)\nprint('Accuracy Score using NearMiss = ',acc_score_sm)","a9ea9cf0":"plot_learning_curve(clf, X_sm, y_sm)","55557d1e":"models = []\nmodels.append(('LRC', LogisticRegression()))\nmodels.append(('RFC', RandomForestClassifier()))\nmodels.append(('DTC', DecisionTreeClassifier()))\nmodels.append(('SVC', SVC()))\nmodels.append(('XGB', XGBClassifier()))\n\nf1_score_all = []\naccuracy_all = []\nrecall_all = []","9f7d0fc3":"\nfor name,model in models:\n\n    model.fit(X_train_sm,y_train_sm)\n    prediction_sm = model.predict(X_valid_sm)\n    acc_score_sm = accuracy_score(y_valid_sm, prediction_sm)\n    recall_sc = recall_score(y_valid_sm, prediction_sm)\n#     print('Accuracy Score using NearMiss = ',acc_score_sm)    \n#     f1_score_all.append(f1_sc)\n    accuracy_all.append(acc_score_sm) \n    recall_all.append(recall_sc)\n    cm = confusion_matrix(y_valid_sm, prediction_sm)\n    print(cm)\n\n# print(f1_score)\nprint(accuracy_all)\nprint(recall_all)","41a7cc50":"clf_names = []\nfor name, model in models:\n    clf_names.append(name)\nclf_names","1bd0e0b3":"df = pd.DataFrame(list(zip(clf_names, accuracy_all, recall_all)), columns = ['model', 'accuracy', 'recall'])\ndf","d4b7a1ed":"values = list(zip(accuracy_all, recall_all))\ndata = pd.DataFrame(values, columns=['accuracy', 'recall'])\n\n\ng = sns.lineplot(data=data, palette=\"tab10\", linewidth=2.5)\ng.set(xticklabels=['kk','LRC', 'RFC', 'DTC', 'SVC', 'XGB'])","8870fd94":"# from sklearn.model_selection import RandomizedSearchCV\n\n# # Number of trees in random forest\n# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 800, num = 4)]\n# # Number of features to consider at every split\n# max_features = ['auto', 'sqrt']\n# # Maximum number of levels in tree\n# max_depth = [int(x) for x in np.linspace(10, 110, num = 5)]\n# max_depth.append(None)\n# # Minimum number of samples required to split a node\n# min_samples_split = [2, 5]\n# # Minimum number of samples required at each leaf node\n# min_samples_leaf = [1, 2]\n# # Method of selecting samples for training each tree\n# # bootstrap = [True, False]\n\n# # Create the random grid\n# random_grid = {'n_estimators': n_estimators,\n#                'max_features': max_features,\n#                'max_depth': max_depth,\n#                'min_samples_split': min_samples_split,\n#                'min_samples_leaf': min_samples_leaf,\n# #                'bootstrap': bootstrap\n#               }\n# print(random_grid)","4d0fca3d":"# # Use the random grid to search for best hyperparameters\n# # First create the base model to tune\n# clf = RandomForestClassifier()\n# # Random search of parameters, using 3 fold cross validation, \n# # search across 100 different combinations, and use all available cores\n# rf_random = RandomizedSearchCV(estimator = clf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = 1)\n# # Fit the random search model\n# rf_random.fit(X_train_sm,y_train_sm)","3b31d2c2":"# rf_random.best_params_","017774ef":"# # Create the parameter grid based on the results of random search \n# from sklearn.model_selection import GridSearchCV\n\n\n# parm_grid = {'n_estimators' : [550,600,650],\n#             'min_samples_split' : [4,5,6],\n#             'min_samples_leaf' : [1,2],\n#             'max_features' : ['sqrt'],\n#             'max_depth' : [None],\n#             'bootstrap' : [True]}\n# clf = RandomForestClassifier()\n\n# grid_search = GridSearchCV(estimator = clf, param_grid = parm_grid, cv = 3, n_jobs = 1, verbose = 2)\n\n# print(grid_search)","bee9a8af":"# grid_search.fit(X_train_sm,y_train_sm)\n# grid_search.best_params_","903ab500":"Since Random Forest Classifier gives the best numbers, we will use this model.\nNext step is to find the best parameters for Random forest Classifier.\nThings to remember - \n- Classifier - Random Forest\n- SMOTE methodology for handeling imbalanced data","46516c33":"1. remove special characters from X and test\n2. convert to lowercase\n3. lemmatize","fd39423a":"Now time to find the best classifier..\n","6c02e306":"SMOTE has helped in increasing the F1 score and the accuracy as compared to Near Miss approach. Learning curve indicates the accuracy will gradually increase with increase in data set!!","e459d12f":"In this NoteBook I will be doing the following\n    - Sentiment Analysis on train data test\n        - WordCloud for most frequent words used for positive and negative reviews\n    - Use TF-IGF to know how important a word is to a document in a collection or corpus\n    - Use of Logistic Regression algorithem to get the baseline score\n    - Handle imbalance data\n    - Check if has any implications on the score\n    - Find the best Classification algorithem.","88657858":"Check my kaggle - Twitter part 2 for Hashtag Analysis, i think i prefer that approach","2fc5c793":"Random Search took almost 3 hours to run!","6f176ca3":"i'll first run the model with a basic test-train split approach, even though we know that its a imbalanced test test. The score from this run will be our baseline.\nI will later handle the imbalace data set to see if there is any change to the score.\nI will also find the best classification model to see if there is any improvement","ad85b66e":"WOW!! look at the F1 score, its almose 75% increase and the accuracy is not bad as well, even though its lower."}}