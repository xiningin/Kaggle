{"cell_type":{"e73f0df7":"code","de776a8c":"code","1aaa894f":"code","c6e629c6":"code","5cd80c29":"code","02d1096e":"code","b9d89c8c":"code","1ae3da7a":"code","3cbb5968":"code","7af1b28a":"code","80baa437":"code","eff17354":"code","6bb4b5b7":"code","8ad8dfde":"code","1502f31f":"code","84fb598d":"code","89190b35":"code","45987725":"code","7d337fb3":"code","b5f01086":"code","727103aa":"code","87133aa7":"code","37763640":"code","21e35d68":"code","d7508c97":"code","abd522fc":"code","4e7da1f6":"code","0d0edc28":"code","e9df7e71":"code","e34281bc":"code","47ed77d0":"code","817c21b2":"code","45327ab5":"code","38b2f827":"code","4492845b":"code","98a8ee77":"code","8ccf78ff":"code","c28bbe7d":"code","12e7ed7a":"code","455cf260":"code","c2c55ff0":"code","58a8626a":"code","292e6fa4":"code","af502a0e":"code","ab595f30":"code","15bf1c3b":"code","2edd2dac":"code","e40ceedd":"code","fcef5acb":"code","8d773e44":"code","386045df":"code","d4aad18b":"code","ec1658a6":"code","62ea2340":"code","4293abba":"code","0b6ca7b0":"code","036b076e":"code","843dd4f5":"code","b57c6720":"code","12161929":"markdown","bb03a373":"markdown","dddba7a7":"markdown","3875da6d":"markdown","6f6d8649":"markdown","cb8ab4e4":"markdown","de953e0f":"markdown","da55d75e":"markdown","97503473":"markdown","23e71099":"markdown","89e4784d":"markdown","4554ab1c":"markdown","1890cac0":"markdown","016adbb8":"markdown","626ae4ac":"markdown","a627389c":"markdown","7fed94a5":"markdown","1e50875e":"markdown","0ecc7c77":"markdown","9146638b":"markdown","12d1ccdc":"markdown","5c328a0c":"markdown","3576efea":"markdown","a7582d88":"markdown","95bf6341":"markdown","390b8d4b":"markdown","97559906":"markdown","026170bc":"markdown","3d110f30":"markdown","f6a8ff96":"markdown","435315e7":"markdown","0d44d2ab":"markdown","735ed290":"markdown","6cbb32b2":"markdown","41d262a4":"markdown","c536a437":"markdown","73f41c98":"markdown","7a7daba2":"markdown"},"source":{"e73f0df7":"from sklearn import metrics,impute\n\nimport numpy as np #for math functions\nimport math # for mathematical operations\nimport pandas as pd #for dataframe management\nimport lightgbm as lgb # machine learning model\nimport missingno as msno # for NaN visualization\nimport warnings # to remove the annoying warnings\nimport seaborn as sns # for graphical visualization\n\nimport matplotlib.pyplot as plt # for graphical visualization\n\nfrom sklearn.linear_model import LinearRegression # for model making\nfrom sklearn.model_selection import train_test_split # for model making\nfrom sklearn.compose import make_column_transformer  # to apply Standard Scaler\nfrom sklearn.metrics import accuracy_score # for the vizualization of the Acuracy score\nfrom sklearn.preprocessing import OrdinalEncoder, LabelEncoder, StandardScaler # for data preprocessing\nfrom sklearn.model_selection import cross_validate,GridSearchCV # to make the cross validation of the model\n\nwarnings.filterwarnings(action='ignore', category=UserWarning)\n\n#Change size of the plot in seaborn\nsns.set(rc={'figure.figsize':(11.7,8.27)})\n","de776a8c":"train= pd.read_csv('..\/input\/titanic\/train.csv',index_col=\"PassengerId\")\ntest = pd.read_csv('..\/input\/titanic\/test.csv',index_col=\"PassengerId\")","1aaa894f":"train.describe().T","c6e629c6":"df_combined= pd.concat([train, test])","5cd80c29":"#get the information about each column of the dataset,\n\ntrain.info()","02d1096e":"#get the information about each column of the dataset\n\ntest.info()","b9d89c8c":"df_combined.info()#get the information about each column of the dataset","1ae3da7a":"print(\"train columns: \", train.columns)\n\nprint(\"test columns: \", test.columns)","3cbb5968":"#Select Numerical columns\n\nnum_cols = df_combined.select_dtypes(exclude=\"object\").columns\n\n#Select Categorical columns\n\ncat_cols = df_combined.select_dtypes(include=\"object\").columns\n\nprint(\"numerical columns are:\", num_cols)\nprint(\"categorical columns are:\", cat_cols)","7af1b28a":"#Check for unique values in each column\n\nn_uniqe = {col:df_combined[col].nunique() for col in cat_cols}\nfor key in n_uniqe:\n    print(f\"{key} has {n_uniqe[key]} unique values\")","80baa437":"#get the information about each column of the dataset\n\ntrain.describe()","eff17354":"#get the information about each column of the dataset\n\ntest.describe()","6bb4b5b7":"#get the information about each column of the dataset\n\ndf_combined.describe()","8ad8dfde":"#get the number of null values in the dataset\n\ntrain.isna().sum()","1502f31f":"#get the number of null values in the dataset\n\ntest.isna().sum()","84fb598d":"#get the number of null values in the dataset\n\ndf_combined.isna().sum()","89190b35":"#Let's start by visualizing the empty rows on each feature using missingno\n\nmsno.matrix(df_combined)","45987725":"#Drop the Cabin columns\ndf_combined = df_combined.drop(['Cabin'], axis=1)\n","7d337fb3":"#Classify the Age group\n\ndf_combined['Age_Class'] = (df_combined['Age']\/20)+0.5\ndf_combined['Age_Class'] = round(df_combined['Age_Class'])\n\n#Classify the Fare group\n\ndf_combined['Fare_Class'] = (df_combined['Fare']\/100)+0.5\ndf_combined['Fare_Class'] = round(df_combined['Fare_Class'])","b5f01086":"#create a heatmap to identify correlation between columns\n\nsns.heatmap(df_combined.corr(),annot=True)\nplt.show()","727103aa":"#create a bar graph with the correlation between age and survival\n\nsns.barplot(data=df_combined,x='Age_Class',y=\"Survived\")\nplt.title(\"Correlation between the Age and survival\")\nplt.show()","87133aa7":"#create a bar graph with the correlation between Age_Class and Fare\n\nsns.barplot(data=df_combined,x='Age_Class',y=\"Fare\")\nplt.title(\"Correlation between the Age and Fare\")\nplt.show()","37763640":"#create a bar graph with the correlation between Fare_Class and survival\n\nsns.barplot(data=df_combined,x='Fare_Class',y=\"Survived\")\nplt.title(\"Correlation between the Fare and survival\")\nplt.show()","21e35d68":"#Check for unique values in each column\n\nn_uniqe = {col:df_combined[col].nunique() for col in df_combined}\nfor key in n_uniqe:\n    print(f\"{key} has {n_uniqe[key]} unique values\")","d7508c97":"df_combined[\"SibSp\"].describe().T","abd522fc":"df_combined['Parch'].describe().T","4e7da1f6":"df_combined['relatives'] = df_combined['Parch']+df_combined[\"SibSp\"]","0d0edc28":"df_combined['relatives'].describe().T","e9df7e71":"#create a bar graph with the correlation between relatives and survival\n\nsns.barplot(data=df_combined,x='relatives',y=\"Survived\")\nplt.title(\"Correlation between # of relatives and survival\")\nplt.show()","e34281bc":"#create a bar graph with the correlation between Pclass and survival - by gender\n\nsns.factorplot(x = 'Pclass', y='Survived', \n               hue = 'Sex',data=df_combined, kind='bar')\nplt.title(\"Correlation between the Pclass and survival\")\nplt.show()","47ed77d0":"msno.matrix(df_combined)","817c21b2":"#input the missing values in the Age and cabin column using Fillna\n\ndf_combined[\"Age\"] = df_combined[\"Age\"].fillna(method='ffill')\n\n\n#Let's Also reset the Age_Class since we filled the columns\n\ndf_combined['Age_Class'] = (df_combined['Age']\/20)+0.5\ndf_combined['Age_Class'] = round(df_combined['Age_Class'])","45327ab5":"#Normalize the sex column setting male to 1 and female to 0\n\ndf_combined[\"Sex\"] = [1 if x==\"male\" else 0 for x in df_combined[\"Sex\"]]","38b2f827":"msno.matrix(df_combined)","4492845b":"#Let's start by filling these few empty rows in Embarked and Fare\n\ndf_combined[\"Embarked\"] = df_combined[\"Embarked\"].fillna(method='ffill')\ndf_combined[\"Fare\"] = df_combined[\"Fare\"].fillna(method='ffill')\n\n\n#Classify the Fare group again, since we filled the values\n\ndf_combined['Fare_Class'] = (df_combined['Fare']\/100)+0.5\ndf_combined['Fare_Class'] = round(df_combined['Fare_Class'])","98a8ee77":"msno.matrix(df_combined)","8ccf78ff":"df_combined","c28bbe7d":"# let's split the names, last names and pronouns to get better details\ndf_combined[\"Last_Name\"] = [name.split(',')[0].strip() for name in df_combined[\"Name\"]]\n\ndf_combined[\"Name\"] = [name.split(',')[1].strip() for name in df_combined[\"Name\"]]\n\ndf_combined[\"Pronoun\"] = [name.split('.')[0].strip() for name in df_combined[\"Name\"]]\n\ndf_combined[\"Name\"] = [name.split('.')[1].strip() for name in df_combined[\"Name\"]]","12e7ed7a":"#let's use the label encoder to encode the categorical features\n\nl1= LabelEncoder()\n\nl1.fit(df_combined['Embarked'])\ndf_combined.Embarked = l1.transform(df_combined.Embarked)","455cf260":"l1.fit(df_combined['Ticket'])\ndf_combined.Ticket = l1.transform(df_combined.Ticket)","c2c55ff0":"l1.fit(df_combined['Last_Name'])\ndf_combined.Last_Name = l1.transform(df_combined.Last_Name)","58a8626a":"l1.fit(df_combined['Pronoun'])\ndf_combined.Pronoun = l1.transform(df_combined.Pronoun)","292e6fa4":"df_combined","af502a0e":"#create a heatmap to identify correlation between columns\nsns.heatmap(df_combined.corr(),annot=True)\nplt.show()","ab595f30":"#let me export it and have a closer look on excel\ndf_combined.to_csv('Titanic_df.csv', index=False)","15bf1c3b":"#let's get the clean version of the train data by dropping the empty rows in Survived\n\ndf_train = df_combined.dropna(subset=[\"Survived\"])\n\n\n#let's get the clean version of the test data by dropping the index from the train data we just got!\n\ndf_test = df_combined.drop(df_train.index)","2edd2dac":"#Then let's drop the target since it's not needed for the test data\n\ndf_test = df_test.drop(['Survived'], axis=1)","e40ceedd":"#First, let's define our data\n\ndata = [c for c in df_train.columns if c not in ['Survived','Name']]","fcef5acb":"#set our data\n\nX=df_train[data]\ndf_test=df_test[data]\n\n#y is our target\n\ny=df_train.Survived","8d773e44":"#Apply train_test_split to get the train and test values for each\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=42)","386045df":"# define the parameters you want to test out in GridSearch\n\nparams={'boosting_type':['gbdt'], \n        'learning_rate':[0.8,0.5,0.3,0.1,0.01,0.001], \n        'max_depth':[1,3,5,7,9,12,15],\n        'n_estimators':[100,300,500,700,1000,1500], \n        'num_leaves':[20,35,50,75,100,150,300,500], \n        'random_state':[42]}","d4aad18b":"#Remember, folks we are dealing with a boolean prediction, the passenger either survived or it didn't, so we shall use a Classifier instead of a Regressor.\n#I totally did not use a Regressor the first time around... O.O\n# for this model i will be using LBGM due to the performance, but you can try whatever you want!\n\nmodel = lgb.LGBMClassifier() \n\n\n#cv is for the number of folds for each fit, n_jobs=-1 automatically sets all the cores on the machine\n\n#gscv= GridSearchCV(model,params,cv=5,n_jobs=-1,verbose=True)\n\n#gscv.fit(X_train,y_train,eval_set=(X_test,y_test),early_stopping_rounds=50,verbose=False)","ec1658a6":"#y_preds=gscv.predict(X_train)\n#t_preds=gscv.predict(X_test)\n#print(accuracy_score(y_preds,y_train))\n#print(accuracy_score(t_preds,y_test))\n\n#preds = gscv.predict(df_test)\n#gscv.best_params_","62ea2340":"best_params = {'boosting_type': ['gbdt'],\n 'learning_rate': [0.3],\n 'max_depth': [8],\n 'n_estimators': [100],\n 'num_leaves': [20],\n 'random_state': [42]}","4293abba":"gscv = GridSearchCV(model,best_params,cv=5,n_jobs=-1,verbose=1)\ngscv.fit(X_train,y_train,eval_set=(X_test,y_test),early_stopping_rounds=5,verbose=False)","0b6ca7b0":"#let's check for overfitting!\n\ny_preds=gscv.predict(X_train)\nt_preds=gscv.predict(X_test)\n\nprint('Train accuracy: ',accuracy_score(y_preds,y_train))\nprint('Test accuracy: ',accuracy_score(t_preds,y_test))\n\npreds = gscv.predict(df_test)","036b076e":"index = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")[\"PassengerId\"]\nmy_submission = pd.DataFrame({'PassengerId': index, \"Survived\": preds})","843dd4f5":"#remember to set the Survived values as integers, otherwise you'll get 0% accuracy\n\nmy_submission['Survived'] = my_submission['Survived'].astype('Int64')","b57c6720":"my_submission.to_csv('my_submission.csv', index=False)\nprint('My Submission')","12161929":"**Don't worry if your model is taking a lot of time to fit, to get the best parameters GridSearch will run across all possible combinations of parameters within the given params so it may take a while depending on how many possibilities you put on the table!**","bb03a373":"We can see that older people payed the higher fares","dddba7a7":"**Very nice**","3875da6d":"**For Train Dataset**\n\n**It seems like we have:**\n12 columns,\n891 rows.\n\n**We have empty values at:**\nAge,\nCabin,\nEmbarked.\n\n**From the 12 columns:**\n5 are integer values,\n2 are float values,\n5 are objects.","6f6d8649":"**Seems we have duplicated and\/or missing values in Ticket, and Cabin columns**","cb8ab4e4":"**For Test Dataset**\n\n**It seems like we have:**\n11 columns,\n418 rows.\n\n**We have empty values at:**\nAge,\nFare,\nCabin,\nEmbarked.\n\n**From the 11 columns:**\n4 are integer values,\n2 are float values,\n5 are objects.","de953e0f":"# Hello There Fellow Novice!\n\nHey! i decided to do this notebook because i saw many great notebooks out there about EDA\/ Feature Engineering\/ Model Making, however, much of that information was really hard to understand, so thought about making it easier and better explained for people that are completely new to ML like me! \n\nFeedback is appreciated!","da55d75e":"# The End\n**That's it folks, hope you learned a thing or two in this journey! be sure to upvote the notebook if you liked, and comment any constructive criticism about the model, this is more of a learning project so the aim wasn't to climb to the top of the leaderboard but to learn the concepts and the practical applications!**","97503473":"**We have a lot of missing values on Age and Cabin, around 80% of cabin values are empty, for this model, we are going to drop this column, but feel free to try different approaches, like inputting values based on correlations!**","23e71099":"# **1.Study the data**\n**Let's take a look at the data we are dealing with!**","89e4784d":"After looking closely, i couldn't see any major link to the ticket number, apart from the relatives which shared the ticket number","4554ab1c":"**First, let's take a look at the state of our dataframe**","1890cac0":"**The Age group we defined is:**\n* From 0 - 20 - group 1\n* From 21- 40 - group 2\n* From 41- 60 - group 3\n* Ages 61+ - group 4 ","016adbb8":"**As we can see, the Test dataset Lacks the \"survived\" column, which is our Target!**","626ae4ac":"**We can see from this description that:\nour ages range from 5 months old to 80 years old,**","a627389c":"# 4.Model Making\nFor this notebook i will be using a lgb model and we will be doing parameter optimization with GridSearchCV!","7fed94a5":"# **2.Exploratory Data Analysis**","1e50875e":"**Now that's perfect! let's give the df a last look and see what we can do before moving on to model making!**","0ecc7c77":"**Then let's merge both to get a full dataset, which we will be working on!**","9146638b":"**After we get the best parameters, we can use those on the model and make our predictions!**","12d1ccdc":"**For the Combined Dataframe**\n\n**It seems like we have:**\n12 columns,\n1309 rows.\n\n**We have empty values at:**\nSurvived,\nAge,\nFare,\nCabin,\nEmbarked.\n\n**From the 12 columns:**\n4 are integer values,\n3 are float values,\n5 are objects.","5c328a0c":"**Seems we could encode the Ticked and Embarked values and split the names to get the last names and pronouns!**","3576efea":"**You may get better results by tweaking the parameters, i encourage you to play around with the parameter optimization!**","a7582d88":"**Let's define our datasets for training and testing:**","95bf6341":"**Now, let's check for the correlation between Survival and Relative count**","390b8d4b":"# 3.Preprocessing Data","97559906":"**Much better, we still have that one empty value at Fare and Embarked, so let's deal with that very quickly**","026170bc":"**There we go! we have a major correlation between the Pclass\/Sex column and survival,females at class 1 and 2 are almost certain to survive! whilst males have a way lower survival chance across all classes!**","3d110f30":"The Train and test scores are very close, so we can't say there is overfitting!","f6a8ff96":"**We can see that the people who paid the highest fares had more chances of survival**","435315e7":"**We still have quite a few missing values at Age, and on Age_Class consequently.\nSince there aren't as much missing values as the Cabin column, let's try to fill in those Gaps using Fillna!**","0d44d2ab":"**Now we have a clean train dataset, let's work it out**","735ed290":"**Let's get the number of relatives for each passenger by getting the sum of Parch and SibSp**","6cbb32b2":"We can see that younger people have greater chances of survival","41d262a4":"First of all, Welcome to Kaggle, this is a great place to learn about ML, and an even better place to practice.\nAs a newcomer myself, i may tell you one thing, don't let yourself down seeing those 100% top 1 notebooks on the top of the leaderboard, there's some shady business up there (extended datasets and stuff!!!)\nInstead of aiming for the 100% which is basically out of reach with the data we're given, let's aim to improve our personal score and learn more along the way.\n\nThe Notebook below may seem confusing and scary at first, but DON'T PANIC, get your towels and come with me, slowly!\n\nLet's get to work then!","c536a437":"**Taking a Final look at the correlations between columns what can we see in the clean data is:**\n* Survived and Fare have a significant correlation - endorsed by the graph we've seen earlier.\n* PClass and ticked have a significant correlation however, we might want to take a closer look at it.\n* Fare has significant correlations with a lot of columns - relatives, age and survival, endorsed by our previous analysis, older people tend to pay more, older people usually have relatives on board, more relatives : higher fare.","73f41c98":"**The Fare group we defined is:**\n* From 0 - 100 - group 1\n* From 101- 200 - group 2\n* From 201- 300 - group 3\n* From 301- 400 - group 4\n* From 401- 500 - group 5\n* Fares 501+ - group 6","7a7daba2":"**We have strong correlations between:**\n* Fare and Survival\n* Parch and SibSp(Family on board)"}}