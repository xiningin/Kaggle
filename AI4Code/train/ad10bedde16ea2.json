{"cell_type":{"5f2b03fc":"code","5a8b1271":"code","45882a72":"code","557949c1":"code","f4af7fa6":"code","ec2efc91":"code","a2336529":"code","864c25a5":"code","6557744f":"code","3de777da":"code","d4379ff7":"code","b35e115d":"code","e7aa5782":"code","a1f10b4a":"code","bfc79021":"code","6a46ef64":"code","fa6849fb":"code","2b5e005f":"code","3785b04c":"code","2c892183":"code","82778a6e":"code","0242251c":"code","4b8a05f3":"markdown","4b0c2cb3":"markdown","4861d5df":"markdown","a49b1e17":"markdown","26b43855":"markdown","585dfcce":"markdown","9b94552f":"markdown","1169ed2b":"markdown","1ff73804":"markdown","90e8c5df":"markdown","3b24a110":"markdown","e6715215":"markdown","f79f7166":"markdown","565a8256":"markdown","59ea00f5":"markdown","38df5ff1":"markdown","b5081e4c":"markdown","57480720":"markdown","21566fa6":"markdown","a63ceeba":"markdown","7455b3dd":"markdown","66a0b88d":"markdown","ae98518a":"markdown","6eebf700":"markdown"},"source":{"5f2b03fc":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","5a8b1271":"happy = pd.read_csv(\"..\/input\/world-happiness-report-2021\/world-happiness-report-2021.csv\")\nhappy.head()","45882a72":"happy = happy[[\"Country name\",\"Regional indicator\",\"Ladder score\",\"Logged GDP per capita\",\"Social support\",\"Healthy life expectancy\",\n               \"Freedom to make life choices\",\"Generosity\",\"Perceptions of corruption\"]]","557949c1":"happy.info()","f4af7fa6":"happy_num = happy.select_dtypes(\"float64\")\nhappy_corr = happy_num.corr()\nsns.heatmap(happy_corr, center = 0)\nplt.show()","ec2efc91":"cond_mean = happy[\"Ladder score\"].groupby(happy[\"Regional indicator\"]).aggregate(\"mean\")\ncond_std = happy[\"Ladder score\"].groupby(happy[\"Regional indicator\"]).aggregate(\"std\")\ncond_mean.plot(kind=\"bar\",figsize=(10,4),yerr = cond_std)\nplt.show()","a2336529":"ax = sns.boxplot(x=\"Ladder score\", y=\"Regional indicator\", data=happy, orient = \"h\")","864c25a5":"import plotly.express as px\npx.choropleth(happy, locations=\"Country name\", \n                    locationmode='country names', color=\"Ladder score\", \n                    color_continuous_scale=\"turbo\", \n                    title='Happiness around the world')","6557744f":"sns.displot(data = happy_num, x = \"Ladder score\",kde = True)\nplt.show()","3de777da":"happy_c = happy.copy()\nconditions = [\n    happy_c[\"Ladder score\"]<=4.5,\n    (happy_c[\"Ladder score\"]>4.5) & (happy_c[\"Ladder score\"]<=6.5),\n    (happy_c[\"Ladder score\"]>6.5)\n]\nvalues = [0,1,2]\nhappy_c[\"simple_happiness\"] = np.select(conditions, values)\n","d4379ff7":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nfeatures = [\"Logged GDP per capita\",\"Social support\",\"Healthy life expectancy\",\"Freedom to make life choices\",\"Generosity\",\n           \"Perceptions of corruption\"]\nx = happy_c.loc[:, features].values\nx = StandardScaler().fit_transform(x)\nfeat_cols = ['feature'+str(i) for i in range(x.shape[1])]\nnormalised_happiness = pd.DataFrame(x,columns=feat_cols)\nhappy_pca = PCA(n_components=2)\nprincipalComponents_happy = happy_pca.fit_transform(x)\nprincipal_happy_Df = pd.DataFrame(data = principalComponents_happy\n             , columns = ['principal component 1', 'principal component 2'])\nprint('Explained variation per principal component: {}'.format(happy_pca.explained_variance_ratio_))","b35e115d":"plt.figure()\nplt.figure(figsize=(10,10))\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=14)\nplt.xlabel('PC1',fontsize=20)\nplt.ylabel('PC 2',fontsize=20)\nplt.title(\"Principal Components for happiness\",fontsize=20)\ntargets = ['Unhappy', 'Average','Happy']\ncolors = np.array(['r','g','b'])\nfor target, color in zip(values,colors):\n    indicesToKeep = happy_c['simple_happiness'] == target\n    plt.scatter(principal_happy_Df.loc[indicesToKeep, 'principal component 1'], principal_happy_Df.loc[indicesToKeep, 'principal component 2'], c = color, s = 50)\nplt.legend(targets,prop={'size': 15})\nplt.show()","e7aa5782":"from sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, verbose=0, perplexity=25, n_iter=5000)\ntsne_results = tsne.fit_transform(x)\nhappy_c['t-SNE1'] = tsne_results[:,0]\nhappy_c['t-SNE2'] = tsne_results[:,1]\nplt.figure(figsize=(16,10))\nsns.scatterplot(\n    x=\"t-SNE1\", y=\"t-SNE2\",\n    hue=\"simple_happiness\",\n    palette=sns.color_palette(\"hls\", 3),\n    data=happy_c,\n    legend=\"full\",\n    s = 50\n)\nplt.show()","a1f10b4a":"happy_pca = PCA(n_components=3)\nprincipalComponents_happy = happy_pca.fit_transform(x)\nprincipal_happy_Df = pd.DataFrame(data = principalComponents_happy\n             , columns = ['principal component 1', 'principal component 2','principal component 3'])\nprint('Explained variation per principal component: {}'.format(happy_pca.explained_variance_ratio_))","bfc79021":"tsne = TSNE(n_components=2, verbose=0, perplexity=25, n_iter=5000)\ntsne_results = tsne.fit_transform(principal_happy_Df)\nhappy_c['t-SNE1'] = tsne_results[:,0]\nhappy_c['t-SNE2'] = tsne_results[:,1]\nplt.figure(figsize=(16,10))\nsns.scatterplot(\n    x=\"t-SNE1\", y=\"t-SNE2\",\n    hue=\"simple_happiness\",\n    palette=sns.color_palette(\"hls\", 3),\n    data=happy_c,\n    legend=\"full\",\n    s = 50\n)\nplt.show()","6a46ef64":"from sklearn.cluster import KMeans\n\ndistortions = []\nK = range(1,10)\nX = happy_c[[\"t-SNE1\",\"t-SNE2\"]]\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k)\n    kmeanModel.fit(X)\n    distortions.append(kmeanModel.inertia_)\n    \nplt.figure(figsize=(16,8))\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('Inertia level per number of clusters')\nplt.show()","fa6849fb":"from sklearn.metrics import silhouette_score,silhouette_samples\nimport matplotlib.cm as cm\n\nrange_n_clusters = [2, 3, 4, 5, 6, 7]\nsil_avg = np.zeros(8)\nfor n_clusters in range_n_clusters:\n\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n    ax1.set_xlim([-0.1, 1])\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    sil_avg[n_clusters] = silhouette_avg\n    print(\"For n_clusters =\", n_clusters,\n          \"The average silhouette_score is :\", silhouette_avg)\n    \n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) \/ n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        y_lower = y_upper + 10\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    colors = cm.nipy_spectral(cluster_labels.astype(float) \/ n_clusters)\n    ax2.scatter(X.iloc[:, 0], X.iloc[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n                c=colors, edgecolor='k')\n\n    centers = clusterer.cluster_centers_\n    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n                c=\"white\", alpha=1, s=200, edgecolor='k')\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n                    s=50, edgecolor='k')\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % n_clusters),\n                 fontsize=14, fontweight='bold')\n\n    plt.show()","2b5e005f":"plt.figure(figsize=(16,8))\nplt.plot(range(8), sil_avg, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Silhouette average')\nplt.title('Average silhouette score per number of clusters')\nplt.show()","3785b04c":"kmeans = KMeans(n_clusters=5)\nkmeans.fit(X)\ny_kmeans = kmeans.predict(X)\nplt.figure(figsize=(16,8))\nplt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y_kmeans, s=50, cmap='viridis')\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);\n","2c892183":"happy_c[\"Cluster\"] = y_kmeans\n\nfig, axes = plt.subplots(3, 2, figsize=(16,15))\nax = sns.boxplot(ax=axes[0,0], x=\"Cluster\", y=\"Logged GDP per capita\", data=happy_c)\nax.title.set_text('GDP')\nax2 = sns.boxplot(ax=axes[0,1], x=\"Cluster\", y=\"Social support\", data=happy_c)\nax2.title.set_text('Welfare')\nax3 = sns.boxplot(ax=axes[1,0], x=\"Cluster\", y=\"Healthy life expectancy\", data=happy_c)\nax3.title.set_text('Health')\nax4 = sns.boxplot(ax=axes[1,1], x=\"Cluster\", y=\"Freedom to make life choices\", data=happy_c)\nax4.title.set_text('Freedom')\nax5 = sns.boxplot(ax=axes[2,0], x=\"Cluster\", y=\"Generosity\", data=happy_c)\nax5.title.set_text('Generosity')\nax6 = sns.boxplot(ax=axes[2,1], x=\"Cluster\", y=\"Perceptions of corruption\", data=happy_c)\nax6.title.set_text('Corruption')\nplt.show()","82778a6e":"def cluster_demo(var):\n    \n    df_0 = happy_c[happy_c['Cluster']==0]\n    df_1 = happy_c[happy_c['Cluster']==1]\n    df_2 = happy_c[happy_c['Cluster']==2]\n    df_3 = happy_c[happy_c['Cluster']==3]\n    df_4 = happy_c[happy_c['Cluster']==4]\n\n    fig, ax = plt.subplots(3, 2, figsize = (12,15))\n    fig.delaxes(ax[2,1])\n\n    ax[0,0].pie(df_0[var].value_counts(), labels=df_0[var].value_counts().index)\n    ax[0,0].title.set_text('Cluster 0')\n    ax[0,1].pie(df_1[var].value_counts(), labels=df_1[var].value_counts().index)\n    ax[0,1].title.set_text('Cluster 1')\n    ax[1,0].pie(df_2[var].value_counts(), labels=df_2[var].value_counts().index)\n    ax[1,0].title.set_text('Cluster 2')\n    ax[1,1].pie(df_3[var].value_counts(), labels=df_3[var].value_counts().index)\n    ax[1,1].title.set_text('Cluster 3')\n    ax[2,0].pie(df_4[var].value_counts(), labels=df_4[var].value_counts().index)\n    ax[2,0].title.set_text('Cluster 4')\n    plt.suptitle(var)\n\n    plt.show()","0242251c":"cluster_demo('Regional indicator')","4b8a05f3":"The boxplot confirms what we found out in the barplot but it also shows that Western Europe has a much larger variance than North America (this is probably due to the fact that European countries are much more numerous than north American countries). We can also see that countries in Middle East and North Africa have a huge variance, meaning that countries in this region are very diverse in terms of happiness. Some are as happy as countries in North America, others are as unhappy as countries in Sub-Saharan Africa. Unfortunately, we can confirm that South Asia and Sub-Saharan Africa are the regions with the most unhappy countries.","4b0c2cb3":"The first 2 principal components explain around 73% of the total variance. Not bad. Let us now plot the happiness of countries in the principal components space and see if the 2 PCs can tell the 3 categories apart.","4861d5df":"Next, let us visualize the level of happiness per region of the world.","a49b1e17":"After this quick analysis using ladder score we can pass to the core of the notebook. The central idea of the analysis is to ignore the level of happiness and see if we can recover the results about the happiness of each region using only unsupervised methods. Let us start with a PCA: to understand if the PCA actually helps in distinguishing countries with different levels of happiness we need to categorize ladder score. Let us give a look at how ladder score is distributed to see how many levels the new categorical variable should have. ","26b43855":"The 5 clusters are easy to identify and clearly separated, which is comforting. Now we want to understand the characteristic of each cluster, that is, we want to study the levels of logged GDP per capita, social support, healthy life expectancy, generosity and perception of corruption for each cluster.","585dfcce":"According to the elbow method the optimal number of clusters is 2, which is somewhat surprising. Giving what we have seen at the beginning of the analysis, we would have expected at least 3 or 4 clusters. Recall that the elbow method is very empirical (it consists in spotting an elbow shape on the plot) and can quite often lead to unprecise decisions. Let us check this result by using silhouette scores. If you are not familiar with them, we suggest [this](https:\/\/towardsdatascience.com\/silhouette-coefficient-validating-clustering-techniques-e976bb81d10c) article. The method assigns a number between -1 and 1 to each cluster, the closer to 1 the better, whereas negative values indicate that clusters are assigned in the wrong way.","9b94552f":"Unsurprisingly, Logged GDP per capita, Social support and Health life expectancy are strongly correlated to the level of happiness of a country. The correlation is pretty large also between ladder score and freedom to make life choices and there is a significant negative correlation between ladder score and perception of corruption, which makes sense. All main variables (Logged GDP per capita, Social support and Health life expectancy) are strongly correlated to each other, which comes as no surprise.","1169ed2b":"Let us now proceed with the PCA: let us try to use 2 PCs, which is great for visualizing the results. ","1ff73804":"Let us see whaht we can gather from the pie charts:\n* as we guessed by the boxplots, cluster 4 is composed by Western European and North American countries, for the most part;\n* cluster 2 is composed mostly by Sub-Saharian countries, characterized by low GDP per capita and healthy life expectancy;\n* cluster 1, which has levels of GDP per capita, social support and healthy life expectancy close to cluster 4, is composed by a mixture of Latin American and Eastern European countries. This makes sense, as these countries have a quality of life relatively similar to Western Europe and North America;\n* cluster 0 is mainly composed by North African and Middle Eastern countries;\n* cluster 3, which has average levels of all the variables, is a mixture of several different regions. It is possible that the algorithm assigned to this cluster the countries that did not quite fit in any of the other clusters.","90e8c5df":"The data contains the country name, the corresponding region and a variable called ladder score which measures the level of happiness. We also have six important variables: Logged GDP per capita, Social Support, Healthy life expectancy, Freedom to make life choices, Generosity and Perceptions of corruption. There is also a number of variable that we do not need for our aims, hence we remove them.","3b24a110":"**Conclusions**\n\nIn this notebook we carried out an unsupervised analysis on the level of happiness of countries around the world. The idea of the analysis was to ignore the variable containing the levels of happiness and see if we could recover that information using unsipervised learning method. To do so we:\n* applied a PCA to reduce the dimensionality of the data;\n* since the result of the PCA was not optimal, we tried to combine it with another dimensionality-reduction technique, called t-SNE;\n* we then applied the elbow method and computed silhouette scores to conclude that splitting the data in 5 clusters could lead to interesting results;\n* we studied the main features of each cluster, concluding that the splitting suggested by the unsupervised method is in agreement with the info in the ladder score column (containing the level of happiness of each country).","e6715215":"Since the categorical variable has only visualizing purposes, we keep it simple and categorize ladder score with a 3-level variable. We are going to call the variable \"simple_happiness\" and is going to split countries in unhappy, average and happy. ","f79f7166":"The line plot makes clearer that, even though the split in 2 clusters has the highest score, also the the split in 5 clusters fares pretty good. Since the latter is more interesting for the problem at hand, we are going to continue the analysis with 5 clusters.","565a8256":"Let us check if there are null values in the data and the type of the columns.","59ea00f5":"From the scatterplot we can see that the t-SNE does a slightly better job than the PCA in distinguishing the 3 classes. To try to improve the classification even further we are going to try to use t-SNE on top of PCA, that is, apply a principal component analysis and retain as many PCs as necessary to explain a large amount of the variance and then apply t-SNE on the principal components.","38df5ff1":"The next figure is an interactive heatmap where each country is coloured according to its level of happiness. Note that few countries are white (mostly in central Africa), as we do not have data about them.","b5081e4c":"Fortunately, we do not have null values and, apart from the name of the countries and the regional indicator, all columns are real numbers. Let us start by visualizing the correlations to have an idea of the degree of dependence between columns.","57480720":"As we can see, the first 3 components explain around 85% of the total variance, which is pretty good. Let us try to feed this into the t-SNE.","21566fa6":"In the previous plot the blue bars represent the average happiness of countries in a specific region and the black bars are the corresponding standard deviations. Countries in North America and Western Europe are happier on average, whereas countries in South Asia and Sub-Saharan Africa are less happy on average. Let us use a boxplot to visualize the level of happiness per region.","a63ceeba":"The 2 principal components make a decent job of telling the 3 categories apart, but they are far from perfect. Happy countries are well identified and the unhappy countries tend to have large values of the first principal component. However, the majority of observations are all over the place. To improve the result we are going to try the t-SNE method: this is another dimensionality-reduction technique, such as PCA, but it is a probabilistic method rather than a mathematical one. For an introduction to the method, we suggest [this](https:\/\/towardsdatascience.com\/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b) blogpost. Once more, we are going to use 2 components. The parameter \"perplexity\" is especially important and, roughly speaking, it measures the degree to which we want to preserve the local topology of the observations against the global topology. It is known that acceptable values for the perplexity lie between 5 and 50, the smaller the value the more the algorithm will try to preserve the local topology, the larger the value the more it will try to preserve the global topology.\n\n\n**Point:** the scatter plot on the space of t-SNE components is not as easy to interpret as the one on the space of PCs, as the representation depends on the perplexity. [This](https:\/\/distill.pub\/2016\/misread-tsne\/) blogpost explains quite well why we should be careful in interpreting the result of the t-SNE method.","7455b3dd":"In the previous visualisation the left-hand plots are the silhouette plots, whereas the right-hand side plots are the scatterplots of the data with a specific number of clusters. The previous plots confirm that a split in 2 clusters is the best choice. However, also the split in 5 clusters is pretty good, achieving a silhouette score of 0.51. To interpret the silhouette plots we have to study:\n* the thickness of the clusters, which is proportional to the size of the clusters. In general, we do not want to have too big of an imbalancement among clusters;\n* the vertical red line represents the average silhouette score and we do not like clusters below this level.\n\n\nLet us visualize the average silhouette scores with a line plot.","66a0b88d":"Hello everyone! In this notebook we are going to study data about the level of happiness troughout the world. World countries are ranked by happiness and the level of happiness depends on different variables, each measuring a different aspect contributing to the quality of life in a country. Let us give a look at the data.","ae98518a":"These boxplot are very interesting. Let us summarise what we can learn from them:\n* cluster 4 scores pretty high in all variables but perception of corruption. We speculate that this cluster is mostly composed by countries from Western Europe and North America;\n* cluster 2 scores pretty low in all variables but perception of corruption. We speculate that this cluster is mostly composed by countries from Sub-Saharan Africa and South Asia;\n* clusters 3 and 0 seem pretty similar, the only significant differences being in generosity and freedom to make life choices, in which cluster 3 scores better;\n* cluster 1 takes the second place, so to speak, as it has values of all variables very similar to cluster 4, apart perception of corruption, which is significantly larger.\n\n\nLet us now see the composition of each cluster in terms of regions.","6eebf700":"Once more, we can see a slight improvement, though the result is far from perfect. However, we have to take into account that both PCA and t-SNE reduce the original problem to a lesser dimensional problem. Hence, a perfect separation of the classes is very hard to achieve.\n\nNow that we obtained a 2-dimensional representation of the data, let us proceed with clustering. As first thing we would like to understand which is the optimal number of clusters: we are going to use K-means as clustering algorithm and the elbow method to identify the number of clusters."}}