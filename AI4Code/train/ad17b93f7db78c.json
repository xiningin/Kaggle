{"cell_type":{"65a59f15":"code","13ed381e":"code","be100ccc":"code","fd39e475":"code","556ee0b7":"code","bb3d04b9":"code","5e17ed55":"code","2c36b65a":"code","c6fc36fe":"code","ac040fb4":"code","1a3291f2":"code","be43947a":"code","02495ae4":"code","fc677b30":"code","fbaa1842":"code","cdc9d87f":"code","d2a3ab19":"code","1cdba3eb":"code","401a5469":"code","a61c036c":"code","8ba98b19":"code","282c3ded":"code","e1ab7076":"code","847c99b6":"code","ff72b2de":"code","bd01f277":"code","2961ee18":"code","6b3e0322":"code","9b2f9cb6":"code","8ba25099":"code","abab8d9e":"code","697e9715":"code","00b3bca0":"code","2c0f64f9":"code","77885ab1":"code","7b6c017d":"code","027f6489":"code","0bf4f16f":"code","7857f46b":"code","85adce45":"code","4836806c":"code","1a9f104a":"code","4f96cff6":"markdown","edbbb974":"markdown","fb3f0e59":"markdown","023f84b2":"markdown","a65df060":"markdown","a3ead083":"markdown","c68770c2":"markdown","fedb11fd":"markdown","6f92e82d":"markdown","457bc4c0":"markdown","3b7085cf":"markdown","05c7c9c8":"markdown","b3ff6d9c":"markdown","a6f7ac65":"markdown","a6ae2950":"markdown","ed9190b9":"markdown","78f9f902":"markdown","b8d152c7":"markdown","4fa8080a":"markdown","16770976":"markdown","27b7e58a":"markdown","e77e71a9":"markdown","8f3018db":"markdown","a452a13b":"markdown","b6613e7b":"markdown","47eb35b5":"markdown","f900c86b":"markdown","94e3e8e7":"markdown","56c3234a":"markdown","15ecd886":"markdown","a6033ea2":"markdown","25278fd9":"markdown","487a9549":"markdown","c4b43598":"markdown","652e8ac7":"markdown","218d7a56":"markdown","74b1b7a5":"markdown","e6525a51":"markdown"},"source":{"65a59f15":"from sklearn.datasets import fetch_openml\nmnist = fetch_openml('mnist_784', version=1)\nmnist.keys()","13ed381e":"X, y = mnist['data'], mnist['target']\nX.shape, y.shape","be100ccc":"import matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nsome_digit = X[0]\nsome_digit_image = some_digit.reshape(28, 28)\n\nplt.imshow(some_digit_image, cmap='binary')\nplt.axis('off')\nplt.show","fd39e475":"import numpy as np\n\ny[0] ## is a string must turn this into number\ny = y.astype(np.uint8)\ny[0]","556ee0b7":"## creating the train and the test set as always, the MNIST dataset is \n## already split into a training set(the first 60,000 images) and a test\n## set (the last 10,000) images.\n\nX_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]","bb3d04b9":"## first of all i'll try to identify if the number is a 5\ny_train_5 = (y_train == 5) ## labels of the train set, 60,000 in size\ny_test_5 = (y_test == 5) ## labels of test set, 10,000 in size\ny_train_5, y_test_5","5e17ed55":"from sklearn.linear_model import SGDClassifier\n\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(X_train, y_train_5)","2c36b65a":"sgd_clf.predict([X_train[0]])","c6fc36fe":"from sklearn.model_selection import StratifiedKFold # performs stratified sampling to produce\nfrom sklearn.base import clone                      # folds that contain a representative ratio\n                                                    # of each class\nskfolds = StratifiedKFold(n_splits=3, random_state=42)\n\nfor train_index, test_index in skfolds.split(X_train, y_train_5): # Generate indices to split \n    clone_clf = clone(sgd_clf) ## using clone()                   # data into training and test set.\n    X_train_folds = X_train[train_index]\n    y_train_folds = y_train_5[train_index]\n    X_test_fold = X_train[test_index]\n    y_test_fold = y_train_5[test_index]\n    \n    print('train_index:', train_index)\n    print('test_index:', test_index)\n    \n    clone_clf.fit(X_train_folds, y_train_folds)\n    y_pred = clone_clf.predict(X_test_fold)\n    n_correct = sum(y_pred == y_test_fold)\n    print('score: ', n_correct \/ len(y_pred), '\\n-----------') ## outputs the ratio of correct predictions","ac040fb4":"#help(skfolds.split)","1a3291f2":"from sklearn.model_selection import cross_val_score\ncross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")","be43947a":"from sklearn.base import BaseEstimator\n\nclass Never5Classifier(BaseEstimator):\n    def fit(self, X, y=None):\n        return self\n    def predict(self, X):\n        return np.zeros((len(X), 1), dtype=bool)\n\nnever_5_clf = Never5Classifier()\ncross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring='accuracy')\n## it will output about 90% accuracy, as there are around 90% non fives in the data set","02495ae4":"from sklearn.model_selection import cross_val_predict\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\n#with this, i can get a prediction for each instance in the training set\ny_train_pred","fc677b30":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(y_train_5, y_train_pred)","fbaa1842":"y_train_5_perfect_predictions = y_train_5\nconfusion_matrix(y_train_5, y_train_5_perfect_predictions)","cdc9d87f":"from sklearn.metrics import precision_score, recall_score\nprecision = precision_score(y_train_5, y_train_pred) ## when it claims it is a 5, the sgd_clf is correct in (precision*100)% of the time\nrecall = recall_score(y_train_5, y_train_pred) ## (1 - recall) says % how many true 5s were not spot by the classifier\nprecision, recall                              ## that is it only detects (recall*100)% of the 5s    ","d2a3ab19":"from sklearn.metrics import f1_score\nf1_score(y_train_5, y_train_pred)","1cdba3eb":"## instead of calling the classifier's predict() method, you can call its decision_function() method,\n## which returns a score for each instance, and then use any threshold you want to make predictions\n## based on those scores:\ny_scores = sgd_clf.decision_function([some_digit])\ny_scores","401a5469":"threshold = 0  #threshold from the tradeoff between Precision and Recall\ny_some_digit_pred = (y_scores > threshold)\ny_some_digit_pred","a61c036c":"threshold = 8000  #threshold from the tradeoff between Precision and Recall\ny_some_digit_pred = (y_scores > threshold)\ny_some_digit_pred","8ba98b19":"y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3,\n                             method='decision_function')\n#wtih 'decision_function' as the method, i'll get the scores of all instances in the training set\ny_scores","282c3ded":"from sklearn.metrics import precision_recall_curve\nprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], 'b--', label='Precision')\n    plt.plot(thresholds, recalls[:-1], 'g-', label='Recall')\n    plt.xlabel('Threshold')\n    plt.legend(loc='upper left')\n    plt.ylim([0,1])\n\n\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.show()","e1ab7076":"plt.plot(recalls, precisions)\nplt.xlabel('Recalls')\nplt.ylabel('Precisions')\nplt.show()","847c99b6":"threshold_90_precision = thresholds[np.argmax(precisions >= 0.90)]\n# to make predictions (on the training set for now), instead of calling the classifier's predict() method,\n# i'll run this code:\ny_train_pred_90 = (y_scores >= threshold_90_precision)","ff72b2de":"prec_s = precision_score(y_train_5, y_train_pred_90)\nrec_s = recall_score(y_train_5, y_train_pred_90)\nprec_s, rec_s","bd01f277":"from sklearn.metrics import roc_curve\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_train_5, y_scores)\nfpr, tpr = false_positive_rate, true_positive_rate\n\ndef plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--') #Dashed diagonal line\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate (Recall)')\n\nplot_roc_curve(fpr, tpr)\nplt.show()","2961ee18":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_train_5, y_scores)","6b3e0322":"## The predict_proba() method returns an array containing a row per instance\n## and a column per class, each containing the probability that the given \n## instance belongs to the given class\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nforest_clf = RandomForestClassifier(random_state=42)\ny_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,\n                                    method='predict_proba')","9b2f9cb6":"y_scores_forest = y_probas_forest[:, 1] # score = proba of positive class\nfpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5, y_scores_forest)","8ba25099":"plt.plot(fpr, tpr, 'b:', label='SGD')\nplot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\nplt.legend(loc='lower right')\nplt.show()","abab8d9e":"roc_auc_score(y_train_5, y_scores_forest)","697e9715":"from sklearn.svm import SVC\nsvm_clf = SVC()\nsvm_clf.fit(X_train, y_train)\n## under the hood scikit-learn is using the OvO strategy: it trained 45 binary classifiers,\n## when we call .predicti() it gets their decision scores for the image, and selected the class \n## that won the most duels. ","00b3bca0":"svm_clf.predict([some_digit])","2c0f64f9":"some_digit_scores = svm_clf.decision_function([some_digit])\nsome_digit_scores","77885ab1":"svm_clf.classes_","7b6c017d":"from sklearn.multiclass import OneVsRestClassifier\novr_clf = OneVsRestClassifier(SVC())\novr_clf.fit(X_train, y_train)","027f6489":"sgd_clf.fit(X_train, y_train)\nsgd_clf.predict([some_digit])","0bf4f16f":"sgd_clf.decision_function([some_digit])","7857f46b":"cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring='accuracy')","85adce45":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\ncross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring='accuracy')","4836806c":"y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\nconf_mx = confusion_matrix(y_train, y_train_pred)\nconf_mx","1a9f104a":"plt.matshow(conf_mx, cmap=plt.cm.gray)\nplt.show()","4f96cff6":"Now, to show that i can create a classifier that can give me virtually any precision i want, i'll find the lowest threshold that gives me at least 90% **precision**","edbbb974":"# MNIST dataset","fb3f0e59":"# Classification Chapter","023f84b2":"Image representation of the confusion matrix:","a65df060":"# Training a Binary Classifier","a3ead083":"The `roc_curve()` function expects labels and scores, but instead of scores I can give it class probabilities:","c68770c2":"Precision may sometimes go down when you raise the threshold.","fedb11fd":"Now, demonstrating why 'accuracy' is not a got performance measure:","6f92e82d":"This confirms that raising the threshold reduces recall. That is what were previously a True positive is now a false negative. Lowering the threshold would reduce the precision.","457bc4c0":"Scaling inputs increases accuracy...","3b7085cf":"Training a `SGDClassifier` (or a `RandomForestClassifier`) is just as easy:","05c7c9c8":"Message from the author of the book: \"You now know how to train binary classifiers, choose the appropriate metric for your task, evaluate your classifiers using cross-validation, select the precision\/recall tradeoff that fits your needs, and use ROC curves and ROC AUC scores to compare various models.\"","b3ff6d9c":"Tying Precision and Recall together, we have the `f1_score`, which is the harmonic mean of both:","a6f7ac65":"The `RandomForestClassifier` class does not have a `decision_function()` method. Insteat it has a `predict_proba()` method.","a6ae2950":"# Measuring acuracy using cross-validation:\nThe folowing code does roughly the same thing as the Scikit-Learn's `cross_val_score()` function, and it prints the same result","ed9190b9":"# Confusion Matrix","78f9f902":"90% of precision, exactly what i forced it to be, but the recall is too low. A high precision classifier is not very usefull if its recall is too low.","b8d152c7":"Raising the threshold:","4fa8080a":"# Multiclass Classification","16770976":"Now using the proper `cross_val_score()` function to evaluate `SGDClassifier`","27b7e58a":"# Performance Measures\n# Many pages are dedicated to Performance Measures...","e77e71a9":"In the cell above Scikit-Learn did not have to run OvR or OvO because SGD classifiers can directly classify instances into multiple classes (as `RandomForestClassifier` can)","8f3018db":"# Error Analysis\nAnalysing the type of errors my model does","a452a13b":"### Evaluating:","b6613e7b":"Now i'll use `precision_recall_curve()` function to compute precision and recall for all possible thresholds:","47eb35b5":"### Trying a Support Vector Machine classifier:","f900c86b":"Now checking these predictions' `precision` and `recall`","94e3e8e7":"## Now I'll trian a `RandomForestClassifier` and compare its ROC curve and ROC AUC to those of the `SGDClassifier`","56c3234a":"# The ROC curve","15ecd886":"measuring performance of classifiers is trickier than that of regressors","a6033ea2":"I can use`OneVsOneClassifier` or `OneVsRestClassifier` classes if I wanna choose which method to use. This code creates a multiclass classifier using the OvR strategy, based on SVC:","25278fd9":"### Precision and Recall:","487a9549":"**LATER I SHOULD TRY SEEING THAT THE FOREST HAS A 99% PRECISION AND 86.6% RECAL !!!**","c4b43598":"The Random Forest classifier is superior to the SGD classifier because its ROC curve is much closer to the top-left corner, and it has a greater AUC.","652e8ac7":"A perfect classifier would have only true positives and true negatives:","218d7a56":"### The `SGDClassifier` uses a threshold equal to 0:","74b1b7a5":"Now ploting the ROC curve","e6525a51":"A good classifier will be far from the dotted line, towards the top left corner. A good way to measure the performance is to value the area under the curve (AUC). A perfect classifier will have AUC = 1, and a random one would have AUC = 0.5. We can test it with `roc_auc_score`:"}}