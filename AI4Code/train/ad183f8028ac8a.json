{"cell_type":{"2748c2f0":"code","e8e600af":"code","7042ad9d":"code","79d39d95":"code","2333d17b":"code","8f72d5ee":"code","16a1731c":"code","f1aecffb":"code","d4b71f71":"code","e20f6120":"code","4fda5f55":"code","d8c6666f":"code","fc56de1d":"code","b8735b96":"code","fc7c8d7d":"code","76b49237":"code","aaa89d84":"code","191931e2":"code","9b8f5e5a":"code","a0084325":"code","5784bb8d":"code","8eca38ef":"code","f2b0cd3b":"code","3cd24cdf":"code","2ffa8151":"code","79f2dab1":"code","67a25431":"code","ca425cd7":"code","20317cb5":"code","e41745da":"code","d7c0feff":"code","2e559363":"code","e691c704":"code","c7ea2440":"code","8022e8ee":"code","4371716d":"code","35257f20":"code","47aee6d4":"code","436d58d8":"code","d89f01ed":"code","7c014076":"code","ec948169":"code","b8f973af":"code","ff0daf01":"code","3acdf835":"code","707617f0":"code","fe71f4b2":"code","170f5d71":"code","ea3c4e41":"code","b73fa23a":"code","7f6b02ab":"code","814e1bd0":"code","df3170af":"code","a41f7b50":"code","77754002":"code","2bd25ec2":"markdown","6adcbbad":"markdown","b2f25924":"markdown","283c4c82":"markdown","2356f6b0":"markdown","238d72b6":"markdown","b4416e2b":"markdown","f14b22d0":"markdown","09759c94":"markdown","29521011":"markdown","e4afff92":"markdown","20c26b0e":"markdown","3027b4fe":"markdown","c7c916f6":"markdown","14d746bc":"markdown","cc3e823b":"markdown","20afe943":"markdown","e67fc837":"markdown","bc0fe82f":"markdown","c09caa9d":"markdown","704e701e":"markdown","cca99500":"markdown","05621e39":"markdown","016a3071":"markdown","a725176f":"markdown","4ac0ffe1":"markdown","1f6d078c":"markdown","30157c86":"markdown","04bccc58":"markdown","cd6e6981":"markdown"},"source":{"2748c2f0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.probability import FreqDist\nfrom nltk import ngrams\nimport string,re\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nimport warnings, os","e8e600af":"plt.figure(figsize=(16,7))\nplt.style.use('ggplot')\nwarnings.filterwarnings('ignore')","7042ad9d":"# Locate the data directories\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","79d39d95":"train=pd.read_csv('\/kaggle\/input\/movie-review-sentiment-analysis-kernels-only\/train.tsv.zip',sep='\\t')\ntest=pd.read_csv('\/kaggle\/input\/movie-review-sentiment-analysis-kernels-only\/test.tsv.zip',sep='\\t')","2333d17b":"train.shape, test.shape","8f72d5ee":"train.head()","16a1731c":"test.head()","f1aecffb":"train.info()","d4b71f71":"train.isnull().sum()","e20f6120":"train.head()","4fda5f55":"train['sentiment_class'] = train['Sentiment'].map({0:'negative',1:'somewhat negative',2:'neutral',3:'somewhat positive',4:'positive'})\ntrain.head()","d8c6666f":"def remove_punctuation(text):\n    return \"\".join([t for t in text if t not in string.punctuation])","fc56de1d":"train['Phrase']=train['Phrase'].apply(lambda x:remove_punctuation(x))\ntrain.head()","b8735b96":"def words_with_more_than_three_chars(text):\n    return \" \".join([t for t in text.split() if len(t)>3])","fc7c8d7d":"train['Phrase']=train['Phrase'].apply(lambda x:words_with_more_than_three_chars(x))\ntrain.head()","76b49237":"stop_words=stopwords.words('english')\ntrain['Phrase']=train['Phrase'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\ntrain.head()","aaa89d84":"train.groupby('Sentiment')['Sentiment'].count()","191931e2":"train.groupby('sentiment_class')['sentiment_class'].count().plot(kind='bar',title='Target class',figsize=(16,7),grid=True)","9b8f5e5a":"((train.groupby('sentiment_class')['sentiment_class'].count()\/train.shape[0])*100).plot(kind='pie',figsize=(7,7),title='% Target class', autopct='%1.0f%%')","a0084325":"train['PhraseLength']=train['Phrase'].apply(lambda x: len(x))","5784bb8d":"train.sort_values(by='PhraseLength', ascending=False).head()","8eca38ef":"plt.figure(figsize=(16,7))\nbins=np.linspace(0,200,50)\nplt.hist(train[train['sentiment_class']=='negative']['PhraseLength'],bins=bins,density=True,label='negative')\nplt.hist(train[train['sentiment_class']=='somewhat negative']['PhraseLength'],bins=bins,density=True,label='somewhat negative')\nplt.hist(train[train['sentiment_class']=='neutral']['PhraseLength'],bins=bins,density=True,label='neutral')\nplt.hist(train[train['sentiment_class']=='somewhat positive']['PhraseLength'],bins=bins,density=True,label='somewhat positive')\nplt.hist(train[train['sentiment_class']=='positive']['PhraseLength'],bins=bins,density=True,label='positive')\nplt.xlabel('Phrase length')\nplt.legend()\nplt.show()","f2b0cd3b":"# Install wordcoud library\n# !pip install wordcloud","3cd24cdf":"from wordcloud import WordCloud, STOPWORDS \nstopwords = set(STOPWORDS) ","2ffa8151":"word_cloud_common_words=[]  \nfor index, row in train.iterrows(): \n    word_cloud_common_words.append((row['Phrase'])) \nword_cloud_common_words\n\nwordcloud = WordCloud(width = 1600, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 5).generate(''.join(word_cloud_common_words)) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (16, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() ","79f2dab1":"text_list=[]  \nfor index, row in train.iterrows(): \n    text_list.append((row['Phrase'])) \ntext_list\n\ntotal_words=''.join(text_list)\ntotal_words=word_tokenize(total_words)","67a25431":"freq_words=FreqDist(total_words)\nword_frequency=FreqDist(freq_words)","ca425cd7":"# 10 common words\nprint(word_frequency.most_common(10))","20317cb5":"# visualize \npd.DataFrame(word_frequency,index=[0]).T.sort_values(by=[0],ascending=False).head(20).plot(kind='bar',figsize=(16,6),grid=True)","e41745da":"neg_text_list=[]  \nfor index, row in train[train['Sentiment']==0].iterrows(): \n    neg_text_list.append((row['Phrase'])) \nneg_text_list\n\nneg_total_words=' '.join(neg_text_list)\nneg_total_words=word_tokenize(neg_total_words)\n\nneg_freq_words=FreqDist(neg_total_words)\nneg_word_frequency=FreqDist(neg_freq_words)","d7c0feff":"# visualize \npd.DataFrame(neg_word_frequency,index=[0]).T.sort_values(by=[0],ascending=False).head(20).plot(kind='bar',figsize=(16,6),grid=True)","2e559363":"pos_text_list=[]  \nfor index, row in train[train['Sentiment']==4].iterrows(): \n    pos_text_list.append((row['Phrase'])) \npos_text_list\n\npos_total_words=' '.join(pos_text_list)\npos_total_words=word_tokenize(pos_total_words)\n\npos_freq_words=FreqDist(pos_total_words)\npos_word_frequency=FreqDist(pos_freq_words)","e691c704":"# visualize \npd.DataFrame(pos_word_frequency,index=[0]).T.sort_values(by=[0],ascending=False).head(20).plot(kind='bar',figsize=(16,6),grid=True)","c7ea2440":"text=\"Tom and Jerry love mickey. But mickey dont love Tom and Jerry. What a love mickey is getting from these two friends\"\nbigram_frequency = FreqDist(ngrams(word_tokenize(text),3))\nbigram_frequency.most_common()[0:5]","8022e8ee":"text_list=[]  \nfor index, row in train.iterrows(): \n    text_list.append((row['Phrase'])) \ntext_list\n\ntotal_words=' '.join(text_list)\ntotal_words=word_tokenize(total_words)\n\nfreq_words=FreqDist(total_words)\nword_frequency=FreqDist(ngrams(freq_words,2))\nword_frequency.most_common()[0:5]","4371716d":"# visualize \npd.DataFrame(word_frequency,index=[0]).T.sort_values(by=[0],ascending=False).head(20).plot(kind='bar',figsize=(16,6),grid=True)","35257f20":"train['tokenized_words']=train['Phrase'].apply(lambda x:word_tokenize(x))\ntrain.head()","47aee6d4":"count_vectorizer=CountVectorizer()\nphrase_dtm=count_vectorizer.fit_transform(train['Phrase'])","436d58d8":"phrase_dtm.shape","d89f01ed":"X_train,X_val,y_train,y_val=train_test_split(phrase_dtm,train['Sentiment'],test_size=0.3, random_state=38)\nX_train.shape,y_train.shape,X_val.shape,y_val.shape","7c014076":"model=LogisticRegression()","ec948169":"model.fit(X_train,y_train)","b8f973af":"accuracy_score(model.predict(X_val),y_val)*100","ff0daf01":"del X_train\ndel X_val\ndel y_train\ndel y_val","3acdf835":"tfidf=TfidfVectorizer()\ntfidf_dtm=tfidf.fit_transform(train['Phrase'])","707617f0":"X_train,X_val,y_train,y_val=train_test_split(tfidf_dtm,train['Sentiment'],test_size=0.3, random_state=38)\nX_train.shape,y_train.shape,X_val.shape,y_val.shape","fe71f4b2":"tfidf_model=LogisticRegression()","170f5d71":"tfidf_model.fit(X_train,y_train)","ea3c4e41":"accuracy_score(tfidf_model.predict(X_val),y_val)*100","b73fa23a":"print(tfidf_model.predict(X_val)[0:10])","7f6b02ab":"def predict_new_text(text):\n    tfidf_text=tfidf.transform([text])\n    return tfidf_model.predict(tfidf_text)","814e1bd0":"predict_new_text(\"The movie is bad and sucks!\")","df3170af":"test['Phrase']=test['Phrase'].apply(lambda x:remove_punctuation(x))\ntest['Phrase']=test['Phrase'].apply(lambda x:words_with_more_than_three_chars(x))\ntest['Phrase']=test['Phrase'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\ntest_dtm=tfidf.transform(test['Phrase'])","a41f7b50":"# Predict with test data\ntest['Sentiment']=tfidf_model.predict(test_dtm)\ntest.set_index=test['PhraseId']\ntest.head()","77754002":"# save results to csv file\n# test.to_csv('Submission.csv',columns=['PhraseId','Sentiment'],index=False)","2bd25ec2":"Common words used for positive sentiment","6adcbbad":"Prepare Training data","b2f25924":"Create Bag of words with CountVectorizer","283c4c82":"Sentiment Description","2356f6b0":"Common words used for negative sentiment","238d72b6":"Common words with word cloud","b4416e2b":"<div align='center'><h1>Import required libraries<\/h1>","f14b22d0":"Adding Phrase length","09759c94":"Preparing data with tf-idf","29521011":"Remove words with less than 2 characters","e4afff92":"new data prediction function","20c26b0e":"check sentiment categories","3027b4fe":"Distribution of phrase length on each class","c7c916f6":"<div align='center'><H1>Part 2 Machine Learning Modeling<\/H1><\/div>","14d746bc":"Measure model performance","cc3e823b":"Common bigram words used for positive sentiment","20afe943":"Predict on test data","e67fc837":"Remove punctuations","bc0fe82f":"Load data","c09caa9d":"Prepare Test Data","704e701e":"<div align='center'><H1>Part 1 Exploratory Data Analysis<\/H1><\/div>","cca99500":"Free up memory for tf-idf","05621e39":"<h1>End-to-End NLP Process with Sentiment Analysis<\/h1>","016a3071":"In this notebook we are going to go through on how to perform text classification using logistic regression and several text encoding techniques such as bag of words and tf-idf. Our task will be to classify text to determine it's sentiment class. Our dataset contains the movie review data with labeled sentiment class of 0,1,2,3 and 4 where 0 is negative, 1 somehow negative, 2 neutral, 3 somehow positive and 4 positive.<br><br>\nWe will start with Exploratory Data Analysis then perform machine learning modeling.","a725176f":"Remove stopwords","4ac0ffe1":"Split data into training and validation sets (70:30) ratio","1f6d078c":"Get percentages of each class","30157c86":"Word Frequency","04bccc58":"Visualize the target variables","cd6e6981":"Train Logistic Regression model"}}