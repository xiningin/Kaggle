{"cell_type":{"70ea2b1e":"code","d390a205":"code","30fa3ada":"code","d10ce1e4":"code","3a8e63ab":"code","8e9aea47":"code","7c6f0698":"code","6c65bf3d":"code","69c4f657":"code","adb9a053":"code","dec8e0f3":"code","f4818853":"code","3a7e3330":"code","a31862e3":"code","9dcfa65e":"code","a1d919f6":"code","491d71f8":"code","d70d7b6b":"code","e3bf530f":"code","289c049e":"code","c1176917":"code","9d44cc06":"code","98e79568":"code","017b865b":"code","fd41c821":"code","b0dfc156":"code","536ea15c":"code","9e464c39":"code","263659ea":"code","4ebccf7f":"code","f3db81d6":"code","66f5ffba":"code","0a2c96d8":"code","75770fd7":"code","faae5962":"code","709a94d5":"code","cb44832b":"code","ffd43788":"code","c98c93d9":"code","f838b080":"code","0e0739cd":"code","3b44ec36":"code","b20eae58":"code","21d81a6e":"code","53986e6a":"code","b8b81776":"code","288a562f":"code","b61ec56a":"code","492197bc":"code","608394a6":"code","bccbf1b1":"code","dabca2e1":"code","c47ace04":"code","232a550c":"code","99370c40":"code","9076389e":"code","bc1ef80c":"code","64ed6d7f":"markdown","2052dd4d":"markdown","4fe339a9":"markdown","63adfe14":"markdown","6286638d":"markdown","30c05d6b":"markdown","05b8afe5":"markdown","2e0bba52":"markdown","e46ad1a5":"markdown","95c2be87":"markdown","91907150":"markdown","eb81f9dd":"markdown","4d44130d":"markdown","51e399e0":"markdown","d0d1b54e":"markdown","9b6f5666":"markdown","3b7d60e8":"markdown","33dd53b0":"markdown","053600d8":"markdown","1940a97a":"markdown","b31b5d22":"markdown"},"source":{"70ea2b1e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\nimport tensorflow as tf \n%matplotlib inline\nimport warnings;\nwarnings.filterwarnings(\"ignore\");\nimport os\nprint(os.listdir(\"..\/input\"))","d390a205":"data = pd.read_csv(\"..\/input\/train.csv\")\ntest_sur = pd.read_csv(\"..\/input\/gender_submission.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","30fa3ada":"data.head()","d10ce1e4":"data.info()","3a8e63ab":"data.isna().sum()","8e9aea47":"import missingno as msno\nmsno.bar(data.sample(890))\nmsno.matrix(data)","7c6f0698":"data = data.drop(columns = ['Name','Ticket','Cabin','PassengerId'])\ndata['Age'][np.isnan(data['Age'])] = np.nanmedian(data['Age'])\ndata = data.dropna()","6c65bf3d":"test_sur.head()","69c4f657":"test.head()","adb9a053":"test = test.drop(columns = ['Name','Ticket','Cabin','PassengerId'])\ntest['Age'][np.isnan(test['Age'])] = np.nanmedian(test['Age'])\ntest['Fare'][np.isnan(test['Fare'])] = np.nanmedian(test['Fare'])\ntest.head()","dec8e0f3":"data.head()","f4818853":"test_df = data.copy()\nmap1 = {\"female\":0 , \"male\":1}\nmap2 = {\"S\":0 , \"C\":1 , \"Q\":2}\ntest_df['Sex']=test_df.Sex.map(map1)\ntest_df[\"Embarked\"] = test_df.Embarked.map(map2)\ncorr_map = test_df.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(corr_map,vmax=.7, square=True,annot=True,fmt=\".2f\",cmap='Blues')","3a7e3330":"sns.scatterplot(data[\"Fare\"],data[\"Age\"],color='Green')","a31862e3":"sns.catplot(x=\"Survived\", y=\"Age\", hue=\"Sex\", kind=\"swarm\", data=data, aspect=1,height=8);","9dcfa65e":"sns.factorplot(x=\"Sex\",col=\"Survived\", data=data , kind=\"count\",size=7, aspect=.7,palette=['red','green'])","a1d919f6":"sns.catplot(x=\"Survived\",hue=\"Pclass\", kind=\"count\",col='Sex', data=data,color='Violet',aspect=0.7,height=7);","491d71f8":"sns.catplot(x=\"Survived\", hue=\"SibSp\", col = 'Sex',kind=\"count\", data=data,height=7);\nsns.catplot(x=\"Survived\", hue=\"Parch\", col = 'Sex', kind=\"count\", data=data,height=7);","d70d7b6b":"emb =data.groupby('Embarked').size()\n\nplt.pie(emb.values,labels = [\"Cherbourg\",\"Queenstown\",\"Southampton\"],startangle=90,autopct='%1.1f%%');","e3bf530f":"pd.crosstab([data.Sex,data.Survived],data.Pclass, margins=True).style.background_gradient(cmap='gist_rainbow')","289c049e":"data.describe()","c1176917":"surv =data.groupby('Survived').size()\nemb_sur = data[data['Survived']==1].groupby('Embarked').size()\nemb_die = data[data['Survived']==0].groupby('Embarked').size()\n\npie_index = ['Cherbourg','Queenstown','Southampton']\n\nfig = plt.figure()\n\nax1 = fig.add_axes([0, 0, 1, 1], aspect=1)\nax1.pie(surv.values,labels=['Died','Survived'],startangle=90,autopct='%1.1f%%')\nplt.title('Survivors to Casualties Ratio',bbox={'facecolor':'0.8', 'pad':5})\n\nax2 = fig.add_axes([1, 0, 1, 1], aspect=1)\nplt.title('Survivors percentage',bbox={'facecolor':'0.8', 'pad':5})\nax2.pie(emb_sur.values,labels = pie_index,startangle = 90, autopct='%1.1f%%')\n\nax3 = fig.add_axes([2, 0, 1, 1], aspect=1)\nplt.title('Casualties percentage',bbox={'facecolor':'0.8', 'pad':5})\nax3.pie(emb_die.values,labels = pie_index,startangle = 90, autopct='%1.1f%%')\n\nplt.show()      ","9d44cc06":"sns.catplot(x=\"Embarked\",hue=\"Survived\", kind=\"count\",col='Sex', data=data,aspect=0.7,height=7);","98e79568":"emb_smt = data[data['Embarked']=='S'].groupby('Survived').size()\nemb_que = data[data['Embarked']=='Q'].groupby('Survived').size()\nemb_che = data[data['Embarked']=='C'].groupby('Survived').size()\n\nfig = plt.figure()\nax1 = fig.add_axes([0, 0, 1,1], aspect=1)\nax1.pie(emb_smt.values,labels = ['Died','Survived'],startangle = 90, autopct='%1.1f%%')\nplt.title('Southampton',bbox={'facecolor':'0.8', 'pad':5})\nax2 = fig.add_axes([1, 0, 1,1], aspect=1)\nax2.pie(emb_que.values,labels = ['Died','Survived'],startangle = 90, autopct='%1.1f%%')\nplt.title('Queenstown',bbox={'facecolor':'0.8', 'pad':5})\nax3 = fig.add_axes([2,0, 1,1], aspect=1)\nax3.pie(emb_che.values,labels = ['Died','Survived'],startangle = 90, autopct='%1.1f%%')\nplt.title('Cherbourg',bbox={'facecolor':'0.8', 'pad':5})\nplt.show()      ","017b865b":"#Converting text data to numerical\nmap1 = {\"female\":0 , \"male\":1}\nmap2 = {\"S\":0 , \"C\":1 , \"Q\":2}\ndata['Sex']=data.Sex.map(map1)\ndata[\"Embarked\"] = data.Embarked.map(map2)\ntest['Sex']=test.Sex.map(map1)\ntest[\"Embarked\"] = test.Embarked.map(map2)\n#one hot encoding\ndata = pd.get_dummies(data)\ntest = pd.get_dummies(test)","fd41c821":"data.head()","b0dfc156":"test.head()","536ea15c":"data['Family'] = data.Parch+data.SibSp\ndata.Age = data.Age\/np.mean(data.Age)\ndata.Fare = data.Fare\/np.mean(data.Fare)\ntest['Family'] = test.Parch+test.SibSp\ntest.Age = test.Age\/np.mean(test.Age)\ntest.Fare = test.Fare\/np.mean(test.Fare)","9e464c39":"data.head()","263659ea":"test.head()","4ebccf7f":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(data.iloc[:,1:],data.iloc[:,0],test_size=0.2)\n#using 70:30 split.","f3db81d6":"print(x_train.head())\nprint(y_train.head())","66f5ffba":"classifiers=[['Logistic Regression :',LogisticRegressionCV()],\n             ['SVM:',svm.LinearSVC()],\n       ['Decision Tree Classification :',DecisionTreeClassifier()],\n       ['Random Forest Classification :',RandomForestClassifier()],\n       ['Gradient Boosting Classification :', GradientBoostingClassifier()],\n       ['Ada Boosting Classification :',AdaBoostClassifier()],\n       ['Extra Tree Classification :', ExtraTreesClassifier()],\n       ['K-Neighbors Classification :',KNeighborsClassifier()],\n       ['Support Vector Classification :',SVC()],\n       ['Gaussian Naive Bayes :',GaussianNB()]]\ncla_pred=[]\nfor name,model in classifiers:\n    model=model\n    model.fit(x_train,y_train)\n    predictions = model.predict(x_test)\n    cla_pred.append(accuracy_score(y_test,predictions))\n    print(name,accuracy_score(y_test,predictions))","0a2c96d8":"plt.bar(x=[1,2,3,4,5,6,7,8,9,10], height=np.multiply(cla_pred,100)\n        ,tick_label=['LR','SVM','DTC', 'RFC', 'GBC', 'ABC', 'ETC', 'KNN', 'SVC','GNB']\n        , color=[\"Blue\",\"Green\",\"red\",\"orange\",\"Yellow\",\"cyan\",\"pink\",\"purple\",\"black\",\"violet\"])\nplt.show()","75770fd7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport keras\nfrom sklearn import preprocessing","faae5962":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\ngen_sub = pd.read_csv(\"..\/input\/gender_submission.csv\")","709a94d5":"train.head()","cb44832b":"test.head()","ffd43788":"gen_sub.head()","c98c93d9":"test.info()","f838b080":"#removing non useful columns\n#train\ntrain_clean = train.iloc[:,[1,2,4,5,6,7,9,11]]\n# train_clean['Sex'].replace('male', 0,inplace=True)\n# train_clean['Sex'].replace('female', 1,inplace=True)\ntrain_clean['Sex'] = train_clean['Sex'].astype('category')\ntrain_clean['Sex'] = train_clean['Sex'].cat.codes\n# train_clean['Embarked'].replace('S', 0,inplace=True)\n# train_clean['Embarked'].replace('C', 1,inplace=True)\n# train_clean['Embarked'].replace('Q', 2,inplace=True)\ntrain_clean.reset_index(inplace=True, drop = True)\ntrain_clean = train_clean[train_clean['Embarked'].notna()]\ncategorical = ['Embarked']\n\nfor var in categorical:\n    train_clean = pd.concat([train_clean, \n                    pd.get_dummies(train_clean[var], prefix=var)], axis=1)\n    del train_clean[var]\ntrain_clean.reset_index(inplace=True, drop = True)\ntrain_clean['Age'].fillna(train_clean['Age'].mean(), inplace=True)\ntrain_clean.reset_index(inplace=True, drop = True)\ntrain_clean['Family_Size']= train_clean['SibSp']+train_clean['Parch']\n\n\n\n\n#test\ntest_clean = test.iloc[:,[0,1,3,4,5,6,8,10]]\n# test_clean['Sex'].replace('male', 0,inplace=True)\n# test_clean['Sex'].replace('female', 1,inplace=True)\ntest_clean['Sex'] = test_clean['Sex'].astype('category')\ntest_clean['Sex'] = test_clean['Sex'].cat.codes\n# test_clean['Embarked'].replace('S', 0,inplace=True)\n# test_clean['Embarked'].replace('C', 1,inplace=True)\n# test_clean['Embarked'].replace('Q', 2,inplace=True)\ntest_clean.reset_index(inplace=True, drop = True)\ntest_clean = test_clean[test_clean['Embarked'].notna()]\ncategorical = ['Embarked']\n\nfor var in categorical:\n    test_clean = pd.concat([test_clean, \n                    pd.get_dummies(test_clean[var], prefix=var)], axis=1)\n    del test_clean[var]\n    \ntest_clean.reset_index(inplace=True, drop = True)\ntest_clean['Age'].fillna(test_clean['Age'].mean(), inplace=True)\ntest_clean['Fare'].fillna(test_clean['Fare'].mean(), inplace=True)\ntest_clean.reset_index(inplace=True, drop = True)\ntest_clean['Family_Size']= test_clean['SibSp']+test_clean['Parch']","0e0739cd":"train_clean.head()","3b44ec36":"train_clean.info()","b20eae58":"test_clean.head()","21d81a6e":"test_clean.info()","53986e6a":"# #Let's normalize these dataframe\n\n# x = train_clean.values #returns a numpy array\n# min_max_scaler = preprocessing.MinMaxScaler()\n# x_scaled = min_max_scaler.fit_transform(x)\n# train_clean = pd.DataFrame(x_scaled,columns=train_clean.columns)\n\n# x = test_clean.values #returns a numpy array\n# min_max_scaler = preprocessing.MinMaxScaler()\n# x_scaled = min_max_scaler.fit_transform(x)\n# test_clean = pd.DataFrame(x_scaled,columns=test_clean.columns)\nfrom sklearn.preprocessing import StandardScaler\n\ncontinuous = ['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Family_Size']\n\nscaler = StandardScaler()\n\nfor var in continuous:\n    train_clean[var] = train_clean[var].astype('float64')\n    train_clean[var] = scaler.fit_transform(train_clean[var].values.reshape(-1, 1))\n\nfor var in continuous:\n    test_clean[var] = test_clean[var].astype('float64')\n    test_clean[var] = scaler.fit_transform(test_clean[var].values.reshape(-1, 1))","b8b81776":"train_clean.head()","288a562f":"test_clean.head()","b61ec56a":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(train_clean.iloc[:,1:],train_clean.iloc[:,0],test_size=0.2)","492197bc":"import tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Activation,Dropout\n\ndef clf_model():\n    model=Sequential()\n    model.add(Dense(10,input_dim=10,kernel_initializer='normal',activation='relu'))\n    model.add(Dense(16,activation='relu'))\n    model.add(Dense(32,activation='relu'))\n    model.add(Dense(16,activation='relu'))\n    model.add(Dense(1,activation='relu'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model\n\nmodel = clf_model()","608394a6":"model.summary()","bccbf1b1":"from keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import ReduceLROnPlateau\nfilepath=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\nfilepath1=\"best_model.hdf5\"\ncheckpoint = ModelCheckpoint(filepath1, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss',\n    patience=4,\n    verbose=1,\n    min_lr=1e-6\n)\ncallbacks_list = [checkpoint,reduce_lr]","dabca2e1":"model.fit(x_train,y_train,validation_data = (x_test,y_test),epochs=100, batch_size=16, callbacks=callbacks_list, verbose=1)","c47ace04":"model.load_weights('best_model.hdf5') #select the best weights file","232a550c":"gen_sub['Survived'] = model.predict_classes(test_clean.iloc[:,1:])","99370c40":"gen_sub.info()","9076389e":"gen_sub.head(10)","bc1ef80c":"gen_sub.to_csv(\"submit.csv\", index=False)","64ed6d7f":"That looks much better!","2052dd4d":"## Brief History\nRMS Titanic was a British passenger liner that sank in the North Atlantic Ocean in 1912, after colliding with an iceberg during her maiden voyage from Southampton to New York City. Of the estimated **2,224 passengers** and crew aboard, more than **1,500 died**, making it one of modern history's deadliest commercial marine disasters during peacetime. RMS Titanic was the largest ship afloat at the time she entered service and was the second of three Olympic-class ocean liners operated by the White Star Line. She was built by the Harland and Wolff shipyard in Belfast. Thomas Andrews, chief naval architect of the shipyard at the time, died in the disaster.\n\nTitanic was under the command of Capt. Edward Smith, who also went down with the ship.  Although Titanic had advanced safety features such as watertight compartments and remotely activated watertight doors, it only carried enough lifeboats for 1,178 people\u2014about half the number on board, and one third of her total capacity\u2014due to outdated maritime safety regulations. The ship carried 16 lifeboat davits which could lower three lifeboats each, for a total of 48 boats. However, Titanic carried only a total of 20 lifeboats, four of which were collapsible and proved hard to launch during the sinking.\n\nAfter leaving Southampton on 10 April 1912, Titanic called at Cherbourg in France and Queenstown (now Cobh) in Ireland before heading west to New York. On 14 April, four days into the crossing and about 375 miles (600 km) south of Newfoundland, she **hit an iceberg at 11:40 p.m. ship's time**.","4fe339a9":"### It is clearly visible from above 2 plots that mostly female passengers and specially younger girls were the most prefered during evacution.","63adfe14":"Now, this gives a very good insight about family and gender relations.\nFemale passengers were helped by there family males so they have higher survival rate in case if there family is onboard.\nBut it was just the opposite for males as they were not prefered and thus most of them died saving there families.","6286638d":"Please **UPVOTE** my work. It really motivates me to create better notebooks.","30c05d6b":"## Lets handle these missing values(Data Preprocessing)","05b8afe5":"So our data has some missing values.\nLets visualize them.","2e0bba52":"## Importing Data","e46ad1a5":"## Model Creation","95c2be87":"### Now lets create some creative new features using the existing ones.","91907150":"### From the above Pie charts it could be seen that, opposing the general trend, Cherbourg has more survivors than casualties. Whereas Southampton has most no of casualties.\nThis could be because of many reasons: \n* One of them could be that some are better swimmers.\n* Other could be that passengers from Cherbourg have more no of female passengers.","eb81f9dd":"Removing name, ticket number and passengerid as they are not important features in predicting if a passenger will die or survive.\nAlso removing cabin no because it contains a lot of nan values.","4d44130d":"lets see some basic correlations in out data.","51e399e0":"### Some important inferences.\n* Fare and passenger class have a negative correlation. It means that as fare increases class becomes better.\n* Fare and survival have some positive correlation meaning rich have more probability of survival, or maybe they were preferred during evacuation.\n* No of siblings and no of parents on board have a high correlation.\n* Fare and Parch are correlated meaning that those travelling with there children or parents prefer higher class compartments.\n* Female had more probability of having parents or children onboard.\n* Younger travellers had more no of siblings or spouse on board for obvious reasons.\n* Age and Pclass have -ve correlation meaning older travellers preferred better class.\n* Female passengers have a very high chance of survival than male passengers meaning female passengers were preferred during evacuation.\n* Also pclass and survival are correlated meaning elite passengers were preferred during evacuation.","d0d1b54e":"## Getting Some Insights From The Data","9b6f5666":"## TODO:\n1. <del>Compare multiple machine learning algorithms\n2. <del>Try neural nets\n3. Try [XGBoost](https:\/\/xgboost.readthedocs.io\/en\/latest\/)","3b7d60e8":"## Setting Up","33dd53b0":"## Training on different models.","053600d8":"## Lets try Neural Networks to get a better classifier ","1940a97a":"#### Most of the passengers were from Southampton. Also Southampton was the starting port of its journey.\n![image](https:\/\/titanicfacts.net\/wp-content\/uploads\/2018\/06\/titanic-maiden-voyage-route.gif)","b31b5d22":"## **RMS Titanic**\n![image](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/fd\/RMS_Titanic_3.jpg)"}}