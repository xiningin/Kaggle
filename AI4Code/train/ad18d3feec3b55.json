{"cell_type":{"beae815d":"code","2b1928f6":"code","3a001284":"code","f6e6a195":"code","0aa884c7":"code","f180b89d":"code","102306ba":"code","c757e62e":"code","217c0e5e":"code","7001db26":"code","03bc0a00":"code","bdfbf615":"markdown"},"source":{"beae815d":"# Importing core libraries\nimport numpy as np\nimport pandas as pd\nfrom time import time\nimport pprint\nimport joblib\nfrom functools import partial\nimport gc\nimport copy\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt\n\n# Suppressing warnings because of skopt verbosity\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Regressors\nimport lightgbm as lgb\n\n# Model selection\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import mutual_info_regression\n\n# Metrics\nfrom sklearn.metrics import mean_squared_error\n\n# Data processing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\n# TensorFlow\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras import Model\nfrom  tensorflow.keras.regularizers import l2\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.metrics import RootMeanSquaredError as RMSE\nfrom tensorflow.keras.models import load_model","2b1928f6":"# Loading data \nX = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\nX_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")","3a001284":"# Preparing data as a tabular matrix\ny = X.target\nX = X.set_index('id').drop('target', axis='columns')\nX_test = X_test.set_index('id')","f6e6a195":"# Stratifying the target\ny_stratified = pd.cut(y, bins=10, labels=False)","0aa884c7":"# Dealing with categorical data\ncategoricals = [item for item in X.columns if 'cat' in item]\nordinal_encoder = OrdinalEncoder()\nX[categoricals] = ordinal_encoder.fit_transform(X[categoricals])\nX_test[categoricals] = ordinal_encoder.transform(X_test[categoricals])","f180b89d":"def rmse(y_true, y_pred):\n    return tf.sqrt(tf.reduce_mean(tf.math.squared_difference(y_true, y_pred)))\n\ndef get_DAE(input_size=256, units=256, layers=3):\n    # denoising autoencoder\n    inputs = Input((input_size,))\n    x = Dense(units, activation='relu')(inputs)\n    if layers > 1:\n        for _ in range(layers-1):\n            x = Dense(units, activation='relu')(x) \n    outputs = Dense(input_size, activation='relu')(x)\n    model = Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='mse', \n                  metrics=[rmse])\n    return model\n\ndef extract_dae_features(autoencoder, matrix, layers=3):\n    dae = list()\n    \n    for layer in range(layers):\n        get_layer_output = K.function([autoencoder.layers[0].input],\n                                      [autoencoder.layers[layer+1].output])\n        dae.append(get_layer_output([matrix])[0])\n\n    dae.append(autoencoder.predict(matrix))\n    dae = np.hstack(dae)\n    return dae   \n\nclass DataGenerator(tf.keras.utils.Sequence):\n    \"\"\"\n    Generates data for Keras\n    X: a pandas DataFrame\n    y: a pandas Series, a NumPy array or a List\n    \"\"\"\n    def __init__(self, X,\n                 col_mix = 0.2, \n                 row_mix = 0.2,\n                 batch_size=256, \n                 shuffle=True,\n                 verbose=False\n                 ):\n        \n        'Initialization'\n        self.X = X\n        self.Xn = X\n        \n        self.col_mix = col_mix\n        self.row_mix = row_mix\n\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.verbose = verbose\n        self.indexes = self._build_index()\n        self.on_epoch_end()\n        self.item = 0\n        \n    def create_noisy(self):\n        A = self.X.copy()\n        mixed_cols = np.random.choice(range(A.shape[1]), \n                                      size=int(A.shape[1]*self.col_mix), \n                                      replace=False)\n\n        for col in mixed_cols:\n            c = A[:, col]\n            np.random.shuffle(c)\n            mixed_rows = np.random.choice(range(A.shape[0]), \n                                          size=int(A.shape[0]*self.row_mix), \n                                          replace=False)\n            A[mixed_rows, col] = c[mixed_rows]\n            \n        self.Xn = A\n        gc.collect()\n        if self.verbose is True:\n            introduced_noise = (self.Xn != self.X).sum() \/ np.prod(self.X.shape)\n            rmse = np.sqrt(mean_squared_error(np.ravel(self.Xn), np.ravel(self.X)))\n            print(f\"Introduced noise: {introduced_noise:0.4f} rmse: {rmse:0.4f}\")\n        \n    def _build_index(self):\n        \"\"\"\n        Builds an index from data\n        \"\"\"\n        return np.arange(len(self.X))\n    \n    def on_epoch_end(self):\n        \"\"\"\n        At the end of every epoch, shuffle if required\n        \"\"\"\n        if self.col_mix > 0 and self.row_mix > 0:\n            self.create_noisy()\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n            \n    def __len__(self):\n        \"\"\"\n        Returns the number of batches per epoch\n        \"\"\"\n        return int(len(self.indexes) \/ self.batch_size) + 1\n    \n    \n    def __iter__(self):\n        \"\"\"\n        returns an iterable\n        \"\"\"\n        for i in range(self.__len__()):\n            self.item = i\n            yield self.__getitem__(index=i)\n        self.item = 0\n        \n    def __next__(self):\n        return self.__getitem__(index=self.item)\n    \n    def __call__(self):\n        return self.__iter__()\n            \n    def __data_generation(self, selection):\n        return self.Xn[selection, :], self.X[selection, :]\n        \n    def __getitem__(self, index):\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        samples, labels = self.__data_generation(indexes)\n        return samples, labels\n    \ndef plot_keras_history(history, measures):\n    \"\"\"\n    history: Keras training history\n    measures = list of names of measures\n    \"\"\"\n    rows = len(measures) \/\/ 2 + len(measures) % 2\n    fig, panels = plt.subplots(rows, 2, figsize=(15, 5))\n    plt.subplots_adjust(top = 0.99, bottom=0.01, hspace=0.4, wspace=0.2)\n    try:\n        panels = [item for sublist in panels for item in sublist]\n    except:\n        pass\n    for k, measure in enumerate(measures):\n        panel = panels[k]\n        panel.set_title(measure + ' history')\n        panel.plot(history.epoch, history.history[measure], label=\"Train \"+measure)\n        try:\n            panel.plot(history.epoch, history.history[\"val_\"+measure], label=\"Validation \"+measure)\n        except:\n            pass\n        panel.set(xlabel='epochs', ylabel=measure)\n        panel.legend()\n        \n    plt.show(fig)","102306ba":"rlr = ReduceLROnPlateau(monitor='val_rmse', factor=0.5, patience=5, \n                        verbose=0, min_delta=1e-6, mode='min')\n\nes = EarlyStopping(monitor='val_rmse', min_delta=1e-6, patience=30, mode='min', \n                   baseline=None, restore_best_weights=True, verbose=0)\n\nchk = ModelCheckpoint('best.model', \n                      monitor='val_rmse',\n                      mode='min', \n                      save_best_only=True, \n                      verbose=0)","c757e62e":"np.random.seed(70)\ntf.random.set_seed(70)\n\nvalidation = np.random.choice(range(len(X)), size=30_000, replace=False)\ntraining = np.array([n for n in range(len(X)) if n not in validation])\n\ntrain_data = DataGenerator(X.iloc[training, :].values,\n                     col_mix = 0.05, \n                     row_mix = 0.20,\n                     batch_size=1024, \n                     shuffle=True)\n\nvalidation_data = DataGenerator(X.iloc[validation, :].values,\n                     col_mix = 0.0, \n                     row_mix = 0.0,\n                     batch_size=512, \n                     shuffle=False)\nlayers = 3\nunits = 24\ntrials = 12\n\ncolumns = [f\"layer_{layer+1}_unit_{unit}\" for layer in range(layers) for unit in range(units)]\n\nresults = {'val_rmse': np.inf}\nfor trial in range(trials):\n    autoencoder = get_DAE(input_size=X.shape[1], units=units, layers=layers)\n\n    history = autoencoder.fit(train_data,\n                              validation_data=validation_data,\n                              epochs=500,\n                              callbacks=[chk, rlr, es],\n                              shuffle=True,\n                              verbose=0\n                             )\n    \n    val_rmse = history.history['val_rmse'][np.argmin(history.history['val_rmse'])]\n    improvement = val_rmse < results['val_rmse']\n    print(f\"Trial {trial} de-noising auto-encoder val rmse: {val_rmse} | DAE is an improvement: {improvement}\")\n    if improvement is True:\n        autoencoder = load_model('best.model', custom_objects = {\"rmse\": rmse})\n        results['val_rmse'] = val_rmse\n        results['dae'] = autoencoder\n        results['history'] = history\n        results['train'] = extract_dae_features(autoencoder,matrix=X.values, layers=layers)\n        results['test'] = extract_dae_features(autoencoder,matrix=X_test.values, layers=layers)\n        lm = LinearRegression().fit(results['train'], y)\n        results['r2'] = lm.score(results['train'], y)","217c0e5e":"results['dae'].save(\"autoencoder.h5\")\nplot_keras_history(results['history'], measures=['loss', 'rmse'])","7001db26":"pd.DataFrame(results['train'].astype(np.float32), columns=columns + list(X.columns), index=X.index).reset_index().to_csv(\"train.csv\", index=False)\npd.DataFrame(results['test'].astype(np.float32), columns=columns + list(X_test.columns), index=X_test.index).reset_index().to_csv(\"test.csv\", index=False)","03bc0a00":"ls -ls","bdfbf615":"### Another smart feature enginering is to encode your categorical using a raw or normalized frequency count (to be done only on the train set). This helps all algorithms in spotting rare feature levels and all the combinations that involve them. \n\nUsing frequency encoding your models will be able to estimate by themselves the confidence of using specific feature levels in their predictions."}}