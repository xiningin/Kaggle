{"cell_type":{"b1745f69":"code","4da8ae07":"code","212dbbae":"code","6f51f553":"code","b8d4d33b":"code","dfc301e1":"code","a1fad7dd":"code","54588f92":"code","a6f59c2e":"code","7979cb73":"code","d47cd79a":"code","cec1b6a0":"code","3747da07":"code","b2a92f34":"code","e4dda9bf":"code","534b720d":"code","a7544037":"code","f1709aae":"code","2216d3cb":"code","a8db3bd5":"code","344a2884":"code","cf55a6da":"code","ce574f76":"code","a52cea1e":"code","3e87f7d2":"code","b509cf74":"code","13d9fd41":"code","2932b309":"code","ad7aa7bf":"code","924ea9d2":"code","0fc5dafe":"code","b3abc95f":"code","db164419":"code","4faf097f":"code","81ff0ca0":"code","7e14374c":"code","9d3f619a":"code","8f9350ed":"code","5262bab6":"code","59f01681":"code","88b8c261":"code","b58e6490":"code","3ead99da":"code","c754e10d":"code","c9f305b6":"code","1b61e560":"markdown","06add695":"markdown","7139cda3":"markdown","9744e416":"markdown","20aa9e84":"markdown","f58dc409":"markdown","66098eb3":"markdown","838b1fac":"markdown","315fd2f5":"markdown","6e7d159d":"markdown","4fb7dae9":"markdown","8445ddd3":"markdown","5dc87cb6":"markdown","134fad6e":"markdown","31f62f73":"markdown","dd0a6021":"markdown","fd5600c4":"markdown","50e4fd94":"markdown","595a612b":"markdown","97bb2d28":"markdown","ea657f6a":"markdown","9495e483":"markdown","0cb23935":"markdown","25fa2b80":"markdown","a90ce14f":"markdown","72a4b7df":"markdown","e4b649f7":"markdown","674df54d":"markdown","70f57973":"markdown","796b9c1a":"markdown","ff3ba103":"markdown","64fbd54e":"markdown","1121caa1":"markdown","8be2c2f5":"markdown"},"source":{"b1745f69":"# COMPUTE_CV = False \nCOMPUTE_CV = False\n\nimport sys\nsys.path.append('..\/input\/timm-pytorch-image-models\/pytorch-image-models-master')\n\nimport numpy as np \nimport pandas as pd \n\nimport math\nimport random \nimport os \nimport cv2\nimport timm\n\nfrom tqdm import tqdm \n\nimport albumentations as A \nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch \nfrom torch.utils.data import Dataset \nfrom torch import nn\nimport torch.nn.functional as F \nimport transformers\nfrom transformers import (BertTokenizer, BertModel,\n                          DistilBertTokenizer, DistilBertModel)\nfrom transformers import AutoTokenizer, AutoModel, BertTokenizer\ntorch.cuda.empty_cache()\n\nimport gc\nimport matplotlib.pyplot as plt\nimport cudf\nimport cuml\nimport cupy\nfrom cuml import PCA\nfrom cuml.feature_extraction.text import TfidfVectorizer\n# from sklearn.feature_extraction.text import TfidfVectorizer\nfrom cuml.neighbors import NearestNeighbors\n\nimport gensim\nfrom gensim.models import Word2Vec\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nimport nltk\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nnltk.download('wordnet')\nstemmer = SnowballStemmer('english')","4da8ae07":"class CFG:\n    img_size = 512\n    batch_size = 50 # 12\n    seed = 2020\n    \n    device = 'cuda'\n    classes = 11014\n    \n    scale = 30 \n    margin = 0.5\n\ndef seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_torch(CFG.seed)","212dbbae":"def f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection \/ (len_y_pred + len_y_true)\n    return f1","6f51f553":"def read_dataset(check=False):\n    if COMPUTE_CV:\n        df = pd.read_csv(\"..\/input\/shopee-product-matching\/train.csv\")\n        if check:\n            df = pd.concat([df]*2).reset_index()\n        tmp = df.groupby('label_group').posting_id.agg('unique').to_dict()\n        df['target'] = df.label_group.map(tmp)\n        df['target'] = df['target'].apply(lambda x: ' '.join(x))\n        df_cu = cudf.DataFrame(df)\n        image_paths = '..\/input\/shopee-product-matching\/train_images\/' + df['image']\n    else:\n        df = pd.read_csv(\"..\/input\/shopee-product-matching\/test.csv\")\n        df_cu = cudf.DataFrame(df)\n        image_paths = '..\/input\/shopee-product-matching\/test_images\/' + df['image']\n    return df, df_cu, image_paths","b8d4d33b":"class ImgDataset(Dataset):\n    def __init__(self, image_paths, transforms=None):\n\n        self.image_paths = image_paths\n        self.augmentations = transforms\n\n    def __len__(self):\n        return self.image_paths.shape[0]\n\n    def __getitem__(self, index):\n        image_path = self.image_paths[index]\n        \n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        if self.augmentations:\n            augmented = self.augmentations(image=image)\n            image = augmented['image']       \n    \n        return image,torch.tensor(1)","dfc301e1":"class TextDataset(Dataset):\n    def __init__(self, csv):\n        self.csv = csv.reset_index()\n\n    def __len__(self):\n        return self.csv.shape[0]\n\n    def __getitem__(self, index):\n        row = self.csv.iloc[index]\n        text = row.title\n        \n        text = TOKENIZER(text, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n        input_ids = text['input_ids'][0]\n        attention_mask = text['attention_mask'][0]  \n        \n        return input_ids, attention_mask","a1fad7dd":"TEXT_MODEL = \"..\/input\/distilbert-base-indonesian\"\nMAX_LEN = 32 # Maximum length of text\nEMBED_DIM = 768\n\nclass TextDatasetB(Dataset):\n    def __init__(self, df, tokenizer=DistilBertTokenizer.from_pretrained(TEXT_MODEL), max_length=MAX_LEN):\n        self.df = df \n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        texts = list(df['title'].apply(lambda o: str(o)).values)\n        self.encodings = tokenizer(texts, \n                                   padding=True, \n                                   truncation=True, \n                                   max_length=max_length)\n        del texts\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        item = {key: torch.tensor(values[idx]) for key, values in self.encodings.items()}  \n        \n        return item","54588f92":"%%capture\n# Dirty code to make it work\n\nimport sys\n!cp -r ..\/input\/openai-clip\/CLIP\/CLIP-main \/tmp\/\n\n# Kaggle likes to unpack .gz files in datasets... so we have to pack it back\n!gzip -c \/tmp\/CLIP-main\/clip\/bpe_simple_vocab_16e6.txt > \/tmp\/CLIP-main\/clip\/bpe_simple_vocab_16e6.txt.gz\nsys.path.append('\/tmp\/CLIP-main')\n\n!pip install ..\/input\/openai-clip\/ftfy-5.9\/ftfy-5.9\nprint(\"ftfy\")","a6f59c2e":"import clip\nfrom PIL import Image\nfrom clip.simple_tokenizer import SimpleTokenizer\n_, preprocess = clip.load(\"..\/input\/openai-clip\/ViT-B-32.pt\", device=CFG.device, jit=False)\n_tokenizer = SimpleTokenizer()\n\n# Copied from https:\/\/github.com\/openai\/CLIP\/blob\/beba48f35392a73c6c47ae67ddffced81ad1916d\/clip\/clip.py#L164\n# but with relaxed exception\ndef tokenize(texts, context_length: int = 77) -> torch.LongTensor:\n    if isinstance(texts, str):\n        texts = [texts]\n        \n    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n\n    for i, tokens in enumerate(all_tokens):\n        n = min(len(tokens), context_length)\n        result[i, :n] = torch.tensor(tokens)[:n]\n        if len(tokens) > context_length:\n            result[i, -1] = tokens[-1]\n\n    return result\n\n# Remove EMOJI\nRE_EMOJI = re.compile(r\"\\\\x[A-Za-z0-9.\/]+\", flags=re.UNICODE)\n\ndef strip_emoji(text):\n    return RE_EMOJI.sub(r'', text)\n\nclass CLIPDataset(Dataset):\n    def __init__(self, df, images_path):\n        super().__init__()\n        self.df = df\n        self.images_path = images_path\n        self.has_target = ('label_group' in df)\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image = preprocess(Image.open(self.images_path + '\/' +  row['image']))\n        text = tokenize([strip_emoji(row['title'])])[0]\n        \n        if self.has_target:\n            return image, text, row['label_group']\n        else:\n            return image, text, 0","7979cb73":"import re\nimport gensim\nfrom gensim.models import Word2Vec\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\nTOKEN_RE = re.compile(r'[\\w]+')\ndef tokenize_text_simple_regex(txt, min_token_size=2):\n    txt = str(txt).lower()\n    all_tokens = TOKEN_RE.findall(txt)\n    return [wordnet_lemmatizer.lemmatize(token, pos=\"v\") for token in all_tokens if len(token) >= min_token_size]\n\ndef tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n    return [tokenizer(text, **tokenizer_kwargs) for text in texts]","d47cd79a":"# Config\ntransformer_model = '..\/input\/sentence-transformer-models\/paraphrase-xlm-r-multilingual-v1\/0_Transformer'\nTOKENIZER = transformers.AutoTokenizer.from_pretrained(transformer_model)\n\nTEXT_MODEL_PATH = '..\/input\/best-multilingual-model\/sentence_transfomer_xlm_best_loss_num_epochs_25_arcface.bin'\n\nmodel_params = {\n    'n_classes':11014,\n    'model_name':transformer_model,\n    'use_fc':False,\n    'fc_dim':512,\n    'dropout':0.3,\n}\n\nclass TextNetA(nn.Module):\n\n    def __init__(self,\n                 n_classes,\n                 model_name='bert-base-uncased',\n                 use_fc=False,\n                 fc_dim=512,\n                 dropout=0.0):\n        \"\"\"\n        :param n_classes:\n        :param model_name: name of model from pretrainedmodels\n            e.g. resnet50, resnext101_32x4d, pnasnet5large\n        :param pooling: One of ('SPoC', 'MAC', 'RMAC', 'GeM', 'Rpool', 'Flatten', 'CompactBilinearPooling')\n        :param loss_module: One of ('arcface', 'cosface', 'softmax')\n        \"\"\"\n        super(TextNetA, self).__init__()\n\n        self.transformer = transformers.AutoModel.from_pretrained(model_name)\n        final_in_features = self.transformer.config.hidden_size\n        \n        self.use_fc = use_fc\n    \n        if use_fc:\n            self.dropout = nn.Dropout(p=dropout)\n            self.fc = nn.Linear(final_in_features, fc_dim)\n            self.bn = nn.BatchNorm1d(fc_dim)\n            self._init_params()\n            final_in_features = fc_dim\n\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, input_ids,attention_mask):\n        feature = self.extract_feat(input_ids,attention_mask)\n        return F.normalize(feature)\n\n    def extract_feat(self, input_ids,attention_mask):\n        x = self.transformer(input_ids=input_ids,attention_mask=attention_mask)\n        \n        features = x[0]\n        features = features[:,0,:]\n\n        if self.use_fc:\n            features = self.dropout(features)\n            features = self.fc(features)\n            features = self.bn(features)\n\n        return features","cec1b6a0":"class TextNetB(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bert_model = DistilBertModel.from_pretrained(TEXT_MODEL)\n    \n    def forward(self, batch):\n        output = self.bert_model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n        last_hidden_state = output.last_hidden_state # shape: (batch_size, seq_length, bert_hidden_dim)\n        CLS_token_state = last_hidden_state[:, 0, :] # obtaining CLS token state which is the first token.\n        return CLS_token_state","3747da07":"class ArcMarginProduct(nn.Module):\n    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n        super(ArcMarginProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.scale = scale\n        self.margin = margin\n        self.ls_eps = ls_eps  # label smoothing\n        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n\n        self.easy_margin = easy_margin\n        self.cos_m = math.cos(margin)\n        self.sin_m = math.sin(margin)\n        self.th = math.cos(math.pi - margin)\n        self.mm = math.sin(math.pi - margin) * margin\n\n    def forward(self, input, label):\n        # --------------------------- cos(theta) & phi(theta) ---------------------------\n        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = torch.where(cosine > 0, phi, cosine)\n        else:\n            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n        # --------------------------- convert label to one-hot ---------------------------\n        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n        one_hot = torch.zeros(cosine.size(), device='cuda')\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps \/ self.out_features\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.scale\n\n        return output\n\nclass ImgNetA(nn.Module):\n\n    def __init__(\n        self,\n        n_classes = CFG.classes,\n        model_name = None,\n        fc_dim = 512,\n        margin = CFG.margin,\n        scale = CFG.scale,\n        use_fc = True,\n        pretrained = False):\n\n\n        super(ImgNetA,self).__init__()\n        print('Building Model Backbone for {} model'.format(model_name))\n\n        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n\n        if model_name == 'resnext50_32x4d':\n            final_in_features = self.backbone.fc.in_features\n            self.backbone.fc = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n\n        elif 'efficientnet' in model_name:\n            final_in_features = self.backbone.classifier.in_features\n            self.backbone.classifier = nn.Identity()\n            self.backbone.global_pool = nn.Identity()\n        \n        elif model_name == 'eca_nfnet_l0' or 'eca_nfnet_l1':\n            final_in_features = self.backbone.head.fc.in_features\n            self.backbone.head.fc = nn.Identity()\n            self.backbone.head.global_pool = nn.Identity()\n\n        self.pooling =  nn.AdaptiveAvgPool2d(1)\n\n        self.use_fc = use_fc\n\n        self.dropout = nn.Dropout(p=0.0)\n        self.fc = nn.Linear(final_in_features, fc_dim)\n        self.bn = nn.BatchNorm1d(fc_dim)\n        self._init_params()\n        final_in_features = fc_dim\n\n        self.final = ArcMarginProduct(\n            final_in_features,\n            n_classes,\n            scale = scale,\n            margin = margin,\n            easy_margin = False,\n            ls_eps = 0.0\n        )\n\n    def _init_params(self):\n        nn.init.xavier_normal_(self.fc.weight)\n        nn.init.constant_(self.fc.bias, 0)\n        nn.init.constant_(self.bn.weight, 1)\n        nn.init.constant_(self.bn.bias, 0)\n\n    def forward(self, image, label):\n        feature = self.extract_feat(image)\n        #logits = self.final(feature,label)\n        return feature\n\n    def extract_feat(self, x):\n        batch_size = x.shape[0]\n        x = self.backbone(x)\n        x = self.pooling(x).view(batch_size, -1)\n\n        if self.use_fc:\n            x = self.dropout(x)\n            x = self.fc(x)\n            x = self.bn(x)\n        return x\n\n# Mish function\nclass Mish_func(torch.autograd.Function):\n    \n    \"\"\"from: https:\/\/github.com\/tyunist\/memory_efficient_mish_swish\/blob\/master\/mish.py\"\"\"\n    \n    @staticmethod\n    def forward(ctx, i):\n        result = i * torch.tanh(F.softplus(i))\n        ctx.save_for_backward(i)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        i = ctx.saved_variables[0]\n  \n        v = 1. + i.exp()\n        h = v.log() \n        grad_gh = 1.\/h.cosh().pow_(2) \n\n        # Note that grad_hv * grad_vx = sigmoid(x)\n        #grad_hv = 1.\/v  \n        #grad_vx = i.exp()\n        \n        grad_hx = i.sigmoid()\n\n        grad_gx = grad_gh *  grad_hx #grad_hv * grad_vx \n        \n        grad_f =  torch.tanh(F.softplus(i)) + i * grad_gx \n        \n        return grad_output * grad_f \n\n\nclass Mish(nn.Module):\n    def __init__(self, **kwargs):\n        super().__init__()\n        pass\n    def forward(self, input_tensor):\n        return Mish_func.apply(input_tensor)\n\n\ndef replace_activations(model, existing_layer, new_layer):\n    \n    \"\"\"A function for replacing existing activation layers\"\"\"\n    \n    for name, module in reversed(model._modules.items()):\n        if len(list(module.children())) > 0:\n            model._modules[name] = replace_activations(module, existing_layer, new_layer)\n\n        if type(module) == existing_layer:\n            layer_old = module\n            layer_new = new_layer\n            model._modules[name] = layer_new\n    return model","b2a92f34":"def get_model(model_name = None, model_path = None, n_classes = None):\n    \n    model = ImgNetA(model_name = model_name)\n    if model_name == 'eca_nfnet_l0' or 'eca_nfnet_l1':\n        model = replace_activations(model, torch.nn.SiLU, Mish())\n    model.eval()\n    model.load_state_dict(torch.load(model_path))\n    model = model.to(CFG.device)\n    return model \n\nclass EnsembleModel(nn.Module):\n    \n    def __init__(self):\n        super(EnsembleModel,self).__init__()\n        \n        self.m1 = get_model('eca_nfnet_l0','..\/input\/shopee-pytorch-models\/arcface_512x512_nfnet_l0 (mish).pt')\n        self.m2 = get_model('tf_efficientnet_b5_ns','..\/input\/shopee-pytorch-models\/arcface_512x512_eff_b5_.pt')\n        \n        self.m3 = get_model('eca_nfnet_l1','..\/input\/my-embeddings\/arcface_512x512_nfnet_l1(mish)_15.pt')\n        \n    def forward(self,img,label):\n        \n        feat1 = self.m1(img,label)\n        feat2 = self.m2(img,label)\n        feat3 = self.m3(img,label)\n        \n        return (feat1 + feat2) \/ 2, feat3\n#         return feat1, feat2\n\n# class EnsembleModelConcat(nn.Module):\n    \n#     def __init__(self):\n#         super(EnsembleModelConcat,self).__init__()\n        \n#         self.m1 = get_model('eca_nfnet_l0','..\/input\/shopee-pytorch-models\/arcface_512x512_nfnet_l0 (mish).pt')\n#         self.m2 = get_model('tf_efficientnet_b5_ns','..\/input\/shopee-pytorch-models\/arcface_512x512_eff_b5_.pt')\n    \n#     def l2_norm(self, input, axit=1):\n#         norm = torch.norm(input,2,axit,True)\n#         output = torch.div(input, norm)\n#         return output\n        \n#     def forward(self,img,label):\n        \n#         feat1 = self.m1(img,label)\n#         feat2 = self.m2(img,label)\n        \n#         feat1_l2 = self.l2_norm(feat1)\n#         feat2_l2 = self.l2_norm(feat2)\n        \n#         feat_l2 = self.l2_norm(torch.cat([feat1_l2, feat2_l2], axis=1))\n    \n#         return feat_l2","e4dda9bf":"def Word2VecModel(VECTOR_SIZE=128):\n#     train = pd.read_csv(\"..\/input\/shopee-product-matching\/train.csv\")\n    test = pd.read_csv(\"..\/input\/shopee-product-matching\/test.csv\")\n#     corpus = tokenize_corpus(list(pd.concat([train['title'], test['title']])))\n    corpus = tokenize_corpus(list(test['title']))\n    \n    model = Word2Vec(\n            sentences=corpus,\n            vector_size=VECTOR_SIZE, \n            window=15, \n            min_count=1, \n            sg=1, #skip-gram\n            negative=10, \n            epochs=25, \n            seed=42,\n            workers=10\n            )\n    del corpus\n    \n    return model","534b720d":"def get_img_transforms():\n    return A.Compose(\n        [\n            A.Resize(CFG.img_size,CFG.img_size,always_apply=True),\n            A.Normalize(),\n        ToTensorV2(p=1.0)\n        ]\n    )\n\ndef get_image_embeddings(image_paths, model_name = None, model_path = None):\n#     embeds = []\n    embed1, embed2 = [], []\n    \n    model = EnsembleModel()\n#     model = EnsembleModelConcat()\n    \n    image_dataset = ImgDataset(image_paths=image_paths,transforms=get_img_transforms())\n    image_loader = torch.utils.data.DataLoader(\n        image_dataset,\n        batch_size=CFG.batch_size,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=4\n    )\n    \n    with torch.no_grad():\n        for img,label in tqdm(image_loader): \n            img = img.cuda()\n            label = label.cuda()\n            \n#             feat = model(img,label)\n#             image_embeddings = feat.detach().cpu().numpy()\n#             embeds.append(image_embeddings)\n            \n            feat1, feat2 = model(img,label)\n            image_embedding1 = feat1.detach().cpu().numpy()\n            image_embedding2 = feat2.detach().cpu().numpy()\n            embed1.append(image_embedding1)\n            embed2.append(image_embedding2)\n    \n    del model\n#     image_embeddings = np.concatenate(embeds)\n    image_embedding1 = np.concatenate(embed1)\n    image_embedding2 = np.concatenate(embed2)\n#     print(f'Our image embeddings shape is {image_embeddings.shape}')\n    print(f'Our image embeddings shape is {image_embedding1.shape}')\n    del embed1, embed2\n#     del embeds\n    gc.collect()\n#     return image_embeddings\n    return image_embedding1, image_embedding2","a7544037":"NUM_WORKERS = 4\nSEED = 42\n\ndef get_bert_embeddings(df):\n    embeds = []\n    \n    model = TextNetA(**model_params)\n    model.eval()\n    \n    model.load_state_dict(dict(list(torch.load(TEXT_MODEL_PATH).items())[:-1]))\n    model = model.to(CFG.device)\n\n    text_dataset = TextDataset(df)\n    text_loader = torch.utils.data.DataLoader(\n        text_dataset,\n        batch_size=CFG.batch_size,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=NUM_WORKERS\n    )\n    \n    \n    with torch.no_grad():\n        for input_ids, attention_mask in tqdm(text_loader): \n            input_ids = input_ids.cuda()\n            attention_mask = attention_mask.cuda()\n            feat = model(input_ids, attention_mask)\n            text_embeddings = feat.detach().cpu().numpy()\n            embeds.append(text_embeddings)\n    \n    \n    del model\n    text_embeddings = np.concatenate(embeds)\n    print(f'Our text embeddings shape is {text_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return text_embeddings","f1709aae":"model_path = '..\/input\/shopee-pytorch-models\/arcface_distilbert_model_512.pt'\n\ndef get_bertB_embeddings(df):\n    model = TextNetB()\n    model.eval()\n    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')), strict=False)\n    model = model.to(CFG.device)\n\n    text_dataset = TextDatasetB(df)\n    text_loader = torch.utils.data.DataLoader(\n        text_dataset,\n        batch_size=CFG.batch_size,\n        pin_memory=True,\n        drop_last=False,\n        num_workers=4\n    )\n\n    embeds = []\n    with torch.no_grad():\n        for data in tqdm(text_loader):\n            for k,v in data.items():\n                data[k] = v.to(CFG.device)\n            features = model(data)\n            embeds.append(features.half())\n\n    # del model\n    text_embeddings = torch.cat(embeds, dim=0).detach().cpu().numpy()\n    # del embeds\n    print(f'Our text embeddings shape is {text_embeddings.shape}')\n#     torch.save(text_embeddings, f'text_embeddings.pt')\n    return text_embeddings","2216d3cb":"def get_clip_embeddings(df, images_path):\n    embed_dim = 512\n    ds = CLIPDataset(df, images_path)\n    dl = torch.utils.data.DataLoader(ds, batch_size=2 * CFG.batch_size, shuffle=False, num_workers=4)\n    \n    model = torch.load(\"..\/input\/my-embeddings\/model_31.pkl\").to(CFG.device)\n    \n    # Allocate memory for features\n    features = np.empty((len(df), 2*embed_dim), dtype=np.float32)\n    \n    # Begin predict\n    i = 0\n    for images, texts, _ in tqdm(dl):\n        n = len(images)\n        with torch.no_grad():\n            # Generate image and text features\n            images_features = model.encode_image(images.cuda())\n            texts_features = model.encode_text(texts.cuda())\n\n        # Concat features (first images then texts)\n        features[i:i+n, :embed_dim] = images_features.cpu()\n        features[i:i+n, embed_dim:] = texts_features.cpu()\n\n        i += n\n\n    # Option to save these features (may be usefull to tune cut value\n    print(f'Our clip embeddings shape is {features.shape}')\n#     np.save(\"clip_embeddings.npy\", features)\n\n    # l2-normalize\n    features \/= np.linalg.norm(features, 2, axis=1, keepdims=True)\n    \n    return features","a8db3bd5":"def get_w2v_embeddings(df):\n    corpus = tokenize_corpus(list(df['title']))\n    \n    VECTOR_SIZE=128\n    model = Word2VecModel(VECTOR_SIZE)\n#     model.save(\"word2vec_train.model\")\n    \n    embeds = []\n    for sentence in corpus:\n        words = [w for w in sentence if w in model.wv.index_to_key]\n        words_vector = np.array([model.wv[w] for w in words])\n        if len(words_vector)==0: \n            embeds.append(np.zeros((VECTOR_SIZE), dtype='float32').tolist())\n        else:\n            embed = np.median(words_vector, axis=0).tolist()\n            embeds.append(embed)\n    embeds = np.array(embeds)\n    del corpus\n\n    print(f'Our word2vec embeddings shape is {embeds.shape}')\n    return embeds","344a2884":"def get_image_predictions(df, embeddings,threshold = 0.0):\n    \n    if len(df) > 3:\n        KNN = 50\n    else : \n        KNN = 3\n    \n    model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine') # euclidean\n    model.fit(embeddings)\n    distances, indices = model.kneighbors(embeddings)\n    print(distances.mean(axis=0))\n    \n    predictions = []\n    df['img_pred'] = ''\n    Sum = 0\n    for k in tqdm(range(embeddings.shape[0])):\n        idx = np.where(distances[k,] < threshold)[0]\n        Sum += len(idx)\n        ids = indices[k,idx]\n        posting_ids = df['posting_id'].iloc[ids].values\n        predictions.append(posting_ids)\n    df['img_pred'] = predictions\n    del model, distances, indices\n    gc.collect()\n    \n    print(Sum \/ embeddings.shape[0])\n    if COMPUTE_CV:\n        df['img_pred'] = df.img_pred.apply(lambda x: ' '.join(x))\n        score = f1_score(df['target'], df['img_pred']).mean()\n        print(f'Our f1 score for threshold {threshold} is {score}')\n    return predictions","cf55a6da":"def get_bert_predictions(df,embeddings, threshold=0.7, chunk=1024*4):\n    '''\n    When using cos_sim use normalized features else use normal features\n    '''\n    embeddings = cupy.array(embeddings)\n  \n            \n################################################# Code for Getting Preds #########################################\n    CHUNK = chunk\n\n    print('Finding similar titles...for threshold :',threshold)\n    CTS = len(embeddings)\/\/CHUNK\n    if len(embeddings)%CHUNK!=0: CTS += 1\n\n    predictions = []\n    df['bert_pred'] = ''\n    Sum = 0\n    for j in range( CTS ):\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(embeddings))\n        print('chunk',a,'to',b)\n        \n        cts = cupy.matmul(embeddings,embeddings[a:b].T).T\n\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>threshold)[0]\n            Sum += len(IDX)\n            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n#             o = ' '.join(o)\n            predictions.append(o)\n######################################################################################################################\n    print(Sum \/ embeddings.shape[0])\n    df['bert_pred'] = predictions\n    del embeddings\n    gc.collect()\n    \n    if COMPUTE_CV:\n        df['bert_pred'] = df.bert_pred.apply(lambda x: ' '.join(x))\n        score = f1_score(df['target'], df['bert_pred']).mean()\n        print(f'Our f1 score for threshold {threshold} is {score}')\n    return predictions","ce574f76":"def get_tfidf_predictions(df, max_features = 25_000, threshold=0.75):\n    \n    model = TfidfVectorizer(stop_words = 'english', binary = True, max_features = max_features)\n    text_embeddings = model.fit_transform(df_cu['title']).toarray()\n    preds = []\n    CHUNK = 1024 * 4\n\n    print('Finding similar titles...')\n    CTS = len(df)\/\/CHUNK\n    if len(df)%CHUNK!=0: CTS += 1\n    Sum = 0\n    for j in range( CTS ):\n\n        a = j*CHUNK\n        b = (j+1)*CHUNK\n        b = min(b,len(df))\n        print('chunk',a,'to',b)\n\n        # COSINE SIMILARITY DISTANCE\n        cts = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n\n        for k in range(b-a):\n            IDX = cupy.where(cts[k,]>threshold)[0]\n            Sum += len(IDX)\n            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n            preds.append(o)\n            \n    df['tfidf_pred'] = preds\n    print(Sum \/ text_embeddings.shape[0])\n    del model,text_embeddings\n    \n    if COMPUTE_CV:\n        df['tfidf_pred'] = df.tfidf_pred.apply(lambda x: ' '.join(x))\n        score = f1_score(df['target'], df['tfidf_pred']).mean()\n        print(f'Our f1 score for threshold {threshold} is {score}')\n    \n    gc.collect()\n    return preds","a52cea1e":"df,df_cu,image_paths = read_dataset(check=False)\nprint(df.shape)","3e87f7d2":"print(\"Hash Prediction...\")\nphash = df.groupby('image_phash').posting_id.agg('unique').to_dict()\nprint(\"Done.\")","b509cf74":"print(\"Image Embedding...\")\nif COMPUTE_CV:\n    image_embedding1, image_embedding2 = get_image_embeddings(image_paths.values)\n#     np.save('nfnet_l0_efficientnet_b5_cat_l0.npy', image_embeddings)\n#     image_embeddings = np.load(\"..\/input\/my-embeddings\/nfnet_l0_efficientnet_b5_add.npy\")\n#     theresholds=np.linspace(0.8,1.2,10)\n#     for threshold in theresholds:\n#         get_image_predictions_score(df,image_embeddings,threshold=threshold)\n#         get_image_predictions(df,image_embeddings,threshold)\nelse:\n#     image_embedding1, image_embedding2 = get_image_embeddings(image_paths.values)\n    image_embedding1, image_embedding2 = get_image_embeddings(image_paths.values)\nprint(\"Done.\")","13d9fd41":"print(\"Image Prediction...\")\nimg_prediction1 = get_image_predictions(df,image_embedding1,threshold=0.36)\nimg_prediction2 = get_image_predictions(df,image_embedding2,threshold=0.32)\n# img_prediction = get_image_predictions(df,image_embeddings,threshold=0.36)","2932b309":"del image_embedding1, image_embedding2\ntorch.cuda.empty_cache()\nprint(\"Done.\")","ad7aa7bf":"print(\"Tfidf Prediction...\")\ntext_predictions = get_tfidf_predictions(df, max_features = 25_000, threshold=0.75)\nprint(\"Done\")","924ea9d2":"print(\"Bert Embedding...\")\nif COMPUTE_CV:\n    bert_embeddings = get_bert_embeddings(df)\n#     np.save('sbert.npy', bert_embeddings)\n#     bert_embeddings = np.load('..\/input\/my-embeddings\/sbert.npy')\nelse:\n    bert_embeddings = get_bert_embeddings(df)\nprint(\"Done\")","0fc5dafe":"print(\"Bert Prediction...\")\nbert_predictions = get_bert_predictions(df, bert_embeddings, threshold=0.85)\n# bert_predictions = get_image_predictions(df, bert_embeddings, threshold=0.15)","b3abc95f":"del bert_embeddings\ntorch.cuda.empty_cache()\nprint(\"Done\")","db164419":"print(\"Bert Embedding...\")\nbert_embeddingsB = get_bertB_embeddings(df)\nprint(\"Done\")","4faf097f":"torch.cuda.empty_cache()\nprint(\"Bert Prediction...\")\nbert_predictionsB = get_image_predictions(df, bert_embeddingsB, threshold=0.15)","81ff0ca0":"del bert_embeddingsB\ntorch.cuda.empty_cache()\nprint(\"Done\")","7e14374c":"print(\"CLIP Embedding...\")\nif COMPUTE_CV:\n    clip_embeddings = get_clip_embeddings(df, '..\/input\/shopee-product-matching\/train_images')\n#     np.save('clip_embeddings.npy', clip_embeddings)\n#     clip_embeddings = np.load('..\/input\/my-embeddings\/clip_embeddings.npy')\nelse:\n    clip_embeddings = get_clip_embeddings(df, '..\/input\/shopee-product-matching\/test_images')\nprint(\"Done\")","9d3f619a":"print(\"Clip Prediction...\")\nclip_predictions = get_image_predictions(df,clip_embeddings,threshold=0.13)","8f9350ed":"del clip_embeddings\ntorch.cuda.empty_cache()\nprint(\"Done\")","5262bab6":"# from sklearn import preprocessing\n# def L2_normalize(data, norms = 'l2'):\n#     return preprocessing.normalize(data, norm = norms)\nprint(\"W2V Embedding...\")\nw2v_embeddings = get_w2v_embeddings(df)\nprint(\"Done\")","59f01681":"print(\"W2V Prediction...\")\nif COMPUTE_CV:\n    w2v_predictions = get_image_predictions(df, w2v_embeddings, threshold=0.03)\nelse:\n    w2v_predictions = get_image_predictions(df, w2v_embeddings, threshold=0.03)\nprint(\"Done.\")","88b8c261":"df['phash'] = df.image_phash.map(phash)\ndf['image_prediction1'] = img_prediction1\ndf['image_prediction2'] = img_prediction2\n# df['image_predictions'] = img_prediction\ndf['text_predictions'] = text_predictions\ndf['bert_predictions'] = bert_predictions\ndf['bert_predictionsB'] = bert_predictionsB\ndf['clip_predictions'] = clip_predictions\ndf['w2v_predictions'] = w2v_predictions","b58e6490":"# def higher(f,*args):\n#     res = {}\n#     keys = np.unique(np.concatenate(args))\n#     for k in keys: \n#         res[k] = np.count_nonzero(np.concatenate(args) == k)\n#     output_dict = dict(filter(lambda item: item[1] >= f, res.items()))\n#     return np.array(list(output_dict.keys()))","3ead99da":"# from collections import Counter\n# Voting = True\n# if Voting:\n#     def combine_cv(row):\n#         T = higher(2, row['image_prediction1'], row['image_prediction2'], row['clip_predictions'],\n#                    row['text_predictions'], row['bert_predictions'], row['bert_predictionsB'])\n#         All = np.unique(np.concatenate([row['image_predictions'], T, row['phash']]))\n#         return All\n\n#     def combine_predictions(row):\n#         T = higher(2, row['text_predictions'], row['bert_predictions'], row['bert_predictionsB'])\n#         All = np.unique(np.concatenate([row['image_predictions'], T, row['phash']]))\n#         return ' '.join(All)\n# else:\n#     def combine_cv(row):\n#         x = np.concatenate([row['image_predictions'], row['text_predictions'], row['phash'], \n#                             row['bert_predictions'], row['bert_predictionsB'], row['clip_predictions']])\n#         return np.unique(x)\n\n#     def combine_predictions(row):\n#         x = np.concatenate([row['image_predictions'], row['text_predictions'], row['phash'], \n#                             row['bert_predictions'], row['bert_predictionsB'], row['clip_predictions']])\n#         return ' '.join( np.unique(x))\n\n# df['matches'] = df.apply(combine_predictions, axis = 1)\n# if COMPUTE_CV:\n#     df['matches_cv'] = df.apply(combine_cv, axis = 1)\n\n# df[['posting_id', 'matches']].to_csv('submission.csv', index = False)","c754e10d":"from collections import Counter\nVoting = True\nif Voting:\n    def combine_cv(row):\n        x = np.concatenate([row['image_prediction1'], row['image_prediction2'], row['clip_predictions'], row['phash'],\n                   row['text_predictions'], row['bert_predictions'], row['bert_predictionsB'], row['w2v_predictions']])\n#         x = np.concatenate([row['bert_predictionsB']])\n        tmp = Counter(x)\n        sorted_tmp = sorted(tmp.items(), key=lambda x: x[1], reverse=True)\n#         res = ([x[0] for x in sorted_tmp])\n        res = []\n        for items in sorted_tmp:\n            if items[1] >= 2:\n                res.append(items[0])\n        return res if len(res)<50 else res[:50]\n\n    def combine_predictions(row):\n        x = np.concatenate([row['image_prediction1'], row['image_prediction2'], row['clip_predictions'], row['phash'],\n                   row['text_predictions'], row['bert_predictions'], row['bert_predictionsB'], row['w2v_predictions']])\n#         x = np.concatenate([row['bert_predictionsB']])\n        tmp = Counter(x)\n        sorted_tmp = sorted(tmp.items(), key=lambda x: x[1], reverse=True)\n#         res = ([x[0] for x in sorted_tmp])\n        res = []\n        for items in sorted_tmp:\n            if items[1] >= 2:\n                res.append(items[0])\n        res if len(res)<50 else res[:50]\n        return ' '.join(res)\nelse:\n    def combine_cv(row):\n        x = np.concatenate([row['image_prediction1'], row['image_prediction2'], row['clip_predictions'], row['phash'],\n                   row['text_predictions'], row['bert_predictions'], row['bert_predictionsB']])\n        return np.unique(x)\n\n    def combine_predictions(row):\n        x = np.concatenate([row['image_prediction1'], row['image_prediction2'], row['clip_predictions'], row['phash'],\n                   row['text_predictions'], row['bert_predictions'], row['bert_predictionsB']])\n        return ' '.join( np.unique(x))\n\ndf['matches'] = df.apply(combine_predictions, axis = 1)\nif COMPUTE_CV:\n    df['matches_cv'] = df.apply(combine_cv, axis = 1)\n\ndf[['posting_id', 'matches']].to_csv('submission.csv', index = False)","c9f305b6":"if COMPUTE_CV:\n    df['f1'] = f1_score(df['target'], df['matches'])\n    score = df['f1'].mean()\n    print(f'Final f1 score is {score}')\n\nif COMPUTE_CV:\n    nums = df['matches_cv'].shape[0]\n    count = {}\n    obj= df['matches_cv']\n    for i in range(nums):\n        max = len(obj.values[i])\n        if max not in count.keys():\n            count[max] = 1\n        else:\n            count[max] += 1\n#     print(count)\n    print(sorted(count.items(), key=lambda d:d[0]))\n#     print(df)","1b61e560":"## 3.3. Bert Embedding for Indonesian","06add695":"# 5. Beginning","7139cda3":"## 6.1. CV Score & Counting","9744e416":"## 2.1. Bert_paraphrase-xlm-r-multilingual-v1\/0_Transformer","20aa9e84":"## 2.2. Bert_Indonesian","f58dc409":"# 2. Model","66098eb3":"## 1.4. CLIP Loader","838b1fac":"## 5.5. CLIP Prediction","315fd2f5":"# 1. Dataset Loader","6e7d159d":"## 4.3. Tfidf Prediction","4fb7dae9":"## 4.2. Bert Prediction","8445ddd3":"### 2.3.1. Img Ensemble","5dc87cb6":"## 5.4. Bert Prediction","134fad6e":"## 2.3. Image Model:\u3000efficientnet\/eca_nfnet_l0","31f62f73":"## 5.3. Tfidf Prediction","dd0a6021":"## 1.1. Image Loader","fd5600c4":"## 5.4. Bert (Indonesian) Prediction","50e4fd94":"## 5.6. Word2Vec Prediction","595a612b":"## 3.2. Bert Embedding","97bb2d28":"## 1.3. Text Loader for Indonesian Bert","ea657f6a":"## 5.1. pHash Prediction","9495e483":"## 3.5. Word2Vec Embedding","0cb23935":"## 4.1. Image Prediction","25fa2b80":"# 3. Get Embedding","a90ce14f":"## F1 score","72a4b7df":"## 3.4. CLIP Embedding","e4b649f7":"# 6. Merging Result","674df54d":"## 1.5. Word2Vec Loader","70f57973":"## 5.2. Image Prediction","796b9c1a":"# 0. Config","ff3ba103":"## 2.4. Word2Vec Model","64fbd54e":"## 1.2. Text Loader","1121caa1":"## 3.1. Image Embedding","8be2c2f5":"# 4. Get Prediction"}}