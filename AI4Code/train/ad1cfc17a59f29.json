{"cell_type":{"9934c439":"code","92b46d41":"code","d4abafe5":"code","334b919c":"code","c7d94bc3":"code","05c5c599":"code","23801c9a":"code","884ff132":"code","8a7eb22a":"code","6c67f411":"code","a069f360":"code","55583523":"code","f8e5a1a9":"code","32138920":"code","907be628":"code","ebf6398e":"code","4fcbd87a":"code","c00fc871":"code","72ce4058":"code","d2a07877":"code","b5c85d96":"code","f52c24d8":"code","48ca62ee":"code","1b1d649a":"code","4122f562":"code","6b56ae92":"code","ffad1bd4":"code","dfa54061":"code","9fd1bbdf":"code","876b30ad":"code","935f3192":"code","8f919536":"code","90e15ecb":"code","66224dc3":"code","39bb17ba":"code","fd865186":"code","bc094272":"code","bd8865f8":"code","6cf16b0b":"code","57ca203a":"code","3d965ff7":"code","5f940585":"code","ba74374c":"code","8c77b69b":"code","c2895bb5":"code","12ec3bca":"code","b4611715":"code","5903d8c8":"code","01096fd3":"code","1762b5f6":"code","39787a45":"code","d0c98f65":"code","c882afa7":"code","30d6a64f":"code","6637359d":"code","4f698a10":"markdown","57eb4c8f":"markdown","649da359":"markdown","5f47ac1e":"markdown","c74deba3":"markdown","ac3c590e":"markdown","71d85068":"markdown","816db598":"markdown","690be1fe":"markdown","25c6e1ea":"markdown","33b9f0f6":"markdown","08ecca8f":"markdown","fc53f2c4":"markdown","61a6cafc":"markdown","72a6c2b3":"markdown","b5e20749":"markdown","a8d65314":"markdown"},"source":{"9934c439":"import numpy as np \nimport pandas as pd \n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","92b46d41":"df = pd.read_csv('..\/input\/ieee-pes-bdc-datathon-year-2020\/train.csv')","d4abafe5":"df.head()","334b919c":"testdf1 = pd.read_csv('..\/input\/ieee-pes-bdc-datathon-year-2020\/test.csv')","c7d94bc3":"testdf1.head()","05c5c599":"testdf = testdf1.drop(columns =['ID'])","23801c9a":"testdf.head()","884ff132":"Y = df['global_horizontal_irradiance']","8a7eb22a":"Y.head()","6c67f411":"X = df.drop(columns =['global_horizontal_irradiance', 'ID'])","a069f360":"X.head()","55583523":"from sklearn.ensemble import RandomForestRegressor\n\n#just for ignoring some unimportant warnings\n\nimport warnings \nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)","f8e5a1a9":"model1 = RandomForestRegressor(verbose = True ,n_estimators=5, n_jobs = -1, oob_score=True, random_state=100) # use n_estimators = 500\nmodel1.fit(X,Y)","32138920":"predicted_values1 = model1.predict(testdf)","907be628":"Output1 = pd.DataFrame()\nOutput1[\"predicted_values1\"] = predicted_values1\nOutput1.to_csv(\"Output1.csv\", index = False)","ebf6398e":"Output1.head()","4fcbd87a":"import os\nos._exit(00)","c00fc871":"import pandas as pd\ndf = pd.read_csv('..\/input\/ieee-pes-bdc-datathon-year-2020\/train.csv')","72ce4058":"df.head()","d2a07877":"testdf1 = pd.read_csv('..\/input\/ieee-pes-bdc-datathon-year-2020\/test.csv')","b5c85d96":"testdf1.head()","f52c24d8":"testdf = testdf1.drop(columns =['ID'])","48ca62ee":"testdf.head()","1b1d649a":"Y = df['global_horizontal_irradiance']","4122f562":"Y.head()","6b56ae92":"X = df.drop(columns =['global_horizontal_irradiance', 'ID'])","ffad1bd4":"X.head()","dfa54061":"#importing the model\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n#just for ignoring some unimportant warnings\n\nimport warnings \nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)","9fd1bbdf":"#creating the model and fitting it to the data\nmodel2 = RandomForestRegressor(verbose = True,n_estimators=6,n_jobs = -1, oob_score=True, random_state=200) # use n_estimators = 600\nmodel2.fit(X,Y)","876b30ad":"#predicting the output\npredicted_values2 = model2.predict(testdf)\n","935f3192":"#saving the predictions to a csv file for averaging it later with other outputs \nOutput2 = pd.DataFrame()\nOutput2[\"predicted_values2\"] = predicted_values2\nOutput2.to_csv(\"Output2.csv\", index = False)","8f919536":"Output2.head()","90e15ecb":"import os\nos._exit(00)","66224dc3":"import pandas as pd\ndf = pd.read_csv('..\/input\/ieee-pes-bdc-datathon-year-2020\/train.csv')","39bb17ba":"df.head()","fd865186":"testdf1 = pd.read_csv('..\/input\/ieee-pes-bdc-datathon-year-2020\/test.csv')","bc094272":"testdf1.head()","bd8865f8":"testdf = testdf1.drop(columns =['ID'])","6cf16b0b":"testdf.head()","57ca203a":"Y = df['global_horizontal_irradiance']","3d965ff7":"Y.head()","5f940585":"X = df.drop(columns =['global_horizontal_irradiance', 'ID'])","ba74374c":"X.head()","8c77b69b":"#importing the model\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n#just for ignoring some unimportant warnings\n\nimport warnings \nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)","c2895bb5":"#creating the model and fitting it to the data\nmodel3 = RandomForestRegressor(verbose = True,n_estimators=7,n_jobs = -1, oob_score=True, random_state=300) #Use n_estimators = 700\nmodel3.fit(X,Y)","12ec3bca":"#predicting the output\npredicted_values3 = model3.predict(testdf)","b4611715":"#saving the predictions to a csv file for averaging it later with other outputs \nOutput3 = pd.DataFrame()\nOutput3[\"predicted_values3\"] = predicted_values3\nOutput3.to_csv(\"Output3.csv\", index = False)","5903d8c8":"Output3.head()","01096fd3":"import os\nos._exit(00)","1762b5f6":"import pandas as pd","39787a45":"predict1 = pd.read_csv(\".\/Output1.csv\")\npredict2 = pd.read_csv(\".\/Output2.csv\")\npredict3 = pd.read_csv(\".\/Output3.csv\")","d0c98f65":"avgPredict = (predict1.iloc[:,0]+ predict2.iloc[:,0] + predict3.iloc[:,0])\/3","c882afa7":"avgPredict","30d6a64f":"finalOutput = pd.DataFrame()\ntestdf = pd.read_csv(\"..\/input\/ieee-pes-bdc-datathon-year-2020\/test.csv\")\nfinalOutput[\"ID\"] = testdf.iloc[:,0]\nfinalOutput[\"global_horizontal_irradiance\"] = avgPredict\nfinalOutput.to_csv(\"finalOutput.csv\", index = False)","6637359d":"finalOutput","4f698a10":"### Importing the model\n","57eb4c8f":">Just start running again from the cell below after all the previous cells have been run and the kernel restarted","649da359":"### Saving the predictions to a csv file for averaging it later with other outputs \n","5f47ac1e":">Just start running again from the cell below after all the previous cells have been run and the kernel restarted","c74deba3":"## Model 3","ac3c590e":"# Model 2","71d85068":"## Averaging","816db598":"### Predicting the output","690be1fe":"### Summary\n- We are going to apply the model `RandomForestRegressor` from `sklearn.ensemble` 3 times with different parameters. Those models are denoted by Model 1, Model 2 and Model 3 \n- After that, in the 4th step, we will take the three outputs from those models and calculate the final prediction by averaging them.\n\n> These models will take a bit of time for training. Fortunately there is a parameter called `n_jobs` in this model. It decides how many threads to use while training the model. If you use `n_jobs = -1` then all available threads of processor will be used and it'll take around 10-15 minutes to train each model. But, on kaggle, if you use the real values of parameters I used for training (n_estimators = 500, 600 and 700), you won't be able to run this notebook with `n_jobs = -1` with normal configuration Kaggle gives you. Because the followed process eats a lot of ram (>10 GB) and the notebooks kernel will restart even before completing the training of a single model. To be honest, I trained the models on my pc (with `n_jobs = -1` of course). My pc was at its limit. \n\n> Unfortunately, even not using `n_jobs = -1` may not be enough to run this notebook with real parameter values  on Kaggle. No matter what value you choose for n_jobs, It will show that `Your notebook tried to allocate more memory than is available. It has restarted`. I would rather suggest to take the key ideas from this notebook and then download the pc version I have provided in this directory `..\/input\/notebook-trained-on-pc-for-ieeepesbdcdatathon2020\/IEEE_PES_BDC_Datathon_pcNotebook.ipynb`.  Moreover I added the outputs of these models that I trained on my pc in the `..\/input\/trained-outputs-for-model-1-2-and-3` directory. You can use it and jump to the 4th step here in this notebook. \n\n>For demonstrating purposes, I will set the parameters of the models of this notebook to such values that Kaggle can quickly give the outputs. (I will set n_estimators = 5, 6 and 7 for model 1,2 and 3 respectively). I will also mention the real values of the parameters that will give the outputs that earned 3rd place.\n\n> Note that 2 of the models cannot be trained consecutively without restarting the kernel (If you use the real values of the parameters). That's why I used `os._exit(00)` between every two models. When you restart the kernel, all the variables and outputs will be lost. So, in each step, the needed variables were re-initialized where possible, and the outputs of each model were being stored in 3 .csv files. In the final (4th) step, the outputs from those .csv files were read and averaged to make the final prediction. ","25c6e1ea":"# IEEE PES BDC DataThon, Year-2020; Predicting The Value of Solar Radiation","33b9f0f6":"**Well, in order to achieve better predictions, use the suggested n_estimator values.**","08ecca8f":"### Reading Training and Test Datasets and Preparing Them","fc53f2c4":"### Creating the model and fitting it to the data\n","61a6cafc":">Just start running again from the cell below after all the previous cells have been run and the kernel restarted","72a6c2b3":"  >If the outputs were loaded from pretrained output files\n  \n    predict1 = pd.read_csv(\"..\/input\/trained-outputs-for-model-1-2-and-3\/Output1.csv\")\n    predict2 = pd.read_csv(\"..\/input\/trained-outputs-for-model-1-2-and-3\/Output2.csv\")\n    predict3 = pd.read_csv(\"..\/input\/trained-outputs-for-model-1-2-and-3\/Output3.csv\")","b5e20749":"### Saving the final output in Proper Format","a8d65314":"## Model 1"}}