{"cell_type":{"00a86bbc":"code","57cdbff3":"code","35fc85e4":"code","d18a9aa4":"code","517114c3":"code","6d8a98a5":"code","e469c8ec":"code","2da4385f":"code","fb5c6027":"code","eeaffc0b":"code","aa9ae044":"code","c4988d2b":"code","0fd668b7":"markdown","2a469993":"markdown","766745da":"markdown","0b6f5070":"markdown","6b9ea3ef":"markdown","3858f3e2":"markdown","66a14f73":"markdown","76d716f5":"markdown","922e947f":"markdown","0ea6ab11":"markdown","9d4bf430":"markdown"},"source":{"00a86bbc":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport random as rand\nimport math\nimport time\nimport glob\nimport pickle\nimport scipy\nimport cv2\nimport re\nimport plotly.express as px\nimport collections\nimport seaborn as sns\nimport pydot\nimport graphviz\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import RobustScaler, Normalizer\nfrom tensorflow.keras import Sequential\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Conv2D, MaxPooling2D, Flatten, concatenate, add\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model\n\n!pip install keras_tuner\nimport keras_tuner as kt","57cdbff3":"train_path = '..\/input\/petfinder-pawpularity-score\/train\/*.jpg'\ntest_path = '..\/input\/petfinder-pawpularity-score\/test\/*.jpg'\ntrain_csv_path = '..\/input\/petfinder-pawpularity-score\/train.csv'\ntest_csv_path = '..\/input\/petfinder-pawpularity-score\/test.csv'\n\ntrain_images = glob.glob(train_path)\ntest_images = glob.glob(test_path)\ndf_train = pd.read_csv(train_csv_path )\ndf_test = pd.read_csv(test_csv_path)\ndf_train","35fc85e4":"df_examine = df_train.drop(['Id','Pawpularity'],axis=1).value_counts()\ndf_examine = df_examine.reset_index().rename(columns={0:'Count'})\n\ndf_train.drop(columns=['Id'],axis=1)\ncolumns = list(df_examine.columns)\ncolumns.remove('Count')\n\nk_values = []\nfor row in df_examine[columns].values:\n    mask = df_train[columns].values == row\n    k = df_train[mask.all(axis=1)]['Pawpularity'].mean()\n    k_values.append(k)\n\ndf_examine['k_values'] = k_values\ndf_examine.corr().to_csv('.\/correlation.csv')\ncorr = df_examine.corr()['k_values']\n\n# finds columns where < 5% contributuion is found to the K-value score. ex. 95% probably that they don't have an effect on the pawpularity scoring metric\nmask = corr.between(-0.05, 0.05) \ncolumns_to_remove = list(corr[mask].index)\ncolumns_to_remove.remove('Count')\ncolumns_to_remove","d18a9aa4":"sq_shape = 128\n\nclass Preprocess:\n    \n    def __init__(self, df, images, columns_to_remove=columns_to_remove, sq_shape=128, is_test=0):\n        self.df = df.drop(columns=columns_to_remove, axis=1)\n        self.images = images\n        self.sq_shape = sq_shape\n        self.is_test = is_test\n        self.X = None\n        self.y = None\n        self.X_meta = None\n        \n        \n    def __process_image(self, image):\n        im = cv2.imread(image, cv2.IMREAD_GRAYSCALE) #reads image as greyscale\n        im = cv2.resize(im, (self.sq_shape, self.sq_shape)) # resizes to 256 x 256 for processing speed\n        im = im\/255 # normalizes\n        return im\n        \n    def compile(self):\n\n        self.X = []\n        self.X_meta = []\n        if not self.is_test:\n            self.y = []\n\n        for image_number, image in enumerate(self.images):\n            image_name = image[image.rfind('\/')+1:image.rfind('.')]\n            mask = self.df['Id'] == image_name\n            im = self.__process_image(image)\n\n            if self.is_test:\n                X_meta_data = self.df[mask].drop(columns=['Id'],axis=1).values[0]\n            else:\n                Pawpularity_score = self.df[mask]['Pawpularity'].values[0]\n                X_meta_data = self.df[mask].drop(columns=['Pawpularity','Id'],axis=1).values[0]\n                \n            self.X.append(im)\n            self.X_meta.append(X_meta_data)\n            if not self.is_test:\n                self.y.append(Pawpularity_score)\n\n            clear_output(wait=True)\n            print(f'{round(image_number\/len(self.images)*100,4)}%, {image_name}')\n\n        self.X = np.asarray(self.X)\n        self.X_meta = np.asarray(self.X_meta)\n        \n        if not self.is_test:\n            self.y = np.asarray(self.y) #\/100 \n        \n        if self.is_test:\n            print(\"Processed Testing Data\")\n        else:\n            print(\"Processed Training Data\")\n        return self.X, self.X_meta, self.y\n        \npp_train = Preprocess(df=df_train, images=train_images, columns_to_remove=columns_to_remove, sq_shape=sq_shape, is_test=0)\nX, X_meta, y = pp_train.compile()","517114c3":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_meta_train, X_meta_test, y_meta_train, y_meta_test = train_test_split(X_meta, y, test_size=0.2, random_state=42)\n\nprint('Image train',X_train.shape, y_train.shape)\nprint('Image test',X_test.shape, y_test.shape)\n\nprint('Meta train', X_meta_train.shape, y_meta_train.shape)\nprint('Meta test', X_meta_test.shape, y_meta_test.shape)\n\nprint(y_train[:10],y_meta_train[:10])","6d8a98a5":"def model_builder(hp):\n\n    \"\"\"\n        Image model & Categorical Data Combination Functional API Model\n    \"\"\"\n    min_node = 8\n    max_node = 1000\n    step_size = 20\n    # Image Data\n    image_input = Input(shape=(sq_shape, sq_shape, 1))\n    x = Conv2D(hp.Int('C_1', min_value=min_node, max_value=max_node, step=step_size), kernel_size=4, activation='relu')(image_input)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    x = Dropout(0.1)(x)\n    x = Conv2D(hp.Int('C_2', min_value=min_node, max_value=max_node, step=step_size), kernel_size=4, activation='relu')(x)\n    x = Conv2D(hp.Int('C_3', min_value=min_node, max_value=max_node, step=step_size), kernel_size=4, activation='relu')(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    x = Dropout(0.1)(x)\n    x = Conv2D(hp.Int('C_4', min_value=min_node, max_value=max_node, step=step_size), kernel_size=4, activation='relu')(x)\n    x = Conv2D(hp.Int('C_5', min_value=min_node, max_value=max_node, step=step_size), kernel_size=4, activation='relu')(x)\n    x = Conv2D(hp.Int('C_6', min_value=min_node, max_value=max_node, step=step_size), kernel_size=4, activation='relu')(x)\n    x = MaxPooling2D(pool_size=(2, 2))(x)\n    x = Dropout(0.1)(x)\n    image_arm = Flatten()(x)\n\n    # Categorical & Meta Data\n    columns = X_meta_train.shape[1]\n    meta_input = Input(shape=(columns))\n    y = Dense(hp.Int('N_Dense_1', min_value=min_node, max_value=max_node, step=step_size), activation='relu')(meta_input)\n    y = Dense(hp.Int('N_Dense_2', min_value=min_node, max_value=max_node, step=step_size), activation='relu')(y)\n    y = Dense(hp.Int('N_Dense_3', min_value=min_node, max_value=max_node, step=step_size), activation='relu')(y)\n    y = Dense(hp.Int('N_Dense_4', min_value=min_node, max_value=max_node, step=step_size), activation='relu')(y)\n    meta_arm = Dense(10, activation='relu')(y)\n\n    # Merging arms of Model\n    merge = concatenate([image_arm, meta_arm])\n\n    # Finalization and Output\n    z = Dense(hp.Int('N_Dense_5', min_value=min_node, max_value=max_node, step=step_size), activation='relu')(merge)\n    z = Dense(hp.Int('N_Dense_6', min_value=min_node, max_value=max_node, step=step_size), activation='relu')(z)\n    output = Dense(1, activation='linear')(z)\n\n    # Initialize Model as model\n    model = Model(inputs=[image_input, meta_input], outputs=output)\n\n    # Compile Model\n    \n    model.compile(loss='mse', optimizer='adam', metrics=['mean_absolute_percentage_error'])\n    print(model.summary())\n    plot_model(model, to_file=f'.\/{sq_shape}x{sq_shape}_{columns}_paw_model_design.png')\n    return model\n\ntuner = kt.Hyperband(model_builder,\n                     objective='mean_absolute_percentage_error',\n                     max_epochs=10,\n                     factor=3,\n                     directory='.\/',\n                     project_name='Paw_model_final')\nprint(\"Model Established\")","e469c8ec":"# Adds Early Stopping\ncallback = tf.keras.callbacks.EarlyStopping(monitor=\"mean_absolute_percentage_error\", patience=5, restore_best_weights=True, verbose=1)\n\ntuner.search(\n    [X_train, X_meta_train], y_train,\n    validation_data=([X_test, X_meta_test], y_test),\n    batch_size = 1,\n    shuffle=True,\n    callbacks=[callback],\n    verbose=1\n    )","2da4385f":"model = tuner.get_best_models(num_models=1)[0]\nprint(\"Model Loaded\")","fb5c6027":"y_pred = model.predict([X_test, X_meta_test])\nsns.displot(y_pred)\nsns.displot(y_test)\n\nprint(y_pred, y_test)\nprint(y_pred.max(),y_pred.min())\n\n#with open('.\/y_pred', 'wb') as pickle_file:\n#    pickle.dump(y_pred, pickle_file)","eeaffc0b":"def model_adjusted(prediction, y_pred):\n    min_val = y_pred.min()\n    max_val = (y_pred - y_pred.min()).max()\n    output = ((prediction - min_val)\/max_val)*100\n    return output","aa9ae044":"model.save(f'.\/{sq_shape}x{sq_shape}_{columns}_paw_model.h5')\nprint(f\"Model Saved as: {sq_shape}x{sq_shape}_{columns}_paw_model.h5\")","c4988d2b":"model_path = '.\/64x64_greyscaled_paw.h5'\npred_path = '.\/y_pred'\n\npre_trained_model = tf.keras.models.load_model(model_path)\n\nwith open(pred_path, 'rb') as pickle_file:\n    y_pred = pickle.load(pickle_file)\ny_pred\n\n# df_final = pd.DataFrame(zip(Name, outputs)).rename(columns={0:'Id',1:'Pawpularity'}).set_index('Id')\n# df_final.to_csv('submission.csv')\n# df_final","0fd668b7":"# Evaluate Model","2a469993":"# Data Exploration","766745da":"# Save Model","0b6f5070":"# Imports","6b9ea3ef":"# Pick Best Model","3858f3e2":"# Example Test [WIP Below]","66a14f73":"\n# Define Model","76d716f5":"# Train Test Split","922e947f":"# Test Training Data","0ea6ab11":"# Loading Images","9d4bf430":"# Fit Model"}}