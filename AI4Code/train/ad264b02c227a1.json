{"cell_type":{"9013e9f1":"code","89104dc9":"code","d28b1230":"code","ae55bbb6":"code","bd2d7179":"code","9ba99267":"code","d89f4429":"code","be96be44":"code","a98358e6":"code","cf7ccfc9":"code","780762d1":"code","a7a8ebd9":"code","959a1096":"code","34368d32":"code","18899099":"code","a8cebd08":"code","47804308":"code","8b40f6dd":"code","60861956":"code","8c8c7971":"code","6aeb6980":"code","926161cb":"code","0bfd6fc4":"code","03a5616f":"code","24f46ed7":"code","897cf010":"code","f113cc9b":"code","50fada78":"code","1e2db9c6":"code","865fe81b":"code","c82cbecf":"code","40404a55":"code","23727a76":"code","d6538de7":"markdown","0417204c":"markdown","d03fd3d9":"markdown","0133733e":"markdown","e994055a":"markdown","3121bb96":"markdown","80aed1e7":"markdown","a24105a7":"markdown","01afd131":"markdown","2ab66d67":"markdown","f52700ae":"markdown","babb0a34":"markdown","63681bc9":"markdown","7e2fad5f":"markdown","8a03f5d5":"markdown","daf7af91":"markdown","c2b2d4b7":"markdown","65ca1e4d":"markdown","11e30e79":"markdown","5d82a548":"markdown","7badd892":"markdown","ca33db05":"markdown","1493860d":"markdown","362cf261":"markdown","f09e9d82":"markdown"},"source":{"9013e9f1":"import pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nimport numpy as np","89104dc9":"#train data\ndf_train=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_label=df_train.target\n\n#submit data\ndf_test=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","d28b1230":"df_train.head(2)","ae55bbb6":"df_train=df_train.drop(['target'],axis=1)\n\nX=df_train.text.tolist()\nX_sub=df_test.text.tolist()\ny=np.array(df_label)","bd2d7179":"\nfig = plt.figure(figsize=(5, 5))\n\nlabels = 'Real news', 'Fake news'\n\nsizes = [sum(df_label==1), sum(df_label==0)] \nplt.pie(sizes, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\n\nplt.axis('equal')  \n\nplt.show()","9ba99267":"import nltk\nimport re\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer\n\n#stopwords\nnltk.download('stopwords')\nstopwords.words('english')[:4]","d89f4429":"class Preprocessing:\n  def __init__(self):\n    self.freq={}\n\n    #stopwords\n    self.stopwords=stopwords.words('english')\n    self.stopwords.append('us')\n\n  def build_word_list(self,tweet):\n    stemmer=PorterStemmer()\n    # stock moket symbol\n    tweet=re.sub(r'\\$\\w*', '', tweet)\n    #old tweet\n    tweet=re.sub(r'^RT[\\s]+', '', tweet)\n    #URL\n    tweet=re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)\n    # # \n    tweet=re.sub(r'#', '', tweet)\n    #tokenize\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)\n    token_tweet=tokenizer.tokenize(tweet)\n    \n    word_list=[]\n    for word in token_tweet:\n      if (word not in self.stopwords) and (word not in string.punctuation) and (word not in string.ascii_lowercase):\n        detect_list=[int(w in self.stopwords) for w in word.split(\"'\") ] #detect a word like \"he's\" --> ['he','s']\n\n        # word not including number and word not like \"he's\"\n        if (len(re.findall(r\"\\d+\\.?\\d*\",word))==0) and (sum(detect_list)<=1): \n          stem_word=stemmer.stem(word)\n          word_list.append(stem_word)\n    return word_list\n\n\n\n  def build_freq(self,tweets,y):\n    y_list =y.tolist()\n    for tweet,y_iter in zip(tweets,y_list): #(X,y)\n        for word in self.build_word_list(tweet):\n            pair = (word, y_iter)\n            if pair in self.freq:\n                self.freq[pair] += 1\n            else:\n                self.freq[pair] = 1","be96be44":"pres=Preprocessing()\npres.build_freq(X,y)\nword_freq=pres.freq","a98358e6":"# 'black' seems to have tendency to being the components of  fake news.\n\nword_freq[('black',1)],word_freq[('black',0)]","cf7ccfc9":"class Naive_Bayse(Preprocessing):\n  def __init__(self,alpha): \n    super(Naive_Bayse,self).__init__()\n    self.alpha=alpha\n\n    #main\n    self.loglikelihood={}\n    self.logprior=0\n\n    #this is used for visualization\n    self.pos_loglikelihood={}\n    self.neg_loglikelihood={}\n    self.pos_neg_prior={}\n\n\n  #training logprior and loglikelihood\n  def fit(self,X,y): \n    self.build_freq(X,y)\n    N_pos = N_neg  = 0\n\n    for pair in self.freq.keys():\n        if pair[1] > 0: \n            N_pos += self.freq[pair] #word positive number\n        else: \n            N_neg += self.freq[pair] # word negative number\n    \n    #logprior+sum(word_loglikelihood)\n    N_text_pos=(y==1).sum()\n    N_text_neg=(y==0).sum()\n    self.logprior=np.log(N_text_pos)-np.log(N_text_neg)  #log(p(pos)\/p(neg))\n    self.pos_neg_prior['p_pos']=N_text_pos\/(N_text_pos+N_text_neg)\n    self.pos_neg_prior['p_neg']=N_text_neg\/(N_text_pos+N_text_neg)\n\n    #loglikelihood\n    vocab = set([pair[0] for pair in self.freq.keys()])\n    N_unique_words=len(vocab)\n\n    for word in vocab:\n      N_word_pos=self.freq.get((word,1),0)\n      N_word_neg=self.freq.get((word,0),0)\n\n      #laplace smoothing\n      p_word_pos=(N_word_pos+self.alpha)\/(N_pos+N_unique_words*self.alpha)\n      p_word_neg=(N_word_neg+self.alpha)\/(N_neg+N_unique_words*self.alpha)\n\n      self.loglikelihood[word]=np.log(p_word_pos)-np.log(p_word_neg)  # sum(log(wp)-log(wn))\n      self.pos_loglikelihood[word]=np.log(p_word_pos)\n      self.neg_loglikelihood[word]=np.log(p_word_neg)\n\n\n\n  def predict(self,X):\n    pred=[]\n    for text in X:\n      p=0\n      for word in self.build_word_list(text):\n        p+=self.loglikelihood.get(word,0)\n      decision=self.logprior+p\n      if decision>0:\n        pred.append(1)\n      else:\n        pred.append(0)\n    return np.array(pred)\n\n  def score(self,X,y):\n    return (self.predict(X)==y).sum()\/y.shape[0]","780762d1":"from sklearn.model_selection import train_test_split as ttp\n\nX_train,X_test,y_train,y_test=ttp(X,y,test_size=0.1,random_state=0)\nlen(X_train),len(X_test)","a7a8ebd9":"model=Naive_Bayse(alpha=1)\nmodel.fit(X_train,y_train)","959a1096":"model.score(X_test,y_test)","34368d32":"#[pos_loglikelihood,neg_loglikelihood]\n\ndef count_text_log_likelihood(text,model,type='pos'):\n  if type=='pos':\n    p=model.pos_neg_prior['p_pos']\n  else:\n    p=model.pos_neg_prior['p_neg']\n\n  word_list=model.build_word_list(text)\n  for word in word_list:\n    if type=='pos':\n      p+=model.pos_loglikelihood.get(word,0)\n    else:\n      p+=model.neg_loglikelihood.get(word,0)\n  return p\n\ndef text_list_process(text_list,model):\n  M=np.empty(shape=(len(text_list),2))\n  for idx,text in enumerate(text_list):\n    text_pos_l=count_text_log_likelihood(text,model)\n    text_neg_l=count_text_log_likelihood(text,model,type='neg')\n    M[idx]=[text_pos_l,text_neg_l]\n  return M","18899099":"word_l_vector=text_list_process(X,model)\n\ndf_pn=pd.DataFrame({'pos_loglikelihood':word_l_vector[:,0],'neg_loglikelihood':word_l_vector[:,1],'target':y})\ndf_pn","a8cebd08":"X_train_p=text_list_process(X_train,model).astype('float32')\nX_test_p=text_list_process(X_test,model).astype('float32')","47804308":"from sklearn.model_selection import learning_curve\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nimport warnings\nwarnings.filterwarnings(\"ignore\")","8b40f6dd":"from sklearn.linear_model import LogisticRegression\n\n\npipe_lr = make_pipeline(StandardScaler(),LogisticRegression(penalty='l2',random_state=1))\ntrain_sizes_l, train_scores_l, test_scores_l =learning_curve(estimator=pipe_lr,X=X_train_p,y=y_train,train_sizes=np.linspace(0.1, 1.0, 10),cv=10,n_jobs=1)\ntrain_mean_l = np.mean(train_scores_l, axis=1)\ntrain_std_l = np.std(train_scores_l, axis=1)\ntest_mean_l = np.mean(test_scores_l, axis=1)\ntest_std_l = np.std(test_scores_l, axis=1)","60861956":"plt.plot(train_sizes_l, train_mean_l,color='blue', marker='o',markersize=5, label='training accuracy')\nplt.fill_between(train_sizes_l,train_mean_l + train_std_l,train_mean_l - train_std_l,alpha=0.15, color='blue')\nplt.plot(train_sizes_l, test_mean_l,color='green', linestyle='--',marker='s', markersize=5,label='validation accuracy')\nplt.fill_between(train_sizes_l,test_mean_l + test_std_l,test_mean_l - test_std_l,alpha=0.15, color='green')\nplt.grid()\nplt.xlabel('Number of training samples')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.8, 1.0])\nplt.show()","8c8c7971":"#available params\npipe_lr.get_params().keys()","6aeb6980":"lr_C = [10.0, 100.0, 1000.0]\n\nlr_grid=[{'logisticregression__penalty':['l1'],'logisticregression__C':lr_C},{'logisticregression__penalty':['l2'],'logisticregression__C':lr_C}]\n\ngs_logic=GridSearchCV(estimator=pipe_lr,param_grid=lr_grid,scoring='accuracy',cv=10,verbose=0)\ngs_logic=gs_logic.fit(X_train_p,y_train)\nprint(gs_logic.best_score_)\nprint(gs_logic.best_params_)","926161cb":"gs_logic.score(X_test_p,y_test)","0bfd6fc4":"from sklearn.svm import SVC\n\npipe_svm = make_pipeline(StandardScaler(),SVC(100,random_state=1))\ntrain_sizes_s, train_scores_s, test_scores_s =learning_curve(estimator=pipe_svm,X=X_train_p,y=y_train,train_sizes=np.linspace(0.1, 1.0, 10),cv=10,n_jobs=1)\ntrain_mean_s = np.mean(train_scores_s, axis=1)\ntrain_std_s = np.std(train_scores_s, axis=1)\ntest_mean_s = np.mean(test_scores_s, axis=1)\ntest_std_s = np.std(test_scores_s, axis=1)","03a5616f":"plt.plot(train_sizes_s, train_mean_s,color='blue', marker='o',markersize=5, label='training accuracy')\nplt.fill_between(train_sizes_s,train_mean_s + train_std_s,train_mean_s - train_std_s,alpha=0.15, color='blue')\nplt.plot(train_sizes_s, test_mean_s,color='green', linestyle='--',marker='s', markersize=5,label='validation accuracy')\nplt.fill_between(train_sizes_s,test_mean_s + test_std_s,test_mean_s - test_std_s,alpha=0.15, color='green')\nplt.grid()\nplt.xlabel('Number of training samples')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.8, 1.0])\nplt.show()","24f46ed7":"#available params\n\npipe_svm.get_params().keys()","897cf010":"\nsvm_C = [50,100]\n\nsvm_grid=[{'svc__C':svm_C,'svc__kernel': ['linear']},{'svc__C':svm_C,'svc__kernel': ['rbf']}]\n\ngs_svm=GridSearchCV(estimator=pipe_svm,param_grid=svm_grid,scoring='accuracy',cv=10)\ngs_svm=gs_svm.fit(X_train_p,y_train)\nprint(gs_svm.best_score_)\nprint(gs_svm.best_params_)","f113cc9b":"gs_svm.best_estimator_.score(X_test_p,y_test)","50fada78":"from mlxtend.plotting import plot_decision_regions","1e2db9c6":"fig,ax=plt.subplots(1,2,figsize=(18,5))\nplot_decision_regions(X_test_p,y_test.astype('int32'), clf=gs_logic.best_estimator_,legend=2,ax=ax[0])\nplot_decision_regions(X_test_p,y_test.astype('int32'), clf=gs_svm.best_estimator_,legend=2,ax=ax[1])\n\n\n\nfor idx,val in enumerate(['logistic','svm']):\n  ax[idx].set_xlabel('positive counts',color='white')\n  ax[idx].set_ylabel('negative counts',color='white')\n\n  ax[idx].set_title(val,color='white')\n\nplt.show()","865fe81b":"model=Naive_Bayse(alpha=1)\nmodel.fit(X,y)","c82cbecf":"sub_model=SVC(C=50,kernel='linear')\nsub_model.fit(text_list_process(X,model),y)","40404a55":"df_submit=pd.DataFrame({'target':sub_model.predict(text_list_process(X_sub,model))},index=df_test.id)\ndf_submit","23727a76":"df_submit.to_csv('.\/submit.csv')","d6538de7":"Decision Region ","0417204c":"# Submit","d03fd3d9":"Build Freq","0133733e":"Train Test","e994055a":"Learning Curve","3121bb96":"# Modling ","80aed1e7":"***SVM***","a24105a7":"***Goal*** : \n\n$y_{pred} = argmax_{class}\\ P(class|{text})=argmax_{class}\\ P(class)\\Pi_{i}P(word_i|class)$ , this is based on iid assumption.","01afd131":"Fit in whole train.csv data","2ab66d67":"# Global package","f52700ae":"Build Model in sklearn form","babb0a34":"learning Curve","63681bc9":"regularzation : l1 , l2\n\nregularzation param : C=$1\/\\lambda$","7e2fad5f":"***SVM***","8a03f5d5":"# Naive Bayse(Baseline Model)","daf7af91":"***preprocessing to loglikelihood Matrix***","c2b2d4b7":"Tunning Hyperparameters","65ca1e4d":"Loglikelihood Matrix","11e30e79":"Tunning Hyperparameters","5d82a548":"# Data","7badd892":"See balance of data","ca33db05":"kernel : Guassian , Linear\n\nregularzation params : C\n\ngamma: 'auto' , 'scale'","1493860d":"* Logistic Regression\n\n\n* SVM\n","362cf261":"***Logistic Regression***","f09e9d82":"# Preprocessing"}}