{"cell_type":{"b100cc15":"code","1de695d6":"code","b90406f2":"code","5618be80":"code","cd6303ca":"code","88549158":"code","8838a8b5":"code","8beb3d78":"code","3f9826b0":"code","6fea46bd":"code","6c3d92ce":"code","10fe21eb":"code","c7dc3ff3":"code","4050e00c":"code","b7717173":"code","4d97fa45":"code","0599e111":"code","c6b855d0":"code","438abb1d":"code","8720a363":"code","29505c47":"code","7f118a95":"code","38a17373":"code","26328381":"code","035b1806":"code","ebfe3d3b":"code","52a4c2da":"code","c14c5f13":"code","51f05da7":"code","61ba69d6":"code","2b4b30cb":"code","a2035bff":"code","ec5749a6":"code","afa91f1d":"code","3aa3a1a9":"code","2f476d37":"code","9cd6dd7d":"code","a7e3dfbb":"code","f5c4b8aa":"code","299e9208":"code","1c0d5cab":"code","4568a152":"code","3148d660":"code","ad7971f2":"code","2b70fa6c":"code","9aba2ddf":"code","58329a46":"code","bddf0a20":"code","8076929d":"code","76c1d6a2":"code","37e38748":"code","6bc02abf":"code","ba7c7f33":"code","37fabe16":"code","5c52b720":"code","bb1103ed":"code","0598b435":"code","f7b09bf2":"code","d2a999af":"code","6d982f66":"code","e7755796":"code","2f3ea5a2":"code","b14a79e1":"code","87aa20a5":"code","166d5d8f":"code","ef617db9":"code","e11db049":"code","72d713b5":"markdown","fcce9b0d":"markdown","d9a679b7":"markdown","14372a95":"markdown","4064dd18":"markdown","76c2928d":"markdown","32576525":"markdown","b60ef858":"markdown","1aba1ee8":"markdown","4127b785":"markdown","6ac36459":"markdown","73b5f4dd":"markdown","361851f0":"markdown","14560a7c":"markdown","c90450cb":"markdown","10b8504c":"markdown","2c3e4c74":"markdown","603cd2f3":"markdown","3f468837":"markdown","1bbee1d4":"markdown","fed84e0a":"markdown","14ad7fe3":"markdown","d137b9c4":"markdown","4e8c0f70":"markdown","79cca52b":"markdown","69e620b1":"markdown","af2f1e0d":"markdown","f2523c8d":"markdown","c288dc6e":"markdown","48558158":"markdown","2d5912e5":"markdown"},"source":{"b100cc15":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1de695d6":"##Importing first-level necessary libraries and we shall import others as we proceed along\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","b90406f2":"train=pd.read_csv('..\/input\/song-popularity-prediction\/train.csv')\ntest=pd.read_csv('..\/input\/song-popularity-prediction\/test.csv')\n","5618be80":"##Printing the row and column of the data\nprint('The row count and column count are....',train.shape[0],'and',train.shape[1],'respectively')","cd6303ca":"##Checking for duplicated data an dno duplicates\ntrain.duplicated().sum()","88549158":"#### We have quite many columns and now let us find the null values and percentage of null values\nnull_values=train.isnull().sum()\/len(train)\nnull_values_perc=round(null_values*100,2)\nnull_values_perc=null_values_perc[null_values.sort_values(ascending=False)>0]\nnull_values_perc","8838a8b5":"##We shall split into numerical and object columns\nnumerical_int_cols=train.select_dtypes(include='int64')\nnumerical_int_cols.columns","8beb3d78":"numerical_float_cols=train.select_dtypes(include='float64')\nnumerical_float_cols.columns","3f9826b0":"object_cols=train.select_dtypes(include='object')\nobject_cols.columns","6fea46bd":"numerical_columns=numerical_int_cols.columns.tolist()+numerical_float_cols.columns.tolist()","6c3d92ce":"##Exploring statistical measures of the data\n\ntrain[numerical_columns].describe()","10fe21eb":"plt.figure(figsize=[10,4])\nfig, ax = plt.subplots()\np1=round(train.song_popularity.value_counts(normalize=True)[0],2)\np2=round(train.song_popularity.value_counts(normalize=True)[1],2)\ny=[p1,p2]\nx=['%Unpopular Song','%Popular Song']\nplt.barh(x,y,color={'red','blue'})\nplt.title('% Song Popularity Distribution')\nfor i, v in enumerate(y):\n    ax.text(v-0.0001, i, str(v), color='blue', fontweight='bold')\nplt.show()","c7dc3ff3":"mean_imputation=[]\nmedian_imputation=[]\nfor j in null_values_perc.index.tolist():\n    highest=train[j].mean() + 3*train[j].std()\n    lowest=train[j].mean() - 3*train[j].std()\n    if len(train[(train[j]>highest)| (train[j]<lowest)])!=0:\n        median_imputation.append(j)\n    else:\n        mean_imputation.append(j)","4050e00c":"print('To impute with median....',median_imputation )\nprint('To impute with mean....',mean_imputation )","b7717173":"!pip install fancyimpute","4d97fa45":"## Use the knn imputer instead of mean as it does not contain outlier\n\nfrom fancyimpute import IterativeImputer\nmice_impute = IterativeImputer()\ntrain[mean_imputation]= mice_impute.fit_transform(train[mean_imputation])\n#train[median_imputation]= mice_impute.fit_transform(train[median_imputation])\n","0599e111":"##Rest other columns contain outliers\n\ntrain[median_imputation]=train[median_imputation].fillna(train[median_imputation].median())","c6b855d0":"##Checking the null values again\n\n#### We have quite many columns and now let us find the null values and percentage of null values\nnull_values=train.isnull().sum()\/len(train)\nnull_values_perc=round(null_values*100,2)\nnull_values_perc=null_values_perc[null_values.sort_values(ascending=False)>0]\nnull_values_perc","438abb1d":"from scipy.stats import skew","8720a363":"skewed_dist=[]\nnormal_dist=[]\nfor i in train.columns.tolist():\n    if (i not in ['song_popularity','id']):\n        if np.abs(skew(train[i]))>1:\n            skewed_dist.append(i)\n        else:\n            normal_dist.append(i)\nprint('The number of highly skewed columns___',len(skewed_dist), skewed_dist)\nprint('The number of non(less-skewed)skewed distributed columns___',len(normal_dist) , normal_dist)","29505c47":"##Let us verify whether the above claim is correct\ndef plot(num_cols,color):\n     fig, ax = plt.subplots(1,2,figsize=(10,4))\n     sns.distplot(train[num_cols], ax=ax[0],kde=True,color=color)\n     sns.boxplot(train[num_cols], ax=ax[1],color=color)\n     plt.title(f\"Before Oultier capping dist {num_cols}\")\n     fig.show()\n     return None\n\n","7f118a95":"color=['red','blue','green','orange','cyan','magenta','black','yellow','blue','green','indigo','purple']\nfor i , j in zip(train.columns.tolist(),color):\n        if (i not in ['song_popularity','id']):\n            plot(i,j)\n        ","38a17373":"##Removing the Id column from train\ntrain.drop('id',inplace=True,axis=1)\n","26328381":"##Let us analyze column by column\ntrain.columns","035b1806":"##Let us convert the song_duration_ms to minute duration\n\ntrain['song_duration_m']=(train['song_duration_ms']\/60000).astype('int32')\ntrain['song_duration_m'].value_counts(normalize=True).plot.bar(color={'red','green','blue','yellow','orange'})\nplt.title('Song duration distribution in minutes')\nplt.show()","ebfe3d3b":"names = list(train.columns)\nfig, axes = plt.subplots(round(len(names)\/3), 3,figsize=[40,45]) \nsns.set(font_scale=4)\ny = 0;\nfor name in names:\n    if name not in 'song_popularity':\n        i, j = divmod(y,3)\n        sns.boxplot(x=train['song_popularity'],y=train[name], ax=axes[i, j])\n    y = y + 1\nfig.tight_layout()\nplt.show()\n","52a4c2da":"##Let us scale the feature\n\nX = train.drop(['song_popularity'], axis=1)\ny = train['song_popularity']\n","c14c5f13":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split","51f05da7":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 42, stratify = y)","61ba69d6":"# apply pca to train data\n#pca = Pipeline([('scaler', StandardScaler()), ('pca', PCA())])","2b4b30cb":"#pca.fit(X_train)\n#X_train = pca.fit_transform(X_train)","a2035bff":"#Extacting the PCA series information from the pipleline\n#pca = pca.named_steps['pca']","ec5749a6":"##plotting the variable significance graph\n#cumulative_variance = np.round(np.cumsum(pca.explained_variance_ratio_)*100, decimals=4)\n#sns.set(font_scale=1)\n#plt.plot(cumulative_variance)","afa91f1d":"##We define the pipeline for PCA on Training data and without class imbalance\n\nsteps = [('scaler', StandardScaler()),\n         (\"logistic\", LogisticRegression())\n        ]\npipeline = Pipeline(steps)","3aa3a1a9":"# fit model\npipeline.fit(X_train, y_train)\n\n# check score on train data\npipeline.score(X_train, y_train)","2f476d37":"##Evaluating on the test data\n\n# predict churn on test data\ny_pred = pipeline.predict(X_test)\n\n# create onfusion matrix\nconfusion = confusion_matrix(y_test, y_pred)\nprint(confusion)\n\n\n# check area under curve\ny_pred_prob = pipeline.predict_proba(X_test)[:, 1]\nprint(\"AUC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))","9cd6dd7d":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV","a7e3dfbb":"##Let us define the class weights\n\ny_train.value_counts(normalize=True)","f5c4b8aa":"##Defining pipeline and including hyper paramter\n#pca=PCA()\n# logistic regression - the class weight is used to handle class imbalance - it adjusts the cost function\nlogistic = LogisticRegression(class_weight={0:0.35, 1: 0.65})\nC = [0.001,0.01,0.1,1,10,100]\npenalty = ['l1', 'l2','Elastic-Net']\n# create pipeline\nsteps = [(\"scaler\", StandardScaler()), \n         (\"logistic\", logistic)\n        ]\n\n# compile pipeline\ncombined_pipeline = Pipeline(steps)\n\n# hyperparameter space\nparams = {'logistic__penalty': penalty,'logistic__C':C}","299e9208":"# create 5 folds\nfolds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)","1c0d5cab":"# create gridsearch object\nmodel = GridSearchCV(estimator=combined_pipeline,\n                     cv=folds,\n                     param_grid=params,\n                     scoring='roc_auc', \n                     n_jobs=-1, \n                     verbose=1)","4568a152":"# fit model\nmodel.fit(X_train, y_train)","3148d660":"# print best hyperparameters\nprint(\"Best AUC: \", model.best_score_)\nprint(\"Best hyperparameters: \", model.best_params_)","ad7971f2":"##Evaluating on the test data\n\n# predict churn on test data\ny_pred = model.best_estimator_.predict(X_test)\n\n# create onfusion matrix\nconfusion = confusion_matrix(y_test, y_pred)\nprint(confusion)\n\n\n# check area under curve\ny_pred_prob = model.predict_proba(X_test)[:, 1]\nprint(\"AUC:    \\t\", round(roc_auc_score(y_test, y_pred_prob),2))","2b70fa6c":"import xgboost as xgb\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.utils.class_weight import compute_sample_weight","9aba2ddf":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train[X_train.columns]=scaler.fit_transform(X_train[X_train.columns])\nX_test[X_train.columns]=scaler.transform(X_test[X_train.columns])","58329a46":"final_model = xgb.XGBClassifier(max_depth= 12,\n n_estimators= 5600,\n learning_rate= 0.07074059946646541,\n subsample= 0.8,\n colsample_bytree=0.6000000000000001,\n colsample_bylevel=0.5,\n min_child_weight=0.00390933891195369,\n reg_lambda=2176.9882633091584,\n reg_alpha=0.00027465681042320085,\n gamma=0.45590270702576924)","bddf0a20":"final_model.fit(X_train, y_train)","8076929d":"y_pred = final_model.predict(X_test)","76c1d6a2":"y_pred_prob = final_model.predict_proba(X_test)[:, 1]\nroc_score=round(roc_auc_score(y_test, y_pred_prob),2)\nroc_score","37e38748":"from sklearn.ensemble import RandomForestClassifier","6bc02abf":"rf_model=RandomForestClassifier(random_state=42, n_jobs=-1,class_weight={0:0.08, 1: 0.92},oob_score=True,\n                          n_estimators=60, min_samples_leaf= 320,max_features=3,max_depth=14)","ba7c7f33":"rf_model.fit(X_train, y_train)","37fabe16":"y_pred = rf_model.predict(X_test)\ny_pred_prob = rf_model.predict_proba(X_test)[:, 1]\nroc_score=round(roc_auc_score(y_test, y_pred_prob),2)\nroc_score","5c52b720":"test=pd.read_csv('..\/input\/song-popularity-prediction\/test.csv')\ntest.drop('id',axis=1,inplace=True)","bb1103ed":"##Imputing missing values\n\n#### We have quite many columns and now let us find the null values and percentage of null values\nnull_values=test.isnull().sum()\/len(test)\nnull_values_perc=round(null_values*100,2)\nnull_values_perc=null_values_perc[null_values.sort_values(ascending=False)>0]\nnull_values_perc\n","0598b435":"mean_imputation=[]\nmedian_imputation=[]\nfor j in null_values_perc.index.tolist():\n    highest=test[j].mean() + 3*train[j].std()\n    lowest=test[j].mean() - 3*train[j].std()\n    if len(test[(test[j]>highest)| (test[j]<lowest)])!=0:\n        median_imputation.append(j)\n    else:\n        mean_imputation.append(j)","f7b09bf2":"print('To impute with median in test....',median_imputation )\nprint('To impute with mean in test....',mean_imputation )","d2a999af":"mice_impute = IterativeImputer()\ntest[mean_imputation]= mice_impute.fit_transform(test[mean_imputation])\n#test[median_imputation]= mice_impute.fit_transform(test[median_imputation])","6d982f66":"test[median_imputation]=test[median_imputation].fillna(test[median_imputation].median())","e7755796":"##Again checking the null values\n\n#### We have quite many columns and now let us find the null values and percentage of null values\nnull_values=test.isnull().sum()\/len(test)\nnull_values_perc=round(null_values*100,2)\nnull_values_perc=null_values_perc[null_values.sort_values(ascending=False)>0]\nnull_values_perc","2f3ea5a2":"##Let us convert the song_duration_ms to minute duration\n\ntest['song_duration_m']=(test['song_duration_ms']\/60000).astype('int32')","b14a79e1":"#Standardizing the tets data\ntest[test.columns]=scaler.transform(test[test.columns])","87aa20a5":"##Predicting on test dataset\nfinal_preds=final_model.predict(test)","166d5d8f":"submission = pd.read_csv('\/kaggle\/input\/song-popularity-prediction\/sample_submission.csv')","ef617db9":"submission['song_popularity'] = final_preds","e11db049":"submission.to_csv('submission.csv', index=False)","72d713b5":"We inferred from the median impuatation that the presence of outliers in the dataset is more, so proceed with capping them.\n\nFor outlier capping we can follow two step process\n\nIf the data is less skewed, we shall cap the data based on (Mean +\/- 3*Std)\nIf the data is skewed ( meaning skewnes measure >|1|), we shall use the IQR treatment (since the aforementioned mean and Std would have serious influence on the data)","fcce9b0d":"Note that we inherently split the training and testing data within the given training data.\nWe shall find the best performing model and apply to the original tes data","d9a679b7":"**Reading the train and test data set**","14372a95":"Building the pipeline which includes both logistic and hyper parameter","4064dd18":"**Extracting numerical and object columns**","76c2928d":"**Analysing the distribution of target variable---->Song popularity**","32576525":"parameters = {'learning_rate': [0.1, 0.2, 0.3, 0.4, 0.5],\n              'max_depth': [2, 4, 6, 8, 10],\n              'min_child_weight': [3, 7, 11, 19, 25],\n              'n_estimators': [50, 100, 150, 200, 300, 500]}\n\nscorer = metrics.make_scorer(metrics.roc_auc_score,\n                             greater_is_better=True,\n                            needs_proba=True,\n                            needs_threshold=False)","b60ef858":"Doing more EDA with respect to independant and target variables(Bi-Variate)","1aba1ee8":"The data is slightly imbalanced","4127b785":"The independant variables are evenly spread out with respect to the target variable","6ac36459":"color=['red','blue','green','orange','cyan','magenta','black','yellow','blue','green','indigo','purple']\nfor i , j in zip(train.columns.tolist(),color):\n        if (i not in ['song_popularity','id']):\n            plot(i,j)\n        ","73b5f4dd":"clf_xgb = model_selection.GridSearchCV(estimator=xgb_model,\n                                      param_grid=parameters,\n                                      n_jobs=-1,\n                                      cv=4,\n                                      scoring=scorer,\n                                      refit=True)\n\nclf_xgb.fit(X_train, y_train)","361851f0":"1. First we try to impute missing values in the test data set\n2. We cap the outlier and use standardisation of the test data set ","14560a7c":"**Importing important libraries**","c90450cb":"**Let us extract the test data set and apply similar pre-processing technique as done on train**","10b8504c":"##Capping the outliers\nfor cols in train.columns.tolist():\n    if cols!='Id' and cols!='song_popularity':\n        upper_limit = train[cols].quantile(0.99)\n        lower_limit = train[cols].quantile(0.01)\n        train[cols] = np.where(train[cols] > upper_limit,\n        upper_limit,\n        np.where(train[cols] < lower_limit,\n        lower_limit,\n        train[cols]))","2c3e4c74":"**Outlier Treatment**","603cd2f3":"Random Forest Model","3f468837":"**Missing Value treatment**","1bbee1d4":"This shows that 9 variables are enough which represent over 90% variance in the data","fed84e0a":"##Capping the outliers:\n##Capping the outliers\nfor cols in test.columns.tolist():\n        upper_limit = test[cols].quantile(0.99)\n        lower_limit = test[cols].quantile(0.01)\n        test[cols] = np.where(test[cols] > upper_limit,\n        upper_limit,\n        np.where(test[cols] < lower_limit,\n        lower_limit,\n        test[cols]))\n","14ad7fe3":"Interestingly son_duration does not have any effect with respect to the song popularity as \nboth the distributions are same","d137b9c4":"There seems to be a minimal outlier in the data which we can inspect later","4e8c0f70":"We shall try using the fancy imputer here since all the columns are numerical\nand we shall analyse how the independant variables are related to each other as fundamentally the MICE uses multiple regression for filling the missing values. Also, we could use the knn method but it is noticeble that both these methods might be affected by the outlier i.,e the missing value obatained may result even more skewed distribution. So we apply knn on the non-outlier missing data\n\nUse ! pip install fancyimpute to load this package","79cca52b":"Let us create a sample code which identifies outliers in the data and impute them with mean else we shall impute them with median","69e620b1":"Part-1\n\n1. Data exploration and missing value identification\n2. Missing value treatment and outlier analysis\n3. Exploratory Data Analysis to understand trend between the target Song Popularity and various independant features.","af2f1e0d":"Null values are removed completely","f2523c8d":"**Model building using PCA**","c288dc6e":"**Data exploration and missing value identification on train data**","48558158":"The song with duration ateast 3 minutes show majority in the data distribution","2d5912e5":"##After outlier capping\ndef plot(num_cols,color):\n     fig, ax = plt.subplots(1,2,figsize=(10,4))\n     sns.distplot(train[num_cols], ax=ax[0],kde=True,color=color)\n     sns.boxplot(train[num_cols], ax=ax[1],color=color)\n     plt.title(f\"After Oultier capping dist {num_cols}\")\n     fig.show()\n     return None\n\n"}}