{"cell_type":{"5d21d51f":"code","dcc2611e":"code","1b919da9":"code","5ec14b43":"code","ceafa403":"code","cff7552e":"code","c51ed1a8":"code","e608290a":"code","e8bfdae9":"code","82da8087":"code","f59f55c3":"code","6b93bcae":"code","e118bfa1":"code","c85e91c4":"code","3304417f":"code","6cf64e9d":"markdown","0bab1850":"markdown","e13abe4b":"markdown","4400a142":"markdown","a9585cc2":"markdown","17d8d162":"markdown","f16faf9d":"markdown","ff3ea388":"markdown","bf7e5ded":"markdown","9044a999":"markdown","c3273cd8":"markdown","1f3b2e5b":"markdown","d9065fa8":"markdown","f045ef64":"markdown","5a6f2625":"markdown","d46b5e6c":"markdown","3ea122d3":"markdown","d42cfa9a":"markdown","ce620599":"markdown","c23a99b4":"markdown","c6071e70":"markdown","835f7c2c":"markdown","a8b1e499":"markdown","9bfc7de7":"markdown","d00b699a":"markdown","b4dab97c":"markdown","4bb92f24":"markdown","43b90939":"markdown","efc524f9":"markdown","87273fda":"markdown"},"source":{"5d21d51f":"import pandas as pd\nimport numpy as np\n\nfrom scipy.stats import ks_2samp\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    confusion_matrix,\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    roc_auc_score\n)","dcc2611e":"df = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\n\ndf.drop(columns=[\"Time\"], inplace=True)","1b919da9":"(df.Class.value_counts(normalize=True)*100).rename(\"Classes' Proportions\").to_frame()","5ec14b43":"X_train, X_test, Y_train, Y_test = train_test_split(df.drop(columns=[\"Class\"]), df.Class)","ceafa403":"print(\n    f\"The positive class proportion in train set is {(Y_train == 1).sum() \/ Y_train.shape[0]:0.3f} \\\nwhereas in test set is {(Y_test == 1).sum() \/ Y_test.shape[0]:.3f}\"\n)","cff7552e":"%%capture\n\nxgb = XGBClassifier()\nxgb.fit(X_train, Y_train)\n\npreds = xgb.predict(X_test)\npred_probas = xgb.predict_proba(X_test)","c51ed1a8":"c = confusion_matrix(Y_test, preds) \/ Y_test.shape[0] * 100\nc = np.round(c, 2)\n\npd.DataFrame(\n    c,\n    columns=[\"Predicted Negative\", \"Predicted Positive\"],\n    index=[\"Negative Label\", \"Positive Label\"]\n)","e608290a":"print(f\"The accuracy score was {accuracy_score(Y_test, preds):.2f}\")","e8bfdae9":"mocking_preds = np.zeros(Y_test.shape[0])\nprint(f\"The accuracy score was {accuracy_score(Y_test, mocking_preds):.2f}\")","82da8087":"print(f\"The Precision score was {precision_score(Y_test, preds):.2f}\")","f59f55c3":"print(f\"The Recall score was {recall_score(Y_test, preds):.2f}\")","6b93bcae":"specificity = c[0][0] \/ (c[0][0] + c[0][1])\nprint(f\"The Specificity score was {specificity*100:.2f}\")","e118bfa1":"print(f\"The F1 score was {f1_score(Y_test, preds):.2f}\")","c85e91c4":"print(f\"The AUC score was {roc_auc_score(Y_test.values, pred_probas[:, 1]):.2f}\")","3304417f":"# Get only the probability of being the positive class\npred_probas = pred_probas[:, 1]\n\nprint(f\"The KS score was {ks_2samp(pred_probas[Y_test == 0], pred_probas[Y_test == 1])[0]:.2f}\")","6cf64e9d":"Here we'll apply our knowledge about classification metrics to a [fraud detection dataset](https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud). In order to present some of the aspects described here, I'll develop a very simple solution based on a XGBoost classifier with no previous feature engineering, or parameter tunning.","0bab1850":"## **F1 Score**\n\nOur next metric is known as F1-score, and is defined simply as the harmonic mean of Precision and Recall. It gives the same importance to both metrics used in its composition, however, this isn\u2019t always seen as an advantageous assumption since the problem may require different importance levels.\n\n- **F1-score:** Harmonic mean of recall and precision.\n\n\\begin{equation}\n2\\frac{Precision * Recall}{Precision + Recall}\n\\end{equation}\n","e13abe4b":"The \"model\" also reached a perfect performance according to Accuracy, even though it hasn't an intelligent decision process.","4400a142":"---","a9585cc2":"---","17d8d162":"---\n## **Thanks for the reading. Feedbacks are very welcome!**","f16faf9d":"---","ff3ea388":"As we saw above, the differentiation between the classes is quite significant. In other words, the function that generates the positive class items, is substantially different than the one that generates the negative class.","bf7e5ded":"## **Specificity**\n\nDiametrically opposite, we have Specificity. This metric has the same intent as Recall, however, instead of considering positive records, we only care about the negative ones.\n\n- **Specificity:** From all negative records, what percentage was actually predicted as negative?\n\n\\begin{equation}\n\\frac{tn}{tn+fp}\n\\end{equation}","9044a999":"Since the sets' proportions are roughly the same, let's train our model and perform the predictions","c3273cd8":"As we can see below, we are dealing with a very skewed positive class. This makes sense because credit card frauds are the exception to the rule. The choice of an anomaly detection problem was intentional since we can highlight some flaws and strengths from each metric.","1f3b2e5b":"Now let's check whether the positive class proportions are similar in both sets, that is, the train\/test split was indeed stratified","d9065fa8":"As well as Confusion Matrix, Recall seems to have been able to identify the main flaw in our classification model, i.e. its False Negative rate. Hence, in this regard, the model doesn't seem to be so powerful as pictured by the previous alternatives.","f045ef64":"## **Kolmogorov-Smirnov (KS) Statistic**\n\nAlmost every interesting dataset we find in real-world scenarios is composed of n-dimensional random vectors, where each of their dimensions is a random variable that can (or cannot) be related to the other dimensions. G(X) is an oracle function capable of predicting an interest variable (our target) given the observation of each random vector. Essentially, when developing machine learning models, what we\u2019re looking for is a function F capable of imitating G\u2019s predicting behavior. Since G and F are functions over random variables, they can also be seen as random variables which follow an unknown distribution and present Cumulative Distribution Functions (CDF). Intuitively, we can think that if both CDFs are similar, the approximation of G through F is reliable. This measuring can be done with the aid of Two-Sample Kolmogorov-Smirnov\u2019s test.\nAdd an image\n\n- **Kolmogorov-Smirnov Test:** Measure the quality of the prediction as the distance between the CDFs from the oracle function and the approximation function.\n\n![ks.png](attachment:a075dc96-4567-4cd0-b225-931f87867b90.png)","5a6f2625":"---","d46b5e6c":"---","3ea122d3":"---","d42cfa9a":"## **Introduction**\n\nWhen developing machine learning models, one of the most crucial things to think about is which metric to use. There\u2019s no silver bullet. For the same solution, there might be metrics indicating that you\u2019ve reached an extremely performative solution, whereas some might point to a hideous model. Consequently, the time you spend finding the proper metrics to track through the development and implementation almost always pays back the effort.\n\nNonetheless, before bending over your metrics toolbox, you need to understand the nature of the problem you\u2019re dealing with. For example, when building a Reinforcement Learning Agent whose main intent is to surpass human level on a chess play, we probably won\u2019t give much importance if it loses a bishop (considered a wrong move\/prediction) as long as the final output happens to be a win. On the other hand, when classifying a breast tumor as benign or malignant we would do our best to identify every malignant occurrence, and regret every wrong prediction along the way. There are at least three major machine learning model families, namely, Supervised, Unsupervised, and Reinforcement Learning. To each one of these families there will be specific metrics to measure the quality of a solution with regard to some specific aspect. However, **the scope of this notebook is restricted to the Binary Supervised Learning Metrics**, which may posteriorly be directly mapped to multiclass problems.","ce620599":"---","c23a99b4":"## **Precision Score**\n\nIn order to introduce our next classification metric, let\u2019s go through a quick example. Imagine you\u2019re a doctor who applies machine learning models to predict if a patient must resort to a surgical procedure as a treatment for a given disease. We must have in mind that surgical interventions must be avoided as much as possible and, hence, you don\u2019t want your machine learning models to make frivolous recommendations, i.e. False Positives. In such scenarios, the models should have high Precisions. \n\n- **Precision:** The precision metric expresses the percentage of predictions that were indeed positive.\n\n\\begin{equation}\n\\frac{tp}{tp+fp}\n\\end{equation}","c6071e70":"## **Area Under the Curve (AUC)**\n\nThe Receiver Operating Characteristic (ROC) curve, is a 2-dimensional plot that presents the model prediction ability at every classification threshold. Each point in the visualization has coordinates True Positive Rate (a.k.a. Recall), and False Positive Rate (a.k.a. **1-specificity**), derived from the model built with the respective thresholds. Intuitively, TPR tells us how good the model is in classifying positive records, whereas Specificity tells how good it is to classify negative points. Consequently, if the solution presents a TPR equal to 1 and a FPR equal to 0 (i.e perfect Specificity), we would have developed an oracle model (see the figure below). \n\n![auc.png](attachment:59347e32-7971-4d88-b03f-65ac04c3ce6f.png)\n\nSince the ROC curve displays the general performance of the model, it follows that the bigger the area under the resulting plot, the better will be the average performance of the model. If we normalize the Y-axis\u2019 values, we will end up finding a figure with area ranging in the interval [0, 1]. The Area Under the ROC Curve (AUC) is a classification metric that succinctly expresses the probability that a classifier will give bigger importances to positive instances than to negative ones. A latent problem, however, is that the metric doesn\u2019t express the model\u2019s confidence level, only its correctness rate.\n\n- **AUC:** The probability of giving more importance to positive records if compared to the negative ones.","835f7c2c":"## **Accuracy Score**\n\nAnother simple classification metric is Accuracy. Accuracy is a composition of the Confusion Matrix cells in which we consider the percentage of guesses that were correctly made. This might seem to be a great metric at first sight, however, it presents a strong shortcoming that we must be aware of when engineering a solution.\n\n- **Accuracy:** Succinctly, we can translate this metric to the following question: out of everything my model has predicted, what percentage was actually correct? Mathematically,\n\n\\begin{equation}\n\\frac{tp+tn}{tp+tn+fp+fn}\n\\end{equation}","a8b1e499":"As we can see above, the confusion matrix allows us to easily spot where the main misclassification errors lie. For example, since we are focusing mostly on positive class items, we need to try reduce the False Negatives ($C_{0,0}$ cell) from our model. Maybe at the cost of a higher False Positive rate.","9bfc7de7":"---\n## **Confusion Matrix**\n\nThe major aspect that differentiates a supervised learning algorithm to the remaining ones is the data used in the training process. Basically, the inputs are (x,y) tuples which correspond to the features, and the class to what the record belongs to, respectively. Consequently, a prediction \u0177 can easily be assessed by comparing the equivalence of y and \u0177. From this comparison we can derive four different status:\n\n- **True Positive:** when the original, and the predicted label both belong to the positive class.\n- **False Positive:** when the original label belongs to the negative class, whereas the predicted label belongs to the positive.\n- **True Negative:** when the original, and the predicted label both belong to the negative class.\n- **False Negative:** when the original label belongs to the positive class, whereas the predicted label belongs to the negative.\n\nThese four states are the basis of almost all supervised learning metrics. We begin with the Confusion Matrix, a simple, yet powerful means to visualize the performance of a given model by simply counting the frequency from each of the status above.\n\n- **Confusion Matrix:** Essentially, a confusion matrix builds a table in which the lines represent the predicted values, whereas the columns represent the true labels. Since with this metric we can visualize every possible outcome for the prediction, we have a general understanding of the whole scenario, and have a rough estimation for the model performance.","d00b699a":"Similar to Precision, Specificity pays attention to the False Positive rate. Nonetheless, instead of considering this rate with regard to the positive class, it considers the negative, a caracteristic not so relevant to our analysis.","b4dab97c":"The Accuracy metric pointed to a perfect classification rate, even though we've checked with the aid of Confusion Matrix that out model isn't this clever. So the main limitation here is the inability to deal with skewed classes. In other words, when one class presents a strong predominance, we should expect accuracy to conclude that we have built a skilled solution even though we only predicted the most frequent class. For example, consider a model that literally predicts no-fraud to every record","4bb92f24":"I've chosen to use a simple cross-validation approach in this notebook","43b90939":"Precision seems to be able to capture some of the imperfections on the model's predictions. However, it pays attention to the False Positive rate, a not so much important concept for the anomaly detection problem (even less important when the checking procedure is relatively cheap). Consequently, the final score still points to a considerably good solution.","efc524f9":"Note that F1-score was capable of pointing to a possible flaw in the model. This is majorly because of the influence coming directly from the model's Recall.","87273fda":"## **Recall Score**\n\nThe Recall, also known as Sensitivity, is a metric concerned with the True Positive Rate. For example, when dealing with fraud detection, it might be cheaper to investigate many suspicious cases - even if they\u2019re false alarms - than to let one single example go by inadvertently. Hence, in such a case we should look for High Sensitivity scores.\n\n- **Recall (Sensitivity):** From all positive records, what percentage was actually predicted as positives?\n\n\\begin{equation}\n\\frac{tp}{tp+fn}\n\\end{equation}"}}