{"cell_type":{"35d274cc":"code","8be2e7ca":"code","1d459c6d":"code","868c65b8":"code","a38f72a6":"code","8d9e546e":"code","c4a81447":"code","ce88adf4":"code","718b2794":"code","b343ea50":"markdown"},"source":{"35d274cc":"!pip install -q efficientnet","8be2e7ca":"import numpy as np\nimport pandas as pd\nimport re\nfrom tqdm.auto import tqdm\nfrom kaggle_datasets import KaggleDatasets\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow_addons as tfa\nimport efficientnet.tfkeras as efn","1d459c6d":"AUTO = tf.data.experimental.AUTOTUNE","868c65b8":"class CFG:\n    N_CLASSES = 64500\n    IMAGE_SIZE = [256, 256]\n    BATCH_SIZE = 16","a38f72a6":"TEST_FILENAMES = tf.io.gfile.glob('..\/input\/herb2021-test-256\/*.tfrec')","8d9e546e":"def get_model():\n    base_model = efn.EfficientNetB0(weights=None, \n                                    include_top=False, \n                                    pooling='avg',\n                                    input_shape=(*CFG.IMAGE_SIZE, 3))\n    model = tf.keras.Sequential([\n        base_model,\n        L.Dense(CFG.N_CLASSES, activation='sigmoid')\n    ])\n    \n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=[tfa.metrics.F1Score(CFG.N_CLASSES, average='weighted')])\n    \n    return model","c4a81447":"model = get_model()\nmodel.load_weights('..\/input\/herb2021-effnet\/best.h5')","ce88adf4":"def decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)  # image format uint8 [0,255]\n    image = tf.cast(image, tf.float32) \/ 255.0  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [*CFG.IMAGE_SIZE, 3])\n    return image\n\ndef get_idx(image, idnum):\n    idnum = tf.strings.split(idnum, sep='\/')[6]\n    idnum = tf.strings.regex_replace(idnum, \".jpg\", \"\")\n    idnum = tf.strings.to_number(idnum, out_type=tf.int64)\n    return image, idnum\n    \ndef data_augment(image, label):\n    image = tf.image.random_flip_left_right(image)\n    return image, label\n\ndef read_unlabeled_tfrecord(example):\n    UNLABELED_TFREC_FORMAT = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'image_idx': tf.io.FixedLenFeature([], tf.string)\n    }\n\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    idnum=example['image_idx']\n    return image, idnum\n\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'image_idx': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([], tf.int64),\n    }\n\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = example['label']\n    return image, label\n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n    return dataset\n\ndef get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES)\n    dataset = dataset.map(onehot, num_parallel_calls=AUTO)\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    dataset = dataset.repeat() # the training dataset must repeat for several epochs\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(CFG.BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_test_dataset(ordered=False, augmented=False):\n    dataset = load_dataset(TEST_FILENAMES, labeled=False, ordered=ordered)\n    dataset = dataset.map(get_idx, num_parallel_calls=AUTO)\n    dataset = dataset.batch(CFG.BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\nprint('Dataset: {} unlabeled test images'.format(NUM_TEST_IMAGES))","718b2794":"print('Calculating predictions...')\ntest_ds = get_test_dataset(ordered=True)\ntest_images_ds = test_ds.map(lambda image, idnum: image)\n\npredictions = np.zeros(NUM_TEST_IMAGES, dtype=np.int32)\nfor i, image in tqdm(enumerate(test_images_ds), total=NUM_TEST_IMAGES\/\/CFG.BATCH_SIZE + 1):\n    idx1 = i*CFG.BATCH_SIZE\n    if (idx1 + CFG.BATCH_SIZE) > NUM_TEST_IMAGES:\n        idx2 = NUM_TEST_IMAGES\n    else:\n        idx2 = idx1 + CFG.BATCH_SIZE\n    predictions[idx1:idx2] = np.argmax(model.predict_on_batch(image), axis=-1)\n\nprint('Generating submission file...')\nsub = pd.read_csv('..\/input\/herbarium-2021-fgvc8\/sample_submission.csv')\nsub['Predicted'] = predictions\nsub.to_csv('submission.csv', index=False)\nprint(sub.head(10))","b343ea50":"This notebook uses parts of code from Martin Goerner's notebook [\nGetting started with 100+ flowers on TPU](https:\/\/www.kaggle.com\/mgornergoogle\/getting-started-with-100-flowers-on-tpu)."}}