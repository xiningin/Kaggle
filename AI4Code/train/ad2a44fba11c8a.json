{"cell_type":{"c7c33d0b":"code","02fa3aca":"code","6826d5ae":"code","e1977b60":"code","6bc8a583":"code","6b236bc9":"code","81417d36":"code","ff4303a2":"code","03b59621":"code","07477961":"code","b07f9a0e":"code","8f214f7c":"code","80365113":"code","dbb741d9":"markdown","d8b1a453":"markdown"},"source":{"c7c33d0b":"from PIL import Image, ImageDraw\nfrom collections import defaultdict\nimport os\nimport numpy as np \nimport pickle\nimport tensorflow\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.models import Model\nimport matplotlib.pyplot as plt\n\ndef save(path,obj, name ):\n    with open(path+ name + '.pkl', 'wb') as f:\n        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n\ndef load(path ,name ):\n    with open(path + name + '.pkl', 'rb') as f:\n        return pickle.load(f)","02fa3aca":"def create_drawing(xcoor,ycoor,draw):\n        points= []\n        list=[]\n        for i in range(len(xcoor)):\n            try:\n                 list.append((xcoor[i],ycoor[i]))\n            except Exception as e:\n                print(i)\n        draw.line(list,fill=\"black\", width=3) ","6826d5ae":"#Prepare Matrix of size 27*27 and create PKL files from ndjson\ndef PrepareData(folderpath):\n    files = glob(folderpath)\n    filename= \"sample\"\n    k=0;\n    for file in files:\n        with nlj.open(file) as src:\n            print(file)\n            imagedata = defaultdict(list)\n            for i, line in enumerate(src):\n                array = np.asarray(line['drawing'])\n                matrix = np.array([[255 for i in range(27)] for i in range(27)])\n                matrix = matrix.astype('float64') \n                if array.ndim == 2:\n                        im = Image.new('L', (255, 255),color=\"white\")\n                        draw = ImageDraw.Draw(im)\n                        for row in range(array.shape[0]):\n                            create_drawing(array[row,0], array[row,1],draw)\n                        resized = im.resize((27,27), Image.ADAPTIVE)\n                        matrix = np.array(resized)\n                imagedata[line['word']].append(matrix)\n                filename = line['word']\n        save('..\\\\batch3\\\\',imagedata[line['word']],filename)\n        \n#PrepareData(\"..\\\\ndjson\\\\*.ndjson\")\n","e1977b60":"#prepare consolidated data\ndef ConsolidateData(folderpath):\n    train = np.zeros(1, dtype = int) \n    train_labels = np.zeros(1, dtype = int)\n    k=0\n    with os.scandir(folderpath) as entries:\n        for entry in entries:\n            name = entry.name.split('.')[0]\n            data =  load_obj(folderpath,name)\n            data_img = np.array(data)\n            length =len(data)\n            input_data = data_img.reshape(length, 27*27)\n            counttrain = round(length * 0.35)\n            countvalidate = round(length * .05)\n            counttest = round(length * 0.05)\n            if train.shape[0] == 1:\n                train = input_data[0:counttrain,:]\n                train_labels = np.array([k for i in range(counttrain)])\n                validate = input_data[counttrain: counttrain + countvalidate,:]\n                validate_labels = np.array([k for i in range(countvalidate)])\n                test =  input_data[counttrain + countvalidate: counttest + counttrain + countvalidate ,:]\n                counttest = test.shape[0]\n                test_labels = np.array([k for i in range(counttest)])\n            else:\n                train = np.concatenate((train, input_data[0:counttrain,:]))\n                train_labels = np.concatenate((train_labels, np.array([k for i in range(counttrain)])))\n                validate = np.concatenate((validate,input_data[counttrain: counttrain + countvalidate,:]))\n                validate_labels = np.concatenate((validate_labels,np.array([k for i in range(countvalidate)])))\n                testdata = input_data[counttrain + countvalidate: counttest + counttrain + countvalidate,:]\n                test = np.concatenate((test, testdata))\n                test_labels = np.concatenate((test_labels, np.array([k for i in range(testdata.shape[0])])))\n            k = k + 1    \n            print(name)\nfolderpath ='..\/batch3\/'\n#ConsolidateData(folderpath)            \npath = '..\/batch3\/'\n# save(path,train,'train_images')\n# save(path,train_labels,'train_labels')\n# save(path,validate,'validate_images')\n# save(path,validate_labels,'validate_labels')\n# save(path,test,'test_images')\n# save(path,test_labels,'test_labels')","6bc8a583":"path = '..\/input\/images\/'\n\ntrain_images = load(path,'train_images')\ntrain_labels =  load(path,'train_labels')\n\nvalidate_images = load(path,'validate_images')\nvalidate_labels = load(path,'validate_labels')\n\ntest_images = load(path,'test_images')\ntest_labels = load(path,'test_labels')","6b236bc9":"path = '..\/input\/images\/'\n\ntrain_images = load(path,'train_images')\ntrain_labels =  load(path,'train_labels')\n\nvalidate_images = load(path,'validate_images')\nvalidate_labels = load(path,'validate_labels')\n\ntest_images = load(path,'test_images')\ntest_labels = load(path,'test_labels')\n\n# import random\n# indexes = np.array([],dtype = int)\n# for x in range(500000):\n#     k = random.randint(1,531699)\n#     indexes = np.concatenate((indexes,[k]))\n\n# # print(indexes)\n# train_images = np.delete(train_images, indexes,axis=0)\n# train_labels = np.delete(train_labels, indexes)\n# print(train_images.shape)\n# print(train_labels.shape)\n\n# print(np.mean(train_images, axis=0))\n################################################################\nindexes = np.array([],dtype = int)\nfor i in range(len(train_images)):\n  result = all(train_images[i][0] == elem  for elem in train_images[i])\n  if result == True:\n       indexes = np.concatenate((indexes,[i]))\n      #print(result,i)\ntrain_images = np.delete(train_images, indexes,axis=0)\ntrain_labels = np.delete(train_labels, indexes)\n#################################################################\nindexes = np.array([],dtype = int)\nfor i in range(len(validate_images)):\n  result = all(validate_images[i][0] == elem  for elem in validate_images[i])\n  if result == True:\n       indexes = np.concatenate((indexes,[i]))\n      #print(result,i)\nvalidate_images = np.delete(validate_images, indexes,axis=0)\nvalidate_labels = np.delete(validate_labels, indexes)\n##################################################################    \nindexes = np.array([],dtype = int)\nfor i in range(len(test_images)):\n  result = all(test_images[i][0] == elem  for elem in test_images[i])\n  if result == True:\n       indexes = np.concatenate((indexes,[i]))\n      #print(result,i)\ntest_images = np.delete(test_images, indexes,axis=0)\ntest_labels = np.delete(test_labels, indexes)\n\nprint(train_images.shape)\nprint(train_labels.shape)\nprint(test_images.shape)\nprint(test_labels.shape)\ntrain_images = train_images.reshape(len(train_images),27,27,1)\ntest_images = test_images.reshape(len(test_images),27,27,1)\nvalidate_images = validate_images.reshape(len(validate_images),27,27,1)\n\n#all(item[2] == 0 \n# for item in train_images:\n#     print(item)\n# for i in range(0, 8):\n#     if train_images[7]\n#     print(train_images[7])\n#     display(image.array_to_img(train_images[7]))\n\n# datagen = ImageDataGenerator(\n#                             rotation_range=40,\n#                             width_shift_range=0.2,\n#                             height_shift_range=0.2,\n#                             shear_range=0.2,\n#                             zoom_range=0.2,\n#                             horizontal_flip=True,\n#                             fill_mode='nearest')\n\n# count = 0\n# for batch_image,batch_label in datagen.flow(train_images,train_labels, batch_size=10000):\n#     if count == 50:\n#         break;\n#     count = count + 1\n#     print(batch_image.shape)\n#     for i in range(0, 100):\n#         print(\"processing\" , i)\n#         img = batch_image[i]\n#         lab = batch_label[i]\n#         data = np.array([img])\n#         label = np.array([lab])\n#         train_labels = np.concatenate((train_labels,label))\n#         train_images = np.concatenate((train_images,data))\n#         #display(image.array_to_img(batch_image[i]))\n#         #print(lab)\n\n\ntrain_images = (train_images - np.mean(train_images, axis=0)) \/ np.std(train_images, axis=0)\nvalidate_images = (validate_images - np.mean(validate_images, axis=0)) \/ np.std(validate_images, axis=0)\ntest_images = (test_images - np.mean(test_images, axis=0)) \/ np.std(test_images, axis=0)\n# # validation_generator = test_datagen.flow(train_images, batch_size=1)\n\ntrain_labels = to_categorical(train_labels)\ntest_labels = to_categorical(test_labels)\nvalidate_labels = to_categorical(validate_labels)\n\n\ntensorflow.keras.backend.clear_session()\ninput_tensor = Input(shape=(27, 27,1))\nbranch_a = layers.Conv2D(60, (3, 3), activation='relu',input_shape=(27, 27,1))(input_tensor) #25*25\nbranch_a = layers.BatchNormalization()(branch_a)\nbranch_a = layers.MaxPooling2D((2, 2))(branch_a)  #12*12\nbranch_a = layers.Conv2D(80, (3,3), activation='relu')(branch_a) #10*10\n# branch_a = layers.BatchNormalization()(branch_a)\nbranch_a = layers.MaxPooling2D((4, 4))(branch_a)  #12*12\nbranch_a = layers.Flatten()(branch_a)\nbranch_a = layers.Dense(500, activation='relu')(branch_a)\n\nbranch_b = layers.Conv2D(20, (3, 27), activation='relu',input_shape=(27, 27,1))(input_tensor)\n#branch_b = layers.BatchNormalization()(branch_b)\nbranch_b = layers.MaxPooling2D((3, 1))(branch_b)\n#branch_b = layers.Conv2D(30, (2,2), activation='relu',strides=1)(branch_b)\n# branch_b = layers.MaxPooling2D((2, 2))(branch_b)\nbranch_b = layers.Flatten()(branch_b)\nbranch_b = layers.Dense(350, activation='relu')(branch_b)\n\nbranch_c = layers.Conv2D(40, (27, 3), activation='relu',input_shape=(27, 27,1))(input_tensor) #\n#branch_c = layers.BatchNormalization()(branch_c)\nbranch_c = layers.MaxPooling2D((1, 3))(branch_c)\nbranch_c = layers.Flatten()(branch_c)\nbranch_c = layers.Dense(500, activation='relu')(branch_c)\n\n# branch_d = layers.Conv2D(30, (9, 9), activation='relu',input_shape=(27, 27,1))(input_tensor) #19*19\n# branch_d = layers.BatchNormalization()(branch_d)\n# # branch_d = layers.MaxPooling2D((2, 2))(branch_d)  #12*12\n# # branch_d = layers.Conv2D(30, (3,3), activation='relu')(branch_d) #10*10\n# # branch_d = layers.BatchNormalization()(branch_d)\n# branch_d = layers.Flatten()(branch_d)\n# branch_d = layers.Dense(500, activation='relu')(branch_d)\n\n\nconcatenated = layers.concatenate([branch_c,branch_a],axis=-1)\noutput_tensor = layers.Dense(12, activation='softmax')(concatenated)\n\nmodel = Model(input_tensor, output_tensor)\nmodel.summary()\n\n#optimizer = optimizers.SGD(lr=0.01, clipnorm=1.)\noptimizer= optimizers.Adadelta()#lr=0.8, clipnorm=1.)\n#optimizer = optimizers.Adamax()\n\nmodel.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n\n#epochs = 1\nepochs = 600\nbatchsize = 35\n\nhistory = model.fit(train_images, train_labels, epochs=epochs, batch_size=batchsize, shuffle=True,\n                     validation_data=(validate_images, validate_labels))\n","81417d36":"results = model.evaluate(test_images, test_labels)\n\nprint(epochs)\nprint(batchsize)\nprint(model.metrics_names)\nprint(results)\nprint(history.history.keys())","ff4303a2":"model.save('..\/working\/Model.h5')","03b59621":"loss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n#'loss', 'acc', 'val_loss', 'val_acc'\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","07477961":"from IPython.display import Image\nImage(\"..\/input\/result\/acc.jpg\")","b07f9a0e":"Image(\"..\/input\/result\/loss.jpg\")","8f214f7c":"from os import walk\nfor (dirpath, dirnames, filenames) in walk(\"..\/\"):\n    print(\"Directory path: \", dirpath)\n    print(\"Folder name: \", dirnames)\n    print(\"File name: \", filenames)","80365113":"#Prediction\n# from PIL import Image\n# img = Image.open('test image path').convert('LA')\n# img = img.resize((27, 27), Image.ANTIALIAS)\n# x = np.array(img)\n# print(x.shape)\n# x = x[:,:,1:1]\n# print(x.shape)\n# mean normalize image before prediction\n# coll= np.array([x])\n# from tensorflow.keras.models import load_model\n# model = load_model('Model.h5')\n# model.predict(coll)\n","dbb741d9":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 27, 27, 1)]  0                                            \n__________________________________________________________________________________________________\nconv2d (Conv2D)                 (None, 25, 25, 60)   600         input_1[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization (BatchNorma (None, 25, 25, 60)   240         conv2d[0][0]                     \n__________________________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)    (None, 12, 12, 60)   0           batch_normalization[0][0]        \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 1, 25, 40)    3280        input_1[0][0]                    \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 10, 10, 80)   43280       max_pooling2d[0][0]              \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 1, 8, 40)     0           conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 2, 2, 80)     0           conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nflatten_2 (Flatten)             (None, 320)          0           max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nflatten (Flatten)               (None, 320)          0           max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 500)          160500      flatten_2[0][0]                  \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 500)          160500      flatten[0][0]                    \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 1000)         0           dense_2[0][0]                    \n                                                                 dense[0][0]                      \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 12)           12012       concatenate[0][0]                \n==================================================================================================\nTotal params: 380,412\nTrainable params: 380,292\nNon-trainable params: 120\n__________________________________________________________________________________________________\nTrain on 236594 samples, validate on 75949 samples\nEpoch 1\/600\n236594\/236594 [==============================] - 32s 136us\/sample - loss: 2.0283 - acc: 0.3773 - val_loss: 1.7627 - val_acc: 0.4908\nEpoch 2\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 1.5016 - acc: 0.5802 - val_loss: 1.4315 - val_acc: 0.5954\nEpoch 3\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 1.2606 - acc: 0.6453 - val_loss: 1.2413 - val_acc: 0.6478\nEpoch 4\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 1.1122 - acc: 0.6841 - val_loss: 1.1142 - val_acc: 0.6818\nEpoch 5\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 1.0121 - acc: 0.7117 - val_loss: 1.0283 - val_acc: 0.7053\nEpoch 6\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.9412 - acc: 0.7310 - val_loss: 0.9658 - val_acc: 0.7218\nEpoch 7\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.8875 - acc: 0.7459 - val_loss: 0.9136 - val_acc: 0.7363\nEpoch 8\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.8449 - acc: 0.7578 - val_loss: 0.8762 - val_acc: 0.7466\nEpoch 9\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.8102 - acc: 0.7669 - val_loss: 0.8401 - val_acc: 0.7567\nEpoch 10\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.7812 - acc: 0.7751 - val_loss: 0.8121 - val_acc: 0.7654\nEpoch 11\/600\n236594\/236594 [==============================] - 31s 129us\/sample - loss: 0.7559 - acc: 0.7823 - val_loss: 0.7911 - val_acc: 0.7717\nEpoch 12\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.7342 - acc: 0.7883 - val_loss: 0.7712 - val_acc: 0.7769\nEpoch 13\/600\n236594\/236594 [==============================] - 30s 129us\/sample - loss: 0.7149 - acc: 0.7935 - val_loss: 0.7483 - val_acc: 0.7834\nEpoch 14\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.6973 - acc: 0.7988 - val_loss: 0.7336 - val_acc: 0.7866\nEpoch 15\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.6817 - acc: 0.8030 - val_loss: 0.7182 - val_acc: 0.7910\nEpoch 16\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.6673 - acc: 0.8073 - val_loss: 0.7048 - val_acc: 0.7946\nEpoch 17\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.6547 - acc: 0.8108 - val_loss: 0.6919 - val_acc: 0.7977\nEpoch 18\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.6422 - acc: 0.8143 - val_loss: 0.6809 - val_acc: 0.8012\nEpoch 19\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.6312 - acc: 0.8175 - val_loss: 0.6678 - val_acc: 0.8053\nEpoch 20\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.6206 - acc: 0.8208 - val_loss: 0.6603 - val_acc: 0.8071\nEpoch 21\/600\n236594\/236594 [==============================] - 30s 129us\/sample - loss: 0.6108 - acc: 0.8239 - val_loss: 0.6480 - val_acc: 0.8109\nEpoch 22\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.6017 - acc: 0.8262 - val_loss: 0.6412 - val_acc: 0.8129\nEpoch 23\/600\n236594\/236594 [==============================] - 30s 129us\/sample - loss: 0.5930 - acc: 0.8286 - val_loss: 0.6316 - val_acc: 0.8155\nEpoch 24\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.5848 - acc: 0.8307 - val_loss: 0.6218 - val_acc: 0.8187\nEpoch 25\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.5770 - acc: 0.8335 - val_loss: 0.6159 - val_acc: 0.8205\nEpoch 26\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.5699 - acc: 0.8352 - val_loss: 0.6095 - val_acc: 0.8225\nEpoch 27\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.5629 - acc: 0.8373 - val_loss: 0.6049 - val_acc: 0.8236\nEpoch 28\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.5562 - acc: 0.8397 - val_loss: 0.5973 - val_acc: 0.8260\nEpoch 29\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.5500 - acc: 0.8414 - val_loss: 0.5896 - val_acc: 0.8282\nEpoch 30\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.5439 - acc: 0.8432 - val_loss: 0.5832 - val_acc: 0.8302\nEpoch 31\/600\n236594\/236594 [==============================] - 30s 129us\/sample - loss: 0.5383 - acc: 0.8450 - val_loss: 0.5780 - val_acc: 0.8314\nEpoch 32\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.5325 - acc: 0.8470 - val_loss: 0.5727 - val_acc: 0.8328\nEpoch 33\/600\n\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.5274 - acc: 0.8479 - val_loss: 0.5679 - val_acc: 0.8348\nEpoch 34\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.5223 - acc: 0.8496 - val_loss: 0.5653 - val_acc: 0.8353\nEpoch 35\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.5173 - acc: 0.8512 - val_loss: 0.5581 - val_acc: 0.8372\nEpoch 36\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.5126 - acc: 0.8529 - val_loss: 0.5526 - val_acc: 0.8391\nEpoch 37\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.5081 - acc: 0.8540 - val_loss: 0.5470 - val_acc: 0.8406\nEpoch 38\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.5036 - acc: 0.8554 - val_loss: 0.5449 - val_acc: 0.8409\nEpoch 39\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.4993 - acc: 0.8567 - val_loss: 0.5386 - val_acc: 0.8426\nEpoch 40\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.4952 - acc: 0.8577 - val_loss: 0.5371 - val_acc: 0.8436\nEpoch 41\/600\n236594\/236594 [==============================] - 31s 129us\/sample - loss: 0.4913 - acc: 0.8588 - val_loss: 0.5317 - val_acc: 0.8451\nEpoch 42\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.4873 - acc: 0.8602 - val_loss: 0.5277 - val_acc: 0.8460\nEpoch 43\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.4835 - acc: 0.8614 - val_loss: 0.5244 - val_acc: 0.8472\nEpoch 44\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.4797 - acc: 0.8624 - val_loss: 0.5205 - val_acc: 0.8482\nEpoch 45\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.4765 - acc: 0.8630 - val_loss: 0.5172 - val_acc: 0.8493\nEpoch 46\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.4729 - acc: 0.8642 - val_loss: 0.5140 - val_acc: 0.8497\nEpoch 47\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.4695 - acc: 0.8651 - val_loss: 0.5119 - val_acc: 0.8506\nEpoch 48\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.4660 - acc: 0.8660 - val_loss: 0.5078 - val_acc: 0.8522\nEpoch 49\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.4630 - acc: 0.8668 - val_loss: 0.5034 - val_acc: 0.8534\nEpoch 50\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.4600 - acc: 0.8677 - val_loss: 0.5014 - val_acc: 0.8541\nEpoch 51\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.4570 - acc: 0.8687 - val_loss: 0.4969 - val_acc: 0.8555\nEpoch 52\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.4539 - acc: 0.8696 - val_loss: 0.4953 - val_acc: 0.8558\nEpoch 53\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.4513 - acc: 0.8699 - val_loss: 0.4925 - val_acc: 0.8567\nEpoch 54\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.4483 - acc: 0.8711 - val_loss: 0.4931 - val_acc: 0.8559\nEpoch 55\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.4451 - acc: 0.8720 - val_loss: 0.4853 - val_acc: 0.8586\nEpoch 56\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.4429 - acc: 0.8725 - val_loss: 0.4846 - val_acc: 0.8592\nEpoch 57\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.4401 - acc: 0.8735 - val_loss: 0.4830 - val_acc: 0.8594\nEpoch 58\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.4376 - acc: 0.8740 - val_loss: 0.4807 - val_acc: 0.8597\nEpoch 59\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.4351 - acc: 0.8744 - val_loss: 0.4759 - val_acc: 0.8619\nEpoch 60\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.4326 - acc: 0.8753 - val_loss: 0.4750 - val_acc: 0.8617\nEpoch 61\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.4306 - acc: 0.8763 - val_loss: 0.4724 - val_acc: 0.8627\nEpoch 62\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.4280 - acc: 0.8771 - val_loss: 0.4705 - val_acc: 0.8633\nEpoch 63\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.4260 - acc: 0.8775 - val_loss: 0.4673 - val_acc: 0.8643\nEpoch 64\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.4237 - acc: 0.8780 - val_loss: 0.4659 - val_acc: 0.8645\nEpoch 65\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.4215 - acc: 0.8788 - val_loss: 0.4632 - val_acc: 0.8655\nEpoch 66\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.4193 - acc: 0.8795 - val_loss: 0.4614 - val_acc: 0.8658\nEpoch 67\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.4173 - acc: 0.8802 - val_loss: 0.4613 - val_acc: 0.8658\nEpoch 68\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.4150 - acc: 0.8804 - val_loss: 0.4581 - val_acc: 0.8668\nEpoch 69\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.4132 - acc: 0.8808 - val_loss: 0.4567 - val_acc: 0.8670\nEpoch 70\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.4112 - acc: 0.8815 - val_loss: 0.4530 - val_acc: 0.8686\nEpoch 71\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.4093 - acc: 0.8820 - val_loss: 0.4522 - val_acc: 0.8684\nEpoch 72\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.4074 - acc: 0.8828 - val_loss: 0.4508 - val_acc: 0.8694\nEpoch 73\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.4055 - acc: 0.8829 - val_loss: 0.4471 - val_acc: 0.8702\nEpoch 74\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.4034 - acc: 0.8837 - val_loss: 0.4462 - val_acc: 0.8705\nEpoch 75\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.4020 - acc: 0.8841 - val_loss: 0.4449 - val_acc: 0.8709\nEpoch 76\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.4003 - acc: 0.8850 - val_loss: 0.4426 - val_acc: 0.8715\nEpoch 77\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3984 - acc: 0.8846 - val_loss: 0.4409 - val_acc: 0.8724\nEpoch 78\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3967 - acc: 0.8855 - val_loss: 0.4403 - val_acc: 0.8724\nEpoch 79\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3951 - acc: 0.8859 - val_loss: 0.4379 - val_acc: 0.8736\nEpoch 80\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3935 - acc: 0.8866 - val_loss: 0.4363 - val_acc: 0.8740\nEpoch 81\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3918 - acc: 0.8869 - val_loss: 0.4356 - val_acc: 0.8734\nEpoch 82\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.3901 - acc: 0.8873 - val_loss: 0.4341 - val_acc: 0.8745\nEpoch 83\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3886 - acc: 0.8880 - val_loss: 0.4330 - val_acc: 0.8745\nEpoch 84\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3871 - acc: 0.8883 - val_loss: 0.4317 - val_acc: 0.8751\nEpoch 85\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.3858 - acc: 0.8888 - val_loss: 0.4305 - val_acc: 0.8757\nEpoch 86\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3843 - acc: 0.8890 - val_loss: 0.4278 - val_acc: 0.8763\nEpoch 87\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3826 - acc: 0.8894 - val_loss: 0.4265 - val_acc: 0.8765\nEpoch 88\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3809 - acc: 0.8902 - val_loss: 0.4267 - val_acc: 0.8763\nEpoch 89\/600\n\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3800 - acc: 0.8902 - val_loss: 0.4249 - val_acc: 0.8771\nEpoch 90\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3783 - acc: 0.8905 - val_loss: 0.4233 - val_acc: 0.8773\nEpoch 91\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3770 - acc: 0.8910 - val_loss: 0.4226 - val_acc: 0.8776\nEpoch 92\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.3756 - acc: 0.8914 - val_loss: 0.4208 - val_acc: 0.8778\nEpoch 93\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3744 - acc: 0.8918 - val_loss: 0.4193 - val_acc: 0.8783\nEpoch 94\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3727 - acc: 0.8924 - val_loss: 0.4187 - val_acc: 0.8787\nEpoch 95\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3714 - acc: 0.8926 - val_loss: 0.4181 - val_acc: 0.8785\nEpoch 96\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.3704 - acc: 0.8929 - val_loss: 0.4156 - val_acc: 0.8791\nEpoch 97\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3688 - acc: 0.8932 - val_loss: 0.4143 - val_acc: 0.8796\nEpoch 98\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3678 - acc: 0.8938 - val_loss: 0.4131 - val_acc: 0.8802\nEpoch 99\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3665 - acc: 0.8941 - val_loss: 0.4116 - val_acc: 0.8803\nEpoch 100\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3654 - acc: 0.8942 - val_loss: 0.4105 - val_acc: 0.8808\nEpoch 101\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3642 - acc: 0.8946 - val_loss: 0.4094 - val_acc: 0.8813\nEpoch 102\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.3629 - acc: 0.8950 - val_loss: 0.4092 - val_acc: 0.8812\nEpoch 103\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3616 - acc: 0.8957 - val_loss: 0.4100 - val_acc: 0.8810\nEpoch 104\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3607 - acc: 0.8959 - val_loss: 0.4080 - val_acc: 0.8814\nEpoch 105\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3593 - acc: 0.8962 - val_loss: 0.4056 - val_acc: 0.8818\nEpoch 106\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.3582 - acc: 0.8964 - val_loss: 0.4057 - val_acc: 0.8821\nEpoch 107\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3572 - acc: 0.8968 - val_loss: 0.4030 - val_acc: 0.8829\nEpoch 108\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3561 - acc: 0.8967 - val_loss: 0.4028 - val_acc: 0.8830\nEpoch 109\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3549 - acc: 0.8973 - val_loss: 0.4036 - val_acc: 0.8825\nEpoch 110\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3537 - acc: 0.8975 - val_loss: 0.4012 - val_acc: 0.8834\nEpoch 111\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3528 - acc: 0.8980 - val_loss: 0.3993 - val_acc: 0.8838\nEpoch 112\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.3516 - acc: 0.8981 - val_loss: 0.3992 - val_acc: 0.8839\nEpoch 113\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3508 - acc: 0.8983 - val_loss: 0.3977 - val_acc: 0.8844\nEpoch 114\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3496 - acc: 0.8988 - val_loss: 0.3971 - val_acc: 0.8853\nEpoch 115\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3486 - acc: 0.8987 - val_loss: 0.3968 - val_acc: 0.8842\nEpoch 116\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3476 - acc: 0.8992 - val_loss: 0.3946 - val_acc: 0.8856\nEpoch 117\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.3468 - acc: 0.8993 - val_loss: 0.3954 - val_acc: 0.8851\nEpoch 118\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3457 - acc: 0.8996 - val_loss: 0.3938 - val_acc: 0.8858\nEpoch 119\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3448 - acc: 0.9001 - val_loss: 0.3926 - val_acc: 0.8862\nEpoch 120\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3438 - acc: 0.9002 - val_loss: 0.3918 - val_acc: 0.8861\nEpoch 121\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3430 - acc: 0.9008 - val_loss: 0.3906 - val_acc: 0.8868\nEpoch 122\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.3417 - acc: 0.9010 - val_loss: 0.3917 - val_acc: 0.8863\nEpoch 123\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3409 - acc: 0.9010 - val_loss: 0.3890 - val_acc: 0.8870\nEpoch 124\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3401 - acc: 0.9017 - val_loss: 0.3883 - val_acc: 0.8875\nEpoch 125\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3391 - acc: 0.9018 - val_loss: 0.3877 - val_acc: 0.8875\nEpoch 126\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3382 - acc: 0.9020 - val_loss: 0.3874 - val_acc: 0.8871\nEpoch 127\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.3373 - acc: 0.9023 - val_loss: 0.3864 - val_acc: 0.8880\nEpoch 128\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3364 - acc: 0.9025 - val_loss: 0.3855 - val_acc: 0.8883\nEpoch 129\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3355 - acc: 0.9027 - val_loss: 0.3845 - val_acc: 0.8885\nEpoch 130\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3349 - acc: 0.9031 - val_loss: 0.3830 - val_acc: 0.8888\nEpoch 131\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3339 - acc: 0.9032 - val_loss: 0.3823 - val_acc: 0.8891\nEpoch 132\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3330 - acc: 0.9036 - val_loss: 0.3825 - val_acc: 0.8892\nEpoch 133\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3321 - acc: 0.9035 - val_loss: 0.3811 - val_acc: 0.8897\nEpoch 134\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3313 - acc: 0.9040 - val_loss: 0.3826 - val_acc: 0.8895\nEpoch 135\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3306 - acc: 0.9043 - val_loss: 0.3810 - val_acc: 0.8897\nEpoch 136\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3297 - acc: 0.9044 - val_loss: 0.3791 - val_acc: 0.8896\nEpoch 137\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.3289 - acc: 0.9048 - val_loss: 0.3797 - val_acc: 0.8902\nEpoch 138\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.3282 - acc: 0.9052 - val_loss: 0.3785 - val_acc: 0.8906\nEpoch 139\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3274 - acc: 0.9050 - val_loss: 0.3783 - val_acc: 0.8903\nEpoch 140\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3264 - acc: 0.9054 - val_loss: 0.3765 - val_acc: 0.8915\nEpoch 141\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3256 - acc: 0.9055 - val_loss: 0.3764 - val_acc: 0.8911\nEpoch 142\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3249 - acc: 0.9057 - val_loss: 0.3756 - val_acc: 0.8915\nEpoch 143\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.3242 - acc: 0.9061 - val_loss: 0.3741 - val_acc: 0.8920\nEpoch 144\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3235 - acc: 0.9062 - val_loss: 0.3732 - val_acc: 0.8924\nEpoch 145\/600\n\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3225 - acc: 0.9068 - val_loss: 0.3731 - val_acc: 0.8924\nEpoch 146\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3219 - acc: 0.9068 - val_loss: 0.3724 - val_acc: 0.8925\nEpoch 147\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3211 - acc: 0.9069 - val_loss: 0.3724 - val_acc: 0.8926\nEpoch 148\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.3205 - acc: 0.9074 - val_loss: 0.3717 - val_acc: 0.8928\nEpoch 149\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3196 - acc: 0.9075 - val_loss: 0.3712 - val_acc: 0.8930\nEpoch 150\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3189 - acc: 0.9076 - val_loss: 0.3707 - val_acc: 0.8930\nEpoch 151\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3183 - acc: 0.9077 - val_loss: 0.3709 - val_acc: 0.8931\nEpoch 152\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3175 - acc: 0.9081 - val_loss: 0.3689 - val_acc: 0.8939\nEpoch 153\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.3168 - acc: 0.9083 - val_loss: 0.3688 - val_acc: 0.8938\nEpoch 154\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3162 - acc: 0.9083 - val_loss: 0.3668 - val_acc: 0.8942\nEpoch 155\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3155 - acc: 0.9089 - val_loss: 0.3671 - val_acc: 0.8940\nEpoch 156\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3149 - acc: 0.9088 - val_loss: 0.3672 - val_acc: 0.8939\nEpoch 157\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3139 - acc: 0.9089 - val_loss: 0.3667 - val_acc: 0.8945\nEpoch 158\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.3134 - acc: 0.9090 - val_loss: 0.3664 - val_acc: 0.8945\nEpoch 159\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.3128 - acc: 0.9096 - val_loss: 0.3644 - val_acc: 0.8948\nEpoch 160\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3120 - acc: 0.9098 - val_loss: 0.3650 - val_acc: 0.8944\nEpoch 161\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3114 - acc: 0.9097 - val_loss: 0.3658 - val_acc: 0.8944\nEpoch 162\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3109 - acc: 0.9100 - val_loss: 0.3641 - val_acc: 0.8951\nEpoch 163\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.3101 - acc: 0.9104 - val_loss: 0.3635 - val_acc: 0.8949\nEpoch 164\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3096 - acc: 0.9102 - val_loss: 0.3622 - val_acc: 0.8956\nEpoch 165\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3090 - acc: 0.9106 - val_loss: 0.3629 - val_acc: 0.8953\nEpoch 166\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3082 - acc: 0.9107 - val_loss: 0.3622 - val_acc: 0.8957\nEpoch 167\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3077 - acc: 0.9109 - val_loss: 0.3608 - val_acc: 0.8960\nEpoch 168\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3072 - acc: 0.9113 - val_loss: 0.3619 - val_acc: 0.8954\nEpoch 169\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.3066 - acc: 0.9113 - val_loss: 0.3615 - val_acc: 0.8958\nEpoch 170\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3059 - acc: 0.9116 - val_loss: 0.3590 - val_acc: 0.8964\nEpoch 171\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3051 - acc: 0.9117 - val_loss: 0.3592 - val_acc: 0.8966\nEpoch 172\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3045 - acc: 0.9121 - val_loss: 0.3590 - val_acc: 0.8964\nEpoch 173\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.3041 - acc: 0.9119 - val_loss: 0.3573 - val_acc: 0.8967\nEpoch 174\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3035 - acc: 0.9122 - val_loss: 0.3576 - val_acc: 0.8966\nEpoch 175\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3028 - acc: 0.9123 - val_loss: 0.3580 - val_acc: 0.8967\nEpoch 176\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.3024 - acc: 0.9128 - val_loss: 0.3565 - val_acc: 0.8972\nEpoch 177\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3019 - acc: 0.9127 - val_loss: 0.3568 - val_acc: 0.8971\nEpoch 178\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.3010 - acc: 0.9130 - val_loss: 0.3558 - val_acc: 0.8976\nEpoch 179\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.3006 - acc: 0.9133 - val_loss: 0.3569 - val_acc: 0.8967\nEpoch 180\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.3001 - acc: 0.9132 - val_loss: 0.3550 - val_acc: 0.8972\nEpoch 181\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2994 - acc: 0.9136 - val_loss: 0.3551 - val_acc: 0.8974\nEpoch 182\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.2990 - acc: 0.9140 - val_loss: 0.3545 - val_acc: 0.8974\nEpoch 183\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2983 - acc: 0.9138 - val_loss: 0.3534 - val_acc: 0.8978\nEpoch 184\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2977 - acc: 0.9136 - val_loss: 0.3538 - val_acc: 0.8980\nEpoch 185\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2972 - acc: 0.9141 - val_loss: 0.3545 - val_acc: 0.8973\nEpoch 186\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2968 - acc: 0.9145 - val_loss: 0.3538 - val_acc: 0.8975\nEpoch 187\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2963 - acc: 0.9145 - val_loss: 0.3529 - val_acc: 0.8980\nEpoch 188\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.2956 - acc: 0.9149 - val_loss: 0.3520 - val_acc: 0.8983\nEpoch 189\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2950 - acc: 0.9150 - val_loss: 0.3515 - val_acc: 0.8983\nEpoch 190\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2947 - acc: 0.9149 - val_loss: 0.3513 - val_acc: 0.8984\nEpoch 191\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2940 - acc: 0.9153 - val_loss: 0.3514 - val_acc: 0.8983\nEpoch 192\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2935 - acc: 0.9152 - val_loss: 0.3511 - val_acc: 0.8984\nEpoch 193\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2929 - acc: 0.9152 - val_loss: 0.3495 - val_acc: 0.8986\nEpoch 194\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2923 - acc: 0.9159 - val_loss: 0.3493 - val_acc: 0.8989\nEpoch 195\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2919 - acc: 0.9158 - val_loss: 0.3489 - val_acc: 0.8988\nEpoch 196\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2916 - acc: 0.9158 - val_loss: 0.3503 - val_acc: 0.8985\nEpoch 197\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2910 - acc: 0.9163 - val_loss: 0.3483 - val_acc: 0.8994\nEpoch 198\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2906 - acc: 0.9161 - val_loss: 0.3482 - val_acc: 0.8993\nEpoch 199\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2901 - acc: 0.9167 - val_loss: 0.3477 - val_acc: 0.8994\nEpoch 200\/600\n236594\/236594 [==============================] - 30s 129us\/sample - loss: 0.2894 - acc: 0.9165 - val_loss: 0.3473 - val_acc: 0.8994\nEpoch 201\/600\n\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2890 - acc: 0.9168 - val_loss: 0.3473 - val_acc: 0.8995\nEpoch 202\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2886 - acc: 0.9168 - val_loss: 0.3462 - val_acc: 0.8997\nEpoch 203\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2879 - acc: 0.9171 - val_loss: 0.3456 - val_acc: 0.9000\nEpoch 204\/600\n236594\/236594 [==============================] - 31s 129us\/sample - loss: 0.2875 - acc: 0.9172 - val_loss: 0.3450 - val_acc: 0.9004\nEpoch 205\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2870 - acc: 0.9172 - val_loss: 0.3456 - val_acc: 0.9001\nEpoch 206\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2866 - acc: 0.9174 - val_loss: 0.3451 - val_acc: 0.9004\nEpoch 207\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2863 - acc: 0.9175 - val_loss: 0.3439 - val_acc: 0.9010\nEpoch 208\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2855 - acc: 0.9177 - val_loss: 0.3447 - val_acc: 0.9003\nEpoch 209\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2854 - acc: 0.9178 - val_loss: 0.3442 - val_acc: 0.9007\nEpoch 210\/600\n236594\/236594 [==============================] - 31s 130us\/sample - loss: 0.2846 - acc: 0.9179 - val_loss: 0.3441 - val_acc: 0.9005\nEpoch 211\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2846 - acc: 0.9178 - val_loss: 0.3443 - val_acc: 0.9005\nEpoch 212\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2838 - acc: 0.9184 - val_loss: 0.3433 - val_acc: 0.9010\nEpoch 213\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2834 - acc: 0.9182 - val_loss: 0.3424 - val_acc: 0.9012\nEpoch 214\/600\n236594\/236594 [==============================] - 30s 129us\/sample - loss: 0.2830 - acc: 0.9184 - val_loss: 0.3439 - val_acc: 0.9007\nEpoch 215\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2825 - acc: 0.9187 - val_loss: 0.3435 - val_acc: 0.9007\nEpoch 216\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2820 - acc: 0.9187 - val_loss: 0.3416 - val_acc: 0.9019\nEpoch 217\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2818 - acc: 0.9189 - val_loss: 0.3419 - val_acc: 0.9016\nEpoch 218\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2813 - acc: 0.9190 - val_loss: 0.3410 - val_acc: 0.9016\nEpoch 219\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2807 - acc: 0.9189 - val_loss: 0.3415 - val_acc: 0.9017\nEpoch 220\/600\n236594\/236594 [==============================] - 30s 129us\/sample - loss: 0.2803 - acc: 0.9194 - val_loss: 0.3408 - val_acc: 0.9016\nEpoch 221\/600\n236594\/236594 [==============================] - 31s 129us\/sample - loss: 0.2799 - acc: 0.9192 - val_loss: 0.3404 - val_acc: 0.9018\nEpoch 222\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2793 - acc: 0.9195 - val_loss: 0.3411 - val_acc: 0.9016\nEpoch 223\/600\n236594\/236594 [==============================] - 30s 129us\/sample - loss: 0.2789 - acc: 0.9197 - val_loss: 0.3395 - val_acc: 0.9021\nEpoch 224\/600\n236594\/236594 [==============================] - 31s 130us\/sample - loss: 0.2785 - acc: 0.9200 - val_loss: 0.3398 - val_acc: 0.9022\nEpoch 225\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2782 - acc: 0.9200 - val_loss: 0.3406 - val_acc: 0.9019\nEpoch 226\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2775 - acc: 0.9201 - val_loss: 0.3406 - val_acc: 0.9018\nEpoch 227\/600\n236594\/236594 [==============================] - 30s 129us\/sample - loss: 0.2774 - acc: 0.9200 - val_loss: 0.3385 - val_acc: 0.9028\nEpoch 228\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2771 - acc: 0.9200 - val_loss: 0.3399 - val_acc: 0.9019\nEpoch 229\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2764 - acc: 0.9205 - val_loss: 0.3377 - val_acc: 0.9029\nEpoch 230\/600\n236594\/236594 [==============================] - 30s 129us\/sample - loss: 0.2762 - acc: 0.9207 - val_loss: 0.3383 - val_acc: 0.9026\nEpoch 231\/600\n236594\/236594 [==============================] - 31s 131us\/sample - loss: 0.2756 - acc: 0.9207 - val_loss: 0.3385 - val_acc: 0.9028\nEpoch 232\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2751 - acc: 0.9207 - val_loss: 0.3368 - val_acc: 0.9035\nEpoch 233\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2748 - acc: 0.9208 - val_loss: 0.3362 - val_acc: 0.9035\nEpoch 234\/600\n236594\/236594 [==============================] - 31s 129us\/sample - loss: 0.2743 - acc: 0.9211 - val_loss: 0.3361 - val_acc: 0.9033\nEpoch 235\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2739 - acc: 0.9210 - val_loss: 0.3361 - val_acc: 0.9035\nEpoch 236\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2735 - acc: 0.9212 - val_loss: 0.3360 - val_acc: 0.9036\nEpoch 237\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2732 - acc: 0.9214 - val_loss: 0.3348 - val_acc: 0.9040\nEpoch 238\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2728 - acc: 0.9213 - val_loss: 0.3359 - val_acc: 0.9038\nEpoch 239\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2723 - acc: 0.9216 - val_loss: 0.3354 - val_acc: 0.9038\nEpoch 240\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2720 - acc: 0.9219 - val_loss: 0.3340 - val_acc: 0.9041\nEpoch 241\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2716 - acc: 0.9217 - val_loss: 0.3349 - val_acc: 0.9038\nEpoch 242\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2712 - acc: 0.9220 - val_loss: 0.3340 - val_acc: 0.9043\nEpoch 243\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2709 - acc: 0.9218 - val_loss: 0.3348 - val_acc: 0.9041\nEpoch 244\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2705 - acc: 0.9221 - val_loss: 0.3344 - val_acc: 0.9042\nEpoch 245\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2701 - acc: 0.9222 - val_loss: 0.3340 - val_acc: 0.9043\nEpoch 246\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.2698 - acc: 0.9224 - val_loss: 0.3346 - val_acc: 0.9043\nEpoch 247\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2694 - acc: 0.9223 - val_loss: 0.3340 - val_acc: 0.9043\nEpoch 248\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.2688 - acc: 0.9227 - val_loss: 0.3337 - val_acc: 0.9043\nEpoch 249\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2684 - acc: 0.9228 - val_loss: 0.3333 - val_acc: 0.9047\nEpoch 250\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2681 - acc: 0.9228 - val_loss: 0.3325 - val_acc: 0.9048\nEpoch 251\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2679 - acc: 0.9230 - val_loss: 0.3322 - val_acc: 0.9049\nEpoch 252\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2673 - acc: 0.9230 - val_loss: 0.3324 - val_acc: 0.9046\nEpoch 253\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2670 - acc: 0.9232 - val_loss: 0.3316 - val_acc: 0.9051\nEpoch 254\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2668 - acc: 0.9231 - val_loss: 0.3312 - val_acc: 0.9051\nEpoch 255\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2663 - acc: 0.9235 - val_loss: 0.3307 - val_acc: 0.9052\nEpoch 256\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.2659 - acc: 0.9233 - val_loss: 0.3314 - val_acc: 0.9050\nEpoch 257\/600\n\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2656 - acc: 0.9235 - val_loss: 0.3311 - val_acc: 0.9053\nEpoch 258\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2652 - acc: 0.9238 - val_loss: 0.3312 - val_acc: 0.9051\nEpoch 259\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2648 - acc: 0.9241 - val_loss: 0.3315 - val_acc: 0.9049\nEpoch 260\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.2644 - acc: 0.9239 - val_loss: 0.3304 - val_acc: 0.9055\nEpoch 261\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2641 - acc: 0.9240 - val_loss: 0.3296 - val_acc: 0.9056\nEpoch 262\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2638 - acc: 0.9242 - val_loss: 0.3305 - val_acc: 0.9055\nEpoch 263\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2635 - acc: 0.9242 - val_loss: 0.3295 - val_acc: 0.9058\nEpoch 264\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2633 - acc: 0.9243 - val_loss: 0.3284 - val_acc: 0.9060\nEpoch 265\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2627 - acc: 0.9245 - val_loss: 0.3290 - val_acc: 0.9058\nEpoch 266\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.2626 - acc: 0.9245 - val_loss: 0.3289 - val_acc: 0.9058\nEpoch 267\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2622 - acc: 0.9246 - val_loss: 0.3294 - val_acc: 0.9058\nEpoch 268\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.2618 - acc: 0.9247 - val_loss: 0.3281 - val_acc: 0.9060\nEpoch 269\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2615 - acc: 0.9247 - val_loss: 0.3278 - val_acc: 0.9061\nEpoch 270\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.2611 - acc: 0.9250 - val_loss: 0.3271 - val_acc: 0.9062\nEpoch 271\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2609 - acc: 0.9252 - val_loss: 0.3277 - val_acc: 0.9062\nEpoch 272\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2603 - acc: 0.9251 - val_loss: 0.3272 - val_acc: 0.9061\nEpoch 273\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.2600 - acc: 0.9253 - val_loss: 0.3277 - val_acc: 0.9062\nEpoch 274\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2598 - acc: 0.9254 - val_loss: 0.3273 - val_acc: 0.9063\nEpoch 275\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2595 - acc: 0.9254 - val_loss: 0.3270 - val_acc: 0.9067\nEpoch 276\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.2592 - acc: 0.9255 - val_loss: 0.3259 - val_acc: 0.9069\nEpoch 277\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2588 - acc: 0.9256 - val_loss: 0.3269 - val_acc: 0.9063\nEpoch 278\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2585 - acc: 0.9254 - val_loss: 0.3257 - val_acc: 0.9069\nEpoch 279\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2580 - acc: 0.9259 - val_loss: 0.3266 - val_acc: 0.9067\nEpoch 280\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2577 - acc: 0.9258 - val_loss: 0.3264 - val_acc: 0.9067\nEpoch 281\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2573 - acc: 0.9260 - val_loss: 0.3267 - val_acc: 0.9068\nEpoch 282\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2573 - acc: 0.9261 - val_loss: 0.3251 - val_acc: 0.9072\nEpoch 283\/600\n236594\/236594 [==============================] - 31s 129us\/sample - loss: 0.2569 - acc: 0.9263 - val_loss: 0.3251 - val_acc: 0.9068\nEpoch 284\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2565 - acc: 0.9266 - val_loss: 0.3250 - val_acc: 0.9069\nEpoch 285\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2561 - acc: 0.9265 - val_loss: 0.3249 - val_acc: 0.9068\nEpoch 286\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2558 - acc: 0.9265 - val_loss: 0.3248 - val_acc: 0.9071\nEpoch 287\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2556 - acc: 0.9268 - val_loss: 0.3244 - val_acc: 0.9072\nEpoch 288\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2553 - acc: 0.9266 - val_loss: 0.3242 - val_acc: 0.9071\nEpoch 289\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2549 - acc: 0.9268 - val_loss: 0.3242 - val_acc: 0.9072\nEpoch 290\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2545 - acc: 0.9268 - val_loss: 0.3232 - val_acc: 0.9073\nEpoch 291\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2543 - acc: 0.9269 - val_loss: 0.3232 - val_acc: 0.9074\nEpoch 292\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2541 - acc: 0.9271 - val_loss: 0.3238 - val_acc: 0.9071\nEpoch 293\/600\n236594\/236594 [==============================] - 31s 130us\/sample - loss: 0.2537 - acc: 0.9271 - val_loss: 0.3236 - val_acc: 0.9073\nEpoch 294\/600\n236594\/236594 [==============================] - 31s 129us\/sample - loss: 0.2534 - acc: 0.9270 - val_loss: 0.3240 - val_acc: 0.9071\nEpoch 295\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2530 - acc: 0.9273 - val_loss: 0.3226 - val_acc: 0.9075\nEpoch 296\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2526 - acc: 0.9275 - val_loss: 0.3231 - val_acc: 0.9077\nEpoch 297\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2523 - acc: 0.9272 - val_loss: 0.3228 - val_acc: 0.9076\nEpoch 298\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2522 - acc: 0.9275 - val_loss: 0.3231 - val_acc: 0.9076\nEpoch 299\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2519 - acc: 0.9276 - val_loss: 0.3219 - val_acc: 0.9077\nEpoch 300\/600\n236594\/236594 [==============================] - 31s 131us\/sample - loss: 0.2516 - acc: 0.9279 - val_loss: 0.3214 - val_acc: 0.9079\nEpoch 301\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2511 - acc: 0.9277 - val_loss: 0.3222 - val_acc: 0.9078\nEpoch 302\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2509 - acc: 0.9278 - val_loss: 0.3228 - val_acc: 0.9076\nEpoch 303\/600\n236594\/236594 [==============================] - 31s 130us\/sample - loss: 0.2505 - acc: 0.9280 - val_loss: 0.3207 - val_acc: 0.9081\nEpoch 304\/600\n236594\/236594 [==============================] - 31s 129us\/sample - loss: 0.2502 - acc: 0.9281 - val_loss: 0.3212 - val_acc: 0.9081\nEpoch 305\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2500 - acc: 0.9283 - val_loss: 0.3212 - val_acc: 0.9080\nEpoch 306\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2498 - acc: 0.9280 - val_loss: 0.3219 - val_acc: 0.9081\nEpoch 307\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2494 - acc: 0.9282 - val_loss: 0.3212 - val_acc: 0.9080\nEpoch 308\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2491 - acc: 0.9284 - val_loss: 0.3200 - val_acc: 0.9084\nEpoch 309\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2487 - acc: 0.9283 - val_loss: 0.3209 - val_acc: 0.9084\nEpoch 310\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2485 - acc: 0.9288 - val_loss: 0.3199 - val_acc: 0.9086\nEpoch 311\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2482 - acc: 0.9286 - val_loss: 0.3210 - val_acc: 0.9084\nEpoch 312\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2481 - acc: 0.9285 - val_loss: 0.3210 - val_acc: 0.9079\nEpoch 313\/600\n\n236594\/236594 [==============================] - 31s 130us\/sample - loss: 0.2475 - acc: 0.9289 - val_loss: 0.3203 - val_acc: 0.9082\nEpoch 314\/600\n236594\/236594 [==============================] - 31s 130us\/sample - loss: 0.2475 - acc: 0.9290 - val_loss: 0.3206 - val_acc: 0.9085\nEpoch 315\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2471 - acc: 0.9292 - val_loss: 0.3199 - val_acc: 0.9083\nEpoch 316\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2470 - acc: 0.9292 - val_loss: 0.3193 - val_acc: 0.9087\nEpoch 317\/600\n236594\/236594 [==============================] - 31s 130us\/sample - loss: 0.2468 - acc: 0.9293 - val_loss: 0.3195 - val_acc: 0.9086\nEpoch 318\/600\n236594\/236594 [==============================] - 31s 129us\/sample - loss: 0.2462 - acc: 0.9293 - val_loss: 0.3196 - val_acc: 0.9082\nEpoch 319\/600\n236594\/236594 [==============================] - 30s 129us\/sample - loss: 0.2461 - acc: 0.9293 - val_loss: 0.3185 - val_acc: 0.9086\nEpoch 320\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2458 - acc: 0.9296 - val_loss: 0.3187 - val_acc: 0.9087\nEpoch 321\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2456 - acc: 0.9296 - val_loss: 0.3190 - val_acc: 0.9087\nEpoch 322\/600\n236594\/236594 [==============================] - 30s 129us\/sample - loss: 0.2452 - acc: 0.9294 - val_loss: 0.3182 - val_acc: 0.9089\nEpoch 323\/600\n236594\/236594 [==============================] - 31s 129us\/sample - loss: 0.2450 - acc: 0.9298 - val_loss: 0.3193 - val_acc: 0.9089\nEpoch 324\/600\n236594\/236594 [==============================] - 31s 132us\/sample - loss: 0.2447 - acc: 0.9297 - val_loss: 0.3197 - val_acc: 0.9083\nEpoch 325\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2445 - acc: 0.9298 - val_loss: 0.3187 - val_acc: 0.9085\nEpoch 326\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2441 - acc: 0.9300 - val_loss: 0.3176 - val_acc: 0.9089\nEpoch 327\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2438 - acc: 0.9300 - val_loss: 0.3182 - val_acc: 0.9090\nEpoch 328\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2436 - acc: 0.9301 - val_loss: 0.3173 - val_acc: 0.9092\nEpoch 329\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2433 - acc: 0.9303 - val_loss: 0.3179 - val_acc: 0.9089\nEpoch 330\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2430 - acc: 0.9300 - val_loss: 0.3180 - val_acc: 0.9087\nEpoch 331\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2429 - acc: 0.9302 - val_loss: 0.3174 - val_acc: 0.9092\nEpoch 332\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2425 - acc: 0.9304 - val_loss: 0.3176 - val_acc: 0.9090\nEpoch 333\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2422 - acc: 0.9303 - val_loss: 0.3170 - val_acc: 0.9093\nEpoch 334\/600\n236594\/236594 [==============================] - 31s 130us\/sample - loss: 0.2419 - acc: 0.9305 - val_loss: 0.3173 - val_acc: 0.9090\nEpoch 335\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2417 - acc: 0.9301 - val_loss: 0.3169 - val_acc: 0.9093\nEpoch 336\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2414 - acc: 0.9306 - val_loss: 0.3171 - val_acc: 0.9089\nEpoch 337\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2412 - acc: 0.9306 - val_loss: 0.3173 - val_acc: 0.9090\nEpoch 338\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2410 - acc: 0.9306 - val_loss: 0.3165 - val_acc: 0.9093\nEpoch 339\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2407 - acc: 0.9310 - val_loss: 0.3164 - val_acc: 0.9095\nEpoch 340\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2404 - acc: 0.9310 - val_loss: 0.3161 - val_acc: 0.9092\nEpoch 341\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2399 - acc: 0.9311 - val_loss: 0.3150 - val_acc: 0.9094\nEpoch 342\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2400 - acc: 0.9311 - val_loss: 0.3166 - val_acc: 0.9093\nEpoch 343\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2395 - acc: 0.9311 - val_loss: 0.3157 - val_acc: 0.9094\nEpoch 344\/600\n236594\/236594 [==============================] - 31s 130us\/sample - loss: 0.2392 - acc: 0.9314 - val_loss: 0.3171 - val_acc: 0.9092\nEpoch 345\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2391 - acc: 0.9314 - val_loss: 0.3150 - val_acc: 0.9095\nEpoch 346\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2389 - acc: 0.9314 - val_loss: 0.3150 - val_acc: 0.9095\nEpoch 347\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2388 - acc: 0.9314 - val_loss: 0.3154 - val_acc: 0.9094\nEpoch 348\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2385 - acc: 0.9315 - val_loss: 0.3155 - val_acc: 0.9094\nEpoch 349\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2382 - acc: 0.9316 - val_loss: 0.3161 - val_acc: 0.9094\nEpoch 350\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2378 - acc: 0.9318 - val_loss: 0.3160 - val_acc: 0.9095\nEpoch 351\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2377 - acc: 0.9318 - val_loss: 0.3154 - val_acc: 0.9096\nEpoch 352\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2373 - acc: 0.9318 - val_loss: 0.3146 - val_acc: 0.9098\nEpoch 353\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2370 - acc: 0.9321 - val_loss: 0.3143 - val_acc: 0.9097\nEpoch 354\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2369 - acc: 0.9318 - val_loss: 0.3148 - val_acc: 0.9096\nEpoch 355\/600\n236594\/236594 [==============================] - 31s 130us\/sample - loss: 0.2366 - acc: 0.9320 - val_loss: 0.3143 - val_acc: 0.9099\nEpoch 356\/600\n236594\/236594 [==============================] - 31s 131us\/sample - loss: 0.2365 - acc: 0.9320 - val_loss: 0.3145 - val_acc: 0.9095\nEpoch 357\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2360 - acc: 0.9323 - val_loss: 0.3133 - val_acc: 0.9099\nEpoch 358\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2357 - acc: 0.9321 - val_loss: 0.3153 - val_acc: 0.9097\nEpoch 359\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2355 - acc: 0.9320 - val_loss: 0.3135 - val_acc: 0.9099\nEpoch 360\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2353 - acc: 0.9324 - val_loss: 0.3145 - val_acc: 0.9098\nEpoch 361\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2352 - acc: 0.9324 - val_loss: 0.3142 - val_acc: 0.9097\nEpoch 362\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2350 - acc: 0.9326 - val_loss: 0.3141 - val_acc: 0.9099\nEpoch 363\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2347 - acc: 0.9327 - val_loss: 0.3140 - val_acc: 0.9098\nEpoch 364\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2342 - acc: 0.9326 - val_loss: 0.3133 - val_acc: 0.9102\nEpoch 365\/600\n236594\/236594 [==============================] - 31s 131us\/sample - loss: 0.2341 - acc: 0.9327 - val_loss: 0.3142 - val_acc: 0.9097\nEpoch 366\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2338 - acc: 0.9328 - val_loss: 0.3131 - val_acc: 0.9100\nEpoch 367\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2337 - acc: 0.9329 - val_loss: 0.3135 - val_acc: 0.9099\nEpoch 368\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2333 - acc: 0.9328 - val_loss: 0.3122 - val_acc: 0.9103\nEpoch 369\/600\n\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2332 - acc: 0.9330 - val_loss: 0.3129 - val_acc: 0.9098\nEpoch 370\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2329 - acc: 0.9330 - val_loss: 0.3126 - val_acc: 0.9102\nEpoch 371\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2327 - acc: 0.9335 - val_loss: 0.3125 - val_acc: 0.9100\nEpoch 372\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2325 - acc: 0.9332 - val_loss: 0.3131 - val_acc: 0.9101\nEpoch 373\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2323 - acc: 0.9332 - val_loss: 0.3128 - val_acc: 0.9101\nEpoch 374\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2321 - acc: 0.9335 - val_loss: 0.3114 - val_acc: 0.9103\nEpoch 375\/600\n236594\/236594 [==============================] - 31s 132us\/sample - loss: 0.2318 - acc: 0.9335 - val_loss: 0.3137 - val_acc: 0.9099\nEpoch 376\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2314 - acc: 0.9335 - val_loss: 0.3131 - val_acc: 0.9101\nEpoch 377\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2313 - acc: 0.9339 - val_loss: 0.3115 - val_acc: 0.9104\nEpoch 378\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2311 - acc: 0.9337 - val_loss: 0.3119 - val_acc: 0.9104\nEpoch 379\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2309 - acc: 0.9338 - val_loss: 0.3118 - val_acc: 0.9105\nEpoch 380\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2308 - acc: 0.9335 - val_loss: 0.3110 - val_acc: 0.9105\nEpoch 381\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2304 - acc: 0.9339 - val_loss: 0.3120 - val_acc: 0.9107\nEpoch 382\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2304 - acc: 0.9337 - val_loss: 0.3117 - val_acc: 0.9102\nEpoch 383\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2299 - acc: 0.9339 - val_loss: 0.3121 - val_acc: 0.9105\nEpoch 384\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2298 - acc: 0.9339 - val_loss: 0.3114 - val_acc: 0.9105\nEpoch 385\/600\n236594\/236594 [==============================] - 31s 131us\/sample - loss: 0.2296 - acc: 0.9339 - val_loss: 0.3116 - val_acc: 0.9107\nEpoch 386\/600\n236594\/236594 [==============================] - 31s 130us\/sample - loss: 0.2293 - acc: 0.9342 - val_loss: 0.3113 - val_acc: 0.9107\nEpoch 387\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2290 - acc: 0.9341 - val_loss: 0.3118 - val_acc: 0.9104\nEpoch 388\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2287 - acc: 0.9346 - val_loss: 0.3106 - val_acc: 0.9103\nEpoch 389\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2284 - acc: 0.9344 - val_loss: 0.3106 - val_acc: 0.9107\nEpoch 390\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2283 - acc: 0.9342 - val_loss: 0.3105 - val_acc: 0.9107\nEpoch 391\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2281 - acc: 0.9345 - val_loss: 0.3104 - val_acc: 0.9108\nEpoch 392\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2279 - acc: 0.9346 - val_loss: 0.3111 - val_acc: 0.9106\nEpoch 393\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2277 - acc: 0.9347 - val_loss: 0.3097 - val_acc: 0.9110\nEpoch 394\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2275 - acc: 0.9345 - val_loss: 0.3103 - val_acc: 0.9107\nEpoch 395\/600\n236594\/236594 [==============================] - 31s 130us\/sample - loss: 0.2273 - acc: 0.9345 - val_loss: 0.3103 - val_acc: 0.9110\nEpoch 396\/600\n236594\/236594 [==============================] - 31s 130us\/sample - loss: 0.2271 - acc: 0.9347 - val_loss: 0.3102 - val_acc: 0.9109\nEpoch 397\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2266 - acc: 0.9349 - val_loss: 0.3087 - val_acc: 0.9111\nEpoch 398\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2265 - acc: 0.9352 - val_loss: 0.3098 - val_acc: 0.9108\nEpoch 399\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2264 - acc: 0.9351 - val_loss: 0.3096 - val_acc: 0.9108\nEpoch 400\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2262 - acc: 0.9351 - val_loss: 0.3091 - val_acc: 0.9113\nEpoch 401\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2260 - acc: 0.9352 - val_loss: 0.3106 - val_acc: 0.9105\nEpoch 402\/600\n236594\/236594 [==============================] - 30s 129us\/sample - loss: 0.2258 - acc: 0.9352 - val_loss: 0.3094 - val_acc: 0.9111\nEpoch 403\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2254 - acc: 0.9354 - val_loss: 0.3095 - val_acc: 0.9109\nEpoch 404\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2254 - acc: 0.9352 - val_loss: 0.3084 - val_acc: 0.9111\nEpoch 405\/600\n236594\/236594 [==============================] - 31s 129us\/sample - loss: 0.2251 - acc: 0.9352 - val_loss: 0.3090 - val_acc: 0.9112\nEpoch 406\/600\n236594\/236594 [==============================] - 31s 130us\/sample - loss: 0.2248 - acc: 0.9357 - val_loss: 0.3103 - val_acc: 0.9108\nEpoch 407\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2246 - acc: 0.9355 - val_loss: 0.3084 - val_acc: 0.9112\nEpoch 408\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2246 - acc: 0.9356 - val_loss: 0.3090 - val_acc: 0.9111\nEpoch 409\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2242 - acc: 0.9356 - val_loss: 0.3086 - val_acc: 0.9111\nEpoch 410\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2240 - acc: 0.9359 - val_loss: 0.3098 - val_acc: 0.9112\nEpoch 411\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2239 - acc: 0.9359 - val_loss: 0.3085 - val_acc: 0.9113\nEpoch 412\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2234 - acc: 0.9359 - val_loss: 0.3075 - val_acc: 0.9116\nEpoch 413\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2234 - acc: 0.9358 - val_loss: 0.3088 - val_acc: 0.9111\nEpoch 414\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2232 - acc: 0.9359 - val_loss: 0.3091 - val_acc: 0.9110\nEpoch 415\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2231 - acc: 0.9359 - val_loss: 0.3085 - val_acc: 0.9112\nEpoch 416\/600\n236594\/236594 [==============================] - 30s 129us\/sample - loss: 0.2228 - acc: 0.9361 - val_loss: 0.3082 - val_acc: 0.9112\nEpoch 417\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2226 - acc: 0.9362 - val_loss: 0.3085 - val_acc: 0.9114\nEpoch 418\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2224 - acc: 0.9362 - val_loss: 0.3087 - val_acc: 0.9111\nEpoch 419\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2223 - acc: 0.9363 - val_loss: 0.3077 - val_acc: 0.9116\nEpoch 420\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2220 - acc: 0.9363 - val_loss: 0.3086 - val_acc: 0.9113\nEpoch 421\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2216 - acc: 0.9364 - val_loss: 0.3085 - val_acc: 0.9110\nEpoch 422\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2215 - acc: 0.9363 - val_loss: 0.3092 - val_acc: 0.9110\nEpoch 423\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2212 - acc: 0.9364 - val_loss: 0.3072 - val_acc: 0.9115\nEpoch 424\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2212 - acc: 0.9365 - val_loss: 0.3079 - val_acc: 0.9114\nEpoch 425\/600\n\n236594\/236594 [==============================] - 31s 129us\/sample - loss: 0.2209 - acc: 0.9367 - val_loss: 0.3080 - val_acc: 0.9114\nEpoch 426\/600\n236594\/236594 [==============================] - 30s 129us\/sample - loss: 0.2207 - acc: 0.9369 - val_loss: 0.3072 - val_acc: 0.9113\nEpoch 427\/600\n236594\/236594 [==============================] - 30s 129us\/sample - loss: 0.2204 - acc: 0.9368 - val_loss: 0.3074 - val_acc: 0.9115\nEpoch 428\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2205 - acc: 0.9367 - val_loss: 0.3073 - val_acc: 0.9115\nEpoch 429\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2199 - acc: 0.9369 - val_loss: 0.3087 - val_acc: 0.9113\nEpoch 430\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2198 - acc: 0.9371 - val_loss: 0.3078 - val_acc: 0.9111\nEpoch 431\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2197 - acc: 0.9369 - val_loss: 0.3075 - val_acc: 0.9110\nEpoch 432\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2197 - acc: 0.9372 - val_loss: 0.3073 - val_acc: 0.9111\nEpoch 433\/600\n236594\/236594 [==============================] - 31s 129us\/sample - loss: 0.2195 - acc: 0.9369 - val_loss: 0.3071 - val_acc: 0.9113\nEpoch 434\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2191 - acc: 0.9374 - val_loss: 0.3065 - val_acc: 0.9119\nEpoch 435\/600\n236594\/236594 [==============================] - 31s 129us\/sample - loss: 0.2189 - acc: 0.9373 - val_loss: 0.3067 - val_acc: 0.9116\nEpoch 436\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2187 - acc: 0.9371 - val_loss: 0.3073 - val_acc: 0.9114\nEpoch 437\/600\n236594\/236594 [==============================] - 31s 129us\/sample - loss: 0.2184 - acc: 0.9374 - val_loss: 0.3074 - val_acc: 0.9114\nEpoch 438\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2183 - acc: 0.9373 - val_loss: 0.3059 - val_acc: 0.9117\nEpoch 439\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2180 - acc: 0.9374 - val_loss: 0.3063 - val_acc: 0.9115\nEpoch 440\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2180 - acc: 0.9371 - val_loss: 0.3068 - val_acc: 0.9117\nEpoch 441\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2178 - acc: 0.9379 - val_loss: 0.3065 - val_acc: 0.9116\nEpoch 442\/600\n236594\/236594 [==============================] - 30s 129us\/sample - loss: 0.2177 - acc: 0.9377 - val_loss: 0.3060 - val_acc: 0.9117\nEpoch 443\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2172 - acc: 0.9378 - val_loss: 0.3065 - val_acc: 0.9118\nEpoch 444\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2173 - acc: 0.9378 - val_loss: 0.3065 - val_acc: 0.9118\nEpoch 445\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2169 - acc: 0.9380 - val_loss: 0.3055 - val_acc: 0.9120\nEpoch 446\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2168 - acc: 0.9378 - val_loss: 0.3066 - val_acc: 0.9120\nEpoch 447\/600\n236594\/236594 [==============================] - 31s 130us\/sample - loss: 0.2166 - acc: 0.9380 - val_loss: 0.3059 - val_acc: 0.9117\nEpoch 448\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2162 - acc: 0.9382 - val_loss: 0.3060 - val_acc: 0.9116\nEpoch 449\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2161 - acc: 0.9382 - val_loss: 0.3064 - val_acc: 0.9115\nEpoch 450\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2159 - acc: 0.9382 - val_loss: 0.3061 - val_acc: 0.9118\nEpoch 451\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2155 - acc: 0.9382 - val_loss: 0.3050 - val_acc: 0.9122\nEpoch 452\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2155 - acc: 0.9381 - val_loss: 0.3065 - val_acc: 0.9119\nEpoch 453\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2154 - acc: 0.9382 - val_loss: 0.3057 - val_acc: 0.9121\nEpoch 454\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2152 - acc: 0.9382 - val_loss: 0.3052 - val_acc: 0.9121\nEpoch 455\/600\n236594\/236594 [==============================] - 31s 129us\/sample - loss: 0.2150 - acc: 0.9385 - val_loss: 0.3058 - val_acc: 0.9116\nEpoch 456\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2150 - acc: 0.9387 - val_loss: 0.3052 - val_acc: 0.9120\nEpoch 457\/600\n236594\/236594 [==============================] - 31s 129us\/sample - loss: 0.2145 - acc: 0.9387 - val_loss: 0.3056 - val_acc: 0.9118\nEpoch 458\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2143 - acc: 0.9388 - val_loss: 0.3052 - val_acc: 0.9121\nEpoch 459\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2142 - acc: 0.9386 - val_loss: 0.3048 - val_acc: 0.9122\nEpoch 460\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2140 - acc: 0.9389 - val_loss: 0.3052 - val_acc: 0.9121\nEpoch 461\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2137 - acc: 0.9389 - val_loss: 0.3049 - val_acc: 0.9119\nEpoch 462\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2137 - acc: 0.9388 - val_loss: 0.3045 - val_acc: 0.9122\nEpoch 463\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2135 - acc: 0.9389 - val_loss: 0.3055 - val_acc: 0.9121\nEpoch 464\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2133 - acc: 0.9392 - val_loss: 0.3053 - val_acc: 0.9121\nEpoch 465\/600\n236594\/236594 [==============================] - 31s 129us\/sample - loss: 0.2131 - acc: 0.9393 - val_loss: 0.3053 - val_acc: 0.9118\nEpoch 466\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2131 - acc: 0.9391 - val_loss: 0.3048 - val_acc: 0.9123\nEpoch 467\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2128 - acc: 0.9393 - val_loss: 0.3048 - val_acc: 0.9121\nEpoch 468\/600\n236594\/236594 [==============================] - 31s 130us\/sample - loss: 0.2127 - acc: 0.9392 - val_loss: 0.3051 - val_acc: 0.9120\nEpoch 469\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2124 - acc: 0.9393 - val_loss: 0.3050 - val_acc: 0.9120\nEpoch 470\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2124 - acc: 0.9395 - val_loss: 0.3038 - val_acc: 0.9126\nEpoch 471\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2121 - acc: 0.9395 - val_loss: 0.3046 - val_acc: 0.9122\nEpoch 472\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2118 - acc: 0.9393 - val_loss: 0.3047 - val_acc: 0.9122\nEpoch 473\/600\n236594\/236594 [==============================] - 31s 130us\/sample - loss: 0.2118 - acc: 0.9395 - val_loss: 0.3049 - val_acc: 0.9120\nEpoch 474\/600\n236594\/236594 [==============================] - 30s 129us\/sample - loss: 0.2114 - acc: 0.9395 - val_loss: 0.3042 - val_acc: 0.9124\nEpoch 475\/600\n236594\/236594 [==============================] - 31s 130us\/sample - loss: 0.2113 - acc: 0.9395 - val_loss: 0.3046 - val_acc: 0.9120\nEpoch 476\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2110 - acc: 0.9396 - val_loss: 0.3046 - val_acc: 0.9120\nEpoch 477\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2110 - acc: 0.9395 - val_loss: 0.3045 - val_acc: 0.9123\nEpoch 478\/600\n236594\/236594 [==============================] - 31s 130us\/sample - loss: 0.2108 - acc: 0.9398 - val_loss: 0.3040 - val_acc: 0.9125\nEpoch 479\/600\n236594\/236594 [==============================] - 30s 129us\/sample - loss: 0.2105 - acc: 0.9399 - val_loss: 0.3047 - val_acc: 0.9123\nEpoch 480\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2106 - acc: 0.9397 - val_loss: 0.3041 - val_acc: 0.9123\nEpoch 481\/600\n\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2103 - acc: 0.9398 - val_loss: 0.3042 - val_acc: 0.9124\nEpoch 482\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2100 - acc: 0.9400 - val_loss: 0.3047 - val_acc: 0.9122\nEpoch 483\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2099 - acc: 0.9398 - val_loss: 0.3048 - val_acc: 0.9121\nEpoch 484\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2097 - acc: 0.9400 - val_loss: 0.3045 - val_acc: 0.9123\nEpoch 485\/600\n236594\/236594 [==============================] - 31s 130us\/sample - loss: 0.2093 - acc: 0.9401 - val_loss: 0.3034 - val_acc: 0.9126\nEpoch 486\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2092 - acc: 0.9404 - val_loss: 0.3030 - val_acc: 0.9129\nEpoch 487\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2092 - acc: 0.9403 - val_loss: 0.3046 - val_acc: 0.9124\nEpoch 488\/600\n236594\/236594 [==============================] - 31s 130us\/sample - loss: 0.2091 - acc: 0.9400 - val_loss: 0.3040 - val_acc: 0.9125\nEpoch 489\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2088 - acc: 0.9403 - val_loss: 0.3036 - val_acc: 0.9127\nEpoch 490\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2086 - acc: 0.9402 - val_loss: 0.3031 - val_acc: 0.9128\nEpoch 491\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2086 - acc: 0.9405 - val_loss: 0.3034 - val_acc: 0.9125\nEpoch 492\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2083 - acc: 0.9405 - val_loss: 0.3046 - val_acc: 0.9122\nEpoch 493\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2081 - acc: 0.9403 - val_loss: 0.3040 - val_acc: 0.9124\nEpoch 494\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2080 - acc: 0.9407 - val_loss: 0.3038 - val_acc: 0.9128\nEpoch 495\/600\n236594\/236594 [==============================] - 31s 131us\/sample - loss: 0.2077 - acc: 0.9407 - val_loss: 0.3024 - val_acc: 0.9130\nEpoch 496\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2075 - acc: 0.9406 - val_loss: 0.3040 - val_acc: 0.9127\nEpoch 497\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2074 - acc: 0.9406 - val_loss: 0.3038 - val_acc: 0.9125\nEpoch 498\/600\n236594\/236594 [==============================] - 30s 129us\/sample - loss: 0.2072 - acc: 0.9408 - val_loss: 0.3033 - val_acc: 0.9130\nEpoch 499\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2070 - acc: 0.9409 - val_loss: 0.3032 - val_acc: 0.9123\nEpoch 500\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2068 - acc: 0.9410 - val_loss: 0.3030 - val_acc: 0.9128\nEpoch 501\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2068 - acc: 0.9410 - val_loss: 0.3027 - val_acc: 0.9130\nEpoch 502\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2066 - acc: 0.9411 - val_loss: 0.3029 - val_acc: 0.9128\nEpoch 503\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2061 - acc: 0.9412 - val_loss: 0.3026 - val_acc: 0.9127\nEpoch 504\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2062 - acc: 0.9413 - val_loss: 0.3027 - val_acc: 0.9128\nEpoch 505\/600\n236594\/236594 [==============================] - 31s 132us\/sample - loss: 0.2060 - acc: 0.9413 - val_loss: 0.3030 - val_acc: 0.9127\nEpoch 506\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2059 - acc: 0.9412 - val_loss: 0.3028 - val_acc: 0.9129\nEpoch 507\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2057 - acc: 0.9412 - val_loss: 0.3033 - val_acc: 0.9128\nEpoch 508\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2055 - acc: 0.9413 - val_loss: 0.3025 - val_acc: 0.9130\nEpoch 509\/600\n236594\/236594 [==============================] - 30s 129us\/sample - loss: 0.2052 - acc: 0.9414 - val_loss: 0.3031 - val_acc: 0.9125\nEpoch 510\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2051 - acc: 0.9416 - val_loss: 0.3026 - val_acc: 0.9128\nEpoch 511\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2050 - acc: 0.9417 - val_loss: 0.3022 - val_acc: 0.9132\nEpoch 512\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2048 - acc: 0.9416 - val_loss: 0.3030 - val_acc: 0.9128\nEpoch 513\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2048 - acc: 0.9416 - val_loss: 0.3031 - val_acc: 0.9126\nEpoch 514\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2045 - acc: 0.9415 - val_loss: 0.3026 - val_acc: 0.9127\nEpoch 515\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2042 - acc: 0.9417 - val_loss: 0.3029 - val_acc: 0.9129\nEpoch 516\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2043 - acc: 0.9416 - val_loss: 0.3028 - val_acc: 0.9130\nEpoch 517\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2039 - acc: 0.9418 - val_loss: 0.3026 - val_acc: 0.9130\nEpoch 518\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2039 - acc: 0.9420 - val_loss: 0.3021 - val_acc: 0.9130\nEpoch 519\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2036 - acc: 0.9418 - val_loss: 0.3027 - val_acc: 0.9130\nEpoch 520\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2035 - acc: 0.9418 - val_loss: 0.3026 - val_acc: 0.9130\nEpoch 521\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2032 - acc: 0.9422 - val_loss: 0.3030 - val_acc: 0.9127\nEpoch 522\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2031 - acc: 0.9423 - val_loss: 0.3022 - val_acc: 0.9130\nEpoch 523\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2029 - acc: 0.9421 - val_loss: 0.3020 - val_acc: 0.9128\nEpoch 524\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2029 - acc: 0.9422 - val_loss: 0.3023 - val_acc: 0.9128\nEpoch 525\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2027 - acc: 0.9422 - val_loss: 0.3024 - val_acc: 0.9130\nEpoch 526\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2025 - acc: 0.9422 - val_loss: 0.3011 - val_acc: 0.9135\nEpoch 527\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2023 - acc: 0.9424 - val_loss: 0.3021 - val_acc: 0.9130\nEpoch 528\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2022 - acc: 0.9423 - val_loss: 0.3020 - val_acc: 0.9130\nEpoch 529\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.2020 - acc: 0.9424 - val_loss: 0.3018 - val_acc: 0.9132\nEpoch 530\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2017 - acc: 0.9425 - val_loss: 0.3031 - val_acc: 0.9126\nEpoch 531\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2016 - acc: 0.9424 - val_loss: 0.3017 - val_acc: 0.9132\nEpoch 532\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2014 - acc: 0.9426 - val_loss: 0.3016 - val_acc: 0.9134\nEpoch 533\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2015 - acc: 0.9426 - val_loss: 0.3034 - val_acc: 0.9125\nEpoch 534\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2011 - acc: 0.9427 - val_loss: 0.3015 - val_acc: 0.9135\nEpoch 535\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2009 - acc: 0.9427 - val_loss: 0.3013 - val_acc: 0.9135\nEpoch 536\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2011 - acc: 0.9425 - val_loss: 0.3019 - val_acc: 0.9131\nEpoch 537\/600\n\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2007 - acc: 0.9426 - val_loss: 0.3016 - val_acc: 0.9131\nEpoch 538\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2005 - acc: 0.9430 - val_loss: 0.3019 - val_acc: 0.9131\nEpoch 539\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.2004 - acc: 0.9430 - val_loss: 0.3011 - val_acc: 0.9131\nEpoch 540\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.2003 - acc: 0.9429 - val_loss: 0.3008 - val_acc: 0.9134\nEpoch 541\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.2001 - acc: 0.9429 - val_loss: 0.3014 - val_acc: 0.9133\nEpoch 542\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1998 - acc: 0.9429 - val_loss: 0.3029 - val_acc: 0.9129\nEpoch 543\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1998 - acc: 0.9431 - val_loss: 0.3011 - val_acc: 0.9132\nEpoch 544\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1995 - acc: 0.9431 - val_loss: 0.3016 - val_acc: 0.9135\nEpoch 545\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1994 - acc: 0.9433 - val_loss: 0.3013 - val_acc: 0.9130\nEpoch 546\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.1993 - acc: 0.9433 - val_loss: 0.3012 - val_acc: 0.9134\nEpoch 547\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1990 - acc: 0.9434 - val_loss: 0.3011 - val_acc: 0.9132\nEpoch 548\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1990 - acc: 0.9432 - val_loss: 0.3036 - val_acc: 0.9124\nEpoch 549\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1987 - acc: 0.9435 - val_loss: 0.3002 - val_acc: 0.9135\nEpoch 550\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.1987 - acc: 0.9435 - val_loss: 0.3024 - val_acc: 0.9128\nEpoch 551\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1984 - acc: 0.9435 - val_loss: 0.3018 - val_acc: 0.9130\nEpoch 552\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1984 - acc: 0.9436 - val_loss: 0.3017 - val_acc: 0.9131\nEpoch 553\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1982 - acc: 0.9435 - val_loss: 0.3016 - val_acc: 0.9133\nEpoch 554\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.1980 - acc: 0.9435 - val_loss: 0.3014 - val_acc: 0.9128\nEpoch 555\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1979 - acc: 0.9437 - val_loss: 0.3018 - val_acc: 0.9131\nEpoch 556\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.1976 - acc: 0.9436 - val_loss: 0.3014 - val_acc: 0.9132\nEpoch 557\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.1976 - acc: 0.9436 - val_loss: 0.3020 - val_acc: 0.9131\nEpoch 558\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1974 - acc: 0.9438 - val_loss: 0.3019 - val_acc: 0.9132\nEpoch 559\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1970 - acc: 0.9437 - val_loss: 0.3023 - val_acc: 0.9127\nEpoch 560\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1970 - acc: 0.9440 - val_loss: 0.3011 - val_acc: 0.9133\nEpoch 561\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.1968 - acc: 0.9439 - val_loss: 0.3000 - val_acc: 0.9137\nEpoch 562\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1968 - acc: 0.9438 - val_loss: 0.3005 - val_acc: 0.9135\nEpoch 563\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.1966 - acc: 0.9439 - val_loss: 0.3011 - val_acc: 0.9131\nEpoch 564\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1962 - acc: 0.9441 - val_loss: 0.3012 - val_acc: 0.9135\nEpoch 565\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1962 - acc: 0.9440 - val_loss: 0.3009 - val_acc: 0.9133\nEpoch 566\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.1962 - acc: 0.9441 - val_loss: 0.3008 - val_acc: 0.9131\nEpoch 567\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1958 - acc: 0.9442 - val_loss: 0.3011 - val_acc: 0.9133\nEpoch 568\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.1958 - acc: 0.9443 - val_loss: 0.3013 - val_acc: 0.9132\nEpoch 569\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1956 - acc: 0.9441 - val_loss: 0.3017 - val_acc: 0.9132\nEpoch 570\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1956 - acc: 0.9447 - val_loss: 0.3015 - val_acc: 0.9132\nEpoch 571\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.1955 - acc: 0.9443 - val_loss: 0.3010 - val_acc: 0.9134\nEpoch 572\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1951 - acc: 0.9445 - val_loss: 0.3008 - val_acc: 0.9133\nEpoch 573\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1950 - acc: 0.9445 - val_loss: 0.3019 - val_acc: 0.9130\nEpoch 574\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.1950 - acc: 0.9445 - val_loss: 0.3012 - val_acc: 0.9133\nEpoch 575\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1947 - acc: 0.9445 - val_loss: 0.3003 - val_acc: 0.9135\nEpoch 576\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.1948 - acc: 0.9445 - val_loss: 0.2994 - val_acc: 0.9137\nEpoch 577\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1944 - acc: 0.9446 - val_loss: 0.3002 - val_acc: 0.9134\nEpoch 578\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1943 - acc: 0.9447 - val_loss: 0.3015 - val_acc: 0.9131\nEpoch 579\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1942 - acc: 0.9446 - val_loss: 0.3002 - val_acc: 0.9137\nEpoch 580\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1940 - acc: 0.9447 - val_loss: 0.3007 - val_acc: 0.9137\nEpoch 581\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.1939 - acc: 0.9448 - val_loss: 0.3002 - val_acc: 0.9136\nEpoch 582\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1937 - acc: 0.9449 - val_loss: 0.2999 - val_acc: 0.9139\nEpoch 583\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1934 - acc: 0.9450 - val_loss: 0.3003 - val_acc: 0.9135\nEpoch 584\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1934 - acc: 0.9450 - val_loss: 0.3000 - val_acc: 0.9137\nEpoch 585\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1931 - acc: 0.9450 - val_loss: 0.3010 - val_acc: 0.9133\nEpoch 586\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.1929 - acc: 0.9450 - val_loss: 0.3002 - val_acc: 0.9135\nEpoch 587\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1931 - acc: 0.9450 - val_loss: 0.2995 - val_acc: 0.9139\nEpoch 588\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1927 - acc: 0.9454 - val_loss: 0.3005 - val_acc: 0.9136\nEpoch 589\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1927 - acc: 0.9452 - val_loss: 0.2999 - val_acc: 0.9140\nEpoch 590\/600\n236594\/236594 [==============================] - 30s 125us\/sample - loss: 0.1925 - acc: 0.9449 - val_loss: 0.3011 - val_acc: 0.9134\nEpoch 591\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1924 - acc: 0.9453 - val_loss: 0.3000 - val_acc: 0.9139\nEpoch 592\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.1920 - acc: 0.9454 - val_loss: 0.3004 - val_acc: 0.9137\nEpoch 593\/600\n\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1921 - acc: 0.9455 - val_loss: 0.3002 - val_acc: 0.9137\nEpoch 594\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1919 - acc: 0.9453 - val_loss: 0.3006 - val_acc: 0.9135\nEpoch 595\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.1918 - acc: 0.9456 - val_loss: 0.3001 - val_acc: 0.9138\nEpoch 596\/600\n236594\/236594 [==============================] - 30s 128us\/sample - loss: 0.1915 - acc: 0.9455 - val_loss: 0.3006 - val_acc: 0.9136\nEpoch 597\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.1915 - acc: 0.9456 - val_loss: 0.3010 - val_acc: 0.9132\nEpoch 598\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.1914 - acc: 0.9455 - val_loss: 0.3003 - val_acc: 0.9137\nEpoch 599\/600\n236594\/236594 [==============================] - 30s 127us\/sample - loss: 0.1911 - acc: 0.9457 - val_loss: 0.2996 - val_acc: 0.9139\nEpoch 600\/600\n236594\/236594 [==============================] - 30s 126us\/sample - loss: 0.1910 - acc: 0.9456 - val_loss: 0.2998 - val_acc: 0.9139\n76123\/76123 [==============================] - 5s 60us\/sample - loss: 0.3035 - acc: 0.9127\n600\n35\n['loss', 'acc']\n[0.3034514742485666, 0.9126545]\ndict_keys(['loss', 'acc', 'val_loss', 'val_acc'])\n\n","d8b1a453":"                 Project\n* Data Source : Google quick draw dataset [https:\/\/github.com\/googlecreativelab\/quickdraw-dataset#get-the-data]\n* Objective : Create model to perform classfication on 12 random picked classes out of 365. [airplane,alarm clock,apple,butterfly,mug,mushroom,owl,palm tree,parachute,parrot,peanut,pencil]\n* Constraints: Data has been reduced to fit kaggle infra.\n* DataSize : Two datasets have been used the one with reduced size and full fledged data ( trimmed to fit kaggle infra)\n* Model trained on both datasets:  Images->  ( Train=143859 , validate=48028 , test=48028)  550k_12class ->( Train = 236594 ,validate = 76123,test =76123)\n\n                Data Preparation\n\n* Create image from strokes from *.ndjson files (details of strokes in obove link).\n* Convert images to arrays and save to files.\n* During image processing many records have been ommited for many reasons.\n* Processed files have can be found in dataset 550k_12class and Images.\n    \n                About network\n                \n * Many blank images were observed and removed from all sets of data.\n * Data has been mean normalized.\n * Many varient of cnn have been tried to achive maximum accuracy and min loss. \n * This model make use of multiple Branches CNN, two branch cnn were used and it gave better results.\n * Various optimizers have been tried, adadelta gave better results.\n * Image augmentation was tried and it increased efficiency, but is not used and is commented because of kaggle infra contraints, it take lot of time to do image augmentation and kaggle session dies after 2hr or before 600 epoch or if you are lucky it may let you run longer.\n * Model is compiled and model saved in output folder.\n * Two dataset have been added (Images-> with less samples) (550k_12class-> full dataset)\n\n\n                Results\n\n* 550k_12class ->( Train = 236594 ,validate = 76123,test =76123)\n\n  Epoch 600\/600\n \n* loss: 0.1910 - acc: 0.9456\n \n* val_loss: 0.2998 - val_acc: 0.9139\n \n* test_loss 0.3034514742485666- test_acc: 0.9126545\n \n After each epoch accuracy keep on increasing and loss keep on decreasing for both train and validation data.\n after 600 epoch kaggle session restarts automatically, results could improve more if run for few hundreds epochs.\n"}}