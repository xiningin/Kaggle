{"cell_type":{"b2599385":"code","55ae39a4":"code","3b4f7cad":"code","46744565":"code","05a63888":"code","f720142f":"code","76acc38c":"code","3827d345":"code","d9943402":"code","afa357f5":"code","d0ed90c3":"code","fdbb7ab7":"code","48ccf189":"code","64c3d4b3":"code","2e982fa2":"code","8f5d828e":"code","154beb01":"code","952bdfd3":"code","02340a11":"code","89117c6b":"code","e3adf705":"code","3d3dfa1b":"code","b2dc6950":"code","0dff7d05":"code","23a0a83f":"code","781e1d46":"code","1d414a6e":"code","6741d2b5":"code","9628b0ad":"code","6723b552":"code","6c43909a":"code","ba34aef6":"code","caef1328":"code","c86a0511":"code","a092024e":"code","1c54fddf":"code","6d4cbcde":"code","9424b06b":"code","b3dd6497":"code","2fba8772":"code","486f5a62":"code","aa9045fc":"code","0632f8f5":"code","cb1ff679":"code","82bbb197":"code","43553a5c":"code","eaafe99f":"code","d85c79ff":"code","d32a29d6":"code","59a3fd8a":"code","1f3173f9":"code","024e0e8b":"code","c9b4012c":"code","8011b818":"code","2f071779":"code","55665220":"code","6feb2700":"code","a5825d9e":"code","c2f749ac":"code","977768ff":"code","820fbdc8":"code","ffd97ece":"code","9304ec0e":"code","cb5642f9":"code","09940e8b":"code","a2580af7":"code","2064dfcf":"code","917da57b":"code","12c6cf2d":"code","70bee353":"code","dbd54f9d":"code","f43a6b98":"code","8ecc0731":"code","924dae80":"markdown","198effaa":"markdown","c1bfb3db":"markdown","e1cc787b":"markdown","8f8108ac":"markdown","81c7fb8b":"markdown","2cdeaab9":"markdown","ec40b0e0":"markdown","e8aad01c":"markdown","2566e73e":"markdown","ede87c39":"markdown","8904aba6":"markdown","49622564":"markdown","c8c071b0":"markdown","4efd0409":"markdown","d2a098f7":"markdown","43c50b0f":"markdown","60d780a9":"markdown","73ab7afd":"markdown","fb34f2eb":"markdown","342bc3b4":"markdown","f8499064":"markdown","8756be73":"markdown","55d34364":"markdown","8f1d1478":"markdown","067a9935":"markdown","75b04f67":"markdown","71a9d90d":"markdown","e5b1163b":"markdown","729b92d1":"markdown","e634c83d":"markdown","6597bc77":"markdown","baef0f83":"markdown","4bd361cc":"markdown","fbca25cd":"markdown","9b157337":"markdown","15be2fbf":"markdown","719642d2":"markdown","283d2400":"markdown","7a80bcbe":"markdown","4b85cc8a":"markdown","d14d6243":"markdown","da57540e":"markdown","da2e398f":"markdown","101a3b49":"markdown","0f665b3a":"markdown","c9b5a263":"markdown","7b183c20":"markdown","137e38fe":"markdown","15d99e0b":"markdown","61751455":"markdown","4163acd1":"markdown","51b15640":"markdown","9d879f40":"markdown","7c7dcc22":"markdown","647aaec5":"markdown","a953395d":"markdown","3d63e938":"markdown","510eb3b6":"markdown"},"source":{"b2599385":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport warnings\nimport sklearn\nimport gensim\nimport scipy\nimport numpy\nimport json\nimport nltk\nimport sys\nimport csv\nimport os","55ae39a4":"print('matplotlib: {}'.format(matplotlib.__version__))\nprint('scipy: {}'.format(scipy.__version__))\nprint('seaborn: {}'.format(sns.__version__))\nprint('pandas: {}'.format(pd.__version__))\nprint('numpy: {}'.format(np.__version__))\nprint('Python: {}'.format(sys.version))","3b4f7cad":"sns.set(style='white', context='notebook', palette='deep')\nwarnings.filterwarnings('ignore')\nsns.set_style('white')\n%matplotlib inline","46744565":"print(os.listdir(\"..\/input\/\"))","05a63888":"gendered_pronoun_df = pd.read_csv('..\/input\/test_stage_1.tsv', delimiter='\\t')","f720142f":"submission = pd.read_csv('..\/input\/sample_submission_stage_1.csv')","76acc38c":"gendered_pronoun_df.shape","3827d345":"submission.shape","d9943402":"gendered_pronoun_df.head()","afa357f5":"print(gendered_pronoun_df.Text.head())","d0ed90c3":"print(\"Shape of train set : \",gendered_pronoun_df.shape)\n","fdbb7ab7":"gendered_pronoun_df.columns","48ccf189":"print(gendered_pronoun_df.info())","64c3d4b3":"gendered_pronoun_df.isna().sum()","2e982fa2":"gendered_pronoun_df[\"num_words\"] = gendered_pronoun_df[\"Text\"].apply(lambda x: len(str(x).split()))","8f5d828e":"#MJ Bahmani\nprint('maximum of num_words in data_df',gendered_pronoun_df[\"num_words\"].max())\nprint('min of num_words in data_df',gendered_pronoun_df[\"num_words\"].min())","154beb01":"gendered_pronoun_df[\"num_unique_words\"] = gendered_pronoun_df[\"Text\"].apply(lambda x: len(set(str(x).split())))\nprint('maximum of num_unique_words in train',gendered_pronoun_df[\"num_unique_words\"].max())\nprint('mean of num_unique_words in data_df',gendered_pronoun_df[\"num_unique_words\"].mean())","952bdfd3":"gendered_pronoun_df[\"num_chars\"] = gendered_pronoun_df[\"Text\"].apply(lambda x: len(str(x)))\nprint('maximum of num_chars in data_df',gendered_pronoun_df[\"num_chars\"].max())","02340a11":"from nltk.corpus import stopwords\neng_stopwords = set(stopwords.words(\"english\"))","89117c6b":"gendered_pronoun_df[\"num_stopwords\"] = gendered_pronoun_df[\"Text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n\nprint('maximum of num_stopwords in data_df',gendered_pronoun_df[\"num_stopwords\"].max())","e3adf705":"import string\ngendered_pronoun_df[\"num_punctuations\"] =gendered_pronoun_df['Text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\nprint('maximum of num_punctuations in data_df',gendered_pronoun_df[\"num_punctuations\"].max())","3d3dfa1b":"gendered_pronoun_df[\"num_words_upper\"] = gendered_pronoun_df[\"Text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\nprint('maximum of num_words_upper in data_df',gendered_pronoun_df[\"num_words_upper\"].max())","b2dc6950":"print(gendered_pronoun_df.columns)\ngendered_pronoun_df.head(1)","0dff7d05":"pronoun=gendered_pronoun_df[\"Pronoun\"]","23a0a83f":"np.unique(pronoun)","781e1d46":"gendered_pronoun_df[\"Pronoun_binary\"] = gendered_pronoun_df[\"Pronoun\"]","1d414a6e":"gendered_pronoun_df[\"Pronoun_binary\"]=gendered_pronoun_df[\"Pronoun_binary\"].str.replace('He','0')\ngendered_pronoun_df[\"Pronoun_binary\"]=gendered_pronoun_df[\"Pronoun_binary\"].str.replace('he','0')\ngendered_pronoun_df[\"Pronoun_binary\"]=gendered_pronoun_df[\"Pronoun_binary\"].str.replace('she','1')\ngendered_pronoun_df[\"Pronoun_binary\"]=gendered_pronoun_df[\"Pronoun_binary\"].str.replace('She','1')\ngendered_pronoun_df[\"Pronoun_binary\"]=gendered_pronoun_df[\"Pronoun_binary\"].str.replace('His','2')\ngendered_pronoun_df[\"Pronoun_binary\"]=gendered_pronoun_df[\"Pronoun_binary\"].str.replace('his','2')\ngendered_pronoun_df[\"Pronoun_binary\"]=gendered_pronoun_df[\"Pronoun_binary\"].str.replace('him','3')\ngendered_pronoun_df[\"Pronoun_binary\"]=gendered_pronoun_df[\"Pronoun_binary\"].str.replace('her','4')\ngendered_pronoun_df[\"Pronoun_binary\"]=gendered_pronoun_df[\"Pronoun_binary\"].str.replace('Her','4')","6741d2b5":"from wordcloud import WordCloud as wc\nfrom nltk.corpus import stopwords\ndef generate_wordcloud(text): \n    wordcloud = wc(relative_scaling = 1.0,stopwords = eng_stopwords).generate(text)\n    fig,ax = plt.subplots(1,1,figsize=(10,10))\n    ax.imshow(wordcloud, interpolation='bilinear')\n    ax.axis(\"off\")\n    ax.margins(x=0, y=0)\n    plt.show()","9628b0ad":"from nltk.corpus import stopwords\neng_stopwords = set(stopwords.words(\"english\"))","6723b552":"text =\" \".join(gendered_pronoun_df.Text)\ngenerate_wordcloud(text)","6c43909a":"gendered_pronoun_df.hist();","ba34aef6":"pd.plotting.scatter_matrix(gendered_pronoun_df,figsize=(10,10))\nplt.figure();","caef1328":"sns.jointplot(x='Pronoun-offset',y='A-offset' ,data=gendered_pronoun_df, kind='reg')","c86a0511":"sns.swarmplot(x='Pronoun-offset',y='B-offset',data=gendered_pronoun_df);","a092024e":"sns.distplot(gendered_pronoun_df[\"Pronoun-offset\"])","1c54fddf":"sns.violinplot(data=gendered_pronoun_df,x=\"Pronoun_binary\", y=\"num_words\")","6d4cbcde":"from nltk.tokenize import sent_tokenize, word_tokenize","9424b06b":"gendered_pronoun_df.Text[0]","b3dd6497":"our_text=gendered_pronoun_df.Text[0]","2fba8772":"print(word_tokenize(our_text))","486f5a62":"from nltk.tokenize import sent_tokenize, word_tokenize\nprint(sent_tokenize(our_text))","aa9045fc":"from nltk.tokenize import sent_tokenize, word_tokenize\n \nphrases = sent_tokenize(our_text)\nwords = word_tokenize(our_text)\nprint(phrases)","0632f8f5":"print(words)","cb1ff679":"from nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords","82bbb197":" \n\nstopWords = set(stopwords.words('english'))\nwords = word_tokenize(our_text)\nwordsFiltered = []\n \nfor w in words:\n    if w not in stopWords:\n        wordsFiltered.append(w)\n \nprint(wordsFiltered)","43553a5c":"from nltk.corpus import stopwords\n","eaafe99f":"stopWords = set(stopwords.words('english'))\n","d85c79ff":"print(len(stopWords))\nprint(stopWords)","d32a29d6":"for w in words:\n    if w not in stopWords:\n        wordsFiltered.append(w)","59a3fd8a":"our_text=gendered_pronoun_df.Text[0]\n","1f3173f9":"from nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize","024e0e8b":"from nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\n\nps = PorterStemmer()\n \nfor word in our_text:\n    print(ps.stem(word))","c9b4012c":"import nltk\nfrom nltk.tokenize import PunktSentenceTokenizer\n \n\nsentences = nltk.sent_tokenize(our_text)   \nfor sent in sentences:\n    print(nltk.pos_tag(nltk.word_tokenize(sent)))","8011b818":"import nltk\nfrom nltk.corpus import state_union\nfrom nltk.tokenize import PunktSentenceTokenizer\n \n\nsentences = nltk.sent_tokenize(our_text)   \n \ndata = []\nfor sent in sentences:\n    data = data + nltk.pos_tag(nltk.word_tokenize(sent))\n \nfor word in data: \n    if 'NNP' in word[1]: \n        print(word)","2f071779":"from nltk.corpus import names\n \n# Load data and training \nnames = ([(name, 'male') for name in names.words('male.txt')] + \n\t [(name, 'female') for name in names.words('female.txt')])","55665220":"[(u'Aaron', 'male'), (u'Abbey', 'male'), (u'Abbie', 'male')]\n[(u'Zorana', 'female'), (u'Zorina', 'female'), (u'Zorine', 'female')]","6feb2700":"def gender_features(word): \n    return {'last_letter': word[-1]}","a5825d9e":"import nltk.classify.util\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.corpus import names\n \ndef gender_features(word): \n    return {'last_letter': word[-1]} \n \n# Load data and training \nnames = ([(name, 'male') for name in names.words('male.txt')] + \n\t [(name, 'female') for name in names.words('female.txt')])\n \nfeaturesets = [(gender_features(n), g) for (n,g) in names] \ntrain_set = featuresets\nclassifier = nltk.NaiveBayesClassifier.train(train_set) \n \n# Predict\nprint(classifier.classify(gender_features('Frank')))","c2f749ac":"# Predict, you can change name\nname = 'Sarah'\nprint(classifier.classify(gender_features(name)))","977768ff":"positive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]\nnegative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]\nneutral_vocab = [ 'movie','the','sound','was','is','actors','did','know','words','not' ]","820fbdc8":"def word_feats(words):\n    return dict([(word, True) for word in words])\n \npositive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\nnegative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\nneutral_features = [(word_feats(neu), 'neu') for neu in neutral_vocab]","ffd97ece":"train_set = negative_features + positive_features + neutral_features","9304ec0e":"import nltk.classify.util\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.corpus import names\n \ndef word_feats(words):\n    return dict([(word, True) for word in words])\n \npositive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]\nnegative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]\nneutral_vocab = [ 'movie','the','sound','was','is','actors','did','know','words','not' ]\n \npositive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\nnegative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\nneutral_features = [(word_feats(neu), 'neu') for neu in neutral_vocab]\n \ntrain_set = negative_features + positive_features + neutral_features\n \nclassifier = NaiveBayesClassifier.train(train_set) \n \n# Predict\nneg = 0\npos = 0\n##sentence = \"Awesome movie, I liked it\"\nour_text = our_text.lower()\nwords = our_text.split(' ')\nfor word in words:\n    classResult = classifier.classify( word_feats(word))\n    if classResult == 'neg':\n        neg = neg + 1\n    if classResult == 'pos':\n        pos = pos + 1\n \nprint('Positive: ' + str(float(pos)\/len(words)))\nprint('Negative: ' + str(float(neg)\/len(words)))","cb5642f9":"import spacy","09940e8b":"nlp = spacy.load('en')\ndoc = nlp(our_text)\ni=0\nfor token in doc:\n    i=i+1;\n    if i<20:\n        print('\"' + token.text + '\"')","a2580af7":"nlp = spacy.load('en')\ndoc=nlp(our_text)\ni=0\nfor sent in doc.sents:\n    i=i+1\n    print(i,' - ',sent)","2064dfcf":"doc = nlp( our_text)\nprint([(token.text, token.tag_) for token in doc])","917da57b":"doc = nlp(our_text)\nfor ent in doc.ents:\n    print(ent.text, ent.label_)","12c6cf2d":"from spacy import displacy\n \ndoc = nlp(our_text )\ndisplacy.render(doc, style='ent', jupyter=True)","70bee353":"from spacy import displacy\n \ndoc = nlp(our_text)\ndisplacy.render(doc, style='dep', jupyter=True, options={'distance': 90})","dbd54f9d":"import gensim","f43a6b98":"import gensim\nfrom gensim import corpora\nfrom pprint import pprint\n# How to create a dictionary from a list of sentences?\ndocuments = [\"The Saudis are preparing a report that will acknowledge that\", \n             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n             \"interrogation that went wrong, one that was intended to lead\", \n             \"to his abduction from Turkey, according to two sources.\"]\n\ndocuments_2 = [\"One source says the report will likely conclude that\", \n                \"the operation was carried out without clearance and\", \n                \"transparency and that those involved will be held\", \n                \"responsible. One of the sources acknowledged that the\", \n                \"report is still being prepared and cautioned that\", \n                \"things could change.\"]\n\n# Tokenize(split) the sentences into words\ntexts = [[text for text in doc.split()] for doc in documents]\n\n# Create dictionary\ndictionary = corpora.Dictionary(texts)\n\n# Get information about the dictionary\nprint(dictionary)","8ecc0731":"# Show the word to id map\nprint(dictionary.token2id)","924dae80":"This example classifies sentences according to the training set.","198effaa":" <a id=\"top\"><\/a> <br>\n## Notebook  Content\n1. [Introduction](#1)\n    1. [Import](#11)\n    1. [Version](#12)\n    1. [Setup](#13)\n    1. [Data set](#14)\n    1. [Gendered Pronoun Analysis](#15)\n        1. [Problem Feature](#151)\n        1. [Variables](#152)\n1. [NLTK](#2)\n    1. [Tokenizing sentences](#21)\n    1. [NLTK and arrays](#22)\n    1. [NLTK stop words](#23)\n    1. [NLTK \u2013 stemming](#24)\n    1. [NLTK speech tagging](#25)\n    1. [Natural Language Processing \u2013 prediction](#26)\n        1. [nlp prediction example](#261)\n    1. [nlp prediction example](#27)\n1. [spaCy](#3)\n    1. [Sentence detection](#31)\n    1. [Part Of Speech Tagging](#32)\n    1. [spaCy](#33)\n    1. [displaCy](#34)\n1. [Gensim](#4)\n1. [conclusion](#5)\n1. [References](#6)","c1bfb3db":"<img src='https:\/\/activewizards.com\/content\/blog\/Comparison_of_Python_NLP_libraries\/nlp-librares-python-prs-and-cons01.png'>","e1cc787b":"<a id=\"14\"><\/a> <br>\n## 1-4 Data set","8f8108ac":"If you want to give the name during runtime, change the last line to:\n\n","81c7fb8b":"And stem the words in the list using:","2cdeaab9":"## 1-5-4 Visualization","ec40b0e0":"We get a set of English stop words using the line:\n\n","e8aad01c":"<img src='http:\/\/s9.picofile.com\/file\/8351628176\/nlp.png' width=600 height=600 >\n<div style=\"text-align:center\">last update: <b>12\/02\/2019<\/b><\/div>\n\n\n>You are reading **10 Steps to Become a Data Scientist** and are now in the 8th step : \n\n1. [Leren Python](https:\/\/www.kaggle.com\/mjbahmani\/the-data-scientist-s-toolbox-tutorial-1)\n2. [Python Packages](https:\/\/www.kaggle.com\/mjbahmani\/the-data-scientist-s-toolbox-tutorial-2)\n3. [Mathematics and Linear Algebra](https:\/\/www.kaggle.com\/mjbahmani\/linear-algebra-for-data-scientists)\n4. <font color=\"red\">You are in the 4th step<\/font>\n5. [Big Data](https:\/\/www.kaggle.com\/mjbahmani\/a-data-science-framework-for-quora)\n6. [Data visualization](https:\/\/www.kaggle.com\/mjbahmani\/top-5-data-visualization-libraries-tutorial)\n7. [Data Cleaning](https:\/\/www.kaggle.com\/mjbahmani\/machine-learning-workflow-for-house-prices)\n8. [Tutorial-on-ensemble-learning](https:\/\/www.kaggle.com\/mjbahmani\/tutorial-on-ensemble-learning)\n9. [A Comprehensive ML  Workflow with Python](https:\/\/www.kaggle.com\/mjbahmani\/a-comprehensive-ml-workflow-with-python)\n10. [Deep Learning](https:\/\/www.kaggle.com\/mjbahmani\/top-5-deep-learning-frameworks-tutorial)\n\n\n\n---------------------------------------------------------------------\nyou can Fork and Run this kernel on Github:\n> ###### [ GitHub](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist)\n\n-------------------------------------------------------------------------------------------------------------\n\n **I hope you find this kernel helpful and some <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated**\n \n -----------","2566e73e":"We create a new list called wordsFiltered which contains all words which are not stop words.\nTo create it we iterate over the list of words and only add it if its not in the stopWords list.","ede87c39":"<a id=\"12\"><\/a> <br>\n## 1-2 Version","8904aba6":"The returned list stopWords contains 153 stop words on my computer.\nYou can view the length or contents of this array with the lines:","49622564":"<a id=\"261\"><\/a> <br>\n### 2-6-1 nlp prediction example\nGiven a name, the classifier will predict if it\u2019s a male or female.\n\nTo create our analysis program, we have several steps:\n\n1. Data preparation\n1. Feature extraction\n1. Training\n1. Prediction\n1. Data preparation\nThe first step is to prepare data. We use the names set included with nltk.","c8c071b0":"### Number of stopwords in the text","4efd0409":"<a id=\"14\"><\/a> <br>\n# Top 3 NLP Libraries Tutorial\n1. NLTK\n1. spaCy\n1. Gensim","d2a098f7":"Go to first step: [**Course Home Page**](https:\/\/www.kaggle.com\/mjbahmani\/10-steps-to-become-a-data-scientist)\n\nGo to next step : [**Mathematics and Linear Algebra**](https:\/\/www.kaggle.com\/mjbahmani\/linear-algebra-for-data-scientists)","43c50b0f":"<a id=\"152\"><\/a> <br>\n### 1-5-2  Variables\n\n1. ID - Unique identifier for an example (Matches to Id in output file format)\n1. Text - Text containing the ambiguous pronoun and two candidate names (about a paragraph in length)\n1. Pronoun - The target pronoun (text)\n1. Pronoun-offset The character offset of Pronoun in Text\n1. A - The first name candidate (text)\n1. A-offset - The character offset of name A in Text\n1. B - The second name candidate\n1. B-offset - The character offset of name B in Text\n1. URL - The URL of the source Wikipedia page for the example","60d780a9":"You can define your own set of tuples if you wish, its simply a list containing many tuples.\n\nFeature extraction\nBased on the dataset, we prepare our feature. The feature we will use is the last letter of a name:\nWe define a featureset using:","73ab7afd":"<a id=\"22\"><\/a> <br>\n## 2-2 NLTK and arrays\nIf you wish to you can store the words and sentences in arrays","fb34f2eb":"<a id=\"11\"><\/a> <br>\n##   1-1 Import","342bc3b4":"A module has been imported:\n\n","f8499064":"## 3-2 Part Of Speech Tagging","8756be73":"<a id=\"154\"><\/a> <br>\n## 1-5-4 some new features\nIn this section, I will extract a few new statistical features from the text field","55d34364":"We can filter this data based on the type of word:","8f1d1478":"Training and prediction\nWe train and predict using:","067a9935":"<a id=\"24\"><\/a> <br>\n## 2-4 NLTK \u2013 stemming\nStart by defining some words:","75b04f67":"We start by defining 3 classes: positive, negative and neutral.\nEach of these is defined by a vocabulary:","71a9d90d":"## 5- Comparison of Python NLP libraries by Activewizards","e5b1163b":">###### you may  be interested have a look at it: [**10-steps-to-become-a-data-scientist**](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist)\n\n\n---------------------------------------------------------------------\nyou can Fork and Run this kernel on Github:\n> ###### [ GitHub](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist)\n\n-------------------------------------------------------------------------------------------------------------\n\n **I hope you find this kernel helpful and some <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated**\n \n -----------","729b92d1":"<a id=\"27\"><\/a> <br>\n## 2-7 Python Sentiment Analysis\nIn Natural Language Processing there is a concept known as **Sentiment Analysis**.\n\n<img src='https:\/\/cdn-images-1.medium.com\/max\/600\/0*ga5rNPmVYBsCm-lz.'>\n[img-ref](https:\/\/medium.com\/@tomyuz\/a-sentiment-analysis-approach-to-predicting-stock-returns-d5ca8b75a42)\n\n1. Given a movie review or a tweet, it can be automatically classified in categories.\n1. These categories can be user defined (positive, negative) or whichever classes you want.\n1. Classification is done using several steps: training and prediction.\n1. The training phase needs to have training data, this is example data in which we define examples. \n1. The classifier will use the training data to make predictions.","e634c83d":"<a id=\"21\"><\/a> <br>\n\n\n\n## 2-1 Tokenizing sentences\nWhat is Tokenizer?\nTokenizing raw text data is an important pre-processing step for many NLP methods. As explained on wikipedia, tokenization is \u201cthe process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements called tokens.\u201d In the context of actually working through an NLP analysis, this usually translates to converting a string like \"My favorite color is blue\" to a list or array like [\"My\", \"favorite\", \"color\", \"is\", \"blue\"].[11]","6597bc77":"<a id=\"3\"><\/a> <br>\n# 3- spaCy\n<img src='https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/8\/88\/SpaCy_logo.svg\/1920px-SpaCy_logo.svg.png' width=400 height=400>\nspaCy is an Industrial-Strength Natural Language Processing in python.[5]","baef0f83":"## 3-3 Named Entity Recognition\n","4bd361cc":"featuresets = [(gender_features(n), g) for (n,g) in names]\nand the features (last letters) are extracted using:","fbca25cd":"<a id=\"4\"><\/a> <br>\n# 4- Gensim\nGensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. Target audience is the natural language processing (NLP) and information retrieval (IR) community.[2]\n1. Gensim is a FREE Python library\n1. Scalable statistical semantics\n1. Analyze plain-text documents for semantic structure\n1. Retrieve semantically similar documents","9b157337":"## 3-1 Sentence detection\n","15be2fbf":"<a id=\"5\"><\/a> <br>\n# 5- conclusion\nAfter the first version of this kernel, in the third edition, we introduced NLTK library. in addition,  we examined each one in detail. this kernel it is not completed yet! Following up!","719642d2":"Every word is converted into a feature using a simplified bag of words model:","283d2400":"<a id=\"15\"><\/a> <br>\n## 1-5 Gendered Pronoun Data set Analysis\n<img src='https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/GoogleAI-GenderedPronoun\/PronounResolution.png' width=600 height=600>\nPronoun resolution is part of coreference resolution, the task of pairing an expression to its referring entity. This is an important task for natural language understanding, and the resolution of ambiguous pronouns is a longstanding challenge.\n<a id=\"151\"><\/a> <br>\n### 1-5-1 Problem Feature\nIn this competition, you must identify the target of a pronoun within a text passage. The source text is taken from Wikipedia articles. You are provided with the pronoun and two candidate names to which the pronoun could refer. You must create an algorithm capable of deciding whether the pronoun refers to name A, name B, or neither.","7a80bcbe":"<a id=\"153\"><\/a> <br>\n## 1-5-3  evaluation\nSubmissions are evaluated using the multi-class logarithmic loss. Each pronoun has been labeled with whether it refers to A, B, or NEITHER. For each pronoun, you must submit a set of predicted probabilities (one for each class). The formula is :\n<img src='http:\/\/s8.picofile.com\/file\/8351608076\/1.png'>","4b85cc8a":"classifier = NaiveBayesClassifier.train(train_set)","d14d6243":"visualizing the dependency tree!","da57540e":"This dataset is simply a collection of tuples. To give you an idea of what the dataset looks like:","da2e398f":"<a id=\"23\"><\/a> <br>\n## 2-3 NLTK stop words\nNatural language processing (nlp) is a research field that presents many challenges such as natural language understanding.\nText may contain stop words like \u2018the\u2019, \u2018is\u2019, \u2018are\u2019. Stop words can be filtered from the text to be processed. There is no universal list of stop words in nlp research, however the nltk module contains a list of stop words.\n\nIn this article you will learn how to remove stop words with the nltk module.","101a3b49":"## 3-4 displaCy ","0f665b3a":"Our training set is then the sum of these three feature sets:","c9b5a263":"<a id=\"6\"><\/a> <br>\n# 6- References & Credits\n1. [Coursera](https:\/\/www.coursera.org\/specializations\/data-science-python)\n1. [gensim](https:\/\/github.com\/chirayukong\/gensim)\n1. [pythonspot](https:\/\/pythonspot.com\/category\/nltk\/)\n1. [sunscrapers](https:\/\/sunscrapers.com\/blog\/6-best-python-natural-language-processing-nlp-libraries\/)\n1. [spacy](https:\/\/spacy.io\/)\n1. [gensim](https:\/\/pypi.org\/project\/gensim\/)\n1. [nlpforhackers](https:\/\/nlpforhackers.io\/complete-guide-to-spacy\/)\n1. [a-sentiment-analysis-approach-to-predicting-stock-returns](https:\/\/medium.com\/@tomyuz\/a-sentiment-analysis-approach-to-predicting-stock-returns-d5ca8b75a42)\n1. [machinelearningplus](https:\/\/www.machinelearningplus.com\/nlp\/gensim-tutorial\/)\n###### [Go to top](#top)","7b183c20":"### Number of words in the text","137e38fe":"<a id=\"26\"><\/a> <br>\n## 2-6 Natural Language Processing \u2013 prediction\nWe can use natural language processing to make predictions. Example: Given a product review, a computer can predict if its positive or negative based on the text. In this article you will learn how to make a prediction program based on natural language processing.","15d99e0b":"### Number of title case words in the text","61751455":"We train the classifier:","4163acd1":"<a id=\"1\"><\/a> <br>\n# 1-Introduction\nThis Kernel is mostly for **beginners**, and of course, all **professionals** who think they need to review  their  knowledge.\nAlso, this is  the second version for (  [The Data Scientist\u2019s Toolbox Tutorial - 1](https:\/\/www.kaggle.com\/mjbahmani\/the-data-scientist-s-toolbox-tutorial-1) ) and we will continue with other important packages in this kernel.keep following!","51b15640":"<a id=\"25\"><\/a> <br>\n## 2-5 NLTK speech tagging\nThe module NLTK can automatically tag speech.\nGiven a sentence or paragraph, it can label words such as verbs, nouns and so on.\n\nNLTK \u2013 speech tagging example\nThe example below automatically tags words with a corresponding class.","9d879f40":"<a id=\"2\"><\/a> <br>\n# 2- NLTK\nThe Natural Language Toolkit (NLTK) is one of the leading platforms for working with human language data and Python, the module NLTK is used for natural language processing. NLTK is literally an acronym for Natural Language Toolkit. with it you can tokenizing words and sentences.\nNLTK is a library of Python that can mine (scrap and upload data) and analyse very large amounts of textual data using computational methods.\n<img src='https:\/\/arts.unimelb.edu.au\/__data\/assets\/image\/0005\/2735348\/nltk.jpg' width=400 height=400>","7c7dcc22":"### Number of punctuations in the text\n","647aaec5":"<a id=\"13\"><\/a> <br>\n## 1-3 Setup\n\nA few tiny adjustments for better **code readability**","a953395d":"### Number of unique words in the text","3d63e938":"### 1-5-4-1 WordCloud","510eb3b6":"### Number of characters in the text"}}