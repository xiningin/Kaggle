{"cell_type":{"47befe82":"code","35f8e2b5":"code","b7b9a1ab":"code","3057d2a9":"code","608a326e":"code","b2e6cbcf":"code","0026e4e2":"code","b4210071":"code","3c9c6bad":"code","5817cdf7":"code","97835497":"code","cb4f1bf6":"code","3b7d427e":"code","153d6fb8":"code","a69958d3":"code","a4d128fc":"code","dd09916d":"code","3be2c2a2":"code","e0760d4a":"code","545c5b08":"code","6944cff6":"code","e813c81a":"code","189ede6d":"code","55f2a38b":"code","d3077640":"code","cb38771e":"code","9997889b":"code","70bd175a":"code","99f5c3cc":"code","c699dc6e":"code","8082637d":"code","39e7a0a4":"code","4c42f10c":"code","65f38e3f":"code","cd1d62a2":"code","097aad15":"code","60ea6d82":"code","da8d72ab":"code","394572fa":"code","258bb341":"code","c29b1b4e":"code","e9f83be9":"code","6f20a639":"code","b2764f20":"code","1e408a47":"markdown","a6a095d0":"markdown","5b47e25c":"markdown","68385387":"markdown","c5ccd77e":"markdown","f549f287":"markdown","c7a5888a":"markdown","c3936ec7":"markdown","0c97fc9e":"markdown","286dd4e7":"markdown","722cebfa":"markdown","2299ee49":"markdown","81fea612":"markdown","fd29749e":"markdown","b46de283":"markdown","4ba9bbb1":"markdown","9b379a3b":"markdown","89daaeba":"markdown","2ee88f5f":"markdown","5834462c":"markdown","81d8081e":"markdown","b457e667":"markdown","82c4585d":"markdown","948e05ec":"markdown","12a3152c":"markdown","9f1b532d":"markdown","87487593":"markdown","b92ffd77":"markdown","b3201643":"markdown","5c23cf24":"markdown","a57a8e7c":"markdown","584eec0f":"markdown","2544388f":"markdown","828c8e44":"markdown","397a5271":"markdown","5e4065e2":"markdown","16588cb0":"markdown","e1d2db10":"markdown","f654262c":"markdown","d1f5e1c9":"markdown"},"source":{"47befe82":"# Import basic required modules\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nplt.style.use('bmh')\nsns.set_style({'axes.grid':False}) \n\n# Advanced visualization modules(datashader)\nimport datashader as ds\nimport datashader.transfer_functions as tf\nfrom datashader.colors import viridis, inferno\n\n# Folium visualization for geographical map\nimport folium as flm\nfrom folium.plugins import HeatMap","35f8e2b5":"%%time\n# Downcasting data types to reduce momory consumption\ndtypes = {}\nfor key in ['fare_amount', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']:\n    dtypes[key] = 'float32'\nfor key in ['passenger_count']:\n    dtypes[key] = 'uint8'\n    \n# Read in train and test data (5 million rows)\ntrain = pd.read_csv('..\/input\/train.csv', nrows = 5_000_000, dtype = dtypes).drop('key', axis = 1)\ntest = pd.read_csv('..\/input\/test.csv', dtype = dtypes)","b7b9a1ab":"# Now check out the data types \nprint('Dtypes after downcasting except pickup_datetime:')\ndisplay(train.dtypes)","3057d2a9":"%%time\n# 'pickup_datetime' should be in datetime format. Let's convert it\n# Don't forget to set 'infer_datetime_format=True'. Otherwise it takes forever :)\ntrain['pickup_datetime'] = pd.to_datetime(train['pickup_datetime'], infer_datetime_format=True)\ntest['pickup_datetime'] = pd.to_datetime(test['pickup_datetime'], infer_datetime_format=True)","608a326e":"# Current memory usage(in MB) by columns after conversion\nprint('Memory usage(in MB) by variables after conversion:')\ndisplay(np.round(train.memory_usage(deep = True)\/1024**2, 4))","b2e6cbcf":"# Look at the data we're going to deal with\nprint('Preview train data:')\ndisplay(train.head())\nprint('Preview test data:')\ndisplay(test.head())","0026e4e2":"# Missing values in train data\nprint('Missing values in train data:')\ndisplay(train.isna().sum())\n\n# Missing values in test data\nprint('Missing values in test data:')\ndisplay(test.isna().sum())","b4210071":"# Drop missing observations from train data.\ntrain.dropna(how = 'any', axis = 0, inplace = True)\n# Shape of the df after dropping missing rows\nprint('Shape of the df after dropping missing rows:{}'.format(train.shape))","3c9c6bad":"# Distrubution of target variable with skewness\nfig, ax = plt.subplots(figsize = (14,6))\nsns.distplot(train.fare_amount, bins = 200, color = 'firebrick', ax = ax)\nax.set_title('Distribution of fare_amount (skewness: {:0.5})'.format(train.fare_amount.skew()))\nax.set_ylabel('realtive frequency')\nplt.show()","5817cdf7":"# Class distribution of passenger_count\nfig, ax = plt.subplots(figsize = (14,6))\nclass_dist = train.passenger_count.value_counts()\nclass_dist.plot(kind = 'bar', ax = ax)\nax.set_title('Class distribution of passenger_count')\nax.set_ylabel('absolute frequency')\nplt.show()","97835497":"# Look at the abnormalities using descritive stats\ntrain.fare_amount.describe()","cb4f1bf6":"# Drop fare_amount less than 0.\nneg_fare = train.loc[train.fare_amount<0, :].index\ntrain.drop(neg_fare, axis = 0, inplace = True)\n\n# Rerun the descriptive stats\ntrain.fare_amount.describe()","3b7d427e":"# Drop rows greater than 100 and lesser than 2.5\nfares_to_drop = train.loc[(train.fare_amount>100) | (train.fare_amount<2.5), :].index\ntrain.drop(fares_to_drop, axis = 0, inplace = True)\nprint('Shape of train data after dropping outliers from fare_amount:{}'.format(train.shape))","153d6fb8":"# Check the 2.5 and 97.5 percentile of los nad lats\ndef percentile(variable):\n    two_and_half = variable.quantile(0.25)\n    ninty_seven_half = variable.quantile(0.975)\n    print('2.5 and 97.5 percentile of {} is respectively: {:0.2f}, and {:0.2f}'.format(variable.name, two_and_half, ninty_seven_half))\n    \npercentile(train.pickup_latitude)\npercentile(train.dropoff_latitude)\npercentile(train.pickup_longitude)\npercentile(train.dropoff_longitude) ","a69958d3":"# For lats, our range is 40 to 42 degrees(with 40 and 42)\ntrain = train.loc[train.pickup_latitude.between(left = 40, right = 42), :]\ntrain = train.loc[train.dropoff_latitude.between(left = 40, right = 42), :]\n\n# For lons, our range is -75 to -72 degrees(with 40 and 42)\ntrain = train.loc[train.pickup_longitude.between(left = -75, right = -72), :]\ntrain = train.loc[train.dropoff_longitude.between(left = -75, right = -72), :]\nprint('Shape of train data after after dropping outliers from lats and lons: {}'.format(train.shape))","a4d128fc":"# Check out the descriptive stats first\ntrain.passenger_count.describe()","dd09916d":"# Drop passenger_count of 208 and 129, 9, and 7.\npassenger_count_to_drop = train.loc[(train.passenger_count==208) | (train.passenger_count==129) | (train.passenger_count==9) | (train.passenger_count==7)].index\ntrain.drop(passenger_count_to_drop, axis = 0, inplace = True)\nprint('Shape of train data after dropping outliers from passenger_count:{}'.format(train.shape))\n\n# Let's check again the passenger_count\ndisplay(train.passenger_count.describe())","3be2c2a2":"# Merged train and test data across rows\nmerged = pd.concat([train,test], axis = 0, sort=False)","e0760d4a":"# Calculate great circle distance using haversine formula\ndef great_circle_distance(lon1,lat1,lon2,lat2):\n    R = 6371000 # Approximate mean radius of earth (in m)\n    \n    # Convert decimal degrees to ridians\n    lon1,lat1,lon2,lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n    \n    # Distance of lons and lats in radians\n    dis_lon = lon2 - lon1\n    dis_lat = lat2 - lat1\n    \n    # Haversine implementation\n    a = np.sin(dis_lat\/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dis_lon\/2)**2\n    c = 2*np.arctan2(np.sqrt(a), np.sqrt(1-a))\n    dis_m = R*c # Distance in meters\n    dis_km = dis_m\/1000 # Distance in km\n    return dis_km\n\n# Create a column named greate_circle_distance\nmerged['great_circle_distance'] = great_circle_distance(merged.pickup_longitude, merged.pickup_latitude, merged.dropoff_longitude, merged.dropoff_latitude)","545c5b08":"# Convert lons and lats into cartesian coordinates. Assume the earth as sphere not ellipsoid\nR = 6371000 # Approximate mean radius of earth (in m)\n # lons and lats must be in radians\nlon1,lat1,lon2,lat2 = map(np.radians, [merged.pickup_longitude, merged.pickup_latitude, merged.dropoff_longitude, merged.dropoff_latitude])\nmerged['pickup_x'] = R*np.cos(lon1)*np.cos(lat1)\nmerged['pickup_y'] = R*np.sin(lon1)*np.cos(lat1)\nmerged['dropoff_x'] = R*np.cos(lon2)*np.cos(lat2)\nmerged['dropoff_y'] = R*np.sin(lon2)*np.cos(lat2)\n\n# Now calculate the euclidean distance\nx1 = merged['pickup_x']\ny1 = merged['pickup_y']\nx2 = merged['dropoff_x']\ny2 = merged['dropoff_y']\nmerged['euclidean_distance'] = (np.sqrt(( x1 - x2)**2 + ( y1 - y2)**2))\/1000 # in km","6944cff6":"# Calculate manhattan distance from x and y coordinates\nmerged['manhattan_distance'] = (np.abs(x1 - x2) + np.abs(y1 - y2))\/1000 # in km","e813c81a":"# Create two variables taking absolute differences of lons and lats\nmerged['abs_lon_diff'] = np.abs(merged.pickup_longitude - merged.dropoff_longitude)\nmerged['abs_lat_diff'] = np.abs(merged.pickup_latitude - merged.dropoff_latitude)","189ede6d":"# Extract pickup_hour, day, date, month, and year from pickup_datetime.\nmerged['pickup_hour'] = merged.pickup_datetime.dt.hour\nmerged['pickup_date'] =  merged.pickup_datetime.dt.day\nmerged['pickup_day_of_week'] =  merged.pickup_datetime.dt.dayofweek\nmerged['pickup_month'] =  merged.pickup_datetime.dt.month\nmerged['pickup_year'] =  merged.pickup_datetime.dt.year","55f2a38b":"# Let's see the current dtypes and total memory consumption by variables in MB\nprint('Current Data Types:')\ndisplay(merged.dtypes)\nprint('\\n Total memory consumption in MB: {}'.format(np.sum(merged.memory_usage(deep = True)\/1024**2)))","d3077640":"# Drop variables \nmerged.drop(['key', 'pickup_datetime'], axis = 1, inplace = True)","cb38771e":"# Downcasting variables\nmerged.loc[:, ['pickup_hour', 'pickup_date', 'pickup_day_of_week', 'pickup_month']] = merged.loc[:, ['pickup_hour', 'pickup_date', 'pickup_day_of_week', 'pickup_month']].astype(np.uint8)\nmerged.loc[:, ['great_circle_distance', 'euclidean_distance', 'manhattan_distance']] = merged.loc[:, ['great_circle_distance', 'euclidean_distance', 'manhattan_distance']].astype(np.float32)\nmerged.loc[:, ['pickup_year']] = merged.loc[:, ['pickup_year']].astype('int16')\n\n# Check total memory consumption after downcasting\nprint('Total memory consumption after downcasting in MB: {}'.format(np.sum(merged.memory_usage(deep = True)\/1024**2)))","9997889b":"# Let's separate train and test data again\ntrain_df = merged.iloc[0:4892576, :]\ntest_df = merged.iloc[4892576:, :] \ntest_df.drop('fare_amount', axis = 1, inplace = True) # Due to concatenation","70bd175a":"# Let's see which variables have the strongest and weakest correlation with fare_amount\ncorr = train_df.corr().sort_values(by='fare_amount', ascending=False)\nfig, ax = plt.subplots(figsize = (20,12))\nsns.heatmap(corr, annot = True, cmap ='BrBG', ax = ax, fmt='.2f', linewidths = 0.05, annot_kws = {'size': 17})\nax.tick_params(labelsize = 15)\nax.set_title('Correlation with fare_amount', fontsize = 22)\nplt.show()","99f5c3cc":"# Plot subplots of regression plots\ncontinuous_var = train_df.iloc[0:10000, :].select_dtypes(include = ['float32', 'float64']).drop('fare_amount', axis = 1)\nfig, axes = plt.subplots(7,2, figsize = (40,80))\nfor ax, column in zip(axes.flatten(), continuous_var.columns):\n    x = continuous_var[column]\n    y = train_df.fare_amount.iloc[0:10000]\n    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n    sns.regplot(x = continuous_var[column], y = y, ax = ax, line_kws={'label':'r: {}\\np: {}'.format(r_value,p_value)})\n    ax.set_title('{} vs fare_amount'.format(column), fontsize = 36)\n    fig.suptitle('Regression Plots', fontsize = 45)\n    ax.tick_params(axis = 'both', which = 'major', labelsize = 22)\n    ax.tick_params(axis = 'both', which = 'minor', labelsize = 22)\n    ax.set_ylabel('')\n    ax.set_xlabel('')\n    ax.legend(loc = 'best', fontsize = 32)\nfig.delaxes(ax = axes[6,1])\nfig.tight_layout(rect = [0, 0.03, 1, 0.97])","c699dc6e":"# Extract categorical variable first\ncat_var = train_df.iloc[0:10000, :].select_dtypes(include = ['uint8'])\ncat_var = pd.concat([cat_var, train_df.pickup_year.iloc[0:10000]], axis = 1)\n\n# A box plot to visualize the association between fare_amount and categorical variables\nfig, axes = plt.subplots(3,2,figsize = (20,25))\nfor ax, column in zip(axes.flatten(), cat_var.columns):\n    sns.boxplot(x = cat_var[column], y = train_df.fare_amount.iloc[0:10000], ax = ax)\n    ax.set_title('{} vs fare_amount'.format(column), fontsize = 22)\n    ax.tick_params(axis = 'both', which = 'major', labelsize = 16)\n    ax.tick_params(axis = 'both', which = 'minor', labelsize = 16)\n    ax.set_xlabel(column, fontsize = 18)\n    ax.set_ylabel('fare_amount', fontsize = 18)\n    fig.suptitle('Association with categorical variables', fontsize = 26)\nfig.tight_layout(rect = [0, 0.03, 1, 0.97])","8082637d":"# Let's group mean fare_amount by pickup_year to see if there is a pattern.\npivot_year = pd.pivot_table(train_df, values = 'fare_amount', index = 'pickup_year', aggfunc = ['mean'])\nprint('Mean fare_amount across the classes of pickup_year: \\n{}'.format(pivot_year))\n# or train.fare_amount.groupby([train.pickup_year]).mean()","39e7a0a4":"# A bar plot would be more helpful to visualize this pattern\nfig, ax = plt.subplots(figsize = (15,5))\npivot_year.plot(kind = 'bar', legend = False, color = 'firebrick', ax = ax)\nax.set(title = 'pickup_year vs mean fare_amount', ylabel= 'mean fare_amount')\nplt.show()","4c42f10c":"# x_range and y_range for pickup_locations\nprint('x_range and y_range for pickup_locations:')\nprint(train_df.pickup_longitude.min(), train_df.pickup_longitude.max())\nprint(train_df.pickup_latitude.min(), train_df.pickup_latitude.max())\n\n# x_range and y_range for dropoff_locations\nprint('\\nx_range and y_range for dropoff_locations:')\nprint(train_df.dropoff_longitude.min(), train_df.dropoff_longitude.max())\nprint(train_df.dropoff_latitude.min(), train_df.dropoff_latitude.max())","65f38e3f":"# Create a function to plot longitudes vs latitudes of rides\ndef plot_location(lon,lat, c_map):\n    # Initial datashader visualization configuration\n    pickup_range = dropoff_range = x_range, y_range = ((-74.05, -73.7), (40.6, 40.85))\n    # Initiate canvas and create grid\n    cvs = ds.Canvas(plot_width = 1080, plot_height = 600, x_range = x_range, y_range = y_range)\n    agg = cvs.points(train, lon, lat)\n    # Create image map with custom color map\n    img = tf.shade(agg, cmap = c_map, how = 'eq_hist')\n    return tf.set_background(img, 'black')","cd1d62a2":"# Show image map of pickup locations with viridis color map\nplot_location('pickup_longitude', 'pickup_latitude', viridis)","097aad15":"# Show image map of pickup locations with inferno color map\nplot_location('pickup_longitude', 'pickup_latitude', inferno)","60ea6d82":"# Create a function to plot folium heatmap\ndef plot_map(lat, lon):\n    # Lat and lon of nyc to plot the map of nyc\n    map_nyc = flm.Map(location = [40.7141667, -74.0063889], zoom_start = 12, tiles = \"Stamen Toner\")\n    # creates a marker for nyc\n    flm.Marker(location = [40.7141667, -74.0063889], icon = flm.Icon(color = 'red'), popup='NYC').add_to(map_nyc)\n    # Plot heatmap of 20000 lats and lons points\n    lat_lon = train.loc[0:20000, [lat, lon]].values\n    HeatMap(lat_lon, radius = 10).add_to(map_nyc)\n    #map_nyc.save('HeatMap.html')\n    return map_nyc","da8d72ab":"# Plot street map of NYC and then plot heatmap of pickup locations on it.\nplot_map('pickup_latitude', 'pickup_longitude')","394572fa":"# Show image map of pickup locations with inferno color map\nplot_location('dropoff_longitude', 'dropoff_latitude', inferno)","258bb341":"# viridis color map is even better to capture patterns\nplot_location('dropoff_longitude', 'dropoff_latitude', viridis)","c29b1b4e":"# Plot street map of NYC and then plot heatmap of dropoffs lats and lons on it.\nplot_map('dropoff_latitude', 'dropoff_longitude')","e9f83be9":"# Get the data ready for training and predicting\ny_train = train_df.fare_amount\nX_train = train_df.drop(['fare_amount'], axis = 1)\nX_test = test_df","6f20a639":"# Train and predict using linear regression\nfrom sklearn.linear_model import LinearRegression\n\n# Instantiate linear regression object\nlinear_reg = LinearRegression()\n\n# Train with the objt\nlinear_reg.fit(X_train, y_train)\n\n# Make prediction\ny_pred = linear_reg.predict(X_test)","b2764f20":"# Create csv file for submission\nsubmission = pd.DataFrame()\nsubmission['key'] = test.key\nsubmission['fare_amount'] = y_pred\nsubmission.to_csv('sub_with_linear_reg', index = False)","1e408a47":"# 10.End Note <a id=\"10\"><\/a>\n**This submission scored 4.26176 on leaderboard.** Yes that's not a great score! But considering the fact that we have only used linear regression, its not a bad score either. May be trying out random forest, gradient boosting or xgboost might improve the score. Any suggestion or query is welcomed. Last but not the least, some upvotes would be appreciated upon finding this kernel useful.","a6a095d0":"## 4.1 fare_amount","5b47e25c":"Again, like pickups, dropoffs are mostly concentrated on the main street of NYC and just a few to 'John F Kenedy Int'l Airport' (zoom in to see it).","68385387":"## 5.2 latitudes and longitudes\nFor the latitude and longitude columns, we can use statistics as well as our intuition for removing outliers. Here I'll find the 2.5% and 97.5% percentile values in each column and keep only the values close to that range. I would encourage to try out other methods like IQR and Z-score to see which works better.","c5ccd77e":"Clearly our target variable is right skewed with a skewness over 4.5 that indicates the presence of outliers. Outliers will be in the following section.\n## 4.2 passenger_count.\nLet's see the class distribution of passenger_count","f549f287":"# 5.Outliers Treatment <a id=\"5\"><\/a>","c7a5888a":"# 3.Missing Value Treatment <a id=\"3\"><\/a>","c3936ec7":"Maximum number of passengers is 208. I don't think a taxi (or even a bus) can carry 208 passengers. This must be an error. Let's drop it. Also from univariate section, we are gonna drop passenger count of value 129, 51, 9, and 7.","0c97fc9e":"We can see the some correlation correlation values are reduced. This is due to downsampling (i.e., we only use 10k samples off over 4.5 million samples to plot regression plots). A p value of 0 indicates that we're 100% confident that there is a statistically significant correlation between target variables and fare_amount.","286dd4e7":"## 6.4 abs_lon_diff, and abs_lat_diff\nNow we also want to create two variables by taking absolute differences of longitudes and latitudes to see how they perform as predictors for fare_amount.","722cebfa":"# 7.Bivariate Analysis <a id=\"7\"><\/a>\nBeing the most important part, bivariate analysis tries to find the relationship between two variables. We will look for correlation or association between our predictor and target variables. Bivariate analysis is performed for any combination of categorical and numerical variables. The combination can be: Numerical & Numerical, Numerical & Categorical and Categorical & Categorical. Different methods are used to tackle these combinations during analysis process. If the combination is continuous numerical vs continuous numerical, we would use regression plot to observe the correlation. One the other hand, for categorical vs continuous numerical combination, we would observe the association with box plots, bar plots and pivot tables.","2299ee49":"Well! There is infact a pattern. There is an increasing trend in mean fare from yeay 2009 to 2015 (this can be expected). Since fare_amount is positively correlated to pickup_year (upward trend), you might need to pay more in the coming year onwards.","81fea612":"# 1.Problem Description and Objective <a id=\"1\"><\/a>\nIn this challange, we're asked to predict the amount of fare for a taxi ride in New York City given the pickup and dropoff locations, number of passengers in a ride, and the pickup data time for a ride. So fare amount is our target variable and rest of the variables are our predictor variables. Hence, its a supervised regression problem.\n\n# 2.Importing Packages and Collecting Data <a id=\"2\"><\/a>\nSince the train data contains about 55 million observations, its not feasible to deal with all of them. Hence, we would only use 5 million instances off 55 million instances to analyse and train our models later on.","fd29749e":"Now the maximum no. of passengers is 6 which makes sense.","b46de283":"## 7.2 categorical vs continuous numerical variable\nLet's dig deeper to explore the relationship between categorical variables and fare_amount. A box plot can reveal if there is any association between predictor categorical variables and target variable. If any catgorical variabes are highly associated with fare_amount, mean fare_amount should be different across different groups of those categorical variables. We can see this pattern using pivot table. Again we will only use 10000 instances to speed up the rendering.","4ba9bbb1":"Well! We're not done yet! Based on [this discussion](https:\/\/www.kaggle.com\/c\/new-york-city-taxi-fare-prediction\/discussion\/63319) on [real-world taxi fares in New York City](http:\/\/nymag.com\/nymetro\/urban\/features\/taxi\/n_20286\/), I'm also going to remove any fares less than $2.50 that appears to be the minimum fare, so any values in the training set less than this amount must be errors in data collection or entry.\n\nI'll also remove any fares greater than $100. I'll justify this based on the limited number of fares outside this bounds, but it might be possible that including these values helps the model! I'd encourage you to try different values and see which works best.","9b379a3b":"# About This Notebook\nThis is my 3rd notebook in the world of Kaggle. Throughtout this notebook, I will try to keep things as simple as possible. I would be happy to take any query you might have after you finish exploring this kernel. If you find this notebook useful, you can check out my other two kernels thta you might find useful as well.","89daaeba":"## 5.3 passenger_count","2ee88f5f":"The class distribution is imbalanced since some classes outnumber some other classes. Most of the passengers like to travel alone. One important takeaway from the plot is that some  claasses(like 208, 129, and 51) turn out to be outliers. Since a taxi is not big enough to carry 208, 129 or 51 passengers, these instances would be removed in the outliers treatment section.","5834462c":"## 6.3 manhattan_distance\nManhattan distance is the sum of the absolute values of the differences of the coordinates. **If a = (x1,y1) and b = (x2,y2), then the manhattan distance between a and b is given by |x1 - x2| + |y1 - y2|**. For more on manhattan distance [see](https:\/\/en.wikipedia.org\/wiki\/Taxicab_geometry)","81d8081e":"# 6.Feature Engineering <a id=\"6\"><\/a>\nIn this section we would try to calculate the travelling distance based on latitudes and longitudes. Also we would like to extract pickup hour, day, date, month, and year from 'pickup_datetime' to see if they influnce the fare_amount. To do so we will merge train and test data.","b457e667":"#### Create a function to plot regression plot\ndef regression_plot(predictor, target, xlabel, title):\n    fig = sns.jointplot(x = predictor, y = target, kind = 'reg')\n    fig.fig.set_size_inches(12,7)\n    ax = plt.gca()\n    ax.set_xlabel(xlabel, size = 15)\n    ax.set_ylabel('fare_amount ($usd)', size = 15)\n    ax.set_title(title, size = 18, fontweight='bold')\n    plt.tight_layout()","82c4585d":"fare_amount shouldn't be negative. So let's drop those negative intances from fare_amount.","948e05ec":"## 5.1 fare_amount","12a3152c":"## 6.2 euclidean_distance\nThe Euclidean distance or Euclidean metric is the \"ordinary\" straight-line distance between two points in Euclidean space. **It assumes the surface to be flat (rather than spherical or ellipsoidal)**. Hence it should not be as accurate as great circle or [vincenty distance](https:\/\/en.wikipedia.org\/wiki\/Vincenty%27s_formulae) (that assumes the earth to be ellipsoid). But we would take that to see if there is any correlation between this distance and fare_amount. Since euclidean distance is for cartesian plane, we need to convert our longitudes and latitudes into cartesian coordinates. See [here](https:\/\/en.wikipedia.org\/wiki\/Euclidean_distance) for more on euclidean distance. **If a = (x1,y1) and b = (x2,y2), then the euclidean distance between a and b is given by np.sqrt(x1-x2)^2+(y1-y2)^2)**","9f1b532d":"None of the catagorical variables seem to be highly associated with fare_amount except pickup_year. Since pickup_year is highly associated with fare_amount, mean fare_amount should be different across the classes (groups) of pickup_year. We can visualize this pattern using pivot table.","87487593":"Again like pickups, most of the drops were to the main street of NYC than any other streets. Let's create heatmap of dropoff lats and lons on the map of NYC.","b92ffd77":"# Outlines\n* [1. Problem Description and Objective](#1)\n* [2. Importing Packages and Collecting Data](#2)\n* [3. Missing Value Treatment](#3)\n* [4. Univariate Analysis](#4)\n* [5. Outliers Treatment](#5)\n* [6. Feature Engineering](#6)\n* [7. Bivariate Analysis](#7)\n* [8. Location Visualization](#8)\n* [9. Model Building](#9)\n* [10. End Note](#10)","b3201643":"We can see some patterns especially on the main streen of NYC. That means more rides start from the main streen than any oher streets. However,  we're only visualizing 4.5 million points. If we would use more data, the trend would be more evident. Now it would be more useful if we could visualize NYC map and then plot those pickup points. This would give us much concise information about the pickup locations on the streets of NYC. For this, we will use package 'folium'. However, for faster rendering (and some limitations of folium), we would plot only 20000 points in folium map. We will also use heatmap instead of scatter points to capture the pattern better.","5c23cf24":"Since we're dealing with 5 millions observations, dropping those 36 missing observations might not affect the model. However, there is no missing values in test data.","a57a8e7c":"# 9.Model Building <a id=\"9\"><\/a>\nTo keep things as simple as possible, we would only try linear regression. I would encourage you to try no-linear models (like random forest, gradient boosting etc).","584eec0f":"## 6.1 great_circle_distance\nThe great-circle distance or orthodromic distance is the shortest distance between two points on the surface of a sphere, measured along the surface of the sphere (as opposed to a straight line through the sphere's interior). The distance between two points in Euclidean space is the length of a straight line between them, but on the sphere there are no straight lines.\nWe can calculate the distance travelled by a passenger given the pickup and dropoff latitudes and longitudes using haversine formula **assuming the earth is a perfect sphere rather than an ellipsoid**. For more on great circle distance and  haversine [see](https:\/\/en.wikipedia.org\/wiki\/Great-circle_distance)","2544388f":"Based on these values, we would remove outliers from lons and lats.","828c8e44":"## 6.5 pickup_hour, day, date, month, and year\nLet's extract pickup_hour, pickup_day, pickup_date, pickup_month, and pickup_year from 'pickup_datetime' to check if there is any association between 'fare_amount', and them. ","397a5271":"# 4.Univariate Analysis <a id=\"4\"><\/a>\nUnivariate analysis separately explores the distribution of each variable in a data set. It looks at the range of values, as well as the central tendency of the values. Univariate data analysis does not look at relationships between various variables (like bivariate and multivariate analysis) rather it summarises each variable on its own. Methods to perform univariate analysis will depend on whether the variable is categorical or numerical. For numerical variable, we would explore its shape of distribution (distribution can either be symmetric or skewed) using histogram and density plots. For categorical variables, we would use bar plots to visualize the absolute and proportional frequency distribution. Knowing the distribution of the feature values becomes important when you use machine learning methods that assume a particular type of it, most often Gaussian. **Let's starts off with our target variable:**","5e4065e2":"That's beautiful, isn't it? Look like some pickups are from water. That's might be errors (or outliers). Again this heatmap shows most pickups are from the main street of NYC and a few of them are from 'John F Kenedy Int'l Airport' (zoom in to see it).\n\n## 8.2 dropoff_locations\nLet's visualize dropoff locations with datashader and then plot heatmap of lats and lons of dropoffs in the streets of NYC with folium.","16588cb0":"We will convert pickup_hour, pickup_date, pickup_day_of_week, pickup_month to uint8. All the distances  will be converted to float32 dtypes. We would also delete key, fare_amount (due to merging), and pickup_datetime and see how much memory we can save.","e1d2db10":"# 8.Locaion Visualization <a id=\"8\"><\/a>\nSince we're dealing with location data, we may want to visualize the pickup and dropoff locations. We can also plot the map of NYC and then plot data points on it to see the concentrations of most pickups and dropoffs. As thre are over 4.5 million observations, its quite impossible to plot longitudes vs latitudes (or location) using conventional plotting packages. Hence we will use 'datashader' to plot longitudes vs latitudes (i.e., locations) using all the 4.5 million instances.\n\n## 8.1 pickup_locations","f654262c":"So we've x_range (-74.99828, -72.06699) and y_range (40.850838, 41.998108). However, most of the rides occur within the x_range (-74.05, -73.7) and y_range (40.6, 40.85). So for nice visualation, we'll skip the outliers to plot both pickup and dropoff locations. And we would also use the same range to plot dropoff locations like pickup locations.","d1f5e1c9":"So it turns out the distance variables (as expected) has the strongest positive correlation with fare_amount followed by abs_lon_diff, and abs_lat_diff. On the other hand pickup_latitude and dropoff latitude have the strongest negative correlation among the variables. Some variables have correlation zero or almost zero.\n\n## 7.1 continuous numerical vs continuous numerical variable\nHere all the distance variables, abs_lon_diff, abs_lat_diff, pickup_x, pickup_longitude, dropoff_x, dropoff_longitude, pickup_y, dropoff_y, dropoff_latitude, pickup_latitude are continuous numerical variables. Hence we would use regression plot to observe the correlation between these variables and fare_amount. To speed up the process, we will take only 10000 instances off 4.5 million instances (scatter plot is very slow for large samples) to plot regression plot between our predictor and target variables."}}