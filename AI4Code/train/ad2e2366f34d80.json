{"cell_type":{"1468ca50":"code","11949f3e":"code","0331a9cf":"code","4d3952df":"code","01887ba5":"code","e2d259de":"code","4016f16b":"code","a484f099":"code","e4d61e7d":"code","effff3d8":"code","280b3ff3":"code","73de28d9":"code","ade2028c":"code","3f5eb5e4":"code","af2aa87a":"code","742fa0ee":"code","5bf9d9af":"code","cb31b916":"code","d48528ef":"code","ec5b25cf":"code","bbdfdc4c":"code","2dbaddcc":"code","51f57572":"code","ac34c4c6":"code","a6c5f3b4":"code","e928e973":"code","2e6c6987":"code","7105e7b2":"code","8e935ccf":"code","e106116f":"code","de5fcb1a":"code","27ac1f54":"code","7cad137a":"code","7364799f":"code","0c51fbed":"code","3c362fe1":"code","b1fcc753":"code","c2f011ae":"code","716430f5":"code","45295965":"code","21e65db6":"code","f0443862":"code","96cd51ad":"code","8f80e5d7":"code","4e576588":"code","089497a8":"code","206de58a":"code","8d22f044":"code","82660b3b":"code","a8f0e71f":"code","14edf3d6":"code","41d69bf2":"code","e3ee1e32":"code","7a16833e":"code","5e4dcd49":"code","27cf2faa":"code","6f0a7b4b":"code","4c2c221c":"code","64056f01":"code","207d90e9":"code","584a1d7d":"code","33b9d748":"code","c010f439":"code","25489b6d":"code","8089d7d4":"code","57b6a3b7":"code","8284eee0":"code","faf0e03f":"code","22225d27":"code","239d7fcb":"code","59d97214":"code","861c7056":"code","71aaec8b":"code","1b2c7d16":"code","94c91b86":"code","a8742d74":"code","c8867a2c":"code","947c0ba6":"code","4dac144b":"code","789e07be":"code","74d1c3f9":"code","266b2e23":"code","3bce8421":"code","acbd1898":"code","9e3c08b5":"code","31887937":"code","15c03355":"code","d6177d22":"code","8686db03":"markdown","f91c7d6d":"markdown","70881d14":"markdown","9141d617":"markdown","303f9a6c":"markdown","e880de29":"markdown","c90583c1":"markdown","20759084":"markdown","0725a956":"markdown","b477beea":"markdown","4105f699":"markdown","d740e83a":"markdown","591cedc7":"markdown","82b02b35":"markdown","2bea99c8":"markdown","39bc8ba6":"markdown","0640a095":"markdown","16cbc055":"markdown","ce8d1dff":"markdown","30b0f287":"markdown","c838ce8e":"markdown","11dc30bc":"markdown","67231c2f":"markdown","75a8e61c":"markdown","887f7a13":"markdown","012fd639":"markdown","f009d9f2":"markdown","9be442fb":"markdown","14bb1e53":"markdown","76b59740":"markdown","23cf4853":"markdown","a81e6d83":"markdown","70962fec":"markdown"},"source":{"1468ca50":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom tensorflow.keras.utils import plot_model","11949f3e":"import zipfile \nimport os","0331a9cf":"os.listdir('..\/input\/intel-image-classification\/seg_train\/seg_train')","4d3952df":"os.listdir('..\/input\/intel-image-classification\/seg_test\/seg_test')","01887ba5":"len(os.listdir('..\/input\/intel-image-classification\/seg_pred\/seg_pred'))","e2d259de":"import cv2","4016f16b":"os.listdir('..\/input\/intel-image-classification\/seg_train\/seg_train\/forest')[0]","a484f099":"import matplotlib.pyplot as plt\n\nplt.imshow(cv2.imread('..\/input\/intel-image-classification\/seg_train\/seg_train\/forest'+'\/'+os.listdir('..\/input\/intel-image-classification\/seg_train\/seg_train\/forest')[0]))","e4d61e7d":"def get_images(directory):\n    Images = []\n    Labels = []  \n    label = 0\n    \n    for labels in os.listdir(directory): \n        if labels == 'glacier': \n            label = 2\n        elif labels == 'sea':\n            label = 4\n        elif labels == 'buildings':\n            label = 0\n        elif labels == 'forest':\n            label = 1\n        elif labels == 'street':\n            label = 5\n        elif labels == 'mountain':\n            label = 3\n            \n        for image_file in os.listdir(os.path.join(directory, labels)): \n            image = cv2.imread(os.path.join(directory, labels)+'\/'+image_file)\n            image = cv2.resize(image,(150,150)) \n            Images.append(image)\n            Labels.append(label)\n    \n    return Images, Labels\n","effff3d8":"def get_classlabel(class_code):\n    labels = {2:'glacier', 4:'sea', 0:'buildings', 1:'forest', 5:'street', 3:'mountain'}\n    \n    return labels[class_code]","280b3ff3":"Images, Labels = get_images('..\/input\/intel-image-classification\/seg_train\/seg_train') \n\nImages = np.array(Images) #Images to numpy array.\nLabels = np.array(Labels)","73de28d9":"Images.shape, Labels.shape","ade2028c":"Labels","3f5eb5e4":"from sklearn.utils import shuffle\nImages,Labels = shuffle(Images,Labels,random_state=42)","af2aa87a":"Labels","742fa0ee":"def get_indexes(label,list_n):\n  for x in range(len(Labels)):\n    if Labels[x]==label:\n      list_n.append(x)\n  return list_n","5bf9d9af":"buildings=[]\nbuildings=get_indexes(0,buildings)\nforest=[]\nforest=get_indexes(1,forest)\nglacier=[]\nglacier=get_indexes(2,glacier)\nmountain=[]\nmountain=get_indexes(3,mountain)\nsea=[]\nsea=get_indexes(4,sea)\nstreet=[]\nstreet=get_indexes(5,street)","cb31b916":"import random\nfrom random import randint\n\nf,ax = plt.subplots(6,3, figsize=(17,17)) \ntypes_img=[buildings,forest,glacier,mountain,sea,street]\n\nfor z in range(0,6,1):\n  for j in range(0,3,1):\n    rnd_number=random.choice(types_img[z])\n    ax[z,j].imshow(Images[rnd_number])\n    ax[z,j].set_title(get_classlabel(Labels[rnd_number]))\n    ax[z,j].axis('off')","d48528ef":"Images_test, Labels_test = get_images('..\/input\/intel-image-classification\/seg_test\/seg_test') \n\nImages_test = np.array(Images_test) \nLabels_test = np.array(Labels_test)","ec5b25cf":"Images_test.shape, Labels_test.shape","bbdfdc4c":"Labels_test","2dbaddcc":"Images_test,Labels_test = shuffle(Images_test,Labels_test,random_state=42)","51f57572":"Labels_test","ac34c4c6":"Images.shape, Labels.shape, Images_test.shape, Labels_test.shape","a6c5f3b4":"df2=pd.DataFrame(Labels, columns=['target'])\npie1=pd.DataFrame(df2['target'].replace(0.0,'Buildings').replace(1.0,'Forest').replace(2.0,'Glacier').replace(3.0,'Mountain').replace(4.0,'Sea').replace(5.0,'Street').value_counts())\npie1.reset_index(inplace=True)\npie1.plot(kind='pie', title='Pie chart of classes in training set',y = 'target',\n          autopct='%1.1f%%', shadow=False, labels=pie1['index'], legend = False, fontsize=14, figsize=(8,8))","e928e973":"import tensorflow as tf\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.optimizers import RMSprop,Adam,SGD,Adadelta\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","2e6c6987":"train_datagen = ImageDataGenerator(\n    rescale = 1.0\/255,\n    featurewise_center=False,\n    samplewise_center=False,\n    featurewise_std_normalization=False,\n    samplewise_std_normalization=False,\n    zca_whitening=False,\n    rotation_range=False,\n    zoom_range = 0.2,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True,\n    vertical_flip=False) \n\nval_datagen = ImageDataGenerator(rescale = 1.0\/255)","7105e7b2":"train_datagen.fit(Images)\nval_datagen.fit(Images_test)","8e935ccf":"class myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    if(logs.get('val_accuracy')>0.95):\n      print(\"\\nReached 95% accuracy so cancelling training!\")\n      self.model.stop_training = True\n        \ncallbacks = myCallback()\n\nfrom keras.callbacks import ReduceLROnPlateau\nlr_reduction = ReduceLROnPlateau(monitor='val_accuracy',\n                                 patience=1, \n                                 verbose=1, \n                                 factor=0.5, \n                                 min_lr=0.000001)","e106116f":"optimizer = Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999)","de5fcb1a":"model=Sequential()\nmodel.add(Conv2D(64,(3,3),strides=1,padding='Same',activation='relu',input_shape=(Images.shape[1],Images.shape[2],3)))\nmodel.add(MaxPool2D(2,2))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128,(3,3), strides=1,padding= 'Same', activation='relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(128,(3,3), strides=1,padding= 'Same', activation='relu'))\nmodel.add(MaxPool2D(2,2))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dense(1024, activation = \"relu\"))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(6, activation = \"softmax\"))\n\nmodel.compile(optimizer = optimizer , loss = \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])","27ac1f54":"history = model.fit_generator(train_datagen.flow(Images, Labels, batch_size=32),\n                              validation_data=val_datagen.flow(Images_test, Labels_test, batch_size=128), \n                              epochs=20, verbose=1,\n                              callbacks=[callbacks, lr_reduction])","7cad137a":"pd.DataFrame(history.history)","7364799f":"def metrics_plot(history):\n  acc = history.history['accuracy']\n  val_acc = history.history['val_accuracy']\n  loss = history.history['loss']\n  val_loss = history.history['val_loss']\n\n  epochs = range(len(acc))\n\n  plt.plot(epochs, acc, 'r', label='Training accuracy')\n  plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n  plt.title('Training and validation accuracy')\n  plt.legend()\n  plt.figure()\n\n  plt.plot(epochs, loss, 'r', label='Training Loss')\n  plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n  plt.title('Training and validation loss')\n  plt.legend()\n\n  plt.show()","0c51fbed":"metrics_plot(history)","3c362fe1":"from keras.models import load_model\n\nmodel.save('CNN_model.h5')","b1fcc753":"from tensorflow.keras.applications.inception_v3 import InceptionV3","c2f011ae":"model_Inc=Sequential()\nmodel_Inc.add(InceptionV3(input_shape=(150,150,3),\n                          include_top=False,\n                          pooling='max',\n                          weights='imagenet'))","716430f5":"model_Inc.summary()","45295965":"model_Inc.layers[0].trainable=False\nmodel_Inc.summary()","21e65db6":"model_Inc.add(Dense(512,activation='relu'))\nmodel_Inc.add(Dropout(0.2))\nmodel_Inc.add(Dense(6,activation='softmax'))","f0443862":"model_Inc.summary()","96cd51ad":"model_Inc.layers","8f80e5d7":"model_Inc.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])","4e576588":"history2 = model_Inc.fit_generator(train_datagen.flow(Images, Labels, batch_size=32),\n                                   validation_data=val_datagen.flow(Images_test, Labels_test, batch_size=128), \n                                   epochs=15, verbose=1,\n                                   callbacks=[callbacks, lr_reduction])","089497a8":"pd.DataFrame(history2.history)","206de58a":"metrics_plot(history2)","8d22f044":"model_Inc.save('Inc_model.h5')","82660b3b":"from tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.applications.vgg16 import preprocess_input","a8f0e71f":"model_VGG=Sequential()\nmodel_VGG.add(VGG16(input_shape=(150,150,3),\n                    include_top=False,\n                    pooling='max',\n                    weights='imagenet'))","14edf3d6":"model_VGG.summary()","41d69bf2":"model_VGG.layers[0].trainable=False\nmodel_VGG.summary()","e3ee1e32":"model_VGG.add(Dense(512,activation='relu'))\nmodel_VGG.add(Dropout(0.2))\nmodel_VGG.add(Dense(6,activation='softmax'))","7a16833e":"model_VGG.summary()","5e4dcd49":"model_VGG.layers","27cf2faa":"model_VGG.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])","6f0a7b4b":"history3 = model_VGG.fit_generator(train_datagen.flow(Images, Labels, batch_size=32),\n                                   validation_data=val_datagen.flow(Images_test, Labels_test, batch_size=128), \n                                   epochs=15, verbose=1,\n                                   callbacks=[callbacks, lr_reduction])","4c2c221c":"pd.DataFrame(history3.history)","64056f01":"metrics_plot(history3)","207d90e9":"model_VGG.save('VGG_model.h5')","584a1d7d":"from keras.models import load_model","33b9d748":"model1 = load_model('..\/input\/loaded-models\/CNN_model.h5')\nmodel2 = load_model('..\/input\/loaded-models\/Inc_model.h5')\nmodel3 = load_model('..\/input\/loaded-models\/VGG_model.h5')","c010f439":"train_loss_cnn, train_acc_cnn = model1.evaluate(train_datagen.flow(Images, Labels, batch_size=32), verbose=2)\ntest_loss_cnn, test_acc_cnn = model1.evaluate(val_datagen.flow(Images_test, Labels_test, batch_size=128), verbose=2)","25489b6d":"train_loss_inc, train_acc_inc = model2.evaluate(train_datagen.flow(Images, Labels, batch_size=32), verbose=2)\ntest_loss_inc, test_acc_inc = model2.evaluate(val_datagen.flow(Images_test, Labels_test, batch_size=128), verbose=2)","8089d7d4":"train_loss_vgg, train_acc_vgg = model3.evaluate(train_datagen.flow(Images, Labels, batch_size=32), verbose=2)\ntest_loss_vgg, test_acc_vgg = model3.evaluate(val_datagen.flow(Images_test, Labels_test, batch_size=128), verbose=2)","57b6a3b7":"data = {'Scratch model':[train_acc_cnn,train_loss_cnn,test_acc_cnn,test_loss_cnn],\n        'InceptionV3':[train_acc_inc,train_loss_inc,test_acc_inc,test_loss_inc],\n        'VGG16': [train_acc_vgg,train_loss_vgg,test_acc_vgg,test_loss_vgg]}\n \npd.DataFrame(data, index=['Train accuracy','Train loss','Val accuracy','Val loss'])","8284eee0":"model2.summary()","faf0e03f":"predicted_test = model2.predict(Images_test\/255.0, batch_size=32)","22225d27":"predicted_test[:5]","239d7fcb":"class_pred_test = [np.argmax(i) for i in predicted_test]","59d97214":"print(class_pred_test[:5])","861c7056":"print(Labels_test[:5])","71aaec8b":"from sklearn.metrics import classification_report\n\nreport = classification_report(Labels_test, class_pred_test)\n\nprint(report)","1b2c7d16":"from sklearn.metrics import confusion_matrix\n\nf,ax = plt.subplots(figsize=(11, 11))\nconfusion_mtx = confusion_matrix(Labels_test, class_pred_test)\nsns.set(font_scale=1.4)\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\",ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix Validation set\")\nplt.show()","94c91b86":"l = []\nfor i in range(len(Labels_test)):\n    if class_pred_test[i] != Labels_test[i]:\n        l.append(i)","a8742d74":"print('Number of misclassifications in validation dataset: ', len(l))","c8867a2c":"l[:20]","947c0ba6":"plt.figure(figsize=(35,35))\nc = 1\nfor i in l[:20]:\n    plt.subplot(4,5, c)\n    plt.imshow(Images_test[i])\n    plt.title('True label:{}\\nPredicted label:{}'.format(get_classlabel(Labels_test[i]),get_classlabel(class_pred_test[i])))\n    plt.axis('off')\n    c = c+1","4dac144b":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import label_binarize","789e07be":"metrics = []\nmodels = ['InceptionV3 model']\npredictions=[class_pred_test]\n\nfor lab,i in zip(models, predictions):\n    precision, recall, fscore, _ = score(Labels_test, i, average='weighted')\n    accuracy = accuracy_score(Labels_test, i)\n    auc = roc_auc_score(label_binarize(Labels_test, classes=[0,1,2,3,4,5]),\n                        label_binarize(i, classes=[0,1,2,3,4,5]),\n                        average='weighted')\n    metrics.append(pd.Series({'precision':precision, 'recall':recall,\n                              'fscore':fscore, 'accuracy':accuracy,\n                              'auc':auc}, name=lab))\n    \nmetrics = pd.concat(metrics, axis=1)","74d1c3f9":"metrics","266b2e23":"Images_pred = []\nfor image_file in os.listdir('..\/input\/intel-image-classification\/seg_pred\/seg_pred'):\n  image = cv2.imread('..\/input\/intel-image-classification\/seg_pred\/seg_pred'+'\/'+image_file)\n  image = cv2.resize(image,(150,150)) \n  Images_pred.append(image)","3bce8421":"Images_pred = np.array(Images_pred) ","acbd1898":"Images_pred.shape","9e3c08b5":"prediction = model2.predict(Images_pred[:10]\/255.0)\n","31887937":"print(prediction)","15c03355":"class_prediction = [np.argmax(i) for i in prediction]\nprint(class_prediction)","d6177d22":"#{2:'glacier', 4:'sea', 0:'buildings', 1:'forest', 5:'street', 3:'mountain'}\n\nplt.figure(figsize=(35,35))\nc = 1\nfor i in range(10):\n    plt.subplot(2,5,c)\n    plt.imshow(Images_pred[i])\n    plt.title('Predicted label:{}'.format(get_classlabel(class_prediction[i])))\n    plt.axis('off')\n    c = c+1","8686db03":"Now that we have our train and test sets defined we can build the Image Classifiers, but before let's create a pie chart showing the classes in the label of the training set so as to confirm it is balanced:","f91c7d6d":"# VGG16:","70881d14":"The following models will be built and compared using their corresponding error measurements:\n\n- Convolutional Neural Network by scratch.\n- Pre-trained InceptionV3.\n- Pre-trained VGG16.\n\nLet's import some libraries useful in the process of building the first network by scratch:","9141d617":"Now that we have obtained all the images from the training folder let us see the shape of them to confirm that their dimensions are proper: ","303f9a6c":"Perfect!, the images list contains 14034 images sized 150x150 of height and width respectively and 3 channels (color image), whereas the label contains 14034 instances corresponding to the class of the images.","e880de29":"Above we see only 20 misclassifications and personally I've found really confusing to classify some of them, also we can see the evidence why the model had problems when trying to differentiate glacier and mountain, these are the most frequent and even for a human it becomes confusing, so a different approach must be taken given this dataset. \n\nAs a final step let's compute and show the error metrics (recall, precision, f1-score, accuracy and AUC) for the best model.","c90583c1":"The list 'l' is storing the indexes of those instances which were misclassified, having said that let's see the first 20 misclassifications and then show these images with their corresponding sigmoid output (probability):","20759084":"In the table above we can see there is not a big difference in the performance of the three models, but something important to look and take into account is that when we trained these models the metrics were different than in the table above, the reason of such change could be due to the stochastic nature of convolutional networks have when evaluating on different batches. Having said that I will continue with InceptionV3 for having the best accuracy on validation set, obviously you can choose whatever you want, but such model outperformed slightly the others.\n\nLet's see the summary of the model chosen:","0725a956":"As the images were gathered from the folders, this process was made one by one, so the first n instances corresponds to the first folder, the following m instances to the second folder so on and so forth, therefore we have to shuffle them to be used in modeling:","b477beea":"It worked perfectly and corresponds to a forest.","4105f699":"Above we can see our model had serious problems with class 2: glacier and class 3: mountain, both recall and precision were low and this corresponds to the reason why it didn't achieve a better performance, because other classes were relatively high as class 1: forest. Let's see in detail the missclassifications plotting a confusion matrix:","d740e83a":"Let's create two functions, the first one to gather the images from the directory where they are stored and label them as integer numbers from 0 to 5, the following numbers represent the classes in our label, these were defined by the author of the dataset. The second function is just to quickly get the name of the classes as next we will just see numbers in the label and remembering what they correspond can be difficult.","591cedc7":"Let us now print random samples of images for each class, the following function will help us get the indexes of the common classes:","82b02b35":"Using the os library let us show the files stored in training and testing:","2bea99c8":"Nice!, our prediction corresponds to integer numbers from 0 to 5, as we want to compare with actual label let's see the first 5 classes in the label too:","39bc8ba6":"## Convolutional Neural Network by scratch:","0640a095":"I would like to know any feedback in order to increase the performance of the models or tell me if you found a different one even better!\n\nIf you liked this notebook I would appreciate so much your upvote if you want to see more projects\/tutorials like this one. I encourage you to see my projects portfolio, am sure you will love it.\n\nThank you!","16cbc055":"## InceptionV3:","ce8d1dff":"# Modeling:","30b0f287":"Given the computed metrics for every model loaded I will summarize and show a table which can facilitate comparing them:","c838ce8e":"Let's see the classification report for this model, we can see the considerably high metrics, later we will in more detail why every model didn't achieve a perfect performance.","11dc30bc":"The following plots 3 random samples for each class, so you can see how complex are the object features in the images and start thinking how powerful should be the model to achieve a high performance:","67231c2f":"We will gather the images from testing folder in the same way as we did for training set:","75a8e61c":"The shape of the images to predict is correct and now I will only use the first 10 images from this folder in order to avoid RAM breaking:","887f7a13":"## Prediction of testing images:\nUsing the os.listdir function let's gather the images contained in the seg_pred folder and resize to 150x150, which will make them suitable to be used by the model :","012fd639":"Before creating the function to get the images and their classes we will print one sample of the training set:","f009d9f2":"The model which had the best performance was the InceptionV3 with 334 misclassification out of 3000 instances, as I said before such number can change due to stochastic nature of these models, since now I will use InceptionV3 to predict the classes of instances stored in seg_pred folder, but firstly let's plot a sample of the images misclassified and see if there is a pattern or general reason of this problem.","9be442fb":"Let's define the following function to show accuracies and losses plots just by using the history object as argument:","14bb1e53":"Training and testing files contain the images stored in unique folders representing the class of the images, therefore above we can confirm the six classes to predict in our label. Instead, 'seg_pred' just stores images without containing them into sub-folders,  listing them with os will print thousand of files, thus we will just print the number of images contained:","76b59740":"Now I'm going to load the 3 models saved and compute their corresponding metrics which should match those from the last epoch for each one.","23cf4853":"As we can see above the prediction for each instance is one-hot encoded so we need to get the index with the highest probability which in simple words corresponds to the class predicted:","a81e6d83":"Let's create two constraints or 'callbacks' which can help us improve the training (ReduceLROnPlateau) and stop the training once it has reached a high threshold (Callback):","70962fec":"We see how the accuracy increases and loss decreases for both, but as we know the prediction for the validation set tends to be a bit inaccurate, because of this let us see these four metrics in a dataframe and in a plot:"}}