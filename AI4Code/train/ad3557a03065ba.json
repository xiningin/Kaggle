{"cell_type":{"aa0976ff":"code","aa00d476":"code","4c084911":"code","64397b28":"code","b9eb620a":"code","7d4b503a":"code","731bd984":"code","c57c1206":"code","e65bb38d":"code","cf293253":"code","e4ddc9f8":"code","677865ba":"code","435b87fd":"code","03b65aed":"code","77199382":"code","de8e6cb6":"code","2224ecad":"code","c1bed7be":"code","56ad2d59":"code","bca479a5":"code","868e715d":"code","9d295777":"code","842a7a60":"code","072e5b2c":"code","ecf94e14":"code","185f7b5a":"code","8538c844":"code","2e304592":"code","b039e95a":"code","4953eb2b":"code","299b1df8":"code","a33510fa":"code","cde93cfa":"code","04d00eaa":"code","f33e67c9":"code","7d646210":"code","ced774e7":"code","b03a54aa":"code","5bd121d5":"code","2f9ee2e2":"code","f9847f20":"code","f85eb1c1":"code","f225c959":"code","006aba94":"code","4251a94c":"code","789d827f":"code","40f57b71":"code","db0c758d":"code","8f750b7f":"code","b86532b5":"code","26c986b1":"code","3aaa3e33":"code","cb35f733":"code","013f7ad5":"code","e908d573":"markdown","46d12c50":"markdown","d6ecc718":"markdown","99d7be98":"markdown","3447e601":"markdown","9325bec5":"markdown","7c14ba29":"markdown","801eefc5":"markdown","4907fb4c":"markdown","bc12edda":"markdown","7649fdf8":"markdown","d88577f2":"markdown","92ae0766":"markdown","4f78120f":"markdown","8dfb9de3":"markdown","a59ae7dd":"markdown","b7d1bdc5":"markdown","d2e7d7de":"markdown","84fdb246":"markdown","9ed58673":"markdown","21c9403e":"markdown","b46cadfe":"markdown","2e2f6d29":"markdown","c0c25959":"markdown","f008ac06":"markdown","87df3544":"markdown","a2dc3c3e":"markdown","7194cb65":"markdown","79a1c5ed":"markdown","be360eeb":"markdown","e8cecfb7":"markdown","f73e60fd":"markdown","ac372c40":"markdown","4729a2f1":"markdown","810747b3":"markdown","b03b4ae7":"markdown","b9ca3d73":"markdown","e7f4ef5e":"markdown","95c909e4":"markdown","ceff6a38":"markdown","89663e68":"markdown","533be8b7":"markdown","ffa4fe9a":"markdown","76d0b12c":"markdown"},"source":{"aa0976ff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport time\nimport datetime\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly\nimport plotly.graph_objs as gro\nfrom plotly import tools\nfrom statistics import *\nimport chart_studio.plotly as ply\nimport plotly.graph_objs as go\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\n# encoder for the nominal categorical values\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression \nfrom sklearn import ensemble\n\nfrom sklearn.svm import SVR\n\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","aa00d476":"data = pd.read_csv(\"\/kaggle\/input\/google-play-store-apps\/googleplaystore.csv\")\ndata_copy_alg=data\ndata.head()","4c084911":"data.shape","64397b28":"data.info()\n#we see that although there are 10841 rows to the dataset, there are null values amongst\n#we want to reduce in order to be able to use the data for prediction","b9eb620a":"total_data_number = data.isnull().sum().sort_values(ascending=False)\npercent_data_null = (data.isnull().sum()\/data.count()).sort_values(ascending=False)\nmissing_data_array = pd.concat([total_data_number, percent_data_null * 100], axis=1, keys=['Total', 'Percent'])\nmissing_data_array.head(15)","7d4b503a":"#fillna with median for rating since we have 1500 rows with nulls, we would loose a lot of data \ndata['Rating'] = data['Rating'].fillna(data['Rating'].mean())\n\ncleaned_data = data.dropna()\n#dropna for other columns\n# cleaned data\ntotal_data_number_after = cleaned_data.isnull().sum().sort_values(ascending=False)\npercent_data_null_after = (cleaned_data.isnull().sum()\/cleaned_data.count()).sort_values(ascending=False)\nmissing_data_array_after = pd.concat([total_data_number_after, percent_data_null_after], axis=1, keys=['Total', 'Percent'])\nmissing_data_array_after.head(15)\n\n","731bd984":"# total duplicated entries by the \"App\" & \"Current Ver\" columns 1155\nduplicates_app_categ = cleaned_data[cleaned_data.duplicated(['App', 'Current Ver'])]\n\nprint(\"Number of duplicates {}\".format(duplicates_app_categ.shape))\nprint(\"Expected number of rows after cleanup: {}\".format(cleaned_data.shape[0] - duplicates_app_categ.shape[0]))","c57c1206":"# Return DataFrame with duplicate rows removed, optionally only considering certain columns. Indexes, including time indexes are ignored.\ncleaned_data = cleaned_data.drop_duplicates(['App', 'Current Ver'])\nprint(\"Final table shape without duplicates in 'App' and 'Current Ver': __{}__\".format(cleaned_data.shape))","e65bb38d":"# Just replace whitespaces with underscore in the headers\ncleaned_data.columns = cleaned_data.columns.str.replace(' ', '_')","cf293253":"# display first 10 number of reviews\ncleaned_data['Reviews'].head(10)","e4ddc9f8":"# Let's change each review to int\ncleaned_data['Reviews'] = cleaned_data['Reviews'].apply(lambda review: int(review))","677865ba":"# display first 10 sizes\ncleaned_data['Size'].head(10)","435b87fd":"# Size Frequencies\ncleaned_data.Size.value_counts().head(10) # 10 rows selected ","03b65aed":"cleaned_data['Size'] = cleaned_data['Size'].apply(lambda size: str(size).replace('M', '') if 'M' in str(size) else size)\ncleaned_data['Size'] = cleaned_data['Size'].apply(lambda size: str(size).replace(',', '') if 'M' in str(size) else size)\n\n# we remove k but we also divide it by 1000 to have a standardized column for the application sizes\ncleaned_data['Size'] = cleaned_data['Size'].apply(lambda size: float(str(size).replace('k', '')) \/ 1000 if 'k' in str(size) else size)","77199382":"redundant_string = 'Varies with device'\n\nprint(\"There are a total of {} rows with the 'Size' column containing '{}' string.\".format(cleaned_data.loc[cleaned_data['Size'] == redundant_string].shape[0], redundant_string))\n\n# Let's replace the redundant string with the average of the Size column, because the number of values that have the redundant string is pretty high and we wouldn't want to loose that data\ncleaned_data.head()\navg_value=np.median(cleaned_data['Size'].str.isnumeric().astype(float))\n\ncleaned_data['Size'] = cleaned_data['Size'].apply(lambda x: str(x).replace(redundant_string, str(0)) if redundant_string in str(x) else x)\n\nprint(\"There are a total of {} rows with the 'Size' column containing '{}' string.\".format(cleaned_data.loc[cleaned_data['Size'] == redundant_string].shape[0], redundant_string))","de8e6cb6":"cleaned_data['Size'] = cleaned_data['Size'].apply(lambda size: float(size))\ncleaned_data.head()","2224ecad":"# display first 10 total installs\ncleaned_data['Installs'].head(10)","c1bed7be":"# Install Frequencies\ncleaned_data.Installs.value_counts().head(10) # 10 rows selected ","56ad2d59":"# We remove the signs from the installs column\n\ncleaned_data['Installs'] = cleaned_data['Installs'].apply(lambda x: x.replace('+', '') if '+' in str(x) else x)\ncleaned_data['Installs'] = cleaned_data['Installs'].apply(lambda x: x.replace(',', '') if ',' in str(x) else x)\ncleaned_data['Installs'] = cleaned_data['Installs'].apply(lambda x: int(x))","bca479a5":"# display first 5 prices\nprint(cleaned_data['Price'].head(5))\n\n# display first 5 prices with a value bigger than 0\nprint(cleaned_data[cleaned_data['Price'] != 0]['Price'].head(5))","868e715d":"paid_apps = cleaned_data[cleaned_data.Price != '0'].shape[0]\nprint(\"Total free apps {}\".format(cleaned_data.shape[0]- paid_apps))\nprint(\"Total paid apps {}\".format(paid_apps))","9d295777":"cleaned_data['Price'] = cleaned_data['Price'].apply(lambda x: str(x).replace('$', '') if '$' in str(x) else str(x))\ncleaned_data['Price'] = cleaned_data['Price'].apply(lambda x: float(x))\n","842a7a60":"# display first 5 genres\nprint(cleaned_data['Genres'].head(5))","072e5b2c":"result = cleaned_data[cleaned_data.Genres == 'Adventure;Action & Adventure']\ngenres = np.unique(cleaned_data['Genres'])\nprint(\"There are a total of {} initial genres\".format(genres.shape[0]))","ecf94e14":"# The idea is that we want to reduce redudant genres or combine \n# the ones which are of the same genre but are marked with different ones;\n# More can be added \/ mixed together; There is no rule but it is about having some sort of a\n# correct intuition which is needed in order to logically group them together;\nnew_genres = {\n'Adventure;Action & Adventure'  :  'Action;Action & Adventure',\n'Educational;Action & Adventure' : 'Action & Adventure',\n'Adventure;Brain Games'   :  'Adventure',\n'Adventure;Education'   : 'Adventure',\n'Arcade;Pretend Play'   : 'Arcade',\n'Art & Design;Pretend Play' : 'Art & Design;Creativity',\n'Board;Pretend Play'  : 'Board;Brain Games',\n'Books & Reference'  : 'Education',\n'Communication;Creativity' : 'Communication',\n'Educational;Education'   : 'Education',\n'Educational' : 'Education',\n'Educational;Brain Games': 'Education;Brain Games',\n'Educational;Creativity': 'Education;Creativity',\n'Educational;Pretend Play': 'Education;Pretend Play',\n'Music;Music & Video' : 'Music',\n'Lifestyle;Pretend Play': 'Lifestyle',\n'Simulation;Education': 'Simulation',\n'Simulation;Pretend Play' : 'Simulation' \n}\n\nfor old, new in new_genres.items():\n    print(\"Replacing [{}] GENRE with [{}] GENRE\".format(old, new))\n    cleaned_data['Genres'] = cleaned_data['Genres'].apply(lambda x: x.replace(old, new) if old in str(x) else x)\n","185f7b5a":"# just checking the results here \ncleaned_data[cleaned_data.Genres == 'Art & Design;Creativity']\n\ngenres = np.unique(cleaned_data['Genres'])\nprint(\"There are a total of {} initial genres\".format(genres.shape[0]))","8538c844":"final_data = cleaned_data.copy()","2e304592":"avg_rating = gro.Histogram(\n    \n    x=final_data.Rating,\n    name='Average Rating',\n    xbins = {'start': 1, 'size': 0.1, 'end' :5},\n    marker=dict(\n        color='#546FDE',\n    ),\n    opacity=0.75\n)\n\nfig = tools.make_subplots(rows=1, cols=1)\nfig.append_trace(avg_rating, 1, 1)\nfig.show()\n\nprint('Average Rating {}'.format(np.mean(final_data['Rating'])))\nprint('Mean Rating {}'.format(np.median(final_data['Rating'])))","b039e95a":"counts = final_data['Category'].value_counts()","4953eb2b":"dict_counts=counts.to_dict()\nz = plt.pie([float(v) for v in dict_counts.values()], labels=[str(k) for k in dict_counts],radius=5, autopct='%.2f')","299b1df8":"list_of_categories = final_data['Category'].unique().tolist()","a33510fa":"fig, axes = plt.subplots(nrows = 17, ncols = 2)\n\nfig.set_figheight(160)\nfig.set_figwidth(40)\n\nc = 0\n\nfor row in range(0, axes.shape[0]):\n    for column in range(0, axes.shape[1]):\n        if c == len(list_of_categories):\n            break\n        current_subplot = axes[row, column]\n        category = list_of_categories[c]\n        filtered = final_data.loc[final_data['Category'] == category]['Rating']\n        current_subplot.hist(filtered)\n        current_subplot.set_title(category,fontsize=20)\n        current_subplot.grid()\n        c += 1\n","cde93cfa":"grouped_by_categ = final_data.groupby('Category').filter(lambda x: len(x) >= 100).reset_index()\n\n\n\nc = ['hsl('+str(h)+',50%'+',50%)' for h in np.linspace(0, 720, len(set(grouped_by_categ.Category)))]\n\n\nlayout = {'title' : 'App ratings across genres',\n        'xaxis': {'tickangle':-40},\n        'yaxis': {'title': 'Rating'},\n          'plot_bgcolor': 'rgb(170,250,200)',\n          'shapes': [{\n              'type' :'line',\n              'x0': -.5,\n              'y0': np.nanmean(list(grouped_by_categ.Rating)),\n              'x1': 23,\n              'y1': np.nanmean(list(grouped_by_categ.Rating)),\n              'line': { 'dash': 'dashdot'}\n          }]\n          }\n\ndata = [{\n    'y': final_data.loc[final_data.Category==cat]['Rating'], \n    'type':'violin',\n    'name' : cat,\n    'showlegend':False,\n   \n    } for i,cat in enumerate(list(set(grouped_by_categ.Category)))]\n\n\n\nplotly.offline.iplot({'data': data, 'layout': layout})\n","04d00eaa":"plt.figure(figsize=(10,10))\n\nlbl_ext=['Applications < 100 k downloads', 'Applications >= 100 k downloads']\npie_size_high_low=list()\npie_size_high_low.append(final_data['App'][final_data['Installs']<100000].count()) \npie_size_high_low.append(final_data['App'][final_data['Installs']>=100000].count()) \n\nlbl_interior=['Paid', 'Free', 'Paid', 'Free']\npie_inner_size=list()\npie_inner_size.append(final_data['Type'][final_data['Type']=='Paid'][final_data['Installs']<100000].count()) \npie_inner_size.append(final_data['Type'][final_data['Type']=='Free'][final_data['Installs']<100000].count()) \npie_inner_size.append(final_data['Type'][final_data['Type']=='Paid'][final_data['Installs']>=100000].count()) \npie_inner_size.append(final_data['Type'][final_data['Type']=='Free'][final_data['Installs']>=100000].count())\n\n\ncolors = ['#ff9933', '#ffff00']\ncolors_inner = ['#1a1aff','#006600', '#1a1aff','#006600']\n\nexplode = (0,0) \nexplode_inner = (0.1,0.1,0.1,0.1)\n\n#pie chart with apps that are downloaded vs apps that are not\nplt.pie(pie_size_high_low,explode=explode,labels=lbl_ext, radius=2, colors=colors, textprops={'fontsize': 18})\n#pie chart with apps interior with free vs paid apps\nplt.pie(pie_inner_size,explode=explode_inner,labels=lbl_interior, radius=1, colors=colors_inner, textprops={'fontsize': 18})\n       \n\n","f33e67c9":"plt.figure(figsize=(10,10))\n\nlbl_ext=['Applications < 4.2 rating', 'Applications >= 4.2 rating']\npie_size_high_low=list()\npie_size_high_low.append(final_data['App'][final_data['Rating']<4.2].count()) \npie_size_high_low.append(final_data['App'][final_data['Rating']>=4.2].count()) \n\nlbl_interior=['Paid', 'Free', 'Paid', 'Free']\npie_inner_size=list()\npie_inner_size.append(final_data['Type'][final_data['Type']=='Paid'][final_data['Rating']<4.2].count()) \npie_inner_size.append(final_data['Type'][final_data['Type']=='Free'][final_data['Rating']<4.2].count()) \npie_inner_size.append(final_data['Type'][final_data['Type']=='Paid'][final_data['Rating']>=4.2].count()) \npie_inner_size.append(final_data['Type'][final_data['Type']=='Free'][final_data['Rating']>=4.2].count())\n\n\ncolors = ['#ff9933', '#ffff00']\ncolors_inner = ['#1a1aff','#006600', '#1a1aff','#006600']\n\nexplode = (0,0) \nexplode_inner = (0.1,0.1,0.1,0.1)\n\n#pie chart with apps that are downloaded vs apps that are not\nplt.pie(pie_size_high_low,explode=explode,labels=lbl_ext, radius=2, colors=colors, textprops={'fontsize': 18})\n#pie chart with apps interior with free vs paid apps\nplt.pie(pie_inner_size,explode=explode_inner,labels=lbl_interior, radius=1, colors=colors_inner, textprops={'fontsize': 18})\n       \n\n","7d646210":"install_data = final_data['Installs'][final_data.Installs !=0]\nreview_data = final_data['Reviews'][final_data.Reviews !=0]\napp_type_data = final_data['Type']\nprice_data = final_data['Price']\n\n\n#using log to better represent the installs and reviews since these tend to get very large\np = sns.pairplot(pd.DataFrame(list(zip(final_data['Rating'], \n                                       final_data['Size'], \n                                       np.log10(install_data), \n                                       np.log10(review_data), \n                                       app_type_data, \n                                       price_data)), \n                columns=['Rating','Size', 'Installs', 'Reviews', 'Type', 'Price']), \n                hue='Type', palette=\"Set1\")","ced774e7":"# Last Updated encoding\nfinal_data['Last_Updated'] = final_data['Last_Updated'].apply(lambda x : time.mktime(datetime.datetime.strptime(x, '%B %d, %Y').timetuple()))\nfinal_data.head()\n#we want to see if there is a correlation between the last update date of the application\n#and its rating, but for this we need to have that date in a kind of numeric format, so we change it to the number of days\n#there have been from the last update compared to the most recently updated app","b03a54aa":"relationships = final_data.corr()","5bd121d5":"trace = go.Heatmap(z=[relationships.Rating.values.tolist(), relationships.Reviews.values.tolist(), relationships.Size.values.tolist(), relationships.Installs.values.tolist(), relationships.Price.values.tolist(), relationships.Last_Updated.values.tolist()],\n                   x=[relationships.columns[0], relationships.columns[1], relationships.columns[2], relationships.columns[3], relationships.columns[4],relationships.columns[5]],\n                   y=[relationships.columns[0], relationships.columns[1], relationships.columns[2], relationships.columns[3], relationships.columns[4], relationships.columns[5]])\ncorelation=[trace]\npy.iplot(corelation, filename='heatmap')","2f9ee2e2":"plt.figure(figsize = (12,10))\n#print(final_data[final_data['Reviews']>=5000000].count()[0]) -result is 83\nsns.regplot(x=\"Reviews\", y=\"Rating\", color = 'green',data=final_data[final_data['Reviews']<=5000000]);\nplt.title('Influence of Reviews on Rating',size = 25)\n","f9847f20":"plt.figure(figsize = (12,10))\n\nsns.regplot(x=\"Installs\", y=\"Rating\", color = 'blue',data=final_data);\nplt.title('Influence of Installs on Rating',size = 25)\n","f85eb1c1":"plt.figure(figsize = (12,10))\n\nsns.regplot(x=\"Price\", y=\"Rating\", color = 'red',data=final_data);\nplt.title('Influence of Price on Rating',size = 25)","f225c959":"plt.figure(figsize = (12,10))\n\nsns.regplot(x=\"Last_Updated\", y=\"Rating\", color = 'yellow',data=final_data);\nplt.title('Influence of when app was Last Updated on Rating',size = 25)\n\n","006aba94":"#using a function to evaluate the accuracy will give us a standardised metric to pass all our methods through\n#so we can know which one performed best\ndef AccuracyMetrics(dataset_y, predicted_y, printing=1):\n    if printing==1:\n        print ('mean squared error: '+ str(metrics.mean_squared_error(dataset_y,predicted_y)))\n        print ('mean absolute error: '+ str(metrics.mean_absolute_error(dataset_y,predicted_y)))\n        print ('mean squared log error: '+ str(metrics.mean_squared_log_error(dataset_y,predicted_y)))\n    return metrics.mean_squared_error(dataset_y,predicted_y)\n\n\n\n","4251a94c":"categories=pd.get_dummies(final_data['Category'],prefix='catg',drop_first=True)\ncategories=pd.get_dummies(final_data['Genres'],prefix='gen',drop_first=True)\ntypes=pd.get_dummies(final_data['Type'],prefix='typ',drop_first=True)\ncontentRatings=pd.get_dummies(final_data['Content_Rating'],prefix='cr',drop_first=True)\nnew_data=[final_data,categories,types,contentRatings]\nfinal_data=pd.concat(new_data,axis=1)\nfinal_data.drop(['Category','Type','Content_Rating','Genres'],axis=1,inplace=True)\nfinal_data.head()\n","789d827f":"X=final_data.drop('Rating',axis=1)\n\n# take out the Rating which will be predicted\ny=final_data['Rating'].values\n#move it into y which will be the predicted vector and transform to int\ny=y.astype('int')\nX=X.drop(['App','Current_Ver','Android_Ver'],axis=1)\nX.head()\n\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=0)","40f57b71":"model = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred=model.predict(X_test)\naccuracy = AccuracyMetrics(y_test,y_pred,1)\n\nprint (accuracy)","db0c758d":"model = KNeighborsRegressor(n_neighbors=200,weights='distance')\nmodel.fit(X_train, y_train)\ny_pred=model.predict(X_test)\n\n","8f750b7f":"accuracy = AccuracyMetrics(y_test,y_pred,1)\n\nprint (accuracy)\n# Try different numbers of n_estimators - this will take a minute or so\nn_neighbors = np.arange(1, 200, 1)\nscores = []\nfor n in n_neighbors:\n    model.set_params(n_neighbors=n)\n    model.fit(X_train, y_train)\n    y_pred=model.predict(X_test)\n    scores.append(AccuracyMetrics(y_test, y_pred,0))\n    \nplt.figure(figsize=(7, 5))\nplt.title(\"Effect of Neighbours\")\nplt.xlabel(\"Number of Neighbors K\")\nplt.ylabel(\"Mean Squared Error\")\nplt.plot(n_neighbors, scores)\n","b86532b5":"RFregressor = RandomForestRegressor(max_depth=2, random_state=0)\nRFregressor.fit(X, y)\ny_pred=RFregressor.predict(X_test)\nprint(y_pred)\n\naccuracy = AccuracyMetrics(y_test,y_pred,1)\nprint (accuracy)\n","26c986b1":"estimators = [10,20,30,40,50,60,70,80,90,100]\nmax_depth=[5,10,15,20,25,30,35,40,45,50]\n\nscores = []\nminimum_score=100;\nbest_estimator=0;\nbest_depth=0;\nfor n in estimators:\n    new_score_line=[]\n    for d in max_depth:\n        RFregressor.set_params(n_estimators=n, max_depth=d)\n        RFregressor.fit(X_train, y_train)\n        y_pred=RFregressor.predict(X_test)\n        mse=AccuracyMetrics(y_test, y_pred,0)\n        if (mse<minimum_score):\n            minimum_score=mse\n            best_estimator=n\n            best_depth=d\n        new_score_line.append(mse)\n    scores.append(new_score_line)\n\nprint(minimum_score)\nprint(best_estimator)\nprint(best_depth)\nprint(scores)\nheaatmap_data = go.Heatmap(z=scores,\n                   x=max_depth,\n                   y=estimators)\nheatmapm=[heaatmap_data]\npy.iplot(heatmapm, filename='heatmap')","3aaa3e33":"SVRegressor = SVR(C=1.0, epsilon=0.2)\nSVRegressor.fit(X, y)\ny_pred=SVRegressor.predict(X_test)\nprint(y_pred)\n\naccuracy_initial = AccuracyMetrics(y_test,y_pred,1)\nprint (accuracy)","cb35f733":"kernels = ['sigmoid','rbf']\n\n\nsvr_scores = []\nminimum_score=100;\nbest_kernel=0;\nfor k in kernels:\n   \n    \n    SVRegressor.set_params(kernel=k)\n    SVRegressor.fit(X_train, y_train)\n    y_pred=SVRegressor.predict(X_test)\n    mse=AccuracyMetrics(y_test, y_pred,0)\n    svr_scores.append(mse)\n    if (mse<minimum_score):\n        minimum_score=mse\n        best_kernel=k\n            \n      \n    \n\nprint(minimum_score)\nprint(best_kernel)\nprint(svr_scores)\nsvr_scores.append(accuracy_initial)\nkernels.append('linear')\nplt.figure(figsize=(7, 5))\nplt.title(\"Effect of Kernel Type\")\nplt.xlabel(\"Kernel Type\")\nplt.ylabel(\"Mean Squared Error\")\nplt.scatter(kernels, svr_scores)","013f7ad5":"params = {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2,\n          'learning_rate': 0.01, 'loss': 'ls'}\nGBSRegressor = ensemble.GradientBoostingRegressor(**params)\nGBSRegressor.fit(X_train, y_train)\ny_pred=GBSRegressor.predict(X_test)\nmse=AccuracyMetrics(y_test, y_pred,0)\nprint(mse)\n\n\n","e908d573":" <a id=\"svr\">Support Vector Regression<\/a>","46d12c50":"## <a id=\"gdc\">Generic Data Cleaning<\/a>","d6ecc718":" <a id=\"knn\">K Nearest Neighbours<\/a>","99d7be98":"As we can observe above all the sizes in our dataset are now set to a standard measurement data, which is megabytes","3447e601":"### <a id='price'>Price<\/a>","9325bec5":"### <a id='size'>Size<\/a>","7c14ba29":"### Universitatea Bucuresti Facultatea de Informatica ","801eefc5":"> We see that most of the apps, the big numbers are somewhere around the 4.5 rating, and the average rating for the entire dataset is 4.17 stars  and the mean rating is 4.2 stars\n","4907fb4c":"### <a id=\"pie_chart\">Pie Chart<\/a>\nWe try to see what categories of apps are most popular in our dataset, Family and Games seem to be the most popular, Tools being on the first place. The other percentages are divided across a vast number of categories.\n","bc12edda":"Most app categories perform similarly, with a few overpeformers such as **books_and_reference and health_and_fitness** and some underperformers as well such as **dating**","7649fdf8":"Here we do the same analysis as above, we see the points being just on specific lines and this does make sense since Google Play doesn't tell you the exact number of installs but rather if it is above a threshold.\n\nThe tendendcy is similar to what we have seen in the Reviews, apps with high numbers of Installs will be generally more well received and have a higher rating. Popular apps are better rated.\n","d88577f2":"### <a id='genre'>Genre<\/a>","92ae0766":"By visually inspecting the above table we can see that the highest number of null values is in the variable we will be trying to predict, Rating.\n\nThe two strategies that we can use to handle such cases are either dropping the rows with null values or replacing it with a median, or mean value of the column. We have chosen to fill the values so that we don't loose a lot of data.","4f78120f":"**Categorical Data Encoding**","8dfb9de3":"### <a id=\"lin_reg\">Linear Regression Plot Fits<\/a>","a59ae7dd":" <a id=\"random_forrest\">Random Forrest Regression<\/a>","b7d1bdc5":"### <a id=\"most_downloaded\">Most downloaded apps and their type: paid or free<\/a>","d2e7d7de":"## DATA CLEANING","84fdb246":"We would be interested to check is if there is a correlation between application category and application rating, do some applications perform better as an overall of their category compaired to others?","9ed58673":"It is very interesting that here we observe the inverse tendency of what we have observed in the previous two analysis. \n\nPeople seem to want more from apps with bigger price tags and thus apps with higher prices get ratings smaller than those that are free or cost a small amount.","21c9403e":"# Google play apps dataset - exploratory data analysis","b46cadfe":"### <a id=\"prediction\">Predicting Application Ratings<\/a>\n\nWe first need to define a function that will calculate how well our algorithms perform, so we can measure their performance\n\n\n","2e2f6d29":"Let's remove M from the size and also divide each number followed by k wtih 1000 while also removing k.","c0c25959":"We apply a filter on the applications, to keep just those with <5 000 000 ratings as there are very few that have more than that (83) apps, some of these few having incredibly high numbers of reviews and it would distort the graph, all the points would be very crowded and it wouldn't be as clear.\n\nWe can observe a basic tendency, that applications with more reviews also get higher ratings. This is a logical finding as you would presume that apps that are used a lot by people would be good apps.\n\nWe should assume that reviews will be a good predictor for rating.","f008ac06":"### <a id=\"pairplot\">Pairplot -  analysis used to identify patterns or associations between features.<\/a>","87df3544":"Removing the + here is a very logical operation, as this it wouldn't be in a numeric format with the pluses and this is just the format that Google Play uses, it wouldn't bring any additional information that we could use in the regression\n","a2dc3c3e":"## Exploratory Data Analysis","7194cb65":"### <a id=\"heatmap\">Heatmap<\/a>","79a1c5ed":"### <a id=\"rating_by_categories\">Rating by Categories<\/a>","be360eeb":" <a id=\"gbr\">Gradient Boosting Regression<\/a>","e8cecfb7":"Removing the $ sign as done to other columns so we can have a numeric column","f73e60fd":"\n## Project Content\n\n\n#### General Data Manipulation<br\/>\n* [Data Import](#data_import)\n* [Generic Data Cleaning](#gdc)\n\n#### Cleaning each feature one by one\n\n* [Reviews](#reviews)\n* [Size](#size)\n* [Installs](#installs)\n* [Price](#price)\n* [Genre](#genre)\n\n#### Exploratory Data Analysis\n* [Rating Histogram](#rating_histogram)\n* [Pie Chart](#pie_chart)\n* [Rating by Categories](#rating_by_categories)\n* [Apps with most downloads](#most_downloaded)\n* [Pairplot](#pairplot)\n* [Heatmap](#heatmap)\n* [Linear Regression Plot Fits](#lin_reg)\n\n\n#### Predicting Application Ratings\n* [Linear Regression](#lin_reg_algo)\n* [K Nearest Neighbours](#knn)\n* [Random Forrest Regression](#random_forrest)\n* [Support Vector Regression](#svr)\n* [Gradient Boosting Regression](#gbr)\n","ac372c40":" <a id=\"lin_reg_algo\">Linear Regression<\/a>","4729a2f1":"### <a id='reviews'>Reviews<\/a>","810747b3":"Computing this heatmap should give us a basic intuition on what we should be looking at that influences the rating most.\n\nWe can see that the things that are most correlated to changes in rating are when the app was **last_updated***, the app's **reviews and rating**. We will investigate this further","b03b4ae7":"It is obvious that some of the genres must be combined with others..\nThis is a good example of inconsistent data which must be cleaned","b9ca3d73":"Analyzing the data, we can also see a  correlation between when the app was last updated and its rating. It makes sense since apps where the developers keep working recieve better ratings since they implement the user feedback, but abandoned apps remain with bugs, etc. and receive bad ratings.","e7f4ef5e":"###          Students: Constantin Dinu Vasiliu, Lucian Nut","95c909e4":"### <a id='installs'>Installs<\/a>","ceff6a38":"### Cleaning each feature one by one","89663e68":"Also we need to split the data into train and test so we can fit the algorithm on a set of data and then check it's accuracy on the test set. We choose a split of 75% train 25% test which is actually pretty common.","533be8b7":"### <a id=\"data_import\">Data Import<\/a>","ffa4fe9a":"### <a id=\"rating_histogram\">Rating Histogram<\/a>","76d0b12c":"### General dataset information"}}