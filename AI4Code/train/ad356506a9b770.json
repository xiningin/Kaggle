{"cell_type":{"e8eb5954":"code","6f693fb0":"code","b86f4616":"code","dce6c000":"code","382a7eb8":"code","ea1d46e3":"code","4943a0e0":"code","0cca0cc6":"code","2f7d36f3":"code","32645756":"code","26740757":"code","fe66779c":"code","33c3b4a5":"code","c63b5047":"code","4a4cab84":"code","0dc23ad6":"code","883090d1":"code","61bd306a":"code","51641fdf":"code","4ca5714d":"code","1762877c":"code","13ddc3ae":"code","3628c501":"code","c5db6eb3":"code","a949af2b":"code","75b4f411":"code","842384bb":"code","d8b7eb60":"code","df915f64":"code","2896d16e":"code","1f621218":"code","fa26c178":"code","ba6ec160":"code","fe62ebfa":"code","e5819ba0":"code","7aaa39ac":"code","b3908992":"code","0a0f6f73":"code","80c7115d":"code","35841127":"code","2a705a06":"code","f5729199":"code","e9d94a53":"code","777511f6":"code","09410ab0":"code","4dcebb4f":"code","de65bd41":"code","2150b56d":"code","cdbfcb8c":"code","120db895":"code","231dd762":"markdown","0b18c685":"markdown","e2d75074":"markdown","c9c792ab":"markdown","414864bf":"markdown","b6e65d16":"markdown","42e2f3d9":"markdown","62d28b82":"markdown","7f4eaf9f":"markdown","08e72d38":"markdown","2adc5fea":"markdown","d44aa91b":"markdown","441cc457":"markdown"},"source":{"e8eb5954":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6f693fb0":"path = '\/kaggle\/input\/yds_data.csv'","b86f4616":"data = pd.read_csv(path) # reading data from the CSV file\ndata.head()","dce6c000":"data.shape # checking how many rows and columns are in the data","382a7eb8":"data.columns","ea1d46e3":"# Using descriptive Statistics to find some insights\ndata.describe()","4943a0e0":"# Finding the dtypes of Columns to get some Insights\ndata.info()","0cca0cc6":"# Percentage and Sum of Missing values in each Columns\nmissing_data = pd.DataFrame({'total_missing': data.isnull().sum(), 'perc_missing': (data.isnull().sum()\/data.shape[0])*100})\nmissing_data","2f7d36f3":"# Exploring The Target Variable 'is_goal'\ndata.is_goal.value_counts()","32645756":"#1. Droping Unnecessary Columns\ndata.drop([\"Unnamed: 0\",  'remaining_min.1', 'power_of_shot.1','knockout_match.1', 'remaining_sec.1', 'distance_of_shot.1'], axis=1, inplace=True)","26740757":"data.head() # looking at the dataset after transformation","fe66779c":"data.columns # to see if the columns are dropped succesfully","33c3b4a5":"#2. Changing dtypes to datetime\ndata.date_of_game = pd.to_datetime(data.date_of_game, errors='coerce')\ndata['game_season'] = data['game_season'].astype('object')\ndata['game_season']","c63b5047":"# Labelencoding the 'game_season' ","4a4cab84":"l_unique = data['game_season'].unique() # fteching out the unique values from game_season\/\nl_unique","0dc23ad6":"v_unique = np.arange(len(l_unique)) # obtaining values in the range of the length of I_unique\nv_unique","883090d1":"data['game_season'].replace(to_replace=l_unique, value=v_unique, inplace=True) # replacing categorical data with numerical values\ndata['game_season'].head()","61bd306a":"data['game_season'] = data['game_season'].astype('int') # converting the datatype of the column from int64 to int32\ndata['game_season'].head()","51641fdf":"# Filling NaN values in Column \"remaining_sec\" with MEAN\ndata['power_of_shot'].fillna(value=data['power_of_shot'].mean(), inplace=True)\ndata.isnull().sum() # number of missing values for power_of_shot column should be zero\n","4ca5714d":"# Filling NaN values in Column \"type_of_combined_shot\" with MODE\nmode_com  = data.type_of_combined_shot.value_counts().keys()[0]\nprint('moded is: ',mode_com)\ndata.type_of_combined_shot.fillna(value=mode_com, inplace=True)\ndata.isnull().sum() # number of missing values for type_of_combined_shot column should be zero","1762877c":"# Filling NaN values in Column \"remaining_sec\" with MEDIAN\ndata.remaining_sec.fillna(value=data.remaining_sec.median(), inplace=True)\ndata.isnull().sum() # number of missing values for remaining_sec column should be zero","13ddc3ae":"# Shot_id_no.\ndata.shot_id_number = pd.Series(np.arange(1,data.shot_id_number.shape[0]+1))\ndata.isnull().sum() # number of missing values for shot_id_number column should be zero","3628c501":"# Filling NaN values in Columns \"location_x\" and \"location_y\" with 0\ndata['location_x'].fillna(value=0, inplace=True)\ndata['location_y'].fillna(value=0, inplace=True)\ndata.isnull().sum() # number of missing values for location_x and location_y columns should be zero","c5db6eb3":"# Using Forward Filling method in appropriate Columns\nprint('Null values in column home\/away before forward fill =',data['home\/away'].isnull().sum())\ncol = ['home\/away','lat\/lng', 'team_name','match_id','match_event_id', 'team_id', 'remaining_min', 'knockout_match',  'game_season' ]\ndata.loc[:,col] = data.loc[:,col].ffill()\nprint('Null values in column home\/away after the forward fill =',data['home\/away'].isnull().sum())","a949af2b":"# Filling Missing Values In \"shot_basics\" based on \"range_of_short\" column!\n# if the range of the shot is 16-24 ft it's a mid range shot\n\ndata.loc[(data.range_of_shot == '16-24 ft.'), 'shot_basics'] = data[data.range_of_shot == '16-24 ft.'].shot_basics.fillna(value='Mid Range')\n\n# if the range of the shot is less than 8 ft then randomly assign goal line or goal area value to the shot \n\ndata.loc[(data.range_of_shot == 'Less Than 8 ft.')&(data.shot_basics.isnull()), 'shot_basics']   =  pd.Series(data[(data.range_of_shot == 'Less Than 8 ft.')&(data.shot_basics.isnull())].shot_basics.apply(lambda x: x if type(x)==str else np.random.choice(['Goal Area', 'Goal Line'],1,p=[0.7590347263095939, 0.24096527369040613])[0]))\n\n# if the range of the shot is  8-16 ft then randomly assign goal line or mid range value to the shot\n\ndata.loc[(data.range_of_shot == '8-16 ft.')&(data.shot_basics.isnull()), 'shot_basics']          =  pd.Series(data[(data.range_of_shot == '8-16 ft.')&(data.shot_basics.isnull())].shot_basics.apply(lambda x: x if type(x)==str else np.random.choice(['Mid Range', 'Goal Line'],1,p=[0.6488754615642833, 0.35112453843571667])[0]))\n\n# if the range of the shot is more than 24 ft then randomly assign one of the values from'Penalty Spot', 'Right Corner', 'Left Corner' to shot_basic field\n\ndata.loc[(data.range_of_shot == '24+ ft.')&(data.shot_basics.isnull()), 'shot_basics']            =  pd.Series(data[(data.range_of_shot == '24+ ft.')&(data.shot_basics.isnull())].shot_basics.apply(lambda x: x if type(x)==str else np.random.choice(['Penalty Spot', 'Right Corner', 'Left Corner'],1,p=[0.8932384341637011, 0.06192170818505338, 0.044839857651245554])[0]))\n\n# if the shot is a back court shot then randomly assign one of the values from''Mid Ground Line', 'Penalty Spot' to shot_basic field\n\ndata.loc[(data.range_of_shot == 'Back Court Shot')&(data.shot_basics.isnull()), 'shot_basics']    =  pd.Series(data[(data.range_of_shot == 'Back Court Shot')&(data.shot_basics.isnull())].shot_basics.apply(lambda x: x if type(x)==str else np.random.choice(['Mid Ground Line', 'Penalty Spot'],1,p=[0.8441558441558441, 0.15584415584415584])[0]))\ndata.isna().sum()","75b4f411":"data['shot_basics'].unique() # now we have populated the shot types and reduced the number of missing values. Earlier we had 1575 missing values for this column, now we have only 66.","842384bb":"# Filling Missing Values In \"range_of_short\" based on \"short_basics\" column!\n\n# if shot_basics is Goal Area, then range of shot is Less Than 8 ft\n\ndata.loc[(data.shot_basics == 'Goal Area'), 'range_of_shot']       = data[data.shot_basics == 'Goal Area'].range_of_shot.fillna(value='Less Than 8 ft.')\n# if shot_basics is Penalty Spot, then range of shot is  24+ ft.\n\ndata.loc[(data.shot_basics == 'Penalty Spot'), 'range_of_shot']    = data[data.shot_basics == 'Penalty Spot'].range_of_shot.fillna(value= '24+ ft.')\n# if shot_basics is Right Corner, then range of shot is  24+ ft.\n\ndata.loc[(data.shot_basics == 'Right Corner'), 'range_of_shot']    = data[data.shot_basics == 'Right Corner'].range_of_shot.fillna(value='24+ ft.')\n# if shot_basics is Left Corner, then range of shot is  24+ ft.\n\ndata.loc[(data.shot_basics == 'Left Corner'), 'range_of_shot']     = data[data.shot_basics == 'Left Corner'].range_of_shot.fillna(value='24+ ft.')\n# if shot_basics is Mid Ground Line , then range of shot is  Back Court Shot\n\ndata.loc[(data.shot_basics == 'Mid Ground Line'), 'range_of_shot'] = data[data.shot_basics == 'Mid Ground Line'].range_of_shot.fillna(value='Back Court Shot')\n# if shot_basics is Mid Range then randomly assign '16-24 ft.' or  '8-16 ft.' to range of shot\n\ndata.loc[(data.shot_basics == 'Mid Range')&(data.range_of_shot.isnull()), 'range_of_shot']       = pd.Series(data[(data.shot_basics == 'Mid Range')&(data.range_of_shot.isnull())].range_of_shot.apply(lambda x: x if type(x)==str else np.random.choice(['16-24 ft.', '8-16 ft.'],1,p=[0.6527708850289495, 0.34722911497105047])[0]))\n# if shot_basics is Goal Line then randomly assign ''8-16 ft.' or  'Less Than 8 ft.' to range of shot\n\ndata.loc[(data.shot_basics == 'Goal Line')&(data.range_of_shot.isnull()), 'range_of_shot']       = pd.Series(data[(data.shot_basics == 'Goal Line')&(data.range_of_shot.isnull())].range_of_shot.apply(lambda x: x if type(x)==str else np.random.choice(['8-16 ft.', 'Less Than 8 ft.'],1,p=[0.5054360956752839, 0.49456390432471614])[0]))\n\ndata.isnull().sum() # number of missing values for range_of_shot column should have been reduced","d8b7eb60":"data['range_of_shot'].unique() # the number of missing values has fallen from 1564 to 66","df915f64":"# Filling the remaining missing values incase they both have NaN values using the forward fill method\ndata.shot_basics.fillna(method='ffill', inplace=True)\ndata.range_of_shot.fillna(method='ffill', inplace=True)\ndata.isnull().sum() # number of missing values for shot_basics and range_of_shot columns should be zero","2896d16e":"# Filling the missing value in \"\u00e4rea_of_short\" Column\ndata.area_of_shot.fillna(value='Center(C)', inplace=True) # all the missing values get filled by  'Centre(C)'\ndata.isnull().sum() # number of missing values for area_of_shot column should be zero","1f621218":"data['distance_of_shot'].unique()","fa26c178":"#Filling the Missing values in \"distance_of_shot\"\n# if distance_of_shot isnull randomly assign a value from 20,45,44,37\ndata.loc[data['distance_of_shot'].isnull(), 'distance_of_shot'] = pd.Series(data.loc[data['distance_of_shot'].isnull(), 'distance_of_shot'].apply(lambda x: x if type(x)==str else np.random.choice([20,45,44,37],1,p=[0.5278056615137523,0.18630797028709095,0.14384661714515157,0.1420397510540052])[0])) \ndata.isnull().sum() # number of missing values for distance_of_shot column should be zero","ba6ec160":"\n# Making the train Dataset\ntrain = data[data.is_goal.notnull()]\nprint('the Shape of Train Dataset',train.shape)\ntrain.set_index(np.arange(train.shape[0]),inplace=True)\ntrain.head()\n","fe62ebfa":"# Making the Test Dataset\ntest = data[data.is_goal.isnull()]\nprint('The Shape of Test Dataset',test.shape)\ntest.set_index(np.arange(test.shape[0]), inplace=True)\ntest.head()","e5819ba0":"l_goal   = train[train.is_goal == 1].type_of_shot.value_counts().head(6).keys()     # Top six shots when it was goal\nl_goal","7aaa39ac":"p_g_sum  = train[train.is_goal == 1].type_of_shot.value_counts().head(6).sum() # Top six shots when it was goal\np_goal   = (train[train.is_goal == 1].type_of_shot.value_counts().head(6) \/ p_g_sum ).tolist()  # There respective probablities\np_goal","b3908992":"# if is_goal is 1, if type of shot is a string value, fill with the same or else fill with randomly choosing value from l_goal\ng = pd.Series(train[train.is_goal == 1].type_of_shot.apply(lambda x: x if type(x)==str else np.random.choice(l_goal,1,p=p_goal)[0]))\ng","0a0f6f73":"# # if is_goal is 1, if type of shot is null then type of shot becomes equal to the value of g based on the index\ntrain.loc[(train.is_goal == 1)&(train.type_of_shot.isnull()), 'type_of_shot'] = g","80c7115d":"train['type_of_shot'].isna().sum() # number of missing values got reduced from more than 15k to 6723","35841127":"l_no_goal   = train[train.is_goal == 0].type_of_shot.value_counts().head(5).keys()     # Top five shots when it was not a goal\np_no_sum  = train[train.is_goal == 0].type_of_shot.value_counts().head(5).sum()\np_no_goal   = (train[train.is_goal == 0].type_of_shot.value_counts().head(5) \/ p_no_sum ).tolist() # There respective probablities \nng = pd.Series(train[train.is_goal == 0].type_of_shot.apply(lambda x: x if type(x)==str else np.random.choice(l_no_goal,1,p=p_no_goal)[0]))\ntrain.loc[(train.is_goal == 0)&(train.type_of_shot.isnull()), 'type_of_shot'] = ng \ntrain['type_of_shot'].isna().sum() # number of missing values got reduced to zero","2a705a06":"#Handeling the remaing values in test dataset with a smilira approach\ntest.loc[test['type_of_shot'].isnull(), \n         'type_of_shot'] = pd.Series(test.loc[test['type_of_shot'].isnull(), \n                                              'type_of_shot'].apply(lambda x: x if type(x)==str else np.random.choice(['shot - 39', 'shot - 36', 'shot - 4'],1,p=[0.37377133988618727, 0.33419555095706155, 0.2920331091567512])[0])) \n","f5729199":"test['type_of_shot'].isna().sum() # we have removed the missing values from test set as well","e9d94a53":"%%time\n# Labeling the catagories with integers\nfor col in train.columns:\n    if train[col].dtypes == object: # if the column has categorical values\n        l_unique = train[col].unique() # find the unique values\n        v_unique = np.arange(len(l_unique)) # create a list of number from zero to the length of the I_unique values\n        train[col].replace(to_replace=l_unique, value=v_unique, inplace=True) # replace the categorical values with numerical values\n        train[col] = train[col].astype('int') # change the type from int64 to int32\n        \n        # same has been done for test data as well\n        test[col].replace(to_replace=l_unique, value=v_unique, inplace=True)\n        test[col] = test[col].astype('int')\n        ","777511f6":"# Dropping the unnecessary Columns\ntrain.drop(['date_of_game'], axis=1, inplace=True)\ntrain.head()","09410ab0":"test.drop(['date_of_game'], axis=1, inplace=True)\ntest.head()","4dcebb4f":"# Splliting the Target Column from the Dataset\ny = train.is_goal\ny.head()","de65bd41":"train.drop(['is_goal'], axis=1, inplace=True)\ntrain.head()\n","2150b56d":"test.drop(['is_goal'], axis=1, inplace=True)\ntest.head()","cdbfcb8c":"train.info() # we have converted all the categorical columns to numeric ones","120db895":"train.isna().sum() # we have don't have any missing values as well. Our data is ready to be fed to a machine learning model.","231dd762":"and we have applied similar concept for the scenarios when there was no goal","0b18c685":"### 1. Dropping unnessary Columns","e2d75074":"Handeling Missing Values in train and Test Dataset\n\n\nFilling the Nan value with a random choice from given list with there appropriate probablities","c9c792ab":"### 1. Exploring the Columns of Dataset","414864bf":"## Making the Train and Test Dataset","b6e65d16":"### # train and test data are divided based on the vaue of is goal column\n","42e2f3d9":"### 3. Handeling Missing Values","62d28b82":"## B. Exploratory Data Analysis","7f4eaf9f":"### Label Encoding the Object type Columns","08e72d38":"# Table Of Contetnt :\n\n- A. Data Preprocessing\n\n    -1. Exploring the Columns of Dataset\n    \n    -2. Checking for Missing Values\n    \n- B. Exploratory Data Analysis\n\n    -1. Dropping unnessary Columns\n    \n    -2. Handeling Missing Values\n    \n- C. Making the Train and Test Dataset","2adc5fea":"## Data Preprocessing","d44aa91b":"### 2. Checking for Missing Values","441cc457":"\" It's a binary classification problem as there are only two values for the target ''is_goal\" column"}}