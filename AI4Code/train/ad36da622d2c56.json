{"cell_type":{"b1db5a7a":"code","25863c43":"code","6b30ce57":"code","f419876a":"code","8758e15b":"code","25ab96dd":"code","88da5293":"code","087760a6":"code","a846ffdd":"code","bc06e397":"code","b7c0259c":"code","cf3a60cb":"code","971f0f5c":"code","869dd924":"code","a689ab38":"code","3246c36c":"code","b652b343":"code","00803b42":"code","88275171":"code","b9ff90e5":"code","d999b642":"code","40ce4839":"code","08f5cd00":"code","14b06789":"code","9ed06b43":"code","3a29d2ff":"code","4623c5d5":"code","88d1bb05":"markdown","58f8aa13":"markdown","e3f84172":"markdown","9911d734":"markdown","2fdb2c42":"markdown","f238352e":"markdown","df9f55e3":"markdown","11c65869":"markdown","09770c4b":"markdown","c610d7f6":"markdown","de918b50":"markdown","e7e46d7c":"markdown","6e31074c":"markdown"},"source":{"b1db5a7a":"#data acquisition modules \nimport pandas_datareader.data as pdr\nimport time\nimport datetime\n# general modules \nimport pandas as pd\npd.options.display.max_rows = 10\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport random\nimport os\n# Deep Learning modules \nimport tensorflow as tf\nimport keras as keras\nfrom keras.models import Sequential\nfrom keras import metrics\nfrom keras.layers import LSTM, Dense, Activation\nimport h5py\n#sklearn and xgboost\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn import preprocessing","25863c43":"def stock_data_unpack(ticker, start_date, end_date):\n    count = 0\n    try:\n        raw_data = pdr.get_data_yahoo(ticker, start_date, end_date)\n    except ValueError:\n        print(\"ValueError, trying again\")\n        count += 1\n        if count < 9:\n            time.sleep(random.randrange(10))\n            get_stock_data(ticker, start_date, end_date)\n        else:\n            print(\"Yahoo error. I will Try in a second or so\")\n            time.sleep(range(10,60))\n            get_stock_data(ticker, start_date, end_date) \n            \n    stock_data = raw_data \n    stock_data.to_csv(\"raw_data.csv\")","6b30ce57":"start = datetime(2016,9,11)\nend =   datetime(2019,9,27)\nseq_len = 1 \n# If you need to read the file from Yahoo activate the following line. I however have uploaded the cvs file.\n#stock_data_unpack(\"^DJI\", start_date=start, end_date=end)\ndf1=pd.read_csv(\"..\/input\/raw-data\/raw_data.csv\", names=['Date','High','Low','Open','Close','Volume','Adj Close'], parse_dates=True)\ndf1=df1.rename(columns={'Open': 'Dow_Open', 'Close': 'Dow_Close','Volume': 'Dow_Volume','High': 'Dow_High','Low': 'Dow_Low'})\ndf1=df1.drop(['Adj Close'],axis=1)\ndf1=df1.drop([0], axis=0)","f419876a":"df1.head()","8758e15b":"def plot_series(time, series, format=\"-\", start=0, end=None):\n    plt.plot(time[start:end], series[start:end], format)\n    plt.xlabel(\"Time index\")\n    plt.ylabel(\"Price Index\")\n    plt.grid(True)\ndef moving_average(t, window):\n    return np.convolve(t, np.ones(window), 'valid') \/ window","25ab96dd":"series = df1['Dow_Close'].to_numpy().astype(float)\ntime = df1.index.values.astype(float)\n\nmaxval=np.max(series)\nprint('The maximum price is=',maxval)\nseries=series\/maxval\nmaxT=np.max(time)\nprint('The number of simulated days is=',maxT)\ntime=time\/maxT","88da5293":"plt.figure(figsize=(10, 6))\nwindow=9\nplot_series(time[window-1:], moving_average(series,window))\nplot_series(time, series)\n","087760a6":"split_time = 365\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_valid = time[split_time:]\nx_valid = series[split_time:]\n\nwindow_size = 40#64 This parameter affects the results greatly as it will be seen\nbatch_size = 8\nshuffle_buffer_size = 1000","a846ffdd":"print(tf.__version__)","bc06e397":"#If the version is less than 2, activate and run the following line\n#!pip install tf-nightly-2.0-preview","b7c0259c":"def windowed_datasetII(series, window_size, batch_size, shuffle_buffer):\n    series = tf.expand_dims(series, axis=-1)\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n    ds = ds.shuffle(shuffle_buffer)\n    ds = ds.map(lambda w: (w[:-1], w[1:]))\n    return ds.batch(batch_size).prefetch(1)","cf3a60cb":"def model_forecast(model, series, window_size):\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size))\n    ds = ds.batch(32).prefetch(1)\n    forecast = model.predict(ds)\n    return forecast","971f0f5c":"#------------------------------------------------\n# Huber Loss\n#------------------------------------------------\ntf.keras.backend.clear_session()\n\ntrain_set = windowed_datasetII(x_train, window_size, batch_size=batch_size, shuffle_buffer=shuffle_buffer_size)\n\nmodel1 = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=32, kernel_size=5,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n  tf.keras.layers.Dense(1),\n])\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(\n    lambda epoch: 1e-6 * 10**(epoch \/ 20))\noptimizer = tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9)\nmodel1.compile(loss=tf.keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\nhistory1 = model1.fit(train_set, epochs=100,verbose=2, callbacks=[lr_schedule])\n\n#------------------------------------------------\n# MAE loss\n#------------------------------------------------\nmodel2 = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=32, kernel_size=5,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n  tf.keras.layers.Dense(1),\n])\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(\n    lambda epoch: 1e-6 * 10**(epoch \/ 20))\noptimizer = tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9)\nmodel2.compile(loss=tf.keras.losses.MeanAbsoluteError(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\nhistory2 = model2.fit(train_set, epochs=100,verbose=2, callbacks=[lr_schedule])\n\n#------------------------------------------------\n#     MSE loss\n#------------------------------------------------\nmodel3 = tf.keras.models.Sequential([\n  tf.keras.layers.Conv1D(filters=32, kernel_size=5,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n  tf.keras.layers.Dense(1),\n])\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(\n    lambda epoch: 1e-6 * 10**(epoch \/ 20))\noptimizer = tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9)\nmodel3.compile(loss=tf.keras.losses.MeanSquaredError(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\nhistory3 = model3.fit(train_set, epochs=100,verbose=2, callbacks=[lr_schedule])","869dd924":"plt.semilogx(history1.history[\"lr\"], history1.history[\"loss\"],'b')\nplt.semilogx(history2.history[\"lr\"], history2.history[\"loss\"],'g')\nplt.semilogx(history3.history[\"lr\"], history3.history[\"loss\"],'r')\nplt.xlim([1e-4,1e-1])\nplt.ylim([0,1e-1])","a689ab38":"tf.keras.backend.clear_session()\ndataset = windowed_datasetII(x_train, window_size, batch_size, shuffle_buffer_size)\nmodel = tf.keras.models.Sequential([tf.keras.layers.Conv1D(filters=32, kernel_size=5,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 1)\n])\n\noptimizer = tf.keras.optimizers.SGD(lr=5*1e-3, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.MeanSquaredError(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\nhistory = model.fit(dataset,epochs=400)","3246c36c":"rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\nrnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]","b652b343":"plt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, rnn_forecast)","00803b42":"tf.keras.backend.clear_session()\ndataset = windowed_datasetII(x_train, window_size, batch_size, shuffle_buffer_size)\nmodel = tf.keras.models.Sequential([tf.keras.layers.Conv1D(filters=32, kernel_size=5,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=True)),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=True)),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=True)),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences=True)),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 1)\n])\n\noptimizer = tf.keras.optimizers.SGD(lr=5*1e-3, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.MeanSquaredError(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\nhistory = model.fit(dataset,epochs=500)","88275171":"rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\nrnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]","b9ff90e5":"plt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, rnn_forecast)\nwindow=40\nplot_series(time_valid[window-1:], moving_average(x_valid,window))","d999b642":"split_time = 365\ntime_train = time[:split_time]\nx_train = series[:split_time]\ntime_valid = time[split_time:]\nx_valid = series[split_time:]\n\nwindow_size = 10# was 64\nbatch_size = 8\nshuffle_buffer_size = 500 #100","40ce4839":"from keras.callbacks import ModelCheckpoint\n\nclass myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('mae')<0.008):\n            print(\"\\nReached reasonable accuracy so cancelling training!\")\n            self.model.stop_training = True\ncallbacks = myCallback()\ntf.keras.backend.clear_session()\ndataset = windowed_datasetII(x_train, window_size, batch_size, shuffle_buffer_size)\nmodel = tf.keras.models.Sequential([tf.keras.layers.Conv1D(filters=window_size, kernel_size=2,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128*window_size, return_sequences=True)),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64*window_size, return_sequences=True)),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 1)\n])\nmodel.load_weights(\"..\/input\/weights\/weights.first.hdf5\")\noptimizer = tf.keras.optimizers.SGD(lr=1*1e-3, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.MeanSquaredError(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\n# checkpoint\n\nfilepath=\"weights.first.hdf5\"\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='mae', verbose=1, save_best_only=False, mode='max')\ncallbacks_list = [checkpoint]\nhistory = model.fit(dataset,epochs=1,callbacks=callbacks_list)","08f5cd00":"rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\nrnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]","14b06789":"plt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, rnn_forecast)\nwindow=10\nplot_series(time_valid[window-1:], moving_average(x_valid,window))","9ed06b43":"from keras.callbacks import ModelCheckpoint\n\nclass myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('mae')<0.008):\n            print(\"\\nReached reasonable accuracy so cancelling training!\")\n            self.model.stop_training = True\ncallbacks = myCallback()\ntf.keras.backend.clear_session()\ndataset = windowed_datasetII(x_train, window_size, batch_size, shuffle_buffer_size)\nmodel = tf.keras.models.Sequential([tf.keras.layers.Conv1D(filters=window_size, kernel_size=2,\n                      strides=1, padding=\"causal\",\n                      activation=\"relu\",\n                      input_shape=[None, 1]),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256*window_size, return_sequences=True)),\n  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128*window_size, return_sequences=True)),\n  tf.keras.layers.Dense(1),\n  tf.keras.layers.Lambda(lambda x: x * 1)\n])\nmodel.load_weights(\"weights.second.hdf5\")\noptimizer = tf.keras.optimizers.SGD(lr=1*1e-3, momentum=0.9)\nmodel.compile(loss=tf.keras.losses.MeanSquaredError(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\n# checkpoint\n\nfilepath=\"weights.second.hdf5\"\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='mae', verbose=1, save_best_only=False, mode='max')\ncallbacks_list = [checkpoint]\nhistory = model.fit(dataset,epochs=2,callbacks=callbacks_list)","3a29d2ff":"rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\nrnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]","4623c5d5":"plt.figure(figsize=(10, 6))\nplot_series(time_valid, x_valid)\nplot_series(time_valid, rnn_forecast)\nwindow=10\nplot_series(time_valid[window-1:], moving_average(x_valid,window))","88d1bb05":"Not bad! The dynamics of the system are captured. But the existence of rooms for improvement is undeniable. \nLet's use the same number of neurons but arrange them in a deeper network .","58f8aa13":"The above helper functions help us with plotting and calculating moving average (MA) plots. Furthermore, we normalize the variable for the luxury of numerical convergence.","e3f84172":"**US Stock price forecasting  using deep Neural Networks**\n\n*Disclaimer*: \n\nThis is not how trading works. Although the results of this work are very promising for the task of forecasting the future price of a single stock, the keen reader should bear the following fact in their mind. Successful trading is all about balancing portfolios and minimizing the risk. A Machine Learning practitioner should try to find and buy undervalued securities  (stocks, bonds, and such) and sell the overpriced ones.\n\nEfforts are made towards the longterm (365-day) prediction of the close price index of DOW Jones Industrial index, using a 3-year window form 2016 to 2019. This is a very interesting choice of data since it can put our ML algorithm to the test. In this window, during the test period, the index mostly shows an upward trend. There however exists a sudden drop In the test part. Its shown that a combination of a convolution layer (to capture the patterns) and LSTM layers  (for the sake of long memory) does a phenomenal job and the model is able to accurately predict the 9-day moving average graph. ","9911d734":"**RNN parameter tunning**","2fdb2c42":"Very well! Let's visuals the time series and also plot a 9-day MA (an important indicator in the trading world).","f238352e":"The following helper functions help us with creating data sequencing and prediction. The functions are borrowed from the Coursera \"TensorFlow in Practice Specialization.\" Courtesy of Laurence Moroney.","df9f55e3":"The network surely behaved more poorly. Clearly, our model has predicted the 40-day MA chart. Note that our chosen window was 40 days and it seems that reducing the window increase the accuracy. This makes statistical sense because the latest data have stronger autocorrelation with future events. As 9-day MA is an important indicator in the financial world, lets chose a window of 10 days.","11c65869":"Let's start with a call back for the learning rate. To find the best loss function, we repeat the process for three loss functions. Our network has an initial convolution layer, followed by two bidirectional LSTM layers.  ","09770c4b":"Furthermore, it seems that having more neurons in the LSTM is more helpful than having multiple LSTM layers of a few neurons.  ","c610d7f6":"Let's read the DOW data from Yahoo and turn it into a Pandas DataFrame.","de918b50":"Looks very good. Now let's try to double the neurons in each LSTM layer. The result will not be repeated as the network takes a couple of hours to train. The data file is also a GB large and not worthy of uploading. Had you seen the final results, the moral of the story would have been that small improvements were made and that nothing could be done with this data and the single feature. \nDo not forget that this is phenomenal as we have practically forecasted the 9-day MA chart a year before its happening.","e7e46d7c":"The loss function \"MeanSquaredError\" shows the greatest promise with a learning rate of $5*e-3$.\nLet's run it.","6e31074c":"The MA plot (in blue) filters out most of the white noise. Next, we split our data into train and test sets."}}