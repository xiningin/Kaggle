{"cell_type":{"72394e8a":"code","bc5c586b":"code","c33c7f80":"code","1faa1c67":"code","dfb91736":"code","e883c65e":"code","4141e440":"code","a045a28d":"code","c652795c":"code","8eae925a":"code","29a77ebf":"code","01cafb60":"code","6a7047dc":"code","4e1714fa":"code","0f703c41":"code","10a7e52d":"code","2fd657a9":"code","3591e578":"code","8a9ab2a5":"code","50f72864":"code","2f43fb16":"code","99f7b6d1":"code","2fddedae":"code","852f2ea5":"code","c6b9c422":"code","0b01a40a":"code","72ab440a":"code","81e46e6b":"code","679127e9":"code","b3138b5f":"code","7fdb0a63":"code","65e9bd53":"code","f425d6a2":"code","6a91bf23":"code","9f25887b":"markdown","90567cfb":"markdown","5071edf7":"markdown","ea2b2cde":"markdown","980fe164":"markdown","a3bacb4a":"markdown","7852e00e":"markdown","c420f55e":"markdown","98765e05":"markdown","eb383b79":"markdown","e87e61a5":"markdown","38b0afeb":"markdown","b70fd311":"markdown","a01a16fc":"markdown","507f3cd3":"markdown","358fb6a1":"markdown","2e8f93c5":"markdown","db799659":"markdown","b9db55df":"markdown","c88cb4ea":"markdown","8f720e4f":"markdown","4374eabe":"markdown","6e3fbe4e":"markdown","2034dade":"markdown","12118dbf":"markdown","7c99b8dc":"markdown","ab060c3e":"markdown","da6d224d":"markdown","fd8763c1":"markdown","d083d385":"markdown","97a97bc4":"markdown","601c9c00":"markdown","dabb3456":"markdown"},"source":{"72394e8a":"import numpy as np\nimport pandas as pd\nimport math\nfrom datetime import datetime\npd.set_option('display.max_rows', 365)\n\nimport os\nlist_datas = []\n\n# \u0412\u0435\u0440\u0441\u0438\u044f \u0434\u043b\u044f Kaggle\n#___________________ \nfor dirname, _, filenames in os.walk('\/kaggle\/input\/private-ready-data'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        list_datas.append(os.path.join(dirname, filename))\n#___________________        \n# \u0412\u0435\u0440\u0441\u0438\u044f \u0434\u043b\u044f Jupyter\n#___________________ \nlist_data2 = ['nizhnevartovsk.csv',\n 'ermitsy.csv',\n 'tolka.csv',\n 'mutnyj.csv',\n 'krasnoselkup.csv',\n 'hantymansiisk.csv',\n 'urengoy.csv',\n 'tazovsky.csv',\n 'pitlyar.csv',\n 'salehard.csv',\n 'ust-usa.csv',\n 'pangody.csv',\n 'sale.csv',\n 'sula.csv',\n 'oksino.csv',\n 'muzhi.csv',\n 'nadym.csv',\n 'yaksha.csv',\n 'volokovaya.csv',\n 'horey.csv']\n#___________________ \n\nfrom scipy.ndimage.filters import gaussian_filter1d\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly\nimport plotly.express as px\nimport random\nrandom.seed(2)\n\nfrom statsmodels.tsa.api import ExponentialSmoothing\nfrom sklearn import tree, linear_model, neighbors, naive_bayes, ensemble\nfrom xgboost import XGBRegressor\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.metrics import r2_score","bc5c586b":"import requests as r\nimport pandas as pd\nimport re\nfrom bs4 import BeautifulSoup as bs\n\ndef t(x):\n    try:\n        return int(x)\n    except Exception:\n        if type(x) == str:\n            try:\n                return int(re.findall(r'{.*}', x)[0][1:-1])\n            except ValueError:\n                return np.nan\n        else:\n            return 0\n\ndef get_weather(id1, y_start):\n\n    flag = False\n    for year in range(y_start, y_start + 1):\n        for month in range(1, 13):\n            if month in {1, 3, 5, 7, 8, 10, 12}:\n                params = {'id': id1, 'bday': 1, 'fday': 31, 'amonth': month, 'ayear': year, 'bot':2}\n            elif month == 2:\n                params = {'id': id1, 'bday': 1, 'fday': 28, 'amonth': month, 'ayear': year, 'bot':2}\n            else:\n                params = {'id': id1, 'bday': 1, 'fday': 30, 'amonth': month, 'ayear': year, 'bot':2}\n\n            res = r.get('http:\/\/www.pogodaiklimat.ru\/weather.php', params=params)\n            res.encoding ='utf-8'\n            soup = bs(res.text)\n            soup.find('table')\n            if not flag:\n                tmp = pd.read_html(str(soup), header = 0)\n                data = pd.concat(tmp[0: 2], axis = 1)\n                data['\u0412\u0440\u0435\u043c\u044f (UTC),\\t\u0434\u0430\u0442\u0430.1'] = data['\u0412\u0440\u0435\u043c\u044f (UTC),\\t\u0434\u0430\u0442\u0430.1'].apply(lambda date: str(date) + '.' + str(year))\n                flag = True\n            else:\n                tmp = pd.read_html(str(soup), header = 0)\n                tmp = pd.concat(tmp[0: 2], axis = 1)\n                tmp['\u0412\u0440\u0435\u043c\u044f (UTC),\\t\u0434\u0430\u0442\u0430.1'] = tmp['\u0412\u0440\u0435\u043c\u044f (UTC),\\t\u0434\u0430\u0442\u0430.1'].apply(lambda date: str(date) + '.' + str(year))\n                data = pd.concat([data, tmp], axis = 0)\n    data = data.drop(['\u041a\u043e\u043c\u0444\u043e\u0440\u0442\u043d\u043e\u0441\u0442\u044c', '\u0422min(\u0421)', 'Tmax(\u0421)', 'P(\u0433\u041f\u0430)', '\u0422e(\u0421)', '\u0422es(\u0421)', 'R24(\u043c\u043c)', '\u0412\u0438\u0434\u0438\u043c.', '\u0412\u0440\u0435\u043c\u044f (UTC),\\t\u0434\u0430\u0442\u0430'], axis = 1)\n    data.columns= ['date', 'wind', 'wind_s', 'rain', 'clouds', 'T', 'Td', 'f', 'P', 'rain_meter', 'snow']\n    data['wind_s'] = data['wind_s'].apply(t)\n    try:\n        data['is_rain'] = data['rain'].str.contains(r'\u0434\u043e\u0436\u0434\u044c|\u0433\u0440\u043e\u0437\u0430|\u043b\u0438\u0432&\u0434\u043e\u0436\u0434\u044c|\u043c\u043e\u0440\u043e\u0441\u044c', regex=True)\n        data['is_snow'] = data['rain'].str.contains(r'\u0441\u043d\u0435|\u043f\u043e\u0437\u0451\u043c\u043e\u043a', regex=True)\n        data['drops'] = data['rain'].str.contains(r'\u043e\u0441\u0430\u0434\u043a\u0438', regex=True)\n        data['heavy_snow'] = data['rain'].str.contains(r'\u043c\u0435\u0442\u0435\u043b\u044c|\u043b\u0438\u0432&\u0441\u043d\u0435\u0433', regex=True)\n    except Exception:\n        pass\n    data = data.drop(['rain', 'clouds', 'wind'], axis=1)\n    data['snow'] = pd.to_numeric(data['snow'], errors='coerce', downcast='float')\n    new_data = data.groupby('date').head(1)\n    try:\n        new_data['snow'] = list(data.groupby('date', sort=False).snow.mean(numeric_only=False))\n    except ValueError:\n        new_data['snow'] = np.nan\n    \n    new_data['rain_meter'] = list(data.groupby('date', sort=False).rain_meter.mean(numeric_only=False))\n    return new_data\n\n    rain_train = data[data['rain_meter'].notnull()]\n    snow_train = data[data['snow'].notnull()]\n    return data","c33c7f80":"def parse_data():\n    \n    nps = {'muzhi':23426, 'salehard':23330,'ust-usa': 23412, 'yaksha': 23812,'longyugan': 23541, 'urengoy': 23453,'new-urengoy': 23541,'sale': 23552,\n    'tazovsky': 23256, 'hantymansiisk': 23933,'surgut': 23849, 'nizhnevartovsk':23471, 'nadym':23445,'krasnoselkup': 23465,'tolka': 23662,\n 'pangody': 23443, 'pitlyar': 23431, 'usinsk': 23410, 'mutnyj': 23411,'haruda': 23313,'oksino': 23205,'ermitsy':23205,'volokovaya': 22383\n    ,'hoseda': 23313,'sula': 23207,'horey': 23215\n    }\n    \n    k = list(nps.keys())\n    year_level = []\n\n    for i in k:\n        print(i)\n        ns_data = pd.DataFrame()\n        flag = False\n        current_file = open('raw\/' + i + '.txt')\n        changed = 0\n        for line in current_file:\n            try:\n                year = int(line[0:4])\n            except Exception:\n                pass\n            if year != changed:\n                print('\u0433\u043e\u0434 ', year)\n                if year_level != []:\n                    if not flag:\n                        ns_data = get_weather(nps[i], year)\n                        if len(year_level) < len(ns_data['date']):\n                            ns_data = ns_data.head(len(year_level))\n                        elif len(year_level) > len(ns_data['date']):\n                            year_level = year_level[0 : len(ns_data['date'])]\n                        ns_data['water_level'] = year_level\n                        year_level = []\n                        flag = True\n                    else:\n                        data = get_weather(nps[i], year)\n                        if len(year_level) < len(data['date']):\n                            data = data.head(len(year_level))\n                        elif len(year_level) > len(data['date']):\n                            year_level = year_level[0 : len(data['date'])]\n                        data['water_level'] = year_level \n                        year_level = []\n                        ns_data = pd.concat([ns_data, data], axis = 0)\n                changed = year\n            else:\n                level = re.findall(r', .*]', line)[0]\n                year_level.append(int(level[2:-1]))\n        data = get_weather(nps[i], year)\n        if len(year_level) < len(data['date']):\n            data = data.head(len(year_level))\n        elif len(year_level) > len(data['date']):\n            year_level = year_level[0 : len(data['date'])]\n        data['water_level'] = year_level \n        year_level = []\n        ns_data = pd.concat([ns_data, data], axis = 0)\n\n        cols = ['wind_s', 'is_rain', 'T', 'f', 'P', 'rain_meter']\n\n\n        train = ns_data[cols]\n        for col in cols:\n            train[col] = pd.to_numeric(train[col])\n        train = train[train['rain_meter'].notnull()]\n        train['wind_s'] = train['wind_s'].apply(t)\n        y = train['rain_meter']\n        train = train.drop(['rain_meter'], axis = 1)\n        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=False)\n        c = train.columns\n        train = pd.DataFrame(imp.fit_transform(train))\n        train.columns = c\n        X_train, X_test, y_train, y_test = train_test_split(train, y)\n        model = XGBRegressor(criterion='mae')\n        model.fit(X_train, y_train)\n        preds = model.predict(X_test)\n        print('rain mae '+ i, mae(preds, y_test))\n        model.fit(train, y)\n\n\n        cols = ['wind_s', 'is_rain', 'T', 'f', 'P']\n        change = ns_data[cols]\n        for col in cols:\n            change[col] = pd.to_numeric(change[col])\n        c = change.columns\n        change = pd.DataFrame(imp.fit_transform(change))\n        change.columns = c\n        for col in cols:\n            change[col] = pd.to_numeric(change[col])\n\n        ns_data = ns_data.reset_index(drop = True)\n        ns_data['rain_meter'] = model.predict(change)\n        ns_data = fix_snow(ns_data)\n\n        ns_data['date'] = pd.to_datetime(ns_data['date'], format='%d.%m.%Y', errors='raise')\n        try:\n            ns_data['is_rain'] = ns_data['is_rain'].replace(np.nan, False)\n            ns_data['is_snow'] = ns_data['is_snow'].replace(np.nan, False)\n            ns_data['drops'] = ns_data['drops'].replace(np.nan, False)\n            ns_data['heavy_snow'] = ns_data['heavy_snow'].replace(np.nan, False)\n        except Exception:\n            pass\n        print(ns_data)\n        ns_data.to_csv('new\/'+i+'.csv')","1faa1c67":"def to_dataset(datas, list_datas=None):\n    \n    scaler_list = []\n    \n    for dt in datas:\n        \n        scaler = MinMaxScaler(feature_range=(0.01,0.9))\n        dt['water_level'] = scaler.fit_transform(np.reshape(dt['water_level'].values,(-1,1)))\n        scaler_list.append(scaler)\n    \n    dataset = pd.DataFrame()\n    for data in datas:\n        dataset = pd.concat([dataset, data], axis=0, ignore_index=True)\n    \n    scaler = MinMaxScaler(feature_range=(0.01,0.9))\n    cols = ['wind_s','T','Td','f','P','rain_meter','snow']\n    \n    for i in cols:\n        dataset[i] = scaler.fit_transform(np.reshape(dataset[i].values,(-1,1)))\n        \n    \n    \n    if list_datas == None:\n        return dataset.drop(['date'], axis=1)\n    else:\n        dictionary = dict()\n        n = 0\n        for i in range(len(datas)):\n            mx = datas[i].shape[0]\n            dictionary[list_datas[i]] = dataset[n:n+mx].reset_index(drop=True).drop(['date'], axis=1)\n            n += mx\n\n        return dictionary, scaler_list","dfb91736":"# \u0412\u0435\u0440\u0441\u0438\u044f \u0441\u0431\u043e\u0440\u0430 \u0434\u043b\u044f Jupyter\n# dataset = to_dataset([pd.read_csv(i,index_col=0).reset_index(drop=True) for i in list_datas2])\n# split_dataset, scaler_list = to_dataset([pd.read_csv(i,index_col=0).reset_index(drop=True) for i in list_datas], list_datas2)\n# split_dataset[list_datas[0]].head()\n\n# \u0412\u0435\u0440\u0441\u0438\u044f \u0441\u0431\u043e\u0440\u0430 \u0434\u043b\u044f Kaggle\ndataset = to_dataset([pd.read_csv(i,index_col=0).reset_index(drop=True) for i in list_datas])\nsplit_dataset, scaler_list = to_dataset([pd.read_csv(i,index_col=0).reset_index(drop=True) for i in list_datas], list_datas)\nsplit_dataset[list_datas[0]].head()","e883c65e":"num_cols = [col for col in dataset.columns if dataset[col].dtype != 'bool']\ncol_labels = ['\u0412\u0435\u0442\u0435\u0440','\u0422\u0435\u043c\u043f\u0435\u0440\u0430\u0442\u0443\u0440\u0430','\u0412\u043b\u0430\u0436\u043d\u044b\u0439 \u0442\u0435\u0440\u043c\u043e\u043c\u0435\u0442\u0440','\u0412\u043b\u0430\u0436\u043d\u043e\u0441\u0442\u044c','\u0414\u0430\u0432\u043b\u0435\u043d\u0438\u0435','\u041a\u043e\u043b-\u0432\u043e \u043e\u0441\u0430\u0434\u043a\u043e\u0432','\u041a\u043e\u043b-\u0432\u043e \u0441\u043d\u0435\u0433\u0430','\u0423\u0440\u043e\u0432\u0435\u043d\u044c \u0432\u043e\u0434\u044b']\nanalysis_data = dataset[num_cols].rename(columns={'wind_s':'\u0412\u0435\u0442\u0435\u0440','T':'\u0422\u0435\u043c\u043f\u0435\u0440\u0430\u0442\u0443\u0440\u0430','Td':'\u0412\u043b\u0430\u0436\u043d\u044b\u0439 \u0442\u0435\u0440\u043c\u043e\u043c\u0435\u0442\u0440','f':'\u0412\u043b\u0430\u0436\u043d\u043e\u0441\u0442\u044c',\n                                                  'P':'\u0414\u0430\u0432\u043b\u0435\u043d\u0438\u0435','rain_meter':'\u041a\u043e\u043b-\u0432\u043e \u043e\u0441\u0430\u0434\u043a\u043e\u0432','snow':'\u041a\u043e\u043b-\u0432\u043e \u0441\u043d\u0435\u0433\u0430','water_level':'\u0423\u0440\u043e\u0432\u0435\u043d\u044c \u0432\u043e\u0434\u044b'})","4141e440":"fig = px.scatter_matrix(analysis_data,width=1200, height=1000)\nfig.show()","a045a28d":"import plotly.graph_objects as go\n\nfig = go.Figure()\n\nfor col in col_labels:\n    fig.add_trace(go.Box(y=analysis_data[col], name=col))\nfig.show()","c652795c":"fig = plt.figure(figsize=(25,7))\nfig.suptitle('\u0414\u0430\u043d\u043d\u044b\u0435 \u043f\u043e \u043f\u0435\u0440\u0432\u044b\u043c \u0447\u0435\u0442\u044b\u0440\u0435\u043c \u0433\u043e\u0434\u0430\u043c', fontsize=20)\ni = 1\nfor col in col_labels:\n    plt.subplot(2, 4, i)\n    sns.lineplot(analysis_data.index[:1430],analysis_data[col][:1430], label=col)\n    i += 1","8eae925a":"for idx in list_datas:\n    ets_train = split_dataset[idx]['water_level'].values\n    ets_model = ExponentialSmoothing(ets_train, seasonal_periods=365, trend='add', seasonal='add', damped=True).fit()\n    ets_preds = ets_model.forecast(365)\n    split_dataset[idx]['ets'] = list(ets_model.forecast(365))*int(split_dataset[idx].shape[0]\/365)","29a77ebf":"train,test = pd.DataFrame(),[]\nfor i in split_dataset.keys():\n    test.append(split_dataset[i][-365:])\n    train = pd.concat([train,split_dataset[i][:-365]],axis=0, ignore_index=True)\ntrain_X, train_y = train.drop(['water_level'], axis=1), train['water_level']","01cafb60":"MLA = [\n    ensemble.AdaBoostRegressor(),\n    ensemble.BaggingRegressor(),\n    ensemble.GradientBoostingRegressor(),\n    ensemble.RandomForestRegressor(), \n    \n    linear_model.SGDRegressor(),\n    linear_model.LinearRegression(),\n\n    tree.DecisionTreeRegressor(),\n    tree.ExtraTreeRegressor(),\n    \n    XGBRegressor()\n]\n\n# 55 \u043b\u0435\u0442 \u0432 \u043e\u0431\u0449\u0435\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435\ncv_split = model_selection.ShuffleSplit(n_splits = 11, test_size = .2, train_size = .8, random_state = 0 )\nfor alg in MLA:\n    \n    cv_results = model_selection.cross_validate(alg, train_X, train_y, cv = cv_split, scoring='r2')\n    print(alg.__class__.__name__ + \" \" + str(cv_results['test_score'].mean()))\n    \n# AdaBoostRegressor 0.7539585753551755\n# BaggingRegressor 0.8996051391102052\n# GradientBoostingRegressor 0.8657257152048669\n# RandomForestRegressor 0.9089635573249661\n# SGDRegressor 0.7240124876444518\n# LinearRegression 0.7298736114852162\n# DecisionTreeRegressor 0.8376708544181736\n# ExtraTreeRegressor 0.7708605632012376\n# XGBRegressor 0.9211091971286323","6a7047dc":"# rfr = ensemble.RandomForestRegressor()\n# parameters = {'max_depth':[5],'max_features':['sqrt'],\n#               'n_estimators':[500],'n_jobs':[4]}\n\n# clf = GridSearchCV(rfr, parameters, scoring='r2')\n# clf.fit(train_X, train_y)\n\n# print(clf.best_score_)\n# print(clf.best_params_)\n# clf.best_estimator_.feature_importances_\n\n#{'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 500, 'n_jobs': 4}\n# imp_f = array([6.85355666e-03, 1.23787022e-01, 6.57596730e-02, 2.40326774e-02,\n#       1.92834025e-02, 2.71969166e-02, 7.92027250e-02, 4.31652113e-04,\n#       6.63999652e-03, 1.90777320e-04, 3.05488309e-04, 6.46316112e-01])","4e1714fa":"# model = ensemble.BaggingRegressor()\n# parameters = {'n_estimators':[300,500,1000,1200]}\n\n# clf = GridSearchCV(model, parameters, scoring='r2')\n# clf.fit(train_X, train_y)\n\n# print(clf.best_score_)\n# print(clf.best_params_)\n\n#{'n_estimators': 1200}","0f703c41":"# model = XGBRegressor()\n# parameters = {'booster':['gbtree'], 'max_depth':[3, 5, 7, 9],'reg_lambda':[0.5,0.7],\n#               'reg_alpha':[0.5,0.7],'learning_rate':[0.03, 0.07], 'n_estimators':[300,500,1000],\n#              'n_jobs':[4]}\n\n# clf = GridSearchCV(model, parameters, scoring='r2')\n# clf.fit(train_X, train_y)\n\n# print(clf.best_score_)\n# print(clf.best_params_)\n# print(clf.best_estimator_.feature_importances_)\n\n# {'booster': 'gbtree', 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 300, 'n_jobs': 4, 'reg_alpha': 0.7, 'reg_lambda': 0.5}","10a7e52d":"class Stack_Level:\n    \n    def __init__(self):\n\n        self.model1 = ensemble.BaggingRegressor(n_estimators=1200)\n        self.model2 = XGBRegressor(booster='gbtree',learning_rate=0.03, max_depth=3, n_estimators=300, reg_alpha=0.7, reg_lambda=0.5)\n        self.model3 = ensemble.RandomForestRegressor(max_depth=5, n_estimators=500,max_features='sqrt')\n        \n    def fit_predict_valid(self, train_X, train_y, test):\n\n        self.model1.fit(train_X, train_y)\n        self.model2.fit(train_X, train_y)\n        self.model3.fit(train_X, train_y)\n        preds, r2, mse_ = [], [], []\n        for ix in range(20):\n            \n            self.preds1 = self.model1.predict(test[ix].drop(['water_level'],axis=1))\n            self.preds2 = self.model2.predict(test[ix].drop(['water_level'],axis=1))\n            self.preds3 = self.model3.predict(test[ix].drop(['water_level'],axis=1))\n            \n            self.balance_preds = self.preds1*0.33 + self.preds2*0.33 + self.preds3 * 0.34\n            preds.append(self.balance_preds)\n            r2.append(r2_score(test[ix]['water_level'].values,self.balance_preds))\n            mse_.append(mse(test[ix]['water_level'].values,self.balance_preds))\n            \n        return preds, r2, mse_\n        \n    def fit(self, train_X, train_y):\n\n        self.model1.fit(train_X, train_y)\n        self.model2.fit(train_X, train_y)\n        self.model3.fit(train_X, train_y)\n        \n    def predict(self, ix=0):  # \u043d\u0430 1 \u0433\u043e\u0434 \u0438 60 \u0434\u043d\u0435\u0439\n\n        self.ix = ix\n        self.preds1 = self.model1.predict(test[ix].drop(['water_level'],axis=1))\n        self.preds2 = self.model2.predict(test[ix].drop(['water_level'],axis=1))\n        self.preds3 = self.model3.predict(test[ix].drop(['water_level'],axis=1))\n        \n        self.balance_preds = self.preds1*0.33 + self.preds2*0.33 + self.preds3 * 0.34\n            \n        return self.balance_preds\n    \n    def plot(self, test, ix, label): # \u043d\u0430 1 \u0433\u043e\u0434\n        \n        self.preds1 = self.model1.predict(test[ix].drop(['water_level'],axis=1))\n        self.preds2 = self.model2.predict(test[ix].drop(['water_level'],axis=1))\n        self.preds3 = self.model3.predict(test[ix].drop(['water_level'],axis=1))\n        \n        self.balance_preds = self.preds1*0.33 + self.preds2*0.33 + self.preds3 * 0.34\n        \n        plt.figure()\n        arr = [i for i in range(len(self.balance_preds))]\n        sns.lineplot(arr, test[ix]['water_level'].values, label=\"historical\")\n        sns.lineplot(arr, self.balance_preds, label=\"prediction\")\n        plt.xlabel(label + ' : R2: ' + str(r2_score(test[ix]['water_level'].values,self.balance_preds)))\n    \n    def validate(self, valid_y):\n        \n        print('mse', mse(valid_y,self.balance_preds))\n        print('r2_score', r2_score(valid_y,self.balance_preds))","2fd657a9":"model = Stack_Level()\npreds, r2, mse_ = model.fit_predict_valid(train_X, train_y, test)","3591e578":"r2_data = pd.DataFrame()\nr2_data['city'] = list_datas\nr2_data['city'] = r2_data['city'].apply(lambda x: x[x.rfind(\"\/\")+1:-4])\nr2_data['r2'] = r2\nr2_data['mse'] = mse_\nr2_data.sort_values(by=['r2'], ascending = False)","8a9ab2a5":"for i in range(len(list_datas)):\n     model.plot(test, i, list_datas[i][list_datas[i].rfind(\"\/\")+1:-4])","50f72864":"top3_index = r2_data.sort_values(by=['r2'], ascending = False).index[:3]","2f43fb16":"for i in top3_index:\n     model.plot(test, i, list_datas[i][list_datas[i].rfind(\"\/\")+1:-4])","99f7b6d1":"least3_index = r2_data.sort_values(by=['r2'], ascending = True).index[:3]","2fddedae":"for i in least3_index:\n     model.plot(test, i, list_datas[i][list_datas[i].rfind(\"\/\")+1:-4])","852f2ea5":"train_60,test_60 = pd.DataFrame(),[]\nfor i in split_dataset.keys():\n    test_60.append(split_dataset[i][-365+60:-365+120])\n    train_60 = pd.concat([train_60,split_dataset[i][:-365+60]],axis=0, ignore_index=True)\ntrain_X_60, train_y_60 = train_60.drop(['water_level'], axis=1), train_60['water_level']","c6b9c422":"preds, r2, mse_ = model.fit_predict_valid(train_X_60, train_y_60, test_60)","0b01a40a":"r2_data = pd.DataFrame()\nr2_data['city'] = list_datas\nr2_data['city'] = r2_data['city'].apply(lambda x: x[x.rfind(\"\/\")+1:-4])\nr2_data['r2'] = r2\nr2_data['mse'] = mse_\nr2_data.sort_values(by=['r2'], ascending = False)","72ab440a":"for i in range(len(list_datas)):\n     model.plot(test_60, i, list_datas[i][list_datas[i].rfind(\"\/\")+1:-4])","81e46e6b":"train_60,test_60 = pd.DataFrame(),[]\nfor i in split_dataset.keys():\n    test_60.append(split_dataset[i][-365+90:-365+150])\n    train_60 = pd.concat([train_60,split_dataset[i][:-365+90]],axis=0, ignore_index=True)\ntrain_X_60, train_y_60 = train_60.drop(['water_level'], axis=1), train_60['water_level']","679127e9":"preds, r2, mse_ = model.fit_predict_valid(train_X_60, train_y_60, test_60)","b3138b5f":"r2_data = pd.DataFrame()\nr2_data['city'] = list_datas\nr2_data['city'] = r2_data['city'].apply(lambda x: x[x.rfind(\"\/\")+1:-4])\nr2_data['r2'] = r2\nr2_data['mse'] = mse_\nr2_data.sort_values(by=['r2'], ascending = False)","7fdb0a63":"for i in range(len(list_datas)):\n     model.plot(test_60, i, list_datas[i][list_datas[i].rfind(\"\/\")+1:-4])","65e9bd53":"preds_0 = np.reshape(scaler_list[0].inverse_transform(np.reshape(preds[0],(-1,1))), (-1))\nix = [i for i in range(len(preds_0))]\nsns.lineplot(ix,preds_0, label=\"\u041f\u0440\u043e\u0433\u043d\u043e\u0437 \u0443\u0440\u043e\u0432\u043d\u044f \u0440\u0435\u043a\u0438 \u043d\u0430 2 \u043c\u0435\u0441\u044f\u0446\u0430\")","f425d6a2":"forest = [5.32137981e-03, 1.19314043e-01, 6.60804017e-02, 1.91678927e-02,\n        1.74863930e-02, 2.84427455e-02, 6.22125605e-02, 4.73156678e-04,\n        5.42709316e-03, 1.50392035e-04, 3.11882199e-04, 6.75612059e-01]\n\nxgb = [5.6946196e-04, 1.6977105e-02, 1.2776641e-02, 1.5319624e-02, 1.6718958e-02,\n 1.9112745e-02, 3.8182937e-02, 0.0000000e+00, 0.0000000e+00, 2.4910141e-03,\n 3.6017373e-03, 8.7424976e-01]\n\ncols = dataset.columns.values\narr = [i for i in range(0,len(forest))]\n\nfeature_imp_data = pd.DataFrame()\nfeature_imp_data['column'] = train_X.columns\nfeature_imp_data['xgb'] = xgb\nfeature_imp_data['random forest'] = forest\nfeature_imp_data['mean'] = (np.array(xgb) + np.array(forest)) \/ 2\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=arr, y=forest, name=\"Forest Features Importance\"))\nfig.add_trace(go.Scatter(x=arr, y=xgb, name=\"XGB Features Importance\"))\nfig.add_trace(go.Scatter(x=arr, y=feature_imp_data['mean'], name=\"Mean Features Importance\"))\nfig.show()\n\n\nfeature_imp_data.sort_values(by='mean', ascending=False)","6a91bf23":"i = 1\nsyntesis_train_data = split_dataset[list_datas[i]]['snow'].values[:-365]\nsyntesis_model = ExponentialSmoothing(syntesis_train_data, seasonal_periods=365, trend='add', seasonal='add', damped=True).fit()\nsyntesis_data = ets_model.forecast(365)\n\nix = [i for i in range(len(syntesis_preds))]\nsns.lineplot(ix,split_dataset[list_datas[i]]['snow'].values[-365:], label=\"\u0438\u0441\u0442\u043e\u0440\u0438\u0447\u0441\u043a\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043f\u043e \u0441\u043d\u0435\u0433\u0443\")\nsns.lineplot(ix,syntesis_preds, label=\"\u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0441\u043d\u0435\u0433\u0430\")","9f25887b":"## \u041f\u043e\u0434\u0431\u043e\u0440 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432","90567cfb":"## \u041f\u0440\u043e\u0433\u043d\u043e\u0437 \u043d\u0430 60 \u0434\u043d\u0435\u0439 \u043d\u0430\u0447\u0438\u043d\u0430\u044f \u0441 \u043d\u0430\u0447\u0430\u043b\u0430 \u043c\u0430\u0440\u0442\u0430","5071edf7":"# \u0421\u0442\u0435\u043a \u043c\u043e\u0434\u0435\u043b\u0435\u0439","ea2b2cde":"\u0427\u0442\u043e\u0431\u044b \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c \u043a\u0430\u043a\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0443\u0436\u043d\u044b \u0432 \u043e\u0441\u043d\u043e\u0432\u043d\u043e\u043c \u0434\u043b\u044f \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0430, \u043f\u0440\u043e\u0432\u0435\u0434\u0435\u043c \u043f\u0440\u043e\u0446\u0435\u0434\u0443\u0440\u0443 \u043f\u043e\u0438\u0441\u043a\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432","980fe164":"\u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438","a3bacb4a":"\u0420\u0430\u0437\u0434\u0435\u043b\u044f\u0435\u043c \u043d\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0439 \u0438 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0439 \u0441\u0435\u0442\u044b","7852e00e":"# \u041a\u0430\u043a \u0434\u0435\u043b\u0430\u0442\u044c \u043f\u0440\u043e\u0433\u043d\u043e\u0437?","c420f55e":"# \u041f\u0430\u0440\u0441\u0438\u043d\u0433 \u0438 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","98765e05":"## \u0412\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432","eb383b79":"\u041f\u0440\u043e\u0432\u0435\u0440\u043a\u0430 \u0440\u0430\u0431\u043e\u0442\u044b \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0441\u043e \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u043c\u0438 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c\u0438 (\u043d\u0438\u0436\u0435 \u043a\u0430\u0436\u0434\u044b\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442)","e87e61a5":"# \u041f\u0440\u043e\u0433\u043d\u043e\u0437 (\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f) \u043c\u043e\u0434\u0435\u043b\u0438","38b0afeb":"## \u0413\u0440\u0430\u0444\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445","b70fd311":"\u041f\u0440\u043e\u0433\u043d\u043e\u0437 \u043d\u0430 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u0433\u043e\u0434*\n\n**\u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u0434\u043e \u043a\u0430\u043a\u043e\u0433\u043e \u0433\u043e\u0434\u0430 \u0434\u0430\u043d\u043d\u044b\u0435 \u043f\u043e \u043a\u0430\u0436\u0434\u043e\u043c\u0443 \u043d\u0430\u0441\u0435\u043b\u0435\u043d\u043d\u043e\u043c\u0443 \u043f\u0443\u043d\u043a\u0442\u0443*","a01a16fc":"\u0427\u0442\u043e\u0431\u044b \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043d\u0430\u043f\u0435\u0440\u0435\u0434, \u043c\u043e\u0436\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0435 \u0444\u0438\u0447\u0438 ['T', 'ets', 'snow'] \u0442.\u043a. \u043e\u043d\u0438 \u0438\u043c\u0435\u044e\u0442 \u0432\u044b\u0441\u043e\u043a\u0438\u0439 \u043f\u0440\u043e\u0446\u0435\u043d\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438.\n\n\u041a\u0430\u043a \u043d\u0430\u0439\u0442\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0430?\nT - \u0422\u0435\u043c\u043f\u0435\u0440\u0430\u0442\u0443\u0440\u0430 \u0432\u043e\u0437\u0434\u0443\u0445\u0430 \n    1. \u0421 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u0435\u0442\u0435\u043e\u0440\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0441\u0430\u0439\u0442\u043e\u0432 \u0431\u0440\u0430\u0442\u044c \u043f\u0440\u043e\u0433\u043d\u043e\u0437 \u043d\u0430 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u0433\u043e\u0434 \u0438\u043b\u0438 \u043d\u0430 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u043c\u0435\u0441\u044f\u0446\n    2. \u0421\u0438\u043d\u0442\u0435\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u043e\u0434\u0435\u043b\u0435\u0439 ETS, XGBRegressor \u0438\u043b\u0438 VAR\nETS - \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0432\u043e\u0434\u044b ets \u043c\u043e\u0434\u0435\u043b\u0438 \n    1. \u041f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u0441 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445, \u043f\u043e\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044f \u043d\u0430 \u043a\u0430\u0436\u0434\u044b\u0439 \u0433\u043e\u0434 \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043d\u0430\u0441\u0435\u043b\u0435\u043d\u043d\u043e\u0433\u043e \u043f\u0443\u043d\u043a\u0442\u0430\nSnow - \u0443\u0440\u043e\u0432\u0435\u043d\u044c \u0441\u043d\u0435\u0433\u0430\n    1. \u0421 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u0435\u0442\u0435\u043e\u0440\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0441\u0430\u0439\u0442\u043e\u0432 \u0431\u0440\u0430\u0442\u044c \u043f\u0440\u043e\u0433\u043d\u043e\u0437 \u043d\u0430 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u0433\u043e\u0434 \u0438\u043b\u0438 \u043d\u0430 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u043c\u0435\u0441\u044f\u0446\n    2. \u0421\u0438\u043d\u0442\u0435\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u043e\u0434\u0435\u043b\u0435\u0439 ETS, XGBRegressor \u0438\u043b\u0438 VAR","507f3cd3":"# \u0420\u0435\u0448\u0435\u043d\u0438\u0435-\u0431\u043b\u043e\u043a\u043d\u043e\u0442 \u0434\u043b\u044f \u0437\u0430\u0434\u0430\u0447\u0438 \"\u041f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0443\u0440\u043e\u0432\u043d\u0435\u0439 \u0432\u043e\u0434\u044b \u0432 \u043f\u0435\u0440\u0438\u043e\u0434 \u0432\u0435\u0441\u0435\u043d\u043d\u0435\u0433\u043e \u043f\u043e\u043b\u043e\u0432\u043e\u0434\u044c\u044f\"\n### \u041a\u043e\u043c\u0430\u043d\u0434\u0430: Motherhacker","358fb6a1":"\u0418\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 Scaling \u0434\u0430\u043d\u043d\u044b\u0445","2e8f93c5":"## \u0422\u043e\u043f-3 \u0445\u0443\u0434\u0448\u0438\u0445 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0430 \u043f\u043e R2 \u043c\u0435\u0442\u0440\u0438\u043a\u0435 \u0437\u0430 \u0433\u043e\u0434","db799659":"## \u041f\u0440\u043e\u0433\u043d\u043e\u0437 \u043d\u0430 \u0433\u043e\u0434","b9db55df":"# \u0418\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u043c\u043e\u0434\u0443\u043b\u0438","c88cb4ea":"## \u0417\u0430\u0434\u0430\u0447\u0438\n### 1. \u0421\u0431\u043e\u0440 \u0438 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0440\u0435\u0442\u0440\u043e\u0441\u043f\u0435\u043a\u0442\u0438\u0432\u043d\u044b\u0445 \u0438 \u0442\u0435\u043a\u0443\u0449\u0438\u0445 \u043c\u0435\u0442\u0435\u043e\u0440\u043e\u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u0442\u0435\u0440\u0440\u0438\u0442\u043e\u0440\u0438\u0438 \u041d\u0435\u043d\u0435\u0446\u043a\u043e\u0433\u043e \u0430\u0432\u0442\u043e\u043d\u043e\u043c\u043d\u043e\u0433\u043e \u043e\u043a\u0440\u0443\u0433\u0430 \u0438 \u043e\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0449\u0438\u0445 \u0432\u043b\u0438\u044f\u043d\u0438\u0435 \u0441\u0443\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0420\u0424\n### 2. \u0410\u043d\u0430\u043b\u0438\u0437 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0443\u0440\u043e\u0432\u043d\u0435\u0439 \u0432\u043e\u0434\u044b \u0432 \u043f\u0435\u0440\u0438\u043e\u0434 \u0432\u0435\u0441\u0435\u043d\u043d\u0435\u0433\u043e \u043f\u043e\u043b\u043e\u0432\u043e\u0434\u044c\u044f \u043d\u0430 \u0442\u0435\u0440\u0440\u0438\u0442\u043e\u0440\u0438\u0438 \u041d\u0435\u043d\u0435\u0446\u043a\u043e\u0433\u043e \u0430\u0432\u0442\u043e\u043d\u043e\u043c\u043d\u043e\u0433\u043e \u043e\u043a\u0440\u0443\u0433\u0430.","8f720e4f":"## \u041f\u0440\u043e\u0433\u043d\u043e\u0437 \u043d\u0430 60 \u0434\u043d\u0435\u0439 \u043d\u0430\u0447\u0438\u043d\u0430\u044f \u0441 \u043d\u0430\u0447\u0430\u043b\u0430 \u0430\u043f\u0440\u0435\u043b\u044f","4374eabe":"Bagging","6e3fbe4e":"## \u0423\u0440\u043e\u0432\u0435\u043d\u044c \u0432\u043e\u0434\u044b","2034dade":"\u0421\u043e\u0431\u0438\u0440\u0430\u0435\u043c \u0434\u0432\u0430 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 - \u0446\u0435\u043b\u044c\u043d\u044b\u0439 \u0438 \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u043d\u044b\u0439 \u043f\u043e \u043d\u0430\u0441\u0435\u043b\u0435\u043d\u043d\u044b\u043c \u043f\u0443\u043d\u043a\u0442\u0430\u043c","12118dbf":"## \u041f\u0440\u0438\u043c\u0435\u0440 \u0441\u0438\u043d\u0442\u0435\u0437\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","7c99b8dc":"# \u0421\u0431\u043e\u0440 \u0438 \u0430\u043d\u0430\u043b\u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445","ab060c3e":"\u0412 \u044d\u0442\u043e\u0439 \u0432\u0435\u0440\u0441\u0438\u0438 \u0431\u043b\u043e\u043a\u043d\u043e\u0442\u0430 \u0434\u0430\u043d\u043d\u044b\u0435 \u0443\u0436\u0435 \u0441\u043e\u0431\u0440\u0430\u043d\u044b \u0438 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u043d\u044b. \u041d\u0438\u0436\u0435 \u043f\u0440\u043e\u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u044b \u0444\u0443\u043d\u043a\u0446\u0438\u0438, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0431\u044b\u043b\u0438 \u0432\u0437\u0430\u0438\u043c\u043e\u0434\u0435\u0439\u0441\u0442\u0432\u043e\u0432\u0430\u043d\u044b \u0434\u043b\u044f \u0441\u0431\u043e\u0440\u0430 \u0438 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445","da6d224d":"RandomForestRegressor","fd8763c1":"## \u0422\u043e\u043f-3 \u043b\u0443\u0447\u0448\u0438\u0445 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0430 \u043f\u043e R2 \u043c\u0435\u0442\u0440\u0438\u043a\u0435 \u0437\u0430 \u0433\u043e\u0434","d083d385":"# \u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438","97a97bc4":"XGB","601c9c00":"\u0414\u0435\u043b\u0430\u0435\u043c \u043f\u0440\u043e\u0433\u043d\u043e\u0437 \u043f\u043e ETS \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0441\u043e\u0435\u0434\u0438\u043d\u044f\u0435\u043c \u0441 \u043a\u0430\u0436\u0434\u044b\u043c \u0433\u043e\u0434\u043e\u043c \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043d\u0430\u0441\u0435\u043b\u0435\u043d\u043d\u043e\u0433\u043e \u043f\u0443\u043d\u043a\u0442\u0430","dabb3456":"\u0441\u0438\u043d\u0442\u0435\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0441\u043d\u0435\u0433\u0430 c ETS"}}