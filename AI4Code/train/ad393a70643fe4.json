{"cell_type":{"504d1d5c":"code","59d3f299":"code","42e2e266":"code","8636a702":"code","8b702a27":"code","c0d64411":"code","ad39b471":"code","6d7027dd":"code","42047f74":"code","b29f541d":"code","c6c7ba80":"code","45997135":"code","241c67af":"code","22bba320":"code","e625a0c1":"code","262cee4c":"code","354a9351":"code","08112e0a":"code","398c06ed":"code","8cf36166":"code","34471892":"code","357828ad":"code","208e0343":"code","168660f6":"code","f2d313ef":"code","b2e6357f":"code","5aa91384":"code","c42f8503":"code","b5ce3ed1":"markdown","84ba6b17":"markdown","c5c3aad5":"markdown","d31828a7":"markdown","13282765":"markdown","2bbe07ab":"markdown","5146ac98":"markdown"},"source":{"504d1d5c":"a=[\"affe\",\"huhn\"]\nb=[\"atte\",\"ratte\",\"huhn\"]\n\n[i in a for i in b]","59d3f299":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.linear_model import LogisticRegression as logit\nfrom sklearn.ensemble import RandomForestClassifier as RF\nfrom sklearn.metrics import roc_auc_score\nimport keras as ks\nimport tensorflow as tf\nfrom scipy.special import erfinv\nimport plotly.express as px\nimport seaborn as sns\nimport plotly.figure_factory as ff\nfrom plotly.offline import iplot\nimport math\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","42e2e266":"compresion = 3\neps = 100\nbs = 2048\nleRa = 0.02\ndec = 0.0001\n\nrandRatioViz = 0.1\n\n\"Learning rate and decay ok?: \" + str(leRa - dec * eps > 0)","8636a702":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8b702a27":"train = pd.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/train.csv\")\ntrain_id = train.id\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/test.csv\")\ntest_id = test.id","c0d64411":"Xx = pd.concat([train.drop(\"target\", axis = 1), test])\nXx = Xx.set_index(\"id\")\n#Xx.dtypes","ad39b471":"Xx.head()","6d7027dd":"catVars = [c for c in Xx.columns if \"cat\" in c]\ncontVars = [c for c in Xx.columns if \"cont\" in c]","42047f74":"le = preprocessing.LabelEncoder()\nfor c in catVars:\n    Xx[f\"{c}\"] = le.fit_transform(Xx[f\"{c}\"])","b29f541d":"r = list(np.random.random_sample(train.shape[0]) <= randRatioViz)\n\ndf = train.loc[r,contVars]\ndf[\"target\"] = train.loc[r,[\"target\"]]\n\nsns.set_theme(style=\"ticks\")\n\nsns.pairplot(df, \n             hue=\"target\",\n             palette =\"viridis\",\n             kind=\"hist\",\n             height=2,\n             diag_kind=\"kde\",\n             corner=True\n             )\n\n#fig = ff.create_scatterplotmatrix(df, \n#                                  diag='box', \n#                                  index='target',\n#                                  colormap='Cividis',\n#                                  colormap_type='cat',\n#                                  height=900, width=900\n#                                  )\n#iplot(fig)","c6c7ba80":"def rg(df, e, Vars):\n    for i in df.loc[:,Vars]:\n        r = df[i].rank()\n        Range = (r\/r.max()-0.5)*2\n        Range = np.clip(Range, a_max = 1-e, a_min = -1+e)\n        rg = erfinv(Range)\n        df[i] = rg * 2**0.5\n    return df","45997135":"Xx_train = rg(Xx, 0.000001, contVars)","241c67af":"#inputDims = len(Xx.cat0.unique())\ndef autoencoder(DataSet, comp):\n    \n    \"\"\"This function returns the encoder, the autoencoder, and the names of the embeddings \n    the output-layer as input for the decoder, the inputs, the outputs and the names ot all inputs\"\"\"\n    \n    inputs = []\n    outputs = []\n    names = []\n\n    for c in catVars:\n\n        inputDims = len(DataSet[f\"{c}\"].unique())\n        embedDim = min([math.ceil(inputDims \/ 10), 10])\n\n        INPUT = ks.layers.Input(shape=(1), name=c + \"_emb\")\n        OUTPUT = ks.layers.Embedding(inputDims + 1, embedDim)(INPUT)\n        OUTPUT = ks.layers.Reshape(target_shape=(embedDim, ))(OUTPUT)\n\n        inputs.append(INPUT)\n        outputs.append(OUTPUT)\n        names.append(c + \"_emb\")\n\n    contNum = len(DataSet.columns) - len(catVars) \n\n    INPUT = ks.layers.Input(shape=(contNum,), name=\"Vars\")\n\n    outputs.append(INPUT)\n    inputs.append(INPUT)\n\n    CONCAT = ks.layers.Concatenate()(outputs)\n\n    OUT = ks.layers.Dropout(0.1)(CONCAT)\n    OUT = ks.layers.BatchNormalization()(OUT)\n    OUT = ks.layers.Dense(150, activation='relu')(OUT)\n\n    OUT = ks.layers.Dropout(0.1)(OUT)\n    OUT = ks.layers.BatchNormalization()(OUT)\n    OUT = ks.layers.Dense(15, activation='relu')(OUT)\n\n    OUT = ks.layers.Dropout(0.1)(OUT)\n    OUT = ks.layers.BatchNormalization()(OUT)\n    OUT = ks.layers.Dense(comp, activation='linear')(OUT)\n\n    encoder = ks.Model(inputs=inputs, outputs=OUT)\n\n    OUT = ks.layers.Dropout(0.1)(OUT)\n    OUT = ks.layers.BatchNormalization()(OUT)\n    OUT = ks.layers.Dense(15, activation='relu')(OUT)\n\n    OUT = ks.layers.Dropout(0.1)(OUT)\n    OUT = ks.layers.BatchNormalization()(OUT)\n    OUT = ks.layers.Dense(150, activation='relu')(OUT)\n\n    OUT = ks.layers.Dropout(0.1)(OUT)\n    OUT = ks.layers.BatchNormalization()(OUT)\n    OUT = ks.layers.Dense(30-len(catVars), activation='linear')(OUT)\n\n    AE = ks.Model(inputs=inputs, outputs=OUT)\n    \n    return encoder, AE, names","22bba320":"help(autoencoder)","e625a0c1":"ENCODER, AUTOENCODER, names = autoencoder(Xx,compresion)\n\nks.utils.plot_model(AUTOENCODER, \n                    show_shapes=True, \n                    show_layer_names=True\n                    )","262cee4c":"def rmse(y_pred, y_true):\n    y_pred = tf.cast(y_pred, dtype=\"float32\")\n    y_true = tf.cast(y_true, dtype=\"float32\")\n    r = tf.sqrt(tf.keras.backend.mean(tf.square(y_pred - y_true)))\n    return r","354a9351":"stop = ks.callbacks.EarlyStopping(monitor='AUC', min_delta=0.000001, patience=10, mode='max')","08112e0a":"optimizer = ks.optimizers.Adam(lr=leRa, decay=dec)\nAUTOENCODER.compile(optimizer = optimizer, loss = rmse)","398c06ed":"X_Train = {names[c]: Xx_train.iloc[:,c] for c in range(len(catVars))}\nX_Train.update({\"Vars\": Xx_train.drop(catVars, axis=1)})","8cf36166":"history = ks.callbacks.History()\n\nAUTOENCODER.fit(X_Train, \n                Xx.drop(catVars, axis=1), \n                epochs = eps, \n                batch_size = bs, \n                shuffle = False,\n                callbacks=[history]\n               )\n\n#print(history.history)","34471892":"Denoised = AUTOENCODER.predict(\n   x=X_Train, \n   workers = 1, \n   use_multiprocessing = True\n)\n\nDenoised = pd.DataFrame(Denoised, columns=contVars)\n\nDenoised = pd.concat([Xx.loc[:,catVars], Denoised], axis=1)\n\n#for c in catVars:\n#    Denoised[f\"{c}\"] = Xx[f\"{c}\"]\n    \nDenoised.head()","357828ad":"#c=1\nX_Compress = {names[c]: Xx_train.iloc[train_id,c] for c in range(len(catVars))}\nX_Compress.update({\"Vars\": Xx_train.iloc[train_id,:].drop(catVars, axis=1)})","208e0343":"Compressed = ENCODER.predict(\n   x=X_Compress, \n   workers = 1, \n   use_multiprocessing = True\n)\n\nCompressed = pd.DataFrame(Compressed, columns=[\"dim_{0}\".format(i) for i in range(Compressed.shape[1])])\nCompressed[\"target\"] = train.target.astype(\"category\")\nCompressed.head()","168660f6":"r = list(np.random.random_sample(Compressed.shape[0]) <= randRatioViz)\n\nCompressed = Compressed.loc[r,:]\n\nfig = px.scatter_3d(\n     Compressed, \n     x='dim_0', \n     y='dim_1', \n     z='dim_2',\n     color='target',\n     hover_data={'dim_0': False, \n                 'dim_1': False,\n                 'dim_2': False,\n                 'target': True\n             },\n     opacity=1,\n     color_discrete_sequence=px.colors.qualitative.Antique,\n     title=\"Compressed Representation\",\n     template=\"simple_white\"\n     )\n\nfig.update_traces(marker=dict(size=6,\n                              line=dict(width=1,\n                                        color='grey')),\n                  selector=dict(mode='markers'))\n\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0),\n                 scene=dict(bgcolor='white'))\n\nfig.show()","f2d313ef":"for c in catVars:\n    Denoised[f\"{c}\"] = Denoised[f\"{c}\"].astype(\"category\")\n\ntr = Denoised.loc[train_id,:]\nprint(tr.shape)\nte = Denoised.loc[test_id,:]\nte.shape","b2e6357f":"cv_size = 0.2\nX_train, X_test, y_train, y_test = tts(tr, train.target, test_size=cv_size, random_state=42)\n\ny_train.describe()","5aa91384":"clf = RF(n_estimators=80, \n         min_samples_leaf=5,\n         max_depth=20, \n         min_samples_split=5, \n         random_state=0,\n         n_jobs=-1\n         )\n\nclf.fit(X_train, y_train)\n#clf.predict(X_test) \n\nprint(\"AUC is: \" + str(roc_auc_score(y_test, clf.predict_proba(X_test)[:,1])))\nprint(\"Accuracy is: \" + str(clf.score(X_test, y_test)))","c42f8503":"submission = pd.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/sample_submission.csv\")\n\nsubmission['target'] = clf.predict_proba(te)\nsubmission['target'].describe()\n\nsubmission.to_csv(\"submission.csv\", index=False)","b5ce3ed1":"# Autoencoder\n\nIn this notebook I show how to use an autoencoder for dimension reduction. The compression in the middle has an additional noise reducing effect. This happens because the decoder takes only the systematic components of the relationships present in the data set and reprocesses them. The noise remains in the bottleneck. Note that denoising autoencoders are usually trained by adding noise rather than by compression, so here it is assumed that the dataset is already noisy.","84ba6b17":"Combining","c5c3aad5":"Compressed Representation","d31828a7":"Let\u00b4s check some correlations:","13282765":"Rank Gauss","2bbe07ab":"Using the denoised data:","5146ac98":"Define the encoder and the decoder separately:"}}