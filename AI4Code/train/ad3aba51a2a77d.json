{"cell_type":{"f762d9a6":"code","2b7d9585":"code","bb7d1d5c":"code","b5c3f3af":"code","ab03ebb4":"code","33e12bdc":"code","5b71e10a":"code","b1784fc5":"code","118cc8a8":"code","b44093a0":"code","23d01cc5":"code","2ce17138":"code","c5dfb4f2":"code","c474d7a4":"code","aa9cd05f":"code","af9c336a":"code","4280b8a2":"code","a16c79dc":"code","87e1fd4c":"code","2434a228":"code","8f2cfbec":"code","2ce9daf9":"code","c672224c":"markdown","15de3b08":"markdown","88c836fe":"markdown","bb671686":"markdown","960729a9":"markdown"},"source":{"f762d9a6":"# Generic\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os, warnings\nwarnings.filterwarnings('ignore')\n\n# TensorFlow\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n# Transformer Model\nfrom transformers import BertTokenizer, TFBertModel               #BERT\nfrom transformers import DistilBertTokenizer, TFDistilBertModel    #DistilBERT\nfrom transformers import XLMRobertaTokenizer, TFXLMRobertaModel    #XLM-RoBERTa\n\n\n# SKLearn Library\nfrom sklearn.model_selection import train_test_split\n\n# Garbage Collector\nimport gc\n\n# Tabulate\nfrom tabulate import tabulate\n\nos.environ[\"WANDB_API_KEY\"] = \"0\"","2b7d9585":"# Initialize TPU\n\ndef Init_TPU():  \n\n    try:\n        resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(resolver)\n        tf.tpu.experimental.initialize_tpu_system(resolver)\n        strategy = tf.distribute.experimental.TPUStrategy(resolver)\n        REPLICAS = strategy.num_replicas_in_sync\n        print(\"Connected to TPU Successfully:\\n TPUs Initialised with Replicas:\",REPLICAS)\n        \n        return strategy\n    \n    except ValueError:\n        \n        print(\"Connection to TPU Falied\")\n        print(\"Using default strategy for CPU and single GPU\")\n        strategy = tf.distribute.get_strategy()\n        \n        return strategy\n    \nstrategy=Init_TPU()","bb7d1d5c":"# Define Dataset Path\npath = '..\/input\/contradictory-my-dear-watson\/'","b5c3f3af":"# Load Training Data\ntrain_url = os.path.join(path,'train.csv')\ntrain_data = pd.read_csv(train_url, header='infer')","ab03ebb4":"# Garbage Collection\ngc.collect()","33e12bdc":"# Transformer Model Name\nBert_model = 'bert-base-multilingual-cased'\ndistilBert_model = 'distilbert-base-multilingual-cased'\nxlmRoberta_model = 'jplu\/tf-xlm-roberta-base'\n\n# Define Tokenizer for each\nBert_toknzr = BertTokenizer.from_pretrained(Bert_model)\ndistilBert_toknzr = DistilBertTokenizer.from_pretrained(distilBert_model)\nxlmRoberta_toknzr = XLMRobertaTokenizer.from_pretrained(xlmRoberta_model)","5b71e10a":"# Checking the output of tokenizer\nsentence = 'Elementary, My Dear Watson!'\n\nprint(\"BERT Model Tokenizer Output:\",Bert_toknzr.convert_tokens_to_ids(list(Bert_toknzr.tokenize(sentence))))\nprint(\"DistilBERT Model Tokenizer Output:\",distilBert_toknzr.convert_tokens_to_ids(list(distilBert_toknzr.tokenize(sentence))))\nprint(\"XLM-RoBERTa Model Tokenizer Output:\",xlmRoberta_toknzr.convert_tokens_to_ids(list(xlmRoberta_toknzr.tokenize(sentence))))","b1784fc5":"# Create seperate list from Train & Test Dataframes with only Premise & Hypothesis\ntrain = train_data[['premise','hypothesis']].values.tolist()","118cc8a8":"# Define Max Length\nmax_len = 80   # << change if you wish\n\n# Encode the training & test data - BERT\ntrain_encode_Bert = Bert_toknzr.batch_encode_plus(train, pad_to_max_length=True, max_length=max_len)\n\n# Encode the training & test data - DistilBERT\ntrain_encode_DistilBert = distilBert_toknzr.batch_encode_plus(train, pad_to_max_length=True, max_length=max_len)\n\n# Encode the training & test data - XLM-RoBERTa\ntrain_encode_XlmRoberta = xlmRoberta_toknzr.batch_encode_plus(train, pad_to_max_length=True, max_length=max_len)\n","b44093a0":"# Split the Training Data into Training (90%) & Validation (10%)\n\ntest_size = 0.1  # << change if you wish\n\n# BERT\nx_tr_bert, x_val_bert, y_tr_bert, y_val_bert = train_test_split(train_encode_Bert['input_ids'], train_data.label.values, test_size=test_size)\n\n# DistilBERT\nx_tr_Dbert, x_val_Dbert, y_tr_Dbert, y_val_Dbert = train_test_split(train_encode_DistilBert['input_ids'], train_data.label.values, test_size=test_size)\n\n# XLM-RoBERTa\nx_tr_XR, x_val_XR, y_tr_XR, y_val_XR = train_test_split(train_encode_XlmRoberta['input_ids'], train_data.label.values, test_size=test_size)\n","23d01cc5":"#garbage collect\ngc.collect()","2ce17138":"# Loading Data Into TensorFlow Dataset\nAUTO = tf.data.experimental.AUTOTUNE\nbatch_size = 16 * strategy.num_replicas_in_sync\n\n#BERT\ntr_ds_bert = (tf.data.Dataset.from_tensor_slices((x_tr_bert, y_tr_bert)).repeat().shuffle(2048).batch(batch_size).prefetch(AUTO))\nval_ds_bert = (tf.data.Dataset.from_tensor_slices((x_val_bert, y_val_bert)).batch(batch_size).prefetch(AUTO))\n\n#DistilBERT\ntr_ds_Dbert = (tf.data.Dataset.from_tensor_slices((x_tr_Dbert, y_tr_Dbert)).repeat().shuffle(2048).batch(batch_size).prefetch(AUTO))\nval_ds_Dbert = (tf.data.Dataset.from_tensor_slices((x_val_Dbert, y_val_Dbert)).batch(batch_size).prefetch(AUTO))\n\n#XLM-RoBERTa\ntr_ds_XR = (tf.data.Dataset.from_tensor_slices((x_tr_XR, y_tr_XR)).repeat().shuffle(2048).batch(batch_size).prefetch(AUTO))\nval_ds_XR = (tf.data.Dataset.from_tensor_slices((x_val_XR, y_val_XR)).batch(batch_size).prefetch(AUTO))","c5dfb4f2":"# Garbage Collection\ngc.collect()","c474d7a4":"def build_model(strategy):\n    with strategy.scope():\n        bert_encoder = TFBertModel.from_pretrained(Bert_model)  #BERT\n        DistilBert_encoder = TFDistilBertModel.from_pretrained(distilBert_model)  #DistilBERT\n        XLMRoberta_encoder = TFXLMRobertaModel.from_pretrained(xlmRoberta_model)  #XLM-RoBERTa\n        \n        input_layer = Input(shape=(max_len,), dtype=tf.int32, name=\"input_layer\")\n        \n        sequence_output_bert = bert_encoder(input_layer)[0]\n        sequence_output_Dbert = DistilBert_encoder(input_layer)[0]\n        sequence_output_XR = XLMRoberta_encoder(input_layer)[0]\n        \n        cls_token_bert = sequence_output_bert[:, 0, :]\n        cls_token_Dbert = sequence_output_Dbert[:, 0, :]\n        cls_token_XR = sequence_output_XR[:, 0, :]\n                \n        output_layer_bert = Dense(3, activation='softmax')(cls_token_bert)\n        output_layer_Dbert = Dense(3, activation='softmax')(cls_token_Dbert)\n        output_layer_XR = Dense(3, activation='softmax')(cls_token_XR)\n        \n        model1 = Model(inputs=input_layer, outputs=output_layer_bert)\n        model2 = Model(inputs=input_layer, outputs=output_layer_Dbert)\n        model3 = Model(inputs=input_layer, outputs=output_layer_XR)\n        \n        \n        model1.compile(\n            Adam(lr=1e-5), \n            loss='sparse_categorical_crossentropy', \n            metrics=['accuracy']\n        )\n        \n        model2.compile(\n            Adam(lr=1e-5), \n            loss='sparse_categorical_crossentropy', \n            metrics=['accuracy']\n        )\n            \n        model3.compile(\n            Adam(lr=1e-5), \n            loss='sparse_categorical_crossentropy', \n            metrics=['accuracy']\n        )\n        \n        \n        return model1, model2, model3\n    \n\n# Applying the build model function\nmodel_bert, model_Dbert, model_XLMRoberta = build_model(strategy)","aa9cd05f":"# Train the Model\n\nepochs = 30  # < change if you wish\nn_steps = len(train_data) \/\/ batch_size ","af9c336a":"# Train BERT Model\n\nmodel_bert.fit(tr_ds_bert, \n          steps_per_epoch = n_steps, \n          validation_data = val_ds_bert,\n          epochs = epochs)","4280b8a2":"# Garbage Collection\ngc.collect()","a16c79dc":"# Train DistilBERT Model\n\nmodel_Dbert.fit(tr_ds_Dbert, \n          steps_per_epoch = n_steps, \n          validation_data = val_ds_Dbert,\n          epochs = epochs)","87e1fd4c":"# Garbage Collection\ngc.collect()","2434a228":"# Train XLM-RobERTa Model\n\nmodel_XLMRoberta.fit(tr_ds_XR, \n          steps_per_epoch = n_steps, \n          validation_data = val_ds_XR,\n          epochs = epochs)","8f2cfbec":"# Garbage Collection\ngc.collect()","2ce9daf9":"# Evaluate BERT\nres_bert = model_bert.evaluate(val_ds_bert, verbose=0)\n\n# Evaluate DistilBERT\nres_Dbert = model_Dbert.evaluate(val_ds_Dbert, verbose=0)\n\n# Evaluate XLM-RoBERTa\nres_XlmRoberta = model_XLMRoberta.evaluate(val_ds_XR, verbose=0)\n\n#Tabulate Data\ntab_data = [[\"BERT\",\"30\",\"Adam\",\"128\",\"1e-5\",'{:.2%}'.format(res_bert[1])],\n            [\"DistilBERT\",\"30\",\"Adam\",\"128\",\"1e-5\",'{:.2%}'.format(res_Dbert[1])],\n            [\"XLM-RoBERTa\",\"30\",\"Adam\",\"128\",\"1e-5\",'{:.2%}'.format(res_XlmRoberta[1])]]   \n    \nprint(tabulate(tab_data, headers=['Models','Epochs','Optimizer','Batch Size','Learning Rate','Accuracy'], tablefmt='pretty'))","c672224c":"# Libraries","15de3b08":"# Data","88c836fe":"# Build & Train Model\n\nNow we shall build a model with the pre-trained BERT transformer model into Keras Functional Model","bb671686":"# Data Prep\n\n* BERT Model = bert-base-multilingual-cased\n* DistilBERT Model = distilbert-base-multilingual-cased\n* XLM-RoBERTa Model = xlm-roberta-base","960729a9":"# Evaluation"}}