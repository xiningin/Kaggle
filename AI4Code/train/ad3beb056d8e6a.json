{"cell_type":{"4dc564fe":"code","1217ee7b":"code","fc18536f":"code","e8936a49":"code","7e561efb":"code","3246156a":"code","09a45f83":"code","e300d65c":"code","022b3893":"code","13cb12a7":"code","5f4c31f1":"code","a41fb0fa":"code","d94266f5":"code","8ded1876":"code","7fa21887":"code","5da1a5d7":"code","5479acae":"code","0e44270e":"code","7fb05351":"code","3f1adf79":"code","82f72e70":"code","430b048e":"code","f8c71a13":"markdown","a9cc3973":"markdown","e06ad9bc":"markdown","6dae9287":"markdown","b6fb7ece":"markdown","c7408469":"markdown","7d3e6044":"markdown","9b26c2f2":"markdown","c8d88fbb":"markdown","4a72c20b":"markdown","afaeb310":"markdown","7884b3de":"markdown","95e63a4b":"markdown","e9038cc7":"markdown","49beab29":"markdown","0f2527bc":"markdown","05245cd8":"markdown","c2a034fd":"markdown","613f7e5a":"markdown","67883b45":"markdown","488ab17b":"markdown"},"source":{"4dc564fe":"import numpy as np\nimport pandas as pd\nfrom numpy.random import *\nfrom numpy import *  \nimport pickle\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n%matplotlib inline","1217ee7b":"#Sigmoid function and Derivative of Sigmoid function\nsigmoid = lambda Z: 1\/(1+exp(-Z))\ndsigmoid = lambda A: A*(1-A)\n\n\n#RelU function & Derivative of ReLU function\nReLU = lambda Z: Z.clip(0)\ndReLU= lambda A: (A > 0)*1\n\n\n#Derivative of tanh\ndtanh = lambda A: 1-A**2\n\n#Derivative of arctanh\ndarctanh = lambda A:1\/(A**2+1)\n\n#Softplus function & Derivative of softplus function\nsplus = lambda Z: log(1+exp(Z))\ndsplus = lambda A: 1\/(1+exp(A))\n\n# Linear model\nlinear = lambda X,w,b: X@w+b","fc18536f":"\"Time step (ts)\"\ndef steps(x, step):   \n    obs  = len(x)-step\n    xt   = x[:obs,:]\n    for i in arange(1,step+1):\n        xt = hstack((xt, x[i:obs+i,:]))   \n    return xt\n\n\"Standard Scaler\"\ndef stdscl(x):\n    maxtemp = max(x)\n    mintemp = min(x)\n    print(\"Max : \",maxtemp)\n    print(\"Min : \",mintemp)\n    x_n = (x-mintemp)\/(maxtemp-mintemp)\n    return x_n, maxtemp, mintemp\n\n\"Undo the standard scale\"\ndef undoscl(x, maxx, minx):\n    x_n = (x*(maxx-minx))+minx\n    return x_n","e8936a49":"\"Seed can be define or undefined\"\nseed(20201212)\nclass NeuralNetwork:\n    #Class init\n    def __init__(self,x,y,hlayers,alpha):\n        \"Class variable passing\"\n        self.X  = x #input\n        self.y  = y #output\n        self.hlayers = hlayers\n        self.\u03b1  = alpha\n        \n        \"Dimension Validation of Inpuit\"\n        assert ndim(y)       == 2   \n        assert type(hlayers) == list    \n        \n        \"Input size to class variable\"\n        ni = shape(x)[1]  # Number of Features\n        self.N, no = shape(y)   # Observation \/ Result Dimension\n        \n        \"Neuron Size Initialization\"\n        neurons = [ni] # First layer Neurons\n        neurons.extend(hlayers) # Second to N-2 Layers Neurons, can be customize by input\n        neurons.append(no) # Last\/ N layer of Neurons\n        self.nlayers  = len(neurons)-1  # Number of Layers\n\n        \"Initial values for weight andb bias\"\n        # w and b matix intialized using standarded random bumber\n        self.w, self.b = [], []\n        for i in arange(self.nlayers):\n            self.w.append(randn(neurons[i], neurons[i+1]))  \n            self.b.append(randn(1, neurons[i+1]))              \n \n\n    \"Training Function Definition\"\n    def training(self):\n        \n        \"Forward propagation\"\n        Z, A = [], [self.X]\n        #Hidden layes dan output layers dipisah\n        \"Hidden Layers\"\n        for j in arange(len(self.hlayers)):\n            Z.append(linear(A[j],self.w[j],self.b[j]))  #Linear Function\n            A.append(tanh(Z[j]))     #Activation function\n        \n        \"Output Layers\"\n        Z.append(linear(A[self.nlayers-1],self.w[self.nlayers-1],self.b[self.nlayers-1])) \n        A.append(sigmoid(Z[self.nlayers-1]))        \n        \n        self.predicted_y = A[self.nlayers]\n        e         = self.y-self.predicted_y\n        self.Cost = e.T@e\/self.N  #mean squared error\n\n        \"Backward propagation (perambatan mundur)\"\n        dCdZ = [(-2*e\/self.N)*dtanh(A[::-1][0])]  #Last layer init values \n        for m in arange(self.nlayers-1):  #Values Assignment from last layer to the first layer\n            dCdZ.append((dCdZ[m]@self.w[::-1][m].T)*dtanh(A[::-1][m+1]))\n        \n        \n        \"Update parameters (w and b):\"\n        one = ones([1,self.N])\n        dCdw, dCdb = [], []  \n    \n        for n in arange(self.nlayers): \n            \"Gradient Descent ALgorithm\"\n            dCdw.append(A[n].T@dCdZ[::-1][n])  \n            self.w[n] -= self.\u03b1*dCdw[n]   \n        \n            dCdb.append(one@dCdZ[::-1][n])  \n            self.b[n] -= self.\u03b1*dCdb[n]           \n      \n    \n    \"Predict Function Definition\"\n    def predict(self, x_test):   \n        #Forward Propragation\n        Z, A = [], [x_test]\n        \n        \"Hidden Layers\"\n        for j in arange(len(self.hlayers)):\n            Z.append(linear(A[j],self.w[j],self.b[j]))  #Reaksi kimia di layer ke (j + 1)\n            A.append(tanh(Z[j]))     #Aliran listrik di layer ke (j + 1)\n        \n        \"Output Layers\"\n        Z.append(linear(A[self.nlayers-1],self.w[self.nlayers-1],self.b[self.nlayers-1])) \n        A.append(sigmoid(Z[self.nlayers-1]))     \n        \n        y_predicted = A[self.nlayers]\n        return y_predicted\n","7e561efb":"Path = r\"C:\\Users\\Windows 10\\Desktop\\Data Science\\Deep Learning MA\\ANTM.JK.csv\"\nraw_data = pd.read_csv(Path)\nraw_data.head(5)","3246156a":"raw_data_AC = raw_data[\"Adj Close\"]","09a45f83":"print(raw_data_AC.isnull().sum())\nprint(raw_data_AC[raw_data_AC.isnull()])\nnull_id = raw_data_AC[raw_data_AC.isnull()].index[0]","e300d65c":"raw_data_AC[null_id] = raw_data_AC[null_id - 1] + raw_data_AC[null_id + 1]\nraw_data_AC[null_id] *= 0.5\nraw_data_AC[null_id]","022b3893":"raw_data_AC.isnull().sum()","13cb12a7":"#Plot Data\nplt.plot(raw_data_AC, label = 'Data Value')\nplt.legend()\nplt.show()","5f4c31f1":"\"Data Strandardization\"\ndata_AC_std, max_data, min_data= stdscl(raw_data_AC)\ndata_AC_t = data_AC_std[:,newaxis]","a41fb0fa":"\"Gives lag to the signal to determne number of feature used\"\nts = 5\ndata_AC_lagged = steps(data_AC_t, ts)\ndata_AC_lagged","d94266f5":"\"Split Data\"\n# Split proportion 80-20\nnum = len(data_AC_lagged)*8\/\/10\n# Training Data\nx_train = data_AC_lagged[0:num,:-1]\ny_train = data_AC_lagged[0:num,-1:]\n\n# Test Data\nx_test = data_AC_lagged[num:,:-1]\ny_test = data_AC_lagged[num:,-1:]\nprint(\"Number of traning data :\", num)\nprint(\"Number of test data :\", len(x_test))","8ded1876":"\"Model Parameter\"\nhlayers = [3]\nalpha = 1.2\nepochs = 32000","7fa21887":"\"Model Initialization\"\nann = NeuralNetwork(x_train, y_train, hlayers, alpha)","5da1a5d7":"\"Model Training\"\n# Init CLock\ntic = datetime.now()\nmse = []\nfor i in range(epochs):\n    ann.training()\n    mse.append(np.squeeze(ann.Cost))\n    if (i+1)%(epochs\/10) == 0:\n        print(\"For ephocs number\", i+1, \", calculated MSE is %8.7f\" %ann.Cost,\" with the training time of \", datetime.now()-tic)     \ntoc = datetime.now()\nprint(\"Training Complete\")\nprint(\"\\nTraining MSE is %8.7f \"%ann.Cost, \"with training time of \", toc-tic )","5479acae":"\"Cost Plot\"\n#Plot Training MSE\nplt.plot(mse, label = 'Training Cost (MSE)')\nplt.legend()\nplt.show()","0e44270e":"\"Model Train Outcome Plot\"\nplt.plot(y_train, label = 'Real Value')\nplt.plot(ann.predicted_y, label = 'Predicted Value')\nplt.legend()\nplt.show()","7fb05351":"\"Model Validation\"\ny_predict = ann.predict(x_test)\n#mse calculation\nmsetest = (y_test - y_predict).T@(y_test - y_predict)\/len(y_test)\n\nprint(\"Training MSE : %8.7f \"%ann.Cost, \" Validation MSE: %8.7f \"%msetest )","3f1adf79":"\"Re-scale the data\"\ny_predict_val = undoscl(y_predict, max_data, min_data)\ny_test_val  = undoscl(y_test, max_data, min_data)","82f72e70":"\"Plot Validation Result\"\nplt.plot(y_test_val, label = 'Real Value')\nplt.plot(y_predict_val, label = 'Predicted Value')\nplt.legend()\nplt.show()","430b048e":"print(\"Next-day predicted Adj-Close price of ANTM is\",y_test_val[-1][0])","f8c71a13":"<p>Data Line Graph<\/p>","a9cc3973":"##### Model Prediction Validation \/ Test","e06ad9bc":"#### Machine Learning Model","6dae9287":"<p><strong> -  Checking the null values<\/strong><\/p>","b6fb7ece":"<p>Because we only got 1 nulll values, we used simple imputation method using the average of previous-after data<\/p>","c7408469":"#### Parameter Variation Result","7d3e6044":"##### Prediction Result","9b26c2f2":"##### Input Data","c8d88fbb":"##### Data Preparation and Feature Engineering","4a72c20b":"<p>\nThe data used in this prediction is a time-series data so the data can't be shuffle. From the graph above, we can see that the range of data at the first 80% and last 20% are not quite different. Because of that, we will use simple split to the data, first 80% for training and last 20% for validating or testing.\n<\/p>","afaeb310":"##### Library","7884b3de":"<p>The training MSE is not far off the Validation MSE, so that we can conclude the model is <strong>not overfitting<\/strong><\/p>","95e63a4b":"##### Additional Function Declaration","e9038cc7":"    1. Activation  Function in hlayers = tanh, output = sigmoid,\n    Training MSE 0.0001805  and Validation MSE 0.0002878 with best lr = 1.2\n    2. Activation  Function in hlayers = sigmoid, output = sigmoid,\n    Training MSE 0.0002095  and Validation MSE 0.0003012 with best lr = 8\n    3. Activation  Function in hlayers = arctanh, output = sigmoid,\n    Training MSE nan  and Validation MSE nan with best lr =1 \n    4. Activation  Function in hlayers = ReLU, output = sigmoid,\n    Training MSE 0.0002711  and Validation MSE 0.0003612 with best lr =0.35\n    5. Activation  Function in hlayers = softplus, output = sigmoid,\n    Training MSE 0.0013956  and Validation MSE 0.0021890 with best lr =0.55","49beab29":"##### Activation Function Declaration","0f2527bc":"##### EDA","05245cd8":"##### Pre-Processing","c2a034fd":"##### Neural Network Class","613f7e5a":"# ANTM  Stock Price prediction Using Neural Network\n#### by Frans Jason","67883b45":"<p>We are going to use only the <strong>\"Adj Close\"<\/strong> Column of te Data<\/p>","488ab17b":"#### Description :\n<p>\nThis notebook will show a simple way to implement a neural network from scratch to solve real-life problem. This notebook will show how to predict next-day price of a stock, in this case is ANTM (Pt. Aneka Tambang) from Infonesia, and show the effect of parameters used in neural network.\n<\/p>"}}