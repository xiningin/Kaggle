{"cell_type":{"ec6bf84b":"code","b9539dc8":"code","945c803b":"code","5d0dd572":"code","6d11c15b":"code","6b34181b":"code","0e198545":"code","7bc9ffba":"code","b9373777":"code","9aa95f75":"code","662cb5ee":"code","e55f5397":"code","e1857ab9":"code","77dfd43c":"code","eb35e284":"code","69dd99e7":"code","08994d2e":"code","a11f6eab":"code","096aa1b5":"code","d3749a10":"code","4e48ce7d":"code","537007c4":"code","f0db02c0":"code","8fe1e39d":"code","e410e87b":"code","8d1cfed4":"code","1ebbcf2f":"code","7b23f48a":"code","c9c09d73":"code","4c4368dc":"code","2a53c4ee":"code","d8e99d49":"code","97ae4c16":"code","c1d2407d":"code","dbe9f3de":"code","51a049c5":"code","2a26eff9":"code","42bd274b":"code","a63b8d01":"code","0294f034":"code","5014f069":"code","b3f0da7d":"code","2d42dce2":"code","471cfd2e":"code","4a02b355":"code","cdf6c248":"code","1d4aa865":"code","f8cbfe44":"code","522967e2":"code","ebdf91de":"code","3731e406":"code","9a5abe06":"code","a52c0d5d":"code","e8410d7a":"code","2aadbc17":"code","ade7f7f0":"code","dc8ad9f6":"code","b869dce9":"code","f1c8a84b":"code","f6ada5eb":"code","3fbfa036":"code","899400b8":"code","9ff134ce":"code","91bce258":"code","a87a0fab":"code","3f84cde5":"code","af23a980":"code","e320e60c":"code","1d3c6407":"code","2fd60b14":"code","6f87672b":"code","6c8bf4c6":"code","ed51ede9":"code","b8e528da":"code","37df6556":"code","3d45d450":"code","3b31287c":"code","b6c97b32":"code","352a195d":"code","544c9fc8":"code","872f8478":"code","883c1f71":"code","47c11d27":"code","4e9a6450":"code","5d142cbd":"code","989a9414":"code","48f1d358":"code","c29721cd":"code","00c2da19":"code","570405a1":"code","20c9225a":"code","59165102":"code","ef91ede5":"code","586e19a3":"code","eb793f29":"code","68be42a2":"code","5fbcfdec":"code","d4496945":"code","2ce9b885":"code","41516e34":"markdown","86b91eb0":"markdown","9fc9dba4":"markdown","232ad42c":"markdown","7eb1c2dd":"markdown","0d3d1eff":"markdown","811b37f5":"markdown","4c238b89":"markdown","3559758e":"markdown","0fe1e1bf":"markdown","fc49281f":"markdown","b594cf92":"markdown","d3fd646d":"markdown","497a34b9":"markdown","72479ab0":"markdown","f43004c2":"markdown","78297d14":"markdown","e4c1675c":"markdown","3c6eb707":"markdown","63900334":"markdown","58839785":"markdown","ebc8f91d":"markdown","840f50d3":"markdown","18e40e5c":"markdown","69162a5b":"markdown","f5311aa9":"markdown","312d0c46":"markdown","787b0181":"markdown","4522f24c":"markdown","947c8073":"markdown","c35c8bbd":"markdown","99204a08":"markdown","f54e1311":"markdown","4df39a58":"markdown","48722a3d":"markdown","95a77a29":"markdown","ade08813":"markdown","fdac5ecd":"markdown","8be1a02b":"markdown","bfe7680f":"markdown","de468a64":"markdown","b96d4ae1":"markdown"},"source":{"ec6bf84b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b9539dc8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport string\nfrom tqdm import tqdm\n\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom bs4 import BeautifulSoup\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom collections import OrderedDict\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split,RandomizedSearchCV\nfrom sklearn.metrics import classification_report,f1_score\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow import keras \nfrom keras import backend as K\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.optimizers import Adam\n\nimport torch\nimport transformers","945c803b":"train = pd.read_csv(r'\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv(r'\/kaggle\/input\/nlp-getting-started\/test.csv')","5d0dd572":"print('Number of datapoints in the train dataset : ',train.shape[0])\nprint('Number of datapoints in the test dataset : ',test.shape[0])","6d11c15b":"train.head()","6b34181b":"train.info()","0e198545":"test.info()","7bc9ffba":"train.describe()","b9373777":"#removing any shortforms if present\ndef remove_shortforms(phrase):\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\ndef remove_special_char(text):\n    text = re.sub('[^A-Za-z0-9]+',\" \",text)\n    return text\n\ndef remove_wordswithnum(text):\n    text = re.sub(\"\\S*\\d\\S*\", \"\", text).strip()\n    return text\n\ndef lowercase(text):\n    text = text.lower()\n    return text\n\ndef remove_stop_words(text):\n    text = remove_stopwords(text)\n    return text\n\nst = SnowballStemmer(language='english')\ndef stemming(text):\n    r= []\n    for word in text :\n        a = st.stem(word)\n        r.append(a)\n    return r\n\ndef listToString(s):  \n    str1 = \" \"   \n    return (str1.join(s))\n\ndef remove_punctuations(text):\n    text = re.sub(r'[^\\w\\s]','',text)\n    return text\n\ndef remove_links(text):\n    text = re.sub(r'http\\S+', '', text)\n    return text\n\nlemmatizer = WordNetLemmatizer()\ndef lemmatize_words(text):\n    text = lemmatizer.lemmatize(text)\n    return text\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)","9aa95f75":"Y = train['target']\ntrain = train.drop('target',axis=1)\ndata = pd.concat([train,test],axis=0).reset_index(drop=True)\ndata.head()","662cb5ee":"for i in range(len(data['text'])):\n    data['text'][i] = str(data['text'][i])","e55f5397":"data['text'][1]","e1857ab9":"for i in range(len(data['text'])):\n    data['text'][i] = remove_shortforms(data['text'][i])\n    data['text'][i] = remove_special_char(data['text'][i])\n    data['text'][i] = remove_wordswithnum(data['text'][i])\n    data['text'][i] = lowercase(data['text'][i])\n    data['text'][i] = remove_stop_words(data['text'][i])\n    text = data['text'][i]\n    text = text.split()\n    data['text'][i] = stemming(text)\n    s = data['text'][i]\n    data['text'][i] = listToString(s)\n    data['text'][i] = lemmatize_words(data['text'][i])","77dfd43c":"data['text'][1]","eb35e284":"cv = CountVectorizer(ngram_range=(1,3))\ntext_bow = cv.fit_transform(data['text'])\nprint(text_bow.shape)","69dd99e7":"train_text = text_bow[:train.shape[0]] \ntest_text = text_bow[train.shape[0]:] ","08994d2e":"print(train_text.shape)\nprint(test_text.shape)","a11f6eab":"X_train,X_test,Y_train,Y_test = train_test_split(train_text,Y,test_size=0.2)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","096aa1b5":"# lr = LogisticRegression(max_iter=2000)\n\n# params = {\n#     'C' :[0.0001,0.001,0.01,0.1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,1,2,3,4,5,6,7,10,100,1000],\n#     'penalty': ['l1','l2']\n# }\n\n# clf = RandomizedSearchCV(lr,params,n_jobs=-1,cv=10)\n# clf.fit(X_train,Y_train)\n# print(clf.best_params_)","d3749a10":"lr = LogisticRegression(C=10,penalty='l2')\nlr.fit(X_train,Y_train)\npred = lr.predict(X_test)\nprint(\"F1 score :\",f1_score(Y_test,pred))\nprint(\"Classification Report \\n\\n:\",classification_report(Y_test,pred))","4e48ce7d":"lr = LogisticRegression(C=10,penalty='l2',max_iter=2000)\nlr.fit(train_text,Y)\npred = lr.predict(test_text)\nsubmit = pd.DataFrame(test['id'],columns=['id'])\nprint(len(pred))\nsubmit.head()","537007c4":"submit['target'] = pred\nsubmit.to_csv(\"realnlp.csv\",index=False)","f0db02c0":"tfidf = TfidfVectorizer(ngram_range=(1,3))\ntext_tfidf = tfidf.fit_transform(data['text'])\nprint(text_tfidf.shape)","8fe1e39d":"train_text = text_tfidf[:train.shape[0]] \ntest_text = text_tfidf[train.shape[0]:] \nprint(train_text.shape)\nprint(test_text.shape)","e410e87b":"X_train,X_test,Y_train,Y_test = train_test_split(train_text,Y,test_size=0.2)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","8d1cfed4":"# lr = LogisticRegression(max_iter=2000)\n\n# params = {\n#     'C' :[0.0001,0.001,0.01,0.1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,1,2,3,4,5,6,7,10,100,1000],\n#     'penalty': ['l1','l2']\n# }\n\n# clf = RandomizedSearchCV(lr,params,n_jobs=-1,cv=10)\n# clf.fit(X_train,Y_train)\n# print(clf.best_params_)","1ebbcf2f":"lr = LogisticRegression(C=100,penalty='l2',max_iter=2000)\nlr.fit(X_train,Y_train)\npred = lr.predict(X_test)\nprint(\"F1 score :\",f1_score(Y_test,pred))\nprint(\"Classification Report :\",classification_report(Y_test,pred))","7b23f48a":"print(\"Number of null values in data keywords column : \",data['keyword'].isnull().sum())","c9c09d73":"data.head()","4c4368dc":"data['keyword'] = data['keyword'].fillna(\"unknown\")\ndata.head()","2a53c4ee":"combined_text = [None] * len(data['text'])\nfor i in range(len(data['text'])):\n    if data['keyword'][i] == 'unknown':\n        combined_text[i] = data['text'][i]\n    else:\n        combined_text[i] = data['text'][i] + \" \" + data['keyword'][i] + \" \" + data['keyword'][i] + \" \" + data['keyword'][i]\ndata['combined_text'] = combined_text","d8e99d49":"data['combined_text'][88]","97ae4c16":"for i in range(len(data['combined_text'])):\n    data['combined_text'][i] = str(data['combined_text'][i])","c1d2407d":"for i in range(len(data['combined_text'])):\n    data['combined_text'][i] = remove_shortforms(data['combined_text'][i])\n    data['combined_text'][i] = remove_special_char(data['combined_text'][i])\n    data['combined_text'][i] = remove_wordswithnum(data['combined_text'][i])\n    data['combined_text'][i] = lowercase(data['combined_text'][i])\n    data['combined_text'][i] = remove_stop_words(data['combined_text'][i])\n    text = data['combined_text'][i]\n    text = text.split()\n    data['combined_text'][i] = stemming(text)\n    s = data['combined_text'][i]\n    data['combined_text'][i] = listToString(s)\n    data['combined_text'][i] = lemmatize_words(data['combined_text'][i])","dbe9f3de":"data['combined_text'][88]","51a049c5":"cv = CountVectorizer(ngram_range=(1,3))\ntext_bow = cv.fit_transform(data['combined_text'])\nprint(text_bow.shape)","2a26eff9":"train_text = text_bow[:train.shape[0]] \ntest_text = text_bow[train.shape[0]:] ","42bd274b":"X_train,X_test,Y_train,Y_test = train_test_split(train_text,Y,test_size=0.2)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","a63b8d01":"# lr = LogisticRegression(max_iter=2000)\n\n# params = {\n#     'C' :[0.0001,0.001,0.01,0.1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,1,2,3,4,5,6,7,10,100,1000],\n#     'penalty': ['l1','l2']\n# }\n\n# clf = RandomizedSearchCV(lr,params,n_jobs=-1,cv=10)\n# clf.fit(X_train,Y_train)\n# print(clf.best_params_)","0294f034":"lr = LogisticRegression(C=1,penalty='l2',max_iter=2000)\nlr.fit(X_train,Y_train)\npred = lr.predict(X_test)\nprint(\"F1 score :\",f1_score(Y_test,pred))\nprint(\"Classification Report :\",classification_report(Y_test,pred))","5014f069":"tfidf = TfidfVectorizer(ngram_range=(1,3))\ntext_tfidf = tfidf.fit_transform(data['combined_text'])\nprint(text_tfidf.shape)","b3f0da7d":"train_text = text_tfidf[:train.shape[0]] \ntest_text = text_tfidf[train.shape[0]:] ","2d42dce2":"X_train,X_test,Y_train,Y_test = train_test_split(train_text,Y,test_size=0.2)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","471cfd2e":"# lr = LogisticRegression(max_iter=2000)\n\n# params = {\n#     'C' :[0.0001,0.001,0.01,0.1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,1,2,3,4,5,6,7,10,100,1000],\n#     'penalty': ['l1','l2']\n# }\n\n# clf = RandomizedSearchCV(lr,params,n_jobs=-1,cv=10)\n# clf.fit(X_train,Y_train)\n# print(clf.best_params_)","4a02b355":"lr = LogisticRegression(C=2,penalty='l2',max_iter=2000)\nlr.fit(X_train,Y_train)\npred = lr.predict(X_test)\nprint(\"F1 score :\",f1_score(Y_test,pred))\nprint(\"Classification Report :\",classification_report(Y_test,pred))","cdf6c248":"print('Loading word vectors...')\nword2vec = {}\nwith open(os.path.join('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.200d.txt'), encoding = \"utf-8\") as f:\n  # is just a space-separated text file in the format:\n  # word vec[0] vec[1] vec[2] ...\n    for line in f:\n        values = line.split() #split at space\n        word = values[0]\n        vec = np.asarray(values[1:], dtype='float32') #numpy.asarray()function is used when we want to convert input to an array.\n        word2vec[word] = vec\nprint('Found %s word vectors.' % len(word2vec))","1d4aa865":"train = pd.read_csv(r'\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv(r'\/kaggle\/input\/nlp-getting-started\/test.csv')","f8cbfe44":"Y = train['target']\ntrain = train.drop('target',axis=1)\ndata = pd.concat([train,test],axis=0).reset_index(drop=True)\ntext_data = data['text']","522967e2":"text_data","ebdf91de":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(text_data)\nsequences = tokenizer.texts_to_sequences(text_data)","3731e406":"word2index = tokenizer.word_index\nprint(\"Number of unique tokens : \",len(word2index))","9a5abe06":"data_padded = pad_sequences(sequences,100)\nprint(data_padded.shape)","a52c0d5d":"data_padded[6]","e8410d7a":"train_pad = data_padded[:train.shape[0]]\ntest_pad = data_padded[train.shape[0]:]","2aadbc17":"embedding_matrix = np.zeros((len(word2index)+1,200))\n\nembedding_vec=[]\nfor word, i in tqdm(word2index.items()):\n    embedding_vec = word2vec.get(word)\n    if embedding_vec is not None:\n        embedding_matrix[i] = embedding_vec","ade7f7f0":"print(embedding_matrix[1])","dc8ad9f6":"model1 = keras.models.Sequential([\n    keras.layers.Embedding(len(word2index)+1,200,weights=[embedding_matrix],input_length=100,trainable=False),\n    keras.layers.LSTM(100,return_sequences=True),\n    keras.layers.LSTM(200),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(1,activation='sigmoid')\n])","b869dce9":"model1.summary()","f1c8a84b":"model1.compile(\n  loss='binary_crossentropy',\n  optimizer='adam',\n  metrics=['accuracy'],\n)","f6ada5eb":"history1 = model1.fit(train_pad,Y,\n                    batch_size=64,\n                    epochs=10,\n                    validation_split=0.2\n)","3fbfa036":"plt.figure(figsize=(20,8))\nplt.plot(history1.history['loss'], label='train')\nplt.plot(history1.history['val_loss'], label='test')\nplt.legend()\nplt.grid()\nplt.show()","899400b8":"plt.figure(figsize=(20,8))\nplt.plot(history1.history['accuracy'], label='train')\nplt.plot(history1.history['val_accuracy'], label='test')\nplt.legend()\nplt.grid()\nplt.show()","9ff134ce":"model2 = keras.models.Sequential([\n    keras.layers.Embedding(len(word2index)+1,200,weights=[embedding_matrix],input_length=100,trainable=False),\n    keras.layers.GRU(100,return_sequences=True),\n    keras.layers.GRU(200),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(1,activation='sigmoid')\n])","91bce258":"model2.summary()","a87a0fab":"model2.compile(\n  loss='binary_crossentropy',\n  optimizer='adam',\n  metrics=['accuracy'],\n)","3f84cde5":"history2 = model2.fit(train_pad,Y,\n                    batch_size=64,\n                    epochs=10,\n                    validation_split=0.2\n)","af23a980":"plt.figure(figsize=(20,8))\nplt.plot(history2.history['loss'], label='train')\nplt.plot(history2.history['val_loss'], label='test')\nplt.legend()\nplt.grid()\nplt.show()","e320e60c":"plt.figure(figsize=(20,8))\nplt.plot(history2.history['accuracy'], label='train')\nplt.plot(history2.history['val_accuracy'], label='test')\nplt.legend()\nplt.grid()\nplt.show()","1d3c6407":"model3 = keras.models.Sequential([\n    keras.layers.Embedding(len(word2index)+1,200,weights=[embedding_matrix],input_length=100,trainable=False),\n    keras.layers.Bidirectional(keras.layers.LSTM(100,return_sequences=True)),\n    keras.layers.Bidirectional(keras.layers.LSTM(200)),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(1,activation='sigmoid')\n])","2fd60b14":"model3.summary()","6f87672b":"model3.compile(\n  loss='binary_crossentropy',\n  optimizer='adam',\n  metrics=['accuracy'],\n)","6c8bf4c6":"history3 = model3.fit(train_pad,Y,\n                    batch_size=64,\n                    epochs=10,\n                    validation_split=0.2\n)","ed51ede9":"plt.figure(figsize=(20,8))\nplt.plot(history3.history['loss'], label='train')\nplt.plot(history3.history['val_loss'], label='test')\nplt.legend()\nplt.grid()\nplt.show()","b8e528da":"plt.figure(figsize=(20,8))\nplt.plot(history3.history['accuracy'], label='train')\nplt.plot(history3.history['val_accuracy'], label='test')\nplt.legend()\nplt.grid()\nplt.show()","37df6556":"es = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',mode='max',verbose=1,patience=3)","3d45d450":"history = model3.fit(train_pad,Y,\n                    batch_size=64,\n                    epochs=30,\n                    validation_split=0.2,\n                    callbacks=[es]\n)","3b31287c":"plt.figure(figsize=(20,8))\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.grid()\nplt.show()","b6c97b32":"plt.figure(figsize=(20,8))\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.grid()\nplt.show()","352a195d":"submit = pd.DataFrame(test['id'],columns=['id'])\npredictions = model3.predict(test_pad)\nsubmit['target_prob'] = predictions\nsubmit.head()","544c9fc8":"target = [None]*len(submit)\nfor i in range(len(submit)):\n    target[i] = np.round(submit['target_prob'][i]).astype(int)\nsubmit['target'] = target\nsubmit.head()","872f8478":"submit = submit.drop('target_prob',axis=1)\nsubmit.to_csv('real-nlp_lstm.csv',index=False)","883c1f71":"train = pd.read_csv(r'\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv(r'\/kaggle\/input\/nlp-getting-started\/test.csv')","47c11d27":"train.head()","4e9a6450":"Y = train['target']\ntrain = train.drop('target',axis=1)\ntext_data_train = train['text']\ntext_data_test = test['text']","5d142cbd":"Y.value_counts()","989a9414":"tokenizer = transformers.BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)\nbert_model = transformers.TFBertModel.from_pretrained('bert-large-uncased')","48f1d358":"def bert_encode(data,maximum_length) :\n    input_ids = []\n    attention_masks = []\n  \n\n    for i in range(len(data)):\n        encoded = tokenizer.encode_plus(\n        \n          data[i],\n          add_special_tokens=True,\n          max_length=maximum_length,\n          pad_to_max_length=True,\n        \n          return_attention_mask=True,\n        \n        )\n      \n        input_ids.append(encoded['input_ids'])\n        attention_masks.append(encoded['attention_mask'])\n    return np.array(input_ids),np.array(attention_masks)","c29721cd":"train_input_ids,train_attention_masks = bert_encode(text_data_train,100)\ntest_input_ids,test_attention_masks = bert_encode(text_data_test,100)","00c2da19":"train_input_ids[1]","570405a1":"train_attention_masks[1]","20c9225a":"def create_model(bert_model):\n    input_ids = tf.keras.Input(shape=(100,),dtype='int32')\n    attention_masks = tf.keras.Input(shape=(100,),dtype='int32')\n  \n    output = bert_model([input_ids,attention_masks])\n    output = output[1]\n    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n    model.compile(Adam(lr=6e-6), loss='binary_crossentropy', metrics=['accuracy'])\n    return model","59165102":"model = create_model(bert_model)\nmodel.summary()","ef91ede5":"history = model.fit([train_input_ids,train_attention_masks],Y,\n                    validation_split=0.2,\n                    epochs=3,\n                    batch_size=5)","586e19a3":"# es = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',mode='max',verbose=1,patience=3)","eb793f29":"# history = model.fit([train_input_ids,train_attention_masks],Y,\n#                     batch_size=10,\n#                     epochs=10,\n#                     validation_split=0.2,\n#                     callbacks=[es]\n# )","68be42a2":"plt.figure(figsize=(20,8))\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.grid()\nplt.show()","5fbcfdec":"plt.figure(figsize=(20,8))\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.grid()\nplt.show()","d4496945":"result = model.predict([test_input_ids,test_attention_masks])\nresult = np.round(result).astype(int)\nsubmit = pd.DataFrame(test['id'],columns=['id'])\nsubmit['target'] = result\nsubmit.head()","2ce9b885":"submit.to_csv('real_nlp_bert.csv',index=False)","41516e34":"<p style=\"color:red\">Here i have done padding on the data itself so as to convert it into equal sized vectors which are processed by the models i am going to use later on these embeddings.\n\n<p style=\"color:green\">To know more about padding of vectors just refer to the video below :\n\nhttps:\/\/www.coursera.org\/lecture\/natural-language-processing-tensorflow\/padding-2Cyzs <--- <span style=\"color:purple\">Padding<\/span>","86b91eb0":"<h1 style=\"color:turquoise\"><span style=\"color:darkblue;font-size:50px\">4. BERT<\/span> (Bidirectional Encoder Representations from Transformers) :<\/h1>\n\n![](https:\/\/searchengineland.com\/figz\/wp-content\/seloads\/2019\/10\/GoogleBert_1920.jpg)\n\n<p style=\"color:green\">It is an open-sourced NLP pre-training model developed by researchers at Google in 2018 and it preforms very well with text data as compared to other models.\n\n<p style=\"color:green\">Most of us may not have an idea about it as i have recently started learning about it too. I am gonna provide some links which are very helpful in starting with it. Just follow them up for the idea about transformers and BERT models\n \nhttp:\/\/jalammar.github.io\/illustrated-transformer\/ <--- <span style=\"color:purple\">Transformers<\/span>\n\nhttp:\/\/jalammar.github.io\/illustrated-bert\/ <--- <span style=\"color:purple\">BERT introduction<\/span>\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2019\/09\/demystifying-bert-groundbreaking-nlp-framework\/ <--- <span style=\"color:purple\">BERT<\/span>","9fc9dba4":"<p style=\"color:teal\">I just ran it for 3 epochs because i got an okayish accuracy but you can change that according to your convenience.\n\n<p style=\"color:red\">You can also use early stopping here if you'd like","232ad42c":"<p style=\"color:red\">Combining the text and the keywords togther\n\n<p style=\"color:red\">Here i have used keyword after the text 3 times to give it more weight as we are predicting the text refers to a disaster or not. Maybe it will help lets see if it does","7eb1c2dd":"<h2 style=\"color:orange\"> Predictions :","0d3d1eff":"<h2 style=\"color:orange\">Bidirectional LSTM :<\/h2>\n\n![](https:\/\/www.i2tutorials.com\/wp-content\/uploads\/2019\/05\/Deep-Dive-into-Bidirectional-LSTM-i2tutorials.jpg)\n\n<p style=\"color:red\">Let us use Bidirectional LSTM's to see if the results change or not\n\n<p style=\"color:red\">If you want to know more about Bidirectional LSTM and how they work please refer to the links below \n\nhttps:\/\/machinelearningmastery.com\/develop-bidirectional-lstm-sequence-classification-python-keras\/ <--- <span style=\"color:purple\">Bidirectional LSTM inroduction<\/span>\n\nhttps:\/\/www.coursera.org\/lecture\/nlp-sequence-models\/bidirectional-rnn-fyXnn<--- <span style=\"color:purple\">Bidirectional LSTM<\/span>","811b37f5":"<h2 style=\"color:orange\">Reading the data :<\/h2>\n\n<p style=\"color:red\">I am now gonna read the data here in the .csv files","4c238b89":"<p style=\"color:red\">This notebook is all about NLP which is a very hot topic in the field of machine learning and deeplearning for a while now.The notebook is sure a big longer but please stick with me to learn about NLPs. It'll surely help you a lot if you are beginner in the topic\nIf you are just beginning to learn about NLP(Natural Language Processing) and its applications please refer to the link i have provided below to get the very basic knowledge about it :\n    \nhttps:\/\/towardsdatascience.com\/a-gentle-introduction-to-natural-language-processing-e716ed3c0863 <---- <span style=\"color:purple\">Introduction to NLP <\/span>\n\n                                                                                                       \n<p style=\"color:red\">If you are using python as your language for ML it provides various tools to process text in different ways. Some of them are :\n<ul style=\"color:green\">\n    <li>SkLearn<\/li>\n    <li>Tensorflow<\/li> \n    <li>Spacy<\/li>\n    <li>NLTK - Natural Language Toolkit <\/li>\n    <li>Gensim<\/li>\n    <li>OpenNLP<\/li>\n    etc..\n<\/ul>\n\n<p style=\"color:red\">If you want to get an idea about these tools just refer to the link below :\n\nhttps:\/\/towardsdatascience.com\/5-heroic-tools-for-natural-language-processing-7f3c1f8fc9f0 <--- <span style=\"color:purple\">Tools for NLP<\/span>\n\n<p style=\"color:red\">Here is a very interesting article about NLP which shows a bit more about the components of the topic. It is a good read if you are interested \n\nhttps:\/\/towardsdatascience.com\/natural-language-processing-a1496244c15c#:~:text=Natural%20Language%20Processing%20(NLP)%20is,algorithms%20to%20text%20and%20speech \n\n<p style=\"color:red\">Now let us move forward","3559758e":"<p style=\"color:green\">Lets try Logistic Regression with some hyperparameter tuning on the BOW encoded data:\n    \n    \n<p style=\"color:red\">If you want to try hyperparameter tuning on your system just uncomment the code in the next block","0fe1e1bf":"<h2 style=\"color:orange\">Logistic Regression with TFIDF encoding :","fc49281f":"<h2 style=\"color:orange\">Importing Libraries :<\/h2>\n\n<p style=\"color:red\">I am going to import the basic necessities required for NLP. If you are not familier with some of them dont worry as you will learn about them sooner or later in this notebook. :)","b594cf92":"<p style=\"color:magenta\">The feature location is not a useful feature in the data as we have so many missing values in it which cannot be dealt with properly. Therefore i am not gonna perform any calculations using the location feature\n\n<h2 style=\"color:turquoise;font-size:30px\">2. Lets use the Keywords feature and see if the results change ","d3fd646d":"<p style=\"color:purple\">Preprocessing the combined data :","497a34b9":"<h2 style=\"color:orange\">Bag of words :<\/h2>\n<p style=\"color:red\">The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. \nIf you are not familier with bag of words this article is for you :\n\nhttps:\/\/machinelearningmastery.com\/gentle-introduction-bag-words-model\/ <--- <span style=\"color:purple\">Bag of Words<\/span>\n\n<p style=\"color:red\">In sklearn which is a very famous lirary in python bag of words is used as the fuction CountVectorizer.For its documentation in sklearb refer to this link below \n\n* https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html <--- <span style=\"color:purple\">Documentation<\/span>","72479ab0":"<h2 style=\"color:orange\">Logistic Regression with TFIDF encoding :","f43004c2":"<h2 style=\"color:orange\">Bag of Words with keywords :<\/h2>\n    \n<p style=\"color:red\">Applying BOW with the combined text","78297d14":"<h1 style=\"color:orange\">TFIDF encoding of the text data :<\/h1>\n\n<p style=\"color:red\">To learn more about TFIDF just follow this link below :\n    \nhttps:\/\/medium.com\/analytics-vidhya\/tf-idf-term-frequency-technique-easiest-explanation-for-text-classification-in-nlp-with-code-8ca3912e58c3 <--- <span style=\"color:purple\">TFIDF encoding of text<\/span>\n\n<p style=\"color:red\">In sklearn TFIDF is applied using the fuction TfidfVectorizer.For its documentation in sklearn refer to this link below :\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html <--- <span style=\"color:purple\">Documentation<\/span>","e4c1675c":"<p style=\"color:green\">Now i am splitting the training and text data with BOW encoding which i had combined earlier to get the dictionary of all words present in the train as well as the text data","3c6eb707":"<p style=\"color:purple\">Look above and see how the sentence changes before and after preprocessing as i have removed stopwords, converted characters to lowercase and applied lemmatization  etc....","63900334":"<h2 style=\"color:orange\">Logistic Regression with BOW :","58839785":"<h2 style=\"color:orange\">Predictions and Submission :","ebc8f91d":"<h2 style=\"color:orange\">Data Preprocessing :<\/h2>\n<p style=\"color:red\">Here i am going to write the preprocessing functions that i am going to use in this notebook. If you want to learn about stemming,lemmatization etc just refer to the link given below :<\/p>\n\nhttps:\/\/towardsdatascience.com\/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79","840f50d3":"<p style=\"color:red\">You guys can also try Bidirectional GRU if you feel like it","18e40e5c":"<p style=\"color:brown;font-size:20px\">Incase you like this notebook do not forget to give an <span style=\"color:purple;font-size:30px\">UPVOTE<\/span>. Thank you for viewing.","69162a5b":"<h2 style=\"color:orange\">GRU :<\/h2> \n\n![](https:\/\/technopremium.com\/blog\/wp-content\/uploads\/2019\/06\/gru-1-1200x600.png)\n\n<p style=\"color:red\">Let us use GRU's to see if the results change or not\n\n<p style=\"color:red\">If you want to know more about GRU and how they work please refer to the links below \n\nhttps:\/\/towardsdatascience.com\/understanding-gru-networks-2ef37df6c9be <--- <span style=\"color:purple\">GRU inroduction<\/span>\n\nhttps:\/\/www.coursera.org\/lecture\/nlp-sequence-models\/gated-recurrent-unit-gru-agZiL <--- <span style=\"color:purple\">GRU<\/span>","f5311aa9":"<p style=\"color:red\">Now lets preprocess the data using the functions defined above","312d0c46":"<p style=\"color:green\">Filling the missing or null values in the keyword feature as 'unknown'","787b0181":"<h2 style=\"color:orange\">Logistic Regression with BOW :","4522f24c":"![](https:\/\/venturebeat.com\/wp-content\/uploads\/2018\/09\/natural-language-processing-e1572968977211.jpg?w=1200&strip=all)","947c8073":"<h2 style=\"color:orange\">Predictions and Submissions :","c35c8bbd":"<p style=\"color:green\">Let me use early stopping on the Bidirectional LSTM model to show how it works","99204a08":"<h2 style=\"color:red\">TFIDF with keyword features :<\/h2>\n\n<p style=\"color:orange\">Applying TFIDF with combined text","f54e1311":"<p style=\"color:red\">Bert model achieves the best validation accuracy so far.\n\n<p style=\"color:darkblue\">I am very new at using BERT itself so if you find some errors or mistakes or if you have some suggesstions related to the kernel you are very welcome in the comments section.I'd appreciate it .\n\n<p style=\"color:darkblue\">Thanks for viewing the kernel I hope it may have helped you :)\n  Please do <span style=\"color:brown;font-size:20px\">UPVOTE<\/span> as a token of appreciation if you liked it or learned from it in any way","4df39a58":"<h1 style=\"color:magenta;font-size:50px;\">RNN :<\/h1>\n\n![](https:\/\/cdn-images-1.medium.com\/fit\/t\/1600\/480\/1*go8PHsPNbbV6qRiwpUQ5BQ.png)\n\n","48722a3d":"<h2 style=\"color:orange\">Train test split :<\/h2>\n\n<p style=\"color:purple\">The fuction used for splitting the train and test data is a sklearn function. For its documentation refer to :\n    \nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html <--- <span style=\"color:red\">Train Test split<\/span>","95a77a29":"<p style=\"color:purple\">I have seperated the target variables with the dataset here and concatenated the train and test dataset into another dataset using pd.concat which is a function in pandas library in python.","ade08813":"<h1 style=\"color:turquoise;font-size:50px\">3. Word Embeddings :<\/h1>\n    \n<p style=\"color:red\">Word embedding are also a very popular way to approach an NLP problem in which words are converted into vectors and used in various ML and deeplearning models.\n\n<p style=\"color:red\">To know more refer to the link below :\n    \nhttps:\/\/machinelearningmastery.com\/what-are-word-embeddings\/ <--- <span style=\"color:purple\">Word Embeddings<\/span>\n\n<p style=\"color:red\">Here i have used GLOVE vectors dataset for my word embeddings. You can download them either on the internet but i have just used this glove dataset provided on kaggle itself to carry out the task. \n\n<p style=\"color:red\">Here is the link :\n\nhttps:\/\/www.kaggle.com\/rtatman\/glove-global-vectors-for-word-representation <--- <span style=\"color:purple\">Glove Vectors<\/span>","fdac5ecd":"<p style=\"color:purple\">You can use Ml models here but i have used Deeplearning with the word embeddings.Lets see what kind of results we get","8be1a02b":"<p style=\"color:green\">For tokenizing the data i have used the keras.preprocessing fuction called Tokenizer.\n    You can search on google for its documentation","bfe7680f":"<h2 style=\"color:orange\">Early Stopping :<\/h2>\n\n<p style=\"color:red\">Let me try to show you how  early stopping works to determine the best validation accuracy for the model as we can see that the accuracy is maximun in the starting epochs and decreasing as we go further. Here i kept the monitor for early stopping as val_accuracy.\n\nhttps:\/\/machinelearningmastery.com\/early-stopping-to-avoid-overtraining-neural-network-models\/ <--- <span style=\"color:purple\">Early Stopping","de468a64":"<p style=\"color:red\">Here i am converting all the text sentences into str just to make sure my code does not get stuck at any point later on.","b96d4ae1":"<h2 style=\"color:orange\">LSTM :<\/h2> \n\n![](https:\/\/miro.medium.com\/max\/2840\/1*0f8r3Vd-i4ueYND1CUrhMA.png)\n\n<p style=\"color:red\">To learn more about LSTM's and how they actually work please use the links below:\n\nhttps:\/\/machinelearningmastery.com\/gentle-introduction-long-short-term-memory-networks-experts\/ <--- <span style=\"color:purple\">LSTM introduction<\/span>\n\nhttps:\/\/www.coursera.org\/lecture\/nlp-sequence-models\/long-short-term-memory-lstm-KXoay <--- <span style=\"color:purple\">LSTMs<\/span>"}}