{"cell_type":{"70fc74e6":"code","24a43f20":"code","fc40237c":"code","14b9ee18":"code","1039f532":"code","fb3a222f":"code","22e9d2e8":"code","87ee7c58":"code","fd72a37c":"code","2b7bcf73":"code","c49ae2d2":"code","ebf27a72":"code","1134b62a":"code","e06d5389":"code","f106b903":"code","b95922e1":"code","016e6273":"code","8d817184":"code","f3692553":"code","50180358":"code","d442700a":"code","bd5d35b6":"code","e6d34cfd":"code","1763b5dd":"code","2a41767d":"code","660790e5":"code","f306ef77":"code","0dc10f87":"code","7b5f480a":"code","e0f1e1e9":"code","9ebe6bff":"code","67545e8e":"code","9d070a01":"code","16cc993a":"code","9c24e4a9":"code","2e8131f1":"code","3405d844":"code","c3e62b88":"code","f6f64b38":"code","39815bd4":"code","2b638592":"code","89cbea34":"code","17a41b92":"code","04a55958":"code","4f3a4acd":"markdown","28e38976":"markdown","3d02a65d":"markdown","b5163a69":"markdown","c3014493":"markdown"},"source":{"70fc74e6":"!pip install -U newspaper3k","24a43f20":"!pip install -U stanza","fc40237c":"!pip install -U numpy","14b9ee18":"import stanza\nimport hashlib\n\nimport requests\nfrom newspaper import Article\nfrom newspaper.article import ArticleException\n\nimport numpy as np","1039f532":"from typing import List, Dict, Tuple, Iterable, Type\nfrom numpy import ndarray","fb3a222f":"stanza.download('en')","22e9d2e8":"nlp = stanza.Pipeline(processors='tokenize', lang='en', use_gpu=True)","87ee7c58":"URL = 'https:\/\/edition.cnn.com\/2020\/03\/04\/health\/debunking-coronavirus-myths-trnd\/'\nprint(URL)","fd72a37c":"def parse_article(url):\n    try:\n        r = requests.get(url, timeout=10)\n        article = Article(url)\n        article.download()\n        article.parse()\n    except:\n        article = None\n    return article","2b7bcf73":"article = parse_article(URL)","c49ae2d2":"ALL_SENTENCES = []\n\nfile_id = hashlib.md5(article.url.encode()).hexdigest()\narticle_url = article.url\narticle_published_datetime = article.publish_date\n\narticle_title = article.title\ndoc = nlp(article_title)\nfor sent in doc.sentences:\n    #S = ' '.join([w.text for w in sent.words])\n    S = str(sent.text)\n    sH = hashlib.md5(S.encode('utf-8')).hexdigest()\n    print(file_id, sH, \"%10i\"%len(ALL_SENTENCES), (S,))\n    ALL_SENTENCES.append([file_id, sH, S, article_published_datetime, article_url])\n\narticle_text = [a.strip() for a in article.text.splitlines() if a.strip()]\nfor paragraph in article_text:\n    doc = nlp(paragraph)\n    for sent in doc.sentences:\n        S = str(sent.text)\n        sH = hashlib.md5(S.encode('utf-8')).hexdigest()\n        print(file_id, sH, \"%10i\"%len(ALL_SENTENCES), (S,))\n        ALL_SENTENCES.append([file_id, sH, S, article_published_datetime, article_url])\n\nlen(ALL_SENTENCES)","ebf27a72":"CLAIMS, NO_CLAIMS = [], []\nfor file_id, sH, sentence, article_published_datetime, article_url in ALL_SENTENCES:\n    # clean the sentences (from \"obvious\" markers)\n    if 'Myth:' in sentence:\n        sentence = sentence.replace('Myth:','').strip()\n        print(sH)\n        print(sentence)\n        print()\n        CLAIMS.append(sentence)\n    else:\n        sentence = sentence.replace('Reality:','').strip()\n        NO_CLAIMS.append(sentence)\nlen(CLAIMS), len(NO_CLAIMS)","1134b62a":"X, Y = [], []\nfor c in list(set(CLAIMS)):\n    X.append(c)\n    Y.append('claim')\nfor s in list(set(NO_CLAIMS)):\n    X.append(s)\n    Y.append('no_claim')\nlen(X), len(Y)","e06d5389":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_fscore_support, classification_report","f106b903":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42, shuffle=True)\nlen(X_train), len(X_test), len(Y_train), len(Y_test)","b95922e1":"from collections import Counter\nprint(len(Y_train), len(Y_test))\nC_train = Counter(Y_train)\nprint(C_train)\nC_test = Counter(Y_test)\nprint(C_test)\nprint(\"%.2f\"%(100*C_train['claim'] \/ sum(list(C_train.values()))), \"%\", \"claims in train set!\")\nprint(\"%.2f\"%(100*C_test['claim'] \/ sum(list(C_test.values()))), \"%\", \"claims in test set!\")","016e6273":"input_length_chars = []\ninput_length_words = []\nfor x,l in zip(X_train, Y_train):\n    l = len(x)\n    input_length_chars.append(l)\n    doc = nlp(x)\n    for sent in doc.sentences:\n        S = ' '.join([w.text for w in sent.words])\n        L = len(S.split(' '))\n        input_length_words.append(L)\n    print(\"%-10s\"%l, \"%4i\"%l, \"%2i\"%L, x)\n(min(input_length_chars), max(input_length_chars)), (min(input_length_words), max(input_length_words))","8d817184":"LENGTH_IN_CHAR_MIN = 10\nLENGTH_IN_CHAR_MAX = 250","f3692553":"!wget https:\/\/github.com\/minimaxir\/char-embeddings\/raw\/master\/glove.840B.300d-char.txt","50180358":"with open('glove.840B.300d-char.txt','r') as f:\n    pretrained_glove_char_embeddings_txt = f.readlines()\nlen(pretrained_glove_char_embeddings_txt)","d442700a":"pretrained_glove_char_embeddings = dict()\nfor v in pretrained_glove_char_embeddings_txt:\n    v = v.strip().split(' ')\n    char = v[0]\n    vec = np.asarray([float(f) for f in v[1:]])\n    assert vec.shape[0]==300\n    print(char, vec.shape)\n    pretrained_glove_char_embeddings[char] = vec\nlen(pretrained_glove_char_embeddings)","bd5d35b6":"np.random.seed(42)\n\nchar = 'UNK'\nvec = np.random.rand(300,)\nprint(char, vec.shape)\n\npretrained_glove_char_embeddings[char] = vec","e6d34cfd":"len(pretrained_glove_char_embeddings)","1763b5dd":"class Embedder():\n    def __init__(self, min_char_len, max_char_len, emb_dim=300):\n        self.min_char_len = min_char_len #TODO: check for min length\n        self.max_char_len = max_char_len\n        self.emb_dim = emb_dim\n    #    self.pretrained_glove_char_embeddings = _load_char_embeddings()\n    \n    #def _load_char_embeddings():\n        \n        \n    def encode(self, sentences: List[str]) -> List[ndarray]:\n        sentences_emb = []\n        for sentence in sentences:\n            self.sentence_emb_seq = np.zeros((min(len(sentence),self.max_char_len),self.emb_dim))\n            for i, char in enumerate(sentence[:self.max_char_len]):\n                if char in pretrained_glove_char_embeddings.keys():\n                    self.sentence_emb_seq[i] = pretrained_glove_char_embeddings[char]\n                else:\n                    self.sentence_emb_seq[i] = pretrained_glove_char_embeddings['UNK']\n            self.sentence_emb = np.average(self.sentence_emb_seq, axis=0)\n            sentences_emb.append(self.sentence_emb)\n        return np.asarray(sentences_emb)","2a41767d":"from sklearn.ensemble import RandomForestClassifier","660790e5":"emb = Embedder(min_char_len=10, max_char_len=250)","f306ef77":"X_train_emb = emb.encode(X_train)\nlen(X_train_emb), X_train_emb[0].shape","0dc10f87":"clr_rf = RandomForestClassifier(max_depth=1, n_estimators=10)\nclr_rf.fit(X_train_emb, Y_train)","7b5f480a":"X_test_emb = emb.encode(X_test)\nlen(X_test_emb), X_test_emb[0].shape","e0f1e1e9":"Y_pred_rfc = clr_rf.predict(X_test_emb)\nlen(Y_pred_rfc)","9ebe6bff":"p,r,f1,s = precision_recall_fscore_support(y_true=Y_test, y_pred=Y_pred_rfc, average='macro', warn_for=tuple())\nprint(\"%.3f Precision\\n%.3f Recall\\n%.3f F1\"%(p,r,f1))","67545e8e":"print(classification_report(y_true=Y_test, y_pred=Y_pred_rfc))","9d070a01":"for i,l in enumerate(Y_pred_rfc):\n    if l=='claim':\n        print(X_test[i])","16cc993a":"!pip install -U skl2onnx","9c24e4a9":"from skl2onnx import convert_sklearn","2e8131f1":"from skl2onnx.common.data_types import FloatTensorType","3405d844":"initial_type = [('float_input', FloatTensorType([None, 4]))]","c3e62b88":"onx = convert_sklearn(clr_rf, initial_types=initial_type)","f6f64b38":"with open(\"rfc_claims.onnx\", \"wb\") as f:\n    f.write(onx.SerializeToString())","39815bd4":"!pip install -U onnxruntime","2b638592":"import onnxruntime as rt","89cbea34":"sess = rt.InferenceSession(\"rfc_claims.onnx\")","17a41b92":"input_name = sess.get_inputs()[0].name\nlabel_name = sess.get_outputs()[0].name","04a55958":"pred_onx = sess.run(\n    [label_name], \n    {\n        input_name: X_test_emb.astype(np.float32)\n    }\n)[0]","4f3a4acd":"#### Compute the prediction with ONNX Runtime","28e38976":"#### Convert into ONNX format","3d02a65d":"#### Train a random forest classifier","b5163a69":"# Project [COVIEWED]()","c3014493":"---"}}