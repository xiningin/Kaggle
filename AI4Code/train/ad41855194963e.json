{"cell_type":{"1f0d7cb0":"code","202a57f0":"code","f442da65":"code","b708e498":"code","34430019":"code","7b29f9e8":"code","6f35324e":"code","dc3695c5":"code","ef72931f":"code","b711c205":"code","3b633303":"code","a6f5e32a":"code","f226f0dd":"code","5eed0be3":"code","5b65576f":"code","eb812c81":"code","506c901b":"code","18912ff4":"code","ffcd37a8":"code","24bf51bb":"code","eb5616c6":"code","69b6ab02":"code","e91b46ec":"code","bf2b89c8":"code","aae22b65":"code","d216481d":"code","2d6d7a5a":"code","c98303ef":"code","ec04b590":"code","9b4e656c":"code","fde73760":"code","25ade641":"code","2640d874":"code","2be99322":"code","cdb1662e":"code","d9ff9cce":"code","09f12a24":"code","2d8de1da":"code","ee512c46":"code","6e631da6":"code","7358eaf6":"code","ddcd191d":"code","4329ebae":"code","ea042fec":"code","68ae1ce4":"code","2c631b52":"code","a72d318c":"code","56de30ce":"code","942f5d38":"code","13ab89d6":"code","78d0fbf1":"code","c5d80a86":"code","586d9c96":"code","5c444f6d":"code","ed1fb6e8":"markdown","cee283df":"markdown","a02737db":"markdown","4f14f3e9":"markdown","aca3acdc":"markdown","9cafec0b":"markdown","d04da09c":"markdown","bc3738cd":"markdown","2186befe":"markdown","719bdf20":"markdown","3510f3c7":"markdown","90990c20":"markdown","f0ef25b8":"markdown","d0a26d04":"markdown","f595f524":"markdown","a6aa33f1":"markdown","f3e1fee4":"markdown","f7ef912b":"markdown","4aa82e67":"markdown","4ad7ac51":"markdown","a1d12aa2":"markdown","f89cce50":"markdown","b3ae7545":"markdown","5071fc00":"markdown","e95119ed":"markdown","a0b1b1fc":"markdown","b58954ed":"markdown","c495d35b":"markdown","14739b67":"markdown","748208f9":"markdown","6b269f70":"markdown","780bb3b6":"markdown","9d7d39b7":"markdown","3d7374f4":"markdown","11b4ac76":"markdown","f51f64b6":"markdown","236275cb":"markdown","8028d98e":"markdown","14002fca":"markdown","78584703":"markdown","cd8deab5":"markdown","d74eb9bd":"markdown","19ff6f8e":"markdown","f8be2ba9":"markdown","6669adab":"markdown","ca1ef98d":"markdown","6b03fbc8":"markdown","2a37e7f2":"markdown","5d15754f":"markdown","75a78044":"markdown","1f03dce3":"markdown","a5ce6dcd":"markdown"},"source":{"1f0d7cb0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","202a57f0":"### import necessary packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt #Visualization packages\nimport missingno as msno\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore') ## ignore all warnings","f442da65":"## Lead the train and test dataset\ndf_train = pd.read_csv(\"\/kaggle\/input\/restaurant-revenue-prediction\/train.csv.zip\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/restaurant-revenue-prediction\/test.csv.zip\")","b708e498":"## Take a quick peak on the train data\ndf_train.head()","34430019":"## Shape of train data\ndf_train.shape","7b29f9e8":"## Take a quick peak on the test data\ndf_test.head()","6f35324e":"## Shape of test data\ndf_test.shape","dc3695c5":"## All features\ndf_train.info()","ef72931f":"## drop the Id columns\ndf_train = df_train.drop(\"Id\",axis=1)","b711c205":"quantitative_feats = [i for i in df_train if df_train[i].dtype != np.object]\nqualitative_feats = [i for i in df_train if df_train[i].dtype == np.object]\n\nprint(\"Quantitative_features: {}\".format(quantitative_feats))\nprint(\"Qualtative_features: {}\".format(qualitative_feats))","3b633303":"## remove the dependant variables out \nquantitative_feats.remove(\"revenue\")\n\n## remove \"Open Date\"(Temporal variable) out \nqualitative_feats.remove(\"Open Date\")","a6f5e32a":"## dependant variable \nrevenue = df_train[\"revenue\"]","f226f0dd":"## months extraction\nmonth = pd.DataFrame()\nmonth[\"months\"] = df_train[\"Open Date\"].transform(lambda x: int(x.split(\"\/\")[0]))\nmonth = pd.concat([month,revenue],axis=1)\n\n## revenue based on months\nrevenue_month = month.groupby(\"months\")[\"revenue\"].median()\n\n\n### Visualization\nfig, ax = plt.subplots(2,1,sharey=False,figsize=(16,9),constrained_layout=True)\n\nbar_1 = sns.countplot(month[\"months\"],ax=ax[0],color=\"green\") ## Number of Revenue based on month\nfor i in bar_1.patches: #label each rectangle\n    height = i.get_height()\n    ax[0].text(i.get_x()+i.get_width()\/2,height*1.02,height,ha=\"center\",fontsize=9)\nax[0].set_title(\"Revenue Count\",fontsize=14)\n\nbar_2 = sns.barplot(revenue_month.index,revenue_month.values,ax=ax[1],color=\"red\") ## Median Revenue\nfor i in bar_2.patches: #label each rectangle\n    height = i.get_height()\n    ax[1].text(i.get_x()+i.get_width()\/2,height*1.02,height,ha=\"center\",fontsize=10)\nax[1].set_title(\"Median Revenue\",fontsize=14)","5eed0be3":"## years extraction\nyear = pd.DataFrame()\nyear[\"years\"] = df_train[\"Open Date\"].transform(lambda x: int(x.split(\"\/\")[-1]))\nyear = pd.concat([year,revenue],axis=1)\n\n## revenue based on months\nrevenue_year = year.groupby(\"years\")[\"revenue\"].median()\n\n### Visualization\nfig, ax = plt.subplots(2,1,sharey=False,figsize=(16,9),constrained_layout=True)\n\nbar_1 = sns.countplot(year[\"years\"],ax=ax[0],color=\"green\") ## Number of Revenue based on month\nfor i in bar_1.patches: #label each rectangle\n    height = i.get_height()\n    ax[0].text(i.get_x()+i.get_width()\/2,height*1.02,height,ha=\"center\",fontsize=10)\nax[0].set_title(\"Revenue Count\",fontsize=14)\nax[0].set_ylabel(\"Number of Opening\")\n\nbar_2 = sns.barplot(revenue_year.index,revenue_year.values,ax=ax[1],color=\"red\") ## Median Revenue\nfor i in bar_2.patches: #label each rectangle\n    height = i.get_height()\n    ax[1].text(i.get_x()+i.get_width()\/2,height*1.02,height,ha=\"center\",fontsize=11)\nax[1].set_title(\"Median Revenue\",fontsize=14)\nax[1].set_ylabel(\"Revenue\")","5b65576f":"## Discrete data\ndiscrete_feats = [feat for feat in quantitative_feats if df_train[feat].nunique() <= 25]\n\ndiscrete_feats","eb812c81":"## Visualization\nfor feat in discrete_feats:\n    plt.figure(constrained_layout=True)\n    sns.scatterplot(df_train[feat],df_train[\"revenue\"])","506c901b":"## Continous Data\ncontinuous_feats = [feat for feat in quantitative_feats if df_train[feat].nunique() > 25]\n\ncontinuous_feats","18912ff4":"for feat in qualitative_feats:\n    groupby_feat = df_train.groupby(feat)[\"revenue\"].median()\n\n    ## Visualization\n    fig, ax = plt.subplots(2,1,figsize=(20,9),constrained_layout=True)\n    bar_1 = sns.countplot(df_train[feat],ax=ax[0])\n    for bar in bar_1.patches:\n        height = bar.get_height()\n        ax[0].text(bar.get_x()+bar.get_width()\/2,height*1.02,height,ha=\"center\")\n    ax[0].set_title(\"Revenue Count ({})\".format(feat))\n    ax[0].set_ylabel(\"Count\")\n    ax[0].set_xticklabels(ax[0].get_xticklabels(),rotation=90)\n\n    bar_2 = sns.barplot(groupby_feat.index,groupby_feat.values,ax=ax[1])\n    for bar in bar_2.patches:\n        height = bar.get_height()\n        ax[1].text(bar.get_x()+bar.get_width()\/2,height*1.02,height,ha=\"center\")\n    ax[1].set_title(\"Revenue based on {}\".format(feat))\n    ax[1].set_ylabel(\"Revenue\")\n    ax[1].set_xticklabels(ax[1].get_xticklabels(),rotation=90)\n    \n","ffcd37a8":"## years extraction\nyear = pd.concat([year,df_train[\"City\"]],axis=1)\n\n## Select cities that has equal or more than 5 count of revenue\ncities = [\"\u0130stanbul\",\"Ankara\",\"Bursa\",\"\u0130zmir\",\"Samsun\"]\n\n\n## Visualization\nfig, ax = plt.subplots(5,1,figsize=(25,35),constrained_layout=True)\nfor num,city in enumerate(cities):\n    df_city = year.loc[year[\"City\"] == city, :]\n    df_city = df_city.sort_values(\"years\")\n    \n    ## Line plot\n    sns.lineplot(df_city[\"years\"],df_city[\"revenue\"],ax=ax[num],linestyle=\"-\")\n    ax[num].set_title(city)\n    for label in ax[num].get_xticklabels():\n        label.set_rotation(90)","24bf51bb":"## missing value\nprint(df_train.isnull().any())","eb5616c6":"## Create a lower triangle heatmap\nmask = np.zeros_like(df_train.corr(),dtype=bool)\nmask[np.triu_indices_from(mask)] = True\n\n## Heatmap visualization\nplt.figure(figsize=(30,20))\nsns.heatmap(df_train.corr(),\n            annot=True,\n            fmt=\".3f\",\n            annot_kws = {\"size\":10},\n            cmap=sns.cubehelix_palette(),\n            mask=mask)","69b6ab02":"# Import library for VIF\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\ndef calc_vif(X):\n\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    \n\n    return(vif)\n\n## Calcualte the VIF of the quantitaive features\nvif = calc_vif(df_train[quantitative_feats])\n\n## VIF exceeding 5 or 10 indicates high multicollinearity between this independent variable and the others\nhigh_vif = vif[vif[\"VIF\"] > 10].sort_values(\"VIF\",ascending=False) # Threshold of 10 is used in this case\n\nhigh_vif","e91b46ec":"## Percentage of high multicollinearity features in the dataset\nprint(len(high_vif)\/df_train.shape[1])","bf2b89c8":"## dependant vairable \ny = df_train[\"revenue\"]\ndf_train = df_train.drop(\"revenue\",axis=1)\n\n## combining\ndf_all = pd.concat([df_train,df_test],axis=0)\n\n## drop the \"Id\" column\ndf_all = df_all.drop(\"Id\",axis=1)\n\n## check on the shape\ndf_all.shape","aae22b65":"## Missing value\ndef missing_value(df):\n    \"\"\" Takes in a dataframe and\n    returns the number and percentage of the \n    missing value\"\"\"\n    \n    ## number of missing values\n    number = df.isnull().sum().sort_values(ascending=False)\n    number = number[number > 0]\n    \n    ## percentage of missing value\n    percentage = df.isnull().sum()*100 \/df.shape[0]\n    percentage = percentage[percentage > 0].sort_values(ascending=False)\n    \n    return pd.concat([number,percentage],axis=1,keys=[\"Total\",\"Percentage\"])\n\nmissing_value(df_all)","d216481d":"## import packages\nimport matplotlib.gridspec as gridspec\nimport scipy.stats as stats\n\n## Visualization\nfig = plt.figure(figsize=(12,8),constrained_layout=True)\ngrid = gridspec.GridSpec(ncols=3,nrows=4,figure=fig)\n# Histrogram\nax1 = fig.add_subplot(grid[0,:])\nsns.distplot(y,ax=ax1)\nax1.set_title(\"Histrogram of revenue\",fontsize=10)\n\n# Probability plot\nax2 = fig.add_subplot(grid[2:,:2])\nstats.probplot(y,plot=ax2)\nax2.set_title(\"QQplot of revenue\")\n\n# Boxplot\nax3 = fig.add_subplot(grid[2:,2])\nsns.boxplot(y,ax=ax3,orient=\"v\")\nax3.set_title(\"Boxplot of revenue\")\n\nplt.show()\n","2d6d7a5a":"## Check on the kurtosis and skewness of revenue\nprint(\"Kurtosis: {}\".format(y.kurt()))\nprint(\"Skewness: {}\".format(y.skew()))","c98303ef":"## Normalization\ny = np.log1p(y)\n\n## Visualization\nfig, ax = plt.subplots(1,2,constrained_layout=True,figsize=(12,8))\n\n## Histrogram\nsns.distplot(y,ax=ax[0])\nax[0].set_title(\"Histrogram of Normalized revenue\",fontsize=10)\n\n## QQplot\nstats.probplot(y,plot=ax[1])\nax[1].set_title(\"Proability Plot of Normalized revenue\",fontsize=10)\n\nplt.show()","ec04b590":"## Check on the normalized revenue kurtosis and skewness\nprint(\"Kurtosis: {}\".format(y.kurt()))\nprint(\"Skewness: {}\".format(y.skew()))","9b4e656c":"## OpenMonth\ndf_all[\"OpenMonth\"] = df_all[\"Open Date\"].transform(lambda x: int(x.split(\"\/\")[0]))\n\n## OpenYear\ndf_all[\"OpenYear\"] = df_all[\"Open Date\"].transform(lambda x: int(x.split(\"\/\")[-1]))\n\n## Open Day\ndf_all[\"Open Day\"] = df_all[\"Open Date\"].transform(lambda x: int(x.split(\"\/\")[1]))\n\n## Remove Open Date\ndf_all = df_all.drop([\"Open Date\"],axis=1)","fde73760":"## Bias feature reducer\nbias_feat = []\nfor feat in df_all.columns:\n    counts = df_all[feat].value_counts().iloc[0] # mode value count\n    if counts*100 \/ len(df_all) >99.94:\n        bias_feat.append(feat)\n\nbias_feat","25ade641":"## Dummy variable\ndf_all = pd.get_dummies(df_all).reset_index(drop=True)","2640d874":"## Split the dataset back into train and test dataset\nn = len(y)\n\n## train dataset\ndf_train = df_all[:n]\n\n## test dataset\ndf_test = df_all[n:]\n\n## Check on thier shapes\nprint(\"Shape of train dataset: {}\".format(df_train.shape))\nprint(\"Shape of test dataset: {}\".format(df_test.shape))","2be99322":"## import necessary package\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n## Split the data into train and test set\nX_train, X_test, y_train, y_test =  train_test_split(df_train,y,test_size=0.33,random_state=42)\n\n\n## Check on the dataset shape\nprint(\"Shapes: \", X_train.shape, X_test.shape, y_train.shape, y_test.shape)","cdb1662e":"## create an empty list to contain all best models for later use\nbest_estimators = []","d9ff9cce":"## import necessary packages\nfrom sklearn.model_selection import GridSearchCV\n\n## models packages\nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet, HuberRegressor, BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor, BaggingRegressor, VotingRegressor, AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nimport lightgbm as lgbm","09f12a24":"## Parameters\nparams = {\n    \"alpha\" : [.01, .1, .5, .7, .9, .95, .99, 1, 5, 10, 20],\n    \"fit_intercept\" : [True, False],\n    \"normalize\" : [True,False],\n    \"solver\" : ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n    \"tol\" : [0.0001, 0.001, 0.01, 0.1],\n    \"random_state\" : [42]\n}\n\n## Ridge\nridge = Ridge()\nridge_grid = GridSearchCV(ridge, params, scoring='r2', cv=7, n_jobs=-1)\nridge_grid.fit(X_train, y_train)\n\n## Output\nprint(\"Best parameters:  {}:\".format(ridge_grid.best_params_))\nprint(\"Best score: {}\".format(ridge_grid.best_score_))\n\n## Append to list\nbest_estimators.append([\"Ridge\",ridge_grid.best_estimator_])","2d8de1da":"## Parameters\nparams = {\n    'alpha' : [.01, .1, .5, .7, .9, .95, .99, 1, 5, 10, 20],\n    'fit_intercept' : [True, False],\n    'normalize' : [True,False],\n    'tol' : [0.0001, 0.001, 0.01, 0.1],\n    \"random_state\" : [42]\n}\n\n## Lasso\nlasso = Lasso()\nlasso_grid = GridSearchCV(lasso, params, scoring='r2', cv=7, n_jobs=-1)\nlasso_grid.fit(X_train, y_train)\n\n## Output\nprint(\"Best parameters:  {}:\".format(lasso_grid.best_params_))\nprint(\"Best score: {}\".format(lasso_grid.best_score_))\n\n## Append to list\nbest_estimators.append([\"Lasso\",lasso_grid.best_estimator_])","ee512c46":"## Parameters\nparams = {\n    \"alpha\" : [.01, .1, .5, .7, .9, .95, .99, 1, 5, 10, 20],\n    \"fit_intercept\" : [True, False],\n    \"normalize\" : [True,False],\n    \"tol\" : [0.0001, 0.001, 0.01, 0.1],\n    \"random_state\" : [42]\n}\n\n## Elastic Net\nEL = ElasticNet()\nEL_grid = GridSearchCV(EL, params, scoring='r2', cv=7, n_jobs=-1)\nEL_grid.fit(X_train, y_train)\n\n## Output\nprint(\"Best parameters:  {}:\".format(EL_grid.best_params_))\nprint(\"Best score: {}\".format(EL_grid.best_score_))\n\n## Append to list\nbest_estimators.append([\"ElasticNet\",EL_grid.best_estimator_])","6e631da6":"## Parameters\nparams = {\n    \"alpha\" : [.01, .1, .5, .7, .9, .95, .99, 1, 5, 10, 20],\n    \"fit_intercept\" : [True, False],\n    \"tol\" : [0.0001, 0.001, 0.01, 0.1],\n    \"max_iter\": [100, 300 , 500]\n}\n\n## HuberRegressor\nHuber_R = HuberRegressor()\nHuber_R_grid = GridSearchCV(Huber_R, params, scoring='r2', cv=7, n_jobs=-1)\nHuber_R_grid.fit(X_train, y_train)\n\n## Output\nprint(\"Best parameters:  {}:\".format(Huber_R_grid.best_params_))\nprint(\"Best score: {}\".format(Huber_R_grid.best_score_))\n\n## Append to list\nbest_estimators.append([\"HuberRegressor\",Huber_R_grid.best_estimator_])","7358eaf6":"## Parameters\nparams = {\n    \"max_depth\": [\"None\",10, 30, 50, 75, 100],\n    \"max_features\": [\"auto\",0.3, 0.6],\n    \"min_samples_leaf\": [1,3,5,7],\n    \"min_samples_split\": [2, 4, 8, 12],\n    \"n_estimators\": [30, 50, 100, 200],\n    \"random_state\" : [42]\n}\n\n## RandomForestRegressor\nRFR = RandomForestRegressor()\nRFR_grid = GridSearchCV(RFR, params, scoring='r2', cv=7, n_jobs=-1)\nRFR_grid.fit(X_train, y_train)\n\n## Output\nprint(\"Best parameters:  {}:\".format(RFR_grid.best_params_))\nprint(\"Best score: {}\".format(RFR_grid.best_score_))\n\n## Append to list\nbest_estimators.append([\"RandomForestR\",RFR_grid.best_estimator_])","ddcd191d":"## Parameters\nparams = {\n    \"max_depth\": [\"None\",10, 30, 50, 75, 100],\n    \"max_features\": [\"auto\",.3, .4, .5, .6],\n    \"min_samples_leaf\": [1,3,5,7],\n    \"min_samples_split\": [2, 4, 8, 12],\n    \"n_estimators\": [30, 50, 100, 200],\n    \"random_state\" : [42]\n}\n\n## ExtraTreesRegressor\nETR = ExtraTreesRegressor()\nETR_grid = GridSearchCV(ETR, params, scoring='r2', cv=7, n_jobs=-1)\nETR_grid.fit(X_train, y_train)\n\n## Output\nprint(\"Best parameters:  {}:\".format(ETR_grid.best_params_))\nprint(\"Best score: {}\".format(ETR_grid.best_score_))\n\n## Append to list\nbest_estimators.append([\"ExtraTreesR\",ETR_grid.best_estimator_])","4329ebae":"## Parameters\nparams = {\n    \"max_features\": [0.2, 0.4, 0.6,1.0],\n    \"n_estimators\": [5, 10, 15, 20],\n    \"random_state\": [42]\n}\n\n## BaggingRegressor\nBR =  BaggingRegressor()\nBR_grid = GridSearchCV(BR, params, scoring='r2', cv=7, n_jobs=-1)\nBR_grid.fit(X_train, y_train)\n\n## Output\nprint(\"Best parameters:  {}:\".format(BR_grid.best_params_))\nprint(\"Best score: {}\".format(BR_grid.best_score_))\n\n## Append to list\nbest_estimators.append([\"BaggingRegressorR\",BR_grid.best_estimator_])","ea042fec":"## parameters\nparams = {\n    \"learning_rate\": [.1, .5, .7, .9, .95, .99, 1],\n    \"colsample_bytree\": [.3, .4, .5, .6],\n    \"max_depth\": [2, 4],\n    \"alpha\": [1, 3, 5],\n    \"subsample\": [.5],\n    \"n_estimators\": [30, 70, 100, 200],\n    \"random_state\" : [42]\n}\n\n## XGBoost Regressor\nXGBR =  XGBRegressor()\nXGBR_grid = GridSearchCV(XGBR, params, scoring='r2', cv=7, n_jobs=-1)\nXGBR_grid.fit(X_train, y_train)\n\n## Output\nprint(\"Best parameters:  {}:\".format(XGBR_grid.best_params_))\nprint(\"Best score: {}\".format(XGBR_grid.best_score_))\n\n## Append to list\nbest_estimators.append([\"XGBoostR\",XGBR_grid.best_estimator_])","68ae1ce4":"## parameters\nparams = {\n    \"max_depth\": [\"None\",10, 40, 80],\n    \"max_features\": [\"auto\",\"sqrt\",\"log2\"],\n    \"min_samples_leaf\": [1,3,5,7],\n    \"min_samples_split\": [2, 6, 12],\n    \"random_state\" : [42],\n    \"splitter\" : [\"best\",\"random\"]\n}\n\n## XGBoost Regressor\nDTR =  DecisionTreeRegressor()\nDTR_grid = GridSearchCV(DTR, params, scoring='r2', cv=7, n_jobs=-1)\nDTR_grid.fit(X_train, y_train)\n\n## Output\nprint(\"Best parameters:  {}:\".format(DTR_grid.best_params_))\nprint(\"Best score: {}\".format(DTR_grid.best_score_))\n\n## Append to list\nbest_estimators.append([\"DecisionTreeR\",DTR_grid.best_estimator_])","2c631b52":"## parameters\nparams = {\n    \"n_estimators\": [10, 30, 50, 100],\n    \"learning_rate\": [.01, 0.1, 0.5, 0.9, 0.95, 1],\n    \"random_state\" : [42]\n}\n\n## XGBoost Regressor\nAdaBoostR =   AdaBoostRegressor()\nAdaBoostR_grid = GridSearchCV(AdaBoostR, params, scoring='r2', cv=7, n_jobs=-1)\nAdaBoostR_grid.fit(X_train, y_train)\n\n## Output\nprint(\"Best parameters:  {}:\".format(AdaBoostR_grid.best_params_))\nprint(\"Best score: {}\".format(AdaBoostR_grid.best_score_))\n\n## Append to list\nbest_estimators.append([\"AdaBoostR\",AdaBoostR_grid.best_estimator_])","a72d318c":"## Parameters\nparams = {\n    \"max_depth\": [2, 3, 6, 10],\n    \"max_features\": [\"auto\",0.3, 0.6],\n    \"min_samples_leaf\": [1,3],\n    \"min_samples_split\": [2, 5],\n    \"n_estimators\": [30, 50, 100, 200],\n    \"random_state\" : [42],\n    \"tol\" : [0.0001, 0.001, 0.01, 0.1]\n}\n\n## GradientBoostingRegressor\nGBR = GradientBoostingRegressor()\nGBR_grid = GridSearchCV(GBR, params, scoring='r2', cv=7, n_jobs=-1)\nGBR_grid.fit(X_train, y_train)\n\n## Output\nprint(\"Best parameters:  {}:\".format(GBR_grid.best_params_))\nprint(\"Best score: {}\".format(GBR_grid.best_score_))\n\n## Append to list\nbest_estimators.append([\"GradientBoostR\",GBR_grid.best_estimator_])","56de30ce":"##import necessary packages\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n## pipeline\npipelines = []\n\nfor name,model in best_estimators:\n    pipeline = Pipeline([(\"Scaler\",StandardScaler()),\n                         (name,model)\n                        ])\n    pipelines.append([\"Scaled_\"+name,pipeline])","942f5d38":"## import packages\nfrom sklearn.model_selection import KFold, cross_val_score\n\n## Create a dataframe to store all the models' cross validation score\nevaluate = pd.DataFrame(columns=[\"model\",\"cv\",\"std\"])\n\n\n## Encoded dataset\nfor name,model in pipelines:\n    kfold = KFold(n_splits=10,random_state=42)\n    cv = cross_val_score(model, X_train, y_train, cv=kfold, n_jobs=-1, scoring=\"neg_root_mean_squared_error\")\n    \n    row = evaluate.shape[0]\n    evaluate.loc[row,\"model\"] = name\n    evaluate.loc[row,\"cv\"] = round(cv.mean(),3)\n    evaluate.loc[row,\"std\"] = \"+\/- {}\".format(round(cv.std(),4))\n    \n    evaluate = evaluate.sort_values(\"cv\",ascending=False)\n\nevaluate","13ab89d6":"## Visualization\nfig, ax = plt.subplots(1,1,sharey=False,figsize=(16,9))\n\n\nbar = sns.barplot(evaluate[\"model\"], evaluate[\"cv\"],ax=ax,palette = sns.cubehelix_palette(evaluate.shape[0]))\nfor rec in bar.patches:\n    height = rec.get_height()\n    ax.text(rec.get_x() + rec.get_width()\/2, height-0.02,height,ha=\"center\")\nax.set_title(\"Cross Validate Score\")\nax.set_xticklabels(ax.get_xticklabels(),rotation = 45)","78d0fbf1":"## Creating a list for all combinations models\nvotings = []\n\n## RandomForestRegressor only, Current best model\nvotings.append([\"Scaled_RFR\",RFR_grid.best_estimator_])\n\n\n##  All models\nvotings.append((\"Scaled_all_models\",Pipeline([(\"Scaler\",StandardScaler()),\n                                      (\"Votings\",VotingRegressor([(\"RFR\",RFR_grid.best_estimator_),\n                                                                  (\"AdaBoostR\", AdaBoostR_grid.best_estimator_),\n                                                                  (\"BaggingR\",BR_grid.best_estimator_),\n                                                                  (\"GBR\",GBR_grid.best_estimator_)\n                                                                 ])\n                                    \n                                    )])))\n\n\n### Combinations of three estimators\n\n## Combination of RandomForestRegressor with  BaggingRegressor & GradientBoostRegressor\nvotings.append((\"Scaled_RFR_BR_GBR\",Pipeline([(\"Scaler\",StandardScaler()),\n                                      (\"Votings\",VotingRegressor([(\"RFR\",RFR_grid.best_estimator_),\n                                                                  (\"BaggingR\",BR_grid.best_estimator_),\n                                                                  (\"GBR\",GBR_grid.best_estimator_)\n                                                                 ])\n                                    \n                                    )])))\n\n## Combination of RandomForestRegressor with BaggingRegressor & AdaBoostRegressor \nvotings.append((\"Scaled_RFR_BR_AdaBR\",Pipeline([(\"Scaler\",StandardScaler()),\n                                      (\"Votings\",VotingRegressor([(\"RFR\",RFR_grid.best_estimator_),\n                                                                  (\"BaggingR\",BR_grid.best_estimator_),\n                                                                  (\"AdaBoostR\", AdaBoostR_grid.best_estimator_)\n                                                                 ])\n                                    \n                                    )])))\n\n## Combination of RandomForestRegressor with AdaBoostRegressor  & GradientBoostRegressor \nvotings.append((\"Scaled_AdaBR_GBR\",Pipeline([(\"Scaler\",StandardScaler()),\n                                      (\"Votings\",VotingRegressor([(\"RFR\",RFR_grid.best_estimator_),\n                                                                  (\"AdaBoostR\", AdaBoostR_grid.best_estimator_),\n                                                                  (\"GBR\",GBR_grid.best_estimator_)\n                                                                 ])\n                                    \n                                    )])))\n\n## Combination of BaggingRegressor with GradientBoostRegressor & AdaBoostRegressor\nvotings.append((\"Scaled_BR_GBR_AdaBR\",Pipeline([(\"Scaler\",StandardScaler()),\n                                      (\"Votings\",VotingRegressor([(\"BaggingR\",BR_grid.best_estimator_),\n                                                                  (\"GBR\",GBR_grid.best_estimator_),\n                                                                  (\"AdaBoostR\", AdaBoostR_grid.best_estimator_)\n                                                                 ])\n                                    \n                                    )])))\n\n### Combination of 2 estimators\n\n## Combination of BaggingRegressor with GradientBoostRegressor\nvotings.append((\"Scaled_BR_GBR\",Pipeline([(\"Scaler\",StandardScaler()),\n                                      (\"Votings\",VotingRegressor([(\"BaggingR\",BR_grid.best_estimator_),\n                                                                  (\"GBR\",GBR_grid.best_estimator_)\n                                                                 ])\n                                    \n                                    )])))\n\n## Combination of BaggingRegressor with AdaBoostRegressor\nvotings.append((\"Scaled_BR_AdaBR\",Pipeline([(\"Scaler\",StandardScaler()),\n                                      (\"Votings\",VotingRegressor([(\"BaggingR\",BR_grid.best_estimator_),\n                                                                  (\"AdaBoostR\", AdaBoostR_grid.best_estimator_)\n                                                                 ])\n                                    \n                                    )])))\n\n## Combination of BaggingRegressor with RandomForestRegressor\nvotings.append((\"Scaled_BR_RFR\",Pipeline([(\"Scaler\",StandardScaler()),\n                                      (\"Votings\",VotingRegressor([(\"BaggingR\",BR_grid.best_estimator_),\n                                                                  (\"RFR\",RFR_grid.best_estimator_)\n                                                                 ])\n                                    \n                                    )])))\n\n## Combination of GradientBoostRegressor with AdaBoostRegressor\nvotings.append((\"Scaled_GBR_AdaBR\",Pipeline([(\"Scaler\",StandardScaler()),\n                                      (\"Votings\",VotingRegressor([(\"GBR\",GBR_grid.best_estimator_),\n                                                                  (\"AdaBoostR\", AdaBoostR_grid.best_estimator_)\n                                                                 ])\n                                    \n                                    )])))\n\n## Combination of GradientBoostRegressor with RandomForestRegressor\nvotings.append((\"Scaled_GBR_RFR\",Pipeline([(\"Scaler\",StandardScaler()),\n                                      (\"Votings\",VotingRegressor([(\"GBR\",GBR_grid.best_estimator_),\n                                                                  (\"RFR\",RFR_grid.best_estimator_)\n                                                                 ])\n                                    \n                                    )])))\n\n## Combination of AdaBoostRegressor with RandomForestRegressor\nvotings.append((\"Scaled_AdaBR_RFR\",Pipeline([(\"Scaler\",StandardScaler()),\n                                      (\"Votings\",VotingRegressor([(\"AdaBoostR\", AdaBoostR_grid.best_estimator_),\n                                                                  (\"RFR\",RFR_grid.best_estimator_)\n                                                                 ])\n                                    \n                                    )])))","c5d80a86":"## Create dataframe for the cross validate score\nevaluate_vote = pd.DataFrame(columns=[\"model\",\"cv\",\"std\"])\n\n## Fitting all the combination models\nfor name, model in votings:\n    kfold = KFold(n_splits=10,random_state=42)\n    \n    cv = cross_val_score(model,X_train,y_train, cv=kfold, scoring=\"neg_root_mean_squared_error\",n_jobs=-1)\n    \n    row = evaluate_vote.shape[0]\n    \n    evaluate_vote.loc[row,\"model\"] = name\n    evaluate_vote.loc[row,\"cv\"] = round(cv.mean(),4)\n    evaluate_vote.loc[row,\"std\"] = \"+- {}\".format(round(cv.std(),5))\n    \nevaluate_vote = evaluate_vote.sort_values(\"cv\",ascending=False)\nevaluate_vote","586d9c96":"## Visualization\nfig, ax = plt.subplots(1,1,figsize=(16,9))\nbar = sns.barplot(evaluate_vote[\"model\"],evaluate_vote[\"cv\"],ax=ax,palette = sns.cubehelix_palette(evaluate.shape[0]))\n\nfor rec in bar.patches:\n    height = rec.get_height()\n    ax.text(rec.get_x() + rec.get_width() \/2, height *1.02, height, ha=\"center\")\nax.set_title(\"Cross Validate Score\",fontsize=14)\nax.set_xticklabels(evaluate_vote[\"model\"].to_list(),rotation=45)","5c444f6d":"## Best Model : Scaled_RFR\nbest_model = Pipeline([(\"Scaler\",StandardScaler()),\n                       (\"RFR\",RFR_grid.best_estimator_)\n                      ])\n## Fit the model \nbest_model = best_model.fit(df_train,y) # fit the model with all the train dataset\n\n## Submission\nsubmission = pd.read_csv(\"\/kaggle\/input\/restaurant-revenue-prediction\/sampleSubmission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(best_model.predict(df_test)))\nsubmission.to_csv('submission', index=False)\nsubmission","ed1fb6e8":"## Create Dummy Variables","cee283df":"### Data Description (Referred from [here](https:\/\/www.kaggle.com\/c\/restaurant-revenue-prediction\/data))\n* Id : Restaurant id. \n* Open Date : opening date for a restaurant\n* City : City that the restaurant is in. Note that there are unicode in the names. \n* City Group: Type of the city. Big cities, or Other. \n* Type: Type of the restaurant. FC: Food Court, IL: Inline, DT: Drive Thru, MB: Mobile\n* P1, P2 - P37: There are three categories of these obfuscated data. Demographic data are gathered from third party providers with GIS systems. These include population in any given area, age and gender distribution, development scales. Real estate data mainly relate to the m2 of the location, front facade of the location, car park availability. Commercial data mainly include the existence of points of interest including schools, banks, other QSR operators.\n* Revenue: The revenue column indicates a (transformed) revenue of the restaurant in a given year and is the target of predictive analysis. Please note that the values are transformed so they don't mean real dollar values. ","a02737db":"****Great!**** Our revenue feature has been normalized. ","4f14f3e9":"### RandomForestRegressor","aca3acdc":"## Data Processing\nCombine both train and test together for data processing","9cafec0b":"### Imputing Missing Values\nCheck on the combined dataset for missing value as we only check for the train dataset only.","d04da09c":"### Quantitative Data & Qualitative Data\n1. Quantitative data is the type of data ****whose value is measured in the form of numbers or counts, with a unique numerical value associated with each data set****\n1. Qualitative data is defined as the data that ****approximates and characterizes****","bc3738cd":"# Outline of this notebook:\n\n## Exploratory Data Analysis (EDA): \nExploration of the dataset through visualization and analyzation to summarize thier main characteristic and gives us a insight on the dataset.\n\n* [Quick Peak](#Quick-Peak)\n* [Quantitative Data & Qualitative Data](#Quantitative-Data-&-Qualitative-Data)\n* [Missing Values](#Missing-Value)\n* [Multicollinearity](#Multicollinearity)\n\n## [Data Processing](#Data-Processing): \nManipulate explored and analyzed data to convert them into meaningful information that can be use the models or estimators.\n\n* [Imputing Missing Values](#Imputing-Missing-Values)\n* [Normalization of Dependant Variable (revenue)](#Normalization-of-Dependant-Variable-(revenue))\n* [Adding New Features](#Adding-New-Features)\n* [Eliminating Biased Features](#Eliminating-Biased-Features)\n* [Create Dummy Variables](#Create-Dummy-Variables)\n\n## [Models Building](#Model-Building):\n****Find the optimised parameters for all the models. Evaluate high performance mode and use them for ensemble regression****\n\n* [Train Test Split](#Train-Test-Split)\n* [GridSearchCV Best Parameters for All Models](#GridSearchCV-Best-Parameters-for-All-Models)\n* [Evaluate All Optimized Estimators](#Evaluate-All-Optimized-Estimators)\n* [Ensemble Models with VotingRegressor](#Ensemble-Models-with-VotingRegressor)\n* [Submission](#Submission)\n\nThese models are:\n\n1. Ridge\n1. Lasso\n1. Elastic Net\n1. HuberRegressor\n1. RandomForestRegressor\n1. ExtraTreeRegressor\n1. BaggingRegressor\n1. XGBRegressor\n1. DecisionTreeRegressor\n1. AdaBoostRegressor\n1. GradientBoostRegressor","2186befe":"There's no obvious correlation between the discrete features with the revenue.","719bdf20":"## GridSearchCV Best Parameters for All Models","3510f3c7":"From the median revenue chart, we can see that September month has the highest revenue while July month has the lowest revenue. ","90990c20":"As indicated in the three charts above, revenue is postively-skewed. revenue is drawn from a Leptokurtic (distributions with wider tails, greater profusion of outliers) distributions. \n\nCode below normalize the dependant variable.","f0ef25b8":"In the city variable visualization: \n* Elazig city has the highest median revenue while Kirklareli city has the lowest median revenue. \n* Istanbul and Ankara have 50 and 19 restaurant openings respectively, the rest of the cities have less than ten restaurant openings.\n\nIn the city group variable:\n* Big cities have more restaurant openings compared to other city group.\n* Big cities have higher revenue median compared to other city group.\n\nIn the type variable:\n* Food court is the most popular restaurant type follow by inline and drive thru being the least popular.\n* Inline restaurants have the highest median revenue compared to both drive thru and food court.","d0a26d04":"### BaggingRegressor","f595f524":"### Lasso","a6aa33f1":"## Submission","f3e1fee4":"## Evaluate All Optimized Estimators\n","f7ef912b":"### Ensemble Models with VotingRegressor\nA voting regressor is an ensemble meta-estimator that fits several base regressors, each on the whole dataset. Then it averages the individual predictions to form a final prediction\n\nNote: VotingRegressor works well with models that are not identical, for example: ExtraTreesRegressor with AdaBoostRegressor. ExtraTreesRegressor and RandomTreesRegressor are highly identifical so we would just pick the one with a higher cv score (in this case RandomForestRegressor) \n\nSo we gonna use just these 4 models :\n\n* RandomForestRegressor\n* AdaBoostRegressor\n* BaggingRegressor\n* GradientBoostRegressor","4aa82e67":"Multicollinearity is a problem because it undermines the statistical significance of an independent variable. As we can see from the heatmap, there are a lot of correlation between independant variables that have high similarity to each other. Code below check on the VIF(Variable Inflation Factors) of each feature.  ","4ad7ac51":"### Missing Value\nCheck on the presence of missing value for the dataset","a1d12aa2":"## Model Building\n### Train Test Split\nThe dataset are split into X_train,X_test, y_train, y_test.","f89cce50":"## Eliminating Biased Features","b3ae7545":"From the chart above, models with the top 5 best negative rmse score are selected. All these models would be use in the ensemble VotingRegressor to further increase the cv score.\n\nThese models are:\n\n* RandomForestRegressor\n* AdaBoostRegressor\n* ExtraTreesRegressor\n* BaggingRegressor\n* GradientBoostRegressor","5071fc00":"It seems like this dataset doesn't contain continuous features.","e95119ed":"There no presence of biased feature. ","a0b1b1fc":"Great News ! Looks like we don't have presence of missing value. ","b58954ed":"From the median revenue chart, we can see that year 2000 has the highest revenue while year 2013 has the lowest revenue. ","c495d35b":"The train data has a total of 137 rows and 43 columns. \"revenue\" feature would be this dataset dependant variables, the rest of the features are independant variables expect \"Id\" feature (We will remove this later)","14739b67":"## Normalization of Dependant Variable (revenue)","748208f9":"The three grahps above show us:\n\n* The revenue is drawn from a normal distribution\n* The revenue is right skewed\/ postively skewed, which indicates that most of the restaurants earn lesser.\n* Present some outliers in revenue","6b269f70":"Great ! Look's like there no missing values in the test dataset too.","780bb3b6":"### Multicollinearity\nHeatmap is an excellent way to check on the correclation between each independant variable.","9d7d39b7":"### Ridge ","3d7374f4":"### GradientBoostRegressor","11b4ac76":"### Temporal variable\nThe Open Date features present its data in a form of MM\/DD\/YY (for example: 01\/22\/2011), we extract the months and years out using the Code below.","f51f64b6":"### Categorical Data","236275cb":"## Adding New Features","8028d98e":"### HuberRegressor","14002fca":"Variable Inflation Factors: VIF score of an independent variable represents how well the variable is explained by other independent variables. \nFor more detail explanation on VIF, click [here](https:\/\/www.analyticsvidhya.com\/blog\/2020\/03\/what-is-multicollinearity\/)","78584703":"The test data has a total fo 100000 rows and 42 columns.","cd8deab5":"There a total of 38 quantitaive features and 4 qualitative features.","d74eb9bd":"85.71% of the features are highly multicollinearity ! \n\nFixing multicollinearity on these features are impossible due the insufficient data description of these, p1,p2 ... to p37 features.","19ff6f8e":"### Quick Peak","f8be2ba9":"### Elastic Net","6669adab":"1. ****Skewness****: Defined as the degree of distortion from the symmetrical bell curve or the normal curve.\n1. ****Kurtosis****: Defined as the measuer of the extreme values (also known as outliers) present in the distribution","ca1ef98d":"It seems like RandomForestRegressor with standardized dataset perform the best compared to all of the ensemble estimators. We will use RandomForestRegressor estimator to predict the test dataset for our submission.","6b03fbc8":"Two types of Quantitative data:\n\n1. Discrete data: Numerical data that has specific values. A great example would be the number of dogs. The number of dogs  are counted as 1 dog, 2 dogs, 3 dogs. There is no such thing as 0.5 dog.\n1. Continuous data: Numerical data that can take on any values. A great example would be the the height of a person. Donald is 6 foot or 182.88cm tall. ","2a37e7f2":"### XGBoost Regressor","5d15754f":"### AdaBoostRegressor","75a78044":"### It's been my pleasure to share my notebook with you.\n### If this notebook do helped you in any way, please hit that \"upvote\" button. Thank you !","1f03dce3":"### DecisionTreeRegressor","a5ce6dcd":"### ExtraTreesRegressor"}}