{"cell_type":{"4eb95a8a":"code","c2e6ff54":"code","4ef5c5f6":"code","435870c1":"code","b9b93c18":"code","f6b4be07":"code","c402ba57":"code","0d43da89":"code","5527c731":"code","0da8b3d8":"code","f81fb85a":"code","8ee4ebd9":"code","3c757182":"code","521e284f":"code","c2f7fc22":"code","90cb71b3":"code","108decc3":"code","3d5283ae":"code","5631cd2e":"code","d29ed4f4":"markdown","949a4166":"markdown","d686edad":"markdown","c15eab5e":"markdown"},"source":{"4eb95a8a":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nimport optuna\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nimport warnings\nimport gc\nwarnings.simplefilter('ignore')\n\nKAGGLE_DIR = r'..\/input\/tabular-playground-series-feb-2022\/'\nLOCAL_DIR = r''\nKAGGLE = True\nRS = 69420","c2e6ff54":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","4ef5c5f6":"%%time\nif KAGGLE:\n    print(f\"{'*'*10} Loading Training Data... {'*'*10}\")\n    df = pd.read_csv(KAGGLE_DIR+'train.csv', index_col=0).pipe(reduce_mem_usage)\n    print(f\"{'*'*10} Loading Testing Data... {'*'*10}\")\n    test = pd.read_csv(KAGGLE_DIR+'test.csv', index_col=0).pipe(reduce_mem_usage)\n    sub = pd.read_csv(KAGGLE_DIR+'sample_submission.csv').pipe(reduce_mem_usage)\nelse:\n    print(f\"{'*'*10} Loading Training Data... {'*'*10}\")\n    df = pd.read_csv(LOCAL_DIR+'train.csv', index_col=0).pipe(reduce_mem_usage)\n    print(f\"{'*'*10} Loading Testing Data... {'*'*10}\")\n    test = pd.read_csv(LOCAL_DIR+'test.csv', index_col=0).pipe(reduce_mem_usage)\n    sub = pd.read_csv(LOCAL_DIR+'sample_submission.csv').pipe(reduce_mem_usage)","435870c1":"lb = LabelEncoder()\nX = df.iloc[:, :-1].values\ny = lb.fit_transform(df['target'])","b9b93c18":"X.shape, y.shape","f6b4be07":"del df\ngc.collect()","c402ba57":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=RS)","0d43da89":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","5527c731":"clf = ExtraTreesClassifier(\n    n_estimators=100,\n    n_jobs=-1,\n    random_state=RS,\n    verbose=1\n)","0da8b3d8":"%%time\nclf.fit(\n    X_train, y_train,\n)","f81fb85a":"y_pred = clf.predict(X_test)","8ee4ebd9":"acc = accuracy_score(y_pred, y_test)\nprint(f\"Model Accuracy: {round(acc, 6)}\")","3c757182":"def objective(trial, X, y):\n\n    param = {\n        #'verbose': trial.suggest_categorical('verbosity', [1]),\n        'random_state': trial.suggest_categorical('random_state', [RS]),\n        'n_jobs': trial.suggest_categorical('n_jobs', [-1]),\n        'n_estimators':trial.suggest_int('n_estimators', 100, 5000, step=100),\n        'max_features':trial.suggest_int('max_features', 1, 286, step=2),\n        'min_samples_split':trial.suggest_int('min_samples_split', 2, 20)\n    }\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=RS)\n\n    clf = ExtraTreesClassifier(**param)\n    \n    clf.fit(X_train, y_train)\n    \n    y_pred = clf.predict(X_test)\n    \n    acc = accuracy_score(y_pred, y_test)\n    loss = round(acc, 6)\n    return loss","521e284f":"study = optuna.create_study(\n    direction='maximize',\n    sampler=optuna.samplers.TPESampler(),\n    pruner=optuna.pruners.HyperbandPruner(),\n    study_name='ExtraTrees-Hyperparameter-Tuning'\n)","c2f7fc22":"func = lambda trial: objective(trial, X, y)","90cb71b3":"default_params = {\n        'verbose':1,\n        'random_state':RS,\n        'n_jobs':-1,\n        'n_estimators':100,\n        'max_features':round(np.sqrt(X.shape[1])),\n        'min_samples_split':2\n}\n\nstudy.enqueue_trial(default_params)","108decc3":"%%time\nstudy.optimize(\n    func,\n#     n_trials=10,\n    timeout=3600*3,\n    gc_after_trial=True\n)","3d5283ae":"optuna.visualization.plot_optimization_history(study)","5631cd2e":"print(f'Best value (Accuracy): {study.best_value:.5f}')\nprint(f'Best params:')\n\nfor key, value in study.best_params.items():\n    print(f'\\t{key}: {value}')\n    \n    best_params = study.best_params\n    \nbest_params","d29ed4f4":"# Optuna Hyperparameter Tuning","949a4166":"# Simple Model","d686edad":"# Preprocessing","c15eab5e":"# Imports"}}