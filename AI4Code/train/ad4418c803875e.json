{"cell_type":{"45dd5b27":"code","82c235bf":"code","aeb6f142":"code","4cf1b0e8":"code","3ccf9c36":"code","e88435a4":"code","a401b30b":"code","88f67e75":"code","ad1b7990":"code","69bc90a0":"code","ac4ad0e0":"code","0315e0f1":"code","7460a932":"markdown","35675b51":"markdown","ed218417":"markdown"},"source":{"45dd5b27":"from IPython.display import clear_output\ntry:\n    import albumentations\nexcept ImportError:\n    !pip install albumentations\n\ntry:\n    import Cython\nexcept ImportError:\n    !pip install Cython\nclear_output()","82c235bf":"import json\nfrom torch.utils.data import Dataset, DataLoader\nfrom skimage import io\nimport pandas as pd\nimport torch\nimport os\nimport numpy as np\nfrom pathlib import Path\nimport cv2, zlib, base64\nfrom PIL import Image\nimport torch.nn as nn\nimport torchvision.transforms.functional as TF\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision","aeb6f142":"# dataset.py\n\nclass FootballBannerDataset(Dataset):\n    \"\"\"Football advertising banners images from UEFA Champions League matches.\"\"\"\n\n    def __init__(self, image_dir: str, mask_dir: str, transform=None):\n        \"\"\"\n        Args:\n            mask_dir (string): Directory with all the annotations.\n            image_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.transform = transform\n        self.images = os.listdir(image_dir)\n\n    def __len__(self):\n        return len(self.images)\n    \n    def extract_mask(self, labels):\n        mask = np.zeros((labels['size']['height'], labels['size']['width']), dtype=np.float32)\n        if len(labels[\"objects\"]) == 0:\n            return mask\n        bitmap = labels[\"objects\"][0][\"bitmap\"][\"data\"]\n        start_point = labels[\"objects\"][0][\"bitmap\"][\"origin\"]\n\n        mask_small = base64_2_mask(bitmap)\n        mask[\n            start_point[1] : start_point[1] + mask_small.shape[0],\n            start_point[0] : start_point[0] + mask_small.shape[1],\n        ] = mask_small\n\n        mask[mask == 255.0] = 1.0\n        return mask\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n        \n        #\u00a0Read Image\n        img_path = os.path.join(self.image_dir, self.images[idx])\n        mask_path = os.path.join(self.mask_dir, self.images[idx]+\".json\")\n        image = np.array(Image.open(img_path).convert(\"RGB\"))\n        \n        \n        with open(mask_path, \"r\", encoding=\"utf-8\") as annotReader:\n            labels = json.loads(annotReader.read())\n        mask = self.extract_mask(labels)\n\n        if self.transform is not None:\n            augmentations = self.transform(image=image, mask=mask)\n            image = augmentations[\"image\"]\n            mask = augmentations[\"mask\"]\n\n        return image, mask","4cf1b0e8":"# model.py\n\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(DoubleConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass UNET(nn.Module):\n    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n        super(UNET, self).__init__()\n        self.ups = nn.ModuleList()\n        self.downs = nn.ModuleList()\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        # Down part of UNET\n        for feature in features:\n            self.downs.append(DoubleConv(in_channels, feature))\n            in_channels = feature\n\n        # Up part of UNET\n        for feature in features[::-1]:\n            self.ups.append(\n                nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2)\n            )\n            self.ups.append(DoubleConv(feature * 2, feature))\n\n        # Bootleneck layer\n        self.bottleneck = DoubleConv(features[-1], features[-1] * 2)\n\n        # Final conv\n        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n\n    def forward(self, x):\n\n        # Down\n        skip_connections = []\n        for down in self.downs:\n            x = down(x)\n            skip_connections.append(x)\n            x = self.pool(x)\n\n        x = self.bottleneck(x)\n\n        # Upper\n        skip_connections = skip_connections[::-1]\n        for idx in range(0, len(self.ups), 2):\n            x = self.ups[idx](x)\n            skip_connection = skip_connections[idx \/\/ 2]\n            if x.shape != skip_connection.shape:\n                x = TF.resize(x, size=skip_connection.shape[2:])\n            concat_skip = torch.cat((skip_connection, x), dim=1)\n            x = self.ups[idx + 1](concat_skip)\n\n        return self.final_conv(x)\n","3ccf9c36":"# utils.py\n\ndef base64_2_mask(s):\n    z = zlib.decompress(base64.b64decode(s))\n    n = np.frombuffer(z, np.uint8)\n    mask = cv2.imdecode(n, cv2.IMREAD_UNCHANGED)[:, :, 3].astype(bool)\n    return mask\n\n\ndef mask_2_base64(mask):\n    img_pil = Image.fromarray(np.array(mask, dtype=np.uint8))\n    img_pil.putpalette([0, 0, 0, 255, 255, 255])\n    bytes_io = io.BytesIO()\n    img_pil.save(bytes_io, format=\"PNG\", transparency=0, optimize=0)\n    bytes = bytes_io.getvalue()\n    return base64.b64encode(zlib.compress(bytes)).decode(\"utf-8\")\n\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n\n\ndef get_loaders(\n    train_dir,\n    train_maskdir,\n    batch_size,\n    train_transform,\n    num_workers=4,\n    pin_memory=True,\n):\n    footballBannerDataset = FootballBannerDataset(\n        image_dir=train_dir,\n        mask_dir=train_maskdir,\n        transform=train_transform,\n    )\n    train_ds, val_ds = torch.utils.data.random_split(footballBannerDataset, [7000, 1851])\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        shuffle=True,\n    )\n\n    val_loader = DataLoader(\n        val_ds,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        shuffle=False,\n    )\n\n    return train_loader, val_loader\n\n\ndef check_accuracy(loader, model, device=\"cuda\"):\n    num_correct = 0\n    num_pixels = 0\n    dice_score = 0\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device)\n            y = y.to(device).unsqueeze(1)\n            preds = torch.sigmoid(model(x))\n            preds = (preds > 0.5).float()\n            num_correct += (preds == y).sum()\n            num_pixels += torch.numel(preds)\n            dice_score += (2 * (preds * y).sum()) \/ ((preds + y).sum() + 1e-8)\n\n    print(f\"Got {num_correct}\/{num_pixels} with acc {num_correct\/num_pixels*100:.2f}\")\n    print(f\"Dice score: {dice_score\/len(loader)}\")\n    model.train()\n\n\ndef save_predictions_as_imgs(loader, model, folder=\"saved_images\", device=\"cuda\"):\n    model.eval()\n    for idx, (x, y) in enumerate(loader):\n        x = x.to(device=device)\n        with torch.no_grad():\n            preds = torch.sigmoid(model(x))\n            preds = (preds > 0.5).float()\n        if not os.path.exists(folder):\n            !mkdir $folder\n        torchvision.utils.save_image(preds, f\"{folder}\/pred_{idx}.png\")\n        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder}\/{idx}.png\")\n\n    model.train()\n","e88435a4":"# train.py\n\n\ndef train_fn(loader, model, optimizer, loss_fn, scaler, scheduler=None):\n    \"\"\"Does one epoch of training.\"\"\"\n    loop = tqdm(loader)\n\n    for batch_idx, (data, targets) in enumerate(loop):\n        data = data.to(device=DEVICE)\n        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n\n        # forward\n        with torch.cuda.amp.autocast():\n            predictions = model(data)\n            loss = loss_fn(predictions, targets)\n\n        # backward\n        optimizer.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        # Update tqdm loop\n        loop.set_postfix(loss=loss.item())\n    return loss\n\n\ndef main():\n    train_transform = A.Compose(\n        [\n            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n            A.Rotate(limit=35, p=1.0),\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.1),\n            A.Normalize(\n                mean=[0.0, 0.0, 0.0],\n                std=[1.0, 1.0, 1.0],\n                max_pixel_value=255.0,\n            ),\n            ToTensorV2(),\n        ],\n    )\n\n    model = UNET(in_channels=3, out_channels=1).to(DEVICE)\n    loss_fn = nn.BCEWithLogitsLoss()  # cross entropy loss\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    train_loader, val_loader = get_loaders(\n        TRAIN_IMG_DIR,\n        TRAIN_MASK_DIR,\n        BATCH_SIZE,\n        train_transform,\n        NUM_WORKERS,\n        PIN_MEMORY,\n    )\n\n    if LOAD_MODEL:\n        load_checkpoint(torch.load(\"..\/input\/unet-football-banner-image-segmentation\/my_checkpoint.pth.tar\"), model)\n\n    check_accuracy(val_loader, model, device=DEVICE)\n    scaler = torch.cuda.amp.GradScaler()\n\n    min_loss = 999999\n    \n    for epoch in range(NUM_EPOCHS):\n        loss = train_fn(train_loader, model, optimizer, loss_fn, scaler)\n\n        # save model\n        checkpoint = {\n            \"state_dict\": model.state_dict(),\n            \"optimizer\": optimizer.state_dict(),\n        }\n        save_checkpoint(checkpoint)\n\n        # check accuracy\n        check_accuracy(val_loader, model, device=DEVICE)\n\n        # print some examples to a folder\n        save_predictions_as_imgs(\n            val_loader, model, folder=\"saved_images\/\", device=DEVICE\n        )\n        \n        \n        if loss > min_loss:\n            stopping_counter+=1\n        else:\n            stopping_counter=0\n        if stopping_counter == EARLY_STOPPING_PATIENCE:\n            break\n        \n        if loss<min_loss:\n            min_loss = loss","a401b30b":"meta_class_data = {\n    \"mastercard\": 0,\n    \"nissan\": 1,\n    \"playstation\": 2,\n    \"unicredit\": 3,\n    \"pepsi\": 4,\n    \"adidas\": 5,\n    \"gazprom\": 6,\n    \"heineken\": 7,\n}\n\n# Hyperparameters etc.\nLEARNING_RATE = 1e-3\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 32  # 32\nNUM_EPOCHS = 16  # 100\nNUM_WORKERS = 2\nEARLY_STOPPING_PATIENCE = 5\nIMAGE_HEIGHT = 72  # 720 originally\nIMAGE_WIDTH = 128  # 1280 originally\nPIN_MEMORY = True\nLOAD_MODEL = True\nTRAIN_IMG_DIR = \"..\/input\/football-advertising-banners-detection\/football\/images\"\nTRAIN_MASK_DIR = \"..\/input\/football-advertising-banners-detection\/football\/annotations\"","88f67e75":"#main()","ad1b7990":"model = UNET(in_channels=3, out_channels=1).to(DEVICE)","69bc90a0":"load_checkpoint(torch.load(\"..\/input\/unet-football-banner-image-segmentation\/my_checkpoint.pth.tar\"), model)","ac4ad0e0":"train_transform = A.Compose(\n        [\n            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n            A.Rotate(limit=35, p=1.0),\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.1),\n            A.Normalize(\n                mean=[0.0, 0.0, 0.0],\n                std=[1.0, 1.0, 1.0],\n                max_pixel_value=255.0,\n            ),\n            ToTensorV2(),\n        ],\n    )\n\ntrain_loader, val_loader = get_loaders(\n        TRAIN_IMG_DIR,\n        TRAIN_MASK_DIR,\n        BATCH_SIZE,\n        train_transform,\n        NUM_WORKERS,\n        PIN_MEMORY,\n    )\n\nnum_correct = 0\nnum_pixels = 0\ndice_score = 0\nwith torch.no_grad():\n    for i, (x, y) in enumerate(val_loader,0):\n        x = x.to(DEVICE)\n        y = y.to(DEVICE).unsqueeze(1)\n        preds = torch.sigmoid(model(x))\n        preds = (preds > 0.5).float()\n        num_correct += (preds == y).sum()\n        num_pixels += torch.numel(preds)\n        dice_score += (2 * (preds * y).sum()) \/ ((preds + y).sum() + 1e-8)\n\nprint(f\"\\nValidation Data Scores:\")\nprint(f\"Got {num_correct}\/{num_pixels} with acc {num_correct\/num_pixels*100:.2f}\")\nprint(f\"Dice score: {dice_score\/len(val_loader)}\")","0315e0f1":"import matplotlib.pyplot as plt\nfrom torchvision import transforms\nfrom torchvision.transforms import functional as F\n\n#fig.suptitle(\"Example Model Predictions\", fontsize=24)\n\nfor x,y in val_loader:\n    x = x.to(device=DEVICE)\n    with torch.no_grad():\n        preds = torch.sigmoid(model(x))\n        preds = (preds > 0.5).float()\n    plt.figure(figsize=(20, 12))\n    for i in range(0,12):\n        plt.subplot(4,3,i+1)\n        img = F.to_pil_image(x[i])\n        plt.imshow(img)\n        mask = F.to_pil_image(preds[i])\n        plt.imshow(mask, cmap='jet', alpha=0.4)\n        plt.tight_layout()\n    plt.show()\n    break\n        ","7460a932":"# Test on Example Case","35675b51":"---","ed218417":"<div align=center>\n<img src=https:\/\/www.markspaneth.com\/assets\/images\/blog\/_list_image\/02_02_18_508408464_AAB_560x292.jpg><\/img>\n<\/div>"}}