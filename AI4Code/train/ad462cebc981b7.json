{"cell_type":{"31854d6a":"code","d9948ede":"code","8c43197e":"code","03ed0581":"code","40575f17":"code","eabc2ffb":"code","196b15c5":"code","fdb4993e":"code","c4e6b443":"code","00439721":"code","46508697":"code","edd2eb5c":"code","84f58785":"code","b8d8a083":"code","7225877e":"code","7dd0a0c2":"code","a0c2337d":"code","f2cf8c33":"code","188dfab4":"code","cca49fce":"code","0d6f9e24":"code","471cbf57":"code","2009db03":"code","81f3d9f3":"code","19fc73c4":"code","39ad19c3":"code","311cf5aa":"code","4611fecd":"code","1a2aeac8":"code","a85fe55d":"code","53e79500":"code","41dc359b":"code","7a972654":"markdown","5444c193":"markdown","89ce1fa0":"markdown","336a253d":"markdown","6a2448c8":"markdown","7f43befe":"markdown","0c09d42d":"markdown","b2c36f80":"markdown","ae7a290f":"markdown","27cbba8c":"markdown","1153e010":"markdown","f5eda8b8":"markdown"},"source":{"31854d6a":"###IMPORTING LIBRARIES","d9948ede":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom feature_engine import missing_data_imputers as mdi\nfrom feature_engine.categorical_encoders import RareLabelCategoricalEncoder\nfrom feature_engine.categorical_encoders import OneHotCategoricalEncoder\nimport seaborn as sns\nfrom feature_engine import variable_transformers as vt\nimport scipy.stats as stats\nfrom feature_engine.discretisers import DecisionTreeDiscretiser\nfrom sklearn.feature_selection import VarianceThreshold\nfrom feature_engine import categorical_encoders as ce\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.pipeline import Pipeline\nfrom feature_engine import missing_data_imputers as mdi\nfrom feature_engine import discretisers as dsc\nfrom feature_engine import categorical_encoders as ce\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score","8c43197e":"data = pd.read_csv('..\/input\/titanic\/train.csv')\ndata.head()","03ed0581":"data.isnull().sum()","40575f17":"data['Cabin'].isnull().groupby(data['Survived']).mean()","eabc2ffb":"data['Age'].isnull().groupby(data['Survived']).mean()","196b15c5":"#making a missing indicator for cabin\ndata['cabin_null'] = np.where(data['Cabin'].isnull(),1,0)\n#replacing missing values of embarked with mode\ndata['Embarked'] = np.where(data['Embarked'].isnull(),'S',data['Embarked'])","fdb4993e":"#deleting passengerId and ticket\ndel data['PassengerId']\ndel data['Ticket']","c4e6b443":"#extarcting and mapping title \ndata['Title'] = data.Name.apply(lambda Name: Name.split(',')[1].split('.')[0].strip())\n\n\nnormalized_titles = {\n    \"Capt\":       \"Officer\",\n    \"Col\":        \"Officer\",\n    \"Major\":      \"Officer\",\n    \"Jonkheer\":   \"Royalty\",\n    \"Don\":        \"Royalty\",\n    \"Sir\" :       \"Royalty\",\n    \"Dr\":         \"Officer\",\n    \"Rev\":        \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Dona\":       \"Royalty\",\n    \"Mme\":        \"Mrs\",\n    \"Mlle\":       \"Miss\",\n    \"Ms\":         \"Mrs\",\n    \"Mr\" :        \"Mr\",\n    \"Mrs\" :       \"Mrs\",\n    \"Miss\" :      \"Miss\",\n    \"Master\" :    \"Master\",\n    \"Lady\" :      \"Royalty\"\n}\n\ndata.Title = data.Title.map(normalized_titles)\n\nprint(data.Title.value_counts())","00439721":"#deleting name column\ndel data['Name']","46508697":"# numerical: discrete vs continuous\ndiscrete = [var for var in data.columns if data[var].dtype!='O' and var!='Survived' and data[var].nunique()<10]\ncontinuous = [var for var in data.columns if data[var].dtype!='O' and var!='Survived' and var not in discrete]\n\n# mixed\nmixed = ['Cabin']\n\n# categorical\ncategorical = [var for var in data.columns if data[var].dtype=='O' and var not in mixed]\n\nprint('There are {} discrete variables'.format(len(discrete)))\nprint('There are {} continuous variables'.format(len(continuous)))\nprint('There are {} categorical variables'.format(len(categorical)))\nprint('There are {} mixed variables'.format(len(mixed)))","edd2eb5c":"for var in discrete+categorical:\n    print(data[var].value_counts())\n    (data.groupby(var)['Survived'].agg(sum)\/data[var].value_counts()).plot(kind='bar')\n    plt.show()","84f58785":"#we can see from 1st graph that people with class 3 have a low survival rate to mark this we create a new feature \ndata['third class'] = np.where(data['Pclass']==3,0,1)\n","b8d8a083":"#people who are travelling alone have a very low survival rate \ndata['no_sibsp'] = np.where(data['SibSp']==0,1,0)\ndata['no_parch'] = np.where(data['Parch']==0,1,0)\ndata['no_family'] = data['no_sibsp']+data['no_parch']","7225877e":"#people with eambark 'C' have a high chance of survival\ndata['embarked_c'] = np.where(data['Embarked']=='C',1,0)","7dd0a0c2":"#people with 'Mr' title have a very low chance of survival\ndata['mr_cat'] = np.where(data['Title']=='Mr',0,1)","a0c2337d":"#plotting relationship of fare with survival\nplt.hist([data[data['Survived']==1]['Fare'],data[data['Survived']==0]['Fare']],\n       stacked=True,\n                                                bins = 50,\n                                                color = ['g','r'],\n                                                label = ['s','d'])\nplt.legend()","f2cf8c33":"plt.hist([data[data['Survived']==1]['Age'],data[data['Survived']==0]['Age']],\n       stacked=True,\n                                                bins = 50,\n                                                color = ['g','r'],\n                                                label = ['s','d'])\nplt.legend()","188dfab4":"# fare price is related to survival\ndata['low_fare'] = np.where(data['Fare']<15,0,1)\n","cca49fce":"##adding family size as a feature\n\ndata['family_size'] = data['Parch']+data['SibSp']\n","0d6f9e24":"# Cabin(mixed feature)\ndata['cabin_num'] = data['Cabin'].str.extract('(\\d+)') # captures numerical part\ndata['cabin_num'] = data['cabin_num'].astype('float')\ndata['cabin_cat'] = data['Cabin'].str[0] # captures the first letter","471cbf57":"#replacing missing values in cabin_cat\ndata['cabin_cat'] = np.where(data['cabin_cat'].isnull(),'M',data['cabin_cat'])\n","2009db03":"#deleting cabin feature\ndel data['Cabin']","81f3d9f3":"mean_imputer = mdi.ArbitraryNumberImputer(arbitrary_number=-1,\n                                variables=['cabin_num'])\nmean_imputer.fit(data)\ndata = mean_imputer.transform(data)","19fc73c4":"#age imputed with mean\nmean_imputer = mdi.MeanMedianImputer(imputation_method = 'mean',\n                                variables=['Age'])\nmean_imputer.fit(data)\ndata = mean_imputer.transform(data)","39ad19c3":"#storing value of survived\ny = data.iloc[:, 0].values\ny_df = data['Survived']\ndel data['Survived']","311cf5aa":"#ordinal encoding\nenc = ce.OrdinalCategoricalEncoder(encoding_method='ordered',\n                                  variables=['cabin_cat', 'Sex', 'Embarked','Title'])\nenc.fit(data,y_df)\ndata = enc.transform(data)","4611fecd":"#train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    data,  # predictors\n    y_df,  # target\n    test_size=0.1,  # percentage of obs in test set\n    random_state=0)","1a2aeac8":"#scaling data\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx = sc.fit_transform(X_train)\nx_test = sc.transform(X_test)","a85fe55d":"#hyperparameter tuning (snippet taken from scikit documentation)\ntuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n                     'C': [1, 10, 100, 1000]},\n                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n\nscores = ['precision', 'recall']\n\nfor score in scores:\n    print(\"# Tuning hyper-parameters for %s\" % score)\n    print()\n\n    clf = GridSearchCV(\n        SVC(), tuned_parameters, scoring='%s_macro' % score\n    )\n    clf.fit(x, y_train)\n\n    print(\"Best parameters set found on development set:\")\n    print()\n    print(clf.best_params_)\n    print()\n    print(\"Grid scores on development set:\")\n    print()\n    means = clf.cv_results_['mean_test_score']\n    stds = clf.cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n        print(\"%0.3f (+\/-%0.03f) for %r\"\n              % (mean, std * 2, params))\n    print()\n\n    print(\"Detailed classification report:\")\n    print()\n    print(\"The model is trained on the full development set.\")\n    print(\"The scores are computed on the full evaluation set.\")\n    print()\n    y_true, y_pred = y_test, clf.predict(x_test)\n    print(classification_report(y_true, y_pred))\n    print()","53e79500":"#fitting model with hyperparameter obtained from previous step\nclf = SVC(C=1000,gamma=0.001,kernel = 'rbf')\n\nclf.fit(x,y_train)\nclf.score(x,y_train)*100","41dc359b":"y_pred = clf.predict(x_test)\naccuracy_score(y_pred,y_test)*100","7a972654":"**Now I will group features into 3 categories discrete,continous and categorical and analyse them**","5444c193":"**We can see that there are more missing values for people who did not survive we will use this information to create a missing indicator for both age and cabin as there are only 2 missing values for embarked we can assume that those values are missing completely at random and impute it with mode**","89ce1fa0":"**As,we can see age,cabin and embarked have missing values lets analyse this further **","336a253d":"**I impute missing value for cabin_num as an arbitary value -1 to reflect the relationship of missing values and people not surviving**","6a2448c8":"**I will plot the discrete and categorical features to gather insights**","7f43befe":"> **This model gives an accuracy of 79.665% on train set provided on kaggle!!!**","0c09d42d":"**85.56% accuracy on train set**","b2c36f80":"**TITANIC DATASET ANALYSIS AND PREDICTIONS(79.665% ACCURACY)**","ae7a290f":"**84.14 % accuracy on train set**","27cbba8c":"**I will delete the passengerId and Ticket column as it will become very difficult to engineer the ticket column as it is mixed and has very high cardinality I will also delete the Name column from data after excracting Title from it**","1153e010":"**Now I will extract title from name column and map it to fewer categories to reduce cardinality**","f5eda8b8":"**splitting cabin feature into two different features to reduce cardinality**"}}