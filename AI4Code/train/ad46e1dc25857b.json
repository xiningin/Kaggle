{"cell_type":{"c4feb861":"code","9da8bdb6":"code","d4b54698":"code","d04cc5f2":"code","87193646":"code","77ac232b":"code","51bfe081":"code","cf0d4fa9":"code","be1a4c6f":"code","34a21986":"code","ff577cfd":"code","b74e8a70":"code","a7a6b1a5":"code","3552b932":"code","470f61a0":"code","b1f16810":"code","e70daf31":"code","e14e8ca0":"code","74ad61e2":"code","e6779518":"code","96ed5419":"code","8083fbe3":"code","65f4312d":"code","4935bd16":"code","94365475":"code","a11fb69f":"code","234b5581":"code","c8f2ea73":"code","0f33b5e9":"code","8864e1aa":"code","877739c1":"code","9f6a3cba":"markdown","b73c21c0":"markdown","763ab9a5":"markdown","f2deaa7b":"markdown","8bc60c67":"markdown","6b9f8987":"markdown","69999e76":"markdown","8bb97075":"markdown","4cd3e8ee":"markdown","3e19b6be":"markdown","97fdf210":"markdown","6c5fd42f":"markdown","c541f47e":"markdown","4ce3989a":"markdown","4024c355":"markdown","7dbdc3bc":"markdown"},"source":{"c4feb861":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9da8bdb6":"# data handling \nimport numpy as np \nimport pandas as pd\n\nfrom statistics import mode, median\n\n# data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport warnings \nwarnings.simplefilter('ignore') \n%matplotlib inline ","d4b54698":"# machine learning\n# data preprocessing\nfrom sklearn.preprocessing import MinMaxScaler, Normalizer\n\n# linear models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n# models metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, plot_confusion_matrix\n\n# models validation\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score","d04cc5f2":"df = pd.read_csv('\/kaggle\/input\/heart-disease-prediction-using-logistic-regression\/framingham.csv')","87193646":"df.shape","77ac232b":"df.describe()","51bfe081":"# splitting variables \nbin_vars = ['male','currentSmoker','BPMeds','prevalentStroke','prevalentHyp','diabetes','TenYearCHD']\nord_vars = ['education']\ncon_vars = ['totChol','sysBP','diaBP','BMI','heartRate','glucose']\ndis_vars = ['age', 'cigsPerDay']","cf0d4fa9":"bin_count = pd.DataFrame(columns = bin_vars)\nfor var in bin_vars: bin_count[var] = round(df[var].value_counts()\/df[var].count()*100)\n\nbin_count.T.plot(kind=\"bar\", stacked=True, color=['cadetblue','indianred'])\nplt.legend(loc='lower left')\nplt.ylabel(\"Percentage\",fontsize=12)\nplt.show()","be1a4c6f":"df.education.value_counts().plot(kind='pie', startangle=90, figsize=(3,3), autopct='%1.1f%%', legend=False, colors=['indianred','darkorange','darkolivegreen','forestgreen'])\nplt.tight_layout()","34a21986":"num_vars = con_vars + dis_vars","ff577cfd":"sns.boxplot(data = df[num_vars])\nplt.show()","b74e8a70":"for var in num_vars:\n    sns.distplot(df[var], bins=5, label= var, axlabel=False)\nplt.legend()\nplt.show()","a7a6b1a5":"df[num_vars].skew().plot(kind='bar', ylabel =\"Skewness\", title= 'Skewness of variables',figsize=(5,2),legend=False, fontsize=12, use_index=True, rot=45, color = 'cadetblue')\nplt.show()","3552b932":"corr = df[num_vars].corr(method='pearson')\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(corr)] = True\nsns.heatmap(corr,mask=mask,annot=True, fmt=\".1f\",cmap=\"YlGnBu\")\nplt.show()","470f61a0":"combis = (('sysBP','diaBP'),('sysBP','BMI'),('sysBP','age'))\n\nfig, axes = plt.subplots(1, len(combis), figsize=(7, 3))\n\nfor i, c in enumerate(combis):\n    sns.regplot(df[list(c)[0]], df[list(c)[1]], line_kws={'color': 'indianred'},scatter_kws={'color': 'cadetblue'}, ax=axes[i])\n\nfig.tight_layout()","b1f16810":"sns.heatmap(df.isna(),yticklabels=False,cbar=False,cmap='summer')\nplt.show()","e70daf31":"md = (df.isna().sum()\/df.shape[0]*100).sort_values(ascending=False)\nmd.get(md.values > 0).plot(kind='bar', ylabel = 'Missing (%)', title =\"Percentage of missing values per variable\",figsize=(5,2),\n                           legend=False, rot=45, color = 'darkolivegreen')\nplt.show()","e14e8ca0":"# Replace missing data\n\nfor var in md.get(md.values > 0).index:  \n    if var in con_vars: #continuous --> mean\n        avg = df[var].mean()\n        df[var].fillna(value=avg, inplace=True)\n        print(f'{var}, average: {round(avg,2)}')\n    elif var in bin_vars or var in ord_vars: #binary\/ordinal --> mode\n        mod = mode(df[var])\n        df[var].fillna(value=mod, inplace=True)\n        print(f'{var}, mode: {mod}')\n    elif var in dis_vars: # discrete --> median\n        med = median(df[var])\n        df[var].fillna(value=med, inplace=True)\n        print(f'{var}, median: {med}')","74ad61e2":"#check\nsns.heatmap(df.isna(),yticklabels=False,cbar=False,cmap='summer')\nplt.show()","e6779518":"def smoker(sub):\n    if sub['cigsPerDay'] == 0:\n        return 0\n    elif 0 < sub['cigsPerDay'] < 21:\n        return 1\n    elif sub['cigsPerDay'] > 20:\n        return 2\n\ndf['smoker'] = df.apply (lambda sub: smoker(sub), axis=1)","96ed5419":"# drop old variable\ndf.drop(columns=['currentSmoker','cigsPerDay'], axis=1, inplace=True) ","8083fbe3":"# update\nbin_vars = ['male','BPMeds','prevalentStroke','prevalentHyp','diabetes','TenYearCHD']\nord_vars = ['education', 'smoker']\ncon_vars = ['totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']\ndis_vars = ['age']","65f4312d":"# reorder variables : categorical, ordinal, continuous\nnew_order = bin_vars[0:5] + ord_vars + dis_vars + con_vars + ['TenYearCHD']\ndf = df[new_order]\ndf.head(2)","4935bd16":"num_vars = con_vars + dis_vars","94365475":"# same scale btw 0 and 1\ndf[num_vars] = MinMaxScaler(feature_range=(0, 1)).fit_transform(df[num_vars])\n\n# normalize\ndf[num_vars]= Normalizer().fit(df[con_vars]).transform(df[num_vars])","a11fb69f":"#check\nsns.boxplot(data = df[num_vars])\nplt.show()","234b5581":"#check\nfor var in num_vars:\n    sns.distplot(df[var], bins=5, label= var,axlabel =False)\nplt.legend()\nplt.show()","c8f2ea73":"# split array into input and output components\narray = df.values\nX = array[:,0:df.shape[1]-1] #in\nY = array[:,df.shape[1]-1] #out\n\n# split between train and Test Sets\nX_train, X_test, Y_train, Y_test = train_test_split(X, \n                                                    Y, \n                                                    test_size=0.33, \n                                                    random_state=7, \n                                                    shuffle=True)\n\n# Logistic regression model\nLR = LogisticRegression(solver='liblinear', random_state=0)\nLR.fit(X_train, Y_train) \n\n#accuracy: the ratio of the number of correct predictions to the number of observations\nprint(f\"Accuracy: {round((LR.score(X_test, Y_test)) *100.0,2)}\")","0f33b5e9":"print('Classification report:\\n', classification_report(Y_test, LR.predict(X_test)))","8864e1aa":"#confusion matrix\ncm = confusion_matrix(Y_test, LR.predict(X_test),labels=[0,1])\n# Plot confusion matrix\ntitles = [(\"Not normalized confusion matrix\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles:\n    disp = plot_confusion_matrix(LR, X_test, Y_test,\n                                 display_labels=['No risk (0)', 'Risk (1)'],\n                                 cmap=plt.cm.Greens,\n                                 normalize=normalize)\n    disp.ax_.set_title(title)\n    plt.grid(False) \n    print(title)\n    print(disp.confusion_matrix)\n\nplt.show()","877739c1":"names = ['logReg','Perc','linearSVC','GauNaiBay','linearDA']\n\nmods = [LogisticRegression(), \n        Perceptron(tol=1e-3, random_state=0), \n        LinearSVC(),\n        GaussianNB(),\n        LinearDiscriminantAnalysis()]\n\n\nfor n, mod in zip(names,mods):\n    \n    mod.fit(X_train, Y_train) \n    \n    print(f\"Accuracy {n}: {round((mod.score(X_test, Y_test)) *100,2)}\")","9f6a3cba":"### Different models\n\nComparison between different linear classification algoritms: \n\n- Logistic regression LR\n- Perceptron\n- Linear support vector machine SVM\n- Gaussian naive bayes classifier GNB\n- Linear discriminant analysys LDA","b73c21c0":"### Feature engineering\nVariable currentSmoker and cigsPerDay are redundant.   \nI will create a multi-level categorical variable where:\n- 0 are non smoker, \n- 1 are light smokers (1-20 cigsPerDay) \n- 2 are heavy smokers (> 20 cigsPerDay)","763ab9a5":"### Visualize categorical\nSeveral binary variables are not balanced between categories with a few positive observations (values = 1)","f2deaa7b":"### Missing data\nMissing values will be replaced by central trend statistics:\n- continuous vars --> mean\n- binary and ordinal vars --> mode\n- discrete vars --> median\n\n\nNo variables will be dropped because missing percent < 15%","8bc60c67":"## Explore ","6b9f8987":"### Conclusions\n\nThe logistic regression model (as the linear SVC) shows an accuracy of nearly 85%, however it generates several false negative, missing cases at risk. ","69999e76":"### Visualize numeric\nSome numeric variables appear to be skewed (glucose) and some highly correlated (sysBP and diaBP)","8bb97075":"## Intro\n\n**Dataset** The Framingham Heart study datasets present in Kaggle is a collection of over 4000 observations of subjects at 10 year risk (or not) of coronary heart disease CHD. The dataset includes 15 demographic, behavioural and medical variables and a variable 'TenYearCHD' of 10 year risk of coronary heart disease CHD.\n\n**Analysis** Logistic regression will be used for binary classification of the 10 year risk\/norisk of CHD (dependent variable). The independent variables will be the 15 numerical and categorical variables of this dataset. The accuracy of logistic regression will be compared to others linear approaches.","4cd3e8ee":"## Logistic regression classification\n","3e19b6be":"## Preprocessing","97fdf210":"## Import packages and modules","6c5fd42f":"**The dataset**\n- The amount of missing data is limited\n- Some categorical variables have few observations in one of the categories\n- Some continuous variables show a not normal distribution\n- Some continuous variables show correlation\n\n**The analysis**\n- Logistic regression is the algorithm showing best accuracy\n- However the sensitivity is very low accounting for many false negative \n\n**Sugegstions**  \n- Use an extended versions of the database (more balanced variables with a more normal distribution)\n- Try with non linear models \n- Try different parameters other than accuracy to evaluate the models \n- Differen preprocessing approaches and feature selections ","c541f47e":"### Split variables \nbetween categorical(binary\/ordinal)   \nand numeric(continuous\/discrete)","4ce3989a":"### Normalization\nof numerical variables","4024c355":"### Pipeline\n- **Import** of python packages and of the dataset\n- **Explore and visualization** of the dataset \n- **Preprocessing**: missing data, feature engineering, normalization of continuous variables  \n- **Logistic regression analysis and comparison** with other linear approaches","7dbdc3bc":"### Import data"}}