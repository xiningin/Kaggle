{"cell_type":{"023e1f21":"code","9750da65":"code","6f07f848":"code","03034a54":"code","d8ad9660":"code","b08701c1":"code","a27a50e8":"code","fc95be1a":"code","1cad71ac":"code","7d122f0c":"code","05399d47":"code","36194168":"code","650ac148":"code","59bcbbe4":"code","94135f0c":"code","df382e0b":"code","22e082d5":"code","43a24a21":"code","6350c9f1":"code","9907da80":"code","c91adfbf":"code","48dcffd0":"code","9386b27a":"code","65f713d5":"code","56362a27":"code","7e159342":"code","2cfc9dce":"code","b30604ab":"code","8dfc4d72":"code","179c7998":"code","b233058c":"code","2f9d9e3e":"code","233da62a":"code","5c536b9f":"code","b39cec4a":"code","e82d1970":"code","98103835":"code","ce5c1084":"code","459bb6ba":"code","57934ba5":"code","343eb3c3":"code","1b3429af":"code","488b4114":"code","f1c9a10c":"code","648e69e4":"code","f4bce829":"code","bcd9c004":"code","e468fbac":"code","0e997182":"code","24c95aff":"code","f2a97b9c":"code","83c1d3fe":"code","4ad3aeba":"code","dbeb598b":"code","1bebc207":"code","3e613fa3":"code","7c20080a":"code","494ebb27":"code","a0bbc147":"code","1968c0c2":"code","e5eca795":"code","bf0aeb16":"code","8f5152c9":"code","2d03ac08":"code","1c3b3fc0":"code","199d6cf1":"code","1174df54":"code","05492376":"code","8ab9f6fc":"code","033ee7e7":"code","b039f0dc":"code","f9742d82":"code","946c8628":"code","c1b9be0a":"code","32c40581":"code","e6981229":"code","41d7a496":"code","21b831eb":"code","ebd27f86":"code","281e76ac":"code","8f88f717":"code","424e8367":"code","d66e6f51":"code","33537a26":"code","bbbf63a6":"code","c7794fc0":"code","1796d07a":"code","a7516efc":"code","076a8e35":"code","be284f93":"markdown","61e20c9d":"markdown","10ce9edb":"markdown","1964d3e8":"markdown","809563e9":"markdown","2a186bae":"markdown","8787d8f7":"markdown","c1c82dd2":"markdown","2714dae0":"markdown","d36356b3":"markdown","cdcc4e99":"markdown","2f852f27":"markdown","0cc24dbd":"markdown","d7b04839":"markdown","109ca481":"markdown","67a00574":"markdown","df51d337":"markdown","58ef6116":"markdown","4c036b5f":"markdown","b02c28db":"markdown","896bc77e":"markdown","7abf101b":"markdown","384d41f7":"markdown","9cc781c3":"markdown","0eb52254":"markdown","775eab72":"markdown","04292bda":"markdown","d40c8e58":"markdown","3b14e3c0":"markdown","8d672660":"markdown","36f97c1e":"markdown","7dcc30a6":"markdown","3c6d6ca3":"markdown","0a20e256":"markdown","674e0ea5":"markdown","c04ab4b2":"markdown","35e45448":"markdown","cc174fd1":"markdown","1ffba58b":"markdown","62dccf41":"markdown","ab9bf8de":"markdown","d552dd31":"markdown","b825de83":"markdown"},"source":{"023e1f21":"## Import Essential Libraries\n!pip install geocoder\n!pip install geopy\nimport os #for file part\nfrom math import * # for mathmatically relations & computation  \nfrom scipy.stats import * # Stat testing\nimport numpy as np # array operations \nimport re # working on text data \nimport pandas as pd # dataframe operations\nimport matplotlib.pyplot as plt # visualisations\nimport seaborn as sns # Viz\nimport geocoder # Geographical Data analysis\nimport geopy # Geographical data\nfrom geopy.geocoders import Nominatim  # Geographical data\n\n# All machine leanring libraries\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.linear_model import LinearRegression, Ridge, ElasticNet, Lasso\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error","9750da65":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6f07f848":"### Load data into a dataframe\ntrain = pd.read_csv('\/kaggle\/input\/rental-price-of-indias-it-capital-pune-mh-ind\/train.csv', encoding = 'utf-8')","03034a54":"train.shape","d8ad9660":"train.info() # lookin for dtypes , null values inside data","b08701c1":"train.isnull().sum()","a27a50e8":"train.describe() # A brief stat of data","fc95be1a":"df = train.copy() # createing a copy of original data of actual cleaning & feature engineering puppose","1cad71ac":"df.head(3)","7d122f0c":"## Warning \/ NOTe\n## few working requied on rent here intial\nprint('shape before', df.shape)\ndf.drop(df[df['rent'] >= 123456789].index, axis = 0, inplace = True) # this values is added by purpose while doing data cleaning \nprint('shape after', df.shape)","05399d47":"df.area.replace(0, np.nan, inplace =  True)\ndf.area.isnull().sum()\n\npd.pivot_table(df, values= 'area', columns='bedroom', aggfunc= 'mean').T\npd.pivot_table(df, values= 'area', columns='bedroom', aggfunc= 'median').T\n\nm1 = df['bedroom']  == 1\nm2 = df['bedroom']  == 2\nm3 = df['bedroom']  == 3\nm4 = df['bedroom']  == 4\nm5 = df['bedroom']  == 5\nm6 = df['bedroom']  == 6\ndf.loc[m1, 'area'] = df.loc[m1,'area'].fillna(df.loc[m1,'area'].median())\ndf.loc[m2, 'area'] = df.loc[m2,'area'].fillna(df.loc[m2,'area'].median())\ndf.loc[m3, 'area'] = df.loc[m3,'area'].fillna(df.loc[m3,'area'].median())\ndf.loc[m4, 'area'] = df.loc[m4,'area'].fillna(df.loc[m4,'area'].median())\ndf.loc[m5, 'area'] = df.loc[m5,'area'].fillna(df.loc[m5,'area'].median())\ndf.loc[m6, 'area'] = df.loc[m6,'area'].fillna(df.loc[m6,'area'].median())\n\n","36194168":"num_columns = df.describe().columns\ncategorical_cols = df.describe(include= 'object').columns","650ac148":"# checking for binary data & oridinal & continous_data \nfor i in df.columns:\n    print(len(df[i].unique()), end = '    ')\n# range varies form 2 to  6387\n","59bcbbe4":"binary_variables = [i  for i in df.columns  if (len(df[i].unique()) == 2)]","94135f0c":"ordinal_variables= [i for i in df.columns if ((len(df[i].unique()) > 2 ) and (len(df[i].unique()) <= 25))]","df382e0b":"continous_variable = [i for i in df.columns if ((len(df[i].unique()) > 25))]\n## Remove rent from data\ncontinous_variable = continous_variable[:-1]","22e082d5":"target_variable = 'rent'","43a24a21":"print(binary_variables, end = ' ')\n\n('element in binary category', len(binary_variables))","6350c9f1":"df.gate_community.value_counts().index","9907da80":"fig, ax = plt.subplots(4,3, figsize = (20,30), sharey = True)\nfig.tight_layout()\nfig.subplots_adjust(top=0.95)\n\nsns.set_style('white')\nfig.suptitle('Binary Variables- Counts', size = 30)\nsns.barplot(ax= ax[0,0], data =df , x = df.gate_community.value_counts().index, y = df.gate_community.value_counts().values,palette= 'muted' )\nax[0,0].set_title('Gate Facility', fontsize = 15)\nsns.barplot(ax= ax[0,1], data =df , x = df.corner_pro.value_counts().index, y = df.corner_pro.value_counts().values,palette= 'pastel' )\nax[0,1].set_title('Conrner Property', fontsize = 15)\nsns.barplot(ax= ax[0,2], data =df , x = df.wheelchairadption.value_counts().index, y = df.wheelchairadption.value_counts().values,palette= 'muted' )\nax[0,2].set_title('Wheel Chair Facility',   fontsize = 15)\nsns.barplot(ax= ax[1,0], data =df , x = df.petfacility.value_counts().index, y = df.petfacility.value_counts().values,palette= 'husl' )\nax[1,0].set_title('Pet Friendly',  fontsize = 15)\nsns.barplot(ax= ax[1,1], data =df , x = df.lightbill.value_counts().index, y = df.lightbill.value_counts().values,palette= 'Set2' )\nax[1,1].set_title('Electricity Bill Included',  fontsize = 15)\nsns.barplot(ax= ax[1,2], data =df , x = df.no_room.value_counts().index, y = df.no_room.value_counts().values,palette= 'flare' )\nax[1,2].set_title('No other rooms',  fontsize = 15)\nsns.barplot(ax= ax[2,0], data =df , x = df.pooja_room.value_counts().index, y = df.pooja_room.value_counts().values,palette= 'ch:s=.25,rot=-.25' )\nax[2,0].set_title('Pooja\/ Holy Room',  fontsize = 15)\nsns.barplot(ax= ax[2,1], data =df , x = df.study_room.value_counts().index, y = df.study_room.value_counts().values,palette= 'muted' )\nax[2,1].set_title('Study_Room',  fontsize = 15)\nsns.barplot(ax= ax[2,2], data =df , x = df.others.value_counts().index, y = df.others.value_counts().values )\nax[2,2].set_title('other Rooms',  fontsize = 15)\nsns.barplot(ax= ax[3,0], data =df , x = df.servant_room.value_counts().index, y = df.servant_room.value_counts().values,palette= 'flare' )\nax[3,0].set_title('Servent Room',  fontsize = 15)\nsns.barplot(ax= ax[3,1], data =df , x = df.store_room.value_counts().index, y = df.store_room.value_counts().values,palette= 'husl' )\nax[3,1].set_title('Store Room',  fontsize = 15)\n# sns.barplot(ax=ax[3,2], x = df.index, y = df.rent)\nfig.delaxes(ax[3][2])\nfig.savefig('my.jpg')","c91adfbf":"print(ordinal_variables, end = ' ')\n('Number of ordinal features is', len(ordinal_variables))","48dcffd0":"### working on oultiers which I found while working on previously on porject\ndf.groupby('bedroom').count()\n# we have more than 6  bedrooms in single homes  so we will drop to reduce outliers  totals around 20\nprint('before shape',df.shape)\ndf.drop(df[df['bedroom']>= 6].index, axis = 0, inplace = True)\nprint('after droping rows the shape is ', df.shape)","9386b27a":"print('before replacement', df.furnishing.unique())\ndf.furnishing.replace('Unfurnishe', 'Unfurnished', inplace = True)\nprint('after replacement', df.furnishing.unique())","65f713d5":"print(ordinal_variables, end = ' ')\n('Number of ordinal features is', len(ordinal_variables))","56362a27":"fig, ax = plt.subplots(4,3, figsize = (20,30), sharey = False)\nfig.tight_layout(pad= 4.0)\nfig.subplots_adjust(top=0.95)\n\nsns.set_style('white')\nfig.suptitle('Ordinal Variable  vs Target Median', size = 30)\nsns.barplot(ax= ax[0,0], data =df , x = df.groupby('bedroom').median().index, y =df.groupby('bedroom').median().rent,palette= 'muted' )\nax[0,0].set_title('Number of Bedrooms', fontsize = 15)\n\nsns.barplot(ax= ax[0,1], data =df , x = df.groupby('bathrooms').median().index, y =df.groupby('bathrooms').median().rent,palette= 'muted' )\nax[0,1].set_title('Number of bathrooms', fontsize = 15)\n\nsns.barplot(ax= ax[0,2], data =df , x = df.groupby('furnishing').median().index, y =df.groupby('furnishing').median().rent,palette= 'muted' )\nax[0,2].set_title('Furnishsing', fontsize = 15)\n\nsns.barplot(ax= ax[1,0], data =df , x = df.groupby('avalable_for').median().index, y =df.groupby('avalable_for').median().rent,palette= 'muted' )\nax[1,0].set_title('Available for', fontsize = 15)\n\nsns.barplot(ax= ax[1,1], data =df , x = df.groupby('floor_number').median().index, y =df.groupby('floor_number').median().rent,palette= 'muted' )\nax[1,1].set_title('Floor Number', fontsize = 15)\n\nsns.barplot(ax= ax[1,2], data =df , x = df.groupby('facing').median().index, y =df.groupby('facing').median().rent,palette= 'muted' )\nax[1,2].set_title('', fontsize = 15)\n\nsns.barplot(ax= ax[2,0], data =df , x = df.groupby('floor_type').median().index, y =df.groupby('floor_type').median().rent,palette= 'muted' )\nax[2,0].set_title('Floor Type', fontsize = 15)\n\nsns.barplot(ax= ax[2,1], data =df , x = df.groupby('parking').median().index, y =df.groupby('parking').median().rent,palette= 'muted' )\nax[2,1].set_title('Count of Parking', fontsize = 15)\n\nsns.barplot(ax= ax[2,2], data =df , x = df.groupby('aggDur').median().index, y =df.groupby('aggDur').median().rent,palette= 'muted' )\nax[2,2].set_title('Agreement Duration', fontsize = 15)\n\nsns.barplot(ax= ax[3,0], data =df , x = df.groupby('noticeDur').median().index, y =df.groupby('noticeDur').median().rent,palette= 'muted' )\nax[3,0].set_title('Notice Duration', fontsize = 15)\n\nsns.barplot(ax= ax[3,1], data =df , x = df.groupby('powerbackup').median().index, y =df.groupby('powerbackup').median().rent,palette= 'muted' )\nax[3,1].set_title('Power Backup Facility', fontsize = 15)\n\nsns.barplot(ax= ax[3,2], data =df , x = df.groupby('propertyage').median().index, y =df.groupby('propertyage').median().rent,palette= 'muted' )\nax[3,2].set_title('Property Age', fontsize = 15)\n\nfig.savefig('ordinal_variable.jpg', dpi = 100)","7e159342":"# ['address']\ncat_variable = ['address']\ndf[cat_variable].head(2)","2cfc9dce":"locality = []\nfor i in df.address:\n    adds = i.split(',')\n    if len(adds) == 4:\n        locality.append(adds[0])\n    elif len(adds) == 5:\n        locality.append(adds[1])\n    elif len(adds)  == 6:\n        if len(adds[1]) == 1:\n            locality.append(adds[2])\n        else:\n            locality.append(adds[1])\n    elif len(adds) == 7:\n        locality.append(' '.join(adds[2:4]))\n    elif len(adds) == 8:\n        locality.append(adds[2])\n    else:\n        locality.append(adds[4])","b30604ab":"df['locality'] = locality\nlocation_summary = df['locality'].value_counts()\nlocations = [i.replace(',', ' ') for i in df.address]\nlen(location_summary.index)\nlocation_summary.to_csv('summary.csv')","8dfc4d72":"# locations = list(df['locality'].value_counts().index)\n# def get_latlng(neighborhood):\n#     # initialize your variable to None\n#     lat_lng_coords = None\n#     # loop until you get the coordinates\n#     while(lat_lng_coords is None):\n#         g = geocoder.arcgis('{} pune maaharashtra india'.format(neighborhood))\n#         lat_lng_coords = g.latlng\n#     return lat_lng_coords\n# coords = list()\n# for neighborhood in df2 [\"Neighborhood\"] .tolist():\n#     coords.append(get_latlng(neighborhood))","179c7998":"# !pip install folium\n# map_data = pd.read_csv('all_location.csv')\n# map_data.drop('Unnamed: 0', axis = 1, inplace =  True)\n# map_data.head()\n# lat_long_pune = [18.516726,73.856255]\n# import folium\n\n# def generateBaseMap(default_location= [18.516726,73.856255], default_zoom_start=11.5):\n#     base_map = folium.Map(location=default_location, control_scale=True, zoom_start=default_zoom_start)\n#     return base_map\n# base_map = generateBaseMap()\n# ## add data to base map\n\n# from folium.plugins import HeatMap\n# base_map = generateBaseMap()\n\n# gradient = {.33: 'black', .5: 'blue', 0.7: 'red'}\n# HeatMap(data=map_data[['Longitude', 'Latitude', 'score']].groupby(['Longitude', 'Latitude']).sum().reset_index().values.tolist(), radius=8, gradient= gradient, max_zoom=13).add_to(base_map)\n# # map_data.columns\n# base_map.save('testmap.html')","b233058c":"# maintenace amt is similar to the mnt_amt & address has more unique values but it comes under categorical columnswe removed\n#  both features & now we have area, brok_amt, deposit_amt & mnt_amount. \n\ncontinous_variable = continous_variable[0:1] +continous_variable[3:]\n","2f9d9e3e":"for i in continous_variable:\n    print(i, len(df[i].unique()))","233da62a":" plt.plot(df.area) # looking for abnormality ","5c536b9f":"print('shape before', df.shape)\ndf.drop(df[df['area'] > 15000].index, axis = 0, inplace = True)\nprint('shape after', df.shape)","b39cec4a":"fig, ax =  plt.subplots(2,2, figsize=(20,15) )\nsub = [(i,j)  for i in range(0,2) for j in range(0,2)]\nfor i, j in enumerate(continous_variable):\n    sns.scatterplot(ax = ax[ sub[i][0],  sub[i][1]],  data = df, x = j, y = 'rent', hue= 'bedroom', size = 'bedroom', sizes = (20,300))","e82d1970":"fig, ax = plt.subplots(1,2, figsize = (20,8), sharey = False)\nfig.tight_layout(pad= 4.0)\nfig.subplots_adjust(top=0.87)\nsns.set_style('white')\nfig.suptitle('Target Varible', size = 30)\nsns.histplot(ax= ax[0], data =df ,x = df.rent, kde =  True, bins = 15)\nax[0].set_title('Kdeplot', fontsize = 15)\nsns.boxplot(ax= ax[1], data =df ,y  = df.rent, palette='muted')\nax[1].set_title('Boxplot', fontsize = 15)\n\nfig.savefig('target.jpg', dpi = 250)","98103835":"continous_variable","ce5c1084":"just_test = df.describe().columns","459bb6ba":"only_file = [i for i in just_test  if i not in binary_variables]\n(only_file)","57934ba5":"fig, ax = plt.subplots(11,2, figsize = (20,30))\nfig.tight_layout(pad = 4.0)\nfig.subplots_adjust(top = 0.95)\nfig.suptitle('Variable  Histplo & Box Plot', size = 30)\nsns.kdeplot(ax= ax[0,0], data = df, x = 'bedroom')\nsns.boxplot(ax= ax[0,1], data = df, x = 'bedroom')\nsns.kdeplot(ax= ax[1,0], data = df, x = 'bathrooms')\nsns.boxplot(ax= ax[1,1], data = df, x = 'bathrooms')\nsns.kdeplot(ax= ax[2,0], data = df, x = 'area')\nsns.boxplot(ax= ax[2,1], data = df, x = 'area')\nsns.kdeplot(ax= ax[3,0], data = df, x = 'parking')\nsns.boxplot(ax= ax[3,1], data = df, x = 'parking')\nsns.kdeplot(ax= ax[4,0], data = df, x = 'aggDur')\nsns.boxplot(ax= ax[4,1], data = df, x = 'aggDur')\nsns.kdeplot(ax= ax[5,0], data = df, x = 'noticeDur')\nsns.boxplot(ax= ax[5,1], data = df, x = 'noticeDur')\nsns.kdeplot(ax= ax[6,0], data = df, x = 'powerbackup')\nsns.boxplot(ax= ax[6,1], data = df, x = 'powerbackup')\nsns.kdeplot(ax= ax[7,0], data = df, x = 'brok_amt')\nsns.boxplot(ax= ax[7,1], data = df, x = 'brok_amt')\nsns.kdeplot(ax= ax[8,0], data = df, x = 'deposit_amt')\nsns.boxplot(ax= ax[8,1], data = df, x = 'deposit_amt')\nsns.kdeplot(ax= ax[9,0], data = df, x = 'mnt_amt')\nsns.boxplot(ax= ax[9,1], data = df, x = 'mnt_amt')\nsns.kdeplot(ax= ax[10,0], data = df, x = 'powerbackup')\nsns.boxplot(ax= ax[10,1], data = df, x = 'powerbackup')\nfig.savefig('outlier.png')","343eb3c3":"df['end_amt'] = (df['brok_amt'] + (df['mnt_amt'] + df['rent'])*df['aggDur'] - df['deposit_amt'])\/df['aggDur']","1b3429af":"df['total_room'] = df['bedroom'] + df['pooja_room'] + df['store_room'] + df['study_room'] + df['others'] +df['servant_room']","488b4114":"plt.figure(figsize= (20,10))\n\nsns.heatmap(df.corr(), annot = True)","f1c9a10c":"df['brok_amt'] = df['brok_amt'] + 1\ndf['deposit_amt'] = df['deposit_amt'] + 1\ndf['mnt_amt'] = df['mnt_amt'] + 1\ndf['rent'] = df['rent'] + 1","648e69e4":"area_up =[]\nfor i in df.area:\n    if i < 510:\n        area_up.append(np.nan)\n    else:\n        area_up.append(i)\ndf['area'] = area_up\ndf.area.isnull().sum()\n","f4bce829":"# pd.pivot_table(df, values= 'area', columns='bedroom', aggfunc= 'mean').T\n# pd.pivot_table(df, values= 'area', columns='bedroom', aggfunc= 'median').T\n\nm1 = df['bedroom']  == 1\nm2 = df['bedroom']  == 2\nm3 = df['bedroom']  == 3\nm4 = df['bedroom']  == 4\nm5 = df['bedroom']  == 5\nm6 = df['bedroom']  == 6\ndf.loc[m1, 'area'] = df.loc[m1,'area'].fillna(df.loc[m1,'area'].median())\ndf.loc[m2, 'area'] = df.loc[m2,'area'].fillna(df.loc[m2,'area'].median())\ndf.loc[m3, 'area'] = df.loc[m3,'area'].fillna(df.loc[m3,'area'].median())\ndf.loc[m4, 'area'] = df.loc[m4,'area'].fillna(df.loc[m4,'area'].median())\ndf.loc[m5, 'area'] = df.loc[m5,'area'].fillna(df.loc[m5,'area'].median())\ndf.loc[m6, 'area'] = df.loc[m6,'area'].fillna(df.loc[m6,'area'].median())\n\ndf.area.isnull().sum()","bcd9c004":"len(df[df['area'] == 0])\n# we dont  have any mising data on area","e468fbac":"f , _ =  boxcox(df.area)\ndf['area'] = f\ndf['brok_bed_cost'] = (np.log1p(df.brok_amt\/ df.bedroom))\ndf['deposit_amt'] = (np.log(df['deposit_amt']**(1\/3)))\n","0e997182":"plt.figure(figsize= (20,10))\n\nsns.heatmap(df.corr(), annot = True)","24c95aff":"print('before',df.shape)\ndf.drop(['mnt_amt', 'parking'], axis =  True, inplace =  True)\nprint('after',df.shape)\n","f2a97b9c":"print('before',df.shape)\ndf.drop(['maintenance_amt', 'address'], axis =  True, inplace =  True)\nprint('after',df.shape)\n","83c1d3fe":"df.head(2)","4ad3aeba":"from sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, MinMaxScaler, StandardScaler\nlb, le, oe, ms,ss = LabelBinarizer(), LabelEncoder(), OrdinalEncoder, MinMaxScaler, StandardScaler","dbeb598b":"bin_trans = [i for i in binary_variables  if i not in df.describe().columns] # geting binary variables & dtype is object\nfor column in bin_trans:\n    df[column] = lb.fit_transform(df[column])","1bebc207":"## before going to further first work on locality\nprint('locality has {} unique values & it is huge'.format(len(df['locality'].unique())))","3e613fa3":"df.locality.replace('', 'Pune', inplace =  True)\n","7c20080a":"df['address'] = df.locality\ndf['sqrt_depo'] = np.sqrt(df['bedroom'])\ndf.head(2)\n","494ebb27":"area_mean = {}\n\nfor area in np.unique(df.address):\n#     print(area)\n    area_mean[area] = df.loc[df['address'] == area]['sqrt_depo'].mean()\n    \ndf['address_num'] = df['address'].map(area_mean)\ndf.head(2)","a0bbc147":"print('shape of data  before', df.shape)\ndf.drop(columns=['address', 'sqrt_depo','locality'], inplace =  True)\nprint('shape of data  after', df.shape)","1968c0c2":"for column in df.describe(include='object').columns:\n    df[column] = le.fit_transform(df[column])","e5eca795":"df.describe().T","bf0aeb16":"plt.figure( figsize =(20,12))\nsns.heatmap(df.corr(),annot = True)","8f5152c9":"fig, ax = plt.subplots(11,2, figsize = (20,30))\nfig.tight_layout(pad = 4.0)\nfig.subplots_adjust(top = 0.95)\nfig.suptitle('Variable  Histplo & Box Plot', size = 30)\nsns.kdeplot(ax= ax[0,0], data = df, x = 'bedroom')\nsns.boxplot(ax= ax[0,1], data = df, x = 'bedroom')\nsns.kdeplot(ax= ax[1,0], data = df, x = 'bathrooms')\nsns.boxplot(ax= ax[1,1], data = df, x = 'bathrooms')\nsns.kdeplot(ax= ax[2,0], data = df, x = 'area')\nsns.boxplot(ax= ax[2,1], data = df, x = 'area')\n# sns.kdeplot(ax= ax[3,0], data = df, x = 'parking')\n# sns.boxplot(ax= ax[3,1], data = df, x = 'parking')\nsns.kdeplot(ax= ax[4,0], data = df, x = 'aggDur')\nsns.boxplot(ax= ax[4,1], data = df, x = 'aggDur')\nsns.kdeplot(ax= ax[5,0], data = df, x = 'noticeDur')\nsns.boxplot(ax= ax[5,1], data = df, x = 'noticeDur')\nsns.kdeplot(ax= ax[6,0], data = df, x = 'powerbackup')\nsns.boxplot(ax= ax[6,1], data = df, x = 'powerbackup')\nsns.kdeplot(ax= ax[7,0], data = df, x = 'brok_amt')\nsns.boxplot(ax= ax[7,1], data = df, x = 'brok_amt')\nsns.kdeplot(ax= ax[8,0], data = df, x = 'deposit_amt')\nsns.boxplot(ax= ax[8,1], data = df, x = 'deposit_amt')\n# sns.kdeplot(ax= ax[9,0], data = df, x = 'mnt_amt')\n# sns.boxplot(ax= ax[9,1], data = df, x = 'mnt_amt')\nsns.kdeplot(ax= ax[10,0], data = df, x = 'powerbackup')\nsns.boxplot(ax= ax[10,1], data = df, x = 'powerbackup')\nfig.savefig('outlier.png')","2d03ac08":"plt.figure(figsize = (20,15))\nsns.heatmap(df.corr(), annot = True, cmap= 'BuPu')","1c3b3fc0":"df.drop(['bathrooms'], axis = 1, inplace = True) # highly corelated","199d6cf1":"for i in df.columns:  #looking for min & maximum\n    print(i, df[i].max(), df[i].min())","1174df54":"# filling infinity values with proper median category\ndf['end_amt'] = df['end_amt'].replace([np.inf, -np.inf], [df['end_amt'].median(),df['end_amt'].median()])\ndf['end_amt'].fillna(df['end_amt'].median(), inplace  =True)","05492376":"X = df.drop(['rent'], axis = 1)\ny = df['rent']\nfinal_train, final_test, final_train_target, final_test_target = train_test_split(X, y, test_size = 0.2)","8ab9f6fc":" X_train, X_test, y_train1, y_test1 = train_test_split(X, y, test_size = 0.2)","033ee7e7":"scaler = MinMaxScaler()\nscaler.fit(X_train)\nX_scaled = scaler.transform(X)\n\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","b039f0dc":"scores = []\n\nfor i in range(10):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n    \n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    scores.append(model.score(X_test, y_test))\n\nprint(np.mean(scores))\n# 0.6829596767515921","f9742d82":"scores = []\n# on scaled data\nfor i in range(10):\n    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.2)\n    \n    model = LinearRegression()\n    model.fit(X_train, y_train)\n    scores.append(model.score(X_test, y_test))\n\nprint(np.mean(scores))\n# 0.6829596767515921","946c8628":"scores = []\nfrom sklearn.ensemble import RandomForestRegressor\nfor i in range(10):\n    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.2)\n    \n    model = RandomForestRegressor()\n    model.fit(X_train, y_train)\n    scores.append(model.score(X_test, y_test))\n\nprint(np.mean(scores))","c1b9be0a":"scores = []\n\nfor i in range(10):\n    X_train, X_test, y_train, y_test = train_test_split(X, (y), test_size = 0.25)\n    \n    model = RandomForestRegressor(n_estimators= 100, bootstrap=True)\n    model.fit(X_train, y_train)\n    scores.append(model.score(X_test, y_test))\n    print(np.sqrt(mean_squared_error(y_test, model.predict(X_test))))\n\nprint(np.mean(scores))","32c40581":"\nimport numpy as np\n\ntarget= np.array(df['rent'])\nfeatures = df.drop('rent', axis = 1)\nfeature_list = list(features.columns)\nfeatures = np.array(features)\n\n## RANDOM FOREST - KFOLD AND MODEL \n\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestRegressor\n    \nkf = KFold(n_splits=10,random_state=42,shuffle=True)\naccuracies = []\nfor train_index, test_index in kf.split(features):\n\n    data_train   = features[train_index]\n    target_train = target[train_index]\n\n    data_test    = features[test_index]\n    target_test  = target[test_index]\n\n    rf = RandomForestRegressor(n_estimators = 10, \n                               random_state = 42, \n                               criterion = 'mse',\n                               bootstrap=True)\n    \n    rf.fit(data_train, target_train)\n\n    predictions = rf.predict(data_test)\n\n    errors = abs(predictions - target_test)\n\n    print('Mean Absolute Error:', round(np.mean(errors), 2))\n    \n    mape = 100 * (errors \/ target_test)\n    accuracy = 100 - np.mean(mape)\n    print('Accuracy:', round(accuracy, 2), '%.')\n    print('train  accuracy  ', np.sqrt(mean_squared_error(target_train, rf.predict(data_train))))\n    print('train  accuracy  ', np.sqrt(mean_squared_error(target_test, rf.predict(data_test))))\n    print('r2score train ', r2_score(target_train, rf.predict(data_train)))\n    print('r2score train ' , r2_score(target_test, rf.predict(data_test)))\n    \n    \n\n    accuracies.append(accuracy)\n\naverage_accuracy = np.mean(accuracies)\nprint('Average accuracy:', average_accuracy)","e6981229":"from pprint import pprint\nfrom sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)","41d7a496":"\n# # Use the random grid to search for best hyperparameters\n# # First create the base model to tune\n# rf = RandomForestRegressor()\n# # Random search of parameters, using 3 fold cross validation, \n# # search across 100 different combinations, and use all available cores\n# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# # Fit the random search model\n# rf_random.fit(data_train, target_train)\n\n# print(rf_random.best_params_)","21b831eb":"\nimport numpy as np\n\ntarget= np.array(df['rent'])\nfeatures = df.drop('rent', axis = 1)\nfeature_list = list(features.columns)\nfeatures = np.array(features)\n\n## RANDOM FOREST - KFOLD AND MODEL \n\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestRegressor\n    \nkf = KFold(n_splits=10,random_state=42,shuffle=True)\naccuracies = []\nfor train_index, test_index in kf.split(features):\n\n    data_train   = features[train_index]\n    target_train = target[train_index]\n\n    data_test    = features[test_index]\n    target_test  = target[test_index]\n\n    rf = RandomForestRegressor(n_estimators = 2000, \n                               min_samples_leaf= 2,\n                               min_samples_split= 5,\n                               max_features= 'auto',\n                               max_depth= 50,\n                               random_state = 42, \n                               criterion = 'mse',\n                               bootstrap=True) \n    \n    rf.fit(data_train, target_train)\n\n    predictions = rf.predict(data_test)\n\n    errors = abs(predictions - target_test)\n\n    print('Mean Absolute Error:', round(np.mean(errors), 2))\n    \n    mape = 100 * (errors \/ target_test)\n    accuracy = 100 - np.mean(mape)\n    print('Accuracy:', round(accuracy, 2), '%.')\n    print('train  accuracy  ', np.sqrt(mean_squared_error(target_train, rf.predict(data_train))))\n    print('train  accuracy  ', np.sqrt(mean_squared_error(target_test, rf.predict(data_test))))\n    print('r2score train ', r2_score(target_train, rf.predict(data_train)))\n    print('r2score test ' , r2_score(target_test, rf.predict(data_test)))\n    \n    \n\n    accuracies.append(accuracy)\n\naverage_accuracy = np.mean(accuracies)\nprint('Average accuracy:', average_accuracy)","ebd27f86":"rf = RandomForestRegressor(n_estimators = 2000, \n                               min_samples_leaf= 2,\n                               min_samples_split= 5,\n                               max_features= 'auto',\n                               max_depth= 50,\n                               random_state = 42, \n                               criterion = 'mse',\n                               bootstrap=True)","281e76ac":"model_final = rf.fit(X,y)\n# scaled_model = rf.fit(X_scaled, y)","8f88f717":"model_final.score(X,y)","424e8367":"model.score(data_test, target_test)","d66e6f51":"np.sqrt(mean_squared_error(target_test, model.predict(data_test)))","33537a26":"test_data = pd.read_csv('test_pre.cvs')\ntest_data.head()","bbbf63a6":"true_test= test_data.drop('rent', axis = 1)\ntrue_value = test_data.rent\n","c7794fc0":"from sklearn.metrics import mean_squared_log_error\nprint(r2_score((true_value), (model_final.predict(true_test))))\nprint(np.sqrt(mean_squared_error((true_value), (model_final.predict(true_test)))))\nprint(np.sqrt(mean_squared_log_error((true_value), (model_final.predict(true_test)))))\nprint((mean_absolute_error((true_value), (model_final.predict(true_test)))))","1796d07a":"model_final","a7516efc":"import pickle\npickl = {'model': model_final}\npickle.dump( pickl, open( 'model_file' + \".p\", \"wb\" ) )","076a8e35":"data_input= list(X.iloc[0])","be284f93":"### Parameter Tuning","61e20c9d":"### Target Varible","10ce9edb":"### Start with Binary Variable Visualisation.\n","1964d3e8":"**creating zero variable to 1** \n\n\nAlso every home has a greater than 510 so we are going replace with bedroom_area median value","809563e9":"**output of base map**\n![map](map.jpg)","2a186bae":"**Cleaning messy area & geting proper locality name from this**","8787d8f7":"Convert all variable into following categories\n1. Binary variables data\n2. Ordinal Varibles \n3. continous  data\n4. target Variable","c1c82dd2":"Based on data following colums has most outliers & mostly left skewed data:\n1. area\n2. parking\n3. brok_amt\n4. seposit_amt\n5. mnt_amt\n\n& Little bit features engineering is needed on \n1. Bedroom\n2. Bathrooms\n3. aggDur\n4. NoticeDur\n\n\n","2714dae0":"## Model Building","d36356b3":"**Area**","cdcc4e99":"### Continous Variables","2f852f27":"### Outlier Treatment","0cc24dbd":"**Scaled data is hace better score than orinigal**\n\n\n### RANDOM FOREST - KFOLD AND MODEL ","d7b04839":"**From above visualization we  have concluded following points for higher rent.**\n1. As number of bedroom increased rent also has incresed\n2. As per furnishing concern Furinished has higher pay than unfurnished\n3. Talking about direction if your home has east face your rent is higher as compare with south.\n4. As floor number goes higher Rent Also increased\n5. Wood Floor design(colors ) has higher pay than marble\n6. As per powerbackup facility, if you have full backup == 2  then you need to pay more rent.","109ca481":"Area has all positive data after apply log, sqrt  tranformation I found that we have positive data we can use boxcox transformation here","67a00574":"**Based on above plots I found that if Facility is available then rent for home is always higher**","df51d337":"{'bootstrap': True,\n 'max_depth': 40,\n 'max_features': 'auto',\n 'min_samples_leaf': 4,\n 'min_samples_split': 2,\n 'n_estimators': 600}","58ef6116":"### Training with best parameters","4c036b5f":"# Rental Price Analysis of India's IT Capital - Pune, MH, IND","b02c28db":"## Exploratory Data Analysis ","896bc77e":"### Ordinal Variables ","7abf101b":"## Feature_selection ","384d41f7":"### Linear regression","9cc781c3":"Based on data shape of data data has around 10884 sample with 30 features. ","0eb52254":"#### pickle","775eab72":"### Tuning preparation","04292bda":"Fitting 3 folds for each of 10 candidates, totalling 30 fits\n\n\n**output {'n_estimators': 2000, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 50, 'bootstrap': True}**","d40c8e58":"## Few Feature Engineering on Target variables","3b14e3c0":"## Feature encoding ","8d672660":"**Categorical Columns using labelencoder**","36f97c1e":"**Know more about dataset** ","7dcc30a6":"**Now we have a proper dataset copy so we can work on that** ","3c6d6ca3":"## Exploring dataset\n### Import Libraries","0a20e256":"## Testing with new donwloaded data you can check my github  repo for this data\n#### Link :[github_test](https:\/\/github.com\/senhorinfinito\/rental_price_analysis)","674e0ea5":"**This Summary.csv is a file of locality & based on that we are going to fetch latitude & logitude for those columns.**\n\n\nI used goecoder library to get latitute & longotide for all locations as follows. & stored into a file **all_locations.csv** Which I will import here\n\n\n\n**If you wanted to do all geoding please uncomment below code**","c04ab4b2":"### Categorical Variables\n","35e45448":"### RandomForest Regressor","cc174fd1":"First we will look for uniques prices then we will check for missing & random  values \n","1ffba58b":"**Problem Defination** \n\nPune is IT capital of India  & a city with 10,089,916 population. Pune is one of the larger employment producing city. So Most of peoples are living in rental houses & appartment.  so I'm going to predict the housing rental prices of areas in city.","62dccf41":"you can check my dataset is here :\n\n1. Rental Price Data - [kaggle](https:\/\/www.kaggle.com\/anantsakhare\/rental-price-of-indias-it-capital-pune-mh-ind) \n2. Housing data Scrapper - [github](https:\/\/github.com\/senhorinfinito\/scrappers\/blob\/main\/rental_analysis\/train.csv)","ab9bf8de":"We have mostly right skewed data in all continous variables ","d552dd31":"1. Outliers treatment\n2. Feature transformation\n3. Feature Scaling","b825de83":"## Feature Engineering "}}