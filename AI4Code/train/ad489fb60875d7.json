{"cell_type":{"26a3f47b":"code","b4fafdac":"code","7feec8c9":"code","d4c0b58c":"code","00c83573":"code","7834b6a1":"code","7f91aee7":"code","d97c9f95":"code","13556698":"code","36859a7a":"code","c3ccfff5":"code","990947c4":"code","e9687f72":"code","5c7225b8":"code","bc30f353":"code","11262f66":"code","bea1bdd1":"code","86037324":"markdown","91615785":"markdown"},"source":{"26a3f47b":"import numpy as np \nimport pandas as pd \nimport os\n\nfrom collections import defaultdict\nfrom glob import glob\nfrom random import choice, sample\nfrom keras.preprocessing import image\nimport cv2\nfrom tqdm import tqdm_notebook\nimport numpy as np\nimport pandas as pd\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract\nfrom keras.models import Model\nfrom keras.optimizers import Adam","b4fafdac":"!pip install git+https:\/\/github.com\/rcmalli\/keras-vggface.git","7feec8c9":"from keras_vggface.utils import preprocess_input\nfrom keras_vggface.vggface import VGGFace","d4c0b58c":"train_file_path = \"..\/input\/train_relationships.csv\"\ntrain_folders_path = \"..\/input\/train\/\"","00c83573":"# train_dirs = os.listdir('..\/input\/train\/')\n# len(train_dirs), train_dirs","7834b6a1":"val_famillies_list = [\"F07\", \"F08\", \"F09\"]\n# val_famillies_list = [\"F09\"]","7f91aee7":"%%time\nall_images = glob(train_folders_path + \"*\/*\/*.jpg\")","d97c9f95":"def get_train_val(family_name):\n    # Get val_person_image_map\n    val_famillies = family_name\n    train_images = [x for x in all_images if val_famillies not in x]\n    val_images = [x for x in all_images if val_famillies in x]\n\n    train_person_to_images_map = defaultdict(list)\n\n    ppl = [x.split(\"\/\")[-3] + \"\/\" + x.split(\"\/\")[-2] for x in all_images]\n\n    for x in train_images:\n        train_person_to_images_map[x.split(\"\/\")[-3] + \"\/\" + x.split(\"\/\")[-2]].append(x)\n\n    val_person_to_images_map = defaultdict(list)\n\n    for x in val_images:\n        val_person_to_images_map[x.split(\"\/\")[-3] + \"\/\" + x.split(\"\/\")[-2]].append(x)\n        \n    # Get the train and val dataset\n    relationships = pd.read_csv(train_file_path)\n    relationships = list(zip(relationships.p1.values, relationships.p2.values))\n    relationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]\n\n    train = [x for x in relationships if val_famillies not in x[0]]\n    val = [x for x in relationships if val_famillies in x[0]]\n    \n    return train, val, train_person_to_images_map, val_person_to_images_map","13556698":"def read_img(path):\n    img = image.load_img(path, target_size=(197, 197))\n    img = np.array(img).astype(np.float)\n    return preprocess_input(img, version=2)\n\ndef gen(list_tuples, person_to_images_map, batch_size=16):\n    ppl = list(person_to_images_map.keys())\n    while True:\n        batch_tuples = sample(list_tuples, batch_size \/\/ 2)\n        labels = [1] * len(batch_tuples)\n        while len(batch_tuples) < batch_size:\n            p1 = choice(ppl)\n            p2 = choice(ppl)\n\n            if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n                batch_tuples.append((p1, p2))\n                labels.append(0)\n\n        for x in batch_tuples:\n            if not len(person_to_images_map[x[0]]):\n                print(x[0])\n\n        X1 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n        X1 = np.array([read_img(x) for x in X1])\n\n        X2 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n        X2 = np.array([read_img(x) for x in X2])\n\n        yield [X1, X2], labels\n\nimport tensorflow as tf\nfrom keras import backend as K\ndef focal_loss(gamma=2., alpha=.25):\n    def focal_loss_fixed(y_true, y_pred):\n        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n        return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(K.epsilon()+pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n    return focal_loss_fixed\n\n\ndef baseline_model():\n    input_1 = Input(shape=(197, 197, 3))\n    input_2 = Input(shape=(197, 197, 3))\n\n    base_model = VGGFace(model='resnet50', include_top=False)\n\n    for x in base_model.layers[:-3]:\n        x.trainable = True\n    for x in base_model.layers[-3:]:\n        x.trainable=False\n\n    x1 = base_model(input_1)\n    x2 = base_model(input_2)\n\n#     x1_ = Reshape(target_shape=(7*7, 2048))(x1)\n#     x2_ = Reshape(target_shape=(7*7, 2048))(x2)\n#     #\n#     x_dot = Dot(axes=[2, 2], normalize=True)([x1_, x2_])\n#     x_dot = Flatten()(x_dot)\n\n    x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])\n    x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n\n    x3 = Subtract()([x1, x2])\n    x3 = Multiply()([x3, x3])\n\n    x1_ = Multiply()([x1, x1])\n    x2_ = Multiply()([x2, x2])\n    x4 = Subtract()([x1_, x2_])\n    x = Concatenate(axis=-1)([x4, x3])\n\n    x = Dense(100, activation=\"relu\")(x)\n    x = Dropout(0.01)(x)\n    out = Dense(1, activation=\"sigmoid\")(x)\n\n    model = Model([input_1, input_2], out)\n    \n    # loss=\"binary_crossentropy\"\n    model.compile(loss=[focal_loss(alpha=.25, gamma=2)], \n                  metrics=['acc'], \n                  optimizer=Adam(0.00003))\n\n    model.summary()\n\n    return model","36859a7a":"model = baseline_model()","c3ccfff5":"n_val_famillies_list = len(val_famillies_list)","990947c4":"n_val_famillies_list","e9687f72":"for i in tqdm_notebook(range(n_val_famillies_list)):\n    train, val, train_person_to_images_map, val_person_to_images_map = get_train_val(val_famillies_list[i])\n    file_path = f\"vgg_face_{i}.h5\"\n    checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n    reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.5, patience=10, verbose=1)\n    es = EarlyStopping(monitor=\"val_acc\", min_delta = 0.001, patience=30, verbose=1)\n    callbacks_list = [checkpoint, reduce_on_plateau, es]\n\n    history = model.fit_generator(gen(train, train_person_to_images_map, batch_size=32), \n                                  use_multiprocessing=True,\n                                  validation_data=gen(val, val_person_to_images_map, batch_size=32), \n                                  epochs=300, verbose=1,\n                                  workers=4, callbacks=callbacks_list, \n                                  steps_per_epoch=400, validation_steps=200)","5c7225b8":"test_path = \"..\/input\/test\/\"\n\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\n\ndef chunker(seq, size=32):\n    return (seq[pos:pos + size] for pos in range(0, len(seq), size))","bc30f353":"preds_for_sub = np.zeros(submission.shape[0])\nfor i in tqdm_notebook(range(n_val_famillies_list)):\n    file_path = f\"vgg_face_{i}.h5\"\n    model.load_weights(file_path)\n    # Get the predictions\n    predictions = []\n\n    for batch in tqdm_notebook(chunker(submission.img_pair.values)):\n        X1 = [x.split(\"-\")[0] for x in batch]\n        X1 = np.array([read_img(test_path + x) for x in X1])\n\n        X2 = [x.split(\"-\")[1] for x in batch]\n        X2 = np.array([read_img(test_path + x) for x in X2])\n\n        pred = model.predict([X1, X2]).ravel().tolist()\n        predictions += pred\n    preds_for_sub += np.array(predictions) \/ n_val_famillies_list","11262f66":"submission['is_related'] = preds_for_sub\nsubmission.to_csv(\"vgg_face.csv\", index=False)","bea1bdd1":"submission","86037324":"## 1 Train","91615785":"## 2 Inference"}}