{"cell_type":{"385adc37":"code","ab961891":"code","56186bdb":"code","e87d8194":"code","57f393c2":"code","e78acd88":"code","a07c92a1":"code","dd415723":"code","31733beb":"code","2939b5dd":"code","cc76d1ac":"code","7d92d3c3":"code","39ddacce":"code","5871b13e":"code","45a48408":"code","5d858314":"code","804f920e":"code","a723c71e":"code","f4f80893":"code","b789301d":"code","f6110659":"code","bc44d1a3":"code","bb9e03fe":"code","35784aba":"code","f716a92f":"code","bbc6470b":"code","99880c51":"markdown","e7713fe4":"markdown","c0591a47":"markdown","42083bc9":"markdown","006d4898":"markdown","47e0e023":"markdown","0234fdcf":"markdown","d39aebeb":"markdown","4376a2d1":"markdown","a829f2fd":"markdown","7aaf61b8":"markdown","87f4f064":"markdown","99b549ca":"markdown","9e3bc01f":"markdown","c369da69":"markdown","56071b03":"markdown","49a0a249":"markdown","5403f675":"markdown","1b632956":"markdown","37b558a0":"markdown","8ee8bd7c":"markdown","eb90553d":"markdown","1f604626":"markdown","afa704ad":"markdown","6ee276bf":"markdown","d1c0150b":"markdown","fdeffe5a":"markdown","2d40e613":"markdown","62b60ebb":"markdown","438da738":"markdown","2cd787f2":"markdown","2a392720":"markdown","4fd61eb5":"markdown","c66a4a8e":"markdown","4e7be285":"markdown"},"source":{"385adc37":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set()\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","ab961891":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n#make a copy so your original data is not touched\ntrain = train_data.copy()\ntest = test_data.copy()\ntrain.shape\ny_train = train['Survived']\n\n#We won't need passenger ID or ticket price for the model! They do not provide much insight on the training.\nId = pd.DataFrame(test['PassengerId'])\ntrain.drop(['PassengerId'], axis = 1, inplace=True)\ntest.drop(['PassengerId'], axis = 1, inplace=True)\ntrain.drop(['Survived'], axis = 1, inplace=True)\ntrain.drop(['Ticket'], axis = 1, inplace=True)\ntest.drop(['Ticket'], axis = 1, inplace=True)","56186bdb":"train.head()","e87d8194":"sns.heatmap(train.isnull(),yticklabels=False,cbar='BuPu')\n","57f393c2":"train.isnull().sum().sort_values(ascending=False)[0:20]\n# we can see that cabin is midding a lot of values, and age is tooi!","e78acd88":"\n\n#clean the train data\nfor i in list(train.columns):\n    dtype = train[i].dtype\n    values = 0\n    if(dtype == float or dtype == int):\n        method = 'mean'\n    else:\n        method = 'mode'\n    if(train[i].notnull().sum() \/ 891 <= .5):\n        train.drop(i, axis = 1, inplace=True)\n    elif method == 'mean':\n        train[i]=train[i].fillna(train[i].mean())\n\n    else:\n        train[i]=train[i].fillna(train[i].mode()[0])\n\n# WE CAN DO THIS FOR THE TEST SET TOO!\n\n#clean the test data\nfor i in list(test.columns):\n    dtype = test[i].dtype\n    values = 0\n    if(dtype == float or dtype == int):\n        method = 'mean'\n    else:\n        method = 'mode'\n    if(test[i].notnull().sum() \/ 418 <= .5):\n        test.drop(i, axis = 1, inplace=True)\n    elif method == 'mean':\n        test[i]=test[i].fillna(test[i].mean())\n\n    else:\n        test[i]=test[i].fillna(test[i].mode()[0])\n\n\n","a07c92a1":"sns.heatmap(train.isnull(),yticklabels=False,cbar='BuPu')\n","dd415723":"#TITLE\n\ntrain_test_data = [train, test] # combining train and test dataset\n\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n\ntitle_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 1, \n                 \"Master\": 0, \"Dr\": 1, \"Rev\": 0, \"Col\": 0, \"Major\": 0, \"Mlle\": 1,\"Countess\": 1,\n                 \"Ms\": 1, \"Lady\": 1, \"Jonkheer\": 1, \"Don\": 0, \"Dona\" : 1, \"Mme\": 0,\"Capt\": 0,\"Sir\": 0 }\nfor dataset in train_test_data:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    \n","31733beb":"    \nsex_mapping = {\"male\": 0, \"female\":1}\nfor dataset in train_test_data:\n    dataset['Sex'] = dataset['Sex'].map(sex_mapping)\n","2939b5dd":"Pclass1 = train_data[train_data['Pclass'] == 1]['Embarked'].value_counts()\nPclass2 = train_data[train_data['Pclass'] == 2]['Embarked'].value_counts()\nPclass3 = train_data[train_data['Pclass'] == 3]['Embarked'].value_counts()\n\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class','2nd class', '3rd class']\ndf.plot(kind='bar',stacked=True, figsize=(10,5))","cc76d1ac":"for data in train_test_data:\n    data['Embarked'] = data['Embarked'].fillna(\"S\")\n    \nembarked_mapping = {\"S\": 0, \"C\": 1, \"Q\": 2}\nfor dataset in train_test_data:\n    dataset['Embarked'] = dataset['Embarked'].map(embarked_mapping)","7d92d3c3":"train[\"FamilySize\"] = train['SibSp'] + train['Parch'] + 1\ntest[\"FamilySize\"] = test['SibSp'] + test['Parch'] + 1","39ddacce":"sns.heatmap(train.corr(),cbar='plasma')\n","5871b13e":"train.drop(['Name'], axis = 1, inplace=True)\ntest.drop(['Name'], axis = 1, inplace=True)","45a48408":"train.head()","5d858314":"test.head()","804f920e":"#imports\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, Flatten, Conv2D,MaxPool2D\n\nimport keras","a723c71e":"continuous = ['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'FamilySize']\n\nscaler = StandardScaler()\n\nfor var in continuous:\n    train[var] = train[var].astype('float64')\n    train[var] = scaler.fit_transform(train[var].values.reshape(-1, 1))\nfor var in continuous:\n    test[var] = test[var].astype('float64')\n    test[var] = scaler.fit_transform(test[var].values.reshape(-1, 1))","f4f80893":"train.describe(include='all').T\n","b789301d":"import tensorflow as tf\ntf.keras.optimizers.Adam(\n    learning_rate=0.003, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n    name='Adam', \n)\n#creating a model with sequential columns\nmodel = Sequential()\n\n#flattens the data into a 1d array\nmodel.add(Flatten())\n#creates the first latyer with the input dimanetion. \n\nmodel.add(Dense(32, input_dim=train.shape[1],kernel_initializer = 'uniform', activation='relu'))\n#next layer with 32 dense nodes\nmodel.add(Dense(32, kernel_initializer = 'uniform', activation = 'relu'))\n#drops 0.4 of the values from the next layer, so it does not over fit!\n\nmodel.add(Dropout(0.4))\n#last layer is initiated here\nmodel.add(Dense(32,kernel_initializer = 'uniform', activation = 'relu'))\n\n# create output layer\n    # Feel free to experiment with the activation functions and the optimizers\nmodel.add(Dense(1, activation='sigmoid'))  # output layer\n    \nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","f6110659":"history = model.fit(train, y_train, epochs=20, batch_size=50, validation_split = 0.2)\n\n#val_acc = np.mean(training.history['val_acc'])\n#print(\"\\n%s: %.2f%%\" % ('val_acc', val_acc*100))","bc44d1a3":"print(model.summary())\n","bb9e03fe":"scores = model.evaluate(train, y_train, batch_size=32)","35784aba":"y_pred = model.predict(test)\n\ny_final = (y_pred > 0.5).astype(int).reshape(test.shape[0])\n\noutput = pd.DataFrame({'PassengerId': test_data['PassengerId'], 'Survived': y_final})\noutput.to_csv('prediction-ann.csv', index=False)","f716a92f":"loss_train = history.history['loss']\nloss_val = history.history['val_loss']\nepochs = range(1,21)\nplt.plot(epochs, loss_train, 'g', label='Training loss')\nplt.plot(epochs, loss_val, 'b', label='validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","bbc6470b":"loss_train = history.history['accuracy']\nloss_val = history.history['val_accuracy']\nepochs = range(1,21)\nplt.plot(epochs, loss_train, 'g', label='Training accuracy')\nplt.plot(epochs, loss_val, 'b', label='validation accuracy')\nplt.title('Training and Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","99880c51":"**Embarked**","e7713fe4":"# **Data Cleaning**","c0591a47":"**Let's fit our model!**","42083bc9":"We can make male and female into catagorical variables such as 0 and 1","006d4898":"# **NN Analyasis**","47e0e023":"**Title**","0234fdcf":"# **Neural Network**","d39aebeb":"**To whom does this notebook appeal to?**\n* If you are just starting the Titanic Competition and want to learn how to implement Neural Networks, I suggest you start here!","4376a2d1":"**Lets see how many null values there are! We need to fill out these values later.**","a829f2fd":"# **PreProcessing**","7aaf61b8":"# **Correlations**","87f4f064":"# Reading In the Data","99b549ca":"![](https:\/\/faithmag.com\/sites\/default\/files\/styles\/article_full\/public\/2018-09\/titanic2.jpg?h=6521bd5e&itok=H8td6QVv)","9e3bc01f":"# **Feature Engineering**","c369da69":"Lets create our model! make sure to add the dropout value, so that our model does not over fit.","56071b03":"**Now that our data looks good, lets get ready to build our models!**","49a0a249":"**Sex**\n","5403f675":"We can assign each embarked value to a numberical value for training later!","1b632956":"**Let's take a look at our data!**","37b558a0":"**We see that we have some midding values from age, and a ton missing from cabin**","8ee8bd7c":"**This is an awesome function I created that preprocesses the data. It does thes following**\n\n* Fills in null values based on mean or mode\n* Drops columns that are missing 50 percent of the data\n\n*You guys are free to copy this for loop for your own projects!*[](http:\/\/)","eb90553d":"We should scale the values in the data, so that the neural network can train better!","1f604626":"Lets take out the Mr, Miss, etc from the name section, and create a new column names title!","afa704ad":"**Evaluate the model!**","6ee276bf":"**Thank you for viewing this notebook!**\n\nHave a great day!","d1c0150b":"**As we can see, all of the missing values are gone!**","fdeffe5a":"**Basic Imports**","2d40e613":"**Family Size**","62b60ebb":"We will be first importing the data, and creating copies. I recommend this because it give you access to a clean untouched base file.\nNext, we will be dropping some things that we don't need such as passenger Id and Ticket price","438da738":"# **Introduction**","2cd787f2":"**Create Predictions**","2a392720":"Parch ( Parent & child ) and Sibsp( Sibling & Spouse ) are both contributing factors to family size, so lets make a new column called family size, and drop the other ones.","4fd61eb5":"***Howdy, Welcome to the Titanic***","c66a4a8e":"We can plot the loss, and val_loss & acc and val_acc\nThe main purpose of this is to make sure the model follows a good trend, and to make sure that you are not overfitteing your model.","4e7be285":"Lets test to see if there is any correlation with Pclass and Embarked "}}