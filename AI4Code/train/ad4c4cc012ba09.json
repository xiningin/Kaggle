{"cell_type":{"a7c95ff8":"code","85622900":"code","9f35860f":"code","583ea99e":"code","9d38391f":"code","bedb51d8":"code","58a3863b":"code","a97a1f34":"code","7d3cea5a":"code","4a078570":"code","7b9e6b24":"code","aeb4662a":"code","fd323e76":"code","e6ff6bf7":"code","a3df440c":"code","ba68b606":"code","6b080a29":"code","210de245":"markdown"},"source":{"a7c95ff8":"pip install feature_engine","85622900":"import warnings\nwarnings.filterwarnings('ignore')\n\n# functions to preprocess and viz data\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\n# function to process data\nfrom sklearn.metrics import confusion_matrix, make_scorer, f1_score\nfrom scipy.optimize import differential_evolution\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom sklearn import metrics\nfrom feature_engine.encoding import MeanEncoder, RareLabelEncoder, CountFrequencyEncoder, OneHotEncoder\nfrom feature_engine.selection import DropFeatures\nfrom feature_engine.imputation import AddMissingIndicator\nfrom feature_engine.imputation import ArbitraryNumberImputer, MeanMedianImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.mixture import GaussianMixture\n\n\n# models to import \nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.ensemble import StackingClassifier, VotingClassifier\nfrom lightgbm import LGBMClassifier# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9f35860f":"train = pd.read_csv('\/kaggle\/input\/porto-seguro-data-challenge\/train.csv', na_values = -999)\ntest = pd.read_csv('\/kaggle\/input\/porto-seguro-data-challenge\/test.csv', na_values = -999)","583ea99e":"# fun\u00e7\u00f5es criadas para resolver o problema do neg\u00f3cio\n\ndef optimal_cutoff(y_target, y_predict_prob, only_fun = True, random_state = 199):\n    \n    optimization = differential_evolution(lambda c: (-f1_score(y_target, (y_predict_prob > c[0]))),\n                                          [(0, 0.5)], tol = 0.0000001, seed = random_state)\n    if only_fun == False:\n        return -optimization['fun'], optimization['x']\n    else:\n        return 'F1', float(-optimization['fun']), True\n\ndef train_get_score_cv(x_train, y_train, x_test, models, k, pipeline_engine, random_state = 199):\n\n    kf = StratifiedKFold(n_splits=k, shuffle = True, random_state = random_state)\n    result = np.zeros((len(models), 4))\n    result_pred_test = []\n    result_pred_train = []\n    \n    for i,model in enumerate(models.keys()):\n        \n        pred = []\n        pred_test = []\n        metric = []\n        cut = []\n        learner = models[model]\n        for fold, (id_train, id_test) in enumerate(kf.split(x_train, y_train)):\n\n            Xt = x_train.iloc[id_train]; yt = y_train.iloc[id_train]\n            Xv = x_train.iloc[id_test]; yv = y_train.iloc[id_test]\n            xtest = x_test.copy()\n            if pipeline_engine != None:\n                preprocess_data_cv = pipeline_engine.fit(Xt, yt)\n    \n                Xt = preprocess_data_cv.transform(Xt)\n                Xv = preprocess_data_cv.transform(Xv)\n                xtest = preprocess_data_cv.transform(x_test)\n                \n            learner.fit(Xt, yt.values)\n            prediction = pd.Series(learner.predict_proba(Xv)[:, -1], index=Xv.index)   \n            prediction_test = pd.Series(learner.predict_proba(xtest)[:, -1], index=x_test.index, \n                                       name=\"fold_\" + str(fold))   \n            pred.append(prediction)\n            pred_test.append(prediction_test)\n            business_metric_opt, cutoff = optimal_cutoff(yv, prediction, only_fun= False)\n            metric.append(business_metric_opt)\n            cut.append(cutoff[0])\n            \n          \n            print(f'Best f1 score: {business_metric_opt} ----  cutoff: {cutoff }')\n                                 \n        pred = pd.concat(pred)\n        pred_test = pd.concat(pred_test, axis =1).mean(axis=1)\n        AUC = metrics.roc_auc_score(y_train.loc[pred.index], pred)\n        log_loss = metrics.log_loss(y_train.loc[pred.index], pred)\n        business_metric_opt = np.mean(metric)\n        cutoff = np.mean(cut)\n        \n        result[i] = [ AUC, log_loss, business_metric_opt, cutoff]\n        result_pred_train.append(pd.Series(pred , name = model  ) )\n        result_pred_test.append(pd.Series(pred_test , name = model  ) )\n    result = pd.DataFrame(result, columns=[\"auc\", \"log_loss\", \"f1-score\", \"cutoff\"],index = list(models.keys()))\n    result_pred_train = pd.concat(result_pred_train, axis =1)\n    result_pred_test = pd.concat(result_pred_test, axis =1)\n\n    return result, result_pred_train, result_pred_test","9d38391f":"train = train.reset_index().set_index('id')\ntest = test.reset_index().set_index('id')\nrandom_state = 199\ny_train = train[\"y\"].copy()\nx_train   = train.drop(\"y\",axis=1).copy()","bedb51d8":"x_train['missing'] = (x_train.isna()).sum(axis=1).astype(float)\ntest['missing'] = (test.isna()).sum(axis=1).astype(float)","58a3863b":"meta = pd.read_csv('..\/input\/porto-seguro-data-challenge\/metadata.csv')\ncat_nom = [ 'var1', 'var2', 'var7', 'var8', 'var9', 'var10', 'var14', 'var15',\n       'var16', 'var17', 'var18', 'var20', 'var22', 'var23', 'var28', 'var29',\n       'var30',  'var33', 'var34', 'var37', 'var39',\n       'var41', 'var5', 'var6', 'var11', 'var57']\ncat_ord = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==\"Qualitativo ordinal\")].iloc[:,0]]\ncat_nom.extend(cat_ord)\ncat_to_engine = ['var3', 'var4', 'var5', 'var6', 'var11', 'var12', 'var13', 'var19',\n       'var21', 'var35']\ncuant_conti = ['var55', 'var56', 'var58', 'var59',\n  'var61', 'var62', 'var63', 'var64']\nto_drop = ['var65', 'var66', 'var19', 'var31', 'var36', 'var68', 'var38']","a97a1f34":"addBinary_imputer = AddMissingIndicator()\ndrop_Features = DropFeatures(features_to_drop = to_drop)\nmedian_Imputer = MeanMedianImputer(imputation_method='median', variables = cuant_conti)\narbitrary_imputer = ArbitraryNumberImputer(arbitrary_number=-999)\nrare_encoder = RareLabelEncoder(tol=0.02, n_categories=2, variables=cat_nom,\n                           replace_with=-999, ignore_format = True)\nmean_encoder = MeanEncoder(variables=cat_nom, ignore_format = True)\n\npipe1 = Pipeline([('indicator', addBinary_imputer),\n    ('dropFeatures', drop_Features),\n                  ('ReplaceNa', arbitrary_imputer),\n                  ('RareLabelEncoder1', rare_encoder),\n                  ('MeanEncoder', mean_encoder)])\naddBinary_imputer = AddMissingIndicator()\narbitrary_imputer = ArbitraryNumberImputer(arbitrary_number=-999)\nonehot_encoder = CountFrequencyEncoder(variables=cat_nom, ignore_format = True)\nrare_encoder2 = RareLabelEncoder(tol=0.02, n_categories=2, variables=cat_nom,\n                           replace_with='-999', ignore_format = True)\n\npipe2 = Pipeline([('ReplaceNa', arbitrary_imputer),\n                  ('dropFeatures', drop_Features),\n                 ('RareLabelEncoder2', rare_encoder2),\n                 ('OneHotEncoder', onehot_encoder)])","7d3cea5a":"cat_var = [ 'var1', 'var2', 'var7', 'var8', 'var9', 'var10', 'var14', 'var15',\n       'var16', 'var17', 'var18', 'var20', 'var22', 'var23', 'var28', 'var29',\n       'var30',  'var33', 'var34', 'var37', 'var39',\n       'var41', 'var5', 'var6', 'var11', 'var57']\ncat_ord = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==\"Qualitativo ordinal\")].iloc[:,0]]\ncat_var.extend(cat_ord)\n\nnum_dis = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==\"Quantitativo discreto\")].iloc[:,0]]\nnum_con = [x for x in meta.iloc[1:-1, :].loc[(meta.iloc[:,1]==\"Quantitativo continua\")].iloc[:,0]]\n\nnum_cols = num_dis + num_con\nto_drop = ['var65', 'var66', 'var19', 'var31', 'var36', 'var68', 'var38']\n           \naddBinary_imputer = AddMissingIndicator()\nmedian_Imputer = MeanMedianImputer(imputation_method='median', variables = num_cols)\narbitrary_imputer = ArbitraryNumberImputer(arbitrary_number=-999)\narbitrary_imputer_zero = ArbitraryNumberImputer(arbitrary_number=0.22)\nrare_encoder = RareLabelEncoder(tol=0.02, n_categories=2, variables=cat_var,\n                           replace_with=-999, ignore_format = True)\nmean_encoder = MeanEncoder(variables=cat_var, ignore_format = True)\ndrop_Features = DropFeatures(features_to_drop = to_drop)\n\n\npipe1 = Pipeline([('indicator', addBinary_imputer),\n                  ('median_imputer', median_Imputer),\n                  ('ReplaceNa', arbitrary_imputer),\n                  ('RareLabelEncoder1', rare_encoder),\n                  ('MeanEncoder', mean_encoder),\n                  ('arbitrary_imputer_zero', arbitrary_imputer_zero),\n                 ('dropFeatures', drop_Features)])","4a078570":"catboost_hyper = {'num_boost_round': 882, 'od_wait': 72, 'bagging_temperature': 0.006893667872463837,\n                  'l2_leaf_reg': 0.3388299396217467, 'learning_rate': 0.02805931739307832, 'max_depth': 4, \n                  'min_data_in_leaf': 24, 'penalties_coefficient': 4.773745911734792, 'random_state': random_state, 'verbose' : 0}\n\nlgbm_hyper = {'n_estimators': 825, 'num_leaves': 12, 'min_child_samples': 24,\n              'learning_rate': 0.018084108016008578, 'subsample': 1.0, 'max_bin': 2980, 'random_state': random_state,\n              'colsample_bytree': 0.5826048866276531, 'reg_alpha': 0.0009765625, 'reg_lambda': 0.4226195455701934}\n\nxgboost_hyper = {'lambda': 0.002318366246534739, 'alpha': 1.2046450112905143, 'colsample_bytree': 0.46773388377212743, \n                 'subsample': 0.9312814033013292, 'learning_rate': 0.002210154248998227, 'n_estimators': 4926, 'max_depth': 5,\n                 'min_child_weight': 35.708577783990094, 'random_state': random_state}","7b9e6b24":"estimators = [\n     ('Light_gbm1',  Pipeline( [\n         ('pipe1', pipe1),\n         ('lgbm',LGBMClassifier(**lgbm_hyper))\n     ]) ),\n    ('Light_gbm2',  Pipeline( [\n         ('pipe2', pipe2),\n         ('lgbm',LGBMClassifier(**lgbm_hyper))\n     ]) ),\n    ('xgboost1',  Pipeline( [\n         ('pipe1', pipe1),\n         ('lgbm',XGBClassifier(**xgboost_hyper))\n     ]) ),\n    ('catboost1',  Pipeline( [\n         ('pipe1', pipe1),\n         ('lgbm',CatBoostClassifier(**catboost_hyper))\n     ]) ),\n    ('catboost2',  Pipeline( [\n         ('pipe2', pipe2),\n         ('lgbm',CatBoostClassifier(**catboost_hyper))\n     ]) )\n ]\n#stacking = VotingClassifier(estimators=estimators, final_estimator=LogisticRegression(max_iter=1000, tol=0.0000001))\n\n#models = {'stack' : stacking }","aeb4662a":"models = {'Light_gbm1':  Pipeline( [\n         ('pipe1', pipe1),\n         ('lgbm',LGBMClassifier(**lgbm_hyper))\n     ]) ,\n    'Light_gbm2':  Pipeline( [\n         ('pipe2', pipe2),\n         ('lgbm',LGBMClassifier(**lgbm_hyper))\n     ]) ,\n    'xgboost1':  Pipeline( [\n         ('pipe1', pipe1),\n         ('lgbm',XGBClassifier(**xgboost_hyper))\n     ]) ,\n    'catboost1':  Pipeline( [\n         ('pipe1', pipe1),\n         ('lgbm',CatBoostClassifier(**catboost_hyper))\n     ]) ,\n    'catboost2':  Pipeline( [\n         ('pipe2', pipe2),\n         ('lgbm',CatBoostClassifier(**catboost_hyper))\n     ]) }","fd323e76":"result, result_pred_train, result_pred_test = train_get_score_cv(x_train, y_train, test,\n                                                                                          models, pipeline_engine = None, k=25)\nresult","e6ff6bf7":"result_pred_test","a3df440c":"models = {'stack' : LogisticRegression(max_iter=1000, tol=0.0000001, random_state = random_state, C = 0.001) }","ba68b606":"result, result_pred_tt, result_pred_t = train_get_score_cv(result_pred_train.loc[y_train.index], y_train, result_pred_test,\n                                                                                          models, pipeline_engine = None, k=25)\nresult","6b080a29":"((result_pred_t>result['cutoff'][0]).astype(int)).rename(\n    {'stack':'predicted'},axis = 1\n).to_csv(\"fscore_stack_logistic_probs.csv\")","210de245":"# level 2"}}