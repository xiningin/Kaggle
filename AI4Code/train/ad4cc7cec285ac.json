{"cell_type":{"54fd8c34":"code","930b2b90":"code","a1402cbf":"code","1be4d104":"code","828ea861":"code","a6499cf1":"code","5796599d":"code","cef84602":"code","ea33b12a":"code","7b70f7a2":"code","b5d394df":"code","bff20541":"code","19c0cc02":"code","1debf3d2":"code","faac0af7":"code","f230d326":"code","d897e4d2":"code","22beb0a8":"code","6fb9f91d":"code","68d1de1a":"code","958a0ec3":"code","622051f4":"code","4709ec49":"code","95642079":"code","cf458640":"code","52ddf7e9":"code","33421edc":"code","d2e1b162":"code","0d71157a":"markdown","c575919f":"markdown","c25bae2a":"markdown","d7477180":"markdown","64437e34":"markdown","62dc6196":"markdown","e0b7fe24":"markdown","64cf64e4":"markdown","4078404d":"markdown","7f5ca38b":"markdown","e8079a7e":"markdown","18901dea":"markdown","585a7505":"markdown","372f4d62":"markdown","4979a555":"markdown","8caaed28":"markdown","1bf215b7":"markdown","a2182057":"markdown","e1b1dc84":"markdown","9a25c4e5":"markdown","a180bc18":"markdown","938caefb":"markdown","248ab311":"markdown","fff14b4a":"markdown","e70749ed":"markdown","1ebcc06a":"markdown","f03ffb84":"markdown","f91b79b4":"markdown","51ec5e0b":"markdown","66c302b6":"markdown","f38b0401":"markdown","80a8aef2":"markdown","278f25be":"markdown","99213d88":"markdown","9148b206":"markdown","0dda6a15":"markdown","8266ed26":"markdown","10416561":"markdown","431819ab":"markdown","668e0b73":"markdown","57884be3":"markdown","1b50fb02":"markdown","2068416e":"markdown"},"source":{"54fd8c34":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nimport random\nimport re\nfrom nltk import sent_tokenize\nimport nltk\nfrom tqdm import tqdm\nfrom albumentations.core.transforms_interface import DualTransform, BasicTransform\nimport gensim.downloader as api\nfrom transformers import BertTokenizer,TFBertForMaskedLM\nimport re\n!pip install nlpaug\n!pip install -q colored\n\nimport nlpaug.augmenter.word as naw\nimport nlpaug.model.word_stats as nmw\nimport albumentations\nfrom torch.utils.data import Dataset\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom torch.multiprocessing import Pipe, Process\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import accuracy_score\nfrom transformers import BertModel, BertTokenizer\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nimport gc\n\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences as pad\nimport time\n\nimport time\nimport colored\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom colored import fg, bg, attr\n","930b2b90":"class NLPTransform(BasicTransform):\n    \"\"\" Transform for nlp task.\"\"\"\n\n    @property\n    def targets(self):\n        return {\"data\": self.apply}\n    \n    def update_params(self, params, **kwargs):\n        if hasattr(self, \"interpolation\"):\n            params[\"interpolation\"] = self.interpolation\n        if hasattr(self, \"fill_value\"):\n            params[\"fill_value\"] = self.fill_value\n        return params\n\n    def get_sentences(self, text, lang='en'):\n        return sent_tokenize(text)","a1402cbf":"class WordEmbeddingSubstitution(NLPTransform):\n    \"\"\" susbtitute similar words \"\"\"\n    def __init__(self, always_apply=False, p=0.5):\n        self.model=api.load('glove-twitter-25')  \n        \n        super(WordEmbeddingSubstitution, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        text=''\n        for word in nltk.word_tokenize(data):\n            try:\n                simword=self.model.most_similar(word,topn=1)\n                if simword[0][1]>0.95:\n                    text=text+' '+simword[0][0]\n                    continue\n            except:\n                text=text+' '+word\n                continue\n\n            text=text+' '+word\n\n        return text","1be4d104":"#Uncomment and see the below trick\n\n# transform = WordEmbeddingSubstitution(p=1.0)\n\n# text = 'This is super cool and amazing'\n\n# transform(data=(text))['data']","828ea861":"class LMmask(NLPTransform):\n    \"\"\" susbtitute similar words \"\"\"\n    def __init__(self, always_apply=False, p=0.5,verbose=False):\n        self.tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\n        self.model= TFBertForMaskedLM.from_pretrained('bert-base-uncased') \n        self.probability=p\n        self.verbose=verbose\n        \n        super(LMmask, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n       \n        flag=True\n        text=data\n        \n        for ix,n in enumerate(text.split()):\n            if random.random() < self.probability and flag:\n                flag=False\n                text=text.replace(n,'[MASK]')\n                break\n        \n        # DEFINE SENTENCE\n        indices = self.tokenizer.encode(text, add_special_tokens=True, return_tensors='tf')\n\n        # PREDICT MISSING WORDS\n        pred = self.model(indices)\n        masked_indices = np.where(indices==103)[1]\n\n        # DISPLAY MISSING WORDS\n        predicted_words = np.argmax( np.asarray(pred[0][0])[masked_indices,:] ,axis=1)\n        \n        if self.verbose:\n            return text,self.tokenizer.decode(predicted_words)\n        \n        return self.tokenizer.decode(predicted_words)\n","a6499cf1":"#Uncomment and see the below trick\n\n# transform = LMmask(p=1,verbose=True)\n\n# text = 'Data science is important'\n\n\n# transform(data=text)['data']","5796599d":"def _tokenizer(text, token_pattern=r\"(?u)\\b\\w\\w+\\b\"):\n    token_pattern = re.compile(token_pattern)\n    return token_pattern.findall(text)\n\n# Load sample data\ntrain=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntrain_x=train['text'].values.tolist()\n\n# Tokenize input\ntrain_x_tokens = [_tokenizer(x) for x in train_x]\n\n# Train TF-IDF model\ntfidf_model = nmw.TfIdf()\ntfidf_model.train(train_x_tokens)\ntfidf_model.save('.')\n\n# Load TF-IDF augmenter\naug = naw.TfIdfAug(model_path='.', tokenizer=_tokenizer)\n\ntexts = [\n    'I was wondering if anyone out there could enlighten me',\n    'well folks, my mac plus finally gave up'\n]\n\nfor text in texts:\n    augmented_text = aug.augment(text)\n    \n    print('-'*20)\n    print('Original Input:{}'.format(text))\n    print('Agumented Output:{}'.format(augmented_text))\n","cef84602":"class ShuffleSentencesTransform(NLPTransform):\n    \"\"\" Do shuffle by sentence \"\"\"\n    def __init__(self, always_apply=False, p=0.5):\n        super(ShuffleSentencesTransform, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        sentences = self.get_sentences(data)\n        random.shuffle(sentences)\n        return ' '.join(sentences)","ea33b12a":"transform = ShuffleSentencesTransform(p=1.0)\n\ntext = train.loc[45]['text']\n\ntransform(data=(text))['data']","7b70f7a2":"class SwapWordsTransform(NLPTransform):\n    \"\"\" Do shuffle by words \"\"\"\n    def __init__(self, always_apply=False, p=0.5,verbose=False):\n        self.probability=p\n        self.verbose=verbose\n        \n        super(SwapWordsTransform, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        \n        words=data.split()\n        \n        if random.random() < self.probability:\n            # Storing the two elements \n            get = random.choice(words),random.choice(words)\n            \n            pos1=words.index(get[0])\n            pos2=words.index(get[1])\n            \n            while pos1==pos2:\n                pos2=words.index(random.choice(words))\n            \n            # unpacking those elements \n            words[pos2], words[pos1] = get \n\n        if self.verbose:\n            return data,' '.join(words)\n        \n        return ' '.join(words)","b5d394df":"transform = SwapWordsTransform(p=1.0,verbose=True)\n\ntext = train.loc[45]['text']\n\ntransform(data=(text))['data']","bff20541":"class DeleteWordsTransform(NLPTransform):\n    \"\"\" Do shuffle by words \"\"\"\n    def __init__(self, always_apply=False, p=0.5,verbose=False):\n        self.probability=p\n        self.verbose=verbose\n        \n        super(DeleteWordsTransform, self).__init__(always_apply, p)\n\n    def apply(self, data, **params):\n        \n        words=data.split()\n        \n        if random.random() < self.probability:\n            get = random.choice(words)\n            words.remove(get)\n\n        if  self.verbose:\n            return data,' '.join(words)\n        \n        return ' '.join(words)","19c0cc02":"transform = DeleteWordsTransform(p=1.0,verbose=True)\n\ntext = train.loc[45]['text']\n\ntransform(data=(text))['data']","1debf3d2":"configs={\n    'EPOCHS' : 2,\n    'SPLIT' : 0.8,\n    'MAXLEN' : 100,\n    'DROP_RATE' : 0.3,\n    'OUTPUT_UNITS' : 2,\n    'BATCH_SIZE' : 64,\n    'LR' : (4e-5, 1e-2),\n    'BERT_UNITS' : 768,\n    'VAL_BATCH_SIZE' : 16,\n    'MODEL_SAVE_PATH' : 'model.pt'\n    \n}","faac0af7":"train=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","f230d326":"train.head(3)","d897e4d2":"def get_train_transforms():\n    return albumentations.Compose([\n        albumentations.OneOf([\n            SwapWordsTransform(p=0.5),\n            ShuffleSentencesTransform(p=0.8)\n        ]),\n        DeleteWordsTransform(p=0.3)\n    ])","22beb0a8":"model = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model)","6fb9f91d":"class DatasetRetriever(Dataset):\n\n    def __init__(self, df,tokenizer, train_transforms=None):\n        self.comment_texts = df['text'].values\n        self.train_transforms = train_transforms\n        self.target = df['target'].values\n        self.tokenizer=tokenizer\n        \n    def __len__(self):\n        return len(self.comment_texts)\n\n    def __getitem__(self, idx):\n        \n        start, finish = 101,102\n        pg, tg = 'post', 'post'\n        tweet = str(self.comment_texts[idx]).strip()\n        target=self.target[idx]\n        \n        if self.train_transforms:\n            text = self.train_transforms(data=(tweet))['data']\n            \n        tweet_ids = self.tokenizer.encode(text)\n        mask = [1] * len(tweet_ids)\n            \n        padding_length = configs['MAXLEN'] - len(tweet_ids)\n        \n        if padding_length > 0:\n            input_ids = np.array(tweet_ids + ([0] * padding_length))\n            mask = np.array(mask + ([0] * padding_length))\n        else:\n            input_ids=np.array(tweet_ids)\n            mask = np.array(mask)\n        \n        #attention_mask = mask.reshape((1, -1))\n        \n        sentiment = torch.FloatTensor(to_categorical(target, num_classes=2))\n        \n        return sentiment, torch.LongTensor(input_ids), torch.LongTensor(mask)","68d1de1a":"dataset = DatasetRetriever(train[:2],tokenizer=tokenizer, train_transforms=get_train_transforms())\n\ndataset[0]","958a0ec3":"del dataset;gc.collect()","622051f4":"class BERT(nn.Module):\n    def __init__(self):\n        super(BERT, self).__init__()\n        self.softmax = nn.Softmax(dim=1)\n        self.drop = nn.Dropout(configs['DROP_RATE'])\n        self.bert = BertModel.from_pretrained(model)\n        self.dense = nn.Linear(configs['BERT_UNITS'],configs['OUTPUT_UNITS'])\n        \n    def forward(self, inp, att):\n        inp,att = inp.view(-1, configs['MAXLEN']),att.view(-1, configs['MAXLEN'])\n        _, self.feat = self.bert(inp, att)\n        return self.softmax(self.dense(self.drop(self.feat)))\n","4709ec49":"def cel(inp, target):\n    _, labels = target.max(dim=1)\n    return nn.CrossEntropyLoss()(inp, labels)\n\ndef accuracy(inp, target):\n    inp_ind = inp.max(axis=1).indices\n    target_ind = target.max(axis=1).indices\n    return (inp_ind == target_ind).float().sum(axis=0)","95642079":"class AverageMeter:\n    \"\"\"\n    Computes and stores the average and current value\n    \"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n\nclass EarlyStopping:\n    def __init__(self, patience=7, mode=\"max\", delta=0.001):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score\n\n\ndef accuracy(inp, target):\n    inp_ind = inp.argmax(axis=1)\n    target_ind = target.argmax(axis=1)\n    return accuracy_score(target_ind,inp_ind)","cf458640":"def train_fn(data_loader, model, optimizer, device, scheduler=None):\n    model.train()\n    losses = AverageMeter()\n    total_accuracy = AverageMeter()\n\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    \n    for bi, d in enumerate(tk0):\n\n        target,ids,mask=d\n\n        ids = ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        target = target.to(device, dtype=torch.long)\n\n        model.zero_grad()\n        output = model(\n            ids,mask\n        )\n        \n        loss = cel(output, target)\n        \n        output=output.cpu().detach().numpy()\n        target=target.cpu().detach().numpy()\n    \n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        accuracies = []\n        for px, tweet in enumerate(target):\n            y_true = target[px].reshape(-1,2)\n            y_pred = output[px].reshape(-1,2)\n          \n            acc_score = accuracy(\n               y_true,y_pred\n            )\n            accuracies.append(acc_score)\n\n        total_accuracy.update(np.mean(accuracies), ids.size(0))\n        losses.update(loss.item(), ids.size(0))\n        tk0.set_postfix(loss=losses.avg, acc=total_accuracy.avg)\n        \n        \ndef eval_fn(data_loader, model, device):\n    model.eval()\n    losses = AverageMeter()\n    total_accuracy = AverageMeter()\n    \n    with torch.no_grad():\n        tk0 = tqdm(data_loader, total=len(data_loader))\n        for bi, d in enumerate(tk0):\n            \n            target,ids,mask=d\n            \n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            target = target.to(device, dtype=torch.long)\n\n            output = model(\n                ids,mask\n            )\n            \n            loss = cel(output, target)\n            \n            output = output.cpu().detach().numpy()\n            target=target.cpu().detach().numpy()\n        \n            \n            accuracies = []\n            for px, tweet in enumerate(target):\n                y_true = target[px].reshape(-1,2)\n                y_pred = output[px].reshape(-1,2)\n\n                acc_score = accuracy(\n                   y_true,y_pred\n                )\n                accuracies.append(acc_score)\n\n            total_accuracy.update(np.mean(accuracies), ids.size(0))\n            losses.update(loss.item(), ids.size(0))\n            tk0.set_postfix(loss=losses.avg, acc=total_accuracy.avg)\n    \n    print(f\"Accuracy = {total_accuracy.avg}\")\n    return total_accuracy.avg","52ddf7e9":"def engine(train_df):\n    train_df = shuffle(train_df)\n    train_df = train_df.reset_index(drop=True)\n    device = torch.device(\"cuda\")\n\n    split = np.int32(configs['SPLIT']*len(train_df))\n    val_df, train_df = train_df[split:], train_df[:split]\n\n    val_df = val_df.reset_index(drop=True)\n    \n    val_dataset = DatasetRetriever(val_df, tokenizer,get_train_transforms())\n    val_loader = DataLoader(val_dataset, batch_size=configs['VAL_BATCH_SIZE'],\n                            num_workers=4)\n\n    train_df = train_df.reset_index(drop=True)\n    \n    train_dataset = DatasetRetriever(train_df, tokenizer,get_train_transforms())\n    train_loader = DataLoader(train_dataset, batch_size=configs['BATCH_SIZE'],\n                              num_workers=4)\n\n    model = BERT().to(device)\n\n    num_train_steps = int(len(train_df) \/ configs['BATCH_SIZE'] * configs['EPOCHS'])\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=0, \n        num_training_steps=num_train_steps\n    )\n\n    es = EarlyStopping(patience=2, mode=\"max\")\n    \n    start = time.time()\n    print(\"STARTING TRAINING ...\\n\")\n\n    \n    for epoch in range(configs['EPOCHS']):\n        train_fn(train_loader, model, optimizer, device, scheduler=scheduler)\n        acc = eval_fn(val_loader, model, device)\n        print(f\"Accuracy Score = {acc}\")\n        es(acc, model, model_path=\"model.bin\")\n        if es.early_stop:\n            print(\"Early stopping\")\n            break\n            \n","33421edc":"gc.collect()","d2e1b162":"engine(train)","0d71157a":"## 4.3 Engine","c575919f":"# 3. Random Noise Injection","c25bae2a":"## 2.2 Masked Language Model","d7477180":"## 1.2 Text augmentation","64437e34":"## 2.3 TF-IDF based word replacement","62dc6196":"## Objective:\n            \n<p> The crux to learn any form of representation is to get more and more `useful data` and feed it into any algorithm which can make use of it to build an intelligent system. Even though we have a lot of unstructured data available everywhere, most of them are not usable directly. We need a systematic approach in collecting the data for our specific problem to solve. The motivation of this notebook is to build a robust nlp pipeline studying different forms of augmentation and making use of it to our specific problem statement. <\/p>\n\n","e0b7fe24":"<img src='https:\/\/miro.medium.com\/max\/721\/1*bBMS9SW60XozrRbBWyc1Dw.jpeg' width=1000>\n<div align=\"center\"><font size=\"3\">Source: Google<\/font><\/div>","64cf64e4":"Note:\n<p>This work is an extensive research\/study based on different resources(acknowledged below) for my personal reference as well as a knowledge sharing to the community<\/p>","4078404d":"<img src='https:\/\/amitness.com\/images\/nlp-aug-bert-augmentations.png' width=700>\n<div align=\"center\"><font size=\"3\">Source: Google<\/font><\/div>","7f5ca38b":"This does'nt seem to work as expected. That may be because of the nature of data, tweets has lot of noise in it, and we can see many common words among tweets","e8079a7e":"## 2.1 Word-Embeddings Substitution","18901dea":"<p> Hope this is helpful. Kindly upvote if you like this work. Thanks for the suppport","585a7505":"In the method, a tweet are divided into two halves and two random tweets of the same polarity(i.e. positive\/negative) have their halves swapped. The hypothesis is that even though the result will be ungrammatical and semantically unsound, the new text will still preserve the sentiment.","372f4d62":"## 4.2 Model","4979a555":"## 3.3 Random deletion","8caaed28":"<img src='https:\/\/amitness.com\/images\/semantic-invariance-nlp.png' width=700>\n<div align=\"center\"><font size=\"3\">Source: Google<\/font><\/div>","1bf215b7":"## 3.1 Sentence shuffling","a2182057":"<font color='#31a04b' size=4>Let's get started<\/font><br>","e1b1dc84":"<img src='https:\/\/amitness.com\/images\/nlp-aug-random-swap.png' width=700>\n<div align=\"center\"><font size=\"3\">Source: Google<\/font><\/div>","9a25c4e5":"## 4.1 Preparing dataset","a180bc18":"<p>Woah that's fascinating. We can mask words to make bert to predict for our augmentation <\/p>","938caefb":"In this, we randomly remove each word in the sentence with some probability p.","248ab311":"# Table of Contents\n\n- 1. Augmentation\n   - 1.1 What is augmentation & its types\n   - 1.2 Text augmentation\n\n- 2. Lexical substitution\n   - 2.1 Word-Embeddings Substitution\n   - 2.2 Masked Language Model\n   - 2.3 TF-IDF based word replacement\n   \n- 3. Random Noise Injection\n   - 3.1 Sentence shuffling\n   - 3.2 Swapping words\n   - 3.3 Random deletion\n   - 3.4 Instance crossover augmentation\n\n- 4. Building the pipeline\n   - 4.1 Preparing dataset\n   - 4.2 Model\n   - 4.3 Engine\n  \n- 5. Acknowledgements","fff14b4a":"## 3.4 Instance crossover augmentation","e70749ed":"<img src='https:\/\/amitness.com\/images\/nlp-aug-sentence-shuffle.png' width=700>\n<div align=\"center\"><font size=\"3\">Source: Google<\/font><\/div>","1ebcc06a":"## What is augmentation:\n\n<p>Augmentation is a technique where we create new data points by adding noise\/sampling from the input data distribution. It\u2019s more useful, when we have a limited amount of data available. We have different types of augmentation<\/p> \n    \n\n    - Image augmentation \n    - Text augmentation\n    - Audio augmentation\n    - Feature data augmentation \n    \n\n<p>Unlike Computer vision it's  not easy to augment the data in text as the representation of language is complex,even a slight change in a word will change its entire meaning. So let's see how we can make use of that noise too.<\/p>","f03ffb84":"# 2. Lexical substitution","f91b79b4":"<p> we have used glove vectors for each words in the sentencec to replace with words which has 95% similarity <\/p>","51ec5e0b":"<img src='https:\/\/amitness.com\/images\/nlp-aug-instance-crossover.png' width=700>\n<div align=\"center\"><font size=\"3\">Source: Google<\/font><\/div>","66c302b6":"<p> With text we can create lot of synthesised samples but retaining the meaning of original text is more important,On the other hand we have noise augmentation,where we feed in these kind of noised samples to add new samples extensively. Let's walk through these  different types one by one <\/p>\n\n<p> We will try to have base form from albumentation library and keep adding our augmentation methods as we move on. Thanks to the author @shonenkov for the super cool albumentation version for nlp<\/p>","f38b0401":"# 1. Augmentation","80a8aef2":"<img src='https:\/\/amitness.com\/images\/nlp-aug-random-deletion.png' width=700>\n<div align=\"center\"><font size=\"3\">Source: Google<\/font><\/div>","278f25be":"This is a naive technique where we shuffle sentences present in a training text to create an augmented version.","99213d88":"# 4. Building the pipeline","9148b206":"# 5. Acknowledgements","0dda6a15":"Uncomment the below code to check the output","8266ed26":"Transformer models such as BERT, ROBERTA and ALBERT have been trained on a large amount of text using a pretext task called \u201cMasked Language Modeling\u201d where the model has to predict masked words based on the context.\n\nThis can be used to augment some text","10416561":"The idea is to randomly swap any two words in the sentence.","431819ab":"<p>In this approach, we take pre-trained word embeddings such as Word2Vec, GloVe, FastText, Sent2Vec, and use the nearest neighbor words in the embedding space as the replacement for some word in the sentence\n<\/p>","668e0b73":"<img src='https:\/\/amitness.com\/images\/nlp-aug-embedding-example.png' width=700>\n<div align=\"center\"><font size=\"3\">Source: Google<\/font><\/div>","57884be3":"* https:\/\/amitness.com\/2020\/05\/data-augmentation-for-nlp\/\n* https:\/\/www.kaggle.com\/shonenkov\/nlp-albumentations\n* https:\/\/www.kaggle.com\/abhishek\/bert-base-uncased-using-pytorch\/data?select=utils.py","1b50fb02":"The basic idea is that words that have low TF-IDF scores are uninformative and thus can be replaced without affecting the ground-truth labels of the sentence.\nTF-IDF based word replacement\n\nThe words that replaces the original word are chosen by calculating TF-IDF scores of words over the whole document and taking the lowest ones","2068416e":"## 3.2 Swapping words"}}