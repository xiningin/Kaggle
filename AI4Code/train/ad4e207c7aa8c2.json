{"cell_type":{"b9ea4b2b":"code","195a6739":"code","8513e724":"code","6cc0c0f0":"code","5addd872":"code","3335f2c7":"code","28bd0c66":"code","a1ce1c2b":"code","12b9675f":"code","40138777":"code","9d67e6bb":"code","6eb191bd":"code","f79b0ba5":"code","3cca4d8b":"code","8211c7e0":"code","e419cbe9":"code","8b6e482f":"code","660615f7":"code","e0d3c2e4":"code","4f156b05":"code","dfa1aa27":"code","fa98cd05":"code","a1960d7a":"code","4033a449":"code","120dda1e":"code","d5c6e74a":"code","6de8679d":"markdown","cd6828a8":"markdown","6f922deb":"markdown"},"source":{"b9ea4b2b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","195a6739":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport cv2\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt","8513e724":"train=pd.read_csv('\/kaggle\/input\/indian-dance-form-recognition\/dataset\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/indian-dance-form-recognition\/dataset\/test.csv')\ntrain.head()","6cc0c0f0":"test.head()","5addd872":"print(train['target'].unique())","3335f2c7":"Class_map={'manipuri':0, 'bharatanatyam':1, 'odissi':2 ,'kathakali':3, 'kathak':4, 'sattriya':5,\n 'kuchipudi':6, 'mohiniyattam':7}\ninverse_map={0:'manipuri', 1:'bharatanatyam', 2:'odissi' ,3:'kathakali',4: 'kathak', 5:'sattriya',\n 6:'kuchipudi', 7:'mohiniyattam'}\ntrain['target']=train['target'].map(Class_map)","28bd0c66":"train.head()","a1ce1c2b":"img_h,img_w= (224,224)","12b9675f":"train_img=[]\ntrain_label=[]\nj=0\npath='\/kaggle\/input\/indian-dance-form-recognition\/dataset\/train'\nfor i in tqdm(train['Image']):\n    final_path=os.path.join(path,i)\n    img=cv2.imread(final_path)\n    img=cv2.resize(img,(img_h,img_w))\n    img=img.astype('float32')\n    train_img.append(img)\n    train_label.append(train['target'][j])\n    j=j+1","40138777":"from sklearn.model_selection import train_test_split\n\nx_train, x_valid, y_train, y_valid = train_test_split(train_img, train_label, test_size=0.30, shuffle= True)","9d67e6bb":"test_img=[]\npath='\/kaggle\/input\/indian-dance-form-recognition\/dataset\/test'\nfor i in tqdm(test['Image']):\n    final_path=os.path.join(path,i)\n    img=cv2.imread(final_path)\n    img=cv2.resize(img,(img_h,img_w))\n    img=img.astype('float32')\n    test_img.append(img)","6eb191bd":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\ntrain_datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,# divide each input by its std\n        rescale=1.\/255,\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.3, # Randomly zoom image \n        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ntest_datagen= ImageDataGenerator(rescale=1.\/255)\nvalid_datagen= ImageDataGenerator(rescale=1.\/255)\ntrain_datagen.fit(x_train)\ntest_datagen.fit(test_img)\nvalid_datagen.fit(x_valid)","f79b0ba5":"train_img=np.array(train_img)\nx_train= np.array(x_train)\nx_valid= np.array(x_valid)\ny_train= np.array(y_train)\ny_valid= np.array(y_valid)\ntest_img=np.array(test_img)\ntrain_label=np.array(train_label)\nprint(\"Shape of training data=\",x_train.shape,\" and shape of labels of training data= \",y_train.shape)\nprint(\"Shape of validation data=\",x_valid.shape,\" and shape of labels of validation data= \",y_valid.shape)\nprint(\"Shape of test data=\",test_img.shape)","3cca4d8b":"from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\nfrom tensorflow.keras.layers import Dropout\n\nbase_model= InceptionResNetV2(include_top=False, weights='imagenet', \n                              input_tensor=None, input_shape=(img_h,img_w,3), pooling='avg')","8211c7e0":"'''for layer in base_model.layers[:-10]:\n    layer.trainable=False'''\nbase_model.trainable=False\n    \nbase_model.summary()","e419cbe9":"from tensorflow.keras.layers import Flatten,Dense,Dropout,BatchNormalization\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\n\nmodel=Sequential()\nmodel.add(base_model)\nmodel.add(Flatten())\nmodel.add(Dropout(0.4))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\n\n#model.add(Dense(256, activation='relu'))\n#model.add(Dropout(0.2))\n#model.add(BatchNormalization())\n\nmodel.add(Dense(8,activation='softmax'))\n\n\nfrom keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n\n\n#NASnet.trainable=False\n\nreduce_learning_rate = ReduceLROnPlateau(monitor='loss',\n                                         factor=0.1,\n                                         patience=2,\n                                         cooldown=2,\n                                         min_lr=0.00001,\n                                         verbose=1)\n\ncallbacks = [reduce_learning_rate]\n    \n\n\nmodel.compile( optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n\nmodel.summary()","8b6e482f":"model.fit(train_datagen.flow(x_train, to_categorical(y_train,8), batch_size=32),\n                    epochs=10,\n          callbacks=callbacks,\n          validation_data= valid_datagen.flow(x_valid, to_categorical(y_valid,8), batch_size=32),\n          verbose=1\n             )","660615f7":"from tensorflow.keras.applications.resnet import ResNet50\nbase_model_2= ResNet50(include_top=False, weights='imagenet',input_shape=(img_h,img_w,3), pooling='max')\n\n'''for layer in base_model_2.layers[:-3]:\n    layer.trainable=False'''\nbase_model_2.trainable=False\n    \nmodel_2=Sequential()\nmodel_2.add(base_model_2)\nmodel_2.add(Flatten())\nmodel_2.add(Dropout(0.4))\nmodel_2.add(BatchNormalization())\n\nmodel_2.add(Dense(768, activation='relu'))\nmodel_2.add(Dropout(0.2))\nmodel_2.add(BatchNormalization())\n\nmodel_2.add(Dense(256, activation='relu'))\nmodel_2.add(Dropout(0.1))\nmodel_2.add(BatchNormalization())\n\n\nmodel_2.add(Dense(8,activation='softmax'))\n\nmodel_2.compile( optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\nmodel_2.summary()","e0d3c2e4":"model_2.fit(train_datagen.flow(x_train, to_categorical(y_train,8), batch_size=32),\n                    epochs=10,\n          callbacks=callbacks,\n          validation_data= valid_datagen.flow(x_valid, to_categorical(y_valid,8), batch_size=32),\n          verbose=1\n             )","4f156b05":"from tensorflow.keras.applications.vgg19 import VGG19,preprocess_input\nbase_model_3=VGG19(include_top=False, weights='imagenet',input_shape=(img_h,img_w,3), pooling='max')\n\nfor layer in base_model_3.layers[:-4]:\n    layer.trainable=False\n#base_model_3.trainable=False\n    \nmodel_3=Sequential()\nmodel_3.add(base_model_3)\nmodel_3.add(Flatten())\n\nmodel_3.add(BatchNormalization())\nmodel_3.add(Dropout(0.2))\n\n\nmodel_3.add(Dense(256, activation='relu'))\nmodel_3.add(BatchNormalization())\n\n\nmodel_3.add(Dense(64, activation='relu'))\nmodel_3.add(BatchNormalization())\nmodel_3.add(Dropout(0.2))\n\n\n\nmodel_3.add(Dense(8,activation='softmax'))\n\nmodel_3.compile( optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\nmodel_3.summary()","dfa1aa27":"model_3.fit(train_datagen.flow(x_train, to_categorical(y_train,8), batch_size=32),\n                    epochs=20,\n          callbacks=callbacks,\n          validation_data= valid_datagen.flow(x_valid, to_categorical(y_valid,8), batch_size=32),\n          verbose=1\n             )","fa98cd05":"from tensorflow.keras import Input\nfrom tensorflow.keras.layers import concatenate\ndef stacking_ensemble(members,input_shape,n_classes):\n  commonInput = Input(shape=input_shape)\n  out=[]\n\n  for model in members:\n    #model._name= model._name+\"test\"+ str(members.index(model)+1)\n    model._name= model.get_layer(index = 0)._name +\"-test\"+ str(members.index(model)+1)\n    out.append(model(commonInput))\n\n  modeltmp = concatenate(out,axis=-1)\n  modeltmp = Dense(32, activation='relu')(modeltmp)\n  modeltmp = Dense(16, activation='relu')(modeltmp)\n  modeltmp = Dense(n_classes, activation='softmax')(modeltmp)\n  stacked_model = Model(commonInput,modeltmp)\n  stacked_model.compile( loss='categorical_crossentropy',optimizer= 'adam', metrics=['accuracy'])\n\n  return stacked_model","a1960d7a":"#members=[model,model_2,model_3]\nmembers=[model,model_2,model_3]","4033a449":"stacked_model= stacking_ensemble(members,(img_h,img_w,3),8)\nstacked_model.summary()","120dda1e":"stacked_model.fit(train_datagen.flow(x_train, to_categorical(y_train,8), batch_size=32),\n                    epochs=10,\n          callbacks=callbacks,\n          validation_data= valid_datagen.flow(x_valid, to_categorical(y_valid,8), batch_size=32),\n          verbose=1\n             )","d5c6e74a":"labels = stacked_model.predict(test_img)\nprint(labels[:4])\nlabel = [np.argmax(i) for i in labels]\nclass_label = [inverse_map[x] for x in label]\nprint(class_label[:3])\nsubmission = pd.DataFrame({ 'Image': test.Image, 'target': class_label })\nsubmission.head(10)\nsubmission.to_csv('submission.csv', index=False)","6de8679d":"Model_3 is VGG19","cd6828a8":"Model_2 is resnetV2","6f922deb":"Stacking ensemble"}}