{"cell_type":{"98901b82":"code","96aae581":"code","07328825":"code","b031a610":"code","d463ce02":"code","3402de60":"code","cefaac7d":"code","8a0c40fe":"code","a29d91a3":"markdown","4978d359":"markdown","2af245a3":"markdown","9d710104":"markdown","c8201997":"markdown","d336815b":"markdown","e72ccb0a":"markdown","ead53185":"markdown"},"source":{"98901b82":"!pip install kaggle-environments --upgrade\n\nfrom kaggle_environments import make\nfrom collections import defaultdict\nimport numpy as np\nimport psutil\nimport ray\nimport scipy.signal\nimport matplotlib.pyplot as plt\n","96aae581":"%%writefile ucb_decay.py\n\nimport numpy as np\n\ndecay = 0.97\ntotal_reward = 0\nbandit = None\n\ndef agent(observation, configuration):\n    global reward_sums, n_selections, total_reward, bandit\n    \n    n_bandits = configuration.banditCount\n\n    if observation.step == 0:\n        n_selections, reward_sums = np.full((2, n_bandits), 1e-32)\n    else:\n        reward_sums[bandit] += decay * (observation.reward - total_reward)\n        total_reward = observation.reward\n\n    avg_reward = reward_sums \/ n_selections    \n    delta_i = np.sqrt(2 * np.log(observation.step + 1) \/ n_selections)\n    bandit = int(np.argmax(avg_reward + delta_i))\n\n    n_selections[bandit] += 1\n\n    return bandit","07328825":"%%writefile bayesian_ucb.py\n\nimport numpy as np\nfrom scipy.stats import beta\n\npost_a, post_b, bandit = [None] * 3\ntotal_reward = 0\nc = 3\n\ndef agent(observation, configuration):\n    global total_reward, bandit, post_a, post_b, c\n\n    if observation.step == 0:\n        post_a, post_b = np.ones((2, configuration.banditCount))\n    else:\n        r = observation.reward - total_reward\n        total_reward = observation.reward\n        # Update Gaussian posterior\n        post_a[bandit] += r\n        post_b[bandit] += 1 - r\n    \n    bound = post_a \/ (post_a + post_b) + beta.std(post_a, post_b) * c\n    bandit = int(np.argmax(bound))\n    \n    return bandit","b031a610":"def plot_final_rewards(hist):\n    num_episodes = 0\n    colors_db = ['g','b']\n\n    plt.figure(figsize=(12,8))\n    for i,agent in enumerate(hist.keys()):\n        plt.plot(hist[agent], label=agent, color=colors_db[i])\n        num_episodes = len(hist[agent])\n        avg_final_reward = np.array(hist[agent]).mean()\n        plt.plot([0, num_episodes-1],[avg_final_reward, avg_final_reward], label=agent+' avg.', color=colors_db[i],linestyle='dashed')\n        \n    plt.legend(bbox_to_anchor=(1.2, 0.5))\n    plt.xlabel(\"Iterations\")\n    plt.ylabel(\"Final Reward\")\n    plt.title(\"Final Agent Rewards for \" \n              + str(num_episodes) + \" Episodes\")\n    plt.show()","d463ce02":"num_cpus = psutil.cpu_count(logical=False)\nray.shutdown()\nray.init(num_cpus=num_cpus)","3402de60":"@ray.remote\n\ndef run_trial(agent1, agent2):\n    hist = defaultdict(list)\n    env = make(\"mab\")\n    env.reset()\n    env.run([agent1, agent2])\n    hist[agent1].append(env.state[0]['reward'])\n    hist[agent2].append(env.state[1]['reward'])\n    return hist\n  ","cefaac7d":"def run_trials(agent1, agent2, num_trails):\n    result_ids = []\n    \n    for i in range(num_trails):\n        result_ids.append(run_trial.remote(agent1, agent2))\n    \n    results = ray.get(result_ids)\n    hist = defaultdict(list)\n    \n    for i in range(num_trails):\n        hist[agent1].append(results[i][agent1])\n        hist[agent2].append(results[i][agent2])\n    \n    return hist","8a0c40fe":"hist = run_trials(\"ucb_decay.py\", \"bayesian_ucb.py\", 30)\nplot_final_rewards(hist)\n","a29d91a3":"Lastly define local function to collate results from remote workers.","4978d359":"Now initialise worker pool that will process trials across available CPU's.","2af245a3":"By processing in parallel across available CPU's you can run more comparison trials in shorter time enabling quicker refinement of agents. This may be especially beneficial if you have high CPU count compute resources avaialble.","9d710104":"Define function run_trial that will be executed on remote workers.","c8201997":"## 4 Run trial and plot results","d336815b":"# Summary\n\nThis notebook uses the ray library https:\/\/docs.ray.io\/en\/latest\/index.html to quickly run trials of two agents in parallel. It may be especially useful for those that have high CPU count local machines on which they are developing agents. In the Kaggle dual CPU kernel environment the speed-up will be limited. \n\nI re-use aspects of other notebooks for which credit goes to their authors:\n\n* @xhlulu https:\/\/www.kaggle.com\/xhlulu\/santa-2020-ucb-and-bayesian-ucb-starter   \n* @isaienkov https:\/\/www.kaggle.com\/xhlulu\/santa-2020-ucb-and-bayesian-ucb-starter\n* @JumabekAlihanov https:\/\/www.kaggle.com\/jumabek\/plot-comparison-ucb-vs-bayesian-isaienkov-s-code\n\n\nConfession time, I'm an R first programmer, so excuse my poor python programming but comments on making improvements are welcome! The ray library itself looks rather interesting as it includes agents and other capabilities for reinformcement learning. \n\n\n\n## 1 Setup libraries","e72ccb0a":"## 3 Create utility functions\n\nFirst function is to plot final rewards trials of two agents. This is from code of @JumabekAlihanov https:\/\/www.kaggle.com\/jumabek\/plot-comparison-ucb-vs-bayesian-isaienkov-s-code ","ead53185":"## 2 Define two agents\n\nThe two agents are taken from the notebook of @xhlulu https:\/\/www.kaggle.com\/xhlulu\/santa-2020-ucb-and-bayesian-ucb-starter\n"}}