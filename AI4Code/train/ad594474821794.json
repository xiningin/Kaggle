{"cell_type":{"853f20c3":"code","69d76d49":"code","50b7f195":"code","3cdbf4ad":"code","5f6fb9ef":"code","27c1bc13":"code","f6eecad3":"code","e9540d5b":"code","23a4b5b8":"code","a367b54c":"code","0c1418da":"code","704a4f49":"code","afc75fca":"code","92a2a46e":"code","d88d6e9a":"code","049d81de":"code","ace2c30e":"code","0b02420d":"code","d4e524e5":"code","1d81cae6":"code","b730cd5b":"code","094d9e59":"code","d435a876":"code","9025c4c3":"code","e4103cfd":"code","22190ec4":"code","a1bba64b":"code","716c459f":"code","203bf394":"code","25928457":"code","2aa1c27e":"code","ef3807dc":"code","6445fa2a":"code","42e59eb4":"code","3bd75935":"code","74b25d31":"code","bdfc7528":"code","f53c0171":"code","b1e1d78c":"code","8ab78534":"code","c2bc9a79":"code","e8fb90f1":"code","ed205f82":"code","379ec068":"code","b7b0b463":"code","98a6cfc3":"code","bd30af6a":"code","5f52f5d5":"code","da1961ef":"code","df377fb7":"code","2ca7bbd0":"markdown","95aeb573":"markdown","17b12b4b":"markdown","9da5c94a":"markdown","0f9e7900":"markdown","cc210091":"markdown","7940eb4c":"markdown","9de8923d":"markdown","8b2a616f":"markdown","83228413":"markdown","8a7061c5":"markdown"},"source":{"853f20c3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","69d76d49":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score, confusion_matrix\n%matplotlib inline\n\nfrom sklearn.feature_extraction.text import CountVectorizer  #DT does not take strings as input for the model fit step....","50b7f195":"df = pd.read_csv(\"..\/input\/bank-full.csv\")","3cdbf4ad":"df.head(15)","5f6fb9ef":"df.shape","27c1bc13":"df.describe()","f6eecad3":"df.info()  # many columns are of type object i.e. strings. These need to be converted to ordinal type","e9540d5b":"for feature in df.columns: # Loop through all columns in the dataframe\n    if df[feature].dtype == 'object': # Only apply for columns with categorical strings\n        df[feature] = pd.Categorical(df[feature])# Replace strings with an integer\ndf.head(10)","23a4b5b8":"df.info()","a367b54c":"plt.figure(figsize=(10,8))\nsns.heatmap(df.corr(),\n            annot=True,\n            linewidths=.5,\n            center=0,\n            cbar=False,\n            cmap=\"YlGnBu\")\nplt.show()","0c1418da":"sns.pairplot(df,diag_kind='kde')","704a4f49":"# df_box = df['age', \t'balance', \t'day', \t'duration', \t'campaign', \t'pdays', \t'previous']\n\nsns.boxplot(x=df['age'], y=df['Target'], data=pd.melt(df))","afc75fca":"sns.boxplot(x=df['balance'], y=df['Target'], data=pd.melt(df))","92a2a46e":"sns.boxplot(x=df['campaign'], y=df['Target'], data=pd.melt(df))","d88d6e9a":"sns.boxplot(x=df['duration'], y=df['Target'], data=pd.melt(df))","049d81de":"# Excluding Outcome column which has only \ndf.drop(['Target'], axis=1).hist(stacked=False, bins=100, figsize=(30,45), layout=(14,4))","ace2c30e":"# from sklearn import preprocessing\n# from sklearn.preprocessing import StandardScaler\n\n# scaler = StandardScaler()\n# print(scaler.fit(df))\n\n# mm_scaler = preprocessing.MinMaxScaler()\n# X_train = mm_scaler.fit_transform(X_train)\n# mm_scaler.transform(X_test)\n\n","0b02420d":"oneHotCols = [\"job\",\"marital\",\"education\",\"default\",\"housing\",\"loan\",\"contact\",\"month\",\"poutcome\"]\ndf_data = pd.get_dummies(df, columns=oneHotCols)\ndf_data.head(10)","d4e524e5":"df_data.info()","1d81cae6":"# splitting data into training and test set for independent attributes\nfrom sklearn.model_selection import train_test_split\n\nX = df_data.drop(\"Target\" , axis=1)\ny = df_data.pop(\"Target\")\n\nfrom scipy.stats import zscore\n# convert the features into z scores as we do not know what units \/ scales were used and store them in new dataframe\n# It is always adviced to scale numeric attributes in models that calculate distances.\n\nXScaled  = X.apply(zscore)  # convert all attributes to Z scale \n\n# XScaled.describe()\n\nX_train, X_test, y_train, y_test = train_test_split(XScaled, y, test_size=.30, random_state=1)\nX_train.shape,X_test.shape","b730cd5b":"# invoking the decision tree classifier function. Using 'entropy' method of finding the split columns.\n\nmodel_entropy = DecisionTreeClassifier(criterion='entropy')","094d9e59":"model_entropy.fit(X_train, y_train)","d435a876":"model_entropy.score(X_train, y_train)  # performance on train data","9025c4c3":"model_entropy.score(X_test, y_test)  # performance on test data","e4103cfd":"clf_pruned = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100, max_depth=3, min_samples_leaf=5)\nclf_pruned.fit(X_train, y_train)","22190ec4":"from sklearn.tree import export_graphviz\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nimport pydotplus\nimport graphviz\n\nxvar = df.drop('Target', axis=1)\nfeature_cols = xvar.columns\nfeature_cols","a1bba64b":"from sklearn import tree\nfrom os import system\n\ntrain_char_label = ['No', 'Yes']\nCredit_Tree_FileR = open('credit_treeR.dot','w')\ndot_data = tree.export_graphviz(model_entropy, out_file=Credit_Tree_FileR, feature_names = list(X_train), class_names = list(train_char_label))\nCredit_Tree_FileR.close()\n\n#Works only if \"dot\" command works on you machine\n\nretCode = system(\"dot -Tpng credit_treeR.dot -o credit_treeR.png\")\nif(retCode>0):\n    print(\"system command returning error: \"+str(retCode))\nelse:\n    display(Image(\"credit_treeR.png\"))","716c459f":"print (pd.DataFrame(model_entropy.feature_importances_, columns = [\"Imp\"], index = X_train.columns))","203bf394":"# X_train, X_test, y_train, y_test\npreds_pruned_test = clf_pruned.predict(X_test)\npreds_pruned_train = clf_pruned.predict(X_train)\n\nprint(accuracy_score(y_test,preds_pruned_test))\nprint(accuracy_score(y_train,preds_pruned_train))","25928457":"#Predict for pruned train set\nmat_train = confusion_matrix(y_train, preds_pruned_train)\nprint(\"For pruned train confusion matrix = \\n\",mat_train)","2aa1c27e":"mat_test = confusion_matrix(y_test, preds_pruned_test)\n\nprint(\"For pruned test confusion matrix = \\n\",mat_test)","ef3807dc":"#Store the accuracy results for each model in a dataframe for final comparison\nacc_DT = accuracy_score(y_test, preds_pruned_test)\nresultsDf = pd.DataFrame({'Method':['Decision Tree'], 'accuracy': acc_DT})\nresultsDf = resultsDf[['Method', 'accuracy']]\nresultsDf","6445fa2a":"dTree = DecisionTreeClassifier(criterion = 'gini', random_state=1)\ndTree.fit(X_train, y_train)","42e59eb4":"print(dTree.score(X_train, y_train))\nprint(dTree.score(X_test, y_test))","3bd75935":"train_char_label = ['No', 'Yes']\nCredit_Tree_File = open('credit_tree_ginni.dot','w')\ndot_data = tree.export_graphviz(dTree, out_file=Credit_Tree_File, feature_names = list(X_train), class_names = list(train_char_label))\nCredit_Tree_File.close()","74b25d31":"\nretCode = system(\"dot -Tpng credit_tree_ginni.dot -o credit_tree_ginni.png\")\nif(retCode>0):\n    print(\"system command returning error: \"+str(retCode))\nelse:\n    display(Image(\"credit_tree_ginni.png\"))","bdfc7528":"dTreeR = DecisionTreeClassifier(criterion = 'gini', max_depth = 3, random_state=1)\ndTreeR.fit(X_train, y_train)\nprint(dTreeR.score(X_train, y_train))\nprint(dTreeR.score(X_test, y_test))","f53c0171":"train_char_label = ['No', 'Yes']\nCredit_Tree_FileR = open('credit_treeR.dot','w')\ndot_data = tree.export_graphviz(dTreeR, out_file=Credit_Tree_FileR, feature_names = list(X_train), class_names = list(train_char_label))\nCredit_Tree_FileR.close()\n\n#Works only if \"dot\" command works on you machine\n\nretCode = system(\"dot -Tpng credit_treeR.dot -o credit_treeR.png\")\nif(retCode>0):\n    print(\"system command returning error: \"+str(retCode))\nelse:\n    display(Image(\"credit_treeR.png\"))","b1e1d78c":"# importance of features in the tree building ( The importance of a feature is computed as the \n#(normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance )\n\nprint (pd.DataFrame(dTreeR.feature_importances_, columns = [\"Imp\"], index = X_train.columns))","8ab78534":"print(dTreeR.score(X_test , y_test))\ny_predict_DTginni = dTreeR.predict(X_test)\n\ncm_DTginni = metrics.confusion_matrix(y_test, y_predict_DTginni, )\n\nprint(\"DT TFor ginni confusion matrix = \\n\",cm_DTginni)","c2bc9a79":"from sklearn.ensemble import BaggingClassifier\n\nbgcl = BaggingClassifier(base_estimator=dTree, n_estimators=50,random_state=1)\n#bgcl = BaggingClassifier(n_estimators=50,random_state=1)\n\nbgcl = bgcl.fit(X_train, y_train)","e8fb90f1":"y_predict = bgcl.predict(X_test)\n\nprint(bgcl.score(X_test , y_test))","ed205f82":"cm_bagging = confusion_matrix(y_test, y_predict)\n\nprint(\"For Bagging confusion matrix = \\n\",cm_bagging)","379ec068":"from sklearn.ensemble import AdaBoostClassifier\nabcl = AdaBoostClassifier(n_estimators=50, random_state=1)\nabcl = abcl.fit(X_train, y_train)","b7b0b463":"y_predict_adaboost = abcl.predict(X_test)\nprint(abcl.score(X_test , y_test))","98a6cfc3":"cm_adaboost = metrics.confusion_matrix(y_test, y_predict_adaboost)\n\nprint(\"For adaboost confusion matrix = \\n\",cm_adaboost)","bd30af6a":"from sklearn.ensemble import GradientBoostingClassifier\ngbcl = GradientBoostingClassifier(n_estimators = 50,random_state=1)\ngbcl = gbcl.fit(X_train, y_train)","5f52f5d5":"y_predict_gradientboost = gbcl.predict(X_test)\nprint(gbcl.score(X_test, y_test))\ncm_gradientboost = metrics.confusion_matrix(y_test, y_predict_gradientboost)\nprint(\"For gradient boost confusion matrix = \\n\",cm_gradientboost)","da1961ef":"from sklearn.ensemble import RandomForestClassifier\nrfcl = RandomForestClassifier(n_estimators = 50, random_state=1,max_features=12)\nrfcl = rfcl.fit(X_train, y_train)","df377fb7":"y_predict_randomforest = rfcl.predict(X_test)\nprint(rfcl.score(X_test, y_test))\ncm_randomforest = metrics.confusion_matrix(y_test, y_predict_randomforest)\nprint(\"For gradient boost confusion matrix = \\n\",cm_randomforest)","2ca7bbd0":"## Ensemble Learning - Bagging","95aeb573":"## Ensemble RandomForest Classifier","17b12b4b":"**Reducing over fitting (Regularization)**","9da5c94a":"#### Print the accuracy of the model & print the confusion matrix","0f9e7900":"## Ensemble Learning - GradientBoost","cc210091":"## Visualizing the tree","7940eb4c":"## Decision Tree Ginni","9de8923d":"## Ensemble Learning - AdaBoosting","8b2a616f":"## Randomforest Classifier and Bagging Classifier are almost has the similar score of 90.7% which may predict the possibility of client will subscribe a term deposite.","83228413":"## Prune the DT\n#### There is a high degree of overfitting in the model due to which the test accuracy drops drastically. This shows why decision trees are prone to overfitting. \n\n#### Regularize\/prune the decision tree by limiting the max. depth of trees and print the accuracy.","8a7061c5":"## Create the decision tree model using \u201centropy\u201d method of finding the split columns and fit it to training data."}}