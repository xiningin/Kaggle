{"cell_type":{"058128ba":"code","540bbcb8":"code","3b2cf5e4":"code","f8955513":"code","7b1f9904":"code","1b5080f9":"code","a346f09e":"code","62ea6a20":"code","d5e343b5":"code","902fa1c3":"markdown","727bd8c0":"markdown","10ef3954":"markdown","91a7e29b":"markdown","3e48a1ae":"markdown"},"source":{"058128ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom glob import glob\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/A_DeviceMotion_data\/A_DeviceMotion_data\"))\n\n# Any results you write to the current directory are saved as output.","540bbcb8":"folders = glob('..\/input\/A_DeviceMotion_data\/A_DeviceMotion_data\/*_*')\nfolders = [s for s in folders if \"csv\" not in s]\ndf_all_list = []\nactivity_codes = {'dws':0,'jog':1,'sit':2,'std':3,'ups':4,'wlk':5}\nactivity_types = list(activity_codes.keys())\n\nfor j in folders:\n    #print('j',j)\n    csv = glob(j + '\/*')\n    for i in csv:\n        df = pd.read_csv(i)\n        df['activity'] = activity_codes[j[49:52]]\n        df['sub_num'] = i[len(j)+5:-4]\n        expnum = np.zeros(df.shape[0])\n        df_all_list.append(df)\ndf_all = pd.concat(df_all_list,axis=0)\ndf_all = df_all.drop('Unnamed: 0',axis=1)\nprint(df_all.shape)\nprint(df_all.columns)","3b2cf5e4":"for act in activity_types:\n    plt.subplot('61'+str(activity_codes[act]))\n    plt.subplots_adjust(hspace=1.0)\n    df = df_all[(df_all['sub_num']=='1') & (df_all['activity']==activity_codes[act])]\n    plt.title(act)\n    plt.plot(df['userAcceleration.z'][:400])\n    plt.xticks([]) # turn off x labels\n    plt.yticks([])  # turn off y labels","f8955513":"segment_size = 400\ndata_all_x_list = []\ndata_all_y_list = []\nfor j in folders:\n    #print('j',j)\n    csv = glob(j + '\/*')\n    for i in csv:\n        df = pd.read_csv(i)\n        df = df.drop('Unnamed: 0',axis=1)\n        win_count = int(df.shape[0]\/segment_size)\n        data_x = np.zeros((win_count,segment_size,df.shape[1]))\n        data_y = np.zeros(win_count)\n        for c in range(win_count):\n            start_idx = c*segment_size\n            end_idx = start_idx + segment_size\n            data_x[c,:,:] = df[start_idx:end_idx].values\n            data_y[:] = activity_codes[j[49:52]]\n        data_all_x_list.append(data_x)\n        data_all_y_list.append(data_y)\ndata_all_x = np.concatenate(data_all_x_list,axis=0)\ndata_all_y = np.concatenate(data_all_y_list,axis=0)\ndata_all_y = data_all_y.astype(int)\nprint(data_all_x.shape)\nprint(data_all_y.shape)","7b1f9904":"def cnn_model_fn(features,labels,mode):\n    conv1 = tf.layers.conv1d(inputs=features,\n                             filters=32,\n                             kernel_size=5,\n                             padding='same',\n                             data_format='channels_last',\n                             activation=tf.nn.relu)\n    print('conv1.shape',conv1.shape)\n    pool1 = tf.layers.max_pooling1d(inputs=conv1,pool_size=2,strides=2)\n    print('pool1.shape',pool1.shape)\n    \n    conv2 = tf.layers.conv1d(inputs=pool1,\n                             filters=64,\n                             kernel_size=5,\n                             padding='same',\n                             data_format='channels_last',\n                             activation=tf.nn.relu)\n    print('conv2.shape',conv2.shape)\n    pool2 = tf.layers.max_pooling1d(inputs=conv2,pool_size=2,strides=2)\n    print('pool2.shape',pool2.shape)\n    \n    pool2_flat = tf.reshape(pool2,[-1,100*64])  \n    dense1 = tf.layers.dense(inputs=pool2_flat,units=500,activation=tf.nn.relu)\n    \n    dropput = tf.layers.dropout(inputs=dense1,rate=0.1,training=(mode==tf.estimator.ModeKeys.TRAIN))\n    \n    dense2 =  tf.layers.dense(inputs=dropput,units=100,activation=tf.nn.relu)\n    \n    logits = tf.layers.dense(inputs=dense2,units=6)\n    \n    predictions = { 'classes':tf.arg_max(logits,dimension=1),\n                 'probabilites': tf.nn.softmax(logits,name = 'softmax_tensor')}\n    \n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(mode=mode,predictions=predictions)\n    \n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels,logits=logits)\n    \n    if mode == tf.estimator.ModeKeys.TRAIN:\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n        train_op = optimizer.minimize(loss=loss,global_step=tf.train.get_global_step())\n        return tf.estimator.EstimatorSpec(mode=mode,loss=loss,train_op=train_op)\n    \n    eval_metric_op = {'accuracy': tf.metrics.accuracy(labels=labels,predictions=predictions['classes'])}\n    return tf.estimator.EstimatorSpec(mode=mode,loss=loss,eval_metric_ops=eval_metric_op)","1b5080f9":"train_x,test_x,train_y,test_y = train_test_split(data_all_x,data_all_y,test_size=0.1)","a346f09e":"har_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn,model_dir='\/tmp\/har_classifer_model')","62ea6a20":"train_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x = train_x,\n    y = train_y,\n    batch_size=10,\n    num_epochs=None,\n    shuffle=True)\n\n\nhar_classifier.train(\n    input_fn=train_input_fn,\n    steps=20000)\n    #hooks=[logging_hook])","d5e343b5":"test_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x = test_x,\n    y = test_y,\n    num_epochs=1,\n    shuffle=False)\n\neval_results = har_classifier.evaluate(input_fn=test_input_fn)\nprint(eval_results)","902fa1c3":"### Data Preprocessing \nFor convolution neural network the input data needs to be in a particular format. We will organize data windows of 400 datapoints with 12 channels (one channel per reading).","727bd8c0":"### Visualization\nLet's visualize one timeseries per acitivty","10ef3954":"### Convolution Neural Network\nFollowing the architecture we will be training for this problem.\n<br>\n<a href=\"https:\/\/imgur.com\/K9YozCL\"><img src=\"https:\/\/i.imgur.com\/K9YozCL.jpg\" title=\"source: imgur.com\" \/><\/a>\n<br>\nThe activation used is <b> relu<\/b>\n<br> the probabilities for each class is calculated applying <b> SoftMax<\/b> on last layer (Logit)","91a7e29b":"### Load Data\nThe dataset includes time-series data generated by accelerometer and gyroscope sensors (attitude, gravity, userAcceleration, and rotationRate). It is collected with an iPhone 6s kept in the participant's front pocket.\n<br><br>\nThe data is organized in multiple folders. Each folder contains 24 data files (one for each subject) for a particular acitivty. <br>\ndws: Walking Downstairs\n<br>\njog: Jogging\n<br>\nsit: Sitting\n<br>\nstd: Standing \n<br>\nups: Walking Upstairs\n<br>\nwlk: Walking","3e48a1ae":"### Results\nThe accuracy achieved on Test set is <b> more then 96% <\/b>"}}