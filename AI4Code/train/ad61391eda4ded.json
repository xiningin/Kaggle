{"cell_type":{"68288337":"code","1d8de867":"code","885e000f":"code","51e9ddf2":"code","ce15740c":"code","4982f24c":"code","c52c6ca8":"code","b88ff7d8":"code","c108189a":"code","e31a9662":"code","ff8b5baa":"code","367bddd7":"code","804c1125":"code","357553ac":"code","4bd545b0":"markdown","590fd7e4":"markdown","a805bb85":"markdown"},"source":{"68288337":"# Importing required libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris #In-built dataset for Iris Flower Classification by Scikit-Learn\nfrom sklearn.model_selection import train_test_split #For spliting the data into train and test set\n\n#Ignore Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","1d8de867":"#Loading our data\niris = load_iris()\nprint(iris.DESCR)","885e000f":"#Seperating our feature variable and converting target variable to binary form\nx = iris[\"data\"][:, 2:] #Petal Length & width\ny = (iris[\"target\"] == 2).astype(np.int) #1 for virginica and 0 for non-virginica","51e9ddf2":"print('X shape : ', x.shape)\nprint('Y shape : ', y.shape)","ce15740c":"#Visualizing our final data\ndata = pd.DataFrame(x, columns = [\"Petal Length\", \"Petal Width\"])\ndata[\"Virginica\"] = y\ndata.head()","4982f24c":"#Again seperating the feature and target variable from the above dataframe\n#Note: The seperating here can be skipped as we had already seperated the same before, but for clearity I have did again.\nX = data.drop(columns = [\"Virginica\"])\ny = data[\"Virginica\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, random_state = 42)","c52c6ca8":"print('X_train Shape : ', X_train.shape)\nprint('X_test Shape : ', X_test.shape)\nprint('y_train Shape : ', y_train.shape)\nprint('y_test Shape : ', y_test.shape)","b88ff7d8":"def logistic(X, y, alpha = 0.2, epoch = 2):\n    X = np.array(X)\n    y = np.array(y)\n    b0_list, b1_list, b2_list = [], [], []\n    global b0, b1, b2, pred\n    b0, b1, b2 = 0, 0, 0\n    for j in range(1, epoch+1):\n        print(f\"Epoch : {j}\/{epoch}\\n\")\n        for i in range(0, len(X)):\n            pred = 1\/(1+np.exp(-(b0+ b1 * x[i][0] + b2 * x[i][0])))\n            b0 = b0 + alpha * (y[i] - pred) * pred * (1 - pred)\n            b1 = b1 + alpha * (y[i] - pred) * pred * (1 - pred) * x[i][0]\n            b2 = b2 + alpha * (y[i] - pred) * pred * (1 - pred) * x[i][1]\n            b0_list.append(b0)\n            b1_list.append(b1)\n            b2_list.append(b2)\n        print(f\"Epoch {j} Completed\")\n        print(f\"b0 : {b0}\\nb1 : {b1}\\nb2 : {b2}\\n\")","c108189a":"def predict(x, b0, b1, b2):\n    global pred_list\n    x = np.array(x)\n    pred_list = []\n    for i in range(0, len(x)):\n        temp = 1\/(1+np.exp(-(b0 + b1 * x[i][0] + b2 * x[i][1])))\n        pred_list.append(temp)\n    return pred_list","e31a9662":"def crisp_logistic(threshold):\n    for i in range(0, len(pred_list)):\n        if pred_list[i] >= threshold:\n            pred_list[i] = 1\n        elif pred_list[i] < threshold:\n            pred_list[i] = 0\n    return np.array(pred_list)","ff8b5baa":"def pred_score(y_true, y_pred):\n    true = 0\n    y_true = np.array(y_true)\n    for i in range(0, len(y_true)):\n        if y_pred[i] == y_true[i]:\n            true+=1\n        else:\n            continue\n    accuracy = (true\/len(y_pred))*100\n    return accuracy","367bddd7":"logistic(X_train, y_train, epoch = 35)","804c1125":"predict(X_test, b0, b1, b2)","357553ac":"crisp_logistic(0.5)\npred_score(y_test, pred_list)","4bd545b0":"# Logistic Regression\n## Artificial Intelligence - The Better Way\nThis notebook would give an overview of low-level implementation of Linear Regression using Normal Equation.\n\n> The methods used in the notebook can't be considered efficient as it would be potratying how stuffs are cooked under the hood.","590fd7e4":"We will be creating a logistic regression model for binary classification, thus our model will be predicting if the flower is a virginica or not, for this model we will be ignoring other classes in target variable","a805bb85":"So here out model predicted with an accuracy of 81% on test data, the model is quite good.\n\n**Hooray!!**\nWe have created logistic regression model from scratch with efficient accuracy.\n<hr>\n\n***Disclaimer***\n* For detailed explaination of the same, you can visit [here](https:\/\/medium.com\/analytics-vidhya\/ml-algorithms-from-scratch-2-2c1aea2bff3b). \n* GitHub Repo for the same notebook is included in my blog itself.\n* Upvote if you like the content, and stay tuned for developing other algorithms from scratch!\n\n<hr>\n\n***Connect***<br>\n[LinkedIn](https:\/\/linkedin.com\/in\/rithuraj-nambiar) &nbsp; [GitHub](https:\/\/github.com\/rithurajnambiar17) &nbsp;  [Kaggle](https:\/\/www.kaggle.com\/rithurajnambiar) &nbsp; [Medium](https:\/\/medium.com\/@rithurajnambiar)"}}