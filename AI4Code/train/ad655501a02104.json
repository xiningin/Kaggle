{"cell_type":{"fddd02ca":"code","33890b54":"code","ef5f15ff":"code","4072bd4a":"code","5cef5978":"code","d7d3d617":"code","36b58130":"code","3db976c5":"code","d4677e1c":"code","48a07e68":"code","d2304455":"code","8ae60268":"code","f5b30022":"code","b5b2cd65":"code","99a35146":"code","892ad43a":"code","2a56c684":"code","da892fde":"code","c21f77e9":"code","541b9dd1":"code","187dfcb0":"code","19c3e0fc":"code","3b6900eb":"code","a2242cbf":"code","0fb8200c":"code","6cf6f1c4":"code","2c39e265":"code","d9dc1d5c":"code","3a499387":"code","b606cb7e":"code","c0a61e1e":"code","b3e857a0":"code","74a52d1f":"code","6b4d1838":"code","043b5e5b":"code","1ffa3f49":"code","e9e6e4c9":"code","f22b7b7b":"code","a2682ddf":"code","8c5e11a0":"code","64dda2d6":"code","cb1e240b":"code","4a7ae89f":"code","070facc8":"code","2ecd7c5c":"code","22e639a0":"markdown","156343f0":"markdown","88630b1b":"markdown","be3974cc":"markdown"},"source":{"fddd02ca":"import pandas as pd                                                                #used for data manipulation and analysis\nimport numpy as np                                              #high performance multidimensional array processing package\nimport matplotlib.pyplot as plt                                                                         #visualisation tool\nimport seaborn as sns                                                                                   #visualisation tool\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","33890b54":"df = pd.read_csv(\"..\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv\")\ndf.head()","ef5f15ff":"df.head()","4072bd4a":"df.shape","5cef5978":"print(f\"The dataset has {df.shape[0]} rows and {df.shape[1]} columns\")","d7d3d617":"df.info()","36b58130":"df.describe()","3db976c5":"sns.pairplot(df,hue='Loan_Status')","d4677e1c":"Categorical_features=[feature for feature in df.columns if df[feature].dtypes == 'object']\nCategorical_features","48a07e68":"Numerical_features=[feature for feature in df.columns if df[feature].dtypes != 'object']\nNumerical_features","d2304455":"Continous_features=[]\nDiscrete_features=[]\nfor feature in Numerical_features:\n    print(f'Number of unique values in column {feature} is {df[feature].nunique()}')\n    if df[feature].nunique()>15:\n        Continous_features.append(feature)\n    else:\n        Discrete_features.append(feature)","8ae60268":"#Taking columns having unique values less than 15 as discrete columns\nContinous_features","f5b30022":"Discrete_features","b5b2cd65":"for feature in Categorical_features[1:]:\n    plt.figure(figsize=(12,6))\n    sns.countplot(data=df,x=feature,palette=\"rainbow\",hue='Loan_Status')\n    plt.title(f\"{feature} Countplot\")\n    plt.tight_layout()\n    plt.xlabel(\"\")\n    plt.show()","99a35146":"for feature in Discrete_features:\n    plt.figure(figsize=(12,6))\n    sns.countplot(data=df,x=feature,palette=\"rainbow\",hue='Loan_Status')\n    plt.title(f\"{feature} Countplot\")\n    plt.tight_layout()\n    plt.xlabel(\"\")\n    plt.show()","892ad43a":"for feature in Continous_features:\n    plt.figure(figsize=(12,6))\n    sns.histplot(df[feature],palette=\"rainbow\",kde=True)\n    plt.title(f\"{feature} Distribution\")\n    plt.tight_layout()\n    plt.show()","2a56c684":"#missing values\nfor feature in df.columns:\n    print(f'No. of missing values in column {feature} is {sum(df[feature].isnull())}')\n    print(f'NUll value percentage : {round(np.mean(df[feature].isnull())*100,2)} % \\n')","da892fde":"#it can be seen that all the columns except Credit_history have less than 5 percent of null value\n#can be easily be substituted by suitable central tendacy \n#in case of categorical and discrete mode will be suitable\n#while for continous values we can make use of boxplot to make the decision\nplt.figure(figsize=(15,15),dpi=150)\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='magma')","c21f77e9":"for feature in Categorical_features+Discrete_features:\n    df[feature].fillna(df[feature].mode()[0],inplace=True)","541b9dd1":"for feature in Continous_features:\n    plt.figure(figsize=(12,6))\n    sns.boxplot(data = df, y=feature)\n    plt.title(f\"{feature}\")\n    plt.tight_layout()\n    plt.show()","187dfcb0":"#since there are large numbers of outliers we are gonna fill the null values with median","19c3e0fc":"for feature in Continous_features:\n    df[feature].fillna(np.nanmedian(df[feature]),inplace=True)","3b6900eb":"plt.figure(figsize=(15,15),dpi=150)\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='magma')","a2242cbf":"#understanding the coleration","0fb8200c":"df.corr()","6cf6f1c4":"plt.figure(figsize=(15,15))\nsns.heatmap(df.corr(),annot=True,cmap='rainbow')","2c39e265":"df.head()","d9dc1d5c":"#Feature Engineering","3a499387":"#creating a copy and working o new dataframe\ndata=df.copy()","b606cb7e":"data.columns","c0a61e1e":"data[\"Total_Income\"]=data[\"ApplicantIncome\"]+data[\"CoapplicantIncome\"]","b3e857a0":"data.drop(['Loan_ID','ApplicantIncome', 'CoapplicantIncome','Dependents'],inplace=True,axis=1)","74a52d1f":"Encoded_data=pd.DataFrame()","6b4d1838":"for feature in data.columns:\n    if(data[feature].nunique()>3):\n        pass\n    else:\n        for i in range(data[feature].nunique()-1):\n            Encoded_data[feature]=np.where(str(data[feature].unique()[i]) == data[feature],1,0)","043b5e5b":"Encoded_data[\"LoanAmount\"]=data[\"LoanAmount\"]\nEncoded_data[\"Loan_Amount_Term\"]=data[\"Loan_Amount_Term\"]\nEncoded_data[\"Total_Income\"]=data[\"Total_Income\"]","1ffa3f49":"Encoded_data[\"Credit_History\"]=data[\"Credit_History\"]\n","e9e6e4c9":"Encoded_data.head()","f22b7b7b":"Standard_data=Encoded_data.copy()\nStandard_data['LoanAmount']=(Standard_data['LoanAmount']-Standard_data['LoanAmount'].min())\/(Standard_data['LoanAmount'].max()-Standard_data['LoanAmount'].min())\nStandard_data['Loan_Amount_Term']=(Standard_data['Loan_Amount_Term']-Standard_data['Loan_Amount_Term'].min())\/(Standard_data['Loan_Amount_Term'].max()-Standard_data['Loan_Amount_Term'].min())\nStandard_data['Total_Income']=(Standard_data['Total_Income']-Standard_data['Total_Income'].min())\/(Standard_data['Total_Income'].max()-Standard_data['Total_Income'].min())\nStandard_data.head()","a2682ddf":"#we have scaled our data \n#since output is not given for test dataset \n#to check the performance we are gonna split our train dataset and work \nx = Standard_data.drop(columns=\"Loan_Status\", axis=1)\ny = Standard_data['Loan_Status']","8c5e11a0":"x_train, x_cv, y_train, y_cv = train_test_split(x, y, test_size=0.2)","64dda2d6":"#using logistic regression model\nloreg = LogisticRegression()\n\nloreg.fit(x_train, y_train)\ny_pred=loreg.predict(x_cv)","cb1e240b":"print(\"Train Accuracy: \", loreg.score(x_cv, y_cv) * 100,' %')","4a7ae89f":"print(classification_report(y_cv,y_pred))","070facc8":"print(confusion_matrix(y_cv,y_pred))","2ecd7c5c":"score = cross_val_score(loreg, x, y, cv=5)\nprint(\"Cross validation is\",np.mean(score)*100)","22e639a0":"## Exploratory Data Analysis","156343f0":"### Understanding the dataset","88630b1b":"### Importing the dataset","be3974cc":"## Importing Libraries"}}