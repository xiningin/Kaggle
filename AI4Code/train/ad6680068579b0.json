{"cell_type":{"e73eab61":"code","957f108c":"code","b6ea8def":"code","28a18b24":"code","0bc26ad5":"code","f021dd25":"code","194da18b":"code","d24c3593":"code","15c7891f":"code","0f858c04":"code","878e1ad0":"code","ce3cb2e2":"code","70b3bf1b":"code","f0052189":"code","a0a70f00":"code","62cc287d":"code","0ba1b87a":"code","ecc28e71":"code","c58e8168":"code","01ce391b":"code","90503def":"code","c773ed0a":"code","4e8a6da6":"code","2ce7753b":"code","2a27151f":"code","d30ff604":"code","73de4213":"code","067a076b":"markdown","497de299":"markdown","d538190b":"markdown","675cdce5":"markdown","7b6a76fa":"markdown","d18917bc":"markdown","d85b3046":"markdown","e2d14b9a":"markdown","2aece551":"markdown","52f6d321":"markdown","f4f606c6":"markdown","d566d804":"markdown","041aaf23":"markdown","d1883157":"markdown","90618b8d":"markdown"},"source":{"e73eab61":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport random\nimport warnings\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, plot_confusion_matrix, plot_precision_recall_curve\n\nwarnings.simplefilter(\"ignore\")","957f108c":"def plot_metric(clf, testX, testY, name):\n    \"\"\"\n    Small function to plot ROC-AUC values and confusion matrix\n    \"\"\"\n    styles = ['bmh', 'classic', 'fivethirtyeight', 'ggplot']\n\n    plt.style.use(random.choice(styles))\n    plot_confusion_matrix(clf, testX, testY)\n    plt.title(f\"Confusion Matrix [{name}]\")","b6ea8def":"data = pd.read_csv(\"..\/input\/60k-stack-overflow-questions-with-quality-rate\/data.csv\")\ndata.head()","28a18b24":"data = data.drop(['Id', 'Tags', 'CreationDate'], axis=1)\ndata['Y'] = data['Y'].map({'LQ_CLOSE':0, 'LQ_EDIT': 1, 'HQ':2})\ndata.head()","0bc26ad5":"labels = ['Open Questions', 'Low Quality Question - Close', 'Low Quality Question - Edit']\nvalues = [len(data[data['Y'] == 2]), len(data[data['Y'] == 0]), len(data[data['Y'] == 1])]\nplt.style.use('classic')\nplt.figure(figsize=(16, 9))\nplt.pie(x=values, labels=labels, autopct=\"%1.1f%%\")\nplt.title(\"Target Value Distribution\")\nplt.show()","f021dd25":"data['text'] = data['Title'] + ' ' + data['Body']\ndata = data.drop(['Title', 'Body'], axis=1)\ndata.head()","194da18b":"# Clean the data\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r'[^(a-zA-Z)\\s]','', text)\n    return text\ndata['text'] = data['text'].apply(clean_text)","d24c3593":"# Define how much percent data you wanna split\nsplit_pcent = 0.20\nsplit = int(split_pcent * len(data))\n\n# Shuffles dataframe\ndata = data.sample(frac=1).reset_index(drop=True)\n\n# Training Sets\ntrain = data[split:]\ntrainX = train['text']\ntrainY = train['Y'].values\n\n# Validation Sets\nvalid = data[:split]\nvalidX = valid['text']\nvalidY = valid['Y'].values\n\nassert trainX.shape == trainY.shape\nassert validX.shape == validY.shape\n\nprint(f\"Training Data Shape: {validX.shape}\\nValidation Data Shape: {validX.shape}\")","15c7891f":"# Load the vectorizer, fit on training set, transform on validation set\nvectorizer = TfidfVectorizer()\ntrainX = vectorizer.fit_transform(trainX)\nvalidX = vectorizer.transform(validX)","0f858c04":"# Define and fit the classifier on the data\nlr_classifier = LogisticRegression(C=1.)\nlr_classifier.fit(trainX, trainY)","878e1ad0":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of Logsitic Regression Classifier is: {(lr_classifier.score(validX, validY))*100:.2f}%\")","ce3cb2e2":"# Also plot the metric\nplot_metric(lr_classifier, validX, validY, \"Logistic Regression\")","70b3bf1b":"# Define and fit the classifier on the data\nnb_classifier = MultinomialNB()\nnb_classifier.fit(trainX, trainY)","f0052189":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of Naive Bayes Classifier is: {(nb_classifier.score(validX, validY))*100:.2f}%\")","a0a70f00":"# Also plot the metric\nplot_metric(nb_classifier, validX, validY, \"Naive Bayes\")","62cc287d":"# Define and fit the classifier on the data\nrf_classifier = RandomForestClassifier()\nrf_classifier.fit(trainX, trainY)","0ba1b87a":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of Random Forest Classifier is: {(rf_classifier.score(validX, validY))*100:.2f}%\")","ecc28e71":"# Also plot the metric\nplot_metric(nb_classifier, validX, validY, \"Random Forest\")","c58e8168":"# Define and fit the classifier on the data\ndt_classifier = DecisionTreeClassifier()\ndt_classifier.fit(trainX, trainY)","01ce391b":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of Decision Tree Clf. is: {(dt_classifier.score(validX, validY))*100:.2f}%\")","90503def":"# Also plot the metric\nplot_metric(dt_classifier, validX, validY, \"Decision Tree Classifier\")","c773ed0a":"# Define and fit the classifier on the data\nkn_classifier = KNeighborsClassifier()\nkn_classifier.fit(trainX, trainY)","4e8a6da6":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of KNN Clf. is: {(kn_classifier.score(validX, validY))*100:.2f}%\")","2ce7753b":"# Also plot the metric\nplot_metric(dt_classifier, validX, validY, \"Decision Tree Classifier\")","2a27151f":"# Define and fit the classifier on the data\nxg_classifier = XGBClassifier()\nxg_classifier.fit(trainX, trainY)","d30ff604":"# Print the accuracy score of the classifier\nprint(f\"Validation Accuracy of XGBoost Clf. is: {(xg_classifier.score(validX, validY))*100:.2f}%\")","73de4213":"# Also plot the metric\nplot_metric(xg_classifier, validX, validY, \"XGBoost Classifier\")","067a076b":"## 5. KNN Classifier\nWe now are going to use KNN Classifier for this task.","497de299":"## Vectorizing the Data\nLet's vectorize the data so it's in the numerical format","d538190b":"<p style=\"color:red\">If you like this notebook, please make sure to give an upvote, it helps a lot and motivates me to make much more good-quality content<\/p>\n<p style=\"color:blue\">If you don't like my work, please leave a comment on what can I do to make it better!<\/p>\n<hr>\n<h3 style=\"color:aqua\">Edits:<\/h3>\n<ul>\n<li style=\"color:green\">All Classifiers now classify for all 3 categories and not just 2. Good Validation Accuracy is maintained.<\/li>\n<\/ul>","675cdce5":"## 1. Logistic Regression\nLet's first start with our good old, Logistic Regression!","7b6a76fa":"Read the data and don't use the low quality edit data","d18917bc":"## 2. Multinomial Naive Bayes\nLet's now switch to the naive the bayes, the NAIVE BAYES!","d85b3046":"# Modelling\nLet's start with different non-deep learning approaches for this task.","e2d14b9a":"# Data Preprocessing and Some EDA","2aece551":"Let's join the title and the body of the text data so that we can use both of them in our classification","52f6d321":"## Splitting the Data\nLet's now split the dataset into training and validation sets","f4f606c6":"## 4. Decision Tree Classifier\nLet's now take some decisions using the Decision Tree Classifer","d566d804":"All the open questions are grouped under a single class (1), while the closed one is grouped under (0)","041aaf23":"# Introduction and Imports\n\n![Credit: Pexels](https:\/\/images.pexels.com\/photos\/3862130\/pexels-photo-3862130.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260)\n\nIn this notebook, I will be using only **Machine Learning** methods to get decent prediction scores. There are much better and sophisticated ways (like RNN, GRU, Fine-tuning BERT, etc) but you have seen them on a lot of notebook already.\n\nThe main aim of this notebook is to just show how quickly and easily you can do Text Classification using Basic Machine Learning Methods, rather than spend waiting 1 hour for a model to train!","d1883157":"## 3. Random Forest Classifier\nLet's now enter the forest with the Random Forest Classifier and see where it takes us!","90618b8d":"## 6. XGBoost\nFinally, let's use the XGBoost Classifier and then we'll compare all the different classifiers so far"}}