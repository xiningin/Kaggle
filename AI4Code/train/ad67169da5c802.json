{"cell_type":{"5f41e13f":"code","4a379738":"code","14cc13f3":"code","b5f0a9a0":"code","672cc039":"code","692e0a81":"code","c7bf3947":"code","a471df9f":"code","1e350eec":"code","20b94c5e":"code","d58ceb56":"code","8941b45f":"code","845315bf":"code","37ad060c":"code","fcfd86c4":"code","9d1c01ff":"markdown","6e89be2a":"markdown","401ed32c":"markdown","f1525326":"markdown","908430e3":"markdown","91a6b43a":"markdown","75502298":"markdown","cc89bea0":"markdown","06f4b961":"markdown","48180840":"markdown","2d0c36b5":"markdown"},"source":{"5f41e13f":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import mean_squared_error \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.pipeline import Pipeline\nimport matplotlib.pyplot as plt\nimport category_encoders as ce\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd \nimport random as rand","4a379738":"#Import training and testing data\ntrain_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")","14cc13f3":"#Which columns have missing values?\ndisplay(train_data.isnull().sum().sort_values(ascending=False))\ndisplay(test_data.isnull().sum().sort_values(ascending=False))","b5f0a9a0":"#Cabin letter\ntrain_data['Cabin_new'] = train_data['Cabin'].str[0]\ntest_data['Cabin_new'] = test_data['Cabin'].str[0]\n\n#Family size\ntrain_data['Fam_size'] = train_data['SibSp'] + train_data['Parch']\ntest_data['Fam_size'] = test_data['SibSp'] + test_data['Parch']\n\n#Title\ntrain_data['Title']=train_data.Name.str.extract('([A-Za-z]+)\\.')\ntest_data['Title']=test_data.Name.str.extract('([A-Za-z]+)\\.')\n\n#Sex-Class\ntrain_data['Sex_class'] = train_data['Sex'] + \"_\" + str(train_data['Pclass'])\ntest_data['Sex_class'] = test_data['Sex'] + \"_\" + str(test_data['Pclass'])","672cc039":"#Fare\ntrain_data['Fare'].fillna(train_data['Fare'].median(), inplace=True)\ntest_data['Fare'].fillna(train_data['Fare'].mean(), inplace=True)\n\n#Age\ntrain_data['Age'].fillna(train_data['Age'].mean(), inplace=True)\ntest_data['Age'].fillna(train_data['Age'].mean(), inplace=True)\n\n#Cabin_new\ntrain_data['Cabin_new'].fillna('X', inplace=True)\ntest_data['Cabin_new'].fillna('X', inplace=True)\n\n#Embarked\ntrain_data['Embarked'].fillna('S', inplace=True) #most common value","692e0a81":"#Setup for model\ny = train_data[\"Survived\"]\nfeatures = [\"Pclass\", \"Sex\", \"Parch\", \"SibSp\", \"Fam_size\", \"Embarked\", \"Fare\", \"Age\", \"Cabin_new\", \"Title\", \"Sex_class\"]\ncat_features = ['Sex', 'Embarked', 'Pclass', \"Cabin_new\", \"Title\", \"Sex_class\"]\n\nX = train_data[features]\nX_test = test_data[features]","c7bf3947":"# Create the count encoder\ncount_enc = ce.CountEncoder(cols=cat_features)\n\n# Learn encoding from the training set\ncount_enc.fit(X[cat_features])\n\n# Apply encoding to the train and validation sets as new columns\n# Make sure to add `_count` as a suffix to the new columns\ntrain_encoded = X.join(count_enc.transform(X[cat_features]).add_suffix('_count'))\ntest_encoded = X_test.join(count_enc.transform(X_test[cat_features]).add_suffix('_count'))","a471df9f":"# Create the CatBoost encoder\ncb_enc = ce.CatBoostEncoder(cols=cat_features, random_state=126)\n\n# Learn encoding from the training set\ncb_enc.fit(X[cat_features], y)\n\n# Apply encoding to the train and validation sets as new columns\n# Make sure to add `_cb` as a suffix to the new columns\ntrain_encoded = X.join(cb_enc.transform(X[cat_features]).add_suffix('_cb'))\ntest_encoded = X_test.join(cb_enc.transform(X_test[cat_features]).add_suffix('_cb'))","1e350eec":"#Update X for Count Encoding\n#features_encoded = [\"Pclass_count\", \"Sex_count\", \"Fam_size\", \"Embarked_count\", \"Fare\", \"Age\", \"Sex_class_count\"]\n#X = train_encoded[features_encoded]\n#X_test = test_encoded[features_encoded]\n\n#Update X for CatBoost Encoding\nfeatures_encoded = [\"Pclass_cb\", \"Sex_cb\", \"Embarked_cb\", \"Fam_size\", \"Fare\", \"Age\", \"Sex_class_cb\"]\nX = train_encoded[features_encoded]\nX_test = test_encoded[features_encoded]","20b94c5e":"scaler = StandardScaler() #to normalize data for neural net\n\nmodel_rf = RandomForestClassifier(random_state=126)\nmodel_xgb = XGBClassifier(random_state=126)\nmodel_mlp = MLPClassifier(random_state=126)\n\nparam_grid_rf = {\n    'model__n_estimators': [90, 110, 130],\n    'model__max_depth': [7, 10, 13],\n    'model__criterion': ['gini', 'entropy']}\n\nparam_grid_xgb = {\n    'model__n_estimators': [150, 180, 210],\n    'model__max_depth': [5, 8],\n    'model__learning_rate': [0.08, 0.1, 0.12]}\n\nparam_grid_mlp = {'model__hidden_layer_sizes': [(25, 25, 25), (50,50,50)],\n                  'model__activation': ['tanh', 'relu'],\n                  'model__solver': ['sgd', 'adam'],\n                  'model__alpha': [0.0001, 0.00001],\n                  'model__learning_rate': ['constant','adaptive']}","d58ceb56":"my_pipeline = Pipeline(steps=[('model', model_rf)])\n\nsearch = GridSearchCV(my_pipeline, param_grid_rf, n_jobs=-1, verbose=10, cv=5)\nsearch.fit(X, y)\nprint(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\nprint(search.best_params_)","8941b45f":"my_pipeline = Pipeline(steps=[('model', model_xgb)])\n\nsearch = GridSearchCV(my_pipeline, param_grid_xgb, n_jobs=-1, verbose=10, cv=5)\nsearch.fit(X, y)\nprint(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\nprint(search.best_params_)","845315bf":"#MLP \nmy_pipeline = Pipeline(steps=[('preprocess', scaler), \n                              ('model', model_mlp)])\n\nsearch = GridSearchCV(my_pipeline, param_grid_mlp, n_jobs=-1, verbose=10, cv=5)\nsearch.fit(X, y)\nprint(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\nprint(search.best_params_)","37ad060c":"#Building a voting classifier \nmodel_rf = RandomForestClassifier(random_state=126, criterion='entropy', max_depth=10, n_estimators=110)\nmodel_xgb = XGBClassifier(random_state=126, learning_rate=0.1, max_depth=5, n_estimators=180)\nmodel_mlp = MLPClassifier(random_state=126, activation='tanh', alpha=1e-04, hidden_layer_sizes=(25, 25, 25), learning_rate='constant', solver='adam')\n\nmodel_vote = VotingClassifier(estimators=[('RF', model_rf), ('XGB', model_xgb), ('MLP', model_mlp)], voting='hard')\nmodel_vote.fit(X, y)","fcfd86c4":"predictions = model_vote.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","9d1c01ff":"# Feature engineering","6e89be2a":"Visualizations created during the development of the model herein can be found here:\nhttps:\/\/www.kaggle.com\/db102291\/titanic-competition-visualization-w-seaborn","401ed32c":"## Random Forest","f1525326":"## XGBoost","908430e3":"## Multi-layer Perceptron","91a6b43a":"# Imputation","75502298":"# Prediction","cc89bea0":"# Missing values?","06f4b961":"# Voting Classifier","48180840":"# Encoding","2d0c36b5":"# Hyperparameter Tuning"}}