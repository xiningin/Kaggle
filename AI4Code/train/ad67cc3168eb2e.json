{"cell_type":{"7f551819":"code","75d38912":"code","c7cdf73c":"code","0e4816e9":"code","06b62040":"code","8be66055":"code","50909003":"code","bb85b3d8":"code","69433cf8":"code","2af56ac7":"code","dc233bc7":"code","b6255e34":"code","0f4e6456":"code","e348e4ed":"code","d4399fc3":"code","71a3aab1":"code","aa9bfab1":"code","c0ab7fbc":"code","2598aeda":"markdown","bdf971af":"markdown","e85f10b9":"markdown","4f689047":"markdown","489140fe":"markdown","e9c8d8a1":"markdown","52964be0":"markdown","9d4d107f":"markdown","22718154":"markdown","21603b22":"markdown","733fd831":"markdown","831b5fdb":"markdown","17427b1a":"markdown","68c5c8c7":"markdown","ab3b67fd":"markdown","49c66fde":"markdown"},"source":{"7f551819":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","75d38912":"import pandas as pd\nimport re\n\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.tokenize import RegexpTokenizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import LogisticRegression","c7cdf73c":"irish_data = pd.read_csv('..\/input\/ireland-historical-news\/irishtimes-date-text.csv')","0e4816e9":"print(\"The shape of the data is \",irish_data.shape)\nirish_data.head()","06b62040":"stop_words = set(stopwords.words('english')) \nlem = WordNetLemmatizer()\nlem.lemmatize('Ready')","8be66055":"def remove_stopwords(line):\n    word_tokens = word_tokenize(line)  \n    filtered_words = [re.sub('[^A-Za-z]+', '', w.lower()) for w in word_tokens if not w in stop_words]\n    return filtered_words","50909003":"irish_data['tokenized'] = irish_data['headline_text'].apply(lambda x: remove_stopwords(x))","bb85b3d8":"get_words = []\n\ndef lemmatize(line):\n    lem_words = []\n    \n    for word in line:\n        lem_word = lem.lemmatize(word,\"v\")\n        if len(lem_word) > 1:\n            lem_words.append(lem_word)\n            get_words.append(lem_word)\n    \n    return lem_words","69433cf8":"irish_data['lemmatized'] = irish_data['tokenized'].apply(lambda x: lemmatize(x))","2af56ac7":"freq_words = nltk.FreqDist(get_words)\nless_words = []\n\nfor word in freq_words:\n    if freq_words[word]<=2:\n        less_words.append(word)","dc233bc7":"def remove_less_words(lists):\n    filters = []\n    \n    for word in lists:\n        if word not in less_words:\n            filters.append(word)\n            \n    return ' '.join(filters)","b6255e34":"irish_data['filtered'] = irish_data['lemmatized'].apply(lambda x: remove_less_words(x))","0f4e6456":"irish_data.head()","e348e4ed":"#token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n#CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,2),tokenizer = token.tokenize)\ncv = CountVectorizer(ngram_range = (1,1))\ntext_counts= cv.fit_transform(irish_data['filtered'])","d4399fc3":"X_train, X_test, y_train, y_test = train_test_split(text_counts, irish_data['headline_category'], test_size=0.3, random_state=101)","71a3aab1":"logistic_model = LogisticRegression()\nlogistic_model.fit(X_train, y_train)","aa9bfab1":"y_pred = logistic_model.predict(X_test)","c0ab7fbc":"accuracy_per = accuracy_score(y_test, y_pred)\n\nprint(\"Accuracy on the dataset: {:.2f}\".format(accuracy_per*100))","2598aeda":"So, now the data is filtered and lemmatized and stored in a column as you can see below","bdf971af":"![The Irish News](https:\/\/www.nova.ie\/wp-content\/uploads\/2017\/12\/The-Irish-Times.jpg)","e85f10b9":"Before dive-in to the NLP, Here are some information about the Irish Times\n\n## Irish Times\n\nThe Irish Times is an Irish daily broadsheet newspaper launched on 29 March 1859. The editor is Paul O'Neill who succeeded Kevin O'Sullivan on 5 April 2017; the deputy editor is Deirdre Veldon. The Irish Times is published every day except Sundays. It employs 420 people. More info in https:\/\/en.wikipedia.org\/wiki\/The_Irish_Times\n\n### Contents of the NLP\nThe following process will be undertaken in the NLP\n1. Importing the libraries needed for the process\n2. Downloading the StopWords and Lemmatizer\n3. Perform Tokenization and Removal of StopWords\n4. Initialization of Count Vectorizer\n5. Split of Train and Test dataset\n6. Creation of Model\n7. Performing Predictions and analyze the performance","4f689047":"Below is the function used to lemmatize the filtered words and also we will create a new column called 'lemmatized' which will contain the lemmatized words.","489140fe":"#### 4. Initialization of Count Vectorizer\nLet's initialize the CountVectorizer. Below you can see that some parameters are initialized. We will go thorugh the parameters.\n\n- ngram_range = (1,2) => We are specifying the Count Vectorizer to use both unigrams and bigrams. Setting it to (2, 2) means only bigrams and (1, 1) means only unigrams.\n\n- lowercase=True => We are transforming the text into lowercase (**Note**: We already did this process before in Step 3, but just to show that we can do it via Count Vectorizer :) )\n\n- stop_words='english' => We are specifying the stopwords as english to remove from the sentence. (**Note**: We already did this process before in Step 3, but just to show that we can do it via Count Vectorizer :) )\n\n- tokenizer = token.tokenize => We are removing the special characters from the text using **RegexpTokenizer**. (**Note**: We already did this process before in Step 3, but just to show that we can do it via Count Vectorizer :) )\n\n","e9c8d8a1":"#### 2. Downloading the StopWords and Lemmatizer\nNow, Let's download the stopwords and initialize the Lemmatizer. Note that, NLTK has a various language version of StopWords. You can have a look at the documentation of NLTK.\n\nWe will initialize the WordNetLemmatizer and also make a call to the function with a random words, because the lemmatizer will take some time to load the words into the workspace initially. So, it's my practice to run the function using initialization.","52964be0":"The dataset consists of 3 columns and 1425460 rows.\n\nThe dataset consists of three columns:\n    1. publish_date - The date when the news gets published. The format of the date is YYYYMMDD\n    2. headline_category - The category of the news\n    3. headline_text - The headline information of the news","9d4d107f":"#### 3. Perform Tokenization and Removal of StopWords","22718154":"### Analysis of the Dataset\nWe use **read_csv** in Pandas to read the dataset","21603b22":"Here, we will use the below function to remove the stopwords and also special characters from the words using **re**","733fd831":"#### 6. Creation of Model\nNow, Let's create the model. We will select **LogisticRegression** as our model for NLP. Below, is the model creation and training the model with the train input and train output.","831b5fdb":"Now, we will use the above function and create a new column called **tokenized** which will contain the filtered words.","17427b1a":"Now, the prediction is performed and we will use the **accuracy score** function to calculate the accuracy level","68c5c8c7":"#### 1. Importing the libraries needed for the process\nLet's have a quick walkthrough about the libraries that gonna be used.\n\n-> **Pandas** - Handling and Manipulation of Data Frame\n\n-> **NLTK (Natural Language Toolkit)** - Powerful NLP libraries which contains packages to make machines understand human language\n\n-> **RE (Regular Expression)** - Special sequence of characters that helps you match or find other strings or sets of strings.\n\n-> **SKLearn (Scikit-learn)** - Machine learning library for the Python programming language. It features various classification, regression and clustering algorithms.\n","ab3b67fd":"#### 7. Performing Predictions and analyze the performance\nNow, the model is trained with the dataset. Let's use the test dataset (X_test) to predict the outputs","49c66fde":"#### 5. Split of Train and Test dataset\nNow, the dataset is ready for training. So,Let's split the data into train and test data. We will use **train_test_split** to perform the step."}}