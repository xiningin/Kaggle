{"cell_type":{"3ba231a2":"code","aed7f270":"code","b6491db7":"code","206f88d0":"code","9a912966":"code","e36ed275":"code","8aadc80d":"code","e4bca28b":"code","75e67de5":"code","c77cb5a6":"code","bc6d3c97":"code","7f1516f1":"code","89dd6cc0":"code","aaf1655a":"code","c670edaa":"code","27e3be5a":"code","1539a771":"code","70962ada":"code","8e2362ed":"code","c9c31284":"code","76229c0a":"code","fd4d6392":"code","fc0a2402":"code","385d9838":"markdown","257d5778":"markdown","1517df73":"markdown","91e9932b":"markdown","315bf955":"markdown","9cd1a322":"markdown"},"source":{"3ba231a2":"!pip install hazm","aed7f270":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport re\nfrom hazm import Normalizer, word_tokenize\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch import nn, optim\n\nimport transformers\nfrom transformers import AutoModel, AutoTokenizer, AutoConfig\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","b6491db7":"comments = pd.read_csv(\"\/kaggle\/input\/persian-sentiment-analysis-dataset\/Instagram labeled comments.csv\").drop(\"Unnamed: 0\", axis=1).reset_index(drop=True)\nprint(comments.shape)\nprint(comments.isna().sum())\ncomments = comments.drop_duplicates(subset=['comment'], keep='first')\ncomments.shape","206f88d0":"e = (\"[\"\n    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n    u\"\\U00002500-\\U00002BEF\"  # chinese char\n    u\"\\U00002702-\\U000027B0\"\n    u\"\\U00002702-\\U000027B0\"\n    u\"\\U000024C2-\\U0001F251\"\n    u\"\\U0001f926-\\U0001f937\"\n    u\"\\U00010000-\\U0010ffff\"\n    u\"\\u2640-\\u2642\" \n    u\"\\u2600-\\u2B55\"\n    u\"\\u200d\"\n    u\"\\u23cf\"\n    u\"\\u23e9\"\n    u\"\\u231a\"\n    u\"\\ufe0f\"  # dingbats\n    u\"\\u3030\"\n     \"]+\")\nallemojies = set()\ndef removeEmojies(text):\n    duplemoj = re.compile('(' + e + '){2,}|((' + e + r')\\s+)' '{2,}', re.UNICODE)\n    return duplemoj.sub('', text)","9a912966":"punctuation = \"?.\u061f!\u060c,\"\nnormalizer = Normalizer()\ndef preprocessing(item):\n    output = normalizer.normalize(item)\n    output = output.replace(\"\\_\", \"\\u200c\")\n#     output = output.replace(\"\\u200c\", \"\")\n    output = re.sub(r\"LINK([^ ])*|TAG|ID|\\-|@|LINK|[A-Za-z]\", \"\", output)\n    for i in punctuation:\n        p = \"\\\\\" + i + \"{2,}\"\n        output = re.sub(p, i, output)\n#     output = re.sub(\"\\?|\\.|\\\u061f|\\!|\\\u060c|\\,\", \"\", output)\n    output = removeEmojies(output)\n    emoj = re.compile(e)\n    for i in emoj.findall(output):\n        allemojies.add(i)\n    output = re.sub(\"\\s+\", \" \", output).strip()\n    return output\n\ncomments['comment'] = comments['comment'].apply(preprocessing)\ncomments['len words'] = comments['comment'].apply(lambda x: len(word_tokenize(x)))\ncomments['len chars'] = comments['comment'].apply(lambda x: len(x))\ncomments = comments[comments['len chars'] >= 3].reset_index(drop=True)\nmaxlen = comments['len words'].max()\n# crossentropy inputs should be in range [0, C-1] so\ncomments['sentiment'] = comments['sentiment'] + 1\n# comments.shape\ncomments.head(20)","e36ed275":"df_train, df_test = train_test_split(comments[['comment', 'sentiment']], test_size=0.1)\ndf_test, df_val = train_test_split(df_test, test_size=0.6)\ndisplay(df_train.shape)\ndisplay(df_val.shape)\ndisplay(df_test.shape)","8aadc80d":"parsbert_model = 'HooshvareLab\/bert-fa-zwnj-base'\ntokenizer = AutoTokenizer.from_pretrained(parsbert_model)\ntokenizer.add_tokens(list(allemojies))","e4bca28b":"class CommentsDataset(torch.utils.data.Dataset):\n    def __init__(self, comments, targets, tokenizer, maxlen):\n        self.comments = comments\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.maxlen = maxlen\n\n    def __len__(self):\n        return len(self.comments)\n\n    def __getitem__(self, item):\n        tokens = tokenizer(\n            str(self.comments[item]),\n            max_length=self.maxlen,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt',\n            return_token_type_ids=False\n        )\n        return {\n            'comment': self.comments[item],\n            'input_ids': tokens['input_ids'].flatten(),\n            'attention_mask': tokens['attention_mask'].flatten(),\n            'targets': torch.tensor(self.targets[item], dtype=torch.long)\n        }","75e67de5":"def create_dataloader(df, tokenizer, maxlen, batch_size):\n    dataset = CommentsDataset(\n        df['comment'].to_numpy(),\n        df['sentiment'].to_numpy(),\n        tokenizer=tokenizer,\n        maxlen=maxlen\n    )\n    return DataLoader(dataset, batch_size=batch_size)\n\nBATCH_SIZE = 256\n\ndataloader_train = create_dataloader(df_train, tokenizer, maxlen, BATCH_SIZE)\ndataloader_val = create_dataloader(df_val, tokenizer, maxlen, BATCH_SIZE)\ndataloader_test = create_dataloader(df_test, tokenizer, maxlen, BATCH_SIZE)","c77cb5a6":"import gc\n# del model\n# del optimizer\ntorch.cuda.empty_cache()\ngc.collect()\n!nvidia-smi","bc6d3c97":"cfg = AutoConfig.from_pretrained(\n    parsbert_model,\n    hidden_dropout_prob= 0.3,\n    classifier_dropout= 0.3\n)\nbert = AutoModel.from_config(cfg)\n# I added some emojies to tokenizer so ...\nbert.resize_token_embeddings(len(tokenizer))\nbert.config","7f1516f1":"class SentimentClassifier(nn.Module):\n    def __init__(self, n_classes, bert):\n        super(SentimentClassifier, self).__init__()\n        self.bert = bert\n        self.gru = nn.GRU(\n            self.bert.config.to_dict()['hidden_size'],\n            hidden_size=256,\n            num_layers=2,\n            bidirectional=True,\n            batch_first = True,\n            dropout = 0.3\n        )\n        self.drop = nn.Dropout(0.3)\n        self.out = nn.Linear(512, n_classes)\n    def forward(self, input_ids, attention_mask):\n        with torch.no_grad():\n            pooled_output = self.bert(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                return_dict=False\n            )[0]\n        hidden = self.gru(pooled_output)[1]\n        hidden = self.drop(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n        return self.out(hidden)\n\nmodel = SentimentClassifier(3, bert).to(device)\nfor name, param in model.named_parameters():                \n    if name.lower().startswith('bert'):\n        param.requires_grad = False","89dd6cc0":"loss_fn = nn.CrossEntropyLoss().to(device)\n\nLR, EPS, WEIGHT_DECAY = 0.0001, 1e-08, 0.01\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=LR,\n    eps=EPS,\n    weight_decay=WEIGHT_DECAY\n)","aaf1655a":"def train_ep(dataloader, model, loss_fn, optimizer, device, n_all):\n    all = len(dataloader)\n    losses = []\n    corrects = 0\n    model.train()\n    n = 0\n    for d in dataloader:\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        targets = d[\"targets\"].to(device)\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n\n        _, pred = torch.max(outputs, dim=1)\n        corrects += torch.sum(pred == targets)\n\n        loss = loss_fn(outputs, targets)\n        losses.append(loss.item())\n\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        optimizer.zero_grad()\n        \n        n += 1\n        print(f\"batch {n} \/ {all} -- loss: {loss.item():.5f}\")\n    return corrects.item() \/ n_all  * 100, np.mean(losses)","c670edaa":"def eval_ep(dataloader, model, loss_fn, device, n_all):\n    all = len(dataloader)\n    losses = []\n    loss = 0\n    corrects = 0\n    n = 0\n    model.eval()\n    with torch.no_grad():\n        for d in dataloader:\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            targets = d[\"targets\"].to(device)\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n\n            _, pred = torch.max(outputs, dim=1)\n            corrects += torch.sum(pred == targets)\n\n            loss = loss_fn(outputs, targets)\n            losses.append(loss.item())\n            n += 1\n            print(f\"batch {n} \/ {all} -- loss: {loss.item():.5f}\")\n\n    return corrects.item() \/ n_all * 100, np.mean(losses)","27e3be5a":"EPOCHS = 100\nlosses_valid = []\nlosses_train = []\nacc_valid = []\nacc_train = []\nbest_acc = 0\n\nfor i in range(EPOCHS):\n    train_c, train_l = train_ep(dataloader_train, model, loss_fn, optimizer, device, len(df_train))\n    print(f\"Epoch {i} ------ train corrects {train_c}    train losses {train_l}\")\n    losses_train.append(train_l)  \n    acc_train.append(train_c)  \n\n    val_c, val_l = eval_ep(dataloader_val, model, loss_fn, device, len(df_val))\n    print(f\"Epoch {i} ------ valid corrects {val_c}      vali losses {val_l}\")\n    losses_valid.append(val_l)\n    acc_valid.append(val_c)\n\n    if val_c > best_acc:\n        state = {\n            'state_dict': model.state_dict(),\n            'optimizer': optimizer.state_dict()\n        }\n        torch.save(state, \"myModel\")\n        best_acc = val_c\n\n","1539a771":"state = torch.load(\"myModel\")\nmodel.load_state_dict(state['state_dict'])\noptimizer.load_state_dict(state['optimizer'])","70962ada":"# figure, axis = plt.subplots(2, 2)\n# # axis[0,0]\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,10))\n# ax1.title(\"Training and Validation Accuracy\")\nprint(ax1.title)\nax1.plot(acc_valid,label=\"val\")\nax1.plot(acc_train,label=\"train\")\nax1.set_xlabel(\"iterations\")\nax1.set_ylabel(\"accuracy\")\nax1.legend()\n\nax2.plot(losses_valid,label=\"val\")\nax2.plot(losses_train,label=\"train\")\nax2.set_xlabel(\"iterations\")\nax2.set_ylabel(\"loss\")\nax2.legend()\n","8e2362ed":"test_accuracy, test_loss = eval_ep(dataloader_val, model, loss_fn, device, len(df_val))\nprint(f\"Accuracy: {test_accuracy}, loss: {test_loss}\\n\")\n\nnb_classes = 3\n\nconfusion_matrix = torch.zeros(nb_classes, nb_classes)\nwith torch.no_grad():\n    for i, d in enumerate(dataloader_test):\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        targets = d[\"targets\"].to(device)\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        _, preds = torch.max(outputs, dim=1)\n        for t, p in zip(targets, preds):\n                confusion_matrix[t, p] += 1\n\nscores = pd.DataFrame(index=['negative', 'natural', 'positive', 'average'], columns=['precision', 'recall', 'F1-Score'])\nfor i, label in enumerate([\"negative\", \"natural\", \"positive\"]):\n    p = scores.loc[label, 'precision'] = (confusion_matrix[i, i] \/ confusion_matrix[i].sum()).item()\n    r = scores.loc[label, 'recall'] = (confusion_matrix[i, i] \/ confusion_matrix[:, i].sum()).item()\n    scores.loc[label, 'F1-Score'] = (2*p*r) \/ (p+r)\nscores.loc['average'] = scores.mean().values\nscores","c9c31284":"display(confusion_matrix)\ndisplay(scores)","76229c0a":"label2class = {0: 'negative', 1: 'natural', 2: 'positive'}\n\nplt.figure(figsize=(15,10))\nsns.set(font_scale=1.8)\n\nclass_names = list(label2class.values())\ndf_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names).astype(int)\nheatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right',fontsize=15)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right',fontsize=15)\nplt.ylabel('True label')\nplt.xlabel('Predicted label')","fd4d6392":"id = np.random.randint(0,comments.shape[0])\ndisplay(id)\nidtolabel = [\"negative\", \"natural\", \"positive\"]\nsampletxt = comments['comment'][id]\ndisplay(sampletxt, idtolabel[comments['sentiment'][id]])\nsampletok = tokenizer(\n        sampletxt,\n        max_length=maxlen,\n        padding='max_length',\n        truncation=True,\n        return_tensors='pt',\n        return_token_type_ids=False\n)\ninput_ids = sampletok['input_ids'].to(device)\nattention_mask = sampletok['attention_mask'].to(device)\noutput = model(input_ids, attention_mask)\n_, prediction = torch.max(output, dim=1)\n\ndisplay(idtolabel[prediction.item()])\n","fc0a2402":"sampletxt = \"\u062a\u0633\u062a\"\nsampletok = tokenizer(\n        sampletxt,\n        max_length=maxlen,\n        padding='max_length',\n        truncation=True,\n        return_tensors='pt',\n        return_token_type_ids=False\n)\ninput_ids = sampletok['input_ids'].to(device)\nattention_mask = sampletok['attention_mask'].to(device)\noutput = model(input_ids, attention_mask)\n_, prediction = torch.max(output, dim=1)\n\ndisplay(idtolabel[prediction.item()])","385d9838":"# read csv and preprocessing texts","257d5778":"# Tokenization and Dataloaders","1517df73":"# Model training","91e9932b":"* normalizing texts using hazm library\n* replace all underline character with ZWNJ character\n* remove meaningless english vocabs\n* remove spammed punctuations and replace it with one of each character\n* remove duplicate emojies (tried without emoji too)\n* find all emojies to add them later to tokenizer\n* remove extra spaces","315bf955":"toekenize texts based on parsbert3 tokenizer to feed them later to bert layer \\\nalso add saved emojies to tokenizer (it would increase the accuracy if i could train the bert layer)","9cd1a322":"# imports"}}