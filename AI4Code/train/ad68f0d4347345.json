{"cell_type":{"42ff68fd":"code","5e5c91c1":"code","05a6de99":"code","743b3baf":"code","d8605422":"code","44eac0eb":"code","87006b1d":"code","f91e4aae":"code","53c1c3c7":"code","a0a4ac1d":"code","002530f5":"code","dea6b27e":"code","b4a240e5":"code","1e900df1":"code","e94b9f96":"code","df46d4e2":"code","2e1762a0":"code","1180fa1b":"code","dd1bdd04":"code","20d962c5":"code","8c6bc06d":"code","9d8851b2":"code","8245b616":"code","68b55cfa":"code","846d54f5":"code","50c5eace":"code","b826d661":"code","cc56de33":"code","2a4dc06e":"code","d28e71eb":"code","832e7d22":"code","b70444a6":"code","5d107329":"code","bd29ce0d":"markdown","40a193bd":"markdown","7fa7f3ed":"markdown","c9355a28":"markdown","94d55ff8":"markdown","46e5b742":"markdown","2c2b56b4":"markdown","fc86303e":"markdown","82891a6f":"markdown","4573ecde":"markdown","1e567a90":"markdown","4b0dd977":"markdown"},"source":{"42ff68fd":"!pip install Ninja","5e5c91c1":"!git clone https:\/\/github.com\/eladrich\/pixel2style2pixel\n%cd pixel2style2pixel","05a6de99":"from argparse import Namespace\nimport time\nimport sys\nimport os\nimport pprint\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision.transforms as transforms\n\nsys.path.append(\".\")\nsys.path.append(\"..\")\n\nfrom datasets import augmentations\nfrom utils.common import tensor2im, log_input_image\nfrom models.psp import pSp","743b3baf":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)","d8605422":"experiment_type = 'celebs_sketch_to_face' #@param ['ffhq_encode', 'ffhq_frontalize', 'celebs_sketch_to_face', 'celebs_seg_to_face', 'celebs_super_resolution', 'toonify']","44eac0eb":"MODEL_PATHS = {\n    \"ffhq_encode\": {\"id\": \"1bMTNWkh5LArlaWSc_wa8VKyq2V42T2z0\", \"name\": \"psp_ffhq_encode.pt\"},\n    \"ffhq_frontalize\": {\"id\": \"1_S4THAzXb-97DbpXmanjHtXRyKxqjARv\", \"name\": \"psp_ffhq_frontalization.pt\"},\n    \"celebs_sketch_to_face\": {\"id\": \"1lB7wk7MwtdxL-LL4Z_T76DuCfk00aSXA\", \"name\": \"psp_celebs_sketch_to_face.pt\"},\n    \"celebs_seg_to_face\": {\"id\": \"1VpEKc6E6yG3xhYuZ0cq8D2_1CbT0Dstz\", \"name\": \"psp_celebs_seg_to_face.pt\"},\n    \"celebs_super_resolution\": {\"id\": \"1ZpmSXBpJ9pFEov6-jjQstAlfYbkebECu\", \"name\": \"psp_celebs_super_resolution.pt\"},\n    \"toonify\": {\"id\": \"1YKoiVuFaqdvzDP5CZaqa3k5phL-VDmyz\", \"name\": \"psp_ffhq_toonify.pt\"}\n}\n\npath = MODEL_PATHS[experiment_type]","87006b1d":"CODE_DIR = 'pixel2style2pixel'\ndef get_download_model_command(file_id, file_name):\n    \"\"\" Get wget download command for downloading the desired model and save to directory ..\/pretrained_models. \"\"\"\n    current_directory = os.getcwd()\n    save_path = os.path.join(os.path.dirname(current_directory), CODE_DIR, \"pretrained_models\")\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n    url = r\"\"\"wget --load-cookies \/tmp\/cookies.txt \"https:\/\/docs.google.com\/uc?export=download&confirm=$(wget --quiet --save-cookies \/tmp\/cookies.txt --keep-session-cookies --no-check-certificate 'https:\/\/docs.google.com\/uc?export=download&id={FILE_ID}' -O- | sed -rn 's\/.*confirm=([0-9A-Za-z_]+).*\/\\1\\n\/p')&id={FILE_ID}\" -O {SAVE_PATH}\/{FILE_NAME} && rm -rf \/tmp\/cookies.txt\"\"\".format(FILE_ID=file_id, FILE_NAME=file_name, SAVE_PATH=save_path)\n    return url\n\ndownload_command = get_download_model_command(file_id=path[\"id\"], file_name=path[\"name\"])","f91e4aae":"!wget {download_command}","53c1c3c7":"EXPERIMENT_DATA_ARGS = {\n    \"ffhq_encode\": {\n        \"model_path\": \"pretrained_models\/psp_ffhq_encode.pt\",\n        \"image_path\": \"notebooks\/images\/input_img.jpg\",\n        \"transform\": transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n    },\n    \"ffhq_frontalize\": {\n        \"model_path\": \"pretrained_models\/psp_ffhq_frontalization.pt\",\n        \"image_path\": \"notebooks\/images\/input_img.jpg\",\n        \"transform\": transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n    },\n    \"celebs_sketch_to_face\": {\n        \"model_path\": \"pretrained_models\/psp_celebs_sketch_to_face.pt\",\n        \"image_path\": \"notebooks\/images\/input_sketch.jpg\",\n        \"transform\": transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor()])\n    },\n    \"celebs_seg_to_face\": {\n        \"model_path\": \"pretrained_models\/psp_celebs_seg_to_face.pt\",\n        \"image_path\": \"notebooks\/images\/input_mask.png\",\n        \"transform\": transforms.Compose([\n            transforms.Resize((256, 256)),\n            augmentations.ToOneHot(n_classes=19),\n            transforms.ToTensor()])\n    },\n    \"celebs_super_resolution\": {\n        \"model_path\": \"pretrained_models\/psp_celebs_super_resolution.pt\",\n        \"image_path\": \"notebooks\/images\/input_img.jpg\",\n        \"transform\": transforms.Compose([\n            transforms.Resize((256, 256)),\n            augmentations.BilinearResize(factors=[16]),\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n    },\n    \"toonify\": {\n        \"model_path\": \"pretrained_models\/psp_ffhq_toonify.pt\",\n        \"image_path\": \"notebooks\/images\/input_img.jpg\",\n        \"transform\": transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n    },\n}    ","a0a4ac1d":"EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[experiment_type]","002530f5":"model_path = EXPERIMENT_ARGS['model_path']\nckpt = torch.load(model_path, map_location='cpu')","dea6b27e":"opts = ckpt['opts']\npprint.pprint(opts)","b4a240e5":"# update the training options\nopts['checkpoint_path'] = model_path\nif 'learn_in_w' not in opts:\n    opts['learn_in_w'] = False\nif 'output_size' not in opts:\n    opts['output_size'] = 1024","1e900df1":"opts = Namespace(**opts)\nnet = pSp(opts)\nnet.eval()\nnet.cuda()\nprint('Model successfully loaded!') ","e94b9f96":"image_path = EXPERIMENT_DATA_ARGS[experiment_type][\"image_path\"]\noriginal_image = Image.open(image_path)\nif opts.label_nc == 0:\n    original_image = original_image.convert(\"RGB\")\nelse:\n    original_image = original_image.convert(\"L\")","df46d4e2":"# download a picture \nimport urllib\nimg = Image.open(urllib.request.urlopen(\"https:\/\/pbs.twimg.com\/profile_images\/1079250910755213313\/fHMhccTC_400x400.jpg\")) # Conan\n\nplt.axis('off')\nplt.imshow(img)\nplt.show()","2e1762a0":"# use opencv to generate edge for sketch\nimport numpy as np\nimport cv2\nimg = np.array(img)\ngray  = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\nedges = cv2.Canny(gray,100,200)\ndilation = cv2.dilate(edges,np.ones((3,3),np.uint8),iterations = 1) # dilation\nsketch = cv2.bitwise_not(dilation)\nplt.axis('off')\nplt.imshow(sketch, cmap='gray')\nplt.show()","1180fa1b":"# convert opencv image to pillow image\noriginal_image= Image.fromarray(sketch)","dd1bdd04":"# resize to 256x256\noriginal_image.resize((256, 256))\noriginal_image","20d962c5":"input_image = original_image","8c6bc06d":"img_transforms = EXPERIMENT_ARGS['transform']\ntransformed_image = img_transforms(input_image)","9d8851b2":"def run_on_batch(inputs, net, latent_mask=None):\n    if latent_mask is None:\n        result_batch = net(inputs.to(\"cuda\").float(), randomize_noise=False)\n    else:\n        result_batch = []\n        for image_idx, input_image in enumerate(inputs):\n            # get latent vector to inject into our input image\n            vec_to_inject = np.random.randn(1, 512).astype('float32')\n            _, latent_to_inject = net(torch.from_numpy(vec_to_inject).to(\"cuda\"),\n                                      input_code=True,\n                                      return_latents=True)\n            # get output image with injected style vector\n            res = net(input_image.unsqueeze(0).to(\"cuda\").float(),\n                      latent_mask=latent_mask,\n                      inject_latent=latent_to_inject)\n            result_batch.append(res)\n        result_batch = torch.cat(result_batch, dim=0)\n    return result_batch","8245b616":"if experiment_type in [\"celebs_sketch_to_face\", \"celebs_seg_to_face\"]:\n    latent_mask = [8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\nelse:\n    latent_mask = None","68b55cfa":"with torch.no_grad():\n    tic = time.time()\n    result_image = run_on_batch(transformed_image.unsqueeze(0), net, latent_mask)[0]\n    toc = time.time()\n    print('Inference took {:.4f} seconds.'.format(toc - tic))","846d54f5":"input_vis_image = log_input_image(transformed_image, opts)\noutput_image = tensor2im(result_image)","50c5eace":"if experiment_type == \"celebs_super_resolution\":\n    res = np.concatenate([np.array(input_image.resize((256, 256))),\n                          np.array(input_vis_image.resize((256, 256))),\n                          np.array(output_image.resize((256, 256)))], axis=1)\nelse:\n    res = np.concatenate([np.array(input_vis_image.resize((256, 256))),\n                          np.array(output_image.resize((256, 256)))], axis=1)","b826d661":"res_image = Image.fromarray(res)\nres_image","cc56de33":"if experiment_type in [\"celebs_sketch_to_face\", \"celebs_seg_to_face\"]:\n    latent_mask = [8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n    mix_alpha = None\nelif experiment_type == \"celebs_super_resolution\":\n    latent_mask = [4, 5, 6, 7]\n    mix_alpha = 0.5\nelse:\n    raise ValueError(\"Multi-modal synthesis is performed only for seg-to-face, sketch-to-face, and super-resolution!\")\n\nn_outputs_to_generate = 5","2a4dc06e":"def get_multi_modal_outputs(input_image, vectors_to_inject):\n    results = []\n    with torch.no_grad():\n        for vec_to_inject in vectors_to_inject:\n            cur_vec = torch.from_numpy(vec_to_inject).unsqueeze(0).to(\"cuda\")\n            # get latent vector to inject into our input image\n            _, latent_to_inject = net(cur_vec,\n                                      input_code=True,\n                                      return_latents=True)\n            # get output image with injected style vector\n            res = net(input_image.unsqueeze(0).to(\"cuda\").float(),\n                      latent_mask=latent_mask,\n                      inject_latent=latent_to_inject,\n                      alpha=mix_alpha)\n            results.append(res[0])\n    return results","d28e71eb":"# randomly draw the latents to use for style mixing\nvectors_to_inject = np.random.randn(n_outputs_to_generate, 512).astype('float32')","832e7d22":"multi_results = get_multi_modal_outputs(transformed_image, vectors_to_inject)","b70444a6":"input_vis_image = log_input_image(transformed_image, opts)\nres = np.array(input_vis_image.resize((256, 256)))\n\nfor output in multi_results:\n    output = tensor2im(output)\n    res = np.concatenate([res, np.array(output.resize((256, 256)))], axis=1)","5d107329":"res_image = Image.fromarray(res)\nres_image","bd29ce0d":"## Select Experiment Type","40a193bd":"## Define Inference Parameters","7fa7f3ed":"## Repro [Github](https:\/\/github.com\/eladrich\/pixel2style2pixel)","c9355a28":"## Download Pretrained Models","94d55ff8":"## Perform Inference","46e5b742":"## Multi-Modal Synthesis\n","2c2b56b4":"### Generate Outputs","fc86303e":"## Convert picture to sketch","82891a6f":"## Load Pretrained Model","4573ecde":"## Visualize Input","1e567a90":"### Visualize Results","4b0dd977":"# Pixel2Style2Pixel \n## **Sketch-to-Face**"}}