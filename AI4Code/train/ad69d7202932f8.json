{"cell_type":{"81c078d0":"code","9579b1d1":"code","e5aa027f":"code","b1086941":"code","1165a868":"code","e39e1f3c":"code","c7a7b8b1":"code","77539d87":"code","e06bfb88":"code","da909518":"code","ba824360":"code","b84ff829":"code","dcd20cb6":"code","e8ef2c72":"code","4620a49a":"code","a91a4e6f":"code","dfe18e8f":"code","de6086e7":"code","40566d29":"code","ae608c13":"code","fd3aebe4":"code","63d5ed03":"code","d191bce4":"code","6b456274":"code","1d1e312b":"markdown","01941d62":"markdown","9a7a66e9":"markdown","0b25f57a":"markdown"},"source":{"81c078d0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9579b1d1":"import pandas as pd\nimport numpy as np\nimport nltk","e5aa027f":"#dataset\ndata = pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv',delimiter=',',encoding='latin-1')","b1086941":"data.head()","1165a868":"#some of the columns are of no use, therefor it will be ideal to drop them\ndata=data.drop([\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\"], axis=1)","e39e1f3c":"data.head()","c7a7b8b1":"#There is no missing values \nimport missingno\nmissingno.matrix(data, figsize=(5,5))","77539d87":"# value count of span\/ham in the data set\ndata[\"v1\"].value_counts()","e06bfb88":"# Label Enciding the data ham=0, spam=1\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(data[\"v1\"])\nprint(data.v1[:10])\nprint(y)","da909518":"text = data.iloc[:,1]\ntext[:10]","ba824360":"# replace all the email address with \"emailaddress\" \nprocessed = text.str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$','emailaddress')\n# replace all the url with \"webaddress\"\nprocessed = processed.str.replace(r'^http\\:\/\/[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(\/\\S*)?$','webaddress')\n# Replace money symbols with 'moneysymb' (\u00a3 can by typed with ALT key + 156)\nprocessed = processed.str.replace(r'\u00a3|\\$', 'moneysymb')\n# Replace 10 digit phone numbers (formats include paranthesis, spaces, no spaces, dashes) with 'phonenumber'\nprocessed = processed.str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$','phonenumbr')\n# Replace numbers with 'numbr'\nprocessed = processed.str.replace(r'\\d+(\\.\\d+)?', 'numbr')\n# Remove punctuation\nprocessed = processed.str.replace(r'[^\\w\\d\\s]', ' ')\n# Replace whitespace between terms with a single space\nprocessed = processed.str.replace(r'\\s+', ' ')\n# Remove leading and trailing whitespace\nprocessed = processed.str.replace(r'^\\s+|\\s+?$', '')","b84ff829":"# change aal the words into lower case\nprocessed = processed.str.lower()\nprint(processed)","dcd20cb6":"# remove all the stop words\nfrom nltk.corpus import stopwords\nstop_words= set(stopwords.words(\"english\"))\nprocessed = processed.apply(lambda x: \" \".join(term for term in x.split() if term not in stop_words))","e8ef2c72":"# breaking the word to its bas word with the help of Porter Stemer\nfrom nltk.stem import PorterStemmer\nps= PorterStemmer()\nprocessed = processed.apply(lambda x: \" \".join(ps.stem(term) for term in x.split()))","4620a49a":"from nltk.tokenize import word_tokenize\nall_words = []\n\nfor message in processed:\n    words = word_tokenize(message)\n    for w in words:\n        all_words.append(w)\n        \nall_words = nltk.FreqDist(all_words)       ","a91a4e6f":"print(f\"Total number of words : {len(all_words)}\")\nprint(f\"Most common words:{all_words.most_common(15)}\")","dfe18e8f":"word_freature = list(all_words.keys())[:1500]","de6086e7":"# The find_features function will determine which of the 1500 word features are contained in the review\ndef find_features(message):\n    words = word_tokenize(message)\n    features = {}\n    for word in word_freature:\n        features[word] = (word in words)\n\n    return features\n\n# Lets see an example!\nfeatures = find_features(processed[0])\nfor key, value in features.items():\n    if value == True:\n        print (key)","40566d29":"# Now lets do it for all the messages\nmessages = zip(processed, y)","ae608c13":"featuresets = [(find_features(text), label) for (text, label) in messages]","fd3aebe4":"# we can split the featuresets into training and testing datasets using sklearn\nfrom sklearn import model_selection\n\n# split the data into training and testing datasets\ntraining, testing = model_selection.train_test_split(featuresets, test_size = 0.25, random_state=1)","63d5ed03":"print(len(training))\nprint(len(testing))","d191bce4":"# We can use sklearn algorithms in NLTK\nfrom nltk.classify.scikitlearn import SklearnClassifier\nfrom sklearn.svm import SVC\nmodel = SklearnClassifier(SVC(kernel = 'linear'))","6b456274":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n\n# Define models to train\nnames = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n         \"Naive Bayes\", \"SVM Linear\"]\n\n\nclassifiers = [\n    KNeighborsClassifier(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    LogisticRegression(),\n    SGDClassifier(max_iter = 100),\n    MultinomialNB(),\n    SVC(kernel = 'linear')]\n\nmodels = zip(names, classifiers)\n\nfor name, model in models:\n    nltk_model = SklearnClassifier(model)\n    nltk_model.train(training)\n    accuracy = nltk.classify.accuracy(nltk_model, testing)*100\n    print(\"{} Accuracy: {}\".format(name, accuracy))\n\n\n","1d1e312b":"3) Data preprocessing","01941d62":"4) Building the model\n","9a7a66e9":"1) Importing the libraries","0b25f57a":"2) Loading the dataset"}}