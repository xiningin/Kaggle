{"cell_type":{"805986af":"code","42fdc91e":"code","77c09576":"code","7ca80322":"code","e3d754a5":"code","aca9a9e9":"code","a8bee523":"code","13597db4":"code","8925f09b":"code","e7c89613":"code","46dfe8a5":"code","402b2fee":"code","4f6460e4":"code","35d3d7b2":"code","48f92232":"code","4a772191":"code","8e3a5e11":"code","330bdd3a":"code","acc4efa2":"code","8ac0967f":"code","9fbd7437":"code","15359482":"code","3cce92c6":"code","72ac7c96":"code","b9f79dd6":"code","ce755875":"code","63bdf3cc":"markdown","97bd51d4":"markdown","5aaff2da":"markdown","bcb56cb0":"markdown","a1cf292e":"markdown","53e81e04":"markdown","c72ec026":"markdown","c90960fd":"markdown","c6b51f6d":"markdown","8f6730df":"markdown","1f96e9b4":"markdown","5d99faa4":"markdown","9248c228":"markdown","a301f69b":"markdown","45e5fa1d":"markdown","9f400530":"markdown","d1a7c38d":"markdown","d997d159":"markdown","82fe9e8f":"markdown","39e83663":"markdown","26030d09":"markdown","da9ab1c8":"markdown","76b2997f":"markdown","0627535e":"markdown","d76507b5":"markdown","84d654a7":"markdown","128b1fba":"markdown","9159ba47":"markdown","09ef70db":"markdown","968790e0":"markdown","de1958f4":"markdown","5879302e":"markdown","67e78f3b":"markdown","1a7fac92":"markdown","3a5ca346":"markdown","a5f64284":"markdown","b3ed082c":"markdown","9073f108":"markdown","5a39da0b":"markdown","8a065108":"markdown","11a6c3e7":"markdown","ef990425":"markdown","adfa4b37":"markdown","a8bb9005":"markdown","05905bd5":"markdown","e41877b6":"markdown","d1982708":"markdown","1515b475":"markdown","f34625c2":"markdown","d5aeea7d":"markdown","9ed5d9f2":"markdown","fe17802a":"markdown","94e76545":"markdown","a6c72cdf":"markdown","bab12402":"markdown","5c7b50e1":"markdown"},"source":{"805986af":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nimport time\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# V\u1ebd\nimport matplotlib.pyplot as plt\n\nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","42fdc91e":"train_df = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/test.csv')\ntrain_df.info()\nprint(\"Train: \", train_df.shape)\nprint(\"Test: \", test_df.shape)\ntrain_df.head()","77c09576":"test_df.head()","7ca80322":"import unidecode\nimport re\nimport nltk\nimport string\nimport codecs\nimport spacy\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport pickle","e3d754a5":"print('S\u1ed1 gi\u00e1 tr\u1ecb NULL trong t\u1eadp train = %d' % (train_df['question_text'].isnull().sum()))\nprint('S\u1ed1 gi\u00e1 tr\u1ecb N\/A trong t\u1eadp train = %d' % (train_df['question_text'].isna().sum()))\n\nprint('S\u1ed1 gi\u00e1 tr\u1ecb NULL trong t\u1eadp test = %d' % (test_df['question_text'].isnull().sum()))\nprint('S\u1ed1 gi\u00e1 tr\u1ecb N\/A trong t\u1eadp test = %d' % (test_df['question_text'].isna().sum()))\n# Kh\u00f4ng c\u00f3 gi\u00e1 tr\u1ecb n\u00e0o","aca9a9e9":"import collections\n\ntarget = train_df['target']\ntarget_1 = 0\nfor target_value in target:\n    if target_value == 1:\n        target_1 += 1\nprint(\"S\u1ed1 c\u00e2u h\u1ecfi trong t\u1ec7p train:\", len(target))\nprint(\"S\u1ed1 c\u00e2u h\u1ecfi \u0111\u1ed9c h\u1ea1i l\u00e0:\", target_1)\n\nfig, ax = plt.subplots(1,1, figsize=(7,7))\nax.set_title(\"T\u1ef7 l\u1ec7 c\u00e1c c\u00e2u h\u1ecfi \u0111\u00e3 \u0111\u01b0\u1ee3c d\u00e1n nh\u00e3n\")\nexplode=(0,0.1)\nlabels ='H\u1eefu \u00edch','\u0110\u1ed9c h\u1ea1i'\ncolors = ['#00ff00', '#ff0000']\n\ncounts = list(dict(collections.Counter(list(train_df.target))).values()) \nax.pie(counts , explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',shadow=True, startangle=90)","a8bee523":"for i in range(5):\n    print(train_df['question_text'][i])","13597db4":"from wordcloud import WordCloud\n\n# Funcion d\u00f9ng \u0111\u1ec3 show wordcloud\ndef plot_cloud(wordcloud):\n    plt.figure(figsize=(10, 10))\n    plt.imshow(wordcloud) \n    plt.axis(\"off\");\n    plt.show();\n    \nprint('C\u00e1c t\u1eeb th\u01b0\u1eddng xu\u1ea5t hi\u1ec7n trong c\u00e2u h\u1ecfi toxic')\ntext =\" \".join(train_df[train_df[\"target\"] == 1][\"question_text\"])\n\n # T\u1ea1o wordcloud \nwordcloud1 = WordCloud(width = 1024, height = 1024, random_state=1, background_color='white', colormap='rainbow', collocations=False).generate(text)\nplot_cloud(wordcloud1)","8925f09b":"print('C\u00e1c t\u1eeb th\u01b0\u1eddng xu\u1ea5t hi\u1ec7n trong c\u00e2u h\u1ecfi ch\u00e2n th\u00e0nh')\ntext =\" \".join(train_df[train_df[\"target\"] == 0][\"question_text\"])\n\n # T\u1ea1o wordcloud \nwordcloud0 = WordCloud(width = 1024, height = 1024, random_state=1, background_color='white', colormap='rainbow', collocations=False).generate(text)\nplot_cloud(wordcloud0)","e7c89613":"stop_words = set(stopwords.words('english'))\n\ncontraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\",  \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" } \n\nfind_lem = WordNetLemmatizer()","46dfe8a5":"def clean_text(text):\n    \n     # Decode d\u1eef li\u1ec7u d\u1ea1ng text\n    try:\n        decoded = unidecode.unidecode(codecs.decode(text, 'unicode_escape'))\n    except:\n        decoded = unidecode.unidecode(text)\n        \n    # Khi xu\u1ea5t hi\u1ec7n d\u1ea5u nh\u00e1y \u0111\u01a1n, ta b\u1eaft \u0111\u1ea7u x\u1eed l\u00fd c\u00e1c ch\u1eef vi\u1ebft t\u1eaft\n    escape_handled = re.sub(\"\u2019\", \"'\", decoded)\n    text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in escape_handled.split(\" \")])\n    \n    # \u0110\u1ed5i c\u00e1c k\u00fd t\u1ef1 vi\u1ebft hoa sang th\u01b0\u1eddng\n    text = re.findall(r\"[a-zA-Z]+\", text.lower())\n    \n    text = [word for word in text if (word not in stop_words and len(word)>2)]\n    \n    # T\u00ecm t\u1eeb g\u1ed1c c\u1ee7a t\u1ea5t c\u1ea3 c\u00e1c t\u1eeb v\u00e0 chuy\u1ec3n \u0111\u1ed5i\n    text = [find_lem.lemmatize(word) for word in text]\n    \n    text = re.sub(r'(.)\\1+', r'\\1\\1', ' '.join(text))\n    \n    return text","402b2fee":"t_cleaned = \"How did Quebec D nationalists # #$%@&*()+_-;:<=>.?{} * Aren't can't've see C 2 ain't their F6 province as a nation in the 1960s?\"\n\nprint('Ch\u01b0a clean: ' + t_cleaned)\nprint('Sau khi clean: ' + clean_text(t_cleaned))","4f6460e4":"train_cleaned = train_df['question_text'].apply(lambda x: clean_text(x))\ntest_cleaned = test_df['question_text'].apply(lambda x: clean_text(x))\n\n# pickle.dump(train_df['question_text_cleaned'], open(\"train-cleaned.pickle\", \"wb\"))\n# pickle.dump(test_df['question_text_cleaned'], open(\"test-cleaned.pickle\", \"wb\"))\n\n# train_cleaned = pickle.load(open(os.path.join('\/kaggle\/input\/question-cleaned', 'train-cleaned.pickle'), 'rb'))\n# test_cleaned = pickle.load(open(os.path.join('\/kaggle\/input\/question-cleaned', 'test-cleaned.pickle'), 'rb'))\n","35d3d7b2":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(train_cleaned, train_df['target'], test_size=0.2, random_state=1)","48f92232":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing import sequence\n\n#max_words: S\u1ed1 l\u01b0\u1ee3ng t\u1eeb l\u1edbn nh\u1ea5t xu\u1ea5t hi\u1ec7n trong tokenizer \u0111\u01b0\u1ee3c l\u1ea5y t\u1eeb t\u1ea7n su\u1ea5t xu\u1ea5t hi\u1ec7n trong v\u0103n b\u1ea3n t\u1eeb cao xu\u1ed1ng th\u1ea5p.\nmax_words=50000\n#max_len: S\u1ed1 l\u01b0\u1ee3ng c\u00e1c t\u1eeb l\u1edbn nh\u1ea5t trong m\u1ed9t c\u00e2u h\u1ecfi.\nmax_len=100\n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(list(train_cleaned))\n\ntrain_tok =  tokenizer.texts_to_sequences(X_train)\ntest_tok =  tokenizer.texts_to_sequences(X_test)\n\ntest_res_tok = tokenizer.texts_to_sequences(test_cleaned)","4a772191":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(ngram_range=(1, 3), max_df=0.9)\n\ntfidf.fit(train_cleaned)\n\ntraining_features_tfidf = tfidf.transform(X_train)\ntest_features_tfidf = tfidf.transform(X_test)\n\nsmpl_vect = tfidf.transform(train_cleaned.sample(1, random_state = 1))\nsmpl_vect.toarray()","8e3a5e11":"from sklearn import metrics\nfrom sklearn.metrics import accuracy_score, f1_score","330bdd3a":"from sklearn.naive_bayes import MultinomialNB\nnb_model = MultinomialNB(alpha=0.08)\nnb_model.fit(training_features_tfidf, y_train)\nnb_predict = nb_model.predict(test_features_tfidf)\n\n# nb_model.fit(train_feature_tok, y_train)\n# nb_predict = nb_model.predict(test_feature_tok)\n\n\nprint('Accuracy :', accuracy_score(y_test, nb_predict))\nprint('F1 score :', f1_score(y_test, nb_predict))","acc4efa2":"from sklearn.linear_model import LogisticRegression\n\nlogReg = LogisticRegression(solver='liblinear')\n\nlogReg.fit(training_features_tfidf, y_train)\nlogReg = nb_model.predict(test_features_tfidf)\n\n# nb_model.fit(train_feature_tok, y_train)\n# nb_predict = nb_model.predict(test_feature_tok)\n\n\nprint('Accuracy :', accuracy_score(y_test, nb_predict))\nprint('F1 score :', f1_score(y_test, nb_predict))","8ac0967f":"from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Activation, Dropout, Flatten\nfrom tensorflow.keras.models import Model, Sequential\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.optimizers import RMSprop, Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping","9fbd7437":"def LSTM_Basic(maxword=50000, embedding_size=300, max_len=100, n_unit_lstm=64, n_unit_dense=64):\n    inputs = Input(name='inputs', shape=[max_len])\n    layer = Embedding(maxword, embedding_size, input_length=max_len)(inputs)\n\n    layer = LSTM(n_unit_lstm)(layer)\n    layer = Dense(n_unit_dense, name='FC1')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.4)(layer)\n    layer = Dense(1, name='out_layer')(layer)\n    layer = Activation('sigmoid')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model","15359482":"lstm = LSTM_Basic()\nlstm.summary()\nlstm.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])","3cce92c6":"# Do c\u00e1c pp nh\u01b0 tf-idf hay countvectorizer khi chuy\u1ec3n c\u00e1c t\u1eeb th\u00e0nh ma tr\u1eadn \u0111\u1ec1u \u0111\u00e3 token c\u00e1c t\u1eeb lu\u00f4n\n# nh\u01b0ng \u0111\u1ec3 s\u1eed d\u1ee5ng trong m\u00f4 h\u00ecnh LSTM, ta \u0111\u00e3 c\u00f3 s\u0103n Embedding Layer n\u00ean kh\u00f4ng c\u1ea7n s\u1eed d\u1ee5ng \u0111\u1ebfn ch\u00fang.\n\n# Padding d\u1eef li\u1ec7u \u0111\u00e3 tokenizer th\u00e0nh c\u00e1c chu\u1ed7i c\u00f3 \u0111\u1ed9 d\u00e0i b\u1eb1ng nhau\ntrain_feature_tok = sequence.pad_sequences(train_tok, maxlen=max_len)\ntest_feature_tok = sequence.pad_sequences(test_tok, maxlen=max_len)\ntest_res_tok = sequence.pad_sequences(test_res_tok, maxlen=max_len)\n\n# b\u1eaft \u0111\u1ea7u train\nlstm.fit(train_feature_tok, y_train, batch_size=512, epochs=2, validation_data=(test_feature_tok, y_test), verbose = 1)","72ac7c96":"pred_val = lstm.predict([test_feature_tok], batch_size=1024, verbose=1)\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(y_test, (pred_val>thresh).astype(int))))","b9f79dd6":"pred_t = lstm.predict([test_res_tok], batch_size=1024, verbose=1)\ntest_df['prediction'] = (np.array(pred_t) >= 0.5).astype(np.int)\nresult = test_df[['qid','prediction']]\nresult.set_index('qid', inplace=True)","ce755875":"result.to_csv('submission.csv')","63bdf3cc":"Th\u00eam m\u1ed9t s\u1ed1 th\u01b0 vi\u1ec7n x\u1eed l\u00fd ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean","97bd51d4":"CountVectorize s\u1ebd bi\u1ebfn t\u1eadp d\u1eef li\u1ec7u train th\u00e0nh m\u1ed9t ma tr\u1eadn v\u1edbi:\n* S\u1ed1 h\u00e0ng \u1ee9ng v\u1edbi s\u1ed1 c\u00e2u h\u1ecfi c\u00f3 trong t\u1eadp data train, s\u1ed1 c\u1ed9t \u1ee9ng v\u1edbi s\u1ed1 t\u1eeb trong t\u1eadp\n* Trong m\u1ed9t h\u00e0ng, gi\u00e1 tr\u1ecb c\u1ee7a c\u00e1c c\u1ed9t th\u1ec3 hi\u1ec7n s\u1ed1 l\u01b0\u1ee3ng c\u00e1c t\u1eeb m\u00e0 h\u00e0ng \u0111\u00f3 c\u00f3 (0->N)<br>\n> Trong t\u1eadp test ta ch\u1ec9 c\u1ea7n \u0111\u1ebfm n\u00f3 theo ma tr\u1eadn \u0111\u00e3 c\u00f3 \u1edf t\u1eadp train","5aaff2da":"**Chuy\u1ec3n c\u00e1c t\u1eeb vi\u1ebft t\u1eaft v\u1ec1 d\u1ea1ng \u0111\u1ea7y \u0111\u1ee7**<br>\nTrong qu\u00e1 tr\u00ecnh train, c\u00e1c t\u1eeb vi\u1ebft t\u1eaft \u0111\u00f4i khi l\u00e0m nhi\u1ec5u th\u00f4ng tin, c\u0169ng nh\u01b0 t\u1ea1o ra c\u00e1c chi\u1ec1u vector th\u1eeba th\u00e3i, kh\u00f4ng c\u00f3 \u00edch, ta c\u1ea7n ph\u1ea3i \u0111\u01b0a ch\u00fang v\u1ec1 d\u1ea1ng ho\u00e0n ch\u1ec9nh <br>\nC\u00e1ch l\u00e0m:\n* \u0110\u1ea7u ti\u00ean, ta khai b\u00e1o m\u1ed9t dict v\u1edbi c\u00e1c t\u1eeb vi\u1ebft t\u1eaft v\u00e0 d\u1ea1ng ho\u00e0n ch\u1ec9nh c\u1ee7a n\u00f3\n* Ti\u1ebfp theo, ta s\u1ebd decode \u0111\u1ec3 t\u00ecm ra c\u00e1c d\u1ea5u nh\u00e1y \u0111\u01a1n nh\u1eb1m t\u00ecm c\u00e1c t\u1eeb vi\u1ebft t\u1eaft, sau \u0111\u00f3 mapping ch\u00fang v\u1ec1 d\u1ea1ng \u0111\u00fang","bcb56cb0":"Khi n\u00f3i \u0111\u1ebfn b\u00e0i to\u00e1n ph\u00e2n l\u1edbp cho d\u1eef li\u1ec7u NLP, m\u00f4 h\u00ecnh \u0111\u1ea7u ti\u00ean ta c\u00f3 th\u1ec3 ngh\u0129 ngay \u0111\u1ebfn l\u00e0 Naive Bayes Classifiers, b\u1edfi v\u00ec \u0111\u00e2y l\u00e0 m\u1ed9t thu\u1eadt to\u00e1n ph\u00e2n l\u1edbp c\u00f3 \u0111\u1ed9 hi\u1ec7u qu\u1ea3, t\u00ednh ch\u00ednh x\u00e1c cao v\u00e0 th\u1eddi gian hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh nh\u1ecf.","a1cf292e":"> \u0110\u00e1nh gi\u00e1: Nh\u01b0 ta th\u1ea5y ph\u1ea7n l\u1edbn c\u00e1c c\u00e2u h\u1ecfi l\u00e0 h\u1eefu \u00edch (93,8%) v\u00e0 ch\u1ec9 c\u00f3 m\u1ed9t ph\u1ea7n nh\u1ecf c\u00e1c c\u00e2u h\u1ecfi l\u00e0 \u0111\u1ed9c h\u1ea1i. D\u1eef li\u1ec7u n\u00e0y b\u1ecb m\u1ea5t c\u00e2n b\u1eb1ng r\u1ea5t nghi\u00eam tr\u1ecdng n\u00ean khi \u0111\u00e1nh gi\u00e1 ch\u1ea5t l\u01b0\u1ee3ng m\u00f4 h\u00ecnh m\u00e0 ta \u00e1p d\u1ee5ng:\n* Ta s\u1ebd \u00edt tin t\u01b0\u1edfng v\u00e0o accuracy score khi \u0111\u00e1nh gi\u00e1. \n* T\u00ecm h\u01b0\u1edbng x\u1eed l\u00fd \u0111\u1ec3 gi\u1ea3i quy\u1ebft t\u00ecnh tr\u1ea1ng n\u00e0y (VD: Thay \u0111\u1ed5i metrix, OverSampling, UnderSampling, SMOTE...v..v).","53e81e04":"# Workflow\n1. <a href=\"#1.-Problem-description\">Problem description<\/a>\n2. <a href=\"#2.-Data-preprocessing\">Data preprocessing<\/a>\n    * <a href=\"#Cleaning-Data\">Cleaning Data<\/a>\n    * <a href=\"#Vectorization\">Vectorization<\/a>\n3. <a href=\"#3.-Training\">Training<\/a>\n    * <a href=\"#Naive-Bayes\">Naive Bayes<\/a>\n    * <a href=\"#Logistic-Regression\">Logistic Regression<\/a>\n    * <a href=\"#LSTM\">LSTM<\/a>\n4. <a href=\"#4.-Conclution\">Conclution<\/a>\n5. <a href=\"#5.-Result\">Result<\/a>","c72ec026":"\u0110\u00e1nh gi\u00e1: Nh\u01b0 v\u1eady m\u1ed9t t\u1eeb ph\u1ed5 bi\u1ebft s\u1ebd c\u00f3 idf c\u00e0ng nh\u1ecf v\u00e0 tf-idf c\u00e0ng l\u1edbn\n\u1ede \u0111\u00e2y ta c\u00f2n s\u1eed d\u1ee5ng th\u00eam tham s\u1ed1 N-gram \u0111\u1ec3 ch\u1ecdn th\u00eam c\u00e1c chi\u1ec1u c\u1ee7a t\u1eeb c\u0169ng nh\u01b0 tham s\u1ed1 max_df \u0111\u1ec3 lo\u1ea1i b\u1ecf nh\u1eefng t\u1eeb m\u00e0 ta s\u1ebd coi n\u00f3 nh\u01b0 m\u1ed9t stopword khi n\u00f3 xu\u1ea5t hi\u1ec7n qu\u00e1 nhi\u1ec1u trong dataset (\u1edf \u0111\u00e2y ta cho l\u00e0 90%) v\u00e0 kh\u00f4ng c\u00f3 \u00edch.","c90960fd":"> C\u00e1ch l\u00e0m nh\u01b0 tr\u00ean l\u00e0 c\u00e1ch \u0111\u01a1n gi\u1ea3n nh\u1ea5t, tuy v\u1eady kh\u00f4ng ph\u1ea3i trong t\u1ea5t c\u1ea3 c\u00e1c c\u00e2u h\u1ecfi c\u1ea7n ph\u00e2n lo\u1ea1i c\u0169ng \u0111\u1ec1u c\u00f3 h\u1ebft nh\u1eefng t\u1eeb thu\u1ed9c t\u1eadp train, n\u00ean ma tr\u1eadn sinh ra, hay c\u1ee5 th\u1ec3 h\u01a1n l\u00e0 c\u00e1c vector c\u1ee7a m\u1ed7i c\u00e2u h\u1ecfi s\u1ebd l\u00e0 nh\u1eefng sparse vector v\u00e0 c\u00e1c vector \u0111\u1eb7c tr\u01b0ng n\u00e0y c\u0169ng r\u1ea5t d\u00e0i, \u0111\u00e2y l\u00e0 m\u1ed9t nh\u01b0\u1ee3c \u0111i\u1ec3m c\u1ee7a ph\u01b0\u01a1ng ph\u00e1p n\u00e0y, ch\u00fang ta c\u00f3 th\u1ec3 c\u1ea3i ti\u1ebfn b\u1eb1ng c\u00e1ch s\u1eed d\u1ee5ng ph\u01b0\u01a1ng ph\u00e1p TF-IDF.","c6b51f6d":"C\u0169ng gi\u1ed1ng nh\u01b0 c\u00e1c thu\u1eadt to\u00e1n Linear (t\u00ecm v\u00e0 t\u1ed1i \u01b0u w) v\u1edbi c\u00f4ng th\u1ee9c chung: $$y = f(w^{T}x),(w \\in [w_{0}, w_{1},... w_{n}])$$\nNh\u01b0ng kh\u00e1c v\u01a1i c\u00e1c thu\u1eadt to\u00e1n nh\u01b0 Perceptron Learning (s\u1eed d\u1ee5ng h\u00e0m sgn() \u0111\u1ec3 ph\u00e2n $w^{T}x$ v\u00e0o 2 class) hay Linear Regression (t\u00ednh tr\u1ef1c ti\u1ebfp v\u00e0 \u0111\u01b0a ra k\u1ebft qu\u1ea3 c\u1ee7a $w^{T}x$). <br>\nLogicstic Regression s\u1eed d\u1ee5ng c\u00e1c h\u00e0m k\u00edch ho\u1ea1t (activation function) \u0111\u1ec3 bi\u1ebfn \u0111\u1ea7u ra c\u1ee7a b\u00e0i to\u00e1n th\u00e0nh x\u00e1c su\u1ea5t m\u00e0 c\u00e2u h\u1ecfi s\u1ebd r\u01a1i v\u00e0o $C_{1}$ hay $C_{0}$. L\u00fac n\u00e0y b\u00e0i to\u00e1n c\u00f3 d\u1ea1ng: $$y = \\theta (w^{T}x)$$ \nV\u1edbi theta($\\theta$) l\u00e0 h\u00e0m k\u00edch ho\u1ea1t. H\u00e0m k\u00edch ho\u1ea1t \u1edf \u0111\u00e2y c\u00f3 th\u1ec3 l\u00e0 (sigmoid, tanh, reLu,...), \u1edf \u0111\u00e2y ta d\u00f9ng h\u00e0m sigmoid, m\u1ed9t h\u00e0m kh\u00e1 ph\u1ed5 bi\u1ebfn v\u1edbi gi\u00e1 tr\u1ecb n\u1eb1m trong \u0111o\u1ea1n t\u1eeb 0 \u0111\u1ebfn 1.<br>\n![img](https:\/\/www.researchgate.net\/profile\/Knut-Kvaal\/publication\/239269767\/figure\/fig2\/AS:643520205430784@1530438581076\/An-illustration-of-the-signal-processing-in-a-sigmoid-function.png)","8f6730df":"**Tokenizer**<br>\nCh\u00fang ta c\u1ea7n t\u1ea1o ra m\u1ed9t t\u1eeb \u0111i\u1ec3n mapping m\u1ed7i t\u1eeb v\u1edbi index t\u01b0\u01a1ng \u1ee9ng c\u1ee7a n\u00f3 \u0111\u1ec3 ti\u1ec7n cho vi\u1ec7c t\u1ea1o ra ma tr\u1eadn s\u1ed1. Module tokenizer d\u1ec5 d\u00e0ng gi\u00fap ta th\u1ef1c hi\u1ec7n vi\u1ec7c n\u00e0y.","1f96e9b4":"# 5. Result","5d99faa4":"# 3. Training\n> Trong c\u00e1c b\u00e0i to\u00e1n ph\u00e2n lo\u1ea1i nh\u1ecb ph\u00e2n nh\u01b0 th\u1ebf n\u00e0y, nh\u1eefng m\u00f4 h\u00ecnh \u0111\u01a1n gi\u1ea3n \u0111\u1ea7u ti\u00ean ta c\u00f3 th\u1ec3 ngh\u0129 \u0111\u1ebfn nh\u01b0 Logistic Regression, native bayes, c\u00e2y quy\u1ebft \u0111\u1ecbnh ...v...v. Trong qu\u00e1 tr\u00ecnh train sau \u0111\u00e2y ta s\u1eed d\u1ee5ng 3 m\u00f4 h\u00ecnh:\n* Naive Bayes\n* Logistic Regression\n* LSTM","9248c228":"# 2. Data preprocessing\nTrong ph\u1ea7n tr\u00ean, ta \u0111\u00e3 hi\u1ec3u s\u01a1 qua v\u1ec1 d\u1eef li\u1ec7u d\u00f9ng \u0111\u1ec3 hu\u1ea5n luy\u1ec7n, nh\u01b0ng nh\u01b0 v\u1eady l\u00e0 ch\u01b0a \u0111\u1ee7.\n    => Ta c\u1ea7n t\u00ecm hi\u1ec3u th\u00eam \u0111\u1ec3 b\u1eaft \u0111\u1ea7u \u0111i v\u00e0o x\u1eed l\u00fd","a301f69b":"### Vectorization\n>Tr\u01b0\u1edbc khi \u0111\u01b0a v\u00e0o hu\u1ea5n luy\u1ec7n ta \u0111\u00e3 l\u00e0m s\u1ea1ch d\u1eef li\u1ec7u m\u1ed9t c\u00e1ch t\u01b0\u01a1ng \u0111\u1ed1i, nh\u01b0ng \u0111\u1ec3 \u0111\u01b0a v\u00e0o m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n n\u00e0y, ta c\u1ea7n ph\u1ea3i m\u00e3 h\u00f3a d\u1eef li\u1ec7u d\u01b0\u1edbi d\u1ea1ng s\u1ed1.","45e5fa1d":"M\u00f4 h\u00ecnh LSTM c\u00f3 k\u1ebft qu\u1ea3 t\u1ed1t nh\u1ea5t n\u00ean ta s\u1ebd s\u1eed d\u1ee5ng \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n\n> V\u00ec k\u1ebft qu\u1ea3 \u0111\u1ea7u ra c\u1ee7a LSTM \u0111\u01b0\u1ee3c ta d\u00f9ng h\u00e0m sigmoid n\u00ean k\u1ebft qu\u1ea3 tr\u1ea3 v\u1ec1 s\u1ebd l\u00e0 x\u00e1c su\u1ea5t m\u00e0 c\u00e2u h\u1ecfi l\u00e0 \u0111\u1ed9c h\u1ea1i, n\u00ean ta s\u1ebd chuy\u1ec3n \u0111\u1ed5i ch\u00fang th\u00e0nh one-hot v\u1edbi vi\u1ec7c p > 50% s\u1ebd l\u00e0 c\u00e2u h\u1ecfi \u0111\u1ed9c h\u1ea1i.","9f400530":"V\u1edbi m\u00f4 h\u00ecnh Multinomial NB m\u00e0 ta s\u1eed d\u1ee5ng, m\u1ed7i c\u00e2u h\u1ecfi \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n b\u1edfi m\u1ed9t vector c\u00f3 \u0111\u1ed9 d\u00e0i d ch\u00ednh l\u00e0 s\u1ed1 t\u1eeb trong t\u1eadp data. Gi\u00e1 tr\u1ecb c\u1ee7a th\u00e0nh ph\u1ea7n th\u1ee9 i trong m\u1ed7i vector ch\u00ednh l\u00e0 s\u1ed1 l\u1ea7n t\u1eeb th\u1ee9 i xu\u1ea5t hi\u1ec7n trong v\u0103n b\u1ea3n \u0111\u00f3 (\u0111\u00e3 \u0111\u01b0\u1ee3c gia\u1ec9 th\u00edch \u1edf ph\u1ea7n Ti\u1ec1n x\u1eed l\u00fd).$$$$\nKhi \u0111\u00f3, $P(x_{i}|C)$ t\u1ec9 l\u1ec7 v\u1edbi t\u1ea7n su\u1ea5t t\u1eeb th\u1ee9 i xu\u1ea5t hi\u1ec7n trong c\u00e1c v\u0103n b\u1ea3n c\u1ee7a class C. B\u00e0i to\u00e1n s\u1ebd quy v\u1ec1 vi\u1ec7c t\u00ednh x\u00e1c su\u1ea5t c\u1ee7a: $$P(x_{i}|C) = \\frac{N_{ci}}{N_{c}}$$\n\nTh\u01b0 v\u1ec9\u1ec7n sklearn \u0111\u00e3 cung c\u1ea5p cho ch\u00fang ta m\u00f4 h\u00ecnh n\u00e0y","d1a7c38d":"> \u0110\u00e1nh gi\u00e1: Qu\u00e1 tr\u00ecnh cleaned m\u1ea5t kh\u00e1 nhi\u1ec1u th\u1eddi gian, \u1edf \u0111\u00e2y sau khi clean ta t\u1ea3i d\u1eef li\u1ec7u l\u00ean input c\u1ee7a b\u00e0i \u0111\u1ec3 d\u1ec5 d\u00e0ng s\u1eed d\u1ee5ng sau n\u00e0y.","d997d159":"Sau khi ki\u1ec3m tra xong d\u1eef li\u1ec7u, ta c\u00f3 th\u1ec3 th\u1ea5y d\u1eef li\u1ec7u \u0111ang kh\u00e1 ph\u1ee9c t\u1ea1p:\n   * \u0110\u1ea7u ti\u00ean ta c\u1ea7n lo\u1ea1i b\u1ecf c\u00e1c kho\u1ea3ng tr\u1ed1ng, c\u00e1c k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t\n   * C\u00e1c t\u1eeb d\u00f9ng \u0111\u1ec3 h\u1ecfi xu\u1ea5t hi\u1ec7n \u1edf m\u1ed7i c\u00e2u, nh\u01b0ng kh\u00f4ng su\u1ea5t hi\u1ec7n tr\u00ean WordCloud => Kh\u00f4ng c\u00f3 \u00edch khi train\n   * C\u00e1c t\u1eeb d\u1eebng (is, a, an,...) c\u0169ng c\u1ea7n \u0111\u01b0\u1ee3c lo\u1ea1i b\u1ecf\n   * C\u00e1c k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t, c\u00e1c s\u1ed1 th\u01b0\u1eddng xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u01b0ng kh\u00f4ng quan tr\u1ecdng trong qu\u00e1 tr\u00ecnh train => lo\u1ea1i b\u1ecf\n   * Chuy\u1ec3n c\u00e1c t\u1eeb r\u00fat g\u1ecdn v\u1ec1 d\u1ea1ng ho\u00e0n ch\u1ec9nh\n   * Chuy\u1ec3n c\u00e1c t\u1eeb vi\u1ebft hoa v\u1ec1 ch\u1eef vi\u1ebft th\u01b0\u1eddng \u0111\u1ec3 d\u1ec5 train\n   * T\u00ecm c\u00e1c t\u1eeb g\u1ed1c c\u1ee7a t\u1eeb","82fe9e8f":"**X\u00f3a c\u00e1c t\u1eeb d\u1eebng, c\u00e1c t\u1eeb d\u00f9ng \u0111\u1ec3 \u0111\u1eb7t c\u00e2u h\u1ecfi** <br>\nC\u00e1c t\u1eeb d\u1eebng trong ti\u1ebfng anh (is, a, an, ...) th\u01b0\u1eddng xu\u1ea5t hi\u1ec7n kh\u00e1 nhi\u1ec1u nh\u01b0ng l\u1ea1i kh\u00f4ng qu\u00e1 quan tr\u1ecdng trong vi\u1ec7c train, nh\u1eefng c\u00e2u d\u00f9ng \u0111\u1ec3 \u0111\u1eb7t c\u00e2u h\u1ecfi c\u0169ng v\u1eady, ta s\u1ebd lo\u1ea1i b\u1ecf ch\u00fang\n<br>C\u00e1ch l\u00e0m: \n* ta khai b\u00e1o c\u00e1c t\u1eeb \u0111\u00f3, t\u00ecm ch\u00fang trong c\u00e2u v\u00e0 lo\u1ea1i b\u1ecf, th\u01b0 vi\u1ec7n 'nltk' \u0111\u00e3 cung c\u1ea5p cho ch\u00fang ta t\u1ea5t c\u1ea3 nh\u1eefng th\u1ee9 c\u1ea7n ph\u1ea3i lo\u1ea1i b\u1ecf n\u00e0y.<br>","39e83663":"Ph\u01b0\u01a1ng ph\u00e1p ph\u1ed5 bi\u1ebfn \u0111\u1ea7u ti\u00ean ta c\u00f3 th\u1ec3 d\u00f9ng \u0111\u1ec3 \u0111\u01b0a c\u00e1c d\u1eef li\u1ec7u d\u1ea1ng text v\u1ec1 c\u00e1c vector m\u00e0 trong \u0111\u00f3 m\u1ed7i ph\u1ea7n t\u1eed l\u00e0 m\u1ed9t s\u1ed1: 'Bags of words'","26030d09":"\u0110\u1ec3 x\u1eed l\u00fd d\u1eef li\u1ec7u, ta c\u1ea7n lo\u1ea1i b\u1ecf c\u00e1c s\u1eef li\u1ec7u b\u1ecb m\u1ea5t, kh\u00f4ng x\u00e1c \u0111\u1ecbnh. <br>  \nTa s\u1ebd ki\u1ec3m tra c\u00e1c gi\u00e1 tr\u1ecb NULL, N\/A c\u00f3 trong t\u1eadp d\u1eef li\u1ec7u, n\u1ebfu c\u00f3 th\u00ec s\u1ebd lo\u1ea1i b\u1ecf","da9ab1c8":"**Chuy\u1ec3n c\u00e1c ch\u1eef vi\u1ebft hoa th\u00e0nh th\u01b0\u1eddng** <br>\n\u0110\u1ec3 d\u1ec5 d\u00e0ng train, c\u0169ng nh\u01b0 gi\u1ea3m chi\u1ec1u c\u1ee7a c\u00e1c vector \u0111\u1eb7c tr\u01b0ng sau n\u00e0y, vi\u1ec7c chuy\u1ec3n \u0111\u1ed5i c\u00e1c ch\u1eef hoa sang th\u01b0\u1eddng l\u00e0 \u0111i\u1ec1u c\u1ea7n thi\u1ebft <br>\nC\u00e1ch l\u00e0m: \n* Ta s\u1ebd t\u00ecm nh\u1eefng ch\u1eef c\u00e1i vi\u1ebft hoa v\u00e0 chuy\u1ec3n to\u00e0n b\u1ed9 ch\u00fang th\u00e0nh ch\u1eef th\u01b0\u1eddng","76b2997f":"### Load data d\u00f9ng \u0111\u1ec3 hu\u1ea5n luy\u1ec7n model\n\u0110\u1ec3 x\u00e2y d\u1ef1ng m\u1ed9t m\u00f4 h\u00ecnh ta c\u1ea7n ph\u1ea3i c\u00f3 m\u1ed9t t\u1eadp d\u1eef li\u1ec7u.$$$$\nTa load d\u1eef li\u1ec7u \u0111\u00e3 cho, sau \u0111\u00f3:\n * Ki\u1ec3m tra c\u00e1c th\u00e0nh ph\u1ea7n trong t\u1eadp d\u1eef li\u1ec7u\n * Ki\u1ec3m tra l\u01b0\u1ee3ng d\u1eef li\u1ec7u c\u00f3 trong c\u00e1c t\u1eadp","0627535e":"Sau \u0111\u00e2y ta s\u1ebd l\u1ea7n l\u01b0\u1ee3t gi\u1ea3i quy\u1ebft t\u1eebng v\u1ea5n \u0111\u1ec1 n\u00e0y","d76507b5":"Ta s\u1ebd th\u1eed clean 1 \u0111o\u1ea1n d\u1eef li\u1ec7u v\u00e0 in ra \u0111\u1ec3 ki\u1ec3m tra k\u1ebft qu\u1ea3 sau khi clean.","84d654a7":"# B\u00e1o C\u00e1o B\u00e0i T\u1eadp L\u1edbn H\u1ecdc M\u00e1y Cu\u1ed1i K\u1ef3\n* **H\u1ecd v\u00e0 t\u00ean:** D\u01b0\u01a1ng Qu\u1ed1c C\u01b0\u1eddng\n* **M\u00e3 s\u1ed1 sinh vi\u00ean:** 18020254\n* **M\u00e3 l\u1edbp:** INT3405_1","128b1fba":"* T\u00ednh c\u00e2n b\u1eb1ng c\u1ee7a t\u1eadp d\u1eef li\u1ec7u \u1ea3nh h\u01b0\u1edfng r\u1ea5t l\u1edbn \u0111\u1ebfn qu\u00e1 tr\u00ecnh train, v\u00e0 trong b\u00e0i to\u00e1n n\u00e0y vi\u1ec7c d\u1ef1 \u0111o\u00e1n \u0111\u01b0\u1ee3c \u0111\u00fang c\u00e2u h\u1ecfi \u0111\u1ed9c h\u1ea1i hay kh\u00f4ng l\u1ea1i quan tr\u1ecdng h\u01a1n vi\u1ec7c d\u1ef1 \u0111o\u00e1n \u0111\u00fang c\u00e2u h\u1ecfi h\u1eefu \u00edch. Th\u1ebf n\u00ean ta c\u1ea7n ph\u1ea3i bi\u1ebft \u0111\u01b0\u1ee3c m\u1ee9c \u0111\u1ed9 c\u00e2n b\u1eb1ng c\u1ee7a t\u1eadp d\u1eef li\u1ec7u. <br>\n* Ta s\u1ebd ki\u1ec3m tra t\u1ef7 l\u1ec7 d\u1eef li\u1ec7u training theo nh\u00e3n \u0111\u1ec3 bi\u1ebft \u0111\u01b0\u1ee3c \u0111i\u1ec1u n\u00e0y:","9159ba47":"> M\u00f4 h\u00ecnh c\u00f3 h\u1ed9i t\u1ee5 nh\u01b0ng do ch\u1ec9 s\u1eed d\u1ee5ng c\u00e1c layer \u0111\u01a1n gi\u1ea3n n\u00ean k\u1ebft qu\u1ea3 s\u1ebd ch\u1ec9 h\u1ed9i t\u1ee5 \u0111\u1ebfn m\u1ed9t ng\u01b0\u1ee1ng n\u00e0o \u0111\u00f3 v\u00e0 ch\u01b0a th\u1ec3 cho c\u00e1c k\u1ebft qu\u1ea3 t\u1ed1t h\u01a1n (acuraccy trong kho\u1ea3ng 9.6-9.7).","09ef70db":"Ta s\u1ebd xem m\u1ed9t ph\u1ea7n d\u1eef li\u1ec7u hi\u1ec7n t\u1ea1i \u0111\u1ec3 t\u00ecm hi\u1ec3u c\u0169ng nh\u01b0 t\u00ecm h\u01b0\u1edbng x\u1eed l\u00fd <br>","968790e0":"> Trong c\u1ea3 2 t\u1eadp train v\u00e0 test \u0111\u1ec1u kh\u00f4ng c\u00f3.","de1958f4":"> \u0110\u00e1nh gi\u00e1: nh\u01b0 v\u1eady d\u1eef li\u1ec7u \u0111\u00e3 \u0111\u01b0\u1ee3c clean th\u00e0nh d\u1ea1ng kh\u00e1 \u0111\u1eb9p, ta c\u00f3 th\u1ec3 b\u1eaft \u0111\u1ea7u clean c\u00e1c t\u1eadp d\u1eef li\u1ec7u train v\u00e0 test<br>","5879302e":"\u0110\u00e1nh gi\u00e1:\n* \u0110\u00e2y l\u00e0 m\u1ed9t m\u00f4 h\u00ecnh kh\u00e1 \u0111\u01a1n gi\u1ea3n v\u1edbi k\u1ebft qu\u1ea3 nh\u1eadn \u0111\u01b0\u1ee3c \u0111\u1ec1u \u1edf m\u1ee9c trung b\u00ecnh.\n* Tuy m\u00f4 h\u00ecnh kh\u00f4ng y\u00eau c\u1ea7u d\u1eef li\u00eau l\u00e0 linearly separable, nh\u01b0ng \u0111\u01b0\u1eddng ph\u00e2n chia gi\u1eefa 2 class v\u1eabn l\u00e0 \u0111\u01b0\u1eddng tuy\u1ebfn t\u00ednh, n\u00ean c\u01a1 b\u1ea3n, b\u1ed9 d\u1eef li\u1ec7u c\u0169ng ph\u1ea3i r\u1ea5t g\u1ea7n v\u1edbi linearly separable, trong b\u00e0i to\u00e1n n\u00e0y, theo em m\u00f4 h\u00ecnh s\u1ebd ho\u1ea1t \u0111\u1ed9ng kh\u00f4ng t\u1ed1t do d\u1eef li\u1ec7u l\u00e0 c\u00e1c t\u1eeb m\u00e0 \u0111\u00f4i khi r\u1ea5t th\u01b0\u1eddng xu\u1ea5t hi\u1ec7n \u1edf c\u1ea3 2 class h\u1eefu \u00edch v\u00e0 \u0111\u1ed9c h\u1ea1i n\u00ean r\u1ea5t kh\u00f3 \u0111\u1ec3 m\u00f4 h\u00ecnh c\u00f3 th\u1ec3 d\u1ef1 \u0111o\u00e1n.\n* M\u00f4 h\u00ecnh c\u00f2n y\u00eau c\u1ea7u c\u00e1c \u0111i\u1ec3m d\u1eef li\u1ec7u l\u00e0 \u0111\u1ed9c l\u1eadp v\u1edbi nhau nh\u01b0ng trong b\u00e0i to\u00e1n n\u00e0y r\u1ea5t kh\u00f3 s\u1ea3y ra, v\u00ec c\u00e1c t\u1eeb c\u00f9ng xu\u1ea5t hi\u1ec7n \u0111\u00f4i khi \u1ea3nh h\u01b0\u1edfng l\u1eabn nhau **=> \u0110i\u1ec1u n\u00e0y d\u1eabn \u0111\u1ebfn vi\u1ec7c \u00e1p d\u1ee5ng m\u1ea1ng neural th\u00f4ng th\u01b0\u1eddng v\u1edbi \u0111\u1ea7u v\u00e0o v\u00e0 \u0111\u1ea7u ra \u0111\u1ed9c l\u1eadp v\u1edbi nhau kh\u00f4ng ph\u00f9 h\u1ee3p v\u1edbi d\u1eef li\u1ec7u d\u1ea1ng n\u00e0y**.\n* Thu\u1eadt to\u00e1n c\u0169ng ch\u1ea1y ch\u1eadm h\u01a1n so v\u1edbi NB v\u00ec m\u1ed7i l\u1ea7n c\u1eadp nh\u1eadt tr\u1ecdng s\u1ed1 ta l\u1ea1i ph\u1ea3i t\u00ednh l\u1ea1i \u0111\u1ea1o h\u00e0m c\u1ee7a Loss funcion v\u1edbi t\u1eebng bi\u1ebfn.<br>\nTa c\u00f3 th\u1ec3 gi\u1ea3m th\u1eddi gian training c\u1ee7a m\u00f4 h\u00ecnh b\u1eb1ng c\u00e1ch s\u1eed d\u1ee5ng c\u00e1c mini batch nh\u01b0ng \u1edf m\u00f4 h\u00ecnh n\u00e0y em s\u1ebd kh\u00f4ng \u0111\u1ec1 c\u1eadp \u0111\u1ebfn n\u00f3 do em \u0111ang h\u01b0\u1edbng \u0111\u1ebfn m\u1ed9t m\u00f4 h\u00ecnh c\u00f3 k\u1ebft qu\u1ea3 t\u1ed1t h\u01a1n.","67e78f3b":"K\u1ebft qu\u1ea3: C\u00e1c c\u00e2u h\u1ecfi trong t\u1eadp train v\u00e0 test \u0111\u1ec1u kh\u00e1 t\u01b0\u01a1ng \u0111\u1ed3ng. T\u1eadp training data c\u00f3 kho\u1ea3ng 1,3 tri\u1ec7u c\u00e2u h\u1ecfi v\u00e0 t\u1eadp test c\u00f3 kho\u1ea3ng 400k <br>\nT\u1eeb 5 d\u00f2ng \u0111\u1ea7u ti\u00ean trong t\u1eadp data ta \u0111\u01b0\u1ee3c bi\u1ebft:\n* qid l\u00e0 id c\u1ee7a c\u00e2u h\u1ecfi \n* question_text ch\u1ee9a n\u1ed9i dung c\u00e2u h\u1ecfi => d\u1eef li\u1ec7u ch\u00ednh c\u1ea7n x\u1eed l\u00fd \u0111\u1ec3 hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh\n* \u0110\u1ed1i v\u1edbi t\u1eadp train, target l\u00e0 label c\u1ee7a c\u00e2u h\u1ecfi \u0111\u00f3(1 n\u1ebfu insincere(sau n\u00e0y t\u1ea1m g\u1ecdi l\u00e0 c\u00e2u h\u1ecfi \u0111\u1ed9c h\u1ea1i) v\u00e0 0 v\u1edbi sincere(h\u1eefu \u00edch))","1a7fac92":"**Chuy\u1ec3n c\u00e1c ch\u1eef vi\u1ebft hoa th\u00e0nh th\u01b0\u1eddng** <br>\n\u0110\u1ec3 d\u1ec5 d\u00e0ng train, c\u0169ng nh\u01b0 gi\u1ea3m chi\u1ec1u c\u1ee7a c\u00e1c vector \u0111\u1eb7c tr\u01b0ng sau n\u00e0y, vi\u1ec7c chuy\u1ec3n \u0111\u1ed5i c\u00e1c ch\u1eef hoa sang th\u01b0\u1eddng l\u00e0 \u0111i\u1ec1u c\u1ea7n thi\u1ebft <br>\nC\u00e1ch l\u00e0m: \n* Ta s\u1ebd t\u00ecm nh\u1eefng ch\u1eef c\u00e1i vi\u1ebft hoa v\u00e0 chuy\u1ec3n to\u00e0n b\u1ed9 ch\u00fang th\u00e0nh ch\u1eef th\u01b0\u1eddng","3a5ca346":"> Ta c\u00f3 th\u1ec3 th\u1ea5y r\u00f5 r\u00e0ng k\u1ebft qu\u1ea3 c\u1ee7a LSTM t\u1ed1t h\u01a1n nhi\u1ec1u so v\u1edbi c\u00e1c m\u00f4 h\u00ecnh c\u00f2n l\u1ea1i","a5f64284":" Ti\u1ebfp t\u1ee5c d\u00f9ng WordCloud \u0111\u1ec3 ki\u1ec3m tra t\u1ea7n su\u1ea5t m\u1ed9t s\u1ed1 t\u1eeb th\u01b0\u1eddng xuy\u00ean xu\u1ea5t hi\u1ec7n trong t\u1eadp d\u1eef li\u1ec7u.","b3ed082c":"**RNN**<br>\nRNN l\u00e0 m\u1ed9t trong hai m\u1ea1ng neural l\u1edbn c\u1ee7a Deep Learning v\u00e0 n\u00f3 d\u00f9ng \u0111\u1ec3 x\u1eed l\u00fd c\u00e1c d\u1eef li\u1ec7u d\u1ea1ng sequence hay time-series. N\u00ean khi \u1ee9ng d\u1ee5ng DL \u0111\u1ec3 x\u1eed l\u00fd b\u00e0i to\u00e1n d\u1ea1ng text n\u00e0y, ta s\u1ebd ngh\u0129 ngay \u0111\u1ebfn RNN.<br>\n![](https:\/\/liyanxu.blog\/wp-content\/uploads\/2019\/01\/Screen-Shot-2019-01-24-at-19.46.54.png)\n* \u00dd t\u01b0\u1edfng c\u1ee7a RNN l\u00e0 s\u1eed d\u1ee5ng c\u00e1c d\u1eef li\u1ec7u \u0111\u00e3 \u0111\u01b0\u1ee3c t\u00ednh to\u00e1n t\u1eeb c\u00e1c b\u01b0\u1edbc x\u1eed l\u00fd tr\u01b0\u1edbc \u0111\u1ec3 \u0111\u01b0a ra d\u1ef1 \u0111o\u00e1n ch\u00ednh x\u00e1c nh\u1ea5t cho b\u01b0\u1edbc hi\u1ec7n t\u1ea1i. <br>\n* V\u1edbi hai \u0111\u1ea7u v\u00e0o $x_{t}$ l\u00e0 d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o(c\u00e2u h\u1ecfi) c\u1ee7a state hi\u1ec7n t\u1ea1i, $h_{t - 1}$ l\u00e0 th\u00f4ng tin \u0111\u01b0\u1ee3c l\u01b0u tr\u1eef \u1edf state tr\u01b0\u1edbc. $h_{t}$ l\u00e0 gi\u00e1 tr\u1ecb \u0111\u1ea7u ra d\u1ef1 \u0111o\u00e1n c\u0169ng nh\u01b0 input c\u1ee7a state ti\u1ebfp theo. Activation funcion th\u01b0\u1eddng \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u1edf \u0111\u00e2y l\u00e0 tanh ho\u1eb7c reLU.(h l\u00e0 hidden state.)<br>\n* Sau nhi\u1ec1u b\u01b0\u1edbc h\u1ecdc v\u00e0 d\u1ef1 \u0111o\u00e1n qua c\u00e1c state, m\u1ea1ng s\u1ebd cho ra k\u1ebft qu\u1ea3 cu\u1ed1i c\u00f9ng \u0111\u01b0\u1ee3c t\u1ed5ng h\u1ee3p t\u1eeb t\u1ea5t c\u1ea3 c\u00e1c input tr\u01b0\u1edbc \u0111\u00f3.\n> **Nh\u1eadn x\u00e9t:** Do v\u1ea5n \u0111\u1ec1 ph\u1ee5 thu\u1ed9c xa, c\u1ee5 th\u1ec3 l\u00e0 c\u00e1c input $h_{t - 1}$ c\u1ee7a 1 state nh\u1ea5t \u0111\u1ecbnh s\u1ebd b\u1ecb ph\u1ee5 thu\u1ed9c v\u00e0o c\u00e1c state tr\u01b0\u1edbc \u0111\u00f3, n\u00ean vi\u1ec7c \u0111\u1ea1o h\u00e0m nhi\u1ec1u l\u1ea7n qua c\u00e1c state s\u1ebd d\u1eabn \u0111\u1ebfn vanishing gradient khi\u1ebfn cho d\u1eef li\u1ec7u m\u00e0 c\u00e1c state \u1edf xa nhau nh\u1eadn \u0111\u01b0\u1ee3c s\u1ebd b\u1ecb m\u1ea5t m\u00e1t, c\u00e1c c\u00e2u h\u1ecfi \u0111\u01b0\u1ee3c d\u1ef1 \u0111o\u00e1n s\u1ebd ch\u1ec9 l\u1ea5y \u0111\u01b0\u1ee3c c\u00e1c th\u00f4ng tin l\u01b0u tr\u1eef \u1edf nh\u1eefng state \u0111\u1ee7 g\u1ea7n v\u1edbi m\u00ecnh.<br>\n\u0110\u1ec3 gi\u1ea3m thi\u1ec3u v\u1ea5n \u0111\u1ec1 n\u00e0y th\u00ec LSTM ra \u0111\u1eddi.","9073f108":"Ta b\u1eaft \u0111\u1ea7u c\u00f4ng vi\u1ec7c hu\u1ea5n luy\u1ec7n.<br>\nV\u00ec d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n kh\u00e1 l\u1edbn v\u00e0 thu\u1eadt to\u00e1n ta d\u00f9ng \u0111\u1ec3 t\u1ed1i \u01b0u l\u00e0 GD, n\u00ean ta ph\u1ea3i chia nh\u1ecf ch\u00fang th\u00e0nh c\u00e1c mini_batch v\u1edbi size=512(v\u00ec th\u1eddi gian kh\u00e1 l\u00e2u n\u00ean ch\u1ec9 set epoch = 3).","5a39da0b":"Ta s\u1eed d\u1ee5ng t\u1eadp val \u0111\u1ec3 th\u1eed d\u1ef1 \u0111o\u00e1n v\u00e0 t\u00ednh c\u00e1c k\u1ebft qu\u1ea3 \u0111\u1ec3 t\u00ecm ra threshold h\u1ee3p l\u00fd, t\u1eeb \u0111\u00f3 \u0111\u01b0a ra \u0111\u01b0\u1ee3c ch\u1ec9 s\u1ed1 f1-score h\u1ee3p l\u00fd cho m\u00f4 h\u00ecnh","8a065108":"Ta b\u1eaft \u0111\u1ea7u kh\u1edfi t\u1ea1o m\u00f4 h\u00ecnh, v\u00ec \u0111\u00e2y l\u00e0 b\u00e0i to\u00e1n ph\u00e2n lo\u1ea1i nh\u1ecb ph\u00e2n n\u00ean h\u00e0m ta d\u00f9ng h\u00e0m loss l\u00e0 binary_crossentropy","11a6c3e7":"### Cleaning Data","ef990425":"**X\u00e2y d\u1ef1ng m\u1ed9t m\u00f4 h\u00ecnh LSTM \u0111\u01a1n gi\u1ea3n**","adfa4b37":"T\u00e1ch d\u1eef li\u1ec7u \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh.<br>\nTa s\u1eed d\u1ee5ng h\u00e0m chia d\u1eef li\u1ec7u \u0111\u00e3 \u0111\u01b0\u1ee3c cho tr\u01b0\u1edbc trong th\u01b0 vi\u1ec7n sklearn","a8bb9005":"### Naive Bayes","05905bd5":"* \u0110\u1ea7u ti\u00ean ta cho d\u1eef li\u1ec7u ti\u1ebfn v\u00e0o Embedding layer, layer n\u00e0y \u0111\u01b0\u1ee3c kh\u1edfi t\u1ea1o v\u1edbi tr\u1ecdng s\u1ed1 (weight) ng\u1eabu nhi\u00ean v\u00e0 n\u00f3 s\u1ebd t\u00ecm hi\u1ec3u c\u00e1ch embedded cho t\u1ea5t c\u1ea3 c\u00e1c t\u1eeb trong t\u1eadp d\u1eef li\u1ec7u training. *Embedding (input_dim: K\u00edch th\u01b0\u1edbc c\u1ee7a t\u1eadp d\u1eef li\u1ec7u, output_dim: K\u00edch th\u01b0\u1edbc kh\u00f4ng gian vector \u0111\u01b0\u1ee3c embedding, input_length: \u0110\u1ed9 d\u00e0i c\u1ee7a c\u00e2u h\u1ecfi)*\n* Sau \u0111\u00f3 ta cho ch\u00fang v\u00e0o m\u1ea1ng LSTM.\n* S\u1eed d\u1ee5ng Dense layer hay Fully-connected layer - m\u1ed9t l\u1edbp c\u1ed5 \u0111i\u1ec3n trong m\u1ea1ng n\u01a1 ron nh\u00e2n t\u1ea1o. M\u1ed7i neural nh\u1eadn \u0111\u1ea7u v\u00e0o t\u1eeb t\u1ea5t c\u1ea3 c\u00e1c output c\u1ee7a m\u1ea1ng LSTM r\u1ed3i \u0111\u01b0a v\u00e0o ReLU activation fuction \u0111\u1ec3 t\u00ednh to\u00e1n.\n* Ta s\u1eed d\u1ee5ng k\u1ef9 thu\u1eadt Dropout \u0111\u1ec3 ng\u1eabu nhi\u00ean lo\u1ea1i b\u1ecf p% s\u1ed1 l\u01b0\u1ee3ng node trong layer nh\u1eb1m tr\u00e1nh vi\u1ec7c model tr\u1edf n\u00ean qu\u00e1 ph\u1ee9c t\u1ea1p, b\u1ecb overfiting.\n* Sau khi dropout ta ti\u1ebfp t\u1ee5c s\u1eed d\u1ee5ng Fully-connected layer \u0111\u1ec3 k\u1ebft n\u1ed1i c\u00e1c node c\u00f2n l\u1ea1i sau \u0111\u00f3 t\u00ednh to\u00e1n, \u0111\u01b0a ch\u00fang v\u00e0o sigmoid activation function v\u00e0 \u0111\u01b0a ra k\u1ebft qu\u1ea3.","e41877b6":"> \u0110\u00e1nh gi\u00e1: Nh\u00ecn chung, m\u00f4 h\u00ecnh n\u00e0y l\u00e0 m\u1ed9t m\u00f4 h\u00ecnh kh\u00e1 l\u00e0 \u0111\u01a1n gi\u1ea3n.<br>\n* V\u00ec Naive Bayes b\u1ecb \u1ea3nh h\u01b0\u1edfng kh\u00e1 nhi\u1ec1u b\u1edfi d\u1eef li\u1ec7u m\u1ea5t c\u00e2n b\u1eb1ng n\u00ean \u1ea3nh h\u01b0\u1edfng kh\u00f4ng t\u1ed1t t\u1edbi k\u1ebft qu\u1ea3 c\u1ee7a m\u00f4 h\u00ecnh.<br>\n* M\u00f4 h\u00ecnh n\u00e0y c\u0169ng s\u1ebd \u0111\u01b0a ra k\u1ebft qu\u1ea3 kh\u00f4ng t\u1ed1t khi d\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o ch\u1ee9a nh\u1eefng t\u1eeb kh\u00f4ng c\u00f3 trong t\u1eadp train, x\u00e1c su\u1ea5t d\u1ef1 \u0111o\u00e1n s\u1ebd ti\u1ebfn g\u1ea7n t\u1edbi kh\u00f4ng, \u0111\u1ec3 gi\u1ea3i quy\u1ebft \u0111i\u1ec1u n\u00e0y ta d\u00f9ng k\u1ef9 thu\u1eadt Laplace smoothing: $$P(x_{i}|C) = \\frac{N_{ci} + \\alpha }{N_{c} + d\\alpha }$$ \u0111\u1ec3 cho x\u00e1c su\u1ea5t $P(x_{i}|C)$ kh\u00e1c kh\u00f4ng, sau nhi\u1ec1u l\u1ea7n th\u1eed th\u00ec h\u1ec7 s\u1ed1 alpha = 0.08 cho k\u1ebft qu\u1ea3 kh\u00e1 t\u1ed1t \u0111\u1ed1i v\u1edbi m\u00f4 h\u00ecnh n\u00e0y\n* T\u1ed1c \u0111\u1ed9 ch\u1ea1y c\u1ee7a m\u00f4 h\u00ecnh r\u1ea5t nhanh do vi\u1ec7c t\u00ednh to\u00e1n r\u1ea5t \u0111\u01a1n gi\u1ea3n.","d1982708":"# 4. Conclution\n* Nh\u00ecn chung c\u00e1c m\u00f4 h\u00ecnh \u0111\u01a1n gi\u1ea3n nh\u01b0 Naive Bayes, Logistic Regression \u0111\u1ec1u ho\u1ea1t \u0111\u1ed9ng kh\u00e1 \u1ed5n tr\u00ean t\u1eadp dataset.\n* V\u00ec LSTM l\u00e0 m\u1ed9t m\u00f4 h\u00ecnh c\u00f3 chi\u1ec1u s\u00e2u h\u01a1n n\u00ean cho ra c\u00e1c k\u1ebft qu\u1ea3 t\u1ed1t h\u01a1n so v\u1edbi c\u00e1c m\u00f4 h\u00ecnh truy\u1ec1n th\u1ed1ng, do th\u1eddi gian nhi\u00ean c\u1ee9u ch\u01b0a l\u00e2u v\u00e0 ch\u01b0a k\u1ef9 n\u00ean ch\u01b0a \u00e1p d\u1ee5ng \u0111\u01b0\u1ee3c c\u00e1c layer ph\u1ed5 bi\u1ebfn nh\u01b0 Time distributed Layer hay Attention Layer \u0111\u1ec3 m\u00f4 h\u00ecnh cho ra c\u00e1c k\u1ebft qu\u1ea3 t\u1ed1t h\u01a1n.\n* Tham kh\u1ea3o m\u1ed9t s\u1ed1 b\u00e0i l\u00e0m tr\u00ean kaggle, \u0111\u1ec3 x\u1eed l\u00fd d\u1eef li\u1ec7u m\u1ea5t c\u00e2n b\u1eb1ng, c\u1ee5 th\u1ec3 l\u00e0 SMOTE th\u00ec k\u1ebft qu\u1ea3 train sau khi SMOTE c\u0169ng kh\u00f4ng \u0111\u01b0\u1ee3c c\u1ea3i thi\u1ec7n so v\u1edbi khi kh\u00f4ng d\u00f9ng:<br> [Smoted, but it did not work very well.](https:\/\/www.kaggle.com\/stefanobromuri\/smoted-but-it-did-not-work-very-well) <br> [Dealing with Class Imbalance with SMOTE](https:\/\/www.kaggle.com\/theoviel\/dealing-with-class-imbalance-with-smote)","1515b475":"\u0110\u1ec3 \u0111\u00e1nh gi\u00e1 m\u00f4 h\u00ecnh, \u1edf \u0111\u00e2y ta d\u00f9ng f1_score do d\u1eef li\u1ec7u \u0111ang b\u1ecb m\u1ea5t c\u00e2n b\u1eb1ng (<a href=\"#Cleaning-Data\">\u0111\u01b0\u1ee3c gi\u1ea3i th\u00edch t\u1ea1i \u0111\u00e2y<\/a>) v\u00e0 \u0111\u00e2y l\u00e0 m\u1ed9t ch\u1ec9 s\u1ed1 thay th\u1ebf l\u00fd t\u01b0\u1edfng cho accuracy khi m\u00f4 h\u00ecnh c\u00f3 t\u1ef7 l\u1ec7 m\u1ea5t c\u00e2n b\u1eb1ng m\u1eabu cao.<br>\n- f1-score l\u00e0 rung b\u00ecnh \u0111i\u1ec1u h\u00f2a gi\u1eefa Precision v\u00e0 Recall $$\\frac{1}{\\frac{1}{precision} + \\frac{1}{recall}}$$\nV\u1edbi:\n> Precision: M\u1ee9c \u0111\u1ed9 d\u1ef1 b\u00e1o ch\u00ednh x\u00e1c trong nh\u1eefng tr\u01b0\u1eddng h\u1ee3p \u0111\u01b0\u1ee3c d\u1ef1 b\u00e1o l\u00e0 Positive.<br>$$precision=\\frac{TP}{TP + FP}$$\n Recall: M\u1ee9c \u0111\u1ed9 d\u1ef1 b\u00e1o chu\u1ea9n x\u00e1c nh\u1eefng tr\u01b0\u1eddng h\u1ee3p l\u00e0 Positive trong nh\u1eefng tr\u01b0\u1eddng h\u1ee3p th\u1ef1c t\u1ebf l\u00e0 Positive. $$recall=\\frac{TP}{TP + FN}$$\nv\u00e0 c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n nh\u01b0 h\u00ecnh sau: <br>\n![](https:\/\/www.kdnuggets.com\/images\/precision-recall-relevant-selected.jpg)","f34625c2":"### LSTM \n* Ta \u0111\u00e3 s\u1eed d\u1ee5ng m\u1ed9t model c\u1ed5 \u0111i\u1ec3n \u0111\u1ec3 gi\u1ea3i quy\u1ebft b\u00e0i to\u00e1n n\u00e0y, ti\u1ebfp theo ta s\u1ebd th\u1eed v\u1edbi m\u1ed9t m\u00f4 h\u00ecnh s\u00e2u h\u01a1n, c\u1ee5 th\u1ec3 l\u00e0 m\u00f4 h\u00ecnh LSTM, m\u1ed9t m\u00f4 h\u00ecnh \u0111\u01b0\u1ee3c c\u1ea3i ti\u1ebfn t\u1eeb m\u1ea1ng RNN.\n* V\u00ec \u0111\u00e2y l\u00e0 m\u1ed9t thu\u1eadt to\u00e1n DL m\u00e0 nhi\u1ec1u ch\u1ed7 em t\u00ecm hi\u1ec3u ch\u01b0a k\u0129, n\u00ean em xin \u0111\u01b0\u1ee3c ph\u00e9p KH\u00d4NG tr\u00ecnh b\u00e0y c\u00e1c c\u00f4ng th\u1ee9c t\u00ednh to\u00e1n hay t\u1ed1i \u01b0u m\u00e0 ch\u1ec9 tr\u00ecnh b\u00e0y c\u00e1c \u00fd t\u01b0\u1edfng m\u00e0 m\u00ecnh \u0111\u00e3 hi\u1ec3u c\u0169ng nh\u01b0 c\u00e1ch s\u1eed d\u1ee5ng, v\u00e0 \u1ee9ng d\u1ee5ng \u0111\u1ec3 x\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh LSTM v\u1edbi c\u00e1c layer \u0111\u01a1n gi\u1ea3n.","d5aeea7d":"# 1. Problem description\n>Trong b\u00e0i to\u00e1n n\u00e0y, ta c\u1ea7n ph\u00e2n lo\u1ea1i c\u00e1c c\u00e2u h\u1ecfi xem li\u1ec7u ch\u00fang c\u00f3 ph\u1ea3i l\u00e0 toxic question hay kh\u00f4ng. \u0110\u00e2y l\u00e0 m\u1ed9t b\u00e0i to\u00e1n kh\u00e1 \u0111\u01a1n gi\u1ea3n v\u1edbi 2 l\u1edbp, l\u1edbp is insincere(1) v\u00e0 sincere(0) \n* \u0110\u1ea7u v\u00e0o c\u1ee7a b\u00e0i to\u00e1n l\u00e0 d\u1eef li\u1ec7u d\u1ea1ng text\n* \u0110\u1ea7u ra c\u1ee7a b\u00e0i to\u00e1n l\u00e0 [\"yes\":1, \"no\":0] \n        => \u0110\u00e2y l\u00e0 m\u1ed9t b\u00e0i to\u00e1n Binari Classifier","9ed5d9f2":"Thu\u1eadt to\u00e1n Naive Bayes Classifiers(NBC) \u0111\u01b0\u1ee3c d\u1ef1a tr\u00ean \u0111\u1ecbnh l\u00fd Bayes, n\u00f3 cho ph\u00e9p ch\u00fang ta t\u00ednh \u0111\u01b0\u1ee3c x\u00e1c su\u1ea5t c\u1ee7a bi\u1ebfn c\u1ed1 A khi bi\u1ebfn c\u1ed1 B s\u1ea3y ra tr\u01b0\u1edbc:\n$$P(B|A) = \\frac{P(A|B)P(B)}{P(A)}$$\nV\u1edbi D l\u00e0 t\u1eadp c\u00e1c c\u00e2u h\u1ecfi hu\u1ea5n luy\u1ec7n, trong \u0111\u00f3 m\u1ed7i c\u00e2u h\u1ecfi A ch\u1ee9a n gi\u00e1 tr\u1ecb thu\u1ed9c t\u00ednh B1,B2,...,Bn \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n b\u1eb1ng m\u1ed9t vector n th\u00e0nh ph\u1ea7n {x1,x2,...,xn} $$$$\nV\u1edbi 2 l\u1edbp [0,1] v\u00e0 m\u1ed9t c\u00e2u h\u1ecfi A cho tr\u01b0\u1edbc, b\u1ed9 ph\u00e2n l\u1edbp s\u1ebd ph\u00e2n cho A v\u00e0o l\u1edbp [0] n\u1ebfu: $P(A|C_{0}) > P(A|C_{1})$ v\u00e0 ng\u01b0\u1ee3c l\u1ea1i. V\u00ec P(A) l\u00e0 kh\u00f4ng \u0111\u1ed5i, x\u00e1c su\u1ea5t $P(C_{i}) (i\\in [0,1])$ ta coi b\u1eb1ng nhau, v\u00e0 gi\u1ea3 thuy\u1ebft c\u00e1c x\u00e1c su\u1ea5t l\u00e0 \u0111\u1ed9c l\u1eadp v\u1edbi nhau n\u00ean ta ch\u1ec9 c\u1ea7m t\u00ecm gi\u00e1 tr\u1ecb: $$P(A|C_{i})_{max} = P(x_{1}|C_{i}). ... .P(x_{n}|C_{n})$$\nTa c\u00f3 th\u1ec3 hi\u1ec3u \u0111\u01a1n gi\u1ea3n h\u01a1n l\u00e0 \u1edf \u0111\u00e2y ta t\u00ednh x\u00e1c su\u1ea5t c\u00e2u h\u1ecfi A thu\u1ed9c v\u00e0o class C d\u1ef1a tr\u00ean vi\u1ec7c t\u00ednh, t\u00ecm x\u00e1c su\u1ea5t l\u1edbn nh\u1ea5t m\u00e0 n\u00f3 thu\u1ed9c v\u00e0o trong m\u1ed7i class\n","fe17802a":"Nh\u01b0 v\u1eady sau khi t\u00ednh \u0111\u01b0\u1ee3c ph\u1ea7n tuy\u1ebfn t\u00ednh l\u00e0 $w^{T}x$, ph\u1ea7n tuy\u1ebfn t\u00ednh n\u00e0y s\u1ebd \u0111\u01b0\u1ee3c \u0111\u01b0a v\u00e0o h\u00e0m sigmoid($\\sigma()$). H\u00e0m $\\sigma()$ s\u1ebd tr\u1ea3 v\u1ec1 x\u00e1c su\u1ea5t m\u00e0 c\u00e2u h\u1ecfi l\u00e0 h\u1eefu \u00edch ($y = 0$) hay \u0111\u1ed9c h\u1ea1i ($y = 1$) v\u1edbi: $$P(y_{1}|x_{i};w) = \\sigma(w^{T}x); P(y_{0}|x_{i};w) = 1 - \\sigma(w^{T}x)$$\nB\u00e0i to\u00e1n quay tr\u1edf v\u1ec1 v\u1edbi vi\u1ec7c **t\u1ed1i \u01b0u h\u00e0m m\u1ea5t m\u00e1t L** v\u1edbi m\u1ee5c ti\u00eau l\u00e0m cho $P(y|x_{i}) = \\sigma(w^{T}x) = \\widehat{y}$ c\u00e0ng ng\u00e0y c\u00e0ng l\u1edbn (ti\u1ebfn g\u1ea7n t\u1edbi 1): $$L = -log(P(y|x_{i}))$$ $$<=> \\frac{\\partial L}{\\partial w} = (\\widehat{y} - y)x = (\\sigma(w^{T}x) - y)x$$ $$=> w = w + \\lambda(\\widehat{y} - y)x$$\nV\u1edbi $\\lambda$ l\u00e0 h\u1ec7 h\u1ecdc learning_rate (l\u00e0 m\u1ed9t con s\u1ed1 nh\u1ecf th\u01b0\u1eddng n\u1eb1m trong kho\u1ea3ng t\u1eeb 0.01 \u0111\u1ebfn 0.0001). <br>\nTh\u01b0 vi\u1ec7n sklearn c\u0169ng \u0111\u00e3 t\u00edch h\u1ee3p s\u1eb5n m\u00f4 h\u00ecnh n\u00e0y n\u00ean ta s\u1ebd l\u1ea5y ra v\u00e0 s\u1eed d\u1ee5ng.","94e76545":"> Nh\u01b0 v\u1eady, ta \u0111\u00e3 x\u00e2y d\u1ef1ng \u0111\u01b0\u1ee3c m\u1ed9t m\u00f4 h\u00ecnh LSTM c\u01a1 b\u1ea3n v\u1edbi c\u00e1c layer nh\u01b0 tr\u00ean.","a6c72cdf":"D\u1ec5 th\u1ea5y, \u0111\u00f4i khi c\u00f3 nh\u1eefng t\u1eeb r\u1ea5t \u00edt khi xu\u1ea5t hi\u1ec7n, nh\u01b0ng l\u1ea1i l\u00e0 th\u00f4ng tin quan tr\u1ecdng d\u00f9ng \u0111\u1ec3 ph\u00e2n lo\u1ea1i, v\u00e0 c\u0169ng c\u00f3 nh\u1eefng t\u1eeb xu\u1ea5t hi\u1ec7n r\u1ea5t nhi\u1ec1u nh\u01b0ng l\u1ea1i kh\u00f4ng qu\u00e1 quan tr\u1ecdng khi training, n\u00ean ph\u01b0\u01a1ng ph\u00e1p TF-IDF c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng. $$$$\nPh\u01b0\u01a1ng ph\u00e1p TF-IDF s\u1ebd \u0111\u00e1nh gi\u00e1 t\u1ea7m quan tr\u1ecdng c\u1ee7a m\u1ed9t t\u1eeb trong m\u1ed9t c\u00e2u h\u1ecfi d\u1ef1a tr\u00ean to\u00e0n b\u1ed9 t\u1eadp d\u1eef li\u1ec7u v\u1edbi \u0111\u1ed9 quan tr\u1ecdng \u0111\u01b0\u1ee3c t\u00ednh b\u1eb1ng c\u00f4ng th\u1ee9c: \n                      $$tf(w, ques) * log(\\frac{n}{n_{w} + 1})$$\n* $tf(w, ques)$: ch\u00ednh l\u00e0 t\u1ea7n su\u1ea5t xu\u1ea5t hi\u1ec7n c\u1ee7a m\u1ed9t t\u1eeb trong m\u1ed9t c\u00e2u h\u1ecfi: *(s\u1ed1 l\u1ea7n xu\u1ea5t hi\u1ec7n trong c\u00e2u h\u1ecfi) \/ (t\u1eeb xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t trong c\u00e2u)*\n* $n$: l\u00e0 t\u1ed5ng s\u1ed1 c\u00e1c c\u00e2u h\u1ecfi trong t\u1eadp\n* $n_{w}$: l\u00e0 s\u1ed1 l\u1ea7n t\u1eeb n\u00e0y xu\u1ea5t hi\u1ec7n trong to\u00e0n b\u1ed9 t\u1eadp","bab12402":"**LSTM**<br>\nLSTM c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c m\u00f4 h\u00ecnh h\u00f3a nh\u01b0 sau:<br>\n![](https:\/\/i2.wp.com\/nttuan8.com\/wp-content\/uploads\/2019\/06\/lstm.png?resize=768%2C466&ssl=1)<br>\n* Nh\u01b0 tr\u1ef1c quan ta th\u1ea5y LSTM t\u01b0\u01a1ng \u0111\u1ed3ng v\u1edbi RNN nh\u01b0ng c\u00f3 th\u00eam m\u1ed9t \u0111i\u1ec3m \u0111\u1eb7c bi\u1ec7t \u1edf \u0111\u1ea7u v\u00e0o l\u00e0 $c_{t - 1}$ v\u00e0 \u1edf \u0111\u1ea7u ra l\u00e0 $c_{t}$.(ta g\u1ecdi c l\u00e0 cell state)<br>\n* Nh\u01b0 trong m\u00f4 h\u00ecnh ta c\u00f3 th\u1ec3 th\u1ea5y $c_{t}$ kh\u00f4ng \u0111i qua qu\u00e1 nhi\u1ec1u b\u01b0\u1edbc t\u00ednh to\u00e1n, n\u00f3 ch\u1ec9 c\u1ea7n \u0111i qua v\u00e0i b\u01b0\u1edbc t\u00ednh to\u00e1n nh\u01b0 element-wise multiplication, c\u1ed9ng ma tr\u1eadn \u0111\u1ec3 t\u1ed5ng h\u1ee3p c\u00e1c th\u00f4ng tin quan trong t\u1eeb $c_{t - 1}$ (cell state tr\u01b0\u1edbc \u0111\u00f3) v\u00e0 $h_{t - 1}$ (hidden state tr\u01b0\u1edbc \u0111\u00f3). N\u00ean n\u00f3 s\u1ebd gi\u1ea3m \u0111\u01b0\u1ee3c vanishing gradient v\u00e0 gi\u00fap l\u01b0u tr\u1eef \u0111\u01b0\u1ee3c th\u00f4ng tin \u0111i xa (long term memory).\n* C\u00e1ch t\u00ednh $\\widetilde{C_{t}}$ gi\u1ed1ng nh\u01b0 c\u00e1ch t\u00ednh $h_{t}$ trong RNN.(\u0111i\u1ec3m t\u01b0\u01a1ng t\u1ef1)\n* Em s\u1ebd kh\u00f4ng \u0111i qu\u00e1 s\u00e2u v\u00e0o ph\u1ea7n gi\u1ea3i th\u00edch c\u00e1c th\u00e0nh ph\u1ea7n b\u00ean trong LSTM do v\u1eabn ch\u01b0a th\u1ec3 th\u1eadt s\u1ef1 gi\u1ea3i th\u00edch r\u00f5 ch\u00fang, v\u00e0 s\u1ebd hi\u1ec3u \u0111\u01a1n gi\u1ea3n \u0111\u00e2y l\u00e0 m\u1ed9t m\u1ea1ng RNN \u0111\u01b0\u1ee3c c\u1ea3i ti\u1ebfn b\u1eb1ng c\u00e1ch s\u1eed d\u1ee5ng cell state \u0111\u1ec3 tr\u00edch su\u1ea5t m\u1ed9t s\u1ed1 l\u01b0\u1ee3ng th\u00f4ng tin quan tr\u1ecdng t\u1eeb c\u00e1c state tr\u01b0\u1edbc, v\u00e0 v\u1edbi \u0111\u1eb7c t\u00ednh l\u01b0u tr\u1eef \u0111\u01b0\u1ee3c c\u00e1c th\u00f4ng tin state \u1edf g\u1ea7n c\u1ee7a RNN => ta g\u1ecdi m\u00f4 h\u00ecnh n\u00e0y l\u00e0 LSTM.","5c7b50e1":"### Logistic Regression\n> Ti\u1ebfp theo ta \u00e1p d\u1ee5ng m\u1ed9t m\u00f4 h\u00ecnh \u0111\u01a1n gi\u1ea3n nh\u01b0ng l\u1ea1i l\u00e0 ti\u1ec1n \u0111\u1ec1 cho m\u1ea1ng neural."}}