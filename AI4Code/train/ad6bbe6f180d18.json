{"cell_type":{"56a80cb4":"code","63453a15":"code","352ec61e":"code","a24595b7":"code","1d784e3f":"code","83027da8":"code","56daa91e":"code","4b76dcec":"code","9a3a8128":"code","11ccb92c":"code","72c84c7a":"code","f1c25edc":"code","eb7f5d2c":"code","0f483209":"code","e67d004f":"code","6e05ce68":"code","8e060247":"code","c68e8838":"code","b5926af6":"code","6d83315b":"code","63321f08":"code","e50b5746":"code","b3baddc5":"code","f8308675":"code","2307ccb6":"code","92dd38a2":"code","b1e31002":"code","cb86879f":"code","73e96635":"code","bc6c8298":"code","871ff4c3":"code","fd7c71b7":"code","729cc255":"code","56a67a84":"code","eed255b3":"code","474e1fde":"code","34572ccf":"code","6b349e38":"code","1583e3c2":"code","70c4a14f":"code","692d2d74":"code","d1e65234":"code","42c9737b":"code","af56094a":"code","1b4a6d96":"code","7d0fe501":"code","e19eb7f4":"code","44effb8c":"markdown","f3dc621a":"markdown","bc5cceb7":"markdown","ee8fa258":"markdown","ccc0e4d0":"markdown","f70befb6":"markdown","8c451d19":"markdown","37c93465":"markdown"},"source":{"56a80cb4":"# Libraries\nimport os\nfrom glob import glob\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\n","63453a15":"# All data\nprint(os.listdir('..\/input'))","352ec61e":"# Data path\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\nsub = pd.read_csv('..\/input\/sample_submission.csv')\ntrain_dir = '..\/input\/train_images'\ntest_dir = '..\/input\/test_images'","a24595b7":"print('Total images for train {0}'.format(len(os.listdir(train_dir))))\nprint('Total images for test {0}'.format(len(os.listdir(test_dir))))","1d784e3f":"train_df.iloc[100:110]","83027da8":"test_df.iloc[100:110]","56daa91e":"test_df.iloc[100:110]","4b76dcec":"# code from https:\/\/www.kaggle.com\/gpreda\/iwildcam-2019-eda\n\nclasses_wild = {0: 'empty', 1: 'deer', 2: 'moose', 3: 'squirrel', 4: 'rodent', 5: 'small_mammal', \\\n                6: 'elk', 7: 'pronghorn_antelope', 8: 'rabbit', 9: 'bighorn_sheep', 10: 'fox', 11: 'coyote', \\\n                12: 'black_bear', 13: 'raccoon', 14: 'skunk', 15: 'wolf', 16: 'bobcat', 17: 'cat',\\\n                18: 'dog', 19: 'opossum', 20: 'bison', 21: 'mountain_goat', 22: 'mountain_lion'}\n\ntrain_df['classes_wild'] = train_df['category_id'].apply(lambda cw: classes_wild[cw])","9a3a8128":"# Category distribution\ntrain_df['classes_wild'].value_counts()","11ccb92c":"plt.figure(figsize=(10,5))\ntrain_df['classes_wild'].value_counts().plot(kind='bar',  title=\"Category distribution\",);\nplt.show()","72c84c7a":"def image_plotting(df, category, data_dir=train_dir):\n    data_dir = data_dir\n    df = train_df[train_df['classes_wild']== category]\n    df = df[['classes_wild', 'file_name']]\n    plt.rcParams['figure.figsize'] = (15, 15)\n    plt.subplots_adjust(wspace=0, hspace=0)\n    i_ = 0\n    \n    \n    for l in range(25):\n        cat, img_name = df.sample(1).values[0]\n        path = os.path.join(train_dir, img_name)\n\n        img = cv2.imread(path)\n        img = cv2.resize(img, (256, 256)) \n\n        plt.subplot(5, 5, i_+1) #.set_title(l)\n        plt.imshow(img); plt.axis('off')\n        i_ += 1\n    print(cat)","f1c25edc":"image_plotting(data_dir=train_dir,category='bobcat',df=train_df)","eb7f5d2c":"image_plotting(train_df, 'cat')","0f483209":"image_plotting(train_df, 'coyote')","e67d004f":"image_plotting(train_df, 'deer')","6e05ce68":"image_plotting(train_df, 'dog')","8e060247":"image_plotting(train_df, 'empty')","c68e8838":"image_plotting(train_df, 'fox')","b5926af6":"image_plotting(train_df, 'mountain_lion' )","6d83315b":"image_plotting(train_df, 'opossum' )","63321f08":"image_plotting(train_df, 'rabbit')","e50b5746":"image_plotting(train_df, 'raccoon' )","b3baddc5":"image_plotting(train_df, 'rodent' )","f8308675":"image_plotting(train_df, 'skunk' )","2307ccb6":"image_plotting(train_df, 'squirrel' )","92dd38a2":"# Libraries\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom torchvision.transforms import transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models\n\nfrom sklearn.model_selection import train_test_split","b1e31002":"train_df = train_df[['file_name','category_id']]\ntrain_df.head()\n","cb86879f":"# code lightly modified from https:\/\/www.kaggle.com\/ateplyuk\/iwildcam2019-pytorch-starter\ncategory = train_df['category_id'].unique()\n\nencoder = dict([(v, k) for v, k in zip(category, range(len(category)))])\ndecoder = dict([(v, k) for k, v in encoder.items()])\n\n\nprint( pd.DataFrame({\n    'Before encoding': list(encoder.keys()),\n    'After encoding': list(encoder.values())}).to_string(index=False))\n\n\ndef encoding(labels):\n        return encoder[int(labels)]","73e96635":"train_df['category_id'] = train_df['category_id'].apply(encoding)\ntrain_df['category_id'].value_counts()","bc6c8298":"# Custom data generator\nclass WildDataset(Dataset):\n    def __init__(self, df, img_dir, transforms=None):\n        self.df = df\n        self.img_dir = img_dir\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        img_name = os.path.join(self.img_dir,\n                               self.df.iloc[idx, 0])\n        image = cv2.imread(img_name)\n        label = self.df.iloc[idx, 1]\n        if self.transforms is not None:\n            image = self.transforms(image)\n        return image, label","871ff4c3":"train, val = train_test_split(train_df, stratify=train_df.category_id, test_size=0.1)\nlen(train), len(val)","fd7c71b7":"# Augmentations for data\n\naug = transforms.Compose([transforms.ToPILImage(),                          \n                          transforms.Resize((32, 32)),\n                          transforms.ToTensor(),\n                          transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n                             ])\n\n\n","729cc255":"# iWildCam dataset\ndataset_train = WildDataset(df=train,\n                            img_dir=train_dir,\n                            transforms=aug)\n\ndataset_valid = WildDataset(df=val,\n                           img_dir=train_dir,\n                           transforms=aug)\n\n# Data loader\ntrain_loader = DataLoader(dataset=dataset_train, batch_size=24, shuffle=True)\nval_loader = DataLoader(dataset_valid, batch_size=24, shuffle=False, num_workers=0)","56a67a84":"# Aug for data img\ndef show_aug(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.figure(figsize=(20,15))\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n    \n# Get a batch of training data\ninputs, _ = next(iter(train_loader))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs, 4)  \n\nshow_aug(out)","eed255b3":"_","474e1fde":"_.shape","34572ccf":"## Parameters for model\n\n# Hyper parameters\nnum_epochs = 2\nnum_classes = 14\nlearning_rate = 0.02\n\n# Device configuration\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","6b349e38":"# 3x3 convolution\ndef conv3x3(in_channels, out_channels, stride=1):\n    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n                     stride=stride, padding=1, bias=False)\n\n  \n# Residual block\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = conv3x3(in_channels, out_channels, stride)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(out_channels, out_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = downsample\n        \n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.downsample:\n            residual = self.downsample(x)\n        out += residual\n        out = self.relu(out)\n        return out\n      \n      \n      \n# ResNet\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=14):\n        super(ResNet, self).__init__()\n        self.in_channels = 16\n        self.conv = conv3x3(3, 16)\n        self.bn = nn.BatchNorm2d(16)\n        self.relu = nn.LeakyReLU(inplace=True)\n        self.layer1 = self.make_layer(block, 16, layers[0])\n        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n        self.layer4 = self.make_layer(block, 128, layers[3], 2)\n        self.avg_pool = nn.AdaptiveAvgPool2d(4)\n        self.fc = nn.Linear(128, num_classes)\n        \n    def make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if (stride != 1) or (self.in_channels != out_channels):\n            downsample = nn.Sequential(\n                conv3x3(self.in_channels, out_channels, stride=stride),\n                nn.BatchNorm2d(out_channels))\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels\n        for i in range(1, blocks):\n            layers.append(block(out_channels, out_channels))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        out = self.conv(x)\n        out = self.bn(out)\n        out = self.relu(out)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.avg_pool(out)\n        out = out.view(out.size(0), -1)\n        out = self.fc(out)\n        return out\n      \n      \n      \ndef create_resnet_model(output_dim: int = 1) -> nn.Module:\n    model = ResNet(ResidualBlock, [2, 2, 2, 2])\n    in_features = model.fc.in_features\n    model.avg_pool = nn.AdaptiveAvgPool2d(1)\n    model.fc = nn.Linear(in_features, output_dim)\n    model = model.to(device)\n    return model\n\nmodel = create_resnet_model(output_dim=num_classes)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adamax(model.parameters(), lr=learning_rate)","1583e3c2":"\n# Train the model\ntotal_step = len(train_loader)\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if (i+1) % 100 == 0:\n            print (\"Epoch [{}\/{}], Step [{}\/{}] Loss: {:.4f}\"\n                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n","70c4a14f":"# Test the model\nmodel.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in val_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    print('Accuracy of the model on the 19630 test images: {} %'.format(100 * correct \/ total))\n","692d2d74":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['Id'] = sub['Id'] + '.jpg'\nsub.head()","d1e65234":"# Dataset for test img\ndataset_valid = WildDataset(df=sub,\n                           img_dir=test_dir,\n                           transforms=aug)\n\n# Data loader\ntest_loader = DataLoader(dataset_valid, batch_size=24, shuffle=False)","42c9737b":"# Test the model\nmodel.eval()\npreds = []\n#with torch.no_grad():\nfor images, labels in test_loader:\n    images = images.to(device)\n    labels = labels.to(device)\n    outputs = model(images)\n    _, predicted = torch.max(outputs.data, 1)\n    predicted\n    for i in predicted.detach().cpu().numpy():\n        preds.append(i)\n","af56094a":"sub['Predicted'] =  preds\nsub['Id'] = sub['Id'].str[:-4]\nsub.head()\n","1b4a6d96":"\ndef decoding(labels):\n        return decoder[int(labels)]","7d0fe501":"sub['Predicted'] = sub['Predicted'].apply(decoding)\nsub.head()\n\nsub.to_csv('submission.csv', index=False)","e19eb7f4":"sub['Predicted'].value_counts()","44effb8c":"From what i saw, i realized that i almost can't see the rodents \nsuch as : rodent, squirrel, and raccoon.\n\nBut neural net is not my eyes - handle","f3dc621a":"Now drawing images samples for each class","bc5cceb7":"**EDA** <br\/> (Exploratory Data Analysis)","ee8fa258":"So, we must make prediction for each picture **\"What category does the animal in the picture belong to?\"** (column name - \"category_id\")","ccc0e4d0":"**Creating the  ResNet model from scratch**","f70befb6":"**Model**","8c451d19":"Data and generator preparation","37c93465":"**Prediction and submission**"}}