{"cell_type":{"34050a53":"code","c5290cb6":"code","ce083a51":"code","b418a826":"code","69b4d1cc":"code","67b16265":"code","91c5f06c":"code","c6bef487":"code","dba43c00":"code","531a6f89":"code","3ca19337":"code","e485c09c":"code","aff2edad":"code","1a1b5b25":"code","c9fd028c":"code","a79d7d77":"code","12f8e98f":"code","52ceb140":"code","932cfe78":"code","524ac8f9":"code","07276cc2":"code","74f63827":"code","6ccc6809":"code","dfe85433":"code","256f593a":"code","ac504312":"code","8073b952":"code","140afe9e":"code","8b0178b4":"code","8f7d3973":"code","da39d4ae":"code","bc48b4fd":"code","58c52ef6":"code","83f47e0a":"code","b47dc299":"code","e27988dc":"code","11511250":"code","8e235ec7":"code","915e2a82":"code","94c167a9":"code","ef163c5d":"code","53d7dc34":"code","bfd8e0b4":"code","d2f2e261":"code","8c8b8013":"code","2c93a77c":"code","ccfeaa1d":"code","5970e8fe":"code","f6c3e54d":"code","31c65909":"code","403c6142":"code","fa28aa95":"code","fca6fbc8":"code","15fd3953":"code","b7aadac7":"code","c266713d":"code","498b0d38":"code","32792298":"code","25deed34":"code","7c36fa3f":"markdown","25029dbf":"markdown","32ec1e43":"markdown","4dc412fe":"markdown","13038137":"markdown","e6cfeed8":"markdown","ebd1c36b":"markdown","17e51669":"markdown","44575706":"markdown","9020ce6d":"markdown","3db0be34":"markdown","eff479f5":"markdown","b917ea81":"markdown","7a13988d":"markdown","e8fdc6a6":"markdown","b382df9e":"markdown","0f5c42c0":"markdown","7bcfb056":"markdown","763a8c16":"markdown","762be32a":"markdown","4fee1ff2":"markdown","83e5edd8":"markdown","ba1962ef":"markdown","fb184ef2":"markdown","790add8b":"markdown","70a4d391":"markdown"},"source":{"34050a53":"import numpy as np\nimport pandas as pd\nimport pickle, gzip, urllib.request, json\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\n\n\nimport os\nimport re\nimport copy\nimport time\nimport io\nimport struct\nfrom time import gmtime, strftime\nimport seaborn as sns\n\nfrom numpy import loadtxt\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_tree\nfrom xgboost import plot_importance\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel","c5290cb6":"%%time \n\ndata_dir = \"\/kaggle\/input\/house-prices-advanced-regression-techniques\/\"\n\n# load the train dataset\ntrain = 'train.csv'\ntrain_df = pd.read_csv(data_dir + train, sep=\",\")\n\n# load the test dataset\ntest = 'test.csv'\ntest_df = pd.read_csv(data_dir + test, sep=\",\")\n","ce083a51":"# lets peek at the train dataset\ntrain_df.head()","b418a826":"# get the shape of the train dataset\ntrain_df.shape","69b4d1cc":"# lets peek at the test dataset\ntest_df.head()","67b16265":"# test dimension\ntest_df.shape","91c5f06c":"# lets look at the data types of train df columns\ntrain_df.info()","c6bef487":"# define helper functions to get categorical and numerical columns\ndef get_object_cols(df):\n    return list(df.select_dtypes(include='object').columns)\n\ndef get_numerical_cols(df):\n    return list(df.select_dtypes(exclude='object').columns)","dba43c00":"corr = train_df[get_numerical_cols(train_df)].corr()\n\nprint (corr['SalePrice'].sort_values(ascending=False)[:10], '\\n') # print top ten features with high correlation\nprint (corr['SalePrice'].sort_values(ascending=False)[-10:])","531a6f89":"# plot correlation values\ncorr_df = pd.DataFrame(corr['SalePrice'].sort_values(ascending=False))\n\ncorr_df = corr_df.reset_index()\n\ncorr_df.columns = ['cols', 'values']\n\nplt.rcdefaults()\nplt.figure(figsize=(10,5))\nax = sns.barplot(x=\"cols\", y=\"values\", data=corr_df)\nax.set_ylim(-1.1, 1.1)\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)\nax.set_title('Correlations between the SalePrice and other features')\nplt.show()","3ca19337":"pd.unique(train_df['OverallQual'])","e485c09c":"# lets plot the unique overall quality values against median price\ngroupedby_quality_df = train_df[['OverallQual', 'SalePrice']].groupby(by='OverallQual').median().reset_index()\n\ngroupedby_quality_df.columns = ['Overall Quality', 'Median Sale Price']\nplt.rcdefaults()\nplt.figure(figsize=(10,5))\nax = sns.barplot(x=\"Overall Quality\", y=\"Median Sale Price\", data=groupedby_quality_df)\nax.set_ylim(0, max(groupedby_quality_df['Median Sale Price'])+10000)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\nax.set_title('Overall Quality vs. Median Sale Prices')\nplt.show()","aff2edad":"plt.scatter(x=train_df['GrLivArea'], y=train_df['SalePrice'])\nplt.ylabel('Sale Price')\nplt.xlabel('GrLivArea: Above grade (ground) living area square feet')\nplt.show()","1a1b5b25":"groupedby_garagecars_df = train_df[['GarageCars', 'SalePrice']].groupby(by='GarageCars').median().reset_index()\n\ngroupedby_quality_df.columns = ['Garage Size', 'Median Sale Price']\nplt.rcdefaults()\nplt.figure(figsize=(10,5))\nax = sns.barplot(x=\"Garage Size\", y=\"Median Sale Price\", data=groupedby_quality_df)\nax.set_ylim(0, max(groupedby_quality_df['Median Sale Price'])+10000)\nax.set_xticklabels(ax.get_xticklabels(), rotation=45)\nax.set_title('Size of garage in car capacity vs. Median Sale Prices')\nplt.show()","c9fd028c":"plt.scatter(x=train_df['GarageArea'], y=train_df['SalePrice'])\nplt.ylabel('Sale Price')\nplt.xlabel('GarageArea: Size of garage in square feet')\nplt.show()","a79d7d77":"def visualize_missing_values(df):\n    total_nans_df = pd.DataFrame(df.isnull().sum(), columns=['values'])\n    total_nans_df = total_nans_df.reset_index()\n    total_nans_df.columns = ['cols', 'values']\n    # calculate % missing values\n    total_nans_df['% missing values'] = 100*total_nans_df['values']\/df.shape[0]\n    total_nans_df = total_nans_df[total_nans_df['% missing values'] > 0 ]\n    total_nans_df = total_nans_df.sort_values(by=['% missing values'])\n\n    plt.rcdefaults()\n    plt.figure(figsize=(10,5))\n    ax = sns.barplot(x=\"cols\", y=\"% missing values\", data=total_nans_df)\n    ax.set_ylim(0, 100)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n    plt.show()","12f8e98f":"# visualise missing values in train data set\nvisualize_missing_values(train_df)","52ceb140":"# visualize columns with missing values in the test dataset\nvisualize_missing_values(test_df)","932cfe78":"# function to drop columns with missing that values based on a predefine cutoff criteria\ndef drop_columns_with_missing_values(df, cutoff):\n    \"\"\"Drop columns with missing values greater than the specified cut-off %\n    \n    Parameters:\n    -----------\n    df     : pandas dataframe\n    cutoff : % missing values\n    \n    Returns:\n    ---------\n    Returns clean dataframe\n    \"\"\"\n    # create a dataframe for missing values by column\n    total_nans_df = pd.DataFrame(df.isnull().sum(), columns=['values'])\n    total_nans_df = total_nans_df.reset_index()\n    total_nans_df.columns = ['cols', 'values']\n    \n    # calculate % missing values\n    total_nans_df['% missing values'] = 100*total_nans_df['values']\/df.shape[0]\n    \n    total_nans_df = total_nans_df[total_nans_df['% missing values'] >= cutoff ]\n    \n    # get columns to drop\n    cols = list(total_nans_df['cols'])\n    print('Features with missing values greater than specified cutoff : ', cols)\n    print('Shape before dropping: ', df.shape)\n    new_df = df.drop(labels=cols, axis=1)\n    print('Shape after dropping: ',new_df.shape)\n    \n    return new_df","524ac8f9":"# drop columns with over 80% missing values from the train dataset\nnew_train_df = drop_columns_with_missing_values(train_df, 80)","07276cc2":"# drop columns with over 80% missing values from the test dataset\nnew_test_df = drop_columns_with_missing_values(test_df, 80)","74f63827":"# Separate features into object and numerical variables\n\n# train object cols\nobject_cols_train = get_object_cols(new_train_df)\n# train numerical cols\nnumerical_cols_train = get_numerical_cols(new_train_df)","6ccc6809":"# test object cols\nobject_cols_test = get_object_cols(new_test_df)\n# test numerical cols\nnumerical_cols_test = get_numerical_cols(new_test_df)","dfe85433":"# columns whose missing values are to be filled with 'None'\ncols = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', \n        'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']\n\n# Now for these columns in train and test dataframes, replace missing values with 'None'\nvalues = {col: 'None' for col in cols}\n\nnew_train_df = new_train_df.fillna(value=values)\nnew_test_df = new_test_df.fillna(value=values)","256f593a":"# for other categorical variables, replace NaNs with the most frequent variable\nother_cols = [col for col in object_cols_train if col not in cols]\nprint(other_cols)\n\n# create a dictionary to use to replace missing values in train dataframe\nvalues = {col:  new_train_df[col].mode()[0] for col in other_cols}\n\n# replace missing values\nnew_train_df = new_train_df.fillna(value=values)\n\n# create a dictionary to use to replace missing values in test dataframe\nvalues = {col:  new_test_df[col].mode()[0] for col in other_cols}\n\n# replace missing values\nnew_test_df = new_test_df.fillna(value=values)","ac504312":"# check missing values in train df\nvisualize_missing_values(new_train_df)","8073b952":"# check missing values in test df\nvisualize_missing_values(new_test_df)","140afe9e":"# create a dictionary to replace missing numerical values in train dataset\nvalues = {'LotFrontage': new_train_df['LotFrontage'].median(),\n          'GarageYrBlt': new_train_df['GarageYrBlt'].median(),\n          'MasVnrArea': 0\n         }\n\n# replace missing values\nnew_train_df = new_train_df.fillna(value=values)","8b0178b4":"# get columns with missing values in test dataframe\ncols = list(new_test_df.columns[new_test_df.isnull().any()])\n\nvalues = {}\nfor col in cols:\n    if col in ['LotFrontage', 'GarageYrBlt']:\n        values[col] = new_test_df[col].median()\n    else:\n        values[col] = 0\n# now replace missing values in test dataframe\nnew_test_df = new_test_df.fillna(value=values)","8f7d3973":"# check missing values in train df\nnp.sum(new_train_df.isnull().sum())","da39d4ae":"# check missing values in test df\nnp.sum(new_test_df.isnull().sum())","bc48b4fd":"# from our correlation graph, we are going to drop columns that do not positively influence the SalePrice\ncols_to_drop = list(corr_df[corr_df['values'] < 0]['cols'])\ncols_to_drop","58c52ef6":"# drop these columns from train and test dfs\nnew_train_df = new_train_df.drop(labels=cols_to_drop, axis=1)\nnew_test_df = new_test_df.drop(labels=cols_to_drop, axis=1)","83f47e0a":"new_train_df.shape","b47dc299":"new_test_df.shape","e27988dc":"# lets combine the train and test dataframes before performing label and one-hot encoding schemes\nnew_train_df['train']  = 1 # the train column will later be used to split the combined dataframes\nnew_test_df['train']  = 0\n\ndf = pd.concat([new_train_df, new_test_df], axis=0,sort=False)","11511250":"df.head()","8e235ec7":"# Using the data description file, lets create a list for columns marked for label encoding\nlabel_enc_cols = [\n    'ExterQual',\n    'ExterCond',\n    'GarageCond',\n    'GarageQual',\n    'FireplaceQu',\n    'KitchenQual',\n    'CentralAir',\n    'HeatingQC',\n    'BsmtFinType2',\n    'BsmtFinType1',\n    'BsmtExposure',\n    'BsmtCond',\n    'BsmtQual'\n]","915e2a82":"# Using the data description file, lets create a dictionary to use for substituting each value in a column Series with the key values\nmap_dict = {\n    'Ex': 5,\n    'Gd': 4,\n    'TA': 3,\n    'Fa': 2,\n    'Po': 1,\n    'None': 0,\n    'Av':   3,\n    'Mn':   2,\n    'No':   1,\n    'GLQ':  6,\n    'ALQ':  5,\n    'BLQ':  4,\n    'Rec':  3,\n    'LwQ':  2,\n    'Unf':  1,\n    'N':    0,\n    'Y':    1\n}","94c167a9":"# now perform label encoding in combined df\nfor col in label_enc_cols:\n    df[col] = df[col].map(map_dict)","ef163c5d":"# check label encoding\nfor col in label_enc_cols:\n    print(pd.unique(df[col]))","53d7dc34":"# create a list of columns to perfom one-hot encoding on\none_hot_enc_cols = [col for col in object_cols_train if col not in label_enc_cols ]","bfd8e0b4":"# perform one hot encoding using pd.get_dummies \none_hot_df = pd.get_dummies(df[one_hot_enc_cols], drop_first=True)","d2f2e261":"one_hot_df.head()","8c8b8013":"# now combine dummies dataframe with the combined train and test dfs\ndf_final = pd.concat([df, one_hot_df], axis=1, sort=False)","2c93a77c":"# drop columns used for one hot encoding\ndf_final = df_final.drop(labels=one_hot_enc_cols, axis=1)","ccfeaa1d":"df_final.head()","5970e8fe":"# Now that we are done with preparing our data for modelling, separate df_final into train and test\n\n# use the 'train' column to separate the combined df\ntrain_df_final =  df_final[df_final['train'] == 1]\ntrain_df_final = train_df_final.drop(labels=['train'], axis=1)\n\n\ntest_df_final = df_final[df_final['train'] == 0]\ntest_df_final = test_df_final.drop(labels=['SalePrice'], axis=1)\ntest_df_final = test_df_final.drop(labels=['train'], axis=1)","f6c3e54d":"# check dimensions\nprint(train_df_final.shape)\nprint(test_df_final.shape)","31c65909":"y= train_df_final['SalePrice']\nX = train_df_final.drop(labels=['SalePrice'], axis=1)\n\n# split data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=50)","403c6142":"# specify parameters via map, definition are same as c++ version\nparam = {\n    'max_depth': 3,\n    'eta' : .1,\n    'gamma' : 0,\n    'min samples split' : 2,\n    'min samples leaf': 1,\n    'objective': \"reg:squarederror\",\n    'subsample': 1\n}\nkfold = KFold(n_splits=10, shuffle=True, random_state=7)\nparams = {\n    'max_depth': [2, 4, 6],\n    'n_estimators': [50, 100, 200, 500, 1000],\n    'learning_rate': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n}\nprint(\"Parameter optimization\")\nxgb_model = xgb.XGBRegressor()\nclf = GridSearchCV(xgb_model,param_grid=params, verbose=0, cv=kfold, n_jobs=-1)\nclf.fit(X, y)\nprint(clf.best_score_)\nprint(clf.best_params_)","fa28aa95":"# lets take a peek at the best model\nclf.best_estimator_","fca6fbc8":"# plot tree\nplot_tree(clf.best_estimator_)\nplt.show()","15fd3953":"# plot feature importance\nax = plot_importance(clf.best_estimator_, height=1)\nfig = ax.figure\nfig.set_size_inches(10, 30)\nplt.show()","b7aadac7":"# fit model on all training data\nmodel = xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.1, max_delta_step=0, max_depth=2,\n             min_child_weight=1, monotone_constraints='()',\n             n_estimators=1000, n_jobs=0, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)\n\nmodel.fit(X_train, y_train)\n# make predictions for test data and evaluate\npredictions = model.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test.values, predictions))\nprint(\"RMSE:\", (rmse))\n# Fit model using each importance as a threshold\nthresholds = np.sort(model.feature_importances_)\nbest_threshold = X_train.shape[1]\nbest_score = rmse\nfor thresh in thresholds:\n    # select features using threshold\n    selection = SelectFromModel(model, threshold=thresh, prefit=True)\n    select_X_train = selection.transform(X_train)\n    \n    # train model\n    selection_model = xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.1, max_delta_step=0, max_depth=2,\n             min_child_weight=1, monotone_constraints='()',\n             n_estimators=1000, n_jobs=0, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)\n    \n    selection_model.fit(select_X_train, y_train)\n    # eval model\n    select_X_test = selection.transform(X_test)\n    predictions = selection_model.predict(select_X_test)\n    score = np.sqrt(mean_squared_error(y_test, predictions))\n    if score < best_score:\n        best_score = score\n        best_threshold = select_X_train.shape[1]\n    \n    print(\"Thresh={}, n={}, RMSE: {}\".format(thresh, select_X_train.shape[1], score))\nprint('Best RMSE: {}, n={}'.format(best_score, best_threshold))","c266713d":"feature_importance = pd.DataFrame(pd.Series(model.feature_importances_, index=X_train.columns, \n                               name='Feature_Importance').sort_values(ascending=False)).reset_index()\nselected_features = feature_importance.iloc[0:best_threshold]['index']\nselected_features = list(selected_features)","498b0d38":"# now use the selected features  and fit the model on X and Y\nnew_X = X[selected_features]\nnew_test = test_df_final[selected_features]\n\nmodel.fit(new_X, y)","32792298":"# Now lets make predictions on the test dataset for submission\nsubmission_predictions = model.predict(new_test)","25deed34":"# prepare a csv file for submission\nsub_df = pd.DataFrame(submission_predictions)\nsub_df['Id'] = test_df['Id']\nsub_df.columns = ['SalePrice', 'Id']\nsub_df = sub_df[['Id', 'SalePrice']]\n\nsub_df.to_csv('submission.csv', index=False)","7c36fa3f":"### Explore GrLivArea feature","25029dbf":"## Split the final train dataset into train and test sets","32ec1e43":"## Handle Columns With Missing Values","4dc412fe":"# Predicting Housing Prices Using XGBoost","13038137":"> Median sales price increases as Overall Quality increases","e6cfeed8":"> Save for a few outliers, it seems that an increase in living area corresponds to increases in sale prices","ebd1c36b":"> As the size of garage in car capacity increases, the median sale price increases","17e51669":"## Analyse correlation between the target feature and other columns in the train dataframe","44575706":"## Feature Selection and Feature engineering","9020ce6d":"Comment:\n> We now have clean dataframes with no missing values, we are now ready to perform feature engineering","3db0be34":"## Train an XGBoost Model","eff479f5":"## Load the dataset","b917ea81":"### Explore OverallQual feature","7a13988d":"Comments:\n> Looking at the data description, for the following columns; <i>BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, FireplaceQu, GarageType, GarageFinish, GarageQual, GarageCond<\/i>, a missing value means that the feature is not available for the specific house: \n\n> For these columns, we will replace missing values with 'None'\n\n> For the other categorical columns with missing values, missing values will be replaced with the most frequent value for that column. ","e8fdc6a6":"### One hot encoding","b382df9e":"## Explore Missing Values","0f5c42c0":"> From the graph we can that OverallQual, GrLivArea, GarageCars and GarageArea are some of the features exhibiting high correlation with the target fearture","7bcfb056":"### Explore GarageArea feature","763a8c16":"## Load Libraries","762be32a":"Comment: \n> It seems that the test dataset has many columns with missing values that the train dataset","4fee1ff2":"Comment:\n> From the graph, we see that columns, Alley, PoolQC, Fence and MiscFeature have over 80% missing values. We are going to drop these columns from the train and test datasets","83e5edd8":"> It does appear that as the size of the garage area increases, the sale price increases. However, we observe that this feature has many outliers. And these outliers can affect our regression performance.","ba1962ef":"Comment:\n> From the visualizations above, it looks like we have more missing values in LotFrontage and GarageYrBlt features, we are not going to simply replace missing values with zeros for these features. We will replace missing values with the median while the rest of the missing values will be replaced with 0.","fb184ef2":"#### Handle missing values in other columns\n> Our approach in handle missing values will be based on whether a feature is categorical or numerical variable","790add8b":"### Explore GarageCars feature","70a4d391":"### Label encoding"}}