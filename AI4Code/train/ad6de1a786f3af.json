{"cell_type":{"5db0d0a2":"code","a0a6ccb7":"code","d225a625":"code","f7601dad":"code","02d1c9d9":"code","3b373fdb":"code","95275c08":"code","68170252":"code","a1f05cd3":"code","084aeab3":"code","1e4ce8fd":"code","1eacaa4d":"code","a244cf6e":"code","f09406fe":"code","703e1e9b":"code","dd98ae27":"code","70539548":"code","6d2f79e3":"code","73763b71":"code","7868da9c":"code","41558030":"code","396fd738":"code","f09b5a75":"code","8f23c0e8":"code","7a1df8b7":"code","6dbc3af6":"code","674fb055":"code","b52d10b6":"code","8f874ea0":"code","ee057608":"code","f5568c36":"code","30dccd60":"code","1e3a6dc4":"code","1a29898e":"code","47b5195c":"code","3d030f24":"code","588301c6":"code","85f85a16":"code","e8dfbf3c":"code","4ea4308d":"code","4a004d42":"code","c2f8e2d5":"code","717ec376":"code","cf334fb1":"code","aad07493":"code","e7b41840":"code","efc83c84":"code","2e3ee53c":"code","8f9f4755":"code","e254742a":"code","6853c64e":"code","5ceab573":"code","16bacb82":"code","2cfdbca5":"code","4c1d77ea":"code","9a1a8951":"code","897d0658":"code","2cdad33d":"code","ffe9a031":"code","9124714f":"code","581ac8a7":"code","be702783":"code","f8a77391":"code","432bf870":"code","9e18346f":"code","b81d8959":"markdown","9b7b4e0e":"markdown","2965d237":"markdown","97b27e62":"markdown","c85b580b":"markdown","72eb2e99":"markdown","db60fff9":"markdown","d58ccbbd":"markdown","2ee1eb13":"markdown","21d506db":"markdown","a2b640b0":"markdown","5f7d6da3":"markdown","f93f0d65":"markdown","7813e40a":"markdown","b51b73fe":"markdown","06b2ef11":"markdown","ebb4084f":"markdown","cbbaf05f":"markdown","2dbbb24c":"markdown","c59ae09b":"markdown","7f0068c0":"markdown","4c07686c":"markdown"},"source":{"5db0d0a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a0a6ccb7":"train_data=pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")","d225a625":"train_data.insert(loc=0,column='rowNum',value=np.arange(len(train_data)))","f7601dad":"train_data=train_data.set_index('rowNum')","02d1c9d9":"train_data.dropna(subset=['Embarked'],inplace=True)","3b373fdb":"%matplotlib inline\nimport matplotlib.pyplot as plt\ntrain_data.hist(bins=50, figsize=(20,15))\nplt.show()","95275c08":"corr_matrix = train_data.corr()\ncorr_matrix[\"Survived\"].sort_values(ascending=False)","68170252":"corr_matrix['Pclass'].sort_values(ascending=False)","a1f05cd3":"import seaborn as sns\nsns.catplot('Pclass', hue='Survived', data=train_data, kind='count')","084aeab3":"sns.catplot('Pclass', hue='Age', data=train_data, kind='count')","1e4ce8fd":"from sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import LinearSVC","1eacaa4d":"Y = train_data['Survived'].copy()\nX_PassengerId=train_data['PassengerId'].copy()\nX = train_data.drop(columns=['Survived'])\n# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)","a244cf6e":"#impute default values\ndef impute_def_values(df):\n    \"Extract the 'Cabin' and 'Family_size' features.\"\n#     df['Cabin'] = df['Cabin'].map(lambda x: 'Known' if type(x) is str else 'Unknown')\n    df['SibSp'].fillna(0,inplace= True)\n    df['Parch'].fillna(0,inplace=True)\n    df['Age'].fillna(df['Age'].median(),inplace= True)\n    df['Embarked'].fillna('S',inplace=True)\n    df['Fare'].fillna(df['Fare'].median(),inplace=True)\n    return df","f09406fe":"X=impute_def_values(X)","703e1e9b":"from sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\n# num_attribs=['Pclass','Age','Fare']\n# cat_attribs=['Sex','Cabin','Embarked']\ndrop_attribs=['Name','Ticket']","dd98ae27":"def extract_fetatures(X):\n    \"Extract the 'Cabin' and 'Family_size' features.\"\n    X['Cabin'] = X['Cabin'].map(lambda x: 'Known' if type(x) is str else 'Unknown')\n    \n    X['Family']=X['SibSp'] + X['Parch']\n    X['Family']=X['Family'].map(lambda x: 'No' if x==0\n                                       else 'Few' if x in [1,2,3]\n                                       else 'Many')\n    return X.drop(columns=['SibSp','Parch'])\n    ","70539548":"column_trans = make_column_transformer(\n                    ('drop', drop_attribs),\n                    (make_pipeline( # Extract the features and then encode them\n                        FunctionTransformer(extract_fetatures),\n                        OneHotEncoder()), ['Cabin','SibSp', 'Parch']),\n                    (OneHotEncoder(), ['Pclass', 'Sex', 'Embarked']),\n                    remainder=StandardScaler()) # Scale the continuous variables","6d2f79e3":"preprocessor = make_pipeline(column_trans,\n                             IterativeImputer(random_state=0)) # Impute missing values","73763b71":"extracted_X = preprocessor.fit_transform(X)","7868da9c":"transfs = preprocessor[0].transformers_","41558030":"extracted_featuers=transfs[1][1][1].get_feature_names(['Cabin','Family'])\nencoded_features=transfs[2][1].get_feature_names(['Pclass', 'Sex', 'Embarked'])\nremainder_features=['Age','Fare','X_PassengerId']","396fd738":"all_features=list(extracted_featuers)+list(encoded_features)+remainder_features","f09b5a75":"extracted_X_train=pd.DataFrame(extracted_X,X.index,all_features)\n# extract_X.head()","8f23c0e8":"# extracted_X_train=pd.concat([extracted_X_train,X_PassengerId],axis=1)","7a1df8b7":"## Applying K-Folds cross-validator\nkf=KFold(n_splits=4,shuffle=True)\n# model = SVC(kernel='linear', C=100)\nmodel = LinearSVC(loss=\"hinge\", C=1)","6dbc3af6":"for nbrOfFolds,(train_index, test_index) in enumerate(kf.split(extracted_X_train)):\n    X_train, X_test = extracted_X_train.iloc[train_index], extracted_X_train.iloc[test_index]\n    Y_train, Y_test  = Y.iloc[train_index], Y.iloc[test_index]\n    model.fit(X_train,Y_train)\n#     predTrain=model.predict((X_train))\n#     tempTrain=tempTrain+accuracy_score(Y_train,predTrain)\n    predTest=model.predict((X_test))\n    n_correct=sum(predTest==Y_test)\n    print(n_correct \/ len(predTest))\n#     tempTest=tempTest+accuracy_score(Y_test,predTest)\n    \n# print(f'Number of Folds{nbrOfFolds+1}')\n# train_accuracy.append(tempTrain*1.0\/(nbrOfFolds+1))\n# test_accuracy.append(tempTest*1.0\/(nbrOfFolds+1))\n# print(\"(Train, Test) accuracy=\",tempTrain*1.0\/(nbrOfFolds+1),tempTest*1.0\/(nbrOfFolds+1))","674fb055":"from sklearn.model_selection import cross_val_predict\ny_train_pred = cross_val_predict(model, extracted_X_train, Y, cv=4)","b52d10b6":"cross_val_pred_accuracy_score=accuracy_score(Y,y_train_pred)\nprint(f\"Accuracy score is {cross_val_pred_accuracy_score}\")","8f874ea0":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import plot_confusion_matrix\n\nconf_matrx=confusion_matrix(Y,y_train_pred)","ee057608":"plt.matshow(conf_matrx,cmap=plt.cm.gray)\nplt.show()","f5568c36":"rows_sum=conf_matrx.sum(axis=1,keepdims=True)\nnorm_conf_mx=conf_matrx\/rows_sum\n\n# print(rows_sum,norm_conf_mx)","30dccd60":"np.fill_diagonal(norm_conf_mx,0)\nplt.matshow(norm_conf_mx,cmap=plt.cm.gray)\nplt.show()","1e3a6dc4":"from sklearn.metrics import precision_score, recall_score\nprecision_score(Y,y_train_pred)","1a29898e":"recall_score(Y,y_train_pred)","47b5195c":"from sklearn.metrics import f1_score\nf1_score(Y,y_train_pred)","3d030f24":"y_train_scores = cross_val_predict(model, extracted_X_train, Y, cv=4,method=\"decision_function\")","588301c6":"from sklearn.metrics import precision_recall_curve\nprecisions, recalls, thresholds = precision_recall_curve(Y, y_train_scores)","85f85a16":"def plot_precision_recall_vs_thresold(precisions, recalls, thresholds):\n    plt.plot(thresholds,precisions[:-1],\"b--\",label=\"Precision\")\n    plt.plot(thresholds,recalls[:-1],\"g-\",label=\"Recall\")\n    plt.xlabel(\"Threshold\")\n    plt.legend(loc='center right')\n    plt.ylim([0,1])","e8dfbf3c":"plot_precision_recall_vs_thresold(precisions, recalls, thresholds)\nplt.show()","4ea4308d":"plt.plot(recalls,precisions)\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.show()","4a004d42":"from sklearn.metrics import roc_curve\nfpr,tpr,thresholds=roc_curve(Y, y_train_scores)","c2f8e2d5":"def plot_roc_curve(fpr,tpr,label=None):\n    plt.plot(fpr,tpr,linewidth=2,label=label)\n    plt.plot([0,1],[0,1],'k--')\n    plt.axis([0,1,0,1])\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    ","717ec376":"plot_roc_curve(fpr,tpr)\nplt.show()","cf334fb1":"from sklearn.metrics import roc_auc_score\nroc_auc_score(Y,y_train_scores)","aad07493":"from sklearn.ensemble import RandomForestClassifier\nforest_clf=RandomForestClassifier(max_depth=8,random_state=13)","e7b41840":"for nbrOfFolds,(train_index, test_index) in enumerate(kf.split(extracted_X_train)):\n    X_train, X_test = extracted_X_train.iloc[train_index], extracted_X_train.iloc[test_index]\n    Y_train, Y_test  = Y.iloc[train_index], Y.iloc[test_index]\n    forest_clf.fit(X_train,Y_train)\n#     predTrain=model.predict((X_train))\n#     tempTrain=tempTrain+accuracy_score(Y_train,predTrain)\n    predTest=model.predict((X_test))\n    n_correct=sum(predTest==Y_test)\n    print(n_correct \/ len(predTest))","efc83c84":"y_train_forest_probas=cross_val_predict(forest_clf,extracted_X_train, Y, cv=4,method=\"predict_proba\")","2e3ee53c":"y_train_forest_pred = cross_val_predict(forest_clf, extracted_X_train, Y, cv=4)","8f9f4755":"cross_val_forest_pred_accuracy_score=accuracy_score(Y,y_train_forest_pred)\nprint(f\"Accuracy score is {cross_val_forest_pred_accuracy_score}\")","e254742a":"y_train_forest_scores=y_train_forest_probas[:,1]\nfpr_forest, tpr_forest, thresholds_forest = roc_curve(Y,y_train_forest_scores)","6853c64e":"plt.plot(fpr,tpr,\"b:\",label='Linear SVC')\nplot_roc_curve(fpr_forest, tpr_forest,\"Random Forest\")\nplt.legend(loc=\"lower right\")\nplt.show()","5ceab573":"roc_auc_score(Y, y_train_forest_scores)","16bacb82":"test_data_X=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n","2cfdbca5":"test_data_X.insert(loc=0,column='rowNum',value=np.arange(len(test_data_X)))\ntest_data_X=test_data_X.set_index('rowNum')","4c1d77ea":"X_test_PassengerId=test_data_X['PassengerId'].copy()\n# test_data_X = test_data_X.drop(columns=['PassengerId'])","9a1a8951":"# test_data_X=test_data_X.set_index('PassengerId')\ntest_data_X=impute_def_values(test_data_X)\nextracted_test_data_X = preprocessor.transform(test_data_X)","897d0658":"X_test=pd.DataFrame(extracted_test_data_X,test_data_X.index,all_features)","2cdad33d":"# X_test=pd.concat([X_test,X_test_PassengerId],axis=1)","ffe9a031":"# forest_clf.predict()\nY_pred = forest_clf.predict(X_test)","9124714f":"X_test_PassengerId","581ac8a7":"submission = pd.DataFrame({\n        \"Survived\": Y_pred\n    })","be702783":"submission.insert(loc=0,column='rowNum',value=np.arange(len(submission)))\nsubmission=submission.set_index('rowNum')","f8a77391":"submission=pd.concat([X_test_PassengerId,submission],axis=1)","432bf870":"submission","9e18346f":"submission.to_csv('submission.csv',index=False)","b81d8959":"ROC Curve comparision","9b7b4e0e":"**Calculate Precision**\n\n**TP\/(TP+FP)**","2965d237":"# *Data Processing*","97b27e62":"**Cross Validation Prediction Accurary**","c85b580b":"**Compute ROC AUC score**\n","72eb2e99":"The PR curve makes it clear that the classifier has room for improvement (the curve could be closer to the topright corner).","db60fff9":"Plot Precision Recall curve\n","d58ccbbd":"**Calculate F1 Score**\n\n**TP\/(TP+(FN+FP)\/2)**\n\nThe choice between two classifiers can be made by calculating ***F1_Score***\n","2ee1eb13":"# **Data Visualization**\n","21d506db":"Pclass vs Survived","a2b640b0":"There are two NA values in Embarked column, so drop the 2 rows","5f7d6da3":"**Calculate Recall**\n\n\n**TP\/(TP+FN)**","f93f0d65":"The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner).","7813e40a":"**Decision threshold and precision\/recall tradeoff**\n\n\n","b51b73fe":"****Confusion Matrix on Train data****\n\nEach row in a confusion matrix represents an actual class, while each column represents\na predicted class.\n\nIn the below array\n* 00--> TN\n* 01--> FP\n* 10--> FN\n* 11--> TP","06b2ef11":"**The ROC Curve**\n\n\n\nTPR Vs (1-TNR) or Sensitivity Vs 1-Specificity","ebb4084f":"**Precision vs Recall PR Curve**","cbbaf05f":"Testing the Model","2dbbb24c":"Unfortunately, you can\u2019t have it both ways: increasing precision reduces recall, and\nvice versa. This is called the precision\/recall tradeoff.","c59ae09b":"Pclass vs Age","7f0068c0":"Split the data into train and test","4c07686c":"# ***Random Forest Classifier***"}}