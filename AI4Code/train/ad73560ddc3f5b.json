{"cell_type":{"67fe0753":"code","438699b8":"code","f8847440":"code","db3ce056":"code","25e198c1":"code","aa6baf20":"code","404b5d64":"code","82b35e97":"code","10b70a3e":"code","09275950":"code","c8a16c10":"code","525fc7e7":"code","816e4d69":"code","e47c3e80":"code","b8238d74":"code","0f56eab6":"code","e7ddf275":"code","8822c2bc":"code","7292d58c":"code","64978361":"code","60ae2521":"code","9948c9ee":"code","14c510af":"code","2a27bed3":"code","f9cd02bb":"code","8855a337":"code","cf5cfccc":"code","09e8c956":"code","00c7af67":"code","92d5561d":"code","182ee25c":"code","0a4fbd96":"code","5e206af0":"code","f0e679d0":"code","1e75862e":"code","94b36221":"code","65322287":"code","dcae32b2":"code","b5c3a63f":"code","ff787cf1":"code","fcae98a6":"code","8f64ffad":"code","2647d122":"code","12733b19":"code","89415a90":"code","0381380f":"code","8521a934":"code","6642593b":"code","ec92afa2":"code","57362d7e":"code","eb56809a":"code","453bc16c":"code","eac1a68d":"code","5dcf6fea":"code","88a2d6c6":"code","2b8d8783":"code","98a238e1":"code","c8430d63":"code","bcdf8516":"code","73861bce":"code","a99d21a2":"code","10dcae10":"code","0c81692a":"code","4cded707":"code","544bc337":"code","21f97025":"code","49a653ae":"code","951bbb4e":"code","fe31050c":"code","3d5bc559":"code","37e026a4":"code","3bff92b8":"code","ab8f5880":"code","abfa2624":"code","77a8ef9d":"markdown","71992fce":"markdown","f8b8fdfe":"markdown","d988e42a":"markdown","09f88792":"markdown","27d9346d":"markdown","ac11234a":"markdown","29d83535":"markdown","7628eca3":"markdown","7dd5c633":"markdown"},"source":{"67fe0753":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","438699b8":"import numpy as np\nimport pandas as pd\n\n# Visualization \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nimport warnings\nwarnings.simplefilter(action='ignore')\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score,GridSearchCV,KFold,RandomizedSearchCV,train_test_split\nimport math\nimport sklearn.model_selection as ms\nimport sklearn.metrics as sklm","f8847440":"# Exploratory data analysis\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","db3ce056":"pd.set_option('display.max_columns',None)\npd.set_option('display.max_rows',None)","25e198c1":"train.head()","aa6baf20":"test.head()","404b5d64":"print('The shape of our training set: ',train.shape[0], 'houses', 'and', train.shape[1], 'features')\nprint('The shape of our testing set: ',test.shape[0], 'houses', 'and', test.shape[1], 'features')\nprint('The testing set has 1 feature less than the training set, which is SalePrice, the target to predict  ')","82b35e97":"# Numerical value correaltion with sales Pricw i.we the target variable\nnum = train.select_dtypes(exclude='object')\nnumcorr = num.corr()\nf,ax = plt.subplots(figsize=(17,1))\nsns.heatmap(numcorr.sort_values(by=['SalePrice'],ascending=False).head(1),cmap='Blues')\nplt.title('Numerical features correlation with the sales price', weight='bold',fontsize=18)\nplt.xticks(weight = 'bold')\nplt.yticks(weight='bold',color='dodgerblue',rotation=0)\nplt.show()","10b70a3e":"Num = numcorr['SalePrice'].sort_values(ascending=False).head(10).to_frame()\n\ncm = sns.light_palette('violet',as_cmap=True)\ns = Num.style.background_gradient(cmap=cm)\ns","09275950":"plt.figure(figsize=(10,6))\nplt.scatter(x=train['GrLivArea'],y=train['SalePrice'],color = 'blue',alpha=0.5)\nplt.title('Ground Living area\/ Sale Price',weight = 'bold',fontsize=16)\nplt.xlabel('Ground living area',weight='bold',fontsize=12)\nplt.ylabel('Sale price',weight = 'bold',fontsize=12)\nplt.xticks(weight='bold')\nplt.yticks(weight='bold')\nplt.show()","c8a16c10":"# Figure Size\nfig, ax = plt.subplots(figsize=(10,6))\ntrain['Neighborhood'].value_counts().sort_values(ascending=True).plot(kind='barh',color='red',alpha=0.5)\nplt.xlabel('Count',weight='bold',fontsize=12)\nplt.title('Most Frequent Neighborhoods',weight='bold',fontsize=14)\n\nfor i in ax.patches:\n    ax.text(i.get_width()+1, i.get_y()+0.25, str(round((i.get_width()), 2)),\n            fontsize=10, fontweight='bold', color='grey')\nplt.yticks(weight='bold')\nplt.show()","525fc7e7":"# Figure Size\nfig, ax = plt.subplots(figsize=(10,6))\ntrain['BldgType'].value_counts().sort_values(ascending=True).plot(kind='barh',color='green',alpha=0.5,ax=ax)\nplt.xlabel('Count',weight='bold',fontsize=12)\nplt.title('Building type: Type of dwelling',loc='center',weight='bold',fontsize=14)\n\n\nfor i in ax.patches:\n    ax.text(i.get_width()+1, i.get_y()+0.25, str(round((i.get_width()), 2)),\n            fontsize=10, fontweight='bold', color='grey',va=\"center\")\n\nplt.yticks(weight='bold')\nplt.show()","816e4d69":"na = train.shape[0]\nnb = test.shape[0]\ny_train = train['SalePrice'].to_frame()\n#Combine train and test sets\nc1 = pd.concat((train, test), sort=False).reset_index(drop=True)\n#Drop the target \"SalePrice\" and Id columns\nc1.drop(['SalePrice'], axis=1, inplace=True)\nc1.drop(['Id'], axis=1, inplace=True)\nprint(\"Total size is :\",c1.shape)","e47c3e80":"missing = pd.DataFrame()\nt1 =(round((c1.isnull().sum()\/len(c1)*100),2))\ncol_name = t1.index.tolist()\nt1 = list(t1.sort_values(ascending=False)) \nmissing['Col_Name']=col_name\nmissing['Pecentage']=t1\nmissing\n","b8238d74":"def msv1(data, thresh=20, color='black', edgecolor='black', width=15, height=3):\n    \"\"\"\n    SOURCE: https:\/\/www.kaggle.com\/amiiiney\/price-prediction-regularization-stacking\n    \"\"\"\n    \n    plt.figure(figsize=(width,height))\n    percentage=(data.isnull().mean())*100\n    percentage.sort_values(ascending=False).plot.bar(color=color, edgecolor=edgecolor)\n    plt.axhline(y=thresh, color='r', linestyle='-')\n    plt.title('Missing values percentage per column', fontsize=20, weight='bold' )\n    plt.text(len(data.isnull().sum()\/len(data))\/1.7, thresh+12.5, 'Columns with more than %s%s missing values' %(thresh, '%'), fontsize=12, color='crimson',\n         ha='left' ,va='top')\n    plt.text(len(data.isnull().sum()\/len(data))\/1.7, thresh - 5, 'Columns with less than %s%s missing values' %(thresh, '%'), fontsize=12, color='green',\n         ha='left' ,va='top')\n    plt.xlabel('Columns', size=15, weight='bold')\n    plt.ylabel('Missing values percentage')\n    plt.yticks(weight ='bold')\n    \n    return plt.show()\nmsv1(c1, 20, color=('silver', 'gold', 'lightgreen', 'skyblue', 'lightpink'))\n","0f56eab6":"c=c1.dropna(thresh=len(c1)*0.8, axis=1)\nprint('We dropped ',c1.shape[1]-c.shape[1], ' features in the combined set')","e7ddf275":"allna = (c.isnull().sum() \/ len(c))*100\nallna = allna.drop(allna[allna == 0].index).sort_values()\n\ndef msv2(data, width=12, height=8, color=('silver', 'gold','lightgreen','skyblue','lightpink'), edgecolor='black'):\n    \"\"\"\n    SOURCE: https:\/\/www.kaggle.com\/amiiiney\/price-prediction-regularization-stacking\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(width, height))\n\n    allna = (data.isnull().sum() \/ len(data))*100\n    tightout= 0.008*max(allna)\n    allna = allna.drop(allna[allna == 0].index).sort_values().reset_index()\n    mn= ax.barh(allna.iloc[:,0], allna.iloc[:,1], color=color, edgecolor=edgecolor)\n    ax.set_title('Missing values percentage per column', fontsize=15, weight='bold' )\n    ax.set_xlabel('Percentage', weight='bold', size=15)\n    ax.set_ylabel('Features with missing values', weight='bold')\n    plt.yticks(weight='bold')\n    plt.xticks(weight='bold')\n    for i in ax.patches:\n        ax.text(i.get_width()+ tightout, i.get_y()+0.1, str(round((i.get_width()), 2))+'%',\n            fontsize=10, fontweight='bold', color='grey')\n    return plt.show()","8822c2bc":"msv2(c)","7292d58c":"print('The shape of the combined dataset after dropping features with more than 80% M.V.', c.shape)","64978361":"NA=c[allna.index.to_list()]","60ae2521":"NAcat=NA.select_dtypes(include='object')\nNAnum=NA.select_dtypes(exclude='object')\nprint('We have :',NAcat.shape[1],'categorical features with missing values')\nprint('We have :',NAnum.shape[1],'numerical features with missing values')","9948c9ee":"# Numerical Feature\nNAnum.head()","14c510af":"#MasVnrArea: Masonry veneer area in square feet, the missing data means no veneer so we fill with 0\nc['MasVnrArea']=c.MasVnrArea.fillna(0)\n#LotFrontage has 16% missing values. We fill with the median\nc['LotFrontage']=c.LotFrontage.fillna(c.LotFrontage.median())\n#GarageYrBlt:  Year garage was built, we fill the gaps with the median: 1980\nc['GarageYrBlt']=c[\"GarageYrBlt\"].fillna(1980)\n#For the rest of the columns: Bathroom, half bathroom, basement related columns and garage related columns:\n#We will fill with 0s because they just mean that the hosue doesn't have a basement, bathrooms or a garage","2a27bed3":"# 2.3 Categorical features:\n# And we have 18 Categorical features with missing values:\n\n# Some features have just 1 or 2 missing values, so we will just use the forward fill method because they are obviously values that can't be filled with 'None's\n# Features with many missing values are mostly basement and garage related (same as in numerical features)\n# so as we did with numerical features (filling them with 0s), we will fill the categorical missing values with \"None\"s assuming that the houses lack basements and garages.\nNAcat.head()","f9cd02bb":"# Number of missing percolumns\nNAcat1= NAcat.isnull().sum().to_frame().sort_values(by=[0]).T\ncm = sns.light_palette(\"lime\", as_cmap=True)\n\nNAcat1 = NAcat1.style.background_gradient(cmap=cm)\nNAcat1","8855a337":"# The table above helps us to locate the categorical features with few missing values.\n\n# We start our cleaning with the features having just few missing value (1 to 4): We fill the gap with forward fill method:\n\nfill_cols = ['Electrical', 'SaleType', 'KitchenQual', 'Exterior1st',\n             'Exterior2nd', 'Functional', 'Utilities', 'MSZoning']\n\nfor col in c[fill_cols]:\n    c[col] = c[col].fillna(method='ffill')\n","cf5cfccc":"#Categorical missing values\nNAcols=c.columns\nfor col in NAcols:\n    if c[col].dtype == \"object\":\n        c[col] = c[col].fillna(\"None\")\n#Numerical missing values\nfor col in NAcols:\n    if c[col].dtype != \"object\":\n        c[col]= c[col].fillna(0)","09e8c956":"c.isnull().sum().sort_values(ascending=False).head()","00c7af67":"# 3- Feature engineering:\n# Since the area is a very important variable, we will create a new feature \"TotalArea\" that sums the area of all the floors and the basement.\n\n# Bathrooms: All the bathroom in the ground floor\n# Year average: The average of the sum of the year the house was built and the year the house was remodeled\nc['TotalArea'] = c['TotalBsmtSF'] + c['1stFlrSF'] + c['2ndFlrSF'] + c['GrLivArea'] +c['GarageArea']\n\nc['Bathrooms'] = c['FullBath'] + c['HalfBath']*0.5 \n\nc['Year average']= (c['YearRemodAdd']+c['YearBuilt'])\/2","92d5561d":"# 4- Encoding categorical features:\n# 4.1 Numerical features:\n# We start with numerical features that are actually categorical, for example \"Month sold\", the values are from 1 to 12, each number is assigned to a month November is number 11 while March is number 3. 11 is just the order of the months and not a given value, so we convert the \"Month Sold\" feature to categorical\n\n#c['MoSold'] = c['MoSold'].astype(str)\nc['MSSubClass'] = c['MSSubClass'].apply(str)\nc['YrSold'] = c['YrSold'].astype(str)","182ee25c":"# 4.2 One hot encoding:\ncb=pd.get_dummies(c)\nprint(\"the shape of the original dataset\",c.shape)\nprint(\"the shape of the encoded dataset\",cb.shape)\nprint(\"We have \",cb.shape[1]- c.shape[1], 'new encoded features')","0a4fbd96":"# We are done with the cleaning and feature engineering. Now, we split the combined dataset to the original train and test sets\n\nTrain = cb[:na]  #na is the number of rows of the original training set\nTest = cb[na:] ","5e206af0":"a = train\nb = test","f0e679d0":"# 5- Outliers detection:\n# 5.1 Outliers visualization:\n# This part of the kernel will be a little bit messy. \n# I didn't want to deal with the outliers in the combined dataset to keep the shape of the original train and test datasets. \n# Dropping them would shift the location of the rows.\n\n# If you know a better solution to this, I will be more than happy to read your recommandations.\n\n# OK. So we go back to our original train dataset to visualize the important features \/ Sale price scatter plot to find outliers\n\n\n\nfig = plt.figure(figsize=(15,15))\nax1 = plt.subplot2grid((3,2),(0,0))\nplt.scatter(x=a['GrLivArea'], y=a['SalePrice'], color=('yellowgreen'), alpha=0.5)\nplt.axvline(x=4600, color='r', linestyle='-')\nplt.title('Ground living Area- Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(0,1))\nplt.scatter(x=a['TotalBsmtSF'], y=a['SalePrice'], color=('red'),alpha=0.5)\nplt.axvline(x=5900, color='r', linestyle='-')\nplt.title('Basement Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(1,0))\nplt.scatter(x=a['1stFlrSF'], y=a['SalePrice'], color=('deepskyblue'),alpha=0.5)\nplt.axvline(x=4000, color='r', linestyle='-')\nplt.title('First floor Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(1,1))\nplt.scatter(x=a['MasVnrArea'], y=a['SalePrice'], color=('gold'),alpha=0.9)\nplt.axvline(x=1500, color='r', linestyle='-')\nplt.title('Masonry veneer Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(2,0))\nplt.scatter(x=a['GarageArea'], y=a['SalePrice'], color=('orchid'),alpha=0.5)\nplt.axvline(x=1230, color='r', linestyle='-')\nplt.title('Garage Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(2,1))\nplt.scatter(x=a['TotRmsAbvGrd'], y=a['SalePrice'], color=('tan'),alpha=0.9)\nplt.axvline(x=13, color='r', linestyle='-')\nplt.title('TotRmsAbvGrd - Price scatter plot', fontsize=15, weight='bold' )\nplt.show()","1e75862e":"# \/The outliers are the points in the right that have a larger area or value but a very low sale price.\n# We localize those points by sorting their respective columns\n\n# Interesting! The outlier in \"basement\" and \"first floor\" features is the same as the first outlier in ground living area: \n# The outlier with index number 1298.\n# 5.2 Outliers localization:\n# We sort the columns containing the outliers shown in the graph, we will use the function head() to show the outliers: \n# head(number of outliers or dots shown in each plot)","94b36221":"a['GrLivArea'].sort_values(ascending=False).head(2)","65322287":"a['TotalBsmtSF'].sort_values(ascending=False).head(1)","dcae32b2":"a['MasVnrArea'].sort_values(ascending=False).head(1)","b5c3a63f":"a['1stFlrSF'].sort_values(ascending=False).head(1)","ff787cf1":"a['GarageArea'].sort_values(ascending=False).head(4)","fcae98a6":"a['TotRmsAbvGrd'].sort_values(ascending=False).head(1)","8f64ffad":"train=Train[(Train['GrLivArea'] < 4600) & (Train['MasVnrArea'] < 1500)]\n\nprint('We removed ',Train.shape[0]- train.shape[0],'outliers')","2647d122":"# We do the same thing with \"SalePrice\" column, we localize those outliers and make sure they are the right outliers to remove.\n\n# They both have the same price range as the detected outliers. So, we can safely drop them.\ntarget=a[['SalePrice']]\ntarget.loc[1298]","12733b19":"target.loc[523]","89415a90":"#pos = [1298,523, 297, 581, 1190, 1061, 635, 197,1328, 495, 583, 313, 335, 249, 706]\npos = [1298,523, 297]\ntarget.drop(target.index[pos], inplace=True)","0381380f":"# P.S. I didn't drop all the outliers because dropping all of them led to a worst RMSE score. More investigation is needed to filter those outliers.\n\nprint('We make sure that both train and target sets have the same row number after removing the outliers:')\nprint( 'Train: ',train.shape[0], 'rows')\nprint('Target:', target.shape[0],'rows')","8521a934":"plt.style.use('seaborn')\nsns.set_style('whitegrid')\nfig = plt.figure(figsize=(15,5))\n#1 rows 2 cols\n#first row, first col\nax1 = plt.subplot2grid((1,2),(0,0))\nplt.scatter(x=a['GrLivArea'], y=a['SalePrice'], color=('orchid'), alpha=0.5)\nplt.title('Area-Price plot with outliers',weight='bold', fontsize=18)\nplt.axvline(x=4600, color='r', linestyle='-')\n#first row sec col\nax1 = plt.subplot2grid((1,2),(0,1))\nplt.scatter(x=train['GrLivArea'], y=target['SalePrice'], color='navy', alpha=0.5)\nplt.axvline(x=4600, color='r', linestyle='-')\nplt.title('Area-Price plot without outliers',weight='bold', fontsize=18)\nplt.show()\n","6642593b":"print(\"Skewness before log transform: \", a['GrLivArea'].skew())\nprint(\"Kurtosis before log transform: \", a['GrLivArea'].kurt())","ec92afa2":"from scipy.stats import skew\n\nprint(\"Skewness after log transform: \", train['GrLivArea'].skew())\nprint(\"Kurtosis after log transform: \", train['GrLivArea'].kurt())","57362d7e":"plt.style.use('seaborn')\nsns.set_style('whitegrid')\nfig = plt.figure(figsize=(15,10))\n#1 rows 2 cols\n#first row, first col\nax1 = plt.subplot2grid((2,2),(0,0))\nsns.distplot(a.GrLivArea, color='plum')\nplt.title('Before: Distribution of GrLivArea',weight='bold', fontsize=18)\n#first row sec col\nax1 = plt.subplot2grid((2,2),(0,1))\nsns.distplot(a['1stFlrSF'], color='tan')\nplt.title('Before: Distribution of 1stFlrSF',weight='bold', fontsize=18)\n\n\nax1 = plt.subplot2grid((2,2),(1,0))\nsns.distplot(train.GrLivArea, color='plum')\nplt.title('After: Distribution of GrLivArea',weight='bold', fontsize=18)\n#first row sec col\nax1 = plt.subplot2grid((2,2),(1,1))\nsns.distplot(train['1stFlrSF'], color='tan')\nplt.title('After: Distribution of 1stFlrSF',weight='bold', fontsize=18)\nplt.show()","eb56809a":"print(\"Skewness before log transform: \", target['SalePrice'].skew())\nprint(\"Kurtosis before log transform: \",target['SalePrice'].kurt())","453bc16c":"#log transform the target:\ntarget[\"SalePrice\"] = np.log1p(target[\"SalePrice\"])","eac1a68d":"plt.style.use('seaborn')\nsns.set_style('whitegrid')\nfig = plt.figure(figsize=(15,5))\n#1 rows 2 cols\n#first row, first col\nax1 = plt.subplot2grid((1,2),(0,0))\nplt.hist(a.SalePrice, bins=10, color='mediumpurple',alpha=0.5)\nplt.title('Sale price distribution before normalization',weight='bold', fontsize=18)\n#first row sec col\nax1 = plt.subplot2grid((1,2),(0,1))\nplt.hist(target.SalePrice, bins=10, color='darkcyan',alpha=0.5)\nplt.title('Sale price distribution after normalization',weight='bold', fontsize=18)\nplt.show()","5dcf6fea":"x=train\ny=np.array(target)","88a2d6c6":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y,test_size = .3, random_state=0)","2b8d8783":"from sklearn.preprocessing import RobustScaler\nscaler= RobustScaler()\n# transform \"x_train\"\nx_train = scaler.fit_transform(x_train)\n# transform \"x_test\"\nx_test = scaler.transform(x_test)\n#Transform the test set\nX_test= scaler.transform(Test)","98a238e1":"import sklearn.model_selection as GridSearchCV\nfrom sklearn.linear_model import Ridge\n\nridge=Ridge()\nparameters= {'alpha':[x for x in range(1,101)]}\n\nridge_reg=ms.GridSearchCV(ridge, param_grid=parameters, scoring='neg_mean_squared_error', cv=15)\nridge_reg.fit(x_train,y_train)\nprint(\"The best value of Alpha is: \",ridge_reg.best_params_)\nprint(\"The best score achieved with Alpha=11 is: \",math.sqrt(-ridge_reg.best_score_))\nridge_pred=math.sqrt(-ridge_reg.best_score_)","c8430d63":"ridge_mod=Ridge(alpha=15)\nridge_mod.fit(x_train,y_train)\ny_pred_train=ridge_mod.predict(x_train)\ny_pred_test=ridge_mod.predict(x_test)\n\nprint('Root Mean Square Error train = ' + str(math.sqrt(sklm.mean_squared_error(y_train, y_pred_train))))\nprint('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, y_pred_test))))   ","bcdf8516":"# lasso Regression\nfrom sklearn.linear_model import Lasso\n\nparameters= {'alpha':[0.0001,0.0009,0.001,0.002,0.003,0.01,0.1,1,10,100]}\n\n\nlasso=Lasso()\nlasso_reg=ms.GridSearchCV(lasso, param_grid=parameters, scoring='neg_mean_squared_error', cv=15)\nlasso_reg.fit(x_train,y_train)\n\nprint('The best value of Alpha is: ',lasso_reg.best_params_)\n","73861bce":"lasso_mod=Lasso(alpha=0.0009)\nlasso_mod.fit(x_train,y_train)\ny_lasso_train=lasso_mod.predict(x_train)\ny_lasso_test=lasso_mod.predict(x_test)\n\nprint('Root Mean Square Error train = ' + str(math.sqrt(sklm.mean_squared_error(y_train, y_lasso_train))))\nprint('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, y_lasso_test))))","a99d21a2":"coefs = pd.Series(lasso_mod.coef_, index = x.columns)\n\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\", color='yellowgreen')\nplt.xlabel(\"Lasso coefficient\", weight='bold')\nplt.title(\"Feature importance in the Lasso Model\", weight='bold')\nplt.show()","10dcae10":"print(\"Lasso kept \",sum(coefs != 0), \"important features and dropped the other \", sum(coefs == 0),\" features\")","0c81692a":"from sklearn.linear_model import ElasticNetCV\n\nalphas = [0.000542555]\nl1ratio = [0.1, 0.3,0.5, 0.9, 0.95, 0.99, 1]\n\nelastic_cv = ElasticNetCV(cv=5, max_iter=1e7, alphas=alphas,  l1_ratio=l1ratio)\n\nelasticmod = elastic_cv.fit(x_train, y_train.ravel())\nela_pred=elasticmod.predict(x_test)\nprint('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, ela_pred))))\nprint(elastic_cv.alpha_)\nprint(elastic_cv.l1_ratio_)","4cded707":"def regularization(x,y,modelo=Ridge, scaler=RobustScaler):\n    \"\"\"\"\n    Function to automate regression with regularization techniques.\n    x expects the features\n    y expects the target\n    modelo: Ridge(default), Lasso, ElasticNetCV\n    scaler: RobustScaler(default), MinMaxSclaer, StandardScaler\n    SOURCE: https:\/\/www.kaggle.com\/amiiiney\/price-prediction-regularization-stacking\n    Contact: amineyamlahi@gmail.com\n    \"\"\"\n    #Split the data to train\/test\n    from sklearn.model_selection import train_test_split\n    x_train, x_test, y_train, y_test = train_test_split(x, y,test_size = .3, random_state=0)\n    \n    #Scale the data. RobustSclaer default\n    from sklearn.preprocessing import RobustScaler\n    from sklearn.preprocessing import MinMaxScaler\n    from sklearn.preprocessing import StandardScaler\n    \n    scaler= scaler()\n    # transform \"x_train\"\n    x_train = scaler.fit_transform(x_train)\n    # transform \"x_test\"\n    x_test = scaler.transform(x_test)\n    #Transform the test set\n    X_test= scaler.transform(Test)\n    \n    if modelo != ElasticNetCV:\n        if modelo == Ridge:\n            parameters= {'alpha':[x for x in range(1,101)]}\n        elif modelo == Lasso:\n            parameters= {'alpha':[0.0001,0.0009,0.001,0.002,0.003,0.01,0.1,1,10,100]}\n            \n        model=modelo()\n            \n        model=ms.GridSearchCV(model, param_grid=parameters, scoring='neg_mean_squared_error', cv=15)\n        model.fit(x_train,y_train)\n        y_pred= model.predict(x_test)\n\n        #print(\"The best value of Alpha is: \",model.best_params_)\n        print(\"The best RMSE score achieved with %s is: %s \" %(model.best_params_,\n                  str(math.sqrt(sklm.mean_squared_error(y_test, y_pred)))))\n    elif modelo == ElasticNetCV:\n        alphas = [0.000542555]\n        l1ratio = [0.1, 0.3,0.5, 0.9, 0.95, 0.99, 1]\n\n        elastic_cv = ElasticNetCV(cv=5, max_iter=1e7, alphas=alphas,  l1_ratio=l1ratio)\n\n        elasticmod = elastic_cv.fit(x_train, y_train.ravel())\n        ela_pred=elasticmod.predict(x_test)\n        print(\"The best RMSE score achieved with alpha %s and l1_ratio %s is: %s \"\n              %(elastic_cv.alpha_,elastic_cv.l1_ratio_,\n            str(math.sqrt(sklm.mean_squared_error(y_test, ela_pred)))))\n        \n            \n","544bc337":"regularization(x,y,Ridge)","21f97025":"regularization(x,y, Lasso)","49a653ae":"regularization(x,y, ElasticNetCV)","951bbb4e":"from xgboost.sklearn import XGBRegressor","fe31050c":"xgb= XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.5, gamma=0,\n             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n             max_depth=3, min_child_weight=0, missing=None, n_estimators=4000,\n             n_jobs=1, nthread=None, objective='reg:squarederror', random_state=0,\n             reg_alpha=0.0001, reg_lambda=0.01, scale_pos_weight=1, seed=None,\n             silent=None, subsample=1, verbosity=1)\nxgmod=xgb.fit(x_train,y_train)\nxg_pred=xgmod.predict(x_test)\nprint('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, xg_pred))))","3d5bc559":"from sklearn.ensemble import VotingRegressor\n\nvote_mod = VotingRegressor([('Ridge', ridge_mod), ('Lasso', lasso_mod), ('Elastic', elastic_cv), \n                            ('XGBRegressor', xgb)])\nvote= vote_mod.fit(x_train, y_train.ravel())\nvote_pred=vote.predict(x_test)\n\nprint('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, vote_pred))))","37e026a4":"# STACKING REGRESSOR:\n# We stack all the previous models, including the votingregressor with XGBoost as the meta regressor:\n\nfrom mlxtend.regressor import StackingRegressor\n\n\nstregr = StackingRegressor(regressors=[elastic_cv,ridge_mod, lasso_mod, vote_mod], \n                           meta_regressor=xgb, use_features_in_secondary=True\n                          )\n\nstack_mod=stregr.fit(x_train, y_train.ravel())\nstacking_pred=stack_mod.predict(x_test)\n\nprint('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, stacking_pred))))","3bff92b8":"# Last thing to do is average our regressors and fit them on the testing dataset\n# Averaging Regressors\nfinal_test=(0.3*vote_pred+0.5*stacking_pred+ 0.2*y_lasso_test)\nprint('Root Mean Square Error test = ' + str(math.sqrt(sklm.mean_squared_error(y_test, final_test))))","ab8f5880":"# 6.6 Fit the model on test data\n# Now, we fit the models on the test data and then submit it to the competition\n\n# We apply np.expm1 to cancel the np.logp1 (we did previously in data processing) and convert the numbers to their original form\n#VotingRegressor to predict the final Test\nvote_test = vote_mod.predict(X_test)\nfinal1=np.expm1(vote_test)\n\n#StackingRegressor to predict the final Test\nstack_test = stregr.predict(X_test)\nfinal2=np.expm1(stack_test)\n\n#LassoRegressor to predict the final Test\nlasso_test = lasso_mod.predict(X_test)\nfinal3=np.expm1(lasso_test)","abfa2624":"#Submission of the results predicted by the average of Voting\/Stacking\/Lasso\nfinal=(0.2*final1+0.6*final2+0.2*final3)\n\nfinal_submission = pd.DataFrame({\n        \"Id\": b[\"Id\"],\n        \"SalePrice\": final\n    })\nfinal_submission.to_csv(\"final_submission.csv\", index=False)\nfinal_submission.head()","77a8ef9d":"The skewness and kurtosis values look fine after log transform. We can now move forward to Machine Learning.\n\nP.S.To get our original SalePrice values back, we will apply np.expm1 at the end of the study to cancel the log1p transformation after training and testing the models.","71992fce":"6.5 ENSEMBLE METHODS:\nVOTING REGRESSOR:\nA voting regressor is an ensemble meta-estimator that fits base regressors each on the whole dataset. It, then, averages the individual predictions to form a final prediction.\nAfter running the regressors, we com","f8b8fdfe":"We first start by trying the very basic regression model: Linear regression.\n\nWe use 5- Fold cross validation for a better error estimate:\n6.2 Linear regression","d988e42a":"Help from this kernel: https:\/\/www.kaggle.com\/amiiiney\/price-prediction-regularization-stacking please upvote this kernel","09f88792":"Next we try Lasso regularization: Similar procedure as ridge regularization but Lasso tends to have a lot of 0 entries in it and just few nonzeros (easy selection). In other words, lasso drops the uninformative features and keeps just the important ones.\nAs with Ridge regularization, we need to find the alpha parameter that penalizes the error","27d9346d":"We check next, the important features that our model used to make predictions\nThe number of uninformative features that were dropped. Lasso give a 0 coefficient to the useless features, we will use the coefficient given to the important feature to plot the graph","ac11234a":"Log transform skewed numeric features:\nWe want our skewness value to be around 0 and kurtosis less than 3. \nFor more information about skewness and kurtosis,I recommend reading this article.\n\nHere are two examples of skewed features: Ground living area and 1st floor SF. We will apply np.log1p to the skewed variables.","29d83535":"6.3 Regularization:\nRidge regression:\nMinimize squared error + a term alpha that penalizes the error\nWe need to find a value of alpha that minimizes the train and test error (avoid overfitting)","7628eca3":"6- Machine Learning:\n6.1 Preprocessing\nWe start machine learning by setting the features and target:\n\nFeatures: x\nTarget: y","7dd5c633":"We use RobustScaler to scale our data because it's powerful against outliers, we already detected some but there must be some other outliers out there, I will try to find them in future versions of the kernel"}}