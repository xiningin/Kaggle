{"cell_type":{"29b503c5":"code","9d4be6f3":"code","bcfcbb77":"code","e89f61c6":"code","8eece8f7":"code","ae81d386":"markdown","d2efc646":"markdown","866c9eb6":"markdown","2f0fcc1a":"markdown","9076c2ee":"markdown"},"source":{"29b503c5":"import numpy  as np\nimport pandas as pd\n\n# To use this experimental feature, we need to explicitly ask for it:\nfrom sklearn.experimental    import enable_hist_gradient_boosting\nfrom sklearn.ensemble        import HistGradientBoostingRegressor\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics         import mean_absolute_error","9d4be6f3":"train  = pd.read_csv('..\/input\/the-volcano-and-the-regularized-greedy-forest\/volcano_train.csv')\ntest   = pd.read_csv('..\/input\/the-volcano-and-the-regularized-greedy-forest\/volcano_test.csv')\nsample = pd.read_csv('..\/input\/predict-volcanic-eruptions-ingv-oe\/sample_submission.csv')\n\nX      = train.drop([\"segment_id\",\"time_to_eruption\"],axis=1).to_numpy()\ny      = (train[\"time_to_eruption\"]).to_numpy().squeeze()\nX_test = test.drop(\"segment_id\",axis=1).to_numpy()","bcfcbb77":"%%time\n\nkf = KFold(n_splits=10, random_state=42, shuffle=True)\n\npredictions_array = []\nCV_score_array    = []\n\nfor train_index, test_index in kf.split(X):\n    \n    X_train, X_valid = X[train_index], X[test_index]\n    y_train, y_valid = y[train_index], y[test_index]\n    \n    regressor =  HistGradientBoostingRegressor()\n    regressor.fit(X_train, y_train)\n    \n    predictions_array.append(regressor.predict(X_test))\n    CV_score_array.append(mean_absolute_error(y_valid,regressor.predict(X_valid)))    \n\npredictions = np.mean(predictions_array,axis=0)","e89f61c6":"print(\"The average CV mean absolute error is %d\" % np.mean(CV_score_array,axis=0))","8eece8f7":"sample.iloc[:,1:] = predictions\nsample.to_csv('submission.csv',index=False)","ae81d386":"read in the datasets","d2efc646":"now write out a `submission.csv` file","866c9eb6":"### See also:\n* [Histogram Gradient Boosting Classifier example](https:\/\/www.kaggle.com\/carlmcbrideellis\/histogram-gradient-boosting-classifier-example) performed on the *Santander Customer Satisfaction* dataset.\n\n## Related reading\n\n* [Aleksei Guryanov \"Histogram-Based Algorithm for Building Gradient Boosting Ensembles of Piecewise Linear Decision Trees\", In: van der Aalst W. et al. (eds) Analysis of Images, Social Networks and Texts. AIST 2019. Lecture Notes in Computer Science, vol 11832. Springer (2019)](https:\/\/link.springer.com\/chapter\/10.1007%2F978-3-030-37334-4_4)","2f0fcc1a":"perform the regression, here with 10-fold cross-validation:","9076c2ee":"## Histogram Gradient Boosting Regression example\n\nThis is a demonstration of the <font color='purple'>(still experimental)<\/font> **histogram-based gradient boosting regression tree estimator** which is now available in scikit-learn as [sklearn.ensemble.HistGradientBoostingRegressor](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.HistGradientBoostingRegressor.html).\n\nFor input we shall be using the data produced by the excellent notebook [\"INGV Volcanic Eruption Prediction - LGBM Baseline\"](https:\/\/www.kaggle.com\/ajcostarino\/ingv-volcanic-eruption-prediction-lgbm-baseline), written by [Adam James](https:\/\/www.kaggle.com\/ajcostarino). The training dataset consists of 4431 rows and 444 columns, and occupies around 23MB.\nFor the estimator I shall simply use the default parameters (see the sklearn page for details)."}}