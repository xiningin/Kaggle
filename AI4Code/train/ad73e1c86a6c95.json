{"cell_type":{"d9845cca":"code","071719ff":"code","805727cf":"code","aafd4b04":"code","dc73079a":"code","c3efca9c":"code","7899b76f":"code","11d9a242":"code","bfb1c7ae":"code","51792b4b":"code","f375181e":"code","1d8388ed":"code","516220a5":"code","eb8c9a38":"code","99dc4c39":"code","17dd3f98":"code","7a2915c9":"code","d0f73136":"code","60ecc564":"code","2ed68ed9":"code","3c84d953":"code","5674968b":"code","c5873764":"code","8d51d417":"code","789fcd26":"code","4fb5c8e4":"code","ea202aa4":"code","24327c4c":"code","f0d43e11":"code","030e8572":"code","76461dca":"code","455398d1":"code","c5a7d17a":"code","8036a4cb":"code","09827744":"code","85402723":"code","bf2481d1":"code","6a37680b":"code","d4394679":"code","4ba6203d":"code","a8dad20c":"code","752746cc":"code","8fae297c":"code","00ab103a":"code","6585b7ff":"code","50020288":"code","dff64ac6":"code","5ebd02be":"code","0680b996":"code","09d0bc0b":"code","3743020c":"code","e8b57b13":"code","459fb611":"code","346f62e3":"code","09308512":"code","1185ed39":"code","d44c7e29":"code","5a4ac7a9":"code","0f385cdb":"code","46bf35e4":"code","57060596":"code","12dcfa9c":"code","c083760e":"code","89e12c13":"markdown","831aa7ed":"markdown","84f05283":"markdown","6970cecf":"markdown","7d152e05":"markdown","3794eb52":"markdown","a5406b28":"markdown","5c913a40":"markdown","0ddda73e":"markdown","f2dd90ab":"markdown","f1c44ddf":"markdown","89baf17f":"markdown","3ad6013a":"markdown","eafbb3d3":"markdown","b6752747":"markdown","5fd2504b":"markdown","95b76857":"markdown","d47c187b":"markdown","65c828aa":"markdown","46362f56":"markdown","553c5018":"markdown","08c62832":"markdown","7a35b377":"markdown","a37c8b1a":"markdown","53fbf7f6":"markdown","d74756f3":"markdown","90d30c35":"markdown","b51d355b":"markdown","054d6ef1":"markdown","8df8e7ff":"markdown","3458f551":"markdown","85e65412":"markdown","2ab5ceb4":"markdown","b127abd3":"markdown","d870079a":"markdown","2bc03d62":"markdown","29967e91":"markdown","c669db28":"markdown","2b4c0653":"markdown","39d7bf85":"markdown","259f0736":"markdown","85d9ee8f":"markdown","151ae764":"markdown","f67cc7ee":"markdown","b9cebf31":"markdown","5f1e278f":"markdown","4bd30d71":"markdown","a1c1b5fc":"markdown","d654af38":"markdown","64cb30db":"markdown","e9390938":"markdown","12f43e37":"markdown","f422cef9":"markdown","940e763d":"markdown","9920bc5a":"markdown","d4f2a054":"markdown","3b86ffd1":"markdown","a6ea1b92":"markdown","af5775a3":"markdown","7c531437":"markdown","640fb266":"markdown","cd0b0e7a":"markdown","bee7343d":"markdown","7a3105a4":"markdown","68c23af3":"markdown","5d01fcf5":"markdown","650b1856":"markdown","f0baa8b3":"markdown","f391e1e5":"markdown","8851a538":"markdown","1308cddd":"markdown","f92821fe":"markdown","08671a37":"markdown","fbaad57d":"markdown","d0a62187":"markdown","3301df2b":"markdown","75f16f41":"markdown","8752d21f":"markdown","eae427f8":"markdown","c6958035":"markdown","55a013d2":"markdown","c32f8f4c":"markdown","c37bc55f":"markdown","f867fe87":"markdown","75637523":"markdown","78343a94":"markdown","f31bb430":"markdown","e04dc2e0":"markdown","20cc746b":"markdown","9a933f93":"markdown","851bf96a":"markdown","23448b86":"markdown","d5966ee7":"markdown","4711b20d":"markdown","321c7565":"markdown","987698a3":"markdown","68bcdf6b":"markdown","efbb801c":"markdown","69e422ba":"markdown","4ed85e4c":"markdown","06e1fbe3":"markdown","92f829fe":"markdown","4fc333cb":"markdown","333e7663":"markdown","63352127":"markdown","31c74314":"markdown"},"source":{"d9845cca":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.gridspec as gs\nfrom scipy import stats\nfrom sklearn.preprocessing import LabelEncoder  \nimport warnings\nwarnings.filterwarnings('ignore')","071719ff":"import math\nimport scipy.stats as ss\ndef convert(data, to, copy=True):\n    converted = None\n    if to == 'array':\n        if isinstance(data, np.ndarray):\n            converted = data.copy() if copy else data\n        elif isinstance(data, pd.Series):\n            converted = data.values\n        elif isinstance(data, list):\n            converted = np.array(data)\n        elif isinstance(data, pd.DataFrame):\n            converted = data.as_matrix()\n    elif to == 'list':\n        if isinstance(data, list):\n            converted = data.copy() if copy else data\n        elif isinstance(data, pd.Series):\n            converted = data.values.tolist()\n        elif isinstance(data, np.ndarray):\n            converted = data.tolist()\n    elif to == 'dataframe':\n        if isinstance(data, pd.DataFrame):\n            converted = data.copy(deep=True) if copy else data\n        elif isinstance(data, np.ndarray):\n            converted = pd.DataFrame(data)\n    else:\n        raise ValueError(\"Unknown data conversion: {}\".format(to))\n    if converted is None:\n        raise TypeError(\n            'cannot handle data conversion of type: {} to {}'.format(\n                type(data), to))\n    else:\n        return converted\ndef cramers_v(x, y):\n   \n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n    rcorr = r-((r-1)**2)\/(n-1)\n    kcorr = k-((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr\/min((kcorr-1),(rcorr-1)))\n\n\n\ndef correlation_ratio(categories,\n                      measurements):\n    \n    categories = convert(categories, 'array')\n    measurements = convert(measurements, 'array')\n    fcat, _ = pd.factorize(categories)\n    cat_num = np.max(fcat) + 1\n    y_avg_array = np.zeros(cat_num)\n    n_array = np.zeros(cat_num)\n    for i in range(0, cat_num):\n        cat_measures = measurements[np.argwhere(fcat == i).flatten()]\n        n_array[i] = len(cat_measures)\n        y_avg_array[i] = np.average(cat_measures)\n    y_total_avg = np.sum(np.multiply(y_avg_array, n_array)) \/ np.sum(n_array)\n    numerator = np.sum(\n        np.multiply(n_array, np.power(np.subtract(y_avg_array, y_total_avg),\n                                      2)))\n    denominator = np.sum(np.power(np.subtract(measurements, y_total_avg), 2))\n    if numerator == 0:\n        eta = 0.0\n    else:\n        eta = np.sqrt(numerator \/ denominator)\n    return eta","805727cf":"def with_hue(plot, feature, Number_of_categories, hue_categories):\n    a = [p.get_height() for p in plot.patches]\n    patch = [p for p in plot.patches]\n    for i in range(Number_of_categories):\n        total = feature.value_counts().values[i]\n        for j in range(hue_categories):\n            percentage = '{:.1f}%'.format(100 * a[(j*Number_of_categories + i)]\/total)\n            x = patch[(j*Number_of_categories + i)].get_x() + patch[(j*Number_of_categories + i)].get_width() \/ 2 - 0.15\n            y = patch[(j*Number_of_categories + i)].get_y() + patch[(j*Number_of_categories + i)].get_height() \n            plot.annotate(percentage, (x, y), size = 12)\n   # plt.show()\n\ndef without_hue(plot, feature):\n    total = len(feature)\n    for p in plot.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_height()\/total)\n        x = p.get_x() + p.get_width() \/ 2 - 0.2\n        y = p.get_y() + p.get_height()\n        plot.annotate(percentage, (x, y), size = 12)\n    #plt.show()","aafd4b04":"df = pd.read_csv(\"..\/input\/student-alcohol-consumption\/student-por.csv\")\ndf.head()","dc73079a":"df.info()","c3efca9c":"df.shape","7899b76f":"df.isna().sum()","11d9a242":"cont = [\"age\", \"absences\", \"G1\", \"G2\", \"G3\"]\nordin = [\"Medu\", \"Fedu\", \"traveltime\", \"studytime\", \"failures\", \"famrel\", \"freetime\", \"goout\", \"Dalc\", \"Walc\", \"health\"]\nnom = [\"Mjob\", \"Fjob\", \"reason\", \"guardian\"]\nbinary = [\"school\", \"sex\", \"address\", \"famsize\", \"Pstatus\", \"schoolsup\", \"famsup\", \"paid\", \"activities\", \"nursery\", \"higher\", \"internet\", \"romantic\"]","bfb1c7ae":"fig = plt.figure(figsize = (15,14))\ng = gs.GridSpec(nrows = 3, ncols = 3, figure = fig)\ni = 0\n\nax1 = plt.subplot(g[0,0])\nax1 = sns.countplot(df[cont[0]])\n\nax2 = plt.subplot(g[0,1])\nax2 = sns.countplot(df[cont[1]])\n\nrg = list(range(0,20))\nfor feature in cont[2:]:\n    ax = plt.subplot(g[1,i])\n    ax = sns.countplot(df[feature], order = rg)\n    i = i+1\ni = 0\nfor feature in cont[2:]:\n    ax = plt.subplot(g[2,i])\n    ax = sns.boxplot(data = df, x = df[feature], order = rg)\n    i = i+1","51792b4b":"df[df[\"G3\"] == 0]","f375181e":"fig = plt.figure(figsize = (15,18))\ng = gs.GridSpec(nrows = 4, ncols = 3, figure = fig)\ni = 0\nj = 0\nfor feature in ordin:\n    if j == 3:\n        i = i + 1\n        j = 0\n    ax1 = plt.subplot(g[i,j])\n    ax1 = sns.countplot(df[feature])\n    without_hue(ax1,df[feature])\n    j = j + 1","1d8388ed":"fig = plt.figure(figsize = (12,8))\ng = gs.GridSpec(nrows = 2, ncols = 2, figure = fig)\n\nax1 = plt.subplot(g[0,0])\nax1 = sns.countplot(df[nom[0]])\nwithout_hue(ax1,df[nom[0]])\n\nax2 = plt.subplot(g[0,1])\nax2 = sns.countplot(df[nom[1]], order = [\"at_home\", \"health\", \"other\", \"services\", \"teacher\"])\nwithout_hue(ax2,df[nom[1]])\n\nax3 = plt.subplot(g[1,0])\nax3 = sns.countplot(df[nom[2]])\nwithout_hue(ax3,df[nom[2]])\n\nax4 = plt.subplot(g[1,1])\nax4 = sns.countplot(df[nom[3]])\nwithout_hue(ax4,df[nom[3]])\n","516220a5":"fig = plt.figure(figsize = (17,15))\ng = gs.GridSpec(nrows = 4, ncols = 4, figure = fig)\ni = 0\nj = 0\nfor feature in binary:\n    if j == 4:\n        i = i + 1\n        j = 0\n    ax1 = plt.subplot(g[i,j])\n    ax1 = sns.countplot(df[feature])\n    without_hue(ax1,df[feature])\n    j = j + 1","eb8c9a38":"fig = plt.figure(figsize = (15,10))\nsns.heatmap(df[cont].corr(method = \"pearson\"), cmap=\"YlGnBu\", annot = True)\nplt.title(\"Continue - Continue\", size = 20)","99dc4c39":"fig = plt.figure(figsize = (15,5))\nplt.suptitle(\"Correlation of grades G1 and G2 with G3\", size = 20)\ng = gs.GridSpec(nrows = 1, ncols = 2, figure = fig)\n\nax1 = plt.subplot(g[0,0])\nax1 = sns.scatterplot(data = df, x = df[\"G1\"], y = df[\"G3\"])\n\nax2 = plt.subplot(g[0,1])\nax2 = sns.scatterplot(data = df, x = df[\"G2\"], y = df[\"G3\"])\n","17dd3f98":"fig = plt.figure(figsize = (15,10))\nsns.heatmap(df[ordin].corr(method = \"spearman\"), cmap=\"YlGnBu\", annot = True)\nplt.title(\"Ordinal - Ordinal\", size = 20)","7a2915c9":"fig = plt.figure(figsize = (15,10))\ng = gs.GridSpec(ncols = 2, nrows = 1, figure = fig)\n\nax1 = plt.subplot(g[0,0])\nax1 = sns.heatmap(df[ordin + cont].corr(method = \"kendall\")[cont].drop(cont), cmap=\"YlGnBu\", annot = True)\nplt.title(\"Kendall\")\n\nax2 = plt.subplot(g[0,1])\nax2 = sns.heatmap(df[ordin + cont].corr(method = \"spearman\")[cont].drop(cont), cmap=\"YlGnBu\", annot = True)\nplt.title(\"Spearman\")\n\n\nplt.suptitle(\"Continue - Ordinal\", size = 20)","d0f73136":"caratt = [\"Medu\", \"studytime\", \"failures\"]\nj = 0\nfig = plt.figure(figsize = (15,6))\ng = gs.GridSpec(nrows = 1, ncols = 3, figure = fig)\n\nfor feat in caratt:\n    ax = plt.subplot(g[0,j])\n    ax = sns.boxplot(data = df, x = df[feat], y = df[\"G3\"])\n    ax = sns.swarmplot(data = df, x = df[feat], y = df[\"G3\"], color = \".25\")\n    j = j+1","60ecc564":"fig = plt.figure(figsize = (15,8))\ng = gs.GridSpec(nrows = 1, ncols = 2, figure = fig)\n\nax = plt.subplot(g[0,0])\nax = sns.boxplot(data = df, x = df[\"Medu\"], y = df[\"G3\"])\nplt.title(\"Medu - G3\")\n\nax2 = plt.subplot(g[0,1])\nax2 = sns.boxplot(data = df, x = df[\"Fedu\"], y = df[\"G3\"])\nplt.title(\"Fedu - G3\")","2ed68ed9":"fig = plt.figure(figsize = (15,8))\ng = gs.GridSpec(nrows = 1, ncols = 2, figure = fig)\n\nax = plt.subplot(g[0,0])\nax = sns.boxplot(data = df, x = df[\"Dalc\"], y = df[\"G3\"])\nplt.title(\"Dalc - G3\")\n\nax2 = plt.subplot(g[0,1])\nax2 = sns.boxplot(data = df, x = df[\"Walc\"], y = df[\"G3\"])\nplt.title(\"Walc - G3\")","3c84d953":"le = LabelEncoder()\nfor feature in binary:\n    df[feature] = le.fit_transform(df[feature])\n    print(feature, le.classes_)","5674968b":"scores = []\nli = []\n\nprint(\"point-biserial correlation > |0.15|\")\nfor feat_bin in binary:\n    for feat_cont in cont:\n        corr = stats.pointbiserialr(df[feat_bin], df[feat_cont])[0]\n        scores.append(corr)\n        if corr >0.15 or corr < -0.15:\n            print(feat_bin, feat_cont, corr)\n            string1 = feat_bin + \"-\" + feat_cont\n            tu = (string1, corr)\n            li.append(tu)","c5873764":"li.sort(key=lambda tup: tup[1], reverse = True) \nname_li = [i[0] for i in li]\ncorr_li = [i[1] for i in li]\nfig = plt.figure(figsize = (15,8))\nsns.barplot(x = corr_li, y = name_li)","8d51d417":"scores = np.array(scores).reshape(len(binary), len(cont))\nfig = plt.figure(figsize = (15,8))\nsns.heatmap(scores, xticklabels = cont, yticklabels = binary, annot = True, cmap=\"YlGnBu\")\nplt.title(\"Continue - Binary\", size = 20)","789fcd26":"caratt = [\"school\", \"higher\",\"address\", \"internet\"]\nfig = plt.figure(figsize = (18,5))\ng = gs.GridSpec(nrows = 1, ncols = 4, figure = fig)\nplt.suptitle(\"G3 - Binary\", size = 20)\n\nj = 0\nfor feat in caratt:\n    ax = plt.subplot(g[0,j])\n    ax = sns.boxplot(data = df, x = df[feat], y = df[\"G3\"])\n    j = j+1","4fb5c8e4":"fig = plt.figure(figsize = (15,8))\nsns.boxplot(data = df, x = df[\"higher\"], y = df[\"G3\"], hue = df[\"school\"])","ea202aa4":"scores = []\nprint(\"Eta-square > |0.15|\")\nfor feature in nom:\n        for var in cont:\n            corr = correlation_ratio(df[feature], df[var])\n            scores.append(corr)\n            if corr >0.15:\n                print(feature, var, corr)","24327c4c":"scores = np.array(scores).reshape(len(nom), len(cont))\nfig = plt.figure(figsize = (15,8))\nsns.heatmap(scores, xticklabels = cont, yticklabels = nom, annot = True, cmap=\"YlGnBu\")\n\nplt.title(\"Continue - Nominal\", size = 20)","f0d43e11":"caratt = [\"Mjob\", \"Fjob\", \"reason\"]\nfig = plt.figure(figsize = (18,5))\ng = gs.GridSpec(nrows = 1, ncols = 3, figure = fig)\nplt.suptitle(\"G3 - Nominal\")\nj = 0\nfor feat in caratt:\n    ax = plt.subplot(g[0,j])\n    ax = sns.boxplot(data = df, x = df[feat], y = df[\"G3\"])\n    j = j+1","030e8572":"nom_bin = nom + binary\nli = []\nscores = []\nprint(\"Cramers_v > |0.20|\")\nfor feat in nom_bin:\n    for feat2 in nom_bin:\n        pres = True\n        corr = cramers_v(df[feat], df[feat2])\n        scores.append(corr)\n        tu = (feat, feat2)\n        if tu in li:\n            pres = False\n        else:\n            tu2 = (feat2, feat)\n            li.append(tu2)\n        if corr > 0.20:\n            if feat != feat2:\n                if pres == True:\n                    print(feat, feat2, corr)\n","76461dca":"scores = np.array(scores).reshape(len(nom_bin), len(nom_bin))\nfig = plt.figure(figsize = (15,15))\nsns.heatmap(scores, xticklabels = nom_bin, yticklabels = nom_bin, annot = True, cmap=\"YlGnBu\")\nplt.title(\"Nominal - Nominal\", size = 20)","455398d1":"li_temp = []\nli_temp2 = []\nscores = []\nprint(\"Spearman rank-order correlation > |0.20|\")\nfor feat_bin in binary:\n    for feat_cont in ordin:\n        corr = stats.spearmanr(df[feat_bin], df[feat_cont])[0]\n        scores.append(corr)\n        if corr >0.20 or corr < -0.20:\n            print(feat_bin, feat_cont, corr)\n            li_temp.append(feat_bin)\n            li_temp2.append(feat_cont)","c5a7d17a":"scores = np.array(scores).reshape(len(binary), len(ordin))\nfig = plt.figure(figsize = (15,15))\nsns.heatmap(scores, xticklabels = ordin, yticklabels = binary, annot = True, cmap=\"YlGnBu\")","8036a4cb":"scores = []\nprint(\"Eta square > 0.20\")\nfor var in ordin:\n        for feature in nom:\n            corr = correlation_ratio(df[feature], df[var])\n            scores.append(corr)\n            if corr >0.20:\n                print(feature, var, corr)\n","09827744":"scores = np.array(scores).reshape(len(ordin), len(nom))\nfig = plt.figure(figsize = (15,15))\nsns.heatmap(scores, xticklabels = nom, yticklabels = ordin, annot = True, cmap=\"YlGnBu\")\nplt.title(\"Nominal - Ordinal\", size = 20)","85402723":"data = pd.get_dummies(df)\nnom_dumm = list(data.iloc[:,29:46].columns)\n\nfor feat_bin in nom_dumm:\n    for feat_cont in cont:\n        corr = stats.pointbiserialr(data[feat_bin], data[feat_cont])[0]\n        if corr >0.15 or corr < -0.15:\n            print(feat_bin, feat_cont, corr)\n","bf2481d1":"labels = data[\"G3\"]\ndata.drop([\"G1\", \"G2\", \"G3\", \"failures\", \"absences\"], inplace= True, axis = 1)\nfeatures = data\n\nfeature_list = list(features.columns)\nfeatures = np.array(features)\nlabels = np.array(labels)\n\n","6a37680b":"from sklearn.model_selection import train_test_split\nfeatures_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size = 0.3, random_state = 42)\n\n\ndiz_performance_train = {}\ndiz_performance_test = {}","d4394679":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nfeatures_train = scaler.fit_transform(features_train)\nfeatures_test = scaler.transform(features_test)","4ba6203d":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error","a8dad20c":"def get_metrics(features, labels, clf):\n    pred = clf.predict(features)\n    R2 = r2_score(labels, pred)\n    MSE = mean_squared_error(labels, pred)\n    MAE = mean_absolute_error(labels, pred)\n    r2_adj = 1 - (1-R2)*(len(labels)-1)\/(len(labels)-features.shape[1]-1)\n    if r2_adj < 0:\n        r2_adj = 0\n    return (R2, r2_adj, MSE, MAE)\n\ndef metrics_print(metrics):\n    \n    print(\"R square = \", round(metrics[0], 3))\n    print(\"R Square adj = \", round(metrics[1],3))\n    print(\"MSE = \", round(metrics[2],3))\n    print(\"MAE = \", round(metrics[3],3))","752746cc":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVR\nsvr = SVR()\nparam = [{\"kernel\": [\"linear\"],\"C\": [0.001,0.01, 0.1, 1,10,100, 1000], \"epsilon\": [0.0, 0.1, 0.01, 0.02]},\n        {\"kernel\": [\"rbf\"], \"C\":[0.001, 0.01, 0.1, 1, 10, 100, 1000], \"gamma\": [0.001, 0.01, 0.1, 1, 10, 100, 1000], \"epsilon\": [0.0, 0.1, 0.01, 0.001]}]\nsvr_best = GridSearchCV(svr, param, verbose = 1, n_jobs = -1, return_train_score = True, scoring= 'neg_mean_absolute_error')\nsvr_best.fit(features_train, labels_train)","8fae297c":"svr_best.best_params_","00ab103a":"print(\"Training set\")\ndiz_performance_train[\"SVR\"] = get_metrics(features_train, labels_train, svr_best)\nmetrics_print(diz_performance_train[\"SVR\"])\n\nprint(\"\\nTest set\")\ndiz_performance_test[\"SVR\"] = get_metrics(features_test, labels_test, svr_best)\nmetrics_print(diz_performance_test[\"SVR\"])","6585b7ff":"from sklearn.tree import DecisionTreeRegressor\n\nmax_features = [int(x) for x in np.arange(1, len(feature_list) + 1)]\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\nmin_samples_split =[0.2, 0.4, 0.6, 0.8, 1.0]\nmin_samples_leaf = [1, 2, 4, 10, 20, 40]\n\ndt = DecisionTreeRegressor(random_state = 0)\nparam = {\"max_features\": max_features,\n        \"max_depth\": max_depth,\n        \"min_samples_split\": min_samples_split,\n        \"min_samples_leaf\": min_samples_leaf}\n\ndt_best = GridSearchCV(dt, param, verbose = 1, n_jobs = -1)\ndt_best.fit(features_train, labels_train)","50020288":"print(\"Training set\")\ndiz_performance_train[\"decision_tree\"] = get_metrics(features_train, labels_train, dt_best)\nmetrics_print(diz_performance_train[\"decision_tree\"])\n\nprint(\"\\nTest set\")\ndiz_performance_test[\"decision_tree\"] = get_metrics(features_test, labels_test, dt_best)\nmetrics_print(diz_performance_test[\"decision_tree\"])","dff64ac6":"dt_best.best_params_","5ebd02be":"from sklearn.ensemble import RandomForestRegressor\n\nn_estimators = [1800]\nmax_features = [int(x) for x in np.arange(1, len(feature_list) + 1)]\nmin_samples_split = [0.1, 0.2, 0.3, 0.4, 0.5]\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'min_samples_split': min_samples_split}\n\n\nrf = RandomForestRegressor()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=1, random_state=42, n_jobs = -1, scoring= 'neg_mean_absolute_error')\nrf_random.fit(features_train, labels_train)","0680b996":"print(\"Training set\")\ndiz_performance_train[\"random_forest\"] = get_metrics(features_train, labels_train, rf_random)\nmetrics_print(diz_performance_train[\"random_forest\"])\n\nprint(\"\\nTest set\")\ndiz_performance_test[\"random_forest\"] = get_metrics(features_test, labels_test, rf_random)\nmetrics_print(diz_performance_test[\"random_forest\"])","09d0bc0b":"rf_random.best_params_","3743020c":"from sklearn.neighbors import KNeighborsRegressor\n\nleaf_size = list(range(1,50))\nn_neighbors = list(range(1,30))\np=[1,2]\n\nparams = {\"leaf_size\": leaf_size, \"n_neighbors\":n_neighbors, \"p\":p}\nknn = KNeighborsRegressor()\nknn_best = GridSearchCV(knn, params, verbose = 1, n_jobs = -1)\nknn_best.fit(features_train, labels_train)\n","e8b57b13":"print(\"Training set\")\ndiz_performance_train[\"knn\"] = get_metrics(features_train, labels_train, knn_best)\nmetrics_print(diz_performance_train[\"knn\"])\n\nprint(\"\\nTest set\")\ndiz_performance_test[\"knn\"] = get_metrics(features_test, labels_test, knn_best)\nmetrics_print(diz_performance_test[\"knn\"])","459fb611":"knn_best.best_params_","346f62e3":"mse_val_test = [diz_performance_test[i][2] for i in diz_performance_test]\nmae_val_test = [diz_performance_test[i][3] for i in diz_performance_test]\n\nmse_val_train = [diz_performance_train[i][2] for i in diz_performance_train]\nmae_val_train = [diz_performance_train[i][3] for i in diz_performance_train]\n\nmodelli = list(diz_performance_test.keys())\n","09308512":"fig = plt.figure(figsize = (15,10))\ng = gs.GridSpec(nrows = 2, ncols = 2, figure = fig)\n\nax1 = plt.subplot(g[0,0])\nax1 = sns.barplot(x = modelli, y = mse_val_train)\nplt.title(\"MSE - Training set\", size = 20)\n\nax2 = plt.subplot(g[0,1], sharey = ax1)\nax2 = sns.barplot(x = modelli, y = mse_val_test)\nplt.title(\"MSE - Test set\", size = 20)\n\nax3 = plt.subplot(g[1,0])\nax3 = sns.barplot(x = modelli, y = mae_val_train)\nplt.title(\"MAE - Training set\", size = 20)\n\nax4 = plt.subplot(g[1,1], sharey = ax3)\nax4 = sns.barplot(x = modelli, y = mae_val_test)\nplt.title(\"MAE - Test set\", size = 20)\n","1185ed39":"for model in diz_performance_test:\n    print(model)\n    metrics_print(diz_performance_test[model])\n    print(\"\\n\")","d44c7e29":"def f_importances(coef, names):\n    imp = coef\n    imp,names = zip(*sorted(zip(imp,names)))\n    plt.barh(range(len(names)), imp, align='center')\n    plt.yticks(range(len(names)), names)\n","5a4ac7a9":"fig = plt.figure(figsize = (12,15))\nfeatures_names = data.columns\nf_importances(np.array(rf_random.best_estimator_.feature_importances_), features_names)\nplt.title(\"Feature importance - Sci-kit learn\")","0f385cdb":"from sklearn.inspection import permutation_importance\nfrom time import time\nt0 = time()\nresult = permutation_importance(rf_random.best_estimator_, features_train, labels_train, n_repeats=10,\n                                random_state=0)\nprint(\"Computing Time:\", round(time()-t0, 3), \"s\")","46bf35e4":"plt.figure(figsize = (10,15))\nf_importances(result[\"importances_mean\"], feature_list)\nplt.title(\"Permutation Feature Importance\")","57060596":"#fonte: https:\/\/towardsdatascience.com\/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e\nfrom sklearn.base import clone \ndef imp_df(column_names, importances):\n    data = {\n        'Feature': column_names,\n        'Importance': importances,\n    }\n    df = pd.DataFrame(data) \\\n        .set_index('Feature') \\\n        .sort_values('Importance', ascending=False)\n\n    return df\ndef drop_col_feat_imp(model, X_train, y_train, random_state = 42):\n    \n    # clone the model to have the exact same specification as the one initially trained\n    model_clone = clone(model)\n    # set random_state for comparability\n    model_clone.random_state = random_state\n    # training and scoring the benchmark model\n    model_clone.fit(X_train, y_train)\n    benchmark_score = model_clone.score(X_train, y_train)\n    # list for storing feature importances\n    importances = []\n    \n    # iterating over all columns and storing feature importance (difference between benchmark and new model)\n    for col in X_train.columns:\n        model_clone = clone(model)\n        model_clone.random_state = random_state\n        model_clone.fit(X_train.drop(col, axis = 1), y_train)\n        drop_col_score = model_clone.score(X_train.drop(col, axis = 1), y_train)\n        importances.append(benchmark_score - drop_col_score)\n    \n    importances_df = imp_df(X_train.columns, importances)\n    return importances_df","12dcfa9c":"features_train = pd.DataFrame(features_train, columns = feature_list)\n\nt1 = time()\nimportanze = drop_col_feat_imp(rf_random.best_estimator_, features_train, labels_train)\nprint(\"Computing Time:\", round(time()-t1, 3), \"s\")","c083760e":"plt.figure(figsize = (10,15))\nf_importances(importanze[\"Importance\"], importanze.index)\nplt.title(\"Drop Columns Feature Importance\")","89e12c13":"In order to understand which factors have the most influence on the final G3 grade, we are going to analyze which features have \"the most weight\" in predicting the final grade in the Random Forest Regressor model.\n\nWe will use 3 different approaches:\n- Sci-kit learn feature_importances_.\n- Permutation Feature importance\n- Drop Column Feature importance","831aa7ed":"### Best parameter combination","84f05283":"#### Observations:\n- We note weak associations between votes with \"Mjob\", \"Fjob, and \"reason\". This means that given a vote, we have a weak chance of recognizing the value of \"Mjob\", \"Fjob\", and \"reason\".","6970cecf":"### Distribution binary variables\nLet's analyze how binary variables are distributed","7d152e05":"We note from the 2 graphs that as G1 and G2 increase, G3 increases","3794eb52":"I divide the dataset into features and targets, where the target is \"G3\". ","a5406b28":"The first value of each variable will be indicated with 0, the second with 1","5c913a40":"# 3 - What factors most influence the final grade? <a class=\"anchor\" id=\"fourth-bullet\"><\/a>","0ddda73e":"Using GridSearchCV we try a total of 2842 combinations, and with cv = 5 we make 14210 fits.","f2dd90ab":"We visualize the correlation between the grades using the scatterplot","f1c44ddf":"### Permutation Feature Importance\nWith this approach we compare the R-square of the model with the R-square of a model where the values of a feature are randomly \"shuffled\" between observations (maintaining the feature distribution). \n\nA feature is important if changing its values among observations increases the model error. A feature is not important if to change its values between the observations does not make to increase the error of the model\n\nWe apply Permutation Importance on Training set, since we are interested in the impact of features on the G3 grade.\n\n(Traning set vs Test set? source: https:\/\/christophm.github.io\/interpretable-ml-book\/feature-importance.html)","89baf17f":"### We visualize the strongest correlations","3ad6013a":"### Performance modello Decision Tree","eafbb3d3":"### Relationship between nominal and ordinal variables\nTo measure the relationship we use eta squared\n","b6752747":"### Heatmap Spearman rank-order correlation","5fd2504b":"Compare SVR MAE with other models","95b76857":"### Functions to improve barplot","d47c187b":"## Model Creation - Support Vector Regression\nSupport Vector Regression is a supervised learning model that aims to reduce error by determining the hyperplane and minimizing the interval between predicted and observed values.","65c828aa":"### Relationship between ordinal variables\nWe analyze the relationships between ordinal qualitative variables. As a measure I use Spearman rank correlation coefficient","46362f56":"### How \"higher\" influences final grade by school.","553c5018":"## Model Creation - Random Forest Regressor\nhe Random Forest algorithm is a supervised learning algorithm.\n\nIt represents a type of ensemble model, which uses bagging as the ensemble method and decision tree as the individual model.\n\nThis means that a random forest combines many decision trees into one model. Individually, the predictions made by the decision trees may not be accurate, but combined together, the predictions will on average be closer to the result.\n\nIn Random Forests, the model bias is equal to the bias of the individual decision tree(which has high variance). By creating a forest of decision trees and combining them together, the variance of the final model can be greatly reduced compared to that of the single decision tree.","08c62832":"The dataset contains 649 observations and 33 variables","7a35b377":"The 7features with a relevant importance according to the Drop Column approach are, in decreasing order:\n- higher\n- school\n- schoolsup\n- Dalc\n- Fedu\n- reason_other\n- reason_reputation\n\nWith this approach the most important feature is \"higher\". Then follow the same features found in the first 2 approaches, but in this case we have reason_other and reason_reputation instead of age and Walc or Medu.\n\n\n\nWe also note features with negative importance, of which the most relevant are:\n- sex\n- Fjob_health\n- famsup\n\nThis means that removing these 3 features increases the performance of the model.","a37c8b1a":"## Table of Contents:\n* [Dataset Features Description](#first-bullet)\n* [Exploratory Data Analysis](#second-bullet)\n* [Final Grade G3 Prediction](#third-bullet)\n* [What Factors Most Influence Final Grade G3?](#fourth-bullet)","53fbf7f6":"#### Osservazioni:\n- Notiamo come la mediana di G3 aumenta nel caso di correlazione positiva con Medu e studytime. Lo vediamo nei primi 2 grafici\n- La mediana di G3 nel caso di correlazione negativa con failures diminuisce","d74756f3":"### Drop Column feature importances\nWith this approach we go to measure the importance of a feature by comparing R-square on training set of the model with all features against a model without the feature we are interested in. \n\nA feature is important if removing it from the dataset increases the model error. A feature is not important if to remove it from the dataset does not increase the error of the model\n\nAlso in this case we apply this approach on training set.\n\nThe Drop Column approach is much more accurate than the previous ones. Its disadvantage is that it is slower to calculate all the importances features.","90d30c35":"## Prepare data\nOur goal is to train the model using features that describe the personal and social situation. For this reason we remove from the features all variables related to the student's career, which are:\n- G1\n- G2\n- failures\n- absences","b51d355b":"### Heatmap Cramer's V","054d6ef1":"## Model Creation - KNeighbors Regressor\nThe k-nearest neighbors (k-NN) is an algorithm used in pattern recognition for classifying objects based on the features of objects close to the one under consideration. In both cases, the input is the k nearest training examples in the feature space","8df8e7ff":"With this approach, the 7 most important features are:\n- school\n- higher\n- Dalc\n- schoolsup\n- Fedu\n- Walc \n- Medu\n\nWe notice that we find 6 of the 7 features found in the first approach. In this case, however, instead of age we have Medu among the most important features.\n\nThe disadvantage of this approach is that it overestimates the importance of features that are highly correlated with the target. School, higher and Dalc are features that have a moderate correlation with G3. We compare these results with the last approach.","3458f551":"These are linear relationships, so non-linear relationships are not captured by this test.\n\n#### Observations:\n- We observe a strong positive correlation between the 3 grades G1, G2, G3. This means that as G1 increases, G2 and G3 also increase.\n- We can see a weak positive correlation between age and absences. This means that as age increases, absences increase weakly.\n- There is also a weak negative correlation between age,absences with respect to grades G1, G2, G3. This means that as age increases, or absences, grades decrease.\n\n","85e65412":"### Feature Scaling\nWe note that the min max values of the variables are very different from each other. Since we are going to create an SVR model, we apply the MinMaxScaler normalization to our observations","2ab5ceb4":"In cases where the correlation with G3 in absolute value(\"higher\" and \"school\") is greater, the difference in distribution of G3 for each value of the binary variable increases.","b127abd3":"### Performance model Random Forest","d870079a":"#### Comments:\n- We note a strong association between \"Mjob\" and \"Medu\", and \"Fjob\" and \"Fedu\". Thus knowing the level of parental education makes it possible to recognize parental work with moderate probability.\n- \"guardian\" has a weak association with \"failures\".","2bc03d62":"### SVR performance results ","29967e91":"### Evaluation Metrics\nTo evaluate the performance of the model, I use 4 evaluation metrics:\n- R-square\/Adjusted R-square, to measure how well the variability of the target can be explained by the model\n- Mean Square Error(MSE), to measure how good the model is. MSE measures how much the model predictions deviate from the target. Since it squares these differences, MSE penalizes errors a lot.\n- Mean Absolute Error(MAE), to measure the goodness of the model. Unlike MSE, which penalizes large errors, MAE handles errors fairly. Thus it is more robust to outliers unlike MSE.\n\nR-square has values from 0 to 1. We will need it to explain the variability of G3 in percentage. The higher R-square the better the performance of the moedllo. The problem with R-square is that it increases as the number of features increases, so it may not be completely reliable. To avoid this, we also use Adjusted R-square which does not depend on the number of features.\n\nMSE and MAE we will use them to compare the various models we are going to use. The lower the error, the higher the goodness of the model.\n\nMAE also has the same unit of measurement as the target.","c669db28":"Observations:\n- we can see the difference in distribution of G3 based on the values of the nominal variables.\n- Students with mothers working in \"health\" and \"teacher\" tend to have higher grades\n- Students with father working in \"teacher\" tend to have higher grades\n- Students who chose the school for its reputations tend to have higher grades.","2b4c0653":"### Heatmap eta squared","39d7bf85":"#### Final Comments:\nIt is very interesting to note what factors most influence students' final grade. \n\nThe school one chooses is the factor that affects the grade the most.\n\nThe choice to continue one's studies, the amount of alcohol consumed on weekdays, the father's level of education, hours of study, and age all affect the student's final grade","259f0736":"### Hyperparameter selection\nPer questo modello andremo a validare 3 iperparametri:\n- leaf_size: grandezza della foglia passato all'algoritmo BallTree o KdTree\n- n_neighbors: numero di vicini da considerare \n- p: parametro per metrica di Minkoswki. Se p = 1, allora si usa manhattan distance, se p = 2 si utilizza euclidean distance\n\nPer trovare la combinazione ottima di iperparametri, andremo a utilizzare GridSearchCV","85d9ee8f":"### Function to display feature importances","151ae764":"### Relationship between binary and ordinal variables\nTo measure the relationship, I use Spearman rank-order correlation.","f67cc7ee":"### Heatmap eta-squared","b9cebf31":"I divide the dataset into 2, training set and test set.","5f1e278f":"### Train Test split","4bd30d71":"### Summary Model Performance on Test Set","a1c1b5fc":"There is no missing data in the dataset. To analyze how they handled any missing data, we are going to analyze the distributions of the variables.","d654af38":"### Model Creation - Decision Tree Regressor\nDecision Tree is an algorithm of supervised learning, where every inner node represents a variable, an arc toward a child node represents a possible value for that property and a leaf the predicted value for the target variable from the values of the other properties, that in the tree is represented from the path from the root node to the leaf node. ","64cb30db":"#### Observations:\nSince these are non-ordinal nominal variables, we cannot say that a nominal variable is positively or negatively correlated with a continuous variable. However, from these data we can see that:\n- Students in the \"GP\" school tend to have higher grades than students in the other school. \n- Students who wish to continue their studies tend to have a lower age.\n- Students who want to continue their studies tend to have higher grades.\n\n","e9390938":"### Functions to visualize the performance of the model","12f43e37":"### Performance model KNN","f422cef9":"### Relationship between nominal variables\nWe analyze relationships between nominal variables. To measure the relationship, we use Cramer's V.","940e763d":"# 0 - Dataset Features Description <a class=\"anchor\" id=\"first-bullet\"><\/a>\nAttributes for both student-mat.csv (Math course) and student-por.csv (Portuguese language course) datasets:\n\nschool - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)\n\nsex - student's sex (binary: 'F' - female or 'M' - male)\n\nage - student's age (numeric: from 15 to 22)\n\naddress - student's home address type (binary: 'U' - urban or 'R' - rural)\n\nfamsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)\n\nPstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)\n\nMedu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 \u2013 5th to 9th grade, 3 \u2013 secondary education or 4 \u2013 higher education)\n\nFedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 \u2013 5th to 9th grade, 3 \u2013 secondary education or 4 \u2013 higher education)\n\nMjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n\nFjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n\nreason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')\n\nguardian - student's guardian (nominal: 'mother', 'father' or 'other')\n\ntraveltime - home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n\nstudytime - weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n\nfailures - number of past class failures (numeric: n if 1<=n<3, else 4)\n\nschoolsup - extra educational support (binary: yes or no)\n\nfamsup - family educational support (binary: yes or no)\n\npaid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n\nactivities - extra-curricular activities (binary: yes or no)\n\nnursery - attended nursery school (binary: yes or no)\n\nhigher - wants to take higher education (binary: yes or no)\n\ninternet - Internet access at home (binary: yes or no)\n\nromantic - with a romantic relationship (binary: yes or no)\n\nfamrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n\nfreetime - free time after school (numeric: from 1 - very low to 5 - very high)\n\ngoout - going out with friends (numeric: from 1 - very low to 5 - very high)\n\nDalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n\nWalc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n\nhealth - current health status (numeric: from 1 - very bad to 5 - very good)\n\nabsences - number of school absences (numeric: from 0 to 93)\n\nThese grades are related with the course subject, Math or Portuguese:\n\nG1 - first period grade (numeric: from 0 to 20)\n\nG2 - second period grade (numeric: from 0 to 20)\n\nG3 - final grade (numeric: from 0 to 20, output target)","9920bc5a":"# 2 - Final grade prediction G3 | ML <a class=\"anchor\" id=\"third-bullet\"><\/a>","d4f2a054":"These values measure the level of associations between our qualitative variables. They do not tell us much and this information is not useful for our purpose.","3b86ffd1":"### Best parameters","a6ea1b92":"### Relationship between continuous and nominal variables\nWe analyze relationships between conitnue variables and nominal variables with more than 2 unique values. To measure the relationship I use eta-squared.","af5775a3":"We visualize the 4 correlations with G3.","7c531437":"Observations:\n- we note that both MAE and MSE increases on Test set\n- SVR and Random Forest got lower MAE and MSE on both Training set and Test set","640fb266":"### How Medu, studytime and failures are associated with G3\nWe will now display the variables with correlation > 0.2 or < -0.2","cd0b0e7a":"Observations:\n- Medu and Fedu have a balanced distribution, with the exception of parents with no education. In fact, they make up only % of all parents.\n- More than half of the students take less than 15 minutes to get to school, 33% take 15 to 30 minutes, and the rest take more than 30 minutes\n- Nearly half of students(47%) study 2 to 5 hours per week, 37% less than 2 hours per week, the rest more than 5 hours per week\n- Most students (85%) have never failed a course. The maximum number of failures in this group of students is 3(2.2%) \n- Nearly half of the students(49%) are comfortable with the family, 28% are very comfortable with the family, 16% fairly comfortable while the rest of the students are not\n- freetime and goout have a normal distribution.\n- Fortunately, weekday alcohol consumption is minimal. In fact about 70% of the students do not consume, or consume very little alcohol on weekdays\n- on weekends the consumption of alcohol increases, but the group of students who consume or consume little alcohol remains dominant\n- almost 40% of students are in good health\n","bee7343d":"### How Medu and Fedu affect final grades","7a3105a4":"# What Factors Most Influence Final Grade?\n## Dataset Student-Por\nThis dataset contains family, social, health, and school information from a group of students at a secondary school. This includes end-of-year grades and weekday and weekend alcohol consumption.\n\nthe 3 goals of this project are:\n- analyze the dataset and the relationships between its variables\n- to train and compare models to predict a student's final grade\n- to understand which factors have the greatest influence on a student's final grade","68c23af3":"We see how grades increase as the level of parental education increases","5d01fcf5":"#### Observations\nPositive correlations:\n- \"Medu\" and \"Fedu\" have a strong positive correlation(0.65). We can say that therefore the parents tend to have the same level of education\n- \"Dalc\" and \"Walc\" have a strong positive correlation(0.61). This means that high weekday alcohol consumption leads to high weekend alcohol consumption\n- \"Walc\" is moderately correlated with \"goout\"(0.37). We can say that people who go out with friends tend to drink alcohol.\n- \"goout\" and \"freetime\" are moderately correlated(0.35). So the frequency with which a student goes out with friends depends on the amount of free time\n\nNegative correlations:\n- \"Walc\" and \"study time\" have a moderate negative correlation(-0.22). So those who spend more time studying drink less alcohol on weekends\n- \"Medu\" and \"Fedu\" have a moderate negative correlation with \"traveltime\". If the parental pair has a high level of education, the student is located closer to school\n- \"Medu\" and \"Fedu\" have a moderate negative correlation with \"failures\". If the student's parents have a high level of education, the student tends to fail less in their course.\n\n","650b1856":"### Import libraries\n","f0baa8b3":"### Load Dataset","f391e1e5":"### Sci-kit learn feature_imporances_\nWe use feature_importances from Scikit-learn at first.\n\nIn decision trees, each node is a condition on how to split the values in a feature, so that similar values of the dependent variable end up in the same set after splitting. The condition is based on impurity, which in the case of regression problems is its variance. So when training a tree we can calculate how much each feature contributes to decreasing the weighted impurity. \n\nfeature_importances_ in Scikit-Learn is based on this logic, but in the case of Random Forest, we're talking about the average impurity decrease over the trees.","8851a538":"#### Comments:\n- The correlation of \"school\" with \"Medu\" and \"Fedu\" is moderate. \"MS\" students tend to have parents with lower levels of education\n- Students who go to \"MS\" school tend to have a longer commute to school\n- Males tend to study fewer hours per week than females\n- \"sex\" has a moderate positive correlation with Dalc and Walc. We can say that male students tend to consume more alcohol than females\n- \"address\" and \"traveltime\" have a moderate negative correlation. In fact we can say that those from urban areas tend to have a shorter trip to school.\n- \"higher\" has a moderate correlation with \"Medu\", \"studytime\" and \"failures. From these we can tell that students who plan to continue their studies tend to have a mother with a high level of education, spend more hours studying, and have fewer failures.\n- We note that students who have a mother with a high level of education tend to have internet\n\n","1308cddd":"We visualize the associations of \"Mjob\", \"Fjob\" and \"reason\" with G3.","f92821fe":"### We visualize the error of the 4 models\nLet's compare the error committed between the models. Since we are using nonlinear models, we do not take into account R-square and Adjusted R-square.","08671a37":"## Relationships between variables\nNow we are going to analyze whether there are any relationships between the variables. We will analyze them according to their type so that we can use the correct measure of association.","fbaad57d":"## Conclusions\n1 - We analyzed the relationships between the various types of variables, using the correct coefficient to measure the associations between them. We also gave importance to the relationships of the variables with G3, visualizing also through graphs these relationships. \n\n2 - We trained 4 different models: SVR, Random Forest, KNN, Decision Tree. Using the correct metrics we analyzed the performance of the models, improved the performance by optimizing their hyperparameters. The models that gave the best performance are Random Forest and SVR.\n\n3 - We took the Random Forest model and analyzed the impact of the features on the final G3 grade. Using 3 different approaches, we can say that the features that affect the final grade the most are: school, higher, Dalc, schoolsup, Fedu, Walc and age.","d0a62187":"### Best combination of hyperparameters","3301df2b":"### Relations between continuous and ordinal variables\nWe now analyze the relationships between quantitative variables and ordinal qualitative variables. To measure the relationships, I use Kendall's coefficient of rank correlation and Spearman's rank correlation coefficient","75f16f41":"### Dataset structure","8752d21f":"### Model comparison\nNow that we have finished training our 4 models, we can say that on test set:\n- Random forest has lower MAE than the other models\n- Decision Tree is the model that showed the worst perfomance, with an MAE = 2.305\n\nWe used MAE to compare the different models, and from the best to the worst, the ranking of the models is:\n- Random Forest\n- SVR\n- KNN\n- Decision Tree\n\nWe can conclude by saying that the best models in terms of performance are Random Forest and SVR.","eae427f8":"### Convert the values of binary nominal variables to 0 and 1 ","c6958035":"### Info Dataset","55a013d2":"### Hyperparameter Optimization\nWe use GridSearchCV to optimize the hyperparameters of the SVR model\nWe are going to optimize the hyperparameters based on the kernel we use. \n\nKernels are mathematical functions. Their purpose is to take data as input and transform it into the required form if it is not possible to determine a linearly separable hyperplane.\n\nWith kernel=\"linear\" we are going to optimize:\n- C: determines how much to penalize each misclassified point\n- epsilon: margin of tolerance where the error is not penalized.\n\nkernel = \"rbf\" we optimize:\n- C\n- gamma: controls the distance of the influence of a single training point\n- epsilon\n\nNote on C:\n\nIf C is small, the penalty for misclassified points is low, so a decision boundary with a large margin is chosen at the expense of more misclassification.\n\nIf C is large, SVM tries to minimize the number of misclassified examples due to the high penalty resulting in a decision boundary with a smaller margin. The penalty is not the same for all misclassified examples. It is directly proportional to the distance from the decision boundary.","c32f8f4c":"### Relationship between continuous variables and binary nominal variables\nWe analyze the relationships between quantitative variables and binary nominal variables. To measure the relationship I use point-biserial correlation coefficient. ","c37bc55f":"### Heatmap point-biserial correlation coefficient","f867fe87":"### Correlation between continuous variables\nWe begin by analyzing the relationships between the continuous variables. To measure their correlation, we will use Pearson's correlation coefficient.","75637523":"Comments:\n- Mother's and father's work prevails \"other\"\n- 44% of students chose the school because of the course of study, others because it was close to home (23%), because of the school's reputation(22%), and a minority for other reasons (11%)\n- Most students(70%) are followed by their mother, 23.6% by their father, and 6.3% other","78343a94":"### Best parameter combination","f31bb430":"### How alcohol consumption affects final grades","e04dc2e0":"Observations:\n- More females than males are present\n- all variables are skewed toward one value over the other, except for the \"activities\" variable","20cc746b":"### Features and Labels,  convert Data in Arrays","9a933f93":"#### Observations\nPositive correlations:\n- \"Medu\" and Fedu\" have a weak correlation with G1, G2, and G3 grades. This tells us that as the level of parental education increases, the student's grades increase\n- \"studytime\" has a weak positive correlation with grades G1,G2 and G3. This correlation is fairly \"obvious\", and tells us that as the hours of study per week increase, the student's grades increase.\n- \"Failures\" is moderately correlated with \"age\". We can say that older students tend to have more failures than younger students.\n\nNegative correlations:\n- \"failures\" has a moderate negative correlation with G1, G2, and G3 grades. This is also a fairly \"obvious\" correlation in that a student with high grades tends to have fewer failures.\n- \"Dalc\", \"Walc\", and \"traveltime\" have a weak negative correlation with grades G1,G2,G3. \n\n","851bf96a":"### Hyperparameter optimization\nAlso for this model we will use GridSearchCV to optimize the hyperparameters.\n\nWe are going to optimize 4 hyperparameters:\n- max_features: number of features to consider\n- max_depth: maximum depth of the trees\n- min_samples_split: The minimum number of observations required to split an internal node\n- min_samples_leaf: The minimum number of observations required to be in a leaf node","23448b86":"### Useful Functions","d5966ee7":"We can see that in both cases as alcohol consumption increases, grades tend to decrease","4711b20d":"# 1 - Exploratory Data Analysis <a class=\"anchor\" id=\"second-bullet\"><\/a>\n","321c7565":"### Distribuzione variabili nominali \nLet's analyze how nominal variables are distributed","987698a3":"### Hyperparameter selection\nFor this model, we will use RandomizedGridSearchCV instead of GridSearchCV to optimize the hyperparameters. The difference between the 2 approaches is that in GridSearch we train the model with all possible combinations among the grid of hyperparameters, while in RandomizedGridSearchCV the model selects combinations randomly.\n\nWe are going to optimize 3 hyperparameters:\n- n_estimators: number of trees in the forest\n- max_features: number of features to consider\n- min_samples_split: The minimum number of observations required to split an internal node","68bcdf6b":"From the chart we notice that the top 7 features are:\n- school\n- higher\n- Dalc\n- schoolsup\n- age\n- Walc\n- Fedu\n\nThe disadvantage of this method is that it has the tendency to overestimate the importance of continuous and ordinal variables with high cardinality. \n\nWe note, however, that the first two most important features with this approach(school and higher) are binary nominal variables. We will compare these importance with the other approaches.","efbb801c":"### Distribution of continuous variables\nWe begin by analyzing the distribution of continuous variables","69e422ba":"These are the 33 students with G3 = 0. We note that the G1 and G2 grade point averages of these students are less than or equal to 10, so it may be that they did not reach enough to take the last exam, automatically taking G3 = 0.","4ed85e4c":"#### GridSearchCV\nGrid search is a technique for hyperparameter optimization that can facilitate model creation and evaluate a model for any combination of parameters.\n\nIn our case, GridSearchCV chooses the combination of hyperparameters that gives us the lowest absolute error.","06e1fbe3":"#### Comments:\n- If the mother stays home, G1 and G2 grades tend to decrease slightly\n- Students who chose the school because of its reputation have slightly higher grades\n- Students who are older tend not to have father or mother as guardians.\n","92f829fe":"- Students are mostly ages <= to 19\n- A large majority of students have never had any absences. However, we find exceptions after 10 absences, with a minority reaching as many as 32 absences\n- Grades have a normal distribution, although in G2 and G3 we notice a small group of students who took 0","4fc333cb":"We divide the dataset by variable type:\n- continuous \n- ordinal \n- nominal\n- binary  ","333e7663":"### Distribution ordinal variables\nLet's now analyze how the ordinal variables are distributed","63352127":"### Missing data","31c74314":"#### Relationship between nominal variables (transformed with one hot encoding) and continuous variables"}}