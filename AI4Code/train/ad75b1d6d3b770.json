{"cell_type":{"25678a7f":"code","02738826":"code","ebe469e7":"code","d9aaf60b":"code","e0fd4ce3":"code","14bafd0d":"code","cb8639e2":"code","9c91530a":"code","7ebf06e6":"code","c530126f":"code","e34e2d60":"code","a1a638bc":"code","d8bca13d":"code","b534e09b":"code","98bc8ae6":"code","2e49428b":"code","f6381ad6":"code","1d9093c5":"code","98c7585e":"code","a4fb8644":"code","5c16dc67":"code","59d3eb6c":"code","b9557395":"code","85bdf507":"code","876990c7":"code","44a82ad5":"code","5749a08f":"code","93bc29b6":"code","a54a9dc2":"code","dfc4a866":"code","4b01eb3d":"code","c7923695":"code","348724de":"code","8f6778df":"code","cfdda55f":"code","0b99b11d":"code","52b3477d":"code","299bd78c":"code","90f729a4":"code","40a62c05":"code","4a261fe8":"code","b9011acd":"code","36418415":"code","10cad62b":"code","613e4e1e":"code","493346bd":"code","481e796e":"code","e5c20733":"code","eb7237a9":"code","b75552cb":"code","26cdc2dc":"code","c066a14a":"code","2c29bfa7":"code","5c4bb6b2":"code","7f5515bc":"code","278778f7":"code","3b018c54":"code","9d8145f8":"code","caa733d7":"markdown","4a4be638":"markdown","cd10ed4e":"markdown","b8b63a71":"markdown","f4411281":"markdown","84d03e55":"markdown","869c6f7c":"markdown","d3e6689b":"markdown","ec430d3b":"markdown","be1f33ea":"markdown","2c524007":"markdown","6b2e5278":"markdown","adb585fc":"markdown","0ac502d9":"markdown","fb8591e5":"markdown","6a82f13a":"markdown","bc7f20a0":"markdown","cffeedc8":"markdown","545915b4":"markdown","183bc6d8":"markdown","53a57ec8":"markdown","894c8a18":"markdown","f6e799c5":"markdown","020db27f":"markdown","b8a23909":"markdown","c19d7243":"markdown","179bb326":"markdown","111b10fd":"markdown","59765c03":"markdown","d6235857":"markdown","3f4ac388":"markdown","47939202":"markdown","5d089825":"markdown","f4baed91":"markdown","6cbbb06c":"markdown","5e01cd21":"markdown","3643cc28":"markdown","a0fcda6d":"markdown","c11ad37f":"markdown","4c8eb9ce":"markdown","8b4008e7":"markdown","cabc0ca3":"markdown","b739aa79":"markdown","8c21565c":"markdown","5da56928":"markdown","25c5a376":"markdown","21ca5722":"markdown","1af18d03":"markdown","0d9eb532":"markdown","94a1539b":"markdown","128f9ac5":"markdown"},"source":{"25678a7f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","02738826":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport re\n\nimport warnings\nwarnings.filterwarnings('ignore')","ebe469e7":"from sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier","d9aaf60b":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\nprint(train_df.shape)\ntrain_df.head()","e0fd4ce3":"test_df = pd.read_csv('..\/input\/titanic\/test.csv')\nprint(test_df.shape)\ntest_df.head()","14bafd0d":"train_df.info()","cb8639e2":"train_df.describe()","9c91530a":"sns.heatmap(train_df.corr(), annot = True)","7ebf06e6":"sns.set_style('darkgrid')\nfig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(20, 6))\nsns.barplot(x = 'SibSp', y = 'Survived', data = train_df, ax = ax1, palette='coolwarm')\nsns.barplot(x = 'Parch', y = 'Survived', data = train_df, ax = ax2, palette = 'magma')\nsns.barplot(x = 'Embarked', y = 'Survived', data = train_df, ax = ax3)","c530126f":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\nfemales = train_df[train_df['Sex'] == 'female']\nmales = train_df[train_df['Sex'] == 'male']\n\nax = sns.distplot(females[females['Survived'] == 1].Age, bins=30, label='Survived', ax=axes[0])\nax = sns.distplot(females[females['Survived'] == 0].Age, bins=30, label='Not Survived', ax=axes[0])\nax.legend()\nax.set_title('Female')\nax = sns.distplot(males[males['Survived'] == 1].Age, bins=30, label='Survived', ax=axes[1])\nax = sns.distplot(males[males['Survived'] == 0].Age, bins=30, label='Not Survived', ax=axes[1], )\nax.legend()\nax.set_title('Male')","e34e2d60":"df = [train_df, test_df]","a1a638bc":"for data in df:\n    data['Title'] = data['Name'].str.extract(r', (\\w+)\\.', expand=False)\npd.crosstab(train_df['Title'], train_df['Sex']).transpose()","d8bca13d":"for data in df:\n    data['Title'] = data['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    data['Title'] = data['Title'].replace('Mlle', 'Miss')\n    data['Title'] = data['Title'].replace('Ms', 'Miss')\n    data['Title'] = data['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title']).mean()\n\nlabels = {'Mr':1, 'Mrs':2, 'Master':3, 'Miss':4, 'Rare':5}\ntest_df.replace({'Title':labels}, inplace = True)\ntrain_df.replace({'Title':labels}, inplace = True)\ntrain_df['Title'] = train_df['Title'].fillna(0)\ntrain_df['Title'] = train_df['Title'].astype(int)                     # this is performed beacuse it was giving float values of title","b534e09b":"pd.DataFrame({'Train':train_df.isnull().sum(), 'Test':test_df.isnull().sum()}).transpose()","98bc8ae6":"print('Missing Values in Age column: ',177\/len(train_df['Age'])*100)\nprint('Missing Values in Cabin column: ',687\/len(train_df['Cabin'])*100)\nprint('Missing Values in Embarked column: ',2\/len(train_df['Embarked'])*100)","2e49428b":"fig, (ax1, ax2) = plt.subplots(ncols = 2, figsize = (15,5))\nsns.heatmap(train_df.isnull(), cmap = 'coolwarm', ax = ax1)\nsns.heatmap(test_df.isnull(), cmap = 'mako_r', ax = ax2)","f6381ad6":"train_df[\"Age\"] = train_df[\"Age\"].fillna(-0.5)\ntest_df[\"Age\"] = test_df[\"Age\"].fillna(-0.5)\nbins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ntrain_df['AgeGroup'] = pd.cut(train_df[\"Age\"], bins, labels = labels)\ntest_df['AgeGroup'] = pd.cut(test_df[\"Age\"], bins, labels = labels)\n","1d9093c5":"mr_age = train_df[train_df[\"Title\"] == 1][\"AgeGroup\"].mode() #Young Adult\nmiss_age = train_df[train_df[\"Title\"] == 2][\"AgeGroup\"].mode() #Student\nmrs_age = train_df[train_df[\"Title\"] == 3][\"AgeGroup\"].mode() #Adult\nmaster_age = train_df[train_df[\"Title\"] == 4][\"AgeGroup\"].mode() #Baby\nrare_age = train_df[train_df[\"Title\"] == 5][\"AgeGroup\"].mode() #Adult\n\nage_title_mapping = {1: \"Young Adult\", 2: \"Student\", 3: \"Adult\", 4: \"Baby\", 5: \"Adult\"}\n\nfor x in range(len(train_df[\"AgeGroup\"])):\n    if train_df[\"AgeGroup\"][x] == \"Unknown\":\n        train_df[\"AgeGroup\"][x] = age_title_mapping[train_df[\"Title\"][x]]\n        \nfor x in range(len(test_df[\"AgeGroup\"])):\n    if test_df[\"AgeGroup\"][x] == \"Unknown\":\n        test_df[\"AgeGroup\"][x] = age_title_mapping[test_df[\"Title\"][x]]","98c7585e":"df_m = train_df[train_df['Survived'] == 0]\ndf_f = train_df[train_df['Survived'] == 1]\ndf_m = df_m['AgeGroup'].value_counts()\ndf_f = df_f['AgeGroup'].value_counts()\n\ntrace1 = go.Bar(x = df_m.index[::-1], y = df_m.values[::-1], name = 'Not Survived', marker = dict(color = 'dodgerblue'))\ntrace2 = go.Bar(x = df_f.index[::-1], y = df_f.values[::-1], name = 'Survived', marker = dict(color = 'deeppink'))\ndata = [trace1, trace2]\nlayout = go.Layout(height = 400, width = 500, title='Age Distribution')\nfig = go.Figure(data = data, layout= layout)\npy.iplot(fig)","a4fb8644":"age_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\ntrain_df['AgeGroup'] = train_df['AgeGroup'].map(age_mapping).astype(int)\ntest_df['AgeGroup'] = test_df['AgeGroup'].map(age_mapping).astype(int)","5c16dc67":"df_m = train_df[train_df['Sex'] == 'male']\ndf_f = train_df[train_df['Sex'] == 'female']\ndf_m = df_m['Embarked'].value_counts()\ndf_f = df_f['Embarked'].value_counts()\n\ntrace1 = go.Bar(x = df_m.index[::-1], y = df_m.values[::-1], name = 'Male', marker = dict(color = 'indigo'))\ntrace2 = go.Bar(x = df_f.index[::-1], y = df_f.values[::-1], name = 'Female', marker = dict(color = 'green'))\ndata = [trace1, trace2]\nlayout = go.Layout(height = 400, width = 500, title='Embarked Distribution with Sex')\nfig = go.Figure(data = data, layout= layout)\npy.iplot(fig)","59d3eb6c":"df_m = train_df[train_df['Survived'] == 0]\ndf_f = train_df[train_df['Survived'] == 1]\ndf_m = df_m['Embarked'].value_counts()\ndf_f = df_f['Embarked'].value_counts()\n\ntrace1 = go.Bar(x = df_m.index[::-1], y = df_m.values[::-1], name = 'Male', marker = dict(color = 'burlywood'))\ntrace2 = go.Bar(x = df_f.index[::-1], y = df_f.values[::-1], name = 'Female', marker = dict(color = 'cadetblue'))\ndata = [trace1, trace2]\nlayout = go.Layout(height = 400, width = 500, title='Embarked Distribution with Survived')\nfig = go.Figure(data = data, layout= layout)\npy.iplot(fig)","b9557395":"train_df['Embarked'].fillna('S', inplace = True)\n\nlabel = {'S':1, 'C':2, 'Q':3}\ntrain_df.replace({'Embarked':label}, inplace = True)\ntest_df.replace({'Embarked':label}, inplace = True)","85bdf507":"train_df['Cabin'] = train_df['Cabin'].fillna('X')\ntest_df['Cabin']=test_df['Cabin'].fillna('X')","876990c7":"for data in df:\n    data['Cabin'] = data['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    \ncategory = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6, 'G':7, 'X':8, 'T':9}\nfor data in df:\n    data['Cabin'] = data['Cabin'].map(category)","44a82ad5":"df_m = train_df[train_df['Survived'] == 0]\ndf_f = train_df[train_df['Survived'] == 1]\ndf_m = df_m['Cabin'].value_counts()\ndf_f = df_f['Cabin'].value_counts()\n\ntrace1 = go.Bar(x = df_m.index[::-1], y = df_m.values[::-1], name = 'Not Survived', marker = dict(color = 'chartreuse'))\ntrace2 = go.Bar(x = df_f.index[::-1], y = df_f.values[::-1], name = 'Survived', marker = dict(color = 'darkred'))\ndata = [trace1, trace2]\nlayout = go.Layout(height = 400, width = 500, title='Cabin Distribution')\nfig = go.Figure(data = data, layout= layout)\npy.iplot(fig)","5749a08f":"test_df['Fare'].fillna(test_df['Fare'].median(), inplace = True)\n\ntrain_df['Fare'] = pd.qcut(train_df['Fare'], 4, labels = [1, 2, 3, 4])\ntest_df['Fare'] = pd.qcut(test_df['Fare'], 4, labels = [1, 2, 3, 4])","93bc29b6":"df_m = train_df[train_df['Survived'] == 0]\ndf_f = train_df[train_df['Survived'] == 1]\ndf_m = df_m['Fare'].value_counts()\ndf_f = df_f['Fare'].value_counts()\n\ntrace1 = go.Bar(x = df_m.index[::-1], y = df_m.values[::-1], name = 'Not Survived', marker = dict(color = 'coral'))\ntrace2 = go.Bar(x = df_f.index[::-1], y = df_f.values[::-1], name = 'Survived', marker = dict(color = 'teal'))\ndata = [trace1, trace2]\nlayout = go.Layout(height = 400, width = 500, title='Fare Distribution')\nfig = go.Figure(data = data, layout= layout)\npy.iplot(fig)","a54a9dc2":"#if we check the data info then fare feature is a category not int, so to convert we are performing this: \ntrain_df['Fare'] = pd.to_numeric(train_df['Fare'])","dfc4a866":"df_m = train_df[train_df['Sex'] == 'male']\ndf_f = train_df[train_df['Sex'] == 'female']\ndf_m = df_m['Survived'].value_counts()\ndf_f = df_f['Survived'].value_counts()\n\ntrace1 = go.Bar(x = df_m.index[::-1], y = df_m.values[::-1], name = 'Male', marker = dict(color = 'lightseagreen'))\ntrace2 = go.Bar(x = df_f.index[::-1], y = df_f.values[::-1], name = 'Female', marker = dict(color = 'crimson'))\ndata = [trace1, trace2]\nlayout = go.Layout(height = 400, width = 500, title='Survival Distribution')\nfig = go.Figure(data = data, layout= layout)\npy.iplot(fig)","4b01eb3d":"label = {'male':1, 'female':0}\ntrain_df.replace({'Sex':label}, inplace = True)\ntest_df.replace({'Sex':label}, inplace = True)","c7923695":"df_m = train_df[train_df['Survived'] == 0]\ndf_f = train_df[train_df['Survived'] == 1]\ndf_m = df_m['Pclass'].value_counts()\ndf_f = df_f['Pclass'].value_counts()\n\ntrace1 = go.Bar(x = df_m.index[::-1], y = df_m.values[::-1], name = 'Not Survived', marker = dict(color = 'firebrick'))\ntrace2 = go.Bar(x = df_f.index[::-1], y = df_f.values[::-1], name = 'Survived', marker = dict(color = 'gold'))\ndata = [trace1, trace2]\nlayout = go.Layout(height = 400, width = 500, title='Pclass Distribution', )\nfig = go.Figure(data = data, layout= layout)\npy.iplot(fig)","348724de":"for data in df:\n    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","8f6778df":"for data in df:\n    data['IsAlone'] = 0\n    data.loc[data['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","cfdda55f":"df_m = train_df[train_df['Survived'] == 0]\ndf_f = train_df[train_df['Survived'] == 1]\ndf_m = df_m['IsAlone'].value_counts()\ndf_f = df_f['IsAlone'].value_counts()\n\ntrace1 = go.Bar(x = df_m.index[::-1], y = df_m.values[::-1], name = 'Not Survived', marker = dict(color = 'seagreen'))\ntrace2 = go.Bar(x = df_f.index[::-1], y = df_f.values[::-1], name = 'Survived', marker = dict(color = 'aqua'))\ndata = [trace1, trace2]\nlayout = go.Layout(height = 400, width = 500, title='IsAlone Distribution', )\nfig = go.Figure(data = data, layout= layout)\npy.iplot(fig)","0b99b11d":"train_df.head(1)","52b3477d":"test_df.head(1)","299bd78c":"train_df.drop(['PassengerId', 'Name', 'Ticket', 'Age', 'SibSp', 'Parch', 'FamilySize'], axis = 1, inplace = True)\ntest_df.drop(['Name', 'Ticket', 'Age', 'SibSp', 'Parch', 'FamilySize'], axis = 1, inplace = True)","90f729a4":"train_df.head(3)","40a62c05":"test_df.head(3)","4a261fe8":"X = train_df.drop('Survived', axis = 1)\ny = train_df['Survived']","b9011acd":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 123)","36418415":"from sklearn.metrics import accuracy_score","10cad62b":"lr = LogisticRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_pred, y_test))\nlr_train_acc = round(lr.score(X_train, y_train) * 100, 2)\nprint('Training Accuracy: ', lr_train_acc)\nlr_test_acc = round(lr.score(X_test, y_test) * 100, 2)\nprint('Testing Accuracy: ', lr_test_acc)","613e4e1e":"error_rate = []\nfor i in range(1,30):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))\n    \nplt.figure(figsize = (8,6))\nplt.plot(range(1,30), error_rate, color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)","493346bd":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_pred, y_test))\nknn_train_acc = round(knn.score(X_train, y_train) * 100, 2)\nprint('Training Accuracy: ', knn_train_acc)\nknn_test_acc = round(knn.score(X_test, y_test) * 100, 2)\nprint('Testing Accuracy: ', knn_test_acc)","481e796e":"# We will use GridSearchCV to find best parameters\nsvc = SVC()\nparam_grid = {'C': [0.01, 0.1, 1 ,10 , 100], 'kernel':['linear', 'rbf'], 'gamma':[0.1, 1, 10, 100]}\ngcv = GridSearchCV(estimator = svc, param_grid = param_grid, cv = 5, n_jobs=-1, refit=True)\ngcv.fit(X_train, y_train)\ngcv.best_params_","e5c20733":"svc = SVC(C = 10, gamma = 0.1, kernel = 'rbf')\nsvc.fit(X_train,y_train)\ny_pred = svc.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_pred, y_test))\nsvc_train_acc = round(svc.score(X_train, y_train) * 100, 2)\nprint('Training Accuracy: ', svc_train_acc)\nsvc_test_acc = round(svc.score(X_test, y_test) * 100, 2)\nprint('Testing Accuracy: ', svc_test_acc)","eb7237a9":"dt = DecisionTreeClassifier(max_depth = 6, min_samples_leaf = 2)\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_pred, y_test))\ndt_train_acc = round(dt.score(X_train, y_train) * 100, 2)\nprint('Training Accuracy: ', dt_train_acc)\ndt_test_acc = round(dt.score(X_test, y_test) * 100, 2)\nprint('Testing Accuracy: ', dt_test_acc)","b75552cb":"rf = RandomForestClassifier()\nparam_grid = {'max_depth': [2, 4, 5, 6, 7, 8], 'criterion':['gini', 'entropy'], 'min_samples_leaf':[1, 2 ,4 ,6], 'max_features':['auto', 'log2'], 'n_estimators':[100,150,200]}\ngcv = GridSearchCV(estimator=rf, param_grid=param_grid, cv = 5, n_jobs = -1)\ngcv.fit(X_train, y_train)\ngcv.best_params_","26cdc2dc":"rf = RandomForestClassifier(max_depth = 8, min_samples_leaf = 6, n_estimators = 150)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_pred, y_test))\nrf_train_acc = round(rf.score(X_train, y_train) * 100, 2)\nprint('Training Accuracy: ', rf_train_acc)\nrf_test_acc = round(rf.score(X_test, y_test) * 100, 2)\nprint('Testing Accuracy: ', rf_test_acc)","c066a14a":"adb = AdaBoostClassifier(rf, n_estimators = 200)\nadb.fit(X_train, y_train)\ny_pred = adb.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_pred, y_test))\nadb_train_acc = round(adb.score(X_train, y_train) * 100, 2)\nprint('Training Accuracy: ', adb_train_acc)\nadb_test_acc = round(adb.score(X_test, y_test) * 100, 2)\nprint('Testing Accuracy: ', adb_test_acc)","2c29bfa7":"gdb = GradientBoostingClassifier()\nparams = {'learning_rate':[0.01,0.1,1,10],'n_estimators':[100,150,200,300],'subsample':[0.6,0.8,1.0],'max_depth':[2,3,4,6],'min_samples_leaf':[1,2,4,6]}\ngcv = GridSearchCV(estimator=gdb, param_grid=params, cv=5, n_jobs=-1)\ngcv.fit(X_train, y_train)\ngcv.best_params_","5c4bb6b2":"gdb = GradientBoostingClassifier(max_depth = 2, n_estimators = 300, subsample = 0.8)\ngdb.fit(X_train, y_train)\ny_pred = gdb.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_pred, y_test))\ngdb_train_acc = round(gdb.score(X_train, y_train) * 100, 2)\nprint('Training Accuracy: ', gdb_train_acc)\ngdb_test_acc = round(gdb.score(X_test, y_test) * 100, 2)\nprint('Testing Accuracy: ', gdb_test_acc)","7f5515bc":"xgbc = XGBClassifier(max_depth = 4)\nxgbc.fit(X_train, y_train)\ny_pred = xgbc.predict(X_test)\n\nprint('Classification Report: \\n', classification_report(y_pred, y_test))\nxgbc_train_acc = round(xgbc.score(X_train, y_train) * 100, 2)\nprint('Training Accuracy: ', xgbc_train_acc)\nxgbc_test_acc = round(xgbc.score(X_test, y_test) * 100, 2)\nprint('Testing Accuracy: ', xgbc_test_acc)","278778f7":"x = ['Logistic Regression', 'KNN', 'SVC', 'Decision Tree','Random Forest','AdaBoost','Gradient Boosting','XGBoost']\ny1 = [lr_train_acc, knn_train_acc, svc_train_acc, dt_train_acc, rf_train_acc, adb_train_acc, gdb_train_acc, xgbc_train_acc]\ny2 = [lr_test_acc, knn_test_acc, svc_test_acc, dt_test_acc, rf_test_acc, adb_test_acc, gdb_test_acc, xgbc_test_acc]\n\ntrace1 = go.Bar(x = x, y = y1, name = 'Training Accuracy', marker = dict(color = 'forestgreen'))\ntrace2 = go.Bar(x = x, y = y2, name = 'Testing Accuracy', marker = dict(color = 'lawngreen'))\ndata = [trace1,trace2]\nlayout = go.Layout(title = 'Accuracy Plot', width = 750)\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","3b018c54":"test_df['Fare'] = pd.to_numeric(test_df['Fare'])","9d8145f8":"test_df['Survived'] = rf.predict(test_df.drop(['PassengerId'], axis = 1))\ntest_df[['PassengerId', 'Survived']].to_csv('MySubmission.csv', index = False)","caa733d7":"[Back to Contents(Click here)](#cont)","4a4be638":"Drop the features PassengerId, Name, Age, SibSp, Parch, FamilySize and Ticket which won't be useful in prediction now.","cd10ed4e":"<a id = \"subsec1\"><\/a>\n**Let's Visualize Data**","b8b63a71":"<a id = \"cont\"><\/a>\n# Contents:","f4411281":"<a id = \"subsec3\"><\/a>\n**Age Feature**","84d03e55":"1. [ Importing libraries and Reading data](#sec1)\n2. [ Feature Engineering and Data Visualization](#sec2)\n    - [Some Visualizations](#subsec1)\n    - [Name Feature](#subsec2)\n    - [Age Feature](#subsec3)\n    - [Embarked Feature](#subsec4)\n    - [Cabin Feature](#subsec5)\n    - [Fare Feature](#subsec6)\n    - [Sex Feature](#subsec7)\n    - [Pclass Feature](#subsec8)\n    - [SibSp and Parch Feature](#subsec9)\n3. [ Train and Predict](#sec3)\n    - [Logistic Regression](#subsec11)\n    - [K Nearest Neighbors](#subsec12)\n    - [Support Vector Machines](#subsec13)\n    - [Decision Tree](#subsec14)\n    - [Random Forest](#subsec15)\n    - [Adaboost](#subsec16)\n    - [Gradient Boosting](#subsec17)\n    - [XGBoost](#subsec18)\n4. [ Accuracy Comparison through Plot](#sec4)","869c6f7c":"Next we'll fill in the missing values in the Age feature. Since a higher percentage of values are missing, it is outlandish to fill every one of them with a similar worth (as we did with Embarked). Rather, how about we attempt to figure out how to predict the missing ages.","d3e6689b":"<a id = \"subsec17\"><\/a>\nGradient Boosting","ec430d3b":"Majority of people who survived embarked from S.","be1f33ea":"# Creating Submission","2c524007":"<a id = \"subsec11\"><\/a>\n**Logistic Regression**","6b2e5278":"[Back to Contents(Click here)](#cont)","adb585fc":"Note that where applicable we perform operations on both training and testing datasets together to stay consistent.","0ac502d9":"[Back to Contents(Click here)](#cont)","fb8591e5":"[Back to Contents(Click here)](#cont)","6a82f13a":"<a id = \"subsec16\"><\/a>\n**AdaBoost**","bc7f20a0":"[Back to Contents(Click here)](#cont)","cffeedc8":"<a id = \"subsec12\"><\/a>\n**K Nearest Neighbors**","545915b4":"[Back to Contents(Click here)](#cont)","183bc6d8":"[Back to Contents(Click here)](#cont)","53a57ec8":"[Back to Contents(Click here)](#cont)","894c8a18":"<a id = \"subsec14\"><\/a>\nDecision Tree","f6e799c5":"[Back to Contents(Click here)](#cont)","020db27f":"<a id = \"subsec7\"><\/a>\n**Sex Feature**","b8a23909":"Most used titles are Mr, Miss, Master ,Mrs. Let's classify features into Mr, Miss, Master, Mrs and Rare.","c19d7243":"People who are alone are more likely to survive.","179bb326":"1. **Numerical Columns: **Age(Continuous), Fare(Continuous), Sibsp(Discrete), Parch(Discrete)\n2. **Categorical Columns: **Survived, Sex, Embarked, Pclass\n3. **Other Columns: **PassengerID, Name, Ticket, Cabin","111b10fd":"<a id  = \"subsec8\"><\/a>\n**Pclass Feature**","59765c03":"**Now our data is ready to train.**","d6235857":"* Age column has ~20% of missing values. Age feature is pretty important to survival, so we should probably attempt to fill these gaps.\n* We will drop Cabin column because it's not possible to fill so many(77%) missing values.\n* Embarked column with just ~0.2% missing values won't causing any issue.","3f4ac388":"<a id = \"sec3\"><\/a>\n# Train and Predict","47939202":"<a id = \"sec2\"><\/a>\n# Feature Engineering And Data Visualization","5d089825":"Correlation in Data","f4baed91":"<a id = \"subsec9\"><\/a>\n**SibSp and Parch Feature**","6cbbb06c":"<a id = \"subsec4\"><\/a>\n**Embarked Feature**","5e01cd21":"<a id = \"subsec15\"><\/a>\n**Random Forest**","3643cc28":"Let's visualize missing values using seaborn.","a0fcda6d":"People in first class are more likely to survive.","c11ad37f":"<a id = \"subsec2\"><\/a>\n**Name Feature**","4c8eb9ce":"<a id = \"sec1\"><\/a>\n# Importing Libraries and Reading Data","8b4008e7":"As indicated by the Kaggle information word reference, both SibSp and Parch identify with going with family. For the wellbeing of simplicity (and to represent conceivable multicollinearity), I'll consolidate the impact of these variables into one categorical predictor: regardless of whether that individual was voyaging alone.","cabc0ca3":"Women are more likely to survive than men.","b739aa79":"# Please upvote if you think it's useful, any suggestions are welcome.","8c21565c":"We will extract first letter of assigned cabin and then map it into a category.","5da56928":"<a id = \"subsec13\"><\/a>\n**Support Vector Machines**","25c5a376":"<a id = \"subsec18\"><\/a>\nXGBoost","21ca5722":"Now we'll extract titles(salutation) from the 'Name' feature","1af18d03":"I think the thought here is that individuals with recorded cabin numbers are of higher financial class, and in this manner bound to survive.","0d9eb532":"<a id = \"subsec6\"><\/a>\n**Fare Feature**","94a1539b":"<a id = \"subsec5\"><\/a>\n**Cabin Feature**","128f9ac5":"<a id = \"sec4\"><\/a>\n# Accuracy Comparison through Plots"}}