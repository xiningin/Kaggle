{"cell_type":{"9491bd93":"code","46cd9272":"code","3787711c":"code","22aa5e59":"code","cabda273":"code","b6aba0b9":"code","feb9af62":"code","4dd6b693":"code","87fb7ad1":"code","d9fbaa1a":"code","3c50c933":"code","629e48bc":"code","6f17040f":"code","bbc0ed52":"code","0b2a1719":"code","815dafd0":"code","42506e5f":"code","480116f7":"code","b5bf5e9b":"code","3171ac20":"code","b1e0a88f":"code","e0fd353d":"code","c14defb9":"code","ba91dc2b":"code","4d1df4f7":"code","e0bd37f5":"code","abdf22bd":"code","f2217d09":"code","eb691974":"code","755045e2":"code","094a093c":"code","782580e2":"code","c62010e8":"code","51c76f65":"code","168c51e0":"code","e6d73551":"code","ab4f4a7e":"code","75d8d1c5":"code","43281348":"code","92092c56":"code","cbde4bdd":"code","3193f042":"code","fd5adab0":"code","50fc03ce":"code","fcd7f822":"code","83639630":"code","3e66fda9":"code","cc3af31f":"markdown","c84d77c7":"markdown","4a2cbe30":"markdown","cec087e0":"markdown","85eb5e0f":"markdown","a8c1e9db":"markdown","c3ccb940":"markdown","ee1fa3b9":"markdown","d1162736":"markdown","5db4a631":"markdown","68e74350":"markdown","33114cb5":"markdown","f7992636":"markdown"},"source":{"9491bd93":"import pandas as pd\nimport pandas_profiling as pp\nimport numpy as np\nimport re\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.impute import KNNImputer\nimport os\n#pip install pandas-profiling","46cd9272":"dataset_path = '\/kaggle\/input\/car-crashes-severity-prediction\/'\ndf = pd.read_csv(os.path.join(dataset_path, 'train.csv'))\nprint(\"The shape of the dataset is {}.\\n\\n\".format(df.shape))\n\ndf.head()","3787711c":"import xml.etree.ElementTree as ET\ndef xml_To_df(xml_path):\n\n    xml_data = open(xml_path, 'r').read()  # Read file\n    root = ET.XML(xml_data)  # Parse XML\n    \n    data = []\n    cols = []\n    for i, child in enumerate(root):\n        data.append([subchild.text for subchild in child])\n        cols.append(child.tag)\n    \n    Holidaydf = pd.DataFrame(data).T  # Write in DF and transpose it\n    Holidaydf.columns = cols  # Update column names\n    Holidaydf=Holidaydf.T\n    Holidaydf=Holidaydf.reset_index()\n    Holidaydf.drop(columns='index',inplace=True)\n    Holidaydf.rename(columns = {0:'justDate',1:'HolidayName'}, inplace = True)\n    Holidaydf= Holidaydf.drop(columns= ['HolidayName'])\n    Holidaydf['justDate']= pd.to_datetime(Holidaydf['justDate'])\n    return Holidaydf","22aa5e59":"# pp.ProfileReport(df)","cabda273":"# df2 = df.drop(columns=['ID','Bump','Roundabout'])","b6aba0b9":"# pp.ProfileReport(df2)","feb9af62":"# categorical_cols= ['Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Stop',\n#                    'Amenity', 'Side']\n# for c in categorical_cols:\n#     print(c, df2[c].value_counts(), \"\\n\")","4dd6b693":"# df3= df2.drop(columns= [\"Give_Way\", \"No_Exit\"])","87fb7ad1":"# df3.head()","d9fbaa1a":"# print('values before: ', df3.Side.unique())\n# # change the side featue into zero for L, one for R\n# df3['Side']= df2['Side'].apply(lambda x: 1 if x== 'R' else -1)\n# print('values after', df3.Side.unique())\n# df3.head()","3c50c933":"# df3['dateyear']= pd.to_datetime(df3['timestamp']).dt.year\n# df3['datemonth']= pd.to_datetime(df3['timestamp']).dt.month\n# df3['dateday']= pd.to_datetime(df3['timestamp']).dt.day\n# df3['datehour']= pd.to_datetime(df3['timestamp']).dt.hour","629e48bc":"# df4= df3.drop(columns= ['timestamp'])\n# df4.head()","6f17040f":"# for col in df4.columns:\n#     print(col+\": \", df4['Severity'].corr(df4[col]))","bbc0ed52":"# df4.head()","0b2a1719":"# pp.ProfileReport(df4)","815dafd0":"# for i in df4.columns:\n#     for j in df4.columns:\n#         print(i+\" and \"+ j, df4[i].corr(df4[j]))\n#     print(\"\\n\")","42506e5f":"# df4.head()","480116f7":"# df4['timestamp']= df['timestamp']\n# df4","b5bf5e9b":"# from geopy.geocoders import Nominatim","3171ac20":"# geolocator = Nominatim(user_agent=\"geoapiExercises\")","b1e0a88f":"# # lat, long= str(df5['Lat'][0]), str(df5['Lng'][0])\n# lat, long","e0fd353d":"# address= geolocator.reverse(lat+\",\"+long).raw['address']\n# zipcode = address.get('postcode')\n# zipcode","c14defb9":"# df100= df5[:5]\n# df100.head()","ba91dc2b":"# def location(df):\n#     return geolocator.reverse(str(df['Lat']) + ',' + str(df['Lng'])).raw['address'].get('postcode')","4d1df4f7":"# df5['location']= df5.apply(location, axis= 1)","e0bd37f5":"# df5['location'].head(50)","abdf22bd":"# df4['timestamp']= pd.to_datetime(df4['timestamp'])\n# df4['dayNum']= df4['timestamp'].dt.dayofweek\n# df4.head()","f2217d09":"# df4['day_off']= df4['dayNum'].apply(lambda x: 1 if (x== 6 or x== 5) else 0)","eb691974":"# df5= df4.drop(columns=['timestamp', 'dayNum'])","755045e2":"# df6.info()","094a093c":"# import xml.etree.ElementTree as ET\n# xml_data = open('holidays.xml', 'r').read()  # Read file\n# root = ET.XML(xml_data)  # Parse XML\n\n# data = []\n# cols = []\n# for i, child in enumerate(root):\n#     data.append([subchild.text for subchild in child])\n#     cols.append(child.tag)\n\n# dfHoliday = pd.DataFrame(data).T  # Write in DF and transpose it\n# dfHoliday.columns = cols  # Update column names\n# dfHoliday=dfHoliday.T\n# dfHoliday=dfHoliday.reset_index()\n# dfHoliday.drop(columns='index',inplace=True)\n# dfHoliday.rename(columns = {0:'justDate',1:'HolidayName'}, inplace = True)\n# dfHoliday","782580e2":"weather_df = pd.read_csv(os.path.join(dataset_path, 'weather-sfcsv.csv'))\nweather_df['timestamp'] = pd.to_datetime(weather_df[['Year','Month','Day','Hour']])\nweather_df.drop(columns=['Year','Month','Day','Hour'],inplace=True)\nweather_df = weather_df.drop_duplicates(subset = ['timestamp'], keep = 'first') \nweather_dfNew= weather_df.drop(columns=['Wind_Chill(F)','Precipitation(in)','Selected'])\nweather_dfNew.sort_values('timestamp',inplace=True)\nimputer = KNNImputer()\nweather_dfNew['Temperature(F)'] = imputer.fit_transform(weather_dfNew[['Temperature(F)']])\nweather_dfNew['Humidity(%)'] = imputer.fit_transform(weather_dfNew[['Humidity(%)']])\nweather_dfNew['Wind_Speed(mph)'] = imputer.fit_transform(weather_dfNew[['Wind_Speed(mph)']])\nweather_dfNew['Visibility(mi)'] = imputer.fit_transform(weather_dfNew[['Visibility(mi)']])\nweather_dfNew.dropna(subset=['Weather_Condition'],inplace=True)\nweather_dfNew.reset_index(drop=True, inplace=True)","c62010e8":"def weather_encoding(df):\n    df6 = df\n    weather ='!'.join(df6['Weather_Condition'].dropna().unique().tolist())\n    weather = np.unique(np.array(re.split(\n        \"!|\\s\/\\s|\\sand\\s|\\swith\\s|Partly\\s|Mostly\\s|Blowing\\s|Freezing\\s\", weather))).tolist()\n    df6['Clear'] = np.where(df6['Weather_Condition'].str.contains('Clear|Fair', case=False, na = False), True, False)\n    df6['Cloud'] = np.where(df6['Weather_Condition'].str.contains('Cloud|Overcast|Partly Cloudy|Mostly Cloudy|Scattered Clouds|Cloudy', case=False, na = False), True, False)\n    df6['Rain'] = np.where(df6['Weather_Condition'].str.contains('Rain|storm|Light Rain|Light Thunderstorms and Rain|Light Drizzle', case=False, na = False), True, False)\n    df6['Heavy_Rain'] = np.where(df6['Weather_Condition'].str.contains('Heavy Rain|Rain Shower|Heavy T-Storm|Heavy Thunderstorms', case=False, na = False), True, False)\n    df6['Snow'] = np.where(df6['Weather_Condition'].str.contains('Snow|Sleet|Ice', case=False, na = False), True, False)\n    df6['Heavy_Snow'] = np.where(df6['Weather_Condition'].str.contains('Heavy Snow|Heavy Sleet|Heavy Ice Pellets|Snow Showers|Squalls', case=False, na = False), True, False)\n    df6['Fog'] = np.where(df6['Weather_Condition'].str.contains('Fog|Patches of Fog|Haze|Shallow Fog|Smoke|Mist', case=False, na = False), True, False)\n    weather = ['Clear','Cloud','Rain','Heavy_Rain','Snow','Heavy_Snow','Fog']\n    for i in weather:\n        df6.loc[df6['Weather_Condition'].isnull(),i] = df6.loc[df6['Weather_Condition'].isnull(),'Weather_Condition']\n        df6[i] = df6[i].astype('bool')\n\n    df6.loc[:,['Weather_Condition'] + weather]\n\n    df6 = df6.drop(['Weather_Condition'], axis=1)\n    def one_column_encoding(row):\n        if row['Fog']:\n            return 6\n        elif row['Heavy_Snow'] == True:\n            return 5\n        elif row['Snow'] == True:\n            return 4\n        elif row['Heavy_Rain'] == True:\n            return 3\n        elif row['Rain'] == True:\n            return 2\n        elif row['Cloud'] == True:\n            return 1\n        else:\n            return 0\n    df6['Condition'] = df6.apply(lambda row: one_column_encoding(row), axis=1)\n#     df6.drop(columns=['Fog','Heavy_Snow','Snow','Heavy_Rain','Rain','Cloud'],inplace=True)\n    df6.drop(columns=['Condition','Heavy_Snow','Snow','Heavy_Rain'],inplace=True)\n    return df6","51c76f65":"def data_prep(file_path, xml_path):\n    df = pd.read_csv(file_path)\n    #droping uninportant functions:\n    df= df.drop(columns=['ID', 'Bump', 'Give_Way', 'No_Exit', 'Roundabout', 'Railway', 'Amenity'])\n    #extracting year, and month\n    df['dateyear']= pd.to_datetime(df['timestamp']).dt.year\n    df['datemonth']= pd.to_datetime(df['timestamp']).dt.month\n#     df['timestamp_w'] = pd.to_datetime(df[\"timestamp\"])\n#     df = df.assign(timestamp_w = df.timestamp_w.dt.floor('H'))\n    df['timestamp']= pd.to_datetime(df['timestamp']).dt.date #elimnating the hour:min:sec part from timestamp\n    df['dayNum']= pd.to_datetime(df['timestamp']).dt.dayofweek #extracting which day of the week was that\n    df['timestamp']= pd.to_datetime(df['timestamp']) #converting timestamp to datetime object\n    #create a label to determine weekend\n    df['day_off']= df['dayNum'].apply(lambda x: 1 if (x== 6 or x== 5) else 0) \n    Holidaydf= xml_To_df(xml_path)\n    \n    #Taking the holidays into consideration in the column of day off:\n    day_off= list(df['day_off'])\n    timestamp= list(df['timestamp'].astype(str))\n    Holiday= list(Holidaydf['justDate'].astype(str))\n    \n    holiday_range= range(len(Holiday))\n    time_range= range(len(timestamp))\n    for i in holiday_range:\n        for j in time_range:\n            if timestamp[j]== Holiday[i]:\n                day_off[j]= 1\n    df['day_off']= pd.Series(day_off)\n    df['timestamp'] = pd.to_datetime(pd.read_csv(file_path)['timestamp'])\n    df = df.assign(timestamp = df.timestamp.dt.floor('H'))\n    df = pd.merge(left=df, right=weather_dfNew, how='left')\n    df['Side']= df['Side'].apply(lambda x: 1 if x== 'R' else -1)\n    df = weather_encoding(df)\n    df= df.drop(columns= ['Crossing', 'Junction', 'Stop', 'Side', 'dayNum', 'timestamp','Humidity(%)'])\n    df.dropna(subset=['Temperature(F)','Wind_Speed(mph)','Visibility(mi)'],inplace=True)\n    return df","168c51e0":"dataset_path = '\/kaggle\/input\/car-crashes-severity-prediction\/'\ntraining_path = os.path.join(dataset_path, 'train.csv')\nxml_path = os.path.join(dataset_path, 'holidays.xml')\ndf = data_prep(training_path,xml_path)","e6d73551":"from sklearn.model_selection import train_test_split\n# df5.drop(columns= ['Lat'], axis= 1)\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Severity']) # Try adding `stratify` here\nprint(df)\nX_train = train_df.drop(columns=['Severity'])\ny_train = train_df['Severity']\n\nX_val = val_df.drop(columns=['Severity'])\ny_val = val_df['Severity']\n","ab4f4a7e":"# X_train.head()","75d8d1c5":"from sklearn.ensemble import RandomForestClassifier\n\n# Create an instance of the classifie\nclassifier = RandomForestClassifier(max_depth= 15, random_state=42)\n\n# Train the classifier\nclassifier = classifier.fit(X_train, y_train)","43281348":"print(\"The accuracy of the classifier on the validation set is \", (classifier.score(X_val, y_val)))","92092c56":"\n# weather_df = pd.read_csv('weather-sfcsv.csv')\n# severity_df = df.drop(df.columns.difference(['Severity','timestamp']), 1)\n# severity_df['timestamp']=pd.to_datetime(df['timestamp'])\n# severity_df = severity_df.assign(timestamp = severity_df.timestamp.dt.round('H'))\n# severity_df\n\n# # severity_df['Month']=pd.DatetimeIndex(df['timestamp']).month\n# # severity_df['Day']=pd.DatetimeIndex(df['timestamp']).day\n# # severity_df['Hour']=pd.DatetimeIndex(df['timestamp']).hour\n# # result = pd.merge(severity_df, weather_df,  how='inner', on=['Year', 'Month', 'Day', 'Hour'])\n# # result\n# # weather_df['Weather_Condition'].value_counts()\n# # weather_df['timestamp'] = weather_df['Year']+\"-\"+weather_df['Month']+\"-\"+weather_df['Day']+\" \"+weather_df['Hour']+\":00:00\"\n# weather_df['timestamp'] = pd.to_datetime(weather_df[['Year','Month','Day','Hour']])\n# weather_df.drop(columns=['Year','Month','Day','Hour'],inplace=True)\n# print(weather_df['timestamp'].value_counts())\n# # print(severity_df['timestamp'].value_counts())\n# weather_df = weather_df.drop_duplicates(subset = ['timestamp'], keep = 'first') \n# print(weather_df['timestamp'].value_counts())\n# severity_df.head()","cbde4bdd":"# df5['timestamp']=pd.to_datetime(df['timestamp'])\n# df5 = df5.assign(timestamp = df5.timestamp.dt.round('H'))\n# df6= df5.merge(weather_df,on='timestamp',how='left')\n# df6.sort_values('timestamp',inplace=True)\n# pp.ProfileReport(df6)\n","3193f042":"# df6.fillna(method='bfill',inplace=True)\n# pp.ProfileReport(df6)","fd5adab0":"dataset_path = '\/kaggle\/input\/car-crashes-severity-prediction\/'\ntest_path = os.path.join(dataset_path, 'test.csv')\nxml_path = os.path.join(dataset_path, 'holidays.xml')\ndf_test = data_prep(test_path,xml_path)","50fc03ce":"X_test = df_test\n# You should update\/remove the next line once you change the features used for training\n# X_test = X_test[['Lat', 'Lng', 'Distance(mi)']]\n\ny_test_predicted = classifier.predict(X_test)\n\nX_test['Severity'] = y_test_predicted\n\nX_test.head()","fcd7f822":"X_test['ID'] = pd.read_csv(test_path)['ID']\nX_test.info()","83639630":"# X_test = test_df.drop(columns=['ID'])\n\n# You should update\/remove the next line once you change the features used for training\n# X_test = X_test[['Lat', 'Lng', 'Distance(mi)']]\n\n# y_test_predicted = classifier.predict(X_test)\n\n# test_df['Severity'] = y_test_predicted\n\n# test_df.head()","3e66fda9":"X_test[['ID', 'Severity']].to_csv('submission.csv', index=False)","cc3af31f":"### Converting the Side column cateogories using into 0s, and 1s: ","c84d77c7":"# Determing days off: ","4a2cbe30":"# ML Model: ","cec087e0":"### Checking correlation of the characteristics of the location and each other ","85eb5e0f":"## Initial profiling of the whole dataset","a8c1e9db":"From the quick investigation above we can see clearly that some of these features are not \nindicative on the sevirity of the accident, so we will remove them.\nThe columns we will be removing are: \"Give_Way\", \"No_Exit\"","c3ccb940":"## Taking Weather into consideration: ","ee1fa3b9":"## Dropping the ID, Bump and Roundabout based on the initial profiling","d1162736":"## Finding the correlation between the sevirity, and the rest of the features: ","5db4a631":"## Checking significange of each of the categorical features in predicting sevirity:","68e74350":"### Trial to find the geo location:","33114cb5":"## Spliting the time stamp data into 4 seprate columns dateyear, datemonth, dateday, datehour: ","f7992636":"## adding an attribute to represent whether the day was a holiday or not:"}}