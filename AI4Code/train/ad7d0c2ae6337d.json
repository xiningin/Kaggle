{"cell_type":{"68113336":"code","c46a4edb":"code","dd7743ea":"code","6dc50bf7":"code","5420c8a0":"code","476498f5":"code","57f286fc":"code","8dcdc1b1":"code","954b1223":"code","03b5427b":"code","310411ee":"code","0a0dd604":"code","5e65e7f6":"code","3cb6585d":"code","6da4ac41":"code","1ed73520":"code","dbaee61a":"code","d4548e57":"code","84928862":"code","9bdb3d26":"code","64a9eb9a":"code","56707906":"code","9e735c2d":"code","db6c04d2":"code","ce444fdb":"code","9a0ca7b4":"code","1eb587af":"code","8e79a942":"code","482de43f":"code","e51d8430":"code","82595a2d":"code","86dd6d20":"code","9aecc1b9":"code","f44ae7d5":"code","f1a2f8c6":"code","689c07c5":"code","0288bcb0":"code","6e6269ef":"code","21ee3ec5":"code","52fee08a":"code","6087d827":"code","abc52930":"code","cf2e29ff":"code","88b97ae9":"code","e5db3571":"code","f1a7a1e7":"code","56c6e5a3":"code","6e3c8199":"code","3eeb57c7":"code","41fe9b45":"code","eab4d9b8":"code","c4b95bb9":"code","7cd21164":"code","b627aa2d":"code","ab3fce1c":"code","98e5e083":"code","7cd73755":"code","b95c64f6":"code","c759f2f1":"code","1a40d9e8":"code","aa2a4b3e":"code","d8c84ac1":"code","8551990d":"code","a030b3df":"code","7be9df0e":"code","ad14d97a":"code","66548aa1":"code","639a2b60":"code","e1c18388":"code","8be41a51":"code","93e452b7":"code","afc6d171":"code","3dc1d63d":"code","8f891c8a":"code","3f0ae6cc":"code","ae8a8b3b":"markdown","3f89cadb":"markdown","3880e416":"markdown","43698ed5":"markdown","65c6ea5b":"markdown","ebfb4774":"markdown"},"source":{"68113336":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c46a4edb":"#for data analysis\nimport random as rnd\n\n\n#for data visulaisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#for machine learning\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report","dd7743ea":"#Loading data\ntrain=pd.read_csv(\"..\/input\/train.csv\")\ntest=pd.read_csv(\"..\/input\/test.csv\")\ncombine=[train,test]\n","6dc50bf7":"train.columns","5420c8a0":"train.head()","476498f5":"ID=test['PassengerId']","57f286fc":"train.info()","8dcdc1b1":"test.info()","954b1223":"\n#There are too many missing values in train['Cabin'],so we may drop that feature\ntrain.drop('Cabin',axis=1,inplace=True)\ntest.drop('Cabin',axis=1,inplace=True)","03b5427b":"train.describe()","310411ee":"train[['Sex','Survived']].groupby(['Sex'],as_index=False).mean().sort_values('Survived',ascending=False)","0a0dd604":"train[['SibSp','Survived']].groupby(['SibSp'],as_index=False).mean().sort_values('Survived',ascending=False)","5e65e7f6":"train[['Parch','Survived']].groupby(['Parch'],as_index=False).mean().sort_values('Survived',ascending=False)","3cb6585d":"train[['Embarked','Survived']].groupby(['Embarked'],as_index=False).mean().sort_values('Survived',ascending=False)","6da4ac41":"#Now let us visualise how survival depends on quantitative variables\n\n#Quatitative variables are Age,Price\ng=sns.FacetGrid(train,col='Survived',size=4)\ng.map(plt.hist,'Age',bins=20,alpha=0.5)","1ed73520":"g=sns.FacetGrid(train,col='Survived',size=4)\ng.map(plt.hist,'Fare',bins=20,alpha=0.5)","dbaee61a":"g=sns.FacetGrid(train,col='Survived',row='Pclass',size=4)\ng.map(plt.hist,'Age',bins=20)","d4548e57":"g=sns.FacetGrid(train,col='Survived',row='Pclass',size=4)\ng.map(plt.hist,'Fare',bins=20)","84928862":"g=sns.FacetGrid(train,col='Survived',row='Sex',size=4)\ng.map(plt.hist,'Embarked')","9bdb3d26":"#Data wrangling","64a9eb9a":"#correcting by dropping features: Here we will drop the featire 'ticket number' as it is of not much use.\ntrain.drop('Ticket',axis=1,inplace=True)\ntest.drop('Ticket',axis=1,inplace=True)\ncombine=[train,test]\ntrain.columns","56707906":"#creating new features\nfor dataset in combine:\n    dataset['Title']=dataset.Name.str.extract('([A-Za-z]+\\.)',expand=False)","9e735c2d":"pd.crosstab(train['Title'],train['Sex'])\ntrain['Title'].value_counts()","db6c04d2":"train.head()","ce444fdb":"train['Title']=train['Title'].replace(['Dr','Rev','Col','Major','Lady','Capt','Sir','Jonkheer','Countess','Don'],'Rare',regex=True)\ntrain['Title']=train['Title'].replace('Ms','Miss',regex=True)\ntrain['Title']=train['Title'].replace('Mlle','Miss',regex=True)\ntrain['Title']=train['Title'].replace('Mme','Mrs',regex=True)\n\ntest['Title']=test['Title'].replace(['Dr','Rev','Col','Major','Lady','Capt','Sir','Jonkheer','Countess','Don'],'Rare',regex=True)\ntest['Title']=test['Title'].replace('Ms','Miss',regex=True)\ntest['Title']=test['Title'].replace('Mlle','Miss',regex=True)\ntest['Title']=test['Title'].replace('Mme','Mrs',regex=True)\n\ntrain['Title'].value_counts()","9a0ca7b4":"train.head(50)","1eb587af":"train[['Title','Survived']].groupby('Title',as_index=False).mean()\ncombine=[train,test]","8e79a942":"title_mapping = {\"Mr.\": 1, \"Miss.\": 2, \"Mrs.\": 3, \"Master.\": 4, \"Rare.\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain.head()","482de43f":"train.head(10)","e51d8430":"test.head()","82595a2d":"train.drop(['PassengerId','Name'],axis=1,inplace=True)","86dd6d20":"test.drop(['PassengerId','Name'],axis=1,inplace=True)","9aecc1b9":"train.head()","f44ae7d5":"test.head()","f1a2f8c6":"gender_mapping={\"female\":1,\"male\":2}\nfor dt in combine:\n   dt['Sex']= dt['Sex'].map(gender_mapping)\n   dt['Sex']=dt['Sex'].fillna(0)","689c07c5":"port_mapping={'S':1,'C':2,'Q':3}\n\nfor dt in combine:\n    dt['Embarked']=dt['Embarked'].map(port_mapping)\n    dt['Embarked']=dt['Embarked'].fillna(0)","0288bcb0":"train.head()","6e6269ef":"test.head()","21ee3ec5":"#filling missing values","52fee08a":"train.info()","6087d827":"grid=sns.FacetGrid(train,row='Pclass',col='Sex')\ngrid.map(plt.hist,'Age',alpha=0.5,bins=20)\ngrid.add_legend\n\n","abc52930":"\n\nage_guess=np.zeros((2,3))\n","cf2e29ff":"for dt in combine:\n    for i in range(1,3):\n        for j in range(1,4):\n            guess=dt[(dt['Sex']==i) & (dt['Pclass']==j)]['Age'].dropna()\n            a=i-1\n            b=j-1\n            age_guess[a,b]=guess.median()\n            \n            \nfor dt in combine:\n    for i in range(1,3):\n        for j in range(1,4):\n            a=i-1\n            b=j-1\n            dt.loc[(dt.Age.isnull())& (dt.Sex==i)&(dt.Pclass==j),'Age']=age_guess[a,b]\n            ","88b97ae9":"train.info()","e5db3571":"train.head()","f1a7a1e7":"test.head()","56c6e5a3":"train['Ageband']=pd.cut(train['Age'],5)\ntrain[['Ageband','Survived']].groupby(['Ageband'],as_index=False).mean().sort_values(by='Ageband',ascending=True)","6e3c8199":"for dt in combine :\n    dt.loc[dt['Age']<=16,'Age']=0\n    dt.loc[(dt['Age']>16)&(dt['Age']<32),'Age']=1\n    dt.loc[(dt['Age']>32)&(dt['Age']<48),'Age']=2\n    dt.loc[(dt['Age']>48)&(dt['Age']<64),'Age']=3\n    dt.loc[(dt['Age']>64),'Age']=4\ntrain.head()    ","3eeb57c7":"train.drop('Ageband',axis=1,inplace=True)\ntrain.head()","41fe9b45":"for dt in combine:\n    dt['Familysize']=dt['SibSp']+dt['Parch']+1\n    \ntrain[['Familysize','Survived']].groupby('Familysize',as_index=False).mean().sort_values('Survived',ascending=False)","eab4d9b8":"# we will check if how were the survival rates among the people whose Familysize is 1\n\nfor dt in combine:\n    dt['Isalone']=0\n    dt.loc[(dt['Familysize']==1),'Isalone']=1\n\ntrain[['Isalone','Survived']].groupby('Isalone',as_index=False).mean()","c4b95bb9":"#we will drop Sibso, Parch,familysize\n\nfor dt in combine:\n    dt.drop(['SibSp','Parch','Familysize'],axis=1,inplace=True)","7cd21164":"train.head()","b627aa2d":"train.info()","ab3fce1c":"test.info()","98e5e083":"test['Fare'].fillna(test['Fare'].dropna().median(),inplace=True)","7cd73755":"test.info()","b95c64f6":"#create a fare band\n\ntrain['fareband']=pd.qcut(train['Fare'],4)","c759f2f1":"train[['fareband','Survived']].groupby('fareband',as_index=False).mean().sort_values('Survived',ascending=True)","1a40d9e8":"for dt in combine:\n    dt.loc[(dt['Fare']<7.91),'Fare']=0\n    dt.loc[(dt['Fare']>=7.91) & (dt['Fare']<14.45),'Fare']=1\n    dt.loc[(dt['Fare']>=14.45) & (dt['Fare']<31),'Fare']=2\n    dt.loc[(dt['Fare']>=31),'Fare']=3","aa2a4b3e":"train.drop('fareband',axis=1,inplace=True)","d8c84ac1":"train.head()","8551990d":"test.head()","a030b3df":"#model predict and solve\n","7be9df0e":"from sklearn.model_selection import train_test_split\n\nXtrain=train.drop('Survived',axis=1)\nytrain=train['Survived']\n","ad14d97a":"#Let us stack models to get better accuracy\n\n#Our base models will depend on how correlated they are we will choose the ones that are uncorrelated.\n#Let us define a class\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import (RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier)\nfrom sklearn.svm import SVC\n\n\nclass base_predictors(object):\n    def __init__(self,clf,params):\n        params['random_state']=0\n        self.clf=clf(**params)\n    def train(self,X_train,y_train):\n        self.clf.fit(X_train,y_train)\n    def predict(self,X_test):\n        return self.clf.predict(X_test)\n    def fit(self,X_test,y_test):\n        self.clf.fit(X_test,y_test)\n    def feature_importances(self,X_test,y_test):\n        return(self.clf.fit(X_test,y_test).feature_importances_)\n    \n    \nfrom sklearn.model_selection import KFold\nNFolds=5\nntrain=train.shape[0]\nkfold = KFold(n_splits= NFolds, random_state=0)\n\n#out of fold predictions\n\ndef out_of_fold(clf,X_train,y_train,X_test):\n    oof_train=np.zeros(train.shape[0])\n    oof_test=np.zeros(test.shape[0])\n    oof_test_training_set=np.empty((NFolds,test.shape[0]))\n    \n    for i,(index_train,index_test) in enumerate(kfold.split(X_train)):\n        x_tr=X_train[index_train]\n        x_ts=X_train[index_test]\n        y_tr=y_train[index_train]\n        \n        clf.train(x_tr,y_tr)\n        oof_train[index_test]=clf.predict(x_ts)\n        oof_test_training_set[i,:]=clf.predict(X_test)\n        \n    oof_test[:]=oof_test_training_set.mean(axis=0)\n        \n    return oof_test.reshape(-1,1),oof_train.reshape(-1,1)\n\n#We will be using logistic regression ,random forests,svm,ada boost, gradient boosting\n\nlr_params={}\nrf_params={'n_estimators':100,'n_jobs':-1,'max_depth':3,'max_features':'sqrt'}\nsvc_params={'C':0.05,'kernel':'linear'}\nada_params={'n_estimators':100,'learning_rate':0.5}\ngrad_params={'n_estimators':100,'max_depth':3}\n\nlr=base_predictors(clf=LogisticRegression,params=lr_params)\nrf= base_predictors(clf=RandomForestClassifier,params=lr_params) \nsvc=base_predictors(clf=SVC,params=svc_params)\nada=base_predictors(clf=AdaBoostClassifier,params=ada_params)\ngrad=base_predictors(clf=GradientBoostingClassifier,params=grad_params)\n\n\n\n\n        \n        ","66548aa1":"train.head()","639a2b60":"y_train=train['Survived'].ravel()\nX_train=train.drop('Survived',axis=1).values\n\nX_test=test.values\nX_train","e1c18388":"    \nlr_test,lr_train=out_of_fold(lr, X_train,y_train,X_test)\nrf_test,rf_train=out_of_fold(rf,X_train,y_train,X_test)\nsvc_test,svc_train=out_of_fold(svc,X_train,y_train,X_test)\nada_test,ada_train=out_of_fold(ada,X_train,y_train,X_test)\ngrad_test,grad_train=out_of_fold(grad,X_train,y_train,X_test)","8be41a51":"base_classifiers_predictions_train=np.concatenate((lr_train,rf_train,svc_train,ada_train,grad_train),axis=1)\nbase_classifiers_predictions_test=np.concatenate((lr_test,rf_test,svc_test,ada_test,grad_test),axis=1)","93e452b7":"\nbase_predictions=pd.DataFrame({'Logistic_Regression':lr_train.ravel(),\n                              'Random_forests':rf_train.ravel(),\n                              'svc':svc_train.ravel(),\n                              'ada':ada_train.ravel(),\n                              'grad':grad_train.ravel()}\n                              )\nbase_predictions.head()","afc6d171":"#let us look at the cprrelation between them\nbase_predictions.corr()\nsns.heatmap(base_predictions.corr(),vmin=0,annot=True)\n","3dc1d63d":"#We see that the the base models are higly correlated.Using models that are not highly correlated will give better results.\n\n","8f891c8a":"from xgboost import XGBClassifier\n\nme_=XGBClassifier(learning_rate=0.05,n_estimators=1000)\nme_.fit(base_classifiers_predictions_train,y_train)\npredictions_=me_.predict(base_classifiers_predictions_test)\naccuracy=round(me_.score(base_classifiers_predictions_train,y_train))\nprint(accuracy)","3f0ae6cc":"submissions=pd.DataFrame({'PassengerId':ID,'Survived':predictions_})\nsubmissions.to_csv('Submissions.csv',index=False)","ae8a8b3b":"\n\nConclusions:\n1)Most of the people in the age group of 20-50 in class 1 survived.\n2)All kids <10 yrs of age in class 2 survived, while slightly more number of adults in the age group 20-40 died.\n3)In class 3 ,most of the youngsters died.\n4)In every port of embarkation, the number females who survived is greater than the number of females who died and viceversa for male candidates.\n\n","3f89cadb":"Conclusion:\nMost of the people belonged to tge age group 15-45.\nMost the people belonging to the age group 15-40 died.\nInfants (age<5yrs) mostly survived.\nold people (age>75yrs) survived.","3880e416":"#The above information tells us that :\n1)About 38 % percent of the whole population survived\n2)Most of the people were belonged to the age group-(15-43)\n3)Most of them had no parent,child ,sibling or spouse.\n\n  ","43698ed5":"Conclusions :\n1)75% of the female population survived and only  19% of the male population survived \n2)While 65% of the people in Class 1 survived ,only 47% and 24% of people in classes 2 and 3 survived\n3)Most of the people who have none or 1 or 2 siblings survived.\n4)Most of the people with Parch number <=3 and >1 survived\n\n\nDecisions made:\n1)Sex and Pclass are highly correlated to the survival rate.\n2)Another feature can be made from SibSp and Parch","65c6ea5b":"\nAge has missing values\nLet us check how age varies across dfferent Pclass for different sexes.\n","ebfb4774":"Conclusions:\nMost of the people purchased tickets worth less than 100$.\nMost of the people whose ticket cost is greater than 100$ survived.\nMost of the people who purchased the least expensive tickets died.\n"}}