{"cell_type":{"a048c2fb":"code","3e758e32":"code","6efaf232":"code","b20a4d00":"code","55e3ac7d":"code","dfdb5e4e":"code","0bb83a61":"code","d6ed89af":"code","0c74f0f8":"code","44947f31":"code","73710947":"code","41c50883":"code","0754d1fa":"code","4a376e86":"code","b9c02322":"code","a17c41dc":"code","4b37c033":"markdown","2ab9df85":"markdown","c8c09b71":"markdown","14c85e1b":"markdown","f15a910f":"markdown","84b3dac1":"markdown"},"source":{"a048c2fb":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import tree, svm, datasets, linear_model, random_projection, feature_selection, feature_extraction, preprocessing, model_selection, feature_selection, metrics, decomposition, cluster, pipeline, impute, compose, ensemble, random_projection\nimport seaborn as sns\nimport os\nfrom ipywidgets import interact\nimport warnings","3e758e32":"warnings.simplefilter(\"ignore\")","6efaf232":"df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","b20a4d00":"def pipe_info(x):\n    \"\"\"Once in a while, you may want to know what is going on in your pipeline.\"\"\"\n    print(x.shape)\n    return x","55e3ac7d":"!pip install git+https:\/\/github.com\/sukruc\/randopts.git\n!pip install bitstring","dfdb5e4e":"import randopts as rnd","0bb83a61":"X_train, X_test, y_train, y_test = model_selection.train_test_split(df.drop('Survived',1 ), df['Survived'], test_size=0.1)","d6ed89af":"cabin_tr = compose.ColumnTransformer([\n    ('p_cabin', pipeline.Pipeline([\n        ('impute', impute.SimpleImputer(strategy='constant', fill_value='U00')),\n        ('cabin_name', preprocessing.FunctionTransformer(lambda x: pd.DataFrame(x)[0].str[0].to_frame().values)),\n        ('encode', preprocessing.OneHotEncoder(handle_unknown='ignore'))\n    ]), ['Cabin']),\n    ('p_sex', pipeline.Pipeline([\n        ('impute', impute.SimpleImputer(strategy='most_frequent')),\n        ('binarize', preprocessing.OneHotEncoder(drop='first'))\n    ]), ['Sex']),\n    ('p_age', pipeline.Pipeline([\n        ('impute', impute.SimpleImputer(strategy='median')),\n        ('discretize', preprocessing.KBinsDiscretizer(n_bins=10))\n    ]), ['Age']),\n    ('p_class', pipeline.Pipeline([\n        ('impute', impute.SimpleImputer(strategy='most_frequent')),\n        ('encode', preprocessing.OneHotEncoder(drop='first'))\n    ]), ['Pclass']),\n    ('p_name', pipeline.Pipeline([\n        ('impute', impute.SimpleImputer(strategy='constant', fill_value='Unknown')),\n        ('tolist', preprocessing.FunctionTransformer(lambda x: x.T[0].tolist())),\n        ('encode', feature_extraction.text.CountVectorizer(min_df=0.0))\n    ]), ['Name']),\n], remainder='drop')\nfeature_pipe = pipeline.Pipeline([\n        ('tr', cabin_tr),\n        ('union', pipeline.FeatureUnion([\n            ('poly', preprocessing.PolynomialFeatures(2, include_bias=False, interaction_only=True)),\n            ('dummy', preprocessing.FunctionTransformer(lambda x: x))\n            ])),\n#         ('count', preprocessing.FunctionTransformer(pipe_info)),  # uncomment to print shape at this point\n#         ('projector1', random_projection.GaussianRandomProjection(n_components=1000)),\n        ('cluster', cluster.KMeans(n_clusters=100)),\n        ('projector2', random_projection.GaussianRandomProjection(n_components=500)),\n#         ('binarizer', preprocessing.Binarizer()),\n#         ('scale', preprocessing.MaxAbsScaler()),\n#         ('standardize', preprocessing.StandardScaler()),\n\n])","0c74f0f8":"def interpreter_func(X, y, transformer, classifier, **cv_args):\n    \"\"\"An interpreter function wrapper to communicate between optimizer,\n    transformation pipeline and the model.\n    \n    Args:\n    ----------\n        X: pd.DataFrame, raw dataset\n        y: pd.DataFrame, pd.Series or numpy.ndarray, target variables\n        classifier: sklearn-like estimator instance\n        **cv_args: keyword arguments to pass on `model_selection.cross_validate` function.\n        \n    Returns:\n    ----------\n        fitness_func: a callable that takes a bit string and yields a fitness score \n    \"\"\"\n    print(\"Creating wrapper... This may take a while.\")\n    X_ = transformer.fit_transform(X)\n    print(\"Done.\")\n    def fitness_func(support_bits: str):\n        support = [bool(int(i)) for i in list(support_bits)]\n        Xselect = X_[:, support]\n        score = model_selection.cross_validate(classifier, Xselect, y, return_train_score=True, **cv_args)['test_score'].mean()\n        return score\n    return fitness_func","44947f31":"Xsmp = feature_pipe.fit_transform(\n    X_train,\n)\nproblem = rnd.problem.CustomProblem(\n    interpreter_func(\n        X_train,\n        y_train,\n        feature_pipe,\n#         svm.LinearSVC(),\n        linear_model.LogisticRegression(max_iter=10000),\n        cv=model_selection.StratifiedKFold(n_splits=(10 \/\/ (1 + 1 + 1))),\n        n_jobs=-1,\n        ),\n    nbits=Xsmp.shape[1]\n    )\n# optimizer = rnd.solver.HillClimber(verbose=1, random_neighbors=None, maksiter=100)\noptimizer = rnd.solver.MIMIC(verbose=1, gain_func='corr', patience=10, population_size=100)\noptimizer.fit(problem)\noptimizer.ara(problem)\nprint(optimizer.best)","73710947":"plt.plot(optimizer.solution_arr)\nplt.semilogx()\nplt.grid()\nplt.xlabel('Function evaluations')\nplt.ylabel('Average Test Accuracy (10-folds)')\nplt.title('MIMIC performance vs function evaluations')","41c50883":"X_ = feature_pipe.transform(df.drop('Survived', 1))\ny_ = df['Survived']\nmodel = linear_model.LogisticRegression()\nsupport = [bool(int(i)) for i in optimizer.best]\nmodel.fit(X_[:, support], y_)","0754d1fa":"feature_pipe.transform(df_test)[:, support].shape","4a376e86":"pd.DataFrame(model_selection.cross_validate(model, X_[:, support], y_, return_train_score=True)).mean()","b9c02322":"pred = pd.Series(model.predict(feature_pipe.transform(df_test)[:, support]), index=df_test['PassengerId']).rename('Survived')","a17c41dc":"pred.to_csv('\/kaggle\/working\/predictions.csv')","4b37c033":"## Feature Transformer Pipeline\nAs discussed in introduction, a pipeline is featured to apply some transformations and yield binary features from original raw dataset. Second order interactions are included as AND operatores. As squared binary variables are equal to themselves, we only include interactions by using a `FeatureUnion` and save 2,000 redundant features! (total number of features at this point is 1M)","2ab9df85":"An interpreter maker wrapper function is provided below to communicate between optimizer and pipeline.","c8c09b71":"We hold out 10% of data, but we really don't do anything with it here. Feel free to use it as your test set.","14c85e1b":"The cell below is the heart of feature selection. \n\n## MIMIC\n\nMIMIC allows user to adjust a set of parameters such as population size, fitness shrinkage (set to 0.5 median population fitness by default\n\n## HillClimber\n\nAs a comparison, we also provide `HillClimber`. Uncomment `HillClimber` line to see how it works and compare to MIMIC. (Pass an integer N to `random_neighbors` argument to speed up search by only using a subset of N neighbors at each iteration.)\n\nNow, this will take a while. Take a break, write some code, do what you do best.","f15a910f":"`randopts` can be installed using code snippet below.","84b3dac1":"# Feature Selection with Randomized Optimization - MIMIC\n\nFeature selection with randomized optimization. Why randomized optimization? Because ..., why not? Whenever a function is non-derivable or its domain is discrete, randomized optimization is there for you, and feature selection problem is no exception.\n\n## Simplicity\n\nWe keep things simple: only a modular feature extraction section is added and for the sake of simplicity, all features are discretized. Categories are encoded, continuous ones are split into bins and again encoded. No fine-granuled rules such as **Masters live**, this is purely extract-and-dump style.\n\n## Believing in Search\nYes, we believe in search, but not brute search. Intelligent, informative search with a structured approach can yield a consisten set of features with high generalization ability for any model. \n\nFor this application, we go back in time and bring back Mutual-Information Maximizing Input Clustering (MIMIC)[[1]] algorithm by Charles Isbell et. al. for feature selection.\n\n## MIMIC\n\nMIMIC is a population based incremental learning algorithm *which attempts to communicate information about the cost function obtained from one iteration of the search to later iterations of the search*. In simple terms, this is done by creating a dependency tree of variables and calculating emprical conditional probability of variables at each iteration while gradually raising fitness threshold to retain candidates, resulting in fitness of candidate set eventually converging to optimum.\n\n### Adaptation to Feature Selection\n\nBy appropriately defining a fitness function, randomized optimization algorithms can be adapted to most tasks. In our case, fitness function is average test score of a 10-fold cross validation process where a logistic regression model is trained and evaluated by features selected by MIMIC.\n\n### Expensive Overhead\n\nCompared to other randomized optimization algorithms, MIMIC has a high overhead because of inner loop consisting of mutual information calculation, dependency tree construction, conditional probability estimation and resampling. However, number of function evaluations (10-fold cross validations in this case) is also considerably fewer due to communication of cost function information.\n\nBelow is MIMIC's performance in K-coloring problem compared to other algorithms [[1]]\n\n![image.png](attachment:c3db8d49-9672-4ee0-a5c3-c3875de09986.png)\n\nTherefore, MIMIC is better suited for scenarios where cost of function evaluation is relatively high (after all, make it worth while). Throw in your most dreaded stacked ensembled reinforced learners.\n\n### Implementation\n\nMIMIC implementation we use in this application is per `randopts` [[2]] package, providing the algorithm itself with a twist (correlation coefficient as a faster yet still usable approximation of mutual info gain). `randopts` also features some handy API to make adaption of any task easier.\n\n(Disclaimer: this code base has nothing to do with CS-7641 Machine Learning (winks) )\n\n## Model\n\nTo further highlight the power of feature selection and how it works with any model, we stay away from abysmally stacked ensembles of models and use ONE simple logistic regression model.\n\n[1]: https:\/\/www.cc.gatech.edu\/~isbell\/papers\/isbell-mimic-nips-1997.pdf \"Mutual-Information Maximizing Input Clustering (MIMIC)\"\n\n[2]: https:\/\/github.com\/sukruc\/randopts \"sukruc\/randopts\"\n"}}