{"cell_type":{"a3d38532":"code","76cdb02c":"code","5750f05b":"code","769c571e":"code","efa1d860":"code","b7c53a7a":"code","b46bb6eb":"code","1330797a":"code","73037cf5":"code","31d56b55":"code","80ec53c2":"code","d4ae0486":"code","30c6ee6b":"code","e2946bf8":"code","60dd7c68":"code","6d5cb41f":"code","79e5f783":"code","49e139d5":"code","78190907":"markdown","fde205b7":"markdown","93d8cce7":"markdown","0c1df824":"markdown","a8980ce2":"markdown","8b162945":"markdown","b12d82a7":"markdown"},"source":{"a3d38532":"import numpy as np\nimport pandas as pd\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport gensim\nfrom nltk.corpus import brown\nimport random\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nimport gc\nfrom keras.callbacks.callbacks import EarlyStopping\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.callbacks.callbacks import EarlyStopping\nfrom scipy.stats import spearmanr\nfrom nltk.corpus import wordnet as wn\nimport tqdm\nfrom sklearn.model_selection import StratifiedKFold","76cdb02c":"train = pd.read_csv(\"..\/input\/google-quest-challenge\/train.csv\")\ntest = pd.read_csv(\"..\/input\/google-quest-challenge\/test.csv\")","5750f05b":"sample_sub = pd.read_csv(\"..\/input\/google-quest-challenge\/sample_submission.csv\")","769c571e":"sample_sub ","efa1d860":"target_cols = ['question_asker_intent_understanding',\n       'question_body_critical', 'question_conversational',\n       'question_expect_short_answer', 'question_fact_seeking',\n       'question_has_commonly_accepted_answer',\n       'question_interestingness_others', 'question_interestingness_self',\n       'question_multi_intent', 'question_not_really_a_question',\n       'question_opinion_seeking', 'question_type_choice',\n       'question_type_compare', 'question_type_consequence',\n       'question_type_definition', 'question_type_entity',\n       'question_type_instructions', 'question_type_procedure',\n       'question_type_reason_explanation', 'question_type_spelling',\n       'question_well_written', 'answer_helpful',\n       'answer_level_of_information', 'answer_plausible', 'answer_relevance',\n       'answer_satisfaction', 'answer_type_instructions',\n       'answer_type_procedure', 'answer_type_reason_explanation',\n       'answer_well_written']","b7c53a7a":"train","b46bb6eb":"def simple_prepro(s):\n    return [w for w in s.replace(\"\\n\",\" \").replace(\",\",\" , \").replace(\"(\",\" ( \").replace(\")\",\" ) \").\n            replace(\".\",\" . \").replace(\"?\",\" ? \").replace(\":\",\" : \").replace(\"n't\",\" not\").\n            replace(\"'ve\",\" have\").replace(\"'re\",\" are\").replace(\"'s\",\" is\").split(\" \") if w != \"\"]","1330797a":"def simple_prepro_tfidf(s):\n    return \" \".join([w for w in s.lower().replace(\"\\n\",\" \").replace(\",\",\" , \").replace(\"(\",\" ( \").replace(\")\",\" ) \").\n            replace(\".\",\" . \").replace(\"?\",\" ? \").replace(\":\",\" : \").replace(\"n't\",\" not\").\n            replace(\"'ve\",\" have\").replace(\"'re\",\" are\").replace(\"'s\",\" is\").split(\" \") if w != \"\"])","73037cf5":"qt_max = max([len(simple_prepro(l)) for l in list(train[\"question_title\"].values)])\nqb_max = max([len(simple_prepro(l))  for l in list(train[\"question_body\"].values)])\nan_max = max([len(simple_prepro(l))  for l in list(train[\"answer\"].values)])\nprint(\"max lenght of question_title is\",qt_max)\nprint(\"max lenght of question_body is\",qb_max)\nprint(\"max lenght of question_answer is\",an_max)","31d56b55":"w2v_model = gensim.models.Word2Vec(brown.sents())","80ec53c2":"def get_word_embeddings(text):\n    np.random.seed(abs(hash(text)) % (10 ** 8))\n    words = simple_prepro(text)\n    vectors = np.zeros((len(words),100))\n    if len(words)==0:\n        vectors = np.zeros((1,100))\n    for i,word in enumerate(simple_prepro(text)):\n        try:\n            vectors[i]=w2v_model[word]\n        except:\n            vectors[i]=np.random.uniform(-0.01, 0.01,100)\n    return np.concatenate([np.max(np.array(vectors), axis=0),\n                          np.array([min(len(text),5000)\/5000,\n                                    min(text.count(\" \"),5000)\/5000,\n                                    min(len(words),1000)\/1000,\n                                    min(text.count(\"\\n\"),100)\/100,\n                                   min(text.count(\"!\"),20)\/20,\n                                   min(text.count(\"?\"),20)\/20])])\n                           ","d4ae0486":"question_title = [get_word_embeddings(l) for l in tqdm.tqdm(train[\"question_title\"].values)]\nquestion_title_test = [get_word_embeddings(l) for l in tqdm.tqdm(test[\"question_title\"].values)]\n\nquestion_body = [get_word_embeddings(l) for l in tqdm.tqdm(train[\"question_body\"].values)]\nquestion_body_test = [get_word_embeddings(l) for l in tqdm.tqdm(test[\"question_body\"].values)]\n\nanswer = [get_word_embeddings(l) for l in tqdm.tqdm(train[\"answer\"].values)]\nanswer_test = [get_word_embeddings(l) for l in tqdm.tqdm(test[\"answer\"].values)]","30c6ee6b":"gc.collect()\ntfidf = TfidfVectorizer(ngram_range=(1, 3))\ntsvd = TruncatedSVD(n_components = 60)\ntfidf_question_title = tfidf.fit_transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(train[\"question_title\"].values)])\ntfidf_question_title_test = tfidf.transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(test[\"question_title\"].values)])\ntfidf_question_title = tsvd.fit_transform(tfidf_question_title)\ntfidf_question_title_test = tsvd.transform(tfidf_question_title_test)\n\ntfidf_question_body = tfidf.fit_transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(train[\"question_body\"].values)])\ntfidf_question_body_test = tfidf.transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(test[\"question_body\"].values)])\ntfidf_question_body = tsvd.fit_transform(tfidf_question_body)\ntfidf_question_body_test = tsvd.transform(tfidf_question_body_test)\n\ntfidf_answer = tfidf.fit_transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(train[\"answer\"].values)])\ntfidf_answer_test = tfidf.transform([simple_prepro_tfidf(l) for l in tqdm.tqdm(test[\"answer\"].values)])\ntfidf_answer = tsvd.fit_transform(tfidf_answer)\ntfidf_answer_test = tsvd.transform(tfidf_answer_test)","e2946bf8":"type2int = {type:i for i,type in enumerate(list(set(train[\"category\"])))}\ncate = np.identity(5)[np.array(train[\"category\"].apply(lambda x:type2int[x]))].astype(np.float64)\ncate_test = np.identity(5)[np.array(test[\"category\"].apply(lambda x:type2int[x]))].astype(np.float64)","60dd7c68":"train_features = np.concatenate([question_title, question_body, answer,\n                                 tfidf_question_title, tfidf_question_body, tfidf_answer, \n                                 cate\n                                ], axis=1)\ntest_features = np.concatenate([question_title_test, question_body_test, answer_test, \n                               tfidf_question_title_test, tfidf_question_body_test, tfidf_answer_test,\n                                cate_test\n                                ], axis=1)","6d5cb41f":"num_folds = 10\nfold_scores = []\nkf = KFold(n_splits = num_folds, shuffle = True, random_state = 42)\ntest_preds = np.zeros((len(test_features), len(target_cols)))\nvalid_preds = np.zeros((train_features.shape[0],30))\nfor train_index, val_index in kf.split(train_features):\n    gc.collect()\n    train_X = train_features[train_index, :]\n    train_y = train[target_cols].iloc[train_index]\n    \n    val_X = train_features[val_index, :]\n    val_y = train[target_cols].iloc[val_index]\n    \n    model = Sequential([\n        Dense(1024, input_shape=(train_features.shape[1],)),\n        Activation('relu'),\n        Dense(512),\n        Activation('relu'),\n        Dense(len(target_cols)),\n        Activation('sigmoid'),\n    ])\n    \n    es = EarlyStopping(monitor='val_loss', min_delta=0, patience=30, verbose=0, mode='auto', baseline=None, restore_best_weights=True)\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy')\n    \n    model.fit(train_X, train_y, epochs = 300, validation_data=(val_X, val_y), callbacks = [es])\n    preds = model.predict(val_X)\n    valid_preds[val_index] = preds\n    overall_score = 0\n    for col_index, col in enumerate(target_cols):\n        overall_score += spearmanr(preds[:, col_index], val_y[col].values).correlation\/len(target_cols)\n        print(col, spearmanr(preds[:, col_index], val_y[col].values).correlation)\n    fold_scores.append(overall_score)\n    print(overall_score)\n    test_preds += model.predict(test_features)\/num_folds\nprint(fold_scores)","79e5f783":"valid = 0\nfor col_index, col in enumerate(target_cols):\n    valid += spearmanr(valid_preds[:, col_index], train[col].values).correlation\/30\nprint(\"valid score is \",valid)","49e139d5":"sub = pd.read_csv(\"..\/input\/google-quest-challenge\/sample_submission.csv\")\nfor col_index, col in enumerate(target_cols):\n    sub[col] = test_preds[:, col_index]\nsub.to_csv(\"submission.csv\", index = False)","78190907":"# fearure engineering","fde205b7":"# Introduction\nI will add new information to TFIDF+NN model(https:\/\/www.kaggle.com\/ryches\/tfidf-benchmark ).<br>\nTFIDF can create features based on actual vocabulary, but it can't handle well when there is another word of close meaning.<br>\nTherefore, I thought that adding SWEM(https:\/\/arxiv.org\/abs\/1805.09843) using learned word2vec as a feature value would increase the score.","93d8cce7":"The contribution of the score was not great, but if you use bert etc. instead of brown, I think the score will go up more.","0c1df824":"This is basic preprocessing. This time, symbols and words are attached, so they are separated here.","a8980ce2":"Here we use a trained word2vec model that is easily available with nltk.<br>\nWe used SWEM with max pooling.<br>\nHere, add information about the length of the sentence and the number of line ,'&nbsp;&nbsp;' ,'?' and '!'.<br>\nConsecutive spaces can be useful information.","8b162945":"From here on, I'm quite referring to https:\/\/www.kaggle.com\/ryches\/tfidf-benchmark.","b12d82a7":"The text is so long that it is difficult to apply RNN to all series."}}