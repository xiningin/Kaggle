{"cell_type":{"a8c42412":"code","d3b38eae":"code","fe2012b6":"code","69381faf":"code","0ec5b322":"code","834eaf71":"code","53b47611":"code","edecc979":"code","9486d104":"code","e1e01bb2":"code","5bdee83b":"code","26d6aa48":"code","afb73716":"code","5af180c5":"code","7dee3656":"code","3e79e4c8":"code","601c2b9c":"code","39b60c8e":"code","24a827bf":"code","51666ba0":"code","8c6c7e20":"markdown","f0337c17":"markdown","071d2cb3":"markdown","9ee3bd9a":"markdown","983357ba":"markdown"},"source":{"a8c42412":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas.core.algorithms as algos\nfrom pandas import Series\nimport scipy.stats.stats as stats\nimport re\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d3b38eae":"train = pd.read_csv(r'\/kaggle\/input\/jobathon-may-2021-credit-card-lead-prediction\/train.csv')\ntest = pd.read_csv(r'\/kaggle\/input\/jobathon-may-2021-credit-card-lead-prediction\/test.csv')","fe2012b6":"train.head()","69381faf":"# plot countplots \ncat_col = ['Occupation','Channel_Code','Credit_Product']\nplt.figure(figsize=(14, 12), dpi=100)\nfor i, feature in enumerate(cat_col):\n    plt.subplot(3, 3, i+1)\n    sns.countplot(data=train, x=feature)\n    \nsns.despine()","0ec5b322":"sns.distplot(train[\"Avg_Account_Balance\"]) # highly positively skewed \n# we will use WOE transformation to transform the variable","834eaf71":"# features to plot in the count plots\n\n\n# plot countplots \ncat_col = ['Gender','Region_Code','Occupation','Channel_Code','Credit_Product','Is_Active']\nplt.figure(figsize=(14, 12), dpi=100)\nfor i, feature in enumerate(cat_col):\n    plt.subplot(3, 3, i+1)\n    sns.countplot(data=train, x=feature, hue='Is_Lead')\n    \nsns.despine()","53b47611":"max_bin = 20\nforce_bin = 3\n\n# Binning Function for continous variables\ndef mono_bin(Y, X, n = max_bin):\n    \n    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n    justmiss = df1[['X','Y']][df1.X.isnull()]\n    notmiss = df1[['X','Y']][df1.X.notnull()]\n    r = 0\n    while np.abs(r) < 1:\n        try:\n            d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.qcut(notmiss.X, n)})\n            d2 = d1.groupby('Bucket', as_index=True)\n            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n            n = n - 1 \n        except Exception as e:\n            n = n - 1\n\n    if len(d2) == 1:\n        n = force_bin         \n        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n        if len(np.unique(bins)) == 2:\n            bins = np.insert(bins, 0, 1)\n            bins[1] = bins[1]-(bins[1]\/2)\n        d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) \n        d2 = d1.groupby('Bucket', as_index=True)\n    \n    d3 = pd.DataFrame({},index=[])\n    d3[\"MIN_VALUE\"] = d2.min().X\n    d3[\"MAX_VALUE\"] = d2.max().X\n    d3[\"COUNT\"] = d2.count().Y\n    d3[\"EVENT\"] = d2.sum().Y\n    d3[\"NONEVENT\"] = d2.count().Y - d2.sum().Y\n    d3=d3.reset_index(drop=True)\n    \n    if len(justmiss.index) > 0:\n        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n        d4[\"MAX_VALUE\"] = np.nan\n        d4[\"COUNT\"] = justmiss.count().Y\n        d4[\"EVENT\"] = justmiss.sum().Y\n        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n        d3 = d3.append(d4,ignore_index=True)\n    \n    d3[\"EVENT_RATE\"] = d3.EVENT\/d3.COUNT\n    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT\/d3.COUNT\n    d3[\"DIST_EVENT\"] = d3.EVENT\/d3.sum().EVENT\n    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT\/d3.sum().NONEVENT\n    d3[\"WOE\"] = np.log(d3.DIST_EVENT\/d3.DIST_NON_EVENT)\n    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT\/d3.DIST_NON_EVENT)\n    d3[\"VAR_NAME\"] = \"VAR\"\n    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]       \n    d3 = d3.replace([np.inf, -np.inf], 0)\n    d3.IV = d3.IV.sum()\n    \n    return(d3)\n\n# Binning Function for Categorical variables\ndef char_bin(Y, X):\n        \n    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n    justmiss = df1[['X','Y']][df1.X.isnull()]\n    notmiss = df1[['X','Y']][df1.X.notnull()]    \n    df2 = notmiss.groupby('X',as_index=True)\n    \n    d3 = pd.DataFrame({},index=[])\n    d3[\"COUNT\"] = df2.count().Y\n    d3[\"MIN_VALUE\"] = df2.sum().Y.index\n    d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n    d3[\"EVENT\"] = df2.sum().Y\n    d3[\"NONEVENT\"] = df2.count().Y - df2.sum().Y\n    \n    if len(justmiss.index) > 0:\n        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n        d4[\"MAX_VALUE\"] = np.nan\n        d4[\"COUNT\"] = justmiss.count().Y\n        d4[\"EVENT\"] = justmiss.sum().Y\n        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n        d3 = d3.append(d4,ignore_index=True)\n    \n    d3[\"EVENT_RATE\"] = d3.EVENT\/d3.COUNT\n    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT\/d3.COUNT\n    d3[\"DIST_EVENT\"] = d3.EVENT\/d3.sum().EVENT\n    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT\/d3.sum().NONEVENT\n    d3[\"WOE\"] = np.log(d3.DIST_EVENT\/d3.DIST_NON_EVENT)\n    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT\/d3.DIST_NON_EVENT)\n    d3[\"VAR_NAME\"] = \"VAR\"\n    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n    d3 = d3.replace([np.inf, -np.inf], 0)\n    d3.IV = d3.IV.sum()\n    d3 = d3.reset_index(drop=True)\n    \n    return(d3)","edecc979":"\ndef comb_category(woe,threshold):\n    \n    count = 0\n    similar_col = dict()\n    col = []\n    columns1 = woe['MIN_VALUE'].unique()\n    columns2 = woe['MIN_VALUE'].unique()\n    for cat1 in columns1 :\n        if cat1 in col: continue\n        woe1 = float(woe[woe['MIN_VALUE'] == cat1]['WOE'].values[0])\n        col1 = []\n\n        for cat in woe['MIN_VALUE'].unique():\n            if cat1 == cat: continue\n            if cat in col: continue\n            woe2 = float(woe[woe['MIN_VALUE'] == cat]['WOE'].values[0])\n\n            if (woe2 - woe1) >0.0 and (woe2 - woe1)<threshold:\n                col1.append(cat)\n                col.append(cat)\n        col.append(cat1)\n\n        similar_col[cat1] = col1\n        \n        if len(col1)>0:\n            count+=1\n    \n    return(similar_col,count)\n","9486d104":"\ndef data_vars(df1,target,test,cat_threshold):\n    \n    \n    \n    x = df1.dtypes.index\n    count = -1\n    replace = {}\n    \n    for i in x:\n        \n        if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n            conv = mono_bin(target, df1[i])\n            conv[\"VAR_NAME\"] = i\n            count = count + 1\n        else:\n            cat_replace = []\n            conv = char_bin(target, df1[i])\n            conv = conv.sort_values('WOE')\n            similar_col,var_count = comb_category(conv,cat_threshold)\n            cat_replace.append(similar_col)\n\n            while var_count>0:\n\n                for x,y in zip(similar_col.keys(),similar_col.values()):\n                    df1.loc[df1[i].isin(y),i] = x\n                    test.loc[test[i].isin(y),i] = x\n                conv = char_bin(target, df1[i])\n                conv = conv.sort_values('WOE')\n                similar_col,var_count = comb_category(conv,cat_threshold)\n                cat_replace.append(similar_col)\n            replace[i] = cat_replace\n            conv[\"VAR_NAME\"] = i            \n            count = count + 1\n\n        if count == 0:\n            iv_df = conv\n        else:\n            iv_df = iv_df.append(conv,ignore_index=True)\n    \n    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n    iv = iv.reset_index()\n    return(iv_df,iv,df1,test,replace)","e1e01bb2":"train_copy = train.copy()\ntest_copy  = test.copy()\n\ntrain_copy.drop('ID',axis =1,inplace = True)\ntrain_copy['Credit_Product'].fillna('NA',inplace = True)\n\ntest_copy.drop('ID',axis =1,inplace = True)\ntest_copy['Credit_Product'].fillna('NA',inplace = True)\n","5bdee83b":"#Calculating WOE and IV values\nfinal_iv, IV,new_train,new_test,cat_replace = data_vars(train_copy,train_copy.Is_Lead,test_copy,cat_threshold = 0.1)","26d6aa48":"final_iv.head()","afb73716":"def woe_replacement(train,transform_vars_list,transform_prefix):\n    for var in transform_vars_list:\n        print(var)\n        \n        small_train = final_iv[final_iv['VAR_NAME'] == var]\n        transform_dict = dict(zip(small_train.MAX_VALUE,small_train.WOE))\n        replace_cmd = ''\n        replace_cmd1 = ''\n        \n        for i in sorted(transform_dict.items()):\n            replace_cmd = replace_cmd + str(i[1]) + str(' if x <= ') + str(i[0]) + ' else '\n            replace_cmd1 = replace_cmd1 + str(i[1]) + str(' if x == \"') + str(i[0]) + '\" else '\n        replace_cmd = replace_cmd + '0'\n        replace_cmd1 = replace_cmd1 + '0'\n        \n        if replace_cmd != '0':\n            try:\n                train[transform_prefix + var] = train[var].apply(lambda x: eval(replace_cmd))\n            except:\n                train[transform_prefix + var] = train[var].apply(lambda x: eval(replace_cmd1))\n                \n    return(train)","5af180c5":"transform_vars_list = new_train.columns.difference(['Is_Lead'])\ntransform_prefix = 'new_'\n\nnew_train = woe_replacement(new_train,transform_vars_list,transform_prefix)","7dee3656":"transform_vars_list = new_test.columns\n\nnew_test = woe_replacement(new_test,transform_vars_list,transform_prefix)","3e79e4c8":"new_train.head()","601c2b9c":"#Function for running cross validation\ndef boosting(clf, fit_params, train, test, features):\n    N_SPLITS = 10\n    oofs = np.zeros(len(train))\n    preds = np.zeros((len(test)))\n\n    folds = StratifiedKFold(n_splits = N_SPLITS)\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train[TARGET_COL])):\n        print(f'\\n------------- Fold {fold_ + 1} -------------')\n\n        ### Training Set\n        X_trn, y_trn = train[features].iloc[trn_idx], train[TARGET_COL].iloc[trn_idx]\n\n        ### Validation Set\n        X_val, y_val = train[features].iloc[val_idx], train[TARGET_COL].iloc[val_idx]\n\n        ### Test Set\n        X_test = test[features]\n\n        #print(X_trn)\n        #exit(0)\n\n        _ = clf.fit(X_trn, y_trn, eval_set = [(X_val, y_val)], **fit_params)\n\n        ### Instead of directly predicting the classes we will obtain the probability of positive class.\n        preds_val = clf.predict_proba(X_val)[:, 1]\n        preds_test = clf.predict_proba(X_test)[:, 1]\n\n        roc_score = roc_auc_score(y_val,preds_val)\n        print(\"ROC for validation set is {}\".format(roc_score))\n\n        oofs[val_idx] = preds_val\n        preds += preds_test \/ N_SPLITS\n\n\n    oofs_score = roc_auc_score(train[TARGET_COL], oofs.round())\n    print('ROC score for oofs is {}'.format(oofs_score))\n\n\n    return oofs, preds","39b60c8e":"#catboost model training\nclf = CatBoostClassifier(n_estimators = 3000,\n                       learning_rate = 0.02,\n                       rsm = 0.4, ## Analogous to colsample_bytree\n                       random_state=2054,\n                       \n                       )\n\nfit_params = {'verbose': 200, 'early_stopping_rounds': 300}\n\nfeatures = ['new_Age', 'new_Avg_Account_Balance', 'new_Channel_Code',\n       'new_Credit_Product', 'new_Gender', 'new_Is_Active', 'new_Occupation',\n       'new_Region_Code', 'new_Vintage']\n\nTARGET_COL = 'Is_Lead'\n\ncb_oofs, cb_preds = boosting(clf, fit_params,new_train,new_test,features)\n\noptimized_roc = roc_auc_score(new_train[TARGET_COL], (cb_oofs  * 1))\nprint(f'Optimized ROC is {optimized_roc}')","24a827bf":"#training LightGBM model \nclf = LGBMClassifier(n_estimators = 200,\n                        learning_rate = 0.05,\n                        colsample_bytree = 0.5,\n                        )\nfit_params = {'verbose': 100, 'early_stopping_rounds': 100}\n\nlgb_oofs, lgb_preds = boosting(clf, fit_params,new_train,new_test,features)\n\n\noptimized_roc = roc_auc_score(new_train[TARGET_COL], (lgb_oofs * 1))\nprint(f'Optimized ROC is {optimized_roc}')","51666ba0":"#training XGB Classifier\nclf = XGBClassifier(n_estimators = 1000,\n                    max_depth = 6,\n                    learning_rate = 0.05,\n                    colsample_bytree = 0.5,\n                    random_state=1452,\n                    )\n\nfit_params = {'verbose': 200, 'early_stopping_rounds': 200}\n\nxgb_oofs, xgb_preds = boosting(clf, fit_params,new_train,new_test,features)\n\n\noptimized_f1 = roc_auc_score(new_train[TARGET_COL], (xgb_oofs * 1))\nprint(f'Optimized F1 is {optimized_f1}')","8c6c7e20":"**Weight of Evidence(WOE) and Information value (IV)**\n\nThe weight of evidence tells the predictive power of an independent variable in relation to the dependent variable. Since it evolved from credit scoring world, it is generally described as a measure of the separation of good and bad customers.\n\n                         WOE  = ln(Distribution of Goods\/ Distribution of Bads)\n                         \n                         Distribution of Goods : % of good customer in a particular group\n                         Distribution of Bads  : % of bad customer in a particular group\n                         ln : Natural log\n                         \n                         ","f0337c17":"**EDA**","071d2cb3":"**Modelling (Boosting Algorithm)**","9ee3bd9a":"**Following problem is based on a hackathon conducted by Analytics Vidhya where the objective was to predict Customers intrested in taking Credit Card. In the code you will find the following:**\n\n* EDA of the continous and categorical variable\n* Weight of Evidence(WOE) and Information value(IV) for continous and Categorical Feature\n* Modelling using Boosting techniques and WOE transformed features\n","983357ba":"**Replacing features with WOE values**"}}