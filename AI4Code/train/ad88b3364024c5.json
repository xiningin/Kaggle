{"cell_type":{"1d0ba80b":"code","fce86645":"code","ae192ce3":"code","ada8acf6":"code","e7a1d875":"code","0cf384af":"code","20461f74":"code","72efbaa5":"code","7a189d40":"code","d2da3e8d":"code","74b25213":"code","7e9dcbfd":"code","c4ebfa90":"code","391a030a":"code","a6d8b68f":"code","dcb16349":"code","c999bd50":"code","c74e40be":"code","f99a95bb":"code","d4db1e54":"code","78fb297e":"code","c88a235b":"code","a570b59c":"code","81a1e0c6":"code","8a2948bf":"code","d37335a4":"code","bfaae44b":"code","e7607fd1":"code","c68b6176":"code","e6a60ba6":"code","d44313a8":"code","feb963cc":"code","4b5f1003":"code","6b196b41":"code","5a13daa6":"code","c3bc8bd4":"code","70d5f546":"markdown","14fc63c3":"markdown","af0b72b3":"markdown","add303c7":"markdown","970052e0":"markdown","13f12c7f":"markdown","61e82007":"markdown","58d16386":"markdown","53944d93":"markdown","247868b4":"markdown","a54a6370":"markdown","daa6fcf2":"markdown","76d42019":"markdown","fbb83cd1":"markdown","3e6e3059":"markdown","99bbdfd8":"markdown","f9be5289":"markdown","9d3ea3c2":"markdown","0213163b":"markdown","d6eae104":"markdown","ba482b1c":"markdown","4785a2e8":"markdown","8fbc7b10":"markdown","0601a3b0":"markdown","3b1056a2":"markdown","6d473d7c":"markdown","40a57739":"markdown","b01f7c92":"markdown","c95887d9":"markdown","5d280502":"markdown","bcc3acb3":"markdown","8fb57f79":"markdown","d8bf595d":"markdown","92655d44":"markdown","3edd1e1f":"markdown","2e91a8a5":"markdown","f40c51c6":"markdown","2ad5ac4c":"markdown","26d594b1":"markdown","cdac7864":"markdown","88c8cbb4":"markdown","5ed19e18":"markdown"},"source":{"1d0ba80b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fce86645":"# graphs\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport warnings\n\n# stats\nfrom scipy.stats import skew\nfrom scipy import stats\nfrom scipy.stats.stats import pearsonr\nfrom scipy.stats import norm\nfrom collections import Counter\n\n# models\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\nfrom sklearn import svm\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.linear_model import LinearRegression,LassoCV, Ridge, LassoLarsCV,ElasticNetCV\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, learning_curve, train_test_split\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, ExtraTreesRegressor, GradientBoostingRegressor\nfrom sklearn import neighbors\nfrom sklearn.neural_network import MLPClassifier, MLPRegressor\n\nfrom sklearn.preprocessing import StandardScaler, Normalizer, RobustScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import balanced_accuracy_score, roc_auc_score, make_scorer\nfrom sklearn.metrics import confusion_matrix # creates a confusion matrix\nfrom sklearn.metrics import plot_confusion_matrix # draws a confusion matrix\nfrom sklearn.metrics.pairwise import rbf_kernel\nfrom sklearn.metrics import precision_score, accuracy_score\n\n# pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\nwarnings.filterwarnings('ignore')\nsns.set(style='white', context='notebook', palette='deep')\n\n%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\n%matplotlib inline\n\n","ae192ce3":"# Loading the data set\napt_price=pd.read_csv(\"..\/input\/seoul-apt-prices-20062012\/APT_price_seoul_2006_2012.csv\")\n\n#change the unit of price from Won to Dollar\napt_price[\"price\"] = apt_price[\"price\"]*10\n","ada8acf6":"# Set of features to be used \nfeatures_selected = ['District', 'maxBuild', 'Hhld', 'Floor', 'Size', 'schoolDistHs', 'new..prop..snu23','Age_complex', 'yearmon', 'BuildId']\n\ndef select_dat_specific_year(yr, fraction=None):\n    # Select data of the APTs that were sold in yr\n    if fraction is None:\n        apt_price12 = apt_price[apt_price['year'] == yr]\n    else:\n        apt_price12 = apt_price[apt_price['year'] == yr]\n        apt_price12 = apt_price12.sample(frac=fraction)\n\n    # Remove rows with missing target, separate target from predictors\n    X_full = apt_price12.dropna(axis=0, subset=['price'])\n\n    X = X_full[features_selected]\n    y = X_full.price\n    \n    return X, y\n\n\n# Making train\/validation data set\n\nX, y = select_dat_specific_year(2012)\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, \n                                                                train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n\n# Select categorical columns with dtype=object that will be encoded in a later part\ncategorical_cols0 = [cname for cname in X_train_full.columns if \n                    X_train_full[cname].dtype == \"object\"]\ncategorical_cols = list(set(categorical_cols0) - set(['yearmon', 'BuildId']))\n\n# Select numerical columns that will be imputed in a later part\nnumerical_cols = [cname for cname in X_train_full.columns if \n                X_train_full[cname].dtype in ['int64', 'float64']]\n\n\n# Keep selected columns only\nmy_cols = categorical_cols0 + numerical_cols\n\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\n\n# Making test data set (we will use 2011 data as test data)\n\nX_test_full, y_test_full = select_dat_specific_year(2011, fraction=0.1)\nX_test = X_test_full[my_cols].copy()\ny_test = y_test_full.copy()","e7a1d875":"# Verify the number of missing values for each numeric variable\nmissing_val_count_by_column = (X_train.isnull().sum())\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])\n\n# Fill in the lines below: imputation\n\n# Imputation for numerical_cols\nmy_imputer = SimpleImputer(strategy=\"mean\") \nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train[numerical_cols]))\nimputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid[numerical_cols]))\nimputed_X_test = pd.DataFrame(my_imputer.transform(X_test[numerical_cols]))\n\n# Fill in the lines below: imputation removed column names; put them back\nimputed_X_train.columns = numerical_cols\nimputed_X_valid.columns = numerical_cols\nimputed_X_test.columns = numerical_cols\n\n# Put index\nimputed_X_train.index=X_train.index\nimputed_X_valid.index=X_valid.index\nimputed_X_test.index=X_test.index\n","0cf384af":"# condition\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\n# do one-hot encode the categorical_cols\nOH_X_train0 = pd.DataFrame(OH_encoder.fit_transform(X_train[categorical_cols])) \nOH_X_valid0 = pd.DataFrame(OH_encoder.transform(X_valid[categorical_cols]))\nOH_X_test0 = pd.DataFrame(OH_encoder.transform(X_test[categorical_cols]))\n\n# put index \nOH_X_train0.index = X_train.index\nOH_X_valid0.index = X_valid.index\nOH_X_test0.index = X_test.index\n\n# combine encoded categorical_cols with numeric variables + rest of variables ('BuildId', 'yearmon')\nOH_X_train = pd.concat([imputed_X_train, OH_X_train0, X_train[['BuildId', 'yearmon']]], axis=1)\nOH_X_valid = pd.concat([imputed_X_valid, OH_X_valid0, X_valid[['BuildId', 'yearmon']]], axis=1)\nOH_X_test = pd.concat([imputed_X_test, OH_X_test0, X_test[['BuildId', 'yearmon']]], axis=1)","20461f74":"# Add the second power of size to capture the nonlinearity of the effect of size on house prices.\n\ndef second_power(series):\n    # series is changed to numpy array.\n    \n    tmp=np.array(series)\n    tmp2=tmp**2\n    \n    # the new series is changed to panda series.\n    \n    tmp3=pd.Series(tmp2)\n    tmp3.index=series.index\n    \n    return tmp3\n\n# Apply the fct defined above.\n\nOH_X_train['Size2']=second_power(OH_X_train['Size'])\nOH_X_valid['Size2']=second_power(OH_X_valid['Size'])\nOH_X_test['Size2']=second_power(OH_X_test['Size'])\n\n\n# Verify if there is missing values in the new series\n\nOH_X_train['Size2'].isnull().sum()\nOH_X_valid['Size2'].isnull().sum()\nOH_X_test['Size2'].isnull().sum()","72efbaa5":"# Add the number of APT sales of the month happened by APT complex\n\ndef count_past_sales(series):\n    \n    # time_stamp\n    \n    series2 = pd.to_datetime(series)\n    sale_time = pd.Series(series2.index, index=series2, name='count_sales_this_month').sort_index() # exchange the positions of index and values\n    count_1month= sale_time.rolling('30D', min_periods=1).count()\n    count_1month_2=count_1month.groupby(count_1month.index.month).transform('last')\n    return count_1month_2\n\naa=OH_X_train.groupby('BuildId')['yearmon'].apply(count_past_sales)\nbb=OH_X_valid.groupby('BuildId')['yearmon'].apply(count_past_sales)\ncc=OH_X_test.groupby('BuildId')['yearmon'].apply(count_past_sales)\n\n# put index\naa.index=OH_X_train.sort_values(by=['BuildId','yearmon']).index\nbb.index=OH_X_valid.sort_values(by=['BuildId','yearmon']).index\ncc.index=OH_X_test.sort_values(by=['BuildId','yearmon']).index\n\nOH_X_train['num_sales_same_month_by_complex']=aa\nOH_X_valid['num_sales_same_month_by_complex']=bb\nOH_X_test['num_sales_same_month_by_complex']=cc\n\n# drop 'BuildId' and 'yearmon' columns\nOH_X_train.drop(['BuildId', 'yearmon'], axis=1, inplace=True)\nOH_X_valid.drop(['BuildId', 'yearmon'], axis=1, inplace=True)\nOH_X_test.drop(['BuildId', 'yearmon'], axis=1, inplace=True)\n","7a189d40":"# Select the optimal set of features (using LogisticRegression classifier)\n## We search for the best value of 'K' which is the number of variables that will be used in training\n\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.feature_selection import SelectFromModel\n\n# def select_features_l1(X, y):\n#     \"\"\"Return selected features using logistic regression with an L1 penalty.\"\"\"\n#     logistic=LogisticRegression(C=0.1, penalty=\"l1\", solver='liblinear', random_state=7).fit(X,y)\n#     model=SelectFromModel(logistic, prefit=True)\n\n#     X_new=model.transform(X)\n#     selected_features=pd.DataFrame(model.inverse_transform(X_new), index=X.index, columns=X.columns)\n#     selected_col=selected_features.columns[selected_features.var()!=0]\n#     return selected_col\n\n# select_col=select_features_l1(OH_X_train, y_train)\n\n# X_train_reg=OH_X_train[select_col]\n# X_valid_reg=OH_X_valid[select_col]\n# X_test_reg=OH_X_test[select_col]","d2da3e8d":"# Description of the APT prices (target variable)\ny_train.describe()","74b25213":"# Plot Histogram\nsns.distplot(y_train, fit=norm)\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(y_train)\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\nfig = plt.figure()\nres = stats.probplot(y_train, plot=plt)\nplt.show()\n\nprint(\"Skewness: %f\" % y_train.skew())\nprint(\"Kurtosis: %f\" % y_train.kurt())","7e9dcbfd":"# We do a same analysis using log(price)\nlogy = np.log(y_train)\n\n# Plot Histogram\nsns.distplot(logy, fit=norm)\n\n# Getting the fitted parameters used by the function\n(mu, sigma) = norm.fit(logy)\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\nfig = plt.figure()\nres = stats.probplot(logy, plot=plt)\nplt.show()\n\nprint(\"Skewness: %f\" % logy.skew())\nprint(\"Kurtosis: %f\" % logy.kurt())","c4ebfa90":"# APT house prices by district\n\nvar = 'District'\ndata = pd.concat([y_train, X_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"price\", data=data)\nfig.set_xticklabels(ax.get_xticklabels(),rotation=90)\nfig.axis(ymin=0, ymax=3000000)\n","391a030a":"# APT house prices by school district\n\nvar = 'schoolDistHs'\ndata = pd.concat([y_train, X_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6));\nfig = sns.boxplot(x=var, y=\"price\", data=data)\nfig.set_xticklabels(ax.get_xticklabels(),rotation=30)\nfig.set_xlabel('School District')\nfig.set_ylabel('APT Price')\nfig.axis(ymin=0, ymax=3000000)\n","a6d8b68f":"# School quality by school district\n\nvar = 'schoolDistHs'\nxx = X_train['new..prop..snu23']\ndata = pd.concat([xx, X_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6));\nfig = sns.boxplot(x='schoolDistHs', y='new..prop..snu23', data=data)\nfig.set_xticklabels(ax.get_xticklabels(),rotation=30)\nfig.set_xlabel('School District')\nfig.set_ylabel('School quality')\nfig.axis(ymin=0, ymax=5)","dcb16349":"# Apt size vs Sale Price\n\nfig= sns.jointplot(x=X_train['Size'], y= y_train, kind=\"reg\" )\nfig.ax_joint.set_xlabel('APT size')\nfig.ax_joint.set_ylabel('Price')","c999bd50":"# Apt floor vs Sale Price\n\n\nfig = sns.jointplot(x=X_train['Floor'], y= y_train, kind='reg')\nfig.ax_joint.set_xlabel('Floor')\nfig.ax_joint.set_ylabel('Price')","c74e40be":"# School quality vs Sale Price\n\nfig = sns.jointplot(x=X_train['new..prop..snu23'], y= y_train, kind='reg')\nfig.ax_joint.set_xlabel('School quality')\nfig.ax_joint.set_ylabel('Price')","f99a95bb":"# Number of Households vs Sale Price\n\nfig = sns.jointplot(x=X_train['Hhld'], y= y_train, kind='reg')\nfig.ax_joint.set_xlabel('Number of households')\nfig.ax_joint.set_ylabel('Price')","d4db1e54":"# No building vs Sale Price\n\nfig = sns.jointplot(x=X_train['maxBuild'], y= y_train, kind='reg')\nfig.ax_joint.set_xlabel('Number of building')\nfig.ax_joint.set_ylabel('Price')","78fb297e":"# Define a fct to get the mean absolute error values for each 'min_samples_leaf' values\n\ndef get_mae(msl, train_X, val_X, train_y, val_y):\n    \n    # Define the model. Set random_state to 1\n    rf_model = RandomForestRegressor(n_estimators=400, random_state=1, criterion = \"mse\", min_samples_leaf=msl, max_features=20)\n\n    # Fit your model\n    rf_model.fit(train_X, train_y)\n    \n    # predict\n    pred = rf_model.predict(val_X)\n    \n    # Calculate the mean absolute error of your Random Forest model on the validation data\n    rf_val_mae = mean_absolute_error(val_y, pred)\n\n    print(\"Validation MAE for Random Forest Model: {}\".format(rf_val_mae))\n   \n    return(rf_val_mae)\n\n# Repeated training with different set of parameters 'min_samples_leaf'\n\ncandidate_min_samples_leaf = [1,2,3,5,10,20,40,80]\n\n# Write a loop function to find the ideal tree size from candidate_max_leaf_nodes\n\nset_mae=[] \n\nfor msl in candidate_min_samples_leaf:\n    set_mae.append(get_mae(msl, OH_X_train, OH_X_valid, y_train, y_valid))\n    \n\n# Store the best value of min_samples_leaf \nbest_leaf_size = candidate_min_samples_leaf[set_mae.index(min(set_mae))]    \n\nprint(\"Value of MAE for the optimal Random Forest Model is as follows:\")\n\nget_mae(best_leaf_size, OH_X_train, OH_X_valid, y_train, y_valid)\n\n\nprint(\"Value of MAE for the optimal Random Forest Model is as follows (test data):\")\n\nget_mae(best_leaf_size, OH_X_train, OH_X_test, y_train, y_test)\n","c88a235b":"print(\"The best parameter for 'min_samples_leaf' is:\", best_leaf_size)","a570b59c":"\n# Define a fct to get mae values. We use this fct to find an optimal set of parameters in using the XGboost algorithm\n\ndef get_mae_xgb(mymodel, earlyval=None, learning_rate=None, xdat = OH_X_valid, ydat= y_valid):\n    \n    my_model=mymodel\n    \n    if earlyval is not None:\n        my_model.fit(OH_X_train, y_train, \n                     early_stopping_rounds=earlyval, \n                     eval_set=[(OH_X_valid, y_valid)],\n                     verbose=False)  \n    else:    \n        my_model.fit(OH_X_train, y_train)\n    \n    predictions = my_model.predict(xdat)\n    \n    mae= mean_absolute_error(predictions,ydat)\n    \n    if earlyval is not None:\n        print('===ealry stopping rounds vary===')\n        print(\"Mean Absolute Error: \" + str(mae))\n    \n    elif learning_rate is not None:\n        print('===learning rates vary===')\n        print(\"Mean Absolute Error: \" + str(mae))\n    \n    else:\n        print(\"Mean Absolute Error: \" + str(mae))\n\n    return mae\n","81a1e0c6":"# Only with 'default parameter' values\n\nmy_model = XGBRegressor(random_state=0)\n\n\nprint(\"Value of MAE for the optimal Random Forest Model is as follows:\")\nget_mae_xgb(my_model)\n\nprint(\"Value of MAE for the optimal Random Forest Model is as follows (test data):\")\nget_mae_xgb(my_model,earlyval=None, learning_rate=None, xdat= OH_X_test, ydat= y_test)\n","8a2948bf":"# Varying 'n_estimators' values\n\nnestim=[300,400,500,1000,1500,2000]\n\nset_mae=[]\n\nfor nn in nestim:\n    \n    my_model1 = XGBRegressor(random_state=0, n_estimators=nn)\n    set_mae.append(get_mae_xgb(my_model1))\n\n    \n\n# Mean absolute error values with the best parameter\n\nbest_nestime_size = nestim[set_mae.index(min(set_mae))]    \nmy_best_model1 = XGBRegressor(random_state=0, n_estimators=best_nestime_size)\n\nprint(\"Value of MAE for the optimal Random Forest Model is as follows:\")\nget_mae_xgb(my_best_model1)\n\n\nprint(\"Value of MAE for the optimal Random Forest Model is as follows (test data):\")\nget_mae_xgb(my_best_model1, xdat= OH_X_test, ydat= y_test)","d37335a4":"print(\"The best parameter for 'n_estimator' is:\", best_nestime_size)\n","bfaae44b":"# Varying 'early stopping rounds' values\n\n\nvall=[5,10,15,20,25,30,40,50,100] # candidates of early stop rounds\n\nset_mae=[]\n\nfor val in vall:\n    my_model2 = XGBRegressor(random_state=0, n_estimators=best_nestime_size)\n    set_mae.append(get_mae_xgb(my_model2, earlyval=val))\n\n# Mean absolute error values with the best parameter\n\nbest_est = vall[set_mae.index(min(set_mae))]    \n\nprint(\"Value of MAE for the optimal Random Forest Model is as follows:\")\nget_mae_xgb(my_model2, earlyval=best_est)\n\n\nprint(\"Value of MAE for the optimal Random Forest Model is as follows (test data):\")\nget_mae_xgb(my_model2, earlyval=best_est, xdat= OH_X_test, ydat= y_test)","e7607fd1":"print(\"The best parameter for 'early stopping rounds' is:\", best_est)\n","c68b6176":"# Varying 'learning rate' values\n\nLRval=[0.01,0.02,0.04, 0.05, 0.1, 0.2,0.4, 0.5]\nval=best_est\n\nset_mae=[]\n\nfor LRvall in LRval:\n    my_model3 = XGBRegressor(random_state=0,n_estimators= best_nestime_size, learning_rate=LRvall)\n    set_mae.append(get_mae_xgb(my_model3, earlyval=val, learning_rate = LRvall))\n    \n# Mean absolute error values with the best parameter\n\nbest_lr = LRval[set_mae.index(min(set_mae))]    \n\nprint(\"Value of MAE for the optimal Random Forest Model is as follows:\")\nget_mae_xgb(my_model3, earlyval=val, learning_rate=best_lr)\n\n\nprint(\"Value of MAE for the optimal Random Forest Model is as follows (test data):\")\nget_mae_xgb(my_model3, earlyval=val, learning_rate=best_lr, xdat= OH_X_test, ydat= y_test)","e6a60ba6":"print(\"The best parameter for 'learning rate' is:\", best_lr)\n","d44313a8":"# Using GridSearchCV() to find the best combination of parameters\n\n## First trial\n\n# param_grid= {\n#     'max_depth': [3,4,5],\n#     'learning_rate': [0.1,0.01,0.05],\n#     'gamma': [0,0.25,1],\n#     'n_estimators': [500, 600, 700],   \n# }\n\n## 1st trial\n#{'early_stopping_rounds': 30, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 700}\n\n\n## Second trial \n# param_grid= {\n#     'max_depth': [5,6,7],\n#     'learning_rate': [0.1,0.2,0.3],\n#     'gamma': [0,0.05,0.01],\n#     'n_estimators': [700,800,900],\n# }\n\n\n\n## 2nd trial\n#{'early_stopping_rounds': 30, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 900}\n\n## Third trial \n# param_grid= {\n#     'max_depth': [6],\n#     'learning_rate': [0.1],\n#     'gamma': [0],\n#     'n_estimators': [1100,1500,2000,2500],\n# }\n\n## 3rd trial\n#{'early_stopping_rounds': 30, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 1500} # best combination of parameters!!\n\n\n## Best parameters obatined through GridSearchCV()\n\nbest_est = 30\n\nparam_grid = {\n            'max_depth': [6],\n            'learning_rate': [0.1],\n            'gamma': [0],\n            'n_estimators': [1500]\n}\n\n    \nXGB=XGBRegressor(random_state=0, \n                         colsample_bytree=0.5)\n\nXGB_grids = GridSearchCV(estimator = XGB,\n                         param_grid=param_grid,\n                         verbose=0,\n                         cv=5)\n\n\n# fitting the model for grid search \n\nXGB_grids.fit(OH_X_train, y_train,\n              early_stopping_rounds = best_est,\n              eval_set=[(OH_X_valid, y_valid)], \n             verbose=False)\n\n\n# Results with the best parameters\nprint('The best model is: ', XGB_grids.best_params_)\nprint('This model produces a mean cross-validated score (precision) of', XGB_grids.best_score_)\n\n# getting the mean absolute error using the test dataset and the best parameters\ny_true, y_pred = y_valid, XGB_grids.predict(OH_X_valid)\n\nprint('MAE on the evaluation set: ',mean_absolute_error(y_true, y_pred))\n\n\n# getting the mean absolute error using the test dataset and the best parameters\ny_true, y_pred = y_test, XGB_grids.predict(OH_X_test)\n\nprint('MAE on the evaluation set: ',mean_absolute_error(y_true, y_pred))\n","feb963cc":"# Modeling and finding the set of parameters optimal for the algorithm\n\nmodelsvr = SVR()\n\n# first trial \n# param = {'kernel' : ['rbf'],\n#          'C' : [1,3,5,7,9,11,13,15],\n#          'gamma' : ['auto']}\n\n# the best set of parameters\n\nparam = {'kernel' : ['rbf'],\n         'C' : [9],\n         'gamma' : ['auto']}\n\nsvr_grids = GridSearchCV(estimator= modelsvr,\n                         param_grid= param,\n                         cv=5)\n\nsvr_grids.fit(OH_X_train, y_train)\n\n\n\n# Results with the best parameters\n\nprint(svr_grids.cv_results_)\nprint('The best model is: ', svr_grids.best_params_)\nprint('This model produces a mean cross-validated score (precision) of', svr_grids.best_score_)\n\n\n# getting the mean absolute error using the test dataset and the best parameters\n\ny_true, y_pred = y_test, svr_grids.predict(OH_X_test)\nprint('MAE on the evaluation set: ',mean_absolute_error(y_true, y_pred))\n\n","4b5f1003":"\n# Modeling and finding the set of parameters optimal for the algorithm\n\ndef sklearn_reg(train_data, label_data, test_data, k_num):\n    knn = neighbors.KNeighborsRegressor(n_neighbors=k_num, weights='uniform', algorithm='auto')\n    # Train\n    knn.fit(train_data, label_data)\n    # Predict\n    predict_label = knn.predict(test_data)\n    # Return\n    return predict_label\n                               \n                               \n# Predict using the training data set, assuming k = 5 \n\ny_predict = sklearn_reg(OH_X_train, y_train, OH_X_valid, 5)\ny_predict\n                               \n\n# mean absolute error calculation\n\ndef get_mae_knear(predictions, y_valid):  \n    mae = mean_absolute_error(predictions, y_valid)\n    return mae\n    \n                               \nget_mae_knear(y_predict, y_valid)\n\n\n# Search for the value 'k' that leads to the minimum absolute error values\n                               \nnormal_mae = []  # Create an empty list of accuracy rates\nk_value = range(2, 30)\n\n\nfor k in k_value:\n    y_predict = sklearn_reg(OH_X_train, y_train, OH_X_valid, k)\n    mae = get_mae_knear(y_valid, y_predict)\n    normal_mae.append(mae)\n\n# draw a graph of accuracy level across 'k'\n\nplt.xlabel(\"k\")\nplt.ylabel(\"Mean absolute error\")\nnew_ticks = np.linspace(0.6, 0.9, 30)  # Set the y-axis display\nplt.yticks(new_ticks)\nplt.plot(k_value, normal_mae, c='r')\nplt.grid(True)  # Add grid\n      \n\n# calculate mae usig test data and best parameter 'k'\nbest_k=k_value[normal_mae.index(min(normal_mae))]\nget_mae_knear(sklearn_reg(OH_X_train, y_train, OH_X_valid,best_k), y_valid)\nget_mae_knear(sklearn_reg(OH_X_train, y_train, OH_X_test,best_k), y_test)\n","6b196b41":"# Modeling and finding the set of parameters optimal for the algorithm\n\nMLP = MLPClassifier(random_state=0)\n\n# First trial\n# param_list = {\"hidden_layer_sizes\": [(100,),(300,),(500,)], \n#               \"activation\": [\"logistic\", \"relu\"], \n#               \"solver\": [\"lbfgs\", \"adam\"], \n#               \"alpha\": [0.001,0.005], # L2 penalty (regularization term) parameter\n#               \"batch_size\": [100,200]\n#              }\n\n# Second trial\n# param_list = {\"hidden_layer_sizes\": [(100,),(50,50), (50,25,25)], \n#               \"activation\": [\"logistic\", \"relu\"], \n#               \"solver\": [\"lbfgs\", \"adam\"], \n#               \"alpha\": [0.001], # L2 penalty (regularization term) parameter\n#               \"batch_size\": [100]\n#              }\n\n# Third trial\n\n# param_list = {\"hidden_layer_sizes\": [(100,)], \n#               \"activation\": [\"logistic\", \"relu\"], \n#               \"solver\": [\"lbfgs\", \"adam\"], \n#               \"alpha\": [0.001,0.005], # L2 penalty (regularization term) parameter\n#               \"batch_size\": [100,200]\n#              }\n\n# Final model (w the set of best parameters)\n\nparam_list = {\"hidden_layer_sizes\": [(100,)], \n              \"activation\": [\"logistic\"], \n              \"solver\": [\"lbfgs\"], \n              \"alpha\": [0.001], # L2 penalty (regularization term) parameter\n              \"batch_size\": [100]\n             }\n\n\nMLP_grids = GridSearchCV(estimator= MLP,\n                     param_grid= param_list,\n                     cv=5)\n\nMLP_grids.fit(OH_X_train, y_train)\n\n# Results with the best parameters\nprint(MLP_grids.cv_results_)            \nprint('The best model is: ', MLP_grids.best_params_)\nprint('This model produces a mean cross-validated score (precision) of', MLP_grids.best_score_)\n\n\n# getting the mean absolute error using the test dataset and the best parameters\ny_true, y_pred = y_test, MLP_grids.predict(OH_X_test)\nprint('MAE on the evaluation set: ',mean_absolute_error(y_true, y_pred))","5a13daa6":"### Summmary of MAE values from different algorithms \n\n## Random Forest model \n\nprint(\"Value of MAE for the Random Forest Model on the evaluation set is as follows (validation data):\")\nget_mae(best_leaf_size, OH_X_train, OH_X_valid, y_train, y_valid)\n\nprint(\"Value of MAE for the Random Forest Model on the evaluation set is as follows (test data):\")\nget_mae(best_leaf_size, OH_X_train, OH_X_test, y_train, y_test)\n\n\n## XGBoost algorithm + Random forest model\n\ny_true, y_pred = y_valid, XGB_grids.predict(OH_X_valid)\nprint('Value of MAE for the XGB on the evaluation set is as follows (validation data): ',mean_absolute_error(y_true, y_pred))\n\ny_true, y_pred = y_test, XGB_grids.predict(OH_X_test)\nprint('Value of MAE for the XGB on the evaluation set is as follows (test data): ',mean_absolute_error(y_true, y_pred))\n\n\n## support vector regressor\ny_true, y_pred = y_valid, svr_grids.predict(OH_X_valid)\nprint('Value of MAE for the Support Vector regressor on the evaluation set (validation data): ',mean_absolute_error(y_true, y_pred))\n\ny_true, y_pred = y_test, svr_grids.predict(OH_X_test)\nprint('Value of MAE for the Support Vector regressor on the evaluation set (test data): ',mean_absolute_error(y_true, y_pred))\n\n\n##  k nearest algorithm \nbest_k=k_value[normal_mae.index(min(normal_mae))]\nprint('Value of MAE for the k neareast algorithm on the evaluation set (validation data): ',get_mae_knear(sklearn_reg(OH_X_train, y_train, OH_X_valid,best_k), y_valid))\nprint('Value of MAE for the k neareast algorithm on the evaluation set (validation data): ',get_mae_knear(sklearn_reg(OH_X_train, y_train, OH_X_test,best_k), y_test))\n\n\n##  Multi layer perceptron\ny_true, y_pred = y_valid, MLP_grids.predict(OH_X_valid)\nprint('Value of MAE for the Multi layer perceptron on the evaluation set (validation data): ',mean_absolute_error(y_true, y_pred))\ny_true, y_pred = y_test, MLP_grids.predict(OH_X_test)\nprint('Value of MAE on  the Multi layer perceptron on the evaluation set (test data): ',mean_absolute_error(y_true, y_pred))\n\n","c3bc8bd4":"# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='mean')\n\n# Preprocessing for categorical data using \"Pipeline\"\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n])\n\n# Bundle preprocessing for numerical and categorical data using \"ColumnTransformer\"\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n\n\n# Define model (optmal model + optimal parameter)\nmodel = RandomForestRegressor(n_estimators=100, random_state=0)\n\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)\n                     ])\n\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\n\n\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(X_valid)\n\nprint('MAE:', mean_absolute_error(y_valid, preds))\n\n\n# Preprocessing of test data, fit model\npreds_test = my_pipeline.predict(X_test)\n\n\n# Save test predictions to file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds_test})\noutput.to_csv('submission.csv', index=False)\n\n","70d5f546":"# Data loading\n\n","14fc63c3":"* Using the default value of parameters, we calculated Mean absolute error values","af0b72b3":"* For more details on the Support vector machine, see this [video.](https:\/\/www.youtube.com\/watch?v=efR1C6CvhmE&t=21s)\n* [Other materials on SVM (SVC)](https:\/\/www.kaggle.com\/fengdanye\/machine-learning-4-support-vector-machine)\n* [Materials on SVR](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVR.html)","add303c7":"# Default setting","970052e0":"## 2.2 Optimizing parameters using Cross Validation and GridSearchCV()\n\n* Until now, we tried to find the optimal set of hyperparameters of XGboost in a rather manual way, and we found that there is a limit to find the best combination of the parameters in this way (keeping the optimal parameters in a row does not lead us to a lower MAE).\n* However, here we use a more elaborate method 'GridSearchCV()' to get the optimal set of hyperparameters since XGboost has a lot of hyperparameters that we have to manually configure, including max_depth (maximum tree depth), learning_rate (the learning rate), gamma (a parameter that encourages pruning), and reg_lambda (the regularization parameter).","13f12c7f":"* Comment: when we transform the APT prices in log(price), its distribution becomes closer to the normal distribution.\n\n* In this analysis, we do not delve into the effect of the features we mentioned, and we are just want to get the prediction and compare the prediction power of several models. \n\n* If we want to know the significance of the effects of the features on house prices, the assumption that prediction error should follow a normal distribution with mean 0 should be satisfied, since the calculation of confidence interval and variable significance is based on this assumption. \n\n* As mentioned before, since this is not a main interest of this analysis, we do not fix the non normality (skewness) of the target variable (price) for this analysis.\n\n* If we do want to select the significant predicting factors, after constructiong a model and predicting, we should plot the chart to see the distribution of prediction error ([cf](https:\/\/towardsdatascience.com\/is-normal-distribution-necessary-in-regression-how-to-track-and-fix-it-494105bc50dd)).","61e82007":"## 4. k-nearest neighbors algorithm (KNN algorithm)","58d16386":"* Now we want to vary the value of n_estimators.","53944d93":"# Features used","247868b4":"* ## Imputation of numeric variables\n\nWe impute missing values with the mean value along each column of numerical variables. \n","a54a6370":"# Preprocessing of features","daa6fcf2":"# Parsing data \n\n## * Average house prices in 2012\n* the average APT price of Seoul is $ 449597 in 2012.\n","76d42019":"## 1. Random Forests\n\n* More details on the model can be found [here.](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html)\n* Here, we also deal with overfitting problem. Different from the case of using Logisticregression classifier, here we select the value of max_leaf_nodes that gives the most accurate model on your data.\n> * Actually, we can also use other parameters such as \"n_estimators\", \"max_features\", and \"max_depth\".\n> * The effect of \"max_depth\" on over-fitting probabilities is similar to the one of \"min_samples_leaf\".\n> * From the analysis with different combination of parameters, we found that the set of parameters leading to the minimum value of MAE of 'n_estimators', 'min_samples_leaf', and 'max_features' are about '400', '1', and '20', respectively.\n* For more details on regularization problem in the RandomForest classifier, see [here](https:\/\/www.quora.com\/How-could-I-regularize-random-forest-classifiers) or [this](https:\/\/stackoverflow.com\/questions\/20463281\/how-do-i-solve-overfitting-in-random-forest-of-python-sklearn) and [that.](https:\/\/www.analyticsvidhya.com\/blog\/2015\/06\/tuning-random-forest-model\/)","fbb83cd1":"# Adding more features and\/or selecting the optimal set of features\n\n* We can add more features by transforming the existing features and\/or limit the number of features to be used for training machines to improve the performances (reducing over-fitting problem).\n* [Reference 1 for feature generation](https:\/\/www.kaggle.com\/mldaniella\/exercise-feature-generation)\n* [Reference 2 for feature selection](https:\/\/www.kaggle.com\/mldaniella\/exercise-feature-selection), [and another reference](https:\/\/blog.datadive.net\/selecting-good-features-part-ii-linear-models-and-regularization\/)","3e6e3059":"* ## Encoding categorical variables\n\n* we encode the categorical variables using the one-hot encoding way.\n* then, we combine encoded categorical variables with numeric variables.\n","99bbdfd8":"## 5. Multi-layer perceptron","f9be5289":"# Models and algorithms used\n\n* For this analysis, we implement those algorithms:\n\n> 1. Random Forests\n> 2. XGBoost\n> 3. Support vector regression\n> 4. k-nearest neighbors\n> 5. Multi-layer perceptron\n\n* Note:\n\n> * In order to find the best model, we use the metric of mean absolute errors. \n> * While using the function GridSearchCV() to find the optimal set of parameters for the algorithms, the Cross Validation score implemented and used is R^2.\n\n","9d3ea3c2":"# Pipeline ","0213163b":"* ## Making sets of data for training and validation of models","d6eae104":"\n# Predicting apartment prices in Seoul\n\nMi Lim Kim - October 2020\n","ba482b1c":"## 3. Support vector machine ","4785a2e8":"# Conclusion\n\n* The summary of MAE values obtained from the differnet algorithm is as follows.\n* With validation data\n* With test data\n\n","8fbc7b10":"## * Average house prices by school district.\n","0601a3b0":"# Questions\n\nThe questions we want to answer by doing this analysis are as follows.\n\n1. Which machine is the best model to predict apartment prices of Seoul? \n2. What is the set of characteristics (features) srelated to the apartments that should be considered to predict house prices effectively?\n","3b1056a2":"* Distributions of APT prices across districts are quite different. ","6d473d7c":"* We select the features ('features_selected') and the target variable ('y').\n\n* target variable **'y'**: apartment transaction price in 2012.\n* the variables **'features_slected'** and explanations on those variables are as follows:\n\n> 1. `BuildId`: apartment complex ID. It indentifies that the apartment belongs to which APT complex.\n> 2. `District`: name of district (borough) where the APT (or APT complex) is located.\n> 3. `maxBuild`: the number of apt building in the APT complex. In South Korea, the apartment complex with many households is more appreciated since when new amenities are introduced for the complex, economic burden for each household is smaller then it facilitates to construct amenities for the complex.\n> 4. `Built_year`: the construction year of the APT complex.\n> 5. `Hhld`: the number of household living in the APT complex.\n> 6. `yearmon`: the year and month when the apartment was sold.\n> 7. `Floor`: floor of the APT\n> 8. `Size`: size of the APT\n> 9. `dong`: name of subdistrict where the APT is located.\n> 10. `schoolDistHS`: name of the school district where the APT is located\n> 11. `new..prop..snu23`: measure of school quality of the school district\n> 12. `Age_complex`: age of the complex where the APT belongs.\n\n* We can see that some features represent same (or similar) characteristics fo the apartments (e.g. 'Age_complex' and 'Built_year'). We deal whith this problem in a later part of this analysis.\n","40a57739":"* Using, GridSearchCV(), we tried to find the best combinations of parameters (especially, 'C' (Regularization parameter))","b01f7c92":"* Conclusion: we found that the `Random forest model` gives the best prediction result.","c95887d9":"* For more details on the k-nearest neighbors algorithm, see this [video.](https:\/\/www.youtube.com\/watch?v=HVXime0nQeI)\n> * We cluster the observations by [PCA](https:\/\/www.youtube.com\/watch?v=FgakZw6K1QQ) first then apply the k nearest neighbors algorithm?[](http:\/\/)","5d280502":"* More details on the model can be found [here](https:\/\/www.kaggle.com\/victorcoo\/learn01-titanic-with-mlpclassifier-mlpregressor) and [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPRegressor.html)","bcc3acb3":"## 2.1 Parameter tuning problem\n\n* More details on parameter tuning can be found [here.](https:\/\/www.kaggle.com\/alexisbcook\/xgboost)\n\n* XGBoost has a few parameters that can dramatically affect accuracy and training speed. \n\n> * **n_estimators**: n_estimators indicates the number of models that we include in the ensemble.\n> > * Too low value causes underfitting, and too high value causes overfitting.\n\n> * **early_stopping_rounds**: early_stopping_rounds indicates the minimum round where the model would stop if the validation score stops improving. It's better to set a high value for 'n_estimators' and then use early_stopping_rounds to find the optimal time to stop iterating.\nFor instance, setting early_stopping_rounds=5 means that we stop after 5 straight rounds of deteriorating validation scores.\n\n> * **learning_rate**: \nInstead of getting predictions by simply adding up the predictions from each component model, we can multiply the predictions from each model by a small number (known as the learning rate) before adding them in. As default, XGBoost sets learning_rate=0.1.\n\n> * **n_jobs**:\nOn larger datasets where runtime is a consideration, you can use parallelism to build your models faster. It's common to set the parameter n_jobs equal to the number of cores on your machine. On smaller datasets, this won't help.\n\n\n","8fb57f79":"## * Average house prices by district.\n\n* We look into a boxplot that represents the summary statistics (min, 1Q, Median, 3Q, max, outliers) of APT prices across districts.","d8bf595d":"* Comment: when we use the raw apartment prices, the APT prices seems not fit to normal distribution. We can find a same phenomenon in the quantile                 plot.","92655d44":"## 2. XGBoost\n\n* XGBoost stands for Extreme Gradient Boosting and it is an implementation of gradient boosting trees algorithm.\n* More details on this algorithm can be found [here.](https:\/\/www.kaggle.com\/mldaniella\/exercise-xgboost) and [there](https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/model.html)\n* [the evolution of tree based algorithm from decision tree to XGboost](https:\/\/towardsdatascience.com\/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d)","3edd1e1f":"## * Average house prices across the apt size, the floor, and the number of households.\n\n* For the details on the seaborn graphs (jointplot), see [here](https:\/\/seaborn.pydata.org\/generated\/seaborn.jointplot.html)\n","2e91a8a5":"* We construct a pipeline for predicting the ATP prices in Seoul. For this analysis, we do not add additional features tha cannot be implemented using the preprocessing functions.\n\n* For more details on Pipelines and its advantages, see [this note.](https:\/\/www.kaggle.com\/dansbecker\/pipelines)\n","f40c51c6":"We utilise apartment sales prices of Seoul registered in 2012.\n\nThe features of apartments can be categorized with severeal levels.\n- Individual apartment level feature (e.g. size, floor)\n- APT complex level feature (e.g. construction year, number of apartments per complex)\n- District (borough) level feature (e.g. school quality)\n\n\n","2ad5ac4c":"* Now we want to vary the value of 'learning rate', keeping the values of 'n_estimator' and 'early stopping rounds' constant.","26d594b1":"# Packages used\n\nFirst, we load some packages by executing the lines below to make an environment for this analysis.","cdac7864":"* We found that the APT prices across school districts are quite different.\n* These variations in prices are due to the variations in the school quality.\n> * school quality is measured as the percentage of students that are accepted to the best university (SNU) of the nearest 3 high schools from the APT.","88c8cbb4":"# Outline\n\nWe will follow those steps to predict house prices of the apartments in Seoul.\n\n\n1. Program Setting and Data Loading\n2. Preprocessing features \n > * numeric & categorical variables (imputation & encoding)\n > * adding new features & regularization\n3. Implementation of models and algorithms\n4. Fitting, prediction and evaluation","5ed19e18":"* Now we want to vary the value of 'early stopping rounds', keeping the value of n_estimator fixed (best_nestime_size)"}}