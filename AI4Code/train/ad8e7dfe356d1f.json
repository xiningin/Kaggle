{"cell_type":{"796398c0":"code","91a1b395":"code","3ebee267":"code","d08d18cf":"code","b581a6c8":"code","c00cfc74":"code","e5296a5a":"code","c879ebc0":"code","4df41e29":"code","aeb03d7f":"code","cf1ef360":"code","24a314d2":"code","67ed5e83":"code","2ceef0f4":"code","8ff4161b":"code","9a85cc41":"markdown","067a087f":"markdown","6731229e":"markdown","74017f5b":"markdown","a028fca4":"markdown","eac7f6fb":"markdown","5e0c17c4":"markdown","d88348ca":"markdown","6d1a0e3d":"markdown","75b2e4c7":"markdown","0b027ecd":"markdown","b305864d":"markdown","f9580e36":"markdown","7296a0eb":"markdown","24a63fc8":"markdown","d3d42634":"markdown","e5167deb":"markdown","d51ed1ea":"markdown","a38be72b":"markdown","4d5cbc39":"markdown","b67a1726":"markdown"},"source":{"796398c0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import ensemble\nfrom scipy.stats import spearmanr\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nimport copy\n\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\n\nimport lightgbm as lgb\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder","91a1b395":"%%time\nsctp_train = pd.read_csv('..\/input\/train.csv')\nsctp_test = pd.read_csv('..\/input\/test.csv')\nprint(\"==\"*45)\nprint(\"The training dataset has {0} rows and {1} columns\".format(sctp_train.shape[0], sctp_train.shape[1]))\nprint(\"The testing dataset has {0} rows and {1} columns\".format(sctp_test.shape[0], sctp_test.shape[1]))\nprint(\"==\"*45)","3ebee267":"sctp_train.info()","d08d18cf":"sctp_train.describe()","b581a6c8":"## target Proportion ##\ncnt = sctp_train['target'].value_counts()\ntr_prop = go.Bar(\n    x=cnt.index,\n    y=cnt.values,\n    marker=dict(\n        color=cnt.values,\n        colorscale = 'Picnic',\n        reversescale = True\n    ),\n)\n\nlayout = go.Layout(\n    title='Target Proportion',\n    font=dict(size=18)\n)\n\ndata = [tr_prop]\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename=\"TargetProp\")\n\n## target distribution ##\nlabels = (np.array(cnt.index))\nsizes = (np.array((cnt \/ cnt.sum())*100))\n\ntr_pie = go.Pie(labels=labels, values=sizes)\nlayout = go.Layout(\n    title='Target Pie',\n    font=dict(size=18),\n    width=600,\n    height=600,\n)\ndata = [tr_pie]\nfig = go.Figure(data=data, layout=layout)\niplot(fig, filename=\"pie_ty\")","c00cfc74":"def missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns\n\nmissing_values_table(sctp_train)","e5296a5a":"labels = []\nvalues = []\nfor col in sctp_train.columns:\n    if col not in [\"ID_code\", \"target\"]:\n        labels.append(col)\n        values.append(spearmanr(sctp_train[col].values, sctp_train[\"target\"].values)[0])\ncorr_df = pd.DataFrame({'col_labels':labels, 'corr_values':values})\ncorr_df = corr_df.sort_values(by='corr_values')\n \nind = np.arange(corr_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,30))\nrects = ax.barh(ind, np.array(corr_df.corr_values.values), color='g')\nax.set_yticks(ind)\nax.set_yticklabels(corr_df.col_labels.values, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation coefficient of the variables\")\nplt.show()","c879ebc0":"corr_df_sel = corr_df.ix[(corr_df['corr_values'] < -0.05) | (corr_df['corr_values']>0.05)]\ncorr_df_sel","4df41e29":"cols_to_use = corr_df.ix[(corr_df['corr_values'] < -0.05) | (corr_df['corr_values']>0.05)].col_labels.tolist()\n\ntemp_df = sctp_train[cols_to_use]\ncorrmat = temp_df.corr(method='spearman')\nf, ax = plt.subplots(figsize=(20, 20))\n\n# Draw the heatmap using seaborn\nsns.heatmap(corrmat, vmax=1., square=True, cmap=\"YlGnBu\", annot=True)\nplt.title(\"Important variables correlation map\", fontsize=15)\nplt.show()","aeb03d7f":"### Get the X and y variables for building model ###\nX = sctp_train.drop([\"ID_code\", \"target\"], axis=1)\nY = sctp_train[\"target\"]\ntest_X = sctp_test.drop([\"ID_code\"], axis=1)\n\n#Train & Validation\nfrom sklearn.model_selection import train_test_split\n# create training and testing vars\nX_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=1)\nprint(X_train.shape, y_train.shape)\nprint(X_val.shape, y_val.shape)","cf1ef360":"from sklearn.ensemble import RandomForestClassifier\nrfc_model = RandomForestClassifier(random_state=0).fit(X_train, y_train)\n\n#Feature importance by eli5\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(rfc_model, random_state=1).fit(X_val, y_val)\neli5.show_weights(perm, feature_names = X_val.columns.tolist())","24a314d2":"#Using SMOTE for class imbalance in target\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\nsm = SMOTE(random_state=42)\nX_resamp_tr, y_resamp_tr = sm.fit_resample(X, Y)\nprint('Resampled dataset shape %s' % Counter(y_resamp_tr))\nX_resamp_tr = pd.DataFrame(X_resamp_tr)\ny_resamp_tr = pd.DataFrame({\"target\": y_resamp_tr})","67ed5e83":"# https:\/\/www.kaggle.com\/dromosys\/sctp-working-lgb\nparams = {'num_leaves': 9,\n         'min_data_in_leaf': 42,\n         'objective': 'binary',\n         'max_depth': 16,\n         'learning_rate': 0.0123,\n         'boosting': 'gbdt',\n         'bagging_freq': 5,\n         'bagging_fraction': 0.8,\n         'feature_fraction': 0.8201,\n         'bagging_seed': 11,\n         'reg_alpha': 1.728910519108444,\n         'reg_lambda': 4.9847051755586085,\n         'random_state': 42,\n         'metric': 'auc',\n         'verbosity': -1,\n         'subsample': 0.81,\n         'min_gain_to_split': 0.01077313523861969,\n         'min_child_weight': 19.428902804238373,\n         'num_threads': 4}","2ceef0f4":"%%time\nimport time\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfold_n=3\nfolds = StratifiedKFold(n_splits=fold_n, shuffle=True, random_state=30)\ny_pred_lgb = np.zeros(len(test_X))\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X_resamp_tr,y_resamp_tr)):\n    print('Fold', fold_n, 'started at', time.ctime())\n    X_train, X_valid = X_resamp_tr.iloc[train_index], X_resamp_tr.iloc[valid_index]\n    y_train, y_valid = y_resamp_tr.iloc[train_index], y_resamp_tr.iloc[valid_index]\n    \n    train_data = lgb.Dataset(X_train, label=y_train)\n    valid_data = lgb.Dataset(X_valid, label=y_valid)\n        \n    lgb_model = lgb.train(params,train_data,num_boost_round=20000,\n                    valid_sets = [train_data, valid_data],verbose_eval=300,early_stopping_rounds = 200)\n            \n    y_pred_lgb += lgb_model.predict(test_X, num_iteration=lgb_model.best_iteration)\/3","8ff4161b":"#Submission file\nsubmission_lgb_smote_2 = pd.DataFrame({\n        \"ID_code\": sctp_test[\"ID_code\"],\n        \"target\": y_pred_lgb\n    })\nsubmission_lgb_smote_2.to_csv('submission_lgb_smote_2.csv', index=False)","9a85cc41":"**Missing Value Proportion**  \nNow, Let us check the proportion of  many missing values in the training dataset.","067a087f":"**Reading the Training and Testing Dataset**","6731229e":"**Feature Importance - eli5 library **  \nFor feature importance, I am going to use the Permutation Importance technique that's being used in the [tutorial](https:\/\/www.kaggle.com\/dansbecker\/permutation-importance)","74017f5b":"The correlation coefficient values are very low and the maximum value is around 0.08 in negative side of the plot, with respect to positive side the maximum value is around 0.07.\n\nOverall, the correlation of the features with respect to target are very low.\n\nSo, We will take some of the features which has high correlation values and plot the heatmap for further analysis.","a028fca4":"**Analysis Playground**  \nAs in every data science prediction problem, I will start with Exploratory Data Analysis (EDA) and move on building model on different machine learning algorithms.","eac7f6fb":"**Target Distribution**  \nFirst let us look at the distribution of the target variable to understand whether the dataset is imbalanced or not.","5e0c17c4":"**Loading the required packages for analysis**","d88348ca":"This is really odd, as I have never come across a scenario where both the training and testing dataset have the same number of rows. Seems interesting. Here the number of features are bit higer in number. So, we will find which all variables are important based on missing values, correlation analysis etc.","6d1a0e3d":"Seems like none of the selected variables have spearman correlation more than 0.7 with each other.\n\nThe above plots helped us in identifying the important individual variables which are correlated with target. However we generally build many non-linear models in Kaggle competitions. So let us build some non-linear models and get variable importance from them.","75b2e4c7":"Plotting heatmap is done to identify if there are any strong monotonic relationships between these important features. If the values are high, then probably we can choose to keep one of those variables in the model building process. But, we are doing this only for small set of features. we can even try other techniques to explore other features in the dataset.","0b027ecd":"**Summary Statistics**","b305864d":"Interpreting Permutation Importances\nThe values towards the top are the most important features, and those towards the bottom matter least.\n\nThe first number in each row shows how much model performance decreased with a random shuffling (in this case, using \"accuracy\" as the performance metric).\n\nLike most things in data science, there is some randomness to the exact performance change from a shuffling a column. We measure the amount of randomness in our permutation importance calculation by repeating the process with multiple shuffles. The number after the \u00b1 measures how performance varied from one-reshuffling to the next.\n\nYou'll occasionally see negative values for permutation importances. In those cases, the predictions on the shuffled (or noisy) data happened to be more accurate than the real data. This happens when the feature didn't matter (should have had an importance close to 0), but random chance caused the predictions on shuffled data to be more accurate. This is more common with small datasets, like the one in this example, because there is more room for luck\/chance.\n\nIn our case, the top 10 most important feature are var_81, var_53, var_139, var_179, var_174, var_40, var_26, var_13, var_24 and var_109. But, Still all the features seems to have value importance close to zero.","f9580e36":"This Looks great as we have no missing values in the dataset.","7296a0eb":"**Submission File**","24a63fc8":"**Information on the training dataset**","d3d42634":"From the target proportion and target pie chart its clearly evident that the target is highly imbalanced with \"0\" class occupying 90% of the target values and 10% of target values with \"1\" class.","e5167deb":"**SMOTE Over-Sampling**  \nAs we have more records for target '0', I am going to over sample the target '1' to the same level as target '0' which is basically oversampling the least class.","d51ed1ea":".info() command in python give the brief glimpse of the dataset. Our traning dataset has three different types of datatypes. most of them are float which are contineous, one feature has integer which is most probably the \"Target\" column and the final one feature is of object type which i think will be the \"ID_code\" column.","a38be72b":"**Time for Modelling**  \n**LGBM**  \nLet's try with Lightgbm and see the accuracy.","4d5cbc39":"**Santander Customer Transaction Prediction  \nCan you identify who will make a transaction? **    \n![](https:\/\/bit.ly\/2BJideW)  \nSantander inivte fellow Kagglers to help them identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided for this competition has the same structure as the real data they have available to solve this problem.","b67a1726":"**Correlation Coefficient Plot**  \nAs there are no missing values in the dataset and all the features are numberic let's try the correlation plot and see how the features are correlated to each other."}}