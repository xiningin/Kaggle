{"cell_type":{"d909d9b0":"code","a75e03d5":"code","3f41881c":"code","47fbc607":"code","daefbd74":"code","b66f66a6":"code","c67dc924":"code","b6643c76":"code","b3f6e329":"code","25bbe5c6":"code","888818f8":"code","93640f7d":"code","5dcfc29d":"code","c1674d52":"code","c71fc48a":"code","3ad181d6":"code","d8328cf4":"code","e4920c1f":"code","e6f5cf63":"code","66f94e13":"code","3de95914":"code","316334c2":"code","6915af73":"code","2ab16b6a":"code","42bc5cbb":"markdown","225db15c":"markdown"},"source":{"d909d9b0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a75e03d5":"import cv2\n!pip install imutils\nimport torch\nimport torchvision\nimport torch.nn as nn\nfrom torchvision import transforms\nimport torch.nn.functional as F\nfrom torchvision import models\nfrom PIL import Image ","3f41881c":"# url='https:\/\/i.pinimg.com\/originals\/99\/0d\/66\/990d660230e267c11d9d29d913ef696a.jpg'\nurl='https:\/\/m.media-amazon.com\/images\/M\/MV5BMjI0MTg3MzI0M15BMl5BanBnXkFtZTcwMzQyODU2Mw@@._V1_UY1200_CR130,0,630,1200_AL_.jpg'\n\n\nstyle_img_url='https:\/\/orabart.weebly.com\/uploads\/8\/6\/9\/2\/8692977\/one-point-perspective-example-7.jpg'\n\nfrom PIL import Image\nimport urllib.request as urllib\nimport io\n\nfd = urllib.urlopen(url)\nimage_file = io.BytesIO(fd.read())\nimg = Image.open(image_file)\n\nfd = urllib.urlopen(style_img_url)\nimage_file = io.BytesIO(fd.read())\nstyle_img = Image.open(image_file)\n","47fbc607":"plt.imshow(img)","daefbd74":"plt.imshow(style_img)","b66f66a6":"vgg=models.vgg19(pretrained=True).features","c67dc924":"for param in vgg.parameters():\n    param.requires_grad_(False)","b6643c76":"device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nvgg.to(device)","b3f6e329":"mean=[0.485, 0.456, 0.406]\nstd=[0.229, 0.224, 0.225]","25bbe5c6":"def transformation(img):\n    transform=transforms.Compose([\n        transforms.Resize(512),\n        transforms.ToTensor(),\n        transforms.Normalize(mean,std)\n        \n    ])\n    img=transform(img)\n    return img.unsqueeze(0)","888818f8":"img=transformation(img).to(device)\nstyle_img=transformation(style_img).to(device)","93640f7d":"def tensor_to_image(tensor):\n    #copy the tensor and remove it from gradient calculation\n    img=tensor.clone().detach()\n    #squeeze removes the extra dimensions\n    img=img.cpu().numpy().squeeze()\n    #to change the color channel to be last, after height and width\n    img=img.transpose(1,2,0)\n    #restoring the original image from standardized form\n    img*=np.array(std)+np.array(mean)\n    img=img.clip(0,1)\n#     img=Image.fromarray(img).convert('RGB')\n    return img","5dcfc29d":"tensor_to_image(img).shape","c1674d52":"plt.imshow(tensor_to_image(img))","c71fc48a":"#Using the first and last convolution layers to extract granular and complex details respectively\nLAYERS_OF_INTREST={\n    '0':'conv1_1',\n    '5':'conv2_1',\n    '10':'conv3_1',\n    '19':'conv4_1',\n    '21':'conv4_2',\n    '28':'conv5_1'\n}","3ad181d6":"def apply_model_and_extract_features(image,model):\n    x=image\n    \n    features={}\n    for name,layer in model._modules.items():\n        x=layer(x)\n        if name in LAYERS_OF_INTREST:\n            features[LAYERS_OF_INTREST[name]]=x\n    \n    return features","d8328cf4":"content_img_features=apply_model_and_extract_features(img,vgg)\nstyle_img_features=apply_model_and_extract_features(style_img,vgg)","e4920c1f":"def calculate_gram_matrix(tensor):\n    _,channels,height,width=tensor.size()\n    tensor=tensor.view(channels,height*width)\n    gram_matrix=torch.mm(tensor,tensor.t())\n    gram_matrix=gram_matrix.div(channels*height*width)\n    return gram_matrix","e6f5cf63":"style_feature_gram_matrix={layer: calculate_gram_matrix(style_img_features[layer]) for layer in style_img_features}","66f94e13":"style_feature_gram_matrix","3de95914":"weights={'conv1_1':1,'conv2_1':0.75,'conv3_1':0.35,'conv4_1':0.25,'conv5_1':0.15}","316334c2":"target=img.clone().requires_grad_(True).to(device)\noptimizer=torch.optim.Adam([target],lr=0.03)","6915af73":"for i in range(1,2000):\n    target_features=apply_model_and_extract_features(target,vgg)\n    \n    content_loss=F.mse_loss(target_features['conv4_2'],content_img_features['conv4_2'])\n    style_loss=0\n    \n    for layer in weights:\n        target_feature=target_features[layer]\n        \n        target_gram_matrix=calculate_gram_matrix(target_feature)\n        style_gram_matrix=style_feature_gram_matrix[layer]\n        \n        layer_loss=F.mse_loss(target_gram_matrix,style_gram_matrix)\n        layer_loss*=weights[layer]\n        _,channels,height,width=target_feature.shape\n        \n        style_loss+=layer_loss\n        \n    total_loss=1000000*style_loss +content_loss\n    \n    if i % 50 ==0:\n        print(f'Epoch {i} ,Style_loss : {style_loss} , Content loss:{content_loss}')\n              \n    optimizer.zero_grad()\n    total_loss.backward()\n    optimizer.step()","2ab16b6a":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))\nax1.imshow(tensor_to_image(img))\nax2.imshow(tensor_to_image(target))","42bc5cbb":"![Screenshot%20from%202020-09-06%2017-23-11.png](attachment:Screenshot%20from%202020-09-06%2017-23-11.png)","225db15c":"![Screenshot%20from%202020-09-06%2017-23-32.png](attachment:Screenshot%20from%202020-09-06%2017-23-32.png)\n"}}