{"cell_type":{"492c94a2":"code","7a42f557":"code","bfd4e71b":"code","4406a86d":"code","82e524f8":"code","b0af0be4":"code","43fd457c":"code","58365376":"code","17efc82a":"code","c22eb55f":"code","6b5bfe46":"code","76e0bb7e":"markdown","e643bd31":"markdown","9b542d08":"markdown","2a42aa37":"markdown","75bee922":"markdown","92a0af1f":"markdown","82198edc":"markdown","4d3d6d76":"markdown","daea05f3":"markdown","360129d5":"markdown","85816f10":"markdown","c8bcf461":"markdown","3b02e1bc":"markdown"},"source":{"492c94a2":"\"\"\"\nCreated on Wed Mar 20 22:32:51 2019\n\n@author: Wajeeha\n\"\"\"\n\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np\nos.getcwd()\nos.chdir(r'..\/input')\n\ndf1=pd.read_csv('OLX_Car_Data_CSV.csv', delimiter=',', encoding = \"ISO-8859-1\")\ndf1.head(5) # to see the first 5 rows of dataset\ndf1.shape \ncolumnNames = pd.Series(df1.columns.values) #get the column names of the dataset\ndescription= df1.describe(include='all') # get the description summary of all columns\n\n#frequency distribution of categorical data\nBrand_vc=df1['Brand'].value_counts()\nCondition_vc= df1['Condition'].value_counts()\nFuel_vc=df1['Fuel'].value_counts()\nModel_vc=df1['Model'].value_counts()\nRegisteredCity_vc= df1['Registered City'].value_counts()\nTransactionType_vc= df1['Transaction Type'].value_counts()","7a42f557":"#Data preprocessing\n#Handling missing values\n# number of missing values in each column as isnull() returns 1, if the value is null.\nMissingData=df1.apply(lambda x: sum(x.isnull()),axis=0) #axis=0 for columns and axis=1 for rows\n\n#dropna\ndf1.dropna(inplace=True)\n\n#checking updated dataframe\ndf1.head(5)","bfd4e71b":"#import Normalisation package\nfrom sklearn.preprocessing import StandardScaler\nsc= StandardScaler()\ncols_scaled = sc.fit_transform(df1.loc[:,['KMs Driven','Price']])\n\n# take copy of df1 for scaling\ndf1_scaled= df1.copy() \ndf1_scaled['KMs Driven'] = pd.Series(cols_scaled[:,0]) \ndf1_scaled['Price'] = pd.Series(cols_scaled[:,1]) \ndf1_scaled.head(5)","4406a86d":"#frequency distribution of categorical data\nBrand_vc=df1_scaled['Brand'].value_counts()\nCondition_vc= df1_scaled['Condition'].value_counts()\nFuel_vc=df1_scaled['Fuel'].value_counts()\nModel_vc=df1_scaled['Model'].value_counts()\nRegisteredCity_vc= df1_scaled['Registered City'].value_counts()\nTransactionType_vc= df1_scaled['Transaction Type'].value_counts()","82e524f8":"#creating sub dataframe df1_bdmodfiltered\ndf1_model=Model_vc[Model_vc >= 100]\ndf1_modfiltered=df1_scaled.loc[df1_scaled['Model'].isin(df1_model.index)] #select rows whose column value is in or matches index ofdf1_model.index\nBrand_vc=df1_modfiltered['Brand'].value_counts()\ndf1_Brand=Brand_vc[Brand_vc >=100]\ndf1_bdmodfiltered=df1_modfiltered.loc[df1_modfiltered['Brand'].isin(df1_Brand.index)]\n#Brand1_vc=df1_bdmodfiltered['Brand'].value_counts() to check if the brand count again\nBrand_vc=df1_bdmodfiltered['Brand'].value_counts()\nModel_vc=df1_bdmodfiltered['Model'].value_counts()\ndf1_bdmodfiltered.head(5)","b0af0be4":"# Remove leasing and keep Karachi data\ndf1_clean = df1_bdmodfiltered[df1_bdmodfiltered['Transaction Type'] != 'Installment\/Leasing']\ndf1_clean = df1_clean.loc[df1_clean['Registered City'] == 'Karachi']\n\nTransactionType_vc= df1_clean['Transaction Type'].value_counts()\nRegisteredCity_vc= df1_clean['Registered City'].value_counts()\n\n# drop registered city\ndf1_clean = df1_clean.drop(['Registered City'], axis=1)\ndf1_clean = df1_clean.drop(['Transaction Type'], axis=1)\n\ndf1_clean.Price\ndf1_clean['KMs Driven']\n\ndf1_clean.dropna(inplace=True)\ndf1_clean.describe()\n\ndf1_clean.head(5)\n\ndf_num = df1_clean.copy()","43fd457c":"#Encoding\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ndf_num['Fuel'] = encoder.fit_transform(df_num['Fuel'])\n# save the mapping for future reference\nfuelMapping = encoder.classes_\ndf_num['Brand'] = encoder.fit_transform(df_num['Brand'])\nbrandMapping = encoder.classes_\ndf_num['Model'] = encoder.fit_transform(df_num['Model'])\nmodelMapping = encoder.classes_\n\ndf_num.Condition = df_num.Condition.map({'Used':0,'New':1})\ndf_num.head(5)","58365376":"import seaborn as sns\ncorr = df_num.corr()\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)\nsns.pairplot(df_num) #using seaborn pairplots to visualize each feature's skewness - ","17efc82a":"#Building models\nX= df_num.drop(['Condition'],axis=1)#drop y from the dataframe -  x part will be everything but not the target y variable\ny= df_num.Condition\n\n#import classifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\nclassifier = LogisticRegression()\n\nMissingData2=df_num.apply(lambda x: sum(x.isnull()),axis=0)\n\n# feature extraction\nmodel = LogisticRegression()\nrfe = RFE(model, 2)\nfit = rfe.fit(X, y)\nno_of_features = fit.n_features_\nsupport_features = fit.support_\nranking_features = fit.ranking_\nranking_features\n\nprint(\"Num Features: %d\" % (no_of_features))\nprint(\"Selected Features: %s\" % (support_features))\nprint(\"Feature Ranking: %s\" % (ranking_features))\n\nX_sub = X.iloc[:,[2,4]]\n\n#split train and test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=0)\n\n#train classifier\nclassifier.fit(X_train,y_train)\n\n#classifier performance on test set\nclassifier.score(X_test,y_test)\n\n#predictions for test\ny_pred = classifier.predict(X_test)","c22eb55f":"#import performance measure tools\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score \ncm=confusion_matrix(y_test,y_pred)\nacs=accuracy_score(y_test,y_pred)\nrs=recall_score(y_test,y_pred, average='macro')\nps=precision_score(y_test,y_pred, average='macro')","6b5bfe46":"y_pred_prob = classifier.predict_proba(X_test)[:,1] #\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfpr,tpr,thresholds= roc_curve(y_test,y_pred_prob)\n#roc curve will return what are the FPR and TPR for different thresholds -  it needs the actual prob so we give predict_prob values instead of predicted values\nimport matplotlib.pyplot as plt\nplt.plot(fpr,tpr)\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nroc_auc_score(y_test,y_pred_prob)","76e0bb7e":"Removing Installment\/Leasing rows as these also appear to have less data compared to Cash. Also removing all rows which are outside of Karachi again due to limited availability of data.","e643bd31":"Check frequency distribution of categorical data","9b542d08":"Encode categorical data using LabelEncoder. This would basically give number to each value found in a categorical column.","2a42aa37":"We get an accuracy score of 0.845 which shows that the model is very accurate in predicting the condition of the cars given other variables. \n\nBelow we plot roc_auc_score","75bee922":"**Exploratory Analysis**\n\nTo begin this exploratory analysis, first import dataset and then call basic data exploration functions","92a0af1f":"Calculating performance of the model","82198edc":"Filter models and brands for which there is insufficient data i.e. rows < 100","4d3d6d76":"**Building Model**\n\nBelow we apply LogisticRegression model to predict the Condition of the car\n","daea05f3":"**Data Preprocessing**\n\nHandle missing values - We drop all rows where we have missing values","360129d5":"Check correlation between numeric columns","85816f10":"**Data Normalisation**\n\nWe noticed that we have a large variance in the numeric data. To normalise these columns we use the StandardScaler for scaling KMs Driven and Price columns.","c8bcf461":"From the above the most interesting correlation that we can see is that Condition has some dependency on Year and Price. ","3b02e1bc":"**Introduction**\n\nThis data was primarily collected for research purposes. The data is regarding the used cars and their prices in Pakistan. It was scrapped from different car selling websites."}}