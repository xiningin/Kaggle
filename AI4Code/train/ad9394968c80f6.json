{"cell_type":{"6ff69083":"code","a3837d3e":"code","c85318f4":"code","c0b88655":"code","b38313d4":"code","f7227dcd":"code","a37217f4":"code","552a7f5c":"code","18923978":"code","7a0bae23":"code","920d6b8f":"code","1940efb0":"code","2e343590":"code","258c27ce":"code","30d7dd1e":"code","2d0e94a0":"code","6571a75a":"code","392019b7":"code","86a674b2":"code","2551618c":"code","bb7e27d9":"code","02f2420e":"code","c6c62a81":"code","dfbf9717":"code","d1c77d46":"code","8ae07fee":"code","19126835":"code","bcd2dd00":"code","d7a1a5d5":"code","d6bdd4d3":"code","86fe83c5":"code","b429b4fe":"code","49b01ae5":"code","b282b013":"code","94308462":"code","85efc6f2":"code","507ed589":"code","813f515d":"code","d64f0789":"code","f32a8434":"code","29052467":"code","fd272429":"markdown","23fc7e69":"markdown","dfb66509":"markdown","4e83d0b2":"markdown","6fa443f5":"markdown","cf427b09":"markdown","b165cb16":"markdown","c272778d":"markdown","eee5a905":"markdown"},"source":{"6ff69083":"import numpy as np #Importamos Numpy y Pandas para cargar el dataset y poder manejarlo\nimport pandas as pd\nimport matplotlib.pyplot as plt #Importamos Matplotlib para graficar\nimport time\n\n#Tambi\u00e9n importamos Torch y Torchvision de PyTorch, la que ser\u00e1 la librer\u00eda con la cual haremos el modelo\n#y nos permitir\u00e1 entrenarlo y realizar predicciones junto con sus tensores\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\n\nimport torchvision\nfrom torch.utils.data import DataLoader, TensorDataset\n","a3837d3e":"procesador = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","c85318f4":"#Cargamos la data de prueba y entrenamiento\ntest_dataset_pandas = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')\ntrain_dataset_pandas = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')","c0b88655":"#Ahora separamos las etiquetas y los datos asociados a dicha etiqueta.\n#As\u00ed podremos tener nuestros datos de salida con los cuales podremos comparar.\ntest_labels = test_dataset_pandas['label']\ntest_dataset_pandas_no_labels = (test_dataset_pandas.drop(['label'], axis=1))\n\ntrain_labels = train_dataset_pandas['label']\ntrain_dataset_pandas_no_labels = (train_dataset_pandas.drop(['label'], axis=1))","b38313d4":"#Ahora pasamos a un array de numpy los datasets\ntrain_dataset_np_array_no_labels = train_dataset_pandas_no_labels.to_numpy()\n\n#Al estar en forma de vector, se utiliza la funci\u00f3n reshape de Numpy para tenerlo en su forma matricial\ntrain_dataset_np_array_no_labels = train_dataset_np_array_no_labels.reshape(60000,1,28,28)\n\n#Como cada p\u00edxel est\u00e1 en un n\u00famero entre 0 y 255, se divide entre 255 cada p\u00edxel para poder estandarizar los valores.\ntrain_dataset_np_array_no_labels = train_dataset_np_array_no_labels\/255\n\n#Tambi\u00e9n se pasan a un array de numpy las etiquetas\ntrain_labels_np_array = train_labels.to_numpy()\n\n#Despu\u00e9s de tenerlos en numpy, los pasamos a tensores de PyTorch para despu\u00e9s normalizarlos usando una media y una desviaci\u00f3n estandar de 0.5\ntrain_dataset_torch_no_labels = torch.tensor(train_dataset_np_array_no_labels)\ntrain_dataset_torch_no_labels = ((train_dataset_torch_no_labels-0.5)\/0.5).float()\ntrain_labels_torch = torch.tensor(train_labels_np_array)\n\n#Al igual que se hizo anteriormente con el dataset de entrenamiento, tambi\u00e9n se aplica para el dataset de prueba, \n#esto para que ambos datasets de muestras se mantengan uniformes\n\ntest_dataset_np_array_no_labels = test_dataset_pandas_no_labels.to_numpy()\ntest_dataset_np_array_no_labels = test_dataset_np_array_no_labels.reshape(10000,1,28,28)\ntest_dataset_np_array_no_labels = test_dataset_np_array_no_labels\/255\ntest_labels_np_array = test_labels.to_numpy()\n\ntest_dataset_torch_no_labels = torch.tensor(test_dataset_np_array_no_labels)\ntest_dataset_torch_no_labels = ((test_dataset_torch_no_labels -0.5) \/ 0.5).float()\ntest_labels_torch = torch.tensor(test_labels_np_array)","f7227dcd":"#Se trabajar\u00e1 con un batch size de 64 muestras\nbatch_size = 64\n\n#Ahora, creamos los datasets de tensores, indicando el batch size\ntrain_torch_dataset = TensorDataset(train_dataset_torch_no_labels, train_labels_torch)\ntrain_torch_dataloader = DataLoader(train_torch_dataset, batch_size = batch_size, shuffle=True)\n\ntest_torch_dataset = TensorDataset(test_dataset_torch_no_labels, test_labels_torch)\ntest_torch_dataloader = DataLoader(test_torch_dataset, batch_size = batch_size, shuffle=True)\n","a37217f4":"#Instanciamos esta funci\u00f3n que nos permitir\u00e1 graficar nuestras muestras para comprobar nuestro dataset\n\ndef imshow(img):\n    img = img \/ 2 + 0.5\n    npimg = img.cpu().numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()","552a7f5c":"#Con fashion_labels lo que buscamos es tener las etiquetas con su nombre, as\u00ed cuando tengamos los resultados del modelo, poderlos comparar con sus gr\u00e1ficas\nfashion_labels = ('T-shirt\/top', 'Trouser', 'Pullover','Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker','bolso','Ankle Boot')","18923978":"#As\u00ed tomamos un batch del modelo y lo graficamos, viendo cada prenda y su correcta etiqueta\ndataiter = iter(train_torch_dataloader)\nimages, labels = dataiter.next()\nprint(images.shape)\nimshow(torchvision.utils.make_grid(images))\nprint(' '.join('%5s' % fashion_labels[labels[j]] for j in range(batch_size)))","7a0bae23":"#Ahora definimos el modelo inicial, m\u00e1s adelante veremos sus variantes\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        #Este tendr\u00e1 2 capas convolucionales\n        #En el primero pasamos un canal (la imagen), y tendremos como salida 32 canales o filtros, el kernel ser\u00e1 de 2x2\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=2)\n        \n        #El pool ser\u00e1 el mismo para ambos, de 2x2 tambi\u00e9n, pero manteniendo un stride de 2, as\u00ed que se mover\u00e1 2 p\u00edxeles\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        #La segunda capa recibir\u00e1 los 32 filtros del anterior y aqu\u00ed devolver\u00e1 64 filtros\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2)\n        \n        #Aplicamos tambi\u00e9n un dropout de .25 para poder mantener el modelo regularizado\n        self.drop = nn.Dropout(0.25)\n        \n        #Luego se realizan 3 regresiones lineales, donde la tercera es la regresi\u00f3n de la predicci\u00f3n\n        #Para estas regresiones utilizamos el m\u00e9todo de activaci\u00f3n ReLU\n        self.fc1 = nn.Linear(2304, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = self.drop(x)\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nnet = Net()\nnet.to(procesador)","920d6b8f":"# Para la p\u00e9rdida se manejar\u00e1 la funci\u00f3n Cross Entropy Loss\ncriterion = nn.CrossEntropyLoss()\n\n#Como optimizador utilizaremos Adam, y un learning rate de 0.001\noptimizer = torch.optim.Adam(net.parameters(), lr=0.001)","1940efb0":"loss_list = []\niteration_list = []\naccuracy_list = []\ntrain_losses, test_losses = [], []\naccuracy_list = []\nepochs = 20\nstart_time = time.time()\nfor epoch in range(epochs):\n    \n    accuracy_train = 0\n    running_loss = 0.0\n    for i, data in enumerate(train_torch_dataloader, 0):\n        inputs, labels = data\n        inputs, labels = inputs.to(procesador), labels.to(procesador)\n\n\n        optimizer.zero_grad()\n\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n    \n    \n        output_probabilities = torch.exp(outputs)\n        top_p, top_class = outputs.topk(1, dim=1)\n        equals = top_class == labels.view(*top_class.shape)\n        accuracy_train += torch.mean(equals.type(torch.FloatTensor))\n    \n    test_loss = 0\n    accuracy_test = 0\n    with torch.no_grad():\n        for images, labels in test_torch_dataloader: \n            \n            images, labels = images.to(procesador), labels.to(procesador)\n\n            prediction = net(images)\n            test_loss += criterion(prediction, labels)\n            prediction_probabilities = torch.exp(prediction)\n            top_p, top_class = prediction_probabilities.topk(1, dim=1)\n            equals = top_class == labels.view(*top_class.shape)\n            accuracy_test += torch.mean(equals.type(torch.FloatTensor))\n    train_losses.append(running_loss\/len(train_torch_dataloader))\n    test_losses.append((test_loss\/len(test_torch_dataloader)))\n\n    print(\"Epoch: {}\/{}..\".format(epoch+1, epochs),\n          \"Training Loss: {:.2f}..\".format(running_loss\/len(train_torch_dataloader)),\n          \"Training Accuracy: {:.2f}..\".format(accuracy_train\/len(train_torch_dataloader)),\n          \"Test Loss: {:.2f}..\".format(test_loss\/len(test_torch_dataloader)),\n          \"Test Accuracy: {:.2f}\".format(accuracy_test\/len(test_torch_dataloader)))\nend_time = time.time()\nprint('Tiempo de ejecuci\u00f3n: {} segundos'.format(end_time - start_time))","2e343590":"plt.plot(range(epochs),test_losses,  label = 'test')\nplt.plot(range(epochs),train_losses, label = 'train')\nplt.xlabel(\"No. of Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","258c27ce":"dataiter = iter(test_torch_dataloader)\nnet.to(procesador)\nimages, labels = dataiter.next()\nimages, labels = images.to(procesador), labels.to(procesador)\nprediction = net(images)\nimg = np.random.randint(10)\nprediction_probabilities = torch.exp(prediction)\ntop_p, top_class = prediction_probabilities.topk(1, dim=1)\nimshow(torchvision.utils.make_grid(images[img]))\nprint('El modelo predijo que es un\/a '+fashion_labels[top_class.reshape(1,-1)[0][img]])\nprint('Y realmente es un\/a '+fashion_labels[labels[img]])","30d7dd1e":"class Net2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        #Este tendr\u00e1 3 capas convolucionales\n        #En el primero pasamos un canal (la imagen), y tendremos como salida 32 canales o filtros, el kernel ser\u00e1 de 2x2\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=2)\n        \n        #El pool ser\u00e1 el mismo para los primeros 2, de 2x2 tambi\u00e9n, pero manteniendo un stride de 2, as\u00ed que se mover\u00e1 2 p\u00edxeles\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        \n        #La segunda capa recibir\u00e1 los 32 filtros del anterior y aqu\u00ed devolver\u00e1 64 filtros\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=2)\n        \n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=2)\n        \n        #Aplicamos tambi\u00e9n un dropout de .2 para poder mantener el modelo regularizado\n        self.drop = nn.Dropout(0.2)\n        \n        #Luego se realizan 3 regresiones lineales, donde la tercera es la regresi\u00f3n de la predicci\u00f3n\n        #Para estas regresiones utilizamos el m\u00e9todo de activaci\u00f3n ReLU\n        self.fc1 = nn.Linear(512, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        x = torch.flatten(x, 1)\n        x = F.relu(self.fc1(x))\n        x = self.drop(x)\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nnet2 = Net2()\nnet2.to(procesador)","2d0e94a0":"# Para la p\u00e9rdida se manejar\u00e1 la funci\u00f3n Cross Entropy Loss\ncriterion = nn.CrossEntropyLoss()\n\n#Como optimizador utilizaremos Adam, y un learning rate de 0.001\noptimizer = torch.optim.Adam(net2.parameters(), lr=0.001)","6571a75a":"iteration_list = []\naccuracy_list = []\ntrain_losses, test_losses = [], []\nepochs = 20\n\nstart_time = time.time()\nfor epoch in range(epochs):\n    \n    accuracy_train = 0\n    running_loss = 0.0\n    for i, data in enumerate(train_torch_dataloader, 0):\n        inputs, labels = data\n        inputs, labels = inputs.to(procesador), labels.to(procesador)\n\n        optimizer.zero_grad()\n\n        outputs = net2(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n    \n    \n        output_probabilities = torch.exp(outputs)\n        top_p, top_class = outputs.topk(1, dim=1)\n        equals = top_class == labels.view(*top_class.shape)\n        accuracy_train += torch.mean(equals.type(torch.FloatTensor))\n    \n    test_loss = 0\n    accuracy_test = 0\n    with torch.no_grad():\n        for images, labels in test_torch_dataloader: \n            \n            images, labels = images.to(procesador), labels.to(procesador)\n            prediction = net2(images)\n            test_loss += criterion(prediction, labels)\n            prediction_probabilities = torch.exp(prediction)\n            top_p, top_class = prediction_probabilities.topk(1, dim=1)\n            equals = top_class == labels.view(*top_class.shape)\n            accuracy_test += torch.mean(equals.type(torch.FloatTensor))\n    train_losses.append(running_loss\/len(train_torch_dataloader))\n    test_losses.append((test_loss\/len(test_torch_dataloader)))\n\n    print(\"Epoch: {}\/{}..\".format(epoch+1, epochs),\n          \"Training Loss: {:.2f}..\".format(running_loss\/len(train_torch_dataloader)),\n          \"Training Accuracy: {:.2f}..\".format(accuracy_train\/len(train_torch_dataloader)),\n          \"Test Loss: {:.2f}..\".format(test_loss\/len(test_torch_dataloader)),\n          \"Test Accuracy: {:.2f}\".format(accuracy_test\/len(test_torch_dataloader)))\nend_time = time.time()\nprint('Tiempo de ejecuci\u00f3n: {} segundos'.format(end_time - start_time))","392019b7":"plt.plot(range(epochs),test_losses,  label = 'test')\nplt.plot(range(epochs),train_losses, label = 'train')\nplt.xlabel(\"No. of Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n\n#Aqu\u00ed vemos que fue un buen resultado, se acerc\u00f3 al 90% el accuracy, pero el modelo principal sigue teniendo entre 91 y 92","86a674b2":"dataiter = iter(test_torch_dataloader)\nimages, labels = dataiter.next()\nimages, labels = images.to(procesador), labels.to(procesador)\nprediction = net2(images)\nimg = np.random.randint(10)\nprediction_probabilities = torch.exp(prediction)\ntop_p, top_class = prediction_probabilities.topk(1, dim=1)\nimshow(torchvision.utils.make_grid(images[img]))\nprint('El modelo predijo que es un\/a '+fashion_labels[top_class.reshape(1,-1)[0][img]])\nprint('Y realmente es un\/a '+fashion_labels[labels[img]])","2551618c":"net3 = Net()\nnet3.to(procesador)","bb7e27d9":"# Para la p\u00e9rdida se manejar\u00e1 la funci\u00f3n Cross Entropy Loss\ncriterion = nn.CrossEntropyLoss()\n\n#Como optimizador utilizaremos Adam, y un learning rate de 0.001\noptimizer = torch.optim.Adam(net3.parameters(), lr=0.001)","02f2420e":"iteration_list = []\naccuracy_list = []\ntrain_losses, test_losses = [], []\nepochs = 40\n\nstart_time = time.time()\nfor epoch in range(epochs):\n    \n    accuracy_train = 0\n    running_loss = 0.0\n    for i, data in enumerate(train_torch_dataloader, 0):\n        inputs, labels = data\n        inputs, labels = inputs.to(procesador), labels.to(procesador)\n\n        optimizer.zero_grad()\n\n        outputs = net3(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n    \n    \n        output_probabilities = torch.exp(outputs)\n        top_p, top_class = outputs.topk(1, dim=1)\n        equals = top_class == labels.view(*top_class.shape)\n        accuracy_train += torch.mean(equals.type(torch.FloatTensor))\n    \n    test_loss = 0\n    accuracy_test = 0\n    with torch.no_grad():\n        for images, labels in test_torch_dataloader: \n            \n            images, labels = images.to(procesador), labels.to(procesador)\n            prediction = net3(images)\n            test_loss += criterion(prediction, labels)\n            prediction_probabilities = torch.exp(prediction)\n            top_p, top_class = prediction_probabilities.topk(1, dim=1)\n            equals = top_class == labels.view(*top_class.shape)\n            accuracy_test += torch.mean(equals.type(torch.FloatTensor))\n    train_losses.append(running_loss\/len(train_torch_dataloader))\n    test_losses.append((test_loss\/len(test_torch_dataloader)))\n\n    print(\"Epoch: {}\/{}..\".format(epoch+1, epochs),\n          \"Training Loss: {:.2f}..\".format(running_loss\/len(train_torch_dataloader)),\n          \"Training Accuracy: {:.2f}..\".format(accuracy_train\/len(train_torch_dataloader)),\n          \"Test Loss: {:.2f}..\".format(test_loss\/len(test_torch_dataloader)),\n          \"Test Accuracy: {:.2f}\".format(accuracy_test\/len(test_torch_dataloader)))\nend_time = time.time()\nprint('Tiempo de ejecuci\u00f3n: {} segundos'.format(end_time - start_time))","c6c62a81":"plt.plot(range(epochs),test_losses,  label = 'test')\nplt.plot(range(epochs),train_losses, label = 'train')\nplt.xlabel(\"No. of Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n\n#Vemos que esto lo que hace es que aumente la varianza, al aumentar la cantidad de iteraciones,\n#el modelo tiende a sobreajustarse y aumenta el la p\u00e9rdida en las pruebas","dfbf9717":"dataiter = iter(test_torch_dataloader)\nimages, labels = dataiter.next()\nimages, labels = images.to(procesador), labels.to(procesador)\nprediction = net3(images)\nimg = np.random.randint(10)\nprediction_probabilities = torch.exp(prediction)\ntop_p, top_class = prediction_probabilities.topk(1, dim=1)\nimshow(torchvision.utils.make_grid(images[img]))\nprint('El modelo predijo que es un\/a '+fashion_labels[top_class.reshape(1,-1)[0][img]])\nprint('Y realmente es un\/a '+fashion_labels[labels[img]])","d1c77d46":"#Se trabajar\u00e1 con un batch size de 16 muestras\nbatch_size = 16\n\ntrain_torch_dataset = TensorDataset(train_dataset_torch_no_labels, train_labels_torch)\ntrain_torch_dataloader = DataLoader(train_torch_dataset, batch_size = batch_size, shuffle=True)\n\ntest_torch_dataset = TensorDataset(test_dataset_torch_no_labels, test_labels_torch)\ntest_torch_dataloader = DataLoader(test_torch_dataset, batch_size = batch_size, shuffle=True)","8ae07fee":"net4 = Net()\nnet4.to(procesador)","19126835":"# Para la p\u00e9rdida se manejar\u00e1 la funci\u00f3n Cross Entropy Loss\ncriterion = nn.CrossEntropyLoss()\n\n#Como optimizador utilizaremos Adam, y un learning rate de 0.001\noptimizer = torch.optim.Adam(net4.parameters(), lr=0.001)","bcd2dd00":"iteration_list = []\naccuracy_list = []\ntrain_losses, test_losses = [], []\nepochs = 20\n\nstart_time = time.time()\nfor epoch in range(epochs):\n    \n    accuracy_train = 0\n    running_loss = 0.0\n    for i, data in enumerate(train_torch_dataloader, 0):\n        inputs, labels = data\n        inputs, labels = inputs.to(procesador), labels.to(procesador)\n\n        optimizer.zero_grad()\n\n        outputs = net4(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n    \n    \n        output_probabilities = torch.exp(outputs)\n        top_p, top_class = outputs.topk(1, dim=1)\n        equals = top_class == labels.view(*top_class.shape)\n        accuracy_train += torch.mean(equals.type(torch.FloatTensor))\n    \n    test_loss = 0\n    accuracy_test = 0\n    with torch.no_grad():\n        for images, labels in test_torch_dataloader: \n            \n            images, labels = images.to(procesador), labels.to(procesador)\n            prediction = net4(images)\n            test_loss += criterion(prediction, labels)\n            prediction_probabilities = torch.exp(prediction)\n            top_p, top_class = prediction_probabilities.topk(1, dim=1)\n            equals = top_class == labels.view(*top_class.shape)\n            accuracy_test += torch.mean(equals.type(torch.FloatTensor))\n    train_losses.append(running_loss\/len(train_torch_dataloader))\n    test_losses.append((test_loss\/len(test_torch_dataloader)))\n\n    print(\"Epoch: {}\/{}..\".format(epoch+1, epochs),\n          \"Training Loss: {:.2f}..\".format(running_loss\/len(train_torch_dataloader)),\n          \"Training Accuracy: {:.2f}..\".format(accuracy_train\/len(train_torch_dataloader)),\n          \"Test Loss: {:.2f}..\".format(test_loss\/len(test_torch_dataloader)),\n          \"Test Accuracy: {:.2f}\".format(accuracy_test\/len(test_torch_dataloader)))\nend_time = time.time()\nprint('Tiempo de ejecuci\u00f3n: {} segundos'.format(end_time - start_time))","d7a1a5d5":"plt.plot(range(epochs),test_losses,  label = 'test')\nplt.plot(range(epochs),train_losses, label = 'train')\nplt.xlabel(\"No. of Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n\n#A pesar de tener un accuracy muy parecido al inicial, tarda mucho m\u00e1s en ejecutarse.","d6bdd4d3":"dataiter = iter(test_torch_dataloader)\nimages, labels = dataiter.next()\nimages, labels = images.to(procesador), labels.to(procesador)\nprediction = net4(images)\nimg = np.random.randint(10)\nprediction_probabilities = torch.exp(prediction)\ntop_p, top_class = prediction_probabilities.topk(1, dim=1)\nimshow(torchvision.utils.make_grid(images[img]))\nprint('El modelo predijo que es un\/a '+fashion_labels[top_class.reshape(1,-1)[0][img]])\nprint('Y realmente es un\/a '+fashion_labels[labels[img]])","86fe83c5":"#Se trabajar\u00e1 con un batch size de 64 muestras\nbatch_size = 64\n\n#Ahora, creamos los datasets de tensores, indicando el batch size\ntrain_torch_dataset = TensorDataset(train_dataset_torch_no_labels, train_labels_torch)\ntrain_torch_dataloader = DataLoader(train_torch_dataset, batch_size = batch_size, shuffle=True)\n\ntest_torch_dataset = TensorDataset(test_dataset_torch_no_labels, test_labels_torch)\ntest_torch_dataloader = DataLoader(test_torch_dataset, batch_size = batch_size, shuffle=True)","b429b4fe":"net5_menor_lr = Net()\nnet5_menor_lr.to(procesador)","49b01ae5":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(net5_menor_lr.parameters(), lr=0.00005)","b282b013":"iteration_list = []\naccuracy_list = []\ntrain_losses, test_losses = [], []\nepochs = 20\n\nstart_time = time.time()\nfor epoch in range(epochs):\n    \n    accuracy_train = 0\n    running_loss = 0.0\n    for i, data in enumerate(train_torch_dataloader, 0):\n        inputs, labels = data\n        inputs, labels = inputs.to(procesador), labels.to(procesador)\n\n        optimizer.zero_grad()\n\n        outputs = net5_menor_lr(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n    \n    \n        output_probabilities = torch.exp(outputs)\n        top_p, top_class = outputs.topk(1, dim=1)\n        equals = top_class == labels.view(*top_class.shape)\n        accuracy_train += torch.mean(equals.type(torch.FloatTensor))\n    \n    test_loss = 0\n    accuracy_test = 0\n    with torch.no_grad():\n        for images, labels in test_torch_dataloader: \n            \n            images, labels = images.to(procesador), labels.to(procesador)\n            prediction = net5_menor_lr(images)\n            test_loss += criterion(prediction, labels)\n            prediction_probabilities = torch.exp(prediction)\n            top_p, top_class = prediction_probabilities.topk(1, dim=1)\n            equals = top_class == labels.view(*top_class.shape)\n            accuracy_test += torch.mean(equals.type(torch.FloatTensor))\n    train_losses.append(running_loss\/len(train_torch_dataloader))\n    test_losses.append((test_loss\/len(test_torch_dataloader)))\n\n    print(\"Epoch: {}\/{}..\".format(epoch+1, epochs),\n          \"Training Loss: {:.2f}..\".format(running_loss\/len(train_torch_dataloader)),\n          \"Training Accuracy: {:.2f}..\".format(accuracy_train\/len(train_torch_dataloader)),\n          \"Test Loss: {:.2f}..\".format(test_loss\/len(test_torch_dataloader)),\n          \"Test Accuracy: {:.2f}\".format(accuracy_test\/len(test_torch_dataloader)))\nend_time = time.time()\nprint('Tiempo de ejecuci\u00f3n: {} segundos'.format(end_time - start_time))","94308462":"plt.plot(range(epochs),test_losses,  label = 'test')\nplt.plot(range(epochs),train_losses, label = 'train')\nplt.xlabel(\"No. of Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n","85efc6f2":"dataiter = iter(test_torch_dataloader)\nimages, labels = dataiter.next()\nimages, labels = images.to(procesador), labels.to(procesador)\nprediction = net5_menor_lr(images)\nimg = np.random.randint(10)\nprediction_probabilities = torch.exp(prediction)\ntop_p, top_class = prediction_probabilities.topk(1, dim=1)\nimshow(torchvision.utils.make_grid(images[img]))\nprint('El modelo predijo que es un\/a '+fashion_labels[top_class.reshape(1,-1)[0][img]])\nprint('Y realmente es un\/a '+fashion_labels[labels[img]])","507ed589":"net5_mayor_lr = Net()\nnet5_mayor_lr.to(procesador)","813f515d":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(net5_menor_lr.parameters(), lr=0.1)","d64f0789":"iteration_list = []\naccuracy_list = []\ntrain_losses, test_losses = [], []\nepochs = 20\n\nstart_time = time.time()\nfor epoch in range(epochs):\n    \n    accuracy_train = 0\n    running_loss = 0.0\n    for i, data in enumerate(train_torch_dataloader, 0):\n        inputs, labels = data\n        inputs, labels = inputs.to(procesador), labels.to(procesador)\n\n        optimizer.zero_grad()\n\n        outputs = net5_mayor_lr(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n    \n    \n        output_probabilities = torch.exp(outputs)\n        top_p, top_class = outputs.topk(1, dim=1)\n        equals = top_class == labels.view(*top_class.shape)\n        accuracy_train += torch.mean(equals.type(torch.FloatTensor))\n    \n    test_loss = 0\n    accuracy_test = 0\n    with torch.no_grad():\n        for images, labels in test_torch_dataloader: \n            \n            images, labels = images.to(procesador), labels.to(procesador)\n            prediction = net5_mayor_lr(images)\n            test_loss += criterion(prediction, labels)\n            prediction_probabilities = torch.exp(prediction)\n            top_p, top_class = prediction_probabilities.topk(1, dim=1)\n            equals = top_class == labels.view(*top_class.shape)\n            accuracy_test += torch.mean(equals.type(torch.FloatTensor))\n    train_losses.append(running_loss\/len(train_torch_dataloader))\n    test_losses.append((test_loss\/len(test_torch_dataloader)))\n\n    print(\"Epoch: {}\/{}..\".format(epoch+1, epochs),\n          \"Training Loss: {:.2f}..\".format(running_loss\/len(train_torch_dataloader)),\n          \"Training Accuracy: {:.2f}..\".format(accuracy_train\/len(train_torch_dataloader)),\n          \"Test Loss: {:.2f}..\".format(test_loss\/len(test_torch_dataloader)),\n          \"Test Accuracy: {:.2f}\".format(accuracy_test\/len(test_torch_dataloader)))\nend_time = time.time()\nprint('Tiempo de ejecuci\u00f3n: {} segundos'.format(end_time - start_time))","f32a8434":"plt.plot(range(epochs),test_losses,  label = 'test')\nplt.plot(range(epochs),train_losses, label = 'train')\nplt.xlabel(\"No. of Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","29052467":"dataiter = iter(test_torch_dataloader)\nimages, labels = dataiter.next()\nimages, labels = images.to(procesador), labels.to(procesador)\nprediction = net5_mayor_lr(images)\nimg = np.random.randint(10)\nprediction_probabilities = torch.exp(prediction)\ntop_p, top_class = prediction_probabilities.topk(1, dim=1)\nimshow(torchvision.utils.make_grid(images[img]))\nprint('El modelo predijo que es un\/a '+fashion_labels[top_class.reshape(1,-1)[0][img]])\nprint('Y realmente es un\/a '+fashion_labels[labels[img]])","fd272429":"## Segundo modelo: Cambios en la arquitectura\n\nSe mantiene el learning rate, la optimizaci\u00f3n y el n\u00famero de epochs, al igual que la funci\u00f3n de p\u00e9rdida","23fc7e69":"## Importaciones\n\nPrimero importamos las librer\u00edas que utilizaremos a lo largo del notebook","dfb66509":"## Funci\u00f3n auxiliar imshow","4e83d0b2":"## Quinto Modelo: cambiar learning rate\nSe trabajar\u00e1 cambiar\u00e1 el learning rate por uno m\u00e1s peque\u00f1o y luego por uno m\u00e1s grande","6fa443f5":"# Proyecto 1 de Computaci\u00f3n Emergente\n\n#### Integrantes:\n* Fernando Baladi 20181110303\n* Katherine Invernon 20181110193\n","cf427b09":"## Tercer modelo: Cambios en el n\u00famero de epochs\n\nSe doblar\u00e1 el n\u00famero de epochs, pasando de 20 a 40 epochs","b165cb16":"## Cuarto Modelo: Colocar menos muestras en el tama\u00f1o del batch\nSe trabajar\u00e1 s\u00f3lo con 16 muestras por batch","c272778d":"## Creaci\u00f3n del modelo principal","eee5a905":"## Carga y manejo de la data"}}