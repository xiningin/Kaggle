{"cell_type":{"9300ab87":"code","fe1e944f":"code","e43b955a":"code","c82159e4":"code","520fb9fb":"code","f85ce92d":"code","47a8d7be":"code","46789df9":"code","8449de05":"code","02b8390e":"code","c3010f4b":"code","aca191e4":"code","a4b11bc0":"code","c0e8707d":"code","1860e42b":"code","66de32dc":"code","3cd7282f":"code","8f7ac44a":"code","2759e611":"code","de7ff977":"code","6241ebe4":"code","8ccdd9db":"code","207cf87d":"code","e5b9d1d3":"code","3ac501e9":"code","fd1daba6":"code","6ac91cbb":"code","9f175147":"code","abaab713":"code","7bf1f1b5":"code","f91812cd":"code","19a356eb":"code","97342133":"code","f21275ea":"code","6a9141f1":"code","5b31a8ab":"code","044d8150":"code","23164d19":"code","914dd984":"code","dfb9ef52":"code","d9559d84":"code","61b2c1c9":"code","3c0e5521":"markdown","8c6ee263":"markdown","85224701":"markdown","84de7372":"markdown","a195b6fb":"markdown","c8ed9af4":"markdown","b638712f":"markdown","403ea8c4":"markdown","1c3c1bd4":"markdown","d8ae4e65":"markdown","c6278c2d":"markdown","37ad0644":"markdown","13101c4d":"markdown","f9b53809":"markdown"},"source":{"9300ab87":"# packages\n\n# standard\nimport numpy as np\nimport pandas as pd\nimport time\nimport gc\n\n# plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.mosaicplot import mosaic\n\n# machine learning tools\nimport h2o\nfrom h2o.estimators import H2OGradientBoostingEstimator","fe1e944f":"# change number format in outputs\npd.options.display.float_format = \"{:.2f}\".format","e43b955a":"# load data + first glance\nt1 = time.time()\ndf_train = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv')\ndf_sub = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv')\nt2 = time.time()\n\nprint('Elapsed time [s]:' , np.round(t2-t1,4))","c82159e4":"# first glance (training data)\ndf_train.head()","520fb9fb":"# dimensions\nprint('Train Set:', df_train.shape)\nprint('Test Set :', df_test.shape)","f85ce92d":"# structure\ndf_train.info()","47a8d7be":"# convert target to categorical\ndf_train.Cover_Type = df_train.Cover_Type.astype('category')","46789df9":"# basic stats\nprint(df_train.Cover_Type.value_counts())\ndf_train.Cover_Type.value_counts().sort_index().plot(kind='bar')\nplt.grid()\nplt.show()","8449de05":"features_num = ['Elevation', 'Aspect', 'Slope',\n                'Horizontal_Distance_To_Hydrology', \n                'Vertical_Distance_To_Hydrology',\n                'Horizontal_Distance_To_Roadways',\n                'Hillshade_9am', 'Hillshade_Noon','Hillshade_3pm',\n                'Horizontal_Distance_To_Fire_Points']","02b8390e":"# basic summary stats\ndf_train[features_num].describe()","c3010f4b":"# plot features\nfor f in features_num:\n    df_train[f].plot(kind='hist', bins=50)\n    plt.title(f + ' - Training Data')\n    plt.grid()\n    plt.show()","aca191e4":"# correlation\ncorr_pearson = df_train[features_num].corr(method='pearson')\nplt.figure(figsize=(5,4))\nsns.heatmap(corr_pearson, annot=False, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Pearson Correlation')\nplt.show()","a4b11bc0":"# rank correlation\ncorr_pearson = df_train[features_num].corr(method='spearman')\nplt.figure(figsize=(5,4))\nsns.heatmap(corr_pearson, annot=False, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Spearman Correlation')\nplt.show()","c0e8707d":"# check if encoding is unique\nfeature_list_wild = ['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3','Wilderness_Area4']\n# type conversion\ndf_train[feature_list_wild] =  df_train[feature_list_wild].astype(np.uint8)\ndf_test[feature_list_wild] =  df_test[feature_list_wild].astype(np.uint8)\n# sum indicators (0\/1)\ndf_train['Wilderness_Area_Sum'] = df_train[feature_list_wild].sum(axis=1)\ndf_train.Wilderness_Area_Sum.value_counts().sort_index()","1860e42b":"# count frequencies nevertheless\nprint(df_train[feature_list_wild].sum())\ndf_train[feature_list_wild].sum().plot(kind='bar')\nplt.grid()\nplt.show()","66de32dc":"# check if encoding is unique\nfeature_list_soil = ['Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n                     'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n                     'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n                     'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n                     'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n                     'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n                     'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n                     'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n                     'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n                     'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\n# type conversion\ndf_train[feature_list_soil] =  df_train[feature_list_soil].astype(np.uint8)\ndf_test[feature_list_soil] =  df_test[feature_list_soil].astype(np.uint8)\n\n# sum indicators (0\/1)\ndf_train['Soil_Type_Sum'] = df_train[feature_list_soil].sum(axis=1)\ndf_train.Soil_Type_Sum.value_counts().sort_index()","3cd7282f":"# count frequencies nevertheless\nprint(df_train[feature_list_soil].sum())\nplt.figure(figsize=(12,4))\ndf_train[feature_list_soil].sum().plot(kind='bar')\nplt.grid()\nplt.show()","8f7ac44a":"# show structure again\ndf_train.info()","2759e611":"# garbage collection\ngc.collect();","de7ff977":"# violinplots by class\nt1 = time.time()\nfor f in features_num:\n    plt.figure(figsize=(10,5))\n    sns.violinplot(x=f, y='Cover_Type', data=df_train)\n    my_title = 'Distribution by class for ' + f\n    plt.title(my_title)\n    plt.grid()\nt2 = time.time()\nprint('Elapsed time [s]:' , np.round(t2-t1,4))","6241ebe4":"# change number format in outputs\npd.options.display.float_format = \"{:.8f}\".format","8ccdd9db":"# evaluate impact of Wilderness_Area\nfor f in feature_list_wild:\n    #### cross table - calc absolute counts...\n    ctab = pd.crosstab(df_train.Cover_Type, df_train[f])\n    # ...and normalized by column\n    ctab_norm = ctab \/ ctab.sum()\n    print(ctab_norm)","207cf87d":"# evaluate impact of Soil_Type\nfor f in feature_list_soil:\n    #### cross table - calc absolute counts...\n    ctab = pd.crosstab(df_train.Cover_Type, df_train[f])\n    # ...and normalized by column\n    ctab_norm = ctab \/ ctab.sum()\n    print(ctab_norm)","e5b9d1d3":"# select predictors\npredictors = features_num + feature_list_wild + feature_list_soil\n# predictors = predictors + ['Wilderness_Area_Sum','Soil_Type_Sum']                 \nprint('Number of predictors: ', len(predictors))","3ac501e9":"# start H2O\nh2o.init(max_mem_size='12G', nthreads=4) # Use maximum of 12 GB RAM and 4 cores","fd1daba6":"# upload data frame in H2O environment\nt1 = time.time()\ntrain_hex = h2o.H2OFrame(df_train)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))\n\n# force categorical target\ntrain_hex['Cover_Type'] = train_hex['Cover_Type'].asfactor()","6ac91cbb":"# memory management: remove original data frame + garbage collection\ndel df_train\ngc.collect();","9f175147":"# fit Gradient Boosting model\nn_cv = 5\n\nfit_GBM = H2OGradientBoostingEstimator(ntrees=100,\n                                       max_depth=6,\n                                       min_rows=50,\n                                       learn_rate=0.05, # default: 0.1\n                                       sample_rate=0.25,\n                                       col_sample_rate=0.5,\n                                       nfolds=n_cv,\n                                       score_each_iteration=True,\n                                       stopping_metric='logloss',\n                                       stopping_rounds=5,\n                                       stopping_tolerance=0.0001,\n                                       seed=999)\n# train model\nt1 = time.time()\nfit_GBM.train(x=predictors,\n              y='Cover_Type',\n              training_frame=train_hex)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","abaab713":"# show cross validation metrics\nfit_GBM.cross_validation_metrics_summary()","7bf1f1b5":"# show scoring history - training vs cross validations\nfor i in range(n_cv):\n    cv_model_temp = fit_GBM.cross_validation_models()[i]\n    df_cv_score_history = cv_model_temp.score_history()\n    my_title = 'CV ' + str(1+i) + ' - Scoring History'\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.training_logloss, \n                c='blue', label='training')\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.validation_logloss, \n                c='darkorange', label='validation')\n    plt.title(my_title)\n    plt.xlabel('Number of Trees')\n    plt.ylabel('logloss')\n    plt.ylim(0,2)\n    plt.legend()\n    plt.grid()\n    plt.show()","f91812cd":"# variable importance\nfit_GBM.varimp_plot(-1)","19a356eb":"# predict on train set\npred_train_GBM = fit_GBM.predict(train_hex).as_data_frame()\n# add ground truth\npred_train_GBM['Cover_Type'] = train_hex['Cover_Type'].as_data_frame()\npred_train_GBM.head()","97342133":"# predicted frequencies\npd.options.display.float_format = \"{:.2f}\".format\npred_train_GBM[['p1','p2','p3','p4','p5','p6','p7']].sum()","f21275ea":"# actual frequencies\ntrain_hex['Cover_Type'].as_data_frame().value_counts().sort_index()","6a9141f1":"# confusion matrix - training data\nconf_train = pd.crosstab(pred_train_GBM.Cover_Type, pred_train_GBM.predict)\nsns.heatmap(conf_train, cmap='Blues',\n            annot=True, fmt='d',\n            vmin=0, vmax=3e6,\n            linecolor='black',\n            linewidths=0.1)\nplt.title('Confusion Matrix - Training')\nplt.show()","5b31a8ab":"t1 = time.time()\ntest_hex = h2o.H2OFrame(df_test)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","044d8150":"# memory management: remove original data frame + garbage collection\ndel df_test\ngc.collect();","23164d19":"# predict on test set\npred_test_GBM = fit_GBM.predict(test_hex).as_data_frame()\npred_test_GBM","914dd984":"# export predictions incl. probabilities\npred_test_GBM.to_csv('pred_test_GBM.csv', index=False)","dfb9ef52":"# submission\ndf_sub_GBM = df_sub.copy()\ndf_sub_GBM.Cover_Type = pred_test_GBM.predict\ndf_sub_GBM","d9559d84":"# check frequencies\ndf_sub_GBM.Cover_Type.value_counts()","61b2c1c9":"# export submission\ndf_sub_GBM.to_csv('submission_GBM.csv', index=False)","3c0e5521":"### Numerical Features:","8c6ee263":"<a id='target'><\/a>\n# Target","85224701":"## WORK IN PROGRESS...","84de7372":"<a id='num'><\/a>\n# Numerical Features","a195b6fb":"<a id='target_feats'><\/a>\n# Target vs Features","c8ed9af4":"### Categorical Features:","b638712f":"### Evaluate on training data:","403ea8c4":"### Hmm, we don't have a one hot encoded variable here. There are many cases having more than one \"1\" in a row.","1c3c1bd4":"### Also Soil_Type is not \"unique\"... and the majority of rows have actually no entry at all.","d8ae4e65":"# Table of Contents\n* [Target](#1)\n* [Numerical Features](#num)\n* [Categorical Features](#cat)\n* [Target vs Features](#target_feats)\n* [Fit Model](#model)\n* [Predict on Test Set](#pred_test)","c6278c2d":"<a id='cat'><\/a>\n# Categorical Features","37ad0644":"#### We can ignore class 5 in the pictures above as we have only one observation for this class!","13101c4d":"<a id='model'><\/a>\n# Fit Model","f9b53809":"<a id='pred_test'><\/a>\n# Predict on Test Set"}}