{"cell_type":{"52c6db4f":"code","0fd5a0c7":"code","969e2808":"code","1f7a3188":"code","04f4a3ce":"code","8bc4d60b":"code","f797b25a":"code","202b7819":"code","7a4b4e80":"code","fd23dd9b":"code","dd527157":"code","79c92190":"code","f5469479":"code","b4c02762":"code","135933a0":"code","96253397":"code","1ebcd139":"code","00b07535":"code","4284f21f":"code","e29ff36c":"markdown","817ec47d":"markdown","d1b362d6":"markdown","f7cfc2bd":"markdown","0f4d0e7e":"markdown","d28de5a4":"markdown","a2fe2d16":"markdown","7e9163cc":"markdown","c3eb8484":"markdown","17c34170":"markdown","4404f0a2":"markdown","2ea65486":"markdown","43732ef3":"markdown"},"source":{"52c6db4f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0fd5a0c7":"from keras.preprocessing.text import Tokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom keras.preprocessing.sequence import pad_sequences\nimport keras\nfrom keras.layers import Input,Embedding,Bidirectional,LSTM,Dense,Dropout,TimeDistributed,GlobalAveragePooling1D,BatchNormalization,GlobalMaxPool1D\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\nfrom sklearn.naive_bayes import GaussianNB,BernoulliNB,MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport re\nimport seaborn as sns","969e2808":"train_df = pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv\",encoding=\"L1\")\ntest_df = pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv\",encoding=\"L1\")\n\nprint(f\"train dataset shape >> {train_df.shape}\")\nprint(f\"test dataset shape >> {test_df.shape}\")\n\ndef data_label_split(dataset):\n    data = dataset['OriginalTweet']\n    label = dataset['Sentiment']\n    return data,label\n\ntrain_data,train_label = data_label_split(train_df)\ntest_data,test_label = data_label_split(test_df)\n\ntrain = pd.DataFrame({\n    'label':train_label,\n    'data':train_data\n})\n\ntest = pd.DataFrame({\n    'label':test_label,\n    'data':test_data\n})\n\ndef reassign_label(x):\n    if x == \"Extremely Positive\" or x == \"Positive\":\n        return 1\n    elif x ==\"Extremely Negative\" or x ==\"Negative\":\n        return -1\n    elif x ==\"Neutral\":\n        return 0\n\ntrain.label = train.label.apply(lambda x:reassign_label(x))\ntest.label = test.label.apply(lambda x:reassign_label(x))\n\n\ntrain_data = train.data\ntest_data = test.data\ntrain_label = train.label\ntest_label = test.label\n\ntrain.sample(15)","1f7a3188":"shortword = re.compile(r\"\\b\\w{1,2}\\b\")\nhashtag = re.compile(r\"@[a-zA-Z0-9_]*\")\nwebsite = re.compile(r\"(http|https):*\/+[a-zA-Z0-9.\/]*\")\n\n\ndef remove_short(data):\n    removed=[]\n    for s in data:\n        removed_sentence = shortword.sub('',s)\n        removed_sentence = hashtag.sub('',removed_sentence)\n        removed_sentence = website.sub('',removed_sentence)\n        removed_sentence = removed_sentence.replace(\"#\",\"\")\n        removed.append(removed_sentence.strip())\n    return removed\n\ntrain_data = remove_short(train_data)\ntest_data = remove_short(test_data)\n\nprint(len(train_data))\nprint(len(test_data))","04f4a3ce":"swords = stopwords\nstop_words = set(swords.words('english'))\nprint(len(stop_words))\nprint(\"stopwords samples >> \",stopwords.words('english')[:10])\n\ndef tokenize(data):\n    ret = []\n    for sentence in data:\n        result = word_tokenize(sentence)\n        ret.append(result)\n        \n    return ret\n\ndef remove_stopwords(data):\n    ret = []\n    for sentence in data:\n        result=[]\n        for tok in sentence:\n            if tok not in stop_words:\n                result.append(tok)\n                \n        ret.append(result)\n        \n    return ret\n        \ntrain_data = tokenize(train_data)\ntest_data = tokenize(test_data)\n\ntrain_data = remove_stopwords(train_data)\ntest_data = remove_stopwords(test_data)\n\nprint(len(train_data))\nprint(len(test_data))","8bc4d60b":"train = pd.DataFrame({\n    'label':train_label,\n    'data':train_data\n})\n\ntest = pd.DataFrame({\n    'label':test_label,\n    'data':test_data\n})\n    \ntrain['data'] = train['data'].apply(lambda x:np.nan if (len(x) == 0) else (x))\ntest['data'] = test['data'].apply(lambda x:np.nan if len(x)==0 else x)\n\ntrain.dropna(inplace=True)\ntest.dropna(inplace=True)\n\ntrain_data = train.data\ntrain_label = train.label\ntest_data = test.data\ntest_label = test.label\n\nprint(len(train_data))\nprint(len(test_data))","f797b25a":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_data)\nprint(f\"Size of vocabs >> {len(tokenizer.word_index)}\")\n\nword_counts = tokenizer.word_counts\nk=0\nfreq=0\ntotal_freq=0\nfor key,value in word_counts.items():\n    total_freq = total_freq + value\n    if value<2:\n        k = k+1\n        freq = freq + value\nprint(f\"freq\/total_freq >> {(freq\/total_freq)*100}\")\nprint(f\"{k} words are used only for once\")","202b7819":"word_size = 25000\nvocab_size = word_size+1\n\ntokenizer = Tokenizer(num_words=word_size)\ntokenizer.fit_on_texts(train_data)\n\nword_to_index = tokenizer.word_index\nindex_to_word = tokenizer.index_word\n\ntrain_data = tokenizer.texts_to_sequences(train_data)\ntest_data = tokenizer.texts_to_sequences(test_data)","7a4b4e80":"lens =  [len(s) for s in train_data]\n\nplt.hist(lens,bins=200)\nplt.show()\n\nsequence_size = 50\n\ntrain_data =pad_sequences(train_data,maxlen=sequence_size,padding='post',truncating='post')\ntest_data = pad_sequences(test_data,maxlen=sequence_size,padding='post',truncating='post')","fd23dd9b":"sns.countplot(x=train_label)\nplt.tight_layout()\nplt.show()\n\ntrain_label = pd.get_dummies(train_label)\ntest_label = pd.get_dummies(test_label)","dd527157":"test_label.shape","79c92190":"word_vec_size = 32\nhidden_size = 256\n\ndef create_LSTM():\n    X = Input(shape=[sequence_size])\n    \n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_size)(X)\n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = GlobalMaxPool1D()(H)\n    H = Dropout(0.4)(H)\n    \n    H = Dense(64,activation='relu')(H)\n    H = Dropout(0.4)(H)\n    Y = Dense(3,activation='softmax')(H)\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n        \n    return model\n\nlstm = create_LSTM()\nhist_lstm = lstm.fit(train_data,train_label,epochs=5,validation_split=0.2,batch_size=128)\nprint(\"\\nEvaluation on test dataset >>\\n\")\nlstm.evaluate(test_data,test_label)","f5469479":"from keras.callbacks import EarlyStopping\nfrom keras.layers import Concatenate,Conv1D,GlobalMaxPooling1D,Flatten\n\nes = EarlyStopping(monitor='val_loss',mode='min',patience=3,verbose=1)\n\n\nword_vec_size = 128\nnum_filters=128\n\ndef create_Conv1D(kernel_sizes=[3,4,5]):\n    X = Input(shape=[sequence_size],name=\"Input\")\n    \n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_size)(X)\n    D = Dropout(0.6)(H)\n    \n    \n    conv_blocks = []\n    for k_size in kernel_sizes:\n        H = Conv1D(filters=num_filters,kernel_size=k_size,padding='valid')(D)\n        H = GlobalMaxPooling1D()(H)\n        H= Flatten()(H)\n        conv_blocks.append(H)\n        \n    H = Concatenate()(conv_blocks) if len(conv_blocks) >1 else conv_blocks[0]\n    H = Dropout(0.8)(H)\n    H = Dense(128,activation='relu')(H)\n    Y = Dense(3,activation='softmax')(H)\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    \n    return model\n\nconv1D  = create_Conv1D()\nhist = conv1D.fit(train_data,train_label,epochs=3,validation_split=0.2,batch_size=128)\nprint(\"\\n\\nEvaluation on test dataset >>\\n\\n\")\nconv1D.evaluate(test_data,test_label)","b4c02762":"# def token_to_text(data):\n#     ret =[]\n#     for sentence in data:\n#         result = []\n#         for tok in sentence:\n#             if tok != 0:\n#                 result.append(index_to_word[tok])\n#         if len(result) != 0:\n#             result = \" \".join(result)\n#             ret.append(result)\n#     return ret\n\n# train_x = token_to_text(train_data)\n# test_x  = token_to_text(test_data)\n\ntrain_x = train_df.OriginalTweet\ntest_x = test_df.OriginalTweet\n\ntrain_y = train_df.Sentiment\ntest_y  =test_df.Sentiment","135933a0":"vectorizer = CountVectorizer()\ntransformer = TfidfTransformer()\n\ntrain_x = vectorizer.fit_transform(train_x)\ntest_x = vectorizer.transform(test_x)\n\ntrain_x = transformer.fit_transform(train_x)\ntest_x = transformer.transform(test_x)\n\nprint(train_x.shape)\nprint(test_x.shape)","96253397":"multiNB = MultinomialNB()\nmultiNB.fit(train_x,train_y)\ntest_pred = multiNB.predict(test_x)\nacc = accuracy_score(test_y,test_pred)\nacc = np.round(acc*100,2)\nprint(f\"test acc >> {acc}%\")","1ebcd139":"train_df = pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv\",encoding=\"L1\")\ntest_df = pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv\",encoding=\"L1\")\n\ndf1 = train_df[['Sentiment','OriginalTweet']].copy()\ndf2 = test_df[['Sentiment','OriginalTweet']].copy()\n\nspace = re.compile(r\"\\s{2,}\")\n\ntrain_df.OriginalTweet = train_df.OriginalTweet.str.replace(hashtag,'')\ntrain_df.OriginalTweet = train_df.OriginalTweet.str.replace(website,'')\ntrain_df.OriginalTweet = train_df.OriginalTweet.str.replace(\"#\",\" \")\ntrain_df.OriginalTweet = train_df.OriginalTweet.str.replace(space,\" \")\ntrain_df.OriginalTweet = train_df.OriginalTweet.str.strip()\ntest_df.OriginalTweet = test_df.OriginalTweet.str.replace(hashtag,'')\ntest_df.OriginalTweet = test_df.OriginalTweet.str.replace(website,'')\ntest_df.OriginalTweet = test_df.OriginalTweet.str.replace(\"#\",'')\ntest_df.OriginalTweet = test_df.OriginalTweet.str.replace(space,\" \")\ntest_df.OriginalTweet = test_df.OriginalTweet.str.strip()\n\n\ntrain_df.dropna(inplace=True)\ntest_df.dropna(inplace=True)\n\ntrain_x = train_df.OriginalTweet\ntrain_y = train_df.Sentiment\ntest_x = test_df.OriginalTweet\ntest_y = test_df.Sentiment","00b07535":"vectorizer = CountVectorizer()\ntransformer = TfidfTransformer()\n\ntrain_x = vectorizer.fit_transform(train_x)\ntest_x = vectorizer.transform(test_x)\n\ntrain_x = transformer.fit_transform(train_x)\ntest_x = transformer.transform(test_x)\n\nprint(train_x.shape)\nprint(test_x.shape)","4284f21f":"multiNB = MultinomialNB()\nmultiNB.fit(train_x,train_y)\n\ntrain_pred = multiNB.predict(train_x)\ntrain_acc = accuracy_score(train_y,train_pred)\ntrain_acc = np.round(train_acc*100,2)\nprint(f\"test acc >> {train_acc}%\")\n\ntest_pred = multiNB.predict(test_x)\ntest_acc = accuracy_score(test_y,test_pred)\ntest_acc = np.round(test_acc*100,2)\nprint(f\"test acc >> {test_acc}%\")","e29ff36c":"+) Remove empty rows","817ec47d":"<2> one-hot encode label","d1b362d6":"<3-3> Naive Bayes Models","f7cfc2bd":"<3-2> Multi Kernel Conv1D","0f4d0e7e":"<1-3> Tokenize again to select only words used more than twice & text-integer mapping","d28de5a4":"@, \uc6f9\uc0ac\uc774\ud2b8, # \uc81c\uac70 \ud588\ub294\ub370\ub3c4 \uacb0\uacfc \ucc98\ucc38","a2fe2d16":"<3> Modeling","7e9163cc":"(1) These models require additional text preprocessing (token to text)","c3eb8484":"<1-4> Padding","17c34170":"<1-1> 2\uae00\uc790 \uc774\ud558 \ub2e8\uc5b4 \uc81c\uac70, @\ub369\uc5b4\ub9ac \uc81c\uac70, #\uc81c\uac70, url \uc8fc\uc18c \uc81c\uac70","4404f0a2":"<1-2> tokenize & stopwords \uc81c\uac70 (nltk.corpus)","2ea65486":"<3-1> LSTM model","43732ef3":"preprocessing \uc548\ud558\uace0 \uadf8\ub0e5 \ub123\uc73c\ub2c8\uae4c \uacb0\uacfc \ucc98\ucc38"}}