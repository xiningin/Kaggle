{"cell_type":{"d12331c5":"code","89336c37":"code","ed0a4446":"code","aafd22c3":"code","85c967a0":"code","23f7d360":"code","61cfd965":"code","dd3969f0":"code","9684a58d":"code","c4aa3e77":"code","3be1717e":"code","61583eb8":"code","74ec9bc6":"code","31493e15":"code","6eb6856f":"code","fd1a6b76":"code","37c7609b":"code","c7e1b572":"code","78f6871c":"code","b64ca5b8":"code","93ee9efb":"code","a49be8f3":"code","b6cf448d":"code","340308cf":"code","48267149":"code","fbad1d59":"code","3afd4e7d":"code","bf83772d":"markdown","52e4c99b":"markdown","64a15b41":"markdown","d48e4824":"markdown","3c8e38d7":"markdown","982e5d6a":"markdown","9d357240":"markdown","5e8fe6e8":"markdown","e2bf4468":"markdown","a1e365f5":"markdown"},"source":{"d12331c5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","89336c37":"from sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nimport keras as K\nfrom keras.utils import to_categorical \nfrom keras.layers import Dense,Flatten,Conv2D,Dropout,BatchNormalization,Activation,Input,MaxPooling2D\nfrom keras import regularizers\nfrom keras.optimizers import Adam\nfrom keras.models import Model\n\nimport matplotlib.pyplot as plt","ed0a4446":"print(tf.test.gpu_device_name())","aafd22c3":"train_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')","85c967a0":"Y_train = train_data['label']\n\nX_train = train_data.drop(labels = ['label'],axis=1).values","23f7d360":"X_train.shape","61cfd965":"X_train = X_train.reshape((X_train.shape[0],28,28))\nX_train = np.expand_dims(X_train, axis=3)\nX_train.shape","dd3969f0":"nb_class = 10\nY_train.shape","9684a58d":"Y_train = to_categorical(Y_train,num_classes = nb_class)","c4aa3e77":"Y_train.shape","3be1717e":"X_train, X_val, Y_train, Y_val = train_test_split(X_train,Y_train,test_size=0.2,random_state=42)","61583eb8":"print(\"Shape of train set: \" + str(X_train.shape))\nprint(\"Shape of validation set: \" + str(X_val.shape))\nprint(\"Shape of train set labels: \" + str(Y_train.shape))\nprint(\"Shape of validation set labels: \" + str(Y_val.shape))","74ec9bc6":"del train_data","31493e15":"X_train = X_train.astype(float)\nX_val = X_val.astype(float)","6eb6856f":"X_train \/= 255\nX_val \/= 255","fd1a6b76":"def conv_block(input_tensor,\n               filters,\n               kernel_size,\n               stage=None,\n               block=None,\n               padding='same',\n               strides=(1,1),\n               batch_mom=0.0,\n               batch_axis=3,\n               initializer='RandomNormal'):\n    \n    conv_name_base = 'model_' + str(stage) + '_' + str(block)\n    \n    x = Conv2D(filters=filters,\n               kernel_size=kernel_size,\n               padding=padding,\n               strides=strides,\n               kernel_initializer=initializer,\n               name = conv_name_base)(input_tensor)\n    x = BatchNormalization(axis=batch_axis,momentum=batch_mom)(x)\n    x = Activation('relu')(x)\n    return x\n    \ninput_shape = (28,28,1) \ninitializer='glorot_normal'\nreg_val=0.0\n\nmain_input = Input(shape=(input_shape),name='input_of_network')\nx = conv_block(main_input,\n               filters=32,\n               kernel_size=(3,3),\n              stage='s1',\n              block='3_3',\n              padding='valid')\n\ny = conv_block(main_input,\n              filters=32,\n              kernel_size=(3,3),\n              stage='s2',\n              block='3_3',\n              padding='valid',\n              strides=(2,2))\n\nz = conv_block(main_input,\n              filters=96,\n              kernel_size=(3,3),\n              stage='s3',\n              block='3_3',\n              padding='valid',\n              strides=(3,3))\n\nx = MaxPooling2D(pool_size=(2,2),strides=(2,2))(x)\n\nx_y = K.layers.concatenate([x,y])\n\nx_y = conv_block(x_y,\n              filters=96,\n              kernel_size=(1,1),\n              stage='2',\n              block='1_1',\n              padding='same',\n              strides=(1,1))\n\nx_y = conv_block(x_y,\n              filters=128,\n              kernel_size=(3,3),\n              stage='2',\n              block='3_3',\n              padding='valid',\n              strides=(1,1))\n\nx_y = conv_block(x_y,\n              filters=144,\n              kernel_size=(3,3),\n              stage='3',\n              block='3_3',\n              padding='valid',\n              strides=(1,1))\n\nx_y_z = K.layers.concatenate([x_y,z])\n\nout = Flatten()(x_y_z)\nout = Dense(256,kernel_initializer=initializer,kernel_regularizer=regularizers.l2(reg_val))(out)\nout = BatchNormalization()(out)\nout = Activation('relu')(out)\nout = Dropout(0.3)(out)\n\nout = Dense(256,kernel_initializer=initializer,kernel_regularizer=regularizers.l2(reg_val))(out)\nout = BatchNormalization()(out)\nout = Activation('relu')(out)\nout = Dropout(0.3)(out)\n\nout = Dense(10,activation='softmax')(out)\n\nmodel = Model(main_input,out)\n\nmodel.summary()\n","37c7609b":"adam = Adam(lr=1e-3)\nmodel.compile(loss='categorical_crossentropy',metrics=['acc'],optimizer=adam)","c7e1b572":"model.fit(x=X_train,\n         y=Y_train,\n         shuffle=True,\n        batch_size = 256,\n         validation_data=(X_val,Y_val),\n         epochs=100,\n         verbose=1)","78f6871c":"plt.plot(model.history.history['acc'],'r-')\nplt.xlabel('Epochs')\nplt.ylabel('Acc')\nplt.show()","b64ca5b8":"plt.plot(model.history.history['loss'],'b-')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.show()","93ee9efb":"plt.plot(model.history.history['val_acc'],'r-')\nplt.xlabel('Epochs')\nplt.ylabel('Validation Acc.')\nplt.show()","a49be8f3":"plt.plot(model.history.history['val_loss'],'b-')\nplt.xlabel('Epochs')\nplt.ylabel('Validation Loss')\nplt.show()","b6cf448d":"test_data.shape","340308cf":"test_data = test_data.values\ntest_data = np.reshape(test_data, (test_data.shape[0],28,28)).astype(float)\ntest_data \/= 255\ntest_data = np.expand_dims(test_data, axis=3)\ntest_data.shape","48267149":"y_pred = model.predict(test_data)","fbad1d59":"y_pred_submission = np.argmax(y_pred,axis = 1)\ny_pred_submission = y_pred_submission.astype(int)\ny_pred_submission[:10]","3afd4e7d":"np.savetxt('submission.csv',y_pred_submission,delimiter=',')","bf83772d":"Okay, this is my fault. I forget to convert out data to float. Because I want to use normalization operation for doing that \u0131 have to convert to data to float.","52e4c99b":"My model is kind of different as comparing to a normal convnet architecture. I assumed that to have different conv strides provides to finding different features of an image. Therefore I used to different strides for the conv operations. you could improve the results by using data augmentation, kernel regularizers or more dropout. If you like my model please leave a comment or like.","64a15b41":"We need to check test data shape for the submission predictions.","d48e4824":"Well, we did some preprocessing to training data set as you see in upside. Test data have to same preprocessing as training data have. Okay than let's do it.","3c8e38d7":"**What will we do?**\n1. Importing libraries\n2. Prepraring data\n3. Train test split\n4. Creating model from scracth\n5. Training and testing\n6. Submission","982e5d6a":"I chose Adam optimizer with its initial learning rate 1e-3. If you want to use different optimizer, you can choose whatever you want..","9d357240":"Okay, Let's check the datasets.","5e8fe6e8":"Checking whether or not using GPU.","e2bf4468":"For releasing to memory delete train_data because we already have X_train and X_val","a1e365f5":"**Results Visualization**\nAfter the training is done, we can do some visualization of the training and validation acc and also losses to."}}