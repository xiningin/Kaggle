{"cell_type":{"17d9f145":"code","560d15ef":"code","fbf3f533":"code","7588fe85":"code","11b8e96d":"code","38178e7b":"code","6a134b18":"code","f9574b30":"code","2611e63c":"code","02edda6a":"code","bf192025":"code","ead12cb3":"code","067e4647":"code","b1f4187a":"code","08fa0b6e":"code","5b9b6c93":"code","25a685bc":"code","dc9828b1":"code","4e7c89e5":"code","2149d68d":"code","a23b91b2":"code","1ed7c9e3":"code","7ff72bbd":"code","7bb93a12":"code","4fb02bdc":"code","a21b5077":"code","7b7b0c25":"code","f0c41945":"code","3e5abf4b":"code","ef7644c2":"code","6b626283":"code","877132af":"code","81e592bf":"code","17db3753":"code","31c78cb0":"code","8d396954":"code","98887f32":"code","d01927ca":"code","0fd38189":"code","0aa7ed06":"code","8edb145c":"code","c1fc6f80":"code","af96e6ee":"code","d780be34":"code","baeb5b38":"code","ef363373":"code","ac526abc":"code","08420a2c":"code","81eb93dc":"code","fa0709e1":"code","0da370fe":"code","a7cb9feb":"code","a280db06":"code","d7960dd5":"code","adf1fb26":"code","77a633da":"code","0c072408":"code","ccf46471":"code","e7e485d1":"code","79eb3e04":"code","ab483201":"code","4cb371ad":"markdown","73104d4e":"markdown","dd6ea236":"markdown","1e3569e2":"markdown","5b2b2f8e":"markdown","cea789d4":"markdown","5d878cf8":"markdown","d21ea4db":"markdown","89b4c1e8":"markdown","3ae8bc6d":"markdown","c14c30b9":"markdown","4370888b":"markdown","e2b8e694":"markdown","08911d2e":"markdown","43df78a7":"markdown","e3571dd6":"markdown","0b953ca3":"markdown","ac142953":"markdown","8f160d48":"markdown","06272039":"markdown","13e7d62e":"markdown","c51d8749":"markdown","67e57f0a":"markdown","7d9abb82":"markdown","965a161d":"markdown","4bdd48d8":"markdown","b1e6e1b8":"markdown","1719bc25":"markdown","054d52c2":"markdown","87d787ef":"markdown","83bad3b7":"markdown","1ede1d07":"markdown","01c053c6":"markdown","a6ae97cd":"markdown","2caf0bbf":"markdown","27bbcd96":"markdown","726a73ab":"markdown","a7f5dce1":"markdown"},"source":{"17d9f145":"from IPython.display import Image","560d15ef":"import pickle\n\nimport pandas as pd\nfrom pathlib import Path\nfrom fastai import *\nfrom fastai.text import *\nfrom sklearn import metrics\nfrom torch import nn","fbf3f533":"import neptune","7588fe85":"class MSELoss(nn.MSELoss):\n  \"Mean Absolute Error Loss\"\n  def forward(self, input:Tensor, target:Tensor) -> Rank0Tensor:\n    return super().forward(input.view(-1), target.view(-1))","11b8e96d":"k_folds = 8\ntarget_col_name = 'target'\ntarget_class_col_name = 'class_target'\nx_col_name = 'comment_text'\n\nepochs = 1\nlayer1_lr = 2e-2\nlayer2_lr = slice(1e-2\/(2.6**4),1e-2)\nmomentum = (0.8,0.7)\nloss_function = MSELoss\nDROP_MULT = 0.5\nfreeze_layer_idx = -2\narchitecture = AWD_LSTM\n\nbs = 40\nSEED = 777\n\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']","38178e7b":"!cp ..\/input\/toxic-regression-model\/exported_reg_mse_toxic_lm_extended_15ep.pkl ..\/working\/","6a134b18":" # data\npath = Path('..\/input\/jigsaw-unintended-bias-in-toxicity-classification')\nclas_csv_file = 'train.csv'\ntest_csv_file = 'test.csv'\n\npath_lm = Path('..\/input\/data')\nlm_csv_file = 'combined_toxic_lm.csv'\n\n# pretrained\npretrained_lm_path = Path('..\/input\/basic-toxic-languagemodel-extended-15ep')\nencoder_file = 'fine_tuned_encoder_basic_toxic_lm15ep_extended'\ndata_lm_file = 'data_reg_basic_toxic_lm_extended_15ep.pkl'\npretrained_reg_path = Path('..\/working')\nreg_model_file = 'exported_reg_mse_toxic_lm_extended_15ep.pkl'\n\n# results\nexp_nb = 2\nmodel_cv_result_file = 'toxic_test_cv_{}'\nmodel_prod_cv_result_file = 'toxic_production_cv_{}'\nk_folds_file = 'holdout_and_k_folds_idxs_exp_nb_{}.pkl'.format(exp_nb)\nmodels_performance_file = 'toxic_CV_models_performance_exp_nb_{}.pkl'.format(exp_nb)\nsubmission_predictions = 'toxic_CV_submission_exp_nb_{}.csv'.format(exp_nb)\n\nfinal_model_result_file = 'toxic_final_model_{}'.format(exp_nb)\nfinal_model_prod_result_file = 'toxic_production_final_{}'.format(exp_nb)\nfinal_submission_predictions = 'toxic_final_submission_exp_nb_{}.csv'.format(exp_nb)","f9574b30":"neptune.init(api_token='YOUR_API_TOKEN',\n             project_qualified_name='lachonman\/toxic-bias')\n\nnep_exp = neptune.create_experiment(name='FastAI_regression_exp_nb_{}'.format(exp_nb),\n                          description=' Regression model implemented in FastAI that predicts'\n                          'probablity that given text belongs to toxic category. It uses pretrained'\n                          'Language Model that was trained on train+test and old toxic text corpora.'\n                          'Official metric used to evaluate model performance is \"toxic_metric\".'\n                          'Sorted stratificated k fold cross validation with 1'\n                          'holdout is used to tune paramters and measure metrics.',\n                          params={'loss_func': str(loss_function.__name__),\n                                  'lr_layer1': layer1_lr,\n                                  'lr_layer2': layer2_lr,\n                                  'momentum': momentum,\n                                  'epochs_count': epochs,\n                                  'dropout': DROP_MULT,\n                                  'freeze_layer_idx': freeze_layer_idx,\n                                  'architecture': architecture,\n                                  'k_folds': k_folds})\n\nnep_exp.append_tag('regression')\nnep_exp.append_tag('k folds CV')\nnep_exp.append_tag('pretrained extended LangModel')","2611e63c":"data_lm = (TextList.from_csv(path_lm, lm_csv_file, cols='comment_text')\n                .split_by_rand_pct(0.1)\n                .label_for_lm()\n                .databunch(bs=bs))","02edda6a":"data_lm.save(pre_trained_path\/data_lm_file)","bf192025":"data_lm = load_data(pre_trained_path, data_lm_file, bs=bs)","ead12cb3":"data_lm.show_batch()","067e4647":"learn = language_model_learner(data_lm, AWD_LSTM, pretrained=True, drop_mult=0.3)","b1f4187a":"learn.lr_find()","08fa0b6e":"learn.recorder.plot(skip_end=15, suggestion=True)","5b9b6c93":"Image(\"..\/input\/fastai-lm-on-extended-datasetcv-regression-source\/lm_lr_find1.png\")","25a685bc":"learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))","dc9828b1":"Image(\"..\/input\/fastai-lm-on-extended-datasetcv-regression-source\/lm_fit_ep1_loss_table.png\")","4e7c89e5":"learn.save('fit_head_basic_toxic_lm1ep_extended')","2149d68d":"learn.load('fit_head_basic_toxic_lm1ep_extended');","a23b91b2":"learn.unfreeze()","1ed7c9e3":"learn.fit_one_cycle(15, 1e-3, moms=(0.8,0.7))","7ff72bbd":"Image(\"..\/input\/fastai-lm-on-extended-datasetcv-regression-source\/lm_fit_ep15_loss_table.png\")","7bb93a12":"learn.save('fine_tuned_basic_toxic_lm15ep_extended')","4fb02bdc":"learn.load('fine_tuned_basic_toxic_lm15ep_extended');","a21b5077":"learn.save_encoder('fine_tuned_encoder_basic_toxic_lm15ep_extended')","7b7b0c25":"with open('fine_tuned_basic_toxic_vocab_lm15ep_extended.pcl', 'wb') as f:\n    pickle.dump(data_lm.vocab, f)","f0c41945":"def get_k_stratified_folds_indexes_given_continues_variable(df, k_folds, continuous_target_col_name):\n    nb_samples = df.shape[0]\n    split_remainder = nb_samples%k_folds\n    groups_count = nb_samples\/\/k_folds\n    \n    idxs_train_remainder = df[:split_remainder].index.values\n    idxs_train_trimmed = df[split_remainder:]\n\n    idxs_train_sorted = idxs_train_trimmed.sort_values(by=continuous_target_col_name).index.values\n\n    k_groups = np.split(idxs_train_sorted, groups_count)\n    list(map(np.random.shuffle, k_groups))\n    folds_idxs = [np.array(x) for x in zip(*k_groups)]\n    folds_idxs[0] = np.concatenate([folds_idxs[0], idxs_train_remainder])\n    \n    return np.array(folds_idxs)\n\ndef get_train_and_valid_idxs_split_given_fold_nb(folds_idxs, k):\n    mask = np.ones(len(folds_idxs), dtype=bool)\n    mask[k] = 0\n    folds_idxs[mask]\n    train_idxs = np.concatenate(folds_idxs[mask])\n    \n    test_idxs = folds_idxs[1]\n    return train_idxs, test_idxs","3e5abf4b":"\n# Convert taget and identity columns to booleans\ndef get_col_converted_to_bool(df, col_name):\n    return np.where(df[col_name] >= 0.5, True, False)\n    \ndef get_df_with_converted_categorical_cols_to_bool(df, categorical_cols):\n    for col in categorical_cols:\n        df[col] = get_col_converted_to_bool(df, col)\n    return df\n\ndef get_df_with_class_target_col(df, target_col_name, target_class_col_name):\n    df[target_class_col_name] = get_col_converted_to_bool(df, target_col_name)\n    return df","ef7644c2":"df_train_initial = pd.read_csv(path\/clas_csv_file)","6b626283":"df_train_initial = get_df_with_converted_categorical_cols_to_bool(\n    df=df_train_initial,\n    categorical_cols=identity_columns)\ndf_train_initial = get_df_with_class_target_col(\n    df=df_train_initial,\n    target_col_name=target_col_name,\n    target_class_col_name=target_class_col_name)","877132af":"df_train_initial = df_train_initial.sample(frac=1 ,random_state=SEED)\n\nholdout_and_k_folds_idxs = get_k_stratified_folds_indexes_given_continues_variable(df_train_initial, k_folds, target_col_name)","81e592bf":"models_performance = {}\n# if holdout then k-1 cuz one fold went on holdout\nfor k in range(k_folds-1):\n\n    train_idxs, valid_idxs = get_train_and_valid_idxs_split_given_fold_nb(folds_idxs, k)\n    \n    data_clas = (TextList.from_df(df_train_initial, path=path, cols=x_col_name, vocab=data_lm.vocab)\n             .split_by_idxs(train_idx=train_idxs, valid_idx=valid_idxs)\n             .label_from_df(cols=target_col_name, label_cls=FloatList)             \n             .databunch(bs=bs))\n\n    learn = text_classifier_learner(data_clas, architecture, drop_mult=DROP_MULT, metrics=[mse, ToxicMetric()])\n    learn.load_encoder(encoder_file)\n    \n    learn.loss = loss_function\n    learn.fit_one_cycle(epochs, layer1_lr, moms=momentum)\n    learn.freeze_to(freeze_layer_idx)\n    learn.fit_one_cycle(epochs, layer2_lr, moms=momentum)\n    learn.save(model_cv_result_file.format(k))\n    learn.export(model_prod_cv_result_file.format(k))\n    \n    learn.data.add_test(df_train_initial[x_col_name][holdout_idx])\n    prob_preds = learn.get_preds(ds_type=DatasetType.Test, ordered=True)\n    batch_preds = [pred[0] for pred in prob_preds[0].numpy()]\n    models_performance[k] = {'holdout_preds': batch_preds,\n                             'train_loss': learn.recorder.losses,\n                             'valid_loss': learn.recorder.val_losses}","17db3753":"train_folds_losses = []\nfor model_nb in models_performance:\n    train_loss = models_performance[model_nb]['train_loss']\n    train_score = [float(step.numpy()) for step in train_loss]\n    train_folds_losses.append(train_score)\n\nfolds_train_losses = [x[-1] for x in train_folds_losses]\nprint('folds train losses', folds_train_losses)\navg_train_losses = np.mean(train_folds_losses, axis=0)\navg_train_loss = avg_train_losses[-1]\nprint('avg train loss', avg_train_loss)","31c78cb0":"folds train losses [0.018044915050268173, 0.019446002319455147, 0.018564680591225624, 0.017788488417863846, 0.018882934004068375, 0.01710507646203041, 0.017085056751966476]\navg train loss 0.01813102194241115","8d396954":"valid_folds_losses = [models_performance[x]['valid_loss'][-1] for x in models_performance]\nprint('folds valid losses', valid_folds_losses)\navg_valid_loss = np.mean(valid_folds_losses, axis=0)\nprint('avg valid loss', avg_valid_loss)","98887f32":"folds valid losses [0.5292527, 54.811653, 0.017135596, 0.29703587, 0.026240299, 26.354286, 0.049951334]\navg valid loss 11.726507","d01927ca":"holdout_group_preds = [models_performance[x]['holdout_preds'] for x in models_performance]","0fd38189":"avg_holdout_preds = np.mean(holdout_group_preds, axis=0)","0aa7ed06":"correct_preds_count = 0\nmse_loss_holdout = []\nsample_count = len(avg_holdout_preds)\nfor i in range(sample_count):\n    single_holdout_idx = holdout_idx[i]\n    target = df_train_initial['target'].loc[single_holdout_idx]\n    ensamble_pred = avg_holdout_preds[i]\n    mse_pred = (ensamble_pred - target)**2\n    mse_loss_holdout.append(mse_pred)\n    if (ensamble_pred>0.5 and target>0.5) or (ensamble_pred<0.5 and target<0.5):\n        correct_preds_count += 1\nholdout_acc = correct_preds_count\/sample_count\nprint('holdout_acc', holdout_acc)\nholdout_loss = sum(mse_loss_holdout)\/len(mse_loss_holdout)\nprint('holdout_loss ', holdout_loss)","8edb145c":"holdout_acc 0.9462703502932038\nholdout_loss  5.278712082137559","c1fc6f80":"SUBGROUP_AUC = 'subgroup_auc'\nBPSN_AUC = 'bpsn_auc'  # stands for background positive, subgroup negative\nBNSP_AUC = 'bnsp_auc'  # stands for background negative, subgroup positive\n\ndef compute_auc(y_true, y_pred):\n    try:\n        return metrics.roc_auc_score(y_true, y_pred)\n    except ValueError:\n        return np.nan\n\ndef compute_subgroup_auc(df, subgroup, label, model_name):\n    subgroup_examples = df[df[subgroup]]\n    return compute_auc(subgroup_examples[label], subgroup_examples[model_name])\n\ndef compute_bpsn_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup negative examples and the background positive examples.\"\"\"\n    subgroup_negative_examples = df[df[subgroup] & ~df[label]]\n    non_subgroup_positive_examples = df[~df[subgroup] & df[label]]\n    examples = subgroup_negative_examples.append(non_subgroup_positive_examples)\n    return compute_auc(examples[label], examples[model_name])\n\ndef compute_bnsp_auc(df, subgroup, label, model_name):\n    \"\"\"Computes the AUC of the within-subgroup positive examples and the background negative examples.\"\"\"\n    subgroup_positive_examples = df[df[subgroup] & df[label]]\n    non_subgroup_negative_examples = df[~df[subgroup] & ~df[label]]\n    examples = subgroup_positive_examples.append(non_subgroup_negative_examples)\n    return compute_auc(examples[label], examples[model_name])\n\ndef compute_bias_metrics_for_model(dataset,\n                                   subgroups,\n                                   model,\n                                   label_col,\n                                   include_asegs=False):\n    \"\"\"Computes per-subgroup metrics for all subgroups and one model.\"\"\"\n    records = []\n    for subgroup in subgroups:\n        record = {\n            'subgroup': subgroup,\n            'subgroup_size': len(dataset[dataset[subgroup]])\n        }\n        record[SUBGROUP_AUC] = compute_subgroup_auc(dataset, subgroup, label_col, model)\n        record[BPSN_AUC] = compute_bpsn_auc(dataset, subgroup, label_col, model)\n        record[BNSP_AUC] = compute_bnsp_auc(dataset, subgroup, label_col, model)\n        records.append(record)\n    return pd.DataFrame(records).sort_values('subgroup_auc', ascending=True)","af96e6ee":"def calculate_overall_auc(df, model_name):\n    true_labels = df[TOXICITY_COLUMN]\n    predicted_labels = df[model_name]\n    return metrics.roc_auc_score(true_labels, predicted_labels)\n\ndef power_mean(series, p):\n    total = sum(np.power(series, p))\n    return np.power(total \/ len(series), 1 \/ p)\n\ndef get_final_metric(bias_df, overall_auc, POWER=-5, OVERALL_MODEL_WEIGHT=0.25):\n    bias_score = np.average([\n        power_mean(bias_df[SUBGROUP_AUC], POWER),\n        power_mean(bias_df[BPSN_AUC], POWER),\n        power_mean(bias_df[BNSP_AUC], POWER)\n    ])\n    return (OVERALL_MODEL_WEIGHT * overall_auc) + ((1 - OVERALL_MODEL_WEIGHT) * bias_score)","d780be34":"MODEL_NAME = 'regression'\nvalidate_df = df_train_initial.loc[holdout_idx]\nvalidate_df[MODEL_NAME] = avg_holdout_preds\nTOXICITY_COLUMN = target_class_col_name\n\nbias_metrics_df = compute_bias_metrics_for_model(validate_df, identity_columns, MODEL_NAME, TOXICITY_COLUMN)\nbias_metrics_df","baeb5b38":"Image(\"..\/input\/fastai-lm-on-extended-datasetcv-regression-source\/toxic_subgroups_auc.png\")","ef363373":"toxic_metric_acc = get_final_metric(bias_metrics_df, calculate_overall_auc(validate_df, MODEL_NAME))","ac526abc":"for fold_nb in range(len(valid_folds_losses)):\n    neptune.send_metric(channel_name='valid_loss per fold', x=fold_nb, y=valid_folds_losses[fold_nb])","08420a2c":"for step in range(len(avg_train_losses)):\n    neptune.send_metric(channel_name='train_avg_folds_losses', x=step, y=avg_train_losses[step])","81eb93dc":"neptune.send_metric(channel_name='avg_train_loss', x=exp_nb, y=avg_train_loss)\nneptune.send_metric(channel_name='avg_valid_loss', x=exp_nb, y=avg_valid_loss)\n\nneptune.send_metric(channel_name='holdout_loss', x=exp_nb, y=holdout_loss)\nneptune.send_metric(channel_name='holdout_acc', x=exp_nb, y=holdout_acc)\n\nneptune.send_metric(channel_name='toxic_metric', x=exp_nb, y=toxic_metric_acc)","fa0709e1":"for _, row in bias_metrics_df.iterrows():\n    key = 'subgroup_'+row['subgroup']+'_auc'\n    neptune.send_metric(channel_name=key, x=exp_nb, y=row['subgroup_auc'])","0da370fe":"neptune.experiments.get_current_experiment().get_hardware_utilization()","a7cb9feb":"neptune.stop()","a280db06":"data_clas = (TextList.from_df(df_train_initial, path=path, cols=x_col_name, vocab=data_lm.vocab)\n         .split_none()\n         .label_from_df(cols=target_col_name, label_cls=FloatList)             \n         .databunch(bs=bs))\n\nlearn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=DROP_MULT)\nlearn.load_encoder(encoder_file)\n\nlearn.loss = loss_function\nlearn.fit_one_cycle(epochs, layer1_lr, moms=momentum)\nlearn.freeze_to(freeze_layer_idx)\nlearn.fit_one_cycle(epochs, layer2_lr, moms=momentum)","d7960dd5":"Image(\"..\/input\/fastai-lm-on-extended-datasetcv-regression-source\/final_model_table_loss.png\")","adf1fb26":"learn.export(final_model_prod_result_file)","77a633da":"test_df = pd.read_csv(path\/test_csv_file)","0c072408":"#learn = load_learner(pretrained_reg_path, reg_model_file)\n\nlearn.data.add_test(test_df[x_col_name])\nprob_preds = learn.get_preds(ds_type=DatasetType.Test, ordered=True)\nbatch_preds = [pred[0] for pred in prob_preds[0].numpy()]\nbatch_preds[:5]","ccf46471":"test_df['prediction'] = batch_preds\nsubmission = test_df[['id','prediction']]\nsubmission[:5]","e7e485d1":"submission.to_csv(path\/final_submission_predictions, index=False)","79eb3e04":"df_submit = pd.read_csv('..\/input\/preds-toxic-reg\/toxic_final_submission_exp_nb_2.csv')","ab483201":"df_submit.to_csv('submission.csv', index=False)","4cb371ad":"We've also used quite a handy API 'neptune' that helps us keep results of our experiments organized","73104d4e":"### Content\nIn this notebook, we are exploring the possibility of using transfer learning in the biased class prediction problem. \n- First, we fit a language model that was pretrained on WikiText to our specific dataset\n- Then we perform stratified sorted spliting of the data into K folds so that we can perform Cross Validation on K-1 of those folds and use Kth fold as testset\n- Using the models trained on K-1 folds we predict probabilites for Kth fold i.e. data that none of the models ever seen and calculate toxic metric used in the competition\n- Finally, we train a model on the full dataset using the best parameters we found on our validation set and create a submission","dd6ea236":"##  Metrics","1e3569e2":"#### Send average training loss history","5b2b2f8e":"## Train model on each of the K folds and store the results\nSince in target we are given a probabilites I wanted to try to perform regression instead of classification and submmit predicted value from 0 to 1 as probability that given text is toxic","cea789d4":"## Predict for submission\nFinally we load our exported model and use it to produce binary class probabilities used in our submission","5d878cf8":"### Summary of ensamble scores\n- train loss 0.021\n- valid loss 0.66\n- holdout loss 0.169\n- holdout acc 0.946 (toxicity classification)\n- toxic holdout metric acc 0.919\n- testset avg CV acc 0.91310\n- toxic metric testset full_train acc 0.91394","d21ea4db":"# Fit pretrained Language Model from FastAI","89b4c1e8":"source kernel: https:\/\/www.kaggle.com\/dborkan\/benchmark-kernel","3ae8bc6d":"### Train loss","c14c30b9":"#### Send registered usage of gpu and cpu during the experiment","4370888b":"### General Improvement Ideas\n- Implement loss functions that weights samples that belong to minority classes according to their importance in regression. Similarly to `sample_weights` in [Simple LSTM](https:\/\/www.kaggle.com\/thousandvoices\/simple-lstm) kernel (but in Simple LSTM we perform binary classification)\n- Use google translate to perform data augmentation. Since it's quite expensive it could be done only on the most underrepresented subgroups\n- Replace words that can are distinctive for given ethnicity with tokens  so that model cant generalize to unwanted bias\n- Apply data cleaning from greate kernel [clean-data-keras-embbedings-cudnn-predict](https:\/\/www.kaggle.com\/nikhilsharma00\/clean-data-keras-embbedings-cudnn-predict) on toxic biased train\/test on check if already cleaned old toxic data can get any cleaner","e2b8e694":"### Convert categorical columns to bool","08911d2e":" ## Send results of training to neptune\n- [Improvment Idea] What you can do is add to callback that updates metrics during traning of a model","43df78a7":"#### Send final valid loss of every fold","e3571dd6":"### Houldout loss and accuracy","0b953ca3":"### Load combined data\nOur combined data consists of toxic biased train+test and old cleaned toxic data. We can use unlabeled data to train the language model because labels for the language model can be created from the text itself","ac142953":"### Additional Remarks","8f160d48":"Encoder is the part of language model that we want to use in our classifier","06272039":"#### End the experiment","13e7d62e":"## Parameters","c51d8749":"### Valid loss","67e57f0a":"We use export method so that only essentail part of the model that is nessecary for predicting is saved","7d9abb82":"### Create K stratified sorted folds\nUse sorted stratification to get more reprezentative distribution of the data in folds. Implemented based on blog post https:\/\/scottclowe.com\/2016-03-19-stratified-regression-partitions\/","965a161d":"##   Final predictions on testset","4bdd48d8":"### Fit Language Model\nBy default langauge model from FastAI is pretrained on WikiText","b1e6e1b8":"# ULMFiT on toxic bias","1719bc25":" ## Calculate biased metric","054d52c2":"[Improvment idea] Check what was the problem with 2 out of 7 folds that had very high loss rate","87d787ef":"## Start logging the experiment using neptune","83bad3b7":"#  Load data and split into folds using sorted stratification","1ede1d07":"#### Send score of each subgroup","01c053c6":"After finding the best parameters using cross validation and holdout we should use those paramters to train a model on full data set","a6ae97cd":"source kernel: https:\/\/www.kaggle.com\/dborkan\/benchmark-kernel","2caf0bbf":"### Load the training data for classificatior","27bbcd96":"- Most of the calculations were performed on the Google cloud and training logs are shown as pictures\n- Ideas that I think might improve the performance of the model will be scattered all over the notebook. I'm planning to implement them when I find a little bit of free time. If they enhance the performance I'll add them to this kernel in the other case there will be a note '(Not improved performance)' next to the ideas that failed\n- Any comments about ideas that might be improved this kernel or any mistakes that I've made are welcome\n- If you spot that I've added some code from other kernel and forgot to add reference please let me know","726a73ab":"If our language model is a bit too big we can save only vocabulaty that will be used by classifier","a7f5dce1":"## Paths"}}