{"cell_type":{"30d776b0":"code","48e0af1e":"code","4d449002":"code","90edf2d9":"code","ed235a82":"code","ac2ae7c0":"code","583af984":"code","0bab5464":"code","d1aba37a":"markdown"},"source":{"30d776b0":"import matplotlib.pyplot as plt\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nfrom collections import OrderedDict\nimport sys\n#import glob\n#import argparse\nimport PIL.Image as pil\nimport matplotlib as mpl\nimport matplotlib.cm as cm\nfrom torchvision import transforms\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","48e0af1e":"def upsample(x):\n    \"\"\"Upsample input tensor by a factor of 2\n    \"\"\"\n    return F.interpolate(x, scale_factor=2, mode=\"nearest\")","4d449002":"class DepthDecoder(nn.Module):\n    def __init__(self, num_ch_enc, scales=range(4), num_output_channels=1, use_skips=True):\n        super(DepthDecoder, self).__init__()\n        self.num_output_channels = num_output_channels\n        self.use_skips = use_skips\n        self.upsample_mode = 'nearest'\n        self.scales = scales\n        self.num_ch_enc = num_ch_enc\n        self.num_ch_dec = np.array([16, 32, 64, 128, 256])\n        # decoder\n        self.convs = OrderedDict()\n        for i in range(4, -1, -1):\n            # upconv_0\n            num_ch_in = self.num_ch_enc[-1] if i == 4 else self.num_ch_dec[i + 1]\n            num_ch_out = self.num_ch_dec[i]\n            self.convs[(\"upconv\", i, 0)] = ConvBlock(num_ch_in, num_ch_out)\n            # upconv_1\n            num_ch_in = self.num_ch_dec[i]\n            if self.use_skips and i > 0:\n                num_ch_in += self.num_ch_enc[i - 1]\n            num_ch_out = self.num_ch_dec[i]\n            self.convs[(\"upconv\", i, 1)] = ConvBlock(num_ch_in, num_ch_out)\n        for s in self.scales:\n            self.convs[(\"dispconv\", s)] = Conv3x3(self.num_ch_dec[s], self.num_output_channels)\n        self.decoder = nn.ModuleList(list(self.convs.values()))\n        self.sigmoid = nn.Sigmoid()\n    def forward(self, input_features):\n        self.outputs = {}\n        # decoder\n        x = input_features[-1]\n        for i in range(4, -1, -1):\n            x = self.convs[(\"upconv\", i, 0)](x)\n            x = [upsample(x)]\n            if self.use_skips and i > 0:\n                x += [input_features[i - 1]]\n            x = torch.cat(x, 1)\n            x = self.convs[(\"upconv\", i, 1)](x)\n            if i in self.scales:\n                self.outputs[(\"disp\", i)] = self.sigmoid(self.convs[(\"dispconv\", i)](x))\n        return self.outputs","90edf2d9":"class ResnetEncoder(nn.Module):\n    \"\"\"Pytorch module for a resnet encoder\n    \"\"\"\n    def __init__(self, num_layers, pretrained, num_input_images=1):\n        super(ResnetEncoder, self).__init__()\n        self.num_ch_enc = np.array([64, 64, 128, 256, 512])\n        resnets = {18: models.resnet18,\n                   34: models.resnet34,\n                   50: models.resnet50,\n                   101: models.resnet101,\n                   152: models.resnet152}\n        if num_layers not in resnets:\n            raise ValueError(\"{} is not a valid number of resnet layers\".format(num_layers))\n        if num_input_images > 1:\n            self.encoder = resnet_multiimage_input(num_layers, pretrained, num_input_images)\n        else:\n            self.encoder = resnets[num_layers](pretrained)\n        if num_layers > 34:\n            self.num_ch_enc[1:] *= 4\n    def forward(self, input_image):\n        self.features = []\n        x = (input_image - 0.45) \/ 0.225\n        x = self.encoder.conv1(x)\n        x = self.encoder.bn1(x)\n        self.features.append(self.encoder.relu(x))\n        self.features.append(self.encoder.layer1(self.encoder.maxpool(self.features[-1])))\n        self.features.append(self.encoder.layer2(self.features[-1]))\n        self.features.append(self.encoder.layer3(self.features[-1]))\n        self.features.append(self.encoder.layer4(self.features[-1]))\n        return self.features","ed235a82":"class ConvBlock(nn.Module):\n    \"\"\"Layer to perform a convolution followed by ELU\n    \"\"\"\n    def __init__(self, in_channels, out_channels):\n        super(ConvBlock, self).__init__()\n        self.conv = Conv3x3(in_channels, out_channels)\n        self.nonlin = nn.ELU(inplace=True)\n\n    def forward(self, x):\n        out = self.conv(x)\n        out = self.nonlin(out)\n        return out\n","ac2ae7c0":"class Conv3x3(nn.Module):\n    \"\"\"Layer to pad and convolve input\n    \"\"\"\n    def __init__(self, in_channels, out_channels, use_refl=True):\n        super(Conv3x3, self).__init__()\n        if use_refl:\n            self.pad = nn.ReflectionPad2d(1)\n        else:\n            self.pad = nn.ZeroPad2d(1)\n        self.conv = nn.Conv2d(int(in_channels), int(out_channels), 3)\n\n    def forward(self, x):\n        out = self.pad(x)\n        out = self.conv(out)\n        return out","583af984":"def estimate_depth():\n    device = torch.device(\"cpu\")\n    image_path ='\/kaggle\/input\/safranbolu-sokak\/safranbolu_sokak.jpg'\n    model_path = \"\/kaggle\/input\/mono-odom-640x192\"\n    print(\"-> Loading model from \", model_path)\n    encoder_path = os.path.join(model_path, \"encoder.pth\")\n    depth_decoder_path = os.path.join(model_path, \"depth.pth\")\n    # LOADING PRETRAINED MODEL\n    print(\"   Loading pretrained encoder\")\n    encoder = ResnetEncoder(18, False)\n    loaded_dict_enc = torch.load(encoder_path, map_location=device)\n    # extract the height and width of image that this model was trained with\n    feed_height = loaded_dict_enc['height']\n    feed_width = loaded_dict_enc['width']\n    filtered_dict_enc = {k: v for k, v in loaded_dict_enc.items() if k in encoder.state_dict()}\n    encoder.load_state_dict(filtered_dict_enc)\n    encoder.to(device)\n    encoder.eval()\n    #print(\"   Loading pretrained decoder\")\n    depth_decoder = DepthDecoder(\n        num_ch_enc=encoder.num_ch_enc, scales=range(4))\n    loaded_dict = torch.load(depth_decoder_path, map_location=device)\n    depth_decoder.load_state_dict(loaded_dict)\n    depth_decoder.to(device)\n    depth_decoder.eval()\n    print(\"-> Predicting on {:d} test image\")\n    # PREDICTING ON EACH IMAGE IN TURN\n    with torch.no_grad():\n        input_image = pil.open(image_path).convert('RGB')\n        original_width, original_height = input_image.size\n        input_image = input_image.resize((feed_width, feed_height), pil.LANCZOS)\n        input_image = transforms.ToTensor()(input_image).unsqueeze(0)\n        # PREDICTION\n        input_image = input_image.to(device)\n        features = encoder(input_image)\n        outputs = depth_decoder(features)\n        disp = outputs[(\"disp\", 0)]\n        disp_resized = torch.nn.functional.interpolate(\n            disp, (original_height, original_width), mode=\"bilinear\", align_corners=True)\n        disp_resized_np = disp_resized.squeeze().cpu().numpy()\n        vmax = np.percentile(disp_resized_np, 95)\n        normalizer = mpl.colors.Normalize(vmin=disp_resized_np.min(), vmax=vmax)\n        mapper = cm.ScalarMappable(norm=normalizer, cmap='magma')\n        colormapped_im = (mapper.to_rgba(disp_resized_np)[:, :, :3] * 255).astype(np.uint8)\n        im = pil.fromarray(colormapped_im,mode=None)\n        input_image=mpl.image.imread('\/kaggle\/input\/safranbolu-sokak\/safranbolu_sokak.jpg')\n        fig = plt.figure()\n        #fig.set_figheight(15)\n        fig.set_figwidth(23)\n        a = fig.add_subplot(1, 2, 1)\n        imgplot = plt.imshow(input_image)\n        a.set_title('Original')\n        a = fig.add_subplot(1, 2, 2)\n        imgplot = plt.imshow(im)\n        a.set_title('Depth Estimated')\n    print('-> Done!')","0bab5464":"if __name__ == '__main__':\n    estimate_depth()","d1aba37a":"Depth Estimation from mono image.\nSoruce: https:\/\/github.com\/nianticlabs\/monodepth2\n\nPaper: https:\/\/arxiv.org\/pdf\/1909.09051.pdf\n"}}