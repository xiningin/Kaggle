{"cell_type":{"a533a0b8":"code","9b00f963":"code","1e993424":"code","9f5929d3":"code","1c42b69d":"code","d830bf18":"code","6b7ee1d2":"code","eadfab28":"code","7e8e473c":"code","71b3eb45":"code","27dad377":"markdown","f92ec1e1":"markdown","0130d08c":"markdown","1703b911":"markdown","f1fd122f":"markdown"},"source":{"a533a0b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9b00f963":"import numpy as np\nimport pandas as pd\n\nfrom matplotlib import pyplot as plot\nfrom matplotlib import rcParams\nrcParams['font.family'] = 'serif'\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\n\nproblemsfile = '..\/input\/problem_data.csv'\nuserfile = '..\/input\/user_data.csv'\ntrainfile = '..\/input\/train_submissions.csv'\ntestfile = '..\/input\/test_submissions_NeDLEvX.csv'\n\nproblems_df = pd.read_csv(problemsfile)\nusers_df = pd.read_csv(userfile)\ntrain_df = pd.read_csv( trainfile )\ntest_df = pd.read_csv( testfile )\n\nprint('Training Data Count: ', len(train_df))\nprint('Null values in training set:\\n', train_df.isnull().sum())\n\nprint('Test Data Count: ', len(test_df))\nprint('Null values in test set:\\n', test_df.isnull().sum())\n\nprint('Number of problems: ', len(problems_df))\nprint('Null values in problems:\\n', problems_df.isnull().sum())\n\nprint('Number of users: ', len(users_df))\nprint('Null values in user data:\\n', users_df.isnull().sum())\n\nprint(problems_df.head())\nprint(users_df.head())\nprint(train_df.head())\nprint(test_df.head())\n\n#merge training data with the problems df and then with the users df\ndf = pd.merge(train_df, problems_df, on='problem_id', how='left')\nX = pd.merge(df, users_df, on='user_id', how='left')\n\nprint('Training data: ', len(X))\nusers = X['user_id'].unique()\nprint('Users: ', len(users)) ## returns 3529 unique users\nproblems = X['problem_id'].unique()\nprint('Problems: ', len(problems)) ## returns 5776 unique problems\n\nprint('Test data: ', len(test_df))\nusers = test_df['user_id'].unique()\nprint('Users: ', len(users)) ## returns 3501 unique users\nproblems = test_df['problem_id'].unique()\nprint('Problems: ', len(problems)) ## returns 4716 unique problems\n\n","1e993424":"X.head()","9f5929d3":"#convert the difficulty level to a numeric value\nfactor = pd.factorize(X['level_type'])\nX['diff_level'] = factor[0]\n\n#convert the rank to a numeric value [beginner, intermediate, advanced, expert]\nfactor = pd.factorize(X['rank'])\nX['rank'] = factor[0]\nfactor = pd.factorize(users_df['rank'])\nusers_df['rank'] = factor[0]\n\n#convert user_id and problem_id to numeric by removing the user_ and prob_\nX['user_id'] = X['user_id'].str.replace('user_', '')\nX['problem_id'] = X['problem_id'].str.replace('prob_', '')\n#convert them to ints as they are strings\nX['user_id'] = X['user_id'].astype('int64', copy=False)\nX['problem_id'] = X['problem_id'].astype('int64', copy=False)\nX.describe()\n\n","1c42b69d":"#Build the y vector from the attempts_range\ny = pd.get_dummies(X['attempts_range'])\n\n#saving for later use\ny_xgb = X['attempts_range']\n\n#drop all non-numeric columns and then some more to improve prediction\n#attempts_range is the y vector, so dropping that also\nX = X.drop(['points','tags','level_type','attempts_range','country','last_online_time_seconds','registration_time_seconds','follower_count'], axis=1)\n\nX.head()","d830bf18":"fig = problems_df['level_type'].value_counts().sort_index().plot(kind=\"bar\", stacked=True, title=\"Problem Difficulty Distribution\")\nfig.set_ylabel(\"Number of questions\")\nplot.show()","6b7ee1d2":"ax = sns.pairplot(users_df[[\"submission_count\", \"problem_solved\", \"rating\", \"rank\"]])","eadfab28":"var_Corr = X[[\"submission_count\", \"problem_solved\", \"rating\", \"rank\"]].corr()\nfig2 = plot.figure()\nfig2 = sns.heatmap(var_Corr, xticklabels=var_Corr.columns, yticklabels=var_Corr.columns, annot=True)\nplot.show()","7e8e473c":"X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.25, random_state=1)\n\n# Feature Scaling\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)\n\n# Fitting Random Forest Classification to the Training set\nclassifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 84)\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n\nypred_df = pd.DataFrame(y_pred)\nypred_df.index = y_test.index\nypred_df.columns=['1','2','3','4','5','6']\n\npredicted_attempt = ypred_df.idxmax(axis=1)\ngiven_attempt = y_test.idxmax(axis=1)\n\n# Making the Confusion Matrix\nprint(pd.crosstab(given_attempt, predicted_attempt, rownames=['Actual Attempt Range'], colnames=['Predicted Attempt Range']))\n\naccuracy = accuracy_score(y_test, ypred_df)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n\nrmse = np.sqrt(mean_squared_error(y_test, ypred_df))\nprint(\"RMSE: %f\" % (rmse))\n","71b3eb45":"# attempting xgboost algorithm\n\nX_train, X_test, y_train, y_test = train_test_split( X, y_xgb, test_size=0.25, random_state=1)\n\neval_set = [(X_train,y_train),(X_test,y_test)]\nxgb_class = xgb.XGBClassifier(max_depth='10',n_estimators=100, gamma=0, objective='multi:softmax')\nxgb_class.fit(X_train,y_train,eval_set=eval_set,verbose=1,eval_metric=['mlogloss'])\nprint(xgb_class)\n\npreds = xgb_class.predict(X_test)\n# Making the Confusion Matrix\nprint(pd.crosstab(given_attempt, preds, rownames=['Actual Attempt Range'], colnames=['Predicted Attempt Range']))\n\ny1 = pd.get_dummies(y_test)\ny2 = pd.get_dummies(preds)\n\naccuracy = accuracy_score(y1, y2)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n\nrmse = np.sqrt(mean_squared_error(y1, y2))\nprint(\"RMSE: %f\" % (rmse))","27dad377":"We have achieved an **accuracy of 37% and the Root Mean Square Error is 0.400**. Next we will apply the gradient boosting algorithm.","f92ec1e1":"So what do the above pair plots tell us?\n* Higher number of submissions were made by fewer number of users\n* Higher count of problems were solved by fewer number of users\n* The rating of users is uniformly spread, and most are with a rating somewhere in the middle\n* Majority users are at an intermediate and beginner level, with very few experts\n* Rating is not directly proportional to the number of problems solved or submissions - this means that difficulty level should have played a part","0130d08c":"The heatmap confirms something which is logical and was expected i.e. Rating and Rank are co-related.\n\nNow let us try to model our training data. As usual, we split our training data into train data and validation data (which is called as test data here, takes a while to remember that this is actually part of the training data set and not test data set). Once we are happy with our model, it can be applied against the actual test data set provided.\n\nWe start with a RandomForestClassifier alogirthm.","1703b911":"We see that there are more problems which are in the first half of problem difficulty than the second half.","f1fd122f":"We have achieved an **accuracy of 54% and the Root Mean Square Error is 0.388**.  The xgboost algorithm has not given a substantially improved model. We will continue to fine tune the parameters and increase the efficiency of the model. Stay tuned!"}}