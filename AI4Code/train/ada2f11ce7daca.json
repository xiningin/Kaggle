{"cell_type":{"a7879866":"code","1fe73f86":"code","e815bdaa":"code","2d89089e":"code","4d8a5eee":"code","7dfbe06c":"code","471687b5":"code","f30e2941":"code","2b6834cc":"code","d5c183bf":"code","30bca1eb":"code","33ca09da":"code","83567262":"code","32ca93dc":"code","86c0740d":"code","3c23bc4e":"code","5a6678da":"code","7b9bd473":"code","18742dd9":"code","daab1c89":"code","4c561363":"code","c2fdfeb3":"code","ab79f9ae":"code","cd073db3":"code","5f4c417e":"code","d589a38d":"code","021adfa2":"code","c2abf3ee":"code","06cdfe6e":"code","94079b1c":"code","75845852":"code","1e004acd":"code","a97af979":"code","969e0e9c":"code","267d27f4":"code","a1931cba":"code","2b9e8797":"code","d21056cd":"code","fbe57cdf":"code","3002a1c4":"code","596169dd":"code","887f3bd2":"code","2d3a5ccb":"code","1274cf6f":"code","99ae638f":"code","60b1e8cd":"code","446488ab":"code","7189eebf":"code","1a5322ef":"code","0fb439c1":"code","6925a5ff":"code","d2a3a97d":"code","d0332335":"code","993e91a0":"code","32162a59":"code","2e95d5eb":"code","74f248b3":"code","accba162":"code","f6fd2504":"code","f50b72dc":"code","395337bd":"code","fc480894":"code","8ac690fa":"code","3fa6e997":"code","270cf2ff":"code","8ba5196f":"code","4a657e2c":"code","0d29773b":"code","07a24227":"code","36efddd3":"code","dd9241b0":"code","5f675eb4":"code","675db2a9":"code","1275ac1b":"code","fc5c234e":"code","e4edc2e5":"code","aec938a9":"code","61fadf57":"code","ace878df":"code","d2593c23":"code","104c5e31":"code","19d0ff7a":"code","8a00fe86":"code","d52481f6":"code","43c33c24":"code","7f61e9b5":"code","68217ef6":"code","65d8738e":"code","16f919d4":"code","66bb0747":"code","27849cad":"code","25632c2e":"code","b76a232c":"code","c444a38b":"code","74913bd7":"code","b864dedc":"code","8672c323":"code","cae9c2e6":"code","5966f945":"code","f5ab78f8":"code","574d135c":"code","adfce76b":"code","44433a9e":"code","4a7d5f6b":"code","0078b956":"code","08699e2f":"code","f95e9d73":"code","0c33c5dc":"code","79169c86":"code","53923010":"code","e2791850":"code","3fa053a3":"code","ecc5a6e4":"code","1ddb4c49":"code","75e156ec":"code","30da8831":"code","42ab888c":"code","35902685":"code","1ad28bab":"code","5c80817e":"code","2cd9e806":"code","864584d9":"code","78b737b2":"code","95481266":"markdown","7f3cb0bc":"markdown","82954922":"markdown","553205a7":"markdown","2feb4ce6":"markdown","a94b61da":"markdown","f817d96f":"markdown","0b49d69a":"markdown","16a76efa":"markdown","6ce06069":"markdown","6e98567e":"markdown","691211a9":"markdown","073af8ed":"markdown","deeed38e":"markdown","b26fc72f":"markdown","7800e774":"markdown","37dd5ea9":"markdown","9b389f18":"markdown","99cddf71":"markdown","af688200":"markdown","1d0b025b":"markdown","de5ac979":"markdown","2a81ff97":"markdown","0abbca81":"markdown","a137feef":"markdown","6de5e36a":"markdown","223c22bd":"markdown","57358f44":"markdown","0a14c9ef":"markdown"},"source":{"a7879866":"import numpy as np \nimport pandas as pd\n\nimport glob\nimport json","1fe73f86":"root_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\nmetadata_path = f'{root_path}\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df.head(2)","e815bdaa":"len(meta_df) - meta_df.has_pdf_parse.sum()","2d89089e":"meta_df.isnull().sum()","4d8a5eee":"meta_df.cord_uid.nunique()","7dfbe06c":"meta_df.sha.nunique()","471687b5":"meta_df.title.nunique()","f30e2941":"all_json = glob.glob(f'{root_path}\/**\/pdf_json\/*.json', recursive=True)\nlen(all_json)","2b6834cc":"all_json_pmc = glob.glob(f'{root_path}\/**\/pmc_json\/*.json', recursive=True)\nlen(all_json_pmc)","d5c183bf":"methods = ['methods','method','statistical methods','materials','materials and methods',\n                'data collection','the study','study design','experimental design','objective',\n                'objectives','procedures','data collection and analysis', 'methodology',\n                'material and methods','the model','experimental procedures','main text']","30bca1eb":"# [''.join(x.lower() for x in m if x.isalpha()) for m in methods]\n\n# for m in methods:\n#     print(''.join(x.lower() for x in m if x.isalpha()))","33ca09da":"class FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            self.methods = []\n            self.results = []\n\n            # Abstract\n            for entry in content['abstract']:\n                self.abstract.append(entry['text'])\n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            # Methods\n            methods = ['methods','method','statistical methods','materials','materials and methods',\n                'data collection','the study','study design','experimental design','objective',\n                'objectives','procedures','data collection and analysis', 'methodology',\n                'material and methods','the model','experimental procedures','main text']\n            for entry in content['body_text']:\n                section_title = ''.join(x.lower() for x in entry['section'] if x.isalpha()) #remove numbers and spaces\n                if any(m in section_title for m in [''.join(x.lower() for x in m if x.isalpha()) for m in methods]) : \n                    self.methods.append(entry['text'])\n            # Results\n            results_synonyms = ['result']\n            for entry in content['body_text']:\n                section_title = ''.join(x.lower() for x in entry['section'] if x.isalpha())\n                if any(r in section_title for r in results_synonyms) :\n                    self.results.append(entry['text'])\n                    \n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n            self.methods = '\\n'.join(self.methods)\n            self.results = '\\n'.join(self.results)\n\n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\nfirst_row = FileReader(all_json[0])\nprint(first_row)","83567262":"dict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'methods': [], 'results': []}\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) \/\/ 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    content = FileReader(entry)\n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\n    dict_['methods'].append(content.methods)\n    dict_['results'].append(content.results)","32ca93dc":"papers = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'methods', 'results'])\npapers.head()","86c0740d":"papers[(papers.results.str.len() != 0) | (papers.methods.str.len() != 0)].shape","3c23bc4e":"df = pd.merge(papers, meta_df, left_on='paper_id', right_on='sha', how='left').drop('sha', axis=1)","5a6678da":"df.columns","7b9bd473":"class FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.body_text = []\n            self.methods = []\n            self.results = []\n\n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            # Methods\n            methods = ['methods','method','statistical methods','materials','materials and methods',\n                'data collection','the study','study design','experimental design','objective',\n                'objectives','procedures','data collection and analysis', 'methodology',\n                'material and methods','the model','experimental procedures','main text']\n            for entry in content['body_text']:\n                section_title = ''.join(x.lower() for x in entry['section'] if x.isalpha()) #remove numbers and spaces\n                if any(m in section_title for m in [''.join(x.lower() for x in m if x.isalpha()) for m in methods]) : \n                    self.methods.append(entry['text'])\n            # Results\n            results_synonyms = ['result']\n            for entry in content['body_text']:\n                section_title = ''.join(x.lower() for x in entry['section'] if x.isalpha())\n                if any(r in section_title for r in results_synonyms) :\n                    self.results.append(entry['text'])\n                    \n            self.body_text = '\\n'.join(self.body_text)\n            self.methods = '\\n'.join(self.methods)\n            self.results = '\\n'.join(self.results)\n\n    def __repr__(self):\n        return f'{self.paper_id}: {self.body_text[:200]}...'\nfirst_row = FileReader(all_json_pmc[0])\nprint(first_row)","18742dd9":"dict_ = {'paper_id': [], 'body_text': [], 'methods': [], 'results': []}\nfor idx, entry in enumerate(all_json_pmc):\n    if idx % (len(all_json_pmc) \/\/ 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json_pmc)}')\n    content = FileReader(entry)\n    dict_['paper_id'].append(content.paper_id)\n    dict_['body_text'].append(content.body_text)\n    dict_['methods'].append(content.methods)\n    dict_['results'].append(content.results)","daab1c89":"pmc_text = pd.DataFrame(dict_, columns=['paper_id', 'body_text', 'methods', 'results'])\npmc_text.head()","4c561363":"pmc_text.shape","c2fdfeb3":"pmc_text[pmc_text.body_text == '']","ab79f9ae":"pmc_text = pmc_text[pmc_text.body_text != '']","cd073db3":"pmc_text.shape","5f4c417e":"df.head()","d589a38d":"df = pd.merge(df, pmc_text, left_on='pmcid', right_on='paper_id', how='left').drop('paper_id_y', axis=1)","021adfa2":"df.head(3)","c2abf3ee":"df.columns","06cdfe6e":"df.drop(columns=['has_pdf_parse', 'has_pmc_xml_parse', 'Microsoft Academic Paper ID', 'WHO #Covidence'], inplace=True)","94079b1c":"df.head(2)","75845852":"df[df.abstract_x != df.abstract_y].shape","1e004acd":"df[df.abstract_x != df.abstract_y][['abstract_x', 'abstract_y', 'url']].tail(10)","a97af979":"df[df.abstract_x != df.abstract_y][['abstract_x', 'abstract_y', 'url']].url.iloc[-1]","969e0e9c":"df[df.abstract_x != df.abstract_y][['abstract_x', 'abstract_y', 'url', 'body_text_x', 'body_text_y']][\n    (df.abstract_y.isnull()) & (df.abstract_x != '') & (~df.url.isnull())]","267d27f4":"df.shape","a1931cba":"df.abstract_x.isnull().sum(), (df.abstract_x =='').sum() # missing abstracts in json files","2b9e8797":"df.abstract_y.isnull().sum(), (df.abstract_y=='').sum() # missing abstracts in metadata","d21056cd":"df.loc[df.abstract_y.isnull() & (df.abstract_x != ''), 'abstract_y'] = df[(df.abstract_y.isnull()) & (df.abstract_x != '')].abstract_x","fbe57cdf":"df.abstract_y.isnull().sum()","3002a1c4":"(df.abstract_y.isnull() & (df.abstract_x!='')).sum()","596169dd":"df.rename(columns = {'abstract_y': 'abstract'}, inplace=True)\ndf.drop('abstract_x', axis=1, inplace=True)","887f3bd2":"df.columns","2d3a5ccb":"df.shape","1274cf6f":"df.shape","99ae638f":"(df.body_text_x != df.body_text_y).sum()","60b1e8cd":"df[(df.body_text_x != df.body_text_y) & df.body_text_y.notnull()][['body_text_x', 'body_text_y']].head(10).iloc[2].values[0][:500]","446488ab":"df[(df.body_text_x != df.body_text_y) & df.body_text_y.notnull()][['body_text_x', 'body_text_y']].head(10).iloc[2].values[1][:500]","7189eebf":"df[df.body_text_x != df.body_text_y].head()","1a5322ef":"df.iloc[34885].body_text_x[:500]","0fb439c1":"df.iloc[34885].body_text_y[:500]","6925a5ff":"df.iloc[34885].url","d2a3a97d":"df.iloc[34888].body_text_x[:500]","d0332335":"df.iloc[34888].body_text_y[:500]","993e91a0":"df.iloc[34888].url","32162a59":"df.iloc[1337].body_text_x[:500]","2e95d5eb":"df.iloc[1337].body_text_y[:500]","74f248b3":"df.iloc[1337].url","accba162":"df.iloc[1242].body_text_x[:500]","f6fd2504":"df.iloc[1242].body_text_y[:500]","f50b72dc":"df.iloc[1242].url","395337bd":"df.body_text_x.isnull().sum(), df.body_text_y.isnull().sum()","fc480894":"(df.body_text_x == '').sum(), (df.body_text_y == '').sum()","8ac690fa":"df.loc[df.body_text_y.notnull(), 'body_text_x'] = df.loc[df.body_text_y.notnull(), 'body_text_y']","3fa6e997":"df.body_text_x.isnull().sum()","270cf2ff":"df.rename(columns = {'body_text_x': 'body_text'}, inplace=True)\ndf.drop('body_text_y', axis=1, inplace=True)","8ba5196f":"df.columns","4a657e2c":"df[['methods_x', 'methods_y', 'url']][df.methods_y.notnull()]","0d29773b":"(df.methods_x == '').sum(), df.methods_x.isnull().sum()","07a24227":"(df.methods_y == '').sum(), df.methods_y.isnull().sum()","36efddd3":"# use methods_y (from pmc) when it's available\nmask = (df.methods_y.notnull()) & (df.methods_y != '')\ndf.loc[mask, 'methods_x'] = df.loc[mask, 'methods_y']\n\n# same for results\nmask = (df.results_y.notnull()) & (df.results_y != '')\ndf.loc[mask, 'results_x'] = df.loc[mask, 'results_y']","dd9241b0":"(df.results_x == '').sum(), df.results_x.isnull().sum()","5f675eb4":"(df.results_y == '').sum(), df.results_y.isnull().sum()","675db2a9":"df.rename(columns = {'methods_x': 'methods', 'results_x': 'results'}, inplace=True)\ndf.drop(columns=['methods_y', 'results_y'], inplace=True)","1275ac1b":"df.rename(columns = {'paper_id_x': 'paper_id', 'source_x': 'source'}, inplace=True)","fc5c234e":"df.columns","e4edc2e5":"df.head()","aec938a9":"len(df)","61fadf57":"df.paper_id.nunique()","ace878df":"df[df.duplicated(subset=['paper_id'], keep=False)][['paper_id', 'body_text']]","d2593c23":"df[df.duplicated(subset=['paper_id', 'body_text'], keep=False)].shape","104c5e31":"df.drop_duplicates(['paper_id', 'body_text'], inplace=True)","19d0ff7a":"len(df)","8a00fe86":"df[df.duplicated(['paper_id'], keep=False)].head(2)","d52481f6":"df.drop_duplicates(['paper_id'], inplace=True)","43c33c24":"df.paper_id.nunique()","7f61e9b5":"df.shape","68217ef6":"df.isnull().sum()","65d8738e":"# some new columns for convenience\ndf['publish_year'] = df.publish_time.str[:4].fillna(-1).astype(int) # 360 times None\n# df['link'] = 'http:\/\/dx.doi.org\/' + df.doi #dataset now has url column","16f919d4":"df['is_covid19'] = df.body_text.str.contains('COVID-19|covid|sar cov 2|SARS-CoV-2|2019-nCov|2019 ncov|SARS Coronavirus 2|2019 Novel Coronavirus|coronavirus 2019| Wuhan coronavirus|wuhan pneumonia|wuhan virus', case=False)","66bb0747":"df.is_covid19.sum()","27849cad":"from IPython.utils import io\n\nwith io.capture_output() as captured:\n    !pip install scispacy\n    !pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz\n    !pip install spacy-langdetect\n    !pip install spac scispacy spacy_langdetect https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.3\/en_core_sci_lg-0.2.3.tar.gz","25632c2e":"import scispacy\nimport spacy\nimport en_core_sci_lg\nfrom spacy_langdetect import LanguageDetector","b76a232c":"# medium model\nnlp = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\nnlp.max_length = 2000000\nnlp.add_pipe(LanguageDetector(), name='language_detector', last=True)","c444a38b":"doc = nlp('This is some English text. Das ist ein Haus. This is a house.')\ndoc._.language","74913bd7":"for s in doc.sents:\n    print(s._.language)","b864dedc":"doc = nlp(df[df.paper_id == '1a8a4dbbaa94ced4ef6af69ec7a09d3fa4c0eece'].body_text.iloc[0])","8672c323":"doc[:500]","cae9c2e6":"doc_engl = ''\nfor s in doc.sents:\n    if (s._.language['language'] == 'en'):\n        doc_engl += s.text ","5966f945":"doc_engl[:2000]","f5ab78f8":"df['text_language'] = df.body_text.apply(lambda x: nlp(str(x[:2000]))._.language['language'])\n\ndf.text_language.value_counts()","574d135c":"df.loc[df[df.text_language != 'en'].index].shape","adfce76b":"df = df.drop(df[df.text_language != 'en'].index)","44433a9e":"# Check language of all abstracts\n\n# df['abstract_lang'] = df.abstract.apply(lambda x: nlp(str(x))._.language['language'])\n\n#  df[df.abstract.isnull()]","4a7d5f6b":"# Number of non-english abstracts\n\n# df[(df.abstract_lang != 'en') & (df.abstract.notnull())].abstract_lang.value_counts()\n\n# Keep all english abstracts and those without abstract\n\n# df = df[(df.abstract_lang == 'en') | (df.abstract.isnull())]\n\n# df.shape\n\n# df.paper_id.nunique()\n\n# Analyze title\/text body of the papers without abstract\n\n# temp = df[df.abstract.isnull()].copy()\n\n# def remove_non_english_sentences(doc):\n#     doc = nlp(doc)\n#     doc_engl = ''\n#     for s in doc.sents:\n#         if (s._.language['language'] == 'en'):\n#             doc_engl += s.text \n#     return doc_engl\n\n# remove_non_english_sentences(df[df.paper_id == '1a8a4dbbaa94ced4ef6af69ec7a09d3fa4c0eece'].body_text.iloc[0])\n\n# temp['text_length'] = temp.body_text.apply(lambda x: len(x))\n\n# temp['english_text'] = temp.body_text.apply(remove_non_english_sentences)\n\n# temp['english_length'] = temp.english_text.apply(lambda x: len(x))\n\n# temp.to_csv('df_english.csv', index=False)\n\n# (temp.english_length\/temp.text_length).hist()\n\n# ((temp.english_length\/temp.text_length)<0.8).sum()\n\n# temp[((temp.english_length\/temp.text_length)<0.8)].head()\n\n# temp[temp.paper_id == '7925057cfe0cb75ae6079879cb2d22d23e42dfa5'].body_text.values[0][:500]\n\n# temp[temp.paper_id == '617197cc751a9208cb0af1b4e31baeddc8d2e985'].body_text.values[0]\n\n# temp[temp.paper_id == 'ca51b53fa512085e1aa166d5308602ff1666a90c'].body_text.values[0][:500]\n\n# df = df.drop(temp[((temp.english_length\/temp.text_length)<0.8)].index)","0078b956":"# temp['title_lang'] = df.title.apply(lambda x: nlp(str(x))._.language['language'])\n\n# temp.title_lang.value_counts()\n\n# Too many false-positves. \n\n# temp[temp.paper_id == '6f6b7b1efffae7f3765f29fe801ab63dd35110bb'].body_text.values[0]\n\n# temp[temp.title_lang == 'de']\n\n# We check the beginning of each text body instead.\n\n# temp['text_lang'] = df.body_text.apply(lambda x: nlp(str(x[:2000]))._.language['language'])\n\n# temp.text_lang.value_counts()\n\n# Number of non-english texts to drop.\n\n# df.loc[temp[temp.text_lang != 'en'].index].shape\n\n# df = df.drop(temp[temp.text_lang != 'en'].index)","08699e2f":"# filter_dict = {\n#     \"discussion\": [\"conclusions\",\"conclusion\",'| discussion', \"discussion\",  'concluding remarks',\n#                    'discussion and conclusions','conclusion:', 'discussion and conclusion',\n#                    'conclusions:', 'outcomes', 'conclusions and perspectives', \n#                    'conclusions and future perspectives', 'conclusions and future directions'],\n#     \"results\": ['executive summary', 'result', 'summary','results','results and discussion','results:',\n#                 'comment',\"findings\"],\n#     \"introduction\": ['introduction', 'background', 'i. introduction','supporting information','| introduction'],\n#     \"methods\": ['methods','method','statistical methods','materials','materials and methods',\n#                 'data collection','the study','study design','experimental design','objective',\n#                 'objectives','procedures','data collection and analysis', 'methodology',\n#                 'material and methods','the model','experimental procedures','main text',],\n#     \"statistics\": ['data analysis','statistical analysis', 'analysis','statistical analyses', \n#                    'statistics','data','measures'],\n#     \"clinical\": ['diagnosis', 'diagnostic features', \"differential diagnoses\", 'classical signs','prognosis', 'clinical signs', 'pathogenesis',\n#                  'etiology','differential diagnosis','clinical features', 'case report', 'clinical findings',\n#                  'clinical presentation'],\n#     'treatment': ['treatment', 'interventions'],\n#     \"prevention\": ['epidemiology','risk factors'],\n#     \"subjects\": ['demographics','samples','subjects', 'study population','control','patients', \n#                'participants','patient characteristics'],\n#     \"animals\": ['animals','animal models'],\n#     \"abstract\": [\"abstract\", 'a b s t r a c t','author summary'], \n#     \"review\": ['review','literature review','keywords']}","f95e9d73":"study_designs = {'RCT': ['RCT', 'randomized controlled trial', 'randomised controlled trial', 'randomized control trial', 'randomised control trial',\n                         'randomized clinical trial','randomised clinical trial'], \n                'time series analysis': ['time series analysis', 'time series', 'survival analysis'],\n                'retrospective cohort': ['retrospective cohort'],\n                'cross-sectional case-control': ['cross-sectional case-control', 'cross sectional case control', 'cross-sectional case control'],\n                'prospective case-control': ['prospective case-control', 'prospective case control'],\n                'matched case-control': ['matched case-control', 'matched case control'],\n                'medical records review': ['medical records review'],\n                'prevalence survey': ['prevalence survey'],\n                'syndromic surveillance': ['syndromic surveillance'],\n                'systematic review': ['systematic review'],\n                'meta-analysis': ['meta-analysis', 'meta analysis', 'meta-syntheses'],\n                'interventional study': ['interventional study'],\n                'association': ['association', 'associated with'],\n                 'p-value': ['p-value', 'p value'],\n                 'pseudo-randomized controlled trial': ['pseudo-randomized controlled trial', 'pseudo-randomised controlled trial']\n                }","0c33c5dc":"generic_keywords = ['estimation',\n 'prevalence survey',\n 'response rate',\n 'incidence',\n 'psychometric evaluation of instrument',\n 'median time to event',\n 'pooled OR',\n 'd-pooled',\n 'randomized controlled trial',\n 'non-randomized',\n 'allocation method',\n 'Cochrane review',\n 'Cox proportional hazards',\n 'gamma',\n 'Weibull',\n 'pseudo-randomised',\n 'chart review',\n 'log odds',\n 'surveillance',\n 'time-to-event analysis',\n 'pooled adjusted odds ratio',\n 'pooled relative risk',\n 'data abstraction forms',\n 'frequency',\n 'etiology logistic regression',\n 'exclusion criteria',\n 'eligibility criteria',\n 'right-censored',\n 'pooled odds ratio',\n 'non-comparative study',\n 'medical records review',\n 'CONSORT',\n 'number of controls per case',\n 'quasi-randomised',\n 'risk of bias',\n 'publication bias',\n 'syndromic surveillance',\n 'truncated',\n 'longitudinal',\n 'matching criteria',\n 'double-blind',\n \"Cohen's d\",\n 'registry data',\n 'Adjusted Odds Ratio',\n 'questionnaire development',\n 'Kaplan-Meier',\n 'heterogeneity',\n 'recruitment',\n 'randomization method',\n 'censoring',\n 'meta-analysis',\n 'non-randomised',\n '\u03b2',\n 'electronic medical records',\n 'eligibility',\n 'cross-sectional survey',\n 'PRISMA',\n 'prevalence',\n 'inclusion criteria',\n 'control arm',\n 'protocol',\n 'pooled risk ratio',\n 'non-response bias',\n 'baseline',\n 'retrospective chart review',\n 'survival analysis',\n 'logistic regression',\n 'blind',\n 'exposure status',\n 'randomized',\n 'associated with',\n 'lognormal',\n 'systematic review',\n 'RCT',\n 'randomised',\n 'survey instrument',\n 'interrater reliability',\n 'randomisation',\n 'pooled RR',\n 'hazard ratio',\n 'AOR',\n 'potential confounders',\n 'treatment effect',\n 'randomized clinical trial',\n 'data collection instrument',\n 'pooled AOR',\n 'association',\n 'power',\n \"cohen's kappa\",\n 'pseudo-randomized',\n 'treatment arm',\n 'search string',\n 'quasi-randomized',\n 'cohort',\n 'risk factors',\n 'difference between means',\n 'registry',\n 'inter-rater reliability',\n 'Odds Ratio',\n 'placebo',\n 'databases searched',\n 'risk factor analysis',\n 'difference in means',\n 'random sample',\n 'etiology',\n 'i2']","79169c86":"for a in generic_keywords:\n    if a not in [x for v in study_designs.values() for x in v]:\n        study_designs[a] = [a]","53923010":"len([x for v in study_designs.values() for x in v])","e2791850":"# def tag_study_design(study_designs):\n#     df['study_design'] = [set() for _ in range(len(df))]\n#     for tag in study_designs.keys():\n#         for synonym in study_designs[tag]:\n#             df[df.abstract.str.contains(synonym, case=False, na=False)].study_design.apply(lambda x: x.add(tag))","3fa053a3":"def tag_study_design(study_designs):\n    df['study_abstract'] = [set() for _ in range(len(df))]\n    df['study_methods'] = [set() for _ in range(len(df))]\n    df['study_results'] = [set() for _ in range(len(df))]\n\n    for tag in study_designs.keys():\n        for synonym in study_designs[tag]:\n            df[df.abstract.str.contains(synonym, case=False, na=False) | df.title.str.contains(synonym, case=False, na=False)].study_abstract.apply(lambda x: x.add(tag))\n            df[df.methods.str.contains(synonym, case=False, na=False)].study_methods.apply(lambda x: x.add(tag))\n            df[df.results.str.contains(synonym, case=False, na=False)].study_results.apply(lambda x: x.add(tag))\n    \n    df['study_design'] = df.apply(lambda x: list(x.study_abstract.union(x.study_methods).union(x.study_results)), axis=1)\n    df.study_abstract = df.study_abstract.apply(lambda x: list(x))\n    df.study_methods = df.study_abstract.apply(lambda x: list(x))\n    df.study_results = df.study_results.apply(lambda x: list(x))","ecc5a6e4":"tag_study_design(study_designs)","1ddb4c49":"df[df.study_design.str.len() != 0].tail(20).study_design","75e156ec":"len(df.study_abstract[df.study_abstract.str.len() != 0])","30da8831":"len(df.study_methods[df.study_methods.str.len() != 0])","42ab888c":"len(df.study_results[df.study_results.str.len() != 0])","35902685":"len(df.study_design[df.study_design.str.len() != 0])","1ad28bab":"len(df.study_design[(df.study_design.str.len() != 0) & df.is_covid19])","5c80817e":"df.drop(columns=['cord_uid', 'pmcid', 'pubmed_id', 'full_text_file', 'license', 'text_language',\n                 'study_abstract', 'study_methods', 'study_results'], inplace=True)","2cd9e806":"df.head()","864584d9":"df.shape","78b737b2":"df.to_csv('cord19_df.csv', index=False)","95481266":"body_text_x is from pdf, body_text_y from pmc","7f3cb0bc":"Careful, some of the new texts are empty strings!","82954922":"Since the abstracts from the metadata seem more reliable we generally use these, but fill the missing values with the abstract from the extracted values from the JSON file.","553205a7":"## Number of non-english texts to drop.","2feb4ce6":"Now the paper_id is unique.","a94b61da":"# Export as .csv","f817d96f":"# pmc_json","0b49d69a":"But luckily they also have the same text body. So we will just keep one article per paper_id.\nCheck for example [https:\/\/www.sciencedirect.com\/science\/article\/pii\/S1386653209701295?via%3Dihub](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S1386653209701295?via%3Dihub) and [https:\/\/www.sciencedirect.com\/science\/article\/pii\/S1386653209701325?via%3Dihub](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S1386653209701325?via%3Dihub) - they have the same content.","16a76efa":"We still have to compare the text body from pdf and pmc files.","6ce06069":"Check language of each text body (only use the first 2000 characters).","6e98567e":"Where available we use the text from the pmc file (body_text_y), trusting the statement that it is of higher quality.","691211a9":"Checking some of the files online, it seems that where the abstract is missing in the metadata, the abstract in the JSON file is simply the beginning of the text.","073af8ed":"abstract_x from json, abstract_y from metadata","deeed38e":"Some exploration and preprocessing to create one dataframe and export it as .csv.\n\nAdjusted to data from 2020-05-01.","b26fc72f":"# Load and Prepare Data","7800e774":"# Extract Study Design\/ Methodological Keywords","37dd5ea9":"# Duplicates","9b389f18":"# Load Packages","99cddf71":"Keywords from  [https:\/\/docs.google.com\/spreadsheets\/d\/1t2e3CHGxHJBiFgHeW0dfwtvCG4x0CDCzcTFX7yz9Z2E\/edit#gid=1217643351](https:\/\/docs.google.com\/spreadsheets\/d\/1t2e3CHGxHJBiFgHeW0dfwtvCG4x0CDCzcTFX7yz9Z2E\/edit#gid=1217643351)","af688200":"# Quick comparison of both texts","1d0b025b":"the remaining missing values are also empty in the json files","de5ac979":"# Exploration\/Cleaning","2a81ff97":"# Some new columns for convenience","0abbca81":"Some paper ids are duplicated","a137feef":"### Different Abstract in Metadata and JSON files","6de5e36a":"# Language Detection to remove non-english articles and abstracts","223c22bd":"To read the JSON files we follow [COVID EDA: Initial Exploration Tool](https:\/\/www.kaggle.com\/ivanegapratama\/covid-eda-initial-exploration-tool).","57358f44":"This only contains the full text - no abstracts!","0a14c9ef":"# pdf_json"}}