{"cell_type":{"c8e1fdbf":"code","53051a16":"code","869ce01c":"code","193db155":"code","1ae3dc8e":"code","75e5c6b6":"code","74d78d92":"code","7853ad32":"code","ae8abf07":"code","05ddd199":"code","e34e7641":"code","3e203377":"code","19adb0a4":"code","b78d146a":"code","33c91c10":"code","29757627":"code","3c34a994":"code","95eed0c2":"markdown","ac786e2e":"markdown","676d78b5":"markdown","7e0da76f":"markdown","f73192a5":"markdown","22204866":"markdown"},"source":{"c8e1fdbf":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Input\nfrom keras.layers import MaxPooling2D, ZeroPadding2D, Conv2D, Flatten\nfrom keras.optimizers import Adam, SGD\nfrom keras.preprocessing.image import img_to_array, load_img, ImageDataGenerator\nfrom keras.utils import to_categorical","53051a16":"# read the CSV that contains the class labels\ndf1=pd.read_csv('..\/input\/dog-breed-identification\/labels.csv')\ndf1.head()","869ce01c":"#location of files \nimg_file='..\/input\/dog-breed-identification\/train\/'\n\n# adding column in the df for the image location\ndf = df1.assign(img_path=lambda x: img_file + x['id']+'.jpg')\ndf.head()","193db155":"print(f\"Total {df.shape[0]} images available for training\")","1ae3dc8e":"# class distribution \ndf.breed.value_counts()","75e5c6b6":"from tqdm import tqdm_notebook as tqdm\n\n''' loading the data directly to the RAM\n    \n    - create a numpy array.\n    - fill it with the images.\n \n    \n'''\n\nparts = 2\nimage_shape = (324, 324, 3)\n\n\nimages = df.img_path.values.tolist()\ntotal_images = len(images)\n\nimg_pixels = np.zeros(shape=(total_images\/\/parts,image_shape[0],image_shape[1],image_shape[2]), dtype=np.uint8)\n\nfor i, img in tqdm(enumerate(images[:len(images)\/\/parts])):\n    pixels = load_img(img, target_size=image_shape)\n    img_pixels[i,:,:,:] = pixels\n\nprint(img_pixels.shape)","74d78d92":"# label encoding\n\nlabels = df.breed[:len(df.breed)\/\/parts]\nimg_label = pd.get_dummies(labels)\nimg_label","7853ad32":"# lets get the dimensions \nX=img_pixels\ny=img_label.values\nprint(X.shape)\nprint(y.shape)","ae8abf07":"# train_test_split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","05ddd199":"# Data Augmentation\ntrain_datagen = ImageDataGenerator(\n#     rotation_range=30,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    rescale=1.\/255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest')\n\ntest_datagen=ImageDataGenerator(rescale=1.\/255)","e34e7641":"batch_size = 64\ntraining_set=train_datagen.flow(X_train,y=y_train,batch_size=batch_size)\ntesting_set=test_datagen.flow(X_test,y=y_test,batch_size=batch_size)","3e203377":"# image preview\ndef show_image(generator):\n    pass\n\nshow_image(training_set)","19adb0a4":"# CNN model definition \nfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers import Dropout, Activation, Dense\nfrom keras.models import Sequential\nfrom keras.layers.normalization import BatchNormalization\n\n\n\nmodel = Sequential()\n\nmodel.add(Conv2D(16, (3, 3), padding='same', use_bias=False, input_shape=image_shape))\nmodel.add(BatchNormalization(axis=3, scale=False))\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), padding='same'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Conv2D(32, (3, 3), padding='same', use_bias=False))\nmodel.add(BatchNormalization(axis=3, scale=False))\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), padding='same'))\nmodel.add(Dropout(0.4))\n\nmodel.add(Conv2D(64, (3, 3), padding='same', use_bias=False))\nmodel.add(BatchNormalization(axis=3, scale=False))\nmodel.add(Activation(\"relu\"))\nmodel.add(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), padding='same'))\nmodel.add(Dropout(0.6))\n\nmodel.add(Conv2D(128, (3, 3), padding='same', use_bias=False))\nmodel.add(BatchNormalization(axis=3, scale=False))\nmodel.add(Activation(\"relu\"))\nmodel.add(Flatten())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(1024, activation='relu'))\nmodel.add(Dense(120, activation='softmax'))\nmodel.summary()","b78d146a":"# compile the model with loss\nfrom keras.losses import categorical_crossentropy\nmodel.compile(loss=categorical_crossentropy,optimizer='adam',metrics=['accuracy'])","33c91c10":"# scheduling learning rate\nimport tensorflow as tf\n\ndef scheduler(epoch, lr):\n    if epoch < 5:\n        return lr\n    else:\n        return lr * tf.math.exp(-0.1)\n\nfrom keras import callbacks \ncallback = callbacks.LearningRateScheduler(scheduler)","29757627":"# Lets train it out \nhistory=model.fit_generator(training_set,\n                            steps_per_epoch = 64,\n                            validation_data = testing_set,\n                            validation_steps = 32,\n                            callbacks=[callback],\n                            epochs = 50,\n                            verbose = 1)","3c34a994":"plt.figure(211)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\n\nplt.figure(212)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\n\nplt.show()","95eed0c2":"# Breed Classification ","ac786e2e":"\nThere looks like a case of class imbalance. We can add synthetic oversampling (for the minority class) and undersampling (for the majority class) in the of data augmentaiton. \n","676d78b5":"# EDA\n","7e0da76f":"# Target\n- A model that can classify 120 classes of the different breeds of dogs.\n\n# Approach \n- A CNN with 120 classes ","f73192a5":"# Model definition [Keras]","22204866":"# Loading dataset\n\nAssuming that we have cleaned the data and the data is ready for modeling, there are multiple ways of loading the dataset. The size of the available RAM dictates our choices here. \n\n\nThe data overfits the RAM in our case, so to keep things a bit simple, we\u2019ll use a third of the data available.\n"}}