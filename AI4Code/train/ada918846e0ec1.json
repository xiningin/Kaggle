{"cell_type":{"2c3b003d":"code","4d7ec5ce":"code","64fc3b75":"code","c1e4ad5b":"code","296d2d5c":"code","aede5de0":"code","762f2e9a":"code","5bf297a6":"code","8db13cab":"code","bebb0a9b":"code","90875220":"code","9c1e48f1":"markdown","ed03194d":"markdown","755c9e3c":"markdown","61e59576":"markdown","93708e37":"markdown","e55b81cc":"markdown"},"source":{"2c3b003d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nfrom sklearn.model_selection import train_test_split \n\n# Import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import AdaBoostClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingRegressor\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\nimport scikitplot as skplt\nimport matplotlib.pyplot as plt\nfrom sklearn import tree\n\n\nimport graphviz\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4d7ec5ce":"df = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf.drop(\"Unnamed: 32\",axis=1,inplace=True) # in this process this \ndf.head()","64fc3b75":"features_mean= (df.columns[1:11])\nfeatures_se= (df.columns[11:20])\nfeatures_worst=(df.columns[21:31])\ndf['diagnosis']=df['diagnosis'].map({'M':1,'B':0})\ny = df['diagnosis']\ndf.drop(\"diagnosis\",axis=1,inplace=True)","c1e4ad5b":"df.describe()","296d2d5c":"y.value_counts().plot(kind = 'bar', rot = 0)\nplt.title(\"Count of Cases\")\nplt.xlabel('Diagnosis')","aede5de0":"X_train,X_test,y_train,y_test = train_test_split(df,y, test_size = 0.3)\n\n# Instantiate dt\ndt = DecisionTreeClassifier(max_depth=2, random_state=1)\n\n# Instantiate ada\nada = AdaBoostClassifier(base_estimator=dt, n_estimators=180, random_state=1)\n\n# Fit ada to the training set\nada.fit(X_train,y_train)\n\n# Compute the probabilities of obtaining the positive class\ny_pred_proba = ada.predict_proba(X_test)[:,1]\n\n# Import roc_auc_score\nfrom sklearn.metrics import roc_auc_score\n\n# Evaluate test-set roc_auc_score\nada_roc_auc = roc_auc_score(y_test, y_pred_proba)\n\n# Print roc_auc_score\nprint('ROC AUC score: {:.2f}'.format(ada_roc_auc))\n\nfpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\nauc = roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","762f2e9a":"dt.fit(X_train,y_train)\nfig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(dt,feature_names = X_train.columns,class_names='MB')","5bf297a6":"features_mean = df.iloc[:,1:11]\n\nX_train,X_test,y_train,y_test = train_test_split(features_mean,y, test_size = 0.3)\n\n# Instantiate dt\ndt = DecisionTreeClassifier(max_depth=2, random_state=1)\n\n# Instantiate ada\nada = AdaBoostClassifier(base_estimator=dt, n_estimators=180, random_state=1)\n\n# Fit ada to the training set\nada.fit(X_train,y_train)\n\n# Compute the probabilities of obtaining the positive class\ny_pred_proba = ada.predict_proba(X_test)[:,1]\n\n# Evaluate test-set roc_auc_score\nada_roc_auc = roc_auc_score(y_test, y_pred_proba)\n\n# Print roc_auc_score\nprint('ROC AUC score: {:.2f}'.format(ada_roc_auc))\n\nfpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\nauc = roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","8db13cab":"dt.fit(X_train,y_train)\nfig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(dt,feature_names = X_train.columns,class_names='BM')","bebb0a9b":"features_mean = df.iloc[:,11:20]\n\nX_train,X_test,y_train,y_test = train_test_split(features_mean,y, test_size = 0.3)\n\n# Instantiate dt\ndt = DecisionTreeClassifier(max_depth=2, random_state=1)\n\n# Instantiate ada\nada = AdaBoostClassifier(base_estimator=dt, n_estimators=180, random_state=1)\n\n# Fit ada to the training set\nada.fit(X_train,y_train)\n\n# Compute the probabilities of obtaining the positive class\ny_pred_proba = ada.predict_proba(X_test)[:,1]\n\n# Evaluate test-set roc_auc_score\nada_roc_auc = roc_auc_score(y_test, y_pred_proba)\n\n# Print roc_auc_score\nprint('ROC AUC score: {:.2f}'.format(ada_roc_auc))\n\nfpr, tpr, _ = roc_curve(y_test,  y_pred_proba)\nauc = roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","90875220":"dt.fit(X_train,y_train)\nfig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(dt,feature_names = X_train.columns,class_names='BM')","9c1e48f1":"********Mean Features","ed03194d":"# Dataset metrics","755c9e3c":"****Features SE","61e59576":"# AdaBoost Classifier with DT","93708e37":"# The Data","e55b81cc":"****All Features"}}