{"cell_type":{"accbc388":"code","83f93441":"code","91e4a269":"code","aaa2bf95":"code","fcb0046c":"code","9f57e336":"code","b5f2b8fd":"code","1070688f":"code","4c474661":"code","325cba5d":"code","2b2216a3":"code","41435039":"markdown","619c7e41":"markdown","4372c097":"markdown","3d80e505":"markdown","4c53e1aa":"markdown","0b1f4c4f":"markdown","df38e7a3":"markdown","36b62afb":"markdown","304df36b":"markdown","eef287c2":"markdown","f0990f48":"markdown"},"source":{"accbc388":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\nimport seaborn as sns\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn import linear_model, naive_bayes, neighbors, tree, ensemble, svm\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# Any results you write to the current directory are saved as output.","83f93441":"data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndata_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndata.head()","91e4a269":"# Start with finding which columns have null values\n\n#print(data.Pclass.isnull().sum())\n#print(data.Name.isnull().sum())\n#print(data.Sex.isnull().sum())\nprint(data.Age.isnull().sum()) # 177 missing\n#print(data.SibSp.isnull().sum())\n#print(data.Parch.isnull().sum())\n#print(data.Ticket.isnull().sum())\n#print(data.Fare.isnull().sum())\nprint(data.Cabin.isnull().sum()) # 687 missing\nprint(data.Embarked.isnull().sum()) # 2 missing\n\n#Find columns with categorical values\ncategorical = [data.Name, data.Sex, data.Ticket, data.Embarked]\npd.DataFrame(categorical)\n","aaa2bf95":"#Idea: Maybe we can determine age based on mean of marital status of that person\nnotMisc = (~data.Name.str.contains('Mr.', regex=False)) & (~data.Name.str.contains('Mrs.', regex=False) & (~data.Name.str.contains('Miss.', regex=False)) & (~data.Name.str.contains('Master.', regex=False)))\n\nmedian_ages = {'Mr.': 0, 'Mrs.': 0, 'Miss.': 0, 'Master.': 0, 'Misc': 0} #Prefixes like Rev., Dr. exist so 'Misc' \nmedian_ages['Mr.'] = math.ceil(data[data.Name.str.contains('Mr.', regex=False)].Age.median())\nmedian_ages['Mrs.'] = math.ceil(data[data.Name.str.contains('Mrs.', regex=False)].Age.median())\nmedian_ages['Miss.'] = math.ceil(data[data.Name.str.contains('Miss.', regex=False)].Age.median())\nmedian_ages['Master.'] = math.ceil(data[data.Name.str.contains('Master.', regex=False)].Age.median())\nmedian_ages['Misc'] = math.ceil(data[notMisc].Age.median())\nprint(median_ages)","fcb0046c":"# #Fill in Age Column now that we have the mean age data\n\ndef set_age(row):\n    if 'Mr.' in row.Name:\n        if math.isnan(row.Age): \n            row.Age = median_ages['Mr.']\n        row.Name = 'Mr.' #Also changing the Name to just the titles to encode the values better later\n    elif 'Mrs.' in row.Name:\n        if math.isnan(row.Age):\n            row.Age = median_ages['Mrs.']\n        row.Name = 'Mrs.'\n    elif 'Miss.' in row.Name:\n        if math.isnan(row.Age):\n            row.Age = median_ages['Miss.']\n        row.Name = 'Miss.'\n    elif 'Master.' in row.Name: \n        if math.isnan(row.Age):\n            row.Age = median_ages['Master.']\n        row.Name = 'Master.'\n    else:\n        row.Name = 'Misc'\n    \n    if math.isnan(row.Age):\n        row.Age = median_ages['Misc']\n    \n        \n\n    return row\n\ndata = data.apply(set_age, axis=\"columns\")\ndata_test = data_test.apply(set_age, axis=\"columns\")\n\n#Drop Cabin column\ntry:\n    data.drop('Cabin', axis=\"columns\", inplace = True)\n    data_test.drop('Cabin', axis=\"columns\", inplace = True)\n\nexcept: #to run this cell multiple times and not have this return an error because there's now no Cabin column\n    pass\n    \n#Fill in Embarked column\ndata['Embarked'].fillna(data['Embarked'].mode()[0], inplace = True)\ndata_test['Embarked'].fillna(data_test['Embarked'].mode()[0], inplace = True)\n\ndata.head()\n","9f57e336":"from sklearn import preprocessing\n\n#Converting categorical variables to integers -- Name, Sex, Embarked, REMOVE Ticket\n\n#Name - I already changed all names to just the titles so it's easier to encode with just 5 different variables\nname_dummies = pd.get_dummies(data.Name, prefix='Name')\nname_dummies2 = pd.get_dummies(data_test.Name, prefix='Name')\n\n#Sex\nsex_dummies = pd.get_dummies(data.Sex, drop_first=True, prefix='Sex') #drop_first drops the 'female' column because it's redundant\nsex_dummies2 = pd.get_dummies(data_test.Sex, drop_first=True, prefix='Sex') \n\n#Embarked\nembarked_dummies = pd.get_dummies(data.Embarked, prefix='Embarked')\nembarked_dummies2 = pd.get_dummies(data_test.Embarked, prefix='Embarked')\n\n#Add these dummy columns to the data\ndata = pd.concat([data, name_dummies, sex_dummies, embarked_dummies], axis=1) # Also, axis=1 is same as axis=\"columns\"\ndata_test = pd.concat([data_test, name_dummies2, sex_dummies2, embarked_dummies2], axis=1) \n\n#Remove the original columns \ndata.drop(['Name', 'Sex', 'Embarked'], axis=1, inplace = True)\ndata_test.drop(['Name', 'Sex', 'Embarked'], axis=1, inplace = True)\n\n#Remove Ticket and PassengerID as well\ndata.drop(['Ticket', 'PassengerId'], axis=1, inplace = True)\ntest_pID = data_test.PassengerId\ndata_test.drop(['Ticket', 'PassengerId'], axis=1, inplace = True)\n\ndata.head()\ndata_test.head()","b5f2b8fd":"# P-class vs Survived\nplt.figure()\nsns.barplot(x = data['Pclass'], y = data['Survived'])\nplt.ylabel('Survival Rate')\nplt.title('Survival Rate by Passenger Class')\n\n# Age vs Survived\nplt.figure()\nplt.figure(figsize = (14,7))\nplt.title('Age vs Survived')\nsns.swarmplot(x = data['Survived'], y = data['Age'], size=3)\n\n# SibSp vs Survived\nplt.figure()\nplt.ylabel('Survival Rate')\nplt.title('Survival Rate by Siblings on Board')\nsns.barplot(x = data['SibSp'], y = data['Survived'])\n\n# Parch vs Survived\nplt.figure()\nplt.ylabel('Survival Rate')\nplt.title('Survival Rate by Family on Board')\nsns.barplot(x = data['Parch'], y = data['Survived'])\n\n#Fare vs Survived\nplt.figure()\nsns.displot(x = data['Fare'], hue=data['Survived'])\n\n# Title vs Survived\nplt.figure()\nplt.subplot(1,9,1)\nsns.barplot(x=data['Name_Mr.'], y = data['Survived'])\nplt.subplot(1,9,3)\nsns.barplot(x=data['Name_Mrs.'], y = data['Survived'])\nplt.subplot(1,9,5)\nsns.barplot(x=data['Name_Miss.'], y = data['Survived'])\nplt.subplot(1,9,7)\nsns.barplot(x=data['Name_Master.'], y = data['Survived'])\nplt.subplot(1,9,9)\nsns.barplot(x=data['Name_Misc'], y = data['Survived'])\n\n# Sex vs Survived\nplt.figure()\nsns.barplot(x=data['Sex_male'], y = data['Survived'])\nplt.xlabel('Gender (0->Female\/1->Male)')\n\n# Title vs Survived\nplt.figure()\nplt.subplot(1,5,1)\nsns.barplot(x=data['Embarked_C'], y = data['Survived'])\nplt.subplot(1,5,3)\nsns.barplot(x=data['Embarked_Q'], y = data['Survived'])\nplt.subplot(1,5,5)\nsns.barplot(x=data['Embarked_S'], y = data['Survived'])\n\n# Heatmap to see any big correlations amongst the features\nplt.figure(figsize=(14,12))\nsns.heatmap(data.astype(float).corr(),linewidths=0.1, annot=True)\n","1070688f":"# Algorithms to test: Linear Regression, Logistic Regression, Naive Bayes, K-Nearest Neighbors, Decision Tree, Random Forest, SVM, SGD\n\nalgo_scores = {'linReg': 0, 'logReg': 0, 'naiveGauss': 0, 'knn': 0, 'dtree': 0, 'rmf': 0, 'svm':0, 'sgd': 0}\n\nX_train, X_test, y_train, y_test = tts(data.iloc[:, 1:], data.iloc[:, 0:1], test_size=0.20)\n\nlinReg = linear_model.LinearRegression().fit(X_train, y_train)\nalgo_scores['linReg'] = linReg.score(X_test, y_test)\n\nlogReg = linear_model.LogisticRegression().fit(X_train, y_train.iloc[:, 0]) #added the iloc because LR expects a 1d array for y\nalgo_scores['logReg'] = logReg.score(X_test, y_test)\n\nnaiveGauss = naive_bayes.GaussianNB().fit(X_train, y_train.iloc[:,0]) #Gaussian seemed better than Bernoulli or Mutlinomial NBs\nalgo_scores['naiveGauss'] = naiveGauss.score(X_test, y_test)\n\nknn = neighbors.KNeighborsClassifier().fit(X_train, y_train.iloc[:,0])\nalgo_scores['knn'] = knn.score(X_test, y_test)\n\ndtree = tree.DecisionTreeClassifier().fit(X_train, y_train.iloc[:,0])\nalgo_scores['dtree'] = dtree.score(X_test, y_test)\n\nrmf = ensemble.RandomForestClassifier(n_estimators=100).fit(X_train, y_train.iloc[:,0])\nalgo_scores['rmf'] = rmf.score(X_test, y_test)\n\nsvc = svm.SVC(kernel='linear', random_state=42).fit(X_train, y_train.iloc[:,0])\nalgo_scores['svm'] = svc.score(X_test, y_test)\n\nsgd = linear_model.SGDClassifier().fit(X_train, y_train.iloc[:,0])\nalgo_scores['sgd'] = sgd.score(X_test, y_test)\n\nalgo_scores","4c474661":"#data['Family'] = data['SibSp'] + data['Parch']\n#data.drop(['SibSp', 'Parch'], axis=1, inplace = True)\n#data.head()\n\nX_train, X_test, y_train, y_test = tts(data.iloc[:, 1:], data.iloc[:, 0:1], test_size=0.20)\nlogReg = linear_model.LogisticRegression().fit(X_train, y_train.iloc[:, 0])\nsvc = svm.SVC(kernel='linear', random_state=42).fit(X_train, y_train.iloc[:,0])\nrmf = ensemble.RandomForestClassifier(n_estimators=100).fit(X_train, y_train.iloc[:,0])\n\n#data_test['Family'] = data_test['SibSp'] + data_test['Parch']\n#data_test.drop(['SibSp', 'Parch'], axis=1, inplace = True)","325cba5d":"data_test['Fare'].fillna(data_test['Fare'].mode()[0], inplace = True)\nprediction_logReg = logReg.predict(data_test)\n\nprediction_logReg = pd.DataFrame({'PassengerId': test_pID, 'Survived': prediction_logReg})\n\nprediction_logReg.to_csv('prediction_logReg.csv', index=False)\n\n\nprediction_svm = svc.predict(data_test)\n\nprediction_svm = pd.DataFrame({'PassengerId': test_pID, 'Survived': prediction_svm})\n\nprediction_svm.to_csv('prediction_svm.csv', index=False)\n\n\nprediction_rmf = rmf.predict(data_test)\n\nprediction_rmf = pd.DataFrame({'PassengerId': test_pID, 'Survived': prediction_rmf})\n\nprediction_rmf.to_csv('prediction_rmf.csv', index=False)\n\nprediction_rmf","2b2216a3":"votingClf = ensemble.VotingClassifier(estimators=[('log', linear_model.LogisticRegression()), \n                                                ('svm', svm.SVC(kernel='linear', random_state=42, probability=True)), \n                                                ('rmf', ensemble.RandomForestClassifier(n_estimators=100))], voting='soft', n_jobs=4)\n\nvotingClf = votingClf.fit(X_train, y_train.iloc[:,0])\n\n\nprediction_ens = votingClf.predict(data_test)\n\nprediction_ens = pd.DataFrame({'PassengerId': test_pID, 'Survived': prediction_ens})\n\nprediction_ens.to_csv('prediction_ens.csv', index=False)","41435039":"### Trying to Improve\n\nI've read a lot about ensembling models together to create a more powerful one so I'm going to try that out below by combining my 3 highest performing ones. \n\nTurns out that ensembling didn't really help improve my score so either I'm ensembling the wrong algorithms or I need to work on my dataset.","619c7e41":"## Model Creation","4372c097":"### Insights so far\nI personally think that this looks like a much cleaner dataset containing only the most important stuff. However, I do have 2 questions. \n1. Are the Fare values ok being decimals with such extended decimal numbers or should we round them in some way?\n2. Can SibSp and Parch be combined into one column called 'Family' or something? (I've read that having too high a dimension space can cause poor performance)\n\nAnswers to these questions:\n1. Ok so I couldn't find too many opinions one way or the other, except this which basically just says if you are going to do a lot of calculations, it's better to have more decimals so I will leave that column as it is.\n2. I'm not too entirely sure about this but I think I will not do anything for now, look at the accuracies and then merge these columns and check again.\n\nSo yay! Our dataset is clean, it's concise, and now we can move onto exploring it some more.","3d80e505":"# Titanic Dataset\n\n1. Preprocessing and cleaning up the data\n2. EDA and Visualization\n3. Model Creation\n4. Testing Models\n5. Generating Submission File","4c53e1aa":"### Encoding Categorical Variables\n\n**There seem to be 3 different ways to encode categorical variables:**\n1. Simple Replace (Changing 'five' to 5, but this doesn't seem to be applicable here as it would be really similar to label encoding)\n2. Label Encoding (Changing Embarked (S,C,Q) -> (0,1,2))\n3. One Hot Encoding (Similar to label encoding but makes one column into multiple and does one hot encoding)\n\nThe benefit of One Hot Encoding is that it doesn't give unintentional weight to certain values that might throw off the machine. For example, since 2 is greater 0 and 1, the final model might presume Q has a higher importance than S or C as ports to embark from. (I was pleasantly surprised to learn this was a concept in data processing because I had learned about it just this semester in my Digital Logic class) The downside to this though is that we will have a lot more columns in the dataset to account for the one-hot encoding.\n\nFor this reason, I'll be using One Hot Encoding to replace the 3 columns.\n\n[Here's a really useful article](https:\/\/towardsdatascience.com\/what-is-one-hot-encoding-and-how-to-use-pandas-get-dummies-function-922eb9bd4970) I learned more about this from.","0b1f4c4f":"## Preprocessing\n|Variable  | Definition    |Key    |\n|----------|:-------------:|------:|\n|survival  |Survival       |  0 = No, 1 = Yes|\n|pclass    |Ticket class   |  1 = 1st, 2 = 2nd, 3 = 3rd|\n|sex       |Sex            | |\n|Age       |Age in years   | |\n|sibsp     |# of siblings \/ spouses aboard the Titanic| |\n|parch     |# of parents \/ children aboard the Titanic|\n|ticket    |Ticket number| |\n|fare      |Passenger fare| |\n|cabin     | Cabin number| |\n|embarked  |Port of Embarkation |C = Cherbourg, Q = Queenstown, S = Southampton|","df38e7a3":"## Insights so far\n1. We need to fill in data for Age, Cabin, and Embarked. However, since **more than 3\/4ths of the data points don't have a Cabin value**, it doesn't seem that it will be very useful so I will skip that and remove that column from the dataset later.\n2. There are 4 columns with categorical data that will need to be mapped to integers. However, **Ticket doesn't seem like it will map easily and also doesn't look like it will be that useful** in determining anything about the individual so I will skip and remove that as well. \n3. PassengerID is the same as the index so remove that as well.\n4. Could try combining the SibSp and Parch columns into one.\n\nNow onto filling in and converting the data --> ","36b62afb":"## Submission File","304df36b":"## EDA and Visualization\n\nI'm going to go with a pretty basic approach, seeing how each column corresponds to survival rate. I think this should be enough to determine if there are any columns I might not want to include in the model's training.","eef287c2":"### Insights so far\n\n**Data Insights**\n1. Survival rate is directly proportional to the class that people are in. (If you're in 1st, you're twice as likely to survive than someone in 3rd)\n2. The only big trend in Ages I noticed was the ratio of death\/survival for people aged 30 was higher than the ratio for other ages. (Although the greatest number of people were 30 than any other age by a long shot)\n3. There's a higher chance of survival if you had 1 or 2 sibling\/spouse on board.\n4. A very small percent of men survived compared to women, children, and those with other titles.\n5. People embarking from Cherbourg had the highest chance of survival (55%) with those from Queenstown having a 38% chance and those from Southamptonn having a 33%. \n\n**Reasoning**\n1. The organization of the Titanic was that the lowest class\/tickets were underneath the higher classes so it makes sense that the higher class people had quicker access to lifeboats and whatnot.\n2. I guess if you had a spouse or sibling on board, you had someone to count on to get out with and find whatever resources were available.\n3. It's common in an emergency scenario to evacuate women and children first.","f0990f48":"### Insights so far\nFrom running these tests multiple times, SVM and Logistic Regression seem to be the best classifiers.\n\n**Results:**\nThe highest score I got was 0.77511 using Logistic Regression\n\nI'm thinking of doing some more feature engineering, like merging the SibSp and Parch columns to see if I can get a better score.\n\nSomething else I might do is learn some more about ensembling and maybe combining the results of my SVM and Logistic Regression models\n"}}