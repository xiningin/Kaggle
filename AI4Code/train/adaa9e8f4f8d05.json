{"cell_type":{"9d97c6bd":"code","3cfe7443":"code","4577a917":"code","5114a9b8":"code","dff5180c":"code","c06ddf70":"code","20aba343":"code","5b0cfa54":"code","199a94d3":"code","4ae74c44":"code","9a358edd":"code","c239a276":"code","2af08551":"code","a79acc4e":"code","c9ff3d39":"code","a6357fdd":"code","c399a958":"code","32c02134":"code","6d409a16":"code","4add80e5":"code","776256fb":"code","4b3b5718":"code","32cfe639":"code","49c37792":"code","c26ca4b2":"code","10bb5ff2":"markdown","b2a6bf5c":"markdown","43b85022":"markdown","cfd7011c":"markdown","04ba9b2b":"markdown","3b4e6e69":"markdown","56c53fff":"markdown","c27f644e":"markdown"},"source":{"9d97c6bd":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\nimport requests\nimport re\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nBASE_DIR = '..\/input\/commonlitreadabilityprize'\n\nprint(os.listdir(BASE_DIR))","3cfe7443":"train = pd.read_csv(os.path.join(BASE_DIR, 'train.csv'))\ntest = pd.read_csv(os.path.join(BASE_DIR, 'test.csv'))\nexternal = pd.read_csv('..\/input\/readability-url-scrape\/external.csv')","4577a917":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","5114a9b8":"external.dropna(inplace=True)","dff5180c":"external","c06ddf70":"def remove_newline(text):\n    text = text.split('\\n')\n    return '\\n'.join(list(filter(lambda x: x != \"\", text)))","20aba343":"# filter out \\n characters\nexternal['external_text'] = external['external_text'].map(remove_newline)","5b0cfa54":"external['excerpt_modified'] = external['excerpt'].apply(lambda x: ' '.join(x.split()))\nexternal['external_text_modified'] = external['external_text'].apply(lambda x: ' '.join(x.split()))","199a94d3":"def compare_columns(col1, col2, compare_func):\n    result = []\n\n    for x, y in zip(external[col1], external[col2]):\n        result.append(compare_func(x, y))\n        \n    return result\n\n# comparison functions\ndef len_diff(col1, col2):\n    return abs(len(col2) - len(col1))\n\ndef word_diff(col1, col2):\n    return abs(len(col2.split()) - len(col1.split()))","4ae74c44":"external['jaccard'] = compare_columns('excerpt_modified','external_text_modified', jaccard)\nexternal['len_diff'] = compare_columns('excerpt_modified','external_text_modified', len_diff)\nexternal['word_diff'] = compare_columns('excerpt_modified','external_text_modified', word_diff)","9a358edd":"# Distribution of Jaccard Scores between scraped and competition texts\nsns.displot(external['jaccard']);","c239a276":"sns.displot(external[external['jaccard'] < 0.2]['target']);","2af08551":"external[external['jaccard'] < 0.2]","a79acc4e":"sns.displot(external[external['jaccard'] < 0.2]['len_diff']);","c9ff3d39":"sns.displot(external[external['jaccard'] < 0.2]['word_diff']);","a6357fdd":"external[external['jaccard'] < 0.2]","c399a958":"stats = []\n\nfor (orig_text, ext_text) in zip(external['excerpt'], external['external_text']):\n    scores = []\n    for orig in orig_text.split('\\n'):\n        for idx, ext in enumerate(ext_text.split('\\n')):\n            scores.append(jaccard(orig, ext))\n        \n    stats.append(max(scores))\n    \nsns.displot(stats);","32c02134":"# Filter out texts that have Jaccard Scores > 0.5 - this value is arbitrary\nfiltered_external_text = []\n\nfor (orig_text, ext_text) in zip(external['excerpt'], external['external_text']):\n    orig_text = orig_text.split('\\n')\n    ext_text = ext_text.split('\\n')\n    scores = []\n    for orig in orig_text:\n        for idx, ext in enumerate(ext_text):\n            scores.append(jaccard(orig, ext))\n        \n    threshold = 0.5\n    if(max(scores) > threshold):\n        start = np.argmax(scores)\n        end = start + len(orig_text)\n        joined_text = '\\n'.join(ext_text[:start] + ext_text[end:])\n    else:\n        joined_text = '\\n'.join(ext_text)\n    filtered_external_text.append(joined_text)","6d409a16":"external['usable_external'] = filtered_external_text","4add80e5":"usable = external.query('jaccard < 0.2 and usable_external != \"\"')","776256fb":"export = usable[['id', 'usable_external']].reset_index(drop=True)\nexport.to_csv('external_df.csv')\nexport.head()","4b3b5718":"merged = pd.merge(train, export, on='id', how='left')","32cfe639":"merged","49c37792":"def create_external_df(train=None, sample_rate=0.1):\n    train = pd.merge(train, export, on='id', how='left')\n    df = train[~train['usable_external'].isnull()]\n    \n    size = 205 # word count in sentence\n    new_text = list(map(lambda x: [x[i:i + size] for i in range(0, len(x), size)], df['usable_external'].map(lambda x: x.split())))\n    new_text = list(map(lambda x: list(filter(lambda y: len(y) > 100, x)), new_text))\n    df['external_text'] = new_text\n    print(f'Rows of size {size}:', sum(list(map(lambda x: len(x), new_text))))\n    \n    data = []\n    for ID, target, text in zip(df['id'], df['target'], df['external_text']):\n        for chunk in text:\n            data.append([ID, target, ' '.join(chunk)])\n    print(f'Size of external df: {len(data)}. Sampling {round(len(data) * sample_rate)} rows')\n    df = pd.DataFrame(data, columns=['id','target', 'excerpt']).sample(frac=sample_rate)\n    return pd.concat([train[['id', 'target', 'excerpt']], df]).sample(frac=1)","c26ca4b2":"kf = KFold(n_splits=5)\nfor trn_idx, val_idx in kf.split(train):\n    shuffled_combined_df = create_external_df(train=train)\n    sns.displot(shuffled_combined_df['target']);\n    break","10bb5ff2":"I am using a threshold of 0.2 which is arbitrary and just based on looking at the distribution of jaccard scores above.","b2a6bf5c":"#### How does the distribution of pseudo-labeled external data look?","43b85022":"## Exploration of External Data","cfd7011c":"Feel free to fork and mess around, however, I did not find the external data to be very useful in increase competition performance.  \n\nI only scraped from -  \n**kids.frontiersin.org,**    \n**en.wikibooks.org,**  \n**simple.wikipedia.org**  \nsince they make up ~570 original sources of external text and represent a huge portion of text.\n\nPlease let me know if you manage to make the external data useful! Good luck!\n\n[Notebook to dataset collection](https:\/\/www.kaggle.com\/teeyee314\/readability-url-scrape)","04ba9b2b":"I am using the jaccard score to measure similarity between scraped and competition text. Perhaps there is a more elegant way to do this.","3b4e6e69":"# Using exported data","56c53fff":"Merge the train dataframe with the external dataframe","c27f644e":"### Export usable external text"}}