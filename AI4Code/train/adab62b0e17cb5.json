{"cell_type":{"8789ce98":"code","c4035728":"code","ca7b81e6":"code","c9dbda2b":"code","a1fc5fd2":"code","754dc73d":"code","39aea8d3":"code","eb410972":"code","acaeb6f3":"code","908d6178":"code","7274be06":"code","71d3e8d2":"code","e361409d":"code","5e3aa9b8":"code","68bb2f99":"code","03ad64a0":"code","028a1297":"code","c99249f0":"code","5a2cc81c":"code","3985b076":"code","9293a291":"code","21940a18":"code","2b6cb8fb":"code","69a97262":"code","2447fdd6":"markdown","a0045c63":"markdown","eb2cd715":"markdown","c0a6859d":"markdown","b8bf7b85":"markdown","50d6c5ab":"markdown","94513632":"markdown","0f1e4a53":"markdown","e7804677":"markdown","09b219e4":"markdown","3e22ceda":"markdown","d57c7359":"markdown","507c4652":"markdown","12077ea8":"markdown","5ab8e6ff":"markdown","43ba0cf6":"markdown"},"source":{"8789ce98":"import numpy as np \nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.probability import FreqDist\nimport string as s\nimport re\n\nimport matplotlib.pyplot as plt\n\n\nimport seaborn as sns\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c4035728":"cb_data= pd.read_csv('\/kaggle\/input\/clickbait-dataset\/clickbait_data.csv')\ncb_data.head()","ca7b81e6":"sns.countplot(cb_data.clickbait)","c9dbda2b":"x=cb_data.headline\ny=cb_data.clickbait\ntrain_x,test_x,train_y,test_y=train_test_split(x,y,test_size=0.25,random_state=22,stratify=cb_data['clickbait'])","a1fc5fd2":"print(\"No. of elements in training set\")\nprint(train_x.size)\nprint(\"No. of elements in testing set\")\nprint(test_x.size)","754dc73d":"print(train_x.head())\nprint(train_y.head())","39aea8d3":"print(test_x.head())\nprint(test_y.head())","eb410972":"def tokenization(text):\n    lst=text.split()\n    return lst\ntrain_x=train_x.apply(tokenization)\ntest_x=test_x.apply(tokenization)","acaeb6f3":"def lowercasing(lst):\n    new_lst=[]\n    for i in lst:\n        i=i.lower()\n        new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(lowercasing)\ntest_x=test_x.apply(lowercasing)  ","908d6178":"def remove_stopwords(lst):\n    stop=stopwords.words('english')\n    new_lst=[]\n    for i in lst:\n        if i not in stop:\n            new_lst.append(i)\n    return new_lst\n\ntrain_x=train_x.apply(remove_stopwords)\ntest_x=test_x.apply(remove_stopwords)  ","7274be06":"def remove_punctuations(lst):\n    new_lst=[]\n    for i in lst:\n        for j in s.punctuation:\n            i=i.replace(j,'')\n        new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(remove_punctuations)\ntest_x=test_x.apply(remove_punctuations)  ","71d3e8d2":"def remove_numbers(lst):\n    nodig_lst=[]\n    new_lst=[]\n    for i in lst:\n        for j in s.digits:    \n            i=i.replace(j,'')\n        nodig_lst.append(i)\n    for i in nodig_lst:\n        if i!='':\n            new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(remove_numbers)\ntest_x=test_x.apply(remove_numbers)","e361409d":"def remove_spaces(lst):\n    new_lst=[]\n    for i in lst:\n        i=i.strip()\n        new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(remove_spaces)\ntest_x=test_x.apply(remove_spaces)","5e3aa9b8":"train_x.head()","68bb2f99":"test_x.head()","03ad64a0":"lemmatizer=nltk.stem.WordNetLemmatizer()\ndef lemmatzation(lst):\n    new_lst=[]\n    for i in lst:\n        i=lemmatizer.lemmatize(i)\n        new_lst.append(i)\n    return new_lst\ntrain_x=train_x.apply(lemmatzation)\ntest_x=test_x.apply(lemmatzation)","028a1297":"train_x=train_x.apply(lambda x: ''.join(i+' ' for i in x))\ntest_x=test_x.apply(lambda x: ''.join(i+' ' for i in x))","c99249f0":"from sklearn.feature_extraction.text import CountVectorizer\ncov=CountVectorizer(analyzer='word', ngram_range=(1,2),max_features=22500)\ntrain_1=cov.fit_transform(train_x)\ntest_1=cov.transform(test_x)","5a2cc81c":"train_arr=train_1.toarray()\ntest_arr=test_1.toarray()","3985b076":"pd.DataFrame(test_arr[:100], columns=cov.get_feature_names())","9293a291":"NB_MN=MultinomialNB()\nNB_MN.fit(train_arr,train_y)\npred=NB_MN.predict(test_arr)\n","21940a18":"from sklearn.metrics import f1_score,accuracy_score\nprint(\"F1 score of the model\")\nprint(f1_score(test_y,pred))\nprint(\"Accuracy of the model\")\nprint(accuracy_score(test_y,pred))\nprint(\"Accuracy of the model in percentage\")\nprint(accuracy_score(test_y,pred)*100,\"%\")","2b6cb8fb":"from sklearn.metrics import confusion_matrix\nprint(\"Confusion Matrix\")\nprint(confusion_matrix(test_y,pred))\n\nfrom sklearn.metrics import classification_report\nprint(\"Classification Report\")\nprint(classification_report(test_y,pred))\n","69a97262":"sns.set(font_scale=1.5)\ncof=confusion_matrix(test_y, pred)\ncof=pd.DataFrame(cof, index=[i for i in range(2)], columns=[i for i in range(2)])\nplt.figure(figsize=(8,8))\n\nsns.heatmap(cof, cmap=\"PuRd\",linewidths=1, annot=True,square=True,cbar=False,fmt='d',xticklabels=['Non-clickbait','Clickbait'],yticklabels=['Non-clickbait','Clickbait'])\nplt.xlabel(\"Predicted Class\")\nplt.ylabel(\"Actual Class\")\n\nplt.title(\"Confusion Matrix for Clickbait Classification\")","2447fdd6":"# Importing different tools and libraries\n\nThe main libraries used are *Numpy*, *Pandas*, *NLTK*(Natural language toolkit) and *Scikit-learn*.","a0045c63":"# Removing Stopwords","eb2cd715":"# Lemmatization\n\nLemmatization in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. It involves the morphological analysis of words.\n\nIn lemmatization we find the root word or base form of the word rather than just clipping some characters from the end e.g. *is, are, am* are all converted to its base form *be* in Lemmatization\n\nHere lemmatization is done using NLTK library.","c0a6859d":"# Loading the Dataset","b8bf7b85":"# Converting to lowercase\n\nThe data is converted into lowercase to avoid ambiguity between same words in different cases like 'NLP', 'nlp' or 'Nlp'. ","50d6c5ab":"# Define Naive Bayes Classifier and training","94513632":"# Clickbait detector using Naive Bayes Classifier\n\nThis kernel focuses on classifying News headlines into clickbaits and non-clickbaits.\n\nThe clickbaits are labelled as **1** and non-clickbaits as **0**.\nThe headlines are collected from different news sites.\n\nThe dataset consists of 32000 headlines of which 50% are clickbaits and the other 50% are non-clickbait.\n\nI have used a *Multinomial Naive Bayes* classification algorithm for text classification of the given dataset. ","0f1e4a53":"# Analyzing Train and Test Data","e7804677":"# Removing punctuation\n\nThe punctuations are removed to increase the efficiency of the model. They are irrelevant because they provide no added information.","09b219e4":"# Analyzing data after preprocessing\n\nAfter preprocessing the data i.e. after removing punctuation, stopwords, spaces and numbers.","3e22ceda":"# Tokenization of Data\n\nThe data is tokenized i.e. split into tokens which are the smallest or minimal meaningful units. The data is split into words.","d57c7359":"# Countvectoriser\n\nThis method is used to convert the text into features.","507c4652":"# Splitting into Train and Test sets\n\nThe dataset is splitted into training and testing sets. The percentage of training data is 75% and testing data is 25%.","12077ea8":"# Evaluation of Result\n\nThe Accuracy and F1 score of the model are printed to evaluate the model for text classification.","5ab8e6ff":"# Removing extra spaces","43ba0cf6":"# Removing Numbers"}}