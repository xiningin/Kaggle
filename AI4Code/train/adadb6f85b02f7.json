{"cell_type":{"d1f548e5":"code","ce147960":"code","d6cd72e5":"code","16406887":"code","3df2a35b":"code","33b54c9f":"code","35266eef":"code","19d0d65d":"code","a6e7892a":"code","07b8eae6":"code","48a19be6":"code","4f8c90a5":"code","580fe635":"code","78d15b82":"code","6906006e":"code","72b9f9fa":"code","2efa8ef2":"code","2cdd9ebb":"code","fe00c805":"code","3b1ff773":"code","c616cc88":"code","f5d54ff6":"code","a94fea70":"code","e4099552":"code","81a6366b":"code","0c051e1b":"code","fa2a349a":"code","7e56b2aa":"code","0e3c0cd5":"code","bd47ab88":"code","052e41b5":"code","6e125dec":"code","a80566c8":"code","ad8409b5":"code","88b7a75e":"code","38a39d3a":"code","fffa07c1":"code","3c3475b7":"code","cb972617":"code","a097c246":"code","d57104f9":"code","1c520285":"code","c5716654":"code","cf85e6bc":"code","08787274":"code","292784c1":"code","fea05480":"code","4fe5c8a3":"markdown","008e4f77":"markdown","25bc2810":"markdown","42100dbb":"markdown","d1330fbd":"markdown","1054c059":"markdown"},"source":{"d1f548e5":"from transformers import pipeline\n\nnlp = pipeline(\"question-answering\")\n\ncontext = r\"\"\"\nBERT has its origins from pre-training contextual representations including Semi-supervised Sequence Learning,Generative Pre-Training, \nELMo, and ULMFit. Unlike previous models, BERT is a deeply bidirectional, unsupervised language representation, pre-trained using only \na plain text corpus. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the \nvocabulary, where BERT takes into account the context for each occurrence of a given word. For instance, whereas the vector for \"running\" \nwill have the same word2vec vector representation for both of its occurrences in the sentences \"He is running a company\" and \"He is \nrunning a marathon\", BERT will provide a contextualized embedding that will be different according to the sentence.\n\"\"\"","ce147960":"result = nlp(question=\"What models are context-free?\", context=context)","d6cd72e5":"result","16406887":"print(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")\nresult = nlp(question=\"Who is the owner of BERT?\", context=context)\nprint(f\"Answer: '{result['answer']}', score: {round(result['score'], 4)}, start: {result['start']}, end: {result['end']}\")","3df2a35b":"nlp(question=\"Who is Roman Kyrychenko?\", context=context)","33b54c9f":"from transformers import AutoTokenizer, AutoModelForQuestionAnswering\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n\n\ntext = r\"\"\"\n\ud83e\udd17 Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\narchitectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet\u2026) for Natural Language Understanding (NLU) and Natural\nLanguage Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\nTensorFlow 2.0 and PyTorch.\n\"\"\"\n\nquestions = [\n    \"How many pretrained models are available in \ud83e\udd17 Transformers?\",\n    \"What does \ud83e\udd17 Transformers provide?\",\n    \"\ud83e\udd17 Transformers provides interoperability between which frameworks?\",\n]\n\nfor question in questions:\n    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].tolist()[0]\n\n    outputs = model(**inputs)\n    answer_start_scores = outputs.start_logits\n    answer_end_scores = outputs.end_logits\n\n    answer_start = torch.argmax(answer_start_scores)\n    answer_end = torch.argmax(answer_end_scores) + 1\n\n    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n\n    print(f\"Question: {question}\")\n    print(f\"Answer: {answer}\")","35266eef":"outputs","19d0d65d":"model_checkpoint = \"DeepPavlov\/rubert-base-cased\"# \"bert-base-multilingual-cased\"\nbatch_size = 16","a6e7892a":"import pandas as pd\n\ndf = pd.read_csv(\"..\/input\/tydiqa-for-chaii\/tydiqa_train.csv\")","07b8eae6":"df = df.loc[df['language'] == \"russian\"]","48a19be6":"df","4f8c90a5":"df['answer_end'] = df['answer_start'] + df['answer_text'].str.len() \ndf","580fe635":"train = df.iloc[:1000, :]\nvalid = df.iloc[6000:, :]","78d15b82":"from transformers import AutoTokenizer\n    \ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","6906006e":"tokenizer(\"\u0427\u043e\u043c\u0443 \u044f \u043d\u0435 \u0441\u043e\u043a\u0456\u043b?\", \"\u0422\u0438 data scientist\")","72b9f9fa":"max_length = 384 # The maximum length of a feature (question and context)\ndoc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.","2efa8ef2":"for i, example in train.iterrows():\n    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n        break\n    example = train.iloc[i]","2cdd9ebb":"example","fe00c805":"len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])","3b1ff773":"len(tokenizer(example[\"question\"], example[\"context\"], max_length=max_length, truncation=\"only_second\")[\"input_ids\"])","c616cc88":"tokenized_example = tokenizer(\n    example[\"question\"],\n    example[\"context\"],\n    max_length=max_length,\n    truncation=\"only_second\",\n    return_overflowing_tokens=True,\n    stride=0,#doc_stride,\n    padding=True,\n    return_tensors='pt'\n)","f5d54ff6":"tokenized_example","a94fea70":"[len(x) for x in tokenized_example[\"input_ids\"]]","e4099552":"for x in tokenized_example[\"input_ids\"][:2]:\n    print(tokenizer.decode(x))","81a6366b":"tokenized_example = tokenizer(\n    example[\"question\"],\n    example[\"context\"],\n    max_length=max_length,\n    truncation=\"only_second\",\n    return_overflowing_tokens=True,\n    return_offsets_mapping=True,\n    stride=doc_stride\n)\nprint(tokenized_example[\"offset_mapping\"][0][:100])","0c051e1b":"tokenized_example.keys()","fa2a349a":"first_token_id = tokenized_example[\"input_ids\"][0][1]\nprint(\"first_token_id\", first_token_id)\noffsets = tokenized_example[\"offset_mapping\"][0][1]\nprint(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])","7e56b2aa":"from torch.utils.data.dataset import Dataset\n\nclass SquadDataset(Dataset):\n    \n    def __init__(self, data): \n        self.data = data\n        \n    def __getitem__(self, index):\n        \n        d = self.data.iloc[index]\n        \n        s = tokenizer(d['question'], d['context'], \n                            padding=\"max_length\",\n                           max_length=max_length,\n                  truncation=True,\n                        return_overflowing_tokens=False,\n                        stride=doc_stride,\n                        return_tensors='pt')\n        \n        s = {key: val.squeeze() for key, val in s.items()}\n        \n        s.update({\n            'start_positions': torch.tensor([d['answer_start']]), \n            'end_positions': torch.tensor([d['answer_end']])\n        })\n        \n        return s\n    \n    def __len__(self):\n        return self.data.shape[0]\n    \ntrain_dataset = SquadDataset(train)\nvalid_dataset = SquadDataset(valid)","0e3c0cd5":"#train_dataset['input_ids'].shape","bd47ab88":"from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)","052e41b5":"from torch.utils.data import DataLoader\nfrom transformers import AdamW\nfrom tqdm import tqdm\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\nmodel.to(device)\n\nmodel.train()\n\noptim = AdamW(model.parameters(), lr=5e-5)\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n\nfor epoch in range(3):\n    model.train()\n    loop = tqdm(train_loader, leave=True)\n    for batch in loop:\n        optim.zero_grad()\n        input_ids = batch['input_ids'].squeeze().to(device)\n        attention_mask = batch['attention_mask'].squeeze().to(device)\n        start_positions = batch['start_positions'].to(device)\n        end_positions = batch['end_positions'].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask,\n                        start_positions=start_positions,\n                        end_positions=end_positions)\n        loss = outputs[0]\n        loss.backward()\n        optim.step()\n        loop.set_description(f'Epoch {epoch}')\n        loop.set_postfix(loss=loss.item())","6e125dec":"loss","a80566c8":"text = r\"\"\"\n\u0412\u0435\u043b\u0438\u0301\u043a\u0430\u044f \u041e\u0442\u0435\u0301\u0447\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u0430\u044f \u0432\u043e\u0439\u043d\u0430\u0301 (22 \u0438\u044e\u043d\u044f 1941 \u2014 9 \u043c\u0430\u044f[5] 1945) \u2014 \u0432\u043e\u0439\u043d\u0430 \u0421\u043e\u044e\u0437\u0430 \u0421\u043e\u0432\u0435\u0442\u0441\u043a\u0438\u0445 \u0421\u043e\u0446\u0438\u0430\u043b\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0420\u0435\u0441\u043f\u0443\u0431\u043b\u0438\u043a, \u043d\u0430\u0447\u0430\u0432\u0448\u0430\u044f\u0441\u044f \u0441 \u0432\u0442\u043e\u0440\u0436\u0435\u043d\u0438\u044f \u043d\u0430 \u0441\u043e\u0432\u0435\u0442\u0441\u043a\u0443\u044e \u0442\u0435\u0440\u0440\u0438\u0442\u043e\u0440\u0438\u044e \u0432\u043e\u0439\u0441\u043a \u043d\u0430\u0446\u0438\u0441\u0442\u0441\u043a\u043e\u0439 \u0413\u0435\u0440\u043c\u0430\u043d\u0438\u0438 \u0438 \u0435\u0451 \u0435\u0432\u0440\u043e\u043f\u0435\u0439\u0441\u043a\u0438\u0445 \u0441\u043e\u044e\u0437\u043d\u0438\u043a\u043e\u0432 (\u0412\u0435\u043d\u0433\u0440\u0438\u0438, \u0418\u0442\u0430\u043b\u0438\u0438, \u0420\u0443\u043c\u044b\u043d\u0438\u0438, \u0421\u043b\u043e\u0432\u0430\u043a\u0438\u0438, \u0424\u0438\u043d\u043b\u044f\u043d\u0434\u0438\u0438, \u0425\u043e\u0440\u0432\u0430\u0442\u0438\u0438) \u0438 \u0437\u0430\u043a\u043e\u043d\u0447\u0438\u0432\u0448\u0430\u044f\u0441\u044f \u043e\u0441\u0432\u043e\u0431\u043e\u0436\u0434\u0435\u043d\u0438\u0435\u043c \u043e\u0442 \u0444\u0430\u0448\u0438\u0437\u043c\u0430 \u0441\u0442\u0440\u0430\u043d \u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u043e\u0439 \u0438 \u0412\u043e\u0441\u0442\u043e\u0447\u043d\u043e\u0439 \u0415\u0432\u0440\u043e\u043f\u044b; \u0432\u0430\u0436\u043d\u0435\u0439\u0448\u0430\u044f \u0441\u043e\u0441\u0442\u0430\u0432\u043d\u0430\u044f \u0447\u0430\u0441\u0442\u044c \u0412\u0442\u043e\u0440\u043e\u0439 \u043c\u0438\u0440\u043e\u0432\u043e\u0439 \u0432\u043e\u0439\u043d\u044b, \u0437\u0430\u0432\u0435\u0440\u0448\u0438\u0432\u0448\u0430\u044f\u0441\u044f \u043f\u043e\u0431\u0435\u0434\u043e\u0439 \u041a\u0440\u0430\u0441\u043d\u043e\u0439 \u0410\u0440\u043c\u0438\u0438 \u0438 \u0431\u0435\u0437\u043e\u0433\u043e\u0432\u043e\u0440\u043e\u0447\u043d\u043e\u0439 \u043a\u0430\u043f\u0438\u0442\u0443\u043b\u044f\u0446\u0438\u0435\u0439 \u0432\u043e\u043e\u0440\u0443\u0436\u0451\u043d\u043d\u044b\u0445 \u0441\u0438\u043b \u0413\u0435\u0440\u043c\u0430\u043d\u0438\u0438. \u0412 \u0437\u0430\u043f\u0430\u0434\u043d\u044b\u0445 \u0441\u0442\u0440\u0430\u043d\u0430\u0445 \u0438\u043c\u0435\u043d\u0443\u0435\u0442\u0441\u044f \u00ab\u0412\u043e\u0441\u0442\u043e\u0447\u043d\u044b\u043c \u0444\u0440\u043e\u043d\u0442\u043e\u043c\u00bb[6], \u0432 \u0413\u0435\u0440\u043c\u0430\u043d\u0438\u0438 \u2014 \u0442\u0430\u043a\u0436\u0435 \u00ab\u041d\u0435\u043c\u0435\u0446\u043a\u043e-\u0421\u043e\u0432\u0435\u0442\u0441\u043a\u043e\u0439 \u0432\u043e\u0439\u043d\u043e\u0439\u00bb.\n\"\"\"\n\nquestions = [\n    \"\u041a\u043e\u0433\u0434\u0430 \u043d\u0430\u0447\u0430\u043b\u0430\u0441\u044c \u0432\u043e\u0439\u043d\u0430?\",\n    \"\u041a\u0442\u043e \u0432\u043e\u0435\u0432\u0430\u043b?\",\n    \"\u041a\u0430\u043a \u044d\u0442\u0443 \u0432\u043e\u0439\u043d\u0443 \u0435\u0449\u0435 \u043d\u0430\u0437\u044b\u0432\u0430\u044e\u0442?\",\n]\n\nfor question in questions:\n    inputs = tokenizer(question, text, add_special_tokens=True, return_tensors=\"pt\")\n    input_ids = inputs[\"input_ids\"].tolist()[0]\n\n    outputs = model.cpu()(**inputs)\n    answer_start_scores = outputs.start_logits\n    answer_end_scores = outputs.end_logits\n\n    answer_end = torch.argmax(answer_start_scores)\n    answer_start = torch.argmax(answer_end_scores) + 1\n\n    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n\n    print(f\"Question: {question}\")\n    print(f\"Answer: {answer}\")","ad8409b5":"torch.nn.Softmax()(answer_start_scores)","88b7a75e":"from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)","38a39d3a":"args = TrainingArguments(\n    f\"test-squad\",\n    evaluation_strategy = \"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=3,\n    weight_decay=0.01,\n)","fffa07c1":"from transformers import default_data_collator\n\ndata_collator = default_data_collator\n\ndata_collator","3c3475b7":"import torch\n\ntrainer = Trainer(\n    model,\n    args,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)","cb972617":"trainer.train()","a097c246":"trainer.save_model(\"test-squad-trained\")","d57104f9":"import torch\n\nfor batch in trainer.get_eval_dataloader():\n    break\nbatch = {k: v.to(trainer.args.device) for k, v in batch.items()}\nwith torch.no_grad():\n    output = trainer.model(**batch)\noutput.keys()","1c520285":"trainer.model(**batch).start_logits","c5716654":"output.start_logits.shape, output.end_logits.shape","cf85e6bc":"output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)","08787274":"n_best_size = 20","292784c1":"import numpy as np\n\nstart_logits = output.start_logits[0].cpu().numpy()\nend_logits = output.end_logits[0].cpu().numpy()\n# Gather the indices the best start\/end logits:\nstart_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\nend_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\nvalid_answers = []\nfor start_index in start_indexes:\n    for end_index in end_indexes:\n        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n            valid_answers.append(\n                {\n                    \"score\": start_logits[start_index] + end_logits[end_index],\n                    \"text\": \"\" # We need to find a way to get back the original substring corresponding to the answer in the context\n                }\n            )","fea05480":"valid_answers","4fe5c8a3":"## Loading the dataset\n\ntydiqa - multilingual dataset for QA","008e4f77":"## Model initialization","25bc2810":"## Fine-tuning the model","42100dbb":"## Inference Example","d1330fbd":"## Preprocessing the training data","1054c059":"## Evaluation"}}