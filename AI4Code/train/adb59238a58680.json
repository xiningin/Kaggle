{"cell_type":{"838f3d7e":"code","461d7385":"code","64024147":"code","dd2ff7f9":"code","dfaf003e":"code","89085a0e":"code","cc909c0e":"code","25716172":"code","66615f79":"code","2567d99c":"code","98017374":"code","b99a3e9c":"code","5ede732f":"code","ec99445c":"code","3650b729":"code","9ea5c4ca":"code","5a685a01":"code","0814cb58":"code","e99016eb":"code","94481971":"code","dcf44b2e":"code","f64f0937":"code","57b0ef73":"code","4a0c099d":"code","185977ce":"code","380a26a3":"code","6a5a857b":"code","343405f4":"code","7b54bf17":"code","06d36ca7":"code","06a2576a":"code","46866f65":"code","bb92a039":"code","da94948c":"code","453a9fd1":"code","d3d30281":"code","b0d627f0":"code","6793b37e":"code","f0895135":"code","ad7b7d34":"code","79ca2a7d":"code","33a9dbc4":"code","eb4f7463":"code","2cffb22c":"code","adcd0ab8":"code","d2429155":"code","0b7340af":"code","7b92785d":"code","1f474aac":"code","29b83185":"code","26fdf9bf":"code","a05dbc49":"code","bdafc89b":"code","2316b363":"code","924801f0":"code","69e6ce47":"code","5e395c5e":"code","6de409ac":"code","457268a8":"code","e931984b":"code","436e9da1":"code","a51711d6":"code","b292684a":"code","246db5d2":"code","1e57e651":"code","76707564":"code","ff2b49ad":"code","76853725":"markdown","1770d21d":"markdown","93d41bac":"markdown","53f94718":"markdown","33f2e478":"markdown","234286fe":"markdown","f05ba332":"markdown","edba6f23":"markdown","ddfb242c":"markdown","ff3f2001":"markdown","83593ec6":"markdown","6500ae47":"markdown","b29a73b1":"markdown","99b5a75f":"markdown","97961135":"markdown","8156cb97":"markdown","3c21dd7b":"markdown","84441292":"markdown","f3619d07":"markdown","2b63351f":"markdown","67cc14ef":"markdown","97975fa3":"markdown","7aa63387":"markdown","429ca9c7":"markdown","df6abe16":"markdown","d9d9a33c":"markdown","4d409505":"markdown","f2c1b2e9":"markdown","06893eac":"markdown","81864660":"markdown","bf87b05b":"markdown","01e9dca0":"markdown","7a00cbbc":"markdown","0733ac50":"markdown"},"source":{"838f3d7e":"#Aliasing (short name for module can be given) is used here like pd for pandas,\n#np for numpy and so on, you can use any name but its kind of\n#standard aliasing name used from long and thus using the same.\n\nimport pandas as pd #import pandas for dataframe operations\nimport numpy as np #import numpy for numerical operation \nimport matplotlib.pyplot as plt #import pyplot module of matplotlib\nimport seaborn as sns #import seaborn \n#to make all plots to be displayed just below the code.\n%matplotlib inline ","461d7385":"#kaggle path for the dataset can be found on right side input section and then copy sign.\n#while using kaggle you can work directly work on any dataset without downloading it too.\n\npath = \"..\/input\/zomato-bangalore-restaurants\/zomato.csv\" #defining path of dataset\ndata = pd.read_csv(path) #loading the dataset into a dataframe named as data.","64024147":"#let us first take a look over few entries of dataset\ndata.head() #returns the first 5 (default) entries of the dataset","dd2ff7f9":"#can use tail for last entries \ndata.tail() #return the last 5 (default) entries of the dataset","dfaf003e":"#let we find out the columns name of our dataset\ndata.columns #returns the list of column names in dataset","89085a0e":"#how many rows and columns, shape help in finding that out\ndata.shape #returns the shape as (rows, columns)","cc909c0e":"data.info() #a short information about dataset","25716172":"#take a look by selecting few sample of our desired features\ndata.loc[:,['url', 'address', 'name', \n       'phone', 'location', 'listed_in(type)', 'listed_in(city)'] ].sample(5)","66615f79":"#make use of this always when to understand which column we have to work on \ndata.columns","2567d99c":"#dropping the non required parameteres or columns, inplace= True makes it permanent change\ndata.drop(['url', 'address', 'listed_in(city)', 'phone'], axis='columns', inplace=True)","98017374":"#just a trial, undesired columns are removed.\ndata.head(2)","b99a3e9c":"#now the number of columns would be 17 - 4 that is 13\nlen(data.columns)","5ede732f":"#lets find out the duplicated entries\ndata.duplicated().sum()","ec99445c":"#dropping the duplicated entries with inplace=True to make the permanent effect\ndata.drop_duplicates(inplace=True)","3650b729":"#can also confirm the same by checking again and understanding.\ndata.duplicated().sum()","9ea5c4ca":"#Also the shape will have less rows now\ndata.shape","5a685a01":"#observing the missing values in percentage \n#missing * 100 \/ total\n#rounding off to 2 decimal values using round()\n#isna() returns true false and sum return count of true values \n(data.isna().sum() * 100 \/ len(data)).round(2)","0814cb58":"#let us first look at the rate feature (rating of the restaurant)\n#value_counts print the count of unique values of that column\ndata['rate'].value_counts()","e99016eb":"#for a clear look, let us check the unique values of the rate feature\ndata['rate'].unique()","94481971":"#let us first remove the whitespace.\n#re assign back this to the same rate column\ndata['rate'] = data['rate'].str.replace(' ', '')","dcf44b2e":"#let us know find the unique values of rate feature\ndata['rate'].unique()","f64f0937":"#lets take care of NEW AND - with nan means marking as missing and then we will take care of that.\n#here we are using np.nan from numpy module, inplace=True makes the permanent impact.\ndata['rate'].replace('NEW', np.nan, inplace=True)\ndata['rate'].replace('-', np.nan, inplace=True)","57b0ef73":"#lets us check the unique values now, would be left with nan to be handled\n#you can also use dot(.) notation to call the column but that fails \n#when you have whitespace in the feature name thus not much recommended\ndata.rate.unique()","4a0c099d":"#so let us check the na values in rate feature\ndata['rate'].isna().sum()","185977ce":"#lets have a look of review data \ndata['reviews_list'].values[1]","380a26a3":"#for good view we are using ast evaluation for string\nimport ast #abstract syntax trees\n#makes good view of the messy review data\nast.literal_eval(data.reviews_list.values[1])","6a5a857b":"#define a function to generate a new rating\n#here we are considering the review having more than 1 review and then extracting the data \n\ndef new_rate_definer(review_data):\n    review_data = ast.literal_eval(review_data)#ast evaluation\n    if not review_data or len(review_data) <= 1: #consider the correct review data \n        return None\n    #extract rating from each tuple of review data list using a loop and check\n    #type to be as string\n    #make rating_extract containg all the ratings type casted in float\n    rating_extract = [float(i[0].replace(\"Rated\", '').strip()) for i in review_data if type(i[0]) == str]\n    avg_r = sum(rating_extract) \/ len(rating_extract)  #calculate the average\n    \n    return avg_r\n\n","343405f4":"#apply the function defined above to review_list to create a new column new_rating\n#rounding off the returned value to 2 decimal values \ndata['new_rating'] = data['reviews_list'].apply(new_rate_definer).round(2)","7b54bf17":"#making rate column with only rating instead of \"\/5\" using string replace() method\n#changing the data type to float using astype()\n#reassign back to that column\ndata['rate'] = data['rate'].str.replace(\"\/5\", \"\").astype('float')","06d36ca7":"#now for a formal look, let us check the two columns for say 10 random\n#sample values rate and new_rating\ndata.loc[:, ['rate', 'new_rating']].sample(10)","06a2576a":"#finding out the index where our rating value are not available in \n#existing feature 'rate'\n#we can use isna() or isnull() both are same.\nna_index = data[data['rate'].isnull()].index","46866f65":"len(na_index)","bb92a039":"#for each na index we will fill it with corresponding value in new_rating feature.\nfor idx in na_index:\n    data.loc[idx,'rate'] = data.loc[idx, 'new_rating']","da94948c":"#let us check the missing values after filling the rate column from new_rating feature\ndata['rate'].isna().sum()","453a9fd1":"#dropping missing values of rate feature\ndata.dropna(subset=['rate', 'approx_cost(for two people)'],axis='index', inplace=True)","d3d30281":"data.shape","b0d627f0":"data.drop('new_rating', axis='columns', inplace=True)","6793b37e":"#let us know check the remaining missing values in the data\ndata.isna().sum()","f0895135":"data['cuisines'].isna().sum()","ad7b7d34":"data.drop(data[data['cuisines'].isna()].index, inplace=True)","79ca2a7d":"data.shape","33a9dbc4":"data.isna().sum()","eb4f7463":"data['rest_type'].value_counts()","2cffb22c":"data['rest_type'].fillna(value='Quick Bites', inplace=True)","adcd0ab8":"data.isna().sum() #now checking the missing values ","d2429155":"data[data['dish_liked'].isna()]","0b7340af":"data['dish_liked'] = data['dish_liked'].apply(lambda x: x.lower().strip() if isinstance(x, str) else x)","7b92785d":"data['dish_liked'] #check the dish_liked data.","1f474aac":"#creating a list of all dishes from dish_liked data.\ndish_liked = []\nfor i in list(data.index):#traversing all element of data\n    if (type(data['dish_liked'][i]) == str):\n        k= data['dish_liked'][i].split(', ') #string is splitted from comma into list elements.\n        dish_liked.extend(k)","29b83185":"len(dish_liked)","26fdf9bf":"len(set(dish_liked))","a05dbc49":"dish_unique = list(set(dish_liked))\ndish_unique = [x.lower() for x in dish_unique]\nlen(dish_unique)","bdafc89b":"sum(data['menu_item'] == np.nan)","2316b363":"data['menu_item'].sample(10)","924801f0":"sum(data['menu_item'] != '[]')","69e6ce47":"import re\navailable_menu_index = data[data['menu_item'] != '[]'].index\n\ndef bracket_remover(txt):\n    t = re.sub(\"[(].*[)]\",'', txt)\n    t = re.sub('\\[.*\\]', '', t)\n    return t.strip()\n    \nfor i in available_menu_index:\n    p = data.loc[i, 'menu_item']\n    p = list(set(p))\n    p = [x.lower() for x in p]\n    p_refined = [bracket_remover(txt) for txt in p ]\n    dish_unique.extend(p_refined)\ndish_unique = list(set(dish_unique))\n\n\ndish_unique = [x for x in dish_unique if len(x) > 3 ]\nprint(len(dish_unique))\n# dish_unique[:200]","5e395c5e":"# ast.literal_eval(data['reviews_list'][2]) #while writing the code used to check particular values.\n\n#define a function to return liked dish from reviewds datat\ndef liked_dish_selector(p):\n    #create a list to be returned in form of string.\n    liked = []\n    #abstract evaluation of input review list\n    p = ast.literal_eval(p)\n    #traverse each and every review of input review list\n    for review in p:\n        #take the rating and review by tuple unpacking\n        a, b = review\n        #define good ratings list\n        good_rating = ['Rated 4.0', 'Rated 5.0', 'Rated 3.0']\n        #checking if rating is good then to proceed for review.\n        if a in good_rating:\n            #perform operation on review data to refine it\n            refine = b.lower().replace(\"\\n\", '').replace('.', ' ').replace('!', '').replace(\",\", '').replace('\\n', '')\n            #check for each dish from dish list \n            for dish in dish_unique:\n                #if that dish is available in refine and not already added then\n                if dish in refine:\n                    if dish not in liked:\n                        #add that list as dish liked.\n                        liked.append(dish)\n    #if liked list have atleast one dish then return it as string separated by comma                    \n    if len(liked) > 0:\n        return ', '.join(liked)\n    #else return Nan value.\n    else:\n        return np.nan\n            ","6de409ac":"%%time \ndata['new_liked'] = data['reviews_list'].apply(lambda x: liked_dish_selector(x))\n#used to check the time for this execution.\n#applying the funnction above and finding out the new_liked dishes.","457268a8":"data[['dish_liked','new_liked']].sample(10) #checking out some samples for both parameter.","e931984b":"data['new_liked'].isna().sum()  #new_liked too dont have values for all entries.","436e9da1":"#for all the location of dish liked where we have NaN, fill it with values from new_liked data\nfor i in data['dish_liked'].isna().index:\n    data.loc[i, 'dish_liked'] = data.loc[i, 'new_liked']","a51711d6":"data['dish_liked'].isna().sum() #still we are unable to fill all values and thus still have NaN values","b292684a":"data['new_liked']","246db5d2":"data['dish_liked']","1e57e651":"data.isna().sum()","76707564":"data.drop(['new_liked'], axis=1, inplace=True)","ff2b49ad":"data.shape","76853725":"* Now we have filled the missing values from new_rating\n* But some places of new_rating might also be missing where rate is missing.\n* thus still we might have some missing values in rate feature.","1770d21d":"* Let us work on cuisines to be worked on\n* We have to work on rest_type, dish_liked, cuisines, and approx_cost feature.","93d41bac":"# Data Visualisations\n","53f94718":"* ast evaluation makes a good understanding about review data.\n* Now to get a rating for missing values we will extract rating from these reviews data for those\n* restaurant and will make a average based on the reviews taken and will update \n* these values in place of missing values","33f2e478":"# 3. Intial Data Observation","234286fe":"* Now you can checkout that spacing thing is tackled and we are left with NEW, nan and - values\n* For that, we have data in review_list means rating as well, we will use that.","f05ba332":"Here is a proper understanding gain from dataset.\n\n* 'url' : listing url of restaurant on Zomato\n* 'address': address of restaurant\n* 'name' : name of the restaurant\n* 'online_order' : yes\/ no values \n* 'book_table' : yes \/ no values\n* 'rate' : overall rating for restaurant\n* 'votes' : total ratings for restaurant\n* 'phone' : phone number of restaurant\n* 'location' : location of the neighbourhood region\n*  'rest_type' : type of the restaurant\n* 'dish_liked' : dish people liked for that restaurant\n*  'cuisines'  : food styles, separated by comma\n* 'approx_cost(for two people)': cost for two people mentioned\n* 'reviews_list' : list of tuple containing review for restaurant\n* 'menu_item' : menu item list separated by comma\n* 'listed_in(type)' : type of meal\n*  listed_in(city) : contains the neighbourhood city in which  it is listed.\n\n\n","edba6f23":"* Now new_rating columns is created with the average values of rating from review\n* and now we can use this rating to fill missing values in rate column.","ddfb242c":"* observation to be taken for memory usages of around 7 MB and datatype of columns data, missing values are there for rate, phone, location, dish_liked, and few more.","ff3f2001":"* Now here we can observe that the adress , listed in city and location are representing the same thing and thus we are removing the non useful thing which is here : - listedincity and address.\n* Also we will have no use of url and phone no so that can be removed as well.","83593ec6":"* Now left with dish_liked column as missing with 15269 values.","6500ae47":"* Around 48 percent values of dish liked are missing\n* Around 10 percent of rating values are missing\n* 0.60 percent in approx_cost\n* and 0.41 percent in rest_type feature","b29a73b1":"* duplicated rows are removed and now we are left with 41908 rows and 13 columns","99b5a75f":"* yes, now we just have to take care of nan values in rate feature","97961135":"# 1. Import the required libraries.","8156cb97":"* Now we have replaced the string \"\/5\" from the rate feature values \n* and now our rate column is also in float values.\n* we will use values from new_rating columns to fill the missing values in rate feature.","3c21dd7b":"* Observe that review data is a list for a restaurant containing a tuple for each review.","84441292":"* we find the rating have spaces, NEW and some other issues to be taken care of","f3619d07":"> > ","2b63351f":"* Now here you can observe that the listings are duplicates means they are only four.\n","67cc14ef":"* most frequent is quick bites, filling the missing values with that.","97975fa3":"*We have to consider this missing value features as very serious and to be taken care one by one*","7aa63387":"> 1. How many Restaurants are taking online orders ?","429ca9c7":"* so total we have 5914 values which are missing\n* let see how much can be filled from new_rating feature","df6abe16":"* 5914 values are missing, that big, we will take care from review data refining","d9d9a33c":"* We have now 37047 rows and 14 columns\n* We can remove the new_rating feature as not required now","4d409505":"* Thus still we have 4861 missing values, means we have filled (5914-4861) , i.e 1053 values.\n* great job but not that great.\n* Now we have to drop the values which are missing.","f2c1b2e9":"# 4. Data Pre-Processing","06893eac":"**Project** (**Problem Statement**): Bangalore, being an IT hub for India, having no time to cook the food, working classes are basically depend on the food supplied by restaurants. There are around 15000 restaurants serving Bangalore but still the demand is rising up and new restaurants are opening day by day. Food covering almost every cuisine is served here and it is being difficult for new resturants to compete with the established restaurants.\n\n\n**Objective**: To get a Exploratory Data Analysis and some good insights from the dataset covered using questions and answers to queries raising analysing the data. \n\n**Dataset Used** : https:\/\/www.kaggle.com\/himanshupoddar\/zomato-bangalore-restaurants\n\n**Tools Requirememnts**: kaggle, python, numpy, pandas, matplotlib, seaborn, scikit-learn, machinelearning algorithms.\n\n**CURRENT STATUS** : WORKING ON IT FOR UNDERSTANDABLE EDA\n","81864660":"* Now here observe -, NEW,nan,and spacing rating are different from without spacing.\n* we need to take care of these things","bf87b05b":"* Here we have some places where both are NaN and we will not be able to fill those values still.","01e9dca0":"# 2. Load the Dataset","7a00cbbc":"* To fill the dish liked we can take help from the reviews and comparing it with the dish liked list.","0733ac50":"* A total of 9809 entries are duplicated in our dataset, thus these should be removed"}}