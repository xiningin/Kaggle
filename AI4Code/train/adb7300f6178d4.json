{"cell_type":{"a76043a8":"code","39cd1d2f":"code","dabcaca7":"code","87dad531":"code","8ae96006":"code","248191a6":"code","df247677":"code","75145f01":"code","b97a3851":"code","9de7f7c8":"code","f1c2d938":"code","a9aee0ce":"code","7fb0b26b":"code","979e5b4b":"code","a79b460b":"code","23107550":"code","9127e143":"code","9885bbf8":"code","8f1f2369":"code","acc35b95":"code","bcbbd908":"code","73855177":"code","de2b259a":"code","9b1fd416":"code","c8850117":"code","489bcd79":"code","175a3f1c":"code","f2181836":"code","90edb4e0":"code","f497fefa":"code","0eeb9212":"code","8453ba7f":"code","a32a3299":"code","8974c681":"code","5d72b1d1":"code","39a7a67b":"code","faa0b9cd":"code","ec56481d":"code","14da3c0d":"code","025fcb13":"code","9265a6f0":"code","c619b3a5":"code","8f9f085c":"code","8f792668":"code","35044fcc":"code","262b4325":"code","a1c1e4ad":"code","9b6f7bc4":"code","0ec80d5a":"code","3e938094":"code","4a9391e6":"code","30d3bf29":"code","a7d566ef":"code","6f52e1af":"code","8ec210ad":"code","f40d6186":"code","a938ad60":"code","540302d3":"code","f0837818":"code","2b96b3b7":"code","bbfaeb40":"code","1765b97e":"code","f3f9e0c4":"code","37186bab":"code","2a83ff30":"code","17160859":"markdown","f41071bd":"markdown","5ab72035":"markdown","454da694":"markdown","c2510534":"markdown","50e3b25b":"markdown","6bc05f72":"markdown","a1e54159":"markdown","5c70b284":"markdown","1e004142":"markdown","e935baa4":"markdown","fa5d45cc":"markdown","b768dd04":"markdown","6cde2eef":"markdown","23f14a40":"markdown","38032113":"markdown","e207109a":"markdown","48eccd7c":"markdown","4b6d0085":"markdown"},"source":{"a76043a8":"import keras\nimport tensorflow as tf\nimport keras.layers as layers\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import array_to_img\nimport numpy as np\nfrom math import floor, ceil\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow import math, random, shape\nimport os\nfrom keras.losses import MeanSquaredError, BinaryCrossentropy\nfrom keras.optimizers import Nadam, SGD, Adam, Adamax\nfrom keras.activations import sigmoid\nfrom tensorflow import convert_to_tensor as tens\nfrom keras import backend as K\nfrom cv2 import getGaborKernel as Gabor\nfrom functools import reduce\nfrom matplotlib import pyplot as plt\nfrom math import sqrt\nimport itertools\nimport re\nfrom random import shuffle, seed\nfrom tensorflow.keras.utils import Sequence\nfrom keras.constraints import NonNeg\nfrom keras.regularizers import l1,l2,l1_l2\nfrom keras.initializers import RandomNormal\nimport pickle","39cd1d2f":"N_EXC = 639\nBATCH_SIZE = 16\nRESOLUTION = 8\nEXCITATORY_SYNAPSES_WANTED = 8\nINHIBITORY_SYNAPSES_WANTED = 6","dabcaca7":"directory = '\/kaggle\/input\/cat-and-dog\/'\nLABELS = ['Cat', 'Dog']\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    directory+'training_set\/training_set',\n    labels='inferred',\n    color_mode='grayscale',\n    validation_split=0.2,\n    subset=\"training\",\n    seed=1337,\n    batch_size=BATCH_SIZE,\n)\nvalid_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    directory+'training_set\/training_set',\n    labels='inferred',\n    color_mode='grayscale',\n    validation_split=0.2,\n    subset=\"validation\",\n    seed=1337,\n    batch_size=BATCH_SIZE,\n)\n\ntest_ds = keras.preprocessing.image_dataset_from_directory(directory+'test_set\/test_set', labels='inferred', color_mode='grayscale', batch_size=BATCH_SIZE)","87dad531":"plt.figure(figsize=(10, 10))\nfor i, (images, labels) in enumerate(train_ds.take(9)):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(images[0].numpy()[:,:,0], cmap='gray')\n    plt.title(LABELS[int(labels[0])])\n    plt.axis(\"off\")","8ae96006":"class Module:\n    def __init__(self, path, module_name, name):\n        self.name = name\n        self.module = self.create_module(path, module_name)\n        self.input = self.module.input\n    \n    def __call__(self, inp):\n        return self.module(inp)\n    \n    def create_module(self, path, module_name):\n        models_folder  = os.path.join(path, 'Models')\n        model_filename  = os.path.join(models_folder, module_name)\n        old_model = keras.models.load_model(model_filename)\n        inp = keras.Input(shape=old_model.layers[0].input.shape[1:])\n        x = old_model.layers[1](inp)\n        for layer in old_model.layers[2:-2-1]:\n            x = layer(x)\n        module = keras.Model(inp, [layer(x) for layer in old_model.layers[-3:-1]])\n#         L5PC_model.compile(optimizer=Nadam(lr=0.0001), loss='binary_crossentropy', loss_weights=[1.])\n\n        for layer in module.layers:\n            layer.trainable = False\n\n        module.summary()\n        return module  ","248191a6":"multiple_neurons = False\ndataset_folder = '..\/input\/single-neurons-as-deep-nets-nmda-test-data'","df247677":"if multiple_neurons:\n    neurons = [Module(dataset_folder, \"NMDA_TCN__DWT_8_224_217__model.h5\", \"Module_8_layers\"), \n           Module(dataset_folder, \"NMDA_TCN__DWT_7_292_169__model.h5\", \"Module_7_layers\"), \n           Module(dataset_folder, \"NMDA_TCN__DWT_9_256_241__model.h5\", \"Module_9_layers\")]\nelse:\n    neurons = Module(dataset_folder, \"NMDA_TCN__DWT_7_128_153__model.h5\", \"module_7_layers\")  \n#     neurons = Module(dataset_folder, \"NMDA_TCN__DWT_9_256_241__model.h5\", \"Module_9_layers\")","75145f01":"class ToBoolLayer(keras.layers.Layer):\n  def __init__(self, threshold=0.5, mult=15, use_sigmoid=True, use_special_sigmoid=None, name=\"ToBoolLayer\"):\n      super().__init__(name=name)\n      self.use_sigmoid = use_sigmoid\n      self.special_sigmoid = use_special_sigmoid\n      self.threshold = threshold\n      self.mult = mult\n\n  def build(self, shape):\n    if self.special_sigmoid is not None:\n      if not len(self.special_sigmoid) == 2:\n        raise Exception(\"Special sigmoid should be in format (pos_mult, neg_mult).\")\n      else:\n        self.sigmoid = SigmoidThreshold(*self.special_sigmoid, self.threshold)\n    elif self.use_sigmoid:\n      self.sigmoid = SigmoidThresholdEasier(threshold=self.threshold, mult=self.mult)\n    \n  def call(self, inputs, training=None):\n    if training is False:\n      inputs = inputs - self.threshold\n      inputs = math_ops.ceil(inputs)\n    elif self.use_sigmoid:\n      inputs = self.sigmoid(inputs)\n    return inputs","b97a3851":"def SigmoidThresholdEasier(mult=1, threshold=0.5):\n  \"\"\"returns a sigmoid function [0,1]->[0,1] with the center at threshold given, and slope multified\"\"\"\n  def sigmoid_threshold(x):\n      return 1\/(1+math.exp(mult*(threshold-x)))\n  return sigmoid_threshold","9de7f7c8":"x = np.arange(0,1,0.01)\nthreshold = 0.9\nmult = 50\nplt.plot(x, SigmoidThresholdEasier(mult, threshold)(x))\nplt.ylim(0,1)\nplt.xlim(0,1)\nplt.axhline(0.5, color='r', linestyle='--')\nplt.axvline(threshold, color='g', linestyle='--')\nplt.show()","f1c2d938":"class ToNeuronInput(keras.layers.Layer):\n    \"\"\"flattens the temporal dimensions, orders by new_order and pads with 0's to fill\"\"\"\n    def __init__(self, full, padding=0, new_order=None, name=\"NeuronInput\", part=200, batch_size=BATCH_SIZE):\n        super().__init__(name=name)\n        self.full = full\n        self.new_order = None\n        self.zero_padding = isinstance(padding, int) and padding == 0\n        self.padding = padding\n        self.part = part\n        self.batch_size = batch_size\n    \n    def build(self, shape):\n        self.ms = shape[1]*shape[2]\n        self.times = self.part \/\/ self.ms\n        self.padding_amount = self.full-self.ms*self.times\n    \n    def call(self, inputs, padding):\n                \n        new_inp = layers.Reshape((self.ms, inputs.shape[-1]))(inputs)\n        if self.new_order is not None:\n            new_inp = gather(new_inp, self.new_order, axis=-2)\n        if self.times > 1:\n            new_inp = layers.Concatenate(axis=1)([new_inp for _ in range(self.times)])\n        if self.zero_padding and False:\n            new_inp = layers.ZeroPadding1D(padding=(self.full - self.ms*self.times, 0))(new_inp)\n        else:\n            padding = tf.reshape(padding, (padding.shape[0]*padding.shape[1]*padding.shape[2], padding.shape[-1]))\n            starting_time = layers.Lambda(lambda x: self.ranInt(x))(padding)\n            padding = padding[np.newaxis, starting_time:starting_time+self.padding_amount]\n            new_inp = layers.Concatenate(axis=-2)([tf.tile(padding, [tf.shape(new_inp)[0], 1, 1]), new_inp])\n        return new_inp\n\n    def ranInt(self, x):\n        return K.random_uniform((1,), 0, 1024-self.padding_amount, dtype=tf.int32)[0]#.numpy()\n    \n    @tf.function\n    def gather(x, ind):\n        return tf.gather(x+0, ind)","a9aee0ce":"class SpikeProcessor(keras.layers.Layer):\n    def __init__(self, nSynapses, start=None, end=None, name=\"SpikeProcessor\"):\n        super().__init__(name=name)\n        self.nSynapses = nSynapses\n        self.start = start\n        self.end = end\n\n    def build(self, shape):\n        self.dims = len(shape)\n        pass\n    \n    def call(self, inputs):\n        if self.start is not None:\n            if self.end is not None:\n                inputs = inputs[:,:,:,self.start:self.end] if self.dims==4 else inputs[:,:,self.start:self.end]\n            else:\n                inputs = inputs[:,:,:,self.start:] if self.dims==4 else inputs[:,:,self.start:]\n        elif self.end is not None:\n            inputs = inputs[:,:,:,:self.end] if self.dims==4 else inputs[:,:,:self.end]\n        preds_sum = math.reduce_sum(inputs, axis=-1, keepdims=True)\n        return_value = preds_sum\/self.nSynapses\n        return layers.Flatten()(return_value)","7fb0b26b":"def MeanSquaredErrorSynapsesPerMS(batch_size=32):\n  def mean_squared_error_synapses_per_ms(y_true, y_preds):\n    squared_difference = tf.square(1-y_preds)\n    mean = tf.reduce_mean(squared_difference, axis=-1)\n    return mean\n  return mean_squared_error_synapses_per_ms","979e5b4b":"max_acceptable_spikes_per_ms = 3.0\nmax_acceptable_spikes_deviation = 20.0\nactivity_reg_constant = 0.2 * 0.0028\n\n\ndef pre_synaptic_spike_regularization(activation_map):\n    # sum over all dendritic locations\n    x = K.sum(activation_map, axis=2)\n\n    # ask if it's above 'max_acceptable_spikes_per_ms'\n    x = K.relu(x - max_acceptable_spikes_per_ms)\n\n    # if above threshold, apply quadratic penelty\n    x = K.square(x \/ max_acceptable_spikes_deviation)\n\n    # average everything\n    x = activity_reg_constant * K.mean(x)\n\n    return x","a79b460b":"def MSE_RMS_SynapsesPerMS(wanted, size=N_EXC, batch_size=32, eps=1e-3):\n    def mse_rms(X, mu):\n        return tf.math.reduce_mean(tf.math.reduce_sum(tf.square(tf.math.sqrt(tf.math.reduce_mean(tf.square(X+eps), axis=-1)) - mu), axis=-1))\n    real_wanted = np.sqrt(wanted \/ size)\n    zeros_wanted = np.sqrt(1. - wanted \/ size)\n    def mse_rms_synapses_per_ms(y_true, y_preds):\n        result = tf.math.sqrt((mse_rms(y_preds, real_wanted)**2 + mse_rms(1. - y_preds, zeros_wanted)**2) \/ 2)\n        return result\n    return mse_rms_synapses_per_ms","23107550":"class MeanSynapsesPerMsMetric:\n    def __init__(self, name=\"nSynapses\"):\n        self.name=name\n    def __call__(self, y_true, y_pred):\n        booleans = tf.cast(tf.math.greater(y_pred, 0.5), tf.float32)\n        summed = tf.math.reduce_sum(booleans, axis=-1)\n        mean_by_ms = tf.math.reduce_mean(summed, axis=-1)\n        meaned_batches = tf.math.reduce_mean(mean_by_ms, axis=-1)\n        return meaned_batches","9127e143":"n_filters = 64\nn_orientations = 4\n\nksizes = [(i, i) for i in range(7,38, 2)]\nthetas = [0 , (45 \/ 180) * np.pi, (90 \/ 180) * np.pi, (135 \/ 180) * np.pi]\ngammas = [0.3] * 16\nsigmas = [2.8, 3.6, 4.5, 5.4, 6.3, 7.3, 8.2, 9.2, 10.2, 11.3, 12.3, 13.4, 14.6, 15.8, 17., 18.2]\nlambdas = [3.5, 4.6, 5.6, 6.8, 7.9, 9.1, 10.3, 11.5, 12.7, 14.1, 15.4, 16.8, 18.2, 19.7, 21.2, 22.8]\n\nall_filters = [[(size, sigma, theta, lambd, gamma) for theta in thetas] \n               for size, gamma, sigma, lambd in zip(ksizes, gammas, sigmas, lambdas)]\nall_filters = reduce(lambda x,y: x+y, all_filters, [])\nreoredering_inds = [ 0,0+4,   1, 1+4,  2, 2+4,  3, 3+4,\n                     8,8+4,   9, 9+4, 10,10+4, 11,11+4,\n                    16,16+4, 17,17+4, 18,18+4, 19,19+4,\n                    24,24+4, 25,25+4, 26,26+4, 27,27+4,\n                    32,32+4, 33,33+4, 34,34+4, 35,35+4,\n                    40,40+4, 41,41+4, 42,42+4, 43,43+4,\n                    48,48+4, 49,49+4, 50,50+4, 51,51+4,\n                    56,56+4, 57,57+4, 58,58+4, 59,59+4]\n\nall_filters_reordered = [all_filters[k] for k in reoredering_inds]","9885bbf8":"class GaborInitializer(tf.keras.initializers.Initializer):\n    def __init__(self, size, sigma, theta, lambd, gamma):\n        self.ksize = size\n        self.sigma = sigma\n        self.theta = theta\n        self.lambd = lambd\n        self.gamma = gamma\n\n    def __call__(self, dtype=None):\n        return tens(Gabor(self.ksize, self.sigma, self.theta, self.lambd, self.gamma))\n\n    def get_config(self):  # To support serialization\n        return {'ksize': self.ksize, 'sigma': self.sigma, 'theta': self.theta, 'lambda': self.lambd, 'gamma': self.gamma}","8f1f2369":"max_filter_shape = all_filters_reordered[-1][0]\ncenter_pixel_ind = int((max_filter_shape[0] - 1 ) \/ 2)\n\n# place to store all filter activations\nfilters_matrix = np.zeros((max_filter_shape[0], max_filter_shape[1], len(all_filters)))\n\nplt.figure(figsize=(18,22))\nplt.subplots_adjust(left=0.04, right=0.96, bottom=0.04, top=0.96, hspace=0.25, wspace=0.1)\nfor i, filt in enumerate(all_filters_reordered):\n    filter_size  = filt[0][0]\n    oritentation = (filt[2] \/ np.pi ) * 180\n    curr_sigma   = filt[1]\n    curr_lambda  = filt[3]\n    \n    half_filter_size = int((filter_size -1 ) \/ 2)\n    upper_left_start = center_pixel_ind - half_filter_size\n    \n    curr_small_filter = GaborInitializer(*filt)().numpy()\n    curr_full_filter = np.zeros((max_filter_shape))\n    curr_full_filter[upper_left_start:upper_left_start + filter_size, upper_left_start:upper_left_start + filter_size] = curr_small_filter\n    \n    # store the filter and the activations for later\n    filters_matrix[:,:,i] = curr_full_filter\n    \n    plt.subplot(8,8,i+1);\n    plt.title('%dx%d, $\\Theta=%d$, \\n$\\sigma=%.1f$, $\\lambda=%.1f$' %(filter_size, filter_size, oritentation, curr_sigma, curr_lambda))\n    plt.imshow(curr_full_filter, cmap='gray')\n    plt.axis(\"off\")","acc35b95":"def simple_cells_module(filters_matrix, strides=(1,1), input_shape=(256,256)):\n    # input is a single channel gray scale image\n    input_tensor = keras.Input(shape=(input_shape[0],input_shape[1],1))\n    \n    # initializer with predefined weights\n    def gabor_filters_init(shape, dtype=None):\n        return -filters_matrix[:,:,np.newaxis,:]\n\n    num_filters = filters_matrix.shape[2]\n    kernel_size = (filters_matrix.shape[0], filters_matrix.shape[1])\n\n    # single conv2d layer with all weights\n    conv_2d_layer = layers.Conv2D(num_filters, kernel_size, strides=strides, kernel_initializer=gabor_filters_init, padding='same')\n    conv_2d_layer.trainable=False\n    simple_cell_activations = conv_2d_layer(input_tensor)\n    simple_cell_module = keras.Model(input_tensor, simple_cell_activations)\n    \n    return simple_cell_module","bcbbd908":"max_pooling_size = list(range(8,23,2))\npooling_size_list = [[(s,s,2)] for s in max_pooling_size]\nnum_orientations = 4","73855177":"def pre_process_module(filter_matrix, pooling_size_list, strides=(1,1), num_orientations=4, input_shape=(256,256)):\n    \n    print('num gabor filters must be %d' %(sum([x[0][-1] for x in pooling_size_list]) * num_orientations))\n\n    # input is a single channel gray scale image\n    input_tensor = keras.Input(shape=(input_shape[0],input_shape[1],1))\n    \n    # calc simple cell activations\n    simple_cells_activations = K.expand_dims(simple_cells_module(filters_matrix, strides=(1,1), input_shape=(256,256))(input_tensor), axis=-1)\n    \n    # add support for subsampling\n    strides_to_use = (strides[0], strides[1], 2)\n\n    # apply max pooling\n    maxpool_3d_layers = []\n    for k, pool_size in enumerate(pooling_size_list):\n        start_ind = num_orientations * pool_size[0][-1] * k\n        end_ind   = num_orientations * pool_size[0][-1] * (k + 1)\n        curr_pool_input_slice  = simple_cells_activations[:,:,:,start_ind:end_ind]\n        curr_pool_output_slice = layers.MaxPooling3D(pool_size=pool_size[0], strides=strides_to_use, padding='same')(curr_pool_input_slice)\n        maxpool_3d_layers.append(curr_pool_output_slice)\n    \n    # squeeze the last dimention\n    concatenated_pooled_layers = K.squeeze(layers.Concatenate(axis=-2)(maxpool_3d_layers), axis=-1)\n    \n    # wrap as module and return\n    complex_cell_module = keras.Model(input_tensor, concatenated_pooled_layers)\n    \n    return complex_cell_module","de2b259a":"def preprocessor(filters_matrix, pooling_sizes, n_orientations=4, conv_strides=(1,1), max_pooling_strides=(1,1), input_shape=(256,256)):\n    return pre_process_module(filters_matrix, pooling_sizes, strides=max_pooling_strides, num_orientations=n_orientations, input_shape=input_shape)\n","9b1fd416":"processor = preprocessor(filters_matrix, pooling_size_list, max_pooling_strides=(RESOLUTION,RESOLUTION))","c8850117":"plt.figure(figsize=(10, 10))\nfor _, (image, label) in enumerate(train_ds.take(1)):\n    image = image[0].numpy()[:,:,0]\n    label = int(label[0])\n    processed = processor(image[np.newaxis,:,:,np.newaxis]).numpy()[0]\n    for i in range(32):\n        ax = plt.subplot(8, 4, i+1)\n        plt.imshow(processed[:,:,i], cmap='gray')\n        plt.title(LABELS[label])\n        plt.axis(\"off\")","489bcd79":"plt.figure(figsize=(5,20))\ndct = {\"Dog\": [], \"Cat\": []}\nfor _, (image, label) in enumerate(train_ds.take(5)):\n    for img, lbl in zip(image, label):\n        dct[LABELS[lbl]].append(img[np.newaxis,])\n\n# print(f\"Dog images: {len(dct[\"Dog\"])}\\n Cat images: {len(dct[\"Cat\"])}\")\n\nall_images = np.concatenate(dct[\"Dog\"] + dct[\"Cat\"], axis=0)\nplt.subplot(4,1,1)\nplt.title(\"Original Image\")\nplt.imshow(all_images[0,:,:,0], cmap=\"gray\")\n\nblocksize = 64\n\n# Create blocks\nshuffled_images = all_images.copy()\nfor j in range(0, all_images.shape[2], blocksize):\n    for i in range(0, all_images.shape[1], blocksize):\n        indxs = np.random.permutation(all_images.shape[0]).tolist()\n        for orig,new in zip(indxs, range(all_images.shape[0])):\n            shuffled_images[orig,i:i+blocksize, j:j+blocksize] = all_images[new, i:i+blocksize, j:j+blocksize]\n\nplt.subplot(4,1,2)\nplt.title(\"Hybrid Image\")\nplt.imshow(shuffled_images[0,:,:,0], cmap=\"gray\")\nprocessed_images = processor(shuffled_images)\n\nplt.subplot(4,1,3)\nplt.title(\"Processed Hybrid Image (Channel 0)\")\nplt.imshow(processed_images[0,:,:,0], cmap=\"gray\")\n\nimage_list = [processed_images[i] for i in range(processed_images.shape[0])]\nPADDING = np.concatenate(image_list, axis=-2)[np.newaxis]\n\nplt.subplot(4,1,4)\nplt.title(\"Concatenated Processed Hybrid Image (Channel 0)\")\nplt.imshow(PADDING[0,:,:,0], cmap=\"gray\")\n\nplt.show()","175a3f1c":"data_augmentation = tf.keras.Sequential([\n  layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n  layers.experimental.preprocessing.RandomRotation(0.1),\n])","f2181836":"plt.figure(figsize=(30, 10))\nfor _, (images, labels) in enumerate(train_ds.take(1)):\n    augmented = data_augmentation(images)\n    for i in range(9):\n        plt.subplot(2, 9, i + 1)\n        plt.imshow(images[i,:,:,0], cmap='gray')\n        plt.title(LABELS[int(labels[i])])\n        plt.axis(\"off\")\n        \n        plt.subplot(2, 9, 9 + i + 1)\n        plt.imshow(augmented[i,:,:,0], cmap='gray')\n        plt.title(\"Augmented \" + LABELS[int(labels[i])])\n        plt.axis(\"off\")","90edb4e0":"def padding_module(use_sigmoid=True, pruning=True, dropout1=False, dropout2=False, \n                  sigmoid_threshold=0.9, sigmoid_mult=50, to_bool=True, \n                  padding=PADDING):\n    \n    def printModule():\n        print(\"~*~ Visual Module ~*~\")\n        print(f\"Image Dropout Rate: {dropout1}\")\n        print(f\"Synapse Threshold: {sigmoid_threshold}\")\n        print(f\"Synapse Training Sigmoid Multiplication: {sigmoid_mult}\")\n        print(f\"Synapses Dropout Rate: {dropout2}\")\n    \n    \n    printModule()\n    \n    inp = keras.Input(shape=(256,256,1))\n    \n    padding = tf.Variable(lambda: padding, trainable=False)\n    x = inp\n    x = processor(x)\n    if dropout1: x = layers.Dropout(dropout1)(x)\n    conv_shape = (x.shape[-3], 1)\n    x = Pad(padding)(x)\n    x = layers.Conv2D(1278, conv_shape, strides=conv_shape, activity_regularizer=pre_synaptic_spike_regularization, name=\"WiringLayer\")(x)\n    x = layers.BatchNormalization(name=\"BatchNorm\")(x)\n    x = layers.Activation(sigmoid)(x)\n    x = ToBoolLayer(threshold=sigmoid_threshold, use_sigmoid=use_sigmoid, mult=sigmoid_mult, name='preNeuronBool')(x)\n    \n    x = K.squeeze(x, axis=-3)\n\n    if dropout2: x = layers.Dropout(dropout2)(x)\n    x, v = L5PC_model(x)\n    x = x[:, -198:, :]\n#     x = L5PC_model(x)[:,-198:,:]   # run through david's model, take only the post-noise time (32 ms X 6 times)\n    x = layers.MaxPooling1D(x.shape[-2], strides=x.shape[-2], name=\"MaxPooling\")(x)\n    output = layers.Flatten(name=\"nSpikes\")(x)\n\n    model = keras.Model(inp, [output, v])\n    model.compile(optimizer=optimizer, loss={'nSpikes': MeanSquaredError()}, metrics={'nSpikes': keras.metrics.BinaryAccuracy()})\n    return model","f497fefa":"def printModule(dropout1, dropout2, sigmoid_threshold, sigmoid_mult, threshold, exWanted, inWanted, qSynapse, augment, synLoss):\n    print(\"~*~ Visual Module ~*~\")\n    if augment: print(f\"Images are augmented (rotated by {augment})\")\n    print(f\"Image Dropout Rate: {dropout1}\")\n    print(f\"Synapse Threshold: {sigmoid_threshold}\")\n    print(f\"Synapse Training Sigmoid Multiplication: {sigmoid_mult}\")\n    print(f\"Synapses Wanted: Exc-{exWanted}; Inh-{inWanted}; Sum-{exWanted+inWanted}\")\n    print(f\"Synapses Loss Rate: Exc-{qSynapse[0]}; Inh-{qSynapse[1]}\")\n    print(f\"Synapses Dropout Rate: {dropout2}\")\n    print(f\"Threshold: {threshold}\")\n    print(f\"Synapse Loss Function: {synLoss.__name__}\")","0eeb9212":"class Pad(keras.layers.Layer):\n    \"\"\"flattens the temporal dimensions, order by new_order and pads with 0's to fill\"\"\"\n    def __init__(self, padding, full=400, times=6, reverse=True, new_order=None, name=\"PadLayer\"):\n        super().__init__(name=name)\n        self.full = full\n        self.padding = padding\n        self.times = times\n        self.reverse = reverse\n        self.new_order = new_order\n        self.shape = None\n        self.pad_to_add = None\n    \n    def build(self, shape):\n        self.shape = shape\n        self.pad_to_add = self.full - self.shape[-2]*self.times\n    \n    def call(self, inputs):\n        if self.new_order is not None:\n            inputs = self.gather(inputs, self.new_order, axis=-2)\n        if self.times > 1:\n            if self.reverse: new_inp = layers.Concatenate(axis=-2)([inputs, K.reverse(inputs,axes=-2)] * (self.times \/\/ 2) + [inputs] * (self.times % 2))\n            else: new_inp = layers.Concatenate(axis=-2)([inputs] * self.times)\n        else: new_inp = inputs\n        starting_time = layers.Lambda(lambda x: self.ranInt(x))(self.padding)\n        padding = self.padding[:,:, starting_time:starting_time+self.pad_to_add]\n        new_inp = layers.Concatenate(axis=-2)([tf.tile(padding, [tf.shape(new_inp)[0], 1, 1, 1]), new_inp])\n        new_inp = K.reshape(new_inp, (tf.shape(new_inp)[0], self.shape[-3], self.shape[-2]*self.times + self.pad_to_add, self.shape[-1]))\n        return new_inp\n\n    def ranInt(self, x):\n        return K.random_uniform((1,), 0, self.padding.shape[-2]-self.pad_to_add, dtype=tf.dtypes.int32)[0]#.numpy()\n    \n    @tf.function\n    def gather(self, x, ind, axis):\n        return tf.gather(x+0, ind, axis)","8453ba7f":"class SliceLayer(tf.keras.layers.Layer):\n    def __init__(self, start=None, end=None, name=\"SliceLayer\"):\n        super().__init__(name=name)\n        self.start = start\n        self.end = end\n\n    def build(self, shape):\n        self.dims = len(shape)\n        pass\n    \n    def call(self, inputs):\n        if self.start is not None:\n            if self.end is not None:\n                return inputs[:,:,:,self.start:self.end] if self.dims==4 else inputs[:,:,self.start:self.end]\n            else:\n                return inputs[:,:,:,self.start:] if self.dims==4 else inputs[:,:,self.start:]\n        elif self.end is not None:\n            return inputs[:,:,:,:self.end] if self.dims==4 else inputs[:,:,:self.end]","a32a3299":"def create_module(use_sigmoid=True, pruning=True, dropout1=False, dropout2=False, \n                  sigmoid_threshold=0.9, sigmoid_mult=50, to_bool=True, \n                  augment=True, optimizer=Nadam(),\n                  padding=PADDING):\n    \n    def printModule():\n        print(\"~*~ Visual Module ~*~\")\n        if augment: print(\"Images are augmented\")\n        print(f\"Image Dropout Rate: {dropout1}\")\n        print(f\"Synapse Threshold: {sigmoid_threshold}\")\n        print(f\"Synapse Training Sigmoid Multiplication: {sigmoid_mult}\")\n        print(f\"Synapses Dropout Rate: {dropout2}\")\n    \n    \n    printModule()\n    \n    inp = keras.Input(shape=(256,256,1))\n    \n    padding = tf.Variable(lambda: padding, trainable=False)\n    x = inp\n    if augment: x = data_augmentation(x)\n    x = processor(x)\n    if dropout1: x = layers.Dropout(dropout1)(x)\n    conv_shape = (x.shape[-3], 1)\n    x = Pad(padding)(x)\n    x = layers.Conv2D(1278, conv_shape, strides=conv_shape, activity_regularizer=pre_synaptic_spike_regularization, name=\"WiringLayer\")(x)\n    x = layers.BatchNormalization(name=\"BatchNorm\")(x)\n    x = layers.Activation(sigmoid)(x)\n    x = ToBoolLayer(threshold=sigmoid_threshold, use_sigmoid=use_sigmoid, mult=sigmoid_mult, name='preNeuronBool')(x)\n    \n    x = K.squeeze(x, axis=-3)\n\n    if dropout2: x = layers.Dropout(dropout2)(x)\n    x, v = L5PC_model(x)\n    x = x[:, -198:, :]\n#     x = L5PC_model(x)[:,-198:,:]   # run through david's model, take only the post-noise time (32 ms X 6 times)\n    x = layers.MaxPooling1D(x.shape[-2], strides=x.shape[-2], name=\"MaxPooling\")(x)\n    output = layers.Flatten(name=\"nSpikes\")(x)\n\n    model = keras.Model(inp, [output, v])\n    model.compile(optimizer=optimizer, loss={'nSpikes': MeanSquaredError()}, metrics={'nSpikes': keras.metrics.BinaryAccuracy()})\n    return model","8974c681":"def create_module_limited_syn(saccades=None, xaxis=True, use_sigmoid=True, pruning=True, dropout1=False, dropout2=False,\n                        sigmoid_threshold=0.9, sigmoid_mult=15, to_bool=True, \n                        excitatory_wanted=EXCITATORY_SYNAPSES_WANTED, inhibitory_wanted=INHIBITORY_SYNAPSES_WANTED,\n                        qSynapse=(0.1, 0.1), augment=False, \n                        optimizer=SGD(momentum=.9), conv_shape=(16,16), threshold=.1,\n                        padding=PADDING, synLoss=MeanSquaredErrorSynapsesPerMS, nSynapse=True, neurons=neurons, regular_sigmoid=True, non_neg=False):\n    printModule(dropout1, dropout2, sigmoid_threshold, sigmoid_mult, threshold, excitatory_wanted, inhibitory_wanted, qSynapse, augment, synLoss)\n    module_name = \"module_syn_\"\n    for i in [dropout1, dropout2, sigmoid_threshold, sigmoid_mult, threshold, excitatory_wanted, inhibitory_wanted, qSynapse, augment, synLoss]:\n        module_name += str(i)\n           \n    inp = keras.Input(shape=(256,256,1))\n    \n    padding = tf.Variable(lambda: padding, trainable=False)\n    x = inp\n    if augment: x = data_augmentation(augment)(x)\n    x = processor(x)\n    if dropout1: x = layers.Dropout(dropout1)(x)\n    conv_shape = (x.shape[-3], 1) if xaxis else conv_shape\n    x = Pad(padding, full=neurons.input.shape[-2])(x)\n    if non_neg: x = layers.Conv2D(2*N_EXC, conv_shape, strides=conv_shape, use_bias=True, kernel_constraint=keras.constraints.NonNeg(), name=\"WiringLayer\")(x)#, activity_regularizer=pre_synaptic_spike_regularization)(x)\n    else: x = layers.Conv2D(2*N_EXC, conv_shape, strides=conv_shape, use_bias=True, name=\"WiringLayer\")(x)#, activity_regularizer=pre_synaptic_spike_regularization)(x)\n    x = layers.BatchNormalization(name=\"BatchNorm\")(x)\n    if regular_sigmoid: x = layers.Activation(sigmoid)(x)\n    x = ToBoolLayer(threshold=sigmoid_threshold, use_sigmoid=use_sigmoid, mult=sigmoid_mult, name='preNeuronBool')(x)\n    \n    x = K.squeeze(x, axis=-3)\n        \n    ExcitatorySynapses = SliceLayer(end=N_EXC, name=\"ExcSyns\")(x)#[:, :, :N_EXC]  #SpikeProcessor(excitatory_wanted, name='nExcitatory', end=639)(x)\n    InhibitorySynapses = SliceLayer(start=N_EXC, name=\"InhSyns\")(x)#[:, :, N_EXC:]  #SpikeProcessor(inhibitory_wanted, name='nInhibitory', start=639)(x)\n\n    if dropout2: x = layers.Dropout(dropout2)(x)\n    if isinstance(neurons, list):\n        res = [neuron(x) for neuron in neurons]\n        ap, v = [r[0][:, -198:, :] for r in res], [r[1] for r in res]\n        max_layer = layers.MaxPooling1D(ap[0].shape[-2], strides=ap[0].shape[-2], name=\"MaxPooling\")\n        x = [max_layer(aps) for aps in ap]\n        bool_layer = ToBoolLayer(threshold=threshold, use_sigmoid=True, mult=1, name='postNeuronBool')\n        x = [bool_layer(xs) for xs in x]\n        flat_layer = layers.Flatten(name=\"nSpikes\")\n        output = [flat_layer(xs) for xs in x]\n        model = keras.Model(inp, [output, ap, v])\n        def loss(y_true, y_pred):\n            l = 0\n            mse = MeanSquaredError()\n            for x in lst:\n                l += mse(y_true, x)\n            return l \/ len(lst)\n        model.compile(optimizer=optimizer, loss={'nSpikes': loss}, \n                      metrics={\"nSpikes\": keras.metrics.BinaryAccuracy()})\n        return model, module_name\n        \n    ap, v = neurons(x)\n    x = ap[:,-198:,:]   # run through david's model, take only the post-noise time (32 ms X 6 times)\n    x = layers.MaxPooling1D(x.shape[-2], strides=x.shape[-2], name=\"MaxPooling\")(x)\n    x = ToBoolLayer(threshold=threshold, use_sigmoid=True, mult=1, name='postNeuronBool')(x)\n    output = layers.Flatten(name=\"nSpikes\")(x)\n    \n    model = keras.Model(inp, [output, ap, v])\n    if not nSynapse:\n        model.compile(optimizer=optimizer, loss={'nSpikes': MeanSquaredError()}, \n                      metrics={\"nSpikes\": keras.metrics.BinaryAccuracy()})\n    else:\n        model.compile(optimizer=optimizer, \n                  loss=[MeanSquaredError(), synLoss(excitatory_wanted), synLoss(inhibitory_wanted)], \n                  metrics={\"nSpikes\": keras.metrics.BinaryAccuracy(), \"ExcSyns\": MeanSynapsesPerMsMetric(), \"InhSyns\": MeanSynapsesPerMsMetric()},\n                  loss_weights=[1.0 -qSynapse[0] - qSynapse[1], qSynapse[0], qSynapse[1]])\n    return model, module_name","5d72b1d1":"TRAIN_MODULE = False","39a7a67b":"if TRAIN_MODULE: model = create_module()","faa0b9cd":"if TRAIN_MODULE: model.summary()","ec56481d":"if TRAIN_MODULE: history = model.fit(train_ds, epochs=200, validation_data=valid_ds)","14da3c0d":"if TRAIN_MODULE:\n    \n    hist = history.history\n\n    plt.figure(figsize=(20,10))\n    plt.subplot(1,2,1)\n    plt.title(\"Loss\")\n    plt.plot(hist[\"loss\"], color=\"k\", label=\"train\")\n    plt.plot(hist[\"val_loss\"], color=\"r\", label=\"validation\")\n    plt.axhline(0, color=\"b\", linestyle=\"--\")\n    plt.xlabel(\"epochs\")\n    plt.legend()\n\n    # accuracy = \"nSpikes_binary_accuracy\"\n    accuracy = \"binary_accuracy\"\n\n    plt.subplot(1,2,2)\n    plt.title(\"Accuracy\")\n    plt.plot(hist[accuracy], color=\"k\", label=f\"train (last: {str(round(hist[accuracy][-1], 2))})\")\n    plt.plot(hist[\"val_\"+accuracy], color=\"r\", label=f\"validation (last: {str(round(hist['val_'+accuracy][-1], 2))})\")\n    plt.axhline(.5, color=\"b\", linestyle=\"--\", label=\"chance\")\n    plt.ylim((0,1))\n    plt.xlabel(\"epochs\")\n    plt.legend()\n\n    plt.show()","025fcb13":"if TRAIN_MODULE: model.evaluate(test_ds)","9265a6f0":"def plot_examples(model, startFrom=200, plot_cycle=False, plot_spikes=True, plot_start=None, write_last=False, weights_start=None, preNeuron=\"NeuronInput\", spikeTrain=\"SpikeTrain\", postNeuron=\"nSpikes\"):\n    plt.figure(figsize=(200, 250))\n    how_many = 10\n    for img, label in train_ds.take(1):\n#         inputs = K.function(model.input, model.get_layer(bool_layers[0]).output)([img])\n        inputs = K.function(model.input, model.get_layer(preNeuron).output)([img])\n        outputs = K.function(model.input, model.get_layer(spikeTrain).output)([img])\n        if write_last: nAP = K.function(model.input, model.get_layer(postNeuron).output)([img])\n\n        for i in range(how_many):\n            plt.subplot(how_many, 3, 3*i+1)\n            plt.imshow(img[i,:,:,0]\/255, cmap='gray')\n            plt.title(LABELS[label[i]], fontdict={'fontsize':200})\n            plt.axis(\"off\")\n            plt.subplot(how_many, 3, 3*i+2)\n            if len(inputs[i].shape)==3:\n                curr_input = np.squeeze(inputs[i], axis=-3)[startFrom:]\n            else:\n                curr_input = inputs[i][startFrom:]\n            num_of_synapses = np.sum(curr_input, axis=-1)\n            mean_synapses = round(num_of_synapses.mean(), 2)\n            std_synapses = round(num_of_synapses.std(), 2)\n            plt.title(f\"\\u03BC: {str(mean_synapses)},  \\u03C3: {str(std_synapses)}\", fontdict={'fontsize':200})\n            plt.imshow(curr_input[-32:,:] if plot_cycle else curr_input, cmap='binary', vmin=0, vmax=1)\n            plt.axis(\"off\")\n            plt.subplot(how_many, 3, 3*i+3)\n            curr_output = outputs[i]            \n            spikes = []\n            curr_index = startFrom\n            while True:\n                start = np.where(curr_output[curr_index:] > 0.25)[0]\n                if not start.shape[0]: break\n                start = curr_index+start[0]\n                end = np.where(curr_output[start:] < 0.1)[0]\n                if not end.shape[0]: break\n                end = end[0] + start\n                spikes.append((start,end))\n                curr_index = end + 1\n            plt.title((f\"Score: {str(round(nAP[i][0],2))},\" if write_last else \"\") +f\"\\u03A3: {str(round(curr_output.sum(),2))}\" + (f\", spikes:{spikes}\" if plot_spikes else \"\"), fontdict={'fontsize':150})\n            plt.plot(curr_output, linewidth=5)\n            if weights_start is not None:\n                plt.plot(np.arange(weights_start, 400), model.get_layer(\"nSpikes\").get_weights()[0][:,0], color='r')\n            plt.ylim(0,1)\n            if plot_start: plt.axvline(plot_start, color='g', linestyle=':', linewidth=10.)\n            plt.axis('on')\n        break","c619b3a5":"def plot_statistics(model, starting_time=0, cycle_time=32, preNeuron=\"NeuronInput\", spikeTrain=\"SpikeTrain\"):\n    \n    all_inputs = []\n    n_synapses = []\n    synapses_mu = []\n    synapses_std = []\n    synapses_max = []\n    synapses_min = []\n#     sum_output = []\n    all_outputs = []\n#     excitatory_synapses = []\n#     inhibitory_synapses = []\n    label_dct = {}\n    synapses_mean_per_label = {}\n    \n    for img, label in train_ds.take(25):\n        inputs = K.function(model.input, model.get_layer(preNeuron).output)([img])\n        outputs = K.function(model.input, model.get_layer(spikeTrain).output)([img])\n        all_inputs.append(inputs)\n        labels = [LABELS[lbl] for lbl in label]\n        for j in range(len(inputs)):\n            if len(inputs[j].shape)==3:\n                curr_input = np.squeeze(inputs[j], axis=-3)\n            else:\n                curr_input = inputs[j]\n            \n            num_of_synapses = np.sum(curr_input[starting_time:starting_time+cycle_time, :N_EXC], axis=-1)\n#             excitatory_synapses.append(np.sum(curr_input[:, :N_EXC], axis=-1).mean())\n#             inhibitory_synapses.append(np.sum(curr_input[:, N_EXC:], axis=-1).mean())\n            n_synapses.append(num_of_synapses)\n            synapses_mu.append(num_of_synapses.mean())\n            synapses_std.append(num_of_synapses.std())\n            synapses_max.append(num_of_synapses.max())\n            synapses_min.append(num_of_synapses.min())\n            curr_output = outputs[j]\n#             sum_output.append(curr_output.sum())\n            all_outputs.append(curr_output)\n            if labels[j] not in label_dct:\n                label_dct[labels[j]] = []\n                synapses_mean_per_label[labels[j]] = {\"Sum\": [], \"Ex\":[], \"Inh\":[]}\n            label_dct[labels[j]].append(curr_output)\n            synapses_mean_per_label[labels[j]][\"Sum\"].append(np.sum(curr_input[starting_time:starting_time+cycle_time, :N_EXC], axis=-1).mean())\n            synapses_mean_per_label[labels[j]][\"Ex\"].append(np.sum(curr_input[starting_time:starting_time+cycle_time, :N_EXC], axis=-1).mean())\n            synapses_mean_per_label[labels[j]][\"Inh\"].append(np.sum(curr_input[starting_time:starting_time+cycle_time, N_EXC:], axis=-1).mean())\n\n    mean_dct = {}\n    for lbl in label_dct.keys():\n        mean_dct[lbl] = np.stack(label_dct[lbl]).mean(axis=0)\n\n    plt.figure(figsize=(15,10))\n\n    plt.subplot(3,3,1)\n    plt.title('input values')\n    plt.hist(np.array(all_inputs).ravel())\n\n    plt.subplot(3,3,2)\n    plt.title('mean of sum synapses')\n    plt.hist(synapses_mu)\n\n    plt.subplot(3,3,3)\n    plt.title('std of sum synapses')\n    plt.hist(synapses_std)\n\n    plt.subplot(3,3,4)\n    plt.title('max sum synapses')\n    plt.hist(synapses_max)\n\n    plt.subplot(3,3,5)\n    plt.title('min sum synapses')\n    plt.hist(synapses_min)\n\n    plt.subplot(3,3,6)\n    plt.title(f'mean output of neuron (AP)')\n    for lbl, value in mean_dct.items():\n        plt.plot(value, label=lbl, linewidth=2.)\n    plt.ylim(0,1)\n    plt.legend()\n\n    plt.subplot(3,3,7)\n    plt.title('Excitatory Synapses Mean per Label')\n    plt.hist([synapses_mean_per_label[lbl][\"Ex\"] for lbl in synapses_mean_per_label.keys()], label=list(synapses_mean_per_label.keys()))\n    plt.legend()\n    plt.xlim((0,15))\n\n\n    plt.subplot(3,3,8)\n    plt.title('Inhibitory Synapses Mean per Label')\n    plt.hist([synapses_mean_per_label[lbl][\"Inh\"] for lbl in synapses_mean_per_label.keys()], label=list(synapses_mean_per_label.keys()))\n    plt.legend()\n    plt.xlim((0,15))\n\n    \n    plt.subplot(3,3,9)\n    plt.title('All Synapses Mean per Label')\n    plt.hist([synapses_mean_per_label[lbl][\"Sum\"] for lbl in synapses_mean_per_label.keys()], label=list(synapses_mean_per_label.keys()))\n    plt.legend()\n\n    plt.show()","8f9f085c":"if TRAIN_MODULE: \n    import re\n    moduleLayer = r\"tf.__operators__.getitem_\\d+\"\n    selectedLayer = None\n    for layer in model.layers:\n        if bool(re.search(moduleLayer, layer.name)):\n            selectedLayer = layer.name\n            break\n        else: print(layer.name)\n    print(selectedLayer)","8f792668":"if TRAIN_MODULE: plot_examples(model, startFrom=0, plot_cycle=False, plot_spikes=False, plot_start=None, write_last=True, preNeuron=\"preNeuronBool\", spikeTrain=selectedLayer)","35044fcc":"if TRAIN_MODULE: plot_statistics(model, 200 + 200%32, preNeuron=\"preNeuronBool\", spikeTrain=selectedLayer)","262b4325":"if TRAIN_MODULE: \n    plt.figure(figsize=(5,15))\n    weights = model.get_layer(\"WiringLayer\").get_weights()[0][0]\n    plt.suptitle(f\"Weights Examples (8 synapses out of {2*N_EXC})\")\n    for i in range(8):\n        for j in range(4):\n            plt.subplot(8, 4, 4*i+j+1)\n            x = np.zeros((32,8))\n            for l in range(32\/\/4):\n                x[:,l] = weights[:,l*4+j,i]\n            plt.imshow(x, vmin=tf.reduce_min(weights), vmax=tf.reduce_max(weights))\n            if not i:\n                plt.title(r\"$\\Theta={}\\pi$\".format(thetas[j]\/np.pi))\n            if not j:\n                plt.ylabel(f\"Syn {i}\\nyaxis pixels\")\n            if i == 7:\n                plt.xlabel(f\"filter\")\n            plt.xticks([])\n            plt.yticks([])","a1c1e4ad":"if TRAIN_MODULE: \n    plt.plot(weights.mean(axis=(0,2)), label=\"mean\")\n    plt.plot(np.abs(weights).mean(axis=(0,2)), label=\"abs mean\")\n    plt.title(\"Mean and Abs Mean of Synapse Weights by Channel (after processing)\")\n    plt.legend()\n    plt.show()","9b6f7bc4":"if TRAIN_MODULE:\n    plt.figure(figsize=(20,20))\n\n    plt.subplot(2,2,1)\n    plt.title(r\"Exc Synapses Weights (std by Synapse)\")\n    plt.hist(weights[:,:,:N_EXC].std(axis=(0,1)))\n\n    plt.subplot(2,2,3)\n    plt.title(r\"Exc Synapses Weights (sum by Synapse)\")\n    plt.hist(weights[:,:,:N_EXC].sum(axis=(0,1)))\n\n    plt.subplot(2,2,2)\n    plt.title(r\"Inh Synapses Weights (std by Synapse)\")\n    plt.hist(weights[:,:,N_EXC:].std(axis=(0,1)))\n\n    plt.subplot(2,2,4)\n    plt.title(r\"Inh Synapses Weights (sum by Synapse)\")\n    plt.hist(weights[:,:,N_EXC:].sum(axis=(0,1)))\n\n    plt.show()","0ec80d5a":"if TRAIN_MODULE: \n    # plt.figure(figsize=(10,20))\n    weights = model.get_layer(\"WiringLayer\").get_weights()[0][0]\n    print(f\"min: {weights.min()}, max: {weights.max()}\\nmean: {weights.mean()}, sd: {weights.std()}\\n\")\n    exc = weights[:,:,:N_EXC]\n    inh = weights[:,:,N_EXC:]\n    amountExc = [(exc>0.01).sum(axis=(0,1))]\n    amountInh = [(inh>0.01).sum(axis=(0,1))]\n\n    plt.subplot(1,2,1)\n    plt.hist(amountExc)\n    plt.title(\"Exc Synapses per Pixel\")\n    plt.ylabel(\"Synapses per Pixel\")\n\n    plt.subplot(1,2,2)\n    plt.hist(amountInh)\n    plt.title(\"Inh Synapses per Pixel\")\n\n    plt.show()","3e938094":"if TRAIN_MODULE: serial = 0","4a9391e6":"if TRAIN_MODULE: \n    serial += 1\n    with open(f\"weights{serial}.npy\", 'wb') as f:\n        np.save(f, model.get_layer(\"WiringLayer\").get_weights()[0])\n    with open(f\"bias{serial}.npy\", 'wb') as f:\n        np.save(f, model.get_layer(\"WiringLayer\").get_weights()[1])\n    with open(f\"batchnorm{serial}.npy\", 'wb') as f:\n        np.save(f, model.get_layer(\"BatchNorm\").get_weights())","30d3bf29":"if TRAIN_MODULE: model.save(f\".\/model{serial}\")","a7d566ef":"create_limited_syn = True","6f52e1af":"if not create_limited_syn: direc = \"..\/input\/758-on-neuron-wiring-79-validation\"","8ec210ad":"if not create_limited_syn: \n    module = create_module(sigmoid_threshold=0.9, sigmoid_mult=50, threshold=0.1)\n    weights = []\n    with open(direc+\"weights1.npy\", 'rb') as f: weights.append(np.load(f))\n    with open(direc+\"bias1.npy\", 'rb') as f: weights.append(np.load(f))\n    module.get_layer(\"WiringLayer\").set_weights(weights)\n    with open(direc+\"batchnorm1.npy\", 'rb') as f: module.get_layer(\"BatchNorm\").set_weights(np.load(f))","f40d6186":"if not create_limited_syn: module.evaluate(test_ds)","a938ad60":"direc = \"..\/input\/758-on-neuron-wiring-79-validation\"\nTHRESHOLD = 0.2","540302d3":"if create_limited_syn:\n    module_syn, module_name = create_module_limited_syn(threshold=THRESHOLD, non_neg=False, nSynapse=False, dropout1=0.4, dropout2=0.4, \n                                                        use_sigmoid=True, regular_sigmoid=True, sigmoid_mult=50, sigmoid_threshold=.9, augment=False, optimizer=Nadam(), qSynapse=(.1, .1), synLoss=MSE_RMS_SynapsesPerMS)\n    weights = []\n    with open(os.path.join(direc,\"weights1.npy\"), 'rb') as f: weights.append(np.load(f))\n    with open(os.path.join(direc, \"bias1.npy\"), 'rb') as f: weights.append(np.load(f))\n    module_syn.get_layer(\"WiringLayer\").set_weights(weights)\n    with open(os.path.join(direc, \"batchnorm1.npy\"), 'rb') as f: module_syn.get_layer(\"BatchNorm\").set_weights(np.load(f))","f0837818":"module_to_use = module_syn if create_limited_syn else module","2b96b3b7":"import re\nmoduleLayer = r\"tf.__operators__.getitem_\\d+\"\nselectedLayer = None\nfor layer in module_to_use.layers:\n    if bool(re.search(moduleLayer, layer.name)):\n        selectedLayer = layer.name\n        break\n    else: print(layer.name)\nprint(selectedLayer)","bbfaeb40":"preNeuron=\"preNeuronBool\"\nspikeTrain=selectedLayer\npostNeuron=\"nSpikes\"","1765b97e":"if not (os.path.isdir('..\/working\/cats-dogs-matrices')):\n    os.makedirs('..\/working\/cats-dogs-matrices')","f3f9e0c4":"how_many_to_transform_per_label = None  # set to zero or None to convert all the dataset\ndatasets_to_convert = [(train_ds, \"training\"), (valid_ds, \"validation\"), (test_ds, \"test\")]\ndatasets_to_convert = datasets_to_convert[1:]  # change ","37186bab":"import shutil\ndef zip_and_delete(directory, zip_name, to_zip=True):\n    if not to_zip: return\n    shutil.make_archive(zip_name, 'zip', directory)\n    print(f'Done Zipping! Check for {zip_name}.')\n    shutil.rmtree(directory)\n    print('Done erasing photos!')","2a83ff30":"stop = False\n# how_much_wanted = 250\nfor ds, dir_name in datasets_to_convert:\n    dct = {lbl: 0 for lbl in LABELS}\n    os.makedirs('..\/working\/cats-dogs-matrices\/'+dir_name)\n    for lbl in LABELS:\n        os.makedirs(f'..\/working\/cats-dogs-matrices\/{dir_name}\/{lbl}')\n    \n    for batchN, (img, label) in enumerate(ds):\n#         if batchN >= how_much_wanted: break\n        inputs = K.function(module_to_use.input, module_to_use.get_layer(preNeuron).output)([img])\n        outputs = K.function(module_to_use.input, module_to_use.get_layer(spikeTrain).output)([img])\n        nAP = K.function(module_to_use.input, module_to_use.get_layer(postNeuron).output)([img])\n        _, ap, somaV = module_to_use(img)\n\n        if how_many_to_transform_per_label and sum(dct.values()) >= 2*how_many_to_transform_per_label: break\n\n        for i in range(inputs.shape[0]):\n            im = img[i]\n            lbl = LABELS[label[i]]\n            inp = inputs[i]\n            out = np.array([o[i] for o in outputs]) if isinstance(outputs, list) else outputs[i]\n            pred = np.array([o[i][0] for o in nAP]) if isinstance(outputs, list) else nAP[i][0]\n            v = np.array([o[i,:,0] for o in somaV]) if isinstance(outputs, list) else somaV[i,:,0]\n            predLbl = LABELS[int(round(pred, 0))]\n            if how_many_to_transform_per_label and dct[lbl] >= how_many_to_transform_per_label: continue\n            else:\n                curDir = f'..\/working\/cats-dogs-matrices\/{dir_name}\/{lbl}\/{dct[lbl]}'\n                os.makedirs(curDir)\n                with open(curDir+\"\/image.npy\", 'wb') as f: np.save(f, im)\n                with open(curDir+\"\/matrix.npy\", 'wb') as f: np.save(f, inp)\n                with open(curDir+\"\/spikePrediction.npy\", 'wb') as f: np.save(f, np.array(out))\n                with open(curDir+\"\/voltagePrediction.npy\", 'wb') as f: np.save(f, np.array(v))\n                with open(curDir+'\/predDict.p', 'wb') as f: pickle.dump({\"correct\": lbl, \"predictedLbl\": predLbl, \"predictSpike\": pred}, f, protocol=pickle.HIGHEST_PROTOCOL)\n                dct[lbl] += 1\n                print(curDir)\n    zip_and_delete('..\/working\/cats-dogs-matrices\/'+dir_name, '..\/working\/cats-dogs-matrices\/'+dir_name+\"zip\", to_zip=True)","17160859":"# Load Pretrained Module","f41071bd":"# Use David Beniaguev's (selfishgene) trained L5PC model","5ab72035":"# Save Weights","454da694":"# Module","c2510534":"# The Same with Limited Synapses","50e3b25b":"# Some custom layers and funcs to use","6bc05f72":"Gabor initializer","a1e54159":"# Start noise","5c70b284":"how the new sigmoid looks like","1e004142":"# Convert Dataset Using Pretrained Module","e935baa4":"# Convert Images to 400X1278 matrices","fa5d45cc":"# Transform data to dataset","b768dd04":"Parameters for gabor filters and max pooling kernel sizes are from \"Robust Object Recognition with Cortex-Like Mechanisms\" (Serre et al.)","6cde2eef":"> # Gabor filters - Simple Cells","23f14a40":"# **Preprocessing**","38032113":"# Plot","e207109a":"# A little augmentation","48eccd7c":"show some photos","4b6d0085":"Function to create Conv2D layer waith gabor filter"}}