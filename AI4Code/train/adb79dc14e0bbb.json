{"cell_type":{"493120e9":"code","5156ce73":"code","9684e865":"code","264aa2a2":"code","8a24dfa1":"code","488d3847":"code","4123c7ba":"code","417a3589":"code","9f44f7e2":"code","673eaa3d":"code","e94cb861":"markdown","799d3c26":"markdown","c45766e7":"markdown","86bd110f":"markdown","3cf75284":"markdown","30a77212":"markdown","1b74f02a":"markdown","f75aaee3":"markdown","8ae904b7":"markdown","c6de6321":"markdown","2b8eb668":"markdown"},"source":{"493120e9":"import pandas as pd\nimport numpy as np\nimport seaborn as sb; sb.set()\nimport matplotlib.pyplot as plt\nimport random\nfrom scipy.optimize import curve_fit\nimport scipy.integrate as spi\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, GlobalMaxPooling1D, Dropout, GRU, BatchNormalization\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\n\n# Some utility functions for data preparation\ndef upsample(country,df):\n    \"\"\"pad zeros before the onset of the outbreak for a given country\"\"\"\n    df = df.loc[df.country==country]\n    days = int((df.date.values[0]-start).astype('timedelta64[D]')\/ np.timedelta64(1, 'D'))\n    data = [[start+ np.timedelta64(i,'D'), country, 0, 0] for i in range(days)]\n    return pd.DataFrame(data, columns=['date','country',target, 'duration_outbreak'])\n\ndef split_sequences(sequences, n_steps):\n    \"\"\"preparation of data with n_steps window predicting the next value\"\"\"\n    X, y = list(), list()\n    for i in range(len(sequences)):\n        # find the end of this pattern\n        end_ix = i + n_steps\n        # check if we are beyond the dataset\n        if end_ix > len(sequences)-1:\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :]\n    X.append(seq_x)\n    y.append(seq_y)\n    return np.array(X), np.array(y)","5156ce73":"df = pd.read_csv('..\/input\/novel-corona-virus-2019-dataset\/covid_19_data.csv')\n\n#load a mapping file to attribute international 3 lettres standard code to communicate with external data\ndf_map = pd.read_csv('..\/input\/coronavirus\/countryMapping.csv')\ndf = df.merge(df_map, how='left', on = 'Country\/Region')\n#transform data and create extra features\ndf['date'] = pd.to_datetime(df.ObservationDate)\ndf = df.loc[df.Confirmed>0]\ndf['Actives'] = df.apply(lambda x: x.Confirmed - x.Deaths - x.Recovered, axis = 1)\ndf.rename(columns= {'Country Code': 'country'}, inplace =True)\nstart = df.groupby('country').min().reset_index().rename(columns={'date':'start_outbreak'})[['country', 'start_outbreak']]\ndf = df.merge(start, on = 'country')\ndf['duration_outbreak'] = df.apply(lambda x: (x.date-x.start_outbreak).days, axis=1)\ndf_li = df[['date', 'country', 'Confirmed', 'Deaths', 'Recovered','Actives', 'duration_outbreak']]\n#sum over regions of a same contry\ndf_li = df_li.groupby(['date','country']).agg({'Confirmed':'sum', 'Deaths':'sum', 'Recovered':'sum', 'Actives':'sum', 'duration_outbreak':'max'})\ndf_li.reset_index(inplace=True)\n\n#select contries with more than 15 days of outbreak\nlastDate = df[['date','country']].groupby('country').max().values[0][0]\nsel = df.loc[df['date']== lastDate].groupby('country').max()\nsel = sel.loc[sel.duration_outbreak > 15]\ndf_li = df_li.loc[df_li.country.isin(sel.index)]\nprint('%i countries'%df_li.country.unique().shape[0])\n#create new features after aggregation and filtering\ndf_li['lethality'] = df_li.apply(lambda x: 100*x.Deaths \/ x.Confirmed , axis = 1)\n#Convert cumulative Confirmed to new cases\nnewCases=[]\nfor cc in df_li.country.unique():\n    cumul = df_li.loc[df_li.country==cc].sort_values('duration_outbreak').Confirmed.values\n    newCases.extend([(cc, 0, cumul[0])] + [(cc, ix+1, i- cumul[ix]) for ix,i in enumerate(cumul[1:])])\nnewCases = pd.DataFrame(newCases, columns = ['country','duration_outbreak','new_cases'])\ndf_li = df_li.merge(newCases, on=['country','duration_outbreak'])\n#Calculate prevalence from world bank population data\ndfp = pd.read_csv('..\/input\/coronavirus\/world.csv')\nwb_pop=  dfp.loc[dfp['Series Code'] == 'SP.POP.TOTL',['Country Code','2018 [YR2018]']]\nwb_pop['2018 [YR2018]'] = wb_pop['2018 [YR2018]'].apply(lambda x: eval(x) if x !='..' else np.nan)\nwb_pop = wb_pop.rename(columns={'2018 [YR2018]': 'population'})\ndf_li = df_li.merge(wb_pop, left_on = 'country', right_on='Country Code')\ndel df_li['Country Code']\ndf_li['prevalence'] = df_li.apply(lambda x: 10000*x.Confirmed\/x.population, axis = 1)\ndf_li.dropna(inplace=True)\n#basic variable transform\ndf_li['confirmed_log'] = df_li.Confirmed.apply(lambda x: np.log(x))\ndf_li.head()","9684e865":"#create dataset for 1DCNN\ntarget = 'confirmed_log'\ndf_tar =  df_li[['date','country']+[target]].dropna()\n\n#upsample for missing values before the onset of outbreak in a given country by padding 0\nstart = df_tar.loc[df_tar.country=='CHN'].date.values[0]\nfor cc in df_tar.country.unique():\n    nsamp=upsample(cc, df_tar)\n    df_tar = pd.concat([df_tar,nsamp], axis = 0, sort=False)\n#Select countries with a significant number of cases    \ncountries = df_li.loc[df_li.Confirmed>200,'country'].unique()\nsel = df_tar.loc[df_tar.country.isin(countries)].sort_values(['date','country'])\n#make dataset compliant for 1DCNN inputs\nsel = sel.pivot(index='date', columns='country', values=target)\n#split on historic samples\ntrain = sel[sel.index < '2020-03-07']\ntest = sel[sel.index >= '2020-03-07']\n#slice the convolution window on 11 days\nn_steps = 11\nX,y = split_sequences(train.values,n_steps)\nn_features = X.shape[2]\nsel.head()","264aa2a2":"# define model\nmodel = Sequential()\nmodel.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps, n_features)))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Conv1D(filters=64, kernel_size=2, activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(48, activation='relu'))\nmodel.add(Dense(n_features))\nmodel.compile(optimizer='adam', loss='mse')\nmodel.summary()\n\n#unsucessful tested layers:\n#model.add(BatchNormalization())\n#model.add(Dropout(0.2))\n#model.add(GRU(64, dropout=0.1, recurrent_dropout=0.1))","8a24dfa1":"# fit model\nrandom.seed(3)\nepochs=500\nstart_plot=0\nhistory = model.fit(X, y, epochs=epochs, verbose=0)\nloss = history.history['loss'][start_plot:]\n\n#plot loss. Accuracy is not calculated because there is no validation set\nstart_plot=0\niepochs = range(start_plot,epochs)\nplt.plot(iepochs, loss, 'bo', label='Training loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","488d3847":"def predict(n, previous=None):\n    X_test = train.iloc[-n_steps+n:]\n    if not previous is None:\n        X_test = pd.concat([X_test, previous], axis = 0, sort=False)\n    ref = test.iloc[n].values    \n    X_test = X_test.values.reshape((1, n_steps, n_features))\n    yhat = [int(round(i)) for i in model.predict(X_test, verbose=0)[0]]\n    df =pd.DataFrame(list(zip(test.columns, yhat,ref)), columns = ['country','yhat','ref'])\n    rmse = round(np.sqrt(np.mean((df.yhat.values-df.ref.values)**2)),2)\n    del df['ref']\n    df['date']=test.iloc[n:n+1].index.values[0]\n    df = df.pivot(index='date', columns='country', values='yhat')\n    if previous is None:\n        previous = df\n    else:\n        previous = pd.concat([previous, df], axis=0, sort=False)\n    return previous,rmse\n\nrmses=[]\nprevious=None\nfor i in range(test.shape[0]): \n    previous,rmse = predict(i,previous)\n    rmses.append(rmse)\n                           \nprint ('rmse : %.2f'%np.mean(rmses))","4123c7ba":"pred = pd.concat([train.iloc[-1:], previous], axis=0, sort=False)\nx = pred.index.values.astype('datetime64[D]')\nx=['-'.join(str(i).split('-')[1:]) for i in x]\nxb = sel.index.values.astype('datetime64[D]')\nxb=['-'.join(str(i).split('-')[1:]) for i in xb][-15:]\n\nfig, axes = plt.subplots(ncols=4, nrows=6, figsize=(16,16))\nfor cc, ax in zip(test.columns, axes.flat):\n    yb=list(sel[cc].values)[-15:]\n    y=list(pred[cc].values)\n    sb.lineplot(x=xb, y=yb, ax=ax).set_title(cc)\n    sb.lineplot(x=x, y=y, ax=ax)\nfig.tight_layout(h_pad=1, w_pad=0)","417a3589":"def diff_eqs(init,t):  \n    y=np.zeros((3))\n    v = init   \n    y[0] = - beta * v[0] * v[1]\n    y[1] = beta * v[0] * v[1] - gamma * v[1]\n    y[2] = gamma * v[1]\n    return y  \n    \ndef SIR(x, *p):\n    beta, gamma, amp, s0 ,i0 = p\n    init = (s0, i0, 0.0)  \n    res = spi.odeint(diff_eqs,init,x)\n    return res[:,1]*amp\n\ndef fitted(y, p_initial):\n    duration= y.shape[0]\n    y = y\/ np.linalg.norm(y)\n    x = np.linspace(0, duration,duration)\n    popt, pcov = curve_fit(SIR, x, y, p0=p_initial)\n    print(popt)\n    return (SIR(x, *popt), popt, x, y)    ","9f44f7e2":"s0=1-1e-4      #initial p(succeptible)\ni0=8e-3        #initial p(infectious)\nbeta = 0.3\ngamma = 0.1\namp = 0.75\np_initial = [beta, gamma, amp,s0,i0]\n\nsel = df_li.loc[df_li.country == 'CHN', 'Actives'].values  \nyhat, popt, x, y = fitted(sel, p_initial)\n\nfig, ax = plt.subplots(ncols=2, nrows=1, figsize=(8,4))\nax[0].plot(x,y, '-r', label='Infectious') \nax[0].plot(yhat, '-b', label='Predicted')\n\nyhat = SIR(range(0,100), *popt)    \nax[1].plot(yhat, '-b', label='Infectious')","673eaa3d":"S0=1-1e-4      #initial p(succeptible)\nI0=1e-5        #initial p(infectious)\nINPUT = (S0, I0, 0.0)\nbeta = 0.43\ngamma = 0.07\namp = 1.2\np_initial = [beta, gamma, amp,s0,i0]\n\nsel = df_li.loc[df_li.country == 'ITA', 'Actives'].values  \nyhat, popt, x, y = fitted(sel, p_initial)\n\nfig, ax = plt.subplots(ncols=2, nrows=1, figsize=(8,4))\nax[0].plot(x,y, '-r', label='Infectious') \nax[0].plot(yhat, '-b', label='Predicted')\n\nyhat = SIR(range(0,120), *popt)    \nax[1].plot(yhat, '-b', label='Infectious')","e94cb861":"# Deep learning vs good old epidemiological compartmental model for COVID-19\n\n\n#### Did you know? Cats are dealing with a very lethal species of coronavirus since centuries. But don't put your cat in the oven, you are safe. The cat coronavirus is an alphavirus never transmited to humans. Close to 70% of cats contains virus in there gut, 10% will be sick and 100% of these sick animals will die in days or months. The particularity of these sick cats is that they are not contagious anymore. The [contamination cycle](http:\/\/www.youtube.com\/watch?v=rkqUjeQNEQs) is oro-fecal. And the pathogeny is explained [here](https:\/\/www.youtube.com\/watch?v=6RyI2LI9R9Q). The new SARS variant (SARS-V2) is a betacoronavirus of the subgenus Sabecovirus close to bat SARS-like viruses and it's epidemiological features are far away from the Feline Infectious Peritonitis. This virus could have been humanized by passing on pangolin, the most poached species.\n\n\n![cat3.jpg](attachment:cat3.jpg)\n\nWe try to implement a simple 1D CNN for forecasting. This method allow to quickly process in paralelle a great number of countries with very few tuning. [compotmental epidemiological model]https:\/\/en.wikipedia.org\/wiki\/Compartmental_models_in_epidemiology#The_SIR_model_with_vital_dynamics_and_constant_population have been and are part of the state of the art. Here we propose a very simple SIR model. It contains a compartment for Sicks, another for the Infectious who are the active cases and the Recovered. We should add a fourth compartment for deads to make a SIRD model, but i'll for another notebook. In the SIR model deads and recovered are in the same compartment, they are not anymore source of infection.","799d3c26":"### Now it's time to prepare data to be fit in the 1DCNN.\n\nFirst data for each country are padded with zeros when outbreaks began after the start of the recording on 2020-01-22.\n\nA filter to keep only countries with more than 200 cases allow to have meaningfull samples. \n\nA train set contains all records before 2020-03-07 and a test set contains the remaining records. There is no validation set for the training, because more days should be necessary in the dataset.\n\nLastly data are organized by arrays of 11 consecutive days of the studied variable, here the \"confirmed_log\", and the target is the next record on the 12th day. Each line contains the values for all studied countries i.e 50, so the shape is (nb_samples, 11,50) for the features and (nb_samples,1,50) for the target.\n","c45766e7":"### Evaluation of the RMSE between prediction and test values for all countries","86bd110f":"### Definition of a 1DCNN model.\nThis model is not optimal because no validation dataset was supplied during training. And probably a LSTM should perform better, but more data is probably required.","3cf75284":"## Conclusions\n\nProbably a SIRD model should be included in the machine learning process, whatever it is.","30a77212":"## The SIR method\nQuick to implement but not easy to automate tunning of beta and gamma needs to be done by hand. A next step should be to use a better optimization procedure of the parameters of the SIR function, but this is not trivial to solve, because it's not convex for beta and gamma with discontinuous gradients.","1b74f02a":"### Let's run it","f75aaee3":"### Case of China. The model is fitted to the known data and the model is extend to make prediction at 100 days.","8ae904b7":"### We are working with the covid_19_data.csv file\n\nFirst cooutries name are cleaned in a 3 letters international code to allow a matching with data [world data bank](https:\/\/databank.worldbank.org\/source\/population-estimates-and-projections).\n\nThen some extra-features are created:\n* Actives are the current infectious\n* duration_outbreak\n* new cases are for incidence calculation\n* prevalence are the sum of Confirmed since the beginning od the outbreak\n* lethality is the proportion of confirmed people who die.\n* log(Confirmed)\n\nLastly we select the countries with more than 15 days of outbreak and aggregate on date and country to get rid of regions.","c6de6321":"### Case of Italy. The model extend to make prediction at 120 days.","2b8eb668":"### Let's see the result.\nThese are uneven results, particularly for Asia. It's interresting because the CNN could only infere from a period ending on 2020-02-26. From this period it predicts incease of prevalence. Since it appears the situation is better tha predicted, then the situation is improving. Remember we gave no guidance at all for the analysis. This is a lazzy supervised method."}}