{"cell_type":{"4479b0bb":"code","5fb65609":"code","b603c60c":"code","6554f270":"code","c4eb97f6":"code","21b6c7be":"code","160c174c":"code","385c421e":"code","0d476207":"code","5b2a409b":"code","e8dd7f6d":"code","818d30b5":"code","7a40b3db":"code","11a94297":"code","310fd91b":"code","b1f112b2":"code","066900b4":"code","4e2743e3":"code","f4cce9b3":"code","9e39638f":"code","0887831e":"code","732e78b6":"code","8bea8d39":"code","33162caf":"code","02518301":"code","29847e57":"code","ef8ba703":"markdown","fa4159a0":"markdown","3581be24":"markdown","fdcec9e7":"markdown","bc76c609":"markdown","badf7525":"markdown","f0e3bc74":"markdown","46c77107":"markdown","bb4f6133":"markdown","03e46fa1":"markdown","3ccd31e5":"markdown","b9f02579":"markdown","c9c43b38":"markdown"},"source":{"4479b0bb":"import pandas as pd\nimport numpy as np\nimport os.path as osp\nfrom tqdm.notebook import tqdm","5fb65609":"INPUT_PATH = '\/kaggle\/input\/jigsaw-toxic-severity-rating\/'\nOUTPUT_PATH = 'checkpoints\/'\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)","b603c60c":"!unzip \/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip","6554f270":"train = pd.read_csv('train.csv')\ntrain.head()","c4eb97f6":"# train['target'] = train['toxic']\n\ntrain['severe_toxic'] = train.severe_toxic * 2\ntrain['target'] = train[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1).astype(int)\ntrain['target'] = train['target']\/train['target'].max()\n\ntrain['target'] = train['target'].where(train['target'] > 0, 1)\ntrain['target'] = train['target'].astype(int)","21b6c7be":"import plotly.express as px\nfig = px.pie(\n    train.toxic, \n    values=train.toxic.value_counts().values, \n    names=[\"nontoxic\", \"toxic\"],\n    width=500,\n    height=500\n)\nfig.update_layout(\n    showlegend=False,\n    title=\"Toxic Rate\"\n)\nfig.update_traces(textinfo='label+percent')\nfig.show()","160c174c":"train_texts = []\ntrain_labels = []\n\nfor i in tqdm(range(len(train))):\n    train_texts.append(train.iloc[i]['comment_text'])\n    train_labels.append(train.iloc[i]['target'])","385c421e":"from sklearn.model_selection import train_test_split\ntrain_texts, test_texts, train_labels, test_labels = \\\ntrain_test_split(\n    train_texts, \n    train_labels, \n    test_size=.2,\n    random_state=RANDOM_SEED,\n    shuffle=False\n)","0d476207":"print(f'Train Example: \\n text: {train_texts[0]}\\n label: {train_labels[0]}\\n\\n')\nprint(f'Val Example: \\n text: {test_texts[0]}\\n label: {test_labels[0]}\\n\\n')","5b2a409b":"from transformers import DistilBertTokenizerFast\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')","e8dd7f6d":"train_encodings = tokenizer(train_texts, truncation=True, padding=True)\ntest_encodings = tokenizer(test_texts, truncation=True, padding=True)","818d30b5":"import torch\n\nclass JigSawDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = JigSawDataset(train_encodings, train_labels)\ntest_dataset = JigSawDataset(test_encodings, test_labels)","7a40b3db":"import wandb\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"wandb_api\")\n    wandb.login(key=api_key)\n    anony = None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https:\/\/wandb.ai\/authorize')","11a94297":"wandb.init(project=\"jigsaw-toxic-severity-rating\", entity=\"taeho\")\n%env WANDB_LOG_MODEL=true","310fd91b":"from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir='.\/results',          # output directory\n    save_strategy=\"no\",              # save strategy\n    num_train_epochs=3,              # total number of training epochs\n    per_device_train_batch_size=16,  # batch size per device during training\n    per_device_eval_batch_size=64,   # batch size for evaluation\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    logging_dir='.\/logs',            # directory for storing logs\n    logging_steps=500,\n    report_to=\"wandb\"\n)\n\nmodel = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")","b1f112b2":"trainer = Trainer(\n    model=model,                         # the instantiated \ud83e\udd17 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=test_dataset            # test dataset\n)","066900b4":"trainer.train()","4e2743e3":"trainer.save_model(OUTPUT_PATH)","f4cce9b3":"trainer.evaluate()","9e39638f":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# model = DistilBertForSequenceClassification.from_pretrained(\"path_to_checkpoint\").to(device)","0887831e":"sample_submission = pd.read_csv(osp.join(INPUT_PATH, 'sample_submission.csv'))\ncomments_to_score = pd.read_csv(osp.join(INPUT_PATH, 'comments_to_score.csv'))\ncomments_to_score['score'] = 0\ncomments_to_score.head()","732e78b6":"x = 0\nfor i in tqdm(range(len(comments_to_score))):\n    input = tokenizer.encode(comments_to_score.iloc[i]['text'], return_tensors=\"pt\").to(device)\n    output = model(input[:, :512])[0]\n    predictions = torch.softmax(output, dim=1)\n    comments_to_score.loc[i, 'score'] = predictions[0][1].item()","8bea8d39":"comments_to_score['score'] = comments_to_score['score'].rank(method='first')","33162caf":"comments_to_score.sort_values('score')","02518301":"sample_submission['score'] = comments_to_score.sort_values('comment_id')['score']","29847e57":"sample_submission","ef8ba703":"## Torch Dataset","fa4159a0":"# Evaluate","3581be24":"# Dataset","fdcec9e7":"# Public LB Score : 0.782 \n# [Inference Notebook : https:\/\/www.kaggle.com\/adldotori\/huggingface-distilbertclassification-inference](https:\/\/www.kaggle.com\/adldotori\/huggingface-distilbertclassification-inference)","bc76c609":"## download pretrained model","badf7525":"# Toxic Rate","f0e3bc74":"Upside is nontoxic text, downside is toxic text.","46c77107":"## training","bb4f6133":"# Use \"Toxic Comment Classfication Challenge\" Train Data","03e46fa1":"## Inference","3ccd31e5":"## wandb setting","b9f02579":"## Tokenizer","c9c43b38":"# Fine-tuning with Trainer"}}