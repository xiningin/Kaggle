{"cell_type":{"2e66a324":"code","5df3a3af":"code","104c7d7a":"code","bce9b8bf":"code","34631bee":"code","7ddcb35b":"code","bc292c74":"code","8f27169a":"code","08ab50ce":"code","9a106c17":"code","42a00f05":"code","66825ba0":"code","dcd79c2a":"code","8923ca3e":"code","45c63dd1":"code","7d574280":"code","91497865":"code","70b41310":"code","e6841404":"code","48d734b7":"code","910a18df":"code","41281c42":"code","d7916fec":"code","178c0bfd":"code","f5f8ad24":"code","8f272edc":"code","f15bde19":"code","04c5866f":"code","7eb30cec":"code","ecc7aac2":"code","67f9d8e3":"code","9dfa49af":"code","2b414eca":"code","a5fc2caf":"code","5569c321":"code","82d53298":"code","b198abe8":"code","b3a1b7f5":"code","9bfa725d":"code","de1ec3af":"code","2a87f64d":"code","92f4c8d2":"code","ca7cdc4e":"code","4837728e":"code","b4909077":"code","7053de34":"code","68c344d2":"code","309d5e86":"code","1fe2efe1":"code","85f991dd":"code","be6779b1":"code","b769da3f":"code","51d61800":"code","9d243fc1":"code","57ce84c8":"code","1803fccc":"code","d49769b0":"code","bcd29e4c":"code","4f737b89":"code","aec3f3ab":"code","e4317717":"code","85ad6f20":"code","bd9da935":"code","dc2bd305":"code","97d806f4":"code","5985a92a":"code","d35e714d":"code","9a46f2fb":"code","5fae06e5":"code","92d2c76f":"code","0e8121c9":"code","c0017d28":"code","7edafcdf":"code","de96ab34":"code","221495e8":"code","e787394f":"code","ee037fa4":"code","c7a50b4a":"code","428a35fb":"code","b5cc1353":"code","6fc3faab":"code","6f0fa8a5":"code","48651d84":"code","1b13c652":"code","d21dd47e":"code","3a26592d":"code","51b2652c":"code","99b838b5":"code","66aeccd6":"code","b829e5f2":"code","49e706f9":"code","17cacbae":"code","54be5cf5":"code","427fa3e7":"code","08119aa5":"code","f8637e95":"code","4261bad0":"code","dc1a0d0d":"code","5211821b":"code","2a2344a1":"code","812dc1db":"markdown","7c653b1f":"markdown","c751907e":"markdown","6acc5d66":"markdown","bb4ea542":"markdown","27d9028f":"markdown","ad03f583":"markdown","8951b826":"markdown","e1ccc9a5":"markdown","197e9ffd":"markdown","7bd21d91":"markdown","9024f0ae":"markdown","4a271859":"markdown","e973eb3c":"markdown","9e2d9b61":"markdown","0c400f82":"markdown","6640ed95":"markdown","683edc6a":"markdown","4965af28":"markdown","ea79f418":"markdown","3225d96b":"markdown","edf8b929":"markdown","0c4fa7c3":"markdown","204db6e6":"markdown","9d4c2b85":"markdown","edd46bf1":"markdown","ccfb71ae":"markdown","18c6d5b4":"markdown","4608d13a":"markdown","c7c8c0e7":"markdown","73f4598b":"markdown","ef182eb4":"markdown","63df6656":"markdown","38b03009":"markdown","36df3090":"markdown","8e897e54":"markdown","34ea5a96":"markdown","e3a7b18f":"markdown","ccaf9bac":"markdown","1bafb0af":"markdown","5160fcf8":"markdown","d993c956":"markdown","11a21716":"markdown","eeaa3ab8":"markdown","3b1a2720":"markdown","968c2616":"markdown","989e60b6":"markdown","d522552a":"markdown","f09a2c6f":"markdown","20b25401":"markdown","bb0d14f7":"markdown","cd25e708":"markdown","4d3845be":"markdown","a0b8afba":"markdown","6009d490":"markdown","efc9e057":"markdown","bd487ca0":"markdown","106846d2":"markdown","2a70df98":"markdown","508a5035":"markdown","04c46faf":"markdown","c2f6feae":"markdown","f37a839a":"markdown","434bf075":"markdown","bc94b125":"markdown","10339214":"markdown","d7cd9f1e":"markdown","a5bd7ddc":"markdown","985379fe":"markdown","1c6e67c6":"markdown","894f0e3f":"markdown","9c99c770":"markdown","c60344e9":"markdown","a678b1ab":"markdown","b1a42602":"markdown","2c568896":"markdown","1c23ea21":"markdown","07f01905":"markdown","bf30b2d4":"markdown","8e1829ea":"markdown","68f53337":"markdown","fb82251e":"markdown","504689ef":"markdown","08f4171a":"markdown","eee2c9e2":"markdown","378fa46a":"markdown","d738219d":"markdown","c64c3d9d":"markdown","ad808df3":"markdown","5f65f742":"markdown","4a1b4742":"markdown","8d4e68fd":"markdown","0761becf":"markdown","a6e1d773":"markdown","178ddeae":"markdown","729dca45":"markdown","4f8b7169":"markdown","a0c2a2af":"markdown","2c90c9fc":"markdown","5e3ab05e":"markdown"},"source":{"2e66a324":"def format_spines(ax, right_border=True):\n    \"\"\"\n    this function sets up borders from an axis and personalize colors\n    \"\"\"    \n    # Setting up colors\n    ax.spines['bottom'].set_color('#CCCCCC')\n    ax.spines['left'].set_color('#CCCCCC')\n    ax.spines['top'].set_visible(False)\n    if right_border:\n        ax.spines['right'].set_color('#CCCCCC')\n    else:\n        ax.spines['right'].set_color('#FFFFFF')\n    ax.patch.set_facecolor('#FFFFFF')","5df3a3af":"# Importando bibliotecas\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Lendo dados\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n\n# Verificando dimens\u00f5es\nprint(f'Dimens\u00f5es dos dados de treino: {train.shape}')\nprint(f'Dimens\u00f5es dos dados de teste: {test.shape}')","104c7d7a":"# Verificando cabe\u00e7alho\ntrain.head()","bce9b8bf":"# Verificando colunas\ntrain.columns","34631bee":"# Lendo csv com dados sobre os atributos levantados nessa pr\u00e9-an\u00e1lise\npre_analise = pd.read_csv('..\/input\/pre-analise-utf\/pre_analise_utf.txt', sep=';', index_col='nome_variavel')\npre_analise.head()","7ddcb35b":"# Quantidade de dados nulos\ntrain_total_null = train.isnull().sum().sum()\ntrain_qtd_attr_null = train.isnull().any().sum()\ntest_total_null = test.isnull().sum().sum()\ntest_qtd_attr_null = test.isnull().any().sum()\n\n# Comunicando\nprint(f'Atributos com dados nulos (treino): {train_qtd_attr_null}')\nprint(f'Total de entradas nulas (treino): {train_total_null}')\nprint(f'\\nAtributos com dados nulos (teste): {test_qtd_attr_null}')\nprint(f'Total de entradas nulas (teste): {test_total_null}')","bc292c74":"# Analisando atributos nulos nos dados de treino\nnull_attr_train = [attr for attr, null in train.isnull().any().items() if null]\nprint(null_attr_train, end=',')\nprint(f'\\n\\nQuantidade: {len(null_attr_train)}')","8f27169a":"# Analisando atributos nulos nos dados de teste\nnull_attr_test = [attr for attr, null in test.isnull().any().items() if null]\nprint(null_attr_test, end=',')\nprint(f'\\n\\nQuantidade: {len(null_attr_test)}')","08ab50ce":"# Detalhes da pr\u00e9-an\u00e1lise sobre dados nulos\nqtd_null_train = [qtd for qtd in train.isnull().sum().values if qtd > 0]\nanalise_null = pre_analise.loc[null_attr_train, :]\nanalise_null['qtd_null'] = qtd_null_train\nanalise_null.sort_values(by='qtd_null', ascending=False)","9a106c17":"# Indexando atributos categ\u00f3ricos\ncat_train_attribs = [attr for attr, dtype in train.dtypes.items() if dtype == 'object']\ncat_test_attribs = [attr for attr, dtype in test.dtypes.items() if dtype == 'object']\n\n# Verificando\ncat_train_attribs == cat_test_attribs","42a00f05":"# Aplicando somat\u00f3rio\ntotal_train_cat_entries = 0\nfor attr in cat_train_attribs:\n    total_train_cat_entries += len(train[attr].value_counts())\n    \nprint(f'Total de entradas categ\u00f3ricas nos dados de treino: {total_train_cat_entries}')","66825ba0":"# Aplicando somat\u00f3rio\ntotal_test_cat_entries = 0\nfor attr in cat_test_attribs:\n    total_test_cat_entries += len(test[attr].value_counts())\n    \nprint(f'Total de entradas categ\u00f3ricas nos dados de treino: {total_test_cat_entries}')","dcd79c2a":"# Criando an\u00e1lise com dados de treino e teste\ncat_entries_info = pd.DataFrame({})\ncat_train_list = []\ntrain_null = []\ncat_test_list = []\ntest_null = []\nfor attr in cat_train_attribs:\n    cat_train_list.append(len(train[attr].value_counts()))\n    cat_test_list.append(len(test[attr].value_counts()))\n    train_null.append(train[attr].isnull().sum())\n    test_null.append(test[attr].isnull().sum())\n\n# Adicionando dados e configurando dataframe\ncat_entries_info['qtd_train_entries'] = cat_train_list\ncat_entries_info['qtd_null_train'] = train_null\ncat_entries_info['qtd_test_entries'] = cat_test_list\ncat_entries_info['qtd_null_test'] = test_null\ncat_entries_info['diff'] = cat_entries_info['qtd_train_entries'] - \\\ncat_entries_info['qtd_test_entries']\ncat_entries_info.index = cat_train_attribs\n\n# Verificando\ncat_entries_info.head()","8923ca3e":"# Adicionando influ\u00eancia da pr\u00e9 an\u00e1lise\nexpect_cat = pre_analise.loc[cat_entries_info.index, ['expect']].values\ncat_entries_info['expect'] = expect_cat\n\n# Visualizando\ncat_entries_info.sort_values(by='qtd_null_train', ascending=False, inplace=True)\ncat_entries_info","45c63dd1":"# Atributos categ\u00f3ricos mantidos\nkeep_cat_df = cat_entries_info.query('expect == \"Alta\" & diff == 0')\ndrop_cat_df = cat_entries_info.query('expect != \"Alta\"')\nkeep_cat_df","7d574280":"# Filtrando colunas\ndrop_cat = drop_cat_df.index\ntrain_cat_attr_filtered = train.drop(drop_cat, axis=1)\n\n# Levantando atributos categ\u00f3ricos\ndefinitive_cat_attr = [attr for attr, dtype in train_cat_attr_filtered.dtypes.items() if dtype == 'object']\n\n# Verificando dimens\u00f5es\nprint(f'Dataset original: {train.shape}')\nprint(f'Dataset filtrado: {train_cat_attr_filtered.shape}')\nprint(f'Total de atributos categ\u00f3ricos exclu\u00eddos: {len(drop_cat)}')\n\nprint(f'Valores batem? {len(drop_cat) == train.shape[1] - train_cat_attr_filtered.shape[1]}')","91497865":"# Criando fun\u00e7\u00e3o para selecionar atributos num\u00e9ricos e categ\u00f3ricos (facilitando implementa\u00e7\u00e3o)\ndef filter_attribs(X, cat=True):\n    \"\"\"\n    seleciona os atributos de acordo com seu tipo primitivo e retorna uma lista como resultado\n    \"\"\"\n    if cat:\n        attribs = [attr for attr, dtype in X.dtypes.items() if dtype == 'object']\n    else:\n        attribs = [attr for attr, dtype in X.dtypes.items() if dtype != 'object']\n    \n    return attribs","70b41310":"# Testando fun\u00e7\u00e3o\nnew_cat_attribs = filter_attribs(train_cat_attr_filtered)\n\n# Verificando se batem\nprint(f'Colunas categ\u00f3ricas (sem fun\u00e7\u00e3o): {len(definitive_cat_attr)}')\nprint(f'Colunas categ\u00f3ricas (ap\u00f3s fun\u00e7\u00e3o): {len(new_cat_attribs)}')\nprint(f'N\u00fameros batem? {len(definitive_cat_attr) == len(new_cat_attribs)}')","e6841404":"# Filtrando atributos que possuem dados nulos\nX = train_cat_attr_filtered.copy()\nX_cat = X.loc[:, definitive_cat_attr]\nX_cat.isnull().any()[X_cat.isnull().any().values].index","48d734b7":"# Vamos analisar se \u00e9 vi\u00e1vel manter GarageType dados a quantidade de nulos\nX['GarageType'].value_counts()","910a18df":"# Retornando a entrada mais comum\nmost_common = X['GarageType'].value_counts().index[0]\nmost_common","41281c42":"# Preenchendo\nX['GarageType'] = X[['GarageType']].fillna(value=most_common)","d7916fec":"# Verificando se ainda existem dados nulos\nX_cat = X.loc[:, definitive_cat_attr]\nX_cat.isnull().any()[X_cat.isnull().any().values].index","178c0bfd":"# Criando classe para ser implementada futuramente em um pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass fillCategoricalNullValues(BaseEstimator, TransformerMixin):\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        # Retornando colunas com dados nulos\n        null_cat_attr = list(X.isnull().any()[X.isnull().any().values].index)\n        \n        # Retornando entradas mais comuns de cada um deles\n        fill_data = {}\n        for attr in null_cat_attr:\n            most_common = X[attr].value_counts().index[0]\n            fill_data[attr] = most_common\n        \n        # Preenchendo dados nulos\n        new_X = X.fillna(value=fill_data)\n            \n        return new_X","f5f8ad24":"# Testando\nfiller = fillCategoricalNullValues()\nX_cat = train_cat_attr_filtered.loc[:, definitive_cat_attr]\nX_filled = filler.transform(X_cat)\n\n# Verificando\nX_filled['GarageType'].isnull().any()","8f272edc":"# Analisando como foi feito o preenchimento\nX_cat['GarageType'].value_counts()","f15bde19":"# Quantidade que deveria ser preenchida na entrada \"Attchd\"\ngarage_null = X_cat[\"GarageType\"].isnull().sum()\ngarage_most_common = X_cat[\"GarageType\"].value_counts().values[0]\nprint(f'Quantidade de nulos em GarageType: {garage_null}')\nprint(f'Quantidade inicial da entrada mais comum: {garage_most_common}')\nprint(f'Quantidade esperada final no atributo mais comum: {garage_most_common + garage_null}')","04c5866f":"# Tirando prova\nX_filled['GarageType'].value_counts()","7eb30cec":"# Filtrando atributos num\u00e9ricos com a nova fun\u00e7\u00e3o criada\nnum_attribs = filter_attribs(train, cat=False)\ncat_attribs = filter_attribs(train)\nprint(f'Quantidade de atributos num\u00e9ricos (original): {len(num_attribs)}')\nprint(f'Quantidade de atributos categ\u00f3ricos (original): {len(cat_attribs)}')\nprint(f'Total sem altera\u00e7\u00f5es j\u00e1 realizadas: {train.shape[1]}')","ecc7aac2":"# Verificando alguns\nnum_attribs[:5]","67f9d8e3":"# Indexando dataframe\ntarget = train['SalePrice']\nX_num = train.loc[:, num_attribs]","9dfa49af":"# Verificando dados nulos\nX_num.isnull().sum()","2b414eca":"# Indexando dados nulos\nnull_num_attribs = X_num.isnull().any()[X_num.isnull().any().values].index\nnull_num_attribs","a5fc2caf":"# Verificando estat\u00edsticas\nfor null_attrib in null_num_attribs:\n    print(f'\u00c0tributo: {null_attrib}')\n    print(X_num[null_attrib].describe())\n    print()","5569c321":"# Criando imputer\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy='median')\nimputer.fit(X_num)\nX_num_imputed = pd.DataFrame(imputer.transform(X_num), columns=X_num.columns)","82d53298":"# Verificando\nX_num_imputed.isnull().sum()","b198abe8":"# Estat\u00edstica\ntarget.describe()","b3a1b7f5":"# Distribui\u00e7\u00e3o\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfig, ax = plt.subplots(figsize=(10, 4))\nax = sns.distplot(target, bins=40)\nformat_spines(ax)\nax.set_title('Distribui\u00e7\u00e3o SalePrice', size=14)\nplt.show()","9bfa725d":"# M\u00e9tricas de similaridade\nskewness = target.skew()\nkurtosis = target.kurt()\nprint(f'Skewness: {skewness:.4f}')\nprint(f'Kurtosis: {kurtosis:.4f}')","de1ec3af":"# Transformando\nimport numpy as np\n\ntarget_log = np.log(target)\nfig, ax = plt.subplots(figsize=(10, 4))\nax = sns.distplot(target_log, bins=40)\nformat_spines(ax)\nax.set_title('Distribui\u00e7\u00e3o SalePrice (Log)', size=14)\nplt.show()\n\n# Comunicando m\u00e9tricas skewness e kurtosis\nprint(f'Skewness: {target_log.skew():.4f}')\nprint(f'Kurtosis: {target_log.kurt():.4f}')","2a87f64d":"# Testando\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ntarget_scaled = target.copy()\ntarget_scaled = scaler.fit_transform(pd.DataFrame(target_scaled))\n\n# Plotando\nfig, ax = plt.subplots(figsize=(10, 4))\nax = sns.distplot(target_scaled, bins=40)\nformat_spines(ax)\nax.set_title('Distribui\u00e7\u00e3o SalePrice (Scaled)', size=14)\nplt.show()\n\n# Comunicando m\u00e9tricas skewness e kurtosis\nprint(f'Skewness: {pd.Series(list(target_scaled)).skew():.4f}')\nprint(f'Kurtosis: {pd.Series(list(target_scaled)).kurt():.4f}')","92f4c8d2":"# Pr\u00e9-An\u00e1lise\nnum_pre_analise = pre_analise.loc[num_attribs]\nnum_pre_analise.drop(['Id', 'SalePrice'], inplace=True)\nnum_pre_analise","ca7cdc4e":"# Indexando somente atributos com alta influ\u00eancia\nhigh_corr_pre_analise = num_pre_analise.query('expect == \"Alta\"')\nhigh_corr_pre_analise","4837728e":"# Matriz de correla\u00e7\u00e3o\ncorrmat = train.corr()\nfig, ax = plt.subplots(figsize=(12, 8))\nsns.heatmap(corrmat, vmax=.8, square=True)","b4909077":"# Vejamos do que se trata esses atributos\npre_analise.loc[['TotalBsmtSF', '1stFlrSF']]","7053de34":"# Vejamos em mais detalhes\npre_analise.loc[['GarageCars', 'GarageArea']]","68c344d2":"corrmat.nlargest(10, 'SalePrice')['SalePrice'].index","309d5e86":"# Top 10 atributos com maior correla\u00e7\u00e3o com o target\nk = 10\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nfig, ax = plt.subplots(figsize=(18, 7))\nax = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 12}, \n                 yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","1fe2efe1":"# Resultado da pr\u00e9 an\u00e1lise\npre_analise.loc[cols[1:]]","85f991dd":"# Lendo novamente os dados para evitar qualquer transforma\u00e7\u00e3o j\u00e1 realizada\nX_train = train.copy()\nX_test = test.copy()\n\n# Target\ny_train = X_train['SalePrice']\nX_train.drop('SalePrice', axis=1, inplace=True)\n\n# Verificando\nX_train.head()","be6779b1":"# Importando bibliotecas\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Defininindo atributos categ\u00f3ricos\ncat_attribs = ['GarageType', 'KitchenQual', 'CentralAir', 'HeatingQC', 'MSZoning', 'Foundation', 'ExterQual',\n               'ExterCond']\nX_cat = X_train.loc[:, cat_attribs]\n\n# Criando pipeline categ\u00f3rico\ncat_pipeline = Pipeline([\n    ('filler', fillCategoricalNullValues()),\n    ('encoder', OneHotEncoder(sparse=False))\n])","b769da3f":"# Testando\nX_cat_prepared = cat_pipeline.fit_transform(X_cat)\n\n# Verificando\nprint(f'Dimens\u00f5es dos dados antes da prepara\u00e7\u00e3o: {X_cat.shape}')\nprint(f'Dimens\u00f5es dos dados ap\u00f3s a prepara\u00e7\u00e3o: {X_cat_prepared.shape}')","51d61800":"# Definindo atributos num\u00e9ricos\nnum_attribs = filter_attribs(train, cat=False)\nnum_attribs = num_attribs[1:-1]\n\nX_num = X_train.loc[:, num_attribs]\n\n# Criando pipeline num\u00e9rico\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])","9d243fc1":"# Testando\nX_num_prepared = num_pipeline.fit_transform(X_num)\n\n# Verificando\nX_num_prepared[0]","57ce84c8":"# Importando bibliotecas\nfrom sklearn.compose import ColumnTransformer\n\n#  Defini\u00e7\u00f5es iniciais\ncat_attribs = ['GarageType', 'KitchenQual', 'CentralAir', 'HeatingQC', 'MSZoning', 'Foundation', 'ExterQual',\n               'ExterCond']\nnum_attribs = filter_attribs(train, cat=False)\nnum_attribs = num_attribs[1:-1]\n\nX_cat = X_train.loc[:, cat_attribs]\nX_num = X_train.loc[:, num_attribs]\n\n# Criando pipeline num\u00e9rico\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\n# Criando pipeline categ\u00f3rico\ncat_pipeline = Pipeline([\n    ('filler', fillCategoricalNullValues()),\n    ('encoder', OneHotEncoder(sparse=False))\n])\n\n# Criando pipeline completo\nfull_pipeline = ColumnTransformer([\n    ('num', num_pipeline, num_attribs),\n    ('cat', cat_pipeline, cat_attribs)\n])\n\n# Preparando os dados\nX_train_prepared = full_pipeline.fit_transform(X_train)","1803fccc":"# Verificando\nprint(f'Dimens\u00f5es antes da prepara\u00e7\u00e3o: {X_train.shape}')\nprint(f'Dimens\u00f5es ap\u00f3s a prepara\u00e7\u00e3o; {X_train_prepared.shape}')","d49769b0":"# Importando bibliotecas para m\u00e9tricas\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\n# Preparando dados de teste para futuras avalia\u00e7\u00f5es\nX_test_prepared = full_pipeline.fit_transform(X_test)","bcd29e4c":"# Importando regressor\nfrom sklearn.linear_model import LinearRegression\n\n# Treinando modelo\nlin_reg = LinearRegression()\nlin_reg.fit(X_train_prepared, y_train)\n\n# Validando (treino)\ntrain_pred = lin_reg.predict(X_train_prepared)\ntrain_score = np.sqrt(mean_squared_error(train_pred, y_train))\nprint(f'RMSE nos dados de treino: {train_score}')","4f737b89":"# Aplicando valida\u00e7\u00e3o cruzada\nlin_reg_scores = cross_val_score(lin_reg, X_train_prepared, y_train, cv=5, scoring='neg_mean_squared_error')\nlin_reg_cv_score = np.sqrt(-lin_reg_scores).mean()\nlin_reg_cv_score","aec3f3ab":"# Importando regressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Treinando modelo\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(X_train_prepared, y_train)\n\n# Avaliando modelo\ntree_pred = tree_reg.predict(X_train_prepared)\ntree_rmse = np.sqrt(mean_squared_error(y_train, tree_pred))\nprint(f'RMSE Decision Trees Regressor: {tree_rmse}')","e4317717":"# Aplicando cross validation\ntree_scores = cross_val_score(tree_reg, X_train_prepared, y_train, cv=5, scoring='neg_mean_squared_error')\ntree_cv_score = np.sqrt(-tree_scores).mean()\ntree_cv_score","85ad6f20":"# Importando biblioteca\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Treinando modelo\nforest_reg = RandomForestRegressor()\nforest_reg.fit(X_train_prepared, y_train)\n\n# Avaliando modelo\nforest_pred = forest_reg.predict(X_train_prepared)\nforest_score = np.sqrt(mean_squared_error(y_train, forest_pred))\nprint(f'RMSE Random Forest Regressor: {forest_score}')","bd9da935":"# Aplicando valida\u00e7\u00e3o cruzada\nforest_scores = cross_val_score(forest_reg, X_train_prepared, y_train, cv=5, scoring='neg_mean_squared_error')\nforest_cv_score = np.sqrt(-forest_scores).mean()\nforest_cv_score","dc2bd305":"# Importando biblioteca\nfrom sklearn.svm import SVR\n\n# Treinando modelo\nsvr = SVR()\nsvr.fit(X_train_prepared, y_train)\n\n# Avaliando modelo\nsvr_pred = svr.predict(X_train_prepared)\nsvr_score = mean_squared_error(y_train, svr_pred)\nprint(f'RMSE SVR Regressor: {svr_score}')","97d806f4":"# Aplicando valida\u00e7\u00e3o cruzada\nsvr_scores = cross_val_score(svr, X_train_prepared, y_train, cv=5, scoring='neg_mean_squared_error')\nsvr_score = np.sqrt(-svr_scores).mean()\nsvr_score","5985a92a":"# Buscando a melhor combina\u00e7\u00e3o para RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n# Definindo par\u00e2metros\nparam_grid = [\n    {'n_estimators': [30, 40, 50, 75, 90], 'max_features': [10, 12, 15, 20, 25]},\n]\n\n# Criando regressor\nforest_reg = RandomForestRegressor()\n\n# Treinando e procurando a melhor combina\u00e7\u00e3o\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(X_train_prepared, y_train)","d35e714d":"# Verificando scores de cada combina\u00e7\u00e3o\nresults = grid_search.cv_results_\nfor mean_score, params in zip(results['mean_test_score'], results['params']):\n    print(np.sqrt(-mean_score), params)","9a46f2fb":"feature_importances = grid_search.best_estimator_.feature_importances_\ncat_encoder = full_pipeline.named_transformers_['cat'].named_steps['encoder']\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\natributos = num_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, atributos), reverse=True)","5fae06e5":"# Top 10 atributos com maior correla\u00e7\u00e3o com o target\nk = 10\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nfig, ax = plt.subplots(figsize=(18, 7))\nax = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 12}, \n                 yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","92d2c76f":"#  Defini\u00e7\u00f5es iniciais\ncat_attribs = ['GarageType', 'KitchenQual', 'CentralAir', 'HeatingQC', 'MSZoning', 'Foundation', 'ExterQual',\n               'ExterCond']\nnum_attribs = cols[1:]\n\nX_cat = X_train.loc[:, cat_attribs]\nX_num = X_train.loc[:, num_attribs]\n\n# Criando pipeline num\u00e9rico\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\n# Criando pipeline categ\u00f3rico\ncat_pipeline = Pipeline([\n    ('filler', fillCategoricalNullValues()),\n    ('encoder', OneHotEncoder(sparse=False))\n])\n\n# Criando pipeline completo\nfull_pipeline = ColumnTransformer([\n    ('num', num_pipeline, num_attribs),\n    ('cat', cat_pipeline, cat_attribs)\n])\n\n# Preparando os dados\nX_train_prepared = full_pipeline.fit_transform(X_train)","0e8121c9":"# Treinando modelo\n# Importando biblioteca\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Treinando modelo\nforest_reg = RandomForestRegressor()\nforest_reg.fit(X_train_prepared, y_train)\n\n# Avaliando modelo\nforest_pred = forest_reg.predict(X_train_prepared)\nforest_score = np.sqrt(mean_squared_error(y_train, forest_pred))\nprint(f'RMSE Random Forest Regressor: {forest_score}')\n\n# Valida\u00e7\u00e3o cruzada\nforest_scores = cross_val_score(forest_reg, X_train_prepared, y_train, cv=5, scoring='neg_mean_squared_error')\nforest_cv_score = np.sqrt(-forest_scores).mean()\nprint(f'RMSE Cross Validation Random Forest: {forest_cv_score}')","c0017d28":"# Buscando a melhor combina\u00e7\u00e3o para RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n# Definindo par\u00e2metros\nparam_grid = [\n    {'n_estimators': [30, 40, 50, 75, 90], 'max_features': [10, 12, 15, 20, 25]},\n]\n\n# Criando regressor\nforest_reg = RandomForestRegressor()\n\n# Treinando e procurando a melhor combina\u00e7\u00e3o\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(X_train_prepared, y_train)","7edafcdf":"# Melhor score\nforest_best_score = np.sqrt(-grid_search.best_score_)\nforest_best_score","de96ab34":"# Vejamos algumas distribui\u00e7\u00f5es\nfig, ax = plt.subplots(figsize=(10, 5))\nax = sns.distplot(target)\nformat_spines(ax, right_border=False)\nax.set_title('Distribui\u00e7\u00e3o SalePrice', size=15)\nplt.show()","221495e8":"# Aplicando transforma\u00e7\u00e3o logar\u00edtima\ntarget_log = np.log(target+1)\nfig, ax = plt.subplots(figsize=(10, 5))\nax = sns.distplot(target_log)\nformat_spines(ax, right_border=False)\nax.set_title('Distribui\u00e7\u00e3o Logar\u00edtima SalePrice', size=15)\nplt.show()","e787394f":"# Aplicando transforma\u00e7\u00e3o logar\u00edtima\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n\nattribute = X_num['GrLivArea']\nlog_attribute = np.log(attribute+1)\n\nsns.distplot(attribute, ax=axs[0])\nformat_spines(axs[0], right_border=False)\nskew = attribute.skew()\nkurt = attribute.kurt()\naxs[0].set_title(f'GrLivArea \\nSkew: {skew:.4f}, Kurt: {kurt:.4f}', size=15)\n\nsns.distplot(log_attribute, ax=axs[1])\nformat_spines(axs[1], right_border=False)\nlog_skew = log_attribute.skew()\nlog_kurt = log_attribute.kurt()\naxs[1].set_title(f'Log GrLivArea\\n Skew: {log_skew:.4f}, Kurt: {log_kurt:.4f}', size=15)\nplt.show()","ee037fa4":"# Verificando em todos os atributos\nfig, axs = plt.subplots(nrows=9, ncols=2, figsize=(12, 25))\ncolumns = list(X_num.columns)\ni = 0\n\nfor col in columns:\n    # Calculando par\u00e2metros\n    attribute = X_num[col]\n    skew = attribute.skew()\n    kurt = attribute.kurt()\n    log_attribute = np.log1p(attribute)\n    log_skew = log_attribute.skew()\n    log_kurt = log_attribute.kurt()\n    \n    # Plotando gr\u00e1ficos\n    sns.distplot(attribute, ax=axs[i, 0])\n    sns.distplot(log_attribute, ax=axs[i, 1])\n    \n    # Configurando plotagens\n    attr_name = columns[i]\n    format_spines(axs[i, 0], right_border=False)\n    format_spines(axs[i, 1], right_border=False)\n    axs[i, 0].set_title(f'{attr_name}\\n Skew:{skew:.4f}, Kurt:{kurt:.4f}')\n    axs[i, 1].set_title(f'{attr_name}\\n Skew:{log_skew:.4f}, Kurt:{log_kurt:.4f}')\n    i += 1\n\nplt.tight_layout()\nplt.show()","c7a50b4a":"# Criando classe para transforma\u00e7\u00e3o logar\u00edtima\nclass logTransformation(BaseEstimator, TransformerMixin):\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return np.log1p(X)","428a35fb":"# Importando bibliotecas\nfrom sklearn.compose import ColumnTransformer\n\n#  Defini\u00e7\u00f5es iniciais\ncat_attribs = ['GarageType', 'KitchenQual', 'CentralAir', 'HeatingQC', 'MSZoning', 'Foundation', 'ExterQual',\n               'ExterCond']\nnum_attribs = cols[1:]\n\nX_cat = X_train.loc[:, cat_attribs]\nX_num = X_train.loc[:, num_attribs]\n\n# Aplicando log\nX_num_log = np.log(X_num)\n\n# Criando pipeline num\u00e9rico\nnum_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('log', logTransformation()),\n    #('scaler', StandardScaler()),\n])\n\n# Criando pipeline categ\u00f3rico\ncat_pipeline = Pipeline([\n    ('filler', fillCategoricalNullValues()),\n    ('encoder', OneHotEncoder(sparse=False))\n])\n\n# Criando pipeline completo\nfull_pipeline = ColumnTransformer([\n    ('num', num_pipeline, num_attribs),\n    ('cat', cat_pipeline, cat_attribs)\n])\n\n# Preparando os dados\nX_train_prepared = full_pipeline.fit_transform(X_train)\ntarget_log = np.log1p(target)","b5cc1353":"# Treinando modelo com regress\u00e3o linear\nlin_reg = LinearRegression()\nlin_reg.fit(X_train_prepared, target_log)\n\n# Cross validation\nlin_reg_scores = cross_val_score(lin_reg, X_train_prepared, target_log, cv=5, scoring='neg_mean_squared_error')\nlin_reg_cv_score = np.sqrt(-lin_reg_scores).mean()\nlin_reg_cv_score","6fc3faab":"# Aplicando modelos\nfrom sklearn.linear_model import Ridge\n\n# Treinando modelo\nridge_reg = Ridge(alpha=1, solver='cholesky')\nridge_reg.fit(X_train_prepared, target_log)\n# Cross validation\nridge_reg_scores = cross_val_score(ridge_reg, X_train_prepared, target_log, cv=5, scoring='neg_mean_squared_error')\nridge_reg_cv_score = np.sqrt(-ridge_reg_scores).mean()\nridge_reg_cv_score","6f0fa8a5":"# Definindo fun\u00e7\u00e3o para facilitar o c\u00e1lculo da valida\u00e7\u00e3o cruzada\ndef calc_rmse(model, X, y, cv=5, scoring='neg_mean_squared_error'):\n    scores = cross_val_score(model, X, y, cv=cv, scoring=scoring)\n    return np.sqrt(-scores).mean()","48651d84":"calc_rmse(ridge_reg, X_train_prepared, target_log)","1b13c652":"alphas = [0.001, 0.003, 0.01, 0.03, 0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\nridge_scores = []\nfor a in alphas:\n    ridge_scores.append(calc_rmse(Ridge(alpha=a), X_train_prepared, target_log))","d21dd47e":"fig, ax = plt.subplots(figsize=(10, 5))\nax = sns.lineplot(alphas, ridge_scores)\nformat_spines(ax, right_border=False)\nax.set_title('Par\u00e2metros Alpha - Ridge Regression')\nplt.show()","3a26592d":"# Melhor resultado\nbest_ridge_rmse = min(ridge_scores)\nbest_ridge_alpha = alphas[ridge_scores.index(best_ridge_rmse)]\n\nprint(f'Melhor score RMSE: {best_ridge_rmse} com alpha = {best_ridge_alpha}')","51b2652c":"# Importando regressor\nfrom sklearn.linear_model import Lasso\n\nlasso_reg = Lasso()\ncalc_rmse(lasso_reg, X_train_prepared, target_log)","99b838b5":"# Verificando regulariza\u00e7\u00e3o\nalphas = [0.0001, 0.0003, 0.001, 0.003, 0.005]\nlasso_scores = []\nfor a in alphas:\n    lasso_scores.append(calc_rmse(Lasso(alpha=a), X_train_prepared, target_log))\n    \nfig, ax = plt.subplots(figsize=(10, 5))\nax = sns.lineplot(alphas, lasso_scores)\nformat_spines(ax, right_border=False)\nax.set_title('Par\u00e2metros Alpha - Lasso Regression')\nplt.show()","66aeccd6":"# Melhor resultado\nbest_lasso_rmse = min(lasso_scores)\nbest_lasso_alpha = alphas[lasso_scores.index(best_lasso_rmse)]\n\nprint(f'Melhor score RMSE: {best_lasso_rmse} com alpha = {best_lasso_alpha}')","b829e5f2":"# Importando biblioteca e treinando\nfrom sklearn.linear_model import ElasticNet\n\nelastic_reg = ElasticNet()\ncalc_rmse(elastic_reg, X_train_prepared, target_log)","49e706f9":"# Verificando melhor modelo\nalphas = [0.001, 0.01, 0.03, 0.05, 0.1, 0.3]\nratios = np.logspace(0, 1, 10)\/10\nelastic_analysis = pd.DataFrame({})\ncols = ['rmse', 'alpha', 'l1_ratio']\nfor col in cols:\n    elastic_analysis[col] = []\nperformances = {}\n\nfor alpha in alphas:\n    for l1_ratio in ratios:\n        score = calc_rmse(ElasticNet(alpha=alpha, l1_ratio=l1_ratio), X_train_prepared, target_log)\n        performances['rmse'] = score\n        performances['alpha'] = alpha\n        performances['l1_ratio'] = l1_ratio\n        elastic_analysis = elastic_analysis.append(performances, ignore_index=True)","17cacbae":"elastic_analysis","54be5cf5":"# Melhor combina\u00e7\u00e3o\nelastic_analysis.iloc[elastic_analysis['rmse'].argmin(), :]","427fa3e7":"# Importando biblioteca\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Treinando modelo\nforest_reg = RandomForestRegressor()\nforest_reg.fit(X_train_prepared, target_log)\n\n# Avaliando modelo\nforest_pred = forest_reg.predict(X_train_prepared)\nforest_score = np.sqrt(mean_squared_error(target_log, forest_pred))\nprint(f'RMSE Random Forest Regressor: {forest_score}')","08119aa5":"# Aplicando valida\u00e7\u00e3o cruzada\ncalc_rmse(forest_reg, X_train_prepared, target_log)","f8637e95":"# Buscando a melhor combina\u00e7\u00e3o para RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n# Definindo par\u00e2metros\nparam_grid = [\n    {'n_estimators': [30, 40, 50, 75, 90], 'max_features': [10, 12, 15, 20, 25]},\n]\n\n# Criando regressor\nforest_reg = RandomForestRegressor()\n\n# Treinando e procurando a melhor combina\u00e7\u00e3o\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(X_train_prepared, target_log)","4261bad0":"# Verificando scores de cada combina\u00e7\u00e3o\nbest_forest_rmse = np.sqrt(-grid_search.best_score_)\nbest_forest_rmse","dc1a0d0d":"# Passando dados de teste no pipeline de prepara\u00e7\u00e3o\nX_test_prepared = full_pipeline.fit_transform(test)\n\nbest_model = grid_search.best_estimator_\ny_pred = best_model.predict(X_test_prepared)","5211821b":"# Gerando arquivo\nsubmission = pd.DataFrame()\nsubmission['Id'] = test['Id']\nsubmission['SalePrice'] = np.exp(y_pred)\nsubmission.to_csv('submission.csv', index=False)","2a2344a1":"# Verificando\nsubmission.head()","812dc1db":"#### Atributos Categ\u00f3ricos","7c653b1f":"Dando in\u00edcio \u00e0s an\u00e1lises via c\u00f3digo, vamos estudar um pouco sobre os atributos nulos contidos neste conjunto de dados.","c751907e":"#### Random Forest","6acc5d66":"### Pr\u00e9-An\u00e1lise","bb4ea542":"\\begin{equation*}\n    J(\\theta)=MSE(\\theta) + \\alpha\\sum_{i=1}^{n}|\\theta_i|\n\\end{equation*}","27d9028f":"Eis um grande problema! Pensando j\u00e1 nas tratativas dos atributos categ\u00f3ricos para o treinamento do modelo, a aplica\u00e7\u00e3o da classe `One Hot Encoder` do scikit-learn transforma uma feature categ\u00f3rica em `n` novas features de acordo com a quantidade `n` de entradas. Vejamos essa quantidade `n` por atributo.","ad03f583":"**Solu\u00e7\u00e3o 1.1:** Preencher os dados nulos com as entradas mais comuns.\n\n**Solu\u00e7\u00e3o 1.2:** Deletar inst\u00e2ncias com dados nulos.","8951b826":"### Segunda Abordagem","e1ccc9a5":"\\begin{equation*}\n    \\begin{cases}\n        \\alpha = 0 & \\mbox{Ridge Regression = Linear Regression} \\\\\n        \\alpha = alto & \\mbox{Alta penaliza\u00e7\u00e3o nos pesos e redu\u00e7\u00e3o no overfitting} \\\\\n        \\alpha = baixo & \\mbox{Baixa penaliza\u00e7\u00e3o nos pesos e aumento do underfitting}\n    \\end{cases}\n\\end{equation*}","197e9ffd":"# Introdu\u00e7\u00e3o","7bd21d91":"Com a an\u00e1lise a partir dos dados padronizados, obtivemos a mesma curva e as mesmas m\u00e9tricas obtidas com os dados sem nenhum tratamento.","9024f0ae":"#### Random Forest","4a271859":"#### Grid Search CV","e973eb3c":"* **4. Suposi\u00e7\u00f5es:** Iremos verificar numericamente se as conclus\u00f5es obtidas fazem sentido para o problema de neg\u00f3cio proposto.","9e2d9b61":"### Pipeline Num\u00e9rico","0c400f82":"### Terceira Abordagem","6640ed95":"\\begin{equation*}\n    J(\\theta)=MSE(\\theta) + \\alpha\\frac{1}{2}\\sum_{i=1}^{n}\\theta_i^2\n\\end{equation*}","683edc6a":"Ap\u00f3s toda a prepara\u00e7\u00e3o dos dados e dos insights iniciais relacionados aos treinamentos de modelos de Regress\u00e3o, vamos agora aplicar uma transforma\u00e7\u00e3o nos dados que, de certa forma, j\u00e1 foi mencionada anteriormente nos t\u00f3picos de prepara\u00e7\u00e3o: **transforma\u00e7\u00e3o logaritma**.","4965af28":"#### Atributos Num\u00e9ricos","ea79f418":"Podemos dizer que o modelo Elatic Net \u00e9 um meio termo entre o Ridge Regression e o Lasso Regressio. Seu termo de regulariza\u00e7\u00e3o \u00e9 uma `mistura` entre ambos citados anteriormente e \u00e9 poss\u00edvel controlar a taxa de mistura `r`.\n\n* Quando $r=0$, o Elastic Net \u00e9 equivalente ao Ridge Regression\n* Quando $r=1$, o Elastic Net \u00e9 equivalente ao Lasso Regression","3225d96b":"* **3. Estudo Multivari\u00e1vel:** Objetivando o completo entendimento dos dados, ser\u00e1 realizado uma an\u00e1lise contemplando todas as vari\u00e1veis (dependente e independentes), averiguando correla\u00e7\u00f5es e dados estat\u00edsticos.","edf8b929":"Para facilitar o acompanhamento do projeto, o desenvolvimento ser\u00e1 dividido em:\n\n* **1. Entendimento do problema:** Veremos a seguir que o conjunto de dados traz consigo 80 vari\u00e1veis divididas entre dados num\u00e9ricos e categ\u00f3ricos, cada qual descrevendo um atributo espec\u00edfico da moradia (tendo como target, o pre\u00e7o de venda _'SalesPrice'_). Pedro Marcelino, em seu kernel, prop\u00f4s uma metodologia interessante para o entendimento do problema: para entender melhor os atributos, nada mais eficiente de que uma an\u00e1lise filos\u00f3fica caso a caso, verificando as descri\u00e7\u00f5es das features, categorizando-as de acordo com funcionalidades presentes no problema de neg\u00f3cio e, por fim, pontuando uma an\u00e1lise subjetiva sobre a import\u00e2ncia de cada uma das features para predi\u00e7\u00e3o do target de pre\u00e7o. Este pacote de an\u00e1lises poderia ser realizado no Excel, por exemplo. ","0c4fa7c3":"#### Linear Regression","204db6e6":"Excelente! Um resultado bem melhor do que o obtido anteriormente. Vamos testar com outros modelos.","9d4c2b85":"##### Filtrando Atributos","edd46bf1":"A partir da an\u00e1lise explorat\u00f3ria aplicada aos dados, podemos definir um pipeline de transforma\u00e7\u00e3o para auxiliar na prepara\u00e7\u00e3o dos dados em uma forma concisa e eficiente.","ccfb71ae":"Ap\u00f3s algumas tentativas, \u00e9 razo\u00e1vel dizer que os scores realmente ficam em torno de 29.000, o que pode ser considerado alto.","18c6d5b4":"Temos apenas 3 atributos num\u00e9ricos com dados nulos em suas entradas. \u00c9 papel do cientista de dados definir a melhor forma de tratar estes dados nulos. Em uma primeira abordagem, vamos definir um pipeline de preenchimento de dados nulos (atributos num\u00e9ricos) com a mediana do conjunto.","4608d13a":"De fato, depois da filtragem dos atributos categ\u00f3ricos, temos apenas `GarageType` contendo dados nulos. Vamos analisa-lo melhor.","c7c8c0e7":"# Lendo os Dados","73f4598b":"**An\u00e1lise:**\n\n* A vari\u00e1vel target difere da distribui\u00e7\u00e3o normal\n* Temos as medidas de Skewness ([Assimetria](https:\/\/pt.wikipedia.org\/wiki\/Assimetria_(estat%C3%ADstica)) ou Obliquidade) e Kurtosis ([Curtose](https:\/\/pt.wikipedia.org\/wiki\/Curtose) ou Achatamento) que indicam:\n    * [Skewness](https:\/\/en.wikipedia.org\/wiki\/Skewness)\n        * positivo: long tail \u00e0 direita;\n        * negativo: long tail \u00e0 esquerda;\n        * igual a 0: distribui\u00e7\u00e3o aproximadamente sim\u00e9trica.\n    * [Kurtosis](https:\/\/en.wikipedia.org\/wiki\/Kurtosis)\n        * igual a 3: achatamento de uma distribui\u00e7\u00e3o normal;\n        * maior que 3: distribui\u00e7\u00e3o mais alta (afunilada);\n        * menor que 3: distribui\u00e7\u00e3o mais achatada que a normal.","ef182eb4":"**1\u00ba ponto:** forte correla\u00e7\u00e3o positiva entre os atributos `TotalBsmtSF` e `1stFlrSF`","63df6656":"Como informado anteriormente, o preenchimento dos dados nulos ser\u00e1 dado pelas entradas mais comuns dentro dos atributos categ\u00f3ricos.","38b03009":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introdu\u00e7\u00e3o\" data-toc-modified-id=\"Introdu\u00e7\u00e3o-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Introdu\u00e7\u00e3o<\/a><\/span><\/li><li><span><a href=\"#Fun\u00e7\u00f5es-\u00dateis\" data-toc-modified-id=\"Fun\u00e7\u00f5es-\u00dateis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Fun\u00e7\u00f5es \u00dateis<\/a><\/span><\/li><li><span><a href=\"#Lendo-os-Dados\" data-toc-modified-id=\"Lendo-os-Dados-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Lendo os Dados<\/a><\/span><\/li><li><span><a href=\"#Explorando-os-Dados\" data-toc-modified-id=\"Explorando-os-Dados-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Explorando os Dados<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Pr\u00e9-An\u00e1lise\" data-toc-modified-id=\"Pr\u00e9-An\u00e1lise-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Pr\u00e9-An\u00e1lise<\/a><\/span><\/li><li><span><a href=\"#Dados-Nulos\" data-toc-modified-id=\"Dados-Nulos-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>Dados Nulos<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Atributos-Categ\u00f3ricos\" data-toc-modified-id=\"Atributos-Categ\u00f3ricos-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;<\/span>Atributos Categ\u00f3ricos<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Filtrando-Atributos\" data-toc-modified-id=\"Filtrando-Atributos-4.2.1.1\"><span class=\"toc-item-num\">4.2.1.1&nbsp;&nbsp;<\/span>Filtrando Atributos<\/a><\/span><\/li><li><span><a href=\"#Preenchendo-Dados-Nulos\" data-toc-modified-id=\"Preenchendo-Dados-Nulos-4.2.1.2\"><span class=\"toc-item-num\">4.2.1.2&nbsp;&nbsp;<\/span>Preenchendo Dados Nulos<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Atributos-Num\u00e9ricos\" data-toc-modified-id=\"Atributos-Num\u00e9ricos-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;<\/span>Atributos Num\u00e9ricos<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Preenchendo-Dados-Nulos\" data-toc-modified-id=\"Preenchendo-Dados-Nulos-4.2.2.1\"><span class=\"toc-item-num\">4.2.2.1&nbsp;&nbsp;<\/span>Preenchendo Dados Nulos<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Correla\u00e7\u00f5es\" data-toc-modified-id=\"Correla\u00e7\u00f5es-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;<\/span>Correla\u00e7\u00f5es<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Pipeline\" data-toc-modified-id=\"Pipeline-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Pipeline<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Pipeline-Categ\u00f3rico\" data-toc-modified-id=\"Pipeline-Categ\u00f3rico-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>Pipeline Categ\u00f3rico<\/a><\/span><\/li><li><span><a href=\"#Pipeline-Num\u00e9rico\" data-toc-modified-id=\"Pipeline-Num\u00e9rico-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>Pipeline Num\u00e9rico<\/a><\/span><\/li><li><span><a href=\"#Pipeline-Completo\" data-toc-modified-id=\"Pipeline-Completo-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;<\/span>Pipeline Completo<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Treinando-um-Modelo\" data-toc-modified-id=\"Treinando-um-Modelo-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Treinando um Modelo<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Primeira-Abordagem\" data-toc-modified-id=\"Primeira-Abordagem-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;<\/span>Primeira Abordagem<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Linear-Regression\" data-toc-modified-id=\"Linear-Regression-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;<\/span>Linear Regression<\/a><\/span><\/li><li><span><a href=\"#Decision-Trees-Regressor\" data-toc-modified-id=\"Decision-Trees-Regressor-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;<\/span>Decision Trees Regressor<\/a><\/span><\/li><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-6.1.3\"><span class=\"toc-item-num\">6.1.3&nbsp;&nbsp;<\/span>Random Forest<\/a><\/span><\/li><li><span><a href=\"#SVR-Regressor\" data-toc-modified-id=\"SVR-Regressor-6.1.4\"><span class=\"toc-item-num\">6.1.4&nbsp;&nbsp;<\/span>SVR Regressor<\/a><\/span><\/li><li><span><a href=\"#Grid-Search-CV\" data-toc-modified-id=\"Grid-Search-CV-6.1.5\"><span class=\"toc-item-num\">6.1.5&nbsp;&nbsp;<\/span>Grid Search CV<\/a><\/span><\/li><li><span><a href=\"#Feature-Importance\" data-toc-modified-id=\"Feature-Importance-6.1.6\"><span class=\"toc-item-num\">6.1.6&nbsp;&nbsp;<\/span>Feature Importance<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Segunda-Abordagem\" data-toc-modified-id=\"Segunda-Abordagem-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;<\/span>Segunda Abordagem<\/a><\/span><\/li><li><span><a href=\"#Terceira-Abordagem\" data-toc-modified-id=\"Terceira-Abordagem-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;<\/span>Terceira Abordagem<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Linear-Regression\" data-toc-modified-id=\"Linear-Regression-6.3.1\"><span class=\"toc-item-num\">6.3.1&nbsp;&nbsp;<\/span>Linear Regression<\/a><\/span><\/li><li><span><a href=\"#Ridge-Regression\" data-toc-modified-id=\"Ridge-Regression-6.3.2\"><span class=\"toc-item-num\">6.3.2&nbsp;&nbsp;<\/span>Ridge Regression<\/a><\/span><\/li><li><span><a href=\"#Lasso-Regression\" data-toc-modified-id=\"Lasso-Regression-6.3.3\"><span class=\"toc-item-num\">6.3.3&nbsp;&nbsp;<\/span>Lasso Regression<\/a><\/span><\/li><li><span><a href=\"#ELastic-Net\" data-toc-modified-id=\"ELastic-Net-6.3.4\"><span class=\"toc-item-num\">6.3.4&nbsp;&nbsp;<\/span>ELastic Net<\/a><\/span><\/li><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-6.3.5\"><span class=\"toc-item-num\">6.3.5&nbsp;&nbsp;<\/span>Random Forest<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Gerando-Predi\u00e7\u00f5es\" data-toc-modified-id=\"Gerando-Predi\u00e7\u00f5es-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Gerando Predi\u00e7\u00f5es<\/a><\/span><\/li><\/ul><\/div>","36df3090":"#### SVR Regressor","8e897e54":"A conclus\u00e3o final para esta segunda abordagem \u00e9 a de que, mesmo selecionando apenas os atributos com maiores \u00edndices de correla\u00e7\u00e3o com nosso target `SalePrice`, n\u00e3o foi poss\u00edvel obter melhores resultados atrav\u00e9s do treinamento com o algoritmo `RandomForest`. A partir disso, vamos tentar uma terceira abordagem com o objetivo de aumentar definitivamente o score rmse.","34ea5a96":"Vejamos uma matriz de correla\u00e7\u00e3o para entender melhor a situa\u00e7\u00e3o dos atributos num\u00e9ricos.","e3a7b18f":"* **2. Estudo Univari\u00e1vel:** Ap\u00f3s esse mergulho no conjunto de dados proporcionado, a ideia \u00e9 analisar separadamente nossa vari\u00e1vel target _SalesPrice_, retirando insights relacionados \u00e0s conclus\u00f5es obtidas no passo **1** e verificando se realmente fazem sentido.","ccaf9bac":"# Pipeline","1bafb0af":"Vamos agora retornar os atributos num\u00e9ricos e analisar a influ\u00eancia estabelecida na pr\u00e9-an\u00e1lise","5160fcf8":"Este notebook tem como objetivo alocar o desenvolvimento pr\u00e1tico e te\u00f3rico da competi\u00e7\u00e3o do [Kaggle](https:\/\/www.kaggle.com\/) de n\u00edvel iniciante: [Housing Prices - Advanced Regression Techniques](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques). Baseado no excelente kernel [Comprehensive data exploration with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python), de [Pedro Marcelino](https:\/\/www.kaggle.com\/pmarcelino) em fevereiro de 2017, a inten\u00e7\u00e3o desta primeira abordagem \u00e9 analisar o conjunto de dados fornecido e averiguar as rela\u00e7\u00f5es e os insights retirados atrav\u00e9s de an\u00e1lises estat\u00edsticas.","d993c956":"#### Linear Regression","11a21716":"Vamos realizar algumas an\u00e1lises relacionadas ao target do modelo (`SalePrice`)","eeaa3ab8":"Antes de comentar sobre o resultado obtido com a matriz acima, \u00e9 necess\u00e1rio entender que:\n* Quadrados claros indicam uma forte correla\u00e7\u00e3o positiva;\n* Quadrados escuros indicam uma forte correla\u00e7\u00e3o negativa.","3b1a2720":"Bom, n\u00e3o podemos dizer que tivemos um aproveitamento de 100%, entretando ao menos tivemos o bom senso de indicar influ\u00eancia `M\u00e9dia` a alguns atributos com influ\u00eancia `Alta`. Analisando individualmente estes atributos, temos:\n\n* **OverallQual:** Ok. Mencionamos, em t\u00f3picos anteriores, que este atributo poderia ser muito importante para a an\u00e1lise;\n* **GrLivArea:** A sala de estar realmente importa para a constru\u00e7\u00e3o do pre\u00e7o da casa e sua respectiva venda. Faz sentido.\n* **GarageCars** e **GarageArea**: Aparentemente o espa\u00e7o proporcionado pela garagem causa bastante influ\u00eancia no pre\u00e7o de venda da casa. Por\u00e9m, como discutido acima, ambos s\u00e3o atributos com alta correla\u00e7\u00e3o entre si, dado que a \u00e1rea calculada da garagem \u00e9 baseada na quantidade de carros que esta comporta. Podemos simplesmente manter o atributo GarageCars, uma vez que sua correla\u00e7\u00e3o com o target SalePrice \u00e9 maior.\n* **TotalBsmtSF** e **1stFlrSF**: Assim como no caso acima, ambos s\u00e3o atributos com alta correla\u00e7\u00e3o entre si. Iremos manter TotalBsmtSF com a mesma justificativa do t\u00f3pico logo acima.\n* **FullBath:** Ok, de certa forma isso era esperado. A quantidade de banheiros em uma casa realmente possui influ\u00eancia em seu pre\u00e7o de venda;\n* **TotRmsAbvGrd:** Tamb\u00e9m trazendo total sentido, a quantidade de quartos \u00e9 importante para o c\u00e1lculo do pre\u00e7o da casa.\n* **YearBuilt:** Tamb\u00e9m concordo que o ano de constru\u00e7\u00e3o da casa pode influenciar em seu pre\u00e7o.","968c2616":"\u00c9 not\u00e1vel que alguns atributos, na pr\u00e9-an\u00e1lise, foram classificados como Categ\u00f3ricos quando, na verdade, foram designados via c\u00f3digo ao grupo dos atributos num\u00e9ricos. Um dos fatores que podem ter levado a essa conclus\u00e3o diz respeito ao significado real do atributo, dado que, em alguns casos, temos n\u00fameros inteiros indicando categorias (1 = 'A', 2 = 'B'), fazendo com que, por linha de c\u00f3digo, esses atributos tenham sido caracterizados como Num\u00e9rico quando, na verdade, traduzem informa\u00e7\u00f5es categ\u00f3ricas.","989e60b6":"Adicionalmente, outros kernels postados no pr\u00f3prio Kaggle ser\u00e3o utilizados como base para solu\u00e7\u00e3o das d\u00favidas que surgirem durante o desenvolvimento deste projeto. De cara, \u00e9 poss\u00edvel citar o kernel [Handling Missing Values](https:\/\/www.kaggle.com\/dansbecker\/handling-missing-values) do instrutor do Kaggle DanB e tamb\u00e9m o excelente kernel [Regularized Linear Models](https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models) de Alexandro Papiu. M\u00e3os \u00e0 obra!","d522552a":"Este termo, adicionado a `fun\u00e7\u00e3o custo`, for\u00e7a o modelo a n\u00e3o apenas se enquadrar aos dados, mas tamb\u00e9m manter os `pesos` do modelo t\u00e3o baixos quanto poss\u00edvel. Este termo de regulariza\u00e7\u00e3o somente deve ser adicionado a fun\u00e7\u00e3o custo durante o treinamento. Uma vez treinado o modelo, sua performance deve ser medida sem regulariza\u00e7\u00e3o.","f09a2c6f":"TODO:\n\n    - Investigar os motivos do resultado ter sido EXTREMAMENTE P\u00c9SSIMO na valida\u00e7\u00e3o cruzada!","20b25401":"Vejamos ap\u00f3s a valida\u00e7\u00e3o cruzada","bb0d14f7":"Seguindo a primeira solu\u00e7\u00e3o, ficar\u00edamos apenas com os atributos _ExterQual_, _ExterCond_, _MSZoning_, _KitchenQual_, _CentralAir_, _HeatingQC_ e _GarageType_.","cd25e708":"Tamb\u00e9m conhecida como _Tikhonov Regulariation_, o modelo de Ridge Regression \u00e9 uma forma regularizada da Regress\u00e3o Linear com um termo de regulariza\u00e7\u00e3o igual a:\n\n\\begin{equation*}\n    \\alpha\\sum_{i=1}^{n}\\theta_i^2 \\text{ = }l_2 \\text{ penalty}\n\\end{equation*} ","4d3845be":"#### Ridge Regression","a0b8afba":"**An\u00e1lise:**\n\n* O valor m\u00ednimo \u00e9 maior que 0, ou seja, n\u00e3o \u00e9 necess\u00e1rio de preocupar com particularidades durante a constru\u00e7\u00e3o do modelo;","6009d490":"\\begin{equation*}\n    J(\\theta)=MSE(\\theta) + r\\alpha\\sum_{i=1}^n|\\theta_i|+\n              \\frac{1-r}{2}\\alpha\\sum_{i=1}^n\\theta_i^2\n\\end{equation*}","efc9e057":"### Dados Nulos","bd487ca0":"Vejamos o procedimento adotado e alguns dos insights retirados.","106846d2":"**Lembrando:**\n\n* Temos as medidas de Skewness ([Assimetria](https:\/\/pt.wikipedia.org\/wiki\/Assimetria_(estat%C3%ADstica)) ou Obliquidade) e Kurtosis ([Curtose](https:\/\/pt.wikipedia.org\/wiki\/Curtose) ou Achatamento) que indicam:\n    * [Skewness](https:\/\/en.wikipedia.org\/wiki\/Skewness)\n        * positivo: long tail \u00e0 direita;\n        * negativo: long tail \u00e0 esquerda;\n        * igual a 0: distribui\u00e7\u00e3o aproximadamente sim\u00e9trica.\n    * [Kurtosis](https:\/\/en.wikipedia.org\/wiki\/Kurtosis)\n        * igual a 3: achatamento de uma distribui\u00e7\u00e3o normal;\n        * maior que 3: distribui\u00e7\u00e3o mais alta (afunilada);\n        * menor que 3: distribui\u00e7\u00e3o mais achatada que a normal.","2a70df98":"### Primeira Abordagem","508a5035":"**2\u00ba ponto:** forte correla\u00e7\u00e3o positiva entre os atributos `GarageCars` e `GarageArea`","04c46faf":"Com rela\u00e7\u00e3o a quantidade, temos praticamente o mesmo n\u00famero de entradas nulas para treino e para teste. Quanto ao total de atirbutos com entradas nulas, o conjunto de teste apresenta um n\u00famero significativamente maior. Vejamos detalhes sobre tais atributos.","c2f6feae":"**An\u00e1lise:**\n\n* Essa forte correla\u00e7\u00e3o positiva entre atributos indica uma situa\u00e7\u00e3o de _multicolinearidade_. Isso indica que, possivelmente, tais atributos podem estar representando a mesma informa\u00e7\u00e3o, o que indesej\u00e1vel para o modelo.\n\n* Um outro ponto extremamente importante diz respeito \u00e0s vari\u00e1veis que possuem alta correla\u00e7\u00e3o com nosso target `SalePrice`, sendo estas: `OverallQual`, `GrLivarea`, `GarageCars`, `GarageArea` e mais algumas.","f37a839a":"# Fun\u00e7\u00f5es \u00dateis","434bf075":"# Gerando Predi\u00e7\u00f5es","bc94b125":"Segundo o usu\u00e1rio _deja vu_ em seu kernel [House Prices: EDA to ML (Beginner)](https:\/\/www.kaggle.com\/dejavu23\/house-prices-eda-to-ml-beginner), a falta de simetria na distribui\u00e7\u00e3o do target `SalePrice` pode reduzir a performance do modelo de Machine Learning, dado que os modelos de regress\u00e3o assumem uma distribui\u00e7\u00e3o normal. Utilizando funcionalidades do `numpy`, \u00e9 poss\u00edvel aplicar uma transforma\u00e7\u00e3o logar\u00edtimica nos dados. Vejamos:","10339214":"Perfeito! Definimos nosso pipeline para filtrar e tratar dados nulos em atributos categ\u00f3ricos. Vejamos agora os procedimentos que ser\u00e3o realizados para os atributos num\u00e9ricos.","d7cd9f1e":"Experi\u00eancia: se aplic\u00e1ssemos uma t\u00e9cnica de normaliza\u00e7\u00e3o (`StandardScaler`), ter\u00edamos resultado semelhante?","a5bd7ddc":"Vejamos ap\u00f3s a valida\u00e7\u00e3o cruzada","985379fe":"Temos alguns problemas:\n\n* Atributos com grandes quantidades de dados nulos;\n* Atributos com diferen\u00e7as na quantidade de entradas entre os conjuntos de treino e teste (diff > 0);\n\n**Solu\u00e7\u00e3o 1:** Deletar todos os atributos categ\u00f3ricos que n\u00e3o tenham _alta_ influ\u00eancia no target e com diferen\u00e7a de entradas entre os dados de treino e teste, evitando problemas com o _one hot encoder_. O preenchimento dos dados nulos dos atributos restantes (se existirem) ser\u00e1 dado pelas entradas mais comuns.;\n\n**Solu\u00e7\u00e3o 2:** Deletar atributos categ\u00f3ricos com presen\u00e7a de dados nulos em uma quantidade acima de um threshold a ser determinado, independente da influ\u00eancia no target. O preenchimento dos dados nulos seria dado pela entrada mais comum;","1c6e67c6":"### Pipeline Categ\u00f3rico","894f0e3f":"Diferente do Ridge Regression, o Lasso Regression aplica uma `penaliza\u00e7\u00e3o` do tipo $l_1$ que, por sua vez, faz com que os coeficientes das features de menor import\u00e2ncia tenham tend\u00eancia a `zerar`. Portanto, \u00e9 poss\u00edvel dizer que este tipo de penalidade pode atuar bem como uma `feature selection` no caso de haver muitas features.","9c99c770":"Como sabemos, os modelos de \u00c1rvores de Decis\u00e3o tendem ao Overfitting dos dados. O \"excelente\" score obtido acima n\u00e3o deve ser puramente levado em considera\u00e7\u00e3o, dado que aplicamos uma simples predi\u00e7\u00e3o com os pr\u00f3prios dados de treino para o modelo. Para avaliar o real comportamento do modelo de \u00c1rvore de Decis\u00e3o, precisamos aplicar a `valida\u00e7\u00e3o cruzada`.","c60344e9":"Vejamos estes efeitos em algumas outras vari\u00e1veis","a678b1ab":"# Treinando um Modelo","b1a42602":"Filtrando apenas atributos num\u00e9ricos com correla\u00e7\u00e3o alta com o target SalePrice.","2c568896":"**Diagn\u00f3stico:**\n\nTemos quantidades diferentes de entradas nulas nos dois conjuntos de dados dispon\u00edveis (treino e teste). A pr\u00e9-an\u00e1lise ir\u00e1 auxiliar na decis\u00e3o sobre o tratamento destes dados nulos de acordo com caracter\u00edsticas como _quantidade_, _expectativa (influ\u00eancia)_ e _tipo primitivo_.","1c23ea21":"##### Preenchendo Dados Nulos","07f01905":"De acordo com os resultados obtidos, \u00e9 poss\u00edvel concluir que provavelmente todos os modelos ir\u00e3o apresentar um melhor score a partir da busca com Grid Search CV. Vamos aplic\u00e1-lo, a princ\u00edpio no algoritmo de Random Forest","bf30b2d4":"##### Preenchendo Dados Nulos","8e1829ea":"Antes de realizar o treinamento dos modelos, vamos criar uma fun\u00e7\u00e3o para armazenar os scores obtidos ao longo dos testes. Dessa forma, ser\u00e1 poss\u00edvel comparar alguns par\u00e2metros de modo a selecionar o melhor modelo para este nosso problema de neg\u00f3cio.","68f53337":"#### Feature Importance","fb82251e":"Segundo o usu\u00e1rio _deja vu_ em seu kernel [House Prices: EDA to ML (Beginner)](https:\/\/www.kaggle.com\/dejavu23\/house-prices-eda-to-ml-beginner), a falta de simetria na distribui\u00e7\u00e3o do target `SalePrice` pode reduzir a performance do modelo de Machine Learning, dado que os modelos de regress\u00e3o assumem uma distribui\u00e7\u00e3o normal. Utilizando funcionalidades do `numpy`, \u00e9 poss\u00edvel aplicar uma transforma\u00e7\u00e3o logar\u00edtimica nos dados. Vejamos:","504689ef":"Vejamos se, na pr\u00e9-an\u00e1lise realizada, conseguimos captar o que os dados acabaram de nos informar","08f4171a":"Neste momento, antes de avaliar as poss\u00edveis decis\u00f5es com maior profundidade, vamos analisar um ponto importante: os atributos _Categ\u00f3ricos_.","eee2c9e2":"Juntando as duas etapas","378fa46a":"\u00c9 importante aplicar a `normaliza\u00e7\u00e3o` dos dados _(Standard Scaler)_ antes de performar o modelo Ridge Regression!","d738219d":"Tendo estes dados em m\u00e3os, as an\u00e1lises futuras se tornam extremamente mais f\u00e1ceis, visto que, neste momento, \u00e9 poss\u00edvel ter como refer\u00eancia um local de consulta que, mesmo apresentando certo vi\u00e9s subjetivo, contempla observa\u00e7\u00f5es sobre o problema do ponto de vista do cliente\/usu\u00e1rio do modelo. Um dos pontos principais dessa an\u00e1lise est\u00e1 relacionado a categoriza\u00e7\u00e3o de features com poss\u00edvel _Alta_ import\u00e2ncia pro algoritmo (foram designadas 20 no total) e precisamos saber se isso realmente faz sentido.","c64c3d9d":"Pela an\u00e1lise do cabe\u00e7alho do conjunto de dados com o m\u00e9todo `head()` j\u00e1 foi poss\u00edvel ter uma no\u00e7\u00e3o sobre dados nulos, dados categ\u00f3ricos, dados num\u00e9ricos e qualquer outra intui\u00e7\u00e3o inicial obtida com essa observa\u00e7\u00e3o. Futuramente, iremos avaliar com mais calma cada um desses subconjuntos mencionados, mas antes, \u00e9 preciso expor as an\u00e1lises individuais propostas para cada um dos atributos contidos no conjunto.","ad808df3":"Vamos somar o total de quantidade de entradas dos dois conjuntos","5f65f742":"# Explorando os Dados","4a1b4742":"O termo de regulariza\u00e7\u00e3o aplicado representa uma penalidade do tipo $l_2$ que, por sua vez, tem como principal caracter\u00edstica a penaliza\u00e7\u00e3o elevada _(quadrado)_ a medida que a magnitude dos par\u00e2metros s\u00e3o altas. [l1_l2_penalties](https:\/\/towardsdatascience.com\/l1-and-l2-regularization-methods-ce25e7fc831c)","8d4e68fd":"Tamb\u00e9m conhecido como _Least Absolut Shrinkage and Selection Operator Regression_ e simplesmente chamado de _Lasso Regression_ \u00e9 uma outra vers\u00e3o da Regress\u00e3o Linear, tamb\u00e9m adicionando um termo de regulariza\u00e7\u00e3o \u00e0 fun\u00e7\u00e3o custo.","0761becf":"Com os atributos num\u00e9ricos devidamente indexados, vamos agora visualizar a correla\u00e7\u00e3o com o target `SalesPrice`. Para isso, vamos avaliar o \u00edndice de correla\u00e7\u00e3o por c\u00f3digo e compar\u00e1-lo a nossa an\u00e1lise inicial.","a6e1d773":"#### ELastic Net","178ddeae":"TODO\n\n    - Plotar pairplot com correla\u00e7\u00f5es de todas as vari\u00e1veis;\n    - Analisar resultados (https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python);\n    - Verificar necessidade de tratar outliers nessa primeira abordagem;\n    - Analisar transforma\u00e7\u00f5es logaritmicas nos dados;\n    - Entender plotagem probabil\u00edstica (Theoretical Quantiles x Ordered Values);\n    - Criar pipeline de transforma\u00e7\u00e3o;\n    - Treinar modelo;\n    - Analisar resultados.","729dca45":"#### Lasso Regression","4f8b7169":"Com os dados preparados, podemos estudar implementa\u00e7\u00f5es para esta primeira abordagem.","a0c2a2af":"### Pipeline Completo","2c90c9fc":"### Correla\u00e7\u00f5es","5e3ab05e":"#### Decision Trees Regressor"}}