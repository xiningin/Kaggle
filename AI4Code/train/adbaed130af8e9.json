{"cell_type":{"be016e51":"code","98c3b259":"code","0eb917ac":"code","e4db333f":"code","a82a590f":"code","183dc72a":"code","ed831909":"code","dcdd82eb":"code","78da9b60":"code","3f6287e5":"code","f57e0c56":"code","2b2b970b":"code","1f56bde8":"code","7c25f479":"code","8691c13e":"code","6d558818":"code","3eec7949":"code","190cd40f":"code","40a698cb":"code","e1fc6039":"code","04b4a37e":"code","1f0eaaa5":"code","8a2660ec":"code","991c026b":"code","f0fc0fb7":"code","85143f69":"code","56406dce":"code","f42e1b06":"code","c7f4d73e":"code","05354cda":"code","7538a73c":"code","afdb706f":"code","08f4c0ee":"code","59f812b8":"code","de83d0e4":"code","4a8babc6":"code","1a639422":"code","2c694df5":"code","fb0597bf":"code","01553b7f":"code","edc868c6":"code","f64d58db":"code","8d91fdec":"code","6779bb32":"code","a6c86367":"code","880f4a34":"code","6aad106a":"code","936966d4":"code","b0a9be1b":"code","c2679d6b":"code","d2e2b93f":"code","64108418":"code","7e6b98d1":"code","852a99c5":"code","d0dc5be7":"code","92ef658a":"code","8cdff19a":"code","eeb2ac10":"code","f90ad0c6":"code","80c22a22":"code","aaa41767":"code","5c8ecae5":"code","70c3dc89":"code","72e11e33":"markdown","4595b398":"markdown","cad8c264":"markdown","6a05bc2b":"markdown","018827bf":"markdown","70ea8430":"markdown","890384e2":"markdown","8f017e8e":"markdown","a4b7667f":"markdown","ed260642":"markdown","c69409cb":"markdown","e7dfd423":"markdown","c1cfd8c8":"markdown","07d5297e":"markdown","465e0e89":"markdown","6b0f0eab":"markdown","82b6641e":"markdown","e25ee23d":"markdown","f5997f3e":"markdown","b0ab061c":"markdown","d0403f24":"markdown","1aab2940":"markdown","c2f2a559":"markdown","39d1efe5":"markdown","1e66f67f":"markdown","be748a47":"markdown","34dec7ce":"markdown","abc78722":"markdown","47e88ac0":"markdown","4c177b1e":"markdown","c0db82a1":"markdown","10896333":"markdown","6fe9c830":"markdown","d74576e9":"markdown","1be4a9ea":"markdown","7b246ec3":"markdown","1ed0de18":"markdown","4e66f7e1":"markdown","bcd2cad0":"markdown","2df5f03b":"markdown","6b3670f2":"markdown","efb6778a":"markdown","4f3d8238":"markdown","e5a08d77":"markdown","7a873b89":"markdown","eaccad2b":"markdown","2661fc29":"markdown"},"source":{"be016e51":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","98c3b259":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve\nfrom sklearn.metrics import roc_auc_score, roc_curve, f1_score, precision_score, recall_score, accuracy_score, auc\n\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport optuna\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\nseed=42\n\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlesize=14, titlepad=10)","0eb917ac":"def num_plot(df, col, title, symb):\n    fig, ax = plt.subplots(2, 1, sharex=True, figsize=(8,5),gridspec_kw={\"height_ratios\": (.2, .8)})\n    ax[0].set_title(title,fontsize=18)\n    sns.boxplot(x=col, data=df, ax=ax[0])\n    ax[0].set(yticks=[])\n    sns.histplot(x=col, data=df, ax=ax[1])\n    ax[1].set_xlabel(col, fontsize=16)\n    plt.axvline(df[col].mean(), color='darkgreen', linewidth=2.2, label='mean=' + str(np.round(df[col].mean(),1)) + symb)\n    plt.axvline(df[col].median(), color='red', linewidth=2.2, label='median='+ str(np.round(df[col].median(),1)) + symb)\n    plt.axvline(df[col].mode()[0], color='purple', linewidth=2.2, label='mode='+ str(np.round(df[col].mode()[0],1)) + symb)\n    plt.legend(bbox_to_anchor=(1, 1.03), ncol=1, fontsize=17, fancybox=True, shadow=True, frameon=True)\n    plt.tight_layout()\n    plt.show()","e4db333f":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv', skipinitialspace=True)","a82a590f":"df.head()","183dc72a":"df.shape","ed831909":"df[df.duplicated()].shape","dcdd82eb":"df = df[~df.duplicated()]","78da9b60":"df.drop('Time', axis=1, inplace=True)","3f6287e5":"df[df.duplicated()]['Class'].value_counts()","f57e0c56":"df = df[~df.duplicated()]","2b2b970b":"df.shape","1f56bde8":"sns.kdeplot(x='Amount', data=df)\nplt.title('Amount distribution')\nplt.show()","7c25f479":"df['Amount'].skew()","8691c13e":"df['Amount_bc'] = boxcox1p(df['Amount'], boxcox_normmax(df['Amount'] + 1))","6d558818":"num_plot(df,'Amount_bc', 'Box Cox transform on Amount','$')","3eec7949":"df['Amount_log'] = np.log(1 + df['Amount'])","190cd40f":"num_plot(df,'Amount_log', 'Log transform on Amount','$')","40a698cb":"df.drop(['Amount','Amount_log'], axis=1, inplace=True)","e1fc6039":"perc_1 = len(df[df['Class']==1])\/len(df)\nperc_0 = len(df[df['Class']==0])\/len(df)\n\nlabels=['No','Yes']\n\nplt.figure(figsize=(4,4))\nax = sns.countplot(x='Class', data=df, palette='mako_r')\nax.bar_label(ax.containers[0])\nplt.title('Target variable count')\nplt.yticks([])\nplt.figtext(.7, .7, \"Fraud?\\nYes:{:.2f}%\\nNo:{:.2f}%\".format(perc_1*100,perc_0*100), bbox ={'facecolor':'white', \n                   'alpha':0.3, 'pad':5}, fontsize=13)\nplt.xticks(df['Class'].value_counts().index, labels)\nplt.show()","04b4a37e":"X = df.drop(\"Class\",axis=1)\ny = df.Class","1f0eaaa5":"scaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)","8a2660ec":"from sklearn.decomposition import PCA\npca = PCA()\npca.fit_transform(X_scaled);","991c026b":"cum_sum = np.cumsum(pca.explained_variance_ratio_)*100\ncomp= [n for n in range(len(cum_sum))]","f0fc0fb7":"plt.figure(figsize=(7,5))\nplt.plot(comp, cum_sum, marker='.')\nplt.xlabel('PCA Components')\nplt.ylabel('Cumulative Explained Variance (%)')\nplt.title('PCA')\nplt.show()","85143f69":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify = y, random_state=seed)","56406dce":"scaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","f42e1b06":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=seed)","c7f4d73e":"print(f\"Training set: {X_train.shape[0]}\")\nprint(f\"Validation set: {X_val.shape[0]}\")\nprint(f\"Test set: {X_test.shape[0]}\")","05354cda":"EPOCHS = 100\nBATCH_SIZE = 256\ndef objective(trial):\n\n    model = keras.Sequential()\n\n    in_feat = X_train.shape[0]\n\n    for i in range(trial.suggest_int(\"n_layers\", 1, 2)):\n        out_feat = trial.suggest_int(\"n_units_{}\".format(i+1), 1, 40)\n        model.add(keras.layers.Dense(units=out_feat, activation='relu'))\n        model.add(keras.layers.Dropout(trial.suggest_uniform(\"dropout_{}\".format(i+1), 0.2, 0.5)))\n        in_feat=out_feat\n \n    model.add(keras.layers.Dense(1, activation='sigmoid'))\n    \n    model.compile(loss='binary_crossentropy',\n                  optimizer=keras.optimizers.Adam(trial.suggest_float(\"lr\", 5e-5, 1e-2, log=True)),\n                  metrics=['accuracy'])\n    \n    early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True)\n    \n    model.fit(X_train, y_train,\n              validation_data = (X_val, y_val),\n              shuffle = True,\n              batch_size = BATCH_SIZE,\n              epochs = EPOCHS,\n              callbacks = [early_stop], \n              verbose = False )\n    \n    score = model.evaluate(X_val, y_val, verbose=0)\n                  \n    return score[1]","7538a73c":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=50)\n\nprint(\"Number of finished trials: {}\".format(len(study.trials)))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nparams_1 = []\n\nfor key, value in trial.params.items():\n    params_1.append(value)\n    print(\"    {}: {}\".format(key, value))","afdb706f":"n_layers_1 = params_1[0]\n\nunits_1_1 = params_1[1]\ndropout_1_1 = np.round(params_1[2],4)\n\nlr_1 = np.round(params_1[3],8)","08f4c0ee":"model= keras.Sequential()\n\nmodel.add(keras.layers.Dense(units=units_1_1, activation='relu'))\nmodel.add(keras.layers.Dropout(dropout_1_1))\n\nmodel.add(keras.layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer=keras.optimizers.Adam(learning_rate=lr_1),\n              metrics=['accuracy'])\n\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=10, restore_best_weights=True)\n\nmodel.fit(X_train, y_train,\n              validation_data = (X_val, y_val),\n              shuffle = True,\n              batch_size = BATCH_SIZE,\n              epochs = EPOCHS,\n              callbacks = [early_stop], \n)","59f812b8":"model.summary()","de83d0e4":"score = model.evaluate(X_test, y_test, verbose=0)  ","4a8babc6":"y_pred_prob_1 = model.predict(X_test)\ny_pred_1 = y_pred_prob_1.round()","1a639422":"print(classification_report(y_test, y_pred_1, target_names = ['No','Yes']))","2c694df5":"y_train.value_counts()","fb0597bf":"rus = RandomUnderSampler(random_state=seed)","01553b7f":"X_train_us, y_train_us = rus.fit_resample(X_train, y_train)","edc868c6":"y_train_us.value_counts()","f64d58db":"X_train_us, X_val_us, y_train_us, y_val_us = train_test_split(X_train_us, y_train_us, test_size=0.2, random_state = seed)","8d91fdec":"EPOCHS = 100\nBATCH_SIZE = 1\ndef objective(trial):\n\n    model = keras.Sequential()\n\n    in_feat = X_train.shape[0]\n\n    for i in range(trial.suggest_int(\"n_layers\", 1, 2)):\n        out_feat = trial.suggest_int(\"n_units_{}\".format(i+1), 1, 40)\n        model.add(keras.layers.Dense(units=out_feat, activation='relu'))\n        model.add(keras.layers.Dropout(trial.suggest_uniform(\"dropout_{}\".format(i+1), 0.2, 0.5)))\n        in_feat=out_feat\n \n    model.add(keras.layers.Dense(1, activation='sigmoid'))\n    \n    model.compile(loss='binary_crossentropy',\n                  optimizer=keras.optimizers.Adam(trial.suggest_float(\"lr\", 5e-5, 1e-2, log=True)),\n                  metrics=['accuracy'])\n    \n    early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True)\n    \n    model.fit(X_train_us, y_train_us,\n              validation_data = (X_val_us, y_val_us),\n              shuffle = True,\n              batch_size = BATCH_SIZE,\n              epochs = EPOCHS,\n              callbacks = [early_stop], \n              verbose = False )\n    \n    score = model.evaluate(X_val_us, y_val_us, verbose=0)\n                  \n    return score[1]","6779bb32":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=50)\n\nprint(\"Number of finished trials: {}\".format(len(study.trials)))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nparams_2 = []\n\nfor key, value in trial.params.items():\n    params_2.append(value)\n    print(\"    {}: {}\".format(key, value))","a6c86367":"n_layers_2 = params_2[0]\n\nunits_1_2 = params_2[1]\ndropout_1_2 = np.round(params_2[2],4)\n\nunits_2_2 = params_2[3]\ndropout_2_2 = np.round(params_2[4],4)\n\n\nlr_2 = np.round(params_2[5],8)","880f4a34":"model= keras.Sequential()\n\nmodel.add(keras.layers.Dense(units=units_1_2, activation='relu'))\nmodel.add(keras.layers.Dropout(dropout_1_2))\n\nmodel.add(keras.layers.Dense(units=units_2_2, activation='relu'))\nmodel.add(keras.layers.Dropout(dropout_2_2))\n\nmodel.add(keras.layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer=keras.optimizers.Adam(learning_rate=lr_2),\n              metrics=['accuracy'])\n\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=10, restore_best_weights=True)\n\nmodel.fit(X_train_us, y_train_us,\n              validation_data = (X_val_us, y_val_us),\n              shuffle = True,\n              batch_size = BATCH_SIZE,\n              epochs = EPOCHS,\n              callbacks = [early_stop], \n)","6aad106a":"model.summary()","936966d4":"score = model.evaluate(X_test, y_test, verbose=0)  ","b0a9be1b":"y_pred_prob_2 = model.predict(X_test)\ny_pred_2 = y_pred_prob_2.round()","c2679d6b":"print(classification_report(y_test, y_pred_2, target_names = ['No','Yes']))","d2e2b93f":"y_train.value_counts()","64108418":"smote = SMOTE(random_state=seed)\n\nX_train_os, y_train_os = smote.fit_resample(X_train, y_train)\n\ny_train_os.value_counts()","7e6b98d1":"X_train_os, X_val_os, y_train_os, y_val_os = train_test_split(X_train_os, y_train_os, stratify=y_train_os, test_size=0.2, random_state = seed)","852a99c5":"EPOCHS = 100\nBATCH_SIZE = 1024\ndef objective(trial):\n\n    model = keras.Sequential()\n\n    in_feat = X_train.shape[0]\n\n    for i in range(trial.suggest_int(\"n_layers\", 1, 3)):\n        out_feat = trial.suggest_int(\"n_units_{}\".format(i+1), 1, 30)\n        model.add(keras.layers.Dense(units=out_feat, activation='relu'))\n        model.add(keras.layers.Dropout(trial.suggest_uniform(\"dropout_{}\".format(i+1), 0.2, 0.5)))\n        in_feat=out_feat\n \n    model.add(keras.layers.Dense(1, activation='sigmoid'))\n    \n    model.compile(loss='binary_crossentropy',\n                  optimizer=keras.optimizers.Adam(trial.suggest_float(\"lr\", 5e-5, 1e-2, log=True)),\n                  metrics=['accuracy'])\n    \n    early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True)\n    \n    model.fit(X_train_os, y_train_os,\n              validation_data = (X_val_os, y_val_os),\n              shuffle = True,\n              batch_size = BATCH_SIZE,\n              epochs = EPOCHS,\n              callbacks = [early_stop], \n              verbose = False )\n    \n    score = model.evaluate(X_val_os, y_val_os, verbose=0)\n                  \n    return score[1]","d0dc5be7":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=50)\n\nprint(\"Number of finished trials: {}\".format(len(study.trials)))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nparams_3 = []\n\nfor key, value in trial.params.items():\n    params_3.append(value)\n    print(\"    {}: {}\".format(key, value))","92ef658a":"n_layers_3 = params_3[0]\n\nunits_1_3 = params_3[1]\ndropout_1_3 = np.round(params_3[2],4)\n\nunits_2_3 = params_3[3]\ndropout_2_3 = np.round(params_3[4],4)\n\n\nlr_3 = np.round(params_3[5],8)","8cdff19a":"model= keras.Sequential()\n\nmodel.add(keras.layers.Dense(units=units_1_3, activation='relu'))\nmodel.add(keras.layers.Dropout(dropout_1_3))\n\nmodel.add(keras.layers.Dense(units=units_2_3, activation='relu'))\nmodel.add(keras.layers.Dropout(dropout_2_3))\n\nmodel.add(keras.layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer=keras.optimizers.Adam(learning_rate=lr_3),\n              metrics=['accuracy'])\n\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=5, restore_best_weights=True)\n\nmodel.fit(X_train_os, y_train_os,\n              validation_data = (X_val_os, y_val_os),\n              shuffle = True,\n              batch_size = BATCH_SIZE,\n              epochs = EPOCHS,\n              callbacks = [early_stop], \n)","eeb2ac10":"model.summary()","f90ad0c6":"score = model.evaluate(X_test, y_test, verbose=0)  ","80c22a22":"y_pred_prob_3 = model.predict(X_test)\ny_pred_3 = y_pred_prob_3.round()","aaa41767":"print(classification_report(y_test, y_pred_3, target_names = ['No','Yes']))","5c8ecae5":"fig, ax = plt.subplots(1,3,figsize=(7,3))\nlabels=['No','Yes']\n\nsns.heatmap(confusion_matrix(y_test, y_pred_1), annot=True, cmap=\"Blues\", fmt='g', cbar=False,ax=ax[0])\nplt.suptitle('Fraudolent Transaction?', fontsize=20)\nax[0].set_title('Unbalanced case')\nax[0].set_yticklabels(labels);\nax[0].set_xticklabels(labels);\nax[0].set_ylabel('Test')\nax[0].set_xlabel('Predicted')\n\nsns.heatmap(confusion_matrix(y_test, y_pred_2), annot=True, cmap=\"Blues\", fmt='g', cbar=False,ax=ax[1])\nplt.suptitle('Fraudolent Transaction?', fontsize=20)\nax[1].set_title('Undersampling case')\nax[1].set_yticklabels(labels);\nax[1].set_xticklabels(labels);\nax[1].set_ylabel('Test')\nax[1].set_xlabel('Predicted')\n\n\nsns.heatmap(confusion_matrix(y_test, y_pred_3), annot=True, cmap=\"Blues\", fmt='g', cbar=False, ax=ax[2])\nax[2].set_title('Oversampling case')\nax[2].set_xticklabels(labels);\nax[2].set_yticklabels(labels);\nax[2].set_ylabel('Test')\nax[2].set_xlabel('Predicted')\nplt.tight_layout()\n\nplt.show()","70c3dc89":"fig, ax = plt.subplots(2,3,figsize=(10,7))\n\n####PR CURVES####\n\np,r,_ = precision_recall_curve(y_test,y_pred_prob_1)\nax[0,0].plot(r,p, label = \"Precision:{:.2f}%\\nRecall:{:.2f}%\".format(precision_score(y_test, y_pred_1)*100,recall_score(y_test, y_pred_1)*100))\nax[0,0].set_title('PR CURVE\\nUnbalanced',fontsize=20)\nax[0,0].set_ylabel('Precision',fontsize=15)\nax[0,0].set_xlabel('Recall',fontsize=15)\nax[0,0].legend(loc = 'lower left', fontsize=13, fancybox=True, shadow=True, frameon=True,handlelength=0)\n\np,r,_ = precision_recall_curve(y_test,y_pred_prob_2)\nax[0,1].plot(r,p, label = \"Precision:{:.2f}%\\nRecall:{:.2f}%\".format(precision_score(y_test, y_pred_2)*100,recall_score(y_test, y_pred_2)*100))\nax[0,1].set_title('PR CURVE\\nUndersampling',fontsize=20)\nax[0,1].set_ylabel('Precision',fontsize=15)\nax[0,1].set_xlabel('Recall',fontsize=15)\nax[0,1].legend(loc = 'lower left', fontsize=13, fancybox=True, shadow=True, frameon=True,handlelength=0)\n\np,r,_ = precision_recall_curve(y_test,y_pred_prob_3)\nax[0,2].plot(r,p, label = \"Precision:{:.2f}%\\nRecall:{:.2f}%\".format(precision_score(y_test, y_pred_3)*100,recall_score(y_test, y_pred_3)*100))\nax[0,2].set_title('PR CURVE\\nOversampling',fontsize=20)\nax[0,2].set_ylabel('Precision',fontsize=15)\nax[0,2].set_xlabel('Recall',fontsize=15)\nax[0,2].legend(loc = 'lower left', fontsize=13, fancybox=True, shadow=True, frameon=True,handlelength=0)\n\n####ROC####\n\n\nfprcat, tprcat, _ = roc_curve(y_test, y_pred_prob_1)\nroc_auccat = auc(fprcat, tprcat)\nax[1,0].plot(fprcat, tprcat, 'b', label = 'AUC = %0.2f' % roc_auccat)\nax[1,0].plot([0, 1], [0, 1],'r--')\nax[1,0].set_title('ROC\\nUnbalanced',fontsize=20)\nax[1,0].set_ylabel('True Positive Rate',fontsize=15)\nax[1,0].set_xlabel('False Positive Rate',fontsize=15)\nax[1,0].legend(loc = 'lower right', fontsize=15, fancybox=True, shadow=True, frameon=True,handlelength=0)\n\nfprcat, tprcat, _ = roc_curve(y_test, y_pred_prob_2)\nroc_auccat = auc(fprcat, tprcat)\nax[1,1].plot(fprcat, tprcat, 'b', label = 'AUC = %0.2f' % roc_auccat)\nax[1,1].plot([0, 1], [0, 1],'r--')\nax[1,1].set_title('ROC\\nUndersampling',fontsize=20)\nax[1,1].set_ylabel('True Positive Rate',fontsize=15)\nax[1,1].set_xlabel('False Positive Rate',fontsize=15)\nax[1,1].legend(loc = 'lower right', fontsize=15, fancybox=True, shadow=True, frameon=True,handlelength=0)\n\n\nfprcat, tprcat, _ = roc_curve(y_test, y_pred_prob_3)\nroc_auccat = auc(fprcat, tprcat)\nax[1,2].plot(fprcat, tprcat, 'b', label = 'AUC = %0.2f' % roc_auccat)\nax[1,2].plot([0, 1], [0, 1],'r--')\nax[1,2].set_title('ROC\\nOversampling',fontsize=20)\nax[1,2].set_ylabel('True Positive Rate',fontsize=15)\nax[1,2].set_xlabel('False Positive Rate',fontsize=15)\nax[1,2].legend(loc = 'lower right', fontsize=15, fancybox=True, shadow=True, frameon=True,handlelength=0)\n\n\nplt.tight_layout()\nplt.show()","72e11e33":"Even if the F1 score obtained using the original unbalanced dataset is slightly higher than the one obtained using SMOTE oversampling, the recall is quite higher in this last case, with good values for precision as well. <br>\n**For this reason, we would prefer using the neural network trained using the oversampled data with SMOTE.**","4595b398":"Indeed the skew value is well over 0.5, which can be considered as a threshold value for skewness.","cad8c264":"## Amount column analysis","6a05bc2b":"## Check for duplicates","018827bf":"# Neural net 2 (Undersampling)","70ea8430":"The main problem with this type of datasets is target class balance, which will be covered in the following section","890384e2":"First, we will train a Neural Network with the unbalanced dataset and check the performance.","8f017e8e":"# Neural net 1 (Unbalanced)","a4b7667f":"The distriibutions look indeed very similar. We can choose one over the other without important differences. For example, we will chose the box-cox transformation.","ed260642":"**We can see that the Neural Network trained with oversampled data performed the best, with a quite high value of recall and good value of precision.**\nIn particular, **the most important score to look at is recall since we want as few false negatives as possible**: by not flagging a fraudolent transaction (false negative), the transaction would still be accepted and processed (worst case). On the other hand, by flagging a non fraudolent transaction as fraudolent, the customer would just see his regular transaction denied. This is not good as well and happened to me some times as well, but it is still better than allow fraudolent transactions.","c69409cb":"# Unbalanced case","e7dfd423":"It looks like there are 1081 duplicate data, we should drop them !","c1cfd8c8":"The training data now is balanced.","07d5297e":"the distribution looks highly skewed","465e0e89":"Then, we can obtain the cumulative explained variance ratios and the number of principal components","6b0f0eab":"**Thank you for reading this notebook. Let me know if you have questions or if you want me to check out your notebooks !! Thanks for reading ! :)**","82b6641e":"This means that we will add 176122 - 302 = 175820 rows with 'Class' = 1, which is indeed a large number of duplicates that will be added.","e25ee23d":"Before applying PCA, we need to scale the data.","f5997f3e":"# Neural net 3 (Oversampling)","b0ab061c":"# Results comparison:","d0403f24":"There are lots of entries (284k) and a quite high number of columns, 31.","1aab2940":"In the following, two different techniques will be used to deal with class unbalance: undersampling and oversampling. <br>\n**This is a very important step: we can only apply resampling techniques (undersampling, oversampling...) on the training set!** I saw on several notebooks that some people apply resampling on the overall dataset and then train test split, but this is wrong. For example, in the case of Oversampling, if we oversample frst the dataset and then split into train and test sets, it is likely that same rows will be present both in train and test sets, compromising the generalization of the algorithm.<br>\nIn case of undersampling, if we undersample first and then train-test split, it means that the test set will be balanced, which would not be the case considering the problem at hand (unabalanced dataset), and would result in optimistic predictions. That is way it is importatnt to keep the test set unbalanced and apply resampling only on the training set.","c2f2a559":"Moreover, we further split the training set into a new train and validation dataset. This is done to monitor a metric ( accuracy or recall for example) on the validation set and so prevent overfitting.","39d1efe5":"# Fraud column analysis","1e66f67f":"The Neural Network hyperparameters will be tuned using OPTUNA library. If you want to know more you can check out my notebook:\n- https:\/\/www.kaggle.com\/ludovicocuoghi\/neural-networks-opt-w-keras-tuner-and-optuna","be748a47":"Let's check for duplicate data again without the column time:","34dec7ce":"We can see a slightly lower F1 score in this case compared to the unbalanced case, but with a higher value of recall.<br>\n**We are particularly satisfied with the result since we achieved higher recall compared to the unbalanced case, at the cost of a slightly lower precision.**","abc78722":"<img src=\"https:\/\/i.imgur.com\/d7sc8ku.png\" width=\"900px\">","47e88ac0":"**We can see a quite different situation in terms of scores between the 'Yes' and 'No' class: F1 score is 100% for class 'No', and much lower for class 'Yes.<br>\nOverall, the Neural Network did not perform too badly even if the dataset is unbalanced.**","4c177b1e":"Now, oversampling will be performed on the training data.","c0db82a1":"This means that we will remove 176122 - 302 = 175820 rows.","10896333":"We assign the values of the optimal parameters to new variables.","6fe9c830":"It looks like just one feature 'could' be removed and allow the model to still have 99% of the explainability. Usually we can keep most of the explainability removing more features, however in this dataset the features appears to be already transformed by PCA and so we could potentially just remove 1 feature. <br>\nFor this reason we won't drop any features and keep the original 28 columns in the X variable.","d74576e9":"# How to deal with unbalanced data? A comparison between Undersampling and Oversampling with SMOTE","1be4a9ea":"We can see a **very low value of precision** (less than 0.1) and **quite high recall (over 0.85) for target = 1.**.<br>\n**By using this model, most of the users would see their non fraudolent transactions labeled as fraudolent.**","7b246ec3":"It looks like there are 8063 duplicate data, all related to non fraudolent transactions. We will drop them !!","1ed0de18":"When undersampling, we aim to remove a number of the rows of the majority class (rows where class=0) in order to match the number of rows of the minority class (rows where class=1).","4e66f7e1":"Finally, we define a Neural Network model with the optimal values by Optuna.","bcd2cad0":"We can start the optimization by Optuna","2df5f03b":"# Dimensionality reduction by PCA","6b3670f2":"# Class balance by Undersampling","efb6778a":"# Class balance by oversampling with SMOTE ","4f3d8238":"# Main results Dashboard:","e5a08d77":"## Custom functions definition:","7a873b89":" The column 'time' should be dropped since it does not give useful information.","eaccad2b":"The following notebook is about the application of undersampling and oversampling techniques on a credit card fraud dataset to deal with heavy unbalanced datasets.<br>\nThe project is structured as follows:\n- Data loading.\n- Data cleaning:\n    - Duplicated data check.\n    - Box Cox tranformation of skewed features.\n    - PCA for dimensionality reduction.\n- Unbalanced problem: Undersampling and Oversampling with SMOTE\n- Neural networks by Keras, and hyperparameter optimization by OPTUNA.\n- Results comparison: ROC, Confusion Matrix, Precision, Recall.","2661fc29":"First, we define a Neural Network hypermodel"}}