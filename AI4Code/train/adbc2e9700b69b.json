{"cell_type":{"41d85a4c":"code","0cc3c07c":"code","fba7bf4c":"code","c702ee34":"code","a6c116c8":"code","0e5ff112":"code","f21dfa63":"code","fef15c16":"code","aab1b4be":"code","dde96349":"code","f8f9875d":"code","ecea01be":"code","da63b8f5":"code","8da5f48d":"markdown","69f6c90a":"markdown","00d86105":"markdown","3e7a9bc8":"markdown","d708b0c8":"markdown","fc33cf3b":"markdown","c9e187b8":"markdown","f455568e":"markdown","bd721090":"markdown","a21ac1f2":"markdown","2c9ae45e":"markdown","14cf9376":"markdown","32cbf9db":"markdown","f51288d1":"markdown","e8d6e821":"markdown"},"source":{"41d85a4c":"import numpy as np\nimport string\nimport pandas as pd \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS\n\nstopwords = set(STOPWORDS)\ndata = \"\"\"\nTo be, or not to be, that is the question:\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune,\nOr to take arms against a sea of troubles\nAnd by opposing end them. To die\u2014to sleep,\nNo more; and by a sleep to say we end\nThe heart-ache and the thousand natural shocks\nThat flesh is heir to: 'tis a consummation\nDevoutly to be wish'd. To die, to sleep;\nTo sleep, perchance to dream\u2014ay, there's the rub:\nFor in that sleep of death what dreams may come,\nWhen we have shuffled off this mortal coil,\nMust give us pause\u2014there's the respect\nThat makes calamity of so long life.\nFor who would bear the whips and scorns of time,\nTh'oppressor's wrong, the proud man's contumely,\nThe pangs of dispriz'd love, the law's delay,\nThe insolence of office, and the spurns\nThat patient merit of th'unworthy takes,\nWhen he himself might his quietus make\n\"\"\"\n\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords,\n                          max_words=200,\n                          max_font_size=40, \n                          random_state=42\n                         ).generate(data)\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(24, 24))\naxes[0].imshow(wordcloud)\naxes[0].axis('off')\naxes[1].imshow(wordcloud)\naxes[1].axis('off')\naxes[2].imshow(wordcloud)\naxes[2].axis('off')\nfig.tight_layout()","0cc3c07c":"data = \"\"\"\nTo be, or not to be, that is the question:\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune,\nOr to take arms against a sea of troubles\nAnd by opposing end them. To die\u2014to sleep,\nNo more; and by a sleep to say we end\nThe heart-ache and the thousand natural shocks\nThat flesh is heir to: 'tis a consummation\nDevoutly to be wish'd. To die, to sleep;\nTo sleep, perchance to dream\u2014ay, there's the rub:\nFor in that sleep of death what dreams may come,\nWhen we have shuffled off this mortal coil,\nMust give us pause\u2014there's the respect\nThat makes calamity of so long life.\nFor who would bear the whips and scorns of time,\nTh'oppressor's wrong, the proud man's contumely,\nThe pangs of dispriz'd love, the law's delay,\nThe insolence of office, and the spurns\nThat patient merit of th'unworthy takes,\nWhen he himself might his quietus make\n\"\"\"","fba7bf4c":"# remove \\n\ndata = data.replace(\"\\n\", \" \")\n\n# lower cases\ndata = data.lower()\n\n# remove punctuation\ndata = data.translate(str.maketrans('', '', string.punctuation))\n\n# remove first and last character\ndata = data[1:-1]\n\nprint(data)","c702ee34":"chars = sorted(set(data))\n\nchar_to_idx = {c:i for (i,c) in enumerate(chars)}\nidx_to_char = {i:c for (i,c) in enumerate(chars)}","a6c116c8":"data_size, char_size = len(data), len(chars)\n\nhidden_size = 10\nweight_sd = 0.1\nz_size = hidden_size + char_size\nt_steps = 25","0e5ff112":"def sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\n\ndef dsigmoid(y):\n    return y * (1 - y)\n\ndef tanh(x):\n    return np.tanh(x)\n\ndef dtanh(y):\n    return 1 - y * y","f21dfa63":"def forward(x, u, q):\n    z = np.row_stack((q, x))\n\n    a = sigmoid(np.dot(wa, z) + ba)\n    b = sigmoid(np.dot(wb, z) + bb)\n    c = tanh(np.dot(wc, z) + bc)\n    d = sigmoid(np.dot(wd, z) + bd)\n\n    e = a * u + b * c\n    h = d * tanh(e)\n\n    v = np.dot(wv, h) + bv\n    y = np.exp(v) \/ np.sum(np.exp(v))\n\n    return z, a, b, c, d, e, h, v, y","fef15c16":"def optimize(grads, theta, lr=0.05):\n    dwa, dwb, dwc, dwd, dwv, dba, dbb, dbc, dbd, dbv = grads\n    wa, wb, wc, wd, wv, ba, bb, bc, bd, bv = theta\n    \n    wa -= dwa * lr\n    wb -= dwb * lr\n    wc -= dwc * lr\n    wd -= dwd * lr\n    wv -= dwv * lr\n    \n    ba -= dba * lr\n    bb -= dbb * lr\n    bc -= dbc * lr\n    bd -= dbd * lr\n    bv -= dbv * lr\n    \n    return wa, wb, wc, wd, wv, ba, bb, bc, bd, bv","aab1b4be":"losses = {}\nz, a, b, c, d, e, h, v, y = {}, {}, {}, {}, {}, {}, {}, {}, {}\nq, x, u = {}, {}, {}","dde96349":"wa, wb, wc, wd = [np.random.randn(hidden_size, z_size) * weight_sd + 0.5 for x in range(4)]\nba, bb, bc, bd = [np.zeros((hidden_size, 1)) for x in range(4)]\n\n# output\nwv = np.random.randn(char_size, hidden_size) * weight_sd\nbv = np.zeros((char_size, 1))","f8f9875d":"q[-1] = np.zeros((hidden_size, 1))\nu[-1] = np.zeros((hidden_size, 1))\n\npointer = 25\nt_steps = 25\n\ninputs = ([char_to_idx[ch] for ch in data[pointer: pointer + t_steps]])\ntargets = ([char_to_idx[ch] for ch in data[pointer + 1: pointer + t_steps + 1]])","ecea01be":"for epoch in range(1000):\n    \n    loss = 0\n    \n    # Forward propagation\n    for t in range(len(inputs)):\n        x[t] = np.zeros((char_size, 1))\n        x[t][inputs[t]] = 1\n\n        z[t], a[t], b[t], c[t], d[t], e[t], h[t], v[t], y[t] = forward(x[t], u[t - 1], q[t - 1])\n\n        u[t], q[t] = e[t], h[t]\n\n        loss += -np.log(y[t][targets[t], 0])\n\n\n    dh_next = np.zeros_like(q[0])\n    de_next = np.zeros_like(u[0])\n    dwa, dwb, dwc, dwd, dwv, dba, dbb, dbc, dbd, dbv = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0\n\n    # Backward propagation\n    for t in reversed(range(len(inputs))):\n        target = targets[t]\n\n        dv = np.copy(y[t])\n        dv[target] -= 1\n\n        dwv += np.dot(dv, h[t].T)\n        dbv += dv\n\n        dh = np.dot(wv.T, dv)\n        dh += dh_next\n\n        dd = dh * tanh(e[t])\n        dd = dsigmoid(d[t]) * dd\n\n        dwd += np.dot(dd, z[t].T)\n        dbd += dd\n\n        de = np.copy(de_next)\n        de += dh * d[t] * dtanh(tanh(e[t]))\n\n        dc = de * b[t]\n        dc = dtanh(c[t]) * dc\n\n        dwc += np.dot(dc, z[t].T)\n        dbc += dc\n\n        db = de * dc\n        db = dsigmoid(b[t]) * db\n\n        dwb += np.dot(db, z[t].T)\n        dbb += db\n\n        da = de * u[t - 1]\n        da = dsigmoid(a[t]) * da\n\n        dwa += np.dot(da, z[t].T)\n        dba += da\n\n        dz = (np.dot(wa.T, da) \n              + np.dot(wb.T, db) \n              + np.dot(wc.T, dc) \n              + np.dot(dd.T, dd))\n\n        dh_next = dz[:hidden_size, :]\n        de_next = a[t] * de\n\n    \n    grads = dwa, dwb, dwc, dwd, dwv, dba, dbb, dbc, dbd, dbv\n    theta = wa, wb, wc, wd, wv, ba, bb, bc, bd, bv\n\n    # optimize with SGD the training data\n    wa, wb, wc, wd, wv, ba, bb, bc, bd, bv = optimize(grads, theta)\n        \n    losses[epoch] = loss","da63b8f5":"plt.plot(list(losses.keys()), [losses[x] for x in list(losses.keys())])","8da5f48d":"![LSTM.png](attachment:LSTM.png)","69f6c90a":"<h1 id=\"activation-functions\" style=\"color:black; background:whte; border:0.5px dotted;\"> \n    <center>Activation Functions\n        <a class=\"anchor-link\" href=\"#activation-functions\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","00d86105":"<h1 id=\"dictionaries\" style=\"color:black; background:whte; border:0.5px dotted;\"> \n    <center>Dictionaries\n        <a class=\"anchor-link\" href=\"#dictionaries\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","3e7a9bc8":"<h1 id=\"initialize\" style=\"color:black; background:whte; border:0.5px dotted;\"> \n    <center>Initialize variables\n        <a class=\"anchor-link\" href=\"#initialize\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","d708b0c8":"<h1 id=\"training\" style=\"color:black; background:whte; border:0.5px dotted;\"> \n    <center>Training\n        <a class=\"anchor-link\" href=\"#training\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","fc33cf3b":"### Data preparation","c9e187b8":"<h1 id=\"analyze\" style=\"color:black; background:whte; border:0.5px dotted;\"> \n    <center>Analyze\n        <a class=\"anchor-link\" href=\"#analyze\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","f455568e":"<h1 id=\"dataset\" style=\"color:black; background:whte; border:0.5px dotted;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","bd721090":"<h1 id=\"parameters\" style=\"color:black; background:whte; border:0.5px dotted;\"> \n    <center>Parameters\n        <a class=\"anchor-link\" href=\"#parameters\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","a21ac1f2":"### Creating the data","2c9ae45e":"If you liked this implementation, don't forget to up-vote :)\n\nP.S: The architecture of LSTM is drawn by me, a little Picasso.","14cf9376":"<h1 id=\"definition\" style=\"color:black; background:whte; border:0.5px dotted;\"> \n    <center>Definition\n        <a class=\"anchor-link\" href=\"#definition\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","32cbf9db":"<h1 id=\"implementation\" style=\"color:black; background:whte; border:0.5px dotted;\"> \n    <center>Implementation\n        <a class=\"anchor-link\" href=\"#implementation\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","f51288d1":"Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems.<br>\n<br>\nThis is a behavior required in complex problem domains like machine translation, speech recognition, and more.<br>\n<br>\nLSTMs are a complex area of deep learning. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional and sequence-to-sequence relate to the field.","e8d6e821":"<h1 id=\"architecture\" style=\"color:black; background:whte; border:0.5px dotted;\"> \n    <center>Architecture\n        <a class=\"anchor-link\" href=\"#architecture\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>"}}