{"cell_type":{"9a7a8cac":"code","686294d6":"code","257d16ca":"code","a62390b7":"code","e6c77818":"code","7c6386fe":"code","c3f65053":"code","05d1f1dc":"code","4f4be724":"code","9648a42b":"code","2ded3b6a":"code","d8cebe25":"code","c08626ea":"code","cd07af48":"code","20c2fc4a":"code","08a3f0be":"code","b995f61c":"code","e836106a":"code","98cbb950":"code","761aceed":"code","4d322642":"markdown","28965cd4":"markdown","a4acf83f":"markdown","10949ae2":"markdown","29428c85":"markdown","8372ba46":"markdown","4e486e11":"markdown","30208823":"markdown","e1855eb9":"markdown","0b22a809":"markdown","e2caf02a":"markdown","ca774601":"markdown","29e6bb11":"markdown","6e15f84d":"markdown","ab0fd706":"markdown","59ef7668":"markdown","2dd4d4db":"markdown","2bc516c1":"markdown","e851eccc":"markdown","bf045a16":"markdown","f1c21590":"markdown","1538712e":"markdown","2c077edd":"markdown","20906b2c":"markdown","bcef3fe0":"markdown","90f3c0e7":"markdown","65a07dcb":"markdown","638d6b11":"markdown","992c4055":"markdown","2d073d77":"markdown","fa6f2583":"markdown","bb45b57e":"markdown","3ab9e46c":"markdown","cc30b1be":"markdown"},"source":{"9a7a8cac":"import pandas as pd\nimport numpy as np\nimport os\nimport seaborn as sns\nimport glob\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\npd.set_option('max_columns', 1000)\nfrom tqdm import tqdm\nfrom sklearn.neighbors import BallTree\nimport math\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nfrom datetime import datetime\nimport pytz\nfrom IPython.display import HTML\nimport scipy.stats as stats\nimport matplotlib as mpl\nfrom matplotlib import animation, rc, use\nfrom matplotlib.patches import Rectangle, Arrow\nimport tensorflow as tf\nfrom matplotlib.patches import Polygon\nimport matplotlib.patheffects as pe\nimport gc\n\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    \n    return df\n\n\ndef get_dx_dy(radian_angle, dist):\n    dx = dist * math.cos(radian_angle)\n    dy = dist * math.sin(radian_angle)\n    return dx, dy\n\n\ndef create_football_field(linenumbers=True,\n                          endzones=True,\n                          highlight_line=False,\n                          highlight_line_number=50,\n                          highlighted_name='Line of Scrimmage',\n                          fifty_is_los=False,\n                          figsize=(12*2, 6.33*2)):\n    \"\"\"\n    Function that plots the football field for viewing plays.\n    Allows for showing or hiding endzones.\n    \"\"\"\n    rect = patches.Rectangle((0, 0), 120, 53.3, linewidth=0.1,\n                             edgecolor='r', facecolor='slategrey', zorder=0)\n\n    fig, ax = plt.subplots(1, figsize=figsize)\n    ax.add_patch(rect)\n\n    plt.plot([10, 10, 10, 20, 20, 30, 30, 40, 40, 50, 50, 60, 60, 70, 70, 80,\n              80, 90, 90, 100, 100, 110, 110, 120, 0, 0, 120, 120],\n             [0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3,\n              53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 53.3, 0, 0, 53.3],\n             color='white')\n    if fifty_is_los:\n        plt.plot([60, 60], [0, 53.3], color='gold')\n        plt.text(62, 50, '<- Player Yardline at Snap', color='gold')\n    # Endzones\n    if endzones:\n        ez1 = patches.Rectangle((0, 0), 10, 53.3,\n                                linewidth=0.3,\n                                edgecolor='k',\n                                facecolor='royalblue',\n                                alpha=0.4,\n                                zorder=1)\n        ez2 = patches.Rectangle((110, 0), 120, 53.3,\n                                linewidth=0.3,\n                                edgecolor='k',\n                                facecolor='royalblue',\n                                alpha=0.4,\n                                zorder=1)\n        ax.add_patch(ez1)\n        ax.add_patch(ez2)\n    plt.xlim(0, 120)\n    plt.ylim(0, 53.3)\n    plt.axis('off')\n    if linenumbers:\n        for x in range(20, 110, 10):\n            numb = x\n            if x > 50:\n                numb = 120 - x\n            plt.text(x, 5, str(numb - 10),\n                     horizontalalignment='center',\n                     fontsize=20,  # fontname='Arial',\n                     color='white')\n            plt.text(x - 0.95, 53.3 - 5, str(numb - 10),\n                     horizontalalignment='center',\n                     fontsize=20,  # fontname='Arial',\n                     color='white', rotation=180)\n    if endzones:\n        hash_range = range(11, 110)\n    else:\n        hash_range = range(1, 120)\n\n    for x in hash_range:\n        ax.plot([x, x], [0.4, 0.7], color='white')\n        ax.plot([x, x], [53.0, 52.5], color='white')\n        ax.plot([x, x], [22.91, 23.57], color='white')\n        ax.plot([x, x], [29.73, 30.39], color='white')\n\n    if highlight_line:\n        hl = highlight_line_number + 10\n        plt.plot([hl, hl], [0, 53.3], color='yellow')\n        plt.text(hl + 2, 50, '<- {}'.format(highlighted_name),\n                 color='yellow')\n    return fig, ax\n\n\n\nclass CreateNFLData:\n\n    def __init__(self):\n        pass\n\n    def LoadData(self, Normal=True):\n        if Normal == True:\n            print(\"Loading Original Data\")\n            globbed_files = glob.glob(\"week*.csv\") #creates a list of all csv files\n            data = []\n            for csv in tqdm(globbed_files):\n                frame = pd.read_csv(csv, index_col=0)\n                data.append(frame)\n\n            WeekData = pd.concat(data).reset_index()\n            WeekData\n        \n        else:\n            print(\"Loading Modified Data\")\n            globbed_files = glob.glob(\"Revised Data\/*.csv\") #creates a list of all csv files\n            data = []\n            for csv in tqdm(globbed_files):\n                frame = pd.read_csv(csv, index_col=0)\n                data.append(frame)\n\n            WeekData = pd.concat(data).reset_index()\n            WeekData\n        return WeekData\n\n\n\n    \n    def FrameData(self,WeekData1):\n        NotNone = WeekData1.query('event != \"None\"')\n        NotNone = NotNone.groupby(['gameId','playId','event'])['frameId'].max().reset_index()\n        NotNone = NotNone.set_index(['gameId','playId','event'], drop= True).unstack('event').reset_index()\n        NotNone.columns = [' '.join(col).strip() for col in NotNone.columns.values]\n        NotNone.columns = NotNone.columns.str.replace('frameId' , '')\n        NotNone.columns = NotNone.columns.str.replace(' ' , '')\n        NotNone['Code'] = NotNone['gameId'].astype(str) + \"-\" + NotNone['playId'].astype(str)\n        NotNone = NotNone.set_index('Code')\n        NotNone = NotNone.loc[~NotNone.index.duplicated(keep='first')]\n\n        for col in tqdm(NotNone.columns):\n            NotNone['Contains_' + str(col)] = np.where(NotNone[col] > 0, True, False)\n\n        Cols = ['ball_snap', 'man_in_motion', 'pass_arrived', 'pass_forward','pass_outcome_caught', 'play_action', 'run_pass_option', 'Contains_man_in_motion', 'Contains_pass_arrived', 'Contains_pass_forward', 'Contains_pass_outcome_caught', 'Contains_play_action','Contains_run_pass_option']\n    #   WeekData1 = pd.merge(df, NotNone, how=\"left\", left_on=['gameId','playId'], right_on=['gameId','playId'] )\n        for col in Cols:\n            WeekData1[col] = WeekData1.Code.map(NotNone[col])\n\n        del NotNone\n        gc.collect()\n\n        WeekData1['After_snap'] = np.where(WeekData1['frameId'] > WeekData1['ball_snap'],1,0)\n        WeekData1['After_Throw'] = np.where(WeekData1['frameId'] > WeekData1['pass_forward'],1,0)\n        WeekData1['After_PassArrived'] = np.where(WeekData1['frameId'] > WeekData1['pass_arrived'],1,0)\n        WeekData1['After_PlayAction'] = np.where(WeekData1['frameId'] > WeekData1['play_action'],1,0)\n    #   WeekData1['After_run_pass_option'] = np.where(WeekData1['frameId'] > WeekData1['run_pass_option'],1,0)\n        WeekData1['After_Catch'] = np.where(WeekData1['frameId'] > WeekData1['pass_outcome_caught'],1,0)\n\n        \n        LOS = WeekData1.query('displayName == \"Football\" & After_snap == 0')\n        LOS = LOS.groupby(['gameId','playId'])['X_std'].agg('median').reset_index()\n        LOS.columns = ['gameId','playId','LOS']\n        LOS['Code'] = LOS['gameId'].astype(str) + \"-\" + LOS['playId'].astype(str)\n        LOS = LOS.set_index('Code')\n        LOS = LOS.loc[~LOS.index.duplicated(keep='first')]\n        WeekData1[\"LOS\"] = WeekData1.Code.map(LOS['LOS'])\n        WeekData1['Distfrom_LOS'] = WeekData1['X_std'] - WeekData1['LOS']\n        WeekData1['AbsDistfrom_LOS'] = np.abs(WeekData1['X_std'] - WeekData1['LOS'])\n        del LOS\n        gc.collect()\n        return WeekData1\n    \n    def import_data(self,file,columns=False,cols=\"\"):\n        \"\"\"create a dataframe and optimize its memory usage\"\"\"\n        if columns == False:\n            df = pd.read_csv(file, low_memory=False)\n            df = reduce_mem_usage(df)\n        else:\n            df = pd.read_csv(file, low_memory=False, usecols=cols)\n            df = reduce_mem_usage(df)\n        return df\n    \n\n\n\n\n\nclass AnimatePlay:\n    def __init__(self, play_df,player_id=[], Tri = False, MPH = False,Text=\"\",Show='jerseyNumber',method='all' ) -> None:\n        self._MAX_FIELD_Y = 53.3\n        self._MAX_FIELD_X = 120\n        self._MAX_FIELD_PLAYERS = 22\n        \n\n        self.Tri = Tri\n        self.MPH = MPH\n        self.player_id = player_id\n        self.Show = Show\n        self.method = method\n        self.Text = Text\n\n        self._CPLT = sns.color_palette(\"husl\", 2)\n        self._frame_data = play_df\n        self._times = sorted(play_df.time.unique())\n        self._stream = self.data_stream()\n        \n        self._date_format = \"%Y-%m-%dT%H:%M:%S.%fZ\" \n        self._mean_interval_ms = np.mean([delta.microseconds\/1000 for delta in np.diff(np.array([pytz.timezone('US\/Eastern').localize(datetime.strptime(date_string, self._date_format)) for date_string in self._times]))])\n        \n        self._fig, self._ax_field = create_football_field()\n\n        self._fig.set_figheight(10)\n        self._fig.set_figwidth(15)\n        \n        self._fig.tight_layout()\n        \n        self._ax_field = plt.gca()\n        \n        self._ax_home = self._ax_field.twinx()\n        self._ax_away = self._ax_field.twinx()\n        self._ax_jersey = self._ax_field.twinx()\n\n        self.ani = animation.FuncAnimation(self._fig, self.update, frames=len(self._times), interval = self._mean_interval_ms, \n                                          init_func=self.setup_plot, blit=False)\n        \n        plt.close()\n       \n    @staticmethod\n    def set_axis_plots(ax, max_x, max_y) -> None:\n        ax.xaxis.set_visible(False)\n        ax.yaxis.set_visible(False)\n\n        ax.set_xlim([0, max_x])\n        ax.set_ylim([0, max_y])\n        \n    @staticmethod\n    def convert_orientation(x):\n        return (-x + 90)%360\n    \n    @staticmethod\n    def polar_to_z(r, theta):\n        return r * np.exp( 1j * theta)\n    \n    @staticmethod\n    def deg_to_rad(deg):\n        return deg*np.pi\/180\n        \n    def data_stream(self):\n        for time in self._times:\n            yield self._frame_data[self._frame_data.time == time]\n    \n    def setup_plot(self): \n        self.set_axis_plots(self._ax_field, self._MAX_FIELD_X, self._MAX_FIELD_Y)\n        \n        ball_snap_df = self._frame_data[(self._frame_data.event == 'ball_snap') & (self._frame_data.team == 'football')]\n        self._ax_field.axvline(ball_snap_df.X_std.to_numpy()[0], color = 'yellow', linestyle = '--')\n        \n        self.set_axis_plots(self._ax_home, self._MAX_FIELD_X, self._MAX_FIELD_Y)\n        self.set_axis_plots(self._ax_away, self._MAX_FIELD_X, self._MAX_FIELD_Y)\n        self.set_axis_plots(self._ax_jersey, self._MAX_FIELD_X, self._MAX_FIELD_Y)\n        \n        for idx in range(10,120,10):\n            self._ax_field.axvline(idx, color = 'k', linestyle = '-', alpha = 0.05)\n            \n        self._scat_field = self._ax_field.scatter([], [], s = 200, color = 'red')\n        self._scat_home = self._ax_home.scatter([], [], s = 900, color = self._CPLT[0], edgecolors = 'k')\n        self._scat_away = self._ax_away.scatter([], [], s = 900, color = self._CPLT[1], edgecolors = 'k')\n        \n        self._scat_jersey_list = []\n        self._scat_number_list = []\n        self._scat_name_list = []\n        self._scat_mph_list = []\n        self._a_dir_list = []\n        self._a_or_list = []\n        self._a_tri_list = []\n        for _ in range(self._MAX_FIELD_PLAYERS):\n            self._scat_jersey_list.append(self._ax_jersey.text(0, 0, '', horizontalalignment = 'center', verticalalignment = 'center', c = 'black',fontweight='bold',fontsize='large',path_effects=[pe.withStroke(linewidth=3, foreground=\"white\")]))\n            self._scat_number_list.append(self._ax_jersey.text(0, 0, '', horizontalalignment = 'center', verticalalignment = 'center', c = 'white',fontweight='bold',fontsize=14,path_effects=[pe.withStroke(linewidth=5, foreground=\"dodgerblue\")]))\n            self._scat_name_list.append(self._ax_jersey.text(0, 0, '', horizontalalignment = 'center', verticalalignment = 'center', c = 'black',fontweight='bold',fontsize='larger',path_effects=[pe.withStroke(linewidth=5, foreground=\"gold\")]))\n            self._scat_mph_list.append(self._ax_jersey.text(0, 0, '', horizontalalignment = 'center', verticalalignment = 'center', c = 'lime',fontweight='bold',fontsize='larger'))\n\n            self._a_dir_list.append(self._ax_field.add_patch(Arrow(0, 0, 0, 0, color = 'k')))\n            self._a_or_list.append(self._ax_field.add_patch(Arrow(0, 0, 0, 0, color = 'k')))\n            self._a_tri_list.append(self._ax_field.add_patch(Arrow(0, 0, 0, 0, color = 'k')))\n            \n        return (self._scat_field, self._scat_home, self._scat_away,*self._scat_mph_list, *self._scat_jersey_list, *self._scat_number_list, *self._scat_name_list)\n        \n    def update(self, anim_frame):\n        pos_df = next(self._stream)\n        \n        for label in pos_df.team.unique():\n            label_data = pos_df[pos_df.team == label]\n\n            if label == 'football':\n                self._scat_field.set_offsets(np.hstack([label_data.X_std, label_data.Y_std]))\n            elif label == 'home':\n                self._scat_home.set_offsets(np.vstack([label_data.X_std, label_data.Y_std]).T)\n            elif label == 'away':\n                self._scat_away.set_offsets(np.vstack([label_data.X_std, label_data.Y_std]).T)\n\n        jersey_df = pos_df[pos_df.jerseyNumber.notnull()]\n        \n        for (index, row) in pos_df[pos_df.jerseyNumber.notnull()].reset_index().iterrows():\n            self._scat_jersey_list[index].set_position((row.X_std, row.Y_std))\n            self._scat_jersey_list[index].set_text(row.position)\n            if self.method == 'single':\n                try:\n                    self._scat_number_list[index].set_text(np.where(np.isin(row.nflId,self.player_id) == True,str(self.Text) +\" \"+ str(round(row[self.Show],2)),\"\"))\n                    self._scat_number_list[index].set_position((row.X_std, row.Y_std+2.4))\n                except:\n                    self._scat_number_list[index].set_text(np.where(np.isin(row.nflId,self.player_id) == True,str(self.Text) +\" \"+ str(row[self.Show]),\"\"))\n                    self._scat_number_list[index].set_position((row.X_std, row.Y_std+2.4))\n                    pass\n            else:\n                try:\n                    self._scat_number_list[index].set_text(str(round(row[self.Show],2)))\n                    self._scat_number_list[index].set_position((row.X_std, row.Y_std+2.4))\n                except:\n                    self._scat_number_list[index].set_text(row[self.Show])\n                    self._scat_number_list[index].set_position((row.X_std, row.Y_std+2.4))\n                    pass               \n\n            self._scat_name_list[index].set_text(np.where(row.frameId <= 10,row.displayName.split()[-1],\"\"))\n            self._scat_name_list[index].set_position((row.X_std, row.Y_std-1.9))\n            if self.MPH == True:\n                self._scat_mph_list[index].set_text(np.where((row.s \/ 0.488889) > 17,str(round(float(row.s \/ 0.488889),2)) + \" MPH\",\"\"))\n                self._scat_mph_list[index].set_position((row.X_std, row.Y_std+1.9))\n            else:\n                pass\n\n            player_vel = np.array([row.dx, row.dy])\n            player_orient = np.array([np.real(self.polar_to_z(3, row.Orientation_std)), np.imag(self.polar_to_z(3, row.Orientation_std))])\n            \n            self._a_dir_list[index].remove()\n            self._a_dir_list[index] = self._ax_field.add_patch(Arrow(row.X_std, row.Y_std, player_vel[0], player_vel[1], color = 'black'))\n            \n            self._a_or_list[index].remove()\n            self._a_or_list[index] = self._ax_field.add_patch(Arrow(row.X_std, row.Y_std, player_orient[0], player_orient[1], color = 'blue', width = 2))\n\n            if self.Tri == True:\n                if (self.method == 'single') & (np.isin(row.nflId,self.player_id) == True):\n                    self._a_tri_list[index].remove()\n                    self._a_tri_list[index] = self._ax_field.add_patch(Polygon([[row.X_std, row.Y_std], [row.X_std_COpp,row.Y_std_COpp],[row.X_std_QB,row.Y_std_QB]], closed=True, fill=False, hatch='\/',color='lime'))\n                else:\n      #              self._a_tri_list[index].remove()\n      #              self._a_tri_list[index] = self._ax_field.add_patch(Polygon([[row.X_std, row.Y_std], [row.X_std_COpp,row.Y_std_COpp],[row.X_std_QB,row.Y_std_QB]], closed=True, fill=False, hatch='\/',color='lime'))\n                    pass\n            else:\n                pass\n        \n        return (self._scat_field, self._scat_home, self._scat_away, *self._scat_jersey_list, *self._scat_number_list, *self._scat_name_list)","686294d6":"import pandas as pd\nimport numpy as np\nimport os\nimport seaborn as sns\nimport glob\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\npd.set_option('max_columns', 1000)\n\nfrom sklearn.neighbors import BallTree\n\n#from BDBUtils.Utilities import CreateNFLData\nfrom IPython.core.display import HTML\nimport time\nimport math\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\nfrom datetime import datetime\nimport pytz\nfrom IPython.display import HTML\nimport scipy.stats as stats\nimport matplotlib as mpl\nfrom matplotlib import animation, rc\nfrom matplotlib.patches import Rectangle, Arrow\nimport tensorflow as tf\n#from BDBUtils.Utilities import CreateNFLData\n\nimport glob\nimport os\n\nnp.set_printoptions(suppress=True)\n\nimport gc\n\ndef convert_orientation(x):\n    return (x)%360\n\ndef deg_to_rad(deg):\n        return deg*np.pi\/180\n\n\n\nCreate = CreateNFLData()\n\nstart = time.process_time()\n\nWeeks = [1,2,3,10]\n\n#globbed_files = glob.glob(\"..\/input\/revised-data\/*.csv\") #creates a list of all csv files\ndata = []\nfor n in tqdm(Weeks):\n    filename = '..\/input\/revised-data\/week' + str(n) + '.csv'\n    frame = Create.import_data(filename)\n    frame['Code'] = frame['gameId'].astype(str) + \"-\" + frame['playId'].astype(str)\n    plays = pd.read_csv('..\/input\/nfl-big-data-bowl-2021\/plays.csv', usecols=['gameId', 'playId','down', 'yardsToGo','penaltyCodes', 'penaltyJerseyNumbers', 'passResult', 'offensePlayResult', 'playResult', 'epa', 'isDefensivePI','offenseFormation',\t'personnelO',\t'defendersInTheBox',\t'numberOfPassRushers',\t'personnelD',\t'typeDropback','playType'])\n    plays['Code'] = plays['gameId'].astype(str) + \"-\" + plays['playId'].astype(str)\n    plays = plays.set_index('Code')\n    plays = plays.loc[~plays.index.duplicated(keep='first')]\n    Cols = ['playId','down', 'yardsToGo','penaltyCodes', 'penaltyJerseyNumbers', 'passResult', 'offensePlayResult', 'playResult', 'epa', 'isDefensivePI','offenseFormation',\t'personnelO',\t'defendersInTheBox',\t'numberOfPassRushers',\t'personnelD',\t'typeDropback','playType']\n    for col in Cols:\n        frame[col] = frame.Code.map(plays[col])\n    data.append(frame)\n    del plays\n    gc.collect()\n    del frame\n    gc.collect()\n\nWeekData = pd.concat(data).reset_index()\ndel data\ngc.collect()\nFinaldf1 = Create.FrameData(WeekData)\ndel WeekData\ngc.collect()\nFinaldf1.drop(['index','Code','IsOnOffense', 'ball_snap', 'man_in_motion', 'pass_arrived', 'pass_forward', 'pass_outcome_caught', 'play_action', 'run_pass_option'], axis=1, inplace=True)\nFinaldf1.memory_usage().sum() \/ (1024**2)\n\ngc.collect()\nFinaldf1.memory_usage().sum() \/ (1024**2)\n\nFinaldf1['QBslope'] = deg_to_rad(convert_orientation(np.rad2deg(np.arctan2(Finaldf1['Y_std_QB'] - Finaldf1['Y_std'], Finaldf1['X_std_QB'] - Finaldf1['X_std']))))\nFinaldf1['WRslope'] = deg_to_rad(convert_orientation(np.rad2deg(np.arctan2(Finaldf1['Y_std_COpp'] - Finaldf1['Y_std'],Finaldf1['X_std_COpp'] - Finaldf1['X_std']))))\n\nFinaldf1['QBslope1'] = convert_orientation(np.rad2deg(np.arctan2(Finaldf1['Y_std_QB'] - Finaldf1['Y_std'], Finaldf1['X_std_QB'] - Finaldf1['X_std'])))\nFinaldf1['WRslope1'] = convert_orientation(np.rad2deg(np.arctan2(Finaldf1['Y_std_COpp'] - Finaldf1['Y_std'],Finaldf1['X_std_COpp'] - Finaldf1['X_std'])))\nFinaldf1['Def_Or'] = convert_orientation(np.rad2deg(Finaldf1['Orientation_std']))\nFinaldf1['Diff_QB'] = Finaldf1['QBslope1'] - Finaldf1['Def_Or']\nFinaldf1['Diff_WR'] = Finaldf1['WRslope1'] - Finaldf1['Def_Or']\n\nFinaldf1['Diff_QB'] = abs(np.where(Finaldf1['Diff_QB'] < -180,Finaldf1['Diff_QB'] + 360,Finaldf1['Diff_QB'] ))\nFinaldf1['Diff_QB'] = abs(np.where(Finaldf1['Diff_QB'] > 180,Finaldf1['Diff_QB'] - 360,Finaldf1['Diff_QB'] ))\n\nFinaldf1['Diff_WR'] = abs(np.where(Finaldf1['Diff_WR'] < -180,Finaldf1['Diff_WR'] + 360,Finaldf1['Diff_WR'] ))\nFinaldf1['Diff_WR'] = abs(np.where(Finaldf1['Diff_WR'] > 180,Finaldf1['Diff_WR'] - 360,Finaldf1['Diff_WR'] ))\n\nFinaldf1['Player_POV'] = np.where(Finaldf1['Diff_QB'] < Finaldf1['Diff_WR'],\"QB\",Finaldf1['position_COpp'])\nFinaldf1['Looking_AtQB'] = np.where(Finaldf1['Diff_QB'] < Finaldf1['Diff_WR'],1,0)\n\nFinaldf1['diffDir'] = np.absolute(Finaldf1['Dir_std'] - Finaldf1['Dir_std_COpp'])\n\nFinaldf1['disRatio'] = Finaldf1['Opp_Dist_COpp'] \/ np.sqrt((Finaldf1['X_std_COpp'] - Finaldf1['X_std_CTm'])**2 + (Finaldf1['Y_std_COpp'] - Finaldf1['Y_std_CTm'])**2)\n\nFinaldf1['Event2'] = np.where(Finaldf1['After_snap'] == 0,\"Before Snap\",\"After Snap - Before Throw\")\nFinaldf1['Event2'] = np.where((Finaldf1['After_Throw'] == 1 & (Finaldf1['After_PassArrived'] == 0)),\"Ball in the Air\", Finaldf1['Event2'])\n\nFinaldf1['EventCount'] = Finaldf1.groupby(['gameId','playId','Event2'])['Event2'].transform('count') \/ Finaldf1.groupby(['gameId','playId'])['nflId'].transform('nunique')\nFinaldf1['EventOrder'] = Finaldf1.groupby(['gameId','playId','Event2'])['frameId'].rank(ascending=True, method='dense').astype(int)\nFinaldf1['EventPct'] = Finaldf1['EventOrder'] \/ Finaldf1['EventCount']\n\nFinaldf1['Partition'] = np.where(Finaldf1['EventPct'] > (1\/2), \"2nd Phase\", \"1st Phase\")\n\nFinaldf1['Group'] = Finaldf1['Event2'] + \"-\" + Finaldf1['Partition']\n","257d16ca":"import math\n#from BDBUtils.Utilities import AnimatePlay\nsns\nimport glob\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\npd.set_option('max_columns', 1000)\n\npd.options.display.float_format = '{:.3f}'.format\n\ngId = 2018110800\t\npId = 274\nnflId = [2560753,2555175]\n\nex = Finaldf1.query('gameId == @gId & playId == @pId')\n\nOne = ex.query('displayName == \"Donte Jackson\" | OnOffense == True | displayName == \"Football\"')\nOne = One.query('After_PassArrived == 0')\n\nfrom matplotlib.patches import Polygon\n\ndel ex\ngc.collect()\n\nanimated_play = AnimatePlay(One,player_id=nflId, Tri = True, MPH = False,Text=\"POV:\",Show='Player_POV',method='single' )\nHTML(animated_play.ani.to_jshtml())","a62390b7":"gId = 2018110800\t\npId = 274\nnflId = [2560753,2555175]\n\nex = Finaldf1.query('gameId == @gId & playId == @pId')\n\nOne = ex.query('displayName == \"James Bradberry\" | OnOffense == True | displayName == \"Football\"')\nOne = One.query('After_PassArrived == 0')\n\nfrom matplotlib.patches import Polygon\n\ndel ex\ngc.collect()\n\nanimated_play = AnimatePlay(One,player_id=nflId, Tri = True, MPH = False,Text=\"POV:\",Show='Player_POV',method='single' )\nHTML(animated_play.ani.to_jshtml())","e6c77818":"Finaldf1['Partition'] = np.where(Finaldf1['EventPct'] > (1\/2), \"2nd Phase\", \"1st Phase\")\n\nFinaldf1['Group'] = Finaldf1['Event2'] + \"-\" + Finaldf1['Partition']\n\nOne = Finaldf1.query('After_PassArrived == 0 ')\n\nClustModel1 = One.groupby(['gameId', 'playId','Group','position','YardLine_std','personnelD','numberOfPassRushers','defendersInTheBox','coverage','nflId']).agg({'X_std':[('X_Var', 'var')],\n'Y_std':[('Y_Var', 'var')],\n's':[('S_var', 'var')],\n'dx':[('Vardx', 'var')],\n'dy':[('Vardy', 'var')],\n'Opp_Dist_COpp':[('oppVar', 'var'),('oppMean', 'mean')],\n'Team_Dist_CTm':[('mateVar', 'var'),('mateMean', 'mean')],\n'diffDir':[('oppDirVar', 'var'),('oppDirMean', 'mean')],\n'disRatio':[('ratVar', 'var'),('ratMean', 'mean')],\n'Diff_WR':[('DiffO_var', 'var'),('DiffO_mean', 'mean')],\n'Diff_QB':[('Diff_QBvar', 'var'),('Diff_QBmean', 'mean')],\n'Looking_AtQB':[('Looking_AtQBvar', 'var'),('Looking_AtQBmean', 'mean'),('Looking_AtQBMax', 'max')],}).reset_index(drop=False)\nClustModel1.columns = ClustModel1.columns.map('_'.join)\nClustModel1 = ClustModel1.dropna()\nClustModel1['Man'] = np.where(ClustModel1['coverage_'].str.contains(\"Zone\") ,0, 1)\n\n\nClustModel1['Code'] = ClustModel1['gameId_'].astype(str) + \"-\" + ClustModel1['playId_'].astype(str)\n\n\nOffense = One.query('position == \"WR\" & OnOffense == True | position == \"TE\" & OnOffense == True | position == \"RB\" & OnOffense == True', engine='python')\n\nOffPos = Offense.groupby(['gameId','playId','position'])['nflId'].nunique().reset_index()\nOffPos = OffPos.pivot_table(index=['gameId','playId'], columns='position', values='nflId',aggfunc=np.sum, fill_value=0).reset_index()\nOffPos['Code'] = OffPos['gameId'].astype(str) + \"-\" + OffPos['playId'].astype(str)\nOffPos = OffPos.set_index('Code')\nOffPos = OffPos.loc[~OffPos.index.duplicated(keep='first')]\n\n\nClustModel1['WR'] = ClustModel1.Code.map(OffPos['WR'])\nClustModel1['TE'] = ClustModel1.Code.map(OffPos['TE'])\nClustModel1['RB'] = ClustModel1.Code.map(OffPos['RB'])\n\nDefense = One.query('position.str.contains(\"LB\", na=False) & OnOffense == False | position == \"CB\" & OnOffense == False | position == \"FS\" & OnOffense == False | position == \"SS\" & OnOffense == False | position == \"DB\" & OnOffense == False | position == \"S\" & OnOffense == False', engine='python')\n\n\nDefPos = Defense.groupby(['gameId','playId','position'])['nflId'].nunique().reset_index()\nDefPos = DefPos.pivot_table(index=['gameId','playId'], columns='position', values='nflId',aggfunc=np.sum, fill_value=0).reset_index()\nDefPos['Code'] = DefPos['gameId'].astype(str) + \"-\" + DefPos['playId'].astype(str)\nDefPos = DefPos.set_index('Code')\nDefPos = DefPos.loc[~DefPos.index.duplicated(keep='first')]\n\nClustModel1['CB'] = ClustModel1.Code.map(DefPos['CB'])\n\nClustModel1['FS'] = ClustModel1.Code.map(DefPos['FS'])\n\n\ndel One\ndel Offense\ndel OffPos\ndel Defense\ndel DefPos\ngc.collect()\n\n\nClustModel1['WRtoCBRatio'] = ClustModel1['WR'] \/ ClustModel1['CB']\nClustModel1 = ClustModel1.query('position_ == \"CB\"')","7c6386fe":"print(\"All Coverages\")\nprint(ClustModel1['coverage_'].value_counts(normalize=True))","c3f65053":"print(\"Man\/Zone Coverages\")\nprint(ClustModel1['Man'].value_counts(normalize=True))","05d1f1dc":"from catboost import CatBoostClassifier\nfrom scipy.stats import randint, uniform\nfrom sklearn.model_selection import StratifiedKFold,GridSearchCV, RandomizedSearchCV,cross_val_score\n\nfrom sklearn.model_selection import cross_val_predict,cross_val_score\nfrom sklearn.metrics import accuracy_score,f1_score\nimport seaborn as sns\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import svm\nfrom sklearn.ensemble import BaggingClassifier, VotingClassifier, RandomTreesEmbedding\n#from sklearn.pipeline import Pipeline\n\nfrom tqdm import tqdm\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import RidgeClassifier, SGDClassifier, LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.ensemble import BaggingClassifier, VotingClassifier, RandomTreesEmbedding\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom IPython.core.display import HTML\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.metrics import classification_report,auc,roc_auc_score,confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.pipeline import Pipeline\n\nimport sys\nimport warnings\nfrom pandas.core.common import SettingWithCopyWarning\n\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\ndef Predictions(X, y, g):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n    sm = SMOTE()  \n    clfs1 = []\n\n    clfs1.append((\"DecisionTree\",\n                Pipeline([(\"Under\", RandomUnderSampler(sampling_strategy='not majority')),\n                        (\"DecisionTree\", DecisionTreeClassifier())])))  \n\n    clfs1.append((\"RandomForestClassifier\",\n                Pipeline([(\"Under\", RandomUnderSampler(sampling_strategy='not majority')),\n                        (\"RandomForestClassifier\", RandomForestClassifier(n_estimators = 1000,random_state = 123,max_depth = 9,criterion = \"gini\"))]))) \n\n    clfs1.append((\"ExtraTreesClassifier\",\n                Pipeline([(\"Under\", RandomUnderSampler(sampling_strategy='not majority')),\n                        (\"ExtraTreesClassifier\", ExtraTreesClassifier())])))      \n\n    clfs1.append((\"AdaBoostClassifier\",\n                Pipeline([(\"Under\", RandomUnderSampler(sampling_strategy='not majority')),\n                        (\"AdaBoostClassifier\", AdaBoostClassifier())])))                             \n\n    clfs1.append((\"GradientBoostingClassifier\",\n                Pipeline([(\"Under\", RandomUnderSampler(sampling_strategy='not majority')),\n                        (\"GradientBoostingClassifier\", GradientBoostingClassifier())]))) \n\n\n    clfs1.append((\"CatboostClassifier\",\n                        Pipeline([(\"Under\", RandomUnderSampler(sampling_strategy='not majority')),\n                                (\"CatboostClassifier\", CatBoostClassifier(logging_level='Silent'))])))\n    \n\n\n    n_folds = 5\n    seed = 42\n    group = []\n    ModelName = []\n    TrainAcc = []\n    TestAcc = []\n    TrainF1 = []\n    TestF1 = []\n    features = []\n    for name, model  in clfs1:\n        try:\n            kfold = KFold(n_splits=n_folds, random_state=seed)\n            group.append(g)\n            ModelName.append(name)\n            OG = cross_val_predict(model, X_train, y_train, cv=kfold, n_jobs=30)\n            TrainAcc.append(accuracy_score(y_train, OG))     \n            model = model\n            model.fit(X_train,y_train)\n            YOne = model.predict(X_test)\n            TestAcc.append(accuracy_score(y_test, YOne))   \n            TrainF1.append(f1_score(y_train, OG))\n            TestF1.append(f1_score(y_test, YOne))\n            fea_imp = pd.DataFrame({'imp': model.steps[1][1].feature_importances_, 'col': X.columns})\n            fea_imp[['imp']] = MinMaxScaler().fit_transform(fea_imp[['imp']])\n            fea_imp = fea_imp.sort_values(['imp','col'], ascending=[False,False]).reset_index()\n    #        display(HTML(fea_imp.iloc[:5].to_html()))\n            features.append(fea_imp)\n        except:\n            continue\n\n\n\n\n\n\n    df = pd.DataFrame(list(zip(group,ModelName,TrainAcc,TestAcc,TrainF1,TestF1)), columns=['group','ModelName','TrainAcc','TestAcc','TrainF1','TestF1'])\n    feats = pd.concat(features)\n  #  display(HTML(df.sort_values(by=['TestF1'], ascending=False).head(5).to_html()))\n    return df,feats","4f4be724":"\n\nnp.set_printoptions(suppress=True)\n\n# Set the train dataset\n\nCols = ['X_std_X_Var', 'Y_std_Y_Var', 's_S_var', 'dx_Vardx', 'dy_Vardy', 'Opp_Dist_COpp_oppVar', 'Opp_Dist_COpp_oppMean', 'Team_Dist_CTm_mateVar', 'Team_Dist_CTm_mateMean', 'diffDir_oppDirVar', 'diffDir_oppDirMean', 'disRatio_ratVar', 'disRatio_ratMean', 'Diff_WR_DiffO_var', 'Diff_WR_DiffO_mean', 'Diff_QB_Diff_QBvar', 'Diff_QB_Diff_QBmean', 'Looking_AtQB_Looking_AtQBvar', 'Looking_AtQB_Looking_AtQBmean', 'WR', 'TE', 'RB', 'CB', 'FS', 'WRtoCBRatio']\n\nGroups = ClustModel1['Group_']\nGroups = Groups.drop_duplicates()\nGroups = [ 'Before Snap-1st Phase','Before Snap-2nd Phase','After Snap - Before Throw-1st Phase','After Snap - Before Throw-2nd Phase','Ball in the Air-1st Phase',\n 'Ball in the Air-2nd Phase']\n\n#Groups = [ 'Before Snap-1st Phase']\n\ndata=[]\nfeat_imp = []\n\nfor g in Groups:\n    grouped = ClustModel1[ClustModel1['Group_'] == g]\n    X = grouped[Cols]\n    y = grouped['Man']\n    # Scale the data\n    X[Cols] = SimpleImputer().fit_transform(X[Cols])\n    X[Cols] = StandardScaler().fit_transform(X[Cols])\n    df, feats = Predictions(X,y,g)\n    feats['Group'] = g\n    data.append(df)\n    feat_imp.append(feats)\n\nFinal = pd.concat(data)\n\ndel Final\ndel data\ngc.collect()\n\nFinalFeats = pd.concat(feat_imp)\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom itertools import cycle\n\nFinalFeats2 = FinalFeats.groupby(['Group','col'])['imp'].mean().reset_index()\n\nimport plotly.express as px\n\n\ndef change_name(x):\n            return x.replace('Opp_Dist_COpp_oppVar','Opp. Distance Var').replace('Team_Dist_CTm_mateVar','Teammate Distance Var').replace('Diff_QB_Diff_QBvar','QB\/Defender Orientation variance').replace('Diff_QB_Diff_QBmean','QB\/Defender Orientation mean').replace('Opp_Dist_COpp_oppMean','Opp. Distance Mean').replace('Team_Dist_CTm_mateMean','Teammate Distance Mean')\n    \nGroups = [ 'Before Snap-1st Phase','Before Snap-2nd Phase','After Snap - Before Throw-1st Phase','After Snap - Before Throw-2nd Phase','Ball in the Air-1st Phase',\n 'Ball in the Air-2nd Phase']\n\nfor g in Groups:\n    print(g)\n    grouped = FinalFeats2[FinalFeats2['Group'] == g].sort_values(by=['imp'], ascending=False).head(5)\n    grouped['col'] = grouped['col'].map(lambda x: change_name(x))\n    fig = px.bar(grouped, x='col', y='imp',color = 'col', template='plotly_dark',barmode='relative')\n    fig.show()\n\ndel FinalFeats\ndel FinalFeats2\ngc.collect()","9648a42b":"ClustModel1.groupby(['gameId_'])['coverage_'].count().reset_index()","2ded3b6a":"Games = ['2018090910','2018090912','2018091000','2018091001']\n\n#EntireData = ClustModel1.query('coverage_ == \"None\"')\n#ClustModel1 #= #ClustModel1.query('coverage_ != \"None\"')\nHoldOut = ClustModel1[(ClustModel1['gameId_'].isin(Games))]\nTrain = pd.concat([ClustModel1, HoldOut]).drop_duplicates(keep=False)","d8cebe25":"print(Train['Man'].value_counts(normalize=True))\nprint(HoldOut['Man'].value_counts(normalize=True))","c08626ea":"def MLPpredict(X_train, X_test, y_train, y_test,HoldX,Holdy,grouped,Holdgrouped,g):\n    if g == 'Before Snap-1st Phase':\n        MLPclf = MLPClassifier(max_iter=100,activation='logistic',alpha=0.0001,hidden_layer_sizes=(10, 120, 10),learning_rate='adaptive',solver='adam',random_state=65)\n    elif g =='Before Snap-2nd Phase':\n        MLPclf = MLPClassifier(max_iter=100,activation='logistic',alpha=0.0001,hidden_layer_sizes=(50, 100, 50),learning_rate='adaptive',solver='adam',random_state=65)\n    elif g == \"After Snap - Before Throw-1st Phase\":\n        MLPclf = MLPClassifier(max_iter=100,activation='tanh',alpha=0.05,hidden_layer_sizes=(100,),learning_rate='adaptive',solver='adam',random_state=65)\n    elif g == \"After Snap - Before Throw-2nd Phase\":\n        MLPclf = MLPClassifier(max_iter=100,activation='relu',alpha=0.01,hidden_layer_sizes=(100,),learning_rate='adaptive',solver='adam',random_state=65)\n    elif g == \"Ball in the Air-1st Phase\":\n        MLPclf = MLPClassifier(max_iter=100,activation='tanh',alpha=0.01,hidden_layer_sizes=(100,),learning_rate='constant',solver='adam',random_state=65)\n    elif g == \"Ball in the Air-2nd Phase\":\n        MLPclf = MLPClassifier(max_iter=100,activation='tanh',alpha= 0.001,hidden_layer_sizes=(100,),learning_rate='adaptive',solver='adam',random_state=65)\n    else:\n        MLPclf = MLPClassifier(max_iter=100,random_state=65)\n    return MLPclf","cd07af48":"from sklearn.mixture import GaussianMixture\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.metrics import classification_report,auc,roc_auc_score,confusion_matrix,precision_recall_curve, make_scorer, recall_score, accuracy_score, precision_score,roc_curve,f1_score\nfrom sklearn.utils.testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning,\n                        module=\"sklearn\")\n    \nnp.set_printoptions(suppress=True)\n\n# Set the train dataset\n\nCols = ['X_std_X_Var', 'Y_std_Y_Var', 's_S_var', 'dx_Vardx', 'dy_Vardy', 'Opp_Dist_COpp_oppVar', 'Opp_Dist_COpp_oppMean', 'Team_Dist_CTm_mateVar', 'Team_Dist_CTm_mateMean', 'diffDir_oppDirVar', 'diffDir_oppDirMean', 'disRatio_ratVar', 'disRatio_ratMean', 'Diff_WR_DiffO_var', 'Diff_WR_DiffO_mean', 'Diff_QB_Diff_QBvar', 'Diff_QB_Diff_QBmean', 'Looking_AtQB_Looking_AtQBvar', 'Looking_AtQB_Looking_AtQBmean', 'Looking_AtQB_Looking_AtQBMax', 'WR', 'TE', 'RB', 'CB', 'FS', 'WRtoCBRatio']\n\nGroups = ClustModel1['Group_']\nGroups = Groups.drop_duplicates()\nGroups = [ 'Before Snap-1st Phase','Before Snap-2nd Phase','After Snap - Before Throw-1st Phase','After Snap - Before Throw-2nd Phase','Ball in the Air-1st Phase',\n 'Ball in the Air-2nd Phase']\n\ndata = []\nHoldData = []\n\nfor g in Groups:\n    print(g)\n    grouped = Train[Train['Group_'] == g]\n    Holdgrouped = HoldOut[HoldOut['Group_'] == g]\n\n    X = grouped[Cols]\n    y = grouped['Man']\n\n    HoldX = Holdgrouped[Cols]\n    Holdy = Holdgrouped['Man']\n\n    HoldX[Cols] = SimpleImputer().fit_transform(HoldX[Cols])\n    HoldX[Cols] = StandardScaler().fit_transform(HoldX[Cols])\n\n\n    # Scale the data\n    X[Cols] = SimpleImputer().fit_transform(X[Cols])\n    X[Cols] = StandardScaler().fit_transform(X[Cols])\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n    #MLP\n    MLPclf = MLPpredict(X_train, X_test, y_train, y_test,HoldX,Holdy,grouped,Holdgrouped,g)\n    MLPclf.fit(X_train, y_train)\n    print('Best parameters found MLP:\\n', MLPclf)\n    y_true, y_pred = y_test, MLPclf.predict(X_test)\n\n    y_scores = MLPclf.predict_proba(X_test)[:,1]\n    predictions = MLPclf.predict(X_test)\n\n    from sklearn.metrics import classification_report\n    print(\"-------------------------------------------\")\n    false_pos_rate, true_pos_rate, proba = roc_curve(y_true, y_scores)\n    optimal_proba_cutoff = sorted(list(zip(np.abs(true_pos_rate - false_pos_rate), proba)), key=lambda i: i[0], reverse=True)[0][1]\n    print(\"Optimal Threshold: \", round(optimal_proba_cutoff,4))\n    roc_predictions = [1 if i >= optimal_proba_cutoff else 0 for i in y_scores]\n    print(\"-------------------------------------------\")\n    print('Results on the test set:')\n    print(\"-------------------------------------------\")\n    print(\"ROC Score Before and After Thresholding: {}, {}\".format(round(roc_auc_score(y_test, predictions),3), round(roc_auc_score(y_test, roc_predictions),3)))\n    print(\"Recall Score Before and After Thresholding: {}, {}\".format(round(recall_score(y_test, predictions),3), round(recall_score(y_test, roc_predictions),3)))\n    print(\"F1 Score Before and After Thresholding: {}, {}\".format(round(f1_score(y_test, predictions),3), round(f1_score(y_test, roc_predictions),3)))\n\n\n    grouped['MLPProb'] = MLPclf.predict_proba(X)[:,1]\n    grouped['MLPPred'] = np.where(grouped['MLPProb'] > optimal_proba_cutoff,1,0)\n    \n    \n\n    print(\"-------------------------------------------\")\n    #Hold Out\n    print(\"Results on the Hold Out set:\")\n    predictions = MLPclf.predict(HoldX)\n    Holdgrouped['MLPProb'] = MLPclf.predict_proba(HoldX)[:,1]\n    Holdgrouped['MLPPred'] = np.where(Holdgrouped['MLPProb'] > optimal_proba_cutoff,1,0)\n #   print(\"MLP: \",roc_auc_score(Holdgrouped['Man'], Holdgrouped['MLPPred']))\n\n    \n    print(\"ROC Score Before and After Thresholding: {}, {}\".format(round(roc_auc_score(Holdy, predictions),3), round(roc_auc_score(Holdy, Holdgrouped['MLPPred']),3)))\n    print(\"Recall Score Before and After Thresholding: {}, {}\".format(round(recall_score(Holdy, predictions),3), round(recall_score(Holdy, Holdgrouped['MLPPred']),3)))\n    print(\"F1 Score Before and After Thresholding: {}, {}\".format(round(f1_score(Holdy, predictions),3), round(f1_score(Holdy, Holdgrouped['MLPPred']),3)))\n\n    #EntireDataGrouped['MLPProb'] = MLPclf.predict_proba(AllX)[:,1]\n    print(\"-------------------------------------------\")\n    print(\"                                           \")\n    print(\"-------------------------------------------\")\n    data.append(grouped)\n    HoldData.append(Holdgrouped)\n\nFinal = pd.concat(data)\nHoldFinal = pd.concat(HoldData)","20c2fc4a":"First = Final.groupby(['gameId_','playId_','Man'])['MLPProb'].mean().reset_index()\nFirst['Pred'] = np.where(First['MLPProb'] > .5,1,0)\n\n\nfalse_pos_rate, true_pos_rate, proba = roc_curve(First['Man'], First['MLPProb'])\noptimal_proba_cutoff = sorted(list(zip(np.abs(true_pos_rate - false_pos_rate), proba)), key=lambda i: i[0], reverse=True)[0][1]\nprint(\"Cutoff: \",optimal_proba_cutoff)\nroc_predictions = [1 if i >= optimal_proba_cutoff else 0 for i in First['MLPProb']]\n\nFirst['Pred'] = np.where(First['MLPProb'] > optimal_proba_cutoff,1,0)\n\n\nprint(classification_report(First['Man'], First['Pred']))\nprint(\"ROC_AUC Score:\",roc_auc_score(First['Man'], First['Pred']))\nprint(\"Recall Score:\",recall_score(First['Man'], First['Pred']))\nprint(confusion_matrix(First['Man'], First['Pred']))","08a3f0be":"Second = HoldFinal.groupby(['gameId_','playId_','Man'])['MLPProb'].mean().reset_index()\nSecond['Pred'] = np.where(Second['MLPProb'] > .5,1,0)\n\n\nSecond['Pred'] = np.where(Second['MLPProb'] > optimal_proba_cutoff,1,0)\n\n\n\n\nprint(classification_report(Second['Man'], Second['Pred']))\nprint(\"ROC_AUC Score:\",roc_auc_score(Second['Man'], Second['Pred']))\nprint(\"Recall Score:\",recall_score(Second['Man'], Second['Pred']))\nprint(confusion_matrix(Second['Man'], Second['Pred']))","b995f61c":"from sklearn.metrics import roc_curve\nfrom matplotlib import pyplot\nAll = pd.concat([First,Second])\n\nprint(classification_report(All['Man'], All['Pred']))\n\nfpr, tpr, thresholds = roc_curve(All['Man'], All['Pred'])\n# plot the roc curve for the model\npyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')\npyplot.plot(fpr, tpr, marker='.', label='Logistic')\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\npyplot.legend()\n# show the plot\npyplot.show()","e836106a":"from sklearn.preprocessing import MinMaxScaler\n\nimport math\n\ndef round_half_down(n, decimals=0):\n    multiplier = 10 ** decimals\n    return math.ceil(n*multiplier - 0.5) \/ multiplier\n\nAll['Code'] = All['gameId_'].astype(str) + \"-\" + All['playId_'].astype(str)\nAll = All.set_index('Code')\nAll = All.loc[~All.index.duplicated(keep='first')]\n\nAllDetail = pd.concat([Final,HoldFinal])\nCols = ['gameId_','playId_','Group_','nflId_','MLPProb','MLPPred']\nAllDetails = AllDetail[Cols]\nAllDetails['Code'] = AllDetails['gameId_'].astype(str) + \"-\" + AllDetails['playId_'].astype(str)\nAllDetails['PlayPredProb'] = AllDetails.Code.map(All['MLPProb'])\nAllDetails['PlayPred'] = AllDetails.Code.map(All['Pred'])\n\nAllDetails['Code'] = AllDetails['gameId_'].astype(str) + \"-\" + AllDetails['playId_'].astype(str) + \"-\" + AllDetails['Group_'].astype(str) + \"-\" + AllDetails['nflId_'].astype(str)\nAllDetails = AllDetails.set_index('Code')\nAllDetails = AllDetails.loc[~AllDetails.index.duplicated(keep='first')]\nAllDetails[['MLPProb']] = MinMaxScaler().fit_transform(AllDetails[['MLPProb']])\nAllDetails[['PlayPredProb']] = MinMaxScaler().fit_transform(AllDetails[['PlayPredProb']])\n\n\nFinaldf1['Code'] = Finaldf1['gameId'].astype(str) + \"-\" + Finaldf1['playId'].astype(str) + \"-\" + Finaldf1['Group'].astype(str) + \"-\" + Finaldf1['nflId'].astype(str)\n\nFinaldf1['PlayerPredProb'] = Finaldf1.Code.map(AllDetails['MLPProb'])\nFinaldf1['PlayerPred'] = Finaldf1.Code.map(AllDetails['MLPPred'])\nFinaldf1['PlayPredProb'] = Finaldf1.Code.map(AllDetails['PlayPredProb'])\nFinaldf1['PlayPred'] = Finaldf1.Code.map(AllDetails['PlayPred'])\n\nFinaldf1['PlayerPredProb'] = Finaldf1['PlayerPredProb'].map(lambda x: np.around(x, decimals=4))\nFinaldf1['PlayPredProb'] = Finaldf1['PlayPredProb'].map(lambda x: np.around(x, decimals=4))","98cbb950":"import math\n#from BDBUtils.Utilities import AnimatePlay\n#sns\nimport glob\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\npd.set_option('max_columns', 1000)\n\n#pd.options.display.float_format = '{:.4f}'.format\n\ngId = 2018090902\t\npId = 2043\nnflId = [2543474,2533060,2556367]\n\n\n\nFinaldf1['NewPlayerPredProb'] = np.where(Finaldf1['PlayerPredProb'] > .5,\"Man \" + (round(Finaldf1['PlayerPredProb'],2)).astype(str), \"Zone \" + round((1 - Finaldf1['PlayerPredProb']),2).astype(str)) \n\n\nex = Finaldf1.query('gameId == @gId & playId == @pId')\n\n\nOne = ex.query('position == \"CB\" | OnOffense == True | displayName == \"Football\"')\n\nfrom matplotlib.patches import Polygon\n\n\nanimated_play = AnimatePlay(One,player_id=nflId, Tri = False, MPH = False,Text=\"\",Show='NewPlayerPredProb',method='single' )\nHTML(animated_play.ani.to_jshtml())","761aceed":"gId = 2018090600\t\npId = 4142\nnflId = [2555383,2557958,2552689]\n\nFinaldf1['CoveragePred'] = np.where(Finaldf1['PlayerPred'] == 1,\"Man\",\"Zone\")\nex = Finaldf1.query('gameId == @gId & playId == @pId')\n\nOne = ex.query('position == \"CB\" | OnOffense == True | displayName == \"Football\"')\ntwo = One.query('After_PassArrived == 0')\n#two.filter(['event','displayName','Orientation_std','QBslope1','WRslope1','Def_Or','Diff_QB','Diff_WR',\t'DIFFQB','DIFFWR','Player_POV'], axis=1).head(70)\nfrom matplotlib.patches import Polygon\n\n\nanimated_play = AnimatePlay(two,player_id=nflId, Tri = False, MPH = False,Text=\"\",Show='NewPlayerPredProb',method='single' )\nHTML(animated_play.ani.to_jshtml())","4d322642":"---------------------------------------------------------------------------------------------------------------------------------------","28965cd4":"The holdout set peformed pretty well! We applied the same .34 cutoff threshold and received An AUC score of 81% and a recall score of 80%, these metrics prove that this model can be quite useful in predicting Man\/Zone coverages","a4acf83f":"You can see cornerback Donte Jackson starts the play looking at the Quarterback, but then immediately turns to face the WR and continued to do so for the rest of the play and we were able to capture that.\n\n* POV = \"Point of View\" + the position of the player he's looking at\n* Blue Arrows = Where the defender is looking\n* Black Arrows = The direction the defender is moving\n* Green Triangle = The triangle formed from the coordinates of the Defender, the closest offensive player at all times, and the Quarterback","10949ae2":"------------------------------------------------------------------------------------------------","29428c85":"# Model\n\nWe will train a model for each partition independently from one another, and gather the expected probabilities for each cornerback in each partition. Since the dataset is imbalanced with nearly 66% of the data favoring zone coverage, we will add in a decision threshold adjuster for each partition that levels the playing field. After we loop through each event, we append the predictions to the original dataset and then concatenate all the partitions back together. We will do this with the train\/test dataset and compare results on the holdout to see how well the model performed.","8372ba46":"We will train\/test the model on the first 9 games, then test the model again on a hold out set using the last 4 games: 2018090910, 2018090912, 2018091000, 2018091001","4e486e11":"# Zone Coverage\n\nHere is an example of Zone coverage prediction","30208823":"# Scoring\n\nInstead of analyzing accuracy, we will instead focus on recall and F1 scores for the model, this is best practice when handling an imbalanced dataset.  ","e1855eb9":"# Other Work:\n\n1. [Shadow Cornerback + Coverage Analysis](https:\/\/www.kaggle.com\/jdruzzi\/shadow-cornerback-coverage-analysis)\n\n2. [Defender Bite Velocity on Play-Action](https:\/\/www.kaggle.com\/jdruzzi\/defender-bite-velocity-on-play-action)\n\n3. [Pass Coverage Classification](https:\/\/www.kaggle.com\/jdruzzi\/pass-coverage-classification-80-recall)\n\n4. [Quantifying Press Coverage](https:\/\/www.kaggle.com\/jdruzzi\/quantifying-press-coverage-ability)\n\n5. [Defender Tendencies: One-Cut Routes + Double Moves](https:\/\/www.kaggle.com\/jdruzzi\/defender-tendencies-one-cut-routes-double-moves)\n\n## Data:\n\n[Revised BDB Data](https:\/\/www.kaggle.com\/jdruzzi\/revised-bdb-data)\n","0b22a809":"## Decision Threshold Adjuster\n\nThe default threshold for a binary classifier model is .5, in our case, indicating any probability below .5 is labeled Zone coverage and anything above .5 is labeled man coverage. With the threshold adjuster, you will see that in some cases the threshold could move down to around .3, meaning a .32 probability of man coverage will be labeled as man coverage. After incorporating this method, our recall and F1 scores vastly improve!","e2caf02a":"# Feature Importance for each Partition\n\nWe will loop through each partition and see which features will have the most influence on our predictions, for each partition.","ca774601":"# The Data","29e6bb11":"The train\/test dataset and the hold out set have roughly the same Man\/Zone distribution","6e15f84d":"![Desktop%20Screenshot%202020.11.27%20-%2023.38.35.44%20%282%29.png](attachment:Desktop%20Screenshot%202020.11.27%20-%2023.38.35.44%20%282%29.png)","ab0fd706":"---------------------------------------------------------------------------------------------------------------------------------------------------------","59ef7668":"# Man Coverage\n\nHere is an example of predicted man coverage play","2dd4d4db":"# Splitting the Data","2bc516c1":"![Desktop%20Screenshot%202020.11.28%20-%2019.25.53.20%20%282%29.png](attachment:Desktop%20Screenshot%202020.11.28%20-%2019.25.53.20%20%282%29.png)","e851eccc":"# For Each Partition\n\n![Desktop%20Screenshot%202020.12.01%20-%2013.36.43.49%20%282%29.png](attachment:Desktop%20Screenshot%202020.12.01%20-%2013.36.43.49%20%282%29.png)","bf045a16":"# Train\/Test Dataset\n\nAfter we get the probabilites from the defenders, we average all the probabilities for each play to get a final probabilitiy prediction which is either Man or Zone. We also include a threshold adjuster for this situation as well.","f1c21590":"We will concatenate all data together and append the results back to the original dataset","1538712e":"# Player Orientation\n\nFirst I want to address the \"Discussion and Future Work\" portion of Rishav Dutta, Ronald Yurko, and Samuel Ventura's paper. They proposed that orientation of the defender could be a key metric when determining defensive coverage, unfortunately they didn't have access to that data at the time. ","2c077edd":"The same can be applied to James Bradberry on the other side.","20906b2c":"# Pass Coverage Classification - A Supervised Approach","bcef3fe0":"You can see after adjusting the threshold cutoff to .34, we have an AUC score and Recall score of 87% for the concatenated train\/test set. \n\n**However,** this is slightly overfitting because a good portion of this data was used to train the model on. The true test will be how well the hold out set did. ","90f3c0e7":"------------------------------------------------------------------------------------------------------------","65a07dcb":"# Introduction\n\nFor this submission, we want to identify Man\/Zone coverage probabilities throughout the play for cornerbacks using a MLP Model based approach for each event partition throughout the play. Then based on the probabilities of the defenders, we can also determine the coverage scheme for the entire play.\n\nThis is a different approach compared to the one proposed in this [paper](https:\/\/arxiv.org\/pdf\/1906.11373.pdf) by Rishav Dutta, Ronald Yurko, Samuel Ventura and @AndikaRachman's rendition of [\"Identifying Coverage Scheme Among Defensive Backs\"](https:\/\/www.kaggle.com\/ar2017\/identifying-coverage-scheme-among-defensive-backs) submission. Both use a Gaussian Mixture Model to determine coverage probabilities, however, the lack of labels in unsupervised models make it difficult to see how well the model actually did. Luckily, @tombliss's [notebook](https:\/\/www.kaggle.com\/tombliss\/additional-data-coverage-schemes-for-week-1) and Telemetry sports gifted us coverage data for nearly all week 1 plays.","638d6b11":"For this study, we will breakdown those coverages into Man or Zone. Zone occurs more often at 66% of plays\n\nFor this study:\n\nZone = 0\n\nMan = 1","992c4055":"# Event Partitioning \n\nFor this modeling approach, we want to isolate the events \"Before the Snap\", \"After Snap - Before the Throw\", and \"Ball in the Air\"\n\nThen for each event, we split into 2 segments, giving us 6 total partitions of data throughout the play.","2d073d77":"# The Variables\n\nNow that we have orientation figured out, we will use some of the same variables in the paper including a couple more.\n\n* X_Var - Variance of X Coordinate\n\n* Y_Var - Variance of Y Coordinate\n\n* S_var - Variance of Speed\n\n* Vardx - Variance of X Velocity\n\n* Vardy - Variance of Y Velocity\n\n* oppMean - Closest Opp. Distance Mean\n\n* oppVar - Closest Opp. Distance Var\n\n* mateMean - Closest Teammate Distance Mean\n\n* mateVar - Closest Teammate Distance Var\n\n* oppDirVar - Difference in Closest Opp. Direction Variance\n\n* oppDirMean - Difference in Closest Opp. Direction Mean\n\n* ratVar - Ratio of the distance to the nearest opponent, and the distance from the nearest opponent to the nearest teammate Variance\n\n* ratMean - Ratio of the distance to the nearest opponent, and the distance from the nearest opponent to the nearest teammate Mean\n\n* DiffO_mean - Orientation difference in Closest Opp. mean\n\n* DiffO_var - Orientation difference in Closest Opp. variance\n\n* Diff_QBmean - Orientation difference in QB mean\n\n* Diff_QBvar - Orientation difference in QB variance\n\n* Looking_AtQBmean - Take the mean of where the Defender is looking (1 = QB, 0 = Closest Offensive Player)\n\n* Looking_AtQBVar - Take the variance of where the Defender is looking (1 = QB, 0 = Closest Offensive Player)\n\n* WR - # of Wide receivers\n\n* TE - # of Tightends\n\n* RB - # of Runningbacks\n\n* CB - # of Cornerbacks\n\n* FS - # of Free Safeties\n\n* WRtoCBRatio - The ratio of Wide receivers to cornerbacks","fa6f2583":"# Final Thoughts:\n\nEven with only 9 games to train and test the model on, it was still able to make solid predictions on 4 out of sample games using only certain features of the cornerbacks throughout the play. With the addition of more coverage label data and fine tuning hyperparameters, this model could vastly improve.","bb45b57e":"# Figuring Out Orientation\n\nUsing some magical geometry techniques we can determine where the cornerback is looking throughout the play.\n\nAll we need is: \n\n* Defender's Orientation\n* Defender's X and Y coordinates\n* Closest Offensive Player's X and Y coordinates\n* Quarterback's X and Y coordinates\n\nThe coordinate metrics will help us form a triangle on the field, and the defenders orientation will be the needle that either approaches the quarterback's slope side of the triangle, or the closest opposing offensive player's slope side.\n\nLet's take a look:","3ab9e46c":"Using the data provided by Telemetry sports, here are all coverage types with their frequencies","cc30b1be":"It appears that \"distance between defender and closest offensive player\" mean has the greatest level of influence, with Defender\/QB orientation difference and distance from closest teammate as close 2nd and 3rd features"}}