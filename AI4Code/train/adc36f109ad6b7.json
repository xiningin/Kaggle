{"cell_type":{"2c736f1a":"code","88bc9075":"code","ffb77f9c":"code","5bd95411":"code","2ea2794d":"code","16110db9":"code","bd74f961":"code","3bcfd428":"code","3ba2b4b8":"code","981cbb76":"code","74dd5c77":"code","0817a7ce":"code","928f4d99":"code","3019942d":"code","69008df4":"code","084e1abb":"code","53609e5c":"code","573aff1d":"code","811ee20c":"code","0d77ca05":"code","50b2522b":"code","287aa754":"code","7b4ea231":"code","03486fa2":"code","5c518df6":"code","4d6ff2e5":"code","355ebdae":"code","3e6de9e9":"code","209da8e7":"code","100662a8":"code","4bda6fc6":"code","6f21c531":"code","dda60df6":"code","b28dac40":"code","f8916ed2":"code","01eb86a7":"code","837e11e5":"code","82ae2dcc":"code","88b9b341":"code","aad0f874":"code","904cc79d":"code","db522a93":"code","e893cce6":"code","ef9be42b":"code","5486f741":"code","ba78c577":"code","6bd66c53":"code","69e037e2":"code","43619385":"code","938996bf":"code","7c325445":"code","70a3ec2d":"code","2a518af6":"code","a1f0f519":"code","89474438":"code","0b1364d2":"code","90711013":"code","b3c5c9b7":"code","293e48dc":"code","db7eb5c7":"code","c3ea6213":"code","ea496b02":"code","ce43a271":"code","ef34666d":"code","8eaae4ff":"code","2d3b70d0":"code","d9f93d9b":"code","ff65b23d":"code","d7efb674":"code","583b553c":"code","0804a8c9":"code","1b165670":"code","46addd4d":"code","61dae59c":"code","d036eeeb":"code","3f9a1086":"code","7bfb007b":"code","38d72930":"code","3f1a8b7a":"code","ec4de761":"code","5f8b57fb":"code","4c8eceba":"code","6ba0c13a":"code","ffdf2827":"code","9965641e":"code","f7dca1bc":"code","96ee7684":"code","1fd35307":"code","02bc6524":"code","b707ef70":"code","75c70478":"code","e5de2b75":"code","e31c975f":"code","56a59d13":"code","68b66b52":"code","31eddfaf":"markdown","4584af2a":"markdown","0876228f":"markdown","9519757c":"markdown","971e826f":"markdown","9ae6d0bc":"markdown","b0220197":"markdown","e1639419":"markdown","8a1c6a00":"markdown","b21f1f9d":"markdown","db622670":"markdown","2110e017":"markdown","d3017ada":"markdown","305ca4e0":"markdown","949b86c6":"markdown","bdc9095a":"markdown","24591d4e":"markdown","567651c0":"markdown","012e1764":"markdown","c4627b27":"markdown","56766a3c":"markdown","4b54a6f6":"markdown","fcf3823b":"markdown","3a76ca4e":"markdown","f7b5bc29":"markdown","b0f0159c":"markdown","94805165":"markdown","84eb7a04":"markdown","89114c58":"markdown","166dd450":"markdown","0c6ff3d0":"markdown","78dbba25":"markdown"},"source":{"2c736f1a":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom scipy.stats import chi2_contingency\nimport category_encoders as ce\nfrom sklearn.model_selection import GridSearchCV","88bc9075":"train=pd.read_csv('..\/input\/titanic\/train.csv')\ntest=pd.read_csv('..\/input\/titanic\/test.csv')","ffb77f9c":"train.head(5)","5bd95411":"# Printing number of columns and rows in the dataset\nprint(\"There are {} number of rows and {} number of columns in training data\".format(train.shape[0],train.shape[1]))\nprint(\"There are {} number of rows and {} number of columns in testing data\".format(test.shape[0],test.shape[1]))","2ea2794d":"# Checking for data imbalanceness if any\nsns.countplot(y=train[\"Survived\"])","16110db9":"# Checking the type of columns in dataset\ntrain.info()","bd74f961":"# Describing columns statistics\ntrain.describe()","3bcfd428":"train.isnull().sum()","3ba2b4b8":"test.isnull().sum()","981cbb76":"plt.figure(figsize=(12,9))\nplt.subplot(1,2,1)\nplt.title(\"Training Data\")\nsns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap=\"viridis\")\n\nplt.subplot(1,2,2)\nplt.title(\"Testing Data\")\nsns.heatmap(test.isnull(),yticklabels=False,cbar=False,cmap=\"viridis\")","74dd5c77":"# Null features\nnull_features = [feature for feature in train.columns if train[feature].isnull().sum()>=1]\nfor features in null_features:\n    print(features,np.round(train[features].isnull().mean(),4),'%missing values')\n    ","0817a7ce":"# Explore Numerical Variables\nnumerical_features = [feature for feature in train.columns if train[feature].dtypes!='O']\nprint(\"The number of numerical features in the dataset are {}.\".format(len(numerical_features)))\ntrain[numerical_features].head()","928f4d99":"# Discrete variables in data\ndiscrete_features = [feature for feature in numerical_features if len(train[feature].unique())<25 and feature not in ['PassengerId','Survived']]\nprint(\"The number of discrete variables is: {}\".format(len(discrete_features)))\ntrain[discrete_features].head()","3019942d":"# Understanding the relationship between discrete and target variables\nfor feature in discrete_features:\n    data = train.copy()\n    data.groupby(feature)['Survived'].mean().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('Survived')\n    plt.show()","69008df4":"# Continuous Distribution \ncontinuous_features = [feature for feature in numerical_features if feature not in discrete_features and feature not in ['PassengerId','Survived']]\nprint(\"The number of continuous features are : {}\".format(len(continuous_features)))\ntrain[continuous_features].head()","084e1abb":"# Relationship between continous and target variable\nfor feature in continuous_features:\n    data = train.copy()\n    data[feature].hist(bins=50)\n    plt.xlabel(feature)\n    plt.ylabel('Survived')\n    plt.show()","53609e5c":"# Detecting Outliers\n\nfor feature in numerical_features:\n    data = train.copy()\n    data.boxplot(column=feature)\n    plt.xlabel(feature)\n    plt.title(feature)\n    plt.show()","573aff1d":"for feature in numerical_features:\n    data = train.copy()\n    sns.scatterplot(x=data[feature],y=data['Survived'])\n    plt.xlabel(feature)\n    plt.title(feature)\n    plt.show()","811ee20c":"# Computing the correlation of variables\nplt.figure(figsize=(20,12))\nsns.heatmap(train[numerical_features].corr(),annot=True)","0d77ca05":"# Categorical Features\ncategorical_feature = [feature for feature in train.columns if feature not in numerical_features]\nprint(\"There are {} number of categorical features\".format(len(categorical_feature)))\ntrain[categorical_feature].head()","50b2522b":"# Determining the cadinality of categorical data\nfor feature in categorical_feature:\n    print(\"The feature is {} and its cardinality is {}\".format(feature,len(train[feature].unique())))","287aa754":"# Visualizing relationsip between categorical and target values\nfor feature in ['Sex','Embarked']:\n    data = train.copy()\n    data.groupby(feature)['Survived'].mean().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('Survived')\n    plt.title(feature)\n    plt.show()","7b4ea231":"dummy = train.copy()\nplt.figure(figsize=(12,7))\nsns.boxplot(x=dummy[\"Pclass\"],y=dummy[\"Age\"])","03486fa2":"# Inspecting Name feature\ntrain[\"Name\"].tail()","5c518df6":"import re\ndummy_train = train.copy()\ndummy_test = test.copy()\ndummy_train[\"Title\"] = dummy_train.Name.apply(lambda x:re.search(' ([A-Z][a-z]+)\\. ',x).group(1))\ndummy_test[\"Title\"] = dummy_test.Name.apply(lambda x:re.search(' ([A-Z][a-z]+)\\. ',x).group(1))\nplt.figure(figsize=(10,9))\nplt.subplot(1,2,1)\nsns.countplot(x=\"Title\",data = dummy_train)\nplt.xticks(rotation=45) \n\nplt.subplot(1,2,2)\nsns.countplot(x=\"Title\",data = dummy_test)\nplt.xticks(rotation=45)                                              ","4d6ff2e5":"dummy_train['Title'] = dummy_train['Title'].replace({'Mlle':'Miss', 'Mme':'Mrs', 'Ms':'Miss'})\ndummy_train['Title'] = dummy_train['Title'].replace(['Don', 'Dona', 'Rev', 'Dr',\n                                            'Major', 'Lady', 'Sir', 'Col', 'Capt', 'Countess', 'Jonkheer'],'Special')\n\ndummy_test['Title'] = dummy_test['Title'].replace({'Ms':'Miss'})\ndummy_test['Title'] = dummy_test['Title'].replace(['Dona', 'Rev', 'Dr',\n                                            'Col'],'Special')\nplt.subplot(1,2,1)\nsns.countplot(x='Title', data=dummy_train);\nplt.xticks(rotation=45);\n\nplt.subplot(1,2,2)\nsns.countplot(x='Title', data=dummy_test);\nplt.xticks(rotation=45);","355ebdae":"#Listing Null values in Numerical data\nnumerical_with_nan = [feature for feature in train.columns if train[feature].isnull().sum()>=1 and train[feature].dtypes!='O' ]\nfor feature in numerical_with_nan:\n    print(feature,np.round(train[feature].isnull().mean(),4),'%missing values')","3e6de9e9":"# Filling the missing values with median\ndef impute_numerical(n_feature):\n    \n        median_value_train = train[n_feature].median()\n        median_value_test = test[n_feature].median()\n        train[feature+'_nan'] = np.where(train[n_feature].isnull(),1,0)\n        train[feature] = train[n_feature].fillna(median_value_train)\n        test[feature+'_nan'] = np.where(test[n_feature].isnull(),1,0)\n        test[feature] = test[n_feature].fillna(median_value_test)\n        return train,test\n    \ntrain,test = impute_numerical(numerical_with_nan)\ntrain[numerical_with_nan].isnull().sum()","209da8e7":"train.head()","100662a8":"test.head()","4bda6fc6":"#Listing Null values in Categorical data\ncategorical_with_nan = [feature for feature in train.columns if train[feature].isnull().sum()>=1 and train[feature].dtypes=='O' ]\nfor feature in categorical_with_nan:\n    print(feature,np.round(train[feature].isnull().mean(),4),'%missing values')","6f21c531":"# Filling the embarked column with mode count.\ndef replace_cat_feature(train,test,feature):\n    train_data = train.copy()\n    test_data = test.copy()\n    train_data[feature] = np.where(train_data[feature].isnull(),train_data[feature].mode(),train_data[feature])\n    test_data[feature] =  np.where(test_data[feature].isnull(),test_data[feature].mode(),test_data[feature])\n    return train_data,test_data\n\ntrain,test = replace_cat_feature(train,test,['Embarked'])\ntrain[categorical_with_nan].isnull().sum()","dda60df6":"train.head()","b28dac40":"train.isnull().sum()  ","f8916ed2":"test.isnull().sum()","01eb86a7":"# Filling missing values in Fare column in test data\ntest['Fare'] = test['Fare'].fillna(test['Fare'].mean())","837e11e5":"test.isnull().sum()","82ae2dcc":"test_data = test.copy()","88b9b341":"# Feature Engineering numerical variables: [Pclass\tAge\tSibSp\tParch\tFare]\n\ndef eng_age(train_data,testing_data):\n    train_data['Age_cat'] = pd.qcut(train_data.Age,  q=4, labels=False)\n    testing_data['Age_cat'] = pd.qcut(testing_data.Age, q=4, labels=False)\n    return train_data,testing_data\n\n\ndef eng_fare(train_data,testing_data):\n    train_data[\"Fare_cat\"] = pd.qcut(train_data[\"Fare\"], q=4, labels=False)\n    testing_data[\"Fare_cat\"] = pd.qcut(testing_data[\"Fare\"], q=4, labels=False)\n    return train_data,testing_data\n\ndef eng_family(train_data,testing_data):\n    train_data[\"Family_size\"] = train_data[\"Parch\"] + train_data[\"SibSp\"]\n    testing_data[\"Family_size\"] = testing_data[\"Parch\"] + testing_data[\"SibSp\"]\n    return train_data,testing_data\n\n\ndef feature_eng_numerical(train,test_data):\n    train_data = train.copy()\n    testing_data = test_data.copy()\n    \n    train_data,testing_data = eng_family(train_data,testing_data)\n    \n    train_data,testing_data = eng_age(train_data,testing_data)\n    \n    train_data,testing_data = eng_fare(train_data,testing_data)\n    \n    return train_data,testing_data\n\ntrain,test_data = feature_eng_numerical(train,test_data) \n","aad0f874":"# Dropping columns which contribute less.['PassengerId','Cabin','Ticket']\n\ntrain = train.drop(columns=['PassengerId','Ticket','Cabin'],axis=1)\ntest_data = test_data.drop(columns=['PassengerId','Ticket','Cabin'],axis=1)","904cc79d":"train.head()","db522a93":"test_data.head()","e893cce6":"# Feature Engineering Categorical Variable: 'Embarked'\nimport re\n\ndef eng_categorical(train,test_data):\n    train_data = train.copy()\n    testing_data = test_data.copy()\n    \n    \n    train_data['Embarked_min'] = np.where(train['Embarked']=='Q',1,0)\n    train_data['Embarked_max'] = np.where(train['Embarked']=='S',1,0)\n    testing_data['Embarked_min'] = np.where(testing_data['Embarked']=='Q',1,0)\n    testing_data['Embarked_max'] = np.where(testing_data['Embarked']=='S',1,0)\n    \n    train_data[\"Title\"] = train_data.Name.apply(lambda x:re.search(' ([A-Z][a-z]+)\\. ',x).group(1))\n    testing_data[\"Title\"] = testing_data.Name.apply(lambda x:re.search(' ([A-Z][a-z]+)\\. ',x).group(1))\n    train_data['Title'] = train_data['Title'].replace({'Mlle':'Miss', 'Mme':'Mrs', 'Ms':'Miss'})\n    train_data['Title'] = train_data['Title'].replace(['Don', 'Dona', 'Rev', 'Dr',\n                                            'Major', 'Lady', 'Sir', 'Col', 'Capt', 'Countess', 'Jonkheer'],'Special')\n    testing_data['Title'] = testing_data['Title'].replace({'Ms':'Miss'})\n    testing_data['Title'] = testing_data['Title'].replace(['Dona', 'Rev', 'Dr',\n                                                'Col'],'Special')\n    return train_data,testing_data\n\ntrain,test_data = eng_categorical(train,test_data)\ntrain.head()\n","ef9be42b":"train = train.drop(columns=[\"Name\",\"Age\",\"Fare\"],axis=1)\ntest_data = test_data.drop(columns=[\"Name\",\"Age\",\"Fare\"],axis=1)","5486f741":"train.head()","ba78c577":"# Encoding Columns with category\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n#cols = ['Cabin']\ncols = ['Sex','Embarked','Title']\n\ndef encoding_columns(train,test_data,cols):\n    encoded_train = train.copy()\n    encoded_test =  test_data.copy()\n    for feature in cols:\n        encoded_train[feature] = le.fit_transform(train[feature])\n        encoded_test[feature] =  le.fit_transform(test_data[feature])\n    return encoded_train,encoded_test\n\n\ntrain_encoded,test_encoded = encoding_columns(train,test_data,cols)\ntrain_encoded.head()\n","6bd66c53":"test_encoded.head()","69e037e2":"# Spitting the independent and dependent variables\ny_train_splitted = train_encoded[[\"Survived\"]]\nx_train = train_encoded.drop(columns=[\"Survived\"],axis=1)\n","43619385":"# Standardizing the variables\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\nX_train_norm = pd.DataFrame(sc.fit_transform(x_train))\nX_train_norm.columns = x_train.columns\n\n\nX_test_norm = pd.DataFrame(sc.fit_transform(test_encoded))\nX_test_norm.columns = test_encoded.columns\n\nX_train_norm.head()","938996bf":"X_test_norm.head()","7c325445":"X_train_norm.to_csv(\"training.csv\",index=False)\nX_test_norm.to_csv(\"testing.csv\",index=False)\n","70a3ec2d":"X_train_normed = pd.read_csv(\"training.csv\")\nX_test_normed = pd.read_csv(\"testing.csv\")","2a518af6":"# We will perform feature selection using SHAP Values\n# Using Extra Tree Classifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nmodel=ExtraTreesClassifier()\nmodel.fit(X_train_normed,y_train_splitted.values.ravel())","a1f0f519":"feat_importances=pd.Series(model.feature_importances_,index=X_train_normed.columns)\nfeat_importances.nlargest(23).plot(kind=\"barh\")","89474438":"\n'''\nX_train_norm_select = X_train_normed\nX_train_norm = X_train_norm_select[['Sex','Fare_cat','Age_cat',\"Title\",'Pclass',\n                                    \"Family_size\",\"SibSp\",\"Parch\",\"Age_nan\",\"Embarked_max\"\n                                ]]\n\n","0b1364d2":"'''\nX_test_normed = X_test_normed[['Sex_female','Fare','Age',\"Title_Mr\",'Pclass',\"Sex_male\",\"Ticket_Freq_Count\",\n                                    \"Has_Cabin\",\"Family_size\",\"Title_Mrs\",\"Title_Miss\",\"SibSp\",\"Parch\"\n                                ]]\n                               #'SibSp','Agenan','Embarked','Embarked_max','Parch']]\n                               #,'Parch','Mean_Fare',\n                                  # 'Age>18','Agenan','Embarked','Embarked_max','Embarked_min']]\n                                  '''\n","90711013":"# Spitting the data\nfrom sklearn.model_selection import train_test_split\nX_train,x_test,Y_train,y_test = train_test_split(X_train_normed,y_train_splitted,test_size=0.1,stratify=y_train_splitted,random_state=0)","b3c5c9b7":"x_train,x_valid,y_train,y_valid = train_test_split(X_train,Y_train,test_size=0.1,stratify=Y_train,random_state=0)","293e48dc":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score,f1_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB","db7eb5c7":"model_1=LogisticRegression(max_iter=500,random_state=0)\nmodel_1.fit(x_train,y_train.values.ravel())\npred = model_1.predict(x_valid)\nscore_1 = accuracy_score(y_valid,pred)\nscore_1","c3ea6213":"predictions_1 = model_1.predict(x_test)\nscore_1 = accuracy_score(y_test,predictions_1)\nscore_1","ea496b02":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,predictions_1,target_names=['0','1']))\n","ce43a271":"model_2 = KNeighborsClassifier()\nmodel_2.fit(x_train,y_train.values.ravel())\npred = model_2.predict(x_valid)\nscore_2 = accuracy_score(y_valid,pred)\nscore_2","ef34666d":"predictions_2 = model_2.predict(x_test)\nprint(classification_report(y_test,predictions_2,target_names=['0','1']))","8eaae4ff":"model_3=SVC(random_state=0)\nmodel_3.fit(x_train,y_train.values.ravel())\npred = model_3.predict(x_valid)\nscore_3 = accuracy_score(y_valid,pred)\nscore_3","2d3b70d0":"predictions_3 = model_3.predict(x_test)\nprint(classification_report(y_test,predictions_3,target_names=['0','1']))","d9f93d9b":"model_4=GaussianNB()\nmodel_4.fit(x_train,y_train.values.ravel())\npred = model_4.predict(x_valid)\nscore_4 = accuracy_score(y_valid,pred)\nscore_4","ff65b23d":"predictions_4 = model_4.predict(x_test)\nprint(classification_report(y_test,predictions_4,target_names=['0','1']))","d7efb674":"model_5=DecisionTreeClassifier(random_state=0)\nmodel_5.fit(x_train,y_train.values.ravel())\npred = model_5.predict(x_valid)\nscore_5 = accuracy_score(y_valid,pred)\nscore_5","583b553c":"predictions_5 = model_5.predict(x_test)\nprint(classification_report(y_test,predictions_5,target_names=['0','1']))","0804a8c9":"model_6=RandomForestClassifier(random_state=0)\nmodel_6.fit(x_train,y_train.values.ravel())\npred = model_6.predict(x_valid)\nscore_6 = accuracy_score(y_valid,pred)\nscore_6","1b165670":"predictions_6 = model_6.predict(x_test)\nprint(classification_report(y_test,predictions_6,target_names=['0','1']))","46addd4d":"from xgboost import XGBClassifier\nmodel_7 = XGBClassifier()\nmodel_7.fit(x_train,y_train.values.ravel())\npred = model_7.predict(x_valid)\nscore_7 = accuracy_score(y_valid,pred)\nscore_7","61dae59c":"predictions_7 = model_7.predict(x_test)\nprint(classification_report(y_test,predictions_7,target_names=['0','1']))","d036eeeb":"from catboost import CatBoostClassifier\ncat_model = CatBoostClassifier(verbose=2,iterations=500,od_type='Iter')\ncat_model.fit(x_train,y_train,eval_set=(x_valid,y_valid))\nprint(cat_model.best_score_)","3f9a1086":"predictions_8 = cat_model.predict(x_test)\nprint(classification_report(y_test,predictions_8,target_names=['0','1']))","7bfb007b":"from sklearn.model_selection import RandomizedSearchCV\nparam_grid={'max_depth':range(3,6),'n_estimators':range(400,700,100),\"max_features\": range(3,6)}\ngrid_search = RandomizedSearchCV(RandomForestClassifier(),param_grid,verbose=1,cv=10,n_jobs=-1)\ngrid_search.fit(x_train,y_train.values.ravel())","38d72930":"grid_search.best_estimator_","3f1a8b7a":"grid_search_pred = grid_search.predict(x_valid)\nscore = accuracy_score(y_valid,grid_search_pred)\nscore","ec4de761":"grid_search_predictions = grid_search.predict(x_test)\nprint(classification_report(y_test,grid_search_predictions,target_names=['0','1']))","5f8b57fb":"param_grid={'C': [100],\n              'gamma': [0.01,0.001,0.0001],\n              'kernel': ['rbf','poly']}\ngrid_search_1 = GridSearchCV(SVC(),param_grid,verbose=1,cv=10,n_jobs=-1)\ngrid_search_1.fit(x_train,y_train.values.ravel())","4c8eceba":"grid_search_1.best_estimator_","6ba0c13a":"grid_search_pred_1 = grid_search_1.predict(x_valid)\nscore = accuracy_score(y_valid,grid_search_pred_1)\nscore","ffdf2827":"grid_search_predictions_1 = grid_search_1.predict(x_test)\nprint(classification_report(y_test,grid_search_predictions_1,target_names=['0','1']))","9965641e":"from sklearn.model_selection import RandomizedSearchCV\nparam_grid_xg={\"learning_rate\" : [0.05] ,\n \"max_depth\"        : [ 1],\n \"min_child_weight\" : [ 1],\n \"gamma\"            : [ 0.0],\n \"colsample_bytree\" : [ 0.1,0.3],\n \"n_estimators\"     : [300,400]}\ngrid_search_xg = RandomizedSearchCV(XGBClassifier(),param_grid_xg,verbose=1,cv=10,n_jobs=-1)\ngrid_search_xg.fit(x_train,y_train.values.ravel())","f7dca1bc":"grid_search_xg.best_estimator_","96ee7684":"grid_search_pred_xg = grid_search_xg.predict(x_valid)\nscore = accuracy_score(y_valid,grid_search_pred_xg)\nscore","1fd35307":"grid_search_predictions_xg = grid_search_xg.predict(x_test)\nprint(classification_report(y_test,grid_search_predictions_xg,target_names=['0','1']))","02bc6524":"param_grid_cat = {'iterations': range(10,100,40),\n                 'depth': range(1, 8),\n                 'learning_rate': [0.03,0.001,0.01,0.1,0.2,0.3],\n                 \n                 'bagging_temperature': [0.0,0.2,0.4,0.6,0.8,1.0],\n                 'border_count': range(1, 255),\n                 'l2_leaf_reg': range(2, 30),\n                 'scale_pos_weight': [0.01,0.1,0.3,0.5,0.7,0.9,1.0]}\n","b707ef70":"grid_search_cat = RandomizedSearchCV(CatBoostClassifier(verbose=2,od_type='Iter'),param_grid_cat,verbose=1,cv=10,n_jobs=-1)\ngrid_search_cat.fit(x_train,y_train.values.ravel())","75c70478":"grid_search_predictions_cat = grid_search_cat.predict(x_test)\nprint(classification_report(y_test,grid_search_predictions_cat,target_names=['0','1']))","e5de2b75":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom tensorflow.keras import regularizers\n\nmodel_neural = Sequential()\n\nmodel_neural.add(Dense(100, activation='relu', input_shape=(12,),kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)))\n\nmodel_neural.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)))\n\nmodel_neural.add(Dense(1, activation='sigmoid'))\n\nmodel_neural.compile(loss='binary_crossentropy',\n              optimizer=Adam(lr=0.00001),\n              metrics=['accuracy'])\n                   \nmodel_neural.fit(x_train, y_train,epochs=150, batch_size=1, verbose=1,validation_data=(x_valid,y_valid))","e31c975f":"# Making predictions using test data\npredict_neural = model_neural.predict_classes(x_test)\nprint(classification_report(y_test,predict_neural,target_names=['0','1']))","56a59d13":"final_predictions =  pd.DataFrame(model_neural.predict_classes(X_test_normed))\nfinal_predictions.columns = [\"Survived\"]\nfinal_predictions = pd.concat([test[\"PassengerId\"],final_predictions],axis=1)\nfinal_predictions.head()","68b66b52":"final_predictions.head()","31eddfaf":"## Hyper-Parameter Tuning XGBOOST","4584af2a":"### Data Cleaning\n","0876228f":"**We will only handle missing values in Embarked column not the Cabin columns since, it contains lots of missing values**","9519757c":"**Observations from above Graphs:**\n**1. Sex: It appears that on an average number of females survived > no. of males.**\n**2. Embarked: It appears the number of person belonging to category 'C' survived the most while in 'S' survived the least.** ","971e826f":"## Random Forest (Untuned)","9ae6d0bc":"**It appears that the median age of people in class 1 is around 37. While it is 29 for class 2 and it is 22 for class 3. We can consider this info. while imputing the missing values in Age Feature**","b0220197":"**It appears like columns Name,Ticket and Cabin has higher cardinality**","e1639419":"**From box plot and scatterplot, it appears that some of the columns contains outliers but we will leave them as it is since they are acceptable**","8a1c6a00":"## KNN","b21f1f9d":"**From above correlation graph, it seems like there is no correlation amongst and with the target variables**","db622670":"#### **Final Model:** Although, most of the models implemented above output accuracy > 80% , but Neural Network is chosen as the final model since, it performed less overfitting on test data.","2110e017":"# Exploratory Data Analysis","d3017ada":"## XGBOOST (Untuned)","305ca4e0":"# Modelling\nNow, we will perform training of data using various Classification Models.","949b86c6":"# Predictions on Test Data","bdc9095a":"## CatBoost (Untuned)","24591d4e":"### From the above statistics, it appears that the dataset is partially imbalanced.","567651c0":"## Using Neural Networks","012e1764":"### Feature Selection","c4627b27":"#### Performing Feature Engineering","56766a3c":"# Handling Missing Values\nWe will first check which all columns have the missing values with the help of Visualization.","4b54a6f6":"## Naive-Bayes","fcf3823b":"## Logistic Regression","3a76ca4e":"## SVM","f7b5bc29":"### Feature Scaling","b0f0159c":"# Necessary Libraries","94805165":"## Hyper-tuning CatBoost Classifier","84eb7a04":"# Reading Train and Test Data","89114c58":"## Decision Tree","166dd450":"**From the above visualization, it appears that the Age,Cabin and Embarked columns contain null values in training data while Age,Cabin and Fare columns contain null values in testing data. The Cabin column contains maximum null values in both the dataset.**","0c6ff3d0":"## Hyper-parameter Tuning SVM Model","78dbba25":"## Hyper-parameter Tuning Random Forest Model"}}