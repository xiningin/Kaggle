{"cell_type":{"4487ea1c":"code","2522ebf2":"code","7cfe8f04":"code","8a322de1":"code","a98d70ac":"code","6cd43c50":"code","911f96a5":"code","46ae8d93":"code","45e3e7bf":"code","1602a106":"code","70b305bc":"code","e837f815":"code","7489e758":"code","ece1061e":"code","5b500603":"code","e7bfa534":"code","070062cd":"code","22145d5c":"code","3094defe":"code","1a1a4098":"code","632467d0":"code","3fb1c941":"code","f2cae91b":"code","b908dace":"code","c3b9a341":"code","5d58c95c":"code","4b9710eb":"code","83836c83":"code","c15e3e53":"code","e9b22668":"code","7883e3a5":"code","175523b1":"code","d1abb78f":"code","c971e42b":"code","a6142fd1":"code","e3db18af":"markdown","8f8c63dc":"markdown","2b0c0474":"markdown","1b5346ed":"markdown","fa80af39":"markdown","d2c88c98":"markdown","d3ee2e1e":"markdown","990538f4":"markdown","8da368d7":"markdown","b4a4b491":"markdown","39d36c6a":"markdown","94445333":"markdown","7e6db178":"markdown","3314d312":"markdown"},"source":{"4487ea1c":"pip install -U lightautoml","2522ebf2":"pip install -U transformers","7cfe8f04":"# Standard python libraries\nimport logging\nimport os\nimport time\nlogging.basicConfig(format='[%(asctime)s] (%(levelname)s): %(message)s', level=logging.INFO)\n\n# Installed libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport matplotlib.pyplot as plt\n\n# Imports from our package\nfrom lightautoml.automl.presets.text_presets import TabularNLPAutoML\nfrom lightautoml.dataset.roles import DatetimeRole\nfrom lightautoml.tasks import Task\n","8a322de1":"N_THREADS = 4 # threads cnt for lgbm and linear models\nRANDOM_STATE = 42 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTIMEOUT = 6 * 3600 # Time in seconds for automl run\nTARGET_NAME = 'UltimateIncurredClaimCost'","a98d70ac":"np.random.seed(RANDOM_STATE)\ntorch.set_num_threads(N_THREADS)","6cd43c50":"%%time\n\ntrain_data = pd.read_csv('..\/input\/actuarial-loss-estimation\/train.csv')\ntrain_data.head()","911f96a5":"test_data = pd.read_csv('..\/input\/actuarial-loss-estimation\/test.csv')\ntest_data.head()","46ae8d93":"set(test_data['ClaimNumber']).intersection(set(train_data['ClaimNumber']))","45e3e7bf":"submission = pd.read_csv('..\/input\/actuarial-loss-estimation\/sample_submission.csv')\nsubmission.head()","1602a106":"train_data.shape, test_data.shape, submission.shape","70b305bc":"np.log(train_data[TARGET_NAME] + 1).hist(figsize = (20, 10), bins = 100, grid = True);","e837f815":"for col in ['DateReported', 'DateTimeOfAccident']:\n    for data in [train_data, test_data]:\n        data[col] = data[col].map(lambda x: x.replace('T', ' ')[:-1])","7489e758":"train_data.head()","ece1061e":"train_data[TARGET_NAME].describe()","5b500603":"train_data['ClaimDescription'].str.split().map(len).describe()","e7bfa534":"test_data['ClaimDescription'].str.split().map(len).describe()","070062cd":"for col in ['DateTimeOfAccident', 'DateReported']:\n    for data in [train_data, test_data]:\n        t = pd.to_datetime(data[col])\n        data['Year' + col] = t.dt.year\n        data['Month' + col] = t.dt.month\n        data['YearMonth' + col] = data['Year' + col].astype(str) + '_' + data['Month' + col].astype(str)","22145d5c":"all_data = pd.concat([train_data, test_data])\nall_data","3094defe":"for col in ['DateTimeOfAccident', 'DateReported']:\n    for part in ['Year', 'Month', 'YearMonth']:\n        mapper_mean = dict(all_data.groupby(part + col)['InitialIncurredCalimsCost'].mean())\n        mapper_std = dict(all_data.groupby(part + col)['InitialIncurredCalimsCost'].std())\n        for data in [train_data, test_data]:\n            data['mapped_mean_'+part+col] = data[part+col].map(mapper_mean)\n            data['mapped_std_'+part+col] = data[part+col].map(mapper_std)\n        ","1a1a4098":"for df in [train_data, test_data]:\n        \n    # Reporting delay of report in days\n    delta = pd.to_datetime(df['DateReported']) - pd.to_datetime(df['DateTimeOfAccident'])\n    df['DaysReportDelay'] = (delta  \/ np.timedelta64(1, 'D')).astype(int)\n    \n    # Clip extrems values\n    df[\"DependentsOther\"].clip(0, 1, inplace=True)\n    df[\"DependentChildren\"].clip(0, 3, inplace = True)\n    df[\"Age\"].clip(14, 75, inplace = True)\n    df[\"Gender\"].replace({\"U\":\"M\"}, inplace = True)\n    df[\"MaritalStatus\"].fillna(\"S\", inplace = True)\n","632467d0":"%%time\n\ndef rmse(y_true, y_pred, **kwargs):\n    return mean_squared_error(y_true, y_pred, squared=False, **kwargs)\n\ntask = Task('reg', metric = rmse)","3fb1c941":"%%time\n\nroles = {'target': TARGET_NAME, \n         'text': ['ClaimDescription'],\n        'drop': ['ClaimNumber']}","f2cae91b":"%%time \n\nautoml = TabularNLPAutoML(task = task, \n                       timeout = TIMEOUT,\n                       cpu_limit = N_THREADS,\n                       reader_params = {'cv': 5},\n                       general_params = {'use_algos': [['linear_l2', 'lgb', 'lgb_tuned', 'cb', 'cb_tuned']]},\n                       tuning_params = {'max_tuning_iter': 101, 'max_tuning_time': 600},\n                       text_params = {'lang': 'en'},\n                       nn_params = {'lang': 'en', 'bert_name': 'bert-base-uncased'}\n                       )\n\noof_pred = automl.fit_predict(train_data, roles = roles)\nprint('oof_pred:\\n{}\\nShape = {}'.format(oof_pred, oof_pred.shape))","b908dace":"automl.collect_used_feats()","c3b9a341":"test_pred = automl.predict(test_data)\nprint('Prediction for test data:\\n{}\\nShape = {}'.format(test_pred, test_pred.shape))","5d58c95c":"submission[TARGET_NAME] = test_pred.data[:, 0]\nsubmission","4b9710eb":"submission[TARGET_NAME].describe()","83836c83":"submission.to_csv('LightAutoML_preds_with_mse_loss_and_rmse_metric_all_algs.csv', index = False)","c15e3e53":"te_data = test_data.copy()\nte_data.head()","e9b22668":"te_data[TARGET_NAME] = test_pred.data[:, 0]\n\nnew_tr_data = pd.concat([train_data, te_data]).sample(frac = 1, random_state = 13).reset_index(drop = True)\nnew_tr_data","7883e3a5":"%%time \n\nautoml = TabularNLPAutoML(task = task, \n                       timeout = TIMEOUT,\n                       cpu_limit = N_THREADS,\n                       reader_params = {'cv': 5},\n                       general_params = {'use_algos': [['linear_l2', 'lgb', 'lgb_tuned', 'cb', 'cb_tuned']]},\n                       tuning_params = {'max_tuning_iter': 101, 'max_tuning_time': 700},\n                       text_params = {'lang': 'en'},\n                       nn_params = {'lang': 'en', 'bert_name': 'bert-base-uncased'}\n                       )\n\noof_pred = automl.fit_predict(new_tr_data, roles = roles)\nprint('oof_pred:\\n{}\\nShape = {}'.format(oof_pred, oof_pred.shape))","175523b1":"new_test_pred = automl.predict(test_data)\nprint('Prediction for test data:\\n{}\\nShape = {}'.format(new_test_pred, new_test_pred.shape))","d1abb78f":"submission[TARGET_NAME] = new_test_pred.data[:, 0]\nsubmission","c971e42b":"submission[TARGET_NAME].describe()","a6142fd1":"submission.to_csv('LightAutoML_preds_with_mse_loss_and_rmse_metric_pseudolabelled.csv', index = False)","e3db18af":"# Step 0.3. Fix torch number of threads and numpy seed ","8f8c63dc":"# Step 0.6. Expert features","2b0c0474":"To create AutoML model here we use `TabularNLPAutoML` preset.\n\n\nAll params we set above can be send inside preset to change its configuration:","1b5346ed":"## Step 2. Setup columns roles","fa80af39":"# Step 0.5. Some EDA","d2c88c98":"## Step 5. Generate submission file","d3ee2e1e":"## Step 6. Create pseudolabelled model","990538f4":"# Step 0. Install LAMA","8da368d7":"# Step 0.1. Import necessary libraries ","b4a4b491":"# Step 0.2. Parameters ","39d36c6a":"# Step 0.4. Example data load ","94445333":"## Step 4. Predict to test data","7e6db178":"## Step 3. Create AutoML from preset","3314d312":"#  ==== AutoML preset usage ====\n\n\n## Step 1. Create Task"}}