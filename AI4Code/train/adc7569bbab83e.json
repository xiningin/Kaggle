{"cell_type":{"eb6bd615":"code","0e78b99f":"code","22026238":"code","ba37906a":"code","0168952b":"code","6a5b911a":"code","1176a314":"code","72139c6a":"code","3c581594":"code","0c4786f0":"code","34f3bda6":"code","57341c32":"code","b4d880e0":"code","1296fd21":"code","90d955b0":"code","7d8ad282":"code","b78e0c31":"code","30759c49":"code","3a3df6eb":"code","031d1a8a":"code","20be9c2f":"code","b2b68ec6":"code","c1808ac7":"code","2e3ae6c4":"markdown","82e8dc9b":"markdown","6ab54c24":"markdown"},"source":{"eb6bd615":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.options.mode.chained_assignment = None \n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0e78b99f":"train_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","22026238":"train_data = train_data.drop(['Id'], axis = 1)\nID = test_data.Id\ntest_data = test_data.drop(['Id'], axis = 1)\n","ba37906a":"print(train_data.info())\nprint(test_data.info())","0168952b":"cat_cols = []\nnum_cols = []\nfor col in train_data.columns:\n    if train_data[col].dtype == 'object':\n        cat_cols.append(col)\n    if train_data[col].dtype in ['float', 'int']:\n        num_cols.append(col)\nlen(cat_cols) + len(num_cols)\ntrain_data.head(10)","6a5b911a":"train_data.SalePrice.hist(bins = 100)\nq97 = train_data.SalePrice.quantile(0.97)\ntrain_data = train_data[(train_data.SalePrice < q97)].reset_index()\ntrain_data.SalePrice.hist(bins = 100)\n","1176a314":"def Na_counter(data):\n    Na_percent = []\n    Uniques = []\n    for feature in list(data.columns):\n        Na_count = len(data[feature][data[feature].isna() == True])\n        Na_percent.append(Na_count \/ len(data))\n        Uniques.append(data[feature].nunique())\n    \n    na_data = pd.DataFrame({'Feature' : list(data.columns), 'Na_rate' : Na_percent, 'Uniques' : Uniques})\n    na_data = na_data[na_data.Na_rate > 0.0].sort_values(by = \"Na_rate\", ascending = False)\n    return na_data\nprint(Na_counter(train_data).head(10))\nprint(Na_counter(test_data).head(10))","72139c6a":"def f(x, q0, q25, q50, q75, q1):\n    if x < q25:\n        return 0\n    if x < q50:\n        return 1\n    if x < q75:\n        return 2\n    if x <= q1:\n        return 3\n    \n\n    \ndef fill_nc(data, nc):\n    for col in nc:\n         if col != 'SalePrice':\n                mean_list  = data[[col, 'quant']].groupby('quant').mean().reset_index()[col].to_list()\n                data[col][(data[col].isna() == True) & (data.quant == 0)] = mean_list[0]\n                data[col][(data[col].isna() == True) & (data.quant == 1)] = mean_list[1]\n                data[col][(data[col].isna() == True) & (data.quant == 2)] = mean_list[2] \n                data[col][(data[col].isna() == True) & (data.quant == 3)] = mean_list[3] \ndef fill_nc_test(data, nc):\n    for col in nc:\n        if col!='SalePrice':\n            data[col][data[col].isna() == True] = data[col][data[col].isna() == True].fillna(data[col].mean())\n\n        \nq0 = 0\nq25 = train_data.SalePrice.quantile(0.25)\nq50 = train_data.SalePrice.quantile(0.5)\nq75 = train_data.SalePrice.quantile(0.75)\nq1 = train_data.SalePrice.max()\ntrain_data['quant'] = train_data['SalePrice'].apply(lambda x: f(x, q0, q25, q50, q75, q1))\nfill_nc(train_data, num_cols)\nfill_nc_test(test_data, num_cols)\ntrain_data = train_data.drop(['quant'], axis = 1)","3c581594":"Na_counter(train_data)\nNa_counter(test_data)","0c4786f0":"def fill_cc(data, cat_cols):\n    for col in cat_cols:\n        data[col][(data[col].isna() == True)] = 'None'\nfill_cc(train_data, cat_cols)\nfill_cc(test_data, cat_cols)\n","34f3bda6":"Na_counter(test_data)","57341c32":"fig, ax = plt.subplots(figsize=(10,10))  \nax = sns.heatmap(train_data.corr())\nfor col in num_cols:\n    print(train_data.corr()[col].nlargest(3).iloc[1:3])","b4d880e0":"def normalize_train(data, nc):    \n    for col in nc:\n        if col == 'SalePrice':\n            mean = data[col].mean()\n            std = data[col].std()\n        data[col] = (data[col] - data[col].mean()) \/ data[col].std()\n    return mean, std\ndef normalize_test(data, nc):    \n    for col in nc:\n        if col == 'SalePrice':\n            return\n        data[col] = (data[col] - data[col].mean()) \/ data[col].std()\n    \nmean, std = normalize_train(train_data, num_cols)\nnormalize_test(test_data, num_cols)\ntarget = train_data.SalePrice\ntrain_data = train_data.drop(['SalePrice'], axis = 1)","1296fd21":"data=pd.concat([train_data,test_data],keys=['train','test']).reset_index().drop(['level_1'], axis = 1)\ndata","90d955b0":"dummies = pd.concat([data.level_0, pd.get_dummies(data[cat_cols], drop_first = True)], axis = 1)\ndummies_train = dummies[dummies.level_0 == 'train'].drop(['level_0'], axis = 1)\ndummies_test = dummies[dummies.level_0 == 'test'].drop(['level_0'], axis = 1).reset_index()\ndummies_test","7d8ad282":"train_data_ready = pd.concat([train_data, dummies_train], axis = 1).drop(cat_cols, axis = 1)\ntest_data_ready = pd.concat([test_data, dummies_test], axis = 1).drop(cat_cols, axis = 1)\ntest_data_ready","b78e0c31":"print(len(test_data_ready.columns), len(train_data_ready.columns))","30759c49":"from sklearn.model_selection import train_test_split\nX_train,X_holdout,y_train,y_holdout = train_test_split(train_data_ready,target,test_size=.30,random_state=17)\nID","3a3df6eb":"from sklearn.ensemble import RandomForestRegressor\nrfr=RandomForestRegressor(n_estimators = 700,random_state=40,\n                          min_impurity_decrease=0.002,min_weight_fraction_leaf=0.001,min_samples_split=10)\nrfr.fit(X_train,y_train)\nID","031d1a8a":"from sklearn.metrics import mean_squared_error\nprint(mean_squared_error(y_train, rfr.predict(X_train), squared=False))\nprint(mean_squared_error(y_holdout, rfr.predict(X_holdout), squared=False))\n","20be9c2f":"preds = rfr.predict(test_data_ready) * std + mean\nprint(ID)\npred = pd.DataFrame({'Id':ID, 'SalePrice' :preds})\npred.to_csv('preds_rf.csv',index=False)","b2b68ec6":"import xgboost as xgb\nxg_reg = xgb.XGBRegressor( learning_rate = 0.1,\n                max_depth =4,objective=\"reg:linear\",n_estimators=555)\nxg_reg.fit(X_train,y_train)\nprint(mean_squared_error(y_train, xg_reg.predict(X_train), squared=False))\nprint(mean_squared_error(y_holdout, xg_reg.predict(X_holdout), squared=False))","c1808ac7":"result1=pd.DataFrame({'id':ID,'SalePrice':np.around(xg_reg.predict(test_data_ready) * std + mean)})\nresult1\nresult1.to_csv('preds_xg.csv',index=False)","2e3ae6c4":"removing outliers","82e8dc9b":"It is necessary to fill in NaN values","6ab54c24":"Normalizing numerical input"}}