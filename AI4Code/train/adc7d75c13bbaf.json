{"cell_type":{"5f8fdc29":"code","e0c6de0e":"code","beedf850":"code","2e11e986":"code","c90fe268":"code","f1318e0d":"code","486ed314":"code","945024bb":"code","10019db5":"code","e6a4920d":"code","87ad2693":"code","aa0b77af":"code","625d4f54":"code","aaae2f5e":"code","c4b6f603":"code","c11b547c":"code","9d5db4a6":"code","4d984316":"code","18b4c42a":"code","5957369f":"code","3c9a7eba":"code","1fae758d":"code","0ef208bf":"code","a41a78eb":"code","cd686550":"code","b5dbed77":"code","739e2918":"code","63508bfc":"code","f8770ef6":"code","27b5ce1a":"code","dd2d422c":"code","1f00797f":"code","9af67bf6":"code","188b75b2":"code","3b0dc02a":"code","6ccfd5a1":"markdown","095ae8dc":"markdown","a67741f3":"markdown","1408b446":"markdown","cc83769e":"markdown","903d28cc":"markdown","76c518e0":"markdown","7b4d7c66":"markdown","38a39dfd":"markdown","b3f7f1ab":"markdown","7c18cb99":"markdown","c0bfc7e1":"markdown","12db4855":"markdown","74432f0a":"markdown","656af6e5":"markdown","f9a6a69b":"markdown","67c9fe39":"markdown","0d02b13e":"markdown","de593050":"markdown","4e2f42b5":"markdown"},"source":{"5f8fdc29":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import Lasso\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler \nimport xgboost as xgb\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e0c6de0e":"file_dir = '\/kaggle\/input\/house-prices-advanced-regression-techniques'\n\ntrain_set = pd.read_csv(f'{file_dir}\/train.csv')\nX_test = pd.read_csv(f'{file_dir}\/test.csv')","beedf850":"train_set.info()","2e11e986":"train_set.drop('Id', axis=1, inplace=True)\ntest_id = X_test['Id']\nX_test.drop('Id', axis=1, inplace=True)","c90fe268":"plt.figure(figsize=(28, 14))\n\nmask = np.triu(np.ones_like(train_set.corr(), dtype=np.bool))\n\nheatmap = sns.heatmap(train_set.corr(), mask=mask, cmap=\"viridis\", annot=True)\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=16);","f1318e0d":"plt.figure(figsize=(8, 12))\nheatmap = sns.heatmap(train_set.corr()[['SalePrice']].sort_values(by='SalePrice', ascending=False), vmin=-0.5, vmax=0.5, annot=True, cmap='BrBG')\nheatmap.set_title('Features correlated with sale price', fontdict={'fontsize':18}, pad=16);","486ed314":"fig, ax = plt.subplots(ncols=2,figsize=(20,10))\nsns.histplot(train_set['SalePrice'], ax=ax[0]);\nsns.boxplot(y='SalePrice', data=train_set)\ndescription = train_set['SalePrice'].describe()\ndescription['skew'] = train_set['SalePrice'].skew()\ndescription['kurt'] = train_set['SalePrice'].kurt()\ndescription","945024bb":"plt.figure(figsize=(12, 8))\nsns.boxplot(x='OverallQual', y='SalePrice', data=train_set)","10019db5":"cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageArea', 'TotalBsmtSF', 'FullBath']\nsns.pairplot(train_set[cols])","e6a4920d":"missing_values = train_set.isnull().sum()\nmissing_values[missing_values != 0].sort_values(ascending=False)","87ad2693":"deleted_features = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu']\ntrain_set.drop(deleted_features, axis=1, inplace=True)\nX_test.drop(deleted_features, axis=1, inplace=True)","aa0b77af":"X_train = train_set.drop('SalePrice', axis=1)\ny_train = train_set['SalePrice']","625d4f54":"cat_attribs = X_train.loc[:,X_train.dtypes==np.object].columns\nnum_attribs = np.setxor1d(X_train.columns.values, cat_attribs)","aaae2f5e":"num_pipeline = Pipeline([\n ('imputer', SimpleImputer(strategy=\"median\")),\n ('scaler', MinMaxScaler()),\n ])\ncat_pipeline = Pipeline([\n ('imputer', SimpleImputer(strategy=\"most_frequent\")),\n ('oneHotEncoder', OneHotEncoder()),\n ])\nfull_pipeline = ColumnTransformer([\n (\"num\", num_pipeline, np.array(num_attribs)),\n (\"cat\", cat_pipeline, np.array(cat_attribs)),\n ])","c4b6f603":"X_train = full_pipeline.fit_transform(X_train)\nX_test = full_pipeline.transform(X_test)","c11b547c":"def show_scores(scores):\n    print(\"Score mean\", scores.mean())\n    print(\"Standard deviation:\", scores.std())","9d5db4a6":"neigh = KNeighborsRegressor()\n\nscores = cross_val_score(neigh, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\nrmse_scores = np.sqrt(-scores)\nprint(rmse_scores)\nshow_scores(rmse_scores)","4d984316":"lasso = Lasso(tol=0.005)\n\nscores = cross_val_score(lasso, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\nrmse_scores = np.sqrt(-scores)\nprint(rmse_scores)\nshow_scores(rmse_scores)","18b4c42a":"param_grid = {'alpha':[1, 10, 50, 100, 125, 150, 175, 200]}\n\ngrid_search = GridSearchCV(lasso,\n                           param_grid,\n                           cv=5,\n                           return_train_score=True,\n                           n_jobs=-1)\n\ngrid_search.fit(X_train, y_train)\n\nlasso_params = grid_search.best_params_\n\nprint(lasso_params)","5957369f":"lasso = Lasso(**lasso_params)\n\nscores = cross_val_score(lasso, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\nrmse_scores = np.sqrt(-scores)\nprint(rmse_scores)\nshow_scores(rmse_scores)","3c9a7eba":"mlp_reg = MLPRegressor(learning_rate_init=1, learning_rate='adaptive', max_iter=400)\nscores = cross_val_score(mlp_reg, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\nrmse_scores = np.sqrt(-scores)\nprint(rmse_scores)\nshow_scores(rmse_scores)","1fae758d":"param_grid = {'alpha':[1, 5, 10, 20, 30]}\n\ngrid_search = GridSearchCV(mlp_reg,\n                           param_grid,\n                           cv=5,\n                           return_train_score=True,\n                           n_jobs=-1)\n\ngrid_search.fit(X_train, y_train)\n\nmlp_params = grid_search.best_params_\n\nprint(mlp_params)","0ef208bf":"mlp_reg = MLPRegressor(learning_rate_init=3, learning_rate='adaptive', max_iter=400, **mlp_params)\n\nscores = cross_val_score(mlp_reg, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\nrmse_scores = np.sqrt(-scores)\nprint(rmse_scores)\nshow_scores(rmse_scores)","a41a78eb":"def get_model_scores(model, dtrain, nfold=5, early_stopping_rounds=20):\n    params = model.get_xgb_params()\n    \n    cvresult = xgb.cv(params, dtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=nfold,\n        metrics='auc', early_stopping_rounds=early_stopping_rounds, show_progress=False)\n    alg.set_params(n_estimators=cvresult.shape[0])\n    ","cd686550":"dtrain = xgb.DMatrix(X_train ,y_train)","b5dbed77":"xgb_params = {\n    'objective': 'reg:squarederror',\n    'colsample_bytree': 0.3,\n    'learning_rate': 0.1,\n    'max_depth': 5,\n    'min_child_weight': 1,\n    'eta': 0.3,\n    'alpha': 10,\n    'eval_metric': \"rmse\",\n}\n","739e2918":"xg_reg = xgb.XGBRegressor()\n\ncv_results = xgb.cv(dtrain=dtrain, params=xgb_params, nfold=5, num_boost_round=500, early_stopping_rounds=10, metrics='rmse', seed=42)\nprint((cv_results['test-rmse-mean']).tail(1))","63508bfc":"from tqdm.notebook import tqdm\ndef get_best_params(initial_params, searched_params, val_combs, metric='rmse', nfold=5):\n    metric_name = f'test-{metric}-mean'\n    best_score = float('Inf')\n    best_params = None\n    \n    for val_comb in tqdm(val_combs):\n        comb_dict = {param: val for param, val in zip(searched_params, val_comb)}\n        initial_params.update(comb_dict)\n        cv_results = xgb.cv(dtrain=dtrain, params=initial_params, nfold=5, num_boost_round=500, early_stopping_rounds=10, metrics=metric, seed=42)\n        score = cv_results[metric_name].tail(1).values[0]\n        if score < best_score:\n            best_score = score\n            best_params = val_comb\n#             print('Better score found with ', {param: val for param, val in zip(searched_params, best_params)}, \" achieving the score of \", best_score)\n        \n    best_params_dict = {param: val for param, val in zip(searched_params, best_params)}\n    print(\"Best parameters are: \", best_params_dict, \"with a score of \", best_score)\n    return best_params_dict","f8770ef6":"combs = [(max_depth, min_weight)\n    for max_depth in range(2, 7)\n    for min_weight in range(1, 6)]\nxgb_params.update(get_best_params(xgb_params, ('max_depth', 'min_child_weight'), combs))","27b5ce1a":"combs = [(subsample, colsample)\n    for subsample in np.linspace(0.6,1,5)\n    for colsample in np.linspace(0.05,0.5,10)]\nxgb_params.update(get_best_params(xgb_params, ('subsample', 'colsample_bytree'), combs))","dd2d422c":"combs = [(eta, ) for eta in [0.4, 0.3, 0.2, 0.1, 0.05]]\nxgb_params.update(get_best_params(xgb_params, ('eta', ), combs))","1f00797f":"xgb_reg = xgb.XGBRegressor(**xgb_params)\nxgb_reg.fit(X_train, y_train)","9af67bf6":"predictions = xgb_reg.predict(X_test)","188b75b2":"X_test","3b0dc02a":"submission = pd.DataFrame({'Id': test_id, 'SalePrice': predictions})\nsubmission.to_csv('submission.csv', index=False)","6ccfd5a1":"# 2. Exploratory data analaysis","095ae8dc":"## 2.2 Exploring particular variables","a67741f3":"# 4. Data preparation","1408b446":"## 5.4 XGBoost","cc83769e":"XGBoost turned out to be the best algorithm in this case with the best RMSE on cross validation with 10 folds after parameter tuning being 27196. Lasso turned out to be close as well and could be considered as well.","903d28cc":"## 5.2 Lasso","76c518e0":"At first we should make a diagram of correlation heatmap. It really gives the spirit and main gist of the data. \nThere is a lot of features in the dataset and we can already see highly correlated features to the target value (SalePrice). We can also spot some signs of multicollinearity, like GarageArea and GarageCars. We should think about discarding some of the features that can be deduced from other ones.","7b4d7c66":"## 5.3 MLP Regressor","38a39dfd":"## 2.1 Correlations","b3f7f1ab":"## 5.1 KNeighborsRegressor","7c18cb99":"# 3. Handling missing values","c0bfc7e1":"Features with the biggest number of missing values can be safely dropped (PoolQC, MiscFeature, Alley, Fence, FireplaceQu). I decided to keep the LotFrontage feature despite also having a relatively large number of lacking values because of how much iniformation it carries: 0.35 correlation score in relation to SalePrice. It also seems relatively independent, the greatest other correlation score is 0.46, not too much from the target value. All other features will have their null value replaced by the most occurring one.","12db4855":"We set some initial parameters that we will be tuning right now","74432f0a":"# 5. Algorithm implementation and hyperparameter tuning","656af6e5":"# 1. Dataset and libraries loading","f9a6a69b":"# 6. Creating submission and conclussions","67c9fe39":"Target value is highlt skewed to right with mean over 18,000 and max value 755,000, due to the uncapped values it may be difficult to predict prices over 450,000.","0d02b13e":"It will be easier to see the connected to target value in another diagram, with all values ordered. There is an exceptional number of highly correlated values in the dataset, so I hope to be able to squeeze out a lot of information from it.","de593050":"We will try out a few chosen algorithms and tune them at the same time.","4e2f42b5":"Underwhelming results, we will not consider this algorithm further"}}