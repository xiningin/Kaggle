{"cell_type":{"18c45903":"code","5bb612ff":"code","cd164845":"code","1062d833":"code","4ea837b7":"code","a5aabaec":"code","621b7fe5":"code","8da2ec1d":"code","ce54cd41":"code","119797a8":"code","c549876d":"code","b729dfc7":"code","3b78bcc5":"code","982e41e0":"code","f7e63b90":"code","db9aa254":"code","f2739233":"code","545d8cc6":"code","7effd346":"code","17c55a21":"code","b2db3418":"code","1ac0746b":"code","29e3008b":"code","4d3c6be0":"code","eca56af7":"code","b6ba8486":"code","fc67828f":"code","3a9dccbf":"code","8faf4132":"code","54e82053":"code","87cf379e":"markdown","0eaa7e6e":"markdown","18e94daa":"markdown","4a9f7406":"markdown","7cdb610c":"markdown","09935d29":"markdown","993c40a5":"markdown","448068c1":"markdown","da2f333d":"markdown","1164ed76":"markdown","6661d554":"markdown","5799676c":"markdown","930554a1":"markdown","cbfab16b":"markdown","c394caee":"markdown","080398cc":"markdown","bbdc4629":"markdown"},"source":{"18c45903":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","5bb612ff":"import nltk\nimport pandas as pd\nimport numpy as n","cd164845":"df=pd.read_csv('..\/input\/imdb-sentiment-analysis\/labeledTrainData.tsv',delimiter='\\t')\n","1062d833":"df.head()","4ea837b7":"df.info()","a5aabaec":"df['sentiment'].value_counts()","621b7fe5":"## We will define a function to clean up the message like removing punctuations, single letter words etc..\nimport re\ndef clean_str(string):\n  \"\"\"\n  String cleaning before vectorization\n  \"\"\"\n  try:    \n    string = re.sub(r'^https?:\\\/\\\/<>.*[\\r\\n]*', '', string, flags=re.MULTILINE)\n    string = re.sub(r\"[^A-Za-z]\", \" \", string)         \n    words = string.strip().lower().split()    \n    words = [w for w in words if len(w)>=1]\n    return \" \".join(words)\t\n  except:\n    return \"\"","8da2ec1d":"df['clean_review']=df['review'].apply(clean_str)","ce54cd41":"df.head()","119797a8":"from sklearn.model_selection import train_test_split","c549876d":"X_train, X_test, y_train, y_test = train_test_split(\n    df['review'],\n    df['sentiment'],\n    test_size=0.2, \n    random_state=42\n)","b729dfc7":"from keras.preprocessing.text import Tokenizer","3b78bcc5":"top_words=10000","982e41e0":"t=Tokenizer(top_words)\nt.fit_on_texts(X_train)","f7e63b90":"#t.word_index.items()","db9aa254":"X_train=t.texts_to_sequences(X_train)\nX_test=t.texts_to_sequences(X_test)","f2739233":"from keras.preprocessing import sequence\nmax_review_length=300\nX_train=sequence.pad_sequences(X_train,maxlen=max_review_length,padding='post')\nX_test=sequence.pad_sequences(X_test,maxlen=max_review_length,padding='post')","545d8cc6":"print(X_train.shape)\nprint(X_test.shape)","7effd346":"import gensim\nword2vec=gensim.models.Word2Vec.load('..\/input\/word2vec-movie-review\/word2vec-movie-review_1')\nword2vec.wv.syn0.shape","17c55a21":"# we will create an embeding Matrix for our Vocab. Above gensim model already has the vector representation of \n# all the majority of the words. We will leverage the model and try to get the vectors of our use case \n# and we will store them in a matrix\n\nembeding_vector_length=word2vec.wv.syn0.shape[1]\nembeding_matrix=np.zeros((top_words+1,embeding_vector_length))\n","b2db3418":"for word,i in sorted(t.word_index.items(),key=lambda x:x[1]):\n    if i>top_words:\n        break\n    if word in word2vec.wv.vocab:\n        embeding_vector=word2vec.wv[word]\n        embeding_matrix[i]=embeding_vector","1ac0746b":"# Just to have a glance at the vector for our words \nembeding_matrix","29e3008b":"# Import the required package from keras library\nfrom keras.models import  Sequential\nfrom keras.layers import Embedding,Dropout,Dense,LSTM","4d3c6be0":"model=Sequential()","eca56af7":"model.add(Embedding(top_words+1,50,input_length=max_review_length,weights=[embeding_matrix],trainable=False))","b6ba8486":"model.add(LSTM(100,dropout=0.2,recurrent_dropout=0.2))","fc67828f":"model.add(Dense(1,activation='sigmoid'))","3a9dccbf":"model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","8faf4132":"model.summary()","54e82053":"model.fit(X_train,y_train,epochs=10,batch_size=100,validation_data=(X_test,y_test))","87cf379e":"The output of the LSTM could be a 2D array or 3D array depending upon the return_sequences argument.\nIf return_sequence is False, the output is a 2D array. (batch_size, units)\nIf return_sequence is True, the output is a 3D array. (batch_size, time_steps, units)\n\nSince we have not set the flag, output is a 2D array .i.e. Batchsize By output value( using LSTM Gates)","0eaa7e6e":"# We will try perform the sentiment analysis using LSTM layers ","18e94daa":"Now that we have a numerical representation of the words in the initial data set. This is required becuase we are going to use the word embeding in Keras(i.e. the Word2Vec implementation) to convert the review words to an  vector in n-dimensional space(n is a tuning parameter here) and embeding layer requries the input to be in the numerical form","4a9f7406":"The input of the LSTM is always is a 3D array. (batch_size, time_steps, seq_len)\n\nSo in our case, time_steps is the number of words in the review ,i.e. 400 &&\nseq_len is the vector size i.e. 50","7cdb610c":"*Conclusion*: So we have got an accuracy of 85%  on avg using one layer of LSTM ","09935d29":"# We will build the tokenizer","993c40a5":"We have built the graph, now its time to execute the graph\n\n## Execute the graph","448068c1":"*Observation*: So the data set is very well balanced given the equal distribution of the output variable","da2f333d":"## Pad Sequences in the review.\n## Max sequence of 300 words , we will fix since each sequence is of different length**","1164ed76":"## Build the Graph including the Embededding layer","6661d554":"Output of the embedding layer is a 3D matrix.\ni.e. \n\nBatchsize that we will give when the model is fit and a 2D matrix with [length of document * Embedding vector size]\n\nlets say the batch size is 100 and embedding vector size is 50, the output shape of the Embedding layer is\n\n[100 by 400 by 50]\n\nSame shape is the input to the LSTM Layer below\n","5799676c":"This means each vector is of length 50 dimension","930554a1":"Let's split the data set to train\/test","cbfab16b":"Before we build the embedding layer, We will import an already built Word2Vec model using Gensim","c394caee":"## Add an LSTM layer with 100 states","080398cc":" Now, we will define the embedding layer that takes the input encoded seqeuence of text and convert them a vector in a 50- dimensional space\nnote: Embedding layer takes 3 key parameters as input \n\n1. The vacab size\n2. max length of the document( review in our case)\n3. vector dimension of the each word we want the model to generate\n","bbdc4629":"This is the continuation of my previous notebook to perform the sentiment Analysis of moview reviews.\nIn this notebook we are going to use LSTM to enhance the accuracy\n\nLets us load the data and do the all the clean up activities similar to the other excersice"}}