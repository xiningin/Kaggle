{"cell_type":{"7e7cfeb9":"code","12ac4ecc":"code","345dc0a8":"code","20f13508":"code","f3bb0d80":"code","988bd114":"code","0460dcbb":"code","d66496de":"code","8f6b8280":"code","13cda70d":"code","0e274c63":"code","36de8cee":"code","a089ee41":"code","248ea976":"code","d34df005":"code","2e3d42b7":"code","cca7d77b":"code","08cddfa8":"code","ba462e10":"code","3a52c08f":"code","1209524a":"markdown","555162a8":"markdown","24ac49cd":"markdown","257b847b":"markdown","3fb13663":"markdown","2e6ec873":"markdown","b98ed085":"markdown","761d89d9":"markdown","9c0187e1":"markdown","f803c722":"markdown"},"source":{"7e7cfeb9":"# Base libraries\nimport numpy as np\nimport pandas as pd\nimport time","12ac4ecc":"# Visualization\nimport matplotlib as mpl\nimport scikitplot as skplt\nimport seaborn as sns\nsns.set()","345dc0a8":"# Preprocessing\nfrom sklearn.preprocessing import LabelEncoder\n# ML Metrics\nfrom sklearn.metrics import make_scorer, accuracy_score\n# ML Model selection\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n# CatBoost model\nfrom catboost import CatBoostClassifier, Pool","20f13508":"train_csv = pd.read_csv(\"..\/input\/train.csv\")\ntest_csv = pd.read_csv(\"..\/input\/test.csv\")\ny_train = train_csv['Survived'].copy()\nX_train = train_csv.drop(['Survived'], axis=1).copy()\nX_test = test_csv.copy()","f3bb0d80":"train_csv.head(5)","988bd114":"train_csv.info()","0460dcbb":"print('Train columns with null values:\\n', X_train.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test\/Validation columns with null values:\\n', X_test.isnull().sum())\nprint(\"-\"*10)","d66496de":"def feature_cleaning(actual):\n    new = actual.copy()\n    #complete missing age with median\n    new['Age'].fillna(new['Age'].median(), inplace = True)\n    #complete embarked with mode\n    new['Embarked'].fillna('S', inplace = True)\n    #complete missing fare with median\n    new['Fare'].fillna(new['Fare'].median(), inplace = True)\n    #drop cabin to ignore it\n    new = new.drop(['Cabin'], axis=1)\n    \n    return new\n","8f6b8280":"X_train_cln, X_test_cln = feature_cleaning(X_train), feature_cleaning(X_test)\n\n# Sanity check\nprint('Train columns with null values:\\n', X_train_cln.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test\/Validation columns with null values:\\n', X_test_cln.isnull().sum())\nprint(\"-\"*10)","13cda70d":"# Prepare the data\n# 'actual': current dataset, 'new': featured engineered dataset to be returned\ndef feature_engineering(actual):\n    \n    # Include all of the features from the actual dataset\n    new = actual.copy()\n    \n    # Engineer new features\n    new['FamilySize'] = new['SibSp'] + new['Parch'] + 1\n    new['IsAlone'] = np.where(new['FamilySize'] == 1, 1, 0)\n    \n    def get_title(name):\n        return name.split(',')[1].split('.')[0].strip()\n    \n    new['Title'] = new['Name'].apply(get_title)\n    new['Title'] = new['Title'].replace('Mlle', 'Miss')\n    new['Title'] = new['Title'].replace('Ms', 'Miss')\n    new['Title'] = new['Title'].replace('Mme', 'Mrs')\n    \n    stat_min = 10\n    title_names = (new['Title'].value_counts() < stat_min)\n    new['Title'] = new['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n    \n    new['FareBin'] = pd.qcut(new['Fare'], 4, labels=[1, 2, 3, 4], duplicates='drop')\n    new['AgeBin'] = pd.qcut(new['Age'], 6, labels=[1, 2, 3, 4, 5, 6], duplicates='drop')\n\n    drop_elements = ['PassengerId', 'Name', 'Age', 'Ticket', 'SibSp', 'Parch', 'Fare']\n    new = new.drop(drop_elements, axis=1)\n    \n    return new","0e274c63":"# Feature engineer X_train and X_test data\nX_train_eng, X_test_eng = feature_engineering(X_train_cln), feature_engineering(X_test_cln)\nX_train_eng.head()","36de8cee":"clf = CatBoostClassifier()\nparams = {'iterations': [500],\n          'depth': [4, 5, 6],\n          'loss_function': ['Logloss', 'CrossEntropy'],\n          'l2_leaf_reg': np.logspace(-20, -19, 3),\n          'leaf_estimation_iterations': [10],\n#           'eval_metric': ['Accuracy'],\n#           'use_best_model': ['True'],\n          'logging_level':['Silent'],\n          'random_seed': [42]\n         }\nscorer = make_scorer(accuracy_score)\nclf_grid = GridSearchCV(estimator=clf, param_grid=params, scoring=scorer, cv=5)","a089ee41":"X_train_enc = X_train_eng.apply(LabelEncoder().fit_transform)\nX_train_enc.head()","248ea976":"clf_grid.fit(X_train_enc, y_train)\nbest_param = clf_grid.best_params_\nbest_param","d34df005":"best_param","2e3d42b7":"# use_best_model params to prevent model overfitting\nmodel = CatBoostClassifier(iterations=1000,\n                           loss_function=best_param['loss_function'],\n                           depth=best_param['depth'],\n                           l2_leaf_reg=best_param['l2_leaf_reg'],\n                           eval_metric='Accuracy',\n                           leaf_estimation_iterations=10,\n                           use_best_model=True,\n                           logging_level='Silent',\n                           random_seed=42\n                          )","cca7d77b":"# make the x for train and test (also called validation data)\nxtrain, xval, ytrain, yval = train_test_split(X_train_eng, y_train,train_size=0.8,random_state=42)\n# sanity check to ensure all features are categories. In our case, yes.\ncate_features_index = np.where(X_train_eng.dtypes != float)[0]\ncate_features_index\n# create a training pool for the model to fit\ntrain_pool = Pool(xtrain, ytrain, cat_features=cate_features_index)","08cddfa8":"model.fit(train_pool, eval_set=(xval,yval))","ba462e10":"y_train_pred = model.predict(X_train_eng)\nskplt.metrics.plot_confusion_matrix(y_train, y_train_pred, normalize=True)","3a52c08f":"# Predicting the Test set results\ny_pred = model.predict(X_test_eng)\ny_pred = list(map(int, y_pred))  # Convert all the y_pred to int in the case y_preds are type floats\n\nsubmission = pd.DataFrame({\n        'PassengerId': test_csv['PassengerId'],\n        'Survived': y_pred\n    })\n\nsubmission.to_csv(\"submission.csv\", index=False)","1209524a":"### Step 4)  Feature engineering","555162a8":"*Note: For this kernel, I will ignore the data exploration and visualization components and go straight to the feature and model preparing. I recommend viewing the kernels mentioned in the **Credits** section and the bottom. ","24ac49cd":"### Step 2)  Extract data from files","257b847b":"### Step 1)  Import relevant libaries","3fb13663":"### Step 5)  Fit the best model","2e6ec873":"### Step 6)  Model checking and submission","b98ed085":"### Step 3)  Cleaning the data","761d89d9":"### Step 5)  GridSearch CV","9c0187e1":"> ","f803c722":"**Credits:**\n\nhttps:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n\nhttps:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n\nhttps:\/\/www.kaggle.com\/manrunning\/catboost-for-titanic-top-7\n\nhttps:\/\/www.kaggle.com\/miklgr500\/catboost-with-gridsearch-cv"}}