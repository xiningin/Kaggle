{"cell_type":{"3a54e4ab":"code","5f650338":"code","113e6440":"code","7ec1ae17":"code","89880747":"code","f88b1f54":"code","6a456c23":"code","eab032d4":"code","5c0782a0":"code","17663ae7":"code","48db1cd6":"code","f4ae5505":"code","c1823c77":"code","1fde813d":"code","6e4ee559":"code","6a88f510":"code","b74323ba":"code","ab6029c4":"code","b0421a7e":"code","f4e549d9":"code","e6053c3b":"code","339968fd":"code","dafe7210":"code","856e57cb":"code","2599a06b":"code","aa3cf26b":"code","572f352f":"code","19180f08":"code","9ad8f011":"code","2ccc8ec8":"code","84628664":"code","155dc027":"code","ec3df942":"code","f235a35a":"code","0cebc4f9":"code","abd517e7":"code","411a3432":"code","174cdf02":"code","6f045cba":"code","eb5a4e9d":"code","a1b4ffa6":"code","a5ef645b":"markdown","4a89467e":"markdown","d62d6227":"markdown","0e0a6423":"markdown","6f2f2aa0":"markdown","5f0ac680":"markdown","3813566a":"markdown","dd4b618c":"markdown","00599cd3":"markdown","636a9434":"markdown","f109f192":"markdown","39889507":"markdown","315653c5":"markdown","50bbce5f":"markdown","65128109":"markdown","b00d7f10":"markdown","38f1aa6d":"markdown","46743ee5":"markdown","948e5147":"markdown","7f823277":"markdown","879d808f":"markdown","5223674f":"markdown","5996058d":"markdown","3267b07e":"markdown","cb3bc862":"markdown","54b9e889":"markdown","699b065b":"markdown","5200f048":"markdown","7aa9fbc0":"markdown","a524b7f1":"markdown","a1562d90":"markdown","fe66ad06":"markdown","5cbdbbcc":"markdown","641448fd":"markdown","5a1e6c7c":"markdown","991912cd":"markdown","c978a1af":"markdown","942b6a99":"markdown","f1bdd373":"markdown"},"source":{"3a54e4ab":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\nsns.set(style='whitegrid')\nimport os\nfrom plotly.offline import init_notebook_mode, iplot\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","5f650338":"train = pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/test.csv')","113e6440":"train.head()","7ec1ae17":"test.head()","89880747":"print(\"Train\")\nprint(train.info())\nprint(train.isnull().sum())\nprint(\"-----------------------------------------------------------\")\nprint(\"Test\")\nprint(test.info())\nprint(test.isnull().sum())","f88b1f54":"train['Vehicle_Age']=train['Vehicle_Age'].replace({'< 1 Year':0,'1-2 Year':1,'> 2 Years':2})\ntrain['Gender']=train['Gender'].replace({'Male':1,'Female':0})\ntrain['Vehicle_Damage']=train['Vehicle_Damage'].replace({'Yes':1,'No':0})\n\ntest['Vehicle_Age']=test['Vehicle_Age'].replace({'< 1 Year':0,'1-2 Year':1,'> 2 Years':2})\ntest['Gender']=test['Gender'].replace({'Male':1,'Female':0})\ntest['Vehicle_Damage']=test['Vehicle_Damage'].replace({'Yes':1,'No':0})","6a456c23":"train.head()","eab032d4":"plt.figure(figsize=(12,12))\nsns.heatmap(train.corr(),annot=True, fmt=\".3f\")","5c0782a0":"sns.countplot(train.Response)","17663ae7":"count_1 = train[train[\"Response\"] == 1].value_counts().sum()\ntotalResponse = train[\"Response\"].value_counts().sum()\nprint(\"The percentage of positive response in train data is :\", round(count_1*100\/totalResponse),\"%\")","48db1cd6":"train.groupby(['Response','Vehicle_Age','Vehicle_Damage']).size()","f4ae5505":"sns.countplot(x='Previously_Insured',data=train,hue='Response')","c1823c77":"print(\"Most used channel:\")\nprint((train['Policy_Sales_Channel'].value_counts()))","1fde813d":"psc_notinterested=(train.loc[train['Response'][train['Response']==1].index.values])['Policy_Sales_Channel']\nsns.distplot(psc_notinterested)\nplt.title(\"Distribution of Policy Sales Channel for customers that were interested\")\nplt.show()","6e4ee559":"from sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score","6a88f510":"train.drop(columns=\"id\", inplace=True, errors=\"ignore\")","b74323ba":"X = train[train.columns[:-1]]\ny = train[train.columns[-1]]","ab6029c4":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","b0421a7e":"k_means = KMeans(n_clusters = 5,init='k-means++',random_state=0) \nclusters = k_means.fit(X) \nX['clusters'] = clusters.labels_","f4e549d9":"k_means.labels_","e6053c3b":"fig=px.bar(X.groupby('clusters').count().reset_index(),x='clusters',y='Gender')\nfig.show()","339968fd":"cluster_2 = X[X['clusters']== 2]\ncluster_2\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)\nplt.figure(figsize=(8,6))\nimportant_features = pd.Series(model.feature_importances_,index = cluster_2.columns)\nimportant_features.nlargest(11).plot(kind = \"bar\")\nplt.show()","dafe7210":"#Creating a Model\ntree_classifier = DecisionTreeClassifier()\n#Building a relationship by looking at x_train and y_train data\ntree_classifier.fit(X_train, y_train) ","856e57cb":"predictions = tree_classifier.predict(X_test)\npredictions","2599a06b":"plot_confusion_matrix(tree_classifier,X_test, y_test)","aa3cf26b":"accuracy_score(y_test, predictions)","572f352f":"precision_score(y_test, predictions)","19180f08":"recall_score(y_test, predictions)","9ad8f011":"f1_score(y_test, predictions)","2ccc8ec8":"rf_classifier = RandomForestClassifier(random_state=1)\nrf_classifier.fit(X_train, y_train)","84628664":"rf_predictions = rf_classifier.predict(X_test)\nrf_predictions","155dc027":"accuracy_score(y_test,rf_predictions), precision_score(y_test,rf_predictions), recall_score(y_test,rf_predictions), f1_score(y_test, rf_predictions)","ec3df942":"KNN = KNeighborsClassifier(n_neighbors=11, metric='minkowski', p = 2)\nKNN.fit(X_train, y_train)","f235a35a":"KNN_predictions = KNN.predict(X_test)\nKNN_predictions","0cebc4f9":"accuracy_score(y_test,KNN_predictions), precision_score(y_test,KNN_predictions), recall_score(y_test,KNN_predictions), f1_score(y_test,KNN_predictions)","abd517e7":"b_classifier = BaggingClassifier()\nb_classifier.fit(X_train, y_train)","411a3432":"b_predictions = b_classifier.predict(X_test)\nb_predictions","174cdf02":"accuracy_score(y_test,b_predictions), precision_score(y_test,b_predictions), recall_score(y_test,b_predictions), f1_score(y_test,b_predictions)","6f045cba":"print(\"DecisionTreeClassifier Accuracy = \",accuracy_score(y_test, predictions))\nprint(\"RandomForestClassifier Accuracy = \",accuracy_score(y_test,rf_predictions))\nprint(\"KNeighborsClassifier Accuracy = \",accuracy_score(y_test,KNN_predictions))\nprint(\"BaggingClassifier Accuracy = \",accuracy_score(y_test,b_predictions))","eb5a4e9d":"responses = KNN.predict(test[test.columns[1:]])","a1b4ffa6":"submission = pd.DataFrame(data = {'id': test['id'], 'Response': responses})\nsubmission.to_csv('submission.csv', index = False)\nsubmission.head()","a5ef645b":"I define the \"Response\" column to y and the other columns to X.","4a89467e":"**f1_score =** The F1 Score value shows us the harmonic mean of the Precision and Recall values.","d62d6227":"First let's read datas and take a look at the data we have","0e0a6423":"A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions to form a final prediction.","6f2f2aa0":"The most used sales channels are 152, 26 and 124. The best channel that results in customer interest is 152.","5f0ac680":"It seems that KNeighborsClassifier has the best accuracy score. So I am going to use this model on submission.csv","3813566a":"First, I delete the \"id\" column as it will not contribute to model training.","dd4b618c":"We will divide our data into 4 variables;\nThe x_train and y_train variables for training, x_test and y_test variables to test the model at the end of the training.\n\nThe test_size parameter specifies what percentage of the data set should be reserved for testing.","00599cd3":"By sending X_test data to the predict function, we get a predict result.","636a9434":"Importing Libraries","f109f192":"On this graph we can see how many of our predictions are correct.\n\n* The bottom right corner is the number of values we guessed to be 1 and are actually 1, so it is True Positive.\n* The bottom left corner is the number of values we guessed to be 0 but are actually 1, so it is False Negative. \n* The top right corner is the number of values we guessed to be 1 but are actually 0, so it is False Positive.\n* The top left corner is the number of values we guessed to be 0 and are actually 0, so it is True Negative.","39889507":"**accuracy_score =** Accuracy is one metric for evaluating classification models. Informally, accuracy is the fraction of predictions our model got right.","315653c5":"Most of the vehicles of customers with response 1 are between the ages of 1-2 and their vehicles are damaged.","50bbce5f":"Let's try different models","65128109":"Customers who were previously insured tend not to be interested. We can think that the reason for this is that their previous insurance agreement has not expired yet.","b00d7f10":"**recall_score =** It is a metric that shows how many of the operations we need to predict positive.","38f1aa6d":"Compare the accuracy scores in all the models : ","46743ee5":"# BaggingClassifier","948e5147":"Let's divide our data into 5 clusters with the K-Means algorithm.","7f823277":"**Let's check the accuracy of the model.**","879d808f":"We can see that both data sets have not null value.","5223674f":"# RandomForestClassifier","5996058d":"# Label Encoding","3267b07e":"Decision trees are one of the algorithms often used in the solution of classification problems. Its purpose is to create a model that estimates the value of a variable by extracting simple rules from data properties and learning these rules.","cb3bc862":"# Correlation","54b9e889":"# Read Data","699b065b":"**precision_score =**  It shows how many of the values we guess as Positive are actually Positive.","5200f048":"We replaced some values in the data sets with numerical values, as follows;\n\n**Vehicle Age ->**\n* \"<1 Year\" = 0\n* \"1-2 Year\" = 1\n* \">2 Year\" = 2\n\n**Gender ->**\n* \"Female\" = 0\n* \"Male\" = 1\n\n**Vehicle Damage ->**\n* \"No\" = 0\n* \"Yes\" = 1","7aa9fbc0":"In machine learning, we usually deal with datasets which contains multiple labels in one or more than one columns. These labels can be in the form of words or numbers. To make the data understandable or in human readable form, the training data is often labeled in words.\nLabel Encoding refers to converting the labels into numeric form so as to convert it into the machine-readable form.","a524b7f1":"# An Overview Of The Data Set","a1562d90":"RandomForestClassifier generate multiple decision trees. When it will produce a result, the average value in these decision trees is taken and the result is produced.","fe66ad06":"We see that most of the customers are gathered in cluster 2.","5cbdbbcc":"# KNeighborsClassifier","641448fd":"# Importing Libraries","5a1e6c7c":"I choose customers in cluster 2 and used ExtraTreesClassifier () to find the most important features. So we can have an idea why customers are gathering more in cluster 2.","991912cd":"# Model Building - DecisionTreeClassifier","c978a1af":"The purpose of the K Nearest Neighbors algorithm, which is a classification algorithm, is to classify our data sets and then place the data whose class is unknown to the closest class.The number of elements to be looked at in the algorithm's work is determined by a K value. When a value comes, the distance between the value is calculated by taking the nearest K number of elements. The Euclidean function is generally used in the distance calculation. After the distance is calculated, it is sorted and the corresponding value is assigned to the appropriate class.","942b6a99":"# File submission","f1bdd373":"We can see that the most influencing factors for Response are Vehicle_Damage and Previously_Insured, followed by Vehicle_Age and Policy_Sales_Channel."}}