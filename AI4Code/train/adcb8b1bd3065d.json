{"cell_type":{"723a6385":"code","c737910b":"code","6e8c24e6":"code","37deb2fe":"code","8d6314d7":"code","9b3dead8":"markdown","bb9c2bdb":"markdown","a65d55cf":"markdown","c98f6840":"markdown","7d96c8d5":"markdown","211673cc":"markdown","e0921bc5":"markdown","cfddce15":"markdown","37a4cabb":"markdown","2c44a26c":"markdown"},"source":{"723a6385":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c737910b":"\n\nfrom torch import nn\n\nimport torch\nimport numpy as np\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\n\n\n# Reshape + Concat layer\n\nclass Reshape_Concat_Adap(torch.autograd.Function):\n    blocksize = 0\n\n    def __init__(self, block_size):\n        # super(Reshape_Concat_Adap, self).__init__()\n        Reshape_Concat_Adap.blocksize = block_size\n\n    @staticmethod\n    def forward(ctx, input_, ):\n        ctx.save_for_backward(input_)\n\n        data = torch.clone(input_.data)\n        b_ = data.shape[0]\n        c_ = data.shape[1]\n        w_ = data.shape[2]\n        h_ = data.shape[3]\n\n        output = torch.zeros((b_, int(c_ \/ Reshape_Concat_Adap.blocksize \/ Reshape_Concat_Adap.blocksize),\n                              int(w_ * Reshape_Concat_Adap.blocksize), int(h_ * Reshape_Concat_Adap.blocksize))).cuda()\n\n        for i in range(0, w_):\n            for j in range(0, h_):\n                data_temp = data[:, :, i, j]\n                # data_temp = torch.zeros(data_t.shape).cuda() + data_t\n                # data_temp = data_temp.contiguous()\n                data_temp = data_temp.view((b_, int(c_ \/ Reshape_Concat_Adap.blocksize \/ Reshape_Concat_Adap.blocksize),\n                                            Reshape_Concat_Adap.blocksize, Reshape_Concat_Adap.blocksize))\n                # print data_temp.shape\n                output[:, :, i * Reshape_Concat_Adap.blocksize:(i + 1) * Reshape_Concat_Adap.blocksize,\n                j * Reshape_Concat_Adap.blocksize:(j + 1) * Reshape_Concat_Adap.blocksize] += data_temp\n\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        inp, = ctx.saved_tensors\n        input_ = torch.clone(inp.data)\n        grad_input = torch.clone(grad_output.data)\n\n        b_ = input_.shape[0]\n        c_ = input_.shape[1]\n        w_ = input_.shape[2]\n        h_ = input_.shape[3]\n\n        output = torch.zeros((b_, c_, w_, h_)).cuda()\n        output = output.view(b_, c_, w_, h_)\n        for i in range(0, w_):\n            for j in range(0, h_):\n                data_temp = grad_input[:, :, i * Reshape_Concat_Adap.blocksize:(i + 1) * Reshape_Concat_Adap.blocksize,\n                            j * Reshape_Concat_Adap.blocksize:(j + 1) * Reshape_Concat_Adap.blocksize]\n                # data_temp = torch.zeros(data_t.shape).cuda() + data_t\n                data_temp = data_temp.contiguous()\n                data_temp = data_temp.view((b_, c_, 1, 1))\n                output[:, :, i, j] += torch.squeeze(data_temp)\n\n        return Variable(output)\n\n\ndef My_Reshape_Adap(input, blocksize):\n    return Reshape_Concat_Adap(blocksize).apply(input)\n\n\n# The residualblock for reconstruction network\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels, has_BN = False):\n        super(ResidualBlock, self).__init__()\n        self.has_BN = has_BN\n        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n        if has_BN:\n            self.bn1 = nn.BatchNorm2d(channels)\n        self.prelu = nn.PReLU()\n        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n        if has_BN:\n            self.bn2 = nn.BatchNorm2d(channels)\n\n    def forward(self, x):\n        residual = self.conv1(x)\n        if self.has_BN:\n            residual = self.bn1(residual)\n        residual = self.prelu(residual)\n        residual = self.conv2(residual)\n        if self.has_BN:\n            residual = self.bn2(residual)\n\n        return x + residual\n\n\n#  code of CSNet\nclass CSNet(nn.Module):\n    def __init__(self, blocksize=32, subrate=0.1):\n\n        super(CSNet, self).__init__()\n        self.blocksize = blocksize\n\n        # for sampling\n        self.sampling = nn.Conv2d(1, int(np.round(blocksize*blocksize*subrate)), blocksize, stride=blocksize, padding=0, bias=False)\n        # upsampling\n        self.upsampling = nn.Conv2d(int(np.round(blocksize*blocksize*subrate)), blocksize*blocksize, 1, stride=1, padding=0)\n\n        # reconstruction network\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n            nn.PReLU()\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.PReLU()\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.PReLU()\n        )\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.PReLU()\n        )\n        \n        self.conv5 = nn.Conv2d(64, 1, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        x = self.sampling(x)\n        x = self.upsampling(x)\n        x = My_Reshape_Adap(x, self.blocksize) # Reshape + Concat\n\n        block1 = self.conv1(x)\n        block2 = self.conv2(block1)\n        block3 = self.conv3(block2)\n        block4 = self.conv4(block3)\n        block5 = self.conv5(block4)\n\n        return block5\n    \n    \n# #  code of CSNet_Enhanced (Enhanced version of CSNet)\n# # class CSNet_Enhanced(nn.Module):\n# class CSNet(nn.Module):\n#     def __init__(self, blocksize=32, subrate=0.1):\n\n# #         super(CSNet_Enhanced, self).__init__()\n#         super(CSNet, self).__init__()\n#         self.blocksize = blocksize\n\n#         # for sampling\n#         self.sampling = nn.Conv2d(1, int(np.round(blocksize*blocksize*subrate)), blocksize, stride=blocksize, padding=0, bias=False)\n#         # upsampling\n#         self.upsampling = nn.Conv2d(int(np.round(blocksize*blocksize*subrate)), blocksize*blocksize, 1, stride=1, padding=0)\n\n#         # reconstruction network\n#         self.block1 = nn.Sequential(\n#             nn.Conv2d(1, 64, kernel_size=7, padding=3),\n#             nn.PReLU()\n#         )\n#         self.block2 = ResidualBlock(64, has_BN=True)\n#         self.block3 = ResidualBlock(64, has_BN=True)\n#         self.block4 = ResidualBlock(64, has_BN=True)\n#         self.block5 = ResidualBlock(64, has_BN=True)\n#         self.block6 = ResidualBlock(64, has_BN=True)\n#         self.block7 = nn.Sequential(\n#             nn.Conv2d(64, 64, kernel_size=3, padding=1),\n#             nn.PReLU()\n#         )\n#         self.block8 = nn.Conv2d(64, 1, kernel_size=3, padding=1)\n\n#     def forward(self, x):\n#         x = self.sampling(x)\n#         x = self.upsampling(x)\n#         x = My_Reshape_Adap(x, self.blocksize) # Reshape + Concat\n\n#         block1 = self.block1(x)\n#         block2 = self.block2(block1)\n#         block3 = self.block3(block2)\n#         block4 = self.block4(block3)\n#         block5 = self.block5(block4)\n#         block6 = self.block6(block5)\n#         block7 = self.block7(block6)\n#         block8 = self.block8(block1 + block7)\n\n#         return block8\n\n\n#  code of CSNet\nclass CSNet2(nn.Module):\n    def __init__(self, blocksize=32, subrate=0.1):\n\n        super(CSNet2, self).__init__()\n        self.blocksize = blocksize\n\n        # for sampling\n        self.sampling = nn.Conv2d(1, int(np.round(blocksize*blocksize*subrate)), blocksize, stride=blocksize, padding=0, bias=False)\n        # upsampling\n        self.upsampling = nn.Conv2d(int(np.round(blocksize*blocksize*subrate)), blocksize*blocksize, 1, stride=1, padding=0)\n\n        # reconstruction network\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n            nn.PReLU()\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.PReLU()\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.PReLU()\n        )\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.PReLU()\n        )\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.PReLU()\n        )\n        self.conv6 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.PReLU()\n        )\n        self.conv7 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.PReLU()\n        )\n        self.conv8 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.PReLU()\n        )\n        self.conv9 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.PReLU()\n        )\n        self.conv10 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.PReLU()\n        )\n        \n        self.conv11 = nn.Conv2d(64, 1, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        x = self.sampling(x)\n        x = self.upsampling(x)\n        x = My_Reshape_Adap(x, self.blocksize) # Reshape + Concat\n\n        block1 = self.conv1(x)\n        block2 = self.conv2(block1)\n        block3 = self.conv3(block2)\n        block4 = self.conv4(block3)\n        block5 = self.conv5(block4)\n        block6 = self.conv6(block5)\n        block7 = self.conv7(block6)\n        block8 = self.conv8(block7)\n        block9 = self.conv9(block8)\n        block10 = self.conv10(block9)\n        block11 = self.conv11(block10)\n\n        return block11\n  \n\n# if __name__ == '__main__':\n#     import torch\n\n#     img = torch.randn(102, 3, 32, 32)\n#     net = CSNet()\n#     out = net(img)\n#     print(out.size())","6e8c24e6":"from os import listdir\nfrom os.path import join\n\nfrom PIL import Image\nfrom torch.utils.data.dataset import Dataset\nfrom torchvision.transforms import Compose, RandomResizedCrop, RandomHorizontalFlip, RandomVerticalFlip, RandomRotation, RandomCrop, ToTensor, ToPILImage, CenterCrop, Resize, Grayscale, ColorJitter\n\nimport random\nimport math\nfrom torch.autograd import Variable\nimport torch\n\nimport torchvision.transforms as transforms\n\n# gray = transforms.Gray()\nimport numpy as np\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in ['.png', 'bmp', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'])\n\n\ndef calculate_valid_crop_size(crop_size, blocksize):\n    return crop_size - (crop_size % blocksize)\n\n\ndef train_hr_transform(crop_size):\n    return Compose([\n        RandomCrop(crop_size),\n        RandomHorizontalFlip(p=0.5),\n        RandomVerticalFlip(p=0.5),\n        RandomRotation((-90,90)),\n        RandomRotation((-180,180)),\n        RandomRotation((-270,270)),\n        ColorJitter(0.5, 0.5, 0.5),\n        Grayscale(),\n        ToTensor(),\n#         torchvision.transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),\n    ])\n\n\n\ndef psnr(img1, img2):\n    mse = torch.mean((img1 - img2) ** 2)\n    if mse < 1.0e-10:\n        return 100\n    PIXEL_MAX = 1.0\n    return 20 * math.log10(PIXEL_MAX\/math.sqrt(mse))\n\n\nclass TrainDatasetFromFolder(Dataset):\n    def __init__(self, dataset_dir, crop_size, blocksize):\n        super(TrainDatasetFromFolder, self).__init__()\n        self.image_filenames = [join(dataset_dir, x) for x in listdir(dataset_dir) if is_image_file(x)]\n        crop_size = calculate_valid_crop_size(crop_size, blocksize)\n        self.hr_transform = train_hr_transform(crop_size)\n\n    def __getitem__(self, index):\n        try:\n            hr_image = self.hr_transform(Image.open(self.image_filenames[index]))\n            return hr_image, hr_image\n        except:\n            hr_image = self.hr_transform(Image.open(self.image_filenames[index+1]))\n            return hr_image, hr_image\n\n    def __len__(self):\n        return len(self.image_filenames)\n\n\nclass TestDatasetFromFolder(Dataset):\n    def __init__(self, dataset_dir, blocksize):\n        super(TestDatasetFromFolder, self).__init__()\n        self.blocksize = blocksize\n        self.image_filenames = [join(dataset_dir, x) for x in listdir(dataset_dir) if is_image_file(x)]\n\n    def __getitem__(self, index):\n        hr_image = Image.open(self.image_filenames[index])\n\n        w, h = hr_image.size\n        w = int(np.floor(w\/self.blocksize)*self.blocksize)\n        h = int(np.floor(h\/self.blocksize)*self.blocksize)\n        crop_size = (h, w)\n\n        hr_image = CenterCrop(crop_size)(hr_image)\n        hr_image = Grayscale()(hr_image)\n\n        return ToTensor()(hr_image), ToTensor()(hr_image)\n\n    def __len__(self):\n        return len(self.image_filenames)","37deb2fe":"import torch\nimport torch.utils.data as Data\nimport torchvision\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n# from lib.network import CSNet_Enhanced as CSNet\n# from lib.network_rgb import CSNet\nfrom torch import nn\nimport time\nimport os\n\nimport argparse\nfrom tqdm import tqdm\n\n# from data_utils_org import TrainDatasetFromFolder\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\n\nparser = argparse.ArgumentParser(description='Train Super Resolution Models')\nparser.add_argument('--crop_size', default=96, type=int, help='training images crop size')\nparser.add_argument('--block_size', default=32, type=int, help='CS block size')\nparser.add_argument('--pre_epochs', default=200, type=int, help='pre train epoch number')\nparser.add_argument('--num_epochs', default=300, type=int, help='train epoch number')\n# parser.add_argument('--num_epochs', default=3000, type=int, help='train epoch number')\n\nparser.add_argument('--batchSize', default=64, type=int, help='train batch size')\nparser.add_argument('--sub_rate', default=0.1, type=float, help='sampling sub rate')\n\nparser.add_argument('--loadEpoch', default=0, type=int, help='load epoch number')\nparser.add_argument('--generatorWeights', type=str, default='', help=\"path to CSNet weights (to continue training)\")\n\nargs = parser.parse_args(args=[\"--sub_rate\",\"0.1\",\"--block_size\", \"32\",\"--batchSize\",\"64\"])\nprint(args)\n\n# opt = parser.parse_args()\nopt = args\n\nCROP_SIZE = opt.crop_size\nBLOCK_SIZE = opt.block_size\nNUM_EPOCHS = opt.num_epochs\nPRE_EPOCHS = opt.pre_epochs\nLOAD_EPOCH = 0\nBATCH_SIZE = opt.batchSize\n\n\n# if __name__ == '__main__':\n\ntrain_set = TrainDatasetFromFolder('..\/input\/bsds-augment8000\/BSDS500aug_v5', crop_size=CROP_SIZE, blocksize=BLOCK_SIZE)\n# train_set = TrainDatasetFromFolder('..\/input\/intel-image-classification\/seg_pred\/seg_pred', crop_size=CROP_SIZE, blocksize=BLOCK_SIZE)\n# train_set = TrainDatasetFromFolder('\/media\/gdh-95\/data\/Train', crop_size=CROP_SIZE, blocksize=BLOCK_SIZE)\ntrain_loader = DataLoader(dataset=train_set, num_workers=4, batch_size=opt.batchSize, shuffle=True)\n# net = CSNet(BLOCK_SIZE, opt.sub_rate)#\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\nnet = CSNet2(BLOCK_SIZE, opt.sub_rate)#\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\n\n\nmse_loss = nn.MSELoss()\n\nif opt.generatorWeights != '':\n    net.load_state_dict(torch.load(opt.generatorWeights))\n    LOAD_EPOCH = opt.loadEpoch\n\nif torch.cuda.is_available():\n    net.cuda()\n    mse_loss.cuda()\n\noptimizer = optim.Adam(net.parameters(), lr=0.0004, betas=(0.9, 0.999))\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n\nfor epoch in range(LOAD_EPOCH, NUM_EPOCHS + 1):\n    train_bar = tqdm(train_loader)\n    running_results = {'batch_sizes': 0, 'g_loss': 0, }\n\n    net.train()\n    scheduler.step()\n\n    for data, target in train_bar:\n        batch_size = data.size(0)\n        if batch_size <= 0:\n            continue\n\n        running_results['batch_sizes'] += batch_size\n\n        real_img = Variable(target)\n        if torch.cuda.is_available():\n            real_img = real_img.cuda()\n        z = Variable(data)\n        if torch.cuda.is_available():\n            z = z.cuda()\n\n        print(\"zshape =\", z.shape)\n        fake_img = net(z)\n        optimizer.zero_grad()\n        g_loss = mse_loss(fake_img, real_img)\n\n        g_loss.backward()\n        optimizer.step()\n\n        running_results['g_loss'] += g_loss.item() * batch_size\n\n        train_bar.set_description(desc='[%d] Loss_G: %.4f lr: %.7f' % (\n            epoch, running_results['g_loss'] \/ running_results['batch_sizes'], optimizer.param_groups[0]['lr']))\n\n    # for saving model\n#     save_dir = '\/kaggle\/working\/BSDS500_epochs' + '_subrate_' + str(opt.sub_rate) + '_blocksize_' + str(BLOCK_SIZE)\n    save_dir = '\/kaggle\/working\/net3bsds_epochs' + '_subrate_' + str(opt.sub_rate) + '_blocksize_' + str(BLOCK_SIZE) \n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    if epoch % 10 == 0:\n        torch.save(net.state_dict(), save_dir + '\/net_epoch_%d_%6f.pth' % (epoch, running_results['g_loss']\/running_results['batch_sizes']))\n    print(save_dir)","8d6314d7":"# import argparse\n# import torch\n# from torch.autograd import Variable\n# import numpy as np\n# import time, math, glob\n# import scipy.io as sio\n# # from lib.network import CSNet\n# from skimage import measure\n# from skimage.metrics import structural_similarity as ssim\n# # from skimage.measure import compare_ssim\n\n# parser = argparse.ArgumentParser(description=\"PyTorch LapSRN Eval\")\n# parser.add_argument(\"--cuda\", action=\"store_true\", help=\"use cuda?\")\n# # parser.add_argument(\"--model\", default=\"epochs_subrate_0.1_blocksize_32\/net_epoch_195_0.001642.pth\", type=str, help=\"model path\")\n# parser.add_argument(\"--model\", default=\"..\/input\/csnet-train400\/net2bsds_epochs_subrate_0.1_blocksize_32\/net_epoch_270_0.002861.pth\", type=str, help=\"model path\")\n# # parser.add_argument(\"--model\", default='..\/input\/csnet\/BSDS500_epochs_subrate_0.1_blocksize_32\/net_epoch_300_0.009575.pth', type=str, help=\"model path\")\n\n\n\n# parser.add_argument(\"--dataset\", default=\"..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\", type=str, help=\"dataset name, Default: Set5\")\n# parser.add_argument('--block_size', default=32, type=int, help='CS block size')\n# parser.add_argument('--sub_rate', default=0.1, type=float, help='sampling sub rate')\n\n\n# def PSNR(pred, gt, shave_border=0):\n#     height, width = pred.shape[:2]\n#     pred = pred[shave_border:height - shave_border, shave_border:width - shave_border]\n#     gt = gt[shave_border:height - shave_border, shave_border:width - shave_border]\n#     imdff = pred - gt\n#     rmse = math.sqrt(np.mean(imdff ** 2))\n#     if rmse == 0:\n#         return 100\n#     return 20 * math.log10(255.0 \/ rmse)\n\n# # opt = parser.parse_args()\n# opt = parser.parse_args(args=[\"--cuda\",\"--sub_rate\",\"0.1\",\"--block_size\", \"32\"])\n# cuda = opt.cuda\n\n# if cuda and not torch.cuda.is_available():\n#     raise Exception(\"No GPU found, please run without --cuda\")\n\n# # model = CSNet(opt.block_size, opt.sub_rate)\n# model = CSNet2(opt.block_size, opt.sub_rate)\n\n# if opt.model != '':\n#     model.load_state_dict(torch.load(opt.model))\n\n\n# image_list = glob.glob(opt.dataset+\"\/*.*\") \n\n# avg_psnr_predicted = 0.0\n# avg_elapsed_time = 0.0\n# avg_ssim_predicted = 0.0\n\n# count = 0\n\n# for image_name in image_list:\n#     count = count + 1\n#     print(\"Processing \", image_name)\n#     im_gt_y = sio.loadmat(image_name)['im_gt_y']\n\n#     im_gt_y = im_gt_y.astype(float)\n\n#     im_input = im_gt_y\/255.\n\n#     im_input = Variable(torch.from_numpy(im_input).float()).view(1, -1, im_input.shape[0], im_input.shape[1])\n\n#     if cuda:\n#         model = model.cuda()\n#         im_input = im_input.cuda()\n#     else:\n#         model = model.cpu()\n\n#     start_time = time.time()\n#     res = model(im_input)\n#     elapsed_time = time.time() - start_time\n#     avg_elapsed_time += elapsed_time\n\n#     res = res.cpu()\n\n#     im_res_y = res.data[0].numpy().astype(np.float32)\n\n#     im_res_y = im_res_y*255.\n#     im_res_y[im_res_y<0] = 0\n#     im_res_y[im_res_y>255.] = 255.\n#     im_res_y = im_res_y[0,:,:]\n\n#     # print(im_res_y)\n\n#     psnr_predicted = PSNR(im_gt_y, im_res_y,shave_border=0)\n#     print(psnr_predicted)\n#     avg_psnr_predicted += psnr_predicted\n#     ssim_predicted = ssim(im_gt_y, im_res_y, data_range=255)\n# #     ssim_predicted = compare_ssim(im_gt_y, im_res_y)\n#     print(ssim_predicted)\n#     avg_ssim_predicted += ssim_predicted\n    \n    \n#     im = Image.fromarray(im_res_y)\n#     im.convert('RGB').save(\".\/\"+str(count) +\"enhanced\"+\".jpeg\", format = 'jpeg')\n    \n\n# print(\"Dataset=\", opt.dataset)\n# print(\"PSNR_predicted=\", avg_psnr_predicted\/len(image_list))\n# print(\"SSIM = \", avg_ssim_predicted\/len(image_list))\n# print(\"It takes average {}s for processing\".format(avg_elapsed_time\/len(image_list)))","9b3dead8":"* CSNet epoch=300\n* Processing  ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\/woman_GT.mat \n* 22.21573028916497\n* Processing  ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\/bird_GT.mat\n* 25.20689062068645\n* Processing  ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\/head_GT.mat\n* 28.546598820705608\n* Processing  ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\/baby_GT.mat\n* 27.53561108444472\n* Processing  ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\/butterfly_GT.mat\n* 17.841248262180137\n* Dataset= ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\n* PSNR_predicted= 24.269215815436375\n* It takes average 0.124904203414917s for processing","bb9c2bdb":"* Processing  ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\/woman_GT.mat\n* 29.15992840193841\n* Processing  ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\/bird_GT.mat\n* 33.04581308878138\n* Processing  ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\/head_GT.mat\n* 32.751008864543245\n* Processing  ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\/baby_GT.mat\n* 34.11356887503709\n* Processing  ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\/butterfly_GT.mat\n* 25.828552197541768\n* Dataset= ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\n* PSNR_predicted= 30.97977428556838\n* It takes average 0.006215858459472656s for processing","a65d55cf":"model","c98f6840":"* 8000\uff0cbatchsize64\uff0cepoch300 subrate=0.1\n* Processing  ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\/woman_GT.mat\n* 29.750817966704485\n* Processing  ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\/bird_GT.mat\n* 33.807882004313086\n* Processing  ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\/head_GT.mat\n* 33.11335584543932\n* Processing  ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\/baby_GT.mat\n* 34.61596175015535\n* Processing  ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\/butterfly_GT.mat\n* 26.42217787980856\n* Dataset= ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\n* PSNR_predicted= 31.542039089284163\n* It takes average 0.1273731231689453s for processing","7d96c8d5":"* CSNet_Enhance epoch=300\n* Processing  ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\/woman_GT.mat\n* 22.02196151473902\n* Processing  ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\/bird_GT.mat\n* 20.721855371034547\n* Processing  ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\/head_GT.mat\n* 20.128885266446925\n* Processing  ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\/baby_GT.mat\n* 21.87925368164791\n* Processing  ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\/butterfly_GT.mat\n* 18.634821271771006\n* Dataset= ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\n* PSNR_predicted= 20.67735542112788\n* It takes average 0.008675336837768555s for processing","211673cc":"training:","e0921bc5":"data processing","cfddce15":"8000\uff0c 7\u5c423x3x64 31.79 0.90","37a4cabb":"* training images = 10000\n* Processing  ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\/woman_GT.mat\n* 29.8838082118016\n* Processing  ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\/bird_GT.mat\n* 33.77557202024809\n* Processing  ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\/head_GT.mat\n* 33.13061511545048\n* Processing  ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\/baby_GT.mat\n* 34.67020714861093\n* Processing  ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\/butterfly_GT.mat\n* 26.34278850817784\n* Dataset= ..\/input\/imagecompress\/CSNet-Pytorch-master\/Test\/Set5_mat\n* PSNR_predicted= 31.56059820085779\n* It takes average 0.0069392681121826175s for processing","2c44a26c":"testing:\uff08test_new\uff09"}}