{"cell_type":{"65d7e498":"code","792fae95":"code","8c8fcab0":"code","c462e94d":"code","3f29f0b1":"code","528ffefa":"code","b39daf28":"code","9e42da09":"code","2a0a08ee":"code","ca3e8024":"code","daffb45e":"code","f129618d":"code","a3f9fe49":"code","96999991":"code","f6775836":"code","db265313":"code","c296e8e4":"code","3430edb1":"code","cc02ad25":"code","3282144a":"code","13e7b6db":"code","68b39d35":"code","9d5b20b7":"code","7f7235c6":"code","9cd6d857":"code","6df129e5":"code","ed5e4fa0":"code","a7c21ef8":"code","2fbe5e2d":"code","9cce45de":"code","f4ba65eb":"code","12361146":"code","2290d142":"code","62bb8776":"code","f90d4c08":"code","8a54fbb1":"code","e2997746":"code","ba66c9f3":"code","fe55840f":"code","ce561399":"code","af508b60":"markdown","7d752519":"markdown","aabaa982":"markdown"},"source":{"65d7e498":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","792fae95":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom numpy import array\nimport sys\nfrom keras.layers import LSTM\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.utils import np_utils\nfrom keras.layers import Dropout\nfrom keras.layers import Embedding\nfrom keras.layers import Bidirectional\nimport re","8c8fcab0":"!pip install openpyxl","c462e94d":"df = pd.read_csv('..\/input\/pink-floyd-lyrics\/pink_floyd_lyrics.csv')\npop = pd.read_excel('..\/input\/pink-floyd-popularity-by-album\/CSPCPinkFloydTotals-2.xlsx',header=0)\ndf.head()","3f29f0b1":"pop = pop.drop(['Unnamed: 5'],axis=1)\npop = pop[1:]\npop.rename(columns={\"Unnamed: 1\": \"Year\", \n                    \"Full Length\": \"Full_Length_Studio_Album\",\n                    \"Unnamed: 3\": \"Full_Length_Other_releases\",\n                   \"Singles\":\"Singles_Physical\",\n                    \"Unnamed: 6\":\"Singles_Digital\",\n                    \"Unnamed: 7\": \"Singles_Streaming\"}, errors=\"raise\",inplace=True)","528ffefa":"df.info()","b39daf28":"df['year'] = df['year'].apply(lambda x: int(x.split('-')[0]))\ndf.head()","9e42da09":"df.year.max()-df.year.min()","2a0a08ee":"plt.figure(figsize=(12,12),edgecolor='blue')\nsns.countplot(x=\"year\", data=df)","ca3e8024":"pop","daffb45e":"pop['Total CSPC'] = [int(i.replace(' ','')) for i in pop['Total CSPC']]\npop['Year'] = [str(i) for i in pop['Year']]\npop = pop[:-1]","f129618d":"plt.figure(figsize=(14, 8))\nplt.xlabel('Year of Album Release',fontsize=12)\nplt.ylabel('Total CSPC',fontsize=12)\nplt.title(\"Pink Floyd's popularity wrt Time\",fontsize=18)\nplt.plot(pop['Year'],pop['Total CSPC'], color='blue', marker='o', markeredgecolor='green', markerfacecolor='red')\nplt.savefig('pop_plot.jpeg')","a3f9fe49":"def count_lines(text):\n  count,k,p=0,0,0\n  for i in text.split('\\n'):\n    res = re.findall(\"[a-zA-Z\u2019]+\", i)\n    if res:\n      p+=1\n      if len(res)>2:\n        k+=1\n        count+=len(res)\n  if k!=0:\n    mean = count\/k\n  else:\n    mean = 2\n  total = p\n  big_lines = k\n  small_lines = (p-k)\n  \n  return mean,total,big_lines,small_lines\n\ncl = []\nfor j in range(0,len(df)):\n  if type(df.lyrics.iloc[j])==str :\n    m,t,bl,sl = count_lines(df.lyrics.iloc[j])\n    year = int(df.year.iloc[j])\n    if sl!=0:\n      l = [m,t,bl,sl,bl\/sl,year]\n    else:\n      l = [m,t,bl,sl,0,year]\n    cl.append(l)\n\ndf_clines = pd.DataFrame(cl,columns=['mean_lines','total_lines','big_lines','small_lines','ratio_of_blsl','year'])\ndf_clines.head()","96999991":"fig,axs = plt.subplots(2,2,figsize=(10,7))\nfig.suptitle('Testing Line count')\n\naxs[0,0].hist(df_clines['mean_lines'],edgecolor='red')\naxs[0,0].set_title('Mean Lines')\naxs[0,1].hist(df_clines['total_lines'],edgecolor='red')\naxs[0,1].set_title('Total Lines')\naxs[1,0].hist(df_clines['big_lines'],edgecolor='red')\naxs[1,0].set_title('Big Lines')\naxs[1,1].hist(df_clines['ratio_of_blsl'],edgecolor='red')\naxs[1,1].set_title('Ratio of Big\/small lines')","f6775836":"df_clines_gb = df_clines.groupby(['year']).mean()\ndf_clines_gb.reset_index(level=0, inplace=True)\ndf_clines_gb","db265313":"fig,axs = plt.subplots(2,2,figsize=(20,10))\nfig.suptitle('Testing Line count')\n\nsns.barplot(ax=axs[0,0],x=\"year\",y='mean_lines', data=df_clines_gb)\naxs[0,0].set_title('Mean Words per Lines')\nsns.barplot(ax=axs[0,1],x=\"year\",y='total_lines', data=df_clines_gb)\naxs[0,1].set_title('Total Lines')\nsns.barplot(ax=axs[1,0],x=\"year\",y='big_lines', data=df_clines_gb)\naxs[1,0].set_title('Big Lines')\nsns.barplot(ax=axs[1,1],x=\"year\",y='ratio_of_blsl', data=df_clines_gb)\naxs[1,1].set_title('Ratio of Big\/small lines')\n","c296e8e4":"tokenizer = Tokenizer(char_level=False)\ntokenizer.fit_on_texts([df.lyrics.iloc[1].replace('\\n',' \\n ')])\ntokenizer.word_index","3430edb1":"text = df.lyrics.iloc[1].split('\\n')\ntext = [re.sub(r'\\d+', '', i) for i in text]\ncorpus = list(set(text))","cc02ad25":"corpus","3282144a":"lines=[]\n\nfor line in corpus:\n    token_list = tokenizer.texts_to_sequences([line])[0]\n    print('--'*25)\n    print(token_list)\n    print('--'*25)\n    for i in range(1, len(token_list)):\n        n_gram_sequence = token_list[:i+1]\n        print(n_gram_sequence)","13e7b6db":"def ngram(token_list):\n  ng = []\n  for i in range(1, len(token_list)):\n    n_gram_sequence = token_list[:i+1]\n    ng.append(n_gram_sequence)\n  return ng","68b39d35":"df.iloc[[0]].lyrics.iloc[0]","9d5b20b7":"def seqform(data):\n  \n  tokenise = Tokenizer()\n  input_sequences = []\n  corpus = []\n  k=0\n\n  for i in range(0,len(df)):\n    \n   text = df.iloc[[i]].lyrics.iloc[0]\n   if type(text)==float:\n     pass\n   else:\n     text = text.lower().split(\"\\n\")\n     text = [re.sub(r'\\d+', '', i) for i in text]\n     text = list(set(text))\n     if text==' ':\n       pass\n     else:\n       corpus.extend(text)\n     k+=1\n  \n  tokenise.fit_on_texts(corpus)\n  for line in corpus:\n    token_list = tokenise.texts_to_sequences([line])[0]\n    input_sequences.extend(ngram(token_list))\n \n  \n  max_sequence_len = max([len(x) for x in input_sequences])\n  input_sequences = np.array(pad_sequences(input_sequences,\n                       maxlen = max_sequence_len, padding='pre'))\n  \n  predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n  fin_data = pd.DataFrame(np.hstack((predictors, label.reshape(-1,1))),columns=np.hstack((np.arange(1,predictors.shape[1]+1),np.array(['label']))))\n  total_words = len(tokenise.word_index) + 1\n  print('{} number of lyrics inputted'.format(k))\n\n  return fin_data,tokenise,max_sequence_len,total_words,predictors,label\n  ","7f7235c6":"fdf,tokenise,max_sequence_len,total_words,predictors,label = seqform(df)\nprint(fdf.shape,max_sequence_len,total_words)","9cd6d857":"fdf.head(10)","6df129e5":"fdf .to_csv('fin_df.csv',index=False)","ed5e4fa0":"print(fdf.shape)\nprint(fdf.values.max())","a7c21ef8":"dataX = [fdf.iloc[i,0:87].tolist() for i in range(0,fdf.shape[0])]\ndataY = [fdf.iloc[i,87] for i in range(0,fdf.shape[0])]\nprint(len(dataX))\nprint(len(dataY))","2fbe5e2d":"# reshape X to be [samples, time steps, features]\nX = np.reshape(dataX, (13839, 87, 1))\n\n# one hot encode the output variable\ny = np_utils.to_categorical(dataY)","9cce45de":"len(y[0])","f4ba65eb":"print(predictors.shape)\nprint(label.shape)\nprint(X.shape,y.shape)\nprint(total_words,max_sequence_len)","12361146":"model = Sequential()\nmodel.add(Embedding(total_words, 150, input_length=max_sequence_len-1))\n# Add an LSTM Layer\nmodel.add(Bidirectional(LSTM(150, return_sequences=True)))  \n# A dropout layer for regularisation\nmodel.add(Dropout(0.2))\n# Add another LSTM Layer\nmodel.add(LSTM(100)) \nmodel.add(Dense(total_words\/2, activation='relu'))  \n# In the last layer, the shape should be equal to the total number of words present in our corpus\nmodel.add(Dense(y.shape[1], activation='softmax'))\n#model.add(Dense(total_words, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')  #(# Pick a loss function and an optimizer)\nprint(model.summary())","2290d142":"# define the checkpoint\nfilepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\ncallbacks_list = [checkpoint]","62bb8776":"model.fit(X,y, epochs= 60,callbacks=callbacks_list)\n#model.fit(predictors, label, epochs=100, callbacks=callbacks_list)","f90d4c08":"#37-4.9734\nfilename = \"weights-improvement-50-0.8289.hdf5\"\nmodel.load_weights(filename)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam',metrics='accuracy')\nmodel.fit(X, y, epochs=20, callbacks=callbacks_list)","8a54fbb1":"model.save_weights(Data_dir+'my_model_weights.h5')\nmodel.save(Data_dir+'my_model.h5')","e2997746":"model.save(Data_dir + 'saved_model\/my_model')","ba66c9f3":"def make_lyrics(seed_text, next_words):\n    pred_index=[]\n    for i in range(next_words):\n        token_list = tokenise.texts_to_sequences([seed_text])[0]\n        token_list = pad_sequences([token_list],\n                     maxlen=max_sequence_len-1,padding='pre')\n        #print(token_list.shape)\n        token_list = np.reshape(token_list, (1, max_sequence_len-1, 1))\n        predicted = model.predict(token_list, verbose=0)\n        predicted_index =  np.argmax(predicted)\n        pred_index.append(predicted_index)\n        \n\n\n        #predicted_index=1\n        output_word = \"\"\n        for word, index in tokenise.word_index.items():\n            if index == predicted_index:\n                output_word = word\n                break\n        seed_text += \" \" + output_word\n    print(seed_text)\n    return seed_text","fe55840f":"reverse_word_map = dict(map(reversed, tokenise.word_index.items()))\nstart = np.random.randint(0, len(dataX)-1)\npattern = dataX[start]\npattern_val = [i for i in pattern if i>0]\nprint(\"Seed:\")\nprint(' '.join([reverse_word_map.get(value) for value in pattern_val]))\nseed_text = [reverse_word_map.get(value)+' ' for value in pattern_val]","ce561399":"import pickle\n\n# saving\nwith open(Data_dir+'tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenise, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n# loading\n#with open('tokenizer.pickle', 'rb') as handle:\n#    tokenizer = pickle.load(handle)","af508b60":"## pink floyd lyrics generator","7d752519":"## LSTM","aabaa982":"![Screenshot%20%28126%29.png](attachment:Screenshot%20%28126%29.png)"}}