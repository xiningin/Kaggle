{"cell_type":{"efce6062":"code","cae1852b":"code","6c119b01":"code","456b244e":"code","dc695f9b":"code","e50616fe":"code","c3e3f4c3":"code","23eec446":"code","01dbf4c9":"code","57f8b9a5":"code","f7b169ff":"code","3bf886e8":"code","33ae13d1":"code","8fa49fee":"code","4c69972c":"code","cf2818df":"code","6a583c54":"code","e47b5fac":"code","e01997f6":"code","a9eae753":"code","b87eb9f3":"code","9cec29e4":"code","6242c720":"code","b7883d40":"code","f4fbe3bf":"code","ea833952":"code","40640271":"code","eeb7f184":"code","e7447c7c":"code","fc1b0ae4":"code","f066bf11":"code","7ed4b667":"code","8456408b":"code","7e5696b4":"code","519a9d34":"code","c6ca2045":"code","0df54d66":"code","79948243":"code","fc3a0842":"code","adeac8b7":"markdown","d78ff1c3":"markdown","2f4d3e92":"markdown","f43c898c":"markdown","23ae48ce":"markdown","0356955d":"markdown","1621955f":"markdown","3e8700ca":"markdown","9728d126":"markdown","b28e17b4":"markdown","76fcb93a":"markdown","88e1347f":"markdown","1b428bf3":"markdown","7545db00":"markdown","47f87b3e":"markdown","bd3aeaea":"markdown","0b0b3ad2":"markdown","3af1d0b1":"markdown","1e5d0873":"markdown","ce94ec63":"markdown","8b806696":"markdown"},"source":{"efce6062":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","cae1852b":"df_file = pd.read_csv(\"\/kaggle\/input\/programming-assignment-linear-regression\/ex1data1.csv\")\ndf_file.head(3) # first 3 rows of our data","6c119b01":"def init_2d_graphs(*colors):\n    '''\n        Just graph initialize in good way\n    '''\n    plt.style.use(colors) # color of your 2d graph\n    plt.figure(figsize=(10,6)) # set the figure size\n    return True","456b244e":"# our initialized and graph draw for our dataset\nx = df_file['city_population'] # x_axis\ny = df_file['food_truck_profit'] # y_axis\n\n# reshape y to be (n,m) of rank-1\ny = y.values.reshape(len(y),1) # because of type Series we use .values\n\ninit_2d_graphs('ggplot', 'dark_background' )\n\nplt.scatter(x,y, s = 300, c = 'red', marker = 'P', label =  'Training data')\nplt.title(\"The Relation between \" + 'Population of City in 10,000s' + \" And \" + 'Profit in $10,000s' )\nplt.legend()\nplt.xlabel('Population of City in 10,000s')\nplt.ylabel('Profit in $10,000s')","dc695f9b":"df_file.describe()","e50616fe":"Image(\"..\/input\/some-helpful-images\/hypothesis_linear.png\")\n","c3e3f4c3":"Image(\"..\/input\/some-helpful-images\/paramters_updated.png\")","23eec446":"m = len(x) \nprint(\"Number of training example: \", m)\nprint(\"#\" * 70)\nAlpha = .01 # learning rate\niterations = 1500 # number of gradient descent iterations\n\nthetas = np.zeros((2,1)) # initialize threats as 2d array and 2*1 dimension with 0 values\nprint('Theats shape is: ', thetas.shape)\nprint(\"#\" * 70)\nprint('Theats values are: ', thetas)\nprint(\"#\" * 70)\n\n# add columns of 1 which is x0 \nX = np.stack((x, np.ones(m)), axis=1) # create x0 = 1 for each example\nprint(\"Now X shape is: \", X.shape)","01dbf4c9":"def cost_function(thetas,x,m,y):\n    '''\n    Arguments:\n        thetas the paramter we need to minimize of shape 2*1\n        x the features of our dataset 97*2\n        m number of training examples\n        y is output we need to predict\n    return:\n        cost function as total squared cost of our predicted values h_x and the real values y\n    '''\n# y_hat = theta0 * x0 + theta1 * x1 and with vectorized will be x = 97*2 * theta =  2*1\n    y_hat = np.matmul(x,thetas) # hypothesis function\n\n# get the cost function\n    cost_function = (1\/(2*m)) * np.sum(np.square(y_hat - y))\n    return cost_function","57f8b9a5":"J = cost_function(thetas, X, m, y)\nprint(\"The cost funtion of our training data is: \", J)","f7b169ff":"Image(\"..\/input\/some-helpful-images\/paramters_updated.png\")","3bf886e8":"def gradient_descent(thetas,x,m,y, learning_rate):\n    '''\n    Arguments:\n        thetas the paramter we need to minimize of shape 2*1\n        x the eatures of our dataset 97*2\n        m number of training examples\n        y is output we need to predict\n        learning rate is alpha which inilized above as .01\n    return:\n        cost function as total squared cost of our predicted values h_x and the real values y\n    '''\n# y_hat = theta0 * x0 + theta1 * x1 and with vectorized will be x = 97*2 * theta =  2*1\n    y_hat = np.matmul(x,thetas)\n\n# y_hat = 97*1 - y = 97*1 will be 97*1 and multiply by x which 97*2 so need to transpose to be 1*97 and x 97*2\n    cost = np.matmul(np.transpose(y_hat-y), x)\n    grad =((learning_rate\/m) * cost)\n    \n# return the gradient but transposed to be 2*1 instead of 1*2 to that maps to theta dimensions\n    return grad.T","33ae13d1":"grad = gradient_descent(thetas, X, m, y, Alpha)\nprint(\"instead of Thetas as zero now thetas paramters after just 1 iteration is: \", grad)","8fa49fee":"def linear_regression_model(thetas, x, m, y, learning_rate, num_of_iterations):\n    '''\n    Arguments:\n        thetas the paramter we need to minimize of shape 2*1\n        x the eatures of our dataset 97*2\n        m number of training examples\n        y is output we need to predict\n        learning rate is alpha which inilized above as .01\n        num_of_iterations you need to minimize the cost function\n    return:\n        cost function as total squared cost of our predicted values h_x and the real values y\n    '''\n    costs = []\n    all_theta = []\n    for i in range(num_of_iterations):\n        J = cost_function(thetas, x, m, y)\n        all_theta.append(thetas)\n        costs.append(J)\n# get new values of theta as gradient descent step\n        grad = gradient_descent(thetas, x, m , y, learning_rate)\n\n# update theta so if grad is negative the theta will increase otherwise will decrease\n        thetas = thetas - grad\n\n    return costs, thetas, all_theta","4c69972c":"all_cost, last_thetas, all_theta = linear_regression_model(thetas, X, m, y, Alpha, iterations)","cf2818df":"J = cost_function(last_thetas, X, m, y)\nprint(\"Our cost function after 1500 iterations is: \", J)","6a583c54":"predict1 = np.abs(np.matmul([1, 3.5],last_thetas))\npredict2 = np.abs(np.matmul([1, 7],last_thetas))\nprint(\"Our Prediction 1\", predict1)\nprint(\"Second Prediction\", predict2)","e47b5fac":"# Plot the graph with first 4 values of thetas and last values of thetas\n\ninit_2d_graphs('ggplot', 'dark_background' ) # initialize graphics size\nfor i in range(4):\n    y_hat = np.matmul(X, all_theta[i])\n    plt.plot(x, y_hat, label='predict ' + str(i+1), linewidth=3)\ny_hat = np.matmul(X, last_thetas)\nplt.plot(x, y_hat, label= 'last predict',linewidth=2)\nplt.scatter(x,y, s = 200, c = 'yellow', marker = 'X', label =  'Training data')\nplt.title(\"The Relation between \" + 'Population of City in 10,000s' + \" And \" + 'Profit in $10,000s' )\nplt.legend()\nplt.xlabel('Population of City in 10,000s')\nplt.ylabel('Profit in $10,000s')\n","e01997f6":"init_2d_graphs() # initialize graphics size\nplt.plot(np.arange(iterations), all_cost, 'r', label='Cost Functions', linewidth=2) \nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Cost Function\")\nplt.title('Error vs. Training Iterations')\nplt.legend()","a9eae753":"df_file = pd.read_csv(\"\/kaggle\/input\/programming-assignment-linear-regression\/ex1data2.csv\")\n#now you can see the data after convert to csv with columns name\ndf_file.head()","b87eb9f3":"df_file.describe()","9cec29e4":"Image(\"..\/input\/some-helpful-images\/nan.png\")","6242c720":"def features_normalization_with_std(X):\n    '''\n        Normalize the data via standard deviation\n    '''\n    X= (X - np.mean(X)) \/ np.std(X)\n    return X","b7883d40":"def features_normalization_with_min_max(X):\n    '''\n        Normalize the data via min max approach\n    '''\n    X = (X - np.mean(X)) \/ (np.max(X) - np.min(X))\n    return X","f4fbe3bf":"df_file = features_normalization_with_std(df_file)","ea833952":"x = np.array(df_file.iloc[:, :2])# get the 2 features columns)\ny = df_file['house_price'] # the real output \n\n# # because y is shape (97,) whcih rank of 0 and we need to be (97,1) to subtract from y_hat\ny = y.values.reshape(len(y),1) # because of type Series we use .values\n\n# variables and parameters initialize\nall_cost = []\nm = len(y) \nprint(x.shape)\nprint(\"#\"*80)\nprint(\"Number of training example: \", m)\nprint(\"#\"*80)\nAlpha = .1 # learning rate\niterations = 100 # number of gradient descent iterations\nthetas = np.zeros((3,1)) # initialize threats as 2d array and 2*1 dimension with 0 values\nprint('Theats shape is: ', thetas.shape)\nprint(\"#\"*80)\nprint('Theats values are: ', thetas)\nprint(\"#\"*80)\nX = np.column_stack((x,np.ones(len(y))))\nprint(\"Now X shape is: \", X.shape)","40640271":"print(\"Now the first 5 rows of x values are: \", X[:5, :])","eeb7f184":"J = cost_function(thetas, X, m, y)\nprint(\"The cost funtion after data scaling is: \", J)","e7447c7c":"grad = gradient_descent(thetas, X, m, y, Alpha)\nprint(\"instead of Thetas as zero now thetas paramters after just 1 iteration is: \", grad)","fc1b0ae4":"all_cost, last_thetas, all_theta = linear_regression_model(grad, X, m, y, Alpha, iterations)","f066bf11":"J = cost_function(last_thetas, X, m, y)\nprint(\"Our cost function after 1000 iterations without feature scaling: \", J)","7ed4b667":"init_2d_graphs() # initialize graphics size\n#np.arange(iters) means from 0 to 1500 iterations\nplt.plot(np.arange(iterations), all_cost, 'r', label='Cost Functions', linewidth=2) \nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Cost Function\")\nplt.title('Error vs. Training Iterations')\nplt.legend()","8456408b":"df_file = pd.read_csv(\"\/kaggle\/input\/programming-assignment-linear-regression\/ex1data2.csv\")\ndf_file.head()","7e5696b4":"df_file = features_normalization_with_min_max(df_file)","519a9d34":"x = np.array(df_file.iloc[:, :2])# get the 2 features columns)\ny = df_file['house_price'] # the real output \n\n# # because y is shape (97,) whcih rank of 0 and we need to be (97,1) to subtract from y_hat\ny = y.values.reshape(len(y),1) # because of type Series we use .values\n\n# variables and parameters initialize\nall_cost = []\nm = len(y) \nprint(x.shape)\nprint(\"#\"*80)\nprint(\"Number of training example: \", m)\nprint(\"#\"*80)\nAlpha = .1 # learning rate\niterations = 100 # number of gradient descent iterations\nthetas = np.zeros((3,1)) # initialize threats as 2d array and 2*1 dimension with 0 values\nprint('Theats shape is: ', thetas.shape)\nprint(\"#\"*80)\nprint('Theats values are: ', thetas)\nprint(\"#\"*80)\nX = np.column_stack((x,np.ones(len(y))))\nprint(\"Now X shape is: \", X.shape)","c6ca2045":"J = cost_function(thetas, X, m, y)\nprint(\"The cost funtion after data scaling is: \", J)\ngrad = gradient_descent(thetas, X, m, y, Alpha)\nprint(\"instead of Thetas as zero now thetas paramters after just 1 iteration is: \", grad)","0df54d66":"all_cost, last_thetas, all_theta = linear_regression_model(grad, X, m, y, Alpha, iterations)","79948243":"J = cost_function(last_thetas, X, m, y)\nprint(\"Our cost function after 1000 iterations without feature scaling: \", J)","fc3a0842":"init_2d_graphs() # initialize graphics size\n#np.arange(iters) means from 0 to 1500 iterations\nplt.plot(np.arange(iterations), all_cost, 'r', label='Cost Functions', linewidth=2) \nplt.xlabel(\"Iterations\")\nplt.ylabel(\"Cost Function\")\nplt.title('Error vs. Training Iterations')\nplt.legend()","adeac8b7":"## call the Cost Function","d78ff1c3":"**Two function implemented for feature scaling choose any of them**","2f4d3e92":"## call the gradient descent","f43c898c":"## Another with features_normalization_with_min_max","23ae48ce":"##  call the linear_regression_model for multiple of iterations","0356955d":"## Graphs of different fitting line at different values of parameters","1621955f":"##  Cost function graph","3e8700ca":"###  Gradient Descent\n\n## Implementation Steps\n\nfirst we have x and we need to map it to y\n- x = Population of City in 10,000s\n- y = Profit in $10,000s\n\n**Hypothesis**\n\n- second the hypothesis function is y_hat = theta0 + theta1 * X1 and we initialize X0 = 1 which have no effect on theta0:","9728d126":"##  Cost Function\n\n\n**theta is 2 * 1 and X is 97 * 2 so we can multiply X*theta and get 97 * 1**","b28e17b4":"**NOTE !!**\n\nIts usful to undrstand that graident descent work as optimization Algorithm that for each number of iteration looks at you cost function and for each iteration trying to minimize it by set the thetas paramters that help fitting the best line to the data.","76fcb93a":"## features normalization or Data Scaling\n\n**Its important step to make the values of different features within spceific range because it help you in:**\n\n- Avoid NAN values because numbers in operations of multiplication\n- its help the machine to deal with numbers within range than different ranges and the operations be less cost\n\n**try to comment the line of calling function features_normalization_with_std and see result of how its affect.**\n","88e1347f":"### Note\n**as you can see in graph above its will be fine to stop after 400 or 600 iterations because it has a small decrease**\n\n# Now it's time for multiple variables of Linear Regression\n\nIn this part, we will implement linear regression with multiple variables to predict  the  prices  of  houses.\n\n**The same process done as above but because of its now has more than one features so ignore some graphs because of 3d dimension**","1b428bf3":"##  Read the csv file","7545db00":"## 1.8 Gradient descent\n\nNext, we will implement gradient descent.\n\n### Remember the  Gradient descent is:\n","47f87b3e":"## parameters initialize","bd3aeaea":"#  Introduction \n\n### The task realted to implement linear regression with one variable to predict profits for a food truck.\n\n### This Task is related to Coursera Machine Learning Course by Andrew NG, but implemnted in Python.\n\n**Most text used in this notebook from ex1.pdf of Coursera**\n\n**Look at ex1.pdf to get more intuition about the task**\n\n**The task will be implemented in three ways and three notebooks and it all about linear regression**\n\n- As manual code which pure code.\n- Using Sklearn library\n- Using Tensflow & Keras\n\n###  linear regression with one variable\n\nIn this part of this exercise,  we will implement linear regression with one variable to predict profits for a food truck.\n\n## Most of code is written to be clean and enhancing with functions","0b0b3ad2":"##  Importing libraries","3af1d0b1":"## Cost function graph\n\nafter we see how the fitting line on our data its usful to see how cost function decreased with differnt steps of gradient descent.","1e5d0873":"### some static of our data","ce94ec63":"###   Plotting the Data\n\nBefore  starting  on  any  task,  it  is  often  useful  to  understand  the  data  by visualizing it. But always its complicated because number of features you have so here we just has one feature so its 2d graph.\n\n### Graph initialize","8b806696":"- so we need to inilize these theats.\n\n**Cost function**\n\n\n- Third the cost function is use m as training example so we need to get the number of our training examples.\n\n**Gradient Steps**\n\n- Fourth in gradient descent there is another parameters alpha we need to initialize which the learning rate.\n\n- Fifth and the last we need to specify the number of iteration will used to iterate and update the parameters:\n\n"}}