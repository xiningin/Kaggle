{"cell_type":{"8c43da14":"code","f9d219ea":"code","6746dc59":"code","568f61d6":"code","e735f525":"code","6158d412":"code","54f0a330":"code","76c9dc46":"code","1cf74c61":"code","eb739960":"code","a4641f09":"code","ee53e061":"code","34c89fa7":"markdown","664bd93d":"markdown"},"source":{"8c43da14":"from fastai.vision.all import *\nimport librosa as librosa","f9d219ea":"def chunk_to_spec(chunk, SPEC_HEIGHT=64,SPEC_WIDTH=256, rate=32000, FMIN=200, FMAX=12500):\n    mel_spec = librosa.feature.melspectrogram(y=chunk, \n                                              sr=32000, \n                                              n_fft=1024, \n                                              hop_length=int(32000 * 5 \/ (SPEC_WIDTH - 1)), \n                                              n_mels=SPEC_HEIGHT, \n                                              fmin=FMIN, \n                                              fmax=FMAX)\n    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n    return mel_spec_db","6746dc59":"df = pd.read_csv('..\/input\/peak-identification\/info_df.csv')\nprint(df.shape)\ndf.head(3)","568f61d6":"fn, l, y, peaks = df.sample().values[0]\nstart_time = min(l\/32000 - 5, max(float(peaks.split('#')[0])-2.5, 2.5))\ny, sr = librosa.load(fn, sr=32000, offset=start_time, duration=5)\nplt.imshow(chunk_to_spec(y, SPEC_HEIGHT=128), cmap='inferno')","e735f525":"class TitledImage(fastuple):\n    def show(self, ctx=None, **kwargs): show_titled_image(self, ctx=ctx, **kwargs)\n\nclass ClipTransform(ItemTransform):\n\n    def __init__(self, df):\n        self.df=df\n        self.vocab,self.o2i = uniqueify(df['label'], sort=True, bidir=True)\n        \n    def encodes(self, i, from_np=False):\n        f, l, label, peaks = df.iloc[i].values\n        clip_num = random.choice([0, 0, 0, 1, 1, 2, 3, 4, 5, 6]) # More prob chose big peak\n        start_time = min(l\/32000 - 5, max(float(peaks.split('#')[clip_num])-2.5, 2.5))\n        y, sr = librosa.load(f, sr=32000, offset=start_time, duration=5)\n        spec = chunk_to_spec(y,SPEC_HEIGHT=112,SPEC_WIDTH=224)\n        spec -= np.min(spec) \n        spec \/= 80 # np.max(spec) # Normalize\n        spec =  torch.unsqueeze(tensor(spec), 0)\n        spec = torch.cat([spec, spec, spec]) # Stack three channels to simulate RGB if using a pretrained model\n        return spec, self.o2i[label]\n    \n    def decodes(self, x):\n        return TitledImage(x[0],self.vocab[x[1]])\n\n\ndf_small = df\nclip_tfm = ClipTransform(df)\ntrain =  df_small.sample(frac=0.8)\ntrain_idx, valid_idx = list(train.index), df_small[~df_small.index.isin(train.index)].index\nprint('train and val size', len(train_idx), len(valid_idx))\ntrain_tl= TfmdLists(train_idx, clip_tfm)\nvalid_tl= TfmdLists(valid_idx, clip_tfm)\ndls = DataLoaders.from_dsets(train_tl, valid_tl, bs=16)\ndls = dls.cuda()\nxb, yb = dls.one_batch()\nprint(xb.shape)\ndls.show_batch(max_n=3)","6158d412":"learn = cnn_learner(dls, models.resnet18, loss_func=FocalLossFlat(), metrics=[accuracy], cbs=[ShowGraphCallback(), CSVLogger()])","54f0a330":"print('Model created and ready for training')","76c9dc46":"learn.unfreeze()\nlearn.fit_one_cycle(3, slice(1e-4, 1e-3))","1cf74c61":"learn.recorder.plot_loss()\nplt.savefig('loss_plot.png')","eb739960":"learn.save('stage-1')","a4641f09":"learn.remove_cb(CSVLogger) # Not pickleable\nlearn.export('baseline_3e.pkl')","ee53e061":"# !pip install -q wandb\n# import wandb\n# wandb.init()\n# from fastai.callback.wandb import *\n# # And add cbs=WandbCallback() to log","34c89fa7":"# DataLoaders","664bd93d":"# Loading Specs"}}