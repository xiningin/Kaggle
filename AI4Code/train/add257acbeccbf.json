{"cell_type":{"9b9fa328":"code","3e919936":"code","aee6c779":"code","1ff9d9c9":"markdown","64eef780":"markdown","5c1e5d2c":"markdown","f40b1b8a":"markdown"},"source":{"9b9fa328":"import numpy as np\nimport matplotlib.pyplot as plt\n\nlines = np.loadtxt('..\/input\/data.csv', delimiter=',', dtype='str')\nx_total = lines[:, 1:3].astype('float')\ny_total = lines[:, 3].astype('float')\n\npos_index = np.where(y_total == 1)\nneg_index = np.where(y_total == 0)\nplt.scatter(x_total[pos_index, 0], x_total[pos_index, 1], marker='o', c='r')\nplt.scatter(x_total[neg_index, 0], x_total[neg_index, 1], marker='x', c='b')\nplt.show()\nprint('Data set size:', x_total.shape[0])","3e919936":"from sklearn import linear_model\n\nlr_clf = linear_model.LogisticRegression()\nlr_clf.fit(x_total, y_total)\nprint(lr_clf.coef_[0])\nprint(lr_clf.intercept_)\n\ny_pred = lr_clf.predict(x_total)\nprint('accuracy:',(y_pred == y_total).mean())\n\nplot_x = np.linspace(-1.0, 1.0, 100)\nplot_y = - (lr_clf.coef_[0][0] * plot_x + lr_clf.intercept_[0]) \/ lr_clf.coef_[0][1]\nplt.scatter(x_total[pos_index, 0], x_total[pos_index, 1], marker='o', c='r')\nplt.scatter(x_total[neg_index, 0], x_total[neg_index, 1], marker='x', c='b')\nplt.plot(plot_x, plot_y, c='g')\nplt.show()","aee6c779":"# 1. finish function my_logistic_regression;\n# 2. draw a training curve (the x-axis represents the number of training iterations, and the y-axis represents the training loss for each round);\n# 3. draw a pic to show the result of logistic regression (just like the pic in section 2);\n\n\nn_iterations = 2000\nlearning_rate = 0.1\nloss_list = []\n\ndef sigmoid(x):\n    # Activation function used to map any real value between 0 and 1\n    return 1 \/ (1 + np.exp(-x))\n\ndef my_logistic_regression(x_total, y_total):\n    # TODO\n    constant_x = np.ones((100,1))\n    x_total = np.c_[constant_x,x_total]\n    x_total = np.mat(x_total)\n    y_total = np.mat(y_total).transpose()\n    \n    # a = number of training examples\n    # b = number of features\n    a,b = x_total.shape\n    weights = np.ones((b,1))\n    \n    j_cost = np.zeros((a,1))\n    j_cost_sum = np.zeros((n_iterations,1))\n    \n    for i in range(n_iterations):\n        hx = sigmoid(x_total * weights)\n        # compute cost for given parameters\n        j_cost = -np.multiply(y_total,np.log10(hx)) - np.multiply((1-y_total),np.log10(1-hx))\n        j_cost_sum[i] = sum(j_cost)\n        # calculate error\n        error = y_total - hx\n        # update weights\n        weights = weights + learning_rate * x_total.transpose() * error\n    \n    plt.plot(range(1, n_iterations + 1), j_cost_sum)\n    plt.xlabel('Training Iterations')\n    plt.ylabel('Training Loss')\n    plt.title('Logistic Regression Training Curve')\n    plt.show()\n    \n    return weights\n\ndef accuracy(weights, x_total, y_total, prob_threshold=0.5):\n    constant_x = np.ones((100,1))\n    x_total = np.c_[constant_x,x_total]\n    a,b = x_total.shape\n    count = 0\n    \n    for i in range(a):\n        predict = sigmoid(x_total[i, :] * weights)[0, 0] >= prob_threshold\n        if predict == bool(y_total[i]):\n            count += 1\n    acc = float(count) \/ a\n    \n    return acc\n\ny_pred = my_logistic_regression(x_total, y_total)\nprint('accuracy:',accuracy(y_pred, x_total, y_total))\n\nx_values = np.linspace(-1.0, 1.0, 100)\ny_values = (-y_pred[0,0] - y_pred[1,0] * x_values) \/ y_pred[2,0]\nplt.scatter(x_total[pos_index, 0], x_total[pos_index, 1], marker='o', c='r')\nplt.scatter(x_total[neg_index, 0], x_total[neg_index, 1], marker='x', c='b')\nplt.plot(x_values, y_values, c='g')\nplt.title('Logistic Regression - Gradient Descent')\nplt.show()","1ff9d9c9":"## Step 1: Data Preparation","64eef780":"## Step 3: Gradient Descent(TO DO)\n$$\\frac{\\partial}{\\partial w_1}L(w,b)=\\frac{1}{N}\\sum\\limits_{i=1}^{N}(f(x^{(i)})-y^{(i)})x_1^{(i)}$$\n$$\\frac{\\partial}{\\partial b}L(w,b)=\\frac{1}{N}\\sum\\limits_{i=1}^{N}f(x^{(i)})-y^{(i)}$$","5c1e5d2c":"# Logistic Regression\nThis is the second homework of EE448. In this homework, you need to be familiar with and independently write the calculation process of logistic regression.\n+ Task: Binary classification problem\n+ Input: Two-dimensional feature\n+ Label: 1 and 0","f40b1b8a":"## Step 2: Sklearn"}}