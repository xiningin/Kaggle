{"cell_type":{"0a2f81eb":"code","8be0dde2":"code","8a2facdd":"code","535dbd91":"code","51b0fef2":"code","5706a1c6":"code","79614f5b":"code","9144993e":"code","eea7a4f5":"code","d710118f":"code","2837ecd7":"code","845e5b72":"code","55e25ec8":"code","b9fc46ea":"code","9957a058":"code","547bb6b2":"code","15aa2093":"code","35fef1b3":"code","91bde13e":"code","a0ecc440":"code","fb84f0c8":"code","42fba4ec":"code","dc6adda8":"code","42e577c3":"code","00aa0371":"code","c5c8ee12":"markdown","dc7bebbb":"markdown","edfc5167":"markdown","eda1ca02":"markdown","f7dd128c":"markdown","5d0982dd":"markdown","a65038af":"markdown","a2f74416":"markdown","292bdbca":"markdown","e00b2b2c":"markdown","f65596f3":"markdown","934cad34":"markdown","5981134a":"markdown","853428c7":"markdown"},"source":{"0a2f81eb":"import subprocess","8be0dde2":"CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"])]\nprint(\"CUDA version:\", CUDA_version)","8a2facdd":"if CUDA_version == \"10.0\":\n    torch_version_suffix = \"+cu100\"\nelif CUDA_version == \"10.1\":\n    torch_version_suffix = \"+cu101\"\nelif CUDA_version == \"10.2\":\n    torch_version_suffix = \"\"\nelse: \n    torch_version_suffix = \"+cu110\"\n!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https:\/\/download.pytorch.org\/whl\/torch_stable.html ftfy regex","535dbd91":"import numpy as np\nimport torch\nimport os\n\nprint(\"Torch version:\", torch.__version__)\n# os.kill(os.getpid(),9)","51b0fef2":"!pip install gdown","5706a1c6":"# clone the CLIP repository\n!git clone https:\/\/github.com\/openai\/CLIP.git\n\nimport sys\nfrom pathlib import Path\n\nclip_dir = Path(\".\").absolute() \/ \"CLIP\"\nsys.path.append(str(clip_dir))\nprint(f\"CLIP dir is: {clip_dir}\")\n\nimport clip","79614f5b":"# Load pre-trained model\nimport os\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, transform = clip.load(\"ViT-B\/32\", device=device)\nprint(f\"Model dir: {os.path.expanduser('~\/.cache\/clip')}\")","9144993e":"!gdown https:\/\/drive.google.com\/uc?id=1sGcW7XsiwvnuRMbeIeQJZCeM9D6R6m_O\n!unzip clip_people.zip; rm clip_people.zip","eea7a4f5":"import os\n# images we want to test are stored in folders with class names\nclass_names = sorted(os.listdir('.\/clip_people\/'))\nclass_to_idx = {class_names[i]: i for i in range(len(class_names))}\nclass_names","d710118f":"class_captions = [f\"An image depicting a {x}\" for x in class_names]\nclass_captions","2837ecd7":"text_input = clip.tokenize(class_captions).to(device)\nprint(f\"Tokens shape: {text_input.shape}\")\n\nwith torch.no_grad():\n    text_features = model.encode_text(text_input).float()\n    text_features \/= text_features.norm(dim=-1, keepdim=True)\nprint(f\"Text features shape: {text_features.shape}\")","845e5b72":"# In order to display the image we will need to de-nonrmalize them\nimage_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073]).to('cpu')\nimage_std = torch.tensor([0.26862954, 0.26130258, 0.27577711]).to('cpu')\n\ndef denormalize_image(image: torch.Tensor) -> torch.Tensor:\n    image *= image_std[:, None, None]    \n    image += image_mean[:, None, None]\n    return image","55e25ec8":"import matplotlib.pyplot as plt\nfrom PIL import Image\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader","b9fc46ea":"dataset = ImageFolder(root=\".\/clip_people\", transform=transform)\ndata_batches = DataLoader(dataset, batch_size=len(dataset), shuffle=False)","9957a058":"plt.figure(figsize=(10, 10))\n\n# Show all images from the dataset since our dataset is small\nfor idx, (image, label_idx) in enumerate(dataset):\n    cur_class = class_names[label_idx]    \n    \n    plt.subplot(4, 4, idx+1)\n    plt.imshow(denormalize_image(image).permute(1, 2, 0))\n    plt.title(f\"{cur_class}\")\n    plt.xticks([])\n    plt.yticks([])\n\nplt.tight_layout()","547bb6b2":"# read out all images and true labels\nimage_input, y_true = next(iter(data_batches))\nimage_input = image_input.to(device)\n\nwith torch.no_grad():\n    image_features = model.encode_image(image_input).float()","15aa2093":"def show_results(image_features, text_features, class_names):\n    # depends on global var dataset\n\n    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n    k = np.min([len(class_names), 5])\n    # top_probs, top_labels = text_probs.cpu().topk(k, dim=-1)\n    text_probs = text_probs.cpu()\n\n    plt.figure(figsize=(26, 16))\n\n    for i, (image, label_idx) in enumerate(dataset):\n        plt.subplot(4, 8, 2 * i + 1)\n        plt.imshow(denormalize_image(image).permute(1, 2, 0))\n        plt.axis(\"off\")\n\n        plt.subplot(4, 8, 2 * i + 2)\n        y = np.arange(k)\n        plt.grid()\n        plt.barh(y, text_probs[i])\n        plt.gca().invert_yaxis()\n        plt.gca().set_axisbelow(True)\n        # plt.yticks(y, [class_names[index] for index in top_labels[i].numpy()])\n        plt.yticks(y, class_names)\n        plt.xlabel(\"probability\")\n\n    plt.subplots_adjust(wspace=0.5)\n    plt.show()    ","35fef1b3":"show_results(image_features, text_features, class_names)","91bde13e":"class_names = ['James', 'Olivia', 'Emma']\nclass_captions = [f\"An image depicting a {x}\" for x in class_names]\ntext_input = clip.tokenize(class_captions).to(device)\n\nwith torch.no_grad():\n    text_features = model.encode_text(text_input).float()\n    text_features \/= text_features.norm(dim=-1, keepdim=True)\n\nshow_results(image_features, text_features, class_names)","a0ecc440":"class_names = ['male', 'female']\nclass_captions = [f\"An image depicting a {x}\" for x in class_names]\ntext_input = clip.tokenize(class_captions).to(device)\n\nwith torch.no_grad():\n    text_features = model.encode_text(text_input).float()\n    text_features \/= text_features.norm(dim=-1, keepdim=True)\n\nshow_results(image_features, text_features, class_names)","fb84f0c8":"class_names = ['microphone', 'chair', 'watch', 'glasses']\nclass_captions = [f\"An image showing a {x}\" for x in class_names]\ntext_input = clip.tokenize(class_captions).to(device)\n\nwith torch.no_grad():\n    text_features = model.encode_text(text_input).float()\n    text_features \/= text_features.norm(dim=-1, keepdim=True)\n\nshow_results(image_features, text_features, class_names)","42fba4ec":"class_names = ['microphone', 'two_microphones', \"else\"]\nclass_captions = [\"An image showing a microphone\", \"An image showing two microphones\", \"An image showing something else\"]\ntext_input = clip.tokenize(class_captions).to(device)\n\nwith torch.no_grad():\n    text_features = model.encode_text(text_input).float()\n    text_features \/= text_features.norm(dim=-1, keepdim=True)\n\nshow_results(image_features, text_features, class_names)","dc6adda8":"class_names = ['microphone', 'two_microphones', \"else\"]\nclass_captions = [\"An image showing a microphone\", \"An image showing 2 microphones\", \"An image showing something else\"]\ntext_input = clip.tokenize(class_captions).to(device)\n\nwith torch.no_grad():\n    text_features = model.encode_text(text_input).float()\n    text_features \/= text_features.norm(dim=-1, keepdim=True)\n\nshow_results(image_features, text_features, class_names)","42e577c3":"class_names = ['Joe', 'Sophia', 'Anna']\nclass_captions = [f\"A photo of {x}\" for x in class_names]\ntext_input = clip.tokenize(class_captions).to(device)\n\nwith torch.no_grad():\n    text_features = model.encode_text(text_input).float()\n    text_features \/= text_features.norm(dim=-1, keepdim=True)\n\nshow_results(image_features, text_features, class_names)","00aa0371":"class_names = [\"man\", \"Sophia\", \"Anna\", \"Anna+hat\"]\nclass_captions = [\"A photo of a man\", \"A photo of Sophia\", \"A photo of a Anna\", \"A photo of a Anna with head covered\"]\ntext_input = clip.tokenize(class_captions).to(device)\n\nwith torch.no_grad():\n    text_features = model.encode_text(text_input).float()\n    text_features \/= text_features.norm(dim=-1, keepdim=True)\n\nshow_results(image_features, text_features, class_names)","c5c8ee12":"# Clone OpenAI CLIP repository","dc7bebbb":"# Extract image features","edfc5167":"# Add class names","eda1ca02":"# Install torch and torch vision","f7dd128c":"# Studies and examples","5d0982dd":"# Extract textual features from class captions","a65038af":"# Add class captions","a2f74416":"# Output results","292bdbca":"# Import libraries","e00b2b2c":"# Load pretrained model","f65596f3":"# Plot data","934cad34":"# Import data from gdrive","5981134a":"# Denormalize image","853428c7":"# Load data"}}