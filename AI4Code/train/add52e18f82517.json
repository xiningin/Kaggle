{"cell_type":{"515fe1f9":"code","6ba4cf62":"code","1cc39a53":"code","5ec345b7":"code","19177593":"code","5f2fdeb5":"code","5439df05":"code","ce9d85fc":"code","e3095b4f":"code","4a66d5d3":"code","454b812d":"code","4ca29ece":"code","b5430945":"code","f7bc448d":"code","fbe60dde":"code","6f9fa4bf":"code","b72276df":"code","e579e5a9":"code","87c10678":"code","b18e78ad":"code","a3079980":"code","27d814fa":"code","574acab2":"code","852b44f7":"code","c0710c4a":"code","8bbac199":"code","73d80d3f":"code","f5432bff":"code","fa7e6ef3":"code","c55ea73b":"code","25ff14ef":"code","4809135a":"markdown","cda49d39":"markdown","2aa00de7":"markdown","559bb08a":"markdown","278f2964":"markdown","36054429":"markdown","9247e41d":"markdown"},"source":{"515fe1f9":"import tensorflow as tf\nimport numpy as np\nimport os\nimport random\nimport pandas as pd\nimport seaborn as sns\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nplt.rc('font', size=16)\nfrom sklearn.preprocessing import MinMaxScaler\nimport warnings\nwarnings.filterwarnings('ignore')\ntf.get_logger().setLevel('ERROR')\n\ntfk = tf.keras\ntfkl = tf.keras.layers\nprint(tf.__version__)","6ba4cf62":"# Random seed for reproducibility\nseed = 42\n\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\ntf.compat.v1.set_random_seed(seed)","1cc39a53":"dataset = pd.read_csv('..\/input\/ann2chall\/Training.csv')\nprint(dataset.shape)\ndataset.head()","5ec345b7":"dataset.info()","19177593":"def inspect_dataframe(df, columns):\n    figs, axs = plt.subplots(len(columns), 1, sharex=True, figsize=(17,17))\n    for i, col in enumerate(columns):\n        axs[i].plot(df[col])\n        axs[i].set_title(col)\n    plt.show()\ninspect_dataframe(dataset, dataset.columns)","5f2fdeb5":"test_size = int(0.1*dataset.shape[0])\n# The last 10% samples of the time series will be used for test\nX_train_raw = dataset.iloc[:-test_size]\n\nX_test_raw = dataset.iloc[-test_size:]\nprint(X_train_raw.shape, X_test_raw.shape)\n\n# Normalize features\nX_min = X_train_raw.min()\nX_max = X_train_raw.max()\n\nX_train_raw = (X_train_raw-X_min)\/(X_max-X_min)\nX_test_raw = (X_test_raw-X_min)\/(X_max-X_min)\n\n#print(X_train_raw.columns)\n#'Sponginess', 'Wonder level', 'Crunchiness', 'Loudness on impact', 'Meme creativity', 'Soap slipperiness', 'Hype root'\nfor col in X_train_raw.columns:\n    plt.figure(figsize=(17,5))\n    plt.title('Train-Test Split')\n    plt.plot(X_train_raw[col], label=col + \" Train\")\n    plt.plot(X_test_raw[col], label=col + \" Test\")\n    plt.legend()\n    plt.show()\n\n","5439df05":"window = 250\nstride = 25","ce9d85fc":"# Keep the last window apart\n# It will be used at the end of the notebook to predict outside the dataset\nfuture = dataset[-window:]\nfuture = (future-X_min)\/(X_max-X_min)\nfuture = np.expand_dims(future, axis=0)\nfuture.shape","e3095b4f":"def build_sequences(df, target_labels, window=200, stride=20, telescope=100):\n    # Target labels allows to choose how many samples we want to predict on the future ??????\n    # Window and stride are like in classification\n    # Telescope is how many samples we want to predict in the future\n    # Sanity check to avoid runtime errors\n    assert window % stride == 0\n    dataset = []\n    labels = []\n    temp_df = df.copy().values\n    temp_label = df[target_labels].copy().values\n    padding_len = len(df)%window\n\n    if(padding_len != 0):\n        # Add zeroes for padding if the time series length is not a multiple of the window size\n        # Compute padding length\n        padding_len = window - len(df)%window\n        padding = np.zeros((padding_len,temp_df.shape[1]), dtype='float32')\n        temp_df = np.concatenate((padding,df))\n        padding = np.zeros((padding_len,temp_label.shape[1]), dtype='float32')\n        temp_label = np.concatenate((padding,temp_label))\n        assert len(temp_df) % window == 0\n\n    for idx in np.arange(0,len(temp_df)-window-telescope,stride):\n        # each temp[idx:idx+window] is considered as a time series itself.\n        # each time series has dim 200x6, which is WINDOW x SAMPLE_DIM. This means that each time series\n        dataset.append(temp_df[idx:idx+window])\n        # the labels associated to each time series refers to what comes after the window for a telescope length\n        labels.append(temp_label[idx+window:idx+window+telescope])\n\n    dataset = np.array(dataset) # numpy array of time series\n    labels = np.array(labels)\n    return dataset, labels","4a66d5d3":"def inspect_multivariate(X, y, columns, telescope, idx=None):\n    if(idx==None):\n        idx=np.random.randint(0,len(X))\n\n    figs, axs = plt.subplots(len(columns), 1, sharex=True, figsize=(17,17))\n    for i, col in enumerate(columns):\n        axs[i].plot(np.arange(len(X[0,:,i])), X[idx,:,i])\n        axs[i].scatter(np.arange(len(X[0,:,i]), len(X_train[0,:,i])+telescope), y[idx,:,i], color='orange')\n        axs[i].set_title(col)\n    plt.show()","454b812d":"def build_CONV_LSTM_model(input_shape, output_shape):\n    # Build the neural network layer by layer\n    input_layer = tfkl.Input(shape=input_shape, name='Input')\n\n    lstm = tfkl.Bidirectional(tfkl.LSTM(64, return_sequences=True))(input_layer)\n    lstm = tfkl.Bidirectional(tfkl.LSTM(64, return_sequences=True))(lstm)\n    lstm = tfkl.Bidirectional(tfkl.LSTM(64, return_sequences=True))(lstm)\n    lstm = tfkl.Conv1D(64, 3, padding='same', activation='relu')(lstm)\n#    print(lstm.shape) #350,128\n    lstm = tfkl.GlobalAveragePooling1D()(lstm)\n    lstm = tfkl.Dropout(.25)(lstm)\n    print(lstm.shape) #256\n\n\n    dense = tfkl.Dense(output_shape[-1]*output_shape[-2], activation='relu')(lstm) \n    print(dense.shape) #350, 140\n    output_layer = tfkl.Reshape((output_shape[-2],output_shape[-1]))(dense)\n    output_layer = tfkl.Conv1D(output_shape[-1], 1, padding='same')(output_layer)\n    print(output_layer.shape)\n\n    # Connect input and output through the Model class\n    model = tfk.Model(inputs=input_layer, outputs=output_layer, name='model_attempts_gian' + datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\") )\n\n    # Compile the model\n    model.compile(loss=tfk.losses.MeanSquaredError(), optimizer=tfk.optimizers.Adam(), metrics=['mae'])\n\n    # Return the model\n    return model","4ca29ece":"def inspect_multivariate_prediction(X, y, pred, columns, telescope, idx=None):\n    if(idx==None):\n        idx=np.random.randint(0,len(X))\n\n    figs, axs = plt.subplots(len(columns), 1, sharex=True, figsize=(17,17))\n    for i, col in enumerate(columns):\n        axs[i].plot(np.arange(len(X[0,:,i])), X[idx,:,i])\n        axs[i].plot(np.arange(len(X[0,:,i]), len(X_train[0,:,i])+telescope), y[idx,:,i], color='orange')\n        axs[i].plot(np.arange(len(X[0,:,i]), len(X_train[0,:,i])+telescope), pred[idx,:,i], color='green')\n        axs[i].set_title(col)\n    plt.show()","b5430945":"target_labels = dataset.columns\ntelescope = 25 # How many samples we want to predict in the future","f7bc448d":"X_train, y_train = build_sequences(X_train_raw, target_labels, window, stride, telescope)\nX_test, y_test = build_sequences(X_test_raw, target_labels, window, stride, telescope)\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","fbe60dde":"inspect_multivariate(X_train, y_train, target_labels, telescope)","6f9fa4bf":"input_shape = X_train.shape[1:]\noutput_shape = y_train.shape[1:]\nbatch_size = 32\nepochs = 500","b72276df":"model = build_CONV_LSTM_model(input_shape, output_shape)\nmodel.summary()\ntfk.utils.plot_model(model, expand_nested=True)","e579e5a9":"# Train the model\nhistory = model.fit(\n    x = X_train,\n    y = y_train,\n    batch_size = batch_size,\n    epochs = epochs,\n    validation_split=.2,\n    callbacks = [\n        tfk.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n        #tfk.callbacks.ReduceLROnPlateau(monitor='val_mae', patience=10, factor=0.25, min_lr=1e-5)\n    ]\n).history","87c10678":"best_epoch = np.argmin(history['val_loss'])\nplt.figure(figsize=(17,4))\nplt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('Mean Squared Error (Loss)')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n\nplt.figure(figsize=(17,4))\nplt.plot(history['mae'], label='Training accuracy', alpha=.8, color='#ff7f0e')\nplt.plot(history['val_mae'], label='Validation accuracy', alpha=.9, color='#5a9aa5')\nplt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\nplt.title('Mean Absolute Error')\nplt.legend()\nplt.grid(alpha=.3)\nplt.show()\n","b18e78ad":"model.save(model.name)\n#model = tfk.models.load_model('AuroregressiveForecasting')","a3079980":"# Predict the test set \npredictions = model.predict(X_test)\nprint(\"Prediction shape\")\nprint(predictions.shape)\n\ntrain_mean_squared_error = tfk.metrics.mse(y_test.flatten(),predictions.flatten())\ntrain_mean_absolute_error = tfk.metrics.mae(y_test.flatten(),predictions.flatten())\ntrain_mean_squared_error, train_mean_absolute_error","27d814fa":"inspect_multivariate_prediction(X_test, y_test, predictions, target_labels, telescope)","574acab2":"reg_telescope = telescope*50 #let's try to predict again 100 values without retraining\nX_test_reg, y_test_reg = build_sequences(X_test_raw, target_labels, window, stride, reg_telescope)\nX_test_reg.shape, y_test_reg.shape","852b44f7":"reg_predictions = np.array([])\nX_temp = X_test_reg\n# autoregressive approach: the model appends at each step the output prediction to the input,\n# increasing its dimention by 1\nfor reg in range(int(reg_telescope\/telescope)):\n    pred_temp = model.predict(X_temp)\n    if(len(reg_predictions)==0):\n        reg_predictions = pred_temp\n    else:\n        reg_predictions = np.concatenate((reg_predictions,pred_temp),axis=1)\n    X_temp = np.concatenate((X_temp[:,telescope:,:],pred_temp), axis=1)","c0710c4a":"reg_predictions.shape","8bbac199":"mean_squared_error = tfk.metrics.mse(y_test_reg.flatten(),reg_predictions.flatten())\nmean_absolute_error = tfk.metrics.mae(y_test_reg.flatten(),reg_predictions.flatten())\nf= open(\".\/\" + model.name + \"\/\" + model.name +\"_results.txt\",\"w+\")\nf.write(\"Window = \" + str(window)+ \"\\n\")\nf.write(\"Stride = \" + str(stride)+ \"\\n\")\nf.write(\"Telescope = \" + str(telescope)+ \"\\n\")\nf.write(\"Before autoregression: \\n\")\nf.write(\"mean_squared_error= \" + str(train_mean_squared_error.numpy()) + \"\\n\")\nf.write(\"mean_absolute_error = \" + str(train_mean_absolute_error.numpy())+ \"\\n\")\nf.write(\"mean_squared_error= \" + str(mean_squared_error.numpy()) + \"\\n\")\nf.write(\"After autoregression: \\n\")\nf.write(\"absolute_error = \" + str(mean_absolute_error.numpy())+ \"\\n\")\n\nf.close()\n","73d80d3f":"inspect_multivariate_prediction(X_test_reg, y_test_reg, reg_predictions, target_labels, reg_telescope)","f5432bff":"maes = []\nfor i in range(reg_predictions.shape[1]):\n    ft_maes = []\n    for j in range(reg_predictions.shape[2]):\n        ft_maes.append(np.mean(np.abs(y_test_reg[:,i,j]-reg_predictions[:,i,j]), axis=0))\n    ft_maes = np.array(ft_maes)\n    maes.append(ft_maes)\nmaes = np.array(maes)","fa7e6ef3":"reg_future = np.array([])\nX_temp = future\nfor reg in range(int(reg_telescope\/telescope)):\n    pred_temp = model.predict(X_temp)\n    if(len(reg_future)==0):\n        reg_future = pred_temp\n    else:\n        reg_future = np.concatenate((reg_future,pred_temp),axis=1)\n    X_temp = np.concatenate((X_temp[:,telescope:,:],pred_temp), axis=1)","c55ea73b":"figs, axs = plt.subplots(len(target_labels), 1, sharex=True, figsize=(17,17))\nfor i, col in enumerate(target_labels):\n    axs[i].plot(np.arange(len(future[0,:,i])), future[0,:,i])\n    axs[i].plot(np.arange(len(future[0,:,i]), len(future[0,:,i])+reg_telescope), reg_future[0,:,i], color='orange')\n    axs[i].fill_between(\n        np.arange(len(future[0,:,i]), len(future[0,:,i])+reg_telescope), \n        reg_future[0,:,i]+maes[:,i], \n        reg_future[0,:,i]-maes[:,i], \n        color='orange', alpha=.3)\n    axs[i].set_title(col)\nplt.show()","25ff14ef":"import shutil\nto_zip = \".\/\" + model.name\nshutil.make_archive(model.name, 'zip', to_zip)","4809135a":"### Set seed for reproducibility","cda49d39":"### Attention Is All You Need","2aa00de7":"Load the dataset","559bb08a":"### Multivariate Forecasting (Autoregression)","278f2964":"Sequential Train-Test split and normalization","36054429":"### Import libraries","9247e41d":"predict the future"}}