{"cell_type":{"2f23b320":"code","de2ab249":"code","49de8d09":"code","ca9eec21":"code","dd3c4c38":"code","a0a13ce7":"code","19fcc5aa":"code","460cc75a":"code","617edcf6":"code","ab5c04ea":"code","9ebdbdec":"code","2772da9d":"code","b6c1b58e":"code","74e64771":"code","d62ebd4b":"code","bbd6bff6":"code","8f753d94":"code","91934af5":"code","77391ea7":"code","985063b8":"code","5d48de3b":"markdown","0bb26877":"markdown","cf560c40":"markdown","0262ebfb":"markdown","0e57d4d9":"markdown","6ab3f4f6":"markdown","7be516d1":"markdown","1a3aefe9":"markdown","dc19a10a":"markdown","cf446b2e":"markdown"},"source":{"2f23b320":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","de2ab249":"train = pd.read_csv(\"..\/input\/learn-together\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/learn-together\/test.csv\", index_col=0)\nX = train.copy()\nX = X.drop('Cover_Type', 1)\ny = train['Cover_Type']","49de8d09":"from sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nimport matplotlib.pyplot as plt","ca9eec21":"model = RandomForestClassifier(n_estimators=100)","dd3c4c38":"model = model.fit(X,y)","a0a13ce7":"from sklearn.model_selection import KFold, cross_val_score\n\ncv = KFold(n_splits=5, shuffle=True, random_state=1)\n\ndef cross_val(model, X=X, y=y):\n    cv_results = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n    return cv_results\n\nprint(\"cross_val = {}\".format(cross_val(model, X, y)))\n","19fcc5aa":"def add_features(X_):\n    X = X_.copy()\n    \n    X['Log_elevation'] = X['Elevation'].apply(np.log)\n    \n    X['Hill_Shade_Mean'] = X[['Hillshade_9am',\n                              'Hillshade_Noon',\n                              'Hillshade_3pm']\n                            ].mean(axis='columns')\n    \n    X['Hillshade_9am_squared'] = X['Hillshade_9am'].apply(np.square)\n    X['Hillshade_Noon_squared'] = X['Hillshade_Noon'].apply(np.square)\n    X['Hillshade_3pm_squared'] = X['Hillshade_3pm'].apply(np.square)\n    \n    X['Cos_Slope'] = X['Slope'].apply(lambda a : np.cos(a\/360*2*np.pi))\n    \n    X['Extremely_Stony'] = X[['Soil_Type' + soilN for soilN in ['24', '28', '29', '30', '31', '32', '33', '34', '36', '37', '38', '39', '40']]].sum(axis='columns')\n    X['Rubbly'] = X[['Soil_Type' + soilN for soilN in ['3', '4', '5', '10', '11', '13']]].sum(axis='columns')\n    X['Rock_Land'] = X[['Soil_Type' + soilN for soilN in ['12', '13', '30', '34' , '36']]].sum(axis='columns')\n    X['Vanet'] = X[['Soil_Type' + soilN for soilN in ['2', '6']]].sum(axis='columns')\n    X['Bullwark'] = X[['Soil_Type' + soilN for soilN in ['10', '11']]].sum(axis='columns')\n    X['Hydro_Elevation_diff'] = X[['Elevation',\n                                   'Vertical_Distance_To_Hydrology']\n                                  ].diff(axis='columns').iloc[:, [1]]\n\n    X['Hydro_Euclidean'] = np.sqrt(X['Horizontal_Distance_To_Hydrology']**2 +\n                                   X['Vertical_Distance_To_Hydrology']**2)\n\n    X['Hydro_Fire_sum'] = X[['Horizontal_Distance_To_Hydrology',\n                             'Horizontal_Distance_To_Fire_Points']\n                            ].sum(axis='columns')\n\n    X['Hydro_Fire_diff'] = X[['Horizontal_Distance_To_Hydrology',\n                              'Horizontal_Distance_To_Fire_Points']\n                             ].diff(axis='columns').iloc[:, [1]].abs()\n\n    X['Hydro_Road_sum'] = X[['Horizontal_Distance_To_Hydrology',\n                             'Horizontal_Distance_To_Roadways']\n                            ].sum(axis='columns')\n\n    X['Hydro_Road_diff'] = X[['Horizontal_Distance_To_Hydrology',\n                              'Horizontal_Distance_To_Roadways']\n                             ].diff(axis='columns').iloc[:, [1]].abs()\n\n    X['Road_Fire_sum'] = X[['Horizontal_Distance_To_Roadways',\n                            'Horizontal_Distance_To_Fire_Points']\n                           ].sum(axis='columns')\n\n    X['Road_Fire_diff'] = X[['Horizontal_Distance_To_Roadways',\n                             'Horizontal_Distance_To_Fire_Points']\n                            ].diff(axis='columns').iloc[:, [1]].abs()\n    \n    cols = [\n        'Horizontal_Distance_To_Roadways',\n        'Horizontal_Distance_To_Fire_Points',\n        'Hydro_Euclidean',\n    ]\n    X['distance_mean'] = X[cols].mean(axis=1)\n    X['distance_road_fire'] = X[cols[:2]].mean(axis=1)\n    X['distance_hydro_fire'] = X[cols[1:]].mean(axis=1)\n    X['distance_road_hydro'] = X[[cols[0], cols[2]]].mean(axis=1)\n    \n    X['shade_noon_diff'] = X['Hillshade_9am'] - X['Hillshade_Noon']\n    X['shade_3pm_diff'] = X['Hillshade_Noon'] - X['Hillshade_3pm']\n    X['shade_mean'] = X[['Hillshade_9am', \n                           'Hillshade_Noon', \n                           'Hillshade_3pm']].mean(axis=1)\n    \n    X['ElevationHydro'] = X['Elevation'] - 0.25 * X['Hydro_Euclidean']\n    X['ElevationV'] = X['Elevation'] - X['Vertical_Distance_To_Hydrology']\n    X['ElevationH'] = X['Elevation'] - 0.19 * X['Horizontal_Distance_To_Hydrology']\n    X['Aspect_sin'] = np.sin(X.Aspect)\n    X['Aspect_cos'] = np.cos(X.Aspect)\n    X['Slope_sin'] = np.sin(X.Slope)\n    X['Slope_cos'] = np.cos(X.Slope)\n    \n    return X\n\n\ndef drop_features(X_):\n    X = X_.copy()\n    drop_cols = ['Soil_Type1']\n    \n    X = X.drop(drop_cols, axis='columns')\n\n    return X","460cc75a":"X_copy = X.copy()\nX_copy = add_features(X_copy)\nX_copy = drop_features(X_copy)\ntest_copy = test.copy()\ntest_copy = add_features(test_copy)\ntest_copy = drop_features(test_copy)","617edcf6":"print(\"cross_val = {}\".format(cross_val(model, X_copy, y)))\n","ab5c04ea":"fs_dict = {}\nfor trial in range(100):\n    X_features = X_copy.copy()\n    num_features = np.random.randint(10,len(X_copy.columns))\n    for col in X_copy.columns:\n        rand = np.random.randint(len(X_copy.columns))\n        if rand > num_features:\n            X_features = X_features.drop([col], axis='columns')\n    cross_v = cross_val(model, X_features, y)\n    print(\"Feature set # {}\\n': X_features.columns = {}\\n, cross_v = {}\\n\\n\\n\".format(trial, X_features.columns, cross_v))\n    fs_dict[trial] = [np.mean(cross_v), X_features.columns]\n","9ebdbdec":"best_features = sorted(fs_dict.items(), key=lambda kv: kv[1][0], reverse=True)[0][1][1].to_list()\nbest_features","2772da9d":"sorted(fs_dict.items(), key=lambda kv: kv[1][0], reverse=True)","b6c1b58e":"X_best = X_copy[best_features]\ntest_best = test_copy[best_features]","74e64771":"print(\"cross_val = {}\".format(cross_val(model, X_best, y)))","d62ebd4b":"def feature_importances(model, X, y, figsize=(18, 6)):\n    model = model.fit(X, y)\n    \n    importances = pd.DataFrame({'Features': X.columns, \n                                'Importances': model.feature_importances_})\n    \n    importances.sort_values(by=['Importances'], axis='index', ascending=False, inplace=True)\n\n    fig = plt.figure(figsize=figsize)\n    sns.barplot(x='Features', y='Importances', data=importances)\n    plt.xticks(rotation='vertical')\n    plt.show()\n\nmodel.fit(X_copy, y)\nfeature_importances(model, X_copy, y)    ","bbd6bff6":"importances = pd.DataFrame({'Features': X_copy.columns, \n                            'Importances': model.feature_importances_})\n    \nimportances.sort_values(by=['Importances'], axis='index', ascending=False, inplace=True)","8f753d94":"important_features_list = importances.iloc[0:50]['Features'].tolist()","91934af5":"X_important = X_copy[important_features_list]\ntest_important = test_copy[important_features_list]","77391ea7":"print(\"cross_val = {}\".format(cross_val(model, X_important, y)))","985063b8":"model.fit(X_best, y)\npredicts = model.predict(test_best)\n\noutput = pd.DataFrame({'ID': test.index,\n                       'Cover_Type': predicts})\noutput.to_csv('my_model.csv', index=False)","5d48de3b":"So from 0.7496 without features engineering I went up to 0.7666 in the competition which brings us to top 30%. As I said - it's good to add some features but it doesn't get you rich :)","0bb26877":"In a previous kernel I showed that in 12 lines of code and by using a simple random forest model you can get 0.7496 which puts you at the top 44% of this competition.\nQuestion is how much can we get from using features engineering.","cf560c40":"Now what if it's kind of difficult for the random forest classifier to deal with so many features. Let's try all sorts of options of having less features and see what happens.","0262ebfb":"Maybe slightly better than just having all of them in the pile. Maybe!","0e57d4d9":"Nada, no joy","6ab3f4f6":"So overall we shall go with the best selection of features and we should expect some improvement","7be516d1":"So from 0.86 I get to maybe 0.88 accuracy.","1a3aefe9":"So how about I take features 'manually' - that is only the best X or so features but according to their important...","dc19a10a":"So the above get ~0.86 accuracy here and around 0.7496 accuracy in the big data set in the competition. This is the base line!","cf446b2e":"Now let's add as many features as possible, you know whatever I was reading about and stealing from others...."}}