{"cell_type":{"006394b4":"code","7acfd701":"code","2d45b2ae":"code","9da9cff5":"code","5cd51b35":"code","e67af601":"code","94e5dc26":"code","f000c8dd":"code","c7905a14":"code","cd68dab6":"code","3afa878f":"code","6e295716":"code","2833dff1":"code","c8092586":"code","47ac694c":"code","ba45d3f5":"code","ab66961f":"code","88c2ebcb":"code","4cbdc2b0":"code","f923dab6":"code","3201b472":"code","e4fbf6f0":"code","067bdda7":"code","d6b1dcea":"code","58cb190f":"code","e3c25637":"code","ed7ba7f8":"code","c25ce84c":"code","91452d62":"code","d2a4d7ca":"code","90f96170":"code","69b58091":"code","3a599ee3":"code","de343e44":"code","4e889ab3":"code","73b52a44":"code","fc968424":"code","0b155b42":"code","37b54367":"code","1926f8c2":"code","33aa1154":"code","4cff48a1":"code","b1cd195b":"code","eb7dfd8e":"code","87d6e892":"code","9567b809":"code","4f10fe44":"code","f51d2075":"code","6dd85725":"code","8ac3fa6a":"code","dc93483f":"code","c288d79b":"code","ad9aebb3":"code","0fcb8b0e":"code","c7012a6b":"code","112880c1":"code","e8de3529":"code","be1d7b17":"code","768c8d9e":"code","e0b56475":"code","5827feac":"code","159a88c9":"code","0670474a":"code","bdb3f0dd":"code","7475dbe7":"code","91bdd0e2":"code","fe0a86b7":"code","0a8a6aca":"code","61c62212":"code","60142608":"code","e2611efb":"code","11b001c8":"code","3e1960f8":"code","e7d5244f":"code","f73c3ce6":"code","742bf3a4":"code","2a00a666":"code","078b2f4a":"code","339baf46":"code","0b84c564":"code","b801855e":"code","b4b66aeb":"code","486834c6":"code","b7cec893":"code","bdc2e1c2":"code","d3e53311":"code","6b2d29fe":"code","2806212b":"code","79dd6916":"code","689ef3f4":"code","19308df5":"code","25a67131":"code","e972fece":"code","cd300cd6":"code","1e9caf60":"code","94cffbe4":"code","589114f4":"code","152b40b9":"code","70f97ee7":"code","4bbae8e9":"code","81d2c5bf":"code","099bc7c6":"code","df156ee1":"code","ff3a925c":"code","c1badccb":"code","b4a4f609":"code","cd71f308":"code","d65e8ac4":"code","9cfb332d":"code","0e0aa9bc":"code","f26d4226":"code","8ac76183":"code","fdecf46a":"code","82e88ab8":"code","a4ba1faf":"code","f59a49df":"code","ae5385d8":"code","ddfd034b":"code","21e79079":"code","f3fca3f3":"code","7343b905":"code","86fb951b":"code","0e834bda":"code","5a240626":"code","889f482f":"code","b7d0535c":"code","85be0b8c":"code","327dd4b9":"code","930700a9":"code","08993e45":"markdown","254cea62":"markdown","1bd51024":"markdown","eaef546f":"markdown","643e09e3":"markdown","b663fc25":"markdown","e78ca540":"markdown","4491352f":"markdown","e79bfb19":"markdown","3e499fb7":"markdown","a8c72772":"markdown","3ac0f774":"markdown","1c4c6fca":"markdown","de6d707c":"markdown","84b97c70":"markdown","9c2e83e4":"markdown","b3dc119b":"markdown","ffdd3ec3":"markdown","fb3a986c":"markdown","99a6e640":"markdown","99f3c5f6":"markdown","a1f73f64":"markdown","8869f547":"markdown","08ab0fca":"markdown","b7be60d1":"markdown","25d33011":"markdown","59db9a70":"markdown","b3e19500":"markdown","00810bd4":"markdown","3c73e675":"markdown","d04799e3":"markdown","cfca7c0b":"markdown","f868242a":"markdown","68cae55f":"markdown","2424dd93":"markdown","ebec881e":"markdown","68f580d7":"markdown","5d1a7a40":"markdown","e4f0b7af":"markdown","bf4b33f5":"markdown","04b0797b":"markdown","95018549":"markdown","03ff259c":"markdown","660135e3":"markdown","837ca51d":"markdown","6c266aae":"markdown","5f9eb6e1":"markdown","1e65a773":"markdown","77e1f5a6":"markdown","fa4c6aac":"markdown","ad9bca31":"markdown","16a3d3fb":"markdown","53d7678a":"markdown","a1b66cfe":"markdown","2553ee2b":"markdown","ba6baf34":"markdown","c7e4b52f":"markdown","c94d5132":"markdown","b23dd282":"markdown","1c61f6f1":"markdown","ff3e7a1f":"markdown","05d0266e":"markdown","106a6c96":"markdown","3feb5451":"markdown","1284c7dd":"markdown","20f4c366":"markdown","c6ea7a67":"markdown","a5fd90ea":"markdown","27b12e85":"markdown","15151829":"markdown","bf94964d":"markdown","cbb8c2c1":"markdown","affef834":"markdown","78b39ec5":"markdown","55504538":"markdown","d049fa72":"markdown","787e9dc7":"markdown","d2584fce":"markdown","b8812fb5":"markdown","4d68cee7":"markdown","443e3cda":"markdown","e7619c64":"markdown","a176db2b":"markdown","d2787d06":"markdown","085fcf9e":"markdown","6469a5e1":"markdown","21295dc6":"markdown","979e8371":"markdown","d51fc8e4":"markdown","c1d51be2":"markdown","adf602da":"markdown","db9d8ed4":"markdown"},"source":{"006394b4":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model as lm\nimport statsmodels.api as sm\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\nfrom statsmodels.tools.eval_measures import mse, rmse\n\nimport warnings\nwarnings.filterwarnings(action= \"ignore\")","7acfd701":"from matplotlib import style\nstyle.use('fivethirtyeight')","2d45b2ae":"from subprocess import check_output\n\nprint(check_output([\"ls\", \"..\/input\/allcsv\"]).decode(\"utf8\"))","9da9cff5":"LifeExpectancyData = pd.read_csv('..\/input\/life-expectancy-who\/led.csv')\nregions = pd.read_csv('..\/input\/allcsv\/all.csv')","5cd51b35":"LifeExpectancyData.head()","e67af601":"LifeExpectancyData.info()","94e5dc26":"LifeExpectancyData.isnull().sum()","f000c8dd":"LifeExpectancyData.columns ","c7905a14":"LifeExpectancyData.columns= ['Country', 'Year', 'Status', 'Life_Expectancy', 'Adult_Mortality',\n       'infant_deaths', 'Alcohol', 'percentage_expenditure', 'Hepatitis_B',\n       'Measles', 'BMI', 'under_five_deaths', 'Polio', 'Total_Expenditure',\n       'Diphtheria', 'HIV\/AIDS', 'GDP','Population', 'thinness_1_19_years', 'thinness_5_9_years',\n       'Income_composition_of_resources', 'Schooling']","cd68dab6":"total_missing_values = LifeExpectancyData.isnull().sum()\nmissing_values_per = LifeExpectancyData.isnull().sum()\/LifeExpectancyData.isnull().count()\nnull_values = pd.concat([total_missing_values, missing_values_per], axis=1, keys=['total_null', 'total_null_perc'])\nnull_values = null_values.sort_values('total_null', ascending=False)","3afa878f":"def null_cell(LifeExpectancyData):\n    total_missing_values = LifeExpectancyData.isnull().sum()\n    missing_values_per = LifeExpectancyData.isnull().sum()\/LifeExpectancyData.isnull().count()\n    null_values = pd.concat([total_missing_values, missing_values_per], axis=1, keys=['total_null', 'total_null_perc'])\n    null_values = null_values.sort_values('total_null', ascending=False)\n    return null_values[null_values['total_null'] > 0]","6e295716":"plt.figure(figsize=(10,8))\nsns.heatmap(LifeExpectancyData.isnull(), cmap='viridis')","2833dff1":"#regions = pd.read_csv('.\/data\/all.csv')","c8092586":"regions.head()","47ac694c":"regions[['name', 'region', 'sub-region']].isnull().sum()","ba45d3f5":"regions.columns","ab66961f":"LifeExpectancyData_merged = pd.merge(LifeExpectancyData, regions[['name', 'region', 'sub-region']],\n                                     left_on='Country', right_on='name')","88c2ebcb":"null_cell(LifeExpectancyData_merged)","4cbdc2b0":"LifeExpectancyData_merged.head()","f923dab6":"LifeExpectancyData_merged.drop('Population', inplace=True, axis=1)","3201b472":"LifeExpectancyData_merged.columns","e4fbf6f0":"fill_list = (null_cell(LifeExpectancyData_merged)).index","067bdda7":"df_interpolate = LifeExpectancyData_merged.copy()\n\nfor col in fill_list:\n    df_interpolate[col] = df_interpolate.groupby(['Country'])[col].transform(lambda x: x.interpolate(limit_direction = 'both'))","d6b1dcea":"null_cell(df_interpolate)","58cb190f":"df_interpolate[df_interpolate['Adult_Mortality'].isna()]","e3c25637":"for col in fill_list:\n    df_interpolate[col] = df_interpolate.groupby(['sub-region', 'Year'])[col].transform(lambda x: x.interpolate(limit_direction='both'))","ed7ba7f8":"null_cell(df_interpolate)","c25ce84c":"LifeExpectancyData_num = df_interpolate._get_numeric_data() ","91452d62":"corr_matrix = LifeExpectancyData_num.corr()\ncorr_list = corr_matrix.Life_Expectancy.abs().sort_values(ascending=False).index[1:]","d2a4d7ca":"corr_list","90f96170":"plt.figure(figsize=(15,15))\nsns.heatmap(corr_matrix, annot=True, cmap='RdBu_r')\nplt.title('Correlation Matrix')","69b58091":"corr_matrix = LifeExpectancyData_num[['Hepatitis_B','Measles', 'Polio','Diphtheria','HIV\/AIDS', 'thinness_1_19_years',\n                                      'thinness_5_9_years','Life_Expectancy']].corr()\ncorr_list = corr_matrix.Life_Expectancy.abs().sort_values(ascending=False).index[1:]","3a599ee3":"corr_list","de343e44":"plt.figure(figsize=(10,10))\nsns.heatmap(corr_matrix, annot=True, cmap='RdBu_r')\nplt.title('Correlation Matrix')","4e889ab3":"plt.figure(figsize=(20,10))\nsns.violinplot(x=df_interpolate[\"Year\"], y=df_interpolate[\"Life_Expectancy\"], data=df_interpolate)\nplt.title('General Looking on Life Expectancy in Years')","73b52a44":"plt.figure(figsize=(20,10))\nsns.violinplot(x=df_interpolate.loc[df_interpolate['Year']>2009][\"Year\"], \n               y=df_interpolate[\"Life_Expectancy\"],\n               hue=df_interpolate[\"region\"], \n               data=df_interpolate.loc[df_interpolate['Year']>2010], \n               palette=\"muted\")\n\nplt.title('Life Expectancy Values in Years by Regions')","fc968424":"plt.figure(figsize=(20,10))\nsns.scatterplot(x='Life_Expectancy', \n                y='Alcohol', \n                hue='region',\n                data=df_interpolate, \n                s=df_interpolate.GDP\/100);\nplt.xlabel('Life_expectancy',size=15)\nplt.ylabel('Alcohol', size =10)\nplt.show()","0b155b42":"plt.rcParams['figure.dpi'] = 60\nplt.rcParams['figure.figsize'] = (8,5.5)","37b54367":"outliers_by_nineteen_variables = ['Year', 'Life_Expectancy','Adult_Mortality', 'infant_deaths', 'Alcohol', 'percentage_expenditure',\n                                    'Hepatitis_B','Measles', 'BMI',\n                                    'under_five_deaths', 'Polio', 'Total_Expenditure','Diphtheria', 'HIV\/AIDS', 'GDP',\n                                    'thinness_1_19_years', 'thinness_5_9_years', 'Income_composition_of_resources', 'Schooling'] \nplt.figure(figsize=(25,25))\n\nfor i in range(0,19):\n    plt.subplot(5, 4, i+1)\n    plt.boxplot(df_interpolate[outliers_by_nineteen_variables[i]])\n    plt.title(outliers_by_nineteen_variables[i])","1926f8c2":"from scipy.stats.mstats import winsorize","33aa1154":"def winsor(x, multiplier=3): \n    upper= x.median() + x.std()*multiplier\n    for limit in np.arange(0.001, 0.20, 0.001):\n        if np.max(winsorize(x,(0,limit))) < upper:\n            return limit\n    return None ","4cff48a1":"#An example to get limit value for winsorization\nlimit= winsor(df_interpolate['infant_deaths'])\nprint(limit)","b1cd195b":"df_interpolate[\"Adult_Mortality\"]        = winsorize(df_interpolate[\"Adult_Mortality\"], (0, 0.018))\ndf_interpolate[\"infant_deaths\"]          = winsorize(df_interpolate[\"infant_deaths\"], (0, 0.018))\ndf_interpolate[\"percentage_expenditure\"] = winsorize(df_interpolate[\"percentage_expenditure\"], (0, 0.036))\ndf_interpolate[\"Hepatitis_B\"]            = winsorize(df_interpolate[\"Hepatitis_B\"], (0,0.001))\ndf_interpolate[\"Measles\"]                = winsorize(df_interpolate[\"Measles\"], (0, 0.018))\ndf_interpolate[\"under_five_deaths\"]      = winsorize(df_interpolate[\"under_five_deaths\"], (0, 0.013))\ndf_interpolate[\"Polio\"]                  = winsorize(df_interpolate[\"Polio\"], (0, 0.001))\ndf_interpolate[\"Total_Expenditure\"]      = winsorize(df_interpolate[\"Total_Expenditure\"], (0, 0.011))\ndf_interpolate[\"Diphtheria\"]             = winsorize(df_interpolate[\"Diphtheria\"], (0, 0.001))\ndf_interpolate[\"HIV\/AIDS\"]               = winsorize(df_interpolate[\"HIV\/AIDS\"], (0, 0.030))\ndf_interpolate[\"GDP\"]                    = winsorize(df_interpolate[\"GDP\"], (0, 0.43))\ndf_interpolate[\"thinness_1_19_years\"]    = winsorize(df_interpolate[\"thinness_1_19_years\"], (0, 0.026))\ndf_interpolate[\"thinness_5_9_years\"]     = winsorize(df_interpolate[\"thinness_5_9_years\"], (0, 0.27))\ndf_interpolate[\"Income_composition_of_resources\"] = winsorize(df_interpolate[\"Income_composition_of_resources\"], (0, 0.001))\ndf_interpolate[\"Schooling\"]              = winsorize(df_interpolate[\"Schooling\"], (0, 0.001))\n","eb7dfd8e":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA","87d6e892":"LifeExpectancyData_num = df_interpolate._get_numeric_data() ","9567b809":"LifeExpectancyData_num = LifeExpectancyData_num.dropna()\n\nX = StandardScaler().fit_transform(LifeExpectancyData_num) #standardize the feature matrix\n\npca = PCA(n_components=0.90, whiten=True)\n\nX_pca = pca.fit_transform(X)","4f10fe44":"print (pca.explained_variance_ratio_)","f51d2075":"print('Original Number of Features', X.shape[1]) \nprint('Reduced Number of Features',X_pca.shape[1])","6dd85725":"#Creating a scaler object\nsc = StandardScaler()\n\n#fit the scaler to the features and transform\nX_std = sc.fit_transform(X)\n\n# Fit the PCA and transform the data\nX_std_pca = pca.fit_transform(X_std)\n\n# View the new feature data's shape\nX_std_pca.shape","8ac3fa6a":"from sklearn.decomposition import PCA\nfrom sklearn import decomposition, datasets","dc93483f":"#Creating a PCA object with 12 components as a parameter\npca = decomposition.PCA(n_components=12) \n# Fit the PCA and transform the data\nX_std_pca = pca.fit_transform(X_std)\n\n# View the new feature data's shape\nX_std_pca.shape","c288d79b":"plt.figure(figsize = (10,5))\nplt.plot(pca.explained_variance_ratio_)\nplt.title('Total variance explained: {}'.format(pca.explained_variance_ratio_.sum()))\nplt.show()","ad9aebb3":"df_interpolate.info()","0fcb8b0e":"df_dummies = pd.get_dummies(df_interpolate)\ndf_dummies.head()","c7012a6b":"df_dummies = df_dummies.dropna()\n\nX = StandardScaler().fit_transform(df_dummies)#standardize the feature matrix\n\npca = PCA(n_components=0.95, whiten=True)\n\nX_pca = pca.fit_transform(X)","112880c1":"print('Original Number of Features', X.shape[1]) \nprint('Reduced Number of Features',X_pca.shape[1])","e8de3529":"#Creating a scaler object\nsc = StandardScaler()\n\n#fit the scaler to the features and transform\nX_std = sc.fit_transform(X)","be1d7b17":"#Creating a PCA object with 178 components as a parameter\npca = decomposition.PCA(n_components=178) \n# Fit the PCA and transform the data\nX_std_pca = pca.fit_transform(X_std)\n\n# View the new feature data's shape\nX_std_pca.shape","768c8d9e":"plt.figure(figsize = (10,5))\nplt.plot(pca.explained_variance_ratio_)\nplt.title('Total variance explained: {}'.format(pca.explained_variance_ratio_.sum()))\nplt.show()","e0b56475":"y_allValues = LifeExpectancyData_num['Life_Expectancy']\nX_allValues = LifeExpectancyData_num[corr_list]","5827feac":"X_train, X_test, y_train, y_test = train_test_split(X_allValues, y_allValues, test_size = 0.2, random_state = 101)\n\nprint(\" Observations in Training Group : {}\".format(X_train.shape[0]))\nprint(\" Observations in Test Group     : {}\".format(X_test.shape[0]))","159a88c9":"X_train = sm.add_constant(X_train)\n\nModel_all = sm.OLS(y_train, X_train).fit()\n\nModel_all.summary()","0670474a":"pValue = Model_all.pvalues\nsignificant_values = list(pValue[pValue<= 0.05].index)","bdb3f0dd":"from sklearn import linear_model","7475dbe7":"Model_all = linear_model.LinearRegression()\nModel_all.fit(X_allValues, y_allValues)","91bdd0e2":"pred = Model_all.predict(X_allValues)\nResiduals = y_allValues - pred","fe0a86b7":"from statsmodels.tsa.stattools import acf\n\nacf_data = acf(Residuals)\n\nplt.figure(figsize=(9,6))\nplt.plot(acf_data[1:])\nplt.show()","0a8a6aca":"rand_nums = np.random.normal(np.mean(Residuals), np.std(Residuals), len(Residuals))\n\nplt.figure(figsize=(12,5))\n\nplt.subplot(1,2,1)\nplt.scatter(np.sort(rand_nums), np.sort(Residuals))\nplt.xlabel(\"Normally Distributed Random Variable\")\nplt.ylabel(\"Residuals\")\nplt.title(\"QQ Plot\")\n\nplt.subplot(1,2,2)\nplt.hist(Residuals)\nplt.xlabel(\"Residuals\")\nplt.title(\"Residuals Histogram\")\n\nplt.tight_layout()\nplt.show()","61c62212":"from scipy.stats import jarque_bera\nfrom scipy.stats import normaltest","60142608":"jb_stats = jarque_bera(Residuals)\nnorm_stats = normaltest(Residuals)\n\nprint(\"Jarque-Bera test value : {0} ve p de\u011feri : {1}\".format(jb_stats[0], jb_stats[1]))\nprint(\"Normal test value      : {0}  ve p de\u011feri : {1:.30f}\".format(norm_stats[0], norm_stats[1]))","e2611efb":"df = LifeExpectancyData_num.drop([\"Life_Expectancy\", \"Year\"], axis=1)","11b001c8":"df.shape","3e1960f8":"from sklearn.preprocessing import PolynomialFeatures ","e7d5244f":"def polynomial(df,pol):\n    poly = PolynomialFeatures(pol)\n    poly_array = poly.fit_transform(df.drop('Life_Expectancy', axis=1))\n    df_dropped = df.drop('Life_Expectancy', axis=1)\n    df_pol = pd.DataFrame(poly_array, columns= poly.get_feature_names(df_dropped.columns))\n    df_pol = pd.concat([df_pol, df['Life_Expectancy']], axis=1)\n    Feature_list = df_pol.corr()['Life_Expectancy'].abs().sort_values(ascending = False)[1:].index\n    return pd.concat([df_pol[Feature_list], df['Life_Expectancy']], axis=1)","f73c3ce6":"df_pol1 = polynomial(LifeExpectancyData_num,1)","742bf3a4":"def model_pol(df,pol):\n    y = df['Life_Expectancy']\n    Feature_list = Feature_list = df.columns[:500] #Having overfitting after 200 variables I prefer to limit until 500\n    MSE_list_test=[]\n    R_list=[]\n    number_of_variables=[]\n    MAE_list=[]\n    RMSE_list=[]\n    MAPE_list=[]\n    R_train_list=[]\n    MSE_train_list=[]\n    adj_R_test=[]\n    adj_R_train=[]\n    for variable in range(1,len(Feature_list)-1, pol**pol*2):\n        selected_features =  Feature_list[:(-1*variable)]\n        X_poly=df[selected_features]\n        X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size = 0.2, random_state = 0)\n        \n        \n        model_poly = LinearRegression()\n        results = model_poly.fit(X_train, y_train)\n        y_pred  = model_poly.predict(X_test)\n        y_pred_train = model_poly.predict(X_train)\n\n        MSE_list_test.append(mse(y_test, y_pred))\n        MSE_train_list.append(mse(y_train, y_pred_train))\n\n        R_list.append(model_poly.score(X_test, y_test))\n        adj_R_test.append(1 - (1-model_poly.score(X_test, y_test))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1))\n        \n        R_train_list.append(model_poly.score(X_train, y_train))\n        adj_R_train.append(1 - (1-model_poly.score(X_train, y_train))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\n\n\n        number_of_variables.append(len(selected_features))\n\n        MAE_list.append(mean_absolute_error(y_test, y_pred))\n\n        RMSE_list.append(rmse(y_test, y_pred))\n\n        MAPE_list.append(np.mean(np.abs((y_test-y_pred) \/ y_test)) * 100)\n        \n    model_means = list(zip(number_of_variables, R_list,R_train_list,MSE_list_test,MSE_train_list,MAE_list,RMSE_list,MAPE_list,adj_R_test,adj_R_train))\n    poly_means = pd.DataFrame(model_means, columns= ['number_of_variables','R_list','R_train_list',\n                                                            'MSE_list_test','MSE_train_list','MAE_list', 'RMSE_list', 'MAPE_list','adj_R_test', 'adj_R_train'])\n    \n    return poly_means","2a00a666":"df_poly_transform1 = polynomial(LifeExpectancyData_num,1)\ndf_pol1 = model_pol(df_poly_transform1,2)","078b2f4a":"df_poly_transform2 = polynomial(LifeExpectancyData_num,2)\ndf_pol2 = model_pol(df_poly_transform2,2)","339baf46":"#%%time #checking total time of process in Pyhton\ndf_poly_transform3 = polynomial(LifeExpectancyData_num,3)\ndf_pol3 = model_pol(df_poly_transform3,3)","0b84c564":"display(df_pol1.sort_values(by='MSE_list_test').head())\ndisplay(df_pol2.sort_values(by='MSE_list_test').head())\ndisplay(df_pol3.sort_values(by='MSE_list_test').head())","b801855e":"plt.figure(1, figsize = (25,10))\nplt.suptitle('MSE TEST TRAIN VALUES', size=20)\n\n\n\nplt.subplot(1,3,1)\nplt.plot(df_pol1.number_of_variables,df_pol1.MSE_list_test, label  = 'MSE Values', color='blue', linewidth=5)\nplt.plot(df_pol1.number_of_variables,df_pol1.MSE_train_list, label = 'MSE_train Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Values ')\nplt.title('POL 1 MSE Test\/Train Values')\nplt.ylim(0,30)\nplt.legend()\n\nplt.subplot(1,3,2)\nplt.plot(df_pol2.number_of_variables, df_pol2.MSE_list_test, label = 'MSE Values', color='blue', linewidth=5)\nplt.plot(df_pol2.number_of_variables, df_pol2.MSE_train_list,label = 'MSE_train Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Values ')\nplt.ylim(0,30)\nplt.title('POL 2 MSE Test\/Train Values')\nplt.legend()\n\nplt.subplot(1,3,3)\nplt.plot(df_pol3.number_of_variables, df_pol3.MSE_list_test, label = 'MSE Values', color='blue', linewidth=5)\nplt.plot(df_pol3.number_of_variables, df_pol3.MSE_train_list,label = 'MSE_train Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Values ')\nplt.ylim(0,30)\nplt.title('POL 3 MSE Test\/Train Values')\n\n\nplt.subplots_adjust()\nplt.legend()\nplt.show()\n\n","b4b66aeb":"plt.figure(figsize=(15,10))\nobjects = ('df_pol1', 'df_pol2', 'df_pol3')\n\ny_pos = np.arange(len(objects)) \nperformance  =[df_pol1.MSE_list_test.min() ,df_pol2.MSE_list_test.min(), df_pol3.MSE_list_test.min()]\nperformance2 =[df_pol1.MSE_train_list.min(), df_pol2.MSE_train_list.min(), df_pol3.MSE_train_list.min()]\n\nplt.subplot(121)\nplt.bar(y_pos, performance, align='center')\nplt.xticks(y_pos, objects,size=10)\nplt.xlabel('Model',size=10)\nplt.ylabel('MSE Values',size=10)\nplt.title('MSE TEST Values \\n', fontsize=10)\n\nplt.subplot(122)\nplt.bar(y_pos, performance2, align='center')\nplt.xticks(y_pos, objects,size=10)\nplt.title('MSE TRAIN Values \\n', size = 10)\n\n\nplt.xlabel('Model',size=10)\nplt.ylabel('MSE Values',size=10)\n\nplt.show()\n\n","486834c6":"plt.figure(figsize=(15,10))\nobjects = ('df_pol1', 'df_pol2', 'df_pol3')\n\ny_pos = np.arange(len(objects)) \nperformance  =[df_pol1.adj_R_test.max() ,df_pol2.adj_R_test.max(), df_pol3.adj_R_test.max()]\nperformance2 =[df_pol1.adj_R_train.max(), df_pol2.adj_R_train.max(), df_pol3.adj_R_train.max()]\n\nplt.subplot(121)\nplt.bar(y_pos, performance, align='center')\nplt.xticks(y_pos, objects,size=10)\nplt.xlabel('Model',size=10)\nplt.ylabel('Adj R Squared Test Values',size=10)\nplt.title('Adj R Squared Test Values \\n', fontsize=10)\n\nplt.subplot(122)\nplt.bar(y_pos, performance2, align='center')\nplt.xticks(y_pos, objects,size=10)\nplt.ylabel('Adj R Squared Train Values',size=10)\nplt.title('Adj R Squared Train Values \\n', size = 10)\nplt.xlabel('Model',size=10)\n\n\nplt.show()","b7cec893":"df = LifeExpectancyData_num.drop([\"Life_Expectancy\", \"Year\"], axis=1)","bdc2e1c2":"poly = PolynomialFeatures(2)\npoly_array = poly.fit_transform(df)","d3e53311":"df_poly2 = pd.DataFrame(poly_array, columns= poly.get_feature_names())","6b2d29fe":"y = LifeExpectancyData_num['Life_Expectancy']\nX = df_poly2\n\nX_train_pol2, X_test_pol2, y_train_pol2, y_test_pol2 = train_test_split(X, y, test_size = 0.2, random_state = 101)\n\nprint(\"E\u011fitim k\u00fcmesindeki g\u00f6zlem say\u0131s\u0131 : {}\".format(X_train.shape[0]))\nprint(\"Test k\u00fcmesindeki g\u00f6zlem say\u0131s\u0131   : {}\".format(X_test.shape[0]))\n\nX_train = sm.add_constant(X_train)\n\npoly_model_2 = sm.OLS(y_train_pol2, X_train_pol2).fit()\ny_preds_pol2 = poly_model_2.predict(X_test_pol2)\ny_preds_train_pol2 = poly_model_2.predict(X_train_pol2)","2806212b":"poly = PolynomialFeatures(3)\npoly_array = poly.fit_transform(df)\ndf_poly3 = pd.DataFrame(poly_array, columns= poly.get_feature_names())\n\ny = LifeExpectancyData_num['Life_Expectancy']\nX = df_poly3\n\nX_train_pol3, X_test_pol3, y_train_pol3, y_test_pol3 = train_test_split(X, y, test_size = 0.2, random_state = 101)\n\nprint(\"Observations in Train Group : {}\".format(X_train.shape[0]))\nprint(\"Observations in Test Group  : {}\".format(X_test.shape[0]))\n\nX_train = sm.add_constant(X_train)\n\npoly_model_3 = sm.OLS(y_train_pol3, X_train_pol3).fit()\ny_preds_pol3 = poly_model_3.predict(X_test_pol3)\ny_preds_train_pol3 = poly_model_3.predict(X_train_pol3)","79dd6916":"poly = PolynomialFeatures(1)\npoly_array = poly.fit_transform(df)\ndf_poly1 = pd.DataFrame(poly_array, columns= poly.get_feature_names())\n\ny = LifeExpectancyData_num['Life_Expectancy']\nX = df_poly1\n\nX_train_pol1, X_test_pol1, y_train_pol1, y_test_pol1 = train_test_split(X, y, test_size = 0.2, random_state = 101)\n\nprint(\"Observations in Train Group : {}\".format(X_train.shape[0]))\nprint(\"Observations in Test Group  : {}\".format(X_test.shape[0]))\n\nX_train = sm.add_constant(X_train)\n\npoly_model_1 = sm.OLS(y_train_pol1, X_train_pol1).fit()\ny_preds_pol1 = poly_model_1.predict(X_test_pol1)\ny_preds_train_pol1 = poly_model_1.predict(X_train_pol1)\n","689ef3f4":"plt.figure(figsize=(18,8))\nplt.suptitle('Scatter Plots of Life Expectancy Predictions', size = 16)\n\nplt.subplot(1,3,1)\nplt.title('Poly 1 Model \\n', size = 14)\nplt.scatter(y_test_pol1, y_preds_pol1)\nplt.scatter(y_train_pol1, y_preds_train_pol1,alpha=0.10)\nplt.plot(y_test_pol1, y_test_pol1, color=\"red\")\nplt.ylim(0,90)\nplt.xlabel(\"True Values\")\nplt.ylabel(\"Predictions\")\n\nplt.subplot(1,3,2)\nplt.title('Poly 2 Model \\n', size = 14)\nplt.scatter(y_test_pol2, y_preds_pol2 )\nplt.scatter(y_train_pol2, y_preds_train_pol2,alpha=0.10)\nplt.plot(y_test_pol2, y_test_pol2, color=\"red\")\nplt.xlabel(\"True Values\")\nplt.ylabel(\"Predictions\")\n\nplt.subplot(1,3,3)\nplt.title('Poly 3 Model \\n', size = 14)\nplt.scatter(y_test_pol3, y_preds_pol3)\nplt.scatter(y_train_pol3, y_preds_train_pol3,alpha=0.10)\nplt.plot(y_test_pol3, y_test_pol3, color=\"red\")\nplt.ylim(0,90)\nplt.xlabel(\"True Values\")\nplt.ylabel(\"Predictions\")\n\n\n\n\nplt.subplots_adjust()\nplt.show()","19308df5":"from sklearn.linear_model import Ridge","25a67131":"def Ridge_model(df,pol, alpha, col=None):\n\n    y = df['Life_Expectancy']\n    Feature_list = df.columns[:500]\n    \n    MSE_list_test=[]\n    R_list=[]\n    adj_R_test=[]\n    number_of_variables=[]\n    MAE_list=[]\n    RMSE_list=[]\n    MAPE_list=[]\n    R_train_list=[]\n    adj_R_train=[]\n    MSE_train_list=[]\n    model_list=[]\n    feature_list=[]\n        \n    \n    for variable in range(1,len(Feature_list)-1, pol**pol*2):\n        selected_features =  Feature_list[:(-1*variable)]\n        X_poly=df[selected_features]\n        X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size = 0.2, random_state = 0)\n                \n        model_poly = Ridge(alpha= alpha) \n        model_poly.fit(X_train, y_train)\n        results = model_poly.fit(X_train, y_train)\n               \n        y_pred  = model_poly.predict(X_test)\n        \n        y_pred_train = model_poly.predict(X_train)\n      \n        MSE_list_test.append(mse(y_test, y_pred))\n        \n        MSE_train_list.append(mse(y_train, y_pred_train))\n        R_list.append(model_poly.score(X_test, y_test))\n        adj_R_test.append(1 - (1-model_poly.score(X_test, y_test))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1))\n        \n        R_train_list.append(model_poly.score(X_train, y_train))\n        adj_R_train.append(1 - (1-model_poly.score(X_train, y_train))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\n        \n        number_of_variables.append(len(selected_features))\n        MAE_list.append(mean_absolute_error(y_test, y_pred))\n        \n        RMSE_list.append(rmse(y_test, y_pred))\n        \n        MAPE_list.append(np.mean(np.abs((y_test-y_pred) \/ y_test)) * 100)\n        model_list.append(model_poly)\n        feature_list.append(selected_features)\n        \n        \n    \n        \n        \n    model_means = list(zip(number_of_variables, R_list, adj_R_test, R_train_list, adj_R_train, MSE_list_test,\n                           MSE_train_list,MAE_list,RMSE_list,MAPE_list,model_list,feature_list))\n    \n    poly_means = pd.DataFrame(model_means, columns= ['number_of_variables', 'R_list','adj_R_test',\n                                                     'R_train_list','adj_R_train',\n                                                     'MSE_list_test','MSE_train_list','MAE_list','RMSE_list','MAPE_list',\n                                                     'model_list', 'feature_list'])\n    \n    \n    return poly_means, (y_pred,y_pred_train, X_train,y_train, X_test, y_test, MSE_list_test,MSE_train_list)","e972fece":"%%time\nfor alpha in [0.000001, 0.00001, 0.0001, 0.001, 0.01, 1, 10, 100, 1000]: \n    df, _  = Ridge_model(df_poly_transform2,2,alpha)\n    print(alpha, df.MSE_list_test.min())","cd300cd6":"#The Best Model option with minimum MSE_test Value on Alpha 10-\u2075 and polynomial 2nd degree.\n\ndf_Ridge_alpha_pol2, degerler1_2 = Ridge_model(df_poly_transform2,2,0.000001)","1e9caf60":"df_Ridge_alpha_pol2.head()","94cffbe4":"%%time\nfor alpha in [0.000001, 0.00001, 0.0001, 0.001, 0.01, 1, 10, 100, 1000]:\n    df, _  = Ridge_model(df_poly_transform3,3,alpha)\n    print(alpha, df.MSE_list_test.min())","589114f4":"#The Best Model option with minimum MSE_test Value on Alpha 10\u00b3 and polynomial 3rd degree.\ndf_Ridge_alpha_pol3, degerler1_3 = Ridge_model(df_poly_transform3,3,1000)","152b40b9":"df_Ridge_alpha_pol3.head()","70f97ee7":"MSE_list_test_alpha_pol2  = df_Ridge_alpha_pol2['MSE_list_test']\nMSE_train_test_alpha_pol2 = df_Ridge_alpha_pol2['MSE_train_list']\nMSE_list_test_alpha_pol3  = df_Ridge_alpha_pol3['MSE_list_test']\nMSE_train_test_alpha_pol3 = df_Ridge_alpha_pol3['MSE_train_list']","4bbae8e9":"plt.figure(1, figsize = (15,8))\n\nplt.subplot(1,2,1)\nplt.plot(df_Ridge_alpha_pol2.number_of_variables, MSE_list_test_alpha_pol2,label  = 'MSE Test Alpha Pol2 Values', color='blue', linewidth=5)\nplt.plot(df_Ridge_alpha_pol2.number_of_variables, MSE_train_test_alpha_pol2,label = 'MSE Train  Alpha Pol2 Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Values')\nplt.title('POLY 2 MSE Test\/Train Values')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(df_Ridge_alpha_pol3.number_of_variables,MSE_list_test_alpha_pol3,label  = 'MSE Test Alpha Pol3 Values', color='blue', linewidth=5)\nplt.plot(df_Ridge_alpha_pol3.number_of_variables, MSE_train_test_alpha_pol3,label = 'MSE Train Alpha Pol3 Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Values')\nplt.title('POLY 3 MSE Test\/Train Values')\n\n\nplt.subplots_adjust()\nplt.legend()\nplt.show()","81d2c5bf":"adj_R_test_alpha_pol2  = df_Ridge_alpha_pol2['adj_R_test']\nadj_R_train_alpha_pol2 = df_Ridge_alpha_pol2['adj_R_train']\nadj_R_test_alpha_pol3  = df_Ridge_alpha_pol3['adj_R_test']\nadj_R_train_alpha_pol3 = df_Ridge_alpha_pol3['adj_R_train']\n\n\n\n\nplt.figure(1, figsize = (15,8))\n\nplt.subplot(1,2,1)\nplt.plot(df_Ridge_alpha_pol2.number_of_variables, adj_R_test_alpha_pol2,label  = 'Adjusted R\u00b2 Test Alpha Pol2 Values', color='blue', linewidth=5)\nplt.plot(df_Ridge_alpha_pol2.number_of_variables, adj_R_train_alpha_pol2,label = 'Adjusted R\u00b2 Train  Alpha Pol2 Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Adjusted Values')\nplt.title('Ridge POLY 2 Adjusted R\u00b2 Test\/Train Values')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(df_Ridge_alpha_pol3.number_of_variables,adj_R_test_alpha_pol3,label  = 'Adjusted R\u00b2 Test Alpha Pol3 Values', color='blue', linewidth=5)\nplt.plot(df_Ridge_alpha_pol3.number_of_variables, adj_R_train_alpha_pol3,label = 'Adjusted R\u00b2 Train Alpha Pol3 Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Adjusted Values')\nplt.title('Ridge POLY 3 Adjusted R\u00b2 Test\/Train Values')\n\n\nplt.subplots_adjust()\nplt.legend()\nplt.show()","099bc7c6":"from sklearn.linear_model import Lasso","df156ee1":"def Lasso_model(df,pol, alpha):\n\n    y = df['Life_Expectancy']\n    Feature_list = df.columns[:500]\n    \n    MSE_list_test=[]\n    R_list=[]\n    adj_R_test=[]\n    number_of_variables=[]\n    MAE_list=[]\n    RMSE_list=[]\n    MAPE_list=[]\n    R_train_list=[]\n    adj_R_train=[]\n    MSE_train_list=[]\n    \n    for variable in range(1,len(Feature_list)-1, pol**pol*2):\n        selected_features =  Feature_list[:(-1*variable)]\n        X_poly=df[selected_features]\n        X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size = 0.2, random_state = 0)\n                \n        model_poly = Lasso(alpha= alpha) \n        model_poly.fit(X_train, y_train)\n        results = model_poly.fit(X_train, y_train)\n               \n        y_pred  = model_poly.predict(X_test)\n        \n        y_pred_train = model_poly.predict(X_train)\n      \n        MSE_list_test.append(mse(y_test, y_pred))\n        \n        MSE_train_list.append(mse(y_train, y_pred_train))\n        \n        R_list.append(model_poly.score(X_test, y_test))\n        adj_R_test.append(1 - (1-model_poly.score(X_test, y_test))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1))\n        \n        R_train_list.append(model_poly.score(X_train, y_train))\n        adj_R_train.append(1 - (1-model_poly.score(X_train, y_train))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\n        \n        number_of_variables.append(len(selected_features))\n        MAE_list.append(mean_absolute_error(y_test, y_pred))\n        \n        RMSE_list.append(rmse(y_test, y_pred))\n        \n        MAPE_list.append(np.mean(np.abs((y_test-y_pred) \/ y_test)) * 100)\n        \n        \n    model_means = list(zip(number_of_variables, R_list, adj_R_test, R_train_list, adj_R_train, MSE_list_test,MSE_train_list,MAE_list,RMSE_list,MAPE_list))\n    \n    poly_means = pd.DataFrame(model_means, columns= ['number_of_variables', 'R_list', 'adj_R_test', 'R_train_list', 'adj_R_train','MSE_list_test','MSE_train_list','MAE_list','RMSE_list','MAPE_list'])\n    \n    \n    return poly_means, (y_pred,y_pred_train, X_train,y_train, X_test, y_test, model_poly, MSE_list_test,MSE_train_list)","ff3a925c":"%%time\nfor alpha in [0.000001, 0.00001, 0.0001, 0.001, 0.01, 1, 10, 100, 1000]:\n    df, _  = Lasso_model(df_poly_transform2,2,alpha)\n    print(alpha, df.MSE_list_test.min())","c1badccb":"#The Best Model option with minimum MSE_test Value on Alpha 10-\u2075 and polynomial 2 degree\n\ndf_Lasso_alpha_pol2, degerler1_2 = Lasso_model(df_poly_transform2,2,0.000001)","b4a4f609":"df_Lasso_alpha_pol2.head()","cd71f308":"%%time\nfor alpha in [0.000001, 0.00001, 0.0001, 0.001, 0.01, 1, 10, 100, 1000]:\n    df, _  = Lasso_model(df_poly_transform3,3,alpha)\n    print(alpha, df.MSE_list_test.min())","d65e8ac4":"# The Best Model option with minimum MSE_test Value on Alpha 10\u00b3 and polynomial 3 degree.\n\ndf_Lasso_alpha_pol3, degerler1_3 = Lasso_model(df_poly_transform3,3,1000)","9cfb332d":"df_Lasso_alpha_pol3.head()","0e0aa9bc":"MSE_list_test_Lasso_alpha_pol2  = df_Lasso_alpha_pol2['MSE_list_test']\nMSE_train_test_Lasso_alpha_pol2 = df_Lasso_alpha_pol2['MSE_train_list']\nMSE_list_test_Lasso_alpha_pol3  = df_Lasso_alpha_pol3['MSE_list_test']\nMSE_train_test_Lasso_alpha_pol3 = df_Lasso_alpha_pol3['MSE_train_list']","f26d4226":"plt.figure(1, figsize = (15,8))\n\nplt.subplot(1,2,1)\nplt.plot(df_Lasso_alpha_pol2.number_of_variables,MSE_list_test_Lasso_alpha_pol2, label = 'MSE Test  Alpha Pol2 Values', color='blue', linewidth=5)\nplt.plot(df_Lasso_alpha_pol2.number_of_variables,MSE_train_test_Lasso_alpha_pol2,label = 'MSE Train  Alpha Pol2 Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Values')\nplt.title('Lasso POLY 2 MSE Test\/Train Values')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(df_Lasso_alpha_pol3.number_of_variables, MSE_list_test_Lasso_alpha_pol3,label = 'MSE Alpha1 Pol3 Values', color='blue', linewidth=5)\nplt.plot(df_Lasso_alpha_pol3.number_of_variables, MSE_train_test_Lasso_alpha_pol3,label = 'MSE Train  Alpha Pol3 Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Values')\nplt.title('Lasso POLY 3 MSE Test\/Train Values')\nplt.legend()\n\nplt.subplots_adjust()\n\nplt.show()","8ac76183":"adj_R_test_Lasso_alpha_pol2  = df_Lasso_alpha_pol2['adj_R_test']\nadj_R_train_Lasso_alpha_pol2 = df_Lasso_alpha_pol2['adj_R_train']\nadj_R_test_Lasso_alpha_pol3  = df_Lasso_alpha_pol3['adj_R_test']\nadj_R_train_Lasso_alpha_pol3 = df_Lasso_alpha_pol3['adj_R_train']\n\nplt.figure(1, figsize = (15,8))\n\nplt.subplot(1,2,1)\nplt.plot(df_Lasso_alpha_pol2.number_of_variables,adj_R_test_Lasso_alpha_pol2, label = 'Adjusted R\u00b2 Test  Alpha Pol2 Values', color='blue', linewidth=5)\nplt.plot(df_Lasso_alpha_pol2.number_of_variables,adj_R_train_Lasso_alpha_pol2,label = 'Adjusted R\u00b2 Train  Alpha Pol2 Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Adjusted R\u00b2 Values')\nplt.title('Lasso POLY 2 Adjusted R\u00b2 Test\/Train Values')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(df_Lasso_alpha_pol3.number_of_variables, adj_R_test_Lasso_alpha_pol3,label = 'Adjusted R\u00b2 Alpha1 Pol3 Values', color='blue', linewidth=5)\nplt.plot(df_Lasso_alpha_pol3.number_of_variables, adj_R_train_Lasso_alpha_pol3,label = 'Adjusted R\u00b2 Train  Alpha Pol3 Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Adjusted R\u00b2 Values')\nplt.title('Lasso POLY 3 Adjusted R\u00b2 Test\/Train Values')\nplt.legend()\n\nplt.subplots_adjust()\n\nplt.show()\n\n","fdecf46a":"from sklearn.linear_model import ElasticNet","82e88ab8":"def ElasticNet_model(df,pol, alpha):\n\n    y = df['Life_Expectancy']\n    Feature_list = df.columns[:500]\n    \n    MSE_list_test=[]\n    R_list=[]\n    adj_R_test=[]\n    number_of_variables=[]\n    MAE_list=[]\n    RMSE_list=[]\n    MAPE_list=[]\n    R_train_list=[]\n    adj_R_train=[]\n    MSE_train_list=[]\n    \n    for variable in range(1,len(Feature_list)-1, pol**pol*2):\n        selected_features =  Feature_list[:(-1*variable)]\n        X_poly=df[selected_features]\n        X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size = 0.2, random_state = 0)\n                \n        model_poly = ElasticNet(alpha=alpha, l1_ratio=0.5)\n        model_poly.fit(X_train, y_train)\n        results = model_poly.fit(X_train, y_train)\n               \n        y_pred  = model_poly.predict(X_test)\n        \n        y_pred_train = model_poly.predict(X_train)\n      \n        MSE_list_test.append(mse(y_test, y_pred))\n        \n        MSE_train_list.append(mse(y_train, y_pred_train))\n        \n        R_list.append(model_poly.score(X_test, y_test))\n        adj_R_test.append(1 - (1-model_poly.score(X_test, y_test))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1))\n        \n        R_train_list.append(model_poly.score(X_train, y_train))\n        adj_R_train.append(1 - (1-model_poly.score(X_train, y_train))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\n                \n        number_of_variables.append(len(selected_features))\n        MAE_list.append(mean_absolute_error(y_test, y_pred))\n        \n        RMSE_list.append(rmse(y_test, y_pred))\n        \n        MAPE_list.append(np.mean(np.abs((y_test-y_pred) \/ y_test)) * 100)\n        \n        \n    model_means = list(zip(number_of_variables, R_list, adj_R_test, R_train_list, adj_R_train, MSE_list_test,MSE_train_list,MAE_list,RMSE_list,MAPE_list))\n    \n    poly_means = pd.DataFrame(model_means, columns= ['number_of_variables', 'R_list', 'adj_R_test', 'R_train_list', 'adj_R_train', 'MSE_list_test','MSE_train_list','MAE_list','RMSE_list','MAPE_list'])\n    \n    \n    return poly_means, (y_pred,y_pred_train, X_train,y_train, X_test, y_test, model_poly, MSE_list_test,MSE_train_list)","a4ba1faf":"%%time\nfor alpha in [0.000001, 0.00001, 0.0001, 0.001, 0.01, 1, 10, 100, 1000]:\n    df, _  = ElasticNet_model(df_poly_transform3,3,alpha)\n    print(alpha, df.MSE_list_test.min())","f59a49df":"#The Best Model with minimum MSE_test Value on Alpha 10\u2074 and polynomial 3 degree \ndf_ElasticNet_alpha_pol3, degerler1_3 = ElasticNet_model(df_poly_transform3,3,0.00001)","ae5385d8":"%%time\nfor alpha in [0.000001, 0.00001, 0.0001, 0.001, 0.01, 1, 10, 100, 1000]:\n    df, _  = ElasticNet_model(df_poly_transform2,2,alpha)\n    print(alpha, df.MSE_list_test.min())","ddfd034b":"#The Best Model with minimum MSE_test Value on Alpha 10-\u2075 and polynomial 2 degree \n\ndf_ElasticNet_alpha_pol2, degerler1_2 = ElasticNet_model(df_poly_transform2,2,0.000001)","21e79079":"MSE_list_test_ElasticNet_alpha_pol2  = df_ElasticNet_alpha_pol2['MSE_list_test']\nMSE_list_train_ElasticNet_alpha_pol2 = df_ElasticNet_alpha_pol2['MSE_train_list']\nMSE_list_test_ElasticNet_alpha_pol3  = df_ElasticNet_alpha_pol3['MSE_list_test']\nMSE_list_train_ElasticNet_alpha_pol3 = df_ElasticNet_alpha_pol3['MSE_train_list']","f3fca3f3":"plt.figure(1, figsize = (15,8))\n\nplt.subplot(1,2,1)\nplt.plot(df_ElasticNet_alpha_pol2.number_of_variables,MSE_list_test_ElasticNet_alpha_pol2, label = 'MSE Test  Alpha Pol2 Values', color='blue', linewidth=5)\nplt.plot(df_ElasticNet_alpha_pol2.number_of_variables,MSE_list_train_ElasticNet_alpha_pol2,label = 'MSE Train  Alpha Pol2 Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Values')\nplt.title('POLY 2 MSE Test\/Train Values')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(df_ElasticNet_alpha_pol3.number_of_variables, MSE_list_test_ElasticNet_alpha_pol3,label = 'MSE Alpha1 Pol3 Values', color='blue', linewidth=5)\nplt.plot(df_ElasticNet_alpha_pol3.number_of_variables, MSE_list_train_ElasticNet_alpha_pol3,label = 'MSE Train  Alpha Pol3 Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Values')\nplt.title('POLY 3 MSE Test\/Train Values')\nplt.legend()\n\nplt.subplots_adjust()\n\nplt.show()","7343b905":"adj_R_test_ElasticNet_alpha_pol2  = df_ElasticNet_alpha_pol2['adj_R_test']\nadj_R_train_ElasticNet_alpha_pol2 = df_ElasticNet_alpha_pol2['adj_R_train']\nadj_R_test_ElasticNet_alpha_pol3  = df_ElasticNet_alpha_pol3['adj_R_test']\nadj_R_train_ElasticNet_alpha_pol3 = df_ElasticNet_alpha_pol3['adj_R_train']\n\n\nplt.figure(1, figsize = (15,8))\n\nplt.subplot(1,2,1)\nplt.plot(df_ElasticNet_alpha_pol2.number_of_variables,adj_R_test_ElasticNet_alpha_pol2, label = 'Adjusted R\u00b2 Test  Alpha Pol2 Values', color='blue', linewidth=5)\nplt.plot(df_ElasticNet_alpha_pol2.number_of_variables,adj_R_train_ElasticNet_alpha_pol2,label = 'Adjusted R\u00b2 Train  Alpha Pol2 Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Adjusted R\u00b2')\nplt.title('Elastic Net POLY 2 Adjusted R\u00b2 Test\/Train Values')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(df_ElasticNet_alpha_pol3.number_of_variables, adj_R_test_ElasticNet_alpha_pol3,label = 'Adjusted R\u00b2 Alpha1 Pol3 Values', color='blue', linewidth=5)\nplt.plot(df_ElasticNet_alpha_pol3.number_of_variables, adj_R_train_ElasticNet_alpha_pol3,label = 'Adjusted R\u00b2 Train  Alpha Pol3 Values', color='red', linewidth=5)\nplt.xlabel('Number of Variable')\nplt.ylabel('Adjusted R\u00b2')\nplt.title('Elastic Net POLY 3 Adjusted R\u00b2 Test\/Train Values')\nplt.legend()\n\nplt.subplots_adjust()\n\nplt.show()","86fb951b":"plt.figure(figsize=(25,20))\n\nobjects=('df_pol1', 'df_pol2', 'df_pol3',\n           'df_Ridge_alpha_pol2', 'df_Ridge_alpha_pol3',\n           'df_Lasso_alpha_pol2', 'df_Lasso_alpha_pol3',\n           'df_ElasticNet_alpha_pol2', 'df_ElasticNet_alpha_pol3' )\n\ny_pos = np.arange(len(objects)) \nperformance  =[df_pol1.MSE_list_test.min() ,df_pol2.MSE_list_test.min(), df_pol3.MSE_list_test.min(),\n               df_Ridge_alpha_pol2.MSE_list_test.min(),df_Ridge_alpha_pol3.MSE_list_test.min(),\n               df_Lasso_alpha_pol2.MSE_list_test.min(), df_Lasso_alpha_pol3.MSE_list_test.min(),\n               df_ElasticNet_alpha_pol2.MSE_list_test.min(), df_ElasticNet_alpha_pol3.MSE_list_test.min()]\n\nperformance2 =[df_pol1.MSE_train_list.min(), df_pol2.MSE_train_list.min(), df_pol3.MSE_train_list.min(),\n               df_Ridge_alpha_pol2.MSE_train_list.min(),df_Ridge_alpha_pol3.MSE_train_list.min(),\n               df_Lasso_alpha_pol2.MSE_train_list.min(), df_Lasso_alpha_pol3.MSE_train_list.min(),\n               df_ElasticNet_alpha_pol2.MSE_train_list.min(), df_ElasticNet_alpha_pol3.MSE_train_list.min()]\n               \n               \nperformance3 = [df_pol1.R_list.max() ,df_pol2.R_list.max(), df_pol3.R_list.max(),\n               df_Ridge_alpha_pol2.R_list.max(),df_Ridge_alpha_pol3.R_list.max(),\n               df_Lasso_alpha_pol2.R_list.max(), df_Lasso_alpha_pol3.R_list.max(),\n               df_ElasticNet_alpha_pol2.R_list.max(), df_ElasticNet_alpha_pol3.R_list.max()]\n\nperformance4 = [df_pol1.adj_R_test.max() ,df_pol2.adj_R_test.max(), df_pol3.adj_R_test.max(),\n               df_Ridge_alpha_pol2.adj_R_test.max(),df_Ridge_alpha_pol3.adj_R_test.max(),\n               df_Lasso_alpha_pol2.adj_R_test.max(), df_Lasso_alpha_pol3.adj_R_test.max(),\n               df_ElasticNet_alpha_pol2.adj_R_test.max(), df_ElasticNet_alpha_pol3.adj_R_test.max()]\n\nplt.subplot(411)\nplt.bar(y_pos, performance, align='center')\nplt.xticks(y_pos, objects,size=13)\n\nplt.ylabel('MSE Values',size=15)\nplt.title('MSE TEST Values \\n', fontsize=15)\n\n\nplt.subplots_adjust()\nplt.subplot(412)\nplt.bar(y_pos, performance2, align='center')\nplt.xticks(y_pos, objects,size=13)\n\nplt.ylabel('MSE TRAIN Values',size=15)\nplt.title('MSE  Values \\n', size = 15)\n\nplt.subplot(413)\nplt.bar(y_pos, performance3, align='center')\nplt.xticks(y_pos, objects,size=13)\nplt.title('R Squared Values \\n', size = 15)\n\nplt.ylabel('R Squared Values',size=15)\n\nplt.subplot(414)\nplt.bar(y_pos, performance4, align='center')\nplt.xticks(y_pos, objects,size=13)\nplt.title('Adjusted R Squared Values \\n', size = 15)\n\nplt.ylabel('Adjusted R Squared Values',size=15)\n\n\nplt.subplots_adjust()\nplt.show()","0e834bda":"objects =(df_pol1, df_pol2, df_pol3,\n             df_Ridge_alpha_pol2, df_Ridge_alpha_pol3,\n             df_Lasso_alpha_pol2, df_Lasso_alpha_pol3,\n             df_ElasticNet_alpha_pol2, df_ElasticNet_alpha_pol3)\n\ndf_results = pd.DataFrame()\nfor df in objects:\n    df_results= df_results.append(df.sort_values(by='MSE_list_test').head(1), ignore_index=True)\n    \n\ndf_results['Model'] = ['Linear Regression (Polynomial 1)',\n                           'Linear Regression (Polynomial 2)',\n                           'Linear Regression (Polynomial 3)',\n                           'Ridge Regression (Polynomial 2)',\n                           'Ridge Regression (Polynomial 3)',\n                           'Lasso Regression (Polynomial 2)',\n                           'Lasso Regression (Polynomial 3)',                           \n                           'ElasticNet Regression(Polynomial 2)',\n                           'ElasticNet Regression(Polynomial 3)']\n    \ndf_results.sort_values('MSE_list_test')[['Model', 'number_of_variables', 'MSE_list_test','MSE_train_list', 'R_list','adj_R_test', 'adj_R_train']]","5a240626":"#As we see on the graph of this model, best performance is starting after 125th variable.\n#Thus, I selected the first 126 variables from our model.\n\ndf_Ridge_alpha_pol2[df_Ridge_alpha_pol2['number_of_variables']== 126 ]","889f482f":"# A rondom row[5] of our data set to find values for each columns as an example:\n\nSelected_Model = df_Ridge_alpha_pol2.iloc[5].model_list","b7d0535c":"#Here are the first 5 coeficiants from our model. \n\nSelected_Model.coef_[:5]","85be0b8c":"#Switching our values to doctionary for the further step.\nLifeExpectancyData_num.iloc[5].to_dict()","327dd4b9":"#Creating a dictionary to have values for each variables.\n\ndictionary = {'Year': 2010.0,\n 'Adult_Mortality': 279.0,\n 'infant_deaths': 74.0,\n 'Alcohol': 0.01,\n 'percentage_expenditure': 79.67936736,\n 'Hepatitis_B': 66.0,\n 'Measles': 1989.0,\n 'BMI': 16.7,\n 'under_five_deaths': 102.0,\n 'Polio': 66.0,\n 'Total_Expenditure': 9.2,\n 'Diphtheria': 66.0,\n 'HIV\/AIDS': 0.1,\n 'GDP': 553.32894,\n 'thinness_1_19_years': 16.6,\n 'thinness_5_9_years': 6.9,\n 'Income_composition_of_resources': 0.45,\n 'Schooling': 9.2}","930700a9":"Example = np.array(list(dictionary.values())).reshape(1,-1)\npoly = PolynomialFeatures(2)\ndf = LifeExpectancyData_num.drop('Life_Expectancy', axis=1)\npoly.fit_transform(df)\n\ndf_example = pd.DataFrame(poly.transform(Example), columns= poly.get_feature_names(df.columns))\n\ndf_Ridge_alpha_pol2, degerler1_2 = Ridge_model(df_poly_transform2,2,0.000001)\nselected_fetures = df_Ridge_alpha_pol2.iloc[5]['feature_list']\nselected_model = df_Ridge_alpha_pol2.iloc[5]['model_list']\n\nSelected_Model.predict(df_example[selected_fetures]) ","08993e45":"#### ***Looking at results***","254cea62":"### **Life Expectancy Values in Years by Regions**","1bd51024":"# **Visualization the three category based models**","eaef546f":"#### ***Merging datasets***","643e09e3":"# <div align=\"center\">  **1. Introduction**","b663fc25":"PCA with 178 variables can explain of 95% of total variance.","e78ca540":"# <font color=\"red\"> <div align=\"center\"> REGRESSION PROJECT  \n    \n## <div align=\"center\"> Life Expectancy Data\n","4491352f":"# 10.6 Building ElasticNet Regression Models","e79bfb19":"<font color=\"green\"> ***Jarque Bera shows us that residuals distributed normally.***","3e499fb7":"#### ***Looking NAN values with heatmap***","a8c72772":"#### ***Getting new dataset***","3ac0f774":"## **10.1 Building Model with All Numerical Variables**","1c4c6fca":"#### <font color=\"green\"> This models gives best adjusted R\u00b2 values with 2nd polynomial degree.","de6d707c":"### <font color=\"green\"> At the further steps, I will search for the best model based on number of variables. This PCS formula above is just an example to get results quickly. I would rather check the best model with MSE and another related values on different regression models in this project. ","84b97c70":"### <font color=\"gray\"> This study aims to search for the elements which effects life expectancy by using statistical tools such as MSE, R squared, RMSE, ect. on different regression models.","9c2e83e4":"#### Getting All Values of each Models in One Data Frame","b3dc119b":"<font color=\"green\"> ***We're splitting the data in two, so out of 100 rows, 80 rows will go into the training set, and 20 rows will go into the testing set.***","ffdd3ec3":"#### <font color=\"green\"> As we see, Poly 2 Model is breaking after 130th variable and train and test values loses direction in Poly 3 Model after 400th variable. \n    \n#### <font color=\"green\"> As Lasso Model eliminates features in function, overfitting is not happening as before.","fb3a986c":"#### <font color=\"green\">***We have a lot of missing population values in many countries. However, having GDP values from population for each country can help us as well. We also have status (Developed or Developing) for each country. Therefore, I preferred to drop column from data frame.***","99a6e640":"#### ***Getting NAN values from index***","99f3c5f6":"####  ***Looking null values***","a1f73f64":"#### ***Checking NULL values***","8869f547":"<font color=\"green\"> ***As we see on the violin graph, general Life Expectancy value is decreasing after 2010 till 2014. Let\u2019s have a look more detailed.***","08ab0fca":"#### ***About data***","b7be60d1":"### <font color=\"green\">  **After selecting the best model of Ridge Regression with 2 polynomial degree on alpha 0.000001, here we will see the results of our model by applying coefficients on each variable as an example to check our model performance.**","25d33011":"### General View in Life Expectancy by Grouping Countries with GDP Values Based on Regions","59db9a70":"#### ***Corellations between illnesses***","b3e19500":"#### ***Getting PCA Model***","00810bd4":"#### ***Looking at columns of the new merged dataset***","3c73e675":"#### <font color=\"green\"> 2nd polynomial degree gives higher Adjusted R squared values compering with 3rd polynomial degree.","d04799e3":"#### <font color=\"green\"> This type of regression is a mixed of Ridge and Lasso Regression models for a huge data set while not keeping all elements in the model. This model also eliminates unnecessary variables. ","cfca7c0b":"# <div align=\"center\">  12. Predicting with the Best Model","f868242a":"### <font color=\"green\"> ***People in Africa and Asia regions have a stable Life Expectancy in general while Ocenania and Europe regions have decreasing trend between 2010-2014.***","68cae55f":"#### Displaying 3 polynomial models with data frames ","2424dd93":"#### ***Last check on column names***","ebec881e":"##  **8.1 Winsorization**","68f580d7":"# <div align=\"center\"> 9. Feature Engineering","5d1a7a40":"# <div align=\"center\"> **5. Cleaning of the Row Data**","e4f0b7af":"#### <font color=\"green\"> ***Now data is ready for further steps.***","bf4b33f5":"# <div align=\"center\">  13. Conclusions","04b0797b":"### General Looking on Life Expectancy in Years","95018549":"###  PCA Results with all features with dummies","03ff259c":"#### ***Checking merged dataset - first 5 rows***","660135e3":"### <font color=\"green\">  We can see that having values as following 'Year': 2010,  'Adult_Mortality': 279.0,  'infant_deaths': 74.0, 'Alcohol': 0.01, 'percentage_expenditure': 79.67936736, 'Hepatitis_B': 66.0, 'Measles': 1989.0, 'BMI': 16.7, 'under_five_deaths': 102.0, 'Polio': 66.0, 'Total_Expenditure': 9.2, 'Diphtheria': 66.0, 'HIV\/AIDS': 0.1, 'GDP': 553.32894, 'thinness_1_19_years': 16.6, 'thinness_5_9_years': 6.9, 'Income_composition_of_resources': 0.45, 'Schooling': 9.2, gives the result of Life Expectancy as '61'. \n \n### <font color=\"green\">  The original value of Life Expectancy was 58.8 in 2010.  MSE Test value is 6.367 with average +-2.52 of RMSE value. Simply 61 minus 2.52 gives results as around 58 from the real value of Life Expectancy.\n    \n### <font color=\"green\">  Regression models is luckily helping us to predict our dependent variabl0 with using many parameters. In order to have an accurate result, we need to check as many as regression models. Having the lowest MSE and highest R squared values are helping us on our way. \n ","837ca51d":"# <div align=\"center\"> **4. Data Exploration**","6c266aae":"# <div align=\"center\">  10.4 Building Ridge Regression Models","5f9eb6e1":"#### ***Looking explained variance ratios***","1e65a773":"<font color=\"gray\">Country : Country\n \nYear : Year \n\nStatus : Developed or Developing status\n\nLife expectancy : Life Expectancy in age\n\nAdult Mortality : Adult Mortality Rates of both sexes (probability of dying between 15 and 60 years per 1000 population)\n\ninfant deaths : Number of Infant Deaths per 1000 population\n\n\nAlcohol          : Alcohol, recorded per capita (15+) consumption (in litres of pure alcohol)\n\nPercentage        :  Expenditure on health as a percentage of Gross Domestic Product per capita(%)\n\nHepatitis B        : Hepatitis B (HepB) immunization coverage among 1-year-olds (%)\n\nMeasles : Measles - number of reported cases per 1000 population\n\nBMI : Average Body Mass Index of entire population\n\nUnder-five deaths : Number of under-five deaths per 1000 population\n\nPolio : Polio (Pol3) immunization coverage among 1-year-olds (%)\n\nTotal expenditure : General government expenditure on health as a percentage of total government expenditure (%)\n\nDiphtheria :  Diphtheria tetanus toxoid and pertussis (DTP3) immunization coverage among 1-year-olds (%)\n\nHIV\/AIDS : Deaths per 1 000 live births HIV\/AIDS (0-4 years)\n\nGDP : Gross Domestic Product per capita (in USD)\n\nPopulation : Population of the country\n\nThinness 1-19 years : Prevalence of thinness among children and adolescents for Age 10 to 19 (% )\n\nIncome composition of resources : Human Development Index in terms of income composition of resources (index ranging from 0 to 1)\n\nThinness 5-9 years   : Prevalence of thinness among children for Age 5 to 9(%)\n\nSchooling : Number of years of Schooling(years)","77e1f5a6":"#### ***Dropping Population column***","fa4c6aac":"#### <font color=\"green\"> While Ridge Regression minimizes the sum of the squared residuals plus lambda and squaring the slope of the regression line, Lasso Regression minimizes the sum of the squared residuals, plus lambda and absolute value of slope of the regression line.\n    \n#### <font color=\"green\">In contrast, Ridge shrink the parameters by keeping all of them, Lasso Regression eliminates and creates a simpler model to explain. Therefore, I would like to have results of this model as well to have a wider range of elements for my prediction.","ad9bca31":"# <div align=\"center\"> 10. Building Regression Models","16a3d3fb":"## **9.2 Getting PCA Values for all Elements by Switching Variables to Dummies**","53d7678a":"### <font color=\"green\"> After searching different type of regression models, we have the minimum MSE and the better adjusted R\u00b2 values from  Linear Regression and Ridge Regression on two polynomial degree. Polynomial degree does not affect values on different type of regression models. \n\n\n### <font color=\"green\"> Low MSE values and highest adjusted R\u00b2 came from two polynomial degree models. Applying other type of regressions with three polynomial degree only increased MSE Test values to a higher level. Therefore, I agree to choose the Ridge Regression with two polynomial degree.","a1b66cfe":"# <div align=\"center\">  11. Evaluating the Model","2553ee2b":"#### ***Correlations Between All Variables.***","ba6baf34":"#### <font color=\"green\"> Lasso 2nd Polynomial degree model gives higher Adjusted R\u00b2 values than 3rd polynomial degree. With the same variables 3rd polynomial degree is not a good option for our regression model. ","c7e4b52f":"## **10.3 Building Polinomial Regression Models**","c94d5132":"#### ***Finding best limit for Winsorize for Each Variables***","b23dd282":"### 0. Importing Packages","1c61f6f1":"## **4.2. Merging two data frame in one, LifeExpectancyData_merged, will help us to fill in accurately.**","ff3e7a1f":"#### ***Checking NAN values after interpolate***","05d0266e":"#### Comparing All Results of our Models in one BarPlot","106a6c96":"# <div align=\"center\">  **3. General Information of the Data**","3feb5451":"### 10.1. b) Jarque Bera Test","1284c7dd":"#### An Example from a Rondom Row to Check The Model Performance","20f4c366":"## **10.2 Adding Polinomial Features**","c6ea7a67":"### <font color=\"gray\"> **Provided Information about The preparation of Row Data:**  The Global Health Observatory (GHO) data repository under World Health Organization (WHO) keeps track of the health status as well as many other related factors for all countries The datasets are made available to public for the purpose of health data analysis. The dataset related to life expectancy, health factors for 193 countries has been collected from the same WHO data repository website and its corresponding economic data was collected from United Nation website. Among all categories of health-related factors only those critical factors were chosen which are more representative. It has been observed that in the past 15 years , there has been a huge development in health sector resulting in improvement of human mortality rates especially in the developing nations in comparison to the past 30 years. Therefore, in this project we have considered data from year 2000-2015 for 193 countries for further analysis. The individual data files have been merged together into a single dataset.\n\n### <font color=\"gray\">On initial visual inspection of the data showed some missing values. As the datasets were from WHO,  None of evident errors are found. It is approved by puplishers that missing data was handled in R software by using Missmap command. The result indicated that most of the missing data was for population, Hepatitis B and GDP. The missing data were from less known countries like Vanuatu, Tonga, Togo,Cabo Verde etc. Finding all data for these countries was difficult and hence, it was decided that we exclude these countries from the final model dataset. The final merged file (final dataset) consists of 22 Columns and 2938 rows which meant 20 predicting variables. All predicting variables was then divided into several broad categories:Immunization related factors, Mortality factors, Economical factors and Social factors.\n\n### <font color=\"gray\"> In order to generate our regression models, I preferred  to merged an external data set to check  values based on  regions, sub-regions and countries to have deeper view on data. It also helped me to filled missing values accurately by using 'Sub-Region' values.\n\n### <font color=\"gray\"> **The preparation on Observations before Machine Learning:** Missing values were filled by interpolate method firstly, but the rest was filled grouping by 'Sub-Region' and 'Year' columns. ","a5fd90ea":"###  <div align=\"center\"> 7. General Looking on Life Expectancy Values Based on Regions and Years ","27b12e85":"# <div align=\"center\"> **6. Filling of the Row Data**","15151829":"**10.3 a)Checking the Performance of Models within Polynomial  Degree**","bf94964d":"#### ***Getting Data***","cbb8c2c1":"### 10.1. a)Residual Distributions on the Model","affef834":"<font color=\"green\">  ***Applying interpolate method on both direction with grouping by Country, does not help on missing values. It only helped to decrease number of missing values at once.\nOn those rows, there is no previous information for relevant countries. Thus, I used interpolte method with grouping by sub-region and Year.***","78b39ec5":"## **9.1 PCA Results with only numeric variables**","55504538":"#### ***Checking for column names for further steps***","d049fa72":"#### ***Looking new dataset - first 5 rows***","787e9dc7":"#### ***Checking for NAN values***","d2584fce":"<font color=\"green\"> There are two critical characteristics of estimators to be considered: the bias and the variance. The bias is the difference between the true population parameter and the expected estimator.\nIt measures the accuracy of the estimates. Variance, on the other hand, measures the spread, or uncertainty, in these estimates. \n\n<font color=\"green\"> So, setting \u03bb to 0 is the same as using the OLS, while the larger its value, the stronger is the coefficients' size penalized\nas \u03bb becomes larger, the variance decreases, and the bias increases.\n    \nA more traditional approach would be to choose \u03bb such that some information criterion,Akaike or Bayesian(AIC or BIC), is the smallest. A more machine learning-like approach is to perform cross-validation and select the value of \u03bb that minimizes the cross-validated sum of squared residuals.\n\nAs we see on scatter plots, True values of Poly 2 model are distributed better than Poly 3 Model on test and train group. Poly 3 Model is not enough to explain some of higher values. ","b8812fb5":"# <div align=\"center\"> **2. The Aim of Analysis**","4d68cee7":"## **4.1. Importing an External Data Frame**\n\n<font color=\"green\"> ***Further steps requires extra tools to compare such as regions and sub regions. Lack of those information does not help us to group data on necessary areas. Therefore, I imported an external data frame with only neccessary columns.***","443e3cda":"## Contents\n1.  Introduction\n2.  The Aim of Analysis\n3.  General Information of the Data\n4.  Data Exploration\n     * 4.1. Importing an External Data Frame\n     * 4.2. Merging Two Data Frame in One\n5.  Cleaning of Row Data \n6.  Filling of the Row Data \n7.  General Looking on Life Expectancy Values Based on Regions and Years\n8.  Overview about Outliers \n     * 8.1 Winsorization\n9.  Feature Engineering\n     * 9.1 Getting PCA Values\n     * 9.2 Getting PCA Values for all Elements by Switching Variables to Dummies*\n10.  Building Models\n     * 10.1 Building Model with All Numerical Variables\n      * 10.1. a)Residual Distributions on the Model\n      * 10.1. b)Jarque Bera Test\n     * 10.2 Adding Polinomial Features\n     * 10.3 Building Polinomial Regression Models\n      * 10.3 a)Checking the Best Polinomial Degree\n      * 10.3 b)Checking the Performance of Models within Polinomial Degree\n     * 10.4 Building Ridge Regression Models\n     * 10.5 Building Lasso Regression Models\n     * 10.6 Building ElasticNet Regression Models\n11. Evaluating the Model\n12. Predicting with the Best Model\n13. Conclusions","e7619c64":"#### ***Filling NAN values with interpolate method with both option as having values for some rows in each countries***","a176db2b":"<font color=\"green\">   ***As we see above 'Income_composition_of_resources' and 'Schooling' have high correlation, while  'Adult_Mortality' has high negative correlation between Life Expectancy.***\n\n\n***'HIV\/AIDS', 'BMI', 'Diphtheria', 'thinness_1_19_years', 'thinness_5_9_years', 'Polio', 'GDP', and 'Alcohol' have medium correlation between Life Expectancy.***\n\n***And the rest of our columns; 'percentage_expenditure', \u2019Hepatitis_B', 'Total_Expenditure', 'under_five_deaths', 'infant_deaths', 'Year', and 'Measles' have low correlation between Life Expectancy.***","d2787d06":"#### ***First 5 rows***","085fcf9e":"#### ***Looking at NAN values***","6469a5e1":"# 10.5 Building Lasso Regression Models","21295dc6":"#### ***Manipulating column names for future steps***","979e8371":"<font color=\"green\">   ***Life Expectancy has high negative correlation with HIV\/AIDS when only consedering correlations based on sicknesses***\n    \n<font color=\"green\">   ***It also has medium correlation with thinness_1_19_years and thinness_5_9_years, Diphtheria and Polio.***\n    \n<font color=\"green\">   ***Life Expectancy has low correlation with Hepatitis_B and Measles.***\n","d51fc8e4":"#### <font color=\"green\">While Least Squares determines values for the parameters in an equation, it minimizes the sum of the squared residuals. On the other hand, Ridge Regression minimizes the sum of the squared residuals plus lambda and the slope of the regression line.\n\n#### <font color=\"green\"> As having mostly parameters important for my prediction, I am willing to use Ridge Model as well to keep all of components in my model. \n    \n#### <font color=\"red\"> I prefered not to use \" sklearn.feature_selection.RFE(estimator, n_features_to_select=None, step=1, verbose=0)\"  considering the size of my data set. However, this function also helps to estimate the most suitable variables within number of features in selected model. ","c1d51be2":"# <div align=\"center\"> 8. Overview about Outliers and Dealing with Them","adf602da":"####  ***Getting numeric values for mathematical and statistical operations.***","db9d8ed4":"#### <font color=\"green\"> While having the same trend until 125th variable on the Poly 2 MSE results, Poly 3 MSE results shows us that after the 200th variable trend is not good any more. \n    \n#### <font color=\"green\"> Because having the low MSE value, I will continue with 2nd polynomial degree ridge Model. Later on, we also compare R squared values as well\n"}}