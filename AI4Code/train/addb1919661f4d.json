{"cell_type":{"ae413186":"code","7fd062af":"code","918ec323":"code","ac4df7d8":"code","122e8686":"code","12844450":"code","2b232f1f":"code","41a7f0b1":"code","f8e8cbf3":"code","be95fc73":"code","2c7bcdc0":"code","023d26c5":"code","6e741b25":"code","c160ce2d":"code","65471dde":"code","bc8e61da":"code","c4f7c8bc":"code","8da682a4":"code","6abb148b":"code","a202669a":"code","6dc1d95b":"code","d20f76f4":"code","fb61aaf3":"code","46b49ec2":"code","c2331ec0":"code","d4b61ccc":"code","36dd8762":"code","8c8272d8":"code","bf6d516a":"code","1ef5cc30":"code","4e7292c8":"code","e6557c45":"code","195f6ac7":"code","57aa7f44":"code","794e1d57":"code","904a5e5f":"code","e80b2477":"code","95a5750f":"code","c656b5c7":"code","917b861e":"code","8d442e71":"code","cd178e6f":"code","c32f23eb":"code","48f00301":"code","c64f5553":"code","15c92b21":"code","568e9f24":"markdown","8b7e6109":"markdown","189a22a0":"markdown","50d385dd":"markdown","d7c83cfb":"markdown","5f8603ca":"markdown","23adbba8":"markdown","59525288":"markdown","e16c7698":"markdown","b935ed5b":"markdown","96ec97d5":"markdown","2171e6e4":"markdown","c561ecf6":"markdown","a2f3eb7c":"markdown","65f6381f":"markdown"},"source":{"ae413186":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport glob\nimport pathlib\n\nimport csv\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn import preprocessing\nfrom sklearn import metrics\n\nimport sklearn.pipeline\n# import lightgbm as lgb\n\nfrom sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom lightgbm import LGBMClassifier","7fd062af":"%matplotlib inline","918ec323":"fpath = glob.glob('..\/input\/*.csv')\nssize = 0.1\ndfnames = []\nfor path in fpath:\n    with open(path,\"r\") as f:\n        reader = csv.reader(f,delimiter = \",\")\n        data = list(reader)\n        row_count = len(data)\n        del data\n    chunksize = int( row_count * ssize )\n    print(path,row_count,chunksize)\n    \n    file = path.split('\/')[2]\n    name = 'df_' + file.split('.csv')[0]\n    fcomm = name + ' = pd.read_csv(pathlib.Path(path))'#.sample( chunksize )'    \n    dfnames.append(name)\n    exec(fcomm)\n\n# del df_application_train\n# del df_application_test\n# df_application_train = pd.read_csv('..\/input\/application_train.csv')\n# df_application_test = pd.read_csv('..\/input\/application_test.csv')","ac4df7d8":"for name in dfnames:\n    fcomm = 'numrow = len(' + name + '.index)'    \n    exec(fcomm)\n    print( name, numrow )","122e8686":"\ndf_bureau.loc[df_bureau['AMT_ANNUITY'] > .8e8, 'AMT_ANNUITY'] = np.nan\ndf_bureau.loc[df_bureau['AMT_CREDIT_SUM'] > 3e8, 'AMT_CREDIT_SUM'] = np.nan\ndf_bureau.loc[df_bureau['AMT_CREDIT_SUM_DEBT'] > 1e8, 'AMT_CREDIT_SUM_DEBT'] = np.nan\ndf_bureau.loc[df_bureau['AMT_CREDIT_MAX_OVERDUE'] > .8e8, 'AMT_CREDIT_MAX_OVERDUE'] = np.nan\ndf_bureau.loc[df_bureau['DAYS_ENDDATE_FACT'] < -10000, 'DAYS_ENDDATE_FACT'] = np.nan\ndf_bureau.loc[(df_bureau['DAYS_CREDIT_UPDATE'] > 0) | (df_bureau['DAYS_CREDIT_UPDATE'] < -40000), 'DAYS_CREDIT_UPDATE'] = np.nan\ndf_bureau.loc[df_bureau['DAYS_CREDIT_ENDDATE'] < -10000, 'DAYS_CREDIT_ENDDATE'] = np.nan\n\ndf_bureau.drop(df_bureau[df_bureau['DAYS_ENDDATE_FACT'] < df_bureau['DAYS_CREDIT']].index, inplace = True)\n\ndf_previous_application.loc[df_previous_application['AMT_CREDIT'] > 6000000, 'AMT_CREDIT'] = np.nan\ndf_previous_application.loc[df_previous_application['SELLERPLACE_AREA'] > 3500000, 'SELLERPLACE_AREA'] = np.nan\ndf_previous_application[['DAYS_FIRST_DRAWING', 'DAYS_FIRST_DUE', 'DAYS_LAST_DUE_1ST_VERSION', \n             'DAYS_LAST_DUE', 'DAYS_TERMINATION']].replace(365243, np.nan, inplace = True)\n\ndf_POS_CASH_balance.loc[df_POS_CASH_balance['CNT_INSTALMENT_FUTURE'] > 60, 'CNT_INSTALMENT_FUTURE'] = np.nan\n\ndf_installments_payments.loc[df_installments_payments['NUM_INSTALMENT_VERSION'] > 70, 'NUM_INSTALMENT_VERSION'] = np.nan\ndf_installments_payments.loc[df_installments_payments['DAYS_ENTRY_PAYMENT'] < -4000, 'DAYS_ENTRY_PAYMENT'] = np.nan\n\ndf_credit_card_balance.loc[df_credit_card_balance['AMT_PAYMENT_CURRENT'] > 4000000, 'AMT_PAYMENT_CURRENT'] = np.nan\ndf_credit_card_balance.loc[df_credit_card_balance['AMT_CREDIT_LIMIT_ACTUAL'] > 1000000, 'AMT_CREDIT_LIMIT_ACTUAL'] = np.nan","12844450":"df_application_train.head(2)","2b232f1f":"df_application_train.info()","41a7f0b1":"# df_bureau1 = pd.merge( \n#              pd.merge( df_application_test,df_bureau, on='SK_ID_CURR', how='left' ),\n#                        df_bureau_balance, on='SK_ID_BUREAU', how='left' )","f8e8cbf3":"# df_bureau_balance.set_index('SK_ID_BUREAU').head(2)","be95fc73":"# def join_df(a,b,key,rsuff):\n#     temp = a.join(b.set_index(key), on=key, rsuffix=rsuff, how='left' )\n#     return temp;\n\n# def join_df_serise(dfra, b1,b2, p1,p2,p3,p4):\n#     tempb= join_df(b1,b2,'SK_ID_BUREAU','')\n#     temp = join_df(\n#            join_df(\n#            join_df(\n#            join_df(dfra,p1,'SK_ID_CURR','_p1'), \n#                         p2,'SK_ID_CURR','_p2'), \n#                         p3,'SK_ID_CURR','_p3') , \n#                         p4,'SK_ID_CURR','_p4')\n#     temp = join_df(temp,tempb,'SK_ID_CURR','_b')\n#     del tempb\n#     return temp;","2c7bcdc0":"def join_df(a,b,key,rsuff):\n    result = pd.merge(a, b.drop_duplicates(subset=key), on=key, \n                      left_index=True, how='left', sort=False, suffixes=('',rsuff))\n    print(len(result[key]))\n    return result;","023d26c5":"def join_df_serise(dfra, b1,b2, p1,p2,p3,p4):\n    \n    # Remove some empty features\n    dfra.drop(['FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_13', 'FLAG_DOCUMENT_14', \n               'FLAG_DOCUMENT_15', 'FLAG_DOCUMENT_16', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_19', 'FLAG_DOCUMENT_20', \n               'FLAG_DOCUMENT_21'], axis = 1, inplace = True)\n    \n    temp = join_df(\n           join_df(\n           join_df(\n           join_df(\n           join_df(\n           join_df(dfra,p1,'SK_ID_CURR','_p1'), \n                        p2, ['SK_ID_CURR','SK_ID_PREV'], '_p2'), \n                        p3, ['SK_ID_CURR','SK_ID_PREV'],'_p3') , \n                        p4, ['SK_ID_CURR','SK_ID_PREV'],'_p4') , \n                        b1,'SK_ID_CURR',''), \n                        b2,'SK_ID_BUREAU','_b')\n    return temp;","6e741b25":"# Remove some rows with values not present in test set\ndf_application_train.drop(df_application_train[df_application_train['CODE_GENDER'] == 'XNA'].index, inplace = True)\ndf_application_train.drop(df_application_train[df_application_train['NAME_INCOME_TYPE'] == 'Maternity leave'].index, inplace = True)\ndf_application_train.drop(df_application_train[df_application_train['NAME_FAMILY_STATUS'] == 'Unknown'].index, inplace = True)\n\ndf_train = join_df_serise ( df_application_train, \n                            df_bureau.drop('AMT_ANNUITY',axis=1),df_bureau_balance, \n                            df_previous_application,df_POS_CASH_balance,\n                            df_installments_payments,df_credit_card_balance )","c160ce2d":"df_train.info()","65471dde":"df_test = join_df_serise( df_application_test, \n                            df_bureau.drop('AMT_ANNUITY',axis=1),df_bureau_balance, \n                            df_previous_application,df_POS_CASH_balance,\n                            df_installments_payments,df_credit_card_balance )","bc8e61da":"df_train.head(2)","c4f7c8bc":"del df_application_test\ndel df_POS_CASH_balance\ndel df_credit_card_balance\ndel df_installments_payments\ndel df_application_train\ndel df_bureau\ndel df_previous_application\ndel df_bureau_balance","8da682a4":"df_train.fillna(0, inplace=True)","6abb148b":"df_train.info()","a202669a":"for col in list(df_train.columns):\n    df_list = list(df_train[col].unique())\n    \n    df_list = [x for x in df_list if x != 0]\n    x = None\n    for x in df_list:\n        if isinstance( x, str ):\n            xmax = len(df_list) + 1\n#             print(col)\n#             print(len(df_list),df_list)\n            df_train[col].replace( df_list, list(range(1,xmax)), inplace=True )\n            break","6dc1d95b":"df_train.info()","d20f76f4":"sns.pairplot(df_train[['AMT_INCOME_TOTAL','AMT_CREDIT','AMT_DOWN_PAYMENT','RATE_DOWN_PAYMENT','TARGET']].dropna(),\n             hue='TARGET',aspect=1.5)","fb61aaf3":"sns.lmplot(  x='AMT_DOWN_PAYMENT',y='AMT_CREDIT', hue='TARGET',\n                  fit_reg=False, data=df_train )\nplt.xscale('log')\nplt.xlim(10**-2,10**7)\n# plt.yscale('log')\n# plt.ylim(10**-1,10**7)\nplt.show()","46b49ec2":"sns.lmplot(  x='AMT_CREDIT_SUM_DEBT',y='AMT_CREDIT_SUM_OVERDUE', hue='TARGET',\n                  fit_reg=False, data=df_train )\nplt.xscale('log')\nplt.xlim(0.01,10**8)\nplt.yscale('log')\nplt.ylim(0.01,10**8)\nplt.show()","c2331ec0":"sns.lmplot(  x='AMT_DOWN_PAYMENT',y='RATE_DOWN_PAYMENT', hue='TARGET',\n                  fit_reg=False, data=df_train )\nplt.xscale('log')\nplt.xlim(0.01,10**7)\n# plt.yscale('log')\n# plt.ylim(10**3,10**7)\nplt.show()","d4b61ccc":"X_train = df_train.drop('TARGET',axis=1)\ny_train = df_train['TARGET']","36dd8762":"# XX_train, XX_test, yy_train, yy_test = train_test_split(X_train, y_train, test_size=0.30)","8c8272d8":"# import sklearn.pipeline\n# scaler = sklearn.preprocessing.StandardScaler()\n\n# import lightgbm as lgb\n# # train\n# gbm = lgb.LGBMRegressor(objective='binary',#'regression',\n#                         metric = 'binary_logloss',\n#                         boosting_type='gbdt',\n#                         num_leaves=1001,\n#                         learning_rate=0.0005,\n#                         n_estimators=200)\n\n# steps = [('scaler', scaler),\n#         ('GBM', gbm)]\n\n# pipeline = sklearn.pipeline.Pipeline(steps)\n\n# ### fit pipeline on X_train and y_train\n# pipeline.fit( XX_train, yy_train)\n\n# ### call pipeline.predict() on X_test data to make a set of test predictions\n# yy_gbm = pipeline.predict( XX_test )","bf6d516a":"# mean = 0.5\n# results = yy_gbm + (1-mean)\n# predictions  = list(map(int, results))\n\n# print('MAE:', metrics.mean_absolute_error(yy_test, predictions))\n# print('MSE:', metrics.mean_squared_error(yy_test, predictions))\n# print('RMSE:', np.sqrt(metrics.mean_squared_error(yy_test, predictions)))","1ef5cc30":"df_test.fillna(0, inplace=True)","4e7292c8":"for col in list(df_test.columns):\n    df_list = list(df_test[col].unique())\n    \n    df_list = [x for x in df_list if x != 0]\n    x = None\n    for x in df_list:\n        if isinstance( x, str ):\n            xmax = len(df_list) + 1\n#             print(col)\n#             print(len(df_list),df_list)\n            df_test[col].replace( df_list, list(range(1,xmax)), inplace=True )\n            break","e6557c45":"X_test = df_test","195f6ac7":"print( len(X_train.columns),len(X_test.columns) )","57aa7f44":"print( len(y_train), len(X_train['SK_ID_CURR']) )","794e1d57":"# import sklearn.pipeline\n# scaler = sklearn.preprocessing.StandardScaler()\n\n# import lightgbm as lgb\n# # train\n# # gbm = lgb.LGBMRegressor(objective='binary',#'regression',\n# #                         metric = 'binary_logloss',\n# #                         boosting_type='gbdt',\n# #                         num_leaves=1001,\n# #                         learning_rate=0.0005,\n# #                         n_estimators=200)\n\n# gbm = lgb.LGBMRegressor(\n#             nthread=4,\n#             n_estimators=50000,\n#             learning_rate=0.0001,\n#             num_leaves=34,\n#             colsample_bytree=0.9497036,\n#             subsample=0.8715623,\n#             max_depth=8,\n#             reg_alpha=0.041545473,\n#             reg_lambda=0.0735294,\n#             min_split_gain=0.0222415,\n#             min_child_weight=39.3259775,\n#             silent=-1,\n#             verbose=-1) \n\n# steps = [('scaler', scaler),\n#         ('GBM', gbm)]\n\n# pipeline = sklearn.pipeline.Pipeline(steps)\n\n# ### fit pipeline on X_train and y_train\n# pipeline.fit( X_train, y_train)\n\n# ### call pipeline.predict() on X_test data to make a set of test predictions\n# y_gbm = pipeline.predict( X_test )","904a5e5f":"# sns.distplot(y_gbm)","e80b2477":"# target = []\n# for y in y_gbm:\n#     target.append( '{:.{prec}f}'.format(y, prec=1) ) ","95a5750f":"# pd.DataFrame( { 'SK_ID_CURR':list(df_test['SK_ID_CURR']),\n#                 'TARGET':target } ).set_index('SK_ID_CURR').to_csv('sample_submission.csv', sep=',')","c656b5c7":"params = {\n         'colsample_bytree': 0.41780363323466824,\n         'learning_rate': 0.010324510220774302,\n         'num_leaves': 97,\n         'subsample': 0.8029241575078704,\n         'max_depth': 6,\n         'reg_alpha': 0.03711256722090833,\n         'reg_lambda': 0.0691714496715749,\n         'min_split_gain': 0.024536673831831966,\n         'min_child_weight': 44.94997450884206\n}","917b861e":"def train_model(data_, test_, y_, folds_):\n\n    oof_preds = np.zeros(data_.shape[0])\n    sub_preds = np.zeros(test_.shape[0])\n\n    feature_importance_df = pd.DataFrame()\n\n    feats = [f for f in data_.columns if f not in ['SK_ID_CURR']]\n    \n    for n_fold, (trn_idx, val_idx) in enumerate(folds_.split(data_, y_)):\n        trn_x, trn_y = data_[feats].iloc[trn_idx], y_.iloc[trn_idx]\n        val_x, val_y = data_[feats].iloc[val_idx], y_.iloc[val_idx]\n                \n        # LightGBM parameters found by Bayesian optimization\n        clf = LGBMClassifier(**params, n_estimators = 10000, nthread = 4)\n#         clf = LGBMClassifier(\n#             objective='binary',\n#             metric = 'auc',\n#             boosting_type='gbdt',\n#             nthread=4,\n#             num_leaves=100,\n#             learning_rate=0.03,\n#             n_estimators=1000 )\n        \n        clf.fit(\n            trn_x,\n            trn_y,\n            eval_set=[(trn_x, trn_y), (val_x, val_y)],\n            eval_metric='auc',\n            verbose=100,\n            early_stopping_rounds=100  #30\n        )\n\n        oof_preds[val_idx] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)[:, 1]\n        sub_preds += clf.predict_proba(test_[feats],\n            num_iteration=clf.best_iteration_)[:, 1] \/ folds_.n_splits\n\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = feats\n        fold_importance_df[\"importance\"] = clf.feature_importances_\n        fold_importance_df[\"fold\"] = n_fold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n        print('Fold %2d AUC : %.6f' %\n              (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n        del clf, trn_x, trn_y, val_x, val_y\n#         gc.collect()\n\n    print('Full AUC score %.6f' % roc_auc_score(y, oof_preds))\n\n    test_['TARGET'] = sub_preds\n\n    df_oof_preds = pd.DataFrame({'SK_ID_CURR':ids, 'TARGET':y, 'PREDICTION':oof_preds})\n    df_oof_preds = df_oof_preds[['SK_ID_CURR', 'TARGET', 'PREDICTION']]\n\n    return oof_preds, df_oof_preds, test_[['SK_ID_CURR', 'TARGET']], feature_importance_df, roc_auc_score(y, oof_preds)","8d442e71":"#https:\/\/www.kaggle.com\/tilii7\ndef display_importances(feature_importance_df_):\n    # Plot feature importances\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\n        \"feature\").mean().sort_values(\n            by=\"importance\", ascending=False)[:50].index\n\n    best_features = feature_importance_df_.loc[\n        feature_importance_df_.feature.isin(cols)]\n\n    plt.figure(figsize=(8, 10))\n    sns.barplot(\n        x=\"importance\",\n        y=\"feature\",\n        data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()","cd178e6f":"#https:\/\/www.kaggle.com\/tilii7\ndef display_roc_curve(y_, oof_preds_, folds_idx_):\n    # Plot ROC curves\n    plt.figure(figsize=(6, 6))\n    scores = []\n    for n_fold, (_, val_idx) in enumerate(folds_idx_):\n        # Plot the roc curve\n        fpr, tpr, thresholds = roc_curve(y_.iloc[val_idx], oof_preds_[val_idx])\n        score = roc_auc_score(y_.iloc[val_idx], oof_preds_[val_idx])\n        scores.append(score)\n        plt.plot(\n            fpr,\n            tpr,\n            lw=1,\n            alpha=0.3,\n            label='ROC fold %d (AUC = %0.4f)' % (n_fold + 1, score))\n\n    plt.plot(\n        [0, 1], [0, 1],\n        linestyle='--',\n        lw=2,\n        color='r',\n        label='Luck',\n        alpha=.8)\n    fpr, tpr, thresholds = roc_curve(y_, oof_preds_)\n    score = roc_auc_score(y_, oof_preds_)\n    plt.plot(\n        fpr,\n        tpr,\n        color='b',\n        label='Avg ROC (AUC = %0.4f $\\pm$ %0.4f)' % (score, np.std(scores)),\n        lw=2,\n        alpha=.8)\n\n    plt.xlim([-0.05, 1.05])\n    plt.ylim([-0.05, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('LightGBM ROC Curve')\n    plt.legend(loc=\"lower right\")\n    plt.tight_layout()","c32f23eb":"#https:\/\/www.kaggle.com\/tilii7\ndef display_precision_recall(y_, oof_preds_, folds_idx_):\n    # Plot ROC curves\n    plt.figure(figsize=(6, 6))\n\n    scores = []\n    for n_fold, (_, val_idx) in enumerate(folds_idx_):\n        # Plot the roc curve\n        fpr, tpr, thresholds = roc_curve(y_.iloc[val_idx], oof_preds_[val_idx])\n        score = average_precision_score(y_.iloc[val_idx], oof_preds_[val_idx])\n        scores.append(score)\n        plt.plot(\n            fpr,\n            tpr,\n            lw=1,\n            alpha=0.3,\n            label='AP fold %d (AUC = %0.4f)' % (n_fold + 1, score))\n\n    precision, recall, thresholds = precision_recall_curve(y_, oof_preds_)\n    score = average_precision_score(y_, oof_preds_)\n    plt.plot(\n        precision,\n        recall,\n        color='b',\n        label='Avg ROC (AUC = %0.4f $\\pm$ %0.4f)' % (score, np.std(scores)),\n        lw=2,\n        alpha=.8)\n\n    plt.xlim([-0.05, 1.05])\n    plt.ylim([-0.05, 1.05])\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('LightGBM Recall \/ Precision')\n    plt.legend(loc=\"best\")\n    plt.tight_layout()","48f00301":"data = X_train\ntest = X_test\ny    = y_train\nids  = X_train['SK_ID_CURR']\n\n# Create Folds\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1001)\n# Train model and get oof and test predictions\noof_preds, df_oof_preds, test_preds, importances, score = train_model(data, test, y, folds)\n# Display a few graphs\ndisplay_importances(feature_importance_df_=importances)\nfolds_idx = [(trn_idx, val_idx)\n             for trn_idx, val_idx in folds.split(data, y)]\ndisplay_roc_curve(y_=y, oof_preds_=oof_preds, folds_idx_=folds_idx)\ndisplay_precision_recall(y_=y, oof_preds_=oof_preds, folds_idx_=folds_idx)","c64f5553":"test_preds.set_index('SK_ID_CURR').to_csv('sample_submission.csv', sep=',')","15c92b21":"sns.distplot(test_preds['TARGET'])","568e9f24":"## EDA","8b7e6109":"## TRAINING DATA SET","189a22a0":"**Let's split up the data into a training set and a test set!**","50d385dd":"## Fit & Test","d7c83cfb":"## Train","5f8603ca":"## TEST DATA SET - Preprocessing\n**(a) Train and Test Datasets**","23adbba8":"## Data Manipulation","59525288":"## DATA READING","e16c7698":"# Main Work Flow\n- **Import libraries**\n- **Cleaning of training dataset**\n- **Choice of method**\n- **Preprocessing of testing dataset**\n- **Transformation and post-processing**","b935ed5b":"## Saving predicted data to a file","96ec97d5":"# Validation\n- The test data doesn't include y_train values \n- Test few methodologies to model the system \n- Test the accuracy of the methods","2171e6e4":"**Input File Names**","c561ecf6":"## Data manupulation outliers","a2f3eb7c":"**Descriptive stats about the data set**","65f6381f":"# TEST Data"}}