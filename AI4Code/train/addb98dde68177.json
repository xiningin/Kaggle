{"cell_type":{"8ce466f1":"code","932c19ca":"code","99ff62f5":"code","b30ea1a4":"code","95f2f84b":"code","6dee4e54":"code","68762549":"code","e2d55d99":"code","11bf4153":"markdown","ca86344b":"markdown","bac4d25f":"markdown","a3a0f61b":"markdown","161c336d":"markdown","a0365d66":"markdown"},"source":{"8ce466f1":"import imageio\nfrom PIL import Image\nimport cv2\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport subprocess\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n%matplotlib inline\nplt.rcParams['figure.dpi'] = 150\n\nimport seaborn as sns\n\nfrom IPython.display import Video, display\n\n#block those warnings from pandas about setting values on a slice\nimport warnings\nwarnings.filterwarnings('ignore')\n","932c19ca":"# Read in the video labels file\nvideo_labels = pd.read_csv('..\/input\/nfl-impact-detection\/train_labels.csv')\nvideo_labels.head()","99ff62f5":"# Define the video we'll process\nvideo_name = video_labels['video'][0]\nvideo_name","b30ea1a4":"## Show original video","95f2f84b":"# Define the path and then display the video using \nvideo_path = f\"..\/input\/nfl-impact-detection\/train\/{video_name}\"\ndisplay(Video(data=video_path, embed=True))","6dee4e54":"# Create a function to annotate the video at the provided path using labels from the provided dataframe, return the path of the video\ndef annotate_video(video_path: str, video_labels: pd.DataFrame, blind=False, speed=1) -> str:\n    VIDEO_CODEC = \"MP4V\"\n    HELMET_COLOR = (255, 255, 255)    # Black\n    IMPACT_COLOR = (0, 0, 255)  # Red\n    video_name = os.path.basename(video_path)\n    \n    vidcap = cv2.VideoCapture(video_path)\n    width = int(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    output_path = \"labeled_\" + video_name\n    tmp_output_path = \"tmp_\" + output_path\n    fps = 60 * speed\n    output_video = cv2.VideoWriter(tmp_output_path, cv2.VideoWriter_fourcc(*VIDEO_CODEC), fps, (width, height))\n    frame = 0\n    while True:\n        it_worked, img = vidcap.read()\n        if not it_worked:\n            break\n        if blind:\n            img = img * 0\n        \n        # We need to add 1 to the frame count to match the label frame index that starts at 1\n        frame += 1\n        \n        # Let's add a frame index to the video so we can track where we are\n        img_name = f\"{video_name}_frame{frame}\"\n        cv2.putText(img, img_name, (0, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, HELMET_COLOR, thickness=2)\n    \n        # Now, add the boxes\n        boxes = video_labels.query(\"video == @video_name and frame == @frame\")\n        for box in boxes.itertuples(index=False):\n            if box.impact == 1 and box.confidence > 1 and box.visibility > 0:    # Filter for definitive head impacts and turn labels red\n                color, thickness = IMPACT_COLOR, 2\n            else:\n                color, thickness = HELMET_COLOR, 1\n            # Add a box around the helmet\n            cv2.rectangle(img, (box.left, box.top), (box.left + box.width, box.top + box.height), color, thickness=thickness)\n            cv2.putText(img, box.label, (box.left, max(0, box.top - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, thickness=1)\n        output_video.write(img)\n    output_video.release()\n    \n    # Not all browsers support the codec, we will re-load the file at tmp_output_path and convert to a codec that is more broadly readable using ffmpeg\n    if os.path.exists(output_path):\n        os.remove(output_path)\n    subprocess.run([\"ffmpeg\", \"-i\", tmp_output_path, \"-crf\", \"18\", \"-preset\", \"veryfast\", \"-vcodec\", \"libx264\", output_path])\n    os.remove(tmp_output_path)\n    \n    return output_path","68762549":"# Label the video and display it - this will take a bit\nlabeled_video = annotate_video(f\"..\/input\/nfl-impact-detection\/train\/{video_name}\", video_labels, speed=0.5)\ndisplay(Video(data=labeled_video, embed=True))","e2d55d99":"# Label the video and display it - this will take a bit\nlabeled_video = annotate_video(f\"..\/input\/nfl-impact-detection\/train\/{video_name}\", video_labels, blind=True, speed=0.3)\ndisplay(Video(data=labeled_video, embed=True))","11bf4153":"# Set up environment","ca86344b":"# Introduction\n\n**Main Topic**\n\nThis notebook is for **Visualizing Impacting Moment** with blind and config speed from `train` *.mp4.\n\nYou can analysis **Impact Moment** from the video.\n\nMain source code is from References, I just a little bit customize the module.\n\n**References**\n\n**Sam Huddleston** : [NFL 1st and Future Getting Started](https:\/\/www.kaggle.com\/samhuddleston\/nfl-1st-and-future-getting-started)","bac4d25f":"## Define annotate_video\n\nI just added blind & speed parameters from Original NB [NFL 1st and Future Getting Started](https:\/\/www.kaggle.com\/samhuddleston\/nfl-1st-and-future-getting-started)","a3a0f61b":"## Set Speed = 0.5 ","161c336d":"# Import video_labels","a0365d66":"## Set Blind Video, Speed = 0.3"}}