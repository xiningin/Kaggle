{"cell_type":{"c057744e":"code","233d6452":"code","84c05714":"code","64b2af2c":"code","c3c7c526":"code","b83c6f31":"code","1977995b":"code","b58e8f72":"code","7caf96b7":"code","2035dce6":"code","5e2407a8":"code","322f490f":"code","5e71bce9":"code","54118973":"code","cd18788c":"code","9cb53fb0":"code","9955d0ec":"code","cdafc586":"code","27d8b737":"code","131ca57a":"code","cc596d85":"code","aa1812e0":"code","c856b09a":"code","6ce1bd38":"code","53d2fcd9":"code","ff4df1ca":"code","245e7d5f":"code","5af7a76d":"code","2e01b6d1":"code","e8d73e56":"code","8a3b5b73":"code","883a601c":"code","2f80769d":"code","b1819432":"code","4ec1f02c":"code","3f6ffa62":"code","5e45d8fa":"code","17a1d187":"code","fd293e25":"code","7807f390":"code","7fdca4f8":"code","b5ebfe06":"code","45e853fd":"code","1a1d870e":"code","3a665431":"code","403a2425":"code","2e2e1aba":"code","d7ab7255":"code","ae3a7406":"code","0d9ec83d":"code","59b0fe89":"code","d0c77371":"code","99581c01":"code","df968566":"code","e4d213bb":"code","23afdac7":"code","fceb14a1":"markdown","ba1688f4":"markdown","034a4264":"markdown","ff65f91a":"markdown","17eadb36":"markdown","904a807d":"markdown","56e0a268":"markdown","252ca9ab":"markdown","4528d385":"markdown","fe123193":"markdown","f8ef6367":"markdown","c9e525e4":"markdown","7113a7c9":"markdown","d1415968":"markdown","cd0d714d":"markdown","6a943b7e":"markdown","f54201a0":"markdown","a6a5d0e1":"markdown","0e6bd31f":"markdown","992f28ae":"markdown","631af7c4":"markdown"},"source":{"c057744e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","233d6452":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use(\"fivethirtyeight\")\nsns.set_style(\"darkgrid\")","84c05714":"from sklearn.model_selection import KFold, cross_validate, train_test_split\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom sklearn import metrics\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom catboost import CatBoostRegressor\n\nfrom sklearn import model_selection\nfrom sklearn.ensemble import StackingRegressor\n\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import median_absolute_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error","64b2af2c":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\")","c3c7c526":"display(train.head())\ndisplay(test.head())\ndisplay(submission.head())","b83c6f31":"display(train.shape)\ndisplay(test.shape)\ndisplay(submission.shape)","1977995b":"display(train.info())\ndisplay(test.info())","b58e8f72":"train_original = train.copy()\ntest_original = test.copy()","7caf96b7":"train.drop(['id'], axis=1, inplace=True)\ntest.drop(['id'], axis=1, inplace=True)","2035dce6":"train.describe()","5e2407a8":"print('Target column basic statistics:')\ntrain['loss'].describe()","322f490f":"print(\"total unique values for loss:\", train['loss'].nunique())\nprint(\"\\n\\n Unique values:\\n\\n\", train['loss'].value_counts())","5e71bce9":"train['loss'].quantile([0.25, 0.5 , 0.75,0.90])","54118973":"plt.figure(figsize=(15,9))\nsns.countplot(data=train, x='loss');","cd18788c":"fig, ax = plt.subplots(figsize=(12, 8))\n\nsns.kdeplot(train['loss'], shade=True, color='blue', Label='Target-loss')\nplt.title(\"Distribution of the Target\", fontsize=\"large\", fontweight=\"bold\", size=20)\n\n# Setting the X and Y Label \nplt.xlabel('Target Loss') \nplt.ylabel('Probability Density')\n\nfig.text(\n    0.4,\n    0.5,\n    \"\"\"\nThe target has a skewed distribution.\n\"\"\",\n    bbox=dict(boxstyle=\"round\", fc=\"#009473\"),\n    fontsize=\"medium\",\n)\n\nplt.show()","9cb53fb0":"plt.figure(figsize = (12, 12))\ncorr = train.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, cmap='twilight_r', robust=True, center=0,square=True, linewidths=.6)\nplt.title(\"Correlation\")\nplt.show()","9955d0ec":"correlations = train.corr()['loss'].sort_values()\nprint(\"Lowest correlation features:\\n\")\nprint(correlations.head(15), \"\\n\")\nprint(\"Highest correlation features:\\n\")\nprint(correlations.tail(15))","cdafc586":"# Plot feature correlations\nplt.figure(figsize = (24, 8))\ncorr[\"loss\"][:-1].plot(kind=\"bar\",grid=True)\nplt.title(\"Features Correlation\")","27d8b737":"train.corr()['loss'].sort_values(ascending=False).head(50)\ndf_corr_train = train.corr()['loss'].sort_values(ascending=False)\n\nplt.figure(figsize=(18,25))\ndf_corr_train.plot(kind='barh')\nplt.show()","131ca57a":"# Checking the skewness of \"f77\" attributes\nplt.figure(figsize=(7,5))\nsns.distplot(train['f77'])\nSkew_f77 = train['f77'].skew()\nplt.title(\"Skew:\"+str(Skew_f77), size=15, fontweight='bold')\nplt.show()","cc596d85":"skew_feats = train.skew().sort_values(ascending=False).head(50)\n\nskewness = pd.DataFrame({'Skew':skew_feats})\nskewness","aa1812e0":"# Listing Number of missing values by feature column wise\ntotal = train.isnull().sum().sort_values(ascending=False)\ntotal = total[total > 0]\ntotal","c856b09a":"train.isnull().sum().sum()","6ce1bd38":"train.duplicated().value_counts()","53d2fcd9":"test.duplicated().value_counts()","ff4df1ca":"# Remove all duplicates\ntrain.drop_duplicates(inplace=True)\nprint(\"Dataset size before removing duplicates:\", train_original.shape)\nprint(\"Dataset size after removing duplicates:\", train.shape)","245e7d5f":"plt.figure(figsize=(22,55))\nsns.boxplot(data=train, orient=\"h\");\nplt.xscale('log')","5af7a76d":"X = train.drop('loss', axis = 1)\ny = train['loss']","2e01b6d1":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)","e8d73e56":"LGBM = LGBMRegressor(learning_rate= 0.07, max_depth= 8, n_estimators= 200, objective='regression', n_jobs = -1)\nLGBM.fit(X_train, y_train)","8a3b5b73":"pred_lgbm = LGBM.predict(X_test)\npred_lgbm_test = LGBM.predict(test)","883a601c":"rmse_LGBM = mean_squared_error(y_test, pred_lgbm, squared=False)\nprint('MSE score: ', rmse_LGBM)","2f80769d":"XGB = XGBRegressor(learning_rate = 0.05, \n                   n_estimators = 200,\n                   min_child_weight = 11,)\nXGB.fit(X_train, y_train)","b1819432":"pred_xgb = XGB.predict(X_test)\npred_xgb_test = XGB.predict(test)","4ec1f02c":"rmse_xgb = mean_squared_error(y_test, pred_xgb, squared=False)\nprint('MSE score: ', rmse_xgb)","3f6ffa62":"fig = plt.figure(figsize=(24,34))\nax = plt.axes()\nxgb.plot_importance(XGB, ax)","5e45d8fa":"Cat = CatBoostRegressor(random_state=42,iterations = 5000,learning_rate=0.005, early_stopping_rounds=50)\nCat.fit(X_train, y_train, verbose = 0)","17a1d187":"pred_cat = Cat.predict(X_test)\npred_cat_test = Cat.predict(test)","fd293e25":"rmse_cat = mean_squared_error(y_test, pred_cat, squared=False)\nprint('MSE score: ', rmse_cat)","7807f390":"ensembled = pred_cat_test*0.4 + pred_lgbm_test *0.4 + pred_xgb_test *0.2","7fdca4f8":"submission['loss'] = ensembled\nsubmission","b5ebfe06":"submission.to_csv(\"submissionensemble.csv\", index = False)","45e853fd":"# Define base learners\nmyclf1 = LGBMRegressor()\nmyclf2 = XGBRegressor()\nmyclf3 = CatBoostRegressor()","1a1d870e":"# Define meta model\nmylr = CatBoostRegressor()","3a665431":"from mlxtend.regressor import StackingCVRegressor\nstack = StackingCVRegressor(regressors=(myclf1, myclf2, myclf3), meta_regressor= mylr, use_features_in_secondary=True)\n\nstack.fit(X_train, y_train)","403a2425":"pred_stack = stack.predict(X_test)\npred_stack_test = stack.predict(test)","2e2e1aba":"rmse_stack = mean_squared_error(y_test, pred_stack, squared=False)\nprint('MSE score: ', rmse_stack)","d7ab7255":"submission['loss'] = pred_stack_test\nsubmission","ae3a7406":"submission.to_csv(\"submissionstack.csv\", index = False)","0d9ec83d":"import datatable as dt\n\ntrain_ML = dt.fread(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv\")\ntest_ML = dt.fread(\"\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv\")","59b0fe89":"!python3 -m pip install -q lightautoml\n\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML\nfrom lightautoml.tasks import Task","d0c77371":"target = train_ML['loss'].to_numpy().ravel()\n\ndel train_ML[:, ['id', 'loss']]\ntest_ids = test_ML[:, 'id']\ntest = test_ML[:, train_ML.names]","99581c01":"train_ML['target'] = dt.Frame(target)\n\nmodel = TabularAutoML(task=Task('reg'), timeout=500, verbose=2)\n\nmodel.fit_predict(train_data=train_ML.to_pandas(), roles={'target': 'target'})\n\ndel train_ML['target']","df968566":"preds = model.predict(test_ML.to_pandas()).data.ravel()","e4d213bb":"submission = dt.Frame(id=test_ids, loss=preds)\n\nsubmission.head()","23afdac7":"submission.to_csv(\"submission_AutoML.csv\")","fceb14a1":"* [1) Import Required Libraries](#1)\n\n* [2) Read Data](#2)\n\n* [3) EDA](#3)\n\n * [3.1) AutoViz](#3.1)\n \n* [4) Feature Engineering](#4)\n\n * [4.1) Missing Values](#4.1)\n \n * [4.2) Outliers](#4.2)\n\n* [5) Model building and Evaluation](#5)\n\n * [5.1) XGBoost Regressor](#5.1)\n\n* [6) AutoML](#6)","ba1688f4":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left; border-radius: 0px 0px;\"> 3.1) Relation between Features <\/h1>","034a4264":"## Baseline","ff65f91a":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 6) AutoML <\/h1>","17eadb36":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> Table of Contents <\/h1>","904a807d":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 2) Read Data <\/h1>","56e0a268":"### Stacking","252ca9ab":"<h1 style=\"background-color:LightBlue; font-family:newtimeroman; font-size:180%; text-align:left; border-radius: 0px 0px;\"> 5.1) LightGBM <\/h1>","4528d385":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 3) EDA <\/h1>","fe123193":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left; border-radius: 0px 0px;\"> 3.3) Skewness and Kurtosis <\/h1>","f8ef6367":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 5) Model building and Evaluation <\/h1>","c9e525e4":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left; border-radius: 0px 0px;\"> 4.1) Missing Values <\/h1>","7113a7c9":"<h1 style=\"background-color:LightBlue; font-family:newtimeroman; font-size:180%; text-align:left; border-radius: 0px 0px;\"> 5.2) XGRegressor <\/h1>","d1415968":"## Ensemble","cd0d714d":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 1) Import Required Libraries <\/h1>","6a943b7e":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left; border-radius: 0px 0px;\"> 4.2) Outliers <\/h1>","f54201a0":"### Target Column","a6a5d0e1":"<h1 style=\"background-color:LimeGreen; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;\"> 4) Feature Engineering <\/h1>","0e6bd31f":"<h1 style=\"background-color:LightBlue; font-family:newtimeroman; font-size:180%; text-align:left; border-radius: 0px 0px;\"> 5.3) CatBoost Regressor <\/h1>","992f28ae":"### INTRODUCTION\n\nKaggle competitions are incredibly fun and rewarding, but they can also be intimidating for people who are relatively new in their data science journey. In the past, Kaggle have launched many Playground competitions that are more approachable than Featured competition, and thus more beginner-friendly.\n\nThe goal of these competitions is to provide a fun, but less challenging, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition.\n\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with calculating the loss associated with a loan defaults. Although the features are anonymized, they have properties relating to real-world features.\n\nSubmissions are evaluated using Submissions are scored on the root mean squared error. RMSE is defined as:\n\nRMSE=1n\u2211i=1n(yi\u2212yi^)2\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u221a\n \nwhere  yi^  is the predicted value,  y  is the ground truth value, and  n  is the number of rows in the test data.","631af7c4":"<h1 style=\"background-color:skyblue; font-family:newtimeroman; font-size:170%; text-align:left; border-radius: 0px 0px;\"> 3.2) The correlation between this continuos features and the target <\/h1>"}}