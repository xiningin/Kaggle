{"cell_type":{"1c2ea517":"code","30083dad":"code","bd8cbd72":"code","5be8c772":"code","d3532516":"code","889c6adc":"code","3f86d5ea":"code","8ebc1ae7":"code","fdbaf3c1":"code","532e0202":"code","ca3d59a7":"code","add842df":"code","f9ccca8a":"code","e66e9c31":"code","9dfbd27d":"code","0fef6def":"code","8e149b93":"code","7789d578":"code","0de29ef3":"code","06481df1":"code","424099aa":"markdown","54f7c4f3":"markdown","a8c88a69":"markdown","d2a30af7":"markdown","fd1762e8":"markdown","4c16c09e":"markdown","e6976e29":"markdown","d65ffb81":"markdown","46354e1c":"markdown","b676b56d":"markdown","9988bc37":"markdown","02b09fe1":"markdown","eb2aa30c":"markdown","93302f12":"markdown","0de5abbb":"markdown","c36f7e68":"markdown","671f1765":"markdown","e3970614":"markdown"},"source":{"1c2ea517":"import numpy as np\nimport pandas as pd\nimport os\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n!pip install nlpaug\nimport nlpaug.augmenter.word as naw\nfrom tqdm import tqdm\nfrom transformers import XLMRobertaTokenizer, TFXLMRobertaModel\nimport matplotlib.pyplot as plt\nimport plotly.express as px","30083dad":"try: # running on TPU\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Running on TPU ', tpu.master())\nexcept ValueError: # running on GPU\/CPU\n    print('Not running on TPU')\n    strategy = tf.distribute.get_strategy()\nprint(\"REPLICAS:\", strategy.num_replicas_in_sync)","bd8cbd72":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain_df = pd.read_csv('..\/input\/contradictory-my-dear-watson\/train.csv')\ntest_df = pd.read_csv('..\/input\/contradictory-my-dear-watson\/test.csv')\nsubmission_df = pd.read_csv('..\/input\/contradictory-my-dear-watson\/sample_submission.csv')\ntrain_df.head()","5be8c772":"test_df.head()","d3532516":"submission_df.head()","889c6adc":"train_text_df, val_text_df, train_label, val_label = train_test_split(\n    train_df[['premise', 'hypothesis']], train_df['label'].values.tolist(),\n    test_size=0.2)\n\ntest_text_df = test_df[['premise', 'hypothesis']]","3f86d5ea":"syn_aug = naw.SynonymAug()\n\noriginal_sample = train_df['premise'].iloc[1]\nprint('Original:  {}'.format(original_sample))\n\naugmented_sample = syn_aug.augment(original_sample)\nprint('Augmented: {}'.format(augmented_sample))","8ebc1ae7":"new_prem_text = []\nnew_hypo_text = []\nnew_label = []\n\nfor i in tqdm(np.random.randint(0, len(train_text_df), len(train_text_df) \/\/ 4)): # divide by 4 to get 25%\n    prem_text = train_text_df.iloc[i]['premise']\n    hypo_text = train_text_df.iloc[i]['hypothesis']\n    \n    # augment both the premise and the hypothesis\n    aug_prem_text = syn_aug.augment(prem_text) \n    aug_hypo_text = syn_aug.augment(hypo_text)\n\n    new_prem_text.append(aug_prem_text)\n    new_hypo_text.append(aug_hypo_text)\n    \n    # match label to new texts\n    text_label = train_label[i]\n    new_label.append(text_label)\n\naug_train_text_df = train_text_df.append(pd.DataFrame({'premise': new_prem_text, 'hypothesis': new_hypo_text}))\naug_train_label = train_label + new_label\n\nprint('Amount before augmentation: {} texts, {} labels.'.format(train_text_df.shape[0], len(train_label)))\nprint('Amount after augmentation:  {} texts, {} labels.'.format(aug_train_text_df.shape[0], len(aug_train_label)))","fdbaf3c1":"PRETRAINED_MODEL = 'jplu\/tf-xlm-roberta-large'\n\ntokenizer = XLMRobertaTokenizer.from_pretrained(PRETRAINED_MODEL)","532e0202":"train_encoded_temp = tokenizer(\n    train_text_df.values.tolist(),\n    return_length=True)\n\nplt.hist(train_encoded_temp.length, bins=25)\nplt.show()","ca3d59a7":"temp_df = pd.DataFrame(train_encoded_temp.length, columns=['length'])\ntemp_df.describe(percentiles=[.75,.9,.95,.99])","add842df":"MAX_LENGTH = 80","f9ccca8a":"train_encoded = tokenizer(\n    aug_train_text_df.values.tolist(), \n    max_length=MAX_LENGTH, \n    padding='max_length', \n    truncation='longest_first')\nval_encoded = tokenizer(\n    val_text_df.values.tolist(), \n    max_length=MAX_LENGTH, \n    padding='max_length', \n    truncation='longest_first')\ntest_encoded = tokenizer(\n    test_text_df.values.tolist(),\n    max_length=MAX_LENGTH, \n    padding='max_length', \n    truncation='longest_first')\n\ntrain_ids = train_encoded['input_ids']\nval_ids = val_encoded['input_ids']\ntest_ids = test_encoded['input_ids']","e66e9c31":"BATCH_SIZE = 16 * strategy.num_replicas_in_sync # for each TPU unit\nAUTO = tf.data.experimental.AUTOTUNE\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((train_ids, aug_train_label))\n    .shuffle(len(train_ids), reshuffle_each_iteration=True) # for perfect shuffle\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO))\nval_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((val_ids, val_label))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO))\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(test_ids)\n    .batch(BATCH_SIZE))","9dfbd27d":"with strategy.scope():\n    # setup XLM-RoBERTa model\n    transformer_encoder = TFXLMRobertaModel.from_pretrained(PRETRAINED_MODEL)\n\n    # take in inputs and generate necessary outputs\n    input_ids = tf.keras.layers.Input(shape=(MAX_LENGTH,), dtype=tf.int32, name='input_ids')\n    sequence_output = transformer_encoder(input_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    \n    # prevent overfitting\n    dropout = tf.keras.layers.Dropout(0.3)(cls_token) # prevent overfitting\n    batch_norm = tf.keras.layers.BatchNormalization()(dropout) # prevent overfitting\n    \n    output = tf.keras.layers.Dense(3, activation='softmax')(batch_norm)\n\n    model = tf.keras.Model(inputs=input_ids, outputs=output)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(lr=1e-6), # default lr doesn't converge\n        loss='sparse_categorical_crossentropy', \n        metrics=['accuracy'])\n\nmodel.summary()","0fef6def":"early_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience= 10, # stops training after 10 consecutive non-decreases in min val loss\n    verbose=1,\n    mode='auto',\n    restore_best_weights=True)","8e149b93":"EPOCHS = 100\n\ntrain_history = model.fit(\n    train_dataset,\n    epochs=EPOCHS,\n    callbacks=[early_stopping],\n    validation_data=val_dataset)","7789d578":"history = train_history.history\n\npx.line(\n    history, \n    x=range(1, len(history['loss'])+1), \n    y=['loss', 'val_loss'], \n    title='Model Loss', \n    labels={'x': 'Epoch', 'value': 'Loss'}\n)","0de29ef3":"px.line(\n    history, \n    x=range(1, len(history['loss'])+1), \n    y=['accuracy', 'val_accuracy'], \n    title='Model Accuracy', \n    labels={'x': 'Epoch', 'value': 'Accuracy'}\n)","06481df1":"test_preds = model.predict(test_dataset, verbose=1)\nsubmission_df['prediction'] = test_preds.argmax(axis=1)\nsubmission_df.head()\nsubmission_df.to_csv('submission.csv', index=False)","424099aa":"## Data Augmentation\n### Synonym Augmenter\nWe perform data augmentation in order to increase the size of our training set and prevent overfitting. As we are dealing with text data, we use a **synonym augmenter** from the [\"nlpaug\"](https:\/\/pypi.org\/project\/nlpaug\/0.0.9\/) library. Below is an example.\n\n\ud6c8\ub828 \ub370\uc774\ud130\uc14b\uc744 \ub354 \ub298\ub824\uc11c \uacfc\uc801\ud569\uc744 \uc608\ubc29\ud558\uae30 \uc704\ud574 \ub370\uc774\ud130 \uc99d\uac15\uc744 \uc801\uc6a9\ud569\ub2c8\ub2e4. \ud14d\uc2a4\ud2b8 \ub370\uc774\ud130\uc774\ubbc0\ub85c [\"nlpaug\"](https:\/\/pypi.org\/project\/nlpaug\/0.0.9\/) \ub77c\uc774\ube0c\ub7ec\ub9ac\uc758 **synonym augmenter**\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc544\ub798\uc5d0 \uc608\uc2dc\uac00 \uc788\uc2b5\ub2c8\ub2e4.","54f7c4f3":"# Model Performance\nWe examine the model training's validation loss and accuracy to see how well it learned.\n\n\ubaa8\ub378\uc774 \uc5bc\ub9c8\ub098 \uc798 \ud559\uc2b5\ub418\uc5c8\ub294\uc9c0 \uc54c\uc544\ubcf4\uae30 \uc704\ud574 \uac80\uc99d \uc190\uc2e4\uac12\uacfc \uc815\ud655\ub3c4\ub97c \uc0b4\ud3b4\ubd05\ub2c8\ub2e4.","a8c88a69":"# Test Predictions\nWe make predictions on the test dataset to submit to the competition.\n\n\ub300\ud68c\uc5d0 \uc81c\ucd9c\ud558\uae30 \uc704\ud574 \uc2dc\ud5d8 \ub370\uc774\ud130\uc14b\uc744 \ubaa8\ub378\uc5d0 \uc785\ub825\ud574\uc11c \uc608\uce21\uac12\ub4e4\uc744 \uc5bb\uc5b4\ub0c5\ub2c8\ub2e4.","d2a30af7":"## Load Data\nWe load and check the given data.\n\n\uc785\ub825 \ub370\uc774\ud130\ub97c \uc77d\uc5b4\ub4e4\uc774\uace0 \ud655\uc778\ud574\ubd05\ub2c8\ub2e4.","fd1762e8":"A synonym augmenter substitutes text data with a corresponding synonym listed in the WordNet database. For our model training, we decide to add about 25% of augmented data to the training set. We **do not apply this data augmentation to the validation dataset** because we should evaluate our model on data closest to real data.\n\n\ub3d9\uc758\uc5b4 augmenter\ub294 WordNet \ub370\uc774\ud130\ubca0\uc774\uc2a4\uc5d0 \uae30\ub85d\ub41c \ud14d\uc2a4\ud2b8 \ub370\uc774\ud130 \ub2e8\uc5b4\uc758 \ub3d9\uc758\uc5b4\ub85c \ub300\uccb4\ud574\uc90d\ub2c8\ub2e4. \uc6b0\ub9ac\uc758 \ubaa8\ub378 \ud6c8\ub828\uc744 \uc704\ud574 \ud6c8\ub828 \ub370\uc774\ud130\uc14b\uc758 25%\ub97c \uc99d\uac15 \ub370\uc774\ud130\ub85c \ucd94\uac00\ud558\uae30\ub85c \ud569\ub2c8\ub2e4. \uc8fc\uc758\ud560 \uac83\uc740 \ubaa8\ub378 \uac80\uc99d\uc744 \uc2e4\uc81c \ub370\uc774\ud130\uc5d0 \uac00\uc7a5 \uac00\uae4c\uc6b4 \ub370\uc774\ud130\ub85c \ud574\uc57c\ud558\ubbc0\ub85c \uc774 **\ub370\uc774\ud130 \uc99d\uac15\uc744 \uac80\uc99d \ub370\uc774\ud130\uc14b\uc5d0 \uc801\uc6a9\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4**. ","4c16c09e":"# Model Training\n## Creating and Loading Model onto TPU\nWe setup the model within a *strategy.scope()* in order to use the TPU. We add dropout and batch normalization layers in order to prevent overfitting, and decrease the Adam optimizer's learning rate to 1e-5 as the default learning rate of 1e-3 seems to cause the model to diverge.\n\n\ubaa8\ub378 \ud6c8\ub828\uc744 TPU\ub85c \ud558\uae30 \uc704\ud574 *strategy.scope()* \uc548\uc5d0\uc11c \uad6c\ucd95\ud569\ub2c8\ub2e4. \uacfc\uc801\ud569\uc744 \uc608\ubc29\ud558\uae30 \uc704\ud574 dropout\uacfc batch normalization layer\ub4e4\uc744 \ucd94\uac00\ud558\uace0, Adam optimizer\uc758 \uae30\ubcf8 \ud559\uc2b5\ub960 1e-3\uc740 \ubaa8\ub378\uc774 diverge\ud558\uac8c \ub9cc\ub4dc\ub294 \uac83\uc73c\ub85c \ubcf4\uc774\uae30\uc5d0 \ud559\uc2b5\ub960\uc744 1e-5\ub85c \ub0ae\ucdb0\uc90d\ub2c8\ub2e4.","e6976e29":"# NLI with XLM-RoBERTa \nThis Kaggle kernel is a submission for the \"Contradictory, My Dear Watson\" competition. The goal is to create an NLI (Natural Language Inferencing) model that, given a premise and a corresponding hypothesis, can determine whether the relationship between the two is an entailment, is neutral, or is a contradiction. We tackle this task by implementing a Hugging Face transformer, **XLM-RoBERTa**, with some other data handling techniques.\n\n\uc774 Kaggle kernel\uc740 \"Contradictory, My Dear Watson\" \ub300\ud68c\ub97c \uc704\ud574 \uc81c\uc791\ub418\uc5c8\uc2b5\ub2c8\ub2e4. \uc774 \ub300\ud68c\uc758 \ubaa9\uc801\uc740 \uc804\uc81c\uc640 \uac00\uc124 \ubb38\uc7a5 \uac01 \ud558\ub098\uc529\uc744 \ub450\uace0 \ub458\uc758 \uad00\uacc4\uac00 \uc218\ubc18\ub418\ub294\uc9c0, \uc911\ub9bd\uc801\uc778\uc9c0, \uc544\ub2c8\uba74 \uc0c1\ubc18\ub418\ub294\uc9c0\ub97c \ubd84\ub958\ud558\ub294 NLI (Natural Language Inferencing) \ubaa8\ub378\uc744 \ub9cc\ub4dc\ub294 \uac83\uc785\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574 Hugging Face\uc758 transformer \uc911 \ud558\ub098\uc778 **XLM-RoBERTa**\uc640 \ub2e4\ub978 \ub370\uc774\ud130\ucc98\ub9ac \uae30\ubc95\ub4e4\uc744 \ud65c\uc6a9\ud569\ub2c8\ub2e4.","d65ffb81":"## Setup Tokenizer\nA transformer takes token input ids as input, so we need a tokenizer to first encode the text data into tokens.\n\nTransformer\ub294 token input id\ub97c \uc785\ub825\uac12\uc73c\ub85c \ubc1b\uae30 \ub54c\ubb38\uc5d0 \uc6b0\uc120 \ud14d\uc2a4\ud2b8 \ub370\uc774\ud130\ub97c token\uc73c\ub85c \ubd80\ud638\ud654\ud574\uc904 tokenizer\uac00 \ud544\uc694\ud569\ub2c8\ub2e4.","46354e1c":"## Early Stopping\nSet up an early stopping callback to train the model up to when the lowest validation loss is obtained.\n\n\ubaa8\ub378\uc744 \ucd5c\uc800 \uac80\uc99d \uc190\uc2e4\uac12\uc744 \uc5bb\uc5c8\uc744 \ub54c\uae4c\uc9c0 \ud6c8\ub828 \uc2dc\ud0a4\uae30 \uc704\ud574 early stopping callback\uc744 \ub9cc\ub4e4\uc5b4\uc90d\ub2c8\ub2e4.","b676b56d":"Observation(s):\n* The *id* column seems unnecessary as it does not include useful information.\n* The *lang_abv* and *language* columns seem unnecessary as the **XLM-RoBERTa** transformer is a multilingual model which [\"does not require *lang* tensors to understand which language is used.\"](https:\/\/huggingface.co\/transformers\/model_doc\/xlmroberta.html)\n\nDecision(s):\n* Drop the *id*, *lang_abv*, and *language* columns from the dataset.\n\n\uad00\ucc30:\n* *id* \uc5f4\uc740 \ubcc4 \ub2e4\ub978 \uc815\ubcf4\ub97c \uc8fc\uc9c0 \uc54a\uc73c\ubbc0\ub85c \ud544\uc694\uac00 \uc5c6\uc5b4 \ubcf4\uc785\ub2c8\ub2e4.\n* **XLM-RoBERTa** transformer\ub294 \ub2e4\uc911\uc5b8\uc5b4 \ubaa8\ub378\ub85c\uc11c [\uc785\ub825 \ub370\uc774\ud130\uc758 \uc5b8\uc5b4\uc5d0 \ub300\ud55c \uc548\ub0b4\uac00 \ud544\uc694 \uc5c6\uc73c\ubbc0\ub85c](https:\/\/huggingface.co\/transformers\/model_doc\/xlmroberta.html) *lang_abv* \uc640 *lanugage* \uc5f4\uc740 \ud544\uc694\uac00 \uc5c6\uc5b4 \ubcf4\uc785\ub2c8\ub2e4.\n\n\uacb0\uc815:\n* *id*, *lang_abv*, *language* \uc5f4\ub4e4\uc744 \ub370\uc774\ud130\uc14b\uc5d0\uc11c \uc81c\uc678\uc2dc\ud0b5\ub2c8\ub2e4.","9988bc37":"## Encode Text Input and Obtain Input Ids\nThe transformer takes tokens by their input ids, so we need to obtain the input ids of the training, validation, and test datasets.\n\nTransformer\ub294 token\ub4e4\uc758 input id\ub4e4\uc744 \uc77d\uc5b4\ub4e4\uc774\uae30 \ub54c\ubb38\uc5d0 \ud6c8\ub828, \uac80\uc99d, \uc2dc\ud5d8 \ub370\uc774\ud130\uc14b\uc758 input id\ub4e4\uc744 \uad6c\ud574\uc57c\ud569\ub2c8\ub2e4.","02b09fe1":"# Preprocess Data\n## Train\/Validation Split\nWe keep only the necessary columns (*premise*, *hypothesis*, *label*) and perform a train\/validation split to evaluate our model during training.\n\n\ud544\uc694\ud55c \uc5f4\ub4e4\ub9cc (*premise*, *hypothesis*, *label*) \uc720\uc9c0\ud558\uace0 \ubaa8\ub378 \ud6c8\ub828 \uc911 \ud3c9\uac00\ub97c \uc704\ud574 \ud6c8\ub828 \ub370\uc774\ud130\uc14b\uc744 \ud6c8\ub828\/\uac80\uc99d \ub370\uc774\ud130\uc14b\ub4e4\ub85c \ub098\ub215\ub2c8\ub2e4.","eb2aa30c":"## Convert to TensorFlow Dataset\nWe convert the data into TensorFlow datasets which the model can read in during training, and to enable training on TPUs.\n\nTPU\uc5d0 \ubaa8\ub378 \ud6c8\ub828\uc744 \uc2dc\ud0ac \uc218 \uc788\uae30 \uc704\ud574 \ub370\uc774\ud130\ub97c TensorFlow dataset \ud615\ud0dc\ub85c \uc804\ud658\ud574\uc90d\ub2c8\ub2e4.","93302f12":"Observation(s):\n* The length distribution is heavily skewed to the left.\n* While the longest is 246 tokens long, the 99th percentile text's length is 98 tokens long.\n\nDecision(s):\n* A max length of around 80 tokens seems appropriate, retaining about 95% of the data.\n\n\uad00\ucc30:\n* \uae38\uc774 \ubd84\ud3ec\uac00 \uc67c\ucabd\uc73c\ub85c \ud06c\uac8c \uc65c\uace1\ub418\uc5b4\uc2b5\ub2c8\ub2e4.\n* \uac00\uc7a5 \uae34 \uac12\uc740 246 \ud1a0\ud070\ub9cc\ud07c \uae38\uc9c0\ub9cc, 99\ubc88\uc9f8 \ubc31\uc704\ubd84\uc218 \ud14d\uc2a4\ud2b8\uc758 \uae38\uc774\ub294 98 \ud1a0\ud070\ub9cc\ud07c \uae41\ub2c8\ub2e4.\n\n\uacb0\uc815:\n* 80 \ud1a0\ud070\uc758 \ucd5c\ub300 \uae38\uc774\uac00 95% \uc815\ub3c4\uc758 \ub370\uc774\ud130\ub97c \ud3ec\ud568\ud558\ubbc0\ub85c \uc801\ub2f9\ud574\ubcf4\uc785\ub2c8\ub2e4. ","0de5abbb":"## Setup TPU\nWe utilize the TPU to accelerate model training.\n\n\ub354 \ube60\ub978 \ubaa8\ub378 \ud6c8\ub828\uc744 \uc704\ud574 TPU\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.","c36f7e68":"## Determine *MAX_LEN* for Encoding\nWe need to set a max length for the text inputs in order to keep the token lengths equal. Shorter texts will be padded up to the max length, while longer texts will be truncated to it. Too much padding will lead to tokens not carrying information, whereas too much truncation will lead to losing useful information. We determine the appropriate size by examining the length distribution of the encoded tokens.\n\nToken\ub4e4\uc758 \uae38\uc774\ub97c \uc77c\uc815\ud558\uac8c \ud558\uae30 \uc704\ud574 \ud14d\uc2a4\ud2b8 \uc785\ub825\uac12\ub4e4\uc758 \ucd5c\ub300 \uae38\uc774\ub97c \uc124\uc815\ud574\uc918\uc57c\ud569\ub2c8\ub2e4. \ucd5c\ub300 \uae38\uc774\ubcf4\ub2e4 \uc9e7\uc740 \ud14d\uc2a4\ud2b8\ub4e4\uc740 \ucd5c\ub300 \uae38\uc774\ub9cc\ud07c padding \ub418\uc5b4\uc11c \uae38\uc5b4\uc9c0\uace0, \ub354 \uae34 \ud14d\uc2a4\ud2b8\ub4e4\uc740 \ucd5c\ub300 \uae38\uc774\ub9cc\ud07c truncate \ub418\uc5b4\uc11c \uc9e7\uc544\uc9d1\ub2c8\ub2e4. \uacfc\ud55c padding\uc740 \uc815\ubcf4\uac00 \ub2f4\uae30\uc9c0 \uc54a\uc740 token\ub4e4\uc744 \uc0dd\uc131\ud558\uace0, \uacfc\ud55c truncation\uc740 \uc720\uc6a9\ud55c \uc815\ubcf4\ub97c \uc783\uac8c \ud569\ub2c8\ub2e4. \uc801\ub2f9\ud55c \uae38\uc774\ub97c \uc815\ud558\uae30 \uc704\ud574 \ubd80\ud638\ud654\ub41c token\ub4e4\uc758 \uae38\uc774 \ubd84\ud3ec\ub97c \ud655\uc778\ud574\ubd05\ub2c8\ub2e4. ","671f1765":"# Kernel Setup\n## Imports","e3970614":"## Train Model\nWe now train the model to learn the premise and hypothesis relationships.\n\n\uc774\uc81c \ubaa8\ub378\uc774 \uc804\uc81c\uc640 \uac00\uc124\ub4e4\uc758 \uad00\uacc4\ub97c \ud559\uc2b5\ud558\uae30 \uc704\ud574 \ud6c8\ub828\uc2dc\ud0b5\ub2c8\ub2e4."}}