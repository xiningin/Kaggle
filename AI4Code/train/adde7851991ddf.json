{"cell_type":{"9951941c":"code","458e0c6a":"code","f6f42547":"code","904becce":"code","cf55708a":"code","ef0230e9":"code","74647dbc":"code","71bfe635":"code","b3f99e6c":"code","6b2a9e30":"code","11285814":"code","d65305c5":"code","e5dcdfe8":"code","b31a205d":"code","3b03a259":"markdown","eb4139db":"markdown","06525e45":"markdown","ed47e644":"markdown","4c1cd09a":"markdown"},"source":{"9951941c":"import numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\n\nfrom bayes_opt import BayesianOptimization\nimport lightgbm as lgb\n\nimport os\n\nfrom sklearn.cluster import KMeans\n\nfrom sklearn.neighbors import DistanceMetric\n\nfrom xgboost import XGBClassifier","458e0c6a":"# control parameters\nnfolds = 5","f6f42547":"xtrain = pd.read_csv(\"..\/input\/train.csv\")\nxtest = pd.read_csv(\"..\/input\/test.csv\")","904becce":"# separate the data\nid_train = xtrain['ID_code']\nytrain = xtrain['target']\nid_test = xtest['ID_code']\nxtrain.drop(['ID_code', 'target'], axis = 1, inplace = True)\nxtest.drop('ID_code', axis = 1, inplace = True)","cf55708a":"folds = KFold(n_splits= nfolds, shuffle=True, random_state= 15)\n\nmindex  = np.zeros((len(xtrain),1) )\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(xtrain.values, ytrain.values)):\n#    print('----')\n    print(\"fold n\u00b0{}\".format(fold_))\n    \n    mindex[val_idx, 0] = fold_\n\nxfolds = pd.DataFrame()\nxfolds['MachineIdentifier'] = id_train\nxfolds['fold_id']= mindex\nxfolds['fold_id'] = xfolds['fold_id'].astype(int).round(0)\nxfolds.to_csv('xfolds.csv', index = False)","ef0230e9":"kmeans = KMeans(init='k-means++', n_clusters= 10, n_init=10)\nkmeans.fit(xtrain)","74647dbc":"# distance of each observation from cluster centers\ndist = DistanceMetric.get_metric('euclidean')\nax_tr = dist.pairwise(xtrain, kmeans.cluster_centers_)\nax_te = dist.pairwise(xtest, kmeans.cluster_centers_)","71bfe635":"# format into dataframe\nax_tr = pd.DataFrame(ax_tr)\nax_te = pd.DataFrame(ax_te)\nxcols =  ['dist' + str(f) for f in range(0, ax_tr.shape[1])]","b3f99e6c":"ax_tr.columns = xcols\nax_te.columns = xcols","6b2a9e30":"m1 = xtrain.max(axis = 1)\nm2 = xtrain.min(axis = 1)\nm3 = xtrain.median(axis = 1)\nm4 = 1\/xtrain.std(axis = 1)\nm5 = 1\/xtrain.mad(axis = 1)\n\nxtrain['xmax'] = m1; xtrain['xmin'] = m2; xtrain['xmed'] = m3; xtrain['xstd'] = m4\n\nm1 = xtest.max(axis = 1)\nm2 = xtest.min(axis = 1)\nm3 = xtest.median(axis = 1)\nm4 = 1\/xtest.std(axis = 1)\nm5 = 1\/xtest.mad(axis = 1)\n\nxtest['xmax'] = m1; xtest['xmin'] = m2; xtest['xmed'] = m3; xtest['xstd'] = m4\n","11285814":"# combine\nxtrain = pd.concat([xtrain, ax_tr], axis = 1)\nxtest = pd.concat([xtest, ax_te], axis = 1)","d65305c5":"xtrain.head(3)","e5dcdfe8":"def lgbcv(learning_rate, subsample, min_child_samples, max_depth,\n                  colsample_bytree, min_child_weight, min_split_gain, \n                  lambda_l1, lambda_l2,bagging_freq, num_leaves,\n                  silent=True, seed=1234):\n\n    params = {                        \n            'boosting_type': 'gbdt','objective': 'binary', 'metric':'auc',\n            'max_depth': -1, 'num_leaves': int(num_leaves),\n            'learning_rate': learning_rate, 'max_depth': int(max_depth),\n            'min_child_samples': int(min_child_samples), \n           'subsample': subsample, 'colsample_bytree': colsample_bytree, 'bagging_seed': 11,\n           'min_child_weight': min_child_weight,  'bagging_freq' : int(bagging_freq),\n           'min_split_gain': min_split_gain,'lambda_l1': lambda_l1,'lambda_l2': lambda_l2,\n           'nthread': 8\n        }\n\n                \n    bst1 = lgb.train(params, trn_data, valid_sets=[trn_data, val_data], valid_names=['train','valid'],\n                          num_boost_round= 5000, verbose_eval= 5000, early_stopping_rounds = 100)\n    \n    ypred = bst1.predict(x1)\n\n    loss = roc_auc_score(y1, ypred)\n    return loss","b31a205d":"## find optimal params\nparam_list = list()\nscore_list = list()\n\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(xtrain.values, ytrain.values)):\n    print('----')\n    print(\"fold n\u00b0{}\".format(fold_))\n    \n    x0,y0 = xtrain.iloc[trn_idx], ytrain[trn_idx]\n    x1,y1 = xtrain.iloc[val_idx], ytrain[val_idx]\n    \n    \n    trn_data = lgb.Dataset(x0, label= y0); val_data = lgb.Dataset(x1, label= y1)\n    # optimization\n    lgbBO = BayesianOptimization(lgbcv, {'learning_rate': (0.0025, 0.05),'max_depth': (int(5), int(15)),\n                                           'min_child_samples': (int(25), int(250)),'subsample': (0.2, 0.95),\n                                            'colsample_bytree': (0.2, 0.95), 'min_child_weight': (int(1), int(150)),\n                                            'min_split_gain': (0.1, 2),'num_leaves': (int(15),int(200)),\n                                             'lambda_l1': (10, 200),'lambda_l2': (10, 200),\n                                             'bagging_freq': (1,20)\n                                                   })\n    lgbBO.maximize(init_points= 25, n_iter= 45, xi=0.06)\n    print('-' * 53)\n    print('Final Results')\n    print('LGB: %f' % lgbBO.res['max']['max_val']);  print('LGB: %s' % lgbBO.res['max']['max_params'])\n\n    param_list.append(lgbBO.res['max']['max_params'])\n    score_list.append(lgbBO.res['max']['max_val'])","3b03a259":"# Data","eb4139db":"# Parameter tuning","06525e45":"## Clustering","ed47e644":"# FE","4c1cd09a":"## Summary statistics"}}