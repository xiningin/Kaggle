{"cell_type":{"aceb4295":"code","4c5561f0":"code","e8b93cf3":"code","4ba1474d":"code","258452ec":"code","2d3c2c45":"code","6621e498":"code","d1e31955":"code","a133f38b":"code","4517b494":"code","3af86dc9":"code","0cbe8eb7":"code","11779d34":"code","005fe90d":"code","266e0180":"code","df774530":"code","ec7ba127":"code","dffa2321":"code","02f6b446":"code","8f20a40d":"code","971fac05":"code","5370f77d":"code","89ff9743":"code","0c9102a1":"code","7e5b23c9":"code","0db416bb":"code","f621a4a2":"code","8fe53bbe":"code","f2acea53":"code","fdb5e7d7":"code","306e247f":"code","06a98436":"code","dba8467b":"code","16ae5c6c":"code","b1dd82b6":"code","4045357d":"code","39042f1c":"code","e60a9dcd":"code","67ff2499":"code","e75e0840":"code","00a268d0":"code","186c0ea3":"code","09e577c0":"code","cd98be2e":"code","36d10796":"code","71b54bbf":"code","71393baa":"markdown","c8bf3513":"markdown","c720b3ce":"markdown","062066d3":"markdown","27975892":"markdown","1a69cb9f":"markdown","bfbc030d":"markdown","b1672ea5":"markdown","7999acd5":"markdown","927eef07":"markdown","f3e38441":"markdown","51243fab":"markdown","b02e5845":"markdown","2b9599a6":"markdown","52fb4459":"markdown","0fc9f92e":"markdown","c5a84f43":"markdown","50286b20":"markdown","d77eb2b5":"markdown","f96bea89":"markdown","d88593d0":"markdown","106571bc":"markdown","05e3fc9c":"markdown","0e633a71":"markdown","69b5270a":"markdown","3db78932":"markdown","83a953df":"markdown","bb1a7117":"markdown","a5132c18":"markdown"},"source":{"aceb4295":"import pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, StratifiedKFold, GridSearchCV\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\nfrom sklearn.metrics import roc_curve, roc_auc_score, auc\nfrom xgboost import XGBClassifier\n\nplt.style.use(\"Solarize_Light2\")\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","4c5561f0":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e8b93cf3":"df = pd.read_csv('\/kaggle\/input\/diabetes-health-indicators-dataset\/diabetes_binary_5050split_health_indicators_BRFSS2015.csv')","4ba1474d":"df","258452ec":"df.info()","2d3c2c45":"df.shape","6621e498":"df.sample(10)","d1e31955":"df.columns","a133f38b":"df.isna().sum() # No missing values","4517b494":"df.nunique()","3af86dc9":"cat_col = ['GenHlth', 'Education', 'Income']","0cbe8eb7":"num = ['BMI', 'MentHlth', 'PhysHlth', 'Age']","11779d34":"df = pd.get_dummies(df, columns=cat_col)","005fe90d":"df['bmi_group'] = pd.cut(df['BMI'], (0, 16, 18.5, 25, 30, 35, 40, np.inf), labels=[1, 2, 3, 4, 5, 6, 7])","266e0180":"df.BMI = df['bmi_group']\ndf.drop('bmi_group', axis=1, inplace=True)","df774530":"df.BMI = df.BMI.astype('float')","ec7ba127":"X = df.drop(['Diabetes_binary'], axis=1)\ny = df['Diabetes_binary']","dffa2321":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)","02f6b446":"num","8f20a40d":"scaler = StandardScaler()\nx_train_sc = scaler.fit_transform(x_train[num])\nx_test_sc = scaler.transform(x_test[num])\n\nx_train[num] = x_train_sc\nx_test[num] = x_test_sc","971fac05":"def xgb_class(clf, param_test, feat, target):\n   \n    kfold = StratifiedKFold(n_splits=3,\n                            shuffle=True,\n                            random_state=42)\n\n    grid_search = GridSearchCV(clf,\n                               param_test,\n                               scoring=\"neg_log_loss\",\n                               n_jobs=-1,\n                               cv=kfold)\n\n    global grid_result\n    grid_result = grid_search.fit(feat, target)\n    \n    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n    means = grid_result.cv_results_['mean_test_score']\n    stds = grid_result.cv_results_['std_test_score']\n    params = grid_result.cv_results_['params']\n    for mean, stdev, param in zip(means, stds, params):\n        print(\"%f (%f) with: %r\" % (mean, stdev, param))","5370f77d":"def logistic_loss(clf, act, feat):\n    return log_loss(act, clf.predict_proba(feat))","89ff9743":"def quality_report(actual, prediction):\n    print(\"Accuracy: {:.3f}\\nPrecision: {:.3f}\\nRecall: {:.3f}\\nf1_score: {:.3f}\\nRoc_auc: {:.3f}\".format(\n        accuracy_score(actual, prediction),\n        precision_score(actual, prediction),\n        recall_score(actual, prediction),\n        f1_score(actual, prediction),\n        roc_auc_score(actual, prediction)))","0c9102a1":"def plot_features(clf):\n    feature_importances = clf.feature_importances_\n    pd.DataFrame({'features': x_train.columns,\n                                           'feature_importances': feature_importances})\\\n    .sort_values('feature_importances', ascending=False).plot.barh(x ='features', figsize=(10, 7))","7e5b23c9":"xgbc_start = XGBClassifier(random_state = 42, eval_metric='auc')\n\nxgbc_start.fit(x_train, y_train)\n\npredict_start = xgbc_start.predict(x_test)\n\nprint('\\nTest quality: \\n')\nquality_report(y_test, predict_start)\nprint('\\nLogistic Loss:')\nlogistic_loss(xgbc_start, y_test, x_test)","0db416bb":"xgbc_clf = XGBClassifier(random_state = 42,\n                         eval_metric='auc',\n                         use_label_encoder=False) \n\n\nn_estimators = [100, 200, 300, 400, 500]\nlearning_rate = [0.0001, 0.001, 0.01, 0.05, 0.1]\n\nparam_grid = dict(learning_rate=learning_rate, n_estimators=n_estimators)\n\nkfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=7)\n\ngrid_search = GridSearchCV(xgbc_clf, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n\ngrid_result = grid_search.fit(x_train, y_train)\n\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n# plot results\nscores = np.array(means).reshape(len(learning_rate), len(n_estimators))\nplt.figure(figsize=(13,10))\nfor i, value in enumerate(learning_rate):\n    plt.plot(n_estimators, scores[i], label='learning_rate: ' + str(value))\nplt.legend()\nplt.xlabel('n_estimators')\nplt.ylabel('Log Loss')","f621a4a2":"xgbc_clf = XGBClassifier(random_state = 42,\n                         eval_metric='auc',\n                         use_label_encoder=False)\n\nlow_e = int((grid_result.best_params_.get('learning_rate'))*100 - 5)\nif low_e <= 0:\n    low_e = int(1)\nhigh_e = int((grid_result.best_params_.get('learning_rate'))*100 + 6)\nif high_e >= 10:\n    high_e = int(11)\n\nparam_test = {\n'n_estimators':[100, 200, 300, 400, 500],\n'learning_rate':[i\/100.0 for i in range(low_e,high_e)]}\n\n\nxgb_class(xgbc_clf, param_test, x_train, y_train)","8fe53bbe":"params = {}\nparams.update(grid_result.best_params_)\nprint(\"Add parameters: %s\" % (grid_result.best_params_))","f2acea53":"xgbc_depth = XGBClassifier(random_state = 42,\n                     eval_metric='auc',\n                     use_label_encoder=False, **params) \n\nparam_test = {\n'max_depth':range(1,6,1),\n'min_child_weight':range(1,6,1)}\n\n\nxgb_class(xgbc_depth, param_test, x_train, y_train)","fdb5e7d7":"params.update(grid_result.best_params_)\nprint(\"Add parameters: %s\" % (grid_result.best_params_))","306e247f":"xgbc_s_c = XGBClassifier(random_state = 42,\n                         eval_metric='auc',\n                         use_label_encoder=False, \n                        **params) \n\nparam_test = {\n 'subsample':[i\/10.0 for i in range(6,11)],\n 'colsample_bytree':[i\/10.0 for i in range(6,11)]\n}\n\n\nxgb_class(xgbc_s_c, param_test, x_train, y_train)","06a98436":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","dba8467b":"xgbc_s_c = XGBClassifier(random_state = 42,\n                         eval_metric='auc',\n                         use_label_encoder=False, \n                        **params)\n\nlow_s = int((grid_result.best_params_.get('subsample'))*100 - 5)\nif low_s <= 0:\n    low_s = int(1)\nhigh_s = int((grid_result.best_params_.get('subsample'))*100 + 6)\nif high_s >= 100:\n    high_s = int(101)\n\n\nlow_c = int((grid_result.best_params_.get('colsample_bytree'))*100 - 5)\nif low_c <= 0:\n    low_c = int(1)\nhigh_c = int((grid_result.best_params_.get('colsample_bytree'))*100 + 6)\nif high_c >= 100:\n    high_c = int(101)\n\nparam_test = {\n 'subsample':[i\/100.0 for i in range(low_s,high_s)],\n 'colsample_bytree':[i\/100.0 for i in range(low_c, high_c)]\n}\n\n\nxgb_class(xgbc_s_c, param_test, x_train, y_train)","16ae5c6c":"\nparams.update(grid_result.best_params_)\nprint(\"Add parameters: %s\" % (grid_result.best_params_))","b1dd82b6":"xgbc_g = XGBClassifier(random_state = 42,\n                         eval_metric='auc',\n                         use_label_encoder=False, \n                        **params) \n\nparam_test = {\n 'gamma':[i\/10.0 for i in range(0,11)]\n}\n\n\nxgb_class(xgbc_g, param_test, x_train, y_train)","4045357d":"print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","39042f1c":"xgbc_g = XGBClassifier(random_state = 42,\n                         eval_metric='auc',\n                         use_label_encoder=False, \n                        **params) \n\nlow_g = int((grid_result.best_params_.get('gamma'))*100 - 5)\nif low_g <= 0:\n    low_g = int(1)\nhigh_g = int((grid_result.best_params_.get('gamma'))*100 + 6)\nif high_g >= 100:\n    high_g = int(101)\n\nparam_test = {\n 'gamma':[i\/100.0 for i in range(low_g,high_g)]\n}\n\n\nxgb_class(xgbc_g, param_test, x_train, y_train)","e60a9dcd":"params.update(grid_result.best_params_)\nprint(\"Add parameters: %s\" % (grid_result.best_params_))","67ff2499":"xgbc_fin = XGBClassifier(random_state = 42,\n                         eval_metric='auc',\n                         use_label_encoder=False, \n                        **params)\n\n\nxgbc_fin.fit(x_train, y_train)\n\npredict_xgbc = xgbc_fin.predict(x_test)\n\nprint('\\nTest quality: \\n')\nquality_report(y_test, predict_xgbc)","e75e0840":"print('The first result: %s' % logistic_loss(xgbc_start, y_test, x_test))\nprint('Result with parameter tuning: %s' % logistic_loss(xgbc_fin, y_test, x_test))","00a268d0":"plot_features(xgbc_fin)","186c0ea3":"pred_proba = xgbc_fin.predict_proba(x_test)","09e577c0":"class_1 = pred_proba[:, 1][y_test == 1]\nclass_0 = pred_proba[:, 1][y_test == 0]\nthreshold = 0.5\nplt.figure(figsize=(17,10))\nplt.scatter(np.arange(len(class_1)), class_1, label='class_1')\nplt.scatter(np.arange(len(class_1), len(class_1)+len(class_0)), class_0, label='class_0')\nplt.plot([-0.2, len(pred_proba[:, 1])], [threshold, threshold], c='b')\nplt.title('Probability of class 1')\nplt.legend();","cd98be2e":"msg_row = pd.DataFrame(columns=['threshold', 'precision', 'recall',\n               'F1', 'accuracy'])\nthreshold = 0.3\ndata_threshold = pd.DataFrame()\nfor i in range(300):\n    threshold += 0.001\n    pred = np.where(pred_proba[:, 1] >= threshold, 1, 0)\n    precision = precision_score(y_test, pred)\n    recall = recall_score(y_test, pred)\n    accuracy = accuracy_score(y_test, pred)\n    f1 = f1_score(y_test, pred)\n    new_row = {'threshold': threshold, 'precision':precision, 'recall':recall,\n               'F1':f1, 'accuracy':accuracy, 'SUM': precision+recall+f1+accuracy}\n    msg_row = msg_row.append(new_row, ignore_index=True)","36d10796":"msg_row.head()","71b54bbf":"msg_row.plot(x='threshold', y=['precision', 'recall',\n               'F1', 'accuracy'], figsize=(19, 10), fontsize=15)","71393baa":"Now let's set up `max_depth` and `min_child_weight`, set a spread of values and look for the best. ","c8bf3513":"Let's add new parameters to the `params` dictionary, which we will use in the next training.","c720b3ce":"# Exploring Dataset","062066d3":"+ Let's define the functions","27975892":"Now let's fine-tune the `XGBClassifier`. To begin with, we determine the optimal learning rate and the number of trees. Let's take `net_log_loss` as a criterion. Although our data is balanced, but due to a logistical error, we will be able to better see the progress in finding parameters. And so it's easier to use any of the `confusion_matrix` metrics.","1a69cb9f":"+ Let's run the model with default settings:","bfbc030d":"Let's run it again, but with boundary values.","b1672ea5":"# **Importing Libraries**","7999acd5":"+ Let's scaled all numeric signs - `'BMI'`, `'MentHlth'`, `'PhysHlth'`, `'Age'`","927eef07":"Now let's configure `subsample` and `com sample_by three`, set a spread of values from 0.6 to 1.","f3e38441":"+ No missing values    \n  \nLet's see how many unique values are in each attribute:","51243fab":"# **Cleaning Dataset**","b02e5845":"Configure the `gamma` parameter, set values from 0 to 1","2b9599a6":"+ Let's break all the values into separate variables","52fb4459":"When the threshold is increased, the `recall` drops sharply and the `precision` increases. At the threshold value of 0.54, the metrics intersect and have an equal value of 0.751.","0fc9f92e":"Let's add new parameters to the `params` dictionary, which we will use in the next training.","c5a84f43":"+ Non-binary categorical features - `GenHlth`, `Education`, `Income`.","50286b20":"let's run it again, but with boundary values.","d77eb2b5":"# Split and Scaler","f96bea89":"Let's run the final model with the values obtained in the sample `test` ","d88593d0":"Let's see which variabilities  had the greatest impact on learning.","106571bc":"There are 70692 values in the dataset, so we will divide the dataset into 2 parts: `train` and `test`.","05e3fc9c":"# XGBClassifier","0e633a71":"+ Divide the values of `BMI` (body mass index) into groups, update `BMI`","69b5270a":" let's run it again, but with boundary values `learning_rate`.","3db78932":"Let's see how the probabilities of classes 1 and 0 were distributed in the predicted model","83a953df":"# **Importing Kaggle Dataset**","bb1a7117":"+ Continuous features - `BMI`, `MentHlth`, `PhysHlth`, `Age`","a5132c18":"The blue labels are class 1, the green labels are class 0 in `y_test`, the blue line is a threshold of 0.5, which are separated by class 1 and 0 in the predicted model. Green labels above the blue line and blue labels below the blue line are incorrectly classified classes.\n\nLet's change the threshold 0.5  to see how the values of `precision`, `recall`, `F1` and `accuracy` will change."}}