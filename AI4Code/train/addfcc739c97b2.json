{"cell_type":{"9d82e2b9":"code","25af6be9":"code","9d01d9a2":"code","0336e70e":"code","cb56dc45":"code","4e3e1f53":"code","10eafef5":"code","f8076a24":"code","8faaad27":"code","ba2063bd":"code","4a549ccd":"code","45ac65bf":"code","d4f1436d":"code","eb68ef5b":"code","5b05b704":"code","65ad6149":"code","d9a79290":"code","c2a29a3a":"code","d421113d":"code","29727eaa":"code","6e5be340":"code","ed0a0ffb":"code","9a6dda9e":"code","f95c33ff":"code","81d0904e":"markdown","7f8f896d":"markdown","59c6787b":"markdown","d45409b2":"markdown","ff9b9183":"markdown","495ae3f8":"markdown","77c6fe76":"markdown","da425e6f":"markdown","06ff218a":"markdown","5779bf68":"markdown","56cf6aab":"markdown"},"source":{"9d82e2b9":"%matplotlib inline","25af6be9":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nimport re\nfrom bisect import bisect\n\nimport xgboost as xgb\n\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold\nfrom sklearn import metrics","9d01d9a2":"data_dir = \"..\/input\"\nos.listdir(f\"{data_dir}\")","0336e70e":"# loading the raw datasets\ntrain_df_raw = pd.read_csv(f'{data_dir}\/train.csv', low_memory=False)\ntest_df_raw = pd.read_csv(f'{data_dir}\/test.csv', low_memory=False)\n\n# just to peek at the raw data\ntrain_df_raw.sample(10)","cb56dc45":"train_df_raw.describe(include='all').T","4e3e1f53":"test_df_raw.describe(include='all').T","10eafef5":"train_df = train_df_raw\ntest_df = test_df_raw\n\nall_df = [train_df, test_df]","f8076a24":"for df in all_df:\n    df['Embarked'] = df['Embarked'].fillna('S')\n    df['Age'] = df['Age'].fillna(df['Age'].mean())\n    df['Cabin'] = df['Cabin'].fillna('')\n    df['Fare'] = df['Fare'].fillna(df['Fare'].mean())","8faaad27":"# accumulated list of maps of categories that are created\ncategory_maps = {}\n\n# todo: to generalize well we should choose better storage types\ndef Categorify(df:pd.DataFrame, cat_names):\n    for cat_name in cat_names:\n        uniques = df[cat_name].unique()\n        category_maps[cat_name] = {i:uniques[i] for i in range(len(uniques))}\n        df[cat_name] = [np.where(uniques == key)[0][0] for key in df[cat_name]]","ba2063bd":"cat_names = ['Sex', 'Embarked']\nlist(map(lambda df: Categorify(df, cat_names), all_df))\n\ncategory_maps","4a549ccd":"def Quantile(df:pd.DataFrame, quant_names, quants = [0.25,0.5,0.75]):\n    for quant_name in quant_names:\n        quant_col_name = f'{quant_name}_quantile'\n        quant_vals = [np.quantile(df[quant_name], quant) for quant in quants]\n        df[quant_col_name] = [bisect(quant_vals, x) for x in df[quant_name]]","45ac65bf":"%%capture\nquant_names = ['Fare', 'Age']\nlist(map(lambda df: Quantile(df, quant_names), all_df))","d4f1436d":"# extracts a title\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\n# title mappings, if it's not in this list then it will be marked as 'Rare'\ntitle_map = {\n    \"Mr\": \"Mr\",\n    \"Mrs\": \"Mrs\",\n    \"Mme\": \"Mrs\",\n    \"Miss\": \"Miss\",\n    \"Mlle\": \"Miss\",\n    \"Ms\": \"Miss\",\n    \"Master\": \"Master\"\n}","eb68ef5b":"# looping through training and test sets to apply new feature generation\nfor df in all_df:\n    # Title\n    df['Title'] = list(map(lambda key: title_map.get(key, 'Rare'), \n                                       df['Name'].apply(get_title)))\n    Categorify(df, ['Title'])\n    \n    # Family Size\n    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n    \n    # Is Alone\n    df['IsAlone'] = [1 if size == 1 else 0 for size in df['FamilySize']]\n    \n    # Has Cabin\n    df['HasCabin'] = [0 if cabin == '' else 1 for cabin in df['Cabin']]","5b05b704":"SEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(n_splits=NFOLDS, random_state=SEED)\n\n# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, name, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.name = name\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        return self.clf.fit(x,y).feature_importances_","65ad6149":"def get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((x_train.shape[0],))\n    oof_test = np.zeros((x_test.shape[0],))\n    oof_test_skf = np.empty((NFOLDS, x_test.shape[0]))\n\n    for i, (train_index, test_index) in enumerate(kf.split(x_train, y_train)):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","d9a79290":"# create the classifiers\nclassifier_stack = [\n    SklearnHelper('RandomForest', clf=RandomForestClassifier, seed=SEED, params={\n        'n_jobs': -1,\n        'n_estimators': 500,\n         #'max_features': 0.2,\n        'max_depth': 6,\n        'min_samples_leaf': 2,\n        'max_features' : 'sqrt',\n        'verbose': 0\n    }),\n    SklearnHelper('ExtraTrees', clf=ExtraTreesClassifier, seed=SEED, params={\n        'n_jobs': -1,\n        'n_estimators':500,\n        #'max_features': 0.5,\n        'max_depth': 8,\n        'min_samples_leaf': 2,\n        'verbose': 0\n    }),\n    SklearnHelper('AdaBoost', clf=AdaBoostClassifier, seed=SEED, params={\n        'n_estimators': 500,\n        'learning_rate' : 0.75\n    }),\n    SklearnHelper('GradientBoost', clf=GradientBoostingClassifier, seed=SEED, params={\n        'n_estimators': 500,\n         #'max_features': 0.2,\n        'max_depth': 5,\n        'min_samples_leaf': 2,\n        'verbose': 0\n    })\n]","c2a29a3a":"dep_var = 'Survived'\ndrop_vars = ['PassengerId', 'Name', 'Ticket', 'Cabin']\n\nx_train_df = train_df.drop(drop_vars, axis=1).drop(dep_var, axis=1)\nx_train = x_train_df.values\ny_train = train_df[dep_var].ravel()\nx_test = test_df.drop(drop_vars, axis=1).values","d421113d":"# train the classifier via the out-of-fold \noofs = { clf.name : get_oof(clf, x_train, y_train, x_test) for clf in classifier_stack }\nprint(\"Training is complete\")","29727eaa":"features = [map(lambda clf: clf.feature_importances(x_train,y_train), classifier_stack)]","6e5be340":"base_predictions_train = pd.DataFrame({ key: oofs[key][0].ravel() for key in oofs })\nbase_predictions_train.head()","ed0a0ffb":"x_train = np.concatenate([oofs[key][0] for key in oofs], axis=1)\nx_test = np.concatenate([oofs[key][1] for key in oofs], axis=1)","9a6dda9e":"gbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)\npredictions = gbm.predict(x_test)","f95c33ff":"# Generate Submission File \nresults_df = pd.DataFrame({ 'PassengerId': test_df['PassengerId'], 'Survived': predictions })\nresults_df.to_csv(\"submission.csv\", index=False)","81d0904e":"Second Level Stack\n------------------","7f8f896d":"Base Model Stacking\n-------------------","59c6787b":"Titanic\n=======\nThis is a learning and experimentation kernel in order to implement and understand the basics of ensembling and stacking.  \nMost of the work in this notebook is guided by Anisotropic's [Introduction to Ensembling\/Stacking in Python](https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python) notebook. I have, however, made a small attempt to take the ideas presented and generalize them code-wise.","d45409b2":"### Classifier Helper","ff9b9183":"Data Exploratoration\n--------------------","495ae3f8":"### Quantiles","77c6fe76":"### Fill Missing Variables\nFilling in the missing data with the best guess I could come up with.\n- **Embarked**: 'S' was the top result\n- **Age, Fare**: just using the statistical mean\n- **Cabin**: filling with empty, can this be figured out by ticket?","da425e6f":"Create New Features\n-------------------\nkeying off of other notebooks as far as new feature generation goes:\n - **Title**:\n - **FamilySize**:\n - **IsAlone**:\n - **HasCabin**:\n - **Fare_quantile**:\n - **Age_quantile**:","06ff218a":"### Categorize Variables\nnaively converting categories to whatever their index would be via pd.DataFrame.uniques()  \nI kept `category_maps` around as a dictionary of dictionarys of the conversions that take place, but it's only there for reference and debugging. ","5779bf68":"### Hyperparameters","56cf6aab":"Initial Cleanup\n---------------\nBased on the simple exploration it is immediately obvious that data is missing from both the training and test sets. Additonally, there are categorical columns that will need to be converted."}}