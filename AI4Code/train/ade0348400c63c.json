{"cell_type":{"2d5339ce":"code","485f0731":"code","0ccb9dfc":"code","bdd6a6f7":"code","ec18d849":"code","d4b5c1a1":"code","c74f030d":"code","f28b1701":"code","b0d55584":"code","64ac67eb":"code","4120ee20":"code","cc88bd0f":"code","0212b995":"code","d9c4a71a":"code","17fd5a85":"code","b517f543":"code","bc1d3837":"code","f68e5cee":"code","b3a66d17":"code","f2aec375":"code","14ed19c5":"code","b2920be9":"code","75767669":"code","b4004225":"code","ad74d161":"code","2c769d06":"code","e7590355":"code","a7039ee3":"code","3a4a7034":"code","0539587e":"code","06282b48":"code","bd6454ef":"code","f54ec2da":"code","1b94d0b1":"code","2def25c8":"code","91b7d3f8":"code","699e380f":"code","ffdd8176":"code","2d25e7de":"code","bfec0716":"code","7be00415":"code","99df903e":"code","a9476edc":"code","eb3c9675":"code","4ef3fab7":"code","1e4172b4":"code","5199c6c5":"code","7511bf33":"code","a8bfa27a":"code","b0f43ddf":"code","4bfa3f51":"code","e560f0e6":"code","f594205f":"code","8c180498":"code","2bc9de43":"code","96ed94f9":"code","7969dd1d":"code","dce225e1":"code","487e0245":"code","f15fd34d":"code","e6952d24":"code","87f5f781":"code","dc9b4fff":"code","55416fdd":"code","18ff7a59":"code","496fcae6":"code","ecf5aabc":"code","f30122e7":"code","ac425276":"code","9ee9ec96":"code","75f8163f":"code","cb528f22":"code","91a3e57f":"code","9aafd2d4":"code","6620c02b":"code","90337bb8":"code","5052a57c":"code","047858d5":"code","8d445f75":"code","93da8f9d":"code","9ae358b7":"code","816db1b2":"code","da557b39":"code","7fd337d8":"code","04852b90":"code","b2b86934":"code","15d543e0":"code","cccc7ad1":"code","4c2db50a":"code","f529bc1d":"code","3a8c2ffc":"code","c13347cb":"code","068e10d6":"code","dba2b400":"code","7aa49c4b":"code","9dadfa11":"code","4ba77f9e":"code","a7a61aed":"code","aac45b3a":"code","499c3249":"code","76a07d31":"code","890838d5":"code","56e3d16e":"code","92ee28f3":"code","e77e9726":"code","cdb15df1":"code","80e41943":"code","253b99c0":"code","943df8c3":"code","9bba2b80":"code","4611f23f":"code","11159449":"code","8cd9383a":"code","df1f79b4":"code","e7e45cb5":"code","db8595a3":"code","6e7d9220":"code","9f3f023d":"code","6d75ba9f":"code","9d114242":"code","cbb50f17":"code","73bed8fd":"markdown","8bbed9f6":"markdown","3383dd71":"markdown","836743bd":"markdown","66036acb":"markdown","ca54df34":"markdown","533c5d74":"markdown","f5c438ea":"markdown","da4829e0":"markdown","e818a594":"markdown"},"source":{"2d5339ce":"# !pip install textblob\n# !pip install advertools\nimport requests\nimport pandas as pd\nimport json\nfrom pandas.io.json import json_normalize\npd.set_option('display.max_colwidth', -1)\npd.set_option('display.max_columns', None)  \n%config InlineBackend.figure_format = 'retina' # high resolution plotting\nimport matplotlib.pyplot as plt","485f0731":"api_key=\"9ec526b31a214d80bfeb5aa83451239c\"\ntype_of_news=\"everything\" ## everything || top-headlines\ndate_from=\"2019-09-05\"\nquerying_keyword=\"Iron ore\"\npageSize=\"100\"\nsource = \"Bloomberg\"\n#source=\"google-news\"\nnews_feed_url = ('https:\/\/newsapi.org\/v2\/'+type_of_news+'?q='+querying_keyword+'&from='+date_from+'&sortBy=popularity&sources='+source+'&pageSize='+pageSize+'&apiKey='+api_key)","0ccb9dfc":"def get_data(news_feed_url):\n    response = requests.get(news_feed_url)\n    return response.json()\n\na=get_data(news_feed_url)","bdd6a6f7":"#print(a)\nnews_data=json_normalize(a['articles'])\ntype(news_data)","ec18d849":"date_stamp=news_data[\"publishedAt\"].str.split(\"T\",n=1,expand=True)\nnews_data[\"published_date\"]=date_stamp[0]\nnews_data[\"published_time\"]=date_stamp[1].str.split(\"Z\",n=1,expand=True)[0]\n","d4b5c1a1":"news_data.head(1)","c74f030d":"news_data.to_csv('data_bloom_iron0906.csv')","f28b1701":"from IPython.display import FileLink\nFileLink(r'data_bloom_iron0906.csv')","b0d55584":"news_data.head(1)","64ac67eb":"import pandas as pd\npdf_oilprice=pd.read_csv(\"..\/input\/oiltweet280819\/oilprice28082019.csv\",encoding = \"ISO-8859-1\")\n\npdf_oilandgas=pd.read_csv(\"..\/input\/oiltweet280819\/oilandgas28082019.csv\",encoding = \"ISO-8859-1\")\n\npdf_oilwar=pd.read_csv(\"..\/input\/oiltweet280819\/oilwar28082019.csv\",encoding = \"ISO-8859-1\")\n\npdf_pertroleum=pd.read_csv(\"..\/input\/oiltweet280819\/petroleum28082019.csv\",encoding = \"ISO-8859-1\")\n\npdf_oilspill=pd.read_csv(\"..\/input\/oiltweet280819\/oilspill28082019.csv\",encoding = \"ISO-8859-1\")","4120ee20":"pdf_tweetapi=pd.concat([pdf_oilspill,pdf_pertroleum,pdf_oilwar,pdf_oilandgas,pdf_oilprice])","cc88bd0f":"pdf_tweetapi=pdf_tweetapi.reset_index(drop=True)","0212b995":"import pandas as pd\n\npdf_reutersoiltanker2807to2808=pd.read_csv(\"..\/input\/news2807to2808\/data_reuters_oil1.csv\",encoding = \"ISO-8859-1\")\npdf_bloomberg2807to2808=pd.read_csv(\"..\/input\/news2807to2808\/data_bloomberg_oil2807.csv\",encoding = \"ISO-8859-1\")\n\npdf_reutersoiltanker2707to2608=pd.read_csv(\"..\/input\/news2807to2808\/data_reuters_oil.csv\",encoding = \"ISO-8859-1\")\n\npdf_googleoil807to2808=pd.read_csv(\"..\/input\/news2807to2808\/data_google_oil2807.csv\",encoding = \"ISO-8859-1\")\n\npdf_reutersoil2807to2808=pd.read_csv(\"..\/input\/reutersoil2807to2808\/data_reuters_oil2807.csv\",encoding = \"ISO-8859-1\")","d9c4a71a":"pdf_newsapi=pd.concat([pdf_bloomberg2807to2808,pdf_reutersoiltanker2807to2808,pdf_reutersoiltanker2707to2608,pdf_googleoil807to2808,pdf_reutersoil2807to2808])","17fd5a85":"pdf_newsapi=pdf_newsapi.reset_index(drop=True)","b517f543":"pdf_newsapi.head(1)","bc1d3837":"ls ..\/input\/tweeterevents\/*.csv","f68e5cee":"# import pandas as pd\nimport glob\nfilepath=\"..\/input\/tweeterevents\/\"\nfiles=glob.glob(filepath+\"*.csv\")\nprint(\"number of file:\"+str(len(files)))\n# # #pdf_419junoiltanker=pd.read_csv(\"..\/input\/newevents\/4jun-19jun-oiltanker.csv\")\n","b3a66d17":"import pandas as pd\npdf_gulfofomantweet=pd.read_csv(\"..\/input\/tweeterevents\/gulfofoman-twitter.csv\",encoding = \"ISO-8859-1\")\n","f2aec375":"data = pd.read_csv(files[0],encoding = \"ISO-8859-1\")\nfor file in files[1:]:\n    newdata=pd.read_csv(file,encoding = \"ISO-8859-1\")\n    data = pd.concat([data,newdata],axis=0)\npdf_tweet=data.reset_index(drop=True)","14ed19c5":"pdf_gulfofoman915may=pd.read_csv(\"..\/input\/newevents\/9may-15may-gulfofomn.csv\",encoding = \"ISO-8859-1\")\n\npdf_1831janironoredam=pd.read_csv(\"..\/input\/newevents\/18jan031jan-ironore-dam.csv\",encoding = \"ISO-8859-1\")\n\npdf_1831juloilspill=pd.read_csv(\"..\/input\/newevents\/18jul-31jul-oilspill.csv\",encoding = \"ISO-8859-1\")\n\npdf_116jannews=pd.read_csv(\"..\/input\/newevents\/1jan-16jan-news.csv\",encoding = \"ISO-8859-1\")\n\npdf_1831juloilspillchinese=pd.read_csv(\"..\/input\/newevents\/1jan-16jan-chinese.csv\",encoding = \"ISO-8859-1\")\n\npdf_419junoiltanker=pd.read_csv(\"..\/input\/newevents\/4jun-19jun-oiltanker.csv\",encoding = \"ISO-8859-1\")","b2920be9":"pdf_news=pd.concat([pdf_gulfofoman915may,pdf_1831janironoredam,pdf_1831juloilspill,pdf_116jannews,pdf_1831juloilspillchinese,pdf_419junoiltanker])","75767669":"len(pdf_news)","b4004225":"pdf_news=pdf_news.reset_index(drop=True)","ad74d161":"len(pdf_tweet)","2c769d06":"pdf_tweet","e7590355":"pdf_news.to_csv('allnews-sprinklr.csv')","a7039ee3":"from IPython.display import FileLink\nFileLink(r'allnews-sprinklr.csv')","3a4a7034":"#Function to drop null\/\/NAN\n","0539587e":"import re\ndef removeSpecialChar(text):\n    s = re.sub(r\"[^a-z0-9]\",\" \",str(text).lower())\n    return s","06282b48":"from nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \n\n\ndef removeStopWords(text):\n    stop_words = set(stopwords.words('english')) \n    word_tokens = word_tokenize(text) \n    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n    return filtered_sentence\n\n","bd6454ef":"def checkretweet(list_of_words):\n    if list_of_words[0]==\"rt\":\n        return 1\n    else :\n        return 0\n\n    \n","f54ec2da":"from textblob import Word\ndef lemmatize(list_of_words):\n    return [Word(word).lemmatize() for word in list_of_words]\n\n","1b94d0b1":"def countLetters(text):\n    return len(text)\n","2def25c8":"def countWords(text):\n    return len(text.split(\" \"))\n","91b7d3f8":"def mediaScore(text):\n    if str(text).find('LINK'):\n        return 25\n    elif str(text).find('PHOTO'):\n        return 50\n    elif str(text).find('VIDEO'):\n        return 75\n    else :\n        return 0\n\ndef confidenceInterval(df):\n    df['confidence_interval']=np.log(df['SenderAge'])+np.log(df['Sender Followers Count'])+np.log(df['Retweets']) + df['engagement score']+ np.log(df['tweet_number_of_char'])+np.log(df['count_noun'])+np.log(df['count_verb'])+np.log(df['Favorites'])+np.log(df['tweet_number_of_char'])+np.log(df['tweet_number_of_words'])+np.log(df['media_score'])\n    return df\n\n\n        ","699e380f":"df['clean_text']=df['Message'].apply(removeSpecialChar)\ndf[\"clean_stop_words\"]=df[\"clean_text\"].apply(removeStopWords)\n#df[\"is_retweet\"]=df[\"clean_stop_words\"].apply(checkretweet)\ndf[\"lemmatize_stop_clean_text\"]=df[\"clean_stop_words\"].apply(lemmatize)\ndf[\"tweet_number_of_char\"]=df[\"clean_text\"].apply(countLetters)\ndf[\"tweet_number_of_words\"]=df[\"clean_text\"].apply(countWords)\ndf[\"count_pronouns\"]=df[\"Message\"].map(lambda z: countTaggedTokens(z,\"pronoun\"))\ndf[\"count_verb\"]=df[\"Message\"].map(lambda z: countTaggedTokens(z,\"verb\"))\ndf[\"count_noun\"]=df[\"Message\"].map(lambda z: countTaggedTokens(z,\"noun\"))\ndf[\"lemmatize_stop_clean_text_rt\"]=df[\"lemmatize_stop_clean_text\"].apply(cleanRTName)\n","ffdd8176":"new_df=create_engagement_metric(df)\n\nnew_df['media_score']=new_df['MediaTypeList'].apply(mediaScore)\nnew_df=confidenceInterval(new_df)\n#from sklearn.preprocessing import StandardScaler\n#scaler = StandardScaler().fit(new_df['confidence_interval'].values.reshape(1,-1))\n#rescaledX = scaler.transform(new_df['confidence_interval'].values.reshape(1,-1))\n#new_df['scaled_confidenceInterval']=scaler.fit_transform()\n#new_df['scaled_confidenceInterval']=rescaledX\n\n#new_df[['scaled_confidenceInterval','confidence_interval']].head()\n","2d25e7de":"#!pip install advertoolsa=new_df.groupby('Event Name')\na.first()\nnew_df.aggregate({\"Retweets\":['sum','max'], \n              \"Favorites\":['max', 'sum'], \n              \"confidence_interval\":['max']}) ","bfec0716":"news['clean_text']=news['Message'].apply(removeSpecialChar)\nnews['Title']=news['Title'].apply(removeSpecialChar)\n","7be00415":"import advertools as adv","99df903e":"[x for x in dir(adv) if x.startswith('extract')]  # currently available extract functions","a9476edc":"hashtag_summary = adv.extract_hashtags(pdf_gulfofomantweet['Message'])\nhashtag_summary.keys()","eb3c9675":"hashtag_summary['overview']","4ef3fab7":"mention_summary = adv.extract_mentions(pdf_gulfofomantweet['Message'])\nmention_summary.keys()","1e4172b4":"mention_summary['overview']","5199c6c5":"word_summary = adv.extract_words(pdf_gulfofomantweet['Message'], \n                                 words_to_extract=['oil', 'tanker', 'attack',],\n                                 entire_words_only=False)","7511bf33":"word_summary.keys()","a8bfa27a":"word_summary['overview']","b0f43ddf":"word_summary['top_words'][:20]","4bfa3f51":"word_summary_oil = adv.extract_words(pdf_gulfofomantweet['Message'],\n                                          ['oil', 'tanker', 'trump', 'donald','Explosion','Gulf','Iran','Fire','Attack',''])","e560f0e6":"word_summary_oil.keys()","f594205f":"word_summary_oil['top_words'][:20]","8c180498":"emoji_summary = adv.extract_emoji(pdf_gulfofomantweet['Message'])\nemoji_summary.keys()","2bc9de43":"emoji_summary['overview']","96ed94f9":"emoji_summary['emoji_flat'][:10]","7969dd1d":"emoji_summary['emoji_flat_text'][:10]","dce225e1":"list(zip(emoji_summary['emoji_flat'][:10], emoji_summary['emoji_flat_text'][:10]))","487e0245":"plt.figure(facecolor='#ebebeb', figsize=(8, 8))\nplt.bar([x[0] for x in emoji_summary['emoji_freq'][:15]],\n        [x[1] for x in emoji_summary['emoji_freq'][:15]])\nplt.title('Emoji frequency', fontsize=18)\nplt.xlabel('Emoji per tweet', fontsize=12)\nplt.ylabel('Number of tweets', fontsize=12)\nplt.grid(alpha=0.5)\nplt.gca().set_frame_on(False)","f15fd34d":"emoji_summary['top_emoji'][:20]","e6952d24":"emoji_summary['top_emoji_text'][:20]","87f5f781":"# plt.figure(facecolor='#ebebeb', figsize=(8, 8))\n# plt.barh([x[0] for x in emoji_summary['top_emoji_text'][:20]][::-1],\n#          [x[1] for x in emoji_summary['top_emoji_text'][:20]][::-1])\n# plt.title('Top Emoji')\n# plt.grid(alpha=0.5)\n# plt.gca().set_frame_on(False)","dc9b4fff":"mention_summary = adv.extract_mentions(pdf_gulfofomantweet['Message'])\nmention_summary.keys()","55416fdd":"mention_summary['overview']","18ff7a59":"mention_summary['mentions_flat'][:10]","496fcae6":"mention_summary['mention_counts'][:20]","ecf5aabc":"plt.figure(facecolor='#ebebeb', figsize=(8, 8))\nplt.bar([x[0] for x in mention_summary['mention_freq'][:15]],\n        [x[1] for x in mention_summary['mention_freq'][:15]])\nplt.title('Mention frequency', fontsize=18)\nplt.xlabel('Mention per tweet', fontsize=12)\nplt.ylabel('Number of tweets', fontsize=12)\nplt.xticks(range(15))\nplt.yticks(range(0, 2800, 200))\nplt.grid(alpha=0.5)\nplt.gca().set_frame_on(False)","f30122e7":"mention_summary['top_mentions'][:10]","ac425276":"plt.figure(facecolor='#ebebeb', figsize=(8, 8))\nplt.barh([x[0] for x in mention_summary['top_mentions'][:15]][::-1],\n         [x[1] for x in mention_summary['top_mentions'][:15]][::-1])\nplt.title('Top Mentions')\nplt.grid(alpha=0.5)\nplt.xticks(range(0, 1100, 100))\nplt.gca().set_frame_on(False)","9ee9ec96":"question_summary = adv.extract_questions(pdf_gulfofomantweet['Message'])","75f8163f":"question_summary.keys()","cb528f22":"question_summary['overview']","91a3e57f":"question_summary['top_question_marks']","9aafd2d4":"[(i,x) for i, x in  enumerate(question_summary['question_text']) if x][:15]","6620c02b":"intense_summary = adv.extract_intense_words(pdf_gulfofomantweet['Message'], min_reps=3)","90337bb8":"intense_summary['overview']","5052a57c":"intense_summary['top_intense_words'][:20]","047858d5":"extracted_tweets =  (pdf_gulfofomantweet[['Message', 'SenderScreenName', 'Sender Followers Count']]\n .assign(hashtags=hashtag_summary['hashtags'],\n         hashcounts=hashtag_summary['hashtag_counts'],\n         mentions=mention_summary['mentions'],\n         mention_count=mention_summary['mention_counts'],\n         emoji=emoji_summary['emoji'],\n         emoji_text=emoji_summary['emoji_text'],\n         emoji_count=emoji_summary['emoji_counts'],))\nextracted_tweets.head()","8d445f75":"word_freq_mention = adv.word_frequency(extracted_tweets['mentions'].str.join(' '), \n                                       extracted_tweets['Sender Followers Count'].fillna(0))\nword_freq_mention.head(10)","93da8f9d":"pdf_gulfofomantweet['CreatedTime'] = pd.to_datetime(pdf_gulfofomantweet['CreatedTime'])","9ae358b7":"pdf_gulfofomantweet['Createddate'] = pd.to_datetime(pdf_gulfofomantweet['CreatedTime'].dt.date)","816db1b2":"# Count the number of times a date appears in the dataset and convert to dataframe\ntweet_trend = pd.DataFrame(pdf_gulfofomantweet['Createddate'].value_counts())\n\n# index is date, columns indicate tweet count on that day\ntweet_trend.columns = ['tweet_count']\n\n# sort the dataframe by the dates to have them in order\ntweet_trend.sort_index(ascending = True, inplace = True)","da557b39":"# make a line plot of the tweet count data and give some pretty labels! ;)\n# the 'rot' argument control x-axis ticks rotation\nplt.style.use('seaborn-darkgrid')\ntweet_trend['tweet_count'].plot(linestyle = \"-\", figsize = (12,8), rot = 45, color = 'B',\n                               linewidth = 3)\nplt.title('Tweet counts by date', fontsize = 20)\nplt.xlabel('Date', fontsize = 15)\n\nplt.ylabel('Tweet Count', fontsize = 13)","7fd337d8":"#dates_list = ['2019-05-12', '2019-05-13', '2019-05-14', '2019-05-20']\n\n# create a series of these dates.\n#important_dates = pd.Series(pd.to_datetime(dates_list))\nimportant_dates= list(pdf_gulfofomantweet['Createddate'])\n# add columns to identify important events, and mark a 0 or 1.\ntweet_trend['Important Events'] = False\ntweet_trend.loc[important_dates, 'Important Events'] = True\ntweet_trend['values'] = 0\ntweet_trend.loc[important_dates, 'values'] = 1","04852b90":"# Calculate the percentage change in tweet counts\ntweet_trend['Pct_Chg_tweets'] = tweet_trend['tweet_count'].pct_change()*100\n\n# Lets see values only for the important dates. This Pct_Chg_tweets shows us the percentage\n# change in tweets for the day of the event versus the previous day!\ntweet_trend.loc[tweet_trend['values'] == 1,['tweet_count', 'Pct_Chg_tweets']]","b2b86934":"# take a look at what the 'text' column holds\npdf_gulfofomantweet['Message'].tail(10)","15d543e0":"# define a function that takes in a tweet and throws out the text without the RT.\nimport re\ndef remove_retweet(tweet):\n    '''Given a tweet, remove the retweet element from it'''\n    text_only = []\n    if len(re.findall(\"^RT.*?:(.*)\", tweet)) > 0:\n        text_only.append(re.findall(\"^RT.*?:(.*)\", tweet)[0])\n    else:\n        text_only.append(tweet)\n    return text_only[0]\n\n# extract texts and place in a list\ntext_only = pdf_gulfofomantweet['Message'].map(remove_retweet)","cccc7ad1":"# this method checks for links and removes these from the tweet provided!\ndef remove_links(tweet):\n    '''Provide a tweet and remove the links from it'''\n    text_only = []\n    if len(re.findall(\"(https:\/\/[^\\s]+)\", tweet)) > 0:\n        tweet = re.sub(\"(https:\/\/[^\\s]+)\", \"\", tweet)\n    if len(re.findall(\"(http:\/\/[^\\s]+)\", tweet)) > 0:\n        tweet = re.sub(\"(http:\/\/[^\\s]+)\", \"\", tweet)    \n    text_only.append(tweet)\n    return text_only[0]\n\ntext_no_links = text_only.map(remove_links)","4c2db50a":"def remove_hashtags(tweet):\n    '''Provide a tweet and remove hashtags from it'''\n    hashtags_only = []\n    if len(re.findall(\"(#[^#\\s]+)\", tweet)) > 0:\n        tweet = re.sub(\"(#[^#\\s]+)\", \"\", tweet) \n    hashtags_only.append(tweet)\n    return hashtags_only[0]\n\ntext_all_removed = text_no_links.map(remove_hashtags)","f529bc1d":"def remove_extraneous(tweet):\n    '''Given a text, remove unnecessary characters from the beginning and the end'''\n    tweet = tweet.rstrip()\n    tweet = tweet.lstrip()\n    tweet = tweet.rstrip(\")\")\n    tweet = tweet.lstrip(\"(\")\n    tweet = re.sub(\"\\.\", \"\", tweet)\n    return tweet\n\ntext_clean = text_all_removed.map(remove_extraneous)","3a8c2ffc":"# in case hashtags are not found, we will use \"0\" as the placeholder\ndef extract_hashtags(tweet):\n    '''Provide a tweet and extract hashtags from it'''\n    hashtags_only = []\n    if len(re.findall(\"(#[^#\\s]+)\", tweet)) > 0:\n        hashtags_only.append(re.findall(\"(#[^#\\s]+)\", tweet))\n    else:\n        hashtags_only.append([\"0\"])\n    return hashtags_only[0]\n\n# make a new column to store the extracted hashtags and view them!\npdf_gulfofomantweet['tweet_hashtags'] = pdf_gulfofomantweet['Message'].map(extract_hashtags)\npdf_gulfofomantweet['tweet_hashtags'].head(10)","c13347cb":"# create a list of all hashtags\nall_hashtags = pdf_gulfofomantweet['tweet_hashtags'].tolist()\n\n# Next we observe that our all_hashtags is a list of lists...lets change that\ncleaned_hashtags = []\nfor i in all_hashtags:\n    for j in i:\n            cleaned_hashtags.append(j)\n\n# Convert cleaned_hashtags to a series and count the most frequent occuring\ncleaned_hashtag_series = pd.Series(cleaned_hashtags)\nhashtag_counts = cleaned_hashtag_series.value_counts()","068e10d6":"# Get hashtag terms from the series and convert to list\nhashes = cleaned_hashtag_series.values\nhashes = hashes.tolist()\n\n# convert list to one string with all the words\nhashes_words = \" \".join(hashes)\n\n# generate the wordcloud. the max_words argument controls the number of words on the cloud\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width= 1600, height = 800, \n                      relative_scaling = 1.0, \n                      colormap = \"Blues\",\n                     max_words = 100).generate(hashes_words)\n\nplt.figure(figsize=(20,10))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","dba2b400":"hashtag_date_df = pdf_gulfofomantweet[['Createddate', 'tweet_hashtags']]\nhashtag_date_df = hashtag_date_df.reset_index(drop = True)\n\n# extract a list of hashtags from the dataframe\nall_hashtags = hashtag_date_df['tweet_hashtags'].tolist()\n\nhashtag_date_df.head()","7aa49c4b":"pdf_gulfofomantweet[['SenderListedName', 'Language', 'Sender Followers Count']].sort_values('Sender Followers Count', \n                                                               ascending = False)[:20]","9dadfa11":"# First we get a count of users from each time-zone and language combination!\nuser_loc_lang = pdf_gulfofomantweet.groupby(['LanguageCode', 'Language'])['SenderUserId'].agg('count').reset_index()\nuser_loc_lang.rename(columns = {'SenderUserId':'user_count'}, inplace = True)\nuser_loc_lang.head(5)","4ba77f9e":"user_tweet_count = pdf_gulfofomantweet.groupby('SenderUserId')['Message'].agg('count').reset_index()\nuser_tweet_count.rename(columns = {'text':'Tweet_count'}, inplace = True)","a7a61aed":"pdf_gulfofomantweet['Favorites'].fillna(0, inplace=True)","aac45b3a":"pdf_gulfofomantweet['Retweets'].fillna(0, inplace=True)","499c3249":"def create_engagement_metric(df):\n    working_df = df.copy()\n    \n    from sklearn.preprocessing import MinMaxScaler\n    # Favorites\n    fav_eng_array = df['Favorites'] \/ df['Sender Followers Count']\n    scaler = MinMaxScaler().fit(fav_eng_array.values.reshape(-1, 1))\n    scaled_favs = scaler.transform(fav_eng_array.values.reshape(-1, 1))\n    \n    # Retweets\n    rt_eng_array = df['Retweets'] \/ df['Sender Followers Count']\n    scaler = MinMaxScaler().fit(rt_eng_array.values.reshape(-1, 1))\n    scaled_rts = scaler.transform(rt_eng_array.values.reshape(-1, 1))\n    \n    mean_eng = (scaled_favs + scaled_rts) \/ 2\n    working_df['engagement score'] = mean_eng\n    \n    return working_df\n","76a07d31":"eng=create_engagement_metric(pdf_gulfofomantweet)","890838d5":"#drop \n#SenderProfileImgUrl\n#SenderScreenName\n#SenderProfileLink,SenderInfluencerScore,SenderAge,SenderGender,Title\n#ReceiverId\tReceiverScreenName\tAssignedBy\tAssignedTo\tSpam\tStatus\tIntel Location\tPriority\tStar Rating\n#Geo Target\tPost Id\tAssociated Cases\tLocation\tCountry\tState\n#Sender Email\n#manipulate SenderAge \n#explore sarcasm code - \n#world map \n#mention map for engagement\n#\n#feature creation with graphs eg age,engage etc","56e3d16e":"import numpy as np\nimport pandas as pd\nimport nltk\nnltk.download('punkt') # one time execution\nimport re\n!pip install rouge\nfrom rouge import Rouge\nrouge = Rouge()\nimport string\nimport networkx as nx\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')","92ee28f3":"# Function to tokenize sentences properly\ndef clean_tokenize(full_text):\n    full_text = re.sub(r\"\\\\'\", \"'\", full_text)        # few modifications in text\n    full_text = re.sub(r\"U.S.\", \"US\", full_text)\n    #full_text = re.sub(r\"[^a-zA-Z0-9]\", \" \", full_text)\n    full_text = full_text.replace('\\n','')\n  \n  \n    tokenized = nltk.sent_tokenize(full_text)         # nltk tokenizer\n  \n    for sentence in tokenized:                        # identifying correct positions for tokenization\n        x=re.findall(r'\\w+[.?!][A-Z]+', sentence)\n        all_delm = []\n        punctuation = [\".\",\"!\",\"?\"]\n        for punct in punctuation:\n            for occurrence in x:\n                try:\n                    idx1 = occurrence.index(punct)\n                    idx2 = sentence.index(occurrence)\n                    punct_idx = idx1+idx2\n                    all_delm .append(punct_idx)\n                except:\n                    continue\n          \n        all_delm.sort()\n        good_tok = []\n        lower_idx = 0\n        higher_idx = 0\n    \n        for i in range(len(all_delm)):                  # creating list of properly tokenized text\n            if i!=0:\n                lower_idx = all_delm[i-1]+1\n            higher_idx = all_delm[i]+1\n            good_tok.append(sentence[lower_idx:higher_idx])\n        good_tok.append(sentence[higher_idx:])\n    \n        sent_idx = tokenized.index(sentence)            \n        for i in range(len(good_tok)):\n            tokenized.insert(sent_idx+i+1, good_tok[i])\n        tokenized.pop(sent_idx)\n  \n    #print (tokenized)\n    return tokenized","e77e9726":"# function to remove stopwords\ndef remove_stopwords(sen):\n    sen_new = \" \".join([i for i in sen if i not in stop_words])\n    return sen_new","cdb15df1":"embeddings_index = {}\nf = open('..\/input\/glove-840b-300d\/glove.840B.300d.txt')\nfor line in f:\n    values = line.split(' ')\n    word = values[0] ## The first entry is the word\n    coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n    embeddings_index[word] = coefs\nf.close()\n\nprint('GloVe data loaded')","80e41943":"# convert sentence to vector\ndef convert_to_vectors(clean_sentences):\n    sentence_vectors = []\n    for i in clean_sentences:\n        if len(i) != 0:\n            v = sum([embeddings_index.get(w, np.zeros((300,))) for w in i.split()])\/(len(i.split())+0.001)\n        else:\n            v = np.zeros((300,))\n        sentence_vectors.append(v)\n    return sentence_vectors","253b99c0":"# calculate similarity of sentences\ndef calculate_similarities(clean_sentences,sentence_vectors):\n    sim_mat = np.zeros([len(clean_sentences), len(clean_sentences)])           # create similarity matrix\n    for i in range(len(clean_sentences)):\n        for j in range(len(clean_sentences)):\n            if i != j:\n                sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,300), sentence_vectors[j].reshape(1,300))[0,0]\n    return sim_mat","943df8c3":"# use textrank algorithm to calculate similarity\ndef TextRank(sim_mat):\n    nx_graph = nx.from_numpy_array(sim_mat)\n    scores = nx.pagerank(nx_graph)\n    return scores","9bba2b80":"\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\n\ndef find_relations(tweets, feature_extraction, tdm):\n    \"\"\"\n    Go through all the tweets and create a map for each tweet and\n    there cosine similarities\n    \"\"\"\n    # a list of dictionaries containing list of related tweets and the\n    # cosine value\n    cosine_value_map = []\n    for tweet in tweets:\n        temp = {tweet:[]}\n        query = feature_extraction.transform([tweet])\n        cosine_similarities = linear_kernel(query, tdm).flatten()\n        related_docs_indices = cosine_similarities.argsort()[:-5:-1]\n        for index in related_docs_indices:\n            temp[tweet].append((tweets[index], cosine_similarities[index]))\n        cosine_value_map.append(temp)\n    return cosine_value_map\n","4611f23f":"feature_extraction = TfidfVectorizer(analyzer=\"word\")\ntdm = feature_extraction.fit_transform(df[\"text\"])\nrelations = find_relations(df[\"text\"], feature_extraction, tdm)\ndata = {'tweet_one':[], 'tweet_two':[], 'cosine_relation':[]}\nlower_threshold=0.5\nhigher_threshold=0.8\nfor item in relations:\n    for key in item.keys():\n        for processed_data in item[key]:\n            if key != processed_data[0] and processed_data[1]>=lower_threshold and  processed_data[1]<=higher_threshold:\n                data['tweet_one'].append(key)\n                data['tweet_two'].append(processed_data[0])\n                data['cosine_relation'].append(processed_data[1])\na=pd.DataFrame.from_dict(data)\n        \na.head(100)","11159449":"\nfrom nltk.corpus import stopwords\nfrom nltk.cluster.util import cosine_distance\nimport numpy as np\nimport networkx as nx\n\ndef sentence_similarity(sent1, sent2, stopwords=None):\n    if stopwords is None:\n        stopwords = []\n \n    sent1 = [w.lower() for w in sent1]\n    sent2 = [w.lower() for w in sent2]\n \n    all_words = list(set(sent1 + sent2))\n \n    vector1 = [0] * len(all_words)\n    vector2 = [0] * len(all_words)\n \n    # build the vector for the first sentence\n    for w in sent1:\n        if w in stopwords:\n            continue\n        vector1[all_words.index(w)] += 1\n \n    # build the vector for the second sentence\n    for w in sent2:\n        if w in stopwords:\n            continue\n        vector2[all_words.index(w)] += 1\n \n    return 1 - cosine_distance(vector1, vector2)\n \ndef build_similarity_matrix(sentences, stop_words):\n    # Create an empty similarity matrix\n    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n \n    for idx1 in range(len(sentences)):\n        for idx2 in range(len(sentences)):\n            if idx1 == idx2: #ignore if both are same sentences\n                continue \n            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n\n    return similarity_matrix\ndef generate_summary(sentences,top_n):\n    stop_words = stopwords.words('english')\n    summarize_text = []\n    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n    print(\"Sentence Similarity Matrix Generation Completed\")\n    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n    scores = nx.pagerank(sentence_similarity_graph)\n\n    # Step 4 - Sort the rank and pick top sentences\n    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n\n    for i in range(top_n):\n        summarize_text.append(\" \".join(ranked_sentence[i][1]))\n\n    # Step 5 - Offcourse, output the summarize texr\n    print(\"Summarize Text: \\n\", \".\".join(summarize_text))","8cd9383a":"articles = list(news_data['title'])\nsummaries = list(news_data['description'])","df1f79b4":"def generate_summary(text):\n    sentences = clean_tokenize(text)                                                              # tokenize text\n    sentences = list(filter(lambda a: a != \"\", sentences))                                        # remove empty string if any (Encode methods gives error if empty string is encountered)\n    clean_sentences = pd.Series(sentences)                                                        # make a copy in pandas for processing\n    clean_sentences = clean_sentences.apply(lambda s: s.translate(str.maketrans('', '', string.punctuation)))\n    clean_sentences = [s.lower() for s in clean_sentences]                                        # make alphabets lowercase\n    clean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]                      # remove stopwords from the sentences\n    sentence_vectors = convert_to_vectors(clean_sentences)                                        # create vectors for sentences\n    sim_mat = calculate_similarities(clean_sentences,sentence_vectors)                            # calculate similarities between sentences\n    scores = TextRank(sim_mat)                                                                    # applying textrank algorithm\n    ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)      # sort sentences on the basis of similarity\n    \n   \n    return ranked_sentences","e7e45cb5":"generate_summary(summaries[5])","db8595a3":"avg_rouge = [0]*9","6e7d9220":"for i in range(5):  #range = len(articles)\n    actual_summary = summaries[i]\n    hypothesis = generate_summary(articles[i])\n    \n    print (\"Actual Summary : \", end = \"\")\n    print (actual_summary)\n    print (\"\\n\")\n    print (\"Predicted Summary : \", end = \"\")\n    print (hypothesis)\n    print (\"\\n\")\n    print (\"Rouge Score : \", end = \"\")\n    score = rouge.get_scores(hypothesis, actual_summary)\n    print (score)\n    \n    dic = score[0]\n    j = 0\n    for k1,v1 in dic.items():\n        for k2,v2 in v1.items():\n            avg_rouge[j%9]+= v2\n            j+=1\n    \n    print (\"\\n\")\n    print (\"\\n\")","9f3f023d":"n = len(avg_rouge)\nfor i in range(n):    \n    avg_rouge[i]\/=500    # divide by len(articles)","6d75ba9f":"print (\"-------Rouge Scores-------\")\nprint (\"-----------with-----------\")\nprint (\"-----GLOVE Embeddings-----\")\n'''\nprint (\"Rouge 1\")\nprint (\"f = \",avg_rouge[0])\nprint (\"p = \",avg_rouge[1])\nprint (\"r = \",avg_rouge[2])\nprint (\"\\n\")\nprint (\"Rouge 2\")\nprint (\"f = \",avg_rouge[3])\nprint (\"p = \",avg_rouge[4])\nprint (\"r = \",avg_rouge[5])\nprint (\"\\n\")\n'''\nprint (\"\\n\")\nprint (\"Rouge L\")\nprint (\"f = \",avg_rouge[6])\nprint (\"p = \",avg_rouge[7])\nprint (\"r = \",avg_rouge[8])","9d114242":"import pyLDAvis\nimport pyLDAvis.gensim","cbb50f17":"# import networkx as nx\n# import community\n# import matplotlib.pyplot as plt\n\n# G = nx.karate_club_graph()  # load a default graph\n\n# partition = community.best_partition(G)  # compute communities\n\n# pos = nx.spring_layout(G)  # compute graph layout\n# plt.figure(figsize=(8, 8))  # image is 8 x 8 inches\n# plt.axis('off')\n# nx.draw_networkx_nodes(G, pos, node_size=600, cmap=plt.cm.RdYlBu, node_color=list(partition.values()))\n# nx.draw_networkx_edges(G, pos, alpha=0.3)\n# plt.show(G)","73bed8fd":"#LIST OF FUNCTIONS","8bbed9f6":"We can see that ------------------******\nRetweets begin with the keyword 'RT'. \nThese are followed by @userkey.\nHashtags begin with a # and are one continuous string with a space next to them!\n\nLinks begin with https:\/\/ or http:\/\/ and can be present anywhere in the string.\nThere can be multiple links and hashtags in a tweet, but retweet identifier is just one.\nUser mentions begin with '@' and are a continuous word!","3383dd71":"> Emoji summary vertically","836743bd":"2. Sprinklr-  \nEvent based News,  \nEvent based Tweeter data ","66036acb":"We have 10+ days  of tweets  Times appear with these dates, so let's create a new column to hold only the date component of this!","ca54df34":"**Data Load**\n> ","533c5d74":"2.Sprinklr- Events bases News,Tweeter loaded ....................................","f5c438ea":"Which users had the most influence?","da4829e0":"API News and Tweet load","e818a594":"1.APIs-\nNews API, \nTwitter API "}}