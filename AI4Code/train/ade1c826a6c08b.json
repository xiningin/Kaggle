{"cell_type":{"2d233d93":"code","9d716242":"code","c5fddf83":"code","587b8e4e":"code","097b492d":"code","13182a25":"code","ef066d23":"code","6a171bfe":"code","cbaaf3d1":"code","28a68be8":"code","b139ce6e":"code","b8dff19d":"markdown","73be8211":"markdown","af8f55c4":"markdown","edc88aab":"markdown","07874eed":"markdown","c51597eb":"markdown","b6e6bb31":"markdown","bb54580d":"markdown","9bd72377":"markdown","b4f79de9":"markdown","0eb5732e":"markdown","cf46d3c9":"markdown","8b65121e":"markdown","36825a07":"markdown","8e70ca8f":"markdown","152c2b0c":"markdown","f2b2cddf":"markdown","855f3305":"markdown","5da50a89":"markdown"},"source":{"2d233d93":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport tensorflow as tf #Import tensorflow in order to use Keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer #Add the keras tokenizer for tweet tokenization\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences #Add padding to help the Keras Sequencing\nimport tensorflow.keras.layers as L #Import the layers as L for quicker typing\nfrom tensorflow.keras.optimizers import Adam #Pull the adam optimizer for usage\n\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy #Loss function being used\nfrom sklearn.model_selection import train_test_split #Train Test Split\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9d716242":"hotel = pd.read_csv(\"..\/input\/trip-advisor-hotel-reviews\/tripadvisor_hotel_reviews.csv\") #Load the review data into pandas\nhotel.head() #Take a peek at the dataset","c5fddf83":"print(hotel.isnull().any()) #Check if there are any null values\nprint(hotel[\"Rating\"].value_counts()) #Checks the rating values in case there is a weird value\nprint(hotel.loc[hotel[\"Review\"] == \"\"]) #Checks for empty review strings","587b8e4e":"print(hotel[\"Review\"][26]) #Print a random review to show off the structure.","097b492d":"punctuations = \"\"\"!()-![]{};:,+'\"\\,<>.\/?@#$%^&*_~\u00c2\"\"\" #List of punctuation to remove\n\n#ReviewParse: Takes the stubborn punctuation off the words for a single review\n#Input: the review to parse\n#Output: the parsed review\ndef reviewParse(review):\n    splitReview = review.split() #Split the review into words\n    parsedReview = \"\".join([word.translate(str.maketrans('', '', punctuations)) + \" \" for word in splitReview]) #Takes the stubborn punctuation out\n    return parsedReview #Returns the parsed review\n\nhotel[\"CleanReview\"] = hotel[\"Review\"].apply(reviewParse) #Parse all the reviews for their punctuation and add it into a new column\nhotel.head() #Take a peek at the dataset","13182a25":"review = hotel[\"CleanReview\"].copy() #Use a copy of the clean reviews\n\n#Print an example sentence to make sure everything is working\nprint(\"Example Sentence: \") \nprint(review[26])\n\ntoken = Tokenizer() #Initialize the tokenizer\ntoken.fit_on_texts(review) #Fit the tokenizer to the reviews\ntexts = token.texts_to_sequences(review) #Convert the reviews into sequences for keras to use\n\n#Print an example sequence to make sure everything is working\nprint(\"Into a Sequence: \")\nprint(texts[26])\n\ntexts = pad_sequences(texts, padding='post') #Pad the sequences to make them similar lengths\n\n#Print an example padded sequence to make sure everything is working\nprint(\"After Padding: \")\nprint(texts[26])","ef066d23":"#EncodeLabel: encode the labels into 0, 1, and 2, back to the issue of explaining positive and extremely positive to a machine\n#Input: the star rating\n#Output: 0, 1, or 2 indicating rating positivity\/negativity\ndef encodeLabel(label):\n    if label == 5 or label == 4: #If the rating is generally positive\n        return 2 #Give the rating a 2 for positive\n    if label == 3: #If the rating is generally neutral\n        return 1 #Give the rating a 1 for neutral\n    return 0 #Give the rating a 0 for negative\n\nlabels = [\"Negative\", \"Neutral\", \"Positive\"] #Give our labels a name\nhotel[\"EncodedRating\"] = hotel[\"Rating\"].apply(encodeLabel) #Encode the ratings to positivity labels\nhotel.head() #Take a peek at the dataset with the new labels","6a171bfe":"#Split the data for training and testing\ntextTrain, textTest, ratingTrain, ratingTest = train_test_split(texts, hotel[\"EncodedRating\"], test_size = 0.33, random_state = 24)","cbaaf3d1":"size = len(token.word_index) + 1 #Set the number of words for the size\nratings = hotel[\"EncodedRating\"].copy() #Get the encoded ratings from the dataframe\n\ntf.keras.backend.clear_session() #Clear any previous model building\n\nepoch = 2 #Number of runs through the data\nbatchSize = 32 #The number of items in each batch\noutputDimensions = 16 #The size of the output\nunits = 256 #Dimensions of the output space\n\nmodel = tf.keras.Sequential([ #Start the sequential model, doing one layer after another in a sequence\n    L.Embedding(size, outputDimensions, input_length = texts.shape[1]), #Embed the model with the number of words and size\n    L.Bidirectional(L.LSTM(units, return_sequences = True)), #Make it so the model looks both forward and backward at the data\n    L.GlobalMaxPool1D(), #Take the max values over time\n    L.Dropout(0.3), #Make the dropout 0.3, making about a third 0 to prevent overfitting\n    L.Dense(64, activation=\"relu\"), #Create a large dense layer\n    L.Dropout(0.3), #Make the dropout 0.3, making about a third 0 to prevent overfitting\n    L.Dense(3) #Create a small dense layer\n])\n\n\nmodel.compile(loss = SparseCategoricalCrossentropy(from_logits = True), #Compile the model with a SparseCategorical loss function\n              optimizer = 'adam', metrics = ['accuracy'] #Add an adam optimizer and collect the accuracy along the way\n             )\n\nhistory = model.fit(textTrain, ratingTrain, epochs = epoch, validation_split = 0.2, batch_size = batchSize) #Fit the model to the data","28a68be8":"predict = model.predict_classes(textTest) #Predict ratings based on the model\nloss, accuracy = model.evaluate(textTest, ratingTest) #Get the loss and Accuracy based on the tests\n\n#Print the loss and accuracy\nprint(\"Test Loss: \", loss)\nprint(\"Test Accuracy: \", accuracy)","b139ce6e":"from sklearn.metrics import classification_report #Import a classification report\nprint(classification_report(ratingTest, predict, target_names = labels)) #Print a classification report","b8dff19d":"# Label Encoding","73be8211":"# Review Punctuation Cleaning","af8f55c4":"# Check for Nulls","edc88aab":"---","07874eed":"# Model Training","c51597eb":"---","b6e6bb31":"---","bb54580d":"---","9bd72377":"# Trip Advisor Hotel Review NLP Project","b4f79de9":"# Make Predictions","0eb5732e":"---","cf46d3c9":"The accuracy on this came to about 84%, which is a whole lot better than my previous NLP's 76% (the Coronavirus Tweet Classification Project used spacy and not keras). This is a pretty big jump. Also, review data is a whole lot bigger than tweet data typically, yet the overall process took about the same amount of time between Keras and Spacy. \n\nAs for the overall data, the model handled the positive values a whole lot better than the others. This is likely due to the huge desparity between the number of positive reviews versus the number of the other labels, which I did point out might come into play earlier. It did come to an 84% accuracy, though, which is a B. I will gladly take this over the 76% with Spacy, as that is an 8% difference.","8b65121e":"Sources for future use: https:\/\/keras.io\/guides\/working_with_rnns\/ , https:\/\/www.kaggle.com\/shahraizanwar\/covid19-tweets-sentiment-prediction-rnn-85-acc ","36825a07":"There are no null values to look at here. The Ratings column also only has values from 1 to 5, so no weird values. There are definitely more positive than negative reviews, so that is something to keep in mind for the model. As for ratings, there are no blank strings. That means there is no cleaning that needs to be done in that regard.","8e70ca8f":"Coded by Luna McBride.\n\nThe purpose for this project is to test Keras for Natural Language Processing. Thus, I will use the keras tokenizer and stuff like that to process instead of Spacy, like I used previously.","152c2b0c":"It also seems as though the reviews have all had their stopwords removed, as seen by the review above. There is still some punctuation I may want to remove, so I will still do that.","f2b2cddf":"# Tokenizing and Padding","855f3305":"---","5da50a89":"Attached punctuation guide for future reference: https:\/\/stackoverflow.com\/questions\/265960\/best-way-to-strip-punctuation-from-a-string"}}