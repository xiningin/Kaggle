{"cell_type":{"a9779e7e":"code","c2ecabc3":"code","ccaf451c":"code","56fa78ab":"code","e7ff9beb":"code","cc2ae8ac":"code","630cd35c":"code","87699908":"code","7bffb808":"code","636f40c9":"code","2ad2d61c":"code","1324be7a":"code","3cc4c254":"code","63ad2b49":"code","1dd3b69f":"code","eb3b502a":"code","1d2ef16a":"code","8c3dd171":"code","7e3b3dc0":"code","1a637541":"code","825a7082":"code","6dafe42e":"code","1480b135":"code","f524b1c9":"code","278b8a52":"code","15118d55":"code","a19cfdf0":"code","af66d66b":"code","b837718a":"code","12eaf592":"code","b277961c":"code","ce3ddf1c":"code","24cc62a6":"code","50f7bbc5":"code","09b54490":"code","5e9aff69":"code","267220b4":"code","0019009c":"code","cdbec328":"code","38eddcef":"code","809cdb57":"code","a70c8342":"code","4f4fa8d9":"code","de1f4dc3":"code","22be85ef":"code","b54b1f6f":"code","100304c0":"code","4fa95b59":"code","d464d23a":"code","c50388e6":"code","4b334a65":"code","463967cc":"code","f1183099":"code","3bc67fb0":"code","5ec07d40":"code","e6842667":"code","187211ab":"code","017d0b77":"markdown","d9263a5c":"markdown","2af962ee":"markdown","babd5105":"markdown","bf6f8c65":"markdown","a5107f00":"markdown","c7bce231":"markdown","cdf552dc":"markdown","f58bdc68":"markdown","e3cece35":"markdown","b57d5f0d":"markdown","8e428fd3":"markdown","5b7c5a2b":"markdown","a11a8601":"markdown","2b51f067":"markdown","523b4475":"markdown","2f9c9ab1":"markdown","af565697":"markdown","ec29f500":"markdown","5e989cc8":"markdown","1018485c":"markdown","739981fe":"markdown","abd24e73":"markdown","2c37ed1e":"markdown","bac63173":"markdown","fa0d0a96":"markdown","7e88d504":"markdown","4023674f":"markdown","be900da2":"markdown","d70d7421":"markdown","0a44f2b2":"markdown","d0667892":"markdown","79d77988":"markdown","07c8670a":"markdown","5f1136ed":"markdown","aa5efe4f":"markdown"},"source":{"a9779e7e":"!pip install gensim\n!pip install keras==2.24\n!pip install pandas==0.23.4","c2ecabc3":"import os\nimport numpy as np\nimport pandas as pd\nimport gensim\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nfrom collections import Counter\n\nfrom gensim.models import Word2Vec\n\nimport keras\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport math","ccaf451c":"pd.set_option('display.max_rows', 10)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","56fa78ab":"EMBED_DIM=10\nMAX_SEQ_LENGTH=100","e7ff9beb":"# filename = os.listdir(\"..\/input\")[1]\n# path = os.path.join(\"..\",\"input\",filename)\n# path","cc2ae8ac":"path = \"..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv\"\ndf_input = pd.read_csv(path, encoding=\"ISO-8859-1\", names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])","630cd35c":"df_raw = df_input[['text','target']]\ndf_raw=df_raw.rename(columns={\"target\": \"prediction\"})\n\nprint(df_raw.head())","87699908":"# dataset_filename = os.listdir(\"..\/input\")[1]\n# dataset_path = os.path.join(\"..\",\"input\",dataset_filename)\n","7bffb808":"tweet_stock_l=[]\nfor i in range(8):\n    tweet_stock_l.append(pd.DataFrame(columns=['Date','created_at', 'text', 'user_id_str']))","636f40c9":"for dirname, _, filenames in os.walk('\/kaggle\/input\/tweetpredictstock\/tweet_train\/'):\n    for filename in filenames:        \n        path = os.path.join(dirname,filename)\n        sts = dirname.split('\/')\n        idx = int(sts[len(sts)-1][0])-1\n        tweet_raw_df = pd.read_json(path, lines=True,orient='columns')\n        tweet_raw_df[\"Date\"] = filename\n        #tweet_raw_df = tweet_raw_df[['Date','created_at', 'text', 'user_id_str']]\n        tweet_stock_l[idx] = pd.concat([tweet_stock_l[idx], tweet_raw_df], ignore_index=True, sort=False)","2ad2d61c":"test_l=[]\nfor i in range(8):\n    test_l.append(pd.DataFrame(columns=['Date','created_at', 'text', 'user_id_str']))","1324be7a":"for dirname, _, filenames in os.walk('\/kaggle\/input\/tweet-testing\/'):\n    for filename in filenames:        \n        path = os.path.join(dirname,filename)\n        sts = path.split('\/')\n        idx = int(sts[len(sts)-2][0])-1\n        test_df = pd.read_json(path, lines=True,orient='columns')\n        test_df[\"Date\"] = filename\n        test_l[idx] = pd.concat([test_l[idx], test_df], ignore_index=True, sort=True).sort_values(by='Date')","3cc4c254":"print(\"Training Data:\")\nprint(tweet_stock_l[1].iloc[0:2,0:3])\n\nprint(\"\\nTesting Data:\")\nprint(test_l[1].iloc[0:2,0:3])","63ad2b49":"for i in range(8):\n    if(i<=2):\n        print(\"(Train data) Stock=\"+str(i+1))\n        print(tweet_stock_l[i].groupby(['Date'])['Date'].count().reset_index(name='count').sort_values(['count'], ascending=False).head(5))","1dd3b69f":"for i in range(8):\n    if(i<=2):\n        print(\"(Test data) Stock=\"+str(i+1))\n        print(test_l[i].groupby(['Date'])['Date'].count().reset_index(name='count').sort_values(['count'], ascending=False).head(5))","eb3b502a":"# dataset_filename = os.listdir(\"..\/input\")[1]\n# dataset_path = os.path.join(\"..\",\"input\",dataset_filename)\n\nraw_price_train_l = []\nfor i in range(1,9,1):\n    filename = \"\/kaggle\/input\/tweetpredictstock\/raw_price_train\/\"+str(i)+\"_r_price_train.csv\"\n    print(filename)\n    raw_price_file = pd.read_csv(filename)\n    price_train = pd.DataFrame(raw_price_file)\n    raw_price_train_l.append(price_train)","1d2ef16a":"def mapLabel(input_val):\n    raw_val = int(input_val)\n    if(raw_val==0):\n        return 0\n    elif(raw_val==2):\n        return 99\n    elif(raw_val==4):\n        return 1\n\ndf_raw.prediction = df_raw.prediction.apply(lambda x: mapLabel(x))","8c3dd171":"df_raw = df_raw[df_raw.prediction != 99]\ndf_pos = df_raw[df_raw.prediction == 1]\ndf_pos=df_pos.sample(frac=0.1, replace=True, random_state=1)\ndf_neg = df_raw[df_raw.prediction == 0]\ndf_neg=df_neg.sample(frac=0.1, replace=True, random_state=1)\nframe = [df_pos, df_neg]\ndf_negative_positive = pd.concat(frame)\n\nprint(df_negative_positive.shape[0])","7e3b3dc0":"#tokenizer that can filter out most punctuation, filtered out words e.g. \"book\", \"didn't\nreg_filter = \"[\\w']+\"\ntoker = RegexpTokenizer(reg_filter) \n\n#lemmatizer return context of word, e.g.\"better->good\", \"rocks->rock\"\nwordnet_lemmatizer=WordNetLemmatizer()\nstopWords = set(stopwords.words('english'))\nstopWords.remove(\"hadn't\")\nstopWords.remove(\"didn't\")","1a637541":"df_tokenized = df_negative_positive\n\n\ndef dataPreProcessing(row):\n    row_lower=row.lower()\n    \n    #tokenization\n    token_l=toker.tokenize(row_lower)\n    \n    #remove stopwrods\n    token_l_no_stop_word=[x for x in token_l if x not in stopWords ]\n\n    #lematization\n    token_l_lemmatized=[wordnet_lemmatizer.lemmatize(x) for x in token_l_no_stop_word]\n    \n    return token_l_lemmatized\n    \ndef joinToken(row):\n    clean_text=\" \".join(row)\n    return clean_text\n\ndf_tokenized['token_list'] = df_tokenized.text.apply(lambda row: dataPreProcessing(row))\ndf_tokenized['clean_text'] = df_tokenized.token_list.apply(lambda row: joinToken(row))\ndf_tokenized=df_tokenized[['text','token_list','clean_text','prediction']]\n\nstoke_tokenized_l=[]\nfor df in tweet_stock_l:\n    df=df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n    df.text = df.text.apply(lambda row: joinToken(row))\n    df['token_list'] = df.text.apply(lambda row: dataPreProcessing(row))\n    df['clean_text'] = df.token_list.apply(lambda row: joinToken(row))\n    stoke_tokenized_l.append(df)\n    \ntest_tokenized_l=[]\nfor df in test_l:\n    df=df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)\n    df.text = df.text.apply(lambda row: joinToken(row))\n    df['token_list'] = df.text.apply(lambda row: dataPreProcessing(row))\n    df['clean_text'] = df.token_list.apply(lambda row: joinToken(row))\n    test_tokenized_l.append(df)","825a7082":"print(df_tokenized[[\"clean_text\",\"token_list\"]].head())","6dafe42e":"stoke_tokenized_l[0]","1480b135":"print(stoke_tokenized_l[0][[\"Date\",\"clean_text\"]].head())","f524b1c9":"test_tokenized_l[0]","278b8a52":"print(test_tokenized_l[0][[\"Date\",\"clean_text\"]].head())","15118d55":"df_positive = df_tokenized[ df_tokenized['prediction'] == 1]\ndf_negative = df_tokenized[ df_tokenized['prediction'] == 0]\n\n\nfig = plt.figure(1,figsize=(30, 20))\ndef makeWordCloud(df, title, a):\n    c = Counter()\n    for i, row in df.iterrows():\n        c.update(row['token_list']) \n\n    cleaned_word = \" \".join(list(df['clean_text']))\n    wc = WordCloud(width=500, height=500, background_color='white',random_state=0)\n    wc.generate_from_frequencies(c)\n    axis_1 = fig.add_subplot(4,5,(a))\n    axis_1.imshow(wc)\n    axis_1.axis('off')\n    plt.title(title)\n    \nmakeWordCloud(df_positive, \"Positive\", 1)\nmakeWordCloud(df_negative, \"Negative\", 2)\n\nfor i in range(8):\n    makeWordCloud(stoke_tokenized_l[i], \"Training Data: Stock=\"+str(i+1), (4+i))\n    \nfor i in range(8):\n    makeWordCloud(test_tokenized_l[i], \"Testing Data: Stock=\"+str(i+1), (13+i))","a19cfdf0":"tokens_all = [ x for x in df_tokenized.token_list]\n\nword_to_vec_model = gensim.models.word2vec.Word2Vec(size=EMBED_DIM, window=5, min_count=1, workers=4, sg=0)\nword_to_vec_model.build_vocab(tokens_all)","af66d66b":"word_to_vec_model.train(tokens_all, total_examples=len(tokens_all), epochs = 4)","b837718a":"word_to_vec_model.most_similar(\"great\")","12eaf592":"toker = Tokenizer()\ntoker.fit_on_texts(df_tokenized.clean_text)\nwd_idx = toker.word_index","b277961c":"seq = toker.texts_to_sequences(df_tokenized.clean_text)","ce3ddf1c":"print(df_tokenized[\"token_list\"].head(1))\nprint(seq[:1])","24cc62a6":"stock_seq_l = []\nfor i in range(8):\n    stock_seq = toker.texts_to_sequences(stoke_tokenized_l[i].clean_text)\n    stock_seq_l.append(stock_seq)","50f7bbc5":"test_seq_l = []\nfor i in range(8):\n    stock_seq = toker.texts_to_sequences(test_tokenized_l[i].clean_text)\n    test_seq_l.append(stock_seq)","09b54490":"print(stoke_tokenized_l[0][\"clean_text\"].head(1))\nprint(stock_seq_l[0][:1])\nprint(\"index of 'top'=\"+str(wd_idx['top']))\nprint(\"index of 'story'=\"+str(wd_idx['story']))","5e9aff69":"print(test_tokenized_l[0][\"clean_text\"].head(1))\nprint(test_seq_l[0][:1])\nprint(\"index of 'top'=\"+str(wd_idx['word']))\nprint(\"index of 'story'=\"+str(wd_idx['tell']))","267220b4":"x_matrix = pad_sequences(seq, maxlen=MAX_SEQ_LENGTH)\nprint(x_matrix[0])","0019009c":"stock_matrix_l=[]\nfor i in range(8):\n    matrix = pad_sequences(stock_seq_l[i], maxlen=MAX_SEQ_LENGTH)\n    stock_matrix_l.append(matrix)\nprint(stock_matrix_l[0][0])","cdbec328":"test_matrix_l=[]\nfor i in range(8):\n    matrix = pad_sequences(test_seq_l[i], maxlen=MAX_SEQ_LENGTH)\n    test_matrix_l.append(matrix)\nprint(test_matrix_l[0][0])","38eddcef":"y_array = np.asarray(df_tokenized[\"prediction\"])\nprint(y_array[:1])\ny_matrix = to_categorical(y_array)\nprint(y_matrix[:1])","809cdb57":"idxs = np.arange(x_matrix.shape[0])\nprint(idxs)\nnp.random.shuffle(idxs)\nprint(idxs)","a70c8342":"x_rand = x_matrix[idxs]\ny_rand = y_matrix[idxs]\nsplit_idx = int(x_rand.shape[0] * 0.8)\nprint(split_idx)\n\nx_train= x_rand[:split_idx]\ny_train= y_rand[:split_idx]\nx_valid= x_rand[split_idx:]\ny_valid= y_rand[split_idx:]\n\nprint(\"Size of train data=\"+str(x_train.shape[0]))\nprint(\"Size of valid data=\"+str(x_valid.shape[0]))\nprint(\"Shape of x train feature matrix=\"+str(x_train.shape))\nprint(\"Shape of x valid feature matrix=\"+str(x_valid.shape))","4f4fa8d9":"embed_grid = np.zeros((len(wd_idx)+1, EMBED_DIM))\nfor ele, i in wd_idx.items():\n    if ele in word_to_vec_model.wv.vocab:\n        embed_grid[i] = word_to_vec_model.wv[ele]\n        if(i<4):\n            print(ele)\n            print(embed_grid[i])","de1f4dc3":"model = Sequential()\nmodel.add(Embedding(len(embed_grid),EMBED_DIM,weights=[embed_grid], input_length=MAX_SEQ_LENGTH))                        \nmodel.add(Flatten())\nmodel.add(Dense(units=2, activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","22be85ef":"model.fit(x_train, y_train, epochs=4)\n\ntrain_loss, train_accuracy = model.evaluate(x_train, y_train)\nprint(\"* Accurancy of training data =\"+str(train_accuracy)+\", it is reasonable\")","b54b1f6f":"valid_loss, valid_accuracy = model.evaluate(x_valid, y_valid)\nprint(\"* Accurancy of validate data =\"+str(valid_accuracy)+\", it is reasonable\")","100304c0":"y_validate_l=[]\nfor i in range(8):\n    y_validate = model.predict(stock_matrix_l[i])\n    y_validate_l.append(y_validate)","4fa95b59":"y_test_l=[]\nfor i in range(8):\n    y_test = model.predict(test_matrix_l[i])\n    y_test_l.append(y_test)","d464d23a":"x_look = (stoke_tokenized_l[0])\ny_look = y_validate_l[0]\n\nfor i in range(100,105,1):\n    clean_txt = x_look.loc[i, \"clean_text\"]\n    score = y_look[i]\n    print(clean_txt)\n    print(score)","c50388e6":"x_look = (test_tokenized_l[0])\ny_look = y_test_l[0]\n\nfor i in range(0,5,1):\n    clean_txt = x_look.loc[i, \"clean_text\"]\n    score = y_look[i]\n    print(clean_txt)\n    print(score)","4b334a65":"original_size_l = []\nfor i in range(8):\n    original_size_l.append(stoke_tokenized_l[i].shape[0])\nprint(\"(Training stock tweet data) no of tweets of each stock=\"+str(original_size_l))","463967cc":"original_test_size_l = []\nfor i in range(8):\n    original_test_size_l.append(test_tokenized_l[i].shape[0])\nprint(\"(Testing stock tweet data) no of tweets of each stock=\"+str(original_test_size_l))","f1183099":"y_score_l = []\nfor i in range(8):\n    y_validate = y_validate_l[i]\n    score = []\n    for j in range(len(y_validate)):\n        if(y_validate[j][1]>0.5):\n            score.append(1)\n        else:\n            score.append(0)\n    y_score_l.append(score)\nprint(\"(Training stock tweet data) score result of some example:\"+ str(y_score_l[0][:5]))","3bc67fb0":"y_test_score_l = []\nfor i in range(8):\n    y_test = y_test_l[i]\n    score = []\n    for j in range(len(y_test)):\n        if(y_test[j][1]>0.5):\n            score.append(1)\n        else:\n            score.append(0)\n    y_test_score_l.append(score)\nprint(\"(Testing stock tweet data) score result of some example:\"+ str(y_test_score_l[0][:5]))","5ec07d40":"def groupByDate(x_df, y_l):\n    x_df[\"score\"]=y_l\n    date_dict ={}\n    for idx,row in x_df.iterrows():\n        date = row[\"Date\"]\n        score = row[\"score\"]\n        sentiment_l = date_dict.get(date, [0,0,0,0])\n        if score==1:\n            sentiment_l[0] += 1\n            date_dict[date] = sentiment_l\n        else:\n            sentiment_l[1] += 1\n            date_dict[date] = sentiment_l\n    for key in date_dict:\n        senti_l = date_dict[key]\n        senti_l[2]=senti_l[0]+senti_l[1]\n        senti_l[3]=senti_l[0]\/senti_l[2]\n        date_dict[key]= senti_l\n    df_out = pd.DataFrame.from_dict(date_dict, orient='index', columns=['no_of_positive_tweet', 'no_of_negative_tweet', 'no_of_tweet', 'score'])\n    df_out = df_out.sort_index()\n    return df_out\n\ndef output_csv(df_out, name, i):\n    name = \"sentiment_output_\"+name +\"_\"+ str(i+1) + \".csv\"\n    df_out.to_csv(name)","e6842667":"metrics_size_l=[]\nfor i in range(8):\n    df_out = groupByDate(stoke_tokenized_l[i], y_score_l[i])\n    output_csv(df_out,\"train\", i)\n    metrics_size_l.append(df_out.shape[0])\n    if(i<=2):\n        print(\"(Training stock tweet data) Result of stock=\"+str(i+1))\n        print(df_out.head())\nprint(\"(Training stock tweet data) size of metrics of each stock=\"+str(metrics_size_l))","187211ab":"test_metrics_size_l=[]\nfor i in range(8):\n    df_out = groupByDate(test_tokenized_l[i], y_test_score_l[i])\n    output_csv(df_out, \"test\", i)\n    test_metrics_size_l.append(df_out.shape[0])\n    if(i<=2):\n        print(\"(Testing stock tweet data) Result of stock=\"+str(i+1))\n        print(df_out.head())\nprint(\"(Testing stock tweet data) size of metrics of each stock=\"+str(test_metrics_size_l))","017d0b77":"* a grid containing the feature vector of every tokens is generated","d9263a5c":"### 3.2.3 Spliting into training and validation set","2af962ee":"* some stock can has maximum of 547 tweet in that day","babd5105":"* Word Embedding is used to extract the meaning of a word.\n* Each word is represented as a vecotr of real number.\n* The detail process can be skip gram model or continuous bag of words.","bf6f8c65":"Only positive and negative text are used","a5107f00":"Tokenized result is shown above. Words like \"didn't\" still retain to keep the best meaning representation of the text.","c7bce231":"Reference: https:\/\/keras-cn.readthedocs.io\/en\/latest\/legacy\/blog\/word_embedding\/","cdf552dc":"parameter:\n* sg=0: continuous of bag of words(CBOW) is chosed to extract the meaning, use the context to predict the word\n* min_count=1: the word with word count smaller than 1 is skipped, that means no words skipped\n* window=5: maximum of 5 words before and maximmum of 5 words after to predict the word\n* size=100: dimensonality of vector, each word is mapped to a vector of length of 100\n* workers=4: 4 worker threads are used in parallell to train","f58bdc68":"## 2.2 Visulization in Word Cloud","e3cece35":"* Tokenization: tokenizer filter to retain words such as \"book\", \"can't\n* Removal of stop words: Some stopwords with negative meaning shall be retained. This is a little amendment to the stop word set.\n* lemmatization: return context of word, e.g.\"better to good\", \"rocks to rock\"","b57d5f0d":"* example of stock one (train data)","8e428fd3":"* sequence: each word in a tweet are mapped with the corresponding index","5b7c5a2b":"# 2. Preprocess Data","a11a8601":"* import stock train y data: stock price","2b51f067":"\nInput traing data.","523b4475":"* mapping \"0\" to 0 means negative\n* mapping \"2\" to 99\n* mapping \"4\" to 1 means positive","2f9c9ab1":"## 3.1 Word To Vector","af565697":"* 0 means negative\n* 1 means positive","ec29f500":"## 2.1. Preprocessing - remove stopword, tokenization, lemmatization","5e989cc8":"* **text**: raw text\n* **prediction**: sentiment of text (neutral, positive, negative)","1018485c":"* in first 2 diagram, they are representing postive word cloud & negative word cloud\n* for other 8 diagrams, they are 8 stock word cloud\n* you can see obviously, stock 1 represent \"apple\", stock 2 represent \"amazon\", stock 5 is \"fb\"","739981fe":"## 3.2 Embedding each tweet\/document\/sentence","abd24e73":"* total_examples: total number of sentences we used to train\n* epochs=4: number of iterations to process overall data is 4 times","2c37ed1e":"* above are examples to validate by eye balls to see\n* e.g. \"apple build growth record 18b profit url aapl\" with \"[0.12691371 0.87673974]\", it is reasonable as the it is positive comment","bac63173":"### 3.2.2 One-hot encoding to prediction(positive\/negative) ","fa0d0a96":"# 1. Import Tweet Data","7e88d504":"# 3. Modelling","4023674f":"* each tweet has its own sequence with length equal max length of all sequence","be900da2":"* example of stock one (tets data)","d70d7421":"* random shuffle index to prepare for validation","0a44f2b2":"The vector of \"great\" is simlar to vectors of \"wonderful\", \"fantastic\", \"fabulous\". These show that the embedded meaning can be extracted in quite good performance.","d0667892":"### 3.2.1 Create sequence of each tweet","79d77988":"* positive sample with \"prediciton\"=1 is mapped into [0,1], means \"is not negative\" and \"is positive\"\n* negative sample with \"prediciton\"=1 is mapped into [1,0], means \"is negative\" and \"is not positive\"\n","07c8670a":"* a small embedding space of 10 dimensions are chosed","5f1136ed":"* all clean text of all document are tokenized and each tokens are arranged with an index","aa5efe4f":"* below are validation to successful mapping of tokens into correct index"}}