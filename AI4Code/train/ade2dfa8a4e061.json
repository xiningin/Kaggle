{"cell_type":{"2c8bd0f5":"code","e1e5c640":"code","018ca839":"code","718301a2":"code","a07b83b6":"code","7124255f":"code","bf9fbf49":"markdown","10de8b9b":"markdown","b1178119":"markdown","af4585b6":"markdown","511011fa":"markdown","eb4a70d3":"markdown","fcf981d1":"markdown","17d08888":"markdown","1fafaf7c":"markdown","99794f50":"markdown","7c221dbd":"markdown","339c2071":"markdown"},"source":{"2c8bd0f5":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom lightgbm import LGBMRegressor\nfrom boruta import BorutaPy\nimport warnings; warnings.filterwarnings('ignore')","e1e5c640":"# load data from csv\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\n# fill NaN of categorical columns with \"NULL\"\ncat_cols = train.loc[:,train.dtypes == 'object'].columns\ntrain[cat_cols] = train[cat_cols].fillna('NULL')\n# label encoding\nfor col in cat_cols:\n    le = LabelEncoder()\n    le.fit(train[col])\n    train[col] = le.transform(train[col])\n# fill NaN of numeric columns with the mean\ntrain['LotFrontage'] = train['LotFrontage'].fillna(train['LotFrontage'].mean())\ntrain['MasVnrArea'] = train['MasVnrArea'].fillna(train['MasVnrArea'].mean())\ntrain['GarageYrBlt'] = train['GarageYrBlt'].fillna(train['GarageYrBlt'].mean())\ntrain.head()","018ca839":"# split data into X_train and y_train\nX_train = train.drop(['SalePrice'], axis=1)\ny_train = train['SalePrice']","718301a2":"lgb = LGBMRegressor(num_boost_round=100)\nfeat_selector = BorutaPy(lgb, n_estimators='auto', verbose=0)\nfeat_selector.fit(X_train.values, y_train.values)","a07b83b6":"from sklearn.utils import check_random_state\n\nclass BorutaPyForLGB(BorutaPy):\n    def __init__(self, estimator, n_estimators=1000, perc=100, alpha=0.05,\n                 two_step=True, max_iter=100, random_state=None, verbose=0):\n        super().__init__(estimator, n_estimators, perc, alpha,\n                         two_step, max_iter, random_state, verbose)\n        self._is_lightgbm = 'lightgbm' in str(type(self.estimator))\n        \n    def _fit(self, X, y):\n        # check input params\n        self._check_params(X, y)\n\n        if not isinstance(X, np.ndarray):\n            X = self._validate_pandas_input(X) \n        if not isinstance(y, np.ndarray):\n            y = self._validate_pandas_input(y)\n\n        self.random_state = check_random_state(self.random_state)\n        # setup variables for Boruta\n        n_sample, n_feat = X.shape\n        _iter = 1\n        # holds the decision about each feature:\n        # 0  - default state = tentative in original code\n        # 1  - accepted in original code\n        # -1 - rejected in original code\n        dec_reg = np.zeros(n_feat, dtype=np.int)\n        # counts how many times a given feature was more important than\n        # the best of the shadow features\n        hit_reg = np.zeros(n_feat, dtype=np.int)\n        # these record the history of the iterations\n        imp_history = np.zeros(n_feat, dtype=np.float)\n        sha_max_history = []\n\n        # set n_estimators\n        if self.n_estimators != 'auto':\n            self.estimator.set_params(n_estimators=self.n_estimators)\n\n        # main feature selection loop\n        while np.any(dec_reg == 0) and _iter < self.max_iter:\n            # find optimal number of trees and depth\n            if self.n_estimators == 'auto':\n                # number of features that aren't rejected\n                not_rejected = np.where(dec_reg >= 0)[0].shape[0]\n                n_tree = self._get_tree_num(not_rejected)\n                self.estimator.set_params(n_estimators=n_tree)\n\n            # make sure we start with a new tree in each iteration\n            if self._is_lightgbm:\n                self.estimator.set_params(random_state=self.random_state.randint(0, 10000))\n            else:\n                self.estimator.set_params(random_state=self.random_state)\n\n            # add shadow attributes, shuffle them and train estimator, get imps\n            cur_imp = self._add_shadows_get_imps(X, y, dec_reg)\n\n            # get the threshold of shadow importances we will use for rejection\n            imp_sha_max = np.percentile(cur_imp[1], self.perc)\n\n            # record importance history\n            sha_max_history.append(imp_sha_max)\n            imp_history = np.vstack((imp_history, cur_imp[0]))\n\n            # register which feature is more imp than the max of shadows\n            hit_reg = self._assign_hits(hit_reg, cur_imp, imp_sha_max)\n\n            # based on hit_reg we check if a feature is doing better than\n            # expected by chance\n            dec_reg = self._do_tests(dec_reg, hit_reg, _iter)\n\n            # print out confirmed features\n            if self.verbose > 0 and _iter < self.max_iter:\n                self._print_results(dec_reg, _iter, 0)\n            if _iter < self.max_iter:\n                _iter += 1\n\n        # we automatically apply R package's rough fix for tentative ones\n        confirmed = np.where(dec_reg == 1)[0]\n        tentative = np.where(dec_reg == 0)[0]\n        # ignore the first row of zeros\n        tentative_median = np.median(imp_history[1:, tentative], axis=0)\n        # which tentative to keep\n        tentative_confirmed = np.where(tentative_median\n                                       > np.median(sha_max_history))[0]\n        tentative = tentative[tentative_confirmed]\n\n        # basic result variables\n        self.n_features_ = confirmed.shape[0]\n        self.support_ = np.zeros(n_feat, dtype=np.bool)\n        self.support_[confirmed] = 1\n        self.support_weak_ = np.zeros(n_feat, dtype=np.bool)\n        self.support_weak_[tentative] = 1\n\n        # ranking, confirmed variables are rank 1\n        self.ranking_ = np.ones(n_feat, dtype=np.int)\n        # tentative variables are rank 2\n        self.ranking_[tentative] = 2\n        # selected = confirmed and tentative\n        selected = np.hstack((confirmed, tentative))\n        # all rejected features are sorted by importance history\n        not_selected = np.setdiff1d(np.arange(n_feat), selected)\n        # large importance values should rank higher = lower ranks -> *(-1)\n        imp_history_rejected = imp_history[1:, not_selected] * -1\n\n        # update rank for not_selected features\n        if not_selected.shape[0] > 0:\n                # calculate ranks in each iteration, then median of ranks across feats\n                iter_ranks = self._nanrankdata(imp_history_rejected, axis=1)\n                rank_medians = np.nanmedian(iter_ranks, axis=0)\n                ranks = self._nanrankdata(rank_medians, axis=0)\n\n                # set smallest rank to 3 if there are tentative feats\n                if tentative.shape[0] > 0:\n                    ranks = ranks - np.min(ranks) + 3\n                else:\n                    # and 2 otherwise\n                    ranks = ranks - np.min(ranks) + 2\n                self.ranking_[not_selected] = ranks\n        else:\n            # all are selected, thus we set feature supports to True\n            self.support_ = np.ones(n_feat, dtype=np.bool)\n\n        self.importance_history_ = imp_history\n\n        # notify user\n        if self.verbose > 0:\n            self._print_results(dec_reg, _iter, 1)\n        return self","7124255f":"# Run BorutaPyForLGB with LGBMRegressor\nlgb = LGBMRegressor(num_boost_round=100)\nfeat_selector = BorutaPyForLGB(lgb, n_estimators='auto', verbose=0, random_state=1)\nfeat_selector.fit(X_train.values, y_train.values)\n# Check the selected features\nX_train.columns[feat_selector.support_]","bf9fbf49":"## 3.1 Redefine BorutaPy class<a id=\"3.1\">\nOverride the _fit method so that random_state returns an integer object when LightGBM is used.","10de8b9b":"As shown above, the selected features are obtained.","b1178119":"* [1. Preparations](#1)\n    * [1.1 Import libraries](#1.1)\n    * [1.2 Load and shape dataset for LightGBM](#1.2)\n* [2. Run BorutaPy with LightGBM](#2)\n* [3. How to solve this problem](#3)\n    * [3.1 Redefine BorutaPy class](#3.1)\n    * [3.2 Run the redefined BorutaPy](#3.2)","af4585b6":"## 1.1 Import libraries<a id=\"1.1\">","511011fa":"<a id=\"2\">\n<h1 style='background:slateblue; border:.; color:white'><center>2. Run BorutaPy with LightGBM<\/center><\/h1>","eb4a70d3":"<a id=\"1\">\n<h1 style='background:slateblue; border:.; color:white'><center>1. Preparations<\/center><\/h1>","fcf981d1":"BorutaPy gets an error when using LightGBM as an estimator.  \nThe reason is that BorutaPy uses np.random.RandomState() to pass the seed to the estimator, but LightGBM needs to receive it as an integer object.","17d08888":"# <u>How to make work BorutaPy properly with LightGBM<\/u>","1fafaf7c":"## 1.2 Load and shape dataset for LightGBM<a id=\"1.2\">","99794f50":"<a id=\"3\">\n<h1 style='background:slateblue; border:.; color:white'><center>3. How to solve this problem<\/center><\/h1>","7c221dbd":"When running Boruta with a LightGBM, it throws an error. This notebook describes a solution to this problem.","339c2071":"## 3.2 Run the redefined BorutaPy<a id=\"3.2\">\nRun BorutaPyForLGB with LightGBM"}}