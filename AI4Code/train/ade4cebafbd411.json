{"cell_type":{"869bad9c":"code","ce79511a":"code","aa5b24bf":"code","5685bd90":"code","b712963f":"code","e91d9570":"code","f0dc3e11":"code","d04ce065":"code","527402e0":"code","a2a05217":"code","29c5ecc1":"code","c6df589a":"code","9609a675":"code","f80817ba":"code","76f87dad":"code","9ca78301":"code","2e369932":"code","801b73f5":"code","107628d2":"code","e8756768":"code","d03fbd98":"code","f7635692":"code","a2dd7e0e":"code","681a86ec":"code","78bd647a":"code","915dd7d5":"markdown","605d9069":"markdown","1e849b6e":"markdown","a2c21297":"markdown","c47f8fc8":"markdown","d93aee3a":"markdown","db2026b5":"markdown","c66be7e5":"markdown","9becb642":"markdown","40dae693":"markdown","51ba6c84":"markdown","b11a4507":"markdown","04651647":"markdown","d590a93d":"markdown","c24ac5ce":"markdown","9d36b19c":"markdown","1418112d":"markdown","fef667e4":"markdown","19f3f0de":"markdown","783e9d65":"markdown","11e3ee7c":"markdown","552cd6f2":"markdown","ac69bfa7":"markdown","8b2d25e4":"markdown","d33e7b25":"markdown","c3bc384a":"markdown","8e267c7f":"markdown"},"source":{"869bad9c":"# Install Numerai's API\n!pip install numerapi\n# Get the latest version of Weights and Biases\n!pip install wandb --upgrade","ce79511a":"# Obfuscated WANDB API Key\nfrom kaggle_secrets import UserSecretsClient\nWANDB_KEY = UserSecretsClient().get_secret(\"WANDB_API_KEY\")","aa5b24bf":"import os\nimport numpy as np\nimport random as rn\nimport pandas as pd\nimport seaborn as sns\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nfrom scipy.stats import spearmanr\nfrom sklearn.metrics import mean_absolute_error\n\n# Initialize Numerai's API\nimport numerapi\nNAPI = numerapi.NumerAPI(verbosity=\"info\")\n\n# Weights and Biases\nimport wandb\nfrom wandb.lightgbm import wandb_callback\nwandb.login(key=WANDB_KEY)\n\n# Data directory\nDIR = \"\/kaggle\/working\"\n\n# Set seed for reproducability\nseed = 1234\nrn.seed(seed)\nnp.random.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\n\n# Surpress Pandas warnings\npd.set_option('chained_assignment', None)","5685bd90":"def download_current_data(directory: str):\n    \"\"\"\n    Downloads the data for the current round\n    :param directory: The path to the directory where the data needs to be saved\n    \"\"\"\n    current_round = NAPI.get_current_round()\n    if os.path.isdir(f'{directory}\/numerai_dataset_{current_round}\/'):\n        print(f\"You already have the newest data! Current round is: {current_round}\")\n    else:\n        print(f\"Downloading new data for round: {current_round}!\")\n        NAPI.download_current_dataset(dest_path=directory, unzip=True)\n\ndef load_data(directory: str, reduce_memory: bool=True) -> tuple:\n    \"\"\"\n    Get data for current round\n    :param directory: The path to the directory where the data needs to be saved\n    :return: A tuple containing the datasets\n    \"\"\"\n    print('Loading the data')\n    full_path = f'{directory}\/numerai_dataset_{NAPI.get_current_round()}\/'\n    train_path = full_path + 'numerai_training_data.csv'\n    test_path = full_path + 'numerai_tournament_data.csv'\n    train = pd.read_csv(train_path)\n    test = pd.read_csv(test_path)\n    \n    # Reduce all features to 32-bit floats\n    if reduce_memory:\n        num_features = [f for f in train.columns if f.startswith(\"feature\")]\n        train[num_features] = train[num_features].astype(np.float32)\n        test[num_features] = test[num_features].astype(np.float32)\n        \n    val = test[test['data_type'] == 'validation']\n    return train, val, test","b712963f":"# Download, unzip and load data\ndownload_current_data(DIR)\ntrain, val, test = load_data(DIR, reduce_memory=True)","e91d9570":"print(\"Training data:\")\ndisplay(train.head(2))\nprint(\"Test data:\")\ndisplay(test.head(2))","f0dc3e11":"print(\"Training set info:\")\ntrain.info()","d04ce065":"print(\"Test set info:\")\ntest.info()","527402e0":"# Extract era numbers\ntrain[\"erano\"] = train.era.str.slice(3).astype(int)\nplt.figure(figsize=[14, 6])\ntrain.groupby(train['erano'])[\"target\"].size().plot(title=\"Era sizes\", figsize=(14, 8));","a2a05217":"feats = [f for f in train.columns if \"feature\" in f]\nplt.figure(figsize=(15, 5))\nsns.distplot(pd.DataFrame(train[feats].std()), bins=100)\nsns.distplot(pd.DataFrame(val[feats].std()), bins=100)\nsns.distplot(pd.DataFrame(test[feats].std()), bins=100)\nplt.legend([\"Train\", \"Val\", \"Test\"], fontsize=20)\nplt.title(\"Standard deviations over all features in the data\", weight='bold', fontsize=20);","29c5ecc1":"def sharpe_ratio(corrs: pd.Series) -> np.float32:\n    \"\"\"\n    Calculate the Sharpe ratio for Numerai by using grouped per-era data\n    \n    :param corrs: A Pandas Series containing the Spearman correlations for each era\n    :return: A float denoting the Sharpe ratio of your predictions.\n    \"\"\"\n    return corrs.mean() \/ corrs.std()\n\n\ndef evaluate(df: pd.DataFrame) -> tuple:\n    \"\"\"\n    Evaluate and display relevant metrics for Numerai \n    \n    :param df: A Pandas DataFrame containing the columns \"era\", \"target\" and \"prediction\"\n    :return: A tuple of float containing the metrics\n    \"\"\"\n    def _score(sub_df: pd.DataFrame) -> np.float32:\n        \"\"\"Calculates Spearman correlation\"\"\"\n        return spearmanr(sub_df[\"target\"], sub_df[\"prediction\"])[0]\n    \n    # Calculate metrics\n    corrs = df.groupby(\"era\").apply(_score)\n    payout_raw = (corrs \/ 0.2).clip(-1, 1)\n    spearman = round(corrs.mean(), 4)\n    payout = round(payout_raw.mean(), 4)\n    numerai_sharpe = round(sharpe_ratio(corrs), 4)\n    mae = mean_absolute_error(df[\"target\"], df[\"prediction\"]).round(4)\n\n    # Display metrics\n    print(f\"Spearman Correlation: {spearman}\")\n    print(f\"Average Payout: {payout}\")\n    print(f\"Sharpe Ratio: {numerai_sharpe}\")\n    print(f\"Mean Absolute Error (MAE): {mae}\")\n    return spearman, payout, numerai_sharpe, mae","c6df589a":"def get_group_stats(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Create features by calculating statistical moments for each group.\n    \n    :param df: Pandas DataFrame containing all features\n    \"\"\"\n    for group in [\"intelligence\", \"wisdom\", \"charisma\", \"dexterity\", \"strength\", \"constitution\"]:\n        cols = [col for col in df.columns if group in col]\n        df[f\"feature_{group}_mean\"] = df[cols].mean(axis=1)\n        df[f\"feature_{group}_std\"] = df[cols].std(axis=1)\n        df[f\"feature_{group}_skew\"] = df[cols].skew(axis=1)\n    return df","9609a675":"# Add group statistics features\ntrain = get_group_stats(train)\nval = get_group_stats(val)\ntest = get_group_stats(test)","f80817ba":"# Calculate correlations with target\nfull_corr = train.corr()\ncorr_with_target = full_corr[\"target\"].T.apply(abs).sort_values(ascending=False)\n\n# Select features with highest correlation to the target variable\nfeatures = corr_with_target[:150]\nfeatures.drop(\"target\", inplace=True)","76f87dad":"print(\"Top 10 Features according to correlation with target:\")\nfeatures[:10]","9ca78301":"# Create list of most correlated features\nfeature_list = features.index.tolist()","2e369932":"# Configuration for hyperparameter sweep\nsweep_config = {\n   'method': 'grid',\n   'metric': {\n          'name': 'mse',\n          'goal': 'minimize'   \n        },\n   'parameters': {\n       \"num_leaves\": {'values': [30, 40, 50]}, \n       \"max_depth\": {'values': [4, 5, 6, 7]}, \n       \"learning_rate\": {'values': [0.1, 0.05, 0.01]},\n       \"bagging_freq\": {'values': [7]}, \n       \"bagging_fraction\": {'values': [0.6, 0.7, 0.8]}, \n       \"feature_fraction\": {'values': [0.85, 0.75, 0.65]},\n   }\n}\nsweep_id = wandb.sweep(sweep_config, project=\"numerai_tutorial\")","801b73f5":"# Prepare data for LightGBM\ndtrain = lgb.Dataset(train[feature_list], label=train[\"target\"])\ndvalid = lgb.Dataset(val[feature_list], label=val[\"target\"])\nwatchlist = [dtrain, dvalid]\n\ndef _train():\n    # Configure and train model\n    wandb.init(name=\"LightGBM_sweep\")\n    lgbm_config = {\"num_leaves\": wandb.config.num_leaves, \"max_depth\": wandb.config.max_depth, \"learning_rate\": wandb.config.learning_rate,\n                   \"bagging_freq\": wandb.config.bagging_freq, \"bagging_fraction\": wandb.config.bagging_fraction, \"feature_fraction\": wandb.config.feature_fraction,\n                   \"metric\": 'mse', \"random_state\": seed}\n    lgbm_model = lgb.train(lgbm_config, train_set=dtrain, num_boost_round=750, valid_sets=watchlist, \n                           callbacks=[wandb_callback()], verbose_eval=100, early_stopping_rounds=50)\n    \n    # Create predictions for evaluation\n    val_preds = lgbm_model.predict(val[feature_list], num_iteration=lgbm_model.best_iteration)\n    val.loc[:, \"prediction\"] = val_preds\n    # W&B log metrics\n    spearman, payout, numerai_sharpe, mae = evaluate(val)\n    wandb.log({\"Spearman\": spearman, \"Payout\": payout, \"Numerai Sharpe Ratio\": numerai_sharpe, \"Mean Absolute Error\": mae})\n    \n# Run hyperparameter sweep (grid search)\nwandb.agent(sweep_id, function=_train)","107628d2":"# Train model with best configuration\nwandb.init(project=\"numerai_tutorial\", name=\"LightGBM\")\nbest_config = {\"num_leaves\": 50, \"max_depth\": 6, \"learning_rate\": 0.1,\n               \"bagging_freq\": 7, \"bagging_fraction\": 0.6, \"feature_fraction\": 0.75,\n               \"metric\": 'mse', \"random_state\": seed}\nlgbm_model = lgb.train(best_config, train_set=dtrain, num_boost_round=750, valid_sets=watchlist, \n                       callbacks=[wandb_callback()], verbose_eval=100, early_stopping_rounds=50)\n    \n# Create final predictions from best model\ntrain.loc[:, \"prediction\"] = lgbm_model.predict(train[feature_list], num_iteration=lgbm_model.best_iteration)\nval.loc[:, \"prediction\"] = lgbm_model.predict(val[feature_list], num_iteration=lgbm_model.best_iteration)","e8756768":"# Evaluate Model\nprint(\"--- Final Training Scores ---\")\nspearman, payout, numerai_sharpe, mae = evaluate(train)\nprint(\"\\n--- Final Validation Scores ---\")\nspearman, payout, numerai_sharpe, mae = evaluate(val)","d03fbd98":"# Calculate feature exposure\nall_features = [col for col in train.columns if 'feature' in col]\nfeature_spearman_val = [spearmanr(val[\"prediction\"], val[f])[0] for f in all_features]\nfeature_exposure_val = np.std(feature_spearman_val).round(4)","f7635692":"print(f\"Feature exposure on validation set: {feature_exposure_val}\")","a2dd7e0e":"# Set API Keys for submitting to Numerai\nPUBLIC_ID = \"YOUR PUBLIC ID\"\nSECRET_KEY = \"YOUR SECRET KEY\"\n\n# Initialize API with API Keys\nnapi = numerapi.NumerAPI(public_id=PUBLIC_ID, \n                          secret_key=SECRET_KEY, \n                          verbosity=\"info\")\n# Upload predictions for current round\ntest.loc[:, \"prediction\"] = lgbm_model.predict(test[feature_list], num_iteration=lgbm_model.best_iteration)\ntest[['id', \"prediction\"]].to_csv(\"submission.csv\", index=False)","681a86ec":"# Upload predictions to Numerai\n# napi.upload_predictions(\"submission.csv\", tournament=napi.get_current_round())","78bd647a":"print(\"Submission File:\")\ntest[['id', \"prediction\"]].head(2)","915dd7d5":"The Numerai data has 310 obfuscated numerical features that can hold values of 0.0, 0.25, 0.5, 0.75, 1.00. The features are divided into 6 groups (\"intelligence\", \"wisdom\", \"charisma\", \"dexterity\", \"strength\" and \"constitution\"). The meaning of the groups is unclear, but we can use the fact that features are within the same group.","605d9069":"This notebook accompanies the [Weights and Biases Gallery Report](https:\/\/app.wandb.ai\/gallery) on getting started with [Numerai](https:\/\/numer.ai). We will go through the whole process from loading the data to submitting your predictions to Numerai. [Weights and Biases](https:\/\/www.wandb.com\/) will be used for experiment tracking and hyperparameter optimization.","1e849b6e":"The features have a remarkably low correlation to the target variable. Even the most correlated features only have around 1.5% correlation with the target. Engineering useful features out of feature and era groupings is key for creating good Numerai models.\n\nAlso, the importance of features may change over time. By selecting a limited number of features we risk having a high \"feature exposure\". Feature exposure can be quantified as the standard deviation of all your predictions' correlations  with each feature. You can mitigate this risk by using dimensionality reduction techniques like Principal Component Analysis (PCA) to integrate almost all features into your model. In this starter example we take the 150 features that are most correlated to the target variable.","a2c21297":"## Modeling (using Weights and Biases)","c47f8fc8":"# How to get started with Numerai\n\n## *The hardest data science tournament on the planet?*","d93aee3a":"In this experiment we will monitor the Spearman correlation (main metric), the Sharpe ratio, payout and Mean Absolute Error (MAE).","db2026b5":"![](https:\/\/miro.medium.com\/max\/4000\/1*g5PtFpII33P5EeHxFZN9YA.png)\n\n![](https:\/\/camo.githubusercontent.com\/55d9a214447683aae34c1c84b29fc401201d751b\/68747470733a2f2f692e696d6775722e636f6d2f52557469567a482e706e67)","c66be7e5":"The features have a remarkably low correlation to the target variable. Even the most correlated features only have around 1.5% correlation with the target. Engineering useful features out of feature + era groups is key for creating good Numerai models.\n\nAdditionally, the importance of features may change over time and by selecting a limited number of features we risk having a high \"feature exposure\". Feature exposure can be quantified as the standard deviation of all your predictions' correlations  with each feature. You can mitigate this risk by using dimensionality reduction techniques like Principal Component Analysis (PCA) to integrate almost all features into your model.\n\nOne example of creating features out of the groups is to calculate statistical moments (mean, standard deviation, skewness) of every group. ","9becb642":"## Preparation","40dae693":"Weights and Biases requires you to add your WandB API key for logging in automatically. Because this is a secret key we will use [Kaggle User Secrets](https:\/\/www.kaggle.com\/product-feedback\/114053) to obfuscate the API key.","51ba6c84":"## Exploratory Data Analysis (EDA)","b11a4507":"That's all! Note that there is still a lot to be improved and that a good model requires more rigorous evaluation. However, I hope this introduction got you excited about starting with Numerai!","04651647":"Add your API keys and uncomment the line of code below to automatically upload your predictions to Numerai.","d590a93d":"## Feature Selection","c24ac5ce":"## Data Processing","9d36b19c":"Now the grid search is finished we select the hyperparameters that lead to the highest Sharpe ratio.","1418112d":"## Submission","fef667e4":"After that we define a function (_train) using wandb.config attributes so Weights and Biases can perform the grid search. We then log all the results and start the agent.","19f3f0de":"When we group by the eras it can be seen that the era sizes change over time. This can be taken into account when creating features using the eras.","783e9d65":"**If you like this Kaggle notebook, feel free to give an upvote and leave a comment! I will try to implement your suggestions in this kernel!**","11e3ee7c":"You can use this code to upload your predictions directly to Numerai. You will need a public and private API key that you can create from your Numerai account settings.","552cd6f2":"To get a first good model for Numerai we will train a [LightGBM](https:\/\/lightgbm.readthedocs.io\/en\/latest) model and use Weights and Biases to do a hyperparameter sweep. In this example it will be a grid search over some of the most important hyperparameters for LightGBM. First, we define the configuration of the sweep.","ac69bfa7":"## Feature Engineering","8b2d25e4":"## Metrics","d33e7b25":"## Evaluation","c3bc384a":"Without much feature engineering it is already possible to get a reasonable score on Numerai. Sharpe ratio is one of the best indications of performance on Numerai because it takes into account the variability across eras.","8e267c7f":"Most of the features have similar standard deviations, but some have very low variability. Consider standardizing the features or removing these low variability features when experimenting with for example neural networks."}}