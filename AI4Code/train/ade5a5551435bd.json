{"cell_type":{"709b82e0":"code","229d7aa9":"code","c5e29c2c":"code","17f84379":"code","87eff3d1":"code","ff88ae40":"code","f6cedf57":"code","45d53d79":"code","e872ed31":"code","bccea869":"code","d7ecbcb4":"code","ac3965eb":"code","30a6c246":"code","bf444a09":"code","47474695":"code","72784fe5":"code","c96c0f02":"code","11c4ca8d":"code","55366bc9":"code","8a594a94":"code","afd8db43":"code","71eb5c51":"code","cce6b8f0":"code","aef6c016":"code","ba016767":"code","d73ac2c0":"code","cb48837d":"code","860315e7":"code","a0ab33c0":"code","08963d98":"code","ad49c637":"code","2a8657f7":"code","c063d414":"code","c655d60e":"code","05e67efb":"code","54b05e58":"code","43738608":"code","de05cb84":"code","bab52d00":"code","b5491cf1":"code","2e738951":"code","a9adee0b":"code","7e52c992":"code","d4b1de2b":"code","ea245421":"code","98fa8d83":"markdown","2af64737":"markdown","a15a51d9":"markdown","08374438":"markdown","ad7fddba":"markdown","6b8998ec":"markdown","444540e4":"markdown","fac02b79":"markdown","28ef0376":"markdown"},"source":{"709b82e0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","229d7aa9":"# additional libraries \n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# for better color representation \nplt.style.use('ggplot')\nfrom inspect import signature\nfrom sklearn import tree\nfrom scipy import stats \nfrom scipy import signal\nfrom sklearn import preprocessing\nfrom scipy.fft import fft, fftfreq\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom sklearn import svm\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV, validation_curve\nfrom sklearn import metrics \nfrom pandas.plotting import lag_plot, autocorrelation_plot\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom imblearn.over_sampling import RandomOverSampler\nimport seaborn as sns\nnp.random.seed(42) # stable execution","c5e29c2c":"# read data \ndf = pd.read_csv(\"\/kaggle\/input\/epileptic-seizure-recognition\/Epileptic Seizure Recognition.csv\")\ndf.name = \"seizure_data\"\nprint(\"DataFrame name: {data_name}\".format(data_name = df.name))\nprint(f\"DataFrame size: {df.shape}\")\nprint(f\"# of datapoints: {df.shape[0]*df.shape[1]}\")\ndf.head(3)","17f84379":"# drop subject identifier\ndf.drop([\"Unnamed\"], inplace = True, axis = 1)\nassert(df.columns[0] != \"Unnamed\")","87eff3d1":"# missing values \ndf.isna().any().sum()","ff88ae40":"# summary statistics of all features\ndf.drop([\"y\"], axis = 1).describe()","f6cedf57":"# correlation among all features \ndf.drop([\"y\"], axis = 1).corr()","45d53d79":"# get a measure of correlation firt two consecutive epochs \n# positive, very high correlation\nprint(pd.DataFrame(data = {\"X1\": df[\"X1\"], \"X2\": df[\"X2\"]}).corr())\nprint()\n\n# get a measure of correlation between first and last epoch\n#\u00a0no correlation \nprint(pd.DataFrame(data = {\"X1\": df[\"X1\"], \"X178\": df[\"X178\"]}).corr())\n\n# plot first and last signal \nfig, (ax1, ax2) = plt.subplots(1, 2, sharex = True, sharey = True, figsize = (15, 5))\nax1.plot(df[\"X1\"], color = \"c\")\nax1.set_title(\"Signal Curve of X1\")\nax2.plot(df[\"X178\"], color = \"g\")\nax2.set_title(\"Signal Curve of X178\")\nplt.show()","e872ed31":"df.iloc[0].plot()","bccea869":"# calculate discrete linear convolution for moving average\n\n# plot first epoch with convolution \nfig, (ax1, ax2) = plt.subplots(1, 2, sharex = True, sharey = True, figsize = (15, 5))\ninterval = 10000 \nwindow = np.ones(interval) \/ interval\nmoving_avg = np.convolve(df[\"X1\"], window, \"same\")\nax1.plot(moving_avg, c = \"c\")\nax1.set_title(\"Smoothed Signal Curve of X1\")\n\n# plot last epoch with alternative method \nax2 = df[\"X178\"].rolling(window = 10000).mean()\nax2.plot(c = \"g\")\nplt.title(\"Smoothed Signal Curve of X178\")\nplt.show()","d7ecbcb4":"# define features, labels \nfeatures = df.drop([\"y\"], axis = 1)\nlabels = df[\"y\"]","ac3965eb":"#\u00a0invert the time domain \nfeatures = features.T","30a6c246":"#\u00a0switch from time domain to frequency domain with fft \n\n# define sampling rate \n\"\"\"\nnum_samples = 4094 \nduration = 23\nsampling_rate = num_samples \/ duration\n\"\"\"\nsampling_rate = df.shape[1]\n\n# remove DC component\nfeatures = features - np.mean(features)\n\n# fast fourier transformation \n# fourier_space = [features.iloc[:,i].ravel() for i in range(features.shape[1])]\n# fourier_space = [k for j in fourier_space for k in j]\nfourier_range = fft(features.T.values.ravel())\nfourier_domain = fftfreq(features.T.values.ravel().size, 1\/sampling_rate)\n\n# use abs to deal with complex numbers \nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (15,5))\nax1.plot(fourier_domain, np.abs(fourier_range), c = \"c\")\nax1.plot(fourier_domain[:fourier_domain.size \/\/ 2], np.abs(fourier_range[:fourier_range.size \/\/ 2]), c = \"g\")\nax1.set_xlabel(\"frequency [Hz]\", fontweight = \"bold\")\nax1.set_ylabel(\"Amplitude [m]\", fontweight = \"bold\")\nax2.plot(fourier_domain[:fourier_domain.size \/\/ 2], np.abs(fourier_range[:fourier_range.size \/\/ 2]), c = \"g\")\nax2.set_xlabel(\"frequency [Hz]\", fontweight = \"bold\")\nax2.set_ylabel(\"Amplitude [m]\", fontweight = \"bold\")\nplt.show()","bf444a09":"#\u00a0Welch's Method \n# make a plot with log scaling on the y-axis \nfreqs, power_spectrum = signal.welch(features.values.ravel(), sampling_rate, \"flattop\", 1024, scaling = \"spectrum\")\n\n# filter frequency and power spectrum \nfreqs, power_spectrum = freqs[(freqs > 1) & (freqs < 89)], power_spectrum[(freqs > 1) & (freqs < 89)]\n\nplt.figure(figsize = (15, 5))\nplt.semilogy(freqs, np.sqrt(power_spectrum), c = \"c\")\nplt.xlabel(\"frequency [Hz]\", fontweight = \"bold\")\nplt.ylabel(\"Linear spectrum [V RMS]\", fontweight = \"bold\")\nplt.show()\n\n# RMS estimate \nprint(f\"RMS estimate: {round(np.sqrt(power_spectrum.max()), 2)}\")","47474695":"# there is a spike in the beta waves, this could be important \n# majority of spikes in power spectrum is composed of beta waves \nwelch_df = pd.DataFrame({\"frequency\": freqs, \"power\": power_spectrum})\nwelch_df.head()","72784fe5":"# extract individual wave amplitudes \nfor wave in range(5):\n    delta = welch_df[welch_df.frequency <= 3].power\n    theta = welch_df[(welch_df.frequency >= 3.5) & (welch_df.frequency <= 7.5)].power\n    alpha = welch_df[(welch_df.frequency >= 7.5) & (welch_df.frequency <= 13)].power\n    beta = welch_df[welch_df.frequency >= 13].power\n\n\n# of datapoints in each wave \nprint(f\"Delta size: {delta.size}\")\nprint(f\"Theta size: {theta.size}\")\nprint(f\"Alpha size: {alpha.size}\")\nprint(f\"Beta size: {beta.size}\")\nprint(f\"# of significant frequencies: {delta.size + theta.size + alpha.size + beta.size}\")\n\n# plot all waves\nfig, axs = plt.subplots(2, 2, figsize = (15,5))\naxs[0, 0].plot(delta, c = \"c\")\naxs[0, 1].plot(theta, c = \"g\")\naxs[1, 0].plot(alpha, c = \"g\")\naxs[1, 1].plot(beta, c = \"c\")\n\nfor ax in axs.flat:\n    ax.set(xlabel=\"#\", ylabel=\"[m]\")\n\nfor ax in axs.flat:\n    ax.label_outer()","c96c0f02":"# --- add feature ---\n\nwaves = [delta, theta, alpha, beta]\nfeatures = features.append(pd.concat(waves))","11c4ca8d":"# run cell after running the above cell only \n\n# recall individual wave counts \nprint(f\"Delta size: {delta.size}\")\nprint(f\"Theta size: {theta.size}\")\nprint(f\"Alpha size: {alpha.size}\")\nprint(f\"Beta size: {beta.size}\")\nprint(f\"# of significant frequencies: {delta.size + theta.size + alpha.size + beta.size}\")\n\n# extend each wave, construct amplitude feature vector\nfeat_vector = []\ntotal = delta.size + theta.size + alpha.size + beta.size\nfactor = features.iloc[-1].size \/\/ total\n\nfor wave in waves:\n    wave = wave.to_list() * factor\n    feat_vector.append(wave)\n    \n# add remainder as a combination of scarce wave types \nfeat_vector = [k for y in feat_vector for k in y]\n\navg = (delta.mean() + theta.mean() + alpha.mean() + beta.mean()) \/ 4 \nremainder = features.iloc[-1].size - len(feat_vector)\nprint(f\"# of beta waves for extension: {remainder}\")\n\nfor k in range(remainder):\n    feat_vector.append(avg)\n\n# check computation\nassert(len(feat_vector) == features.iloc[-1].size)\n\n# --- add feature ---\nfeatures = features.iloc[:-1]\nfeatures = features.append(pd.DataFrame(feat_vector, columns = [\"amplitudes\"]).T)","55366bc9":"# run cell after running the above cell only \n\n# add fourier domain, i.e frequencies to the feature space\nfor wave in range(5):\n    delta = welch_df[welch_df.frequency <= 3].frequency\n    theta = welch_df[(welch_df.frequency >= 3.5) & (welch_df.frequency <= 7.5)].frequency\n    alpha = welch_df[(welch_df.frequency >= 7.5) & (welch_df.frequency <= 13)].frequency\n    beta = welch_df[welch_df.frequency >= 13].frequency\n\n# --- add feature ---\nwaves = [delta, theta, alpha, beta]\nfeatures = features.append(pd.DataFrame(pd.concat(waves), columns = [\"frequency\"]).T)","8a594a94":"# run cell after running the above cell only \n\n# replace amplitude with frequency\n# perform same arithmetic operations \nfeat_vector = []\ntotal = delta.size + theta.size + alpha.size + beta.size\nfactor = features.iloc[-1].size \/\/ total\n\nfor wave in waves:\n    wave = wave.to_list() * factor\n    feat_vector.append(wave)\n    \n# add remainder as a combination of scarce wave types \nfeat_vector = [k for y in feat_vector for k in y]\n\navg = (delta.mean() + theta.mean() + alpha.mean() + beta.mean()) \/ 4\nremainder = features.iloc[-1].size - len(feat_vector)\nprint(f\"# of beta waves for extension: {remainder}\")\n\nfor k in range(remainder):\n    feat_vector.append(avg)\n\n# check computation\nassert(len(feat_vector) == features.iloc[-1].size)\n\n# --- add feature --- \nfeatures = features.iloc[:-1]\nfeatures = features.append(pd.DataFrame(feat_vector, columns = [\"frequency\"]).T)","afd8db43":"# sample eeg-values across features \nsample = np.random.randint(low = 0, high = 178, size = 3)\nfig, ax = plt.subplots(3, figsize = (15, 5))\nfor x in range(3):\n    if x % 2 == 0:\n        color = \"c\"\n    else:\n        color = \"g\"\n    ax[x].plot(features.iloc[sample[x]], c = color)\n    ax[x].set_xticks([])","71eb5c51":"# make labels binary\nbinary = lambda label: 0 if label != 1 else label\nlabels = labels.apply(binary)\n\n# features, labels as numpy arrays \nfeatures = features.T.values \nlabels = labels.values","cce6b8f0":"# below is principal component analysis\n# it is not necessary unless you have trouble with high dimensionality \n\n# regular train, test split here with stratification \nX_train, X_test, y_train, y_test =  train_test_split(features, labels, test_size = 0.4, \n                                                     random_state = 42, stratify = labels)\nassert(X_train.shape[0] == y_train.shape[0])\nassert(X_test.shape[0] == y_test.shape[0])\n\nprint(f\"Size of training sample: {X_train.shape}\")\nprint(f\"Size of test sample: {X_test.shape}\")","aef6c016":"# principal component analysis \n\n# scale \nfeatures = preprocessing.StandardScaler().fit_transform(features)\n\npca = PCA(n_components = 50)\nreduced = pca.fit_transform(features)\npca_data = pd.DataFrame(reduced, columns = [\"component_\" + str(idx) for idx in range(1,51)])\n\n# display first five principal components \nprint(pca_data[[\"component_\" + str(idx) for idx in range(1,6)]])\n\n# total information stored \nprint(f\"\\nCumulative variance explained: {np.sum(pca.explained_variance_ratio_)}\")","ba016767":"# linear support vector machine \n# train, test split \nX_train, X_test, y_train, y_test = train_test_split(pca_data.values, labels, \n                                                    test_size = 0.33,random_state = 42)","d73ac2c0":"# ratio of labels in data \nprint(\"Class distribution before over-sampling\")\nprint(f\"Ratio of label = 1: {np.sum(labels) \/ labels.size}\")\nprint(f\"Ratio of label = 0: {1 - np.sum(labels) \/ labels.size}\")\n\n# data is heavily unbalanced","cb48837d":"\"\"\"\n# oversample training data\n# apply to training set only \nX_train, y_train = RandomOverSampler(sampling_strategy = \"minority\").fit_resample(X_train, y_train)\n\n# observe new ratio \nprint(\"\\nClass distribution after over-sampling\")\nprint(f\"Ratio of label = 1: {np.sum(y_train) \/ y_train.size}\")\nprint(f\"Ratio of label = 0: {1 - np.sum(y_train) \/ y_train.size}\")\n\"\"\"","860315e7":"# classification \nclassifier = svm.SVC(kernel = \"linear\", C = 1, random_state = 42)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)","a0ab33c0":"# accuracy \nprint(f\"Accuracy of linear SVM: {metrics.accuracy_score(y_test, y_pred)}\\n\")\n\n# classification report \nprint(metrics.classification_report(y_test, y_pred))","08963d98":"# random forest classifier \nclassifier = RandomForestClassifier(random_state = 42)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)","ad49c637":"# accuracy \nprint(f\"Accuracy of random forest classifier: {metrics.accuracy_score(y_test, y_pred)}\\n\")\n\n# classification report \nprint(metrics.classification_report(y_test, y_pred))","2a8657f7":"# Create range of values for parameter\nparam_range = np.arange(1, 250, 50)\n\n# Calculate accuracy on training and test set using range of parameter values\ntrain_scores, test_scores = validation_curve(RandomForestClassifier(), \n                                             features, \n                                             labels, \n                                             param_name=\"n_estimators\", \n                                             param_range=param_range,\n                                             cv=3, \n                                             scoring=\"accuracy\", \n                                             n_jobs=-1)\n\n\n# Calculate mean and standard deviation for training set scores\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\n\n# Calculate mean and standard deviation for test set scores\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\n# Plot mean accuracy scores for training and test sets\nplt.plot(param_range, train_mean, label=\"Training score\", color=\"black\")\nplt.plot(param_range, test_mean, label=\"Cross-validation score\", color=\"dimgrey\")\n\n# Plot accurancy bands for training and test sets\nplt.fill_between(param_range, train_mean - train_std, train_mean + train_std, color=\"gray\")\nplt.fill_between(param_range, test_mean - test_std, test_mean + test_std, color=\"gainsboro\")\n\n# Create plot\nplt.title(\"Validation Curve With Random Forest\")\nplt.xlabel(\"Number Of Trees\")\nplt.ylabel(\"Accuracy Score\")\nplt.tight_layout()\nplt.legend(loc=\"best\")\nplt.show()","c063d414":"# k-fold cross validation \naccs = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10).mean()*100\nprint(f\"10-fold accuracy: {round(accs, 2)}%\")","c655d60e":"# no parameter optimization necessary ","05e67efb":"features = df.drop([\"y\"], axis = 1).T","54b05e58":"# reindex \nidx = {}\nfor i in range(1,179):\n    idx[\"X\" + str(i)] = float(i)\nfeatures.rename(index = idx, inplace = True)\nfeatures.head(5)","43738608":"# average the data before further analysis\navg_df = pd.DataFrame(features.mean(axis = 1), columns = [\"t\"])\navg_df.head(5)","de05cb84":"# plot for lag = 1\nlag_plot(avg_df)","bab52d00":"# check correlation between (t+1) and (t-1)\ncorr_df = pd.concat([avg_df.shift(1), avg_df], axis=1)\ncorr_df.columns = [\"t-1\", \"t+1\"]\ncorr_df.corr()","b5491cf1":"# for further analysis \ncorr_df.fillna(method = \"bfill\", inplace = True)\ncorr_df.head(1)","2e738951":"# autocorrelation \nautocorrelation_plot(avg_df)\nplot_acf(avg_df, lags = 31)\nplt.show()","a9adee0b":"# baseline persistence model \n\n# create lagged data \nX = corr_df.values \n\n# perform the split \ntrain, test = X[:len(X)-7], X[len(X)-7:]\nX_train, y_train = train[:,0], train[:,1]\nX_test, y_test = test[:,0], test[:,1]\n\n\n# every item in X_test is a forecast \npred = [x for x in X_test]\n\n# scoring\nmse = metrics.mean_squared_error(y_test, pred)\nprint(f\"Mean squared error for persistence model: {round(mse,2)}\")\n\n# plot persistence model\nfig, ax = plt.subplots(figsize = (15,5))\nax.plot(y_test)\nax.plot(pred, c = \"b\")\nax.legend([\"true\", \"prediction\"])\nax.set_title(\"Baseline model for autoregression\", fontweight = \"bold\")\nplt.show()","7e52c992":"# make arrays one dimensional \ntrain = train.ravel()\ntest = test.ravel()","d4b1de2b":"# autoregression model\n\nmodel = AutoReg(train, lags = 31)\nmodel_fit = model.fit()\nprint(f\"Coefficients are {model_fit.params}\")\n\n# make predictions \nprint()\npred = model_fit.predict(len(train), (len(train) + len(test) - 1), dynamic = False)\nfor p in range(len(pred)):\n    print(\"predicted = {predicted}\\tactual = {actual}\".format(predicted = pred[p], actual = test[p]))\n    \n# root mean squared error \nrmse = np.sqrt(metrics.mean_squared_error(test, pred))\nprint(f\"\\nRoot mean squared error: {rmse}\")\n\n# plot true, pred values with cutoff\nfig, ax = plt.subplots(figsize = (15,5))\nplt.axvline(x = 6, c = \"g\")\nax.plot(test)\nax.plot(pred, c = \"b\")\nax.legend([\"cutoff\",\"true\", \"prediction\"])\nax.set_title(\"Autoregression model\", fontweight = \"bold\")\nplt.show()","ea245421":"# learn the coefficients \nwindow = 31\nmodel = AutoReg(train, lags = window)\nmodel_fit = model.fit()\ncoef = model_fit.params\n\n# get prior 31 observations \n# make predictions \nhistory = train[len(train) - window:]\nhistory = [history[i] for i in range(len(history))]\nyhat = coef[0]\npred = []\nfor p in range(len(test)):\n    hist_len = len(history)\n    lag = [history[i] for i in range(hist_len - window, hist_len)]\n    yhat = coef[0]\n    for d in range(window):\n        yhat += coef[d+1] * lag[window-d-1]\n    obs = test[p]\n    pred.append(yhat)\n    history.append(obs)\n    print(\"predicted = {predicted}\\tactual = {actual}\".format(predicted = yhat, actual = obs))\n\n# root mean squared error \nrmse = np.sqrt(metrics.mean_squared_error(test, pred))\nprint(f\"\\nRoot mean squared error: {rmse}\")\n\n# plot true, pred values with cutoff\nfig, ax = plt.subplots(figsize = (15,5))\nplt.axvline(x = 6, c = \"g\")\nplt.axvline(x = 8, c = \"g\")\nax.plot(test)\nax.plot(pred, c = \"b\")\nax.legend([\"cutoff\",\"true\", \"prediction\"])\nax.set_title(\"Autoregression model (learned coefficients)\", fontweight = \"bold\")\nplt.show()","98fa8d83":"### Binary Classification","2af64737":"### Data preprocessing","a15a51d9":"### TODO: Multi-class Classification","08374438":"### Significant Frequency Bands \n\n1. **Delta:** has a frequency of 3 Hz or below.\n1. **Theta:** has a frequency of 3.5 to 7.5 Hz and is classified as \"slow\" activity.\n1. **Alpha:** has a frequency between 7.5 and 13 Hz.\n1. **Beta:**  has a frequency bigger than 13 Hz.\n\n> [See source](https:\/\/www.medicine.mcgill.ca\/physio\/vlab\/biomed_signals\/eeg_n.htm)\n","ad7fddba":"### Oversampling\n* Do not run for complexity reasons ","6b8998ec":"### Major improvement in RMSE","444540e4":"### Can we predict seizures beforehand ? ","fac02b79":"### Multivariate linear regression, robustness, polynomial, lasso, etc. ","28ef0376":"### Discrepancy starts at lag = 6"}}