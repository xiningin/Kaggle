{"cell_type":{"5244a381":"code","f70a5c55":"code","b82ff9d3":"code","8cb280b2":"code","b92d7009":"code","f065636b":"code","2a0a7933":"code","cf74c530":"code","6f05226b":"code","f4494a6c":"code","ce61e022":"code","10f1b577":"code","e7b90ed0":"code","81d55570":"code","efba5949":"code","1d4eea08":"code","eaa70c54":"code","63f14dae":"code","2ad94cb7":"code","064a1438":"code","29f1ece3":"code","babd27f5":"code","1d73264d":"code","e48f486a":"code","77a2d427":"code","db5144fe":"code","dc2ed6ab":"code","d344eafd":"code","3a664eb1":"code","fac0b62e":"code","8b4fa74f":"code","1484b765":"code","69ef50d0":"code","32feea77":"code","5aabe090":"code","c2299713":"code","f1fb28c3":"code","70a94bc9":"code","7aa0d76e":"code","534397f5":"code","5bb74905":"code","20831cbf":"code","7c65ff5e":"code","4e4e92a7":"code","0e64de2a":"code","d77b085f":"code","595fd280":"code","f889d9a8":"code","c8cd0ff2":"code","85e19fd8":"code","f49e0a6a":"code","a03dea93":"code","5c8ebb83":"code","59a059a0":"markdown","54a008c4":"markdown","e7397c71":"markdown","fb981bde":"markdown","4ff2f0b8":"markdown","befce33a":"markdown","034974c4":"markdown","adc700b0":"markdown","a5c8f814":"markdown","ee833327":"markdown","1a84b1eb":"markdown","56aeec4f":"markdown","659660d7":"markdown","3a79c6e3":"markdown","7d4bca17":"markdown","c6e80bea":"markdown","68484255":"markdown","19f193c4":"markdown","a16a2825":"markdown","9ff27d6a":"markdown","b95d8193":"markdown","afe35a78":"markdown"},"source":{"5244a381":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\n\n### Setting Style of matplotlib\nfrom matplotlib import style\nstyle.use('seaborn')","f70a5c55":"X = np.array([10, 17, 22, 25, 30, 32, 39, 45, 77, 80, 91, 85, 97, 80, 65]).reshape(-1, 1) # AGE\ny = np.array([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0])","b82ff9d3":"plt.scatter(X, y)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Likes Pizza-1\\nDoes not likes Pizza-0\")\nplt.show()","8cb280b2":"clf = KNeighborsClassifier()\nclf.fit(X, y)","b92d7009":"clf.predict([[16]])","f065636b":"clf.predict_proba([[12]])","2a0a7933":"from sklearn.datasets import load_breast_cancer","cf74c530":"df = load_breast_cancer()","6f05226b":"df.target_names","f4494a6c":"df.feature_names","ce61e022":"X = df.data\ny = df.target","10f1b577":"X.shape","e7b90ed0":"y.shape","81d55570":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nX = StandardScaler().fit_transform(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)","efba5949":"print(X_train.shape, X_test.shape)","1d4eea08":"print(y_train.shape, y_test.shape)","eaa70c54":"clf2 = KNeighborsClassifier(n_neighbors=21)\nclf2.fit(X_train, y_train)","63f14dae":"clf.classes_","2ad94cb7":"clf2.effective_metric_","064a1438":"y_pred = clf2.predict(X_test)","29f1ece3":"clf2.score(X_test, y_test)","babd27f5":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred)","1d73264d":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(clf2, X_test, y_test)","e48f486a":"from sklearn.linear_model import LogisticRegression","77a2d427":"logit_reg = LogisticRegression()\nlogit_reg.fit(X_train, y_train)","db5144fe":"logit_reg.score(X_test, y_test)","dc2ed6ab":"import numpy as np\n\nclass KNNClassifier:\n\n    def __init__(self, n_neighbours='auto', p=2):\n        self.n_neighbours = n_neighbours\n        self.p = p # order of norm\n    \n    def fit(self, X, y):\n        self.X = X\n        self.y = y\n\n        if self.n_neighbours == 'auto': # if n_neighbours='auto' it automatically finds correct n_neighbours acc. to your data\n            self.n_neighbours = int(np.sqrt(len(self.X)))\n            if self.n_neighbours % 2 != 0:\n                self.n_neighbours += 1\n\n    \n    def predict(self, X):\n        predictions = []\n        for pred_row in X:\n            euclidean_distances = []\n            for X_row in self.X:\n                distance = np.linalg.norm(X_row - pred_row, ord=self.p)\n                euclidean_distances.append(distance)\n            \n            indices = np.argsort(euclidean_distances)[:self.n_neighbours]\n            neighbours = self.y[indices]\n            coefs = [self.X[indices], self.y[indices]]\n            prediction = np.argmax(np.bincount(neighbours))\n            predictions.append(prediction)\n\n        predictions = np.array(predictions)\n        return predictions, neighbours, coefs","d344eafd":"from sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)","3a664eb1":"plt.scatter(X[np.where(y == 0)][:, 0], X[np.where(y == 0)][:, 1], color='red')\nplt.scatter(X[np.where(y == 1)][:, 0], X[np.where(y == 1)][:, 1], color='orange')\nplt.show()","fac0b62e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)","8b4fa74f":"knn = KNNClassifier(n_neighbours=5)\nknn.fit(X, y)","1484b765":"knn.n_neighbours","69ef50d0":"pred, _, _ = knn.predict(X_test)","32feea77":"pred","5aabe090":"y_test","c2299713":"from sklearn.metrics import accuracy_score","f1fb28c3":"accuracy_score(y_test, pred)","70a94bc9":"X_test[10]","7aa0d76e":"plt.scatter(X[np.where(y == 0)][:, 0], X[np.where(y == 0)][:, 1], color='red')\nplt.scatter(X[np.where(y == 1)][:, 0], X[np.where(y == 1)][:, 1], color='orange')\nplt.scatter(X_test[10][0], X_test[10][1], marker='x', s=200)\nplt.show()","534397f5":"pred, neighbours, coefs = knn.predict(np.expand_dims(X_test[10], axis=0))\nprint(\"Final Prediction:\", pred)\nprint(\"Neighbours:\", neighbours)\nprint(\"Coefficients:\\n\", coefs)","5bb74905":"plt.scatter(X[np.where(y == 0)][:, 0], X[np.where(y == 0)][:, 1], color='red', label='Negative class')\nplt.scatter(X[np.where(y == 1)][:, 0], X[np.where(y == 1)][:, 1], color='orange', label='Positive class')\nplt.scatter(coefs[0][:, 0], coefs[0][:, 1], color='black', marker='x', s=15, label='n neighbours')\nplt.scatter(X_test[10][0], X_test[10][1], marker='x', s=200, label='Choosen Point')\nplt.legend()\nplt.show()","20831cbf":"pred, neighbours, coefs = knn.predict(X[-1])\nprint(\"Final Prediction:\", pred)\nprint(\"Neighbours:\", neighbours)\nprint(\"Coefficients:\\n\", coefs)","7c65ff5e":"plt.title('Visualizing n-neighbours')\nplt.scatter(X[np.where(y == 0)][:, 0], X[np.where(y == 0)][:, 1], color='red', label='Negative class')\nplt.scatter(X[np.where(y == 1)][:, 0], X[np.where(y == 1)][:, 1], color='orange', label='Positive class')\nplt.scatter(coefs[0][:, 0], coefs[0][:, 1], color='black', marker='x', s=15, label='n neighbours')\nplt.scatter(X[-1][0], X[-1][1], marker='x', s=200, label='Choosen Point')\nplt.legend()\nplt.show()","4e4e92a7":"x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nprint(x_min, x_max, y_min, y_max)","0e64de2a":"h = 0.1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))","d77b085f":"xx","595fd280":"yy","f889d9a8":"print(xx.shape, yy.shape)","c8cd0ff2":"Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])","85e19fd8":"Z[0]","f49e0a6a":"from matplotlib.colors import ListedColormap\nstyle.use('seaborn')\nplt.figure(figsize=(8, 6))\nplt.contourf(xx, yy, Z[0].reshape(xx.shape), cmap=ListedColormap(['orange', 'cornflowerblue']))\nplt.scatter(X[np.where(y == 0)][:, 0], X[np.where(y == 0)][:, 1], color='white', label='Negative class')\nplt.scatter(X[np.where(y == 1)][:, 0], X[np.where(y == 1)][:, 1], color='black', label='Positive class')\nplt.legend()\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.show()","a03dea93":"clf3 = KNeighborsClassifier(n_neighbors=5)\nclf3.fit(X, y)\npred_sk = clf3.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, pred_sk))\nZ = clf3.predict(np.c_[xx.ravel(), yy.ravel()])","5c8ebb83":"plt.figure(figsize=(8, 6))\nplt.contourf(xx, yy, Z.reshape(xx.shape), cmap=ListedColormap(['orange', 'cornflowerblue']))\nplt.scatter(X[np.where(y == 0)][:, 0], X[np.where(y == 0)][:, 1], color='white', label='Negative class')\nplt.scatter(X[np.where(y == 1)][:, 0], X[np.where(y == 1)][:, 1], color='black', label='Positive class')\nplt.legend()\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.show()","59a059a0":"## With Sklearn","54a008c4":"### Fitting our data and making Predictions","e7397c71":"Our model has predicted only 2 malignant tumour as benign and else all samples correctly!","fb981bde":"### Visualizing our dummy data","4ff2f0b8":"### Checking Accuracy","befce33a":"Let's compare the Predicted values with real values!","034974c4":"Let's tackle this problem with K-Nearest Neighbour!","adc700b0":"For Age 16, model is predicting it likes pizza and according to our imaginary data it is correct that a 16 year old will like pizza!","a5c8f814":"## Let's see what is happening Under the Hood","ee833327":"Now, Let us play with some real world data...","1a84b1eb":"Same Accuracy as ours!","56aeec4f":"### Making KNN Classifier from scratch","659660d7":"Which corresponds to ['malignant', 'benign']<br>\nmalignant - 0<br>\nBenign - 1","3a79c6e3":"### train test split","7d4bca17":"That's pretty nice accuracy!","c6e80bea":"## Testing This Data With Logistic Regression","68484255":"Do Visit my Channel for ML\/DL Videos:- https:\/\/www.youtube.com\/c\/PritishMishra\n\n## sklearn.neighbors.KNeighborsClassifier\n`sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)`\n\n- In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n- In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.\n\n\n![image.png](attachment:image.png)\n\n\nNow, we will make simple Classifier with KNN that someone likes Pizza or not based on their age.<br>\nLikes Pizza - 1<br>\nDoes not like Pizza - 0\n\nNote- This is a fake data\n\nIt is obvious that Old people don't like Pizza (generally!) and young people like to eat Pizza!","19f193c4":"### Making a dummy data","a16a2825":"## Tumour Detection with KNN","9ff27d6a":"KNN is based on the assumption that similar things are close to each other! \nHere, K means the number of neighbours to check for..\nIn sklearn, default k is 5. So, it will check for 5 nearest neighbour and vote for the highest number of classes. In our example, if sklearn finds 3 neighbours who like pizza and 2 neighbour who doesn't so here highest number of class is \"Who like Pizza\" so it is obvious that given person also likes pizza!","b95d8193":"It gives same accuracy!","afe35a78":"### Visualizing n-neighbours of 10th data point in our test set"}}