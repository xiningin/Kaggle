{"cell_type":{"f9da598e":"code","159797f9":"code","d7c8fc97":"code","34d5222a":"code","8588c567":"code","f96bd67f":"code","a08c01fb":"code","1c8bece5":"code","d2746254":"code","d54c0fb0":"code","cf36e662":"code","035221cb":"code","13dee1a9":"code","adc07120":"code","4c286328":"code","b1e081a3":"code","1dda7d29":"code","b3fda107":"code","775562b3":"code","9b569eb9":"code","46b3d07b":"code","acee51bc":"code","73af677c":"code","01103cb8":"code","8e2419b9":"code","36ff5501":"code","6f47c51e":"code","744bae3b":"code","874ab355":"code","756e9c92":"code","11c7fea4":"code","10a1b91c":"code","98995944":"code","04fa4845":"code","f5ec0b12":"code","b5a12571":"code","4396b486":"code","ecc67222":"code","db53cfbb":"markdown","a7ba73ac":"markdown","7439cb8e":"markdown","f5affdc7":"markdown","ad0433d6":"markdown","cfffe8ae":"markdown","c23542e4":"markdown","9ec0cedd":"markdown","5d6d4e96":"markdown","0ce68892":"markdown","c24dbdc7":"markdown","f8d0b6e3":"markdown","b565bc32":"markdown","394bfef7":"markdown","c418c2e1":"markdown","65ed2e75":"markdown"},"source":{"f9da598e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","159797f9":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn import model_selection\nimport scipy.stats as stats\nimport sklearn.linear_model as linear_model\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nimport xgboost\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import RidgeCV,LassoCV,ElasticNetCV\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn import neighbors\nfrom sklearn.svm import SVR\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","d7c8fc97":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","34d5222a":"train.head()","8588c567":"train.info()","f96bd67f":"#statistics details for sale price\ntrain['SalePrice'].describe()","a08c01fb":"plt.figure(figsize=(10,4))\nsns.distplot(train[\"SalePrice\"]);","1c8bece5":"plt.figure(figsize=(10,4))\nsns.boxplot(train[\"SalePrice\"]);","d2746254":"#It is apparent that SalePrice doesn't follow normal distribution\nprint(\"Skewness: \" , train['SalePrice'].skew())\nprint(\"Kurtosis: \" , train['SalePrice'].kurt())","d54c0fb0":"#Sale Price vs LotArea\nsns.scatterplot(x=\"LotArea\", y=\"SalePrice\",data=train);","cf36e662":"#Sale Price vs GrLivArea\nsns.scatterplot(x=\"GrLivArea\", y=\"SalePrice\",data=train);","035221cb":"#Sale Price vs YearBuilt and YearBuilt\nsns.scatterplot(x=\"GrLivArea\", y=\"SalePrice\",hue=\"YearBuilt\",data=train);","13dee1a9":"#Sale Price vs Neighborhood:\nsns.boxplot(x=\"Neighborhood\",y=\"SalePrice\",data=train);","adc07120":"#Sale Price vs OverallQual\nsns.boxplot(x=\"OverallQual\",y=\"SalePrice\",data=train);","4c286328":"#Correlation \nplt.figure(figsize=(11,9))\ncorr = train.corr()\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr,vmax=.8, square=True,cmap=cmap);","b1e081a3":"#Saleprice correlation \nplt.figure(figsize=(8,6))\nk = 10 #number of variables for heatmap\ncols = corr.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values,cmap=cmap)\nplt.show()","1dda7d29":"cols1 = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train[cols1], size = 2.5);\nplt.show();","b3fda107":"corr = train.corr()\ncorr['SalePrice'].sort_values(ascending=False)[:11]","775562b3":"#Sale Price - GrLivArea\nsns.regplot(x=\"GrLivArea\",y=\"SalePrice\",data=train);","9b569eb9":"train.sort_values(by = 'GrLivArea', ascending = False)[:2]\n#Although these two values have a very high area, their prices are very low. We will delete \n#these two values as this may mislead our model.","46b3d07b":"train = train.drop(train[train['Id'] == 1298].index)\ntrain = train.drop(train[train['Id'] == 523].index)","acee51bc":"#Sale Price - GarageArea\nsns.regplot(x=\"GarageArea\",y=\"SalePrice\",data=train);","73af677c":"#Sale Price - TotalBsmtSF\nsns.regplot(x=\"TotalBsmtSF\",y=\"SalePrice\",data=train);","01103cb8":"#Sale Price - 1stFlrSF\nsns.regplot(x=\"1stFlrSF\",y=\"SalePrice\",data=train);","8e2419b9":"#histogram and normal probability plot\nsns.distplot(train[\"SalePrice\"],fit=norm);\nfig=plt.figure()\nres=stats.probplot(train[\"SalePrice\"],plot=plt);","36ff5501":"#applying log transformation\ntrain[\"SalePrice\"] = np.log(train[\"SalePrice\"])\n\n#transformed histogram and normal probability plot\nsns.distplot(train[\"SalePrice\"],fit=norm);\nfig=plt.figure()\nres=stats.probplot(train[\"SalePrice\"],plot=plt)","6f47c51e":"#Checking GrLivArea\n#histogram and normal probability plot\nsns.distplot(train[\"GrLivArea\"],fit=norm);\nfig=plt.figure()\nres=stats.probplot(train[\"GrLivArea\"],plot=plt)","744bae3b":"#applying log transformation\ntrain[\"GrLivArea\"] = np.log(train[\"GrLivArea\"])\n\n#transformed histogram and normal probability plot\nsns.distplot(train[\"GrLivArea\"],fit=norm);\nfig=plt.figure()\nres=stats.probplot(train[\"GrLivArea\"],plot=plt)","874ab355":"#Checking TotalBsmtSF\n#histogram and normal probability plot\nsns.distplot(train[\"TotalBsmtSF\"],fit=norm);\nfig=plt.figure()\nres=stats.probplot(train[\"TotalBsmtSF\"],plot=plt)","756e9c92":"print(\"train shape: \",train.shape)\nprint(\"test shape: \",test.shape)","11c7fea4":"#Concatenating our train and test dataframes\nntrain = train.shape[0]\nntest = test.shape[0]\n\ny_train = train['SalePrice'].values\n\ntrain.drop(['SalePrice'], axis=1, inplace=True)\nall_data = pd.concat([train, test]).reset_index(drop=True)\nprint(\"all_data shape: \",all_data.shape)\n\nnindex, nfeatures = all_data.shape","10a1b91c":"#Missing Values in all_data\nmissing_data_all=all_data.isnull().sum().sort_values(ascending=False)\nmissing_data_all.head(25)","98995944":"all_data = all_data.drop(missing_data[missing_data>1].index,1)\nall_data = all_data.drop(train.loc[train['Electrical'].isnull()].index)\nall_data.isnull().sum().max() #just checking that there's no missing data missing...","04fa4845":"#one-hot encoding \nall_data_one_hot = pd.get_dummies(all_data)","f5ec0b12":"X_train =  all_data_one_hot[:ntest-1]\nprint(\"X_train shape: \",X_train.shape)\nprint(\"y_train shape: \",y_train.shape)\n\n#train-test split\nX_train,X_test,y_train,y_test=train_test_split(X_train,y_train,test_size=0.25,random_state=42)","b5a12571":"def compML(df,y,alg):\n    #modelleme\n    model = alg().fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    RMSE=np.sqrt(mean_squared_error(y_pred,y_test))\n    model_name =alg.__name__\n    print(model_name,\":\",RMSE)","4396b486":"models = [LGBMRegressor,\n         XGBRegressor,\n         GradientBoostingRegressor,\n         RandomForestRegressor,\n         DecisionTreeRegressor,\n         KNeighborsRegressor,\n         SVR]","ecc67222":"for i in models:\n    compML(all_data_one_hot,\"SalePrice\",i)","db53cfbb":"So far we have only analysed five variables,however,since there are many more variables,we must examine in more detail. For this we will first use heatmap.\n","a7ba73ac":"### Log Transformation ","7439cb8e":"### Data Visulization\n","f5affdc7":"### Loading Data","ad0433d6":"### Data Exploration","cfffe8ae":"Houses without basements appear to be zero, which is a big problem because log transformation doesn't work with zeros. We'll have to do a conversion for that.\n\nTo apply a log transformation here, we'll create a variable that can get the effect of having or not having basement (binary variable). Then, we'll do a log transformation to all the non-zero observations, ignoring those with value zero. This way we can transform data, without losing the effect of having or not basement.\n","c23542e4":"When there are too many missing values in data, it would be better to delete them directly than to manipulate them. \"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\" we can delete LotFrontage variables from our data.\n\nMoreover, when we look at the data, we can see that the \"bsmt\" and \"garage\" variables have the same number of missing values, and when we examine the data details, we can say that they all refer to the same variable. Since we have the most efficient for these two variables, it is okay to delete this data as well.\n\nRegarding 'MasVnrArea' and 'MasVnrType', we can consider that these variables are not essential. Furthermore, they have a strong correlation with 'YearBuilt' and 'OverallQual' which are already considered. Thus, we will not lose information if we delete 'MasVnrArea' and 'MasVnrType'.\n\nFinally, we have one missing observation in 'Electrical'. Since it is just one observation, we'll delete this observation and keep the variable.\n\nIn summary, to handle missing data, we'll delete all the variables with missing data, except the variable 'Electrical'. In 'Electrical' we'll just delete the observation with missing data.","9ec0cedd":"The main reason why we use log transformation is to reduce skewness in our data.For possible statistical analysis that require the data to be normalized\n\n* Histogram - Kurtosis and skewness.\n* Normal probability plot - Data distribution should closely follow the diagonal that represents the normal distribution.","5d6d4e96":"Some variables may mean the same thing, *TotalBsmtSF: Total square feet of basement area* and *1stFlrSF: First Floor square feet* can be the same thing.","0ce68892":"We will examine in more detail by making visualization of outliers.\n","c24dbdc7":"### Categorical Values ","f8d0b6e3":"'SalePrice' is not normal. It shows 'peakedness', positive skewness and does not follow the diagonal line. In case of positive skewness, log transformations usually works well.","b565bc32":"We will not make any changes on this variables. ","394bfef7":"### Modelling ","c418c2e1":"### Missing Values","65ed2e75":"### Outliers "}}