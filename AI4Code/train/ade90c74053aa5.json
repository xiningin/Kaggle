{"cell_type":{"d883fb38":"code","a1be0f0f":"code","c5a34d33":"code","3cd1e0aa":"code","57d2a38c":"code","2a2bf201":"code","0157ebe1":"code","4da6a1c1":"code","6a95ffbd":"code","f2bf5848":"code","9c7c2b1c":"code","75925bb7":"code","b12d2730":"code","a9a81c28":"code","bb06f5d2":"code","90c2cdca":"code","4d44a72f":"code","2a82654a":"code","87fff631":"code","6ecd0573":"code","fc454147":"code","a59254d6":"code","676ee5a4":"code","99ab1341":"code","62f335bb":"code","1aee07d1":"code","86c3f882":"code","6eaf4206":"code","e555e9a7":"code","453d41e3":"code","ad0948f4":"code","0e3f2f0a":"markdown","8b6a5796":"markdown","9520c286":"markdown","44264566":"markdown","74e6cb65":"markdown","b93698dc":"markdown","44c20e5f":"markdown","fcd1480f":"markdown","f4c29d96":"markdown","9c42e41b":"markdown","6b088d9b":"markdown","d5e6a1e4":"markdown","53281617":"markdown","ca0d4451":"markdown","f08ab1f6":"markdown","74ae940f":"markdown","bc39ebb5":"markdown","6d28fade":"markdown","428d6f91":"markdown","f3098ad5":"markdown","4f44050e":"markdown","82a84c4e":"markdown","a6675c6c":"markdown","98c46d76":"markdown","ecf33001":"markdown","5f6f10e2":"markdown","08d976d2":"markdown","8a3f242a":"markdown","9aa46662":"markdown","58cbf87f":"markdown","dc23c47b":"markdown","b877e1ba":"markdown","793f7aa8":"markdown","9a728920":"markdown","be8c7978":"markdown","5ef4f136":"markdown","38d00c78":"markdown","a9238d91":"markdown","f00d306c":"markdown","d8e56f1f":"markdown","af7e9b7d":"markdown","40604c6a":"markdown"},"source":{"d883fb38":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a1be0f0f":"import numpy as np\nimport pandas as pd\nimport datatable as dt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\n%matplotlib inline\nwarnings.filterwarnings('ignore')","c5a34d33":"%%time\n\ntrain_filename = \"..\/input\/tabular-playground-series-sep-2021\/train.csv\"\ntest_filename = \"..\/input\/tabular-playground-series-sep-2021\/test.csv\"\n\ntrain_orig = dt.fread(train_filename).to_pandas()\ntest_orig = dt.fread(test_filename).to_pandas()\n\ntrain_orig = train_orig.set_index('id')\ntest_orig = test_orig.set_index('id')","3cd1e0aa":"train_orig.shape","57d2a38c":"train_orig.claim = train_orig.claim.astype('int16')\ntrain_orig.info()\nprint()\ntest_orig.info()","2a2bf201":"pd.set_option('display.max_columns', 125)\ntrain_orig.describe()","0157ebe1":"train_orig.head(10)","4da6a1c1":"train_memory_orig = train_orig.memory_usage().sum() \/ 1024**2\nprint('Memory usage of original training set(in MB): {}'.format(train_memory_orig))\n\ndef reduce_memory(df):\n    for col in df.columns:\n        if str(df[col].dtypes)[:5] == 'float':\n            low = df[col].min()\n            high = df[col].max()\n            if((low > np.finfo(np.float16).min) and (high < np.finfo(np.float16).max)):\n                df[col] = df[col].astype('float16')\n            elif((low > np.finfo(np.float32).min) and (high < np.finfo(np.float).max)):\n                df[col] = df[col].astype('float32')\n    return df\n\nreduce_memory(train_orig)\ntrain_memory_reduced = train_orig.memory_usage().sum() \/ 1024**2\nprint('Memory usage of reduced training set(in MB): {}'.format(train_memory_reduced))","6a95ffbd":"test_memory_orig = test_orig.memory_usage().sum() \/ 1024**2\nprint('Memory usage of original test set(in MB): {}'.format(test_memory_orig))\n\nreduce_memory(test_orig)\ntest_memory_reduced = test_orig.memory_usage().sum() \/ 1024**2\nprint('Memory usage of reduced test set(in MB): {}'.format(test_memory_reduced))","f2bf5848":"#prints the number of duplicated entries(rows)\nn_duplicates_train = train_orig.duplicated().sum()\nprint(\"Number of duplicated entries in trainng set: {}\".format(n_duplicates_train))\nn_duplicates_test = test_orig.duplicated().sum()\nprint('Number of duplicated entries in test set: {}'.format(n_duplicates_test))","9c7c2b1c":"print(train_orig.claim.dtype)\nprint(train_orig.claim[:10])","75925bb7":"claim_dist = train_orig.claim.value_counts()\ndisplay(claim_dist)","b12d2730":"plt.figure(figsize = (10,6))\nclaim_dist.plot.pie(autopct = '%.1f', colors = ['powderblue', 'slateblue'])\nplt.title(\"Claim vlaue distribution pie chart\", pad = 20, fontdict = {'size' : 15, 'color' : 'darkblue', 'weight' : 'bold'})\nplt.show()","a9a81c28":"train_orig['count_missing'] = train_orig.isna().sum(axis = 1)\ntest_orig['count_missing'] = test_orig.isna().sum(axis = 1)\n\nprint(train_orig['count_missing'].value_counts())\nprint(test_orig['count_missing'].value_counts())","bb06f5d2":"train_frac = train_orig.sample(frac = 0.01).reset_index(drop = True)\n#train_frac = train_orig[0:9579]\ntarget = train_frac.claim\n#txt = \"Kernel Density Estimation Plots w.r.t. the target 'claim' for {} training examples\".format(train_frac.shape[0]).center(110)\n#print(txt)\n\nc = 4\n#r_ = int(np.ceil(len(train_frac.columns)\/4))\nr = int(np.ceil(train_frac.shape[1]\/4))\n#print(r, r_)\nfig, ax = plt.subplots(nrows = r, ncols = c, figsize = (25,80))\ni = 1\nfor col in train_frac.columns:\n    plt.subplot(r, c, i)\n    ax = sns.kdeplot(train_frac[col], hue = target, fill = True, multiple = 'stack')\n    plt.xlabel(col, fontsize = 15)\n    i = i + 1\n    \nfig.tight_layout(pad = 2.0)\nfig.subplots_adjust(top = 0.97)\nplt.suptitle(\"Kernel Density Estimation Plots w.r.t. the target 'claim' for {} training examples\".format(train_frac.shape[0]), fontsize = 20)\nplt.show()","90c2cdca":"corrMat = train_frac.corr()\n\nfig, ax = plt.subplots(figsize = (20,20))\ncmap = sns.diverging_palette(230, 20, as_cmap = True)\nmask = np.triu(np.ones_like(corrMat, dtype = bool))\nsns.heatmap(corrMat, square = True, annot = False, linewidths = 1, cmap = cmap, mask = mask)","4d44a72f":"from sklearn.model_selection import train_test_split\n\nX = train_orig.copy()\nY = X.claim\nX.drop('claim', axis = 1, inplace = True)\n\nX_train_orig, X_valid_orig, Y_train_orig, Y_valid_orig = train_test_split(X, Y, test_size = 0.2,\n                                                                         random_state = 42)\nX_test_orig = test_orig.copy()","2a82654a":"missing_val_cols = X_train_orig.isnull().sum().sort_values(ascending = False)\nmissing_val_cols = missing_val_cols[missing_val_cols > 0]\nratio_of_missing = missing_val_cols \/ X_train_orig.shape[0]\nmissing = pd.concat([missing_val_cols,ratio_of_missing], axis = 1, \n                   keys = ['Count','%'])\nmissing","87fff631":"from sklearn.impute import SimpleImputer\n\nmy_imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean', verbose = False)\nmy_imputer.fit(X_train_orig)\nX_train_imputed = my_imputer.transform(X_train_orig)\nX_valid_imputed = my_imputer.transform(X_valid_orig)\nX_test_imputed = my_imputer.transform(X_test_orig)","6ecd0573":"from sklearn.preprocessing import RobustScaler, StandardScaler\n\nrobust_scaler = RobustScaler()\nrobust_scaler.fit(X_train_imputed)\nX_train_robust = robust_scaler.transform(X_train_imputed)\nX_valid_robust = robust_scaler.transform(X_valid_imputed)\nX_test_robust = robust_scaler.transform(X_test_imputed)\n\nstandard_scaler = StandardScaler()\nstandard_scaler.fit(X_train_imputed)\nX_train_scaled = standard_scaler.transform(X_train_imputed)\nX_valid_scaled = standard_scaler.transform(X_valid_imputed)\nX_test_scaled = standard_scaler.transform(X_test_imputed)","fc454147":"X_train_final = pd.DataFrame(X_train_scaled, index = X_train_orig.index,\n                            columns = X_train_orig.columns)\nX_valid_final = pd.DataFrame(X_valid_scaled, index = X_valid_orig.index, \n                            columns = X_valid_orig.columns)\nX_test_final = pd.DataFrame(X_test_scaled, index = X_test_orig.index, \n                           columns = X_test_orig.columns)","a59254d6":"#final training set\nX_train_final.describe()","676ee5a4":"from tensorflow import keras\nfrom tensorflow.keras import layers","99ab1341":"nn = keras.Sequential([\n    layers.BatchNormalization(input_shape = [X_train_final.shape[1]]),\n    layers.Dense(units = 128, activation = 'relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(rate = 0.5),\n    layers.Dense(units = 64, activation = 'relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(rate = 0.4),\n    layers.Dense(units = 32, activation = 'relu'),\n    layers.BatchNormalization(),\n    layers.Dropout(rate = 0.3),\n    layers.Dense(units = 1, activation = 'sigmoid')\n])","62f335bb":"auc = keras.metrics.AUC(name = 'auc')\nnn.compile(optimizer = keras.optimizers.Adam(learning_rate = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate = 0.01, decay_steps = 9000, decay_rate = 0.9)),\n             loss = 'binary_crossentropy',\n             metrics = [auc])","1aee07d1":"early_stopping = keras.callbacks.EarlyStopping(patience = 20,\n                                min_delta = 0.001,\n                                restore_best_weights = False)","86c3f882":"history = nn.fit(X_train_final, Y_train_orig,\n                   validation_data = (X_valid_final,Y_valid_orig),\n                   batch_size = 256,\n                   epochs = 100,\n                   callbacks = [early_stopping])","6eaf4206":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:,['loss','val_loss']].plot(title = 'Loss')\nhistory_df.loc[:,['auc','val_auc']].plot(title = 'Dev')","e555e9a7":"X_test_final.shape","453d41e3":"#prediction\npreds = nn.predict(X_test_final)\n\n#reshape distorted prediction array\npreds = preds.reshape(len(X_test_final),)\n\npreds[:10]","ad0948f4":"output = pd.DataFrame({'id' : X_test_final.index,\n                      'claim' : preds})\noutput.to_csv('submission.csv', index = False)","0e3f2f0a":"That's a lot of plots to look at. However, at a quick glance at all the plots, there doesn't seem to be a pattern in any of the distributions w.r.t. the target variable. We will now analyse these weak relations further using a correlation matrix.","8b6a5796":"\n\nDo checkout my previous notebook on this month's playground series - [EDA & Optuna with GPU](https:\/\/www.kaggle.com\/jaikr18\/tps-sept-detailed-eda-optuna-with-gpu-support), wherein I focused on EDA and trained optimized models for XGBClassifier, CatBoostClassifier, and LGBMClassifier. In this notebook, I will tackle the same problem with a neural network approach.","9520c286":"# **REFERENCES**\n\n- Keras documentation -> [Documentation](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras)","44264566":"<a id = \"dataset-split\"><\/a>\n**2.0 DATASET SPLIT**\n\nBefore proceeding any further, it is recommended to split the dataset into a training set and a hold-out cross-validation set. This is to ensure that the model we build won't be adversely affected by data leakage.\n\n> Any feature whose value would not actually be available in practice at the time you\u2019d want to use the model to make a prediction, is a feature that can introduce leakage to your model","74e6cb65":"<a id = \"dataset-overview\"><\/a>\n**0.1 DATASET OVERVIEW**","b93698dc":"<a id = \"memory-reduction\"><\/a>\n**0.2 MEMORY REDUCTION**","44c20e5f":"On passing through the Scalers, our Data Frame has now been converted to a numpy array. So, for convention, we will convert the array back to a Data Frame.","fcd1480f":"<a id = \"missing-values\"><\/a>\n**2.1 MISSING VALUES**\n\nAs we saw earlier, most of the features have missing values. We will take care of that now.\n\nLuckily, for the given dataset, we have only numerical features and hence, imputation will be lot more simpler. For numerical data, two most suitable imputation techniques that could be used here are *mean imputation* and *median imputation*. I will try both these techniques and compare their performance on the validation set. In the final notebook, you will only see the technique which performed better. ","f4c29d96":"<a id = \"correlation-analysis\"><\/a>\n**1.4 CORRELATION ANALYSIS**\n\nWe noticed earlier that the relation between features and the target variable is most likely weak. To check that further, we'll make use of a correlation matrix. Also, this matrix will help us to check which features are strongly related to one another.","9c42e41b":"Surprisingly, every feature has missing entries. However, the number of missing entries as compared to the entire dataset is quite small.","6b088d9b":"<a id = \"target-column\"><\/a>\n**1.1 TARGET COLUMN**","d5e6a1e4":"<a id = \"eda\"><\/a>\n# **1. EXPLORATORY DATA ANALYSIS**","53281617":"For a large dataset such as this one, one might often face situations where the system runs out of RAM. Thus, it would be wise to cut down on the memory usage.","ca0d4451":"<a id = \"pruning\"><\/a>\n**4.3 Pruning**\n\nEarly-Stopping is when we pre-maturely stop the training of a model when a certain monitored metric has stopped improving. \n\nFor more on EarlyStopping, check out the tensorflow documentation [here](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/EarlyStopping).","f08ab1f6":"<a id = \"duplicate-removal\"><\/a>\n**1.0 DUPLICATE REMOVAL**\n\nFirst off, there is always a possibility that our dataset is having duplicate entries. This is typically a fault of the data acquisition step.","74ae940f":"<a id = \"feature-scaling\"><\/a>\n# **3. FEATURE SCALING**\n\n\n\nMany machine learning algorithms perform better when numerical input variables are scaled to a standard range.\n\nStandardizing is a popular scaling technique that subtracts the mean from values and divides by the standard deviation, transforming the probability distribution for an input variable to a standard Gaussian (zero mean and unit variance). Standardization can become skewed or biased if the input variable contains outlier values.\n\nTo overcome this, the median and interquartile range can be used when standardizing numerical input variables, generally referred to as robust scaling.","bc39ebb5":"<a id = \"data-cleaning\"><\/a>\n# **2. DATA CLEANING**","6d28fade":"All the features in the dataset are of type *float64*, and the ground truth column, i.e. *claim* is of type *int16*.","428d6f91":"There are a total of *957919* training examples, having *118* features ranging from 'f1' to 'f118', and *1* target column, i.e. *claim* which corresponds to - whether the claim was made (1) or not (0).","f3098ad5":"<a id = \"train-model\"><\/a>\n**4.4 Train Model**\n\nWith everything in line, we can now train our neural network. \n\n- We want to use mini-batch gradient descent algorithm, and for that the argument *batch_size* will be set to a certain number(256 in this case).\n- 1 epoch means 1 iteration over the entire training set. Setting *epochs = 100* means we want to iterate over the training set 100 times, with each step of gradient descnet training on 256 training examples because of the *batch_size* setting.","4f44050e":"# THANK YOU FOR READING!","82a84c4e":"We can clearly see that the validation loss and auc fluctuates as the training process continues. This is okay since are using mini-batch gradient descent. The fluctuations arise because different mini-batches can have varying distributions. The main idea is to analyze the general trend of the curve - it should go downwards. ","a6675c6c":"<a id = \"review-performance\"><\/a>\n**4.5 Review Performance**","98c46d76":"**Conclusion:** We can now safely say that none of the features have a strong correlation among one another, or with the target variable. This marks the end of a fruitless correlation analysis. ","ecf33001":">\"Perfectly balanced, as all things should be.\"","5f6f10e2":"As expected, the dataset is far from standard with some features taking exponentially large values while some other taking exponentially small values. Also, most features seem to be having missing values, so we will have to take care of these things at a later point.","08d976d2":"<a id = \"model-structure\"><\/a>\n**4.1 Define Model Structure**","8a3f242a":"<a id = \"modelling\"><\/a>\n# **4. MODEL FITTING AND EVALUATION**\n\nIn the last notebook, linked -> [EDA & Optuna with GPU](https:\/\/www.kaggle.com\/jaikr18\/tps-sept-detailed-eda-optuna-with-gpu-support), we trained three classifier models, namely *XGBClassifier*, *CatBoostClassifier*, and *LGBMClassifier*; we used *Optuna* to tune the hyperparameters in order to optimize the three models. Their performance evaluations are as shown below:\n\n![image.png](attachment:a8a40ba0-e530-460a-801a-aa3bfdb020a7.png)\n\nNow, let us look at how we can train a simple Neural Network using tensorflow on this dataset.","9aa46662":"<a id = \"model-compilation\"><\/a>\n**4.2 Model Compilation**\n\n- Here, we will define the type of optimizer we will use for our gradient descent algorithm. *Adam()* is almost always a good choice as it combines the advantages of both *RMSProp* and *SGD* which implements momentum. \n- For the loss function, binary cross-entropy is a good choice since we're working with probabilities.\n- AUC-ROC score is a good evaluation metric when working with probabilistic outputs.","58cbf87f":"# **PROBLEM STATEMENT**\n\nWe are given the following:\n\n1. A train dataset (.csv) containing the index column (0 to n_train_examples-1), features ('f1' to 'f118') and the ground truth *claim* (0 or 1) respectively.\n2. A test dataset (.csv) containing the index column (0 to n_test_examples-1), features ('f1' to 'f118') respectively.\n\nWe are required to implement a binary-classification algorithm which predicts for each example of the test dataset, whether a customer made a claim upon an insurance policy. A '1' value means a claim was made, and '0' means a claim was not made.","dc23c47b":"<div style=\"background-color:#B4DBE9; color:#636363; padding-top: 10px;\">\n    <h1><center>Table Of Contents<\/center><\/h1>\n<\/div>\n\n* [0 DATASET](#dataset)\n\n    - [0.0 Loading Dataset](#loading-dataset)\n    \n    - [0.1 Dataset Overview](#dataset-overview)\n    \n    - [0.2 Memory Reduction](#memory-reduction)\n    \n* [1 EXPLORATORY DATA ANALYSIS](#eda)\n\n    - [1.0 Duplicate Removal](#duplicate-removal)\n    \n    - [1.1 Target Column](#target-column)\n    \n    - [1.2 Feature Engineering](#feature-engineering)\n    \n    - [1.3 Distribution Analysis](#distribution-analysis)\n    \n    - [1.4 Correlation Analysis](#correlation-analysis)   \n    \n* [2 DATA CLEANING](#data-cleaning)\n\n    - [2.0 Dataset Split](#dataset-split)\n    \n    - [2.1 Missing Values](#missing-values)\n    \n* [3 FEATURE SCALING](#feature-scaling)\n\n* [4 MODEL FITTING & EVALUATION](#modelling)\n    \n    - [4.0 Import Libraries](#import-libraries)\n    \n    - [4.1 Define Model Structure](#model-structure)\n    \n    - [4.2 Model Compilation](#model-compilation)\n    \n    - [4.3 Pruning](#pruning)\n    \n    - [4.4 Train Model](#train-model)\n    \n    - [4.5 Review Performance](#review-performance)\n    \n* [5 SUBMISSION](#submission)","b877e1ba":"<a id = \"loading-dataset\"><\/a>\n**0.0 LOADING DATASET**","793f7aa8":"<a id = \"import-libraries\"><\/a>\n**4.0 Import Libraries**","9a728920":"Seems pretty well balanced. Let's confirm this notion through a pie chart (because \"visual data is always more convincing\").","be8c7978":"There are a few darker cells, which represent relatively strong correlation between the concerning features\/variables. However, even these *relatively* strong correlations have very small correlation coefficient values from a general P.O.V. To elaborate, the slider on the right depicts that the upper bound on positive correlations is approx 0.04 and the lower bound on negative correlations is approx -0.06. These two bounds are too small to declare a strong correlation between the features.\n\n**Ps:** Here, I define a *strong correlation* as one having correlation coefficient value greater than 0.6 (meaning strong positive correlation) or less than -0.6 (meaning strong negative correlation). Of course, these thresholds are subject to the author.","5ef4f136":"<a id = \"distribution-analysis\"><\/a>\n**1.3 DISTRIBUTION ANALYSIS**\n\nLet's see how the features are distributed w.r.t. the target variable.\n\n**NOTE:** Since we have a very large dataset, we will plot these distributions taking a small sample from the dataset. For better estimations, we will take a random sample, preferably of fraction 1\/100 of the original dataset. This will help in faster generation of plots.","38d00c78":"<a id = \"feature-engineering\"><\/a>\n**1.2 FEATURE ENGINEERING**\n\nSince we are going to impute NaN later, we would end up losing the information about missing values for the training examples. For that reason, let's add a column which stores the number of NaN entries for each training example.","a9238d91":"# **IMPORT LIBRARIES**","f00d306c":"This structure was decided upon after training different structures on the train set and analyzing the performances. Most of the times, this back & forth process of defining a structure then evaluating performance is an integral part of training a neural network.\n\n- **Dropout:** Dropout leaves out some nodes (or activation units) from the previous layer. Dropout is random, i.e. differnt nodes will be shut off for different iterations. Doing this introduces some noise into the current layer evaluations and thus, it helps to preven overfitting of the training set.\n- Applying *BatchNormalization()* to every layer helps by harmonizing the different features to a common scale of values. This helps in training speed-up for out learning algorithm.\n- We use the sigmoid activation function at the output layer since we need probabilities.","d8e56f1f":"<a id = \"submission\"><\/a>\n# **5. SUBMISSION**\n\nLet's predict the claim variable for test set and submit our results!","af7e9b7d":"This means that our dataset has only unique entries. Having ensured this, now we can proceed to the actual EDA for our dataset.","40604c6a":"<a id = \"dataset\"><\/a>\n# **0. DATASET**"}}