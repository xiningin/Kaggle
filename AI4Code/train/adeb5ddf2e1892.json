{"cell_type":{"355cf3b6":"code","8a71fe65":"code","99731807":"code","338cd098":"code","5e87b97e":"code","bfd04825":"code","ed08db02":"code","42225c7b":"code","2abaf7de":"code","4a4aa2d3":"code","65ddd090":"code","ad2d931a":"code","61284557":"code","28d88429":"code","3300d7a4":"code","4a259b6d":"code","0f6c5be8":"code","d727d864":"code","6f0f4987":"code","669991cf":"code","ff2bdccc":"code","b7a96042":"code","d0c6becf":"code","9674053c":"code","7e48a8fc":"code","2ec39314":"code","052bb8ef":"code","9bf2ed96":"code","1879cdb2":"code","2b82d83a":"code","39172627":"code","1095abe1":"code","c2450bc4":"code","96239640":"code","9e369cbd":"code","43ae0b4d":"code","dafbeff8":"code","8530e097":"code","2ca5f4a2":"markdown","7f816bb1":"markdown","80fccad5":"markdown","6debb935":"markdown","b92a2d06":"markdown","e775fb38":"markdown","9c3bfead":"markdown","7059e1c4":"markdown","0dd6e6d2":"markdown","d6afb186":"markdown","daf9008e":"markdown","7139d2fd":"markdown","666989ca":"markdown","eebc20e1":"markdown"},"source":{"355cf3b6":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nimport sklearn.metrics\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import permutation_importance\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense, Dropout\nimport tensorflow as tf\nimport math\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error","8a71fe65":"rul_1_gt = pd.read_csv('..\/input\/cmapssdata\/RUL_FD001.txt', header=None)\n\nrul_1_gt.rename(columns={0: \"RUL_1gt\"}, inplace=True)\n################   ################   ################   ################   \n\ncolumn_name_dict={ 0: \"engine_id\", 1: \"cycle\", 2: \"op_set_1\", 3: \"op_set_2\", 4: \"op_set_3\", 5:\"sensor_1\", 6: \"sensor_2\",\n                   7: \"sensor_3\", 8: \"sensor_4\", 9: \"sensor_5\", 10: \"sensor_6\", 11: \"sensor_7\", 12: \"sensor_8\", 13: \"sensor_9\",\n                  14: \"sensor_10\", 15: \"sensor_11\", 16: \"sensor_12\", 17: \"sensor_13\", 18: \"sensor_14\", 19: \"sensor_15\", 20: \"sensor_16\",\n                  21: \"sensor_17\", 22: \"sensor_18\", 23: \"sensor_19\", 24: \"sensor_20\", 25: \"sensor_21\", 26: \"sensor_22\", 27: \"sensor_23\"}\n\ntest_1 = pd.read_csv('..\/input\/cmapssdata\/test_FD001.txt', header=None, sep=' ')\n\ntest_1.rename(columns=column_name_dict, inplace=True)\n\n################   ################   ################   ################ \n\ntrain_1 = pd.read_csv('..\/input\/cmapssdata\/train_FD001.txt', header=None, sep=' ')\n\ntrain_1.rename(columns=column_name_dict, inplace=True)\n","99731807":"train_1","338cd098":"#drop the last 2 columns from train and test sets since NaN\ntrain_1.drop(columns=[\"sensor_22\", \"sensor_23\"], inplace=True)\ntest_1.drop(columns=[\"sensor_22\", \"sensor_23\"], inplace=True)","5e87b97e":"#RUL for train_1 \nrul_1 = pd.DataFrame(train_1.groupby('engine_id')['cycle'].max()).reset_index()\nrul_1.columns = ['engine_id', 'max']\n\nrul_train_1 = train_1.merge(rul_1, on=['engine_id'], how='left')\nrul_train_1['RUL'] = rul_train_1['max'] - rul_train_1['cycle']\nrul_train_1.drop(['max'], axis=1, inplace=True)","bfd04825":"#RUL for test_1\nrul_1_gt[\"engine_id\"]=rul_1_gt.index + 1\n\nmax_1 = pd.DataFrame(test_1.groupby('engine_id')['cycle'].max()).reset_index()\nmax_1.columns = ['engine_id', 'max']\nmax_test_1 = test_1.merge(max_1, on=['engine_id'], how='left')\nrul_test_1 = max_test_1.merge(rul_1_gt, on=['engine_id'], how='left')\n\nrul_test_1['RUL'] = rul_test_1['max'] - rul_test_1['cycle'] + rul_test_1[\"RUL_1gt\"] \nrul_test_1.drop(['max', 'RUL_1gt'], axis=1, inplace=True)\n","ed08db02":"rul_train_1.groupby('engine_id')['cycle'].describe()","42225c7b":"rul_train_1.describe().transpose()","2abaf7de":"#drop columns which has std=0 because they don't carry info about the state of the unit\ndeleted_1=[\"op_set_3\", \"sensor_18\", \"sensor_19\"]\n\nrul_train_1.drop(columns=deleted_1, inplace=True)\nrul_test_1.drop(columns=deleted_1, inplace=True)","4a4aa2d3":"#heatmap for correlation coefficient\n\n# calculate correlation\ndf_corr = rul_train_1.drop(columns=[\"engine_id\"]).corr()\n\n# correlation matrix\nsns.set(font_scale=0.8)\nplt.figure(figsize=(24,16))\nsns.heatmap(df_corr, annot=True, fmt=\".4f\",vmin=-1, vmax=1, linewidths=.5, cmap = sns.color_palette(\"coolwarm\", 200))\n\nplt.figtext(.45, 0.9,'correlation matrix of train_1', fontsize=16, ha='center')\nplt.xticks(rotation=90)\nplt.show()\n","65ddd090":"#corr(RUL&feature )==0 ==> will be deleted since it has no corr between target(RUL) \ncorr_del=[\"sensor_1\", \"sensor_5\", \"sensor_10\", \"sensor_16\"]\nrul_train_1.drop(columns=corr_del, inplace=True)","ad2d931a":"#X, y splitting\nX_imp=rul_train_1.drop(columns=[\"RUL\", \"engine_id\", \"cycle\"])\ny_imp=rul_train_1[\"RUL\"]\n\n#feature importances\nrf_clf = RandomForestClassifier(n_estimators = 500, max_depth=5)\nrf_clf.fit(X_imp, y_imp)\n\npd.Series(rf_clf.feature_importances_, index = X_imp.columns).nlargest(30).plot(kind = 'pie',\n                                                                                figsize = (8, 8),\n                                                                                title = 'Feature importance from RandomForest for train set', colormap='twilight', fontsize=10)","61284557":"# permutation importance with RF classifier for deciding to elimination bet. 9&14 since they are highly correlated\natts=rul_train_1.columns\natts_arr = np.array(atts)\nresult = permutation_importance(rf_clf, X_imp, y_imp, n_repeats=10, random_state=42, n_jobs=-1)\nsorted_idx = result.importances_mean.argsort()\n\nfig, ax = plt.subplots()\nax.boxplot(result.importances[sorted_idx].T, vert=False, labels=atts_arr[sorted_idx])\nax.set_title(\"Permutation Importances (train set)\")\nfig.tight_layout()\nplt.show()\n\nfor i in result.importances_mean.argsort()[::-1]:\n     print(f\"{atts_arr[i]:<8} \"\n           f\"{result.importances_mean[i]:.3f}\"\n           f\" +\/- {result.importances_std[i]:.3f}\")\n","28d88429":"#corr matrix results to be deleted 14-9\ntrain_1_del=[\"sensor_14\"]\ntrain_1.drop(columns=train_1_del, inplace=True)","3300d7a4":"dd_1=train_1.groupby('engine_id')['cycle'].max().reset_index().sort_values(\"cycle\", ascending=False)\n\ndd_1.reset_index(drop=True, inplace=True)\ntrain_1_top=dd_1.head(10)\n\nplt.figure(figsize=(7,7))\nsns.barplot(x=\"engine_id\", y=\"cycle\", data=train_1_top, palette=\"rocket\", order=train_1_top['engine_id'])\nplt.title(\"Train_1-->Top 10 engines\")\nplt.xticks(rotation=90);","4a259b6d":"#train_1 minmax scaling\ncols_normalize_1 = rul_train_1.columns.difference(['engine_id','cycle'])\n\nscaler_1 = MinMaxScaler()\nnorm_rul_train_1 = pd.DataFrame(scaler_1.fit_transform(rul_train_1[cols_normalize_1]), \n                                columns=cols_normalize_1, \n                                index=rul_train_1.index)\n\nnorm_rul_train_1=pd.concat([norm_rul_train_1, rul_train_1[[\"engine_id\", \"cycle\"]]], axis=1)\n\n################   ################   ################   ################   ################\n\n#test_1 minmax scaling\nnorm_rul_test_1 = pd.DataFrame(scaler_1.transform(rul_test_1[cols_normalize_1]), \n                                columns=cols_normalize_1, \n                                index=rul_test_1.index)\n\nnorm_rul_test_1=pd.concat([norm_rul_test_1, rul_test_1[[\"engine_id\", \"cycle\"]]], axis=1)\n\n","0f6c5be8":"#split engines for set_1\ng_1=norm_rul_train_1.groupby('engine_id')\ng_test1=norm_rul_test_1.groupby('engine_id')\n\n#list of dfs(engines)\ntrain_list = []\ntest_list = []  \n\nfor engineid in g_1.groups.keys():\n    train_list.append(g_1.get_group(engineid)) \n\nfor engineid in g_test1.groups.keys():\n    test_list.append(g_test1.get_group(engineid))","d727d864":"len(train_list)","6f0f4987":"len(test_list)","669991cf":"#generating sequences for each engine --> multivariate one step problem\nfrom numpy import array\n\n#df: df extracted from train_list, n_steps: window size\ndef split_sequences(df, n_steps):\n    X, y = list(), list()\n    for i in range(len(df)):\n        end_ix = i + n_steps\n        if end_ix > len(df):\n            break\n        seq_x, seq_y = df[i:end_ix, 1:], df[end_ix-1, 0]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)\n\n\nlist_x=[]\nlist_y=[]\nfor engine_df in train_list:\n    #convert df to arr \n    engine_arr=engine_df.drop(columns=[\"engine_id\", \"cycle\"]).to_numpy()\n    X, y = split_sequences(engine_arr, 21)#since smallest df has 21 rows\n    list_x.append(X)\n    list_y.append(y)\n\n#concat alt alta  \nX_arr=np.concatenate(list_x)\ny_arr=np.concatenate(list_y)","ff2bdccc":"engine_arr.shape #input of split_seq function","b7a96042":"X.shape #output of split_seq function","d0c6becf":"y.shape #output of split_seq function","9674053c":"X_arr.shape #concat all engines","7e48a8fc":"y_arr.shape","2ec39314":"seq_len=X_arr.shape[1]\nnum_features=X_arr.shape[2]\n\nmodel = Sequential()\nmodel.add(LSTM(input_shape=(seq_len, num_features), units=100, return_sequences=True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units=50, return_sequences=True))\nmodel.add(Dropout(0.2))\n\nmodel.add(LSTM(units=10, return_sequences=False))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units=1))\nmodel.compile(optimizer='adam', loss='mse')\n\n#This callback will stop the training when there is no improvement in the validation loss for 20 consecutive epochs. \n#regularization technique as preventing overfitting, model runtime gets shorter\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=20)\n\n# fit model\nhistory=model.fit(X_arr, y_arr, epochs=200, callbacks=[callback])\nlen(history.history['loss']) \n","052bb8ef":"#prepare test set to make predictions\nlist_x_test=[]\nlist_y_test=[]\n\nfor engine_df in test_list:\n    #convert df to arr \n    engine_arr=engine_df.drop(columns=[\"engine_id\", \"cycle\"]).to_numpy()\n    X, y = split_sequences(engine_arr, seq_len)\n    \n    #use only last seq for each engine\n    list_x_test.append(X[-1].reshape((1, seq_len, num_features)))\n    list_y_test.append(y[-1].reshape((1, )))\n\n#concat alt alta  \nX_arr_test=np.concatenate(list_x_test)\ny_arr_test=np.concatenate(list_y_test)","9bf2ed96":"X_arr_test.shape","1879cdb2":"y_arr_test.shape","2b82d83a":"#create a dummy df to take inverse only on one col --> \"y_pred\"\ndef invTransform(scaler, y_pred, colNames):\n    dummy = pd.DataFrame(np.zeros((len(y_pred), len(colNames))), columns=colNames)\n    dummy[\"RUL\"] = y_pred\n    dummy = pd.DataFrame(scaler.inverse_transform(dummy), columns=colNames)\n    return dummy[\"RUL\"].values","39172627":"#prediction\ny_pred = model.predict(X_arr_test, verbose=0)\n\n#inverse scaling for y_pred values\ny_pred_inv=invTransform(scaler_1, y_pred, cols_normalize_1)\n\ny_pred_reshaped=y_pred_inv.reshape((len(y_pred_inv, )))\ny_pred_reshaped.shape","1095abe1":"y_truth=rul_1_gt[\"RUL_1gt\"].values\ny_truth.shape","c2450bc4":"y_truth","96239640":"y_pred_reshaped","9e369cbd":"#error cal. for paper\ndef score(y_true, y_pred, a1=10, a2=13):\n    score = 0\n    d = y_pred - y_true\n    for i in d:\n        if i >= 0 :\n            score += math.exp(i\/a2) - 1   \n        else:\n            score += math.exp(- i\/a1) - 1\n    return score","43ae0b4d":"score(y_truth,y_pred_reshaped)","dafbeff8":"def score_func(y_true, y_pred):\n    score_list = [\n                  round(score(y_true,y_pred), 2), \n                  round(mean_absolute_error(y_true,y_pred), 2),\n                  round(mean_squared_error(y_true,y_pred),2) ** 0.5,\n                  round(r2_score(y_true,y_pred), 2)\n                  ]\n    \n    print(f' R2 score: {score_list[3]}')\n    print(f' compatitive score: {score_list[0]}')\n    print(f' mean absolute error: {score_list[1]}')\n    print(f' root mean squared error: {score_list[2]}')\n    \n    return 0","8530e097":"score_func(y_truth,y_pred_reshaped)","2ca5f4a2":"1. Random Forest importances:","7f816bb1":"Each row is a snapshot of data taken during a single operational cycle, each column is a different variable.","80fccad5":"Compute RUL values","6debb935":"- [permutation importances](http:\/\/scikit-learn.org\/stable\/modules\/permutation_importance.html#permutation-importance)\n\n- This procedure breaks the relationship between the feature and the target, thus the drop in the model score is indicative of how much the model depends on the feature.","b92a2d06":"2. w Permutation importance:","e775fb38":"We have to make inverse scaling for RUL column in order to produce proper prediction results","9c3bfead":"# Define LSTM Model","7059e1c4":"# PREDICTION","0dd6e6d2":"# SCORES","d6afb186":"Corr Matrix","daf9008e":"# FEATURE SELECTION","7139d2fd":"- Damage propagation was allowed to continue until a failure criterion was reached. A health index was defined as the minimum of several superimposed operational margins at any given time instant and the failure criterion is reached when health index reaches zero. Output of the model was the time series (cycles) of sensed measurements typically available from aircraft gas turbine engines. \n\n- Prognostics here exclusively as the estimation of remaining useful component life. The remaining useful life (RUL) estimates are in units of time (e.g., hours or cycles). End-of-life can be subjectively determined as a function of operational thresholds that can be measured. These thresholds depend on user specifications to determine safe operational limits.\n\n- An important requirement for the damage modeling process was the availability of a suitable system model that allows input variations of health related parameters and recording of the resulting output sensor measurements. The recently released C-MAPSS (Commercial Modular Aero- Propulsion System Simulation)\n\n- C-MAPSS is a tool for simulating a realistic large commercial turbofan engine. The software is coded in the MATLAB\u00ae and Simulink\u00ae environment, and includes a number of editable input parameters that allow the user to enter specific values of his\/her own choice regarding operational profile, closed-loop controllers, environmental conditions, etc\n\nData Set: FD001 ==>\nTrain trjectories: 100,\nTest trajectories: 100,\nConditions: ONE (Sea Level),\nFault Modes: ONE (HPC Degradation)\n\nData Set: FD002 ==>\nTrain trjectories: 260,\nTest trajectories: 259,\nConditions: SIX,\nFault Modes: ONE (HPC Degradation)\n\n###Experimental Scenario###\n\nData sets consists of multiple multivariate time series. Each data set is further divided into training and test subsets. Each time series is from a different engine \u00f1 i.e., the data can be considered to be from a fleet of engines of the same type. Each engine starts with different degrees of initial wear and manufacturing variation which is unknown to the user. This wear and variation is considered normal, i.e., it is not considered a fault condition. There are three operational settings that have a substantial effect on engine performance. These settings are also included in the data. The data is contaminated with sensor noise.\n\nThe engine is operating normally at the start of each time series, and develops a fault at some point during the series. In the training set, the fault grows in magnitude until system failure. In the test set, the time series ends some time prior to system failure. The objective of the competition is to predict the number of remaining operational cycles before failure in the test set, i.e., the number of operational cycles after the last cycle that the engine will continue to operate. Also provided a vector of true Remaining Useful Life (RUL) values for the test data.","666989ca":"#  NORMALIZATION: MinMax Scaling","eebc20e1":"# Damage Propagation Modeling \n# Aircraft Engine Run-to-Failure Simulation"}}