{"cell_type":{"0af4cf9a":"code","680b3d8e":"code","983d3651":"code","07c92ae9":"code","e136c6fa":"code","5f6e83a9":"code","56c2faed":"code","5d890064":"code","a7edb2b9":"code","3282bcc5":"code","0a102f65":"code","f1e956e3":"code","107f4d08":"code","f0691cfb":"code","5a69dde0":"code","d7377f5d":"code","0222421f":"code","3b28eb96":"code","45b34d9a":"code","fde03f1c":"code","75535bcc":"code","2f5cc93b":"code","2ca5527e":"code","3925c887":"code","e9f1958f":"code","736ea8fe":"code","00be96c5":"code","997f6754":"code","58670c69":"code","f381cb95":"code","e0e9cd7b":"code","ce20e095":"code","62b64ab3":"code","fa6850f4":"code","116c30e8":"code","484a8850":"code","00245575":"code","e88c4658":"code","5a9e18dd":"code","12eab150":"code","f3fa6f61":"code","f5feca45":"code","9c8e28ee":"code","b2756ff5":"code","45f4ef3b":"code","6dd11330":"code","6ebcfe3b":"code","4215d8ac":"code","18e81f5a":"code","1ab1dac7":"code","01d6de37":"code","d7945507":"code","c371fdc9":"code","9183a6ab":"code","a645fff3":"code","967d303c":"code","4b20b9e1":"code","95251407":"code","1eff68fe":"code","c6586d38":"code","df5204f4":"markdown","dc3f8717":"markdown","329beacc":"markdown","135783d9":"markdown","b88be6f8":"markdown","417b0359":"markdown","021219a1":"markdown","cc57cafb":"markdown","dea10b5c":"markdown","48a78525":"markdown","216c9d1f":"markdown","38a083dc":"markdown","250f9315":"markdown","875e3457":"markdown","bb16e7bb":"markdown","04979e80":"markdown","38550acc":"markdown","fb05ef77":"markdown","bd6102a2":"markdown","b1ccfdb8":"markdown","a7657455":"markdown","6bf95eb1":"markdown","5582a47c":"markdown","a895e716":"markdown","8f551381":"markdown","04813047":"markdown","b465619f":"markdown","b0653b5d":"markdown","79c697a9":"markdown","d9935b00":"markdown","f95d968c":"markdown","592ad2ac":"markdown","52532964":"markdown","d9825f62":"markdown","fe4ac7c1":"markdown","db1d4239":"markdown","36283863":"markdown","131145dc":"markdown","e23f1aab":"markdown","972d65bd":"markdown","7f40b7e8":"markdown","22c0ba91":"markdown","c3b0f797":"markdown","da1d638c":"markdown","a0ef5cf8":"markdown","d3ae930e":"markdown","fb4863f0":"markdown"},"source":{"0af4cf9a":"pip install visualkeras","680b3d8e":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport cv2\nimport visualkeras\n\nfrom sklearn.model_selection import train_test_split","983d3651":"data_dir = (r'\/kaggle\/input\/brain-tumor-classification-mri\/Training\/')\ncategories = ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\nfor i in categories:\n    path = os.path.join(data_dir, i)\n    for img in os.listdir(path):\n        img_array = cv2.imread(os.path.join(path,img))   ","07c92ae9":"plt.imshow(img_array)","e136c6fa":"img_array.shape","5f6e83a9":"new_size = 200\nnew_array = cv2.resize(img_array,(new_size,new_size))    \nplt.imshow(new_array,cmap = \"gray\")","56c2faed":"plt.figure(figsize=(20, 16))\n\nfileNames = ['glioma_tumor\/gg (10).jpg', 'meningioma_tumor\/m (108).jpg', 'no_tumor\/image (16).jpg', 'pituitary_tumor\/p (12).jpg']\n\nfor i in range(4):\n    ax = plt.subplot(2, 2, i + 1)\n    img = cv2.imread(data_dir + fileNames[i])\n    img = cv2.resize(img, (new_size, new_size))\n    plt.imshow(img)\n    plt.title(categories[i])","5d890064":"x_train=[]\ny_train=[]\n\nfor i in categories:\n    train_path = os.path.join(data_dir,i)\n    for j in os.listdir(train_path):\n        img = cv2.imread(os.path.join(train_path,j),cv2.IMREAD_GRAYSCALE)\n        img = cv2.resize(img,(new_size,new_size))\n        x_train.append(img)\n        y_train.append(i)","a7edb2b9":"x_train=np.array(x_train)\nx_train=x_train\/255.0\nx_train = x_train.reshape(-1,new_size,new_size,1)\nx_train.shape","3282bcc5":"y_train = np.array(y_train)\ny_train.shape","0a102f65":"sns.countplot(y_train)","f1e956e3":"from sklearn.model_selection import train_test_split\nX_train,X_val,y_train,y_val = train_test_split(x_train,y_train,test_size=0.2,random_state=42)","107f4d08":"import tensorflow as tf","f0691cfb":"y_train_new = []\nfor i in y_train:\n    y_train_new.append(categories.index(i))\ny_train = y_train_new\ny_train = tf.keras.utils.to_categorical(y_train)\n\n\ny_val_new = []\nfor i in y_val:\n    y_val_new.append(categories.index(i))\ny_val = y_val_new\ny_val = tf.keras.utils.to_categorical(y_val)","5a69dde0":"X_train.shape, y_train.shape","d7377f5d":"X_val.shape, y_val.shape","0222421f":"from sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.layers import BatchNormalization\nfrom keras.optimizers import RMSprop,Adam,SGD,Adadelta\nfrom keras.preprocessing.image import ImageDataGenerator","3b28eb96":"epochs = 30 \nbatch_size = 32","45b34d9a":"datagen = ImageDataGenerator(\n    featurewise_center=False,\n    samplewise_center=False,\n    featurewise_std_normalization=False,\n    samplewise_std_normalization=False,\n    zca_whitening=False,\n    rotation_range=0,\n    zoom_range = 0,\n    width_shift_range=0,\n    height_shift_range=0,\n    horizontal_flip=True,\n    vertical_flip=False)  ","fde03f1c":"model = Sequential()\nmodel.add(Conv2D(filters=64, kernel_size=(3,3), padding= 'Same', activation='relu',input_shape=(X_train.shape[1],X_train.shape[2],1)))\nmodel.add(MaxPool2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding= 'Same', activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(filters=128, kernel_size=3, padding= 'Same', activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(filters=256, kernel_size=3, padding= 'Same', activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(filters=256, kernel_size=3, padding= 'Same', activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(filters=512, kernel_size=3, padding= 'Same', activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(1024, activation = \"relu\"))\n\nmodel.add(Dense(512, activation = \"relu\"))\n\nmodel.add(Dense(4, activation = \"softmax\"))\noptimizer = SGD(lr=0.01)\n\nmodel.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","75535bcc":"datagen.fit(X_train)\nhistory = model.fit_generator(datagen.flow(X_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (X_val,y_val))","2f5cc93b":"model1 = Sequential()\nmodel1.add(Conv2D(filters=64, kernel_size=3, padding= 'Same', activation='relu', input_shape=(X_train.shape[1],X_train.shape[2],1)))\nmodel1.add(MaxPool2D(pool_size=(2,2)))\nmodel1.add(BatchNormalization())\n\nmodel1.add(Conv2D(filters=128, kernel_size=3, padding= 'Same', activation='relu'))\nmodel1.add(MaxPool2D(pool_size=(2,2)))\nmodel1.add(BatchNormalization())\n\nmodel1.add(Conv2D(filters=128, kernel_size=3, padding= 'Same', activation='relu'))\nmodel1.add(MaxPool2D(pool_size=(2,2)))\nmodel1.add(BatchNormalization())\n\nmodel1.add(Conv2D(filters=256, kernel_size=3, padding= 'Same', activation='relu'))\nmodel1.add(MaxPool2D(pool_size=(2,2)))\nmodel1.add(BatchNormalization())\n\nmodel1.add(Conv2D(filters=256, kernel_size=3, padding= 'Same', activation='relu'))\nmodel1.add(MaxPool2D(pool_size=(2,2)))\nmodel1.add(BatchNormalization())\n\nmodel1.add(Conv2D(filters=512, kernel_size=3, padding= 'Same', activation='relu'))\nmodel1.add(MaxPool2D(pool_size=(2,2)))\nmodel1.add(BatchNormalization())\n\nmodel1.add(Flatten())\nmodel1.add(Dense(1024, activation = \"relu\"))\nmodel1.add(Dropout(0.2))\n\nmodel1.add(Dense(512, activation = \"relu\"))\nmodel1.add(Dropout(0.2))\n\nmodel1.add(Dense(4, activation = \"softmax\"))\noptimizer = SGD(lr=0.01)\n\nmodel1.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","2ca5527e":"history1 = model1.fit_generator(datagen.flow(X_train,y_train, batch_size=batch_size),\n                                epochs = epochs, validation_data = (X_val,y_val))","3925c887":"fig, axs = plt.subplots(2, 2, figsize=(20,17))\n\naxs[0, 0].plot(history.history[\"loss\"],c = \"purple\")\naxs[0, 0].plot(history.history[\"val_loss\"],c = \"orange\")\naxs[0, 0].set_ylim([0,2])\naxs[0, 0].legend([\"train\", \"test\"])\naxs[0, 0].set_title('Loss non-regularized model')\n\naxs[0, 1].plot(history1.history[\"loss\"],c = \"purple\")\naxs[0, 1].plot(history1.history[\"val_loss\"],c = \"orange\")\naxs[0, 1].set_ylim([0,2])\naxs[0, 1].legend([\"train\", \"test\"])\naxs[0, 1].set_title('Loss regularized model')\n\naxs[1, 0].plot(history.history[\"accuracy\"],c = \"purple\")\naxs[1, 0].plot(history.history[\"val_accuracy\"],c = \"orange\")\naxs[1, 0].set_ylim([0,1.1])\naxs[1, 0].legend([\"train\", \"test\"])\naxs[1, 0].set_title('Accuracy non-regularized model')\n\naxs[1, 1].plot(history1.history[\"accuracy\"],c = \"purple\")\naxs[1, 1].plot(history1.history[\"val_accuracy\"],c = \"orange\")\naxs[1, 1].set_ylim([0,1.1])\naxs[1, 1].legend([\"train\", \"test\"])\naxs[1, 1].set_title('Accuracy regularized model')","e9f1958f":"model2 = Sequential()\nmodel2.add(Conv2D(filters=64, kernel_size=5, padding= 'Same', activation='relu', input_shape=(X_train.shape[1],X_train.shape[2],1)))\nmodel2.add(MaxPool2D(pool_size=(2,2)))\nmodel2.add(BatchNormalization())\n\nmodel2.add(Conv2D(filters=128, kernel_size=5, padding= 'Same', activation='relu'))\nmodel2.add(MaxPool2D(pool_size=(2,2)))\nmodel2.add(BatchNormalization())\n\nmodel2.add(Conv2D(filters=128, kernel_size=5, padding= 'Same', activation='relu'))\nmodel2.add(MaxPool2D(pool_size=(2,2)))\nmodel2.add(BatchNormalization())\n\nmodel2.add(Conv2D(filters=256, kernel_size=5, padding= 'Same', activation='relu'))\nmodel2.add(MaxPool2D(pool_size=(2,2)))\nmodel2.add(BatchNormalization())\n\nmodel2.add(Conv2D(filters=256, kernel_size=5, padding= 'Same', activation='relu'))\nmodel2.add(MaxPool2D(pool_size=(2,2)))\nmodel2.add(BatchNormalization())\n\nmodel2.add(Conv2D(filters=512, kernel_size=5, padding= 'Same', activation='relu'))\nmodel2.add(MaxPool2D(pool_size=(2,2)))\nmodel2.add(BatchNormalization())\n\nmodel2.add(Flatten())\nmodel2.add(Dense(1024, activation = \"relu\"))\nmodel2.add(Dropout(0.2))\n\nmodel2.add(Dense(512, activation = \"relu\"))\nmodel2.add(Dropout(0.2))\n\nmodel2.add(Dense(4, activation = \"softmax\"))\noptimizer = SGD(lr=0.01)\n\nmodel2.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","736ea8fe":"history2 = model2.fit_generator(datagen.flow(X_train,y_train, batch_size=batch_size),\n                                epochs = epochs, validation_data = (X_val,y_val))","00be96c5":"model3 = Sequential()\nmodel3.add(Conv2D(filters=64, kernel_size=7, padding= 'Same', activation='relu', input_shape=(X_train.shape[1],X_train.shape[2],1)))\nmodel3.add(MaxPool2D(pool_size=(2,2)))\nmodel3.add(BatchNormalization())\n\nmodel3.add(Conv2D(filters=128, kernel_size=7, padding= 'Same', activation='relu'))\nmodel3.add(MaxPool2D(pool_size=(2,2)))\nmodel3.add(BatchNormalization())\n\nmodel3.add(Conv2D(filters=128, kernel_size=7, padding= 'Same', activation='relu'))\nmodel3.add(MaxPool2D(pool_size=(2,2)))\nmodel3.add(BatchNormalization())\n\nmodel3.add(Conv2D(filters=256, kernel_size=7, padding= 'Same', activation='relu'))\nmodel3.add(MaxPool2D(pool_size=(2,2)))\nmodel3.add(BatchNormalization())\n\nmodel3.add(Conv2D(filters=256, kernel_size=7, padding= 'Same', activation='relu'))\nmodel3.add(MaxPool2D(pool_size=(2,2)))\nmodel3.add(BatchNormalization())\n\nmodel3.add(Conv2D(filters=512, kernel_size=7, padding= 'Same', activation='relu'))\nmodel3.add(MaxPool2D(pool_size=(2,2)))\nmodel3.add(BatchNormalization())\n\nmodel3.add(Flatten())\nmodel3.add(Dense(1024, activation = \"relu\"))\nmodel3.add(Dropout(0.2))\n\nmodel3.add(Dense(512, activation = \"relu\"))\nmodel3.add(Dropout(0.2))\n\nmodel3.add(Dense(4, activation = \"softmax\"))\noptimizer = SGD(lr=0.01)\n\nmodel3.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","997f6754":"history3 = model3.fit_generator(datagen.flow(X_train,y_train, batch_size=batch_size),\n                                epochs = epochs, validation_data = (X_val,y_val))","58670c69":"from keras.models import load_model\n\nmodel3.save('my_model.h5')","f381cb95":"fig, axs = plt.subplots(3, 2, figsize=(20,17))\n\naxs[0, 0].plot(history1.history[\"loss\"],c = \"purple\")\naxs[0, 0].plot(history1.history[\"val_loss\"],c = \"orange\")\naxs[0, 0].set_ylim([-0.1,2])\naxs[0, 0].legend([\"train\", \"test\"])\naxs[0, 0].set_title('Loss model kernels 3x3')\n\naxs[0, 1].plot(history1.history[\"accuracy\"],c = \"purple\")\naxs[0, 1].plot(history1.history[\"val_accuracy\"],c = \"orange\")\naxs[0, 1].set_ylim([0.8,1.1])\naxs[0, 1].legend([\"train\", \"test\"])\naxs[0, 1].set_title('Accuracy model kernels 3x3')\n\naxs[1, 0].plot(history2.history[\"loss\"],c = \"purple\")\naxs[1, 0].plot(history2.history[\"val_loss\"],c = \"orange\")\naxs[1, 0].set_ylim([-0.1,2])\naxs[1, 0].legend([\"train\", \"test\"])\naxs[1, 0].set_title('Loss model kernels 5x5')\n\naxs[1, 1].plot(history2.history[\"accuracy\"],c = \"purple\")\naxs[1, 1].plot(history2.history[\"val_accuracy\"],c = \"orange\")\naxs[1, 1].set_ylim([0.8,1.1])\naxs[1, 1].legend([\"train\", \"test\"])\naxs[1, 1].set_title('Accuracy model kernels 5x5')\n\naxs[2, 0].plot(history3.history[\"loss\"],c = \"purple\")\naxs[2, 0].plot(history3.history[\"val_loss\"],c = \"orange\")\naxs[2, 0].set_ylim([-0.1,2])\naxs[2, 0].legend([\"train\", \"test\"])\naxs[2, 0].set_title('Loss model kernels 7x7')\n\naxs[2, 1].plot(history3.history[\"accuracy\"],c = \"purple\")\naxs[2, 1].plot(history3.history[\"val_accuracy\"],c = \"orange\")\naxs[2, 1].set_ylim([0.8,1.1])\naxs[2, 1].legend([\"train\", \"test\"])\naxs[2, 1].set_title('Accuracy model kernels 7x7')","e0e9cd7b":"history4 = model3.fit_generator(datagen.flow(X_train,y_train, batch_size=16),\n                                epochs = epochs, validation_data = (X_val,y_val))","ce20e095":"history5 = model3.fit_generator(datagen.flow(X_train,y_train, batch_size=64),\n                                epochs = epochs, validation_data = (X_val,y_val))","62b64ab3":"fig, axs = plt.subplots(3, 2, figsize=(20,17))\n\naxs[0, 0].plot(history4.history[\"loss\"],c = \"purple\")\naxs[0, 0].plot(history4.history[\"val_loss\"],c = \"orange\")\naxs[0, 0].set_ylim([-0.1,2])\naxs[0, 0].legend([\"train\", \"test\"])\naxs[0, 0].set_title('Loss model batch size 16')\n\naxs[0, 1].plot(history4.history[\"accuracy\"],c = \"purple\")\naxs[0, 1].plot(history4.history[\"val_accuracy\"],c = \"orange\")\naxs[0, 1].set_ylim([0.8,1.1])\naxs[0, 1].legend([\"train\", \"test\"])\naxs[0, 1].set_title('Accuracy model batch size 16')\n\naxs[1, 0].plot(history3.history[\"loss\"],c = \"purple\")\naxs[1, 0].plot(history3.history[\"val_loss\"],c = \"orange\")\naxs[1, 0].set_ylim([-0.1,2])\naxs[1, 0].legend([\"train\", \"test\"])\naxs[1, 0].set_title('Loss model batch size 32')\n\naxs[1, 1].plot(history3.history[\"accuracy\"],c = \"purple\")\naxs[1, 1].plot(history3.history[\"val_accuracy\"],c = \"orange\")\naxs[1, 1].set_ylim([0.8,1.1])\naxs[1, 1].legend([\"train\", \"test\"])\naxs[1, 1].set_title('Accuracy model batch size 32')\n\naxs[2, 0].plot(history5.history[\"loss\"],c = \"purple\")\naxs[2, 0].plot(history5.history[\"val_loss\"],c = \"orange\")\naxs[2, 0].set_ylim([-0.1,2])\naxs[2, 0].legend([\"train\", \"test\"])\naxs[2, 0].set_title('Loss model batch size 64')\n\naxs[2, 1].plot(history5.history[\"accuracy\"],c = \"purple\")\naxs[2, 1].plot(history5.history[\"val_accuracy\"],c = \"orange\")\naxs[2, 1].set_ylim([0.8,1.1])\naxs[2, 1].legend([\"train\", \"test\"])\naxs[2, 1].set_title('Accuracy model batch size 64')","fa6850f4":"history7 = model3.fit_generator(datagen.flow(X_train,y_train, batch_size=32),\n                                epochs = 50, validation_data = (X_val,y_val))","116c30e8":"history8 = model3.fit_generator(datagen.flow(X_train,y_train, batch_size=32),\n                                epochs = 70, validation_data = (X_val,y_val))","484a8850":"fig, axs = plt.subplots(3, 2, figsize=(20,17))\n\naxs[0, 0].plot(history3.history[\"loss\"],c = \"purple\")\naxs[0, 0].plot(history3.history[\"val_loss\"],c = \"orange\")\naxs[0, 0].set_ylim([-0.1,2])\naxs[0, 0].legend([\"train\", \"test\"])\naxs[0, 0].set_title('Loss model b32 30 epochs')\n\naxs[0, 1].plot(history3.history[\"accuracy\"],c = \"purple\")\naxs[0, 1].plot(history3.history[\"val_accuracy\"],c = \"orange\")\naxs[0, 1].set_ylim([0.8,1.1])\naxs[0, 1].legend([\"train\", \"test\"])\naxs[0, 1].set_title('Accuracy model b32 30 epochs')\n\naxs[1, 0].plot(history7.history[\"loss\"],c = \"purple\")\naxs[1, 0].plot(history7.history[\"val_loss\"],c = \"orange\")\naxs[1, 0].set_ylim([-0.1,2])\naxs[1, 0].legend([\"train\", \"test\"])\naxs[1, 0].set_title('Loss model b32 50 epochs')\n\naxs[1, 1].plot(history7.history[\"accuracy\"],c = \"purple\")\naxs[1, 1].plot(history7.history[\"val_accuracy\"],c = \"orange\")\naxs[1, 1].set_ylim([0.8,1.1])\naxs[1, 1].legend([\"train\", \"test\"])\naxs[1, 1].set_title('Accuracy model b32 50 epochs')\n\naxs[2, 0].plot(history8.history[\"loss\"],c = \"purple\")\naxs[2, 0].plot(history8.history[\"val_loss\"],c = \"orange\")\naxs[2, 0].set_ylim([-0.1,2])\naxs[2, 0].legend([\"train\", \"test\"])\naxs[2, 0].set_title('Loss model b32 70 epochs')\n\naxs[2, 1].plot(history8.history[\"accuracy\"],c = \"purple\")\naxs[2, 1].plot(history8.history[\"val_accuracy\"],c = \"orange\")\naxs[2, 1].set_ylim([0.8,1.1])\naxs[2, 1].legend([\"train\", \"test\"])\naxs[2, 1].set_title('Accuracy model b32 70 epochs')","00245575":"#history3 was the best one until now and its characteristics were:\nb_size=32\nep=30","e88c4658":"optimizer1 = Adam(lr=0.01, beta_1=0.9, beta_2=0.999)\nmodel3.compile(optimizer = optimizer1 , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\nhistory9 = model3.fit_generator(datagen.flow(X_train,y_train, batch_size=b_size),\n                                epochs = ep, validation_data = (X_val,y_val))","5a9e18dd":"optimizer2 = RMSprop(lr=0.01, rho=0.9)\nmodel3.compile(optimizer = optimizer2 , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\nhistory10 = model3.fit_generator(datagen.flow(X_train,y_train, batch_size=b_size),\n                                 epochs = ep, validation_data = (X_val,y_val))","12eab150":"optimizer3 = Adadelta(lr=0.01, rho=0.95)\nmodel3.compile(optimizer = optimizer3 , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\nhistory11 = model3.fit_generator(datagen.flow(X_train,y_train, batch_size=b_size),\n                                 epochs = ep, validation_data = (X_val,y_val))","f3fa6f61":"fig, axs = plt.subplots(2, 2, figsize=(20,17))\n\naxs[0, 0].plot(history3.history[\"loss\"],c = \"purple\")\naxs[0, 0].plot(history9.history[\"loss\"],c = \"orange\")\naxs[0, 0].plot(history10.history[\"loss\"],c = \"green\")\naxs[0, 0].plot(history11.history[\"loss\"],c = \"blue\")\naxs[0, 0].set_ylim([-0.1,2])\naxs[0, 0].legend([\"SGD\", \"Adam\", \"RMSprop\", \"Adadelta\"])\naxs[0, 0].set_title('Optimizers Train-Loss')\n\naxs[0, 1].plot(history3.history[\"val_loss\"],c = \"purple\")\naxs[0, 1].plot(history9.history[\"val_loss\"],c = \"orange\")\naxs[0, 1].plot(history10.history[\"val_loss\"],c = \"green\")\naxs[0, 1].plot(history11.history[\"val_loss\"],c = \"blue\")\naxs[0, 1].set_ylim([-0.1,2])\naxs[0, 1].legend([\"SGD\", \"Adam\", \"RMSprop\", \"Adadelta\"])\naxs[0, 1].set_title('Optimizers Val-Loss')\n\naxs[1, 0].plot(history3.history[\"accuracy\"],c = \"purple\")\naxs[1, 0].plot(history9.history[\"accuracy\"],c = \"orange\")\naxs[1, 0].plot(history10.history[\"accuracy\"],c = \"green\")\naxs[1, 0].plot(history11.history[\"accuracy\"],c = \"blue\")\naxs[1, 0].set_ylim([0.7,1.1])\naxs[1, 0].legend([\"SGD\", \"Adam\", \"RMSprop\", \"Adadelta\"])\naxs[1, 0].set_title('Optimizers Train-Accuracy')\n\naxs[1, 1].plot(history3.history[\"val_accuracy\"],c = \"purple\")\naxs[1, 1].plot(history9.history[\"val_accuracy\"],c = \"orange\")\naxs[1, 1].plot(history10.history[\"val_accuracy\"],c = \"green\")\naxs[1, 1].plot(history11.history[\"val_accuracy\"],c = \"blue\")\naxs[1, 1].set_ylim([0.7,1.1])\naxs[1, 1].legend([\"SGD\", \"Adam\", \"RMSprop\", \"Adadelta\"])\naxs[1, 1].set_title('Optimizers Val-Accuracy')","f5feca45":"model_x = load_model('my_model.h5')","9c8e28ee":"model3.summary()","b2756ff5":"visualkeras.layered_view(model3)","45f4ef3b":"X_val.shape, y_val.shape","6dd11330":"y_pred = model_x.predict(X_val)\nprint(y_pred[:5])","6ebcfe3b":"class_pred = [np.argmax(i) for i in y_pred]\nprint(class_pred[:5])","4215d8ac":"val_labels =[np.argmax(i) for i in y_val]\nprint(val_labels[:5])","18e81f5a":"from sklearn.metrics import classification_report\n\nreport = classification_report(val_labels, class_pred)\n\nprint(report)","1ab1dac7":"from sklearn.metrics import confusion_matrix\n\nf,ax = plt.subplots(figsize=(10, 10))\nconfusion_mtx = confusion_matrix(val_labels, class_pred)\nsns.set(font_scale=1.4)\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\",ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix Validation set\")\nplt.show()","01d6de37":"data_dir_test=(r'\/kaggle\/input\/brain-tumor-classification-mri\/Testing\/')","d7945507":"x_test=[]\ny_test=[]\n\nfor i in categories:\n    test_path = os.path.join(data_dir_test,i)\n    for j in os.listdir(test_path):\n        img = cv2.imread(os.path.join(test_path,j),cv2.IMREAD_GRAYSCALE)\n        img = cv2.resize(img,(new_size,new_size))\n        x_test.append(img)\n        y_test.append(i)","c371fdc9":"x_test=np.array(x_test)\nx_test=x_test\/255.0\nx_test=x_test.reshape(-1,new_size,new_size,1)\nx_test.shape","9183a6ab":"y_test=np.array(y_test)\ny_test.shape","a645fff3":"y_test_new = []\nfor i in y_test:\n    y_test_new.append(categories.index(i))\ny_test = y_test_new\ny_test = tf.keras.utils.to_categorical(y_test)","967d303c":"predicted_labels = model_x.predict(x_test)\nprint(predicted_labels[:5])","4b20b9e1":"predicted_labels = [np.argmax(i) for i in predicted_labels]\nprint(predicted_labels[:5])","95251407":"original_labels =[np.argmax(i) for i in y_test]\nprint(original_labels[:5])","1eff68fe":"report2 = classification_report(original_labels, predicted_labels)\n\nprint(report2)","c6586d38":"f,ax = plt.subplots(figsize=(10, 10))\nconfusion_mtx2 = confusion_matrix(original_labels, predicted_labels)\nsns.set(font_scale=1.4)\nsns.heatmap(confusion_mtx2, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\",ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix Testing set\")\nplt.show()","df5204f4":"## Performance on validation dataset:","dc3f8717":"**Insights:**  In the figure above we can see how SGD performed without a doubt as the best one in the four plots, the three other optimizers didn't even have an accuracy over 70%!, RMSprop and Adadelta seems unuseful for this data and Adam had too much transitories and does not reduce significantly in loss nor increase accuracy.  \n**Note:** I am novice in Deep Learning and thus when I was doing the 1st comparation I used Adam as optimizer and got horrible results, spent more than 2 days finding the reason and finally tried SGD and it works astonishingly, as recommendation when you are doing modeling by scratch and if you get bad performance always change the optimizer! Adam is hugely used, but as everything it depends on your data, there are at least 8 types which can be suit.","329beacc":"The following will show one sample of each category in the repository:","135783d9":"Predict the class of images in x_test:","b88be6f8":"### Plotting Loss and Accuracy for both models: ","417b0359":"Regularized model using Batch Normalization for Convolutional layers and Dropout for Fully conected layers:","021219a1":"## Best model: ","cc57cafb":"## Performance on testing dataset:","dea10b5c":"Compare with the original labels:","48a78525":"Transform our multiclass label to one hot encode type:","216c9d1f":"Later we will see why I am saving this model:","38a083dc":"## 5th comparison, models using optimizers Adam, SGD, RMSProp, Adadelta:","250f9315":"Splitting into training and validation sets at 20%:","875e3457":"**Insights:** Clearly as we add more epochs above 30 there is not a difference, it is the same value, meaning the same performance with less training.  \nBest number of epochs:30.  \nBest number of batch size:32.","bb16e7bb":"Predict the classes for x-validation set:","04979e80":"Get the index (label) of the class predicted:","38550acc":"Batch size = 16:","fb05ef77":"Convert to numpy array and reshape both x_test and y_test:","bd6102a2":"The following model is non-regularized and its layers will be used for the rest of the project:","b1ccfdb8":"## 4th comparison, models with number of epochs equal to 30, 50 or 70:","a7657455":"**Insights:** Firstly, again there is not a significant difference between the accuracies in the lasts epochs for the models, but batch size=16 offers the lowest accuracy and batch size=64 seems very strange as looks like it's extremely overfitting because of its perfect accuracy since the very first epoch and the same happens with validation set being always 96%, which is evidence that something is not working properly. However batch size=32 looks much better by increasing gradually and stably its accuracy in both training and validation sets.  \n**Note:** In the box comments I would love to know your ideas of why you think batch size=64 had such behaviour, I've researched a lot and didn't find enough. ","6bf95eb1":"Kernel 7x7:","5582a47c":"Printing the original classes for the 5 first records:","a895e716":"## 3rd comparison, models with Batch size equal to 16, 32 or 64:","8f551381":"Bar plot showing how many records are in each category inside training folder:","04813047":"Finally plot the classification report and confusion matrix for testing dataset: ","b465619f":"Kernel 5x5: ","b0653b5d":"### Plotting Loss and Accuracy for the three models: ","79c697a9":"**Insights:** The three models had a flawless performance reaching validation accuracies over 95%, also we can see how by increasing the kernel size implies a softening of the validation curve (less transitions), despite the fact that there is not a huge difference between them kernel size 7x7 was chosen to continue for offering the highest accuracy and most stable curve.","d9935b00":"Classification report for the validation dataset:","f95d968c":"Let's define the number of epochs and batch size that the firsts models will have:","592ad2ac":"Training model with 70 epochs: ","52532964":"Batch size = 64:","d9825f62":"## 2nd comparison, Kernel size: 3x3, 5x5, 7x7:","fe4ac7c1":"### Plotting Loss and Accuracy for the three models:","db1d4239":"**Insights:** The regularization had a positive effect by reducing the overfitting as can be seen, this lead to a better accuracy in both training and validation sets.","36283863":"### Plotting Loss and Accuracy for every optimizer:","131145dc":"## 1st comparison, non-regularized vs regularized model:","e23f1aab":"The size will be reduced in order to speed up the model processing:","972d65bd":"Below we will Define the training sets as following:   \nGrayscale images from training folder as x_train and its corresponding categories as y_train.","7f40b7e8":"Original shape of the images in the repository:","22c0ba91":"Finally print the shape of the four sets, here we can see how the y_train and y_val contains 4 columns which represent the classes encoded:","c3b0f797":"Normalizing and reshape of images to be suit for modeling:","da1d638c":"Defining x_test and y_test:","a0ef5cf8":"### Plotting Loss and Accuracy for the three models: ","d3ae930e":"The best model built had the following characteristics:  \n(By default our model contains 6 Convolutional, 6 Max-pooling and 3 Fully-conected layers).  \n    - Kernels size 7x7.  \n    - Regularization using Batch Normalization after each Convolutional layers and Dropout after each Fully conected layer.  \n    - Batch size = 32.  \n    - Number of epochs = 30.  \n    - Gradient Descent Optimizer: SGD.","fb4863f0":"Training model with 50 epochs:"}}