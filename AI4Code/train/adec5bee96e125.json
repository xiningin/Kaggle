{"cell_type":{"2c258dcd":"code","84261ab8":"code","f2613520":"code","be3a2a0c":"code","9e7cc095":"code","8b35adc8":"code","dc2bd77a":"code","eddf6b16":"code","4b25b080":"code","6374699f":"code","ada74857":"code","2334f5b7":"code","62e1b8ac":"code","22a7edb3":"code","7319df5b":"code","d7397db6":"code","b97635e7":"code","eda15cc7":"code","43c76d85":"code","bb3685a0":"code","5a7ddb52":"code","b846f89b":"code","29694953":"code","220198c7":"code","5e96776d":"code","9cd5613c":"code","8170cb40":"code","5e810722":"code","1b3d694a":"code","7b7bc1a3":"code","20531f2f":"code","a9150337":"code","2868fd8c":"code","529ef75a":"code","8cd4aee1":"code","1a1d4afc":"code","f5f38820":"code","fccc9cd1":"code","1742d8a8":"code","b292c4e7":"code","a1de6524":"code","d5dd2b69":"code","1740841c":"code","934760bd":"code","4139fe60":"code","aa038f33":"code","579df410":"code","a18fbcb3":"code","3e90f80b":"code","82881321":"code","35d14724":"code","09f21ccc":"code","be80c1ab":"code","35e5912a":"markdown","92bd3e93":"markdown","1e875048":"markdown","0a7fc19b":"markdown","7dfc8dc7":"markdown","2c95a0c7":"markdown","e55cc403":"markdown","f75d7933":"markdown","cca55bd6":"markdown","44328b79":"markdown","84db341f":"markdown","10087da5":"markdown","6c0dfa98":"markdown","593ea0d1":"markdown","f3d5736a":"markdown","c0272d93":"markdown","879b413e":"markdown","7a20c283":"markdown","62e376fd":"markdown","9e4af83e":"markdown"},"source":{"2c258dcd":"#importing important libraries\nimport pandas as pd  #for data analysis\nimport numpy as np   #for linear algebra\nimport matplotlib.pyplot as plt   #for visulaization\nimport seaborn as sns\n%matplotlib inline","84261ab8":"#importing dataset\ndata=pd.read_csv('..\/input\/-social-networking-ads\/Social_Network_Ads.csv')","f2613520":"# displaying all the features of the data\ndata.columns","be3a2a0c":"data.head()","9e7cc095":"#Checking the shape of the dataset\ndata.shape ","8b35adc8":"#checking the information of the Data\ndata.info()","dc2bd77a":"#Checking if there is any missing Value present in the dataset\ndata.isnull().sum()\n#There is no missing value present in the dataset","eddf6b16":"#Checking the Datatype of the features\ndata.dtypes","4b25b080":"#Information About the Data\ndata.describe()","6374699f":"data['Purchased'].value_counts()","ada74857":"data['Purchased'].value_counts().plot.bar()","2334f5b7":"sns.distplot(data['Age'])","62e1b8ac":"sns.distplot(data['EstimatedSalary'])\n#EstimatedSalary is normally distributed","22a7edb3":"plt.boxplot(data['EstimatedSalary'])\n#Boxplot help us to check if there is any outlier present in the feature\n#There is no outlier present in the EstimatedSalary Feature","7319df5b":"plt.boxplot(data['Age'])\n#There is no outlier present in the Age feature as well","d7397db6":"plt.boxplot(data['User ID'])","b97635e7":"#Scatter Plot of estimatedsalary and Dependent variable purchased\nsns.scatterplot(x='EstimatedSalary',y='Purchased',data=data)","eda15cc7":"sns.scatterplot(x='Age',y='Purchased',data=data)","43c76d85":"sns.scatterplot(x='User ID',y='Purchased',data=data)","bb3685a0":"#We have one Categorical Variable as well\ndata.Gender.unique()","5a7ddb52":"df1=pd.get_dummies(data=data)\n#Here we change the categorical variable into continous variable","b846f89b":"#Here we change the categorical variable Gender into continous variables Gender_Male and Gender_female\ndf1.head()","29694953":"#Correlation between different features of our dataset\nsns.heatmap(df1.corr(),annot=True)","220198c7":"#The dependent variable Purchased is very less correlated with User ID so we drop that feature\n#We creates dummy variables so we drop one \ndf2=df1.drop('User ID',axis=1)\ndf2=df2.drop('Gender_Male',axis=1)","5e96776d":"df2.head()","9cd5613c":"#Separating Depenent and Independent Features\nX=df2.iloc[:,[0,1,3]]\nY=df2.iloc[:,2]","8170cb40":"X.head()","5e810722":"Y.head()","1b3d694a":"#Checking the shape of independent and dependent variables\nprint(\"Shape of Independent features:\",X.shape)\nprint(\"Shape of dependent feature:\",Y.shape)","7b7bc1a3":"plt.title(\"Correlation matrix\")\nsns.heatmap(df2.corr(),annot=True)\n","20531f2f":"#Splitting the Data into training and test dataset\nfrom sklearn.model_selection import train_test_split","a9150337":"X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=2)","2868fd8c":"print(\"Shape of X_train:\",X_train.shape)\nprint(\"Shape pf X_test:\",X_test.shape)\nprint(\"shape of Y_train:\",Y_train.shape)\nprint(\"Shape of Y_test:\",Y_test.shape)","529ef75a":"#Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nXX_train=sc.fit_transform(X_train)\nXX_test=sc.transform(X_test)","8cd4aee1":"from sklearn.linear_model import LogisticRegression\nloreg=LogisticRegression()","1a1d4afc":"#Fitting logistic Regression to the training set\nloreg.fit(XX_train,Y_train)","f5f38820":"#Score of the training set\nloreg.score(XX_train,Y_train)","fccc9cd1":"#Predicting the test Dataset\npred=loreg.predict(XX_test)","1742d8a8":"# Accuracy Score of test set\nfrom sklearn.metrics import accuracy_score\naccuracy_score(Y_test,pred)","b292c4e7":"inter=loreg.intercept_\nprint(inter)","a1de6524":"#Coefficients of regression model\ncoeff=loreg.coef_\nprint(coeff)","d5dd2b69":"#Making Confusion Matrix\nfrom sklearn import metrics\ncm=metrics.confusion_matrix(Y_test,pred)\nprint(cm)","1740841c":"TP=48\nFP=12\nTN=20\nFN=3\nacc=(TP+TN)\/(TP+TN+FP+FN)\nrc=TP\/(TP+FN)\npre=TP\/(TP+FP)","934760bd":"#Printing Accuracy,Recall and Precision\n\nprint(acc)\nprint(rc)\nprint(pre)","4139fe60":"f_measure=(2*rc*pre)\/(rc+pre)\nprint(f_measure)","aa038f33":"#Importing support vector classifier\nfrom sklearn.svm import SVC\nsvc=SVC()","579df410":"#fitting the training set \nsvc.fit(XX_train,Y_train)","a18fbcb3":"#predicting the test dataset\npred1=svc.predict(XX_test)","3e90f80b":"#accuracy of training dataset\nsvc.score(XX_train,Y_train)","82881321":"from sklearn.metrics import accuracy_score\naccuracy_score(Y_test,pred1)","35d14724":"from sklearn import metrics","09f21ccc":"#Making Confusion matrix\ncm1=metrics.confusion_matrix(Y_test,pred1)\nprint(cm1)","be80c1ab":"TP=45\nFP=4\nFN=3\nTN=28\nacc=(TP+TN)\/(TP+TN+FP+FN)\nrc=TP\/(TP+FN)\npre=TP\/(TP+FP)\nprint(acc)\nprint(rc)\nprint(pre)","35e5912a":"We have two measures(Recall and precision) f-measure helps to have a measurement that represents both of them.","92bd3e93":"**We see that there are four continous type variables and one categorical type variable 'Gender'.**","1e875048":"**Univariate analysis**","0a7fc19b":"Out of 400,143 people have purchased the advertised product which is around 35% of the total. ","7dfc8dc7":"* High Recall indicates the class is correctly recognised.\n* High Precision indicates an example labeled as positive is indeed positive.","2c95a0c7":"The score of training Dataset is 0.84375 whic is a good score.","e55cc403":"Bivariate Analysis","f75d7933":"**Goal:-**\nTo Predict the users on the social network who on interacting wiht the advertisements either purchased the product or not.","cca55bd6":"* Confusion Matrix is a performance measurement for machine learning Classification where output can be two or more classes.\n* It gives us insight not only into the errors being made by a classifier but also the type of erroe being made.","44328b79":"* Accuracy of the model is 0.9125\n* Recall of the model is 0.9375\n* Precision is 0.91836","84db341f":"Model Building","10087da5":"* TP- observation is positive and predicted to be positive.\n* FN- observation is positive and predicted to be negative.\n* TN- observation is negative and predicted to be negative.\n* FP- observation is negative and predicted to be positive.","6c0dfa98":"This gives us the describtion about the continous datatype variables. ","593ea0d1":"This is good accuracy score for the test Dataset","f3d5736a":"1) We infer from this correlation matrix that the dependent variable Purchased is most correlated with the variable Age and then with variable EstimatedSalary.                                                                            \n2) Dependent variable \"Purchased\" is least correlated with UserId ,so we can drop the feature UserID for model building.       \n3) We also observed that there is no multicollinearity between the independent features which is good for model building.","c0272d93":"We infer that maximum of the people lie in the age group between 25-45 and this variable is normally distributed.","879b413e":"Logistic Regression Model","7a20c283":"Support vector Machine","62e376fd":"Score of test datset is 0.8125 which is good enough.","9e4af83e":"----------------**CLASSIFICATION PROBLEM**-----------------"}}