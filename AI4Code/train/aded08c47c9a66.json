{"cell_type":{"db9f3ee5":"code","4f1068b0":"code","d031e9ec":"code","0b81dd26":"code","d3072bc1":"code","8877e0ac":"code","cc7764f2":"code","1224642a":"code","5b52ae17":"code","29e0dff8":"code","1c5c6f6e":"code","d6b96acb":"code","30503ee8":"code","5b4276d1":"code","558d1642":"code","c4694c4e":"code","b1e9f683":"code","cbf55f6d":"code","de140a19":"code","a3c8c10b":"code","e8d36709":"code","f784c629":"code","85081ee3":"code","2411d737":"code","cc1e5ebf":"code","8e079028":"code","422a9e5a":"code","33d51098":"code","c92f0973":"code","cb01df54":"code","132d7e75":"code","2b4842a2":"code","33377b6a":"code","b5f10f68":"code","69b04517":"code","00ca31d8":"code","9f2cdcc0":"code","4bd3af7d":"code","3a34c189":"code","2312e4e1":"code","5e6f0bb2":"code","f9f633a5":"code","187e4a5b":"code","d824772b":"code","b566afc6":"code","64734e9a":"code","5ddf81b7":"code","2283cf0a":"code","d2e25839":"code","8fceb5f0":"code","dadcb7e4":"code","a3322f1e":"code","413de3ad":"code","4b5268a5":"code","d40a4e8a":"code","458b90ae":"code","0f1e76b4":"code","28a82e3b":"code","1b6974f3":"code","43a80442":"code","5a1a4c08":"code","ac69cb36":"code","51a6c453":"code","80470c0e":"code","d3cf53a7":"code","28180808":"code","130245b0":"code","cd79717e":"code","ef533389":"code","d13e6ac7":"code","8bb1f74a":"code","9e9b036b":"code","d6eb1e2d":"code","a1bc1403":"code","09425339":"code","c5f086bc":"code","fec06817":"code","f8ada441":"code","4cb690cb":"code","b06ded42":"code","ac270e27":"code","a38b4731":"code","9c6437fc":"code","4fc9ca9c":"code","eb19a3e3":"code","cd9a2368":"code","655f5141":"code","5ae35092":"markdown","99e688cd":"markdown","aa70034e":"markdown","5ce39009":"markdown","e33ac03b":"markdown","69d9556a":"markdown","3b1aafc3":"markdown","f6a64bdc":"markdown","db62035c":"markdown","ebf2d178":"markdown","1aa33611":"markdown","7e36a417":"markdown","e51c4ab0":"markdown","b495e63f":"markdown","41cb23b0":"markdown","31093e82":"markdown","6f507916":"markdown","4a42d073":"markdown","a7afca75":"markdown","eeb5ad3f":"markdown"},"source":{"db9f3ee5":"FINAL_COUNT= 1","4f1068b0":"!conda install '\/kaggle\/input\/pydicom-conda-helper\/libjpeg-turbo-2.1.0-h7f98852_0.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/libgcc-ng-9.3.0-h2828fa1_19.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/gdcm-2.8.9-py37h500ead1_1.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/conda-4.10.1-py37h89c1867_0.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/certifi-2020.12.5-py37h89c1867_1.tar.bz2' -c conda-forge -y\n!conda install '\/kaggle\/input\/pydicom-conda-helper\/openssl-1.1.1k-h7f98852_0.tar.bz2' -c conda-forge -y\n\nFINAL_COUNT += 1","d031e9ec":"!pip install --no-deps '..\/input\/pycocotools\/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl' > \/dev\/null\n\nFINAL_COUNT += 1","0b81dd26":"package_paths = [\n    '..\/input\/weighted-boxes-fusion\/Weighted-Boxes-Fusion-master',\n    '..\/input\/pytorchimagemodels\/pytorch-image-models-master', #'..\/input\/efficientnet-pytorch-07\/efficientnet_pytorch-0.7.0'\n    \"..\/input\/timmeffdetclsv2\/timm-efficientdet-pytorch\",\n    \"..\/input\/omegaconf\"\n#     '..\/input\/image-fmix\/FMix-master'\n]\nimport sys; \n\nfor pth in package_paths:\n    sys.path.append(pth)\n    \nFINAL_COUNT += 1","d3072bc1":"import os\n\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom ensemble_boxes import *\nimport matplotlib.pyplot as plt\n\nFINAL_COUNT += 1","8877e0ac":"df = pd.read_csv('..\/input\/siim-covid19-detection\/sample_submission.csv')\nif df.shape[0] == 2477:\n    fast_sub = True\n    fast_df = pd.DataFrame(([['00086460a852_study', 'negative 1 0 0 1 1'], \n                         ['000c9c05fd14_study', 'negative 1 0 0 1 1'], \n                         ['65761e66de9f_image', 'none 1 0 0 1 1'], \n                         ['51759b5579bc_image', 'none 1 0 0 1 1']]), \n                       columns=['id', 'PredictionString'])\nelse:\n    fast_sub = False\n\nFINAL_COUNT += 1","cc7764f2":"import numpy as np\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n\ndef read_xray(path, voi_lut = True, fix_monochrome = True):\n    # Original from: https:\/\/www.kaggle.com\/raddar\/convert-dicom-to-np-array-the-correct-way\n    dicom = pydicom.read_file(path)\n    \n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \n    # \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n               \n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n        \n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n        \n    return data\n\nFINAL_COUNT += 1","1224642a":"def resize(array, size, keep_ratio=False, resample=Image.LANCZOS):\n    # Original from: https:\/\/www.kaggle.com\/xhlulu\/vinbigdata-process-and-resize-to-image\n    im = Image.fromarray(array)\n    \n    if keep_ratio:\n        im.thumbnail((size, size), resample)\n    else:\n        im = im.resize((size, size), resample)\n    \n    return im\n\nFINAL_COUNT += 1","5b52ae17":"split = 'test'\nsave_dir = f'\/kaggle\/tmp\/{split}\/'\n\nos.makedirs(save_dir, exist_ok=True)\n\nsave_dir = f'\/kaggle\/tmp\/{split}\/study\/'\nos.makedirs(save_dir, exist_ok=True)\nif fast_sub:\n    xray = read_xray('..\/input\/siim-covid19-detection\/train\/00086460a852\/9e8302230c91\/65761e66de9f.dcm')\n    im = resize(xray, size=640)  \n    study = '00086460a852' + '_study.png'\n    im.save(os.path.join(save_dir, study))\n    xray = read_xray('..\/input\/siim-covid19-detection\/train\/000c9c05fd14\/e555410bd2cd\/51759b5579bc.dcm')\n    im = resize(xray, size=640)  \n    study = '000c9c05fd14' + '_study.png'\n    im.save(os.path.join(save_dir, study))\nelse:   \n    for dirname, _, filenames in tqdm(os.walk(f'..\/input\/siim-covid19-detection\/{split}')):\n        for file in filenames:\n            # set keep_ratio=True to have original aspect ratio\n            xray = read_xray(os.path.join(dirname, file))\n            im = resize(xray, size=640)  \n            study = dirname.split('\/')[-2] + '_study.png'\n            im.save(os.path.join(save_dir, study))\n\nFINAL_COUNT += 1","29e0dff8":"image_id = []\ndim0 = []\ndim1 = []\nsplits = []\nsave_dir = f'\/kaggle\/tmp\/{split}\/image\/'\nos.makedirs(save_dir, exist_ok=True)\nif fast_sub:\n    xray = read_xray('..\/input\/siim-covid19-detection\/train\/00086460a852\/9e8302230c91\/65761e66de9f.dcm')\n    im = resize(xray, size=640)  \n    im.save(os.path.join(save_dir,'65761e66de9f_image.png'))\n    image_id.append('65761e66de9f.dcm'.replace('.dcm', ''))\n    dim0.append(xray.shape[0])\n    dim1.append(xray.shape[1])\n    splits.append(split)\n    xray = read_xray('..\/input\/siim-covid19-detection\/train\/000c9c05fd14\/e555410bd2cd\/51759b5579bc.dcm')\n    im = resize(xray, size=640)  \n    im.save(os.path.join(save_dir, '51759b5579bc_image.png'))\n    image_id.append('51759b5579bc.dcm'.replace('.dcm', ''))\n    dim0.append(xray.shape[0])\n    dim1.append(xray.shape[1])\n    splits.append(split)\nelse:\n    for dirname, _, filenames in tqdm(os.walk(f'..\/input\/siim-covid19-detection\/{split}')):\n        for file in filenames:\n            # set keep_ratio=True to have original aspect ratio\n            xray = read_xray(os.path.join(dirname, file))\n            im = resize(xray, size=640)  \n            im.save(os.path.join(save_dir, file.replace('.dcm', '_image.png')))\n            image_id.append(file.replace('.dcm', ''))\n            dim0.append(xray.shape[0])\n            dim1.append(xray.shape[1])\n            splits.append(split)\nmeta = pd.DataFrame.from_dict({'image_id': image_id, 'dim0': dim0, 'dim1': dim1, 'split': splits})\n\nFINAL_COUNT += 1","1c5c6f6e":"from glob import glob\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\nimport cv2\nfrom skimage import io\nimport torch\nfrom torch import nn\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport torchvision\nfrom torchvision import transforms\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom  torch.cuda.amp import autocast, GradScaler\n\nimport sklearn\nimport warnings\nimport joblib\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn import metrics\nimport warnings\nimport cv2\nimport pydicom\nimport timm #from efficientnet_pytorch import EfficientNet\nfrom scipy.ndimage.interpolation import zoom\nfrom sklearn.metrics import log_loss\n\nFINAL_COUNT += 1","d6b96acb":"CFG = {\n    'fold_num': 5,\n    'seed': 2011,\n    'model_arch': 'tf_efficientnetv2_m_in21k',\n    'vit_model_arch': 'vit_deit_base_patch16_384',\n    'img_size': 640,\n    'epochs': 10,\n    'train_bs': 32,\n    'valid_bs': 32,\n    'lr': 1e-4,\n    'num_workers': 4,\n    'accum_iter': 1, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n    'verbose_step': 1,\n    'device': 'cuda:0',\n    'tta': 3,\n    'model_paths': ['..\/input\/siimcovid19-pseudo-labeling-classification\/tf_efficientnetv2_m_in21k_fold0_epoch17_colab_pseudo_labeling_v5.pt',\n                   '..\/input\/siimcovid19-classification-models\/tf_efficientnetv2_m_in21k_fold1_epoch20_colab_uda_v8.pt',\n                   '..\/input\/siimcovid19-pseudo-labeling-classification\/tf_efficientnetv2_m_in21k_fold2_epoch6_colab_pseudo_labeling_v7.pt',\n                   '..\/input\/siimcovid19-pseudo-labeling-classification\/tf_efficientnetv2_m_in21k_fold3_epoch15_colab_pseudo_labeling_v8.pt',\n                   '..\/input\/siimcovid19-pseudo-labeling-classification\/tf_efficientnetv2_m_in21k_fold4_epoch26_colab_pseudo_labeling_v9.pt',\n                   '..\/input\/siimcovid19-vision-transformer-model\/vit_deit_base_patch16_384_fold0_epoch10_colab_vit_v7.pt',\n                   '..\/input\/siimcovid19-vision-transformer-model\/vit_deit_base_patch16_384_fold1_epoch9_colab_vit_v9.pt',\n                   '..\/input\/siimcovid19-vision-transformer-model\/vit_deit_base_patch16_384_fold2_epoch14_colab_vit_v11.pt',\n                   '..\/input\/siimcovid19-vision-transformer-model\/vit_deit_base_patch16_384_fold3_epoch6_colab_vit_v12.pt',\n                   '..\/input\/siimcovid19-vision-transformer-model\/vit_deit_base_patch16_384_fold4_epoch13_colab_vit_v13.pt'],\n    'num_classes': 4,\n    'weights': [38, 38, 38, 38, 38, 37, 37, 37, 37, 37]\n}\n\nFINAL_COUNT += 1","30503ee8":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \ndef get_img(path):\n    im_bgr = cv2.imread(path)\n    im_rgb = im_bgr[:, :, ::-1]\n    #print(im_rgb)\n    return im_rgb\n\nFINAL_COUNT += 1","5b4276d1":"class ChestXRayDataset(Dataset):\n    def __init__(\n        self, df, data_root, transforms=None, output_label=True\n    ):\n        \n        super().__init__()\n        self.df = df.reset_index(drop=True).copy()\n        self.transforms = transforms\n        self.data_root = data_root\n        self.output_label = output_label\n    \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, index: int):\n        \n        # get labels\n        if self.output_label:\n            target = self.df.iloc[index]['target']\n        \n        study_id = self.df.loc[index]['image_id']\n        image_path = \"{}\/{}.png\".format(self.data_root, study_id)\n        if (image_path.find('.png') == -1):\n            image_path = image_path + '.png'\n        \n        img = get_img(image_path)\n        \n        if self.transforms:\n            img = self.transforms(image=img)['image']\n            \n        # do label smoothing\n        if self.output_label == True:\n            return img, target\n        else:\n            return img\n\nFINAL_COUNT += 1","558d1642":"from albumentations import (\n    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n)\n\nfrom albumentations.pytorch import ToTensorV2\n\ndef get_train_transforms():\n    return Compose([\n            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            #VerticalFlip(p=0.5),\n            ShiftScaleRotate(p=0.5),\n            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            #CoarseDropout(p=0.5),\n            #Cutout(p=0.5),\n            ToTensorV2(p=1.0),\n        ], p=1.)\n  \n        \ndef get_valid_transforms():\n    return Compose([\n            CenterCrop(CFG['img_size'], CFG['img_size'], p=1.),\n            Resize(CFG['img_size'], CFG['img_size']),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.)\n\ndef get_inference_transforms():\n    return Compose([\n            RandomResizedCrop(CFG['img_size'], CFG['img_size'], scale=(0.8, 1.0)),\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.)\n\n# def get_inference_transforms():\n#     return Compose([\n#             Resize(CFG['img_size'], CFG['img_size']),\n#             Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n#             ToTensorV2(p=1.0),\n#         ], p=1.)\n\nFINAL_COUNT += 1","c4694c4e":"class ChestXrayImgClassifier(nn.Module):\n    def __init__(self, model_arch, n_class, pretrained=False):\n        super().__init__()\n        # tf_efficientnet_b6_ns\n        self.model = timm.create_model(model_arch, pretrained=pretrained, drop_rate=0.5, drop_path_rate=0.2)\n        n_features = self.model.classifier.in_features\n        self.model.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(n_features, n_class, bias=True)\n        )\n        \n    def forward(self, x):\n        x = self.model(x)\n        return x\n    \nFINAL_COUNT += 1","b1e9f683":"class ChestXrayImgClassifierVIT(nn.Module):\n    def __init__(self, model_arch, n_class, pretrained=False):\n        super().__init__()\n        # vit_base_patch16\n        self.model = timm.create_model(model_arch, pretrained=pretrained, img_size=640)\n        n_features = self.model.head.in_features\n        self.model.head = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(n_features, n_class, bias=True)\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n    \nFINAL_COUNT += 1","cbf55f6d":"import numpy as np \nimport pandas as pd\n\nif fast_sub:\n    df = fast_df.copy()\nelse:\n    df = pd.read_csv('..\/input\/siim-covid19-detection\/sample_submission.csv')\nid_laststr_list  = []\nfor i in range(df.shape[0]):\n    id_laststr_list.append(df.loc[i,'id'][-1])\ndf['id_last_str'] = id_laststr_list\n\nstudy_len = df[df['id_last_str'] == 'y'].shape[0]\n\nFINAL_COUNT += 1","de140a19":"FINAL_COUNT += 1\ndf","a3c8c10b":"study_lst = df[df['id_last_str'] == 'y']['id'].values\n\nFINAL_COUNT += 1\nstudy_lst","e8d36709":"def inference_one_epoch(model, data_loader, device):\n    model.eval()\n\n    image_preds_all = []\n    \n    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n    for step, (imgs) in pbar:\n        imgs = imgs.to(device).float()\n        \n        image_preds = model(imgs)   #output = model(input)\n        image_preds_all += [torch.softmax(image_preds, 1).detach().cpu().numpy()]\n        \n    \n    image_preds_all = np.concatenate(image_preds_all, axis=0)\n    return image_preds_all\n\nFINAL_COUNT += 1","f784c629":"fold = 0\ntest_dataset_dir = f'\/kaggle\/tmp\/{split}\/study'\n # for training only, need nightly build pytorch\n\nseed_everything(CFG['seed'])\n\nprint('Inference fold {} started'.format(fold))\n\ntest = pd.DataFrame()\ntest['image_id'] = study_lst\ntest_ds = ChestXRayDataset(test, test_dataset_dir, transforms=get_inference_transforms(), output_label=False)\n\ntst_loader = torch.utils.data.DataLoader(\n    test_ds, \n    batch_size=CFG['valid_bs'],\n    num_workers=CFG['num_workers'],\n    shuffle=False,\n    pin_memory=False,\n)\n\ndevice = torch.device(CFG['device'])\nmodel = ChestXrayImgClassifier(CFG['model_arch'], CFG['num_classes']).to(device)\n\ntst_preds = []\n\nfor i, model_path in enumerate(CFG['model_paths']):\n    if i > 4:\n        break\n    model.load_state_dict(torch.load(model_path))\n\n    with torch.no_grad():\n        for _ in range(CFG['tta']):\n            infer_one = inference_one_epoch(model, tst_loader, device)\n    #         print(infer_one)\n            tst_preds += [CFG['weights'][i]\/sum(CFG['weights'])\/CFG['tta']*infer_one]\n\n# print('tst_preds')\n# print(tst_preds[:10])\n# print(len(tst_preds))\n# tst_preds = np.sum(tst_preds, axis=0)\n# print(tst_preds[:10])\n# print(len(tst_preds))\n\ndel model\ntorch.cuda.empty_cache()\n\nFINAL_COUNT += 1","85081ee3":"model_vit = ChestXrayImgClassifierVIT(CFG['vit_model_arch'], CFG['num_classes']).to(device)\nVIT_model_paths = CFG['model_paths'][5:]\n\nfor i, model_path in enumerate(VIT_model_paths):\n    model_vit.load_state_dict(torch.load(model_path))\n\n    with torch.no_grad():\n        for _ in range(CFG['tta']):\n            infer_one = inference_one_epoch(model_vit, tst_loader, device)\n    #         print(infer_one)\n            tst_preds += [CFG['weights'][i+5]\/sum(CFG['weights'])\/CFG['tta']*infer_one]\n\ntst_preds = np.sum(tst_preds, axis=0)\ndel model_vit\ntorch.cuda.empty_cache()\n\nFINAL_COUNT += 1","2411d737":"print(len(tst_preds))\n\nFINAL_COUNT += 1","cc1e5ebf":"FINAL_COUNT += 1\ntst_preds[:10]","8e079028":"debug = False\nclass_labels = ['0', '1', '2', '3']\n\ntest.loc[:99 if debug else test.shape[0], class_labels] = tst_preds\n\nFINAL_COUNT += 1\ntest.head()","422a9e5a":"name2label = { \n    'negative': 0,\n    'indeterminate': 1,\n    'typical': 2,\n    'atypical': 3}\nlabel2name  = {v:k for k, v in name2label.items()}\n\nFINAL_COUNT += 1","33d51098":"def get_PredictionString(row, thr=0):\n    string = ''\n    for idx in range(4):\n        conf =  row[str(idx)]\n        if conf>thr:\n            string+=f'{label2name[idx]} {conf:0.3f} 0 0 1 1 '\n    string = string.strip()\n    return string\n\nFINAL_COUNT += 1","c92f0973":"test['PredictionString'] = test.apply(get_PredictionString, axis=1)\ntest = test.drop(class_labels, axis=1)\ntest.rename(columns={'image_id':'id'}, inplace=True)\n\nFINAL_COUNT += 1\ntest.head()","cb01df54":"df_study = test\n\nFINAL_COUNT += 1\ndf_study","132d7e75":"package_paths = [\n    '..\/input\/pytorchimagemodels\/pytorch-image-models-master', #'..\/input\/efficientnet-pytorch-07\/efficientnet_pytorch-0.7.0'\n#     '..\/input\/image-fmix\/FMix-master'\n]\nimport sys; \n\nfor pth in package_paths:\n    sys.path.append(pth)\n    \nFINAL_COUNT += 1","2b4842a2":"from glob import glob\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\nimport cv2\nfrom skimage import io\nimport torch\nfrom torch import nn\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport torchvision\nfrom torchvision import transforms\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom  torch.cuda.amp import autocast, GradScaler\n\nimport sklearn\nimport warnings\nimport joblib\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn import metrics\nimport warnings\nimport cv2\nimport pydicom\nimport timm #from efficientnet_pytorch import EfficientNet\nfrom scipy.ndimage.interpolation import zoom\nfrom sklearn.metrics import log_loss\n\nFINAL_COUNT += 1","33377b6a":"CFG = {\n    'fold_num': 5,\n    'seed': 2021,\n    'model_arch': 'tf_efficientnetv2_m_in21k',\n    'vit_model_arch': 'vit_deit_base_patch16_384',\n    'img_size': 640,\n    'epochs': 10,\n    'train_bs': 32,\n    'valid_bs': 32,\n    'lr': 1e-4,\n    'num_workers': 4,\n    'accum_iter': 1, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n    'verbose_step': 1,\n    'device': 'cuda:0',\n    'tta': 3,\n    'model_paths': ['..\/input\/siimcovid192classes-classifier\/tf_efficientnetv2_m_in21k_fold0_epoch1_colab_v1.pt',\n                   '..\/input\/siimcovid192classes-classifier\/tf_efficientnetv2_m_in21k_fold1_epoch20_colab_v2.pt',\n                   '..\/input\/siimcovid192classes-classifier\/tf_efficientnetv2_m_in21k_fold2_epoch14_colab_v3.pt',\n                   '..\/input\/siimcovid192classes-classifier\/tf_efficientnetv2_m_in21k_fold3_epoch13_colab_v4.pt',\n                   '..\/input\/siimcovid192classes-classifier\/tf_efficientnetv2_m_in21k_fold4_epoch5_colab_v5.pt',\n                   '..\/input\/siimcovid192classes-classifier\/vit_deit_base_patch16_384_fold0_epoch8_colab_v6.pt',\n                   '..\/input\/siimcovid192classes-classifier\/vit_deit_base_patch16_384_fold1_epoch8_colab_v7.pt',\n                   '..\/input\/siimcovid192classes-classifier\/vit_deit_base_patch16_384_fold2_epoch9_colab_v8.pt'],\n    'num_classes': 2,\n    'weights': [59, 59, 59, 59, 59, 58, 58, 58]\n}\n\nFINAL_COUNT += 1","b5f10f68":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \ndef get_img(path):\n    im_bgr = cv2.imread(path)\n    im_rgb = im_bgr[:, :, ::-1]\n    return im_rgb\n\nFINAL_COUNT += 1","69b04517":"from albumentations import (\n    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n)\n\nfrom albumentations.pytorch import ToTensorV2\n\ndef get_train_transforms():\n    return Compose([\n            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            #VerticalFlip(p=0.5),\n            ShiftScaleRotate(p=0.5),\n            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            #CoarseDropout(p=0.5),\n            #Cutout(p=0.5),\n            ToTensorV2(p=1.0),\n        ], p=1.)\n  \n        \ndef get_valid_transforms():\n    return Compose([\n            CenterCrop(CFG['img_size'], CFG['img_size'], p=1.),\n            Resize(CFG['img_size'], CFG['img_size']),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.)\n\ndef get_inference_transforms():\n    return Compose([\n            RandomResizedCrop(CFG['img_size'], CFG['img_size'], scale=(0.8, 1.0)),\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.)\n\n# def get_inference_transforms():\n#     return Compose([\n#             Resize(CFG['img_size'], CFG['img_size']),\n#             Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n#             ToTensorV2(p=1.0),\n#         ], p=1.)\n\nFINAL_COUNT += 1","00ca31d8":"class ChestXrayImgClassifier(nn.Module):\n    def __init__(self, model_arch, n_class, pretrained=False):\n        super().__init__()\n        # tf_efficientnet_b6_ns\n        self.model = timm.create_model(model_arch, pretrained=pretrained, drop_rate=0.3, drop_path_rate=0.2)\n        n_features = self.model.classifier.in_features\n        self.model.classifier = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(n_features, n_class, bias=True)\n        )\n        \n    def forward(self, x):\n        x = self.model(x)\n        return x\n    \nFINAL_COUNT += 1","9f2cdcc0":"def inference_one_epoch(model, data_loader, device):\n    model.eval()\n\n    image_preds_all = []\n    \n    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n    for step, (imgs) in pbar:\n        imgs = imgs.to(device).float()\n        \n        image_preds = model(imgs)   #output = model(input)\n        image_preds_all += [torch.softmax(image_preds, 1).detach().cpu().numpy()]\n        \n    \n    image_preds_all = np.concatenate(image_preds_all, axis=0)\n    return image_preds_all\n\nFINAL_COUNT += 1","4bd3af7d":"class ChestXRayDataset(Dataset):\n    def __init__(\n        self, df, data_root, transforms=None, output_label=True\n    ):\n        \n        super().__init__()\n        self.df = df.reset_index(drop=True).copy()\n        self.transforms = transforms\n        self.data_root = data_root\n        self.output_label = output_label\n    \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, index: int):\n        \n        # get labels\n        if self.output_label:\n            target = self.df.iloc[index]['target']\n        \n        image_id = self.df.loc[index]['id']\n        image_path = \"{}\/{}\".format(self.data_root, image_id)\n        if (image_path.find('.png') == -1):\n            image_path = image_path + '.png'\n        img = get_img(image_path)\n        \n        if self.transforms:\n            img = self.transforms(image=img)['image']\n            \n        # do label smoothing\n        if self.output_label == True:\n            return img, target\n        else:\n            return img\n        \n        \nFINAL_COUNT += 1","3a34c189":"if fast_sub:\n    sub_df = fast_df.copy()\nelse:\n    sub_df = pd.read_csv('..\/input\/siim-covid19-detection\/sample_submission.csv')\nsub_df = sub_df[study_len:]\ntest_paths = f'\/kaggle\/tmp\/{split}\/image\/' + sub_df['id'] +'.png'\nsub_df['none'] = 0\n\nFINAL_COUNT += 1\nsub_df","2312e4e1":"fold = 0\ntest_dataset_dir = f'\/kaggle\/tmp\/{split}\/image'\n # for training only, need nightly build pytorch\n\nseed_everything(CFG['seed'])\n\nprint('Inference fold {} started'.format(fold))\n\ntest = pd.DataFrame()\ntest_ds = ChestXRayDataset(sub_df, test_dataset_dir, transforms=get_inference_transforms(), output_label=False)\n\ntst_loader = torch.utils.data.DataLoader(\n    test_ds, \n    batch_size=CFG['valid_bs'],\n    num_workers=CFG['num_workers'],\n    shuffle=False,\n    pin_memory=False,\n)\n\ndevice = torch.device(CFG['device'])\nmodel = ChestXrayImgClassifier(CFG['model_arch'], CFG['num_classes']).to(device)\n\ntst_preds = []\n\nfor i, model_path in enumerate(CFG['model_paths'][0:5]):\n    model.load_state_dict(torch.load(model_path))\n\n    with torch.no_grad():\n        for _ in range(CFG['tta']):\n            infer_one = inference_one_epoch(model, tst_loader, device)\n    #         print(infer_one)\n            tst_preds += [CFG['weights'][i]\/sum(CFG['weights'])\/CFG['tta']*infer_one]\n\n# tst_preds = np.sum(tst_preds, axis=0)\n\ndel model\ntorch.cuda.empty_cache()\n\nFINAL_COUNT += 1","5e6f0bb2":"model_vit = ChestXrayImgClassifierVIT(CFG['vit_model_arch'], CFG['num_classes']).to(device)\nVIT_model_paths = CFG['model_paths'][5:]\n\nfor i, model_path in enumerate(VIT_model_paths):\n    model_vit.load_state_dict(torch.load(model_path))\n\n    with torch.no_grad():\n        for _ in range(CFG['tta']):\n            infer_one = inference_one_epoch(model_vit, tst_loader, device)\n    #         print(infer_one)\n            tst_preds += [CFG['weights'][i+5]\/sum(CFG['weights'])\/CFG['tta']*infer_one]\n\ntst_preds = np.sum(tst_preds, axis=0)\ndel model_vit\ntorch.cuda.empty_cache()\n\nFINAL_COUNT += 1","f9f633a5":"FINAL_COUNT += 1\ntst_preds","187e4a5b":"debug = False\nclass_labels = ['none']\n\nsub_df[class_labels] = tst_preds[:, 1]\n\nFINAL_COUNT += 1\nsub_df.head()","d824772b":"df_2class = sub_df.reset_index(drop=True)\n\nFINAL_COUNT += 1\ndf_2class.head()","b566afc6":"%load_ext autoreload\n%autoreload 2\n\nFINAL_COUNT += 1","64734e9a":"%cd \/kaggle\/working\/\n\nFINAL_COUNT += 1","5ddf81b7":"from ensemble_boxes import *\nimport torch\nimport os\nfrom datetime import datetime\nimport time\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom glob import glob\nimport gc\n\nSEED = 42\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)\n\nFINAL_COUNT += 1","2283cf0a":"meta_copy = meta.copy()\n\nFINAL_COUNT += 1\nmeta_copy","d2e25839":"meta = meta_copy.copy()\nmeta = meta[meta['split'] == 'test']\nif fast_sub:\n    test_df = fast_df.copy()\nelse:\n    test_df = pd.read_csv('..\/input\/siim-covid19-detection\/sample_submission.csv')\ntest_df = df[study_len:].reset_index(drop=True) \nmeta['image_id'] = meta['image_id'] + '_image'\nmeta.columns = ['id', 'dim0', 'dim1', 'split']\n# print(test_df)\ntest_df = pd.merge(test_df, meta, on = 'id', how = 'left')\n\nFINAL_COUNT += 1","8fceb5f0":"FINAL_COUNT += 1\ntest_df","dadcb7e4":"def get_valid_transforms_effdet():\n    return A.Compose(\n        [\n            A.Resize(height=640, width=640, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.0)\n\nFINAL_COUNT += 1","a3322f1e":"DATA_ROOT_PATH = f'\/kaggle\/tmp\/{split}\/image\/'\n\nclass DatasetRetriever(Dataset):\n    def __init__(self, image_ids, transforms=None):\n        super().__init__()\n        self.image_ids = image_ids\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        image = cv2.imread(f'{DATA_ROOT_PATH}\/{image_id}.png', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        if self.transforms:\n            sample = {'image': image}\n            sample = self.transforms(**sample)\n            image = sample['image']\n        return image, image_id\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n    \nFINAL_COUNT += 1","413de3ad":"validation_dataset = DatasetRetriever(\n    image_ids=meta[\"id\"].values,\n    transforms=get_valid_transforms_effdet()\n)\n\nFINAL_COUNT += 1","4b5268a5":"import warnings\nimport matplotlib.pyplot as plt\n\nwarnings.filterwarnings(\"ignore\")\n\nSKIP_BOX_THR = 0.001\nIMAGE_ID_TO_SIZE = {}\nfor i in range(len(meta)):\n    IMAGE_ID_TO_SIZE[meta.at[i, \"id\"]] = {\"width\": meta.at[i, \"dim1\"], \"height\": meta.at[i, \"dim0\"]}\n    \ndef get_boxes_classes_from_preds(preds, score_threshold=SKIP_BOX_THR):\n    det = preds\n    predictions = []\n    for i in range(len(det)):\n        boxes = det[i].detach().cpu().numpy()[:,:4]    \n        scores = det[i].detach().cpu().numpy()[:,4]\n        indexes = np.where(scores > score_threshold)[0]\n        boxes = boxes[indexes]\n        boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n        boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n        predictions.append({\n            'boxes': boxes[indexes],\n            'scores': scores[indexes],\n        })\n    return [predictions]\n\n\ndef run_wbf(predictions, image_index, image_size=512, iou_thr=0.44, skip_box_thr=SKIP_BOX_THR, weights=None):\n    boxes = [(prediction[image_index]['boxes']\/(image_size-1)).tolist()  for prediction in predictions]\n    scores = [prediction[image_index]['scores'].tolist()  for prediction in predictions]\n    labels = [np.ones(prediction[image_index]['scores'].shape[0]).tolist() for prediction in predictions]\n    boxes, scores, labels = weighted_boxes_fusion(boxes, scores, labels, weights=None, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    boxes = boxes*(image_size-1)\n    return boxes, scores, labels\n\ndef run_nms(predictions, image_index, image_size=640, iou_thr=0.5, weights=None):\n    boxes = [(prediction[image_index]['boxes']\/(image_size-1)).tolist()  for prediction in predictions]\n    scores = [prediction[image_index]['scores'].tolist()  for prediction in predictions]\n    labels = [np.ones(prediction[image_index]['scores'].shape[0]).tolist() for prediction in predictions]\n    boxes, scores, labels = nms(boxes, scores, labels, weights=None, iou_thr=iou_thr)\n    boxes = boxes*(image_size-1)\n    return boxes, scores, labels\n\ndef box_label_conf_to_pred_str(boxes, labels, confs):\n    pred_str = []\n    for i in range(len(boxes)):\n        x1, y1, x2, y2 = boxes[i]\n        pred_str.append(f\"{labels[i]} {confs[i]} {x1} {y1} {x2} {y2}\")\n    pred_str = \" \".join(pred_str)\n    return pred_str\n\ndef box_to_original_size(boxes, image_id, image_id_to_size, imsize=512):\n    boxes = boxes.copy()\n    for i, box in enumerate(boxes):\n        w = image_id_to_size[image_id][\"width\"]\n        h = image_id_to_size[image_id][\"height\"]\n        box[[0,2]] = box[[0,2]] \/ imsize * w\n        box[[1,3]] = box[[1,3]] \/ imsize * h\n        boxes[i] = box\n    return boxes    \n\nclass Fitter:\n    def __init__(self, model, device, config):\n        self.config = config\n\n        self.base_dir = f'.\/{config.folder}'\n        if not os.path.exists(self.base_dir):\n            os.makedirs(self.base_dir)\n        \n        self.log_path = f'{self.base_dir}\/log.txt'\n\n        self.model = model\n        self.device = device\n\n        self.log(f'Fitter prepared. Device is {self.device}')\n\n    def infer(self, val_loader, iou_threshold, visualize=False):\n        print(\"iou threshold:\", iou_threshold)\n        self.log(f\"iou threshold: {iou_threshold}\")\n        self.model.eval()\n        t = time.time()\n        results = []\n        for step, (images, image_ids) in tqdm(enumerate(val_loader)):            \n            with torch.no_grad():\n                images = torch.stack(images)\n                batch_size = images.shape[0]\n                images = images.to(self.device).float()\n\n                pred = self.model(images, torch.tensor([1]*images.shape[0]).float().cuda())\n                \n                predictions = get_boxes_classes_from_preds(pred)\n                for i, image in enumerate(images):\n#                     boxes, scores, labels = run_wbf(predictions, image_index=i, iou_thr=iou_threshold)\n                    boxes, scores, labels = run_nms(predictions, image_index=i, iou_thr=iou_threshold)\n                    boxes = boxes.astype(np.int32).clip(min=0, max=640)\n                    image_id = image_ids[i]\n                    text_labels = [\"opacity\"] * len(boxes)\n\n                    boxes_ori_size = box_to_original_size(boxes, image_id, IMAGE_ID_TO_SIZE, imsize=640)\n                    result = {\n                        'id': image_id,\n                        'PredictionString': box_label_conf_to_pred_str(boxes_ori_size, text_labels, scores)\n                    }\n                    results.append(result)\n                    \n\n                    #visualize\n                    if visualize:\n                      if i==0 and np.random.uniform() < 0.02:\n                          sample = images[i].permute(1,2,0).cpu().numpy()\n                          sample = np.ascontiguousarray(sample)\n                    \n                          fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\n                          for score, box in zip(scores, boxes):\n                              sample[int(box[1]):int(box[3]), int(box[0]):int(box[2]), 1] += score\n\n                          for score, box in zip(scores, boxes):\n                              sample = cv2.rectangle(sample, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (1, 0, 0, min(score*3,1)), 2)\n                              \n                          ax.set_axis_off()\n                          ax.imshow(sample)\n                          plt.show()\n                          print(result)\n        \n        result_df = pd.DataFrame(results, columns=['id', 'PredictionString'])\n        os.makedirs('\/kaggle\/working\/effdet_results', exist_ok=True)\n        result_file = f'\/kaggle\/working\/effdet_results\/result.csv'\n        result_df.to_csv(result_file, index=False)\n        return result_df\n\n    def log(self, message):\n        if self.config.verbose:\n            print(message)\n        with open(self.log_path, 'a+') as logger:\n            logger.write(f'{message}\\n')\n            \nFINAL_COUNT += 1","d40a4e8a":"class TrainGlobalConfig:\n    num_workers = 2\n    batch_size = 4\n    folder = 'effdet5-cutmix-augmix'\n    verbose = True\n    verbose_step = 10\n    \nFINAL_COUNT += 1","458b90ae":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ndef run_inference():\n    device = torch.device('cuda:0')\n    net.to(device)\n\n    val_loader = torch.utils.data.DataLoader(\n        validation_dataset, \n        batch_size=TrainGlobalConfig.batch_size,\n        num_workers=TrainGlobalConfig.num_workers,\n        shuffle=False,\n        sampler=SequentialSampler(validation_dataset),\n        pin_memory=False,\n        collate_fn=collate_fn,\n    )\n\n    fitter = Fitter(model=net, device=device, config=TrainGlobalConfig)\n    result_df = fitter.infer(val_loader, iou_threshold=0.5, visualize=True)\n    return result_df\n\nFINAL_COUNT += 1","0f1e76b4":"from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchEval\nfrom effdet.efficientdet import HeadNet\n\ndef load_net(checkpoint_path):\n    config = get_efficientdet_config('tf_efficientdet_d5')\n    net = EfficientDet(config, pretrained_backbone=False)\n\n    config.num_classes = 1\n    config.image_size=640\n    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n\n    checkpoint = torch.load(checkpoint_path)\n    net.load_state_dict(checkpoint['model_state_dict'])\n\n    del checkpoint\n    gc.collect()\n\n    net = DetBenchEval(net, config)\n    net.eval();\n    return net.cuda()\n\nFINAL_COUNT += 1","28a82e3b":"# net = load_net('\/kaggle\/input\/siimcovid19-effdet\/effdet5_640_fold0_epoch20.bin')\n# effdet_fold0 = run_inference()\n# effdet_fold0","1b6974f3":"# net = load_net('\/kaggle\/input\/siimcovid19-effdet\/effdet5_640_fold1_epoch21.bin')\n# effdet_fold1 = run_inference()\n# effdet_fold1","43a80442":"# net = load_net('\/kaggle\/input\/siimcovid19-effdet\/effdet5_640_fold2_epoch24.bin')\n# effdet_fold2 = run_inference()\n# effdet_fold2","5a1a4c08":"net = load_net('\/kaggle\/input\/siimcovid19-effdet\/effdet5_640_fold3_epoch20.bin')\neffdet_fold3 = run_inference()\neffdet_fold3","ac69cb36":"net = load_net('\/kaggle\/input\/siimcovid19-effdet\/effdet5_640_fold4_epoch24.bin')\neffdet_fold4 = run_inference()\neffdet_fold4\n\nFINAL_COUNT += 1","51a6c453":"import numpy as np, pandas as pd\nfrom glob import glob\nimport shutil, os\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GroupKFold\nfrom tqdm.notebook import tqdm\nimport seaborn as sns\nimport torch\n\nFINAL_COUNT += 1","80470c0e":"FINAL_COUNT += 1\nmeta","d3cf53a7":"meta = meta[meta['split'] == 'test']\nif fast_sub:\n    test_df = fast_df.copy()\nelse:\n    test_df = pd.read_csv('..\/input\/siim-covid19-detection\/sample_submission.csv')\ntest_df = df[study_len:].reset_index(drop=True) \n# meta['image_id'] = meta['image_id'] + '_image'\n# meta.columns = ['id', 'dim0', 'dim1', 'split']\ntest_df = pd.merge(test_df, meta, on = 'id', how = 'left')\ntest_df.head()\n\nFINAL_COUNT += 1","28180808":"shutil.copytree('\/kaggle\/input\/yolov5-official-v31-dataset\/yolov5', '\/kaggle\/working\/yolov5')\nos.chdir('\/kaggle\/working\/yolov5') # install dependencies\n\nFINAL_COUNT += 1","130245b0":"dim = 640 #1024, 256, 'original'\ntest_dir = f'\/kaggle\/tmp\/{split}\/image'\nweights_dir = '\/kaggle\/input\/siimcovid19-detection-models\/yolov5x_fold0_nb_v14_best.pt'\n\nimport torch\n#from IPython.display import Image, clear_output  # to display images\n\n#clear_output()\n#print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n\n\n!python detect.py --weights $weights_dir\\\n--img 640\\\n--conf 0.001\\\n--iou 0.5\\\n--source $test_dir\\\n--save-txt --save-conf --augment\n\n\ndef yolo2voc(image_height, image_width, bboxes):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    voc  => [x1, y1, x2, y1]\n\n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n\n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]\/2\n    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n\n    return bboxes\n\nimage_ids = [] # (num_images, )\nPredictionStrings = [] # (num_images, boxes_lst)\n\nfor file_path in tqdm(glob('runs\/detect\/exp\/labels\/*.txt')):\n    image_id = file_path.split('\/')[-1].split('.')[0]\n    w, h = test_df.loc[test_df.id==image_id,['dim1', 'dim0']].values[0]\n    f = open(file_path, 'r')\n    data = np.array(f.read().replace('\\n', ' ').strip().split(' ')).astype(np.float32).reshape(-1, 6)\n    data = data[:, [0, 5, 1, 2, 3, 4]]\n    bboxes = list(np.round(np.concatenate((data[:, :2], np.round(yolo2voc(h, w, data[:, 2:]))), axis =1).reshape(-1), 12).astype(str))\n    new_bboxes = []\n    for idx in range(len(bboxes)):\n        if idx%6 == 0:\n            new_bboxes.append('opacity')\n        else :\n            new_bboxes.append(bboxes[idx])\n    image_ids.append(image_id)\n    PredictionStrings.append(' '.join(new_bboxes))\n\n\npred_yolov5_fold0_df = pd.DataFrame({'id':image_ids,\n                        'PredictionString':PredictionStrings})\n\nFINAL_COUNT += 1\npred_yolov5_fold0_df.head()","cd79717e":"dim = 640 #1024, 256, 'original'\ntest_dir = f'\/kaggle\/tmp\/{split}\/image'\nweights_dir = '\/kaggle\/input\/siimcovid19-detection-models\/yolov5x_fold1_nb_v19_best.pt'\n\nimport torch\n#from IPython.display import Image, clear_output  # to display images\n\n#clear_output()\n#print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n\n\n!python detect.py --weights $weights_dir\\\n--img 640\\\n--conf 0.001\\\n--iou 0.5\\\n--source $test_dir\\\n--save-txt --save-conf --augment\n\n\ndef yolo2voc(image_height, image_width, bboxes):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    voc  => [x1, y1, x2, y1]\n\n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n\n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]\/2\n    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n\n    return bboxes\n\nimage_ids = [] # (num_images, )\nPredictionStrings = [] # (num_images, boxes_lst)\n\nfor file_path in tqdm(glob('runs\/detect\/exp2\/labels\/*.txt')):\n    image_id = file_path.split('\/')[-1].split('.')[0]\n    w, h = test_df.loc[test_df.id==image_id,['dim1', 'dim0']].values[0]\n    f = open(file_path, 'r')\n    data = np.array(f.read().replace('\\n', ' ').strip().split(' ')).astype(np.float32).reshape(-1, 6)\n    data = data[:, [0, 5, 1, 2, 3, 4]]\n    bboxes = list(np.round(np.concatenate((data[:, :2], np.round(yolo2voc(h, w, data[:, 2:]))), axis =1).reshape(-1), 12).astype(str))\n    new_bboxes = []\n    for idx in range(len(bboxes)):\n        if idx%6 == 0:\n            new_bboxes.append('opacity')\n        else :\n            new_bboxes.append(bboxes[idx])\n    image_ids.append(image_id)\n    PredictionStrings.append(' '.join(new_bboxes))\n\n\npred_yolov5_fold1_df = pd.DataFrame({'id':image_ids,\n                        'PredictionString':PredictionStrings})\n\nFINAL_COUNT += 1\npred_yolov5_fold1_df.head()","ef533389":"dim = 640 #1024, 256, 'original'\ntest_dir = f'\/kaggle\/tmp\/{split}\/image'\nweights_dir = '\/kaggle\/input\/siimcovid19-detection-models\/yolov5x_fold2_nb_v20_best.pt'\n\nimport torch\n#from IPython.display import Image, clear_output  # to display images\n\n#clear_output()\n#print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n\n\n!python detect.py --weights $weights_dir\\\n--img 640\\\n--conf 0.001\\\n--iou 0.5\\\n--source $test_dir\\\n--save-txt --save-conf --augment\n\n\ndef yolo2voc(image_height, image_width, bboxes):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    voc  => [x1, y1, x2, y1]\n\n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n\n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]\/2\n    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n\n    return bboxes\n\nimage_ids = [] # (num_images, )\nPredictionStrings = [] # (num_images, boxes_lst)\n\nfor file_path in tqdm(glob('runs\/detect\/exp3\/labels\/*.txt')):\n    image_id = file_path.split('\/')[-1].split('.')[0]\n    w, h = test_df.loc[test_df.id==image_id,['dim1', 'dim0']].values[0]\n    f = open(file_path, 'r')\n    data = np.array(f.read().replace('\\n', ' ').strip().split(' ')).astype(np.float32).reshape(-1, 6)\n    data = data[:, [0, 5, 1, 2, 3, 4]]\n    bboxes = list(np.round(np.concatenate((data[:, :2], np.round(yolo2voc(h, w, data[:, 2:]))), axis =1).reshape(-1), 12).astype(str))\n    new_bboxes = []\n    for idx in range(len(bboxes)):\n        if idx%6 == 0:\n            new_bboxes.append('opacity')\n        else :\n            new_bboxes.append(bboxes[idx])\n    image_ids.append(image_id)\n    PredictionStrings.append(' '.join(new_bboxes))\n\n\npred_yolov5_fold2_df = pd.DataFrame({'id':image_ids,\n                        'PredictionString':PredictionStrings})\n\nFINAL_COUNT += 1\npred_yolov5_fold2_df.head()","d13e6ac7":"dim = 640 #1024, 256, 'original'\ntest_dir = f'\/kaggle\/tmp\/{split}\/image'\nweights_dir = '\/kaggle\/input\/siimcovid19-detection-models\/yolov5x_fold3_nb_v21_best.pt'\n\nimport torch\n#from IPython.display import Image, clear_output  # to display images\n\n#clear_output()\n#print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n\n\n!python detect.py --weights $weights_dir\\\n--img 640\\\n--conf 0.001\\\n--iou 0.5\\\n--source $test_dir\\\n--save-txt --save-conf --augment\n\n\ndef yolo2voc(image_height, image_width, bboxes):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    voc  => [x1, y1, x2, y1]\n\n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n\n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]\/2\n    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n\n    return bboxes\n\nimage_ids = [] # (num_images, )\nPredictionStrings = [] # (num_images, boxes_lst)\n\nfor file_path in tqdm(glob('runs\/detect\/exp4\/labels\/*.txt')):\n    image_id = file_path.split('\/')[-1].split('.')[0]\n    w, h = test_df.loc[test_df.id==image_id,['dim1', 'dim0']].values[0]\n    f = open(file_path, 'r')\n    data = np.array(f.read().replace('\\n', ' ').strip().split(' ')).astype(np.float32).reshape(-1, 6)\n    data = data[:, [0, 5, 1, 2, 3, 4]]\n    bboxes = list(np.round(np.concatenate((data[:, :2], np.round(yolo2voc(h, w, data[:, 2:]))), axis =1).reshape(-1), 12).astype(str))\n    new_bboxes = []\n    for idx in range(len(bboxes)):\n        if idx%6 == 0:\n            new_bboxes.append('opacity')\n        else :\n            new_bboxes.append(bboxes[idx])\n    image_ids.append(image_id)\n    PredictionStrings.append(' '.join(new_bboxes))\n\n\npred_yolov5_fold3_df = pd.DataFrame({'id':image_ids,\n                        'PredictionString':PredictionStrings})\n\nFINAL_COUNT += 1\npred_yolov5_fold3_df.head()","8bb1f74a":"dim = 640 #1024, 256, 'original'\ntest_dir = f'\/kaggle\/tmp\/{split}\/image'\nweights_dir = '\/kaggle\/input\/siimcovid19-detection-models\/yolov5x_fold4_nb_v22_best.pt'\n\nimport torch\n#from IPython.display import Image, clear_output  # to display images\n\n#clear_output()\n#print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n\n\n!python detect.py --weights $weights_dir\\\n--img 640\\\n--conf 0.001\\\n--iou 0.5\\\n--source $test_dir\\\n--save-txt --save-conf --augment\n\n\ndef yolo2voc(image_height, image_width, bboxes):\n    \"\"\"\n    yolo => [xmid, ymid, w, h] (normalized)\n    voc  => [x1, y1, x2, y1]\n\n    \"\"\" \n    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n\n    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n\n    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]\/2\n    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n\n    return bboxes\n\nimage_ids = [] # (num_images, )\nPredictionStrings = [] # (num_images, boxes_lst)\n\nfor file_path in tqdm(glob('runs\/detect\/exp5\/labels\/*.txt')):\n    image_id = file_path.split('\/')[-1].split('.')[0]\n    w, h = test_df.loc[test_df.id==image_id,['dim1', 'dim0']].values[0]\n    f = open(file_path, 'r')\n    data = np.array(f.read().replace('\\n', ' ').strip().split(' ')).astype(np.float32).reshape(-1, 6)\n    data = data[:, [0, 5, 1, 2, 3, 4]]\n    bboxes = list(np.round(np.concatenate((data[:, :2], np.round(yolo2voc(h, w, data[:, 2:]))), axis =1).reshape(-1), 12).astype(str))\n    new_bboxes = []\n    for idx in range(len(bboxes)):\n        if idx%6 == 0:\n            new_bboxes.append('opacity')\n        else :\n            new_bboxes.append(bboxes[idx])\n    image_ids.append(image_id)\n    PredictionStrings.append(' '.join(new_bboxes))\n\n\npred_yolov5_fold4_df = pd.DataFrame({'id':image_ids,\n                        'PredictionString':PredictionStrings})\n\nFINAL_COUNT += 1\npred_yolov5_fold4_df.head()","9e9b036b":"!pip install \/kaggle\/input\/mmdet2100\/mmdetection-2.10.0\/addict-2.4.0-py3-none-any.whl\n!pip install \/kaggle\/input\/mmdet2100\/mmdetection-2.10.0\/yapf-0.31.0-py2.py3-none-any.whl\n!pip install \/kaggle\/input\/pycocotools202\/pycocotools-2.0.2-cp37-cp37m-linux_x86_64.whl\n!pip install \/kaggle\/input\/mmcvfull134\/mmcv_full-1.3.4-cp37-cp37m-manylinux1_x86_64.whl\n!pip install \/kaggle\/input\/mmdetection2120\/mmdetection-2.12.0 -f .\/ --no-index\n\nFINAL_COUNT += 1","d6eb1e2d":"import os\nimport shutil\nimport yaml\nimport time\nimport json\nimport cv2\nimport random\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\nfrom tqdm.notebook import tqdm\nimport seaborn as sns\nimport torch\nfrom IPython.display import Image, clear_output\nfrom collections import Counter\nfrom ensemble_boxes import *\nimport copy\nimport os.path as osp\nimport mmcv\nimport mmdet\nimport numpy as np\nimport albumentations as A\nfrom mmdet.datasets.builder import DATASETS\nfrom mmdet.datasets.custom import CustomDataset\nfrom mmcv import Config\nfrom mmdet.apis import set_random_seed\nfrom mmdet.apis import inference_detector, init_detector, show_result_pyplot\nfrom mmdet.datasets import build_dataset\nfrom mmdet.models import build_detector\nfrom mmdet.apis import train_detector\nprint(mmdet.__version__)\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\nFINAL_COUNT += 1","a1bc1403":"checkpoint = '\/kaggle\/input\/siimcovid19-detection-models\/cascade_rcnn_x101_32x4d_fpn_1x_fold0_epoch10_public_nb_v8.pth'\ncfg = '\/kaggle\/input\/siimcovid19-detection-models\/cascade_rcnn_x101_32x4d_fpn_1x_fold0_epoch10_public_nb_v8_config.py'\n\ncfg = Config.fromfile(cfg)\n\ncfg.classes = (\"Covid_Abnormality\")\ncfg.data.test.img_prefix = ''\ncfg.data.test.classes = cfg.classes\n\n# cfg.model.roi_head.bbox_head.num_classes = 1\n# cfg.model.bbox_head.num_classes = 1\nfor head in cfg.model.roi_head.bbox_head:\n    head.num_classes = 1\n\n# Set seed thus the results are more reproducible\ncfg.seed = 211\nset_random_seed(211, deterministic=False)\ncfg.gpu_ids = [0]\n\ncfg.data.test.pipeline=[\n            dict(type='LoadImageFromFile'),\n            dict(\n                type='MultiScaleFlipAug',\n                img_scale=(1333, 800),\n                flip=False,\n                transforms=[\n                    dict(type='Resize', keep_ratio=True),\n                    dict(type='RandomFlip', direction='horizontal'),\n                    dict(\n                        type='Normalize',\n                        mean=[123.675, 116.28, 103.53],\n                        std=[58.395, 57.12, 57.375],\n                        to_rgb=True),\n                    dict(type='Pad', size_divisor=32),\n                    dict(type='DefaultFormatBundle'),\n                    dict(type='Collect', keys=['img'])\n                ])\n        ]\n\ncfg.test_pipeline = [\n            dict(type='LoadImageFromFile'),\n            dict(\n                type='MultiScaleFlipAug',\n                img_scale=(1333, 800),\n                flip=False,\n                transforms=[\n                    dict(type='Resize', keep_ratio=True),\n                    dict(type='RandomFlip', direction='horizontal'),\n                    dict(\n                        type='Normalize',\n                        mean=[123.675, 116.28, 103.53],\n                        std=[58.395, 57.12, 57.375],\n                        to_rgb=True),\n                    dict(type='Pad', size_divisor=32),\n                    dict(type='DefaultFormatBundle'),\n                    dict(type='Collect', keys=['img'])\n                ])\n        ]\n\n# cfg.data.samples_per_gpu = 4\n# cfg.data.workers_per_gpu = 4\n# cfg.model.test_cfg.nms.iou_threshold = 0.3\ncfg.model.test_cfg.rcnn.score_thr = 0.001\n\nmodel_test = init_detector(cfg, checkpoint, device='cuda:0')\nprint(model_test)\n\nFINAL_COUNT += 1","09425339":"def nms_one_img(preds, img_path, img_width, img_height):\n    # print('img width:', img_width)\n    # print('img height:', img_height)\n    boxes_list = []\n    scores_list = []\n    labels_list = []\n    weights = [1]\n    # print(preds)\n    for i, pred in enumerate(preds):\n        if len(pred):\n            for p in pred:\n                box = [0, 0, 0, 0]\n                box[0] = min(1.0, p[0] \/ img_width)\n                box[1] = min(1.0, p[1] \/ img_height)\n                box[2] = min(1.0, p[2] \/ img_width)\n                box[3] = min(1.0, p[3] \/ img_height)\n                # print(box)\n                for b in box:\n                    if b > 1:\n                        print(img_path)\n                boxes_list.append(box)\n                score = p[4].astype(float)\n                scores_list.append(score)\n                labels_list.append(i)\n    # print('Before:')\n    # print(boxes_list)\n    boxes_list, scores_list, labels_list = nms([boxes_list], [scores_list], [labels_list], weights=weights, iou_thr=0.6)\n    # print('After:')\n    # print(boxes_list)\n    return boxes_list, scores_list, labels_list\n\nFINAL_COUNT += 1","c5f086bc":"def infer_img(model, img_path):\n    img = mmcv.imread(img_path)\n    result = inference_detector(model, img_path)\n    # print(img.shape)\n    # print(img.shape[0])\n    # print(img.shape[1])\n    boxes_list, scores_list, labels_list = nms_one_img(result, img_path, img.shape[1], img.shape[0])\n    return boxes_list, scores_list, labels_list\n\nFINAL_COUNT += 1","fec06817":"def round_float(str_float, dec=3):\n    return str(round(float(str_float), 3))\n\nFINAL_COUNT += 1","f8ada441":"def convert_to_df_row_list(img_id, boxes_list, scores_list, labels_list):\n    res = []\n    res.append(img_id)\n    prediction_string = ''\n    for i, label in enumerate(labels_list):\n        if int(label) == 1:\n            prediction_string = 'none 1 0 0 1 1'\n            continue\n        else:\n            prediction_string += 'opacity'\n        prediction_string += ' '\n        prediction_string += str(round(float(scores_list[i]), 3))\n        prediction_string += ' '\n        prediction_string += round_float(boxes_list[i][0], 3) + ' ' + round_float(boxes_list[i][1], 3) + ' ' + round_float(boxes_list[i][2], 3) + ' ' + round_float(boxes_list[i][3], 3) + ' '\n    res.append(prediction_string.rstrip())\n    return res\n\nFINAL_COUNT += 1","4cb690cb":"vfnet_pred_lst = []\ntest_dir = f'\/kaggle\/tmp\/{split}\/image'\ncount = 0\n\nfor img_name in os.listdir(test_dir):\n    img_path = test_dir + '\/' + img_name\n    # boxes_list, scores_list, labels_list = infer_img(model, '..\/vinbigdata-cocodataset\/ori_vinbigdata_3xdownsampled\/test\/test\/b461cd28bc17c294dd986d0d91577ac3.jpg')\n    boxes_list, scores_list, labels_list = infer_img(model_test, img_path)\n    # print(boxes_list)\n    # print(scores_list)\n    # print(labels_list)\n    image_id = img_name.split('.png')[0]\n    ori_img_width = test_df[test_df.id == image_id]['dim1'].iloc[0]\n    ori_img_height = test_df[test_df.id == image_id]['dim0'].iloc[0]\n    boxes_list[:, 0] = boxes_list[:, 0] * ori_img_width\n    boxes_list[:, 2] = boxes_list[:, 2] * ori_img_width\n    boxes_list[:, 1] = boxes_list[:, 1] * ori_img_height\n    boxes_list[:, 3] = boxes_list[:, 3] * ori_img_height\n    df_row_lst = convert_to_df_row_list(image_id, boxes_list, scores_list, labels_list)\n    vfnet_pred_lst.append(df_row_lst)\n    \nFINAL_COUNT += 1","b06ded42":"vfnet_pred_df = pd.DataFrame(vfnet_pred_lst, columns=['id', 'PredictionString'])\n\nFINAL_COUNT += 1\nvfnet_pred_df.head()","ac270e27":"# %%\nimport numpy as np\nimport pandas as pd\nfrom ensemble_boxes import nms, soft_nms, non_maximum_weighted, weighted_boxes_fusion\n\n# %%\n''' Merge output of two models after 2cls filter\n- Detectron2: https:\/\/www.kaggle.com\/corochann\/vinbigdata-detectron2-prediction\n- Yolov5: https:\/\/www.kaggle.com\/awsaf49\/vinbigdata-2-class-filter\nReference:\n- https:\/\/github.com\/ZFTurbo\/Weighted-Boxes-Fusion\n'''\n\n# %%\nweights = [54, 54, 54, 54, 54, 58, 58, 54]\niou_thr = 0.6\nskip_box_thr = 0.01 # 0.0001 for non soft-nms\nsigma = 0.1\n\n# %%\ntest_meta = test_df\nmerged_df = pd.DataFrame(columns=['id', 'PredictionString'])\n\n# %%\n'''Weighted Boxes Fusion'''\nimage_id_lst = pred_yolov5_fold0_df['id'].unique()\n\n# %%\n# Helper functions\ndef extract_data(data, img_height, img_width):\n    boxes_lst = []\n    scores_lst = []\n    labels_lst = []\n    data_lst = data.split(' ')\n    for i in range(0, len(data_lst), 6):\n        labels_lst.append(0)\n        scores_lst.append(float(data_lst[i + 1]))\n        x_min = float(data_lst[i + 2]) \/ img_width\n        y_min = float(data_lst[i + 3]) \/ img_height\n        x_max = float(data_lst[i + 4]) \/ img_width\n        y_max = float(data_lst[i + 5]) \/ img_height\n        boxes_lst.append([x_min, y_min, x_max, y_max])\n    return boxes_lst, scores_lst, labels_lst\n\ndef convert_data_to_row(boxes, scores, labels):\n    data_lst = []\n    for i in range(len(boxes)):\n        data_lst.append('opacity')\n        data_lst.append(str(scores[i]))\n        data_lst.append(str(boxes[i][0]))\n        data_lst.append(str(boxes[i][1]))\n        data_lst.append(str(boxes[i][2]))\n        data_lst.append(str(boxes[i][3]))\n    data = ' '.join(data_lst)\n    return data\n\ndef get_height_width(image_id):\n    # dim0: heigth, dim1: width\n    height = test_meta[test_meta['id'] == image_id]['dim0'].values[0]\n    width = test_meta[test_meta['id'] == image_id]['dim1'].values[0]\n    return height, width\n\ndef scale_data(boxes, img_height, img_width):\n    res = []\n    for box in boxes:\n        temp = []\n        temp.append(box[0] * img_width)\n        temp.append(box[1] * img_height)\n        temp.append(box[2] * img_width)\n        temp.append(box[3] * img_height)\n        res.append(temp)\n    return res\n\n# %%\ndef wbf(image_id):\n    img_height, img_width = get_height_width(image_id)\n    boxes_lst, scores_lst, labels_lst = [], [], []\n\n    pred_yolov5_fold0_data = pred_yolov5_fold0_df[pred_yolov5_fold0_df['id'] == image_id]['PredictionString'].values[0]\n    model_boxes_lst, model_scores_lst, model_labels_lst = extract_data(pred_yolov5_fold0_data, img_height, img_width)\n    boxes_lst.append(model_boxes_lst)\n    scores_lst.append(model_scores_lst)\n    labels_lst.append(model_labels_lst)\n\n    pred_yolov5_fold1_data = pred_yolov5_fold1_df[pred_yolov5_fold1_df['id'] == image_id]['PredictionString'].values[0]\n    model_boxes_lst, model_scores_lst, model_labels_lst = extract_data(pred_yolov5_fold1_data, img_height, img_width)\n    boxes_lst.append(model_boxes_lst)\n    scores_lst.append(model_scores_lst)\n    labels_lst.append(model_labels_lst)\n\n    pred_yolov5_fold2_data = pred_yolov5_fold2_df[pred_yolov5_fold2_df['id'] == image_id]['PredictionString'].values[0]\n    model_boxes_lst, model_scores_lst, model_labels_lst = extract_data(pred_yolov5_fold2_data, img_height, img_width)\n    boxes_lst.append(model_boxes_lst)\n    scores_lst.append(model_scores_lst)\n    labels_lst.append(model_labels_lst)\n\n    pred_yolov5_fold3_data = pred_yolov5_fold3_df[pred_yolov5_fold3_df['id'] == image_id]['PredictionString'].values[0]\n    model_boxes_lst, model_scores_lst, model_labels_lst = extract_data(pred_yolov5_fold3_data, img_height, img_width)\n    boxes_lst.append(model_boxes_lst)\n    scores_lst.append(model_scores_lst)\n    labels_lst.append(model_labels_lst)\n\n    pred_yolov5_fold4_data = pred_yolov5_fold4_df[pred_yolov5_fold4_df['id'] == image_id]['PredictionString'].values[0]\n    model_boxes_lst, model_scores_lst, model_labels_lst = extract_data(pred_yolov5_fold4_data, img_height, img_width)\n    boxes_lst.append(model_boxes_lst)\n    scores_lst.append(model_scores_lst)\n    labels_lst.append(model_labels_lst)\n    \n#     effdet_fold0_data = effdet_fold0[effdet_fold0['id'] == image_id]['PredictionString'].values[0]\n#     model_boxes_lst, model_scores_lst, model_labels_lst = extract_data(effdet_fold0_data, img_height, img_width)\n#     boxes_lst.append(model_boxes_lst)\n#     scores_lst.append(model_scores_lst)\n#     labels_lst.append(model_labels_lst)\n    \n#     effdet_fold1_data = effdet_fold1[effdet_fold1['id'] == image_id]['PredictionString'].values[0]\n#     model_boxes_lst, model_scores_lst, model_labels_lst = extract_data(effdet_fold1_data, img_height, img_width)\n#     boxes_lst.append(model_boxes_lst)\n#     scores_lst.append(model_scores_lst)\n#     labels_lst.append(model_labels_lst)\n    \n#     effdet_fold2_data = effdet_fold2[effdet_fold2['id'] == image_id]['PredictionString'].values[0]\n#     model_boxes_lst, model_scores_lst, model_labels_lst = extract_data(effdet_fold2_data, img_height, img_width)\n#     boxes_lst.append(model_boxes_lst)\n#     scores_lst.append(model_scores_lst)\n#     labels_lst.append(model_labels_lst)\n    \n    effdet_fold3_data = effdet_fold3[effdet_fold3['id'] == image_id]['PredictionString'].values[0]\n    model_boxes_lst, model_scores_lst, model_labels_lst = extract_data(effdet_fold3_data, img_height, img_width)\n    boxes_lst.append(model_boxes_lst)\n    scores_lst.append(model_scores_lst)\n    labels_lst.append(model_labels_lst)\n    \n    effdet_fold4_data = effdet_fold4[effdet_fold4['id'] == image_id]['PredictionString'].values[0]\n    model_boxes_lst, model_scores_lst, model_labels_lst = extract_data(effdet_fold4_data, img_height, img_width)\n    boxes_lst.append(model_boxes_lst)\n    scores_lst.append(model_scores_lst)\n    labels_lst.append(model_labels_lst)\n    \n    vfnet_pred_data = vfnet_pred_df[vfnet_pred_df['id'] == image_id]['PredictionString'].values[0]\n    model_boxes_lst, model_scores_lst, model_labels_lst = extract_data(vfnet_pred_data, img_height, img_width)\n    boxes_lst.append(model_boxes_lst)\n    scores_lst.append(model_scores_lst)\n    labels_lst.append(model_labels_lst)\n\n    boxes, scores, labels = weighted_boxes_fusion(boxes_lst, scores_lst, labels_lst, weights=weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    # boxes, scores, labels = non_maximum_weighted(boxes_lst, scores_lst, labels_lst, weights=weights, iou_thr=iou_thr, skip_box_thr=skip_box_thr)\n    # boxes, scores, labels = nms(boxes_lst, scores_lst, labels_lst, weights=weights, iou_thr=iou_thr)\n    # boxes, scores, labels = soft_nms(boxes_lst, scores_lst, labels_lst, weights=weights, iou_thr=iou_thr, sigma=sigma, thresh=skip_box_thr)\n    boxes = scale_data(boxes, img_height, img_width)\n    # return boxes, scores, labels\n    merged_data = convert_data_to_row(boxes, scores, labels)\n    merged_data = pd.DataFrame([[image_id, merged_data]], columns=['id', 'PredictionString'])\n    return merged_data\n\n# %%\n# Test\n# test_boxes, test_scores, test_labels = wbf(image_id_lst[0])\ntest_wbf = wbf(image_id_lst[0])\n\n# %%\nfor image_id in image_id_lst:\n    merged_data = wbf(image_id)\n    merged_df = merged_df.append(merged_data, ignore_index=True)\n    \nFINAL_COUNT += 1","a38b4731":"FINAL_COUNT += 1\nmerged_df","9c6437fc":"FINAL_COUNT += 1\ntest_df","4fc9ca9c":"test_df = test_df.drop(['PredictionString'], axis=1)\nsub_df = pd.merge(test_df, merged_df, on = 'id', how = 'left').fillna(\"none 1 0 0 1 1\")\nFINAL_COUNT += 1\nsub_df","eb19a3e3":"sub_df = sub_df[['id', 'PredictionString']]\nsub_df['none'] = df_2class['none']\nFINAL_COUNT += 1\nsub_df","cd9a2368":"for i in range(sub_df.shape[0]):\n    if sub_df.loc[i,'PredictionString'] == \"none 1 0 0 1 1\":\n        continue\n    sub_df_split = sub_df.loc[i,'PredictionString'].split()\n    none_score = float(sub_df.loc[i, 'none'])\n    sub_df_list = []\n    for j in range(int(len(sub_df_split) \/ 6)):\n        sub_df_list.append('opacity')\n        opacity_score = float(sub_df_split[6 * j + 1])\n        calibrated_opacity_score = opacity_score * ((1 - none_score) ** 0.5) \n        sub_df_list.append(str(calibrated_opacity_score))\n        sub_df_list.append(sub_df_split[6 * j + 2])\n        sub_df_list.append(sub_df_split[6 * j + 3])\n        sub_df_list.append(sub_df_split[6 * j + 4])\n        sub_df_list.append(sub_df_split[6 * j + 5])\n    sub_df.loc[i,'PredictionString'] = ' '.join(sub_df_list)\nfor i in range(sub_df.shape[0]):\n    if sub_df.loc[i,'PredictionString'] != 'none 1 0 0 1 1':\n        sub_df.loc[i,'PredictionString'] = sub_df.loc[i,'PredictionString'] + ' none ' + str(sub_df.loc[i,'none']) + ' 0 0 1 1'\nsub_df = sub_df[['id', 'PredictionString']]   \ndf_study = df_study[:study_len]\ndf_study = df_study.append(sub_df).reset_index(drop=True)\nif FINAL_COUNT==81:\n    df_study.to_csv('\/kaggle\/working\/submission.csv',index = False)  \nshutil.rmtree('\/kaggle\/working\/yolov5')","655f5141":"df_study","5ae35092":"# 2 class","99e688cd":"## Model","aa70034e":"## yolov5 fold 3","5ce39009":"## yolov5 fold 4","e33ac03b":"# yolov5 predict","69d9556a":"## Define Train\\Validation Image Augmentations","3b1aafc3":"## yolov5 fold 2","f6a64bdc":"# study predict","db62035c":"# Detection Emsemble","ebf2d178":"# study string","1aa33611":"# MMDetection","7e36a417":"## yolov5 fold 1","e51c4ab0":"# Effdet predict","b495e63f":"thanks to https:\/\/www.kaggle.com\/xhlulu\/siim-covid-19-convert-to-jpg-256px  \nthanks to https:\/\/www.kaggle.com\/awsaf49\/vinbigdata-cxr-ad-yolov5-14-class-infer  \ntrain_study: https:\/\/www.kaggle.com\/h053473666\/siim-covid19-efnb7-train-study  \ntrain_image: https:\/\/www.kaggle.com\/h053473666\/siim-cov19-yolov5-train  \ntrain_2class: https:\/\/www.kaggle.com\/h053473666\/siim-covid19-efnb7-train-fold0-5-2class  \n  \nversion1:Original hyperparameters (yolov5)  \nversion4:New hyperparameters (yolov5)\n","41cb23b0":"# .dcm to .png","31093e82":"### Note:\n- df_2class and sub_df should be merged by id, not sub_df['none'] = df_2class['none'] because there might be a mix of ids","6f507916":"## Main Loop","4a42d073":"## yolov5 fold 0","a7afca75":"# Version\n\n* `v6`: \n    classification: tf_efficientnetv2_m_in21k_fold0_epoch17_colab_pseudo_labeling_v5.pt, tf_efficientnetv2_m_in21k_fold1_epoch20_colab_uda_v8.pt, tf_efficientnetv2_m_in21k_fold2_epoch6_colab_pseudo_labeling_v7.pt, tf_efficientnetv2_m_in21k_fold3_epoch15_colab_pseudo_labeling_v8.pt, tf_efficientnetv2_m_in21k_fold4_epoch26_colab_pseudo_labeling_v9.pt, vit_deit_base_patch16_384_fold0_epoch10_colab_vit_v7.pt, vit_deit_base_patch16_384_fold1_epoch9_colab_vit_v9.pt, vit_deit_base_patch16_384_fold2_epoch14_colab_vit_v11.pt, vit_deit_base_patch16_384_fold3_epoch6_colab_vit_v12.pt, vit_deit_base_patch16_384_fold4_epoch13_colab_vit_v13.pt weights=[38,38,38,38,38,37,37,37,37,37] 5_tta\n    \n    detection: yolov5x_fold0_nb_v14_best.pt + yolov5x_fold1_nb_v19_best.pt + yolov5x_fold2_nb_v20_best.pt + yolov5x_fold3_nb_v21_best.pt + yolov5x_fold4_nb_v22_best.pt + siimcovid19-effdet\/effdet5_640_fold3_epoch20.bin + siimcovid19-effdet\/effdet5_640_fold4_epoch24.bin nms_0.5 img_640 + cascade_rcnn_x101_32x4d_fpn_1x_fold0_epoch10_public_nb_v8.pth nms_iou_0.5 wbf iou=0.6 weight=[54,54,54,54,54,58,58,58,58,58,54] skipbox_thr=0.01 tta (1 - none_score) ** 0.5 img_640\n\n* `v5`: \n    classification: tf_efficientnetv2_m_in21k_fold0_epoch17_colab_pseudo_labeling_v5.pt, tf_efficientnetv2_m_in21k_fold1_epoch0_colab_v33.pt, tf_efficientnetv2_m_in21k_fold2_epoch6_colab_pseudo_labeling_v7.pt, tf_efficientnetv2_m_in21k_fold3_epoch15_colab_pseudo_labeling_v8.pt, tf_efficientnetv2_m_in21k_fold4_epoch26_colab_pseudo_labeling_v9.pt, vit_deit_base_patch16_384_fold0_epoch10_colab_vit_v7.pt, vit_deit_base_patch16_384_fold1_epoch9_colab_vit_v9.pt, vit_deit_base_patch16_384_fold2_epoch14_colab_vit_v11.pt, vit_deit_base_patch16_384_fold3_epoch6_colab_vit_v12.pt, vit_deit_base_patch16_384_fold4_epoch13_colab_vit_v13.pt weights=[38,38,38,38,38,37,37,37,37,37] 5_tta\n    \n    detection: yolov5x_fold0_nb_v14_best.pt + yolov5x_fold1_nb_v19_best.pt + yolov5x_fold2_nb_v20_best.pt + yolov5x_fold3_nb_v21_best.pt + yolov5x_fold4_nb_v22_best.pt + siimcovid19-effdet\/effdet5_640_fold3_epoch20.bin + siimcovid19-effdet\/effdet5_640_fold4_epoch24.bin nms_0.5 img_640 + cascade_rcnn_x101_32x4d_fpn_1x_fold0_epoch10_public_nb_v8.pth nms_iou_0.5 wbf iou=0.6 weight=[54,54,54,54,54,58,58,58,58,58,54] skipbox_thr=0.01 tta (1 - none_score) ** 0.5 img_640\n\n* `v4`: \n    classification: tf_efficientnetv2_m_in21k_fold0_epoch17_colab_pseudo_labeling_v5.pt, tf_efficientnetv2_m_in21k_fold1_epoch0_colab_v33.pt, tf_efficientnetv2_m_in21k_fold2_epoch6_colab_pseudo_labeling_v7.pt, tf_efficientnetv2_m_in21k_fold3_epoch15_colab_pseudo_labeling_v8.pt, tf_efficientnetv2_m_in21k_fold4_epoch26_colab_pseudo_labeling_v9.pt, vit_deit_base_patch16_384_fold0_epoch10_colab_vit_v7.pt, vit_deit_base_patch16_384_fold1_epoch9_colab_vit_v9.pt, vit_deit_base_patch16_384_fold2_epoch14_colab_vit_v11.pt, vit_deit_base_patch16_384_fold3_epoch6_colab_vit_v12.pt, vit_deit_base_patch16_384_fold4_epoch13_colab_vit_v13.pt weights=[38,38,38,38,38,37,37,37,37,37] 5_tta\n\n* `v3`: \n    classification: tf_efficientnetv2_m_in21k_fold0_epoch17_colab_pseudo_labeling_v5.pt, tf_efficientnetv2_m_in21k_fold1_epoch20_colab_uda_v8.pt, tf_efficientnetv2_m_in21k_fold2_epoch6_colab_pseudo_labeling_v7.pt, tf_efficientnetv2_m_in21k_fold3_epoch15_colab_pseudo_labeling_v8.pt, tf_efficientnetv2_m_in21k_fold4_epoch26_colab_pseudo_labeling_v9.pt, vit_deit_base_patch16_384_fold0_epoch10_colab_vit_v7.pt, vit_deit_base_patch16_384_fold1_epoch9_colab_vit_v9.pt, vit_deit_base_patch16_384_fold2_epoch14_colab_vit_v11.pt, vit_deit_base_patch16_384_fold3_epoch6_colab_vit_v12.pt, vit_deit_base_patch16_384_fold4_epoch13_colab_vit_v13.pt weights=[38,38,38,38,38,37,37,37,37,37] 5_tta\n\n* `v2`:\n    classification: tf_efficientnetv2_m_in21k_fold0_epoch17_colab_pseudo_labeling_v5.pt, tf_efficientnetv2_m_in21k_fold1_epoch0_colab_v33.pt, tf_efficientnetv2_m_in21k_fold2_epoch6_colab_pseudo_labeling_v7.pt, tf_efficientnetv2_m_in21k_fold3_epoch15_colab_pseudo_labeling_v8.pt, tf_efficientnetv2_m_in21k_fold4_epoch26_colab_pseudo_labeling_v9.pt, vit_deit_base_patch16_384_fold0_epoch10_colab_vit_v7.pt, vit_deit_base_patch16_384_fold1_epoch9_colab_vit_v9.pt, vit_deit_base_patch16_384_fold2_epoch14_colab_vit_v11.pt, vit_deit_base_patch16_384_fold3_epoch6_colab_vit_v12.pt, vit_deit_base_patch16_384_fold4_epoch13_colab_vit_v13.pt weights=[38,38,38,38,38,20,20,20,20,20] 3_tta\n\n* `v1`: \n    classification: tf_efficientnetv2_m_in21k_fold0_epoch17_colab_pseudo_labeling_v5.pt, tf_efficientnetv2_m_in21k_fold1_epoch0_colab_v33.pt, tf_efficientnetv2_m_in21k_fold2_epoch6_colab_pseudo_labeling_v7.pt, tf_efficientnetv2_m_in21k_fold3_epoch15_colab_pseudo_labeling_v8.pt, tf_efficientnetv2_m_in21k_fold4_epoch26_colab_pseudo_labeling_v9.pt, vit_deit_base_patch16_384_fold0_epoch10_colab_vit_v7.pt, vit_deit_base_patch16_384_fold1_epoch9_colab_vit_v9.pt, vit_deit_base_patch16_384_fold2_epoch14_colab_vit_v11.pt, vit_deit_base_patch16_384_fold3_epoch6_colab_vit_v12.pt, vit_deit_base_patch16_384_fold4_epoch13_colab_vit_v13.pt weights=[38,38,38,38,38,37,37,37,37,37] 3_tta\n    \n    2-class: tf_efficientnet_v2 + vit_deit_base_patch16_384_fold0_epoch8_colab_v6.pt + vit_deit_base_patch16_384_fold1_epoch8_colab_v7.pt + vit_deit_base_patch16_384_fold2_epoch9_colab_v8.pt weights=[59,59,59,59,59,58,58,58] 3_tta\n    \n    detection: yolov5x_fold0_nb_v14_best.pt + yolov5x_fold1_nb_v19_best.pt + yolov5x_fold2_nb_v20_best.pt + siimcovid19-effdet\/effdet5_640_fold3_epoch20.bin + siimcovid19-effdet\/effdet5_640_fold4_epoch24.bin nms_0.5 img_640 + cascade_rcnn_x101_32x4d_fpn_1x_fold0_epoch10_public_nb_v8.pth nms_iou_0.5 wbf iou=0.6 weight=[54,54,54,54,54,58,58,58,58,58,54] skipbox_thr=0.01 tta (1 - none_score) ** 0.5 img_640","eeb5ad3f":"## Dataset"}}