{"cell_type":{"0846505f":"code","c33e423c":"code","87e243d8":"code","34ea9288":"code","636171c2":"code","d62a7fe9":"code","56754c36":"code","20a01dba":"code","3484ec0f":"code","f167022e":"code","0cf871a9":"code","417de638":"code","d8a07f14":"code","fcb39044":"code","eb04d53b":"code","1201b706":"code","f6cdd446":"code","604e3153":"code","48d4fc58":"code","af413203":"code","3f61d763":"code","5bb0518c":"code","5d49000a":"code","654a1537":"code","9a804117":"code","949678d7":"code","81fc1a09":"code","48259e58":"code","7ec554ef":"code","c59913ca":"code","ae4cef3c":"code","42f6f21d":"code","14c8a6f7":"code","654c5665":"code","506e417a":"code","3c11ef9f":"code","412e8ecd":"markdown","7a7441b7":"markdown","59cb0d47":"markdown","76f522a3":"markdown","be863a12":"markdown","61b2898d":"markdown","654c1362":"markdown","580c0bf2":"markdown","7a7030a6":"markdown","7571482d":"markdown","9c37c1ca":"markdown","723b0aa2":"markdown","1de195d7":"markdown","f5faaada":"markdown","e5cdb15c":"markdown","e2128032":"markdown"},"source":{"0846505f":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport time\nfrom datetime import datetime\n\n# ML libraries\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error","c33e423c":"#submission = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-1\/submission.csv\")\ntest = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-1\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-1\/train.csv\")\ndisplay(train.head(5))\ndisplay(train.describe())\nprint(\"Number of Country\/Region: \", train['Country\/Region'].nunique())\nprint(\"Dates go from day\", max(train['Date']), \"to day\", min(train['Date']), \", a total of\", train['Date'].nunique(), \"days\")","87e243d8":"print(\"Number of Country\/Region: \", test['Country\/Region'].nunique())\nprint(\"Dates go from day\", max(test['Date']), \"to day\", min(test['Date']), \", a total of\", test['Date'].nunique(), \"days\")","34ea9288":"# Merge train and test, exclude overlap from test set so that the forecast id are correct\n#dates_overlap = ['2020-03-12','2020-03-13','2020-03-14','2020-03-15','2020-03-16','2020-03-17','2020-03-18','2020-03-19','2020-03-20','2020-03-21','2020-03-22']\ndates=set(test.Date)\ndates_overlap=set(train[train['Date'].isin(dates)]['Date'])\ndates_prediction=dates-dates_overlap\n\n# train2 = train.loc[~train['Date'].isin(dates_overlap)]\ntest2 = test.loc[~test['Date'].isin(dates_overlap)]\nall_data = pd.concat([train, test2], axis = 0, sort=False)\nall_data=all_data.sort_values(by=['Country\/Region','Province\/State','Date'])\n\n#update the forecastid from test set\nall_data.loc[all_data['Date']>='2020-03-12','ForecastId']=test['ForecastId'].to_list()\n\n# Double check that there are no informed ConfirmedCases and Fatalities after 2020-03-11\n# all_data.loc[all_data['Date'] >= '2020-03-12', 'ConfirmedCases'] = np.nan\n# all_data.loc[all_data['Date'] >= '2020-03-12', 'Fatalities'] = np.nan\nall_data['Date'] = pd.to_datetime(all_data['Date'])\n\n# Create column Day, label encoding Date\nle = preprocessing.LabelEncoder()\nall_data['Day'] = le.fit_transform(all_data.Date)\n\n# # Country wise days since 1st case\ncountrydate = all_data[all_data['ConfirmedCases']>0].groupby('Country\/Region').agg({\"Date\":'min'}).reset_index()\ncountrydate.columns=['Country\/Region','Dayof1stcase']\nall_data=all_data.merge(countrydate, left_on='Country\/Region', right_on='Country\/Region', how='left')\nall_data['Dayofcases'] = (all_data['Date']-all_data['Dayof1stcase']).dt.days\nall_data.loc[~(all_data['Dayofcases']>0),'Dayofcases']=-1\nall_data=all_data.drop(columns=['Dayof1stcase'])\n\n# Aruba has no Lat nor Long. Inform it manually\nall_data.loc[all_data['Lat'].isna()==True, 'Lat'] = 12.510052\nall_data.loc[all_data['Long'].isna()==True, 'Long'] = -70.009354\n\n# Fill null values given that we merged train-test datasets\nall_data['Province\/State'].fillna(\"None\", inplace=True)\nall_data['ConfirmedCases'].fillna(0, inplace=True)\nall_data['Fatalities'].fillna(0, inplace=True)\nall_data['Id'].fillna(-1, inplace=True)\nall_data['ForecastId'].fillna(-1, inplace=True)\n\n#Add day of week and month\nall_data['dayofweek'] = all_data['Date'].dt.dayofweek\n# all_data['month'] = all_data['Date'].dt.month\n# all_data['dayofyear'] = all_data['Date'].dt.dayofyear\n\ndisplay(all_data)\ndisplay(all_data.loc[all_data['Date'] == '2020-03-12'])","636171c2":"missings_count = {col:all_data[col].isnull().sum() for col in all_data.columns}\nmissings = pd.DataFrame.from_dict(missings_count, orient='index')\nprint(missings.nlargest(30, 0))","d62a7fe9":"world_population = pd.read_csv(\"\/kaggle\/input\/population-by-country-2020\/population_by_country_2020.csv\")\ncountry_data = pd.read_csv(\"\/kaggle\/input\/countryinfo\/covid19countryinfo.csv\")\ncontinent = pd.read_csv(\"\/kaggle\/input\/country-to-continent\/countryContinent.csv\", encoding = 'ISO-8859-1')\neconomy = pd.read_csv(\"\/kaggle\/input\/the-economic-freedom-index\/economic_freedom_index2019_data.csv\", encoding = 'ISO-8859-1')\n\n\n# Select desired columns and rename some of them\nworld_population = world_population[['Country (or dependency)', 'Population (2020)', 'Density (P\/Km\u00b2)', 'Land Area (Km\u00b2)', 'Med. Age', 'Urban Pop %']]\nworld_population.columns = ['country', 'Population (2020)', 'Density', 'Land Area', 'Med Age', 'Urban Pop']\n\ncountry_data = country_data[['country','quarantine', 'schools', 'restrictions', 'hospibed','smokers']]\ncountry_data.columns = ['country', 'Quarantine', 'Schools', 'Restrictions', 'Hospibed','Smokers']\n\ncontinent = continent[['country','continent','sub_region']]\ncontinent.columns = ['country', 'Continent','Sub_Region']\n\neconomy = economy[['Country Name', 'World Rank','Region Rank', '2019 Score', 'Property Rights', 'Judical Effectiveness',\n       'Government Integrity', 'Tax Burden', \"Gov't Spending\", 'Fiscal Health',\n       'Business Freedom', 'Labor Freedom', 'Monetary Freedom',\n       'Trade Freedom', 'Investment Freedom ', 'Financial Freedom',\n       'Tariff Rate (%)', 'Income Tax Rate (%)', 'Corporate Tax Rate (%)',\n       'Tax Burden % of GDP', \"Gov't Expenditure % of GDP \",  'GDP (Billions, PPP)', 'GDP Growth Rate (%)',\n       '5 Year GDP Growth Rate (%)', 'GDP per Capita (PPP)',\n       'Unemployment (%)', 'Inflation (%)', 'FDI Inflow (Millions)',\n       'Public Debt (% of GDP)']]\n\neconomy.columns=['country', 'World Rank','Region Rank', '2019 Score', 'Property Rights', 'Judical Effectiveness',\n       'Government Integrity', 'Tax Burden', \"Gov't Spending\", 'Fiscal Health',\n       'Business Freedom', 'Labor Freedom', 'Monetary Freedom',\n       'Trade Freedom', 'Investment Freedom ', 'Financial Freedom',\n       'Tariff Rate (%)', 'Income Tax Rate (%)', 'Corporate Tax Rate (%)',\n       'Tax Burden % of GDP', \"Gov't Expenditure % of GDP \",  'GDP (Billions, PPP)', 'GDP Growth Rate (%)',\n       '5 Year GDP Growth Rate (%)', 'GDP per Capita (PPP)',\n       'Unemployment (%)', 'Inflation (%)', 'FDI Inflow (Millions)',\n       'Public Debt (% of GDP)']","56754c36":"\n# Replace United States by US\nworld_population.loc[world_population['country']=='United States', 'country'] = 'US'\nworld_population.loc[world_population['country']==\"Gambia\",'country']='The Gambia'\nworld_population.loc[world_population['country']==\"Bahamas\",'country']='The Bahamas'\nworld_population.loc[world_population['country']==\"R\u00e9union\",'country']='Reunion'\nworld_population.loc[world_population['country']==\"Czech Republic (Czechia)\",'country']='Czechia'\nworld_population.loc[world_population['country']==\"DR Congo\",'country']='Congo (Kinshasa)'\nworld_population.loc[world_population['country']==\"Congo\",'country']='Congo (Brazzaville)'\nworld_population.loc[world_population['country']==\"C\u00f4te d'Ivoire\",'country']=\"Cote d'Ivoire\"\nworld_population.loc[world_population['country']==\"South Korea\",'country']=\"Korea, South\"\nworld_population.loc[world_population['country']==\"St. Vincent & Grenadines\",'country']='Saint Vincent and the Grenadines'\n\n# continent\ncontinent.loc[continent['country']==\"United States of America\",'country']='US'\ncontinent.loc[continent['country']==\"Bolivia (Plurinational State of)\",'country']='Bolivia'\ncontinent.loc[continent['country']==\"Brunei Darussalam\" ,'country'] = 'Brunei'\ncontinent.loc[continent['country']==\"Gambia\",'country']='The Gambia'\ncontinent.loc[continent['country']==\"Bahamas\",'country']='The Bahamas'\ncontinent.loc[continent['country']==\"R\u00e9union\",'country']='Reunion'\ncontinent.loc[continent['country']==\"Congo (Democratic Republic of the)\",'country']='Congo (Kinshasa)'\ncontinent.loc[continent['country']==\"Congo\",'country']='Congo (Brazzaville)'\ncontinent.loc[continent['country']==\"Czech Republic\",'country']='Czechia'\ncontinent.loc[continent['country']==\"C\u00f4te d'Ivoire\",'country']=\"Cote d'Ivoire\"\ncontinent.loc[continent['country']==\"Macedonia (the former Yugoslav Republic of)\",'country']=\"North Macedonia\"\ncontinent.loc[continent['country']==\"Viet Nam\",'country']='Vietnam'\ncontinent.loc[continent['country']==\"Venezuela (Bolivarian Republic of)\",'country']='Venezuela'\ncontinent.loc[continent['country']==\"United Kingdom of Great Britain and Northern Ireland\",'country']='United Kingdom'\ncontinent.loc[continent['country']==\"Tanzania, United Republic of\",'country']='Tanzania'\ncontinent.loc[continent['country']==\"Russian Federation\",'country']='Russia'\ncontinent.loc[continent['country']==\"Moldova (Republic of)\",'country']='Moldova'\ncontinent.loc[continent['country']==\"Korea (Republic of)\",'country']='Korea, South'\ncontinent.loc[continent['country']==\"Iran (Islamic Republic of)\",'country']='Iran'\ncontinent.loc[continent['country']==\"Iran (Islamic Republic of)\",'country']='Iran'\ncontinent.loc[continent['country']==\"Iran (Islamic Republic of)\",'country']='Iran'\n\n\n# actual data\n# all_data.loc[train['Country\/Region']==\"Republic of the Congo\",'country']='Congo (Kinshasa)'\nall_data.loc[all_data['Country\/Region']==\"Gambia, The\",'Country\/Region']='The Gambia'","20a01dba":"# DEMOGRAPHICS\n# Remove the % character from Urban Pop values\nworld_population['Urban Pop'] = world_population['Urban Pop'].str.rstrip('%')\n\n# Replace Urban Pop and Med Age \"N.A\" by their respective modes, then transform to int\nworld_population.loc[world_population['Urban Pop']=='N.A.', 'Urban Pop'] = int(world_population.loc[world_population['Urban Pop']!='N.A.', 'Urban Pop'].mode()[0])\nworld_population['Urban Pop'] = world_population['Urban Pop'].astype('int16')\nworld_population.loc[world_population['Med Age']=='N.A.', 'Med Age'] = int(world_population.loc[world_population['Med Age']!='N.A.', 'Med Age'].mode()[0])\nworld_population['Med Age'] = world_population['Med Age'].astype('int16')\n\nprint(\"Cleaned country details dataset\")\ndisplay(world_population)\n\n# Now join the dataset to our previous DataFrame and clean missings (not match in left join)\nprint(\"Enriched dataset\")\nall_data = all_data.merge(world_population, left_on='Country\/Region', right_on='country', how='left')\nall_data[['Population (2020)', 'Density', 'Land Area', 'Med Age', 'Urban Pop']] = all_data[['Population (2020)', 'Density', 'Land Area', 'Med Age', 'Urban Pop']].fillna(0)\nall_data=all_data.drop(columns=['country'])\n# display(all_data)\n\n\n# CONTINENT INFO\n# Now join the dataset to our previous DataFrame and clean missings (not match in left join)\nprint(\"Enriched dataset\")\nall_data = all_data.merge(continent, left_on='Country\/Region', right_on='country', how='left')\nall_data[['Continent','Sub_Region']] = all_data[['Continent','Sub_Region']].fillna('Other')\nall_data=all_data.drop(columns=['country'])\n# display(all_data)\n\n\n# ECONOMY DATA\n# Remove the $ character from GDP values\neconomy['GDP per Capita (PPP)'] = economy['GDP per Capita (PPP)'].str.strip('$')\neconomy['GDP (Billions, PPP)']  = economy['GDP (Billions, PPP)'].str.strip('$')\n\n# Now join the dataset to our previous DataFrame and clean missings (not match in left join)\nprint(\"Enriched dataset\")\nall_data = all_data.merge(economy, left_on='Country\/Region', right_on='country', how='left')\nall_data[economy.columns] = all_data[economy.columns].fillna('0')\nall_data=all_data.drop(columns=['country'])\n# display(all_data)\nall_data['GDP per Capita (PPP)'].str.split('(').apply(lambda x: x[0]).str.replace(\",\",\"\").astype(float)\nall_data['GDP (Billions, PPP)'].str.split('\\ ').apply(lambda x: x[0]).str.replace(\",\",\"\").astype(float)\n\n\n# Covid Country Hospital Smokers Quarantine Data\ncountry_data['Smokers'].fillna(country_data.Smokers.mode()[0],inplace=True)\ncountry_data['Hospibed'].fillna(country_data.Hospibed.mode()[0],inplace=True)\ncountry_data['Quarantine']=pd.to_datetime(country_data['Quarantine'])\ncountry_data['Schools']=pd.to_datetime(country_data['Schools'])\ncountry_data['Restrictions']=pd.to_datetime(country_data['Restrictions'])\n\n\nprint(\"Enriching country actions dataset\")\nall_data = all_data.merge(country_data, left_on='Country\/Region', right_on='country', how='left')\nall_data['Smokers'].fillna(country_data.Smokers.mode()[0],inplace=True)\nall_data['Hospibed'].fillna(country_data.Hospibed.mode()[0],inplace=True)\nall_data=all_data.drop(columns=['country'])\ndisplay(all_data)\n\n","3484ec0f":"# Quarantine info flags\nall_data['quarantine_flag']=(pd.to_datetime(all_data['Quarantine'])<all_data['Date'])\nall_data['schools_flag']=(pd.to_datetime(all_data['Schools'])<all_data['Date'])\nall_data['restrictions_flag']=(pd.to_datetime(all_data['Restrictions'])<all_data['Date'])","f167022e":"def calculate_trend(df, lag_list, column):\n    for lag in lag_list:\n        trend_column_lag = \"Trend_\" + column + \"_\" + str(lag)\n        df[trend_column_lag] = (df[column].shift(lag, fill_value=0)-df[column].shift(lag+1, fill_value=-999))\/df[column].shift(lag+1, fill_value=0)\n    return df\n\n\ndef calculate_lag(df, lag_list, column):\n    for lag in lag_list:\n        column_lag = column + \"_\" + str(lag)\n        df[column_lag] = df[column].shift(lag, fill_value=0)\n    return df\n\n\nts = time.time()\nall_data=all_data.sort_values(by=['Country\/Region','Province\/State','Date'])\n\nall_data = calculate_lag(all_data, range(1,7), 'ConfirmedCases')\nall_data = calculate_lag(all_data, range(1,7), 'Fatalities')\nall_data = calculate_trend(all_data, [1], 'ConfirmedCases')\nall_data = calculate_trend(all_data, [1], 'Fatalities')\nall_data.replace([np.inf, -np.inf], 0, inplace=True)\nall_data.fillna(0, inplace=True)\nprint(\"Time spent: \", time.time()-ts)","0cf871a9":"raw_data=all_data.copy()","417de638":"# Label encode country names\ndata = all_data.copy()\ndata = data.drop(columns=['Quarantine','Schools', 'Restrictions'])\n\ndata['Country\/Region'] = le.fit_transform(data['Country\/Region'])\n\n# Save dictionary for exploration purposes\nnumber = data['Country\/Region']\ncountries = le.inverse_transform(data['Country\/Region'])\ncountry_dict = dict(zip(countries, number)) \n\ndata['Continent'] = le.fit_transform(data['Continent'])\ndata['Sub_Region'] = le.fit_transform(data['Sub_Region'])\n","d8a07f14":"# data = data[['Date','Id', 'ForecastId','Lat', 'Long', 'Country\/Region', 'ConfirmedCases', 'Fatalities', \n#        'Day', 'Dayofcases', 'dayofweek', 'ConfirmedCases_1', 'ConfirmedCases_2',\n#        'ConfirmedCases_3', 'ConfirmedCases_4', 'ConfirmedCases_5',\n#        'ConfirmedCases_6', 'Fatalities_1', 'Fatalities_2', 'Fatalities_3',\n#        'Fatalities_4', 'Fatalities_5', 'Fatalities_6',\n#        'Trend_ConfirmedCases_1', 'Trend_Fatalities_1','Population (2020)', 'Density', 'Land Area',\n#        'Med Age', 'Urban Pop', 'Continent', 'Sub_Region','Hospibed', 'Smokers', 'quarantine_flag',\n#        'schools_flag', 'restrictions_flag']]","fcb39044":"\ndef split_data_with_custom_date(data,valid_date,end_date):\n    \n    # Train set\n    X_train   = data[(data.ForecastId == -1)&(data.Date<valid_date)].drop(['ConfirmedCases', 'Fatalities'], axis=1)\n    Y_train_1 = data[(data.ForecastId == -1)&(data.Date<valid_date)]['ConfirmedCases']\n    Y_train_2 = data[(data.ForecastId == -1)&(data.Date<valid_date)]['Fatalities']\n\n    # Test set\n    X_test = data[(data.ForecastId == -1)&(data.Date>=valid_date)&(data.Date<end_date)].drop(['ConfirmedCases', 'Fatalities'], axis=1)\n    #Y_test_1 = test['ConfirmedCases']\n    #Y_test_2 = test['Fatalities']\n\n    # Test set\n    #X_test = data[data.Day > day_valid].drop(['ConfirmedCases', 'Fatalities'], axis=1)\n    \n    X_train.drop(columns=['Id','ForecastId','Date'], inplace=True, errors='ignore')\n    \n#     X_test.drop('Id', inplace=True, errors='ignore')\n    index = X_test['ForecastId']\n    X_test.drop(columns=['Id','ForecastId'], inplace=True, errors='ignore')\n#     testdata=X_test.copy()\n#     X_test.drop('Date', inplace=True, errors='ignore')\n    \n    return X_train, Y_train_1, Y_train_2, X_test, index\n\n\n\ndef split_data(data):\n        # Train set\n    new_train=data[data.ForecastId == -1]\n    X_train = new_train.drop(['Province\/State','ConfirmedCases', 'Fatalities'], axis=1)\n    Y_train_1 = new_train['ConfirmedCases']\n    Y_train_2 = new_train['Fatalities']\n\n    # Valid set\n    valid=data[(data.ForecastId != -1)&(data.Date.isin(dates_overlap))]\n    X_valid = valid.drop(['Province\/State','ConfirmedCases', 'Fatalities'], axis=1)\n    Y_valid_1 = valid['ConfirmedCases']\n    Y_valid_2 = valid['Fatalities']\n\n    # Test set\n    new_test=data[(data.ForecastId != -1)&(data.Date.isin(dates))]\n    X_test = new_test.drop(['Province\/State','ConfirmedCases', 'Fatalities'], axis=1)\n\n    # Test set\n    #X_test = data[data.Day > day_valid].drop(['ConfirmedCases', 'Fatalities'], axis=1)\n    \n    X_train.drop(columns=['Id','ForecastId','Date'], inplace=True, errors='ignore')\n    \n    valid_index= X_valid['ForecastId']\n    test_index = X_test['ForecastId']\n    X_valid.drop(columns=['Id','ForecastId'], inplace=True, errors='ignore')\n    X_test.drop(columns=['Id','ForecastId'], inplace=True, errors='ignore')\n#     testdata=X_test.copy()\n#     X_test.drop('Date', inplace=True, errors='ignore')\n\n    return new_train, X_train, Y_train_1, Y_train_2, valid, X_valid, Y_valid_1, Y_valid_2, new_test, X_test, valid_index, test_index","eb04d53b":"restrictions=all_data[all_data['restrictions_flag']==True].groupby('Country\/Region').agg({\"Date\":'min'})\nrestrictions.columns=['restricted_date']\nnew_data = all_data.merge(restrictions, left_on=['Country\/Region','Date'], right_on=['Country\/Region','restricted_date'], how='inner')\nnew_data.head()","1201b706":"new_data[['ForecastId','restricted_date','Dayofcases','ConfirmedCases','Country\/Region','Province\/State']]","f6cdd446":"def recreate_valid_split(data):\n    # Valid set\n    valid   = data[(data.ForecastId != -1)&(data.Date.isin(dates_overlap))]\n    X_valid = valid.drop(['Province\/State','ConfirmedCases', 'Fatalities'], axis=1)\n    Y_valid_1 = valid['ConfirmedCases']\n    Y_valid_2 = valid['Fatalities']\n    X_valid.drop(columns=['Id','ForecastId'], inplace=True, errors='ignore')\n#     testdata=X_test.copy()\n#     X_test.drop('Date', inplace=True, errors='ignore')\n    return X_valid#, Y_valid_1, Y_valid_2\n\ndef recreate_submission_split(data):\n    # Valid set\n    sub   = data[(data.ForecastId != -1)&(data.Date.isin(dates))]\n    X_sub = sub.drop(['Province\/State','ConfirmedCases', 'Fatalities'], axis=1)\n    Y_sub_1 = sub['ConfirmedCases']\n    Y_sub_2 = sub['Fatalities']\n    X_sub.drop(columns=['Id','ForecastId'], inplace=True, errors='ignore')\n#     testdata=X_test.copy()\n#     X_test.drop('Date', inplace=True, errors='ignore')\n    return X_sub#, Y_valid_1, Y_valid_2\n\ndef recalculate_lags(df):\n    df = df.sort_values(by=['Country\/Region','Province\/State','Date'])\n    df = calculate_lag(df, range(1,7), 'ConfirmedCases')\n    df = calculate_lag(df, range(1,7), 'Fatalities')\n    df = calculate_trend(df, [1], 'ConfirmedCases')\n    df = calculate_trend(df, [1], 'Fatalities')\n    df.replace([np.inf, -np.inf], 0, inplace=True)\n    df.fillna(0, inplace=True)\n    return df\n#     print(\"Time spent: \", time.time()-ts)","604e3153":"economy_columns=['World Rank', 'Region Rank', '2019 Score',\n       'Property Rights', 'Judical Effectiveness', 'Government Integrity',\n       'Tax Burden', \"Gov't Spending\", 'Fiscal Health', 'Business Freedom',\n       'Labor Freedom', 'Monetary Freedom', 'Trade Freedom',\n       'Investment Freedom ', 'Financial Freedom', 'Tariff Rate (%)',\n       'Income Tax Rate (%)', 'Corporate Tax Rate (%)', 'Tax Burden % of GDP',\n       \"Gov't Expenditure % of GDP \", 'GDP (Billions, PPP)',\n       'GDP Growth Rate (%)', '5 Year GDP Growth Rate (%)',\n       'GDP per Capita (PPP)', 'Unemployment (%)', 'Inflation (%)',\n       'FDI Inflow (Millions)', 'Public Debt (% of GDP)']\n","48d4fc58":"new_train,X_train, Y_train_1, Y_train_2, valid,X_valid, Y_valid_1, Y_valid_2, new_test, X_test, valid_index, test_index=split_data(data.drop(columns=economy_columns))","af413203":"from sklearn.ensemble import RandomForestRegressor\nimport  xgboost as xgb\nfrom sklearn.metrics import mean_squared_log_error","3f61d763":"random_grid={'bootstrap': [True, False],\n 'max_depth': [10, 30, 50, None],\n 'max_features': ['auto', 'sqrt'],\n 'min_samples_leaf': [1, 2, 4],\n 'min_samples_split': [2, 5, 10],\n 'n_estimators': [100,200,300,400]}\n\nrf = RandomForestRegressor()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 2, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, Y_train_1)\n\nrf = RandomForestRegressor()\nrf_random2 = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 2, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random2.fit(X_train, Y_train_2)","5bb0518c":"# regr1=RandomForestRegressor(bootstrap=True,max_depth=None, max_features='auto', max_leaf_nodes=None,\n#                             n_estimators=150, random_state=None, n_jobs=1, verbose=0)\n# regr1.fit(X_train, Y_train_1)\n\n# regr2=RandomForestRegressor(bootstrap=True,max_depth=None, max_features='auto', max_leaf_nodes=None,\n#                             n_estimators=150, random_state=None, n_jobs=1, verbose=0)\n# regr2.fit(X_train, Y_train_2)\n# # X_test.head()","5d49000a":"regr1=rf_random.best_estimator_\nregr1.fit(X_train, Y_train_1)\n\nregr2=rf_random2.best_estimator_\nregr2.fit(X_train, Y_train_2)\n# X_test.head()","654a1537":"display(regr1.feature_importances_)\ndisplay(X_train.columns)\ndisplay(regr2.feature_importances_)","9a804117":"x_pred=valid.copy()\npredictions1=regr1.predict(X_valid.drop(columns=['Date']))\nx_pred['predictions1']=predictions1\npredictions2=regr2.predict(X_valid.drop(columns=['Date']))\nx_pred['predictions2']=predictions2\n\nxpred = x_pred[['Date','Country\/Region','ConfirmedCases','Fatalities','predictions1','predictions2']]\ndisplay(xpred[xpred['Country\/Region']==74])\ndisplay(xpred[xpred['Country\/Region']==68])","949678d7":"print(np.sqrt(mean_squared_log_error( Y_valid_1, predictions1 )))\nprint(np.sqrt(mean_squared_log_error( Y_valid_2, predictions2 )))","81fc1a09":"# valid_dataset=X_valid.drop(columns=['Date'])\n# confirmed=sorted(X_valid.columns[X_valid.columns.str.contains('Confirmed')])\n# fatalities=sorted(X_valid.columns[X_valid.columns.str.contains('Fatalities')])\n# train,X_train, Y_train_1, Y_train_2, valid,X_valid, Y_valid_1, Y_valid_2, test,X_test, valid_index, test_index=split_data(data.drop(columns=economy_columns))\ntemp_data=data.copy()\ntemp_x_valid= X_valid.copy()\nx_pred=valid.copy() # Final predictions\nx_pred['Predictions1']=0\nx_pred['Predictions2']=0\n\nfor date in sorted(dates_overlap):  \n    if date>min(dates_overlap):\n        temp_data.loc[temp_data['ForecastId'].isin(forecastids),'ConfirmedCases']=predictions1\n        temp_data.loc[temp_data['ForecastId'].isin(forecastids),'Fatalities']=predictions2\n        temp_data = recalculate_lags(temp_data)\n        temp_x_valid = recreate_valid_split(temp_data.drop(columns=economy_columns))\n    forecastids=temp_data[temp_data['Date']==date]['ForecastId']\n    valid_dataset=temp_x_valid[temp_x_valid['Date']==date].drop(columns=['Date'])\n    predictions1=regr1.predict(valid_dataset)\n    predictions2=regr2.predict(valid_dataset)\n    x_pred.loc[x_pred['ForecastId'].isin(forecastids),'Predictions1']=predictions1\n    x_pred.loc[x_pred['ForecastId'].isin(forecastids),'Predictions2']=predictions2\n\n    ","48259e58":"temp_data.head()","7ec554ef":"print(np.sqrt(mean_squared_log_error( Y_valid_1, x_pred['Predictions1'] )))\nprint(np.sqrt(mean_squared_log_error( Y_valid_2, x_pred['Predictions2'] )))\n\nprint(np.sqrt(mean_squared_log_error( Y_valid_1, np.round(x_pred['Predictions1']) )))\nprint(np.sqrt(mean_squared_log_error( Y_valid_2, np.round(x_pred['Predictions2']) )))","c59913ca":"print(np.sqrt(mean_squared_log_error( Y_valid_1[x_pred['Date']>='2020-03-20'], x_pred[x_pred['Date']>='2020-03-20']['Predictions1'] )))\nprint(np.sqrt(mean_squared_log_error( Y_valid_2[x_pred['Date']>='2020-03-20'], x_pred[x_pred['Date']>='2020-03-20']['Predictions2'] )))","ae4cef3c":"# train,X_train, Y_train_1, Y_train_2, valid,X_valid, Y_valid_1, Y_valid_2, test,X_test, valid_index, test_index = split_data(data.drop(columns=economy_columns))\ntemp_data=data.copy()\ntemp_x_sub= X_test.copy()\nx_pred2=new_test.copy() # Final predictions\nx_pred2['Predictions1']=0\nx_pred2['Predictions2']=0\n\nfor date in sorted(dates):  \n    if date>min(dates):\n        temp_data.loc[temp_data['ForecastId'].isin(forecastids),'ConfirmedCases']=predictions1\n        temp_data.loc[temp_data['ForecastId'].isin(forecastids),'Fatalities']=predictions2\n        temp_data = recalculate_lags(temp_data)\n        temp_x_sub = recreate_submission_split(temp_data.drop(columns=economy_columns))\n    forecastids=temp_data[temp_data['Date']==date]['ForecastId']\n    sub_dataset=temp_x_sub[temp_x_sub['Date']==date].drop(columns=['Date'])\n    predictions1=regr1.predict(sub_dataset)\n    predictions2=regr2.predict(sub_dataset)\n    x_pred2.loc[x_pred2['ForecastId'].isin(forecastids),'Predictions1']=predictions1\n    x_pred2.loc[x_pred2['ForecastId'].isin(forecastids),'Predictions2']=predictions2\n\n    ","42f6f21d":"print(np.sqrt(mean_squared_log_error( Y_valid_1, np.ceil(x_pred2[x_pred2['Date']<='2020-03-23']['Predictions1']) )))\nprint(np.sqrt(mean_squared_log_error( Y_valid_2, np.round(x_pred2[x_pred2['Date']<='2020-03-23']['Predictions2']) )))","14c8a6f7":"def get_submission(x_pred):\n    # Submit predictions\n#     submission = pd.DataFrame({\n#         \"ForecastId\": index, \n#         \"ConfirmedCases\": prediction1,\n#         \"Fatalities\": predictions2\n#     })\n    submission=x_pred[['ForecastId','Predictions1','Predictions2']].copy()\n    submission.columns=['ForecastId','ConfirmedCases','Fatalities']\n    submission.loc[:,'ForecastId']=submission['ForecastId'].astype(int)\n    submission.loc[:,'ConfirmedCases']=np.ceil(submission['ConfirmedCases'])\n    submission.loc[:,'Fatalities']=np.round(submission['Fatalities'])\n    \n#     submission['ConfirmedCases']=np.ceil(submission['ConfirmedCases'])\n    submission.to_csv('submission.csv', index=False)\n    ","654c5665":"get_submission(x_pred2)\n# type(x_pred['ForecastId'][50])","506e417a":"# new_test[new_test['Province\/State']=='New Brunswick'].head(50)","3c11ef9f":"# country_dict","412e8ecd":"## One shot predictions based on forecasted data points","7a7441b7":"The dataset covers 163 countries and almost 2 full months from 2020, which is enough data to get some clues about the pandemic.","59cb0d47":"## Submissions Part","76f522a3":"Using RandomizedSearchCV","be863a12":"## 2.3 Compute lags and trends\n\nEnriching a dataset is key to obtain good results. In this case we will apply 2 different transformations:\n\n**Lag**. Lags are a way to compute the previous value of a column, so that the lag 1 for ConfirmedCases would inform the this column from the previous day. The lag 3 of a feature X is simply:\n$$X_{lag3}(t) = X(t-3)$$\n\n\n**Trend**. Transformig a column into its trend gives the natural tendency of this column, which is different from the raw value. The definition of trend I will apply is: \n$$Trend_{X} = {X(t) - X(t-1) \\over X(t-1)}$$\n\nThe backlog of lags I'll apply is 14 days, while for trends is 7 days.  For ConfirmedCases and Fatalities:","61b2898d":"As you see, the process is really fast. An example of some of the lag\/trend columns for Spain:","654c1362":"## 4.1 Random Forest Regressor \n\nRecalculate lags and trends after each date of predictions","580c0bf2":"# Prepare data for Fitting model","7a7030a6":"# StepFunction Team COVID Global Forecast\n\nIn the context of the global COVID-19 pandemic, Kaggle has launched several challenges in order to provide useful insights that may answer some of the open scientific questions about the virus. This is the case of the [COVID19 Global Forecasting](https:\/\/www.kaggle.com\/c\/covid19-global-forecasting-week-1), in which participants are encouraged to fit worldwide data in order to predict the pandemic evolution, hopefully helping to determine factors that impact the transmission rate of COVID-19.","7571482d":"**Observations**: \n* \"ConfirmedCases\" and \"Fatalities\" are now only informed for dates previous to 2020-03-12\n* The dataset includes all countries and dates, which is required for the lag\/trend step\n* Missing values for \"ConfirmedCases\" and \"Fatalities\" have been replaced by 0, which may be dangerous if we do not remember it at the end of the process. However, since we will train only on dates previous to 2020-03-12, this won't impact our prediction algorithm\n* A new column \"Day\" has been created, as a day counter starting from the first date\n\nDouble-check that there are no remaining missing values:","9c37c1ca":"## By using actual data from last day (next day prediction only\/ lesser error )","723b0aa2":"# 1. Load Data <a id=\"section1\"><\/a>\n\nFirst of all, let's take a look on the data structure:","1de195d7":"## 2.2. Add country details\n\nVariables like the total population of a country, the average age of citizens or the fraction of peoople living in cities may strongly impact on the COVID-19 transmission behavior. Hence, it's important to consider these factors. I'm using [Tanu's dataset](https:\/\/www.kaggle.com\/tanuprabhu\/population-by-country-2020) based on Web Scrapping for this purpose.","f5faaada":"# 2. Data enrichment <a id=\"section3\"><\/a>\n\nMain workflow of this section:\n1. Join data, filter dates and clean missings\n2. Add country related details\n3. Compute lags and trends\n\n\n**Disclaimer**: this data enrichment is not mandatory and we could end up not using all of the new features in our models. However I consider it a didactical step that will surely add some value, for example in an in-depth exploratory analysis.","e5cdb15c":"# 4. Predictions with machine learning <a id=\"section4\"><\/a>\n\nOur obective in this section consists on  predicting the evolution of the expansion from a data-centric perspective, like any other regression problem. To do so, remember that the challenge specifies that submissions on the public LB should only contain data previous to 2020-03-12.\n\nModels to apply:\n1. Random Forest\n","e2128032":"## 2.1. Join data, filter dates and clean missings\n\nFirst of all, we perform some pre-processing prepare the dataset, consisting on:\n\n* **Join data**. Join train\/test to facilitate data transformations\n* **Filter dates**. According to the challenge conditions, remove ConfirmedCases and Fatalities post 2020-03-12\n* **Missings**. Analyze and fix missing values"}}