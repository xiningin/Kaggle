{"cell_type":{"52d40bae":"code","e68e92d9":"code","b61fb83b":"code","89c4bf0d":"code","d312529b":"code","21561981":"code","09554960":"code","6668c19c":"code","46e75f1e":"code","a002e9d1":"code","64d13f92":"code","bb5b9181":"code","26b6d9f8":"code","864f75e1":"code","eb5ac3e1":"code","717391bd":"code","c42141ea":"code","90960c10":"code","0c64ce97":"code","77d4cb7f":"code","877d223a":"markdown"},"source":{"52d40bae":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport json\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e68e92d9":"root_path =\"\/kaggle\/input\/tabular-playground-series-may-2021\/\"\ntrain = pd.read_csv(os.path.join(root_path, 'train.csv'))\ntest = pd.read_csv(os.path.join(root_path, 'test.csv'))\nsample_submission = pd.read_csv(os.path.join(root_path, 'sample_submission.csv'))","b61fb83b":"# train.head()\nunique_targets = train['target'].unique().tolist()\nlabel_mapping = dict(zip(unique_targets, [int(i[-1]) - 1 for i in unique_targets]))\n# label_mapping\ntrain['target'] = train['target'].map(label_mapping)\n# train.head(5).T","89c4bf0d":"y_train = train['target'][:95000]\nx_train = train.drop(['id', 'target'], axis=1)[:95000]\nx_val = x_train[-5000:]\ny_val = y_train[-5000:]\ntest = test.drop(['id'],axis=1)","d312529b":"print(x_train.shape)\nprint(y_train.shape)\nprint(x_val.shape)\nprint(y_val.shape)\nprint(test.shape)\nprint(np.unique(y_train))\nprint(sample_submission.shape)","21561981":"pipeline_lr=Pipeline([('scalar1',StandardScaler()),\n                     ('pca1',PCA(n_components=2)),\n                     ('lr_classifier',LogisticRegression(random_state=0))])\npipeline_randomforest=Pipeline([('scalar3',StandardScaler()),\n                     ('pca3',PCA(n_components=2)),\n                     ('rf_classifier',RandomForestClassifier())])\npipeline_lightgbm  = Pipeline([('scalar3',StandardScaler()),\n                              ('lgbm_classifier',LGBMClassifier(n_jobs=-1))])\n\n","09554960":"pipelines = [pipeline_lr,  pipeline_randomforest,pipeline_lightgbm]\n","6668c19c":"for pipe in pipelines:\n    pipe.fit(x_train,y_train)","46e75f1e":"pipe_dict = {0: 'Logistic Regression', 1: 'RandomForest',2:'lightGBM'}\n\nfor  i,model in enumerate(pipelines):\n    print(pipe_dict[i],model.score(x_val,y_val))","a002e9d1":"# test_output = pipelines[0].predict_proba(test)\nx_train_new = pd.concat([x_train,x_val],axis=0,ignore_index= True)\ny_train_new = pd.concat([y_train,y_val],axis=0,ignore_index= True)\n#([train, test], axis = 0, ignore_index = True\nprint(x_train_new.shape,y_train_new.shape)","64d13f92":"# train on all data\n# final_model  = pipelines[2].fit(x_train_new,y_train_new)\n","bb5b9181":"# grid-search params on final_model\nparams = {'boosting_type': 'gbdt',\n          'max_depth' : -1,\n          'objective': 'binary',\n          'device_type':'cuda',\n          'nthread': 3, # Updated from nthread\n          'num_leaves': 64,\n          'learning_rate': 0.05,\n          'max_bin': 512,\n          'subsample_for_bin': 200,\n          'subsample': 1,\n          'subsample_freq': 1,\n          'colsample_bytree': 0.8,\n          'reg_alpha': 5,\n          'reg_lambda': 10,\n          'min_split_gain': 0.5,\n          'min_child_weight': 1,\n          'min_child_samples': 5,\n          'scale_pos_weight': 1,\n          'num_class' : 1,\n          'metric' : 'binary_error',\n         }\n\n# Create parameters to search\ngridParams = {\n    'learning_rate': [0.005],\n    'n_estimators': [40],\n    'num_leaves': [6,8,12,16],\n    'boosting_type' : ['gbdt'],\n    'objective' : ['binary'],\n    'random_state' : [501], # Updated from 'seed'\n    'colsample_bytree' : [0.65, 0.66],\n    'subsample' : [0.7,0.75],\n    'reg_alpha' : [1,1.2],\n    'reg_lambda' : [1,1.2,1.4],\n    }\n# gridParams = {\n#     'n_estimators': [40],\n#     'boosting_type' : ['gbdt'],\n#     'objective' : ['binary'],\n#     'random_state' : [501], # Updated from 'seed'\n\n#     }\n\nmdl = LGBMClassifier(boosting_type= 'gbdt',\n          objective = 'binary',\n          n_jobs = 3, # Updated from 'nthread'\n          silent = True,\n                     device_type='cuda',\n          max_depth = params['max_depth'],\n          max_bin = params['max_bin'],\n          subsample_for_bin = params['subsample_for_bin'],\n          subsample = params['subsample'],\n          subsample_freq = params['subsample_freq'],\n          min_split_gain = params['min_split_gain'],\n          min_child_weight = params['min_child_weight'],\n          min_child_samples = params['min_child_samples'],\n          scale_pos_weight = params['scale_pos_weight'])\n\n# create a gridsearch of the pipeline, the fit the best model\ngridsearch = GridSearchCV(mdl, gridParams, cv=5, verbose=0,n_jobs=-1) # Fit grid search","26b6d9f8":"gridsearch.fit(x_train_new, y_train_new)","864f75e1":"test_output = gridsearch.best_estimator_.predict_proba(test)","eb5ac3e1":"# test_output = final_model.predict_proba(test)","717391bd":"predictions_df = pd.DataFrame(test_output, columns = [\"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\"])\npredictions_df['id'] = sample_submission['id']","c42141ea":"predictions_df.to_csv(\"submission3.csv\",index=False)","90960c10":"! pip install kaggle\n","0c64ce97":"api_token= {\"username\":\"lol\",\"key\":\"lol\"}\nwith open('\/root\/.kaggle\/kaggle.json', 'w') as file:\n    json.dump(api_token, file)\n!chmod 600 \/root\/.kaggle\/kaggle.json\n","77d4cb7f":"! kaggle competitions submit -c tabular-playground-series-may-2021 -f submission3.csv -m \"Gridsearch new\"","877d223a":"**Running Gridsearch on the chosen model**"}}