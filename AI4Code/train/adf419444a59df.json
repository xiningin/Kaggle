{"cell_type":{"63c4e090":"code","fe0ab676":"code","fdbf7bc1":"code","9e6f0929":"code","b96a1b5c":"code","d8c71fbb":"code","7530dc92":"code","a7e5e102":"code","f9ad59f9":"code","4cc0b28d":"code","33786e61":"code","1acdaad7":"code","9ceb4e37":"code","48db39ce":"code","5c168b91":"code","636ef0ea":"code","96dd17c9":"code","d18f4c67":"code","40dd9b11":"code","bd08e2a0":"code","490c1a80":"code","2c95da88":"code","a86c310a":"markdown","e190e65e":"markdown","3b496bb9":"markdown","bf9eafd4":"markdown","2c207077":"markdown","f1eff34d":"markdown","e286c50e":"markdown","7bd9d64e":"markdown","47eccb58":"markdown","c5e484e0":"markdown","a7c1957b":"markdown"},"source":{"63c4e090":"import torch\nimport torch.nn as nn","fe0ab676":"class CNNBlock(nn.Module):\n    def __init__(self,in_channels, out_channels ,kernel_size = 3, stride =1  , padding = 1):## if groups = in_channels then this is depth_wise convolution operation\n        super(CNNBlock , self).__init__()\n        self.conv2d = nn.Conv2d(in_channels = in_channels , out_channels = out_channels , kernel_size = kernel_size , stride = stride , padding = padding , bias = False)\n        self.batchnorm = nn.BatchNorm2d(out_channels)\n        self.leaky_relu = nn.LeakyReLU(0.1)\n    def forward(self,x):\n        return self.leaky_relu(self.batchnorm(self.conv2d(x)))\n    \n    ","fdbf7bc1":"class YoloV1(nn.Module):\n    def __init__(self,grid_size,n_box , n_class , input_channels = 3):# S = grid_size , n_box = Bounding boxes , n_class = how many classes for classification!\n        super(YoloV1 , self).__init__()\n        ## This will be a long one so you can pass it if you want. Basicly creating the same model on the pic. above.\n        \n        self.darknet = nn.Sequential(\n                     CNNBlock(in_channels = input_channels , out_channels = 64,kernel_size = 7 , stride = 2 , padding = 3),\n                     nn.MaxPool2d(kernel_size = 2 , stride=2),\n                     CNNBlock(in_channels = 64 , out_channels = 192,kernel_size = 3 , stride = 2 , padding = 1),   \n                     nn.MaxPool2d(kernel_size = 2 , stride=2),\n                     CNNBlock(in_channels = 192 , out_channels = 128,kernel_size = 1 , stride = 1 , padding = 0), \n                     CNNBlock(in_channels = 128 , out_channels = 256,kernel_size = 3 , stride = 1 , padding = 1),    \n                     CNNBlock(in_channels = 256 , out_channels = 256,kernel_size = 1 , stride = 1 , padding = 0), \n                     CNNBlock(in_channels = 256 , out_channels = 512,kernel_size = 3 , stride = 1 , padding = 1),   \n                     nn.MaxPool2d(kernel_size = 2 , stride=2),\n                     CNNBlock(in_channels = 512 , out_channels = 256,kernel_size = 1 , stride = 1 , padding = 0), \n                     CNNBlock(in_channels = 256 , out_channels = 512,kernel_size = 3 , stride = 1 , padding = 1),    \n                     CNNBlock(in_channels = 512 , out_channels = 256,kernel_size = 1 , stride = 1 , padding = 0), \n                     CNNBlock(in_channels = 256 , out_channels = 512,kernel_size = 3 , stride = 1 , padding = 1),      \n                     CNNBlock(in_channels = 512 , out_channels = 256,kernel_size = 1 , stride = 1 , padding = 0), \n                     CNNBlock(in_channels = 256 , out_channels = 512,kernel_size = 3 , stride = 1 , padding = 1),    \n                     CNNBlock(in_channels = 512 , out_channels = 256,kernel_size = 1 , stride = 1 , padding = 0), \n                     CNNBlock(in_channels = 256 , out_channels = 512,kernel_size = 3 , stride = 1 , padding = 1),\n                     CNNBlock(in_channels = 512 , out_channels = 512,kernel_size = 1 , stride = 1 , padding = 0), \n                     CNNBlock(in_channels = 512 , out_channels = 1024,kernel_size = 3 , stride = 1 , padding = 1), \n                     nn.MaxPool2d(kernel_size = 2 , stride=2),\n                     CNNBlock(in_channels = 1024 , out_channels = 512,kernel_size = 1 , stride = 1 , padding = 0), \n                     CNNBlock(in_channels = 512 , out_channels = 1024,kernel_size = 3 , stride = 1 , padding = 1),\n                     CNNBlock(in_channels = 1024 , out_channels = 512,kernel_size = 1 , stride = 1 , padding = 0), \n                     CNNBlock(in_channels = 512 , out_channels = 1024,kernel_size = 3 , stride = 1 , padding = 1), \n                     CNNBlock(in_channels = 1024 , out_channels = 1024,kernel_size = 3 , stride = 1 , padding = 1), \n                     CNNBlock(in_channels = 1024 , out_channels = 1024,kernel_size = 3 , stride = 2 , padding = 2), \n                     CNNBlock(in_channels = 1024 , out_channels = 1024,kernel_size = 3 , stride = 1 , padding = 2), \n                     CNNBlock(in_channels = 1024 , out_channels = 1024,kernel_size = 3 , stride = 1 , padding = 1),   \n                        )\n        self.fc = nn.Sequential(\n                nn.Flatten(),\n                nn.Linear((1024*grid_size*grid_size) , 4096), #output of darknet will be S x S x 1024\n                nn.Dropout(0.4),\n                nn.LeakyReLU(0.1),\n                nn.Linear(4096 , grid_size*grid_size*(n_class + n_box*5)) # there will be n_class + n_box*5 outputs for each grid (we multiplt n_box with  5 because each box contains; midpoint_x , midpoint_y , width , height ,confidence  )\n                )\n        \n    def forward(self,x):\n        x = self.darknet(x)\n        return self.fc(x)\n        \n        \n        ","9e6f0929":"## Test the model for S = 7 , Box = 2 , Classes = 20 \n\n# Expected output shape [2,1470] where 1470 is 7 x 7  x (5 x 2 + 20)\n\n#model = YoloV1(7,2,20)\n#x = torch.randn((2,3,448,448))\n#print(model(x).shape)","b96a1b5c":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom collections import Counter","d8c71fbb":"def IOU(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n    \"\"\"\n    Calculates intersection over union\n    Parameters:\n        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n        box_format (str): midpoint\/corners, if boxes (x,y,w,h) or (x1,y1,x2,y2)\n    Returns:\n        tensor: Intersection over union for all examples\n    \"\"\"\n\n    if box_format == \"midpoint\":\n        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] \/ 2\n        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] \/ 2\n        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] \/ 2\n        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] \/ 2\n        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] \/ 2\n        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] \/ 2\n        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] \/ 2\n        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] \/ 2\n\n    if box_format == \"corners\":\n        box1_x1 = boxes_preds[..., 0:1]\n        box1_y1 = boxes_preds[..., 1:2]\n        box1_x2 = boxes_preds[..., 2:3]\n        box1_y2 = boxes_preds[..., 3:4]  # (N, 1)\n        box2_x1 = boxes_labels[..., 0:1]\n        box2_y1 = boxes_labels[..., 1:2]\n        box2_x2 = boxes_labels[..., 2:3]\n        box2_y2 = boxes_labels[..., 3:4]\n\n    x1 = torch.max(box1_x1, box2_x1)\n    y1 = torch.max(box1_y1, box2_y1)\n    x2 = torch.min(box1_x2, box2_x2)\n    y2 = torch.min(box1_y2, box2_y2)\n\n    # .clamp(0) is for the case when they do not intersect\n    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n\n    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n\n    return intersection \/ (box1_area + box2_area - intersection + 1e-6)","7530dc92":"def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n    \"\"\"\n    Does Non Max Suppression given bboxes\n    Parameters:\n        bboxes (list): list of lists containing all bboxes with each bboxes\n        specified as [class_pred, prob_score, x1, y1, x2, y2]\n        iou_threshold (float): threshold where predicted bboxes is correct\n        threshold (float): threshold to remove predicted bboxes (independent of IoU) \n        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n    Returns:\n        list: bboxes after performing NMS given a specific IoU threshold\n    \"\"\"\n\n    assert type(bboxes) == list\n\n    bboxes = [box for box in bboxes if box[1] > threshold]\n    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n    bboxes_after_nms = []\n\n    while bboxes:\n        chosen_box = bboxes.pop(0)\n\n        bboxes = [\n            box\n            for box in bboxes\n            if box[0] != chosen_box[0]\n            or intersection_over_union(\n                torch.tensor(chosen_box[2:]),\n                torch.tensor(box[2:]),\n                box_format=box_format,\n            )\n            < iou_threshold\n        ]\n\n        bboxes_after_nms.append(chosen_box)\n\n    return bboxes_after_nms","a7e5e102":"def mean_average_precision(\n    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20\n):\n    \"\"\"\n    Calculates mean average precision \n    Parameters:\n        pred_boxes (list): list of lists containing all bboxes with each bboxes\n        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n        true_boxes (list): Similar as pred_boxes except all the correct ones \n        iou_threshold (float): threshold where predicted bboxes is correct\n        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n        num_classes (int): number of classes\n    Returns:\n        float: mAP value across all classes given a specific IoU threshold \n    \"\"\"\n\n    # list storing all AP for respective classes\n    average_precisions = []\n\n    # used for numerical stability later on\n    epsilon = 1e-6\n\n    for c in range(num_classes):\n        detections = []\n        ground_truths = []\n\n        # Go through all predictions and targets,\n        # and only add the ones that belong to the\n        # current class c\n        for detection in pred_boxes:\n            if detection[1] == c:\n                detections.append(detection)\n\n        for true_box in true_boxes:\n            if true_box[1] == c:\n                ground_truths.append(true_box)\n\n        # find the amount of bboxes for each training example\n        # Counter here finds how many ground truth bboxes we get\n        # for each training example, so let's say img 0 has 3,\n        # img 1 has 5 then we will obtain a dictionary with:\n        # amount_bboxes = {0:3, 1:5}\n        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n\n        # We then go through each key, val in this dictionary\n        # and convert to the following (w.r.t same example):\n        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n        for key, val in amount_bboxes.items():\n            amount_bboxes[key] = torch.zeros(val)\n\n        # sort by box probabilities which is index 2\n        detections.sort(key=lambda x: x[2], reverse=True)\n        TP = torch.zeros((len(detections)))\n        FP = torch.zeros((len(detections)))\n        total_true_bboxes = len(ground_truths)\n        \n        # If none exists for this class then we can safely skip\n        if total_true_bboxes == 0:\n            continue\n\n        for detection_idx, detection in enumerate(detections):\n            # Only take out the ground_truths that have the same\n            # training idx as detection\n            ground_truth_img = [\n                bbox for bbox in ground_truths if bbox[0] == detection[0]\n            ]\n\n            num_gts = len(ground_truth_img)\n            best_iou = 0\n\n            for idx, gt in enumerate(ground_truth_img):\n                iou = intersection_over_union(\n                    torch.tensor(detection[3:]),\n                    torch.tensor(gt[3:]),\n                    box_format=box_format,\n                )\n\n                if iou > best_iou:\n                    best_iou = iou\n                    best_gt_idx = idx\n\n            if best_iou > iou_threshold:\n                # only detect ground truth detection once\n                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n                    # true positive and add this bounding box to seen\n                    TP[detection_idx] = 1\n                    amount_bboxes[detection[0]][best_gt_idx] = 1\n                else:\n                    FP[detection_idx] = 1\n\n            # if IOU is lower then the detection is a false positive\n            else:\n                FP[detection_idx] = 1\n\n        TP_cumsum = torch.cumsum(TP, dim=0)\n        FP_cumsum = torch.cumsum(FP, dim=0)\n        recalls = TP_cumsum \/ (total_true_bboxes + epsilon)\n        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n        precisions = torch.cat((torch.tensor([1]), precisions))\n        recalls = torch.cat((torch.tensor([0]), recalls))\n        # torch.trapz for numerical integration\n        average_precisions.append(torch.trapz(precisions, recalls))\n\n    return sum(average_precisions) \/ len(average_precisions)\n","f9ad59f9":"def plot_image(image, boxes):\n    \"\"\"Plots predicted bounding boxes on the image\"\"\"\n    im = np.array(image)\n    height, width, _ = im.shape\n\n    # Create figure and axes\n    fig, ax = plt.subplots(1)\n    # Display the image\n    ax.imshow(im)\n\n    # box[0] is x midpoint, box[2] is width\n    # box[1] is y midpoint, box[3] is height\n\n    # Create a Rectangle potch\n    for box in boxes:\n        box = box[2:]\n        assert len(box) == 4, \"Got more values than in x, y, w, h, in a box!\"\n        upper_left_x = box[0] - box[2] \/ 2\n        upper_left_y = box[1] - box[3] \/ 2\n        rect = patches.Rectangle(\n            (upper_left_x * width, upper_left_y * height),\n            box[2] * width,\n            box[3] * height,\n            linewidth=1,\n            edgecolor=\"r\",\n            facecolor=\"none\",\n        )\n        # Add the patch to the Axes\n        ax.add_patch(rect)\n\n    plt.show()","4cc0b28d":"def get_bboxes(\n    loader,\n    model,\n    iou_threshold,\n    threshold,\n    pred_format=\"cells\",\n    box_format=\"midpoint\",\n    device=\"cuda\",\n):\n    all_pred_boxes = []\n    all_true_boxes = []\n\n    # make sure model is in eval before get bboxes\n    model.eval()\n    train_idx = 0\n\n    for batch_idx, (x, labels) in enumerate(loader):\n        x = x.to(device)\n        labels = labels.to(device)\n\n        with torch.no_grad():\n            predictions = model(x)\n\n        batch_size = x.shape[0]\n        true_bboxes = cellboxes_to_boxes(labels)\n        bboxes = cellboxes_to_boxes(predictions)\n\n        for idx in range(batch_size):\n            nms_boxes = non_max_suppression(\n                bboxes[idx],\n                iou_threshold=iou_threshold,\n                threshold=threshold,\n                box_format=box_format,\n            )\n\n\n            #if batch_idx == 0 and idx == 0:\n            #    plot_image(x[idx].permute(1,2,0).to(\"cpu\"), nms_boxes)\n            #    print(nms_boxes)\n\n            for nms_box in nms_boxes:\n                all_pred_boxes.append([train_idx] + nms_box)\n\n            for box in true_bboxes[idx]:\n                # many will get converted to 0 pred\n                if box[1] > threshold:\n                    all_true_boxes.append([train_idx] + box)\n\n            train_idx += 1\n\n    model.train()\n    return all_pred_boxes, all_true_boxes","33786e61":"def convert_cellboxes(predictions, S=7):\n    \"\"\"\n    Converts bounding boxes output from Yolo with\n    an image split size of S into entire image ratios\n    rather than relative to cell ratios. Tried to do this\n    vectorized, but this resulted in quite difficult to read\n    code... Use as a black box? Or implement a more intuitive,\n    using 2 for loops iterating range(S) and convert them one\n    by one, resulting in a slower but more readable implementation.\n    \"\"\"\n\n    predictions = predictions.to(\"cpu\")\n    batch_size = predictions.shape[0]\n    predictions = predictions.reshape(batch_size, 7, 7, 30)\n    bboxes1 = predictions[..., 21:25]\n    bboxes2 = predictions[..., 26:30]\n    scores = torch.cat(\n        (predictions[..., 20].unsqueeze(0), predictions[..., 25].unsqueeze(0)), dim=0\n    )\n    best_box = scores.argmax(0).unsqueeze(-1)\n    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n    x = 1 \/ S * (best_boxes[..., :1] + cell_indices)\n    y = 1 \/ S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n    w_y = 1 \/ S * best_boxes[..., 2:4]\n    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n    predicted_class = predictions[..., :20].argmax(-1).unsqueeze(-1)\n    best_confidence = torch.max(predictions[..., 20], predictions[..., 25]).unsqueeze(\n        -1\n    )\n    converted_preds = torch.cat(\n        (predicted_class, best_confidence, converted_bboxes), dim=-1\n    )\n\n    return converted_preds\n\n\ndef cellboxes_to_boxes(out, S=7):\n    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n    converted_pred[..., 0] = converted_pred[..., 0].long()\n    all_bboxes = []\n\n    for ex_idx in range(out.shape[0]):\n        bboxes = []\n\n        for bbox_idx in range(S * S):\n            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n        all_bboxes.append(bboxes)\n\n    return all_bboxes","1acdaad7":"def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])","9ceb4e37":"class LossFunction(nn.Module):\n    def __init__(self , grid_size = 7 ,n_box = 2 , n_class = 20,lambda_coord = 5, lambda_noobj = 0.5):\n        super(LossFunction,self).__init__()\n        self.mse = nn.MSELoss(reduction = \"sum\")\n        self.lambda_coord = lambda_coord\n        self.lambda_noobj = lambda_noobj\n        self.n_grid = grid_size\n        self.n_box = n_box\n        self.n_class = n_class\n        \n    def forward(self,preds , target):\n        \"\"\"\n         \n            C: Confidience that there is an object\n            X , Y , W , H: (X , Y) midpoints of an object and (W, H) is the width and the height \n            cn: object Class n lower c means class probability. \n         \n         \n            1 grid prediction [C1 X1 Y1 W1 H1 C2 X2 Y2 W2 H2 c1 c2 c3 ... c(n_class)] we constrain the grids so that each grid can only contain 1 object\n            1 grid Target [C X Y W H c1 c2 c3 ... c(n_class)]\n            Target.shape = [N , S , S , (5 + c)]\n            preds.shape =  [N, S , S ,(c + 5*B)]\n        \"\"\"    \n            \n        \n        predictions = preds.reshape(-1,self.n_grid , self.n_grid ,(5 * self.n_box + self.n_class)) ## [1 , 1470] -> [N, S , S ,(C + 5*B)]\n       \n            \n        iou1 = IOU(predictions[... , 1:5] , target[... , 1:5])\n        iou2 = IOU(predictions[... , 6:10] , target[... , 1:5])\n        \n        ious = torch.cat([iou1.unsqueeze(0), iou2.unsqueeze(0)], dim=0) # [2 , N , S , S]\n        \n        iou_maxes, bestbox = torch.max(ious, dim=0) # bestbox  = [0 1 0 0 1 1 1 0 0 ... 0 0 1 0 ...] shape is [N , S , S , 1] each index represents which bounding box to choose\n        \n        \n        \n        # This gives us if there is an object at grid i for every sample in the batch\n        exists_box = target[..., 0].unsqueeze(3) # shape [N ,S , S, 1] we put a dummy dimension to match the dimensions of predictions\n        \n        \n        \n        ## box_predictions.shape = [N, S , S ,4] - > [X Y W H]\n        box_predictions = exists_box * ( ## If there is an object we calculate which bounding box had the highest IOU and we take it\n            (\n                bestbox * predictions[..., 6:10] # [N , S , S, 1] * [ N , S , S , 4]\n                + (1 - bestbox) * predictions[..., 1:5]\n            )\n        ) # we have predicted x , y , w ,h \n        \n        ## Calculate the target boxes\n        box_target = exists_box * target[... , 1:5] ## box_target.shape = [N, S , S ,4] - > [X Y W H]\n        \n        \n        \n        ## We take the sqrt of W H for preds and targets\n        \n        \"\"\"\n         We take the sign of the prediction to preserve the distance, absolute value to prevent error ( sqrt(negative_value) ) and 1e-6 for \n         numeric stability because derivative(sqrt(0)) is discontinuous.\n         \n        \"\"\"\n        \n        \n        box_preds = torch.sign(box_predictions[... , 2:4])*torch.sqrt(torch.abs(box_predictions[... , 2:4] + 1e-6)) \n        \n        box_target = box_target[... , 2:4]*torch.sqrt(box_target[... , 2:4])\n        \n        \n        \n        \n        \n        ## Now let calculate the loss for boxes which will cover the first two lines of the loss function\n        \n        box_loss = self.mse(\n            torch.flatten(box_preds, end_dim = -2),## output shape is [N*S*S , 4], what we do is we calculate the distances of 4 elements for each batch sample and grid\n            torch.flatten(box_target, end_dim = -2)\n        )\n        \n        \n        \n        ## Now the third row of the loss function\n        \n        \n        obj_box_conf = exists_box *(# Calculating the same thing with boxes for Object confidience\n            bestbox * predictions[..., 5:6] + (1 - bestbox) * predictions[..., 0:1]\n        ) # confidence for Highest IOU boxes if there is an object\n        \n        object_loss = self.mse(## Shape [N*S*S , 1] \n            torch.flatten(obj_box_conf, end_dim=-2),\n            torch.flatten(exists_box * target[..., 0:1], end_dim=-2),\n        )\n        \n        \n        \n        \n        ## Now the fourth row of the loss function\n        \n        bbox_j = (1 - exists_box)*(\n            bestbox * predictions[... , 4:5] + (1-bestbox)*predictions[... , 0:1]\n        )\n        \n        \n        no_object_loss = self.mse(## Shape [N*S*S , 1] \n            torch.flatten(bbox_j,end_dim=-2),\n            torch.flatten((1 - exists_box) * target[..., 0:1], end_dim=-2),\n        )\n\n        \n        # And lastly , class probability loss\n        \n        class_loss = self.mse(## Shape [N*S*S , n_class] \n            torch.flatten(exists_box * predictions[..., 10:], end_dim=-2),\n            torch.flatten(exists_box * target[..., 5:], end_dim=-2,),\n        )\n        \n        ## Now we implement lamdas and here is our final loss \n        loss = (\n            self.lambda_coord * box_loss  # first two rows in paper\n            + object_loss  # third row in paper\n            + self.lambda_noobj * no_object_loss  # forth row\n            + class_loss  # fifth row\n        )\n\n        return loss\n        ","48db39ce":"import glob\nlabel_path = glob.glob(\"..\/input\/pascalvoc-yolo\/labels\/*.txt\") \nimg_path = glob.glob(\"..\/input\/pascalvoc-yolo\/images\/*.jpg\") \n    \nprint(\"# text data is: \",len(label_path) , \"# Images: \",len(img_path))    ","5c168b91":"import pandas as pd\nfrom PIL import Image\nimport numpy as np\nfrom io import open\nimport glob\nimport torch\nimport os\n\nclass VOCDataset(torch.utils.data.Dataset):\n    def __init__(\n        self\n    ):\n       #                                               X  Y  x y w h c\n      ## We need to outpu a label matrix whcich is = [S ,S, 5 + c] \n        label_path = glob.glob(\"..\/input\/pascalvoc-yolo\/labels\/*.txt\") \n    \n        self.datadict = {}\n        index_allocator = []\n        a = 1\/7\n        restriction = 0 ## load 500 images\n        for filename in label_path:\n            file_number = os.path.splitext(os.path.basename(filename))[0]\n            lines = open(filename).read().replace(\"\\n\", \" \")\n            l = [\n                float(x) if float(x) != int(float(x)) else int(x)\n                for x in lines.split()\n            ]\n            index_allocator.append(file_number)\n            labels = np.zeros((7,7,25))\n            l = np.array(l).reshape(-1,5) ## Class x y w h\n            for i in l:\n                x_grid = i[1]\/a\n                y_grid = i[2]\/a\n                \n                one_hot = np.zeros((20))\n                one_hot[int(i[0])] = 1\n                labels[int(x_grid),int(y_grid)] = np.concatenate( ( np.array([1 ,(i[1] % 1),(i[2]%1) , i[3] , i[4]]) , one_hot ) ,axis = 0)\n            self.datadict[len(index_allocator)-1] = [labels]    \n            if restriction == 500:\n                break\n            restriction += 1 \n            \n           \n            \n            \n        \n            \n        for img_name in index_allocator:\n            img_path = \"..\/input\/pascalvoc-yolo\/images\/\"+img_name+\".jpg\"\n            \n            img = Image.open(img_path)\n            \n            self.datadict[index_allocator.index(img_name)].append(img.resize((448,448)))\n           \n                \n        del index_allocator\n        \n    def __len__(self):\n        return len(self.datadict)\n    \n    \n    def __getitem__(self, index):\n        \n        return torch.tensor(np.array(self.datadict[index][1])).permute(2,0,1).to(torch.float) , torch.tensor(self.datadict[index][0])\n        \n        \n       \n  ","636ef0ea":"## Testing the dataset\ncustom_dataset = VOCDataset()","96dd17c9":"'''\n    from matplotlib.pyplot import imshow\n    import numpy as np\n    from PIL import Image\n\n    %matplotlib inline\n    pil_im = custom_dataset.__getitem__(100)[1]\n    imshow(np.asarray(pil_im))\n    '''","d18f4c67":"custom_dataset.__len__()","40dd9b11":"import torch\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torchvision.transforms.functional as FT\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\n\n\nmodel = YoloV1(7,2,20)\n\n\nloss =  LossFunction()\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\noptimizer = optim.Adam(\n        model.parameters(), lr=2e-5, weight_decay=0\n    )\n\ndataloader = DataLoader(dataset = custom_dataset , batch_size = 32  , shuffle = True )\n\nloop = tqdm(dataloader, leave=True)\n\nmodel.to(device)\nmodel.train()\nepochs = 20\noptimizer.zero_grad()\nlosses = 0\nloss_list = []\nfor i in range(epochs):\n    \n    for batch_index ,(inp , target) in enumerate(loop):\n        optimizer.zero_grad()\n        inp = inp.to(device)\n        target = target.to(device)\n            \n        out = model(inp)\n        \n        #print(\"Target shape is: \",target.shape)\n        #print(\"output size is: \", out.shape)\n        losses = loss(out , target)\n        loss_list.append(losses.item())\n        \n        losses.backward()\n        optimizer.step()\n        loop.set_postfix(loss = losses.item())\n        del inp\n        del target\n        del out\n    print(f\"Mean loss was {sum(loss_list)\/len(loss_list)}\")   \n        ","bd08e2a0":"import matplotlib.pyplot as plt\n\nplt.figure()\nplt.plot(loss_list)","490c1a80":"x = len(loss_list)\/(300)\nclear = []\nfor i in range(50):\n    clear.append(loss_list[int(i*x)])","2c95da88":"plt.figure()\nplt.plot(clear)","a86c310a":"### In this notebook I will try to design yolov1 from stratch in pytorch. The code will be basic and understandable anyone who know about pytorchs nn module and basic training steps. I will be trying to comment anything that you might miss.\n### You can read the original paper  <a href = 'https:\/\/arxiv.org\/pdf\/1506.02640.pdf'>here <\/a>.\n### Some parts of this code taken from  <a href = 'https:\/\/www.youtube.com\/watch?v=n9_XyCGr-MI'>Aladdin Persson <\/a> video.","e190e65e":"If you want to learn more about pytorch conv2d you check <a href = \"https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.Conv2d.html\">this<\/a> link","3b496bb9":"First we will define a basic CNNBlock for our architecture.We will be using leaky ReLU as activation function. We will add a batch norm to our model although original yolo didnt have but it will help training and accuracy so we will include it.","bf9eafd4":"## Training and evaluation","2c207077":"## 1. Creating the Model \nWe will create the original yolov1 model from scartch so if you are not familiar with the model you can check this image.\n![Screenshot 2021-08-22 210816.png](attachment:05157939-7be4-4965-9327-5142a37c461e.png)","f1eff34d":"## Utility Functions","e286c50e":"## 2. Defining the Loss function","7bd9d64e":"## Dataset","47eccb58":"![Screenshot 2021-08-22 231028.png](attachment:95ec6d0e-93aa-49b6-a6ca-6865df67daf3.png)","c5e484e0":"What we have here is a bit long loss function but we can understand it by diving it into smaller pieces.\n\nIn the first line we see the loss for midpoints. Lets just ignore lambda values for now (Lambda_coord , Lambda_noobj). \nWe see a basic mse loss with the indicator function in front of it. In simple words it says; sum up all the distances between the target and prediction midpoints for all grids iff grid i contains an object and bounding box j is \"responsible\" for outputing that object. Now here responsible means that the bounding box with the highest IOU(intersection over union) score with the target box. So which means we ignore the grid i that does not contain an object and also the bounding boxes that are not responsible. In this way we just penalize the model for the distance for true labels. \n\nIn the second line everyhing is the same except now we calculate the distances for width and height. The reason we use sqrt here is we dont want to penalize the model for the difference of w and h for large objects. And the rest is the same for indcator and bounding boxes.\n\nIn the third line we calculate the confidance score loss which means if there was an object at grid i and responsible bounding box had a confidence score for that object, how bad our prediction is? And we still just calculate it for responsible bounding boxes and True labeled grids.\n\n\nThe fourth part that is the inverse of part 3 we calculate the same thing for no object at grid i and the responsible bounding box j.\n\nAnd lastly, we calculate the differences of probabilities of class predictions and ground truth labels if the grid i contains an object. \n\n\nThe lamdas here helps us to modify the loss we set Lambda_noobj to 0.5 and Lambda_coord to 5 here what we do is, we are pushing the model for better bounding box predictions and we give some room to model for wrong object predictions.","a7c1957b":"This picture shows how the model is doing!\nAnd we will discuss how to remove redundant boxes later.(SPOILER: non-max supression)\n\n![Screenshot 2021-08-22 214753.png](attachment:4bee5791-7c58-4025-a4c0-3286237c2bc7.png)\n"}}