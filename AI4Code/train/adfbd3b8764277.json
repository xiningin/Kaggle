{"cell_type":{"33685bc0":"code","53d40c2e":"code","2936b2bf":"code","413a10fa":"code","e552c8b7":"code","7d07ddb8":"code","5e92e8b5":"code","21f5acfe":"code","ad4b5134":"code","68f863fc":"code","f1ecb352":"code","312d45f5":"code","78a40aee":"code","7ccae6ad":"code","204fda48":"code","20370f5e":"code","582938bb":"code","6dbcb01b":"code","efa4d2d6":"code","44edef2a":"code","b0d97566":"code","f1dd85dd":"code","00396e0a":"code","ee1ed244":"code","5319c737":"code","7b73f45f":"code","ccf698c0":"code","21d70a8a":"markdown","0ceefb15":"markdown","35676e98":"markdown","908865a1":"markdown","7c64a086":"markdown","6bad47ee":"markdown","a729c963":"markdown","046028bb":"markdown","04f90418":"markdown","49c91f23":"markdown","a8296e5a":"markdown","d7c72fba":"markdown","5457d4b6":"markdown","d05a2329":"markdown","a9bfbd3f":"markdown","a39c1e4c":"markdown","0d753b19":"markdown","82bbf961":"markdown","e1f67b0b":"markdown","afde012d":"markdown","b7dfd71c":"markdown","87af902f":"markdown"},"source":{"33685bc0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","53d40c2e":"df = pd.read_csv(\"\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")","2936b2bf":"df","413a10fa":"df.isna().any()","e552c8b7":"df.bmi = df.bmi.fillna(df.bmi.mean())","7d07ddb8":"numerical = df[[\"age\", \"avg_glucose_level\", \"bmi\"]]","5e92e8b5":"categorical = df[[\"gender\", \"hypertension\", \"heart_disease\", \"ever_married\", \"work_type\", \"Residence_type\", \"smoking_status\", \"stroke\"]]","21f5acfe":"sns.countplot(df[\"stroke\"])","ad4b5134":"\nfig, axes = plt.subplots(3,3, figsize=(15,15))\na = 0\nb = 0\nfor col in categorical.columns:\n    sns.countplot(ax=axes[a][b], x=col, hue=\"stroke\", data=categorical)\n    fig.tight_layout() \n    axes[a][b].set_xticklabels(axes[a][b].get_xticklabels(), rotation=10)\n    axes\n    a+=1\n    if a==3:\n        a = 0\n        b+=1\n    \n    \n","68f863fc":"df = df[df.gender != \"Other\"]","f1ecb352":"fig, axes = plt.subplots(3,3, figsize=(15,15))\na = 0\nb = 0\nfor col in categorical.columns:\n    sns.countplot(ax=axes[a][b], x=col, hue=\"gender\", data= categorical[categorical.gender != \"Other\"])\n    fig.tight_layout() \n    axes[a][b].set_xticklabels(axes[a][b].get_xticklabels(), rotation=10)\n    axes\n    a+=1\n    if a==3:\n        a = 0\n        b+=1\n    \n    \n","312d45f5":"fig, axes = plt.subplots(1,3, figsize=(15,4))\na = 0\nfor x in numerical.columns:\n    sns.distplot(df[x], ax=axes[a])\n    a+=1\n    ","78a40aee":"sns.scatterplot(x=\"age\", y=\"avg_glucose_level\", hue=\"stroke\", data=df)","7ccae6ad":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf_prep = df.apply(le.fit_transform)\ncorrs = df_prep.corr()\nplt.figure(figsize=(12,8))\nsns.heatmap(corrs, annot=True, cmap=\"Blues\")","204fda48":"corrs[\"stroke\"]","20370f5e":"from sklearn.preprocessing import LabelEncoder\nLE = LabelEncoder()\n\nencode_cols = [\"gender\", \"hypertension\", \"ever_married\", \"work_type\", \"Residence_type\", \"smoking_status\"]\n\nfor col in encode_cols:\n    LE.fit(df[col])\n    df[col] = LE.transform(df[col])","582938bb":"df.stroke.value_counts()","6dbcb01b":"from imblearn.over_sampling import SMOTE","efa4d2d6":"sampler = SMOTE(random_state = 42)\nX = df.drop(['stroke'],axis=1)\ny = df[['stroke']]\nX,y= sampler.fit_resample(X,y['stroke'].values.ravel())\ny = pd.DataFrame({'stroke':y})\nsns.countplot(data = y, x = 'stroke', y= None)\nplt.show()","44edef2a":"df.info()","b0d97566":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)","f1dd85dd":"from sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline, Pipeline\n\ncol_trans = make_column_transformer(\n            (OneHotEncoder(),['hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']),\n            (StandardScaler(),['age','avg_glucose_level', 'bmi']), \n            remainder = 'passthrough') \n","00396e0a":"df.gender","ee1ed244":"logR = LogisticRegression()\n\npipe = make_pipeline(col_trans, logR)\npipe.fit(X_train, y_train)\npipe.score(X_test, y_test)","5319c737":"RF = RandomForestClassifier(n_estimators = 50, max_depth = 3, random_state = 2 )\n\npipe = make_pipeline(col_trans, RF)\npipe.fit(X_train, y_train)\npipe.score(X_test, y_test)","7b73f45f":"GB = GradientBoostingClassifier(n_estimators = 50, max_depth = 3, random_state = 2)\n\npipe =  make_pipeline(col_trans, GB)\npipe.fit(X_train, y_train)\npipe.score(X_test, y_test)","ccf698c0":"NB = GaussianNB()\n\npipe =  make_pipeline(col_trans, NB)\npipe.fit(X_train, y_train)\npipe.score(X_test, y_test)","21d70a8a":"**Logistic Regression**","0ceefb15":"Since our goal is to predict people with stroke, let's check the distribution of stroke","35676e98":"Splitting data into train and test","908865a1":"As we all know that stroke usually attack older people, and they tend to have higher blood pressure which brings some heart diseases.","7c64a086":"**Gradient Boosting**","6bad47ee":"# Preprocessing","a729c963":"**Random Forest**","046028bb":"Building pipelines that will be automatically preprocessing the dataset when used by the model","04f90418":"**Naive Bayes**","49c91f23":"There's an anomaly on gender data, I'm just gonna drop it since it only one data\n\nafter that, let's check the gender with other features, we might get something","a8296e5a":"Very unbalanced, we can't directly put this dataset into models, we have to do some preprocessing first,\n\nlet's check other categorical values distribution compared to stroke column first","d7c72fba":"It's pretty hard to tell, let's jump to the correlation between each columns","5457d4b6":"How about numerical values?","d05a2329":"Now we are ready for modelling","a9bfbd3f":"Now we have balanced data!","a39c1e4c":"# Modelling","0d753b19":"Fill null values with mean","82bbf961":"That's it, Gradient Boosting results the highest accuracy (86%)\n\nI will find out something to improve these models, please give me feedback or suggestion since I'm still a beginner :)","e1f67b0b":"Apply SMOTE","afde012d":"# Stroke Prediction\n\nYou can see the data and other details [here](https:\/\/www.kaggle.com\/fedesoriano\/stroke-prediction-dataset)\n\nI'm trying to predict stroke with some common classifier algorithms, and also doing some preprocessing, please give me suggestions to improve my notebook :)\n\n# **EDA**","b7dfd71c":"Since the stroke data is imbalanced, we have to make them balance. And one way to do that is using SMOTE,\n\nSmote is an oversampling method by augmenting the minority classes, you can read more explanation [here](https:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/)\n\nSince the augmentation will involve all features, we have to make sure that every features is numerical, let's encode the all of the categorical features first","87af902f":"From the visualization we understand that:\n* There are more female population in this data\n* Most of women are not smoking\n* Can't tell the rest"}}