{"cell_type":{"b29ee03b":"code","6e810ce9":"code","2f4d4391":"code","9bd988cc":"code","4a1d39b5":"code","473c8540":"code","5ff01ee7":"code","67286dbe":"code","4b740aa5":"code","05ee0ce5":"code","99f74a46":"code","8f095893":"code","856b91a7":"code","a5297070":"code","3ffb485e":"code","7ba6d7a5":"code","51b56fb6":"code","1d2d9eb6":"code","8a7cbe2e":"code","6d0ca689":"code","7c384b76":"code","b3269e64":"code","fe641328":"code","e53d69d7":"code","c0a87d79":"code","217a6885":"code","3431d4dd":"code","e3a90f2f":"code","74c9ea7b":"code","74ebb9dc":"code","1658d78e":"code","f5c25b4d":"code","27f2a7b4":"markdown","50c6ec8b":"markdown","fd71696e":"markdown","cdc45f1f":"markdown","6d12d0c3":"markdown","54748b6e":"markdown","1f15d2be":"markdown","f928b535":"markdown","89e93773":"markdown","b38080e5":"markdown","468e948c":"markdown","6fe6274e":"markdown","13894df4":"markdown","6f4014aa":"markdown","40cd302e":"markdown"},"source":{"b29ee03b":"import torch\nimport torch.nn as nn","6e810ce9":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\nimport numpy as np\nimport os\nimport torch\nfrom torch.jit import script, trace\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nimport csv\nimport random\nimport re\nimport os\nimport unicodedata\nimport codecs\nfrom io import open\nimport itertools\nimport math\n\n%matplotlib inline\n","2f4d4391":"use_cuda = torch.cuda.is_available()\ndevice = torch.device('cuda' if use_cuda else 'cpu')\ndevice","9bd988cc":"corpus_name = 'cornell-moviedialog-corpus'\ncorpus = os.path.join('\/kaggle\/input', corpus_name)\ndef printLines(filename, n=10):\n    with open(filename, 'rb') as f:\n        lines = f.readlines()\n    for line in lines[:n]:\n        print(line)\n        \nprintLines(os.path.join(corpus,'movie_lines.txt'))\n        ","4a1d39b5":"column_names = [\"lineID\",\"characterID\",\"movieID\",\"character\",\"text\"]\ndef LoadLines(file, column_names):\n    lines = {}\n    with open(file, 'r', encoding='iso-8859-1') as f:\n        for line in f:\n            dict = {}\n            list_field = line.split(' +++$+++ ')\n            for i, field in enumerate(list_field):\n                dict[column_names[i]] = field\n            lines[dict['lineID']] = dict\n    return lines","473c8540":"lines = LoadLines(os.path.join(corpus, 'movie_lines.txt'), column_names)","5ff01ee7":"# as we can see, after split the \"utteranceIDs\" is a string :  \" ['L2460', 'L2461', 'L2462']\\n\", what we want is retrieve the list inside the string,\n# to do this we use eva function that do the expression inside the input\n# In the 'movie_conversations.txt', the columns are:  [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"] \n\ndef Loadconversation(file, lines, column_names):\n    conversation = []\n    with open(file, 'r', encoding='iso-8859-1') as f:\n        for line in f:\n            dict_column = {}\n            list_column = line.split(' +++$+++ ')\n            for i, col in enumerate(list_column):\n                dict_column[column_names[i]] = col\n            line_id_list = eval(dict_column['utteranceIDs'])\n            dict_column['lines'] = []\n            for line in line_id_list:\n                dict_column['lines'].append(lines[line])\n            conversation.append(dict_column)\n    return conversation\nconversations = Loadconversation(os.path.join(corpus, 'movie_conversations.txt'),lines,[\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"])","67286dbe":"def get_pair_conversation(conversations):\n    \"\"\"\n    return list of pair conversation  [[input1, response1], [input2, response2],....]\n    \"\"\"\n    pair = []\n    for conversation in conversations:\n        num_sentence = len(conversation['lines'])\n        for i in range(num_sentence-1):\n            input = conversation['lines'][i]['text'].strip()\n            response = conversation['lines'][i+1]['text'].strip()\n            if input and response:\n                pair.append([input, response])\n    return pair","4b740aa5":"# create new file to overwrite into it\nos.chdir('\/kaggle\/')\nos.getcwd()\nif not os.path.exists('data_save'):\n    os.makedirs('data_save')\nos.chdir('data_save')\n\npath_save = '\/kaggle\/data_save'\ndatafile = os.path.join(path_save, \"formatted_movie_lines.txt\")\n\ndelimiter = '\\t'\n# Unescape the delimiter\ndelimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n\nprint(\"\\nWriting newly formatted file...\")\nwith open(datafile, 'w', encoding='utf-8') as outputfile:\n    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n    for pair in get_pair_conversation(conversations):\n        writer.writerow(pair)\n","05ee0ce5":"pad_token = 0\nsos_token = 1\neos_token = 2\n\nclass Voc:\n    def __init__(self, name):\n        self.name = name\n        self.trimmed = False\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {pad_token:'PAD', sos_token:'SOS', eos_token : 'EOS'}\n        self.numword = 3\n        \n    def add_sentence(self, sentence):\n        for word in sentence.split(' '):\n            self.addword(word)\n            \n    def addword(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.numword\n            self.word2count[word] = 1\n            self.index2word[self.numword] = word\n            self.numword += 1\n        else:\n            self.word2count[word] += 1\n    def trim(self, min_count):\n        \"\"\"\n        based on the wordcount dictionary, Filter of the word frequency at least more than min_count\n        \"\"\"\n        if self.trimmed:\n            return\n        self.trimmed = True\n        \n        keep_word = []\n        for word, num_frequency in self.word2count.items():\n            if num_frequency >= min_count:\n                keep_word.append(word)\n        \n        # reinitialize dictionaries\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {pad_token:'PAD', sos_token:'SOS', eos_token : 'EOS'}\n        self.numword = 3\n        for word in keep_word:\n            self.addword(word)","99f74a46":"# Convert (or remove accents) sentence to non_accents sentence\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n    )\n\n# Lowercase, trim, and remove non-letter characters\ndef normalizeString(s):\n    s = unicodeToAscii(s.lower().strip())\n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n    s = re.sub(r\"\\s+\", r\" \", s).strip()\n    return s","8f095893":"lines = open(datafile, encoding='utf-8').\\\n        read().strip().split('\\n')\nlines[0]  ## Each string in lines list is a pair (input, response)","856b91a7":"def readVocs(datafile, corpus_name):\n    lines = open(datafile, 'r', encoding='utf-8').\\\n            read().strip().split('\\n')\n    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n    voc = Voc(corpus_name)\n    return voc, pairs\n\n## we ensure every sentences must have the length smaller than max_length\n## max_length value is based on our choice, the greater value, the more data training we have and also the more parameter the model have to train on\ndef filterpair(pairs, max_length):\n    \"\"\"\n    Input: pair with format: [input, response] such as:  ['how are you', 'I am ok']\n    we check the length of both input, response to identify where or not they smaller than max_length\n    return pair with length < max_length\n    \"\"\"\n    valid_pair = []\n    for pair in pairs:\n        input_words, response_words = pair[0].split(' '), pair[1].split(' ')\n        if len(input_words) < max_length and len(response_words) < max_length:\n            valid_pair.append(pair)\n    return valid_pair\n            \n    \ndef loadPrepareData(datafile, corpus_name, max_length):\n    voc, pairs = readVocs(datafile, corpus_name) \n    valid_pair = filterpair(pairs, max_length)\n    print(f'load total {len(pairs)} pairs')\n    print(f'load total {len(valid_pair)} pairs with length <= max_length (10)')\n    for pair in valid_pair:\n        voc.add_sentence(pair[0])\n        voc.add_sentence(pair[1])\n    print(f'total word in vocabulary is : {voc.numword}')\n    return voc, valid_pair\n    ","a5297070":"voc, valid_pair = loadPrepareData(datafile, corpus_name, max_length = 10)\nprint('examples of 10 first pairs')\nfor pair in valid_pair[:3]:\n    print(pair)","3ffb485e":"def trim_rareword(voc, pairs, min_count):\n    voc.trim(min_count) ## trim the voc class with min_count word so that every word in voc.word2index will satisfied the min_count frequency requirement\n    trimmed_pair = []\n    for pair in pairs:\n        input_sentence = pair[0]\n        response_sentence = pair[1]\n        keep_input = True\n        keep_response = True\n        ## Loop over every word in both input and response sentence\n        # Loop over input sentence\n        for word in input_sentence.split(' '):\n            if word not in voc.word2index: # condition \n                keep_input = False\n                break  ## it will end the process right away as long as meet condition, the rest loop process will not run anymore\n         # Loop over output sentence\n        for word in response_sentence.split(' '):\n            if word not in voc.word2index: # condition \n                keep_input = False\n                break\n            \n        if keep_input and keep_response:\n            trimmed_pair.append(pair)\n    print(f'the trimming process make the total {len(pairs)} ==> {len(trimmed_pair)} trimmed pair)')\n    return  voc,trimmed_pair","7ba6d7a5":"voc, trimmed_pair =  trim_rareword(voc, valid_pair, min_count=3)","51b56fb6":"def index_from_sentence(voc, sentence):\n    \"\"\" \n    Input: a single sentence\n    output: return index respectively matching with words in sentence based on voc.word2index \n    \"\"\"\n    return [voc.word2index[word] for word in sentence.split(' ')] + [eos_token] ## to indicate that the sentence is ended here","1d2d9eb6":"# def indexesFromSentence(voc, sentence):\n#     return [voc.word2index[word] for word in sentence.split(' ')] + [eos_token]\nindex_from_sentence(voc, trimmed_pair[5][0])","8a7cbe2e":"# Python\u2019s Itertool is a module that provides various functions that work on iterators (list, tuple, string,...)\ndef zeroPadding(l,fillvalue=pad_token):\n    return list(itertools.zip_longest(*l, fillvalue = fillvalue))\n\ndef binaryMatrix(l, value=pad_token):\n    m = []\n    for i, seq in enumerate(l):\n        m.append([])\n        for token in seq:\n            if token == pad_token:\n                m[i].append(0)\n            else:\n                m[i].append(1)\n    return m\n\ndef input_to_torch(l, voc):\n    \"\"\"\n    Purpos: convert to torch.tensor , (Returns padded input sequence tensor and lengths)\n    \"\"\"\n    indexes_batch = [index_from_sentence(voc, sentence) for sentence in l]\n    padded_list_index = zeroPadding(indexes_batch)\n    padded_tensor_index = torch.LongTensor(padded_list_index)\n    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n    return padded_tensor_index, lengths","6d0ca689":"def output_to_torch(l, voc):\n    \"\"\"\n    Purpos: convert to torch.tensor , (Returns padded output sequence tensor, mask tensor, max_lengths)\n    \"\"\"\n    indexes_batch = [index_from_sentence(voc, sentence) for sentence in l]\n    padded_list_index = zeroPadding(indexes_batch)\n    padded_tensor_index = torch.LongTensor(padded_list_index)\n    max_output_length = max([len(indexes) for indexes in indexes_batch])\n    mask = binaryMatrix(padded_list_index)\n    mask = torch.ByteTensor(mask)\n    return padded_tensor_index, mask, max_output_length","7c384b76":"## Combine all and return all items needed given a batch of pairs\ndef get_batch_pair(voc, batch_pair):\n    \"\"\"\n    sort the len of input sentence in desc\n    return all input and output items\n    \"\"\"\n    # sort len(input sentence) in batch_pair with decreasing order\n    batch_pair.sort(key = lambda x: len(x[0].split(\" \")), reverse = True )\n    # devide the batch pair to batch_input and batch_response\n    input_batch, response_batch = [], []\n    for pair in batch_pair:\n        input_batch.append(pair[0])\n        response_batch.append(pair[1])\n    \n    input_tensor, length_input = input_to_torch(input_batch, voc)\n    output_tensor, mask, max_length = output_to_torch(response_batch, voc)\n    return input_tensor, length_input, output_tensor, mask, max_length","b3269e64":"# Things to remember : output_size : (seq_len, batch, num_directions * hidden_size), num_directions = 1 if unidirectional and 2 if bidirectional\n\nclass EncoderRNN(nn.Module):\n    def __init__(self, embedding, hidden_size, num_layers = 1,dropout = 0):\n        super(EncoderRNN, self).__init__()\n        self.num_layers = num_layers\n        self.embedding = embedding\n        self.hidden_size = hidden_size\n        # Define GRU layers, this GRU cell return 2 things: Output and hidden_state cell\n        self.gru = nn.GRU(  input_size = hidden_size  ## in input_size, number of features = hidden_size\n                          , hidden_size = hidden_size\n                          , num_layers = num_layers\n                          , dropout = (0 if num_layers == 1 else dropout)\n                          , bidirectional = True)\n    def forward(self, input_seq, input_length, hidden = None):\n        ## Convert input seq to embedding format\n        embedding = self.embedding(input_seq)\n        packed_input = torch.nn.utils.rnn.pack_padded_sequence(embedding, input_length)\n        ## forward to gru cell\n        output, hidden_cell = self.gru(packed_input, hidden)\n        output, _ = torch.nn.utils.rnn.pad_packed_sequence(output)\n        ## Sum bidirectional GRU output\n        output = output[:,:,:self.hidden_size] + output[:,:,self.hidden_size:]\n        return output, hidden_cell\n    ","fe641328":"class Luong_attention_layer(nn.Module):\n    def __init__(self, method, hidden_size):\n        super(Luong_attention_layer, self).__init__()\n        self.method = method\n        self.hidden_size = hidden_size\n        \n        if self.method not in ['dot', 'general', 'concat']:\n            raise ValueError(self.method, 'is not appropriate attention method')\n        if self.method == 'general':\n            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n        elif self.method == 'concat':\n            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n            self.weight = nn.Parameter(torch.FloatTensor(hidden_size))\n            \n    def get_dot_score(self, hidden, encoder_outputs):\n        return torch.sum(hidden*encoder_outputs, dim=2)\n    \n    def get_general_score(self, hidden, encoder_outputs):\n        energy = self.attn(encoder_outputs)\n        return torch.sum(hidden * energy, dim=2)\n    \n    def get_concat_score(self, hidden, encoder_outputs):\n        concat = torch.cat((hidden.expand(encoder_outputs.size(0),-1,-1), encoder_outputs), dim=2)\n        energy = torch.tanh(self.attn(concat))\n        return torch.sum(self.weight * energy, dim=2)\n                           \n    def forward(self, hidden, encoder_outputs):\n        if self.method == 'dot':\n            attn_energy = self.get_dot_score(hidden, encoder_outputs)\n        elif self.method == 'general':\n            attn_energy = self.get_general_score(hidden, encoder_outputs)\n        elif self.method == 'concat':\n            attn_energy = self.get_concat_score(hidden, encoder_outputs)\n        \n        ## Transpose  attn_energy\n        attn_energy = attn_energy.t()\n                           \n        # Softmanx the attn_energy to return the weight corresponding to each encoder output\n        return F.softmax(attn_energy, dim=1).unsqueeze(1)","e53d69d7":"class Luong_attention_decoder(nn.Module):\n    def __init__(self, embedding, attn_model, hidden_size, output_size, n_layers=1, dropout = 0.1):\n        super(Luong_attention_decoder, self).__init__()\n        ## Define properties for self\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.dropout = dropout\n        self.attn_model = attn_model\n        \n        ## Define layers\n        self.embedding = embedding\n        self.embedding_dropout = nn.Dropout(dropout)\n        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n        ## self.concat for transform the concat tensor size [hidden,encoder_output] with size = (hidden_size*2) ==> (hidden_size)\n        self.concat = nn.Linear(hidden_size*2, hidden_size)\n        ## self.out for Dense the gru_ouput to return predict value\n        self.out = nn.Linear(hidden_size, output_size)\n        self.attention = Luong_attention_layer(attn_model, hidden_size)\n        \n    def forward(self, input_step, last_hidden, encoder_outputs):\n        ## One step one word through batch\n        embedded = self.embedding(input_step)\n        embedded = self.embedding_dropout(embedded)\n        # forward through unidirrectional GRU\n        rnn_output, hidden = self.gru(embedded, last_hidden)\n        # Feed output and encoder_outputs to attention layer\n        attention_weights = self.attention(rnn_output, encoder_outputs)\n        # caculate context vector\n        context = attention_weights.bmm(encoder_outputs.transpose(0,1))\n        # concat context vector with output\n        rnn_output = rnn_output.squeeze(0)\n        context = context.squeeze(1)\n        concat_input =  torch.cat((rnn_output, context), 1)\n        concat_output = torch.tanh(self.concat(concat_input))\n        # return output predict\n        output = self.out(concat_output)\n        output = F.softmax(output, dim=1)\n        return output, hidden\n\n","c0a87d79":"def maskNLLLoss(input, target, mask):\n    nTotal = mask.sum()\n    crossEntropy = -torch.log(torch.gather(input, 1, target.view(-1, 1)).squeeze(1))\n    loss = crossEntropy.masked_select(mask).mean()\n    loss = loss.to(device)\n    return loss, nTotal.item()","217a6885":"np.random.seed(42)\nmax_length = 10\ndef train(input_variable, lengths, target_variable, embedding, encoder, decoder, encoder_optimizer, decoder_optimizer, max_target_lens\n            , batch_size, clip, mask,max_length = max_length):\n    \"\"\"\n    this train function is responsible for one iteration\n    \"\"\"\n    ## Zeros gradients\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n    ## Set device \n    input_variable = input_variable.to(device)\n    target_variable = target_variable.to(device)\n    lengths = lengths.to(device)\n    mask = mask.bool()\n    mask = mask.to(device)\n    ## Initialize variable\n    loss = 0\n    print_loss = []\n    n_totals = 0\n    ## Pass input through encoder\n    output_encoders,  hidden_encoders = encoder(input_variable, lengths)\n    ## Create initial hidden input\n    input_decoders = torch.LongTensor([[sos_token for _ in range(batch_size)]])\n    input_decoders = input_decoders.to(device)\n    ## Set initial decoder hidden\n    hidden_decoders = hidden_encoders[:decoder.n_layers]\n    ## Determine to use teacher forcing or not\n    teacher_forcing = True if random.random() < teacher_forcing_rate else False\n    \n\n    if teacher_forcing:\n        for t in range(max_target_lens):\n            output_decoders, hidden_decoders = decoder(input_decoders, hidden_decoders, output_encoders)\n            # in case teacher forcing, current target is set to next decoder input\n            input_decoders = target_variable[t].view(1, -1)\n            # Caculate loss\n            mask_loss, nTotal = maskNLLLoss(output_decoders, target_variable[t], mask[t])\n            loss+=mask_loss   # the most important is loss function, this is place where all gradients will be calculated\n            print_loss.append(mask_loss.item() * nTotal)\n            n_totals += nTotal\n    else:\n        for t in range(max_target_lens):\n            output_decoders, hidden_encoders = decoder(input_decoders, hidden_decoders, output_encoders)\n            # in case None teacher forcing, current output decoder is set to next decoder input\n            # torch.topk(i) return (value,index of that value) of \"i\" highest values of tensor, in this case,we want return the only\n            # (___, index) with highest probability value, so we set i ==> 1\n            _, topi = output_decoders.topk(1) ## output_decoder is tensor softmax: ex: [0.3,0.6,01], topk(1) meaning return one highest value\n            input_decoders = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n            input_decoders = input_decoders.to(device)  ## because decoder_input in this case is newly created and have to switch to device\n            # Caculate loss \n            mask_loss, nTotal = maskNLLLoss(output_decoders, target_variable[t], mask[t])\n            loss += mask_loss\n            print_loss.append(mask_loss.item() * nTotal)\n            n_totals += nTotal\n    # Backprob gradient in loss function\n    loss.backward()\n    # Clip the gradients in both encoder, decoder\n    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n    _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n    # Calling the step function on an Optimizer makes an update to its parameters\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n    # return average loss\n    return sum(print_loss) \/ n_totals\n    ","3431d4dd":"def trainIters(model_name, voc, trimmed_pair, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers,\n               decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n    # Load batch for each iteration \n    training_batches = [get_batch_pair(voc, [random.choice(trimmed_pair) for _ in range(batch_size)]) for _ in range(n_iteration)]\n    # Initialization \n    print('initializing...')\n    start_iteration = 1\n    print_loss = 0\n    if loadFilename:\n        start_iteration = checkpoint['iteration'] + 1\n    \n    # Training loop\n    print('tranining')\n    for iteration in range(start_iteration, n_iteration +1):\n        training_batch = training_batches[iteration-1]\n        # Extract fields from batch\n        input_variable, lengths, target_variable, mask, max_target_lens = training_batch\n        # training on batch\n        loss = train(input_variable, lengths, target_variable, embedding, encoder, decoder, encoder_optimizer, decoder_optimizer, max_target_lens\n            , batch_size, clip, mask)\n        print_loss += loss\n        # Print loss after \"print_every step\"\n        if (iteration % print_every) == 0:\n            print_loss_avg = print_loss \/ print_every\n            print(f'loss_avg at {iteration} is: {print_loss_avg}, in {100 * iteration \/ n_iteration } % progress complete')\n            print_loss = 0\n        # Save checkpoint\n        if (iteration % save_every) == 0:\n           directory = os.path.join(path_save, model_name, corpus_name, f'{encoder_n_layers}-{decoder_n_layers}_{hidden_size}')\n           if not os.path.exists(directory):\n               os.makedirs(directory)\n           torch.save({\n               'iteration': iteration,\n               'encoder' : encoder.state_dict(),\n               'decoder' : decoder.state_dict(),\n               'encoder_optimizer': encoder_optimizer.state_dict(),\n               'decoder_optimizer': decoder_optimizer.state_dict(),\n               'loss' : loss,\n               'voc_dict'  : voc.__dict__,\n               'embedding': embedding.state_dict()\n            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))            ","e3a90f2f":"class Greedysearch_decoder(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(Greedysearch_decoder, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n    \n    def forward(self, input_seq, input_length, max_length):\n        output_encoder, hidden_encoder = self.encoder(input_seq, input_length)\n        # Set the final hidden encoder to be initial hidden decoder\n        hidden_decoder = hidden_encoder[:decoder.n_layers]\n        # Initialize decoder input with sos_token\n        input_decoder = torch.ones(1,1,device = device, dtype = torch.long) * sos_token\n        # Create tensors to contain output word\n        all_tokens = torch.zeros([0], device=device, dtype = torch.long)\n        all_score  = torch.zeros([0], device=device)\n        # Loop over decoder - one word per time step\n        for _ in range(max_length):\n            output_decoder, hidden_decoder = self.decoder(input_decoder, hidden_decoder, output_encoder)\n            # Feed output_decoder to torch.max() to return (max_value, index) ( softmax)\n            max_score, output_index = torch.max(output_decoder, dim = 1)\n            # Append to all_tokens and all_scores\n            all_tokens = torch.cat((all_tokens, output_index), dim = 0)\n            all_score = torch.cat((all_score, max_score), dim = 0)\n            # Set current output_index to the next input decoder\n            input_decoder = torch.unsqueeze(output_index, 0)\n        # Return collections of words token and score\n        return all_tokens, all_score\n            \n            ","74c9ea7b":"def evaluate(encoder, decoder, searcher, voc, sentence, max_length = max_length):\n    # transform word to index\n    index_sentence_list = [index_from_sentence(voc, sentence)]\n    input_lengths = torch.tensor([len(index) for index in index_sentence_list])\n    # transform index list to tensor\n    index_sentence = torch.LongTensor(index_sentence_list)\n    # Now index_sentence is [[idx1, idx2,...]], what we want is       [[idx1],  as we defince our sentence shape before ( here batchsize = 1)\n                                                                     # [idx2],\n                                                                     # [...]]\n    # Transform index_sentence to shape (n_words, 1) to act as input\n    input_batch = index_sentence.transpose(0,1)\n    # Feed to device\n    input_batch = input_batch.to(device)\n    input_lengths = input_lengths.to(device)\n    # Now we pass index_sentence, lengths through encoder to return output, hidden encoder\n    output_tokens, output_scores = searcher(input_batch, input_lengths, max_length) \n    words_decoder = [voc.index2word[index.item()] for index in output_tokens]\n    return words_decoder\n\ndef Loop_evaluate(encoder, decoder, search, voc):\n    \"\"\"\n    This function take input sentence from your keyboard, \n    loop through evaluate function above util it reach 'q' or 'quit' input, the process will end here \n    \"\"\"\n    input_sentence = ''\n    while True:\n        try:\n            input_sentence = input('Me: ')\n            if input_sentence in ['q','quit']: break\n            # normalize string\n            input_sentence = normalizeString(input_sentence)\n            # feed to evaluate to return words\n            words_decoder = evaluate(encoder, decoder, search, voc, input_sentence)\n            words_decoder[:]  = [word for word in words_decoder if word not in ['PAD','EOS']]\n            print('Bot: ', ' '.join(words_decoder))\n        except KeyError:\n            print('Unknown word in memory, please try another word')\n        ","74ebb9dc":"# Configure models\nmodel_name = 'cb_model'\nattn_model = 'concat'\n#attn_model = 'general'\n#attn_model = 'concat'\nhidden_size = 500\nencoder_n_layers = 3\ndecoder_n_layers = 3\ndropout = 0.1\nbatch_size = 64\n\n# Set checkpoint to load from; set to None if starting from scratch\nloadFilename = None\ncheckpoint_iter = 10000\n#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n\n\n# Load model if a loadFilename is provided\nif loadFilename:\n    # If loading on same machine the model was trained on\n    checkpoint = torch.load(loadFilename)\n    # If loading a model trained on GPU to CPU\n    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n    encoder_sd = checkpoint['encoder']\n    decoder_sd = checkpoint['decoder']\n    encoder_optimizer_sd = checkpoint['encoder_optimizer']\n    decoder_optimizer_sd = checkpoint['decoder_optimizer']\n    embedding_sd = checkpoint['embedding']\n    voc.__dict__ = checkpoint['voc_dict']\n\n\nprint('Building encoder and decoder ...')\n# Initialize word embeddings\nembedding = nn.Embedding(voc.numword, hidden_size)\nif loadFilename:\n    embedding.load_state_dict(embedding_sd)\n# Initialize encoder & decoder models\nencoder = EncoderRNN(embedding, hidden_size, encoder_n_layers, dropout)\ndecoder = Luong_attention_decoder(embedding, attn_model, hidden_size, voc.numword, decoder_n_layers, dropout)\nif loadFilename:\n    encoder.load_state_dict(encoder_sd)\n    decoder.load_state_dict(decoder_sd)\n# Use appropriate device\nencoder = encoder.to(device)\ndecoder = decoder.to(device)\nprint('Models built and ready to go!')","1658d78e":"\nclip = 50.0\nteacher_forcing_rate = 1.0\nlearning_rate = 3e-4\ndecoder_learning_rate = 5.0\nn_iteration = 10000\nprint_every = 1000\nsave_every = 500\n\n# Ensure dropout layers are in train mode\nencoder.train()\ndecoder.train()\n\n# Initialize optimizers\nprint('Building optimizers ...')\nencoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\ndecoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_rate)\nif loadFilename:\n    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n\n# Run training iterations\nprint(\"Starting Training!\")\ntrainIters(model_name, voc, trimmed_pair, encoder, decoder, encoder_optimizer, decoder_optimizer,\n           embedding, encoder_n_layers, decoder_n_layers, path_save, n_iteration, batch_size,\n           print_every, save_every, clip, corpus_name, loadFilename)","f5c25b4d":"# Set dropout layers to eval mode\nencoder.eval()\ndecoder.eval()\n\n# Initialize search module\nsearcher = Greedysearch_decoder(encoder, decoder)\n\n# Begin chatting, we type some sentence and play with chatbot\nLoop_evaluate(encoder, decoder, searcher, voc)","27f2a7b4":"## Play with chatbot","50c6ec8b":"## Evaluate our own sentence","fd71696e":"### Understand torch.gather\nhttps:\/\/stackoverflow.com\/questions\/50999977\/what-does-the-gather-function-do-in-pytorch-in-layman-terms\nin torch.gather(input, dim = (0 or 1 or 2), index)\n* if dim = 0, we go through rows, from top to bottom, \n* if dim = 1, we go through columns, left to right","cdc45f1f":"## Computational graph for Luong attention\n\n![image.png](attachment:image.png)\n\n**Step 1: Caculating encoder hidden state**\n\n```python\nclass Encoder_LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, n_layers=1, drop_prob=0):\n        super(EncoderLSTM, self).__init__()\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, dropout=drop_prob, batch_first=True)\n    \n    def forward(self, inputs, hidden):\n        # Embed input words\n        embedded = self.embedding(inputs)\n        # Pass the embedded word vectors into LSTM and return all outputs\n        output, hidden = self.lstm(embedded, hidden)\n        return output, hidden\n```\n\n\n**Step 2-->6**\n\n```python\n\nclass Luong_Decoder(nn.Module):\n    def __init__(self, hidden_size, output_size, attention, n_layers=1, drop_prob=0.1):\n        super(LuongDecoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.drop_prob = drop_prob\n\n    # The Attention layer is defined in a separate class\n        self.attention = attention\n        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n        self.dropout = nn.Dropout(self.drop_prob)\n        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)\n        self.classifier = nn.Linear(self.hidden_size*2, self.output_size)\n\n    def forward(self, inputs, hidden, encoder_outputs):\n        # Embed input words\n        embedded = self.embedding(inputs).view(1,1,-1)\n        embedded = self.dropout(embedded)\n\n    # STEP 2: GENERATE NEW HIDDEN STATE FOR DECODER\n        lstm_out, hidden = self.lstm(embedded, hidden)\n\n    # STEP 3: Calculating Alignment Scores \n        alignment_scores = self.attention(lstm_out,encoder_outputs)\n        \n    # STEP 4: Softmaxing alignment scores to obtain Attention weights\n        attn_weights = F.softmax(alignment_scores.view(1,-1), dim=1)\n\n    # STEP 5: CACULATING CONTEXT VECTOR by Multiplying Attention weights with encoder outputs\n        context_vector = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs)\n\n    # STEP 6: CACULATING THE FINAL DECODER OUTPUT by Concatenating output from LSTM with context vector\n        output = torch.cat((lstm_out, context_vector),-1)\n        # Pass concatenated vector through Linear layer acting as a Classifier\n        output = F.log_softmax(self.classifier(output[0]), dim=1)\n        return output, hidden, attn_weights\n\n```\nExploring the attention class in STEP 3: Caculating alignment score\n\nIn Luong Attention, there are 3 different ways (dot, general, concat) to caculate the alignment score.\n\n```\n1. Dot function\n  This is the simplest of the functions: alignment score calculated by multiplying the hidden encoder and the hidden decoder.\n  SCORE = H(encoder) * H(decoder)\n2. General function\n  similar to the dot function, except that a weight matrix is added into the equation\n  SCORE = W(H(encoder) * H(decoder))\n3. Concat function\n  Concating encoder and decoder first, the feed to nn.Linear and activation it, finally we add W2 to get final Score\n  SCORE = W2 * tanh(W1(H(encoder) + H(decoder)))\n```\nImplementing attention class:\n\n```python\nclass Luong_attention_layer(nn.Module):\n    def __init__(self, method, hidden_size):\n        super(Luong_attention_layer, self).__init__()\n        self.method = method\n        self.hidden_size = hidden_size\n        \n        if self.method not in ['dot', 'general', 'concat']:\n            raise ValueError(self.method, 'is not appropriate attention method')\n        if self.method == 'general':\n            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n        elif self.method == 'concat':\n            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n            self.weight = nn.Parameter(torch.FloatTensor(hidden_size))\n            \n    def get_dot_score(self, hidden, encoder_outputs):\n        return torch.sum(hidden*encoder_outputs, dim=2)\n    \n    def get_general_score(self, hidden, encoder_outputs):\n        energy = self.attn(encoder_outputs)\n        return torch.sum(hidden * energy, dim=2)\n    \n    def get_concat_score(self, hidden, encoder_outputs):\n        concat = torch.cat((hidden.expand(encoder_outputs.size(0),-1,-1), encoder_outputs), dim=2)\n        energy = torch.tanh(self.attn(concat))\n        return torch.sum(self.weight * energy, dim=2)\n                           \n    def forward(self, hidden, encoder_outputs):\n        if self.method == 'dot':\n            attn_energy = self.get_dot_score(hidden, encoder_outputs)\n        elif self.method == 'general':\n            attn_energy = self.get_general_score(hidden, encoder_outputs)\n        elif self.method == 'concat':\n            attn_energy = self.get_concat_score(hidden, encoder_outputs)\n        \n        ## Transpose  attn_energy\n        attn_energy = attn_energy.t()\n                           \n        # Softmanx the attn_energy to return the weight corresponding to each encoder output\n        return F.softmax(attn_energy, dim=1).unsqueeze(1)\n\n```\n  ","6d12d0c3":"To facilite the greedy decoding operation, we define a\n``GreedySearchDecoder`` class. When run, an object of this class takes\nan input sequence (``input_seq``) of shape *(input_seq length, 1)*, a\nscalar input length (``input_length``) tensor, and a ``max_length`` to\nbound the response sentence length. The input sentence is evaluated\nusing the following computational graph:\n\n**Computation Graph:**\n\n1. Forward input through encoder model.\n2. Prepare encoder's final hidden layer to be first hidden input to the decoder.\n3. Initialize decoder's first input as SOS_token.\n4. Initialize tensors to append decoded words to.\n5. Iteratively decode one word token at a time:\n * Forward pass through decoder.\n * Obtain most likely word token and its softmax score.\n * Record token and score.\n * Prepare current token to be next decoder input.\n7. Return collections of word tokens and scores.","54748b6e":"## Step 1: Preparing data","1f15d2be":"Transform data to tensor","f928b535":"For this we define a ``Voc`` class, which keeps a mapping from words to\nindexes, a reverse mapping of indexes to words, a count of each word and\na total word count. The class provides methods for adding a word to the\nvocabulary (``addWord``), adding all words in a sentence\n(``addSentence``) and trimming infrequently seen words (``trim``). More\non trimming later.","89e93773":"This kernel covers the main concepts behind Attention techniques used in recurrent neural network. \n\n1. Part I: focusing on the attention understanding\n1. Part II: Applying attention mechanism in building chatbot seq2seq step by step\n\n## Part I: Attention understanding\n\nJust like in \u201cAttention\u201d meaning, in real life when we looking at a picture or hearing the song, we usally focus more on some parts and pay less attention in the rest. The Attention mechanism in Deep Learning is also the same flow, paying greater attention to certain parts when processing the data\n\nAttention is one component of a network\u2019s architecture.\n\nFollow the specific tasks, the encoder & decoder will be different. In machine translation, the encoder often set to LSTM\/GRU\/Bi_RNN, in image captioning, the encoder often set to CNN.\n\nSuch as for the task: **Translating the sentence: 'le chat est noir' to English sentence (the cat is black)**\n\nThe input has 4 words, plus EOS token at the end (stop word) corresponding 5 time steps in translating to English.\nEach time step, Attention is applied by assigning weights to input words, the more important words, the bigger weights will be assigned (Done by backprob gradient process). So There are 5 differrent times weights assigned (coresponding to 5 time steps)\nThe general architecture in seq2seq as follow: \n\n![image.png](attachment:image.png)\n\n\n\nWithout attention, The input in **decoder** based on 2 component: the initial decoder input (often we set it to EOS token first (start word)) and the last hidden encoder. This way has the drawback in case some informations of very first encoder cell would be loss during the process. To handle this problem, the attention weight is added to all encoder outputs. \n\n![Capture.JPG](attachment:Capture.JPG)      \nAs we can see, through each decoder output word, the attention weights colors of encoder input is changed differently along itself importance\n\n\nYou may ask how can we appropriately set the weight to encoder outputs. The answer is: we just randomly set the weights, and the backpropagation gradient process will take care about it during the training. What we have to do is correctly build the forward computational graph.\n\n![Capture1.JPG](attachment:Capture1.JPG)\n\nAfter attention weight was caculated, now we have three components: decoder input, decoder hidden, (attention weights * encoder outputs), we feed them to decoder to return decoder output\n\n\nThere are two primary types of attention: Bahdanau Attention vs Luong Attention. Luong attention is built on top of Bahdanau attention and have proved better scores in several tasks. This kernel is focus on Luong attention.","b38080e5":"### Run our model","468e948c":"## Part II: Building chatbot seq2seq with Luong attention mechanism\n\nThe step by step for building chatbot with attention as follow: \n![Capture%204.JPG](attachment:Capture%204.JPG)\n\nAfter running this kernel. you can play with chatbot and have some fun with him like this:)) :\n\n![Capture6.JPG](attachment:Capture6.JPG)\n\nThe code is based on : https:\/\/pytorch.org\/tutorials\/beginner\/chatbot_tutorial.html. I have modified this toturial on something because the Author used some pytorch features that currently depressed. Through this kernel, I added explaination on my own understanding step by step so you might find it friendly to understand all the concepts.\n","6fe6274e":"## Step 4: Create function to interact with chatbot","13894df4":"## Step 2: Define model","6f4014aa":"In the vocabulary pairs, it's include some rare words and this make model difficult to convergance because it try hard to approximate in output predict and real output when one of them they include rare word. make the rest hard to approximate ==> take out these word from pairs","40cd302e":"## Step 3: Creating training function"}}