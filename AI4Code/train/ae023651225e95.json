{"cell_type":{"50ba4403":"code","145663d6":"code","2312fd5b":"code","e54a3f24":"code","05b23eec":"code","f8c1521a":"code","a8a869e6":"code","3651305a":"code","bbe10854":"code","328ba66f":"code","27648cc5":"code","aefc4325":"code","902a5912":"code","b550d78b":"code","887be55a":"code","e62617ae":"code","0531a5c4":"code","28f431df":"code","3f973a8b":"code","5354ac96":"code","5fa42d1a":"code","75e35e10":"code","b63c21fb":"code","74b8ce52":"code","8ac7b38b":"code","4fdebe32":"code","d707d019":"code","fd7bfcc2":"code","091513f9":"code","5c2a8984":"code","4b32b8ff":"code","2acb2068":"code","822fe336":"code","f933778f":"code","467059cd":"markdown","b1882843":"markdown","ab44ca64":"markdown","80175856":"markdown","423e25f0":"markdown","337f6a0c":"markdown","26ebf5b1":"markdown","0ebbcc95":"markdown","d3c5e08a":"markdown"},"source":{"50ba4403":"!pip install pycaret\n!pip install pandas_profiling","145663d6":"import pandas as pd \nimport pandas_profiling as pp","2312fd5b":"#read the data\ntrain_titanic=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_titanic=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain_house=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_house=pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","e54a3f24":"#data dimensions\nprint('Titanic train:',train_titanic.shape)\nprint('Titanic test:',test_titanic.shape)\nprint('House price train:',train_house.shape)\nprint('House price test:',test_house.shape)","05b23eec":"#pandas profiling report for Titanic train data\npp.ProfileReport(train_titanic)","f8c1521a":"#import pycaret classification library\nfrom pycaret.classification import * #for classification\n#classification setup\nclassification_setup =setup(data = train_titanic, \n             target = 'Survived',\n             numeric_imputation = 'mean', #fill missing value with mean for numeric features\n             categorical_features = ['Sex','Embarked','Pclass','Ticket','Cabin'], #we know categorical features from pandas profiling report\n             ignore_features = ['Name','PassengerId'],\n             train_size=0.8, #0.7 as default\n             high_cardinality_features=['Cabin'],\n             normalize=True,\n             normalize_method='minmax',\n             handle_unknown_categorical=True,\n             unknown_categorical_method='most_frequent',  #fill missing value with most frequent value for categorical features\n             remove_outliers=True, #it automatically applies PCA for removing outliers,\n             outliers_threshold=0.05, #By default, 0.05 is used which means 0.025 of the values on each side of the distribution\u2019s tail are dropped from training data.\n             silent=True,\n             profile=True #a data profile for Exploratory Data Analysis will be displayed in an interactive HTML report. It also generates pandas profiling report\n     )","a8a869e6":"#comparing models\nblacklist_models = ['svm','rbfsvm','mlp']\n\ncompare_models(\n    blacklist=blacklist_models, #blacklisted models won't work.\n    fold = 5,\n    sort = 'Accuracy', ## competition metric\n    turbo = True\n)","3651305a":"#creating model with selected estimator.\nxgb=create_model(estimator='xgboost',fold=5)","bbe10854":"#tune the model\ntuned_xgb = tune_model('xgboost')","328ba66f":"# ensembling a trained xgboost model\nxgb_bagged = ensemble_model(xgb)","27648cc5":"plot_model(xgb, plot = 'boundary')# Decision Boundary","aefc4325":"plot_model(xgb, plot = 'pr')# Precision Recall Curve","902a5912":"plot_model(xgb, plot = 'vc')# Validation Curve","b550d78b":"plot_model(xgb, plot='confusion_matrix') # Confusion Matrix","887be55a":"#Evaluating model is a good option. Because you don't need to plot different plots seperately. It provides all of them in the same cell.\nevaluate_model(xgb)","e62617ae":"#As you remember, we split 80% of the data for training. The rest of the data can be used for holdout prediction.\nxgb_holdout_pred = predict_model(xgb)","0531a5c4":"#Or you can use your test data for prediction.\ntitanic_prediction =  predict_model(xgb, data=test_titanic)\ntitanic_prediction.head()","28f431df":"#prepare the submission file\ntitanic_prediction['Survived'] = round(titanic_prediction['Score']).astype(int)\nsubmission=titanic_prediction[['PassengerId','Survived']]\nsubmission.to_csv('submission.csv',index=False)\nsubmission.head()","3f973a8b":"#pandas profiling report for House price train data\npp.ProfileReport(train_house)","5354ac96":"from pycaret.regression import * #for regression\n\n#regression setup\nregression_setup =setup(data = train_house, \n             target = 'SalePrice',\n             numeric_imputation = 'mean', #fill missing value with mean for numeric features\n             categorical_features = ['MSZoning','Exterior1st','Exterior2nd','KitchenQual','Functional','SaleType',\n                                     'Street','LotShape','LandContour','LotConfig','LandSlope','Neighborhood',   \n                                     'Condition1','Condition2','BldgType','HouseStyle','RoofStyle','RoofMatl',    \n                                     'MasVnrType','ExterQual','ExterCond','Foundation','BsmtQual','BsmtCond',   \n                                     'BsmtExposure','BsmtFinType1','BsmtFinType2','Heating','HeatingQC','CentralAir',   \n                                     'Electrical','GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive',\n                                     'SaleCondition'], #we know categorical features from pandas profiling report\n             ignore_features = ['Id'],\n             train_size=0.8, #0.7 as default\n             normalize=True,\n             normalize_method='minmax',\n             handle_unknown_categorical=True,\n             unknown_categorical_method='most_frequent',  #fill missing value with most frequent value for categorical features\n             remove_outliers=True, #it automatically applies PCA for removing outliers,\n             outliers_threshold=0.05, #By default, 0.05 is used which means 0.025 of the values on each side of the distribution\u2019s tail are dropped from training data.\n             silent=True,\n             profile=True #a data profile for Exploratory Data Analysis will be displayed in an interactive HTML report. It also generates pandas profiling report\n     )","5fa42d1a":"bl_models = ['ransac', 'tr', 'rf', 'et', 'ada', 'gbr']\n\ncompare_models(\n    blacklist = bl_models,\n    fold = 5,\n    sort = 'MAE', ## competition metric\n    turbo = True\n)","75e35e10":"#creating model.\nlgbm = create_model(\n    estimator='lightgbm',\n    fold=5\n)","b63c21fb":"#Evaluating model.\nevaluate_model(lgbm)","74b8ce52":"# use rest of the training data for holdout prediction.\nlgbm_holdout_pred = predict_model(lgbm)","8ac7b38b":"#prediction with test data.\nhouse_prediction =  predict_model(lgbm, data=test_house)\nhouse_prediction.head()","4fdebe32":"#prepare the submission file\nhouse_prediction.rename(columns={'Label':'SalePrice'}, inplace=True)\nhouse_prediction[['Id','SalePrice']].to_csv('submission_house.csv', index=False)","d707d019":"from pycaret.clustering import * #for clustering\n#clustering setup doesn't support silent True options. So, you need to hit ENTER manually.\n#if silent is set True, it means that you approve data types which were inferred from PyCaret.\n\n#clustering setup\nclustering_setup =setup(data = train_titanic, \n             numeric_imputation = 'mean', #fill missing value with mean for numeric features\n             categorical_features = ['Sex','Embarked','Pclass','Ticket','Cabin'], #we know categorical features from pandas profiling report\n             ignore_features = ['Name','PassengerId'],\n             high_cardinality_features=['Cabin'],\n             normalize=True,\n             normalize_method='minmax',\n             handle_unknown_categorical=True,\n             unknown_categorical_method='most_frequent',  #fill missing value with most frequent value for categorical features\n             verbose=False        \n    \n     )","fd7bfcc2":"#use k-means for clustering. You can check other clustering algorithms from https:\/\/pycaret.org\/clustering\/\nkmeans = create_model('kmeans')","091513f9":"#assigning data to clusters.\nkmeans_df = assign_model(kmeans)\nkmeans_df.head()","5c2a8984":"# PCA Plot\nplot_model(kmeans) ","4b32b8ff":"#Also you can plot Silhouette\nplot_model(kmeans, plot='silhouette') ","2acb2068":"# Or you can plot Elbow etc.\nplot_model(kmeans, plot='elbow') ","822fe336":"# tunes the num_clusters model parameter using a predefined grid with the objective of optimizing a supervised learning metric as defined in the optimize param. \ntuned_kmeans = tune_model(model = 'kmeans', supervised_target = 'Survived')","f933778f":"# Also, you can specify estimator for tuning.\ntuned_kmeans = tune_model(model = 'kmeans', supervised_target = 'Survived', estimator='xgboost')","467059cd":"# *4. Read the data*","b1882843":"# *2. Installing necessary libraries*","ab44ca64":"# *8. Some last words*\n\nIn one of my previous notebooks, I prepared a Simple Guide of PyCaret for how to use it for regression type of problems. After getting good feedbacks for PyCaret, I just want to prepare a new extended guide for PyCaret. My main objective was not improving the results. I just want to show as many as features and functions of PyCaret. You can find more detailed information from https:\/\/pycaret.org\/\n\nIf you have any ideas to feedback please let me know in comments, and if you liked my work please don't forget to vote, thank you!\n\n![](https:\/\/pycaret.org\/wp-content\/uploads\/2020\/03\/Divi93_43.png)","80175856":"# *7. Clustering*","423e25f0":"# *3. Importing neccessary libraries*","337f6a0c":"# *6. Regression*","26ebf5b1":"# *5. Classification*","0ebbcc95":"# ****Warning: PyCaret setup will work for last imported PyCaret library. \n# So, if you import clustering library, you will get an error for classification setup****","d3c5e08a":"# ***PyCaret Extended Tutorial for Classification, Regression and Clustering***\n\n\n# *1. Evaluate 3 different types of modelling*\n* **Classification:** We will use Titanic challenge dataset\n* **Regression:** We will use House prices advanced regression challenge dataset\n* **Clustering:** We will use Titanic challenge dataset again\n\n\n"}}