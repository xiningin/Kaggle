{"cell_type":{"4a12ba9b":"code","96fc306e":"code","71d72327":"code","9a203ec3":"code","77da2da9":"code","75895b05":"code","ecd40fa2":"code","c3ed4a4c":"code","e86958bb":"code","291a1f8e":"code","3a1b594f":"code","d224936a":"code","fda1ed47":"code","c2dedd5e":"code","f88a5149":"code","c113f123":"code","d29615ca":"code","847117c5":"code","b96590b7":"code","c9070b7d":"code","e3705dd8":"code","ddd1f640":"code","fc58b7b5":"code","02026fbf":"code","5d789827":"code","ae38dd94":"code","07b4f2ac":"code","f7d5e2e2":"code","cb5b02ff":"code","48cb99ed":"code","d7d27f17":"code","91557e9e":"code","e0dae98f":"code","68611819":"code","e346db6e":"code","3b893278":"code","d1d29e53":"code","7bbaefa2":"code","85672ee4":"code","6cde3451":"code","d16ea07d":"code","4688daf7":"code","a6e8c70f":"code","4f6e68cb":"code","7d89c19a":"code","65c216bb":"code","715d7435":"code","fb4b6f25":"code","cf8548d1":"code","00796b6e":"code","b58987c4":"code","a46f5d43":"code","e318bb65":"code","1bf0f17c":"code","8c098779":"code","6b21eed6":"code","8b7932d6":"code","544fb849":"code","1ab66109":"code","da0efbcc":"code","031f6894":"code","48f44916":"code","82cdb4a3":"code","2c599676":"code","43d1434f":"code","3a84942c":"code","e05677a2":"code","e2c833a7":"code","47bdc7e0":"code","854038e3":"code","a5e19b9c":"code","1e11c0b8":"code","c38c58c8":"code","e7974bb8":"code","dc91f7fc":"code","819d7581":"code","507bee8d":"code","f5d6f348":"code","2ba2d586":"code","c7170283":"code","d1d7f41d":"code","b400d9fb":"code","d4b9ad19":"code","ca1ed7e8":"code","0b5f8446":"code","d1911715":"code","68985f9f":"code","3deebf56":"code","a86226fd":"code","a8465753":"code","713f60b3":"code","3b793d0f":"code","716b1358":"code","f8d6bdbd":"markdown","9e213ea3":"markdown","c6896363":"markdown","47fd86f3":"markdown","9950a32b":"markdown","0fe2a941":"markdown","955ca64c":"markdown","d66dbdc1":"markdown","2457ffa0":"markdown"},"source":{"4a12ba9b":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import chi2_contingency\nimport statsmodels.api\nfrom sklearn import svm\nimport xgboost as xgb\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nimport tensorflow as tf\nfrom keras import layers \nfrom keras import models\nfrom keras import regularizers","96fc306e":"\ncd ..\/input\/titanic","71d72327":"#import train.csv\ntrain_data = pd.read_csv('train.csv')\ntrain_data.shape\n","9a203ec3":"# First 5 rows of the DataFrame\ntrain_data.head(6)\n","77da2da9":"train_data.info()","75895b05":"# We have a lot of missing values in \"Age\", \"Cabin\" and 2 NaN values in \"Embarked\"","ecd40fa2":"# Understand \"Age\" variable\ntrain_data.describe()","c3ed4a4c":"# Min age is 0.42 !! we can see from the Variables Notes that  \"Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\"\n# So it's normal that we have 0.42 value (new born baby) no modification is necessary.\n\ntrain_data[train_data.Age <1 ]","e86958bb":"sns.displot(train_data.Age,aspect = 2);\nplt.title('Age distribution');","291a1f8e":"# We can see that age follows \"normal distribution\" so we can fill the missing values with a random numbers from this distribution \ndef Random_Fillna_Gauss(col): \n    a = col.values\n    m = np.isnan(a) #boolean for NaN\n    mu, sigma = col.mean(), col.std()\n    np.random.seed(seed = 1994) #to have always the same random numbers (reproducible)\n    # the 'abs' for positive numbers, the \"+1\" because i dont want zeros, as you saw above we dont have too many new born babies so i want integer ages > 0\n    a[m] = abs(np.random.normal(mu, sigma, size=m.sum())).astype('int') + 1 #using the  m boolean (when True) randomly fill the NaNs based on the normal distribution(Guassian)\n    print(\"{} NaN values have been filled\".format(m.sum()))  \n    return col\n","3a1b594f":"train_data.Age = Random_Fillna_Gauss(train_data.Age)","d224936a":"sns.displot(train_data.Age, aspect = 2);\nplt.title('Age distribution after randomly filling the NaNs');","fda1ed47":"#I noticed earlier from above that there are 2 diffrent names with the same ticket number let's take closer look","c2dedd5e":"# Understand 'Ticket' 'SibSp' 'Parch' variables","f88a5149":"train_data[train_data.Ticket=='2666']","c113f123":"# So ticket number \"2666\" is for two new born babies twins ( age 9 months ), one child (5 years old) and their mother (24 years old)\n# we can see that \"SibSp == 2\" for the children, that means that every child has 2 sibling, and one parent \"Parch == 1\" \n# and the mother has only three children \"Parch == 3 \" with no spouse or sibling \"SibSp == 0\".","d29615ca":"#To see if there is a relation between two categorical variables we can do the X\u00b2 test (proportion test),\n#First we need a contingency table, let's look for a relation between \"Cabin\" and \"Survived\" \ncont = pd.crosstab(train_data.Survived,train_data.Cabin)\n","847117c5":"# For this test, we have the null hypothesis H0: \"The two variables tested are independent\".\n# We can preform this test thanks to the chi2_contingency function of scipy applied to the contingency table\n# It returns an array of 4 elements: the test statistic, the p-value, the degree of freedom and the list of expected frequencies.","b96590b7":"resultats_test = chi2_contingency(cont)\nprint('Statistic = {} P_value = {} Degree of liberty = {} '.format(resultats_test[0],resultats_test[1], resultats_test[2]))","c9070b7d":"#P-value > 0.05, we fail to reject the H0 hypothesis (we can drop 'Cabin' which is also full of missing values )","e3705dd8":" adults = train_data[train_data.Age > 15]","ddd1f640":"sns.catplot(x =\"Survived\", y =\"Age\", kind = 'bar',data = adults, aspect = 2,col = 'Sex', hue = 'Parch');","fc58b7b5":"sns.catplot(x =\"Survived\", kind = 'count',data = adults, aspect = 2,hue = 'SibSp',col = 'Sex' );","02026fbf":"# we can see that males older than 35 with 3 or more children did not survive, even single men didn't have a chance to survive too.\n# On the other hand single women survived more. ","5d789827":"# Understand the 'Embarked' variable\n\n# lets take another variable this time \"Embarked\" for example \ncont = pd.crosstab(train_data.Survived,train_data.Embarked)\nresultats_test = chi2_contingency(cont)\nprint('Statistic = {} P_value = {} Degree of liberty = {} '.format(resultats_test[0],resultats_test[1], resultats_test[2]))","ae38dd94":"#P-value < 0.05, we reject the H0 hypothesis","07b4f2ac":"#Now we can measure the level of correlation between those two variables, applying V-Cramer :\ndef V_Cramer(tab,N):\n    stat_chi2 = chi2_contingency(tab)[0]\n    k = cont.shape[0]\n    r = cont.shape[1]\n    phi = max(0,(stat_chi2\/N)-((k-1)*(r-1)\/(N-1)))\n    k_corr = k - (np.square(k-1)\/(N-1))\n    r_corr = r - (np.square(r-1)\/(N-1))\n    return np.sqrt(phi\/min(k_corr - 1,r_corr - 1))\n\nV_Cramer(cont,train_data.shape[0])\n# We can see that there is a relation but its not that strong, same for 'Parch', 'SibSp' variables.","f7d5e2e2":"sns.set(font_scale=1.2) #change font size","cb5b02ff":"sns.catplot(x = 'Embarked', hue = 'Survived', data = train_data, kind = \"count\", aspect = 2)","48cb99ed":"train_data.Embarked.value_counts(normalize = True)","d7d27f17":"# (72%) of the passengers embarked from Southampton UK, a few (0.08%) from Queenstown ireland and (18%) from Cherbourg France","91557e9e":"# i want to fill the NaNs in Embarekd with the most occurrence\ntrain_data.Embarked.fillna(train_data.Embarked.mode()[0],inplace = True)","e0dae98f":"sns.catplot(x ='Pclass', y = 'Fare', data = train_data, kind = 'bar', hue = 'Survived' ,aspect= 2)","68611819":"# We can see that passengers who paid more for 1st class have survived more than 2nd and 3rd class","e346db6e":"# There is a relation between 'Pclass' and 'Fare' for sure, let's confirme it \n# In this case we have a relation between quantitative and qualitative variables.\n# we will use the analysis of variance (ANOVA) via the statsmodels module and visualize the p-value\n\ncorr = statsmodels.formula.api.ols('Fare ~ Pclass', data = train_data).fit()\ncorr_res = statsmodels.api.stats.anova_lm(corr)\ncorr_res\n\n","3b893278":"#the p-value (PR (> F)) is less than 5%, we reject the null hypothesis. We can drop one of them later\n#Understand \"Sex\" and 'Pclass' variables","d1d29e53":"sns.catplot(x = 'Sex', hue = 'Survived', data = train_data,kind = \"count\", aspect = 2)","7bbaefa2":"train_data.groupby('Sex').Survived.value_counts() \n# 64% of passnegers were Male (577), while only 35% were Female (314) ----> Only 233 women and 109 managed to survive. ","85672ee4":"sns.catplot(x = 'Embarked', data = train_data,kind = 'count',hue = 'Survived',aspect = 2 ,col = 'Pclass', row = 'Sex' )","6cde3451":"train_data.Pclass.value_counts(normalize = True)","d16ea07d":"train_data.groupby(['Pclass','Sex']).Survived.value_counts(normalize = True)","4688daf7":"# From above we can notice the following statement: \n\n# Almost all people who have embarked from Queenstown booked third class ticket.\n# 55% of the passengers were on third class. \n# Almost all female passengers who were on first and second class managed to survived.\n# All females on 3rd class have 50\/50 chance to survive, and more chance if you have been embarked from 'S' or 'Q' ports","a6e8c70f":"train_data[(train_data.Sex == \"male\") & (train_data.Survived == 1)].groupby(['Pclass']).Embarked.value_counts(normalize = True)","4f6e68cb":"# lets drop the useless columns \ncol_to_drop = ['PassengerId','Name','Ticket','Cabin','Fare']\ntrain_data.drop(col_to_drop,axis = 1 ,inplace = True)","7d89c19a":"train_data.head()","65c216bb":"train_data.info() ","715d7435":"# Data vectorization for catagorical data\nencoder = LabelEncoder()\ntrain_data.Embarked = encoder.fit_transform(train_data.Embarked)\ntrain_data.Sex.replace( {'male': 0, 'female' : 1}, inplace = True)","fb4b6f25":"features = train_data.drop('Survived', axis=1)\ntarget = train_data.Survived","cf8548d1":"# Split our training set to train \/ test \nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.1, random_state = 0)\nX_train.shape , y_train.shape, X_test.shape, y_test.shape","00796b6e":"# Standarization (rescales data to have a mean of 0 and a standard deviation of 1 (unit variance)) for for numerical data\nscaler = StandardScaler()\ntrain_scaled = scaler.fit_transform(X_train)\ntest_scaled = scaler.transform(X_test)","b58987c4":"# A first classification model (Support Vector machines) \nclf1 = svm.SVC() ","a46f5d43":"# Tuning parmameters with GridSearchCV \nparametres = {'C':[0.1,1,10],\n              'kernel':['linear', 'poly', 'rbf'],\n              'gamma':[0.001, 0.1, 0.5],\n               }\ngrid_svc = GridSearchCV (estimator = clf1, param_grid = parametres, cv = 5) # fold cross validation\nbst_svc = grid_svc.fit(train_scaled,y_train)","e318bb65":"print(pd.DataFrame.from_dict(bst_svc.cv_results_).loc[:,[\"params\",\"mean_test_score\"]]) ","1bf0f17c":"grid_svc.best_params_","8c098779":"# Accuracy of the training set \/ test\nprint('Accuracy of the training set :', bst_svc.score(train_scaled,y_train))\nprint('Accuracy of the test set :', bst_svc.score(test_scaled,y_test))","6b21eed6":"y_pred_svc = bst_svc.predict(test_scaled)\npd.crosstab(y_test, y_pred_svc, rownames=['Actual'], colnames=['Predicted'], normalize = False, margins = True)","8b7932d6":"print(classification_report(y_pred_svc, y_test))","544fb849":"# From the classification report we can see that our previous accuracy 81% came from over-prediction on the majority class (0) .\n# As you can see the precision is low for class (1) , which means that this model can't detect good enough the survivors.","1ab66109":"# Let's see if we can do better with XGBOOST\n","da0efbcc":"# Tuning parmameters with GridSearchCV \nparametres = {'max_depth': [3, 5, 7, 9, 11] ,\n              'learning_rate' :[0.1, 0.01, 1],\n              'n_estimators' : [100, 200, 300],\n              }\ngrid_xgb = GridSearchCV (estimator = xgb.XGBClassifier(use_label_encoder = False,verbosity=0), param_grid = parametres, cv = 5) # fold cross validation\ngrid_xgb.fit(train_scaled, y_train)","031f6894":"#I got this warning in Kaggle notebook \n#The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following:\n#1) Pass option use_label_encoder =False when constructing XGBClassifier object;\n#2) Encode your labels (y)","48f44916":"grid_xgb.best_params_\n","82cdb4a3":"# Accuracy of the training set \/ test\nprint('Accuracy of the training set :',grid_xgb.score(train_scaled,y_train))\nprint('Accuracy of the test set :',grid_xgb.score(test_scaled,y_test))","2c599676":"xgb_model = xgb.XGBClassifier(booster='gbtree', learning_rate = 0.01, n_estimators = 300, max_depth = 7,use_label_encoder = False,verbosity=0)\neval_set = [(train_scaled, y_train), (test_scaled, y_test)]\nxgb_model.fit(train_scaled, y_train, eval_metric=[\"error\", \"logloss\"], eval_set = eval_set, early_stopping_rounds = 25 , verbose=False)","43d1434f":"y_pred_xgb = np.round(xgb_model.predict(test_scaled))\naccuracy = accuracy_score(y_test, y_pred_xgb)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","3a84942c":"pd.crosstab(y_test,y_pred_xgb, rownames=['Actual'], colnames=['Predicted'], normalize = False, margins = True)","e05677a2":"print(classification_report(y_pred_xgb, y_test))","e2c833a7":"# we can see the gap between training and valdiation accuracy so the model is overfitting\n# Still we have better precision and recall for class (1), which is a good thing. XGB preforms better than SVC","47bdc7e0":"\nresults = xgb_model.evals_result()\nepochs = len(results[\"validation_0\"][\"error\"])\nx_axis = range(0, epochs)\n\n# plot log loss\nfig, ax = plt.subplots(figsize=(10,5))\nax.plot(x_axis, results[\"validation_0\"][\"logloss\"], label=\"Train\")\nax.plot(x_axis, results[\"validation_1\"][\"logloss\"], label=\"Test\")\nax.legend()\nplt.ylabel(\"Log Loss\")\nplt.title(\"XGBoost Log Loss\")\nplt.show()\n\n# plot classification error\nfig, ax = plt.subplots(figsize=(10,5))\nax.plot(x_axis, results[\"validation_0\"][\"error\"], label=\"Train\")\nax.plot(x_axis, results[\"validation_1\"][\"error\"], label=\"Test\")\nax.legend()\nplt.ylabel(\"Classification Error\")\nplt.title(\"XGBoost Classification Error\")\nplt.show()","854038e3":"xgb_model.get_booster().feature_names = ['Pclass','Sex','Age', 'SibSp', 'Parch','Embarked']","a5e19b9c":"#we can check features importance using the method below \ntypes= ['weight', 'gain', 'cover']\n#'Weight' (default): the relative number of times a feature appears in the model trees.\n#'Cover': The number of times a feature is used to separate data across all trees, weighted by the number of training data that passes through those separations.\n#'Gain': The average reduction in the loss function obtained when using a feature to separate a branch.\nfor f in types:\n    xgb.plot_importance(xgb_model, importance_type = f, title='Importance: '+f)","1e11c0b8":"cd ..\/features-namestxt\/","c38c58c8":"xgb.to_graphviz(xgb_model,fmap = 'features_names.txt')\n# we can see how the model make its decision based on decision trees.","e7974bb8":"cd ..\/titanic","dc91f7fc":"# Let's see if we can use deep learning approach to solve this classification problem. Fully connected neurals netowrks preform well on those kind of problem.  \n","819d7581":"np.random.seed(0)\ntf.random.set_seed(0)\nmodel = models.Sequential()\n\nmodel.add(layers.Dense(units = 32 ,kernel_regularizer = regularizers.l2(0.001), activation='relu', input_shape = (6,)))\n\nmodel.add(layers.Dense(units = 32 ,kernel_regularizer = regularizers.l2(0.001), activation='relu'))\n\n\n#I went with a simple and small architecte because bigger ones start overfitting from the beginning \n# Adding weight regularization will reduce overfitting (you can try dropout too)\n\nmodel.add(layers.Dense(units = 1, activation='sigmoid'))","507bee8d":"model.summary()","f5d6f348":"\nmodel.compile(optimizer = 'rmsprop',\n                loss = 'binary_crossentropy', # loss function \n                metrics = ['acc'])","2ba2d586":"history = model.fit(x = train_scaled,\n                    y = y_train,\n                    epochs = 8,\n                    batch_size = 32,\n                    validation_data = (test_scaled,y_test),\n                    shuffle = False  )","c7170283":"#Plotting the training and validation loss\/accuracy\n\nloss_values = history.history['loss']\nval_loss_values = history.history['val_loss']\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nepochs = range(1, len(acc) + 1)\n\n\nplt.figure(figsize = (15,5))\nplt.subplot(121)\nplt.plot(epochs, acc, label = 'Training accuracy' )\nplt.plot(epochs, val_acc, label = 'Validation accuracy')\nplt.title(\"Training and validation accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.legend()\nplt.subplot(122)\nplt.plot(epochs, loss_values, label = 'Training loss' )\nplt.plot(epochs, val_loss_values, label = 'Validation loss')\nplt.title(\"Training and validation loss\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend()\n\nplt.show()","d1d7f41d":"test_loss, test_acc = model.evaluate(test_scaled, y_test)\ntrain_loss, train_acc = model.evaluate(train_scaled, y_train)\nprint('Train accuracy :', train_acc)\nprint('Test accuracy :', test_acc)\n","b400d9fb":"y_pred_dense = model.predict(test_scaled)\npd.crosstab(y_test,np.round(y_pred_dense.reshape(-1)), rownames=['Actual'], colnames=['Predicted'], normalize = False, margins = True)","d4b9ad19":"print(classification_report(np.round(y_pred_dense.reshape(-1)), y_test))","ca1ed7e8":"# With a good accuracy on training and validation set and good precision and recall for class (1) with no over-prediction on the majority of class (0) as seen before.\n# Therefore in my opinion this model is our best for now. I got 0.78708 with this model (Top 13%) ","0b5f8446":"#Preprocessing on test data same as before\ntest_data = pd.read_csv('test.csv')\nsubmission = pd.DataFrame({'PassengerId':test_data['PassengerId']})","d1911715":"test_data.Age = Random_Fillna_Gauss(test_data.Age) # Fillna with normal distribution in \"Age\"\ntest_data.Embarked.fillna(test_data.Embarked.mode()[0], inplace = True) #Fillna with the most occurring in 'Embarked'\n\n#test_data.Fare.fillna(test_data[test_data.Embarked == 'S'].Fare.mean(), inplace = True) # Fill one Nan value with mean for 'Fare', embarked from 'S'\ntest_data.Embarked = encoder.fit_transform(test_data.Embarked) #Vectorizing \"Embarked\"\ntest_data.Sex.replace( {'male': 0, 'female' : 1},inplace = True) #Vectorizing 'Sex'\n\ncol_to_drop = ['PassengerId','Name','Ticket','Cabin','Fare']\ntest_data.drop(col_to_drop, axis = 1 ,inplace = True) # drop useless columns","68985f9f":"test_data.info()","3deebf56":"test_scaled_sub = scaler.transform(test_data)\ntest_scaled_sub.shape","a86226fd":"# to submit \n\n#xgb_model.get_booster().feature_names = ['f0', 'f1', 'f2', 'f3', 'f4', 'f5']\n#y_pred_xgb_sub = np.round(xgb_model.predict(test_scaled_sub))\n#y_pred_xgb_sub.shape","a8465753":"y_pred_dense_sub = model.predict(test_scaled_sub)","713f60b3":"submission['Survived'] =np.round(y_pred_dense_sub).astype('int') ","3b793d0f":"cd ..\/..\/working\n","716b1358":"submission.to_csv('Titanic predections',index=False)","f8d6bdbd":"# Preprocessing","9e213ea3":"###Variables Notes\n**Pclass**: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\n**Age**: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n**Sibsp**: The dataset defines family relations in this way...\n*Sibling* = brother, sister, stepbrother, stepsister\n*Spouse* = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n**Parch**: The dataset defines family relations in this way...\n*Parent* = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.","c6896363":"##Import data and libraires ","47fd86f3":"## A first classification model (Support Vector machines)","9950a32b":"# Machine learning ","0fe2a941":"#Data exploration, descriptive statistics\n\n\n\n","955ca64c":"\n##Variable\tDefinition\tKey\n\n**Survival** --->\tSurvival\t0 = No, 1 = Yes\n\n**Pclass** --->\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\n\n**Sex**\t---> male \/ female\n\n**Age** --->\tAge in years\t\n\n**sibsp** --->\tnumber of siblings \/ spouses aboard the Titanic\t\n\n**Parch** --->\tnumber of parents \/ children aboard the Titanic\t\n\n**Ticket** --->\tTicket number\t\n\n**Fare** --->\tPassenger fare\n\n**Cabin** --->\tCabin number\t\n\n**Embarked** --->\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton\n","d66dbdc1":"## XGBOOST","2457ffa0":"# Deep learning"}}