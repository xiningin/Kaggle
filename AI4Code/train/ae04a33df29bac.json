{"cell_type":{"6e94576d":"code","4a483212":"code","986107b9":"code","a0a8691c":"code","0c21caf6":"code","2513d60e":"code","1a655e3b":"code","62b380e6":"code","5753f692":"code","41ef40bb":"code","aa646b81":"code","e05829ad":"code","40ab9a09":"code","e8e4d9f2":"code","ee7b2f8f":"code","1bcae9bb":"code","d41aa70c":"code","110fe988":"code","9e650229":"code","d34d8225":"code","30539587":"code","f7228fc3":"code","aa05c658":"code","dab0f0a1":"code","ff34725a":"code","d0ab1c6f":"code","60f0a5ba":"code","9d6da7bc":"code","0ec684c7":"code","b96130ad":"code","3df3162a":"code","a22e9b3e":"code","4791b45d":"code","769af09e":"code","d0c9a1cc":"code","61937a0b":"code","2eb605a4":"code","cfdb67a1":"code","0e2c4bef":"code","1a4a5053":"code","40b4a633":"code","311c8d8b":"code","1d0ea60e":"code","aaa4f0c4":"code","d82d652d":"code","b08ee5eb":"code","23789c61":"code","ee8feb50":"code","88d37602":"code","6848079e":"code","3c601146":"markdown","118e8ce9":"markdown","fb491242":"markdown","d28192bb":"markdown","68526041":"markdown","887031be":"markdown","c2f4ead7":"markdown","e25c6668":"markdown"},"source":{"6e94576d":"%matplotlib inline\n%reload_ext autoreload\n%autoreload 2","4a483212":"import numpy as np \nimport pandas as pd\nimport os\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.metrics import precision_score, recall_score\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nimport time\nimport torch\nimport transformers\nfrom transformers import BertConfig\nfrom transformers import get_linear_schedule_with_warmup","986107b9":"import sys\nsys.path.insert(1, '..\/input\/tweet-classification-huggingface-wandb1\/')","a0a8691c":"import config\nimport dataset\nimport model\nimport train\nimport util","0c21caf6":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","2513d60e":"warmup_epochs = 0\nepochs = 5\nmodel_name = 'bert-base-uncased'\ntest_size = 0.3\ndropout = 0.1\nnum_classes = 2\nlinear_in = 768\nmax_len = 160\ntrain_bs = 16\nvalid_bs = 8\nstart_lr = 2e-5\n\nSEED = 42","1a655e3b":"util.set_seed(SEED)","62b380e6":"train_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","5753f692":"import preprocess\nimport re\nimport string","41ef40bb":"ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\ntrain_df = preprocess.fix_erraneous(train_df, ids_with_target_error, 0)","aa646b81":"def remove_tabs_newlines(text):\n    text = re.sub(r'\\t', ' ', text) # remove tabs\n    text = re.sub(r'\\n', ' ', text) # remove line jump\n    return text","e05829ad":"train_df['text'] = train_df['text'].apply(lambda k : remove_tabs_newlines(k))\ntest_df['text'] = test_df['text'].apply(lambda k : remove_tabs_newlines(k))","40ab9a09":"def remove_url(text):\n    return re.sub(r\"http\\S+\", \"URL\", text)","e8e4d9f2":"def remove_non_ascii(text):\n    return ''.join([x for x in text if x in string.printable])","ee7b2f8f":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","1bcae9bb":"def remove_usrname(text):\n    text = re.sub(r'@\\S{0,}', ' USER ', text)\n    text = re.sub(r\"\\s+\", \" \", text)\n    text = re.sub(r'\\b(USER)( \\1\\b)+', r'\\1', text)\n    return text","d41aa70c":"# A list of contractions from \n# http:\/\/stackoverflow.com\/questions\/19790188\/expanding-english-language-contractions-in-python\ncontractions = { \n    \"ain't\": \"am not\",\n    \"aren't\": \"are not\",\n    \"can't\": \"cannot\",\n    \"can't've\": \"cannot have\",\n    \"'cause\": \"because\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"couldn't've\": \"could not have\",\n    \"didn't\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"don't\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he'd\": \"he would\",\n    \"he'd've\": \"he would have\",\n    \"he'll\": \"he will\",\n    \"he's\": \"he is\",\n    \"how'd\": \"how did\",\n    \"how'll\": \"how will\",\n    \"how's\": \"how is\",\n    \"i'd\": \"i would\",\n    \"i'll\": \"i will\",\n    \"i'm\": \"i am\",\n    \"i've\": \"i have\",\n    \"isn't\": \"is not\",\n    \"it'd\": \"it would\",\n    \"it'll\": \"it will\",\n    \"it's\": \"it is\",\n    \"let's\": \"let us\",\n    \"ma'am\": \"madam\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"needn't\": \"need not\",\n    \"oughtn't\": \"ought not\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"she'd\": \"she would\",\n    \"she'll\": \"she will\",\n    \"she's\": \"she is\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"that'd\": \"that would\",\n    \"that's\": \"that is\",\n    \"there'd\": \"there had\",\n    \"there's\": \"there is\",\n    \"they'd\": \"they would\",\n    \"they'll\": \"they will\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"wasn't\": \"was not\",\n    \"we'd\": \"we would\",\n    \"we'll\": \"we will\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"weren't\": \"were not\",\n    \"what'll\": \"what will\",\n    \"what're\": \"what are\",\n    \"what's\": \"what is\",\n    \"what've\": \"what have\",\n    \"where'd\": \"where did\",\n    \"where's\": \"where is\",\n    \"who'll\": \"who will\",\n    \"who's\": \"who is\",\n    \"won't\": \"will not\",\n    \"wouldn't\": \"would not\",\n    \"you'd\": \"you would\",\n    \"you'll\": \"you will\",\n    \"you're\": \"you are\",\n    \"thx\"   : \"thanks\"\n}\n\ndef remove_contractions(text):\n    return contractions[text.lower()] if text.lower() in contractions.keys() else text","110fe988":"def remove_punctuations(text):\n    return text.translate(str.maketrans('', '', string.punctuation))","9e650229":"train_df['text'] = train_df['text'].apply(lambda k: remove_usrname(k))\ntrain_df['text'] = train_df['text'].apply(lambda k: remove_contractions(k))\ntrain_df['text'] = train_df['text'].apply(lambda k: remove_url(k))\ntrain_df['text'] = train_df['text'].apply(lambda k: remove_emoji(k))\ntrain_df['text'] = train_df['text'].apply(lambda k: remove_non_ascii(k))\ntrain_df['text'] = train_df['text'].apply(lambda k: remove_punctuations(k))","d34d8225":"test_df['text'] = test_df['text'].apply(lambda k: remove_usrname(k))\ntest_df['text'] = test_df['text'].apply(lambda k: remove_contractions(k))\ntest_df['text'] = test_df['text'].apply(lambda k: remove_url(k))\ntest_df['text'] = test_df['text'].apply(lambda k: remove_emoji(k))\ntest_df['text'] = test_df['text'].apply(lambda k: remove_non_ascii(k))\ntest_df['text'] = test_df['text'].apply(lambda k: remove_punctuations(k))","30539587":"def cleanup(text):\n    text = re.sub(r\"\\s+\", \" \", text)\n    text = text.strip()\n    return text","f7228fc3":"def replace_nums(text):\n    text = re.sub(r'^\\d\\S{0,}| \\d\\S{0,}| \\d\\S{0,}$', ' NUMBER ', text)\n    text = re.sub(r\"\\s+\", \" \", text)\n    text = re.sub(r'\\b(NUMBER)( \\1\\b)+', r'\\1', text)\n    text = re.sub(r\"[0-9]\", \" \", text)\n    return text","aa05c658":"train_df['text'] = train_df['text'].apply(lambda k: replace_nums(k))\ntest_df['text'] = test_df['text'].apply(lambda k: replace_nums(k))","dab0f0a1":"train_df['text'] = train_df['text'].apply(lambda k: cleanup(k))\ntest_df['text'] = test_df['text'].apply(lambda k: cleanup(k))","ff34725a":"train_df['text'][:5]","d0ab1c6f":"train_y = train_df['target']\ntrain_df.drop(['target'], axis=1, inplace=True)","60f0a5ba":"X_train, X_test, y_train, y_test \\\n        = train_test_split(train_df['text'], train_y, random_state=SEED, test_size=test_size, stratify=train_y.values) ","9d6da7bc":"tokenizer = transformers.BertTokenizer.from_pretrained(\n        model_name,\n        do_lower_case=True\n    )","0ec684c7":"train_dataset = dataset.BertDataset(\n        text=X_train.values,\n        tokenizer= tokenizer,\n        max_len= max_len,\n        target=y_train.values\n    )","b96130ad":"valid_dataset = dataset.BertDataset(\n        text=X_test.values,\n        tokenizer= tokenizer,\n        max_len= max_len,\n        target=y_test.values\n    )","3df3162a":"train_dl = torch.utils.data.DataLoader(\n        train_dataset,\n        train_bs,\n        shuffle=True,\n        num_workers=4\n    )","a22e9b3e":"valid_dl = torch.utils.data.DataLoader(\n        valid_dataset,\n        valid_bs,\n        shuffle=True,\n        num_workers=1\n    )","4791b45d":"bert = model.BertHf(model_name, dp=dropout, num_classes=num_classes, linear_in=linear_in)\nbert = bert.to(device)","769af09e":"for param in bert.parameters():\n    param.requires_grad = True","d0c9a1cc":"no_decay = ['bias', 'LayerNorm.weight', 'LayerNorm.bias']\nparams = list(bert.named_parameters())\nmodified_params = [\n    {'params': [p for n, p in params if not any(nd in n for nd in no_decay)], 'weight_decay': 0.1},\n    {'params': [p for n, p in params if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n]","61937a0b":"optim = torch.optim.AdamW(modified_params, lr=start_lr, eps=1e-8)","2eb605a4":"total_steps = int(len(train_df) * epochs \/ train_bs)\nwarmup_steps = int(len(train_df) * warmup_epochs \/ train_bs)","cfdb67a1":"sched = get_linear_schedule_with_warmup(optim, num_warmup_steps=warmup_steps, num_training_steps=total_steps)","0e2c4bef":"criterion = torch.nn.CrossEntropyLoss()","1a4a5053":"train_stat = util.AvgStats()\nvalid_stat = util.AvgStats()","40b4a633":"best_acc = 0.0\nbest_model_file = str(model_name) + '_best.pth.tar'","311c8d8b":"print(\"\\nEpoch\\tTrain Acc\\tTrain Loss\\tTrain F1\\tValid Acc\\tValid Loss\\tValid F1\")\nfor i in range(epochs):\n    start = time.time()\n    losses, ops, targs = train.train(train_dl, bert, criterion, optim, sched, device)\n    duration = time.time() - start\n    train_acc = accuracy_score(targs, ops)\n    train_f1_score = f1_score(targs, ops)\n    train_loss = sum(losses)\/len(losses)\n    train_prec = precision_score(targs, ops)\n    train_rec = recall_score(targs, ops)\n    train_roc_auc = roc_auc_score(targs, ops)\n    train_stat.append(train_loss, train_acc, train_f1_score, train_prec, train_rec, train_roc_auc, duration)\n    start = time.time()\n    lossesv, opsv, targsv = train.test(valid_dl, bert, criterion, device)\n    duration = time.time() - start\n    valid_acc = accuracy_score(targsv, opsv)\n    valid_f1_score = f1_score(targsv, opsv)\n    valid_loss = sum(lossesv)\/len(lossesv)\n    valid_prec = precision_score(targsv, opsv)\n    valid_rec = recall_score(targsv, opsv)\n    valid_roc_auc = roc_auc_score(targsv, opsv)\n    valid_stat.append(valid_loss, valid_acc, valid_f1_score, valid_prec, valid_rec, valid_roc_auc, duration)\n\n    if valid_acc > best_acc:\n        best_acc = valid_acc\n        util.save_checkpoint(bert, True, best_model_file)\n        tfpr, ttpr, _ = roc_curve(targs, ops)\n        train_stat.update_best(ttpr, tfpr, train_acc, i)\n        vfpr, vtpr, _ = roc_curve(targsv, opsv)\n        valid_stat.update_best(vtpr, vfpr, best_acc, i)\n\n    print(\"\\n{}\\t{:06.8f}\\t{:06.8f}\\t{:06.8f}\\t{:06.8f}\\t{:06.8f}\\t{:06.8f}\".format(i+1, train_acc*100, train_loss, \n                                                train_f1_score, valid_acc*100, \n                                                valid_loss, valid_f1_score))","1d0ea60e":"print(\"Summary of best run::\")\nprint(\"Best Accuracy: {}\".format(valid_stat.best_acc))\nprint(\"Roc Auc score: {}\".format(valid_stat.roc_aucs[valid_stat.best_epoch]))\nprint(\"Loss: {}\".format(valid_stat.losses[valid_stat.best_epoch]))\nprint(\"Area Under Curve: {}\".format(auc(valid_stat.fprs, valid_stat.tprs)))","aaa4f0c4":"test_dataset = dataset.BertDataset(\n        text=test_df.text.values,\n        tokenizer= tokenizer,\n        max_len= max_len\n    )","d82d652d":"test_dl = torch.utils.data.DataLoader(\n        test_dataset,\n        1,\n        shuffle=False,\n        num_workers=1\n    )\n","b08ee5eb":"util.load_checkpoint(bert, best_model_file)","23789c61":"_, opst, _ = train.test(test_dl, bert, criterion, device)","ee8feb50":"sub_csv = pd.DataFrame(columns=['id', 'target'])","88d37602":"sub_csv['id'] = test_df['id']\nsub_csv['target'] = opst","6848079e":"sub_csv.to_csv('submission.csv', index=False)","3c601146":"# Submission","118e8ce9":"# Model","fb491242":"# Huggingface Transformer Pretrained Model\n\nThis is very basic introductory repository which uses pretrained huggingface tokenizer and bert model.<br>\nThis uses on 'text' field.<br>\n<br>\nRepository can be found: https:\/\/github.com\/nachiket273\/Tweet_Classification_Huggingface_Wandb <br>\n<br>\n\nFollowing steps are performed: <br>\n<br>\n1) Pre-processing:<br>\n    i) Remove urls , non-ascii characters, emoji's,  punctuations, contractions (standard preprocessing <br>\n    most of the notebooks :) <br>\n    ii) to-lower <br>\n<br>   \n2) Class - Balancing : Sampling from class with more training example to balance class.<br>\n<br>\n3) Model, Optimizer and Other configurations:<br>\ntokenizer and bert model = 'bert-base-uncased' <br>\noptimizer= AdamW <br>\nscheduler linear with warmup <br>\nstart_lr = 2e-5 <br>\ntrain_bs = 8 <br>\nvalid_bs = 8 <br>\nepochs = 5 <br>\nmax_len = 160 <br>\ndropout_ratio = 0.1 <br>\nwarmup_epochs = 0 <br>\ntest_size = 0.2 <br>\n<br>\n4) Ran for 3 different seed - 42, 11, 2020 and averaged the predictions.<br>\n<br>\n5)Repository also contains basic visualizations and tracking using 'weights and biases'.<br>\n<br>","d28192bb":"# **Encode and get dataloaders**","68526041":"# Split Dataset","887031be":"# **Preprocessing**","c2f4ead7":"# Now make predictions","e25c6668":"# Now Train"}}