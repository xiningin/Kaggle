{"cell_type":{"a575109b":"code","af64c70f":"code","9a2c6cbf":"code","ff07db1a":"code","bb59b30b":"code","28981113":"code","563434a0":"code","69703a09":"code","6c780001":"code","66188d72":"code","906ff777":"code","1ad04c79":"code","3c740109":"code","d30d8f5d":"code","e349f6d2":"code","fed42273":"code","78a5261f":"code","354d5aab":"code","10016b6d":"code","eeea2186":"code","e0e8a7cd":"code","d58081d6":"code","5f386948":"code","f3ce375e":"code","79872ec2":"code","59a808d0":"code","bd83dd0a":"code","d2b3446d":"code","aa88a734":"code","f4afa771":"code","141eb801":"code","6cc55f05":"code","c55d40a5":"code","bb2a5927":"code","8b3e974a":"code","32d4323c":"code","e0bb4127":"code","86155090":"code","c4d558ca":"code","7cedd3c3":"code","58a5949b":"code","ada2b4ab":"code","679d5bcd":"code","183421ca":"code","b80b3640":"code","a782dff2":"code","e9a18b73":"code","ad63adb0":"code","8db49d86":"code","ab30f6e9":"code","8d73a21a":"code","394ff5f6":"code","c3dc6e3f":"code","4aaf8813":"code","5a3dd71f":"code","045e2263":"code","021924d5":"code","5f4e96f9":"code","2154ee50":"code","5a2cb53d":"code","2e21b52c":"code","b39b7d1f":"code","2824cb48":"code","53791658":"code","faf893c6":"code","fb6ef124":"code","ceadfca6":"code","e6f57108":"code","c761917e":"code","31b0c1ab":"code","b91e8ba4":"markdown","c58f4392":"markdown","4e503ade":"markdown","1829b504":"markdown","1809b088":"markdown","725c0969":"markdown","7c8d8b4b":"markdown","15632ca6":"markdown","03c9ba6e":"markdown","58cd9b6f":"markdown","167280d0":"markdown","a9548ab6":"markdown","5d4d9f61":"markdown","e1b3c046":"markdown","46f981ae":"markdown","26778744":"markdown","3c1697fe":"markdown","8c9c73e3":"markdown","2ee2efed":"markdown","5204ca5d":"markdown","52611533":"markdown","5f354a02":"markdown","cde683f1":"markdown","6a02f5d1":"markdown","6b6c65aa":"markdown","4f7e12b9":"markdown","df41b740":"markdown","7233dfbb":"markdown","e9a0506e":"markdown","84683048":"markdown","bfccb8db":"markdown","fe9769b5":"markdown","11f8f982":"markdown","1764b08d":"markdown","0df47c59":"markdown","4bf7060f":"markdown","cc3552ba":"markdown","b7e02f74":"markdown","cf35de39":"markdown","bc30cdbd":"markdown","edf93212":"markdown","ff02ead0":"markdown","4374ae52":"markdown","849fa13f":"markdown","aeaa52de":"markdown","9f064065":"markdown","465c1fbb":"markdown","9fead200":"markdown","10a2d59a":"markdown","dec1199f":"markdown","381446ec":"markdown","dc298144":"markdown"},"source":{"a575109b":"import numpy as np\nfrom numpy import mean\nfrom numpy import std\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom statsmodels.sandbox.regression.predstd import wls_prediction_std\nimport math\nfrom sklearn import feature_selection \nfrom sklearn.model_selection import GridSearchCV,train_test_split,KFold, cross_val_score\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, LassoCV, ElasticNet\nfrom sklearn.feature_selection import RFE\nfrom sklearn.feature_selection import SelectFromModel, RFE\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.preprocessing import PolynomialFeatures, PowerTransformer, FunctionTransformer\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom category_encoders import OneHotEncoder \nfrom scipy.stats import skew\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.pipeline import make_pipeline\nimport xgboost as xgb\nfrom sklearn.pipeline import Pipeline","af64c70f":"from category_encoders import OneHotEncoder ","9a2c6cbf":"house_ts = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv',index_col=0)\nhouse_tr = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv',index_col=0)\n","ff07db1a":"# check shape of the train and test sets\nprint(house_ts.shape)\nprint(house_tr.shape)\n\n# Separate the price from train set\ny_tr = house_tr['SalePrice']\nprint(y_tr.shape)","bb59b30b":"cols = house_tr.columns.intersection(house_ts.columns)\n\n# Train shape and Length\nhouse_tr = house_tr[cols]\nprint(house_tr.shape)\ntrain_length = len(house_tr)\n\nhouse_ts = house_ts[cols]\nprint(house_ts.shape)\ntest_length = len(house_ts)","28981113":"# Concat the two dataframes\ntrain_test = pd.concat([house_tr,house_ts],axis=0,sort=False)\nprint(train_test.shape)","563434a0":"# Heatmap highlighting Null counts from train set\nplt.figure(figsize = (28,20))\nsns.heatmap(train_test.isnull(), yticklabels=False,cmap=\"PiYG\", cbar=False)","69703a09":"# Check all features with more than half columns nans from test set\nfor column in train_test.columns:\n  if train_test[column].isnull().sum()> len(train_test)\/2:\n    print(column)\n\n# Delete these columns\ntrain_test = train_test.dropna(thresh=len(train_test)\/2,axis=1)\nprint('New Shape ',train_test.shape)","6c780001":"# object datatypes for the joint datasets\nobj_tr = train_test.select_dtypes(include=['object']).copy()\nprint(obj_tr.shape)\n\n# numeric datatypes from the joint datasets\nnum_tr = train_test.select_dtypes(include=['int64','float64']).copy()\nprint(num_tr.shape)","66188d72":"# Train Set\nnum_trn = num_tr.loc[0:train_length,:]\nobj_trn = obj_tr.loc[0:train_length,:]\n\n# Test Set\nnum_tst = num_tr.loc[train_length+1:,:]\nobj_tst = obj_tr.loc[train_length+1:,:]","906ff777":"# Replace numeric nans with zero in the train set\ntr_num_nan=[feature for feature in num_trn.columns if num_trn[feature].isnull().sum()>=1 and num_trn[feature].dtypes!='O']\nfor feature in tr_num_nan:\n    # num_tr[feature].fillna(0,inplace=True)\n  \n    ## We will replace by using median since there are outliers\n    median_value=num_trn[feature].median()\n    num_trn[feature].fillna(median_value,inplace=True)\n\n# Count null in train set \nnum_trn[tr_num_nan].isnull().sum()\n\n# Replace numeric nans with zero in the test set\ntr_num_nan=[feature for feature in num_tst.columns if num_tst[feature].isnull().sum()>=1 and num_tst[feature].dtypes!='O']\nfor feature in tr_num_nan:\n  \n    ## We will replace by using median since there are outliers\n    median_value=num_tst[feature].median()\n    num_tst[feature].fillna(median_value,inplace=True)\n\n# Count null in test set \nnum_tst[tr_num_nan].isnull().sum()","1ad04c79":"num_tr_ts = pd.concat([num_trn,num_tst],axis=0,sort=False)\nprint(num_tr_ts.shape)","3c740109":"from scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom scipy.stats import boxcox\nfrom scipy.special import inv_boxcox\n\nnumeric_features = list(num_tr_ts.columns)\n\n#Absolute Skewness of features\nfrom scipy.stats import skew\nskewed_features = num_tr_ts[numeric_features].apply(lambda x: abs(skew(x))).sort_values(ascending=False)\nprint(skewed_features)\n  ","d30d8f5d":"#test\nhigh_skewness = skewed_features[skewed_features >= 1]\nskewed_features = high_skewness.index","e349f6d2":"# skewed features Train set\nnum_tr_tsk = num_tr_ts[list(skewed_features)]\nnum_tr_tsk = num_tr_tsk.iloc[0:train_length,:]\n\n# plt.style.use('dark_background')\nfig, axes = plt.subplots(10, 2,figsize=(20,80))\nfig.subplots_adjust(hspace=0.6)\ncolors=[plt.cm.prism_r(each) for each in np.linspace(0, 1, len(num_tr_tsk.columns))]\nfor i,ax,color in zip(num_tr_tsk,axes.flatten(),colors):\n    sns.regplot(x=num_tr_tsk[i], y= y_tr, fit_reg=True,marker='o',scatter_kws={'s':50,'alpha':0.8},color=color,ax=ax)\n    plt.xlabel(i,fontsize=12)\n    plt.ylabel('SalePrice',fontsize=12)\n    ax.set_title('SalePrice'+' - '+str(i),color=color,fontweight='bold',size=20)","fed42273":"# Create the dataframe of top 5 most skeweed features for Vizualization\n# top_5 = list(skewed_features)[0:5]\n# top_5sk = num_tr_ts.loc[:,top_5]\n\n# fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(22,5))\n\n# # flatten axes for easy iterating\n# for i, ax in enumerate(axes.flatten()):\n#   sns.distplot(top_5sk.iloc[:, i], ax=ax)\n\n# fig.tight_layout()\n\n# # plot the top 4 skewed features before\n# fig, ax=plt.subplots(1,2,figsize=(20,5))\n# sns.distplot(num_tr_ts['MiscVal'], ax=ax[0])\n# sns.distplot(num_tr_ts['PoolArea'], ax=ax[1])\n# sns.distplot(num_tr_ts['LotArea'], ax=ax[2])\n# sns.distplot(num_tr_ts['LowQualFinSF'], ax=ax[3])","78a5261f":"# Apply a Power Transformer since power transformers are good at reducing Heteroskedacity\nnum_unskwed = PowerTransformer(method='yeo-johnson').fit_transform(num_tr_ts[list(skewed_features)])\nnum_unskw = pd.DataFrame(num_unskwed,columns=list(skewed_features))","354d5aab":"numeric_after = list(num_unskw.columns)\n\nskewed_after = num_unskw[numeric_after].apply(lambda x: skew(x)).sort_values(ascending=False)\nprint(skewed_after)","10016b6d":"from scipy import stats\nfig, ax=plt.subplots(1,2,figsize=(20,5))\nax[0].set_title('Histogram of SalePrice')\nsns.distplot(y_tr, ax=ax[0])\n\nax[1].set_title('QQ_plot')\n    ## Plotting the QQ_Plot. \nstats.probplot(y_tr, plot=ax[1])","eeea2186":"fig, ax=plt.subplots(1,2,figsize=(20,5))\n\nax[0].set_title('Log Transformed SalePrice')\nsns.distplot(np.log1p(y_tr), ax=ax[0])\n\nstats.probplot(np.log1p(y_tr), plot=ax[1])","e0e8a7cd":"# Unskwed features Train set\nnum_unskw_Tr = num_unskw.iloc[0:train_length,:]\n\n# plt.style.use('dark_background')\nfig, axes = plt.subplots(10, 2,figsize=(20,80))\nfig.subplots_adjust(hspace=0.6)\ncolors=[plt.cm.prism_r(each) for each in np.linspace(0, 1, len(num_unskw_Tr.columns))]\nfor i,ax,color in zip(num_unskw_Tr,axes.flatten(),colors):\n    sns.regplot(x=num_unskw_Tr[i], y= np.log1p(y_tr), fit_reg=True,marker='o',scatter_kws={'s':50,'alpha':0.8},color=color,ax=ax)\n    plt.xlabel(i,fontsize=12)\n    plt.ylabel('SalePrice',fontsize=12)\n    # ax.set_yticks(np.arange(-100000,900001,100000))\n    ax.set_title('SalePrice'+' - '+str(i),color=color,fontweight='bold',size=20)","d58081d6":"from sklearn.preprocessing import MinMaxScaler\n\nskew_over_1 = num_unskw[['PoolArea','KitchenAbvGr','3SsnPorch','BsmtHalfBath','BsmtFinSF2','LowQualFinSF','EnclosedPorch']]\nnum_unskwed_MinMax = MinMaxScaler().fit_transform(skew_over_1 )\nnum_unskwed_MinMax = pd.DataFrame(num_unskwed_MinMax,columns=skew_over_1.columns)\nnum_unskwed_MinMax","5f386948":"print(num_unskwed_MinMax['PoolArea'].value_counts())\nprint(num_unskwed_MinMax['KitchenAbvGr'].value_counts())\nprint(num_unskwed_MinMax['3SsnPorch'].value_counts())\nprint(num_unskwed_MinMax['BsmtHalfBath'].value_counts())\nprint(num_unskwed_MinMax['BsmtFinSF2'].value_counts())\nprint(num_unskwed_MinMax['LowQualFinSF'].value_counts())\nprint(num_unskwed_MinMax['EnclosedPorch'].value_counts())","f3ce375e":"# Custom Transformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass ExperimentalTransformer_2(BaseEstimator, TransformerMixin):\n  # add another additional parameter, just for fun, while we are at it\n  def __init__(self, feature_name):  \n    # print('\\n>>>>>>>init() called.\\n')\n    self.feature_name = feature_name\n\n  def fit(self, X, y = None):\n    # print('\\n>>>>>>>fit() called.\\n')\n    return self\n\n  def transform(self, X, y = None):\n    # print('\\n>>>>>>>transform() called.\\n')\n    X_ = X.copy() # creating a copy to avoid changes to original dataset\n    # Function to Binarize the data\n    X_[self.feature_name] = np.where(X_[self.feature_name] > 0.5, 1,0)\n    return X_\n","79872ec2":"Transformed_df = ExperimentalTransformer_2(['PoolArea','KitchenAbvGr','3SsnPorch','BsmtHalfBath','BsmtFinSF2','LowQualFinSF','EnclosedPorch']).fit_transform(num_unskwed_MinMax)\nTransformed_df","59a808d0":"# x_y['SalePrice1'] = np.log1p(x_y.SalePrice)\n# x_y['SalePrice2'] = np.expm1(x_y.SalePrice1)\n# xy_stdScaler = StandardScaler().fit_transform(x_y[['SalePrice2']])\n# xy_Scaler = StandardScaler().fit_transform(x_y[['SalePrice1']])\n\n# fig, ax=plt.subplots(1,4,figsize=(20,5))\n# sns.distplot(x_y['SalePrice1'], ax=ax[0])\n# sns.distplot(x_y['SalePrice2'], ax=ax[1])\n# sns.distplot(xy_stdScaler, ax=ax[2])\n# sns.distplot(xy_Scaler, ax=ax[3])","bd83dd0a":"train_test_obj = pd.concat([obj_trn,obj_tst],axis=0,sort=False)\nprint(train_test_obj.shape)","d2b3446d":"train_test_obj['MasVnrType']=train_test_obj['MasVnrType'].fillna('MissingMasVnrType')\ntrain_test_obj['BsmtQual']=train_test_obj['BsmtQual'].fillna('MissingBsmtQual')\ntrain_test_obj['BsmtCond']=train_test_obj['BsmtCond'].fillna('MissingBsmtCond')\ntrain_test_obj['BsmtExposure']=train_test_obj['BsmtExposure'].fillna('MissingBsmtExposure')\ntrain_test_obj['BsmtFinType1']=train_test_obj['BsmtFinType1'].fillna('MissingBsmtFinType1')\ntrain_test_obj['BsmtFinType2']=train_test_obj['BsmtFinType2'].fillna('MissingBsmtFinType2')\ntrain_test_obj['FireplaceQu']=train_test_obj['FireplaceQu'].fillna('MissingFireplaceQu')\ntrain_test_obj['GarageType']=train_test_obj['GarageType'].fillna('MissingGarageType')\ntrain_test_obj['GarageFinish']=train_test_obj['GarageFinish'].fillna('MissingGarageFinish')\ntrain_test_obj['GarageQual']=train_test_obj['GarageQual'].fillna('MissingGarageQual')\ntrain_test_obj['GarageCond']=train_test_obj['GarageCond'].fillna('MissingGarageCond')","aa88a734":"# Ordinal Features to numeric\ncleanup_nums = {\"Utilities\":{\"ELO\": 0, \"NoSeWa\": 1, \"NoSewr\": 2, \"AllPub\": 3},\n                \"ExterQual\": {\"Po\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 3},\n                \"ExterCond\": {\"Po\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 3},\n                \"BsmtQual\": {'MissingBsmtQual': 0,\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 4},\n                \"BsmtCond\": {'MissingBsmtCond': 0,\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 4},\n                \"BsmtExposure\":{'MissingBsmtExposure': 0,\"Po\": 1, \"Fa\": 2, \"TA\": 2, \"Gd\": 4},\n                \"BsmtFinType1\":{'MissingBsmtFinType1': 0,\"Unf\": 1, \"LwQ\": 2, \"Rec\": 2, \"BLQ\": 3, \"ALQ\": 3,\"GLQ\":4},\n                \"BsmtFinType2\":{'MissingBsmtFinType2': 0,\"Unf\": 1, \"LwQ\": 2, \"Rec\": 2, \"BLQ\": 3, \"ALQ\": 3,\"GLQ\":6},\n                \"HeatingQC\":{\"Po\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 3},\n                \"CentralAir\":{\"N\": 0, \"Y\": 1},\n                \"KitchenQual\":{\"Po\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 3},\n                \"Functional\":{'Sal':0,'Sev': 0,\"Maj2\": 1, \"Maj1\": 1, \"Mod\": 2, \"Min2\": 3, \"Min1\": 3,\"Typ\":4},\n                \"FireplaceQu\":{'MissingFireplaceQu': 0,\"Po\": 1, \"Fa\": 2, \"TA\": 2, \"Gd\": 3, \"Ex\": 3},\n                \"GarageQual\":{'MissingGarageQual': 0,\"Po\": 1, \"Fa\": 2, \"TA\": 2, \"Gd\": 3, \"Ex\": 3},\n                \"GarageCond\":{'MissingGarageCond': 0,\"Po\": 1, \"Fa\": 2, \"TA\": 2, \"Gd\": 3, \"Ex\": 3}}","f4afa771":"#Observe the values b4 as all objects\ntrain_test_obj.dtypes.value_counts()","141eb801":"# Apply the ordinal Encode\ntrain_test_obj.replace(cleanup_nums,inplace=True)","6cc55f05":"# See the Ordinal Values after are now numeric\ntrain_test_obj.dtypes.value_counts()","c55d40a5":"train_test_obj.shape","bb2a5927":"# Ordinal features only\nobj_tr_tst_ordinal = train_test_obj.select_dtypes(include = ['int64','float64'])","8b3e974a":"# one hot encoding for nominal variable\nobj_1h_tr_ts = train_test_obj.select_dtypes(include=['object']).copy()\n\nmyEncoder = OneHotEncoder(handle_unknown='ignore')\n\ncodestr = myEncoder.fit_transform(obj_1h_tr_ts)\nobj_1h_tr_ts = pd.DataFrame(codestr)","32d4323c":"obj_1h_tr_ts.shape","e0bb4127":"# View each shape\nprint(obj_1h_tr_ts.shape)\nprint(num_tr_ts.shape)\nprint(obj_tr_tst_ordinal.shape)","86155090":"# Merge them\nobj_tr_try = pd.concat([obj_1h_tr_ts,obj_tr_tst_ordinal,num_tr_ts],axis =1)\nobj_tr_try.shape","c4d558ca":"Train_Set = obj_tr_try.iloc[0:train_length,:]\nprint(Train_Set.shape)\nTest_Set = obj_tr_try.iloc[train_length:,:]\nprint(Test_Set.shape)","7cedd3c3":"# Concat the Y\nobj_tr_nw = pd.concat([Train_Set,y_tr],axis =1)\nobj_tr_nw.shape\n\ncorr_new_train=obj_tr_nw.corr().abs()\nplt.figure(figsize=(8,25))\n# Top 40 correlations to price in descending order\ntop_40 = corr_new_train[['SalePrice']].sort_values(by=['SalePrice'],ascending=False).head(50)\nsns.heatmap(top_40,annot_kws={\"size\": 16},vmin=-1, cmap='PiYG', annot=True)\nsns.set(font_scale=2)","58a5949b":"# find the absolute covariances \ncorrmatrix =obj_tr_nw.corr().abs()\n\n# Create a mask for cov matrix and plot the masp\nmask = np.triu(np.ones_like(corrmatrix,dtype=bool))\nx_y_msk = corrmatrix.mask(mask)\n# f, ax = plt.subplots(figsize=(20,15))\n# sns.heatmap(x_y_msk, vmax=0.8, square=True)\n# plt.title('Covariance Matrix of Features before Dropping',fontsize =16)\n\n#Create a list of all correlated features > 0.90\ndrop_vars = [c for c in x_y_msk.columns if any(x_y_msk[c] > 0.90)]\n\n# rankin highest correlated pairs\ns = corrmatrix.unstack()\nso = s.sort_values(ascending=False)\nprint('Top correlated Variables over 90%')\nprint(so[237:262])","ada2b4ab":"top_cov = list(top_40.index)\n# 1 of pair 2 be removed\ncorr_pair = {'SalePrice','SaleCondition_3','Exterior1st_1','GarageFinish_4','GarageType_5','LotShape_2' } \ntop_cov = [ele for ele in top_cov if ele not in corr_pair] ","679d5bcd":"# Select only the Top rated features X\nTrain_Set = Train_Set.dropna(axis=0,how='any')\nTrain_Set = Train_Set[top_cov]","183421ca":"from sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.preprocessing import MinMaxScaler,RobustScaler,FunctionTransformer\nfrom sklearn.preprocessing import StandardScaler\n\n#Scale the data\nrbst_scaler = RobustScaler()\ntrain_std = rbst_scaler.fit_transform(Train_Set)\n# train_rbst = X\n\npca=PCA().fit(train_std)\n#pca=PCA(35).fit(X)\nplt.figure(figsize=(24,15))\nplt.plot(pca.explained_variance_ratio_.cumsum())\nplt.xticks(np.arange(0, 48, 1))\nplt.xlabel('Number of components',fontweight='bold',size=14)\nplt.ylabel('Explanined variance ratio for number of components',fontweight='bold',size=14)\n","b80b3640":"pca_50 = PCA(27)\ntrain_pca=pca_50.fit_transform(train_std)\ntrain_pca.shape\n\n# Variance of the 22 principal components\nVar_count = list(pca_50.explained_variance_ratio_)\nprint(sum(Var_count))\nprint(Var_count)","a782dff2":"from sklearn.neighbors import NearestNeighbors\nneigh = NearestNeighbors(n_neighbors=2)\n\nnbrs = neigh.fit(train_std)\ndistances, indices = nbrs.kneighbors(train_std)\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\nplt.figure(figsize=(15,15))\nplt.plot(distances)\nplt.title('K Distance Curve')\nplt.xlabel('Number of datapoints',fontweight='bold',size=14)\nplt.ylabel('Normalized Distance away from nearest neighbour',fontweight='bold',size=14)\n# plt.yticks(np.arange(0, 8, 0.5))","e9a18b73":"from sklearn.cluster import DBSCAN\n# Initialize the DB Scan\ndbscan = DBSCAN(eps=4, min_samples= 20).fit(train_pca)\ncore_samples_mask = np.zeros_like(dbscan.labels_, dtype=bool)\ncore_samples_mask[dbscan.core_sample_indices_] = True\nlabels=dbscan.labels_\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)","ad63adb0":"plt.style.use('Solarize_Light2')\nunique_labels = set(labels)\nplt.figure(figsize=(12,12))\ncolors = [plt.cm.prism(each)  for each in np.linspace(0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = (labels == k)\n    \n    xy = train_pca[class_member_mask & core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=14)\n\n    xy = train_pca[class_member_mask & ~core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=6)\n\nplt.title('Estimated number of clusters: %d' % n_clusters_)\nplt.show()","8db49d86":"# label these outliers above as -1\nlabels=pd.DataFrame(labels,columns=['Classes'])\nprint(labels[labels['Classes']==-1])","ab30f6e9":"# Concat outliers with Train Set\nX=pd.concat([Train_Set,labels],axis=1)\n\nX = X.dropna(axis=0,how='any')\n\n# located outlier indicies\nout_inx = list(X[X.Classes==-1].index)\n(out_inx)\n\n#Drop the outliers\nX.drop(out_inx,axis=0,inplace=True)","8d73a21a":"# Merge X and Y\nx_y = pd.concat([X, y_tr],axis =1)\n\n# Remove Nans and Infinitis\nx_y = x_y[~x_y.isin([np.nan, np.inf, -np.inf]).any(1)]\n\n# Drop the classes column column\nx_y.drop(['Classes'],axis=1,inplace=True)\nx_y.shape","394ff5f6":"Y = x_y['SalePrice']\n\nX = x_y.drop(['SalePrice'],axis=1)\n\n# Train,Test Split\nfrom sklearn.preprocessing import StandardScaler\nX_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.1, random_state=0)","c3dc6e3f":"# This is the Y Test!!!\nsamp_sub = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv',index_col=0)\nsamp_sub.head(10)","4aaf8813":"# Ensure the real test set uses only the same variables from the X_test\nTest_Set_red = pd.DataFrame(Test_Set, columns=X_test.columns)\n\n#Check any Nans\nTest_Nans = Test_Set_red[Test_Set_red.isna().any(axis=1)]\nprint('Row with Nan Values:\\n\\n',Test_Nans)\n\n# We see the kitchenQual has a Nan,and as we did with the train X,we replace \"Nan\" with \"0\"\nTest_Set_red[\"KitchenQual\"] = Test_Set_red[\"KitchenQual\"].fillna(0)\n\n# This is the X Test!!!!\nTest_Set_red","5a3dd71f":"# Intersection Function needed \ndef intersection(lst1, lst2): \n    lst3 = [value for value in lst1 if value in lst2] \n    return lst3\n\n\n# The list of variables to unskew the first time with Powertransformer\nfix_1st = intersection(top_cov, numeric_after)\n\n# The list of variables to unskew the second time with MinMax and ExperimentalTransformer\nfix_2st = intersection(top_cov, skew_over_1)","045e2263":"# Lasso\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import Normalizer\n\n\n#Create the list of column transformers\nSkew_Transf = [('pow_trans', PowerTransformer(method='yeo-johnson'), fix_1st),\n                (\"MinMax\", MinMaxScaler(), fix_2st),\n                  ('custom_trans',ExperimentalTransformer_2(intersection(top_cov, skew_over_1)),fix_2st )]\n\n\n# Apply to the Column Transformer function\ncol_tran = ColumnTransformer(transformers=Skew_Transf,remainder='passthrough')\n\n# Set the pipeline function for the independent variables\npipeline = Pipeline(steps=[('prep',col_tran),('Lasso', Lasso())])\n\n# Initialize the trans reg to rescale the target \nmodel = TransformedTargetRegressor(regressor=pipeline,func=np.log1p,inverse_func=np.expm1) #, transformer = transformer\n\n# Tuning Parameters\nparameters={'regressor__Lasso__alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,10,100,300,500,700,1000,10000]}\n\ncv = KFold(n_splits=5, random_state=1, shuffle=True)\n\nLasso_reg_MSE=GridSearchCV(model,parameters, scoring=['neg_mean_squared_error'], cv=cv,refit='neg_mean_squared_error')","021924d5":"#Lasso_reg_MSE.get_params().keys()\nLasso_reg_MSE.fit(X_train, y_train)","5f4e96f9":"#yhat \nyhat = Lasso_reg_MSE.predict(Test_Set_red)\n#print(Lasso_reg_MSE.best_params_)\n\nprint('Test MSE',mean_squared_error(np.log1p(samp_sub), np.log1p(yhat)))\n\n# Lets output 15 values to compare\npred = pd.DataFrame((yhat),columns=['Predicted SalePrice'])\npred.reset_index(drop=True, inplace=True)\n\nobs = pd.DataFrame((samp_sub))\nobs.reset_index(drop=True, inplace=True)\n\nlasso_y = pd.concat([obs,pred],axis=1,join='inner')\nlasso_y","2154ee50":"#Create the list of column transformers\nSkew_Transf = [('pow_trans', PowerTransformer(method='yeo-johnson'), fix_1st),\n                (\"MinMax\", MinMaxScaler(), fix_2st),\n                  ('custom_trans',ExperimentalTransformer_2(intersection(top_cov, skew_over_1)),fix_2st)]\n\n\n# Apply to the Column Transformer function\ncol_tran = ColumnTransformer(transformers=Skew_Transf,remainder='passthrough')\n\n# Set the pipeline function for the independent variables\npipeline = Pipeline(steps=[('prep',col_tran),('Ridge', Ridge())])\n\n# Initialize the trans reg to rescale the target \nmodelrd = TransformedTargetRegressor(regressor=pipeline,func=np.log1p,inverse_func=np.expm1) \n\n# Ridge Parameters\nparameters={'regressor__Ridge__alpha':[1e-15,1e-10,1e-8,1e-3,1,5,10,20,30,40,50,70,100,200,400,700,1000,100000]}\n\n#Initialize the cross validation\ncv = KFold(n_splits=5, random_state=1, shuffle=True)\n\n# Gradserach for optimal parameters\nridge_reg_MSE=GridSearchCV(modelrd,parameters,scoring=['neg_mean_squared_error'],cv=cv,refit='neg_mean_squared_error')","5a2cb53d":"# fit the model\nridge_reg_MSE.fit(X_train,y_train)\n#ridge_reg_MSE.get_params().keys()","2e21b52c":"#predict the y's\nyhatrd= ridge_reg_MSE.predict(Test_Set_red)\n\nprint('Test MSE',mean_squared_error(np.log1p(samp_sub), np.log1p(yhatrd)))\n\n# Lets output 15 values to compare\npred_15 = pd.DataFrame((yhatrd)[0:15],columns=['Predicted SalePrice'])\npred_15.reset_index(drop=True, inplace=True)\n\nobs_15 = pd.DataFrame((samp_sub)[0:15])\nobs_15.reset_index(drop=True, inplace=True)\nrid_y = pd.concat([obs_15,pred_15],axis=1,join='inner')\nrid_y\n","b39b7d1f":"#Create the list of column transformers\nSkew_Transf = [('pow_trans', PowerTransformer(method='yeo-johnson'), fix_1st),\n                (\"MinMax\", MinMaxScaler(), fix_2st),\n                  ('custom_trans',ExperimentalTransformer_2(intersection(top_cov, skew_over_1)),fix_2st)]\n\n\n# Apply to the Column Transformer function\ncol_tran = ColumnTransformer(transformers=Skew_Transf,remainder='passthrough')\n\n# Set the pipeline function for the independent variables\npipeline = Pipeline(steps=[('prep',col_tran),('xgb', xgb.XGBRegressor())])\n\n# Initialize the trans reg to rescale the target \nmodelxgb = TransformedTargetRegressor(regressor=pipeline,func=np.log1p,inverse_func=np.expm1) \nparameters = {'regressor__xgb__learning__rate': [0.01,0.05, 0.07],\n              'regressor__xgb__max__depth': [2,3,4,5, 6, 7],\n              'regressor__xgb__n__estimators': [100,200,300,600,1200,2400]}\n\n#Initialize the cross validation\ncv = KFold(n_splits=5, random_state=1, shuffle=True)\n\n# Gradserach for optimal parameters\nXGB_Grid =GridSearchCV(modelxgb,parameters,scoring=['neg_mean_squared_error'],cv=cv,refit='neg_mean_squared_error')","2824cb48":"# modelxgb.get_params().keys()","53791658":"#modelxgb.get_params().keys()\nXGB_Grid.fit(X_train,y_train)","faf893c6":"pred_XG = XGB_Grid.predict(Test_Set_red)\n\nprint('Test MSE',mean_squared_error(np.log1p(samp_sub), np.log1p(pred_XG)))\n\n#inverse transform the Y values\n# Lets output 15 values to compare\npred_15 = pd.DataFrame((pred_XG)[0:15],columns=['Predicted SalePrice'])\npred_15.reset_index(drop=True, inplace=True)\n\nobs_15 = pd.DataFrame((samp_sub)[0:15])\nobs_15.reset_index(drop=True, inplace=True)\n\nXGB_pred = pd.concat([obs_15,pred_15],axis=1,join='inner')\nXGB_pred","fb6ef124":"# Linear model with Recurrsive Feature Elimination\n\n#Create the list of column transformers\nSkew_Transf = [('pow_trans', PowerTransformer(method='yeo-johnson'), fix_1st),\n                (\"MinMax\", MinMaxScaler(), fix_2st),\n                  ('custom_trans',ExperimentalTransformer_2(intersection(top_cov, skew_over_1)),fix_2st)]\n\n\n# Make the pipeline\npipeline = Pipeline([('pow_trans',PowerTransformer(method='yeo-johnson')),\n                     ('s', RFE(estimator = LinearRegression())),\n                      ('m',LinearRegression())])\n\n# Transform the SalePrice\nfmodel = TransformedTargetRegressor(regressor=pipeline,func=np.log1p,inverse_func=np.expm1) \n\nparameters={'regressor__s__n_features_to_select': [1,2,3,4,5,6,7,8,9,10,15,16,17,18,20,25,26,27,28,29,30,35,40]}\n\ncv = KFold(n_splits=5, random_state=1, shuffle=True)\n\nRFE_MS = GridSearchCV(fmodel,parameters,scoring=['neg_mean_squared_error'],cv=cv,refit='neg_mean_squared_error',verbose=1)","ceadfca6":"# RFE_MS.get_params().keys() \nRFE_MS.fit(X_train,y_train)","e6f57108":"y_predrfe = RFE_MS.predict(Test_Set_red)\n\nprint('Test MSE',mean_squared_error(np.log1p(samp_sub), np.log1p(y_predrfe)))\n\n# Lets output 15 values to compare\npred_15 = pd.DataFrame((y_predrfe),columns=['Predicted SalePrice'])\npred_15.reset_index(drop=True, inplace=True)\n\nobs_15 = pd.DataFrame((samp_sub))\nobs_15.reset_index(drop=True, inplace=True)\n\nRFE_res = pd.concat([obs_15,pred_15],axis=1,join='inner')\nRFE_res ","c761917e":"# Submission from Lasso\nresults_lasso = lasso_y.copy()\n\nresults_lasso = results_lasso.iloc[:,[1]]\n\nresults_lasso.rename(columns={\"Predicted SalePrice\": \"SalePrice\"},inplace=True)\nresults_lasso = results_lasso[[\"SalePrice\"]].round(2)\n\nresults_lasso['Id'] = results_lasso.index + 1461\n\nresults_lasso = results_lasso[[\"Id\", \"SalePrice\"]]\nresults_lasso.head(5)","31b0c1ab":"# results_lasso.to_csv(r\"C:\\Users\\Rae-Djamaal\\Anaconda3\\Lib\\Git_Uploads\\house-prices-advanced-regression-techniques\\submission.csv\",index = False)","b91e8ba4":"**Analysis of Results before Transformation**\n\n'MiscVal', 'PoolArea', 'LowQualFinSF', 'KitchenAbvGr', '3SnsPorch', 'BsmtHalfBath', 'BsmtFinSF2', 'EnclosedPorch', 'ScreenPorch' all have data that is clumped up together and thus these can be classified. Lets wait till we Transform them to see the results after**\n","c58f4392":"**Feature Selection- Get Rid of the Outliers**","4e503ade":"Now Apply a log Transformation to SalePrice and Vizualize the Reshaped Distribution","1829b504":"Split train and test set to avoid Data Leakage\n","1809b088":"Distribution Plot of Skewed features","725c0969":"**Feature engineering Ordinal Variables**\n*  Converting Catergory text to Numbers","7c8d8b4b":"## Concat both Dataframes Together to save time","15632ca6":"**Note Again Scaling is only for Illustration Purposes,final scaling wil be done in the modeling section**\n\nApply a **PowerTransformer** to the Skewed data\n\nPower transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like and thus **minimize skewness**.We will use the default distribution given more on powertransformer below\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.PowerTransformer.html","03c9ba6e":"Apply A Min Max Scaler to these variables that still have high skewness","58cd9b6f":"**Determine the \u201cknee\u201d, which corresponds to the optimal epsilon parameter.A knee corresponds to a threshold where a sharp change occurs along the k-distance curve. More simply,the Knee indicates where is a sizeable gap in euclidian distance from a few datapoints(the outliers) to the majority of the datapoints.**\n\nhttps:\/\/www.datanovia.com\/en\/lessons\/dbscan-density-based-clustering-essentials\/","167280d0":"**Feature Engineering-Creating a custom Transformer**\n\nNow Lets Create a Function to binarize the data.A value Less that 0.5 = 0 and greater than 0.5 = 1.\n\nAs you may have noticed from the values above,based on how the MinMaxScaler function Works,no values will be on the 'border' of 0 or 1.                                                \nFor example, there will be no values of 0.55 or 0.47. The closest I can see to the threshold of 0.5 from the printout above is around the 0.75 in the 'PoolArea' variable.\n\nFor more on custom Transformers: \nhttps:\/\/github.com\/HCGrit\/MachineLearning-iamJustAStudent\/blob\/master\/PipelineFoundation\/Pipeline_Experiment.ipynb","a9548ab6":"**One hot encode categorical variables**\n\nA one hot encoding is a representation of categorical variables as binary vectors. Many machine learning algorithms cannot work with categorical data directly,such as our Linear Regression. The categories must be converted into numbers. This is required for both input and output variables that are categorical.\n\nFor More on this: https:\/\/machinelearningmastery.com\/how-to-one-hot-encode-sequence-data-in-python\/","5d4d9f61":"Remerge the Train and Test Objects","e1b3c046":"Observe the new shape of the binary variables after 1hot encoding\nBefore the Old shape was (2919,25)","46f981ae":"**Analysis after Transformation**\n\nMiscVal', 'PoolArea', 'LowQualFinSF', 'KitchenAbvGr', '3SnsPorch', 'BsmtHalfBath', 'BsmtFinSF2', 'EnclosedPorch' all still have high skewness because they can be classified we will apply another Transformation to these.","26778744":"**Lasso Regression**","3c1697fe":"Submission","8c9c73e3":"**Recursive Feature Elimination-RFE**","2ee2efed":"**Around 27 features or so explain 95% of the variance based on the magnitudes of the eigenvalues as seen below,we will stick with these 25**","5204ca5d":"**Using the 'sample_submission' from Kaggle** to predict the new sales prices in the regression models to come,setting that up in this phase.","52611533":"**Skewness in Independent Variables**\n\n\n\n![Skewness](https:\/\/lh3.google.com\/u\/0\/d\/1XD_72DoPo_5srmi3U40S4bApoxRGLLmk=k)\n\n\n\n\nCheck Skewness in the Independent features.Skewness is a measure of lack of symmetry.Thus skewness reduces the linear relationship between the predictor variables and the repsonse variable.\n\nMore about when and when not to Transform independent variables in this blog:\nhttps:\/\/www.researchgate.net\/post\/Should_I_transform_non-normal_independent_variables_in_logistic_regression","5f354a02":"**Feature Engineering**\n\nReplace all remaining null values with the median for the numeric train and numeric test sets.\n\nMode and Mean are options as well depending on your data\n","cde683f1":"**Model Selection**\n\nNow I bring back up the topic of **Data Leakage**. This is the reason why I merged and separated the original Train and Test sets a few times. I wanted to do the 1 hot encoding,removing null values etc. for both datasets combined and these procedure will not cause bais\/overfit.The **DATA TRANSFORMATIONS** however across the combinded datasets would cause data leakage as it give information to the test set.\n\nIn order to save time I could just separate the data before but I did not do this because I wanted to do **Cross validation**.Doing Transformations before cross validation is an easy way to cause Data Leakage.It is definately more time consuming but because this is a project to learn and there is noo time crunch,I thought I would take extra time to try it this way and improve my modeling skills.\n\n**Some more notes on Data Leakage:**\n\n*'Here we see the intrinsic problem of applying a transformer and an estimator separately where the parameters for estimator (SVM) are determined using GridSearchCV . The scaled features used for cross-validation is separated into test and train fold but the test fold already contains the info about training set as the whole training set (X_train) was used for standardization. In a simpler note when SVC.fit() is done using cross-validation the features already include info from the test-fold as StandardScaler.fit() was done on the whole training set.'*\nhttps:\/\/towardsdatascience.com\/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976\n\nhttps:\/\/machinelearningmastery.com\/data-leakage-machine-learning\/","6a02f5d1":"**Exploratory Data Analysis**","6b6c65aa":"**Ridge Regression**","4f7e12b9":"# Giving credit where credit is due:Specifically for these 3 steps PCA,KNN with DBSCAN,for outlier detection> I refered to this great submission from **Tolga Kapla:** https:\/\/www.kaggle.com\/darkside92\/detailed-examination-for-house-price-top-10\/#data\n\n\n**Feature Selection-Outlier Detection**\n","df41b740":"Get a list of the high skewed features,those with skewness >1","7233dfbb":"**Correlation of independent variables with SalePricr. I arbitrarily choose the top 50 Correlated Features**","e9a0506e":"**XGBoost**","84683048":"**Feature Engineering-Data Transformations**\n\nHere I will **re-merge** the Train and Test **numeric dataframes** since final Data Transformations will take place in the modeling phase.Transformations Prior to modeling are for Illustration purpuses.Doing them in the modeling phase is to reduce **Data Leakage**. Some of the data transformations to come include: PowerTransformer, MinMaxScaler, StandardScaler, RobustScaler etc.","bfccb8db":"**Intersection of Variable Names for functions below, Recall from previously in the notebook:**\n* top_cov\n* numeric_after\n* skew_over_1","fe9769b5":"**Remove 1 of the highly correlated pairs that are also in in the 'top 50' correlation to 'SalePrice'.**\n\n![Correlated Variables](https:\/\/lh3.google.com\/u\/0\/d\/1Ub01FQAJ6Xtu13gnpqv07qZgv9_kdPSd=k)\n\n**For example: RoofStyle_1 is correlated with RoofStyle_2 so we remove\nRoofStyle_1 since it has a smaller correlation coefficient with the target variable SalePrice**","11f8f982":"**Lets check the Correlation of independent variables with each other.**\n","1764b08d":"#Now that I merged the Train and Test,here is a good place to talk about **Data Leakage**\n\nI will go about separating the Train Set From the Test Set sometimes when doing the feature engineering. Doing this would reduce any potential Data Leakage in our model.\n\n**Data leakage** is when information from outside the training dataset is used to create the model. This additional information can allow the model to learn or know something that it otherwise would not know and in turn invalidate the estimated performance of the mode being constructed.\n\nhttps:\/\/machinelearningmastery.com\/data-leakage-machine-learning\/","0df47c59":"Lets get the intersection of Train and test Dataframes since they may have some different columns","4bf7060f":"**KNN**","cc3552ba":"**DBScan**\n\nIt is a density-based clusering algorithm.The key idea is that for each point of a cluster, the neighborhood of a given radius has to contain at least a minimum number of points\n\nThe two main advantages of DBSCAN for our purposes is:\n* DBSCAN can identify outliers\n* DBSCAN can find any shape of clusters. The cluster doesn\u2019t have to be circular.\n\nhttps:\/\/www.datanovia.com\/en\/lessons\/dbscan-density-based-clustering-essentials\/#advantages","b7e02f74":"**Feature Engineering**\n*   Separate Object types from Numeric types\n\n\n","cf35de39":"Check the reduced Skewness","bc30cdbd":"Visualize the distribution of the target variable SalePrice\n\nWe see that the normality assumptions do not hold here saleprice is nor normally distributed and can assume all of the other assumptions fail as well\n","edf93212":"**Here is where we separate the Train and Test sets since we\nare doing correlation with y_values**","ff02ead0":"**From the KNN we see the sharp increase at around 4 units epsilon.This indicates where is a sizeable gap in euclidian distance from a few datapoints to the majority of datapoints,for more on DBScab:**\nDBSCAN \nhttps:\/\/ieeexplore.ieee.org\/document\/8342664","4374ae52":"**Feature engineering Ordinal Variables**\n*  Fill Nans","849fa13f":"**Data Vizualization**\n\n* Regression Plots of each skewed feature with the target variable,SalePrice, \nbefore applying Transformations.\n\n\n\n\n","aeaa52de":"Lets Use Value_counts() to check how the data Looks after applying MinMaxScaler","9f064065":"**Prinicipal Component Analysis(PCA)**\nPrincipal Component Analyis is a statistical procedure to convert a set of observation of possibly correlated variables into a set of values of linearly uncorrelated variables.\n\nData must be normalized with the since PCA is based off of the euclidian distance. I use robustscaler() normalizer since it is robust to outliers.","465c1fbb":"Train Test Split - **Note this X_test and y_test is just for model fitting!As explained in the next 2 blocks of code**\n\n*   actual X_test is the \"Test_Set\" from the beginning\n*   actual 'y_test' is the  'sample_submission' from Kaggle\n","9fead200":"**Feature Selection**\n\n* Vizualize a heatmap of the null values.Green indicates the amount of null values in the variable\n\n\n\n","10a2d59a":"**Finally Combine all relevant dataframes**\n* obj_1h_tr_ts - orginial NOMINAL variables that were 1hot encoded\n* obj_tr_tst_ordinal -original ORDINAL variables that we made numeric\n* num_tr_ts - original Train,Test combined after removing Null columns >50% and replacing all other nulls with the median","dec1199f":"Run the Custom Transformer","381446ec":"**Feature Selection**\n* Remove columns with Null Values > 50%**","dc298144":"**Data Vizualization**\n\nRegression Plots after transformation to both Dependent and Independen Variables"}}