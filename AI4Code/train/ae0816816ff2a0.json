{"cell_type":{"21552acc":"code","e5433a4b":"code","9bbd90ac":"code","45384352":"code","c12c73b2":"code","c38edfd4":"code","24123b9a":"code","6a0594a5":"code","ebae7a0f":"code","5fd811c4":"code","725330de":"code","8e0d5105":"code","cd276287":"code","6b47004e":"code","ba37a326":"code","fb12bf56":"code","65dcd1bd":"code","4e2a4d28":"code","535a28d2":"code","685d9308":"code","37d93b46":"code","93244e14":"code","5477327a":"code","14c21e88":"code","6153d2fe":"code","de0bed65":"code","ab29f6a9":"code","5e5d5c09":"markdown","60d97b28":"markdown","59ebec20":"markdown","ad58e949":"markdown","7c131e71":"markdown","2ed4269f":"markdown","5aff3e1d":"markdown","80e89240":"markdown","18ecc133":"markdown","34cfbbf3":"markdown","ef1979f9":"markdown","ee25605b":"markdown","19d54525":"markdown","8430b600":"markdown","d6785bce":"markdown","191cf42d":"markdown","e44e02da":"markdown"},"source":{"21552acc":"#Importing all necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import *\nfrom sklearn.metrics import *\nfrom sklearn.ensemble import *\nfrom sklearn.svm import *","e5433a4b":"#Loading dataset and reading the first few columns\ndf = pd.read_csv('..\/input\/diabetescsv\/diabetes.csv')\ndf.head()","9bbd90ac":"#  Check the number of rows and columns in the dataset\ndf.shape","45384352":"# Get basic statistics - gives statistics of only numerical columns in the dataset. \n#714 columns for age indicates missing values in age column\ndf.describe()","c12c73b2":"df.groupby('Glucose').size()","c38edfd4":"df.groupby('BloodPressure').size()","24123b9a":"df.groupby('BMI').size()","6a0594a5":"sns.set_style('darkgrid')\ncols = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age']\nn_rows = 2\nn_cols = 4\n\nfig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols*3.5, n_rows*3.5))\n\nfor r in range(0, n_rows):\n    for c in range(0,n_cols):\n        i = r*n_cols +c  \n        ax = axs[r][c] \n        \n        sns.distplot(df[cols[i]], ax = ax)\n        \n    plt.tight_layout()\n","ebae7a0f":"sns.set_style('darkgrid')\ncols = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age']\nn_rows = 2\nn_cols = 4\n\nfig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols*3.5, n_rows*3.5))\n\nfor r in range(0, n_rows):\n    for c in range(0,n_cols):\n        i = r*n_cols +c  \n        ax = axs[r][c] \n        \n        sns.boxplot(df[cols[i]], ax = ax)\n        \n    plt.tight_layout()","5fd811c4":"sns.pairplot(df, height = 3, hue = 'Outcome', diag_kind = 'kde')\nplt.figure(figsize=(10,10))\nplt.show()","725330de":"\ndf_correlation = df.corr()\ndf_correlation","8e0d5105":"fig, ax = plt.subplots(figsize = (16,6))\nsns.heatmap(df_correlation, annot = True, annot_kws= {'size': 12})","cd276287":"filt1=(df['Outcome']==0)&(df['Glucose']==0)\nfilt2=(df['Outcome']==1)&(df['Glucose']==0)\ndf.loc[filt1,'Glucose']=110\ndf.loc[filt2,'Glucose']=141\n\nfilt1=(df['Outcome']==0)&(df['BloodPressure']==0)\nfilt2=(df['Outcome']==1)&(df['BloodPressure']==0)\ndf.loc[filt1,'BloodPressure']=68\ndf.loc[filt2,'BloodPressure']=71\n\nfilt1=(df['Outcome']==0)&(df['SkinThickness']==0)\nfilt2=(df['Outcome']==1)&(df['SkinThickness']==0)\ndf.loc[filt1,'SkinThickness']=20\ndf.loc[filt2,'SkinThickness']= 22\n\nfilt1=(df['Outcome']==0)&(df['Insulin']==0)\nfilt2=(df['Outcome']==1)&(df['Insulin']==0)\ndf.loc[filt1,'Insulin']=69\ndf.loc[filt2,'Insulin']=100\n\nfilt1=(df['Outcome']==0)&(df['BMI']==0)\nfilt2=(df['Outcome']==1)&(df['BMI']==0)\ndf.loc[filt1,'BMI']=30\ndf.loc[filt2,'BMI']=37\ndf.head()","6b47004e":"X = df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']]\ny = df['Outcome']\nprint(\"Shape of X :\", X.shape)\nprint(\"Shape of y :\",y.shape)","ba37a326":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)","fb12bf56":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","65dcd1bd":"def models(X_train, y_train):\n    \n    #1st we will use Logistic regression\n    from sklearn.linear_model import LogisticRegression\n    log = LogisticRegression(random_state=0)\n    log.fit(X_train, y_train)\n\n    from sklearn.linear_model import RidgeClassifier\n    clf = RidgeClassifier()\n    clf.fit(X_train, y_train)\n    \n    # Using KNeighbors \n    from sklearn.neighbors import KNeighborsClassifier\n    knn = KNeighborsClassifier(n_neighbors =5, metric = 'minkowski', p=2)\n    knn.fit(X_train,y_train)\n    \n    # Using SVM (linear kernel)\n    from sklearn.svm import SVC\n    svc_lin = SVC(kernel = 'linear', random_state = 0)\n    svc_lin.fit(X_train,y_train)\n    \n    # Using SVM (RBF kernel)\n    from sklearn.svm import SVC\n    svc_rbf = SVC(kernel = 'rbf', random_state = 0)\n    svc_rbf.fit(X_train,y_train)\n    \n    #Use GaussianNB\n    from sklearn.naive_bayes import GaussianNB\n    gauss = GaussianNB()\n    gauss.fit(X_train,y_train)\n    \n    #Using Decision tree\n    from sklearn.tree import DecisionTreeClassifier\n    tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n    tree.fit(X_train, y_train)\n    \n    #Using Random Forest Classifier\n    from sklearn.ensemble import RandomForestClassifier\n    forest = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state =0)\n    forest.fit(X_train, y_train)\n\n    import xgboost as xgb\n    xb = xgb.XGBClassifier(random_state=0)\n    xb.fit(X_train,y_train)\n\n    print('[0]Logistic Regression Training Accuracy', log.score(X_train, y_train))\n    print('[1]K Nearest Neighbors Regression Training Accuracy', knn.score(X_train, y_train))\n    print('[2]SVC Linear Regression Training Accuracy', svc_lin.score(X_train, y_train))\n    print('[3]SVC RBF Regression Training Accuracy', svc_rbf.score(X_train, y_train))\n    print('[4]Gaussian Regression Training Accuracy', gauss.score(X_train, y_train))\n    print('[5]Decision Tree Regression Training Accuracy', tree.score(X_train, y_train))\n    print('[6]Random Forest Regression Training Accuracy', forest.score(X_train, y_train))\n    print('[7]Ridge Classifier Training Accuracy', clf.score(X_train, y_train))  \n    print('[8]XGB Classifier Training Accuracy', xb.score(X_train, y_train)) \n      \n    return log, knn, svc_lin, svc_rbf, gauss, tree, forest, clf,xb","4e2a4d28":"models = models(X_train, y_train)","535a28d2":"from sklearn.metrics import confusion_matrix\n\nfor i in range(len(models)):\n    cm = confusion_matrix(y_test, models[i].predict(X_test))\n    \n    #Extracting TN, FN, TP, FP\n    TN, FN, TP, FP = confusion_matrix(y_test, models[i].predict(X_test)).ravel()\n    test_score = (TP + TN)\/(TP + TN + FN + FP)\n    print(cm)\n    print('Model[{}] Testing Accuracy = \"{}\"'.format(i, test_score))\n    print()","685d9308":"from sklearn.linear_model import LogisticRegression \nlog = LogisticRegression(random_state=0)\nfrom sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nfrom sklearn.linear_model import RidgeClassifier\nrc = RidgeClassifier()\nfrom sklearn.ensemble import VotingClassifier\nvt = VotingClassifier(estimators = [('log',log),(\"tree\",tree),('rc',rc)],voting=\"hard\", flatten_transform=True)\nvt.fit(X_train,y_train)\nvt.score(X_test, y_test)","37d93b46":"from sklearn.ensemble import GradientBoostingClassifier\ngb_clf = GradientBoostingClassifier(random_state =0)\ngb_clf.fit(X_train,y_train)\ngb_score = gb_clf.score(X_test,y_test)\ngb_score","93244e14":"import xgboost as xgb\nxgb_model = xgb.XGBClassifier(random_state=0)\nxgb_model.fit(X_train,y_train)","5477327a":"from sklearn.metrics import accuracy_score\ny_pred = xgb_model.predict(X_test)\naccuracy_score(y_test,y_pred)","14c21e88":"!pip install catboost\nfrom catboost import CatBoostClassifier, Pool\ncat_model=CatBoostClassifier()\ncat_model.fit(X_train, y_train)\ny_pred=model.predict(X_test)\nscore=accuracy_score(y_pred,y_test)\nprint(\"Test score is \",score)","6153d2fe":"pred = cat_model.predict(X_test)","de0bed65":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,pred))","ab29f6a9":"from sklearn.metrics import roc_curve,auc\nfpr, tpr, thresholds = roc_curve(y_test,pred)\nauc_vt = auc(fpr, tpr)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='CatBoost Classifier (auc = %0.3f)'% auc_vt )\nplt.xlabel('True Positive Rate')\nplt.xlabel('False Positive')\nplt.title('CatBoost ROC curve')\nplt.legend()\nplt.show()","5e5d5c09":"Standardizing the data by using StandardScalar package","60d97b28":"The voting classifier did improve the test accuracy significantly. However, we can further imporve this. I am using Gradient Boosting Classifier to check if we can imporve on the results further","59ebec20":"Spliting the dataset into test and train ","ad58e949":"Gradient Boosting Classifier further imporves the test data accuracy. For additional imporvements, I am also checking for XGB CLassifier, which again enhances our test results.","7c131e71":"The above pairplots shows how the outcomes have varied based on different features. To get the exact correlation or dependency on a partifular feature, I will check for correlation below. The below correlation matrix shows Glucose, BMI and Pregnency to be highly correlated with the outcome.I will also visualize the same using a heatmap.","2ed4269f":"Now I am going to use confusion matrix to check testing accuracy of all the above models","5aff3e1d":"From all the analysis, processing and model selection, it is clear that CatBoost model performs the best on the given dataset\n","80e89240":"Now, separating outcome which is the dependent variable and rest of the features which are independent","18ecc133":"The above dataframe describes all key information in the dataset. I have made a few google searches to gain some domain knowledge. This is mainly to identify any outlier or inconsistent values in the dataset. As per the secondary research, Skin Thickness COlumn has outliers since max values are seen as 99 while average values among Indian women ranges from 18-22 for tricep area.\n\nBelow are the counts of 0 values for Glucose, BP and BMI. As seen, Glucose and BMI has 5 and 11 0 values resp. and apart from these values, the distributions in the graphs are gaussian. We can later correct these 0 values to have normal distributions for both. Blood Pressure has 35 0 values and I will not adjust these 0 values. italicized text\n","34cfbbf3":"Checking the distributions of all column data to identify the distribution type and outliers if any.\n\nThe graphs for Glucose, Blood Pressure and BMI shows 0 values. Most of the distributions do not follow Gussian distribution and shows signs of positive skewness and kurtosis\n\nNow I will further check for outlier distributions and quartile quartile distributions. I already have an idea of the outliers as mentioned above \n","ef1979f9":"This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on. italicized text italicized text\n\n# Get basic statistics - gives statistics of only numerical col","ee25605b":"The correlation heatmap shows 4 main correlated features to outcome, Age, BMI, Glucose, Pregnancies. However, Pregnancy and Age are highly correlated and will result in co-linearity. Since no particular feature shows high correlation, I will not perform feature selection, and will continue with the existing features to run prediction models","19d54525":"As clearly seen above, no particular model is performing well on test data. All are weak learners. I will further use voting classifier to combine weak learners to give better results","8430b600":"The box plots for Glucose and BMI clearcly shows the distributions of majority data lying between 100-140 for Glucose and 25-35 for BMI. BMI also has outliers in the right quartile, and their numbers are high. I will treat the Glucose and BMI columns for 0 values since there are few.\n\nBefore I do that, lets also check the outcome across features using pairplot\n","d6785bce":"Printing the performance report for XGB model ","191cf42d":"In the below cell, I am defining a function to train any model we need to check\n","e44e02da":"Before we do that, important data processing is needed. I am replacing 0 values in Glucose, BMI, Blood Pressure, Insulin and SKinTHickness with its mean values. However, mean values are added in below code separately for people with Diabetes and people without since these features are important contributors to diabetes."}}