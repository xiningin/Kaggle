{"cell_type":{"4662c5a9":"code","8f6af697":"code","012cf341":"code","26406eb7":"code","efbc6a00":"code","6461bd7e":"code","b4d54e19":"code","1c31d487":"code","8c139a5d":"code","96770aef":"code","76ecdd5f":"code","b65a5207":"code","0b93e2f4":"code","3bffb764":"code","c689d89f":"code","c470d1d4":"code","aa05227b":"code","faad04a5":"code","511a9cad":"code","c2408dd8":"code","241819b8":"code","d4d9ef1c":"code","f184fccf":"code","55d6d2cb":"code","e8b34f8e":"code","f7f0f0a0":"code","ec41f74d":"code","33631f7b":"code","b3ba4078":"code","b4b40838":"code","9b1a9ab0":"code","5d50e479":"code","b9f12509":"code","840ee08e":"code","1572d24f":"code","cf135a2c":"code","2e074350":"code","bd0e578e":"code","b16213d8":"code","36bad095":"code","459f6fd6":"code","0bb08785":"code","fe32fffc":"code","c9905234":"code","566b96a7":"code","53eec4db":"code","a3702f38":"code","6362c1f8":"code","7e404ee8":"code","a8559726":"code","e8668788":"code","5f44477f":"code","2504dd88":"code","863e4667":"code","bfcf8979":"code","5dffe95b":"code","1642c898":"code","3fbb604b":"markdown","7a094346":"markdown","9a829786":"markdown","f25e84a0":"markdown","87fa1375":"markdown","a40f06e8":"markdown","7ff2a729":"markdown","82260bac":"markdown","bba9debf":"markdown","59347946":"markdown","fe66ae1c":"markdown","fe365ae5":"markdown","6ef70cea":"markdown","35476ce8":"markdown","c4bf8800":"markdown","9f04528e":"markdown","55fe1e66":"markdown","87647488":"markdown","656b588c":"markdown","dce13724":"markdown","1533893c":"markdown","5e45987e":"markdown","184e09bc":"markdown","8b0b786e":"markdown","96b0e01f":"markdown","b9b98100":"markdown","21ed0d29":"markdown","0e48a8ee":"markdown","75f5f061":"markdown","cd47b104":"markdown","f2196672":"markdown","fe9a6dbd":"markdown","ce1e61e3":"markdown","ab02ffdf":"markdown","e1ed3389":"markdown","efbf9e19":"markdown"},"source":{"4662c5a9":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","8f6af697":"#load data\nportfolio= pd.read_csv('..\/input\/starbucks-customer-data\/portfolio.csv')\nprofile= pd.read_csv('..\/input\/starbucks-customer-data\/profile.csv')\ntranscript= pd.read_csv('..\/input\/starbucks-customer-data\/transcript.csv')","012cf341":"portfolio.describe()","26406eb7":"profile.describe()","efbc6a00":"transcript.describe()","6461bd7e":"\nportfolio.isnull().sum()","b4d54e19":"profile.isnull().sum()","1c31d487":"transcript.isnull().sum()","8c139a5d":"#in gender column group NaNs with other, since we don't know thier gender\nprofile['gender'].fillna(\"O\", inplace=True)","96770aef":"#from document 118 == null. impute with the average for each group in gender\n#find wich groups has null (118) in thier age\nprofile[profile['age']==118]['gender'].value_counts()","76ecdd5f":"#find average age for 'other' group\naverage_age=np.round(profile.groupby('gender').mean().age.loc['O'])","b65a5207":"#since all missing ages from the other group, use the average for the group\nprofile['age']= profile['age'].apply(lambda x : average_age if x == 118 else x)","0b93e2f4":"#find wich groups has null in thier income\nprofile[profile['income'].isnull()].groupby('gender').count()['income']","3bffb764":"#average income for 'other'\naverage_income= np.round(profile.groupby('gender').mean().income.loc['O'])","c689d89f":"#imput nan with average\nprofile['income']= profile['income'].apply(lambda x : average_income if np.isnan(x) else x)","c470d1d4":"#drop unnamed:0 column from all dataframes\nprofile.drop('Unnamed: 0', axis=1, inplace=True)\ntranscript.drop('Unnamed: 0', axis=1, inplace=True)\nportfolio.drop('Unnamed: 0', axis=1, inplace=True)","aa05227b":"#analyse profile file first\n#see gender count\nsns.countplot(profile['gender'])","faad04a5":"sns.displot(profile, x='income', hue='gender', bins=30)\nplt.title('Income per gender')","511a9cad":"profile.groupby('gender').sum()['income'].plot(kind='bar')\nplt.title('Total income per gender')\nplt.ylabel('income')","c2408dd8":"#extract only the year from 'became_member_on' and create column 'year'\nprofile['year']= profile['became_member_on'].apply(lambda x : str( x )[:4])","241819b8":"#see membership regstration throug the years\nprofile['year'].value_counts().sort_index().plot()\nplt.xlabel('year')\nplt.ylabel('new members')\nplt.title('loyalty program regestration per year')","d4d9ef1c":"\n#transcript['person'] == profile['id']\n#change column name from person to id, to merge them\ntranscript.rename(columns={'person':'id'}, inplace=True)\n","f184fccf":"#merge profile and transcript on 'id' column\ndf= profile.merge(transcript, on='id', how='outer')","55d6d2cb":"#map the offer_id from df with id in portfolio\n\n#create dict such that offers['offer_id']= offerName_difficulty_duration\noffers=dict()\nfor offer, i, diff, dur in portfolio[['offer_type', 'id', 'difficulty', 'duration']].values:\n    offers[i]= f'{offer}_{diff}_{dur}'\n\n\ndef value_col(col):\n    \"\"\"\n    extract offer_id from value column and mapped it to the offer name\n    \n    input- value column\n    \n    output-  mapped offer names\n    \"\"\"\n    value_type= col.split(':')[0].replace(\"'\", \"\").replace('{', \"\")\n    \n    if value_type == 'offer id':\n        value= col.split(':')[1].replace(\"'\", \"\").replace('}', \"\").strip()\n        return offers[value]\n    elif value_type == 'offer_id':\n        value= col.split(':')[1].split(',')[0].replace(\"'\", \"\").strip()\n        return offers[value]\n    else:\n        return 'None'","e8b34f8e":"#get offer names\ndf['offer']= df['value'].apply(value_col)","f7f0f0a0":"def value_col_id(col):\n    \"\"\"\n    extract offer_id from value column\n    \n    input- value column\n    \n    output- offer_ids\n    \"\"\"\n    value_type= col.split(':')[0].replace(\"'\", \"\").replace('{', \"\")\n    \n    if value_type == 'offer id':\n        value= col.split(':')[1].replace(\"'\", \"\").replace('}', \"\").strip()\n        return value\n    elif value_type == 'offer_id':\n        value= col.split(':')[1].split(',')[0].replace(\"'\", \"\").strip()\n        return value\n    else:\n        return 'None'","ec41f74d":"#get offer ids\ndf['offer_id']= df['value'].apply(value_col_id)","33631f7b":"def value_col_trans(col):\n    \"\"\"\n    get transaction amount from value column\n    \n    input- value column\n    \n    output- transaction amount\n    \"\"\"\n    value_type= col.split(':')[0].replace(\"'\", \"\").replace('{', \"\")\n    \n    if value_type == 'amount':\n        value= col.split(':')[1].replace(\"'\", \"\").replace('}', \"\").strip()\n        return np.round(float(value), 2)\n    else:\n        return np.nan","b3ba4078":"#get transaction amount\ndf['amount']= df['value'].apply(value_col_trans)","b4b40838":"df.groupby('year').sum().amount.plot()\nplt.ylabel(\"amount\")\nplt.title('total transactions per year')","9b1a9ab0":"sns.barplot(x= ['F','M','O'], y=df.groupby('gender').sum().amount.values)\nplt.xlabel('gender')\nplt.ylabel('total')\nplt.title('total amount spent per gender')","5d50e479":"sns.lineplot(x=df.groupby(['gender','year']).sum().amount.loc['O'].index, y=df.groupby(['gender','year']).sum().amount.loc['O'].values , label='Other')\nsns.lineplot(x=df.groupby(['gender','year']).sum().amount.loc['M'].index, y=df.groupby(['gender','year']).sum().amount.loc['M'].values , label='Male')\nsns.lineplot(x=df.groupby(['gender','year']).sum().amount.loc['F'].index, y=df.groupby(['gender','year']).sum().amount.loc['F'].values , label='Female')\nplt.ylabel('total transactions')\nplt.title('Total transactions per year per gender')","b9f12509":"# get dummies for event column\ndf=pd.concat([df, pd.get_dummies(df['event'])], axis=1)","840ee08e":"def infor(offer, viewed, complete):\n    \"\"\"\n    if a informational offer is viewd then counted as completed\n    \n    input:\n    offer column\n    offer viewed column\n    offer completed column\n    \n    output:\n    0- not completed\n    1- completed\n    \"\"\"\n    if offer == 'informational_0_3' or offer == 'informational_0_4':\n        if viewed == 1:\n            return 1\n    \n    return complete","1572d24f":"df['offer completed']=df.apply( lambda x : infor( x['offer'], x['offer viewed'], x['offer completed'] ) , axis=1)","cf135a2c":"#store tatal spending per person\ntotal_amount=df.groupby('id').sum().amount","2e074350":"temp= df.sort_values('event')#sort values\nnew_df= temp[temp['offer completed']== 1]# extract completed offers to new_df\ntemp= temp[temp['offer completed'] != 1]# delete completed offers from temp\ntemp= temp[temp.event != 'transaction']# delete transactions from temp\ntemp.drop_duplicates(subset=['id', 'offer_id'], inplace=True)# drop duplicated rows\nnew_df= pd.concat([new_df, temp], ignore_index=True) # concat temp and new_df\nnew_df= pd.concat([new_df, pd.get_dummies(new_df['offer'], drop_first=True)], axis=1) #concat new_df and dummyies for offer","bd0e578e":"#map total transaction amount with person id\nnew_df['total_transaction']= new_df['id'].apply(lambda x : total_amount.loc[x])","b16213d8":"#drop uneeded columns\nnew_df.drop(['became_member_on', 'event', 'time', 'amount', 'offer_id', 'id', 'value', 'offer received', 'offer viewed', 'transaction', 'offer'], axis=1, inplace=True)","36bad095":"# make dummiys for gender and year and concat with new_df\nnew_df = pd.concat([new_df, pd.get_dummies(new_df['gender']), pd.get_dummies(new_df['year'], drop_first=True)], axis=1)","459f6fd6":"#drop year and gender\nnew_df.drop(['gender', 'year', 'O'], axis=1, inplace=True)","0bb08785":"#rename target column to completed\nnew_df.rename(columns= {'offer completed': 'completed'}, inplace=True)","fe32fffc":"#get classes dist\nnew_df['completed'].value_counts()","c9905234":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import MinMaxScaler","566b96a7":"#create train test data\nx=new_df.drop('completed', axis=1)\ny= new_df['completed']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)","53eec4db":"#normalaize data then use knn\npipeline= Pipeline([\n    ('scaler', MinMaxScaler()),\n    ('cls',  KNeighborsClassifier())\n])\n","a3702f38":"#adjust how many nighbors affect the selection\nparams={'cls__n_neighbors': [1, 5, 10, 15, 20, 25]}\n\ncv= GridSearchCV(pipeline, params, verbose=3)","6362c1f8":"cv.fit(x_train, y_train)#train","7e404ee8":"pred = cv.predict(x_test)#predict","a8559726":"#evaluate\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(\"classification report:\")\nprint(classification_report(y_test, pred))\nprint(\"-------------------------------------\")\nprint('confusion matrix:')\nprint(confusion_matrix(y_test, pred))","e8668788":"new_df.completed.value_counts()#data unbalanced","5f44477f":"#use downsampling\nfrom sklearn.utils import resample\ndf_majority = new_df[new_df.completed==0]\ndf_minority = new_df[new_df.completed==1]\n \n# Downsample majority class\ndf_majority_downsampled = resample(df_majority, \n                                 replace=False,    # sample without replacement\n                                 n_samples=len(df_minority),     # to match minority class\n                                 random_state=123) # reproducible results\n \n# Combine minority class with downsampled majority class\ndf_downsampled = pd.concat([df_majority_downsampled, df_minority])\n \n# Display new class counts\ndf_downsampled.completed.value_counts()","2504dd88":"#crete train test data using balansed data\nx=df_downsampled.drop('completed', axis=1)\ny= df_downsampled['completed']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)","863e4667":"cv.fit(x_train, y_train)#train","bfcf8979":"pred = cv.predict(x_test)","5dffe95b":"print(\"classification report:\")\nprint(classification_report(y_test, pred))\nprint(\"-------------------------------------\")\nprint('confusion matrix:')\nprint(confusion_matrix(y_test, pred))","1642c898":"import pickle\nf= open('model.pkl', 'wb')\npickle.dump(cv, f)\nf.close()\n\nnew_df.to_csv('df.csv')\ndf_downsampled.to_csv('downsampled.csv')","3fbb604b":"## Reflection\nthe current solution is limited in the data it captures a better solutiom would be where the behavour of customers after viewing an offer is captured as well. using deep learning could also capture more data than is possable with this soultion.  \n","7a094346":"finally make dummies for gender and yearm then drop all unneeded coluns and with this the data is ready for the ML model.","9a829786":"using the 'value' column (df) create 'offer' column (df) which contains offer name in the following format: offerName_difficulty_duration by cross refrencing with portfolio's 'id' column.  ","f25e84a0":"## Conclusion\nwe reached the goal of predicting if a customer will responce to an offer or not, the solution leaves a lot to be desired but it does the job. i would loved to explore how users spending behavior changes after reading an offer, it is quite diffuclite to do at my current level.  \nAlso deep learning or better feature engineering could be used to achive better results.  \n\nFrom a business point it seems like males customers are spending less even though they are the majority some steps are needed to encourge them to spend more.","87fa1375":"both total transactions and # of regestration follow the same trend, climping up till 2017 then dropping.","a40f06e8":"## save the model & data","7ff2a729":"looks like males are more than female customers. lets see which one of them has more income","82260bac":"add 'total_transaction' column which holds total spending for each customer.","bba9debf":"## Model Evaluation and Validation pt.2  \n\nan increase in the precision score for the '1' label can observed as well as an increase in accuracy.","59347946":"around 2015 females started spending more at starbucks than males, then a sharp drop for 2018.","fe66ae1c":"## Refinement","fe365ae5":"Drop Unnamed: 0 since its just repeats the index.","6ef70cea":"## Justification\nBy simply balancing the data we gaind an increas in precision for '1' class from 44% to 56% and a jump in accuracy from 55% to 58%. this could indecate that the model was biased toward the '0' class before balancing the data.","35476ce8":"From [Project Overview](#Project-Overview) we can see that the 118 in hte age column is equal too NaN we will have to deal with it later. seems like the average age for the program member is 62 which is old.","c4bf8800":"## Data Exploration","9f04528e":"keep total transactions made per customer stored. then delete duplacte rows such that only the final result of each (customer, offer) interiaction is present example:  \nif we have the following data:  \n\ncus_id, offer_id, event      , completed  \n1     , qq1     , recived    , 0  \n1     , qq1     , viewed     , 0  \n1     , qq1     , completed  , 1  \n2     , qq1     , recived    , 0  \n2     , qq1     , viewed     , 0  \n\nthe output should be:  \n1     , qq1     , completed  , 1  \n2     , qq1     , viewed     , 0  \n","55fe1e66":"Check for Nans to deal with.","87647488":"## Improvement\nInstead of predicting if a customer will responce to an offer or not, a prediction could be made about what offers this user will responce too.","656b588c":"## Data Preprocessing","dce13724":"## Model Evaluation and Validation pt.1  \n\nLooks like the model can predict '0' class better than the '1' class this could be because the data is unbalanced.","1533893c":"informational is a special kind of offer that does not need a spending condition to complete but rather it should affect the customer's spending behavior. for simplicty i assume that if a customer reads the offer then it is completed.","5e45987e":"males total income is higher even though the number of females in the higher income (78k and above) are higher than males.","184e09bc":"Now let's deal with the categorical data in 'event' by making dummies.","8b0b786e":"remember that all the missing income were from the 'other' group, because I made every person with no gender as 'other'","96b0e01f":"add an 'amount' column (df) which hosts the amount in trasaction event.","b9b98100":"Since data is unbalanced lets try undersampling which shrink the dominant class and see if there is an improvment.","21ed0d29":"## Implementation  \n\nthe ML pipeline consist of normilazing the data by using minmax then feeding it to a KNN algorithm.  \nIf 2 similar customers where one responcded to an offer then it is likely that the other customer will respond to it as weill and that's why KNN eas used.","0e48a8ee":"Merge profile with transcript but before that the 'id' column does not exist in transcript but data in 'person' column (transcript) data matches the 'id' column (profile) so change column name 'person' to 'id'.  ","75f5f061":"# Analysis","cd47b104":"Impute NaN values.","f2196672":"# Index  \n[Project Overview](#Project-Overview)  \n[Problem Statement](#Problem-Statement)  \n[Metrics](#Metrics)  \n[Data Exploration](#Data-Exploration)  \n[Data Visualization](#Data-Visualization)  \n[Data Preprocessing](#Data-Preprocessing)  \n[Implementation](#Implementation)  \n[Model Evaluation and Validation pt.1](#Model-Evaluation-and-Validation-pt.1)  \n[Refinement](#Refinement)  \n[Model Evaluation and Validation pt.2](#Model-Evaluation-and-Validation-pt.2)  \n[Justification](#Justification)  \n[Reflection](#Reflection)  \n[Improvement](#Improvement)  \n[Conclusion](#Conclusion)\n","fe9a6dbd":"Check the statistcal information for each file.","ce1e61e3":"# Project Definition\n\n## Project Overview  \nThis project tackles the problem of sending offers to the right customers to increase revenue.  \nData is simulated data from [StarBucks](https:\/\/starbucks.com).  \n\nData Dictionary:  \n\nprofile.csv\n\nRewards program users (17000 users x 5 fields)\n\n    gender: (categorical) M, F, O, or null\n    age: (numeric) missing value encoded as 118\n    id: (string\/hash)\n    became_member_on: (date) format YYYYMMDD\n    income: (numeric)\n\nportfolio.csv\n\nOffers sent during 30-day test period (10 offers x 6 fields)\n\n    reward: (numeric) money awarded for the amount spent\n    channels: (list) web, email, mobile, social\n    difficulty: (numeric) money required to be spent to receive reward\n    duration: (numeric) time for offer to be open, in days\n    offer_type: (string) bogo, discount, informational\n    id: (string\/hash)\n\ntranscript.csv\n\nEvent log (306648 events x 4 fields)\n\n    person: (string\/hash)\n    event: (string) offer received, offer viewed, transaction, offer completed\n    value: (dictionary) different values depending on event type\n        offer id: (string\/hash) not associated with any \"transaction\"\n        amount: (numeric) money spent in \"transaction\"\n        reward: (numeric) money gained from \"offer completed\"\n    time: (numeric) hours after start of test\n\n\n## Problem Statement  \nThe problem is how do we know if an offer is sutable for a customer or not? to keep it simple we will use the 'offer copleted' event as an indicator that a customer is happy with the offer they got, and for offers that don't have a completion condition such as informitive offers the 'offer viewd' will indicate that a customer is happy (even though spending changes after reciving the informitive offer are better indecator of customer's responce).  \n\n## Metrics  \nSince it is a classification problem both precision and accuracy will be used to judge the model performance.\n\n\n\n\n[Github repo](https:\/\/github.com\/FancyWhale69\/Predict_customer_responce_SB) for the web app \n\n","ab02ffdf":"after that create the 'offer_id' (df) which contains the id of the offer by cross refrencing with portfolio's 'id' column. ","e1ed3389":"## Data visualization ","efbf9e19":"2017 was the highest year then it went down the next year."}}