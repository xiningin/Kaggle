{"cell_type":{"d6e3599b":"code","5c1c7303":"code","c83171d0":"code","1a05aa35":"code","35138e15":"code","33116cc8":"code","8e3922e9":"code","2718eb58":"code","e71b3690":"code","19837078":"code","5c4eeac1":"code","5842bd1f":"code","5a119130":"code","23593434":"code","d451165d":"code","7b21ba4a":"code","e2378c10":"code","a1403362":"code","d04710a4":"code","3f76e02f":"code","6b1e0d51":"code","b87446c2":"code","9f6c860d":"code","6600f1fb":"code","1678f404":"code","bb2a452a":"code","77a6e36f":"code","e6cda55a":"code","7cd39734":"code","b9309646":"code","37c0fb82":"code","220e3509":"code","65cde512":"code","634ec988":"code","f5f34602":"code","271eb0f0":"code","84db6c24":"code","08459c9b":"code","522c38ee":"code","bd61e11e":"code","6b0e714c":"code","1fc89dac":"code","9e0c7e52":"code","d0398295":"markdown"},"source":{"d6e3599b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5c1c7303":"pip install ta","c83171d0":"#Imports libraries \nimport numpy as np\nimport pandas as pd\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom matplotlib import pyplot\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,mean_absolute_percentage_error\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler\nimport statsmodels.api as sm\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import ParameterGrid\nimport warnings\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance,plot_tree\n\n#TA-lib is not include in Kaggle Python Interpreter \n#Please firstly install the library\n#Write the console what is in below the page \"pip install --upgrade ta\" command\nfrom ta import add_all_ta_features\nfrom ta.utils import dropna","1a05aa35":"FROTO,SP500,TUPRS,XBANK,YKBNK=1,2,3,4,5\nDATA=1","35138e15":"#Loading Data\nif DATA==1:\n    stock=pd.read_csv('\/kaggle\/input\/thesis-data\/FROTO.IS.csv',header=0,index_col=\"Date\")\nelif DATA==2:\n    stock=pd.read_csv('\/kaggle\/input\/thesis-data\/SP500.csv',header=0,index_col=\"Date\")[2000:-500]\nelif DATA==3:\n    stock=pd.read_csv('\/kaggle\/input\/thesis-data\/TUPRS.IS.csv',header=0,index_col=\"Date\")[2000:]\nelif DATA==4:\n    stock=pd.read_csv('\/kaggle\/input\/thesis-data\/XBANK.IS.csv',header=0,index_col=\"Date\")[:-200]\nelif DATA==5:\n    stock=pd.read_csv('\/kaggle\/input\/thesis-data\/YKBNK.IS.csv',header=0,index_col=\"Date\")[1500:-300]\n    \ndt = dropna(stock[[\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]])\ndt = add_all_ta_features(dt, open=\"Open\", high=\"High\", low=\"Low\", close=\"Close\", volume=\"Volume\")[100:].dropna(axis=1)\n ","33116cc8":"#scaler=RobustScaler()\ndf=dt[dt.columns]\n#df[df.columns] = scaler.fit_transform(df[df.columns])\nseries=df.Close","8e3922e9":"result = adfuller(np.log10(series))\nprint('ADF Statistic: {}'.format(result[0]))\nprint('p-value: {}'.format(result[1]))\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t{}: {}'.format(key, value))","2718eb58":"#Results of 2 lagged moving average\nif DATA==1:\n    m=2\nelif DATA==2:\n    m=2\nelif DATA==3:\n    m=9\nelif DATA==4:\n    m=7\nelif DATA==5:\n    m=6","e71b3690":"def difference(dataset, interval=1):\n    if interval!=0:\n        diff = np.zeros(shape=len((dataset)\/interval))\n        for i in range(interval, len(dataset)):\n            diff[i]=dataset[i] - dataset[i - interval]\n        return pd.DataFrame(diff,index=dataset.index,columns=[\"Close\"])\n    else:\n        return dataset","19837078":"# Interval her verisetine bir de\u011ferinin verilmesi yerine hepsi i\u00e7in farkl\u0131la\u015ft\u0131r\u0131labilir\nINTERVAL = 1\ndiffer = difference(series,INTERVAL)\ndiff = differ[INTERVAL:]","5c4eeac1":"rolling=diff.rolling(m)\nma_results=rolling.mean()[m:]\n\n#Residuals of MA and results\nma_errors=diff[m:]- ma_results\n\n\n# MA plot\npyplot.figure(figsize=(24,10))\npyplot.plot(diff.values, color='skyblue', label = 'Original Datas')\npyplot.plot(ma_results.values, color='tomato', label = 'MA Results')\npyplot.legend(loc='best')\npyplot.show()","5842bd1f":"result = adfuller(ma_results)\nprint('ADF Statistic: {}'.format(result[0]))\nprint('p-value: {}'.format(result[1]))\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t{}: {}'.format(key, value))","5a119130":"print(\"series Length      : \",len(series))\nprint(\"ma_results Length  : \",len(ma_diff_results))\nprint(\"ma_errors Length   : \",len(ma_errors))","23593434":"# fit model\norders=()\nif DATA==1:\n    orders=(2,0,1)\nelif DATA==2:\n    orders=(5,1,0)\nelif DATA==3:\n    orders=(3,1,0)\nelif DATA==4:\n    orders=(4,0,0)\nelif DATA==5:\n    orders=(5,1,0)","d451165d":"#Constants\nlag_num=2\nobs_num=1\ndata_sep=26","7b21ba4a":"#Data Reshape\nlin_data = ma_diff_results\n\n#Assigning a partition constant and seperate the samples from the constant\ntrain, test = lin_data[:data_sep], lin_data[data_sep:]\nhistory = [x for x in train.values]\ncons=int(len(test)*0.8)\npredictions = np.zeros(shape=len(test))","e2378c10":"warnings.filterwarnings('ignore')","a1403362":"#ARIMA Forecasting Process\nfor t in range(len(test)):\n    model = ARIMA(history, order=orders)\n    model_fit = model.fit(dsip=0)\n    output = model_fit.forecast()\n    yhat = output[0]\n    predictions[t]=yhat\n    obs = test.values[t]\n    history.append(obs)\n    if t%int(len(test)\/50)==0:\n        progress=int((t\/len(test))*100)\n        print('Progress: | %{prog} |'.format(prog=progress)+('-'*progress),sep='')\n        \nprint(\"\\nCompleted\")\n","d04710a4":"#Data seperation for ANN training and testing\n\nmse = mean_squared_error(test, predictions)\nmae= mean_absolute_error(test,predictions)\nmape=mean_absolute_percentage_error(test,predictions)*100\nprint('Test MSE: '+str(mse)+'  MAE:'+ str(mae)+'  MAPE:'+ str(mape))\n# error plots\npyplot.figure(figsize=(24,8)) \npyplot.plot(test)\npyplot.plot(predictions, color='red')\n\npyplot.show()","3f76e02f":"indicator=None\ndfy=df\nif DATA==4:\n    indicator = dfy[['volume_em', 'trend_sma_fast', 'trend_ema_fast', 'momentum_kama',\n                    'volatility_kcc', 'volatility_kch', 'volatility_kcl','volatility_kcw', 'volatility_kcp', \n                    'volatility_kchi','volatility_kcli', 'volatility_dcl', 'volatility_dch', 'volatility_dcm','volatility_dcw',\n                    'volatility_dcp']]\nelse:\n    indicator = dfy[['trend_ichimoku_conv','trend_ichimoku_base', 'trend_ichimoku_a', 'trend_ichimoku_b','volume_vwap',\n                   'trend_sma_fast', 'trend_ema_fast', 'trend_ema_slow','momentum_kama']]","6b1e0d51":"ROLL = 10\nrolling = dfy.Close.rolling(ROLL)\nrolling.mean()","b87446c2":"ROLL = 10\nrolling = dfy.Close.rolling(ROLL)\n\n# Extracted features from stock data\nattribute=pd.DataFrame(index=dfy.index)\nattribute[\"Mean\"] = rolling.mean()\nattribute[\"Variance\"]=rolling.var()\nattribute[\"Std\"]=rolling.std()\nattribute[\"Skewness\"]=rolling.skew()\nattribute[\"Open_CHT\"] = dfy.Open.pct_change()\nattribute[\"High_CHT\"] = dfy.High.pct_change()\nattribute[\"Low_CHT\"] = dfy.Low.pct_change()\nattribute[\"Volume_CHT\"] = dfy.Volume.pct_change()","9f6c860d":"model = XGBClassifier()\n\nmodel.fit(attribute.values,dfy[\"Close\"].values)\n# plot feature importance\nax = plot_importance(model) \nfeat_importances = pd.Series(model.feature_importances_, index=attribute.columns)\nfeat_importances.nlargest(20).plot(kind='barh')","6600f1fb":"if DATA == 1 :\n    attribute = attribute[attribute[[\"Open_CHT\",\"High_CHT\",\"Volume_CHT\",\"Low_CHT\"]].notna().all(axis=1)]","1678f404":"attribute[attribute[[\"Open_CHT\",\"High_CHT\",\"Volume_CHT\",\"Low_CHT\"]].notna().all(axis=1)]","bb2a452a":"print(len(test))\nprint(len(predictions))\nprint(len(indicator))\n","77a6e36f":"X=pd.DataFrame(ma_errors[data_sep-1:-1].values, columns=['Residual1'], index=ma_errors[data_sep-1:-1].index)\nX.index.name = \"Date\"\nX['Residual2'] = ma_errors[data_sep-2:-2].values\nX['Residual3'] = ma_errors[data_sep-3:-3].values\nX['Observed1'] = diff[data_sep+m-1:-1].values\nX['Observed2'] = diff[data_sep+m-2:-2].values\nX['Observed3'] = diff[data_sep+m-3:-3].values\nX['Observed4'] = diff[data_sep+m-4:-4].values\nX['Observed5'] = diff[data_sep+m-5:-5].values\nX['Observed6'] = diff[data_sep+m-6:-6].values\nX['Observed7'] = diff[data_sep+m-7:-7].values\nX['Observed8'] = diff[data_sep+m-8:-8].values\nX['ARIMA'] = predictions\nX=pd.merge(pd.merge(X,indicator,on=\"Date\"),attribute,on=\"Date\", how=\"inner\")\nX","e6cda55a":"rawY=diff[m+data_sep:].values","7cd39734":"y_train = rawY[:cons]\ny_test = rawY[cons:]\nx_train = np.reshape(X[:cons].values, (X[:cons].shape[0], 1, X[:cons].shape[1]))\nx_test = np.reshape(X[cons:].values, (X[cons:].shape[0], 1, X[cons:].shape[1]))","b9309646":"X[cons:]","37c0fb82":"print(\"X train   : \",x_train.shape)\nprint(\"Y train   : \",y_train.shape)\nprint(\"X test    : \",x_test.shape)\nprint(\"Y test    : \",y_test.shape)","220e3509":"VERBOSE=0\nSHAPE=(1, x_train.shape[2])\n\ndef froto_rnn():\n    model = keras.Sequential()\n    model.add(layers.LSTM(12, input_shape=SHAPE))\n    model.add(layers.Dense(1))\n\n    model.compile(loss='mean_absolute_error',\n                optimizer=tf.keras.optimizers.Adam(0.001))\n\n    return model\n\ndef froto():\n    model = keras.Sequential()\n    model.add(layers.Dense(12, input_shape=SHAPE))\n    model.add(layers.Dense(1))\n\n    model.compile(loss='mean_absolute_error',\n                optimizer=tf.keras.optimizers.Adam(0.001))\n\n    return model\n\ndef sp500():\n    model = keras.Sequential()\n    model.add(layers.LSTM(12, input_shape=SHAPE, return_sequences=True))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LSTM(12))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dense(1))\n\n    model.compile(loss='mean_absolute_error',\n                  optimizer=tf.keras.optimizers.Adam(0.001))\n    return model\n\ndef tupras():\n    model = keras.Sequential()\n    model.add(layers.LSTM(12, input_shape=SHAPE, return_sequences=True))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dense(1))\n\n    model.compile(loss='mean_absolute_error',\n                  optimizer=tf.keras.optimizers.Adam(0.001))\n    return model\n\ndef xbank():\n    model = keras.Sequential()\n    model.add(layers.LSTM(24, input_shape=SHAPE, return_sequences=True))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LSTM(12))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dense(1))\n\n    model.compile(loss='mean_absolute_error',\n                  optimizer=tf.keras.optimizers.Adam(0.001))\n    return model\n\ndef ykbnk():\n    model = keras.Sequential()\n    model.add(layers.LSTM(8, input_shape=SHAPE, return_sequences=True))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LSTM(4))\n    model.add(layers.BatchNormalization())\n    model.add(layers.Dense(1))\n    \n    model.compile(loss='mean_absolute_error',\n                  optimizer=tf.keras.optimizers.Adam(0.001))\n    return model","65cde512":"model = None\nif DATA == 1:\n    model = froto()\nelif DATA == 2:\n    model = sp500()\nelif DATA == 3:\n    model = tupras()\nelif DATA == 4:\n    model = xbank()\nelif DATA == 5:\n    model = ykbnk()","634ec988":"print(len(y_test))\nprint(len(x_test))","f5f34602":"best_score =np.inf\nbest_params = None\n\nepochs = [100,250,500]\nbatches = [16, 64, 128, 256]\nval_split = [0.05,0.1,0.15]\n\n\n\ngrid = {'epochs' : epochs, 'batch_size' : batches, 'validation_split' : val_split}\nfor g in ParameterGrid(grid):\n    ensemble = model.fit(\n    x_train,y_train,\n    validation_split=g['validation_split'], epochs=g['epochs'],  verbose =  VERBOSE)\n    \n    tf_preds=model.predict(x_test)\n    tf_preds=tf_preds.reshape(tf_preds.shape[0],1)\n    \n    mse = mean_squared_error(y_test, tf_preds)\n    mae= mean_absolute_error(y_test, tf_preds)\n    mape= mean_absolute_percentage_error(y_test, tf_preds)*100\n    print('Test MSE: '+str(mse)+'  MAE:'+ str(mae)+'  MAPE:'+ str(mape))\n    if best_score > mse :\n        best_score = mse\n        best_params = g","271eb0f0":"ensemble = model.fit(\nx_train,y_train,\nvalidation_split=best_params['validation_split'],\n    epochs=best_params['epochs'], \n    batch_size = best_params['batch_size'], \n    verbose =  VERBOSE)","84db6c24":"# ensemble = model.fit(\n# x_train,y_train,\n# validation_split=0.05, epochs=100, batch_size =1024, verbose = 0)","08459c9b":"tf_preds=model.predict(x_test)\ntf_preds=tf_preds.reshape(tf_preds.shape[0],1)","522c38ee":"# tf_preds=robust_y.inverse_transform(tf_preds)\n# y_test = robust_y.inverse_transform(y_test)","bd61e11e":"print(tf_preds.shape)\nprint(y_test.shape)","6b0e714c":"series[data_sep+m+cons-1:-2]","1fc89dac":"true_tests = series[data_sep+m+cons-1:-2].values + y_test.reshape(1,-1)[0]\ntrue_preds = series[data_sep+m+cons-1:-2].values + tf_preds.reshape(1,-1)[0]","9e0c7e52":"mse = mean_squared_error(true_tests, true_preds)\nmae= mean_absolute_error(true_tests, true_preds)\nmape= mean_absolute_percentage_error(true_tests, true_preds)*100\nprint('Test MSE: '+str(mse)+'  MAE:'+ str(mae)+'  MAPE:'+ str(mape))\npyplot.figure(figsize=(32,8))\npyplot.plot(true_tests)\npyplot.plot(true_preds, color='red')\npyplot.plot(df.Close[data_sep+m+cons-1:-2], color='green')\n\npyplot.show()","d0398295":"# "}}