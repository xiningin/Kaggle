{"cell_type":{"acf4c01b":"code","10c56e8d":"code","716aa97d":"code","3642c25b":"code","42f1d60e":"code","5cb3639d":"code","ba10d814":"code","628a8833":"code","0a7d6da3":"code","84f7e45f":"code","d41002e8":"code","9278ac9b":"code","29aa7563":"code","1504505b":"code","c362f364":"code","cf7bfa6c":"markdown","d82f0916":"markdown","6052b3ee":"markdown","7088a4cb":"markdown","99b1a77e":"markdown","e5dbb150":"markdown","985ba0f2":"markdown","45e9e917":"markdown","08e95553":"markdown","96a1b09e":"markdown","e0b5c684":"markdown","2169444b":"markdown","e3c93e09":"markdown","44e73c85":"markdown","1597602c":"markdown","e292522d":"markdown"},"source":{"acf4c01b":"import matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np # linear algebra\n\n\n# for inline plot\n%matplotlib inline","10c56e8d":"#plot function\n# used later for plotting\ndef plot_fig(x,y, xlabel, ylabel, title, figsize):\n    print(f\"Plotting of {title} ...\")\n    plt.figure(figsize=figsize)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(title)\n    plt.plot(x, y)","716aa97d":"    def mse_loss(true, pred):\n        \"\"\"\n        true: array of true values    \n        pred: array of predicted values\n\n        returns: mean square error loss\n        \"\"\"\n\n        return np.sum((true - pred)**2)","3642c25b":"# array of same target value 10000 times\ntarget = np.repeat(100, 10000) \npred = np.arange(-10000,10000, 2)\n\nloss_mse = [mse_loss(target[i], pred[i]) for i in range(len(pred))] \nplot_fig(pred,loss_mse, \"Predictions\", \"MSE Loss\", \"MSE Loss vs. Predictions\", (7,5))\nplt.show()","42f1d60e":"def mae_loss(true, pred):\n    \"\"\"\n    true: array of true values    \n    pred: array of predicted values\n    \n    returns: mean absolute error loss\n    \"\"\"\n    return np.sum(np.abs(true - pred))","5cb3639d":"# array of same target value 10000 times\ntarget = np.repeat(100, 10000) \npred = np.arange(-10000,10000, 2)\n\nloss_mse = [mae_loss(target[i], pred[i]) for i in range(len(pred))]\nplot_fig(pred,loss_mse, \"Predictions\", \"MAE Loss\", \"MAE Loss vs. Predictions\", (7,5))","ba10d814":"def huber_loss(true, pred, delta):\n    \"\"\"\n    true: array of true values    \n    pred: array of predicted values\n    \n    returns: smoothed mean absolute error loss\n    \"\"\"\n    loss = np.where(np.abs(true-pred) < delta , 0.5*((true-pred)**2), delta*np.abs(true - pred) - 0.5*(delta**2))\n    return np.sum(loss)","628a8833":"plt.figure(figsize=(7,5))\n\ntarget = np.repeat(0, 1000) \npred = np.arange(-10,10, 0.02)\n\ndelta = [0.1, 1, 10]\n\nlosses_huber = [[huber_loss(target[i], pred[i], q) for i in range(len(pred))] for q in delta]\n\n# plot\nfor i in range(len(delta)):\n    plt.plot(pred, losses_huber[i], label=f\"$\\delta={delta[i]}$\")\nplt.xlabel(\"Predictions\")\nplt.ylabel(\"Huber Loss\")\nplt.title(\"Huber Loss vs. Predictions\")\nplt.legend()","0a7d6da3":"fig, ax1 = plt.subplots(1,1, figsize = (10,6.5))\n\ntarget = np.repeat(0, 1000) \npred = np.arange(-10,10, 0.02)\n\n# calculating loss function for all predictions. \nloss_mse = [mse_loss(target[i], pred[i]) for i in range(len(pred))]\nloss_mae = [mae_loss(target[i], pred[i]) for i in range(len(pred))]\nloss_sm_mae1 = [huber_loss(target[i], pred[i], 5) for i in range(len(pred))]\nloss_sm_mae2 = [huber_loss(target[i], pred[i], 10) for i in range(len(pred))]\n\n\n\nlosses = [loss_mse, loss_mae, loss_sm_mae1, loss_sm_mae2]\nnames = ['MSE', 'MAE','Huber (5)', 'Huber (10)']\ncmap = ['#d53e4f',\n'#fc8d59',\n'#fee08b',\n'#e6f598'\n]\n\nfor lo in range(len(losses)):\n    ax1.plot(pred, losses[lo], label = names[lo], color= cmap[lo], linewidth=6)\nax1.set_xlabel('Predictions')\nax1.set_ylabel('Loss')\nax1.set_title(\"Loss with Predicted values\")\nax1.legend()\nax1.set_ylim(bottom=0, top=40)","84f7e45f":"def bin_ce(true, pred):\n    \"\"\"\n    true: array of true values    \n    pred: array of predicted values\n    \n    returns: binary cross entropy loss\n    \"\"\"\n    loss = np.where(true==1, np.log(pred), np.log(1-pred))\n    return -np.sum(loss)","d41002e8":"# array of same target value 10000 times\ntarget = np.repeat(1, 10000) # considering prediction to be 1\npred = np.arange(0,1, 0.0001) # all predictions b\/w 0 and 1 for 10k values\n\n# calculating loss function for all predictions. \nloss_bin_ce = [bin_ce(target[i], pred[i]) for i in range(len(pred))]\n\n# plot\nplot_fig(pred,loss_bin_ce, \"Predictions\", \"Binary Cross Entropy Loss\", \"Binary Cross Entropy Loss vs. Predictions\", (7,5))","9278ac9b":"def hinge_loss(true, pred):\n    \"\"\"\n    true: array of true values    \n    pred: array of predicted values\n    \n    returns: negative log likelihood loss\n    \"\"\"\n    loss = np.max((0, (1 - pred*true)))\n    return np.sum(loss)","29aa7563":"plt.figure(figsize=(7,5))\n# array of same target value 10000 times\ntarget = np.repeat(1, 10000) # considering prediction to be 1\npred = np.arange(0,1, 0.0001) # all predictions b\/w 0 and 1 for 10k values\n\n# calculating loss function for all predictions. \nloss_hinge = [hinge_loss(target[i], pred[i]) for i in range(len(pred))]\n\nplot_fig(pred,loss_hinge, \"Predictions\", \"Hinge Loss\", \"Hinge Loss vs. Predictions\", (7,5))","1504505b":"#kl_divergence\ndef kl_divergence_loss(true, pred):\n    \"\"\"\n    true: array of true values    \n    pred: array of predicted values\n    \n    returns: kl divergence loss\n    \"\"\"\n    return np.sum(true * np.log(true\/pred))","c362f364":"plt.figure(figsize=(7,5))\n\n\ntarget = np.repeat(100, 10000) \npred = np.arange(-10000,10000, 2)\n\nkl_divergence_losses = [kl_divergence_loss(target[i], pred[i]) for i in range(len(pred))] \n\nplot_fig(pred,kl_divergence_losses, \"Predictions\", \"KL Divergence Loss\", \"KL Divergence Loss vs. Predictions\", (7,5))","cf7bfa6c":"### Hinge Loss\nHinge loss is used for training classifiers. It is used for maximum margin classification, support vector machines(SVM). The hinge loss is a convex function, so many of the usual convex optimizers used in machine learning can work with it.\n\nHinge loss for an input-output pair (true, pred) is given as:\n### $Loss = max(0, 1 - pred * true)$","d82f0916":"###  Mean Absolute Error(MAE) or L1 Loss: \n ###  $MAE = \\sum_{i=1}^{n} |Y_i - Y_{pred}|$","6052b3ee":"###  Huber Loss or Smooth l1 loss\n Combine L1 and L2 Loss.\n * Less Sensitive to outliers in data than l2 loss.\n * Basically absolute error which becomes quadratic when the error is small. How small that error has to be to make it quadratic depends on a hyperparameter , **$\\delta$**, which can be tuned.\n \n NOTE: The choice of delta is critical because it determines what you're willing to consider outlier\n \n![huber_loss](https:\/\/user-images.githubusercontent.com\/30827903\/123542499-abdf2d00-d769-11eb-832c-2ac2f662645d.png)\n\n","7088a4cb":"### Multi-class Cross Entropy\nThe multi-class cross-entropy loss is a generalization of the Binary Cross Entropy loss.  If c>2 (i.e. multiclass classification), we calculate a separate loss for each class label per observation and sum the result.\n![image](https:\/\/user-images.githubusercontent.com\/30827903\/123580280-33c04800-d7f9-11eb-8e2f-0906f9913772.png)\n\nThe plot is same as Binary cross entropy and the main difference is it is used when there is more than one class.","99b1a77e":"### Binary Classification Loss\n* Binary cross entropy\n* Hinge Loss","e5dbb150":"All regression loss in one place","985ba0f2":"#### First try to understand why we need loss function in Neural Network(Deep Learning):\n\n**Overview:**\n\nLoss function which takes in two parameters:\n1. Predicted output\n2. True output\n\nOur main goal is to minimize loss function i.e network should learn as per the given target \u2192 predictions should be similar to target labels.\n\n<i>Simple Neural Network example with two hidden layers.<\/i>\n\n![image](https:\/\/user-images.githubusercontent.com\/30827903\/123585890-a7ffe900-d803-11eb-94f1-f6ad18cd49ee.png)\n\nWhat does it do ?\n* The function calculate how given model is performing i.e. based on comparison (Y_pred, Y) \n* If Y_pred is very far off from y, the loss is very high.\n\n    Simply we can use square distance calculation between them:\n     $Loss = (Y_{pred} - Y)^2$ \n     \n* If both values are almost similar, the loss value will be very low i.e. distance between them will be low.\n\n    * If the loss is very high, the huge value will propagate through network i.e weights updated during backpropagation with greater Amount. This task is done by our pretty cool optimizer. \n    \n    Simple Example of gradient descent optimizer:\n    \n    <h4>$weight_{new} = weight_{old} - learningrate * \\frac{\\partial Loss}{\\partial weight}$ and \n    \n    $bias_{new} = bias_{old} - learningrate * \\frac{\\partial Loss}{\\partial bias}$<\/h4>\n    The main target is to optimize our network in such a direction, loss will be small. That direction find out by the gradient of loss w.r.to parameter.\n    * If loss is small then the weights won\u2019t change since the network is already doing a good job. Again that is handled by our pretty intelligent optimizer.\n\n","45e9e917":"## Mean Squared Error(MSE) or L2 Loss :\n## $MSE = \\sum_{i=1}^{n} (Y_i - Y_{pred})^2$\n\n","08e95553":"# Loss Functions\n* ### In this notebook, we look on different loss functions: Formula, scratch implementation and Visualization\n\n## Regression Loss\u00b6\n* MSE\n* MAE \n* Huber Loss\n\n## Classification Loss\n ### Binary Classification Loss\n* Binary cross entropy\n* Hinge Loss\n\n### Multi-class Classification Loss\n* Multi-class Cross Entropy Loss\n* KL Divergence Loss","96a1b09e":"**Here is differences between l1 and l2 loss functions:**\n\n![image](https:\/\/user-images.githubusercontent.com\/30827903\/123583443-505f7e80-d7ff-11eb-8a9e-2aea276ac911.png)\n","e0b5c684":"\n## Regression loss\u00b6\n* MSE\n* MAE \n* Huber Loss\n","2169444b":"#### Hope this helps you. You learn about different loss functions and there visualiation. Please thumb up \ud83d\udc4d if you like it and share to others. \n\n#### If there is any queries, let me know in comment. Thanks for reading. \ud83d\ude10","e3c93e09":"#### Forward Pass:\n![image](https:\/\/user-images.githubusercontent.com\/30827903\/123587085-7720b380-d805-11eb-9983-c118354381aa.png)\n\n#### Backward Pass:\n1. If loss is high: \n\n    High weights update since networks have to learn more in the direction where loss is minimized. The direction is given by the gradient of loss. Weight update in that direction is done by the optimizer.\n    \n    ![image](https:\/\/user-images.githubusercontent.com\/30827903\/123587554-2e1d2f00-d806-11eb-8ddc-dbcf1080ee9a.png)\n\n2. If loss is low:\n    \n    Low update weights since the network is already doing a good job.\n\n\n![image](https:\/\/user-images.githubusercontent.com\/30827903\/123587577-337a7980-d806-11eb-924e-7b0f2bfb46b1.png)\n\n\nFrom this understanding its time for see implementation of each and visualization. Lets' start .","44e73c85":"### Binary Cross Entropy\nAlso called log loss. \n```\nProbability that the element belongs to class 1 (or positive class) = p\nThen, the probability that the element belongs to class 0 (or negative class) = 1 - p\n```\nThen, the cross-entropy loss for output label y (can take values 0 and 1) and predicted probability p is defined as:\n![image](https:\/\/user-images.githubusercontent.com\/30827903\/123546105-fae18e00-d77a-11eb-9fd6-7a994d3a14e8.png)\n","1597602c":"## Multi-class Classification Loss\n* Multi-class Cross entropy\n* Kullback\u2013Leibler divergence","e292522d":"### Kullback-leibler divergence\n\nThe Kullback-Liebler Divergence is a measure of how a probability distribution differs from another distribution. A KL-divergence of zero indicates that the distributions are identical.\n\n![image](https:\/\/user-images.githubusercontent.com\/30827903\/123546464-55c7b500-d77c-11eb-87fd-10ed1c0f7f21.png)\n"}}