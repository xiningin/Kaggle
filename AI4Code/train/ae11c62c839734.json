{"cell_type":{"100abb1c":"code","ac8a1ca3":"code","59b37a08":"code","ac1d9b2b":"code","67bc927f":"code","e486760c":"code","cee59d0e":"code","1705df17":"code","8ff936bb":"code","0a49174d":"code","eba01174":"code","7151bf58":"code","bad0d128":"code","5b39f978":"code","38749eae":"code","601bfbcb":"code","b2ab9b9c":"code","254246f5":"code","db88f04b":"code","4affd4c7":"code","f9256901":"code","b612b667":"code","5343351b":"markdown"},"source":{"100abb1c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ac8a1ca3":"!pip install transformers","59b37a08":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nimport torch\nimport transformers as ppb\nimport warnings\nwarnings.filterwarnings('ignore')","ac1d9b2b":"df = pd.read_csv('\/kaggle\/input\/stockmarket-sentiment-dataset\/stock_data.csv', header=None, skiprows=[0])","67bc927f":"df.shape","e486760c":"batch_1 = df[:5791]","cee59d0e":"print(batch_1[0][6])","1705df17":"batch_1[1].value_counts()","8ff936bb":"model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\ntokenizer = tokenizer_class.from_pretrained(pretrained_weights)\nmodel = model_class.from_pretrained(pretrained_weights)","0a49174d":"tokenized = batch_1[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))","eba01174":"tokenized[:1]","7151bf58":"max_len = 0\nfor i in tokenized.values:\n    if len(i)>max_len:\n        max_len = len(i)\n\npadded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])","bad0d128":"padded.shape","5b39f978":"attention_mask = np.where(padded != 0, 1, 0)\nattention_mask.shape","38749eae":"input_ids = torch.tensor(padded)\nattention_mask = torch.tensor(attention_mask)\nwith torch.no_grad():\n    last_hidden_states = model(input_ids, attention_mask=attention_mask)","601bfbcb":"features = last_hidden_states[0][:, 0, :].numpy()\n","b2ab9b9c":"labels = batch_1[1]","254246f5":"print(features[:10])","db88f04b":"train_features, test_features, train_labels, test_labels = train_test_split(features, labels)","4affd4c7":"lr_clf = LogisticRegression()\nlr_clf.fit(train_features, train_labels)","f9256901":"lr_clf.score(test_features, test_labels)","b612b667":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\nlogit_roc_auc = roc_auc_score(test_labels, lr_clf.predict(test_features))\nfpr, tpr, thresholds = roc_curve(test_labels, lr_clf.predict_proba(test_features)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","5343351b":"# BERT for performing sentiment analysis on financial data\n\nThis notbook is an attempt to perform sentiment analysis on stock-news using BERT. The results show that model has achieved 74.2% accuracy."}}