{"cell_type":{"a4c186cd":"code","c47bc2b6":"code","79d11d0c":"code","e9707a3e":"code","6d51c3b1":"code","211e9529":"code","d68f89c7":"code","207d4f32":"code","744726cc":"code","da070752":"code","55301887":"code","11ea1731":"code","0086f07c":"code","9d0b962e":"code","22471357":"code","39fc492e":"code","90f2473c":"code","46416c8c":"code","7dbefde8":"code","3fb03e0d":"code","de64340a":"code","ab975959":"code","f4f7c0a2":"code","8bf7e95d":"code","0cfdc47d":"code","ba0700c3":"code","dd0dd276":"code","1db15851":"code","89e8ed71":"code","739cb7af":"code","e8fd40fa":"code","a3828454":"code","f9d402c7":"code","854b2366":"code","232639e3":"code","62cff15f":"code","f0b6b4bf":"markdown","5c617104":"markdown","542126e5":"markdown","eadce2f9":"markdown","7e91381f":"markdown","6382e7ab":"markdown","a9652998":"markdown","37dfdb19":"markdown","81f5d8ae":"markdown","869b673f":"markdown","4b8b5764":"markdown","9780fced":"markdown","21f0c180":"markdown","1ac2582c":"markdown","c5a6d1e9":"markdown","1f478365":"markdown","d9bbde82":"markdown","63554faf":"markdown","fbc27fa5":"markdown"},"source":{"a4c186cd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# PreProcessing\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder,RobustScaler,MinMaxScaler\nimport category_encoders as ce\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\n# Splitting Data\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n\n# Resampling\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler, NearMiss\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.pipeline import Pipeline\n\n# Modeling, Fitting and Evaluation\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, precision_score, roc_auc_score, plot_roc_curve,recall_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom datetime import datetime\nimport datetime\nfrom sklearn import metrics\n\n# Boosting\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost.sklearn import XGBClassifier\n\n#feature Selection\nfrom sklearn.feature_selection import SelectPercentile, RFE\n\n#saving\nimport pickle\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c47bc2b6":"data = pd.read_csv('\/kaggle\/input\/marketing-data\/marketing_data.csv')","79d11d0c":"#most successful campaign\ncampaign = data.loc[:,['Response','AcceptedCmp1','AcceptedCmp2','AcceptedCmp3','AcceptedCmp4','AcceptedCmp5']]\n\ncampaign = campaign.melt()\ncampaign = pd.crosstab(campaign[\"variable\"], campaign[\"value\"]).sort_values(0)\n\ncols = list(campaign.columns)\na, b = cols.index(0), cols.index(1)\ncols[b], cols[a] = cols[a], cols[b]\ncampaign = campaign[cols]\n\ncampaign.columns = \"Yes\",\"No\"\ncampaign.plot.bar(stacked=True)\nplt.title('Acceptance of Marketing Campaigns')\nplt.xlabel('Campaign')\nplt.ylabel('Acceptance')\nplt.legend(title='Response',loc='upper right')\nplt.savefig('Campaign.png')\nplt.show()","e9707a3e":"#check value unique in every columns\nfor i in data.columns:\n    result = data[i].unique()\n    print (i,'\\n',result,'\\n')","6d51c3b1":"#Make Column Customer Age\ndata['Dt_Customer']= pd.to_datetime(data['Dt_Customer'])\ndata['Customer_Age'] = data['Dt_Customer'].dt.year - data['Year_Birth']","211e9529":"#change cust join date to how long cust has joined\ntodayy = pd.Timestamp('28\/2\/21') #tanggal perhitungan terakhir\ndata['Dt_Customer'] = (todayy - data['Dt_Customer']).dt.days","d68f89c7":"#rename column income \ndata.rename(columns={' Income ':'Income'},inplace=True)\ndata['Income']=data['Income'].str.replace('[$,]','').astype(float)","207d4f32":"#Summarizing Categori from Marital_Status\ndata['Marital_Status'] = data['Marital_Status'].replace(['Widow','Divorced','Alone'],'Single')\ndata['Marital_Status'] = data['Marital_Status'].replace(['Married'],'Together')\ndata['Marital_Status'] = data['Marital_Status'].replace(['Absurd','YOLO'],'Other')","744726cc":"mean_scale = Pipeline([\n    ('impute', SimpleImputer(strategy = 'mean')),\n    ('scaling', RobustScaler()),\n])\n\ntransformer = ColumnTransformer([\n    ('impute',mean_scale,['Income']),\n    ('encoder',OneHotEncoder(handle_unknown='ignore'),['Education','Marital_Status']),\n    ('binary',ce.BinaryEncoder(),['Country']),\n    ('scale',RobustScaler(),['Customer_Age','Recency'])\n],remainder='passthrough')\n\ndata=data.drop(['MntWines','MntFruits','MntMeatProducts','MntFishProducts','MntSweetProducts','MntGoldProds','NumDealsPurchases','NumWebPurchases','NumCatalogPurchases','NumStorePurchases','NumWebVisitsMonth','Complain','Dt_Customer','ID','Year_Birth','AcceptedCmp3','AcceptedCmp4','AcceptedCmp5','AcceptedCmp1','AcceptedCmp2'],axis=1)","da070752":"X=data.drop(['Response'],axis=1)\ny=data['Response']","55301887":"#check transform\ntransformer.fit_transform(data)","11ea1731":"X_train,X_test,y_train,y_test=train_test_split(X,y,stratify=y,test_size=0.3,random_state=2020)","0086f07c":"data['Response'].value_counts()\/data.shape[0]*100","9d0b962e":"logreg = LogisticRegression()\ntree = DecisionTreeClassifier(random_state = 2020)\nknn = KNeighborsClassifier()\nrf = RandomForestClassifier(random_state = 2020)","22471357":"logreg_pipe = Pipeline([\n    ('transform',transformer),\n    ('logreg',logreg)\n])\n\ntree_pipe= Pipeline([\n    ('transform',transformer),\n    ('tree',tree)\n])\n\nknn_pipe =Pipeline([\n    ('transform',transformer),\n    ('knn',knn)\n])\n\nrf_pipe = Pipeline([\n    ('transform',transformer),\n    ('rf',rf)\n])","39fc492e":"def model_evaluation(model, metric):\n    skfold = StratifiedKFold(n_splits = 5)\n    model_cv = cross_val_score(model, X_train, y_train, cv = skfold, scoring = metric)\n    return model_cv\n\nlogreg_pipe_cv = model_evaluation(logreg_pipe, 'precision')\ntree_pipe_cv = model_evaluation(tree_pipe, 'precision')\nknn_pipe_cv = model_evaluation(knn_pipe, 'precision')\nrf_pipe_cv = model_evaluation(rf_pipe, 'precision')\n\nfor model in [logreg_pipe,tree_pipe, knn_pipe,rf_pipe]:\n    model.fit(X_train, y_train)\n    \nscore_mean = [logreg_pipe_cv.mean(),tree_pipe_cv.mean(),knn_pipe_cv.mean(),rf_pipe_cv.mean()]\nscore_std = [logreg_pipe_cv.std(),tree_pipe_cv.std(),knn_pipe_cv.std(),rf_pipe_cv.std()]\nscore_precision_score = [precision_score(y_test, logreg_pipe.predict(X_test)),\n            precision_score(y_test, tree_pipe.predict(X_test)),\n            precision_score(y_test, knn_pipe.predict(X_test)),\n            precision_score(y_test, rf_pipe.predict(X_test))]\nmethod_name = ['Logistic Regression','Decision Tree Classifier','KNN Classifier', 'Random Forest Classifier']\ncv_result = pd.DataFrame({\n    'method': method_name,\n    'mean score': score_mean,\n    'std score': score_std,\n    'precision score': score_precision_score\n})\ncv_result","90f2473c":"rus = RandomUnderSampler(random_state = 2020)\nX_under, y_under = rus.fit_resample(X_train, y_train) ","46416c8c":"logreg_pipe_under = Pipeline([\n    ('transformer', transformer),\n    ('rus', rus),\n    ('logreg', logreg)\n])\n\ntree_pipe_under = Pipeline([\n    ('transformer', transformer),\n    ('rus', rus),\n    ('tree', tree)\n])\n\nknn_pipe_under = Pipeline([\n    ('transformer', transformer),\n    ('rus', rus),\n    ('knn', knn)\n])\n\nrf_pipe_under = Pipeline([\n    ('transformer', transformer),\n    ('rus', rus),\n    ('rf', rf)\n])","7dbefde8":"def model_evaluation(model, metric):\n    skfold = StratifiedKFold(n_splits = 5)\n    model_cv = cross_val_score(model, X_train, y_train, cv = skfold, scoring = metric) \n    return model_cv\n\nlogreg_under_cv = model_evaluation(logreg_pipe_under, 'precision') \ntree_under_cv = model_evaluation(tree_pipe_under, 'precision')\nknn_under_cv = model_evaluation(knn_pipe_under, 'precision')\nrf_under_cv = model_evaluation(rf_pipe_under, 'precision')\n\nfor model in [logreg_pipe_under, tree_pipe_under, knn_pipe_under, rf_pipe_under]:\n    model.fit(X_train, y_train)\n\nscore_mean = [logreg_under_cv.mean(), tree_under_cv.mean(), knn_under_cv.mean(),\n              rf_under_cv.mean()]\nscore_std = [logreg_under_cv.std(), tree_under_cv.std(), knn_under_cv.std(),\n             rf_under_cv.std()]\nscore_precision_score = [precision_score(y_test, logreg_pipe_under.predict(X_test)),\n            precision_score(y_test, tree_pipe_under.predict(X_test)), \n            precision_score(y_test, knn_pipe_under.predict(X_test)), \n            precision_score(y_test, rf_pipe_under.predict(X_test))]\nmethod_name = ['Logistic Regression UnderSampling', 'Decision Tree Classifier UnderSampling',\n              'KNN Classifier UnderSampling', 'Random Forest Classifier UnderSampling']\nunder_result = pd.DataFrame({\n    'method': method_name,\n    'mean score': score_mean,\n    'std score': score_std,\n    'precision score': score_precision_score\n})\nunder_result","3fb03e0d":"ros = RandomOverSampler(random_state = 2020)\nX_over, y_over = ros.fit_resample(X_train, y_train)","de64340a":"logreg_pipe_over = Pipeline([\n    ('transformer', transformer),\n    ('ros', ros), \n    ('logreg', logreg)\n])\n\ntree_pipe_over = Pipeline([\n    ('transformer', transformer),\n    ('ros', ros), \n    ('tree', tree)\n])\n\nknn_pipe_over = Pipeline([\n    ('transformer', transformer),\n    ('ros', ros), \n    ('knn', knn)\n])\n\nrf_pipe_over = Pipeline([\n    ('transformer', transformer),\n    ('ros', ros),\n    ('rf', rf)\n])","ab975959":"def model_evaluation(model, metric):\n    skfold = StratifiedKFold(n_splits = 5)\n    model_cv = cross_val_score(model, X_train, y_train, cv = skfold, scoring = metric)\n    return model_cv\n\nlogreg_over_cv = model_evaluation(logreg_pipe_over, 'precision') \ntree_over_cv = model_evaluation(tree_pipe_over, 'precision')\nknn_over_cv = model_evaluation(knn_pipe_over, 'precision')\nrf_over_cv = model_evaluation(rf_pipe_over, 'precision')\n\nfor model in [logreg_pipe_over, tree_pipe_over, knn_pipe_over, rf_pipe_over]:\n    model.fit(X_train, y_train)\n\nscore_mean = [logreg_over_cv.mean(), tree_over_cv.mean(), knn_over_cv.mean(),\n              rf_over_cv.mean()]\nscore_std = [logreg_over_cv.std(), tree_over_cv.std(), knn_over_cv.std(),\n             rf_over_cv.std()]\nscore_precision_score = [precision_score(y_test, logreg_pipe_over.predict(X_test)),\n            precision_score(y_test, tree_pipe_over.predict(X_test)), \n            precision_score(y_test, knn_pipe_over.predict(X_test)), \n            precision_score(y_test, rf_pipe_over.predict(X_test))]\nmethod_name = ['Logistic Regression OverSampling', 'Decision Tree Classifier OverSampling',\n              'KNN Classifier OverSampling', 'Random Forest Classifier OverSampling']\nover_summary = pd.DataFrame({\n    'method': method_name,\n    'mean score': score_mean,\n    'std score': score_std,\n    'precision score': score_precision_score\n})\nover_summary","f4f7c0a2":"nm = NearMiss(version = 1)","8bf7e95d":"logreg_pipe_nm = Pipeline([\n    ('transformer', transformer),\n    ('nm', nm),\n    ('logreg', logreg)\n])\n\ntree_pipe_nm = Pipeline([\n    ('transformer', transformer),\n    ('nm', nm),\n    ('tree', tree)\n])\n\nknn_pipe_nm = Pipeline([\n    ('transformer', transformer),\n    ('nm', nm),\n    ('knn', knn)\n])\n\nrf_pipe_nm = Pipeline([\n    ('transformer', transformer),\n    ('nm', nm),\n    ('rf', rf)\n])","0cfdc47d":"def model_evaluation(model, metric):\n    skfold = StratifiedKFold(n_splits = 5)\n    model_cv = cross_val_score(model, X_train, y_train, cv = skfold, scoring = metric)\n    return model_cv\n\nlogreg_nm_cv = model_evaluation(logreg_pipe_nm, 'precision') \ntree_nm_cv = model_evaluation(tree_pipe_nm, 'precision')\nknn_nm_cv = model_evaluation(knn_pipe_nm, 'precision')\nrf_nm_cv = model_evaluation(rf_pipe_nm, 'precision')\n\nfor model in [logreg_pipe_nm, tree_pipe_nm, knn_pipe_nm, rf_pipe_nm]:\n    model.fit(X_train, y_train)\n    \nscore_mean = [logreg_nm_cv.mean(), tree_nm_cv.mean(), knn_nm_cv.mean(),\n              rf_nm_cv.mean()]\nscore_std = [logreg_nm_cv.std(), tree_nm_cv.std(), knn_nm_cv.std(),\n             rf_nm_cv.std()]\nscore_precision_score = [precision_score(y_test, logreg_pipe_nm.predict(X_test)),\n            precision_score(y_test, tree_pipe_nm.predict(X_test)), \n            precision_score(y_test, knn_pipe_nm.predict(X_test)), \n            precision_score(y_test, rf_pipe_nm.predict(X_test))]\nmethod_name = ['Logistic Regression NearMiss', 'Decision Tree Classifier NearMiss',\n              'KNN Classifier NearMiss', 'Random Forest Classifier NearMiss']\nnm_summary = pd.DataFrame({\n    'method': method_name,\n    'mean score': score_mean,\n    'std score': score_std,\n    'precision score': score_precision_score\n})\nnm_summary","ba0700c3":"#Summary Balancing Dataset\nresume_balancing = pd.concat([under_result,over_summary,nm_summary], axis=0)\nresume_balancing","dd0dd276":"adaboost = AdaBoostClassifier(\n            tree,\n            n_estimators = 50,\n            learning_rate = 0.1,\n            random_state = 2020)\n\npipe_ada = Pipeline([\n    ('transformer', transformer),\n    ('adaboost', adaboost)\n])\n\ngradboost = GradientBoostingClassifier(\n            n_estimators = 50,\n            learning_rate = 0.1,\n            max_depth = 3,\n            random_state = 2020)\n\npipe_grad = Pipeline([\n    ('transformer', transformer),\n    ('gradboost', gradboost)\n])\n\nXGBOOST = XGBClassifier(\n            n_estimators = 50,\n            learning_rate = 0.1,\n            max_depth = 3,\n            random_state = 2020)\n\npipe_XGB = Pipeline([\n    ('transformer', transformer),\n    ('XGBOOST', XGBOOST)\n])","1db15851":"def model_evaluation(model, metric):\n    skfold = StratifiedKFold(n_splits = 5)\n    model_cv = cross_val_score(model, X_train, y_train, cv = skfold, scoring = metric, n_jobs = -1)\n    return model_cv\n\npipe_ada_cv = model_evaluation(pipe_ada, 'precision')\npipe_grad_cv = model_evaluation(pipe_grad, 'precision')\npipe_XGB_cv = model_evaluation(pipe_XGB, 'precision')\n\nfor model in [pipe_ada, pipe_grad, pipe_XGB]:\n    model.fit(X_train, y_train)\n    \nscore_mean = [pipe_ada_cv.mean(), pipe_grad_cv.mean(), pipe_XGB_cv.mean()]\nscore_std = [pipe_ada_cv.std(), pipe_grad_cv.std(), pipe_XGB_cv.std()]\nscore_precision_score = [precision_score(y_test, pipe_ada.predict(X_test)),\n            precision_score(y_test, pipe_grad.predict(X_test)), \n            precision_score(y_test, pipe_XGB.predict(X_test))]\nmethod_name = ['Ada Boost Classifier', 'Gradient Boost Classifier',\n              'XGB Classifier']\nboost_summary = pd.DataFrame({\n    'method': method_name,\n    'mean score': score_mean,\n    'std score': score_std,\n    'precision score': score_precision_score\n})\nboost_summary","89e8ed71":"resume_model = pd.concat([cv_result,boost_summary], axis=0)\nresume_model","739cb7af":"#cek confusion matrix Logistic regression\nlogreg_pipe.fit(X_train, y_train)\nypred=logreg_pipe.predict(X_test)\nprint(classification_report(y_test,ypred))\nprint(metrics.confusion_matrix(y_test,ypred))","e8fd40fa":"#cek confusion matrix XGB boost\npipe_XGB.fit(X_train, y_train)\nypred=pipe_XGB.predict(X_test)\nprint(classification_report(y_test,ypred))\nprint(metrics.confusion_matrix(y_test,ypred))","a3828454":"logreg=LogisticRegression()\n\nestimator = Pipeline([\n    ('transformer', transformer),\n    ('model', logreg)\n])\n\nhyperparam_space =  {\n    'model__C': [100, 10, 1, 0.1, 0.01, 0.001],\n    'model__solver': ['liblinear', 'newton-cg']\n}","f9d402c7":"grid_search = GridSearchCV(\n                estimator,\n                param_grid = hyperparam_space,\n                cv = StratifiedKFold(n_splits = 5),\n                scoring = 'precision',\n                n_jobs = -1)","854b2366":"grid_search.fit(X_train, y_train)","232639e3":"print('best score', grid_search.best_score_)\nprint('best param', grid_search.best_params_)","62cff15f":"logreg_pipe.fit(X_train, y_train)\ny_pred_estimator = logreg_pipe.predict(X_test)\nprecision_estimator = precision_score(y_test, y_pred_estimator)\n\ngrid_search.best_estimator_.fit(X_train, y_train)\ny_pred_grid = grid_search.best_estimator_.predict(X_test)\nprecision_best_estimator = precision_score(y_test, y_pred_grid)\n\nscore_list = [precision_estimator, precision_best_estimator]\nmethod_name = ['Logistic Regression Before', 'Logistic Regression After']\nbest_summary = pd.DataFrame({\n    'method': method_name,\n    'score': score_list\n})\nbest_summary","f0b6b4bf":"data imbalance, so when the precision score is low, we can try balancing dataset for choose the best model","5c617104":"I will try for Handling imbalance dataset","542126e5":"The Company wants to conduct a Campaign and the goal is for increase the number of member customer.Campaigns method based on the last campaign that most successful than the previous campaign. and Consumer response will be predicted based on the profile and when the last consumer purchases (Recency, if never purchase then (-1))\n\n* *0 = No respon*\n* *1 = yes*\n\n        - TN: Consumers who are predicted will not respond to the campaign, in fact it does not respond\n        - TP: Consumers are predicted to respond the campaign, actually it does respond\n        - FP: Consumers who are predicted to respond the campaign, actually do not respond\n        - FN: Predicted consumers do not respond to the campaign, actually respond\nError that occurred:\n* FN: Wrong prediction, the company only loses prospective customers, but not financial losses\n* FP: the company loses more such as time, energy and financial, because it has prepared everything for the campaign to people, but those people is not response.\n\n**So the most influential mistake for financial losses is FP**\n\n**The selected metric evaluation is Precision because it will press FP value**","eadce2f9":"preprocessing scheme:\n>     one hot : education,marital_status\n>     binary : country\n>     drop : ID,yearbirth,AcceptedCmp3,AcceptedCmp4,AcceptedCmp5,AcceptedCmp1,AcceptedCmp2,Complain,Dt_Customer,ID(karena tdk ada corelasi dengan respon yang ingin mencari Customer baru)","7e91381f":"**Analysis**","6382e7ab":"**After balancing the dataset the precision score is decrease, so model is used without balancing the dataset. And based on the resume above a stable model is KNN and Logistic regression. will then try the Boosting model**","a9652998":"# **Handling Imbalance Dataset**","37dfdb19":"*Random Under Sampling*","81f5d8ae":"*Random Over Sampling*","869b673f":"# **Model Benchmark**","4b8b5764":"**Preprocessing**","9780fced":"**Precision score before is better than after tunning, so the model will be choose is Logistic Regression without tuning**","21f0c180":"# **Data Splitting**","1ac2582c":"# Hyperparameter Tunning\n","c5a6d1e9":"*Cek Balancing Data*","1f478365":"**Data Cleansing**","d9bbde82":"*NearMiss*","63554faf":"**Based on the resume, we will choose Logistic regression, because it has the highest precision score and precision score between class 1 and 0 is balance. then it will proceed to hyperparameter tunning **","fbc27fa5":"# **BOOSTING**"}}