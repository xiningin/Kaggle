{"cell_type":{"1ebfb805":"code","69631182":"code","e8693a5b":"code","aef0952a":"code","a553bfbd":"code","b1b0f5c7":"code","ea664e64":"code","2577b9f6":"code","a5bf5a59":"code","dbef63b9":"code","5556a6d3":"code","edd86d19":"code","522282f9":"code","5b3a737c":"code","49894b53":"code","82740462":"code","3152a935":"code","2ebef795":"code","aafbe624":"code","21b6af52":"markdown","7148699c":"markdown","98564c15":"markdown","e73dc058":"markdown","b2146d26":"markdown","2045c734":"markdown","34bb0461":"markdown","75055f09":"markdown","505e8a30":"markdown","de0db0fe":"markdown","90e00257":"markdown","51e59922":"markdown","a3d2c147":"markdown","266a4486":"markdown","78c22bc3":"markdown","2016e97d":"markdown","3aa34bc3":"markdown","729b895f":"markdown","b1d9686f":"markdown","c5da4ea7":"markdown","85278b49":"markdown","7abfb2d7":"markdown","ddde77b0":"markdown","23e1bbe5":"markdown","84c262f5":"markdown","e7ea5c90":"markdown"},"source":{"1ebfb805":"import numpy as np\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt","69631182":"data = load_breast_cancer(as_frame=True)\ndata = data[\"frame\"]\ndata.head(10)","e8693a5b":"print(\"Malignant: \", len(data[\"target\"][data[\"target\"] == 1]))\nprint(\"Denied: \", len(data[\"target\"][data[\"target\"] == 0]))","aef0952a":"data.shape","a553bfbd":"features = list(data.columns[0:30])\nX = data[features]\nY = data[\"target\"]\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=40)","b1b0f5c7":"scaler = preprocessing.StandardScaler().fit(X_train)  # create object of normalizator and train it at train(!) data. now our data has not changed.\nX_train_normalized = scaler.transform(X_train)      # Changing our data with fuction transform(). Now our data is normalized\nX_test_normalized = scaler.transform(X_test)        # Doing the same with test data\nX_train_normalized","ea664e64":"pca = PCA(n_components=2)  #create object and indicate htat finish number of features must be 2.\npca.fit(X_train_normalized) # train PCA\nX_test_normalized_for_visualization = pca.transform(X_test_normalized) # Transform data\nX_train_normalized_for_visualization = pca.transform(X_train_normalized)\nX_train_normalized_for_visualization[1:10]","2577b9f6":"color = []\nfor i in y_train:\n    if i == 0:\n        color.append(\"green\")\n    elif i == 1:\n        color.append(\"red\")\n\nplt.scatter(X_train_normalized_for_visualization[:,0], X_train_normalized_for_visualization[:,1], c = color)","a5bf5a59":"model_logistic_reg = LogisticRegression().fit(X_train_normalized, y_train)","dbef63b9":"y_pred = model_logistic_reg.predict(X_test_normalized) #predict answers for test data\ny_pred","5556a6d3":"accuracy_score(y_test, y_pred) # pass real answers and our orediction","edd86d19":"f1_score(y_test, y_pred)","522282f9":"\nrange_for_C = list(np.linspace(1, 5, 21))\nparameters = {\n    \"kernel\": (\"linear\", \"poly\", \"rbf\", \"sigmoid\"), \n    \"C\": range_for_C, \n    \"degree\": [1, 2, 3, 4, 5], # this is just additional hyperparameters\n    \"gamma\": (\"scale\", \"auto\")\n}","5b3a737c":"svc = SVC() #create object of SVC (support vector classifier)\nmodel_svc = GridSearchCV(svc, parameters) # pass model and dictionary with parameters","49894b53":"model_svc.fit(X_train_normalized, y_train) #training","82740462":"model_svc.best_params_","3152a935":"y_pred_svc = model_svc.predict(X_test_normalized)\naccuracy_score(y_test, y_pred_svc)","2ebef795":"f1_score(y_test, y_pred_svc)","aafbe624":"y_pred_train = model_svc.predict(X_train_normalized)\n\ncolor1 = []\nfor i in y_pred_svc:\n    if i == 0:\n        color1.append(\"green\")\n    elif i == 1:\n        color1.append(\"red\")\n\ncolor2 = []\nfor i in y_test:\n    if i == 0:\n        color2.append(\"green\")\n    elif i == 1:\n        color2.append(\"red\")\nplt.figure(figsize = (15,5))\nplt.subplot(1,2,1)\nplt.title(\"Original Data\")\nplt.scatter(X_test_normalized_for_visualization[:,0], X_test_normalized_for_visualization[:,1], c = color2)\nplt.subplot(1,2,2)\nplt.title(\"Predicted Data\")\nplt.scatter(X_test_normalized_for_visualization[:,0], X_test_normalized_for_visualization[:,1], c = color1)","21b6af52":"Now we must split the data into test set and train set. We *must* do this because we will train our model at train set and testing at test set and it is very important our model never \"see\" test set befor testing, otherwise model assessment will be incorrect. \n\nWe have 31 columns of features and one of them is column \"target\". So we split y = \"target\" and X = all the rest.   \n\nThan use function train_test_split() to split data into train and test sets. test_size = 0.2 means that test set will be 20% from all dataset. I also use parametr random_state to shaffle the data befor splitting, just in case.","7148699c":"# Visualization and PCA (optional step)\nBefore creating the model, let's first draw our data, just to have an idea about them.","98564c15":"This guide is for completely *begginer data scientist*. Here I will consider **Logistic Regression** and **SVM** algorithm from Scikit-learn (sklearn) library with detailed explanation. We will also touch **Grid Search** (for getting better hyper parameters) and **PCA** (for dimensionality reduction to project it to a lower dimensional space) algorithms.","e73dc058":"Again, predict and count score.","b2146d26":" **Normalization**   \n We will use Logistic Regression and SVM algorithm, both of them use [gradiend descent](https:\/\/ml-cheatsheet.readthedocs.io\/en\/latest\/gradient_descent.html) to find minimum of function. If the data range is large, gradient descent will move very slowly. For ex. see  there are small values like 0,07 in column \"mean fractal dimension\" and at the same time there are much bigger values like 1700 in the column \"worst area\".\n So it is very important for us make normalization to gradient descent move faster.\n \n *Note: it is not always necessary to do data normalization*. there are algorithms that do not use gradient descent (for ex. Random Forest). in this case, it doesn\u2019t matter if the data is normalized or not.","2045c734":"Here we just importing all that we will need further. ","34bb0461":"Now let's choose better hyperparameters. SVM have two main hyperparameters \"kernel\" and \"C\". We will use grid search to get the best hyperparameters for our model. ","75055f09":"Greate job! 99% of accuracy.","505e8a30":"# SVM\nsuport vector mashine ","de0db0fe":"Above is prediction of our model. Now we gave prediction and real answer, so let's count accuracy of our model! ","90e00257":"We have 30 features, so our chart will be 30D :-)\nFortunatly, there is greate algorithm PCA wich allow us to reduce demensoin of data. So we going to reduce dimensional of our data from 30 features to 2 features. This is only for visualization purpose.","51e59922":"For this purpose I decided to use [StandartScaler()](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html) this is normalizator.","a3d2c147":"Shape of our dataset. We have 569 rows(samples) and 31 columns(features) ","266a4486":"For download breast canser dataset use function [load_breast_cancer()](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.load_breast_cancer.html). I set up parament as_frame=True, because I want function to return pandas DataFrame.\nThis function will return the dictionary, so in our variable \"data\" will be dictionary and for getting the pandas DataFrame use key \"frame\". Read more in documentation.\n\nNow let's look at our breast canser dataset, for this use function .head(n), and you will se first n samples of our dataset (it is 10 in this case).\n\nWe can see a lot of column of data about tumor and last column is \"target\" wich contain 0 and 1. So it is 1 if tumor malignant and 0 if denied.    ","78c22bc3":"At the end I decided to visualize real test data and our prediction.\n\nLook at this charts, they almost the same!","2016e97d":"Create the dictionary where key will be the name of the paramener and value will be different values for this paramenet. And then GridSearch does all job for us. ","3aa34bc3":"I think we should use different metrict at the end because samles with malignant tumor is 1.7 times more than with denied tumor. So, we will use both of them.","729b895f":"Here I check if the data are balanced.\n\n*Balanced data* is when, if you have 2 classes, then the number of examples in these two classes is approximately the same. And the data is not balanced if some class has much more examples than the other class.\n\nIt is important because if data are not balanced than we can't just use simple [accuracy_score()](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.accuracy_score.html) to evaluate the accuracy of the model. Because accuracy_score just compares your prediction with the correct answer and counts the percentage of correct predictions. And if you have for ex. 100 samples and 99 of this samples has target 1, than program that will just print(1) will have 99% accuracy. In this case much better use [f1_score()](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html).","b1d9686f":"Here you can see the best parameters which GridSearch finds","c5da4ea7":"So now we have only two features, let's draw train data!\nI will indicate malignant tumor in red dot and denied tumor in green dot.","85278b49":"That's all with Logistic Regression","7abfb2d7":"Good resut 96% of right answers.\n\nAlso use f1_score (becouse of imbalanced of data)","ddde77b0":"# Logistic Regression\n\nCreate object of LogisticRegression and train it passing the train data and target column.","23e1bbe5":"*Thank you for your attention!*","84c262f5":"*Note: I did not use function .fit() with test data. StandartScaler was fitting only with train data.*","e7ea5c90":"For this guide I choosed [breast cancer wisconsin dataset (classification)](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.load_breast_cancer.html) - standart dataset from sklearn.datasets.  \nIt contains 30 parameters of tumor and we will must **determine it is malignant(1) or denied(0)**.  \n\nSo let's go :-)"}}