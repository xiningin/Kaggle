{"cell_type":{"6ca026cb":"code","f40cedda":"code","dcc72d84":"code","6fba4be1":"code","8b485c29":"code","7b053182":"code","7abe0ed9":"code","0cf295bc":"code","89da068f":"code","c6598d6a":"code","b653d160":"code","4561777b":"code","20f82975":"code","08b7ec1b":"code","7f1de155":"code","6b96a93e":"code","cf730865":"code","6c39f9bb":"code","8e80170c":"code","16df887d":"code","2408a9b4":"code","c2696833":"code","99e5c28c":"code","d96aae76":"code","21c0a594":"code","05156801":"code","1bbf558a":"markdown","dd25e03a":"markdown","a36c7ec4":"markdown","c0c3f261":"markdown","7e342a6a":"markdown","d7366ebf":"markdown","0e059790":"markdown","1caacaa3":"markdown","f85c4de6":"markdown","e4018280":"markdown","a33b33dd":"markdown","e914b192":"markdown","6507f1ee":"markdown","5003f03a":"markdown","b70fdc52":"markdown","ca56d676":"markdown","98de4a6c":"markdown","d46a7bcd":"markdown","257e5036":"markdown","d47a326d":"markdown","ce5af937":"markdown","0e2e363c":"markdown"},"source":{"6ca026cb":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV","f40cedda":"pd.set_option('display.max_columns', 20)\npd.set_option('display.width', 2000)\n\ndf = pd.read_csv('..\/input\/price-of-flats-in-moscow\/flats_moscow.csv')\n\n\"\"\"\nprice: \u0446\u0435\u043d\u0430 \u043a\u0432\u0430\u0440\u0442\u0438\u0440\u044b \u0432 $1000 \/ the price of a flat in 1000$;\ntotsp: \u043e\u0431\u0449\u0430\u044f \u043f\u043b\u043e\u0449\u0430\u0434\u044c \u043a\u0432\u0430\u0440\u0442\u0438\u0440\u044b, \u043a\u0432.\u043c. \/ total square of a flat, m2;\nlivesp: \u0436\u0438\u043b\u0430\u044f \u043f\u043b\u043e\u0449\u0430\u0434\u044c \u043a\u0432\u0430\u0440\u0442\u0438\u0440\u044b, \u043a\u0432.\u043c. \/ living square of a flat, m2;\nkitsp: \u043f\u043b\u043e\u0449\u0430\u0434\u044c \u043a\u0443\u0445\u043d\u0438, \u043a\u0432.\u043c. \/ kitchen square, m2;\ndist: \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u0435 \u043e\u0442 \u0446\u0435\u043d\u0442\u0440\u0430 \u0432 \u043a\u043c. \/ distance to the city center, km;\nmetrdist: \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u0435 \u0434\u043e \u043c\u0435\u0442\u0440\u043e \u0432 \u043c\u0438\u043d\u0443\u0442\u0430\u0445 \/ distance to the nearest metro station, min;\nwalk: 1 \u2013 \u043f\u0435\u0448\u043a\u043e\u043c \u043e\u0442 \u043c\u0435\u0442\u0440\u043e, 0 \u2013 \u043d\u0430 \u0442\u0440\u0430\u043d\u0441\u043f\u043e\u0440\u0442\u0435 \/ 1 - walk to metro, 0 - using transport;\nbrick: 1 \u2013 \u043a\u0438\u0440\u043f\u0438\u0447\u043d\u044b\u0439, \u043c\u043e\u043d\u043e\u043b\u0438\u0442 \u0436\/\u0431, 0 \u2013 \u0434\u0440\u0443\u0433\u043e\u0439 \/ 1 - brick, monolithic house, 0 - anothers;\nfloor: 1 \u2013 \u044d\u0442\u0430\u0436 \u043a\u0440\u043e\u043c\u0435 \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u0438 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0433\u043e, 0 \u2013 \u0438\u043d\u0430\u0447\u0435 \/ 0 - the first or the last floor, 1 - anothers;\ncode: \u0447\u0438\u0441\u043b\u043e \u043e\u0442 1 \u0434\u043e 8, \u043e\u0431\u043e\u0437\u043d\u0430\u0447\u0430\u044e\u0449\u0435\u0435 \u0440\u0430\u0439\u043e\u043d \u0433\u043e\u0440\u043e\u0434\u0430 \/ number from 1 to 8 of the city area:\n    1. \u0421\u0435\u0432\u0435\u0440, \u043e\u043a\u043e\u043b\u043e \u041a\u0430\u043b\u0443\u0436\u0441\u043a\u043e-\u0420\u0438\u0436\u0441\u043a\u043e\u0439 \u043b\u0438\u043d\u0438\u0438 \u043c\u0435\u0442\u0440\u043e \/ North of the city, around Kaluzhsko-Rizhskaya metro line;\n    2. \u0421\u0435\u0432\u0435\u0440, \u043e\u043a\u043e\u043b\u043e \u0421\u0435\u0440\u043f\u0443\u0445\u043e\u0432\u0441\u043a\u043e-\u0422\u0438\u043c\u0438\u0440\u044f\u0437\u0435\u0432\u0441\u043a\u043e\u0439 \u043b\u0438\u043d\u0438\u0438 \u043c\u0435\u0442\u0440\u043e \/ North of the city, around Serpukhovsko-Timiryazevskaya metro line;\n    3. \u0421\u0435\u0432\u0435\u0440\u043e-\u0437\u0430\u043f\u0430\u0434, \u043e\u043a\u043e\u043b\u043e \u0417\u0430\u043c\u043e\u0441\u043a\u0432\u043e\u0440\u0435\u0446\u043a\u043e\u0439 \u043b\u0438\u043d\u0438\u0438\u0438 \u043c\u0435\u0442\u0440\u043e \/ North-West, around Zamoskvoretskaya metro line;\n    4. \u0421\u0435\u0432\u0435\u0440\u043e-\u0437\u0430\u043f\u0430\u0434, \u043e\u043a\u043e\u043b\u043e \u0422\u0430\u0433\u0430\u043d\u0441\u043a\u043e-\u041a\u0440\u0430\u0441\u043d\u043e\u043f\u0440\u0435\u0441\u043d\u0435\u043d\u0441\u043a\u043e\u0439 \u043b\u0438\u043d\u0438\u0438\u0438 \u043c\u0435\u0442\u0440\u043e \/ North-West, around Tagansko-Krasnopresnenskaya metro line;\n    5. \u042e\u0433\u043e-\u0432\u043e\u0441\u0442\u043e\u043a, \u043e\u043a\u043e\u043b\u043e \u041b\u044e\u0431\u043b\u0438\u043d\u0441\u043a\u043e\u0439 \u043b\u0438\u043d\u0438\u0438\u0438 \u043c\u0435\u0442\u0440\u043e \/ South-East, around Lyublinskaya metro line;\n    6. \u042e\u0433\u043e-\u0432\u043e\u0441\u0442\u043e\u043a, \u043e\u043a\u043e\u043b\u043e \u0422\u0430\u0433\u0430\u043d\u0441\u043a\u043e-\u041a\u0440\u0430\u0441\u043d\u043e\u043f\u0440\u0435\u0441\u043d\u0435\u043d\u0441\u043a\u043e\u0439 \u043b\u0438\u043d\u0438\u0438\u0438 \u043c\u0435\u0442\u0440\u043e \/ South-East, around Tagansko-Krasnopresnenskaya metro line;\n    7. \u0412\u043e\u0441\u0442\u043e\u043a, \u043e\u043a\u043e\u043b\u043e \u041a\u0430\u043b\u0438\u043d\u0438\u043d\u0441\u043a\u043e\u0439 \u043b\u0438\u043d\u0438\u0438\u0438 \u043c\u0435\u0442\u0440\u043e \/ East, around Kalininskaya metro line;\n    8. \u0412\u043e\u0441\u0442\u043e\u043a, \u043e\u043a\u043e\u043b\u043e \u0410\u0440\u0431\u0430\u0442\u0441\u043a\u043e-\u041f\u043e\u043a\u0440\u043e\u0432\u0441\u043a\u043e\u0439 \u043b\u0438\u043d\u0438\u0438\u0438 \u043c\u0435\u0442\u0440\u043e \/ East, around Arbatsko-Pokrovskaya metro line.\n\"\"\"","dcc72d84":"df","6fba4be1":"df.drop('Unnamed: 0', axis=1, inplace=True)","8b485c29":"df.dtypes","7b053182":"df.isnull().sum()","7abe0ed9":"corr = df.corr(method='pearson')\ncorr","0cf295bc":"plt.figure(figsize=(15,10))\nsns.heatmap(corr, annot=True, cmap='YlGn')\nsns.pairplot(df)","89da068f":"plt.figure(figsize=(15,10))\nsns.scatterplot(x=df.dist, y=df.price)","c6598d6a":"df.query('(400 < price < 500) & (14 < dist < 15)')","b653d160":"df = df[df.price < 600] #the first value\ndf = df.drop(df.index[885]).reset_index() #the second value (I found its index before)","4561777b":"X = df[['totsp', 'livesp', 'kitsp', 'dist', 'metrdist', 'walk', 'brick', 'floor', 'code']]\ny = df.price","20f82975":"np.random.seed(0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)","08b7ec1b":"from sklearn.decomposition import PCA\npca_test = PCA(n_components=X_train.shape[1])\nX_train_scaled = X_train\npca_test.fit(X_train_scaled)\nplt.plot(np.cumsum(pca_test.explained_variance_ratio_))\nplt.show()","7f1de155":"def best_params(models_names):\n    \"\"\"models_names = ['name1', 'name2', ...]\n    names from LinearRegression, RandomForestRegressor, KNeighborsRegressor, SVR only\"\"\"\n\n    d = {}\n\n    for model_name in models_names:\n        if model_name == 'LinearRegression':\n            lin_reg_parameters = {\"fit_intercept\": [True, False], 'normalize': [True, False]}\n            lin_reg_search = GridSearchCV(LinearRegression(), lin_reg_parameters, cv=5)\n            lin_reg_search.fit(X_train, y_train)\n            d['LinearRegression'] = lin_reg_search.best_params_\n            global lin_reg_search_fitted\n            lin_reg_search_fitted = lin_reg_search\n\n        elif model_name == 'RandomForestRegressor':\n            rf_reg_parameters = {'n_estimators': range(1, 200), 'max_depth': range(1, 30), 'min_samples_split': range(2, 30), 'min_samples_leaf': range(1, 30)}\n            rf_reg_search = RandomizedSearchCV(RandomForestRegressor(), rf_reg_parameters, cv=5, n_jobs=-1, random_state=0)\n            rf_reg_search.fit(X_train, y_train)\n            d['RandomForestRegressor'] = rf_reg_search.best_params_\n            global rf_reg_search_fitted\n            rf_reg_search_fitted = rf_reg_search\n\n        elif model_name == 'KNeighborsRegressor':\n            kn_reg_parameters = {'n_neighbors': range(3, 10), 'weights': ['uniform', 'distance'], 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'], 'leaf_size': range(1, 40)}\n            kn_reg_search = GridSearchCV(KNeighborsRegressor(), kn_reg_parameters, cv=5)\n            kn_reg_search.fit(X_test, y_test)            \n            d['KNeighborsRegressor'] = kn_reg_search.best_params_\n            global kn_reg_search_fitted\n            kn_reg_search_fitted = kn_reg_search\n\n        elif model_name == 'SVR':\n            svr_reg_parameters = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n            svr_reg_search = GridSearchCV(SVR(), svr_reg_parameters, cv=5, n_jobs=-1)\n            svr_reg_search.fit(X, y)\n            d['SVR'] = svr_reg_search.best_params_\n            global svr_reg_search_fitted\n            svr_reg_search_fitted = svr_reg_search\n\n    return d","6b96a93e":"best_params(['LinearRegression', 'RandomForestRegressor', 'KNeighborsRegressor', 'SVR'])","cf730865":"def best_model(X_train, y_train, X_test, y_test):\n    import re\n    models = [LinearRegression(fit_intercept=True, normalize=True, n_jobs=-1),\n              RandomForestRegressor(n_estimators=65, min_samples_split=13, min_samples_leaf=6, max_depth=10, n_jobs=-1, random_state=0),\n              KNeighborsRegressor(algorithm='auto', leaf_size=21, n_neighbors=5, weights='distance', n_jobs=-1),\n              SVR(kernel='linear')]\n    d = {}\n    for model in models:\n        model.fit(X_train, y_train)\n        model_name = re.match(r'\\w+', str(model)).group()\n        d[model_name] = r2_score(y_test, model.predict(X_test))\n    mdf = pd.DataFrame(d, index=['r2_score:'])\n    \n    plt.figure(figsize=(9,5))\n    bp = sns.barplot(x=mdf.columns, y=mdf.values.flatten())\n    bp.set_xlabel(\"Models\", fontsize=14)\n    bp.set_ylabel(\"R2\", fontsize=14)\n\n    best_regression_model = mdf.idxmax(axis=1)[0]\n    return ('Best_regression_model: ', best_regression_model, '\\n', mdf)","6c39f9bb":"print(*best_model(X_train, y_train, X_test, y_test))","8e80170c":"rf = RandomForestRegressor(n_estimators=65, min_samples_split=13, min_samples_leaf=6, max_depth=10, n_jobs=-1, random_state=0)\nrf.fit(X_train, y_train)\nprint(rf.feature_importances_) #X = df[['totsp', 'livesp', 'kitsp', 'dist', 'metrdist', 'walk', 'brick', 'floor', 'code']]\nprint('R2:', r2_score(y_test, rf.predict(X_test)))","16df887d":"#X = df[['totsp', 'livesp', 'kitsp', 'dist', 'metrdist', 'walk', 'brick', 'floor', 'code']] # R2_score = 0.74448\n\n#X = df[['totsp', 'livesp', 'kitsp', 'dist', 'metrdist', 'walk', 'brick', 'floor']] # R2_score = 0.73568\n#X = df[['totsp', 'livesp', 'kitsp', 'dist', 'metrdist', 'walk', 'brick', 'code']] # R2_score = 0.74297\n#X = df[['totsp', 'livesp', 'kitsp', 'dist', 'metrdist', 'walk', 'floor', 'code']] # R2_score = 0.74279\n#X = df[['totsp', 'livesp', 'kitsp', 'dist', 'metrdist', 'brick', 'floor', 'code']] # R2_score = 0.74019\n#X = df[['totsp', 'livesp', 'kitsp', 'dist', 'walk', 'brick', 'floor', 'code']] # R2_score = 0.743839\n#X = df[['totsp', 'livesp', 'kitsp', 'metrdist', 'walk', 'brick', 'floor', 'code']] # R2_score = 0.71029\n#X = df[['totsp', 'livesp', 'dist', 'metrdist', 'walk', 'brick', 'floor', 'code']] # R2_score = 0.74771!!!!!! The best result without one variable\n#X = df[['totsp', 'kitsp', 'dist', 'metrdist', 'walk', 'brick', 'floor', 'code']] # R2_score = 0.71649\n#X = df[['livesp', 'kitsp', 'dist', 'metrdist', 'walk', 'brick', 'floor', 'code']] # R2_score = 0.71274\n\n#X = df[['totsp', 'livesp', 'dist', 'metrdist', 'walk', 'brick', 'floor']] # R2_score = 0.73633\n#X = df[['totsp', 'livesp', 'dist', 'metrdist', 'walk', 'brick', 'code']] # R2_score = 0.74645\n#X = df[['totsp', 'livesp', 'dist', 'metrdist', 'walk', 'floor', 'code']] # R2_score = 0.74474\n#X = df[['totsp', 'livesp', 'dist', 'metrdist', 'brick', 'floor', 'code']] # R2_score = 0.74134\n#X = df[['totsp', 'livesp', 'dist', 'walk', 'brick', 'floor', 'code']] # R2_score = 0.74858!!!!!! The best result without two variables\n#X = df[['totsp', 'livesp', 'metrdist', 'walk', 'brick', 'floor', 'code']] # R2_score = 0.71198\n#X = df[['totsp', 'dist', 'metrdist', 'walk', 'brick', 'floor', 'code']] # R2_score = 0.71347\n#X = df[['livesp', 'dist', 'metrdist', 'walk', 'brick', 'floor', 'code']] # R2_score = 0.66966\n\n#X = df[['totsp', 'livesp', 'dist', 'walk', 'brick', 'floor']] # R2_score = 0.73481\n#X = df[['totsp', 'livesp', 'dist', 'walk', 'brick', 'code']] # R2_score = 0.74708\n#X = df[['totsp', 'livesp', 'dist', 'walk', 'floor', 'code']] # R2_score = 0.74676\n#X = df[['totsp', 'livesp', 'dist', 'brick', 'floor', 'code']] # R2_score = 0.74269\n#X = df[['totsp', 'livesp', 'walk', 'brick', 'floor', 'code']] # R2_score = 0.71311\n#X = df[['totsp', 'dist', 'walk', 'brick', 'floor', 'code']] # R2_score = 0.71147\n#X = df[['livesp', 'dist', 'walk', 'brick', 'floor', 'code']] # R2_score = 0.67486","2408a9b4":"X = df[['totsp', 'livesp', 'dist', 'walk', 'brick', 'floor', 'code']]\ny = df.price\nnp.random.seed(0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)","c2696833":"best_params(['RandomForestRegressor'])\nreg_search_df = pd.DataFrame(rf_reg_search_fitted.cv_results_)\nreg_search_df = reg_search_df[['param_n_estimators', 'param_max_depth', 'param_min_samples_split', 'param_min_samples_leaf', 'mean_test_score']]\nreg_search_df","99e5c28c":"fig, axs = plt.subplots(ncols=4)\nfig.set_size_inches(20, 7)\nsns.barplot(x='param_n_estimators', y='mean_test_score', data=reg_search_df, ax=axs[0])\nsns.barplot(x='param_max_depth', y='mean_test_score', data=reg_search_df, ax=axs[1])\nsns.barplot(x='param_min_samples_split', y='mean_test_score', data=reg_search_df, ax=axs[2])\nsns.barplot(x='param_min_samples_leaf', y='mean_test_score', data=reg_search_df, ax=axs[3])","d96aae76":"parameters = {'n_estimators': [65, 177, 20], 'max_depth': [10, 21, 18], 'min_samples_split': [13, 6, 14], 'min_samples_leaf': range(1, 8)}\nreg_search = GridSearchCV(RandomForestRegressor(random_state=0), parameters, cv=5, n_jobs=-1)\nreg_search.fit(X_train, y_train)\nreg_search.best_params_","21c0a594":"rf = RandomForestRegressor(n_estimators=177, min_samples_split=6, min_samples_leaf=1, max_depth=18, n_jobs=-1, random_state=0)\nrf.fit(X_train, y_train)\nprint('R2:', r2_score(y_test, rf.predict(X_test)))","05156801":"total_space = 64\nlive_space = 45\ncity_center_distance = 10\nwalk = 1\nbrick = 1\nfloor = 0\ncode = 3\n\nX_flat = [[total_space, live_space, city_center_distance, walk, brick, floor, code]]\ny_pred = rf.predict(X_flat)\nprint('Recommended price of your flat in RUB:', (y_pred[0] * 1000 * 75).round(1))","1bbf558a":"Reading a Dataset:","dd25e03a":"Searching for N\/A values:","a36c7ec4":"First, I import all libraries I need:","c0c3f261":"As we can see, r2_score became a little better! Previous R squared was 0.74858, now it's 0.76008.","7e342a6a":"In the beginning of analysis I use all variables to predict price:","d7366ebf":"As result, we can predict the price of some flat, using new predictors values:","0e059790":"After that, I take these parameters and put them in the model.","1caacaa3":"![dist-price.PNG](attachment:6d8b3d71-397c-4ba2-ac19-56debe53ccba.PNG)","f85c4de6":"Next I wanna see the correlation of the data variables. Let's have a look at correlation of a predictable variable \"y\" (price):","e4018280":"Creating a function for searching the best hyperparameters of models:","a33b33dd":"As we can see, there is no N\/A values in the data.","e914b192":"Creating a function for searching the best model with the best parameters.","6507f1ee":"As you can see, I get the best r2_score = 0.74858 for X = df[['totsp', 'livesp', 'dist', 'walk', 'brick', 'floor', 'code']]. Now let's refresh X and y for the renewed data:","5003f03a":"After getting the best model let's focus on it. Firstly, I wanna leave only important values to simplify the model and increase the calculation speed. I need to get the best r2_score. I'll delete each variable alternately and see the results. If I get a better one, I'll repeat the searching without found variable again several times untill getting worse results.","b70fdc52":"Let's find the index of the second outlier value:","ca56d676":"We can see the best separate correlation of the \"price\" with \"totsp\", \"livesp\", \"kitsp\" and \"dist\". There are some outlier observations at the graphs \"price\" - \"totsp\", \"price\" - \"kitsp\" and \"price\" - \"dist\". Let's take a look at the \"price\" - \"dist\", where we can see two obvious outlier observations.","98de4a6c":"Let's see the data:","d46a7bcd":"As you can see, there two outlier values at it, that we can delete:","257e5036":"# Regression Analysis\n***Here I'm comparing several regression models to get the best prediction accuracy using the coefficient of determination (R^2).***","d47a326d":"Let's find the best hyperparameters for the model using GridSearchCV instead of RandomizedSearchCV. In the beginning I run the existing model, make a graph with the best parameters and choose some of them to reduce search range.","ce5af937":"I can choose some best parameters, when I analyze the graph. Let's search n_estimators in 65, 177 and 20; max_depth in 10, 21 and 18; min_samples_split in 13, 6 and 14; min_samples_leaf in range from 1 to 8.","0e2e363c":"As you can see, 3 variables explain more than 95% of varience. If I have much more variables, I'd use this method and get rid of some variables. But I have only 9, so I'll leave them."}}