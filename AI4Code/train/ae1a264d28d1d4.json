{"cell_type":{"39243343":"code","0a797833":"code","def4d446":"code","58aa2f64":"code","9678bfac":"code","19de2678":"code","09a1ee3d":"code","f676b7e5":"code","88e2c5b0":"code","9ced7795":"code","7da214dc":"code","daf193a8":"code","ad69f1d3":"code","700c04fb":"code","af143e3b":"code","0ff91a0f":"code","0516b94a":"code","99596bd8":"code","bae3221c":"code","4e8d84e2":"code","448304d4":"code","36a82666":"markdown","9d3ee481":"markdown","c818ef17":"markdown","3ee93211":"markdown","64cbb676":"markdown","6a869ab9":"markdown","5a78972f":"markdown","1bec1b55":"markdown","64c1a81e":"markdown","9cd2e304":"markdown","2312c5c8":"markdown","7cc287eb":"markdown","3199045a":"markdown","57e06f47":"markdown","fad39e14":"markdown","afeb2475":"markdown","012527ca":"markdown","1409522f":"markdown","64989ec8":"markdown"},"source":{"39243343":"# Install Numerai's API\n!pip install numerapi","0a797833":"# Obfuscated API keys\nfrom kaggle_secrets import UserSecretsClient\n\n# API Key for Stock News API\nSTOCK_NEWS_API_TOKEN = UserSecretsClient().get_secret(\"STOCK_NEWS_API_TOKEN\")\n\n# API settings for submitting to Numerai\nNMR_PUBLIC_ID = UserSecretsClient().get_secret(\"NMR_PUBLIC_ID\")\nNMR_SECRET_KEY = UserSecretsClient().get_secret(\"NMR_SECRET_KEY\")\nMODEL_NAME = UserSecretsClient().get_secret(\"MODEL_NAME\")","def4d446":"import os\nimport ast\nimport torch\nimport pickle\nimport requests\nimport numpy as np\nimport random as rn\nimport pandas as pd\nfrom tqdm import tqdm\nfrom datetime import datetime\nfrom typing import Tuple, List\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler\nfrom dateutil.relativedelta import relativedelta, FR\n\n# Machine Learning\nimport torch.nn.functional as F\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n# Numerai's API\nimport numerapi\n\n# Core data paths\nMODEL_PATH = \"ipuneetrathore\/bert-base-cased-finetuned-finBERT\"\nTICKER_PATH = \"https:\/\/numerai-signals-public-data.s3-us-west-2.amazonaws.com\/signals_ticker_map_w_bbg.csv\"\nNEWS_DATA_PATH = \"news_data.csv\"\nSUB_PATH = \"finbert_submission.csv\"\n\n# Model inference parameters\nMAX_LEN = 256\nBATCH_SIZE = 8\n\n# Set seed for reproducability\nseed = 1234\nrn.seed(seed)\nnp.random.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\n\n# Surpress Pandas warnings\npd.set_option('chained_assignment', None)","58aa2f64":"with open(\"\/kaggle\/input\/valid-numerai-signals-tickers-stocknewsapi\/relevant_tickers.pkl\", 'rb') as f:\n    stock_news_tickers = set(pickle.load(f)['tickers'])\n    \ntickers = pd.read_csv(TICKER_PATH)\nnumerai_tickers = set(tickers[\"ticker\"].values)\n\nrelevant_tickers = list(set(stock_news_tickers).intersection(numerai_tickers))","9678bfac":"print(f\"There are a total of {len(relevant_tickers)} intersecting stock tickers.\")","19de2678":"class StockNewsAPILoader:\n    \"\"\"\n    Data loader for Stock News API: https:\/\/stocknewsapi.com\n    :param token: API Key for Stock News API\n    \"\"\"\n    def __init__(self, token: str):\n        self.domain = \"https:\/\/stocknewsapi.com\/api\/v1\"\n        self.token = token\n\n        # Safety parameter to limit unnecessary API calls\n        # Retrieving all news articles for a ticker requires on average 2 API calls\n        self.page_cutoff_point = 3\n\n    def get_last_week_news(self, ticker: str) -> pd.DataFrame:\n        \"\"\"\n        Get all news for a ticker from last week (monday to friday)\n\n        :param ticker: Valid ticker symbol for stocknewsAPI\n        :return: Pandas DataFrame containing all data for ticker in the past week\n        \"\"\"\n        i = 1\n        dfs = []\n        while i <= self.page_cutoff_point:\n            api_request = f\"{self.domain}?tickers={ticker}&items=50&type=article&page={i}&sortby=rank&days=7&token={self.token}\"\n            r = requests.get(api_request)\n            try:\n                json_request = r.json()['data']\n            except KeyError:\n                break\n            df = pd.DataFrame(json_request)\n            if df.empty:\n                break\n            dfs.append(df)\n            i += 1\n            \n        final_df = pd.concat(dfs)\n        return final_df\n\n    def get_last_week_news_multiple_tickers(self, tickers: list) -> pd.DataFrame:\n        dfs = []\n        for t in tqdm(tickers):\n            df = self.get_last_week_news(t)\n            dfs.append(df)\n        final_df = pd.concat(dfs)\n        return final_df","09a1ee3d":"# Retrieve data from API\nsnl = StockNewsAPILoader(STOCK_NEWS_API_TOKEN)\ndata = snl.get_last_week_news_multiple_tickers(relevant_tickers)\ndata.to_csv(NEWS_DATA_PATH, index=False)","f676b7e5":"class StockNewsProcessor:\n    \"\"\"\n    Preprocessor for Stock News API output\n    https:\/\/stocknewsapi.com\n    \"\"\"\n    def __init__(self, relevant_tickers = list):\n        self.tickers = pd.read_csv(TICKER_PATH)\n        self.unnecessary_cols = ['text', 'news_url', 'image_url', 'topics', 'source_name']\n        self.text_cols = ['title']\n        self.relevant_tickers = relevant_tickers\n\n    def preprocess(self, data: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Data cleaning for Stock News API data\n\n        :param data: A raw Pandas DataFrame containing at least the columns 'type', 'sentiment', 'date' and\n                    columns specified in self.unnecessary_cols.\n        :return: A clean and sorted DataFrame with date as index.\n        \"\"\"\n        data = data.drop(self.unnecessary_cols, axis=1)\n        data = data.drop_duplicates()\n        data['tickers'] = data['tickers'].apply(lambda s: list(ast.literal_eval(s)))\n        data = data.drop('type', axis=1)\n\n        data = data[data['sentiment'] != \"Neutral\"]\n        data = data.drop('sentiment', axis=1)\n\n        data['date'] = data['date'].apply(lambda x: pd.to_datetime(x).tz_convert('UTC'))\n        data = data.set_index(data['date'], drop=True).sort_index()\n        return data\n\n    def aggregate(self, data: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Combine weekly text and order by Friday date\n\n        :param data: A preprocessed DataFrame\n        :return: Data grouped by ticker and friday dates\n        \"\"\"\n        for col in self.text_cols:\n            data.loc[:, col] = data[col] + \" [SEP] \"\n\n        dfs = []\n        for ticker in tqdm(self.relevant_tickers):\n            aggregated = data[data['tickers'].apply(lambda x: ticker in x)].resample(\"W-fri\", on='date').sum()\n            aggregated = aggregated.drop(\"tickers\", axis=1)\n            aggregated['ticker'] = ticker\n            aggregated = aggregated.drop_duplicates(\"ticker\", keep='last')\n            if aggregated.empty:\n                continue\n            dfs.append(aggregated)\n        new_df = pd.concat(dfs)\n        new_df['title'] = new_df['title'].astype(str)\n        merged = new_df.merge(self.tickers, on='ticker')\n        merged = merged.drop(\"yahoo\", axis=1).dropna()\n        return merged\n\n    def full_preprocessing(self, data: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"\n        Reads in API data and makes it ready for further analysis\n\n        :param data: Pandas DataFrame generated with stocknewsapi_loader\n        :return: Data grouped by ticker and friday dates\n        \"\"\"\n        proc_data = self.preprocess(data)\n        agg_data = self.aggregate(proc_data)\n        return agg_data","88e2c5b0":"# Process and aggregate collected data\nraw_data = pd.read_csv(NEWS_DATA_PATH)\nsnp = StockNewsProcessor(relevant_tickers)\nproc_data = snp.full_preprocessing(raw_data)","9ced7795":"print(\"Preprocessed rows:\")\nproc_data.head()","7da214dc":"sample = proc_data.iloc[0]\nprint(f\"Example of aggregated headlines for '{sample['bloomberg_ticker']}' stock:\\n\")\nprint(sample['title'])","daf193a8":"class FinBertCased:\n    def __init__(self, max_len: int, batch_size: int, model_path: str):\n        self.max_len = max_len\n        self.model_path = model_path\n        self.batch_size= batch_size\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_path).eval().to('cuda')\n        self.label_dict = {0: 'negative', 1: 'neutral', 2: 'positive'}\n        self.inverse_label_dict = {v: k for k, v in self.label_dict.items()}\n\n    def full_preprocess(self, text: List[str]) -> Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\" Preprocessing pipeline from string to ids and attention mask. \"\"\"\n        encoded = self.tokenizer(text, \n                                 add_special_tokens=True,\n                                 max_length=self.max_len,\n                                 padding='max_length',\n                                 return_attention_mask=True,\n                                 return_tensors='pt',\n                                 truncation=True)\n        input_ids = torch.cat([encoded['input_ids']], dim=0).to('cuda')\n        attention_mask = torch.cat([encoded['attention_mask']], dim=0).to('cuda')\n        return input_ids, attention_mask\n\n    def predict_raw(self, text: List[str]) -> torch.Tensor:\n        \"\"\" Predict raw logits \"\"\"\n        input_ids, attention_mask = self.full_preprocess(text)\n        model_output = self.model(input_ids, token_type_ids=None, attention_mask=attention_mask)\n        logits = model_output[0]\n        return logits\n\n    def predict_score(self, text: List[str]) -> np.float32:\n        \"\"\" Predict a single sentiment score (positive_sentiment - negative_sentiment) \"\"\"\n        logits = self.predict_raw(text)\n        softmax_output = F.softmax(logits, dim=1).cpu().detach().numpy()\n        pos_idx = self.inverse_label_dict['positive']\n        neg_idx = self.inverse_label_dict['negative']\n        return softmax_output[:, pos_idx] - softmax_output[:, neg_idx]\n\n    def predict_signals(self, text: pd.Series) -> List[float]:\n        \"\"\"\n        Get ranking of average sentiment scores for every ticker in the data.\n        :param text: Pandas Series of articles grouped by week and ticker\n        :return: Scaled sentiment scores in range [0...1]\n        \"\"\"\n        sent_scores = []\n        for row in tqdm(text):\n            sents = row.split(\" [SEP] \")[:-1]\n            sent_scores_ticker = []\n            for batch in self._chunks(sents, self.batch_size):\n                batch_sents = self.predict_score(batch)\n                sent_scores_ticker.append(batch_sents)\n            mean_score = np.array(np.concatenate(sent_scores_ticker)).ravel().mean()\n            sent_scores.append(mean_score)\n        signals = self._scale_sentiment(sent_scores)\n        return signals\n    \n    @staticmethod\n    def _chunks(lst, n):\n        \"\"\" Yield successive n-sized chunks from list. \"\"\"\n        for i in range(0, len(lst), n):\n            yield lst[i:i + n]\n\n    @staticmethod\n    def _scale_sentiment(sentiments: List[float]):\n        \"\"\" Scale sentiment scores from [-1...1] to [0...1] \"\"\"\n        mm = MinMaxScaler()\n        sent_proc = np.array(sentiments).reshape(-1, 1)\n        return mm.fit_transform(sent_proc)","ad69f1d3":"# Predict signals for all preprocessed data\nfbc = FinBertCased(max_len=MAX_LEN, batch_size=BATCH_SIZE, model_path=MODEL_PATH)\nproc_data.loc[:, 'signal'] = fbc.predict_signals(proc_data['title'])","700c04fb":"print(\"Some of the predictions made by FinBERT:\")\nproc_data[['title', 'ticker', 'signal']].head()","af143e3b":"buy = proc_data[proc_data['signal'] == proc_data['signal'].max()]\nprint(f\"Stock we should buy this week: '{buy['bloomberg_ticker'].item()}'\")\nprint(f\"Signal: {buy['signal'].item()}\")\nprint(f\"\\nNews headlines:\")\nfor i, item in enumerate(buy['title'].item().split(' [SEP] ')[:-1]):\n    print(f\"{i+1}. {item}\")","0ff91a0f":"sell = proc_data[proc_data['signal'] == proc_data['signal'].min()]\nprint(f\"Stock we should sell this week: '{sell['bloomberg_ticker'].item()}'\")\nprint(f\"Signal: {sell['signal'].item()}\")\nprint(f\"\\nNews headlines:\")\nfor i, item in enumerate(sell['title'].item().split(' [SEP] ')[:-1]):\n    print(f\"{i+1}. {item}\")","0516b94a":"plt.figure(figsize=(10, 5))\nplt.title(\"Signal prediction distribution\", weight='bold', fontsize=18)\nplt.xlabel(\"signal\", fontsize=16)\nplt.ylabel(\"Frequency\", fontsize=16)\nproc_data['signal'].plot(kind='hist', bins=100);","99596bd8":"class SignalsSubmit:\n    \"\"\" Submit class for Numerai Signals \"\"\"\n    def __init__(self, test_df: pd.DataFrame):\n        self.test_df = test_df\n        self.test_df.loc[:, \"data_type\"] = \"live\"\n        last_friday = int(str((datetime.now() + relativedelta(weekday=FR(-1))).date()).replace(\"-\", \"\"))\n        self.test_df[\"friday_date\"] = last_friday\n        self.napi = numerapi.SignalsAPI(NMR_PUBLIC_ID, NMR_SECRET_KEY)\n\n    def create_final_csv(self, path: str):\n        \"\"\"\n        Create a csv for submission data\n\n        :param path: Path which will be used to save the csv\n        \"\"\"\n        cols = [\"bloomberg_ticker\", \"friday_date\", \"data_type\", \"signal\"]\n        self.test_df[cols].reset_index(drop=True).to_csv(path, index=False)\n\n    def _upload_predictions(self, model_id: str, path: str):\n        \"\"\"\n        Upload the predictions with Numerapi\n\n        :param path: The path from which to retrieve the CSV submission file\n        :param model_id: Numerai model_id. Retrieved through calling napi.get_models()['MODEL_NAME']\n        \"\"\"\n        print(f\"Submitting for Numerai Signals\")\n        self.napi.upload_predictions(path, model_id=model_id)\n        print(f\"Submission of {path} is done!\")\n\n    def full_submission(self, model_name: str, path: str):\n        \"\"\"\n        Create a final csv and upload the predictions to Numerai\n\n        :param path: The path from which to retrieve the CSV submission file\n        :param model_name: Model name on numerai signals\n        \"\"\"\n        model_id = self.napi.get_models()[model_name]\n        self.create_final_csv(path=path)\n        self._upload_predictions(path=path, model_id=model_id)","bae3221c":"# Submit to Numerai Signals\ns = SignalsSubmit(proc_data)\ns.full_submission(model_name=MODEL_NAME, path=SUB_PATH)","4e8d84e2":"sub = pd.read_csv(SUB_PATH)\nprint(f\"Submission shape: {sub.shape}\")\nprint(\"First 5 rows:\")\nsub.head()","448304d4":"print(\"Last 5 rows:\")\nsub.tail()","36a82666":"This class predicts sentiment score on the aggregated weekly data we have prepared for every ticker. The output of FinBERT is a softmax activation over three classes ('negative', 'neutral' and 'positive'). Our sentiment score will be the probability of positive sentiment minus the probability of negative sentiment. There may be multiple news articles written about a ticker in a given week so we take the mean of all predictions for a ticker.\n\n\n$$Pred(T) = \\frac{1}{n} \\sum_{i=1}^{n} positive_i - negative_i$$\n\nwhere $T$ denotes a specific stock ticker and $n$ the total amount of headlines.\n","9d3ee481":"### Data loading using Stock News API","c818ef17":"### Analysis","3ee93211":"The predicted signal denotes a ranking of all stocks we are considering. Close to 0 means we think the stock will go down in the upcoming week, close to 1 we believe it will go up. This is the signal we submit to Numerai Signals. Let's look at a few of the predictions that the model made.","64cbb676":"### Final Check","6a869ab9":"# [Accompanying Article \u2192](https:\/\/wandb.ai\/carlolepelaars\/finbert-stocknewsapi-numerai-signals\/reports\/Getting-Started-with-Numerai-Signals-Sentiment-Analysis--Vmlldzo0OTg2MDU)","5a78972f":"### FinBERT Inference","1bec1b55":"## Getting started with Numerai Signals: Sentiment Analysis using Stock News API and FinBERT\n\n","64c1a81e":"That's it! I hope this notebook gave you inspiration to start with Numerai Signals and sentiment analysis! \n\nIf you have any questions or feedback, feel free to comment below. You can also contact me on Twitter [@carlolepelaars](https:\/\/twitter.com\/carlolepelaars).","9cd2e304":"This notebook accompanies a [Weights and Biases blog post](https:\/\/wandb.ai\/carlolepelaars\/finbert-stocknewsapi-numerai-signals\/reports\/Getting-Started-with-Numerai-Signals-Sentiment-Analysis--Vmlldzo0OTg2MDU) on [Numerai Signals](https:\/\/signals.numer.ai\/), [Stock News API](https:\/\/stocknewsapi.com\/) and [FinBERT](https:\/\/arxiv.org\/abs\/1908.10063). In this notebook we will show you how to load news articles on several stocks with Stock News API, do sentiment analysis with FinBERT and submit predictions with Numerai's API ([numerapi](https:\/\/github.com\/uuazed\/numerapi)).","2312c5c8":"# [Accompanying Article \u2192](https:\/\/wandb.ai\/carlolepelaars\/finbert-stocknewsapi-numerai-signals\/reports\/Getting-Started-with-Numerai-Signals-Sentiment-Analysis--Vmlldzo0OTg2MDU)","7cc287eb":"Stock News API has news articles on 1000s of stocks. Some may or may not be relevant for the Numerai Signals tournament. We therefore compute the intersection of Stock News API tickers and Numerai Signals Tickers:\n\n$$relevant\\_tickers = stock\\_news\\_tickers \\cap numerai\\_tickers$$","3199045a":"## Preparation","57e06f47":"In order to access Stock News API and the Numerai Signals API we need to set up private keys. \n\n1. To get the Stock News API key sign up [here (14-day free trail)](https:\/\/stocknewsapi.com\/).\n2. To get the Numerai Signals API keys go to [signals.numer.ai\/tournament](https:\/\/signals.numer.ai\/tournament) -> Log in\/Sign up for an account -> select \"Account\" in the top-right -> \"Settings\" -> \"Automation\". From here you can create an API Key for NumerAPI.","fad39e14":"### Data Wrangling","afeb2475":"To prepare the data for sentiment analysis predictions we go through a series of steps. At a high level the following preprocessing operations are performed:\n1. Remove unnecessary columns ('text', 'news_url', 'image_url', 'topics' and 'source_name')\n2. Keep only API calls with the \"Article\" type. \n3. Filter all articles with \"Neutral\" sentiment.\n4. Convert all timestamp to UTC (Coordinated Universal Time) so all rows have a common datetime format.\n\nBy aggregating all news headlines that refer to a ticker we get a row for each ticker and the headlines separated by a $[SEP]$ token, which we will use to batch our news headline input. Lastly, we merge the new DataFrame on the Numerai tickers to retrieve a Bloomberg ticker format that we need later in the submission step.","012527ca":"[![](https:\/\/signals.numer.ai\/homepage-signals\/img\/signals-logo.png)](https:\/\/wandb.ai\/carlolepelaars\/finbert-stocknewsapi-numerai-signals\/reports\/Getting-Started-with-Numerai-Signals-Sentiment-Analysis--Vmlldzo0OTg2MDU)\n\n\n[![](https:\/\/lever-client-logos.s3.amazonaws.com\/bb006941-a5fe-4d4c-b13d-931f9b9c303f-1569362661885.png)](https:\/\/wandb.ai\/carlolepelaars\/finbert-stocknewsapi-numerai-signals\/reports\/Getting-Started-with-Numerai-Signals-Sentiment-Analysis--Vmlldzo0OTg2MDU)","1409522f":"Now that we have the predictions, we are almost ready to submit to Numerai Signals. To finalize the DataFrame a column is added indicating that the predictions are for \"live\" data (e.g. the upcoming week). Also, we make a date column denoting the upcoming Friday. Lastly, the DataFrame is written to CSV and uploaded using Numerai's API. ","64989ec8":"### Submission"}}