{"cell_type":{"d351062f":"code","546734ed":"code","88150acd":"code","bdf701c9":"code","e0e19437":"code","c19e06ad":"code","ee20c3d7":"code","2d6df8a5":"code","c33b684f":"code","9815b37e":"markdown","7d210a93":"markdown","ec55f232":"markdown","dc423a4f":"markdown"},"source":{"d351062f":"import os\nimport pandas as pd\nimport numpy as np\nimport random as rn\nfrom datetime import datetime\nfrom tensorflow.keras.layers import (Conv1D, MaxPooling1D, Dense, GlobalMaxPooling1D, \n                                     Flatten, TimeDistributed, LSTM, Input, \n                                     Bidirectional, Multiply,\n                                     Add, concatenate, Dropout)\nfrom tensorflow.compat.v1.keras.layers import CuDNNLSTM\nimport tensorflow as tf\n#import tensorflow_addons as tfa\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau, LearningRateScheduler\nfrom tensorflow.keras import losses, models, optimizers\nfrom tensorflow.keras.initializers import RandomUniform, Orthogonal\nimport tensorflow.keras.backend as K\n\nfrom plotnine import * #ggplot functionality\nfrom typing import Optional, List\n\nfrom sklearn.utils.class_weight import compute_sample_weight\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\n#tf.config.set_visible_devices([], 'GPU')\n#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n\"\"\"\nThis notebook implements a stacked WaveNet-LSTM architecture that seems to have an upper \nperformance threshold of 0.941 regardless of structure changes. Because of this I have added \na section on possible further work; topics such as switching losses part way through and \nconsidering more of a many to one or many to many approach but only taking middle predictions\n\nThanks to siavrez for the Wavenet concept and residual block \nhttps:\/\/www.kaggle.com\/siavrez\/wavenet-keras\n\nThanks to Chris Deotte for removing drift from the datasets \nhttps:\/\/www.kaggle.com\/cdeotte\/data-without-drift\n\nI'd be more than happy to discuss different strategies, look at any possible mistakes, \nor help walk anyone through the code\n\"\"\"\n\n\ntrain = pd.read_csv('\/kaggle\/input\/data-without-drift\/train_clean.csv', \n                    dtype={'time': np.float32, 'signal': np.float32, 'open_channels':np.int32})\n\ntest  = pd.read_csv('\/kaggle\/input\/data-without-drift\/test_clean.csv', \n                    dtype={'time': np.float32, 'signal': np.float32})\n","546734ed":"class WaveModeler:\n    \"\"\"\n    The WaveModeler is designed to set up the data and network upon instantiation. \n    Passing in only the datasets and hyperparameters. Other methods can then be called \n    to fit the model and get different datasets\/result.\n    \n    The implemented network is a stacked Wavenet for feature engineering follow by an LSTM(s). \n    The output of each WaveNet is concatenated with the original raw signal and passed to the LSTM(s). \n    This allows for multiple different kernel sizes\/architectures.\n    \n    \"\"\"\n    def __init__(self, train : pd.DataFrame, test : pd.DataFrame, \n                 group_size : int, \n                 features : List[str],\n                 num_filters : List[int], kernel_size : List[int], \n                 repeats : List[int], dilations : List[int], densers : List[int], \n                 LSTMs : List[int],\n                 learning_rate : float, metrics : list,\n                 the_loss = tf.keras.losses.CategoricalCrossentropy(),\n                 normalize = True):\n        \"\"\"\n    \n        Parameters\n        ----------\n        train : pd.DataFrame\n            training dataset\n        test : pd.DataFrame\n            testing dataset\n        group_size : int\n            The length of each sample fed into the model. \n            for the training data this would be (# of groups, group_size, # of predictors). \n            Current code requires sample size to be divisible by group_size\n        features : List[str]\n            string list of the features used for predicting\n        num_filters : List[int]\n            integer list of the # of filters for each Wavenet\n        kernel_size : List[int]\n            integer list of the kernal size for each Wavenet\n        repeats : List[int]\n            integer list of the amount of times to repeat each Wavenet\n        dilations : List[int]\n            integer list of the # of stacked layers for each Wavenet\n        densers : List[int]\n            integer list of the # of units for a Dense layer following each Wavenet \n            (0 for no Dense layer)\n        LSTMs : List[int]\n            integer list of the units for each LSTM\n        learning_rate : float\n            starting learning rate for the network\n        metrics : list\n            list of tensorflow metrics for the model to monitor\n        the_loss : TYPE, optional\n            the loss funciton of the network for compilation. \n            The default is tf.keras.losses.CategoricalCrossentropy().\n        normalize : TYPE, optional\n            Whether or not to standardize the data. The default is True.\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n        self.train = train\n        self.test = test\n        self.normalize = normalize\n        self.group_size = group_size\n        self.features = features\n        #Set up the data for modeling\n        means = []\n        sds = []\n        for i in self.features:\n            means.append(np.mean(self.train[i]))\n            sds.append(np.std(self.train[i]))\n            \n        self.trainX, self.trainy = self.__SetupData(self.train, means, \n                                                    sds, normalize = self.normalize)\n        self.testX = self.__SetupData(self.test, means, \n                                      sds, normalize = self.normalize, \n                                      response = False)\n        \n        \n        #set up the model\n        inp = Input(shape=(self.group_size, self.trainX.shape[2]))\n        \n        #get the first wavenet\n        x = Conv1D(num_filters[0], 1, padding='same')(inp)\n        x = self.__WaveNetResidualConv1D(num_filters[0], kernel_size[0], dilations[0])(x)\n        \n        if repeats[0] > 0:\n            for _ in range(repeats[0]):\n                x = Conv1D(num_filters[0], 1, padding='same')(x)\n                x = self.__WaveNetResidualConv1D(num_filters[0],  kernel_size[0], dilations[0])(x)\n        if densers[0] > 0:\n            x = Dense(densers[0], activation = 'relu')(x)\n            \n        #concatenate all other wavenets to the first wavenet\n        for i in range(len(kernel_size) - 1):\n            \n            x1 = Conv1D(num_filters[i + 1], 1, padding='same')(inp)\n            x1 = self.__WaveNetResidualConv1D(num_filters[i + 1],  kernel_size[i + 1], dilations[i + 1])(x1)\n            \n            if repeats[i + 1] > 0:\n                for _ in range(repeats[i+1]):\n                    x1 = Conv1D(num_filters[i + 1], 1, padding='same')(x1)\n                    x1 = self.__WaveNetResidualConv1D(num_filters[i + 1],  kernel_size[i + 1], dilations[i + 1])(x1)\n            \n            if densers[i + 1] > 0:\n                x1 = Dense(densers[i + 1], activation = 'relu')(x1)\n            x = concatenate([x, x1])\n        #concatenate the signal\n        x = concatenate([x, inp])        \n        #finish it off with an LSTM\n        for lstm_size in LSTMs:      \n            x = Bidirectional(CuDNNLSTM(lstm_size, return_sequences = True))(x)\n\n        out = Dense(self.trainy.shape[2], activation='softmax')(x)\n        self.model = models.Model(inputs=[inp], outputs=[out])\n        opt = Adam(lr=learning_rate)\n        \n        #opt = tfa.optimizers.SWA(opt, start_averaging = 30, average_period = 5)\n        self.model.compile(loss=the_loss, optimizer=opt, metrics=metrics)\n\n\n    def __SetupData(self, df : pd.DataFrame, means : Optional[List[float]], \n                    sds : Optional[List[float]], normalize : bool, response = True):\n        \"\"\"\n        A private method designed to clean up the data for modeling\n\n        Parameters\n        ----------\n        df : pd.DataFrame\n            dataset to be cleaned for modeling\n        means : Optional[List[float]]\n            list of means for each predictor\n        sds : Optional[List[float]]\n            list of means for each predictor\n        normalize : bool\n            whether or not to standardize\n        response : TYPE, optional\n            whether or not to create y. The default is True.\n\n        Returns\n        -------\n        \n        X : np.array\n            predictor matrix to be fed into tf.keras\n        y : np.array, optional\n            if response then the response variable is one-hot encoded and returned\n        \"\"\"\n        if self.normalize:\n            for i, feats in enumerate(self.features):\n                df[feats] = (df[feats] - means[i])\/sds[i]\n            \n        #create \n        df['group'] = df.groupby(df.index\/\/self.group_size, \n                                 sort=False)['signal'].agg(['ngroup'])\n        self.ngroups = len(np.unique(df['group']))\n\n        X = np.array(list(df.groupby('group').apply(lambda x: x[self.features].values)))\n        print(f'The data shape is {X.shape}')\n        if response:\n            y = pd.concat([pd.get_dummies(df['open_channels']), \n                           df['group']], axis = 1) #get the dummies\n            y = np.array([gr.drop(columns = 'group').values for _, gr in y.groupby('group')]).astype(np.float32)\n            print(f'The data shape is {y.shape}')\n            return X, y\n    \n        return X\n    \n    #this private function was taken from https:\/\/www.kaggle.com\/siavrez\/wavenet-keras\n    def __WaveNetResidualConv1D(self, num_filters : int, kernel_size : int, stacked_layer : int):\n        \"\"\"\n        Private function designed to create the residual block for the Wavenet\n\n        Parameters\n        ----------\n        num_filters : int\n            number of filters.\n        kernel_size : int\n            the kernel size.\n        stacked_layer : int\n            the amount of stacked layers (dilations).\n\n        Returns\n        -------\n        TYPE\n            the created residual block.\n\n        \"\"\"\n        def build_residual_block(l_input):\n            resid_input = l_input\n            for dilation_rate in [2**i for i in range(stacked_layer)]:\n                l_sigmoid_conv1d = Conv1D(\n                  num_filters, kernel_size, dilation_rate=dilation_rate,\n                  padding='same', activation='sigmoid')(l_input)\n                l_tanh_conv1d = Conv1D(\n                 num_filters, kernel_size, dilation_rate=dilation_rate,\n                 padding='same', activation='relu')(l_input)\n                l_input = Multiply()([l_sigmoid_conv1d, l_tanh_conv1d])\n                l_input = Conv1D(num_filters, 1, padding='same')(l_input)\n                resid_input = Add()([resid_input ,l_input])\n            return resid_input\n        return build_residual_block\n    \n    def fit_model(self, nepochs : int, batch_size : int, callbacks : list, \n                  fit_type : str, split_size = 0.1, seed = 412, **kwargs):\n        \"\"\"\n        Used to fit the model. Allowing the user to specify the fitting strategy. \n        Whether that be train, train\/val split or cross validation (not yet implemented)\n\n        Parameters\n        ----------\n        nepochs : int\n            number of epochs to run for.\n        batch_size : int\n            batch size.\n        callbacks : list\n            any callbacks wanted.\n        fit_type : str\n            train for training on entire train dataset, cv for cross validation, \n            and otherwise it will do a train\/val split.\n        split_size : TYPE, optional\n            split size if fit_type is train\/val. The default is 0.1.\n        verbose : TYPE, optional\n            verbosity of the model fit. The default is 2.\n        seed : TYPE, optional\n            seed for the train\/val split. The default is 412.\n\n        Returns\n        -------\n        None.\n\n        \"\"\"\n        \n        if fit_type == 'train':\n          self.history = self.model.fit(self.trainX, self.trainy, \n                                      epochs = nepochs,\n                                      batch_size = batch_size,\n                                      callbacks = callbacks,\n                                      **kwargs)\n        elif fit_type == 'cv':\n          #create cross validated splits to loop through and output results\n          print('Not Currently Available')\n          \n        else:\n            try:\n                self.train_data\n            except AttributeError:\n                self.train_data, self.val_data, self.train_datay, self.val_datay = train_test_split(self.trainX, self.trainy, \n                                                                                                    test_size=split_size, \n                                                                                                    random_state=seed)\n        \n            self.history = self.model.fit(self.train_data, self.train_datay, \n                                      epochs = nepochs,\n                                      validation_data = (self.val_data, self.val_datay),\n                                      batch_size = batch_size,\n                                      callbacks = callbacks,\n                                      **kwargs)\n    \n    #Get Methods\n    def get_train_preds(self, **kwargs):\n        \"\"\"\n        Get predictions on the entire training dataset\n\n        Parameters\n        ----------\n        **kwargs : TYPE\n            any extra paremeters to be fed to tf.keras predict functionality.\n\n        Returns\n        -------\n        TYPE\n            model predictions.\n\n        \"\"\"\n        return self.model.predict(self.trainX, **kwargs)\n    \n    def get_test_preds(self, **kwargs):\n        \"\"\"\n        Get prdictions on the test dataset\n\n        Parameters\n        ----------\n        **kwargs : TYPE\n            any extra paremeters to be fed to tf.keras predict functionality.\n\n        Returns\n        -------\n        TYPE\n            model predictions.\n\n        \"\"\"\n        return self.model.predict(self.testX, **kwargs)\n    \n    def get_train_data(self):\n        \"\"\"\n        Get the training data if train\/val split was used\n        \n        Returns\n        -------\n        The predictor matrix along with the response one-hot matrix\n        \"\"\"\n        return self.train_data, self.train_datay\n    \n    def get_val_data(self):\n        \"\"\"\n        Get the validation data if train\/val split was used\n        \n        Returns\n        -------\n        The predictor matrix along with the response one-hot matrix\n        \"\"\"\n        return self.val_data, self.val_datay\n    \n    def get_model(self):\n        \"\"\"\n        Get the model\n\n        Returns\n        -------\n        tf.keras model\n\n        \"\"\"\n        return self.model\n    \n    def get_history(self):\n        \"\"\"\n        Get the history from the latest model fit\n\n        Returns\n        -------\n        tf.keras history.\n\n        \"\"\"\n        return self.history\n    \n    def get_trainX(self):\n        \"\"\"\n        Get the full training predictor matrix\n\n        Returns\n        -------\n        predictor matrix.\n\n        \"\"\"\n        return self.trainX\n    \n    def get_testX(self):\n        \"\"\"\n        Get the full test predictor matrix\n\n        Returns\n        -------\n        predictor matrix\n\n        \"\"\"\n        return self.testX\n\n\n\n\"\"\"\nA weighted version of categorical_crossentropy for keras (2.0.6). \nThis lets you apply a weight to unbalanced classes.\n@url: https:\/\/gist.github.com\/wassname\/ce364fddfc8a025bfab4348cf5de852d\n@author: wassname\n\"\"\"\ndef weighted_categorical_crossentropy(weights):\n    \"\"\"\n    A weighted version of keras.objectives.categorical_crossentropy\n    \n    Variables:\n        weights: numpy array of shape (C,) where C is the number of classes\n    \n    Usage:\n        weights = np.array([0.5,2,10]) # Class one at 0.5\n        loss = weighted_categorical_crossentropy(weights)\n        model.compile(loss=loss,optimizer='adam')\n    \"\"\"\n    \n    weights = K.variable(weights)\n        \n    def loss(y_true, y_pred):\n        # scale predictions so that the class probas of each sample sum to 1\n        y_pred \/= K.sum(y_pred, axis=-1, keepdims=True)\n        # clip to prevent NaN's and Inf's\n        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n        # calc\n        loss = y_true * K.log(y_pred) * weights\n        loss = -K.sum(loss, -1)\n        return loss\n    \n    return loss","88150acd":"#seeding\nthe_seed = 7456\ntf.random.set_seed(the_seed)\nnp.random.seed(the_seed)\nrn.seed(the_seed)\nos.environ['PYTHONHASHSEED'] = str(the_seed)\n\n#setting up the session\nK.clear_session()\nconfig = tf.compat.v1.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = tf.compat.v1.Session(config=config)\ntf.compat.v1.keras.backend.set_session(session)\n\n#compute weights based on class frequency\n#sample_weights = compute_sample_weight(class_weight='balanced', y=train.open_channels)\nfrequencies = train.open_channels.value_counts()\nwgts = (frequencies.mean()\/frequencies).values\n\nlearning_rate = 0.001\n#Instantiate a model instance\nmodeler = WaveModeler(train, test, 2000, ['signal'],\n                      num_filters = [12, 12, 12, 12],\n                      kernel_size = [24, 6, 3, 1],\n                      repeats = [0, 0, 0, 1],\n                      dilations =  [8, 8, 8, 8],\n                      densers = [4, 4, 4, 4],\n                      LSTMs = [256, 256],\n                      learning_rate = learning_rate,\n                      the_loss = weighted_categorical_crossentropy(wgts),\n                      metrics = [tf.keras.metrics.Precision(name='precision'),\n                                 tf.keras.metrics.Recall(name='recall')])\n\n#modeler.get_model().summary()\n\n\"\"\"\nIn my experience the LSTM's struggled to generalize better to drifted data.\nIf the goal is to have a similar structure generalize in such a manner \nperhaps it would be better to switch over to Dense layer(s) \nand rely more heavily on the WaveNets\n\"\"\"","bdf701c9":"#Define the Keras LR Scheduler Callback\ndef lrs(epoch):\n    if epoch<10:\n        lr = learning_rate\n    else:\n        lr = learning_rate\/2\n    return lr\n\n\nlr_schedule = LearningRateScheduler(lrs)\n\n#Define the Keras Checkpoint Callback\nfilepath = '\/kaggle\/input\/StackedWavenetLSTM.hdf5'\nsave_checkpoint = ModelCheckpoint(filepath, monitor='val_recall', verbose=1, \\\n                             save_best_only=True, save_weights_only=False, \\\n                             mode='max', period=1)\n\n# Define the Keras TensorBoard callback.\nlogdir=os.path.join(\n    \"logs\",\n    \"fit\",\n    datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n)\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n\n\n\nmodeler.fit_model(nepochs = 25, batch_size = 10,\n                  fit_type = 'train_val',\n                  split_size = 0.1,\n                  callbacks = [lr_schedule, save_checkpoint, tensorboard_callback],\n                  verbose = 0)\n#K.set_value(model.optimizer.lr, np.float32(0.0005))\n\n#%load_ext tensorboard\n#%tensorboard --logdir logs\n#http:\/\/localhost:6006\/","e0e19437":"#getting different models\nmodel = modeler.get_model()\nmodel = tf.keras.models.load_model('\/kaggle\/input\/StackedWavenetLSTM.hdf5', \n                                   custom_objects = {'loss' : weighted_categorical_crossentropy(wgts)})\n#model = tf.keras.models.load_model('StackedWavenetLSTM_softF1.hdf5', \n#                                   custom_objects = {'macro_soft_f1' : macro_soft_f1})\n\n#can also call modeler.get_train_predictions(batch_size = 1) to get prediction on the latest fit\ntrainX = modeler.get_trainX()\ntestX = modeler.get_testX()\ntrainPreds = model.predict(trainX, batch_size = 1)#lowering batchsize for memory issues\ntestPreds = model.predict(testX, batch_size = 1)\n#np.save('latest_train_preds.npy', trainPreds)\n#np.save('latest_test_preds.npy', testPreds)\n\n\ntrain['preds'] = np.argmax(trainPreds, axis=2).reshape(-1)\ntest['preds'] = np.argmax(testPreds, axis=2).reshape(-1)\n\nggplot(train.sample(50000)) + geom_point(aes(x = 'time', y = 'signal', color = 'preds'))\nggplot(test.sample(n = 50000)) + geom_point(aes(x = 'time', y = 'signal', color = 'preds'))","c19e06ad":"#check F1 Scores\nf1_score(train['open_channels'],  train['preds'], average = 'macro')\nf1_score(train['open_channels'],  train['preds'], average = None)\n#performance falls off above 5 channels\n\n\n#make predictions file\nsub  = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/sample_submission.csv', dtype={'time': str})\nsub.open_channels = np.argmax(testPreds, axis=2).reshape(-1)\nsub.to_csv('\/kaggle\/input\/final_waveNetLSTM_softF1.csv', index = False)","ee20c3d7":"model = tf.keras.models.load_model('\/kaggle\/input\/StackedWavenetLSTM.hdf5', \n                                   custom_objects = {'loss' : weighted_categorical_crossentropy(wgts)})\n\n\n#loss function taken from Mohamed-Achref Maiza's post on soft f1 loss\n#https:\/\/towardsdatascience.com\/the-unknown-benefits-of-using-a-soft-f1-loss-in-classification-systems-753902c0105d\n\n#It can be beneficial to recompile the model using soft f1 and further optimizing the network \n#after crossentropy given Macro F1 is the evaluation metric\n#may be valuable to consider a higher batch size with this loss\ndef macro_soft_f1(y, y_hat):\n    \"\"\"Compute the macro soft F1-score as a cost.\n    Average (1 - soft-F1) across all labels.\n    Use probability values instead of binary predictions.\n    \n    Args:\n        y (int32 Tensor): targets array of shape (BATCH_SIZE, N_LABELS)\n        y_hat (float32 Tensor): probability matrix of shape (BATCH_SIZE, N_LABELS)\n        \n    Returns:\n        cost (scalar Tensor): value of the cost function for the batch\n    \"\"\"\n    \n    y = tf.cast(y, tf.float32)\n    y_hat = tf.cast(y_hat, tf.float32)\n    tp = tf.reduce_sum(y_hat * y, axis=0)\n    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n    soft_f1 = 2*tp \/ (2*tp + fn + fp + 1e-16)\n    cost = 1 - soft_f1 # reduce 1 - soft-f1 in order to increase soft-f1\n    macro_cost = tf.reduce_mean(cost) # average on all labels\n    \n    return macro_cost\n\n\nmodel.compile(loss=macro_soft_f1, \n              optimizer=Adam(lr=learning_rate\/2), \n              metrics= [tf.keras.metrics.Precision(name='precision'),\n                        tf.keras.metrics.Recall(name='recall')])\n\n#to run the model outside of the class pull out the needed data\ntrain_data, train_datay = modeler.get_train_data()\nval_data, val_datay = modeler.get_val_data()\n\nfilepath = '\/kaggle\/input\/StackedWavenetLSTM_softF1.hdf5'\nsave_checkpoint = ModelCheckpoint(filepath, monitor='val_recall', verbose=1, \\\n                             save_best_only=True, save_weights_only=False, \\\n                             mode='max', period=1)\n\n\n\n\nhistory = model.fit(train_data, train_datay, \n                                      epochs = 10,\n                                      validation_data = (val_data, val_datay),\n                                      batch_size = 10,\n                                      callbacks = [save_checkpoint],\n                                      verbose = 1)","2d6df8a5":"#visualize where the mistakes were\nbad = train.query('open_channels != preds')\nggplot(bad) + geom_point(aes(x = 'time', y = 'signal', color = 'preds'))","c33b684f":"#Look into if a many to one strategy may be better\ntrain['groupings'] = np.repeat(np.arange(0, train.shape[0]\/\/2000), 2000)\nfirst100 = train.groupby('groupings', as_index=False).head(10)\nlast100 =  train.groupby('groupings', as_index=False).tail(10)\nmiddle100 = train.groupby('groupings', as_index=False).apply( lambda df : df.iloc[950:1050])\n\nf1_score(middle100['open_channels'],  middle100['preds'], average = 'macro')\nf1_score(middle100['open_channels'],  middle100['preds'], average = None)\n\n","9815b37e":"![mistakes.png](attachment:mistakes.png)\nIt becomes rather apparent as shown in the F1 score that the peak groups are where most mistakes happen. Perhaps it would also makes sense to handle extreme outliers in some groups that are most likely measurement errors","7d210a93":"![trainPreds.png](attachment:trainPreds.png)\n![testPreds.png](attachment:testPreds.png)","ec55f232":"Using StackedWavenetLSTM_softF1, Macro F1 was calculated on the first 100 of each sample, the middle 100, and then the last 100.\n\n* First F1  : 0.93511\n* Middle F1 : 0.93984\n* Last F1   : 0.93793\n\nLooking at just the last 50 seconds (one of the peak groups) this issue becomes much more apparent  \n  \n  \n\n* First F1  : 0.77191  \n* Middle F1 : 0.79262  \n* Last F1   : 0.75379  ","dc423a4f":"# **Possible Further Work**\n\nI did not dig too deep into many of these concepts below however they seem worth looking into. First the Macro Soft F1 loss performed poorly (possibly due to my batch size) unless categorical crossentropy first trained the weights however it shows promise in getting slightly better performance as its focus is on a modified Macro F1. Visualizing mistakes also offers some insight as does checking F1 performance at different points of a sample "}}