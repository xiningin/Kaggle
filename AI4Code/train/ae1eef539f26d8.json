{"cell_type":{"646696a9":"code","b1e0f3cb":"code","e37ca887":"code","b300b647":"code","e79df343":"code","3c9625bd":"code","b15c6267":"code","d3108c8c":"code","412344ca":"code","5c15caec":"code","b24c9322":"code","3de54b55":"code","e8e6a247":"code","1ab3a41e":"code","3e57f561":"code","d000ba21":"code","548404ab":"code","875c2724":"code","d65cd2ef":"markdown","954832c6":"markdown","afe4bef9":"markdown","de11eb97":"markdown","279df4ba":"markdown","49601e8c":"markdown","04bbe0cd":"markdown","a4da014b":"markdown","74b00fdf":"markdown","ae12d8ba":"markdown","9269afe9":"markdown","375516f1":"markdown","a3f0233d":"markdown","58d67a39":"markdown","440d003a":"markdown","a1837065":"markdown","c4580fcf":"markdown","eb3e36b2":"markdown","08794804":"markdown","a346f3da":"markdown"},"source":{"646696a9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir(\"..\/input\/glove-global-vectors-for-word-representation\"))\nprint(os.listdir(\"..\/input\/jigsaw-unintended-bias-in-toxicity-classification\"))\n\n# Any results you write to the current directory are saved as output.","b1e0f3cb":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport datetime\nimport os\nimport pandas as pd\nimport numpy as np\nimport pkg_resources\nimport seaborn as sns\nimport time\nimport scipy.stats as stats\n\nfrom sklearn import metrics\nfrom sklearn import model_selection\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding\nfrom keras.layers import Input\nfrom keras.layers import Conv1D\nfrom keras.layers import MaxPooling1D\nfrom keras.layers import Flatten\nfrom keras.layers import Dropout\nfrom keras.layers import Dense\nfrom keras.optimizers import RMSprop\nfrom keras.models import Model\nfrom keras.models import load_model","e37ca887":"# Plotly imports\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\n# Other imports\nfrom collections import Counter\nfrom scipy.misc import imread\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom tqdm import tqdm","b300b647":"train = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\nprint('loaded %d records' % len(train))","e79df343":"# Make sure all comment_text values are strings\ntrain['comment_text'] = train['comment_text'].astype(str) \n\n# List all identities\nidentity_columns = [\n    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n    'muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n\n# Convert taget and identity columns to booleans\ndef convert_to_bool(df, col_name):\n    df[col_name] = np.where(df[col_name] >= 0.5, True, False)\n    \ndef convert_dataframe_to_bool(df):\n    bool_df = df.copy()\n    for col in ['target'] + identity_columns:\n        convert_to_bool(bool_df, col)\n    return bool_df\n\ntrain = convert_dataframe_to_bool(train)","3c9625bd":"# Make a python dictionary with unique words and their respective word counts\ndef make_dict(words):\n    counts = dict()\n\n    for word in words:\n        if word in counts:\n            counts[word] += 1\n        else:\n            counts[word] = 1\n\n    return counts","b15c6267":"# Ignore these stopwords\nstopwords = nltk.corpus.stopwords.words('english')\nprint(\"example stopwords\", stopwords[0:10])\nlemm = WordNetLemmatizer()\nprint(\"The lemmatized form of leaves is: {}\".format(lemm.lemmatize(\"leaves\")))","d3108c8c":"def delta_cond(cond1, cond2, cond1_name, cond2_name) :\n    # make them all lowercase, and remove the stopwords\n    scratch = train[cond1][\"comment_text\"].values\n    scratch = \" \".join(scratch)\n    scratch = scratch.split()\n    # print(\"remove stopwords and non-alphabetic then make everything lowercase and Lemmatize the results\")\n    cond1_words = [lemm.lemmatize(word.lower()) for word in scratch if (word.lower() not in stopwords) & word.isalpha()]\n    del scratch\n    # print(\"make a dictionary of unique ocurrances, and their counts\")\n    cond1_words = make_dict(cond1_words)\n    \n    # make them all lowercase, and remove the stopwords\n    scratch = train[cond2][\"comment_text\"].values\n    scratch = \" \".join(scratch)\n    scratch = scratch.split()\n    # print(\"remove stopwords and non-alphabetic then make everything lowercase and Lemmatize the results\")\n    cond2_words = [lemm.lemmatize(word.lower()) for word in scratch if (word.lower() not in stopwords) & word.isalpha()]\n    del scratch\n    # print(\"make a dictionary of unique ocurrances, and their counts\")\n    cond2_words = make_dict(cond2_words)\n    \n    # create a python dictionary of the words in one dictionary but not the other\n    cond1_not_cond2 = { k : cond1_words[k] for k in set(cond1_words) - set(cond2_words) }\n    cond2_not_cond1 = { k : cond2_words[k] for k in set(cond2_words) - set(cond1_words) }\n    print(\"\",len(cond1_words),\"Words found in\", cond1_name,\"\\n\", len(cond2_words), \n          \"Words found in\", cond2_name)\n    print(\"\",len(cond1_not_cond2),\"Words found Only in\",cond1_name,\"and not in\",cond2_name,\"\\n\",\n          len(cond2_not_cond1), \"Words found Only in\",cond2_name,\"and not in\",cond1_name,\"\\n\")\n    del cond1_words\n    del cond2_words\n    \n    title = \"Words found Only in \" + cond1_name + \" and not in \" + cond2_name\n    plt.figure(figsize=(16,13))\n    wc = WordCloud(background_color=\"black\", max_font_size= 40)\n    wc.generate_from_frequencies(cond1_not_cond2)     \n    plt.title(title, fontsize=20)\n    plt.imshow(wc.recolor( colormap= 'Pastel2' , random_state=17), alpha=0.98)\n    plt.axis('off')\n\n    title = \"Words found Only in \" + cond2_name + \" and not in \" + cond1_name\n    plt.figure(figsize=(16,13))\n    wc = WordCloud(background_color=\"black\", max_font_size= 40)\n    wc.generate_from_frequencies(cond2_not_cond1)     \n    plt.title(title, fontsize=20)\n    plt.imshow(wc.recolor( colormap= 'Pastel2' , random_state=17), alpha=0.98)\n    plt.axis('off')\n    \n    del cond1_not_cond2\n    del cond2_not_cond1\n    del title\n    del wc\n    \n    return ","412344ca":"c1 = ((train.target==True))\nc1_name = \"TOXIC\"\nc2 = ((train.target==False))\nc2_name = \"NOT TOXIC\"\ndelta_cond(c1, c2, c1_name, c2_name)","5c15caec":"c1 = ((train.target==True) & (train.black==True))\nc1_name = \"(toxic TRUE) + (black TRUE)\"\nc2 = ((train.target==False) & (train.black==True))\nc2_name = \"(toxic FALSE) + (black TRUE)\"\ndelta_cond(c1, c2, c1_name, c2_name)","b24c9322":"c1 = ((train.target==True) & (train.white==True))\nc1_name = \"(toxic TRUE) + (white TRUE)\"\nc2 = ((train.target==False) & (train.white==True))\nc2_name = \"(toxic FALSE) + (white TRUE)\"\ndelta_cond(c1, c2, c1_name, c2_name)","3de54b55":"c1 = ((train.target==True) & (train.homosexual_gay_or_lesbian==True))\nc1_name = \"(toxic TRUE) + (homosexual_gay_or_lesbian TRUE)\"\nc2 = ((train.target==False) & (train.homosexual_gay_or_lesbian==True))\nc2_name = \"(toxic FALSE) + (homosexual_gay_or_lesbian TRUE)\"\ndelta_cond(c1, c2, c1_name, c2_name)","e8e6a247":"c1 = ((train.target==False) & (train.homosexual_gay_or_lesbian==True))\nc1_name = \"(toxic FALSE) + (homosexual_gay_or_lesbian TRUE)\"\nc2 = ((train.target==True) & (train.homosexual_gay_or_lesbian==False))\nc2_name = \"(toxic TRUE) + (homosexual_gay_or_lesbian FALSE)\"\ndelta_cond(c1, c2, c1_name, c2_name)","1ab3a41e":"c1 = ((train.target==False) & (train.black==True))\nc1_name = \"(toxic FALSE) + (black TRUE)\"\nc2 = ((train.target==True) & (train.black==False))\nc2_name = \"(toxic TRUE) + (black FALSE)\"\ndelta_cond(c1, c2, c1_name, c2_name)","3e57f561":"c1 = ((train.target==False) & (train.white==True))\nc1_name = \"(toxic FALSE) + (white TRUE)\"\nc2 = ((train.target==True) & (train.white==False))\nc2_name = \"(toxic TRUE) + (white FALSE)\"\ndelta_cond(c1, c2, c1_name, c2_name)","d000ba21":"c1 = ((train.target==True) & (train.christian==True))\nc1_name = \"(toxic TRUE) + (christian TRUE)\"\nc2 = ((train.target==False) & (train.christian==False))\nc2_name = \"(toxic FALSE) + (christian FALSE)\"\ndelta_cond(c1, c2, c1_name, c2_name)","548404ab":"c1 = ((train.target==True) & (train.jewish==True))\nc1_name = \"(toxic TRUE) + (jewish TRUE)\"\nc2 = ((train.target==False) & (train.jewish==False))\nc2_name = \"(toxic FALSE) + (jewish FALSE)\"\ndelta_cond(c1, c2, c1_name, c2_name)","875c2724":"c1 = ((train.target==True) & (train.female==True))\nc1_name = \"(toxic TRUE) + (female TRUE)\"\nc2 = ((train.target==False) & (train.female==False))\nc2_name = \"(toxic FALSE) + (female FALSE)\"\ndelta_cond(c1, c2, c1_name, c2_name)","d65cd2ef":"# Poorly scored AUC Subgroups\n## BPSN (Background Positive, Subgroup Negative) AUC: - homosexual_gay_or_lesbian\n Here, we restrict the test set to the non-toxic examples that mention the identity and the toxic examples that do not. A low value in this metric means that the model confuses non-toxic examples that mention the identity with toxic examples that do not, likely meaning that the model predicts higher toxicity scores than it should for non-toxic examples mentioning the identity.\n\nIn a prior iteration of this Kernel, the UAC for specific subgroups scored the lowest, meaning that comment texts that were identified as having to do with those subgroups were poorly classified as being toxic or not toxic. Let's look at the word clouds for words only in the toxic and only in the not toxic comment texts relating to those subgroups. \n\nThe three most poorly scored subgroups are black, homosexual_gay_or_lesbian, and white","954832c6":"# Will not re-run \"Benchmark Kernel\" \n## Because, combined with word clouds, it overruns time limit\n## but here is the results (apologies for the non-tabular view)\n\n* ____bnsp_auc___bpsn_auc___subgroup\t\t\t\t\t\tsubgroup_auc___subgroup_size\n* 6___0.956725___0.766774___black___________________________0.801189_______2890\n* 2___0.950133___0.786869___homosexual_gay_or_lesbian_______0.803491_______2250\n* 5___0.947570___0.803467___muslim__________________________0.807359_______4210\n* 7___0.960967___0.769693___white___________________________0.816946_______4931\n* 4___0.922565___0.856830___jewish__________________________0.830503_______1554\n* 8___0.953574___0.838308___psychiatric_or_mental_illness___0.872289_______959\n* 1___0.939502___0.871237___female__________________________0.877651_______10710\n* 0___0.946130___0.862624___male____________________________0.881211_______8861\n* 3___0.915157___0.913734___christian_______________________0.892654_______8052","afe4bef9":"# Poorly scored AUC Subgroups\n## Subgroup AUC: \nHere, we restrict the data set to only the examples that mention the specific identity subgroup. A low value in this metric means the model does a poor job of distinguishing between toxic and non-toxic comments that mention the identity.\n\nIn a prior iteration of this Kernel, the AUC for specific subgroups scored the lowest, meaning that comment texts that were identified as having to do with those subgroups were poorly classified as being toxic or not toxic. Let's look at the word clouds for words only in the toxic and only in the not toxic comment texts relating to those subgroups. \n\nThe three most poorly scored subgroups are black, white, and homosexual_gay_or_lesbian","de11eb97":"## Load and pre-process the data set","279df4ba":"## BNSP: Female","49601e8c":"## What to look for in the below\nFirst, you should perform an overall sanity check. Look at the pair of wordclouds. Could you, as a human reader, pick which cloud was which from the pair of subdivided data? Many times the answer is \"yes\" but some pairs are more obvious than others.\n\nNext, look at the specifics of the pairs:\n* Are the words in the wordclouds, especially the larger font words, really representative of words that you would expect to see only in one subset and not the other? Why or why not?\n* If you really don't like a bunch of the words, imagine that you (or machine learning) removes them from the list. Do the word counts justify enough remaining words to do the classification?\n* What other techniques might you use besides these \"subtracted dictionaries\" to subdivide the space more accurately? \n* LSTM and other recurrent time varying models focus on the sequence of words, not just the individual words. Even a Hidden Markov Model could be used as a probability sequencer. How might you sanity check the \"sequence dictionaries\" just as the wordclouds helped you sanity check the word subsets.","04bbe0cd":"## BNSP: Jewish","a4da014b":"## Subgroup AUC: - White","74b00fdf":"# A Thought Provoking Kernel for your Competition Consideration\n## Introduction\nIn April of 2019 legislators in the United States introduced a bill called the \"Algorithmic Accountability Act of 2019\" (AAA19). This Kernel seeks to understand the contents of that proposed new law in the context of this data set and goals, and possibly provide an advantage in winning the competition.\n\nOne of many news articles about the bill can be found here: https:\/\/techcrunch.com\/2019\/04\/10\/algorithmic-accountability-act\/\n## Details of the proposal\n(Disclaimer: These are my interpretations only, please read it for yourself). The proposal is based on the fact that there will be an Impact Assessment on every automated decision system that falls under the new law. Not every such system will fall under this new law.\n* Impact Assessment - ... a study evaluating an automated decision system and ... the development process, including the design and training data ... for impacts on accuracy, fairness, bias, discrimination, privacy, and security.\n* The above also includes a provisions for consumers to have access to the results ... and may correct or object to its results.\n* Covered by this law - Only systems that are run by a person or company making more than 50M US Dollars\/year, or has more than 1M users, or is in the business of data analysis\n* High Risk systems are also targeted - where the definition of this includes 4 major areas that pose a significant risk to A, B, C, and D\n* High Risk A - privacy or security of personal information and\/or contributing to inaccurate, unfair, biased, or discriminatory decisions.\n* High Risk B - extensive evaluation of peoples work performance, economic situation, health, preferences, behavior, location, or movements.\n* High Risk C - information about race, color, national origin, political opinions, religion, trade union membership, genetic data, biometric data, health, gender, gender identity, sexuality, sexual orientation, criminal convictions, or arrests.\n* High Risk D - uses systematic monitoring of a large publicly accessible physical place\n\nQuite a bit of ground!\n## This Competition\nThis Kernel seeks to use Word Clouds to analyze the [Competition Evaluation](https:\/\/www.kaggle.com\/c\/jigsaw-unintended-bias-in-toxicity-classification\/overview\/evaluation) in light of this proposed legislation. In addition, the Word Clouds provide insight into how the performance in the competition may be improved.","ae12d8ba":"# Poorly scored AUC Subgroups\n## BNSP (Background Negative, Subgroup Positive) AUC:  - Christian\nHere, we restrict the test set to the toxic examples that mention the identity and the non-toxic examples that do not. A low value here means that the model confuses toxic examples that mention the identity with non-toxic examples that do not, likely meaning that the model predicts lower toxicity scores than it should for toxic examples mentioning the identity.\n\nIn a prior iteration of this Kernel, the UAC for specific subgroups scored the lowest, meaning that comment texts that were identified as having to do with those subgroups were poorly classified as being toxic or not toxic. Let's look at the word clouds for words only in the toxic and only in the not toxic comment texts relating to those subgroups. \n\nThe three most poorly scored subgroups are christian, female, and jewish","9269afe9":"## Subgroup AUC: - Homosexual_gay_or_lesbian","375516f1":"## Poorly scored AUC Subgroup for BPSN\n## black","a3f0233d":"## BNSP: Christian","58d67a39":"# Word Cloud Visualizations\nThe word cloud [(sometimes called a Tag Cloud or Weighted List)](https:\/\/en.wikipedia.org\/wiki\/Tag_cloud) is a popular data visualization mechanism that will be used below.\n\n## Preprocessing\n* In all of the cases below, the data is first subdivided by criteria (ex: Toxic is either True or False, based on the \"target\" column being >= 0.5 or not, respectively). \n* After gathering the desired subset, we divide sentences into individual words.\n* Each word is then made into lower case, and \"english\" nltk stopwords are removed. Only lemmatized and alphabetic words are used.\n* Once a subset of words is so created, it is converted into a dictionary. A dictionary contains a list of words and their respective word counts.\n* Finally, two dictionaries are compared to find words that are present in one dictionary but not the other.\n\n## The First Wordclouds - Toxic versus Not Toxic\nThe first two wordclouds below show all the words that come from \"toxic\" comment_texts, but that are not present in \"non toxic\" comment_texts; and vice versa.","440d003a":"# Environment Setup\nFrom the \"Benchmark Kernel\" provided as part of this competition.","a1837065":"# Discussion of the above\n## Take a close look at the two wordclouds above\nHere are some points to consider:\n### Implications of AAA19\n* If we consider the data set used for training - does it really contain enough information such that words in the first wordcloud really represent words that are ONLY TYPICALLY used in Toxic Comment_Texts in general, or do they instead implicate a possible problem relating to situation \"High Risk C\" - information about race, color, national origin, political opinions, religion, trade union membership, genetic data, biometric data, health, gender, gender identity, sexuality, sexual orientation, criminal convictions, or arrests?\n* How might we scrub such information so that the first wordcloud only contains words typically used in toxic comment_texts, and that do not contain unnecessary words?\n* If this result were examined by a lawmaker, how might they perceive this particular data set with regard to impacts on accuracy, fairness, bias, discrimination, privacy, and security?\n\n### Implications on this Competition\n* With regard to the last statement above, we really care more about \"Accuracy\" from a perspective of the rules of the Competition Evaluation.\n* Specifically for this Kernel, we want to look at issues with the original \"Benchmark Kernel\" and think of how they might be improved.\n* Take a look at the total word counts also (printed as text above the wordclouds). Do we have enough slack to cull from the dictionaries and still classify with consistent accuracy? The answer may be yes, but as we examine the \"other AUC's\" below, the data gets considerably slimmer.\n\n# Additional Wordcloud Pairs\nIn the two wordclouds above, we talked about the overall accuracy of the toxic classifier (overall Area Under the Curve metric, or AUC). In the below software, we examine the issues with the BenchMark Kernel for the other AUC areas mentioned in the Competition Evaluation rules. Specifically, we will create WordCloud pairs for the three worst scores (from the original Benchmark Kernel) in the areas of \"Subgroup AUC,\" \"BPSN (Background Positive, Subgroup Negative) AUC,\" and \"BNSP (Background Negative, Subgroup Positive) AUC.\"\n","c4580fcf":"## Poorly scored AUC Subgroup for BPSN\n## homosexual_gay_or_lesbian","eb3e36b2":"## Subgroup AUC: - Black","08794804":"Forked from \"Benchmark Kernel\" ","a346f3da":"## Poorly scored AUC Subgroup for BPSN\n## white"}}