{"cell_type":{"655bf61f":"code","60f4ece9":"code","1b16183e":"code","b29f859d":"code","604eca76":"code","f0bb972f":"code","2094a5b4":"code","fdcb3841":"code","152e0009":"code","01bd70ee":"code","792a6225":"code","6cbe07a9":"code","77cb2fd1":"code","7584f9a5":"code","9bac3aa5":"code","7846f8f1":"code","64ab8ced":"code","ee945851":"code","eee58466":"code","e9dcfcdf":"code","767d0a79":"code","038ffc72":"code","412518d2":"code","c1e9e588":"code","bc048ec0":"markdown","350be397":"markdown","d06b0163":"markdown"},"source":{"655bf61f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","60f4ece9":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import fbeta_score, make_scorer\n","1b16183e":"train=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","b29f859d":"train.head()","604eca76":"print(\"the training dataset has\",train.shape[0],'rows',train.shape[1],\"columns\")","f0bb972f":"plt.style.use('seaborn')\np1=sns.countplot(x='target',data=train)\nfor p in p1.patches:\n        p1.annotate('{:6.2f}%'.format(p.get_height()\/len(train)*100), (p.get_x()+0.1, p.get_height()+50))\n        \nplt.gca().set_ylabel('samples')\n\n","2094a5b4":"def cleaned(text):\n    text = re.sub(r\"\\n\",\"\",text)\n    text = text.lower()\n    text = re.sub(r\"\\d\",\"\",text)        #Remove digits\n    text = re.sub(r'[^\\x00-\\x7f]',r' ',text) # remove non-ascii\n    text = re.sub(r'[^\\w\\s]','',text) #Remove punctuation\n    text = re.sub(r'http\\S+|www.\\S+', '', text) #Remove http\n    return text","fdcb3841":"train['cleaned'] = train['text'].apply(lambda x : cleaned(x))\ntest['cleaned'] = test['text'].apply(lambda x : cleaned(x))\n","152e0009":"train.head()","01bd70ee":"sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nX = train['cleaned'].to_numpy()\ny = train['target'].to_numpy()\nfor train_index, test_index in sss.split(X, y):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    #X_train = X.loc[train_index]\n    X_train, X_test = X[train_index], X[test_index]\n\n    y_train, y_test = y[train_index], y[test_index]\n","792a6225":"tweets_pipeline = Pipeline([('CVec', CountVectorizer(stop_words='english')),\n                     ('Tfidf', TfidfTransformer())])\n\nX_train_transformed = tweets_pipeline.fit_transform(X_train)\nX_test_transformed = tweets_pipeline.transform(X_test)","6cbe07a9":"SVC_clf=SVC()","77cb2fd1":"SVC_clf.fit(X_train_transformed,y_train)","7584f9a5":"y_pred=SVC_clf.predict(X_test_transformed)","9bac3aa5":"print('accuracy of SVC classifier {}'.format(accuracy_score(y_pred,y_test)))","7846f8f1":"accuracy_scoring=make_scorer(accuracy_score)\nparams1={'C':[0.001, 0.01, 0.1, 1, 10, 100],'kernel':['poly', 'rbf', 'sigmoid']}\nclf_gsc=GridSearchCV(SVC_clf,param_grid=params1,n_jobs=-1,scoring=accuracy_scoring)\nclf_gsc.fit(X_train_transformed,y_train)","64ab8ced":"print('best score for Grid_searchCV',clf_gsc.best_score_)","ee945851":"print('best params for Grid_searchCV',clf_gsc.best_params_)","eee58466":"clf_best=clf_gsc.best_estimator_","e9dcfcdf":"clf_best.fit(X_train_transformed,y_train)","767d0a79":"y_pred_tuned=clf_best.predict(X_test_transformed)","038ffc72":"print('scores for best estimator',accuracy_score(y_pred_tuned,y_test))\ny_pred","412518d2":"test_clean=test['cleaned'].to_numpy()\ntest_transformed = tweets_pipeline.transform(test_clean)\ny_pred_test=SVC_clf.predict(test_transformed)\ny_pred_series=pd.Series(y_pred_test,name='target')\nsub=pd.concat([test['id'],y_pred_series ], axis=1)\nsub.to_csv('submission.csv',index=False)\n","c1e9e588":"sub","bc048ec0":"Perform hyperparameter tuning on SVC","350be397":"Since we had found that there is a difference in distribution of data we will use stratified sampling ","d06b0163":"making submission"}}