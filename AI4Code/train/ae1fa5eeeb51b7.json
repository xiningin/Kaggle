{"cell_type":{"895c0639":"code","c96a5893":"code","12f63d2a":"code","23c871e9":"code","3a3085d5":"code","71ae24d2":"code","13d4fafa":"code","f4f73381":"code","9edb8b93":"code","ce53b62d":"code","15c61b88":"code","bfe884da":"code","2118a99a":"code","04c6fa06":"code","d15e5b54":"code","ec303078":"code","b3028f23":"code","8fd325cd":"code","d5c7e191":"code","bf84e7d8":"code","7c91349b":"code","02801a91":"code","5b19b2b3":"code","c3b5dfcc":"code","a295001b":"markdown","08f7493f":"markdown","200757c6":"markdown","4065682c":"markdown","0b66e750":"markdown","3de5619e":"markdown","a6b8dbc7":"markdown","263fd90a":"markdown","58181e5a":"markdown","50a07a30":"markdown","d894bf24":"markdown","75672619":"markdown","9c6534f1":"markdown","65d3a3fb":"markdown","d6ccc636":"markdown","70cc8940":"markdown","c67b6617":"markdown","6efce13c":"markdown","5be156f2":"markdown","ded4105f":"markdown","63fc2ba8":"markdown","e92ca4f7":"markdown","42150bc4":"markdown","ff26aa49":"markdown"},"source":{"895c0639":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch\nfrom torchvision import models\nfrom torchvision import transforms\nfrom torch import nn, optim\nfrom torch.utils.data.sampler import SubsetRandomSampler, SequentialSampler\nimport time\nimport cv2\nimport os\nfrom zipfile import ZipFile\nimport gc\n\ngc.collect()","c96a5893":"if os.environ.get('KAGGLE_KERNEL_RUN_TYPE',''):\n    print(\"Running a Kaggle Notebook\/Script - Could be Interactive or Batch Mode\")\n    host = 'Kaggle'\n    inp = '..\/input\/'\n    dir = ''\n    frame_color = 'black'\n    if os.environ.get('KAGGLE_KERNEL_RUN_TYPE','') == 'Interactive':\n        print(\"Running a Kaggle Notebook\/Script - Interactive Mode\")\n\n    if os.environ.get('KAGGLE_KERNEL_RUN_TYPE','') == 'Batch':\n        print(\"Running a Kaggle Notebook\/Script - Batch Mode\")\n\nelse:\n    try:\n        import google.colab\n        host = 'Colab'\n        drive.mount('\/content\/drive')\n        dir = 'drive\/MyDrive\/studies\/My projects\/Kaggle\/Facial_keypoints\/'\n        frame_color = 'black'\n        inp = ''\n        print(\"Running on Colab\")\n    except ModuleNotFoundError:\n        host = None\n        dir = ''\n        inp = ''\n        frame_color = 'white'\n        print(\"Running on Localhost\")\n\ngpu = torch.cuda.is_available()\nprint('GPU: ' + str(gpu))","12f63d2a":"with ZipFile(inp + 'facial-keypoints-detection\/test.zip', 'r') as zipObj:\n   zipObj.extractall()\nwith ZipFile(inp + 'facial-keypoints-detection\/training.zip', 'r') as zipObj:\n   zipObj.extractall()\ntrain_and_val_csv = pd.read_csv(dir + 'training.csv')\ntest_csv = pd.read_csv(dir + 'test.csv')\nprint('Train and validation set length {}'.format(len(train_and_val_csv)))\nprint('Test set length {}'.format(len(test_csv)))","23c871e9":"print(train_and_val_csv.info())\nprint(test_csv.info())\n","3a3085d5":"auto_fill = train_and_val_csv.ffill()\nfull_only = train_and_val_csv.dropna()\nmissing_only = train_and_val_csv[train_and_val_csv.isna().sum(axis=1) != 0]\n# missing_only = missing_only.fillna(method='ffill').fillna(method='bfill')\n\nprint(auto_fill.shape)\nprint(missing_only.shape)\nprint(full_only.shape)\n","71ae24d2":"def get_keypoints_features(keypoint_data):\n    keypoint_features = []\n    for _, sample_keypoints in keypoint_data.iterrows():\n        keypoint_features.append(sample_keypoints)\n\n    keypoint_features = np.array(keypoint_features, dtype=\"float\")\n    return keypoint_features\n\nclass FaceKeypointDataSet(torch.utils.data.Dataset):\n    def __init__(self, data, transformer=None, transformer_factor=None, is_test_set=False):\n        imgs = data.Image\n        imgs = np.array(imgs)\n        for j in range(len(imgs)):\n            imgs[j] = np.fromstring(imgs[j], sep = ' ')\n        self.image_data = imgs\n        self.feature_data = data.drop(['Image'], axis=1)\n        self.transformer = transformer\n        self.transformer_factor = transformer_factor\n        self.is_test_set = is_test_set\n        self.transform = transforms.Compose([transforms.ToPILImage(),\n                                transforms.ToTensor(),\n                                transforms.Normalize(mean=(0.5,), std=(0.5,))])\n\n    def __len__(self):\n        return len(self.image_data)\n\n    def __getitem__(self,index):\n        img =  self.image_data[index]\n        img = img.astype(np.uint8).reshape(96,96)\n        img = self.transform(img)\n        if self.is_test_set:\n                return img\n        feature = np.array(self.feature_data.iloc[index])\n        if self.transformer is not None:\n            img, feature = self.transformer(img,feature,self.transformer_factor)\n        return img, feature\n\noriginal_train_data = FaceKeypointDataSet(train_and_val_csv, transformer=None)\n","13d4fafa":"all_datasets = []\ndef create_aug_sets(aug_transformer, params):\n    aug_sets = []\n    for param in params:\n        aug_data = FaceKeypointDataSet(train_and_val_csv, transformer=aug_transformer,\n                                       transformer_factor=param)\n        aug_sets.append(aug_data)\n    show_aug(aug_sets)\n    global all_datasets\n    all_datasets += aug_sets\n    return aug_sets\n\ndef show_img_and_features(data, img_index):\n    plt.imshow(data[img_index][0].reshape(96, 96), cmap='gray')\n    plt.scatter(data[img_index][1][::2], data[img_index][1][1::2], marker='o', s=100)\n\ndef show_aug(datasets):\n    '''\n    Show the diffrence between the augmented dataset to the original\n    '''\n    fig = plt.figure(figsize=(10, 20))\n    plt.tight_layout()\n    num_of_datasets = len(datasets)\n    rand_img = np.random.randint(0, len(original_train_data))\n    for index, aug_data in enumerate(datasets):\n        fig.add_subplot(num_of_datasets, 2, (index + 1) * 2 - 1)\n        show_img_and_features(original_train_data, rand_img)\n        fig.add_subplot(num_of_datasets, 2, (index + 1) * 2)\n        show_img_and_features(aug_data, rand_img)\n    plt.show()\n","f4f73381":"original_index = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\nflip_index = [2,3,0,1,8,9,10,11,4,5,6,7,16,17,18,19,12,13,14,15,20,21,24,25,22,23,26,27,28,29]\n\ndef flip_aug(img, fea, factor):\n    img = np.array(img)\n    img = img[:,:,::-1]\n    img = torch.tensor(img.copy()).reshape(1,96,96)\n    fea = fea[flip_index]\n    fea[::2] = 96 - fea[::2]\n    return img, fea\n\ntransformer_params = [None]\naug_sets = create_aug_sets(flip_aug, transformer_params)","9edb8b93":"def noise_aug(img, fea, factor):\n    img = np.array(img)\n    img = img + 0.008 * np.random.randn(1,96,96)\n    img = torch.tensor(img.copy(),dtype=torch.float32).reshape(1,96,96)\n    return img, fea\n\ntransformer_params = [None]\naug_sets = create_aug_sets(noise_aug, transformer_params)\n","ce53b62d":"def brightness_aug(img, fea, factor):\n    img = np.clip(img + factor, -1, 1)\n    img = img.reshape(1,96,96)\n    return img, fea\n\ntransformer_params = [1, -1, 0.5, -0.5]\naug_sets = create_aug_sets(brightness_aug, transformer_params)\n","15c61b88":"def rotate_aug(img, fea, factor=-30):\n    rad = -factor\/180 * np.pi\n    rot = cv2.getRotationMatrix2D((48,48),factor,1)\n    img = cv2.warpAffine(np.array(img).reshape(96,96),rot,(96,96),flags=cv2.INTER_CUBIC)\n    img = torch.tensor(img).reshape(1,96,96)\n    fea -= 48\n    for index in range(0,len(fea),2):\n        x = fea[index]\n        y = fea[index + 1]\n        fea[index] = x * np.cos(rad) - y * np.sin(rad)\n        fea[index + 1] = x * np.sin(rad) + y * np.cos(rad)\n    fea += 48\n    return img,fea\n\ntransformer_params = [30, -30, 15, -15]\naug_sets = create_aug_sets(rotate_aug, transformer_params)\n","bfe884da":"tav_split = 0.65\nindices = list(range(len(full_only)))\nnp.random.shuffle(indices)\nsplit = int(np.floor((1 - tav_split) * len(full_only)))\ntrain_indices, val_indices = indices[split:], indices[:split]\ntrain_full = full_only.iloc[train_indices]\nval_sampler =  SequentialSampler(val_indices)\nmissing_set = FaceKeypointDataSet(missing_only, transformer=None)\ntrain_full_set = FaceKeypointDataSet(train_full, transformer=None)\nfull_only_set = FaceKeypointDataSet(full_only,transformer=None)\nall_datasets.append(missing_set)\nall_datasets.append(train_full_set)\ntrain_data = torch.utils.data.ConcatDataset(all_datasets)\ntrain_sampler = SubsetRandomSampler(range(len(train_data)))\nprint(f\"Train set length: {len(train_data)}\")\nprint(f\"Validation set length: {len(val_indices)}\")\ndel indices, train_indices, train_full, train_full_set, val_indices,\\\n    missing_set, all_datasets, original_train_data\ngc.collect()","2118a99a":"if gpu:\n    batch_size = 256\nelse:\n    batch_size = 32\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\nval_loader = torch.utils.data.DataLoader(full_only_set, batch_size=batch_size, sampler=val_sampler)\ndel full_only_set, train_data,val_sampler,train_sampler\ngc.collect()\nprint('Data loader is ready, wait for saving')\ntorch.save(val_loader, 'val_loader.pt')\nprint('Val loader is saved')","04c6fa06":"def visualize_examples(images, features, pred_labels=None):\n    '''\n    Make 16 images and labels examplesf= from the input,\n    if the input includes predicted labels - show them with x marks\n    '''\n    fig = plt.figure(figsize=(20,10))\n    for i in range(16):\n        fig.add_subplot(4, 4, i + 1)\n        plt.imshow(images[i].reshape(96, 96), cmap='gray')\n        plt.axis('off')\n        plt.tight_layout()\n        plt.scatter(features[i][::2], features[i][1::2], marker='o', s=100)\n        if pred_labels is not None:\n                plt.scatter(pred_labels[i][::2], pred_labels[i][1::2], marker='x',color='red', s=100)\n\n    plt.show()","d15e5b54":"print('Training set')\nimages, features = next(iter(train_loader))\nvisualize_examples(images, features)\nprint('Validation set')\nimages, features = next(iter(val_loader))\nvisualize_examples(images, features)\n\n","ec303078":"out_features = 30\nclass Basic(nn.Module):\n    '''\n    Some self ensmbeled model for fun\n    '''\n    def __init__(self):\n        super(Basic, self).__init__()\n        self.conv1 = nn.Sequential(nn.Conv2d(1,4,(3,3),padding=1),\n                                   nn.ReLU(),\n                                   nn.Conv2d(4,16,(3,3),padding=1),\n                                   nn.ReLU(),\n                                   nn.BatchNorm2d(16))\n        self.conv2 = nn.Sequential(nn.Conv2d(16,32,(3,3)),\n                                   nn.ReLU(),\n                                   nn.Conv2d(32,64,(3,3)),\n                                   nn.ReLU(),\n                                   nn.BatchNorm2d(64))\n        self.conv3 = nn.Sequential(nn.Conv2d(64,128,(5,5)),\n                                   nn.ReLU(),\n                                   nn.Conv2d(128,256,(5,5)),\n                                   nn.ReLU(),\n                                   nn.BatchNorm2d(256))\n        self.conv4 = nn.Sequential(nn.Conv2d(256,526,(3,3)),\n                                   nn.ReLU(),\n                                   nn.Conv2d(526,1024,(3,3)),\n                                   nn.ReLU(),\n                                   nn.BatchNorm2d(1024))\n        self.max_pool = nn.Sequential(nn.MaxPool2d(2))\n        self.fc1 = nn.Linear(9216,1024)\n        self.fc2 = nn.Linear(1024,256)\n        self.fc3 = nn.Linear(256,out_features)\n\n    def forward(self, x):\n        x = self.conv1(x) + x\n        x = self.max_pool(x)\n        x = self.conv2(x)\n        x = self.max_pool(x)\n        x = self.conv3(x)\n        x = self.max_pool(x)\n        x = self.conv4(x)\n        x = x.view(x.shape[0],-1)\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc3(x)\n        return x","b3028f23":"def RMSELoss(pred,y):\n    return torch.sqrt(torch.mean((pred-y)**2))\n\ndef RMSELoss_custom(pred,y):\n    not_nan = (batch_size * 30 - y.isnan().sum())*0.0001\n    return torch.sqrt(torch.mean((pred-y).nan_to_num()**2)) \/ not_nan\nepochs = 150\n\nresnet50 = models.resnet50(num_classes = 30)\nresnet50.inplanes = 96\nresnet50.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\nmodel = resnet50\n","8fd325cd":"learning_rate = 0.001\ncriterion = RMSELoss_custom\noptimizer = optim.Adam(model.parameters(),lr=learning_rate)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',verbose=True, patience=5)\nif gpu:\n    model.cuda()\n\ntrain_losses, val_losses = [], []\nval_loss_min = np.inf\nfor e in range(1, epochs + 1):\n    start = time.perf_counter()\n    model.float().train()\n    train_loss = 0\n    for images, labels in train_loader:\n        if gpu:\n            images = images.cuda()\n            labels = labels.cuda()\n        optimizer.zero_grad()\n        prediction = model(images)\n        loss = criterion(prediction, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n    val_loss = 0\n    with torch.no_grad():\n        model.eval()\n        for images, labels in val_loader:\n            if gpu:\n                images = images.cuda()\n                labels = labels.cuda()\n            prediction = model(images)\n            loss = criterion(prediction, labels)\n            val_loss += loss.item()\n        scheduler.step(val_loss)\n    train_losses.append(train_loss\/len(train_loader))\n    val_losses.append(val_loss\/len(val_loader))\n    print(\"Epoch: {}\/{} \".format(e, epochs),\n                  \"Training Loss: {:.4f}\".format(train_losses[-1]),\n                  \"Val Loss: {:.4f}\".format(val_losses[-1]))\n    if val_loss < val_loss_min:\n        val_loss_min = val_loss\n        torch.save(model.state_dict(), dir + 'model_so_far.pt')\n        print('Detected network improvement, saving current model')\n    end = time.perf_counter()\n    total = (end - start)*(epochs - e)\n    print('Estimated time: {} hours, '\n          '{} minutes, {} seconds'.format(total\/\/3600,\n                                          total%3600\/\/60,int(total%60)), end=\"\\r\")\n\nmodel = resnet50\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\ncheckpoint = torch.load(dir + 'model_so_far.pt')\nmodel.load_state_dict(checkpoint)\n\nplt.tick_params(colors=frame_color)\nplt.plot(train_losses, label='Training Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.legend(frameon=False)","d5c7e191":"model = resnet50\n\ncheckpoint = torch.load(dir + 'model_so_far.pt',map_location=torch.device('cpu'))\nmodel.load_state_dict(checkpoint)\nmodel.cpu()\nprint('Model is ready!')","bf84e7d8":"val_loader = torch.load(dir + 'val_loader.pt')\nfig = plt.figure(figsize=(20,10))\nimages, labels = next(iter(val_loader))\nwith torch.no_grad():\n    model.eval()\n    predicted_labels = model(images)\nvisualize_examples(images, labels, predicted_labels)","7c91349b":"fig = plt.figure(figsize=(20,10))\nimages, labels = next(iter(train_loader))\nwith torch.no_grad():\n    model.eval()\n    predicted_labels = model(images)\nvisualize_examples(images, labels, predicted_labels)","02801a91":"test_csv = pd.read_csv(dir + 'test.csv')\ntest_data = FaceKeypointDataSet(test_csv,is_test_set=True)\ntest_sampler =  SequentialSampler(range(len(test_data)))\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=len(test_data), sampler=test_sampler)\nimages = next(iter(test_loader))\nwith torch.no_grad():\n    model.eval()\n    predicted_labels = model(images)","5b19b2b3":"print('test set')\nvisualize_examples(images,predicted_labels)\n","c3b5dfcc":"keypts_labels = train_and_val_csv.columns.tolist()\nid_lookup = pd.read_csv(inp + 'facial-keypoints-detection\/IdLookupTable.csv')\nid_lookup_features = list(id_lookup['FeatureName'])\nid_lookup_image = list(id_lookup['ImageId'])\n\nfor i in range(len(id_lookup_features)):\n    id_lookup_features[i] = keypts_labels.index(id_lookup_features[i])\n\nlocation = []\nfor i in range(len(id_lookup_features)):\n    value = float(predicted_labels[id_lookup_image[i]-1][id_lookup_features[i]])\n    if value < 0:\n        print(id_lookup_image[i] - 1)\n        value = 0\n    if value > 96:\n        print(id_lookup_image[i] - 1)\n        value = 96\n    location.append(value)\nid_lookup['Location'] = location\nsubmission = id_lookup[['RowId', 'Location']]\nsubmission.to_csv('submission.csv',index=False)\nprint('Total test images labeled:')\nprint(len(submission) - 1)\nprint('Submission file is ready')\n","a295001b":"# Test prediction\n## Predict","08f7493f":"## New train set","200757c6":"# Data visualization","4065682c":"### Add noise","0b66e750":"## Train and test set info","3de5619e":"## Help function","a6b8dbc7":"# Facial keypoints detection\n# Preparation\n## Libraries import","263fd90a":"### Brightness trim","58181e5a":"# Augmentation\n### Help functions","50a07a30":"## Some examples","d894bf24":"# Train!\n","75672619":"## Arrange the data","9c6534f1":"## load the model","65d3a3fb":"## Visualization","d6ccc636":"# Validation prediction visualization\n## Some examples","70cc8940":"### Horizontal flip","c67b6617":"## Read the data","6efce13c":"# Model","5be156f2":"## DataLoader","ded4105f":"# Submission","63fc2ba8":"## Constract the loader","e92ca4f7":"## Environment settings","42150bc4":"## Training parameters","ff26aa49":"### Rotate"}}