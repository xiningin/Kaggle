{"cell_type":{"0bf9ac4c":"code","81386e4e":"code","8cf21663":"code","bd83ba6a":"code","93b83e50":"code","4d395807":"code","cbe52ec4":"code","f34d773d":"code","cb0ec992":"code","02f6cf0b":"code","56f602a5":"code","bc13c82c":"code","2754293d":"code","e3f35069":"code","dcfbdb56":"code","510e1656":"code","0542403a":"code","3c641b9c":"code","1ece5c6b":"code","baf44255":"code","e5766935":"code","580986d8":"code","435fe1f0":"code","a61a3146":"code","fe7f3cfb":"code","dae7d921":"code","3846c708":"code","84e506ab":"code","81a915dc":"code","8c780bc5":"code","c25aba91":"code","273572aa":"code","47001f02":"code","e40b283d":"code","8aa0db13":"code","9447b21e":"code","c9816021":"code","7e557d8f":"code","060951cb":"markdown","6d5d3aef":"markdown","cdb17c2c":"markdown","a5779232":"markdown","9571eb53":"markdown","553f3e25":"markdown","5a62fe17":"markdown","f7a9cbf0":"markdown","6064466e":"markdown","4f17301f":"markdown","748c1098":"markdown","485a33f5":"markdown","4a9943b8":"markdown","28e14aae":"markdown","cbcf44f0":"markdown","4252bb5c":"markdown","2da55d3e":"markdown","4ad7d81a":"markdown","41f631c7":"markdown","6bf4e3cf":"markdown","56d2bc6c":"markdown","f9c2849c":"markdown","76674b72":"markdown","71952b2d":"markdown","c0c4beea":"markdown","a20dca39":"markdown","fe8f5671":"markdown","87397efa":"markdown","1828d27f":"markdown","12b9d343":"markdown","8cc3b9b8":"markdown","9892b5c7":"markdown","e3992413":"markdown","79811ba6":"markdown","856a735b":"markdown","98b81c27":"markdown","301edaf5":"markdown"},"source":{"0bf9ac4c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom pylab import rcParams\nwarnings.filterwarnings('ignore')","81386e4e":"sns.set_style(\"darkgrid\")","8cf21663":"data = pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')","bd83ba6a":"data.head()","93b83e50":"data.describe()","4d395807":"data.columns","cbe52ec4":"data.isna().sum()","f34d773d":"data['age'].nunique()","cb0ec992":"for x in data.columns.values:\n    print(x+': '+str(data[x].nunique()))","02f6cf0b":"c_data = data.copy()\n\nc_data['sex'] = c_data['sex'].map({0:'Male', 1:'Female'})\nc_data['exng'] = c_data['exng'].map({0:'False', 1:'True'})\nc_data['cp'] = c_data['cp'].map({0:'typical angina', 1:'atypical angina', 2:'non-anginal pain', 3:'asymptomatic'})\nc_data['fbs'] = c_data['fbs'].map({0:'False', 1:'True'})\nc_data['output'] = c_data['output'].map({0:'Less Chance', 1:'More Chance'})\n\nc_data.head()","56f602a5":"def graph(name, u, title):\n    sns.countplot(x=c_data[name], hue=c_data['output'], ax=u)\n    \n    plt.setp(u.get_xticklabels(), rotation=0)\n    u.set_title(title, fontsize=11, fontdict={\"fontweight\": \"bold\"})\n    \n    for p in u.patches:\n        text = str(int(p.get_height()))\n        u.annotate(text, (p.get_x()+p.get_width()\/2, p.get_height()+3),\n                   ha=\"center\", va='center', fontsize=10, fontweight=\"bold\")\n\nfig2, ax2 = plt.subplots(4,2, figsize=(15, 15), gridspec_kw={\"wspace\" : 0.4, \"hspace\" : 0.3, \"top\": 0.95})\n\ncolors=[\"#ff0000\",\"#ff8000\",\"#ffff00\",\"#80ff00\",\"#00ff00\", \"#00ff80\", \"#00ffff\", \"#0080ff\", \"#0000ff\", \"#8000ff\", \"#ff00ff\", \"#ff0080\"]\n\ngraph(\"sex\", ax2[0,0], 'sex')\ngraph(\"exng\", ax2[0,1], 'Exercise induced angina')\ngraph(\"cp\", ax2[1,0], 'Chest Pain Type')\ngraph(\"fbs\", ax2[1,1], 'Fasting Blood Sugar > 120 mg\/dl')\ngraph('restecg', ax2[2,0], 'Resting Electrocardiographic Results')\ngraph('caa', ax2[2,1], 'Number of Major Vessels')\ngraph('slp', ax2[3,0], 'Slope')\ngraph('thall', ax2[3,1], 'Thal Rate')\n\nplt.rcParams['axes.axisbelow'] = True","bc13c82c":"display(c_data.groupby(['sex','output']).agg(['count'])['age'])\nfemale_rate = round(93\/207*100,2)\nmale_rate = round(72\/96*100,2)\nprint(f'\\n\\nFemale Heart Attack Rate: {female_rate}%')\nprint(f'Male Heart Attack Rate: {male_rate}%')","2754293d":"fig, ax = plt.subplots(2, 1,figsize=(12,10))\na = sns.histplot(c_data['age'].loc[c_data['output']=='More Chance'], bins=10, binwidth=10, binrange=(10,80), color='red', ax=ax[0])\nfor p in a.patches:\n    a.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() \/ 2., p.get_height()-1), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nax[0].set_title('More Chance of Heart Attack', fontsize=11, fontdict={\"fontweight\": \"bold\"})\n\nb = sns.histplot(c_data['age'].loc[c_data['output']=='Less Chance'], bins=10, binwidth=10, binrange=(10,80), color='blue', ax=ax[1])\nfor p in b.patches:\n    b.annotate(format(p.get_height(), '.2f'), (p.get_x() + p.get_width() \/ 2., p.get_height()-1), ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\nax[1].set_title('Less Chance of Heart Attack', fontsize=11, fontdict={\"fontweight\": \"bold\"})\n\nplt.rcParams['axes.axisbelow'] = True","e3f35069":"age_data = c_data.copy()\nindex = 0\n\nfor x in age_data['age']:\n    if x>=20 and x<30:\n        age_data['age'][index] = '20s'\n    elif x>=30 and x<40:\n        age_data['age'][index] = '30s'\n    elif x>=40 and x<50:\n        age_data['age'][index] = '40s'\n    elif x>=50 and x<60:\n        age_data['age'][index] = '50s'\n    elif x>=60 and x<70:\n        age_data['age'][index] = '60s'\n    elif x>=70 and x<80:\n        age_data['age'][index] = '70s'\n    index+=1\n    \nage = pd.DataFrame(age_data.groupby(['age','output'])['sex'].count())\ndisplay(age)\n\nthirties = round(11\/15*100, 2)\nfourties = round(50\/72*100, 2)\nfifties = round(65\/125*100, 2)\nsixties = round(32\/80*100, 2)\nseventies = round(6\/10*100, 2)\n\nprint('More Chance of Heart Attack Percentages... \\n\\n')\nprint(f'Thirties: {thirties}%\\n')\nprint(f'Fourties: {fourties}%\\n')\nprint(f'Fifties: {fifties}%\\n')\nprint(f'Sixties: {sixties}%\\n')\nprint(f'Seventies: {seventies}%\\n')","dcfbdb56":"data.head()","510e1656":"con_data = data[['age', 'trtbps', 'chol', 'thalachh', 'oldpeak']]","0542403a":"plt.figure(figsize=(15,8))\nmask = np.triu(np.ones_like(con_data.corr(), dtype=np.bool))\nsns.heatmap(data=con_data.corr(),annot=True,cmap='BrBG',mask=mask)","3c641b9c":"def graph1(name, u, title):\n    sns.kdeplot(x=con_data[name],hue=data['output'], ax=u, shade=True, palette=['#2271b1','#68de7c'])\n    u.set_title(title, fontsize=11, fontdict={\"fontweight\": \"bold\"})\n    \n\nfig2, ax2 = plt.subplots(3,2, figsize=(15, 15), gridspec_kw={\"wspace\" : 0.4, \"hspace\" : 0.3, \"top\": 0.95})\n\ncolors=[\"#ff0000\",\"#ff8000\",\"#ffff00\",\"#80ff00\",\"#00ff00\", \"#00ff80\", \"#00ffff\", \"#0080ff\", \"#0000ff\", \"#8000ff\", \"#ff00ff\", \"#ff0080\"]\n\ngraph1(\"age\", ax2[0,0], 'Age')\ngraph1(\"trtbps\", ax2[0,1], 'Resting Blood Pressure')\ngraph1(\"chol\", ax2[1,0], 'Cholestoral in mg\/dl fetched via BMI sensor')\ngraph1(\"thalachh\", ax2[1,1], 'Thal Rate')\ngraph1('oldpeak', ax2[2,0], 'Previous Peak')\n\n\nplt.rcParams['axes.axisbelow'] = True","1ece5c6b":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","baf44255":"X = data.drop('output', axis=1)\ny = data['output']\n\nsc = StandardScaler()\nscaled_X = sc.fit_transform(X)","e5766935":"X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.2, random_state=42)","580986d8":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","435fe1f0":"from sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, roc_auc_score\nfrom sklearn import metrics","a61a3146":"model1 = LogisticRegression(max_iter=1000)\nmodel1.fit(X_train, y_train)\npred1 = model1.predict(X_test)\nacc1 = accuracy_score(pred1, y_test)\nprint(classification_report(pred1, y_test))\nprint(acc1)","fe7f3cfb":"plt.figure(figsize = (10, 6))\ncolors={0:'red', 1:'green'}\nplt.scatter(data['thalachh'], data['oldpeak'], c=data['output'].map(colors))\nplt.show()","dae7d921":"model2 = GaussianNB()\nmodel2.fit(X_train, y_train)\npred2 = model2.predict(X_test)\nacc2 = accuracy_score(pred2, y_test)\nprint(classification_report(pred2, y_test))\nprint(acc2)","3846c708":"from scipy.stats import shapiro\ncon_features = ['oldpeak','thalachh', 'chol', 'age', 'trtbps']\nfor i in data[con_features].columns:\n    print(f'{i} {\"Not Gaussian\" if shapiro(data[i])[1]<0.05 else \"Gaussian\"}  {shapiro(data[i])}')","84e506ab":"model3 = KNeighborsClassifier(n_neighbors=6)\nmodel3.fit(X_train, y_train)\npred3 = model3.predict(X_test)\nacc3 = accuracy_score(pred3, y_test)\nprint(classification_report(pred3, y_test))\nprint(acc3)","81a915dc":"model4= DecisionTreeClassifier(max_depth=10, min_samples_leaf=15)\nmodel4.fit(X_train, y_train)\npred4 = model4.predict(X_test)\nacc4 = accuracy_score(pred4, y_test)\nprint(classification_report(pred4, y_test))\nprint(acc4)","8c780bc5":"model5 = RandomForestClassifier()\nmodel5.fit(X_train, y_train)\npred5 = model5.predict(X_test)\nacc5 = accuracy_score(pred5, y_test)\nprint(classification_report(pred5, y_test))\nprint(acc5)","c25aba91":"model6 = SVC()\nmodel6.fit(X_train, y_train)\npred6 = model6.predict(X_test)\nacc6 = accuracy_score(pred6, y_test)\nprint(classification_report(pred6, y_test))\nprint(acc6)","273572aa":"model7 = XGBClassifier()\neval_set = [(X_train, y_train),(X_test, y_test)]\nmodel7.fit(X_train, y_train, eval_metric=['error','logloss'], eval_set = eval_set)\npred7 = model7.predict(X_test)\nacc7 = accuracy_score(pred7, y_test)\nprint(classification_report(pred7, y_test))\nprint(acc7)","47001f02":"results = model7.evals_result()\nepochs = len(results['validation_0']['error'])\nx_axis = range(0,epochs)\n\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['logloss'], label='Train')\nax.plot(x_axis, results['validation_1']['logloss'], label='Test')\nax.legend()\nplt.ylabel('Log Loss')\nplt.title('XGBoost Log Loss')\nplt.show()\n# plot classification error\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['error'], label='Train')\nax.plot(x_axis, results['validation_1']['error'], label='Test')\nax.legend()\nplt.ylabel('Classification Error')\nplt.title('XGBoost Classification Error')\nplt.show()","e40b283d":"model7 = XGBClassifier()\neval_set = [(X_test, y_test)]\nmodel7.fit(X_train, y_train, early_stopping_rounds = 5, eval_metric='logloss', eval_set = eval_set)\npred7 = model7.predict(X_test)\nacc7 = accuracy_score(pred7, y_test)\nprint(classification_report(pred7, y_test))\nprint(acc7)","8aa0db13":"acc_table = pd.DataFrame({'Model': ['Logistic Regression',\n                                   'Naive Bayes',\n                                   'KNN',\n                                   'Decision Tree',\n                                   'Random Forest Tree',\n                                   'SVC',\n                                   'XGB'],\n                         'Accuracy Score': [acc1,\n                                           acc2,\n                                           acc3,\n                                           acc4,\n                                           acc5,\n                                           acc6,\n                                           acc7]})\nacc_table = acc_table.sort_values(by='Accuracy Score', ascending=False)\nacc_table.style.background_gradient(cmap='Blues')","9447b21e":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\ny_scores = model3.predict_proba(X_test)\nfpr, tpr, threshold = roc_curve(y_test, y_scores[:, 1])\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC Curve of kNN')\nplt.show()","c9816021":"plt.figure(figsize=(15,8))\nmask = np.triu(np.ones_like(data.corr(), dtype=np.bool))\nsns.heatmap(data=data.corr(),annot=True,cmap='BrBG',mask=mask)","7e557d8f":"from matplotlib.colors import ListedColormap\nfrom sklearn.metrics import accuracy_score, classification_report\n# filter warnings\nwarnings.filterwarnings(\"ignore\")\n\ndef accuracy(k, X_train, y_train, X_test, y_test):\n    # instantiate learning model and fit data\n    knn = KNeighborsClassifier(n_neighbors=k)    \n    knn.fit(X_train, y_train)\n\n    # predict the response\n    pred = knn.predict(X_test)\n\n    # evaluate and return  accuracy\n    return accuracy_score(y_test, pred)\n\ndef classify_and_plot(X, y):\n    ''' \n    split data, fit, classify, plot and evaluate results \n    '''\n    # split data into training and testing set\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\n    # init vars\n    n_neighbors = 7\n    h           = .02  # step size in the mesh\n\n    # Create color maps\n    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\n    cmap_bold  = ListedColormap(['#FF0000', '#0000FF'])\n\n    rcParams['figure.figsize'] = 5, 5\n        \n    clf = KNeighborsClassifier(n_neighbors)\n    clf.fit(X_train, y_train)\n\n        # Plot the decision boundary. For that, we will assign a color to each\n        # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                        np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n        # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    fig = plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n        # Plot also the training points, x-axis = 'Glucose', y-axis = \"BMI\"\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, edgecolor='k', s=20)   \n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(\"0\/1 outcome classification (k = %i)\" % (n_neighbors))\n    plt.show()\n\n        # evaluate\n    y_expected  = y_test\n    y_predicted = clf.predict(X_test)\n\n        # print results\n    print('----------------------------------------------------------------------')\n    print('Classification report')\n    print('----------------------------------------------------------------------')\n    print('\\n', classification_report(y_expected, y_predicted))\n    print('----------------------------------------------------------------------')\n    print('Accuracy = %5s' % round(accuracy(n_neighbors, X_train, y_train, X_test, y_test), 3))\n    print('----------------------------------------------------------------------')\n\n# we only take the best two features and prepare them for the KNN classifier\nrows_nbr = 303 # data.shape[0]\nX_prime  = np.array(data.iloc[:rows_nbr, [2,9]])\nX        = X_prime # preprocessing.scale(X_prime)\ny        = np.array(data.iloc[:rows_nbr, 13])\n\n# classify, evaluate and plot results\nclassify_and_plot(X, y)","060951cb":"**ROC Curve of KNN Model**","6d5d3aef":"**Naive Bayes - Gaussian NB**","cdb17c2c":"Interpretation: \n\nWhen the green and blue curves are almost the same, it means the feature does not separate the outcomes. The amount of impact that the feature affects is similar on both outcomes. Larger the difference between two curves, more important of the feature.\n\nTherefore, significant features that impact whether individual is less or more likely to have heart attack are oldpeak, Thalrate, and Age (Relatively). We can also conclude that the amount of cholestorl fetched via BMI sensor and resting blood pressure are not related to the chance of getting heart attack.","a5779232":"From Shapiro test, it turned out that all the continuous features were not Gaussian Distriution and I believe that is why it underperformed the prediction score.","9571eb53":"**Age Visualization**","553f3e25":"# **Data Preprocessing**","5a62fe17":"**Decision Tree**","f7a9cbf0":"In order to make a better prediction by using Gaussian NB model, it performs better if the distributions of variables are Gaussian or near-Gaussian. We can look at the KDE plots to see which feature has a gaussian distribution, but it is hard to tell, so I decided to take a shapiro test to distinguish which feature has a gaussian distribution","6064466e":"# **Model Comparison Table**","4f17301f":"XGBoost Evaluation","748c1098":"Since the dataset includes both categorical and numerical features, I thought this would give us the most accurate predictions. However, both decision tree and random forest did not give us the prediction as accurate as KNN model. The accuracy score of decision tree might have underperformed because of the overfitting problem since the model applies greedy approach and high-variance. Random Forest which handles those issue has improved a little bit, but still got lower accuracy compared to KNN. Since Random Forest require a lot of datasets, the relatively small amount of datasets might have caused both Random Forest and Decision Tree to get a lower accuracy score","485a33f5":"**XGBoost Classifier**","4a9943b8":"Since the classification error and logloss start to increase around the epoch 10ish, the accuracy score might be better if we stop early. ","28e14aae":"**Visualize Categorical Columns**","cbcf44f0":"**Create Copied Data for Visualization**","4252bb5c":"# **Visualization**","2da55d3e":"Logistic Regression is a relatively fast supervised classification technique compared to other supervised classification models. The Accuracy Score of 85% is pretty good. Since Logistic Regression performs better with linearly separable classes, so lets visualize the data if we can linearly separate them. I chose two features that have the most impact on the output which are 'Thalrate', and 'oldpeak'","4ad7d81a":"**Random Forest**","41f631c7":"**Visualization of the Continuous Features**","6bf4e3cf":"Since the age group of 20s are significantly small, it is hard to make an analysis on that age group.","56d2bc6c":"Need to find out which features are correlated the most","f9c2849c":"**I decided to apply various kinds of classification models and have an analysis with each model**","76674b72":"# **KNN Visualization**","71952b2d":"Also with SVC, it performs better with a large number of samples and that might be the reason why it has lower accuracy score than KNN. But it still got pretty high accuracy score.","c0c4beea":"Even though KNN have a disadvantage that it requires high storage and it gets slower as the number of examples increase, but for this prediction, the amount of data is affordable. However, it gives you high accuracy and good for non-linear data. Therefore, I believe this model gave us the relatively high accuracy compared to other machine learning models.","a20dca39":"**SVC**","fe8f5671":"After applying changes on early stopping rounds, the model performed better compared to the model without early stopping. Since XGBoost also applies ensemble learning method like Random Forest and requires significant amount of datasets, it still has lower accuracy than KNN model ","87397efa":"* Feature to Feature Correlations - Higher value indicates simillarity of both two features. Therefore, the less value the better.\n* Feature to Outcome Correlations - Higher value indicates the importance of feature \n\nChose feature cp and oldpeak because feature to feature correlation is -0.15 which is low and both of their feature to outcome correlations are high (0.43, -0.43)","1828d27f":"**KNN Classification with only two features gives us 80% of accuracy which is pretty good**","12b9d343":"**Logistic Regression**","8cc3b9b8":"As we can see that data is hard to be separated linearly and I believe this is why it underperformed.","9892b5c7":"**Gender Percentage**","e3992413":"**Number of Unique Values for Each Column**","79811ba6":"# Data Exploration","856a735b":"# **Machine Learning**","98b81c27":"**Analysis on 40s,50s,60s,70s**\n\nIt is interesting to see that age of 50s both have similar rate of 'More chance of Heart Attack' and 'Less chance of Heart Attack'.\n\nHowever, with the consideration of the number of datas, there was a distinct difference of chance of heart attack in 40s. Among 40s, the number of more chance of heart attack were way much higher than it's less chance of heart attack.\n\nFrom this analysis, we can conclude that 40s is the age group that is the most vulnerable to heart attack. By just looking at the percentage, Thirties and Seventies should also be aware of the possibility of getting heart attack, but there is not enough data so we should not make a conclusion rashly.|","301edaf5":"**KNN**"}}