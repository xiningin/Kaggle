{"cell_type":{"cafe13f8":"code","345ea230":"code","0e6018cf":"code","38a3c464":"code","ccd8a0d0":"code","85877b3b":"code","efd2dd2a":"code","241e2340":"code","c9f63b6e":"code","e91648a7":"code","52796b02":"code","99c58940":"code","1c3519ca":"code","8648804e":"code","7ef68c60":"code","3076384e":"code","990bac8f":"code","be0d40f6":"code","4da3e64c":"code","23645701":"code","eb80ba87":"code","9a6c65ed":"code","9bbc6504":"code","678116e7":"code","b6fa3941":"code","70984809":"code","b0c589f3":"code","119e393d":"code","df9828a6":"code","77efb811":"code","61a49e43":"code","0ea67821":"code","41af5d45":"code","3865099a":"code","05001284":"code","8a753711":"code","44cfd5a2":"code","9af8d8c3":"code","f4dd84c7":"code","870a1df6":"code","149a596f":"code","bc0eec9d":"code","5746781d":"markdown","ad7f5e4e":"markdown","e53275d1":"markdown","ceb6fb5a":"markdown","e435f5f3":"markdown","6bcb1c21":"markdown","0dcc5c67":"markdown","af48ee1e":"markdown","a7e915e0":"markdown","2e61b743":"markdown","35fb33c4":"markdown","b470ac8b":"markdown","5e550348":"markdown","6b442763":"markdown","cdfb2554":"markdown","e417c2d7":"markdown","4e4b1368":"markdown","17259b6f":"markdown","3ed458bb":"markdown","64123887":"markdown","c0305abb":"markdown","f79f7023":"markdown","5db727ae":"markdown","c331a841":"markdown","9d33dcab":"markdown","90a8fac5":"markdown","2593d44e":"markdown","afd34589":"markdown","d30f84a9":"markdown","2d02ea3e":"markdown"},"source":{"cafe13f8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","345ea230":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn import linear_model\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics.cluster import contingency_matrix\nfrom statsmodels.stats.contingency_tables import mcnemar","0e6018cf":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ngender_submission_data = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\n\nprint(\"Les donn\u00e9es d'apprentissage train_data ont pour dimension\",train_data.shape)\nprint(\"Les donn\u00e9es de test test_data ont pour dimension\",test_data.shape)\nprint(\"Les donn\u00e9es d'exemple de soummision gender_submission ont pour dimension\",gender_submission_data.shape)","38a3c464":"train_data.info()","ccd8a0d0":"train_data.head()","85877b3b":"test_data.head()","efd2dd2a":"gender_submission_data.head()","241e2340":"train_data.head()","c9f63b6e":"train_data.Survived.value_counts()","e91648a7":"# Visualiation des variables cat\u00e9gorielles et quantitatives \n\n# variables quantitatives\nvar_num = list(train_data.columns[(train_data.dtypes==\"int\")|(train_data.dtypes==\"float\")])\n\nprint(\"\\n On a {} variables numeriques : \\n {} \".format(len(var_num) ,var_num))\n\n# Variables categorielles\nvar_cat = list(train_data.columns[train_data.dtypes==\"object\"]) #possible de rajouter ceci si on veut exclure par exemple la var cat\u00e9gorielle y : .drop(\"y\")) \nprint(\"\\n On a {} variables categorielles : \\n {} \".format(len(var_cat),var_cat))","52796b02":"train_data.info()","99c58940":"# Examen d'une des variables cat\u00e9gorielles\nprint(\"\\n Cat\u00e9gories d'une des variables {}\".format(train_data[\"Sex\"]. unique()))\nplt.figure(figsize = (15,4))\nsb.countplot(x = \"Sex\", data = train_data)\n\n# Analyse statistique selon la variable cat\u00e9gorielle\nprint(\"\\n Analyse statistique selon la variable Sex\") \nprint(\"Moyenne\")\nprint(train_data.groupby(\"Sex\").mean())\nprint(\"Variance\")\nprint(train_data.groupby(\"Sex\").var())","1c3519ca":"# Voir le nombre de personnes par classe\ntrain_data.Sex.value_counts()","8648804e":"# Code Tutoriel Kaggle \n# ? : pourquoi la somme des pourcentage n'est pas \u00e9gale \u00e0 100 ?\n\n# Proportion de femmes qui ont surv\u00e9cu\nwomen = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = round(sum(women)\/len(women)*100,2)\nprint(\"Le pourcentage de femmes qui ont surv\u00e9cu est :\", rate_women)\n\n# Proportion d'hommes qui ont surv\u00e9cu\nmen = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = round(sum(men)\/len(men)*100,2)\nprint(\"\\nLe pourcentage d'hommes qui ont surv\u00e9cu est :\", rate_men)","7ef68c60":"# Examen d'une des variables quantitatives\nprint(\"\\n Cat\u00e9gories d'une des variables {}\".format(train_data[\"Survived\"]. unique()))\nplt.figure(figsize = (15,4))\nsb.countplot(x = \"Survived\", data = train_data)\n\n# Analyse statistique selon la variable quantitative\nprint(\"\\n Analyse statistique selon la variable Survived\") \nprint(\"Moyenne\")\nprint(train_data.groupby(\"Survived\").mean())\nprint(\"Variance\")\nprint(train_data.groupby(\"Survived\").var())","3076384e":"train_data.Survived.value_counts()","990bac8f":"train_data.Pclass.value_counts()","be0d40f6":"# Proportion de personnes par classe sociale\n\nPclass_1 = train_data.loc[train_data.Pclass == 1]\nrate_Pclass_1 = round((Pclass_1.shape[0]\/train_data.Pclass.shape[0])*100,2)\nprint(\"Le pourcentage de personnes appartenant \u00e0 la classe 1  :\", rate_Pclass_1)\n\nPclass_2 = train_data.loc[train_data.Pclass == 2]\nrate_Pclass_2 = round((Pclass_2.shape[0]\/train_data.Pclass.shape[0])*100,2)\nprint(\"Le pourcentage de personnes appartenant \u00e0 la classe 2  :\", rate_Pclass_2)\n\nPclass_3 = train_data.loc[train_data.Pclass == 3]\nrate_Pclass_3 = round((Pclass_3.shape[0]\/train_data.Pclass.shape[0])*100,2)\nprint(\"Le pourcentage de personnes appartenant \u00e0 la classe 3  :\", rate_Pclass_3)","4da3e64c":"# Proportion de personnes (survivantes et non-survivantes) par classe sociale\n\nPclass_1 = train_data.loc[train_data.Pclass == 1][\"Survived\"]\nrate_Pclass_1 = round((Pclass_1.shape[0]\/train_data.Pclass.shape[0])*100,2)\nprint(\"Le pourcentage de personnes (survivantes et non-survivantes) qui appartienennt \u00e0 classe sociale 1 est :\", rate_Pclass_1)\n\nPclass_2 = train_data.loc[train_data.Pclass == 2][\"Survived\"]\nrate_Pclass_2 = round((Pclass_2.shape[0]\/train_data.Pclass.shape[0])*100,2)\nprint(\"Le pourcentage de personnes (survivantes et non-survivantes) et qui appartienennt \u00e0 classe sociale 2 est :\", rate_Pclass_2)\n\nPclass_3 = train_data.loc[train_data.Pclass == 3][\"Survived\"]\nrate_Pclass_3 = round((Pclass_3.shape[0]\/train_data.Pclass.shape[0])*100,2)\nprint(\"Le pourcentage de personnes (survivantes et non-survivantes) et qui appartienennt \u00e0 classe sociale 3 est :\", rate_Pclass_3)","23645701":"# Examen d'une des variables quantitatives\nprint(\"\\n Cat\u00e9gories d'une des variables {}\".format(train_data[\"Pclass\"]. unique()))\nplt.figure(figsize = (15,4))\nsb.countplot(x = \"Pclass\", data = train_data)\n\n# Analyse statistique selon la variable quantitative\nprint(\"\\n Analyse statistique selon la variable Pclass\") \nprint(\"Moyenne\")\nprint(train_data.groupby(\"Pclass\").mean())\nprint(\"Variance\")\nprint(train_data.groupby(\"Pclass\").var())","eb80ba87":"# Examen d'une des variables cat\u00e9gorielles\nprint(\"\\n Cat\u00e9gories d'une des variables {}\".format(train_data[\"Embarked\"]. unique()))\nplt.figure(figsize = (15,4))\nsb.countplot(x = \"Embarked\", data = train_data)\n\n# Analyse statistique selon la variable cat\u00e9gorielle\nprint(\"\\n Analyse statistique selon la variable Embarked\") \nprint(\"Moyenne\")\nprint(train_data.groupby(\"Embarked\").mean())\nprint(\"Variance\")\nprint(train_data.groupby(\"Embarked\").var())","9a6c65ed":"# Code tutoriel Kaggle: Random Forest\n\n# from sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n\nX = pd.get_dummies(train_data[features])\n\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1) # cf RandomForestClassifier dans sklearn\n\n# n_estimators : nombre d'arbres dans la foret\n\n# max_depth : profondeur maximale des arbres\n\n# random_state : Contr\u00f4le \u00e0 la fois le caract\u00e8re al\u00e9atoire du bootstrap des \u00e9chantillons utilis\u00e9s \n# lors de la construction d'arbres (si bootstrap=True) et l'\u00e9chantillonnage des entit\u00e9s \u00e0 prendre en compte \n# lors de la recherche de la meilleure r\u00e9partition sur chaque n\u0153ud (si )\n\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission_1_RANDOMFOREST.csv', index=False)\nprint(\"Your submission was successfully saved!\")","9bbc6504":"print(output.shape)","678116e7":"output.head(10)","b6fa3941":"# D\u00e9coupage du jeu de donn\u00e9es en apprentissage et validation\nXa, Xv, Ya, Yv = train_test_split(X, y, shuffle=True, test_size=0.3, stratify=y)","70984809":"print(X.shape)\nprint(y.shape)\nprint(Xa.shape)\nprint(Xv.shape)\nprint(Ya.shape)\nprint(Yv.shape)","b0c589f3":"# LDA sur donn\u00e9es d'apprentissage et validation et calculs d'erreur de classification\n\n# LDA\nclf_lda = LinearDiscriminantAnalysis(solver='svd', store_covariance = False)\nclf_lda.fit(Xa, Ya)\nY_lda = clf_lda.predict(Xa)\nerr_lda = sum(Y_lda != Ya)\/Ya.size\nprint('LDA : taux d''erreur apprentissage = {}%'.format(100*err_lda))\nY_ldat = clf_lda.predict(Xv)\nerr_ldat = sum(Y_ldat != Yv)\/Yv.size\nprint('LDA : taux d''erreur validation= {}%'.format(100*err_ldat))\n\nprint('\\n \\n')\n# QDA\nclf_qda = QuadraticDiscriminantAnalysis(store_covariance = False)\nclf_qda.fit(Xa, Ya)\nY_qda = clf_qda.predict(Xa)\nerr_qda = sum(Y_qda!= Ya)\/Ya.size\nprint('QDA : taux d''erreur apprentissage = {}%'.format(100*err_qda))\nY_qdat = clf_qda.predict(Xv)\nerr_qdat = sum(Y_qdat!= Yv)\/Yv.size\nprint('QDA : taux d''erreur validation = {}%'.format(100*err_qdat))","119e393d":"# Recherche de la valeur optimale du param\u00e8tre de r\u00e9gularisaton C avec GridSearchCV\n\nC_grid = np.logspace(-1, 1, 50)\n\n#C_grid = [i\/100 for i in range (1, 150)]\n\n#from sklearn.model_selection import GridSearchCV\n\n# the grid\nparameters = [{\"C\": C_grid}]\n\n# the classifier\nclf_reg = linear_model.LogisticRegression(tol=1e-5, multi_class='multinomial', solver='lbfgs') # solveur par defaut\n\n#NB : mutlinomial marche m\u00eame face \u00e0 un cas de classification binaire, minimise perte, cf sklearn pour plus d'infos\n\n# Perf a K-fold validation using the accuracy as the performance measure\nK = 10 # feel free to adapt the value of $K$\n\n# we will do it on a grid search using n_jobs processors\nclf_reg = GridSearchCV(clf_reg, param_grid=parameters, cv=K, scoring=\"accuracy\", verbose=1, n_jobs = 2)\nclf_reg.fit(Xa, Ya)","df9828a6":"# Calcul des erreurs de classification \n\n# Get the best parameters\nprint(\"\\nRegression logistique - optimal hyper-parameters = {}\".format(clf_reg.best_params_))\nprint(\"Regression logistique - best cross-val accuracy = {} \\n\".format(clf_reg.best_score_))\n\ny_app_reg = clf_reg.predict(Xa)\nerr_app_reg = sum(y_app_reg != Ya)\/Ya.size\nprint('Regression logistique : taux d''erreur apprentissage = {}%'.format(100*err_app_reg))\n\ny_val_reg = clf_reg.predict(Xv)\nerr_val_reg = sum(y_val_reg != Yv)\/Yv.size\nprint('Regression logistique : taux d''erreur validation = {}%'.format(100*err_val_reg))","77efb811":"# Recherche de la valeur optimale du param\u00e8tre de r\u00e9gularisation C avec GridSearchCV\n\nC_grid = np.logspace(-1, 1, 7)\n#C_grid = [i\/100 for i in range (1, 150)]\n\n#from sklearn.model_selection import GridSearchCV\n\n# the grid\nparameters = [{\"C\": C_grid}]\n\n# the classifier\nclf_svm_lin = SVC(kernel='linear', probability=True)\n\n# Perf a K-fold validation using the accuracy as the performance measure\nK = 5 # feel free to adapt the value of $K$\n\n# we will dot it on a grid search using n_jobs processors\nclf_svm_lin = GridSearchCV(clf_svm_lin, param_grid=parameters, cv=K, scoring=\"accuracy\", verbose=1, n_jobs = 2)\nclf_svm_lin.fit(Xa, Ya)","61a49e43":"# Calcul des erreurs de classification \n\n# Get the best parameters\nprint(\"\\nSVM lin\u00e9aire - optimal hyper-parameters = {}\".format(clf_svm_lin.best_params_))\nprint(\"SVM lin\u00e9aire - best cross-val accuracy = {} \\n\".format(clf_svm_lin.best_score_))\n\ny_app_svm_lin = clf_reg.predict(Xa)\nerr_app_svm_lin = sum(y_app_svm_lin != Ya)\/Ya.size\nprint('SVM lin\u00e9aire : taux d''erreur apprentissage = {}%'.format(100*err_app_svm_lin))\n\ny_val_svm_lin = clf_svm_lin.predict(Xv)\nerr_val_svm_lin = sum(y_val_svm_lin != Yv)\/Yv.size\nprint('SVM lin\u00e9aire : taux d''erreur validation = {}%'.format(100*err_val_svm_lin))","0ea67821":"# Recherche des valeurs optimales des param\u00e8tres C et gamma avec GridSearchCV\n\ngamma_grid = np.logspace(-1.5, 0, 3)\nC_grid = np.logspace(-1, 1.5, 3)\n\n# the grid\nparameters = [{\"gamma\": gamma_grid, \"C\": C_grid}]\n\n# the classifier\nclf_svm_rbf = SVC(kernel=\"rbf\", tol=0.01, cache_size = 1000, probability=True)\n\n# Perf a K-fold validation using the accuracy as the performance measure\nK = 3 # feel free to adapt the value of $K$\n\n# we will do it on a grid search using n_jobs processors\nclf_svm_rbf = GridSearchCV(clf_svm_rbf, param_grid=parameters, cv=K, scoring=\"accuracy\", verbose=1, n_jobs = 2)\nclf_svm_rbf.fit(Xa, Ya)","41af5d45":"# Calcul des erreurs de classification\n\n# Get the best parameters\nprint(\"\\nSVM non lin\u00e9aire - optimal hyper-parameters = {}\".format(clf_svm_rbf.best_params_))\nprint(\"SVM non lin\u00e9aire - best cross-val accuracy = {} \\n\".format(clf_svm_rbf.best_score_))\n\ny_app_svm_rbf = clf_reg.predict(Xa)\nerr_app_svm_rbf = sum(y_app_svm_rbf != Ya)\/Ya.size\nprint('SVM non lin\u00e9aire : taux d''erreur apprentissage = {}%'.format(100*err_app_svm_rbf))\n\ny_val_svm_rbf = clf_svm_rbf.predict(Xv)\nerr_val_svm_rbf = sum(y_val_svm_rbf != Yv)\/Yv.size\nprint('SVM non lin\u00e9aire : taux d''erreur validation = {}%'.format(100*err_val_svm_rbf))","3865099a":"# Kppv et GridSearchCV\n\nk_values = [k for k in range(2,10)]\n\n# the grid\nparameters = [{\"n_neighbors\": k_values}]\n\n# the classifier\nclf_knn = KNeighborsClassifier()\n# Perf a K-fold validation using the accuracy as the performance measure\nK = 5 # feel free to adapt the value of $K$\n# we will dot it on a grid search using n_jobs processors\nclf_knn = GridSearchCV(clf_knn, param_grid=parameters, cv=K, scoring=\"accuracy\", verbose=1, n_jobs = 2)\nclf_knn.fit(Xa, Ya)","05001284":"# Calcul des erreurs de classification\n\n# Get the best parameters\nprint(\"\\nKNN - optimal hyper-parameters = {}\".format(clf_knn.best_params_))\nprint(\"KNN - best cross-val accuracy = {} \\n\".format(clf_knn.best_score_))\n\ny_app_knn = clf_knn.predict(Xa)\nerr_app_knn = sum(y_app_knn != Ya)\/Ya.size\nprint('KNN : taux d''erreur apprentissage = {}%'.format(100*err_app_knn))\n\ny_val_knn = clf_knn.predict(Xv)\nerr_val_knn = sum(y_val_knn != Yv)\/Yv.size\nprint('KNN : taux d''erreur validation = {}%'.format(100*err_val_knn))","8a753711":"# Pr\u00e9diction sur les donn\u00e9es de validation et matrice de confusion\n\nY_ldav = clf_lda.predict(Xv)\nY_qdav = clf_qda.predict(Xv)\ny_val_reg = clf_reg.predict(Xv)\ny_val_svm_lin = clf_svm_lin.predict(Xv)\ny_val_svm_rbf = clf_svm_rbf.predict(Xv)\ny_val_knn = clf_knn.predict(Xv)\n\n\n# matrices de confusion\nconfmat1 = confusion_matrix(y_true=Yv, y_pred=Y_ldav)\nconfmat2 = confusion_matrix(y_true=Yv, y_pred=Y_qdav)\nconfmat3 = confusion_matrix(y_true=Yv, y_pred=y_val_reg)\nconfmat4 = confusion_matrix(y_true=Yv, y_pred=y_val_svm_lin)\nconfmat5 = confusion_matrix(y_true=Yv, y_pred=y_val_svm_rbf)\nconfmat6 = confusion_matrix(y_true=Yv, y_pred=y_val_knn)\n","44cfd5a2":"fig, (ax1, ax2, ax3, ax4, ax5, ax6) = plt.subplots(1, 6,figsize=(20,15))\n\nax1.matshow(confmat1, cmap=plt.cm.Blues, alpha=0.3)\nax2.matshow(confmat2, cmap=plt.cm.Blues, alpha=0.3)\nax3.matshow(confmat3, cmap=plt.cm.Blues, alpha=0.3)\nax4.matshow(confmat4, cmap=plt.cm.Blues, alpha=0.3)\nax5.matshow(confmat5, cmap=plt.cm.Blues, alpha=0.3)\nax6.matshow(confmat6, cmap=plt.cm.Blues, alpha=0.3)\n\nfor i in range(confmat1.shape[0]):\n    for j in range(confmat1.shape[1]):\n        ax1.text(x=j, y=i, s=confmat1[i, j], va=\"center\", ha=\"center\")\nax1.set(xlabel='Label predit', ylabel='Vrai label')\nax1.set_title('LDA')\n\nfor i in range(confmat2.shape[0]):\n    for j in range(confmat2.shape[1]):\n        ax2.text(x=j, y=i, s=confmat2[i, j], va=\"center\", ha=\"center\")\nax2.set(xlabel='Label predit', ylabel='Vrai label')\nax2.set_title('QDA')\n\nfor i in range(confmat3.shape[0]):\n    for j in range(confmat3.shape[1]):\n        ax3.text(x=j, y=i, s=confmat3[i, j], va=\"center\", ha=\"center\")\nax3.set(xlabel='Label predit', ylabel='Vrai label')\nax3.set_title('R\u00e9gression logistique')\n\nfor i in range(confmat4.shape[0]):\n    for j in range(confmat4.shape[1]):\n        ax4.text(x=j, y=i, s=confmat4[i, j], va=\"center\", ha=\"center\")\nax4.set(xlabel='Label predit', ylabel='Vrai label')\nax4.set_title('SVM lin\u00e9aire')\n\nfor i in range(confmat5.shape[0]):\n    for j in range(confmat5.shape[1]):\n        ax5.text(x=j, y=i, s=confmat5[i, j], va=\"center\", ha=\"center\")\nax5.set(xlabel='Label predit', ylabel='Vrai label')\nax5.set_title('SVM non lin\u00e9aire')\n\nfor i in range(confmat6.shape[0]):\n    for j in range(confmat6.shape[1]):\n        ax6.text(x=j, y=i, s=confmat6[i, j], va=\"center\", ha=\"center\")\nax6.set(xlabel='Label predit', ylabel='Vrai label')\nax6.set_title('KNN')\n\nplt.show()\n\n# accuracy: (tp + tn) \/ (p + n)\n# precision tp \/ (tp + fp)\n# recall: tp \/ (tp + fn)\n# f1: 2 tp \/ (2 tp + fp + fn)\n\naccuracy_lda = accuracy_score(Yv, Y_ldav)\nprecision_lda = precision_score(Yv, Y_ldav)\nrecall_lda = recall_score(Yv, Y_ldav)\nf1_lda = f1_score(Yv, Y_ldav)\n\naccuracy_qda = accuracy_score(Yv, Y_qdav)\nprecision_qda = precision_score(Yv, Y_qdav)\nrecall_qda = recall_score(Yv, Y_qdav)\nf1_qda = f1_score(Yv, Y_qdav)\n\naccuracy_reg = accuracy_score(Yv, y_val_reg)\nprecision_reg = precision_score(Yv, y_val_reg)\nrecall_reg = recall_score(Yv, y_val_reg)\nf1_reg = f1_score(Yv, y_val_reg)\n\naccuracy_svn_lin = accuracy_score(Yv, y_val_svm_lin)\nprecision_svn_lin = precision_score(Yv, y_val_svm_lin)\nrecall_svn_lin = recall_score(Yv, y_val_svm_lin)\nf1_svn_lin = f1_score(Yv, y_val_svm_lin)\n\naccuracy_svn_rbf = accuracy_score(Yv, y_val_svm_rbf)\nprecision_svn_rbf = precision_score(Yv, y_val_svm_rbf)\nrecall_svn_rbf = recall_score(Yv, y_val_svm_rbf)\nf1_svn_rbf = f1_score(Yv, y_val_svm_rbf)\n\naccuracy_knn = accuracy_score(Yv, y_val_knn)\nprecision_knn = precision_score(Yv, y_val_knn)\nrecall_knn = recall_score(Yv, y_val_knn)\nf1_knn = f1_score(Yv, y_val_knn)\n\nresultats = {'LDA':[accuracy_lda, precision_lda, recall_lda, f1_lda],\n        'QDA':[accuracy_qda, precision_qda, recall_qda, f1_reg],\n        'Reg. Log':[accuracy_reg, precision_reg, recall_reg, f1_reg],\n        'SVM lin':[accuracy_svn_lin, precision_svn_lin, recall_svn_lin, f1_svn_lin], \n        'SVM non lin':[accuracy_svn_rbf, precision_svn_rbf, recall_svn_rbf, f1_svn_rbf], \n        'KNN':[accuracy_knn, precision_knn, recall_knn, f1_knn]}\n\ndf_res = pd.DataFrame(resultats, index= ['Accuracy', 'Precision', 'Recall', 'F-mesure'])\ndf_res","9af8d8c3":"clf_svm_rbf.fit(X, y)\npredictions_2 = clf_svm_rbf.predict(X_test)\n\noutput_2 = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions_2})\noutput_2.to_csv('my_submission_2_NON_LINEAR_SVM.csv', index=False)\nprint(\"Your submission was successfully saved!\")","f4dd84c7":"clf_knn.fit(X, y)\npredictions_3 = clf_knn.predict(X_test)\n\noutput_3 = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions_3})\noutput_3.to_csv('my_submission_3_KNN.csv', index=False)\nprint(\"Your submission was successfully saved!\")","870a1df6":"clf_reg.fit(X, y)\npredictions_4 = clf_reg.predict(X_test)\n\noutput_4 = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions_4})\noutput_4.to_csv('my_submission_4_LOGISTIC_REGRESSION.csv', index=False)\nprint(\"Your submission was successfully saved!\")","149a596f":"def plot_roc_curve(fpr, tpr, AUC, modele): \n    plt.plot(fpr, tpr, color='orange', label='ROC - AUC = ' + str(round(AUC,2)))\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve - ' + modele)\n    plt.legend()\n    plt.show()\n    \n#fpr : false positive rate\n#tpr : true positive rate\n    \nY_ldav_proba = clf_lda.predict_proba(Xv)\nY_qdav_proba = clf_qda.predict_proba(Xv)\ny_val_reg_proba = clf_reg.predict_proba(Xv)\ny_val_svm_lin_proba = clf_svm_lin.predict_proba(Xv)\ny_val_svm_rbf_proba = clf_svm_rbf.predict_proba(Xv)\ny_val_knn_proba = clf_knn.predict_proba(Xv)\n\nmodele = [\"LDA\", \"QDA\", \"Reg. Log\", \"SVM Lin\", \"SVM non lin\", \"KNN\"]\nres = [Y_ldav_proba[:,1], Y_qdav_proba[:,1], y_val_reg_proba[:,1], y_val_svm_lin_proba[:,1], y_val_svm_rbf_proba[:,1], y_val_knn_proba[:,1]]\n\nfor i, nom_modele in enumerate(modele): \n    print(i)\n    auc = roc_auc_score(Yv, res[i])    \n    fpr, tpr, thresholds = roc_curve(Yv, res[i])\n    plt.figure(figsize=(16, 12))\n    plt.subplot(3,2,i+1)\n    plot_roc_curve(fpr, tpr, auc, nom_modele)","bc0eec9d":"TC_LDA_REGLOG = contingency_matrix(Y_ldav, y_val_reg )\nTC_LDA_SVM_NL = contingency_matrix(Y_ldav, y_val_svm_rbf )\nTC_REGLOG_SVM_NL = contingency_matrix(y_val_reg, y_val_svm_rbf )\n\nresult_LDA_REGLOG = mcnemar(TC_LDA_REGLOG, exact=True)\nresult_LDA_SVM_NL = mcnemar(TC_LDA_SVM_NL, exact=True)\nresult_REGLOG_SVM_NL = mcnemar(TC_REGLOG_SVM_NL, exact=True)\n\nresultats = {'LDA \/ Reg. log':[result_LDA_REGLOG.statistic, result_LDA_REGLOG.pvalue],\n        'LDA \/ SVM non lin':[result_LDA_SVM_NL.statistic, result_LDA_SVM_NL.pvalue],\n        'Reg. Log \/ SVM non lin':[result_REGLOG_SVM_NL.statistic, result_REGLOG_SVM_NL.pvalue]}\n\ndf_res = pd.DataFrame(resultats, index= ['Statistic', 'p-value'])\ndf_res","5746781d":"## 4.2 - Analyse statistique et sommaire des donn\u00e9es \u00e0 disposition en fonction des cat\u00e9gories","ad7f5e4e":"On constate qu'il y a plus de personnes appartenant \u00e0 la classe 3 que de personnes appartenant \u00e0 la classe 1.","e53275d1":"Les variables sont colin\u00e9aires, ce qui signifie qu'elles sont li\u00e9es \u00e0 Y. LDA et QDA comme la plupart des techniques de r\u00e9gression impliquent le calcul d'une inversion de matrice, ce qui est inexact si le d\u00e9terminant est proche de 0 (c'est-\u00e0-dire lorsque 2 variables ou plus sont presque une combinaison lin\u00e9aire l'une de l'autre). ","ceb6fb5a":"# 3 - Chargement des datas depuis Kaggle","e435f5f3":"# 1 - Introduction : test de run du code pr\u00e9-existant lors de la cr\u00e9ation du notebook","6bcb1c21":"# METHODE 2 : R\u00e9gression logistique","0dcc5c67":"Il y a majoritaitement plus de femmes que d'hommes parmi les survivants.","af48ee1e":"# TITANIC Machine Learning from Disaster (Beginner)","a7e915e0":"# 2 - Chargement de toutes les librairies qui seront utilis\u00e9es dans ce notebook","2e61b743":"On peut obtenir la m\u00eame information avec .info().","35fb33c4":"Le test de McNemar permet de comparer plusieurs classifieurs. \n- L'hypoth\u00e8se H0 est que les 2 classifieurs ont une proportion similaire d'erreurs sur le jeu de donn\u00e9es de validation (autrement dit il n'y a aucune diff\u00e9rence dans les d\u00e9saccords entre les 2 classifieurs).\n- L'hypoth\u00e8se H1 est que les 2 classifieurs ont une proportion diff\u00e9rente d'erreurs sur le jeu de donn\u00e9es de validation (autrement dit il y a une diff\u00e9rence signifcative dans les d\u00e9saccords entre les 2 classifieurs).\n\n\nLorsque : \n- la p-value est inf\u00e9rieure au seuil alpha (exemple alpha = 0.05), on rejette H0. Il y a une diff\u00e9rence significative entre les 2 classifieurs.\n- la p-value est sup\u00e9rieure au seuil alpha (exemple alpha = 0.05), on ne rejette pas H0, il n'y a aucune ou peu de diff\u00e9rence entre les 2 classifieurs.","b470ac8b":"### Mon code :  ","5e550348":"On constate que la majorit\u00e9 des personnes ont embarqu\u00e9 \u00e0 S, tr\u00e8s peu ont embarqu\u00e9 \u00e0 Q. ","6b442763":"## 4.1 - Visualiation des variables cat\u00e9gorielles et quantitatives ","cdfb2554":"# METHODE 5 : KPPV","e417c2d7":"Pour rappel : \n\npclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\n=> Un peu plus de la moiti\u00e9 des personnes pr\u00e9sentes sur le bateau appartenant \u00e0 la classe 3 (Lower). Un quart appartenait \u00e0 la classe Upper et un cinqui\u00e8me \u00e0 la classe middle.","4e4b1368":"Le fichier final de soummission doit ressembler exactement \u00e0 ceci, avec les bonnes valeurs pr\u00e9dites des classes (0 ou 1, 0 si le passager n'a pas surv\u00e9cu, 1 sinon).","17259b6f":"Nous avons 2 classes 0 et 1. Nous sommes donc face \u00e0 un probl\u00e8me de classification binaire.","3ed458bb":"On constate qu'il y a plus de personnes qui n'ont pas surv\u00e9cu que de personnes qui ont surv\u00e9cu.","64123887":"# 4- Analyse des donn\u00e9s d'apprentissage","c0305abb":"# METHODE 3 : SVM lin\u00e9aire","f79f7023":"On constate que seul 4 des 7 variables quantitatives ont \u00e9t\u00e9 utilis\u00e9es ici pour expliquer Y.","5db727ae":"Ce code permet d'afficher les r\u00e9p\u00e9rtoires o\u00f9 se trouvent les datas sur Kaggle. Ces informations sont tr\u00e8s utiles car nous permet de charger les datas depuis Kaggle. Il n'est pas n\u00e9cessaire de les t\u00e9l\u00e9charger en local.","c331a841":"On constate : \n- l'absence de la colonne \"Survived\" dans le jeu de donn\u00e9es de test. C'est la valeur de cette colonne que nous devons pr\u00e9dire. \n- la pr\u00e9sence de variables cat\u00e9gorielles (exemple : Name, Sexe), de variables quantitatives (exemple : Survived, Pclass), et de valeurs NAN.","9d33dcab":"# 5 - Machine learning","90a8fac5":"## METHODE 1 : LDA & QDA","2593d44e":"NB : le fait de rajouter [\"Survived\"] ne change absolument rien aux pourcentages.","afd34589":"On consid\u00e8rera que le meilleur classifieur est celui qui donne le score le plus \u00e9lev\u00e9 en accuracy et F-mesure.","d30f84a9":"# Comparaison des mod\u00e8les\n","2d02ea3e":"# METHODE 4 : SVM non-lin\u00e9aire"}}