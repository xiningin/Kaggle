{"cell_type":{"558bf728":"code","6c5169fc":"code","46c5b3aa":"code","b179deaf":"code","c39ab082":"code","ea06dd67":"code","0ebc5718":"markdown","ef18c01a":"markdown","8ce1989b":"markdown","1d7499ab":"markdown","bc62a9a6":"markdown","4a24f4b5":"markdown","952491d7":"markdown","7162ba40":"markdown","3cde0745":"markdown","1ea1f20f":"markdown","29302989":"markdown","3880ee94":"markdown","6f19f475":"markdown","91843460":"markdown","c3711760":"markdown","75f3cccf":"markdown"},"source":{"558bf728":"%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport warnings\nwarnings.filterwarnings('ignore')","6c5169fc":"train = pd.read_csv('..\/input\/forest-cover-type-prediction\/train.csv')\ntrain = train.iloc[:,1:]\ntrain.iloc[:,:10].hist(figsize=(16,12), bins=50)\nplt.show()","46c5b3aa":"cat_features = train.iloc[:, 10:-1]\nwild_data, soil_data = cat_features.iloc[:,:4], cat_features.iloc[:,4:]\n\nsns.set_style(\"darkgrid\", {'grid.color':'.1'})\nflatui = [\"#e74c3c\", \"#34495e\", \"#2ecc71\",\"#3498db\"]\n\npalette = sns.color_palette(flatui)\n\nwild_data.sum().plot(kind='bar', figsize=(10,8), color='#34a028')\nplt.title('# of Observations of Wilderness Areas', size=18)\nplt.xlabel('Wilderness Areas', size=16)\nplt.ylabel('# of Observations', size=16)\nplt.xticks(rotation='horizontal', size=12)\nplt.yticks(size=12)\n\nsns.despine()\nplt.show()","b179deaf":"# plot bg\nsns.set_style(\"darkgrid\", {'grid_color': '.1'})\n\n# sum soil data, pass it as a series\nsoil_sum = pd.Series(soil_data.sum())\nsoil_sum.sort_values(ascending=False, inplace=True)\n\n# plot bar\nsoil_sum.plot(kind='barh', figsize=(23,17), color='#a87539')\nplt.gca().invert_yaxis()\nplt.title('# of Observations of Soil Types', size=18)\nplt.xlabel('# of Observation', size=16)\nplt.ylabel('Soil Types', size=16)\nplt.xticks(rotation='horizontal',size=14)\nplt.yticks(size=14)\n\nsns.despine()\nplt.show()","c39ab082":"num_features = train.iloc[:,:10]\nplt.subplots(figsize=(15,10))\nnum_features_corr = num_features.corr()\n\nmask = np.zeros_like(num_features_corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(num_features_corr, mask=mask, center=0, square=True, annot=True, annot_kws={\"size\": 15}, cbar_kws={\"shrink\": .8})\nplt.xticks(size=13)\nplt.yticks(size=13)\n\nplt.show()","ea06dd67":"fig, ax = plt.subplots(2,2, figsize=(24,18))\n\nsns.scatterplot(data=train, x=\"Horizontal_Distance_To_Hydrology\", y=\"Vertical_Distance_To_Hydrology\", hue=\"Cover_Type\", legend='full', palette='rainbow_r', ax=ax[0,0])\nsns.scatterplot(data=train, x=\"Elevation\", y=\"Horizontal_Distance_To_Roadways\", hue=\"Cover_Type\", legend='full', palette='rainbow_r', ax=ax[0,1])\nsns.scatterplot(data=train, x=\"Aspect\", y=\"Hillshade_3pm\", hue=\"Cover_Type\", legend='full', palette='rainbow_r', ax=ax[1,0])\nsns.scatterplot(data=train, x=\"Hillshade_3pm\", y=\"Hillshade_Noon\", hue=\"Cover_Type\", legend='full', palette='rainbow_r', ax=ax[1,1])\nplt.show()","0ebc5718":"The purpose of the EDA is to provide an overview of how python visualization tools can be used to understand the complex and large dataset. EDA is the first step in this workflow where the decision-making process is initiated for the feature selection. Some valuable insights can be obtained by looking at the distribution of the target, relationship to the target and link between the features. A more detailed notebook for the project is provided here: [[G2] ForestCoverType_EDA Notebook v2](https:\/\/www.kaggle.com\/mariannejoyleano\/g2-forestcovertype-eda-notebook-v2#Feature-Visualization)","ef18c01a":"# Group 2 Report Notebook","8ce1989b":"### Visualize Categorical Variables\n- The plots below show the number of observations of the different Wilderness Areas and Soil Types.\n- Wilderness Areas 3 and 4 have the most presence.\n- Wilderness Area 2 has the least amount of observations.\n- The most observations are seen having Soil Type 10 followed by Soil Type 29.\n- The Soil Types with the least amount of observations are Soil Type 7 and 15.","1d7499ab":"### Feature Engineering Challenges\n- Adding new variables during feature engineering often produced lower accuracy.\n- Automated feature engineering using entities and transformations amongst existing columns from a single dataset created many new columns that did not positively contribute to the model's accuracy - even after feature selection.\n- Testing the new features produced was very time consuming, even with the GPU accelerator.\n- After playing around with several different sets of new features, we found that only including manually created new features yielded the highest results.","bc62a9a6":"## Project Description","4a24f4b5":"## Summary of Challenges","952491d7":"## Links\n- [**[G2] ForestCoverType_EDA Notebook**](https:\/\/www.kaggle.com\/mariannejoyleano\/g2-forestcovertype-eda-notebook-v2)\n- [**[G2] ForestCoverType_Feature_Engineering Notebook**](https:\/\/www.kaggle.com\/jamiesperos\/g2-forestcovertype-feature-engineering-notebook\/)\n- [**[G2] ForestCoverType_Training Notebook**](https:\/\/www.kaggle.com\/emknowles\/g2-forestcovertype-training-notebook\/)\n- [**[G2] ForestCoverType_ModelParams Notebook**](https:\/\/www.kaggle.com\/emknowles\/g2-forestcovertype-modelparams-notebook\/)\n- [**[G2] ForestCoverType_FinalModelEvaluation Notebook**](https:\/\/www.kaggle.com\/emknowles\/g2-forestcovertype-finalmodelevaluation-notebook)\n- [**[G2] ForestCoverType_Submission Notebook**](https:\/\/www.kaggle.com\/emknowles\/g2-forest-cover-type-submission-v05)\n","7162ba40":"## Summary of Final Results\n\nThe model with the highest accuracy on the out of sample (test set) data was selected as our final model. It should be noted that the model with the highest accuracy according to 10-fold cross validation was not the most accurate model on the out of sample data (although it was close). The best model was the Extra Tree Classifier with an accuracy of .7962 on the test set. The Extra Trees model outperformed our Ensemble model (.7952), which had been our best model for several weeks. See the Submission Notebook and FinalModelEvaluation Notebook for additional detail. ","3cde0745":"## EDA Summary","1ea1f20f":"### Modeling Challenges\n\n- Ensemble and stacking methods initially resulted in models yielding higher accuracy on the test set, but as we added features and refined the parameters for each individual model, an individual model yielded a better score on the test set. \n- Performing hyperparameter tuning and training for several of the models was computationally expensive. While we were able to enable GPU acceleration for the XGBoost model, activating the GPU accelerator seemed to increase the tuning and training for the other models in the training notebook. \n- Optuna worked to reduce the time to process hyperparameter trials, but some of the hyperparameters identified through this method yielded weaker models than the hyperparameters identified through GridSearchCV. A balance between the two was needed. ","29302989":"### Feature Correlation     \nWith the heatmap excluding binary variables this help us visualize the correlations of the features. We were also able to provide scatterplots for four pairs of features that had a **positive correlation greater then 0.5**. These are one of the many visualizations that helped us understand the characteristics of the features for future feature engineering and model selection.","3880ee94":"### Visualize Numerical Variables\n- Using histograms, we can visualize the spread and values of the 10 numeric variables.\n- The `Slope`, `Vertical Distance to Hydrology`, and `Horizontal Distance to Hydrology`, `Roadways and Firepoints` are all skewed right.\n- `Hillshade 9am, Noon and 3pm` are all skewed left.","6f19f475":"- Not including the ID column, the training dataset has 55 columns and 15,120 rows (including the target variable column)\n    - 10 numeric and 44 categorical variables\n    - 7 different forest cover types (target variable)\n        1. Spruce\/Fir\n        2. Lodgepole Pine\n        3. Ponderosa Pine\n        4. Cottonwood\/Willow\n        5. Aspen\n        6. Douglas-fir\n        7. Krummholz","91843460":"- The objective of this project is to predict the forest cover type (the predominant kind of tree cover) using the cartographic variables given in the training\/test datasets. You can find more about this project at [Forest Cover Type Prediction](https:\/\/www.kaggle.com\/c\/forest-cover-type-prediction).\n- We are given raw unscaled data with both numerical and categorical variables.\n- First we performed Exploratory Data Analysis in order to visualize the characteristics of our given variables. (See EDA Summary)\n- We constructed various models to train our data - utilizing Optuna hyperparameter tuning to get parameters that maximize the model accuracies. (See Summary of Modeling Techniques Section)\n- Using feature engineering techniques, we built new variables to help improve the accuracy of our models.\n- Using the strategies above, we built our final model and generated forest cover type predictions for the test dataset.","c3711760":"## Summary of Modeling Techniques\n\nWe used several modeling techniques for this project. We began by training simple, standard models and applying the predictions to the test set. This resulted in models with only 50%-60% accuracy, necessitating more complex methods. The following process was used to develop the final model:\n- Scaling the training data to perform PCA and identify the most important features (see the Feature_Engineering Notebook for more detail)\n- Preprocessing the training data to add in new features\n- Performing GridSearchCV and using the Optuna approach (see the ModelParams Notebook for more detail) for identifying optimal parameters for the following models with corresponding training set accuracy scores:\n    - Logistic Regression (.7126)\n    - Decision Tree (.9808)\n    - Random Forest (1.0)\n    - Extra Tree Classifier (1.0)\n    - Gradient Boosting Classifier (1.0)\n    - Extreme Gradient Boosting Classifier (using GPU acceleration; 1.0)\n    - AdaBoost Classifier (.5123)\n    - Light Gradient Boosting Classifier (.8923)\n    - Ensemble\/Voting Classifiers (assorted combinations of the above models; 1.0)\n- Saving and exporting the preprocessor\/scaler and each each version of the model with the highest accuracy on the training set and highest cross validation score (see the Training notebook for more detail).\n- Calculating each model's predictions for the test set and submitting to determine accuracy on the test set:\n    - Logistic Regression (.6020)\n    - Decision Tree (.7102)\n    - Random Forest (.7465)\n    - Extra Tree Classifier (.7962)\n    - Gradient Boosting Classifier (.7905)\n    - Extreme Gradient Boosting Classifier (using GPU acceleration; .7803)\n    - AdaBoost Classifier (.1583)\n    - Light Gradient Boosting Classifier (.6891)\n    - Ensemble\/Voting Classifiers (assorted combinations of the above models; .7952)","75f3cccf":"### EDA Challenges\n- This project consists of a lot of data and can have countless of patterns and details to look at. \n- The training data was not a simple random sample of the entire dataset, but a stratified sample of the seven forest cover type classes which may not represent the final predictions well.\n- Creating a \"story\" to be easily incorporated into the corresponding notebooks such as Feature Engineering, Models, etc.\n- Manipulating the `Wilderness_Area` and `Soil_Type` (one-hot encoded variables) to visualize its distribution compared to `Cover_Type`."}}