{"cell_type":{"51420b8b":"code","0ae69228":"code","2e6f8d75":"code","953f267a":"code","f1ef542a":"code","fba3d000":"code","b4f5c558":"code","1815da54":"code","c3e6d119":"code","700c52f8":"code","3243aecb":"code","63d603f2":"code","bdc1c523":"code","76eac9bd":"code","eab4f345":"code","a43195c8":"code","d6e98138":"code","f68faf88":"code","97806163":"code","53967b53":"code","288b2ae5":"code","34a437a2":"code","87a03edf":"code","55f3d564":"code","0ba21920":"code","d9eff74b":"code","eecd72c4":"code","3aa9c45a":"code","4269b1aa":"code","c5a45aa1":"code","0d75bbd2":"code","f6d75e0f":"code","10cd0710":"code","1961ea67":"code","85fd7141":"code","a14c6e6f":"code","33a7dd25":"code","6e0ff285":"code","49b7909d":"code","49d8fc36":"code","544a5a26":"code","5c981dd9":"code","d1f87177":"code","b1355b5a":"code","39756b5a":"code","52629cfd":"code","7ea23aee":"code","590a2317":"code","111ec401":"code","b8053cfe":"code","2bd7f798":"code","fa449e2c":"code","676d49c2":"code","04a5a7a4":"code","843eba34":"code","b24d2c9c":"code","134d7c8f":"code","ebde3c77":"code","2fe7be94":"code","3bea1f7e":"code","70ef1688":"code","5c5800d1":"code","446f150c":"code","f255bc9b":"code","2d20df0f":"code","33c7caad":"code","85738e6d":"code","8bde6806":"code","a47033ee":"code","06a56181":"code","f893b99e":"code","34688a38":"code","db488079":"code","4b197d53":"code","592a5fa3":"code","61678b84":"code","0e3bd385":"code","b3d29ecf":"code","77ee101e":"code","fa8f47fb":"code","51d450d7":"code","99df7315":"code","5aea45ec":"code","17a80c38":"code","e2fd24d9":"code","514af4f6":"code","22b5f3ab":"code","02401d7e":"code","3e93aded":"code","860d47aa":"code","94001864":"code","a2b11388":"code","25f9d7cb":"code","c252fad6":"code","4fb620d5":"code","e6001597":"code","68abf8ff":"code","b60449cc":"code","74501c46":"code","57acffd1":"code","2a752bdb":"code","2d454d2c":"code","7aa022ca":"code","2d877c1a":"code","fd06e9ff":"code","766246fc":"code","83cfb9a1":"code","536090c2":"code","b4fadb21":"code","8c7581ca":"code","b9e65ee5":"code","beccc3d8":"code","ab70e41e":"code","189ae6a1":"code","f54daf87":"code","d2ea70ed":"code","83e3053d":"code","bfe8698d":"code","6c79d28d":"code","6d2be50c":"code","24c1bbd4":"code","e0af4ac8":"code","fb53fc84":"code","2269b384":"code","f07ee0b9":"code","3879f662":"code","9618a556":"code","103eed35":"code","3e94a30f":"code","3faa530d":"code","2bf31f6c":"code","f7ee0dc5":"code","8cf63fc3":"code","6c344040":"code","e19df1d0":"code","9d6e12bb":"code","41abe0cf":"code","98416038":"code","bdea8594":"code","0d840c1b":"code","2afda85f":"code","4fd5187a":"code","1845b7fd":"code","dfb01d8e":"code","02925b85":"code","4f773762":"code","235b084a":"code","707c95ca":"code","c95f0aab":"code","27c8be91":"code","4d23c836":"code","e739279e":"code","bff40a04":"code","4af7fdbc":"code","ece89151":"code","da040ab0":"code","bcc80685":"code","5f9f74ec":"code","bafd9943":"code","55a11d9e":"code","5a0c6521":"code","476b6c00":"code","9f67b630":"code","75495c27":"code","b014fa68":"code","64da7482":"code","e039146f":"code","c2a35482":"code","96439e7c":"code","a55dfdb1":"code","aa21f96d":"code","a5e883c5":"code","a56c2ba1":"code","a13e485d":"code","dcab94e2":"code","96057a97":"code","f7572d6f":"code","70f24619":"code","8f81093f":"code","084b86f4":"code","29a1290e":"code","7f34c419":"code","511e9fc6":"code","60ba80a4":"code","6bacfe1f":"code","32ad6b64":"code","a8544507":"code","f0b2f6f7":"code","debc9b05":"code","45f62def":"code","57f1283c":"code","534ca24e":"code","0e7ac3c1":"code","63857141":"code","62705391":"code","47d87f7d":"code","85a0f04d":"code","bfdc0dae":"code","5ecc4bee":"code","590d4237":"code","c5f5dcd4":"code","93af6b3b":"code","b7994cdb":"code","adbb1ff9":"code","ec68353f":"code","733c9753":"code","1b368448":"code","7afba59b":"code","4d50820e":"code","0680d193":"code","5099d4d2":"code","55a94ce9":"code","1155c2e3":"code","cbdd66be":"code","b0c8233f":"code","8789c044":"code","b1982772":"code","acf0d641":"code","29b9c50a":"code","a4a1dfb2":"code","c6ecf014":"code","dce85cda":"code","896f389b":"code","c8a932b3":"code","719a46f4":"code","4ba3680b":"code","7ed6ad65":"code","29396d2a":"code","24683b4c":"code","c2467338":"code","ead0f4be":"code","2ebed5c4":"code","e99743c0":"code","f74d5e6d":"code","b88c0e4a":"code","074c8b82":"code","3950793e":"code","19632bed":"code","450b8b2a":"code","e56b6b99":"code","76230bcd":"code","bbcce378":"code","eb93ed16":"code","44afb508":"code","1fa05e58":"code","db90eeb0":"code","c178e759":"code","c377804a":"code","e7a385c9":"code","9e78a28d":"code","0c5bf384":"code","728c62cb":"code","3ca895a6":"code","e36b51a9":"code","05b53da4":"code","a606a513":"code","28879cd4":"code","00221e51":"code","20d9669c":"code","1f7c9923":"code","f418f710":"code","0dc242d0":"code","0477ddbf":"code","ee2fdbc4":"code","fa9450a0":"code","8f7ec6fe":"code","bb49229d":"code","34146147":"code","c4264aeb":"code","5a72fa51":"code","884ac83c":"code","286d04ce":"code","8580519d":"code","5ee135e1":"code","23fa96db":"code","08edcd42":"code","7666f397":"code","4496ee2c":"code","6e93c1de":"code","8491b6a3":"code","b03418bb":"code","9017d9ae":"code","8919ec31":"code","ed78876d":"code","f9d6f34c":"code","a9444a14":"code","9cee58e1":"code","328af8f2":"code","cd59c886":"code","bfe23986":"code","edac54c3":"code","0e4ef79e":"code","e9f6d4b5":"code","cf6a9721":"code","cde563f5":"code","81a986f9":"code","af7cb353":"code","aacac484":"code","906b5119":"code","c7f18e87":"code","db4c7413":"code","9e79ee60":"code","90c1a05b":"code","c7c4ee57":"code","4a45a1e8":"code","ece5f5f9":"code","74b84c8f":"code","a377ae9a":"code","56cdfc0f":"code","ac7f67c2":"code","3d60977e":"code","e9e685da":"code","53648181":"code","0a8e42e4":"code","07b1a715":"code","3f620197":"code","960726d7":"code","75da954b":"code","9dd9a07a":"code","8318c694":"code","baf74823":"code","0ee8a45b":"code","23427091":"code","ee8dda5d":"code","f945d78b":"code","1df2b9ea":"code","6c5b95d1":"code","d7392c4b":"code","6f27835d":"code","b7b855ca":"code","f13bb0b3":"code","2de59549":"code","43d75d28":"code","7aad060e":"code","1e7e5ae7":"code","716b4c4e":"code","bf8d9a61":"code","98b9460e":"code","9d383c28":"code","85e99d69":"code","cabebef9":"code","0a237ac7":"code","379e7ab6":"code","63d7f678":"code","aa988228":"code","b11417a4":"code","6faf4262":"code","1eb2ba60":"code","9b39c91c":"code","3320c96a":"code","411dd90b":"code","de6bf482":"code","221c5e0e":"code","27b812e8":"code","ea964fca":"code","6177bb88":"code","75ed13bf":"code","e5f71d70":"markdown","4d750c24":"markdown","50726666":"markdown","f43a0250":"markdown","711c10cb":"markdown","20c3eb99":"markdown","7a388345":"markdown","c93907ce":"markdown","c344b771":"markdown","3629acf8":"markdown","65eb13c3":"markdown","23a85c79":"markdown","79b8d370":"markdown","c6465da2":"markdown","fb8a1e3c":"markdown","ee5c9b1a":"markdown","8b7c5f1e":"markdown","ad9a7d2a":"markdown","8d46570d":"markdown","86b910dd":"markdown","10da1a12":"markdown","a6222760":"markdown","60da3308":"markdown","2238d066":"markdown","e3bbcb0e":"markdown","0680778c":"markdown","ce08d761":"markdown","e99728cf":"markdown","97bb86d7":"markdown","26d74711":"markdown","1a970b85":"markdown","1cdec0e0":"markdown","f945f0ba":"markdown","e6c3d331":"markdown","7c83dbf4":"markdown","e6274e9a":"markdown","39795d1b":"markdown","92e868f3":"markdown","9abf4e92":"markdown","4fd2ae97":"markdown","6206d58b":"markdown","6b5153da":"markdown","7b37b50d":"markdown","f1dfcd3c":"markdown","6a0556c6":"markdown","c60cfc8a":"markdown","526b6d4d":"markdown","609e3f35":"markdown","9e782362":"markdown","61078141":"markdown","43af7e9c":"markdown","861d2c4d":"markdown","261eb72c":"markdown","26366972":"markdown","ff11de3b":"markdown","f28c225d":"markdown","9ced5211":"markdown","180c0ae6":"markdown","eab1a49e":"markdown","ef4c73e2":"markdown","473117cc":"markdown","f30b1d1a":"markdown","c311cf9e":"markdown","a3ca2157":"markdown","6997eab1":"markdown","86160def":"markdown","a3479304":"markdown","46aa5fae":"markdown","8b0d056e":"markdown","b9713814":"markdown","021bc323":"markdown","2a48ca6b":"markdown","7e28a3cc":"markdown","3833da09":"markdown","f1ee4b5b":"markdown","798704d3":"markdown","00853eeb":"markdown","0b16b789":"markdown","416c5f5f":"markdown","b9405198":"markdown","7319d24a":"markdown","52b2a851":"markdown","c55cb410":"markdown","947faffe":"markdown","b2d37d4c":"markdown","a236e5a9":"markdown","4827c876":"markdown","0c6379f4":"markdown","9e85096e":"markdown","b00e6d8d":"markdown","e603a802":"markdown","c9a8df3f":"markdown"},"source":{"51420b8b":"# imports\nimport numpy as np \nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nimport os\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')","0ae69228":"# list of data files available\nprint(os.listdir('..\/input'))","2e6f8d75":"# training data\napp_train = pd.read_csv('..\/input\/application_train.csv')\nprint('training data shape: ', app_train.shape)\napp_train.head()","953f267a":"# testing data\napp_test = pd.read_csv('..\/input\/application_test.csv')\nprint('testing data shape: ', app_test.shape)\napp_test.head()","f1ef542a":"# review target variable\napp_train['TARGET'].value_counts()\napp_train['TARGET'].astype(int).plot.hist();\n(app_train['TARGET']).describe()","fba3d000":"# summary of training data\napp_train.describe()","b4f5c558":"# join training and testing data sets so we keep the same number of features in both\ntrain_len = len(app_train)\ndataset = pd.concat(objs=[app_train, app_test], axis=0).reset_index(drop=True)\n# shape of combined dataset should be sum of rows in training and testing (307511 + 48744 = 356255) and 122 columns (testing data doesn't have target)\nprint('dataset data shape: ', dataset.shape)","1815da54":"# missing values\ndataset.isnull().sum()","c3e6d119":"# types of data\ndataset.dtypes.value_counts()","700c52f8":"# categorical data - how many different categories for each variable\ndataset.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","3243aecb":"dataset.describe(include=[np.object])","63d603f2":"# use label encoding for categorical variables with only two categories\nle = LabelEncoder()\ncount = 0\nle_vars = []\nfor col in dataset:\n    if dataset[col].dtype == 'object':\n        if len(list(dataset[col].unique())) == 2:\n            le.fit(dataset[col])\n            dataset[col] = le.transform(dataset[col])\n            count += 1\n            le_vars.append(col)\n            \nprint('%d columns were label encoded' % count)\nprint(le_vars)","bdc1c523":"# use one-hot encoding for remaining categorical variables\ndataset = pd.get_dummies(dataset)\nprint('dataset data shape: ', dataset.shape)","76eac9bd":"(dataset['CNT_CHILDREN']).describe()","eab4f345":"dataset['CNT_CHILDREN'].plot.hist(title = 'CNT_CHILDREN Histogram');\nplt.xlabel('CNT_CHILDREN')","a43195c8":"# plot CNT_CHILDREN against the TARGET to better understand the data\ng = sns.factorplot(x='CNT_CHILDREN', y='TARGET', data=app_train, kind=\"bar\", size = 6, palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"default probability\")","d6e98138":"outlier_children = app_train[app_train['CNT_CHILDREN'] > 6]\nprint('count of outlier_children: ', len(outlier_children))\nprint('default probability of outlier_children: %0.2f%%' %(100 * outlier_children['TARGET'].mean()))\n(outlier_children['CNT_CHILDREN']).describe()","f68faf88":"# create a flag for outliers in the CNT_CHILDREN column, and then replace these values with nan\ndataset['CNT_CHILDREN_outlier'] = dataset['CNT_CHILDREN'] > 6\nfor i in dataset['CNT_CHILDREN']:\n    if i > 6:\n        dataset['CNT_CHILDREN'].replace({i: np.nan}, inplace = True)","97806163":"# review CNT_CHILDREN after our modifications\n(dataset['CNT_CHILDREN']).describe()","53967b53":"dataset['CNT_CHILDREN'].plot.hist(title = 'CNT_CHILDREN Histogram');\nplt.xlabel('CNT_CHILDREN')","288b2ae5":"(dataset['AMT_INCOME_TOTAL']).describe()","34a437a2":"dataset['AMT_INCOME_TOTAL'].plot.hist(range = (1,1000000), title = 'AMT_INCOME_TOTAL Histogram');\nplt.xlabel('AMT_INCOME_TOTAL')","87a03edf":"(dataset['AMT_CREDIT']).describe()","55f3d564":"dataset['AMT_CREDIT'].plot.hist(title = 'AMT_CREDIT Histogram');\nplt.xlabel('AMT_CREDIT')","0ba21920":"(dataset['AMT_ANNUITY']).describe()","d9eff74b":"dataset['AMT_ANNUITY'].plot.hist(title = 'AMT_ANNUITY Histogram');\nplt.xlabel('AMT_ANNUITY')","eecd72c4":"(dataset['AMT_GOODS_PRICE']).describe()","3aa9c45a":"dataset['AMT_GOODS_PRICE'].plot.hist(title = 'AMT_GOODS_PRICE Histogram');\nplt.xlabel('AMT_GOODS_PRICE')","4269b1aa":"(dataset['REGION_POPULATION_RELATIVE']).describe()","c5a45aa1":"dataset['REGION_POPULATION_RELATIVE'].plot.hist(title = 'REGION_POPULATION_RELATIVE Histogram');\nplt.xlabel('REGION_POPULATION_RELATIVE')","0d75bbd2":"# plot REGION_POPULATION_RELATIVE against the TARGET to better understand the data\ng = sns.lineplot(x='REGION_POPULATION_RELATIVE', y='TARGET', data=app_train, palette = \"muted\")","f6d75e0f":"g = sns.heatmap(app_train[['TARGET','CNT_CHILDREN','AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY','AMT_GOODS_PRICE','REGION_POPULATION_RELATIVE']].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","10cd0710":"(dataset['DAYS_BIRTH']).describe()","1961ea67":"# this variable appears to be equal to (date of birth) minus (date of application), which is producing negative numbers\n# if we look again at the data transformed into positive numbers and into years (by dividing by -365.25) we get the following\n(dataset['DAYS_BIRTH'] \/ -365.25).describe()","85fd7141":"(dataset['DAYS_BIRTH'] \/ -365.25).plot.hist(title = 'DAYS_BIRTH Histogram');\nplt.xlabel('DAYS_BIRTH')","a14c6e6f":"(dataset['DAYS_EMPLOYED']).describe()","33a7dd25":"# this variable appears to be equal to (date of employment) minus (date of application), which is producing negative numbers\n# if we look again at the data transformed into positive numbers and into years (by dividing by -365.25) we get the following\n(dataset['DAYS_EMPLOYED'] \/ -365.25).describe()","6e0ff285":"# it appears that a dummy value was used, possibly for people who didn't have a date of employment to enter into the application\n# this group had 365243 in the data, which is approximately -1000 years\n# we should also look at the other side of the distribution - 49 years of employment is a long time\ndataset['DAYS_EMPLOYED'].plot.hist(title = 'DAYS_EMPLOYED Histogram');\nplt.xlabel('DAYS_EMPLOYED')","49b7909d":"outlier_days_employed = app_train[app_train['DAYS_EMPLOYED'] == 365243]\nprint('count of outlier_days_employed: ', len(outlier_days_employed))\nprint('default probability of outlier_days_employed: %0.2f%%' %(100 * outlier_days_employed['TARGET'].mean()))\n(outlier_days_employed['DAYS_EMPLOYED']).describe()","49d8fc36":"# create a flag for outliers in the DAYS_EMPLOYED column, and then replace these values with nan\ndataset['DAYS_EMPLOYED_outlier'] = dataset['DAYS_EMPLOYED'] == 365243\ndataset['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)","544a5a26":"# review DAYS_EMPLOYED after our modifications\n(dataset['DAYS_EMPLOYED']).describe()","5c981dd9":"(dataset['DAYS_EMPLOYED'] \/ -365.25).plot.hist(title = 'DAYS_EMPLOYED Histogram');\nplt.xlabel('DAYS_EMPLOYED')","d1f87177":"(dataset['DAYS_REGISTRATION']).describe()","b1355b5a":"# this variable appears to be equal to (date of registration) minus (date of application), which is producing negative numbers\n# if we look again at the data transformed into positive numbers and into years (by dividing by 365.25) we get the following\n(dataset['DAYS_REGISTRATION'] \/ -365.25).describe()","39756b5a":"(dataset['DAYS_REGISTRATION'] \/ -365.25).plot.hist(title = 'DAYS_REGISTRATION Histogram');\nplt.xlabel('DAYS_REGISTRATION')","52629cfd":"(dataset['DAYS_ID_PUBLISH']).describe()","7ea23aee":"# convert to positive years again\n(dataset['DAYS_ID_PUBLISH'] \/ -365.25).describe()","590a2317":"(dataset['DAYS_ID_PUBLISH'] \/ -365.25).plot.hist(title = 'DAYS_ID_PUBLISH Histogram');\nplt.xlabel('DAYS_ID_PUBLISH')","111ec401":"g = sns.heatmap(app_train[['TARGET','DAYS_BIRTH','DAYS_EMPLOYED','DAYS_REGISTRATION','DAYS_ID_PUBLISH']].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","b8053cfe":"(dataset['OWN_CAR_AGE']).describe()","2bd7f798":"dataset['OWN_CAR_AGE'].plot.hist(title = 'OWN_CAR_AGE Histogram');\nplt.xlabel('OWN_CAR_AGE')","fa449e2c":"# it seems that there are some outliers on the car age, as the max is 91\n# let's get a better look at the values in the tail and whether these have a higher probability of default than average\noutlier_car_age = app_train[app_train['OWN_CAR_AGE'] > 60]\nprint('count of outlier_car_age: ', len(outlier_car_age))\nprint('default probability of outlier_car_age: %0.2f%%' %(100 * outlier_car_age['TARGET'].mean()))\n(outlier_car_age['OWN_CAR_AGE']).describe()","676d49c2":"outlier_car_age['OWN_CAR_AGE'].plot.hist(title = 'OWN_CAR_AGE Outlier Histogram');\nplt.xlabel('OWN_CAR_AGE')","04a5a7a4":"# create a flag for outliers in the OWN_CAR_AGE column, and then replace these values with nan\ndataset['OWN_CAR_AGE_outlier'] = dataset['OWN_CAR_AGE'] > 60\nfor i in dataset['OWN_CAR_AGE']:\n    if i > 60:\n        dataset['OWN_CAR_AGE'].replace({i: np.nan}, inplace = True)","843eba34":"# review OWN_CAR_AGE after our modifications\n(dataset['OWN_CAR_AGE']).describe()","b24d2c9c":"# now this data should look more like we expect\ndataset['OWN_CAR_AGE'].plot.hist(title = 'OWN_CAR_AGE Histogram');\nplt.xlabel('OWN_CAR_AGE')","134d7c8f":"(dataset['CNT_FAM_MEMBERS']).describe()","ebde3c77":"dataset['CNT_FAM_MEMBERS'].plot.hist(title = 'CNT_FAM_MEMBERS Histogram');\nplt.xlabel('CNT_FAM_MEMBERS')","2fe7be94":"# it seems that there are some outliers on the count of family members, as the max is 21\n# let's look at the 99th percentile\nprint(np.nanpercentile(dataset['CNT_FAM_MEMBERS'], 99))","3bea1f7e":"# let's get a better look at the values in the tail and whether these have a higher probability of default than average\noutlier_fam_mem = app_train[app_train['CNT_FAM_MEMBERS'] > 5]\nprint('count of outlier_fam_mem: ', len(outlier_fam_mem))\nprint('default probability of outlier_fam_mem: %0.2f%%' %(100 * outlier_fam_mem['TARGET'].mean()))\n(outlier_fam_mem['CNT_FAM_MEMBERS']).describe()","70ef1688":"# create a flag for outliers in the CNT_FAM_MEMBERS column, and then replace these values with nan\ndataset['CNT_FAM_MEMBERS_outlier'] = dataset['CNT_FAM_MEMBERS'] > 5\nfor i in dataset['CNT_FAM_MEMBERS']:\n    if i > 5:\n        dataset['CNT_FAM_MEMBERS'].replace({i: np.nan}, inplace = True)","5c5800d1":"# review CNT_FAM_MEMBERS after our modifications\n(dataset['CNT_FAM_MEMBERS']).describe()","446f150c":"dataset['CNT_FAM_MEMBERS'].plot.hist(title = 'CNT_FAM_MEMBERS Histogram');\nplt.xlabel('CNT_FAM_MEMBERS')","f255bc9b":"(dataset['REGION_RATING_CLIENT']).describe()","2d20df0f":"dataset['REGION_RATING_CLIENT'].plot.hist(title = 'REGION_RATING_CLIENT Histogram');\nplt.xlabel('REGION_RATING_CLIENT')","33c7caad":"(dataset['REGION_RATING_CLIENT_W_CITY']).describe()","85738e6d":"dataset['REGION_RATING_CLIENT_W_CITY'].plot.hist(title = 'REGION_RATING_CLIENT_W_CITY Histogram');\nplt.xlabel('REGION_RATING_CLIENT_W_CITY')","8bde6806":"# how many are equal to -1 in the dataset?\ndataset['REGION_RATING_CLIENT_W_CITY'].map(lambda s: 1 if s == -1 else 0).sum()","a47033ee":"# this appears to be a data entry error\n# let's set the value of -1 equal to 1 instead\nfor i in dataset['REGION_RATING_CLIENT_W_CITY']:\n    if i == -1:\n        dataset['REGION_RATING_CLIENT_W_CITY'].replace({i: 1}, inplace = True)","06a56181":"(dataset['REGION_RATING_CLIENT_W_CITY']).describe()","f893b99e":"(dataset['HOUR_APPR_PROCESS_START']).describe()","34688a38":"dataset['HOUR_APPR_PROCESS_START'].plot.hist(title = 'HOUR_APPR_PROCESS_START Histogram');\nplt.xlabel('HOUR_APPR_PROCESS_START')","db488079":"g = sns.heatmap(app_train[['TARGET','OWN_CAR_AGE','CNT_FAM_MEMBERS','REGION_RATING_CLIENT','REGION_RATING_CLIENT_W_CITY','HOUR_APPR_PROCESS_START']].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","4b197d53":"(dataset['EXT_SOURCE_1']).describe()","592a5fa3":"# this data looks pretty good from the above, check the histogram\ndataset['EXT_SOURCE_1'].plot.hist(title = 'EXT_SOURCE_1 Histogram');\nplt.xlabel('EXT_SOURCE_1')","61678b84":"(dataset['EXT_SOURCE_2']).describe()","0e3bd385":"# this data also looks pretty good from the above, check the histogram\ndataset['EXT_SOURCE_2'].plot.hist(title = 'EXT_SOURCE_2 Histogram');\nplt.xlabel('EXT_SOURCE_2')","b3d29ecf":"(dataset['EXT_SOURCE_3']).describe()","77ee101e":"# this doesn't appear to have issues either, check the histogram\ndataset['EXT_SOURCE_3'].plot.hist(title = 'EXT_SOURCE_3 Histogram');\nplt.xlabel('EXT_SOURCE_3')","fa8f47fb":"g = sns.heatmap(app_train[['TARGET','EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3']].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","51d450d7":"(dataset['OBS_30_CNT_SOCIAL_CIRCLE']).describe(percentiles=[0.25,0.5,0.75,0.99,0.999,0.9999,0.99999])","99df7315":"# there appear to be some outliers here we may need to deal with (max = 354??)\ndataset['OBS_30_CNT_SOCIAL_CIRCLE'].plot.hist(title = 'OBS_30_CNT_SOCIAL_CIRCLE Histogram');\nplt.xlabel('OBS_30_CNT_SOCIAL_CIRCLE')","5aea45ec":"# let's look more at these outliers and whether these have a higher probability of default than average\noutlier_obs_30_social = app_train[app_train['OBS_30_CNT_SOCIAL_CIRCLE'] > 17]\nprint('count of outlier_obs_30_social: ', len(outlier_obs_30_social))\nprint('default probability of outlier_obs_30_social: %0.2f%%' %(100 * outlier_obs_30_social['TARGET'].mean()))\n(outlier_obs_30_social['OBS_30_CNT_SOCIAL_CIRCLE']).describe()","17a80c38":"# create a flag for outliers in the OBS_30_CNT_SOCIAL_CIRCLE column, and then replace these values with nan\ndataset['OBS_30_CNT_SOCIAL_CIRCLE_outlier'] = dataset['OBS_30_CNT_SOCIAL_CIRCLE'] > 17\nfor i in dataset['OBS_30_CNT_SOCIAL_CIRCLE']:\n    if i > 17:\n        dataset['OBS_30_CNT_SOCIAL_CIRCLE'].replace({i: np.nan}, inplace = True)","e2fd24d9":"# review OBS_30_CNT_SOCIAL_CIRCLE after our modifications\n(dataset['OBS_30_CNT_SOCIAL_CIRCLE']).describe()","514af4f6":"dataset['OBS_30_CNT_SOCIAL_CIRCLE'].plot.hist(title = 'OBS_30_CNT_SOCIAL_CIRCLE Histogram');\nplt.xlabel('OBS_30_CNT_SOCIAL_CIRCLE')","22b5f3ab":"(dataset['DEF_30_CNT_SOCIAL_CIRCLE']).describe(percentiles=[0.25,0.5,0.75,0.99,0.999,0.9999,0.99999])","02401d7e":"dataset['DEF_30_CNT_SOCIAL_CIRCLE'].plot.hist(title = 'DEF_30_CNT_SOCIAL_CIRCLE Histogram');\nplt.xlabel('DEF_30_CNT_SOCIAL_CIRCLE')","3e93aded":"outlier_def_30_social = app_train[app_train['DEF_30_CNT_SOCIAL_CIRCLE'] > 5]\nprint('count of outlier_def_30_social: ', len(outlier_def_30_social))\nprint('default probability of outlier_def_30_social: %0.2f%%' %(100 * outlier_def_30_social['TARGET'].mean()))\n(outlier_def_30_social['DEF_30_CNT_SOCIAL_CIRCLE']).describe()","860d47aa":"dataset['DEF_30_CNT_SOCIAL_CIRCLE_outlier'] = dataset['DEF_30_CNT_SOCIAL_CIRCLE'] > 5\nfor i in dataset['DEF_30_CNT_SOCIAL_CIRCLE']:\n    if i > 5:\n        dataset['DEF_30_CNT_SOCIAL_CIRCLE'].replace({i: np.nan}, inplace = True)","94001864":"(dataset['DEF_30_CNT_SOCIAL_CIRCLE']).describe()","a2b11388":"dataset['DEF_30_CNT_SOCIAL_CIRCLE'].plot.hist(title = 'DEF_30_CNT_SOCIAL_CIRCLE Histogram');\nplt.xlabel('DEF_30_CNT_SOCIAL_CIRCLE')","25f9d7cb":"(dataset['OBS_60_CNT_SOCIAL_CIRCLE']).describe(percentiles=[0.25,0.5,0.75,0.99,0.999,0.9999,0.99999])","c252fad6":"dataset['OBS_60_CNT_SOCIAL_CIRCLE'].plot.hist(title = 'OBS_60_CNT_SOCIAL_CIRCLE Histogram');\nplt.xlabel('OBS_60_CNT_SOCIAL_CIRCLE')","4fb620d5":"outlier_obs_60_social = app_train[app_train['OBS_60_CNT_SOCIAL_CIRCLE'] > 16]\nprint('count of outlier_obs_60_social: ', len(outlier_obs_60_social))\nprint('default probability of outlier_obs_60_social: %0.2f%%' %(100 * outlier_obs_60_social['TARGET'].mean()))\n(outlier_obs_60_social['OBS_60_CNT_SOCIAL_CIRCLE']).describe()","e6001597":"dataset['OBS_60_CNT_SOCIAL_CIRCLE_outlier'] = dataset['OBS_60_CNT_SOCIAL_CIRCLE'] > 16\nfor i in dataset['OBS_60_CNT_SOCIAL_CIRCLE']:\n    if i > 16:\n        dataset['OBS_60_CNT_SOCIAL_CIRCLE'].replace({i: np.nan}, inplace = True)","68abf8ff":"(dataset['OBS_60_CNT_SOCIAL_CIRCLE']).describe()","b60449cc":"dataset['OBS_60_CNT_SOCIAL_CIRCLE'].plot.hist(title = 'OBS_60_CNT_SOCIAL_CIRCLE Histogram');\nplt.xlabel('OBS_60_CNT_SOCIAL_CIRCLE')","74501c46":"(dataset['DEF_60_CNT_SOCIAL_CIRCLE']).describe(percentiles=[0.25,0.5,0.75,0.99,0.999,0.9999,0.99999])","57acffd1":"dataset['DEF_60_CNT_SOCIAL_CIRCLE'].plot.hist(title = 'DEF_60_CNT_SOCIAL_CIRCLE Histogram');\nplt.xlabel('DEF_60_CNT_SOCIAL_CIRCLE')","2a752bdb":"outlier_def_60_social = app_train[app_train['DEF_60_CNT_SOCIAL_CIRCLE'] > 4]\nprint('count of outlier_def_60_social: ', len(outlier_def_60_social))\nprint('default probability of outlier_def_60_social: %0.2f%%' %(100 * outlier_def_60_social['TARGET'].mean()))\n(outlier_def_60_social['DEF_60_CNT_SOCIAL_CIRCLE']).describe()","2d454d2c":"dataset['DEF_60_CNT_SOCIAL_CIRCLE_outlier'] = dataset['DEF_60_CNT_SOCIAL_CIRCLE'] > 4\nfor i in dataset['DEF_60_CNT_SOCIAL_CIRCLE']:\n    if i > 4:\n        dataset['DEF_60_CNT_SOCIAL_CIRCLE'].replace({i: np.nan}, inplace = True)","7aa022ca":"(dataset['DEF_60_CNT_SOCIAL_CIRCLE']).describe()","2d877c1a":"dataset['DEF_60_CNT_SOCIAL_CIRCLE'].plot.hist(title = 'DEF_60_CNT_SOCIAL_CIRCLE Histogram');\nplt.xlabel('DEF_60_CNT_SOCIAL_CIRCLE')","fd06e9ff":"g = sns.heatmap(app_train[['TARGET','OBS_30_CNT_SOCIAL_CIRCLE','DEF_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE','DEF_60_CNT_SOCIAL_CIRCLE']].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","766246fc":"(dataset['DAYS_LAST_PHONE_CHANGE']).describe()","83cfb9a1":"# let's transform this into positive years, as we did with the other DAYS_ variables above\n(dataset['DAYS_LAST_PHONE_CHANGE'] \/ -365.25).describe()","536090c2":"(dataset['DAYS_LAST_PHONE_CHANGE'] \/ -365.25).plot.hist(title = 'DAYS_LAST_PHONE_CHANGE Histogram');\nplt.xlabel('DAYS_LAST_PHONE_CHANGE')","b4fadb21":"(dataset['AMT_REQ_CREDIT_BUREAU_HOUR']).describe()","8c7581ca":"dataset['AMT_REQ_CREDIT_BUREAU_HOUR'].plot.hist(title = 'AMT_REQ_CREDIT_BUREAU_HOUR Histogram');\nplt.xlabel('AMT_REQ_CREDIT_BUREAU_HOUR')","b9e65ee5":"dataset['AMT_REQ_CREDIT_BUREAU_HOUR_outlier'] = dataset['AMT_REQ_CREDIT_BUREAU_HOUR'] > 1\nfor i in dataset['AMT_REQ_CREDIT_BUREAU_HOUR']:\n    if i > 1:\n        dataset['AMT_REQ_CREDIT_BUREAU_HOUR'].replace({i: np.nan}, inplace = True)","beccc3d8":"(dataset['AMT_REQ_CREDIT_BUREAU_DAY']).describe()","ab70e41e":"dataset['AMT_REQ_CREDIT_BUREAU_DAY'].plot.hist(title = 'AMT_REQ_CREDIT_BUREAU_DAY Histogram');\nplt.xlabel('AMT_REQ_CREDIT_BUREAU_DAY')","189ae6a1":"dataset['AMT_REQ_CREDIT_BUREAU_DAY_outlier'] = dataset['AMT_REQ_CREDIT_BUREAU_DAY'] > 2\nfor i in dataset['AMT_REQ_CREDIT_BUREAU_DAY']:\n    if i > 2:\n        dataset['AMT_REQ_CREDIT_BUREAU_DAY'].replace({i: np.nan}, inplace = True)","f54daf87":"(dataset['AMT_REQ_CREDIT_BUREAU_WEEK']).describe()","d2ea70ed":"dataset['AMT_REQ_CREDIT_BUREAU_WEEK'].plot.hist(title = 'AMT_REQ_CREDIT_BUREAU_WEEK Histogram');\nplt.xlabel('AMT_REQ_CREDIT_BUREAU_WEEK')","83e3053d":"dataset['AMT_REQ_CREDIT_BUREAU_WEEK_outlier'] = dataset['AMT_REQ_CREDIT_BUREAU_WEEK'] > 2\nfor i in dataset['AMT_REQ_CREDIT_BUREAU_WEEK']:\n    if i > 2:\n        dataset['AMT_REQ_CREDIT_BUREAU_WEEK'].replace({i: np.nan}, inplace = True)","bfe8698d":"(dataset['AMT_REQ_CREDIT_BUREAU_MON']).describe()","6c79d28d":"dataset['AMT_REQ_CREDIT_BUREAU_MON'].plot.hist(title = 'AMT_REQ_CREDIT_BUREAU_MON Histogram');\nplt.xlabel('AMT_REQ_CREDIT_BUREAU_MON')","6d2be50c":"dataset['AMT_REQ_CREDIT_BUREAU_MON_outlier'] = dataset['AMT_REQ_CREDIT_BUREAU_MON'] > 5\nfor i in dataset['AMT_REQ_CREDIT_BUREAU_MON']:\n    if i > 5:\n        dataset['AMT_REQ_CREDIT_BUREAU_MON'].replace({i: np.nan}, inplace = True)","24c1bbd4":"(dataset['AMT_REQ_CREDIT_BUREAU_QRT']).describe()","e0af4ac8":"dataset['AMT_REQ_CREDIT_BUREAU_QRT'].plot.hist(title = 'AMT_REQ_CREDIT_BUREAU_QRT Histogram');\nplt.xlabel('AMT_REQ_CREDIT_BUREAU_QRT')","fb53fc84":"dataset['AMT_REQ_CREDIT_BUREAU_QRT_outlier'] = dataset['AMT_REQ_CREDIT_BUREAU_QRT'] > 5\nfor i in dataset['AMT_REQ_CREDIT_BUREAU_QRT']:\n    if i > 5:\n        dataset['AMT_REQ_CREDIT_BUREAU_QRT'].replace({i: np.nan}, inplace = True)","2269b384":"(dataset['AMT_REQ_CREDIT_BUREAU_YEAR']).describe()","f07ee0b9":"dataset['AMT_REQ_CREDIT_BUREAU_YEAR'].plot.hist(title = 'AMT_REQ_CREDIT_BUREAU_YEAR Histogram');\nplt.xlabel('AMT_REQ_CREDIT_BUREAU_YEAR')","3879f662":"dataset['AMT_REQ_CREDIT_BUREAU_YEAR_outlier'] = dataset['AMT_REQ_CREDIT_BUREAU_YEAR'] > 10\nfor i in dataset['AMT_REQ_CREDIT_BUREAU_YEAR']:\n    if i > 10:\n        dataset['AMT_REQ_CREDIT_BUREAU_YEAR'].replace({i: np.nan}, inplace = True)","9618a556":"g = sns.heatmap(app_train[['TARGET','DAYS_LAST_PHONE_CHANGE','AMT_REQ_CREDIT_BUREAU_HOUR','AMT_REQ_CREDIT_BUREAU_DAY','AMT_REQ_CREDIT_BUREAU_WEEK','AMT_REQ_CREDIT_BUREAU_MON','AMT_REQ_CREDIT_BUREAU_QRT','AMT_REQ_CREDIT_BUREAU_YEAR']].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","103eed35":"# create new variables\ndataset['EMPLOY_AGE'] = dataset['DAYS_EMPLOYED'] \/ dataset['DAYS_BIRTH']\ndataset['INCOME_AGE'] = dataset['AMT_INCOME_TOTAL'] \/ dataset['DAYS_BIRTH']\ndataset['CREDIT_AGE'] = dataset['AMT_CREDIT'] \/ dataset['DAYS_BIRTH']\ndataset['CREDIT_INCOME'] = dataset['AMT_CREDIT'] \/ dataset['AMT_INCOME_TOTAL']\ndataset['ANNUITY_INCOME'] = dataset['AMT_ANNUITY'] \/ dataset['AMT_INCOME_TOTAL']\ndataset['ANNUITY_CREDIT'] = dataset['AMT_ANNUITY'] \/ dataset['AMT_CREDIT']","3e94a30f":"# let's look at the correlations of the new variables we created along with TARGET\ng = sns.heatmap(dataset[['TARGET','EMPLOY_AGE','INCOME_AGE','CREDIT_AGE','CREDIT_INCOME','ANNUITY_INCOME','ANNUITY_CREDIT']].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","3faa530d":"# EMPLOY_AGE seems to be most correlated with TARGET of our new variables\n# we can plot EMPLOY_AGE relative to TARGET using KDE\nplt.figure(figsize = (8, 6))\nsns.kdeplot(dataset.loc[dataset['TARGET'] == 0, 'EMPLOY_AGE'], label = 'TARGET == 0')\nsns.kdeplot(dataset.loc[dataset['TARGET'] == 1, 'EMPLOY_AGE'], label = 'TARGET == 1')\nplt.xlabel('EMPLOY_AGE'); plt.ylabel('Density'); plt.title('KDE of EMPLOY_AGE');","2bf31f6c":"# the first file we will investigate is bureau\nbureau = pd.read_csv('..\/input\/bureau.csv')\nbureau.head()","f7ee0dc5":"print('bureau data shape: ', bureau.shape)","8cf63fc3":"bureau.describe()","6c344040":"# the first item to look at is how many records are in this for each applicant\nBUREAU_count = bureau.groupby('SK_ID_CURR', as_index=False)['SK_ID_BUREAU'].count().rename(columns = {'SK_ID_BUREAU': 'bureau_count'})\n(BUREAU_count['bureau_count']).describe()","e19df1d0":"dataset = dataset.merge(BUREAU_count, on = 'SK_ID_CURR', how = 'left')\ncorr = dataset['TARGET'].corr(dataset['bureau_count'])\nprint('%.4f' % corr)","9d6e12bb":"# review the data, and divide by -365.25 to turn this into positive years\n(bureau['DAYS_CREDIT'] \/ -365.25).describe()","41abe0cf":"(bureau['DAYS_CREDIT'] \/ -365.25).plot.hist(title = 'DAYS_CREDIT Histogram');\nplt.xlabel('DAYS_CREDIT')","98416038":"DAYS_CREDIT_max = bureau.groupby('SK_ID_CURR', as_index=False)['DAYS_CREDIT'].max().rename(columns = {'DAYS_CREDIT': 'bureau_DAYS_CREDIT_max'})\nDAYS_CREDIT_max.head()","bdea8594":"# merge with the dataset\ndataset = dataset.merge(DAYS_CREDIT_max, on = 'SK_ID_CURR', how = 'left')","0d840c1b":"# what is the correlation of our new variable with TARGET\ncorr = dataset['TARGET'].corr(dataset['bureau_DAYS_CREDIT_max'])\nprint('%.4f' % corr)","2afda85f":"# evaluate the new variable with a KDE plot\nplt.figure(figsize = (8, 6))\nsns.kdeplot(dataset.loc[dataset['TARGET'] == 0, 'bureau_DAYS_CREDIT_max'], label = 'TARGET == 0')\nsns.kdeplot(dataset.loc[dataset['TARGET'] == 1, 'bureau_DAYS_CREDIT_max'], label = 'TARGET == 1')\nplt.xlabel('bureau_DAYS_CREDIT_max'); plt.ylabel('Density'); plt.title('KDE of bureau_DAYS_CREDIT_max');","4fd5187a":"(bureau['CREDIT_DAY_OVERDUE']).describe([.25, .5, .75, .9, .99, .999])","1845b7fd":"# this looks like virtually all are zero, but there are some outliers\nbureau['CREDIT_DAY_OVERDUE'].plot.hist(title = 'CREDIT_DAY_OVERDUE Histogram');\nplt.xlabel('CREDIT_DAY_OVERDUE')","dfb01d8e":"# let's take the max of this variable\nCREDIT_DAY_OVERDUE_max = bureau.groupby('SK_ID_CURR', as_index=False)['CREDIT_DAY_OVERDUE'].max().rename(columns = {'CREDIT_DAY_OVERDUE': 'bureau_CREDIT_DAY_OVERDUE_max'})","02925b85":"(CREDIT_DAY_OVERDUE_max['bureau_CREDIT_DAY_OVERDUE_max']).describe([.25, .5, .75, .9, .99, .999])","4f773762":"# most of the data in this column is zero\n# how many non-zero items exist?\nCREDIT_DAY_OVERDUE_max[CREDIT_DAY_OVERDUE_max['bureau_CREDIT_DAY_OVERDUE_max'] > 0].count()","235b084a":"# let's turn this into a flag, since 99% of the data is zero\nCREDIT_DAY_OVERDUE_max['bureau_CREDIT_DAY_OVERDUE_max_flag'] = CREDIT_DAY_OVERDUE_max['bureau_CREDIT_DAY_OVERDUE_max'].where(CREDIT_DAY_OVERDUE_max['bureau_CREDIT_DAY_OVERDUE_max']==0,other=1)","707c95ca":"# drop the max variable and merge in the flag\nCREDIT_DAY_OVERDUE_max = CREDIT_DAY_OVERDUE_max.drop('bureau_CREDIT_DAY_OVERDUE_max', axis=1)\ndataset = dataset.merge(CREDIT_DAY_OVERDUE_max, on = 'SK_ID_CURR', how = 'left')","c95f0aab":"corr = dataset['TARGET'].corr(dataset['bureau_CREDIT_DAY_OVERDUE_max_flag'])\nprint('%.4f' % corr)","27c8be91":"(bureau['DAYS_CREDIT_ENDDATE']).describe()","4d23c836":"bureau['DAYS_CREDIT_ENDDATE'].plot.hist(title = 'DAYS_CREDIT_ENDDATE Histogram');\nplt.xlabel('DAYS_CREDIT_ENDDATE')","e739279e":"# let's take the max of this variable\nDAYS_CREDIT_ENDDATE_max = bureau.groupby('SK_ID_CURR', as_index=False)['DAYS_CREDIT_ENDDATE'].max().rename(columns = {'DAYS_CREDIT_ENDDATE': 'bureau_DAYS_CREDIT_ENDDATE_max'})\n(DAYS_CREDIT_ENDDATE_max['bureau_DAYS_CREDIT_ENDDATE_max']).describe()","bff40a04":"# it appears that we have a few outliers around -41875\nDAYS_CREDIT_ENDDATE_max['bureau_DAYS_CREDIT_ENDDATE_max_outlier'] = DAYS_CREDIT_ENDDATE_max['bureau_DAYS_CREDIT_ENDDATE_max'] < -10000\nfor i in DAYS_CREDIT_ENDDATE_max['bureau_DAYS_CREDIT_ENDDATE_max']:\n    if i < -10000:\n        DAYS_CREDIT_ENDDATE_max['bureau_DAYS_CREDIT_ENDDATE_max'].replace({i: np.nan}, inplace = True)","4af7fdbc":"# merge both our max variable and outlier flag into the dataset\ndataset = dataset.merge(DAYS_CREDIT_ENDDATE_max, on = 'SK_ID_CURR', how = 'left')","ece89151":"corr = dataset['TARGET'].corr(dataset['bureau_DAYS_CREDIT_ENDDATE_max'])\nprint('%.4f' % corr)","da040ab0":"(bureau['DAYS_ENDDATE_FACT'] \/ -365.25).describe()","bcc80685":"(bureau['DAYS_ENDDATE_FACT'] \/ -365.25).plot.hist(title = 'DAYS_ENDDATE_FACT Histogram');\nplt.xlabel('DAYS_ENDDATE_FACT')","5f9f74ec":"# let's take the average of this variable\nDAYS_ENDDATE_FACT_mean = bureau.groupby('SK_ID_CURR', as_index=False)['DAYS_ENDDATE_FACT'].mean().rename(columns = {'DAYS_ENDDATE_FACT': 'bureau_DAYS_ENDDATE_FACT_mean'})\n(DAYS_ENDDATE_FACT_mean['bureau_DAYS_ENDDATE_FACT_mean']).describe()","bafd9943":"(DAYS_ENDDATE_FACT_mean['bureau_DAYS_ENDDATE_FACT_mean']).plot.hist(title = 'bureau_DAYS_ENDDATE_FACT_mean Histogram');\nplt.xlabel('bureau_DAYS_ENDDATE_FACT_mean')","55a11d9e":"# it appears that we have a few outliers around -8000 days that we can handle\nDAYS_ENDDATE_FACT_mean['bureau_DAYS_ENDDATE_FACT_mean_outlier'] = DAYS_ENDDATE_FACT_mean['bureau_DAYS_ENDDATE_FACT_mean'] < -4000\nfor i in DAYS_ENDDATE_FACT_mean['bureau_DAYS_ENDDATE_FACT_mean']:\n    if i < -4000:\n        DAYS_ENDDATE_FACT_mean['bureau_DAYS_ENDDATE_FACT_mean'].replace({i: np.nan}, inplace = True)","5a0c6521":"# merge both our mean variable and outlier flag into the dataset\ndataset = dataset.merge(DAYS_ENDDATE_FACT_mean, on = 'SK_ID_CURR', how = 'left')","476b6c00":"corr = dataset['TARGET'].corr(dataset['bureau_DAYS_ENDDATE_FACT_mean'])\nprint('%.4f' % corr)","9f67b630":"# evaluate the new variable with a KDE plot\nplt.figure(figsize = (8, 6))\nsns.kdeplot(dataset.loc[dataset['TARGET'] == 0, 'bureau_DAYS_ENDDATE_FACT_mean'], label = 'TARGET == 0')\nsns.kdeplot(dataset.loc[dataset['TARGET'] == 1, 'bureau_DAYS_ENDDATE_FACT_mean'], label = 'TARGET == 1')\nplt.xlabel('bureau_DAYS_ENDDATE_FACT_mean'); plt.ylabel('Density'); plt.title('KDE of bureau_DAYS_ENDDATE_FACT_mean');","75495c27":"(bureau['AMT_CREDIT_MAX_OVERDUE']).describe()","b014fa68":"# let's take the max of this variable\nAMT_CREDIT_MAX_OVERDUE_max = bureau.groupby('SK_ID_CURR', as_index=False)['AMT_CREDIT_MAX_OVERDUE'].max().rename(columns = {'AMT_CREDIT_MAX_OVERDUE': 'bureau_AMT_CREDIT_MAX_OVERDUE_max'})\n(AMT_CREDIT_MAX_OVERDUE_max['bureau_AMT_CREDIT_MAX_OVERDUE_max']).describe()","64da7482":"# I'm also curious on the average of this variable\nAMT_CREDIT_MAX_OVERDUE_mean = bureau.groupby('SK_ID_CURR', as_index=False)['AMT_CREDIT_MAX_OVERDUE'].mean().rename(columns = {'AMT_CREDIT_MAX_OVERDUE': 'bureau_AMT_CREDIT_MAX_OVERDUE_mean'})\n(AMT_CREDIT_MAX_OVERDUE_mean['bureau_AMT_CREDIT_MAX_OVERDUE_mean']).describe()","e039146f":"# I'm not sure which of these two variables may work better in this case, so let's bring them both into the dataset for now\ndataset = dataset.merge(AMT_CREDIT_MAX_OVERDUE_max, on = 'SK_ID_CURR', how = 'left')\ndataset = dataset.merge(AMT_CREDIT_MAX_OVERDUE_mean, on = 'SK_ID_CURR', how = 'left')","c2a35482":"corr_max = dataset['TARGET'].corr(dataset['bureau_AMT_CREDIT_MAX_OVERDUE_max'])\ncorr_mean = dataset['TARGET'].corr(dataset['bureau_AMT_CREDIT_MAX_OVERDUE_mean'])\nprint('correlation for max variable: %.4f' % corr_max)\nprint('correlation for mean variable: %.4f' % corr_mean)","96439e7c":"(bureau['CNT_CREDIT_PROLONG']).describe()","a55dfdb1":"# since these are counts, let's sum this variable\nCNT_CREDIT_PROLONG_sum = bureau.groupby('SK_ID_CURR', as_index=False)['CNT_CREDIT_PROLONG'].sum().rename(columns = {'CNT_CREDIT_PROLONG': 'bureau_CNT_CREDIT_PROLONG_sum'})\n(CNT_CREDIT_PROLONG_sum['bureau_CNT_CREDIT_PROLONG_sum']).describe()","aa21f96d":"# merge into our dataset\ndataset = dataset.merge(CNT_CREDIT_PROLONG_sum, on = 'SK_ID_CURR', how = 'left')","a5e883c5":"corr = dataset['TARGET'].corr(dataset['bureau_CNT_CREDIT_PROLONG_sum'])\nprint('%.4f' % corr)","a56c2ba1":"(bureau['AMT_CREDIT_SUM']).describe()","a13e485d":"# since these are amounts, let's average this variable\nAMT_CREDIT_SUM_mean = bureau.groupby('SK_ID_CURR', as_index=False)['AMT_CREDIT_SUM'].mean().rename(columns = {'AMT_CREDIT_SUM': 'bureau_AMT_CREDIT_SUM_mean'})\n(AMT_CREDIT_SUM_mean['bureau_AMT_CREDIT_SUM_mean']).describe()","dcab94e2":"# merge into our dataset\ndataset = dataset.merge(AMT_CREDIT_SUM_mean, on = 'SK_ID_CURR', how = 'left')","96057a97":"corr = dataset['TARGET'].corr(dataset['bureau_AMT_CREDIT_SUM_mean'])\nprint('%.4f' % corr)","f7572d6f":"(bureau['AMT_CREDIT_SUM_DEBT']).describe()","70f24619":"# since these are amounts, let's average this variable\nAMT_CREDIT_SUM_DEBT_mean = bureau.groupby('SK_ID_CURR', as_index=False)['AMT_CREDIT_SUM_DEBT'].mean().rename(columns = {'AMT_CREDIT_SUM_DEBT': 'bureau_AMT_CREDIT_SUM_DEBT_mean'})\n(AMT_CREDIT_SUM_DEBT_mean['bureau_AMT_CREDIT_SUM_DEBT_mean']).describe()","8f81093f":"# merge into our dataset and look at the correlation\ndataset = dataset.merge(AMT_CREDIT_SUM_DEBT_mean, on = 'SK_ID_CURR', how = 'left')\ncorr = dataset['TARGET'].corr(dataset['bureau_AMT_CREDIT_SUM_DEBT_mean'])\nprint('%.4f' % corr)","084b86f4":"(bureau['AMT_CREDIT_SUM_LIMIT']).describe()","29a1290e":"# since these are amounts, let's average this variable\nAMT_CREDIT_SUM_LIMIT_mean = bureau.groupby('SK_ID_CURR', as_index=False)['AMT_CREDIT_SUM_LIMIT'].mean().rename(columns = {'AMT_CREDIT_SUM_LIMIT': 'bureau_AMT_CREDIT_SUM_LIMIT_mean'})\n(AMT_CREDIT_SUM_LIMIT_mean['bureau_AMT_CREDIT_SUM_LIMIT_mean']).describe()","7f34c419":"# merge into our dataset and look at the correlation\ndataset = dataset.merge(AMT_CREDIT_SUM_LIMIT_mean, on = 'SK_ID_CURR', how = 'left')\ncorr = dataset['TARGET'].corr(dataset['bureau_AMT_CREDIT_SUM_LIMIT_mean'])\nprint('%.4f' % corr)","511e9fc6":"(bureau['AMT_CREDIT_SUM_OVERDUE']).describe()","60ba80a4":"# since these are amounts, let's average this variable\nAMT_CREDIT_SUM_OVERDUE_mean = bureau.groupby('SK_ID_CURR', as_index=False)['AMT_CREDIT_SUM_OVERDUE'].mean().rename(columns = {'AMT_CREDIT_SUM_OVERDUE': 'bureau_AMT_CREDIT_SUM_OVERDUE_mean'})\n(AMT_CREDIT_SUM_OVERDUE_mean['bureau_AMT_CREDIT_SUM_OVERDUE_mean']).describe()","6bacfe1f":"# merge into our dataset and look at the correlation\ndataset = dataset.merge(AMT_CREDIT_SUM_OVERDUE_mean, on = 'SK_ID_CURR', how = 'left')\ncorr = dataset['TARGET'].corr(dataset['bureau_AMT_CREDIT_SUM_OVERDUE_mean'])\nprint('%.4f' % corr)","32ad6b64":"# divide by -365.25 to turn this into positive years\n(bureau['DAYS_CREDIT_UPDATE'] \/ -365.25).describe()","a8544507":"# since this is a days variable, let's use max\nDAYS_CREDIT_UPDATE_max = bureau.groupby('SK_ID_CURR', as_index=False)['DAYS_CREDIT_UPDATE'].max().rename(columns = {'DAYS_CREDIT_UPDATE': 'bureau_DAYS_CREDIT_UPDATE_max'})\n(DAYS_CREDIT_UPDATE_max['bureau_DAYS_CREDIT_UPDATE_max']).describe()","f0b2f6f7":"# merge into our dataset and look at the correlation\ndataset = dataset.merge(DAYS_CREDIT_UPDATE_max, on = 'SK_ID_CURR', how = 'left')\ncorr = dataset['TARGET'].corr(dataset['bureau_DAYS_CREDIT_UPDATE_max'])\nprint('%.4f' % corr)","debc9b05":"(bureau['AMT_ANNUITY']).describe()","45f62def":"# since these are amounts, let's average this variable\nAMT_ANNUITY_mean = bureau.groupby('SK_ID_CURR', as_index=False)['AMT_ANNUITY'].mean().rename(columns = {'AMT_ANNUITY': 'bureau_AMT_ANNUITY_mean'})\n(AMT_ANNUITY_mean['bureau_AMT_ANNUITY_mean']).describe()","57f1283c":"# merge into our dataset and look at the correlation\ndataset = dataset.merge(AMT_ANNUITY_mean, on = 'SK_ID_CURR', how = 'left')\ncorr = dataset['TARGET'].corr(dataset['bureau_AMT_ANNUITY_mean'])\nprint('%.4f' % corr)","534ca24e":"bureau.describe(include=[np.object])","0e7ac3c1":"# let's use one-hot encoding on these variables\nbureau_cats = pd.get_dummies(bureau.select_dtypes('object'))\nbureau_cats['SK_ID_CURR'] = bureau['SK_ID_CURR']\nbureau_cats.head()","63857141":"bureau_cats_grouped = bureau_cats.groupby('SK_ID_CURR').agg('sum')\nbureau_cats_grouped.head()","62705391":"#merge into our dataset\ndataset = dataset.merge(bureau_cats_grouped, on = 'SK_ID_CURR', right_index = True, how = 'left')\ndataset.head()","47d87f7d":"# the next file we will investigate is bureau_balance\nbureau_balance = pd.read_csv('..\/input\/bureau_balance.csv')\nbureau_balance.head()","85a0f04d":"# this appears to be the number of months of balance relative to the application date\n# let's start with the count\nMONTHS_BALANCE_count = bureau_balance.groupby('SK_ID_BUREAU', as_index=False)['MONTHS_BALANCE'].count().rename(columns = {'MONTHS_BALANCE': 'bureau_bal_MONTHS_BALANCE_count'})\n(MONTHS_BALANCE_count['bureau_bal_MONTHS_BALANCE_count']).describe()","bfdc0dae":"# let's also look at the mean\nMONTHS_BALANCE_mean = bureau_balance.groupby('SK_ID_BUREAU', as_index=False)['MONTHS_BALANCE'].mean().rename(columns = {'MONTHS_BALANCE': 'bureau_bal_MONTHS_BALANCE_mean'})\n(MONTHS_BALANCE_mean['bureau_bal_MONTHS_BALANCE_mean']).describe()","5ecc4bee":"MONTHS_BAL = MONTHS_BALANCE_mean.merge(MONTHS_BALANCE_count, on = 'SK_ID_BUREAU', right_index = True, how = 'inner')\nMONTHS_BAL.head()","590d4237":"# now let's get our categoricals\nbureau_bal_cats = pd.get_dummies(bureau_balance.select_dtypes('object'))\nbureau_bal_cats['SK_ID_BUREAU'] = bureau_balance['SK_ID_BUREAU']\nbureau_bal_cats.head()","c5f5dcd4":"bureau_bal_cats_grouped = bureau_bal_cats.groupby('SK_ID_BUREAU').agg('sum')\nbureau_bal_cats_grouped.head()","93af6b3b":"# now let's merge the MONTHS_BAL with our categoricals by SK_ID_BUREAU, then merge with bureau to add in SK_ID_CURR\nbureau_bal_merged = MONTHS_BAL.merge(bureau_bal_cats_grouped, right_index = True, left_on = 'SK_ID_BUREAU', how = 'outer')\nbureau_bal_merged = bureau_bal_merged.merge(bureau[['SK_ID_BUREAU', 'SK_ID_CURR']], on = 'SK_ID_BUREAU', how = 'left')\nbureau_bal_merged.head()","b7994cdb":"bureau_bal_MONTHS_BALANCE_mean_mean = bureau_bal_merged.groupby('SK_ID_CURR', as_index=False)['bureau_bal_MONTHS_BALANCE_mean'].mean().rename(columns = {'bureau_bal_MONTHS_BALANCE_mean': 'bureau_bal_MONTHS_BALANCE_mean_mean'})\ndataset = dataset.merge(bureau_bal_MONTHS_BALANCE_mean_mean, on = 'SK_ID_CURR', how = 'left')\ncorr = dataset['TARGET'].corr(dataset['bureau_bal_MONTHS_BALANCE_mean_mean'])\nprint('%.4f' % corr)","adbb1ff9":"bureau_bal_MONTHS_BALANCE_count_mean = bureau_bal_merged.groupby('SK_ID_CURR', as_index=False)['bureau_bal_MONTHS_BALANCE_count'].mean().rename(columns = {'bureau_bal_MONTHS_BALANCE_count': 'bureau_bal_MONTHS_BALANCE_count_mean'})\ndataset = dataset.merge(bureau_bal_MONTHS_BALANCE_count_mean, on = 'SK_ID_CURR', how = 'left')\ncorr = dataset['TARGET'].corr(dataset['bureau_bal_MONTHS_BALANCE_count_mean'])\nprint('%.4f' % corr)","ec68353f":"bureau_bal_STATUS_0_mean = bureau_bal_merged.groupby('SK_ID_CURR', as_index=False)['STATUS_0'].mean().rename(columns = {'STATUS_0': 'bureau_bal_STATUS_0_mean'})\ndataset = dataset.merge(bureau_bal_STATUS_0_mean, on = 'SK_ID_CURR', how = 'left')\ncorr = dataset['TARGET'].corr(dataset['bureau_bal_STATUS_0_mean'])\nprint('%.4f' % corr)","733c9753":"bureau_bal_STATUS_1_mean = bureau_bal_merged.groupby('SK_ID_CURR', as_index=False)['STATUS_1'].mean().rename(columns = {'STATUS_1': 'bureau_bal_STATUS_1_mean'})\ndataset = dataset.merge(bureau_bal_STATUS_1_mean, on = 'SK_ID_CURR', how = 'left')\ncorr = dataset['TARGET'].corr(dataset['bureau_bal_STATUS_1_mean'])\nprint('%.4f' % corr)","1b368448":"bureau_bal_STATUS_2_mean = bureau_bal_merged.groupby('SK_ID_CURR', as_index=False)['STATUS_2'].mean().rename(columns = {'STATUS_2': 'bureau_bal_STATUS_2_mean'})\ndataset = dataset.merge(bureau_bal_STATUS_2_mean, on = 'SK_ID_CURR', how = 'left')\ncorr = dataset['TARGET'].corr(dataset['bureau_bal_STATUS_2_mean'])\nprint('%.4f' % corr)","7afba59b":"bureau_bal_STATUS_3_mean = bureau_bal_merged.groupby('SK_ID_CURR', as_index=False)['STATUS_3'].mean().rename(columns = {'STATUS_3': 'bureau_bal_STATUS_3_mean'})\ndataset = dataset.merge(bureau_bal_STATUS_3_mean, on = 'SK_ID_CURR', how = 'left')\ncorr = dataset['TARGET'].corr(dataset['bureau_bal_STATUS_3_mean'])\nprint('%.4f' % corr)","4d50820e":"bureau_bal_STATUS_4_mean = bureau_bal_merged.groupby('SK_ID_CURR', as_index=False)['STATUS_4'].mean().rename(columns = {'STATUS_4': 'bureau_bal_STATUS_4_mean'})\ndataset = dataset.merge(bureau_bal_STATUS_4_mean, on = 'SK_ID_CURR', how = 'left')\ncorr = dataset['TARGET'].corr(dataset['bureau_bal_STATUS_4_mean'])\nprint('%.4f' % corr)","0680d193":"bureau_bal_STATUS_5_mean = bureau_bal_merged.groupby('SK_ID_CURR', as_index=False)['STATUS_5'].mean().rename(columns = {'STATUS_5': 'bureau_bal_STATUS_5_mean'})\ndataset = dataset.merge(bureau_bal_STATUS_5_mean, on = 'SK_ID_CURR', how = 'left')\ncorr = dataset['TARGET'].corr(dataset['bureau_bal_STATUS_5_mean'])\nprint('%.4f' % corr)","5099d4d2":"bureau_bal_STATUS_C_mean = bureau_bal_merged.groupby('SK_ID_CURR', as_index=False)['STATUS_C'].mean().rename(columns = {'STATUS_C': 'bureau_bal_STATUS_C_mean'})\ndataset = dataset.merge(bureau_bal_STATUS_C_mean, on = 'SK_ID_CURR', how = 'left')\ncorr = dataset['TARGET'].corr(dataset['bureau_bal_STATUS_C_mean'])\nprint('%.4f' % corr)","55a94ce9":"bureau_bal_STATUS_X_mean = bureau_bal_merged.groupby('SK_ID_CURR', as_index=False)['STATUS_X'].mean().rename(columns = {'STATUS_X': 'bureau_bal_STATUS_X_mean'})\ndataset = dataset.merge(bureau_bal_STATUS_X_mean, on = 'SK_ID_CURR', how = 'left')\ncorr = dataset['TARGET'].corr(dataset['bureau_bal_STATUS_X_mean'])\nprint('%.4f' % corr)","1155c2e3":"dataset.head()","cbdd66be":"# let's free up some memory by deleting some of the dataframes we are done with\ngc.enable()\ndel bureau, BUREAU_count, DAYS_CREDIT_max, CREDIT_DAY_OVERDUE_max, DAYS_CREDIT_ENDDATE_max, DAYS_ENDDATE_FACT_mean, AMT_CREDIT_MAX_OVERDUE_max, \nAMT_CREDIT_MAX_OVERDUE_mean, CNT_CREDIT_PROLONG_sum, AMT_CREDIT_SUM_mean, AMT_CREDIT_SUM_DEBT_mean, AMT_CREDIT_SUM_LIMIT_mean, AMT_CREDIT_SUM_OVERDUE_mean, \nDAYS_CREDIT_UPDATE_max, AMT_ANNUITY_mean, bureau_cats, bureau_cats_grouped, bureau_balance, MONTHS_BALANCE_count, MONTHS_BALANCE_mean, MONTHS_BAL, bureau_bal_cats, \nbureau_bal_cats_grouped, bureau_bal_merged, bureau_bal_MONTHS_BALANCE_mean_mean, bureau_bal_MONTHS_BALANCE_count_mean, bureau_bal_STATUS_0_mean, \nbureau_bal_STATUS_1_mean, bureau_bal_STATUS_2_mean, bureau_bal_STATUS_3_mean, bureau_bal_STATUS_4_mean, bureau_bal_STATUS_5_mean, bureau_bal_STATUS_C_mean, \nbureau_bal_STATUS_X_mean\ngc.collect()","b0c8233f":"# the next file we will investigate is credit_card_balance\ncredit = pd.read_csv('..\/input\/credit_card_balance.csv')\ncredit.head()","8789c044":"credit_stats_by_prev = credit[['SK_ID_PREV', 'SK_ID_CURR']]","b1982772":"credit_MONTHS_BALANCE_count = credit.groupby('SK_ID_PREV', as_index=False)['MONTHS_BALANCE'].count().rename(columns = {'MONTHS_BALANCE': 'credit_MONTHS_BALANCE_count'})\ncredit_MONTHS_BALANCE_mean = credit.groupby('SK_ID_PREV', as_index=False)['MONTHS_BALANCE'].mean().rename(columns = {'MONTHS_BALANCE': 'credit_MONTHS_BALANCE_mean'})","acf0d641":"credit_stats_by_prev = credit_stats_by_prev.merge(credit_MONTHS_BALANCE_count, on = 'SK_ID_PREV', how = 'left')\ncredit_stats_by_prev = credit_stats_by_prev.merge(credit_MONTHS_BALANCE_mean, on = 'SK_ID_PREV', how = 'left')\ncredit_stats_by_prev.head()","29b9c50a":"gc.enable()\ndel credit_MONTHS_BALANCE_count, credit_MONTHS_BALANCE_mean\ngc.collect()","a4a1dfb2":"credit_AMT_BALANCE_mean = credit.groupby('SK_ID_PREV', as_index=False)['AMT_BALANCE'].mean().rename(columns = {'AMT_BALANCE': 'credit_AMT_BALANCE_mean'})\ncredit_stats_by_prev = credit_stats_by_prev.merge(credit_AMT_BALANCE_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel credit_AMT_BALANCE_mean\ngc.collect()","c6ecf014":"credit_AMT_CREDIT_LIMIT_ACTUAL_mean = credit.groupby('SK_ID_PREV', as_index=False)['AMT_CREDIT_LIMIT_ACTUAL'].mean().rename(columns = {'AMT_CREDIT_LIMIT_ACTUAL': 'credit_AMT_CREDIT_LIMIT_ACTUAL_mean'})\ncredit_stats_by_prev = credit_stats_by_prev.merge(credit_AMT_CREDIT_LIMIT_ACTUAL_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel credit_AMT_CREDIT_LIMIT_ACTUAL_mean\ngc.collect()","dce85cda":"credit_AMT_DRAWINGS_ATM_CURRENT_mean = credit.groupby('SK_ID_PREV', as_index=False)['AMT_DRAWINGS_ATM_CURRENT'].mean().rename(columns = {'AMT_DRAWINGS_ATM_CURRENT': 'credit_AMT_DRAWINGS_ATM_CURRENT_mean'})\ncredit_stats_by_prev = credit_stats_by_prev.merge(credit_AMT_DRAWINGS_ATM_CURRENT_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel credit_AMT_DRAWINGS_ATM_CURRENT_mean\ngc.collect()","896f389b":"credit_AMT_DRAWINGS_CURRENT_mean = credit.groupby('SK_ID_PREV', as_index=False)['AMT_DRAWINGS_CURRENT'].mean().rename(columns = {'AMT_DRAWINGS_CURRENT': 'credit_AMT_DRAWINGS_CURRENT_mean'})\ncredit_stats_by_prev = credit_stats_by_prev.merge(credit_AMT_DRAWINGS_CURRENT_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel credit_AMT_DRAWINGS_CURRENT_mean\ngc.collect()","c8a932b3":"credit_AMT_DRAWINGS_OTHER_CURRENT_mean = credit.groupby('SK_ID_PREV', as_index=False)['AMT_DRAWINGS_OTHER_CURRENT'].mean().rename(columns = {'AMT_DRAWINGS_OTHER_CURRENT': 'credit_AMT_DRAWINGS_OTHER_CURRENT_mean'})\ncredit_stats_by_prev = credit_stats_by_prev.merge(credit_AMT_DRAWINGS_OTHER_CURRENT_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel credit_AMT_DRAWINGS_OTHER_CURRENT_mean\ngc.collect()","719a46f4":"credit_AMT_DRAWINGS_POS_CURRENT_mean = credit.groupby('SK_ID_PREV', as_index=False)['AMT_DRAWINGS_POS_CURRENT'].mean().rename(columns = {'AMT_DRAWINGS_POS_CURRENT': 'credit_AMT_DRAWINGS_POS_CURRENT_mean'})\ncredit_stats_by_prev = credit_stats_by_prev.merge(credit_AMT_DRAWINGS_POS_CURRENT_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel credit_AMT_DRAWINGS_POS_CURRENT_mean\ngc.collect()","4ba3680b":"credit_AMT_INST_MIN_REGULARITY_mean = credit.groupby('SK_ID_PREV', as_index=False)['AMT_INST_MIN_REGULARITY'].mean().rename(columns = {'AMT_INST_MIN_REGULARITY': 'credit_AMT_INST_MIN_REGULARITY_mean'})\ncredit_stats_by_prev = credit_stats_by_prev.merge(credit_AMT_INST_MIN_REGULARITY_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel credit_AMT_INST_MIN_REGULARITY_mean\ngc.collect()","7ed6ad65":"credit_AMT_PAYMENT_CURRENT_mean = credit.groupby('SK_ID_PREV', as_index=False)['AMT_PAYMENT_CURRENT'].mean().rename(columns = {'AMT_PAYMENT_CURRENT': 'credit_AMT_PAYMENT_CURRENT_mean'})\ncredit_stats_by_prev = credit_stats_by_prev.merge(credit_AMT_PAYMENT_CURRENT_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel credit_AMT_PAYMENT_CURRENT_mean\ngc.collect()","29396d2a":"credit_AMT_PAYMENT_TOTAL_CURRENT_mean = credit.groupby('SK_ID_PREV', as_index=False)['AMT_PAYMENT_TOTAL_CURRENT'].mean().rename(columns = {'AMT_PAYMENT_TOTAL_CURRENT': 'credit_AMT_PAYMENT_TOTAL_CURRENT_mean'})\ncredit_stats_by_prev = credit_stats_by_prev.merge(credit_AMT_PAYMENT_TOTAL_CURRENT_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel credit_AMT_PAYMENT_TOTAL_CURRENT_mean\ngc.collect()","24683b4c":"credit_AMT_RECEIVABLE_PRINCIPAL_mean = credit.groupby('SK_ID_PREV', as_index=False)['AMT_RECEIVABLE_PRINCIPAL'].mean().rename(columns = {'AMT_RECEIVABLE_PRINCIPAL': 'credit_AMT_RECEIVABLE_PRINCIPAL_mean'})\ncredit_stats_by_prev = credit_stats_by_prev.merge(credit_AMT_RECEIVABLE_PRINCIPAL_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel credit_AMT_RECEIVABLE_PRINCIPAL_mean\ngc.collect()","c2467338":"credit_AMT_RECIVABLE_mean = credit.groupby('SK_ID_PREV', as_index=False)['AMT_RECIVABLE'].mean().rename(columns = {'AMT_RECIVABLE': 'credit_AMT_RECIVABLE_mean'})\ncredit_stats_by_prev = credit_stats_by_prev.merge(credit_AMT_RECIVABLE_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel credit_AMT_RECIVABLE_mean\ngc.collect()","ead0f4be":"credit_AMT_TOTAL_RECEIVABLE_mean = credit.groupby('SK_ID_PREV', as_index=False)['AMT_TOTAL_RECEIVABLE'].mean().rename(columns = {'AMT_TOTAL_RECEIVABLE': 'credit_AMT_TOTAL_RECEIVABLE_mean'})\ncredit_stats_by_prev = credit_stats_by_prev.merge(credit_AMT_TOTAL_RECEIVABLE_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel credit_AMT_TOTAL_RECEIVABLE_mean\ngc.collect()","2ebed5c4":"credit_CNT_DRAWINGS_ATM_CURRENT_mean = credit.groupby('SK_ID_PREV', as_index=False)['CNT_DRAWINGS_ATM_CURRENT'].mean().rename(columns = {'CNT_DRAWINGS_ATM_CURRENT': 'credit_CNT_DRAWINGS_ATM_CURRENT_mean'})\ncredit_stats_by_prev = credit_stats_by_prev.merge(credit_CNT_DRAWINGS_ATM_CURRENT_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel credit_CNT_DRAWINGS_ATM_CURRENT_mean\ngc.collect()","e99743c0":"credit_CNT_DRAWINGS_CURRENT_mean = credit.groupby('SK_ID_PREV', as_index=False)['CNT_DRAWINGS_CURRENT'].mean().rename(columns = {'CNT_DRAWINGS_CURRENT': 'credit_CNT_DRAWINGS_CURRENT_mean'})\ncredit_stats_by_prev = credit_stats_by_prev.merge(credit_CNT_DRAWINGS_CURRENT_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel credit_CNT_DRAWINGS_CURRENT_mean\ngc.collect()","f74d5e6d":"credit_CNT_DRAWINGS_OTHER_CURRENT_mean = credit.groupby('SK_ID_PREV', as_index=False)['CNT_DRAWINGS_OTHER_CURRENT'].mean().rename(columns = {'CNT_DRAWINGS_OTHER_CURRENT': 'credit_CNT_DRAWINGS_OTHER_CURRENT_mean'})\ncredit_stats_by_prev = credit_stats_by_prev.merge(credit_CNT_DRAWINGS_OTHER_CURRENT_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel credit_CNT_DRAWINGS_OTHER_CURRENT_mean\ngc.collect()","b88c0e4a":"credit_CNT_DRAWINGS_POS_CURRENT_mean = credit.groupby('SK_ID_PREV', as_index=False)['CNT_DRAWINGS_POS_CURRENT'].mean().rename(columns = {'CNT_DRAWINGS_POS_CURRENT': 'credit_CNT_DRAWINGS_POS_CURRENT_mean'})\ncredit_stats_by_prev = credit_stats_by_prev.merge(credit_CNT_DRAWINGS_POS_CURRENT_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel credit_CNT_DRAWINGS_POS_CURRENT_mean\ngc.collect()","074c8b82":"credit_CNT_INSTALMENT_MATURE_CUM_mean = credit.groupby('SK_ID_PREV', as_index=False)['CNT_INSTALMENT_MATURE_CUM'].mean().rename(columns = {'CNT_INSTALMENT_MATURE_CUM': 'credit_CNT_INSTALMENT_MATURE_CUM_mean'})\ncredit_stats_by_prev = credit_stats_by_prev.merge(credit_CNT_INSTALMENT_MATURE_CUM_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel credit_CNT_INSTALMENT_MATURE_CUM_mean\ngc.collect()","3950793e":"credit_SK_DPD_mean = credit.groupby('SK_ID_PREV', as_index=False)['SK_DPD'].mean().rename(columns = {'SK_DPD': 'credit_SK_DPD_mean'})\ncredit_stats_by_prev = credit_stats_by_prev.merge(credit_SK_DPD_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel credit_SK_DPD_mean\ngc.collect()","19632bed":"credit_SK_DPD_DEF_mean = credit.groupby('SK_ID_PREV', as_index=False)['SK_DPD_DEF'].mean().rename(columns = {'SK_DPD_DEF': 'credit_SK_DPD_DEF_mean'})\ncredit_stats_by_prev = credit_stats_by_prev.merge(credit_SK_DPD_DEF_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel credit_SK_DPD_DEF_mean\ngc.collect()","450b8b2a":"# now let's deal with our one categorical variable, NAME_CONTRACT_STATUS\ncredit_cats = pd.get_dummies(credit.select_dtypes('object'))\ncredit_cats['SK_ID_PREV'] = credit['SK_ID_PREV']\ncredit_cats.head()","e56b6b99":"credit_cats_grouped = credit_cats.groupby('SK_ID_PREV').agg('sum')\ncredit_cats_grouped.head()","76230bcd":"credit_stats_by_prev = credit_stats_by_prev.merge(credit_cats_grouped, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel credit_cats_grouped, credit_cats\ngc.collect()","bbcce378":"credit_stats_by_prev.head()","eb93ed16":"credit_MONTHS_BALANCE_count_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['credit_MONTHS_BALANCE_count'].mean().rename(columns = {'credit_MONTHS_BALANCE_count': 'credit_MONTHS_BALANCE_count_mean'})\ndataset = dataset.merge(credit_MONTHS_BALANCE_count_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_MONTHS_BALANCE_count_mean\ngc.collect()","44afb508":"credit_MONTHS_BALANCE_mean_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['credit_MONTHS_BALANCE_mean'].mean().rename(columns = {'credit_MONTHS_BALANCE_mean': 'credit_MONTHS_BALANCE_mean_mean'})\ndataset = dataset.merge(credit_MONTHS_BALANCE_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_MONTHS_BALANCE_mean_mean\ngc.collect()","1fa05e58":"credit_AMT_BALANCE_mean_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['credit_AMT_BALANCE_mean'].mean().rename(columns = {'credit_AMT_BALANCE_mean': 'credit_AMT_BALANCE_mean_mean'})\ndataset = dataset.merge(credit_AMT_BALANCE_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_AMT_BALANCE_mean_mean\ngc.collect()","db90eeb0":"credit_AMT_CREDIT_LIMIT_ACTUAL_mean_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['credit_AMT_CREDIT_LIMIT_ACTUAL_mean'].mean().rename(columns = {'credit_AMT_CREDIT_LIMIT_ACTUAL_mean': 'credit_AMT_CREDIT_LIMIT_ACTUAL_mean_mean'})\ndataset = dataset.merge(credit_AMT_CREDIT_LIMIT_ACTUAL_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_AMT_CREDIT_LIMIT_ACTUAL_mean_mean\ngc.collect()","c178e759":"credit_AMT_DRAWINGS_ATM_CURRENT_mean_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['credit_AMT_DRAWINGS_ATM_CURRENT_mean'].mean().rename(columns = {'credit_AMT_DRAWINGS_ATM_CURRENT_mean': 'credit_AMT_DRAWINGS_ATM_CURRENT_mean_mean'})\ndataset = dataset.merge(credit_AMT_DRAWINGS_ATM_CURRENT_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_AMT_DRAWINGS_ATM_CURRENT_mean_mean\ngc.collect()","c377804a":"credit_AMT_DRAWINGS_CURRENT_mean_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['credit_AMT_DRAWINGS_CURRENT_mean'].mean().rename(columns = {'credit_AMT_DRAWINGS_CURRENT_mean': 'credit_AMT_DRAWINGS_CURRENT_mean_mean'})\ndataset = dataset.merge(credit_AMT_DRAWINGS_CURRENT_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_AMT_DRAWINGS_CURRENT_mean_mean\ngc.collect()","e7a385c9":"credit_AMT_DRAWINGS_OTHER_CURRENT_mean_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['credit_AMT_DRAWINGS_OTHER_CURRENT_mean'].mean().rename(columns = {'credit_AMT_DRAWINGS_OTHER_CURRENT_mean': 'credit_AMT_DRAWINGS_OTHER_CURRENT_mean_mean'})\ndataset = dataset.merge(credit_AMT_DRAWINGS_OTHER_CURRENT_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_AMT_DRAWINGS_OTHER_CURRENT_mean_mean\ngc.collect()","9e78a28d":"credit_AMT_DRAWINGS_POS_CURRENT_mean_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['credit_AMT_DRAWINGS_POS_CURRENT_mean'].mean().rename(columns = {'credit_AMT_DRAWINGS_POS_CURRENT_mean': 'credit_AMT_DRAWINGS_POS_CURRENT_mean_mean'})\ndataset = dataset.merge(credit_AMT_DRAWINGS_POS_CURRENT_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_AMT_DRAWINGS_POS_CURRENT_mean_mean\ngc.collect()","0c5bf384":"credit_AMT_INST_MIN_REGULARITY_mean_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['credit_AMT_INST_MIN_REGULARITY_mean'].mean().rename(columns = {'credit_AMT_INST_MIN_REGULARITY_mean': 'credit_AMT_INST_MIN_REGULARITY_mean_mean'})\ndataset = dataset.merge(credit_AMT_INST_MIN_REGULARITY_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_AMT_INST_MIN_REGULARITY_mean_mean\ngc.collect()","728c62cb":"credit_AMT_PAYMENT_CURRENT_mean_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['credit_AMT_PAYMENT_CURRENT_mean'].mean().rename(columns = {'credit_AMT_PAYMENT_CURRENT_mean': 'credit_AMT_PAYMENT_CURRENT_mean_mean'})\ndataset = dataset.merge(credit_AMT_PAYMENT_CURRENT_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_AMT_PAYMENT_CURRENT_mean_mean\ngc.collect()","3ca895a6":"credit_AMT_PAYMENT_TOTAL_CURRENT_mean_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['credit_AMT_PAYMENT_TOTAL_CURRENT_mean'].mean().rename(columns = {'credit_AMT_PAYMENT_TOTAL_CURRENT_mean': 'credit_AMT_PAYMENT_TOTAL_CURRENT_mean_mean'})\ndataset = dataset.merge(credit_AMT_PAYMENT_TOTAL_CURRENT_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_AMT_PAYMENT_TOTAL_CURRENT_mean_mean\ngc.collect()","e36b51a9":"credit_AMT_RECEIVABLE_PRINCIPAL_mean_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['credit_AMT_RECEIVABLE_PRINCIPAL_mean'].mean().rename(columns = {'credit_AMT_RECEIVABLE_PRINCIPAL_mean': 'credit_AMT_RECEIVABLE_PRINCIPAL_mean_mean'})\ndataset = dataset.merge(credit_AMT_RECEIVABLE_PRINCIPAL_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_AMT_RECEIVABLE_PRINCIPAL_mean_mean\ngc.collect()","05b53da4":"credit_AMT_RECIVABLE_mean_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['credit_AMT_RECIVABLE_mean'].mean().rename(columns = {'credit_AMT_RECIVABLE_mean': 'credit_AMT_RECIVABLE_mean_mean'})\ndataset = dataset.merge(credit_AMT_RECIVABLE_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_AMT_RECIVABLE_mean_mean\ngc.collect()","a606a513":"credit_AMT_TOTAL_RECEIVABLE_mean_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['credit_AMT_TOTAL_RECEIVABLE_mean'].mean().rename(columns = {'credit_AMT_TOTAL_RECEIVABLE_mean': 'credit_AMT_TOTAL_RECEIVABLE_mean_mean'})\ndataset = dataset.merge(credit_AMT_TOTAL_RECEIVABLE_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_AMT_TOTAL_RECEIVABLE_mean_mean\ngc.collect()","28879cd4":"credit_CNT_DRAWINGS_ATM_CURRENT_mean_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['credit_CNT_DRAWINGS_ATM_CURRENT_mean'].mean().rename(columns = {'credit_CNT_DRAWINGS_ATM_CURRENT_mean': 'credit_CNT_DRAWINGS_ATM_CURRENT_mean_mean'})\ndataset = dataset.merge(credit_CNT_DRAWINGS_ATM_CURRENT_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_CNT_DRAWINGS_ATM_CURRENT_mean_mean\ngc.collect()","00221e51":"credit_CNT_DRAWINGS_CURRENT_mean_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['credit_CNT_DRAWINGS_CURRENT_mean'].mean().rename(columns = {'credit_CNT_DRAWINGS_CURRENT_mean': 'credit_CNT_DRAWINGS_CURRENT_mean_mean'})\ndataset = dataset.merge(credit_CNT_DRAWINGS_CURRENT_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_CNT_DRAWINGS_CURRENT_mean_mean\ngc.collect()","20d9669c":"credit_CNT_DRAWINGS_OTHER_CURRENT_mean_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['credit_CNT_DRAWINGS_OTHER_CURRENT_mean'].mean().rename(columns = {'credit_CNT_DRAWINGS_OTHER_CURRENT_mean': 'credit_CNT_DRAWINGS_OTHER_CURRENT_mean_mean'})\ndataset = dataset.merge(credit_CNT_DRAWINGS_OTHER_CURRENT_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_CNT_DRAWINGS_OTHER_CURRENT_mean_mean\ngc.collect()","1f7c9923":"credit_CNT_DRAWINGS_POS_CURRENT_mean_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['credit_CNT_DRAWINGS_POS_CURRENT_mean'].mean().rename(columns = {'credit_CNT_DRAWINGS_POS_CURRENT_mean': 'credit_CNT_DRAWINGS_POS_CURRENT_mean_mean'})\ndataset = dataset.merge(credit_CNT_DRAWINGS_POS_CURRENT_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_CNT_DRAWINGS_POS_CURRENT_mean_mean\ngc.collect()","f418f710":"credit_CNT_INSTALMENT_MATURE_CUM_mean_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['credit_CNT_INSTALMENT_MATURE_CUM_mean'].mean().rename(columns = {'credit_CNT_INSTALMENT_MATURE_CUM_mean': 'credit_CNT_INSTALMENT_MATURE_CUM_mean_mean'})\ndataset = dataset.merge(credit_CNT_INSTALMENT_MATURE_CUM_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_CNT_INSTALMENT_MATURE_CUM_mean_mean\ngc.collect()","0dc242d0":"credit_SK_DPD_mean_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['credit_SK_DPD_mean'].mean().rename(columns = {'credit_SK_DPD_mean': 'credit_SK_DPD_mean_mean'})\ndataset = dataset.merge(credit_SK_DPD_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_SK_DPD_mean_mean\ngc.collect()","0477ddbf":"credit_SK_DPD_DEF_mean_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['credit_SK_DPD_DEF_mean'].mean().rename(columns = {'credit_SK_DPD_DEF_mean': 'credit_SK_DPD_DEF_mean_mean'})\ndataset = dataset.merge(credit_SK_DPD_DEF_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_SK_DPD_DEF_mean_mean\ngc.collect()","ee2fdbc4":"credit_NAME_CONTRACT_STATUS_Active_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['NAME_CONTRACT_STATUS_Active'].mean().rename(columns = {'NAME_CONTRACT_STATUS_Active': 'credit_NAME_CONTRACT_STATUS_Active_mean'})\ndataset = dataset.merge(credit_NAME_CONTRACT_STATUS_Active_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_NAME_CONTRACT_STATUS_Active_mean\ngc.collect()","fa9450a0":"credit_NAME_CONTRACT_STATUS_Approved_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['NAME_CONTRACT_STATUS_Approved'].mean().rename(columns = {'NAME_CONTRACT_STATUS_Approved': 'credit_NAME_CONTRACT_STATUS_Approved_mean'})\ndataset = dataset.merge(credit_NAME_CONTRACT_STATUS_Approved_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_NAME_CONTRACT_STATUS_Approved_mean\ngc.collect()","8f7ec6fe":"credit_NAME_CONTRACT_STATUS_Completed_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['NAME_CONTRACT_STATUS_Completed'].mean().rename(columns = {'NAME_CONTRACT_STATUS_Completed': 'credit_NAME_CONTRACT_STATUS_Completed_mean'})\ndataset = dataset.merge(credit_NAME_CONTRACT_STATUS_Completed_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_NAME_CONTRACT_STATUS_Completed_mean\ngc.collect()","bb49229d":"credit_NAME_CONTRACT_STATUS_Demand_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['NAME_CONTRACT_STATUS_Demand'].mean().rename(columns = {'NAME_CONTRACT_STATUS_Demand': 'credit_NAME_CONTRACT_STATUS_Demand_mean'})\ndataset = dataset.merge(credit_NAME_CONTRACT_STATUS_Demand_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_NAME_CONTRACT_STATUS_Demand_mean\ngc.collect()","34146147":"credit_NAME_CONTRACT_STATUS_Refused_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['NAME_CONTRACT_STATUS_Refused'].mean().rename(columns = {'NAME_CONTRACT_STATUS_Refused': 'credit_NAME_CONTRACT_STATUS_Refused_mean'})\ndataset = dataset.merge(credit_NAME_CONTRACT_STATUS_Refused_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_NAME_CONTRACT_STATUS_Refused_mean\ngc.collect()","c4264aeb":"credit_NAME_CONTRACT_STATUS_Sent_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['NAME_CONTRACT_STATUS_Sent proposal'].mean().rename(columns = {'NAME_CONTRACT_STATUS_Sent proposal': 'credit_NAME_CONTRACT_STATUS_Sent_mean'})\ndataset = dataset.merge(credit_NAME_CONTRACT_STATUS_Sent_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_NAME_CONTRACT_STATUS_Sent_mean\ngc.collect()","5a72fa51":"credit_NAME_CONTRACT_STATUS_Signed_mean = credit_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['NAME_CONTRACT_STATUS_Signed'].mean().rename(columns = {'NAME_CONTRACT_STATUS_Signed': 'credit_NAME_CONTRACT_STATUS_Signed_mean'})\ndataset = dataset.merge(credit_NAME_CONTRACT_STATUS_Signed_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel credit_NAME_CONTRACT_STATUS_Signed_mean\ngc.collect()","884ac83c":"print('dataset data shape: ', dataset.shape)\ndataset.head()","286d04ce":"# let's free up some memory by deleting some of the dataframes we are done with\ngc.enable()\ndel credit, credit_stats_by_prev\ngc.collect()","8580519d":"# the next file we will investigate is installments_payments\ninstall = pd.read_csv('..\/input\/installments_payments.csv')\ninstall.head()","5ee135e1":"# create the additional variables\ninstall['DAYS_DIFF'] = install['DAYS_INSTALMENT'] - install['DAYS_ENTRY_PAYMENT']\ninstall['AMT_DIFF'] = install['AMT_INSTALMENT'] - install['AMT_PAYMENT']\ninstall.head()","23fa96db":"install_stats_by_prev = install[['SK_ID_PREV', 'SK_ID_CURR']]","08edcd42":"install_NUM_INSTALMENT_VERSION_count = install.groupby('SK_ID_PREV', as_index=False)['NUM_INSTALMENT_VERSION'].count().rename(columns = {'NUM_INSTALMENT_VERSION': 'install_NUM_INSTALMENT_VERSION_count'})\ninstall_NUM_INSTALMENT_VERSION_max = install.groupby('SK_ID_PREV', as_index=False)['NUM_INSTALMENT_VERSION'].max().rename(columns = {'NUM_INSTALMENT_VERSION': 'install_NUM_INSTALMENT_VERSION_max'})\ninstall_stats_by_prev = install_stats_by_prev.merge(install_NUM_INSTALMENT_VERSION_count, on = 'SK_ID_PREV', how = 'left')\ninstall_stats_by_prev = install_stats_by_prev.merge(install_NUM_INSTALMENT_VERSION_max, on = 'SK_ID_PREV', how = 'left')","7666f397":"gc.enable()\ndel install_NUM_INSTALMENT_VERSION_count, install_NUM_INSTALMENT_VERSION_max\ngc.collect()","4496ee2c":"install_DAYS_INSTALMENT_mean = install.groupby('SK_ID_PREV', as_index=False)['DAYS_INSTALMENT'].mean().rename(columns = {'DAYS_INSTALMENT': 'install_DAYS_INSTALMENT_mean'})\ninstall_stats_by_prev = install_stats_by_prev.merge(install_DAYS_INSTALMENT_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel install_DAYS_INSTALMENT_mean\ngc.collect()","6e93c1de":"install_DAYS_ENTRY_PAYMENT_mean = install.groupby('SK_ID_PREV', as_index=False)['DAYS_ENTRY_PAYMENT'].mean().rename(columns = {'DAYS_ENTRY_PAYMENT': 'install_DAYS_ENTRY_PAYMENT_mean'})\ninstall_stats_by_prev = install_stats_by_prev.merge(install_DAYS_ENTRY_PAYMENT_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel install_DAYS_ENTRY_PAYMENT_mean\ngc.collect()","8491b6a3":"install_AMT_INSTALMENT_mean = install.groupby('SK_ID_PREV', as_index=False)['AMT_INSTALMENT'].mean().rename(columns = {'AMT_INSTALMENT': 'install_AMT_INSTALMENT_mean'})\ninstall_stats_by_prev = install_stats_by_prev.merge(install_AMT_INSTALMENT_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel install_AMT_INSTALMENT_mean\ngc.collect()","b03418bb":"install_AMT_PAYMENT_mean = install.groupby('SK_ID_PREV', as_index=False)['AMT_PAYMENT'].mean().rename(columns = {'AMT_PAYMENT': 'install_AMT_PAYMENT_mean'})\ninstall_stats_by_prev = install_stats_by_prev.merge(install_AMT_PAYMENT_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel install_AMT_PAYMENT_mean\ngc.collect()","9017d9ae":"# capture the mean, max, and min for DAYS_DIFF\ninstall_DAYS_DIFF_mean = install.groupby('SK_ID_PREV', as_index=False)['DAYS_DIFF'].mean().rename(columns = {'DAYS_DIFF': 'install_DAYS_DIFF_mean'})\ninstall_DAYS_DIFF_max = install.groupby('SK_ID_PREV', as_index=False)['DAYS_DIFF'].max().rename(columns = {'DAYS_DIFF': 'install_DAYS_DIFF_max'})\ninstall_DAYS_DIFF_min = install.groupby('SK_ID_PREV', as_index=False)['DAYS_DIFF'].min().rename(columns = {'DAYS_DIFF': 'install_DAYS_DIFF_min'})\ninstall_stats_by_prev = install_stats_by_prev.merge(install_DAYS_DIFF_mean, on = 'SK_ID_PREV', how = 'left')\ninstall_stats_by_prev = install_stats_by_prev.merge(install_DAYS_DIFF_max, on = 'SK_ID_PREV', how = 'left')\ninstall_stats_by_prev = install_stats_by_prev.merge(install_DAYS_DIFF_min, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel install_DAYS_DIFF_mean, install_DAYS_DIFF_max, install_DAYS_DIFF_min\ngc.collect()","8919ec31":"# capture the mean, max, and min for AMT_DIFF\ninstall_AMT_DIFF_mean = install.groupby('SK_ID_PREV', as_index=False)['AMT_DIFF'].mean().rename(columns = {'AMT_DIFF': 'install_AMT_DIFF_mean'})\ninstall_AMT_DIFF_max = install.groupby('SK_ID_PREV', as_index=False)['AMT_DIFF'].max().rename(columns = {'AMT_DIFF': 'install_AMT_DIFF_max'})\ninstall_AMT_DIFF_min = install.groupby('SK_ID_PREV', as_index=False)['AMT_DIFF'].min().rename(columns = {'AMT_DIFF': 'install_AMT_DIFF_min'})\ninstall_stats_by_prev = install_stats_by_prev.merge(install_AMT_DIFF_mean, on = 'SK_ID_PREV', how = 'left')\ninstall_stats_by_prev = install_stats_by_prev.merge(install_AMT_DIFF_max, on = 'SK_ID_PREV', how = 'left')\ninstall_stats_by_prev = install_stats_by_prev.merge(install_AMT_DIFF_min, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel install_AMT_DIFF_mean, install_AMT_DIFF_max, install_AMT_DIFF_min\ngc.collect()","ed78876d":"install_stats_by_prev.head()","f9d6f34c":"install_NUM_INSTALMENT_VERSION_count_mean = install_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['install_NUM_INSTALMENT_VERSION_count'].mean().rename(columns = {'install_NUM_INSTALMENT_VERSION_count': 'install_NUM_INSTALMENT_VERSION_count_mean'})\ndataset = dataset.merge(install_NUM_INSTALMENT_VERSION_count_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel install_NUM_INSTALMENT_VERSION_count_mean\ngc.collect()","a9444a14":"install_NUM_INSTALMENT_VERSION_max_max = install_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['install_NUM_INSTALMENT_VERSION_max'].max().rename(columns = {'install_NUM_INSTALMENT_VERSION_max': 'install_NUM_INSTALMENT_VERSION_max_max'})\ndataset = dataset.merge(install_NUM_INSTALMENT_VERSION_max_max, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel install_NUM_INSTALMENT_VERSION_max_max\ngc.collect()","9cee58e1":"install_DAYS_INSTALMENT_mean_mean = install_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['install_DAYS_INSTALMENT_mean'].mean().rename(columns = {'install_DAYS_INSTALMENT_mean': 'install_DAYS_INSTALMENT_mean_mean'})\ndataset = dataset.merge(install_DAYS_INSTALMENT_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel install_DAYS_INSTALMENT_mean_mean\ngc.collect()","328af8f2":"install_DAYS_ENTRY_PAYMENT_mean_mean = install_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['install_DAYS_ENTRY_PAYMENT_mean'].mean().rename(columns = {'install_DAYS_ENTRY_PAYMENT_mean': 'install_DAYS_ENTRY_PAYMENT_mean_mean'})\ndataset = dataset.merge(install_DAYS_ENTRY_PAYMENT_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel install_DAYS_ENTRY_PAYMENT_mean_mean\ngc.collect()","cd59c886":"install_AMT_INSTALMENT_mean_mean = install_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['install_AMT_INSTALMENT_mean'].mean().rename(columns = {'install_AMT_INSTALMENT_mean': 'install_AMT_INSTALMENT_mean_mean'})\ndataset = dataset.merge(install_AMT_INSTALMENT_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel install_AMT_INSTALMENT_mean_mean\ngc.collect()","bfe23986":"install_AMT_PAYMENT_mean_mean = install_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['install_AMT_PAYMENT_mean'].mean().rename(columns = {'install_AMT_PAYMENT_mean': 'install_AMT_PAYMENT_mean_mean'})\ndataset = dataset.merge(install_AMT_PAYMENT_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel install_AMT_PAYMENT_mean_mean\ngc.collect()","edac54c3":"install_DAYS_DIFF_mean_mean = install_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['install_DAYS_DIFF_mean'].mean().rename(columns = {'install_DAYS_DIFF_mean': 'install_DAYS_DIFF_mean_mean'})\ndataset = dataset.merge(install_DAYS_DIFF_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel install_DAYS_DIFF_mean_mean\ngc.collect()","0e4ef79e":"install_DAYS_DIFF_max_mean = install_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['install_DAYS_DIFF_max'].mean().rename(columns = {'install_DAYS_DIFF_max': 'install_DAYS_DIFF_max_mean'})\ndataset = dataset.merge(install_DAYS_DIFF_max_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel install_DAYS_DIFF_max_mean\ngc.collect()","e9f6d4b5":"install_DAYS_DIFF_min_mean = install_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['install_DAYS_DIFF_min'].mean().rename(columns = {'install_DAYS_DIFF_min': 'install_DAYS_DIFF_min_mean'})\ndataset = dataset.merge(install_DAYS_DIFF_min_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel install_DAYS_DIFF_min_mean\ngc.collect()","cf6a9721":"install_AMT_DIFF_mean_mean = install_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['install_AMT_DIFF_mean'].mean().rename(columns = {'install_AMT_DIFF_mean': 'install_AMT_DIFF_mean_mean'})\ndataset = dataset.merge(install_AMT_DIFF_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel install_AMT_DIFF_mean_mean\ngc.collect()","cde563f5":"install_AMT_DIFF_max_mean = install_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['install_AMT_DIFF_max'].mean().rename(columns = {'install_AMT_DIFF_max': 'install_AMT_DIFF_max_mean'})\ndataset = dataset.merge(install_AMT_DIFF_max_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel install_AMT_DIFF_max_mean\ngc.collect()","81a986f9":"install_AMT_DIFF_min_mean = install_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['install_AMT_DIFF_min'].mean().rename(columns = {'install_AMT_DIFF_min': 'install_AMT_DIFF_min_mean'})\ndataset = dataset.merge(install_AMT_DIFF_min_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel install_AMT_DIFF_min_mean\ngc.collect()","af7cb353":"print('dataset data shape: ', dataset.shape)\ndataset.head()","aacac484":"# let's free up some memory by deleting some of the dataframes we are done with\ngc.enable()\ndel install, install_stats_by_prev\ngc.collect()","906b5119":"# the next file we will investigate is POS_CASH_balance\ncash = pd.read_csv('..\/input\/POS_CASH_balance.csv')\ncash.head()","c7f18e87":"cash_stats_by_prev = cash[['SK_ID_PREV', 'SK_ID_CURR']]","db4c7413":"cash_MONTHS_BALANCE_count = cash.groupby('SK_ID_PREV', as_index=False)['MONTHS_BALANCE'].count().rename(columns = {'MONTHS_BALANCE': 'cash_MONTHS_BALANCE_count'})\ncash_MONTHS_BALANCE_mean = cash.groupby('SK_ID_PREV', as_index=False)['MONTHS_BALANCE'].mean().rename(columns = {'MONTHS_BALANCE': 'cash_MONTHS_BALANCE_mean'})\ncash_stats_by_prev = cash_stats_by_prev.merge(cash_MONTHS_BALANCE_count, on = 'SK_ID_PREV', how = 'left')\ncash_stats_by_prev = cash_stats_by_prev.merge(cash_MONTHS_BALANCE_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel cash_MONTHS_BALANCE_count, cash_MONTHS_BALANCE_mean\ngc.collect()","9e79ee60":"cash_CNT_INSTALMENT_mean = cash.groupby('SK_ID_PREV', as_index=False)['CNT_INSTALMENT'].mean().rename(columns = {'CNT_INSTALMENT': 'cash_CNT_INSTALMENT_mean'})\ncash_stats_by_prev = cash_stats_by_prev.merge(cash_CNT_INSTALMENT_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel cash_CNT_INSTALMENT_mean\ngc.collect()","90c1a05b":"cash_CNT_INSTALMENT_FUTURE_mean = cash.groupby('SK_ID_PREV', as_index=False)['CNT_INSTALMENT_FUTURE'].mean().rename(columns = {'CNT_INSTALMENT_FUTURE': 'cash_CNT_INSTALMENT_FUTURE_mean'})\ncash_stats_by_prev = cash_stats_by_prev.merge(cash_CNT_INSTALMENT_FUTURE_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel cash_CNT_INSTALMENT_FUTURE_mean\ngc.collect()","c7c4ee57":"cash_SK_DPD_mean = cash.groupby('SK_ID_PREV', as_index=False)['SK_DPD'].mean().rename(columns = {'SK_DPD': 'cash_SK_DPD_mean'})\ncash_stats_by_prev = cash_stats_by_prev.merge(cash_SK_DPD_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel cash_SK_DPD_mean\ngc.collect()","4a45a1e8":"cash_SK_DPD_DEF_mean = cash.groupby('SK_ID_PREV', as_index=False)['SK_DPD_DEF'].mean().rename(columns = {'SK_DPD_DEF': 'cash_SK_DPD_DEF_mean'})\ncash_stats_by_prev = cash_stats_by_prev.merge(cash_SK_DPD_DEF_mean, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel cash_SK_DPD_DEF_mean\ngc.collect()","ece5f5f9":"# now let's deal with our one categorical variable, NAME_CONTRACT_STATUS, in this cash file\ncash_cats = pd.get_dummies(cash.select_dtypes('object'))\ncash_cats['SK_ID_PREV'] = cash['SK_ID_PREV']\ncash_cats.head()","74b84c8f":"cash_cats_grouped = cash_cats.groupby('SK_ID_PREV').agg('sum')\ncash_cats_grouped.head()","a377ae9a":"cash_stats_by_prev = cash_stats_by_prev.merge(cash_cats_grouped, on = 'SK_ID_PREV', how = 'left')\ngc.enable()\ndel cash_cats_grouped, cash_cats\ngc.collect()","56cdfc0f":"cash_stats_by_prev.head()","ac7f67c2":"cash_MONTHS_BALANCE_count_mean = cash_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['cash_MONTHS_BALANCE_count'].mean().rename(columns = {'cash_MONTHS_BALANCE_count': 'cash_MONTHS_BALANCE_count_mean'})\ndataset = dataset.merge(cash_MONTHS_BALANCE_count_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel cash_MONTHS_BALANCE_count_mean\ngc.collect()","3d60977e":"cash_MONTHS_BALANCE_mean_mean = cash_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['cash_MONTHS_BALANCE_mean'].mean().rename(columns = {'cash_MONTHS_BALANCE_mean': 'cash_MONTHS_BALANCE_mean_mean'})\ndataset = dataset.merge(cash_MONTHS_BALANCE_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel cash_MONTHS_BALANCE_mean_mean\ngc.collect()","e9e685da":"cash_CNT_INSTALMENT_mean_mean = cash_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['cash_CNT_INSTALMENT_mean'].mean().rename(columns = {'cash_CNT_INSTALMENT_mean': 'cash_CNT_INSTALMENT_mean_mean'})\ndataset = dataset.merge(cash_CNT_INSTALMENT_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel cash_CNT_INSTALMENT_mean_mean\ngc.collect()","53648181":"cash_CNT_INSTALMENT_FUTURE_mean_mean = cash_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['cash_CNT_INSTALMENT_FUTURE_mean'].mean().rename(columns = {'cash_CNT_INSTALMENT_FUTURE_mean': 'cash_CNT_INSTALMENT_FUTURE_mean_mean'})\ndataset = dataset.merge(cash_CNT_INSTALMENT_FUTURE_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel cash_CNT_INSTALMENT_FUTURE_mean_mean\ngc.collect()","0a8e42e4":"cash_SK_DPD_mean_mean = cash_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['cash_SK_DPD_mean'].mean().rename(columns = {'cash_SK_DPD_mean': 'cash_SK_DPD_mean_mean'})\ndataset = dataset.merge(cash_SK_DPD_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel cash_SK_DPD_mean_mean\ngc.collect()","07b1a715":"cash_SK_DPD_DEF_mean_mean = cash_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['cash_SK_DPD_DEF_mean'].mean().rename(columns = {'cash_SK_DPD_DEF_mean': 'cash_SK_DPD_DEF_mean_mean'})\ndataset = dataset.merge(cash_SK_DPD_DEF_mean_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel cash_SK_DPD_DEF_mean_mean\ngc.collect()","3f620197":"cash_NAME_CONTRACT_STATUS_Active_mean = cash_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['NAME_CONTRACT_STATUS_Active'].mean().rename(columns = {'NAME_CONTRACT_STATUS_Active': 'cash_NAME_CONTRACT_STATUS_Active_mean'})\ndataset = dataset.merge(cash_NAME_CONTRACT_STATUS_Active_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel cash_NAME_CONTRACT_STATUS_Active_mean\ngc.collect()","960726d7":"cash_NAME_CONTRACT_STATUS_Amortized_mean = cash_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['NAME_CONTRACT_STATUS_Amortized debt'].mean().rename(columns = {'NAME_CONTRACT_STATUS_Amortized debt': 'cash_NAME_CONTRACT_STATUS_Amortized_mean'})\ndataset = dataset.merge(cash_NAME_CONTRACT_STATUS_Amortized_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel cash_NAME_CONTRACT_STATUS_Amortized_mean\ngc.collect()","75da954b":"cash_NAME_CONTRACT_STATUS_Approved_mean = cash_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['NAME_CONTRACT_STATUS_Approved'].mean().rename(columns = {'NAME_CONTRACT_STATUS_Approved': 'cash_NAME_CONTRACT_STATUS_Approved_mean'})\ndataset = dataset.merge(cash_NAME_CONTRACT_STATUS_Approved_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel cash_NAME_CONTRACT_STATUS_Approved_mean\ngc.collect()","9dd9a07a":"cash_NAME_CONTRACT_STATUS_Canceled_mean = cash_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['NAME_CONTRACT_STATUS_Canceled'].mean().rename(columns = {'NAME_CONTRACT_STATUS_Canceled': 'cash_NAME_CONTRACT_STATUS_Canceled_mean'})\ndataset = dataset.merge(cash_NAME_CONTRACT_STATUS_Canceled_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel cash_NAME_CONTRACT_STATUS_Canceled_mean\ngc.collect()","8318c694":"cash_NAME_CONTRACT_STATUS_Completed_mean = cash_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['NAME_CONTRACT_STATUS_Completed'].mean().rename(columns = {'NAME_CONTRACT_STATUS_Completed': 'cash_NAME_CONTRACT_STATUS_Completed_mean'})\ndataset = dataset.merge(cash_NAME_CONTRACT_STATUS_Completed_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel cash_NAME_CONTRACT_STATUS_Completed_mean\ngc.collect()","baf74823":"cash_NAME_CONTRACT_STATUS_Demand_mean = cash_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['NAME_CONTRACT_STATUS_Demand'].mean().rename(columns = {'NAME_CONTRACT_STATUS_Demand': 'cash_NAME_CONTRACT_STATUS_Demand_mean'})\ndataset = dataset.merge(cash_NAME_CONTRACT_STATUS_Demand_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel cash_NAME_CONTRACT_STATUS_Demand_mean\ngc.collect()","0ee8a45b":"cash_NAME_CONTRACT_STATUS_Returned_mean = cash_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['NAME_CONTRACT_STATUS_Returned to the store'].mean().rename(columns = {'NAME_CONTRACT_STATUS_Returned to the store': 'cash_NAME_CONTRACT_STATUS_Returned_mean'})\ndataset = dataset.merge(cash_NAME_CONTRACT_STATUS_Returned_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel cash_NAME_CONTRACT_STATUS_Returned_mean\ngc.collect()","23427091":"cash_NAME_CONTRACT_STATUS_Signed_mean = cash_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['NAME_CONTRACT_STATUS_Signed'].mean().rename(columns = {'NAME_CONTRACT_STATUS_Signed': 'cash_NAME_CONTRACT_STATUS_Signed_mean'})\ndataset = dataset.merge(cash_NAME_CONTRACT_STATUS_Signed_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel cash_NAME_CONTRACT_STATUS_Signed_mean\ngc.collect()","ee8dda5d":"cash_NAME_CONTRACT_STATUS_XNA_mean = cash_stats_by_prev.groupby('SK_ID_CURR', as_index=False)['NAME_CONTRACT_STATUS_XNA'].mean().rename(columns = {'NAME_CONTRACT_STATUS_XNA': 'cash_NAME_CONTRACT_STATUS_XNA_mean'})\ndataset = dataset.merge(cash_NAME_CONTRACT_STATUS_XNA_mean, on = 'SK_ID_CURR', how = 'left')\ngc.enable()\ndel cash_NAME_CONTRACT_STATUS_XNA_mean\ngc.collect()","f945d78b":"# let's free up some memory by deleting some of the dataframes we are done with\ngc.enable()\ndel cash, cash_stats_by_prev\ngc.collect()","1df2b9ea":"print('dataset data shape: ', dataset.shape)\ndataset.head()","6c5b95d1":"# let's review our dataset data types\ndataset.dtypes.value_counts()","d7392c4b":"# it looks like we have a couple of objects still in our data?\ndataset.describe(include=[np.object])","6f27835d":"# our outlier variables appear to still be in the format of True or False, so we need to fix this before continuing.\ndataset['bureau_DAYS_CREDIT_ENDDATE_max_outlier'] = dataset['bureau_DAYS_CREDIT_ENDDATE_max_outlier'].map({False:0, True:1})\ndataset['bureau_DAYS_ENDDATE_FACT_mean_outlier'] = dataset['bureau_DAYS_ENDDATE_FACT_mean_outlier'].map({False:0, True:1})","b7b855ca":"# check again\ndataset['bureau_DAYS_CREDIT_ENDDATE_max_outlier'].describe()","f13bb0b3":"dataset['bureau_DAYS_ENDDATE_FACT_mean_outlier'].describe()","2de59549":"# because the dataset file is so large, let's use a subsample of the data to evaluate the collinear variables\ny_temp = dataset[['TARGET']]\nX_temp = dataset.drop(['TARGET'], axis=1)\nX_big, X_small, y_big, y_small = train_test_split(X_temp, y_temp, test_size=0.2, random_state=1)","43d75d28":"# let's first make the correlation matrix\ncorr = X_small.drop(['SK_ID_CURR'], axis=1)\ncorr_matrix = corr.corr().abs()","7aad060e":"upper_corr = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper_corr.head()","1e7e5ae7":"# let's drop any columns with correlations above 0.9\ndrop_cols = [column for column in upper_corr.columns if any(upper_corr[column] > 0.9)]\nprint('Columns to remove: ', len(drop_cols))","716b4c4e":"# now we can drop these columns from the full dataset file\ndataset = dataset.drop(columns = drop_cols)\nprint('dataset data shape: ', dataset.shape)","bf8d9a61":"# delete the dataframes we don't need anymore\ngc.enable()\ndel X_temp, X_big, X_small, y_temp, y_big, y_small, corr, corr_matrix, upper_corr, drop_cols\ngc.collect()","98b9460e":"# missing values (in percent)\ndataset_missing = (dataset.isnull().sum() \/ len(dataset)).sort_values(ascending = False)\ndataset_missing.head(10)","9d383c28":"# let's remove columns with more than 75% missing data\ndataset_missing = dataset_missing.index[dataset_missing > 0.75]\nprint('Columns with more than 75% missing values: ', len(dataset_missing))","85e99d69":"# let's drop these columns\ndataset = dataset.drop(columns = dataset_missing)\nprint('dataset data shape: ', dataset.shape)","cabebef9":"# separate training and testing data for modeling\ntrain = dataset[:train_len]\nx_test = dataset[train_len:]\ntrain_ids = train['SK_ID_CURR']\ntest_ids = x_test['SK_ID_CURR']\ntrain.drop(columns=['SK_ID_CURR'], axis = 1, inplace=True)\nx_test.drop(columns=['TARGET', 'SK_ID_CURR'], axis = 1, inplace=True)","0a237ac7":"# separate training data\ntrain['TARGET'] = train['TARGET'].astype(int)\ny_train = train['TARGET']\nx_train = train.drop(columns=['TARGET'], axis = 1)","379e7ab6":"print('x_train data shape: ', x_train.shape)\nprint('y_train data shape: ', y_train.shape)\nprint('x_test data shape: ', x_test.shape)","63d7f678":"# create a dataframe of all zeroes to hold feature importance calculations\nfeature_imp = np.zeros(x_train.shape[1])","aa988228":"# create the model to use\n# for the parameters, objective is binary (as this is either default or no default that we are predicting),\n# boosting type is gradient-based one-side sampling (larger gradients contribute more to information gain so this keeps those \n# with larger gradients and only randomly drops those with smaller), class weight is balanced\n# (automatically adjust the weights to be inversely proportional to the frequencies)\n\nmodel = lgb.LGBMClassifier(objective='binary', boosting_type='goss', n_estimators=10000, class_weight='balanced')","b11417a4":"# we will fit the model twice and record the feature importances each time\n# note that we will use auc (area under the curve) for evaluation, as on this is what our model will be judged\n\nfor i in range(2):\n    train_x1, train_x2, train_y1, train_y2 = train_test_split(x_train, y_train, test_size = 0.25, random_state = i)\n    model.fit(train_x1, train_y1, early_stopping_rounds=100, eval_set = [(train_x2, train_y2)], eval_metric = 'auc', verbose = 200)\n    feature_imp += model.feature_importances_","6faf4262":"# review features with most importance\nfeature_imp = feature_imp \/ 2\nfeature_imp = pd.DataFrame({'feature': list(x_train.columns), 'importance': feature_imp}).sort_values('importance', ascending = False)\nfeature_imp.head(10)","1eb2ba60":"# review features with zero importance\nzero_imp = list(feature_imp[feature_imp['importance'] == 0.0]['feature'])\nprint('count of features with 0 importance: ', len(zero_imp))\nfeature_imp.tail(10)","9b39c91c":"# let's drop the features with zero importance\nx_train = x_train.drop(columns = zero_imp)\nx_test = x_test.drop(columns = zero_imp)","3320c96a":"print('x_train data shape: ', x_train.shape)\nprint('x_test data shape: ', x_test.shape)","411dd90b":"# dataframe to hold predictions\ntest_predictions = np.zeros(x_test.shape[0])\n# dataframe for out of fold validation predictions\nout_of_fold = np.zeros(x_train.shape[0])\n# lists for validation and training scores\nvalid_scores = []\ntrain_scores = []","de6bf482":"k_fold = KFold(n_splits = 5, shuffle = False, random_state = 50)","221c5e0e":"x_train = np.array(x_train)\nx_test = np.array(x_test)","27b812e8":"# iterate through each of the five folds\nfor train_indices, valid_indices in k_fold.split(x_train):\n    train_features, train_labels = x_train[train_indices], y_train[train_indices]\n    valid_features, valid_labels = x_train[valid_indices], y_train[valid_indices]\n    \n    # create the model, similar to the one used above for feature importances\n    model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', boosting_type='goss',class_weight = 'balanced', \n                               learning_rate = 0.05, reg_alpha = 0.1, reg_lambda = 0.1, n_jobs = -1, random_state = 50)\n    \n    # train the model\n    model.fit(train_features, train_labels, eval_metric = 'auc',\n              eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n              eval_names = ['valid', 'train'], early_stopping_rounds = 100, verbose = 200)\n    \n    # record the best iteration\n    best_iteration = model.best_iteration_\n    \n    # test predictions\n    test_predictions += model.predict_proba(x_test, num_iteration = best_iteration)[:, 1] \/ k_fold.n_splits\n    \n    # out of fold predictions\n    out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n    \n    # record scores\n    valid_score = model.best_score_['valid']['auc']\n    train_score = model.best_score_['train']['auc']\n    valid_scores.append(valid_score)\n    train_scores.append(train_score)\n    \n    # Clean up memory\n    gc.enable()\n    del model, train_features, valid_features\n    gc.collect()","ea964fca":"# scores\nvalid_auc = roc_auc_score(y_train, out_of_fold)\n\nvalid_scores.append(valid_auc)\ntrain_scores.append(np.mean(train_scores))\n\nfold_names = list(range(5))\nfold_names.append('overall')\n\nmetrics = pd.DataFrame({'fold': fold_names, 'train': train_scores, 'valid': valid_scores}) ","6177bb88":"metrics","75ed13bf":"# make submission file\nsubmission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': test_predictions})\nsubmission.to_csv('submission.csv', index = False)","e5f71d70":"Let's look at the correlations between these social variables and our TARGET.","4d750c24":"I don't see any outliers, although the shape of this distribution is different from the others we have seen above.","50726666":"## DAYS_LAST_PHONE_CHANGE","f43a0250":"## AMT_CREDIT","711c10cb":"## Categoricals in Bureau data\n\nNow let's look at the categorical variables that are left in this data.  There are three of them that we need to deal with.","20c3eb99":"## AMT_REQ_CREDIT_BUREAU_YEAR","7a388345":"From the above, it appears that the probability of default for our outliers is 13.23%, which far exceeds that of the entire training data of 8.07%.  Let's remove these as outliers and keep track of which records are outliers.","c93907ce":"For this file, I think we want to look at creating a couple of additional variables.  For the dates of payments, I would like to see the difference between the due date and the actual payment date.  For the amounts of payments, I would like to see the difference between the amount owed and the actual amount paid.  So let's create a few new variables.","c344b771":"## OBS_60_CNT_SOCIAL_CIRCLE","3629acf8":"Now that we have reviewed the initial data provided, there are a few more variables we can create from the existing data.  We can later determine if any of these interactions can add to our model.\n\nNew variables to try:\n* EMPLOY_AGE = DAYS_EMPLOYED \/ DAYS_BIRTH: how long was the applicant employed relative to how old the applicant was - employed for a larger portion may indicate reliability\n* INCOME_AGE = AMT_INCOME_TOTAL \/ DAYS_BIRTH: how large is the income relative to how old the applicant was - may indicate potential for income to rise and may repayment easier in the future\n* CREDIT_AGE = AMT_CREDIT \/ DAYS_BIRTH: how much credit relative to how old the applicant was - may indicate sources of other financial stress\n* CREDIT_INCOME = AMT_CREDIT \/ AMT_INCOME_TOTAL: how much credit relative to total income - too much credit may be too risky\n* ANNUITY_INCOME = AMT_ANNUITY \/ AMT_INCOME_TOTAL: how large are the loan payments relative to total income - too large of payments may not be sustainable\n* ANNUITY_CREDIT = AMT_ANNUITY \/ AMT_CREDIT: how large are the loan payments relative to the credit amount (how long will it take to pay it back, without accounting for different interest rates)\n\nThese may or may not be good predictors, but these were the ones I thought could be useful in the model.","65eb13c3":"None of the above variables are significantly correlated with TARGET.  However, AMT_CREDIT, AMT_ANNUITY, and AMT_GOODS_PRICE are all highly correlated with each other, especially AMT_CREDIT and AMT_GOODS_PRICE.  It appears that we may need to drop at least one of these two variables.","23a85c79":"## AMT_GOODS_PRICE","79b8d370":"## DAYS_BIRTH","c6465da2":"Most applicants have no children or only one child.  However, the maximum is 20, which seems high.  Let's look closer at this data.","fb8a1e3c":"## AMT_CREDIT_MAX_OVERDUE","ee5c9b1a":"## DEF_60_CNT_SOCIAL_CIRCLE","8b7c5f1e":"## HOUR_APPR_PROCESS_START","ad9a7d2a":"## Feature Importance\nTo evaluate feature importance, I will use the LightGBM model.  I will run the model twice to capture the feature importances, and then average the results.","8d46570d":"## CNT_CHILDREN","86b910dd":"## CREDIT_DAY_OVERDUE","10da1a12":"## REGION_RATING_CLIENT","a6222760":"## EXT_SOURCE_1","60da3308":"For the non-indicator variables, let's look at individual variables for outliers or other interesting information.","2238d066":"## REGION_RATING_CLIENT_W_CITY","e3bbcb0e":"Similar to the above, there do not appear to be any outliers here, though the distribution is also definitely skewed.","0680778c":"## AMT_CREDIT_SUM_OVERDUE","ce08d761":"Note that four columns had two categories, but only three were label encoded - EMERGENCYSTATE_MODE was not.\nThis is because the three label encoded did not have any missing values, while EMERGENCYSTATE_MODE was missing values, so it really has three categories - Yes, No, and missing.","e99728cf":"There do not appear to be any outliers here, though the distribution is definitely skewed.","97bb86d7":"## Next look at missing values","26d74711":"This is my introduction on the Home Credit Default Risk problem.  I first spend time reviewing the variables and examining the data in the training data.  Next, I move on to incorporating data from the other data files we have been given.  Then I begin paring down the data to the data I will use for modeling.","1a970b85":"I am going to treat all of these credit bureau variables similarly.  I am going to remove the outliers but create a flag for the outliers.  As the period of evaluation increases (from hour to day to week, etc.), the cutoff for the outliers will also increase.","1cdec0e0":"This distribution is definitely skewed, but looks like we would expect for a distribution of credit.","f945f0ba":"## MONTHS_BALANCE","e6c3d331":"# Additional Features to Add from the Training Data","7c83dbf4":"## AMT_CREDIT_SUM_DEBT","e6274e9a":"# Adding Features from the Other Data Files","39795d1b":"## COUNT","92e868f3":"## DAYS_CREDIT","9abf4e92":"## REGION_POPULATION_RELATIVE","4fd2ae97":"This distribution of ages at application date seems reasonable - no small children and reasonable max age.","6206d58b":"Now we will take the mean when grouping by SK_ID_CURR for each of the above variables and then add them to our dataset.  There are more possibilities here (min, max, count, sum, etc.) but we will just do mean for now.","6b5153da":"### Let's review the correlation matrix for the variables examined so far:","7b37b50d":"# Feature Selection\nIn this next section, I will work on reducing the number of variables.  I will do this by getting rid of collinear variables, variables with too many missing values, and feature importance.  After completing these steps, we will have a dataset better suited for modeling.","f1dfcd3c":"## AMT_REQ_CREDIT_BUREAU_DAY","6a0556c6":"This looks to be close to what we would expect.  There are relatively fewer high incomes, and the mean is greater than the median.","c60cfc8a":"These three are all strongly correlated with our TARGET, and they are not too highly correlated with each other.  These look like important variables for later.","526b6d4d":"## DAYS_ID_PUBLISH","609e3f35":"# Introduction","9e782362":"## AMT_ANNUITY","61078141":"## OBS_30_CNT_SOCIAL_CIRCLE","43af7e9c":"## DAYS_CREDIT_UPDATE","861d2c4d":"## DAYS_EMPLOYED","261eb72c":"## AMT_REQ_CREDIT_BUREAU_MON","26366972":"Let's look at the correlations of this last group of variables.","ff11de3b":"The above looks as expected.","f28c225d":"## EXT_SOURCE_3","9ced5211":"## DAYS_CREDIT_ENDDATE","180c0ae6":"This is a lot of missing values to deal with.  A number of variables are missing over two-thirds of the data.","eab1a49e":"It appears that if CNT_CHILDREN is greater than six, the probability of default is higher.  From above, we know that on average the default rate is about 8.07%.  Let's look further at CNT_CHILDREN.","ef4c73e2":"REGION_RATING_CLIENT and REGION_RATING_CLIENT_W_CITY are the most correlated with our TARGET, though these two variables are highly correlated with each other.  We will probably want to include only one of these two.","473117cc":"### Let's review the correlation matrix for the additional variables examined so far:","f30b1d1a":"The larger values do not appear to be correlated with higher-than-average or lower-than-average default rates.  For now, I will leave these as is.","c311cf9e":"Note that using one-hot encoding, we went from 122 variables to 243 variables - a significant increase.  At a later point, we will probably want to remove those that are not relevant.","a3ca2157":"These three external sources variables appear similar but have different distributions.  Let's look at their correlations with each other and with our TARGET.","6997eab1":"## Start with collinear variables","86160def":"In this section, I will begin adding to my data, incorporating the information from the other data files we were provided.","a3479304":"DAYS_LAST_PHONE_CHANGE is more correlated with our TARGET, while the credit bureau variables don't seem to be correlated with TARGET much at all.","46aa5fae":"DAYS_BIRTH appears to be the most correlated with TARGET so far.","8b0d056e":"## AMT_INCOME_TOTAL","b9713814":"## CNT_FAM_MEMBERS","021bc323":"Now we have all of this data by previous ID, but we need to aggregate this on SK_ID_CURR.  We will repeat what we did above for the bureau balance data, averaging these variables for each applicant.","2a48ca6b":"## DAYS_ENDDATE_FACT","7e28a3cc":"These two are very similarly correlated with our TARGET.  For now, we will keep both of these, but we may remove one later on.","3833da09":"### Let's review the correlation matrix for the additional variables examined so far:","f1ee4b5b":"While the distribution is skewed, it is now what we expect - long-tenure employees are rare and short-tenure employees are much more common.","798704d3":"Now we have all of this data by previous ID, but we need to aggregate this on SK_ID_CURR.  We will repeat what we did above for the credit data.  For most cases, I will use the average to combine.","00853eeb":"## AMT_CREDIT_SUM_LIMIT","0b16b789":"## AMT_CREDIT_SUM","416c5f5f":"Next we will look at feature importance.  But before we do so, we will need to split our data back into test and train.","b9405198":"## DAYS_REGISTRATION","7319d24a":"The DEF counts are highly correlated with each other, but none of these are very correlated with our TARGET.","52b2a851":"## CNT_CREDIT_PROLONG","c55cb410":"## AMT_REQ_CREDIT_BUREAU_WEEK","947faffe":"## AMT_REQ_CREDIT_BUREAU_QRT","b2d37d4c":"# Modeling\nLet's begin the modeling section now.  We will use LightGBM with cross validation.","a236e5a9":"# Explore the Data","4827c876":"## AMT_REQ_CREDIT_BUREAU_HOUR","0c6379f4":"## DEF_30_CNT_SOCIAL_CIRCLE","9e85096e":"## EXT_SOURCE_2","b00e6d8d":"## AMT_ANNUITY","e603a802":"## OWN_CAR_AGE","c9a8df3f":"I will need to determine how to incorporate each of these items into the data.  Generally, for time measurements (numbers of days since something), I will want to use the max or min.  For other items, like amounts, I will want to use the mean instead.  I may also want to use count or sum, depending on the item.\n\nTo get this data into the main dataset file, I will need to group the data in the new file by SK_ID_CURR.  I will then apply the max or mean (or other function) to the data and merge this into the dataset file."}}