{"cell_type":{"c39109b3":"code","8b37b50a":"code","fa1c06ce":"code","8bc96cfb":"code","e340e615":"code","6c537593":"code","bbdb7521":"code","94509899":"code","183baf67":"code","d7785851":"code","9793ca48":"code","07db5c12":"code","683c1930":"code","4a405872":"code","d050b82a":"code","6d0cd014":"code","a137880b":"markdown","9130ec4d":"markdown","344e2410":"markdown","834efcc8":"markdown"},"source":{"c39109b3":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","8b37b50a":"filepath = '..\/input\/diamonds\/diamonds.csv'\ndiamond_data = pd.read_csv(filepath, index_col=0)","fa1c06ce":"diamond_data.columns","8bc96cfb":"diamond_data.head()\ndiamond_data.isnull().sum()","e340e615":"from sklearn.preprocessing import LabelEncoder\n\ndataTypes = (diamond_data.dtypes == 'object')\ncategories = list(dataTypes[dataTypes].index)\n\ndiamond_data_cat = diamond_data.copy()\nlabel_encoder = LabelEncoder()\n\nfor category in categories:\n    diamond_data_cat[category] = label_encoder.fit_transform(diamond_data_cat[category])\n    \ndiamond_data_cat.head()","6c537593":"data_correlation = diamond_data_cat.corr(method='pearson')\nmask = np.triu(np.ones_like(data_correlation, dtype=np.bool))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nplt.figure(figsize=(30, 10))\nsns.heatmap(data_correlation, cmap=cmap, vmax=.3, center=0, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)","bbdb7521":"unstacked_correlation = data_correlation.abs().unstack()\nsorted_correlation = unstacked_correlation.sort_values(kind='quicksort', ascending=False)\nsorted_correlation['price']","94509899":"plt.figure(figsize=(20,10))\nsns.regplot(data=diamond_data, x='carat', y='price')\n","183baf67":"plt.figure(figsize=(20,10))\nsns.regplot(data=diamond_data, x='x', y='price')","d7785851":"plt.figure(figsize=(20,10))\nsns.regplot(x=np.log(diamond_data['x']), y=np.log(diamond_data['price']))","9793ca48":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n#Select Columns here:\nX_columns = ['carat', 'x', 'y', 'z']\ny_columns = ['price']\nX = diamond_data_cat[X_columns]\ny = diamond_data[y_columns]\n\n#splitting the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)","07db5c12":"model = LinearRegression().fit(X_train, np.log(y_train))\npredictions = model.predict(X_test)\nX_test","683c1930":"stats_df = pd.DataFrame(X_test.copy())\nstats_df['price'] = y_test['price'].copy()\nstats_df['predictions'] = predictions.copy()\nstats_df['predictions'] = np.exp(stats_df['predictions'])\nstats_df.sort_values(by='carat')","4a405872":"from sklearn.metrics import mean_squared_error\nimport math\n\n#Finding the RMSE of the model\nmse = mean_squared_error(predictions, y_test['price'])\nrmse = math.sqrt(mse)\nrmse","d050b82a":"second_model = LinearRegression().fit(X_train, y_train)\nsecond_predictions = second_model.predict(X_test)\nsecond_metric = mean_squared_error(second_predictions, y_test['price'])\nsecond_metric = math.sqrt(second_metric)\nsecond_metric","6d0cd014":"second_stats_df = pd.DataFrame(X_test.copy())\nsecond_stats_df['price'] = y_test['price'].copy()\nsecond_stats_df['predictions'] = second_predictions.copy()\nsecond_stats_df['predictions'] = second_stats_df['predictions']\nsecond_stats_df.sort_values(by='carat')","a137880b":"Now looking at the qualitative data...","9130ec4d":"Personal Skills Gained:\n-As someone new, I familiarized myself with Categorical Feature labelling and feature selection. Using the correlation heatmap helped me find relevant features more effectively.\n\nMy Takeaways:\n-Linear Regression fits \"ok\" with the data. But, it doesn't accurately predict the ends of the values of each feature.\n\nAdjustments that can be done:\n- remove carat values that are lesser than 0.5\n","344e2410":"Coding the Model:","834efcc8":"Getting the log values of price and inserting it to the prediction removes negative values. However, it inflates the overall predictions making it inaccurate in general. The code below shows a lower RMSE with negative values on the table."}}