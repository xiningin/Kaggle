{"cell_type":{"828b4aa5":"code","6c78c076":"code","b837bd34":"code","085e7fe0":"code","9531c346":"code","3ca5c5ef":"code","543884c9":"code","55aff2a8":"code","474b5194":"code","b3190a14":"code","27f02c74":"code","93c43052":"code","e8026003":"code","b39d271c":"code","98a6cd5b":"code","797b5484":"code","489521c3":"code","2d0031ba":"code","85c2c125":"code","65a7bbef":"code","48fa9144":"code","2d49fed1":"code","4ac92f52":"code","e67dd726":"code","50e13925":"code","514a8e08":"code","2b114eb3":"code","a899fe1c":"code","281b465b":"code","b912a5e0":"code","222ac925":"code","f4961961":"code","205ca78b":"code","44f155cd":"code","3c4faf21":"code","97409ee7":"code","27a86268":"code","01e272ba":"code","956ec676":"code","1e54be8f":"code","50e75230":"code","a3bd29cb":"code","6870e44a":"code","14c3c441":"code","33b50dc5":"code","0a42437c":"code","500feaee":"code","fd6795ba":"code","fd9d04e1":"code","cde8071d":"markdown","e7da09bb":"markdown","35f78d5d":"markdown","41397e0e":"markdown","17a79f09":"markdown","90a1c510":"markdown","63acbe32":"markdown","d73452f1":"markdown","07cd31eb":"markdown"},"source":{"828b4aa5":"import numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics import confusion_matrix","6c78c076":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n     \nfrom keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, BatchNormalization\nfrom keras import backend as K\n\nfrom keras.callbacks import ModelCheckpoint\n\n\nfilepath=\"\/kaggle\/working\/best_model2.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')","b837bd34":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","085e7fe0":"import os\nimport time\n\nimport numpy as np\nfrom tqdm.notebook import tqdm\n\nimport matplotlib.pyplot as plt","9531c346":"def averaged_by_N_rows(a, n):\n    \"\"\" \u041d\u0430\u0431\u0440\u043e\u0441\u043e\u043a \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0443\u0441\u0440\u0435\u0434\u043d\u044f\u044e\u0449\u0438\u0439 n-\u043a\u0438 \u0441\u0442\u0440\u043e\u043a \u0432 \u043c\u0430\u0442\u0440\u0438\u0446\u0435\n    \"\"\"\n    shape = a.shape\n    assert len(shape) == 2\n    assert shape[0] % n == 0\n    b = a.reshape(shape[0] \/\/ n, n, shape[1])\n    mean_vec = b.mean(axis=1)\n    return mean_vec\n    ","3ca5c5ef":"demographic = pd.read_csv(\"\/kaggle\/input\/button-tone-sz\/demographic.csv\")\ndemographic","543884c9":"demographic[\" group\"].mean()","55aff2a8":"for i, t in enumerate(list(demographic[\" group\"])):\n    if t:\n        print(f\"{i})   \u0428\u0418\u0417\")\n    else:\n        print(f\"{i})   \u0417\u0414\u041e\u0420\")\n        ","474b5194":"diagnosis_dict = dict(zip(demographic.subject, demographic[\" group\"]))\ndel demographic","b3190a14":"diagnosis_dict[25]","27f02c74":"electrodes_list = list(pd.read_csv(\"\/kaggle\/input\/button-tone-sz\/columnLabels.csv\").columns[4:])\nprint(electrodes_list)","93c43052":"(9216 * len(electrodes_list))","e8026003":"9216 \/ 4","b39d271c":"INTERACTIVE = False\n\nif INTERACTIVE and input(\"\u041d\u0430\u0447\u0438\u043d\u0430\u0435\u043c \u043e\u0442\u0431\u043e\u0440 \u0434\u0430\u043d\u043d\u044b\u0445 \u0437\u0430\u043d\u043e\u0432\u043e, \u0437\u0430\u0442\u0438\u0440\u0430\u044f \u0442\u043e, \u0447\u0442\u043e \u0435\u0441\u0442\u044c?\").lower() in (\"yes\", \"\u0434\u0430\"):\n\n    N_AVERAGED = 16\n    X = np.zeros((81 * 100,  9216 * len(electrodes_list) \/\/ N_AVERAGED), dtype=\"float32\")\n    Y = np.zeros(len(X))\n\n    part1_path = \"..\/input\/button-tone-sz\"\n    part2_path = \"..\/input\/buttontonesz2\"\n\n    # \u0412\u044b\u0442\u0430\u0441\u043a\u0438\u0432\u0430\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u0435 \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u044b, \u0433\u0434\u0435 \u0438\u0437\u043c\u0435\u0440\u0435\u043d\u0438\u0439 \u0431\u044b\u043b\u043e 9216 (\u0447\u0430\u0449\u0435 \u0432\u0441\u0435\u0433\u043e \u0438\u043c\u0435\u043d\u043e \u0441\u0442\u043e\u043b\u044c\u043a\u043e \u0440\u0430\u0437 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434)\n    x_counter = 0\n    column_list = pd.read_csv(\"\/kaggle\/input\/button-tone-sz\/columnLabels.csv\").columns\n    for person_number in tqdm(range(1, 81 + 1)):\n                \n        \n        csv_path = f\"{part1_path}\/{person_number}.csv\/{person_number}.csv\"\n        if not os.path.exists(csv_path):\n            csv_path = f\"{part2_path}\/{person_number}.csv\/{person_number}.csv\"\n        df = pd.read_csv(csv_path, \n                    header=None,\n                    names=column_list\n                    )\n        #df = df[column_list]\n        trials_list = set(df.trial)\n\n        \n        \n\n        for t1, trial_number in enumerate(trials_list):\n            number_of_trials = len(df[df.trial == trial_number])\n            if number_of_trials == 9216.0:\n                current_sample_matrix = df[df.trial == trial_number][electrodes_list].values\n                averaged_by_N = averaged_by_N_rows(current_sample_matrix, n=N_AVERAGED)\n                averaged_by_N_big_vec = averaged_by_N.reshape(-1)\n                X[x_counter] = averaged_by_N_big_vec.astype(np.float32)\n                Y[x_counter] = diagnosis_dict[person_number]\n                x_counter += 1\n            #print(f\"\u0418\u0441\u043f\u044b\u0442\u0430\u043d\u0438\u0435 \u043f\u043e\u0434 \u043d\u043e\u043c\u0435\u0440\u043e\u043c {trial_number} \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 \u0438\u0437\u043c\u0435\u0440\u0435\u043d\u0438\u0439 {number_of_trials} \")\n    print(\"\u0412\u0441\u0435\u0433\u043e \u0438\u0441\u043f\u044b\u0442\u0430\u043d\u0438\u0439 \u0441 \u043f\u043e\u0434\u0445\u043e\u0434\u044f\u0449\u0438\u043c \u0447\u0438\u0441\u043b\u043e\u043c \u0438\u0437\u043c\u0435\u0440\u0435\u043d\u0438\u0439 - \", x_counter)\n    X = X[: x_counter]\n    Y = Y[: x_counter]\n        \n","98a6cd5b":"#print(\"\u0412\u0441\u0435\u0433\u043e \u0438\u0441\u043f\u044b\u0442\u0430\u043d\u0438\u0439 \u0441 \u043f\u043e\u0434\u0445\u043e\u0434\u044f\u0449\u0438\u043c \u0447\u0438\u0441\u043b\u043e\u043c \u0438\u0437\u043c\u0435\u0440\u0435\u043d\u0438\u0439 - \", x_counter)\n\n","797b5484":"if INTERACTIVE and input(\"\u0417\u0430\u043f\u0438\u0441\u0430\u0442\u044c X_big \u0438 Y_big \u0432 \u0444\u0430\u0439\u043b?\").lower() in (\"yes\", \"\u0434\u0430\"):\n    X.tofile(\"\/kaggle\/working\/goodX_every4.bin\")\n    Y.astype(\"uint8\").tofile(\"\/kaggle\/working\/goodY_every4.bin\")","489521c3":"#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=True, random_state=42)","2d0031ba":"# obj2delete_list = \"X_test X_train Y_train, nn model\".split()\n# for t in obj2delete_list:\n#     try:\n#         exec(f\"del {t}\")\n#     except:\n#         print(t, \"is no longer present\")\n# import gc; gc.collect()","85c2c125":"AUTO_YES = True\n\nif AUTO_YES or input(\"\u0421\u0447\u0438\u0442\u0430\u0442\u044c X_big \u0438 Y_big \u0438\u0437 \u0444\u0430\u0439\u043b\u0430? \").lower() in (\"yes\", \"\u0434\u0430\"):\n    Y = np.fromfile(\"\/kaggle\/input\/good-schizophrenia-x-by-16\/goodY_every4.bin\", dtype=np.uint8)    \n    X = np.fromfile(\"\/kaggle\/input\/good-schizophrenia-x-by-16\/goodX_every4.bin\", dtype=np.float32).reshape(len(Y), -1)\n    print(\"\u0413\u043e\u0442\u043e\u0432\u043e! \u0412\u0440\u0435\u043c\u044f:\", time.ctime())","65a7bbef":"X_norm = (normalize(X.reshape(-1, 70), axis=0, norm='max')).reshape(X.shape)","48fa9144":"X_train_norm, X_test_norm, Y_train_norm, Y_test_norm = train_test_split(X_norm, Y, test_size=0.2, shuffle=True, random_state=42)","2d49fed1":"_norm = X","4ac92f52":"X_train_2d = X_train_norm.reshape(X_train_norm.shape[0], len(electrodes_list), X_train_norm.shape[1] \/\/ len(electrodes_list), 1)\nX_test_2d = X_test_norm.reshape(X_test_norm.shape[0], len(electrodes_list), X_test_norm.shape[1] \/\/ len(electrodes_list), 1)","e67dd726":"#model = keras.models.load_model(\"..\/input\/eeg-schiz-trained-model75\/eeg_schiz_model_acc75.h5\")\n#model.evaluate(X_test_2d, Y_test_norm)","50e13925":"filepath=\"\/kaggle\/working\/best_model2.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(5, 20),\n                 activation='tanh',\n                 input_shape=(X_train_2d.shape[1:])))\nmodel.add(MaxPooling2D(pool_size=(5, 15)))\n\nmodel.add(Conv2D(13, kernel_size=(3, 3),\n                 activation='tanh',))\nmodel.add(MaxPooling2D(pool_size=(3, 3)))\n          \n\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\n\nmodel.add(Dense(317, activation='relu'))\n\nmodel.add(Dense(1, activation='sigmoid'))","514a8e08":"model.compile(loss=keras.losses.binary_crossentropy,\n              optimizer=keras.optimizers.Adam(0.00001),\n              metrics=['acc'])","2b114eb3":"history_params = []","a899fe1c":"# \u0421\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u0430\u044f\nhistory = model.fit(X_train_2d, Y_train_norm,\n          batch_size=17,\n          epochs=100,\n          verbose=1,\n          shuffle=True,\n          validation_data=(X_test_2d, Y_test_norm), callbacks=[checkpoint])","281b465b":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('acc')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","b912a5e0":"accuracy, loss = model.evaluate(X_test_2d, Y_test_norm)\nprint(f\"Val accuracy={accuracy}  val loss={loss}\")","222ac925":"y_predicted =  model.predict(X_test_2d)\n\nconfusion_matr = confusion_matrix(Y_test_norm, np.round(model.predict(X_test_2d)))\nconfusion_matr","f4961961":"print(\n \" \u0421\u0442\u043e\u043b\u044c\u043a\u043e \u0412\u0415\u0420\u041d\u042b\u0425 \u0434\u0438\u0430\u0433\u043d\u043e\u0437\u043e\u0432 \u0428\u0418\u0417(1): \", confusion_matr[0, 0], \"\\n\",\n \"\u0421\u0442\u043e\u043b\u044c\u043a\u043e  \u041e\u0428\u0418\u0411\u041e\u0427\u041d\u042b\u0425 \u0434\u0438\u0430\u0433\u043d\u043e\u0437\u043e\u0432 \u0428\u0418\u0417(0):\", confusion_matr[0, 1], \"\\n\",\n  \"\u0421\u0442\u043e\u043b\u044c\u043a\u043e  \u041e\u0428\u0418\u0411\u041e\u0427\u041d\u042b\u0425 \u0434\u0438\u0430\u0433\u043d\u043e\u0437\u043e\u0432 \u0417\u0414\u041e\u0420(0):\", confusion_matr[1, 0], \"\\n\",\n  \"\u0421\u0442\u043e\u043b\u044c\u043a\u043e  \u0412\u0415\u0420\u041d\u042b\u0425 \u0434\u0438\u0430\u0433\u043d\u043e\u0437\u043e\u0432 \u0417\u0414\u041e\u0420(0):\", confusion_matr[1, 1], \"\\n\"    \n)","205ca78b":"TP = confusion_matr[0, 0]\nFP = confusion_matr[0, 1]\nFN = confusion_matr[1, 0]\nTN = confusion_matr[1, 1]\nTP, FP, FN, TN","44f155cd":"# model.save(\"\/kaggle\/working\/eeg_schiz_model_acc75.h5\")","3c4faf21":"# Sensitivity (\u0447\u0443\u0432\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c) \u0434\u043b\u044f \u043f\u0435\u0440\u0432\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0441\u0435\u0442\u0438","97409ee7":"sensitivity = TP \/ (TP + FN)\nsensitivity","27a86268":"specificity = TN \/ (TN + FP)\nspecificity","01e272ba":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=True, random_state=42)","956ec676":"filepath=\"\/kaggle\/working\/best_model2.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n\nmodel_2 = Sequential()\nmodel_2.add(Dense(5000,\n                  activation='relu',\n                  input_shape=(X_train.shape[1:])))\n \n\n\nmodel_2.add(Dense(1, activation='sigmoid'))\n\nmodel_2.compile(loss=keras.losses.binary_crossentropy,\n               optimizer=keras.optimizers.Adam(),\n               metrics=['acc'])","1e54be8f":"history_params_2 = []\nhistory_2 = model_2.fit(X_train, Y_train,\n          batch_size=17,\n          epochs=100,\n          verbose=1,\n          shuffle=True,\n          validation_data=(X_test, Y_test), callbacks=[checkpoint])","50e75230":"loss, accuracy = model_2.evaluate(X_test, Y_test)\nloss, accuracy ","a3bd29cb":"print(history_2.history.keys())\n# summarize history for accuracy\nplt.plot(history_2.history['acc'])\nplt.plot(history_2.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('acc')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_2.history['loss'])\nplt.plot(history_2.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","6870e44a":"# model = keras.models.load_model(\"..\/input\/eeg-schiz-trained-model75\/eeg_schiz_model_acc75.h5\")","14c3c441":"y_predicted =  model_2.predict(X_test)\n\nconfusion_matr = confusion_matrix(Y_test, np.round(model_2.predict(X_test)))\nconfusion_matr","33b50dc5":"print(\n \" \u0421\u0442\u043e\u043b\u044c\u043a\u043e \u0412\u0415\u0420\u041d\u042b\u0425 \u0434\u0438\u0430\u0433\u043d\u043e\u0437\u043e\u0432 \u0428\u0418\u0417(1): \", confusion_matr[0, 0], \"\\n\",\n \"\u0421\u0442\u043e\u043b\u044c\u043a\u043e  \u041e\u0428\u0418\u0411\u041e\u0427\u041d\u042b\u0425 \u0434\u0438\u0430\u0433\u043d\u043e\u0437\u043e\u0432 \u0428\u0418\u0417(0):\", confusion_matr[0, 1], \"\\n\",\n  \"\u0421\u0442\u043e\u043b\u044c\u043a\u043e  \u041e\u0428\u0418\u0411\u041e\u0427\u041d\u042b\u0425 \u0434\u0438\u0430\u0433\u043d\u043e\u0437\u043e\u0432 \u0417\u0414\u041e\u0420(0):\", confusion_matr[1, 0], \"\\n\",\n  \"\u0421\u0442\u043e\u043b\u044c\u043a\u043e  \u0412\u0415\u0420\u041d\u042b\u0425 \u0434\u0438\u0430\u0433\u043d\u043e\u0437\u043e\u0432 \u0417\u0414\u041e\u0420(0):\", confusion_matr[1, 1], \"\\n\"    \n)","0a42437c":"TP = confusion_matr[0, 0]\nFP = confusion_matr[0, 1]\nFN = confusion_matr[1, 0]\nTN = confusion_matr[1, 1]\nTP, FP, FN, TN","500feaee":"sensitivity = TP \/ (TP + FN)\nsensitivity","fd6795ba":"specificity = TN \/ (TN + FP)\nspecificity","fd9d04e1":"specificity = TN \/ (TN + FP)\nspecificity","cde8071d":"${\\displaystyle {\\begin{aligned}{\\text{sensitivity}}&={\\frac {\\text{number of true positives}}{{\\text{number of true positives}}+{\\text{number of false negatives}}}}\\\\[8pt]&={\\frac {\\text{number of true positives}}{\\text{total number of sick individuals in population}}}\\\\[8pt]&={\\text{probability of a positive test given that the patient has the disease}}\\end{aligned}}}$","e7da09bb":"$${\\displaystyle {\\begin{aligned}{\\text{specificity}}&={\\frac {\\text{number of true negatives}}{{\\text{number of true negatives}}+{\\text{number of false positives}}}}\\\\[8pt]&={\\frac {\\text{number of true negatives}}{\\text{total number of well individuals in population}}}\\\\[8pt]&={\\text{probability of a negative test given that the patient is well}}\\end{aligned}}}$$","35f78d5d":"# \u041f\u043e\u043b\u043d\u043e\u0441\u0432\u044f\u0437\u043d\u0430\u044f \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u0430\u044f \u0441\u0435\u0442\u044c[[](http:\/\/)](http:\/\/)","41397e0e":"## \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0441\u0432\u0451\u0440\u0442\u043e\u0447\u043d\u0443\u044e \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u0443\u044e \u0441\u0435\u0442\u044c","17a79f09":"${\\displaystyle {\\begin{aligned}{\\text{sensitivity}}&={\\frac {\\text{number of true positives}}{{\\text{number of true positives}}+{\\text{number of false negatives}}}}\\\\[8pt]&={\\frac {\\text{number of true positives}}{\\text{total number of sick individuals in population}}}\\\\[8pt]&={\\text{probability of a positive test given that the patient has the disease}}\\end{aligned}}}$","90a1c510":"# \u0421\u0432\u0451\u0440\u0442\u043e\u0447\u043d\u0430\u044f \u043d\u0441","63acbe32":"# \u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u043e\u0448\u0438\u0431\u043e\u043a (Confusion matrix)","d73452f1":"# Specificity \u0434\u043b\u044f \u043f\u0435\u0440\u0432\u043e\u0439 \u0441\u0435\u0442\u0438","07cd31eb":"# Convnet"}}