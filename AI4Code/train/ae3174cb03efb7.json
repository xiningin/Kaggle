{"cell_type":{"cf15ac74":"code","66f30ce5":"code","f9e5f024":"code","b719374b":"code","ba7ae0b5":"code","dbcd2fa2":"code","a08c910d":"code","7fffae40":"code","57db9497":"code","53fa00b0":"code","3948776e":"code","e644ceac":"code","f1548ec5":"code","2f53b1fd":"code","6d5284ca":"code","a82c7628":"code","25fb67c2":"code","010eddb4":"code","6bfe15bb":"code","30e579dc":"code","79cca3cd":"code","8d994f46":"code","f5098a66":"code","253124bc":"code","f502e030":"code","5d4f49ff":"code","b1b72b59":"code","9c15b7e2":"code","fdc9880e":"code","1dab4861":"code","10b1613a":"code","26efd334":"code","e6b840b1":"code","9bbd56f2":"code","62849872":"code","0d19b258":"code","a388f2a6":"code","603a35da":"code","95621cd7":"code","49849d22":"code","76485654":"code","cae909ab":"code","d106347c":"code","559bcdd6":"code","1a960279":"code","dcfaa14a":"code","edefde51":"code","e5610c81":"code","ce306a7e":"markdown","85dbee5b":"markdown","9dce7f9a":"markdown","d8ed7b2d":"markdown","5984e936":"markdown","a41f3b9c":"markdown","6d5af5cd":"markdown","793f7e50":"markdown","cd51b394":"markdown","f129bec0":"markdown","057b4ca7":"markdown","8f219a80":"markdown","a205f081":"markdown","749ab261":"markdown","8a4abf57":"markdown","e1759b41":"markdown","e17cf61a":"markdown","dcfd42fc":"markdown","93ae4fe4":"markdown","a1e82974":"markdown","e6eb97bc":"markdown","be018d4b":"markdown","20890298":"markdown","db3e8162":"markdown","94e29776":"markdown","18000643":"markdown"},"source":{"cf15ac74":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"\/kaggle\/input\"))\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datetime as dt\nimport math\nimport gc","66f30ce5":"train = pd.read_csv('..\/input\/train.csv', nrows=5000000)\ntest = pd.read_csv('..\/input\/test.csv')\ntrain.head()","f9e5f024":"print(train.isnull().sum())\ntrain = train.dropna(how='any',axis=0)","b719374b":"train['abs_diff_longitude'] = np.abs(train['dropoff_longitude'] - train['pickup_longitude'])\ntrain['abs_diff_latitude'] = np.abs(train['dropoff_latitude'] - train['pickup_latitude'])\ntest['abs_diff_longitude'] = np.abs(test['dropoff_longitude'] - test['pickup_longitude'])\ntest['abs_diff_latitude'] = np.abs(test['dropoff_latitude'] - test['pickup_latitude'])","ba7ae0b5":"# sns.relplot(x='passenger_count',y='fare_amount',data=train.iloc[:1000,:],kind='scatter')\nsns.countplot(x='passenger_count',data=train.iloc[:1000,:])\nsns.relplot(x='abs_diff_longitude', y=\"fare_amount\",data=train.loc[:1000,:])\nsns.relplot(x=\"abs_diff_latitude\", y='fare_amount',data=train.loc[:1000,:])","dbcd2fa2":"train = train.loc[train['fare_amount']>0,:]\ntrain = train.loc[(train[\"passenger_count\"]<=6) & (train[\"passenger_count\"]>0),:]\ntrain = train.loc[(train[\"abs_diff_latitude\"]<2) & (train[\"abs_diff_longitude\"]<2),:]","a08c910d":"train = train[train['fare_amount'].between(left = 2.5, right = 100)]","7fffae40":"# sns.relplot(x='passenger_count',y='fare_amount',data=train,kind='scatter')\nsns.countplot(x='passenger_count',data=train.iloc[:1000000,:])\n# f,ax = plt.subplots(1,2,figsize=(18,8))\nsns.relplot(x='abs_diff_longitude', y=\"fare_amount\",data=train.loc[:1000,:])\nsns.relplot(x=\"abs_diff_latitude\", y='fare_amount',data=train.loc[:1000,:])","57db9497":"train.loc[:,'timestamp_with_key'] = train.loc[:,'key'] \ntest.loc[:,'timestamp_with_key'] = test.loc[:,'key']\ntrain.key = pd.DataFrame({'key':train['key'].str.split('.').str[1].astype('int')})\ntest.key = pd.DataFrame({'key':test['key'].str.split('.').str[1].astype('int')})","53fa00b0":"# #using datetime try to analyse is there cor b\/w time and price (you could get time windows\n# #(divide a day into 8 parts) and add that feature and see cor b\/w time and price)\n# from math import floor\n# def chooseSlot(x):\n#     hr = x.hour\n#     return int(hr\/3 + 1)\n\n# train['pickup_datetime'] = pd.to_datetime(train['pickup_datetime'], infer_datetime_format=True).dt.tz_localize('UTC')\n# test['pickup_datetime'] = pd.to_datetime(test['pickup_datetime'], infer_datetime_format=True).dt.tz_localize('UTC')\n# train['time_slot'] = pd.DataFrame(list(map(lambda x : chooseSlot(x), train['pickup_datetime'][:])), index=train.index)\n# test['time_slot'] = pd.DataFrame(list(map(lambda x : chooseSlot(x), test['pickup_datetime'][:])), index=test.index)","3948776e":"train['pickup_datetime'] = pd.to_datetime(train['pickup_datetime'], infer_datetime_format=True).dt.tz_localize('UTC')\ntest['pickup_datetime'] = pd.to_datetime(test['pickup_datetime'], infer_datetime_format=True).dt.tz_localize('UTC')\ntrain.loc[:,'hour_no'] = train['pickup_datetime'][:].dt.strftime('%-H').astype('int')\ntest.loc[:,'hour_no'] = test['pickup_datetime'][:].dt.strftime('%-H').astype('int')\ntrain.loc[:,'weekday_no'] = train['pickup_datetime'][:].dt.strftime('%w').astype('int')\ntest.loc[:,'weekday_no'] = test['pickup_datetime'][:].dt.strftime('%w').astype('int')\ntrain.loc[:,'day_no'] =  train['pickup_datetime'][:].dt.strftime('%-d').astype('int')\ntest.loc[:,'day_no'] = test['pickup_datetime'][:].dt.strftime('%-d').astype('int')\ntrain.loc[:,'year_no'] = train['pickup_datetime'][:].dt.strftime('%-y').astype('int')\ntest.loc[:,'year_no'] = test['pickup_datetime'][:].dt.strftime('%-y').astype('int')","e644ceac":"sns.countplot(x='hour_no',data=train.loc[:1000,:])\n# sns.countplot(x='weekday_no',data=train.loc[:1000,:])\n# sns.countplot(x='year_no',data=train.loc[:1000,:])","f1548ec5":"# def eucledian(x):\n#     dist = np.sqrt( np.power(x[0]-x[2],2) + np.power(x[1]-x[3],2))\n#     return dist\n\n# train['dist_eucledian'] = pd.DataFrame(list(\n#     map(lambda x: eucledian(x), train[[\"pickup_longitude\",\"pickup_latitude\",\"dropoff_longitude\",\"dropoff_latitude\"]].values)),\n#                                       index=train.index)","2f53b1fd":"def dist_haversine(x):\n    R = 6371 #for metres 6371e3\n    picklat = math.radians(x[1])\n    droplat = math.radians(x[3])\n    latdiff = abs(droplat-picklat)\n    picklon = math.radians(x[0])\n    droplon = math.radians(x[2])\n    londiff = abs(droplon-picklon)\n\n    a = math.sin(latdiff\/2) * math.sin(latdiff\/2) +\\\n            math.cos(picklat) * math.cos(droplat) *\\\n            math.sin(londiff\/2) * math.sin(londiff\/2)\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n    return (R * c)\n\ntrain['dist_haversine_km'] = pd.DataFrame(list(\n    map(lambda x: dist_haversine(x), train[[\"pickup_longitude\",\"pickup_latitude\",\"dropoff_longitude\",\"dropoff_latitude\"]].values)),\n                                      index=train.index)\n\ntest['dist_haversine_km'] = pd.DataFrame(list(\n    map(lambda x: dist_haversine(x), test[[\"pickup_longitude\",\"pickup_latitude\",\"dropoff_longitude\",\"dropoff_latitude\"]].values)),\n                                      index=test.index)\n","6d5284ca":"# train['dist_haversine_km'] = 1\n# for i in range(9):\n#     train['dist_haversine_km'][(i*1000000):((i+1) * 1000000)] = pd.DataFrame(list(\n#         map(lambda x: dist_haversine(x),\n#             train[[\"pickup_longitude\",\"pickup_latitude\",\"dropoff_longitude\",\"dropoff_latitude\"]][(i*1000000):((i+1) * 1000000)].values)))\n                                                                             \n# # ),index=train.index[(i*1000000):((i+1) * 1000000)].reshape(len(train.index[(i*1000000):((i+1) * 1000000)]),1)\n                                                                             \n# train['dist_haversine_km'][(i*1000000):] = pd.DataFrame(list(\n#     map(lambda x: dist_haversine(x),\n#         train[[\"pickup_longitude\",\"pickup_latitude\",\"dropoff_longitude\",\"dropoff_latitude\"]][(i*1000000):].values)))\n# # ),index=train.index[(i*1000000):].reshape(len(train.index[(i*1000000):]),1)","a82c7628":"train['fare_per_km'] = train['fare_amount']\/(train[\"dist_haversine_km\"])\ntrain['fare_per_km_passenger'] = train['fare_amount']\/(train['dist_haversine_km']*train['passenger_count'])","25fb67c2":"train.groupby('key').agg({'fare_per_km_passenger':'mean','key':'count','passenger_count':'mean','fare_amount':'mean'})","010eddb4":"train.loc[train['fare_per_km_passenger']>20,['fare_per_km_passenger','fare_amount','dist_haversine_km']]\n# train[['fare_per_km_passenger','fare_amount','dist_haversine_km','passenger_count']][30:60]","6bfe15bb":"grouped_df = train.groupby('key')\ncount = 0\nfor key, item in grouped_df:\n    count += 1\n    if count == 2: ## to view key = 2\n        filtered = grouped_df.get_group(key)[\"dist_haversine_km\"]>1 #ignoring drives within 1km\n        df = pd.DataFrame(grouped_df.get_group(key).loc[filtered,:].sort_values(by='pickup_datetime'))\n#         print(grouped_df.get_group(key)[['pickup_datetime','fare_per_km_passenger']][grouped_df.get_group(key)[\"dist_haversine_km\"]>1].describe())\n        break\n\nindexes = ['key',df['pickup_datetime'].dt.strftime('%a'),'hour_no']\ngrouped = df[:][:].groupby(indexes).agg({'fare_per_km_passenger':'mean','fare_per_km':'mean','fare_amount':'count'}) \ngrouped.rename(columns={'fare_amount':'count'},inplace=True)\ngrouped","30e579dc":"reindexed = grouped.reset_index().drop('key',axis=1)\nget_max_count = reindexed.groupby(['pickup_datetime']).agg({'count':'max'})\nget_max_count = get_max_count.reindex(reindexed['pickup_datetime'], method='ffill')\nreindexed = reindexed.set_index('pickup_datetime')\nreindexed.loc[get_max_count['count'] == reindexed['count'],:]","79cca3cd":"del reindexed, grouped, get_max_count, grouped_df","8d994f46":"#train.loc[ ((train['fare_per_km']<0.2) & (train['dist_haversine_km']>1))]\ntrain = train.loc[ ~ (((train['dist_haversine_km']>1) & train['fare_per_km']<0.2))]\ntrain = train.loc[~ ((train['dist_haversine_km']<0.01) & (train['fare_per_km']>50))]","f5098a66":"train = train.drop(['fare_per_km_passenger','fare_per_km'],axis=1)","253124bc":"# train['pickup_latitude'].shape[0] - train['pickup_latitude'].between(40,41).sum()\n# print(train.shape[0] - train.loc[train['pickup_latitude'].between(40.5,41) & train['dropoff_latitude'].between(40.5,41) &\n#           train['pickup_longitude'].between(-74,-73.9) &  train['dropoff_longitude'].between(-74,-73.9)].shape[0])\n# old_train = train.copy()\n\ntrain = train.loc[train['pickup_latitude'].between(40, 42)]\ntrain = train.loc[train['pickup_longitude'].between(-75, -72)]\ntrain = train.loc[train['dropoff_latitude'].between(40, 42)]\ntrain = train.loc[train['dropoff_longitude'].between(-75, -72)]\n\n# train = train.loc[train['pickup_latitude'].between(40.5,41) & train['dropoff_latitude'].between(40.5,41) &\n#                   train['pickup_longitude'].between(-74,-73.9) &  train['dropoff_longitude'].between(-74,-73.9)]\n# dont alter test data","f502e030":"train.loc[:,'pickuplat_no'], pick_lat_bin = pd.cut(train['pickup_latitude'],100, labels=False, retbins=True)\ntrain.loc[:,'pickuplong_no'], pick_long_bin = pd.cut(train['pickup_longitude'],100, labels=False, retbins=True)\ntrain.loc[:,'dropofflat_no'], drop_lat_bin = pd.cut(train['dropoff_latitude'],100, labels=False, retbins=True)\ntrain.loc[:,'dropofflong_no'], drop_long_bin = pd.cut(train['dropoff_longitude'],100, labels=False, retbins=True)\ntest['pickuplat_no'] = pd.cut(test['pickup_latitude'], pick_lat_bin, labels=False).fillna(int(train.loc[:,'pickuplat_no'].mean()))\ntest['pickuplong_no'] = pd.cut(test['pickup_longitude'], pick_long_bin, labels=False).fillna(int(train.loc[:,'pickuplong_no'].mean()))\ntest['dropofflat_no'] = pd.cut(test['dropoff_latitude'], drop_lat_bin, labels=False).fillna(int(train.loc[:,'dropofflat_no'].mean()))\ntest['dropofflong_no'] = pd.cut(test['dropoff_longitude'], drop_long_bin, labels=False).fillna(int(train.loc[:,'dropofflong_no'].mean()))","5d4f49ff":"train.loc[:,'pickdrop_lat_diff'] = abs(train['pickuplat_no'].astype(int) - train['dropofflat_no'].astype(int))\ntrain.loc[:,'pickdrop_long_diff'] = abs(train['pickuplong_no'].astype(int) - train['dropofflong_no'].astype(int))\ntrain.loc[:,'final_dist_factor'] = (train['pickdrop_lat_diff'].astype(int) + train['pickdrop_long_diff'].astype(int))\ntest.loc[:,'pickdrop_lat_diff'] = abs(test['pickuplat_no'].astype(int) - test['dropofflat_no'].astype(int))\ntest.loc[:,'pickdrop_long_diff'] = abs(test['pickuplong_no'].astype(int) - test['dropofflong_no'].astype(int))\ntest.loc[:,'final_dist_factor'] = (test['pickdrop_lat_diff'].astype(int) + test['pickdrop_long_diff'].astype(int))","b1b72b59":"#  train.groupby('pickuplat_no')['key'].count().sort_values(ascending=False).reset_index()","9c15b7e2":"def calc_cwd_factor(df, col):\n    new_df = df.groupby(col)['key'].count().sort_values(ascending=False).reset_index()\n    new_df['cwd_factor'] = 1\n    count = 1\n    for i in range(1,new_df.shape[0]):\n        count += 1\n        if new_df.loc[i-1,'key'] == new_df.loc[i,'key']:\n            count -= 1\n        new_df.loc[i,'cwd_factor'] = count\n    new_df.index = new_df[col]\n    return new_df\n\nfact_df = calc_cwd_factor(train, 'pickuplat_no')\ntrain.loc[:,'pickuplat_cwd_factor'] = list(map(lambda x: fact_df.loc[x,'cwd_factor'], train['pickuplat_no']))\nfact_df = calc_cwd_factor(train, 'pickuplong_no')\ntrain.loc[:,'pickuplong_cwd_factor'] = list(map(lambda x: fact_df.loc[x,'cwd_factor'], train['pickuplong_no']))\nfact_df = calc_cwd_factor(train, 'dropofflat_no')\ntrain.loc[:,'dropofflat_cwd_factor'] = list(map(lambda x: fact_df.loc[x,'cwd_factor'], train['dropofflat_no']))\nfact_df = calc_cwd_factor(train, 'dropofflong_no')\ntrain.loc[:,'dropofflong_cwd_factor'] = list(map(lambda x: fact_df.loc[x,'cwd_factor'], train['dropofflong_no']))\n#do the same for test set:\nfact_df = calc_cwd_factor(test, 'pickuplat_no')\ntest.loc[:,'pickuplat_cwd_factor'] = list(map(lambda x: fact_df.loc[x,'cwd_factor'], test['pickuplat_no']))\nfact_df = calc_cwd_factor(test, 'pickuplong_no')\ntest.loc[:,'pickuplong_cwd_factor'] = list(map(lambda x: fact_df.loc[x,'cwd_factor'], test['pickuplong_no']))\nfact_df = calc_cwd_factor(test, 'dropofflat_no')\ntest.loc[:,'dropofflat_cwd_factor'] = list(map(lambda x: fact_df.loc[x,'cwd_factor'], test['dropofflat_no']))\nfact_df = calc_cwd_factor(test, 'dropofflong_no')\ntest.loc[:,'dropofflong_cwd_factor'] = list(map(lambda x: fact_df.loc[x,'cwd_factor'], test['dropofflong_no']))\ndel fact_df","fdc9880e":"print(train.shape,test.shape)","1dab4861":"gc.collect()","10b1613a":"import sklearn\nfrom sklearn import *\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import shuffle","26efd334":"# orig_train = train.copy()\n# orig_test = test.copy()\n# train = orig_train.copy()\n# test = orig_test.copy()","e6b840b1":"train = shuffle(train.iloc[:,:]).reset_index(drop=True)\nval = train.iloc[int(0.9*train.shape[0]):,:]\ntrain = train.iloc[:int(0.9*train.shape[0]),:]\n\n# scaler = StandardScaler().fit(train.loc[:,train_cols])\n# train.loc[:,train_cols] = scaler.transform(train.loc[:,train_cols])\n# val.loc[:,train_cols] = scaler.transform(val.loc[:,train_cols])         \n# test.loc[:,test_cols] = scaler.transform(test.loc[:,test_cols])\n\n# cols_to_normalize = ['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude',\n#               'abs_diff_longitude', 'abs_diff_latitude', 'dist_haversine_km']\n# transformer = Normalizer().fit(train.loc[:,cols_to_normalize])\n# train.loc[:,cols_to_normalize] = transformer.transform(train.loc[:,cols_to_normalize])\n# val.loc[:,cols_to_normalize] = transformer.transform(val.loc[:,cols_to_normalize])         \n# test.loc[:,cols_to_normalize] = transformer.transform(test.loc[:,cols_to_normalize])","9bbd56f2":"#pearson correlation:\nimport seaborn as sns\n\nobject_cols = ['key','pickup_datetime','timestamp_with_key']\n\ncategorical_cols = ['passenger_count', 'hour_no', 'weekday_no', 'day_no', 'year_no', 'pickuplat_no', 'pickuplong_no',\n'dropofflat_no', 'dropofflong_no', 'pickdrop_lat_diff', 'pickdrop_long_diff', 'final_dist_factor',\n'pickuplat_cwd_factor', 'pickuplong_cwd_factor', 'dropofflat_cwd_factor', 'dropofflong_cwd_factor']\n\nnumerical_cols =['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude',\n'abs_diff_longitude', 'abs_diff_latitude', 'dist_haversine_km']\n\ntarget = ['fare_amount']\n\ncor = train.loc[:,numerical_cols+target]\nf, ax = plt.subplots(1,1,figsize=(18,7))\nsns.heatmap(cor.corr(), annot=True)","62849872":"gc.collect()","0d19b258":"#f_regression,\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.feature_selection import mutual_info_regression\n\nf_reg = f_regression(train[categorical_cols+numerical_cols],train['fare_amount'], center=True)\nmir_c = mutual_info_regression(train[categorical_cols][:10000],  train['fare_amount'][:10000])\nmir_n = mutual_info_regression(train[numerical_cols][:10000],  train['fare_amount'][:10000])\n\nimpt_cols_freg = list(zip(f_reg[0], categorical_cols+numerical_cols ))\nimpt_cols_freg = sorted(impt_cols_freg, key = lambda x:x[0], reverse=True)\nimpt_cols_c = list(zip(mir_c ,categorical_cols))\nimpt_cols_c = sorted(impt_cols_c, key = lambda x:x[0], reverse=True)\nimpt_cols_n = list(zip(mir_n ,numerical_cols))\nimpt_cols_n = sorted(impt_cols_n, key = lambda x:x[0], reverse=True)","a388f2a6":"drop_cols = []\nfor i in impt_cols_c[-8:]:\n    drop_cols.append(i[1])\ntrain = train.drop(drop_cols, axis=1)\ntest = test.drop(drop_cols, axis=1)","603a35da":"gc.collect()","95621cd7":"train_y = train['fare_amount'][:]\nval_y = val['fare_amount'][:]\ncols = [i for i in train.columns if i not in ['fare_amount'] + object_cols]\ntrain_x = train.loc[:, cols]\nval_x = val.loc[:, cols]\ntest_x = test.loc[:,cols]\n# print(train.columns, train.shape)\n# print(val.columns, val.shape)\n# print(test.columns, test.shape)","49849d22":"del train, val","76485654":"# import xgboost as xgb\n# from xgboost import XGBRegressor\n\n# xgbr = XGBRegressor()\n# xgb_train = xgbr.fit(train_x, train_y)\n\n# pred_train = xgbr.predict(train_x).round(decimals = 2)\n# pred_val = xgbr.predict(val_x).round(decimals = 2)\n# pred_test = xgbr.predict(test_x).round(decimals = 2)","cae909ab":"# from sklearn.model_selection import GridSearchCV\n# #xgbr = XGBRegressor()\n# params = {\n#         'nthread': [4],\n#         'num_leaves': [30, 35, 40],\n#         'learning_rate': [0.03, 0.05, 0.08], \n# #         'max_depth': [],\n#         'subsample': [0.8],\n#         'bagging_fraction' : [1],\n#         'max_bin' : [5000] ,\n#         'bagging_freq': [20],\n#         'colsample_bytree': [0.6],\n#         'metric': ['rmse'],\n#         'min_split_gain': [0.5],\n#         'min_child_weight': [1],\n#         'min_child_samples': [10],\n#         'scale_pos_weight':[1],\n#         'seed':[0],\n#         'num_rounds':[50000]\n#     }\n# xgb_grid = GridSearchCV(xgbr,\n#                         params,\n#                         n_jobs = 4,\n#                         verbose=True)\n\n# xgb_grid.fit(train_x, train_y)\n\n# print(xgb_grid.best_score_)\n# print(xgb_grid.best_params_)\n\n# pred_train = xgbr_grid.predict(train_x).round(decimals = 2)\n# pred_val = xgbr_grid.predict(val_x).round(decimals = 2)\n# pred_test = xgbr_grid.predict(test_x).round(decimals = 2)","d106347c":"import lightgbm as lgbm\n\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'nthread': 4,\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': True,\n        'seed':0,\n        'num_rounds':50000\n    }\ncols = list(train_x.select_dtypes(exclude=['float64']).columns)\n\ntrain_set = lgbm.Dataset(train_x, train_y, silent=False,categorical_feature=cols)\nvalid_set = lgbm.Dataset(val_x, val_y, silent=False,categorical_feature=cols)\n\nmodel = lgbm.train(params, train_set = train_set, num_boost_round=10000,early_stopping_rounds=500,verbose_eval=500, valid_sets=valid_set)","559bcdd6":"pred_train = model.predict(train_x, num_iteration = model.best_iteration)#.round(decimals = 2)\npred_val = model.predict(val_x, num_iteration = model.best_iteration)#.round(decimals = 2)\npred_test = model.predict(test_x, num_iteration = model.best_iteration)#.round(decimals = 2)","1a960279":"gc.collect()","dcfaa14a":"from sklearn.metrics import mean_squared_error\nrmse_train = np.sqrt(mean_squared_error(train_y, pred_train))\nrmse_val = np.sqrt(mean_squared_error(val_y, pred_val))\nprint(rmse_train, rmse_val)","edefde51":"# import eli5\n# from eli5.sklearn import PermutationImportance\n\n# perm = PermutationImportance(lgbm, random_state=1).fit(val_x, val_y)\n# eli5.show_weights(perm, feature_names = val_x.columns.tolist())","e5610c81":"final = pd.DataFrame({'key':test.timestamp_with_key, 'fare_amount':pred_test}, columns = ['key', 'fare_amount'])\nfinal.to_csv('submission_lgb.csv', index = False)\nfinal.head()","ce306a7e":"We'll split timestamp and key column so that we can perform groupby operations on key.","85dbee5b":"How come fare_per_km_passenger values are so high, this makes no sense because some rides have very high fare so mean would be more than expected. Lets check whats happening.","9dce7f9a":"Minimum fare is 2.5 in NY, so drop outliers.","d8ed7b2d":"As all those above created features points to specific locations of New York, if we take difference it could relate to distance as well.","5984e936":"Now lets try to divide the whole new york into 100 places and get correlation b\/w places and prices.This can be using pd.cut() on train set and use the same bins of train set to cut test set. Similar to fit-transform on train and transform on test.","a41f3b9c":"Now plot them again and check to see any changes.","6d5af5cd":"Lets do some **feature selection** or atleast know which features are very important.\n* correlations - heatmaps \n* shuffling columns and seeing accuracies\n\n![image.png](attachment:image.png) All we need now is Pearson correlation and Anova","793f7e50":"Delete all missing values because there are only few out of millions of data points . Dont drop from test data.","cd51b394":"Delete some of the possible outliers such as, data with distance is more than kilometer but fare amount is less and vice-versa.","f129bec0":"\n* some have negative fare_amount.\n* some obs have more passenger count(>6) and still fare_amount is 0.\n* some obs have 0 passenger_count\n* outliers are making visualisation very difficult, remove them\n* clearly many have false data so delete it.\n* most of the passengers travel alone","057b4ca7":"Visualize how taxi counts vary according to date and time.","8f219a80":"There is something wrong here, latitudes and longitudes of New york are around 40 and -73 respectively.Lets check how many have wrong values. We'll drop observations that have location values way out of new york.","a205f081":"Shuffling each column data and seeing final rmse on validation to see how much value each column is adding for predicting target variable.","749ab261":"Find out absolute difference between longitudes and latitudes which could relate to distance.","8a4abf57":"Lets create features based on price per distance and also per passenger.","e1759b41":"Actually Eucledian distance doesn't make any sense as lat and log is in degrees. In order to obtain distance in meters, miles (used kilometer here) use Haversine distance measure.","e17cf61a":"Group by latitudes and longitudes based on count to see crowded area, and assign crowd factor 1 for most crowded area and 2 for next most crowded area and so on. (To see groupby data uncomment below). \nIf count at a particular region is same as other region allocate same crowd factor for both.","dcfd42fc":"Plot different variables and see what we can get.","93ae4fe4":"Let me know in the comments if there are any suggestions. ","a1e82974":"The column Key indicates the ids of the taxis not customers!!. So anything directly related to customers cant be obtained such as:\nTo check if fares decreases or increases for frequent customers based on distance and datetime.","e6eb97bc":"* Number of taxi drives are more during night time. But Fares are high between  9am to 3pm.","be018d4b":"As there map() is used calculation takes long time. If there is better way let me know.","20890298":"we are getting so high fare\/km\/passenger not just because some observations have wrong data but also because we are calculating fare\/km, when haversine dist is less than one km it is increasing the fare prices to calculate per km. \nSo what we know now is, for very short distances they charge more than actual price (particularly if the distance is less than a kilometer, because even if the prices are extrapolated to kilometer, prices shouldn't have been so high).","db3e8162":"We cannot use fare_per_km_passenger for training bcuz it is dependent on fare_amount(target variable). we used it just to check how hour_no and haversine distance correlate with taxi fares.","94e29776":"Now lets analyse indivisual keys to know which time slots have high prices.","18000643":"Using datetime data create features for hour, weekday, year, so that we could analyse correlation between time and fare_amount."}}