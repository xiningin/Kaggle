{"cell_type":{"4fe05012":"code","b8550a44":"code","91041c47":"code","e8fe821c":"code","224dfabc":"code","704eda52":"code","86f96556":"code","e6ab250e":"code","9a6d86f9":"code","0dd38900":"code","a82f5ed5":"code","078e5de0":"code","1a6ae4ab":"code","9671b46d":"code","7cd65810":"code","db7413ca":"code","b2184c41":"code","4774d240":"code","10336ab8":"code","c2d4a5bc":"code","5d4e6402":"code","c2bd81ae":"code","3be1e255":"code","26ad2a30":"code","6b5de256":"code","d4a62ee8":"code","523090e6":"code","8c528426":"code","74655eac":"code","aaa263ae":"code","dd9b3646":"code","e1fae775":"code","2823c235":"code","3525b95a":"code","5526946d":"code","c46b32b4":"code","8b07832e":"code","5901c6a1":"markdown","a419c880":"markdown","02bc94b9":"markdown","73e6485e":"markdown","db7922bb":"markdown","3251c3d0":"markdown","fadeb741":"markdown","17d9ec32":"markdown","3b6961d0":"markdown","1ae2ad08":"markdown","623a2ccd":"markdown","8f1a2609":"markdown","9f1c874a":"markdown","33ccd535":"markdown","504d9262":"markdown","ffc2cf80":"markdown","16a07a5a":"markdown","887f922a":"markdown","7c0dd6d7":"markdown","1fcc18eb":"markdown"},"source":{"4fe05012":"\n{\n   \"schemaVersion\": 2,\n   \"mediaType\": \"application\/vnd.docker.distribution.manifest.v2+json\",\n   \"config\": {\n      \"mediaType\": \"application\/vnd.docker.container.image.v1+json\",\n      \"size\": 14966,\n      \"digest\": \"sha256:328178db27b4735499af91d40fefd588e49ccc87d1b88a5edbb9fed6a89edb4d\"\n   },\n   \"layers\": [\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 50382957,\n         \"digest\": \"sha256:7e2b2a5af8f65687add6d864d5841067e23bd435eb1a051be6fe1ea2384946b4\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 222909892,\n         \"digest\": \"sha256:59c89b5f9b0c6d94c77d4c3a42986d420aaa7575ac65fcd2c3f5968b3726abfc\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 195204532,\n         \"digest\": \"sha256:4017849f9f85133e68a4125e9679775f8e46a17dcdb8c2a52bbe72d0198f5e68\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 1522,\n         \"digest\": \"sha256:c8b29d62979a416da925e526364a332b13f8d5f43804ae98964de2a60d47c17a\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 717,\n         \"digest\": \"sha256:12004028a6a740ac35e69f489093b860968cc37b9668f65b1e2f61fd4c4ad25c\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 247,\n         \"digest\": \"sha256:3f09b9a53dfb03fd34e35d43694c2d38656f7431efce0e6647c47efb5f7b3137\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 408,\n         \"digest\": \"sha256:03ed58116b0cb733cc552dc89ef5ea122b6c5cf39ec467f6ad671dc0ba35db0c\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 331594702,\n         \"digest\": \"sha256:7844554d9ef75bb3f1d224e166ed12561e78add339448c52a8e5679943b229f1\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 112665095,\n         \"digest\": \"sha256:c84072c312680c652a6c518b253687b37a1693af7a1752b9700f394c27eeb40a\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 426,\n         \"digest\": \"sha256:acf8406fd31d7cd1a5d10608ee0da8c774e6b30d148cdca688324bf6244d6293\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 5494,\n         \"digest\": \"sha256:abe41b7b0add0c404b69a76a20e2ed75574a584679184b791a3e0812e75a0a34\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 1948,\n         \"digest\": \"sha256:3281cc773713413d3c1b4f2e92887d2051cb23972abbf796f8d1a3204ae7a980\"\n      },\n      {\n         \"mediaType\": \"application\/vnd.docker.image.rootfs.diff.tar.gzip\",\n         \"size\": 2456434655,\n         \"digest\": \"sha256:bf03198f391f55ab1b27d866c137f28faa8b8a0ddc3131e4957cb733ca97c678\"\n      }\n   ]\n}\n","b8550a44":"\nfrom __future__ import print_function, division, absolute_import\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom keras.layers import Embedding, Dense, Dropout, Input, Reshape, Concatenate, BatchNormalization\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nimport logging\nimport numpy as np\nfrom scipy import sparse\nfrom sklearn import base\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\nimport pandas as pd\nimport tensorflow as tf\n\nfrom .const import NAN_INT, MIN_EMBEDDING\n\n\nlogger = logging.getLogger('Kaggler')\nEMBEDDING_SUFFIX = '_emb'\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\n\n\nclass LabelEncoder(base.BaseEstimator):\n    \"\"\"Label Encoder that groups infrequent values into one label.\n\n    Attributes:\n        min_obs (int): minimum number of observation to assign a label.\n        label_encoders (list of dict): label encoders for columns\n        label_maxes (list of int): maximum of labels for columns\n    \"\"\"\n\n    def __init__(self, min_obs=10):\n        \"\"\"Initialize the OneHotEncoder class object.\n\n        Args:\n            min_obs (int): minimum number of observation to assign a label.\n        \"\"\"\n\n        self.min_obs = min_obs\n\n    def __repr__(self):\n        return ('LabelEncoder(min_obs={})').format(self.min_obs)\n\n    def _get_label_encoder_and_max(self, x):\n        \"\"\"Return a mapping from values and its maximum of a column to integer labels.\n\n        Args:\n            x (pandas.Series): a categorical column to encode.\n\n        Returns:\n            (tuple):\n                - (dict): mapping from values of features to integers\n                - (int): maximum label\n        \"\"\"\n\n        # NaN cannot be used as a key for dict. Impute it with a random\n        # integer.\n        label_count = x.fillna(NAN_INT).value_counts()\n        n_uniq = label_count.shape[0]\n\n        label_count = label_count[label_count >= self.min_obs]\n        n_uniq_new = label_count.shape[0]\n\n        # If every label appears more than min_obs, new label starts from 0.\n        # Otherwise, new label starts from 1 and 0 is used for all old labels\n        # that appear less than min_obs.\n        offset = 0 if n_uniq == n_uniq_new else 1\n\n        label_encoder = pd.Series(np.arange(n_uniq_new) + offset,\n                                  index=label_count.index)\n        max_label = label_encoder.max()\n        label_encoder = label_encoder.to_dict()\n\n        return label_encoder, max_label\n\n    def _transform_col(self, x, i):\n        \"\"\"Encode one categorical column into labels.\n\n        Args:\n            x (pandas.Series): a categorical column to encode\n            i (int): column index\n\n        Returns:\n            (pandas.Series): a column with labels.\n        \"\"\"\n        return x.fillna(NAN_INT).map(self.label_encoders[i]).fillna(0).astype(int)\n\n    def fit(self, X, y=None):\n        self.label_encoders = [None] * X.shape[1]\n        self.label_maxes = [None] * X.shape[1]\n\n        for i, col in enumerate(X.columns):\n            self.label_encoders[i], self.label_maxes[i] = \\\n                self._get_label_encoder_and_max(X[col])\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Encode categorical columns into label encoded columns\n\n        Args:\n            X (pandas.DataFrame): categorical columns to encode\n\n        Returns:\n            (pandas.DataFrame): label encoded columns\n        \"\"\"\n\n        X = X.copy()\n        for i, col in enumerate(X.columns):\n            X.loc[:, col] = self._transform_col(X[col], i)\n\n        return X\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Encode categorical columns into label encoded columns\n\n        Args:\n            X (pandas.DataFrame): categorical columns to encode\n\n        Returns:\n            (pandas.DataFrame): label encoded columns\n        \"\"\"\n\n        self.label_encoders = [None] * X.shape[1]\n        self.label_maxes = [None] * X.shape[1]\n\n        for i, col in enumerate(X.columns):\n            self.label_encoders[i], self.label_maxes[i] = \\\n                self._get_label_encoder_and_max(X[col])\n\n            X.loc[:, col] = (X[col].fillna(NAN_INT)\n                             .map(self.label_encoders[i])\n                             .fillna(0))\n\n        return X\n\n\nclass OneHotEncoder(base.BaseEstimator):\n    \"\"\"One-Hot-Encoder that groups infrequent values into one dummy variable.\n\n    Attributes:\n        min_obs (int): minimum number of observation to create a dummy variable\n        label_encoders (list of (dict, int)): label encoders and their maximums\n                                              for columns\n    \"\"\"\n\n    def __init__(self, min_obs=10):\n        \"\"\"Initialize the OneHotEncoder class object.\n\n        Args:\n            min_obs (int): minimum number of observations required to create\n                a dummy variable\n            label_encoder (LabelEncoder): LabelEncoder that transofrm\n        \"\"\"\n\n        self.min_obs = min_obs\n        self.label_encoder = LabelEncoder(min_obs)\n\n    def __repr__(self):\n        return ('OneHotEncoder(min_obs={})').format(self.min_obs)\n\n    def _transform_col(self, x, i):\n        \"\"\"Encode one categorical column into sparse matrix with one-hot-encoding.\n\n        Args:\n            x (pandas.Series): a categorical column to encode\n            i (int): column index\n\n        Returns:\n            (scipy.sparse.coo_matrix): sparse matrix encoding a categorical\n                                       variable into dummy variables\n        \"\"\"\n\n        labels = self.label_encoder._transform_col(x, i)\n        label_max = self.label_encoder.label_maxes[i]\n\n        # build row and column index for non-zero values of a sparse matrix\n        index = np.array(range(len(labels)))\n        i = index[labels > 0]\n        j = labels[labels > 0] - 1  # column index starts from 0\n\n        if len(i) > 0:\n            return sparse.coo_matrix((np.ones_like(i), (i, j)),\n                                     shape=(x.shape[0], label_max))\n        else:\n            # if there is no non-zero value, return no matrix\n            return None\n\n    def fit(self, X, y=None):\n        self.label_encoder.fit(X)\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Encode categorical columns into sparse matrix with one-hot-encoding.\n\n        Args:\n            X (pandas.DataFrame): categorical columns to encode\n\n        Returns:\n            (scipy.sparse.coo_matrix): sparse matrix encoding categorical\n                                       variables into dummy variables\n        \"\"\"\n\n        for i, col in enumerate(X.columns):\n            X_col = self._transform_col(X[col], i)\n            if X_col is not None:\n                if i == 0:\n                    X_new = X_col\n                else:\n                    X_new = sparse.hstack((X_new, X_col))\n\n            logger.debug('{} --> {} features'.format(\n                col, self.label_encoder.label_maxes[i])\n            )\n\n        return X_new\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Encode categorical columns into sparse matrix with one-hot-encoding.\n\n        Args:\n            X (pandas.DataFrame): categorical columns to encode\n\n        Returns:\n            sparse matrix encoding categorical variables into dummy variables\n        \"\"\"\n\n        self.label_encoder.fit(X)\n\n        return self.transform(X)\n\n\nclass TargetEncoder(base.BaseEstimator):\n    \"\"\"Target Encoder that encode categorical values into average target values.\n\n    Smoothing and min_samples are added based on olivier's kernel at Kaggle:\n    https:\/\/www.kaggle.com\/ogrellier\/python-target-encoding-for-categorical-features\n\n    , which is based on Daniele Micci-Barreca (2001):\n    https:\/\/dl.acm.org\/citation.cfm?id=507538\n\n    Attributes:\n        target_encoders (list of dict): target encoders for columns\n    \"\"\"\n\n    def __init__(self, smoothing=1, min_samples=10, cv=kfold):\n        \"\"\"Initialize the TargetEncoder class object.\n\n        Args:\n            smoothing (int): smoothing effect to balance between the categorical average vs global mean\n            min_samples (int): minimum samples to take category average into account\n            cv (sklearn.model_selection._BaseKFold, optional): sklearn CV object. default=KFold(5, True, 42)\n        \"\"\"\n        assert (min_samples >= 0) and (smoothing >= 0), 'min_samples and smoothing should be positive'\n        self.smoothing = smoothing\n        self.min_samples = min_samples\n        self.cv = cv\n\n    def __repr__(self):\n        return('TargetEncoder(smoothing={}, min_samples={}, cv={})'.format(self.smoothing, self.min_samples, self.cv))\n\n    def _get_target_encoder(self, x, y):\n        \"\"\"Return a mapping from categories to average target values.\n\n        Args:\n            x (pandas.Series): a categorical column to encode.\n            y (pandas.Series): the target column\n\n        Returns:\n            (dict): mapping from categories to average target values\n        \"\"\"\n\n        assert len(x) == len(y)\n\n        # NaN cannot be used as a key for dict. So replace it with a random\n        # integer\n        mean_count = pd.DataFrame({y.name: y, x.name: x.fillna(NAN_INT)}).groupby(x.name)[y.name].agg(['mean', 'count'])\n        smoothing = 1 \/ (1 + np.exp(-(mean_count['count'] - self.min_samples) \/ self.smoothing))\n\n        mean_count[y.name] = self.target_mean * (1 - smoothing) + mean_count['mean'] * smoothing\n        return mean_count[y.name].to_dict()\n\n    def fit(self, X, y):\n        \"\"\"Encode categorical columns into average target values.\n\n        Args:\n            X (pandas.DataFrame): categorical columns to encode\n            y (pandas.Series): the target column\n\n        Returns:\n            (pandas.DataFrame): encoded columns\n        \"\"\"\n        self.target_encoders = [None] * X.shape[1]\n        self.target_mean = y.mean()\n\n        for i, col in enumerate(X.columns):\n            if self.cv is None:\n                self.target_encoders[i] = self._get_target_encoder(X[col], y)\n            else:\n                self.target_encoders[i] = []\n                for i_cv, (i_trn, i_val) in enumerate(self.cv.split(X[col], y), 1):\n                    self.target_encoders[i].append(self._get_target_encoder(X.loc[i_trn, col], y[i_trn]))\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Encode categorical columns into average target values.\n\n        Args:\n            X (pandas.DataFrame): categorical columns to encode\n\n        Returns:\n            (pandas.DataFrame): encoded columns\n        \"\"\"\n        for i, col in enumerate(X.columns):\n            if self.cv is None:\n                X.loc[:, col] = (X[col].fillna(NAN_INT)\n                                       .map(self.target_encoders[i])\n                                       .fillna(self.target_mean))\n            else:\n                for i_enc, target_encoder in enumerate(self.target_encoders[i], 1):\n                    if i_enc == 1:\n                        x = X[col].fillna(NAN_INT).map(target_encoder).fillna(self.target_mean)\n                    else:\n                        x += X[col].fillna(NAN_INT).map(target_encoder).fillna(self.target_mean)\n\n                X.loc[:, col] = x \/ i_enc\n\n        return X.astype(float)\n\n    def fit_transform(self, X, y):\n        \"\"\"Encode categorical columns into average target values.\n\n        Args:\n            X (pandas.DataFrame): categorical columns to encode\n            y (pandas.Series): the target column\n\n        Returns:\n            (pandas.DataFrame): encoded columns\n        \"\"\"\n        self.target_encoders = [None] * X.shape[1]\n        self.target_mean = y.mean()\n\n        for i, col in enumerate(X.columns):\n            if self.cv is None:\n                self.target_encoders[i] = self._get_target_encoder(X[col], y)\n\n                X.loc[:, col] = (X[col].fillna(NAN_INT)\n                                       .map(self.target_encoders[i])\n                                       .fillna(self.target_mean))\n            else:\n                self.target_encoders[i] = []\n                for i_cv, (i_trn, i_val) in enumerate(self.cv.split(X[col], y), 1):\n                    target_encoder = self._get_target_encoder(X.loc[i_trn, col], y[i_trn])\n\n                    X.loc[i_val, col] = (X.loc[i_val, col].fillna(NAN_INT)\n                                                          .map(target_encoder)\n                                                          .fillna(y[i_trn].mean()))\n\n                    self.target_encoders[i].append(target_encoder)\n\n        return X.astype(float)\n\n\nclass EmbeddingEncoder(base.BaseEstimator):\n    \"\"\"EmbeddingEncoder encodes categorical features to numerical embedding features.\n\n    Reference: 'Entity embeddings to handle categories' by Abhishek Thakur\n    at https:\/\/www.kaggle.com\/abhishek\/entity-embeddings-to-handle-categories\n    \"\"\"\n\n    def __init__(self, cat_cols, num_cols=[], n_emb=[], min_obs=10, n_epoch=100, batch_size=1024, cv=None,\n                 random_state=42):\n        \"\"\"Initialize an EmbeddingEncoder class object.\n\n        Args:\n            cat_cols (list of str): the names of categorical features to create embeddings for.\n            num_cols (list of str): the names of numerical features to train embeddings with.\n            n_emb (int or list of int): the numbers of embedding features used for columns.\n            min_obs (int): categories observed less than it will be grouped together before training embeddings\n            n_epoch (int): the number of epochs to train a neural network with embedding layer\n            batch_size (int): the size of mini-batches in model training\n            cv (sklearn.model_selection._BaseKFold): sklearn CV object\n            random_state (int): random seed.\n        \"\"\"\n        self.cat_cols = cat_cols\n        self.num_cols = num_cols\n\n        if isinstance(n_emb, int):\n            self.n_emb = [n_emb] * len(cat_cols)\n        elif isinstance(n_emb, list):\n            if not n_emb:\n                self.n_emb = [None] * len(cat_cols)\n            else:\n                assert len(cat_cols) == len(n_emb)\n                self.n_emb = n_emb\n        else:\n            raise ValueError('n_emb should be int or list')\n\n        self.min_obs = min_obs\n        self.n_epoch = n_epoch\n        self.batch_size = batch_size\n        self.cv = cv\n        self.random_state = random_state\n\n        self.lbe = LabelEncoder(min_obs=self.min_obs)\n\n    @staticmethod\n    def auc(y, p):\n        return tf.py_function(roc_auc_score, (y, p), tf.double)\n\n    @staticmethod\n    def _get_model(X, cat_cols, num_cols, n_uniq, n_emb, output_activation):\n        inputs = []\n        num_inputs = []\n        embeddings = []\n        for i, col in enumerate(cat_cols):\n\n            if not n_uniq[i]:\n                n_uniq[i] = X[col].nunique()\n            if not n_emb[i]:\n                n_emb[i] = max(MIN_EMBEDDING, 2 * int(np.log2(n_uniq[i])))\n\n            _input = Input(shape=(1,), name=col)\n            _embed = Embedding(input_dim=n_uniq[i], output_dim=n_emb[i], name=col + EMBEDDING_SUFFIX)(_input)\n            _embed = Dropout(.2)(_embed)\n            _embed = Reshape((n_emb[i],))(_embed)\n\n            inputs.append(_input)\n            embeddings.append(_embed)\n\n        if num_cols:\n            num_inputs = Input(shape=(len(num_cols),), name='num_inputs')\n            merged_input = Concatenate(axis=1)(embeddings + [num_inputs])\n\n            inputs = inputs + [num_inputs]\n        else:\n            merged_input = Concatenate(axis=1)(embeddings)\n\n        x = BatchNormalization()(merged_input)\n        x = Dense(128, activation='relu')(x)\n        x = Dropout(.5)(x)\n        x = BatchNormalization()(x)\n        x = Dense(64, activation='relu')(x)\n        x = Dropout(.5)(x)\n        x = BatchNormalization()(x)\n        output = Dense(1, activation=output_activation)(x)\n\n        model = Model(inputs=inputs, outputs=output)\n\n        return model, n_emb, n_uniq\n\n    def fit(self, X, y):\n        \"\"\"Train a neural network model with embedding layers.\n\n        Args:\n            X (pandas.DataFrame): categorical features to create embeddings for\n            y (pandas.Series): a target variable\n\n        Returns:\n            A trained EmbeddingEncoder object.\n        \"\"\"\n        is_classification = y.nunique() == 2\n\n        X_cat = self.lbe.fit_transform(X[self.cat_cols])\n        if is_classification:\n            assert np.isin(y, [0, 1]).all(), 'Target values should be 0 or 1 for classification.'\n            output_activation = 'sigmoid'\n            loss = 'binary_crossentropy'\n            metrics = [self.auc]\n            monitor = 'val_auc'\n            mode = 'max'\n        else:\n            output_activation = 'linear'\n            loss = 'mse'\n            metrics = ['mse']\n            monitor = 'val_mse'\n            mode = 'min'\n\n        n_uniq = [X_cat[col].nunique() for col in self.cat_cols]\n        if self.cv:\n            self.embs = []\n            n_fold = self.cv.get_n_splits(X)\n            for i_cv, (i_trn, i_val) in enumerate(self.cv.split(X, y), 1):\n                model, self.n_emb, _ = self._get_model(X_cat, self.cat_cols, self.num_cols, n_uniq, self.n_emb,\n                                                       output_activation)\n                model.compile(optimizer=Adam(lr=0.01), loss=loss, metrics=metrics)\n\n                features_trn = [X_cat[col][i_trn] for col in self.cat_cols]\n                features_val = [X_cat[col][i_val] for col in self.cat_cols]\n                if self.num_cols:\n                    features_trn += [X[self.num_cols].values[i_trn]]\n                    features_val += [X[self.num_cols].values[i_val]]\n\n                es = EarlyStopping(monitor=monitor, min_delta=.001, patience=5, verbose=1, mode=mode,\n                                   baseline=None, restore_best_weights=True)\n                rlr = ReduceLROnPlateau(monitor=monitor, factor=.5, patience=3, min_lr=1e-6, mode=mode)\n                model.fit(x=features_trn,\n                          y=y[i_trn],\n                          validation_data=(features_val, y[i_val]),\n                          epochs=self.n_epoch,\n                          batch_size=self.batch_size,\n                          callbacks=[es, rlr])\n\n                for i_col, col in enumerate(self.cat_cols):\n                    emb = model.get_layer(col + EMBEDDING_SUFFIX).get_weights()[0]\n                    if i_cv == 1:\n                        self.embs.append(emb \/ n_fold)\n                    else:\n                        self.embs[i_col] += emb \/ n_fold\n\n        else:\n            model, self.n_emb, _ = self._get_model(X_cat, self.cat_cols, self.num_cols, n_uniq, self.n_emb,\n                                                   output_activation)\n            model.compile(optimizer=Adam(lr=0.01), loss=loss, metrics=metrics)\n\n            features = [X_cat[col] for col in self.cat_cols]\n            if self.num_cols:\n                features += [X[self.num_cols].values]\n\n            es = EarlyStopping(monitor=monitor, min_delta=.001, patience=5, verbose=1, mode=mode,\n                               baseline=None, restore_best_weights=True)\n            rlr = ReduceLROnPlateau(monitor=monitor, factor=.5, patience=3, min_lr=1e-6, mode=mode)\n            model.fit(x=features,\n                      y=y,\n                      epochs=self.n_epoch,\n                      validation_split=.2,\n                      batch_size=self.batch_size,\n                      callbacks=[es, rlr])\n\n            self.embs = []\n            for i, col in enumerate(self.cat_cols):\n                self.embs.append(model.get_layer(col + EMBEDDING_SUFFIX).get_weights()[0])\n                logger.debug('{}: {}'.format(col, self.embs[i].shape))\n\n    def transform(self, X):\n        X_cat = self.lbe.transform(X[self.cat_cols])\n        X_emb = []\n\n        for i, col in enumerate(self.cat_cols):\n            X_emb.append(self.embs[i][X_cat[col].values])\n\n        return np.hstack(X_emb)\n\n    def fit_transform(self, X, y):\n        self.fit(X, y)\n        return self.transform(X)\n\n\nclass FrequencyEncoder(base.BaseEstimator):\n    \"\"\"Frequency Encoder that encode categorical values by counting frequencies.\n\n    Attributes:\n        frequency_encoders (list of dict): frequency encoders for columns\n    \"\"\"\n\n    def __init__(self, cv=None):\n        \"\"\"Initialize the FrequencyEncoder class object.\n        Args:\n            cv (sklearn.model_selection._BaseKFold, optional): sklearn CV object\n        \"\"\"\n        self.cv = cv\n\n    def __repr__(self):\n        return('FrequencyEncoder(cv={})'.format(self.cv))\n\n    def _get_frequency_encoder(self, x):\n        \"\"\"Return a mapping from categories to frequency.\n\n        Args:\n            x (pandas.Series): a categorical column to encode.\n\n        Returns:\n            (dict): mapping from categories to frequency\n        \"\"\"\n\n        # NaN cannot be used as a key for dict. So replace it with a random\n        # integer\n        df = pd.DataFrame({x.name: x.fillna('NaN')})\n        df[x.name + '_freq'] = df[x.name].map(df[x.name].value_counts())\n        return df.groupby(x.name)[x.name + '_freq'].size().to_dict()\n\n    def fit(self, X, y=None):\n        \"\"\"Encode categorical columns into frequency.\n\n        Args:\n            X (pandas.DataFrame): categorical columns to encode\n            y (pandas.Series, optional): the target column\n\n        Returns:\n            (pandas.DataFrame): encoded columns\n        \"\"\"\n        self.frequency_encoders = [None] * X.shape[1]\n\n        for i, col in enumerate(X.columns):\n            if self.cv is None:\n                self.frequency_encoders[i] = self._get_frequency_encoder(X[col])\n            else:\n                self.frequency_encoders[i] = []\n                for i_cv, (i_trn, i_val) in enumerate(self.cv.split(X[col]), 1):\n                    self.frequency_encoders[i].append(self._get_frequency_encoder(X.loc[i_trn, col]))\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Encode categorical columns into feature frequency counts.\n\n        Args:\n            X (pandas.DataFrame): categorical columns to encode\n\n        Returns:\n            (pandas.DataFrame): encoded columns\n        \"\"\"\n        for i, col in enumerate(X.columns):\n            if self.cv is None:\n                X.loc[:, col] = X[col].fillna('NaN').map(self.frequency_encoders[i]).fillna(0)\n            else:\n                for i_enc, frequency_encoder in enumerate(self.frequency_encoders[i], 1):\n                    if i_enc == 1:\n                        x = X[col].fillna('NaN').map(frequency_encoder).fillna(0)\n                    else:\n                        x += X[col].fillna('NaN').map(frequency_encoder).fillna(0)\n\n                X.loc[:, col] = x \/ i_enc\n\n        return X\n\n    def fit_transform(self, X, y=None):\n        \"\"\"Encode categorical columns into feature frequency counts.\n\n        Args:\n            X (pandas.DataFrame): categorical columns to encode\n            y (pandas.Series, optional): the target column\n        \"\"\"\n        self.frequency_encoders = [None] * X.shape[1]\n\n        for i, col in enumerate(X.columns):\n            if self.cv is None:\n                self.frequency_encoders[i] = self._get_frequency_encoder(X[col])\n\n                X.loc[:, col] = X[col].fillna('NaN').map(self.frequency_encoders[i]).fillna(0)\n            else:\n                self.frequency_encoders[i] = []\n                for i_cv, (i_trn, i_val) in enumerate(self.cv.split(X[col]), 1):\n                    frequency_encoder = self._get_frequency_encoder(X.loc[i_trn, col])\n\n                    X.loc[i_val, col] = X.loc[i_val, col].fillna('NaN').map(frequency_encoder).fillna(0)\n                    self.frequency_encoders[i].append(frequency_encoder)\n\n        return X\n","91041c47":"!pip install pysolr\n!pip install transformers\n!pip install pandas","e8fe821c":"from pysolr import Solr\nimport json\nimport os\nimport pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering, BertForQuestionAnswering\nimport shutil\nimport torch\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nfrom nltk.corpus import stopwords\nimport requests\nfrom bs4 import BeautifulSoup","224dfabc":"stop_words = set(stopwords.words('english'))\ntitle_dict = {}","704eda52":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"solr_server\")\nsolr = Solr(secret_value_0)","86f96556":"def preprocess(outer_folder, folder_name):\n    fname = \"..\/input\/CORD-19-research-challenge\/\"+outer_folder+folder_name + \"\/\"\n    contents = os.listdir(fname)\n    address = \"processed\/\" + folder_name + \"\/\"\n    new_files = []\n    result = solr.search(\"path:\"+ folder_name + \" path: \" + folder_name + \"\/\", rows=40000, **{\"fl\":\"id\"})\n    dct = set()\n    for i in result:\n        dct.add(address + i[\"id\"])\n    new_docs = set()\n    for i in contents:\n        d = json.load(open(fname+i))\n        id = d['paper_id']\n        path = folder_name\n        title=d['metadata']['title']\n        abstract = \"\"\n        if 'abstract' in d:\n            abst = d['abstract']\n            for j in abst:\n                abstract = abstract + j[\"text\"] + \"\\n\"\n        body = \"\"\n        for j in d['body_text']:\n            body = body + j[\"text\"] + \"\\n\"\n        data = {}\n        title_dict[id]=title\n        data['title']=title\n        data['abstract']=abstract\n        data['id']=id\n        data['body']=body\n        data['path']=path\n        with open(address+i,'w') as out:\n            json.dump(data,out)\n        new_docs.add(address + id)\n        #new_files.append(address+i)\n    to_add = new_docs.difference(dct)\n    to_remove = dct.difference(new_docs)\n    return [*to_add,], [*to_remove,]","e6ab250e":"# Create directory structure for processed files\ndef modify_directory_structure():\n    if not os.path.exists(\"processed\"):\n        os.mkdir(\"processed\")\n\n    if not os.path.exists(\"processed\/biorxiv_medrxiv\"):\n        os.mkdir(\"processed\/biorxiv_medrxiv\")\n    if not os.path.exists(\"processed\/biorxiv_medrxiv\/pdf_json\"):\n        os.mkdir(\"processed\/biorxiv_medrxiv\/pdf_json\")\n\n    if not os.path.exists(\"processed\/comm_use_subset\"):\n        os.mkdir(\"processed\/comm_use_subset\")\n    if not os.path.exists(\"processed\/comm_use_subset\/pdf_json\"):\n        os.mkdir(\"processed\/comm_use_subset\/pdf_json\")\n    if not os.path.exists(\"processed\/comm_use_subset\/pmc_json\"):\n        os.mkdir(\"processed\/comm_use_subset\/pmc_json\")\n\n    if not os.path.exists(\"processed\/noncomm_use_subset\"):\n        os.mkdir(\"processed\/noncomm_use_subset\")\n    if not os.path.exists(\"processed\/noncomm_use_subset\/pdf_json\"):\n        os.mkdir(\"processed\/noncomm_use_subset\/pdf_json\")\n    if not os.path.exists(\"processed\/noncomm_use_subset\/pmc_json\"):\n        os.mkdir(\"processed\/noncomm_use_subset\/pmc_json\")\n\n    if not os.path.exists(\"processed\/custom_license\"):\n        os.mkdir(\"processed\/custom_license\")\n    if not os.path.exists(\"processed\/custom_license\/pdf_json\"):\n        os.mkdir(\"processed\/custom_license\/pdf_json\")\n    if not os.path.exists(\"processed\/custom_license\/pmc_json\"):\n        os.mkdir(\"processed\/custom_license\/pmc_json\")\n","9a6d86f9":"#Updating index in batches of 500 to prevent http request timeout\ndef update_index(new_files):\n    j = 0\n    while j < len(new_files): \n        x = []\n        for i in new_files[j:j+500]:\n            if 'PMC' in i:\n                r = json.load(open(i+'.xml.json','r'))\n            else:\n                r = json.load(open(i+'.json','r'))\n            x.append(r)\n        j = j + 500\n        solr.add(x)\n        \n# Remove those documents from the index which have been removed from dataset\ndef clean_index(removable_files):\n    j = 0\n    while j < len(removable_files): \n        x = \"\"\n        for i in removable_files[j:j+10]:\n            x = x + \"id: \" + i.split('\/')[-1] + \" \"\n        j = j + 10 #Update is done in batches of size 10 due to URL size restriction\n        solr.delete(q=x)\n           \ndef handle_changes():\n    modify_directory_structure()\n    new_files = []\n    removable_files = []\n    n1, r1 = preprocess('biorxiv_medrxiv\/','biorxiv_medrxiv\/pdf_json')\n    new_files = new_files + n1\n    removable_files = removable_files + r1\n    print(len(new_files))\n    print(new_files[:10])\n    \n    n1, r1 = preprocess('comm_use_subset\/','comm_use_subset\/pdf_json')\n    new_files = new_files + n1\n    removable_files = removable_files + r1\n    \n    n1, r1 = preprocess('comm_use_subset\/','comm_use_subset\/pmc_json')\n    new_files = new_files + n1\n    removable_files = removable_files + r1\n    print(len(new_files))\n    \n    \n    n1, r1 = preprocess('noncomm_use_subset\/','noncomm_use_subset\/pdf_json')\n    new_files = new_files + n1\n    removable_files = removable_files + r1\n    \n    n1, r1 = preprocess('noncomm_use_subset\/','noncomm_use_subset\/pmc_json')\n    new_files = new_files + n1\n    removable_files = removable_files + r1\n    print(len(new_files))\n    \n    \n    n1, r1 = preprocess('custom_license\/','custom_license\/pdf_json')\n    new_files = new_files + n1\n    removable_files = removable_files + r1\n    \n    n1, r1 = preprocess('custom_license\/','custom_license\/pmc_json')\n    new_files = new_files + n1\n    removable_files = removable_files + r1\n    print(len(new_files))\n    \n    print(str(len(new_files)) + \" new files were found\")\n    print(\"Modifying search index... This might take some time\")\n    print(new_files[:10])\n    update_index(new_files)\n    clean_index(removable_files)\n    print(\"done updating\")","0dd38900":"handle_changes()","a82f5ed5":"def scraper():\n    terms = []\n    for i in range(26):\n        html = \"https:\/\/www.medicinenet.com\/medications\/alpha_\" + chr(97+i) + '.htm'\n        r = requests.get(html)\n        soup = BeautifulSoup(r.content, 'lxml')\n        c = soup.find('div', attrs = {'id':'AZ_container'})\n        z = c.findAll('li')\n        z = [i.text for i in z]\n        terms = terms + z\n    return terms","078e5de0":"!git clone https:\/\/github.com\/glutanimate\/wordlist-medicalterms-en\nwordnet_lemmatizer = WordNetLemmatizer()\n\nwords = open('wordlist-medicalterms-en\/wordlist.txt').readlines()\nwords = [wordnet_lemmatizer.lemmatize(i.strip()) for i in words]\nwords = words + scraper() + ['COVID-19']","1a6ae4ab":"def query_maker(query_nlp, medical_words):\n    \"\"\"\n        Formulates the query to send to Solr server for retrieving relevant documents.\n    \"\"\"\n    \n    query_words = query_nlp.strip().split()\n    query = \"(body:\"\n    essentials = []\n    \n    for i in query_words:\n        if i in stop_words:\n            continue\n        if i[0]=='+':\n            essentials.append(i)\n        elif wordnet_lemmatizer.lemmatize(i) in medical_words or i in medical_words or i.lower() in medical_words:\n            essentials.append(i)\n        else:\n            query = query + \" \" + i\n    query = query + \")\"\n    if query==\"(body:)\":\n        query=\"\"\n    for i in essentials:\n        if i[0]=='+':\n            query = query + \" body:\" + i[1:] + \"^4\"\n        else:\n            query = query + \" \" + \"+body: \" + i\n    print(query)\n    essesntials = [i for i in essentials if i[0]!='+']\n    return query, essentials","9671b46d":"def get_relevant_docs(query, max_docs=10, show_hits=False):\n    \"\"\"\n        Return contents of the relevant documents and score corresponding to a query\n    \"\"\"\n    result = solr.search(query, rows=max_docs, **{\"fl\":\"*,score\"}) #rows is length of result returned by server\n    doc_names = []\n    for i in result.docs:\n        if i[\"path\"][-1]=='\/':\n            doc_names.append((str(i[\"path\"])+str(i[\"id\"]),i[\"score\"]))\n        else:\n            doc_names.append((str(i[\"path\"])+\"\/\"+str(i[\"id\"]),i[\"score\"]))\n    docs = []\n    if show_hits:\n        print(result.hits)\n    for i in doc_names:\n        if \"pmc\" in i[0]:\n            dname = i[0] + \".xml.json\"\n        else:\n            dname = i[0] + \".json\"\n        docs.append((json.load(open('processed\/'+dname)), i[1]))\n    return docs","7cd65810":"def get_docs(query):\n    q, keywords = query_maker(query, words)\n    kw = [[i]+get_synonyms(i) for i in keywords]\n    result = get_relevant_docs(q,100,True)\n    p_ids = [i[0][\"id\"] for i in result]\n    titles = [i[0][\"title\"] for i in result]\n    abstracts = [i[0][\"abstract\"] for i in result]\n    body_lens = [len(i[0][\"body\"].split()) for i in result]\n    scores = [i[1] for i in result]\n    data = {\"id\":p_ids, \"score\":scores,\"title\":titles,\"length of body\":body_lens, \"abstract\":abstracts}\n    df = pd.DataFrame (data)\n    return result, kw, df","db7413ca":"# tokenizer = AutoTokenizer.from_pretrained(\"ktrapeznikov\/biobert_v1.1_pubmed_squad_v2\")\n\n# model = AutoModelForQuestionAnswering.from_pretrained(\"ktrapeznikov\/biobert_v1.1_pubmed_squad_v2\")\n\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"gdario\/biobert_bioasq\")\n\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"gdario\/biobert_bioasq\")\n\n# from transformers import AutoTokenizer, AutoModelForQuestionAnswering, BertForQuestionAnswering, BertTokenizer\n# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n# model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n\ntokenizer.add_tokens(['corona'])\ntokenizer.encode('assay')\nmodel.resize_token_embeddings(len(tokenizer))\n\n\n\nmodel = model.cuda()","b2184c41":"import nltk \nfrom nltk.corpus import wordnet \n\n\ndef get_synonyms(word):\n    synonyms = [] \n    for syn in wordnet.synsets(word): \n        for l in syn.lemmas(): \n            synonyms.append(l.name())\n    return synonyms","4774d240":"def has_keywords(context, keywords):\n    c2 = context.lower()\n    k = 0\n    for i in keywords:\n        #print(i)\n        b = False\n        for j in i:\n            if j.lower() in c2:\n                b = True\n                #k = k + 1\n            else:\n                continue\n        if b:\n            k = k + 1\n    return k > (2*len(keywords))\/3","10336ab8":"from spacy.lang.en import English # updated\nfrom nltk.corpus import wordnet\nimport re\n\nnlp = English()\nnlp.max_length = 5101418\nnlp.add_pipe(nlp.create_pipe('sentencizer')) # updated\npattern = re.compile(\"^\\[[0-9]*\\]$\")\n\ndef answer(context, question):\n    #context = context.lower()\n    context = context.replace('COVID-19', 'coronavirus disease\/COVID-19')\n    context = context.replace('MERS-CoV', 'MERS-coronavirus')\n    context = context.replace('SARS-CoV-2', 'coronavirus')\n    y = tokenizer.encode(question, context, max_length=256)\n    sep_index = y.index(tokenizer.sep_token_id)\n    num_seg_a = sep_index + 1\n    #print(num_seg_a)\n    num_seg_b = len(y) - num_seg_a\n    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n    #y = y.cuda()\n    with torch.no_grad():\n        start_scores, end_scores = model(torch.tensor([y]).cuda(), token_type_ids=torch.tensor([segment_ids]).cuda())\n    #start_scores, end_scores = model(torch.tensor([y]), token_type_ids=torch.tensor([segment_ids]))\n    answer_start = torch.argmax(start_scores)\n    answer_end = torch.argmax(end_scores)\n    if answer_start<num_seg_a:\n        return ()\n    else:\n        \n        fstops = [ i for i in range(len(y)) if y[i] == 119]\n        fstops = [num_seg_a] + fstops + [len(y)-1]\n        start = 0\n        for i in fstops:\n            if i + 1 <= answer_start:\n                start = i + 1\n        for i in fstops:\n            if i >= answer_end:\n                end = i\n                break\n        start = max(start, num_seg_a)\n        #print(\"Here\")\n        return [start_scores[0][answer_start] + end_scores[0][answer_end],tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(y[answer_start:answer_end+1])).strip().replace('corona virus', 'coronavirus'), \n                tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(y[start:end+1])).strip().replace('corona virus', 'coronavirus')]           \n    \ndef non_redundant(out, answers):\n    for i in answers:\n        if out[1]==i[1] or out[2]==i[2]:\n            return False\n    return True\n    \ndef get_answer(doc, question, keywords):\n    docx = nlp(doc[\"body\"].replace('\u25a0', ' '))\n    sentences = [sent.string.strip() for sent in docx.sents]\n    i = 0\n    context = \"\"\n    answers = []\n    prev_i = 0\n    while i < len(sentences):  #Use sliding window to search for answers in the document\n        if len(context.split())>=100:\n            if has_keywords(context, keywords): #Search for answers only if the context has all the identified keywords\n                out = answer(context, question)\n                if len(out) > 0 and non_redundant(out, answers) and out[1] not in question:\n                    if not pattern.match(out[1]):\n                        answers.append(out)\n                        prev_i += 2 #Slide the window further if an answer is found\n                        out = []\n            context = \"\"\n            i = prev_i + 1\n            prev_i = i\n        else:\n            context = context + \" \" + sentences[i]\n            i = i + 1\n            if i==len(sentences) and has_keywords(context, keywords):\n                out = answer(context, question)\n                if len(out) > 0 and non_redundant(out, answers) and out[1] not in question:\n                    \n                    if not pattern.match(out[1]):\n                        answers.append(out)\n                        out = []\n    answers = answers[1:]\n    if len(answers) > 0:\n        answers.sort()\n        answers.reverse()\n        answers = answers[:2]\n        answers = [[doc[\"id\"], i] for i in answers]\n        return answers\n    else:\n        return None","c2d4a5bc":"retrieval_queries = [ ['Clinical trials to investigate effect viral inhibitors like naproxen on coronavirus patients', \n                       'Clinical trials to investigate effect viral inhibitors like clarithromycin on coronavirus patients', \n                       'Clinical trials to investigate effect viral inhibitors like minocyclineth on coronavirus patients'],\n                     ['evaluate complication of antibody dependent enhancement in vaccine recipients'],\n                     ['animal models for vaccine evaluation and efficacy in trials and prediction for human vaccine'],\n                     ['capabilities to discover a therapeutic for COVID-19',\n                      'clinical effectiveness studies to discover therapeutics for COVID-19',\n                      'clinical effectiveness studies to include antiviral agents for COVID-19'],\n                     ['accelerate production of therapeutics',\n                      'timely and equitable distribution of therapeutics to people'],\n                     ['efforts targeted at universal coronavirus vaccine'],\n                     ['animal models for vaccine evaluation and efficacy in trials and prediction for human vaccine and stadardize challenge studies'],\n                     ['develop prophylaxis for COVID-19',\n                      'clinical studies and prioritize in healthcare workers for COVID-19'],\n                     ['evaluate or assess risk for enhanced disease after vaccination'],\n                     ['assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models with therapeutics']\n                    ]\n\nquestions = [ ['What are the clinical trials to investigate effect of naproxen on coronavirus patients?',\n               'What are the clinical trials to investigate effect of clarithromycin on coronavirus patients?',\n               'What are the clinical trials to investigate effect of monocyclineth on coronavirus patients?'\n              ],\n             ['how to evaluate complications of antibody dependent enhancement in vaccine recipients?'],\n             ['What are the best animal models and their predictive value for human vaccine for coronavirus?'],\n             ['What are the capabilities to discover a therapeutic for COVID-19?',\n             'Which are the clinical effectiveness studies to discover therapeutics for COVID-19?',\n             'Which are the clinical effectiveness studies to include antiviral agents for COVID-19?'],\n             ['How to increase production capacity of therapeutics?',\n             'How were therapeutics distributed to the population'],\n             ['What are the efforts targeted at universal coronavirus vaccine?'],\n             ['What are the efforts to develop animal models and standardize challenge studies?'],\n             ['What are the efforts to develop prophylaxis for COVID-19?',\n             'What are the efforts to develop clinical studies and prioritize in healthcare workers for COVID-19?'],\n             ['What are the approaches to evaluate risk for enhanced disease after vaccination?'],\n             ['What are the assays procedures to evaluate vaccine immune response and process development for vaccines with suitable animal models?']            \n            ]","5d4e6402":"answer_set = []\nfor i in zip(retrieval_queries, questions):\n    answers = []\n    docs = []\n    for j in zip(i[0],i[1]):\n        result, keywords, df = get_docs(j[0])\n        incl = 0\n        for k in range(min(100, len(result))):\n            x = get_answer(result[k][0], j[1], keywords)\n            torch.cuda.empty_cache()\n            if x is not None:\n                toadd = []\n                for ax in x:\n                    adding = True\n                    for ans in answers:     \n                        if ans[1][1] == ax[1][1] or ans[1][2] == ax[1][2]:\n                            adding=False\n                    if adding:\n                        toadd.append(ax)\n                    \n                if len(toadd) > 0:\n                    incl = incl + 1\n                    answers = answers + toadd\n                if incl == 7:\n                    break\n    answer_set.append(answers)","c2bd81ae":"len(os.listdir('processed\/custom_license\/pdf_json'))","3be1e255":"pd.set_option('display.max_colwidth', -1)\ndef display_answer(result):\n    p_ids = [i[0] for i in result]\n    titles = [title_dict[i[0]] for i in result]\n    answers = [i[1][2] for i in result]\n    data = {\"id\":p_ids,\"title\":titles, \"Answer\":answers}\n    df = pd.DataFrame (data)\n    return df","26ad2a30":"display_answer(answer_set[0])","6b5de256":"display_answer(answer_set[1])","d4a62ee8":"display_answer(answer_set[2])","523090e6":"display_answer(answer_set[3])","8c528426":"display_answer(answer_set[4])","74655eac":"display_answer(answer_set[5])","aaa263ae":"display_answer(answer_set[6])","dd9b3646":"display_answer(answer_set[7])","e1fae775":"display_answer(answer_set[8])","2823c235":"display_answer(answer_set[9])","3525b95a":"shutil.rmtree('processed')","5526946d":"task_questions = [\n    'Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.',\n    'Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.',\n    'Exploration of use of best animal models and their predictive value for a human vaccine.',\n    'Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.',\n    'Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.',\n    'Efforts targeted at a universal coronavirus vaccine.',\n    'Efforts to develop animal models and standardize challenge studies',\n    'Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers',\n    'Approaches to evaluate risk for enhanced disease after vaccination',\n    'Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models [in conjunction with therapeutics]'\n]","c46b32b4":"def prepare_submission(answer_set):\n    spids = []\n    stitles = []\n    sanswers = []\n    squestions = []\n    qnum = 0\n    for result in answer_set:\n        p_ids = [i[0] for i in result]\n        titles = [title_dict[i[0]] for i in result]\n        answers = [i[1][2] for i in result]\n        questions = [task_questions[qnum] for i in result]\n        spids = spids + p_ids\n        stitles = stitles + titles\n        sanswers = sanswers + answers\n        squestions = squestions + questions\n        qnum = qnum + 1\n    data = {\"question\":squestions,\"id\":spids,\"title\":stitles, \"Answer\":sanswers}\n    df = pd.DataFrame (data)\n    df.to_csv('submission.csv',index=False)","8b07832e":"prepare_submission(answer_set)","5901c6a1":"# Discussion\n\n**Advantage:**\n* Method is scalable and the search index can be built incrementally. The method deals with natural language queries(some terms may require boosting to get better search results) so doesn't require expertise to use \n\n**Drawback:** \n* Our approach uses pretrained model(BioBert with Base vocab). Training the model using domain specific vocabulary or using pretrained model with domain specific vocabulary can improve question answering.\n\n\n**Future directions:**\n* Using language model with vocabulary of medical terms.\n* Indexing sentence vectors(using USE) and searching for relevant text using approximate nearest neighbour method and then applying question answering on relevant text.","a419c880":"9. Approaches to evaluate risk for enhanced disease after vaccination","02bc94b9":"# Findings","73e6485e":"10. Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models [in conjunction with therapeutics]\n","db7922bb":"# Document Retrieval query formulation\nTo retrieve data we modify natural language queries based on the occurence of medical terms. To identify the medical terms we compare the terms in query with a list of medical terms and set the terms' occurence mandatory in a document to be retrieved. \n\nWe use two sources for making up the list of medical terms:\n\n1. https:\/\/github.com\/glutanimate\/wordlist-medicalterms-en \n2. https:\/\/www.medicinenet.com\/medications\/alpha_[a-z]\n\nThe first source is available as text file which can be downloaded directly while the second source is scraped to provide list of chemicals and salts used in medicine. We provide code for scraping the data.","3251c3d0":"**Handling BERT's input size limitation**\n\n\nBERT has a limit of 512 tokens only while the average length of the paragraphs in the dataset is much higher. To handle this limitation we use a sliding window approach which takes small window and keeps sliding it over the text(sentences) from the retrieved paper searching for answer.\n\nAs the process of searching for answer is time consuming, we have set a criteria for a piece of text to be considered for being searched for answer. The text window has to contain at least 2\/3rd of the keywords(or their synonyms) present in the query. \n\nEven though Bert can take a maximum of 512 tokens at a time, we are not using it to its full capacity, the reason for this is that the accuracy of Bert deteriorates for longer context. ","fadeb741":"2. Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.\n","17d9ec32":"5. Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.","3b6961d0":"# COVID-19 Kaggle Challenge\n![image.png](attachment:image.png)\n\n**Task: What do we know about vaccines and therapeutics?** \n\nThe goal of this task is to use the given medical data set and find answers to questions related to the ongoing COVID-19 pandemic.\n","1ae2ad08":"1. Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication","623a2ccd":"# **Document Search\/Retrieval System**\n\nWe use an [Apache Solr](https:\/\/lucene.apache.org\/solr\/) server hosted on an AWS EC2 instance to retrieve list of relevant documents using the pysolr library.\n\nThe server returns the folder and file name for the relevant documents. The index doesn't store a copy of the text in the document. \n\nIndex was uses synonym check, Stop word removal and Stemming.\n\nDocuments are ranked using BM25\n","8f1a2609":"6. Efforts targeted at a universal coronavirus vaccine.","9f1c874a":"7. Efforts to develop animal models and standardize challenge studies\n","33ccd535":"# Question Answering\nWe use pretrained BioBert model finetuned for question answering on BioASQ dataset. We use HuggingFace Transformer library which provides an easy to use implementation of a large number of language models. The model has been built using the vocabulary of BertBase thus doesn't have a large number of biological\/medical terms in its vocabulary.","504d9262":"# Overview of approach\n\n\n![Untitled%20Diagram.png](attachment:Untitled%20Diagram.png)\n\nThe approach can be divided into two main steps:\n1. Document Retrieval\n2. Question Answering","ffc2cf80":"4. Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.\n","16a07a5a":"3. Exploration of use of best animal models and their predictive value for a human vaccine.","887f922a":"Solr Index can be built incrementally so the index can be modified to include new documents and delete the removed documents. ","7c0dd6d7":"Questions and Queries for the task","1fcc18eb":"8. Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers\n"}}