{"cell_type":{"b5470440":"code","09c00d49":"code","8b3d5e14":"code","47c05bf2":"code","c46c4a2e":"code","d47bfba6":"code","ce9841d8":"code","c9b80f99":"code","300ef3f2":"code","2f827ef7":"code","c62cde02":"code","f9221770":"code","61b2dae7":"code","f804da79":"code","a8e72a13":"code","4871b0fe":"markdown","7a402db9":"markdown","e6509a31":"markdown","f52bc659":"markdown","510472ae":"markdown","8cefc955":"markdown"},"source":{"b5470440":"import pandas as pd\nprint(f\"Last updated on {pd.to_datetime('today').strftime('%m\/%d\/%Y')}\")","09c00d49":"import glob\nimport json\nimport os\nimport numpy as np\nimport pandas as pd\nfrom pprint import pprint\n\ninput_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\n\n# See JSON schema here: https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge#json_schema.txt\ndef read_text(file, key):\n    entries = file[key]\n    if isinstance(entries, dict):\n        entries = entries.values()\n    return '\\n'.join([x['text'] for x in entries])\n\n# Each paragraph is a separate entry in the body_text data.\ndef read_body_text(file):\n    prev_section = None\n    text = []\n    for x in file['body_text']:\n        section = x['section']\n        if section != prev_section:\n            text.append('<SECTION>')\n            text.append(section)\n            prev_section = section\n        text.append(x['text'])\n    return '\\n'.join(text)\n    \ndef read_file(filename):\n    file = json.load(open(filename, 'rb'))\n    data = {\n        'paper_id': file['paper_id'],\n        'title': file['metadata']['title'],\n        'abstract': read_text(file, 'abstract'),\n        'body_text': read_body_text(file),\n        'ref_entries': read_text(file, 'ref_entries'),\n    }\n    return data","8b3d5e14":"json_files = sorted(glob.glob(f'{input_path}\/**\/pdf_json\/*.json', recursive=True))\nprint('Total pdf files: ', len(json_files))\n\nfile_contents = []\nfor i, file in enumerate(json_files):\n    if i % 2000 == 0:\n        print(f'Processing {i} of {len(json_files)}')\n    file_contents.append(read_file(file))\ncontent_df = pd.DataFrame(file_contents)","47c05bf2":"metadata_path = f'{input_path}\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'sha': str,\n    'doi': str,\n    'pmcid': str,\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str,\n    'WHO #Covidence': str,\n})\npd.options.display.max_colwidth = 100\nmeta_df.info()","c46c4a2e":"print('PDF-only: ', meta_df[meta_df.has_pdf_parse & ~meta_df.has_pmc_xml_parse].cord_uid.count())\nprint('PMC-only: ', meta_df[~meta_df.has_pdf_parse & meta_df.has_pmc_xml_parse].cord_uid.count())","d47bfba6":"full_df = content_df.merge(meta_df, how='inner', left_on='paper_id', right_on='sha')\nfull_df = full_df.replace(r'^\\s*$', np.nan, regex=True)\nfull_df.info()","ce9841d8":"full_df['title'] = full_df.title_y.fillna(full_df.title_x)\nfull_df['abstract'] = full_df.abstract_y.fillna(full_df.abstract_x)\nfull_df.drop(columns=['sha', 'title_x', 'title_y', 'abstract_x', 'abstract_y'], inplace=True)\nfull_df.info()","c9b80f99":"stats = {}\nstats['total'] = meta_df.shape[0]\nstats['full text'] = full_df.shape[0]\ndf = full_df\n\n# Drop duplicates by title (an article may come from multiple sources resulting in dups).\ndf = df.drop_duplicates(['title'])\nstats['uniques'] = df.shape[0]\ndf.to_csv('cord19.csv', index=False)\n\n# Filter by publish time.\ndf = df[pd.to_datetime(df.publish_time, errors='coerce') >= pd.to_datetime('2019-12-01')]\nstats['after 12\/01\/2019'] = df.shape[0]\n\n# Filter by covid-19 keywords.\nkeywords = ['novel coronavirus', 'COVID-19', 'COVID19', 'SARS-CoV-2', '2019-nCov']\nkeywords_regex = '|'.join(keywords)\ndf_covid19 = df[df.title.str.contains(keywords_regex, na=False, case=False, regex=True) | df.abstract.str.contains(keywords_regex, na=False, case=False, regex=True)]\nstats['covid-19'] = df_covid19.shape[0] \ndf_covid19.to_csv('covid19.csv', index=False)","300ef3f2":"# Write full metadata as well:\nmeta_df.to_csv('cord19_meta.csv', index=False)","2f827ef7":"# Also write covid19_metadata for all articles, including those without full text.\ndf = meta_df\ndf = df.drop_duplicates(['title'])\ndf = df[pd.to_datetime(df.publish_time, errors='coerce') >= pd.to_datetime('2019-12-01')]\ndf_covid19_meta = df[df.title.str.contains(keywords_regex, na=False, case=False, regex=True) | df.abstract.str.contains(keywords_regex, na=False, case=False, regex=True)]\nstats['covid-19-meta'] = df_covid19_meta.shape[0] \ndf_covid19_meta.to_csv('covid19_meta.csv', index=False)\npd.DataFrame([stats]).transpose()","c62cde02":"df_covid19_meta.head(1).transpose()","f9221770":"df_covid19.head(1).transpose()","61b2dae7":"print(df_covid19.iloc[0].abstract)","f804da79":"print(df_covid19.iloc[0].body_text.split('<SECTION>')[1])","a8e72a13":"meta_df[meta_df.title.str.contains('Coronaviruses: a paradigm of new emerging zoonotic diseases', case=False, na=False)]","4871b0fe":"# Sample data","7a402db9":"# Load all pdf json article contents","e6509a31":"[CORD-19](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge) data set of scholarly articles about COVID-19 and the coronavirus group. This notebook uses the title and publication dates of articles to create a subset that are specifically about Covid-19 or SARS-CoV-2.\n\nDataset | # Articles | Output Files\n---|---|---\nCORD-19 | 51078 | cord19_meta.csv\nFull text | 36023 |\nUniques | 35668 | cord19.csv\nAfter 12\/01\/2019 | 3865\nCovid-19 | 2430 | covid19.csv\nCovid-19-meta | 3746 | covid19_meta.csv\n","f52bc659":"# Load metadata and join with contents","510472ae":"# Covid-19 subset of articles","8cefc955":"# Dedup, filter, and write to csv"}}