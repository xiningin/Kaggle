{"cell_type":{"6e591abc":"code","c112f5be":"code","41d66b13":"code","da734e7c":"code","165a6470":"code","e2ba5e4a":"code","1a509b46":"code","f31c9d4f":"code","8eda5e3c":"code","e591f875":"code","0b98f23a":"code","e14b22e4":"code","adc59302":"code","2df8d80e":"code","18047282":"code","58a2296f":"code","5b232613":"code","9356ce2a":"code","cfd67e86":"code","10613ed8":"code","f97afc9d":"code","b908bec8":"code","e508ec9e":"code","26bdab53":"code","7857c5db":"code","a17159ec":"code","5fdaad65":"code","cb465259":"code","ee15ee72":"code","c6d8be45":"code","b6b42b97":"code","debe1975":"code","a28985df":"code","4ec95d63":"code","c3423bc5":"code","59ca7924":"code","f0803cd8":"code","6f620bdb":"code","5f252adb":"code","65a77744":"code","399e48ad":"code","34d3092e":"code","83c8d264":"code","e56cabfe":"code","03017c77":"code","fe463b49":"code","40d6e2f4":"code","95b7c3fe":"code","2e64cec4":"code","6f9d5a1a":"code","6f899b89":"code","c6616d7f":"code","27b5da0b":"code","832e168a":"code","fe9b0080":"code","466625ee":"code","61b6fe2a":"markdown","914a3301":"markdown","09ae1485":"markdown","6e0ed02e":"markdown","80c677d3":"markdown","8c1ed8e0":"markdown","aca77b59":"markdown","2505301b":"markdown","ead33c78":"markdown","78e44f94":"markdown","936f115a":"markdown","a3e557a4":"markdown"},"source":{"6e591abc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os, matplotlib, math, copy\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nprint(os.listdir(\"..\/input\"))\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import PowerTransformer","c112f5be":"new_merchant_transactions = pd.read_csv('..\/input\/new_merchant_transactions.csv')\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nmerchants = pd.read_csv('..\/input\/merchants.csv')\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv')\nhistorical_transactions = pd.read_csv('..\/input\/historical_transactions.csv')\nData_Dictionary = pd.read_excel('..\/input\/Data_Dictionary.xlsx')","41d66b13":"sample_submission.head()","da734e7c":"sns.distplot(train['target']);","165a6470":"print(new_merchant_transactions.shape)\nnew_merchant_transactions.head(2)","e2ba5e4a":"print(train.shape)\ntrain.head(2)","1a509b46":"train.nunique()","f31c9d4f":"print(test.shape)\ntest.head(2)","8eda5e3c":"print(merchants.shape)\nmerchants.head(2)\n#merchants.nunique()","e591f875":"print(historical_transactions.shape)\nhistorical_transactions.head(2)","0b98f23a":"print(historical_transactions[['card_id','merchant_id']].nunique())\nprint(train.shape[0])\nprint(test.shape[0])\nprint((train.shape[0])+(test.shape[0]))","e14b22e4":"Data_Dictionary","adc59302":"historical_trans_sample = historical_transactions[:100000].nunique()\ndata_types = historical_transactions.dtypes\nfeatures = pd.DataFrame({'unique':historical_trans_sample,'dtypes':data_types})\nfeatures","2df8d80e":"features_numeric = features[features['dtypes']!=object].index.tolist()\nprint(features_numeric)","18047282":"features_dummy = features[(features['dtypes']==object)&(features['unique']<30)].index.tolist()\nfeatures_dummy","58a2296f":"#note that this just pulls a sample from each\ncolumns = 3\nrows = math.ceil(len(features_numeric)\/columns)\n\nfig, ax = plt.subplots(rows,columns,figsize = (16,8))\nfor i, feature in list(enumerate(features_numeric)):\n    sns.distplot(historical_transactions[feature].dropna().sample(20000),ax= ax[math.floor(i\/columns),i%columns],axlabel=feature)\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","5b232613":"def tf(pd_series):\n    return np.array(pd_series).reshape(-1,1)\n\ndef transform_viz(pd_series,transform_list = [QuantileTransformer(), MinMaxScaler(), StandardScaler(),PowerTransformer()]):\n    transform_list = [QuantileTransformer(), MinMaxScaler(), StandardScaler(),PowerTransformer()]\n    fig, ax = plt.subplots(1,len(transform_list)+1,figsize = (18,3))\n    sns.distplot(pd_series.dropna().sample(1000),ax = ax[0]);\n    for i, transformer in enumerate(transform_list):\n        sns.distplot(transformer.fit_transform(tf(pd_series.dropna().sample(1000))),ax = ax[i+1]);","9356ce2a":"def plot_details(pd_series):\n    plt.figure(figsize = (3,3))\n    std, mean, skew = pd_series.std(), pd_series.mean(), pd_series.skew()\n    print('shape: {}, mean: {:.2f}, std: {:.2f}, skew: {:.2f}'.format(pd_series.shape,mean,std,skew))\n    sns.distplot(pd_series)\n    plt.show();\n    return std, mean, skew","cfd67e86":"def remove_outliers_and_skew(pd_series,skew_threshold = 1, remove_outliers = True, deviations = 2.15):\n    \"\"\"\n    args:\n        pd-series: required\n        skew_threshold: skew calc above which series with be log-transformed\n        remove_outliers: will return pd_series with ...\n        deviations: ...\n    \"\"\"\n    print('original distribution')\n    std, mean, skew = plot_details(pd_series)\n    if skew > 1:\n        print('log transform original')\n        new_series = np.log1p(pd_series)\n        if remove_outliers:\n            std, mean, skew = plot_details(new_series)\n            print('update size')\n            new_series = new_series[new_series.between(mean - (std * deviations),mean + (std * deviations))]\n    else:\n        if remove_outliers:\n            print('update size')\n            new_series = new_series[new_series.between(mean - (std * deviations),mean + (std * deviations))]\n        else:\n            new_series = pd_series\n    plot_details(new_series)\n    return new_series","10613ed8":"historical_transactions['purchase_amount'] = historical_transactions['purchase_amount'] - historical_transactions['purchase_amount'].min()\npurchase_amount_sum = historical_transactions[['purchase_amount','card_id']].groupby('card_id').sum()['purchase_amount']\npurchase_amount_sum_transformed = remove_outliers_and_skew(purchase_amount_sum,1,False,6) ##########\ndel purchase_amount_sum","f97afc9d":"sns.distplot(historical_transactions['month_lag'].abs().sample(20000))","b908bec8":"historical_transactions['month_lag'] = historical_transactions['month_lag'].abs()\nmonth_lag_abs_mean = historical_transactions[['month_lag','card_id']].groupby('card_id').mean()['month_lag'] ##########\nmonth_lag_abs_mean = remove_outliers_and_skew(month_lag_abs_mean,1,False,6)","e508ec9e":"category_2_dummies = pd.get_dummies(historical_transactions['category_2'],prefix='cat_2')\ncategory_2_dummies = pd.merge(category_2_dummies,historical_transactions[['card_id']],left_index = True, right_index = True)\ncategory_2_dummies = category_2_dummies.groupby('card_id').sum()","26bdab53":"category_1_dummies = pd.get_dummies(historical_transactions['category_1'],prefix='cat_1')\ncategory_1_dummies = pd.merge(category_1_dummies,historical_transactions[['card_id']],left_index = True, right_index = True)\ncategory_1_dummies = category_1_dummies.groupby('card_id').sum()","7857c5db":"card_data = pd.merge(month_lag_abs_mean.to_frame(), purchase_amount_sum_transformed.to_frame(),left_index = True, right_index = True,how = 'inner')\nprint(card_data.shape)\ncard_data.head(2)","a17159ec":"dummies = pd.merge(category_1_dummies, category_2_dummies, left_index = True, right_index = True, how = 'inner')\ndel category_1_dummies, category_2_dummies\nprint(dummies.shape)\ndummies.head(2)","5fdaad65":"card_data = pd.merge(card_data,dummies, left_index = True, right_index = True, how = 'inner')\ncard_data.head(2)","cb465259":"columns = 2\n\ntrain_numeric = list(train.dtypes[train.dtypes != object].index)\nrows = math.ceil(len(train_numeric)\/columns)\n\nfig, ax = plt.subplots(rows,columns,figsize = (8,4))\nfor i, feature in list(enumerate(train_numeric)):\n    #sns.distplot(train[feature].dropna().sample(20000),ax= ax[math.floor(i\/columns),i%columns],axlabel=feature)\n    sns.distplot(train[feature].dropna().sample(20000),ax= ax[math.floor(i\/columns),i%columns])\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","ee15ee72":"results = pd.concat([train,test])\nresults = results.drop(['first_active_month'],axis = 1)\nprint(results.shape)\nresults.head()","c6d8be45":"card_data.shape","b6b42b97":"results[results['target'].isnull()].shape[0] + results.dropna().shape[0] - results.shape[0]","debe1975":"all_data = pd.merge(card_data, results, left_index = True, right_on = 'card_id', how = 'inner')\nall_data.set_index('card_id', inplace = True)","a28985df":"results_train = all_data.dropna()\nresults_train.shape","4ec95d63":"results_test = all_data[all_data['target'].isnull()]\nresults_test.shape","c3423bc5":"results_train.shape[0] + results_test.shape[0]","59ca7924":"results_test.head(2)","f0803cd8":"results_test = results_test.drop('target',axis = 1)","6f620bdb":"X, y = results_train.drop(['target'],axis = 1), results_train['target']","5f252adb":"from sklearn.model_selection import train_test_split","65a77744":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)","399e48ad":"from sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n\ndef get_mse(model,alpha):\n    run_model = model(alpha = alpha)\n    run_model.fit(X_train,y_train)\n    return mean_squared_error(y_pred=run_model.predict(X_test),y_true=y_test)\n\nalphas = [.01,.05,0.1,0.5,1,2,3,5,10,20,40,100,1000,5000]\nridge_mses = [get_mse(Ridge,x) for x in alphas]\nplt.plot(alphas,ridge_mses)","34d3092e":"from sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor()\nmodel.fit(X_train,y_train)\nmean_squared_error(y_pred=model.predict(X_test),y_true=y_test)","83c8d264":"from sklearn.ensemble import AdaBoostRegressor\nmodel = AdaBoostRegressor()\nmodel.fit(X_train,y_train)\nmean_squared_error(y_pred=model.predict(X_test),y_true=y_test)","e56cabfe":"from sklearn.ensemble import BaggingRegressor\nmodel = BaggingRegressor()\nmodel.fit(X_train,y_train)\nmean_squared_error(y_pred=model.predict(X_test),y_true=y_test)","03017c77":"from sklearn.linear_model import Lasso\nmodel = Lasso()\nmodel.fit(X_train,y_train)\nmean_squared_error(y_pred=model.predict(X_test),y_true=y_test)","fe463b49":"from sklearn.linear_model import ElasticNet\nmodel = ElasticNet()\nmodel.fit(X_train,y_train)\nmean_squared_error(y_pred=model.predict(X_test),y_true=y_test)","40d6e2f4":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train,y_train)\nmean_squared_error(y_pred=model.predict(X_test),y_true=y_test)","95b7c3fe":"from sklearn.ensemble import ExtraTreesRegressor\nmodel = ExtraTreesRegressor()\nmodel.fit(X_train,y_train)\nmean_squared_error(y_pred=model.predict(X_test),y_true=y_test)","2e64cec4":"from sklearn.linear_model import \nmodel = ExtraTreesRegressor()\nmodel.fit(X_train,y_train)\nmean_squared_error(y_pred=model.predict(X_test),y_true=y_test)","6f9d5a1a":"from sklearn.ensemble import GradientBoostingRegressor\nmodel = GradientBoostingRegressor()\nmodel.fit(X_train,y_train)\nmean_squared_error(y_pred=model.predict(X_test),y_true=y_test)","6f899b89":"sample_submission.head(2)","c6616d7f":"results_test.head()","27b5da0b":"predictions = model.predict(results_test)\nsub = pd.DataFrame({'card_id':results_test.index, 'target':predictions})\nsub.head()","832e168a":"sns.distplot(sub['target'])","fe9b0080":"sns.distplot(train['target'])`","466625ee":"sub.to_csv('submission.csv',index=False)","61b6fe2a":"# **Imports**","914a3301":"### Unique Counts\nTaking a quick sample of 100,000 to choose which non-numeric features can be dummy variable based on unique counts","09ae1485":"Will pull customers' total purchase amount","6e0ed02e":"# **Intro**\nI used this competition\/kernel to just add to my portfolio a bit while also working on my skills. I didn't at all look at what others were doing for this kernel, since I wanted to work on this as a real world project where one doesn't have all this external help.  I am now going to look at what others did to learn from them, but I will still leave this as is.  The main idea for me here was just to practice and learn.  You will notice that I only used a subset of features.  I did this to try to get a simple model working without spending a ton of time.  I wasn't worried about the score as much as building something from start to finish independently.\nThanks for checking it out, and please feel free to comment.","80c677d3":"# **Feature Engineering**","8c1ed8e0":"# **Merge Data for Train and Test**","aca77b59":"# **Model**","2505301b":"Maybe it's already obvious... but we're looking for unique cardholders and just checking on the counts here","ead33c78":"### purchase_amount","78e44f94":"## Data Types and Basic Visuals (for historicals_transactions)","936f115a":"# Data Downloads","a3e557a4":"# **EDA - Exploratory Data Analysis**"}}