{"cell_type":{"9ead1af7":"code","84750452":"code","00a38ad9":"code","8d4559f0":"code","b5ed9deb":"code","585ff9c1":"code","37866b3d":"code","2d15d3da":"code","cbe45545":"code","7d56de4a":"code","58592dda":"code","cb2477a3":"code","1e600ae9":"code","9acc554d":"code","93173248":"code","c73878ff":"code","c9e2bca1":"code","7d78fb26":"code","a7a980a4":"code","57f10bc8":"code","e93766b5":"code","33f04018":"code","72b7211f":"code","c312516a":"code","12b8c6ff":"code","87b0374b":"code","baef3936":"code","ab539600":"code","1eaed210":"code","d8369c9f":"code","f077e8f6":"code","98d17849":"code","b869c394":"code","0a869e8a":"code","73fbcf78":"code","e157a6b8":"code","b39bddea":"code","ba9a2635":"code","29a5c9b7":"code","4a6c8b4e":"code","9edf79da":"code","1fa44732":"code","d387d63f":"code","5f0d0062":"code","40d67b39":"code","892f8018":"code","02947516":"code","6d0f354a":"code","24dc0249":"code","77ed5f25":"code","f3d66b3d":"code","e6e3fc51":"code","41dee4c8":"code","650825b3":"code","6294de5b":"code","57d92b45":"code","724d4917":"code","cbf172d1":"code","0a5278fa":"code","82de734b":"code","28cf3148":"code","84e3b228":"markdown","2e37fbbe":"markdown","d9a84f49":"markdown","17d3f7dd":"markdown","38552a9c":"markdown","11d002c5":"markdown","17295a02":"markdown","f3eca1eb":"markdown","8e2c9bfe":"markdown","24f52f7a":"markdown","9313c3af":"markdown","9d045d4c":"markdown","ef1d2afa":"markdown","849d21d7":"markdown","2c806b5f":"markdown","f3dc8b96":"markdown","55af3997":"markdown","8ea9475b":"markdown","84ac83d6":"markdown","66fa9777":"markdown","fa39995b":"markdown","2de2e0c7":"markdown","66e41f05":"markdown","47f5f2bd":"markdown","d63902a2":"markdown","130ad21f":"markdown","b02d7c48":"markdown","c3350428":"markdown","460901e2":"markdown"},"source":{"9ead1af7":"from numpy.random import seed\nseed(101)\nfrom tensorflow import set_random_seed\nset_random_seed(101)\n\nimport pandas as pd\nimport numpy as np\n\nimport tensorflow\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.metrics import binary_accuracy\n\nimport os\nimport cv2\n\nimport imageio\nimport skimage\nimport skimage.io\nimport skimage.transform\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline","84750452":"# Total number of images we want to have in each class\nNUM_AUG_IMAGES_WANTED = 1000 \n\n# We will resize the images\nIMAGE_HEIGHT = 96\nIMAGE_WIDTH = 96","00a38ad9":"os.listdir('..\/input')","8d4559f0":"print(len(os.listdir('..\/input\/ChinaSet_AllFiles\/ChinaSet_AllFiles\/CXR_png')))\nprint(len(os.listdir('..\/input\/Montgomery\/MontgomerySet\/CXR_png')))","b5ed9deb":"shen_image_list = os.listdir('..\/input\/ChinaSet_AllFiles\/ChinaSet_AllFiles\/CXR_png')\nmont_image_list = os.listdir('..\/input\/Montgomery\/MontgomerySet\/CXR_png')","585ff9c1":"# put the images into dataframes\ndf_shen = pd.DataFrame(shen_image_list, columns=['image_id'])\ndf_mont = pd.DataFrame(mont_image_list, columns=['image_id'])\n\n# remove the 'Thunbs.db' line\ndf_shen = df_shen[df_shen['image_id'] != 'Thumbs.db']\ndf_mont = df_mont[df_mont['image_id'] != 'Thumbs.db']\n\n# Reset the index or this will cause an error later\ndf_shen.reset_index(inplace=True, drop=True)\ndf_mont.reset_index(inplace=True, drop=True)\n\nprint(df_shen.shape)\nprint(df_mont.shape)","37866b3d":"df_shen.head()","2d15d3da":"df_mont.head()","cbe45545":"# Function to select the 4th index from the end of the string (file name)\n# example: CHNCXR_0470_1.png --> 1 is the label, meaning TB is present.\n\ndef extract_target(x):\n    target = int(x[-5])\n    if target == 0:\n        return 'Normal'\n    if target == 1:\n        return 'Tuberculosis'","7d56de4a":"# Assign the target labels\n\ndf_shen['target'] = df_shen['image_id'].apply(extract_target)\n\ndf_mont['target'] = df_mont['image_id'].apply(extract_target)\n","58592dda":"# Shenzen Dataset\n\ndf_shen['target'].value_counts()","cb2477a3":"# Montgomery Dataset\n\ndf_mont['target'].value_counts()","1e600ae9":"# source: https:\/\/www.kaggle.com\/gpreda\/honey-bee-subspecies-classification\n\ndef draw_category_images(col_name,figure_cols, df, IMAGE_PATH):\n    \n    \"\"\"\n    Give a column in a dataframe,\n    this function takes a sample of each class and displays that\n    sample on one row. The sample size is the same as figure_cols which\n    is the number of columns in the figure.\n    Because this function takes a random sample, each time the function is run it\n    displays different images.\n    \"\"\"\n    \n\n    categories = (df.groupby([col_name])[col_name].nunique()).index\n    f, ax = plt.subplots(nrows=len(categories),ncols=figure_cols, \n                         figsize=(4*figure_cols,4*len(categories))) # adjust size here\n    # draw a number of images for each location\n    for i, cat in enumerate(categories):\n        sample = df[df[col_name]==cat].sample(figure_cols) # figure_cols is also the sample size\n        for j in range(0,figure_cols):\n            file=IMAGE_PATH + sample.iloc[j]['image_id']\n            im=imageio.imread(file)\n            ax[i, j].imshow(im, resample=True, cmap='gray')\n            ax[i, j].set_title(cat, fontsize=14)  \n    plt.tight_layout()\n    plt.show()\n    ","9acc554d":"# Shenzen Dataset\n\nIMAGE_PATH = '..\/input\/ChinaSet_AllFiles\/ChinaSet_AllFiles\/CXR_png\/' \n\ndraw_category_images('target',4, df_shen, IMAGE_PATH)","93173248":"# Montgomery Dataset\n\nIMAGE_PATH = '..\/input\/Montgomery\/MontgomerySet\/CXR_png\/'\n\ndraw_category_images('target',4, df_mont, IMAGE_PATH)","c73878ff":"def read_image_sizes(file_name):\n    \"\"\"\n    1. Get the shape of the image\n    2. Get the min and max pixel values in the image.\n    Getting pixel values will tell if any pre-processing has been done.\n    3. This info will be added to the original dataframe.\n    \"\"\"\n    image = cv2.imread(IMAGE_PATH + file_name)\n    max_pixel_val = image.max()\n    min_pixel_val = image.min()\n    \n    # image.shape[2] represents the number of channels: (height, width, num_channels).\n    # Here we are saying: If the shape does not have a value for num_channels (height, width)\n    # then assign 1 to the number of channels.\n    if len(image.shape) > 2: # i.e. more than two numbers in the tuple\n        output = [image.shape[0], image.shape[1], image.shape[2], max_pixel_val, min_pixel_val]\n    else:\n        output = [image.shape[0], image.shape[1], 1, max_pixel_val, min_pixel_val]\n    return output\n\n","c9e2bca1":"IMAGE_PATH = '..\/input\/ChinaSet_AllFiles\/ChinaSet_AllFiles\/CXR_png\/'\n\nm = np.stack(df_shen['image_id'].apply(read_image_sizes))\ndf = pd.DataFrame(m,columns=['w','h','c','max_pixel_val','min_pixel_val'])\ndf_shen = pd.concat([df_shen,df],axis=1, sort=False)\n\ndf_shen.head()","7d78fb26":"IMAGE_PATH = '..\/input\/Montgomery\/MontgomerySet\/CXR_png\/'\n\nm = np.stack(df_mont['image_id'].apply(read_image_sizes))\ndf = pd.DataFrame(m,columns=['w','h','c','max_pixel_val','min_pixel_val'])\ndf_mont = pd.concat([df_mont,df],axis=1, sort=False)\n\ndf_mont.head()","a7a980a4":"df_shen['c'].value_counts()","57f10bc8":"df_mont['c'].value_counts()","e93766b5":"df_mont['target'].value_counts()","33f04018":"### Combine the two dataframes and shuffle\n\ndf_data = pd.concat([df_shen, df_mont], axis=0).reset_index(drop=True)\n\ndf_data = shuffle(df_data)\n\n\ndf_data.shape","72b7211f":"# Create a new column called 'labels' that maps the classes to binary values.\ndf_data['labels'] = df_data['target'].map({'Normal':0, 'Tuberculosis':1})","c312516a":"df_data.head()","12b8c6ff":"# train_test_split\n\ny = df_data['labels']\n\ndf_train, df_val = train_test_split(df_data, test_size=0.15, random_state=101, stratify=y)\n\nprint(df_train.shape)\nprint(df_val.shape)","87b0374b":"df_train['target'].value_counts()","baef3936":"df_val['target'].value_counts()","ab539600":"# Create a new directory\nbase_dir = 'base_dir'\nos.mkdir(base_dir)\n\n\n#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n\n# now we create 2 folders inside 'base_dir':\n\n# train\n    # Normal\n    # Tuberculosis\n\n# val\n    # Normal\n    # Tuberculosis\n\n\n# create a path to 'base_dir' to which we will join the names of the new folders\n# train_dir\ntrain_dir = os.path.join(base_dir, 'train_dir')\nos.mkdir(train_dir)\n\n# val_dir\nval_dir = os.path.join(base_dir, 'val_dir')\nos.mkdir(val_dir)\n\n\n# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\n# Inside each folder we create seperate folders for each class\n\n# create new folders inside train_dir\nNormal = os.path.join(train_dir, 'Normal')\nos.mkdir(Normal)\nTuberculosis = os.path.join(train_dir, 'Tuberculosis')\nos.mkdir(Tuberculosis)\n\n\n# create new folders inside val_dir\nNormal = os.path.join(val_dir, 'Normal')\nos.mkdir(Normal)\nTuberculosis = os.path.join(val_dir, 'Tuberculosis')\nos.mkdir(Tuberculosis)\n","1eaed210":"# Set the image_id as the index in df_data\ndf_data.set_index('image_id', inplace=True)","d8369c9f":"# Get a list of images in each of the two folders\nfolder_1 = os.listdir('..\/input\/ChinaSet_AllFiles\/ChinaSet_AllFiles\/CXR_png')\nfolder_2 = os.listdir('..\/input\/Montgomery\/MontgomerySet\/CXR_png')\n\n# Get a list of train and val images\ntrain_list = list(df_train['image_id'])\nval_list = list(df_val['image_id'])\n\n\n\n# Transfer the train images\n\nfor image in train_list:\n    \n    fname = image\n    label = df_data.loc[image,'target']\n    \n    if fname in folder_1:\n        # source path to image\n        src = os.path.join('..\/input\/ChinaSet_AllFiles\/ChinaSet_AllFiles\/CXR_png', fname)\n        # destination path to image\n        dst = os.path.join(train_dir, label, fname)\n        \n        image = cv2.imread(src)\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n        # save the image at the destination\n        cv2.imwrite(dst, image)\n        #shutil.copyfile(src, dst)\n\n    if fname in folder_2:\n        # source path to image\n        src = os.path.join('..\/input\/Montgomery\/MontgomerySet\/CXR_png', fname)\n        # destination path to image\n        dst = os.path.join(train_dir, label, fname)\n        \n        image = cv2.imread(src)\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n        # save the image at the destination\n        cv2.imwrite(dst, image)\n        \n        # copy the image from the source to the destination\n        #shutil.copyfile(src, dst)\n\n\n# Transfer the val images\n\nfor image in val_list:\n    \n    fname = image\n    label = df_data.loc[image,'target']\n    \n    if fname in folder_1:\n        # source path to image\n        src = os.path.join('..\/input\/ChinaSet_AllFiles\/ChinaSet_AllFiles\/CXR_png', fname)\n        # destination path to image\n        dst = os.path.join(val_dir, label, fname)\n        \n        image = cv2.imread(src)\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n        # save the image at the destination\n        cv2.imwrite(dst, image)\n        \n        # copy the image from the source to the destination\n        #shutil.copyfile(src, dst)\n\n    if fname in folder_2:\n        # source path to image\n        src = os.path.join('..\/input\/Montgomery\/MontgomerySet\/CXR_png', fname)\n        # destination path to image\n        dst = os.path.join(val_dir, label, fname)\n        \n        image = cv2.imread(src)\n        image = cv2.resize(image, (IMAGE_HEIGHT, IMAGE_WIDTH))\n        # save the image at the destination\n        cv2.imwrite(dst, image)\n        \n        # copy the image from the source to the destination\n        #shutil.copyfile(src, dst)","f077e8f6":"# check how many train images we have in each folder\n\nprint(len(os.listdir('base_dir\/train_dir\/Normal')))\nprint(len(os.listdir('base_dir\/train_dir\/Tuberculosis')))\n","98d17849":"# check how many val images we have in each folder\n\nprint(len(os.listdir('base_dir\/val_dir\/Normal')))\nprint(len(os.listdir('base_dir\/val_dir\/Tuberculosis')))\n","b869c394":"class_list = ['Normal','Tuberculosis']\n\nfor item in class_list:\n    \n    # We are creating temporary directories here because we delete these directories later.\n    # create a base dir\n    aug_dir = 'aug_dir'\n    os.mkdir(aug_dir)\n    # create a dir within the base dir to store images of the same class\n    img_dir = os.path.join(aug_dir, 'img_dir')\n    os.mkdir(img_dir)\n\n    # Choose a class\n    img_class = item\n\n    # list all images in that directory\n    img_list = os.listdir('base_dir\/train_dir\/' + img_class)\n\n    # Copy images from the class train dir to the img_dir e.g. class 'Normal'\n    for fname in img_list:\n            # source path to image\n            src = os.path.join('base_dir\/train_dir\/' + img_class, fname)\n            # destination path to image\n            dst = os.path.join(img_dir, fname)\n            # copy the image from the source to the destination\n            shutil.copyfile(src, dst)\n\n\n    # point to a dir containing the images and not to the images themselves\n    path = aug_dir\n    save_path = 'base_dir\/train_dir\/' + img_class\n\n    # Create a data generator\n    datagen = ImageDataGenerator(\n        rotation_range=10,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        zoom_range=0.1,\n        horizontal_flip=True,\n        fill_mode='nearest')\n\n    batch_size = 50\n\n    aug_datagen = datagen.flow_from_directory(path,\n                                           save_to_dir=save_path,\n                                           save_format='png',\n                                                    target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\n                                                    batch_size=batch_size)\n    \n    \n    # Generate the augmented images and add them to the training folders\n    \n    \n    num_files = len(os.listdir(img_dir))\n    \n    # this creates a similar amount of images for each class\n    num_batches = int(np.ceil((NUM_AUG_IMAGES_WANTED-num_files)\/batch_size))\n\n    # run the generator and create augmented images\n    for i in range(0,num_batches):\n\n        imgs, labels = next(aug_datagen)\n        \n    # delete temporary directory with the raw image files\n    shutil.rmtree('aug_dir')","0a869e8a":"# Check how many train images we now have in each folder.\n# This is the original images plus the augmented images.\n\nprint(len(os.listdir('base_dir\/train_dir\/Normal')))\nprint(len(os.listdir('base_dir\/train_dir\/Tuberculosis')))","73fbcf78":"# Check how many val images we have in each folder.\n\nprint(len(os.listdir('base_dir\/val_dir\/Normal')))\nprint(len(os.listdir('base_dir\/val_dir\/Tuberculosis')))","e157a6b8":"# plots images with labels within jupyter notebook\n# source: https:\/\/github.com\/smileservices\/keras_utils\/blob\/master\/utils.py\n\ndef plots(ims, figsize=(20,10), rows=5, interp=False, titles=None): # 12,6\n    if type(ims[0]) is np.ndarray:\n        ims = np.array(ims).astype(np.uint8)\n        if (ims.shape[-1] != 3):\n            ims = ims.transpose((0,2,3,1))\n    f = plt.figure(figsize=figsize)\n    cols = len(ims)\/\/rows if len(ims) % 2 == 0 else len(ims)\/\/rows + 1\n    for i in range(len(ims)):\n        sp = f.add_subplot(rows, cols, i+1)\n        sp.axis('Off')\n        if titles is not None:\n            sp.set_title(titles[i], fontsize=16)\n        plt.imshow(ims[i], interpolation=None if interp else 'none')\n        \n","b39bddea":"plots(imgs, titles=None) # titles=labels will display the image labels","ba9a2635":"# End of Data Preparation\n### ===================================================================================== ###\n# Start of Model Building\n","29a5c9b7":"train_path = 'base_dir\/train_dir'\nvalid_path = 'base_dir\/val_dir'\n\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = 10\nval_batch_size = 10\n\n\ntrain_steps = np.ceil(num_train_samples \/ train_batch_size)\nval_steps = np.ceil(num_val_samples \/ val_batch_size)","4a6c8b4e":"\ndatagen = ImageDataGenerator(rescale=1.0\/255)\n\ntrain_gen = datagen.flow_from_directory(train_path,\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\n                                        batch_size=train_batch_size,\n                                        class_mode='categorical')\n\nval_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\n                                        batch_size=val_batch_size,\n                                        class_mode='categorical')\n\n# Note: shuffle=False causes the test dataset to not be shuffled\ntest_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_HEIGHT,IMAGE_WIDTH),\n                                        batch_size=val_batch_size,\n                                        class_mode='categorical',\n                                        shuffle=False)","9edf79da":"# Source: https:\/\/www.kaggle.com\/fmarazzi\/baseline-keras-cnn-roc-fast-5min-0-8253-lb\n\nkernel_size = (3,3)\npool_size= (2,2)\nfirst_filters = 32\nsecond_filters = 64\nthird_filters = 128\n\ndropout_conv = 0.3\ndropout_dense = 0.3\n\n\nmodel = Sequential()\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu', \n                 input_shape = (IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size)) \nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(dropout_dense))\nmodel.add(Dense(2, activation = \"softmax\"))\n\nmodel.summary()\n","1fa44732":"model.compile(Adam(lr=0.0001), loss='binary_crossentropy', \n              metrics=['accuracy'])","d387d63f":"filepath = \"model.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n                             save_best_only=True, mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, \n                                   verbose=1, mode='max', min_lr=0.00001)\n                              \n                              \ncallbacks_list = [checkpoint, reduce_lr]\n\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n                            validation_data=val_gen,\n                            validation_steps=val_steps,\n                            epochs=100, verbose=1,\n                           callbacks=callbacks_list)","5f0d0062":"# get the metric names so we can use evaulate_generator\nmodel.metrics_names","40d67b39":"# Here the best epoch will be used.\n\nmodel.load_weights('model.h5')\n\nval_loss, val_acc = \\\nmodel.evaluate_generator(test_gen, \n                        steps=val_steps)\n\nprint('val_loss:', val_loss)\nprint('val_acc:', val_acc)","892f8018":"# display the loss and accuracy curves\n\nimport matplotlib.pyplot as plt\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()","02947516":"# Get the labels of the test images.\n\ntest_labels = test_gen.classes","6d0f354a":"# We need these to plot the confusion matrix.\ntest_labels","24dc0249":"# Print the label associated with each class\ntest_gen.class_indices","77ed5f25":"# make a prediction\npredictions = model.predict_generator(test_gen, steps=val_steps, verbose=1)","f3d66b3d":"predictions.shape","e6e3fc51":"# Source: Scikit Learn website\n# http:\/\/scikit-learn.org\/stable\/auto_examples\/\n# model_selection\/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n# selection-plot-confusion-matrix-py\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","41dee4c8":"test_labels.shape","650825b3":"# argmax returns the index of the max value in a row\ncm = confusion_matrix(test_labels, predictions.argmax(axis=1))","6294de5b":"test_gen.class_indices","57d92b45":"# Define the labels of the class indices. These need to match the \n# order shown above.\ncm_plot_labels = ['Normal', 'Tuberculosis']\n\nplot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')","724d4917":"# Get the filenames, labels and associated predictions\n\n# This outputs the sequence in which the generator processed the test images\ntest_filenames = test_gen.filenames\n\n# Get the true labels\ny_true = test_gen.classes\n\n# Get the predicted labels\ny_pred = predictions.argmax(axis=1)","cbf172d1":"from sklearn.metrics import classification_report\n\n# Generate a classification report\n\nreport = classification_report(y_true, y_pred, target_names=cm_plot_labels)\n\nprint(report)\n","0a5278fa":"!pip install tensorflowjs","82de734b":"# Use the command line conversion tool to convert the model\n\n!tensorflowjs_converter --input_format keras model.h5 tfjs\/model","28cf3148":"# Delete the image data directory we created to prevent a Kaggle error.\n# Kaggle allows a max of 500 files to be saved.\n\nshutil.rmtree('base_dir')","84e3b228":"### Set Up the Generators","2e37fbbe":"### Check the class distribution","d9a84f49":"### Plot the Training Curves","17d3f7dd":"### Display a ramdom sample of images from each dataset by target","38552a9c":"**Introduction**\n\nIn this kernel we will build a model that can look at a chest x-ray and predict whether a person has TB or not. The model will be trained on a dataset of 800 images from two sources:\n\n- Shenzhen, China (Folder: ChinaSet_AllFiles)\n- Montgomery, USA (Folder: Montgomery)\n\n**Results:**\n\nThe dataset is quite small but by using a CNN and data augmentation, the final accuracy and F1 score that we get will be greater than 0.8. Because we need to use as many images as possible for training, the validation set will contain only 120 images. This is 15% of the data.\n\nWith a small dataset and a very small validation set, will this model generalize well? I don't really know. However, I've deployed the model as a Tensorflowjs web app so it can be tested. Using a web app is one way that a tool like this could - quickly and cheaply - be put in the hands of medical personnel that need it.\n\n***\n\n\nAll the html, css and javascript code used to build the app is available on Github.<br>\n\nWeb App:<br>\nhttp:\/\/tb.test.woza.work\/<br>\nGithub:<br>\nhttps:\/\/github.com\/vbookshelf\/Tuberculosis-TB-Analyzer\n\n\n\n\n","11d002c5":"### Create a Classification Report","17295a02":"### Assign labels to the images","f3eca1eb":"### Create the Model Architecture","8e2c9bfe":"### Create a Directory Structure","24f52f7a":"### Convert the model to from Keras to Tensorflowjs\n\nThis conversion needs to be done so that the model can be loaded into the web app.","9313c3af":"### Create a Dataframe containing all images","9d045d4c":"### Create the Train and Val Sets","ef1d2afa":"### Visualize a batch of augmented images","849d21d7":"### What is the shape of each image and what are its max and min pixel values?\nLet's include all this info in the dataframes we created above.","2c806b5f":"**Reference Kernels:**\n\nI found the following kernels very helpful:\n\n1. Gabriel Preda, Honey Bee Subspecies Classification<br>\nhttps:\/\/www.kaggle.com\/gpreda\/honey-bee-subspecies-classification\n\n2. Francesco Marazzi, Baseline Keras CNN<br>\nhttps:\/\/www.kaggle.com\/fmarazzi\/baseline-keras-cnn-roc-fast-5min-0-8253-lb","f3dc8b96":"> *Half a million children become ill with TB each year. There are 10 million children worldwide who had been orphaned because a parent died of TB.*<br>\n> *Basic diagnosis of TB has not changed for more than a century. New genetic tests for TB make it possible to rapidly identify people who need TB treatment. But a simple quick test of the sort already available for diseases like HIV and malaria is needed urgently.* <br>\n>*** ~ stoptb.org***","55af3997":"We see that all images have 3 channels.","8ea9475b":"### How many channels do the images in each dataset have?","84ac83d6":"### Copy the train images into aug_dir\naug_dir is where we temporarily store images from a given class before feeding them into the generator for augmentation.\u00b6\n\nWe will not be augmenting on the fly. We will create augmented images, store them in folders together with the raw images and then feed these into the generators. I found that working this way makes the training process run faster.","66fa9777":"**Recall **= Given a class, will the classifier be able to detect it?<br>\n**Precision** = Given a class prediction from a classifier, how likely is it to be correct?<br>\n**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.\n\nThe F1 score is greater than 0.8. From the confusion matrix we see that our model has a  tendency to classify TB images as Normal, more so than to classify Normal images as TB. ","fa39995b":"### What files are available?","2de2e0c7":"### What are the labels?","66e41f05":"\nThe label is part of the file name.\n\nExample: *CHNCXR_0470__**1**.png*<br>\n\n**0** = Normal (No TB)<br>\n**1** = TB<br>\n\nEach of the two datasets has a text file containing meta-data.","47f5f2bd":"### Evaluate the model using the val set","d63902a2":"### Train the Model","130ad21f":"### Create a Confusion Matrix","b02d7c48":"### Transfer the images into the folders","c3350428":"### How many images are in each folder?\nNote: In both the Mongomery and Shenzhen folders there is a non image file called 'Thumbs.db'","460901e2":"**Conclusion**\n\nMany thanks to Kevin Mader for making this dataset available on Kaggle.\n\nThank you for reading."}}