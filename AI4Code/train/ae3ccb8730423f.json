{"cell_type":{"7f5f8c59":"code","894510e8":"code","ae6f051e":"code","4f1d3497":"code","96f0b2d5":"code","909be40a":"code","a3bd3eb0":"code","cd51604d":"code","1fedbc57":"code","ca20efdc":"code","4eff96cc":"code","bec561af":"code","79023b7b":"code","9f68ff6d":"code","1c9b449b":"code","ffa99e60":"code","b5daa8f8":"code","fdf7e751":"code","851fb396":"code","275038c2":"code","26e1d396":"code","c63c5021":"code","47ce5286":"code","bae40b99":"code","05e9dbd5":"code","507a6cd9":"code","e1361eeb":"code","2e66ea63":"code","9f5541b2":"code","72bdb8b5":"code","e6a4cd1d":"markdown","b9676501":"markdown","90276b62":"markdown","e2dd56e9":"markdown","caa749f7":"markdown","c4d58364":"markdown","ac4a349a":"markdown"},"source":{"7f5f8c59":"!pip install -U imbalanced-learn","894510e8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        \nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport seaborn as sns; sns.set()\n%matplotlib inline\n\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nfrom imblearn.pipeline import make_pipeline, Pipeline\nfrom imblearn.over_sampling import SMOTE\n\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import KBinsDiscretizer, FunctionTransformer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom category_encoders import WOEEncoder\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, balanced_accuracy_score, roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n\n\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\nfrom sklearn.preprocessing import MinMaxScaler, KBinsDiscretizer, FunctionTransformer\nfrom category_encoders import OneHotEncoder\n\nfrom mlxtend.evaluate import feature_importance_permutation\nfrom sklearn.model_selection import train_test_split\n\nfrom mlxtend.feature_extraction import PrincipalComponentAnalysis\nfrom mlxtend.preprocessing import standardize\n\nfrom mlxtend.plotting import plot_pca_correlation_graph\n\n\nfrom mlxtend.plotting import plot_decision_regions\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import KBinsDiscretizer, FunctionTransformer\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.pipeline import make_pipeline, Pipeline\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Any results you write to the current directory are saved as output.","ae6f051e":"df = pd.read_csv('..\/input\/churn-in-telecoms-dataset\/bigml_59c28831336c6604c800002a.csv')\ndf_orig = pd.read_csv('..\/input\/churn-in-telecoms-dataset\/bigml_59c28831336c6604c800002a.csv')\ntarget = 'churn'\ny = df[target]\nlabels = df.columns","4f1d3497":"df.head()","96f0b2d5":"df.info()","909be40a":"df = df.drop(['phone number'], axis = 1)\ndf = df.drop(['area code'], axis = 1)\ndf = df.drop(['state'], axis = 1)","a3bd3eb0":"df.head()","cd51604d":"df['international plan'] = df['international plan'].map({'yes': 1, 'no': 0})\ndf['voice mail plan'] = df['voice mail plan'].map({'yes': 1, 'no': 0})\ndf['churn'] = df['churn'].map({True: 1, False: 0})","1fedbc57":"df.head()","ca20efdc":"X = df.drop([target],axis = 1)","4eff96cc":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)","bec561af":"scaler = MinMaxScaler()\nlr = LogisticRegression()\npipe = make_pipeline(scaler, lr)\n\npipe.fit(X_train, y_train)\n\ntrain_preds = pipe.predict(X_train)\ntest_preds = pipe.predict(X_test)","79023b7b":"scores = cross_val_score(pipe, X, y, cv=5, scoring='roc_auc')\nprint(scores.mean(), \"+\/-\", scores.std())","9f68ff6d":"def stringify(data):\n    df = pd.DataFrame(data)\n    for c in df.columns.tolist():\n        df[c] = df[c].astype(str)\n    return df\n\nbinner = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile')\nobjectify = FunctionTransformer(func=stringify, \n                                validate=False)\nclf = LogisticRegression(class_weight='balanced')\nencoder = WOEEncoder()\nscorecard = make_pipeline(binner, objectify, encoder, lr)\n\n\nscores = cross_val_score(scorecard, X, y, cv=5, scoring='roc_auc')\nprint(scores.mean(), \"+\/-\", scores.std())","1c9b449b":"X = Pipeline(scorecard.steps[:-1]).fit_transform(X, y).values\nused_cols = [c for c in df.columns.tolist() if c not in [target]]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=1, stratify=y)\n\nclf.fit(X_train, y_train)\nimp_vals, imp_all = feature_importance_permutation(\n    predict_method=clf.predict, \n    X=X_test,\n    y=y_test,\n    metric='accuracy',\n    num_rounds=10,\n    seed=1)\n\nstd = np.std(imp_all, axis=1)\nindices = np.argsort(imp_vals)[::-1]\n\nplt.figure()\nplt.title(\"Scorecard Feature Importance via Permutation Importance\")\nplt.bar(range(X.shape[1]), imp_vals[indices],\n        yerr=std[indices])\n# plt.xticks(range(X.shape[1]), indices)\nplt.xticks(range(X.shape[1]), np.array(used_cols)[indices], rotation = 90)\nplt.xlim([-1, X.shape[1]])\nplt.ylim([0, 0.05])\nplt.show()","ffa99e60":"important_feat = ['customer service calls', 'total day minutes','total intl calls']","b5daa8f8":"from sklearn.base import TransformerMixin\n\nclass ForestEncoder(TransformerMixin):\n    \n    def __init__(self, forest):\n        self.forest = forest\n        self.n_trees = 1\n        try:\n            self.n_trees = self.forest.n_estimators\n        except:\n            pass\n        self.ohe = OneHotEncoder(cols=range(self.n_trees), use_cat_names=True)\n        \n    def fit(self, X, y=None):\n        self.forest.fit(X, y)\n        self.ohe.fit(self.forest.apply(X))\n        return self\n    \n    def transform(self, X, y=None):\n        return self.ohe.transform(self.forest.apply(X))","fdf7e751":"#entropy criterion\nused_cols = [c for c in df.columns.tolist() if c not in [target]]\nX, y = df[used_cols].values, df[target].values\n\nN = 5\n\nrf = RandomForestClassifier(max_depth = N, n_estimators=100, n_jobs=-1, random_state=42,criterion = 'entropy', max_leaf_nodes = 2**N-1)\nencoder = ForestEncoder(rf)\nclf = LogisticRegression(class_weight='balanced')\npipe = make_pipeline(encoder, clf)\npipe.fit(X, y)\n\nscores = cross_val_score(rf, X, y, cv=5, scoring='roc_auc')\nprint(scores.mean(), \"+\/-\", scores.std())","851fb396":"#gini criterion\n\nrf = RandomForestClassifier(max_depth = N, n_estimators=100, n_jobs=-1, random_state=42,criterion = 'gini', max_leaf_nodes = 2**N-1)\nencoder = ForestEncoder(rf)\nclf = LogisticRegression(class_weight='balanced')\npipe = make_pipeline(encoder, clf)\npipe.fit(X, y)\n\nscores = cross_val_score(rf, X, y, cv=5, scoring='roc_auc')\nprint(scores.mean(), \"+\/-\", scores.std())","275038c2":"encoder = ForestEncoder(rf)\nclf = LogisticRegression(class_weight='balanced')\npipe = make_pipeline(encoder, clf)\n\nscores = cross_val_score(pipe, X, y, cv=5, scoring='roc_auc')\nprint(scores.mean(), \"+\/-\", scores.std())","26e1d396":"importances = rf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in rf.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), np.array(used_cols)[indices], rotation = 90)\nplt.xlim([-1, X.shape[1]])\nplt.show()","c63c5021":"rf_imp_feat = ['total day charge','total day minutes','total eve charge']","47ce5286":"#smote to affect imbalances\n\nsmote = SMOTE()\nX_resampled, y_resampled = smote.fit_resample(X, y)\n\nN = 5\n\nrf = RandomForestClassifier(max_depth = N, n_estimators=100, n_jobs=-1, \n                            random_state=42,criterion = 'entropy', \n                            max_leaf_nodes = 2**N-1)\nencoder = ForestEncoder(rf)\nclf = LogisticRegression(class_weight='balanced')\npipe = make_pipeline(encoder, clf)\npipe.fit(X_resampled, y_resampled)\n\n","bae40b99":"scores = cross_val_score(rf, X_resampled, y_resampled, cv=5, scoring='roc_auc')\nprint(scores.mean(), \"+\/-\", scores.std())","05e9dbd5":"#create train test split for resampled data\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_resampled, y_resampled, test_size=0.2, random_state=1, stratify=y_resampled)","507a6cd9":"grid_p = {\"n_estimators\": [20, 50, 100],\n          \"criterion\": [\"gini\", \"entropy\"],\n          \"max_features\": ['sqrt', 'log2', 0.2],\n          \"max_depth\": [4, 6, 10],\n          \"min_samples_split\": [2, 5, 10],\n          \"min_samples_leaf\": [1, 5, 10]}\n\ngrid_search = GridSearchCV(rf, grid_p, n_jobs=-1, cv=5, scoring='roc_auc')\ngrid_search.fit(X_train, y_train)","e1361eeb":"grid_search.best_score_","2e66ea63":"grid_search.best_params_","9f5541b2":"rf = RandomForestClassifier(criterion='entropy',\n max_depth=10,\n max_features='sqrt',\n min_samples_leaf=1,\n min_samples_split=5,\n n_estimators=20)\nencoder = ForestEncoder(rf)\nclf = LogisticRegression(class_weight='balanced')\npipe = make_pipeline(encoder, clf)\npipe.fit(X_resampled, y_resampled)\n\nscores = cross_val_score(rf, X_resampled, y_resampled, cv=5, scoring='roc_auc')\nprint(scores.mean(), \"+\/-\", scores.std())","72bdb8b5":"rf = RandomForestClassifier(criterion='entropy',\n max_depth=10,\n max_features='sqrt',\n min_samples_leaf=5,\n min_samples_split=2,\n n_estimators=100)\nencoder = ForestEncoder(rf)\nclf = LogisticRegression(class_weight='balanced')\npipe = make_pipeline(encoder, clf)\npipe.fit(X_resampled, y_resampled)\n\nscores = cross_val_score(rf, X_resampled, y_resampled, cv=5, scoring='roc_auc')\nprint(scores.mean(), \"+\/-\", scores.std())","e6a4cd1d":"**Look at the Data**","b9676501":"**Read the Data**","90276b62":"**Random Forest**","e2dd56e9":"**Create Train Test split**","caa749f7":"**Log Reg**","c4d58364":"**Dealing with Imbalances **","ac4a349a":"**Hyper Parameter Tuning**"}}