{"cell_type":{"19fd9cba":"code","40c0827a":"code","3d7e1fec":"code","66f60d38":"code","a3b70cf6":"code","3df1bd78":"code","75b2fc9d":"code","c283c12f":"code","93595fbc":"code","ea58ed15":"code","99e761fc":"code","9e3e2928":"code","c53291d1":"code","4cce0aa8":"code","079fce86":"code","c78b520e":"code","bc69820c":"code","bf6c7c32":"code","b60bdd30":"code","b347be97":"code","e75193aa":"code","2d2f60bf":"code","2faa0fce":"code","857f5ddc":"code","cccf49e4":"code","66792795":"code","eef87620":"code","cdcb7fa0":"code","3daf37e3":"markdown","02a8dde0":"markdown","5ea2cf5e":"markdown","8b12b51f":"markdown","fd9285a3":"markdown"},"source":{"19fd9cba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\npd.set_option('display.max_colwidth', 100)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","40c0827a":"# Reading the data\n\ndf = pd.read_csv(\"\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv\")\n\n# NAN value checking \n\ndf_nan_count = pd.DataFrame(df.isnull().sum())\ndf_nan_count = df_nan_count.reset_index()\ndf_nan_count.columns = [\"colname\",\"count of null value\"]\ndisplay(df_nan_count)","3d7e1fec":"# taking only relevant columns\ndf = df[['v1', 'v2']]","66f60d38":"df.head()","a3b70cf6":"# % of label\npd.DataFrame(df[\"v1\"].value_counts() \/ df[\"v1\"].count().sum() * 100 )\n# 86 % ham and 13 % spam","3df1bd78":"# What is the shape of the dataset?\n\nprint(\"Input data has {} rows and {} columns\".format(len(df), len(df.columns)))","75b2fc9d":"df.columns = ['label', 'body_text']","c283c12f":"# How many spam\/ham are there?\n\nprint(\"Out of {} rows, {} are spam, {} are ham\".format(len(df),\n                                                       len(df[df['label']=='spam']),\n                                                       len(df[df['label']=='ham'])))","93595fbc":"# How much missing data is there?\n\nprint(\"Number of null in label: {}\".format(df['label'].isnull().sum()))\nprint(\"Number of null in text: {}\".format(df['body_text'].isnull().sum()))","ea58ed15":"import nltk\nnltk.download('stopwords')\nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nimport string\n\nstopwords = nltk.corpus.stopwords.words('english')\nps = nltk.PorterStemmer()\n\n\n\ndef count_punct(text):\n    count = sum([1 for char in text if char in string.punctuation])\n    return round(count\/(len(text) - text.count(\" \")), 3)*100\n\n\ndef clean_text(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens = re.split('\\W+', text)\n    text = [ps.stem(word) for word in tokens if word not in stopwords]\n    return text\n\n","99e761fc":"df['body_clean_text'] = df['body_text'].apply(lambda x: clean_text(x))\ndf['body_len'] = df['body_text'].apply(lambda x: len(x) - x.count(\" \"))\ndf['punct%'] = df['body_text'].apply(lambda x: count_punct(x))\n","9e3e2928":"from matplotlib import pyplot\nimport numpy as np\n%matplotlib inline\nbins = np.linspace(0, 200, 40)\n\npyplot.hist(df[df['label']=='spam']['body_len'], bins, alpha=0.5, density = True, label='spam')\npyplot.hist(df[df['label']=='ham']['body_len'], bins, alpha=0.5, density = True, label='ham')\npyplot.legend(loc='upper left')\npyplot.show()\n#### spam are more lengthy than ham","c53291d1":"bins = np.linspace(0, 50, 40)\n\npyplot.hist(df[df['label']=='spam']['punct%'], bins, alpha=0.5, density = True, label='spam')\npyplot.hist(df[df['label']=='ham']['punct%'], bins, alpha=0.5, density = True, label='ham')\npyplot.legend(loc='upper right')\npyplot.show()\n\n# checking if punctuation can be a feature of creating this model or not","4cce0aa8":"#### ","079fce86":"\n\n# TF-IDF\ntfidf_vect = TfidfVectorizer(analyzer=clean_text)\nX_tfidf = tfidf_vect.fit_transform(df['body_text'])\nX_tfidf_feat = pd.concat([df['body_len'], df['punct%'], pd.DataFrame(X_tfidf.toarray())], axis=1)\n\n# CountVectorizer\ncount_vect = CountVectorizer(analyzer=clean_text)\nX_count = count_vect.fit_transform(df['body_text'])\nX_count_feat = pd.concat([df['body_len'], df['punct%'], pd.DataFrame(X_count.toarray())], axis=1)\n\nX_count_feat.head()","c78b520e":"df.head()","bc69820c":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.model_selection import train_test_split\nprint(dir(RandomForestClassifier))\nprint(RandomForestClassifier()) # exploring hyperparameters","bf6c7c32":"# train test split\nX_train, X_test, y_train, y_test = train_test_split(X_tfidf_feat, df['label'], test_size=0.2)","b60bdd30":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=50, max_depth=20, n_jobs=-1)\nrf_model = rf.fit(X_train, y_train)","b347be97":"sorted(zip(rf_model.feature_importances_, X_train.columns), reverse=True)[0:10]","e75193aa":"y_pred = rf_model.predict(X_test)\nprecision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')","2d2f60bf":"print('Precision: {} \/ Recall: {} \/ Accuracy: {}'.format(round(precision, 3),\n                                                        round(recall, 3),\n                                                        round((y_pred==y_test).sum() \/ len(y_pred),3)))","2faa0fce":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV","857f5ddc":"rf = RandomForestClassifier()\nparam = {'n_estimators': [10, 150, 300],\n        'max_depth': [30, 60, 90, None]}\n\ngs = GridSearchCV(rf, param, cv=5, n_jobs=-1)\ngs_fit = gs.fit(X_tfidf_feat, df['label'])\ndisplay(pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5])\n\ny_pred = gs_fit.predict(X_test)\nprecision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\n\nprint('Precision: {} \/ Recall: {} \/ Accuracy: {}'.format(round(precision, 3),\n                                                        round(recall, 3),\n                                                        round((y_pred==y_test).sum() \/ len(y_pred),3)))","cccf49e4":"#### ","66792795":"from sklearn.ensemble import GradientBoostingClassifier\nprint(dir(GradientBoostingClassifier))\nprint(GradientBoostingClassifier())","eef87620":"gb = GradientBoostingClassifier()\nparam = {\n    'n_estimators': [100, 150], \n    'max_depth': [7, 11, 15],\n    'learning_rate': [0.1]\n}\n\nclf = GridSearchCV(gb, param, cv=5, n_jobs=-1)\ncv_fit = clf.fit(X_tfidf_feat, df['label'])\npd.DataFrame(cv_fit.cv_results_).sort_values('mean_test_score', ascending=False)[0:5]","cdcb7fa0":"y_pred = cv_fit.predict(X_test)\nprecision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\n\nprint('Precision: {} \/ Recall: {} \/ Accuracy: {}'.format(round(precision, 3),\n                                                        round(recall, 3),\n                                                        round((y_pred==y_test).sum() \/ len(y_pred),3)))","3daf37e3":"#### Using Grid Search ","02a8dde0":"#### Using XG boost","5ea2cf5e":"#### spam are more lengthy than ham","8b12b51f":"#### Using Random Forest","fd9285a3":"#### creating TF-IDf and Document term matrix both to compare our model"}}