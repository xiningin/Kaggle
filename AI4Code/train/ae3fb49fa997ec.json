{"cell_type":{"1ba74626":"code","314433b3":"code","74f12aaa":"code","5c8d6df8":"code","f454ff59":"code","60561b22":"code","22996aac":"code","fa270640":"code","b526da0f":"code","65bceb16":"code","ac5f963e":"code","9d8f3840":"code","d1403940":"code","ddd4ed01":"code","0add8cc3":"code","896310d0":"code","a2b4d245":"code","7519da01":"code","d8f5e26d":"code","bbcbe5af":"code","b686da72":"code","83b2ebc8":"code","f8e6dbfb":"code","4fd70ed3":"code","8539ec5f":"code","f5431ecb":"code","5b26b271":"code","1207d74e":"code","b112da9f":"code","9b31fab6":"code","1306f9d4":"code","1f76375a":"code","c922777f":"code","01a966c2":"code","837b973e":"code","77a0447d":"code","8bcaad9f":"code","c8df1823":"code","0cec28d9":"code","97371f7c":"code","7e18c212":"code","bdcc056c":"code","f5ba9bc8":"code","244b7162":"code","c867716d":"code","4a0f872e":"code","8ecdf0c8":"code","cda84d03":"code","6f382422":"code","d2749fa0":"code","5d998bb6":"code","c44d7bd7":"code","96943bfa":"code","a86d7d77":"code","9d0f4848":"code","6845728d":"code","257e8913":"code","30dac905":"code","09b6d955":"markdown","ccc6fd82":"markdown","44b9d410":"markdown","cdfed486":"markdown","049b4582":"markdown","c2c69087":"markdown","23534064":"markdown","01b10c36":"markdown","646a4eb6":"markdown","da0fc616":"markdown","ae43b2d7":"markdown","55f7e590":"markdown","24d15ca5":"markdown"},"source":{"1ba74626":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","314433b3":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split","74f12aaa":"housing = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')","5c8d6df8":"housing.info()","f454ff59":"houses = housing.drop(['Alley', 'PoolQC', 'Fence', 'MiscFeature'],axis=1)","60561b22":"plt.subplots(figsize=(12, 9))\nsns.heatmap(housing.corr(), vmin=-1, vmax=1, cmap='RdGy')\nplt.show()","22996aac":"def select_cols_corr(df_corr, target_col, min_corr, max_corr): \n    target_corr = df_corr[target_col].reset_index()\n    return target_corr.loc[(target_corr.iloc[:,1] < max_corr) & (target_corr.iloc[:,1] > min_corr),:]\n\nnum_cols = select_cols_corr(houses.corr(), 'SalePrice', min_corr=.4, max_corr=.95).iloc[:,0].tolist()","fa270640":"cat_cols = houses.select_dtypes(['object']).columns.tolist()\nhouses.drop(houses.columns.difference(cat_cols+num_cols+['SalePrice']), axis=1,inplace=True)","b526da0f":"for col in cat_cols:\n    houses[col] = houses[col].astype('category')","65bceb16":"houses.info()","ac5f963e":"X = houses.drop(columns=['SalePrice'])\ny = houses['SalePrice']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=.8, random_state=8)\nX_train.head()","9d8f3840":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import max_error\nfrom sklearn.metrics import mean_squared_log_error\n\ndef plot_predictions(y_true, y_pred): \n    \n    print(\n        f\"\"\"\n        MSE: {mean_squared_error(y_true, y_pred)}\n        RMSE: {mean_squared_error(y_true, y_pred)**0.5}\n        MAE: {mean_absolute_error(y_true, y_pred)}\n        R_SQR: {r2_score(y_true, y_pred)}\n        EXV: {explained_variance_score(y_true, y_pred)}\n        ME: {max_error(y_true, y_pred)}\n        RMSLE: {mean_squared_log_error(y_true, y_pred)**0.5}\n        \"\"\"\n    )\n    \n    max_preds = min([max(y_pred.tolist()), max(y_true.tolist())])\n    min_preds = max([min(y_pred.tolist()), min(y_true.tolist())])\n    print(max_preds, min_preds)\n    # plot\n    plt.figure(figsize=(8,8))\n    sns.scatterplot(x=y_pred, y=y_true)\n    sns.lineplot(x=[min_preds,max_preds], y=[min_preds, max_preds], color='red')\n    plt.ylabel('Reference')\n    plt.xlabel('Predictions')\n    plt.show()\n\n    errors = y_pred - y_true\n    plt.subplots(figsize=(12, 8))\n    sns.histplot(errors)\n    plt.vlines(x=0, ymin=0, ymax=150, color='red')\n    plt.show()\n\n    p_df = (\n        pd.DataFrame({'y_true':y_true, 'y_pred':y_pred})\n        .assign(error = lambda x: x['y_pred'] - x['y_true'])\n        .sort_values(by='y_true')\n        )\n\n    plt.subplots(figsize=(12, 8))\n    sns.scatterplot(data=p_df, x='y_true', y='error')\n    plt.hlines(y=0, xmin=0, xmax=800000, color='red')\n    plt.show()","d1403940":"numeric_pipeline = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='mean')),\n    ('scale', MinMaxScaler())\n\n])\n\ncategorical_pipeline = Pipeline(steps=[\n    ('impute', SimpleImputer(strategy='most_frequent')),\n    ('one-hot', OneHotEncoder(handle_unknown='ignore' ))\n])\n\n\nfull_processor = ColumnTransformer(transformers=[\n    ('number', numeric_pipeline, num_cols),\n    ('category', categorical_pipeline, cat_cols)\n])","ddd4ed01":"results_all = {'Method':[],'MSE':[],'RMSE':[],}","0add8cc3":"from xgboost.sklearn import XGBRegressor\n\nXGBR_pipeline = Pipeline(steps=[\n    ('processor', full_processor), \n    ('model', XGBRegressor(learning_rate=0.001,n_estimators=4600, max_depth=7, min_child_weight=0, gamma=0, subsample=0.7, colsample_bytree=0.7, scale_pos_weight=1, seed=27, reg_alpha=0.00006))\n])\n\n_ = XGBR_pipeline.fit(X_train, y_train)","896310d0":"XGBR_pipeline.named_steps.model.get_params().keys()","a2b4d245":"plot_predictions(y_train, XGBR_pipeline.predict(X_train))","7519da01":"XGBR_pipeline.score(X_test,y_test)","d8f5e26d":"plot_predictions(y_test, XGBR_pipeline.predict(X_test))","bbcbe5af":"house_test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","b686da72":"final = house_test.drop(house_test.columns.difference(cat_cols+num_cols), axis=1)","83b2ebc8":"XGBR_pipeline.fit(X,y)","f8e6dbfb":"Predictions = XGBR_pipeline.predict(final)","4fd70ed3":"submission_file = pd.DataFrame({\n    'Id': house_test['Id'],\n    'SalePrice': Predictions\n}).to_csv('submission.csv', index=None )","8539ec5f":"new_data =(\npd.concat([final, pd.DataFrame({\n    'Id': house_test['Id'],\n    'SalePrice': Predictions\n})], axis=1)\n)","f5431ecb":"new_data","5b26b271":"houses_exp_test = new_data[new_data.SalePrice > 400000]","1207d74e":"cheap_houses = new_data[new_data.SalePrice <= 400000]","b112da9f":"houses_exp_test","9b31fab6":"houses_exp = houses[houses.SalePrice > 400000]","1306f9d4":"X_t = houses_exp.drop(columns=['SalePrice'])\ny_t = houses_exp['SalePrice']\n\nX_train_t, X_test_t, y_train_t, y_test_t = train_test_split(X_t, y_t, train_size=.8, random_state=8)","1f76375a":"XGBR_pipeline.fit(X_t,y_t)","c922777f":"Predictions_exp = XGBR_pipeline.predict(houses_exp_test.drop('Id',axis=1))","01a966c2":"exp = pd.DataFrame({\n    'Id': houses_exp_test['Id'],\n    'SalePrice': Predictions_exp\n})","837b973e":"cheap_houses.drop(cheap_houses.columns.difference(['Id']+['SalePrice']),axis=1,inplace=True)","77a0447d":"final2 = pd.concat([exp,cheap_houses])","8bcaad9f":"final2 = final2.sort_values('Id')","c8df1823":"final2","0cec28d9":"final2.to_csv('submission.csv', index=None )","97371f7c":"from sklearn.linear_model import LinearRegression\n\nlm_pipeline = Pipeline(steps=[\n    ('processor', full_processor), \n    ('model', LinearRegression())\n])\n\n_ = lm_pipeline.fit(X_train, y_train)","7e18c212":"lm_pipeline.named_steps.model.get_params().keys()","bdcc056c":"plot_predictions(y_train, lm_pipeline.predict(X_train))","f5ba9bc8":"lm_pipeline.score(X_test,y_test)","244b7162":"plot_predictions(y_test, lm_pipeline.predict(X_test))","c867716d":"from sklearn.ensemble import RandomForestRegressor\n\nran_for_pipeline = Pipeline(steps=[\n    ('processor', full_processor), \n    ('model', RandomForestRegressor())\n])\n\n_ = ran_for_pipeline.fit(X_train, y_train)","4a0f872e":"# from sklearn.model_selection import GridSearchCV\n\n# param_grid = {\n#     'model__max_depth': [10,20,40],\n#     'model__min_samples_leaf': [1, 2, 5],\n#     'model__min_samples_split': [2, 3, 5]\n#     }\n\n# search = GridSearchCV(\n#     ran_for_pipeline, \n#     param_grid, \n#     cv=5,\n#     scoring='accuracy', \n#     verbose=1, \n#     refit=True, \n#     n_jobs=-1\n#     )","8ecdf0c8":"# _ = search.fit(X_train, y_train)","cda84d03":"ran_for_pipeline.named_steps.model.get_params().keys()","6f382422":"plot_predictions(y_train, ran_for_pipeline.predict(X_train))","d2749fa0":"ran_for_pipeline.score(X_test,y_test)","5d998bb6":"plot_predictions(y_test, ran_for_pipeline.predict(X_test))","c44d7bd7":"from sklearn.linear_model import Lasso\n\nlasso_pipeline = Pipeline(steps=[\n    ('processor', full_processor), \n    ('model', Lasso())\n])\n\n_ = lasso_pipeline.fit(X_train, y_train)","96943bfa":"lasso_pipeline.named_steps.model.get_params().keys()","a86d7d77":"plot_predictions(y_train, lasso_pipeline.predict(X_train))","9d0f4848":"lasso_pipeline.score(X_test,y_test)","6845728d":"plot_predictions(y_test, lasso_pipeline.predict(X_test))","257e8913":"ran_for_pipeline.fit(X_train, np.log(y_train))\nplot_predictions(y_train, np.exp(ran_for_pipeline.predict(X_train)))","30dac905":"plot_predictions(y_test, np.exp(ran_for_pipeline.predict(X_test)))","09b6d955":"## Lasso","ccc6fd82":"### Test set","44b9d410":"## Linear Regression","cdfed486":"### Training set","049b4582":"### Test set","c2c69087":"### Test set","23534064":"### Train set","01b10c36":"### Testing set","646a4eb6":"learning_rate=0.001,n_estimators=4600,\n                                max_depth=7, min_child_weight=0,\n                                gamma=0, subsample=0.7,\n                                colsample_bytree=0.7,\n                                scale_pos_weight=1, seed=27,\n                                reg_alpha=0.00006)","da0fc616":"## XGBR Regressor","ae43b2d7":"### Train set","55f7e590":"## Logarithmic transformation","24d15ca5":"## TEST SET FROM KAGGLE"}}