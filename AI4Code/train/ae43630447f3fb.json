{"cell_type":{"b3894490":"code","cf5ed491":"code","3cd2bf17":"code","f77b4f4e":"code","abebfcdc":"code","2f3ee0ad":"code","9ff5250d":"code","9584fd04":"code","fa3ef7ee":"code","01cbbc76":"code","6a343252":"code","a0c26c6c":"code","9cdc2c98":"code","3775d45b":"code","6e0be80d":"code","0d876962":"markdown","136b5cd9":"markdown","eb84739f":"markdown","16ac419c":"markdown","ad9587d2":"markdown","506c9112":"markdown","6d230030":"markdown","ee52480c":"markdown","91a00253":"markdown","7f3bf8bb":"markdown","e657468e":"markdown","95d04d76":"markdown","5f8621d9":"markdown","338f101c":"markdown"},"source":{"b3894490":"import numpy as np\nimport pandas as pd \nimport os\nimport datetime as dt","cf5ed491":"!ls ..\/input\/jigsaw-toxic-comment-classification-challenge","3cd2bf17":"data = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv')\ndata.head()","f77b4f4e":"data['toxic'] = data[data.columns[2:]].sum(axis=1)\ntoxic = data[data['toxic'] > 0]\nnot_toxic = data[data['toxic'] == 0]\nprint(f'toxic comments: {toxic.shape[0]}, normal: {not_toxic.shape[0]}')","abebfcdc":"!ls ..\/input\/wikipedia-talk-labels-personal-attacks\/","2f3ee0ad":"data_commets = pd.read_csv('..\/input\/wikipedia-talk-labels-personal-attacks\/attack_annotated_comments.csv')\ndata_attack =  pd.read_csv('..\/input\/wikipedia-talk-labels-personal-attacks\/attack_annotations.csv')","9ff5250d":"data_attack.drop(columns=data_attack.columns[1:-1], inplace=True)\nsummery = data_attack.groupby(['rev_id']).sum()\ndata = data_commets.set_index('rev_id').join(summery)","9584fd04":"data.head()","fa3ef7ee":"toxic = data[data['attack'] > 0]\nnot_toxic = data[data['attack'] == 0]\nprint(f'toxic comments: {toxic.shape[0]}, normal: {not_toxic.shape[0]}')","01cbbc76":"!ls ..\/input\/wikipedia-talk-corpus-sample\/","6a343252":"# The whole corpus is too large to load in kernel, here is a small sample\ndata = pd.read_csv('..\/input\/wikipedia-talk-corpus-sample\/chunk_0.tsv', sep='\\t')\ndata.head()","a0c26c6c":"!pip install psaw","9cdc2c98":"# Example of general use\nfrom psaw import PushshiftAPI\napi = PushshiftAPI()\n\n# The `search_comments` and `search_submissions` methods return generator objects\ngen = api.search_submissions(limit=100)\nresults = list(gen)\n\n# There are 2 main attributes we may be interested in:\n# title - provides the title of a submission\nprint(results[1].title)\n# selftext - provides main text, if text exists\nprint(results[1].selftext)","3775d45b":"# Start time\nstart_epoch=int(dt.datetime(2017, 1, 1).timestamp())\n# Found submissions\nshit_reddit_says = list(api.search_submissions(after=start_epoch,\n                                               subreddit='ShitRedditSays',\n                                               filter=['url','author', 'title', 'subreddit'],\n                                               limit=10))\n\n# Some questionable comments\nfor i in range (6):\n    print(shit_reddit_says[i].title, '\\n')","6e0be80d":"# Start time\nstart_epoch=int(dt.datetime(2018, 1, 1).timestamp())\n# Found submissions\nlgbt = list(api.search_submissions(after=start_epoch,\n                                               subreddit='lgbt',\n                                               filter=['url','author', 'title', 'subreddit'],\n                                               limit=10))\n\n# Some positive comments\nfor i in range (6):\n    print(lgbt[i].title, '\\n')","0d876962":"[ShitRedditSays](https:\/\/www.reddit.com\/r\/ShitRedditSays\/) is\nself appointed reddit hate speech watchdog.\nAccording to web.archive.org subreddit started in the midst of 2011","136b5cd9":"#### Male\n* [MensRights](https:\/\/www.reddit.com\/r\/MensRights\/)\n* [AskMen](https:\/\/www.reddit.com\/r\/AskMen)\n\n#### Female\n* [AskWomen](https:\/\/www.reddit.com\/r\/AskWomen)\n* [women](https:\/\/www.reddit.com\/r\/women\/)\n* [femenism](https:\/\/www.reddit.com\/r\/femenism\/)\n\n#### homosexual_gay_or_lesbian\n* [lgbt](https:\/\/www.reddit.com\/r\/lgbt\/)\n* [gaybros](https:\/\/www.reddit.com\/r\/gaybros\/)\n* [LesbianActually](https:\/\/www.reddit.com\/r\/LesbianActually\/)\n\n#### christian\n* [Catholicism](https:\/\/www.reddit.com\/r\/Catholicism\/)\n\n#### Jewish \n* [Jewish](https:\/\/www.reddit.com\/r\/Jewish\/)\n* [Judaism](https:\/\/www.reddit.com\/r\/Judaism\/)\n\n#### muslim\n* [islam](https:\/\/www.reddit.com\/r\/islam\/)\n\n#### black\n* [BlackPeopleTwitter](https:\/\/www.reddit.com\/r\/BlackPeopleTwitter\/)\n* [Blackfellas](https:\/\/www.reddit.com\/r\/Blackfellas\/)\n* [AsABlackMan](https:\/\/www.reddit.com\/r\/AsABlackMan\/)\n\n#### white\nWasn't able to find anything particularly positive)))\n\n#### psychiatric_or_mental_illness\n* [mentalhealth](https:\/\/www.reddit.com\/r\/mentalhealth\/)","eb84739f":"### Wholsome reddit","16ac419c":"### Wikipedia Talk Corpus\n-----------\nhttps:\/\/figshare.com\/articles\/Wikipedia_Talk_Corpus\/4264973","ad9587d2":"## Reddit\n--------------------------------\nwill use https:\/\/github.com\/dmarx\/psaw \nA minimalist wrapper for searching public reddit comments\/submissions via the pushshift.io API.","506c9112":"### K-nearest neuborghs\n_____________________________\nThe idea is, using embedding and k-nn to find \"close\" words to each other.\nWas tried:\n\nhttps:\/\/www.kaggle.com\/theoviel\/using-word-embeddings-for-data-augmentation\n\nhttps:\/\/www.kaggle.com\/shujian\/fake-some-positive-data-data-augmentation\n\nDidn't work for people, who tried, so I've skiped implementation\n","6d230030":"# Third party data sources","ee52480c":"### Translation\n-------------------------\nProposed in https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\/discussion\/48038#272289\nby @pavelost\n\nThe code and instruction can be found in my GitHub repo: https:\/\/github.com\/PavelOstyakov\/toxic\/tree\/master\/tools","91a00253":"### Marcov chains\n_______________________\nWas tried in\nhttps:\/\/www.kaggle.com\/jpmiller\/extending-train-data-with-markov-chains\n\nDidn't work for people, who tried, so I've skiped implementation","7f3bf8bb":"## Data Augmentation","e657468e":"### Wikipedia Talk Labels: Personal Attacks\n-------------\nhttps:\/\/figshare.com\/articles\/Wikipedia_Detox_Data\/4054689","95d04d76":"### Toxic Reddit","5f8621d9":"### First jigsaw competition","338f101c":"This kernel is an attempt to list as many as possible ways to augment or parse more data for the \"jigsaw II competition\".\nWork in progress."}}