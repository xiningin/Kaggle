{"cell_type":{"898989ab":"code","90890e25":"code","0804c7a8":"code","476bc7a8":"code","b7af7b31":"code","8755938a":"code","1d8ae5af":"code","f8d9aec0":"code","69057892":"code","c9564193":"code","c2150ba3":"code","810a1ea5":"code","3a1f69c6":"code","a597073f":"code","70ebab9c":"code","b7d97fbc":"markdown","6e91f29c":"markdown"},"source":{"898989ab":"import xgboost as xgb\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\nimport numpy as np \nimport pandas as pd \n\nimport gc\nimport datetime\nimport joblib\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","90890e25":"##### Functions \n\n# 1st function: Adds number of ocurrences to the column element to see if it is a frequent value or not.\ndef frecuency_encoder(cols, df_train, df_test):\n    for col in cols:\n        temp = df_train[col].value_counts().to_dict()\n        df_train[f'{col}_counts'] = df_train[col].map(temp).astype('float32')\n        \n        temp = df_test[col].value_counts().to_dict()\n        df_test[f'{col}_counts']  = df_test[col].map(temp).astype('float32')\n\n# 2nd function : creates a list of statistics (in aggregations) to be applied to a list of columns, given to group specified by group.\ndef aggregations(cols, group, aggregations, df_train, df_test):\n    for col in cols:\n        for aggr in aggregations:\n            temp     = df_train.groupby([group])[col].agg([aggr])\n            dict_aux = temp[aggr].to_dict()\n            df_train[f'{col}_{aggr}'] = df_train[group].map(dict_aux).astype('float32')\n            \n            temp     = df_test.groupby([group])[col].agg([aggr])\n            dict_aux = temp[aggr].to_dict()\n            df_test[f'{col}_{aggr}'] = df_test[group].map(dict_aux).astype('float32')   ","0804c7a8":"##### Download of files.\n\nprint('Downloading datasets...')\nprint(' ')\ntrain = pd.read_pickle('\/kaggle\/input\/ieee-cis-preprocessing\/train.pkl')\nprint('Train has been downloaded... (1\/2)')\ntest = pd.read_pickle('\/kaggle\/input\/ieee-cis-preprocessing\/test.pkl')\nprint('Test has been downloaded... (2\/2)')\nprint(' ')\nprint('All files are downloaded')","476bc7a8":"##### Additional Preprocessing\n# Filling NaNs. Does not change engineered Ds with negatives (-9999).\n# Rest of the columns model worked better with Nan -> -1 than -999.\n\nfill_nan_exceptions = ['D1achr', 'D2achr', 'D4achr', 'D6achr', 'D10achr', 'D11achr', 'D12achr', 'D13achr', 'D14achr', 'D15achr']\n\nfill_nan = list(train.columns)\nfill_nan.remove('isFraud')\n\nfor col in fill_nan_exceptions:\n    fill_nan.remove(col)\n    \ntrain[fill_nan] = train[fill_nan].replace(np.nan, -1)\ntest[fill_nan]  = test[fill_nan].replace(np.nan, -1)\n\ntrain[fill_nan_exceptions] = train[fill_nan_exceptions].replace(np.nan, -9999)\ntest[fill_nan_exceptions]  = test[fill_nan_exceptions].replace(np.nan, -9999) ","b7af7b31":"##### Feature Engineering\nfrecuency_encoder(['card1', 'card2','D1achr','addr1'], train, test)","8755938a":"##### Feature selection for the model\n\nfeature_list = list(train.columns)\n\n# Remove overfitting. Target - original engineered Ds from model features\n\nremove_features = ['TransactionID', 'isFraud', 'TransactionDT', 'uid1', 'uid2', 'uids', \n                   'DT', 'D1', 'D2', 'D4', 'D6', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15']\n\n# Remove less important features (from feature importance results)\n\nworst_features = ['V150', 'V48', 'V164', 'D5', 'V125', 'V277', 'V182', 'D7', 'V271', 'V261', 'V196', 'V168', \n                  'V275', 'V5', 'V114', 'V132', 'V339', 'V250', 'V219', 'V103', 'V37', 'V307', 'V213', 'V186',\n                  'V105', 'V297', 'V291', 'V9', 'V278', 'V221', 'V214', 'dist2', 'V303', 'V180', 'V270', 'V98',\n                  'V304', 'V238', 'V290', 'V215', 'V306', 'V192', 'V236', 'V255', 'V130', 'V3', 'V46', 'V7',\n                  'V260', 'V212', 'V185', 'V319', 'V237', 'V120', 'V203', 'V101', 'V191', 'V247', 'V269', 'V273',\n                  'V157', 'V8', 'V167', 'V228', 'V127', 'V316', 'V4', 'M1', 'V84', 'V222', 'V128', 'V226', 'V116',\n                  'V177', 'V134', 'V276', 'V263', 'D9', 'V264', 'V176', 'V6', 'V265', 'V126', 'V199', 'V183', 'V202',\n                  'V211', 'V240', 'V218', 'V11', 'V302', 'V135', 'V204', 'V229', 'V292', 'V328', 'V230', 'V2', 'V122',\n                  'V10', 'V95', 'V96', 'V190', 'V104', 'V118', 'V97', 'V181', 'V1', 'V41', 'V117', 'V119', 'V28',\n                  'V65', 'V107', 'V305', 'V68', 'V27', 'V241', 'V89']\n\n# Get the feature list\n\nremove_features.extend(worst_features)\n\nfor feat in remove_features:\n    try:\n        feature_list.remove(feat)\n    except:\n        pass\n\nprint('Columns selected:', feature_list)\nprint('Length:', len(feature_list))\n\n# Get target\ntarget = 'isFraud'","1d8ae5af":"##### Train + val separation\n# Train: days 1  to 150 (5 months)\n# Test: days  150 to the end (1 month)\n\ndf_train = train.query('DT <= 150') \ndf_val  = train.query('DT >  150')","f8d9aec0":"##### XGBoosting\n\nclf = xgb.XGBClassifier(\n                objective = 'binary:logistic', \n                base_score = 0.5, \n                booster = 'gbtree', \n                colsample_bylevel = 1,\n                colsample_bynode = 1,\n                colsample_bytree = 0.4,     #percentage of columns by tree, overfitting parameter\n                gamma = 0,\n                gpu_id = -1,\n                importance_type = 'gain',\n                interaction_constraints = '',\n                learning_rate = 0.02,\n                max_delta_step = 0,\n                max_depth = 12,             #max tree depth, overfitting parameter\n                min_child_weight = 1,\n                missing = -1,               #missing value is the value to replace NaN. Default, np.nan\n                monotone_constraints =(),\n                n_estimators = 2000,        #number of trees. \n                n_jobs = 0,\n                num_parallel_tree = 1,\n                random_state = 0,\n                reg_alpha = 0, \n                reg_lambda = 1,\n                scale_pos_weight = 1,\n                subsample = 0.8,            #percentage of rows by tree, overfitting parameter\n                tree_method = 'gpu_hist',   #use gpu to speed the training\/fit of the model.\n                validate_parameters = 1,\n                verbosity = None,           #verbosity (0-3) O is silent, and 3 debug. It looks this does not impact in verbose.\n                eval_metric = 'auc')\n\nclf.fit(df_train[feature_list], df_train[target], \neval_set=[(df_train[feature_list],   df_train[target]), (df_val[feature_list],   df_val[target])],\n        verbose=100, early_stopping_rounds=100)","69057892":"##### AUC\n\nauc = clf.best_score\nprint('AUC:',auc) ","c9564193":"##### ROC Curve\ny_pred_valid = clf.predict_proba(df_val[feature_list])\nr_curve = roc_curve(df_val[target].values, y_pred_valid[:,1])\n\n\nplt.figure(figsize=(8,8))\nplt.title('ROC')\nplt.plot(r_curve[0], r_curve[1], 'b', label = 'AUC = %0.4f' % auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","c2150ba3":"##### Feature Importance\n\nfeatures_importance = clf.feature_importances_\n   \nfeatures_array = np.array(feature_list)\nfeatures_array_ordered = features_array[(features_importance).argsort()[::-1]]\nfeatures_array_ordered\n\nplt.figure(figsize=(16,10))\nsns.barplot(y=features_array, x=features_importance, orient='h', order=features_array_ordered[:50])\nplt.show()","810a1ea5":"##### Saving this results into an Excel file\n\nparams = clf.get_params()\n\ntry:\n    model_results = pd.read_excel('model_results.xlsx')\n    \n    model_results = model_results.append({'datetime' : datetime.datetime.now(), 'clf':clf.__class__, 'features': feature_list, 'parameters' : params,\n                                       'AUC': auc, 'features_importance': list(features_importance), 'features_ordered': list(features_array_ordered), \n                                       'r_curve': list(r_curve)}, ignore_index=True)\n        \nexcept:\n    model_results = pd.DataFrame(columns = ['summary', 'datetime', 'clf', 'features', 'parameters', 'entry1', 'entry2', \n                                            'AUC', 'features_importance', 'features_ordered', 'r_curve', 'output2'])\n    \n    model_results = model_results.append({'datetime' : datetime.datetime.now(), 'clf':clf.__class__, 'features': feature_list, 'parameters' : params, \n                                       'AUC': auc, 'features_importance': list(features_importance), 'features_ordered': list(features_array_ordered),\n                                       'r_curve': list(r_curve)}, ignore_index=True)\nfinally:\n    model_results.to_excel('model_results.xlsx')","3a1f69c6":"##### Saving this model\n\njoblib.dump(clf, 'model_xgboost.pkl')","a597073f":"y_pred_submission = clf.predict_proba(test[feature_list])[:,1]\nprint (y_pred_submission)\n\ndf_submission = pd.DataFrame({'TransactionID': test.TransactionID, 'isFraud': y_pred_submission})\ndf_submission\ndf_submission.to_csv('submission.csv', index=False)","70ebab9c":"df_val.to_pickle('val_xgb.pkl')\nprint('Done!')","b7d97fbc":"# Submission to Kaggle","6e91f29c":"# Modelling with XGBoost"}}