{"cell_type":{"9cda4b23":"code","0125b758":"code","7ea8b7be":"code","6900ec4f":"code","13606eb0":"code","386706dc":"code","128dff0d":"code","6e2bab11":"code","ebbb3ff2":"code","683624ff":"code","f56395a0":"code","51a5d64a":"code","671cb972":"code","702515ac":"code","382b6e8b":"code","ba13e639":"code","c13f71be":"code","3215eec2":"code","ba7c06b5":"code","54e65c8e":"code","3ba97c36":"code","aeca03f0":"code","7a6120cc":"code","c5eeb482":"code","e0c294c9":"code","94488110":"code","937bcf27":"code","d1fbad27":"code","edd8f3ae":"code","bcd261e0":"code","b4b3f39f":"code","89a71d88":"code","84c10af9":"code","040bb9d3":"code","077b66e1":"code","56747ce9":"code","afcb9241":"code","737a298f":"code","a7400bc3":"code","013e2581":"code","4acc95a7":"code","d1073e04":"code","e5e1a04a":"code","e6193a51":"code","3a089fa4":"code","32773883":"code","864d4733":"code","36fa7689":"code","c62c7c1d":"code","477b79b8":"code","8798dc86":"code","8c3cf309":"code","74f9620d":"code","c1c8b784":"code","a44083c7":"code","1eb2f60a":"code","373835d3":"code","b2f6cf87":"code","00845452":"code","1c9a227b":"code","74624d3f":"code","9321d122":"code","c739f582":"code","44a1d75e":"code","92437f8b":"code","904ce2e6":"code","e3119287":"code","8f51c328":"code","d37c54d7":"code","3a0f2def":"code","1f2cf2f8":"code","ca044301":"code","f5b05983":"code","183b8aa6":"code","24860504":"code","0659822d":"code","5458193c":"code","0484e435":"code","8f0ff1d0":"code","da1cc629":"code","d172479e":"code","ba81f815":"code","77b87bbc":"code","86442069":"code","24c0d1c3":"code","8b824c01":"code","09042bbd":"code","b0aa9f1e":"code","560b96b9":"code","3a5847f3":"code","c586b10b":"code","1f1273d6":"code","40651e5d":"code","2889b893":"code","d12b389b":"code","3744635a":"code","f1bb3366":"code","d60d4894":"code","196a95e1":"code","246c4c31":"code","3bb4c55f":"code","17a41462":"code","d3dd249b":"code","e5cdbff4":"code","09718a4c":"code","3cbfb1cb":"code","e58f05cf":"code","00b76c10":"code","d22dfd86":"code","5ccaef81":"code","f54a4817":"code","f844310d":"code","c771ff1e":"code","bbfc579f":"code","aa346637":"code","2adc2881":"code","f8f5f7e9":"code","50579d79":"code","323754de":"code","b8ea8ba5":"code","9ee9125c":"code","efb890e0":"code","817ca0cf":"code","17e53cfd":"code","a5c3f387":"code","bb3fc696":"code","053bcf52":"code","c267250f":"code","2c5eb37f":"code","1d922334":"code","60ab2af4":"code","bfe7368f":"code","6740228f":"code","051a1afe":"code","9c53ccba":"code","44371a13":"code","ca53f636":"code","5fa84f8f":"code","27f69223":"code","d12b6ccd":"code","56891262":"code","363b6adc":"code","26b1c5c8":"code","4c75696c":"code","5973c3c5":"code","bb57977f":"code","558745f5":"code","9457064c":"code","1d367aa3":"code","5ae27cdf":"code","5d5877ab":"code","660c5dd0":"code","06c28033":"code","5d8ad195":"code","17e3dc91":"code","78e8f578":"code","7e3128c8":"code","3247917a":"markdown","3250c401":"markdown","5b82ac2f":"markdown","37eb424a":"markdown","289a98f7":"markdown","36561002":"markdown","acd2e62a":"markdown","ca1ea154":"markdown","54498813":"markdown","d49fc3cf":"markdown","8ee31fe8":"markdown","3c4867b2":"markdown","aca1e89b":"markdown","d3b98886":"markdown","3e9f639a":"markdown","fcc63b5f":"markdown","315c4b1e":"markdown","92016213":"markdown","c6960b07":"markdown","b161aa0f":"markdown","73feb112":"markdown","3301e15a":"markdown","de037ae5":"markdown","29bc11de":"markdown","54a69bbf":"markdown","7547aa10":"markdown","7f720c47":"markdown","c848d3cc":"markdown","bb621803":"markdown","10823e25":"markdown","2e5c9fea":"markdown","e1032c97":"markdown","8646cab3":"markdown","6a8b7492":"markdown","7e34e4fa":"markdown","95194aa4":"markdown","43905172":"markdown","36d51667":"markdown","23f42208":"markdown","c8fe5261":"markdown","293da053":"markdown","fcf6d401":"markdown","ae81234f":"markdown","1d43171b":"markdown","dfe272c7":"markdown","15e844e0":"markdown"},"source":{"9cda4b23":"import numpy as np\nimport pandas as pd\n\nimport xgboost as xgb\nimport lime\nimport lime.lime_tabular\n\nfrom sklearn.model_selection import cross_val_score\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","0125b758":"#gender_submission.csv is an example prediction file predicting all female passengers survive, and no others do\n#i guess thats politically correct\n#!cat \/kaggle\/input\/titanic\/gender_submission.csv","7ea8b7be":"df_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_train.head()","6900ec4f":"df_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ndf_test.head()","13606eb0":"df_train[\"train\"] = 1\ndf_test[\"train\"] = 0\ndf_all = pd.concat([df_train, df_test], sort=False)\ndf_all.head()","386706dc":"df_all[\"Embarked\"].unique()","128dff0d":"df_all[\"Embarked\"] = df_all[\"Embarked\"].fillna(\"N\")","6e2bab11":"def parse_cabin_type(x):\n    if pd.isnull(x):\n        return None\n    #print(\"X:\"+x[0])\n    #cabin id consists of letter+numbers. letter is the type\/deck, numbers are cabin number on deck\n    return x[0]\n\ndef parse_cabin_number(x):\n    if pd.isnull(x):\n        return -1\n#        return np.nan\n    cabs = x.split()\n    cab = cabs[0]\n    num = cab[1:]\n    if len(num) < 2:\n        return -1\n        #return np.nan\n    return num\n\ndef parse_cabin_count(x):\n    if pd.isnull(x):\n        return np.nan\n    #a typical passenger has a single cabin but some had multiple. in that case they are space separated\n    cabs = x.split()\n    return len(cabs)\n\n","ebbb3ff2":"df_all[\"cabin_type\"] = df_all[\"Cabin\"].apply(lambda x: parse_cabin_type(x))\ndf_all[\"cabin_num\"] = df_all[\"Cabin\"].apply(lambda x: parse_cabin_number(x))\ndf_all[\"cabin_count\"] = df_all[\"Cabin\"].apply(lambda x: parse_cabin_count(x))\ndf_all[\"cabin_num\"] = df_all[\"cabin_num\"].astype(int)\ndf_all.head()","683624ff":"df_all[\"family_size\"] = df_all[\"SibSp\"] + df_all[\"Parch\"] + 1\ndf_all.head()","f56395a0":"# Cleaning name and extracting Title\nfor name_string in df_all['Name']:\n    df_all['Title'] = df_all['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\ndf_all.head()","51a5d64a":"# Replacing rare titles \nmapping = {'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs', 'Major': 'Other', \n           'Col': 'Other', 'Dr' : 'Other', 'Rev' : 'Other', 'Capt': 'Other', \n           'Jonkheer': 'Royal', 'Sir': 'Royal', 'Lady': 'Royal', \n           'Don': 'Royal', 'Countess': 'Royal', 'Dona': 'Royal'}\n           \ndf_all.replace({'Title': mapping}, inplace=True)\n#titles = ['Miss', 'Mr', 'Mrs', 'Royal', 'Other', 'Master']","671cb972":"titles = df_all[\"Title\"].unique()\ntitles","702515ac":"titles = list(titles)\n# Replacing missing age by median age for title \nfor title in titles:\n    age_to_impute = df_all.groupby('Title')['Age'].median()[titles.index(title)]\n    df_all.loc[(df_all['Age'].isnull()) & (df_all['Title'] == title), 'Age'] = age_to_impute","382b6e8b":"df_all[df_all[\"Fare\"].isnull()]","ba13e639":"df_all.loc[152]","c13f71be":"p3_median_fare = df_all[df_all[\"Pclass\"] == 3][\"Fare\"].median()\np3_median_fare","3215eec2":"df_all[\"Fare\"].fillna(p3_median_fare, inplace=True)","ba7c06b5":"df_all.loc[152]","54e65c8e":"df_all = df_all.drop([\"Name\", \"Ticket\", \"Cabin\", \"PassengerId\"], axis=1)","3ba97c36":"df_all[\"cabin_type\"].value_counts()","aeca03f0":"df_all[\"cabin_type\"] = df_all[\"cabin_type\"].fillna(\"Z\")","7a6120cc":"df_all[\"cabin_type\"].value_counts()","c5eeb482":"label_encode_cols = [\"Sex\", \"Embarked\", \"Title\", \"cabin_type\"]","e0c294c9":"from sklearn.preprocessing import LabelEncoder\nlabel_encoders = {}\nfor col in label_encode_cols:\n    le = LabelEncoder()\n    label_encoders[col] = le\n    df_all[col] = le.fit_transform(df_all[col])\ndf_all.head()","94488110":"cat_cols = label_encode_cols","937bcf27":"for col in cat_cols:\n    df_all[col] = df_all[col].astype('category')","d1fbad27":"df_all.isnull().sum()","edd8f3ae":"df_all[\"cabin_count\"] = df_all[\"cabin_count\"].fillna(1)","bcd261e0":"df_all_oh = pd.get_dummies( df_all, columns = cat_cols )\ndf_all_oh.head()","b4b3f39f":"df_all_oh.columns","89a71d88":"df_train = df_all[df_all[\"train\"] == 1]\ndf_test = df_all[df_all[\"train\"] == 0]","84c10af9":"df_train_oh = df_all_oh[df_all_oh[\"train\"] == 1]\ndf_test_oh = df_all_oh[df_all_oh[\"train\"] == 0]","040bb9d3":"df_train = df_train.drop(\"train\", axis=1)\ndf_test = df_test.drop(\"train\", axis=1)\ndf_train.head()\n","077b66e1":"df_train_oh = df_train_oh.drop(\"train\", axis=1)\ndf_test_oh = df_test_oh.drop(\"train\", axis=1)\ndf_train_oh.head()\n","56747ce9":"target = df_train[\"Survived\"]\ntarget.head()","afcb9241":"df_train = df_train.drop(\"Survived\", axis=1)\ndf_test = df_test.drop(\"Survived\", axis=1)","737a298f":"df_train.head()","a7400bc3":"df_train_oh = df_train_oh.drop(\"Survived\", axis=1)\ndf_test_oh = df_test_oh.drop(\"Survived\", axis=1)","013e2581":"df_train_oh.head()","4acc95a7":"import lightgbm as lgb\n\nl_clf = lgb.LGBMClassifier(\n                        num_leaves=1024,\n                        learning_rate=0.01,\n                        n_estimators=5000,\n                        boosting_type=\"gbdt\",\n                        min_child_samples = 100,\n                        verbosity = 0)","d1073e04":"x_clf = xgb.XGBClassifier()","e5e1a04a":"import catboost\n\nc_clf = catboost.CatBoostClassifier()","e6193a51":"from sklearn.model_selection import train_test_split\n\nX = df_train\ny = target\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)\n\nX_oh = df_train_oh\nX_train_oh, X_val_oh = train_test_split(X_oh, test_size=0.33, random_state=42)","3a089fa4":"df_train.dtypes","32773883":"df_train_oh.dtypes","864d4733":"#the if True parts are just to make it simpler to disable some algorithm.\n\nif True:\n    l_clf.fit(\n        X_train, y_train,\n        eval_set=[(X_val, y_val)],\n        eval_metric='mae',\n        early_stopping_rounds=5,\n        verbose=False\n    )\n    \nif True:\n    c_clf.fit(\n        X_train, y_train,\n        eval_set=[(X_val, y_val)],\n        early_stopping_rounds=5,\n        cat_features=cat_cols,\n        verbose=False\n    )\n\nif True:\n    x_clf.fit(\n        X_train_oh, y_train,\n        eval_set=[(X_val_oh, y_val)],\n        early_stopping_rounds=5,\n        verbose=False\n    )","36fa7689":"import matplotlib.pyplot as plt\n\ndef plot_feat_importance(clf, train):\n    if hasattr(clf, 'feature_importances_'):\n        importances = clf.feature_importances_\n        features = train.columns\n\n        feat_importances = pd.DataFrame()\n        feat_importances[\"weight\"] = importances\n        feat_importances.index = features\n        feat_importances.sort_values(by=\"weight\", ascending=False).to_csv(f\"top_features.csv\")\n        feat_importances.nlargest(30, [\"weight\"]).sort_values(by=\"weight\").plot(kind='barh', title=f\"top features\", color='#86bf91', figsize=(10, 8))\n        # kaggle shows output image files (like this png) under \"output visualizations\", others (such as pdf) under \"output\"\n        plt.savefig(f'feature-weights.png')\n        plt.savefig(f'feature-weights.pdf')\n        plt.show()\n","c62c7c1d":"\ndef plot_pimp(pimps, train):\n    importances = pimps.importances_mean\n    features = train.columns\n\n    feat_importances = pd.DataFrame()\n    feat_importances[\"weight\"] = importances\n    feat_importances.index = features\n    feat_importances.sort_values(by=\"weight\", ascending=False).to_csv(f\"top_features.csv\")\n    feat_importances.nlargest(30, [\"weight\"]).sort_values(by=\"weight\").plot(kind='barh', title=f\"top features\", color='#86bf91', figsize=(10, 8))\n    # kaggle shows output image files (like this png) under \"output visualizations\", others (such as pdf) under \"output\"\n    plt.savefig(f'feature-weights.png')\n    plt.savefig(f'feature-weights.pdf')\n    plt.show()\n","477b79b8":"plot_feat_importance(l_clf, X_train)","8798dc86":"from sklearn.inspection import permutation_importance\n\nl_pimps = permutation_importance(l_clf, X_train, y_train, n_repeats=10, random_state=0)\ndir(l_pimps)","8c3cf309":"plot_pimp(l_pimps, X_train)","74f9620d":"plot_feat_importance(c_clf, X_train)","c1c8b784":"c_pimps = permutation_importance(c_clf, X_train, y_train, n_repeats=10, random_state=0)\nplot_pimp(c_pimps, X_train)","a44083c7":"c_pimps.importances_mean","1eb2f60a":"X_train.columns[np.argmin(c_pimps.importances_mean)]","373835d3":"plot_feat_importance(x_clf, X_train_oh)","b2f6cf87":"x_pimps = permutation_importance(x_clf, X_train_oh, y_train, n_repeats=10, random_state=0)\nplot_pimp(x_pimps, X_train_oh)","00845452":"from sklearn.metrics import accuracy_score, log_loss\n\nval_pred_proba = l_clf.predict_proba(X_val)\n#val_pred = np.array(val_pred[:, 1] > 0.5)\nval_pred = np.where(val_pred_proba > 0.5, 1, 0)\n\nacc_score = accuracy_score(y_val, val_pred[:,1])\nacc_score","1c9a227b":"val_pred_proba = c_clf.predict_proba(X_val)\n#val_pred = np.array(val_pred[:, 1] > 0.5)\nval_pred = np.where(val_pred_proba > 0.5, 1, 0)\n\nacc_score = accuracy_score(y_val, val_pred[:,1])\nacc_score","74624d3f":"val_pred_proba = x_clf.predict_proba(X_val_oh)\n#val_pred = np.array(val_pred[:, 1] > 0.5)\nval_pred = np.where(val_pred_proba > 0.5, 1, 0)\n\nacc_score = accuracy_score(y_val, val_pred[:,1])\nacc_score","9321d122":"feature_names = list(df_train.columns)\nfeature_names","c739f582":"#cat_cols was set up earliner in the notebook to contain list of categorical feature\/column names\ncat_cols","44a1d75e":"#the corresponding indices of the cat_cols columns in the list of features\ncat_indices = [feature_names.index(col) for col in cat_cols]\ncat_indices","92437f8b":"#mapping the category values to their names. for example, {\"sex\"={0=\"female\", 1=\"male\"}}\ncat_names = {}\nfor label_idx in cat_indices:\n    label = feature_names[label_idx]\n    print(label)\n    le = label_encoders[label]\n    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n    le_value_mapping = dict(zip(le.transform(le.classes_), le.classes_))\n    print(le_value_mapping)\n    cat_names[label_idx] = le_value_mapping\n","904ce2e6":"cat_names #its actually the feature index mapped to the values and their names","e3119287":"explainer = lime.lime_tabular.LimeTabularExplainer(df_train.values, discretize_continuous=True,\n                                                   class_names=['not survived', 'survived'], \n                                                   mode=\"classification\",\n                                                   feature_names = feature_names,\n                                                   categorical_features=cat_indices,\n                                                   categorical_names=cat_names, \n                                                   kernel_width=10, verbose=True)","8f51c328":"#def row_to_df(row):\n#    rows = []\n#    rows.append(X_val.values[0])\n#    df = pd.DataFrame(rows, columns=X_val.columns)\n#    for col in cat_cols:\n#        df[col] = df[col].astype('category')    \n#    return df\n\n#when LIME passes synthetic data to your predict function, it gives a list of N synthetic datapoints as a numpy matrix\n#this converts that matrix into a dataframe, since some algorithms choke on the pure numpy array (catboost)\ndef rows_to_df(rows):\n    df = pd.DataFrame(rows, columns=X_val.columns)\n    #set category columns first to short numeric to save memory etc, then convert to categorical for catboost\n    for col in cat_cols:\n        df[col] = df[col].astype('int8')\n        df[col] = df[col].astype('category')\n    #and finally convert all non-categoricals to their original type. since we had to create a fresh dataframe this is needed\n    for col in X_val.columns:\n        if col not in cat_cols:\n            df[col] = df[col].astype(X_val[col].dtype)\n    return df\n\n#for one-hot encoding the values from LIME, which uses numbers in a single column to represent categories\n#needed for xgboost\ndef rows_to_df_oh(rows):\n    df = pd.DataFrame(rows, columns=X_val_oh.columns)\n    for col in cat_cols:\n        df[col] = df[col].astype('int8')\n    return df\n\n#the function to pass to LIME for running catboost on the synthetic data\ndef c_run_pred(x):\n    p = c_clf.predict_proba(rows_to_df(x))\n    return p\n\n#the function to pass to LIME to run LGBM on the synthetic data\ndef l_run_pred(x):\n    p = l_clf.predict_proba(x)\n    return p\n\n#the function to pass to LIME to run XGBoost on the synthetic data\ndef x_run_pred(x):\n    df = rows_to_df(x)\n    df = pd.get_dummies( df, columns = cat_cols )\n\n    new_df = pd.DataFrame()\n    #this look ensure the column order of the dataframe created is same as the original\n    for col in X_val_oh.columns:\n        if col in df.columns:\n            new_df[col] = df[col]\n        else:\n            #sometimes it seems to happen that a specific value is missing from a category in generation,\n            #which leads to missing that column. this zeroes it to ensure it exists\n            #print(f\"missed col:{col}\")\n            new_df[col] = 0\n    df = new_df\n\n    p = x_clf.predict_proba(df)\n    return p\n\nc_predict_fn = lambda x: c_run_pred(x)\n\nl_predict_fn = lambda x: l_run_pred(x)\n\nx_predict_fn = lambda x: x_run_pred(x)\n","d37c54d7":"l_clf.predict_proba([X_val.values[0]])\n","3a0f2def":"c_predict_fn(X_val.values)[0]","1f2cf2f8":"x_predict_fn(X_val.values)[0]","ca044301":"\nx_clf.predict_proba(X_val_oh)[0]","f5b05983":"#this demonstrates the missing value branch of x_run_pred() with cabin_type=7\ndf = rows_to_df(X_val.values)\nprint(df.shape)\nprint(f\"cat cols: {cat_cols}\")\ndf = pd.get_dummies( df, columns = cat_cols )\nmissing_cols = set( X_val_oh.columns ) - set( df.columns )\nprint(f\"missing: {missing_cols}\")\n# Add a missing column in test set with default value equal to 0\nnew_df = pd.DataFrame()\nfor col in X_val_oh.columns:\n    if col in df.columns:\n        new_df[col] = df[col]\n    else:\n        new_df[col] = 0\ndf = new_df","183b8aa6":"#p = x_clf.predict_proba(df)\n#p","24860504":"#x_predict_fn(X_val.values)","0659822d":"def explain_item(predictor, item):\n    exp = explainer.explain_instance(item, predictor, num_features=10, top_labels=1)\n    exp.show_in_notebook(show_table=True, show_all=False)\n","5458193c":"#this allows running the experiments N times to see if the random synthetic value generation of LIME has some effect on the results over different runs.\n\ndef explain_x_times(x, idx, invert_gender=False):\n    row = X_val.values[idx]\n    if invert_gender:\n        if row[1] > 0:\n            row[1] = 0\n        else:\n            row[1] = 1\n    print(f\"columns={X_val.columns}\")\n    \n    for i in range(x):\n        print(f\"Explaining LGBM: index={idx}, row={row}\")\n        explain_item(l_predict_fn, row)\n    for i in range(x):\n        print(f\"Explaining CatBoost: index={idx}, row={row}\")\n        explain_item(c_predict_fn, row)\n    for i in range(x):\n        print(f\"Explaining XGBoost: index={idx}, row={row}\")\n        explain_item(x_predict_fn, row)\n","0484e435":"explain_x_times(2, 0)","8f0ff1d0":"explain_x_times(2, 0, invert_gender=True)","da1cc629":"explain_x_times(2, 1, invert_gender=False)","d172479e":"explain_x_times(2, 1, invert_gender=True)","ba81f815":"df_train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","77b87bbc":"df_train.head()","86442069":"for col in ('Alley','Utilities','MasVnrType','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',\n            'BsmtFinType2','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond',\n           'PoolQC','Fence','MiscFeature'):\n    df_train[col]=df_train[col].fillna('None')\n    df_test[col]=df_test[col].fillna('None')\n\nfor col in ('Electrical','MSZoning','Exterior1st','Exterior2nd','KitchenQual','SaleType','Functional'):\n    df_train[col]=df_train[col].fillna(df_train[col].mode()[0])\n    df_test[col]=df_test[col].fillna(df_train[col].mode()[0])\n\nfor col in ('MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath',\n            'GarageYrBlt','GarageCars','GarageArea'):\n    df_train[col]=df_train[col].fillna(0)\n    df_test[col]=df_test[col].fillna(0)\n\ndf_train['LotFrontage']=df_train['LotFrontage'].fillna(df_train['LotFrontage'].mean())\ndf_test['LotFrontage']=df_test['LotFrontage'].fillna(df_train['LotFrontage'].mean())","24c0d1c3":"df_train.dtypes","8b824c01":"#removing outliers recomended by author\ndf_train = df_train[df_train['GrLivArea']<4000]","09042bbd":"len_traindf = df_train.shape[0]\nhouses = pd.concat([df_train, df_test], sort=False)\nhouses = houses.fillna(0)\n","b0aa9f1e":"\n# turning some ordered categorical variables into ordered numerical\n# maybe this information about order can help on performance\nfor col in [\"ExterQual\", \"ExterCond\", \"BsmtQual\", \"BsmtCond\", \"HeatingQC\", \"KitchenQual\",\n            \"FireplaceQu\",\"GarageQual\",\"GarageCond\",\"PoolQC\"]:\n    houses[col]= houses[col].map({\"Gd\": 4 , \"TA\": 3, \"Ex\": 5, \"Fa\":2, \"Po\":1})\nhouses = houses.fillna(0)","560b96b9":"import numbers\n\ncdf = df_train.select_dtypes(include=np.number)\ncat_names = [key for key in df_train.columns if key not in cdf.columns]\ncat_names","3a5847f3":"len_traindf = df_train.shape[0]\n\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoders = {}\nfor col in cat_names:\n    le = LabelEncoder()\n    label_encoders[col] = le\n    houses[col] = le.fit_transform(houses[col])\n","c586b10b":"\ndf_train = houses[:len_traindf]\ndf_train = df_train.drop('SalePrice', axis=1)\ndf_test = houses[len_traindf:]\ndf_test = df_test.drop('SalePrice', axis=1)\n\n# turning categoric into numeric\nhouses_oh = pd.get_dummies(houses)\n\n# separating\ndf_train_oh = houses_oh[:len_traindf]\ndf_test_oh = houses_oh[len_traindf:]","1f1273d6":"# x\/y split\nX_train_oh = df_train_oh.drop('SalePrice', axis=1)\ny_train = df_train_oh['SalePrice']\nX_test_oh = df_test_oh.drop('SalePrice', axis=1)","40651e5d":"from hyperopt import hp, tpe, fmin\n\nspace = {'n_estimators':hp.quniform('n_estimators', 1000, 4000, 100),\n         'gamma':hp.uniform('gamma', 0.01, 0.05),\n         'learning_rate':hp.uniform('learning_rate', 0.00001, 0.03),\n         'max_depth':hp.quniform('max_depth', 3,7,1),\n         'subsample':hp.uniform('subsample', 0.60, 0.95),\n         'colsample_bytree':hp.uniform('colsample_bytree', 0.60, 0.95),\n         'colsample_bylevel':hp.uniform('colsample_bylevel', 0.60, 0.95),\n         'reg_lambda': hp.uniform('reg_lambda', 1, 20)\n        }\n\ndef objective(params):\n    params = {'n_estimators': int(params['n_estimators']),\n             'gamma': params['gamma'],\n             'learning_rate': params['learning_rate'],\n             'max_depth': int(params['max_depth']),\n             'subsample': params['subsample'],\n             'colsample_bytree': params['colsample_bytree'],\n             'colsample_bylevel': params['colsample_bylevel'],\n             'reg_lambda': params['reg_lambda']}\n    \n    xb_a = xgb.XGBRegressor(**params)\n    score = cross_val_score(xb_a, X_train_oh, y_train, scoring='neg_mean_squared_error', cv=5, n_jobs=-1).mean()\n    return -score","2889b893":"best = fmin(fn= objective, space= space, max_evals=4, rstate=np.random.RandomState(1), algo=tpe.suggest)\n#max_evals=20","d12b389b":"X_clf = xgb.XGBRegressor(random_state=0,\n                        n_estimators=int(best['n_estimators']), \n                        colsample_bytree= best['colsample_bytree'],\n                        gamma= best['gamma'],\n                        learning_rate= best['learning_rate'],\n                        max_depth= int(best['max_depth']),\n                        subsample= best['subsample'],\n                        colsample_bylevel= best['colsample_bylevel'],\n                        reg_lambda= best['reg_lambda']\n                       )\n\nX_clf.fit(X_train_oh, y_train)","3744635a":"all_cols = list(df_train.columns)\ncat_indices = []\nfor cat_name in cat_names:\n    cat_indices.append(all_cols.index(cat_name))\nprint(cat_indices)","f1bb3366":"houses.head()\n","d60d4894":"feature_names = list(df_train.columns)\nprint(feature_names)","196a95e1":"cat_names = {}\nfor label_idx in cat_indices:\n    label = feature_names[label_idx]\n    print(label)\n    le = label_encoders[label]\n    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n    le_value_mapping = dict(zip(le.transform(le.classes_), le.classes_))\n    print(le_value_mapping)\n    cat_names[label_idx] = le_value_mapping","246c4c31":"feature_names_oh = list(df_train_oh.columns)","3bb4c55f":"explainer = lime.lime_tabular.LimeTabularExplainer(df_train.values, \n                                                   feature_names=feature_names, \n                                                   class_names=['price'], \n                                                   categorical_features=cat_indices,\n                                                   categorical_names=cat_names,\n                                                   verbose=True, \n                                                   discretize_continuous=False,\n                                                   mode='regression')\n","17a41462":"def explain_xreg(row):\n    df = pd.DataFrame(data=row, columns=df_train.columns)\n    row = pd.get_dummies(df)\n    return X_clf.predict(row)","d3dd249b":"def explain_item(item):\n    exp = explainer.explain_instance(item, explain_xreg, num_features=10, top_labels=1)\n    exp.show_in_notebook(show_table=True, show_all=False)\n    return exp","e5cdbff4":"df_test.iloc[0]","09718a4c":"df_test.iloc[0].values.shape","3cbfb1cb":"explain_xreg([df_test.iloc[0].values])","e58f05cf":"exp = explain_item(df_test.iloc[0])\n","00b76c10":"df_test.head()","d22dfd86":"df_test.describe()","5ccaef81":"top_features = exp.as_list()\ntop_features","f54a4817":"for feat, weight in top_features:\n    print(feat)","f844310d":"%matplotlib inline\ntop_names = [tup[0].split(\"=\")[0] for tup in top_features]\ndf_test[top_names].hist(figsize=(15,10))","c771ff1e":"df_test[top_names].head(1)","bbfc579f":"y_train.describe()","aa346637":"df_test.iloc[0][\"KitchenQual\"]","2adc2881":"row = df_test.iloc[0]\nrow[\"KitchenQual\"] = 2","f8f5f7e9":"row[\"KitchenQual\"]","50579d79":"exp = explain_item(row)","323754de":"!ls ..\/input","b8ea8ba5":"# read the csv\ncleveland = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')","9ee9125c":"# remove missing data (indicated with a \"?\")\ndata = cleveland[~cleveland.isin(['?'])]","efb890e0":"#drop nans\ndata = data.dropna(axis=0)","817ca0cf":"data = data.apply(pd.to_numeric)\ndata.dtypes","17e53cfd":"X = np.array(data.drop(['target'], 1))\ny = np.array(data['target'])","a5c3f387":"from sklearn import model_selection\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, stratify=y, random_state=42, test_size = 0.2)","bb3fc696":"# convert the data to categorical labels\nfrom keras.utils.np_utils import to_categorical\n\nY_train = to_categorical(y_train, num_classes=None)\nY_test = to_categorical(y_test, num_classes=None)\nprint (Y_train.shape)\nprint (Y_train[:10])","053bcf52":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom keras.layers import Dropout\nfrom keras import regularizers\n\n# define a function to build the keras model\ndef create_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(16, input_dim=13, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n    model.add(Dropout(0.25))\n    model.add(Dense(8, kernel_initializer='normal', kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n    model.add(Dropout(0.25))\n    model.add(Dense(2, activation='softmax'))\n    \n    # compile model\n    adam = Adam(lr=0.001)\n    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n    return model\n\nmodel = create_model()\n\nprint(model.summary())","c267250f":"# fit the model to the training data\n#verbose=1 for full output, verbose=2 for list of epochs. 0 for quiet\nhistory=model.fit(X_train, Y_train, validation_data=(X_test, Y_test),epochs=50, batch_size=10, verbose=0)","2c5eb37f":"#to see training results, exact accuracy and loss\n#history.history","1d922334":"import matplotlib.pyplot as plt\n%matplotlib inline\n# Model accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()","60ab2af4":"# Model Losss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()","bfe7368f":"# convert into binary classification problem - heart disease or no heart disease\nY_train_binary = y_train.copy()\nY_test_binary = y_test.copy()\n\nY_train_binary[Y_train_binary > 0] = 1\nY_test_binary[Y_test_binary > 0] = 1\n\nprint(Y_train_binary[:20])","6740228f":"# define a new keras model for binary classification\ndef create_binary_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(16, input_dim=13, kernel_initializer='normal',  kernel_regularizer=regularizers.l2(0.001),activation='relu'))\n    model.add(Dropout(0.25))\n    model.add(Dense(8, kernel_initializer='normal',  kernel_regularizer=regularizers.l2(0.001),activation='relu'))\n    model.add(Dropout(0.25))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    # Compile model\n    adam = Adam(lr=0.001)\n    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n    return model\n\nbinary_model = create_binary_model()\n\nprint(binary_model.summary())","051a1afe":"from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n\nbinary_model = KerasClassifier(build_fn=create_binary_model, epochs=50, batch_size=10, verbose=0)\n","9c53ccba":"# fit the binary model on the training data\nhistory=binary_model.fit(X_train, Y_train_binary, validation_data=(X_test, Y_test_binary), epochs=50, batch_size=10, verbose=0)","44371a13":"import matplotlib.pyplot as plt\n%matplotlib inline\n# Model accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()","ca53f636":"# Model Losss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()","5fa84f8f":"\ndef plot_pimp_2(pimps, features):\n    importances = pimps.importances_mean\n\n    feat_importances = pd.DataFrame()\n    feat_importances[\"weight\"] = importances\n    feat_importances.index = features\n    feat_importances.sort_values(by=\"weight\", ascending=False).to_csv(f\"top_features.csv\")\n    feat_importances.nlargest(30, [\"weight\"]).sort_values(by=\"weight\").plot(kind='barh', title=f\"top features\", color='#86bf91', figsize=(10, 8))\n    # kaggle shows output image files (like this png) under \"output visualizations\", others (such as pdf) under \"output\"\n    plt.savefig(f'feature-weights.png')\n    plt.savefig(f'feature-weights.pdf')\n    plt.show()\n","27f69223":"cleveland.shape","d12b6ccd":"df_X = data.drop(['target'], 1)\nk_pimps = permutation_importance(binary_model, X_train, y_train, n_repeats=10, random_state=0)\nplot_pimp_2(k_pimps, df_X.columns)","56891262":"# generate classification report using predictions for categorical model\nfrom sklearn.metrics import classification_report, accuracy_score\n\ncategorical_pred = np.argmax(model.predict(X_test), axis=1)\n\nprint('Results for Categorical Model')\nprint(accuracy_score(y_test, categorical_pred))\nprint(classification_report(y_test, categorical_pred))","363b6adc":"#model.predict_proba(X_test)\n","26b1c5c8":"feature_names = cleveland.columns","4c75696c":"data.nunique()","5973c3c5":"#data[\"thalach\"].describe()","bb57977f":"cat_cols = [\"sex\", \"cp\", \"fbs\", \"restecg\", \"exang\", \"ca\", \"thal\"]\nfor col in cat_cols:\n    print(f\"{col}: {data[col].unique()}\")","558745f5":"feature_names = list(feature_names)\ncat_indices = [feature_names.index(col) for col in cat_cols]\ncat_indices","9457064c":"#explainer = lime.lime_tabular.LimeTabularExplainer(df_train, feature_names=iris.feature_names, class_names=iris.target_names, discretize_continuous=True)\nexplainer = lime.lime_tabular.LimeTabularExplainer(X_train, discretize_continuous=True,\n                                                   class_names=['no risk', 'risk of heart'], \n                                                   mode=\"classification\",\n                                                   feature_names = feature_names,\n                                                   categorical_features=cat_indices,\n                                                   categorical_names=[], \n                                                   kernel_width=10, verbose=True)\n","1d367aa3":"def explain_item(item):\n\n    #    exp = explainer.explain_instance(item, l_predict_fn, num_features=10, top_labels=1)\n    exp = explainer.explain_instance(item, model.predict_proba, num_features=10, top_labels=1)\n#    exp = explainer.explain_instance(item, l_clf.predict_proba, num_features=10, top_labels=1)\n    exp.show_in_notebook(show_table=True, show_all=False)","5ae27cdf":"def explain_item_flipped(row, flip=False):\n    if flip:\n        if row[2] > 0:\n            row[2] = 0\n        else:\n            row[2] = 1\n    print(f\"columns={X_val.columns}\")\n    #    exp = explainer.explain_instance(item, l_predict_fn, num_features=10, top_labels=1)\n    exp = explainer.explain_instance(row, model.predict_proba, num_features=10, top_labels=1)\n#    exp = explainer.explain_instance(item, l_clf.predict_proba, num_features=10, top_labels=1)\n    exp.show_in_notebook(show_table=True, show_all=False)\n","5d5877ab":"cat_cols","660c5dd0":"cat_indices","06c28033":"explain_item(X_test[0])","5d8ad195":"explain_item_flipped(X_test[0], False)","17e3dc91":"explain_item_flipped(X_test[1], False)","78e8f578":"explain_item_flipped(X_test[0], True)","7e3128c8":"explain_item_flipped(X_test[1], True)","3247917a":"As with the Titanic dataset, I will just use a bunch of derived features copied from some existing Kaggle notebooks. Thanks again :)","3250c401":"# Setting up LIME parameters\n\nRunning the LIME explainer requires \n- a list of features\n- a prediction function\n- a function to translate data from LIME format to prediction algorithm format\n- list of categorical variable names\n- list of categorical variable indices\n- list of value names for categorical variables\n\nThe categorical variable data are used to define how LIME will permutate variables (categorical vs continous), and how it will display the results (variables and values have names vs numbers).","5b82ac2f":"Categorical columns and number of values \/ categories for each:","37eb424a":"## One-Hot Encode for XGBoost\n\nLGBM and CatBoost can use categorical columns as is, but XGBoost needs one-hot encoding. So the \"_oh\" ending dataframes created here are the same as the ones without the ending, but one-hot encoded.","289a98f7":"Following are some extra features I picked up earlier from various Titanic kernels here at Kaggle. Unfortunately that was a while back and I lost all the references to the original ones, but thanks anyway:","36561002":"Categorical column indices in the list of columns, needed for LIME to print the names on explanations:","acd2e62a":"## Point 1 explained\n\nI will run the explainer first once per datapoint. Starting with point 1, or index 0 in the dataset. And then modify one of the higher ranked features to see if it has some effect.","ca1ea154":"The kernel I am basing on finished with a binary model simpler analysis of output. So here we go.","54498813":"Since I am using XGBoostRegressor, I need the one-hot encoded categorical variables again, just like with the Titanic classifier above:","d49fc3cf":"Visualizing the accuracy and loss over epochs takes much less space..","8ee31fe8":"Plot permutation importance:","3c4867b2":"### Explain Point 1 with all 3 Boosters","aca1e89b":"## CatBoost predictions","d3b98886":"Interesting, permutation importance actually ranks \"Parch\" as contributing negatively to prediction accuracy.","3e9f639a":"All three seem to rank very similar features highest overall. Title, gender, passenger class, fare, ... I expect fare to be a kind of a proxy for passenger class, which likely defines something about your location on the ship, and so on.\n\nXGBoost here has much more variables that LGBM or CatBoost. Since XGBoost uses one-hot encoded variables, it has a much larger number of \"features\", which are just different values of a categorical variable. However, it also shows to rank a specific gender and title higher, so overall its is very similar.\n\nThe less weighted variables seem to have much more variation across the classifiers, but as far as I could tell including them still provides some small gains in accuracy. Just that the specific ways they are combined by the different classifiers has some variation, although results are very similar.\n\nThe difference between the lower ranked features is something that also comes up later with my LIME experiments.\n","fcc63b5f":"Now to recall the variables before we try to explain some datapoints from this predictor:","315c4b1e":"# Explaining a Tabular Model with LIME\n\n[LIME](https:\/\/github.com\/marcotcr\/lime), or Local Interpretable Model-Agnostic Explanations, is one technique that comes up a lot as I search for explaining machine learning models. \"local\" refers to trying to explain which features contribute the most to explain a prediction\/classification of a given datapoint. For example, explaining why a patient was classified with a flu, which features and their values contributed to the classification for this specific patient.\n\nHere I try the LIME Python implementation on a few datasets to see what kind of results I get. This is on tabular datasets (so CSV files). LIME can also be applied on images and text, maybe another day. Datasets:\n- Titanic: What features contribute to a specific person classified as survivor or not?\n- Heart disease UCI: What features contribute to a specific person being classified at risk of heart disease?\n- Boston housing dataset: What features contribute positively to predicted house price, and what negatively?\n\nAlgorithms applied:\n- Titanic: classifiers from LGBM, CatBoost, XGBoost\n- Heart disease UCI: Keras multi-layer perceptron NN architecture\n- Boston housing dataset: regressor from XGBoost","92016213":"## Explaining Titanic Survival Classification","c6960b07":"### Invert Gender for Point 1, Classify and Explain Again","b161aa0f":"### LGBM","73feb112":"List of all feature names for LIME:","3301e15a":"A bit of an overkill to optimize the hyperparameters with multiple runs over hyperopt, but the notebook I used as a source does it, so here we go:","de037ae5":"Because the explainer was created above using the top 10 features, the list also shows these 10 features. A brief look at each in this instance vs the overall dataset:","29bc11de":"## Explain Point 2 with All Boosters","54a69bbf":"# Explaining a Datapoint with LIME \/ Booster Classifiers","7547aa10":"## XGBoost predictions","7f720c47":"# Explaining Keras NN Classifier","c848d3cc":"## Fit all boosters","bb621803":"### Catboost","10823e25":"Above I tried to explain booster based classifiers and regressors using the scikit interfaces. Neural nets are another type of network often used, so how to fit LIME on a neural net? Here is an example of a classifier based on Keras fully connected neural network.\n\nThis one uses the UCI heart disease dataset. As usual, the base preprocessing and model are based on some existing notebooks. Thanks for your efforts, whoever it was.. My point is simply to use it to try out LIME.","2e5c9fea":"## LGBM predictions","e1032c97":"So the above shows an example of application of the explainer, but with very conflicting results. Most variables \/ features are shown to highly bias the predition towards a risk of heart-disease. Yet the prediction by the classifier is the exact opposite. So there are a few options here:\n\n- Maybe I am doing something wrong in using LIME. But honestly, whatever I check, I seem to be using it exact as all others. \n- Maybe LIME sometimes doesn't work so good? But when? How can you rely on it to explain anything if it is that random?\n- Maybe I don't know how to correctly interpret the output? Like should the weights be further weighted by something else? It does not seem to make sense if so, so I guess this is not the case..\n- Sometimes, like here, the explanations vs the predictions seem to be exact opposite of what they should be. But why would it be like that? Can one make such an error? \n\n\n\n\n\n\n","8646cab3":"## Functions to run Classifiers from LIME\n\nLIME takes a datapoint, generates N (by default N=5000) synthetic samples around it, and runs the classifier on those synthetic samples to get some estimate on the effects of features. In many classifiers, LIME uses different format for the dataframes than the actual classifier you give it, so have to write a function to convert them to classifier format. The following are functions to do that for LGBM, CatBoost, XGBoost.","6a8b7492":"### XGBoost","7e34e4fa":"As before, need the list of categorical feature names and indices as input for LIME explainer:","95194aa4":"Mapping of categorical feature names to their values and their names. Similar to the Titanic dataset above:","43905172":"## Feature Importance from Classifier\n\nPlot the overall feature importance given by the algorithms themselves:\n","36d51667":"And build a list of categorical column indices for LIME:","23f42208":"- age\n- sex\n- chest pain type (4 values)\n- resting blood pressure\n- serum cholestoral in mg\/dl\n- fasting blood sugar > 120 mg\/dl\n- resting electrocardiographic results (values 0,1,2)\n- maximum heart rate achieved\n- exercise induced angina\n- oldpeak = ST depression induced by exercise relative to rest\n- the slope of the peak exercise ST segment\n- number of major vessels (0-3) colored by flourosopy\n- thal: 3 = normal; 6 = fixed defect; 7 = reversable defect","c8fe5261":"## Create Booster Classifiers\n\nHere I create the boosting classifiers. The hyperparameters are not especially tuned since the point of this notebook is to explore LIME for feature explanations, not to optimize for fractions of accuracy.","293da053":"# Conclusions\n\nWhatever the reason, more critical evaluations on larger scale attempts at using the LIME explainers, and information on why the results sometimes seem so different would be great. \n\nThat's all for LIME for now. I think I should try SHAP next, since it seems better documented, and more refined in its usage API. But this was an interesting look. Even if the explainers with LIME are not perfect, they can give some good ideas on how to move forward with buildling such explainers, how to present the results to users (I like the weight bars for variables etc).","fcf6d401":"### Invert Gender for Point 2, Classify and Explain Again","ae81234f":"Embarked column has some null values. Filling those with some value that is not in the dataset:","1d43171b":"This will be the actual regressor used and explained:","dfe272c7":"# Regression\n\nThe above was an experiment on explaining a classification model with LIME. What about tabular data in regression models? In such case we try to predict a continous variable based on a set of features, as opposed to trying to classify someting to a specific category.\n\nThis one uses the Boston housing prices dataset.","15e844e0":"## Train-Test Splits"}}