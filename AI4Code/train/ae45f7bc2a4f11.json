{"cell_type":{"428dc206":"code","73b02778":"code","adff27fe":"code","238f76dd":"code","4b8b1ac4":"code","c56e5b68":"code","061aedba":"code","2ea2053e":"code","df2a94cb":"code","69528e1c":"code","03140518":"code","1c42a3dc":"code","d970ecbb":"code","8f71b6ad":"code","9a230f4f":"code","584a56d9":"code","b6f13b57":"code","a5715b50":"code","6465b319":"code","dc2cc3cb":"code","1b4ea889":"code","d5a46338":"code","2b6ed408":"code","0dd91b7e":"code","4e27cc4d":"code","b163e7b1":"code","d797c9e6":"code","5ad0014c":"code","1806356e":"code","72707d24":"code","25dcf111":"code","af281ede":"code","8fa1fc00":"code","ca4463e4":"code","608c41b1":"code","43393a8c":"code","dcef6d93":"code","50ce53f4":"code","223cb15d":"code","28ec4af3":"code","3d956244":"code","fee60fb5":"code","a0067192":"code","2e04da63":"code","30ef9791":"code","a2bc0178":"code","b83aa934":"code","08e9ab98":"code","79a43afb":"code","61dfe70e":"code","896ddfec":"code","98217136":"code","f3973fe0":"code","dd478c00":"code","462c8740":"code","4c5cb047":"code","611934aa":"code","f5d61f06":"code","d63051c2":"code","273c2caa":"code","1899a4d1":"code","6741b0ad":"code","7bc275f9":"code","b38bc867":"code","b1ebfa29":"code","02f8ced3":"code","60b645ea":"code","8628d500":"code","6841bf08":"markdown","c365062a":"markdown","1303a835":"markdown","bbd87594":"markdown","b90399d7":"markdown","836fce39":"markdown","7fb5df5f":"markdown","6505b918":"markdown","68b82447":"markdown","310aec00":"markdown","7abede52":"markdown","235aaa14":"markdown","70639a9c":"markdown","2be2f173":"markdown","0b92502b":"markdown","d55df130":"markdown","96134d31":"markdown","7d38d687":"markdown"},"source":{"428dc206":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n","73b02778":"!pip install pytorch_lightning","adff27fe":"import torch\nimport torch.nn as nn\nimport torchvision\nimport pytorch_lightning as pl\nimport torch.optim as optim","238f76dd":"import matplotlib.pyplot as plt\nfrom PIL import Image","4b8b1ac4":"PATH = '..\/input\/fruits\/fruits-360\/Test\/Banana\/102_100.jpg'\nbanana = Image.open(PATH)\nplt.imshow(banana)","c56e5b68":"banana_tensor = torchvision.transforms.Compose([torchvision.transforms.Resize((224,224)),torchvision.transforms.ToTensor()])(banana)\nprint(banana_tensor.size())","061aedba":"print(banana_tensor)","2ea2053e":"normalizer = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\nbanana_tensor = normalizer(banana_tensor)\nbanana_tensor.requires_grad_(True)","df2a94cb":"print(banana_tensor.requires_grad)","69528e1c":"model = torchvision.models.vgg16(pretrained=False)","03140518":"#model.to('cuda')","1c42a3dc":"#for layer in model.parameters():\n    #layer.requires_grad_(False) \nmodel.train()","d970ecbb":"print(model.classifier[6])\nprint(type(model.classifier[0].children()))\nprint('\\n\\n')\nfor x in model.classifier.children():\n    print(x)\nprint('\\n\\n')\nprint(type(model.features))\nprint('\\n')\nfor x in model.features:\n    print(x)","8f71b6ad":"print(model.state_dict().keys())\nprint(type(model.state_dict().values()))\nprint()\nprint(len(model.state_dict().values()))\nprint()\n\n#for key, value in model.state_dict().values().items():\n #   print(type(key), type(value))\n#print(type(model.state_dict().values().keys()))","9a230f4f":"layer_name = 'features.24.weight'\nprint(type(model.state_dict()[layer_name]))\nprint(model.state_dict()[layer_name].size())\n","584a56d9":"print(model)","b6f13b57":"a = model.features(banana_tensor.unsqueeze(dim=0))\nprint(a.size())\nb = model.avgpool(a)\nprint(b.view(b.size(0),-1).size())\nprint(model.classifier(b.view(b.size(0),-1)).size())","a5715b50":"#SANDBOX\nt = torch.rand(1000)\nprint(t.argmax(dim=0))\nprint(t)\n","6465b319":"\nmodel.train()","dc2cc3cb":"model.classifier[-1].weight","1b4ea889":"print(model.features[0].__class__)","d5a46338":"'''\nA printable method?, but __print__ function can be defined only for classes?\n'''\ntype(model.classifier[0].weight.retain_grad)\n\nprint(model.classifier[0].weight.retain_grad)","2b6ed408":"print(model.classifier[3].weight.grad)\nderivative = model.classifier[3].bias.grad\nprint(derivative)","0dd91b7e":"sample_noise = torch.randn(10,1000)\ntorch.argmax(sample_noise,1)\nsample_noise_2 = torch.randn(10,1000)\nsample_noise_2 = torch.Tensor(sample_noise)\nprint(sample_noise_2.data)\nprint(sample_noise_2 == sample_noise) #This should give all True as we just assigned the values of sample_data to sample_data_2\nfor i in range(sample_noise.size(0)):\n    sample_noise[i] = torch.randn(1000) #Overwriting sample_data\nprint(sample_noise)\nprint(sample_noise_2)\nprint(sample_noise_2 == sample_noise) #This still gives true, that means, sample_data and sample_data_2 now point to the same memory, even though I used .clone() &.detach()","4e27cc4d":"'''\nDAY 1:\n\n\nPROBLEM: \n---> The gradients do not retain in .grad attribute even though retain_grad is set to True for all parameters of model\n---> There are all zero values in .grad attributes for some reason\n---> There are random values in .retain_grad attribute (same name as retain_grad() method), could this be what I'm looking for?\nlook into '.register_hook' method for Tensors\n\n\nDAY 2:\n\nThe model.parameters() and model.state_dict().values() are supposed to point to the same Tensors, however, setting .requires_grad attribute of model.parameters() does not \naffect the .requires_grad of the model.state_dict().values() Tensors\n\nThe pretrained model's parameters have .grad_fn attribute set to 'None', no matter what I do (foward passes through the whole model, \nbackward pass in the loss(which yields 0 and NaN of course, as there's no grad_fn))\n\n\nDAY 3:\n\nSuppose we have a 'nn.Parameter' instance in the model's '__init__' function, let's call it 'imp_tensor', \nIn the forward pass function of the model, we should use 'imp_tensor.data' as the input to layers (like nn.Conv2d)\n\nSee 'Trying to copy the value of one tensor to another, without causing them to point to the same memory!!!' section above, this is gonna be problematic with the 2nd loop of\nMAIN section (below), as 'cnn_model.inp' might get overridden or override the input 'batch' attribute every forward pass\n\n'''","b163e7b1":"#for tensor in model.parameters():\n    #tensor.requires_grad_(True)\n   # tensor.requires_grad = True\nfor tensor in model.parameters():\n    print(tensor.requires_grad)\n#model(zero_image)  #a forward pass hoping to update the state_dict","d797c9e6":"for layer,tensor in zip(model.state_dict().keys(),model.state_dict().values()):\n    #WTF, why does it show 'False' for the 'classifier'part, I just set them all to True 3 lines above\n     print(layer, tensor.requires_grad, tensor.grad_fn)","5ad0014c":"c =0 \nfor x in model.parameters():\n    c+=1\nprint(c)\nprint(len(model.state_dict().keys()))","1806356e":"hparams = {\n    'device': torch.device('cuda:0'),\n    'batch_size': 10,\n    'image_size_x' : 96,   #size of original images which were reshaped to 3x224x224 to fit the pretrained model\n    'image_size_y' : 96\n}","72707d24":"torch.set_default_tensor_type('torch.cuda.FloatTensor')","25dcf111":"model = model.to(hparams['device'])","af281ede":"class cnn_model_class(nn.Module):\n    def __init__(self,model,hparams):\n        #self.features = model.features\n        super().__init__()\n        self.hparams = hparams\n        self.inp = nn.Parameter(torch.empty((self.hparams['batch_size'],3,224,224),requires_grad = True)).to(hparams['device'])   \n        #this is only for passing the 'noise' of size 'batch_size'\n        #for passing the single image to reconstructed, directly use 'model(banana_tensor)'\n        self.cnn = model.features\n        self.avgpool = model.avgpool\n        self.fcc_1 = nn.Parameter(torch.empty((self.hparams['batch_size'],25088),requires_grad=True)).to(hparams['device']) \n        \n    def forward(self,batch):\n        self.inp.data = batch.detach()\n        self.fcc_1.data = self.avgpool(self.cnn(self.inp.data)).view(-1,25088)\n        #final = self.classifier(self.inp.data)\n        return self.fcc_1.data\n    def forward(self):   #FUNCTION OVERLOADING IN PYTHON?\n        self.fcc_1.data = self.avgpool(self.cnn(self.inp.data)).view(-1,25088)\n        #final = self.classifier(self.inp.data)\n        return self.fcc_1.data","8fa1fc00":"class classifier_model_class(nn.Module):\n    def __init__(self,model,hparams):\n        super().__init__()\n        self.hparams = hparams\n        self.classifier = model.classifier\n    def forward(self,batch):\n        out = self.classifier(batch)\n        #final = self.classifier(self.inp.data)\n        return out","ca4463e4":"class model_with_fcc_1_param(nn.Module):\n    def __init__(self,cnn_model):\n        super().__init__()\n        self.param = cnn_model.fcc_1\n    def forward(self,batch):\n        return self.param.data","608c41b1":"class model_with_inp_param(nn.Module):\n    def __init__(self,cnn_model):\n        super().__init__()\n        self.param = cnn_model.inp\n    def forward(self,batch):\n        return self.param.data","43393a8c":"#def fcc_layer(out):  #creating iterable form of the first fcc layer's output so that it can be passed to optimizer \n    #yield out","dcef6d93":"model = model.to(hparams['device'])\ncnn_model = cnn_model_class(model,hparams).to(hparams['device'])\nclassifier_model = classifier_model_class(model,hparams).to(hparams['device'])\nfcc_1_param = model_with_fcc_1_param(cnn_model).to(hparams['device'])\ninp_param = model_with_inp_param(cnn_model).to(hparams['device'])","50ce53f4":"model.train()","223cb15d":"for x in cnn_model.parameters():\n    x.requires_grad_(True)","28ec4af3":"for x in classifier_model.parameters():\n    x.retain_grad()\nfor x in cnn_model.parameters():\n    x.retain_grad()\nfor x in fcc_1_param.parameters():\n    x.retain_grad()","3d956244":"for x in fcc_1_param.parameters():\n    print(x)\n    print('-----------------------------------------')\n    print(x.data)\n    print('-----------------------------------------')\n    print(x.grad)\n    print('-----------------------------------------')\n    print(x.requires_grad)","fee60fb5":"#flattened = cnn_model(zero_image)\n#noise_out =  classifier_model(flattened).view(zero_image.size(0), -1)\n\n","a0067192":"print(cnn_model.fcc_1.grad)","2e04da63":"#loss = loss_fn(noise_out,target)\n#loss.backward()","30ef9791":"for x,y in cnn_model.named_parameters():\n    print(x)","a2bc0178":"for x in cnn_model.parameters():\n    #print(x.data)\n    print('---------------------')\n    print(x.requires_grad)\n    print(x.grad)\n    print('\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\/\\n\\n')","b83aa934":"for x in model.parameters():\n    x.requires_grad_(True)\nfor x in fcc_1_param.parameters():\n    x.requires_grad_(True)","08e9ab98":"for x in cnn_model.parameters():\n    x.requires_grad_(False)","79a43afb":"for x in classifier_model.parameters():\n    print(x.requires_grad)","61dfe70e":"for x in cnn_model.parameters():\n    print(x.requires_grad)","896ddfec":"for x in model.parameters():\n    print(x.requires_grad)","98217136":"'''\nThankfully, the claim was true\n\n'''","f3973fe0":"epochs = 1\niters_classifier = 100\niters_cnn = 50","dd478c00":"zero_image = torch.rand([hparams['batch_size'],banana_tensor.size(0),banana_tensor.size(1),banana_tensor.size(2)])\nfor i in range(zero_image.size(0)):\n    zero_image[i] = normalizer(zero_image[i])\nprint(zero_image.size())","462c8740":"print(zero_image[0])","4c5cb047":"sample_noise = torch.randn(10,1000)\ntorch.argmax(sample_noise,1)","611934aa":"model.cuda()\nzero_image.cuda()\nbanana_tensor.cuda()","f5d61f06":"for i in cnn_model.parameters():\n    print(i.device)","d63051c2":"for i in classifier_model.parameters():\n    print(i.device)","273c2caa":"for i in fcc_1_param.parameters():\n    print(i)","1899a4d1":"for i in fcc_1_param.parameters():\n    print(i.device)\n    print('\\n')\nfor i in inp_param.parameters():\n    print(i.device)","6741b0ad":"for i in fcc_1_param.parameters():\n    i.to(torch.device('cuda:0'))\nfor i in inp_param.parameters():\n    i.to(torch.device('cuda:0'))\n#THESE DIDN'T WORK","7bc275f9":"for i in cnn_model.parameters():\n    i.cuda()","b38bc867":"banana_tensor = banana_tensor.to(hparams['device'])","b1ebfa29":"\n#single forward pass of the noise through the whole model\n\nfcc_1 = cnn_model(zero_image)\nnoise_out =  classifier_model(fcc_1).view(zero_image.size(0), -1)\n\nprint(noise_out.size())\nprint(noise_out.requires_grad)\n#fcc_1 = fcc_1.view(fcc_1.size(0),-1)\n\n\n\n'''\nmaking a reference for measuring the change in first FCC layer activations after Gradient Ascent\n'''\nfcc_1_original = fcc_1.detach()\n\n\nmodel_opt = torch.optim.Adam(model.parameters()) #TO BE USED ONLY TO SET GRADIENTS TO ZERO\nfcc_1_optimizer = torch.optim.Adam(fcc_1_param.parameters())     \ninp_optimizer = torch.optim.Adam(fcc_1_param.parameters())\n\nprint(fcc_1_optimizer)\nprint(inp_optimizer)\n\n\n#FORWARD PASS OF ORIGINAL IMAGE\n\n'''\nCalculating final layer's scores on the original image to be regenerated\n'''\nscores = model(banana_tensor.unsqueeze(0))\n\nprint(scores.size())\nprint(scores.argmax(1).data)\n\n\n'''\nHaving faith in the model to properly identify the fruit\n'''\n\ntarget = torch.zeros((1,1000))\ntarget[0,scores.argmax(1)] = 1 \ntarget = target.expand(hparams['batch_size'],1000)\n\nprint(target.size())\n\nloss_fn = nn.MSELoss()\n\n\nfcc_grad = []\ninp_grad = []\n\n\nfor e in range(epochs):\n    \n    print(f'-----------------------------------EPOCH {e}--------------------------------')\n    \n    fcc_1 = cnn_model(zero_image)\n    noise_out =  classifier_model(fcc_1).view(zero_image.size(0), -1)\n    \n   \n    '''\n    To save computational efficiency as the earlier layers are not required in the backpropagation\n    '''\n    \n    for i in cnn_model.parameters():             \n        i.requires_grad_(False)\n    for i in fcc_1_param.parameters():             \n        i.requires_grad_(True)\n    \n    '''\n    Passing the fcc_1 output as a parameter to be optimized, should be easier as we sMorTly made a new model 'fcc_1_param' with fcc_1 as the sole parameter.\n    Loop to optimize the fcc_1 activations with respect to the loss\n    '''\n    print(f'----------------------Classifier-----------------------')\n    \n    for t1 in range(iters_classifier):\n        model_opt.zero_grad()\n        fcc_1_optimizer.zero_grad()\n        \n        #noise_out =  classifier_model(cnn_model.fcc_1.data).view(zero_image.size(0), -1)              \n        #THIS DOESN'T UPDATE THE GRAD OF 'cnn_model.fcc_1' PARAMETER, hence, it doesn't update 'cnn_model.fcc_1' either\n        \n        #THIS WORKS\n        noise_out =  classifier_model(cnn_model.fcc_1).view(zero_image.size(0), -1)\n        loss = loss_fn(noise_out,target.detach())        #to exclude 'target' from the computational graph\n        loss.backward(retain_graph= True)\n        \n        if t1 % 5 == 0:     #frequency of sampling results displayed \n            for i in fcc_1_param.parameters():\n                print(i.grad)\n                fcc_grad.append(i.grad.cpu().mean())\n        fcc_1_optimizer.step()\n    \n    print(((fcc_1_original - cnn_model.fcc_1.data)**2).sum())      #To check the L2 distance between the original fcc_1 tensor and updated one\n    \n    plt.figure()\n    plt.title(f'Mean fcc_grad for Epoch {e} every 5 steps')\n    plt.plot(fcc_grad)\n    plt.show()\n    fcc_grad.clear()\n    \n    '''\n    Now, the whole network is required in the backpropagation as we need to calculate gradients of updated fcc_1 layer with respect to the input layer\n    '''\n    \n    for i in cnn_model.parameters():             \n        i.requires_grad_(True)\n    \n    \n    print(f'----------------------CNN-----------------------')\n    \n    for t2 in range(iters_cnn):\n        model_opt.zero_grad()\n        inp_optimizer.zero_grad()\n        #noise_out =  classifier_model(cnn_model.fcc_1).view(zero_image.size(0), -1)              #THIS DOESN'T UPDATE THE GRAD OF 'cnn_model.fcc_1' PARAMETER\n        \n        #THIS WORKS\n        noise_out =  cnn_model(cnn_model.inp.detach).view(zero_image.size(0), -1)\n        loss = loss_fn(noise_out,target.detach())        #to exclude 'target' from the computational graph\n        loss.backward(retain_graph= True)\n        \n        if t2 % 20 == 0:    #frequency of sampling results displayed \n            for i in inp_param.parameters():\n                print(i)\n                inp_grad.append(i.grad.cpu().mean())\n        inp_optimizer.step()\n        zero_image\n    \n    print(((zero_image - cnn_model.inp.data)**2).sum()) \n    \n    plt.figure()\n    plt.title(f'Mean inp_grad for Epoch {e} every 20 steps')\n    plt.plot(fcc_grad)\n    plt.show()\n    inp_grad.clear()","02f8ced3":"class UnNormalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        \"\"\"\n        Args:\n            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n        Returns:\n            Tensor: Normalized image.\n        \"\"\"\n        for t, m, s in zip(tensor, self.mean, self.std):\n            t.mul_(s).add_(m)\n            # The normalize code -> t.sub_(m).div_(s)\n        return tensor","60b645ea":"detransform = torchvision.transforms.Compose([torchvision.transforms.Lambda(lambda x: UnNormalize(mean=(0.485, 0.456, 0.406) ,std=(0.229, 0.224, 0.225))(x[0])),\n                                              torchvision.transforms.ToPILImage(),\n                                              torchvision.transforms.Resize((96,96))])\n\nimages = cnn_model.inp","8628d500":"fig, ax = plt.subplots(2,5)\nfor row in ax:\n    for col in row:\n        col.imshow()","6841bf08":"**Checking the working of the model**","c365062a":"# **Revelation: model.parameters() and model.state_dict().values() are supposed to point to the same model attributes, like weights and biases of Conv2d layers, but the requires_grad of model.state_dict().values().requires_grad are all FALSE, while model.parameters().requires_grad are adjustible with .requires_grad_() method**","1303a835":"Checking the working of torch.argmax\n","bbd87594":"# **Logs**","b90399d7":"Normalizing Image Tensor for usage with VGG16","836fce39":"Verifying the one-ness of model.parameters() and model.state_dict().values() Tensors","7fb5df5f":"# **Investigating if cnn_model & classifier_model point to the same memory as preloaded 'model'**","6505b918":"# **Defining the training loops**","68b82447":"It is observed that the custom created 'fcc_1' and 'inp' nn.Parameter instances remained in the CPU, we have to move it to the GPU too\n","310aec00":"Block to enable retain_grad of classifier_model to visualize if the method is actually filling the 'grad' in the parameters","7abede52":"Moving the relevant tensors and models to GPU\n","235aaa14":"Loading the image to be reconstructed\n","70639a9c":"As expected, since the cnn_model and classifier_model point to the 'pretrained model', moving the pretrained model to 'GPU' also has effect on the overlapping parts in cnn_model and classifier_model","2be2f173":"# **MAIN**","0b92502b":"Freezing the layers of the pretrained model","d55df130":"Analysing the utilities available to work with the models","96134d31":"# **Defining Dummy Models (pointing to the segments of the preloaded model, in their same memory locations**","7d38d687":"# **Trying to copy the value of one tensor to another, without causing them to point to the same memory!!!**"}}