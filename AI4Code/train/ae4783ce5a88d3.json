{"cell_type":{"1c8ec1a3":"code","5b6d39d4":"code","ab11c0f9":"code","cd6750e7":"code","a5f34bbc":"code","58b2833c":"code","5d59ed09":"code","cffc08f5":"code","817f3f03":"code","5d8efd14":"code","ec8b5fd2":"code","fa9b664b":"code","6dc8f0ef":"code","d745af5f":"code","bd1d1a13":"code","f97bfcf1":"code","415a587f":"markdown","1a7f4123":"markdown","79cc2730":"markdown","84f3c122":"markdown","a6ceba14":"markdown","d0b99192":"markdown","e45525f8":"markdown","fb7c31b5":"markdown","8bf29dac":"markdown"},"source":{"1c8ec1a3":"import torch\nimport torch.nn as nn\n\nclass TransformerNetwork(nn.Module):\n\n    def __init__(self):\n        super(TransformerNetwork, self).__init__()\n        self.ConvBlock = nn.Sequential(\n            ConvLayer(3, 32, 9, 1),\n            nn.ReLU(),\n            ConvLayer(32, 64, 3, 2),\n            nn.ReLU(),\n            ConvLayer(64, 128, 3, 2),\n            nn.ReLU()\n        )\n        self.ResidualBlock = nn.Sequential(\n            ResidualLayer(128, 3), \n            ResidualLayer(128, 3), \n            ResidualLayer(128, 3), \n            ResidualLayer(128, 3), \n            ResidualLayer(128, 3)\n        )\n        self.DeconvBlock = nn.Sequential(\n            DeconvLayer(128, 64, 3, 2, 1),\n            nn.ReLU(),\n            DeconvLayer(64, 32, 3, 2, 1),\n            nn.ReLU(),\n            ConvLayer(32, 3, 9, 1, norm=\"None\")\n        )\n\n    def forward(self, x):\n        x = self.ConvBlock(x)\n        x = self.ResidualBlock(x)\n        out = self.DeconvBlock(x)\n        return out\n\n","5b6d39d4":"class ConvLayer(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, norm=\"instance\"):\n        super(ConvLayer, self).__init__()\n        # Padding Layers\n        padding_size = kernel_size \/\/ 2\n        self.reflection_pad = nn.ReflectionPad2d(padding_size)\n\n        # Convolution Layer\n        self.conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n\n        # Normalization Layers\n        self.norm_type = norm\n        if (norm==\"instance\"):\n            self.norm_layer = nn.InstanceNorm2d(out_channels, affine=True)\n        elif (norm==\"batch\"):\n            self.norm_layer = nn.BatchNorm2d(out_channels, affine=True)\n\n    def forward(self, x):\n        x = self.reflection_pad(x)\n        x = self.conv_layer(x)\n        if (self.norm_type==\"None\"):\n            out = x\n        else:\n            out = self.norm_layer(x)\n        return out\n","ab11c0f9":"\nclass ResidualLayer(nn.Module):\n    def __init__(self, channels=128, kernel_size=3):\n        super(ResidualLayer, self).__init__()\n        self.conv1 = ConvLayer(channels, channels, kernel_size, stride=1)\n        self.relu = nn.ReLU()\n        self.conv2 = ConvLayer(channels, channels, kernel_size, stride=1)\n\n    def forward(self, x):\n        identity = x                     # preserve residual\n        out = self.relu(self.conv1(x))   # 1st conv layer + activation\n        out = self.conv2(out)            # 2nd conv layer\n        out = out + identity             # add residual\n        return out\n\n","cd6750e7":"class DeconvLayer(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, output_padding, norm=\"instance\"):\n        super(DeconvLayer, self).__init__()\n\n        # Transposed Convolution \n        padding_size = kernel_size \/\/ 2\n        self.conv_transpose = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding_size, output_padding)\n\n        # Normalization Layers\n        self.norm_type = norm\n        if (norm==\"instance\"):\n            self.norm_layer = nn.InstanceNorm2d(out_channels, affine=True)\n        elif (norm==\"batch\"):\n            self.norm_layer = nn.BatchNorm2d(out_channels, affine=True)\n\n    def forward(self, x):\n        x = self.conv_transpose(x)\n        if (self.norm_type==\"None\"):\n            out = x\n        else:\n            out = self.norm_layer(x)\n        return out","a5f34bbc":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","58b2833c":"TransformerNetwork = TransformerNetwork().to(device)\nprint(TransformerNetwork)\n","5d59ed09":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torchvision.utils import save_image\nimport datetime\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport cv2\nimport os\nplt.rcParams[\"figure.figsize\"] = (20,10)","cffc08f5":"class VGG(nn.Module):\n    def __init__(self):\n        super(VGG, self).__init__()\n        # The first number x in convx_y gets added by 1 after it has gone\n        # through a maxpool, and the second y if we have several conv layers\n        # in between a max pool. These strings (0, 5, 10, ..) then correspond\n        # to conv1_1, conv2_1, conv3_1, conv4_1, conv5_1 mentioned in NST paper\n        self.chosen_features = [\"0\", \"5\", \"10\", \"19\", \"28\"]\n\n        # We don't need to run anything further than conv5_1 (the 28th module in vgg)\n        # Since remember, we dont actually care about the output of VGG: the only thing\n        # that is modified is the generated image (i.e, the input).\n        self.model = models.vgg19(pretrained=True).features[:29]\n\n    def forward(self, x):\n        # Store relevant features\n        features = []\n\n        # Go through each layer in model, if the layer is in the chosen_features,\n        # store it in features. At the end we'll just return all the activations\n        # for the specific layers we have in chosen_features\n        for layer_num, layer in enumerate(self.model):\n            x = layer(x)\n\n            if str(layer_num) in self.chosen_features:\n                features.append(x)\n\n        return features\nmodel = VGG().to(device).eval()\nmodel","817f3f03":"imsize = 256\nloader = transforms.Compose(\n    [\n        transforms.Resize((imsize, imsize)),\n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: x.repeat(3, 1, 1) if x.shape[0] == 1 else x  )\n        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]\n)\ndef load_image(image_name):\n    image = Image.open(image_name)\n    image = loader(image).unsqueeze(0)\n    #print(image.shape)\n    \n    return image.to(device)","5d8efd14":"from torchvision import datasets\n\ndataset = '..\/input\/neural-style-transfer\/'\nstyle_name='mosaic'\nstyle_img = load_image(\"..\/input\/neural-style-transfer\/style-images\/\"+style_name+\".jpg\")\n\n# Hyperparameters\ntotal_steps = 12000\nlearning_rate = 0.0001\nBATCH_SIZE = 1 \nCONTENT_WEIGHT = 17\nSTYLE_WEIGHT = 50\nTV_WEIGHT = 1e-6\n\ntrain_dataset = datasets.ImageFolder(dataset, transform=loader)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)","ec8b5fd2":"def plotT(tensor):\n    # Add the means\n    #ttoi_t = transforms.Compose([\n    #    transforms.Normalize([-103.939, -116.779, -123.68],[1,1,1])])\n\n    # Remove the batch_size dimension\n    tensor = tensor.squeeze()\n    #img = ttoi_t(tensor)\n    img = tensor.cpu().numpy()\n    \n    # Transpose from [C, H, W] -> [H, W, C]\n    img = img.transpose(1, 2, 0)\n    img = np.array(img).clip(0,1)\n    \n    plt.imshow(img)\n    plt.show()","fa9b664b":"for ind, mat in enumerate(train_loader):\n    if(mat[1].item() == 1):\n            continue\n    #print(mat[0][0].permute(1,2,0).shape)\n    plt.imshow(mat[0][0].permute(1,2,0))\n    plt.show()","6dc8f0ef":"plotT(style_img)","d745af5f":"# Optimizer settings\nimport torch.optim as optim\noptimizer = optim.Adam(TransformerNetwork.parameters(), lr=learning_rate)","bd1d1a13":"import time\nstart_time = time.time()\nfor i in range(1000):\n    \n    \n    if i%100 == 0:\n        print('| ------------- Epoch '+str(i)+' ------------- |')\n    iteration = 0\n    Batch_loss = 0\n    T = True\n    for _, data in enumerate(train_loader):\n        \n        if(data[1].item() == 1):\n            continue\n        iteration += 1\n        optimizer.zero_grad()\n        content = data[0]\n        content = content.type(torch.cuda.FloatTensor)\n        generated = TransformerNetwork(content)\n        \n        c_f = model(content)\n        g_f = model(generated)\n        s_f = model(style_img)\n        \n        MSELoss = nn.MSELoss().to(device)\n        c_loss = 0\n        s_loss =0\n        for j in range(5):\n            c_loss += MSELoss(g_f[j],c_f[j])\n            B,C,W,H = g_f[j].shape\n            g_gram = g_f[j].view(B,C, H*W).bmm(g_f[j].view(B,C, H*W).transpose(1,2) ) \/ (C*H*W)\n            s_gram = s_f[j].view(B,C, H*W).bmm(s_f[j].view(B,C, H*W).transpose(1,2) ) \/ (C*H*W)\n            s_loss += MSELoss(g_gram , s_gram)\n            \n        s_loss *= 5000000\n        c_loss *= 100\n        total = c_loss + s_loss\n        Batch_loss += total\n        total.backward()\n        optimizer.step()\n        if i%100 == 0 and T:\n            T = False\n            \n    if i%100 == 0:\n        print(\" Total Loss: \"+ str(round(Batch_loss.item(),2))+ \" Time: \"+str(round(time.time() - start_time,2)))\n        \n        start_time = time.time()","f97bfcf1":"counter = 1\n\nplt.figure(figsize=(15,100)) \nfor ind, mat in enumerate(train_loader):\n    if(mat[1].item() == 1):\n            continue\n    op = TransformerNetwork(mat[0].type(torch.cuda.FloatTensor))\n    plt.subplot(9, 2, counter)\n    plt.imshow(mat[0][0].permute(1,2,0))\n    plt.subplot(9, 2, counter+1)\n    plt.imshow(op[0].cpu().permute(1,2,0).detach().numpy() )\n   \n    counter +=2\n\n    \nplt.show()","415a587f":"# Hyperparameter","1a7f4123":"# Transformer Network Architecture\n\n**[Addn material](https:\/\/cs.stanford.edu\/people\/jcjohns\/papers\/eccv16\/JohnsonECCV16Supplementary.pdf)**\n\n* The style transfer networks use the architecture shown in Table 1 and our superresolution networks use the architecture shown in Table 2. In these tables \u201cC \u00d7 H \u00d7 W conv\u201d denotes a convolutional layer with C filters size H \u00d7 W which is immediately followed by spatial batch normalization [1] and a ReLU nonlinearity.\n* Our residual blocks each contain two 3\u00d73 convolutional layers with the same number of filters on both layer. We use the residual block design of Gross and Wilber [2] (shown in Figure 1), which differs from that of He et al [3] in that the ReLU nonlinearity following the addition is removed; this modified design was found in [2] to perform slightly better for image classification.\n* For style transfer, we found that standard zero-padded convolutions resulted in severe artifacts around the borders of the generated image. We therefore remove padding from the convolutions in residual blocks. A 3 \u00d7 3 convolution with no padding reduces the size of a feature map by 1 pixel on each side, so in this case the identity connection of the residual block performs a center crop on the input feature map. We also add spatial reflection padding to the beginning of the network so that the input and output of the network have the same size\n\n\n\n        Layer Activation size\n\n        Input 3 \u00d7 256 \u00d7 256\n\n        Reflection Padding (40 \u00d7 40) 3 \u00d7 336 \u00d7 336\n\n        32 \u00d7 9 \u00d7 9 conv, stride 1 32 \u00d7 336 \u00d7 336\n\n        64 \u00d7 3 \u00d7 3 conv, stride 2 64 \u00d7 168 \u00d7 168\n\n        128 \u00d7 3 \u00d7 3 conv, stride 2 128 \u00d7 84 \u00d7 84\n\n        Residual block, 128 filters 128 \u00d7 80 \u00d7 80\n\n        Residual block, 128 filters 128 \u00d7 76 \u00d7 76\n\n        Residual block, 128 filters 128 \u00d7 72 \u00d7 72\n\n        Residual block, 128 filters 128 \u00d7 68 \u00d7 68\n\n        Residual block, 128 filters 128 \u00d7 64 \u00d7 64\n\n        64 \u00d7 3 \u00d7 3 conv, stride 1\/2 64 \u00d7 128 \u00d7 128\n\n        32 \u00d7 3 \u00d7 3 conv, stride 1\/2 32 \u00d7 256 \u00d7 256\n\n        3 \u00d7 9 \u00d7 9 conv, stride 1 3 \u00d7 256 \u00d7 256\n\n","79cc2730":"# Basic Intro\n\n![Y](https:\/\/miro.medium.com\/max\/875\/0*F3xvwBKFhaQ3Mh_k)","84f3c122":"# Data or Content Image","a6ceba14":"# Intro to Transfer Learning\nFor intro to transfer learning check out:\n\n[Blood cell Classification with TF](https:\/\/www.kaggle.com\/tathagatbanerjee\/blood-cell-detection-100-acc-transfer-learning)\n\n[Hand Recognition using TF](https:\/\/www.kaggle.com\/tathagatbanerjee\/hand-recognition-cnn-model-plot-feature-plot)\n\n\nIf your good to go with python and Transfer Learning lets move forward: Although the use of different transfer learing algorithms, techniques and optimization is vividly used in the domain but in 2016 gatys first thought about what if we freeze the initialised weights and optimise the image. The idea is to tender a loss built over content and style images and optimise the image to minimise the loss. Chillax if you dont understand and find it latin, lets go slow and see what Transfer learning especially vgg model has for us\n\nVGG\nVGG16\n\nNow let\u2019s consider the 10th convolution layer of vgg16 which uses a 3x3 kernel with 512 feature maps to train and finally generates a output of 28X28x512 image representation, just for sake of simplicity let\u2019s assume that there are certain units in this 10th layer which gets activated by an image containing circles like wheel of a car or there might be some which get activated by an image having some pattern similar to three intersecting lines etc.\nIt\u2019s safe to assume that CNN does not learn to encode what image is but it actually learns to encode what image represents or what contents are visible in the image and due to inherent nonlinear nature of neural networks has we go from shallow layers to deeper layers the hidden units become capable to detect more and more complex feature from a given image.","d0b99192":"# Style Image","e45525f8":"If yoy read till here please upvote and support\n\ncheers\n\n\nTathagat","fb7c31b5":"Tutorial for Neural Style Transfer :    [Machines Can Draw NST](https:\/\/www.kaggle.com\/tathagatbanerjee\/machines-can-draw-neural-style-transfer-pytorch)\n\nThe notebook works over the paper : [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https:\/\/arxiv.org\/abs\/1603.08155)\n\nThis implementation made some modifications in Johnson et. al.'s proposed architecture, particularly:\n\n* The use of reflection padding in every Convolutional Layer, instead of big single reflection padding before the first convolution layer\n* Ditching of the Tanh output. The generated image are the raw outputs of the convolutional layer. While the Tanh model produces visually pleasing results, the model fails to transfer the vibrant and loud colors of the style image (i.e. generated images are usually darker). This however makes for a good retro style effect.\n* Use of Instance Normalization, instead of Batch Normalization after Convolutional and Deconvolutional layers, as discussed in Instance Normalization: The Missing Ingredient for Fast Stylization paper by Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.\n* The original caffe pretrained weights of VGG16 were used for this implementation, instead of the pretrained VGG16's in PyTorch's model zoo.","8bf29dac":"# Image Loading"}}