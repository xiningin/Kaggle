{"cell_type":{"1103dbfb":"code","51bacef9":"code","a44c70d3":"code","20df4d4a":"code","ce709049":"code","cce13cee":"code","4b8daf29":"code","24bf62fc":"code","38a26b6c":"code","08109937":"code","afb079ad":"code","98065014":"code","4e0fb5ac":"code","2a60c590":"code","48a359f8":"code","c8467982":"markdown","af1b1fd1":"markdown","e7bdaf64":"markdown","01d4f4a5":"markdown","ad6f3cdf":"markdown"},"source":{"1103dbfb":"!pip install ..\/input\/kerasapplications\/keras-team-keras-applications-3b180cb -f .\/ --no-index\n!pip install ..\/input\/efficientnet\/efficientnet-1.1.0\/ -f .\/ --no-index","51bacef9":"import os\nimport cv2\nimport pydicom\nimport pandas as pd\nimport numpy as np \nimport tensorflow as tf \nimport matplotlib.pyplot as plt \nfrom tqdm.notebook import tqdm \nfrom tensorflow.keras.layers import (\n    Dense, Dropout, Activation, Flatten, Input, BatchNormalization, GlobalAveragePooling2D, Add, Conv2D, AveragePooling2D, \n    LeakyReLU, Concatenate \n)\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.utils import Sequence\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.applications as tfa\nimport efficientnet.tfkeras as efn\nfrom sklearn.model_selection import train_test_split, KFold\nimport seaborn as sns","a44c70d3":"config = tf.compat.v1.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = tf.compat.v1.Session(config=config)","20df4d4a":"EPOCHS = 20\nBATCH_SIZE = 8\nNFOLD = 5\nLR = 0.003\nSAVE_BEST = True\nMODEL_CLASS = 'b1'","ce709049":"train = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/train.csv') ","cce13cee":"train.head()","4b8daf29":"train.SmokingStatus.unique()","24bf62fc":"def get_tab(df):\n    vector = [(df.Age.values[0] - 30) \/ 30] \n    \n    if df.Sex.values[0].lower() == 'male':\n       vector.append(0)\n    else:\n       vector.append(1)\n    \n    if df.SmokingStatus.values[0] == 'Never smoked':\n        vector.extend([0,0])\n    elif df.SmokingStatus.values[0] == 'Ex-smoker':\n        vector.extend([1,1])\n    elif df.SmokingStatus.values[0] == 'Currently smokes':\n        vector.extend([0,1])\n    else:\n        vector.extend([1,0])\n    return np.array(vector) ","38a26b6c":"A = {} \nTAB = {} \nP = [] \nfor i, p in tqdm(enumerate(train.Patient.unique())):\n    sub = train.loc[train.Patient == p, :] \n    fvc = sub.FVC.values\n    weeks = sub.Weeks.values\n    c = np.vstack([weeks, np.ones(len(weeks))]).T\n    a, b = np.linalg.lstsq(c, fvc)[0]\n    \n    A[p] = a\n    TAB[p] = get_tab(sub)\n    P.append(p)","08109937":"def get_img(path):\n    d = pydicom.dcmread(path)\n    return cv2.resize((d.pixel_array - d.RescaleIntercept) \/ (d.RescaleSlope * 1000), (512, 512))","afb079ad":"x, y = [], []\nfor p in tqdm(train.Patient.unique()):\n    try:\n        ldir = os.listdir(f'..\/input\/osic-pulmonary-fibrosis-progression-lungs-mask\/mask_noise\/mask_noise\/{p}\/')\n        numb = [float(i[:-4]) for i in ldir]\n        for i in ldir:\n            x.append(cv2.imread(f'..\/input\/osic-pulmonary-fibrosis-progression-lungs-mask\/mask_noise\/mask_noise\/{p}\/{i}', 0).mean())\n            y.append(float(i[:-4]) \/ max(numb))\n    except:\n        pass","98065014":"class IGenerator(Sequence):\n    BAD_ID = ['ID00011637202177653955184', 'ID00052637202186188008618']\n    def __init__(self, keys, a, tab, batch_size=BATCH_SIZE):\n        self.keys = [k for k in keys if k not in self.BAD_ID]\n        self.a = a\n        self.tab = tab\n        self.batch_size = batch_size\n        \n        self.train_data = {}\n        for p in train.Patient.values:\n            self.train_data[p] = os.listdir(f'..\/input\/osic-pulmonary-fibrosis-progression\/train\/{p}\/')\n    \n    def __len__(self):\n        return 1000\n    \n    def __getitem__(self, idx):\n        x = []\n        a, tab = [], [] \n        keys = np.random.choice(self.keys, size = self.batch_size)\n        for k in keys:\n            try:\n                i = np.random.choice(self.train_data[k], size=1)[0]\n                img = get_img(f'..\/input\/osic-pulmonary-fibrosis-progression\/train\/{k}\/{i}')\n                x.append(img)\n                a.append(self.a[k])\n                tab.append(self.tab[k])\n            except:\n                print(k, i)\n       \n        x,a,tab = np.array(x), np.array(a), np.array(tab)\n        x = np.expand_dims(x, axis=-1)\n        return [x, tab] , a","4e0fb5ac":"def get_efficientnet(model, shape):\n    models_dict = {\n        'b0': efn.EfficientNetB0(input_shape=shape,weights=None,include_top=False),\n        'b1': efn.EfficientNetB1(input_shape=shape,weights=None,include_top=False),\n        'b2': efn.EfficientNetB2(input_shape=shape,weights=None,include_top=False),\n        'b3': efn.EfficientNetB3(input_shape=shape,weights=None,include_top=False),\n        'b4': efn.EfficientNetB4(input_shape=shape,weights=None,include_top=False),\n        'b5': efn.EfficientNetB5(input_shape=shape,weights=None,include_top=False),\n        'b6': efn.EfficientNetB6(input_shape=shape,weights=None,include_top=False),\n        'b7': efn.EfficientNetB7(input_shape=shape,weights=None,include_top=False)\n    }\n    return models_dict[model]\n\ndef build_model(shape=(512, 512, 1), model_class=None):\n    inp = Input(shape=shape)\n    base = get_efficientnet(model_class, shape)\n    x = base(inp)\n    x = GlobalAveragePooling2D()(x)\n    inp2 = Input(shape=(4,))\n    x2 = tf.keras.layers.GaussianNoise(0.2)(inp2)\n    x = Concatenate()([x, x2]) \n    x = Dropout(0.5)(x) \n    x = Dense(1)(x)\n    model = Model([inp, inp2] , x)\n    return model","2a60c590":"kf = KFold(n_splits=NFOLD, random_state=42,shuffle=False)\nP = np.array(P)\nsubs = []\nfolds_history = []\nfor fold, (tr_idx, val_idx) in enumerate(kf.split(P)):\n    print('#####################')\n    print('####### Fold %i ######'%fold)\n    print('#####################')\n    print('Training...')\n    \n    er = tf.keras.callbacks.EarlyStopping(\n        monitor=\"val_loss\",\n        min_delta=1e-3,\n        patience=10,\n        verbose=1,\n        mode=\"auto\",\n        baseline=None,\n        restore_best_weights=True,\n    )\n\n    cpt = tf.keras.callbacks.ModelCheckpoint(\n        filepath='fold-%i.h5'%fold,\n        monitor='val_loss', \n        verbose=1, \n        save_best_only=SAVE_BEST,\n        mode='auto'\n    )\n\n    rlp = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss', \n        factor=0.5,\n        patience=5, \n        verbose=1, \n        min_lr=1e-8\n    )\n    model = build_model(model_class=MODEL_CLASS)\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LR), loss=\"mae\") \n    history = model.fit_generator(IGenerator(keys=P[tr_idx], \n                                   a = A, \n                                   tab = TAB), \n                        steps_per_epoch = 32,\n                        validation_data=IGenerator(keys=P[val_idx], \n                                   a = A, \n                                   tab = TAB),\n                        validation_steps = 16, \n                        callbacks = [cpt, rlp], \n                        epochs=EPOCHS)\n    folds_history.append(history.history)\n    print('Training done!')","48a359f8":"if SAVE_BEST:\n    mean_val_loss = np.mean([np.min(h['val_loss']) for h in folds_history])\nelse:\n    mean_val_loss = np.mean([h['val_loss'][-1] for h in folds_history])\nprint('Our mean CV MAE is: ' + str(mean_val_loss))","c8467982":"# Training Parameters\n\n- `EPOCHS`: number of epochs to train for in each fold\n- `BATCH_SIZE`: batch size of images during training\n- `NFOLD`: number of folds in K-fold cross-validation (CV)\n- `LR`: learning rate\n- `SAVE_BEST`: default is True to save best weights on validation loss\n- `MODEL_CLASS`: the class of model. E.g. \"b1\" for EfficientNet-B1","af1b1fd1":"# CV Evaluation","e7bdaf64":"# Overview & Remarks\n\nThis notebook contains the configurations required to train an efficientnet model for K-folds.\n\nIt is possible to hit -0.6910 LB by tweaking parameters in this notebook!","01d4f4a5":"# Training","ad6f3cdf":"# My Other Notebooks\n\nMy other notebooks in the competition:\n- [EfficientNet + Quantile Regression (Inference)](https:\/\/www.kaggle.com\/khoongweihao\/efficientnets-quantile-regression-inference)\n\n# Future Work\n\n- add in augmentations\n- more models to be included, stay tuned!\n\n# References\n\n- Michael Kazachok's Linear Decay (based on ResNet CNN)\n    - Model that uses images can be found at: https:\/\/www.kaggle.com\/miklgr500\/linear-decay-based-on-resnet-cnn\n- Ulrich GOUE's Osic-Multiple-Quantile-Regression-Starter\n    - Model that uses tabular data can be found at: https:\/\/www.kaggle.com\/ulrich07\/osic-multiple-quantile-regression-starter"}}