{"cell_type":{"f7cce0c4":"code","96b93cb8":"code","2a34e9d9":"code","8180d076":"code","f11b6016":"code","297469bf":"code","d3d98941":"code","362c7e5d":"code","c27bc429":"code","8cd126c3":"code","d8dc8472":"code","61882adc":"code","96852898":"code","6e30e410":"code","07c4280b":"code","1de6ac98":"code","c78c0aec":"code","12654738":"code","c06032b1":"code","6acdc345":"code","e02ae82e":"code","d79b1fb4":"code","ac587b8b":"code","fccb26fe":"code","3e8052e3":"code","980e4f81":"code","7a102d74":"markdown","93ec9069":"markdown","e746eeb2":"markdown","c507b3b3":"markdown"},"source":{"f7cce0c4":"#standard import\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n# To visualise in the notebook\n%matplotlib inline","96b93cb8":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2a34e9d9":"data = pd.read_csv('\/kaggle\/input\/advertising.csv')","8180d076":"#TV Radio NewsPapers are independent variable and Sales in dependent variable\ndata.head()","f11b6016":"#exploring the data - (no need to clean it as it was already cleaned)\ndata.info()","297469bf":"#the data variable must be normalized\ndata_nor = (data - data.mean())\/data.std()\ndata_nor","d3d98941":"sns.pairplot(data_nor)","362c7e5d":"# we can run gradient decent on two variables like sales and tv\nX = np.array(data_nor['TV'])\nY = np.array(data_nor['Sales'])","c27bc429":"#ploting the data for only X and Y we get\nsns.pairplot(data_nor, x_vars='TV', y_vars='Sales',height=5, kind='scatter')","8cd126c3":"# implementing the gradient decent we get\n# we'll start the initial value of m(Slope) and c(Intercept) to be as 0\n# iters it the number of iteration and rate is the learing rate (because) we are using the iterative method not closed\n# for minimization\n\n\ndef gradient(X, y, m_curr=0, c_curr=0, iters=1000, rate=0.01):\n    N = float(len(y)) # length of the data set\n    gd_df = pd.DataFrame(columns = ['m_curr', 'c_curr','cost'])\n    for i in range(iters):\n        y_curr = (m_curr * X) + c_curr # or model with the current slope and intercept\n        cost =sum([data**2 for data in (y-y_curr)]) \/ N # (y-y_current) diff square\n        m_grad = -(2\/N) * sum(X * (y - y_curr))\n        c_grad = -(2\/N) * sum(y - y_curr)\n        m_curr = m_curr - (rate * m_grad)\n        c_curr = c_curr - (rate * c_grad)\n        gd_df.loc[i] = [m_curr,c_curr,cost]\n    return gd_df\n\n    ","d8dc8472":"cost = gradient(X=X,y=Y)\ncost","61882adc":"cost.reset_index().plot.line(x='index',y=['cost'],figsize=(8,6))","96852898":"#above we have only consider the variable for tv and sales let use more variable now\nX_multi = data_nor[['TV','Radio','Newspaper']]\nY = data_nor['Sales']","6e30e410":"# adding and intercept column to X\nX_multi['Intercept'] = 1\n\nX_multi = X_multi.reindex(['Intercept','TV','Radio','Newspaper'],axis=1)\nX_multi","07c4280b":"#converting X and Y to numpy arrays\nX_multi = np.array(X_multi)\nY = np.array(Y)","1de6ac98":"# we need as vector representation for the intercept i.e intercept c, tv, newspaper, radio \n# these are all the independent variable on which the dependent variable sales depend upon\ntheta = np.matrix(np.array([0,0,0,0]))\nalpha = 0.01 # learing rate\niteration = 1000\n","c78c0aec":"#now for defining our cost function we have use theta to get the cost\n#this will be used in simultanious theta updates\n\ndef compute_cost(X, y, theta):\n    return np.sum(np.square(np.matmul(X, theta) - y))\/(2*len(y))","12654738":"#gradient decent for multiple variable would be\n\ndef gradient_multi(X,y,theta,alpha,iteration):\n    theta = np.zeros(X.shape[1])\n    m = len(X)\n    gdm_df = pd.DataFrame( columns = ['Bets','cost'])\n    \n    for i in range(iteration):\n        gradient = (1\/m) * np.matmul(X.T, np.matmul(X, theta) - y)\n        theta = theta - alpha * gradient\n        cost = compute_cost(X, y, theta)\n        gdm_df.loc[i] = [theta,cost]\n\n    return gdm_df\n    ","c06032b1":"cost = gradient_multi(X_multi, Y, theta, alpha, iteration)","6acdc345":"cost.reset_index().plot.line(x=\"index\",y=\"cost\",figsize=(8,6))","e02ae82e":"# event for the multiple variable the gradient decent is acting as a cost optimization function","d79b1fb4":"cost","ac587b8b":"# import LinearRegression from sklearn\nfrom sklearn.linear_model import LinearRegression\n\n# Representing LinearRegression as lr(Creating LinearRegression Object)\nlr = LinearRegression()\n\n#You don't need to specify an object to save the result because 'lr' will take the results of the fitted model.\nlr.fit(X_multi, Y)","fccb26fe":"#Calculated vaue using sklearn\nprint(lr.intercept_)\nprint(lr.coef_)","3e8052e3":"print(cost.tail(1)['Bets'])","980e4f81":"# slight difference in coefficient because of the change in algorithm used by sklearn","7a102d74":"### Implementing Gradient Decent using python","93ec9069":"#### Now our cost function would be the sum of squared error between the actual value and predected value\n\nEquation of straight line is Y=mX + C so we can us this to implement the gradient decent and see if the cost is decreasing or not\n\nCost function would be J(Theta)","e746eeb2":"# Gradient Decent can also be used on more than one variable","c507b3b3":"# We can also use sklearn linear regressor to compute the coefficients"}}