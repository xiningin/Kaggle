{"cell_type":{"613424d2":"code","4ab56a49":"code","5b947269":"code","5cc7f14c":"code","dd193b9b":"code","e3daf906":"code","b70454bd":"code","3e6a612e":"code","5c79d0ff":"code","ba866ff4":"code","aa11de86":"code","968c5d84":"code","14d1d716":"code","8426e4ec":"code","3cb31f8c":"code","459fab21":"code","5fc35422":"code","3ac90e4d":"code","84c8b01d":"code","7dcaf5c0":"code","9000c135":"code","eb3f2d95":"code","8f518880":"code","c478f904":"code","2ea61218":"code","af926a1c":"code","7df8dbd5":"code","ed4e2a0f":"code","5992b2de":"markdown"},"source":{"613424d2":"#Smit Patel\n\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4ab56a49":"#New Version with all of the data added to input\nimport os\nos.chdir('\/kaggle\/input\/alldata\/Project')","5b947269":"!pip install 'git+https:\/\/github.com\/cocodataset\/cocoapi.git#subdirectory=PythonAPI'","5cc7f14c":"!pip install Cython\n!pip install git+https:\/\/github.com\/philferriere\/cocoapi.git#egg=pycocotools^&subdirectory=PythonAPI","dd193b9b":"os.chdir('detr')","e3daf906":"#DETR Imports\n!pwd\n\nimport argparse\nimport random\nimport cv2\nfrom pathlib import Path\nimport torch\nimport torchvision.transforms as T\nimport PIL.Image\nfrom models import build_model\nfrom main import get_args_parser\nimport sys\nimport json\nimport shutil\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt","b70454bd":"import cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Layer\n\nfrom models_lfw.face_recognition.model_small import create_model\nfrom models_lfw.face_recognition.align import AlignDlib\nfrom models_lfw.triplet_loss import TripletLossLayer\nfrom coml_preprocessor import ComlFaceDataGenerator","3e6a612e":"##WE NEED TO ADD IN A DATSET FOR THE \nimport csv\nmapping = {}\nwith open('\/content\/drive\/Shareddrives\/2021 FIRE-COML-STUDENTS\/Spring\/Facial Recognition Team Project\/Face Photos Dataset\/person_name_id_mapping.csv', newline='') as csvfile:\n     reader = csv.DictReader(csvfile)\n     for row in reader:\n         print(row[\"Name\"], int(row[\"ID\"]))\n         mapping[row[\"Name\"]] = int(row[\"ID\"]) # This stores Id in a key-value pair format similar to a java map","5c79d0ff":"# Model Configurations\nparser = argparse.ArgumentParser(description='DETR args parser', parents=[get_args_parser()])\nargs = parser.parse_args(args=[])\nargs.resume = '\/content\/drive\/Shareddrives\/2021 FIRE-COML-STUDENTS\/Spring\/Facial Recognition Team Project\/Team 3\/Project\/models_lfw\/checkpoint.pth'\nargs.device = 'cpu'\n\nif args.output_dir:\n    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n\nargs.distributed = False\n\nprint(args) \n\n# Buuld Model\nmodel, criterion, postprocessors = build_model(args)\n\ndevice = torch.device(args.device)\nmodel.to(device)\n\n# Load Weights\noutput_dir = Path(args.output_dir)\nif args.resume:\n    # the model will download the weights and model state from the https link provided\n    if args.resume.startswith('https'):\n      checkpoint = torch.hub.load_state_dict_from_url(\n          args.resume, map_location='cpu', check_hash = True)\n    else:\n        checkpoint = torch.load(args.resume, map_location='cpu')\n\n    # this load the weights and model state into the model\n    model.load_state_dict(checkpoint['model'], strict=True)","ba866ff4":"# COCO classes\nCLASSES = [\n   'N\/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n   'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N\/A',\n   'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n   'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N\/A', 'backpack',\n   'umbrella', 'N\/A', 'N\/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n   'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n   'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N\/A', 'wine glass',\n   'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n   'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n   'chair', 'couch', 'potted plant', 'bed', 'N\/A', 'dining table', 'N\/A',\n   'N\/A', 'toilet', 'N\/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n   'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N\/A',\n   'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n   'toothbrush'\n]\n\n# colors for visualization\nCOLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n\n# standard PyTorch mean-std input image normalization\ntransform = T.Compose([\n    T.ToTensor(),\n    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# for output bounding box post-processing\ndef box_cxcywh_to_xyxy(x):\n    x_c, y_c, w, h = x.unbind(1)\n    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n    return torch.stack(b, dim=1)\n\ndef rescale_bboxes(out_bbox, size):\n    img_w, img_h = size\n    b = box_cxcywh_to_xyxy(out_bbox)\n    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n    return b\n\ndef detect(im, model, transform):\n    # mean-std normalize the input image (batch-size: 1)\n    img = transform(im).unsqueeze(0)\n\n    assert img.shape[-2] <= 1600 and img.shape[-1] <=1600, 'demo model only supports images up to 1600 pixels on each side'\n\n    outputs = model(img)\n\n    probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n    keep = probas.max(-1).values > 0.7\n\n    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n    return probas[keep], bboxes_scaled\n\ndef plot_results(pil_img, prob, boxes, classes):\n    plt.figure(figsize=(16,10))\n    plt.imshow(pil_img)\n    ax = plt.gca()\n    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), COLORS * 100):\n        c1 = p.argmax()\n        if CLASSES[c1] not in classes:\n            continue\n        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n                                   fill=False, color=c, linewidth=3))\n        \n        text= f'Persons name'\n        #f'{CLASSES[c1]}: {p[c1]:0.2f}'\n        ax.text(xmin, ymin, text, fontsize=15,\n                bbox=dict(facecolor='yellow', alpha=.5))\n    plt.axis('off')\n    plt.show()\n\n# Stores coordinates of the bounding box\ndef bbox_dims(pil_img, prob, boxes, classes):\n    bbox_list = []\n    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), COLORS * 100):\n        c1 = p.argmax()\n        if CLASSES[c1] not in classes:\n            continue\n        bbox_list.append([xmin, ymin, xmax, ymax])\n    return bbox_list","aa11de86":"the_image = PIL.Image.open('\/content\/a.jpg')\n\nwidth, height = the_image.size\n\nif width > 1600 and height > 1600:\n  new_size = (1600, 1600)\n  the_image = the_image.resize(new_size)\n\n#resized_im = the_image.resize((round(the_image.size[0]*0.5), round(the_image.size[1]*0.5)))\nscores, boxes = detect(the_image, model, transform)\nplot_classes = ['person']\nplot_results(the_image, scores, boxes, plot_classes)\n\n# Obtains bbox coordinates for each bbox and adds them to the list\nbbox_dim_list = bbox_dims(the_image, scores, boxes, plot_classes)\n\n#for bbox in bbox_dim_list:\n#  print(bbox)","968c5d84":"# Crop Images, this will be fed into the coml generator\ncropped_images = []\nfor bbox in bbox_dim_list:\n    cropped_images.append(the_image.crop((bbox[0], bbox[1], bbox[2], bbox[3]))) # Left, upper, right, bottom\nprint(cropped_images)\n\ni = 1\nfor bbox in bbox_dim_list:\n  plt.subplot(1, len(cropped_images), i)\n  plt.imshow(cropped_images[i - 1])\n  i += 1\nplt.show()","14d1d716":"# Input for anchor, positive, and negative images\nin_a = Input(shape=(96, 96, 3), name=\"img_a\")\nin_p = Input(shape=(96, 96, 3), name=\"img_p\")\nin_n = Input(shape=(96, 96, 3), name=\"img_n\")\n\n# create the base model from model_small\nmodel_sm = create_model()\n\n# Output the embedding vectors from anchor, positive, and negative images\n# The model weights are shared (Triplet network)\nemb_a = model_sm(in_a)\nemb_p = model_sm(in_p)\nemb_n = model_sm(in_n)\n\n# Layer that computes the triplet loss from anchor, positive and negative embedding vectors\ntriplet_loss_layer = TripletLossLayer(alpha=0.2, name='triplet_loss_layer')([emb_a, emb_p, emb_n])\n\n# Model that can be trained with anchor, positive, and negative images\nmodel = Model([in_a, in_p, in_n], triplet_loss_layer)\n\n# Load weights\nmodel.load_weights(\"\/content\/drive\/Shareddrives\/2021 FIRE-COML-STUDENTS\/Spring\/Facial Recognition Team Project\/Team 3\/Project\/ckpts(lfw)\/epoch024_loss1.440.hdf5\")\n\nbase_model = model.layers[3]","8426e4ec":"class IdentityMetadata():\n    def __init__(self, base, name, file):\n        # dataset base directory\n        self.base = base\n        # identity name\n        self.name = name\n        # image file name\n        self.file = file\n\n    def __repr__(self):\n        return self.image_path()\n\n    def image_path(self):\n        return os.path.join(self.base, self.name, self.file) \n    \ndef load_metadata(path, upper_limit):\n    metadata = []\n    count = 0\n    for i in sorted(os.listdir(path)):\n        if count == upper_limit: \n            break\n            \n        for f in sorted(os.listdir(os.path.join(path, i))):\n            if count == upper_limit: \n                break\n                \n            count += 1\n            # Check file extension. Allow only jpg\/jpeg' files.\n            ext = os.path.splitext(f)[1]\n            if ext == '.jpg' or ext == '.jpeg':\n                metadata.append(IdentityMetadata(path, i, f))\n    return np.array(metadata)\n\ndef load_image(path):\n    img = cv2.imread(path, 1)\n    # OpenCV loads images with color channels\n    # in BGR order. So we need to reverse them\n    return img[...,::-1]\n    \ndef align_image(img):\n        alignment = AlignDlib('models_lfw\/landmarks.dat')\n        bb = alignment.getLargestFaceBoundingBox(img)\n        if bb is None:\n            return cv2.resize(img, (96,96))\n        else:\n            return alignment.align(96, \n                                   img, \n                                   bb,\n                                   landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)","3cb31f8c":"metadata = load_metadata('\/content\/drive\/Shareddrives\/2021 FIRE-COML-STUDENTS\/Spring\/Facial Recognition Team Project\/Team 3\/Project\/data\/fire', 100)\nalignment = AlignDlib('models_lfw\/landmarks.dat')","459fab21":"myEmbeddings = np.empty((metadata.shape[0], 128))\n\nfor i,img in enumerate(cropped_images):\n  img = align_image(np.asarray(img))\n  img = img.astype('float32')\n  img =img \/ 255.0\n  img = np.expand_dims(img, axis=0)\n  person = str(i)#m.image_path().split('\/')[11].replace('_', ' ')\n  # base_model.summary()\n  myEmbeddings[i] = base_model.predict(img)\n    # for displaying the progress of creating embeddings\n  if i%1 == 0: print(str(i) + \"\\n\", end=\" \")","5fc35422":"embeddings = np.empty((metadata.shape[0], 128))\n\nfor i, m in enumerate(metadata):\n    img = load_image(m.image_path())\n    img = align_image(img)\n    img = img.astype('float32')\n    img = img \/ 255.0\n    img = np.expand_dims(img, axis=0)\n    person = m.image_path().split('\/')[11].replace('_', ' ')\n   # base_model.summary()\n    embeddings[i] = base_model.predict(img)\n    \n    # for displaying the progress of creating embeddings\n    if i%1 == 0: print(str(i) + \" \" + person + \"\\n\", end=\" \")\n    if i == len(metadata)-1: print('done')","3ac90e4d":"def distance(emb1, emb2):\n    return np.sum(np.square(emb1 - emb2))","84c8b01d":"person = {}\ndef show_pair_m(myEmbeddings, embeddings, idMy, idE):\n    plt.figure(figsize=(4,2))\n    plt.suptitle(f'Distance = {distance(myEmbeddings[idMy], embeddings[idE]):.2f}') \n\n    plt.subplot(1,2,1)\n    plt.imshow(align_image(np.asarray(cropped_images[idMy])))\n    plt.subplot(1,2,2)\n    plt.imshow(align_image(load_image(metadata[idE].image_path())))\n\n    if (idMy == 0):\n      person[idMy] = [metadata[idE].image_path().split('\/')[11].replace('_', ' ')]\n    if (idMy != 0):\n      dist = distance(myEmbeddings[idMy], embeddings[idE])\n      if dist < distance(myEmbeddings[idMy - 1], embeddings[idE - 1]):\n        #p.append(metadata[idE].image_path().split('\/')[11].replace('_', ' '))\n        personidMy = [metadata[idE].image_path().split('\/')[11].replace('_', ' ')]\n\n    plt.show()\n    \n# show positive pair (change this to what you want)\n\nfor i in range(len(myEmbeddings)):\n  for b in range(len(embeddings)):\n    show_pair_m(myEmbeddings, embeddings, i, b)\n\nprint(person)\n\n#def show_pair_m(myEmbeddings, embeddings, idMy, idE):\n #   plt.figure(figsize=(4,2))\n  #  plt.suptitle(f'Distance = {distance(myEmbeddings[idMy], embeddings[idE]):.2f}') \n#\n #   plt.subplot(1,2,1)\n  #  plt.imshow(align_image(np.asarray(cropped_images[idMy])))\n   # plt.subplot(1,2,2)\n    #plt.imshow(align_image(load_image(metadata[idE].image_path())))\n\n    #plt.show()\n    \n# show positive pair (change this to what you want)\n#show_pair_m(myEmbeddings, embeddings, 10, 2)","7dcaf5c0":"# Obtain ID Numbers\nfor i in range(len(person)):\n  print(mapping[str(person[i]).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")])","9000c135":"def plot_results2(pil_img, prob, boxes, classes):\n    plt.figure(figsize=(16,10))\n    plt.imshow(pil_img)\n    ax = plt.gca()\n    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), COLORS * 100):\n        c1 = p.argmax()\n        if CLASSES[c1] not in classes:\n            continue\n        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n                                   fill=False, color=c, linewidth=3))\n        \n        \n        for i in range(len(person)):\n          text = str(person[i]).replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\")\n        \n\n\n        #f'{CLASSES[c1]}: {p[c1]:0.2f}'\n        ax.text(xmin, ymin, text, fontsize=15,\n                bbox=dict(facecolor='yellow', alpha=.5))\n    plt.axis('off')\n    plt.show()","eb3f2d95":"plot_results2(the_image, scores, boxes, plot_classes)","8f518880":"persons = []\npath = []\n\ndef show_pair(embeddings, id1,secondemb, id2):\n    plt.figure(figsize=(4,2))\n    plt.suptitle(f'Distance = {distance(embeddings[id1], embeddings[id2]):.2f}')\n\n    #imagePath = str(metadata[id2].image_path()) # Stores ImagePath, idea is to use ID2 as the name of the person it could be\n    #path.append(imagePath)\n\n    plt.subplot(1,2,1)\n    plt.imshow(align_image(load_image(metadata[id1].image_path())))\n    plt.subplot(1,2,2)\n    \"person \" + str(id2)\n    plt.imshow(cropped_images[id2])\n    plt.imshow(align_image(load_image(metadata[id2].image_path())))\n\n    dist = distance(embeddings[id1], embeddings[id2])\n    if dist < .5:\n        persons.append(metadata[id1].image_path().split('\/')[11].replace('_', ' '))\n        persons.append(metadata[id2].image_path().split('\/')[11].replace('_', ' '))\n    plt.show()\n    \n# show positive pair (change this to what you want)\nshow_pair(embeddings, 3, 9)\n\n# show negative pair (change this to what you want)\n#show_pair(embeddings, 3, 9)\n\nprint(persons)\n#print(imagePath)\nprint(path)","c478f904":"import tensorflow as tf\nfrom tensorflow import keras\n\nclass ComlFaceDataGenerator(keras.utils.Sequence):\n    def __init__(self, data_dir, batch_size, anchor_shape = (96,96), n_channels = 3, img_aug = False, face_align = True, shuffle = True):\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.img_aug = img_aug\n        self.anchor_shape = anchor_shape\n        self.n_channels = n_channels\n        self.img_aug = img_aug\n        self.shuffle = shuffle\n        self.face_align = face_align\n        self.alignment = AlignDlib('models_lfw\/landmarks.dat')\n        \n        persons = os.listdir(data_dir)\n        img_exts = ('.png', '.jpg', '.jpeg')\n\n        # process pos and neg image pairs from data_dir\n        self.positive_pairs = []\n        self.negative_pairs = []\n        for i, person in enumerate(persons):\n            person_path = os.path.join(data_dir, person)\n            person_files = os.listdir(person_path)\n            # get all image files based on extension\n            image_files = [image_file for image_file in person_files if image_file.endswith(img_exts)]\n            \n            # randomly pick 2 images of same person as positive pair \n            if len(image_files) >= 2:\n                image_ids = np.arange(len(image_files))\n                random.shuffle(image_ids)\n                self.positive_pairs.append([(person,image_files[image_ids[0]]),\n                                            (person,image_files[image_ids[1]])])\n                \n                # randomly pick another image as negative pair\n                remaining_persons = persons.copy()\n                remaining_persons.pop(i)\n                while True:\n                    other_person = random.choice(remaining_persons)\n                    other_person_path = os.path.join(data_dir, other_person)\n                    other_person_files = os.listdir(other_person_path)\n                    other_person_image_files = [image_file for image_file in other_person_files if image_file.endswith(img_exts)]\n                    if len(other_person_image_files) >= 1:\n                        random_id = random.randrange(len(other_person_image_files))\n                        self.negative_pairs.append([(person,image_files[image_ids[0]]),\n                                                    (other_person,other_person_image_files[random_id])])\n                        break\n            else:\n                continue\n\n        self.on_epoch_end()\n\n    # update indexs after each epoch\n    def on_epoch_end(self):\n        self.pos_indexes = np.arange(len(self.positive_pairs))\n        self.neg_indexes = np.arange(len(self.negative_pairs))\n        if self.shuffle == True:\n            np.random.shuffle(self.pos_indexes)\n            np.random.shuffle(self.neg_indexes)\n\n    # return the number of batches per epoch\n    def __len__(self):        \n        return int(np.floor(len(self.pos_indexes) \/ self.batch_size))\n\n    # return the path to the image\n    def get_image_path(self, person, image_file):\n        return os.path.join(self.data_dir, person, image_file)\n\n    # augment img as a batch\n    def augment_images(self, img_batch):\n        seq = iaa.Sequential([\n            iaa.Crop(px=(0, 16)), # crop images from each side by 0 to 16px (randomly chosen)\n            iaa.Fliplr(0.5), # horizontally flip 50% of the images\n            iaa.GaussianBlur(sigma=(0, 1.0)),\n            # Strengthen or weaken the contrast in each image.\n            iaa.LinearContrast((0.75, 1.5)),# blur images with a sigma of 0 to 3.0\n        ])\n        return seq(images=img_batch)\n\n    def align_image(self, img):\n        alignment = self.alignment\n        bb = alignment.getLargestFaceBoundingBox(img)\n        if bb is None:\n            return cv2.resize(img, self.anchor_shape)\n        else:\n            return alignment.align(self.anchor_shape[0],\n                                   img,\n                                   bb,\n                                   landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n    \n    # return the image as numpy tensor\n    def get_image_tensor(self, person, image_file):\n        # read and remove alpha channel\n        img = cv2.imread(self.get_image_path(person, image_file))[:,:,:3] \n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        if self.face_align:\n            img = self.align_image(img)\n        else:\n            img = cv2.resize(img, self.anchor_shape)\n        img = img \/ 255.0\n        img = img.astype(np.float32)\n        img = img[None, ...]\n        return img\n\n    # feed in an index and return a batch\n    def __getitem__(self, index):\n        # slice indexes for current batch\n        pos_indexes = self.pos_indexes[index*self.batch_size:(index+1)*self.batch_size]\n        neg_indexes = self.neg_indexes[index*self.batch_size:(index+1)*self.batch_size]\n        \n        anchor_img_arr = np.empty( (self.batch_size, *self.anchor_shape, self.n_channels), dtype=np.float32 )\n        pos_img_arr = np.empty( (self.batch_size, *self.anchor_shape, self.n_channels), dtype=np.float32 )\n        neg_img_arr = np.empty( (self.batch_size, *self.anchor_shape, self.n_channels), dtype=np.float32 )\n\n        pos_pairs_batch = [self.positive_pairs[j] for j in pos_indexes]\n        neg_pairs_batch = [self.negative_pairs[k] for k in neg_indexes]\n\n        for i, pos_pair in enumerate(pos_pairs_batch):\n            # process anchor image\n            anc_name, anc_id = pos_pair[0]\n            anchor_img_arr[i] = self.get_image_tensor(anc_name, anc_id)\n\n            # process postive image\n            pos_name, pos_id = pos_pair[1]\n            pos_img_arr[i] = self.get_image_tensor(pos_name, pos_id)\n            \n            # Process negative image\n            neg_pair = neg_pairs_batch[i]\n            neg_name, neg_id = neg_pair[0]\n            if pos_name == neg_name:\n                neg_name, neg_id = neg_pair[1]\n            neg_img_arr[i] = self.get_image_tensor(neg_name, neg_id)\n\n        if self.img_aug:\n            anchor_img_arr = self.augment_images(anchor_img_arr)\n            pos_img_arr = self.augment_images(pos_img_arr)\n            neg_img_arr = self.augment_images(neg_img_arr)\n\n        return anchor_img_arr, pos_img_arr, neg_img_arr","2ea61218":"#from coml_preprocessor import ComlFaceDataGenerator\ndata_dir = '\/content\/drive\/Shareddrives\/2021 FIRE-COML-STUDENTS\/Spring\/Facial Recognition Team Project\/Team 3\/Project\/data\/fire'\nbatch_size = 5\ncoml_generator = ComlFaceDataGenerator(data_dir=data_dir,\n                                       batch_size=batch_size,\n                                       img_aug=False)\n\npos_pairs_embeddings = np.empty((len(coml_generator.positive_pairs), 2, 128))\npos_images = {}\n\nfor i, pair in enumerate(coml_generator.positive_pairs):\n    person, image_file = pair[0]\n    img = coml_generator.get_image_tensor(person, image_file)\n    pos_pairs_embeddings[i, 0] = base_model.predict(img)\n\n    person, image_file = pair[1]\n    img1 = coml_generator.get_image_tensor(person, image_file)\n    pos_pairs_embeddings[i, 1] = base_model.predict(img1)\n    \n    pos_images[i] = [img[0,:,:,:], img1[0,:,:,:]]\n\n    if i%1 == 0: print(i, end=\" \")\n    if i == len(coml_generator.positive_pairs)-1: print('done')","af926a1c":"import tensorflow as tf\nfrom tensorflow import keras\n\nclass ComlFaceDataGenerator(keras.utils.Sequence):\n    def __init__(self, data_dir, batch_size, anchor_shape = (96,96), n_channels = 3, img_aug = False, face_align = True, shuffle = True):\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n        self.img_aug = img_aug\n        self.anchor_shape = anchor_shape\n        self.n_channels = n_channels\n        self.img_aug = img_aug\n        self.shuffle = shuffle\n        self.face_align = face_align\n        self.alignment = AlignDlib('models_lfw\/landmarks.dat')\n        \n        persons = os.listdir(data_dir)\n        img_exts = ('.png', '.jpg', '.jpeg')\n\n        # process pos and neg image pairs from data_dir\n        self.positive_pairs = []\n        self.negative_pairs = []\n        for i, person in enumerate(persons):\n            person_path = os.path.join(data_dir, person)\n            person_files = os.listdir(person_path)\n            # get all image files based on extension\n            image_files = [image_file for image_file in person_files if image_file.endswith(img_exts)]\n            \n            # randomly pick 2 images of same person as positive pair \n            if len(image_files) >= 2:\n                image_ids = np.arange(len(image_files))\n                random.shuffle(image_ids)\n                self.positive_pairs.append([(person,image_files[image_ids[0]]),\n                                            (person,image_files[image_ids[1]])])\n                \n                # randomly pick another image as negative pair\n                remaining_persons = persons.copy()\n                remaining_persons.pop(i)\n                while True:\n                    other_person = random.choice(remaining_persons)\n                    other_person_path = os.path.join(data_dir, other_person)\n                    other_person_files = os.listdir(other_person_path)\n                    other_person_image_files = [image_file for image_file in other_person_files if image_file.endswith(img_exts)]\n                    if len(other_person_image_files) >= 1:\n                        random_id = random.randrange(len(other_person_image_files))\n                        self.negative_pairs.append([(person,image_files[image_ids[0]]),\n                                                    (other_person,other_person_image_files[random_id])])\n                        break\n            else:\n                continue\n\n        self.on_epoch_end()\n\n    # update indexs after each epoch\n    def on_epoch_end(self):\n        self.pos_indexes = np.arange(len(self.positive_pairs))\n        self.neg_indexes = np.arange(len(self.negative_pairs))\n        if self.shuffle == True:\n            np.random.shuffle(self.pos_indexes)\n            np.random.shuffle(self.neg_indexes)\n\n    # return the number of batches per epoch\n    def __len__(self):        \n        return int(np.floor(len(self.pos_indexes) \/ self.batch_size))\n\n    # return the path to the image\n    def get_image_path(self, person, image_file):\n        return os.path.join(self.data_dir, person, image_file)\n\n    # augment img as a batch\n    def augment_images(self, img_batch):\n        seq = iaa.Sequential([\n            iaa.Crop(px=(0, 16)), # crop images from each side by 0 to 16px (randomly chosen)\n            iaa.Fliplr(0.5), # horizontally flip 50% of the images\n            iaa.GaussianBlur(sigma=(0, 1.0)),\n            # Strengthen or weaken the contrast in each image.\n            iaa.LinearContrast((0.75, 1.5)),# blur images with a sigma of 0 to 3.0\n        ])\n        return seq(images=img_batch)\n\n    def align_image(self, img):\n        alignment = self.alignment\n        bb = alignment.getLargestFaceBoundingBox(img)\n        if bb is None:\n            return cv2.resize(img, self.anchor_shape)\n        else:\n            return alignment.align(self.anchor_shape[0],\n                                   img,\n                                   bb,\n                                   landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n    \n    # return the image as numpy tensor\n    def get_image_tensor(self, person, image_file):\n        # read and remove alpha channel\n        img = cv2.imread(self.get_image_path(person, image_file))[:,:,:3] \n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        if self.face_align:\n            img = self.align_image(img)\n        else:\n            img = cv2.resize(img, self.anchor_shape)\n        img = img \/ 255.0\n        img = img.astype(np.float32)\n        img = img[None, ...]\n        return img\n\n    # feed in an index and return a batch\n    def __getitem__(self, index):\n        # slice indexes for current batch\n        pos_indexes = self.pos_indexes[index*self.batch_size:(index+1)*self.batch_size]\n        neg_indexes = self.neg_indexes[index*self.batch_size:(index+1)*self.batch_size]\n        \n        anchor_img_arr = np.empty( (self.batch_size, *self.anchor_shape, self.n_channels), dtype=np.float32 )\n        pos_img_arr = np.empty( (self.batch_size, *self.anchor_shape, self.n_channels), dtype=np.float32 )\n        neg_img_arr = np.empty( (self.batch_size, *self.anchor_shape, self.n_channels), dtype=np.float32 )\n\n        pos_pairs_batch = [self.positive_pairs[j] for j in pos_indexes]\n        neg_pairs_batch = [self.negative_pairs[k] for k in neg_indexes]\n\n        for i, pos_pair in enumerate(pos_pairs_batch):\n            # process anchor image\n            anc_name, anc_id = pos_pair[0]\n            anchor_img_arr[i] = self.get_image_tensor(anc_name, anc_id)\n\n            # process postive image\n            pos_name, pos_id = pos_pair[1]\n            pos_img_arr[i] = self.get_image_tensor(pos_name, pos_id)\n            \n            # Process negative image\n            neg_pair = neg_pairs_batch[i]\n            neg_name, neg_id = neg_pair[0]\n            if pos_name == neg_name:\n                neg_name, neg_id = neg_pair[1]\n            neg_img_arr[i] = self.get_image_tensor(neg_name, neg_id)\n\n        if self.img_aug:\n            anchor_img_arr = self.augment_images(anchor_img_arr)\n            pos_img_arr = self.augment_images(pos_img_arr)\n            neg_img_arr = self.augment_images(neg_img_arr)\n\n        return anchor_img_arr, pos_img_arr, neg_img_arr","7df8dbd5":"#from coml_preprocessor import ComlFaceDataGenerator\ndata_dir = '\/content\/drive\/Shareddrives\/2021 FIRE-COML-STUDENTS\/Spring\/Facial Recognition Team Project\/Team 3\/Project\/data\/fire'\nbatch_size = 5\ncoml_generator = ComlFaceDataGenerator(data_dir=data_dir,\n                                       batch_size=batch_size,\n                                       img_aug=False)\n\npos_pairs_embeddings = np.empty((len(coml_generator.positive_pairs), 2, 128))\npos_images = {}\n\nfor i, pair in enumerate(coml_generator.positive_pairs):\n    person, image_file = pair[0]\n    img = coml_generator.get_image_tensor(person, image_file)\n    pos_pairs_embeddings[i, 0] = base_model.predict(img)\n\n    person, image_file = pair[1]\n    img1 = coml_generator.get_image_tensor(person, image_file)\n    pos_pairs_embeddings[i, 1] = base_model.predict(img1)\n    \n    pos_images[i] = [img[0,:,:,:], img1[0,:,:,:]]\n\n    if i%1 == 0: print(i, end=\" \")\n    if i == len(coml_generator.positive_pairs)-1: print('done')","ed4e2a0f":"def distance(emb1, emb2):\n    return np.sum(np.square(emb1 - emb2))\n\ndef show_pair_dist(pair_embeddings, images, id):\n    plt.figure(figsize=(4,2))\n    plt.suptitle(f'Distance = {distance(pair_embeddings[id, 0], pair_embeddings[id, 1]):.2f}')\n    plt.subplot(121)\n    plt.imshow(images[id][0])\n    plt.subplot(122)\n    plt.imshow(images[id][1])\n    plt.show()\nshow_pair_dist(pos_pairs_embeddings, pos_images, 2)","5992b2de":"LFW"}}