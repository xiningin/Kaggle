{"cell_type":{"108f9895":"code","082ed5a0":"code","606785bb":"code","b7806c03":"code","3288d273":"code","66b0cb69":"code","a0a33533":"code","49d1513a":"code","9507fff2":"code","c35942f0":"code","01ed932a":"code","f0d65a54":"code","74dd2467":"code","482a9965":"code","4e946a34":"code","21c2f79b":"code","8f7fc1e6":"code","6f758cb3":"code","c860074f":"code","8bac78f5":"code","7e11f1e8":"code","e25cae38":"code","8c331d9f":"code","95b69a3a":"code","7f077303":"code","fd2c03d9":"markdown","5f532843":"markdown","48289225":"markdown","e9a86fae":"markdown","dbc2dad2":"markdown","2ba5b0ce":"markdown","153b396f":"markdown","5a536ec0":"markdown","7798225d":"markdown","267072d9":"markdown","9e8f0d7a":"markdown","12bfa208":"markdown","1addab06":"markdown"},"source":{"108f9895":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport re\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,f1_score, confusion_matrix\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers","082ed5a0":"df = pd.read_csv(\"\/kaggle\/input\/amazon-fine-food-reviews\/Reviews.csv\")\nprint(\"Szie of the data : \", df.shape)","606785bb":"df.head()","b7806c03":"df_filtered = df[df[\"Score\"]!=3]\ndf_filtered.shape","3288d273":"df_filtered[\"Score\"] = df_filtered[\"Score\"].apply(lambda x : 1 if x>3 else 0)","66b0cb69":"sorted_data=df_filtered.sort_values('ProductId', kind='quicksort', na_position='last')","a0a33533":"final_df=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"}, keep='first', inplace=False)\nfinal_df.shape","49d1513a":"final_df=final_df[final_df[\"HelpfulnessNumerator\"]<=final_df[\"HelpfulnessDenominator\"]]\nfinal_df.shape","9507fff2":"final_df['Score'].value_counts()","c35942f0":"plt.figure(figsize = (10,7))\nsns.countplot(final_df['Score'])\nplt.title(\"Bar plot of sentiments\")","01ed932a":"def decontract(text):\n    text = re.sub(r\"won\\'t\", \"will not\", text)\n    text = re.sub(r\"can\\'t\", \"can not\", text)\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    return text","f0d65a54":"#set of custom stop words\nstop_words= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"])","74dd2467":"lemmatizer = WordNetLemmatizer()","482a9965":"def preprocess_text(review):\n    review = re.sub(r\"http\\S+\", \"\", review)             # removing website links\n    review = BeautifulSoup(review, 'lxml').get_text()   # removing html tags\n    review = decontract(review)                         # decontracting\n    review = re.sub(\"\\S*\\d\\S*\", \"\", review).strip()     # removing the words with numeric digits\n    review = re.sub('[^A-Za-z]+', ' ', review)          # removing non-word characters\n    review = review.lower()                             # converting to lower case\n    review = [word for word in review.split(\" \") if not word in stop_words] # removing stop words\n    review = [lemmatizer.lemmatize(token, \"v\") for token in review] #Lemmatization\n    review = \" \".join(review)\n    review.strip()\n    return review\nfinal_df['Text'] = final_df['Text'].apply(lambda x: preprocess_text(x))","4e946a34":"final_df['Text'].head()","21c2f79b":"train_df, test_df = train_test_split(final_df, test_size = 0.2, random_state = 42)\nprint(\"Training data size : \", train_df.shape)\nprint(\"Test data size : \", test_df.shape)","8f7fc1e6":"top_words = 6000\ntokenizer = Tokenizer(num_words=top_words)\ntokenizer.fit_on_texts(train_df['Text'])\nlist_tokenized_train = tokenizer.texts_to_sequences(train_df['Text'])\n\nmax_review_length = 130\nX_train = pad_sequences(list_tokenized_train, maxlen=max_review_length)\ny_train = train_df['Score']","6f758cb3":"embedding_vecor_length = 32\nmodel = Sequential()\nmodel.add(Embedding(top_words+1, embedding_vecor_length, input_length=max_review_length))\nmodel.add(LSTM(100))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","c860074f":"model.fit(X_train,y_train, nb_epoch=3, batch_size=64, validation_split=0.2)","8bac78f5":"list_tokenized_test = tokenizer.texts_to_sequences(test_df['Text'])\nX_test = pad_sequences(list_tokenized_test, maxlen=max_review_length)\ny_test = test_df['Score']\nprediction = model.predict(X_test)\ny_pred = (prediction > 0.5)\nprint(\"Accuracy of the model : \", accuracy_score(y_pred, y_test))\nprint('F1-score: ', f1_score(y_pred, y_test))\nprint('Confusion matrix:')\nconfusion_matrix(y_test,y_pred)","7e11f1e8":"from sklearn.externals import joblib","e25cae38":"joblib.dump(model, 'model.pkl')","8c331d9f":"joblib.dump(tokenizer, 'tokenizer.pkl')","95b69a3a":"from IPython.display import FileLink\nFileLink(r'model.pkl')","7f077303":"FileLink(r'tokenizer.pkl')","fd2c03d9":"#### 6.2 Evaluating model performance on test data","5f532843":"### Notebook - Table of Content\n\n1. [**Importing necessary libraries**](#1.-Importing-necessary-libraries)   \n2. [**Loading data**](#2.-Loading-data) \n3. [**Data preprocessing**](#3.-Data-preprocessing)  \n    3.1 [**Removing all the records with Score = 3**](#3.1-Removing-all-the-records-with-Score-=-3)   \n    3.2 [**Defining sentiments based on score values**](#3.2-Defining-sentiments-based-on-score-values)   \n    3.3 [**Checking for duplicate records**](#3.3-Checking-for-duplicate-records)   \n    3.4 [**Checking Data consistency of HelpfulnessNumerator and HelpfulnessDenominator feature**](#3.4-Checking-Data-consistency-of-HelpfulnessNumerator-and-HelpfulnessDenominator-feature)  \n    3.5 [**Checking for class imbalance**](#3.5-Checking-for-class-imbalance)  \n4. [**Text preprocessing**](#4.-Text-preprocessing)  \n5. [**Splitting into train and test set with 80:20 ratio**](#5.-Splitting-into-train-and-test-set-with-80:20-ratio) \n6. [**Model Building**](#6.-Model-Building)   \n    6.1 [**Fitting LSTM with Embedding layer**](#6.1-Fitting-LSTM-with-Embedding-layer)  \n    6.2 [**Evaluating model performance on test data**](#6.2-Evaluating-model-performance-on-test-data)  ","48289225":"#### 3.4 Checking Data consistency of HelpfulnessNumerator and HelpfulnessDenominator feature","e9a86fae":"#### 3.5 Checking for class imbalance","dbc2dad2":"### 1. Importing necessary libraries ","2ba5b0ce":"**Additional NOTE**\n\nIf you are interested in learning or exploring more about importance of feature selection in machine learning, then refer to my below blog offering.\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/a-comprehensive-guide-to-feature-selection-using-wrapper-methods-in-python\/","153b396f":"### 4. Text preprocessing\n\nIn this step, following operations are performed on the review text\n\n* Removing website links\n* Removing html tags\n* Decontracting(expanding from the original form)\n* Removing the words with numeric digits\n* Removing non-word characters\n* Converting to lower case\n* Removing stop words\n* Performing Lemmatization","5a536ec0":"### 2. Loading data","7798225d":"### 6. Model Building\n\n#### 6.1 Fitting LSTM with Embedding layer","267072d9":"### 5. Splitting into train and test set with 80:20 ratio","9e8f0d7a":"#### 3.3 Checking for duplicate records","12bfa208":"#### 3.2 Defining sentiments based on score values","1addab06":"### 3. Data preprocessing\n\n#### 3.1 Removing all the records with Score = 3"}}