{"cell_type":{"a8c50dc3":"code","cebd2ccb":"code","db26db78":"code","169dbacb":"code","d1b55eee":"code","0b11a39d":"code","ae134966":"code","6d4c523e":"code","7263354e":"code","bd8af837":"code","696cd510":"code","01de0296":"code","321820ee":"code","c824458a":"code","242ed216":"code","8640bee5":"code","2ca759a2":"code","53569ece":"code","ea78cae8":"code","6ef0165b":"code","4c6c273e":"code","176e2bc2":"code","78e9e721":"code","70024f3b":"code","5ec4d616":"code","be7693a5":"code","7937fdeb":"code","a7cc01b3":"code","b74fb571":"code","a73c1d2f":"code","67be9e99":"code","543e2fdc":"code","0e05abdc":"markdown","5c99c721":"markdown","0e50aeee":"markdown","b153b5ee":"markdown","6d3549df":"markdown","7c6afbfd":"markdown","d2bf33b7":"markdown","6d9fe810":"markdown","38e3c309":"markdown","9e498410":"markdown"},"source":{"a8c50dc3":"!pip install --upgrade git+https:\/\/github.com\/stanfordmlgroup\/ngboost.git","cebd2ccb":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","db26db78":"df_train = pd.read_csv('..\/input\/predicting-energy-consumption-of-turkey-lightgbm\/energy_cons_train.csv', parse_dates = ['Date'])\ndf_test = pd.read_csv('..\/input\/predicting-energy-consumption-of-turkey-lightgbm\/energy_cons_test.csv', parse_dates = ['Date'])","169dbacb":"df_train.info()","d1b55eee":"df_test.info()","0b11a39d":"df_train.head()","ae134966":"df_train[['Consumption (MWh)']].describe()","6d4c523e":"df_train[df_train['Consumption (MWh)'] == 0] # I received an overflow error while training the model, I suspected some zeros may be the reason, yes found it!","7263354e":"#df_train = df_train.loc['2016-03-28':, :].reset_index()","bd8af837":"df_test[['Consumption (MWh)']].describe()","696cd510":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=df_train.Date, y=np.expm1(df_train['Consumption (MWh)']),\n                    mode='markers',\n                    name='TRAIN SET'))\nfig.add_trace(go.Scatter(x=df_test.Date, y=np.expm1(df_test['Consumption (MWh)']),\n                    mode='markers',\n                    name='TEST SET'))\nfig.show()","01de0296":"def percentage_error(actual, predicted):\n    res = np.empty(actual.shape)\n    for j in range(actual.shape[0]):\n        if actual[j] != 0:\n            res[j] = (actual[j] - predicted[j]) \/ actual[j]\n        else:\n            res[j] = predicted[j] \/ np.mean(actual)\n    return res\n\ndef mean_absolute_percentage_error(y_true, y_pred): \n    return np.mean(np.abs(percentage_error(np.asarray(y_true), np.asarray(y_pred)))) * 100","321820ee":"print(df_train.shape, df_test.shape) ","c824458a":"y_train = df_train['Consumption (MWh)'].values\nX_train = df_train.drop(['Date','Consumption (MWh)'], axis=1).values\n\ny_test = df_test['Consumption (MWh)'].values\nX_test = df_test.drop(['Date','Consumption (MWh)'], axis=1).values","242ed216":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom ngboost import NGBRegressor\nfrom ngboost.learners import default_tree_learner\nfrom ngboost.distns import Normal\nfrom ngboost.scores import MLE","8640bee5":"model_ngb = NGBRegressor(n_estimators=650,  natural_gradient=True, learning_rate = 0.01,Base=default_tree_learner, Dist=Normal, Score=MLE, verbose_eval =50)\nmodel_ngb.fit(X_train, y_train, X_val = X_test, Y_val = y_test, early_stopping_rounds = 10)","2ca759a2":"y_train_ngb = model_ngb.pred_dist(X_train)\nprint(\"Train set RMSE (Log): \" + str(np.sqrt(mean_squared_error(y_train_ngb.loc, y_train))))\nprint(\"Train set MAPE (Log): \" + str(mean_absolute_percentage_error(y_train, y_train_ngb.loc)))\nprint(\"Train set RMSE (Non-Log): \" + str(np.sqrt(mean_squared_error(np.expm1(y_train_ngb.loc), np.expm1(y_train)))))\nprint(\"Train set MAPE (Non-Log): \" + str(mean_absolute_percentage_error(np.expm1(y_train), np.expm1(y_train_ngb.loc))))","53569ece":"y_test_ngb= model_ngb.pred_dist(X_test)\nprint(\"Validation set RMSE (Log): \" + str(np.sqrt(mean_squared_error(y_test_ngb.loc, y_test))))\nprint(\"Validation set MAPE (Log): \" + str(mean_absolute_percentage_error(y_test, y_test_ngb.loc)))\nprint(\"Validation set RMSE (Non-Log): \" + str(np.sqrt(mean_squared_error(np.expm1(y_test_ngb.loc), np.expm1(y_test)))))\nprint(\"Validation set MAPE (Non-Log): \" + str(mean_absolute_percentage_error(np.expm1(y_test), np.expm1(y_test_ngb.loc))))","ea78cae8":"predictions = pd.DataFrame(y_test_ngb.loc, columns=['Predictions'])\npredictions_upper = pd.DataFrame(y_test_ngb.dist.interval(0.95)[1], columns=['Predictions_upper'])\npredictions_lower = pd.DataFrame(y_test_ngb.dist.interval(0.95)[0], columns=['Predictions_lower'])","6ef0165b":"df_figure = pd.concat([df_test[['Date', 'Consumption (MWh)']], predictions, predictions_lower, predictions_upper], axis=1)\ndf_figure","4c6c273e":"df_figure.isna().sum()","176e2bc2":"df_figure[['Consumption (MWh)','Predictions', 'Predictions_lower', 'Predictions_upper']] = np.expm1(df_figure[['Consumption (MWh)','Predictions', 'Predictions_lower', 'Predictions_upper']])","78e9e721":"df_figure","70024f3b":"def plot_results(df, title):\n    fig, ax = plt.subplots(figsize=(22, 5))\n    plt.plot(df.Date, df.Predictions, label = 'Consumption_Predicted', color='b', lw=2)\n    plt.fill_between(df.Date, df.Predictions_lower,  df.Predictions_upper,label = '95% Prediction Interval', color='gray', alpha=0.5)\n    plt.scatter(df.Date, df['Consumption (MWh)'], label = 'Consumption (MWh) Actual', color='g', lw=3)\n    ax.legend(fontsize = 14)\n    plt.title('Hourly Power Consumption Actual vs. Predicted Values with Prediction Intervals')\n    plt.xlabel(title)\n    plt.show()","5ec4d616":"print(df_test.Date.min(), df_test.Date.max())","be7693a5":"df_figure_fw = df_figure.set_index('Date').loc['2020-01-01 00:00:00':'2020-01-08 00:00:00', :].reset_index()\ndf_figure_fw","7937fdeb":"plot_results(df_figure_fw, 'First Week in Test Set')","a7cc01b3":"df_figure_fm = df_figure.set_index('Date').loc['2020-01-01 00:00:00':'2020-02-15 00:00:00', :].reset_index()\ndf_figure_fm","b74fb571":"plot_results(df_figure_fm, 'First Month in Test Set')","a73c1d2f":"df_figure_lm = df_figure.set_index('Date').loc['2020-02-15 00:00:00':'2020-03-14 23:00:00', :].reset_index()\ndf_figure_lm","67be9e99":"plot_results(df_figure_lm, 'First Month in Test Set')","543e2fdc":"import plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=df_figure.Date, y=df_figure.Predictions,\n    fill=None,\n    mode='lines',\n    line_color='blue',\n    name='Predictions'\n    ))\nfig.add_trace(go.Scatter(x=df_figure.Date, y=df_figure.Predictions_lower,\n    fill=None,\n    mode='lines',\n    line_color='gray',\n    name='Predictions_Lower',\n    ))\nfig.add_trace(go.Scatter(\n    x=df_figure.Date, y=df_figure.Predictions_upper,\n    fill='tonexty', # fill area between trace0 and trace1\n    mode='lines', line_color='gray',name='Predictions_Upper'\n    ))\nfig.add_trace(go.Scatter(x=df_figure.Date, y=df_figure['Consumption (MWh)'],\n    fill=None,\n    mode='markers',\n    line_color='green',\n    name='Actual Consumption'\n    ))\n\nfig.show()","0e05abdc":"This Notebook aims to train an NGBoost Model for the same Energy Consumption Prediction Problem [I have attempted in this Notebook with LightGBM](https:\/\/www.kaggle.com\/hgultekin\/predicting-energy-consumption-of-turkey-lightgbm). Before going further into the code, please familiarize yourself with this novel boosting algorithm, which makes probabilistic predictions with Gradient Boosting, through below references. There are these great sources detailing the Natural Gradient Boosting (NGBoost) including [the official research paper](https:\/\/arxiv.org\/abs\/1910.03225) and the [towardsthedatascience article](https:\/\/towardsdatascience.com\/interpreting-the-probabilistic-predictions-from-ngboost-868d6f3770b2) by one of the authors (Alejandro Schuler), therefore I will not repeat the process here in this notebook:","5c99c721":"## Or with my favorite visualization tool :)","0e50aeee":"# Energy Consumption Prediction with NGBoost","b153b5ee":"## First Week Predictions:","6d3549df":"## Last Month Predictions","7c6afbfd":"![resim.png](attachment:resim.png)","d2bf33b7":"**Final Thoughts**: NGboost seems to give more accurate results for this particular problem w.r.t. the LightGBM, along with an additional flexibility of easily generating the prediction intervals along with the predictions. Based on my multiple observations of comparing the two boosting algorithms, I can say that the fact that the LightGBM faster and NGboost is slightly more accurate (for similar hyperparameters) usually holds true.","6d9fe810":"## First Month Predictions:","38e3c309":"![resim.png](attachment:resim.png)\n","9e498410":"**References**:\n\n* https:\/\/stanfordmlgroup.github.io\/projects\/ngboost\/\n* https:\/\/github.com\/stanfordmlgroup\/ngboost\n* https:\/\/arxiv.org\/abs\/1910.03225\n* https:\/\/towardsdatascience.com\/interpreting-the-probabilistic-predictions-from-ngboost-868d6f3770b2\n* https:\/\/towardsdatascience.com\/ngboost-explained-comparison-to-lightgbm-and-xgboost-fda510903e53\n"}}