{"cell_type":{"1be6628d":"code","3ef26732":"code","45750ddc":"code","ef854e77":"code","ff8eb91b":"code","11d68840":"code","7ce3d70b":"code","e3633207":"code","a0c12316":"code","b322c770":"code","0f9f4c58":"code","a6b5b0cd":"code","225d1f96":"code","e1585440":"code","13e29add":"code","3c45e194":"code","79a345e2":"code","b57c06b4":"code","11fd22a9":"code","6f62bb11":"code","594e59a4":"code","df0256b2":"code","cd84369c":"code","2afbc6df":"code","4495eefd":"code","97a41a4a":"code","782cbea8":"code","25abfa5e":"code","3adce875":"code","6c8b6564":"code","f9c0a495":"code","877e31e1":"code","878abec3":"code","de002d79":"code","c3c78011":"code","64d8e411":"code","c67ee0c7":"code","5303c5fd":"code","3a2ab3f3":"code","56f44a49":"code","6a336a4b":"code","0e3b92ef":"code","9e76cee3":"code","49f024f9":"code","5384d8b6":"code","e6eceb15":"code","420c0434":"code","753e5fb0":"markdown","990e668b":"markdown","ac71c4cc":"markdown","1e21e45b":"markdown","24bf0eb4":"markdown","a5e96159":"markdown"},"source":{"1be6628d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3ef26732":"from sklearn.datasets import load_boston","45750ddc":"import matplotlib.pyplot as plt\nimport seaborn as sns","ef854e77":"boston_data = load_boston()","ff8eb91b":"print(boston_data.data)","11d68840":"boston_data.feature_names","7ce3d70b":"df= pd.DataFrame(data=boston_data.data ,columns =boston_data.feature_names )","e3633207":"df.head()","a0c12316":"df.isnull().sum()","b322c770":"df.info()","0f9f4c58":"df.describe()","a6b5b0cd":"df.corr()","225d1f96":"plt.figure(figsize =(16,10))\nsns.heatmap(df.corr(), annot = True, cmap=\"plasma\",  annot_kws ={'size':14} )\nplt.show()","e1585440":"masker = np.zeros_like(df.corr())\ntriangle_indices = np.triu_indices_from(masker)\nmasker[triangle_indices] = True \nplt.figure(figsize =(16,10))\nsns.heatmap(df.corr(), annot = True, annot_kws ={'size':14}, mask = masker )\nplt.xticks(fontsize = 10)\nplt.yticks(fontsize = 10)\nplt.show()","13e29add":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression","3c45e194":"df['PRICE']=boston_data.target","79a345e2":"df.head()","b57c06b4":"prices = df['PRICE'] # Target\nfeatures = df.drop(['PRICE'], axis = 1 )","11fd22a9":"X_train, X_test, y_train, y_test = train_test_split(features, prices, test_size = 0.2, random_state = 88888)\nlen(X_train)\/len(features)","6f62bb11":"regr = LinearRegression()\nregr.fit(X_train, y_train)","594e59a4":"regr.coef_","df0256b2":"regr.intercept_","cd84369c":"pd.DataFrame(data = regr.coef_, index = X_train.columns, columns =['Coef'] )","2afbc6df":"regr.score(X_train, y_train)","4495eefd":"regr.score(X_test, y_test)","97a41a4a":" from scipy.stats import norm","782cbea8":"df['PRICE'].skew()","25abfa5e":"y_log = np.log(df['PRICE'])","3adce875":"y_log.skew()","6c8b6564":"new_df =features\nnew_df['PRICE']=y_log\nnew_df","f9c0a495":"x=new_df.drop(['PRICE'],axis=True)\ny=new_df['PRICE']","877e31e1":"X_train, X_test, y_train, y_test = train_test_split(x, y, \n                                                    test_size=0.2, random_state=10)\n\nregr = LinearRegression()\nregr.fit(X_train, y_train)\n\nprint('Training data r-squared:', regr.score(X_train, y_train))\nprint('Test data r-squared:', regr.score(X_test, y_test))\n\n \npd.DataFrame(data=regr.coef_, index=X_train.columns, columns=['coef'])\n","878abec3":"import statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport warnings\nwarnings.simplefilter('ignore')","de002d79":"results = sm.OLS(y_train, sm.add_constant(X_train)).fit()\nprint(results.params, '\\n')\nprint(results.pvalues,'\\n')\npd.DataFrame({'coef': results.params, 'p-value': round(results.pvalues, 3)})","c3c78011":"variance_inflation_factor(exog=sm.add_constant(X_train).values, exog_idx=10)","64d8e411":"X_incl_const = sm.add_constant(X_train)\nvif = []\n\nfor i in range(X_incl_const.shape[1]):\n    \n    vif.append(variance_inflation_factor(exog=sm.add_constant(X_train).values, exog_idx=i))\n    \nprint(vif)","c67ee0c7":"pd.DataFrame({'coef_name': X_incl_const.columns, 'vif': np.around(vif, 2)})","5303c5fd":"X_incl_const = sm.add_constant(X_train)\n\nmodel = sm.OLS(y_train, X_incl_const)\n\nresults_1 = model.fit()\n\norg_coef = pd.DataFrame({'coef': results_1.params, 'p-value': round(results_1.pvalues, 3)})\n\nprint('BIC with all features and the log price is      :' , results_1.bic, '\\n')\n\nprint('r-squared is with all features and log price is :' , results_1.rsquared, '\\n')\n\nprint(org_coef) ","3a2ab3f3":"#Model without INDUS\nX_incl_const = sm.add_constant(X_train)\n\nX_incl_const = X_incl_const.drop(['INDUS'], axis =1)\n\nmodel = sm.OLS(y_train, X_incl_const)\n\nresults_2 = model.fit()\n\ncoef_minus_indus = pd.DataFrame({'coef': results_2.params, 'p-value': round(results_2.pvalues, 3)})\n\nprint('BIC without INDUS features and the log price is      :' , results_2.bic, '\\n')\n\nprint('r-squared without INDUS feature  and log price is    :' , results_2.rsquared, '\\n')\n\nprint(coef_minus_indus) ","56f44a49":"#Model without INDUS AND AGE\nX_incl_const = sm.add_constant(X_train)\n\nX_incl_const = X_incl_const.drop(['INDUS', 'AGE'], axis =1)\n\nmodel = sm.OLS(y_train, X_incl_const)\n\nresults_3 = model.fit()\n\ncoef_minus_indus_age = pd.DataFrame({'coef': results_3.params, 'p-value': round(results_3.pvalues, 3)})\n\nprint('BIC without INDUS features and the log price is      :' , results_3.bic, '\\n')\n\nprint('r-squared without INDUS feature  and log price is    :' , results_3.rsquared, '\\n')\n\nprint(coef_minus_indus_age) ","6a336a4b":"#RESIDIAL PLOT OF MODEL 3 THE LOG MODEL MINUS INDUS AND AGE\nplt.figure(figsize=(20,6))\nplt.scatter(x=results_3.fittedvalues, y=results_3.resid, c='brown', alpha=0.8)\nplt.xlabel('Predicted log prices $\\hat y _i$', fontsize=20)\nplt.ylabel('Residuals', fontsize=20)\nplt.title('Residuals vs Fitted Values of the reduced log model', fontsize=20)\n\nplt.show()\n","0e3b92ef":"# Mean Squared Error & R-Squared\nreduced_log_mse_minus_indus_minus_age = round(results_3.mse_resid, 3)\nreduced_log_rsquared_minus_indus_minus_age = round(results_3.rsquared, 3)","9e76cee3":"reduced_log_mse_minus_indus_minus_age","49f024f9":"reduced_log_rsquared_minus_indus_minus_age","5384d8b6":"resid_mean = round(results_3.resid.mean(), 3)\nresid_skew = round(results_3.resid.skew(), 3)","e6eceb15":"plt.figure(figsize= (20,6))\nsns.distplot(results_3.resid, color='navy')\nplt.title(f'Residual Dist of Log price model w\/o INDUS and AGE:residuals Skew ({resid_skew}) Mean ({resid_mean})',fontsize = 20)\nplt.show()","420c0434":"sd_1 = round(np.sqrt(results_3.mse_resid),3)\nsd_2 = 2*(round(np.sqrt(results_3.mse_resid),3))\nsd_3 = 3*(round(np.sqrt(results_3.mse_resid),3))\n\nprint('1 s.d. in log prices is', sd_1)\nprint('2 s.d. in log prices is', sd_2)\nprint('3 s.d. in log prices is', sd_3 )\n\nupper_bound = np.log(30) + sd_2\nprint('The upper bound in log prices for a 95% prediction interval is ', upper_bound)\nprint('The upper bound in normal prices is $', np.e**upper_bound * 1000)\n\nlower_bound = np.log(30) - sd_2\nprint('The lower bound in log prices for a 95% prediction interval is ', lower_bound)\nprint('The lower bound in normal prices is $', np.e**lower_bound * 1000)","753e5fb0":" #Our Estimate for a house is 30000....calculate the upper bound and the lower bound using the log model_minus_indus_minus_age","990e668b":"    Still the accuracy for testing is less .So we use stats model to find P value.\n    ","ac71c4cc":"P value is higher for indus and age","1e21e45b":"check whether VIF value is greater for any feature.Here all good.\n\nNow we will build a model with all features and log price\n","24bf0eb4":"Skewness reduced\n","a5e96159":"Score is bad\n"}}