{"cell_type":{"94d07755":"code","91b0cf2e":"code","32d9d49d":"code","3a41afeb":"code","97d6002f":"code","ac2b9b71":"code","076d618a":"code","4161802b":"code","ea1038b4":"code","f8c725d6":"code","44989c08":"code","6d3c62d6":"markdown","813c7da3":"markdown","956e0722":"markdown","4dde9a1e":"markdown","41c41f0c":"markdown","c259b12c":"markdown","aa76a533":"markdown","dd2d1c48":"markdown","47742423":"markdown","36cb3317":"markdown"},"source":{"94d07755":"# Let us import required libraries first\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.models import Sequential\nimport tensorflow.keras.utils as ku \nimport tensorflow\n\n# set seeds for reproducability\ntensorflow.random.set_seed(200)\n\nimport pandas as pd\nimport numpy as np\nimport string\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","91b0cf2e":"all_headlines = []\narticle_df = pd.read_csv('..\/input\/news-headlines\/ArticlesMarch2018.csv')\nall_headlines.extend(list(article_df.headline.values))\nall_headlines = [h for h in all_headlines if h != \"Unknown\"]\nlen(all_headlines)","32d9d49d":"all_headlines[:10]","3a41afeb":"def clean_text(txt):\n    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n    return txt \n\ncorpus = [clean_text(x) for x in all_headlines]\ncorpus[:10]","97d6002f":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\ntoken_list = tokenizer.texts_to_sequences([\"I am happy to see you here today\"])[0]\nprint(token_list)\n\ncheck=[]\n\nfor i in range(1, len(token_list)):\n  n_gram_sequence = token_list[:i+1]\n  check.append(n_gram_sequence)\n\ncheck","ac2b9b71":"tokenizer = Tokenizer()\n\ndef get_sequence_of_tokens(corpus):\n    ## tokenization\n    tokenizer.fit_on_texts(corpus)\n    total_words = len(tokenizer.word_index) + 1    \n    \n    ## convert data to sequence of tokens \n    input_sequences = []\n    for line in corpus:\n        token_list = tokenizer.texts_to_sequences([line])[0]\n        for i in range(1, len(token_list)):\n            n_gram_sequence = token_list[:i+1]\n            input_sequences.append(n_gram_sequence)\n    return input_sequences, total_words\n\ninp_sequences, total_words = get_sequence_of_tokens(corpus)\n\ninp_sequences[:10], total_words","076d618a":"def generate_padded_sequences(input_sequences):\n    max_sequence_len = max([len(x) for x in input_sequences])\n    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n    \n    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n    label = ku.to_categorical(label, num_classes=total_words)\n    return predictors, label, max_sequence_len\n\npredictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)\npredictors,label,len(label[0]),max_sequence_len","4161802b":"def create_model(max_sequence_len, total_words):\n    input_len = max_sequence_len - 1\n    model = Sequential()\n    \n    # Setting early_stopping feature to stop early on getting stagnant\n    early_stopping = EarlyStopping(\n    min_delta=0.01, # minimium amount of change to count as an improvement\n    patience=5, # how many epochs to wait before stopping\n    restore_best_weights=True,\n    )\n    \n    # Add Input Embedding Layer\n    model.add(Embedding(total_words, 32, input_length=input_len))\n   \n    # input_dim: Integer. Size of the vocabulary, i.e. maximum integer index + 1.\n    # output_dim: Integer. Dimension of the dense embedding.\n    # input_length: Length of input sequences, when it is constant. \n\n    # Add Hidden Layer 1 - RNN Layer\n    model.add(SimpleRNN(200))\n    \n    # Add Hidden Layer 2 - Dropout Layer\n    model.add(Dropout(0.1))\n    \n    # Add Output Layer\n    model.add(Dense(total_words, activation='softmax'))\n\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    \n    return model\n\nmodel = create_model(max_sequence_len, total_words)\nmodel.summary()","ea1038b4":"model.fit(predictors, label, epochs=200)","f8c725d6":"def generate_text(seed_text, next_words, model, max_sequence_len):\n    for _ in range(next_words):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n        predicted = model.predict_classes(token_list, verbose=0)\n        \n        output_word = \"\"\n        for word,index in tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word\n                break\n        seed_text += \" \"+output_word\n    return seed_text.title()","44989c08":"print (generate_text(\"jack \", 5, model, max_sequence_len))\nprint (generate_text(\"president trump\",5, model, max_sequence_len))\nprint (generate_text(\"donald\", 6, model, max_sequence_len))\nprint (generate_text(\"india and china\", 2, model, max_sequence_len))\nprint (generate_text(\"new york\", 4, model, max_sequence_len))\nprint (generate_text(\"science and technology\", 6, model, max_sequence_len))","6d3c62d6":"### 2. Load the dataset\n\nLoad the dataset of news headlines","813c7da3":"## 6. Some Results","956e0722":"#### 3.2 Generating Sequence of N-gram Tokens\nThe next step is Tokenization. Tokenization is a process of extracting tokens (terms \/ words) from a corpus. Python\u2019s library Keras has inbuilt model for tokenization which can be used to obtain the tokens and their index in the corpus. After this step, every text document in the dataset is converted into sequence of tokens. \n","4dde9a1e":"In the above output [1119, 1120], [1119, 1120,116], [1119, 1120, 116, 1121] and so on represents the ngram phrases generated from the input data. where every integer corresponds to the index of a particular word in the complete vocabulary of words present in the text. For example\n\n**Headline:** i stand  with the shedevils  \n**Ngrams:** | **Sequence of Tokens**\n\n<table>\n<tr><td>Ngram <\/td><td> Sequence of Tokens<\/td><\/tr>\n<tr> <td>i stand <\/td><td> [30, 507] <\/td><\/tr>\n<tr> <td>i stand with <\/td><td> [30, 507, 11] <\/td><\/tr>\n<tr> <td>i stand with the <\/td><td> [30, 507, 11, 1] <\/td><\/tr>\n<tr> <td>i stand with the shedevils <\/td><td> [30, 507, 11, 1, 975] <\/td><\/tr>\n<\/table>\n\n\n\n#### 3.3 Padding the Sequences and obtain Variables : Predictors and Target\nNow we have generated a data-set which contains sequence of tokens. Before starting training the model, we need to pad the sequences and make their lengths equal. We can use pad_sequence function of Kears for this purpose. To input this data into a learning model, we need to create predictors and label. For example:\n\n\nHeadline:  they are learning data science\n\n<table>\n<tr><td>PREDICTORS <\/td> <td>           LABEL <\/td><\/tr>\n<tr><td>they                   <\/td> <td>  are<\/td><\/tr>\n<tr><td>they are               <\/td> <td>  learning<\/td><\/tr>\n<tr><td>they are learning      <\/td> <td>  data<\/td><\/tr>\n<tr><td>they are learning data <\/td> <td>  science<\/td><\/tr>\n<\/table>","41c41f0c":"## Generating text for news headlines:\nLanguage Modelling is the core problem for a number of of natural language processing tasks such as speech to text, conversational system, and text summarization. A trained language model learns the likelihood of occurrence of a word\/ character based on the previous sequence of words\/ characters used in the text. Language models can be operated at character level, n-gram level, sentence level or even paragraph level. We will create a language model for predicting next word by implementing and training state-of-the-art Recurrent Neural Networks under Deep Learning. \n\nWe will use **Keras** library in python to implement our project.","c259b12c":"### 1. Import the libraries","aa76a533":"## 5. Generating the text \n\nGreat, our model architecture is now ready and we can train it using our data. Next lets write the function to predict the next word based on the input words (or seed text). We will first tokenize the seed text, pad the sequences and pass into the trained model to get predicted word. The multiple predicted words can be appended together to get predicted sequence.\n","dd2d1c48":"### 3. Dataset preparation\n\n#### 3.1 Dataset cleaning \n\nIn dataset preparation step, we will first perform text cleaning of the data which includes removal of punctuations and lower casing all the words. ","47742423":"Lets train our model now","36cb3317":"Perfect, now we can obtain the input vector X (predictors) and the label vector Y (label) which can be used for the training purposes. Now we will create RNN model for out data.\n\n### 4. RNN for Text Generation\nUnlike Feed-forward neural networks in which activation outputs are propagated only in one direction, the activation outputs from neurons propagate in both directions (from inputs to outputs and from outputs to inputs) in Recurrent Neural Networks. This creates loops in the neural network architecture which acts as a \u2018memory state\u2019 of the neurons. This state allows the neurons an ability to remember what have been learned so far.\n\nThe memory state in RNNs gives an advantage over traditional neural networks. Lets architecture a RNN model in our code. We have added total three layers in the model.\n\n1. Input Layer : Takes the sequence of words as input\n2. RNN Layer : Computes the output using RNN units. We have added 200 units in the layer.\n3. Dropout Layer : A regularisation layer which randomly turns-off the activations of some neurons in the RNN layer. It helps in preventing over fitting. \n4. Output Layer : Computes the probability of the best possible next word as output\n\nWe will run this model for total 100 epoochs but it can be experimented further."}}