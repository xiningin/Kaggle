{"cell_type":{"ab4eb482":"code","4e4968ae":"code","c10b1eac":"code","b9b46e9d":"code","66e52c72":"code","0a3e173b":"code","d454060a":"code","a1218bc8":"code","0e1b953c":"code","1fb7057d":"code","d8437169":"code","9e65bbf7":"code","c93f8da9":"code","2e23c831":"code","79cea7b3":"code","bd1e88f0":"code","16a20667":"code","1055705f":"code","2844f3bb":"code","9076f7e8":"code","804f6968":"code","6f66f33b":"code","4660290d":"code","d3f719a7":"code","9a0fa46d":"markdown","0d5f8a9a":"markdown","610dcbd9":"markdown","2ad71957":"markdown","14cb76a5":"markdown","536831e9":"markdown","d78f1d01":"markdown","ce6e28ba":"markdown","41747e2a":"markdown","5bbd3d9b":"markdown","495233fa":"markdown","89931806":"markdown","e97d1da6":"markdown","0fa5f821":"markdown","e4146c1d":"markdown","2484d064":"markdown","ecece20b":"markdown","e63bcf84":"markdown","07024848":"markdown","0cff472d":"markdown","acc83000":"markdown"},"source":{"ab4eb482":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport string\n\nplt.style.use('seaborn')\nplt.rcParams['lines.linewidth'] = 1\n\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nfrom sklearn.metrics import f1_score\n\nNBR_STAR=70\n\nX = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ny = X[\"target\"]\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\n\nX.head()","4e4968ae":"X['keyword'].fillna('',inplace=True)\nX['keyword'] = X['keyword'].map(lambda x:x.replace('%20', ' '))\ntest['keyword'].fillna('',inplace=True)\ntest['keyword'] = test['keyword'].map(lambda x:x.replace('%20', ' '))\n\n# source https:\/\/www.kaggle.com\/sahib12\/nlp-starter-for-beginners\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n#X['text'] = X['text'].apply(lambda x: clean_text(x))\n#test['text'] = test['text'].apply(lambda x: clean_text(x))\n","c10b1eac":"# list of stop words\nstop_word = list(ENGLISH_STOP_WORDS)\nstop_word.append('http')\nstop_word.append('https')\nstop_word.append('\u00fb_')","b9b46e9d":"X['target'].value_counts().plot(kind = 'barh')\nplt.show()","66e52c72":"def plot_sample_length_distribution(sample_texts):\n    plt.figure(figsize=(10,10))\n    plt.hist([len(s) for s in sample_texts], 50)\n    plt.xlabel('Length of a sample')\n    plt.ylabel('Number of samples')\n    plt.title('Sample length distribution')\n    plt.show()\n\nplot_sample_length_distribution(X['text'])","0a3e173b":"keyword_stats = X.groupby('keyword').agg({'text':np.size, 'target':np.mean}).rename(columns={'text':'Count', 'target':'Disaster Probability'})\nkeywords_disaster = keyword_stats.loc[keyword_stats['Disaster Probability']==1]\nkeywords_no_disaster  = keyword_stats.loc[keyword_stats['Disaster Probability']==0]\nkeyword_stats.sort_values('Disaster Probability', ascending=False).head(10)","d454060a":"from wordcloud import WordCloud, STOPWORDS\n\nSTOPWORDS.add('http')  \nSTOPWORDS.add('https')  \nSTOPWORDS.add('CO')  \nSTOPWORDS.add('\u00fb_')\nno_disaster_text = \" \".join(X[X[\"target\"] == 0].text.to_numpy().tolist())\nreal_disaster_text = \" \".join(X[X[\"target\"] == 1].text.to_numpy().tolist())\n\nno_disaster_cloud = WordCloud(stopwords=stop_word, background_color=\"white\").generate(no_disaster_text)\nreal_disaster_cloud = WordCloud(stopwords=stop_word, background_color=\"white\").generate(real_disaster_text)\n\ndef show_word_cloud(cloud, title):\n  plt.figure(figsize = (16, 10))\n  plt.imshow(cloud, interpolation='bilinear')\n  plt.title(title)\n  plt.axis(\"off\")\n  plt.show();\n\nshow_word_cloud(no_disaster_cloud, \"No disaster common words\")\nshow_word_cloud(real_disaster_cloud, \"Real disaster common words\")","a1218bc8":"vect = CountVectorizer(min_df=2,ngram_range=(1, 2), stop_words=stop_word)\nX_train = vect.fit_transform(X['text'])\nprint(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))","0e1b953c":"all_ngrams = list(vect.get_feature_names())\nnum_ngrams = 50\n\nall_counts = X_train.sum(axis=0).tolist()[0]\nall_counts, all_ngrams = zip(*[(c, n) for c, n in sorted(\n    zip(all_counts, all_ngrams), reverse=True)])\nngrams = list(all_ngrams)[:num_ngrams]\ncounts = list(all_counts)[:num_ngrams]\n\nidx = np.arange(num_ngrams)\nplt.figure(figsize=(10,10))\nplt.barh(idx, counts,  color='orange')\nplt.ylabel('N-grams')\nplt.xlabel('Frequencies')\nplt.title('Frequency distribution of n-grams')\nplt.yticks(idx, ngrams)\nplt.show()","1fb7057d":"# First try with this bag of word and a logistic regression\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import ShuffleSplit\n\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=123)\nscores = cross_val_score(LogisticRegression(), X_train, y,scoring=\"f1\", cv=cv)\nprint(\"*\"*NBR_STAR+\"\\n LogisticRegression on bag of word - cross-validation f1_score: {:.5f}\\n\".format(np.mean(scores))+\"*\"*NBR_STAR)","d8437169":"from sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer(#max_df=0.1,         # drop words that occur in more than X percent of documents\n                             min_df=10,      # only use words that appear at least X times\n                             stop_words='english', # remove stop words\n                             lowercase=True, # Convert everything to lower case \n                             use_idf=True,   # Use idf\n                             norm=u'l2',     # Normalization\n                             smooth_idf=True, # Prevents divide-by-zero errors\n                             ngram_range=(1,3)\n                            )\nX_train = vect.fit_transform(X['text'])\n# find maximum value for each of the features over dataset:\nmax_value = X_train.max(axis=0).toarray().ravel()\nsorted_by_tfidf = max_value.argsort()\n# get feature names\nfeature_names = np.array(vect.get_feature_names())\n\nprint(\"Features with lowest tfidf:\\n{}\".format(\n      feature_names[sorted_by_tfidf[:20]]))\n\nprint(\"Features with highest tfidf: \\n{}\".format(\n      feature_names[sorted_by_tfidf[-20:]]))","9e65bbf7":"scores = cross_val_score(LogisticRegression(), X_train, y,scoring=\"f1\", cv=cv)\nprint(\"*\"*NBR_STAR+\"\\n LogisticRegression with tfidf - cross-validation f1_score: {:.5f}\\n\".format(np.mean(scores))+\"*\"*NBR_STAR)","c93f8da9":"from matplotlib.colors import ListedColormap, colorConverter, LinearSegmentedColormap\n\ndef visualize_coefficients(coefficients, feature_names, n_top_features=25):\n    coefficients = coefficients.squeeze()\n    if coefficients.ndim > 1:\n        # this is not a row or column vector\n        raise ValueError(\"coeffients must be 1d array or column vector, got\"\n                         \" shape {}\".format(coefficients.shape))\n    coefficients = coefficients.ravel()\n\n    if len(coefficients) != len(feature_names):\n        raise ValueError(\"Number of coefficients {} doesn't match number of\"\n                         \"feature names {}.\".format(len(coefficients),\n                                                    len(feature_names)))\n    # get coefficients with large absolute values\n    coef = coefficients.ravel()\n    positive_coefficients = np.argsort(coef)[-n_top_features:]\n    negative_coefficients = np.argsort(coef)[:n_top_features]\n    interesting_coefficients = np.hstack([negative_coefficients,\n                                          positive_coefficients])\n    # plot them\n    plt.figure(figsize=(20, 7))\n    cm = ListedColormap(['#0000aa', '#ff2020'])\n    colors = [cm(1) if c < 0 else cm(0)\n              for c in coef[interesting_coefficients]]\n    plt.bar(np.arange(2 * n_top_features), coef[interesting_coefficients],\n            color=colors)\n    feature_names = np.array(feature_names)\n    plt.subplots_adjust(bottom=0.3)\n    plt.xticks(np.arange(1, 1 + 2 * n_top_features),\n               feature_names[interesting_coefficients], rotation=60,\n               ha=\"right\")\n    plt.ylabel(\"Coefficient magnitude\")\n    plt.xlabel(\"Words\")\n","2e23c831":"logreg = LogisticRegression()\nlogreg.fit(X_train, y)\ny_predict = logreg.predict(X_train)\nprint(\"*\"*NBR_STAR+\"\\n LogisticRegression with tfidf, no cross validation f1_score: {:.5f}\\n\".format(f1_score(y, y_predict, average='weighted'))+\"*\"*NBR_STAR)\nvisualize_coefficients(logreg.coef_, feature_names, n_top_features=50)","79cea7b3":"from tensorflow.python.keras import models\nfrom tensorflow.python.keras.layers import Dense\nfrom tensorflow.python.keras.layers import Dropout\nimport tensorflow as tf\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif","bd1e88f0":"train_texts, val_texts, train_labels , val_labels = train_test_split(\n    X['text'].values, X[\"target\"].values, test_size=0.10, random_state=42)","16a20667":"vectorizer = TfidfVectorizer(\n                             min_df=2,      # only use words that appear at least X times\n                             #stop_words='english', # remove stop words\n                             #lowercase=True, # Convert everything to lower case \n                             use_idf=True,   # Use idf\n                             norm=u'l2',     # Normalization\n                             smooth_idf=True, # Prevents divide-by-zero errors\n                             ngram_range=(1,3),\n                             #dtype='int32',\n                             analyzer='word',\n                             strip_accents = 'unicode',\n                             decode_error = 'replace'\n                            )\nx_train = vectorizer.fit_transform(train_texts)\nx_val = vectorizer.transform(val_texts)","1055705f":"selector = SelectKBest(f_classif, k=min(10000, x_train.shape[1]))\nselector.fit(x_train, train_labels)\nx_train = selector.transform(x_train)\nx_val = selector.transform(x_val)\n\nx_train = x_train.astype('float32')\nx_val = x_val.astype('float32')","2844f3bb":"# model parameters\nlearning_rate=1e-4\nepochs=1000\nbatch_size=128\nlayers=2\nunits=64\ndropout_rate=0.2\n\nmodel = models.Sequential()\nmodel.add(Dropout(rate=dropout_rate, input_shape=x_train.shape[1:]))\n\nfor _ in range(layers-1):\n    model.add(Dense(units=units, activation='relu'))\n    model.add(Dropout(rate=dropout_rate))\n\nmodel.add(Dense(units=1, activation='sigmoid'))","9076f7e8":"loss = 'binary_crossentropy'\noptimizer = tf.keras.optimizers.Adam(lr=learning_rate)\nmodel.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n\n# Create callback for early stopping on validation loss. If the loss does\n# not decrease in two consecutive tries, stop training.\ncallbacks = [tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', patience=2)]\n\n# Train and validate model.\nhistory = model.fit(\n        x_train.toarray(),\n        train_labels,\n        epochs=epochs,\n        callbacks=callbacks,\n        validation_data=(x_val.toarray(), val_labels),\n        verbose=0,  # Logs once per epoch.\n        batch_size=batch_size)\n\n# Print results.\nhistory = history.history\nprint('Validation accuracy: {acc}, loss: {loss}'.format(\n        acc=history['val_acc'][-1], loss=history['val_loss'][-1]))","804f6968":"plt.plot(history['loss'])\nplt.plot(history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","6f66f33b":"x_all = vectorizer.transform(X['text'].values)\nx_all = selector.transform(x_all)\ny_predict = model.predict_classes(x_all.toarray())\nfrom sklearn.metrics import f1_score\n\nscore = f1_score(y, y_predict, average='weighted')\nprint(\"*\"*NBR_STAR+\"\\n MLP Model f1_score: {:.5f}\\n\".format(score)+\"*\"*NBR_STAR)","4660290d":"y_predict[X.loc[X['keyword'].isin(list(keywords_disaster.index) )].index]=1\ny_predict[X.loc[X['keyword'].isin(list(keywords_no_disaster.index) )].index]=0\nscore = f1_score(y, y_predict, average='weighted')\nprint(\"*\"*NBR_STAR+\"\\n MLP Model f1_score: {:.5f}\\n\".format(score)+\"*\"*NBR_STAR)","d3f719a7":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\ntest_all = vectorizer.transform(test['text'].values)\ntest_all = selector.transform(test_all)\n\ny_predict = model.predict_classes(test_all.toarray())\ny_predict[test.loc[test['keyword'].isin(list(keywords_disaster.index) )].index]=1\ny_predict[test.loc[test['keyword'].isin(list(keywords_no_disaster.index) )].index]=0\n\nsample_submission[\"target\"] = y_predict\nsample_submission.to_csv(\"submission.csv\", index=False)\nsample_submission.head()","9a0fa46d":"# Wordcloud","0d5f8a9a":"# Improve (I hope so...) prediction with column keyword\nThe idea is simple, for instance, in train data, if the keyword is \"wreckage\", this is always a disaster, so we can force the prediction to 1 ...","610dcbd9":"<div class=\"alert alert-block alert-info\"><b><span>&#171;<\/span> please don't forget to upvote, that will keep me motivated <span>&#187;<\/span><\/b><\/div> \n","2ad71957":"### Build the model","14cb76a5":"# Apply bag-of-words to the dataset  -  ngram_range=(1, 2)","536831e9":"### Vectorize train and val texts.","d78f1d01":"# Build an MLP model with (1,2) n_grams","ce6e28ba":"# Class balance\n<span>&#171;<\/span> \nA given tweet is about a real disaster (target=1) or not (target=0)\n<span>&#187;<\/span>","41747e2a":"# Prediction from term frequency-inverse document frequency\n","5bbd3d9b":"# List of stop words","495233fa":"### Train the model","89931806":"# Explore the column keyword","e97d1da6":"# Plots the frequency distribution of first 50 n-grams","0fa5f821":"### Select top 'k' of the vectorized features. top_k = 10000","e4146c1d":"# Submission","2484d064":"# Load librairies and Data","ecece20b":"# Data cleaning\nI've not written the function clean_text, the writter is Sahib here https:\/\/www.kaggle.com\/sahib12\/nlp-starter-for-beginners","e63bcf84":"# Plots the text length distribution.","07024848":"# First try with this bag of word and a logistic regression","0cff472d":"### F1_SCORE","acc83000":" \n# Visualize the largest (most positive) and smallest (most negative)  n_top_features coefficients of the logistic regression\n<span>&#171;<\/span>\nI have found this visualization in this book : **\"Introduction to Machine Learning with Python\" by [Andreas Mueller](http:\/\/amueller.io) and [Sarah Guido](https:\/\/twitter.com\/sarah_guido).**\n<span>&#187;<\/span>"}}