{"cell_type":{"c4956b91":"code","aaea217b":"code","afa81993":"code","137035f9":"code","e298d646":"code","1e79c679":"code","027df7df":"code","2694a4e9":"code","342b889e":"code","760a0a6f":"code","b0d17f6a":"code","556ac6b4":"code","ced10133":"code","3506a69b":"code","bd4e158f":"code","940865f9":"code","0d92a986":"code","352304cc":"code","23a47d3a":"code","c7bd3a88":"code","72db2cd2":"code","d3a6643a":"code","d2764978":"code","0b8a7980":"code","09e5b6a1":"code","8a85fd45":"markdown","35c0ed17":"markdown","549d9cf8":"markdown","87dae4ea":"markdown","b5b793c2":"markdown","259d8f6c":"markdown","8dd5fb97":"markdown","0e132ce0":"markdown","998df18c":"markdown","3b67ea92":"markdown","bf1de698":"markdown","faf58bef":"markdown","dd1fd1f3":"markdown","042b314d":"markdown","9ffe6319":"markdown","d0880c4e":"markdown","592be384":"markdown","657d65e9":"markdown","658e1fdb":"markdown","2536f66e":"markdown","ff610587":"markdown"},"source":{"c4956b91":"from plotly.offline import init_notebook_mode, iplot_mpl, download_plotlyjs, plot, iplot\nimport plotly_express as px\nimport plotly.figure_factory as ff\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport warnings\nwarnings.filterwarnings('ignore')\ninit_notebook_mode(connected=True)\nimport pandas_profiling\nimport statsmodels.formula.api as sm\nimport missingno as msno\nfrom sklearn.preprocessing import LabelEncoder\nfrom statsmodels.compat import lzip\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, silhouette_samples\ntry:\n    import apyori\nexcept:\n    !pip install apyori\n\nfrom apyori import apriori","aaea217b":"#we load the data\ndata=pd.read_csv(\"..\/input\/lastfm\/lastfm.csv\")","afa81993":"data","137035f9":"df=data.copy()","e298d646":"df.info()","1e79c679":"pandas_profiling.ProfileReport(df)","027df7df":"df[df.duplicated(keep=False)]","2694a4e9":"df.drop_duplicates(inplace=True)","342b889e":"print(\"User unique: \",len(df.user.unique()))\nprint(\"Artist unique: \",len(df.artist.unique()))\nprint(\"Country unique: \",len(df.country.unique()))","760a0a6f":"len(df.artist.unique())","b0d17f6a":"fig = px.treemap(df, path=['sex','country'], title='DataSet Treemap by Sex & Country',\n                 color_discrete_sequence=px.colors.qualitative.Pastel).update_traces(dict(marker_line_width=1,\n          marker_line_color=\"black\")).update_layout(paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\n\nfig.show() ","556ac6b4":"df.sex.value_counts(normalize=True)","ced10133":"df.artist.value_counts(normalize=True)","3506a69b":"fig = px.treemap(df, path=['artist'], title='DataSet Treemap by Artist',\n                 color_discrete_sequence=px.colors.qualitative.Pastel).update_traces(dict(marker_line_width=1,\n          marker_line_color=\"black\")).update_layout(paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\n\nfig.show() ","bd4e158f":"#select the relevant columns for the algorithm (user and artist)\ndf = df[['user','artist']]","940865f9":"transactions = []\nfor i in df['user'].unique():\n    transactions.append(list(df[df['user'] == i]['artist'].values))","0d92a986":"len(transactions)","352304cc":"# We check the values of the first 5 users in the list\ntransactions[0:5]","23a47d3a":"association_rules = apriori(transactions, min_support=0.03, min_confidence=0.4, min_lift=2)\nassociation_results = list(association_rules)","c7bd3a88":"#Funci\u00f3n para generar el DataSet\ndef inspect(results):\n    lhs         = [tuple(result[2][0][0])[0] for result in results]\n    rhs         = [tuple(result[2][0][1])[0] for result in results]\n    supports    = [result[1] for result in results]\n    confidences = [result[2][0][2] for result in results]\n    lifts       = [result[2][0][3] for result in results]\n    return list(zip(lhs, rhs, supports, confidences, lifts))\n\n\nApriori = pd.DataFrame(inspect(association_results), columns = ['Left Hand Side', 'Right Hand Side', 'Support', 'Confidence', 'Lift'])\n\nApriori['Transaction']= Apriori['Left Hand Side']+\"--\"+Apriori['Right Hand Side']\nApriori.sort_values(by='Lift',inplace=True,ascending=False)","72db2cd2":"Apriori","d3a6643a":"association_rules = apriori(transactions, min_support=0.02, min_confidence=0.25, min_lift=2)\nassociation_results = list(association_rules)\n\nApriori = pd.DataFrame(inspect(association_results), columns = ['Left Hand Side', 'Right Hand Side', 'Support', 'Confidence', 'Lift'])\n\nApriori['Transaction']= Apriori['Left Hand Side']+\"--\"+Apriori['Right Hand Side']\nApriori.sort_values(by='Lift',inplace=True,ascending=False)","d2764978":"Apriori","0b8a7980":"association_rules = apriori(transactions, min_support=0.01, min_confidence=0.2, min_lift=1.5)\nassociation_results = list(association_rules)\n\nApriori = pd.DataFrame(inspect(association_results), columns = ['Left Hand Side', 'Right Hand Side', 'Support', 'Confidence', 'Lift'])\n\nApriori['Transaction']= Apriori['Left Hand Side']+\"--\"+Apriori['Right Hand Side']\nApriori.sort_values(by='Lift',inplace=True,ascending=False)","09e5b6a1":"Apriori","8a85fd45":"#### 4.2 Parameterization with lift> 1.5, confidence> 0.2 and support> 0.01\n\nWe lowered the margin levels of all parameters even further.","35c0ed17":"We have transformed the data set into a list of all transactions (artists) that correspond to each of the users of the DataSet. (All artists played by each of the 15,000 users.)\n\nThe objective of the algorithm will be to generate knowledge by identifying similarities between users.","549d9cf8":"We note that the DataSet has neither missing nor null values. All variables are Strings, except the user variable which is an integer.","87dae4ea":"With the selected parameters, the a priori algorithm has only found 7 rules ordered by their Lift score. These rules can be interpreted as \"if the author is heard\" Left Hand Side \", then\" Right Hand Side \"is predicted with the corresponding level of support, confidence and lift.","b5b793c2":"The most played artists (with the most support):","259d8f6c":"#### 4.1 Parameterization with lift> 2, confidence> 0.4 and support> 0.03\n\nWe are going to run the ariori algorithm with min_support (0.03), min_confidence = 0.4, min_lift = 2 to see the associations that the algorithm finds with a minimum of robustness.","8dd5fb97":"Explanation of the algorithm:\n\n    The Apriori algorithm is a machine learning algorithm that is used to obtain information about the structured relationships between the different elements involved. It is a data mining technique used to extract frequent item sets and relevant association rules.\n\nThings to know before implementation:\n\n**Association rule:**\n\nIdentification of frequent patterns and associations (relationships) between a set of elements.\n\n             \n**Support.**\n\nIt simply measures how popular an item is as measured by the proportion of transactions in which it appears. It is a metric derived from the frequency of a certain set within the data set. By selecting this parameter we select the frequency of transactions necessary for the rule to take effect.\n\n\n**Trust.**\n\nThis indicates the probability that element will be executed and, when element X has been processed, expressed as {X -> Y}. This is measured by the proportion of transactions with item X, in which item Y also appears.\n\nOne drawback of the confidence measure is that it could misrepresent the importance of a partnership. This is because it only explains how popular item X is, but not Y. If item y is also very popular in general, there will be a higher chance that a transaction containing X will also contain it, thus inflating the measure of confidence.\n\n**Lift.**\n\nThis indicates the probability that item Y will be purchased when item X is purchased, while monitoring the popularity of item Y.","0e132ce0":"Here we see the association rules created by our a priori algorithm. For example, let's take a look at rule number 2.\n\n   - The second value, rhs, corresponds to the Beatles, the artist predicted by a priori who is heard in association with Bob Dylan.\n   \n   \n   - The third value, support, is the number of transactions, which include these two elements, divided by the total number of transactions. (As described above when we chose the parameters for Apriori). Thus, of the total transactions, 3.4% corresponds to Bob Dylan with The Beatles.\n   \n      \n   - The fourth value, confidence, is the percentage probability that the rule will be maintained. It tells us, of all the transactions that contain bob dylan, how often they also incorporate The Beatles. In this case it is 49.71%.\n   \n     \n   - The fifth value, lift, gives us the independence \/ dependency of a rule. It takes into account the trust value and its relationship to the entire data set. The lift is the increase in the probability of listening to The Beatles with the knowledge that Bob Dylan has been heard on the probability of hearing The Beatles without any knowledge of the presence of Bob Dylan. A lift value greater than 1 guarantees a high association between {Y} and {X}. The higher the value of the lift, the greater the chances of hearing an artist Y if the user has already listened to an artist X. In this case, we have that with a Lift of 2.79 The Beatles are 2.79 times more likely to be heard if the user has also listened to Bob Dylan (without any prior knowledge about the presence of Bob Dylan).","998df18c":"#### 4.2 Parameterization with lift> 2, confidence> 0.4 and support> 0.02\n\nWe will lower the minimum support and lift levels for you to investigate further associations with lower support levels.","3b67ea92":"In this case, the algorithm has found 70 rules. For example, we observed that 49.99% of users who listened to iron maiden also listened to metallica. Having a very strong lift between iron maiden - metallica. Thus, the 4.85 Lift tells us that Metalica is 4.85 times more likely to be heard if the user has also listened to Iron Maiden (controlling for Metalica's popularity).","bf1de698":"## 2. DataSet Exploration","faf58bef":"## 5. Conclusions\n  We have executed the Apriori algorithm with different levels of confidence and support and interpreted some of the obtained rules. Depending on the purpose of the data, we will choose different levels of confidence that are more or less rigorous. The knowledge generated with the algorithm is useful for useful to know the preferences of its users on the different musical artists. (with the respective confidence levels), as well as being able to make smart data-drive decisions.","dd1fd1f3":"73% of the observations are men.","042b314d":"## 1. DataSet Loading","9ffe6319":"## 3. Transformation of the data set\n\nIn this section we will transform the data set to be able to execute the a priori algorithm properly.","d0880c4e":"We have 2 duplicate rows. We assume it is an error in the DataSet.","592be384":"We remove duplicate values.","657d65e9":"******\n## Association rules mining - Apriori Algorithm using Python\n- author:: Xavier Martinez Bartra\"\n- date: \"November 2020\"\n******\n\nIn this project we follow the steps of a data mining project for the case of an association rule generation algorithm. The DataSet contains a set of records with the history of the songs that a user (user) has listened to in a music web portal. \"artist\" is the name of the group that has listened, sex and country correspond to variables that describe the user.","658e1fdb":"With this parameterization, the algorithm has found 762 rules. The more we lower the margins of the parameters, the more rules the algorithm will generate, although the margin of confidence in them will be weaker.","2536f66e":"The DataSet contains 15,000 unique users from 159 countries and 1,004 artists.","ff610587":" ## 4. Apriori algorithm"}}