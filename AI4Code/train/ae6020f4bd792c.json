{"cell_type":{"3019035c":"code","743f2d40":"code","7016fcd9":"code","78ca19ab":"code","76f8e01a":"markdown","cd047b70":"markdown","21c9986b":"markdown","0f5c4c5e":"markdown","94ef3d0d":"markdown","840f6cea":"markdown","4d8267c1":"markdown","b404a349":"markdown","58ffe36d":"markdown","9fd81481":"markdown","eab87c15":"markdown","fd35f9e4":"markdown","9d01c3e7":"markdown","0b559b10":"markdown"},"source":{"3019035c":"# Try and get keras plot to work\n!pip install -q pydot\n!pip install -q pydotplus\n!apt-get install -q graphviz\n!pip install -q imagehash\nimport imagehash\n\nprint(\"\\n... IMPORTS STARTING ...\\n\")\nprint(\"\\n\\tVERSION INFORMATION\")\n\n# Machine Learning and Data Science Imports\nimport tensorflow_addons as tfa; print(f\"\\t\\t\u2013 TENSORFLOW ADDONS VERSION: {tfa.__version__}\");\nimport tensorflow as tf; print(f\"\\t\\t\u2013 TENSORFLOW VERSION: {tf.__version__}\");\nimport pandas as pd; pd.options.mode.chained_assignment = None;\nimport numpy as np; print(f\"\\t\\t\u2013 NUMPY VERSION: {np.__version__}\");\nimport scipy; print(f\"\\t\\t\u2013 SCIPY VERSION: {scipy.__version__}\");\n\n# Built In Imports\nfrom collections import Counter\nfrom datetime import datetime\nimport multiprocessing\nfrom glob import glob\nimport warnings\nimport requests\nimport imageio\nimport IPython\nimport urllib\nimport zipfile\nimport pickle\nimport random\nimport shutil\nimport string\nimport math\nimport tqdm\nimport time\nimport gzip\nimport io\nimport os\nimport gc\nimport re\n\n# Visualization Imports\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.patches as patches\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport plotly.express as px\nimport seaborn as sns\nfrom PIL import Image\nimport matplotlib; print(f\"\\t\\t\u2013 MATPLOTLIB VERSION: {matplotlib.__version__}\");\nimport plotly\nimport PIL\nimport cv2\nimport ast\n\nprint(\"\\n\\n... IMPORTS COMPLETE ...\\n\")","743f2d40":"# Define the path to the root data directory\nROOT_DIR = \"\/kaggle\/input\"\n\n# Define the path to the competition data directory\nCOMP_DIR = os.path.join(ROOT_DIR, \"shopee-product-matching\")\n\n# Define the paths to the training and testing image folders respectively for the competition data\nTRAIN_IMG_DIR = os.path.join(COMP_DIR, \"train_images\")\nTEST_IMG_DIR = os.path.join(COMP_DIR, \"test_images\")\n\n# Capture all the relevant full image paths for the competition dataset\nTRAIN_IMG_PATHS = sorted([os.path.join(TRAIN_IMG_DIR, f_name) for f_name in os.listdir(TRAIN_IMG_DIR)])\nTEST_IMG_PATHS = sorted([os.path.join(TEST_IMG_DIR, f_name) for f_name in os.listdir(TEST_IMG_DIR)])\n\nprint(f\"... The first 4 training files are:\")\nfor path in [x.rsplit('\/',1)[1] for x in TRAIN_IMG_PATHS[:4]]: print(f\"... \\t\\t\u2013 {path}\")\nprint(f\"\\n... The number of training images is {len(TRAIN_IMG_PATHS)} i.e. {len(TRAIN_IMG_PATHS)\/\/4} 4-channel images ...\")\nprint(f\"... The number of testing images is {len(TEST_IMG_PATHS)} i.e. {len(TEST_IMG_PATHS)\/\/4} 4-channel images ...\")\n\n# Define paths to the relevant csv files\nTRAIN_CSV = os.path.join(COMP_DIR, \"train.csv\")\nTEST_CSV = os.path.join(COMP_DIR, \"test.csv\")\nSS_CSV = os.path.join(COMP_DIR, \"sample_submission.csv\")\n\n\n# Create the relevant dataframe objects and update image name to be image path\ntrain_df = pd.read_csv(TRAIN_CSV)\ntrain_df[\"image\"] = train_df[\"image\"].apply(lambda x: os.path.join(TRAIN_IMG_DIR, x))\n\ntest_df = pd.read_csv(TEST_CSV)\ntest_df[\"image\"] = test_df[\"image\"].apply(lambda x: os.path.join(TEST_IMG_DIR, x))\n\nss_df = pd.read_csv(SS_CSV)\n\nprint(\"\\n\\nTRAIN DATAFRAME\\n\\n\")\ndisplay(train_df.head(3))\n\nprint(\"\\n\\nTEST DATAFRAME\\n\\n\")\ndisplay(test_df.head(3))\n\nprint(\"\\n\\nSS DATAFRAME\\n\\n\")\ndisplay(ss_df.head(3))","7016fcd9":"\" \".join(train_df.posting_id.iloc[np.where(train_df.duplicated(subset=[\"image_phash\"])==True)].to_list())\n# train_df","78ca19ab":"for k in train_df.image_phash.value_counts().keys()[:3]:\n    similar_images = train_df[train_df.image_phash==k].image.values\n    n_rows = min(int(np.ceil(len(similar_images)\/5)), 10)\n    plt.figure(figsize=(18, 5*n_rows))\n    plt.suptitle(f\"pHash is {k}\", fontweight=\"bold\")\n    for i, image in enumerate(similar_images):\n        img = Image.open(image)\n        plt.subplot(n_rows,5,i+1)\n        plt.imshow(np.asarray(img))\n        plt.title(f\"pHash={imagehash.phash(img)}\", fontweight=\"bold\")\n        plt.axis(False)\n    plt.tight_layout(rect=[0, 0.2, 1, 0.97])\n    plt.show()","76f8e01a":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: navy; background-color: #ffffff;\" id=\"background_information\">1&nbsp;&nbsp;BACKGROUND INFORMATION&nbsp;&nbsp;&nbsp;&nbsp; <a href=\"#toc\">&#10514;<\/a> <\/h1>","cd047b70":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: navy; background-color: #ffffff;\" id=\"setup\">2&nbsp;&nbsp;NOTEBOOK SETUP&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;<\/a><\/h1>","21c9986b":"<h1 style=\"text-align: center; font-family: Verdana; font-size: 32px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; font-variant: small-caps; letter-spacing: 3px; color: gray; background-color: #ffffff;\">Shopee - Price Match Guarantee<\/h1>\n<h2 style=\"text-align: center; font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: underline; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">Exploratory Data Analaysis & Baseline<\/h2>\n<h5 style=\"text-align: center; font-family: Verdana; font-size: 12px; font-style: normal; font-weight: bold; text-decoration: None; text-transform: none; letter-spacing: 1px; color: black; background-color: #ffffff;\">CREATED BY: DARIEN SCHETTLER<\/h5>\n\n<br><br>\n\n<h1 style=\"text-align: center; font-family: Verdana; font-size: 32px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; font-variant: small-caps; letter-spacing: 3px; color: gray; background-color: #ffffff;\">Still Majorly A Work In Progress - Lots of Updates Coming Tomorrow<\/h1>\n\n<br><br>","0f5c4c5e":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: navy; background-color: #ffffff;\" id=\"structured_investigation\">4&nbsp;&nbsp;STRUCTURED DATA INVESTIGATION&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;<\/a><\/h1>\n\nExplore what we can from the structured data.\n\n---\n\nWe are given the following columns to work with:<br><br>\n&nbsp;&nbsp;&nbsp;&nbsp;- **`posting_id`**<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- UID (Unique Identifier)<br>\n&nbsp;&nbsp;&nbsp;&nbsp;- **`image`**<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Image ID [+path] which should be unique but isn't in the training set. In the test set these will all be unique... I think<br>\n&nbsp;&nbsp;&nbsp;&nbsp;- **`image_phash`**<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- Perceptual Hash<br>\n&nbsp;&nbsp;&nbsp;&nbsp;- **`title`**<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- String Description of Product<br><br>","94ef3d0d":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: navy; background-color: #ffffff;\" id=\"dataset\">6&nbsp;&nbsp;PREPARING THE DATASET - TF.DATA&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;<\/a><\/h1>\n\nThis section will explore how to use tf.data to setup the input pipeline","840f6cea":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: navy; background-color: #ffffff;\" id=\"visual_investigation\">5&nbsp;&nbsp;VISUAL INVESTIGATION&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;<\/a><\/h1>\n\nExplore what we can from the visual data","4d8267c1":"<h3 style=\"text-align: font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">1.2  THE GOAL<\/h3>\n\n---\n\nIn this competition, we will find a robust, efficient way to identify matching images.","b404a349":"<p id=\"toc\"><\/p>\n\n<br><br>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: navy; background-color: #ffffff;\">TABLE OF CONTENTS<\/h1>\n\n<br>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#imports\">0&nbsp;&nbsp;&nbsp;&nbsp;IMPORTS<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#background_information\">1&nbsp;&nbsp;&nbsp;&nbsp;BACKGROUND INFORMATION<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#setup\">2&nbsp;&nbsp;&nbsp;&nbsp;SETUP<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#helper_functions\">3&nbsp;&nbsp;&nbsp;&nbsp;HELPER FUNCTIONS<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#structured_investigation\">4&nbsp;&nbsp;&nbsp;&nbsp;STRUCTUED DATA INVESTIGATION<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#visual_investigation\">5&nbsp;&nbsp;&nbsp;&nbsp;VISUAL INVESTIGATION<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#dataset\">6&nbsp;&nbsp;&nbsp;&nbsp;PREPARING THE DATASET - TF.DATA<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#modelling\">7&nbsp;&nbsp;&nbsp;&nbsp;TRAINING<\/a><\/h3>\n\n---","58ffe36d":"<h3 style=\"text-align: font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">1.3  PERCEPTUAL HASHING (P-HASH)<\/h3>\n\n---\n\n<i><sub><a href=\"https:\/\/en.wikipedia.org\/wiki\/Perceptual_hashing\">From the Wizards at Wikipedia<\/a><\/sub>\n\n>\"Perceptual hashing is the use of an algorithm that produces a snippet or fingerprint of various forms of multimedia. Perceptual hash functions are analogous if features of the multimedia are similar, whereas cryptographic hashing relies on the avalanche effect of a small change in input value creating a drastic change in output value. Perceptual hash functions are widely used in finding cases of online copyright infringement as well as in digital forensics because of the ability to have a correlation between hashes so similar data can be found (for instance with a differing watermark). Based on research at Northumbria University, it can also be applied to simultaneously identify similar contents for video copy detection and detect malicious manipulations for video authentication. The system proposed performs better than current video hashing techniques in terms of both identification and authentication.\n>\n>In addition to its uses in digital forensics, research has shown that perceptual hashing can be applied to a wide variety of situations. Similar to comparing images for copyright infringement, a group of researchers[4] found that it could be used to compare and match images in a database. Their proposed algorithm proved to be not only effective, but more efficient than the standard means of database image searching. In addition, a team from China[5] discovered that applying perceptual hashing to speech encryption proved to be effective. They were able to create a system in which the encryption was not only more accurate, but more compact as well.\"\n\n<\/i>\n\n![pHash](https:\/\/miro.medium.com\/max\/1440\/1*_RT4M10OFKqmA8XYavEN3Q.png)\n\n<br>","9fd81481":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: navy; background-color: #ffffff;\" id=\"helper_functions\">3&nbsp;&nbsp;HELPER FUNCTIONS&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;<\/a><\/h1>","eab87c15":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: navy; background-color: #ffffff;\" id=\"training\">7&nbsp;&nbsp;TRAINING&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;<\/a><\/h1>\n\nThis section will explore how to use tf.data to setup the input pipeline","fd35f9e4":"<h3 style=\"text-align: font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\">1.1  THE DATA<\/h3>\n\n---\n\n<b style=\"text-decoration: underline; font-family: Verdana;\">BACKGROUND INFORMATION<\/b>\n\nIn this competition, we are predicting all the duplicate images associated with a given image located within the dataset.\n\n<i>\n    \n**COMPETITION DESCRIPTION**\n    \n>Do you scan online retailers in search of the best deals? You're joined by the many savvy shoppers who don't like paying extra for the same product depending on where they shop. Retail companies use a variety of methods to assure customers that their products are the cheapest. Among them is product matching, which allows a company to offer products at rates that are competitive to the same product sold by another retailer. To perform these matches automatically requires a thorough machine learning approach, which is where your data science skills could help.\n>\n>Two different images of similar wares may represent the same product or two completely different items. Retailers want to avoid misrepresentations and other issues that could come from conflating two dissimilar products. Currently, a combination of deep learning and traditional machine learning analyzes image and text information to compare similarity. But major differences in images, titles, and product descriptions prevent these methods from being entirely effective.\n>\n>Shopee is the leading e-commerce platform in Southeast Asia and Taiwan. Customers appreciate its easy, secure, and fast online shopping experience tailored to their region. The company also provides strong payment and logistical support along with a 'Lowest Price Guaranteed' feature on thousands of Shopee's listed products.\n>\n>In this competition, you\u2019ll apply your machine learning skills to build a model that predicts which items are the same products.\n>\n>The applications go far beyond Shopee or other retailers. Your contributions to product matching could support more accurate product categorization and uncover marketplace spam. Customers will benefit from more accurate listings of the same or similar products as they shop. Perhaps most importantly, this will aid you and your fellow shoppers in your hunt for the very best deals.\n>\n    \n    \n**ADDITIONAL INFO ON DATA PAGE**\n\n> Finding near-duplicates in large datasets is an important problem for many online businesses. In Shopee's case, everyday users can upload their own images and write their own product descriptions, adding an extra layer of challenge. Your task is to identify which products have been posted repeatedly. The differences between related products may be subtle while photos of identical products may be wildly different!\n>\n>As this is a code competition, only the first few rows\/images of the test set are published; the remainder are only available to your notebook when it is submitted. Expect to find roughly 70,000 images in the hidden test set. The few test rows and images that are provided are intended to illustrate the hidden test set format and folder structure.\n   \n<\/i>\n\n---\n\n***Author's Take On Approach (Still Uncertain):***<br><br>\n&nbsp;&nbsp;&nbsp;&nbsp;**1. Create Fingerprints of All Images in Dataset**<br>\n&nbsp;&nbsp;&nbsp;&nbsp;**2. Perform Computationally Efficient Similarity Matching** \n\n---\n\nFor each image in the test set we must predict a a space-delimited list of all **`posting_ids`** that match the posting in the **`posting_id`** column. \n&nbsp;&nbsp;&nbsp;&nbsp;- Posts always self-match\n&nbsp;&nbsp;&nbsp;&nbsp;- Group sizes were capped at 50, so there is no benefit to predict more than 50 matches\n\nThe file should have a header, be named **`submission.csv`**, and look like the following:\n\n```python\nposting_id,matches\ntest_123,test_123\ntest_456,test_456 test_789\n```\n\n**Note that you should predict matches for every `posting_id`. For example, if you believe `A` matches `B` and `C`, then predict `A,A B C`, as well as predicting `B,B A C` and `C,C A B`**\n[train\/test].csv - the training set metadata. Each row contains the data for a single posting. Multiple postings might have the exact same image ID, but with different titles or vice versa.<br><br>\n\n<b style=\"text-decoration: underline; font-family: Verdana;\">DATA FILES<\/b>\n> **`train[test].csv`** - the training set metadata. Each row contains the data for a single posting. Multiple postings might have the exact same image ID, but with different titles or vice versa.<br>\n&nbsp;&nbsp;&nbsp;&nbsp;- **`posting_id`** - the ID code for the posting.<br>\n&nbsp;&nbsp;&nbsp;&nbsp;- **`image`** - the image id\/md5sum.<br>\n&nbsp;&nbsp;&nbsp;&nbsp;- **`image_phash`** - a perceptual hash of the image.<br>\n&nbsp;&nbsp;&nbsp;&nbsp;- **`title`** - the product description for the posting.<br>\n&nbsp;&nbsp;&nbsp;&nbsp;- **`label_group`** - ID code for all postings that map to the same product. Not provided for the test set.<br><br>\n> **`train[test]images`** - the images associated with the postings.<br><br>\n**`sample_submission.csv`** - a sample submission file in the correct format<br>\n&nbsp;&nbsp;&nbsp;&nbsp;- **`posting_id`** - the ID code for the posting.<br>\n&nbsp;&nbsp;&nbsp;&nbsp;- **`matches`** - Space delimited list of all posting IDs that match this posting.<br><br>","9d01c3e7":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: navy;\" id=\"imports\">0&nbsp;&nbsp;IMPORTS&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;<\/a><\/h1>","0b559b10":"<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: navy; background-color: #ffffff;\" id=\"training\">8&nbsp;&nbsp;INFERENCE&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;<\/a><\/h1>\n\nThis section will predict on the test submission file"}}