{"cell_type":{"9cb8261b":"code","2d72cbf0":"code","51bef634":"code","7fd9f0d6":"code","ae4d8649":"code","f19b12ad":"code","92c9e10c":"code","05ad4cc7":"code","a3a69403":"code","cbc84bfa":"code","9423cb2a":"code","a1286f5d":"code","ba470054":"code","6ab83d24":"code","0bd02f80":"code","ff4f2084":"code","0d41029a":"code","eefb7de8":"code","505b2f16":"code","f8d4311c":"code","24c421e2":"code","a5594545":"code","e336065a":"code","cf0db3dd":"code","90ba8d46":"code","6577543b":"code","21f83703":"code","514d8511":"code","44c21bf8":"code","469262a7":"code","a150daa8":"code","8ce20133":"code","14483d1d":"code","d396ff31":"code","19534fa4":"code","23ac775c":"code","eac874ae":"code","38d94ee8":"code","a866776a":"code","67948294":"code","e71cc977":"code","3e98c06e":"code","ed644a4a":"code","10565355":"code","4ff12947":"markdown","a0f5a9ac":"markdown","22478f70":"markdown","0811ac45":"markdown","db4600d1":"markdown","1bf29a6a":"markdown","78c7712a":"markdown","c1cda530":"markdown","878d2d46":"markdown","d1660e0a":"markdown","95be1ed8":"markdown","006f3d18":"markdown","d8da8ea5":"markdown","5d620ada":"markdown","5e70b78e":"markdown","334f8400":"markdown","b32ae477":"markdown","ea6ed4a1":"markdown","92efec7b":"markdown","19f27ce9":"markdown","485b7b58":"markdown","f74cce2a":"markdown","4050cd6d":"markdown","65d1e0c0":"markdown","9afb2b92":"markdown","33af19c9":"markdown","f5098a21":"markdown","06b0017b":"markdown","124346ee":"markdown","6e0f88cd":"markdown","04120433":"markdown","e923cad9":"markdown","35d441c3":"markdown","3df7a35c":"markdown","81636129":"markdown","d0be52ad":"markdown"},"source":{"9cb8261b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport missingno as msno\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom scipy.stats import randint\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# To center the resulting plots in the jupyter notebook\nfrom IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n}\n<\/style>\n\"\"\")","2d72cbf0":"diabetes = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndiabetes.head()","51bef634":"diabetes.isnull().sum()","7fd9f0d6":"sns.set_style('ticks')\n\nfig, axs = plt.subplots(2, 4, figsize=(15, 8))\nfig.tight_layout()\n\nsns.histplot(data=diabetes, x=\"Pregnancies\", kde=True, color=\"skyblue\", bins = 10, ax=axs[0, 0])\naxs[0, 0].annotate(text='0s',\n    xy=(1, 200),\n    xycoords='data',\n    fontsize=20,\n    xytext=(100,0),\n    textcoords='offset points',\n    arrowprops=dict(arrowstyle='->', color='black'),  # Use color black\n    horizontalalignment='center',  # Center horizontally\n    verticalalignment='center')  # Center vertically\n\nsns.histplot(data=diabetes, x=\"Glucose\", kde=True, color=\"olive\", bins=10, ax=axs[0, 1])\nsns.histplot(data=diabetes, x=\"BloodPressure\", kde=True, color=\"olive\",bins=10, ax=axs[0, 2])\nsns.histplot(data=diabetes, x=\"SkinThickness\", kde=True, color=\"gold\", bins = 10, ax=axs[0, 3])\naxs[0, 3].annotate(text='0s',\n    xy=(10, 55),\n    xycoords='data',\n    fontsize=20,\n    xytext=(75,0),\n    textcoords='offset points',\n    arrowprops=dict(arrowstyle='->', color='black'),  # Use color black\n    horizontalalignment='center',  # Center horizontally\n    verticalalignment='center')  # Center vertically\n\nsns.histplot(data=diabetes, x=\"Insulin\", kde=True, color=\"teal\", bins = 10, ax=axs[1, 0])\naxs[1, 0].annotate(text='0s',\n    xy=(70, 140),\n    xycoords='data',\n    fontsize=20,\n    xytext=(75,0),\n    textcoords='offset points',\n    arrowprops=dict(arrowstyle='->', color='black'),  # Use color black\n    horizontalalignment='center',  # Center horizontally\n    verticalalignment='center')  # Center vertically\n\n\nsns.histplot(data=diabetes, x=\"BMI\", kde=True, color=\"teal\",bins=10,ax=axs[1, 1])\nsns.histplot(data=diabetes, x=\"DiabetesPedigreeFunction\", kde=True,bins=10,color=\"teal\", ax=axs[1, 2])\nsns.histplot(data=diabetes, x=\"Age\", kde=True, color=\"teal\",bins=10, ax=axs[1, 3])\nplt.show()","ae4d8649":"for col in diabetes.columns:  \n    print(\"The feature {} contains: {}\".format(col,(diabetes[col]==0).sum()) + \" 0s entries\")","f19b12ad":"diabetes[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = diabetes[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)\ndiabetes.isna().sum()\/len(diabetes) * 100","92c9e10c":"msno.matrix(diabetes)\nplt.show()","05ad4cc7":"temp = diabetes[diabetes['Glucose'].notnull()]\ntemp = temp[['Glucose', 'Outcome']].groupby(['Outcome'])[['Glucose']].median().reset_index()\ntemp","a3a69403":"diabetes.loc[(diabetes['Outcome'] == 0 ) & (diabetes['Glucose'].isnull()), 'Glucose'] = 107\ndiabetes.loc[(diabetes['Outcome'] == 1 ) & (diabetes['Glucose'].isnull()), 'Glucose'] = 140","cbc84bfa":"temp = diabetes[diabetes['BloodPressure'].notnull()]\ntemp = temp[['BloodPressure', 'Outcome']].groupby(['Outcome'])[['BloodPressure']].median().reset_index()\ntemp","9423cb2a":"diabetes.loc[(diabetes['Outcome'] == 0 ) & (diabetes['BloodPressure'].isnull()), 'BloodPressure'] = 70\ndiabetes.loc[(diabetes['Outcome'] == 1 ) & (diabetes['BloodPressure'].isnull()), 'BloodPressure'] = 74.5","a1286f5d":"temp = diabetes[diabetes['BMI'].notnull()]\ntemp = temp[['BMI', 'Outcome']].groupby(['Outcome'])[['BMI']].median().reset_index()\ntemp","ba470054":"diabetes.loc[(diabetes['Outcome'] == 0 ) & (diabetes['BMI'].isnull()), 'BMI'] = 30.1\ndiabetes.loc[(diabetes['Outcome'] == 1 ) & (diabetes['BMI'].isnull()), 'BMI'] = 34.3","6ab83d24":"f, ax = plt.subplots(figsize=(8, 5))\ndiabetes_nomissing = diabetes.dropna().copy()\nsns.heatmap(diabetes_nomissing.corr(), cmap=\"YlGnBu\",annot=True,linewidths=.5)\nplt.show()","0bd02f80":"sns.boxplot(diabetes['Glucose'])\nplt.show()\nnp.percentile(diabetes['Glucose'],[25,50,75])","ff4f2084":"Glu_cat = []\n\nfor glucose in diabetes['Glucose']:\n    if glucose < 99.75:\n        Glu_cat.append('Hypoglycemia')\n    elif 99 <= glucose <= 140.25:\n        Glu_cat.append('Normoglycemia')\n    elif glucose > 140.25:\n        Glu_cat.append('Hyperglycemia')\n\ndiabetes['Glu_cat'] = Glu_cat","0d41029a":"display(diabetes.groupby(['Glu_cat','Outcome'])['Insulin'].mean().reset_index())","eefb7de8":"#\u00a0Imputing Insulin with the insulin median grouped by Outcome and Glucose status.\n\ndiabetes.loc[(diabetes['Outcome'] == 0 ) & (diabetes['Glu_cat'] == 'Hypoglycemia' ) & (diabetes['Insulin'].isnull()), 'Insulin'] = 70.180851\ndiabetes.loc[(diabetes['Outcome'] == 0 ) & (diabetes['Glu_cat'] == 'Normoglycemia' ) & (diabetes['Insulin'].isnull()), 'Insulin'] = 141.888889\ndiabetes.loc[(diabetes['Outcome'] == 0 ) & (diabetes['Glu_cat'] == 'Hyperglycemia' ) & (diabetes['Insulin'].isnull()), 'Insulin'] = 246.971429\n\ndiabetes.loc[(diabetes['Outcome'] == 1 ) & (diabetes['Glu_cat'] == 'Hypoglycemia' ) & (diabetes['Insulin'].isnull()), 'Insulin'] = 122.750000\ndiabetes.loc[(diabetes['Outcome'] == 1 ) & (diabetes['Glu_cat'] == 'Normoglycemia' ) & (diabetes['Insulin'].isnull()), 'Insulin'] = 162.750000\ndiabetes.loc[(diabetes['Outcome'] == 1 ) & (diabetes['Glu_cat'] == 'Hyperglycemia' ) & (diabetes['Insulin'].isnull()), 'Insulin'] = 249.214286","505b2f16":"bmi_cat = []\n\nfor BMI in diabetes['BMI']:\n    if BMI < 18:\n        bmi_cat.append('Underweight')\n    elif 18.1 <= BMI <= 24.9:\n        bmi_cat.append('Normal')\n    elif 24.91 <= BMI <= 29.9:\n        bmi_cat.append('Overweight')\n    elif 29.91 <= BMI <= 34.9:\n        bmi_cat.append('Obese')\n    elif BMI > 34.91:\n        bmi_cat.append('Extremely obese')\n\ndiabetes['BMI_cat'] = bmi_cat","f8d4311c":"display(diabetes.groupby(['BMI_cat','Outcome'])['SkinThickness'].mean().reset_index())","24c421e2":"# Imputing SkinThickness\n\ndiabetes.loc[(diabetes['Outcome'] == 0 ) & (diabetes['BMI_cat'] == 'Normal' ) & (diabetes['SkinThickness'].isnull()), 'SkinThickness'] = 15\ndiabetes.loc[(diabetes['Outcome'] == 0 ) & (diabetes['BMI_cat'] == 'Overweight' ) & (diabetes['SkinThickness'].isnull()), 'SkinThickness'] = 22.830000\ndiabetes.loc[(diabetes['Outcome'] == 0 ) & (diabetes['BMI_cat'] == 'Obese' ) & (diabetes['SkinThickness'].isnull()), 'SkinThickness'] = 27.969388\ndiabetes.loc[(diabetes['Outcome'] == 0 ) & (diabetes['BMI_cat'] == 'Extremely obese' ) & (diabetes['SkinThickness'].isnull()), 'SkinThickness'] = 36.388350\n\ndiabetes.loc[(diabetes['Outcome'] == 1 ) & (diabetes['BMI_cat'] == 'Normal' ) & (diabetes['SkinThickness'].isnull()), 'SkinThickness'] = 17.666667\ndiabetes.loc[(diabetes['Outcome'] == 1 ) & (diabetes['BMI_cat'] == 'Overweight' ) & (diabetes['SkinThickness'].isnull()), 'SkinThickness'] = 24.666667\ndiabetes.loc[(diabetes['Outcome'] == 1 ) & (diabetes['BMI_cat'] == 'Obese' ) & (diabetes['SkinThickness'].isnull()), 'SkinThickness'] = 31.843750\ndiabetes.loc[(diabetes['Outcome'] == 1 ) & (diabetes['BMI_cat'] == 'Extremely obese' ) & (diabetes['SkinThickness'].isnull()), 'SkinThickness'] = 36.850575","a5594545":"diabetes.isna().sum()","e336065a":"# Creating dataset\nDiabetes_code = ['0', '1']\n  \ndata = diabetes['Outcome'].value_counts().to_list()\n  \n# Creating color parameters\ncolors = ( \"orange\", \"cyan\")\n  \n# Wedge properties\nwp = { 'linewidth' : 1, 'edgecolor' : \"black\" }\n  \n# Creating autocpt arguments\ndef func(pct, allvalues):\n    absolute = int(pct \/ 100.*np.sum(allvalues))\n    return \"{:.1f}%\\n({:d})\".format(pct, absolute)\n  \n# Creating plot\nfig, ax = plt.subplots(figsize =(10, 7))\nwedges, texts, autotexts = ax.pie(data, \n                                  autopct = lambda pct: func(pct, data), \n                                  labels = Diabetes_code,\n                                  shadow = False,\n                                  colors = colors,\n                                  startangle = 90,\n                                  wedgeprops = wp,\n                                  textprops = dict(color =\"black\",fontsize=20))\n# Adding legend\nDiabetes_status = ['Healthy', 'Diabetic']\n\nax.legend(wedges, Diabetes_status,\n          title =\"Diabetes status\",\n          fontsize = 'medium',\n          loc =\"center left\",\n          bbox_to_anchor =(1, 0, 0.5, 1))\n\nplt.setp(autotexts, size = 15)\nax.set_title(\"Women distribution according to their health status\",size = 20, weight = \"bold\",loc='right')\n  \n# show plot\nplt.show()","cf0db3dd":"sns.boxplot(y='Age',x='Outcome',data=diabetes)\nplt.show()\n\ntemp = diabetes.groupby('Outcome')['Age'].median().reset_index()\n\nfor idx in temp.index:\n    print(\"The median of the group {} is: {}\".format(idx,temp.iloc[idx]['Age']) + ' years')","90ba8d46":"temp = diabetes.groupby('Outcome')['BMI_cat'].value_counts(normalize=True).reset_index(name=\"Percentage\")","6577543b":"temp","21f83703":"sns.barplot(y='Percentage',x = 'BMI_cat',hue = 'Outcome',order = ['Normal','Overweight','Obese','Extremely obese'],data=temp)\nplt.show()","514d8511":"temp = diabetes.groupby(['BMI_cat','Outcome'])['Age'].median().reset_index()\ntemp","44c21bf8":"sns.boxplot(y='Age',x = 'BMI_cat',hue = 'Outcome',order = ['Normal','Overweight','Obese','Extremely obese'],data=diabetes)\nplt.show()","469262a7":"g = sns.FacetGrid(diabetes, col=\"Outcome\",col_wrap=2,sharex=True,sharey=True,height=5)\ng.map_dataframe(sns.boxplot, x=\"Glu_cat\", y=\"Glucose\",hue = 'BMI_cat',order = ['Hypoglycemia','Normoglycemia','Hyperglycemia'])\ng.add_legend()\nplt.show()","a150daa8":"r1 = ((diabetes['Glu_cat'] == 'Hyperglycemia') & (diabetes['Outcome'] == 0)).sum()\nr2 = ((diabetes['Glu_cat'] == 'Normoglycemia') & (diabetes['Outcome'] == 1)).sum()\n\nprint(\"There are {}\".format(r1 + r2) + ' questionable data points in this dataset')","8ce20133":"# 5 - Metabolic_state\ndiabetes['Metabolic_state'] = diabetes['Glu_cat'] + '_' + diabetes['BMI_cat']","14483d1d":"#\u00a0Defining Target variable\ntarget_col = [\"Outcome\"] #\u00a00 = Healthy and 1 = Diabetes\n\n# Defining categorical variables\nprint(diabetes.nunique()[diabetes.nunique() <= 17].keys().tolist())\ncat_cols   = diabetes.nunique()[diabetes.nunique() < 17].keys().tolist()\n\n# numerical columns\nnum_cols   = [x for x in diabetes.columns if x not in cat_cols + target_col]\n\n# Binary columns\nbin_cols   = diabetes.nunique()[diabetes.nunique() == 2].keys().tolist()\n\n#Columns with more than 2 categories\nmulti_cols = [i for i in cat_cols if i not in bin_cols]\n\n# ENCODING CATEGORICAL FEATURES\n\n# Label encoding Binary columns\nle = LabelEncoder()\nfor i in bin_cols :\n    diabetes[i] = le.fit_transform(diabetes[i])\n    \n# Duplicating columns for multi value columns\ndiabetes = pd.get_dummies(data = diabetes,columns = multi_cols)","d396ff31":"# Scaling Numerical columns: Only numerical features\nstd = StandardScaler()\n    # We index numerical features and create a new df scaled.\nscaled = std.fit_transform(diabetes[num_cols])\nscaled = pd.DataFrame(scaled,columns=num_cols)\n\n# Dropping original values merging scaled values for numerical columns\ndf_data_og = diabetes.copy()\ndiabetes = diabetes.drop(columns = num_cols,axis = 1)\ndiabetes = diabetes.merge(scaled,left_index=True,right_index=True,how = \"left\")","19534fa4":"to_drop_feat = ['BMI_cat_Normal','BMI_cat_Obese','BMI_cat_Extremely obese','BMI_cat_Overweight',\n              'Glu_cat_Hyperglycemia','Glu_cat_Hypoglycemia','Glu_cat_Normoglycemia']\n\ndiabetes = diabetes.drop(to_drop_feat,1)","23ac775c":"# Let's evaluate which variables are telling me the same\n\ndiabetes_corr = diabetes.corr()\nplt.figure(figsize=(30, 20))\nsns.heatmap(diabetes_corr, cmap=\"YlGnBu\",annot=True)\nplt.show()","eac874ae":"upper_tri = diabetes_corr.where(np.triu(np.ones(diabetes_corr.shape),k=1).astype(np.bool))\nto_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.7)]\nprint(to_drop)","38d94ee8":"diabetes2 = diabetes.drop(columns = 'BMI',axis = 1).copy()","a866776a":"X = diabetes2.drop('Outcome', 1)\ny = diabetes2['Outcome'] ","67948294":"steps = [('knn', KNeighborsClassifier())]\n        \n# Create the pipeline: pipeline\npipeline = Pipeline(steps)\n\n# Specify the hyperparameter space\n\nneighbors = np.arange(1, 10)\nparameters = {'knn__n_neighbors':neighbors}\n\n# Create train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42,stratify=diabetes2['Outcome'])\n\n# Instantiate the GridSearchCV object: cv\ncv = GridSearchCV(pipeline,parameters,cv=5)\n\n# Fit to the training set\n\ncv.fit(X_train,y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred = cv.predict(X_test)\n\n# Compute and print metrics\n\nprint(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\nprint(classification_report(y_test, y_pred))\nprint(\"Tuned Model Parameters: {}\".format(cv.best_params_))\n\ny_pred = cv.predict(X_test)\nprint(classification_report(y_test, y_pred))","e71cc977":"################### 21 - Machine learning model pipeline - Logistic Regression ######################################## \n\n# 1-Create a dictionary storing the parameters(keys) and their values(values)\n\nc_space = np.logspace(-5, 8, 15)\ntol = [0.01,0.001,0.0001]\nmax_iter = [100,150,200]\nparam_grid = {'C': c_space, 'penalty': ['l1', 'l2'],'tol': tol, 'max_iter': max_iter}\n\n# Instantiate the logistic regression classifier: logreg\n# 2 - Instate the classifier of choice, in this case logistic regression\nlogreg = LogisticRegression()\n\n# Create train and test sets\n#\u00a03 - Split the data into train and test set\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42,stratify=diabetes2['Outcome'])\n\n# Instantiate the GridSearchCV object: logreg_cv\n#\u00a04 - Run GridSearchCV using the: A)classifier , B) parameters and C) CV (different portions of the data)\nlogreg_cv = GridSearchCV(logreg,param_grid,cv=5)\n\n# Fit it to the training data\n# 5 - Fitting the model using the train data\nlogreg_cv.fit(X_train,y_train)","3e98c06e":"print(\"Tuned Logistic Regression Parameter: {}\".format(logreg_cv.best_params_))\nprint(\"Tuned Logistic Regression Accuracy: {}\".format(logreg_cv.best_score_))\nprint(\" \")\ny_pred = logreg_cv.predict(X_test)\nprint(classification_report(y_test, y_pred))","ed644a4a":"# DecisionTreeClassifier\n\n# Setup the parameters and distributions to sample from: param_dist\n#\u00a0We create a dictionary with all the keys (hyperparameters) and the values (parameters value) we would like to test.\n\nparam_dist = {\"max_depth\": [3, None],\n              \"max_features\": randint(1, 9),\n              \"min_samples_leaf\": randint(1, 9),\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n\n# Instantiate a Decision Tree classifier: tree\ntree = DecisionTreeClassifier()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\ntree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n\n# Fit it to the data\n\ntree_cv.fit(X,y)\ny_pred = tree_cv.predict(X_test)\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\nprint(\"Best score is {}\".format(tree_cv.best_score_))\nprint(\" \")\nprint(classification_report(y_test, y_pred))","10565355":"# 1 - SVM\n\nsteps = [('SVM', SVC())]\n\npipeline = Pipeline(steps)\n\n# Specify the hyperparameter space\n\nparameters = {'SVM__C':[1, 10, 100],\n              'SVM__gamma':[0.1, 0.01]}\n\n\n# Instantiate the GridSearchCV object: cv\ncv = GridSearchCV(pipeline,parameters,cv=3)\n\n# Fit to the training set\n\ncv.fit(X_train,y_train)\n\n# Predict the labels of the test set: y_pred\ny_pred = cv.predict(X_test)\n\n# Compute and print metrics\n\nprint(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\nprint(\"Tuned Model Parameters: {}\".format(cv.best_params_))\nprint(\" \")\nprint(classification_report(y_test, y_pred))","4ff12947":"#### 3.3.1.1. Glucose","a0f5a9ac":"#### 4.2 - What are the characteristics of patients who suffer from diabetes?\n\n##### 4.2.1 - Age","22478f70":"## 3. Data cleaning\n\n### 3.1. Evaluate whether the dataframe diabetes contains missing values","0811ac45":"# Pima Indian diabetes data analysis with Python: Application of supervised learning models for the prediction of type 2 diabetes in females\n\n## Felipe N\u00fa\u00f1ez Villena\n\n### 12-09-2021\n\n\n## Table of contents\n\n1. <a href =\"#1.-Load-Packages\">Load Packages<\/a>\n2. <a href =\"#2.-Load-data\">Load data<\/a>\n3. <a href =\"#3.-Data-cleaning\">Data cleaning<\/a><br> \n    3.1 <a href =\"#3.1.-Evaluate-whether-the-dataframe-diabetes-contains-missing-values\">Evaluate whether the dataframe diabetes contains missing values<\/a><br> \n    3.2 <a href =\"#3.2.-Replacing-0s-with-missing-values\">Replacing 0s with missing values<\/a><br> \n    3.3   <a href =\"#3.3.-Imputation-of-missing-values\">Imputation of missing values<\/a><br>\n4. <a href =\"#4.-Exploratory-data-analysis\">Exploratory data analysis<\/a><br>\n5. <a href =\"#5.-Feature-engineering\">Feature engineering<\/a><br>\n6. <a href =\"#6.-Encoding-categorical-data\">Encoding categorical data<\/a><br>\n7. <a href =\"#7.-Scaling-data\">Scaling data<\/a><br>\n8. <a href =\"#8.-Identification-of-correlated-features\">Identification of correlated features<\/a><br>\n9. <a href =\"#9.-Machine-learning\">Machine learning<\/a><br>","db4600d1":"As we can see in the heatmap, insulin exhibits the highest correlation with Glucose (0.58)\n\nTherefore, we should consider glucose to impute insulin. \n\n#### Based on their glucose levels, subjects can be categorized as:\n\n1 - Hypoglycemic\n\n2 - Normoglycemic\n\n3 - Hyperglycemic\n","1bf29a6a":"##### 8.1 - Remove BMI feature","78c7712a":"##### 9.3 - Logistic Regression","c1cda530":"As we can see in the boxplot, older people are more prone to develop diabetes. \n\nThis is not suprising since type 2 diabetes is a progressive condition and therefore needs time to manifest their symptoms.","878d2d46":"##### 9.4 - Decision Tree Classifier","d1660e0a":"As we can see, this dataset contains much more information of healthy patients (65.1%)","95be1ed8":"Irrespective of their BMI category, every diabetic group was clearly older than the healthy group.\nConfirming that age is a risk factor for the development of type 2 diabetes.\n\nThe median for healthy patients was between 26 - 28 years.\n\nThe median for diabetic patients with normal BMI was 50 years.\n\nFurthermore, the median for diabetic patients whose BMI was overweight, Obese or extremely obese was in the range of \n27 - 36 years, suggesting that a higher BMI accelerate the onset of diabetes.","006f3d18":"##### 9.2 - KNN ","d8da8ea5":"### 3.3. Imputation of missing values\n\nThere is not an specific part of the data that is missing. On the contrary, the missing values seems to be \nrandomly distributed within the Insulin and SkinThickness features.\n\nSince we can not afford to miss approximately 50% of the data, features containing missing values should be imputed.","5d620ada":"As we can see in the barplot, the BMI distribution across healthy patients is similar (19.8% - 27.8%).\nOn the contrary, approximately 82 % of diabetic patients are obese or extremely obese based on their BMI.\n\nNormally, one would imagine that most of the people extremely obese should be diabetic. \nHowever, we observed a significant portion of extremely obese patients that are labeled as healthy (26%)\n\n##### 4.3 - What's the difference between healthy and diabetic Extremely obese patients?","5e70b78e":"## 1. Load Packages","334f8400":"We found 6 features containing 0s:\n\n1 - Pregnancies (111)\n\n2 - Glucose (5)\n\n3 - Blood Pressure (35)\n\n4 - Skin Thickness (227)\n\n5 - Insulin (374)\n\n6 - BMI (11)\n\n\nPREGNANCIES: It is plausible to find women with no pregnancies. Therefore, 0 in this feature does not represent missing values.\n\n##\u00a03.2. Replacing 0s with missing values\n\nThe remaining features are measurements and the read-outs can not be 0, suggesting that 0 represents missing data.\nTherefore, let's replace 0s with missing values (NaNs)\n","b32ae477":"#### BMI","ea6ed4a1":"# 3.3.2 - Imputation of features with high number of missing values\n\n#### Insulin\n\nInsulin is a hormone secreted in the pancreatic beta cells. It's secretion rate increases in response to higher metabolic demands for insulin.\n\nHigh Insulin levels are not purely characteristic of diabetic subjects.\n\nLet's evaluate whether insulin is correlated with other features in this dataset","92efec7b":"##### 4.2.2 - BMI","19f27ce9":"<a href =\"#Table-of-contents\">Back to top<\/a>","485b7b58":"## 6. Encoding categorical data","f74cce2a":"#### 4.4 - Glucose analysis\n\nDiabetes is defined as the incapacity of the organism to properly regulate blood glucose levels.\n\nAccording to the mayo clinic, 2 hours after an Oral Glucose Tolerance Test (OGTT)\n\n1) Below: 140 mg\/dL - Normal blood glucose\n\n2) 140 and 199 mg\/dL (Subjects at risk to onset diabetes) - Impaired Glucose metabolism: \n\n3) 200 mg\/dL or more - Diabetic\n\nTo understand the glycemic statuts of the patients in this dataset, let's plot glucose levels by 2 categorical variables: Glucose categories (Glu_cat) and health status (Output)","4050cd6d":"The data frame diabetes does not contain entries listed as missing. However missing values can be expressed as 0s or \nwith other symbology (i.e: ?). Let's evaluate whether diabetes is truly clean or possess missing data","65d1e0c0":"#### 3.3.1.2. Blood Pressure","9afb2b92":"## 2. Load data","33af19c9":"##### 9.5 SVM","f5098a21":"Despite we have 192 data points between 140 - 199, we do not have any value higher than 199 for Glucose after a 2hr OGTT. This is surprising, because this is the gold-standard indicator for the diagnosis of diabetes.\n\nTherefore, Why do we observe a total of 500 patients labeled as diabetic?\n\nSince my understanding of diabetes, it is plausible to think that hyperglycemic subjects were considered as diabetic.\nHowever, i don't understand why subjects with normal blood glucose levels (Normoglycemia) were considered diabetic.","06b0017b":"### 3.2.1. Exploring the missingness","124346ee":"## 9. Machine learning\n\n##### 9.1 - Segregate features (X) and labels (y) into separable variables","6e0f88cd":"## 4. Exploratory data analysis\n\n#### 4.1 - Is the dataset balanced?","04120433":"## 7. Scaling data","e923cad9":"### 3.3.1. Imputation of features with a low number of missing values (<4.5%)\n\nThe features Glucose, BloodPressure and BMI contain an small fraction of missing data. Therefore, features can be\n\nimputed with the median grouped by the target variable (Outcome).\n","35d441c3":"## 8. Identification of correlated features","3df7a35c":"## 5. Feature engineering","81636129":"As we can see in this dataset, irrespective whether the patient is diabetic, hyperglycemic patients exhibited\nincreasing levels of insulin","d0be52ad":"#### Skin thickness\n\nAs we saw in the heatmap, skin thickness exhibits the highest correlation with BMI (0.67)\n\nTherefore, we should consider BMI as well.\n\nBased on the BMI, subjects can be categorized as:\n\n1 - Underweight (<18)\n\n2 - Normal (18.5 - 24.9)\n\n3 - Overweight (25 - 29.9)\n\n4 - Obese (30 - 34.9)\n\n5 - Extremely obese (>35)"}}