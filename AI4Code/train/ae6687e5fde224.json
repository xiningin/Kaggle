{"cell_type":{"33e4758b":"code","153c82ef":"code","1dc8bdbc":"code","0a0e530c":"code","2e0d7e04":"code","93f94ea7":"code","024cf30e":"code","9f17a220":"code","a8a3700f":"code","57a406de":"code","51e2ad60":"code","f8d94ca4":"code","8b4d1d7f":"code","7fd3c80d":"code","be27af0e":"code","98b362cc":"code","163e7fca":"code","0ff5a988":"code","ea173718":"code","305c427e":"code","a09295b2":"code","8e1f83be":"code","1a42f372":"code","f19268c5":"code","eae64fc0":"code","9ae5996c":"code","92138b35":"code","64a6e28b":"code","94357ee8":"code","62c88b13":"code","17e15ab6":"code","9402fe35":"code","94de43aa":"code","b48005be":"code","1dd041a0":"code","dd023494":"markdown","f3031195":"markdown","f66cccda":"markdown","cdb35690":"markdown","f11d0568":"markdown","cca343d4":"markdown","dcc084d7":"markdown","7ad82bb2":"markdown","8c2aa0e2":"markdown","981fa17a":"markdown","7b15d23b":"markdown","cf97dfa8":"markdown","5b453567":"markdown","3b862d62":"markdown","8b0c0b9f":"markdown","f63bbad7":"markdown","afbae86f":"markdown","d6722000":"markdown","09559f51":"markdown","f878263d":"markdown","d77f0f03":"markdown","26282002":"markdown","8fca50c3":"markdown","aeb198bd":"markdown","512dbca7":"markdown","b795e303":"markdown","233cceda":"markdown","0d006f8e":"markdown","3c165650":"markdown","3c5515c3":"markdown","f6988af9":"markdown","e659dab6":"markdown","769556ec":"markdown","8993407e":"markdown","fe798f58":"markdown"},"source":{"33e4758b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","153c82ef":"# sets the backend of matplotlib to the inline backend\n%matplotlib inline \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import average_precision_score\nfrom xgboost.sklearn import XGBClassifier\nfrom xgboost import plot_importance","1dc8bdbc":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","0a0e530c":"#raw_csv_data = pd.read_csv('PS_20174392719_1491204439457_log.csv')\n#ip_data = raw_csv_data.copy()\nip_data = pd.read_csv('..\/input\/PS_20174392719_1491204439457_log.csv')","2e0d7e04":"ip_data.head()","93f94ea7":"ip_data = ip_data.rename(columns = {'oldbalanceOrg': 'oldBalanceOrig', 'newbalanceOrig':'newBalanceOrig', \n                                    'oldbalanceDest':'oldBalanceDest', 'newbalanceDest' : 'newBalanceDest'})","024cf30e":"# check for missing values.\nip_data.isnull().values.any()","9f17a220":"ip_data.info()","a8a3700f":"from collections import Counter\n\nstep_list = list(ip_data.loc[ip_data.isFraud == 1].step.values)\n\nstep_counted_list = Counter(step_list)\nstep_counted_list.most_common(40)","57a406de":"type_list = list(ip_data.loc[ip_data.isFraud == 1].type.values)\n\ntype_counted_list = Counter(type_list)\ntype_counted_list.most_common(20)","51e2ad60":"amount_list = list(ip_data.loc[ip_data.isFraud == 1].amount.values)\n\namount_counted_list = Counter(amount_list)\namount_counted_list.most_common(20)","f8d94ca4":"nameOrig_list = list(ip_data.loc[ip_data.isFraud == 1].nameOrig.values)\n\nnameOrig_counted_list = Counter(nameOrig_list)\nnameOrig_counted_list.most_common(20)","8b4d1d7f":"oldBalanceOrig_list = list(ip_data.loc[ip_data.isFraud == 1].oldBalanceOrig.values)\n\noldBalanceOrig_counted_list = Counter(oldBalanceOrig_list)\noldBalanceOrig_counted_list.most_common(20)","7fd3c80d":"newBalanceOrig_list = list(ip_data.loc[ip_data.isFraud == 1].newBalanceOrig.values)\n\nnewBalanceOrig_counted_list = Counter(newBalanceOrig_list)\nnewBalanceOrig_counted_list.most_common(20)","be27af0e":"nameDest_list = list(ip_data.loc[ip_data.isFraud == 1].nameDest.values)\n\nnameDest_counted_list = Counter(nameDest_list)\nnameDest_counted_list.most_common(20)","98b362cc":"oldBalanceDest_list = list(ip_data.loc[ip_data.isFraud == 1].oldBalanceDest.values)\n\noldBalanceDest_counted_list = Counter(oldBalanceDest_list)\noldBalanceDest_counted_list.most_common(20)","163e7fca":"newBalanceDest_list = list(ip_data.loc[ip_data.isFraud == 1].newBalanceDest.values)\n\nnewBalanceDest_counted_list = Counter(newBalanceDest_list)\nnewBalanceDest_counted_list.most_common(20)","0ff5a988":"isFlaggedFraud_list = list(ip_data.loc[ip_data.isFraud == 1].isFlaggedFraud.values)\n\nisFlaggedFraud_counted_list = Counter(isFlaggedFraud_list)\nisFlaggedFraud_counted_list.most_common(20)","ea173718":"X = ip_data.loc[(ip_data.type == 'CASH_OUT') | (ip_data.type == 'TRANSFER')]\nrandomState = 5\nnp.random.seed(randomState)","305c427e":"Y = X['isFraud']\ndel X['isFraud']\n\n\ndel X['nameDest'] \ndel X['nameOrig']\ndel X['isFlaggedFraud']","a09295b2":"X.head()","8e1f83be":"#Binary-encoding of labelled data in 'type'\nX.loc[X.type == 'TRANSFER', 'type'] = 0\nX.loc[X.type == 'CASH_OUT', 'type'] = 1\n\nX.type = X.type.astype(int)\n","1a42f372":"X_fraud = X.loc[Y==1]\nX_nonFraud = X.loc[Y==0]","f19268c5":"print('skew = {}'.format(len(X_fraud)\/float(len(X))))","eae64fc0":"#Split the data into training and test sets in a 80:20 ratio\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = randomState)\n\nweights = (Y==0).sum()\/(1.0 *  (Y==1).sum())\nxgb_classifier1 = XGBClassifier(max_depth = 3, scale_pos_weight = weights, n_jobs = 4)\nxgb_prediction1 = xgb_classifier1.fit(X_train, Y_train).predict_proba(X_test)\n\nprint('AUPRC = {}'.format(average_precision_score(Y_test, xgb_prediction1[:,1])))","9ae5996c":"fig = plt.figure(figsize = (14, 9))\nax = fig.add_subplot(111)\n\ncolours = plt.cm.Set1(np.linspace(0, 1, 9))\n\nax = plot_importance(xgb_classifier1, height = 1, color = colours, grid = False, \\\n                     show_values = False, importance_type = 'cover', ax = ax);\nfor axis in ['top','bottom','left','right']:\n            ax.spines[axis].set_linewidth(2)\n        \nax.set_xlabel('importance score', size = 16);\nax.set_ylabel('features', size = 16);\nax.set_yticklabels(ax.get_yticklabels(), size = 12);\nax.set_title('Ordering of features by importance to the model learnt', size = 20);","92138b35":"#Xfraud = X.loc[Y == 1]\n#XnonFraud = X.loc[Y == 0]\nprint('\\nThe fraction of fraudulent transactions with \\'oldBalanceDest\\' = \\\n\\'newBalanceDest\\' = 0 although the transacted \\'amount\\' is non-zero is: {}'.\\\nformat(len(X_fraud.loc[(X_fraud.oldBalanceDest == 0) & \\\n(X_fraud.newBalanceDest == 0) & (X_fraud.amount)]) \/ (1.0 * len(X_fraud))))\n\nprint('\\nThe fraction of genuine transactions with \\'oldBalanceDest\\' = \\\nnewBalanceDest\\' = 0 although the transacted \\'amount\\' is non-zero is: {}'.\\\nformat(len(X_nonFraud.loc[(X_nonFraud.oldBalanceDest == 0) & \\\n(X_nonFraud.newBalanceDest == 0) & (X_nonFraud.amount)]) \/ (1.0 * len(X_nonFraud))))","64a6e28b":"X.loc[(X.oldBalanceDest == 0) & (X.newBalanceDest == 0) & (X.amount != 0), \\\n      ['oldBalanceDest', 'newBalanceDest']] = - 1","94357ee8":"X.loc[(X.oldBalanceOrig == 0) & (X.newBalanceOrig == 0) & (X.amount != 0), \\\n      ['oldBalanceOrig', 'newBalanceOrig']] = np.nan","62c88b13":"X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = randomState)\n\nweights = (Y==0).sum()\/(1.0 *  (Y==1).sum())\nxgb_classifier2 = XGBClassifier(max_depth = 3, scale_pos_weight = weights, n_jobs = 4)\nxgb_prediction2 = xgb_classifier2.fit(X_train, Y_train).predict_proba(X_test)\n\nprint('AUPRC = {}'.format(average_precision_score(Y_test, xgb_prediction2[:,1])))","17e15ab6":"fig = plt.figure(figsize = (14, 9))\nax = fig.add_subplot(111)\n\ncolours = plt.cm.Set1(np.linspace(0, 1, 9))\n\nax = plot_importance(xgb_classifier2, height = 1, color = colours, grid = False, \\\n                     show_values = False, importance_type = 'cover', ax = ax);\nfor axis in ['top','bottom','left','right']:\n            ax.spines[axis].set_linewidth(2)\n        \nax.set_xlabel('importance score', size = 16);\nax.set_ylabel('features', size = 16);\nax.set_yticklabels(ax.get_yticklabels(), size = 12);\nax.set_title('Ordering of features by importance to the model learnt', size = 20);","9402fe35":"X['errorBalanceOrig'] = X.newBalanceOrig + X.amount - X.oldBalanceOrig\nX['errorBalanceDest'] = X.oldBalanceDest + X.amount - X.newBalanceDest","94de43aa":"X","b48005be":"X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = randomState)\n\nweights = (Y==0).sum()\/(1.0 *  (Y==1).sum())\nxgb_classifier3 = XGBClassifier(max_depth = 3, scale_pos_weight = weights, n_jobs = 4)\nxgb_prediction3 = xgb_classifier3.fit(X_train, Y_train).predict_proba(X_test)\n\nprint('AUPRC = {}'.format(average_precision_score(Y_test, xgb_prediction3[:,1])))","1dd041a0":"fig = plt.figure(figsize = (14, 9))\nax = fig.add_subplot(111)\n\ncolours = plt.cm.Set1(np.linspace(0, 1, 9))\n\nax = plot_importance(xgb_classifier3, height = 1, color = colours, grid = False, \\\n                     show_values = False, importance_type = 'cover', ax = ax);\nfor axis in ['top','bottom','left','right']:\n            ax.spines[axis].set_linewidth(2)\n        \nax.set_xlabel('importance score', size = 16);\nax.set_ylabel('features', size = 16);\nax.set_yticklabels(ax.get_yticklabels(), size = 12);\nax.set_title('Ordering of features by importance to the model learnt', size = 20);","dd023494":"<a href='#top'>back to top<\/a>","f3031195":"<a id='ML1_features_imp'> <\/a>\n#### 4.1. What are the important features for the ML model?","f66cccda":"<a href='#top'>back to top<\/a>","cdb35690":"*Selection of metric*: \nSince the data is highly skewed, I use the area under the precision-recall curve (AUPRC) rather than the conventional area under the receiver operating characteristic (AUROC). This is because the AUPRC is more sensitive to differences between algorithms and their parameter settings rather than the AUROC (see <a href='http:\/\/pages.cs.wisc.edu\/~jdavis\/davisgoadrichcamera2.pdf'>Davis and Goadrich, 2006<\/a>).","f11d0568":"<a id = 'ML_wo_iORf'> <\/a>\n### 4. Machine Learning (without any imputation or feature engineering)","cca343d4":"My ML learning attempt. This kernel is based on https:\/\/www.kaggle.com\/arjunjoshua\/predicting-fraud-in-financial-payment-services","dcc084d7":"Since the destination account balances being zero is a strong indicator of fraud, we do not impute the account balance (before the transaction is made) with a statistic or from a distribution with a subsequent adjustment for the amount transacted. Doing so would mask this indicator of fraud and make fraudulent transactions appear genuine. Instead, below we replace the value of 0 with -1 which will be more useful to a suitable machine-learning (ML) algorithm detecting fraud.","7ad82bb2":"<a id='ML3_features_imp'> <\/a>\n#### 8.1. What are the important features for the ML model?\nThe figure below shows that the new feature errorBalanceOrig that we created is the most relevant feature for the model. The features are ordered based on the number of samples affected by splits on those features.","8c2aa0e2":"In this section and until section 4, we wrangle with the data exclusively using Dataframe methods. This is the most succinct way to gain insights into the dataset.","981fa17a":"<a id = 'ML'> <\/a>\n### 8. Machine Learning (after imputation and feature engineering)","7b15d23b":"<a id='top'><\/a>\n#### Outline: \n#### 1. <a href='#import'>Import relevant libraries and data<\/a>\n11. <a href='#col_corr'>Import data and correct the spelling of original column headers for consistency<\/a>\n\n#### 2. <a href='#eda'>Feature Selection<\/a>\n21. <a href='#eda_feature'>Find the relevance of each feature in identifying the fraudulent transactions<\/a>\n22. <a href='#eda_conclusions'>Feature Selection- Conclusions<\/a>\n\n#### 3. <a href='#dataclean'>Data Cleaning<\/a>\n\n#### 4. <a href='#ML_wo_iORf'>Machine Learning (without any imputation or feature engineering)<\/a>\n41. <a href='#ML1_features_imp'>What are the important features for the ML model?<\/a>\n\n#### 5 <a href='#imputation'>Imputation of Latent Missing Values<\/a>\n\n#### 6. <a href='#ML_after_imputation'>Machine Learning (after imputation, but not feature engineering)<\/a>\n61. <a href='#ML2_features_imp'>What are the important features for the ML model?<\/a>\n\n#### 7. <a href='#feature'>Feature Engineering<\/a>\n\n#### 8. <a href='#ML'>Machine Learning (after imputation and feature engineering)<\/a>\n81. <a href='#ML3_features_imp'>What are the important features for the ML model?<\/a>\n","cf97dfa8":"<a id = 'ML_after_imputation'> <\/a>\n### 6. Machine Learning (after imputation, but not feature engineering)","5b453567":"<a id='import'><\/a>\n### 1. Import relevant libraries and data","3b862d62":"<a id = 'eda_conclusions'> <\/a>\n#### 2.2 Feature Selection - Conclusions\nBy analysing the dataset, we come to the following conclusions about the features:\n\n#### Features:                         \n<b>1. step <\/b>   :         Include this feature. The fraudulent transactions distributed in many 'step' values.\n\n<b>2. type <\/b>\t    :         Include this feature. The fraudulent transaction happened only in 'CASH_OUT' and 'TRANSFER' transaction types. So we will include only the records with type as 'CASH_OUT' and 'TRANSFER.'\n\n<b>3. amount<\/b>:         Include this feature. Though it won't explain all fraudulent transactions, amount as 10000000.0 and 0.0 denotes a high chance of fraud. \n\n<b>4. nameOrig\t<\/b>:         Drop this feature. There is no useful information from this column.\n\n<b>5. oldbalanceOrig <\/b>:   Include this feature. You could see that in almost all fraudulent transactions, 'oldbalanceOrig' and 'amount' has the same value. This is a strong indicator of a fraudulent transaction. \n\n<b>6. newbalanceOrig <\/b>:   Include this feature. For most of the fraudulent transactions, 'newbalanceOrig' = 0 (this fact supports our finding in #5)\n\n<b>7. nameDest\t<\/b>:         Drop this feature. There is no useful information from this column.\n\n<b>8. oldbalanceDest <\/b>:   Include this feature. Value of 'oldbalanceDest' is zero for nearly half of the fraudulent transaction.\n\n<b>9. newbalanceDest <\/b>:   Include this feature. Value of 'oldbalanceDest' is zero for more than half of the fraudulent transaction. We will include this feature in our model. \n\n<b>10. isFlaggedFraud <\/b>:  Drop this feature. Only 16 transactions flagged correctly. We can drop this feature.\n","8b0c0b9f":"<a id = 'col_corr'> <\/a>\n#### 1.1 Import data and correct the spelling of original column headers for consistency","f63bbad7":"<a href='#top'>back to top<\/a>","afbae86f":"<a href='#top'>back to top<\/a>","d6722000":"The data also has several transactions with zero balances in the originating account both before and after a non-zero amount is transacted. In this case, the fraction of such transactions is much smaller in fraudulent (0.3%) compared to genuine transactions (47%). Once again, from similar reasoning as above, instead of imputing a numerical value we replace the value of 0 with a null value.","09559f51":"<a id='imputation'> <\/a>\n### 5 Imputation of Latent Missing Values\n\nThe data has several transactions with zero balance in the destination account both before and after a non-zero amount is transacted. The fraciton of such transactions, where zero likely denotes a missing value, is much larger in fraudelent (50%) compared to genuine transactions (0.06%)","f878263d":"<a href='#top'>back to top<\/a>","d77f0f03":"<a href='#top'>back to top<\/a>","26282002":"<a id='feature'><\/a>\n### 7. Feature-engineering","8fca50c3":"*Selection of ML algorithm*: A first approach to deal with imbalanced data is to balance it by discarding the majority class before applying an ML algorithm. The disadvantage of  undersampling is that a model trained in this way will not perform well on real-world skewed test data since almost all the information was discarded. A better approach might be to oversample the minority class, say by the synthetic minority oversampling technique (SMOTE) contained in the 'imblearn' library. Motivated by this, I tried a variety of anomaly-detection and supervised learning approaches. I find, however, that the best result is obtained on the original dataset by using a ML algorithm based on ensembles of decision trees that intrinsically performs well on imbalanced data. Such algorithms not only allow for constructing a model that can cope with the missing values in our data, but they naturally allow for speedup via parallel-processing. Among these algorithms, the extreme gradient-boosted (XGBoost) algorithm used below slightly outperforms random-forest. Finally, XGBoost, like several other ML algorithms, allows for weighting the positive class more compared to the negative class --- a setting that also allows to account for the skew in the data.","aeb198bd":"<a id = 'eda_feature'> <\/a>\n#### 2.1 Find the relevance of each feature in identifying the fraudulent transactions","512dbca7":"<a id='ML2_features_imp'> <\/a>\n#### 6.1. What are the important features for the ML model?","b795e303":"<a href='#top'>back to top<\/a>","233cceda":"<a id = 'dataclean'> <\/a>\n### 3. Data Cleaning","0d006f8e":"## Financial Fraud Detection-XGBoost","3c165650":"<a id='eda'><\/a>\n### 2. Feature Selection","3c5515c3":"<a href='#top'>back to top<\/a>","f6988af9":"Motivated by the possibility of zero-balances serving to differentiate between\nfraudulent and genuine transactions, we take the data-imputation of section <a href='#imputation'>3.1<\/a> a\nstep further and create 2 new features (columns) recording errors in the \noriginating and\ndestination accounts for each transaction. These new features turn out to be \nimportant in obtaining the best performance from the ML algorithm that we will\nfinally use.\n\nNote : from tests, I could find that the model can achieve 99.85% accuracy only with this feature engineering. It doesn't require any impuatation. ","e659dab6":"<a href='#top'>back to top<\/a>","769556ec":"<a href='#top'>back to top<\/a>","8993407e":"It turns out there are no obvious missing values but as we will see above, this does not rule out proxies by a numerical value like 0","fe798f58":"<a href='#top'>back to top<\/a>"}}