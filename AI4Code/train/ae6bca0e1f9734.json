{"cell_type":{"bb427d51":"code","98897a9a":"code","25ffef5c":"code","6d7c1671":"code","71d09ba0":"code","db79a44a":"code","9b0efe97":"code","917e4616":"code","e05c964e":"code","3d55b322":"code","ccc99952":"code","39cb2408":"code","328d95b0":"code","51c044ee":"code","92c468c7":"code","5b109f96":"code","f0e78214":"code","b0cd2e3f":"code","1ff0a421":"code","33131c6c":"code","8ccaee02":"code","fc31866e":"code","88ca8905":"code","28407e9b":"code","1bcda4e7":"code","49b8e66e":"code","e83f5ace":"code","c80f5e4a":"code","14f64ec0":"code","ddb33c68":"code","b382a80b":"code","45c9b362":"code","d3fa2e86":"code","718cc0b4":"code","09105499":"code","83ccc665":"code","e80ffcf4":"code","f328f070":"code","9f3e5956":"code","ed2ad6a1":"code","9177884d":"code","fc54f490":"code","0599df8a":"code","7a18c480":"code","1bd71d23":"code","98d52360":"code","c58f0b5c":"code","ee6b2879":"code","90b7f7ac":"code","7996aa5c":"code","29bfc748":"code","34633a23":"code","8ec0e7d5":"markdown","a43219e8":"markdown","15222fb6":"markdown","3f84d2d6":"markdown","6637d424":"markdown","ed80a034":"markdown","d8409944":"markdown","d2849ace":"markdown","ee1b8284":"markdown","62ca6515":"markdown","ee6c6dc1":"markdown","9a739e9c":"markdown"},"source":{"bb427d51":"import pandas as pd\nimport numpy as np\nfrom scipy.special import erfinv\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.utils.data import *\nfrom torch.optim import *\nfrom fastai.tabular import *\nimport torch.utils.data as Data\nfrom fastai.basics import *\nfrom fastai.callbacks.hooks import *\nfrom tqdm import tqdm_notebook as tqdm\nimport gc\nimport joblib\n\n%matplotlib inline","98897a9a":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","25ffef5c":"train.head()","6d7c1671":"train_test = pd.concat([train, test]).reset_index()","71d09ba0":"#train_test = train_test.sample(frac=0.5)","db79a44a":"train_test.reset_index(inplace=True)","9b0efe97":"train_test.head()","917e4616":"def preprocess(data):\n    usecols = [c for c in data.columns]\n    data = data.loc[:,usecols]\n    #cat_features = [c for c in data.columns if 'cat' in c]\n    #add_df = data[cat_features]\n    #data = pd.get_dummies(data, columns=cat_features)\n    #data = pd.concat([data, add_df], axis= 1).drop('index', 1)\n    data = data.drop(['index', 'level_0'], 1)\n    return data","e05c964e":"data = preprocess(train_test)","3d55b322":"data.head()","ccc99952":"def to_gauss(x): return np.sqrt(2)*erfinv(x)  #from scipy\n\ndef normalize(data, exclude=None):\n    # if not binary, normalize it\n    norm_cols = [n for n, c in data.drop(exclude, 1).items() if len(np.unique(c)) > 2]\n    n = data.shape[0]\n    for col in norm_cols:\n        sorted_idx = data[col].sort_values().index.tolist()# list of sorted index\n        uniform = np.linspace(start=-0.99, stop=0.99, num=n) # linsapce\n        normal = to_gauss(uniform) # apply gauss to linspace\n        normalized_col = pd.Series(index=sorted_idx, data=normal) # sorted idx and normalized space\n        data[col] = normalized_col # column receives its corresponding rank\n    return data","39cb2408":"norm_data = normalize(data, exclude=['ID_code', 'target'])","328d95b0":"dropcols= ['ID_code', 'target']\n#X = np.array(norm_data.drop(dropcols, 1))\nsave = norm_data.loc[:, dropcols+['var_0']]\nX = norm_data.drop(dropcols, 1)","51c044ee":"X.shape","92c468c7":"save.shape","5b109f96":"del norm_data\ndel train\ndel test\ndel train_test\ndel data","f0e78214":"def inputSwapNoise(arr, p):\n    ### Takes a numpy array and swaps a row of each \n    ### feature with another value from the same column with probability p\n    \n    n, m = arr.shape\n    idx = range(n)\n    swap_n = round(n*p)\n    for i in range(m):\n        col_vals = np.random.permutation(arr[:, i]) # change the order of the row\n        swap_idx = np.random.choice(idx, size= swap_n) # choose row\n        arr[swap_idx, i] = np.random.choice(col_vals, size = swap_n) # n*p row and change it \n    return arr","b0cd2e3f":"class BatchSwapNoise(nn.Module):\n    \"\"\"Swap Noise module\"\"\"\n\n    def __init__(self, p):\n        super().__init__()\n        self.p = p\n\n    def forward(self, x):\n        if self.training:\n            mask = torch.rand(x.size()) > (1 - self.p)\n            idx = torch.add(torch.arange(x.nelement()),\n                            (torch.floor(torch.rand(x.size()) * x.size(0)).type(torch.LongTensor) *\n                             (mask.type(torch.LongTensor) * x.size(1))).view(-1))\n            idx[idx>=x.nelement()] = idx[idx>=x.nelement()]-x.nelement()\n            return x.view(-1)[idx].view(x.size())\n        else:\n            return x","1ff0a421":"class ArraysItemList(FloatList):\n    def __init__(self, items:Iterator, log:bool=False, **kwargs):\n        if isinstance(items, ItemList):\n            items = items.items\n        super(FloatList,self).__init__(items,**kwargs)\n    \n    def get(self,i):\n        return Tensor(super(FloatList,self).get(i).astype('float32'))","33131c6c":"x_il = ArraysItemList(X)\nx_ils = x_il.split_by_rand_pct()\nlls = x_ils.label_from_lists(x_ils.train, x_ils.valid)\ndata = lls.databunch(bs=32)","8ccaee02":"x,y = next(iter(data.train_dl))\nx.shape,y.shape","fc31866e":"data.train_ds","88ca8905":"class Autoencoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.noise = BatchSwapNoise(0.15)\n        self.encoder = nn.Sequential(\n            nn.Linear(200, 300),\n            nn.Linear(300, 300),\n            nn.Linear(300, 300)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(300, 300),\n            nn.Linear(300, 200)\n        )\n\n    def forward(self, xb): \n        encoder = self.encoder(self.noise(xb))\n        decoder = self.decoder(encoder)\n        return decoder","28407e9b":"model = Autoencoder().cuda()","1bcda4e7":"loss_func = F.mse_loss","49b8e66e":"learn = Learner(data, Autoencoder(), loss_func=loss_func)","e83f5ace":"learn.lr_find()\nlearn.recorder.plot(stop_div=False)","c80f5e4a":"learn.fit_one_cycle(10)","14f64ec0":"learn.recorder.plot_losses(True)","ddb33c68":"learn.recorder.plot_lr(True)","b382a80b":"m = learn.model.eval()\njoblib.dump(m, open('m.p', 'wb'))","45c9b362":"#x,y = data.train_ds[0]\n# have to convert into batch before putting into the model\n#xb,_ = data.one_item(x)\n#xb = xb.cuda()","d3fa2e86":"def hooked_backward(xb, cat=y):\n    with hook_output(m.encoder[0]) as hook_a: \n        with hook_output(m.encoder[1]) as hook_b: \n            with hook_output(m.encoder[2]) as hook_c: \n                with hook_output(m.decoder[0]) as hook_d: \n                    with hook_output(m.decoder[1]) as hook_e: \n                        preds = m(xb)\n    return hook_a, hook_b, hook_c, hook_d, hook_e","718cc0b4":"#hook_a, hook_b, hook_c, hook_d, hook_e = hooked_backward(xb)\n#acts_a = hook_a.stored[0].cpu()\n#acts_b = hook_b.stored[0].cpu()\n#acts_c = hook_c.stored[0].cpu()\n#acts_d = hook_d.stored[0].cpu()\n#acts_e = hook_e.stored[0].cpu()\n\n#a = np.concatenate((acts_a, acts_b, acts_c, acts_d, acts_e))\n#b = np.concatenate((acts_a, acts_b, acts_c, acts_d, acts_e))\n#result_array = np.empty((0, 1400))\n\n#np.vstack((result_array, a)).shape","09105499":"data = None\ngc.collect()","83ccc665":"X1 = X.iloc[:int(X.shape[0]*0.25), ]","e80ffcf4":"x_il = ArraysItemList(X1)\nx_ils = x_il.split_none()\nlls = x_ils.label_from_lists(x_ils.train, [])\ndata = x_ils.databunch(bs=32)","f328f070":"def extract_features(data, learner):\n    len_data = len(data.train_ds)\n    result = np.empty((len_data, 1400))\n    for i in tqdm(range(len_data)):\n        x,y = data.train_ds[i]\n        xb,_ = data.one_item(x)\n        xb = xb.cuda()\n        hook_a, hook_b, hook_c, hook_d, hook_e = hooked_backward(xb)\n        \n        acts_a = hook_a.stored[0].cpu()\n        acts_b = hook_b.stored[0].cpu()\n        acts_c = hook_c.stored[0].cpu()\n        acts_d = hook_d.stored[0].cpu()\n        acts_e = hook_e.stored[0].cpu()\n        result[i] = np.concatenate((acts_a, acts_b, acts_c, acts_d, acts_e))\n        \n    return result","9f3e5956":"result = extract_features(data, learn)\n#data = None \n#x_il = None\n#x_ils = None \n#lls = None\n#gc.collect()","ed2ad6a1":"#a = pd.DataFrame(result)\n#result = None\n#gc.collect()","9177884d":"#a = pd.concat([save.iloc[:int(X.shape[0]*0.25), :2], a], axis=1)","fc54f490":"#a.head()","0599df8a":"joblib.dump(a, open('a.p', 'wb'))\na = None\ngc.collect()","7a18c480":"#X2 = X.iloc[int(X.shape[0]*0.25):int(X.shape[0]*0.5), ]\n#x_il = ArraysItemList(X2)\n#x_ils = x_il.split_none()\n#lls = x_ils.label_from_lists(x_ils.train, [])\n#data = x_ils.databunch(bs=32)","1bd71d23":"#result = extract_features(data, learn)\n#data = None \n#x_il = None\n#x_ils = None \n#lls = None\n#gc.collect()\n\n#b = pd.DataFrame(result)\n#b = pd.concat([save.iloc[int(X.shape[0]*0.25):int(X.shape[0]*0.5), :2], b], axis=1)","98d52360":"#joblib.dump(b, open('b.p', 'wb'))\n#b = None\n#gc.collect()","c58f0b5c":"#X3 = X.iloc[int(X.shape[0]*0.5):int(X.shape[0]*0.75), ]\n#x_il = ArraysItemList(X3)\n#x_ils = x_il.split_none()\n#lls = x_ils.label_from_lists(x_ils.train, [])\n#data = x_ils.databunch(bs=32)","ee6b2879":"##result = extract_features(data, learn)\n#data = None \n#x_il = None\n#x_ils = None \n#lls = None\n#gc.collect()\n#c = pd.DataFrame(result)\n#c = pd.concat([save.iloc[int(X.shape[0]*0.5):int(X.shape[0]*0.75), :2], c], axis=1)","90b7f7ac":"##joblib.dump(c, open('c.p', 'wb'))\n#c = None\n#gc.collect()","7996aa5c":"#X4 = X.iloc[int(X.shape[0]*0.75):, ]\n#x_il = ArraysItemList(X4)\n#x_ils = x_il.split_none()\n#lls = x_ils.label_from_lists(x_ils.train, [])\n#data = x_ils.databunch(bs=32)","29bfc748":"#result = extract_features(data, learn)\n\n#data = None \n#x_il = None\n#x_ils = None \n#lls = None\n#gc.collect()\n\n#d = pd.DataFrame(result)\n#d = pd.concat([save.iloc[int(X.shape[0]*0.75):, :2], c], axis=1)","34633a23":"#joblib.dump(d, open('d.p', 'wb'))\n#d = None\n#gc.collect()","8ec0e7d5":"# First batch ","a43219e8":"# Second batch","15222fb6":"This is based on the [Porto Seguro winner's solution](https:\/\/www.google.com\/search?q=porto+seguro+winn+kaggle&oq=porto+seguro+winn+kaggle&aqs=chrome..69i57j69i60l3.10900j0j4&sourceid=chrome&ie=UTF-8).\n\n[Autoencoder](https:\/\/alanbertl.com\/autoencoder-with-fast-ai\/)\n\n[Hook](https:\/\/nbviewer.jupyter.org\/github\/fastai\/course-v3\/blob\/master\/nbs\/dl1\/lesson6-pets-more.ipynb)\n\n[Fast.ai](https:\/\/nbviewer.jupyter.org\/github\/fastai\/course-v3\/blob\/master\/nbs\/dl1\/lesson5-sgd-mnist.ipynb)","3f84d2d6":"# Import modules","6637d424":"# Hook to extract activations","ed80a034":"# Third batch","d8409944":"# Fourth batch","d2849ace":"# DAE","ee1b8284":"# Import dataset and concatenate it","62ca6515":"to see each batch size","ee6c6dc1":"# Rank Guass","9a739e9c":"# Preprocess for Neural Net"}}