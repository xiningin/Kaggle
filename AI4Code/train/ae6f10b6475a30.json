{"cell_type":{"59cd1bca":"code","022f07ad":"code","45885047":"code","7df1b974":"code","616f08eb":"code","a888aa82":"code","e0f57db7":"code","e9305435":"code","3ec91225":"code","e618223a":"code","c6766904":"code","395c2844":"code","ead74cfa":"code","4d87e294":"code","266fc59b":"code","a1fb7231":"code","a7b88f82":"code","0620828f":"code","c9652e94":"code","03aedbe6":"code","b79230c3":"code","3896461d":"code","97967327":"code","c4b510cd":"code","f9b57ad7":"code","fcff4917":"code","aff9fb38":"markdown","5f6e3ff5":"markdown","1a65594e":"markdown","9bcb0aa0":"markdown","6d910a6e":"markdown","f0acfcb8":"markdown","63890e10":"markdown","cff6e9c8":"markdown","5300a9fd":"markdown","d306506b":"markdown","be3380ab":"markdown","7d54c8d0":"markdown","ea7b2295":"markdown","2fa66712":"markdown","e00c0481":"markdown","e0f15061":"markdown","2e13c778":"markdown"},"source":{"59cd1bca":"# Standard imports\nimport os\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import trange\nfrom colorama import Fore\nfrom glob import glob\nimport json\nfrom pprint import pprint\nimport time\nimport cv2\nfrom enum import Enum\nfrom IPython.display import display\n\n# For Data preparation\nfrom sklearn.preprocessing import *\nfrom sklearn.model_selection import *\nfrom sklearn.metrics import *\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","022f07ad":"class Config(Enum):\n    '''\n    It basically contains all the path location and other stuffs\n    \n    '''\n    \n    def __str__(self):\n        return self.value\n\n    TRAIN_CSV = \"..\/input\/petfinder-pawpularity-score\/train.csv\"\n    TEST_CSV = \"..\/input\/petfinder-pawpularity-score\/test.csv\"\n    SAMPLE_CSV = \"..\/input\/petfinder-pawpularity-score\/sample_submission.csv\"\n    TRAIN_DIR = \"..\/input\/petfinder-pawpularity-score\/train\"\n    TEST_DIR = \"..\/input\/petfinder-pawpularity-score\/test\"","45885047":"data_df = pd.read_csv(Config.TRAIN_CSV.value)\ntest_df = pd.read_csv(Config.TEST_CSV.value)\nsample_df = pd.read_csv(Config.SAMPLE_CSV.value)","7df1b974":"data_df","616f08eb":"test_df","a888aa82":"sample_df","e0f57db7":"labels = data_df[\"Pawpularity\"]\nprint(f\"min value of Pawpularity is : {min(labels)}\")\nprint(f\"max value of Pawpularity is : {max(labels)}\")","e9305435":"def giveHistogram(df : \"data File\", col_name : str, bins = None, dark = False):\n    \"\"\"\n    To create histogram plots\n\n    \"\"\"\n    fig = px.histogram(df, x = col_name, template = \"plotly_dark\" if dark else \"ggplot2\", nbins = bins if bins != None else 1 + int(np.log2(len(df))))\n    fig.update_layout(\n            title_text = f\"Distribution of {col_name}\",\n            title_x = 0.5,\n    )\n    fig.show()\n\ngiveHistogram(data_df, \"Pawpularity\")","3ec91225":"data_df[\"path\"] = data_df[\"Id\"].apply(lambda x : Config.TRAIN_DIR.value + f\"\/{x}.jpg\")\ntest_df[\"path\"] = test_df[\"Id\"].apply(lambda x : Config.TEST_DIR.value + f\"\/{x}.jpg\")","e618223a":"def widthAndHeightDist(df : \"data_file\", col_name : \"col name that contains the img path\", dark = False):\n    widths = []; heights = []; bins = 1 + int(np.log2(len(df)))\n    total_images = list(df[col_name].values) \n    for idx in trange(len(total_images), desc = \"Collecting widths and heights...\", bar_format=\"{l_bar}%s{bar:50}%s{r_bar}\" % (Fore.CYAN, Fore.RESET), position = 0, leave = True):\n        cur_path = total_images[idx]\n        h, w, _ = cv2.imread(cur_path).shape\n        widths.append(w)\n        heights.append(h)\n\n    figW = px.histogram(widths, nbins = bins, template = \"plotly_dark\" if dark else \"ggplot2\")\n    figW.update_layout(title = 'Distribution of Image Widths', title_x = 0.5)\n    figW.show();\n    \n    figH = px.histogram(heights, nbins = bins, template = \"plotly_dark\" if dark else \"ggplot2\")\n    figH.update_layout(title = 'Distribution of Image Heights', title_x = 0.5)\n    figH.show();\n    \nwidthAndHeightDist(data_df, \"path\")","c6766904":"def buildGridImages(df : \"data_file\", img_path_col_name: str, label_col_name: str, nrows = 5, ncols = 4, img_size = 512):\n    \"\"\"\n    To build an image grid\n    \"\"\"\n    \n    df = df.sample(nrows*ncols)\n    paths = df[img_path_col_name].values\n    labels = df[label_col_name].values\n\n    text_color = (255, 255, 255)\n    box_color = (0, 0, 0)\n    \n    plt.figure(figsize=(20,12))\n    for i in range(nrows * ncols):\n        plt.subplot(nrows,ncols,i+1)\n        img = cv2.imread(paths[i])\n        img = cv2.resize(img, (img_size, img_size))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        plt.axis(\"off\")\n        plt.title(str(labels[i]))\n        plt.imshow(img)\n\n\n    plt.tight_layout()\n    plt.show()    \n\nbuildGridImages(data_df, \"path\", \"Pawpularity\", 6, 6, 256)","395c2844":"pp_100_df = data_df.loc[data_df.Pawpularity == 100]\npp_1_df = data_df.loc[data_df.Pawpularity == 1]\n\nprint(f\"Num of images having 100 score : {len(pp_100_df)}\")\nprint(f\"Num of images having 1 score : {len(pp_1_df)}\")","ead74cfa":"pp_1_df","4d87e294":"buildGridImages(pp_1_df, \"path\", \"Pawpularity\", 1, 4, 256)","266fc59b":"pp_100_df.head()","a1fb7231":"buildGridImages(pp_100_df, \"path\", \"Pawpularity\", 4, 4, 256)","a7b88f82":"req_cols = [\n    'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n       'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'\n]\n\nfor col in req_cols:\n    tmp_df = data_df.loc[data_df[col] == 1].sample(5)\n    print(f\"################### {col} ###################\")\n    buildGridImages(tmp_df, \"path\", \"Pawpularity\", 1, 5, 256)\n","0620828f":"def create_folds_regression(data, target=\"target\", num_splits = 5): \n    \"\"\"\n    Helper function to create folds\n    \n    \"\"\"\n    data[\"kfold\"] = -1 \n    data = data.sample(frac=1).reset_index(drop=True)\n    \n    # Applying Sturg's rule to calculate the no. of bins for target\n    num_bins = int(1 + np.log2(len(data))) \n\n    data.loc[:, \"bins\"] = pd.cut(data[target], bins=num_bins, labels=False) \n    \n    kf = StratifiedKFold(n_splits=num_splits)\n    \n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)): \n        data.loc[v_, 'kfold'] = f\n        \n    data = data.drop([\"bins\"], axis = 1)         \n    return data \n\n\ndata_df = create_folds_regression(data_df, target = 'Pawpularity', num_splits = 5)\ndata_df.kfold.value_counts()","c9652e94":"data_df.head()","03aedbe6":"# Regression Models\nfrom sklearn.linear_model import LinearRegression, Ridge, ElasticNet\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, VotingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor,GradientBoostingRegressor, StackingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\n\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n# Evalution Metrix\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import cross_validate","b79230c3":"def rmse_score(y_label, y_preds):\n    \"\"\"\n    Gives RMSE score\n    \"\"\"\n    return np.sqrt(mean_squared_error(y_label, y_preds))\n    \n\ndef trainRegModels(df : \"data_file\", features : list, label: str):\n    \"\"\"\n    To automate the training of regression models. Considering\n        > RMSE\n        > R2 score\n    \n    \"\"\"\n    regModels = {\n            \"LinearRegression\": LinearRegression(),\n            \"KNeighborsRegressor\": KNeighborsRegressor(n_neighbors=2),\n            \"AdaBoostRegressor\": AdaBoostRegressor(random_state=0, n_estimators=100),\n            \"LGBMRegressor\": LGBMRegressor(),\n            \"Ridge\": Ridge(alpha=1.0),\n            \"ElasticNet\": ElasticNet(random_state=0),\n            \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=0),\n            \"DecisionTreeRegressor\": DecisionTreeRegressor(),\n            \"ExtraTreesRegressor\": ExtraTreesRegressor(n_jobs=-1),\n            \"RandomForestRegressor\": RandomForestRegressor(n_jobs=-1),\n            \"XGBRegressor\": XGBRegressor(n_jobs=-1),\n            \"CatBoostRegressor\": CatBoostRegressor(iterations=900, depth=5, learning_rate=0.05, loss_function = 'RMSE'),\n        }\n    \n    # Will return this as a data frame\n    summary = {\n        \"Model\" : [],\n        \"Avg R2 Train Score\" : [],\n        \"Avg R2 Val Score\" : [],\n        \"Avg RSME Train Score\" : [],\n        \"Avg RSME Val Score\" : []\n    }\n    \n    # Training\n    for idx in trange(len(regModels.keys()), desc = \"Models are training...\", bar_format=\"{l_bar}%s{bar:50}%s{r_bar}\" % (Fore.CYAN, Fore.RESET), position = 0, leave = True):\n        name = list(regModels.keys())[idx]\n        model = regModels[name]\n        \n        # Initializing all the scores to 0\n        r2_train = 0; r2_val = 0\n        rmse_train = 0; rmse_val = 0\n        \n        # Running K-fold Cross-validation on every model\n        for fold in range(5):\n            train_df = df.loc[df.kfold != fold].reset_index(drop = True)\n            val_df = df.loc[df.kfold == fold].reset_index(drop = True)\n            \n            train_X = train_df[features]; train_Y = train_df[label]\n            val_X = val_df[features]; val_Y = val_df[label]\n            \n            cur_model = model\n            if name == 'CatBoostRegressor':\n                cur_model.fit(train_X, train_Y,verbose=False)\n            else:\n                cur_model.fit(train_X, train_Y)\n\n            Y_train_preds = model.predict(train_X)\n            Y_val_preds = model.predict(val_X)\n            \n            # Collecting the scores\n            r2_train += r2_score(train_Y, Y_train_preds)\n            r2_val += r2_score(val_Y, Y_val_preds)\n            \n            rmse_train += rmse_score(train_Y, Y_train_preds)\n            rmse_val += rmse_score(val_Y, Y_val_preds)\n        \n        # Pushing the scores and the Model names\n        summary[\"Model\"].append(name)\n        summary[\"Avg R2 Train Score\"].append(r2_train\/5)\n        summary[\"Avg R2 Val Score\"].append(r2_val\/5)\n        summary[\"Avg RSME Train Score\"].append(rmse_train\/5)\n        summary[\"Avg RSME Val Score\"].append(rmse_val\/5)\n    \n    # Finally returning the summary dictionary as a dataframe\n    summary_df = pd.DataFrame(summary)\n    return summary_df\n\n","3896461d":"training_summary = trainRegModels(data_df, req_cols, \"Pawpularity\")\ntraining_summary","97967327":"training_summary.sort_values(\"Avg RSME Val Score\", axis = 0, ascending = True)","c4b510cd":"en = ElasticNet(random_state=0)\ngbr = GradientBoostingRegressor(random_state=0)\nVR_model = VotingRegressor([('en', en),('gbr', gbr)], n_jobs=-1)\n\nr2_train = 0; r2_val = 0\nrmse_train = 0; rmse_val = 0\n\nmodel = VR_model\nfor fold in trange(5, desc = \"Models are training...\", bar_format=\"{l_bar}%s{bar:50}%s{r_bar}\" % (Fore.CYAN, Fore.RESET), position = 0, leave = True):\n    train_df = data_df.loc[data_df.kfold != fold].reset_index(drop = True)\n    val_df = data_df.loc[data_df.kfold == fold].reset_index(drop = True)\n\n    train_X = train_df[req_cols]; train_Y = train_df[\"Pawpularity\"]\n    val_X = val_df[req_cols]; val_Y = val_df[\"Pawpularity\"]\n    \n    model.fit(train_X, train_Y)\n\n    Y_train_preds = model.predict(train_X)\n    Y_val_preds = model.predict(val_X)\n\n    # Collecting the scores\n    r2_train += r2_score(train_Y, Y_train_preds)\n    r2_val += r2_score(val_Y, Y_val_preds)\n\n    rmse_train += rmse_score(train_Y, Y_train_preds)\n    rmse_val += rmse_score(val_Y, Y_val_preds)\n\nprint(f\"Avg R2 Train Score : {r2_train\/5}\")\nprint(f\"Avg R2 Val Score : {r2_val\/5}\")\nprint(f\"Avg RSME Train Score : {rmse_train\/5}\")\nprint(f\"Avg RSME Val Score : {rmse_val\/5}\")","f9b57ad7":"sample_df","fcff4917":"test_X = test_df[req_cols]\n\nmodel_preds = model.predict(test_X)\ntest_df[\"Pawpularity\"] = model_preds\n\nsubmission = test_df[[\"Id\", \"Pawpularity\"]]\nsubmission.to_csv(\"submission.csv\", index = False)\ndata_df.to_csv(\"data.csv\", index = False)\ntest_df.to_csv(\"test.csv\", index = False)\nsubmission","aff9fb38":"# Prediction Time \ud83d\ude0e","5f6e3ff5":"# Lets see the label distribution","1a65594e":"***Let's see the combined power of top-2 models***","9bcb0aa0":"#### *Lets look 1 score images*","6d910a6e":"# Lets see the distribution of widths and heights of the images","f0acfcb8":"# Lets create folds of our dataset","63890e10":"#### Lets some perfect score images","cff6e9c8":"\ud83e\udd14 ***Hmm...Nothing much improvement but lets see the submission results***","5300a9fd":"# Mapping the images location ","d306506b":"# Reading Data files","be3380ab":"# Lets automate our training","7d54c8d0":"# Importing Modules","ea7b2295":"# Configs","2fa66712":"#### Lets see some of the images that are 1 for each meta data","e00c0481":"# Lets see the key differences between minimum and maximum marks images\n\n- Minimum is 1\n- Maximum is 100\n- what is **pawpularity** ?\n    - *Feature engineering that the Petfinder team would find valuable would be determining if certain factors from pet profile images increase the popularity of the profile - e.g. \"When dogs wear color collars, their popularity increases by x%\"*\n    - Answered in the [discussion](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/discussion\/274025)","e0f15061":"# Lets look at some images","2e13c778":"#### Observations\n- Pets in the images are getting blended with their background"}}