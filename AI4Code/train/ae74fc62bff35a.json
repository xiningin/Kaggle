{"cell_type":{"be6c2778":"code","8a9598ad":"code","aff9de5d":"code","319de68b":"code","eb4ff71a":"code","29bc8794":"code","d3966b3a":"code","6efba307":"code","6db6aafa":"code","0f237a8c":"code","6271b7bb":"code","55dc2ad4":"code","985d5d08":"code","3fd8746c":"code","a235513b":"code","e1440d66":"code","a8f9d5dd":"code","8aa50b73":"code","5b807bca":"code","9542b82b":"code","759adf56":"code","4b216ab1":"code","a25cf092":"code","f496d0d4":"code","30ccc716":"code","88495873":"code","a37b6c59":"code","e6cf2aaf":"code","66f7cd03":"code","680c527a":"code","3cb599b4":"code","df03f783":"code","cbc0958f":"code","792a2d5b":"code","53ba96b2":"code","0ad15fd3":"code","a88eb480":"code","47f90bc8":"code","1997f935":"code","e3cdc9b3":"markdown","0ef06935":"markdown","afac8952":"markdown","410f76f6":"markdown","764d6f82":"markdown","e9e07173":"markdown","b9fcd345":"markdown","57d33a0a":"markdown","d4a6cce4":"markdown","b6112e2a":"markdown","d86f34e0":"markdown"},"source":{"be6c2778":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xbg\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,classification_report","8a9598ad":"df = pd.read_csv(\"..\/input\/beginner-datasets\/beginner_datasets\/wine.csv\")\ndf.head()","aff9de5d":"df.info()\n\n# looks like we dont have any null values \n# (sometimes some other values can be used instead of null (eg:- -999 or -1), so we'll use df.describe to check for such values)","319de68b":"df.describe()\n\n# looks like we're good but we notice their are some values above the 75th percentile\n# these might be outliers, so we'll check them","eb4ff71a":"# lets do a box plot to check for outliers","29bc8794":"plt.figure(figsize=(15,5))\ndf.iloc[:,:5].boxplot();","d3966b3a":"# use boxplot to see the outliers\nfor col in df.iloc[:,:-1].columns:\n    sns.boxplot(x=df[col])\n    plt.show()\n    \n# some of the first columns have a lot of outliers...\n# for a classification problem we might not have to worry about them as we can use random forest which is unaffected by ouliers\n\n# but ofcourse if we try to use logistic regression then we might need to do something about that...","6efba307":"# lets check their distributions\n\nfor col in df.iloc[:,:-1].columns:\n    sns.distplot(df[col])\n    plt.show()","6db6aafa":"# These distributions are actually quiet good\n# we can apply a log transform to level them up a little bit\n\n# lets try\n\nfor col in df.iloc[:,:-1].columns:\n    sns.distplot(np.log1p(df[col]))     # log1p is good because log(0) is infinity so log1p adds 1 to every value\n    plt.show()","0f237a8c":"# there are other transformations but, i currently need more knowledge about where all each performs best...\n# log and boxcox have a good name...","6271b7bb":"X_log = df.drop(['quality'], axis=1).copy()\ny = df['quality']","55dc2ad4":"for col in X_log.iloc[:,:-1]:\n    X_log[col] = np.log1p(X_log[col])","985d5d08":"# now lets use label encoder on the last column 'type'\n\nencoder = LabelEncoder()\nX_log['type'] = encoder.fit_transform(X_log['type'])","3fd8746c":"# lets standardize it as well...","a235513b":"scaler = StandardScaler()\nX_scaled = pd.DataFrame(scaler.fit_transform(X_log), columns=X_log.columns)","e1440d66":"data = pd.concat([X_scaled,y],axis=1)","a8f9d5dd":"cor = data.corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(cor, annot=True, cmap='Greens');","8aa50b73":"### Lets use a sklearn library","5b807bca":"from sklearn.feature_selection import mutual_info_classif\n\nmutual_info_vals = mutual_info_classif(X_scaled,y)\nmutual_val_df = pd.DataFrame({\"vals\":mutual_info_vals},index=X_scaled.columns) # we're keeping the passenger id\nplt.figure(figsize=(10,5))\nmutual_val_df.vals.sort_values(ascending=False).plot(kind='bar');\n\n\n### this shows us the mutual information (infomation gain)\n\n## lets try one which calculates linear relationship","9542b82b":"from sklearn.feature_selection import f_classif\n\nf_vals,_ = f_classif(X_scaled, y)\nf_vals_df = pd.DataFrame({\"vals\":f_vals},index=X_scaled.columns) # we're keeping the passenger id\nplt.figure(figsize=(10,5))\nf_vals_df.vals.sort_values(ascending=False).plot(kind='bar');","759adf56":"mut_feat = mutual_val_df.vals.sort_values(ascending=False)[:7].index\nlin_feat = f_vals_df.vals.sort_values(ascending=False)[:5].index","4b216ab1":"models = [LogisticRegression(), DecisionTreeClassifier(), RandomForestClassifier() ,xbg.XGBRFClassifier() ,SVC()]\n","a25cf092":"result_cv = []\nfor model in models:\n    result_cv.append((model,cross_val_score(model,X_scaled[mut_feat],y,cv=5)));","f496d0d4":"print(result_cv)","30ccc716":"X_scaled[mut_feat]","88495873":"y_easy = np.where(y>7,1,0)","a37b6c59":"result_cv = []\nfor model in models:\n    result_cv_easy.append(cross_val_score(model,X_scaled[mut_feat],y_easy,cv=5));","e6cf2aaf":"# lets just compute the average score\n\nfor vals in result_cv_easy:\n    print(np.mean(vals))","66f7cd03":"# logistic and svc have preformed the best...","680c527a":"y_type = X_log['type']\nX_type = pd.concat([ X_log.drop(['type'], axis=1), y], axis=1)","3cb599b4":"result_cv_type = []\nfor model in models:\n    result_cv_type.append(cross_val_score(model,X_scaled,y_easy,cv=5));","df03f783":"result_cv_type","cbc0958f":"for vals in result_cv_type:\n    print(np.mean(vals))","792a2d5b":"# im just gonna be using it on good\/bad wine prediction\n# we'll go with logistic regression\n# we will need y_predicted and y_test so lets perform train_test_split on it...\n\nX_train,X_test,y_train,y_test = train_test_split(X_scaled[mut_feat] ,y_easy, test_size = 0.3, random_state=42)","53ba96b2":"log_clf = LogisticRegression()\nlog_clf.fit(X_train,y_train)\nlog_clf.score(X_test,y_test)","0ad15fd3":"y_pred = log_clf.predict(X_test)","a88eb480":"confusion_matrix(y_test,y_pred)\n\n# now, the top right is false positive which means all good wines were acurately predicted...\n# the bottom left one is the false negative which are the negative values that were falsely predicted...\n\n### yeah... it classified everything as good and we had less bad wine in test set (50) so our accuracy was high...\n\n### lets check other's","47f90bc8":"for model in models:\n    model.fit(X_train,y_train)\n    print(confusion_matrix(y_test,y_pred),end='\\n\\n')\n    \n# all are bad models...","1997f935":"print(classification_report(y_test,y_pred))","e3cdc9b3":"### Feature engineering","0ef06935":"### Imports","afac8952":"## EDA","410f76f6":"### I've seen in towardsdatascence post (by Terence shin ) that you can divide the wine category into 2 ( that is <7 is bad, >7 is good)\n### we'll try that\n### he also made subsets of data based on good and bad quality and saw how do they vary? ( use dot describe on subsets to see the means, max, mins of independent features... that was good)","764d6f82":"### Their aren't that many correlations for quality...\n\n### But for \"type\" there is a lot of correlation from independent features (thinking of taking it instead, maybe in the end)","e9e07173":"### This is a classification problem\n### we will have to find the quality of wine given the various independent features","b9fcd345":"### Feature selection","57d33a0a":"### Remember we had talked about the using the dataset to predict the wine type?\n### let's go... we'll use the x_log version","d4a6cce4":"## We can use sampling techniques on this to make it predict both good and bad wines but this is my first notebook so like...           \"Thank you\"","b6112e2a":"### Further we would use some metrics to check the performance and so hyperparameter tuning...","d86f34e0":"### model building"}}