{"cell_type":{"bfd4a4ef":"code","e1c64cb3":"code","6aaf0998":"code","73cb57e0":"code","9d69f6e3":"code","d73e7d03":"code","39d09c6d":"code","3b13e0ff":"code","6f253d48":"code","dbfac48e":"code","cca4c229":"code","2b23d983":"code","ff3ec458":"code","362ae052":"code","6673bbfa":"code","df080aa3":"code","f00dae8d":"code","ff98469a":"code","8b6eb1b8":"code","506d77cf":"code","ad8c7eb3":"code","50a5637b":"code","b0b89d21":"code","94052c6c":"code","147b4844":"code","3634cc6e":"code","605693f9":"code","05d1477c":"code","5bd7a077":"code","4401ab15":"markdown","b61e262c":"markdown","f55c2230":"markdown","58778ae5":"markdown","a6a3ce84":"markdown","df138047":"markdown","ac19d021":"markdown","3e6ea938":"markdown","116e697f":"markdown","1f64583b":"markdown","36919ba2":"markdown","eb9a95f2":"markdown","7b4e2048":"markdown","1793fe57":"markdown","706c4512":"markdown","23258b4f":"markdown","7d2239aa":"markdown","4712eb4f":"markdown","9f44bc8b":"markdown","f3cf95f9":"markdown","59dc9c94":"markdown"},"source":{"bfd4a4ef":"# Familiar imports\nimport numpy as np\nimport pandas as pd\n# For ordinal encoding categorical variables, splitting data\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\n# For training random forest model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\nimport warnings\nwarnings.filterwarnings('ignore')","e1c64cb3":"# import lux\n# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\n\n# Preview the data\ntrain.head()","6aaf0998":"#train.save_as_html('eda.html')","73cb57e0":"train.isnull().sum().values","9d69f6e3":"train.describe()","d73e7d03":"# Check the structure of the data\nprint(train.info())","39d09c6d":"n, bins, patches = plt.hist(x=train['target'], bins='auto', color='blue',alpha=0.7, rwidth=0.5)\nplt.xlabel(\"Loan Amount\")\nplt.show()","3b13e0ff":"#Looking unique values\nl=dict(train.nunique())\nprint(l)","6f253d48":"import seaborn as sns \nred = sns.light_palette(\"red\", as_cmap=True)\ncross_tab=pd.crosstab(train['cat0'], train['cat1'], margins = True)\nH=cross_tab\/cross_tab.loc[\"All\"] # Divide by column totals\nH.style.background_gradient(cmap=red)","dbfac48e":"# Create a cross table of the loan intent and loan status\nt1=pd.crosstab(train['cat0'], train['cat2'], margins = True)\nH=t1\/t1.loc[\"All\"] # Divide by column totals\nH.style.background_gradient(cmap=red)","cca4c229":"t2=pd.crosstab(train['cat0'], train['cat1'],values=train['target'], aggfunc='mean')\n#H=cross_tab\/cross_tab.loc[\"All\"] # Divide by column totals\nt2.style.background_gradient(cmap=red)","2b23d983":"train.boxplot(column = ['target'], by = 'cat0')\nplt.title('Average Percent capital  by Loan Status')\nplt.suptitle('')\nplt.show()","ff3ec458":"train['target'].describe()","362ae052":"import seaborn as sns \nwarnings.filterwarnings(\"ignore\")\nsns.heatmap(train.corr(), square=True, cmap='RdYlGn')","6673bbfa":"train.dtypes","df080aa3":"import matplotlib.pyplot as plt\n# select the cat columns\ncat_columns = train.select_dtypes(exclude=['int64','float64']).columns\n# Create a histogram\n# create distplots\n\n    \nfig, axes =plt.subplots(5,2, figsize=(10,10), sharex=True)\naxes = axes.flatten()\n\nfor ax, catplot in zip(axes, cat_columns):\n    sns.countplot(y=catplot, data=train, ax=ax)\n\nplt.tight_layout()  \nplt.show()","f00dae8d":"import matplotlib.pyplot as plt\nimport seaborn as sns \n# select the float columns\nnum_columns = train.select_dtypes(include=['int64','float64']).columns\n# Create a histogram\n# create distplots\n\n    \nfig, axes =plt.subplots(8,2, figsize=(15,15), sharex=True)\naxes = axes.flatten()\n\nfor ax, numplot in zip(axes, num_columns):\n     sns.distplot(train[numplot],ax=ax,kde=True)\n\nplt.tight_layout()  \nplt.show()","ff98469a":"train.var()","8b6eb1b8":"X_num = train.select_dtypes(include=['int64','float64']).drop(['id','target'], axis=1)\nsns.boxplot(data=X_num.melt(value_vars=X_num.columns),\n            x='variable', y='value')","506d77cf":"import seaborn as sns \nwarnings.filterwarnings(\"ignore\")\n\nsns.pairplot(train)","ad8c7eb3":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import KNeighborsRegressor\nknn = KNeighborsRegressor()\n#### Create the encoder.\n# Create arrays for the features and the response variable\ny = train['target'].to_numpy()\nX = train.select_dtypes(include=['int64','float64']).drop(['id','target'], axis=1).to_numpy()","50a5637b":"train.select_dtypes(include=['int64','float64']).drop(['id','target'], axis=1).columns","b0b89d21":"X.shape","94052c6c":"# Split the dataset and labels into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,y)\n# Fit the k-nearest neighbors model to the training data\nknn.fit(X_train, y_train)\n# Score the model on the test data\nprint(knn.score(X_test, y_test))","147b4844":"# Import StandardScaler from scikit-learn\nfrom sklearn.preprocessing import StandardScaler\n# Create the scaling method.\nss = StandardScaler()\n\n# Apply the scaling method to the dataset used for modeling.\nX_scaled =ss.fit_transform(X) \nX_train, X_test, y_train, y_test = train_test_split(X_scaled,y)\n\n# Fit the k-nearest neighbors model to the training data\nknn.fit(X_train, y_train)\n\n# Score the model on the test data\nprint(knn.score(X_test, y_test))","3634cc6e":"#knn_r_acc = []\n#for i in range(1,5,1):\n #   knn = KNeighborsRegressor(n_neighbors=i)\n  #  knn.fit(X_train,y_train)   \n   # test_score = knn.score(X_test,y_test)\n    #train_score = knn.score(X_train,y_train)  \n   # knn_r_acc.append((i, test_score ,train_score))\n    \n    #df = pd.DataFrame(knn_r_acc, columns=['K','Test Score','Train Score'])\n#print(df)","605693f9":"knn_without_cat = KNeighborsRegressor(n_neighbors=2)\nknn_without_cat.fit(X_train,y_train)\npreds_valid = knn_without_cat.predict(X_test)\nprint(mean_squared_error(y_test, preds_valid, squared=False))","05d1477c":"y = train['target'].to_numpy()\nX = train.select_dtypes(include=['int64','float64']).drop(['id','target'], axis=1).to_numpy()\nknn_without_cat_final  = KNeighborsRegressor(n_neighbors=2)\nX_final_scaled =ss.fit_transform(X) \nknn_without_cat_final.fit(X_final_scaled,y)","5bd7a077":"test_num= test.select_dtypes(include=['int64','float64']).drop(['id'], axis=1).to_numpy()\n# Use the model to generate predictions\ntest_num_scaled =ss.fit_transform(test_num) \npredictions = knn_without_cat_final.predict(test_num_scaled)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': test.id,\n                       'target': predictions})\noutput.to_csv('submission1.csv', index=False)","4401ab15":"# Cat Features ","b61e262c":"\n# Step 2: Load the data\n\nNext, we'll load the training and test data.\n\nWe set index_col=0 in the code cell below to use the id column to index the DataFrame. (If you're not sure how this works, try temporarily removing index_col=0 and see how it changes the result.)\n","f55c2230":"# Num Features","58778ae5":"#  Submit to the competition\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file.\n","a6a3ce84":"# Step 1: Import helpful libraries","df138047":"# Null","ac19d021":"# To be contiuned next notebook  to add add cat features ","3e6ea938":"# Crosstab","116e697f":"# KNN on non-scaled data","1f64583b":"# Modeling without normalizing\n![image.png](attachment:b018925c-f1d7-4cba-a566-9608a921cedd.png)","36919ba2":"# boxplot:\n","eb9a95f2":"# Correlation","7b4e2048":"# Distribution of Target","1793fe57":"# Hyper param tuning","706c4512":"# Explore the  data\n- Null Data\n- Categorical data \n- Is there Text data \n- wich columns will we use \n- IS there outliers that can destory our algo \n- IS there diffrent range  of data ","23258b4f":"# Box_plot\n","7d2239aa":"# Feature engineering\n\n##  What is feature engineering?\n\nFeature engineering is the creation of new features based on existing features, and it adds information to your dataset that is useful in some way: it adds features useful for your prediction or clustering task, or it sheds insight into relationships between features. Real world data is often not neat and tidy, and in addition to preprocessing steps like standardization, you'll likely have to extract and expand information that exists in the columns in your dataset. Feature engineering is a subject that could definitely be given its own entire course, so we're just going to go over some basics","4712eb4f":"# Train on all data ","9f44bc8b":"# Evaluate model :\n","f3cf95f9":"# Scaling data for num feature comparison","59dc9c94":"#  Working with data types"}}